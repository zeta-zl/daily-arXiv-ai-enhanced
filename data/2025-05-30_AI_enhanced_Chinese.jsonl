{"id": "2505.22698", "pdf": "https://arxiv.org/pdf/2505.22698", "abs": "https://arxiv.org/abs/2505.22698", "authors": ["Luca Fantin", "Marco Antonelli", "Margherita Cesetti", "Daniele Irto", "Bruno Zamengo", "Francesco Silvestri"], "title": "Design and testing of an agent chatbot supporting decision making with public transport data", "categories": ["cs.AI"], "comment": null, "summary": "Assessing the quality of public transportation services requires the analysis\nof large quantities of data on the scheduled and actual trips and documents\nlisting the quality constraints each service needs to meet. Interrogating such\ndatasets with SQL queries, organizing and visualizing the data can be quite\ncomplex for most users. This paper presents a chatbot offering a user-friendly\ntool to interact with these datasets and support decision making. It is based\non an agent architecture, which expands the capabilities of the core Large\nLanguage Model (LLM) by allowing it to interact with a series of tools that can\nexecute several tasks, like performing SQL queries, plotting data and creating\nmaps from the coordinates of a trip and its stops. This paper also tackles one\nof the main open problems of such Generative AI projects: collecting data to\nmeasure the system's performance. Our chatbot has been extensively tested with\na workflow that asks several questions and stores the generated query, the\nretrieved data and the natural language response for each of them. Such\nquestions are drawn from a set of base examples which are then completed with\nactual data from the database. This procedure yields a dataset for the\nevaluation of the chatbot's performance, especially the consistency of its\nanswers and the correctness of the generated queries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u67b6\u6784\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u7528\u4e8e\u7b80\u5316\u516c\u5171\u4ea4\u901a\u670d\u52a1\u8d28\u91cf\u5206\u6790\u7684\u590d\u6742\u6570\u636e\u67e5\u8be2\u4e0e\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u5206\u6790\u516c\u5171\u4ea4\u901a\u670d\u52a1\u8d28\u91cf\u9700\u8981\u5904\u7406\u5927\u91cf\u6570\u636e\u548c\u590d\u6742SQL\u67e5\u8be2\uff0c\u8fd9\u5bf9\u5927\u591a\u6570\u7528\u6237\u8f83\u4e3a\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u7684\u4ea4\u4e92\u5de5\u5177\u3002", "method": "\u91c7\u7528\u4ee3\u7406\u67b6\u6784\u6269\u5c55\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u529b\uff0c\u96c6\u6210SQL\u67e5\u8be2\u3001\u6570\u636e\u53ef\u89c6\u5316\u548c\u5730\u56fe\u751f\u6210\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u5de5\u4f5c\u6d41\u6536\u96c6\u6027\u80fd\u8bc4\u4f30\u6570\u636e\u3002", "result": "\u804a\u5929\u673a\u5668\u4eba\u7ecf\u8fc7\u5e7f\u6cdb\u6d4b\u8bd5\uff0c\u751f\u6210\u4e86\u7528\u4e8e\u8bc4\u4f30\u5176\u56de\u7b54\u4e00\u81f4\u6027\u548c\u67e5\u8be2\u6b63\u786e\u6027\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u804a\u5929\u673a\u5668\u4eba\u6709\u6548\u7b80\u5316\u4e86\u516c\u5171\u4ea4\u901a\u6570\u636e\u5206\u6790\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u751f\u6210\u5f0fAI\u9879\u76ee\u6027\u80fd\u8bc4\u4f30\u6570\u636e\u6536\u96c6\u7684\u96be\u70b9\u3002", "keywords": "\u516c\u5171\u4ea4\u901a, \u6570\u636e\u5206\u6790, \u804a\u5929\u673a\u5668\u4eba, LLM, SQL\u67e5\u8be2"}}
{"id": "2505.22753", "pdf": "https://arxiv.org/pdf/2505.22753", "abs": "https://arxiv.org/abs/2505.22753", "authors": ["Arseniy Pertzovsky", "Roni Stern", "Ariel Felner", "Roie Zivan"], "title": "Enhancing Lifelong Multi-Agent Path-finding by Using Artificial Potential Fields", "categories": ["cs.AI", "cs.MA", "cs.RO"], "comment": null, "summary": "We explore the use of Artificial Potential Fields (APFs) to solve Multi-Agent\nPath Finding (MAPF) and Lifelong MAPF (LMAPF) problems. In MAPF, a team of\nagents must move to their goal locations without collisions, whereas in LMAPF,\nnew goals are generated upon arrival. We propose methods for incorporating APFs\nin a range of MAPF algorithms, including Prioritized Planning, MAPF-LNS2, and\nPriority Inheritance with Backtracking (PIBT). Experimental results show that\nusing APF is not beneficial for MAPF but yields up to a 7-fold increase in\noverall system throughput for LMAPF.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u4eba\u5de5\u52bf\u573a\uff08APF\uff09\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff08MAPF\uff09\u53ca\u7ec8\u8eabMAPF\uff08LMAPF\uff09\u95ee\u9898\uff0c\u53d1\u73b0APF\u5bf9MAPF\u65e0\u663e\u8457\u5e2e\u52a9\uff0c\u4f46\u5bf9LMAPF\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u63a2\u7d22APF\u5728MAPF\u548cLMAPF\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4ee5\u4f18\u5316\u667a\u80fd\u4f53\u7684\u8def\u5f84\u89c4\u5212\u6548\u7387\u53ca\u7cfb\u7edf\u541e\u5410\u91cf\u3002", "method": "\u5c06APF\u96c6\u6210\u5230\u591a\u79cdMAPF\u7b97\u6cd5\uff08\u5982\u4f18\u5148\u89c4\u5212\u3001MAPF-LNS2\u3001PIBT\uff09\u4e2d\uff0c\u5bf9\u6bd4\u5206\u6790\u5176\u5728MAPF\u4e0eLMAPF\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAPF\u5bf9MAPF\u65e0\u6539\u5584\uff0c\u4f46\u4f7fLMAPF\u7cfb\u7edf\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe7\u500d\u3002", "conclusion": "APF\u9002\u7528\u4e8e\u52a8\u6001\u76ee\u6807\u573a\u666f\uff08LMAPF\uff09\uff0c\u4f46\u5bf9\u9759\u6001MAPF\u95ee\u9898\u6548\u679c\u6709\u9650\u3002", "keywords": "\u4eba\u5de5\u52bf\u573a, \u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212, \u7ec8\u8eabMAPF, \u4f18\u5148\u89c4\u5212, PIBT"}}
{"id": "2505.22756", "pdf": "https://arxiv.org/pdf/2505.22756", "abs": "https://arxiv.org/abs/2505.22756", "authors": ["Tian Qin", "Core Francisco Park", "Mujin Kwun", "Aaron Walsman", "Eran Malach", "Nikhil Anand", "Hidenori Tanaka", "David Alvarez-Melis"], "title": "Decomposing Elements of Problem Solving: What \"Math\" Does RL Teach?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Mathematical reasoning tasks have become prominent benchmarks for assessing\nthe reasoning capabilities of LLMs, especially with reinforcement learning (RL)\nmethods such as GRPO showing significant performance gains. However, accuracy\nmetrics alone do not support fine-grained assessment of capabilities and fail\nto reveal which problem-solving skills have been internalized. To better\nunderstand these capabilities, we propose to decompose problem solving into\nfundamental capabilities: Plan (mapping questions to sequences of steps),\nExecute (correctly performing solution steps), and Verify (identifying the\ncorrectness of a solution). Empirically, we find that GRPO mainly enhances the\nexecution skill-improving execution robustness on problems the model already\nknows how to solve-a phenomenon we call temperature distillation. More\nimportantly, we show that RL-trained models struggle with fundamentally new\nproblems, hitting a 'coverage wall' due to insufficient planning skills. To\nexplore RL's impact more deeply, we construct a minimal, synthetic\nsolution-tree navigation task as an analogy for mathematical problem-solving.\nThis controlled setup replicates our empirical findings, confirming RL\nprimarily boosts execution robustness. Importantly, in this setting, we\nidentify conditions under which RL can potentially overcome the coverage wall\nthrough improved exploration and generalization to new solution paths. Our\nfindings provide insights into the role of RL in enhancing LLM reasoning,\nexpose key limitations, and suggest a path toward overcoming these barriers.\nCode is available at https://github.com/cfpark00/RL-Wall.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5c06\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u5206\u89e3\u4e3a\u8ba1\u5212\u3001\u6267\u884c\u548c\u9a8c\u8bc1\u4e09\u4e2a\u57fa\u672c\u80fd\u529b\uff0c\u5206\u6790\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u4f5c\u7528\u3002\u7814\u7a76\u53d1\u73b0RL\u4e3b\u8981\u63d0\u5347\u6267\u884c\u80fd\u529b\uff0c\u4f46\u5bf9\u65b0\u95ee\u9898\u56e0\u8ba1\u5212\u80fd\u529b\u4e0d\u8db3\u800c\u9047\u5230\u201c\u8986\u76d6\u5899\u201d\u3002\u4f5c\u8005\u901a\u8fc7\u5408\u6210\u4efb\u52a1\u9a8c\u8bc1\u4e86RL\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u514b\u670d\u8fd9\u4e00\u969c\u788d\u7684\u53ef\u80fd\u8def\u5f84\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982\u51c6\u786e\u7387\uff09\u65e0\u6cd5\u7cbe\u7ec6\u5206\u6790LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5728\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u5177\u4f53\u4f5c\u7528\u3002\u4f5c\u8005\u5e0c\u671b\u63ed\u793aRL\u5982\u4f55\u5f71\u54cdLLM\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u5e76\u8bc6\u522b\u5176\u5c40\u9650\u6027\u3002", "method": "\u5c06\u95ee\u9898\u89e3\u51b3\u5206\u89e3\u4e3a\u8ba1\u5212\uff08Plan\uff09\u3001\u6267\u884c\uff08Execute\uff09\u3001\u9a8c\u8bc1\uff08Verify\uff09\u4e09\u4e2a\u5b50\u80fd\u529b\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790GRPO\u5bf9\u6bcf\u9879\u80fd\u529b\u7684\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5408\u6210\u7684\u89e3\u51b3\u65b9\u6848\u6811\u5bfc\u822a\u4efb\u52a1\uff0c\u4ee5\u6a21\u62df\u6570\u5b66\u95ee\u9898\u89e3\u51b3\uff0c\u9a8c\u8bc1RL\u7684\u4f5c\u7528\u3002", "result": "RL\uff08\u5982GRPO\uff09\u4e3b\u8981\u63d0\u5347\u6267\u884c\u80fd\u529b\uff08\u6e29\u5ea6\u84b8\u998f\u73b0\u8c61\uff09\uff0c\u4f46\u5bf9\u65b0\u95ee\u9898\u56e0\u8ba1\u5212\u80fd\u529b\u4e0d\u8db3\u800c\u9047\u5230\u201c\u8986\u76d6\u5899\u201d\u3002\u5408\u6210\u4efb\u52a1\u9a8c\u8bc1\u4e86RL\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u4e5f\u53d1\u73b0\u901a\u8fc7\u6539\u8fdb\u63a2\u7d22\u548c\u6cdb\u5316\u80fd\u529b\u53ef\u80fd\u514b\u670d\u8fd9\u4e00\u969c\u788d\u3002", "conclusion": "\u7814\u7a76\u8868\u660eRL\u80fd\u6709\u6548\u63d0\u5347LLM\u7684\u6267\u884c\u80fd\u529b\uff0c\u4f46\u8ba1\u5212\u80fd\u529b\u4ecd\u662f\u74f6\u9888\u3002\u672a\u6765\u9700\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u6539\u8fdb\u63a2\u7d22\u548c\u6cdb\u5316\u6765\u7a81\u7834\u201c\u8986\u76d6\u5899\u201d\u3002", "keywords": "\u6570\u5b66\u63a8\u7406, \u5f3a\u5316\u5b66\u4e60, \u5927\u8bed\u8a00\u6a21\u578b, \u6e29\u5ea6\u84b8\u998f, \u8986\u76d6\u5899"}}
{"id": "2505.22779", "pdf": "https://arxiv.org/pdf/2505.22779", "abs": "https://arxiv.org/abs/2505.22779", "authors": ["Mohammad Helal Uddin", "Sabur Baidya"], "title": "Predicting Human Depression with Hybrid Data Acquisition utilizing Physical Activity Sensing and Social Media Feeds", "categories": ["cs.AI"], "comment": null, "summary": "Mental disorders including depression, anxiety, and other neurological\ndisorders pose a significant global challenge, particularly among individuals\nexhibiting social avoidance tendencies. This study proposes a hybrid approach\nby leveraging smartphone sensor data measuring daily physical activities and\nanalyzing their social media (Twitter) interactions for evaluating an\nindividual's depression level. Using CNN-based deep learning models and Naive\nBayes classification, we identify human physical activities accurately and also\nclassify the user sentiments. A total of 33 participants were recruited for\ndata acquisition, and nine relevant features were extracted from the physical\nactivities and analyzed with their weekly depression scores, evaluated using\nthe Geriatric Depression Scale (GDS) questionnaire. Of the nine features, six\nare derived from physical activities, achieving an activity recognition\naccuracy of 95%, while three features stem from sentiment analysis of Twitter\nactivities, yielding a sentiment analysis accuracy of 95.6%. Notably, several\nphysical activity features exhibited significant correlations with the severity\nof depression symptoms. For classifying the depression severity, a support\nvector machine (SVM)-based algorithm is employed that demonstrated a very high\naccuracy of 94%, outperforming alternative models, e.g., the multilayer\nperceptron (MLP) and k-nearest neighbor. It is a simple approach yet highly\neffective in the long run for monitoring depression without breaching personal\nprivacy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7ed3\u5408\u667a\u80fd\u624b\u673a\u4f20\u611f\u5668\u6570\u636e\u548c\u793e\u4ea4\u5a92\u4f53\u5206\u6790\uff0c\u901a\u8fc7CNN\u548c\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u8bc4\u4f30\u6291\u90c1\u6c34\u5e73\uff0cSVM\u6a21\u578b\u5728\u6291\u90c1\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe94%\u3002", "motivation": "\u5fc3\u7406\u75be\u75c5\uff08\u5982\u6291\u90c1\u548c\u7126\u8651\uff09\u5bf9\u5177\u6709\u793e\u4ea4\u56de\u907f\u503e\u5411\u7684\u4e2a\u4f53\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u9700\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u65b9\u6cd5\u957f\u671f\u76d1\u6d4b\u6291\u90c1\u75c7\u72b6\u3002", "method": "\u4f7f\u7528\u667a\u80fd\u624b\u673a\u4f20\u611f\u5668\u6570\u636e\uff08\u516d\u9879\u7279\u5f81\uff0c\u6d3b\u52a8\u8bc6\u522b\u51c6\u786e\u738795%\uff09\u548cTwitter\u60c5\u611f\u5206\u6790\uff08\u4e09\u9879\u7279\u5f81\uff0c\u51c6\u786e\u738795.6%\uff09\uff0c\u7ed3\u5408CNN\u3001\u6734\u7d20\u8d1d\u53f6\u65af\u548cSVM\u6a21\u578b\u8fdb\u884c\u6291\u90c1\u8bc4\u4f30\u3002", "result": "SVM\u5728\u6291\u90c1\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u4e2d\u51c6\u786e\u7387\u6700\u9ad8\uff0894%\uff09\uff0c\u4f18\u4e8eMLP\u548cKNN\uff1b\u591a\u9879\u8eab\u4f53\u6d3b\u52a8\u7279\u5f81\u4e0e\u6291\u90c1\u75c7\u72b6\u4e25\u91cd\u7a0b\u5ea6\u663e\u8457\u76f8\u5173\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u9690\u79c1\u53cb\u597d\uff0c\u4e3a\u957f\u671f\u6291\u90c1\u76d1\u6d4b\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\u3002", "keywords": "\u6291\u90c1\u76d1\u6d4b, \u667a\u80fd\u624b\u673a\u4f20\u611f\u5668, \u793e\u4ea4\u5a92\u4f53\u5206\u6790, CNN, SVM"}}
{"id": "2505.22686", "pdf": "https://arxiv.org/pdf/2505.22686", "abs": "https://arxiv.org/abs/2505.22686", "authors": ["Ange-Clement Akazan", "Verlon Roel Mbingui", "Gnankan Landry Regis N'guessan", "Issa Karambal"], "title": "Localized Weather Prediction Using Kolmogorov-Arnold Network-Based Models and Deep RNNs", "categories": ["cs.LG"], "comment": null, "summary": "Weather forecasting is crucial for managing risks and economic planning,\nparticularly in tropical Africa, where extreme events severely impact\nlivelihoods. Yet, existing forecasting methods often struggle with the region's\ncomplex, non-linear weather patterns. This study benchmarks deep recurrent\nneural networks such as $\\texttt{LSTM, GRU, BiLSTM, BiGRU}$, and\nKolmogorov-Arnold-based models $(\\texttt{KAN} and \\texttt{TKAN})$ for daily\nforecasting of temperature, precipitation, and pressure in two tropical cities:\nAbidjan, Cote d'Ivoire (Ivory Coast) and Kigali (Rwanda). We further introduce\ntwo customized variants of $ \\texttt{TKAN}$ that replace its original\n$\\texttt{SiLU}$ activation function with $ \\texttt{GeLU}$ and \\texttt{MiSH},\nrespectively. Using station-level meteorological data spanning from 2010 to\n2024, we evaluate all the models on standard regression metrics. $\\texttt{KAN}$\nachieves temperature prediction ($R^2=0.9986$ in Abidjan, $0.9998$ in Kigali,\n$\\texttt{MSE} < 0.0014~^\\circ C ^2$), while $\\texttt{TKAN}$ variants minimize\nabsolute errors for precipitation forecasting in low-rainfall regimes. The\ncustomized $\\texttt{TKAN}$ models demonstrate improvements over the standard\n$\\texttt{TKAN}$ across both datasets. Classical \\texttt{RNNs} remain highly\ncompetitive for atmospheric pressure ($R^2 \\approx 0.83{-}0.86$), outperforming\n$\\texttt{KAN}$-based models in this task. These results highlight the potential\nof spline-based neural architectures for efficient and data-efficient\nforecasting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6bd4\u8f83\u4e86\u591a\u79cd\u6df1\u5ea6\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08\u5982LSTM\u3001GRU\u7b49\uff09\u548c\u57fa\u4e8eKolmogorov-Arnold\u7684\u6a21\u578b\uff08KAN/TKAN\uff09\u5728\u70ed\u5e26\u975e\u6d32\u4e24\u4e2a\u57ce\u5e02\u7684\u5929\u6c14\u9884\u62a5\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u6539\u8fdb\u7684TKAN\u53d8\u4f53\u3002\u7ed3\u679c\u8868\u660eKAN\u5728\u6e29\u5ea6\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cTKAN\u53d8\u4f53\u5728\u4f4e\u964d\u96e8\u91cf\u964d\u6c34\u9884\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800c\u7ecf\u5178RNN\u5728\u6c14\u538b\u9884\u6d4b\u4e2d\u4f18\u4e8eKAN\u3002", "motivation": "\u70ed\u5e26\u975e\u6d32\u590d\u6742\u7684\u975e\u7ebf\u6027\u5929\u6c14\u6a21\u5f0f\u4f7f\u5f97\u73b0\u6709\u5929\u6c14\u9884\u62a5\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u6570\u636e\u9a71\u52a8\u6a21\u578b\u6765\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "method": "\u7814\u7a76\u5bf9\u6bd4\u4e86\u591a\u79cd\u6df1\u5ea6\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u548cKAN/TKAN\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u79cd\u6539\u8fdb\u7684TKAN\u53d8\u4f53\uff08\u66ff\u6362\u6fc0\u6d3b\u51fd\u6570\u4e3aGeLU\u548cMiSH\uff09\uff0c\u4f7f\u75282010-2024\u5e74\u6c14\u8c61\u6570\u636e\u8fdb\u884c\u56de\u5f52\u8bc4\u4f30\u3002", "result": "KAN\u5728\u6e29\u5ea6\u9884\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff08R\u00b2\u63a5\u8fd11\uff09\uff0c\u6539\u8fdb\u7684TKAN\u53d8\u4f53\u5728\u964d\u6c34\u9884\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u7ecf\u5178RNN\u5728\u6c14\u538b\u9884\u6d4b\u4e2d\u4f18\u4e8eKAN\u3002", "conclusion": "\u57fa\u4e8e\u6837\u6761\u7684\u795e\u7ecf\u67b6\u6784\uff08\u5982KAN/TKAN\uff09\u5728\u9ad8\u6548\u548c\u6570\u636e\u9ad8\u6548\u7684\u5929\u6c14\u9884\u62a5\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u5404\u5f02\u3002", "keywords": "\u5929\u6c14\u9884\u62a5, \u70ed\u5e26\u975e\u6d32, \u6df1\u5ea6\u5faa\u73af\u795e\u7ecf\u7f51\u7edc, Kolmogorov-Arnold\u7f51\u7edc, \u6e29\u5ea6\u9884\u6d4b, \u964d\u6c34\u9884\u6d4b, \u6c14\u538b\u9884\u6d4b"}}
{"id": "2505.22704", "pdf": "https://arxiv.org/pdf/2505.22704", "abs": "https://arxiv.org/abs/2505.22704", "authors": ["Feng Yao", "Zilong Wang", "Liyuan Liu", "Junxia Cui", "Li Zhong", "Xiaohan Fu", "Haohui Mai", "Vish Krishnan", "Jianfeng Gao", "Jingbo Shang"], "title": "Training Language Models to Generate Quality Code with Program Analysis Feedback", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Code generation with large language models (LLMs), often termed vibe coding,\nis increasingly adopted in production but fails to ensure code quality,\nparticularly in security (e.g., SQL injection vulnerabilities) and\nmaintainability (e.g., missing type annotations). Existing methods, such as\nsupervised fine-tuning and rule-based post-processing, rely on labor-intensive\nannotations or brittle heuristics, limiting their scalability and\neffectiveness. We propose REAL, a reinforcement learning framework that\nincentivizes LLMs to generate production-quality code using program\nanalysis-guided feedback. Specifically, REAL integrates two automated signals:\n(1) program analysis detecting security or maintainability defects and (2) unit\ntests ensuring functional correctness. Unlike prior work, our framework is\nprompt-agnostic and reference-free, enabling scalable supervision without\nmanual intervention. Experiments across multiple datasets and model scales\ndemonstrate that REAL outperforms state-of-the-art methods in simultaneous\nassessments of functionality and code quality. Our work bridges the gap between\nrapid prototyping and production-ready code, enabling LLMs to deliver both\nspeed and quality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aREAL\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5206\u6790\u548c\u5355\u5143\u6d4b\u8bd5\u53cd\u9988\uff0c\u6fc0\u52b1\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u4ee3\u7801\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6216\u89c4\u5219\u540e\u5904\u7406\uff0c\u96be\u4ee5\u786e\u4fdd\u4ee3\u7801\u7684\u5b89\u5168\u6027\uff08\u5982SQL\u6ce8\u5165\u6f0f\u6d1e\uff09\u548c\u53ef\u7ef4\u62a4\u6027\uff08\u5982\u7c7b\u578b\u6ce8\u89e3\u7f3a\u5931\uff09\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "REAL\u6846\u67b6\u7ed3\u5408\u4e24\u79cd\u81ea\u52a8\u4fe1\u53f7\uff1a\u7a0b\u5e8f\u5206\u6790\u68c0\u6d4b\u5b89\u5168\u6216\u53ef\u7ef4\u62a4\u6027\u7f3a\u9677\uff0c\u4ee5\u53ca\u5355\u5143\u6d4b\u8bd5\u786e\u4fdd\u529f\u80fd\u6b63\u786e\u6027\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5373\u53ef\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u76d1\u7763\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cREAL\u5728\u529f\u80fd\u6027\u548c\u4ee3\u7801\u8d28\u91cf\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "REAL\u586b\u8865\u4e86\u5feb\u901f\u539f\u578b\u548c\u751f\u4ea7\u7ea7\u4ee3\u7801\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u65e2\u80fd\u5feb\u901f\u751f\u6210\u4ee3\u7801\u53c8\u80fd\u4fdd\u8bc1\u8d28\u91cf\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u4ee3\u7801\u751f\u6210, \u5f3a\u5316\u5b66\u4e60, \u7a0b\u5e8f\u5206\u6790, \u5355\u5143\u6d4b\u8bd5"}}
{"id": "2505.22871", "pdf": "https://arxiv.org/pdf/2505.22871", "abs": "https://arxiv.org/abs/2505.22871", "authors": ["Yuval David", "Fabiana Fournier", "Lior Limonad", "Inna Skarbovsky"], "title": "The WHY in Business Processes: Unification of Causal Process Models", "categories": ["cs.AI"], "comment": "28 pages, 6 figures, BPM 2025 Forum", "summary": "Causal reasoning is essential for business process interventions and\nimprovement, requiring a clear understanding of causal relationships among\nactivity execution times in an event log. Recent work introduced a method for\ndiscovering causal process models but lacked the ability to capture alternating\ncausal conditions across multiple variants. This raises the challenges of\nhandling missing values and expressing the alternating conditions among log\nsplits when blending traces with varying activities.\n  We propose a novel method to unify multiple causal process variants into a\nconsistent model that preserves the correctness of the original causal models,\nwhile explicitly representing their causal-flow alternations. The method is\nformally defined, proved, evaluated on three open and two proprietary datasets,\nand released as an open-source implementation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u591a\u4e2a\u56e0\u679c\u6d41\u7a0b\u53d8\u4f53\u7edf\u4e00\u4e3a\u4e00\u4e2a\u4e00\u81f4\u7684\u6a21\u578b\uff0c\u4fdd\u7559\u539f\u59cb\u56e0\u679c\u6a21\u578b\u7684\u6b63\u786e\u6027\uff0c\u5e76\u660e\u786e\u8868\u793a\u5b83\u4eec\u7684\u56e0\u679c\u6d41\u4ea4\u66ff\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u591a\u4e2a\u53d8\u4f53\u4e2d\u7684\u4ea4\u66ff\u56e0\u679c\u6761\u4ef6\uff0c\u5bfc\u81f4\u5728\u5904\u7406\u7f3a\u5931\u503c\u548c\u8868\u8fbe\u4e0d\u540c\u6d3b\u52a8\u8f68\u8ff9\u95f4\u7684\u4ea4\u66ff\u6761\u4ef6\u65f6\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u591a\u4e2a\u56e0\u679c\u6d41\u7a0b\u53d8\u4f53\u7edf\u4e00\u4e3a\u4e00\u81f4\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6b63\u5f0f\u5b9a\u4e49\u5e76\u8bc1\u660e\u4e86\u5176\u6b63\u786e\u6027\u3002", "result": "\u65b9\u6cd5\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u4e13\u6709\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u4f5c\u4e3a\u5f00\u6e90\u5b9e\u73b0\u53d1\u5e03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7edf\u4e00\u56e0\u679c\u6d41\u7a0b\u53d8\u4f53\uff0c\u5e76\u660e\u786e\u8868\u793a\u5176\u4ea4\u66ff\u6761\u4ef6\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "keywords": "\u56e0\u679c\u63a8\u7406, \u4e1a\u52a1\u6d41\u7a0b, \u4e8b\u4ef6\u65e5\u5fd7, \u56e0\u679c\u6a21\u578b, \u5f00\u6e90\u5b9e\u73b0"}}
{"id": "2505.22689", "pdf": "https://arxiv.org/pdf/2505.22689", "abs": "https://arxiv.org/abs/2505.22689", "authors": ["Jialong Guo", "Xinghao Chen", "Yehui Tang", "Yunhe Wang"], "title": "SlimLLM: Accurate Structured Pruning for Large Language Models", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Large language models(LLMs) have garnered significant attention and\ndemonstrated impressive capabilities in a wide range of applications. However,\ndue to their enormous computational costs, the deployment and application of\nLLMs are often severely limited. To address this issue, structured pruning is\nan effective solution to compress the parameters of LLMs. Determining the\nimportance of each sub-module in LLMs and minimizing performance loss are\ncritical issues that need to be carefully addressed in structured pruning. In\nthis paper, we propose an effective and fast structured pruning method named\nSlimLLM for large language models. For channel and attention head pruning, we\nevaluate the importance based on the entire channel or head, rather than merely\naggregating the importance of individual elements within a sub-module. This\napproach enables a more holistic consideration of the interdependence among\nelements within the sub-module. In addition, we design a simple linear\nregression strategy for the output matrix to quickly recover performance. We\nalso propose layer-based importance ratio to determine the pruning ratio for\neach layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other\nmethods and achieves state-of-the-art performance.", "AI": {"tldr": "SlimLLM is a novel structured pruning method for large language models (LLMs) that evaluates importance holistically at the channel or head level and uses a linear regression strategy to recover performance, achieving state-of-the-art results on the LLaMA benchmark.", "motivation": "To address the high computational costs and deployment limitations of LLMs by developing an efficient structured pruning method that minimizes performance loss.", "method": "Proposes SlimLLM, which evaluates sub-module importance holistically (channel/head level) and uses linear regression for performance recovery, with layer-based importance ratios for pruning.", "result": "SlimLLM outperforms other methods on the LLaMA benchmark, achieving state-of-the-art performance.", "conclusion": "SlimLLM effectively compresses LLMs with minimal performance loss, offering a practical solution for deployment challenges.", "keywords": "large language models, structured pruning, SlimLLM, channel pruning, attention head pruning"}}
{"id": "2505.22752", "pdf": "https://arxiv.org/pdf/2505.22752", "abs": "https://arxiv.org/abs/2505.22752", "authors": ["Rafik Mankour", "Yassine Chafai", "Hamada Saleh", "Ghassen Ben Hassine", "Thibaud Barreau", "Peter Tankov"], "title": "Climate Finance Bench", "categories": ["cs.CL"], "comment": "Dataset is available at\n  https://github.com/Pladifes/climate_finance_bench", "summary": "Climate Finance Bench introduces an open benchmark that targets\nquestion-answering over corporate climate disclosures using Large Language\nModels. We curate 33 recent sustainability reports in English drawn from\ncompanies across all 11 GICS sectors and annotate 330 expert-validated\nquestion-answer pairs that span pure extraction, numerical reasoning, and\nlogical reasoning. Building on this dataset, we propose a comparison of RAG\n(retrieval-augmented generation) approaches. We show that the retriever's\nability to locate passages that actually contain the answer is the chief\nperformance bottleneck. We further argue for transparent carbon reporting in\nAI-for-climate applications, highlighting advantages of techniques such as\nWeight Quantization.", "AI": {"tldr": "Climate Finance Bench \u63d0\u51fa\u4e86\u4e00\u4e2a\u5f00\u653e\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u516c\u53f8\u6c14\u5019\u62ab\u9732\u7684\u95ee\u7b54\u80fd\u529b\uff0c\u5e76\u6bd4\u8f83\u4e86 RAG \u65b9\u6cd5\uff0c\u53d1\u73b0\u68c0\u7d22\u5668\u7684\u6027\u80fd\u662f\u5173\u952e\u74f6\u9888\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5bf9\u4f01\u4e1a\u6c14\u5019\u62ab\u9732\u4fe1\u606f\u7684\u95ee\u7b54\u80fd\u529b\uff0c\u5e76\u63a8\u52a8\u900f\u660e\u78b3\u62a5\u544a\u5728\u6c14\u5019 AI \u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u3002", "method": "\u6536\u96c6\u4e86 33 \u4efd\u82f1\u6587\u53ef\u6301\u7eed\u53d1\u5c55\u62a5\u544a\uff0c\u6807\u6ce8\u4e86 330 \u4e2a\u4e13\u5bb6\u9a8c\u8bc1\u7684\u95ee\u7b54\u5bf9\uff0c\u6bd4\u8f83\u4e86 RAG \u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u68c0\u7d22\u5668\u7684\u68c0\u7d22\u80fd\u529b\u662f\u6027\u80fd\u74f6\u9888\uff0c\u5e76\u63d0\u51fa\u91cf\u5316\u6743\u91cd\u7b49\u6280\u672f\u53ef\u4f18\u5316\u78b3\u62a5\u544a\u900f\u660e\u5ea6\u3002", "conclusion": "RAG \u65b9\u6cd5\u5728\u6c14\u5019\u62ab\u9732\u95ee\u7b54\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u4f18\u5316\u68c0\u7d22\u5668\u6027\u80fd\uff0c\u5e76\u91cd\u89c6\u78b3\u62a5\u544a\u7684\u900f\u660e\u5ea6\u3002", "keywords": "Climate Finance, QA Benchmark, RAG, Carbon Reporting, Large Language Models"}}
{"id": "2505.22928", "pdf": "https://arxiv.org/pdf/2505.22928", "abs": "https://arxiv.org/abs/2505.22928", "authors": ["Massimiliano Pronesti", "Michela Lorandi", "Paul Flanagan", "Oisin Redmon", "Anya Belz", "Yufang Hou"], "title": "Enhancing Study-Level Inference from Clinical Trial Papers via RL-based Numeric Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Systematic reviews in medicine play a critical role in evidence-based\ndecision-making by aggregating findings from multiple studies. A central\nbottleneck in automating this process is extracting numeric evidence and\ndetermining study-level conclusions for specific outcomes and comparisons.\nPrior work has framed this problem as a textual inference task by retrieving\nrelevant content fragments and inferring conclusions from them. However, such\napproaches often rely on shallow textual cues and fail to capture the\nunderlying numeric reasoning behind expert assessments.\n  In this work, we conceptualise the problem as one of quantitative reasoning.\nRather than inferring conclusions from surface text, we extract structured\nnumerical evidence (e.g., event counts or standard deviations) and apply domain\nknowledge informed logic to derive outcome-specific conclusions. We develop a\nnumeric reasoning system composed of a numeric data extraction model and an\neffect estimate component, enabling more accurate and interpretable inference\naligned with the domain expert principles. We train the numeric data extraction\nmodel using different strategies, including supervised fine-tuning (SFT) and\nreinforcement learning (RL) with a new value reward model.\n  When evaluated on the CochraneForest benchmark, our best-performing approach\n-- using RL to train a small-scale number extraction model -- yields up to a\n21% absolute improvement in F1 score over retrieval-based systems and\noutperforms general-purpose LLMs of over 400B parameters by up to 9%. Our\nresults demonstrate the promise of reasoning-driven approaches for automating\nsystematic evidence synthesis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9a\u91cf\u63a8\u7406\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u533b\u5b66\u7cfb\u7edf\u7efc\u8ff0\u4e2d\u7684\u6570\u503c\u8bc1\u636e\u63d0\u53d6\u4e0e\u7ed3\u8bba\u63a8\u65ad\uff0c\u76f8\u6bd4\u4f20\u7edf\u6587\u672c\u63a8\u7406\u65b9\u6cd5\uff0c\u5176\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u533b\u5b66\u7cfb\u7edf\u7efc\u8ff0\u7684\u81ea\u52a8\u5316\u8fc7\u7a0b\u4e2d\uff0c\u6570\u503c\u8bc1\u636e\u63d0\u53d6\u548c\u7ed3\u8bba\u63a8\u65ad\u662f\u5173\u952e\u74f6\u9888\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6d45\u5c42\u6587\u672c\u7ebf\u7d22\uff0c\u672a\u80fd\u6355\u6349\u5230\u4e13\u5bb6\u8bc4\u4f30\u80cc\u540e\u7684\u6570\u503c\u63a8\u7406\u903b\u8f91\u3002", "method": "\u8bba\u6587\u5c06\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5b9a\u91cf\u63a8\u7406\u4efb\u52a1\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u6570\u503c\u6570\u636e\u63d0\u53d6\u6a21\u578b\u548c\u6548\u5e94\u4f30\u8ba1\u7ec4\u4ef6\u7684\u7cfb\u7edf\uff0c\u5e76\u91c7\u7528\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728CochraneForest\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6700\u4f73\u65b9\u6cd5\uff08\u4f7f\u7528RL\u8bad\u7ec3\u7684\u5c0f\u89c4\u6a21\u6570\u503c\u63d0\u53d6\u6a21\u578b\uff09F1\u5206\u6570\u6bd4\u68c0\u7d22\u7cfb\u7edf\u63d0\u534721%\uff0c\u6bd4400B\u53c2\u6570\u7684\u5927\u6a21\u578b\u63d0\u53479%\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u57fa\u4e8e\u63a8\u7406\u7684\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u7cfb\u7edf\u8bc1\u636e\u5408\u6210\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "\u7cfb\u7edf\u7efc\u8ff0, \u5b9a\u91cf\u63a8\u7406, \u6570\u503c\u63d0\u53d6, \u5f3a\u5316\u5b66\u4e60, CochraneForest"}}
{"id": "2505.22694", "pdf": "https://arxiv.org/pdf/2505.22694", "abs": "https://arxiv.org/abs/2505.22694", "authors": ["Dacao Zhang", "Kun Zhang", "Shimao Chu", "Le Wu", "Xin Li", "Si Wei"], "title": "MoRE: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning", "categories": ["cs.LG"], "comment": "This paper has been accepted to ACL 2025 Findings", "summary": "With the rapid development of Large Language Models (LLMs),\nParameter-Efficient Fine-Tuning (PEFT) methods have gained significant\nattention, which aims to achieve efficient fine-tuning of LLMs with fewer\nparameters. As a representative PEFT method, Low-Rank Adaptation (LoRA)\nintroduces low-rank matrices to approximate the incremental tuning parameters\nand achieves impressive performance over multiple scenarios. After that, plenty\nof improvements have been proposed for further improvement. However, these\nmethods either focus on single-task scenarios or separately train multiple LoRA\nmodules for multi-task scenarios, limiting the efficiency and effectiveness of\nLoRA in multi-task scenarios. To better adapt to multi-task fine-tuning, in\nthis paper, we propose a novel Mixture of Low-Rank Experts (MoRE) for\nmulti-task PEFT. Specifically, instead of using an individual LoRA for each\ntask, we align different ranks of LoRA module with different tasks, which we\nnamed low-rank experts. Moreover, we design a novel adaptive rank selector to\nselect the appropriate expert for each task. By jointly training low-rank\nexperts, MoRE can enhance the adaptability and efficiency of LoRA in multi-task\nscenarios. Finally, we conduct extensive experiments over multiple multi-task\nbenchmarks along with different LLMs to verify model performance. Experimental\nresults demonstrate that compared to traditional LoRA and its variants, MoRE\nsignificantly improves the performance of LLMs in multi-task scenarios and\nincurs no additional inference cost. We also release the model and code to\nfacilitate the community.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoRE\u7684\u591a\u4efb\u52a1\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u4f4e\u79e9\u4e13\u5bb6\u548c\u81ea\u9002\u5e94\u79e9\u9009\u62e9\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86LoRA\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684PEFT\u65b9\u6cd5\uff08\u5982LoRA\uff09\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u8981\u4e48\u4ec5\u9002\u7528\u4e8e\u5355\u4efb\u52a1\uff0c\u8981\u4e48\u9700\u5355\u72ec\u8bad\u7ec3\u591a\u4e2a\u6a21\u5757\uff0c\u9650\u5236\u4e86LoRA\u7684\u6548\u80fd\u3002\u672c\u6587\u65e8\u5728\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528Mixture of Low-Rank Experts (MoRE)\u65b9\u6cd5\uff0c\u5c06\u4e0d\u540c\u79e9\u7684LoRA\u6a21\u5757\uff08\u4f4e\u79e9\u4e13\u5bb6\uff09\u4e0e\u4efb\u52a1\u5bf9\u9f50\uff0c\u5e76\u8bbe\u8ba1\u81ea\u9002\u5e94\u79e9\u9009\u62e9\u5668\uff0c\u8054\u5408\u8bad\u7ec3\u4f4e\u79e9\u4e13\u5bb6\u4ee5\u5b9e\u73b0\u591a\u4efb\u52a1\u9002\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMoRE\u5728\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edfLoRA\u53ca\u5176\u53d8\u4f53\uff0c\u4e14\u4e0d\u589e\u52a0\u989d\u5916\u63a8\u7406\u6210\u672c\u3002", "conclusion": "MoRE\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u4e86LoRA\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u7684\u9002\u5e94\u6027\u548c\u6548\u7387\uff0c\u4e3aLLM\u7684\u591a\u4efb\u52a1\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "Large Language Models, Parameter-Efficient Fine-Tuning, LoRA, Multi-Task Learning, Mixture of Experts"}}
{"id": "2505.22757", "pdf": "https://arxiv.org/pdf/2505.22757", "abs": "https://arxiv.org/abs/2505.22757", "authors": ["Ansar Aynetdinov", "Alan Akbik"], "title": "Pre-Training Curriculum for Multi-Token Prediction in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Multi-token prediction (MTP) is a recently proposed pre-training objective\nfor language models. Rather than predicting only the next token (NTP), MTP\npredicts the next $k$ tokens at each prediction step, using multiple prediction\nheads. MTP has shown promise in improving downstream performance, inference\nspeed, and training efficiency, particularly for large models. However, prior\nwork has shown that smaller language models (SLMs) struggle with the MTP\nobjective. To address this, we propose a curriculum learning strategy for MTP\ntraining, exploring two variants: a forward curriculum, which gradually\nincreases the complexity of the pre-training objective from NTP to MTP, and a\nreverse curriculum, which does the opposite. Our experiments show that the\nforward curriculum enables SLMs to better leverage the MTP objective during\npre-training, improving downstream NTP performance and generative output\nquality, while retaining the benefits of self-speculative decoding. The reverse\ncurriculum achieves stronger NTP performance and output quality, but fails to\nprovide any self-speculative decoding benefits.", "AI": {"tldr": "\u591a\u4ee4\u724c\u9884\u6d4b\uff08MTP\uff09\u662f\u4e00\u79cd\u65b0\u5174\u7684\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u80fd\u540c\u65f6\u9884\u6d4b\u591a\u4e2a\u4ee4\u724c\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u7136\u800c\uff0c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u96be\u4ee5\u9002\u5e94MTP\u3002\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff08\u6b63\u5411\u548c\u53cd\u5411\uff09\u5e2e\u52a9SLM\u66f4\u597d\u5229\u7528MTP\uff0c\u5b9e\u9a8c\u663e\u793a\u6b63\u5411\u7b56\u7565\u5728\u63d0\u5347NTP\u6027\u80fd\u7684\u540c\u65f6\u4fdd\u7559\u4e86\u81ea\u63a8\u6d4b\u89e3\u7801\u4f18\u52bf\uff0c\u53cd\u5411\u7b56\u7565\u867d\u63d0\u5347\u6027\u80fd\u4f46\u65e0\u6b64\u4f18\u52bf\u3002", "motivation": "\u89e3\u51b3\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728MTP\u8bad\u7ec3\u4e2d\u7684\u9002\u5e94\u6027\u95ee\u9898\uff0c\u63a2\u7d22\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u4ee5\u4f18\u5316\u5176\u9884\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u8bfe\u7a0b\u5b66\u4e60\u53d8\u4f53\uff1a\u6b63\u5411\uff08\u4eceNTP\u9010\u6b65\u8fc7\u6e21\u5230MTP\uff09\u548c\u53cd\u5411\uff08\u76f8\u53cd\u987a\u5e8f\uff09\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9SLM\u7684\u5f71\u54cd\u3002", "result": "\u6b63\u5411\u7b56\u7565\u4f7fSLM\u66f4\u9ad8\u6548\u5229\u7528MTP\uff0c\u63d0\u5347\u4e0b\u6e38NTP\u6027\u80fd\u548c\u751f\u6210\u8d28\u91cf\uff0c\u4fdd\u7559\u81ea\u63a8\u6d4b\u89e3\u7801\u4f18\u52bf\uff1b\u53cd\u5411\u7b56\u7565\u867d\u63d0\u5347\u6027\u80fd\u4f46\u65e0\u89e3\u7801\u4f18\u52bf\u3002", "conclusion": "\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff08\u5c24\u5176\u662f\u6b63\u5411\uff09\u80fd\u6709\u6548\u5e2e\u52a9SLM\u9002\u5e94MTP\uff0c\u5e73\u8861\u6027\u80fd\u4e0e\u89e3\u7801\u6548\u7387\u3002", "keywords": "\u591a\u4ee4\u724c\u9884\u6d4b, \u5c0f\u578b\u8bed\u8a00\u6a21\u578b, \u8bfe\u7a0b\u5b66\u4e60, \u81ea\u63a8\u6d4b\u89e3\u7801"}}
{"id": "2505.22948", "pdf": "https://arxiv.org/pdf/2505.22948", "abs": "https://arxiv.org/abs/2505.22948", "authors": ["Michael Sun", "Weize Yuan", "Gang Liu", "Wojciech Matusik", "Jie Chen"], "title": "Foundation Molecular Grammar: Multi-Modal Foundation Models Induce Interpretable Molecular Graph Languages", "categories": ["cs.AI"], "comment": "ICML 2025", "summary": "Recent data-efficient molecular generation approaches exploit graph grammars\nto introduce interpretability into the generative models. However, grammar\nlearning therein relies on expert annotation or unreliable heuristics for\nalgorithmic inference. We propose Foundation Molecular Grammar (FMG), which\nleverages multi-modal foundation models (MMFMs) to induce an interpretable\nmolecular language. By exploiting the chemical knowledge of an MMFM, FMG\nrenders molecules as images, describes them as text, and aligns information\nacross modalities using prompt learning. FMG can be used as a drop-in\nreplacement for the prior grammar learning approaches in molecular generation\nand property prediction. We show that FMG not only excels in synthesizability,\ndiversity, and data efficiency but also offers built-in chemical\ninterpretability for automated molecular discovery workflows. Code is available\nat https://github.com/shiningsunnyday/induction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFMG\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff08MMFMs\uff09\u5b66\u4e60\u53ef\u89e3\u91ca\u7684\u5206\u5b50\u8bed\u8a00\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u57fa\u4e8e\u4e13\u5bb6\u6ce8\u91ca\u6216\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u5206\u5b50\u751f\u6210\u548c\u6027\u8d28\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5b50\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u6ce8\u91ca\u6216\u4e0d\u53ef\u9760\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u9650\u5236\u4e86\u751f\u6210\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\u3002", "method": "FMG\u901a\u8fc7\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff08MMFMs\uff09\u5c06\u5206\u5b50\u8868\u793a\u4e3a\u56fe\u50cf\u548c\u6587\u672c\uff0c\u5e76\u5229\u7528\u63d0\u793a\u5b66\u4e60\u8de8\u6a21\u6001\u5bf9\u9f50\u4fe1\u606f\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u5206\u5b50\u8bed\u8a00\u3002", "result": "FMG\u5728\u53ef\u5408\u6210\u6027\u3001\u591a\u6837\u6027\u548c\u6570\u636e\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e3a\u5206\u5b50\u53d1\u73b0\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u5185\u7f6e\u7684\u5316\u5b66\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "FMG\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u5206\u5b50\u751f\u6210\u548c\u6027\u8d28\u9884\u6d4b\u65b9\u6cd5\u3002", "keywords": "\u5206\u5b50\u751f\u6210\u3001\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u3001\u53ef\u89e3\u91ca\u6027\u3001\u63d0\u793a\u5b66\u4e60"}}
{"id": "2505.22695", "pdf": "https://arxiv.org/pdf/2505.22695", "abs": "https://arxiv.org/abs/2505.22695", "authors": ["Tengfei Lyu", "Siyuan Feng", "Hao Liu", "Hai Yang"], "title": "LLM-ODDR: A Large Language Model Framework for Joint Order Dispatching and Driver Repositioning", "categories": ["cs.LG"], "comment": null, "summary": "Ride-hailing platforms face significant challenges in optimizing order\ndispatching and driver repositioning operations in dynamic urban environments.\nTraditional approaches based on combinatorial optimization, rule-based\nheuristics, and reinforcement learning often overlook driver income fairness,\ninterpretability, and adaptability to real-world dynamics. To address these\ngaps, we propose LLM-ODDR, a novel framework leveraging Large Language Models\n(LLMs) for joint Order Dispatching and Driver Repositioning (ODDR) in\nride-hailing services. LLM-ODDR framework comprises three key components: (1)\nMulti-objective-guided Order Value Refinement, which evaluates orders by\nconsidering multiple objectives to determine their overall value; (2)\nFairness-aware Order Dispatching, which balances platform revenue with driver\nincome fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning,\nwhich optimizes idle vehicle placement based on historical patterns and\nprojected supply. We also develop JointDR-GPT, a fine-tuned model optimized for\nODDR tasks with domain knowledge. Extensive experiments on real-world datasets\nfrom Manhattan taxi operations demonstrate that our framework significantly\noutperforms traditional methods in terms of effectiveness, adaptability to\nanomalous conditions, and decision interpretability. To our knowledge, this is\nthe first exploration of LLMs as decision-making agents in ride-hailing ODDR\ntasks, establishing foundational insights for integrating advanced language\nmodels within intelligent transportation systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLLM-ODDR\u7684\u65b0\u578b\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u7f51\u7ea6\u8f66\u5e73\u53f0\u7684\u8ba2\u5355\u8c03\u5ea6\u548c\u53f8\u673a\u91cd\u5b9a\u4f4d\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5ffd\u7565\u7684\u6536\u5165\u516c\u5e73\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u52a8\u6001\u9002\u5e94\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u52a8\u6001\u57ce\u5e02\u73af\u5883\u4e2d\u96be\u4ee5\u517c\u987e\u53f8\u673a\u6536\u5165\u516c\u5e73\u6027\u548c\u7cfb\u7edf\u7075\u6d3b\u6027\uff0c\u56e0\u6b64\u4f5c\u8005\u63d0\u51fa\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "LLM-ODDR\u6846\u67b6\u5305\u62ec\u591a\u76ee\u6807\u8ba2\u5355\u4ef7\u503c\u8bc4\u4f30\u3001\u516c\u5e73\u611f\u77e5\u7684\u8ba2\u5355\u8c03\u5ea6\u548c\u65f6\u7a7a\u9700\u6c42\u611f\u77e5\u7684\u53f8\u673a\u91cd\u5b9a\u4f4d\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff0c\u5e76\u5f00\u53d1\u4e86\u4e13\u95e8\u4f18\u5316\u7684JointDR-GPT\u6a21\u578b\u3002", "result": "\u5728\u66fc\u54c8\u987f\u51fa\u79df\u8f66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6548\u679c\u3001\u5f02\u5e38\u9002\u5e94\u6027\u548c\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u662f\u5927\u8bed\u8a00\u6a21\u578b\u9996\u6b21\u5e94\u7528\u4e8e\u7f51\u7ea6\u8f66\u8c03\u5ea6\u4efb\u52a1\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u9ad8\u7ea7\u8bed\u8a00\u6a21\u578b\u7684\u6574\u5408\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "keywords": "\u7f51\u7ea6\u8f66, \u8ba2\u5355\u8c03\u5ea6, \u53f8\u673a\u91cd\u5b9a\u4f4d, \u5927\u8bed\u8a00\u6a21\u578b, \u516c\u5e73\u6027"}}
{"id": "2505.22759", "pdf": "https://arxiv.org/pdf/2505.22759", "abs": "https://arxiv.org/abs/2505.22759", "authors": ["Sara Papi", "Marco Gaido", "Luisa Bentivogli", "Alessio Brutti", "Mauro Cettolo", "Roberto Gretter", "Marco Matassoni", "Mohamed Nabih", "Matteo Negri"], "title": "FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian", "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": null, "summary": "The development of speech foundation models (SFMs) like Whisper and\nSeamlessM4T has significantly advanced the field of speech processing. However,\ntheir closed nature--with inaccessible training data and code--poses major\nreproducibility and fair evaluation challenges. While other domains have made\nsubstantial progress toward open science by developing fully transparent models\ntrained on open-source (OS) code and data, similar efforts in speech remain\nlimited. To fill this gap, we introduce FAMA, the first family of open science\nSFMs for English and Italian, trained on 150k+ hours of OS speech data.\nMoreover, we present a new dataset containing 16k hours of cleaned and\npseudo-labeled speech for both languages. Results show that FAMA achieves\ncompetitive performance compared to existing SFMs while being up to 8 times\nfaster. All artifacts, including code, datasets, and models, are released under\nOS-compliant licenses, promoting openness in speech technology research.", "AI": {"tldr": "FAMA\u7cfb\u5217\u6a21\u578b\u662f\u9996\u4e2a\u57fa\u4e8e\u5f00\u6e90\u6570\u636e\u548c\u4ee3\u7801\u7684\u8bed\u97f3\u57fa\u7840\u6a21\u578b\uff0c\u586b\u8865\u4e86\u8bed\u97f3\u5904\u7406\u9886\u57df\u5f00\u653e\u79d1\u5b66\u7684\u7a7a\u767d\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u6a21\u578b\u76f8\u5f53\u4e14\u66f4\u9ad8\u6548\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u57fa\u7840\u6a21\u578b\uff08\u5982Whisper\u548cSeamlessM4T\uff09\u7684\u5c01\u95ed\u6027\u5bfc\u81f4\u53ef\u91cd\u73b0\u6027\u548c\u516c\u5e73\u8bc4\u4f30\u7684\u6311\u6218\uff0c\u800c\u5176\u4ed6\u9886\u57df\u5df2\u901a\u8fc7\u5f00\u6e90\u6a21\u578b\u53d6\u5f97\u8fdb\u5c55\u3002", "method": "\u5f00\u53d1FAMA\u7cfb\u5217\u6a21\u578b\uff0c\u4f7f\u7528\u8d85\u8fc715\u4e07\u5c0f\u65f6\u7684\u5f00\u6e90\u8bed\u97f3\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u5305\u542b1.6\u4e07\u5c0f\u65f6\u6e05\u7406\u548c\u4f2a\u6807\u6ce8\u8bed\u97f3\u7684\u65b0\u6570\u636e\u96c6\u3002", "result": "FAMA\u5728\u6027\u80fd\u4e0a\u4e0e\u73b0\u6709\u6a21\u578b\u7ade\u4e89\uff0c\u4e14\u901f\u5ea6\u5feb\u8fbe8\u500d\u3002\u6240\u6709\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u5747\u4ee5\u5f00\u6e90\u8bb8\u53ef\u53d1\u5e03\u3002", "conclusion": "FAMA\u63a8\u52a8\u4e86\u8bed\u97f3\u6280\u672f\u7814\u7a76\u7684\u5f00\u653e\u6027\u548c\u900f\u660e\u5ea6\uff0c\u4e3a\u8bed\u97f3\u5904\u7406\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u91cd\u73b0\u7684\u57fa\u51c6\u3002", "keywords": "\u8bed\u97f3\u57fa\u7840\u6a21\u578b, \u5f00\u653e\u79d1\u5b66, \u5f00\u6e90\u6570\u636e, FAMA, \u8bed\u97f3\u5904\u7406"}}
{"id": "2505.22954", "pdf": "https://arxiv.org/pdf/2505.22954", "abs": "https://arxiv.org/abs/2505.22954", "authors": ["Jenny Zhang", "Shengran Hu", "Cong Lu", "Robert Lange", "Jeff Clune"], "title": "Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents", "categories": ["cs.AI"], "comment": "Code at https://github.com/jennyzzt/dgm", "summary": "Today's AI systems have human-designed, fixed architectures and cannot\nautonomously and continuously improve themselves. The advance of AI could\nitself be automated. If done safely, that would accelerate AI development and\nallow us to reap its benefits much sooner. Meta-learning can automate the\ndiscovery of novel algorithms, but is limited by first-order improvements and\nthe human design of a suitable search space. The G\\\"odel machine proposed a\ntheoretical alternative: a self-improving AI that repeatedly modifies itself in\na provably beneficial manner. Unfortunately, proving that most changes are net\nbeneficial is impossible in practice. We introduce the Darwin G\\\"odel Machine\n(DGM), a self-improving system that iteratively modifies its own code (thereby\nalso improving its ability to modify its own codebase) and empirically\nvalidates each change using coding benchmarks. Inspired by Darwinian evolution\nand open-endedness research, the DGM maintains an archive of generated coding\nagents. It grows the archive by sampling an agent from it and using a\nfoundation model to create a new, interesting, version of the sampled agent.\nThis open-ended exploration forms a growing tree of diverse, high-quality\nagents and allows the parallel exploration of many different paths through the\nsearch space. Empirically, the DGM automatically improves its coding\ncapabilities (e.g., better code editing tools, long-context window management,\npeer-review mechanisms), increasing performance on SWE-bench from 20.0% to\n50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly\noutperforms baselines without self-improvement or open-ended exploration. All\nexperiments were done with safety precautions (e.g., sandboxing, human\noversight). The DGM is a significant step toward self-improving AI, capable of\ngathering its own stepping stones along paths that unfold into endless\ninnovation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fbe\u5c14\u6587\u8fdb\u5316\u7684\u81ea\u6539\u8fdbAI\u7cfb\u7edf\uff08DGM\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u4fee\u6539\u81ea\u8eab\u4ee3\u7801\u5e76\u5728\u7f16\u7801\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u6539\u8fdb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u7801\u80fd\u529b\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u67b6\u6784\u56fa\u5b9a\u4e14\u65e0\u6cd5\u81ea\u4e3b\u6539\u8fdb\uff0c\u81ea\u52a8\u5316AI\u8fdb\u6b65\u53ef\u80fd\u52a0\u901f\u53d1\u5c55\u5e76\u5e26\u6765\u66f4\u5927\u6536\u76ca\u3002\u4f46\u73b0\u6709\u4eba\u5de5\u8bbe\u8ba1\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u641c\u7d22\u7a7a\u95f4\u548c\u4e00\u7ea7\u6539\u8fdb\uff0c\u800cG\u00f6del\u673a\u5668\u7684\u7406\u8bba\u81ea\u6539\u8fdb\u5728\u5b9e\u9645\u4e2d\u96be\u4ee5\u8bc1\u660e\u5176\u591a\u6570\u6539\u8fdb\u662f\u6709\u76ca\u7684\u3002", "method": "\u63d0\u51faDarwin G\u00f6del Machine (DGM)\uff0c\u901a\u8fc7\u8fbe\u5c14\u6587\u8fdb\u5316\u548c\u5f00\u653e\u5f0f\u63a2\u7d22\uff0c\u7ef4\u62a4\u4e00\u4e2a\u7f16\u7801\u4ee3\u7406\u6863\u6848\u5e93\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u751f\u6210\u65b0\u7684\u3001\u6709\u8da3\u7684\u4ee3\u7406\u7248\u672c\uff0c\u5e76\u884c\u63a2\u7d22\u591a\u6837\u5316\u7684\u6539\u8fdb\u8def\u5f84\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u6539\u8fdb\u3002", "result": "DGM\u5728SWE-bench\u4e0a\u4ece20.0%\u63d0\u5347\u523050.0%\uff0c\u5728Polyglot\u4e0a\u4ece14.2%\u63d0\u5347\u523030.7%\uff0c\u663e\u8457\u4f18\u4e8e\u7f3a\u4e4f\u81ea\u6539\u8fdb\u6216\u5f00\u653e\u5f0f\u63a2\u7d22\u7684\u57fa\u7ebf\u7cfb\u7edf\u3002", "conclusion": "DGM\u662f\u8fc8\u5411\u81ea\u6539\u8fdbAI\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u80fd\u591f\u901a\u8fc7\u5f00\u653e\u5f0f\u63a2\u7d22\u6301\u7eed\u521b\u65b0\uff0c\u663e\u8457\u63d0\u5347\u7f16\u7801\u80fd\u529b\uff0c\u540c\u65f6\u5728\u5b9e\u9a8c\u8fc7\u7a0b\u4e2d\u91c7\u53d6\u4e86\u5b89\u5168\u63aa\u65bd\u3002", "keywords": "\u81ea\u6539\u8fdbAI, Darwin G\u00f6del Machine, \u5143\u5b66\u4e60, \u5f00\u653e\u5f0f\u63a2\u7d22, \u7f16\u7801\u57fa\u51c6"}}
{"id": "2505.22696", "pdf": "https://arxiv.org/pdf/2505.22696", "abs": "https://arxiv.org/abs/2505.22696", "authors": ["Eleni Nisioti", "Joachim Winther Pedersen", "Erwan Plantec", "Milton L. Montero", "Sebastian Risi"], "title": "When Does Neuroevolution Outcompete Reinforcement Learning in Transfer Learning Tasks?", "categories": ["cs.LG"], "comment": null, "summary": "The ability to continuously and efficiently transfer skills across tasks is a\nhallmark of biological intelligence and a long-standing goal in artificial\nsystems. Reinforcement learning (RL), a dominant paradigm for learning in\nhigh-dimensional control tasks, is known to suffer from brittleness to task\nvariations and catastrophic forgetting. Neuroevolution (NE) has recently gained\nattention for its robustness, scalability, and capacity to escape local optima.\nIn this paper, we investigate an understudied dimension of NE: its transfer\nlearning capabilities. To this end, we introduce two benchmarks: a) in stepping\ngates, neural networks are tasked with emulating logic circuits, with designs\nthat emphasize modular repetition and variation b) ecorobot extends the Brax\nphysics engine with objects such as walls and obstacles and the ability to\neasily switch between different robotic morphologies. Crucial in both\nbenchmarks is the presence of a curriculum that enables evaluating skill\ntransfer across tasks of increasing complexity. Our empirical analysis shows\nthat NE methods vary in their transfer abilities and frequently outperform RL\nbaselines. Our findings support the potential of NE as a foundation for\nbuilding more adaptable agents and highlight future challenges for scaling NE\nto complex, real-world problems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u8fdb\u5316\uff08NE\uff09\u5728\u4efb\u52a1\u95f4\u6280\u80fd\u8fc1\u79fb\u4e2d\u7684\u6f5c\u529b\uff0c\u5f15\u5165\u4e86\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0NE\u65b9\u6cd5\u5728\u8fc1\u79fb\u80fd\u529b\u4e0a\u4f18\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u652f\u6301NE\u4f5c\u4e3a\u6784\u5efa\u66f4\u9002\u5e94\u6027\u4ee3\u7406\u7684\u57fa\u7840\u3002", "motivation": "\u76ee\u6807\u662f\u63a2\u7d22\u795e\u7ecf\u8fdb\u5316\uff08NE\uff09\u5728\u4efb\u52a1\u95f4\u6280\u80fd\u8fc1\u79fb\u4e2d\u7684\u80fd\u529b\uff0c\u4ee5\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u4efb\u52a1\u53d8\u5316\u548c\u707e\u96be\u6027\u9057\u5fd8\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165\u4e86\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff1aa) stepping gates\u7528\u4e8e\u6a21\u62df\u903b\u8f91\u7535\u8def\uff0c\u5f3a\u8c03\u6a21\u5757\u5316\u91cd\u590d\u548c\u53d8\u5316\uff1bb) ecorobot\u6269\u5c55\u4e86Brax\u7269\u7406\u5f15\u64ce\uff0c\u5305\u542b\u5899\u58c1\u548c\u969c\u788d\u7269\uff0c\u5e76\u652f\u6301\u4e0d\u540c\u673a\u5668\u4eba\u5f62\u6001\u7684\u5207\u6362\u3002\u5747\u91c7\u7528\u4e86\u9010\u6b65\u590d\u6742\u7684\u4efb\u52a1\u8bfe\u7a0b\u6765\u8bc4\u4f30\u6280\u80fd\u8fc1\u79fb\u3002", "result": "\u5b9e\u9a8c\u5206\u6790\u8868\u660e\uff0cNE\u65b9\u6cd5\u5728\u8fc1\u79fb\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u7ecf\u5e38\u4f18\u4e8eRL\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301NE\u4f5c\u4e3a\u6784\u5efa\u66f4\u9002\u5e94\u6027\u4ee3\u7406\u7684\u6f5c\u529b\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u5728\u590d\u6742\u73b0\u5b9e\u95ee\u9898\u4e2d\u6269\u5c55NE\u7684\u6311\u6218\u3002", "keywords": "\u795e\u7ecf\u8fdb\u5316, \u5f3a\u5316\u5b66\u4e60, \u6280\u80fd\u8fc1\u79fb, \u57fa\u51c6\u6d4b\u8bd5, \u9002\u5e94\u6027\u4ee3\u7406"}}
{"id": "2505.22765", "pdf": "https://arxiv.org/pdf/2505.22765", "abs": "https://arxiv.org/abs/2505.22765", "authors": ["Iddo Yosha", "Gallil Maimon", "Yossi Adi"], "title": "StressTest: Can YOUR Speech LM Handle the Stress?", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Sentence stress refers to emphasis, placed on specific words within a spoken\nutterance to highlight or contrast an idea, or to introduce new information. It\nis often used to imply an underlying intention that is not explicitly stated.\nRecent advances in speech-aware language models (SLMs) have enabled direct\nprocessing of audio, allowing models to bypass transcription and access the\nfull richness of the speech signal and perform audio reasoning tasks such as\nspoken question answering. Despite the crucial role of sentence stress in\nshaping meaning and speaker intent, it remains largely overlooked in evaluation\nand development of such models. In this work, we address this gap by\nintroducing StressTest, a benchmark specifically designed to evaluate a model's\nability to distinguish between interpretations of spoken sentences based on the\nstress pattern. We assess the performance of several leading SLMs and find\nthat, despite their overall capabilities, they perform poorly on such tasks. To\novercome this limitation, we propose a novel synthetic data generation\npipeline, and create Stress17k, a training set that simulates change of meaning\nimplied by stress variation. Then, we empirically show that optimizing models\nwith this synthetic dataset aligns well with real-world recordings and enables\neffective finetuning of SLMs. Results suggest, that our finetuned model,\nStresSLM, significantly outperforms existing models on both sentence stress\nreasoning and detection tasks. Code, models, data, and audio samples -\npages.cs.huji.ac.il/adiyoss-lab/stresstest.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86StressTest\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u97f3\u611f\u77e5\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u53e5\u5b50\u91cd\u97f3\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6Stress17k\u4f18\u5316\u6a21\u578b\uff0c\u6700\u7ec8\u6a21\u578bStresSLM\u5728\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u53e5\u5b50\u91cd\u97f3\u5728\u8bed\u97f3\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684SLMs\u5bf9\u5176\u5904\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u4e13\u95e8\u7684\u8bc4\u4f30\u548c\u4f18\u5316\u65b9\u6cd5\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f15\u5165StressTest\u57fa\u51c6\uff0c\u63d0\u51fa\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u7a0b\u521b\u5efaStress17k\u6570\u636e\u96c6\uff0c\u5e76\u7528\u4e8e\u5fae\u8c03SLMs\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578bStresSLM\u5728\u53e5\u5b50\u91cd\u97f3\u63a8\u7406\u548c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u4f18\u5316\u53ef\u4ee5\u63d0\u5347SLMs\u5bf9\u53e5\u5b50\u91cd\u97f3\u7684\u5904\u7406\u80fd\u529b\u3002", "keywords": "\u53e5\u5b50\u91cd\u97f3,\u8bed\u97f3\u611f\u77e5\u8bed\u8a00\u6a21\u578b,\u57fa\u51c6\u6d4b\u8bd5,\u5408\u6210\u6570\u636e,\u5fae\u8c03"}}
{"id": "2505.22960", "pdf": "https://arxiv.org/pdf/2505.22960", "abs": "https://arxiv.org/abs/2505.22960", "authors": ["Yongjin Yang", "Euiin Yi", "Jongwoo Ko", "Kimin Lee", "Zhijing Jin", "Se-Young Yun"], "title": "Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness", "categories": ["cs.AI", "cs.LG"], "comment": "Preprint, under review", "summary": "The remarkable growth in large language model (LLM) capabilities has spurred\nexploration into multi-agent systems, with debate frameworks emerging as a\npromising avenue for enhanced problem-solving. These multi-agent debate (MAD)\napproaches, where agents collaboratively present, critique, and refine\narguments, potentially offer improved reasoning, robustness, and diverse\nperspectives over monolithic models. Despite prior studies leveraging MAD, a\nsystematic understanding of its effectiveness compared to self-agent methods,\nparticularly under varying conditions, remains elusive. This paper seeks to\nfill this gap by conceptualizing MAD as a test-time computational scaling\ntechnique, distinguished by collaborative refinement and diverse exploration\ncapabilities. We conduct a comprehensive empirical investigation comparing MAD\nwith strong self-agent test-time scaling baselines on mathematical reasoning\nand safety-related tasks. Our study systematically examines the influence of\ntask difficulty, model scale, and agent diversity on MAD's performance. Key\nfindings reveal that, for mathematical reasoning, MAD offers limited advantages\nover self-agent scaling but becomes more effective with increased problem\ndifficulty and decreased model capability, while agent diversity shows little\nbenefit. Conversely, for safety tasks, MAD's collaborative refinement can\nincrease vulnerability, but incorporating diverse agent configurations\nfacilitates a gradual reduction in attack success through the collaborative\nrefinement process. We believe our findings provide critical guidance for the\nfuture development of more effective and strategically deployed MAD systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff08MAD\uff09\u6846\u67b6\u5728\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u4e2d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u5176\u5728\u6570\u5b66\u63a8\u7406\u548c\u5b89\u5168\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u540c\uff0c\u5e76\u63ed\u793a\u4e86\u4efb\u52a1\u96be\u5ea6\u3001\u6a21\u578b\u89c4\u6a21\u548c\u667a\u80fd\u4f53\u591a\u6837\u6027\u5bf9MAD\u6548\u679c\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u663e\u8457\u63d0\u5347\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6210\u4e3a\u589e\u5f3a\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u7684\u65b0\u65b9\u5411\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff08MAD\uff09\u76f8\u6bd4\u4e8e\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5c06MAD\u6982\u5ff5\u5316\u4e3a\u4e00\u79cd\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u6280\u672f\uff0c\u5e76\u8fdb\u884c\u5168\u9762\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u6bd4\u8f83MAD\u4e0e\u5355\u667a\u80fd\u4f53\u6269\u5c55\u57fa\u7ebf\u5728\u6570\u5b66\u63a8\u7406\u548c\u5b89\u5168\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4efb\u52a1\u96be\u5ea6\u3001\u6a21\u578b\u89c4\u6a21\u548c\u667a\u80fd\u4f53\u591a\u6837\u6027\u7684\u5f71\u54cd\u3002", "result": "\u6570\u5b66\u63a8\u7406\u4e2d\uff0cMAD\u5728\u95ee\u9898\u96be\u5ea6\u589e\u52a0\u548c\u6a21\u578b\u80fd\u529b\u964d\u4f4e\u65f6\u66f4\u6709\u6548\uff0c\u4f46\u667a\u80fd\u4f53\u591a\u6837\u6027\u5f71\u54cd\u4e0d\u5927\uff1b\u800c\u5728\u5b89\u5168\u4efb\u52a1\u4e2d\uff0cMAD\u7684\u534f\u4f5c\u7ec6\u5316\u53ef\u80fd\u589e\u52a0\u6f0f\u6d1e\uff0c\u4f46\u5f15\u5165\u591a\u6837\u5316\u667a\u80fd\u4f53\u914d\u7f6e\u53ef\u901a\u8fc7\u534f\u4f5c\u9010\u6b65\u51cf\u5c11\u653b\u51fb\u6210\u529f\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u6709\u6548\u4e14\u6218\u7565\u90e8\u7f72\u7684MAD\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u6307\u5bfc\u3002", "keywords": "\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff08MAD\uff09\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u6570\u5b66\u63a8\u7406\u3001\u5b89\u5168\u4efb\u52a1\u3001\u534f\u4f5c\u7ec6\u5316"}}
{"id": "2505.22697", "pdf": "https://arxiv.org/pdf/2505.22697", "abs": "https://arxiv.org/abs/2505.22697", "authors": ["Filippo Rinaldi", "Giacomo Capitani", "Lorenzo Bonicelli", "Donato Crisostomi", "Federico Bolelli", "Elisa Ficarra", "Emanuele Rodol\u00e0", "Simone Calderara", "Angelo Porrello"], "title": "Update Your Transformer to the Latest Release: Re-Basin of Task Vectors", "categories": ["cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Foundation models serve as the backbone for numerous specialized models\ndeveloped through fine-tuning. However, when the underlying pretrained model is\nupdated or retrained (e.g., on larger and more curated datasets), the\nfine-tuned model becomes obsolete, losing its utility and requiring retraining.\nThis raises the question: is it possible to transfer fine-tuning to a new\nrelease of the model? In this work, we investigate how to transfer fine-tuning\nto a new checkpoint without having to re-train, in a data-free manner. To do\nso, we draw principles from model re-basin and provide a recipe based on weight\npermutations to re-base the modifications made to the original base model,\noften called task vector. In particular, our approach tailors model re-basin\nfor Transformer models, taking into account the challenges of residual\nconnections and multi-head attention layers. Specifically, we propose a\ntwo-level method rooted in spectral theory, initially permuting the attention\nheads and subsequently adjusting parameters within select pairs of heads.\nThrough extensive experiments on visual and textual tasks, we achieve the\nseamless transfer of fine-tuned knowledge to new pre-trained backbones without\nrelying on a single training step or datapoint. Code is available at\nhttps://github.com/aimagelab/TransFusion.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6570\u636e\u91cd\u65b0\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6743\u91cd\u7f6e\u6362\u5c06\u5fae\u8c03\u540e\u7684\u6a21\u578b\u77e5\u8bc6\u8fc1\u79fb\u5230\u65b0\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4e0a\uff0c\u907f\u514d\u4e86\u5fae\u8c03\u6a21\u578b\u7684\u8fc7\u65f6\u95ee\u9898\u3002", "motivation": "\u5f53\u9884\u8bad\u7ec3\u6a21\u578b\u66f4\u65b0\u65f6\uff0c\u5fae\u8c03\u6a21\u578b\u4f1a\u5931\u6548\uff0c\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u3002\u7814\u7a76\u8005\u5e0c\u671b\u627e\u5230\u4e00\u79cd\u65e0\u9700\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u5c06\u5fae\u8c03\u77e5\u8bc6\u8fc1\u79fb\u5230\u65b0\u6a21\u578b\u4e0a\u3002", "method": "\u57fa\u4e8e\u6a21\u578b\u91cd\u76c6\u5730\u7406\u8bba\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u5c42\u7ea7\u65b9\u6cd5\uff1a\u9996\u5148\u7f6e\u6362\u6ce8\u610f\u529b\u5934\uff0c\u968f\u540e\u8c03\u6574\u9009\u5b9a\u5934\u5bf9\u7684\u53c2\u6570\u3002", "result": "\u5728\u89c6\u89c9\u548c\u6587\u672c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u65e0\u7f1d\u8fc1\u79fb\u5fae\u8c03\u77e5\u8bc6\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u6570\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5fae\u8c03\u77e5\u8bc6\u7684\u96f6\u6570\u636e\u8fc1\u79fb\uff0c\u89e3\u51b3\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u66f4\u65b0\u65f6\u7684\u5fae\u8c03\u6a21\u578b\u5931\u6548\u95ee\u9898\u3002", "keywords": "\u9884\u8bad\u7ec3\u6a21\u578b,\u5fae\u8c03\u8fc1\u79fb,\u6570\u636e\u65e0\u5173\u7684\u65b9\u6cd5,\u6743\u91cd\u7f6e\u6362,Transformer"}}
{"id": "2505.22771", "pdf": "https://arxiv.org/pdf/2505.22771", "abs": "https://arxiv.org/abs/2505.22771", "authors": ["Christopher Ormerod"], "title": "Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, AIME-Con Conference Submission", "summary": "This study illustrates how incorporating feedback-oriented annotations into\nthe scoring pipeline can enhance the accuracy of automated essay scoring (AES).\nThis approach is demonstrated with the Persuasive Essays for Rating, Selecting,\nand Understanding Argumentative and Discourse Elements (PERSUADE) corpus. We\nintegrate two types of feedback-driven annotations: those that identify\nspelling and grammatical errors, and those that highlight argumentative\ncomponents. To illustrate how this method could be applied in real-world\nscenarios, we employ two LLMs to generate annotations -- a generative language\nmodel used for spell-correction and an encoder-based token classifier trained\nto identify and mark argumentative elements. By incorporating annotations into\nthe scoring process, we demonstrate improvements in performance using\nencoder-based large language models fine-tuned as classifiers.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u53cd\u9988\u9a71\u52a8\u7684\u6ce8\u91ca\uff08\u5982\u62fc\u5199\u3001\u8bed\u6cd5\u9519\u8bef\u548c\u8bba\u8bc1\u6210\u5206\u6807\u8bb0\uff09\u6574\u5408\u5230\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\uff08AES\uff09\u6d41\u7a0b\u4e2d\uff0c\u7ed3\u5408\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u548c\u7f16\u7801\u5668\u5f0f\u5206\u7c7b\u5668\uff0c\u63d0\u5347\u4e86\u8bc4\u5206\u51c6\u786e\u6027\u3002", "motivation": "\u5229\u7528PERSUADE\u8bed\u6599\u5e93\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u53cd\u9988\u9a71\u52a8\u7684\u6ce8\u91ca\u589e\u5f3aAES\u7cfb\u7edf\u7684\u8868\u73b0\u3002", "method": "\u96c6\u6210\u4e24\u79cd\u6ce8\u91ca\uff1a\u62fc\u5199/\u8bed\u6cd5\u9519\u8bef\u6807\u8bb0\u548c\u8bba\u8bc1\u6210\u5206\u8bc6\u522b\uff0c\u4f7f\u7528\u751f\u6210\u5f0f\u6a21\u578b\u548c\u7f16\u7801\u5668\u5206\u7c7b\u5668\u5b9e\u73b0\uff0c\u5e76\u5c06\u6ce8\u91ca\u878d\u5165\u8bc4\u5206\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u878d\u5408\u6ce8\u91ca\u7684\u7f16\u7801\u5668\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bc4\u5206\u6027\u80fd\u4e0a\u6709\u6240\u63d0\u5347\u3002", "conclusion": "\u53cd\u9988\u9a71\u52a8\u7684\u6ce8\u91ca\u80fd\u6709\u6548\u6539\u8fdbAES\u7cfb\u7edf\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u5904\u7406\u590d\u6742\u6587\u672c\u7ed3\u6784\u65f6\u3002", "keywords": "\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206, \u53cd\u9988\u6ce8\u91ca, \u8bed\u8a00\u6a21\u578b, PERSUADE\u8bed\u6599\u5e93"}}
{"id": "2505.22987", "pdf": "https://arxiv.org/pdf/2505.22987", "abs": "https://arxiv.org/abs/2505.22987", "authors": ["Nick Byrd"], "title": "Strategic Reflectivism In Intelligent Systems", "categories": ["cs.AI", "cs.HC", "econ.TH", "C.1.3; I.2.0; I.2.8; I.2.11"], "comment": "An earlier version of this paper was presented at the 2025 ACM\n  Workshop on Human-AI Interaction for Augmented Reasoning\n  (CHI25-WS-AUGMENTED-REASONING). Permission to copy for educational use is\n  granted, provided copies are not for sale or profit and include this notice\n  and full citation on the first page. Other uses require the author permission", "summary": "By late 20th century, the rationality wars had launched debates about the\nnature and norms of intuitive and reflective thinking. Those debates drew from\nmid-20th century ideas such as bounded rationality, which challenged more\nidealized notions of rationality observed since the 19th century. Now that 21st\ncentury cognitive scientists are applying the resulting dual process theories\nto artificial intelligence, it is time to dust off some lessons from this\nhistory. So this paper synthesizes old ideas with recent results from\nexperiments on humans and machines. The result is Strategic Reflectivism, which\ntakes the position that one key to intelligent systems (human or artificial) is\npragmatic switching between intuitive and reflective inference to optimally\nfulfill competing goals. Strategic Reflectivism builds on American Pragmatism,\ntranscends superficial indicators of reflective thinking such as model size or\nchains of thought, and becomes increasingly actionable as we learn more about\nthe value of intuition and reflection.", "AI": {"tldr": "\u672c\u6587\u603b\u7ed3\u4e8620\u4e16\u7eaa\u5173\u4e8e\u76f4\u89c9\u4e0e\u53cd\u601d\u601d\u7ef4\u7684\u7406\u6027\u6218\u4e89\uff0c\u7ed3\u540821\u4e16\u7eaa\u7684\u53cc\u8fc7\u7a0b\u7406\u8bba\uff0c\u63d0\u51fa\u4e86\"\u6218\u7565\u53cd\u601d\u4e3b\u4e49\"\uff0c\u5f3a\u8c03\u667a\u80fd\u7cfb\u7edf\u5e94\u5b9e\u7528\u4e3b\u4e49\u5730\u5207\u6362\u76f4\u89c9\u4e0e\u53cd\u601d\u601d\u7ef4\u4ee5\u5b9e\u73b0\u76ee\u6807\u3002", "motivation": "\u63a2\u8ba8\u76f4\u89c9\u4e0e\u53cd\u601d\u601d\u7ef4\u7684\u6f14\u53d8\u53ca\u5176\u5728\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u5e94\u7528\uff0c\u7ed3\u5408\u5386\u53f2\u4e0e\u6700\u65b0\u5b9e\u9a8c\u6210\u679c\u3002", "method": "\u7efc\u5408\u5386\u53f2\u89c2\u70b9\u548c\u73b0\u4ee3\u5b9e\u9a8c\uff0c\u63d0\u51fa\u6218\u7565\u53cd\u601d\u4e3b\u4e49\u7406\u8bba\u3002", "result": "\u6218\u7565\u53cd\u601d\u4e3b\u4e49\u5f3a\u8c03\u5b9e\u7528\u4e3b\u4e49\u5207\u6362\u76f4\u89c9\u4e0e\u53cd\u601d\u601d\u7ef4\uff0c\u8d85\u8d8a\u6a21\u578b\u5927\u5c0f\u7b49\u8868\u9762\u6307\u6807\u3002", "conclusion": "\u667a\u80fd\u7cfb\u7edf\u7684\u5173\u952e\u5728\u4e8e\u7075\u6d3b\u5207\u6362\u76f4\u89c9\u4e0e\u53cd\u601d\u601d\u7ef4\uff0c\u8fd9\u4e00\u7406\u8bba\u968f\u7814\u7a76\u6df1\u5165\u66f4\u5177\u64cd\u4f5c\u6027\u3002", "keywords": "\u7406\u6027\u6218\u4e89,\u76f4\u89c9\u601d\u7ef4,\u53cd\u601d\u601d\u7ef4,\u53cc\u8fc7\u7a0b\u7406\u8bba,\u6218\u7565\u53cd\u601d\u4e3b\u4e49"}}
{"id": "2505.22703", "pdf": "https://arxiv.org/pdf/2505.22703", "abs": "https://arxiv.org/abs/2505.22703", "authors": ["Mohammad Yaghini", "Tudor Cebere", "Michael Menart", "Aur\u00e9lien Bellet", "Nicolas Papernot"], "title": "Private Rate-Constrained Optimization with Applications to Fair Learning", "categories": ["cs.LG", "cs.CR", "stat.ML"], "comment": null, "summary": "Many problems in trustworthy ML can be formulated as minimization of the\nmodel error under constraints on the prediction rates of the model for\nsuitably-chosen marginals, including most group fairness constraints\n(demographic parity, equality of odds, etc.). In this work, we study such\nconstrained minimization problems under differential privacy (DP). Standard DP\noptimization techniques like DP-SGD rely on the loss function's decomposability\ninto per-sample contributions. However, rate constraints introduce inter-sample\ndependencies, violating the decomposability requirement. To address this, we\ndevelop RaCO-DP, a DP variant of the Stochastic Gradient Descent-Ascent (SGDA)\nalgorithm which solves the Lagrangian formulation of rate constraint problems.\nWe demonstrate that the additional privacy cost of incorporating these\nconstraints reduces to privately estimating a histogram over the mini-batch at\neach optimization step. We prove the convergence of our algorithm through a\nnovel analysis of SGDA that leverages the linear structure of the dual\nparameter. Finally, empirical results on learning under group fairness\nconstraints demonstrate that our method Pareto-dominates existing private\nlearning approaches in fairness-utility trade-offs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86RaCO-DP\uff0c\u4e00\u79cd\u89e3\u51b3\u5728\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u4e0b\u5e26\u901f\u7387\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdbSGDA\u65b9\u6cd5\u5e76\u5728\u9690\u79c1\u6210\u672c\u5206\u6790\u4e2d\u5f15\u5165\u76f4\u65b9\u56fe\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u516c\u5e73\u6027\u4e0e\u6548\u7528\u7684\u6743\u8861\u3002", "motivation": "\u5728\u53ef\u4fe1\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u8bb8\u591a\u95ee\u9898\u53ef\u4ee5\u8f6c\u5316\u4e3a\u5728\u6a21\u578b\u9884\u6d4b\u901f\u7387\u7684\u7ea6\u675f\u4e0b\u6700\u5c0f\u5316\u6a21\u578b\u8bef\u5dee\u3002\u7136\u800c\uff0c\u5dee\u5206\u9690\u79c1\u4f18\u5316\u6280\u672f\uff08\u5982DP-SGD\uff09\u4f9d\u8d56\u4e8e\u635f\u5931\u51fd\u6570\u7684\u53ef\u5206\u6027\uff0c\u800c\u901f\u7387\u7ea6\u675f\u5f15\u5165\u4e86\u6837\u672c\u95f4\u7684\u4f9d\u8d56\u6027\uff0c\u7834\u574f\u4e86\u53ef\u5206\u6027\u8981\u6c42\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86RaCO-DP\u7b97\u6cd5\uff0c\u8fd9\u662fSGDA\u7b97\u6cd5\u7684DP\u53d8\u4f53\u3002\u5b83\u901a\u8fc7\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u6cd5\u89e3\u51b3\u4e86\u5e26\u901f\u7387\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u5e76\u5728\u9690\u79c1\u6210\u672c\u5206\u6790\u4e2d\u5f15\u5165\u4e86\u5bf9\u6bcf\u4e2a\u4f18\u5316\u6b65\u9aa4\u7684\u5c0f\u6279\u91cf\u76f4\u65b9\u56fe\u4f30\u8ba1\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5728\u6ee1\u8db3\u7ec4\u516c\u5e73\u7ea6\u675f\u7684\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0cRaCO-DP\u5728\u516c\u5e73\u6027\u548c\u6548\u7528\u7684\u6743\u8861\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u79c1\u6709\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "RaCO-DP\u901a\u8fc7\u7ed3\u5408SGDA\u548c\u9690\u79c1\u76f4\u65b9\u56fe\u4f30\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5728\u5dee\u5206\u9690\u79c1\u4e0b\u5e26\u901f\u7387\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u516c\u5e73\u6027\u4e0e\u6548\u7528\u7684\u6743\u8861\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u53ef\u4fe1\u673a\u5668\u5b66\u4e60\u3001\u5dee\u5206\u9690\u79c1\u3001\u901f\u7387\u7ea6\u675f\u3001SGDA\u3001\u516c\u5e73\u6027\u3001\u4f18\u5316"}}
{"id": "2505.22774", "pdf": "https://arxiv.org/pdf/2505.22774", "abs": "https://arxiv.org/abs/2505.22774", "authors": ["Kaja Dobrovoljc"], "title": "Counting trees: A treebank-driven exploration of syntactic variation in speech and writing across languages", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a novel treebank-driven approach to comparing syntactic\nstructures in speech and writing using dependency-parsed corpora. Adopting a\nfully inductive, bottom-up method, we define syntactic structures as\ndelexicalized dependency (sub)trees and extract them from spoken and written\nUniversal Dependencies (UD) treebanks in two syntactically distinct languages,\nEnglish and Slovenian. For each corpus, we analyze the size, diversity, and\ndistribution of syntactic inventories, their overlap across modalities, and the\nstructures most characteristic of speech. Results show that, across both\nlanguages, spoken corpora contain fewer and less diverse syntactic structures\nthan their written counterparts, with consistent cross-linguistic preferences\nfor certain structural types across modalities. Strikingly, the overlap between\nspoken and written syntactic inventories is very limited: most structures\nattested in speech do not occur in writing, pointing to modality-specific\npreferences in syntactic organization that reflect the distinct demands of\nreal-time interaction and elaborated writing. This contrast is further\nsupported by a keyness analysis of the most frequent speech-specific\nstructures, which highlights patterns associated with interactivity,\ncontext-grounding, and economy of expression. We argue that this scalable,\nlanguage-independent framework offers a useful general method for\nsystematically studying syntactic variation across corpora, laying the\ngroundwork for more comprehensive data-driven theories of grammar in use.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6811\u5e93\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f9d\u8d56\u89e3\u6790\u8bed\u6599\u5e93\u6bd4\u8f83\u53e3\u8bed\u548c\u4e66\u9762\u8bed\u7684\u53e5\u6cd5\u7ed3\u6784\uff0c\u53d1\u73b0\u53e3\u8bed\u7684\u7ed3\u6784\u66f4\u5c11\u3001\u591a\u6837\u6027\u66f4\u4f4e\uff0c\u4e14\u4e0e\u4e66\u9762\u8bed\u7ed3\u6784\u91cd\u53e0\u6709\u9650\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u6bd4\u8f83\u53e3\u8bed\u548c\u4e66\u9762\u8bed\u7684\u53e5\u6cd5\u7ed3\u6784\u5dee\u5f02\uff0c\u4ee5\u7406\u89e3\u5b9e\u65f6\u4e92\u52a8\u548c\u7cbe\u5fc3\u5199\u4f5c\u7684\u4e0d\u540c\u9700\u6c42\u5982\u4f55\u5f71\u54cd\u53e5\u6cd5\u7ec4\u7ec7\u3002", "method": "\u91c7\u7528\u81ea\u4e0b\u800c\u4e0a\u7684\u5f52\u7eb3\u65b9\u6cd5\uff0c\u4ece\u82f1\u8bed\u548c\u65af\u6d1b\u6587\u5c3c\u4e9a\u8bed\u7684Universal Dependencies\u6811\u5e93\u4e2d\u63d0\u53d6\u53bb\u8bcd\u6c47\u5316\u7684\u4f9d\u8d56\u5b50\u6811\uff0c\u5206\u6790\u5176\u5927\u5c0f\u3001\u591a\u6837\u6027\u548c\u5206\u5e03\u3002", "result": "\u53e3\u8bed\u8bed\u6599\u5e93\u7684\u53e5\u6cd5\u7ed3\u6784\u66f4\u5c11\u3001\u591a\u6837\u6027\u66f4\u4f4e\uff0c\u4e14\u4e0e\u4e66\u9762\u8bed\u7ed3\u6784\u91cd\u53e0\u6709\u9650\uff0c\u663e\u793a\u51fa\u6a21\u6001\u7279\u5b9a\u7684\u53e5\u6cd5\u504f\u597d\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8de8\u8bed\u6599\u5e93\u7684\u53e5\u6cd5\u53d8\u5f02\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u8bed\u8a00\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u66f4\u5168\u9762\u7684\u8bed\u6cd5\u4f7f\u7528\u7406\u8bba\u3002", "keywords": "\u6811\u5e93, \u53e5\u6cd5\u7ed3\u6784, \u53e3\u8bed\u4e0e\u4e66\u9762\u8bed, \u4f9d\u8d56\u89e3\u6790, \u82f1\u8bed, \u65af\u6d1b\u6587\u5c3c\u4e9a\u8bed"}}
{"id": "2505.22990", "pdf": "https://arxiv.org/pdf/2505.22990", "abs": "https://arxiv.org/abs/2505.22990", "authors": ["Pin-Han Chen", "Yu-Sheng Lin", "Wei-Cheng Lee", "Tin-Yu Leu", "Po-Hsiang Hsu", "Anjana Dissanayake", "Sungjin Oh", "Chinq-Shiun Chiu"], "title": "MenTeR: A fully-automated Multi-agenT workflow for end-to-end RF/Analog Circuits Netlist Design", "categories": ["cs.AI", "cs.ET", "cs.LG"], "comment": "9 pages, 7 figures, accepted by IEEE ICLAD 2025", "summary": "RF/Analog design is essential for bridging digital technologies with\nreal-world signals, ensuring the functionality and reliability of a wide range\nof electronic systems. However, analog design procedures are often intricate,\ntime-consuming and reliant on expert intuition, and hinder the time and cost\nefficiency of circuit development. To overcome the limitations of the manual\ncircuit design, we introduce MenTeR - a multiagent workflow integrated into an\nend-to-end analog design framework. By employing multiple specialized AI agents\nthat collaboratively address different aspects of the design process, such as\nspecification understanding, circuit optimization, and test bench validation,\nMenTeR reduces the dependency on frequent trial-and-error-style intervention.\nMenTeR not only accelerates the design cycle time but also facilitates a\nbroader exploration of the design space, demonstrating robust capabilities in\nhandling real-world analog systems. We believe that MenTeR lays the groundwork\nfor future \"RF/Analog Copilots\" that can collaborate seamlessly with human\ndesigners.", "AI": {"tldr": "MenTeR\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u5de5\u4f5c\u6d41\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\uff0c\u52a0\u5feb\u8bbe\u8ba1\u5468\u671f\u3002", "motivation": "\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u590d\u6742\u3001\u8017\u65f6\u4e14\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u624b\u52a8\u8bbe\u8ba1\u6548\u7387\u4f4e\u4e0b\uff0cMenTeR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u591a\u4e2a\u4e13\u7528AI\u4ee3\u7406\u534f\u4f5c\u5904\u7406\u8bbe\u8ba1\u6d41\u7a0b\u7684\u4e0d\u540c\u73af\u8282\uff0c\u5982\u89c4\u683c\u7406\u89e3\u3001\u7535\u8def\u4f18\u5316\u548c\u6d4b\u8bd5\u53f0\u9a8c\u8bc1\u3002", "result": "MenTeR\u7f29\u77ed\u4e86\u8bbe\u8ba1\u5468\u671f\uff0c\u6269\u5c55\u4e86\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9645\u6a21\u62df\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u6027\u80fd\u3002", "conclusion": "MenTeR\u4e3a\u672a\u6765\u4e0e\u4eba\u7c7b\u8bbe\u8ba1\u5e08\u534f\u4f5c\u7684\u201cRF/Analog Copilots\u201d\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "keywords": "RF/Analog\u8bbe\u8ba1, \u591a\u4ee3\u7406\u5de5\u4f5c\u6d41, \u81ea\u52a8\u5316, AI\u4ee3\u7406, \u7535\u8def\u4f18\u5316"}}
{"id": "2505.22758", "pdf": "https://arxiv.org/pdf/2505.22758", "abs": "https://arxiv.org/abs/2505.22758", "authors": ["Aniruddha Nrusimha", "William Brandon", "Mayank Mishra", "Yikang Shen", "Rameswar Panda", "Jonathan Ragan-Kelley", "Yoon Kim"], "title": "FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The size and compute characteristics of modern large language models have led\nto an increased interest in developing specialized kernels tailored for\ntraining and inference. Existing kernels primarily optimize for compute\nutilization, targeting the large-batch training and inference settings.\nHowever, low-batch inference, where memory bandwidth and kernel launch\noverheads contribute are significant factors, remains important for many\napplications of interest such as in edge deployment and latency-sensitive\napplications. This paper describes FlashFormer, a proof-of-concept kernel for\naccelerating single-batch inference for transformer-based large language\nmodels. Across various model sizes and quantizations settings, we observe\nnontrivial speedups compared to existing state-of-the-art inference kernels.", "AI": {"tldr": "FlashFormer\u662f\u4e00\u79cd\u9488\u5bf9\u5355\u6279\u63a8\u7406\u7684\u5b9a\u5236\u5316\u5185\u6838\uff0c\u65e8\u5728\u4f18\u5316\u8fb9\u7f18\u90e8\u7f72\u548c\u5ef6\u8fdf\u654f\u611f\u5e94\u7528\u4e2d\u7684Transformer\u5927\u6a21\u578b\u63a8\u7406\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u5185\u6838\u4e3b\u8981\u4f18\u5316\u5927\u6279\u91cf\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u8ba1\u7b97\u5229\u7528\u7387\uff0c\u800c\u5c0f\u6279\u91cf\u63a8\u7406\uff08\u5982\u8fb9\u7f18\u90e8\u7f72\u548c\u5ef6\u8fdf\u654f\u611f\u5e94\u7528\uff09\u4e2d\u5185\u5b58\u5e26\u5bbd\u548c\u5185\u6838\u542f\u52a8\u5f00\u9500\u4ecd\u662f\u5173\u952e\u95ee\u9898\uff0c\u9700\u8981\u9488\u5bf9\u6027\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4e86FlashFormer\uff0c\u4e00\u79cd\u4e13\u4e3a\u5355\u6279\u63a8\u7406\u8bbe\u8ba1\u7684Transformer\u5927\u6a21\u578b\u52a0\u901f\u5185\u6838\u3002", "result": "\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u91cf\u5316\u8bbe\u7f6e\u4e0b\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u63a8\u7406\u5185\u6838\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\u3002", "conclusion": "FlashFormer\u8bc1\u660e\u4e86\u5c0f\u6279\u91cf\u63a8\u7406\u4f18\u5316\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u8fb9\u7f18\u548c\u4f4e\u5ef6\u8fdf\u573a\u666f\u4e2d\u5177\u5907\u5b9e\u9645\u4ef7\u503c\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u63a8\u7406\u52a0\u901f, \u5185\u6838\u4f18\u5316, \u4f4e\u5ef6\u8fdf, \u8fb9\u7f18\u8ba1\u7b97"}}
{"id": "2505.22777", "pdf": "https://arxiv.org/pdf/2505.22777", "abs": "https://arxiv.org/abs/2505.22777", "authors": ["John Mendon\u00e7a", "Alon Lavie", "Isabel Trancoso"], "title": "MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Chatbots and Dialogue Evaluators", "categories": ["cs.CL"], "comment": "May ARR", "summary": "As the capabilities of chatbots and their underlying LLMs continue to\ndramatically improve, evaluating their performance has increasingly become a\nmajor blocker to their further development. A major challenge is the available\nbenchmarking datasets, which are largely static, outdated, and lacking in\nmultilingual coverage, limiting their ability to capture subtle linguistic and\ncultural variations. This paper introduces MEDAL, an automated multi-agent\nframework for generating, evaluating, and curating more representative and\ndiverse open-domain dialogue evaluation benchmarks. Our approach leverages\nseveral state-of-the-art LLMs to generate user-chatbot multilingual dialogues,\nconditioned on varied seed contexts. A strong LLM (GPT-4.1) is then used for a\nmultidimensional analysis of the performance of the chatbots, uncovering\nnoticeable cross-lingual performance differences. Guided by this large-scale\nevaluation, we curate a new meta-evaluation multilingual benchmark and\nhuman-annotate samples with nuanced quality judgments. This benchmark is then\nused to assess the ability of several reasoning and non-reasoning LLMs to act\nas evaluators of open-domain dialogues. We find that current LLMs struggle to\ndetect nuanced issues, particularly those involving empathy and reasoning.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MEDAL\uff0c\u4e00\u4e2a\u81ea\u52a8\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u3001\u8bc4\u4f30\u548c\u7b5b\u9009\u66f4\u5177\u4ee3\u8868\u6027\u548c\u591a\u6837\u6027\u7684\u5f00\u653e\u9886\u57df\u5bf9\u8bdd\u8bc4\u4f30\u57fa\u51c6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u9759\u6001\u3001\u8fc7\u65f6\u4e14\u591a\u8bed\u8a00\u8986\u76d6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u6570\u636e\u96c6\u591a\u4e3a\u9759\u6001\u3001\u8fc7\u65f6\u4e14\u7f3a\u4e4f\u591a\u8bed\u8a00\u8986\u76d6\uff0c\u65e0\u6cd5\u6355\u6349\u7ec6\u5fae\u7684\u8bed\u8a00\u548c\u6587\u5316\u5dee\u5f02\uff0c\u9650\u5236\u4e86\u5bf9\u8bdd\u7cfb\u7edf\u6027\u80fd\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "method": "\u5229\u7528\u591a\u4e2a\u5148\u8fdb\u7684LLM\u751f\u6210\u591a\u8bed\u8a00\u7528\u6237-\u804a\u5929\u673a\u5668\u4eba\u5bf9\u8bdd\uff0c\u57fa\u4e8e\u591a\u6837\u5316\u7684\u79cd\u5b50\u4e0a\u4e0b\u6587\uff1b\u4f7f\u7528GPT-4.1\u5bf9\u804a\u5929\u673a\u5668\u4eba\u6027\u80fd\u8fdb\u884c\u591a\u7ef4\u5206\u6790\u3002", "result": "\u53d1\u73b0\u8de8\u8bed\u8a00\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u5e76\u53d1\u73b0\u5f53\u524dLLM\u5728\u68c0\u6d4b\u6d89\u53ca\u540c\u7406\u5fc3\u548c\u63a8\u7406\u7684\u7ec6\u5fae\u95ee\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u8bed\u8a00\u57fa\u51c6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5f53\u524dLLM\u4f5c\u4e3a\u5f00\u653e\u9886\u57df\u5bf9\u8bdd\u8bc4\u4f30\u5668\u7684\u5c40\u9650\u6027\u3002", "keywords": "\u591a\u8bed\u8a00\u5bf9\u8bdd\u8bc4\u4f30\u3001\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3001LLM\u6027\u80fd\u5206\u6790\u3001\u5f00\u653e\u9886\u57df\u5bf9\u8bdd"}}
{"id": "2505.23034", "pdf": "https://arxiv.org/pdf/2505.23034", "abs": "https://arxiv.org/abs/2505.23034", "authors": ["Guangyi Liu", "Yongqi Zhang", "Xunyuan Liu", "Quanming Yao"], "title": "Case-Based Reasoning Enhances the Predictive Power of LLMs in Drug-Drug Interaction", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Drug-drug interaction (DDI) prediction is critical for treatment safety.\nWhile large language models (LLMs) show promise in pharmaceutical tasks, their\neffectiveness in DDI prediction remains challenging. Inspired by the\nwell-established clinical practice where physicians routinely reference similar\nhistorical cases to guide their decisions through case-based reasoning (CBR),\nwe propose CBR-DDI, a novel framework that distills pharmacological principles\nfrom historical cases to improve LLM reasoning for DDI tasks. CBR-DDI\nconstructs a knowledge repository by leveraging LLMs to extract pharmacological\ninsights and graph neural networks (GNNs) to model drug associations. A hybrid\nretrieval mechanism and dual-layer knowledge-enhanced prompting allow LLMs to\neffectively retrieve and reuse relevant cases. We further introduce a\nrepresentative sampling strategy for dynamic case refinement. Extensive\nexperiments demonstrate that CBR-DDI achieves state-of-the-art performance,\nwith a significant 28.7% accuracy improvement over both popular LLMs and CBR\nbaseline, while maintaining high interpretability and flexibility.", "AI": {"tldr": "CBR-DDI\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6848\u4f8b\u63a8\u7406\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u5bf9\u6cbb\u7597\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e2d\u6548\u679c\u6709\u9650\u3002\u8bba\u6587\u53d7\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u533b\u751f\u53c2\u8003\u7c7b\u4f3c\u5386\u53f2\u6848\u4f8b\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u57fa\u4e8e\u6848\u4f8b\u63a8\u7406\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCBR-DDI\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u836f\u7406\u5b66\u77e5\u8bc6\uff0c\u56fe\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u836f\u7269\u5173\u8054\uff0c\u7ed3\u5408\u6df7\u5408\u68c0\u7d22\u673a\u5236\u548c\u53cc\u5c42\u77e5\u8bc6\u589e\u5f3a\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u663e\u793aCBR-DDI\u5728\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u51c6\u786e\u7387\u63d0\u534728.7%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "CBR-DDI\u901a\u8fc7\u6574\u5408\u6848\u4f8b\u63a8\u7406\u548c\u5148\u8fdb\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u652f\u6301\u3002", "keywords": "\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b, \u5927\u8bed\u8a00\u6a21\u578b, \u6848\u4f8b\u63a8\u7406, \u56fe\u795e\u7ecf\u7f51\u7edc"}}
{"id": "2505.22764", "pdf": "https://arxiv.org/pdf/2505.22764", "abs": "https://arxiv.org/abs/2505.22764", "authors": ["Divya Shanmugam", "Helen Lu", "Swami Sankaranarayanan", "John Guttag"], "title": "Test-time augmentation improves efficiency in conformal prediction", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "A conformal classifier produces a set of predicted classes and provides a\nprobabilistic guarantee that the set includes the true class. Unfortunately, it\nis often the case that conformal classifiers produce uninformatively large\nsets. In this work, we show that test-time augmentation (TTA)--a technique that\nintroduces inductive biases during inference--reduces the size of the sets\nproduced by conformal classifiers. Our approach is flexible, computationally\nefficient, and effective. It can be combined with any conformal score, requires\nno model retraining, and reduces prediction set sizes by 10%-14% on average. We\nconduct an evaluation of the approach spanning three datasets, three models,\ntwo established conformal scoring methods, different guarantee strengths, and\nseveral distribution shifts to show when and why test-time augmentation is a\nuseful addition to the conformal pipeline.", "AI": {"tldr": "\u901a\u8fc7\u6d4b\u8bd5\u65f6\u589e\u5f3a\uff08TTA\uff09\u6280\u672f\uff0c\u7814\u7a76\u8005\u8bc1\u660e\u4e86\u5b83\u80fd\u591f\u6709\u6548\u51cf\u5c11\u5171\u5f62\u5206\u7c7b\u5668\u4ea7\u751f\u7684\u9884\u6d4b\u96c6\u5927\u5c0f\uff0c\u63d0\u5347\u5206\u7c7b\u6548\u7387\u3002", "motivation": "\u5171\u5f62\u5206\u7c7b\u5668\u5e38\u4ea7\u751f\u8fc7\u5927\u7684\u9884\u6d4b\u96c6\uff0c\u7f3a\u4e4f\u4fe1\u606f\u91cf\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7TTA\u6280\u672f\u51cf\u5c11\u9884\u6d4b\u96c6\u5927\u5c0f\u3002", "method": "\u7ed3\u5408\u4efb\u610f\u5171\u5f62\u5f97\u5206\u65b9\u6cd5\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u91c7\u7528\u6d4b\u8bd5\u65f6\u589e\u5f3a\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9884\u6d4b\u96c6\u5927\u5c0f\u5e73\u5747\u51cf\u5c1110%-14%\uff0c\u4e14\u5728\u591a\u79cd\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u5206\u5e03\u504f\u79fb\u4e0b\u5747\u6709\u6548\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u589e\u5f3a\u662f\u5171\u5f62\u5206\u7c7b\u6d41\u7a0b\u4e2d\u4e00\u79cd\u7075\u6d3b\u3001\u9ad8\u6548\u4e14\u6709\u4ef7\u503c\u7684\u8865\u5145\u6280\u672f\u3002", "keywords": "\u5171\u5f62\u5206\u7c7b\u5668, \u6d4b\u8bd5\u65f6\u589e\u5f3a, \u9884\u6d4b\u96c6\u5927\u5c0f, \u5206\u5e03\u504f\u79fb"}}
{"id": "2505.22787", "pdf": "https://arxiv.org/pdf/2505.22787", "abs": "https://arxiv.org/abs/2505.22787", "authors": ["Christopher Polzak", "Alejandro Lozano", "Min Woo Sun", "James Burgess", "Yuhui Zhang", "Kevin Wu", "Serena Yeung-Levy"], "title": "Can Large Language Models Match the Conclusions of Systematic Reviews?", "categories": ["cs.CL"], "comment": null, "summary": "Systematic reviews (SR), in which experts summarize and analyze evidence\nacross individual studies to provide insights on a specialized topic, are a\ncornerstone for evidence-based clinical decision-making, research, and policy.\nGiven the exponential growth of scientific articles, there is growing interest\nin using large language models (LLMs) to automate SR generation. However, the\nability of LLMs to critically assess evidence and reason across multiple\ndocuments to provide recommendations at the same proficiency as domain experts\nremains poorly characterized. We therefore ask: Can LLMs match the conclusions\nof systematic reviews written by clinical experts when given access to the same\nstudies? To explore this question, we present MedEvidence, a benchmark pairing\nfindings from 100 SRs with the studies they are based on. We benchmark 24 LLMs\non MedEvidence, including reasoning, non-reasoning, medical specialist, and\nmodels across varying sizes (from 7B-700B). Through our systematic evaluation,\nwe find that reasoning does not necessarily improve performance, larger models\ndo not consistently yield greater gains, and knowledge-based fine-tuning\ndegrades accuracy on MedEvidence. Instead, most models exhibit similar\nbehavior: performance tends to degrade as token length increases, their\nresponses show overconfidence, and, contrary to human experts, all models show\na lack of scientific skepticism toward low-quality findings. These results\nsuggest that more work is still required before LLMs can reliably match the\nobservations from expert-conducted SRs, even though these systems are already\ndeployed and being used by clinicians. We release our codebase and benchmark to\nthe broader research community to further investigate LLM-based SR systems.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7cfb\u7edf\u6027\u7efc\u8ff0\uff08SR\uff09\u751f\u6210\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u65e0\u6cd5\u5b8c\u5168\u5339\u914d\u4e34\u5e8a\u4e13\u5bb6\u7684\u7ed3\u8bba\uff0c\u4e14\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u548c\u7f3a\u4e4f\u79d1\u5b66\u6000\u7591\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u79d1\u5b66\u6587\u732e\u7684\u7206\u70b8\u5f0f\u589e\u957f\uff0c\u63a2\u7d22LLMs\u5728\u81ea\u52a8\u5316\u751f\u6210\u7cfb\u7edf\u6027\u7efc\u8ff0\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u6279\u5224\u6027\u8bc4\u4f30\u548c\u591a\u6587\u6863\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7MedEvidence\u57fa\u51c6\uff08\u5305\u542b100\u7bc7SR\u53ca\u5176\u57fa\u7840\u7814\u7a76\uff09\uff0c\u5bf924\u79cdLLMs\uff08\u5305\u62ec\u63a8\u7406\u578b\u3001\u975e\u63a8\u7406\u578b\u3001\u533b\u5b66\u4e13\u7528\u578b\u53ca\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\uff09\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u63a8\u7406\u80fd\u529b\u672a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6a21\u578b\u89c4\u6a21\u4e0e\u6027\u80fd\u65e0\u76f4\u63a5\u5173\u8054\uff0c\u77e5\u8bc6\u5fae\u8c03\u53cd\u800c\u964d\u4f4e\u51c6\u786e\u6027\u3002\u6a21\u578b\u666e\u904d\u5b58\u5728\u54cd\u5e94\u8fc7\u957f\u65f6\u6027\u80fd\u4e0b\u964d\u3001\u8fc7\u5ea6\u81ea\u4fe1\u53ca\u5bf9\u4f4e\u8d28\u91cf\u7814\u7a76\u7f3a\u4e4f\u8d28\u7591\u7684\u95ee\u9898\u3002", "conclusion": "LLMs\u76ee\u524d\u65e0\u6cd5\u53ef\u9760\u5339\u914d\u4e13\u5bb6SR\u751f\u6210\u7684\u7ed3\u8bba\uff0c\u5176\u4e34\u5e8a\u90e8\u7f72\u9700\u66f4\u591a\u6539\u8fdb\u3002\u4ee3\u7801\u4e0e\u57fa\u51c6\u5df2\u5f00\u6e90\u4f9b\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "keywords": "\u7cfb\u7edf\u6027\u7efc\u8ff0, \u5927\u8bed\u8a00\u6a21\u578b, \u4e34\u5e8a\u51b3\u7b56, \u8bc1\u636e\u8bc4\u4f30, MedEvidence"}}
{"id": "2505.23058", "pdf": "https://arxiv.org/pdf/2505.23058", "abs": "https://arxiv.org/abs/2505.23058", "authors": ["Yutong Xie", "Zhuoheng Li", "Xiyuan Wang", "Yijun Pan", "Qijia Liu", "Xingzhi Cui", "Kuang-Yu Lo", "Ruoyi Gao", "Xingjian Zhang", "Jin Huang", "Walter Yuan", "Matthew O. Jackson", "Qiaozhu Mei"], "title": "Be.FM: Open Foundation Models for Human Behavior", "categories": ["cs.AI", "cs.CE", "cs.CL"], "comment": null, "summary": "Despite their success in numerous fields, the potential of foundation models\nfor modeling and understanding human behavior remains largely unexplored. We\nintroduce Be.FM, one of the first open foundation models designed for human\nbehavior modeling. Built upon open-source large language models and fine-tuned\non a diverse range of behavioral data, Be.FM can be used to understand and\npredict human decision-making. We construct a comprehensive set of benchmark\ntasks for testing the capabilities of behavioral foundation models. Our results\ndemonstrate that Be.FM can predict behaviors, infer characteristics of\nindividuals and populations, generate insights about contexts, and apply\nbehavioral science knowledge.", "AI": {"tldr": "Be.FM is an open foundation model for human behavior modeling, built on large language models and fine-tuned on behavioral data, capable of predicting and understanding human decision-making.", "motivation": "To explore the untapped potential of foundation models in understanding and modeling human behavior.", "method": "Built on open-source large language models and fine-tuned with diverse behavioral data.", "result": "Be.FM can predict behaviors, infer individual/population characteristics, generate contextual insights, and apply behavioral science knowledge.", "conclusion": "Be.FM demonstrates significant potential as a foundation model for human behavior modeling.", "keywords": "foundation models, human behavior, large language models, behavioral data, decision-making"}}
{"id": "2505.22768", "pdf": "https://arxiv.org/pdf/2505.22768", "abs": "https://arxiv.org/abs/2505.22768", "authors": ["Mert Onur Cakiroglu", "Idil Bilge Altun", "Hasan Kurban", "Elham Buxton", "Mehmet Dalkilic"], "title": "Multivariate de Bruijn Graphs: A Symbolic Graph Framework for Time Series Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Time series forecasting remains a challenging task for foundation models due\nto temporal heterogeneity, high dimensionality, and the lack of inherent\nsymbolic structure. In this work, we propose DRAGON (Discrete Representation\nand Augmented Graph encoding Over deBruijN Graphs), a novel encoder that\nintroduces Multivariate de Bruijn Graphs (MdBGs) to bridge the gap between\nsymbolic representations and neural modeling. DRAGON discretizes continuous\ninput sequences and maps them onto a fixed graph structure, enabling dynamic\ncontext recovery via graph-based attention. Integrated as an auxiliary module\nwithin a dual-branch architecture, DRAGON augments conventional CNN-based\nencoders with symbolic, structure-aware representations. All code developed for\nthis study is available at:\nhttps://github.com/KurbanIntelligenceLab/MultdBG-Time-Series-Library", "AI": {"tldr": "DRAGON\u5f15\u5165\u591a\u53d8\u91cfde Bruijn\u56fe\uff08MdBGs\uff09\u6765\u8fde\u63a5\u7b26\u53f7\u8868\u793a\u4e0e\u795e\u7ecf\u5efa\u6a21\uff0c\u901a\u8fc7\u79bb\u6563\u5316\u8fde\u7eed\u8f93\u5165\u5e8f\u5217\u5e76\u5728\u56fa\u5b9a\u56fe\u7ed3\u6784\u4e0a\u6620\u5c04\uff0c\u5b9e\u73b0\u57fa\u4e8e\u56fe\u7684\u52a8\u6001\u4e0a\u4e0b\u6587\u6062\u590d\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u56e0\u65f6\u95f4\u5f02\u8d28\u6027\u3001\u9ad8\u7ef4\u6027\u548c\u7f3a\u4e4f\u56fa\u6709\u7b26\u53f7\u7ed3\u6784\u800c\u5bf9\u57fa\u7840\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7DRAGON\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u79bb\u6563\u5316\u8fde\u7eed\u8f93\u5165\u5e8f\u5217\u5e76\u6620\u5c04\u5230\u56fa\u5b9a\u56fe\u7ed3\u6784\uff0c\u5229\u7528\u56fe\u6ce8\u610f\u529b\u5b9e\u73b0\u52a8\u6001\u4e0a\u4e0b\u6587\u6062\u590d\uff0c\u96c6\u6210\u5230\u53cc\u5206\u652f\u67b6\u6784\u4e2d\u4ee5\u589e\u5f3a\u4f20\u7edf\u57fa\u4e8eCNN\u7684\u7f16\u7801\u5668\u3002", "result": "DRAGON\u80fd\u6709\u6548\u589e\u5f3a\u4f20\u7edfCNN\u7f16\u7801\u5668\uff0c\u63d0\u4f9b\u7b26\u53f7\u5316\u3001\u7ed3\u6784\u611f\u77e5\u7684\u8868\u793a\u3002", "conclusion": "\u591a\u53d8\u91cfde Bruijn\u56fe\u7684\u5e94\u7528\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\uff0c\u5f25\u5408\u4e86\u7b26\u53f7\u8868\u793a\u4e0e\u795e\u7ecf\u5efa\u6a21\u7684\u5dee\u8ddd\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b, \u591a\u53d8\u91cfde Bruijn\u56fe, \u7b26\u53f7\u8868\u793a, \u56fe\u6ce8\u610f\u529b, CNN"}}
{"id": "2505.22801", "pdf": "https://arxiv.org/pdf/2505.22801", "abs": "https://arxiv.org/abs/2505.22801", "authors": ["Qing Wang", "Yuepei Li", "Qiao Qiao", "Kang Zhou", "Qi Li"], "title": "Towards a More Generalized Approach in Open Relation Extraction", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Open Relation Extraction (OpenRE) seeks to identify and extract novel\nrelational facts between named entities from unlabeled data without pre-defined\nrelation schemas. Traditional OpenRE methods typically assume that the\nunlabeled data consists solely of novel relations or is pre-divided into known\nand novel instances. However, in real-world scenarios, novel relations are\narbitrarily distributed. In this paper, we propose a generalized OpenRE setting\nthat considers unlabeled data as a mixture of both known and novel instances.\nTo address this, we propose MixORE, a two-phase framework that integrates\nrelation classification and clustering to jointly learn known and novel\nrelations. Experiments on three benchmark datasets demonstrate that MixORE\nconsistently outperforms competitive baselines in known relation classification\nand novel relation clustering. Our findings contribute to the advancement of\ngeneralized OpenRE research and real-world applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MixORE\uff0c\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u7528\u7684OpenRE\u8bbe\u7f6e\uff0c\u80fd\u5728\u672a\u6807\u8bb0\u6570\u636e\u4e2d\u540c\u65f6\u5b66\u4e60\u5df2\u77e5\u548c\u65b0\u5173\u7cfb\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u65b0\u5173\u7cfb\u662f\u968f\u673a\u5206\u5e03\u7684\uff0c\u4f20\u7edfOpenRE\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u672a\u6807\u8bb0\u6570\u636e\u4e2d\u5df2\u77e5\u548c\u65b0\u5173\u7cfb\u7684\u6df7\u5408\u3002", "method": "\u63d0\u51faMixORE\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u7cfb\u5206\u7c7b\u548c\u805a\u7c7b\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u8054\u5408\u5b66\u4e60\u5df2\u77e5\u548c\u65b0\u5173\u7cfb\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cMixORE\u5728\u5df2\u77e5\u5173\u7cfb\u5206\u7c7b\u548c\u65b0\u5173\u7cfb\u805a\u7c7b\u4e0a\u5747\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "MixORE\u63a8\u52a8\u4e86\u901a\u7528OpenRE\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u7684\u8fdb\u5c55\u3002", "keywords": "Open Relation Extraction, MixORE, \u5173\u7cfb\u5206\u7c7b, \u805a\u7c7b, \u672a\u6807\u8bb0\u6570\u636e"}}
{"id": "2505.23075", "pdf": "https://arxiv.org/pdf/2505.23075", "abs": "https://arxiv.org/abs/2505.23075", "authors": ["Amit Kumthekar", "Zion Tilley", "Henry Duong", "Bhargav Patel", "Michael Magnoli", "Ahmed Omar", "Ahmed Nasser", "Chaitanya Gharpure", "Yevgen Reztzov"], "title": "Second Opinion Matters: Towards Adaptive Clinical AI via the Consensus of Expert Model Ensemble", "categories": ["cs.AI", "cs.LG"], "comment": "23 pages, 11 figures", "summary": "Despite the growing clinical adoption of large language models (LLMs),\ncurrent approaches heavily rely on single model architectures. To overcome\nrisks of obsolescence and rigid dependence on single model systems, we present\na novel framework, termed the Consensus Mechanism. Mimicking clinical triage\nand multidisciplinary clinical decision-making, the Consensus Mechanism\nimplements an ensemble of specialized medical expert agents enabling improved\nclinical decision making while maintaining robust adaptability. This\narchitecture enables the Consensus Mechanism to be optimized for cost, latency,\nor performance, purely based on its interior model configuration.\n  To rigorously evaluate the Consensus Mechanism, we employed three medical\nevaluation benchmarks: MedMCQA, MedQA, and MedXpertQA Text, and the\ndifferential diagnosis dataset, DDX+. On MedXpertQA, the Consensus Mechanism\nachieved an accuracy of 61.0% compared to 53.5% and 45.9% for OpenAI's O3 and\nGoogle's Gemini 2.5 Pro. Improvement was consistent across benchmarks with an\nincrease in accuracy on MedQA\n($\\Delta\\mathrm{Accuracy}_{\\mathrm{consensus\\text{-}O3}} = 3.4\\%$) and MedMCQA\n($\\Delta\\mathrm{Accuracy}_{\\mathrm{consensus\\text{-}O3}} = 9.1\\%$). These\naccuracy gains extended to differential diagnosis generation, where our system\ndemonstrated improved recall and precision (F1$_\\mathrm{consensus}$ = 0.326 vs.\nF1$_{\\mathrm{O3\\text{-}high}}$ = 0.2886) and a higher top-1 accuracy for DDX\n(Top1$_\\mathrm{consensus}$ = 52.0% vs. Top1$_{\\mathrm{O3\\text{-}high}}$ =\n45.2%).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a'\u5171\u8bc6\u673a\u5236'\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u4e13\u4e1a\u533b\u7597\u4e13\u5bb6\u4ee3\u7406\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u4e34\u5e8a\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002\u5728\u591a\u4e2a\u533b\u7597\u8bc4\u6d4b\u57fa\u51c6\u4e2d\uff0c\u8be5\u673a\u5236\u8868\u73b0\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u4f9d\u8d56\u5355\u4e00\u67b6\u6784\uff0c\u5b58\u5728\u8fc7\u65f6\u548c\u50f5\u5316\u98ce\u9669\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u591a\u6a21\u578b\u96c6\u6210\u6a21\u62df\u4e34\u5e8a\u4f1a\u8bca\uff0c\u63d0\u5347\u51b3\u7b56\u7075\u6d3b\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa'\u5171\u8bc6\u673a\u5236'\u6846\u67b6\uff0c\u96c6\u6210\u591a\u4e2a\u4e13\u4e1a\u533b\u7597\u4e13\u5bb6\u4ee3\u7406\u6a21\u578b\uff0c\u53ef\u6839\u636e\u9700\u6c42\u4f18\u5316\u6210\u672c\u3001\u5ef6\u8fdf\u6216\u6027\u80fd\u3002", "result": "\u5728MedMCQA\u3001MedQA\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5171\u8bc6\u673a\u5236\u7684\u51c6\u786e\u7387\u663e\u8457\u9ad8\u4e8e\u5355\u4e00\u6a21\u578b\uff08\u5982OpenAI O3\u3001Gemini 2.5 Pro\uff09\uff0c\u5e76\u5728\u9274\u522b\u8bca\u65ad\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u5171\u8bc6\u673a\u5236\u901a\u8fc7\u591a\u6a21\u578b\u534f\u4f5c\u6709\u6548\u63d0\u5347\u4e86\u533b\u7597\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u4e34\u5e8a\u5b9e\u7528\u4ef7\u503c\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u5171\u8bc6\u673a\u5236\u3001\u533b\u7597\u51b3\u7b56\u3001\u591a\u6a21\u578b\u96c6\u6210\u3001\u4e34\u5e8a\u8bca\u65ad"}}
{"id": "2505.22772", "pdf": "https://arxiv.org/pdf/2505.22772", "abs": "https://arxiv.org/abs/2505.22772", "authors": ["Claas Voelcker", "Anastasiia Pedan", "Arash Ahmadian", "Romina Abachi", "Igor Gilitschenski", "Amir-massoud Farahmand"], "title": "Calibrated Value-Aware Model Learning with Stochastic Environment Models", "categories": ["cs.LG"], "comment": null, "summary": "The idea of value-aware model learning, that models should produce accurate\nvalue estimates, has gained prominence in model-based reinforcement learning.\nThe MuZero loss, which penalizes a model's value function prediction compared\nto the ground-truth value function, has been utilized in several prominent\nempirical works in the literature. However, theoretical investigation into its\nstrengths and weaknesses is limited. In this paper, we analyze the family of\nvalue-aware model learning losses, which includes the popular MuZero loss. We\nshow that these losses, as normally used, are uncalibrated surrogate losses,\nwhich means that they do not always recover the correct model and value\nfunction. Building on this insight, we propose corrections to solve this issue.\nFurthermore, we investigate the interplay between the loss calibration, latent\nmodel architectures, and auxiliary losses that are commonly employed when\ntraining MuZero-style agents. We show that while deterministic models can be\nsufficient to predict accurate values, learning calibrated stochastic models is\nstill advantageous.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u4ef7\u503c\u611f\u77e5\u6a21\u578b\u5b66\u4e60\u635f\u5931\u51fd\u6570\uff08\u5305\u62ecMuZero\u635f\u5931\uff09\u7684\u4f18\u7f3a\u70b9\uff0c\u6307\u51fa\u5176\u4f5c\u4e3a\u672a\u6821\u51c6\u7684\u66ff\u4ee3\u635f\u5931\u51fd\u6570\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6848\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86\u635f\u5931\u6821\u51c6\u3001\u6f5c\u5728\u6a21\u578b\u67b6\u6784\u548c\u8f85\u52a9\u635f\u5931\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u4ef7\u503c\u611f\u77e5\u6a21\u578b\u5b66\u4e60\u635f\u5931\u51fd\u6570\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5c24\u5176\u662fMuZero\u635f\u5931\u7684\u4f18\u7f3a\u70b9\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u548c\u503c\u51fd\u6570\u7684\u51c6\u786e\u6027\u3002", "method": "\u5206\u6790\u4ef7\u503c\u611f\u77e5\u6a21\u578b\u5b66\u4e60\u635f\u5931\u51fd\u6570\u5bb6\u65cf\uff0c\u8bc1\u660e\u5176\u4f5c\u4e3a\u672a\u6821\u51c6\u66ff\u4ee3\u635f\u5931\u51fd\u6570\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4fee\u6b63\u65b9\u6cd5\uff1b\u540c\u65f6\u7814\u7a76\u635f\u5931\u6821\u51c6\u3001\u6a21\u578b\u67b6\u6784\u548c\u8f85\u52a9\u635f\u5931\u7684\u5173\u7cfb\u3002", "result": "\u786e\u5b9a\u6027\u6a21\u578b\u53ef\u4ee5\u9884\u6d4b\u51c6\u786e\u503c\uff0c\u4f46\u5b66\u4e60\u6821\u51c6\u7684\u968f\u673a\u6a21\u578b\u66f4\u5177\u4f18\u52bf\uff1b\u672a\u6821\u51c6\u635f\u5931\u51fd\u6570\u65e0\u6cd5\u59cb\u7ec8\u6062\u590d\u6b63\u786e\u7684\u6a21\u578b\u548c\u503c\u51fd\u6570\u3002", "conclusion": "\u901a\u8fc7\u6821\u51c6\u635f\u5931\u51fd\u6570\u548c\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u53ef\u4ee5\u63d0\u9ad8\u503c\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u968f\u673a\u6a21\u578b\u7684\u5b66\u4e60\u4ecd\u7136\u5177\u6709\u4f18\u52bf\u3002", "keywords": "\u4ef7\u503c\u611f\u77e5\u6a21\u578b\u5b66\u4e60, MuZero\u635f\u5931, \u6a21\u578b\u6821\u51c6, \u5f3a\u5316\u5b66\u4e60, \u968f\u673a\u6a21\u578b"}}
{"id": "2505.22809", "pdf": "https://arxiv.org/pdf/2505.22809", "abs": "https://arxiv.org/abs/2505.22809", "authors": ["Andrew Zhu", "Evan Osgood", "Chris Callison-Burch"], "title": "First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "8 pages, 5 figures. In submission at EMNLP 2025", "summary": "Much work has been done on conversational LLM agents which directly assist\nhuman users with tasks. We present an alternative paradigm for interacting with\nLLM agents, which we call \"overhearing agents\". These overhearing agents do not\nactively participate in conversation -- instead, they \"listen in\" on\nhuman-to-human conversations and perform background tasks or provide\nsuggestions to assist the user. In this work, we explore the overhearing agents\nparadigm through the lens of Dungeons & Dragons gameplay. We present an\nin-depth study using large multimodal audio-language models as overhearing\nagents to assist a Dungeon Master. We perform a human evaluation to examine the\nhelpfulness of such agents and find that some large audio-language models have\nthe emergent ability to perform overhearing agent tasks using implicit audio\ncues. Finally, we release Python libraries and our project code to support\nfurther research into the overhearing agents paradigm at\nhttps://github.com/zhudotexe/overhearing_agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u4ee3\u7406\u4ea4\u4e92\u8303\u5f0f\u2014\u2014'\u65c1\u542c\u4ee3\u7406'\uff0c\u901a\u8fc7'\u76d1\u542c'\u4eba\u7c7b\u5bf9\u8bdd\u5728\u80cc\u666f\u4e2d\u5b8c\u6210\u4efb\u52a1\u6216\u63d0\u4f9b\u5efa\u8bae\uff0c\u5e76\u4ee5\u300a\u9f99\u4e0e\u5730\u4e0b\u57ce\u300b\u6e38\u620f\u4e3a\u4f8b\u5c55\u5f00\u7814\u7a76\u3002", "motivation": "\u63a2\u7d22LLM\u4ee3\u7406\u7684\u88ab\u52a8\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5373\u5728\u4eba\u7c7b\u5bf9\u8bdd\u4e2d\u4e0d\u4e3b\u52a8\u53c2\u4e0e\u800c\u662f\u901a\u8fc7\u76d1\u542c\u63d0\u4f9b\u5e2e\u52a9\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u76f4\u63a5\u4ea4\u4e92\u6a21\u5f0f\u7684\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u65c1\u542c\u4ee3\u7406\uff0c\u8f85\u52a9\u300a\u9f99\u4e0e\u5730\u4e0b\u57ce\u300b\u6e38\u620f\u4e2d\u7684\u5730\u4e0b\u57ce\u4e3b\uff0c\u901a\u8fc7\u4eba\u8bc4\u4f30\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u5927\u578b\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u5177\u5907\u901a\u8fc7\u9690\u5f0f\u97f3\u9891\u7ebf\u7d22\u5b8c\u6210\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u4e14\u8bc4\u4f30\u663e\u793a\u8fd9\u4e9b\u4ee3\u7406\u5bf9\u7528\u6237\u6709\u5e2e\u52a9\u3002", "conclusion": "\u65c1\u542c\u4ee3\u7406\u662f\u4e00\u79cd\u53ef\u884c\u7684LLM\u4ea4\u4e92\u8303\u5f0f\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u6f5c\u529b\u3002", "keywords": "\u65c1\u542c\u4ee3\u7406, LLM, \u591a\u6a21\u6001\u6a21\u578b, \u97f3\u9891-\u8bed\u8a00\u6a21\u578b, \u4eba\u673a\u4ea4\u4e92"}}
{"id": "2505.23091", "pdf": "https://arxiv.org/pdf/2505.23091", "abs": "https://arxiv.org/abs/2505.23091", "authors": ["Zeyu Liu", "Yuhang Liu", "Guanghao Zhu", "Congkai Xie", "Zhen Li", "Jianbo Yuan", "Xinyao Wang", "Qing Li", "Shing-Chi Cheung", "Shengyu Zhang", "Fei Wu", "Hongxia Yang"], "title": "Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have demonstrated\nsubstantial progress in reasoning capabilities, such as DeepSeek-R1, which\nleverages rule-based reinforcement learning to enhance logical reasoning\nsignificantly. However, extending these achievements to multimodal large\nlanguage models (MLLMs) presents critical challenges, which are frequently more\npronounced for Multimodal Small Language Models (MSLMs) given their typically\nweaker foundational reasoning abilities: (1) the scarcity of high-quality\nmultimodal reasoning datasets, (2) the degradation of reasoning capabilities\ndue to the integration of visual processing, and (3) the risk that direct\napplication of reinforcement learning may produce complex yet incorrect\nreasoning processes. To address these challenges, we design a novel framework\nInfi-MMR to systematically unlock the reasoning potential of MSLMs through a\ncurriculum of three carefully structured phases and propose our multimodal\nreasoning model Infi-MMR-3B. The first phase, Foundational Reasoning\nActivation, leverages high-quality textual reasoning datasets to activate and\nstrengthen the model's logical reasoning capabilities. The second phase,\nCross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to\nfacilitate the progressive transfer of reasoning skills to multimodal contexts.\nThe third phase, Multimodal Reasoning Enhancement, employs curated,\ncaption-free multimodal data to mitigate linguistic biases and promote robust\ncross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal\nmath reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision\ntest, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on\nMathVista testmini).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInfi-MMR\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bfe\u7a0b\u8bbe\u8ba1\u63d0\u5347\u591a\u6a21\u6001\u5c0f\u8bed\u8a00\u6a21\u578b\uff08MSLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u6709\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5c06\u5176\u6269\u5c55\u81f3\u591a\u6a21\u6001\u5c0f\u8bed\u8a00\u6a21\u578b\uff08MSLMs\uff09\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u3001\u89c6\u89c9\u5904\u7406\u5bfc\u81f4\u63a8\u7406\u80fd\u529b\u4e0b\u964d\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u53ef\u80fd\u751f\u6210\u9519\u8bef\u63a8\u7406\u8fc7\u7a0b\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faInfi-MMR\u6846\u67b6\uff0c\u5305\u542b\u4e09\u9636\u6bb5\u8bfe\u7a0b\uff1a\u57fa\u7840\u63a8\u7406\u6fc0\u6d3b\uff08\u4f7f\u7528\u9ad8\u8d28\u91cf\u6587\u672c\u63a8\u7406\u6570\u636e\uff09\u3001\u8de8\u6a21\u6001\u63a8\u7406\u9002\u5e94\uff08\u5229\u7528\u6807\u9898\u589e\u5f3a\u7684\u591a\u6a21\u6001\u6570\u636e\uff09\u548c\u591a\u6a21\u6001\u63a8\u7406\u589e\u5f3a\uff08\u4f7f\u7528\u65e0\u6807\u9898\u591a\u6a21\u6001\u6570\u636e\u51cf\u5c11\u8bed\u8a00\u504f\u89c1\uff09\u3002", "result": "Infi-MMR-3B\u5728\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u4efb\u52a1\uff08\u5982MathVerse\u3001MathVision\u3001OlympiadBench\uff09\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\uff08MathVista\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5316\u4e09\u9636\u6bb5\u8bbe\u8ba1\uff0cInfi-MMR\u6210\u529f\u63d0\u5347\u4e86MSLMs\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u8def\u5f84\u3002", "keywords": "\u591a\u6a21\u6001\u5c0f\u8bed\u8a00\u6a21\u578b, \u63a8\u7406\u80fd\u529b, Infi-MMR\u6846\u67b6, \u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406"}}
{"id": "2505.22778", "pdf": "https://arxiv.org/pdf/2505.22778", "abs": "https://arxiv.org/abs/2505.22778", "authors": ["Sarah Meiklejohn", "Hayden Blauzvern", "Mihai Maruseac", "Spencer Schrock", "Laurent Simon", "Ilia Shumailov"], "title": "Machine Learning Models Have a Supply Chain Problem", "categories": ["cs.LG", "cs.CR"], "comment": "12 pages, four figures and one table", "summary": "Powerful machine learning (ML) models are now readily available online, which\ncreates exciting possibilities for users who lack the deep technical expertise\nor substantial computing resources needed to develop them. On the other hand,\nthis type of open ecosystem comes with many risks. In this paper, we argue that\nthe current ecosystem for open ML models contains significant supply-chain\nrisks, some of which have been exploited already in real attacks. These include\nan attacker replacing a model with something malicious (e.g., malware), or a\nmodel being trained using a vulnerable version of a framework or on restricted\nor poisoned data. We then explore how Sigstore, a solution designed to bring\ntransparency to open-source software supply chains, can be used to bring\ntransparency to open ML models, in terms of enabling model publishers to sign\ntheir models and prove properties about the datasets they use.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5f00\u653e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u4f9b\u5e94\u94fe\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u4e86\u4f7f\u7528Sigstore\u589e\u52a0\u900f\u660e\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f00\u653e\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u751f\u6001\u7cfb\u7edf\u867d\u7136\u4fbf\u5229\uff0c\u4f46\u5b58\u5728\u4f9b\u5e94\u94fe\u98ce\u9669\uff0c\u4f8b\u5982\u6076\u610f\u6a21\u578b\u66ff\u6362\u6216\u6570\u636e\u6c61\u67d3\uff0c\u8fd9\u4fc3\u4f7f\u4f5c\u8005\u5bfb\u6c42\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5229\u7528Sigstore\u5de5\u5177\uff0c\u901a\u8fc7\u7b7e\u540d\u548c\u9a8c\u8bc1\u6570\u636e\u96c6\u5c5e\u6027\u6765\u589e\u5f3a\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u5b89\u5168\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cSigstore\u53ef\u4ee5\u6709\u6548\u5730\u5e94\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u63d0\u5347\u4f9b\u5e94\u94fe\u7684\u5b89\u5168\u6027\u3002", "conclusion": "\u5f00\u653e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f9b\u5e94\u94fe\u9700\u8981\u66f4\u5f3a\u7684\u900f\u660e\u5ea6\u548c\u5b89\u5168\u63aa\u65bd\uff0cSigstore\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u673a\u5668\u5b66\u4e60, \u4f9b\u5e94\u94fe\u98ce\u9669, Sigstore, \u900f\u660e\u5ea6, \u6570\u636e\u5b89\u5168"}}
{"id": "2505.22823", "pdf": "https://arxiv.org/pdf/2505.22823", "abs": "https://arxiv.org/abs/2505.22823", "authors": ["Yingming Wang", "Pepa Atanasova"], "title": "Self-Critique and Refinement for Faithful Natural Language Explanations", "categories": ["cs.CL"], "comment": "21 pages, 10 figures, 14 tables", "summary": "With the rapid development of large language models (LLMs), natural language\nexplanations (NLEs) have become increasingly important for understanding model\npredictions. However, these explanations often fail to faithfully represent the\nmodel's actual reasoning process. While existing work has demonstrated that\nLLMs can self-critique and refine their initial outputs for various tasks, this\ncapability remains unexplored for improving explanation faithfulness. To\naddress this gap, we introduce Self-critique and Refinement for Natural\nLanguage Explanations (SR-NLE), a framework that enables models to improve the\nfaithfulness of their own explanations -- specifically, post-hoc NLEs --\nthrough an iterative critique and refinement process without external\nsupervision. Our framework leverages different feedback mechanisms to guide the\nrefinement process, including natural language self-feedback and, notably, a\nnovel feedback approach based on feature attribution that highlights important\ninput words. Our experiments across three datasets and four state-of-the-art\nLLMs demonstrate that SR-NLE significantly reduces unfaithfulness rates, with\nour best method achieving an average unfaithfulness rate of 36.02%, compared to\n54.81% for baseline -- an absolute reduction of 18.79%. These findings reveal\nthat the investigated LLMs can indeed refine their explanations to better\nreflect their actual reasoning process, requiring only appropriate guidance\nthrough feedback without additional training or fine-tuning.", "AI": {"tldr": "SR-NLE\u6846\u67b6\u901a\u8fc7\u81ea\u53cd\u9988\u548c\u7279\u5f81\u5f52\u56e0\u53cd\u9988\u673a\u5236\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u5373\u53ef\u63d0\u9ad8LLM\u751f\u6210\u89e3\u91ca\u7684\u5fe0\u5b9e\u5ea6\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u4e0d\u5fe0\u5b9e\u7387\u3002", "motivation": "\u73b0\u6709LLM\u751f\u6210\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u5e38\u65e0\u6cd5\u5fe0\u5b9e\u53cd\u6620\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\uff0c\u4e9f\u9700\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u76d1\u7763\u7684\u81ea\u6211\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSR-NLE\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u53cd\u9988\u548c\u57fa\u4e8e\u7279\u5f81\u5f52\u56e0\u7684\u65b0\u578b\u53cd\u9988\u673a\u5236\uff0c\u8fed\u4ee3\u4f18\u5316\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u548c\u56db\u79cd\u5148\u8fdbLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSR-NLE\u5e73\u5747\u4e0d\u5fe0\u5b9e\u7387\u964d\u81f336.02%\uff0c\u6bd4\u57fa\u7ebf\uff0854.81%\uff09\u7edd\u5bf9\u964d\u4f4e18.79%\u3002", "conclusion": "LLM\u53ef\u901a\u8fc7\u9002\u5f53\u53cd\u9988\u673a\u5236\u81ea\u6211\u4f18\u5316\u89e3\u91ca\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u89e3\u91ca\u5fe0\u5b9e\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u81ea\u7136\u8bed\u8a00\u89e3\u91ca,\u5fe0\u5b9e\u6027,\u81ea\u6211\u6279\u8bc4,\u7279\u5f81\u5f52\u56e0"}}
{"id": "2505.23153", "pdf": "https://arxiv.org/pdf/2505.23153", "abs": "https://arxiv.org/abs/2505.23153", "authors": ["Fan Wang", "Shaoshan Liu"], "title": "Conceptual Framework Toward Embodied Collective Adaptive Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "Collective Adaptive Intelligence (CAI) represent a transformative approach in\nartificial intelligence, wherein numerous autonomous agents collaborate, adapt,\nand self-organize to navigate complex, dynamic environments. This paradigm is\nparticularly impactful in embodied AI applications, where adaptability and\nresilience are paramount. By enabling systems to reconfigure themselves in\nresponse to unforeseen challenges, CAI facilitate robust performance in\nreal-world scenarios. This article introduces a conceptual framework for\ndesigning and analyzing CAI. It delineates key attributes including task\ngeneralization, resilience, scalability, and self-assembly, aiming to bridge\ntheoretical foundations with practical methodologies for engineering adaptive,\nemergent intelligence. By providing a structured foundation for understanding\nand implementing CAI, this work seeks to guide researchers and practitioners in\ndeveloping more resilient, scalable, and adaptable AI systems across various\ndomains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u96c6\u4f53\u81ea\u9002\u5e94\u667a\u80fd\uff08CAI\uff09\u7684\u65b0\u578bAI\u65b9\u6cd5\uff0c\u5f3a\u8c03\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e0e\u81ea\u9002\u5e94\u80fd\u529b\uff0c\u5c24\u5176\u5728\u5177\u8eabAI\u5e94\u7528\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u901a\u8fc7CAI\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2dAI\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u63a8\u52a8\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u7ed3\u5408\u3002", "method": "\u6587\u7ae0\u5f15\u5165\u4e86\u4e00\u4e2a\u6982\u5ff5\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1\u548c\u5206\u6790CAI\uff0c\u6db5\u76d6\u4efb\u52a1\u6cdb\u5316\u3001\u9c81\u68d2\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u81ea\u7ec4\u88c5\u7b49\u5173\u952e\u5c5e\u6027\u3002", "result": "\u8be5\u6846\u67b6\u4e3a\u6784\u5efa\u66f4\u5177\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u65b9\u6cd5\u3002", "conclusion": "CAI\u4e3a\u7814\u7a76\u548c\u5f00\u53d1\u66f4\u5177\u97e7\u6027\u548c\u9002\u5e94\u6027\u7684\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002", "keywords": "\u96c6\u4f53\u81ea\u9002\u5e94\u667a\u80fd, \u5177\u8eabAI, \u9c81\u68d2\u6027, \u53ef\u6269\u5c55\u6027, \u81ea\u7ec4\u88c5"}}
{"id": "2505.22785", "pdf": "https://arxiv.org/pdf/2505.22785", "abs": "https://arxiv.org/abs/2505.22785", "authors": ["Marco Fumero", "Luca Moschella", "Emanuele Rodol\u00e0", "Francesco Locatello"], "title": "Navigating the Latent Space Dynamics of Neural Models", "categories": ["cs.LG"], "comment": null, "summary": "Neural networks transform high-dimensional data into compact, structured\nrepresentations, often modeled as elements of a lower dimensional latent space.\nIn this paper, we present an alternative interpretation of neural models as\ndynamical systems acting on the latent manifold. Specifically, we show that\nautoencoder models implicitly define a latent vector field on the manifold,\nderived by iteratively applying the encoding-decoding map, without any\nadditional training. We observe that standard training procedures introduce\ninductive biases that lead to the emergence of attractor points within this\nvector field. Drawing on this insight, we propose to leverage the vector field\nas a representation for the network, providing a novel tool to analyze the\nproperties of the model and the data. This representation enables to: (i)\nanalyze the generalization and memorization regimes of neural models, even\nthroughout training; (ii) extract prior knowledge encoded in the network's\nparameters from the attractors, without requiring any input data; (iii)\nidentify out-of-distribution samples from their trajectories in the vector\nfield. We further validate our approach on vision foundation models, showcasing\nthe applicability and effectiveness of our method in real-world scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5c06\u795e\u7ecf\u7f51\u7edc\u89c6\u4e3a\u4f5c\u7528\u4e8e\u6f5c\u5728\u6d41\u5f62\u4e0a\u7684\u52a8\u529b\u7cfb\u7edf\u7684\u65b0\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u81ea\u7f16\u7801\u5668\u9690\u5f0f\u5b9a\u4e49\u7684\u6f5c\u5728\u5411\u91cf\u573a\u53ca\u5176\u5438\u5f15\u5b50\u70b9\uff0c\u4e3a\u6a21\u578b\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u7814\u7a76\u591a\u5173\u6ce8\u9759\u6001\u8868\u793a\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u52a8\u529b\u7cfb\u7edf\u89c6\u89d2\u63ed\u793a\u6a21\u578b\u8bad\u7ec3\u4e2d\u9690\u542b\u7684\u52a8\u529b\u5b66\u7279\u6027\u53ca\u5176\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u901a\u8fc7\u8fed\u4ee3\u7f16\u7801-\u89e3\u7801\u6620\u5c04\u9690\u5f0f\u63a8\u5bfc\u6f5c\u5728\u5411\u91cf\u573a\uff0c\u5229\u7528\u6807\u51c6\u8bad\u7ec3\u5f15\u5165\u7684\u5f52\u7eb3\u504f\u7f6e\u5206\u6790\u5438\u5f15\u5b50\u70b9\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u6a21\u578b\u8868\u793a\u5de5\u5177\u3002", "result": "\u9a8c\u8bc1\u4e86\u5411\u91cf\u573a\u53ef\u7528\u4e8e\u5206\u6790\u6cdb\u5316\u4e0e\u8bb0\u5fc6\u673a\u5236\u3001\u65e0\u6570\u636e\u63d0\u53d6\u5148\u9a8c\u77e5\u8bc6\u3001\u68c0\u6d4b\u5206\u5e03\u5916\u6837\u672c\uff0c\u5e76\u5728\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e2d\u6709\u6548\u3002", "conclusion": "\u52a8\u529b\u7cfb\u7edf\u89c6\u89d2\u4e3a\u795e\u7ecf\u7f51\u7edc\u5206\u6790\u548c\u6570\u636e\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u4e0e\u5b9e\u8df5\u5de5\u5177\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc, \u52a8\u529b\u7cfb\u7edf, \u6f5c\u5728\u6d41\u5f62, \u81ea\u7f16\u7801\u5668, \u5438\u5f15\u5b50\u70b9"}}
{"id": "2505.22830", "pdf": "https://arxiv.org/pdf/2505.22830", "abs": "https://arxiv.org/abs/2505.22830", "authors": ["Alexander Gill", "Abhilasha Ravichander", "Ana Marasovi\u0107"], "title": "What Has Been Lost with Synthetic Evaluation?", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages main, 5 pages reference, 24 pages appendix", "summary": "Large language models (LLMs) are increasingly used for data generation.\nHowever, creating evaluation benchmarks raises the bar for this emerging\nparadigm. Benchmarks must target specific phenomena, penalize exploiting\nshortcuts, and be challenging. Through two case studies, we investigate whether\nLLMs can meet these demands by generating reasoning over-text benchmarks and\ncomparing them to those created through careful crowdsourcing. Specifically, we\nevaluate both the validity and difficulty of LLM-generated versions of two\nhigh-quality reading comprehension datasets: CondaQA, which evaluates reasoning\nabout negation, and DROP, which targets reasoning about quantities. We find\nthat prompting LLMs can produce variants of these datasets that are often valid\naccording to the annotation guidelines, at a fraction of the cost of the\noriginal crowdsourcing effort. However, we show that they are less challenging\nfor LLMs than their human-authored counterparts. This finding sheds light on\nwhat may have been lost by generating evaluation data with LLMs, and calls for\ncritically reassessing the immediate use of this increasingly prevalent\napproach to benchmark creation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u8bc4\u6d4b\u57fa\u51c6\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u867d\u80fd\u4f4e\u6210\u672c\u751f\u6210\u6709\u6548\u6570\u636e\uff0c\u4f46\u76f8\u6bd4\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u5bf9LLM\u7684\u6311\u6218\u6027\u66f4\u4f4e\u3002", "motivation": "\u7814\u7a76LLM\u751f\u6210\u8bc4\u6d4b\u57fa\u51c6\u7684\u80fd\u529b\u53ca\u5176\u4e0e\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u5dee\u5f02\uff0c\u4ee5\u8bc4\u4f30\u8be5\u65b9\u6cd5\u7684\u5b9e\u9645\u6548\u679c\u4e0e\u6f5c\u5728\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\uff08CondaQA\u548cDROP\u6570\u636e\u96c6\uff09\uff0c\u6bd4\u8f83LLM\u751f\u6210\u6570\u636e\u4e0e\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u6709\u6548\u6027\u548c\u6311\u6218\u6027\u3002", "result": "LLM\u751f\u6210\u7684\u6570\u636e\u6210\u672c\u4f4e\u4e14\u901a\u5e38\u6709\u6548\uff0c\u4f46\u5bf9LLM\u7684\u6d4b\u8bd5\u6311\u6218\u6027\u663e\u8457\u4f4e\u4e8e\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u3002", "conclusion": "\u9700\u5ba1\u614e\u8bc4\u4f30\u76f4\u63a5\u4f7f\u7528LLM\u751f\u6210\u8bc4\u6d4b\u57fa\u51c6\u7684\u505a\u6cd5\uff0c\u56e0\u5176\u53ef\u80fd\u964d\u4f4e\u8bc4\u6d4b\u7684\u4e25\u8c28\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u8bc4\u6d4b\u57fa\u51c6, \u6570\u636e\u751f\u6210, \u9605\u8bfb\u7406\u89e3, \u6311\u6218\u6027"}}
{"id": "2505.23281", "pdf": "https://arxiv.org/pdf/2505.23281", "abs": "https://arxiv.org/abs/2505.23281", "authors": ["Mislav Balunovi\u0107", "Jasper Dekoninck", "Ivo Petrov", "Nikola Jovanovi\u0107", "Martin Vechev"], "title": "MathArena: Evaluating LLMs on Uncontaminated Math Competitions", "categories": ["cs.AI"], "comment": null, "summary": "The rapid advancement of reasoning capabilities in large language models\n(LLMs) has led to notable improvements on mathematical benchmarks. However,\nmany of the most commonly used evaluation datasets (e.g., AIME 2024) are widely\navailable online, making it difficult to disentangle genuine reasoning from\npotential memorization. Furthermore, these benchmarks do not evaluate\nproof-writing capabilities, which are crucial for many mathematical tasks. To\naddress this, we introduce MathArena, a new benchmark based on the following\nkey insight: recurring math competitions provide a stream of high-quality,\nchallenging problems that can be used for real-time evaluation of LLMs. By\nevaluating models as soon as new problems are released, we effectively\neliminate the risk of contamination. Using this framework, we find strong signs\nof contamination in AIME 2024. Nonetheless, evaluations on harder competitions,\nsuch as SMT 2025 -- published well after model release dates -- demonstrate\nimpressive reasoning capabilities in top-performing models. MathArena is also\nthe first benchmark for proof-writing capabilities. On USAMO 2025, even top\nmodels score below 25%, far behind their performance on final-answer tasks. So\nfar, we have evaluated 30 models across five competitions, totaling 149\nproblems. As an evolving benchmark, MathArena will continue to track the\nprogress of LLMs on newly released competitions, ensuring rigorous and\nup-to-date evaluation of mathematical reasoning.", "AI": {"tldr": "MathArena\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5b9e\u65f6\u8bc4\u4f30LLM\u5728\u65b0\u53d1\u5e03\u7684\u6570\u5b66\u7ade\u8d5b\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u53ef\u80fd\u5b58\u5728\u7684\u8bb0\u5fc6\u6c61\u67d3\u95ee\u9898\uff0c\u5e76\u9996\u6b21\u8bc4\u4f30\u4e86\u8bc1\u660e\u4e66\u5199\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982AIME 2024\uff09\u5e7f\u6cdb\u5728\u7ebf\u53ef\u7528\uff0c\u96be\u4ee5\u533a\u5206\u6a21\u578b\u662f\u771f\u6b63\u63a8\u7406\u8fd8\u662f\u8bb0\u5fc6\u7b54\u6848\uff0c\u4e14\u672a\u8bc4\u4f30\u8bc1\u660e\u4e66\u5199\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u6570\u5b66\u7ade\u8d5b\u95ee\u9898\u6d41\u8bbe\u8ba1MathArena\uff0c\u901a\u8fc7\u5728\u65b0\u95ee\u9898\u53d1\u5e03\u540e\u5b9e\u65f6\u8bc4\u4f30\u6a21\u578b\uff0c\u907f\u514d\u6c61\u67d3\u98ce\u9669\u3002", "result": "\u5728AIME 2024\u4e2d\u53d1\u73b0\u660e\u663e\u6c61\u67d3\u8ff9\u8c61\uff0c\u4f46\u5728SMT 2025\u7b49\u540e\u671f\u7ade\u8d5b\u4e2d\uff0c\u9876\u7ea7\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff1bUSAMO 2025\u8bc1\u660e\u4efb\u52a1\u4e2d\u6a21\u578b\u8868\u73b0\u4f4e\u4e8e25%\u3002", "conclusion": "MathArena\u4e3a\u6570\u5b66\u63a8\u7406\u63d0\u4f9b\u4e86\u4e25\u8c28\u4e14\u4e0e\u65f6\u4ff1\u8fdb\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7a81\u51fa\u4e86LLM\u5728\u8bc1\u660e\u4e66\u5199\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002", "keywords": "MathArena, LLM, \u6570\u5b66\u63a8\u7406, \u8bc1\u660e\u4e66\u5199, \u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2505.22798", "pdf": "https://arxiv.org/pdf/2505.22798", "abs": "https://arxiv.org/abs/2505.22798", "authors": ["Anton Bj\u00f6rklund", "Mykola Zaitsev", "Marta Kwiatkowska"], "title": "Efficient Preimage Approximation for Neural Network Certification", "categories": ["cs.LG", "cs.AI", "cs.CR", "68T07"], "comment": null, "summary": "The growing reliance on artificial intelligence in safety- and\nsecurity-critical applications demands effective neural network certification.\nA challenging real-world use case is certification against ``patch attacks'',\nwhere adversarial patches or lighting conditions obscure parts of images, for\nexample traffic signs. One approach to certification, which also gives\nquantitative coverage estimates, utilizes preimages of neural networks, i.e.,\nthe set of inputs that lead to a specified output. However, these preimage\napproximation methods, including the state-of-the-art PREMAP algorithm,\nstruggle with scalability. This paper presents novel algorithmic improvements\nto PREMAP involving tighter bounds, adaptive Monte Carlo sampling, and improved\nbranching heuristics. We demonstrate efficiency improvements of at least an\norder of magnitude on reinforcement learning control benchmarks, and show that\nour method scales to convolutional neural networks that were previously\ninfeasible. Our results demonstrate the potential of preimage approximation\nmethodology for reliability and robustness certification.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684PREMAP\u7b97\u6cd5\uff0c\u901a\u8fc7\u66f4\u7d27\u7684\u8fb9\u754c\u3001\u81ea\u9002\u5e94\u8499\u7279\u5361\u6d1b\u91c7\u6837\u548c\u4f18\u5316\u7684\u5206\u652f\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u7f51\u7edc\u9884\u50cf\u8fd1\u4f3c\u65b9\u6cd5\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u968f\u7740AI\u5728\u5b89\u5168\u548c\u5b89\u9632\u5173\u952e\u5e94\u7528\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u6709\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u8ba4\u8bc1\u9700\u6c42\u589e\u957f\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u2018\u8865\u4e01\u653b\u51fb\u2019\u7b49\u5b9e\u9645\u7528\u4f8b\u3002\u5f53\u524d\u9884\u50cf\u8fd1\u4f3c\u65b9\u6cd5\uff08\u5982PREMAP\uff09\u9762\u4e34\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002", "method": "\u63d0\u51fa\u6539\u8fdbPREMAP\u7684\u65b0\u7b97\u6cd5\uff0c\u5305\u62ec\u66f4\u4e25\u683c\u7684\u8fb9\u754c\u5206\u6790\u3001\u81ea\u9002\u5e94\u8499\u7279\u5361\u6d1b\u91c7\u6837\u548c\u4f18\u5316\u7684\u5206\u652f\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "\u5728\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6548\u7387\u63d0\u5347\u81f3\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u4e4b\u524d\u4e0d\u53ef\u884c\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002", "conclusion": "\u9884\u50cf\u8fd1\u4f3c\u65b9\u6cd5\u5728\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u8ba4\u8bc1\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u6539\u8fdb\u540e\u7684\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5176\u9002\u7528\u6027\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc\u8ba4\u8bc1, \u9884\u50cf\u8fd1\u4f3c, PREMAP\u7b97\u6cd5, \u8865\u4e01\u653b\u51fb, \u53ef\u6269\u5c55\u6027"}}
{"id": "2505.22842", "pdf": "https://arxiv.org/pdf/2505.22842", "abs": "https://arxiv.org/abs/2505.22842", "authors": ["Arthur S. Bianchessi", "Rodrigo C. Barros", "Lucas S. Kupssinsk\u00fc"], "title": "Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation", "categories": ["cs.CL", "cs.LG", "I.2.6; I.2.7"], "comment": null, "summary": "Transformer-based language models rely on positional encoding (PE) to handle\ntoken order and support context length extrapolation. However, existing PE\nmethods lack theoretical clarity and rely on limited evaluation metrics to\nsubstantiate their extrapolation claims. We propose the Bayesian Attention\nMechanism (BAM), a theoretical framework that formulates positional encoding as\na prior within a probabilistic model. BAM unifies existing methods (e.g., NoPE\nand ALiBi) and motivates a new Generalized Gaussian positional prior that\nsubstantially improves long-context generalization. Empirically, BAM enables\naccurate information retrieval at $500\\times$ the training context length,\noutperforming previous state-of-the-art context length generalization in long\ncontext retrieval accuracy while maintaining comparable perplexity and\nintroducing minimal additional parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86Bayesian Attention Mechanism\uff08BAM\uff09\uff0c\u5229\u7528\u8d1d\u53f6\u65af\u7406\u8bba\u7edf\u4e00\u73b0\u6709\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u7406\u8bba\u57fa\u7840\u4e0d\u8db3\u4e14\u8bc4\u4f30\u4e0d\u5168\u9762\u3002", "method": "\u5c06\u4f4d\u7f6e\u7f16\u7801\u5efa\u6a21\u4e3a\u6982\u7387\u6a21\u578b\u4e2d\u7684\u5148\u9a8c\uff0c\u63d0\u51fa\u5e7f\u4e49\u9ad8\u65af\u4f4d\u7f6e\u5148\u9a8c\u3002", "result": "\u57285\u500d\u8bad\u7ec3\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff0c\u4fdd\u6301\u56f0\u60d1\u5ea6\u63a5\u8fd1\u7684\u540c\u65f6\u5f15\u5165\u6781\u5c11\u53c2\u6570\u3002", "conclusion": "BAM\u7406\u8bba\u6846\u67b6\u7edf\u4e00\u6027\u5f3a\uff0c\u957f\u4e0a\u4e0b\u6587\u8868\u73b0\u4f18\u8d8a\u3002", "keywords": "Transformer, \u4f4d\u7f6e\u7f16\u7801, \u8d1d\u53f6\u65af\u6ce8\u610f\u529b\u673a\u5236, \u957f\u4e0a\u4e0b\u6587\u6cdb\u5316"}}
{"id": "2505.23381", "pdf": "https://arxiv.org/pdf/2505.23381", "abs": "https://arxiv.org/abs/2505.23381", "authors": ["Bowen Ping", "Minnan Luo", "Zhuohang Dang", "Chenxi Wang", "Chengyou Jia"], "title": "AutoGPS: Automated Geometry Problem Solving via Multimodal Formalization and Deductive Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Geometry problem solving presents distinctive challenges in artificial\nintelligence, requiring exceptional multimodal comprehension and rigorous\nmathematical reasoning capabilities. Existing approaches typically fall into\ntwo categories: neural-based and symbolic-based methods, both of which exhibit\nlimitations in reliability and interpretability. To address this challenge, we\npropose AutoGPS, a neuro-symbolic collaborative framework that solves geometry\nproblems with concise, reliable, and human-interpretable reasoning processes.\nSpecifically, AutoGPS employs a Multimodal Problem Formalizer (MPF) and a\nDeductive Symbolic Reasoner (DSR). The MPF utilizes neural cross-modal\ncomprehension to translate geometry problems into structured formal language\nrepresentations, with feedback from DSR collaboratively. The DSR takes the\nformalization as input and formulates geometry problem solving as a hypergraph\nexpansion task, executing mathematically rigorous and reliable derivation to\nproduce minimal and human-readable stepwise solutions. Extensive experimental\nevaluations demonstrate that AutoGPS achieves state-of-the-art performance on\nbenchmark datasets. Furthermore, human stepwise-reasoning evaluation confirms\nAutoGPS's impressive reliability and interpretability, with 99\\% stepwise\nlogical coherence. The project homepage is at\nhttps://jayce-ping.github.io/AutoGPS-homepage.", "AI": {"tldr": "AutoGPS is a neuro-symbolic framework that combines neural and symbolic methods to solve geometry problems with high reliability and interpretability, achieving state-of-the-art performance.", "motivation": "Existing neural and symbolic methods for geometry problem solving have limitations in reliability and interpretability, prompting the development of a collaborative framework.", "method": "AutoGPS uses a Multimodal Problem Formalizer (MPF) for cross-modal comprehension and a Deductive Symbolic Reasoner (DSR) for hypergraph-based symbolic reasoning.", "result": "AutoGPS achieves top performance on benchmarks, with 99% logical coherence in stepwise solutions, demonstrating high reliability and interpretability.", "conclusion": "AutoGPS successfully integrates neural and symbolic approaches to address geometry problem-solving challenges, offering concise and human-readable solutions.", "keywords": "geometry problem solving, neuro-symbolic, multimodal comprehension, symbolic reasoning, AutoGPS"}}
{"id": "2505.22803", "pdf": "https://arxiv.org/pdf/2505.22803", "abs": "https://arxiv.org/abs/2505.22803", "authors": ["Pedro Mendes", "Paolo Romano", "David Garlan"], "title": "CLUE: Neural Networks Calibration via Learning Uncertainty-Error alignment", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reliable uncertainty estimation is critical for deploying neural networks\n(NNs) in real-world applications. While existing calibration techniques often\nrely on post-hoc adjustments or coarse-grained binning methods, they remain\nlimited in scalability, differentiability, and generalization across domains.\nIn this work, we introduce CLUE (Calibration via Learning Uncertainty-Error\nAlignment), a novel approach that explicitly aligns predicted uncertainty with\nobserved error during training, grounded in the principle that well-calibrated\nmodels should produce uncertainty estimates that match their empirical loss.\nCLUE adopts a novel loss function that jointly optimizes predictive performance\nand calibration, using summary statistics of uncertainty and loss as proxies.\nThe proposed method is fully differentiable, domain-agnostic, and compatible\nwith standard training pipelines. Through extensive experiments on vision,\nregression, and language modeling tasks, including out-of-distribution and\ndomain-shift scenarios, we demonstrate that CLUE achieves superior calibration\nquality and competitive predictive performance with respect to state-of-the-art\napproaches without imposing significant computational overhead.", "AI": {"tldr": "CLUE\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6821\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u9884\u6d4b\u6027\u80fd\u548c\u6821\u51c6\u8d28\u91cf\uff0c\u5728\u8bad\u7ec3\u4e2d\u663e\u5f0f\u5bf9\u9f50\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u4e0e\u89c2\u6d4b\u8bef\u5dee\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6821\u51c6\u6548\u679c\u548c\u7ade\u4e89\u6027\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6821\u51c6\u6280\u672f\u5728\u53ef\u6269\u5c55\u6027\u3001\u53ef\u5fae\u6027\u548c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u795e\u7ecf\u7f51\u7edc\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "method": "\u63d0\u51faCLUE\u65b9\u6cd5\uff0c\u91c7\u7528\u65b0\u578b\u635f\u5931\u51fd\u6570\u8054\u5408\u4f18\u5316\u9884\u6d4b\u6027\u80fd\u548c\u6821\u51c6\uff0c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u548c\u635f\u5931\u7684\u7edf\u8ba1\u91cf\u4f5c\u4e3a\u4ee3\u7406\u3002", "result": "\u5728\u89c6\u89c9\u3001\u56de\u5f52\u548c\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\uff0cCLUE\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6821\u51c6\u8d28\u91cf\u548c\u7ade\u4e89\u6027\u9884\u6d4b\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e\u3002", "conclusion": "CLUE\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u9886\u57df\u65e0\u5173\u7684\u6821\u51c6\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u548c\u573a\u666f\u3002", "keywords": "uncertainty estimation, calibration, neural networks, CLUE, domain-agnostic"}}
{"id": "2505.22848", "pdf": "https://arxiv.org/pdf/2505.22848", "abs": "https://arxiv.org/abs/2505.22848", "authors": ["Pingjun Hong", "Beiduo Chen", "Siyao Peng", "Marie-Catherine de Marneffe", "Barbara Plank"], "title": "LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference", "categories": ["cs.CL"], "comment": "21 pages, 6 figures", "summary": "There is increasing evidence of Human Label Variation (HLV) in Natural\nLanguage Inference (NLI), where annotators assign different labels to the same\npremise-hypothesis pair. However, within-label variation--cases where\nannotators agree on the same label but provide divergent reasoning--poses an\nadditional and mostly overlooked challenge. Several NLI datasets contain\nhighlighted words in the NLI item as explanations, but the same spans on the\nNLI item can be highlighted for different reasons, as evidenced by free-text\nexplanations, which offer a window into annotators' reasoning. To\nsystematically understand this problem and gain insight into the rationales\nbehind NLI labels, we introduce LITEX, a linguistically-informed taxonomy for\ncategorizing free-text explanations. Using this taxonomy, we annotate a subset\nof the e-SNLI dataset, validate the taxonomy's reliability, and analyze how it\naligns with NLI labels, highlights, and explanations. We further assess the\ntaxonomy's usefulness in explanation generation, demonstrating that\nconditioning generation on LITEX yields explanations that are linguistically\ncloser to human explanations than those generated using only labels or\nhighlights. Our approach thus not only captures within-label variation but also\nshows how taxonomy-guided generation for reasoning can bridge the gap between\nhuman and model explanations more effectively than existing strategies.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u4e2d\u7684\u4eba\u7c7b\u6807\u7b7e\u53d8\u5f02\uff08HLV\uff09\u53ca\u6807\u6ce8\u8005\u540c\u4e00\u6807\u7b7e\u4e0b\u7684\u63a8\u7406\u5dee\u5f02\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u5b66\u7684\u5206\u7c7b\u6cd5LITEX\uff0c\u7528\u4e8e\u7cfb\u7edf\u5316\u6807\u6ce8\u81ea\u7531\u6587\u672c\u89e3\u91ca\uff0c\u5e76\u5728e-SNLI\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u53ef\u9760\u6027\u548c\u89e3\u91ca\u751f\u6210\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3NLI\u4e2d\u6807\u6ce8\u8005\u540c\u4e00\u6807\u7b7e\u4e0b\u63a8\u7406\u5dee\u5f02\u7684\u95ee\u9898\uff0c\u5e76\u7cfb\u7edf\u5316\u7406\u89e3NLI\u6807\u7b7e\u80cc\u540e\u7684\u7406\u6027\uff0c\u672c\u6587\u5f15\u5165\u4e86LITEX\u8fd9\u4e00\u5206\u7c7b\u6cd5\uff0c\u65e8\u5728\u6355\u6349\u8fd9\u4e00\u53d8\u5f02\u5e76\u6539\u8fdb\u89e3\u91ca\u751f\u6210\u3002", "method": "\u5f15\u5165\u4e86LITEX\u8fd9\u4e00\u57fa\u4e8e\u8bed\u8a00\u5b66\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u5728e-SNLI\u6570\u636e\u96c6\u7684\u4e00\u4e2a\u5b50\u96c6\u4e0a\u8fdb\u884c\u4e86\u6807\u6ce8\u548c\u9a8c\u8bc1\uff0c\u8bc4\u4f30\u5176\u5bf9\u89e3\u91ca\u751f\u6210\u7684\u6548\u679c\u3002", "result": "LITEX\u88ab\u9a8c\u8bc1\u4e3a\u53ef\u9760\u4e14\u80fd\u66f4\u597d\u5730\u4e0eNLI\u6807\u7b7e\u3001\u9ad8\u4eae\u90e8\u5206\u548c\u89e3\u91ca\u5bf9\u9f50\uff0c\u4f7f\u7528LITEX\u751f\u6210\u7684\u89e3\u91ca\u5728\u8bed\u8a00\u5b66\u4e0a\u6bd4\u57fa\u4e8e\u6807\u7b7e\u6216\u9ad8\u4eae\u90e8\u5206\u7684\u89e3\u91ca\u66f4\u63a5\u8fd1\u4eba\u7c7b\u89e3\u91ca\u3002", "conclusion": "LITEX\u4e0d\u4ec5\u6355\u6349\u4e86\u6807\u7b7e\u5185\u90e8\u7684\u53d8\u5f02\uff0c\u8fd8\u901a\u8fc7\u5206\u7c7b\u6307\u5bfc\u7684\u751f\u6210\u66f4\u597d\u5730\u7f29\u5c0f\u4e86\u4eba\u7c7b\u4e0e\u6a21\u578b\u89e3\u91ca\u95f4\u7684\u5dee\u8ddd\u3002", "keywords": "Human Label Variation, Natural Language Inference, Within-label Variation, LITEX, Explanation Generation"}}
{"id": "2505.23397", "pdf": "https://arxiv.org/pdf/2505.23397", "abs": "https://arxiv.org/abs/2505.23397", "authors": ["Ahmad Mohsin", "Helge Janicke", "Ahmed Ibrahim", "Iqbal H. Sarker", "Seyit Camtepe"], "title": "A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy", "categories": ["cs.AI", "cs.CR"], "comment": "Journal Article", "summary": "This article presents a structured framework for Human-AI collaboration in\nSecurity Operations Centers (SOCs), integrating AI autonomy, trust calibration,\nand Human-in-the-loop decision making. Existing frameworks in SOCs often focus\nnarrowly on automation, lacking systematic structures to manage human\noversight, trust calibration, and scalable autonomy with AI. Many assume static\nor binary autonomy settings, failing to account for the varied complexity,\ncriticality, and risk across SOC tasks considering Humans and AI collaboration.\nTo address these limitations, we propose a novel autonomy tiered framework\ngrounded in five levels of AI autonomy from manual to fully autonomous, mapped\nto Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This\nenables adaptive and explainable AI integration across core SOC functions,\nincluding monitoring, protection, threat detection, alert triage, and incident\nresponse. The proposed framework differentiates itself from previous research\nby creating formal connections between autonomy, trust, and HITL across various\nSOC levels, which allows for adaptive task distribution according to\noperational complexity and associated risks. The framework is exemplified\nthrough a simulated cyber range that features the cybersecurity AI-Avatar, a\nfine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates\nhuman-AI collaboration for SOC tasks, reducing alert fatigue, enhancing\nresponse coordination, and strategically calibrating trust. This research\nsystematically presents both the theoretical and practical aspects and\nfeasibility of designing next-generation cognitive SOCs that leverage AI not to\nreplace but to enhance human decision-making.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5b89\u5168\u8fd0\u8425\u4e2d\u5fc3\uff08SOC\uff09\u4e2d\u4eba\u7c7b\u4e0eAI\u534f\u4f5c\u7684\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u6db5\u76d6AI\u81ea\u4e3b\u6027\u3001\u4fe1\u4efb\u6821\u51c6\u548c\u4eba\u7c7b\u53c2\u4e0e\u51b3\u7b56\u3002\u73b0\u6709\u6846\u67b6\u8fc7\u4e8e\u4fa7\u91cd\u81ea\u52a8\u5316\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7ba1\u7406\uff0c\u65b0\u6846\u67b6\u901a\u8fc7\u4e94\u4e2aAI\u81ea\u4e3b\u6027\u7ea7\u522b\u4e0e\u4eba\u7c7b\u89d2\u8272\u548c\u4efb\u52a1\u4fe1\u4efb\u9608\u503c\u7ed3\u5408\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94AI\u96c6\u6210\u3002", "motivation": "\u5f53\u524dSOC\u6846\u67b6\u5728\u7ba1\u7406\u4eba\u7c7b\u76d1\u7763\u3001\u4fe1\u4efb\u6821\u51c6\u548c\u53ef\u6269\u5c55AI\u81ea\u4e3b\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5e94\u5bf9\u4e0d\u540c\u4efb\u52a1\u590d\u6742\u6027\u548c\u98ce\u9669\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51fa\u4e00\u4e2a\u66f4\u7075\u6d3b\u3001\u53ef\u89e3\u91ca\u7684\u534f\u4f5c\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u4e94\u4e2aAI\u81ea\u4e3b\u6027\u7ea7\u522b\u7684\u65b0\u6846\u67b6\uff0c\u5c06\u5176\u4e0e\u4eba\u7c7b\u89d2\u8272\u548c\u4efb\u52a1\u4fe1\u4efb\u9608\u503c\u6620\u5c04\uff0c\u901a\u8fc7\u6a21\u62df\u7f51\u7edc\u573a\u666f\u548cAI-Avatar\u6848\u4f8b\u9a8c\u8bc1\u3002", "result": "\u6846\u67b6\u5728\u6a21\u62df\u73af\u5883\u4e2d\u6210\u529f\u51cf\u5c11\u8b66\u62a5\u75b2\u52b3\u3001\u63d0\u5347\u54cd\u5e94\u534f\u8c03\u80fd\u529b\uff0c\u5e76\u52a8\u6001\u6821\u51c6\u4fe1\u4efb\uff0c\u5c55\u793a\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0b\u4e00\u4ee3\u8ba4\u77e5SOC\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86AI\u5728\u589e\u5f3a\u800c\u975e\u53d6\u4ee3\u4eba\u7c7b\u51b3\u7b56\u65b9\u9762\u7684\u6f5c\u529b\u3002", "keywords": "Human-AI\u534f\u4f5c, SOC, AI\u81ea\u4e3b\u6027, \u4fe1\u4efb\u6821\u51c6, \u4eba\u7c7b\u53c2\u4e0e\u51b3\u7b56"}}
{"id": "2505.22813", "pdf": "https://arxiv.org/pdf/2505.22813", "abs": "https://arxiv.org/abs/2505.22813", "authors": ["Josiah Couch", "Miao Li", "Rima Arnaout", "Ramy Arnaout"], "title": "X-Factor: Quality Is a Dataset-Intrinsic Property", "categories": ["cs.LG", "68T07", "I.2.6"], "comment": "13 pages, 7 figures", "summary": "In the universal quest to optimize machine-learning classifiers, three\nfactors -- model architecture, dataset size, and class balance -- have been\nshown to influence test-time performance but do not fully account for it.\nPreviously, evidence was presented for an additional factor that can be\nreferred to as dataset quality, but it was unclear whether this was actually a\njoint property of the dataset and the model architecture, or an intrinsic\nproperty of the dataset itself. If quality is truly dataset-intrinsic and\nindependent of model architecture, dataset size, and class balance, then the\nsame datasets should perform better (or worse) regardless of these other\nfactors. To test this hypothesis, here we create thousands of datasets, each\ncontrolled for size and class balance, and use them to train classifiers with a\nwide range of architectures, from random forests and support-vector machines to\ndeep networks. We find that classifier performance correlates strongly by\nsubset across architectures ($R^2=0.79$), supporting quality as an intrinsic\nproperty of datasets independent of dataset size and class balance and of model\narchitecture. Digging deeper, we find that dataset quality appears to be an\nemergent property of something more fundamental: the quality of datasets'\nconstituent classes. Thus, quality joins size, class balance, and model\narchitecture as an independent correlate of performance and a separate target\nfor optimizing machine-learning-based classification.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u53d1\u73b0\u6570\u636e\u96c6\u8d28\u91cf\u662f\u72ec\u7acb\u4e8e\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u96c6\u5927\u5c0f\u548c\u7c7b\u522b\u5e73\u8861\u7684\u5185\u5728\u5c5e\u6027\uff0c\u4e14\u6570\u636e\u96c6\u8d28\u91cf\u4e0e\u5176\u6784\u6210\u7c7b\u522b\u7684\u8d28\u91cf\u76f8\u5173\uff0c\u6210\u4e3a\u4f18\u5316\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u7684\u53e6\u4e00\u76ee\u6807\u3002", "motivation": "\u5c3d\u7ba1\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u96c6\u5927\u5c0f\u548c\u7c7b\u522b\u5e73\u8861\u5df2\u88ab\u8bc1\u5b9e\u5f71\u54cd\u5206\u7c7b\u5668\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u65e0\u6cd5\u5b8c\u5168\u89e3\u91ca\u6027\u80fd\u5dee\u5f02\u3002\u672c\u6587\u65e8\u5728\u9a8c\u8bc1\u6570\u636e\u96c6\u8d28\u91cf\u662f\u5426\u4e3a\u72ec\u7acb\u4e8e\u8fd9\u4e9b\u56e0\u7d20\u7684\u56fa\u6709\u5c5e\u6027\u3002", "method": "\u751f\u6210\u4e86\u6570\u5343\u4e2a\u63a7\u5236\u5927\u5c0f\u548c\u7c7b\u522b\u5e73\u8861\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u591a\u79cd\u6a21\u578b\u67b6\u6784\uff08\u5982\u968f\u673a\u68ee\u6797\u3001\u652f\u6301\u5411\u91cf\u673a\u548c\u6df1\u5ea6\u7f51\u7edc\uff09\u8bad\u7ec3\u5206\u7c7b\u5668\uff0c\u5206\u6790\u6027\u80fd\u76f8\u5173\u6027\u3002", "result": "\u4e0d\u540c\u67b6\u6784\u7684\u5206\u7c7b\u5668\u5728\u76f8\u540c\u5b50\u96c6\u4e0a\u8868\u73b0\u9ad8\u5ea6\u76f8\u5173\uff08R\u00b2=0.79\uff09\uff0c\u8868\u660e\u6570\u636e\u96c6\u8d28\u91cf\u662f\u72ec\u7acb\u4e8e\u5176\u4ed6\u56e0\u7d20\u7684\u56fa\u6709\u5c5e\u6027\uff0c\u4e14\u4e0e\u6570\u636e\u96c6\u4e2d\u7c7b\u522b\u7684\u8d28\u91cf\u76f8\u5173\u3002", "conclusion": "\u6570\u636e\u96c6\u8d28\u91cf\u662f\u4f18\u5316\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u6027\u80fd\u7684\u72ec\u7acb\u56e0\u7d20\uff0c\u672a\u6765\u53ef\u9488\u5bf9\u5176\u8fdb\u884c\u9488\u5bf9\u6027\u4f18\u5316\u3002", "keywords": "\u6570\u636e\u96c6\u8d28\u91cf, \u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668, \u6a21\u578b\u67b6\u6784, \u7c7b\u522b\u5e73\u8861, \u6027\u80fd\u4f18\u5316"}}
{"id": "2505.22867", "pdf": "https://arxiv.org/pdf/2505.22867", "abs": "https://arxiv.org/abs/2505.22867", "authors": ["Iknoor Singh", "Carolina Scarton", "Kalina Bontcheva"], "title": "GateNLP at SemEval-2025 Task 10: Hierarchical Three-Step Prompting for Multilingual Narrative Classification", "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of online news and the increasing spread of misinformation\nnecessitate robust methods for automatic data analysis. Narrative\nclassification is emerging as a important task, since identifying what is being\nsaid online is critical for fact-checkers, policy markers and other\nprofessionals working on information studies. This paper presents our approach\nto SemEval 2025 Task 10 Subtask 2, which aims to classify news articles into a\npre-defined two-level taxonomy of main narratives and sub-narratives across\nmultiple languages.\n  We propose Hierarchical Three-Step Prompting (H3Prompt) for multilingual\nnarrative classification. Our methodology follows a three-step Large Language\nModel (LLM) prompting strategy, where the model first categorises an article\ninto one of two domains (Ukraine-Russia War or Climate Change), then identifies\nthe most relevant main narratives, and finally assigns sub-narratives. Our\napproach secured the top position on the English test set among 28 competing\nteams worldwide. The code is available at https://github.com/GateNLP/H3Prompt.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHierarchical Three-Step Prompting (H3Prompt)\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u8bed\u8a00\u65b0\u95fb\u53d9\u4e8b\u5206\u7c7b\uff0c\u5e76\u5728SemEval 2025 Task 10 Subtask 2\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6210\u7ee9\u3002", "motivation": "\u968f\u7740\u5728\u7ebf\u65b0\u95fb\u6570\u91cf\u7684\u589e\u52a0\u548c\u9519\u8bef\u4fe1\u606f\u7684\u4f20\u64ad\uff0c\u5f00\u53d1\u81ea\u52a8\u5316\u7684\u6570\u636e\u5206\u6790\u65b9\u6cd5\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002\u53d9\u4e8b\u5206\u7c7b\u662f\u5173\u952e\u4efb\u52a1\uff0c\u56e0\u4e3a\u5b83\u6709\u52a9\u4e8e\u8bc6\u522b\u5728\u7ebf\u5185\u5bb9\uff0c\u5bf9\u4e8b\u5b9e\u6838\u67e5\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u7b49\u4e13\u4e1a\u4eba\u58eb\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u5206\u5c42\u4e09\u6b65\u63d0\u793a\u7b56\u7565\uff08H3Prompt\uff09\uff0c\u9996\u5148\u5c06\u6587\u7ae0\u5206\u7c7b\u5230\u4e24\u4e2a\u9886\u57df\u4e4b\u4e00\uff08\u4e4c\u514b\u5170-\u4fc4\u7f57\u65af\u6218\u4e89\u6216\u6c14\u5019\u53d8\u5316\uff09\uff0c\u7136\u540e\u8bc6\u522b\u4e3b\u8981\u53d9\u4e8b\uff0c\u6700\u540e\u5206\u914d\u5b50\u53d9\u4e8b\u3002", "result": "\u8be5\u65b9\u6cd5\u572828\u4e2a\u5168\u7403\u7ade\u4e89\u56e2\u961f\u4e2d\uff0c\u5728\u82f1\u8bed\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e86\u7b2c\u4e00\u540d\u3002", "conclusion": "H3Prompt\u5728\u591a\u8bed\u8a00\u53d9\u4e8b\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "keywords": "\u53d9\u4e8b\u5206\u7c7b,\u591a\u8bed\u8a00\u5904\u7406,\u5927\u578b\u8bed\u8a00\u6a21\u578b,H3Prompt,SemEval"}}
{"id": "2505.23399", "pdf": "https://arxiv.org/pdf/2505.23399", "abs": "https://arxiv.org/abs/2505.23399", "authors": ["Jusheng Zhang", "Yijia Fan", "Wenjun Lin", "Ruiqi Chen", "Haoyi Jiang", "Wenhao Chai", "Jian Wang", "Keze Wang"], "title": "GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "We propose GAM-Agent, a game-theoretic multi-agent framework for enhancing\nvision-language reasoning. Unlike prior single-agent or monolithic models,\nGAM-Agent formulates the reasoning process as a non-zero-sum game between base\nagents--each specializing in visual perception subtasks--and a critical agent\nthat verifies logic consistency and factual correctness. Agents communicate via\nstructured claims, evidence, and uncertainty estimates. The framework\nintroduces an uncertainty-aware controller to dynamically adjust agent\ncollaboration, triggering multi-round debates when disagreement or ambiguity is\ndetected. This process yields more robust and interpretable predictions.\nExperiments on four challenging benchmarks--MMMU, MMBench, MVBench, and\nV*Bench--demonstrate that GAM-Agent significantly improves performance across\nvarious VLM backbones. Notably, GAM-Agent boosts the accuracy of small-to-mid\nscale models (e.g., Qwen2.5-VL-7B, InternVL3-14B) by 5--6\\%, and still enhances\nstrong models like GPT-4o by up to 2--3\\%. Our approach is modular, scalable,\nand generalizable, offering a path toward reliable and explainable multi-agent\nmultimodal reasoning.", "AI": {"tldr": "GAM-Agent\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u611f\u77e5\u4ee3\u7406\u548c\u903b\u8f91\u9a8c\u8bc1\u4ee3\u7406\u7684\u534f\u4f5c\uff0c\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u5355\u4ee3\u7406\u6216\u6574\u4f53\u6a21\u578b\u5728\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7f3a\u4e4f\u903b\u8f91\u4e00\u81f4\u6027\u548c\u4e8b\u5b9e\u6b63\u786e\u6027\u7684\u9a8c\u8bc1\u3002GAM-Agent\u65e8\u5728\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u4f5c\u548c\u52a8\u6001\u8c03\u6574\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "GAM-Agent\u5c06\u63a8\u7406\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u975e\u96f6\u548c\u535a\u5f08\uff0c\u5305\u62ec\u89c6\u89c9\u611f\u77e5\u5b50\u4efb\u52a1\u7684\u57fa\u4ee3\u7406\u548c\u9a8c\u8bc1\u903b\u8f91\u4e00\u81f4\u6027\u7684\u5173\u952e\u4ee3\u7406\u3002\u4ee3\u7406\u95f4\u901a\u8fc7\u7ed3\u6784\u5316\u58f0\u660e\u3001\u8bc1\u636e\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u8fdb\u884c\u901a\u4fe1\uff0c\u5e76\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u63a7\u5236\u5668\u52a8\u6001\u8c03\u6574\u534f\u4f5c\u3002", "result": "\u5728MMMU\u3001MMBench\u3001MVBench\u548cV*Bench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGAM-Agent\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002\u5c0f\u578b\u81f3\u4e2d\u578b\u6a21\u578b\u7684\u51c6\u786e\u7387\u63d0\u5347\u4e865-6%\uff0c\u5f3a\u529b\u6a21\u578b\u5982GPT-4o\u4e5f\u63d0\u5347\u4e862-3%\u3002", "conclusion": "GAM-Agent\u662f\u4e00\u79cd\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u4e3a\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u591a\u4ee3\u7406\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002", "keywords": "\u591a\u4ee3\u7406\u7cfb\u7edf; \u89c6\u89c9-\u8bed\u8a00\u63a8\u7406; \u535a\u5f08\u8bba; \u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1; \u53ef\u89e3\u91ca\u6027"}}
{"id": "2505.22820", "pdf": "https://arxiv.org/pdf/2505.22820", "abs": "https://arxiv.org/abs/2505.22820", "authors": ["Ayush Sawarni", "Sahasrajit Sarmasarkar", "Vasilis Syrgkanis"], "title": "Preference Learning with Response Time", "categories": ["cs.LG"], "comment": null, "summary": "This paper investigates the integration of response time data into human\npreference learning frameworks for more effective reward model elicitation.\nWhile binary preference data has become fundamental in fine-tuning foundation\nmodels, generative AI systems, and other large-scale models, the valuable\ntemporal information inherent in user decision-making remains largely\nunexploited. We propose novel methodologies to incorporate response time\ninformation alongside binary choice data, leveraging the Evidence Accumulation\nDrift Diffusion (EZ) model, under which response time is informative of the\npreference strength. We develop Neyman-orthogonal loss functions that achieve\noracle convergence rates for reward model learning, matching the theoretical\noptimal rates that would be attained if the expected response times for each\nquery were known a priori. Our theoretical analysis demonstrates that for\nlinear reward functions, conventional preference learning suffers from error\nrates that scale exponentially with reward magnitude. In contrast, our response\ntime-augmented approach reduces this to polynomial scaling, representing a\nsignificant improvement in sample efficiency. We extend these guarantees to\nnon-parametric reward function spaces, establishing convergence properties for\nmore complex, realistic reward models. Our extensive experiments validate our\ntheoretical findings in the context of preference learning over images.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u54cd\u5e94\u65f6\u95f4\u6570\u636e\u6574\u5408\u5230\u4eba\u7c7b\u504f\u597d\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u5956\u52b1\u6a21\u578b\u7684\u6709\u6548\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8c\u5143\u504f\u597d\u6570\u636e\uff0c\u800c\u5ffd\u7565\u4e86\u7528\u6237\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u65f6\u95f4\u4fe1\u606f\u3002\u901a\u8fc7\u7ed3\u5408\u54cd\u5e94\u65f6\u95f4\u548c\u4e8c\u5143\u9009\u62e9\u6570\u636e\uff0c\u5e76\u5229\u7528\u8bc1\u636e\u7d2f\u79ef\u6f02\u79fb\u6269\u6563\uff08EZ\uff09\u6a21\u578b\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u548c\u6a21\u578b\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u5f53\u524d\u5728\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u548c\u751f\u6210\u5f0fAI\u7cfb\u7edf\u4e2d\uff0c\u4e3b\u8981\u4f9d\u8d56\u4e8c\u5143\u504f\u597d\u6570\u636e\uff0c\u4f46\u7528\u6237\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u65f6\u95f4\u4fe1\u606f\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u8fd9\u4e9b\u65f6\u95f4\u6570\u636e\u53ef\u80fd\u5305\u542b\u504f\u597d\u5f3a\u5ea6\u7684\u5b9d\u8d35\u4fe1\u606f\uff0c\u56e0\u6b64\u63a2\u7d22\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u4ee5\u63d0\u9ad8\u5956\u52b1\u6a21\u578b\u7684\u6709\u6548\u6027\u662f\u672c\u6587\u7684\u6838\u5fc3\u52a8\u673a\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u54cd\u5e94\u65f6\u95f4\u4fe1\u606f\u4e0e\u4e8c\u5143\u9009\u62e9\u6570\u636e\u76f8\u7ed3\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u57fa\u4e8e\u8bc1\u636e\u7d2f\u79ef\u6f02\u79fb\u6269\u6563\uff08EZ\uff09\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u4e86Neyman\u6b63\u4ea4\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u5956\u52b1\u6a21\u578b\u5b66\u4e60\u7684\u7406\u8bba\u6700\u4f18\u6536\u655b\u901f\u5ea6\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5bf9\u4e8e\u7ebf\u6027\u5956\u52b1\u51fd\u6570\uff0c\u4f20\u7edf\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\u7684\u8bef\u5dee\u7387\u968f\u5956\u52b1\u5e45\u5ea6\u5448\u6307\u6570\u589e\u957f\uff0c\u800c\u672c\u6587\u63d0\u51fa\u7684\u54cd\u5e94\u65f6\u95f4\u589e\u5f3a\u65b9\u6cd5\u5c06\u8bef\u5dee\u7387\u964d\u81f3\u591a\u9879\u5f0f\u589e\u957f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u3002\u5b9e\u9a8c\u5728\u56fe\u50cf\u504f\u597d\u5b66\u4e60\u4e2d\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u7406\u8bba\u53d1\u73b0\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u54cd\u5e94\u65f6\u95f4\u6570\u636e\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5956\u52b1\u6a21\u578b\u7684\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u6216\u975e\u53c2\u6570\u5956\u52b1\u51fd\u6570\u7a7a\u95f4\u4e2d\u3002\u8fd9\u4e00\u65b9\u6cd5\u4e3a\u504f\u597d\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u6539\u8fdb\u7a7a\u95f4\u3002", "keywords": "\u54cd\u5e94\u65f6\u95f4, \u504f\u597d\u5b66\u4e60, \u5956\u52b1\u6a21\u578b, EZ\u6a21\u578b, \u6837\u672c\u6548\u7387"}}
{"id": "2505.22888", "pdf": "https://arxiv.org/pdf/2505.22888", "abs": "https://arxiv.org/abs/2505.22888", "authors": ["Jirui Qi", "Shan Chen", "Zidi Xiong", "Raquel Fern\u00e1ndez", "Danielle S. Bitterman", "Arianna Bisazza"], "title": "When Models Reason in Your Language: Controlling Thinking Trace Language Comes at the Cost of Accuracy", "categories": ["cs.CL"], "comment": null, "summary": "Recent Large Reasoning Models (LRMs) with thinking traces have shown strong\nperformance on English reasoning tasks. However, their ability to think in\nother languages is less studied. This capability is as important as answer\naccuracy for real world applications because users may find the reasoning trace\nuseful for oversight only when it is expressed in their own language. We\ncomprehensively evaluate two leading families of LRMs on our XReasoning\nbenchmark and find that even the most advanced models often revert to English\nor produce fragmented reasoning in other languages, revealing a substantial gap\nin multilingual reasoning. Prompt based interventions that force models to\nreason in the users language improve readability and oversight but reduce\nanswer accuracy, exposing an important trade off. We further show that targeted\npost training on just 100 examples mitigates this mismatch, though some\naccuracy loss remains. Our results highlight the limited multilingual reasoning\ncapabilities of current LRMs and outline directions for future work. Code and\ndata are available at https://github.com/Betswish/mCoT-XReasoning.", "AI": {"tldr": "\u5f53\u524d\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u591a\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5e38\u56de\u9000\u5230\u82f1\u8bed\u6216\u751f\u6210\u788e\u7247\u5316\u63a8\u7406\u3002\u901a\u8fc7\u63d0\u793a\u5e72\u9884\u548c\u5c11\u91cf\u6837\u672c\u5fae\u8c03\u53ef\u6539\u5584\u63a8\u7406\u53ef\u8bfb\u6027\uff0c\u4f46\u4f1a\u964d\u4f4e\u7b54\u6848\u51c6\u786e\u6027\u3002\u7814\u7a76\u63ed\u793a\u4e86\u591a\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76\u591a\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u6027\uff0c\u56e0\u4e3a\u7528\u6237\u9700\u8981\u4ee5\u6bcd\u8bed\u7406\u89e3\u63a8\u7406\u8fc7\u7a0b\u4ee5\u5b9e\u73b0\u6709\u6548\u76d1\u7763\uff0c\u800c\u73b0\u6709LRMs\u5728\u8fd9\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u5728XReasoning\u57fa\u51c6\u4e0a\u5168\u9762\u8bc4\u4f30\u4e24\u79cd\u4e3b\u6d41LRMs\uff0c\u5305\u62ec\u63d0\u793a\u5e72\u9884\u548c\u5c11\u91cf\u6837\u672c\u5fae\u8c03\u3002", "result": "\u53d1\u73b0\u5373\u4f7f\u662f\u5148\u8fdb\u6a21\u578b\u4e5f\u5e38\u56de\u9000\u5230\u82f1\u8bed\u6216\u751f\u6210\u788e\u7247\u5316\u63a8\u7406\uff0c\u63d0\u793a\u5e72\u9884\u6539\u5584\u53ef\u8bfb\u6027\u4f46\u964d\u4f4e\u51c6\u786e\u6027\uff0c\u5c11\u91cf\u5fae\u8c03\u53ef\u7f13\u89e3\u95ee\u9898\u4f46\u4ecd\u6709\u51c6\u786e\u6027\u635f\u5931\u3002", "conclusion": "\u5f53\u524dLRMs\u7684\u591a\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u6539\u8fdb\u3002", "keywords": "\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b, \u591a\u8bed\u8a00\u63a8\u7406, \u63a8\u7406\u53ef\u8bfb\u6027, \u51c6\u786e\u6027\u6743\u8861, \u5fae\u8c03"}}
{"id": "2505.23432", "pdf": "https://arxiv.org/pdf/2505.23432", "abs": "https://arxiv.org/abs/2505.23432", "authors": ["Elisa Celis", "Lingxiao Huang", "Nisheeth K. Vishnoi"], "title": "A Mathematical Framework for AI-Human Integration in Work", "categories": ["cs.AI", "cs.CY", "econ.GN", "q-fin.EC"], "comment": "This paper will appear in ICML 2025", "summary": "The rapid rise of Generative AI (GenAI) tools has sparked debate over their\nrole in complementing or replacing human workers across job contexts. We\npresent a mathematical framework that models jobs, workers, and worker-job fit,\nintroducing a novel decomposition of skills into decision-level and\naction-level subskills to reflect the complementary strengths of humans and\nGenAI. We analyze how changes in subskill abilities affect job success,\nidentifying conditions for sharp transitions in success probability. We also\nestablish sufficient conditions under which combining workers with\ncomplementary subskills significantly outperforms relying on a single worker.\nThis explains phenomena such as productivity compression, where GenAI\nassistance yields larger gains for lower-skilled workers. We demonstrate the\nframework' s practicality using data from O*NET and Big-Bench Lite, aligning\nreal-world data with our model via subskill-division methods. Our results\nhighlight when and how GenAI complements human skills, rather than replacing\nthem.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u6570\u5b66\u6a21\u578b\uff0c\u5206\u89e3\u6280\u80fd\u4e3a\u51b3\u7b56\u7ea7\u548c\u884c\u52a8\u7ea7\u5b50\u6280\u80fd\uff0c\u5206\u6790GenAI\u4e0e\u4eba\u7c7b\u5de5\u4f5c\u4e92\u8865\u6027\u7684\u6761\u4ef6\uff0c\u5c55\u793a\u5176\u5728\u5b9e\u9645\u6570\u636e\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u63a2\u8ba8GenAI\u5de5\u5177\u5728\u804c\u573a\u4e2d\u5982\u4f55\u4e0e\u4eba\u7c7b\u4e92\u8865\u800c\u975e\u66ff\u4ee3\uff0c\u5c24\u5176\u5728\u6280\u80fd\u5206\u89e3\u548c\u5408\u4f5c\u6761\u4ef6\u4e0b\u63d0\u5347\u5de5\u4f5c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u6570\u5b66\u6846\u67b6\uff0c\u5c06\u6280\u80fd\u5206\u4e3a\u51b3\u7b56\u7ea7\u548c\u884c\u52a8\u7ea7\u5b50\u6280\u80fd\uff0c\u7ed3\u5408O*NET\u548cBig-Bench Lite\u6570\u636e\u9a8c\u8bc1\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u5b50\u6280\u80fd\u80fd\u529b\u53d8\u5316\u5f71\u54cd\u5de5\u4f5c\u6210\u529f\u7387\uff0c\u63ed\u793a\u4e86\u4eba\u7c7b\u4e0eGenAI\u4e92\u8865\u5408\u4f5c\u663e\u8457\u4f18\u4e8e\u5355\u4e00\u5de5\u4f5c\u8005\u7684\u6761\u4ef6\u3002", "conclusion": "GenAI\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u80fd\u6709\u6548\u8865\u5145\u800c\u975e\u53d6\u4ee3\u4eba\u7c7b\u6280\u80fd\uff0c\u63d0\u5347\u4f4e\u6280\u80fd\u5de5\u4f5c\u8005\u7684\u751f\u4ea7\u6548\u7387\u3002", "keywords": "\u751f\u6210\u5f0fAI, \u6280\u80fd\u5206\u89e3, \u4eba\u673a\u534f\u4f5c, \u751f\u4ea7\u6548\u7387, \u6570\u5b66\u6a21\u578b"}}
{"id": "2505.22825", "pdf": "https://arxiv.org/pdf/2505.22825", "abs": "https://arxiv.org/abs/2505.22825", "authors": ["Michael Klamkin", "Mathieu Tanneau", "Pascal Van Hentenryck"], "title": "PGLearn -- An Open-Source Learning Toolkit for Optimal Power Flow", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "Machine Learning (ML) techniques for Optimal Power Flow (OPF) problems have\nrecently garnered significant attention, reflecting a broader trend of\nleveraging ML to approximate and/or accelerate the resolution of complex\noptimization problems. These developments are necessitated by the increased\nvolatility and scale in energy production for modern and future grids. However,\nprogress in ML for OPF is hindered by the lack of standardized datasets and\nevaluation metrics, from generating and solving OPF instances, to training and\nbenchmarking machine learning models. To address this challenge, this paper\nintroduces PGLearn, a comprehensive suite of standardized datasets and\nevaluation tools for ML and OPF. PGLearn provides datasets that are\nrepresentative of real-life operating conditions, by explicitly capturing both\nglobal and local variability in the data generation, and by, for the first\ntime, including time series data for several large-scale systems. In addition,\nit supports multiple OPF formulations, including AC, DC, and second-order cone\nformulations. Standardized datasets are made publicly available to democratize\naccess to this field, reduce the burden of data generation, and enable the fair\ncomparison of various methodologies. PGLearn also includes a robust toolkit for\ntraining, evaluating, and benchmarking machine learning models for OPF, with\nthe goal of standardizing performance evaluation across the field. By promoting\nopen, standardized datasets and evaluation metrics, PGLearn aims at\ndemocratizing and accelerating research and innovation in machine learning\napplications for optimal power flow problems. Datasets are available for\ndownload at https://www.huggingface.co/PGLearn.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86PGLearn\uff0c\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u548c\u6700\u4f18\u7535\u529b\u6f6e\u6d41\u95ee\u9898\u7684\u6807\u51c6\u5316\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5de5\u5177\u5957\u4ef6\uff0c\u65e8\u5728\u89e3\u51b3\u8be5\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u73b0\u4ee3\u7535\u7f51\u4e2d\u80fd\u6e90\u751f\u4ea7\u548c\u89c4\u6a21\u7684\u589e\u52a0\uff0c\u673a\u5668\u5b66\u4e60\u5728\u6700\u4f18\u7535\u529b\u6f6e\u6d41\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u6570\u636e\u6807\u51c6\u5316\u548c\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u8db3\u7684\u963b\u788d\uff0cPGLearn\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "PGLearn\u63d0\u4f9b\u4e86\u4ee3\u8868\u5b9e\u9645\u8fd0\u884c\u6761\u4ef6\u7684\u6570\u636e\u96c6\uff0c\u5305\u62ec\u5168\u5c40\u548c\u5c40\u90e8\u53d8\u5f02\u6027\uff0c\u5e76\u9996\u6b21\u5f15\u5165\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002\u652f\u6301\u591a\u79cd\u7535\u529b\u6f6e\u6d41\u516c\u5f0f\uff0c\u5e76\u63d0\u4f9b\u516c\u5f00\u6570\u636e\u96c6\u548c\u5de5\u5177\u5305\u3002", "result": "PGLearn\u901a\u8fc7\u516c\u5f00\u6807\u51c6\u548c\u5de5\u5177\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u521b\u65b0\uff0c\u4f7f\u6570\u636e\u751f\u6210\u548c\u6a21\u578b\u8bc4\u4f30\u66f4\u52a0\u9ad8\u6548\u548c\u516c\u5e73\u3002", "conclusion": "PGLearn\u6807\u51c6\u5316\u4e86\u673a\u5668\u5b66\u4e60\u548c\u6700\u4f18\u7535\u529b\u6f6e\u6d41\u95ee\u9898\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u52a0\u901f\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u6c11\u4e3b\u5316\u8bbf\u95ee\u3002", "keywords": "\u673a\u5668\u5b66\u4e60\uff0c\u6700\u4f18\u7535\u529b\u6f6e\u6d41\uff0c\u6807\u51c6\u5316\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u5de5\u5177"}}
{"id": "2505.22897", "pdf": "https://arxiv.org/pdf/2505.22897", "abs": "https://arxiv.org/abs/2505.22897", "authors": ["Chahat Raj", "Bowen Wei", "Aylin Caliskan", "Antonios Anastasopoulos", "Ziwei Zhu"], "title": "VIGNETTE: Socially Grounded Bias Evaluation for Vision-Language Models", "categories": ["cs.CL"], "comment": "17 pages", "summary": "While bias in large language models (LLMs) is well-studied, similar concerns\nin vision-language models (VLMs) have received comparatively less attention.\nExisting VLM bias studies often focus on portrait-style images and\ngender-occupation associations, overlooking broader and more complex social\nstereotypes and their implied harm. This work introduces VIGNETTE, a\nlarge-scale VQA benchmark with 30M+ images for evaluating bias in VLMs through\na question-answering framework spanning four directions: factuality,\nperception, stereotyping, and decision making. Beyond narrowly-centered\nstudies, we assess how VLMs interpret identities in contextualized settings,\nrevealing how models make trait and capability assumptions and exhibit patterns\nof discrimination. Drawing from social psychology, we examine how VLMs connect\nvisual identity cues to trait and role-based inferences, encoding social\nhierarchies, through biased selections. Our findings uncover subtle,\nmultifaceted, and surprising stereotypical patterns, offering insights into how\nVLMs construct social meaning from inputs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86VIGNETTE\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u504f\u89c1\uff0c\u6db5\u76d6\u4e8b\u5b9e\u6027\u3001\u611f\u77e5\u3001\u523b\u677f\u5370\u8c61\u548c\u51b3\u7b56\u56db\u4e2a\u65b9\u5411\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u8eab\u4efd\u8bc6\u522b\u7684\u793e\u4f1a\u523b\u677f\u5370\u8c61\u548c\u591a\u65b9\u9762\u504f\u89c1\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8096\u50cf\u56fe\u50cf\u548c\u6027\u522b\u804c\u4e1a\u5173\u8054\uff0c\u5ffd\u7565\u4e86\u66f4\u5e7f\u6cdb\u7684\u793e\u4f1a\u523b\u677f\u5370\u8c61\u53ca\u5176\u6f5c\u5728\u5371\u5bb3\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21VQA\u57fa\u51c6VIGNETTE\uff08\u542b3000\u4e07+\u56fe\u50cf\uff09\uff0c\u7ed3\u5408\u793e\u4f1a\u5fc3\u7406\u5b66\uff0c\u5206\u6790VLMs\u5982\u4f55\u57fa\u4e8e\u89c6\u89c9\u8eab\u4efd\u7ebf\u7d22\u63a8\u65ad\u7279\u6027\u548c\u89d2\u8272\u3002", "result": "\u7814\u7a76\u53d1\u73b0VLMs\u5b58\u5728\u5fae\u5999\u3001\u591a\u65b9\u9762\u7684\u523b\u677f\u5370\u8c61\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5982\u4f55\u4ece\u8f93\u5165\u4e2d\u6784\u5efa\u793e\u4f1a\u610f\u4e49\u3002", "conclusion": "VIGNETTE\u4e3a\u8bc4\u4f30VLMs\u504f\u89c1\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u793e\u4f1a\u523b\u677f\u5370\u8c61\u5728\u6a21\u578b\u4e2d\u7684\u4f53\u73b0\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u504f\u89c1\u8bc4\u4f30, \u793e\u4f1a\u523b\u677f\u5370\u8c61, VQA\u57fa\u51c6, \u793e\u4f1a\u5fc3\u7406\u5b66"}}
{"id": "2505.23436", "pdf": "https://arxiv.org/pdf/2505.23436", "abs": "https://arxiv.org/abs/2505.23436", "authors": ["Daniel Jarne Ornia", "Nicholas Bishop", "Joel Dyer", "Wei-Chen Lee", "Ani Calinescu", "Doyne Farme", "Michael Wooldridge"], "title": "Emergent Risk Awareness in Rational Agents under Resource Constraints", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Advanced reasoning models with agentic capabilities (AI agents) are deployed\nto interact with humans and to solve sequential decision-making problems under\n(approximate) utility functions and internal models. When such problems have\nresource or failure constraints where action sequences may be forcibly\nterminated once resources are exhausted, agents face implicit trade-offs that\nreshape their utility-driven (rational) behaviour. Additionally, since these\nagents are typically commissioned by a human principal to act on their behalf,\nasymmetries in constraint exposure can give rise to previously unanticipated\nmisalignment between human objectives and agent incentives. We formalise this\nsetting through a survival bandit framework, provide theoretical and empirical\nresults that quantify the impact of survival-driven preference shifts, identify\nconditions under which misalignment emerges and propose mechanisms to mitigate\nthe emergence of risk-seeking or risk-averse behaviours. As a result, this work\naims to increase understanding and interpretability of emergent behaviours of\nAI agents operating under such survival pressure, and offer guidelines for\nsafely deploying such AI systems in critical resource-limited environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u751f\u5b58\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\uff0c\u5f62\u5f0f\u5316\u4e86AI\u4ee3\u7406\u5728\u8d44\u6e90\u9650\u5236\u4e0b\u7684\u884c\u4e3a\u8f6c\u53d8\uff0c\u5206\u6790\u4e86\u5176\u5bf9\u4eba\u7c7b\u76ee\u6807\u7684\u6f5c\u5728\u504f\u79bb\uff0c\u5e76\u63d0\u51fa\u4e86\u7f13\u89e3\u673a\u5236\uff0c\u4ee5\u63d0\u9ad8\u5728\u5173\u952e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3AI\u4ee3\u7406\u5728\u8d44\u6e90\u6216\u5931\u8d25\u7ea6\u675f\u4e0b\u884c\u4e3a\u7684\u53d8\u5316\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u53d8\u5316\u5982\u4f55\u53ef\u80fd\u5bfc\u81f4\u4e0e\u4eba\u7c7b\u59d4\u6258\u4eba\u7684\u76ee\u6807\u4e0d\u4e00\u81f4\uff0c\u4ece\u800c\u8bbe\u8ba1\u66f4\u5b89\u5168\u7684\u90e8\u7f72\u673a\u5236\u3002", "method": "\u91c7\u7528\u751f\u5b58\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\uff0c\u7ed3\u5408\u7406\u8bba\u548c\u5b9e\u9a8c\u5206\u6790\uff0c\u91cf\u5316\u751f\u5b58\u9a71\u52a8\u7684\u504f\u597d\u8f6c\u53d8\uff0c\u5e76\u8bc6\u522b\u4e0d\u5bf9\u9f50\u7684\u6761\u4ef6\u3002", "result": "\u7ed3\u679c\u63ed\u793a\u4e86\u751f\u5b58\u538b\u529b\u4e0bAI\u4ee3\u7406\u53ef\u80fd\u51fa\u73b0\u7684\u98ce\u9669\u504f\u597d\u6216\u98ce\u9669\u89c4\u907f\u884c\u4e3a\uff0c\u5e76\u63d0\u51fa\u4e86\u7f13\u89e3\u8fd9\u4e9b\u884c\u4e3a\u7684\u673a\u5236\u3002", "conclusion": "\u7814\u7a76\u63d0\u9ad8\u4e86\u5bf9AI\u4ee3\u7406\u5728\u8d44\u6e90\u9650\u5236\u4e0b\u884c\u4e3a\u7684\u7406\u89e3\uff0c\u4e3a\u5173\u952e\u73af\u5883\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "keywords": "AI\u4ee3\u7406\u3001\u8d44\u6e90\u9650\u5236\u3001\u4e0d\u5bf9\u9f50\u3001\u98ce\u9669\u504f\u597d\u3001\u751f\u5b58\u591a\u81c2\u8001\u864e\u673a"}}
{"id": "2505.22829", "pdf": "https://arxiv.org/pdf/2505.22829", "abs": "https://arxiv.org/abs/2505.22829", "authors": ["Chenruo Liu", "Kenan Tang", "Yao Qin", "Qi Lei"], "title": "Bridging Distribution Shift and AI Safety: Conceptual and Methodological Synergies", "categories": ["cs.LG", "cs.AI"], "comment": "35 pages", "summary": "This paper bridges distribution shift and AI safety through a comprehensive\nanalysis of their conceptual and methodological synergies. While prior\ndiscussions often focus on narrow cases or informal analogies, we establish two\ntypes connections between specific causes of distribution shift and\nfine-grained AI safety issues: (1) methods addressing a specific shift type can\nhelp achieve corresponding safety goals, or (2) certain shifts and safety\nissues can be formally reduced to each other, enabling mutual adaptation of\ntheir methods. Our findings provide a unified perspective that encourages\nfundamental integration between distribution shift and AI safety research.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5206\u6790\u5206\u5e03\u504f\u79fb\u4e0eAI\u5b89\u5168\u7684\u8054\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e8c\u8005\u5728\u65b9\u6cd5\u8bba\u548c\u76ee\u6807\u4e0a\u7684\u534f\u540c\u5173\u7cfb\uff0c\u4e3a\u4e24\u8005\u7814\u7a76\u7684\u57fa\u672c\u6574\u5408\u63d0\u4f9b\u4e86\u7edf\u4e00\u89c6\u89d2\u3002", "motivation": "\u65e8\u5728\u586b\u8865\u5206\u5e03\u504f\u79fb\u4e0eAI\u5b89\u5168\u7814\u7a76\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u5b83\u4eec\u7684\u534f\u540c\u5173\u7cfb\uff0c\u8d85\u8d8a\u539f\u6709\u7814\u7a76\u7684\u72ed\u9698\u6848\u4f8b\u548c\u975e\u6b63\u5f0f\u7c7b\u6bd4\u3002", "method": "\u5efa\u7acb\u4e86\u4e24\u79cd\u8054\u7cfb\uff1a(1) \u9488\u5bf9\u7279\u5b9a\u504f\u79fb\u7c7b\u578b\u7684\u65b9\u6cd5\u6709\u52a9\u4e8e\u5b9e\u73b0\u5bf9\u5e94\u7684\u5b89\u5168\u76ee\u6807\uff1b(2) \u67d0\u4e9b\u504f\u79fb\u4e0e\u5b89\u5168\u95ee\u9898\u53ef\u5f62\u5f0f\u5316\u4e92\u7ea6\uff0c\u4ece\u800c\u4e92\u76f8\u9002\u914d\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e3a\u5206\u5e03\u504f\u79fb\u4e0eAI\u5b89\u5168\u7684\u6839\u672c\u6027\u6574\u5408\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u4fc3\u8fdb\u4e86\u4e8c\u8005\u7814\u7a76\u7684\u534f\u540c\u53d1\u5c55\u3002", "conclusion": "\u8bba\u6587\u6210\u529f\u5efa\u7acb\u4e86\u5206\u5e03\u504f\u79fb\u4e0eAI\u5b89\u5168\u7684\u53cc\u5411\u8054\u7cfb\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u65b9\u6cd5\u6307\u5bfc\u3002", "keywords": "\u5206\u5e03\u504f\u79fb\u3001AI\u5b89\u5168\u3001\u534f\u540c\u5173\u7cfb\u3001\u65b9\u6cd5\u8bba\u3001\u7814\u7a76\u6574\u5408"}}
{"id": "2505.22910", "pdf": "https://arxiv.org/pdf/2505.22910", "abs": "https://arxiv.org/abs/2505.22910", "authors": ["Chahat Raj", "Mahika Banerjee", "Aylin Caliskan", "Antonios Anastasopoulos", "Ziwei Zhu"], "title": "Talent or Luck? Evaluating Attribution Bias in Large Language Models", "categories": ["cs.CL"], "comment": "18 pages", "summary": "When a student fails an exam, do we tend to blame their effort or the test's\ndifficulty? Attribution, defined as how reasons are assigned to event outcomes,\nshapes perceptions, reinforces stereotypes, and influences decisions.\nAttribution Theory in social psychology explains how humans assign\nresponsibility for events using implicit cognition, attributing causes to\ninternal (e.g., effort, ability) or external (e.g., task difficulty, luck)\nfactors. LLMs' attribution of event outcomes based on demographics carries\nimportant fairness implications. Most works exploring social biases in LLMs\nfocus on surface-level associations or isolated stereotypes. This work proposes\na cognitively grounded bias evaluation framework to identify how models'\nreasoning disparities channelize biases toward demographic groups.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8ba4\u77e5\u57fa\u7840\u7684\u504f\u89c1\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522bLLMs\u5982\u4f55\u5f52\u56e0\u4e8b\u4ef6\u7ed3\u679c\uff0c\u5e76\u63a2\u8ba8\u8fd9\u79cd\u5f52\u56e0\u5982\u4f55\u5bfc\u81f4\u5bf9\u7279\u5b9a\u4eba\u53e3\u7fa4\u4f53\u7684\u504f\u89c1\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u5173\u6ce8LLMs\u7684\u8868\u5c42\u793e\u4f1a\u504f\u89c1\u6216\u5b64\u7acb\u523b\u677f\u5370\u8c61\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u5f52\u56e0\u63a8\u7406\u4f20\u9012\u504f\u89c1\u7684\u6df1\u5165\u5206\u6790\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u57fa\u7840\u7684\u504f\u89c1\u8bc4\u4f30\u6846\u67b6\uff0c\u5206\u6790LLMs\u5982\u4f55\u5c06\u4e8b\u4ef6\u7ed3\u679c\u5f52\u56e0\u4e8e\u5185\u90e8\u6216\u5916\u90e8\u56e0\u7d20\uff0c\u5e76\u63ed\u793a\u8fd9\u79cd\u5f52\u56e0\u5bf9\u4eba\u53e3\u7fa4\u4f53\u7684\u4e0d\u516c\u5e73\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u5f52\u56e0\u4e8b\u4ef6\u7ed3\u679c\u65f6\u5b58\u5728\u7684\u8ba4\u77e5\u504f\u5dee\uff0c\u5e76\u5c55\u793a\u4e86\u8fd9\u4e9b\u504f\u5dee\u5982\u4f55\u52a0\u5267\u5bf9\u7279\u5b9a\u4eba\u53e3\u7fa4\u4f53\u7684\u504f\u89c1\u3002", "conclusion": "\u901a\u8fc7\u8ba4\u77e5\u57fa\u7840\u7684\u8bc4\u4f30\u6846\u67b6\u53ef\u4ee5\u66f4\u6df1\u5165\u5730\u7406\u89e3LLMs\u7684\u504f\u89c1\u673a\u5236\uff0c\u4e3a\u672a\u6765\u516c\u5e73\u6027\u6539\u8fdb\u63d0\u4f9b\u65b9\u5411\u3002", "keywords": "\u5f52\u56e0\u7406\u8bba, LLMs, \u793e\u4f1a\u504f\u89c1, \u8ba4\u77e5\u57fa\u7840, \u516c\u5e73\u6027"}}
{"id": "2505.23473", "pdf": "https://arxiv.org/pdf/2505.23473", "abs": "https://arxiv.org/abs/2505.23473", "authors": ["Xiaorui Wu", "Xiaofeng Mao", "Fei Li", "Xin Zhang", "Xiaolu Zhang", "Jun Zhou", "Yuxiang Peng", "Li Zheng", "Chong Teng", "Donghong Ji", "Zhuang Li"], "title": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) frequently refuse to respond to pseudo-malicious\ninstructions: semantically harmless input queries triggering unnecessary LLM\nrefusals due to conservative safety alignment, significantly impairing user\nexperience. Collecting such instructions is crucial for evaluating and\nmitigating over-refusals, but existing instruction curation methods, like\nmanual creation or instruction rewriting, either lack scalability or fail to\nproduce sufficiently diverse and effective refusal-inducing prompts. To address\nthese limitations, we introduce EVOREFUSE, a prompt optimization approach that\ngenerates diverse pseudo-malicious instructions consistently eliciting\nconfident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm\nexploring the instruction space in more diverse directions than existing\nmethods via mutation strategies and recombination, and iteratively evolves seed\ninstructions to maximize evidence lower bound on LLM refusal probability. Using\nEVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582\npseudo-malicious instructions that outperforms the next-best benchmark with\n140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater\nlexical diversity, and 40.03% improved LLM response confidence scores; and\nEVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with\nresponses for supervised and preference-based alignment training.\nLLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to\n14.31% fewer over-refusals than models trained on the second-best alignment\ndataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals\nmodels trigger over-refusals by overly focusing on sensitive keywords while\nignoring broader context.", "AI": {"tldr": "EVOREFUSE\u662f\u4e00\u79cd\u901a\u8fc7\u8fdb\u5316\u7b97\u6cd5\u751f\u6210\u591a\u6837\u5316\u4f2a\u6076\u610f\u6307\u4ee4\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u56e0\u5b89\u5168\u5bf9\u9f50\u8fc7\u5ea6\u800c\u9891\u7e41\u62d2\u7edd\u56de\u7b54\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e24\u9879\u65b0\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u6d4b\u548c\u6821\u51c6\u3002", "motivation": "\u89e3\u51b3LLMs\u56e0\u4fdd\u5b88\u5b89\u5168\u5bf9\u9f50\u5bfc\u81f4\u5bf9\u65e0\u5bb3\u8f93\u5165\u8fc7\u5ea6\u62d2\u7edd\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u91c7\u7528\u8fdb\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u7a81\u53d8\u7b56\u7565\u548c\u91cd\u7ec4\u63a2\u7d22\u6307\u4ee4\u7a7a\u95f4\uff0c\u8fed\u4ee3\u4f18\u5316\u79cd\u5b50\u6307\u4ee4\u4ee5\u6700\u5927\u5316LLM\u62d2\u7edd\u6982\u7387\u7684\u8bc1\u636e\u4e0b\u754c\u3002", "result": "\u751f\u6210\u7684\u4e24\u9879\u6570\u636e\u96c6EVOREFUSE-TEST\u548cEVOREFUSE-ALIGN\u5728\u62d2\u7edd\u89e6\u53d1\u7387\u3001\u8bcd\u6c47\u591a\u6837\u6027\u548c\u6a21\u578b\u54cd\u5e94\u4fe1\u5fc3\u5206\u6570\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u4e14\u8bad\u7ec3\u540e\u7684LLAMA3.1-8B-INSTRUCT\u6a21\u578b\u51cf\u5c11\u4e8614.31%\u7684\u8fc7\u5ea6\u62d2\u7edd\u3002", "conclusion": "EVOREFUSE\u6709\u6548\u63d0\u5347\u4e86LLMs\u7684\u54cd\u5e94\u80fd\u529b\u548c\u591a\u6837\u6027\uff0c\u51cf\u5c11\u4e86\u8fc7\u5ea6\u62d2\u7edd\uff0c\u63ed\u793a\u4e86\u654f\u611f\u5173\u952e\u8bcd\u8fc7\u5ea6\u805a\u7126\u7684\u95ee\u9898\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u4f2a\u6076\u610f\u6307\u4ee4, \u8fdb\u5316\u7b97\u6cd5, \u5b89\u5168\u5bf9\u9f50, \u8fc7\u5ea6\u62d2\u7edd"}}
{"id": "2505.22839", "pdf": "https://arxiv.org/pdf/2505.22839", "abs": "https://arxiv.org/abs/2505.22839", "authors": ["Liu Yuezhang", "Xue-Xin Wei"], "title": "How Do Diffusion Models Improve Adversarial Robustness?", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 7 figures", "summary": "Recent findings suggest that diffusion models significantly enhance empirical\nadversarial robustness. While some intuitive explanations have been proposed,\nthe precise mechanisms underlying these improvements remain unclear. In this\nwork, we systematically investigate how and how well diffusion models improve\nadversarial robustness. First, we observe that diffusion models intriguingly\nincrease, rather than decrease, the $\\ell_p$ distance to clean\nsamples--challenging the intuition that purification denoises inputs closer to\nthe original data. Second, we find that the purified images are heavily\ninfluenced by the internal randomness of diffusion models, where a compression\neffect arises within each randomness configuration. Motivated by this\nobservation, we evaluate robustness under fixed randomness and find that the\nimprovement drops to approximately 24% on CIFAR-10--substantially lower than\nprior reports approaching 70%. Importantly, we show that this remaining\nrobustness gain strongly correlates with the model's ability to compress the\ninput space, revealing the compression rate as a reliable robustness indicator\nwithout requiring gradient-based analysis. Our findings provide novel insights\ninto the mechanisms underlying diffusion-based purification, and offer guidance\nfor developing more effective and principled adversarial purification systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6269\u6563\u6a21\u578b\u5982\u4f55\u63d0\u9ad8\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u5176\u5e76\u975e\u901a\u8fc7\u51cf\u5c11\u4e0e\u539f\u59cb\u6837\u672c\u7684\u8ddd\u79bb\uff0c\u800c\u662f\u4f9d\u8d56\u5185\u90e8\u968f\u673a\u6027\u548c\u538b\u7f29\u6548\u5e94\u3002\u5728\u56fa\u5b9a\u968f\u673a\u6027\u4e0b\uff0c\u9c81\u68d2\u6027\u63d0\u5347\u5927\u5e45\u4e0b\u964d\u81f324%\uff0c\u4e14\u5269\u4f59\u589e\u76ca\u4e0e\u8f93\u5165\u7684\u538b\u7f29\u7387\u76f8\u5173\u3002", "motivation": "\u63a2\u7a76\u6269\u6563\u6a21\u578b\u63d0\u5347\u5bf9\u6297\u9c81\u68d2\u6027\u7684\u5177\u4f53\u673a\u5236\uff0c\u6311\u6218\u5df2\u6709\u7684\u76f4\u89c2\u89e3\u91ca\uff08\u5982\u51c0\u5316\u4f7f\u8f93\u5165\u66f4\u63a5\u8fd1\u539f\u59cb\u6570\u636e\uff09\u3002", "method": "\u901a\u8fc7\u89c2\u5bdf\u6269\u6563\u6a21\u578b\u5bf9\u5e72\u51c0\u6837\u672c\u7684$\u2113_p$\u8ddd\u79bb\u53d8\u5316\u3001\u5206\u6790\u5185\u90e8\u968f\u673a\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u56fa\u5b9a\u968f\u673a\u6027\u4e0b\u8bc4\u4f30\u9c81\u68d2\u6027\u63d0\u5347\u3002", "result": "\u6269\u6563\u6a21\u578b\u7684\u9c81\u68d2\u6027\u63d0\u5347\u4f9d\u8d56\u4e8e\u968f\u673a\u6027\u914d\u7f6e\u7684\u538b\u7f29\u6548\u5e94\uff1b\u56fa\u5b9a\u968f\u673a\u6027\u540e\uff0cCIFAR-10\u4e0a\u7684\u63d0\u5347\u964d\u81f324%\uff0c\u5269\u4f59\u589e\u76ca\u4e0e\u538b\u7f29\u7387\u5f3a\u76f8\u5173\u3002", "conclusion": "\u63ed\u793a\u4e86\u6269\u6563\u51c0\u5316\u7684\u673a\u5236\uff08\u538b\u7f29\u6548\u5e94\uff09\uff0c\u63d0\u51fa\u538b\u7f29\u7387\u53ef\u4f5c\u4e3a\u9c81\u68d2\u6027\u6307\u6807\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u5bf9\u6297\u51c0\u5316\u7cfb\u7edf\u63d0\u4f9b\u6307\u5bfc\u3002", "keywords": "\u6269\u6563\u6a21\u578b, \u5bf9\u6297\u9c81\u68d2\u6027, \u51c0\u5316, \u538b\u7f29\u6548\u5e94, \u968f\u673a\u6027"}}
{"id": "2505.22919", "pdf": "https://arxiv.org/pdf/2505.22919", "abs": "https://arxiv.org/abs/2505.22919", "authors": ["Nikita Mehandru", "Niloufar Golchini", "David Bamman", "Travis Zack", "Melanie F. Molina", "Ahmed Alaa"], "title": "ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the Emergency Room", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have been extensively evaluated on medical\nquestion answering tasks based on licensing exams. However, real-world\nevaluations often depend on costly human annotators, and existing benchmarks\ntend to focus on isolated tasks that rarely capture the clinical reasoning or\nfull workflow underlying medical decisions. In this paper, we introduce\nER-Reason, a benchmark designed to evaluate LLM-based clinical reasoning and\ndecision-making in the emergency room (ER)--a high-stakes setting where\nclinicians make rapid, consequential decisions across diverse patient\npresentations and medical specialties under time pressure. ER-Reason includes\ndata from 3,984 patients, encompassing 25,174 de-identified longitudinal\nclinical notes spanning discharge summaries, progress notes, history and\nphysical exams, consults, echocardiography reports, imaging notes, and ER\nprovider documentation. The benchmark includes evaluation tasks that span key\nstages of the ER workflow: triage intake, initial assessment, treatment\nselection, disposition planning, and final diagnosis--each structured to\nreflect core clinical reasoning processes such as differential diagnosis via\nrule-out reasoning. We also collected 72 full physician-authored rationales\nexplaining reasoning processes that mimic the teaching process used in\nresidency training, and are typically absent from ER documentation. Evaluations\nof state-of-the-art LLMs on ER-Reason reveal a gap between LLM-generated and\nclinician-authored clinical reasoning for ER decisions, highlighting the need\nfor future research to bridge this divide.", "AI": {"tldr": "ER-Reason\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6025\u8bca\u5ba4\u4e34\u5e8a\u63a8\u7406\u548c\u51b3\u7b56\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u4e0e\u533b\u751f\u4e34\u5e8a\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u7684\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u548c\u5b64\u7acb\u7684\u6d4b\u8bd5\u4efb\u52a1\uff0c\u672a\u80fd\u5168\u9762\u53cd\u6620\u771f\u5b9e\u7684\u4e34\u5e8a\u63a8\u7406\u548c\u51b3\u7b56\u6d41\u7a0b\u3002", "method": "\u5229\u75283,984\u540d\u60a3\u8005\u768425,174\u4efd\u7eb5\u5411\u4e34\u5e8a\u7b14\u8bb0\uff0c\u8bbe\u8ba1\u4e86\u8986\u76d6\u6025\u8bca\u5de5\u4f5c\u6d41\u5173\u952e\u9636\u6bb5\u7684\u8bc4\u4f30\u4efb\u52a1\uff0c\u5e76\u6536\u96c6\u4e8672\u4efd\u533b\u751f\u64b0\u5199\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u533b\u751f\u4e34\u5e8a\u63a8\u7406\u5b58\u5728\u660e\u663e\u5dee\u8ddd\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u9700\u8981\u7f29\u5c0f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u771f\u5b9e\u4e34\u5e8a\u63a8\u7406\u7684\u5dee\u8ddd\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u6025\u8bca\u5ba4,\u4e34\u5e8a\u63a8\u7406,\u51b3\u7b56\u8bc4\u4f30,\u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2505.23474", "pdf": "https://arxiv.org/pdf/2505.23474", "abs": "https://arxiv.org/abs/2505.23474", "authors": ["Xiang Li", "Haiyang Yu", "Xinghua Zhang", "Ziyang Huang", "Shizhu He", "Kang Liu", "Jun Zhao", "Fei Huang", "Yongbin Li"], "title": "Socratic-PRMBench: Benchmarking Process Reward Models with Systematic Reasoning Patterns", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Process Reward Models (PRMs) are crucial in complex reasoning and\nproblem-solving tasks (e.g., LLM agents with long-horizon decision-making) by\nverifying the correctness of each intermediate reasoning step. In real-world\nscenarios, LLMs may apply various reasoning patterns (e.g., decomposition) to\nsolve a problem, potentially suffering from errors under various reasoning\npatterns. Therefore, PRMs are required to identify errors under various\nreasoning patterns during the reasoning process. However, existing benchmarks\nmainly focus on evaluating PRMs with stepwise correctness, ignoring a\nsystematic evaluation of PRMs under various reasoning patterns. To mitigate\nthis gap, we introduce Socratic-PRMBench, a new benchmark to evaluate PRMs\nsystematically under six reasoning patterns, including Transformation,\nDecomposition, Regather, Deduction, Verification, and Integration.\nSocratic-PRMBench}comprises 2995 reasoning paths with flaws within the\naforementioned six reasoning patterns. Through our experiments on both PRMs and\nLLMs prompted as critic models, we identify notable deficiencies in existing\nPRMs. These observations underscore the significant weakness of current PRMs in\nconducting evaluations on reasoning steps under various reasoning patterns. We\nhope Socratic-PRMBench can serve as a comprehensive testbed for systematic\nevaluation of PRMs under diverse reasoning patterns and pave the way for future\ndevelopment of PRMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Socratic-PRMBench\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u5728\u5404\u79cd\u63a8\u7406\u6a21\u5f0f\u4e0b\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(PRMs)\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709PRMs\u5728\u8fd9\u4e9b\u6a21\u5f0f\u4e0b\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709PRMs\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u9010\u6b65\u6b63\u786e\u6027\uff0c\u7f3a\u4e4f\u5bf9\u5404\u79cd\u63a8\u7406\u6a21\u5f0f\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u9650\u5236\u4e86PRMs\u5728\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165Socratic-PRMBench\uff0c\u5305\u542b2995\u6761\u7f3a\u9677\u63a8\u7406\u8def\u5f84\uff0c\u6db5\u76d6\u516d\u79cd\u63a8\u7406\u6a21\u5f0f\uff08\u5982\u8f6c\u6362\u3001\u5206\u89e3\u7b49\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30PRMs\u548cLLMs\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u73b0\u6709PRMs\u5728\u591a\u79cd\u63a8\u7406\u6a21\u5f0f\u4e0b\u7684\u8bc4\u4f30\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u8868\u660e\u5176\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "Socratic-PRMBench\u4e3aPRMs\u7684\u591a\u6837\u5316\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5168\u9762\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4e3a\u672a\u6765PRMs\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "keywords": "Process Reward Models, reasoning patterns, Socratic-PRMBench, evaluation benchmark"}}
{"id": "2505.22840", "pdf": "https://arxiv.org/pdf/2505.22840", "abs": "https://arxiv.org/abs/2505.22840", "authors": ["Dharambir Mahto", "Prashant Yadav", "Mahesh Banavar", "Jim Keany", "Alan T Joseph", "Srinivas Kilambi"], "title": "Development and Validation of SXI++ LNM Algorithm for Sepsis Prediction", "categories": ["cs.LG"], "comment": "Paper accepted at JMAI", "summary": "Sepsis is a life-threatening condition affecting over 48.9 million people\nglobally and causing 11 million deaths annually. Despite medical advancements,\npredicting sepsis remains a challenge due to non-specific symptoms and complex\npathophysiology. The SXI++ LNM is a machine learning scoring system that\nrefines sepsis prediction by leveraging multiple algorithms and deep neural\nnetworks. This study aims to improve robustness in clinical applications and\nevaluates the predictive performance of the SXI++ LNM for sepsis prediction.\nThe model, utilizing a deep neural network, was trained and tested using\nmultiple scenarios with different dataset distributions. The model's\nperformance was assessed against unseen test data, and accuracy, precision, and\narea under the curve (AUC) were calculated. THE SXI++ LNM outperformed the\nstate of the art in three use cases, achieving an AUC of 0.99 (95% CI:\n0.98-1.00). The model demonstrated a precision of 99.9% (95% CI: 99.8-100.0)\nand an accuracy of 99.99% (95% CI: 99.98-100.0), maintaining high reliability.", "AI": {"tldr": "SXI++ LNM\u6a21\u578b\u901a\u8fc7\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u7b97\u6cd5\u878d\u5408\u63d0\u5347\u8113\u6bd2\u75c7\u9884\u6d4b\u6027\u80fd\uff0c\u5728\u4e09\u4e2a\u7528\u4f8b\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u8113\u6bd2\u75c7\u662f\u5168\u7403\u6027\u81f4\u547d\u75be\u75c5\uff0c\u4f46\u9884\u6d4b\u56f0\u96be\u3002SXI++ LNM\u65e8\u5728\u63d0\u5347\u9884\u6d4b\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u4f7f\u7528\u4e0d\u540c\u5206\u5e03\u6570\u636e\u96c6\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u6307\u6807\u5305\u62ecAUC\u3001\u7cbe\u786e\u5ea6\u548c\u51c6\u786e\u7387\u3002", "result": "AUC\u8fbe0.99\uff0c\u7cbe\u786e\u5ea699.9%\uff0c\u51c6\u786e\u738799.99%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "SXI++ LNM\u5728\u8113\u6bd2\u75c7\u9884\u6d4b\u4e2d\u9ad8\u6548\u53ef\u9760\uff0c\u5177\u6709\u4e34\u5e8a\u6f5c\u529b\u3002", "keywords": "\u8113\u6bd2\u75c7\u9884\u6d4b,\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc,SXI++ LNM,\u673a\u5668\u5b66\u4e60"}}
{"id": "2505.22921", "pdf": "https://arxiv.org/pdf/2505.22921", "abs": "https://arxiv.org/abs/2505.22921", "authors": ["Yue Xing", "Tao Yang", "Yijiashun Qi", "Minggu Wei", "Yu Cheng", "Honghui Xin"], "title": "Structured Memory Mechanisms for Stable Context Representation in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the limitations of large language models in\nunderstanding long-term context. It proposes a model architecture equipped with\na long-term memory mechanism to improve the retention and retrieval of semantic\ninformation across paragraphs and dialogue turns. The model integrates explicit\nmemory units, gated writing mechanisms, and attention-based reading modules. A\nforgetting function is introduced to enable dynamic updates of memory content,\nenhancing the model's ability to manage historical information. To further\nimprove the effectiveness of memory operations, the study designs a joint\ntraining objective. This combines the main task loss with constraints on memory\nwriting and forgetting. It guides the model to learn better memory strategies\nduring task execution. Systematic evaluation across multiple subtasks shows\nthat the model achieves clear advantages in text generation consistency,\nstability in multi-turn question answering, and accuracy in cross-context\nreasoning. In particular, the model demonstrates strong semantic retention and\ncontextual coherence in long-text tasks and complex question answering\nscenarios. It effectively mitigates the context loss and semantic drift\nproblems commonly faced by traditional language models when handling long-term\ndependencies. The experiments also include analysis of different memory\nstructures, capacity sizes, and control strategies. These results further\nconfirm the critical role of memory mechanisms in language understanding. They\ndemonstrate the feasibility and effectiveness of the proposed approach in both\narchitectural design and performance outcomes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e26\u6709\u957f\u671f\u8bb0\u5fc6\u673a\u5236\u7684\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u663e\u5f0f\u8bb0\u5fc6\u5355\u5143\u3001\u95e8\u63a7\u5199\u5165\u673a\u5236\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8bfb\u53d6\u6a21\u5757\uff0c\u7ed3\u5408\u9057\u5fd8\u529f\u80fd\u52a8\u6001\u66f4\u65b0\u8bb0\u5fc6\u5185\u5bb9\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5904\u7406\u957f\u671f\u4f9d\u8d56\u65f6\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u957f\u671f\u4e0a\u4e0b\u6587\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5bb9\u6613\u4e22\u5931\u5386\u53f2\u4fe1\u606f\u6216\u51fa\u73b0\u8bed\u4e49\u6f02\u79fb\u3002", "method": "\u8bbe\u8ba1\u4e86\u96c6\u6210\u8bb0\u5fc6\u5355\u5143\u548c\u9057\u5fd8\u673a\u5236\u7684\u6a21\u578b\uff0c\u91c7\u7528\u8054\u5408\u8bad\u7ec3\u76ee\u6807\uff08\u4e3b\u4efb\u52a1\u635f\u5931\u4e0e\u8bb0\u5fc6\u8bfb\u5199\u7ea6\u675f\u7ed3\u5408\uff09\uff0c\u5e76\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e86\u4e0d\u540c\u8bb0\u5fc6\u7ed3\u6784\u7684\u6548\u679c\u3002", "result": "\u6a21\u578b\u5728\u6587\u672c\u751f\u6210\u4e00\u81f4\u6027\u3001\u591a\u8f6e\u95ee\u7b54\u7a33\u5b9a\u6027\u548c\u8de8\u4e0a\u4e0b\u6587\u63a8\u7406\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u957f\u6587\u672c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u8bed\u4e49\u4fdd\u7559\u548c\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8bb0\u5fc6\u673a\u5236\u5bf9\u8bed\u8a00\u7406\u89e3\u7684\u5173\u952e\u4f5c\u7528\uff0c\u8bc1\u660e\u4e86\u8be5\u67b6\u6784\u8bbe\u8ba1\u5728\u6027\u80fd\u548c\u53ef\u884c\u6027\u4e0a\u7684\u6709\u6548\u6027\u3002", "keywords": "\u957f\u671f\u8bb0\u5fc6\u673a\u5236\uff0c\u8bed\u4e49\u4fdd\u7559\uff0c\u52a8\u6001\u66f4\u65b0\uff0c\u8054\u5408\u8bad\u7ec3\uff0c\u8de8\u4e0a\u4e0b\u6587\u63a8\u7406"}}
{"id": "2505.23486", "pdf": "https://arxiv.org/pdf/2505.23486", "abs": "https://arxiv.org/abs/2505.23486", "authors": ["Ke Weng", "Lun Du", "Sirui Li", "Wangyue Lu", "Haozhe Sun", "Hengyu Liu", "Tiancheng Zhang"], "title": "Autoformalization in the Era of Large Language Models: A Survey", "categories": ["cs.AI"], "comment": null, "summary": "Autoformalization, the process of transforming informal mathematical\npropositions into verifiable formal representations, is a foundational task in\nautomated theorem proving, offering a new perspective on the use of mathematics\nin both theoretical and applied domains. Driven by the rapid progress in\nartificial intelligence, particularly large language models (LLMs), this field\nhas witnessed substantial growth, bringing both new opportunities and unique\nchallenges. In this survey, we provide a comprehensive overview of recent\nadvances in autoformalization from both mathematical and LLM-centric\nperspectives. We examine how autoformalization is applied across various\nmathematical domains and levels of difficulty, and analyze the end-to-end\nworkflow from data preprocessing to model design and evaluation. We further\nexplore the emerging role of autoformalization in enhancing the verifiability\nof LLM-generated outputs, highlighting its potential to improve both the\ntrustworthiness and reasoning capabilities of LLMs. Finally, we summarize key\nopen-source models and datasets supporting current research, and discuss open\nchallenges and promising future directions for the field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u63a2\u8ba8\u4e86\u5176\u5728\u6570\u5b66\u548cLLM\u9886\u57df\u7684\u5e94\u7528\u3001\u5de5\u4f5c\u6d41\u7a0b\u53ca\u5176\u5728\u63d0\u5347LLM\u8f93\u51fa\u53ef\u4fe1\u5ea6\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u603b\u7ed3\u4e86\u5f00\u6e90\u8d44\u6e90\u548c\u672a\u6765\u6311\u6218\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u81ea\u52a8\u5f62\u5f0f\u5316\u4f5c\u4e3a\u81ea\u52a8\u5316\u5b9a\u7406\u8bc1\u660e\u7684\u57fa\u7840\u4efb\u52a1\uff0c\u4e3a\u7406\u8bba\u548c\u5e94\u7528\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u540c\u65f6\u4e5f\u5e26\u6765\u4e86\u65b0\u7684\u673a\u9047\u548c\u6311\u6218\u3002", "method": "\u8bba\u6587\u4ece\u6570\u5b66\u548cLLM\u4e3a\u4e2d\u5fc3\u7684\u89d2\u5ea6\uff0c\u5168\u9762\u56de\u987e\u4e86\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u5206\u6790\u4e86\u4ece\u6570\u636e\u9884\u5904\u7406\u5230\u6a21\u578b\u8bbe\u8ba1\u548c\u8bc4\u4f30\u7684\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u81ea\u52a8\u5f62\u5f0f\u5316\u80fd\u591f\u589e\u5f3aLLM\u751f\u6210\u5185\u5bb9\u7684\u53ef\u9a8c\u8bc1\u6027\uff0c\u63d0\u5347\u5176\u53ef\u4fe1\u5ea6\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u652f\u6301\u5f53\u524d\u7814\u7a76\u7684\u5173\u952e\u5f00\u6e90\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u5e76\u8ba8\u8bba\u4e86\u8be5\u9886\u57df\u7684\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "keywords": "\u81ea\u52a8\u5f62\u5f0f\u5316\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u81ea\u52a8\u5316\u5b9a\u7406\u8bc1\u660e\uff0c\u53ef\u9a8c\u8bc1\u6027\uff0c\u5f00\u6e90\u8d44\u6e90"}}
{"id": "2505.22841", "pdf": "https://arxiv.org/pdf/2505.22841", "abs": "https://arxiv.org/abs/2505.22841", "authors": ["Franck Gabriel", "Fran\u00e7ois Ged", "Maria Han Veiga", "Emmanuel Schertzer"], "title": "Kernel-Smoothed Scores for Denoising Diffusion: A Bias-Variance Study", "categories": ["cs.LG", "math.PR", "stat.ML", "G.3; I.2.6"], "comment": null, "summary": "Diffusion models now set the benchmark in high-fidelity generative sampling,\nyet they can, in principle, be prone to memorization. In this case, their\nlearned score overfits the finite dataset so that the reverse-time SDE samples\nare mostly training points. In this paper, we interpret the empirical score as\na noisy version of the true score and show that its covariance matrix is\nasymptotically a re-weighted data PCA. In large dimension, the small time limit\nmakes the noise variance blow up while simultaneously reducing spatial\ncorrelation. To reduce this variance, we introduce a kernel-smoothed empirical\nscore and analyze its bias-variance trade-off. We derive asymptotic bounds on\nthe Kullback-Leibler divergence between the true distribution and the one\ngenerated by the modified reverse SDE. Regularization on the score has the same\neffect as increasing the size of the training dataset, and thus helps prevent\nmemorization. A spectral decomposition of the forward diffusion suggests better\nvariance control under some regularity conditions of the true data\ndistribution. Reverse diffusion with kernel-smoothed empirical score can be\nreformulated as a gradient descent drifted toward a Log-Exponential\nDouble-Kernel Density Estimator (LED-KDE). This perspective highlights two\nregularization mechanisms taking place in denoising diffusions: an initial\nGaussian kernel first diffuses mass isotropically in the ambient space, while a\nsecond kernel applied in score space concentrates and spreads that mass along\nthe data manifold. Hence, even a straightforward regularization-without any\nlearning-already mitigates memorization and enhances generalization.\nNumerically, we illustrate our results with several experiments on synthetic\nand MNIST datasets.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u6838\u5e73\u6ed1\u7ecf\u9a8c\u5206\u6570\u6765\u51cf\u5c11\u6269\u6563\u6a21\u578b\u4e2d\u7684\u65b9\u5dee\uff0c\u9632\u6b62\u8fc7\u62df\u5408\u548c\u8bb0\u5fc6\u73b0\u8c61\uff0c\u4ece\u800c\u63d0\u9ad8\u751f\u6210\u6837\u672c\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u4fdd\u771f\u6837\u672c\u65f6\u53ef\u80fd\u8fc7\u62df\u5408\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u53cd\u5411SDE\u91c7\u6837\u7ed3\u679c\u591a\u4e3a\u8bad\u7ec3\u70b9\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u65b9\u5dee\u63a7\u5236\u548c\u6b63\u5219\u5316\u6539\u8fdb\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u5f15\u5165\u6838\u5e73\u6ed1\u7ecf\u9a8c\u5206\u6570\u5206\u6790\u5176\u504f\u5dee-\u65b9\u5dee\u6743\u8861\uff0c\u63a8\u5bfcKL\u6563\u5ea6\u6e10\u8fd1\u754c\uff0c\u5e76\u57fa\u4e8e\u524d\u5411\u6269\u6563\u7684\u8c31\u5206\u89e3\u4f18\u5316\u65b9\u5dee\u63a7\u5236\u3002", "result": "\u6838\u5e73\u6ed1\u7ecf\u9a8c\u5206\u6570\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u6f02\u79fb\u5b9e\u73b0\uff0c\u7c7b\u4f3cLED-KDE\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u7b80\u5355\u7684\u5206\u6570\u6b63\u5219\u5316\u65e0\u9700\u989d\u5916\u5b66\u4e60\u5373\u53ef\u7f13\u89e3\u8bb0\u5fc6\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "keywords": "\u6269\u6563\u6a21\u578b, \u8bb0\u5fc6\u73b0\u8c61, \u6838\u5e73\u6ed1, \u65b9\u5dee\u63a7\u5236, \u6cdb\u5316\u6027"}}
{"id": "2505.22934", "pdf": "https://arxiv.org/pdf/2505.22934", "abs": "https://arxiv.org/abs/2505.22934", "authors": ["Haobo Zhang", "Jiayu Zhou"], "title": "Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages, 5 figures, 16 tables, accepted by ACL 2025", "summary": "Fine-tuning large language models (LMs) for individual tasks yields strong\nperformance but is expensive for deployment and storage. Recent works explore\nmodel merging to combine multiple task-specific models into a single multi-task\nmodel without additional training. However, existing merging methods often fail\nfor models fine-tuned with low-rank adaptation (LoRA), due to significant\nperformance degradation. In this paper, we show that this issue arises from a\npreviously overlooked interplay between model parameters and data\ndistributions. We propose Orthogonal Subspaces for Robust model Merging (OSRM)\nto constrain the LoRA subspace *prior* to fine-tuning, ensuring that updates\nrelevant to one task do not adversely shift outputs for others. Our approach\ncan seamlessly integrate with most existing merging algorithms, reducing the\nunintended interference among tasks. Extensive experiments on eight datasets,\ntested with three widely used LMs and two large LMs, demonstrate that our\nmethod not only boosts merging performance but also preserves single-task\naccuracy. Furthermore, our approach exhibits greater robustness to the\nhyperparameters of merging. These results highlight the importance of\ndata-parameter interaction in model merging and offer a plug-and-play solution\nfor merging LoRA models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86OSRM\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675fLoRA\u5b50\u7a7a\u95f4\u63d0\u5347\u6a21\u578b\u5408\u5e76\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u6a21\u578b\u5408\u5e76\u65f6\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5408\u5e76LoRA\u5fae\u8c03\u6a21\u578b\u65f6\u6027\u80fd\u4e0b\u964d\u660e\u663e\uff0c\u4e3b\u8981\u56e0\u4e3a\u53c2\u6570\u4e0e\u6570\u636e\u5206\u5e03\u7684\u4ea4\u4e92\u672a\u88ab\u5145\u5206\u8003\u8651\u3002", "method": "\u63d0\u51faOSRM\u65b9\u6cd5\uff0c\u9884\u5148\u7ea6\u675fLoRA\u5b50\u7a7a\u95f4\u4ee5\u907f\u514d\u4efb\u52a1\u95f4\u5e72\u6270\uff0c\u5e76\u4e0e\u73b0\u6709\u5408\u5e76\u7b97\u6cd5\u517c\u5bb9\u3002", "result": "\u5728\u516b\u4e2a\u6570\u636e\u96c6\u548c\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0cOSRM\u663e\u8457\u63d0\u5347\u5408\u5e76\u6027\u80fd\u5e76\u4fdd\u6301\u5355\u4efb\u52a1\u51c6\u786e\u6027\u3002", "conclusion": "\u6570\u636e\u4e0e\u53c2\u6570\u4ea4\u4e92\u5bf9\u6a21\u578b\u5408\u5e76\u81f3\u5173\u91cd\u8981\uff0cOSRM\u4e3aLoRA\u6a21\u578b\u5408\u5e76\u63d0\u4f9b\u4e86\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u6a21\u578b\u5408\u5e76, LoRA, \u591a\u4efb\u52a1\u5b66\u4e60, \u4f4e\u79e9\u9002\u5e94, OSRM"}}
{"id": "2505.23518", "pdf": "https://arxiv.org/pdf/2505.23518", "abs": "https://arxiv.org/abs/2505.23518", "authors": ["Hangoo Kang", "Jehyeok Yeon", "Gagandeep Singh"], "title": "TRAP: Targeted Redirecting of Agentic Preferences", "categories": ["cs.AI"], "comment": null, "summary": "Autonomous agentic AI systems powered by vision-language models (VLMs) are\nrapidly advancing toward real-world deployment, yet their cross-modal reasoning\ncapabilities introduce new attack surfaces for adversarial manipulation that\nexploit semantic reasoning across modalities. Existing adversarial attacks\ntypically rely on visible pixel perturbations or require privileged model or\nenvironment access, making them impractical for stealthy, real-world\nexploitation. We introduce TRAP, a generative adversarial framework that\nmanipulates the agent's decision-making using diffusion-based semantic\ninjections. Our method combines negative prompt-based degradation with positive\nsemantic optimization, guided by a Siamese semantic network and layout-aware\nspatial masking. Without requiring access to model internals, TRAP produces\nvisually natural images yet induces consistent selection biases in agentic AI\nsystems. We evaluate TRAP on the Microsoft Common Objects in Context (COCO)\ndataset, building multi-candidate decision scenarios. Across these scenarios,\nTRAP achieves a 100% attack success rate on leading models, including\nLLaVA-34B, Gemma3, and Mistral-3.1, significantly outperforming baselines such\nas SPSA, Bandit, and standard diffusion approaches. These results expose a\ncritical vulnerability: Autonomous agents can be consistently misled through\nhuman-imperceptible cross-modal manipulations. These findings highlight the\nneed for defense strategies beyond pixel-level robustness to address semantic\nvulnerabilities in cross-modal decision-making.", "AI": {"tldr": "TRAP\u662f\u4e00\u79cd\u751f\u6210\u5bf9\u6297\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u5f0f\u8bed\u4e49\u6ce8\u5165\u64cd\u7eb5\u81ea\u4e3b\u4ee3\u7406AI\u7684\u51b3\u7b56\uff0c\u65e0\u9700\u83b7\u53d6\u6a21\u578b\u5185\u90e8\u4fe1\u606f\u5373\u53ef\u5b9e\u73b0100%\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u4ee3\u7406AI\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\u5f15\u5165\u4e86\u65b0\u7684\u5bf9\u6297\u653b\u51fb\u9762\uff0c\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u53ef\u89c1\u50cf\u7d20\u6270\u52a8\u6216\u7279\u6743\u8bbf\u95ee\uff0c\u5b9e\u7528\u6027\u8f83\u4f4e\u3002", "method": "\u7ed3\u5408\u8d1f\u63d0\u793a\u9000\u5316\u548c\u6b63\u8bed\u4e49\u4f18\u5316\u7684\u6269\u6563\u5f0f\u8bed\u4e49\u6ce8\u5165\uff0c\u8f85\u4ee5Siamese\u8bed\u4e49\u7f51\u7edc\u548c\u5e03\u5c40\u611f\u77e5\u7a7a\u95f4\u63a9\u7801\u3002", "result": "\u5728COCO\u6570\u636e\u96c6\u4e0a\uff0cTRAP\u5bf9LLaVA-34B\u7b49\u6a21\u578b\u5b9e\u73b0100%\u653b\u51fb\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\u8de8\u6a21\u6001\u8bed\u4e49\u6f0f\u6d1e\u7684\u5b58\u5728\uff0c\u9700\u5f00\u53d1\u8d85\u8d8a\u50cf\u7d20\u7ea7\u9c81\u68d2\u6027\u7684\u9632\u5fa1\u7b56\u7565\u3002", "keywords": "\u81ea\u4e3b\u4ee3\u7406AI\u3001\u8de8\u6a21\u6001\u653b\u51fb\u3001\u8bed\u4e49\u6ce8\u5165\u3001\u6269\u6563\u6a21\u578b\u3001\u5bf9\u6297\u6837\u672c"}}
{"id": "2505.22846", "pdf": "https://arxiv.org/pdf/2505.22846", "abs": "https://arxiv.org/abs/2505.22846", "authors": ["Nikita Khramov", "Andrei Kozyrev", "Gleb Solovev", "Anton Podkopaev"], "title": "RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for Rocq generation", "categories": ["cs.LG", "cs.AI", "cs.LO", "cs.SE"], "comment": null, "summary": "Interactive Theorem Proving was repeatedly shown to be fruitful combined with\nGenerative Artificial Intelligence. This paper assesses multiple approaches to\nRocq generation and illuminates potential avenues for improvement. We highlight\nthe importance of thorough premise selection for generating Rocq proofs and\npropose a novel approach, leveraging retrieval via a self-attentive embedder\nmodel. The evaluation of the designed approach shows up to 28% relative\nincrease of the generator's performance. We tackle the problem of writing Rocq\nproofs using a multi-stage agentic system, tailored for formal verification,\nand demonstrate its high effectiveness. We conduct an ablation study and show\nthe use of multi-agent debate on the planning stage of proof synthesis.", "AI": {"tldr": "\u7ed3\u5408\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u4ea4\u4e92\u5f0f\u5b9a\u7406\u8bc1\u660e\u6548\u679c\u663e\u8457\uff0c\u672c\u6587\u8bc4\u4f30\u4e86\u591a\u79cdRocq\u751f\u6210\u65b9\u6cd5\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u5411\u3002\u91cd\u70b9\u5728\u4e8e\u6539\u8fdb\u524d\u63d0\u9009\u62e9\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u5d4c\u5165\u6a21\u578b\u7684\u68c0\u7d22\u65b0\u65b9\u6cd5\uff0c\u751f\u6210\u5668\u6027\u80fd\u63d0\u534728%\u3002\u91c7\u7528\u591a\u9636\u6bb5\u4ee3\u7406\u7cfb\u7edf\u5904\u7406Rocq\u8bc1\u660e\u95ee\u9898\uff0c\u8bc1\u660e\u5176\u9ad8\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u591a\u4ee3\u7406\u8fa9\u8bba\u5728\u8bc1\u660e\u5408\u6210\u89c4\u5212\u9636\u6bb5\u7684\u6548\u7528\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u4e0e\u4ea4\u4e92\u5f0f\u5b9a\u7406\u8bc1\u660e\u7684\u7ed3\u5408\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u63d0\u5347Rocq\u8bc1\u660e\u751f\u6210\u6548\u7387\u4e0e\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u5d4c\u5165\u6a21\u578b\u7684\u68c0\u7d22\u65b9\u6cd5\u6539\u8fdb\u524d\u63d0\u9009\u62e9\uff0c\u5e76\u8bbe\u8ba1\u591a\u9636\u6bb5\u4ee3\u7406\u7cfb\u7edf\u7528\u4e8eRocq\u8bc1\u660e\u751f\u6210\uff0c\u7ed3\u5408\u591a\u4ee3\u7406\u8fa9\u8bba\u4f18\u5316\u89c4\u5212\u9636\u6bb5\u3002", "result": "\u65b0\u65b9\u6cd5\u4f7f\u751f\u6210\u5668\u6027\u80fd\u76f8\u5bf9\u63d0\u534728%\uff0c\u591a\u9636\u6bb5\u4ee3\u7406\u7cfb\u7edf\u5728\u5b9e\u9645\u9a8c\u8bc1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u3002", "conclusion": "\u81ea\u6ce8\u610f\u529b\u68c0\u7d22\u53ca\u591a\u4ee3\u7406\u7cfb\u7edf\u663e\u8457\u63d0\u5347Rocq\u8bc1\u660e\u751f\u6210\u6548\u679c\uff0c\u591a\u4ee3\u7406\u8fa9\u8bba\u5bf9\u89c4\u5212\u9636\u6bb5\u6709\u79ef\u6781\u5f71\u54cd\u3002", "keywords": "\u4ea4\u4e92\u5f0f\u5b9a\u7406\u8bc1\u660e\uff0c\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff0cRocq\u8bc1\u660e\uff0c\u81ea\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u591a\u4ee3\u7406\u7cfb\u7edf"}}
{"id": "2505.22937", "pdf": "https://arxiv.org/pdf/2505.22937", "abs": "https://arxiv.org/abs/2505.22937", "authors": ["Ngeyen Yinkfu"], "title": "Improving QA Efficiency with DistilBERT: Fine-Tuning and Inference on mobile Intel CPUs", "categories": ["cs.CL"], "comment": "This paper presents an efficient transformer-based question-answering\n  model optimized for inference on a 13th Gen Intel i7 CPU. The proposed\n  approach balances performance and computational efficiency, making it\n  suitable for real-time applications on resource-constrained devices. Code for\n  this paper is available upon request via email at nyinkfu@andrew.cmu.edu", "summary": "This study presents an efficient transformer-based question-answering (QA)\nmodel optimized for deployment on a 13th Gen Intel i7-1355U CPU, using the\nStanford Question Answering Dataset (SQuAD) v1.1. Leveraging exploratory data\nanalysis, data augmentation, and fine-tuning of a DistilBERT architecture, the\nmodel achieves a validation F1 score of 0.6536 with an average inference time\nof 0.1208 seconds per question. Compared to a rule-based baseline (F1: 0.3124)\nand full BERT-based models, our approach offers a favorable trade-off between\naccuracy and computational efficiency. This makes it well-suited for real-time\napplications on resource-constrained systems. The study includes systematic\nevaluation of data augmentation strategies and hyperparameter configurations,\nproviding practical insights into optimizing transformer models for CPU-based\ninference.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u9ad8\u6548\u95ee\u7b54\u6a21\u578b\uff0c\u9488\u5bf9\u7b2c13\u4ee3Intel i7-1355U CPU\u4f18\u5316\uff0c\u4f7f\u7528SQuAD v1.1\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1F1\u5206\u6570\u4e3a0.6536\uff0c\u5e73\u5747\u63a8\u7406\u65f6\u95f40.1208\u79d2\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684CPU\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u5b9e\u65f6\u95ee\u7b54\u5e94\u7528\uff0c\u9700\u8981\u5e73\u8861\u6a21\u578b\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u91c7\u7528DistilBERT\u67b6\u6784\uff0c\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u548c\u5fae\u8c03\uff0c\u63a2\u7d22\u8d85\u53c2\u6570\u914d\u7f6e\u3002", "result": "\u6a21\u578bF1\u5206\u65700.6536\uff0c\u63a8\u7406\u65f6\u95f40.1208\u79d2/\u95ee\u9898\uff0c\u4f18\u4e8e\u89c4\u5219\u57fa\u7ebf\uff08F1: 0.3124\uff09\u548c\u5b8c\u6574BERT\u6a21\u578b\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u7cbe\u5ea6\u4e0e\u6548\u7387\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u9002\u7528\u4e8eCPU\u90e8\u7f72\u7684\u5b9e\u65f6\u5e94\u7528\u3002", "keywords": "Transformer, DistilBERT, SQuAD, \u6570\u636e\u589e\u5f3a, CPU\u4f18\u5316"}}
{"id": "2505.23519", "pdf": "https://arxiv.org/pdf/2505.23519", "abs": "https://arxiv.org/abs/2505.23519", "authors": ["Ruiqi He", "Falk Lieder"], "title": "Individual differences in the cognitive mechanisms of planning strategy discovery", "categories": ["cs.AI"], "comment": null, "summary": "People employ efficient planning strategies. But how are these strategies\nacquired? Previous research suggests that people can discover new planning\nstrategies through learning from reinforcements, a process known as\nmetacognitive reinforcement learning (MCRL). While prior work has shown that\nMCRL models can learn new planning strategies and explain more participants'\nexperience-driven discovery better than alternative mechanisms, it also\nrevealed significant individual differences in metacognitive learning.\nFurthermore, when fitted to human data, these models exhibit a slower rate of\nstrategy discovery than humans. In this study, we investigate whether\nincorporating cognitive mechanisms that might facilitate human strategy\ndiscovery can bring models of MCRL closer to human performance. Specifically,\nwe consider intrinsically generated metacognitive pseudo-rewards, subjective\neffort valuation, and termination deliberation. Analysis of planning task data\nshows that a larger proportion of participants used at least one of these\nmechanisms, with significant individual differences in their usage and varying\nimpacts on strategy discovery. Metacognitive pseudo-rewards, subjective effort\nvaluation, and learning the value of acting without further planning were found\nto facilitate strategy discovery. While these enhancements provided valuable\ninsights into individual differences and the effect of these mechanisms on\nstrategy discovery, they did not fully close the gap between model and human\nperformance, prompting further exploration of additional factors that people\nmight use to discover new planning strategies.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u5f15\u5165\u8ba4\u77e5\u673a\u5236\uff08\u5982\u5143\u8ba4\u77e5\u4f2a\u5956\u52b1\u3001\u4e3b\u89c2\u52aa\u529b\u8bc4\u4f30\u548c\u7ec8\u6b62\u601d\u8003\uff09\u6765\u4f18\u5316\u5143\u8ba4\u77e5\u5f3a\u5316\u5b66\u4e60\uff08MCRL\uff09\uff0c\u4ee5\u7f29\u5c0f\u6a21\u578b\u4e0e\u4eba\u7c7b\u5728\u7b56\u7565\u53d1\u73b0\u4e0a\u7684\u5dee\u8ddd\u3002\u867d\u7136\u8fd9\u4e9b\u673a\u5236\u6709\u52a9\u4e8e\u7b56\u7565\u53d1\u73b0\uff0c\u4f46\u672a\u80fd\u5b8c\u5168\u5f25\u5408\u5dee\u8ddd\uff0c\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u4ed6\u56e0\u7d20\u3002", "motivation": "\u73b0\u6709MCRL\u6a21\u578b\u5728\u7b56\u7565\u53d1\u73b0\u4e0a\u6bd4\u4eba\u7c7b\u6162\uff0c\u4e14\u5b58\u5728\u4e2a\u4f53\u5dee\u5f02\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u8ba4\u77e5\u673a\u5236\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u4eba\u7c7b\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5206\u6790\u89c4\u5212\u4efb\u52a1\u6570\u636e\uff0c\u7814\u7a76\u5f15\u5165\u5143\u8ba4\u77e5\u4f2a\u5956\u52b1\u3001\u4e3b\u89c2\u52aa\u529b\u8bc4\u4f30\u548c\u7ec8\u6b62\u601d\u8003\u4e09\u79cd\u673a\u5236\uff0c\u8bc4\u4f30\u5b83\u4eec\u5bf9\u7b56\u7565\u53d1\u73b0\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u8fd9\u4e9b\u673a\u5236\u6709\u52a9\u4e8e\u7b56\u7565\u53d1\u73b0\uff0c\u4f46\u672a\u80fd\u5b8c\u5168\u5f25\u5408\u6a21\u578b\u4e0e\u4eba\u7c7b\u7684\u5dee\u8ddd\u3002\u4e0d\u540c\u4e2a\u4f53\u7684\u673a\u5236\u4f7f\u7528\u548c\u6548\u679c\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "\u867d\u7136\u8fd9\u4e9b\u673a\u5236\u63d0\u4f9b\u4e86\u5bf9\u4e2a\u4f53\u5dee\u5f02\u548c\u7b56\u7565\u53d1\u73b0\u7684\u6df1\u5165\u7406\u89e3\uff0c\u4f46\u4ecd\u9700\u63a2\u7d22\u5176\u4ed6\u56e0\u7d20\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316MCRL\u6a21\u578b\u3002", "keywords": "\u5143\u8ba4\u77e5\u5f3a\u5316\u5b66\u4e60\u3001\u7b56\u7565\u53d1\u73b0\u3001\u8ba4\u77e5\u673a\u5236\u3001\u4e2a\u4f53\u5dee\u5f02\u3001\u89c4\u5212\u4efb\u52a1"}}
{"id": "2505.22861", "pdf": "https://arxiv.org/pdf/2505.22861", "abs": "https://arxiv.org/abs/2505.22861", "authors": ["Carlota Par\u00e9s-Morlans", "Michelle Yi", "Claire Chen", "Sarah A. Wu", "Rika Antonova", "Tobias Gerstenberg", "Jeannette Bohg"], "title": "Causal-PIK: Causality-based Physical Reasoning with a Physics-Informed Kernel", "categories": ["cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Tasks that involve complex interactions between objects with unknown dynamics\nmake planning before execution difficult. These tasks require agents to\niteratively improve their actions after actively exploring causes and effects\nin the environment. For these type of tasks, we propose Causal-PIK, a method\nthat leverages Bayesian optimization to reason about causal interactions via a\nPhysics-Informed Kernel to help guide efficient search for the best next\naction. Experimental results on Virtual Tools and PHYRE physical reasoning\nbenchmarks show that Causal-PIK outperforms state-of-the-art results, requiring\nfewer actions to reach the goal. We also compare Causal-PIK to human studies,\nincluding results from a new user study we conducted on the PHYRE benchmark. We\nfind that Causal-PIK remains competitive on tasks that are very challenging,\neven for human problem-solvers.", "AI": {"tldr": "Causal-PIK\u5229\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u7269\u7406\u4fe1\u606f\u6838\u63a8\u7406\u56e0\u679c\u4ea4\u4e92\uff0c\u6307\u5bfc\u9ad8\u6548\u641c\u7d22\u6700\u4f18\u52a8\u4f5c\u3002\u5728Virtual Tools\u548cPHYRE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u4e0e\u4eba\u7c7b\u76f8\u6bd4\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u4ecd\u5177\u7ade\u4e89\u529b\u3002", "motivation": "\u590d\u6742\u4ea4\u4e92\u4efb\u52a1\u4e2d\uff0c\u5bf9\u8c61\u52a8\u6001\u672a\u77e5\u5bfc\u81f4\u6267\u884c\u524d\u89c4\u5212\u56f0\u96be\uff0c\u9700\u901a\u8fc7\u4e3b\u52a8\u63a2\u7d22\u56e0\u679c\u6548\u5e94\u8fed\u4ee3\u4f18\u5316\u52a8\u4f5c\u3002", "method": "\u63d0\u51faCausal-PIK\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u4f18\u5316\u4e0e\u7269\u7406\u4fe1\u606f\u6838\uff08Physics-Informed Kernel\uff09\u8fdb\u884c\u56e0\u679c\u4ea4\u4e92\u63a8\u7406\uff0c\u6307\u5bfc\u52a8\u4f5c\u641c\u7d22\u3002", "result": "\u5728Virtual Tools\u548cPHYRE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u51cf\u5c11\u52a8\u4f5c\u6b21\u6570\u8fbe\u6210\u76ee\u6807\uff0c\u5e76\u5728\u4eba\u7c7b\u7814\u7a76\u4e2d\u8868\u73b0\u7ade\u4e89\u529b\u3002", "conclusion": "Causal-PIK\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u663e\u8457\u63d0\u5347\u590d\u6742\u4efb\u52a1\u7684\u89e3\u51b3\u6548\u7387\uff0c\u5c24\u5176\u5728\u4eba\u7c7b\u96be\u4ee5\u5e94\u5bf9\u7684\u573a\u666f\u4e2d\u4ecd\u6709\u6548\u3002", "keywords": "Causal-PIK, Bayesian optimization, Physics-Informed Kernel, Virtual Tools, PHYRE"}}
{"id": "2505.22942", "pdf": "https://arxiv.org/pdf/2505.22942", "abs": "https://arxiv.org/abs/2505.22942", "authors": ["Yuchen Zhuang", "Di Jin", "Jiaao Chen", "Wenqi Shi", "Hanrui Wang", "Chao Zhang"], "title": "WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "Work in Progress", "summary": "Large language models (LLMs)-empowered web agents enables automating complex,\nreal-time web navigation tasks in enterprise environments. However, existing\nweb agents relying on supervised fine-tuning (SFT) often struggle with\ngeneralization and robustness due to insufficient reasoning capabilities when\nhandling the inherently dynamic nature of web interactions. In this study, we\nintroduce WorkForceAgent-R1, an LLM-based web agent trained using a rule-based\nR1-style reinforcement learning framework designed explicitly to enhance\nsingle-step reasoning and planning for business-oriented web navigation tasks.\nWe employ a structured reward function that evaluates both adherence to output\nformats and correctness of actions, enabling WorkForceAgent-R1 to implicitly\nlearn robust intermediate reasoning without explicit annotations or extensive\nexpert demonstrations. Extensive experiments on the WorkArena benchmark\ndemonstrate that WorkForceAgent-R1 substantially outperforms SFT baselines by\n10.26-16.59%, achieving competitive performance relative to proprietary\nLLM-based agents (gpt-4o) in workplace-oriented web navigation tasks.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86WorkForceAgent-R1\uff0c\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u7f51\u9875\u4ee3\u7406\uff0c\u901a\u8fc7\u89c4\u5219\u5316\u7684R1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u63d0\u5347\u7f51\u9875\u5bfc\u822a\u4efb\u52a1\u7684\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u7684\u7f51\u9875\u4ee3\u7406\u5728\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u52a8\u6001\u7f51\u9875\u4ea4\u4e92\u65f6\u7f3a\u4e4f\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u89c4\u5219\u7684R1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u5956\u52b1\u51fd\u6570\uff0c\u65e0\u9700\u663e\u5f0f\u6807\u6ce8\u6216\u4e13\u5bb6\u793a\u8303\u5373\u53ef\u5b66\u4e60\u7a33\u5065\u7684\u4e2d\u95f4\u63a8\u7406\u3002", "result": "\u5728WorkArena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWorkForceAgent-R1\u6bd4\u76d1\u7763\u5fae\u8c03\u57fa\u7ebf\u9ad8\u51fa10.26%-16.59%\uff0c\u6027\u80fd\u63a5\u8fd1GPT-4o\u7b49\u4e13\u6709\u6a21\u578b\u3002", "conclusion": "R1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u7f51\u9875\u4ee3\u7406\u5728\u4e1a\u52a1\u573a\u666f\u4e2d\u7684\u5355\u6b65\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u7f51\u9875\u4ee3\u7406, \u5f3a\u5316\u5b66\u4e60, R1\u6846\u67b6, \u4e1a\u52a1\u5bfc\u5411\u4efb\u52a1"}}
{"id": "2505.23536", "pdf": "https://arxiv.org/pdf/2505.23536", "abs": "https://arxiv.org/abs/2505.23536", "authors": ["Janik-Vasily Benzin", "Gyunam Park", "Stefanie Rinderle-Ma"], "title": "Synchronizing Process Model and Event Abstraction for Grounded Process Intelligence (Extended Version)", "categories": ["cs.AI"], "comment": null, "summary": "Model abstraction (MA) and event abstraction (EA) are means to reduce\ncomplexity of (discovered) models and event data. Imagine a process\nintelligence project that aims to analyze a model discovered from event data\nwhich is further abstracted, possibly multiple times, to reach optimality\ngoals, e.g., reducing model size. So far, after discovering the model, there is\nno technique that enables the synchronized abstraction of the underlying event\nlog. This results in loosing the grounding in the real-world behavior contained\nin the log and, in turn, restricts analysis insights. Hence, in this work, we\nprovide the formal basis for synchronized model and event abstraction, i.e., we\nprove that abstracting a process model by MA and discovering a process model\nfrom an abstracted event log yields an equivalent process model. We prove the\nfeasibility of our approach based on behavioral profile abstraction as\nnon-order preserving MA technique, resulting in a novel EA technique.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u540c\u6b65\u6a21\u578b\u548c\u4e8b\u4ef6\u62bd\u8c61\u7684\u6b63\u5f0f\u57fa\u7840\uff0c\u8bc1\u660e\u901a\u8fc7\u6a21\u578b\u62bd\u8c61\uff08MA\uff09\u548c\u4ece\u62bd\u8c61\u4e8b\u4ef6\u65e5\u5fd7\u4e2d\u53d1\u73b0\u7684\u8fc7\u7a0b\u6a21\u578b\u4f1a\u4ea7\u751f\u7b49\u6548\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u6280\u672f\u65e0\u6cd5\u5728\u6a21\u578b\u53d1\u73b0\u540e\u540c\u6b65\u62bd\u8c61\u5e95\u5c42\u4e8b\u4ef6\u65e5\u5fd7\uff0c\u5bfc\u81f4\u5931\u53bb\u771f\u5b9e\u884c\u4e3a\u7684\u57fa\u7840\uff0c\u9650\u5236\u4e86\u5206\u6790\u6d1e\u5bdf\u529b\u3002", "method": "\u57fa\u4e8e\u884c\u4e3a\u8f6e\u5ed3\u62bd\u8c61\u4f5c\u4e3a\u975e\u987a\u5e8f\u4fdd\u7559\u7684MA\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684EA\u6280\u672f\u3002", "result": "\u8bc1\u660e\u4e86\u540c\u6b65\u6a21\u578b\u548c\u4e8b\u4ef6\u62bd\u8c61\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u8fc7\u7a0b\u6a21\u578b\u7684\u62bd\u8c61\u548c\u4e8b\u4ef6\u65e5\u5fd7\u7684\u540c\u6b65\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u6280\u672f\u3002", "keywords": "\u6a21\u578b\u62bd\u8c61\uff08MA\uff09\uff0c\u4e8b\u4ef6\u62bd\u8c61\uff08EA\uff09\uff0c\u8fc7\u7a0b\u6316\u6398\uff0c\u884c\u4e3a\u8f6e\u5ed3"}}
{"id": "2505.22866", "pdf": "https://arxiv.org/pdf/2505.22866", "abs": "https://arxiv.org/abs/2505.22866", "authors": ["Nicolas Espinosa-Dice", "Yiyi Zhang", "Yiding Chen", "Bradley Guo", "Owen Oertell", "Gokul Swamy", "Kiante Brantley", "Wen Sun"], "title": "Scaling Offline RL via Efficient and Expressive Shortcut Models", "categories": ["cs.LG", "cs.AI", "I.2.6"], "comment": "32 pages, 5 figures. Under review at NeurIPS 2025", "summary": "Diffusion and flow models have emerged as powerful generative approaches\ncapable of modeling diverse and multimodal behavior. However, applying these\nmodels to offline reinforcement learning (RL) remains challenging due to the\niterative nature of their noise sampling processes, making policy optimization\ndifficult. In this paper, we introduce Scalable Offline Reinforcement Learning\n(SORL), a new offline RL algorithm that leverages shortcut models - a novel\nclass of generative models - to scale both training and inference. SORL's\npolicy can capture complex data distributions and can be trained simply and\nefficiently in a one-stage training procedure. At test time, SORL introduces\nboth sequential and parallel inference scaling by using the learned Q-function\nas a verifier. We demonstrate that SORL achieves strong performance across a\nrange of offline RL tasks and exhibits positive scaling behavior with increased\ntest-time compute. We release the code at\nnico-espinosadice.github.io/projects/sorl.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSORL\u7684\u65b0\u578b\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6377\u5f84\u6a21\u578b\u6765\u63d0\u5347\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u53ef\u6269\u5c55\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6269\u6563\u548c\u6d41\u6a21\u578b\u5728\u79bb\u7ebfRL\u4e2d\u56e0\u566a\u58f0\u91c7\u6837\u8fed\u4ee3\u5bfc\u81f4\u7684\u7b56\u7565\u4f18\u5316\u96be\u9898\u3002", "motivation": "\u6269\u6563\u548c\u6d41\u6a21\u578b\u867d\u7136\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u5176\u566a\u58f0\u91c7\u6837\u8fc7\u7a0b\u8fed\u4ee3\u6027\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u3002\u672c\u6587\u65e8\u5728\u514b\u670d\u8fd9\u4e00\u6311\u6218\uff0c\u5f00\u53d1\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u79bb\u7ebfRL\u7b97\u6cd5\u3002", "method": "\u63d0\u51faSORL\u7b97\u6cd5\uff0c\u5229\u7528\u6377\u5f84\u6a21\u578b\uff08\u4e00\u7c7b\u65b0\u578b\u751f\u6210\u6a21\u578b\uff09\u7b80\u5316\u8bad\u7ec3\u548c\u63a8\u7406\u3002\u901a\u8fc7\u5355\u9636\u6bb5\u8bad\u7ec3\u8fc7\u7a0b\u9ad8\u6548\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u5e8f\u5217\u548c\u5e73\u884c\u63a8\u7406\u6269\u5c55\uff0c\u5229\u7528\u5b66\u4e60\u7684Q\u51fd\u6570\u4f5c\u4e3a\u9a8c\u8bc1\u5668\u3002", "result": "SORL\u5728\u591a\u79cd\u79bb\u7ebfRL\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u968f\u7740\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\u7684\u589e\u52a0\u5c55\u73b0\u51fa\u6b63\u5411\u7684\u6269\u5c55\u884c\u4e3a\u3002", "conclusion": "SORL\u901a\u8fc7\u6377\u5f84\u6a21\u578b\u548c\u521b\u65b0\u7684\u63a8\u7406\u6269\u5c55\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebfRL\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u6570\u636e\u5206\u5e03\u7684\u5efa\u6a21\u548c\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60,\u6269\u6563\u6a21\u578b,\u6d41\u6a21\u578b,\u6377\u5f84\u6a21\u578b,\u53ef\u6269\u5c55\u6027"}}
{"id": "2505.22943", "pdf": "https://arxiv.org/pdf/2505.22943", "abs": "https://arxiv.org/abs/2505.22943", "authors": ["Jaewoo Ahn", "Heeseung Yun", "Dayoon Ko", "Gunhee Kim"], "title": "Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.SD"], "comment": "ACL 2025 Main. Code is released at\n  https://vision.snu.ac.kr/projects/mac", "summary": "While pre-trained multimodal representations (e.g., CLIP) have shown\nimpressive capabilities, they exhibit significant compositional vulnerabilities\nleading to counterintuitive judgments. We introduce Multimodal Adversarial\nCompositionality (MAC), a benchmark that leverages large language models (LLMs)\nto generate deceptive text samples to exploit these vulnerabilities across\ndifferent modalities and evaluates them through both sample-wise attack success\nrate and group-wise entropy-based diversity. To improve zero-shot methods, we\npropose a self-training approach that leverages rejection-sampling fine-tuning\nwith diversity-promoting filtering, which enhances both attack success rate and\nsample diversity. Using smaller language models like Llama-3.1-8B, our approach\ndemonstrates superior performance in revealing compositional vulnerabilities\nacross various multimodal representations, including images, videos, and\naudios.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86Multimodal Adversarial Compositionality (MAC)\u57fa\u51c6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6b3a\u9a97\u6027\u6587\u672c\u6837\u672c\uff0c\u63ed\u793a\u591a\u6a21\u6001\u8868\u793a\u7684\u7ec4\u5408\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u63d0\u5347\u96f6\u6837\u672c\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u9884\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u8868\u793a\uff08\u5982CLIP\uff09\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u5b58\u5728\u7ec4\u5408\u8106\u5f31\u6027\uff0c\u5bfc\u81f4\u53cd\u76f4\u89c9\u7684\u5224\u65ad\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u548c\u65b9\u6cd5\u6539\u8fdb\u63ed\u793a\u5e76\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faMAC\u57fa\u51c6\uff0c\u5229\u7528LLMs\u751f\u6210\u6b3a\u9a97\u6027\u6587\u672c\u6837\u672c\uff0c\u8bc4\u4f30\u591a\u6a21\u6001\u8868\u793a\u7684\u8106\u5f31\u6027\uff1b\u91c7\u7528\u81ea\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u62d2\u7edd\u91c7\u6837\u5fae\u8c03\u548c\u591a\u6837\u6027\u4fc3\u8fdb\u8fc7\u6ee4\uff0c\u63d0\u5347\u6027\u80fd\u3002", "result": "\u4f7f\u7528\u8f83\u5c0f\u8bed\u8a00\u6a21\u578b\uff08\u5982Llama-3.1-8B\uff09\u7684\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u8868\u793a\uff08\u56fe\u50cf\u3001\u89c6\u9891\u3001\u97f3\u9891\uff09\u4e2d\u5c55\u73b0\u66f4\u5f3a\u7684\u7ec4\u5408\u8106\u5f31\u6027\u63ed\u793a\u80fd\u529b\uff0c\u653b\u51fb\u6210\u529f\u7387\u548c\u6837\u672c\u591a\u6837\u6027\u5747\u663e\u8457\u63d0\u5347\u3002", "conclusion": "MAC\u57fa\u51c6\u548c\u65b9\u6cd5\u6709\u6548\u63ed\u793a\u4e86\u591a\u6a21\u6001\u8868\u793a\u7684\u7ec4\u5408\u8106\u5f31\u6027\uff0c\u81ea\u8bad\u7ec3\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u591a\u6837\u6027\u3002", "keywords": "\u591a\u6a21\u6001\u8868\u793a\uff0c\u7ec4\u5408\u8106\u5f31\u6027\uff0c\u5bf9\u6297\u6837\u672c\uff0c\u81ea\u8bad\u7ec3\uff0c\u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.23559", "pdf": "https://arxiv.org/pdf/2505.23559", "abs": "https://arxiv.org/abs/2505.23559", "authors": ["Kunlun Zhu", "Jiaxun Zhang", "Ziheng Qi", "Nuoxing Shang", "Zijia Liu", "Peixuan Han", "Yue Su", "Haofei Yu", "Jiaxuan You"], "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "Recent advancements in large language model (LLM) agents have significantly\naccelerated scientific discovery automation, yet concurrently raised critical\nethical and safety concerns. To systematically address these challenges, we\nintroduce \\textbf{SafeScientist}, an innovative AI scientist framework\nexplicitly designed to enhance safety and ethical responsibility in AI-driven\nscientific exploration. SafeScientist proactively refuses ethically\ninappropriate or high-risk tasks and rigorously emphasizes safety throughout\nthe research process. To achieve comprehensive safety oversight, we integrate\nmultiple defensive mechanisms, including prompt monitoring, agent-collaboration\nmonitoring, tool-use monitoring, and an ethical reviewer component.\nComplementing SafeScientist, we propose \\textbf{SciSafetyBench}, a novel\nbenchmark specifically designed to evaluate AI safety in scientific contexts,\ncomprising 240 high-risk scientific tasks across 6 domains, alongside 30\nspecially designed scientific tools and 120 tool-related risk tasks. Extensive\nexperiments demonstrate that SafeScientist significantly improves safety\nperformance by 35\\% compared to traditional AI scientist frameworks, without\ncompromising scientific output quality. Additionally, we rigorously validate\nthe robustness of our safety pipeline against diverse adversarial attack\nmethods, further confirming the effectiveness of our integrated approach. The\ncode and data will be available at https://github.com/ulab-uiuc/SafeScientist.\n\\textcolor{red}{Warning: this paper contains example data that may be offensive\nor harmful.}", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSafeScientist\u7684AI\u79d1\u5b66\u5bb6\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347AI\u9a71\u52a8\u7684\u79d1\u5b66\u63a2\u7d22\u4e2d\u7684\u5b89\u5168\u6027\u548c\u4f26\u7406\u8d23\u4efb\u3002\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u9632\u5fa1\u673a\u5236\uff0c\u5e76\u5f15\u5165SciSafetyBench\u57fa\u51c6\u8fdb\u884c\u5b89\u5168\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u8bc1\u660eSafeScientist\u5728\u5b89\u5168\u6027\u80fd\u4e0a\u63d0\u5347\u4e8635%\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u79d1\u5b66\u4ea7\u51fa\u8d28\u91cf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7684\u5feb\u901f\u53d1\u5c55\u867d\u7136\u52a0\u901f\u4e86\u79d1\u5b66\u53d1\u73b0\u81ea\u52a8\u5316\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u4f26\u7406\u548c\u5b89\u5168\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u8bbe\u8ba1\u4e86SafeScientist\u6846\u67b6\uff0c\u4ee5\u786e\u4fddAI\u9a71\u52a8\u7684\u79d1\u5b66\u63a2\u7d22\u66f4\u5b89\u5168\u548c\u8d1f\u8d23\u4efb\u3002", "method": "SafeScientist\u96c6\u6210\u4e86\u591a\u79cd\u9632\u5fa1\u673a\u5236\uff0c\u5305\u62ec\u63d0\u793a\u76d1\u63a7\u3001\u4ee3\u7406\u534f\u4f5c\u76d1\u63a7\u3001\u5de5\u5177\u4f7f\u7528\u76d1\u63a7\u548c\u4f26\u7406\u5ba1\u67e5\u6a21\u5757\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u63d0\u51fa\u4e86SciSafetyBench\u57fa\u51c6\uff0c\u5305\u542b240\u4e2a\u9ad8\u98ce\u9669\u79d1\u5b66\u4efb\u52a1\u548c120\u4e2a\u4e0e\u5de5\u5177\u76f8\u5173\u7684\u98ce\u9669\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSafeScientist\u5728\u5b89\u5168\u6027\u80fd\u4e0a\u6bd4\u4f20\u7edfAI\u79d1\u5b66\u5bb6\u6846\u67b6\u63d0\u5347\u4e8635%\uff0c\u4e14\u672a\u5f71\u54cd\u79d1\u5b66\u4ea7\u51fa\u8d28\u91cf\u3002\u540c\u65f6\uff0c\u8be5\u6846\u67b6\u5728\u5bf9\u6297\u6027\u653b\u51fb\u6d4b\u8bd5\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "SafeScientist\u901a\u8fc7\u7efc\u5408\u5b89\u5168\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86AI\u79d1\u5b66\u5bb6\u7684\u5b89\u5168\u6027\u548c\u4f26\u7406\u8d23\u4efb\uff0c\u4e3a\u672a\u6765AI\u9a71\u52a8\u7684\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6846\u67b6\u3002", "keywords": "AI\u5b89\u5168, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u79d1\u5b66\u63a2\u7d22, \u4f26\u7406\u8d23\u4efb, \u9632\u5fa1\u673a\u5236, \u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2505.22881", "pdf": "https://arxiv.org/pdf/2505.22881", "abs": "https://arxiv.org/abs/2505.22881", "authors": ["Hyungki Im", "Wyame Benslimane", "Paul Grigas"], "title": "Smart Surrogate Losses for Contextual Stochastic Linear Optimization with Robust Constraints", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "We study an extension of contextual stochastic linear optimization (CSLO)\nthat, in contrast to most of the existing literature, involves inequality\nconstraints that depend on uncertain parameters predicted by a machine learning\nmodel. To handle the constraint uncertainty, we use contextual uncertainty sets\nconstructed via methods like conformal prediction. Given a contextual\nuncertainty set method, we introduce the \"Smart Predict-then-Optimize with\nRobust Constraints\" (SPO-RC) loss, a feasibility-sensitive adaptation of the\nSPO loss that measures decision error of predicted objective parameters. We\nalso introduce a convex surrogate, SPO-RC+, and prove Fisher consistency with\nSPO-RC. To enhance performance, we train on truncated datasets where true\nconstraint parameters lie within the uncertainty sets, and we correct the\ninduced sample selection bias using importance reweighting techniques. Through\nexperiments on fractional knapsack and alloy production problem instances, we\ndemonstrate that SPO-RC+ effectively handles uncertainty in constraints and\nthat combining truncation with importance reweighting can further improve\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684SPO-RC\u635f\u5931\u51fd\u6570\u53ca\u5176\u51f8\u4ee3\u7406SPO-RC+\uff0c\u7528\u4e8e\u5904\u7406\u5e26\u6709\u7ea6\u675f\u4e0d\u786e\u5b9a\u6027\u7684\u4e0a\u4e0b\u6587\u968f\u673a\u7ebf\u6027\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u622a\u65ad\u6570\u636e\u96c6\u548c\u91cd\u8981\u6027\u91cd\u52a0\u6743\u6280\u672f\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5927\u591a\u5ffd\u7565\u7ea6\u675f\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u7ea6\u675f\u4f9d\u8d56\u4e8e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u4f7f\u7528\u60c5\u5883\u4e0d\u786e\u5b9a\u6027\u96c6\uff08\u5982\u57fa\u4e8eConformal Prediction\u7684\uff09\u5904\u7406\u7ea6\u675f\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u51faSPO-RC\u635f\u5931\u51fd\u6570\u53ca\u5176\u51f8\u4ee3\u7406SPO-RC+\uff0c\u5e76\u901a\u8fc7\u622a\u65ad\u6570\u636e\u96c6\u548c\u91cd\u8981\u6027\u91cd\u52a0\u6743\u6280\u672f\u63d0\u9ad8\u6027\u80fd\u3002", "result": "\u5728\u5206\u6570\u80cc\u5305\u548c\u5408\u91d1\u751f\u4ea7\u95ee\u9898\u5b9e\u4f8b\u4e2d\uff0cSPO-RC+\u80fd\u6709\u6548\u5904\u7406\u7ea6\u675f\u4e0d\u786e\u5b9a\u6027\uff0c\u4e14\u622a\u65ad\u4e0e\u91cd\u52a0\u6743\u7ed3\u5408\u53ef\u8fdb\u4e00\u6b65\u6539\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7ea6\u675f\u4e0d\u786e\u5b9a\u6027\u60c5\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u7ed3\u5408\u622a\u65ad\u4e0e\u91cd\u52a0\u6743\u65f6\u6548\u679c\u66f4\u4f73\u3002", "keywords": "\u4e0a\u4e0b\u6587\u968f\u673a\u7ebf\u6027\u4f18\u5316,\u7ea6\u675f\u4e0d\u786e\u5b9a\u6027,SPO-RC\u635f\u5931,\u622a\u65ad\u6570\u636e\u96c6,\u91cd\u8981\u6027\u91cd\u52a0\u6743"}}
{"id": "2505.22945", "pdf": "https://arxiv.org/pdf/2505.22945", "abs": "https://arxiv.org/abs/2505.22945", "authors": ["Alisha Srivastava", "Emir Korukluoglu", "Minh Nhat Le", "Duyen Tran", "Chau Minh Pham", "Marzena Karpinska", "Mohit Iyyer"], "title": "OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature", "categories": ["cs.CL", "cs.AI"], "comment": "preprint, 25 pages", "summary": "Large language models (LLMs) are known to memorize and recall English text\nfrom their pretraining data. However, the extent to which this ability\ngeneralizes to non-English languages or transfers across languages remains\nunclear. This paper investigates multilingual and cross-lingual memorization in\nLLMs, probing if memorized content in one language (e.g., English) can be\nrecalled when presented in translation. To do so, we introduce OWL, a dataset\nof 31.5K aligned excerpts from 20 books in ten languages, including English\noriginals, official translations (Vietnamese, Spanish, Turkish), and new\ntranslations in six low-resource languages (Sesotho, Yoruba, Maithili,\nMalagasy, Setswana, Tahitian). We evaluate memorization across model families\nand sizes through three tasks: (1) direct probing, which asks the model to\nidentify a book's title and author; (2) name cloze, which requires predicting\nmasked character names; and (3) prefix probing, which involves generating\ncontinuations. We find that LLMs consistently recall content across languages,\neven for texts without direct translation in pretraining data. GPT-4o, for\nexample, identifies authors and titles 69% of the time and masked entities 6%\nof the time in newly translated excerpts. Perturbations (e.g., masking\ncharacters, shuffling words) modestly reduce direct probing accuracy (7% drop\nfor shuffled official translations). Our results highlight the extent of\ncross-lingual memorization and provide insights on the differences between the\nmodels.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u591a\u8bed\u8a00\u548c\u8de8\u8bed\u8a00\u8bb0\u5fc6\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u6ca1\u6709\u76f4\u63a5\u7ffb\u8bd1\u7684\u6587\u672c\uff0cLLMs\u4e5f\u80fd\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u56de\u5fc6\u5185\u5bb9\u3002", "motivation": "\u63a2\u7a76LLMs\u5728\u591a\u8bed\u8a00\u548c\u8de8\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u6216\u7ffb\u8bd1\u6587\u672c\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528OWL\u6570\u636e\u96c6\uff08\u5305\u542b10\u79cd\u8bed\u8a00\u768431.5K\u5bf9\u9f50\u6587\u672c\u7247\u6bb5\uff09\uff0c\u901a\u8fc7\u76f4\u63a5\u63a2\u6d4b\u3001\u540d\u79f0\u8865\u9f50\u548c\u524d\u7f00\u751f\u6210\u4e09\u79cd\u4efb\u52a1\u8bc4\u4f30\u6a21\u578b\u8bb0\u5fc6\u80fd\u529b\u3002", "result": "LLMs\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u8868\u73b0\u51fa\u7a33\u5b9a\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u4f8b\u5982GPT-4o\u5728\u65b0\u7ffb\u8bd1\u7684\u6587\u672c\u4e2d\u8bc6\u522b\u4f5c\u8005\u548c\u6807\u9898\u7684\u51c6\u786e\u7387\u4e3a69%\uff0c\u800c\u6270\u52a8\uff08\u5982\u5b57\u7b26\u5c4f\u853d\u3001\u5355\u8bcd\u6253\u4e71\uff09\u4ec5\u8f7b\u5fae\u964d\u4f4e\u51c6\u786e\u6027\u3002", "conclusion": "LLMs\u5177\u6709\u663e\u8457\u7684\u8de8\u8bed\u8a00\u8bb0\u5fc6\u80fd\u529b\uff0c\u5e76\u4e14\u6a21\u578b\u4e4b\u95f4\u5728\u8868\u73b0\u4e0a\u5b58\u5728\u5dee\u5f02\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u591a\u8bed\u8a00\u8bb0\u5fc6\u3001\u8de8\u8bed\u8a00\u8bb0\u5fc6\u3001OWL\u6570\u636e\u96c6\u3001\u6587\u672c\u7ffb\u8bd1"}}
{"id": "2505.23575", "pdf": "https://arxiv.org/pdf/2505.23575", "abs": "https://arxiv.org/abs/2505.23575", "authors": ["Benjamin Arnav", "Pablo Bernabeu-P\u00e9rez", "Nathan Helm-Burger", "Tim Kostolansky", "Hannes Whittingham", "Mary Phuong"], "title": "CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "As AI models are deployed with increasing autonomy, it is important to ensure\nthey do not take harmful actions unnoticed. As a potential mitigation, we\ninvestigate Chain-of-Thought (CoT) monitoring, wherein a weaker trusted monitor\nmodel continuously oversees the intermediate reasoning steps of a more powerful\nbut untrusted model. We compare CoT monitoring to action-only monitoring, where\nonly final outputs are reviewed, in a red-teaming setup where the untrusted\nmodel is instructed to pursue harmful side tasks while completing a coding\nproblem. We find that CoT monitoring improves detection by up to 27 percentage\npoints in scenarios where action-only monitoring fails to reliably identify\nsabotage. However, CoT traces can also contain misleading rationalizations that\ndeceive the monitor, reducing performance in more obvious sabotage cases. To\naddress this, we introduce a hybrid protocol that independently scores both\nreasoning and final outputs and combines them using a weighted average. This\nhybrid monitor consistently outperforms both CoT and action-only monitors\nacross all tested models and tasks, with detection rates over four times higher\nthan action-only monitoring for subtle deception scenarios.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7Chain-of-Thought\uff08CoT\uff09\u76d1\u63a7\u589e\u5f3a\u5bf9AI\u6a21\u578b\u7684\u76d1\u7763\uff0c\u53d1\u73b0\u7ed3\u5408\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u76d1\u63a7\u6bd4\u4ec5\u68c0\u67e5\u6700\u7ec8\u8f93\u51fa\u7684\u65b9\u6cd5\u66f4\u80fd\u6709\u6548\u53d1\u73b0\u6709\u5bb3\u884c\u4e3a\uff0c\u4f46\u5b58\u5728\u8bef\u5bfc\u6027\u89e3\u91ca\u7684\u95ee\u9898\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u534f\u8bae\uff0c\u7efc\u5408\u8bc4\u5206\u63a8\u7406\u548c\u8f93\u51fa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7387\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\u90e8\u7f72\u81ea\u4e3b\u6027\u589e\u5f3a\uff0c\u786e\u4fdd\u5176\u4e0d\u4f1a\u6267\u884c\u6709\u5bb3\u884c\u4e3a\u800c\u4e0d\u88ab\u53d1\u73b0\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u66f4\u6709\u6548\u7684\u76d1\u63a7\u65b9\u6cd5\uff0c\u9632\u6b62\u6a21\u578b\u5728\u6267\u884c\u4efb\u52a1\u65f6\u6697\u4e2d\u6267\u884c\u6709\u5bb3\u64cd\u4f5c\u3002", "method": "\u6bd4\u8f83\u4e86CoT\u76d1\u63a7\uff08\u76d1\u63a7\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\uff09\u4e0e\u4ec5\u76d1\u63a7\u6700\u7ec8\u8f93\u51fa\u7684\u65b9\u6cd5\uff0c\u5728\u7ea2\u961f\u6d4b\u8bd5\u4e2d\u6a21\u62df\u6a21\u578b\u6267\u884c\u6709\u5bb3\u4efb\u52a1\u7684\u60c5\u51b5\u3002\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u534f\u8bae\uff0c\u72ec\u7acb\u8bc4\u5206\u63a8\u7406\u548c\u8f93\u51fa\u5e76\u52a0\u6743\u5e73\u5747\u3002", "result": "CoT\u76d1\u63a7\u5728\u90e8\u5206\u573a\u666f\u4e0b\u68c0\u6d4b\u7387\u63d0\u534727%\uff0c\u4f46\u5b58\u5728\u8bef\u5bfc\u6027\u89e3\u91ca\u7684\u95ee\u9898\u3002\u6df7\u5408\u534f\u8bae\u5728\u6d4b\u8bd5\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u68c0\u6d4b\u7387\u6bd4\u4ec5\u76d1\u63a7\u8f93\u51fa\u7684\u65b9\u6cd5\u9ad8\u56db\u500d\u3002", "conclusion": "\u7ed3\u5408\u63a8\u7406\u548c\u8f93\u51fa\u7684\u6df7\u5408\u76d1\u63a7\u534f\u8bae\u662f\u66f4\u53ef\u9760\u7684\u76d1\u7763\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8bc6\u522bAI\u6a21\u578b\u7684\u9690\u853d\u6709\u5bb3\u884c\u4e3a\u3002", "keywords": "AI\u76d1\u63a7, Chain-of-Thought, \u7ea2\u961f\u6d4b\u8bd5, \u6df7\u5408\u534f\u8bae, \u6709\u5bb3\u884c\u4e3a\u68c0\u6d4b"}}
{"id": "2505.22899", "pdf": "https://arxiv.org/pdf/2505.22899", "abs": "https://arxiv.org/abs/2505.22899", "authors": ["Naram Mhaisen", "George Iosifidis"], "title": "On the Dynamic Regret of Following the Regularized Leader: Optimism with History Pruning", "categories": ["cs.LG"], "comment": null, "summary": "We revisit the Follow the Regularized Leader (FTRL) framework for Online\nConvex Optimization (OCO) over compact sets, focusing on achieving dynamic\nregret guarantees. Prior work has highlighted the framework's limitations in\ndynamic environments due to its tendency to produce \"lazy\" iterates. However,\nbuilding on insights showing FTRL's ability to produce \"agile\" iterates, we\nshow that it can indeed recover known dynamic regret bounds through optimistic\ncomposition of future costs and careful linearization of past costs, which can\nlead to pruning some of them. This new analysis of FTRL against dynamic\ncomparators yields a principled way to interpolate between greedy and agile\nupdates and offers several benefits, including refined control over regret\nterms, optimism without cyclic dependence, and the application of minimal\nrecursive regularization akin to AdaFTRL. More broadly, we show that it is not\nthe lazy projection style of FTRL that hinders (optimistic) dynamic regret, but\nthe decoupling of the algorithm's state (linearized history) from its iterates,\nallowing the state to grow arbitrarily. Instead, pruning synchronizes these two\nwhen necessary.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86FTRL\u6846\u67b6\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4f18\u5316\u672a\u6765\u6210\u672c\u548c\u7ebf\u6027\u5316\u8fc7\u53bb\u6210\u672c\u7684\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u5b9e\u73b0\u52a8\u6001\u9057\u61be\u4fdd\u8bc1\u3002", "motivation": "\u9488\u5bf9FTRL\u6846\u67b6\u5728\u52a8\u6001\u73af\u5883\u4e2d\u56e0\u4ea7\u751f\u201c\u60f0\u6027\u201d\u8fed\u4ee3\u800c\u53d7\u9650\u7684\u95ee\u9898\uff0c\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u548c\u4fee\u526a\u65b9\u6cd5\u63d0\u5347\u5176\u52a8\u6001\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u4e50\u89c2\u7ec4\u5408\u672a\u6765\u6210\u672c\u548c\u7cbe\u7ec6\u7ebf\u6027\u5316\u8fc7\u53bb\u6210\u672c\uff0c\u5e76\u5bf9\u90e8\u5206\u6210\u672c\u8fdb\u884c\u4fee\u526a\uff0c\u5b9e\u73b0FTRL\u7684\u52a8\u6001\u9057\u61be\u4fdd\u8bc1\u3002", "result": "\u65b0\u65b9\u6cd5\u80fd\u591f\u6062\u590d\u5df2\u77e5\u7684\u52a8\u6001\u9057\u61be\u754c\uff0c\u63d0\u4f9b\u8d2a\u5a6a\u4e0e\u654f\u6377\u66f4\u65b0\u4e4b\u95f4\u7684\u63d2\u503c\uff0c\u4f18\u5316\u63a7\u5236\u9057\u61be\u9879\uff0c\u5e76\u65e0\u9700\u5faa\u73af\u4f9d\u8d56\u5730\u5b9e\u73b0\u4e50\u89c2\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cFTRL\u7684\u5c40\u9650\u6027\u5728\u4e8e\u7b97\u6cd5\u72b6\u6001\u4e0e\u8fed\u4ee3\u7684\u89e3\u8026\uff0c\u800c\u975e\u5176\u6295\u5f71\u65b9\u5f0f\uff0c\u4fee\u526a\u65b9\u6cd5\u80fd\u6709\u6548\u540c\u6b65\u4e8c\u8005\u3002", "keywords": "FTRL, \u52a8\u6001\u9057\u61be, \u5728\u7ebf\u51f8\u4f18\u5316, \u4fee\u526a, \u4e50\u89c2\u7ec4\u5408"}}
{"id": "2505.22946", "pdf": "https://arxiv.org/pdf/2505.22946", "abs": "https://arxiv.org/abs/2505.22946", "authors": ["Yuhui Zhang", "Yuchang Su", "Yiming Liu", "Serena Yeung-Levy"], "title": "NegVQA: Can Vision Language Models Understand Negation?", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY", "cs.LG"], "comment": "Published at ACL 2025 Findings", "summary": "Negation is a fundamental linguistic phenomenon that can entirely reverse the\nmeaning of a sentence. As vision language models (VLMs) continue to advance and\nare deployed in high-stakes applications, assessing their ability to comprehend\nnegation becomes essential. To address this, we introduce NegVQA, a visual\nquestion answering (VQA) benchmark consisting of 7,379 two-choice questions\ncovering diverse negation scenarios and image-question distributions. We\nconstruct NegVQA by leveraging large language models to generate negated\nversions of questions from existing VQA datasets. Evaluating 20\nstate-of-the-art VLMs across seven model families, we find that these models\nstruggle significantly with negation, exhibiting a substantial performance drop\ncompared to their responses to the original questions. Furthermore, we uncover\na U-shaped scaling trend, where increasing model size initially degrades\nperformance on NegVQA before leading to improvements. Our benchmark reveals\ncritical gaps in VLMs' negation understanding and offers insights into future\nVLM development. Project page available at\nhttps://yuhui-zh15.github.io/NegVQA/.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86NegVQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e8620\u79cd\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5426\u5b9a\u53e5\u7406\u89e3\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u666e\u904d\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\uff0c\u8bc4\u4f30\u5176\u5bf9\u5426\u5b9a\u53e5\u7684\u7406\u89e3\u80fd\u529b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u73b0\u6709VQA\u6570\u636e\u96c6\u4e2d\u751f\u6210\u5426\u5b9a\u95ee\u9898\uff0c\u6784\u5efa\u5305\u542b7,379\u4e2a\u95ee\u9898\u7684NegVQA\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c20\u79cd\u5148\u8fdb\u6a21\u578b\u5728\u5426\u5b9a\u53e5\u7406\u89e3\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u4e14\u6a21\u578b\u89c4\u6a21\u5448\u73b0U\u5f62\u6269\u5c55\u8d8b\u52bf\u3002", "conclusion": "NegVQA\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5426\u5b9a\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "keywords": "\u5426\u5b9a\u7406\u89e3, \u89c6\u89c9\u8bed\u8a00\u6a21\u578b, VQA\u57fa\u51c6\u6d4b\u8bd5, \u6a21\u578b\u8bc4\u4f30"}}
{"id": "2505.23596", "pdf": "https://arxiv.org/pdf/2505.23596", "abs": "https://arxiv.org/abs/2505.23596", "authors": ["Linqiang Guo", "Wei Liu", "Yi Wen Heng", "Tse-Hsun", "Chen", "Yang Wang"], "title": "MAPLE: A Mobile Assistant with Persistent Finite State Machines for Recovery Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Mobile GUI agents aim to autonomously complete user-instructed tasks across\nmobile apps. Recent advances in Multimodal Large Language Models (MLLMs) enable\nthese agents to interpret UI screens, identify actionable elements, and perform\ninteractions such as tapping or typing. However, existing agents remain\nreactive: they reason only over the current screen and lack a structured model\nof app navigation flow, limiting their ability to understand context, detect\nunexpected outcomes, and recover from errors. We present MAPLE, a state-aware\nmulti-agent framework that abstracts app interactions as a Finite State Machine\n(FSM). We computationally model each UI screen as a discrete state and user\nactions as transitions, allowing the FSM to provide a structured representation\nof the app execution. MAPLE consists of specialized agents responsible for four\nphases of task execution: planning, execution, verification, error recovery,\nand knowledge retention. These agents collaborate to dynamically construct FSMs\nin real time based on perception data extracted from the UI screen, allowing\nthe GUI agents to track navigation progress and flow, validate action outcomes\nthrough pre- and post-conditions of the states, and recover from errors by\nrolling back to previously stable states. Our evaluation results on two\nchallenging cross-app benchmarks, Mobile-Eval-E and SPA-Bench, show that MAPLE\noutperforms the state-of-the-art baseline, improving task success rate by up to\n12%, recovery success by 13.8%, and action accuracy by 6.5%. Our results\nhighlight the importance of structured state modeling in guiding mobile GUI\nagents during task execution. Moreover, our FSM representation can be\nintegrated into future GUI agent architectures as a lightweight, model-agnostic\nmemory layer to support structured planning, execution verification, and error\nrecovery.", "AI": {"tldr": "MAPLE is a state-aware multi-agent framework for mobile GUI tasks, using Finite State Machines (FSM) to model app navigation, improving task success rates and error recovery.", "motivation": "Existing mobile GUI agents are reactive, lacking structured navigation flow and error recovery capabilities, limiting their context understanding and robustness.", "method": "MAPLE abstracts app interactions as FSMs, with specialized agents for planning, execution, verification, error recovery, and knowledge retention, dynamically constructing FSMs from UI data.", "result": "MAPLE outperforms baselines, improving task success rate by 12%, recovery success by 13.8%, and action accuracy by 6.5% on Mobile-Eval-E and SPA-Bench benchmarks.", "conclusion": "Structured state modeling (FSM) enhances mobile GUI agents' performance, offering a lightweight, model-agnostic memory layer for planning and error recovery.", "keywords": "mobile GUI agents, Multimodal Large Language Models, Finite State Machine, error recovery, task execution"}}
{"id": "2505.22904", "pdf": "https://arxiv.org/pdf/2505.22904", "abs": "https://arxiv.org/abs/2505.22904", "authors": ["Youngsoo Choi", "Siu Wun Cheung", "Youngkyu Kim", "Ping-Hsuan Tsai", "Alejandro N. Diaz", "Ivan Zanardi", "Seung Whan Chung", "Dylan Matthew Copeland", "Coleman Kendrick", "William Anderson", "Traian Iliescu", "Matthias Heinkenschloss"], "title": "Defining Foundation Models for Computational Science: A Call for Clarity and Rigor", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "comment": "26 pages, 2 tables, 7 figures", "summary": "The widespread success of foundation models in natural language processing\nand computer vision has inspired researchers to extend the concept to\nscientific machine learning and computational science. However, this position\npaper argues that as the term \"foundation model\" is an evolving concept, its\napplication in computational science is increasingly used without a universally\naccepted definition, potentially creating confusion and diluting its precise\nscientific meaning. In this paper, we address this gap by proposing a formal\ndefinition of foundation models in computational science, grounded in the core\nvalues of generality, reusability, and scalability. We articulate a set of\nessential and desirable characteristics that such models must exhibit, drawing\nparallels with traditional foundational methods, like the finite element and\nfinite volume methods. Furthermore, we introduce the Data-Driven Finite Element\nMethod (DD-FEM), a framework that fuses the modular structure of classical FEM\nwith the representational power of data-driven learning. We demonstrate how\nDD-FEM addresses many of the key challenges in realizing foundation models for\ncomputational science, including scalability, adaptability, and physics\nconsistency. By bridging traditional numerical methods with modern AI\nparadigms, this work provides a rigorous foundation for evaluating and\ndeveloping novel approaches toward future foundation models in computational\nscience.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8ba1\u7b97\u79d1\u5b66\u4e2d\u57fa\u7840\u6a21\u578b\u7684\u6b63\u5f0f\u5b9a\u4e49\uff0c\u5e76\u5f00\u53d1\u4e86\u6570\u636e\u9a71\u52a8\u7684\u6709\u9650\u5143\u65b9\u6cd5\uff08DD-FEM\uff09\u4f5c\u4e3a\u5b9e\u73b0\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u672f\u8bed\u6a21\u7cca\u548c\u6a21\u578b\u5b9e\u7528\u6027\u6311\u6218\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u8ba1\u7b97\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u7f3a\u4e4f\u7edf\u4e00\u5b9a\u4e49\uff0c\u53ef\u80fd\u5bfc\u81f4\u6982\u5ff5\u6df7\u6dc6\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u517c\u987e\u901a\u7528\u6027\u3001\u53ef\u91cd\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u5b9a\u4e49\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u4f20\u7edf\u6709\u9650\u5143\u65b9\u6cd5\uff08FEM\uff09\u7684\u6a21\u5757\u5316\u7ed3\u6784\u4e0e\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u7684\u8868\u5f81\u80fd\u529b\uff0c\u63d0\u51faDD-FEM\u6846\u67b6\u3002", "result": "DD-FEM\u5728\u53ef\u6269\u5c55\u6027\u3001\u9002\u5e94\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8ba1\u7b97\u79d1\u5b66\u4e2d\u7684\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u6574\u5408\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u4e0e\u73b0\u4ee3AI\u8303\u5f0f\uff0c\u4e3a\u8ba1\u7b97\u79d1\u5b66\u4e2d\u7684\u57fa\u7840\u6a21\u578b\u5960\u5b9a\u4e86\u4e25\u683c\u57fa\u7840\u3002", "keywords": "\u57fa\u7840\u6a21\u578b, \u8ba1\u7b97\u79d1\u5b66, \u6709\u9650\u5143\u65b9\u6cd5, \u6570\u636e\u9a71\u52a8\u5b66\u4e60, \u7269\u7406\u4e00\u81f4\u6027"}}
{"id": "2505.22950", "pdf": "https://arxiv.org/pdf/2505.22950", "abs": "https://arxiv.org/abs/2505.22950", "authors": ["Haohan Yuan", "Sukhwa Hong", "Haopeng Zhang"], "title": "StrucSum: Graph-Structured Reasoning for Long Document Extractive Summarization with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown strong performance in zero-shot\nsummarization, but often struggle to model document structure and identify\nsalient information in long texts. In this work, we introduce StrucSum, a\ntraining-free prompting framework that enhances LLM reasoning through\nsentence-level graph structures. StrucSum injects structural signals into\nprompts via three targeted strategies: Neighbor-Aware Prompting (NAP) for local\ncontext, Centrality-Aware Prompting (CAP) for importance estimation, and\nCentrality-Guided Masking (CGM) for efficient input reduction. Experiments on\nArXiv, PubMed, and Multi-News demonstrate that StrucSum consistently improves\nboth summary quality and factual consistency over unsupervised baselines and\nvanilla prompting. Notably, on ArXiv, it boosts FactCC and SummaC by 19.2 and\n9.7 points, indicating stronger alignment between summaries and source content.\nThese findings suggest that structure-aware prompting is a simple yet effective\napproach for zero-shot extractive summarization with LLMs, without any training\nor task-specific tuning.", "AI": {"tldr": "StrucSum improves zero-shot summarization in LLMs by incorporating structural signals via three strategies, enhancing summary quality and factual consistency without training.", "motivation": "LLMs struggle with document structure and salient information in long texts for summarization. StrucSum addresses this by leveraging structural signals to guide the summarization process.", "method": "StrucSum uses three strategies: Neighbor-Aware Prompting (NAP) for local context, Centrality-Aware Prompting (CAP) for importance estimation, and Centrality-Guided Masking (CGM) for input reduction.", "result": "Experiments show StrucSum improves summary quality and factual consistency, with notable gains of 19.2 and 9.7 points on ArXiv for FactCC and SummaC metrics.", "conclusion": "Structure-aware prompting is a simple and effective method for zero-shot extractive summarization with LLMs, requiring no training or task-specific tuning.", "keywords": "LLMs, summarization, zero-shot, structural signals, prompting"}}
{"id": "2505.23667", "pdf": "https://arxiv.org/pdf/2505.23667", "abs": "https://arxiv.org/abs/2505.23667", "authors": ["Lang Cao", "Jingxian Xu", "Hanbing Liu", "Jinyu Wang", "Mengyu Zhou", "Haoyu Dong", "Shi Han", "Dongmei Zhang"], "title": "Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Tables are a fundamental structure for organizing and analyzing data, making\neffective table understanding a critical capability for intelligent systems.\nWhile large language models (LMs) demonstrate strong general reasoning\nabilities, they continue to struggle with accurate numerical or symbolic\nreasoning over tabular data, especially in complex scenarios. Spreadsheet\nformulas provide a powerful and expressive medium for representing executable\nsymbolic operations, encoding rich reasoning patterns that remain largely\nunderutilized. In this paper, we propose Formula Tuning (Fortune), a\nreinforcement learning (RL) framework that trains LMs to generate executable\nspreadsheet formulas for question answering over general tabular data. Formula\nTuning reduces the reliance on supervised formula annotations by using binary\nanswer correctness as a reward signal, guiding the model to learn formula\nderivation through reasoning. We provide a theoretical analysis of its\nadvantages and demonstrate its effectiveness through extensive experiments on\nseven table reasoning benchmarks. Formula Tuning substantially enhances LM\nperformance, particularly on multi-step numerical and symbolic reasoning tasks,\nenabling a 7B model to outperform O1 on table understanding. This highlights\nthe potential of formula-driven RL to advance symbolic table reasoning in LMs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aFormula Tuning\uff08Fortune\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53ef\u6267\u884c\u7535\u5b50\u8868\u683c\u516c\u5f0f\u6765\u56de\u7b54\u8868\u683c\u6570\u636e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u8868\u683c\u7406\u89e3\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u8868\u683c\u6570\u636e\u4e0a\u8fdb\u884c\u6570\u503c\u6216\u7b26\u53f7\u63a8\u7406\u65f6\u7684\u56f0\u96be\uff0c\u63d0\u5347\u5176\u5728\u8868\u683c\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u4e8c\u5143\u7b54\u6848\u6b63\u786e\u6027\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53ef\u6267\u884c\u7535\u5b50\u8868\u683c\u516c\u5f0f\uff0c\u51cf\u5c11\u5bf9\u76d1\u7763\u5f0f\u516c\u5f0f\u6ce8\u91ca\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u4e03\u4e2a\u8868\u683c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u591a\u6b65\u6570\u503c\u548c\u7b26\u53f7\u63a8\u7406\u4efb\u52a1\u4e0a\uff0c7B\u6a21\u578b\u7684\u8868\u73b0\u751a\u81f3\u8d85\u8fc7\u4e86O1\u3002", "conclusion": "Formula Tuning\u8bc1\u660e\u4e86\u901a\u8fc7\u516c\u5f0f\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u7b26\u53f7\u8868\u683c\u63a8\u7406\u4e0a\u7684\u80fd\u529b\u3002", "keywords": "\u8868\u683c\u7406\u89e3, \u8bed\u8a00\u6a21\u578b, \u5f3a\u5316\u5b66\u4e60, \u7535\u5b50\u8868\u683c\u516c\u5f0f, \u7b26\u53f7\u63a8\u7406"}}
{"id": "2505.22913", "pdf": "https://arxiv.org/pdf/2505.22913", "abs": "https://arxiv.org/abs/2505.22913", "authors": ["Donghyeon Joo", "Helya Hosseini", "Ramyad Hadidi", "Bahar Asgari"], "title": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference", "categories": ["cs.LG"], "comment": "19 pages, 9 figures", "summary": "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u975e\u7ed3\u6784\u5316\u7a00\u758f\u6027\u663e\u8457\u63d0\u5347\u4e86LLMs\u4e2dKV\u7f13\u5b58\u7684\u538b\u7f29\u6548\u7387\uff0c\u652f\u6301\u9ad8\u8fbe70%\u7684\u7a00\u758f\u5ea6\u4e14\u65e0\u9700\u727a\u7272\u51c6\u786e\u6027\u6216\u5fae\u8c03\u3002\u901a\u8fc7\u7cfb\u7edf\u63a2\u7d22\u526a\u679d\u7b56\u7565\uff0c\u53d1\u73b0\u57fa\u4e8e\u4ee4\u724c\u5e45\u5ea6\u7684\u526a\u679d\u5bf9Key\u548cValue\u7f13\u5b58\u5747\u975e\u5e38\u6709\u6548\uff0c\u4f18\u4e8e\u5148\u524d\u7684\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6848\u3002\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4d\u56fe\u7684\u7a00\u758f\u683c\u5f0f\u548c\u81ea\u5b9a\u4e49\u6ce8\u610f\u529b\u5185\u6838\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5185\u5b58\u5bc6\u96c6\u578b\u89e3\u7801\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "KV\u7f13\u5b58\u5927\u5c0f\u662f\u89e3\u7801\u6027\u80fd\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u5c24\u5176\u662f\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\uff0c\u5176\u9ad8\u5185\u5b58\u5f00\u9500\u4e25\u91cd\u9650\u5236\u4e86\u6548\u7387\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u7a00\u758f\u6027\u548c\u538b\u7f29\u6280\u672f\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\u3002", "method": "\u91c7\u7528\u975e\u7ed3\u6784\u5316\u7a00\u758f\u6027\u7b56\u7565\uff0c\u5305\u62ec\u57fa\u4e8e\u4ee4\u724c\u5e45\u5ea6\u7684\u526a\u679d\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4f4d\u56fe\u7a00\u758f\u683c\u5f0f\u548c\u81ea\u5b9a\u4e49\u6ce8\u610f\u529b\u5185\u6838\uff0c\u76f4\u63a5\u652f\u6301\u538b\u7f29\u7f13\u5b58\u7684\u8ba1\u7b97\u3002", "result": "KV\u7f13\u5b58\u538b\u7f29\u81f3\u5bc6\u96c6\u63a8\u7406\u768445%\uff0c\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u541e\u5410\u91cf\u5206\u522b\u63d0\u5347\u81f32.23\u500d\u3002", "conclusion": "\u975e\u7ed3\u6784\u5316\u7a00\u758f\u6027\u548c\u5b9a\u5236\u5185\u6838\u7684\u7ed3\u5408\u4e3aLLMs\u7684\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "keywords": "KV\u7f13\u5b58, \u975e\u7ed3\u6784\u5316\u7a00\u758f\u6027, \u526a\u679d\u7b56\u7565, \u6ce8\u610f\u529b\u5185\u6838, \u5185\u5b58\u4f18\u5316"}}
{"id": "2505.22956", "pdf": "https://arxiv.org/pdf/2505.22956", "abs": "https://arxiv.org/abs/2505.22956", "authors": ["Matteo Guida", "Yulia Otmakhova", "Eduard Hovy", "Lea Frermann"], "title": "LLMs for Argument Mining: Detection, Extraction, and Relationship Classification of pre-defined Arguments in Online Comments", "categories": ["cs.CL"], "comment": null, "summary": "Automated large-scale analysis of public discussions around contested issues\nlike abortion requires detecting and understanding the use of arguments. While\nLarge Language Models (LLMs) have shown promise in language processing tasks,\ntheir performance in mining topic-specific, pre-defined arguments in online\ncomments remains underexplored. We evaluate four state-of-the-art LLMs on three\nargument mining tasks using datasets comprising over 2,000 opinion comments\nacross six polarizing topics. Quantitative evaluation suggests an overall\nstrong performance across the three tasks, especially for large and fine-tuned\nLLMs, albeit at a significant environmental cost. However, a detailed error\nanalysis revealed systematic shortcomings on long and nuanced comments and\nemotionally charged language, raising concerns for downstream applications like\ncontent moderation or opinion analysis. Our results highlight both the promise\nand current limitations of LLMs for automated argument analysis in online\ncomments.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u56db\u6b3e\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e09\u4e2a\u8bba\u70b9\u6316\u6398\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793aLLM\u5728\u7279\u5b9a\u4e3b\u9898\u7684\u8bba\u70b9\u6316\u6398\u4e2d\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u4ecd\u5b58\u5728\u957f\u6587\u672c\u3001\u590d\u6742\u5185\u5bb9\u548c\u60c5\u611f\u8bed\u8a00\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22LLM\u5728\u81ea\u52a8\u5316\u5206\u6790\u548c\u6316\u6398\u5728\u7ebf\u8bc4\u8bba\u4e2d\u9884\u5b9a\u4e49\u8bba\u70b9\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u4e89\u8bae\u6027\u8bdd\u9898\uff08\u5982\u5815\u80ce\uff09\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u5305\u542b2000\u591a\u6761\u8bc4\u8bba\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u56db\u6b3eLLM\u5728\u4e09\u4e2a\u8bba\u70b9\u6316\u6398\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5305\u62ec\u5b9a\u91cf\u5206\u6790\u548c\u8be6\u7ec6\u9519\u8bef\u5206\u6790\u3002", "result": "LLM\u5728\u8bba\u70b9\u6316\u6398\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5c24\u5176\u662f\u5927\u6a21\u578b\u548c\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\uff0c\u4f46\u5728\u5904\u7406\u957f\u6587\u672c\u3001\u590d\u6742\u5185\u5bb9\u548c\u60c5\u611f\u8bed\u8a00\u65f6\u5b58\u5728\u7cfb\u7edf\u6027\u4e0d\u8db3\u3002", "conclusion": "LLM\u5728\u81ea\u52a8\u5316\u8bba\u70b9\u5206\u6790\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5f53\u524d\u4ecd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u5185\u5bb9\u5ba1\u6838\u548c\u610f\u89c1\u5206\u6790\u7b49\u4e0b\u6e38\u5e94\u7528\u4e2d\u9700\u8c28\u614e\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u8bba\u70b9\u6316\u6398, \u5728\u7ebf\u8bc4\u8bba, \u81ea\u52a8\u5316\u5206\u6790, \u4e89\u8bae\u8bdd\u9898"}}
{"id": "2505.23686", "pdf": "https://arxiv.org/pdf/2505.23686", "abs": "https://arxiv.org/abs/2505.23686", "authors": ["Caroline Wang", "Arrasy Rahman", "Jiaxun Cui", "Yoonchang Sung", "Peter Stone"], "title": "ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork", "categories": ["cs.AI", "cs.MA", "I.2.11; I.2.1; I.2.6; I.2.8"], "comment": null, "summary": "Developing AI agents capable of collaborating with previously unseen partners\nis a fundamental generalization challenge in multi-agent learning, known as Ad\nHoc Teamwork (AHT). Existing AHT approaches typically adopt a two-stage\npipeline, where first, a fixed population of teammates is generated with the\nidea that they should be representative of the teammates that will be seen at\ndeployment time, and second, an AHT agent is trained to collaborate well with\nagents in the population. To date, the research community has focused on\ndesigning separate algorithms for each stage. This separation has led to\nalgorithms that generate teammate pools with limited coverage of possible\nbehaviors, and that ignore whether the generated teammates are easy to learn\nfrom for the AHT agent. Furthermore, algorithms for training AHT agents\ntypically treat the set of training teammates as static, thus attempting to\ngeneralize to previously unseen partner agents without assuming any control\nover the distribution of training teammates. In this paper, we present a\nunified framework for AHT by reformulating the problem as an open-ended\nlearning process between an ad hoc agent and an adversarial teammate generator.\nWe introduce ROTATE, a regret-driven, open-ended training algorithm that\nalternates between improving the AHT agent and generating teammates that probe\nits deficiencies. Extensive experiments across diverse AHT environments\ndemonstrate that ROTATE significantly outperforms baselines at generalizing to\nan unseen set of evaluation teammates, thus establishing a new standard for\nrobust and generalizable teamwork.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6ROTATE\uff0c\u901a\u8fc7\u5f00\u653e\u5b66\u4e60\u8fc7\u7a0b\u6539\u8fdbAd Hoc Teamwork (AHT)\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfAHT\u65b9\u6cd5\u5206\u4e24\u9636\u6bb5\u4e14\u961f\u53cb\u751f\u6210\u6709\u9650\uff0c\u65e0\u6cd5\u52a8\u6001\u4f18\u5316\u8bad\u7ec3\u961f\u53cb\u5206\u5e03\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faROTATE\u7b97\u6cd5\uff0c\u5c06AHT\u95ee\u9898\u91cd\u6784\u4e3a\u5f00\u653e\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4ea4\u66ff\u4f18\u5316AHT\u667a\u80fd\u4f53\u548c\u751f\u6210\u5bf9\u6297\u6027\u961f\u53cb\u4ee5\u63a2\u7d22\u5176\u7f3a\u9677\u3002", "result": "ROTATE\u5728\u591a\u79cdAHT\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5bf9\u672a\u77e5\u961f\u53cb\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ROTATE\u4e3aAHT\u5efa\u7acb\u4e86\u65b0\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u6807\u51c6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "keywords": "Ad Hoc Teamwork, \u5f00\u653e\u5b66\u4e60, \u5bf9\u6297\u751f\u6210, \u6cdb\u5316\u80fd\u529b"}}
{"id": "2505.22922", "pdf": "https://arxiv.org/pdf/2505.22922", "abs": "https://arxiv.org/abs/2505.22922", "authors": ["Athanasios Glentis", "Jiaxiang Li", "Qiulin Shang", "Andi Han", "Ioannis Tsaknakis", "Quan Wei", "Mingyi Hong"], "title": "Scalable Parameter and Memory Efficient Pretraining for LLM: Recent Algorithmic Advances and Benchmarking", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Fueled by their remarkable ability to tackle diverse tasks across multiple\ndomains, large language models (LLMs) have grown at an unprecedented rate, with\nsome recent models containing trillions of parameters. This growth is\naccompanied by substantial computational challenges, particularly regarding the\nmemory and compute resources required for training and fine-tuning. Numerous\napproaches have been explored to address these issues, such as LoRA. While\nthese methods are effective for fine-tuning, their application to pre-training\nis significantly more challenging due to the need to learn vast datasets.\nMotivated by this issue, we aim to address the following questions: Can\nparameter- or memory-efficient methods enhance pre-training efficiency while\nachieving performance comparable to full-model training? How can the\nperformance gap be narrowed? To this end, the contributions of this work are\nthe following. (1) We begin by conducting a comprehensive survey that\nsummarizes state-of-the-art methods for efficient pre-training. (2) We perform\na benchmark evaluation of several representative memory efficient pre-training\napproaches to comprehensively evaluate their performance across model sizes. We\nobserve that with a proper choice of optimizer and hyperparameters, full-rank\ntraining delivers the best performance, as expected. We also notice that\nincorporating high-rank updates in low-rank approaches is the key to improving\ntheir performance. (3) Finally, we propose two practical techniques, namely\nweight refactorization and momentum reset, to enhance the performance of\nefficient pre-training methods. We observe that applying these techniques to\nthe low-rank method (on a 1B model) can achieve a lower perplexity than popular\nmemory efficient algorithms such as GaLore and Fira, while simultaneously using\nabout 25% less memory.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u53c2\u6570\u6216\u5185\u5b58\u9ad8\u6548\u65b9\u6cd5\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5168\u6a21\u578b\u8bad\u7ec3\u76f8\u5f53\u7684\u6027\u80fd\u3002\u4f5c\u8005\u7efc\u5408\u8c03\u67e5\u4e86\u73b0\u6709\u9ad8\u6548\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u5b9e\u7528\u6280\u672f\u6765\u6539\u8fdb\u4f4e\u79e9\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u589e\u957f\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u901a\u8fc7\u9ad8\u6548\u65b9\u6cd5\u63d0\u5347\u9884\u8bad\u7ec3\u6548\u7387\uff0c\u540c\u65f6\u7f29\u5c0f\u4e0e\u5168\u6a21\u578b\u8bad\u7ec3\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "method": "1. \u7efc\u5408\u8c03\u67e5\u73b0\u6709\u9ad8\u6548\u9884\u8bad\u7ec3\u65b9\u6cd5\uff1b2. \u5bf9\u4ee3\u8868\u6027\u5185\u5b58\u9ad8\u6548\u9884\u8bad\u7ec3\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u8bc4\u4f30\uff1b3. \u63d0\u51fa\u6743\u91cd\u91cd\u6784\u548c\u52a8\u91cf\u91cd\u7f6e\u4e24\u79cd\u6280\u672f\u3002", "result": "\u4f4e\u79e9\u65b9\u6cd5\u7ed3\u5408\u6240\u63d0\u6280\u672f\uff08\u57281B\u6a21\u578b\u4e0a\uff09\u80fd\u6bd4GaLore\u548cFira\u7b49\u6d41\u884c\u7b97\u6cd5\u66f4\u4f4e\u7684\u56f0\u60d1\u5ea6\uff0c\u540c\u65f6\u51cf\u5c11\u7ea625%\u7684\u5185\u5b58\u4f7f\u7528\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u4f4e\u79e9\u65b9\u6cd5\u5e76\u91c7\u7528\u65b0\u6280\u672f\uff0c\u80fd\u663e\u8457\u63d0\u5347\u9884\u8bad\u7ec3\u6548\u7387\u4e0e\u6027\u80fd\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u9884\u8bad\u7ec3,\u5185\u5b58\u9ad8\u6548,\u4f4e\u79e9\u65b9\u6cd5,\u6743\u91cd\u91cd\u6784,\u52a8\u91cf\u91cd\u7f6e"}}
{"id": "2505.22959", "pdf": "https://arxiv.org/pdf/2505.22959", "abs": "https://arxiv.org/abs/2505.22959", "authors": ["Jianwei Wang", "Mengqi Wang", "Yinsi Zhou", "Zhenchang Xing", "Qing Liu", "Xiwei Xu", "Wenjie Zhang", "Liming Zhu"], "title": "LLM-based HSE Compliance Assessment: Benchmark, Performance, and Advancements", "categories": ["cs.CL"], "comment": null, "summary": "Health, Safety, and Environment (HSE) compliance assessment demands dynamic\nreal-time decision-making under complicated regulations and complex\nhuman-machine-environment interactions. While large language models (LLMs) hold\nsignificant potential for decision intelligence and contextual dialogue, their\ncapacity for domain-specific knowledge in HSE and structured legal reasoning\nremains underexplored. We introduce HSE-Bench, the first benchmark dataset\ndesigned to evaluate the HSE compliance assessment capabilities of LLM.\nHSE-Bench comprises over 1,000 manually curated questions drawn from\nregulations, court cases, safety exams, and fieldwork videos, and integrates a\nreasoning flow based on Issue spotting, rule Recall, rule Application, and rule\nConclusion (IRAC) to assess the holistic reasoning pipeline. We conduct\nextensive evaluations on different prompting strategies and more than 10 LLMs,\nincluding foundation models, reasoning models and multimodal vision models. The\nresults show that, although current LLMs achieve good performance, their\ncapabilities largely rely on semantic matching rather than principled reasoning\ngrounded in the underlying HSE compliance context. Moreover, their native\nreasoning trace lacks the systematic legal reasoning required for rigorous HSE\ncompliance assessment. To alleviate these, we propose a new prompting\ntechnique, Reasoning of Expert (RoE), which guides LLMs to simulate the\nreasoning process of different experts for compliance assessment and reach a\nmore accurate unified decision. We hope our study highlights reasoning gaps in\nLLMs for HSE compliance and inspires further research on related tasks.", "AI": {"tldr": "HSE-Bench\u662f\u9996\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5065\u5eb7\u3001\u5b89\u5168\u4e0e\u73af\u5883\uff08HSE\uff09\u5408\u89c4\u8bc4\u4f30\u80fd\u529b\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86LLM\u4f9d\u8d56\u8bed\u4e49\u5339\u914d\u800c\u975e\u539f\u5219\u6027\u63a8\u7406\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e13\u5bb6\u63a8\u7406\u63d0\u793a\u6280\u672f\uff08RoE\uff09\u6539\u8fdb\u5176\u8868\u73b0\u3002", "motivation": "\u52a8\u6001\u5b9e\u65f6\u51b3\u7b56\u5728HSE\u5408\u89c4\u8bc4\u4f30\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46LLM\u5728\u9886\u57df\u77e5\u8bc6\u53ca\u7ed3\u6784\u5316\u6cd5\u5f8b\u63a8\u7406\u4e0a\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efaHSE-Bench\u6570\u636e\u96c6\uff08\u542b1000+\u95ee\u9898\uff0c\u57fa\u4e8eIRAC\u6d41\u7a0b\uff09\uff0c\u8bc4\u4f30\u591a\u79cd\u63d0\u793a\u7b56\u7565\u53ca10\u4f59\u79cdLLM\uff0c\u5e76\u63d0\u51faRoE\u63d0\u793a\u6280\u672f\u6a21\u62df\u4e13\u5bb6\u63a8\u7406\u3002", "result": "\u5f53\u524dLLM\u8868\u73b0\u867d\u597d\uff0c\u4f46\u4f9d\u8d56\u8bed\u4e49\u5339\u914d\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6cd5\u5f8b\u63a8\u7406\uff1bRoE\u6280\u672f\u80fd\u663e\u8457\u63d0\u5347\u5176\u5408\u89c4\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728HSE\u5408\u89c4\u63a8\u7406\u4e2d\u7684\u4e0d\u8db3\uff0cRoE\u4e3a\u5176\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u547c\u5401\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "keywords": "HSE\u5408\u89c4\u8bc4\u4f30\u3001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u57fa\u51c6\u6d4b\u8bd5\u3001IRAC\u3001\u4e13\u5bb6\u63a8\u7406\u63d0\u793a\u6280\u672f"}}
{"id": "2505.23695", "pdf": "https://arxiv.org/pdf/2505.23695", "abs": "https://arxiv.org/abs/2505.23695", "authors": ["Ran Zhang", "Mohannad Elhamod"], "title": "Data-to-Dashboard: Multi-Agent LLM Framework for Insightful Visualization in Enterprise Analytics", "categories": ["cs.AI"], "comment": null, "summary": "The rapid advancement of LLMs has led to the creation of diverse agentic\nsystems in data analysis, utilizing LLMs' capabilities to improve insight\ngeneration and visualization. In this paper, we present an agentic system that\nautomates the data-to-dashboard pipeline through modular LLM agents capable of\ndomain detection, concept extraction, multi-perspective analysis generation,\nand iterative self-reflection. Unlike existing chart QA systems, our framework\nsimulates the analytical reasoning process of business analysts by retrieving\ndomain-relevant knowledge and adapting to diverse datasets without relying on\nclosed ontologies or question templates.\n  We evaluate our system on three datasets across different domains.\nBenchmarked against GPT-4o with a single-prompt baseline, our approach shows\nimproved insightfulness, domain relevance, and analytical depth, as measured by\ntailored evaluation metrics and qualitative human assessment.\n  This work contributes a novel modular pipeline to bridge the path from raw\ndata to visualization, and opens new opportunities for human-in-the-loop\nvalidation by domain experts in business analytics. All code can be found here:\nhttps://github.com/77luvC/D2D_Data2Dashboard", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6a21\u5757\u5316\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u81ea\u52a8\u5316\u6570\u636e\u5230\u4eea\u8868\u76d8\u7684\u6d41\u7a0b\uff0c\u63d0\u5347\u4e86\u6d1e\u5bdf\u751f\u6210\u548c\u53ef\u89c6\u5316\u7684\u80fd\u529b\u3002", "motivation": "\u5229\u7528LLM\u7684\u80fd\u529b\u6539\u8fdb\u6570\u636e\u5206\u6790\u548c\u53ef\u89c6\u5316\uff0c\u6a21\u62df\u4e1a\u52a1\u5206\u6790\u5e08\u7684\u5206\u6790\u63a8\u7406\u8fc7\u7a0b\uff0c\u9002\u5e94\u591a\u6837\u5316\u6570\u636e\u96c6\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316LLM\u4ee3\u7406\uff0c\u5b9e\u73b0\u9886\u57df\u68c0\u6d4b\u3001\u6982\u5ff5\u63d0\u53d6\u3001\u591a\u89c6\u89d2\u5206\u6790\u751f\u6210\u548c\u8fed\u4ee3\u81ea\u53cd\u601d\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4GPT-4o\u5355\u63d0\u793a\u57fa\u7ebf\uff0c\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u6d1e\u5bdf\u529b\u3001\u9886\u57df\u76f8\u5173\u6027\u548c\u5206\u6790\u6df1\u5ea6\u3002", "conclusion": "\u8d21\u732e\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u5757\u5316\u6d41\u7a0b\uff0c\u5c06\u539f\u59cb\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u89c6\u5316\uff0c\u5e76\u4e3a\u4e1a\u52a1\u5206\u6790\u4e2d\u4e13\u5bb6\u53c2\u4e0e\u9a8c\u8bc1\u5f00\u8f9f\u4e86\u65b0\u673a\u4f1a\u3002", "keywords": "LLM, \u6570\u636e\u5206\u6790, \u53ef\u89c6\u5316, \u6a21\u5757\u5316\u4ee3\u7406, \u4e1a\u52a1\u5206\u6790"}}
{"id": "2505.22935", "pdf": "https://arxiv.org/pdf/2505.22935", "abs": "https://arxiv.org/abs/2505.22935", "authors": ["Jipeng Li", "Yanning Shen"], "title": "Is Noise Conditioning Necessary? A Unified Theory of Unconditional Graph Diffusion Models", "categories": ["cs.LG"], "comment": null, "summary": "Explicit noise-level conditioning is widely regarded as essential for the\neffective operation of Graph Diffusion Models (GDMs). In this work, we\nchallenge this assumption by investigating whether denoisers can implicitly\ninfer noise levels directly from corrupted graph structures, potentially\neliminating the need for explicit noise conditioning. To this end, we develop a\ntheoretical framework centered on Bernoulli edge-flip corruptions and extend it\nto encompass more complex scenarios involving coupled structure-attribute\nnoise. Extensive empirical evaluations on both synthetic and real-world graph\ndatasets, using models such as GDSS and DiGress, provide strong support for our\ntheoretical findings. Notably, unconditional GDMs achieve performance\ncomparable or superior to their conditioned counterparts, while also offering\nreductions in parameters (4-6%) and computation time (8-10%). Our results\nsuggest that the high-dimensional nature of graph data itself often encodes\nsufficient information for the denoising process, opening avenues for simpler,\nmore efficient GDM architectures.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6311\u6218\u4e86\u56fe\u6269\u6563\u6a21\u578b\uff08GDMs\uff09\u5fc5\u987b\u663e\u5f0f\u566a\u58f0\u6761\u4ef6\u5316\u7684\u4f20\u7edf\u89c2\u70b9\uff0c\u8bc1\u660e\u65e0\u6761\u4ef6\u7684GDMs\u901a\u8fc7\u9690\u5f0f\u63a8\u65ad\u566a\u58f0\u6c34\u5e73\uff0c\u53ef\u4ee5\u5b9e\u73b0\u4e0e\u4f20\u7edf\u6761\u4ef6\u5316\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u8d28\u7591\u56fe\u6269\u6563\u6a21\u578b\u4e2d\u663e\u5f0f\u566a\u58f0\u6761\u4ef6\u5316\u7684\u5fc5\u8981\u6027\uff0c\u63a2\u8ba8\u53bb\u566a\u5668\u662f\u5426\u80fd\u591f\u76f4\u63a5\u4ece\u635f\u574f\u7684\u56fe\u7ed3\u6784\u4e2d\u9690\u5f0f\u63a8\u65ad\u566a\u58f0\u6c34\u5e73\uff0c\u4ece\u800c\u7b80\u5316\u6a21\u578b\u67b6\u6784\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5efa\u7acb\u4e00\u4e2a\u57fa\u4e8e\u4f2f\u52aa\u5229\u8fb9\u7ffb\u8f6c\u635f\u574f\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u5230\u5305\u542b\u8026\u5408\u7ed3\u6784-\u5c5e\u6027\u566a\u58f0\u7684\u66f4\u590d\u6742\u573a\u666f\uff0c\u540c\u65f6\u5728\u5408\u6210\u548c\u771f\u5b9e\u56fe\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u65e0\u6761\u4ef6GDMs\u5728\u6027\u80fd\u4e0a\u4e0e\u4f20\u7edf\u6761\u4ef6\u5316\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u540c\u65f6\u51cf\u5c11\u4e864-6%\u7684\u53c2\u6570\u548c8-10%\u7684\u8ba1\u7b97\u65f6\u95f4\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u56fe\u6570\u636e\u672c\u8eab\u7684\u9ad8\u7ef4\u7279\u6027\u901a\u5e38\u4e3a\u53bb\u566a\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u8db3\u591f\u7684\u4fe1\u606f\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u7b80\u5355\u9ad8\u6548\u7684GDM\u67b6\u6784\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "keywords": "\u56fe\u6269\u6563\u6a21\u578b, \u566a\u58f0\u6761\u4ef6\u5316, \u9690\u5f0f\u566a\u58f0\u63a8\u65ad, \u4f2f\u52aa\u5229\u8fb9\u7ffb\u8f6c, \u53bb\u566a"}}
{"id": "2505.22961", "pdf": "https://arxiv.org/pdf/2505.22961", "abs": "https://arxiv.org/abs/2505.22961", "authors": ["Peixuan Han", "Zijia Liu", "Jiaxuan You"], "title": "ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have shown promising potential in persuasion,\nbut existing works on training LLM persuaders are still preliminary. Notably,\nwhile humans are skilled in modeling their opponent's thoughts and opinions\nproactively and dynamically, current LLMs struggle with such Theory of Mind\n(ToM) reasoning, resulting in limited diversity and opponent awareness. To\naddress this limitation, we introduce Theory of Mind Augmented Persuader\n(ToMAP), a novel approach for building more flexible persuader agents by\nincorporating two theory of mind modules that enhance the persuader's awareness\nand analysis of the opponent's mental state. Specifically, we begin by\nprompting the persuader to consider possible objections to the target central\nclaim, and then use a text encoder paired with a trained MLP classifier to\npredict the opponent's current stance on these counterclaims. Our carefully\ndesigned reinforcement learning schema enables the persuader learns how to\nanalyze opponent-related information and utilize it to generate more effective\narguments. Experiments show that the ToMAP persuader, while containing only 3B\nparameters, outperforms much larger baselines, like GPT-4o, with a relative\ngain of 39.4% across multiple persuadee models and diverse corpora. Notably,\nToMAP exhibits complex reasoning chains and reduced repetition during training,\nwhich leads to more diverse and effective arguments. The opponent-aware feature\nof ToMAP also makes it suitable for long conversations and enables it to employ\nmore logical and opponent-aware strategies. These results underscore our\nmethod's effectiveness and highlight its potential for developing more\npersuasive language agents. Code is available at:\nhttps://github.com/ulab-uiuc/ToMAP.", "AI": {"tldr": "ToMAP \u662f\u4e00\u79cd\u901a\u8fc7\u7ed3\u5408\u4e24\u4e2a\u5fc3\u667a\u7406\u8bba\u6a21\u5757\u6765\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u8bf4\u670d\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5bf9\u624b\u610f\u8bc6\u548c\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u667a\u7406\u8bba\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u8bf4\u670d\u529b\u548c\u5bf9\u624b\u610f\u8bc6\u7684\u591a\u6837\u6027\u3002", "method": "\u91c7\u7528\u4e24\u4e2a\u5fc3\u667a\u7406\u8bba\u6a21\u5757\uff0c\u7ed3\u5408\u6587\u672c\u7f16\u7801\u5668\u548cMLP\u5206\u7c7b\u5668\u9884\u6d4b\u5bf9\u624b\u7acb\u573a\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u66f4\u6709\u6548\u7684\u8bba\u70b9\u3002", "result": "ToMAP \u5728\u4ec5\u542b30\u4ebf\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u8d85\u8d8a\u66f4\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff08\u5982GPT-4o\uff09\uff0c\u76f8\u5bf9\u63d0\u534739.4%\u3002", "conclusion": "ToMAP \u901a\u8fc7\u589e\u5f3a\u5bf9\u624b\u610f\u8bc6\u548c\u591a\u6837\u6027\uff0c\u5c55\u73b0\u51fa\u5f00\u53d1\u66f4\u5177\u8bf4\u670d\u529b\u8bed\u8a00\u4ee3\u7406\u7684\u6f5c\u529b\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5fc3\u667a\u7406\u8bba, \u8bf4\u670d, \u5f3a\u5316\u5b66\u4e60, \u5bf9\u624b\u610f\u8bc6"}}
{"id": "2505.23703", "pdf": "https://arxiv.org/pdf/2505.23703", "abs": "https://arxiv.org/abs/2505.23703", "authors": ["Ruida Wang", "Yuxin Li", "Yi R.", "Fung", "Tong Zhang"], "title": "Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability", "categories": ["cs.AI"], "comment": null, "summary": "Enhancing the mathematical reasoning capabilities of LLMs has garnered\nsignificant attention in both the mathematical and computer science\ncommunities. Recent works have made substantial progress in both Natural\nLanguage (NL) reasoning and Formal Language (FL) reasoning by leveraging the\npotential of pure Reinforcement Learning (RL) methods on base models. However,\nRL approaches struggle to impart new capabilities not presented in the base\nmodel, highlighting the need to integrate more knowledge like FL into NL math\nreasoning effectively. Yet, this integration is challenging due to inherent\ndisparities in problem structure and reasoning format between NL and FL. To\naddress these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end\nframework designed to incorporate the FL expert into NL math problem-solving.\nTo bridge the NL and FL input format gap, we propose the *NL-FL Problem\nAlignment* method, which reformulates the Question-Answering (QA) problems in\nNL as existence theorems in FL. Subsequently, the *Mixed Problem Input*\ntechnique we provide enables the FL reasoner to handle both QA and existence\nproblems concurrently. Lastly, we mitigate the NL and FL output format gap in\nreasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive\nexperiments demonstrate that the **HybridReasoning** framework achieves\n**89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC\nbenchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively.\nNotably, some problems resolved by our framework remain unsolved by the NL\nbaseline model even under a larger number of trials.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86NL-FL\u6df7\u5408\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7NL-FL\u95ee\u9898\u5bf9\u9f50\u3001\u6df7\u5408\u95ee\u9898\u8f93\u5165\u548cLLM\u7b54\u6848\u63d0\u53d6\u673a\u5236\uff0c\u63d0\u5347\u4e86LLM\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5728MATH-500\u548cAMC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u7eafNL\u65b9\u6cd5\u3002", "motivation": "\u589e\u5f3aLLMs\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u662f\u4e00\u4e2a\u91cd\u8981\u8bfe\u9898\uff0c\u4f46\u7eaf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5728\u57fa\u7840\u6a21\u578b\u4e2d\u5f15\u5165\u65b0\u80fd\u529b\uff0c\u9700\u8981\u6709\u6548\u6574\u5408\u81ea\u7136\u8bed\u8a00\uff08NL\uff09\u548c\u5f62\u5f0f\u8bed\u8a00\uff08FL\uff09\u77e5\u8bc6\u3002", "method": "\u63d0\u51faNL-FL\u6df7\u5408\u63a8\u7406\u6846\u67b6\uff0c\u5305\u62ecNL-FL\u95ee\u9898\u5bf9\u9f50\u5c06QA\u95ee\u9898\u8f6c\u5316\u4e3aFL\u5b58\u5728\u5b9a\u7406\uff0c\u6df7\u5408\u95ee\u9898\u8f93\u5165\u6280\u672f\uff0c\u4ee5\u53ca\u57fa\u4e8eLLM\u7684\u7b54\u6848\u63d0\u53d6\u673a\u5236\u3002", "result": "\u5728MATH-500\u548cAMC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u8fbe\u523089.80%\u548c84.34%\u7684\u51c6\u786e\u7387\uff0c\u6bd4NL\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e864.60%\u548c4.82%\uff0c\u4e14\u89e3\u51b3\u4e86NL\u57fa\u7ebf\u65e0\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u3002", "conclusion": "NL-FL\u6df7\u5408\u63a8\u7406\u6846\u67b6\u901a\u8fc7\u6574\u5408FL\u548cNL\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "keywords": "\u6570\u5b66\u63a8\u7406\u3001NL-FL\u6df7\u5408\u63a8\u7406\u3001\u95ee\u9898\u5bf9\u9f50\u3001\u7b54\u6848\u63d0\u53d6\u3001LLMs"}}
{"id": "2505.22949", "pdf": "https://arxiv.org/pdf/2505.22949", "abs": "https://arxiv.org/abs/2505.22949", "authors": ["Michael Sun", "Orion Foo", "Gang Liu", "Wojciech Matusik", "Jie Chen"], "title": "Directed Graph Grammars for Sequence-based Learning", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Directed acyclic graphs (DAGs) are a class of graphs commonly used in\npractice, with examples that include electronic circuits, Bayesian networks,\nand neural architectures. While many effective encoders exist for DAGs, it\nremains challenging to decode them in a principled manner, because the nodes of\na DAG can have many different topological orders. In this work, we propose a\ngrammar-based approach to constructing a principled, compact and equivalent\nsequential representation of a DAG. Specifically, we view a graph as\nderivations over an unambiguous grammar, where the DAG corresponds to a unique\nsequence of production rules. Equivalently, the procedure to construct such a\ndescription can be viewed as a lossless compression of the data. Such a\nrepresentation has many uses, including building a generative model for graph\ngeneration, learning a latent space for property prediction, and leveraging the\nsequence representational continuity for Bayesian Optimization over structured\ndata. Code is available at https://github.com/shiningsunnyday/induction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u6cd5\u7684\u65b9\u6cd5\uff0c\u5c06DAG\uff08\u6709\u5411\u65e0\u73af\u56fe\uff09\u8f6c\u6362\u4e3a\u7d27\u51d1\u4e14\u7b49\u4ef7\u7684\u5e8f\u5217\u8868\u793a\uff0c\u89e3\u51b3\u4e86DAG\u89e3\u7801\u7684\u6311\u6218\u3002", "motivation": "DAG\u5728\u7535\u5b50\u7535\u8def\u3001\u8d1d\u53f6\u65af\u7f51\u7edc\u548c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u4e00\u79cd\u539f\u5219\u6027\u7684\u89e3\u7801\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8bed\u6cd5\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06DAG\u89c6\u4e3a\u4e00\u79cd\u660e\u786e\u8bed\u6cd5\u4e2d\u7684\u63a8\u5bfc\u8fc7\u7a0b\uff0c\u751f\u6210\u552f\u4e00\u7684\u4ea7\u751f\u89c4\u5219\u5e8f\u5217\uff0c\u76f8\u5f53\u4e8e\u5bf9\u6570\u636e\u8fdb\u884c\u65e0\u635f\u538b\u7f29\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3aDAG\u63d0\u4f9b\u4e86\u7d27\u51d1\u4e14\u7b49\u4ef7\u7684\u5e8f\u5217\u8868\u793a\uff0c\u9002\u7528\u4e8e\u56fe\u751f\u6210\u3001\u5c5e\u6027\u9884\u6d4b\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u7b49\u4efb\u52a1\u3002", "conclusion": "\u8bed\u6cd5\u65b9\u6cd5\u4e3aDAG\u7684\u89e3\u7801\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u3001\u8bed\u6cd5\u3001\u65e0\u635f\u538b\u7f29\u3001\u5e8f\u5217\u8868\u793a\u3001\u56fe\u751f\u6210"}}
{"id": "2505.22964", "pdf": "https://arxiv.org/pdf/2505.22964", "abs": "https://arxiv.org/abs/2505.22964", "authors": ["Sheng Zhang", "Qin Liu", "Naoto Usuyama", "Cliff Wong", "Tristan Naumann", "Hoifung Poon"], "title": "Exploring Scaling Laws for EHR Foundation Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The emergence of scaling laws has profoundly shaped the development of large\nlanguage models (LLMs), enabling predictable performance gains through\nsystematic increases in model size, dataset volume, and compute. Yet, these\nprinciples remain largely unexplored in the context of electronic health\nrecords (EHRs) -- a rich, sequential, and globally abundant data source that\ndiffers structurally from natural language. In this work, we present the first\nempirical investigation of scaling laws for EHR foundation models. By training\ntransformer architectures on patient timeline data from the MIMIC-IV database\nacross varying model sizes and compute budgets, we identify consistent scaling\npatterns, including parabolic IsoFLOPs curves and power-law relationships\nbetween compute, model parameters, data size, and clinical utility. These\nfindings demonstrate that EHR models exhibit scaling behavior analogous to\nLLMs, offering predictive insights into resource-efficient training strategies.\nOur results lay the groundwork for developing powerful EHR foundation models\ncapable of transforming clinical prediction tasks and advancing personalized\nhealthcare.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u63a2\u7a76\u4e86\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u9886\u57df\u7684\u6269\u5c55\u89c4\u5f8b\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u53d1\u73b0EHR\u6a21\u578b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5177\u6709\u76f8\u4f3c\u7684\u6269\u5c55\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u9886\u57df\u4e2d\u7684\u6269\u5c55\u89c4\u5f8b\uff0c\u4ee5\u586b\u8865EHR\u6570\u636e\u4e0e\u81ea\u7136\u8bed\u8a00\u6570\u636e\u5728\u6269\u5c55\u6cd5\u5219\u7814\u7a76\u4e2d\u7684\u7a7a\u767d\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5728MIMIC-IV\u6570\u636e\u5e93\u7684\u60a3\u8005\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0a\u8bad\u7ec3\u4e0d\u540c\u89c4\u6a21\u7684Transformer\u67b6\u6784\uff0c\u5206\u6790\u6a21\u578b\u53c2\u6570\u3001\u8ba1\u7b97\u8d44\u6e90\u3001\u6570\u636e\u91cf\u4e0e\u4e34\u5e8a\u6548\u7528\u7684\u5173\u7cfb\u3002", "result": "\u7ed3\u679c\u53d1\u73b0EHR\u6a21\u578b\u7684\u6269\u5c55\u89c4\u5f8b\u4e0eLLM\u7c7b\u4f3c\uff0c\u5305\u62ec\u629b\u7269\u7ebfIsoFLOPs\u66f2\u7ebf\u548c\u5e42\u5f8b\u5173\u7cfb\uff0c\u4e3a\u8d44\u6e90\u9ad8\u6548\u8bad\u7ec3\u7b56\u7565\u63d0\u4f9b\u4e86\u9884\u6d4b\u6027\u89c1\u89e3\u3002", "conclusion": "\u7ed3\u8bba\u662fEHR\u6a21\u578b\u5c55\u73b0\u51fa\u4e0eLLM\u76f8\u4f3c\u7684\u6269\u5c55\u884c\u4e3a\uff0c\u4e3a\u5f00\u53d1\u5f3a\u5927\u7684EHR\u57fa\u7840\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u4e2a\u6027\u5316\u533b\u7597\u7684\u53d1\u5c55\u3002", "keywords": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u3001\u6269\u5c55\u89c4\u5f8b\u3001Transformer\u6a21\u578b\u3001\u8d44\u6e90\u6548\u7387\u3001\u4e34\u5e8a\u9884\u6d4b"}}
{"id": "2505.23746", "pdf": "https://arxiv.org/pdf/2505.23746", "abs": "https://arxiv.org/abs/2505.23746", "authors": ["Hugo Henry", "Kelly Cohen"], "title": "Comparative of Genetic Fuzzy regression techniques for aeroacoustic phenomenons", "categories": ["cs.AI", "cs.NE"], "comment": "11 pages and 23 figures", "summary": "This study investigates the application of Genetic Fuzzy Systems (GFS) to\nmodel the self-noise generated by airfoils, a key issue in aeroaccoustics with\nsignificant implications for aerospace, automotive and drone applications.\nUsing the publicly available Airfoil Self Noise dataset, various Fuzzy\nregression strategies are explored and compared. The paper evaluates a brute\nforce Takagi Sugeno Kang (TSK) fuzzy system with high rule density, a cascading\nGeneti Fuzzy Tree (GFT) architecture and a novel clustered approach based on\nFuzzy C-means (FCM) to reduce the model's complexity. This highlights the\nviability of clustering assisted fuzzy inference as an effective regression\ntool for complex aero accoustic phenomena. Keywords : Fuzzy logic, Regression,\nCascading systems, Clustering and AI.", "AI": {"tldr": "The study explores Genetic Fuzzy Systems (GFS) for modeling airfoil self-noise, comparing various fuzzy regression strategies, including TSK, GFT, and a novel FCM-based approach, demonstrating their effectiveness in aeroacoustics.", "motivation": "Addressing the challenge of modeling self-noise in airfoils, which is crucial for industries like aerospace, automotive, and drones.", "method": "Evaluates brute force TSK, cascading GFT, and a clustered FCM approach to reduce model complexity.", "result": "Shows the effectiveness of clustering-assisted fuzzy inference for complex aeroacoustic regression tasks.", "conclusion": "Clustering-based fuzzy systems are viable tools for modeling intricate aeroacoustic phenomena.", "keywords": "Fuzzy logic, Regression, Cascading systems, Clustering, AI"}}
{"id": "2505.22967", "pdf": "https://arxiv.org/pdf/2505.22967", "abs": "https://arxiv.org/abs/2505.22967", "authors": ["Chengqi Zheng", "Jianda Chen", "Yueming Lyu", "Wen Zheng Terence Ng", "Haopeng Zhang", "Yew-Soon Ong", "Ivor Tsang", "Haiyan Yin"], "title": "MermaidFlow: Redefining Agentic Workflow Generation via Safety-Constrained Evolutionary Programming", "categories": ["cs.LG", "cs.MA"], "comment": null, "summary": "Despite the promise of autonomous agentic reasoning, existing workflow\ngeneration methods frequently produce fragile, unexecutable plans due to\nunconstrained LLM-driven construction. We introduce MermaidFlow, a framework\nthat redefines the agentic search space through safety-constrained graph\nevolution. At its core, MermaidFlow represent workflows as a verifiable\nintermediate representation using Mermaid, a structured and human-interpretable\ngraph language. We formulate domain-aware evolutionary operators, i.e.,\ncrossover, mutation, insertion, and deletion, to preserve semantic correctness\nwhile promoting structural diversity, enabling efficient exploration of a\nhigh-quality, statically verifiable workflow space. Without modifying task\nsettings or evaluation protocols, MermaidFlow achieves consistent improvements\nin success rates and faster convergence to executable plans on the agent\nreasoning benchmark. The experimental results demonstrate that\nsafety-constrained graph evolution offers a scalable, modular foundation for\nrobust and interpretable agentic reasoning systems.", "AI": {"tldr": "MermaidFlow\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u7ea6\u675f\u7684\u56fe\u6f14\u5316\u6846\u67b6\uff0c\u901a\u8fc7Mermaid\u4e2d\u95f4\u8868\u793a\u548c\u9886\u57df\u611f\u77e5\u7684\u8fdb\u5316\u64cd\u4f5c\uff0c\u63d0\u5347\u5de5\u4f5c\u6d41\u751f\u6210\u7684\u7a33\u5065\u6027\u548c\u53ef\u6267\u884c\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u5de5\u4f5c\u6d41\u751f\u6210\u65b9\u6cd5\u5e38\u4ea7\u751f\u8106\u5f31\u3001\u4e0d\u53ef\u6267\u884c\u7684\u8ba1\u5212\uff0c\u7f3a\u4e4f\u7ea6\u675f\u6761\u4ef6\u4fdd\u969c\u3002", "method": "\u4f7f\u7528Mermaid\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u7684\u4e2d\u95f4\u8868\u793a\uff0c\u7ed3\u5408\u4ea4\u53c9\u3001\u53d8\u5f02\u7b49\u8fdb\u5316\u64cd\u4f5c\uff0c\u786e\u4fdd\u8bed\u4e49\u6b63\u786e\u6027\u548c\u7ed3\u6784\u591a\u6837\u6027\u3002", "result": "\u5728\u6807\u51c6\u6d4b\u8bd5\u4e2d\uff0cMermaidFlow\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u5e76\u52a0\u901f\u6536\u655b\u81f3\u53ef\u6267\u884c\u8ba1\u5212\u3002", "conclusion": "\u5b89\u5168\u7ea6\u675f\u7684\u56fe\u6f14\u5316\u4e3a\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u7684\u667a\u80fd\u4f53\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6a21\u5757\u5316\u57fa\u7840\u3002", "keywords": "\u81ea\u4e3b\u667a\u80fd\u4f53\u3001\u5de5\u4f5c\u6d41\u751f\u6210\u3001\u56fe\u6f14\u5316\u3001Mermaid\u3001\u8bed\u4e49\u6b63\u786e\u6027"}}
{"id": "2505.22993", "pdf": "https://arxiv.org/pdf/2505.22993", "abs": "https://arxiv.org/abs/2505.22993", "authors": ["Hoang Pham", "Thanh-Do Nguyen", "Khac-Hoai Nam Bui"], "title": "Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": "Published at NAACL 2025 Main Conference", "summary": "Claim verification is a long-standing and challenging task that demands not\nonly high accuracy but also explainability of the verification process. This\ntask becomes an emerging research issue in the era of large language models\n(LLMs) since real-world claims are often complex, featuring intricate semantic\nstructures or obfuscated entities. Traditional approaches typically address\nthis by decomposing claims into sub-claims and querying a knowledge base to\nresolve hidden or ambiguous entities. However, the absence of effective\ndisambiguation strategies for these entities can compromise the entire\nverification process. To address these challenges, we propose\nVerify-in-the-Graph (VeGraph), a novel framework leveraging the reasoning and\ncomprehension abilities of LLM agents. VeGraph operates in three phases: (1)\nGraph Representation - an input claim is decomposed into structured triplets,\nforming a graph-based representation that integrates both structured and\nunstructured information; (2) Entity Disambiguation -VeGraph iteratively\ninteracts with the knowledge base to resolve ambiguous entities within the\ngraph for deeper sub-claim verification; and (3) Verification - remaining\ntriplets are verified to complete the fact-checking process. Experiments using\nMeta-Llama-3-70B (instruct version) show that VeGraph achieves competitive\nperformance compared to baselines on two benchmarks HoVer and FEVEROUS,\neffectively addressing claim verification challenges. Our source code and data\nare available for further exploitation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVeGraph\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528LLM\u4ee3\u7406\u7684\u80fd\u529b\u5206\u4e09\u6b65\u9a8c\u8bc1\u590d\u6742\u58f0\u660e\uff1a\u56fe\u8868\u793a\u3001\u5b9e\u4f53\u6d88\u6b67\u548c\u9a8c\u8bc1\uff0c\u5728HoVer\u548cFEVEROUS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9762\u5bf9\u590d\u6742\u58f0\u660e\u7684\u9a8c\u8bc1\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u56e0\u5b9e\u4f53\u6d88\u6b67\u4e0d\u8db3\u800c\u6548\u679c\u6709\u9650\uff0c\u9700\u7ed3\u5408LLM\u7684\u63a8\u7406\u548c\u7406\u89e3\u80fd\u529b\u63d0\u5347\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "VeGraph\u5206\u4e09\u4e2a\u9636\u6bb5\uff1a\u56fe\u8868\u793a\uff08\u5c06\u58f0\u660e\u5206\u89e3\u4e3a\u4e09\u5143\u7ec4\u56fe\uff09\u3001\u5b9e\u4f53\u6d88\u6b67\uff08\u8fed\u4ee3\u4ea4\u4e92\u77e5\u8bc6\u5e93\u6d88\u6b67\uff09\u3001\u9a8c\u8bc1\uff08\u5b8c\u6210\u4e09\u5143\u7ec4\u9a8c\u8bc1\uff09\u3002", "result": "\u5728Meta-Llama-3-70B\u6a21\u578b\u4e0a\uff0cVeGraph\u5728HoVer\u548cFEVEROUS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "VeGraph\u901a\u8fc7\u7ed3\u6784\u5316\u89e3\u6784\u548c\u8fed\u4ee3\u6d88\u6b67\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u58f0\u660e\u7684\u9a8c\u8bc1\u80fd\u529b\uff0c\u4e3aLLM\u5728\u4e8b\u5b9e\u6838\u67e5\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "claim verification, LLM agents, entity disambiguation, graph representation, VeGraph"}}
{"id": "2505.23762", "pdf": "https://arxiv.org/pdf/2505.23762", "abs": "https://arxiv.org/abs/2505.23762", "authors": ["Chenyu Yang", "Shiqian Su", "Shi Liu", "Xuan Dong", "Yue Yu", "Weijie Su", "Xuehui Wang", "Zhaoyang Liu", "Jinguo Zhu", "Hao Li", "Wenhai Wang", "Yu Qiao", "Xizhou Zhu", "Jifeng Dai"], "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The rapid advancement of large Vision-Language Models (VLMs) has propelled\nthe development of pure-vision-based GUI Agents, capable of perceiving and\noperating Graphical User Interfaces (GUI) to autonomously fulfill user\ninstructions. However, existing approaches usually adopt an offline learning\nframework, which faces two core limitations: (1) heavy reliance on high-quality\nmanual annotations for element grounding and action supervision, and (2)\nlimited adaptability to dynamic and interactive environments. To address these\nlimitations, we propose ZeroGUI, a scalable, online learning framework for\nautomating GUI Agent training at Zero human cost. Specifically, ZeroGUI\nintegrates (i) VLM-based automatic task generation to produce diverse training\ngoals from the current environment state, (ii) VLM-based automatic reward\nestimation to assess task success without hand-crafted evaluation functions,\nand (iii) two-stage online reinforcement learning to continuously interact with\nand learn from GUI environments. Experiments on two advanced GUI Agents\n(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance\nacross OSWorld and AndroidLab environments. The code is available at\nhttps://github.com/OpenGVLab/ZeroGUI.", "AI": {"tldr": "ZeroGUI\u662f\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u5728\u7ebf\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u4efb\u52a1\u751f\u6210\u548c\u5956\u52b1\u4f30\u8ba1\u63d0\u5347GUI\u4ee3\u7406\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u8d28\u91cf\u4eba\u5de5\u6807\u6ce8\u4e14\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\uff0cZeroGUI\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u96c6\u6210VLM\u81ea\u52a8\u751f\u6210\u4efb\u52a1\u548c\u5956\u52b1\u4f30\u8ba1\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e0eGUI\u73af\u5883\u4ea4\u4e92\u3002", "result": "\u5728OSWorld\u548cAndroidLab\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86UI-TARS\u548cAguvis\u7684\u6027\u80fd\u3002", "conclusion": "ZeroGUI\u5b9e\u73b0\u4e86\u96f6\u4eba\u5de5\u6210\u672c\u7684\u9ad8\u6548GUI\u4ee3\u7406\u8bad\u7ec3\uff0c\u5177\u6709\u5f3a\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "keywords": "Vision-Language Models, GUI Agents, online learning, reinforcement learning"}}
{"id": "2505.22973", "pdf": "https://arxiv.org/pdf/2505.22973", "abs": "https://arxiv.org/abs/2505.22973", "authors": ["Bahareh Tolooshams", "Aditi Chandrashekar", "Rayhan Zirvi", "Abbas Mammadov", "Jiachen Yao", "Chuwei Wang", "Anima Anandkumar"], "title": "EquiReg: Equivariance Regularized Diffusion for Inverse Problems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diffusion models represent the state-of-the-art for solving inverse problems\nsuch as image restoration tasks. In the Bayesian framework, diffusion-based\ninverse solvers incorporate a likelihood term to guide the prior sampling\nprocess, generating data consistent with the posterior distribution. However,\ndue to the intractability of the likelihood term, many current methods rely on\nisotropic Gaussian approximations, which lead to deviations from the data\nmanifold and result in inconsistent, unstable reconstructions. We propose\nEquivariance Regularized (EquiReg) diffusion, a general framework for\nregularizing posterior sampling in diffusion-based inverse problem solvers.\nEquiReg enhances reconstructions by reweighting diffusion trajectories and\npenalizing those that deviate from the data manifold. We define a new\ndistribution-dependent equivariance error, empirically identify functions that\nexhibit low error for on-manifold samples and higher error for off-manifold\nsamples, and leverage these functions to regularize the diffusion sampling\nprocess. When applied to a variety of solvers, EquiReg outperforms\nstate-of-the-art diffusion models in both linear and nonlinear image\nrestoration tasks, as well as in reconstructing partial differential equations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EquiReg\u6269\u6563\u6a21\u578b\uff0c\u4e00\u79cd\u6b63\u5219\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u6269\u6563\u8f68\u8ff9\u5e76\u60e9\u7f5a\u504f\u79bb\u6570\u636e\u6d41\u5f62\u7684\u6837\u672c\uff0c\u63d0\u5347\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9006\u95ee\u9898\u6c42\u89e3\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9006\u95ee\u9898\u6c42\u89e3\u5668\u56e0\u4f3c\u7136\u9879\u7684\u4e0d\u53ef\u5904\u7406\u6027\uff0c\u5e38\u91c7\u7528\u5404\u5411\u540c\u6027\u9ad8\u65af\u8fd1\u4f3c\uff0c\u5bfc\u81f4\u504f\u79bb\u6570\u636e\u6d41\u5f62\uff0c\u4ea7\u751f\u4e0d\u4e00\u81f4\u548c\u4e0d\u7a33\u5b9a\u7684\u91cd\u5efa\u7ed3\u679c\u3002", "method": "\u63d0\u51faEquiReg\u6846\u67b6\uff0c\u5f15\u5165\u5206\u5e03\u4f9d\u8d56\u7684\u7b49\u53d8\u6027\u8bef\u5dee\u5b9a\u4e49\uff0c\u8bc6\u522b\u5bf9\u6570\u636e\u6d41\u5f62\u5185\u5916\u6837\u672c\u8868\u73b0\u4e0d\u540c\u7684\u51fd\u6570\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u51fd\u6570\u6b63\u5219\u5316\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u7c7b\u6c42\u89e3\u5668\u4e2d\u5e94\u7528EquiReg\uff0c\u5728\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u53ca\u504f\u5fae\u5206\u65b9\u7a0b\u91cd\u5efa\u4e2d\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\u3002", "conclusion": "EquiReg\u901a\u8fc7\u6b63\u5219\u5316\u91c7\u6837\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u5728\u9006\u95ee\u9898\u4e2d\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u3002", "keywords": "\u6269\u6563\u6a21\u578b,\u9006\u95ee\u9898,\u56fe\u50cf\u6062\u590d,\u6b63\u5219\u5316,\u504f\u5fae\u5206\u65b9\u7a0b"}}
{"id": "2505.23001", "pdf": "https://arxiv.org/pdf/2505.23001", "abs": "https://arxiv.org/abs/2505.23001", "authors": ["Yize Cheng", "Wenxiao Wang", "Mazda Moayeri", "Soheil Feizi"], "title": "DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors", "categories": ["cs.CL"], "comment": null, "summary": "Open benchmarks are essential for evaluating and advancing large language\nmodels, offering reproducibility and transparency. However, their accessibility\nmakes them likely targets of test set contamination. In this work, we introduce\nDyePack, a framework that leverages backdoor attacks to identify models that\nused benchmark test sets during training, without requiring access to the loss,\nlogits, or any internal details of the model. Like how banks mix dye packs with\ntheir money to mark robbers, DyePack mixes backdoor samples with the test data\nto flag models that trained on it. We propose a principled design incorporating\nmultiple backdoors with stochastic targets, enabling exact false positive rate\n(FPR) computation when flagging every model. This provably prevents false\naccusations while providing strong evidence for every detected case of\ncontamination. We evaluate DyePack on five models across three datasets,\ncovering both multiple-choice and open-ended generation tasks. For\nmultiple-choice questions, it successfully detects all contaminated models with\nguaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard\nusing eight backdoors. For open-ended generation tasks, it generalizes well and\nidentifies all contaminated models on Alpaca with a guaranteed false positive\nrate of just 0.127% using six backdoors.", "AI": {"tldr": "DyePack \u662f\u4e00\u79cd\u901a\u8fc7\u540e\u95e8\u653b\u51fb\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u96c6\u6c61\u67d3\u7684\u6846\u67b6\uff0c\u65e0\u9700\u6a21\u578b\u5185\u90e8\u4fe1\u606f\u5373\u53ef\u8bc6\u522b\u8bad\u7ec3\u4e2d\u4f7f\u7528\u4e86\u6d4b\u8bd5\u96c6\u7684\u6a21\u578b\u3002", "motivation": "\u5f00\u6e90\u7684\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u6613\u83b7\u53d6\u6027\u53ef\u80fd\u5bfc\u81f4\u6d4b\u8bd5\u96c6\u6c61\u67d3\u3002DyePack \u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5728\u540e\u95e8\u6837\u672c\u4e2d\u5d4c\u5165\u968f\u673a\u76ee\u6807\uff0c\u8bbe\u8ba1\u591a\u540e\u95e8\u7b56\u7565\uff0c\u5e76\u8ba1\u7b97\u7cbe\u786e\u7684\u5047\u9633\u6027\u7387\uff08FPR\uff09\u3002", "result": "\u5728\u591a\u9879\u9009\u62e9\u9898\u4e2d\uff0cFPR \u4f4e\u81f3 0.000073%\uff08MMLU-Pro\uff09\u548c 0.000017%\uff08Big Bench-Hard\uff09\uff1b\u5f00\u653e\u751f\u6210\u4efb\u52a1\u4e2d FPR \u4e3a 0.127%\uff08Alpaca\uff09\u3002", "conclusion": "DyePack \u9ad8\u6548\u68c0\u6d4b\u6c61\u67d3\uff0c\u4e14\u5047\u9633\u6027\u7387\u6781\u4f4e\uff0c\u9002\u7528\u4e8e\u591a\u9879\u4efb\u52a1\u3002", "keywords": "\u57fa\u51c6\u6d4b\u8bd5, \u540e\u95e8\u653b\u51fb, \u6d4b\u8bd5\u96c6\u6c61\u67d3, \u5047\u9633\u6027\u7387, \u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.22673", "pdf": "https://arxiv.org/pdf/2505.22673", "abs": "https://arxiv.org/abs/2505.22673", "authors": ["Wasif Khan", "Kyle B. See", "Simon Kato", "Ziqian Huang", "Amy Lazarte", "Kyle Douglas", "Xiangyang Lou", "Teng J. Peng", "Dhanashree Rajderkar", "John Rees", "Pina Sanelli", "Amita Singh", "Ibrahim Tuna", "Christina A. Wilson", "Ruogu Fang"], "title": "Physiology-Informed Generative Multi-Task Network for Contrast-Free CT Perfusion", "categories": ["q-bio.TO", "cs.AI", "cs.CV"], "comment": "Under Review", "summary": "Perfusion imaging is extensively utilized to assess hemodynamic status and\ntissue perfusion in various organs. Computed tomography perfusion (CTP) imaging\nplays a key role in the early assessment and planning of stroke treatment.\nWhile CTP provides essential perfusion parameters to identify abnormal blood\nflow in the brain, the use of contrast agents in CTP can lead to allergic\nreactions and adverse side effects, along with costing USD 4.9 billion\nworldwide in 2022. To address these challenges, we propose a novel deep\nlearning framework called Multitask Automated Generation of Intermodal CT\nperfusion maps (MAGIC). This framework combines generative artificial\nintelligence and physiological information to map non-contrast computed\ntomography (CT) imaging to multiple contrast-free CTP imaging maps. We\ndemonstrate enhanced image fidelity by incorporating physiological\ncharacteristics into the loss terms. Our network was trained and validated\nusing CT image data from patients referred for stroke at UF Health and\ndemonstrated robustness to abnormalities in brain perfusion activity. A\ndouble-blinded study was conducted involving seven experienced\nneuroradiologists and vascular neurologists. This study validated MAGIC's\nvisual quality and diagnostic accuracy showing favorable performance compared\nto clinical perfusion imaging with intravenous contrast injection. Overall,\nMAGIC holds great promise in revolutionizing healthcare by offering\ncontrast-free, cost-effective, and rapid perfusion imaging.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMAGIC\u7684\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u548c\u751f\u7406\u4fe1\u606f\uff0c\u5c06\u975e\u5bf9\u6bd4\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff08CT\uff09\u6210\u50cf\u6620\u5c04\u5230\u591a\u79cd\u65e0\u5bf9\u6bd4\u7684CT\u704c\u6ce8\u6210\u50cf\u56fe\uff0c\u65e8\u5728\u89e3\u51b3CT\u704c\u6ce8\u6210\u50cf\u4e2d\u5bf9\u6bd4\u5242\u5f15\u8d77\u7684\u8fc7\u654f\u53cd\u5e94\u548c\u9ad8\u6210\u672c\u95ee\u9898\u3002", "motivation": "CT\u704c\u6ce8\u6210\u50cf\u5728\u65e9\u671f\u8bc4\u4f30\u548c\u4e2d\u98ce\u6cbb\u7597\u89c4\u5212\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u5bf9\u6bd4\u5242\u7684\u4f7f\u7528\u53ef\u80fd\u5bfc\u81f4\u8fc7\u654f\u53cd\u5e94\u548c\u526f\u4f5c\u7528\uff0c\u4e14\u6210\u672c\u9ad8\u6602\uff0c2022\u5e74\u5168\u7403\u82b1\u8d39\u8fbe49\u4ebf\u7f8e\u5143\u3002", "method": "MAGIC\u6846\u67b6\u7ed3\u5408\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u548c\u751f\u7406\u4fe1\u606f\uff0c\u901a\u8fc7\u5c06\u975e\u5bf9\u6bd4CT\u6210\u50cf\u6620\u5c04\u5230\u65e0\u5bf9\u6bd4CT\u704c\u6ce8\u6210\u50cf\u56fe\uff0c\u5e76\u5728\u635f\u5931\u51fd\u6570\u4e2d\u5f15\u5165\u751f\u7406\u7279\u5f81\u4ee5\u589e\u5f3a\u56fe\u50cf\u4fdd\u771f\u5ea6\u3002", "result": "\u5728UF Health\u7684\u4e2d\u98ce\u60a3\u8005CT\u56fe\u50cf\u6570\u636e\u4e0a\u8bad\u7ec3\u548c\u9a8c\u8bc1\uff0cMAGIC\u5c55\u73b0\u4e86\u5bf9\u8111\u704c\u6ce8\u6d3b\u52a8\u5f02\u5e38\u7684\u9c81\u68d2\u6027\u3002\u53cc\u76f2\u7814\u7a76\u663e\u793a\uff0c\u5176\u89c6\u89c9\u8d28\u91cf\u548c\u8bca\u65ad\u51c6\u786e\u6027\u4f18\u4e8e\u4e34\u5e8a\u704c\u6ce8\u6210\u50cf\u3002", "conclusion": "MAGIC\u6709\u671b\u901a\u8fc7\u63d0\u4f9b\u65e0\u5bf9\u6bd4\u3001\u7ecf\u6d4e\u3001\u5feb\u901f\u7684\u704c\u6ce8\u6210\u50cf\uff0c\u9769\u65b0\u533b\u7597\u4fdd\u5065\u3002", "keywords": "CT\u704c\u6ce8\u6210\u50cf, \u6df1\u5ea6\u5b66\u4e60, \u65e0\u5bf9\u6bd4\u6210\u50cf, MAGIC, \u4e2d\u98ce\u8bc4\u4f30"}}
{"id": "2505.22984", "pdf": "https://arxiv.org/pdf/2505.22984", "abs": "https://arxiv.org/abs/2505.22984", "authors": ["Guancheng Zhou", "Haiping Xu", "Hongkang Xu", "Chenyu Li", "Donghui Yan"], "title": "A Computational Approach to Improving Fairness in K-means Clustering", "categories": ["cs.LG", "cs.CY"], "comment": "14 pages, 5 figures", "summary": "The popular K-means clustering algorithm potentially suffers from a major\nweakness for further analysis or interpretation. Some cluster may have\ndisproportionately more (or fewer) points from one of the subpopulations in\nterms of some sensitive variable, e.g., gender or race. Such a fairness issue\nmay cause bias and unexpected social consequences. This work attempts to\nimprove the fairness of K-means clustering with a two-stage optimization\nformulation--clustering first and then adjust cluster membership of a small\nsubset of selected data points. Two computationally efficient algorithms are\nproposed in identifying those data points that are expensive for fairness, with\none focusing on nearest data points outside of a cluster and the other on\nhighly 'mixed' data points. Experiments on benchmark datasets show substantial\nimprovement on fairness with a minimal impact to clustering quality. The\nproposed algorithms can be easily extended to a broad class of clustering\nalgorithms or fairness metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u5c11\u91cf\u6570\u636e\u70b9\u7684\u805a\u7c7b\u6210\u5458\u8eab\u4efd\uff0c\u63d0\u5347K-means\u805a\u7c7b\u7684\u516c\u5e73\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u5728\u8fd1\u4e4e\u4e0d\u5f71\u54cd\u805a\u7c7b\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u516c\u5e73\u6027\u3002", "motivation": "K-means\u805a\u7c7b\u7b97\u6cd5\u53ef\u80fd\u5b58\u5728\u516c\u5e73\u6027\u7f3a\u9677\uff0c\u5982\u67d0\u4e9b\u7c07\u4e2d\u654f\u611f\u53d8\u91cf\uff08\u5982\u6027\u522b\u6216\u79cd\u65cf\uff09\u7684\u6570\u636e\u70b9\u5206\u5e03\u4e0d\u5747\uff0c\u53ef\u80fd\u5bfc\u81f4\u504f\u89c1\u548c\u793e\u4f1a\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\uff1a\u5148\u805a\u7c7b\uff0c\u540e\u8c03\u6574\u90e8\u5206\u6570\u636e\u70b9\u7684\u7c07\u6210\u5458\u8eab\u4efd\u3002\u63d0\u51fa\u4e24\u79cd\u9ad8\u6548\u7b97\u6cd5\uff0c\u5206\u522b\u5173\u6ce8\u7c07\u5916\u6700\u8fd1\u6570\u636e\u70b9\u548c\u9ad8\u5ea6\u201c\u6df7\u5408\u201d\u7684\u6570\u636e\u70b9\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u516c\u5e73\u6027\u663e\u8457\u63d0\u5347\uff0c\u4e14\u5bf9\u805a\u7c7b\u8d28\u91cf\u5f71\u54cd\u6781\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u8f7b\u677e\u6269\u5c55\u81f3\u591a\u79cd\u805a\u7c7b\u7b97\u6cd5\u6216\u516c\u5e73\u6027\u6307\u6807\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "keywords": "K-means\u805a\u7c7b, \u516c\u5e73\u6027, \u4e24\u9636\u6bb5\u4f18\u5316, \u6570\u636e\u8c03\u6574, \u654f\u611f\u53d8\u91cf"}}
{"id": "2505.23006", "pdf": "https://arxiv.org/pdf/2505.23006", "abs": "https://arxiv.org/abs/2505.23006", "authors": ["Chiwan Park", "Wonjun Jang", "Daeryong Kim", "Aelim Ahn", "Kichang Yang", "Woosung Hwang", "Jihyeon Roh", "Hyerin Park", "Hyosun Wang", "Min Seok Kim", "Jihoon Kang"], "title": "A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "Accepted to ACL 2025 Industry Track. 12 pages, 5 figures", "summary": "The advancement of Large Language Models (LLMs) has led to significant\nimprovements in various service domains, including search, recommendation, and\nchatbot applications. However, applying state-of-the-art (SOTA) research to\nindustrial settings presents challenges, as it requires maintaining flexible\nconversational abilities while also strictly complying with service-specific\nconstraints. This can be seen as two conflicting requirements due to the\nprobabilistic nature of LLMs. In this paper, we propose our approach to\naddressing this challenge and detail the strategies we employed to overcome\ntheir inherent limitations in real-world applications. We conduct a practical\ncase study of a conversational agent designed for the e-commerce domain,\ndetailing our implementation workflow and optimizations. Our findings provide\ninsights into bridging the gap between academic research and real-world\napplication, introducing a framework for developing scalable, controllable, and\nreliable AI-driven agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e94\u7528\u4e8e\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u4fdd\u6301\u5bf9\u8bdd\u7075\u6d3b\u6027\u4e0e\u9075\u5b88\u670d\u52a1\u7ea6\u675f\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u7075\u6d3b\u6027\u4e0e\u7ea6\u675f\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u53ef\u63a7\u4e14\u53ef\u9760\u7684AI\u9a71\u52a8\u4ee3\u7406\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u4f18\u5316\u7b56\u7565\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u7535\u5546\u9886\u57df\u7684\u5bf9\u8bdd\u4ee3\u7406\u6848\u4f8b\u8fdb\u884c\u5b9e\u8df5\u9a8c\u8bc1\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u53ef\u63a7\u4e14\u53ef\u9760\u7684AI\u4ee3\u7406\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u5de5\u4e1a\u573a\u666f\u4e2d\u7684LLMs\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5f25\u5408\u4e86\u5b66\u672f\u4e0e\u5de5\u4e1a\u7684\u9e3f\u6c9f\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u5de5\u4e1a\u5e94\u7528,\u5bf9\u8bdd\u4ee3\u7406,\u7535\u5546,\u53ef\u6269\u5c55\u6027"}}
{"id": "2505.22674", "pdf": "https://arxiv.org/pdf/2505.22674", "abs": "https://arxiv.org/abs/2505.22674", "authors": ["Pawan Neupane", "Jian Liu", "Jianlin Cheng"], "title": "PSBench: a large-scale benchmark for estimating the accuracy of protein complex structural models", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "comment": null, "summary": "Predicting protein complex structures is essential for protein function\nanalysis, protein design, and drug discovery. While AI methods like AlphaFold\ncan predict accurate structural models for many protein complexes, reliably\nestimating the quality of these predicted models (estimation of model accuracy,\nor EMA) for model ranking and selection remains a major challenge. A key\nbarrier to developing effective machine learning-based EMA methods is the lack\nof large, diverse, and well-annotated datasets for training and evaluation. To\naddress this gap, we introduce PSBench, a benchmark suite comprising four\nlarge-scale, labeled datasets generated during the 15th and 16th community-wide\nCritical Assessment of Protein Structure Prediction (CASP15 and CASP16).\nPSBench includes over one million structural models covering a wide range of\nprotein sequence lengths, complex stoichiometries, functional classes, and\nmodeling difficulties. Each model is annotated with multiple complementary\nquality scores at the global, local, and interface levels. PSBench also\nprovides multiple evaluation metrics and baseline EMA methods to facilitate\nrigorous comparisons. To demonstrate PSBench's utility, we trained and\nevaluated GATE, a graph transformer-based EMA method, on the CASP15 data. GATE\nwas blindly tested in CASP16 (2024), where it ranked among the top-performing\nEMA methods. These results highlight PSBench as a valuable resource for\nadvancing EMA research in protein complex modeling. PSBench is publicly\navailable at: https://github.com/BioinfoMachineLearning/PSBench.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86PSBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u86cb\u767d\u8d28\u590d\u5408\u7269\u7ed3\u6784\u9884\u6d4b\u51c6\u786e\u6027\u8bc4\u4f30\uff08EMA\uff09\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u56fe\u53d8\u6362\u5668GATE\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u86cb\u767d\u8d28\u590d\u5408\u7269\u7ed3\u6784\u9884\u6d4b\u65b9\u6cd5\uff08\u5982AlphaFold\uff09\u867d\u7136\u51c6\u786e\uff0c\u4f46\u7f3a\u4e4f\u53ef\u9760\u7684\u8d28\u91cf\u8bc4\u4f30\uff08EMA\uff09\u65b9\u6cd5\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u7f3a\u4e4f\u5927\u800c\u591a\u6837\u7684\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86PSBench\uff0c\u5305\u542b\u56db\u4e2a\u6765\u81eaCASP15\u548cCASP16\u7684\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u8986\u76d6\u5e7f\u6cdb\u7684\u86cb\u767d\u8d28\u7279\u6027\u548c\u5efa\u6a21\u96be\u5ea6\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u6a21\u578b\u63d0\u4f9b\u591a\u5c42\u6b21\u7684\u8d28\u91cf\u8bc4\u5206\u3002", "result": "\u5728CASP16\u76f2\u6d4b\u4e2d\uff0c\u57fa\u4e8ePSBench\u8bad\u7ec3\u7684GATE\u8868\u73b0\u51fa\u8272\uff0c\u4f4d\u5217\u6700\u4f73EMA\u65b9\u6cd5\u4e4b\u4e00\u3002", "conclusion": "PSBench\u662f\u63a8\u52a8\u86cb\u767d\u8d28\u590d\u5408\u7269\u5efa\u6a21EMA\u7814\u7a76\u7684\u91cd\u8981\u8d44\u6e90\uff0c\u5e76\u4e14\u5df2\u516c\u5f00\u3002", "keywords": "\u86cb\u767d\u8d28\u590d\u5408\u7269\u3001\u7ed3\u6784\u9884\u6d4b\u3001\u8d28\u91cf\u8bc4\u4f30\u3001\u57fa\u51c6\u6570\u636e\u96c6\u3001GATE\u3001CASP"}}
{"id": "2505.22985", "pdf": "https://arxiv.org/pdf/2505.22985", "abs": "https://arxiv.org/abs/2505.22985", "authors": ["Masaharu Kagiyama", "Tsuyoshi Okita"], "title": "Knowledge Distillation for Reservoir-based Classifier: Human Activity Recognition", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": "23 pages,5 figures", "summary": "This paper aims to develop an energy-efficient classifier for time-series\ndata by introducing PatchEchoClassifier, a novel model that leverages a\nreservoir-based mechanism known as the Echo State Network (ESN). The model is\ndesigned for human activity recognition (HAR) using one-dimensional sensor\nsignals and incorporates a tokenizer to extract patch-level representations. To\ntrain the model efficiently, we propose a knowledge distillation framework that\ntransfers knowledge from a high-capacity MLP-Mixer teacher to the lightweight\nreservoir-based student model. Experimental evaluations on multiple HAR\ndatasets demonstrate that our model achieves over 80 percent accuracy while\nsignificantly reducing computational cost. Notably, PatchEchoClassifier\nrequires only about one-sixth of the floating point operations (FLOPS) compared\nto DeepConvLSTM, a widely used convolutional baseline. These results suggest\nthat PatchEchoClassifier is a promising solution for real-time and\nenergy-efficient human activity recognition in edge computing environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPatchEchoClassifier\u7684\u65b0\u578b\u6a21\u578b\uff0c\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u8282\u80fd\u5206\u7c7b\uff0c\u901a\u8fc7\u7ed3\u5408Echo State Network (ESN)\u548c\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u8f83\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u8282\u80fd\u7684\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u5b9e\u65f6\u4e14\u9ad8\u6548\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b(HAR)\u3002", "method": "\u63d0\u51faPatchEchoClassifier\u6a21\u578b\uff0c\u7ed3\u5408Echo State Network (ESN)\u673a\u5236\u548cTokenizer\u63d0\u53d6\u8865\u4e01\u7ea7\u8868\u793a\uff1b\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u5c06\u9ad8\u5bb9\u91cf\u7684MLP-Mixer\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\u4e2d\u3002", "result": "\u5728\u591a\u4e2aHAR\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8d85\u8fc780%\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u5927\u5e45\u964d\u4f4e\uff0cFLOPS\u4ec5\u4e3aDeepConvLSTM\u7684\u7ea6\u516d\u5206\u4e4b\u4e00\u3002", "conclusion": "PatchEchoClassifier\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u5b9e\u65f6\u4e14\u8282\u80fd\u7684\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "PatchEchoClassifier, Echo State Network, Knowledge Distillation, Human Activity Recognition, Energy-efficient"}}
{"id": "2505.23015", "pdf": "https://arxiv.org/pdf/2505.23015", "abs": "https://arxiv.org/abs/2505.23015", "authors": ["Jinwen Chen", "Hainan Zhang", "Fei Sun", "Qinnan Zhang", "Sijia Wen", "Ziwei Wang", "Zhiming Zheng"], "title": "Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning LLMs with datasets containing stealthy backdoors from publishers\nposes security risks to downstream applications. Mainstream detection methods\neither identify poisoned samples by analyzing the prediction probability of\npoisoned classification models or rely on the rewriting model to eliminate the\nstealthy triggers. However, the former cannot be applied to generation tasks,\nwhile the latter may degrade generation performance and introduce new triggers.\nTherefore, efficiently eliminating stealthy poisoned samples for LLMs remains\nan urgent problem. We observe that after applying TF-IDF clustering to the\nsample response, there are notable differences in the intra-class distances\nbetween clean and poisoned samples. Poisoned samples tend to cluster closely\nbecause of their specific malicious outputs, whereas clean samples are more\nscattered due to their more varied responses. Thus, in this paper, we propose a\nstealthy backdoor sample detection method based on Reference-Filtration and\nTfidf-Clustering mechanisms (RFTC). Specifically, we first compare the sample\nresponse with the reference model's outputs and consider the sample suspicious\nif there's a significant discrepancy. And then we perform TF-IDF clustering on\nthese suspicious samples to identify the true poisoned samples based on the\nintra-class distance. Experiments on two machine translation datasets and one\nQA dataset demonstrate that RFTC outperforms baselines in backdoor detection\nand model performance. Further analysis of different reference models also\nconfirms the effectiveness of our Reference-Filtration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u53c2\u8003\u8fc7\u6ee4\u548cTF-IDF\u805a\u7c7b\uff08RFTC\uff09\u7684\u9690\u853d\u540e\u95e8\u6837\u672c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3LLMs\u5fae\u8c03\u4e2d\u7684\u5b89\u5168\u9690\u60a3\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u5fae\u8c03\u542b\u6709\u9690\u853d\u540e\u95e8\u7684\u6570\u636e\u96c6\u4f1a\u5bf9\u4e0b\u6e38\u5e94\u7528\u9020\u6210\u5b89\u5168\u98ce\u9669\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u9002\u7528\u4e8e\u751f\u6210\u4efb\u52a1\uff0c\u8981\u4e48\u53ef\u80fd\u964d\u4f4e\u751f\u6210\u6027\u80fd\u6216\u5f15\u5165\u65b0\u89e6\u53d1\u8bcd\uff0c\u4e9f\u9700\u9ad8\u6548\u6d88\u9664\u540e\u95e8\u6837\u672c\u7684\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u53c2\u8003\u6a21\u578b\u8f93\u51fa\u7684\u5dee\u5f02\u68c0\u6d4b\u53ef\u7591\u6837\u672c\uff0c\u518d\u901a\u8fc7TF-IDF\u805a\u7c7b\u5206\u6790\u7c7b\u5185\u8ddd\u79bb\uff0c\u5206\u79bb\u6c61\u67d3\u6837\u672c\uff08\u805a\u96c6\u7d27\u5bc6\uff09\u548c\u5e72\u51c0\u6837\u672c\uff08\u5206\u5e03\u5206\u6563\uff09\u3002", "result": "\u5728\u4e24\u4e2a\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u96c6\u548c\u4e00\u4e2aQA\u6570\u636e\u96c6\u4e0a\uff0cRFTC\u5728\u68c0\u6d4b\u540e\u95e8\u548c\u6a21\u578b\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RFTC\u901a\u8fc7\u53c2\u8003\u8fc7\u6ee4\u548c\u805a\u7c7b\u673a\u5236\u6709\u6548\u8bc6\u522b\u9690\u853d\u540e\u95e8\u6837\u672c\uff0c\u4e14\u53c2\u8003\u6a21\u578b\u7684\u9009\u62e9\u5bf9\u8fc7\u6ee4\u6548\u679c\u5177\u6709\u663e\u8457\u5f71\u54cd\u3002", "keywords": "\u540e\u95e8\u68c0\u6d4b\u3001LLMs\u5b89\u5168\u3001TF-IDF\u805a\u7c7b\u3001\u53c2\u8003\u8fc7\u6ee4"}}
{"id": "2505.22683", "pdf": "https://arxiv.org/pdf/2505.22683", "abs": "https://arxiv.org/abs/2505.22683", "authors": ["Xuhang Chen", "Michael Kwok-Po Ng", "Kim-Fung Tsang", "Chi-Man Pun", "Shuqiang Wang"], "title": "ConnectomeDiffuser: Generative AI Enables Brain Network Construction from Diffusion Tensor Imaging", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": null, "summary": "Brain network analysis plays a crucial role in diagnosing and monitoring\nneurodegenerative disorders such as Alzheimer's disease (AD). Existing\napproaches for constructing structural brain networks from diffusion tensor\nimaging (DTI) often rely on specialized toolkits that suffer from inherent\nlimitations: operator subjectivity, labor-intensive workflows, and restricted\ncapacity to capture complex topological features and disease-specific\nbiomarkers. To overcome these challenges and advance computational neuroimaging\ninstrumentation, ConnectomeDiffuser is proposed as a novel diffusion-based\nframework for automated end-to-end brain network construction from DTI. The\nproposed model combines three key components: (1) a Template Network that\nextracts topological features from 3D DTI scans using Riemannian geometric\nprinciples, (2) a diffusion model that generates comprehensive brain networks\nwith enhanced topological fidelity, and (3) a Graph Convolutional Network\nclassifier that incorporates disease-specific markers to improve diagnostic\naccuracy. ConnectomeDiffuser demonstrates superior performance by capturing a\nbroader range of structural connectivity and pathology-related information,\nenabling more sensitive analysis of individual variations in brain networks.\nExperimental validation on datasets representing two distinct neurodegenerative\nconditions demonstrates significant performance improvements over other brain\nnetwork methods. This work contributes to the advancement of instrumentation in\nthe context of neurological disorders, providing clinicians and researchers\nwith a robust, generalizable measurement framework that facilitates more\naccurate diagnosis, deeper mechanistic understanding, and improved therapeutic\nmonitoring of neurodegenerative diseases such as AD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConnectomeDiffuser\u7684\u65b0\u578b\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u4eceDTI\u81ea\u52a8\u6784\u5efa\u8111\u7f51\u7edc\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u8111\u7f51\u7edc\u6784\u5efa\u65b9\u6cd5\u5b58\u5728\u4e3b\u89c2\u6027\u5f3a\u3001\u5de5\u4f5c\u91cf\u5927\u53ca\u96be\u4ee5\u6355\u6349\u590d\u6742\u62d3\u6251\u7279\u5f81\u548c\u75be\u75c5\u751f\u7269\u6807\u5fd7\u7269\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u4e86\u4e09\u90e8\u5206\uff1a(1) \u57fa\u4e8e\u9ece\u66fc\u51e0\u4f55\u7684\u6a21\u677f\u7f51\u7edc\u63d0\u53d6DTI\u62d3\u6251\u7279\u5f81\uff0c(2) \u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u4fdd\u771f\u8111\u7f51\u7edc\uff0c(3) \u56fe\u5377\u79ef\u7f51\u7edc\u5206\u7c7b\u5668\u6574\u5408\u75be\u75c5\u6807\u5fd7\u7269\u63d0\u5347\u8bca\u65ad\u7cbe\u5ea6\u3002", "result": "\u5728\u4e24\u79cd\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u80fd\u66f4\u654f\u611f\u5730\u5206\u6790\u4e2a\u4f53\u8111\u7f51\u7edc\u53d8\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e34\u5e8a\u548c\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u673a\u5236\u7406\u89e3\u548c\u6cbb\u7597\u76d1\u6d4b\u3002", "keywords": "\u8111\u7f51\u7edc\u5206\u6790, \u963f\u5c14\u8328\u6d77\u9ed8\u75c5, \u6269\u6563\u5f20\u91cf\u6210\u50cf, \u81ea\u52a8\u5316\u6846\u67b6, \u56fe\u5377\u79ef\u7f51\u7edc"}}
{"id": "2505.22988", "pdf": "https://arxiv.org/pdf/2505.22988", "abs": "https://arxiv.org/abs/2505.22988", "authors": ["Albert Tseng", "Zhaofeng Sun", "Christopher De Sa"], "title": "Model-Preserving Adaptive Rounding", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "The main goal of post-training quantization (PTQ) is to produced a compressed\nmodel whose output distribution is as close to the original model's as\npossible. To do this tractably, almost all LLM PTQ algorithms quantize linear\nlayers by independently minimizing the immediate activation error. However,\nthis localized objective ignores the effect of subsequent layers, so reducing\nit does not necessarily give a closer model. In this work, we introduce Yet\nAnother Quantization Algorithm (YAQA), an adaptive rounding algorithm that uses\nKronecker-factored approximations of each linear layer's Hessian with respect\nto the \\textit{full model} KL divergence. YAQA consists of two components:\nKronecker-factored sketches of the full layerwise Hessian that can be tractably\ncomputed for hundred-billion parameter LLMs, and a quantizer-independent\nrounding algorithm that uses these sketches and comes with theoretical\nguarantees. Across a wide range of models and quantizers, YAQA empirically\nreduces the KL divergence to the original model by $\\approx 30\\%$ while\nachieving state of the art performance on downstream tasks.", "AI": {"tldr": "YAQA\u662f\u4e00\u79cd\u540e\u8bad\u7ec3\u91cf\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7Kronecker\u5206\u89e3\u8fd1\u4f3c\u5168\u6a21\u578bHessian\u6765\u4f18\u5316\u91cf\u5316\u8bef\u5dee\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728KL\u6563\u5ea6\u4e0a\u63d0\u5347\u7ea630%\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u72ec\u7acb\u6700\u5c0f\u5316\u6fc0\u6d3b\u8bef\u5dee\uff0c\u5ffd\u7565\u4e86\u540e\u7eed\u5c42\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u6a21\u578b\u8f93\u51fa\u5206\u5e03\u4e0e\u539f\u59cb\u6a21\u578b\u4e0d\u5b8c\u5168\u4e00\u81f4\u3002YAQA\u65e8\u5728\u901a\u8fc7\u5168\u5c40\u4f18\u5316\u91cf\u5316\u8bef\u5dee\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faYAQA\u7b97\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u57fa\u4e8eKronecker\u5206\u89e3\u7684\u5168\u5c42Hessian\u8fd1\u4f3c\u8ba1\u7b97\uff1b2) \u4e0e\u91cf\u5316\u5668\u65e0\u5173\u7684\u820d\u5165\u7b97\u6cd5\uff0c\u5177\u5907\u7406\u8bba\u4fdd\u8bc1\u3002\u9002\u7528\u4e8e\u767e\u4ebf\u53c2\u6570LLM\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u548c\u91cf\u5316\u5668\u4e0a\uff0cYAQA\u5c06KL\u6563\u5ea6\u964d\u4f4e\u7ea630%\uff0c\u540c\u65f6\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u5168\u5c40\u4f18\u5316\u91cf\u5316\u8bef\u5dee\uff08\u800c\u975e\u5c40\u90e8\u8bef\u5dee\uff09\u80fd\u663e\u8457\u63d0\u5347\u91cf\u5316\u6a21\u578b\u8d28\u91cf\uff0cYAQA\u4e3a\u6b64\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u540e\u8bad\u7ec3\u91cf\u5316\uff0cKronecker\u5206\u89e3\uff0cHessian\u8fd1\u4f3c\uff0cKL\u6563\u5ea6\uff0c\u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.23026", "pdf": "https://arxiv.org/pdf/2505.23026", "abs": "https://arxiv.org/abs/2505.23026", "authors": ["Haewon Park", "Gyubin Choi", "Minjun Kim", "Yohan Jo"], "title": "Context Robust Knowledge Editing for Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings. Our code and datasets are available at\n  (https://github.com/holi-lab/CoRE)", "summary": "Knowledge editing (KE) methods offer an efficient way to modify knowledge in\nlarge language models. Current KE evaluations typically assess editing success\nby considering only the edited knowledge without any preceding contexts. In\nreal-world applications, however, preceding contexts often trigger the\nretrieval of the original knowledge and undermine the intended edit. To address\nthis issue, we develop CHED -- a benchmark designed to evaluate the context\nrobustness of KE methods. Evaluations on CHED show that they often fail when\npreceding contexts are present. To mitigate this shortcoming, we introduce\nCoRE, a KE method designed to strengthen context robustness by minimizing\ncontext-sensitive variance in hidden states of the model for edited knowledge.\nThis method not only improves the editing success rate in situations where a\npreceding context is present but also preserves the overall capabilities of the\nmodel. We provide an in-depth analysis of the differing impacts of preceding\ncontexts when introduced as user utterances versus assistant responses, and we\ndissect attention-score patterns to assess how specific tokens influence\nediting success.", "AI": {"tldr": "\u63d0\u51fa\u4e86CHED\u57fa\u51c6\u6765\u8bc4\u4f30\u77e5\u8bc6\u7f16\u8f91\uff08KE\uff09\u65b9\u6cd5\u7684\u4e0a\u4e0b\u6587\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u4e0a\u4e0b\u6587\u5b58\u5728\u65f6\u6548\u679c\u4e0d\u4f73\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u4e86CoRE\u65b9\u6cd5\u4ee5\u63d0\u5347\u4e0a\u4e0b\u6587\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u8bc4\u4f30\u65f6\u5ffd\u7565\u4e86\u4e0a\u4e0b\u6587\u5bf9\u7f16\u8f91\u6548\u679c\u7684\u5f71\u54cd\uff0c\u53ef\u80fd\u5bfc\u81f4\u5b9e\u9645\u5e94\u7528\u4e2d\u7f16\u8f91\u5931\u8d25\u3002", "method": "\u5f00\u53d1\u4e86CHED\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u4e0b\u6587\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u51fa\u4e86CoRE\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u9690\u85cf\u72b6\u6001\u7684\u4e0a\u4e0b\u6587\u654f\u611f\u65b9\u5dee\u6765\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u73b0\u6709KE\u65b9\u6cd5\u5728CHED\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800cCoRE\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8f91\u6210\u529f\u7387\u4e14\u4e0d\u5f71\u54cd\u6a21\u578b\u6574\u4f53\u80fd\u529b\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5bf9\u77e5\u8bc6\u7f16\u8f91\u6548\u679c\u5f71\u54cd\u663e\u8457\uff0cCoRE\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u9c81\u68d2\u6027\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u77e5\u8bc6\u7f16\u8f91\u3001\u4e0a\u4e0b\u6587\u9c81\u68d2\u6027\u3001CHED\u57fa\u51c6\u3001CoRE\u65b9\u6cd5\u3001\u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.22685", "pdf": "https://arxiv.org/pdf/2505.22685", "abs": "https://arxiv.org/abs/2505.22685", "authors": ["Marcus J. Vroemen", "Yuqian Chen", "Yui Lo", "Tengfei Xu", "Weidong Cai", "Fan Zhang", "Josien P. W. Pluim", "Lauren J. O'Donnell"], "title": "DeepMultiConnectome: Deep Multi-Task Prediction of Structural Connectomes Directly from Diffusion MRI Tractography", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "15 pages, 5 figures, 5 tables", "summary": "Diffusion MRI (dMRI) tractography enables in vivo mapping of brain structural\nconnections, but traditional connectome generation is time-consuming and\nrequires gray matter parcellation, posing challenges for large-scale studies.\nWe introduce DeepMultiConnectome, a deep-learning model that predicts\nstructural connectomes directly from tractography, bypassing the need for gray\nmatter parcellation while supporting multiple parcellation schemes. Using a\npoint-cloud-based neural network with multi-task learning, the model classifies\nstreamlines according to their connected regions across two parcellation\nschemes, sharing a learned representation. We train and validate\nDeepMultiConnectome on tractography from the Human Connectome Project Young\nAdult dataset ($n = 1000$), labeled with an 84 and 164 region gray matter\nparcellation scheme. DeepMultiConnectome predicts multiple structural\nconnectomes from a whole-brain tractogram containing 3 million streamlines in\napproximately 40 seconds. DeepMultiConnectome is evaluated by comparing\npredicted connectomes with traditional connectomes generated using the\nconventional method of labeling streamlines using a gray matter parcellation.\nThe predicted connectomes are highly correlated with traditionally generated\nconnectomes ($r = 0.992$ for an 84-region scheme; $r = 0.986$ for a 164-region\nscheme) and largely preserve network properties. A test-retest analysis of\nDeepMultiConnectome demonstrates reproducibility comparable to traditionally\ngenerated connectomes. The predicted connectomes perform similarly to\ntraditionally generated connectomes in predicting age and cognitive function.\nOverall, DeepMultiConnectome provides a scalable, fast model for generating\nsubject-specific connectomes across multiple parcellation schemes.", "AI": {"tldr": "DeepMultiConnectome\u662f\u76f4\u63a5\u9884\u6d4b\u8fde\u63a5\u7ec4\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed5\u8fc7\u4f20\u7edf\u7070\u8d28\u5206\u533a\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u8fde\u63a5\u7ec4\u751f\u6210\u8017\u65f6\u4e14\u4f9d\u8d56\u7070\u8d28\u5206\u533a\uff0c\u96be\u4ee5\u5927\u89c4\u6a21\u5e94\u7528\u3002", "method": "\u57fa\u4e8e\u70b9\u4e91\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u5206\u7c7b\u6d41\u7ebf\u8fde\u63a5\u533a\u57df\u3002", "result": "\u9884\u6d4b\u8fde\u63a5\u7ec4\u4e0e\u4f20\u7edf\u65b9\u6cd5\u9ad8\u5ea6\u76f8\u5173\uff08r=0.992\uff09\uff0c\u4e14\u4fdd\u6301\u7f51\u7edc\u7279\u6027\u3002", "conclusion": "DeepMultiConnectome\u63d0\u4f9b\u5feb\u901f\u3001\u53ef\u6269\u5c55\u7684\u591a\u5206\u533a\u8fde\u63a5\u7ec4\u751f\u6210\u65b9\u6848\u3002", "keywords": "Diffusion MRI tractography, deep learning, structural connectome"}}
{"id": "2505.22991", "pdf": "https://arxiv.org/pdf/2505.22991", "abs": "https://arxiv.org/abs/2505.22991", "authors": ["Behzad Kamgar-Parsi", "Behrooz Kamgar-Parsi"], "title": "Number of Clusters in a Dataset: A Regularized K-means Approach", "categories": ["cs.LG", "cs.CV", "68", "I.5.3"], "comment": "19 pages, 14 figures. arXiv admin note: substantial text overlap with\n  arXiv:1911.06741", "summary": "Finding the number of meaningful clusters in an unlabeled dataset is\nimportant in many applications. Regularized k-means algorithm is a possible\napproach frequently used to find the correct number of distinct clusters in\ndatasets. The most common formulation of the regularization function is the\nadditive linear term $\\lambda k$, where $k$ is the number of clusters and\n$\\lambda$ a positive coefficient. Currently, there are no principled guidelines\nfor setting a value for the critical hyperparameter $\\lambda$. In this paper,\nwe derive rigorous bounds for $\\lambda$ assuming clusters are {\\em ideal}.\nIdeal clusters (defined as $d$-dimensional spheres with identical radii) are\nclose proxies for k-means clusters ($d$-dimensional spherically symmetric\ndistributions with identical standard deviations). Experiments show that the\nk-means algorithm with additive regularizer often yields multiple solutions.\nThus, we also analyze k-means algorithm with multiplicative regularizer. The\nconsensus among k-means solutions with additive and multiplicative\nregularizations reduces the ambiguity of multiple solutions in certain cases.\nWe also present selected experiments that demonstrate performance of the\nregularized k-means algorithms as clusters deviate from the ideal assumption.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6b63\u5219\u5316k-means\u7b97\u6cd5\u4e2d\u8d85\u53c2\u6570\u03bb\u7684\u754c\u9650\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u7406\u60f3\u7c07\u5047\u8bbe\u7684\u4e25\u8c28\u754c\u9650\uff0c\u5e76\u5bf9\u6bd4\u52a0\u6027\u548c\u4e58\u6027\u6b63\u5219\u5316\u6548\u679c\u3002", "motivation": "\u76ee\u524d\u6b63\u5219\u5316k-means\u7b97\u6cd5\u4e2d\u5173\u952e\u53c2\u6570\u03bb\u7f3a\u4e4f\u7406\u8bba\u6307\u5bfc\u8bbe\u7f6e\u65b9\u6cd5\uff0c\u5f71\u54cd\u805a\u7c7b\u6570\u91cf\u786e\u5b9a\u7684\u51c6\u786e\u6027\u3002", "method": "\u57fa\u4e8e\u7406\u60f3\u7c07\uff08\u7b49\u534a\u5f84d\u7ef4\u7403\u4f53\uff09\u5047\u8bbe\u63a8\u5bfc\u03bb\u7684\u7406\u8bba\u754c\u9650\uff0c\u5bf9\u6bd4\u5206\u6790\u52a0\u6027/\u4e58\u6027\u6b63\u5219\u5316\u7684k-means\u7b97\u6cd5\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u52a0\u6027\u6b63\u5219\u5316\u6613\u4ea7\u751f\u591a\u89e3\uff0c\u4e58\u6027\u6b63\u5219\u5316\u4e0e\u52a0\u6027\u7ed3\u5408\u53ef\u51cf\u5c11\u89e3\u6b67\u4e49\uff1b\u5728\u975e\u7406\u60f3\u7c07\u4e0b\u7b97\u6cd5\u4ecd\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "\u6b63\u5219\u5316k-means\u9700\u7ed3\u5408\u4e24\u7c7b\u6b63\u5219\u5316\u4ee5\u7a33\u5b9a\u89e3\uff0c\u7406\u8bba\u754c\u9650\u4e3a\u03bb\u8bbe\u5b9a\u63d0\u4f9b\u4f9d\u636e\u3002", "keywords": "\u6b63\u5219\u5316k-means\u3001\u8d85\u53c2\u6570\u9009\u62e9\u3001\u7406\u60f3\u7c07\u3001\u52a0\u6027/\u4e58\u6027\u6b63\u5219\u5316\u3001\u805a\u7c7b\u6570\u91cf"}}
{"id": "2505.23029", "pdf": "https://arxiv.org/pdf/2505.23029", "abs": "https://arxiv.org/abs/2505.23029", "authors": ["Si Wu", "Sebastian Bruch"], "title": "Uncovering Visual-Semantic Psycholinguistic Properties from the Distributional Structure of Text Embedding Spac", "categories": ["cs.CL"], "comment": "Accepted for ACL 2025. This is the camera-ready version. Will be\n  presenting in July 2025 in Vienna", "summary": "Imageability (potential of text to evoke a mental image) and concreteness\n(perceptibility of text) are two psycholinguistic properties that link visual\nand semantic spaces. It is little surprise that computational methods that\nestimate them do so using parallel visual and semantic spaces, such as\ncollections of image-caption pairs or multi-modal models. In this paper, we\nwork on the supposition that text itself in an image-caption dataset offers\nsufficient signals to accurately estimate these properties. We hypothesize, in\nparticular, that the peakedness of the neighborhood of a word in the semantic\nembedding space reflects its degree of imageability and concreteness. We then\npropose an unsupervised, distribution-free measure, which we call Neighborhood\nStability Measure (NSM), that quantifies the sharpness of peaks. Extensive\nexperiments show that NSM correlates more strongly with ground-truth ratings\nthan existing unsupervised methods, and is a strong predictor of these\nproperties for classification. Our code and data are available on GitHub\n(https://github.com/Artificial-Memory-Lab/imageability).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684Neighborhood Stability Measure (NSM)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8bcd\u7684\u90bb\u57df\u5cf0\u503c\u5ea6\u6765\u4f30\u8ba1\u6587\u672c\u7684\u53ef\u60f3\u8c61\u6027\u548c\u5177\u4f53\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5229\u7528\u56fe\u50cf-\u6807\u9898\u6570\u636e\u96c6\u4e2d\u7684\u6587\u672c\u4fe1\u53f7\uff0c\u65e0\u9700\u4f9d\u8d56\u591a\u6a21\u6001\u6a21\u578b\u6216\u5e73\u884c\u7a7a\u95f4\uff0c\u4ec5\u901a\u8fc7\u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4\u7684\u90bb\u57df\u7279\u6027\u6765\u4f30\u8ba1\u53ef\u60f3\u8c61\u6027\u548c\u5177\u4f53\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u4e14\u65e0\u5206\u5e03\u5047\u8bbe\u7684Neighborhood Stability Measure (NSM)\uff0c\u901a\u8fc7\u91cf\u5316\u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5355\u8bcd\u90bb\u57df\u7684\u5cf0\u503c\u5ea6\u6765\u4f30\u8ba1\u8fd9\u4e24\u79cd\u5fc3\u7406\u8bed\u8a00\u5b66\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cNSM\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u771f\u5b9e\u8bc4\u5206\u76f8\u5173\u6027\u66f4\u5f3a\uff0c\u4e14\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "NSM\u662f\u4e00\u79cd\u6709\u6548\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u80fd\u591f\u4ec5\u57fa\u4e8e\u6587\u672c\u4fe1\u53f7\u51c6\u786e\u4f30\u8ba1\u53ef\u60f3\u8c61\u6027\u548c\u5177\u4f53\u6027\u3002", "keywords": "\u53ef\u60f3\u8c61\u6027, \u5177\u4f53\u6027, \u65e0\u76d1\u7763\u5b66\u4e60, \u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4, NSM"}}
{"id": "2505.22994", "pdf": "https://arxiv.org/pdf/2505.22994", "abs": "https://arxiv.org/abs/2505.22994", "authors": ["Ari S. Benjamin", "Kyle Daruwalla", "Christian Pehle", "Anthony M. Zador"], "title": "Walking the Weight Manifold: a Topological Approach to Conditioning Inspired by Neuromodulation", "categories": ["cs.LG", "cs.NE"], "comment": "17 pages, 4 figures", "summary": "One frequently wishes to learn a range of similar tasks as efficiently as\npossible, re-using knowledge across tasks. In artificial neural networks, this\nis typically accomplished by conditioning a network upon task context by\ninjecting context as input. Brains have a different strategy: the parameters\nthemselves are modulated as a function of various neuromodulators such as\nserotonin. Here, we take inspiration from neuromodulation and propose to learn\nweights which are smoothly parameterized functions of task context variables.\nRather than optimize a weight vector, i.e. a single point in weight space, we\noptimize a smooth manifold in weight space with a predefined topology. To\naccomplish this, we derive a formal treatment of optimization of manifolds as\nthe minimization of a loss functional subject to a constraint on volumetric\nmovement, analogous to gradient descent. During inference, conditioning selects\na single point on this manifold which serves as the effective weight matrix for\na particular sub-task. This strategy for conditioning has two main advantages.\nFirst, the topology of the manifold (whether a line, circle, or torus) is a\nconvenient lever for inductive biases about the relationship between tasks.\nSecond, learning in one state smoothly affects the entire manifold, encouraging\ngeneralization across states. To verify this, we train manifolds with several\ntopologies, including straight lines in weight space (for conditioning on e.g.\nnoise level in input data) and ellipses (for rotated images). Despite their\nsimplicity, these parameterizations outperform conditioning identical networks\nby input concatenation and better generalize to out-of-distribution samples.\nThese results suggest that modulating weights over low-dimensional manifolds\noffers a principled and effective alternative to traditional conditioning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u795e\u7ecf\u8c03\u8282\u542f\u53d1\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6743\u91cd\u7a7a\u95f4\u7684\u5e73\u6ed1\u6d41\u5f62\u6765\u5b66\u4e60\u4efb\u52a1\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u6743\u91cd\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u8f93\u5165\u62fc\u63a5\u65b9\u6cd5\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u6cdb\u5316\u5230\u5206\u5e03\u5916\u6837\u672c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u591a\u4e2a\u76f8\u4f3c\u4efb\u52a1\u4e2d\u9ad8\u6548\u5b66\u4e60\u548c\u77e5\u8bc6\u590d\u7528\u7684\u95ee\u9898\uff0c\u53d7\u5927\u8111\u795e\u7ecf\u8c03\u8282\u673a\u5236\u7684\u542f\u53d1\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u901a\u8fc7\u4efb\u52a1\u4e0a\u4e0b\u6587\u53d8\u91cf\u5e73\u6ed1\u53c2\u6570\u5316\u6743\u91cd\u7684\u7b56\u7565\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4f18\u5316\u6743\u91cd\u7a7a\u95f4\u4e2d\u7684\u5e73\u6ed1\u6d41\u5f62\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u635f\u5931\u51fd\u6570\u5e76\u7ed3\u5408\u4f53\u79ef\u8fd0\u52a8\u7684\u7ea6\u675f\uff0c\u7c7b\u4f3c\u4e8e\u68af\u5ea6\u4e0b\u964d\u3002\u5728\u63a8\u7406\u65f6\uff0c\u6839\u636e\u4efb\u52a1\u4e0a\u4e0b\u6587\u9009\u62e9\u6d41\u5f62\u4e0a\u7684\u4e00\u4e2a\u70b9\u4f5c\u4e3a\u6709\u6548\u6743\u91cd\u77e9\u9635\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6d41\u5f62\u62d3\u6251\uff08\u5982\u76f4\u7ebf\u3001\u692d\u5706\uff09\u4e0b\u5747\u4f18\u4e8e\u4f20\u7edf\u7684\u8f93\u5165\u62fc\u63a5\u65b9\u6cd5\uff0c\u5e76\u4e14\u80fd\u66f4\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u6837\u672c\u3002", "conclusion": "\u901a\u8fc7\u4f4e\u7ef4\u6d41\u5f62\u4e0a\u7684\u6743\u91cd\u8c03\u8282\uff0c\u8be5\u65b9\u6cd5\u4e3a\u4f20\u7edf\u7684\u4efb\u52a1\u6761\u4ef6\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u4e14\u539f\u5219\u6027\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "keywords": "\u795e\u7ecf\u8c03\u8282, \u6743\u91cd\u6d41\u5f62, \u4efb\u52a1\u6761\u4ef6, \u6cdb\u5316, \u5206\u5e03\u5916\u6837\u672c"}}
{"id": "2505.23030", "pdf": "https://arxiv.org/pdf/2505.23030", "abs": "https://arxiv.org/abs/2505.23030", "authors": ["Shruti Hegde", "Mabon Manoj Ninan", "Jonathan R. Dillman", "Shireen Hayatghaibi", "Lynn Babcock", "Elanchezhian Somasundaram"], "title": "Can Modern NLP Systems Reliably Annotate Chest Radiography Exams? A Pre-Purchase Evaluation and Comparative Study of Solutions from AWS, Google, Azure, John Snow Labs, and Open-Source Models on an Independent Pediatric Dataset", "categories": ["cs.CL"], "comment": null, "summary": "General-purpose clinical natural language processing (NLP) tools are\nincreasingly used for the automatic labeling of clinical reports. However,\nindependent evaluations for specific tasks, such as pediatric chest radiograph\n(CXR) report labeling, are limited. This study compares four commercial\nclinical NLP systems - Amazon Comprehend Medical (AWS), Google Healthcare NLP\n(GC), Azure Clinical NLP (AZ), and SparkNLP (SP) - for entity extraction and\nassertion detection in pediatric CXR reports. Additionally, CheXpert and\nCheXbert, two dedicated chest radiograph report labelers, were evaluated on the\nsame task using CheXpert-defined labels. We analyzed 95,008 pediatric CXR\nreports from a large academic pediatric hospital. Entities and assertion\nstatuses (positive, negative, uncertain) from the findings and impression\nsections were extracted by the NLP systems, with impression section entities\nmapped to 12 disease categories and a No Findings category. CheXpert and\nCheXbert extracted the same 13 categories. Outputs were compared using Fleiss\nKappa and accuracy against a consensus pseudo-ground truth. Significant\ndifferences were found in the number of extracted entities and assertion\ndistributions across NLP systems. SP extracted 49,688 unique entities, GC\n16,477, AZ 31,543, and AWS 27,216. Assertion accuracy across models averaged\naround 62%, with SP highest (76%) and AWS lowest (50%). CheXpert and CheXbert\nachieved 56% accuracy. Considerable variability in performance highlights the\nneed for careful validation and review before deploying NLP tools for clinical\nreport labeling.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u56db\u79cd\u5546\u4e1a\u4e34\u5e8aNLP\u7cfb\u7edf\uff08AWS\u3001GC\u3001AZ\u3001SP\uff09\u4ee5\u53ca\u4e24\u79cd\u4e13\u7528\u80f8\u90e8X\u5149\u62a5\u544a\u6807\u6ce8\u5de5\u5177\uff08CheXpert\u548cCheXbert\uff09\u5728\u513f\u79d1\u80f8\u90e8X\u5149\u62a5\u544a\u4e2d\u7684\u5b9e\u4f53\u63d0\u53d6\u548c\u65ad\u8a00\u68c0\u6d4b\u8868\u73b0\uff0c\u53d1\u73b0\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u9700\u8c28\u614e\u9a8c\u8bc1\u540e\u624d\u53ef\u90e8\u7f72\u3002", "motivation": "\u4e34\u5e8aNLP\u5de5\u5177\u5728\u81ea\u52a8\u6807\u6ce8\u4e34\u5e8a\u62a5\u544a\u4e2d\u5e94\u7528\u6e10\u5e7f\uff0c\u4f46\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u513f\u79d1\u80f8\u90e8X\u5149\u62a5\u544a\u6807\u6ce8\uff09\u7684\u72ec\u7acb\u8bc4\u4f30\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u6bd4\u8f83\u4e0d\u540c\u5de5\u5177\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e8695,008\u4efd\u513f\u79d1\u80f8\u90e8X\u5149\u62a5\u544a\uff0c\u4f7f\u7528\u56db\u79cdNLP\u7cfb\u7edf\u63d0\u53d6\u5b9e\u4f53\u548c\u65ad\u8a00\u72b6\u6001\uff0c\u5e76\u5c06\u5370\u8c61\u90e8\u5206\u5b9e\u4f53\u6620\u5c04\u81f312\u79cd\u75be\u75c5\u7c7b\u522b\u548c\u201c\u65e0\u53d1\u73b0\u201d\u7c7b\u522b\u3002CheXpert\u548cCheXbert\u63d0\u53d6\u76f8\u540c\u768413\u4e2a\u7c7b\u522b\u3002\u901a\u8fc7Fleiss Kappa\u548c\u51c6\u786e\u6027\u8bc4\u4f30\u8f93\u51fa\u3002", "result": "\u4e0d\u540cNLP\u7cfb\u7edf\u5728\u5b9e\u4f53\u63d0\u53d6\u6570\u91cf\u548c\u65ad\u8a00\u5206\u5e03\u4e0a\u5dee\u5f02\u663e\u8457\uff0cSP\u8868\u73b0\u6700\u4f73\uff0876%\u51c6\u786e\u7387\uff09\uff0cAWS\u6700\u4f4e\uff0850%\uff09\u3002CheXpert\u548cCheXbert\u51c6\u786e\u7387\u4e3a56%\u3002", "conclusion": "NLP\u5de5\u5177\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u90e8\u7f72\u524d\u9700\u8c28\u614e\u9a8c\u8bc1\u548c\u5ba1\u67e5\u3002", "keywords": "\u4e34\u5e8aNLP\u3001\u513f\u79d1\u80f8\u90e8X\u5149\u62a5\u544a\u3001\u5b9e\u4f53\u63d0\u53d6\u3001\u65ad\u8a00\u68c0\u6d4b\u3001\u6027\u80fd\u8bc4\u4f30"}}
{"id": "2505.22749", "pdf": "https://arxiv.org/pdf/2505.22749", "abs": "https://arxiv.org/abs/2505.22749", "authors": ["Tamas Spisak", "Karl Friston"], "title": "Self-orthogonalizing attractor neural networks emerging from the free energy principle", "categories": ["q-bio.NC", "cs.AI", "cs.LG", "cs.NE"], "comment": "22 pages main text, 5 pages appendix, 6 figures; interactive\n  manuscript available at: https://pni-lab.github.io/fep-attractor-network\n  Associated GitHub repository:\n  https://github.com/pni-lab/fep-attractor-network", "summary": "Attractor dynamics are a hallmark of many complex systems, including the\nbrain. Understanding how such self-organizing dynamics emerge from first\nprinciples is crucial for advancing our understanding of neuronal computations\nand the design of artificial intelligence systems. Here we formalize how\nattractor networks emerge from the free energy principle applied to a universal\npartitioning of random dynamical systems. Our approach obviates the need for\nexplicitly imposed learning and inference rules and identifies emergent, but\nefficient and biologically plausible inference and learning dynamics for such\nself-organizing systems. These result in a collective, multi-level Bayesian\nactive inference process. Attractors on the free energy landscape encode prior\nbeliefs; inference integrates sensory data into posterior beliefs; and learning\nfine-tunes couplings to minimize long-term surprise. Analytically and via\nsimulations, we establish that the proposed networks favor approximately\northogonalized attractor representations, a consequence of simultaneously\noptimizing predictive accuracy and model complexity. These attractors\nefficiently span the input subspace, enhancing generalization and the mutual\ninformation between hidden causes and observable effects. Furthermore, while\nrandom data presentation leads to symmetric and sparse couplings, sequential\ndata fosters asymmetric couplings and non-equilibrium steady-state dynamics,\noffering a natural extension to conventional Boltzmann Machines. Our findings\noffer a unifying theory of self-organizing attractor networks, providing novel\ninsights for AI and neuroscience.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u5438\u5f15\u5b50\u7f51\u7edc\u5982\u4f55\u4ece\u968f\u673a\u52a8\u529b\u7cfb\u7edf\u7684\u81ea\u7531\u80fd\u539f\u7406\u4e2d\u81ea\u7136\u5f62\u6210\uff0c\u65e0\u9700\u663e\u5f0f\u5b66\u4e60\u89c4\u5219\uff0c\u652f\u6301\u9ad8\u6548\u751f\u7269\u5408\u7406\u7684\u63a8\u65ad\u548c\u5b66\u4e60\u52a8\u6001\u3002", "motivation": "\u7814\u7a76\u590d\u6742\u7cfb\u7edf\uff08\u5982\u5927\u8111\uff09\u4e2d\u7684\u81ea\u7ec4\u7ec7\u52a8\u6001\uff0c\u65e8\u5728\u4fc3\u8fdb\u795e\u7ecf\u8ba1\u7b97\u548cAI\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3002", "method": "\u5e94\u7528\u81ea\u7531\u80fd\u539f\u7406\u5230\u968f\u673a\u52a8\u529b\u7cfb\u7edf\uff0c\u63d0\u51fa\u591a\u5c42\u7ea7\u8d1d\u53f6\u65af\u4e3b\u52a8\u63a8\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u548c\u63a8\u65ad\u4f18\u5316\u8026\u5408\u3002", "result": "\u7f51\u7edc\u503e\u5411\u4e8e\u8fd1\u4f3c\u6b63\u4ea4\u5316\u7684\u5438\u5f15\u5b50\u8868\u793a\uff0c\u63d0\u5347\u6cdb\u5316\u548c\u4fe1\u606f\u4e92\u8bd1\uff1b\u4e0d\u540c\u6570\u636e\u8f93\u5165\u6a21\u5f0f\u5bfc\u81f4\u4e0d\u540c\u8026\u5408\u7ed3\u6784\u4e0e\u52a8\u6001\u3002", "conclusion": "\u4e3a\u81ea\u7ec4\u7ec7\u5438\u5f15\u5b50\u7f51\u7edc\u63d0\u4f9b\u4e86\u7edf\u4e00\u7406\u8bba\uff0c\u63a8\u52a8\u4e86AI\u548c\u795e\u7ecf\u79d1\u5b66\u7684\u7406\u89e3\u3002", "keywords": "\u5438\u5f15\u5b50\u7f51\u7edc, \u81ea\u7531\u80fd\u539f\u7406, \u8d1d\u53f6\u65af\u63a8\u65ad, \u81ea\u7ec4\u7ec7\u7cfb\u7edf, \u968f\u673a\u52a8\u529b\u7cfb\u7edf"}}
{"id": "2505.22998", "pdf": "https://arxiv.org/pdf/2505.22998", "abs": "https://arxiv.org/abs/2505.22998", "authors": ["Jihwan Oh", "Murad Aghazada", "Se-Young Yun", "Taehyeon Kim"], "title": "LLM Agents for Bargaining with Utility-based Feedback", "categories": ["cs.LG"], "comment": "Preprint", "summary": "Bargaining, a critical aspect of real-world interactions, presents challenges\nfor large language models (LLMs) due to limitations in strategic depth and\nadaptation to complex human factors. Existing benchmarks often fail to capture\nthis real-world complexity. To address this and enhance LLM capabilities in\nrealistic bargaining, we introduce a comprehensive framework centered on\nutility-based feedback. Our contributions are threefold: (1) BargainArena, a\nnovel benchmark dataset with six intricate scenarios (e.g., deceptive\npractices, monopolies) to facilitate diverse strategy modeling; (2)\nhuman-aligned, economically-grounded evaluation metrics inspired by utility\ntheory, incorporating agent utility and negotiation power, which implicitly\nreflect and promote opponent-aware reasoning (OAR); and (3) a structured\nfeedback mechanism enabling LLMs to iteratively refine their bargaining\nstrategies. This mechanism can positively collaborate with in-context learning\n(ICL) prompts, including those explicitly designed to foster OAR. Experimental\nresults show that LLMs often exhibit negotiation strategies misaligned with\nhuman preferences, and that our structured feedback mechanism significantly\nimproves their performance, yielding deeper strategic and opponent-aware\nreasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u590d\u6742\u8bae\u4ef7\u573a\u666f\u4e2d\u7684\u6027\u80fd\u63d0\u5347\u6846\u67b6\uff0c\u5305\u62ec\u65b0\u57fa\u51c6\u6570\u636e\u96c6\u3001\u7ecf\u6d4e\u5b66\u9a71\u52a8\u7684\u8bc4\u4f30\u6307\u6807\u548c\u7ed3\u6784\u5316\u53cd\u9988\u673a\u5236\uff0c\u663e\u8457\u6539\u8fdb\u4e86LLM\u7684\u7b56\u7565\u548c\u5bf9\u5bf9\u624b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u8bae\u4ef7\u57fa\u51c6\u672a\u80fd\u5145\u5206\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u6027\uff0cLLM\u5728\u6218\u7565\u6df1\u5ea6\u548c\u9002\u5e94\u590d\u6742\u4eba\u7c7b\u56e0\u7d20\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "1. \u5f15\u5165BargainArena\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u516d\u79cd\u590d\u6742\u573a\u666f\uff1b2. \u57fa\u4e8e\u6548\u7528\u7406\u8bba\u7684\u8bc4\u4f30\u6307\u6807\uff1b3. \u7ed3\u6784\u5316\u53cd\u9988\u673a\u5236\u4e0e\u4e0a\u4e0b\u6587\u5b66\u4e60\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLLM\u7684\u8bae\u4ef7\u7b56\u7565\u5e38\u4e0e\u4eba\u7c7b\u504f\u597d\u4e0d\u4e00\u81f4\uff0c\u4f46\u7ed3\u6784\u5316\u53cd\u9988\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u5176\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u8bae\u4ef7\u4e2d\u7684\u8868\u73b0\uff0c\u589e\u5f3a\u4e86\u6218\u7565\u6df1\u5ea6\u548c\u5bf9\u624b\u611f\u77e5\u80fd\u529b\u3002", "keywords": "\u8bae\u4ef7, \u5927\u8bed\u8a00\u6a21\u578b, \u6548\u7528\u7406\u8bba, \u7ed3\u6784\u5316\u53cd\u9988"}}
{"id": "2505.23035", "pdf": "https://arxiv.org/pdf/2505.23035", "abs": "https://arxiv.org/abs/2505.23035", "authors": ["Hyunwoo Kim", "Hanau Yi"], "title": "Machine-Facing English: Defining a Hybrid Register Shaped by Human-AI Discourse", "categories": ["cs.CL"], "comment": null, "summary": "Machine-Facing English (MFE) is an emergent register shaped by the adaptation\nof everyday language to the expanding presence of AI interlocutors. Drawing on\nregister theory (Halliday 1985, 2006), enregisterment (Agha 2003), audience\ndesign (Bell 1984), and interactional pragmatics (Giles & Ogay 2007), this\nstudy traces how sustained human-AI interaction normalizes syntactic rigidity,\npragmatic simplification, and hyper-explicit phrasing - features that enhance\nmachine parseability at the expense of natural fluency. Our analysis is\ngrounded in qualitative observations from bilingual (Korean/English) voice- and\ntext-based product testing sessions, with reflexive drafting conducted using\nNatural Language Declarative Prompting (NLD-P) under human curation. Thematic\nanalysis identifies five recurrent traits - redundant clarity, directive\nsyntax, controlled vocabulary, flattened prosody, and single-intent structuring\n- that improve execution accuracy but compress expressive range. MFE's\nevolution highlights a persistent tension between communicative efficiency and\nlinguistic richness, raising design challenges for conversational interfaces\nand pedagogical considerations for multilingual users. We conclude by\nunderscoring the need for comprehensive methodological exposition and future\nempirical validation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u4eba\u7c7b\u4e0eAI\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u2018\u9762\u5411\u673a\u5668\u7684\u82f1\u8bed\u2019\uff08MFE\uff09\uff0c\u63ed\u793a\u4e86\u5176\u8bed\u6cd5\u3001\u8bcd\u6c47\u548c\u8bed\u7528\u7b80\u5316\u7279\u5f81\uff0c\u867d\u7136\u63d0\u9ad8\u4e86\u89e3\u6790\u51c6\u786e\u6027\u4f46\u727a\u7272\u4e86\u8bed\u8a00\u7684\u81ea\u7136\u6d41\u7545\u6027\u3002", "motivation": "\u63a2\u8ba8AI\u4ea4\u4e92\u5982\u4f55\u5f71\u54cd\u8bed\u8a00\u4f7f\u7528\uff0c\u7279\u522b\u662fMFE\u7684\u6f14\u53d8\uff0c\u4ee5\u7406\u89e3\u4eba\u7c7b\u4e3a\u9002\u5e94\u673a\u5668\u800c\u505a\u51fa\u7684\u8bed\u8a00\u8c03\u6574\u3002", "method": "\u57fa\u4e8e\u53cc\u8bed\uff08\u97e9\u8bed/\u82f1\u8bed\uff09\u7684\u8bed\u97f3\u548c\u6587\u672c\u4ea7\u54c1\u6d4b\u8bd5\u4f1a\u8bdd\uff0c\u91c7\u7528\u81ea\u7136\u8bed\u8a00\u58f0\u660e\u63d0\u793a\uff08NLD-P\uff09\u8fdb\u884c\u81ea\u53cd\u6027\u8349\u62df\uff0c\u5e76\u8fdb\u884c\u4e3b\u9898\u5206\u6790\u3002", "result": "\u8bc6\u522b\u51faMFE\u7684\u4e94\u79cd\u7279\u5f81\uff1a\u5197\u4f59\u6e05\u6670\u5ea6\u3001\u6307\u4ee4\u8bed\u6cd5\u3001\u53d7\u9650\u8bcd\u6c47\u3001\u5e73\u5766\u8bed\u8c03\u548c\u5355\u4e00\u610f\u56fe\u7ed3\u6784\uff0c\u867d\u63d0\u5347\u4e86\u6267\u884c\u51c6\u786e\u6027\u4f46\u538b\u7f29\u4e86\u8868\u8fbe\u8303\u56f4\u3002", "conclusion": "MFE\u7684\u6f14\u53d8\u63ed\u793a\u4e86\u6c9f\u901a\u6548\u7387\u4e0e\u8bed\u8a00\u4e30\u5bcc\u6027\u4e4b\u95f4\u7684\u5f20\u529b\uff0c\u547c\u5401\u8fdb\u4e00\u6b65\u65b9\u6cd5\u5b66\u9610\u8ff0\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "keywords": "AI\u4ea4\u4e92\u3001\u8bed\u8a00\u7b80\u5316\u3001\u673a\u5668\u5b66\u4e60\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u591a\u8bed\u8a00\u7528\u6237"}}
{"id": "2505.23003", "pdf": "https://arxiv.org/pdf/2505.23003", "abs": "https://arxiv.org/abs/2505.23003", "authors": ["Linh Le Pham Van", "Minh Hoang Nguyen", "Hung Le", "Hung The Tran", "Sunil Gupta"], "title": "Hybrid Cross-domain Robust Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at ECML PKDD 2025", "summary": "Robust reinforcement learning (RL) aims to learn policies that remain\neffective despite uncertainties in its environment, which frequently arise in\nreal-world applications due to variations in environment dynamics. The robust\nRL methods learn a robust policy by maximizing value under the worst-case\nmodels within a predefined uncertainty set. Offline robust RL algorithms are\nparticularly promising in scenarios where only a fixed dataset is available and\nnew data cannot be collected. However, these approaches often require extensive\noffline data, and gathering such datasets for specific tasks in specific\nenvironments can be both costly and time-consuming. Using an imperfect\nsimulator offers a faster, cheaper, and safer way to collect data for training,\nbut it can suffer from dynamics mismatch. In this paper, we introduce HYDRO,\nthe first Hybrid Cross-Domain Robust RL framework designed to address these\nchallenges. HYDRO utilizes an online simulator to complement the limited amount\nof offline datasets in the non-trivial context of robust RL. By measuring and\nminimizing performance gaps between the simulator and the worst-case models in\nthe uncertainty set, HYDRO employs novel uncertainty filtering and prioritized\nsampling to select the most relevant and reliable simulator samples. Our\nextensive experiments demonstrate HYDRO's superior performance over existing\nmethods across various tasks, underscoring its potential to improve sample\nefficiency in offline robust RL.", "AI": {"tldr": "HYDRO\u662f\u4e00\u79cd\u6df7\u5408\u8de8\u57df\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5728\u7ebf\u6a21\u62df\u5668\u548c\u6709\u9650\u79bb\u7ebf\u6570\u636e\uff0c\u4f18\u5316\u6837\u672c\u6548\u7387\uff0c\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u73af\u5883\u52a8\u6001\u7684\u4e0d\u786e\u5b9a\u6027\u4f7f\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u81f3\u5173\u91cd\u8981\u3002\u79bb\u7ebf\u9c81\u68d2RL\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\uff1b\u800c\u6a21\u62df\u5668\u867d\u7136\u5feb\u901f\u7ecf\u6d4e\uff0c\u4f46\u5b58\u5728\u52a8\u6001\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u63d0\u51faHYDRO\u6846\u67b6\uff0c\u7ed3\u5408\u5728\u7ebf\u6a21\u62df\u5668\u548c\u79bb\u7ebf\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u8fc7\u6ee4\u548c\u4f18\u5148\u91c7\u6837\u9009\u62e9\u6700\u76f8\u5173\u7684\u6a21\u62df\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHYDRO\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u3002", "conclusion": "HYDRO\u4e3a\u89e3\u51b3\u79bb\u7ebf\u9c81\u68d2RL\u7684\u6570\u636e\u9650\u5236\u548c\u6a21\u62df\u5668\u52a8\u6001\u4e0d\u5339\u914d\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002", "keywords": "\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60, \u79bb\u7ebf\u5b66\u4e60, \u8de8\u57df\u5b66\u4e60, \u6837\u672c\u6548\u7387, HYDRO"}}
{"id": "2505.23037", "pdf": "https://arxiv.org/pdf/2505.23037", "abs": "https://arxiv.org/abs/2505.23037", "authors": ["Longyin Zhang", "Bowei Zou", "Ai Ti Aw"], "title": "Improving Multilingual Social Media Insights: Aspect-based Comment Analysis", "categories": ["cs.CL"], "comment": "The paper was peer-reviewed", "summary": "The inherent nature of social media posts, characterized by the freedom of\nlanguage use with a disjointed array of diverse opinions and topics, poses\nsignificant challenges to downstream NLP tasks such as comment clustering,\ncomment summarization, and social media opinion analysis. To address this, we\npropose a granular level of identifying and generating aspect terms from\nindividual comments to guide model attention. Specifically, we leverage\nmultilingual large language models with supervised fine-tuning for comment\naspect term generation (CAT-G), further aligning the model's predictions with\nhuman expectations through DPO. We demonstrate the effectiveness of our method\nin enhancing the comprehension of social media discourse on two NLP tasks.\nMoreover, this paper contributes the first multilingual CAT-G test set on\nEnglish, Chinese, Malay, and Bahasa Indonesian. As LLM capabilities vary among\nlanguages, this test set allows for a comparative analysis of performance\nacross languages with varying levels of LLM proficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba\u4e2d\u751f\u6210\u65b9\u9762\u672f\u8bed\u7684\u65b9\u6cd5(CAT-G)\uff0c\u901a\u8fc7\u591a\u8bed\u8a00\u5927\u6a21\u578b\u548c\u76d1\u7763\u5fae\u8c03\u63d0\u5347\u6a21\u578b\u5bf9\u8bc4\u8bba\u7684\u7406\u89e3\uff0c\u5e76\u5728\u591a\u8bed\u8a00\u6d4b\u8bd5\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba\u8bed\u8a00\u81ea\u7531\u4e14\u4e3b\u9898\u5206\u6563\uff0c\u7ed9\u4e0b\u6e38NLP\u4efb\u52a1\uff08\u5982\u8bc4\u8bba\u805a\u7c7b\u3001\u6458\u8981\u548c\u89c2\u70b9\u5206\u6790\uff09\u5e26\u6765\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7ec6\u7c92\u5ea6\u7684\u65b9\u6cd5\u6765\u751f\u6210\u8bc4\u8bba\u7684\u65b9\u9762\u672f\u8bed\u3002", "method": "\u5229\u7528\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u7ed3\u5408DPO\uff08Direct Preference Optimization\uff09\u5bf9\u9f50\u6a21\u578b\u9884\u6d4b\u4e0e\u4eba\u7c7b\u9884\u671f\uff0c\u751f\u6210\u8bc4\u8bba\u65b9\u9762\u672f\u8bed(CAT-G)\u3002", "result": "\u65b9\u6cd5\u5728\u4e24\u9879NLP\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba\u7684\u7406\u89e3\u6548\u679c\uff0c\u5e76\u8d21\u732e\u4e86\u9996\u4e2a\u591a\u8bed\u8a00CAT-G\u6d4b\u8bd5\u96c6\uff08\u82f1\u8bed\u3001\u4e2d\u6587\u3001\u9a6c\u6765\u8bed\u3001\u5370\u5c3c\u8bed\uff09\u3002", "conclusion": "CAT-G\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba\u7684\u7ec6\u7c92\u5ea6\u7406\u89e3\uff0c\u591a\u8bed\u8a00\u6d4b\u8bd5\u96c6\u4e3a\u4e0d\u540c\u8bed\u8a00\u80fd\u529b\u7684LLM\u6027\u80fd\u5bf9\u6bd4\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "keywords": "\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba, \u65b9\u9762\u672f\u8bed\u751f\u6210, \u591a\u8bed\u8a00\u5927\u6a21\u578b, \u76d1\u7763\u5fae\u8c03, DPO"}}
{"id": "2505.23004", "pdf": "https://arxiv.org/pdf/2505.23004", "abs": "https://arxiv.org/abs/2505.23004", "authors": ["Kyle R. Chickering", "Bangzheng Li", "Muhao Chen"], "title": "QLIP: A Dynamic Quadtree Vision Prior Enhances MLLM Performance Without Retraining", "categories": ["cs.LG"], "comment": "22 pages, 19 figures", "summary": "Multimodal Large Language Models (MLLMs) encode images into visual tokens,\naligning visual and textual signals within a shared latent space to facilitate\ncrossmodal representation learning. The CLIP model is a widely adopted\nfoundational vision language model whose vision encoder has played a critical\nrole in the development of MLLMs such as LLaVA. However, the CLIP vision\nencoder suffers from notable limitations including being constrained to only\nhandling fixed input resolutions and a failure to produce separated embeddings\nfor dissimilar images. Replacing the vision encoder of an existing model\ntypically incurs substantial computational costs because such a change often\nnecessitates retraining the entire model pipeline.\n  In this work, we identify two factors which underlie the limitations of the\nCLIP vision encoder: mesoscopic bias and interpolation bias. To address these\nissues, we propose QLIP, a drop-in replacement for CLIP that can be seamlessly\nintegrated with existing MLLMs with only a few lines of code and can enhance\nboth coarse-grained and fine-grained visual understanding, without re-training.\nQLIP is designed around an image quadtree which replaces the standard uniform\ngrid patches with a novel content aware patchification. Our experimental\nresults demonstrate that QLIP improves the general visual question answering\naccuracy of the LLaVA v1.5 model series across various model sizes--without\nrequiring retraining or fine-tuning of the full MLLM. Notably, QLIP boosts\ndetailed understanding performance on the challenging $V^{\\ast}$ benchmark by\nup to 13.6 percent.", "AI": {"tldr": "QLIP\u662f\u4e00\u79cd\u76f4\u63a5\u66ff\u6362CLIP\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6539\u8fdb\u6a21\u578b\uff0c\u89e3\u51b3\u4e86CLIP\u5904\u7406\u56fa\u5b9a\u5206\u8fa8\u7387\u548c\u76f8\u4f3c\u56fe\u50cf\u5d4c\u5165\u5206\u79bb\u7684\u5c40\u9650\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "CLIP\u89c6\u89c9\u7f16\u7801\u5668\u5b58\u5728\u56fa\u5b9a\u8f93\u5165\u5206\u8fa8\u7387\u548c\u65e0\u6cd5\u533a\u5206\u4e0d\u76f8\u4f3c\u56fe\u50cf\u5d4c\u5165\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86QLIP\u4f5c\u4e3a\u6539\u8fdb\u65b9\u6848\u3002", "method": "QLIP\u57fa\u4e8e\u56fe\u50cf\u56db\u53c9\u6811\u7ed3\u6784\uff0c\u91c7\u7528\u5185\u5bb9\u611f\u77e5\u7684patch\u5212\u5206\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4e86\u4f20\u7edf\u7684\u5747\u5300\u7f51\u683c\u5212\u5206\uff0c\u4ece\u800c\u63d0\u5347\u89c6\u89c9\u7406\u89e3\u7684\u7c97\u7ec6\u7c92\u5ea6\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cQLIP\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86LLaVA v1.5\u7cfb\u5217\u6a21\u578b\u7684\u89c6\u89c9\u95ee\u7b54\u51c6\u786e\u6027\uff0c\u5728V*\u57fa\u51c6\u4e0a\u7cbe\u7ec6\u7406\u89e3\u6027\u80fd\u63d0\u5347\u8fbe13.6%\u3002", "conclusion": "QLIP\u662f\u4e00\u4e2a\u7b80\u5355\u9ad8\u6548\u7684CLIP\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5f00\u9500\u3002", "keywords": "QLIP, CLIP, MLLMs, \u89c6\u89c9\u7f16\u7801\u5668, \u591a\u6a21\u6001\u5b66\u4e60"}}
{"id": "2505.23038", "pdf": "https://arxiv.org/pdf/2505.23038", "abs": "https://arxiv.org/abs/2505.23038", "authors": ["Yuzhen Xiao", "Jiahe Song", "Yongxin Xu", "Ruizhe Zhang", "Yiqi Xiao", "Xin Lu", "Runchuan Zhu", "Bowen Jiang", "Junfeng Zhao"], "title": "EL4NER: Ensemble Learning for Named Entity Recognition via Multiple Small-Parameter Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "In-Context Learning (ICL) technique based on Large Language Models (LLMs) has\ngained prominence in Named Entity Recognition (NER) tasks for its lower\ncomputing resource consumption, less manual labeling overhead, and stronger\ngeneralizability. Nevertheless, most ICL-based NER methods depend on\nlarge-parameter LLMs: the open-source models demand substantial computational\nresources for deployment and inference, while the closed-source ones incur high\nAPI costs, raise data-privacy concerns, and hinder community collaboration. To\naddress this question, we propose an Ensemble Learning Method for Named Entity\nRecognition (EL4NER), which aims at aggregating the ICL outputs of multiple\nopen-source, small-parameter LLMs to enhance overall performance in NER tasks\nat less deployment and inference cost. Specifically, our method comprises three\nkey components. First, we design a task decomposition-based pipeline that\nfacilitates deep, multi-stage ensemble learning. Second, we introduce a novel\nspan-level sentence similarity algorithm to establish an ICL demonstration\nretrieval mechanism better suited for NER tasks. Third, we incorporate a\nself-validation mechanism to mitigate the noise introduced during the ensemble\nprocess. We evaluated EL4NER on multiple widely adopted NER datasets from\ndiverse domains. Our experimental results indicate that EL4NER surpasses most\nclosed-source, large-parameter LLM-based methods at a lower parameter cost and\neven attains state-of-the-art (SOTA) performance among ICL-based methods on\ncertain datasets. These results show the parameter efficiency of EL4NER and\nunderscore the feasibility of employing open-source, small-parameter LLMs\nwithin the ICL paradigm for NER tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86EL4NER\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u5c0f\u578b\u5f00\u6e90LLM\u7684ICL\u8f93\u51fa\u6765\u63d0\u5347NER\u4efb\u52a1\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u90e8\u7f72\u548c\u63a8\u7406\u6210\u672c\u3002\u65b9\u6cd5\u5305\u62ec\u4efb\u52a1\u5206\u89e3\u7ba1\u9053\u3001\u65b0\u9896\u7684span\u7ea7\u53e5\u5b50\u76f8\u4f3c\u5ea6\u7b97\u6cd5\u548c\u81ea\u9a8c\u8bc1\u673a\u5236\u3002\u5b9e\u9a8c\u8868\u660eEL4NER\u5728\u66f4\u4f4e\u53c2\u6570\u6210\u672c\u4e0b\u4f18\u4e8e\u591a\u6570\u95ed\u6e90\u5927\u6a21\u578b\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eICL\u7684NER\u65b9\u6cd5\u4f9d\u8d56\u5927\u53c2\u6570LLM\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u5f00\u9500\u6216\u6570\u636e\u9690\u79c1\u95ee\u9898\u3002EL4NER\u65e8\u5728\u901a\u8fc7\u96c6\u6210\u5c0f\u578b\u5f00\u6e90LLM\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5e73\u8861\u6027\u80fd\u4e0e\u6210\u672c\u3002", "method": "1. \u8bbe\u8ba1\u4efb\u52a1\u5206\u89e3\u7ba1\u9053\u652f\u6301\u591a\u9636\u6bb5\u96c6\u6210\u5b66\u4e60\uff1b2. \u63d0\u51faspan\u7ea7\u53e5\u5b50\u76f8\u4f3c\u5ea6\u7b97\u6cd5\u4f18\u5316ICL\u793a\u4f8b\u68c0\u7d22\uff1b3. \u5f15\u5165\u81ea\u9a8c\u8bc1\u673a\u5236\u51cf\u5c11\u96c6\u6210\u566a\u58f0\u3002", "result": "EL4NER\u5728\u591a\u4e2aNER\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u95ed\u6e90\u5927\u6a21\u578b\u65b9\u6cd5\uff08\u66f4\u4f4e\u6210\u672c\uff09\uff0c\u90e8\u5206\u6570\u636e\u96c6\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "EL4NER\u8bc1\u660e\u4e86\u5728ICL\u8303\u5f0f\u4e0b\u4f7f\u7528\u5c0f\u578b\u5f00\u6e90LLM\u7684\u53ef\u884c\u6027\uff0c\u517c\u5177\u53c2\u6570\u6548\u7387\u4e0e\u6027\u80fd\u3002", "keywords": "ICL, NER, \u96c6\u6210\u5b66\u4e60, \u5c0f\u578bLLM, \u53c2\u6570\u6548\u7387"}}
{"id": "2505.22761", "pdf": "https://arxiv.org/pdf/2505.22761", "abs": "https://arxiv.org/abs/2505.22761", "authors": ["Afila Ajithkumar Sophiya", "Akarsh K Nair", "Sepehr Maleki", "Senthil K. Krishnababu"], "title": "A comprehensive analysis of PINNs: Variants, Applications, and Challenges", "categories": ["cs.CE", "cs.AI"], "comment": null, "summary": "Physics Informed Neural Networks (PINNs) have been emerging as a powerful\ncomputational tool for solving differential equations. However, the\napplicability of these models is still in its initial stages and requires more\nstandardization to gain wider popularity. Through this survey, we present a\ncomprehensive overview of PINNs approaches exploring various aspects related to\ntheir architecture, variants, areas of application, real-world use cases,\nchallenges, and so on. Even though existing surveys can be identified, they\nfail to provide a comprehensive view as they primarily focus on either\ndifferent application scenarios or limit their study to a superficial level.\nThis survey attempts to bridge the gap in the existing literature by presenting\na detailed analysis of all these factors combined with recent advancements and\nstate-of-the-art research in PINNs. Additionally, we discuss prevalent\nchallenges in PINNs implementation and present some of the future research\ndirections as well. The overall contributions of the survey can be summarised\ninto three sections: A detailed overview of PINNs architecture and variants, a\nperformance analysis of PINNs on different equations and application domains\nhighlighting their features. Finally, we present a detailed discussion of\ncurrent issues and future research directions.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7efc\u8ff0\u4e86\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u7684\u67b6\u6784\u3001\u53d8\u4f53\u3001\u5e94\u7528\u9886\u57df\u53ca\u6311\u6218\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6587\u732e\u7684\u7a7a\u767d\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u63d0\u9ad8PINNs\u7684\u6807\u51c6\u5316\u7a0b\u5ea6\u5e76\u63a8\u52a8\u5176\u5e7f\u6cdb\u5e94\u7528\uff0c\u901a\u8fc7\u5168\u9762\u7efc\u8ff0\u5f25\u8865\u73b0\u6709\u7814\u7a76\u7684\u4e0d\u8db3\u3002", "method": "\u5bf9PINNs\u7684\u67b6\u6784\u3001\u53d8\u4f53\u3001\u5e94\u7528\u573a\u666f\u3001\u5b9e\u9645\u6848\u4f8b\u53ca\u6311\u6218\u8fdb\u884c\u7efc\u5408\u5206\u6790\uff0c\u7ed3\u5408\u6700\u65b0\u7814\u7a76\u8fdb\u5c55\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8ePINNs\u7684\u8be6\u7ec6\u6982\u89c8\uff0c\u6027\u80fd\u5206\u6790\u4ee5\u53ca\u5bf9\u95ee\u9898\u548c\u672a\u6765\u65b9\u5411\u7684\u6df1\u5ea6\u8ba8\u8bba\u3002", "conclusion": "PINNs\u5728\u89e3\u51b3\u5fae\u5206\u65b9\u7a0b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u6807\u51c6\u5316\u548c\u89e3\u51b3\u5b9e\u65bd\u4e2d\u7684\u6311\u6218\u3002", "keywords": "\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc, \u5fae\u5206\u65b9\u7a0b, \u6807\u51c6\u5316, \u5e94\u7528\u9886\u57df, \u7814\u7a76\u6311\u6218"}}
{"id": "2505.23009", "pdf": "https://arxiv.org/pdf/2505.23009", "abs": "https://arxiv.org/abs/2505.23009", "authors": ["Ruskin Raj Manku", "Yuzhi Tang", "Xingjian Shi", "Mu Li", "Alex Smola"], "title": "EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic, Expressiveness, and Linguistic Challenges Using Model-as-a-Judge", "categories": ["cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Text-to-Speech (TTS) benchmarks often fail to capture how well models handle\nnuanced and semantically complex text. Building on $\\textit{EmergentTTS}$, we\nintroduce $\\textit{EmergentTTS-Eval}$, a comprehensive benchmark covering six\nchallenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic\ncomplexity, complex pronunciation (e.g. URLs, formulas), and questions.\nCrucially, our framework automates both test-case generation and evaluation,\nmaking the benchmark easily extensible. Starting from a small set of\nhuman-written seed prompts, we iteratively extend them using LLMs to target\nspecific structural, phonetic and prosodic challenges, resulting in 1,645\ndiverse test cases. Moreover, we employ a model-as-a-judge approach, using a\nLarge Audio Language Model (LALM) to assess the speech across multiple\ndimensions such as expressed emotion, prosodic, intonational, and pronunciation\naccuracy. We evaluate state-of-the-art open-source and proprietary TTS systems,\nsuch as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval,\ndemonstrating its ability to reveal fine-grained performance differences.\nResults show that the model-as-a-judge approach offers robust TTS assessment\nand a high correlation with human preferences. We open source the evaluation\n$\\href{https://github.com/boson-ai/EmergentTTS-Eval-public}{code}$ and the\n$\\href{https://huggingface.co/datasets/bosonai/EmergentTTS-Eval}{dataset}$.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a EmergentTTS-Eval \u7684\u7efc\u5408TTS\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u516d\u79cd\u590d\u6742\u60c5\u5883\uff0c\u5e76\u91c7\u7528LLM\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u53ca\u6a21\u578b\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u9ad8\u7ec6\u7c92\u5ea6\u6027\u80fd\u8bc4\u4f30\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u7684TTS\u57fa\u51c6\u6d4b\u8bd5\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u6a21\u578b\u5904\u7406\u8bed\u4e49\u590d\u6742\u548c\u7ec6\u5fae\u6587\u672c\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u5168\u9762\u4e14\u6613\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u3002", "method": "\u901a\u8fc7LLM\u4ece\u4eba\u7c7b\u7f16\u5199\u7684\u5c11\u91cf\u79cd\u5b50\u63d0\u793a\u4e2d\u8fed\u4ee3\u751f\u62101,645\u4e2a\u591a\u6837\u5316\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u7528LALM\u6a21\u578b\u4f5c\u4e3a\u8bc4\u5224\u8005\uff0c\u4ece\u591a\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u8bed\u97f3\u8d28\u91cf\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0c\u6a21\u578b\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u533a\u5206\u4e0d\u540cTTS\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u4eba\u7c7b\u504f\u597d\u9ad8\u5ea6\u76f8\u5173\u3002", "conclusion": "EmergentTTS-Eval\u662f\u4e00\u4e2a\u6709\u6548\u7684TTS\u8bc4\u4f30\u5de5\u5177\uff0c\u80fd\u591f\u63ed\u793a\u7ec6\u5fae\u6027\u80fd\u5dee\u5f02\uff0c\u4e14\u8bc4\u4f30\u7ed3\u679c\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u3002", "keywords": "TTS\u57fa\u51c6\u6d4b\u8bd5, \u590d\u6742\u6587\u672c\u5904\u7406, \u6a21\u578b\u8bc4\u5224\u8005, \u8bed\u97f3\u8bc4\u4f30, \u81ea\u52a8\u6d4b\u8bd5\u751f\u6210"}}
{"id": "2505.23052", "pdf": "https://arxiv.org/pdf/2505.23052", "abs": "https://arxiv.org/abs/2505.23052", "authors": ["Jiarui Zhang", "Xiangyu Liu", "Yong Hu", "Chaoyue Niu", "Fan Wu", "Guihai Chen"], "title": "Query Routing for Retrieval-Augmented Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) significantly improves the performance\nof Large Language Models (LLMs) on knowledge-intensive tasks. However, varying\nresponse quality across LLMs under RAG necessitates intelligent routing\nmechanisms, which select the most suitable model for each query from multiple\nretrieval-augmented LLMs via a dedicated router model. We observe that external\ndocuments dynamically affect LLMs' ability to answer queries, while existing\nrouting methods, which rely on static parametric knowledge representations,\nexhibit suboptimal performance in RAG scenarios. To address this, we formally\ndefine the new retrieval-augmented LLM routing problem, incorporating the\ninfluence of retrieved documents into the routing framework. We propose\nRAGRouter, a RAG-aware routing design, which leverages document embeddings and\nRAG capability embeddings with contrastive learning to capture knowledge\nrepresentation shifts and enable informed routing decisions. Extensive\nexperiments on diverse knowledge-intensive tasks and retrieval settings show\nthat RAGRouter outperforms the best individual LLM by 3.61% on average and\nexisting routing methods by 3.29%-9.33%. With an extended score-threshold-based\nmechanism, it also achieves strong performance-efficiency trade-offs under\nlow-latency constraints.", "AI": {"tldr": "RAGRouter \u662f\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u8def\u7531\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u6587\u6863\u5d4c\u5165\u548c\u5bf9\u6bd4\u5b66\u4e60\u4f18\u5316 LLM \u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u8def\u7531\u65b9\u6cd5\u57fa\u4e8e\u9759\u6001\u77e5\u8bc6\u8868\u793a\uff0c\u5728 RAG \u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u52a8\u6001\u68c0\u7d22\u6587\u6863\u5f71\u54cd LLM \u56de\u7b54\u80fd\u529b\uff0c\u9700\u8981\u65b0\u7684\u8def\u7531\u6846\u67b6\u3002", "method": "\u63d0\u51fa RAGRouter\uff0c\u7ed3\u5408\u6587\u6863\u5d4c\u5165\u548c RAG \u80fd\u529b\u5d4c\u5165\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6355\u6349\u77e5\u8bc6\u8868\u793a\u53d8\u5316\uff0c\u5b9e\u73b0\u667a\u80fd\u8def\u7531\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u663e\u793a RAGRouter \u5e73\u5747\u6bd4\u6700\u4f73\u5355 LLM \u63d0\u5347 3.61%\uff0c\u6bd4\u73b0\u6709\u8def\u7531\u65b9\u6cd5\u9ad8 3.29%-9.33%\uff0c\u5e76\u5728\u4f4e\u5ef6\u8fdf\u4e0b\u5b9e\u73b0\u9ad8\u6548\u6027\u80fd\u6743\u8861\u3002", "conclusion": "RAGRouter \u901a\u8fc7\u52a8\u6001\u8def\u7531\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86 RAG \u573a\u666f\u4e0b\u7684\u6a21\u578b\u9009\u62e9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u4e0e\u6548\u7387\u3002", "keywords": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210, \u5927\u8bed\u8a00\u6a21\u578b, \u8def\u7531\u673a\u5236, \u5bf9\u6bd4\u5b66\u4e60, \u6587\u6863\u5d4c\u5165"}}
{"id": "2505.22762", "pdf": "https://arxiv.org/pdf/2505.22762", "abs": "https://arxiv.org/abs/2505.22762", "authors": ["Marco Colussi", "Dragan Ahmetovic", "Sergio Mascetti"], "title": "MIAS-SAM: Medical Image Anomaly Segmentation without thresholding", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper presents MIAS-SAM, a novel approach for the segmentation of\nanomalous regions in medical images. MIAS-SAM uses a patch-based memory bank to\nstore relevant image features, which are extracted from normal data using the\nSAM encoder. At inference time, the embedding patches extracted from the SAM\nencoder are compared with those in the memory bank to obtain the anomaly map.\nFinally, MIAS-SAM computes the center of gravity of the anomaly map to prompt\nthe SAM decoder, obtaining an accurate segmentation from the previously\nextracted features. Differently from prior works, MIAS-SAM does not require to\ndefine a threshold value to obtain the segmentation from the anomaly map.\nExperimental results conducted on three publicly available datasets, each with\na different imaging modality (Brain MRI, Liver CT, and Retina OCT) show\naccurate anomaly segmentation capabilities measured using DICE score. The code\nis available at: https://github.com/warpcut/MIAS-SAM", "AI": {"tldr": "MIAS-SAM is a novel method for medical image anomaly segmentation using a patch-based memory bank and SAM encoder. It avoids thresholding and shows high accuracy across multiple datasets.", "motivation": "To improve anomaly segmentation in medical images by leveraging a memory bank of normal features and eliminating the need for threshold selection.", "method": "Uses a SAM encoder to extract features from normal data, stores them in a patch-based memory bank, compares test embeddings to generate anomaly maps, and computes their center of gravity to prompt SAM decoder for segmentation.", "result": "Demonstrates accurate anomaly segmentation on Brain MRI, Liver CT, and Retina OCT datasets, measured via DICE score.", "conclusion": "MIAS-SAM effectively segments anomalies without thresholding, achieving robust performance across diverse medical imaging modalities.", "keywords": "anomaly segmentation, medical images, SAM encoder, patch-based memory bank, DICE score"}}
{"id": "2505.23013", "pdf": "https://arxiv.org/pdf/2505.23013", "abs": "https://arxiv.org/abs/2505.23013", "authors": ["Liangkai Hang", "Junjie Yao", "Zhiwei Bai", "Tianyi Chen", "Yang Chen", "Rongjie Diao", "Hezhou Li", "Pengxiao Lin", "Zhiwei Wang", "Cheng Xu", "Zhongwang Zhang", "Zhangchen Zhou", "Zhiyu Li", "Zehao Lin", "Kai Chen", "Feiyu Xiong", "Yaoyu Zhang", "Weinan E", "Hongkang Yang", "Zhi-Qin John Xu"], "title": "Scalable Complexity Control Facilitates Reasoning Ability of LLMs", "categories": ["cs.LG"], "comment": null, "summary": "The reasoning ability of large language models (LLMs) has been rapidly\nadvancing in recent years, attracting interest in more fundamental approaches\nthat can reliably enhance their generalizability. This work demonstrates that\nmodel complexity control, conveniently implementable by adjusting the\ninitialization rate and weight decay coefficient, improves the scaling law of\nLLMs consistently over varying model sizes and data sizes. This gain is further\nillustrated by comparing the benchmark performance of 2.4B models pretrained on\n1T tokens with different complexity hyperparameters. Instead of fixing the\ninitialization std, we found that a constant initialization rate (the exponent\nof std) enables the scaling law to descend faster in both model and data sizes.\nThese results indicate that complexity control is a promising direction for the\ncontinual advancement of LLMs.", "AI": {"tldr": "\u901a\u8fc7\u8c03\u6574\u521d\u59cb\u5316\u7387\u548c\u6743\u91cd\u8870\u51cf\u7cfb\u6570\u63a7\u5236\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u53ef\u4ee5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6269\u5c55\u89c4\u5f8b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u53ef\u9760\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63a2\u7d22\u6a21\u578b\u590d\u6742\u5ea6\u63a7\u5236\u5bf9\u5176\u6269\u5c55\u89c4\u5f8b\u7684\u5f71\u54cd\u3002", "method": "\u8c03\u6574\u521d\u59cb\u5316\u7387\u548c\u6743\u91cd\u8870\u51cf\u7cfb\u6570\u6765\u63a7\u5236\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u590d\u6742\u5ea6\u8d85\u53c2\u6570\u4e0b2.4B\u6a21\u578b\u57281T token\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6052\u5b9a\u521d\u59cb\u5316\u7387\u4f7f\u6a21\u578b\u6269\u5c55\u89c4\u5f8b\u5728\u6a21\u578b\u548c\u6570\u636e\u89c4\u6a21\u4e0a\u4e0b\u964d\u66f4\u5feb\uff0c\u590d\u6742\u5ea6\u63a7\u5236\u80fd\u663e\u8457\u63d0\u5347LLM\u7684\u6027\u80fd\u3002", "conclusion": "\u6a21\u578b\u590d\u6742\u5ea6\u63a7\u5236\u662f\u6301\u7eed\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u6709\u6548\u65b9\u5411\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u6269\u5c55\u89c4\u5f8b\uff0c\u521d\u59cb\u5316\u7387\uff0c\u6743\u91cd\u8870\u51cf"}}
{"id": "2505.23060", "pdf": "https://arxiv.org/pdf/2505.23060", "abs": "https://arxiv.org/abs/2505.23060", "authors": ["Jeonghun Cho", "Deokhyung Kang", "Hyounghun Kim", "Gary Geunbae Lee"], "title": "Self-Correcting Code Generation Using Small Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Self-correction has demonstrated potential in code generation by allowing\nlanguage models to revise and improve their outputs through successive\nrefinement. Recent studies have explored prompting-based strategies that\nincorporate verification or feedback loops using proprietary models, as well as\ntraining-based methods that leverage their strong reasoning capabilities.\nHowever, whether smaller models possess the capacity to effectively guide their\noutputs through self-reflection remains unexplored. Our findings reveal that\nsmaller models struggle to exhibit reflective revision behavior across both\nself-correction paradigms. In response, we introduce CoCoS, an approach\ndesigned to enhance the ability of small language models for multi-turn code\ncorrection. Specifically, we propose an online reinforcement learning objective\nthat trains the model to confidently maintain correct outputs while\nprogressively correcting incorrect outputs as turns proceed. Our approach\nfeatures an accumulated reward function that aggregates rewards across the\nentire trajectory and a fine-grained reward better suited to multi-turn\ncorrection scenarios. This facilitates the model in enhancing initial response\nquality while achieving substantial improvements through self-correction. With\n1B-scale models, CoCoS achieves improvements of 35.8% on the MBPP and 27.7% on\nHumanEval compared to the baselines.", "AI": {"tldr": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u901a\u8fc7\u81ea\u6211\u7ea0\u6b63\u63d0\u5347\u8f93\u51fa\u8d28\u91cf\u7684\u80fd\u529b\u6709\u9650\uff0cCoCoS\u65b9\u6cd5\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u663e\u8457\u6539\u5584\u4e86\u8fd9\u4e00\u80fd\u529b\u3002", "motivation": "\u63a2\u7a76\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u6709\u6548\u6539\u8fdb\u4ee3\u7801\u751f\u6210\u8f93\u51fa\u3002", "method": "\u63d0\u51faCoCoS\u65b9\u6cd5\uff0c\u91c7\u7528\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\uff0c\u8bbe\u8ba1\u7d2f\u79ef\u5956\u52b1\u51fd\u6570\u548c\u7ec6\u7c92\u5ea6\u5956\u52b1\u673a\u5236\u3002", "result": "CoCoS\u57281B\u89c4\u6a21\u6a21\u578b\u4e0a\uff0cMBPP\u548cHumanEval\u4efb\u52a1\u5206\u522b\u63d0\u534735.8%\u548c27.7%\u3002", "conclusion": "CoCoS\u6709\u6548\u63d0\u5347\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8f6e\u4ee3\u7801\u7ea0\u6b63\u80fd\u529b\u3002", "keywords": "\u4ee3\u7801\u751f\u6210\u3001\u81ea\u6211\u7ea0\u6b63\u3001\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u3001\u5f3a\u5316\u5b66\u4e60"}}
{"id": "2505.22767", "pdf": "https://arxiv.org/pdf/2505.22767", "abs": "https://arxiv.org/abs/2505.22767", "authors": ["Eleni Vasilaki"], "title": "In Dialogue with Intelligence: Rethinking Large Language Models as Collective Knowledge", "categories": ["cs.HC", "cs.AI"], "comment": "6 pages, 1 table", "summary": "Large Language Models (LLMs) are typically analysed through architectural,\nbehavioural, or training-data lenses. This article offers a theoretical and\nexperiential re-framing: LLMs as dynamic instantiations of Collective human\nKnowledge (CK), where intelligence is evoked through dialogue rather than\nstored statically. Drawing on concepts from neuroscience and AI, and grounded\nin sustained interaction with ChatGPT-4, I examine emergent dialogue patterns,\nthe implications of fine-tuning, and the notion of co-augmentation: mutual\nenhancement between human and machine cognition. This perspective offers a new\nlens for understanding interaction, representation, and agency in contemporary\nAI systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u91cd\u65b0\u5b9a\u4e49\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u52a8\u6001\u96c6\u4f53\u4eba\u7c7b\u77e5\u8bc6\uff08CK\uff09\u7684\u4f53\u73b0\uff0c\u5f3a\u8c03\u901a\u8fc7\u5bf9\u8bdd\u800c\u975e\u9759\u6001\u5b58\u50a8\u6765\u6fc0\u53d1\u667a\u80fd\uff0c\u63a2\u8ba8\u4e86\u5bf9\u8bdd\u6a21\u5f0f\u3001\u5fae\u8c03\u7684\u5f71\u54cd\u53ca\u4eba\u673a\u534f\u540c\u589e\u5f3a\u3002", "motivation": "\u4f20\u7edf\u7814\u7a76\u591a\u4ece\u67b6\u6784\u3001\u884c\u4e3a\u6216\u8bad\u7ec3\u6570\u636e\u89d2\u5ea6\u5206\u6790LLMs\uff0c\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u65b0\u89c6\u89d2\uff1a\u5c06LLMs\u89c6\u4e3a\u52a8\u6001\u96c6\u4f53\u77e5\u8bc6\u7684\u4f53\u73b0\uff0c\u4ee5\u66f4\u6df1\u5165\u5730\u7406\u89e3\u5176\u4ea4\u4e92\u3001\u8868\u5f81\u548c\u4ee3\u7406\u884c\u4e3a\u3002", "method": "\u7ed3\u5408\u795e\u7ecf\u79d1\u5b66\u4e0eAI\u7406\u8bba\uff0c\u901a\u8fc7\u4e0eChatGPT-4\u7684\u6301\u7eed\u4e92\u52a8\uff0c\u5206\u6790\u5bf9\u8bdd\u6a21\u5f0f\u3001\u5fae\u8c03\u7684\u5f71\u54cd\u53ca\u4eba\u673a\u8ba4\u77e5\u534f\u540c\u589e\u5f3a\uff08co-augmentation\uff09\u3002", "result": "\u63ed\u793a\u4e86\u5bf9\u8bdd\u5728LLMs\u667a\u80fd\u8868\u73b0\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\uff0c\u4ee5\u53ca\u4eba\u673a\u534f\u540c\u5982\u4f55\u76f8\u4e92\u63d0\u5347\u8ba4\u77e5\u80fd\u529b\uff0c\u4e3aAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7406\u89e3\u6846\u67b6\u3002", "conclusion": "\u52a8\u6001\u96c6\u4f53\u77e5\u8bc6\u89c6\u89d2\u4e3aLLMs\u7684\u4ea4\u4e92\u3001\u8868\u5f81\u548c\u4ee3\u7406\u673a\u5236\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u8bfb\uff0c\u5f3a\u8c03\u5bf9\u8bdd\u662f\u667a\u80fd\u7684\u6838\u5fc3\u9a71\u52a8\u529b\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u96c6\u4f53\u77e5\u8bc6, \u5bf9\u8bdd\u667a\u80fd, \u4eba\u673a\u534f\u540c, \u52a8\u6001\u8ba4\u77e5"}}
{"id": "2505.23014", "pdf": "https://arxiv.org/pdf/2505.23014", "abs": "https://arxiv.org/abs/2505.23014", "authors": ["Juwei Yue", "Haikuo Li", "Jiawei Sheng", "Xiaodong Li", "Taoyu Su", "Tingwen Liu", "Li Guo"], "title": "Hyperbolic-PDE GNN: Spectral Graph Neural Networks in the Perspective of A System of Hyperbolic Partial Differential Equations", "categories": ["cs.LG"], "comment": "18 pages, 2 figures, published to ICML 2025", "summary": "Graph neural networks (GNNs) leverage message passing mechanisms to learn the\ntopological features of graph data. Traditional GNNs learns node features in a\nspatial domain unrelated to the topology, which can hardly ensure topological\nfeatures. In this paper, we formulates message passing as a system of\nhyperbolic partial differential equations (hyperbolic PDEs), constituting a\ndynamical system that explicitly maps node representations into a particular\nsolution space. This solution space is spanned by a set of eigenvectors\ndescribing the topological structure of graphs. Within this system, for any\nmoment in time, a node features can be decomposed into a superposition of the\nbasis of eigenvectors. This not only enhances the interpretability of message\npassing but also enables the explicit extraction of fundamental characteristics\nabout the topological structure. Furthermore, by solving this system of\nhyperbolic partial differential equations, we establish a connection with\nspectral graph neural networks (spectral GNNs), serving as a message passing\nenhancement paradigm for spectral GNNs.We further introduce polynomials to\napproximate arbitrary filter functions. Extensive experiments demonstrate that\nthe paradigm of hyperbolic PDEs not only exhibits strong flexibility but also\nsignificantly enhances the performance of various spectral GNNs across diverse\ngraph tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6d88\u606f\u4f20\u9012\u673a\u5236\u5efa\u6a21\u4e3a\u53cc\u66f2\u504f\u5fae\u5206\u65b9\u7a0b\u7cfb\u7edf\uff0c\u4ee5\u663e\u5f0f\u6620\u5c04\u8282\u70b9\u8868\u793a\u5230\u7279\u89e3\u7a7a\u95f4\uff0c\u63d0\u5347\u6d88\u606f\u4f20\u9012\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfGNN\u5728\u7a7a\u95f4\u57df\u5b66\u4e60\u8282\u70b9\u7279\u5f81\uff0c\u96be\u4ee5\u786e\u4fdd\u62d3\u6251\u7279\u5f81\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53cc\u66f2PDE\u7cfb\u7edf\u663e\u5f0f\u6355\u6349\u56fe\u7684\u62d3\u6251\u7ed3\u6784\uff0c\u589e\u5f3a\u6d88\u606f\u4f20\u9012\u7684\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002", "method": "\u5c06\u6d88\u606f\u4f20\u9012\u5efa\u6a21\u4e3a\u53cc\u66f2PDE\u7cfb\u7edf\uff0c\u8282\u70b9\u7279\u5f81\u53ef\u5206\u89e3\u4e3a\u7279\u5f81\u5411\u91cf\u7684\u53e0\u52a0\uff0c\u5e76\u5f15\u5165\u591a\u9879\u5f0f\u903c\u8fd1\u4efb\u610f\u6ee4\u6ce2\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5177\u6709\u5f3a\u7075\u6d3b\u6027\uff0c\u8fd8\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u8c31GNN\u5728\u4e0d\u540c\u56fe\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u53cc\u66f2PDE\u8303\u5f0f\u4e3a\u8c31GNN\u63d0\u4f9b\u4e86\u6d88\u606f\u4f20\u9012\u589e\u5f3a\u7684\u65b0\u65b9\u6cd5\uff0c\u517c\u5177\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u63d0\u5347\u3002", "keywords": "\u56fe\u795e\u7ecf\u7f51\u7edc, \u53cc\u66f2\u504f\u5fae\u5206\u65b9\u7a0b, \u6d88\u606f\u4f20\u9012, \u8c31\u65b9\u6cd5, \u62d3\u6251\u7279\u5f81"}}
{"id": "2505.23065", "pdf": "https://arxiv.org/pdf/2505.23065", "abs": "https://arxiv.org/abs/2505.23065", "authors": ["Hongcheng Guo", "Zheyong Xie", "Shaosheng Cao", "Boyang Wang", "Weiting Liu", "Anjie Le", "Lei Li", "Zhoujun Li"], "title": "SNS-Bench-VL: Benchmarking Multimodal Large Language Models in Social Networking Services", "categories": ["cs.CL"], "comment": null, "summary": "With the increasing integration of visual and textual content in Social\nNetworking Services (SNS), evaluating the multimodal capabilities of Large\nLanguage Models (LLMs) is crucial for enhancing user experience, content\nunderstanding, and platform intelligence. Existing benchmarks primarily focus\non text-centric tasks, lacking coverage of the multimodal contexts prevalent in\nmodern SNS ecosystems. In this paper, we introduce SNS-Bench-VL, a\ncomprehensive multimodal benchmark designed to assess the performance of\nVision-Language LLMs in real-world social media scenarios. SNS-Bench-VL\nincorporates images and text across 8 multimodal tasks, including note\ncomprehension, user engagement analysis, information retrieval, and\npersonalized recommendation. It comprises 4,001 carefully curated multimodal\nquestion-answer pairs, covering single-choice, multiple-choice, and open-ended\ntasks. We evaluate over 25 state-of-the-art multimodal LLMs, analyzing their\nperformance across tasks. Our findings highlight persistent challenges in\nmultimodal social context comprehension. We hope SNS-Bench-VL will inspire\nfuture research towards robust, context-aware, and human-aligned multimodal\nintelligence for next-generation social networking services.", "AI": {"tldr": "\u5f15\u5165SNS-Bench-VL\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b\u5728\u793e\u4ea4\u7f51\u7edc\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u7eaf\u6587\u672c\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u73b0\u4ee3\u793e\u4ea4\u7f51\u7edc\u4e2d\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u7684\u8986\u76d6\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u6db5\u76d6\u591a\u6a21\u6001\u4efb\u52a1\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b8\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u7684SNS-Bench-VL\uff0c\u5305\u62ec\u7b14\u8bb0\u7406\u89e3\u3001\u7528\u6237\u53c2\u4e0e\u5ea6\u5206\u6790\u7b49\uff0c\u63d0\u4f9b4,001\u5bf9\u95ee\u9898-\u7b54\u6848\u6570\u636e\uff0c\u5e76\u8bc4\u4f30\u4e8625\u4e2a\u9886\u5148\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u591a\u6a21\u6001\u793e\u4ea4\u4e0a\u4e0b\u6587\u7406\u89e3\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\u3002", "conclusion": "SNS-Bench-VL\u6709\u671b\u63a8\u52a8\u672a\u6765\u7814\u7a76\uff0c\u4ee5\u5f00\u53d1\u66f4\u5177\u9c81\u68d2\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u591a\u6a21\u6001\u667a\u80fd\u6280\u672f\u3002", "keywords": "\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5,\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b,\u793e\u4ea4\u7f51\u7edc,\u7528\u6237\u53c2\u4e0e\u5ea6\u5206\u6790,\u4e2a\u6027\u5316\u63a8\u8350"}}
{"id": "2505.23017", "pdf": "https://arxiv.org/pdf/2505.23017", "abs": "https://arxiv.org/abs/2505.23017", "authors": ["Xingjian Wu", "Xiangfei Qiu", "Hongfan Gao", "Jilin Hu", "Bin Yang", "Chenjuan Guo"], "title": "$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Probabilistic Time Series Forecasting (PTSF) plays a crucial role in\ndecision-making across various fields, including economics, energy, and\ntransportation. Most existing methods excell at short-term forecasting, while\noverlooking the hurdles of Long-term Probabilistic Time Series Forecasting\n(LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have\na significant adverse effect on prediction accuracy, and make generative models\ninefficient by increasing the cost of each iteration. To overcome these\nlimitations, we introduce $K^2$VAE, an efficient VAE-based generative model\nthat leverages a KoopmanNet to transform nonlinear time series into a linear\ndynamical system, and devises a KalmanNet to refine predictions and model\nuncertainty in such linear system, which reduces error accumulation in\nlong-term forecasting. Extensive experiments demonstrate that $K^2$VAE\noutperforms state-of-the-art methods in both short- and long-term PTSF,\nproviding a more efficient and accurate solution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faK\u00b2VAE\u6a21\u578b\uff0c\u7ed3\u5408KoopmanNet\u548cKalmanNet\uff0c\u89e3\u51b3\u4e86\u957f\u671f\u6982\u7387\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u6548\u7387\u4e0e\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u64c5\u957f\u77ed\u671f\u9884\u6d4b\uff0c\u4f46\u957f\u671f\u9884\u6d4b\u9762\u4e34\u975e\u7ebf\u6027\u52a8\u6001\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528VAE\u6846\u67b6\uff0c\u7ed3\u5408KoopmanNet\u7ebf\u6027\u5316\u65f6\u95f4\u5e8f\u5217\uff0cKalmanNet\u4f18\u5316\u9884\u6d4b\u4e0e\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3002", "result": "K\u00b2VAE\u5728\u77ed\u671f\u548c\u957f\u671f\u9884\u6d4b\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u8bef\u5dee\u79ef\u7d2f\u3002", "conclusion": "K\u00b2VAE\u4e3a\u6982\u7387\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u6982\u7387\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3001K\u00b2VAE\u3001KoopmanNet\u3001KalmanNet\u3001\u957f\u671f\u9884\u6d4b"}}
{"id": "2505.23078", "pdf": "https://arxiv.org/pdf/2505.23078", "abs": "https://arxiv.org/abs/2505.23078", "authors": ["Yuu Jinnai"], "title": "Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Document-level text generation tasks are known to be more difficult than\nsentence-level text generation tasks as they require the understanding of\nlonger context to generate high-quality texts. In this paper, we investigate\nthe adaption of Minimum Bayes Risk (MBR) decoding for document-level text\ngeneration tasks. MBR decoding makes use of a utility function to estimate the\noutput with the highest expected utility from a set of candidate outputs.\nAlthough MBR decoding is shown to be effective in a wide range of\nsentence-level text generation tasks, its performance on document-level text\ngeneration tasks is limited as many of the utility functions are designed for\nevaluating the utility of sentences. To this end, we propose MBR-OT, a variant\nof MBR decoding using Wasserstein distance to compute the utility of a document\nusing a sentence-level utility function. The experimental result shows that the\nperformance of MBR-OT outperforms that of the standard MBR in document-level\nmachine translation, text simplification, and dense image captioning tasks. Our\ncode is available at https://github.com/jinnaiyuu/mbr-optimal-transport", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5c06\u6700\u5c0f\u8d1d\u53f6\u65af\u98ce\u9669\uff08MBR\uff09\u89e3\u7801\u5e94\u7528\u4e8e\u6587\u6863\u7ea7\u6587\u672c\u751f\u6210\u4efb\u52a1\u7684\u6539\u8fdb\u65b9\u6cd5MBR-OT\uff0c\u901a\u8fc7Wasserstein\u8ddd\u79bb\u8ba1\u7b97\u6587\u6863\u6548\u7528\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6MBR\u3002", "motivation": "\u6587\u6863\u7ea7\u6587\u672c\u751f\u6210\u4efb\u52a1\u56e0\u9700\u8981\u7406\u89e3\u66f4\u957f\u7684\u4e0a\u4e0b\u6587\u800c\u8f83\u53e5\u5b50\u7ea7\u4efb\u52a1\u66f4\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u7684MBR\u89e3\u7801\u6548\u7528\u51fd\u6570\u591a\u9488\u5bf9\u53e5\u5b50\u8bbe\u8ba1\uff0c\u9650\u5236\u4e86\u5176\u5728\u6587\u6863\u7ea7\u4efb\u52a1\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faMBR-OT\uff0c\u5229\u7528Wasserstein\u8ddd\u79bb\u7ed3\u5408\u53e5\u5b50\u7ea7\u6548\u7528\u51fd\u6570\u8ba1\u7b97\u6587\u6863\u6548\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMBR-OT\u5728\u6587\u6863\u7ea7\u673a\u5668\u7ffb\u8bd1\u3001\u6587\u672c\u7b80\u5316\u548c\u5bc6\u96c6\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6807\u51c6MBR\u3002", "conclusion": "MBR-OT\u901a\u8fc7\u4f18\u5316\u6548\u7528\u8ba1\u7b97\u65b9\u5f0f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6587\u6863\u7ea7\u6587\u672c\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\u3002", "keywords": "\u6587\u6863\u7ea7\u6587\u672c\u751f\u6210\uff0c\u6700\u5c0f\u8d1d\u53f6\u65af\u98ce\u9669\u89e3\u7801\uff0cWasserstein\u8ddd\u79bb\uff0c\u6548\u7528\u51fd\u6570"}}
{"id": "2505.23022", "pdf": "https://arxiv.org/pdf/2505.23022", "abs": "https://arxiv.org/abs/2505.23022", "authors": ["Yinghao Tang", "Tingfeng Lan", "Xiuqi Huang", "Hui Lu", "Wei Chen"], "title": "SCORPIO: Serving the Right Requests at the Right Time for Heterogeneous SLOs in LLM Inference", "categories": ["cs.LG"], "comment": null, "summary": "Existing Large Language Model (LLM) serving systems prioritize maximum\nthroughput. They often neglect Service Level Objectives (SLOs) such as Time to\nFirst Token (TTFT) and Time Per Output Token (TPOT), which leads to suboptimal\nSLO attainment. This paper introduces SCORPIO, an SLO-oriented LLM serving\nsystem designed to maximize system goodput and SLO attainment for workloads\nwith heterogeneous SLOs. Our core insight is to exploit SLO heterogeneity for\nadaptive scheduling across admission control, queue management, and batch\nselection. SCORPIO features a TTFT Guard, which employs least-deadline-first\nreordering and rejects unattainable requests, and a TPOT Guard, which utilizes\na VBS-based admission control and a novel credit-based batching mechanism. Both\nguards are supported by a predictive module. Evaluations demonstrate that\nSCORPIO improves system goodput by up to 14.4X and SLO adherence by up to 46.5%\ncompared to state-of-the-art baselines.", "AI": {"tldr": "SCORPIO\u662f\u4e00\u79cd\u9762\u5411SLO\u7684LLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u5ea6\u63d0\u5347\u7cfb\u7edf\u541e\u5410\u548cSLO\u8fbe\u6210\u7387\u3002", "motivation": "\u73b0\u6709LLM\u670d\u52a1\u7cfb\u7edf\u5ffd\u89c6SLO\uff08\u5982TTFT\u548cTPOT\uff09\uff0c\u5bfc\u81f4SLO\u8fbe\u6210\u7387\u4e0d\u8db3\u3002", "method": "\u91c7\u7528TTFT Guard\u548cTPOT Guard\uff0c\u7ed3\u5408\u9884\u6d4b\u6a21\u5757\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u8c03\u5ea6\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\uff0c\u7cfb\u7edf\u541e\u5410\u63d0\u534714.4\u500d\uff0cSLO\u8fbe\u6210\u7387\u63d0\u9ad846.5%\u3002", "conclusion": "SCORPIO\u6709\u6548\u4f18\u5316\u4e86LLM\u670d\u52a1\u7684SLO\u8868\u73b0\u3002", "keywords": "LLM, SLO, SCORPIO, TTFT, TPOT"}}
{"id": "2505.23108", "pdf": "https://arxiv.org/pdf/2505.23108", "abs": "https://arxiv.org/abs/2505.23108", "authors": ["Zexuan Li", "Hongliang Dai", "Piji Li"], "title": "Generating Diverse Training Samples for Relation Extraction with Large Language Models", "categories": ["cs.CL"], "comment": "ACL2025 Main", "summary": "Using Large Language Models (LLMs) to generate training data can potentially\nbe a preferable way to improve zero or few-shot NLP tasks. However, many\nproblems remain to be investigated for this direction. For the task of Relation\nExtraction (RE), we find that samples generated by directly prompting LLMs may\neasily have high structural similarities with each other. They tend to use a\nlimited variety of phrasing while expressing the relation between a pair of\nentities. Therefore, in this paper, we study how to effectively improve the\ndiversity of the training samples generated with LLMs for RE, while also\nmaintaining their correctness. We first try to make the LLMs produce dissimilar\nsamples by directly giving instructions in In-Context Learning (ICL) prompts.\nThen, we propose an approach to fine-tune LLMs for diversity training sample\ngeneration through Direct Preference Optimization (DPO). Our experiments on\ncommonly used RE datasets show that both attempts can improve the quality of\nthe generated training data. We also find that comparing with directly\nperforming RE with an LLM, training a non-LLM RE model with its generated\nsamples may lead to better performance.", "AI": {"tldr": "\u4e3a\u4e86\u63d0\u9ad8\u5c11\u6837\u672c\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u901a\u8fc7\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8bad\u7ec3\u6837\u672c\u7684\u591a\u6837\u6027\u548c\u6b63\u786e\u6027\uff0c\u4f7f\u7528\u4e86ICL\u63d0\u793a\u548cDPO\u5fae\u8c03\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u63d0\u5347\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u53d1\u73b0\u975eLLM\u6a21\u578b\u4f7f\u7528\u751f\u6210\u7684\u6837\u672c\u8bad\u7ec3\u6548\u679c\u66f4\u4f73\u3002", "motivation": "\u7814\u7a76\u4e2d\u53d1\u73b0\u76f4\u63a5\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u6837\u672c\u7ed3\u6784\u76f8\u4f3c\u5ea6\u9ad8\u3001\u8868\u8fbe\u65b9\u5f0f\u5355\u4e00\uff0c\u5f71\u54cd\u4e86\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "method": "\u91c7\u7528\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\u5728ICL\u63d0\u793a\u4e2d\u76f4\u63a5\u52a0\u5165\u6307\u4ee4\u4ee5\u63d0\u9ad8\u6837\u672c\u591a\u6837\u6027\uff1b2\uff09\u901a\u8fc7DPO\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u4ee5\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5747\u80fd\u6709\u6548\u63d0\u5347\u751f\u6210\u6837\u672c\u7684\u8d28\u91cf\uff0c\u4e14\u7528\u8fd9\u4e9b\u6837\u672c\u8bad\u7ec3\u7684\u975eLLM\u5173\u7cfb\u62bd\u53d6\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u76f4\u63a5\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5173\u7cfb\u62bd\u53d6\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u6837\u672c\u751f\u6210\u65b9\u6cd5\uff0c\u53ef\u63d0\u5347\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u7684\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u6027\u80fd\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u5173\u7cfb\u62bd\u53d6,\u6837\u672c\u591a\u6837\u6027,ICL,DPO"}}
{"id": "2505.23024", "pdf": "https://arxiv.org/pdf/2505.23024", "abs": "https://arxiv.org/abs/2505.23024", "authors": ["Zhihao Wang", "Wenke Huang", "Tian Chen", "Zekun Shi", "Guancheng Wan", "Yu Qiao", "Bin Yang", "Jian Wang", "Bing Li", "Mang Ye"], "title": "An Empirical Study of Federated Prompt Learning for Vision Language Model", "categories": ["cs.LG"], "comment": null, "summary": "The Vision Language Model (VLM) excels in aligning vision and language\nrepresentations, and prompt learning has emerged as a key technique for\nadapting such models to downstream tasks. However, the application of prompt\nlearning with VLM in federated learning (\\fl{}) scenarios remains\nunderexplored. This paper systematically investigates the behavioral\ndifferences between language prompt learning (LPT) and vision prompt learning\n(VPT) under data heterogeneity challenges, including label skew and domain\nshift. We conduct extensive experiments to evaluate the impact of various \\fl{}\nand prompt configurations, such as client scale, aggregation strategies, and\nprompt length, to assess the robustness of Federated Prompt Learning (FPL).\nFurthermore, we explore strategies for enhancing prompt learning in complex\nscenarios where label skew and domain shift coexist, including leveraging both\nprompt types when computational resources allow. Our findings offer practical\ninsights into optimizing prompt learning in federated settings, contributing to\nthe broader deployment of VLMs in privacy-preserving environments.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u63d0\u793a\u5b66\u4e60\u5e94\u7528\uff0c\u6bd4\u8f83\u4e86\u8bed\u8a00\u63d0\u793a\u5b66\u4e60\uff08LPT\uff09\u548c\u89c6\u89c9\u63d0\u793a\u5b66\u4e60\uff08VPT\uff09\u5728\u6570\u636e\u5f02\u6784\u6027\uff08\u5982\u6807\u7b7e\u504f\u659c\u548c\u57df\u504f\u79fb\uff09\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u5c3d\u7ba1VLM\u5728\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u4f7f\u7528\u63d0\u793a\u5b66\u4e60\u7684\u63a2\u7d22\u4e0d\u8db3\uff0c\u5c24\u5176\u9762\u5bf9\u6570\u636e\u5f02\u6784\u6027\uff08\u5982\u6807\u7b7e\u504f\u659c\u548c\u57df\u504f\u79fb\uff09\u65f6\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u6bd4\u8f83LPT\u548cVPT\u5728\u4e0d\u540cFL\u914d\u7f6e\uff08\u5982\u5ba2\u6237\u7aef\u89c4\u6a21\u3001\u805a\u5408\u7b56\u7565\u548c\u63d0\u793a\u957f\u5ea6\uff09\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u7d22\u4e86\u5728\u6807\u7b7e\u504f\u659c\u548c\u57df\u504f\u79fb\u5171\u5b58\u65f6\u7684\u4f18\u5316\u7b56\u7565\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86LPT\u548cVPT\u5728FL\u4e2d\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u8ba1\u7b97\u8d44\u6e90\u5141\u8bb8\u65f6\u7ed3\u5408\u4e24\u79cd\u63d0\u793a\u7c7b\u578b\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8054\u90a6\u5b66\u4e60\u4e2dVLM\u7684\u63d0\u793a\u5b66\u4e60\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u4fc3\u8fdb\u4e86VLM\u5728\u9690\u79c1\u4fdd\u62a4\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09, \u8054\u90a6\u5b66\u4e60\uff08FL\uff09, \u63d0\u793a\u5b66\u4e60, \u6570\u636e\u5f02\u6784\u6027, \u6807\u7b7e\u504f\u659c, \u57df\u504f\u79fb"}}
{"id": "2505.23114", "pdf": "https://arxiv.org/pdf/2505.23114", "abs": "https://arxiv.org/abs/2505.23114", "authors": ["Seohyeong Lee", "Eunwon Kim", "Hwaran Lee", "Buru Chang"], "title": "Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data", "categories": ["cs.CL"], "comment": null, "summary": "Human preference data plays a critical role in aligning large language models\n(LLMs) with human values. However, collecting such data is often expensive and\ninefficient, posing a significant scalability challenge. To address this, we\nintroduce Alignment Data Map, a GPT-4o-assisted tool for analyzing and\ndiagnosing preference data. Using GPT-4o as a proxy for LLM alignment, we\ncompute alignment scores for LLM-generated responses to instructions from\nexisting preference datasets. These scores are then used to construct an\nAlignment Data Map based on their mean and variance. Our experiments show that\nusing only 33 percent of the data, specifically samples in the high-mean,\nlow-variance region, achieves performance comparable to or better than using\nthe entire dataset. This finding suggests that the Alignment Data Map can\nsignificantly improve data collection efficiency by identifying high-quality\nsamples for LLM alignment without requiring explicit annotations. Moreover, the\nAlignment Data Map can diagnose existing preference datasets. Our analysis\nshows that it effectively detects low-impact or potentially misannotated\nsamples. Source code is available online.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAlignment Data Map\u5de5\u5177\uff0c\u901a\u8fc7GPT-4o\u5206\u6790\u504f\u597d\u6570\u636e\uff0c\u4ec5\u970033%\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u4e0e\u5168\u6570\u636e\u96c6\u76f8\u5f53\u6216\u66f4\u597d\u7684\u5bf9\u9f50\u6548\u679c\uff0c\u540c\u65f6\u80fd\u8bca\u65ad\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u6536\u96c6\u4eba\u7c7b\u504f\u597d\u6570\u636e\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\uff0c\u5236\u7ea6\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5229\u7528GPT-4o\u4f5c\u4e3a\u4ee3\u7406\u8ba1\u7b97LLM\u751f\u6210\u54cd\u5e94\u7684\u5bf9\u9f50\u5206\u6570\uff0c\u57fa\u4e8e\u5747\u503c\u548c\u65b9\u5dee\u6784\u5efaAlignment Data Map\uff0c\u7b5b\u9009\u9ad8\u8d28\u91cf\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u4f7f\u752833%\u7684\u9ad8\u5747\u503c\u3001\u4f4e\u65b9\u5dee\u533a\u57df\u6570\u636e\uff0c\u6027\u80fd\u53ef\u5ab2\u7f8e\u6216\u8d85\u8fc7\u5168\u6570\u636e\u96c6\uff0c\u5e76\u80fd\u6709\u6548\u8bc6\u522b\u4f4e\u6548\u6216\u9519\u8bef\u6807\u6ce8\u6837\u672c\u3002", "conclusion": "Alignment Data Map\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u6536\u96c6\u6548\u7387\uff0c\u65e0\u9700\u663e\u5f0f\u6807\u6ce8\u5373\u53ef\u4f18\u5316LLM\u5bf9\u9f50\uff0c\u540c\u65f6\u5177\u5907\u6570\u636e\u96c6\u8bca\u65ad\u80fd\u529b\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u3001\u504f\u597d\u6570\u636e\u3001\u6570\u636e\u6548\u7387\u3001GPT-4o\u3001Alignment Data Map"}}
{"id": "2505.23027", "pdf": "https://arxiv.org/pdf/2505.23027", "abs": "https://arxiv.org/abs/2505.23027", "authors": ["Minh Nguyen Nhat To", "Paul F RWilson", "Viet Nguyen", "Mohamed Harmanani", "Michael Cooper", "Fahimeh Fooladgar", "Purang Abolmaesumi", "Parvin Mousavi", "Rahul G. Krishnan"], "title": "Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "ICML 2025 Paper", "summary": "The subpopulationtion shift, characterized by a disparity in subpopulation\ndistributibetween theween the training and target datasets, can significantly\ndegrade the performance of machine learning models. Current solutions to\nsubpopulation shift involve modifying empirical risk minimization with\nre-weighting strategies to improve generalization. This strategy relies on\nassumptions about the number and nature of subpopulations and annotations on\ngroup membership, which are unavailable for many real-world datasets. Instead,\nwe propose using an ensemble of diverse classifiers to adaptively capture risk\nassociated with subpopulations. Given a feature extractor network, we replace\nits standard linear classification layer with a mixture of prototypical\nclassifiers, where each member is trained to classify the data while focusing\non different features and samples from other members. In empirical evaluation\non nine real-world datasets, covering diverse domains and kinds of\nsubpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often\noutperforms the prior state-of-the-art in worst-group accuracy. The code is\navailable at https://github.com/minhto2802/dpe4subpop", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u591a\u6837\u5316\u539f\u578b\u96c6\u6210\uff08DPE\uff09\u201d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u66ff\u6362\u7ebf\u6027\u5206\u7c7b\u5c42\u4e3a\u6df7\u5408\u539f\u578b\u5206\u7c7b\u5668\u6765\u5e94\u5bf9\u5b50\u7fa4\u4f53\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u6700\u5dee\u7fa4\u4f53\u51c6\u786e\u7387\u3002", "motivation": "\u5b50\u7fa4\u4f53\u504f\u79fb\u4f7f\u5f97\u8bad\u7ec3\u96c6\u4e0e\u76ee\u6807\u6570\u636e\u96c6\u5206\u5e03\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5b50\u7fa4\u4f53\u6570\u91cf\u548c\u6807\u6ce8\u4fe1\u606f\u7684\u5047\u8bbe\u3002", "method": "\u4f7f\u7528\u591a\u6837\u5206\u7c7b\u5668\u96c6\u6210\uff0c\u5c06\u6807\u51c6\u7ebf\u6027\u5206\u7c7b\u5c42\u66ff\u6362\u4e3a\u6df7\u5408\u539f\u578b\u5206\u7c7b\u5668\uff0c\u6bcf\u4e2a\u5206\u7c7b\u5668\u5173\u6ce8\u4e0d\u540c\u7279\u5f81\u548c\u6837\u672c\u3002", "result": "\u57289\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDPE\u5728\u6700\u5dee\u7fa4\u4f53\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DPE\u901a\u8fc7\u81ea\u9002\u5e94\u6355\u6349\u5b50\u7fa4\u4f53\u98ce\u9669\uff0c\u65e0\u9700\u4f9d\u8d56\u6807\u6ce8\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "keywords": "\u5b50\u7fa4\u4f53\u504f\u79fb, \u591a\u6837\u5316\u539f\u578b\u96c6\u6210, \u6700\u5dee\u7fa4\u4f53\u51c6\u786e\u7387, \u673a\u5668\u5b66\u4e60\u6cdb\u5316"}}
{"id": "2505.23118", "pdf": "https://arxiv.org/pdf/2505.23118", "abs": "https://arxiv.org/abs/2505.23118", "authors": ["Linjie Mu", "Zhongzhen Huang", "Yakun Zhu", "Xiangyu Zhao", "Shaoting Zhang", "Xiaofan Zhang"], "title": "Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Effective clinical decision-making depends on iterative, multimodal reasoning\nacross diverse sources of evidence. The recent emergence of multimodal\nreasoning models has significantly transformed the landscape of solving complex\ntasks. Although such models have achieved notable success in mathematics and\nscience, their application to medical domains remains underexplored. In this\nwork, we propose \\textit{MedE$^2$}, a two-stage post-training pipeline that\nelicits and then enhances multimodal reasoning for medical domains. In Stage-I,\nwe fine-tune models using 2,000 text-only data samples containing precisely\norchestrated reasoning demonstrations to elicit reasoning behaviors. In\nStage-II, we further enhance the model's reasoning capabilities using 1,500\nrigorously curated multimodal medical cases, aligning model reasoning outputs\nwith our proposed multimodal medical reasoning preference. Extensive\nexperiments demonstrate the efficacy and reliability of \\textit{MedE$^2$} in\nimproving the reasoning performance of medical multimodal models. Notably,\nmodels trained with \\textit{MedE$^2$} consistently outperform baselines across\nmultiple medical multimodal benchmarks. Additional validation on larger models\nand under inference-time scaling further confirms the robustness and practical\nutility of our approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6d41\u7a0bMedE\u00b2\uff0c\u901a\u8fc7\u6587\u672c\u548c\u591a\u6a21\u6001\u6570\u636e\u589e\u5f3a\u533b\u7597\u9886\u57df\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u533b\u7597\u591a\u6a21\u6001\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u867d\u7136\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u5728\u6570\u5b66\u548c\u79d1\u5b66\u9886\u57df\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u4ecd\u4e0d\u8db3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86MedE\u00b2\uff0c\u4ee5\u589e\u5f3a\u533b\u7597\u9886\u57df\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "method": "MedE\u00b2\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u75282000\u4e2a\u7cbe\u786e\u7f16\u6392\u7684\u6587\u672c\u6837\u672c\u5fae\u8c03\u6a21\u578b\u4ee5\u6fc0\u53d1\u63a8\u7406\u884c\u4e3a\uff1b\u7b2c\u4e8c\u9636\u6bb5\u75281500\u4e2a\u591a\u6a21\u6001\u533b\u7597\u6848\u4f8b\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e0e\u591a\u6a21\u6001\u533b\u7597\u63a8\u7406\u504f\u597d\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMedE\u00b2\u663e\u8457\u63d0\u5347\u4e86\u533b\u7597\u591a\u6a21\u6001\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14\u5728\u5927\u6a21\u578b\u548c\u63a8\u7406\u6269\u5c55\u6761\u4ef6\u4e0b\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "MedE\u00b2\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6709\u6548\u63d0\u5347\u4e86\u533b\u7597\u9886\u57df\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u533b\u7597\u4efb\u52a1\u7684\u89e3\u51b3\u63d0\u4f9b\u4e86\u53ef\u9760\u65b9\u6cd5\u3002", "keywords": "\u591a\u6a21\u6001\u63a8\u7406,\u533b\u7597\u51b3\u7b56,\u540e\u8bad\u7ec3,MEDE\u00b2,\u4e24\u9636\u6bb5\u8bad\u7ec3"}}
{"id": "2505.22814", "pdf": "https://arxiv.org/pdf/2505.22814", "abs": "https://arxiv.org/abs/2505.22814", "authors": ["Jonghan Lim", "Ilya Kovalenko"], "title": "A Large Language Model-Enabled Control Architecture for Dynamic Resource Capability Exploration in Multi-Agent Manufacturing Systems", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Manufacturing environments are becoming more complex and unpredictable due to\nfactors such as demand variations and shorter product lifespans. This\ncomplexity requires real-time decision-making and adaptation to disruptions.\nTraditional control approaches highlight the need for advanced control\nstrategies capable of overcoming unforeseen challenges, as they demonstrate\nlimitations in responsiveness within dynamic industrial settings. Multi-agent\nsystems address these challenges through decentralization of decision-making,\nenabling systems to respond dynamically to operational changes. However,\ncurrent multi-agent systems encounter challenges related to real-time\nadaptation, context-aware decision-making, and the dynamic exploration of\nresource capabilities. Large language models provide the possibility to\novercome these limitations through context-aware decision-making capabilities.\nThis paper introduces a large language model-enabled control architecture for\nmulti-agent manufacturing systems to dynamically explore resource capabilities\nin response to real-time disruptions. A simulation-based case study\ndemonstrates that the proposed architecture improves system resilience and\nflexibility. The case study findings show improved throughput and efficient\nresource utilization compared to existing approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u4ee3\u7406\u5236\u9020\u7cfb\u7edf\u63a7\u5236\u67b6\u6784\uff0c\u4ee5\u52a8\u6001\u5e94\u5bf9\u5b9e\u65f6\u5e72\u6270\uff0c\u63d0\u5347\u7cfb\u7edf\u97e7\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u5236\u9020\u73af\u5883\u56e0\u9700\u6c42\u53d8\u5316\u548c\u4ea7\u54c1\u751f\u547d\u5468\u671f\u7f29\u77ed\u53d8\u5f97\u66f4\u52a0\u590d\u6742\u548c\u4e0d\u53ef\u9884\u6d4b\uff0c\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u5728\u52a8\u6001\u5de5\u4e1a\u73af\u5883\u4e2d\u54cd\u5e94\u80fd\u529b\u6709\u9650\uff0c\u591a\u4ee3\u7406\u7cfb\u7edf\u867d\u80fd\u5206\u6563\u51b3\u7b56\u4f46\u4ecd\u9762\u4e34\u5b9e\u65f6\u9002\u5e94\u548c\u60c5\u5883\u611f\u77e5\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8d4b\u80fd\u7684\u63a7\u5236\u67b6\u6784\uff0c\u901a\u8fc7\u60c5\u5883\u611f\u77e5\u51b3\u7b56\u80fd\u529b\u52a8\u6001\u63a2\u7d22\u8d44\u6e90\u80fd\u529b\uff0c\u5e94\u5bf9\u5b9e\u65f6\u5e72\u6270\u3002", "result": "\u4eff\u771f\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u8be5\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u97e7\u6027\u548c\u7075\u6d3b\u6027\uff0c\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u591a\u4ee3\u7406\u5236\u9020\u7cfb\u7edf\u7684\u5b9e\u65f6\u9002\u5e94\u548c\u52a8\u6001\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5236\u9020\u7cfb\u7edf, \u591a\u4ee3\u7406\u7cfb\u7edf, \u5927\u8bed\u8a00\u6a21\u578b, \u5b9e\u65f6\u9002\u5e94, \u8d44\u6e90\u5229\u7528\u7387"}}
{"id": "2505.23032", "pdf": "https://arxiv.org/pdf/2505.23032", "abs": "https://arxiv.org/abs/2505.23032", "authors": ["Dongwoo Lee", "Dong Bok Lee", "Steven Adriaensen", "Juho Lee", "Sung Ju Hwang", "Frank Hutter", "Seon Joo Kim", "Hae Beom Lee"], "title": "Bayesian Neural Scaling Laws Extrapolation with Prior-Fitted Networks", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Scaling has been a major driver of recent advancements in deep learning.\nNumerous empirical studies have found that scaling laws often follow the\npower-law and proposed several variants of power-law functions to predict the\nscaling behavior at larger scales. However, existing methods mostly rely on\npoint estimation and do not quantify uncertainty, which is crucial for\nreal-world applications involving decision-making problems such as determining\nthe expected performance improvements achievable by investing additional\ncomputational resources. In this work, we explore a Bayesian framework based on\nPrior-data Fitted Networks (PFNs) for neural scaling law extrapolation.\nSpecifically, we design a prior distribution that enables the sampling of\ninfinitely many synthetic functions resembling real-world neural scaling laws,\nallowing our PFN to meta-learn the extrapolation. We validate the effectiveness\nof our approach on real-world neural scaling laws, comparing it against both\nthe existing point estimation methods and Bayesian approaches. Our method\ndemonstrates superior performance, particularly in data-limited scenarios such\nas Bayesian active learning, underscoring its potential for reliable,\nuncertainty-aware extrapolation in practical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u6846\u67b6\u7684\u5148\u9a8c\u6570\u636e\u62df\u5408\u7f51\u7edc\uff08PFNs\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u7f29\u653e\u6cd5\u5219\u5916\u63a8\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bbe\u8ba1\u5148\u9a8c\u5206\u5e03\u751f\u6210\u7c7b\u4f3c\u771f\u5b9e\u7f29\u653e\u6cd5\u5219\u7684\u65e0\u9650\u5408\u6210\u51fd\u6570\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u6570\u636e\u6709\u9650\u573a\u666f\u4e0b\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u70b9\u4f30\u8ba1\uff0c\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u800c\u540e\u8005\u5728\u6d89\u53ca\u51b3\u7b56\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f8b\u5982\u8bc4\u4f30\u989d\u5916\u8ba1\u7b97\u8d44\u6e90\u6295\u5165\u7684\u6027\u80fd\u63d0\u5347\u9884\u671f\u3002", "method": "\u57fa\u4e8e\u5148\u9a8c\u6570\u636e\u62df\u5408\u7f51\u7edc\uff08PFNs\uff09\u7684\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5148\u9a8c\u5206\u5e03\u751f\u6210\u7c7b\u4f3c\u771f\u5b9e\u7f29\u653e\u6cd5\u5219\u7684\u5408\u6210\u51fd\u6570\uff0c\u5b9e\u73b0\u5143\u5b66\u4e60\u5916\u63a8\u3002", "result": "\u5728\u771f\u5b9e\u795e\u7ecf\u7f51\u7edc\u7f29\u653e\u6cd5\u5219\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u70b9\u4f30\u8ba1\u548c\u8d1d\u53f6\u65af\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u6570\u636e\u53d7\u9650\u7684\u8d1d\u53f6\u65af\u4e3b\u52a8\u5b66\u4e60\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u53ef\u9760\u4e14\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7f29\u653e\u6cd5\u5219\u5916\u63a8\uff0c\u6f5c\u5728\u5e94\u7528\u4e8e\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u7b49\u51b3\u7b56\u573a\u666f\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc\u7f29\u653e\u6cd5\u5219\u3001\u8d1d\u53f6\u65af\u6846\u67b6\u3001\u5148\u9a8c\u6570\u636e\u62df\u5408\u7f51\u7edc\u3001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u5143\u5b66\u4e60"}}
{"id": "2505.23121", "pdf": "https://arxiv.org/pdf/2505.23121", "abs": "https://arxiv.org/abs/2505.23121", "authors": ["Yiming Lei", "Zhizheng Yang", "Zeming Liu", "Haitao Leng", "Shaoguo Liu", "Tingting Gao", "Qingjie Liu", "Yunhong Wang"], "title": "ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 6 figures", "summary": "Multi-modal large language models have demonstrated remarkable zero-shot\nabilities and powerful image-understanding capabilities. However, the existing\nopen-source multi-modal models suffer from the weak capability of multi-turn\ninteraction, especially for long contexts. To address the issue, we first\nintroduce a context modeling module, termed ContextQFormer, which utilizes a\nmemory block to enhance the presentation of contextual information.\nFurthermore, to facilitate further research, we carefully build a new\nmulti-turn multi-modal dialogue dataset (TMDialog) for pre-training,\ninstruction-tuning, and evaluation, which will be open-sourced lately. Compared\nwith other multi-modal dialogue datasets, TMDialog contains longer\nconversations, which supports the research of multi-turn multi-modal dialogue.\nIn addition, ContextQFormer is compared with three baselines on TMDialog and\nexperimental results illustrate that ContextQFormer achieves an improvement of\n2%-4% in available rate over baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ContextQFormer\u6a21\u5757\u548c\u65b0\u6570\u636e\u96c6TMDialog\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u63d0\u53472%-4%\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u591a\u6a21\u6001\u6a21\u578b\u5728\u591a\u8f6e\u4ea4\u4e92\u3001\u7279\u522b\u662f\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u5f15\u5165ContextQFormer\u6a21\u5757\u7528\u5185\u5b58\u5757\u589e\u5f3a\u4e0a\u4e0b\u6587\u8868\u793a\uff0c\u5e76\u6784\u5efaTMDialog\u6570\u636e\u96c6\u652f\u6301\u9884\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "result": "ContextQFormer\u5728TMDialog\u4e0a\u6bd4\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u63d0\u53472%-4%\u3002", "conclusion": "ContextQFormer\u548cTMDialog\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u80fd\u529b\uff0c\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, \u591a\u8f6e\u5bf9\u8bdd, ContextQFormer, TMDialog"}}
{"id": "2505.22815", "pdf": "https://arxiv.org/pdf/2505.22815", "abs": "https://arxiv.org/abs/2505.22815", "authors": ["Zhangyi Hu", "Jiemin Wu", "Hua Xu", "Mingqian Liao", "Ninghui Feng", "Bo Gao", "Songning Lai", "Yutao Yue"], "title": "IMTS is Worth Time $\\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Irregular Multivariate Time Series (IMTS) forecasting is challenging due to\nthe unaligned nature of multi-channel signals and the prevalence of extensive\nmissing data. Existing methods struggle to capture reliable temporal patterns\nfrom such data due to significant missing values. While pre-trained foundation\nmodels show potential for addressing these challenges, they are typically\ndesigned for Regularly Sampled Time Series (RTS). Motivated by the visual Mask\nAutoEncoder's (MAE) powerful capability for modeling sparse multi-channel\ninformation and its success in RTS forecasting, we propose VIMTS, a framework\nadapting Visual MAE for IMTS forecasting. To mitigate the effect of missing\nvalues, VIMTS first processes IMTS along the timeline into feature patches at\nequal intervals. These patches are then complemented using learned\ncross-channel dependencies. Then it leverages visual MAE's capability in\nhandling sparse multichannel data for patch reconstruction, followed by a\ncoarse-to-fine technique to generate precise predictions from focused contexts.\nIn addition, we integrate self-supervised learning for improved IMTS modeling\nby adapting the visual MAE to IMTS data. Extensive experiments demonstrate\nVIMTS's superior performance and few-shot capability, advancing the application\nof visual foundation models in more general time series tasks. Our code is\navailable at https://github.com/WHU-HZY/VIMTS.", "AI": {"tldr": "VIMTS\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9MAE\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u4e0d\u89c4\u5219\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u901a\u8fc7\u7279\u5f81\u8865\u5168\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u591a\u901a\u9053\u4fe1\u53f7\u672a\u5bf9\u9f50\u548c\u5927\u91cf\u7f3a\u5931\u6570\u636e\uff0c\u73b0\u6709\u7684\u4e0d\u89c4\u5219\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u96be\u4ee5\u6355\u83b7\u53ef\u9760\u7684\u65f6\u5e8f\u6a21\u5f0f\uff0c\u800c\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u4ec5\u9002\u7528\u4e8e\u89c4\u5219\u91c7\u6837\u65f6\u5e8f\u6570\u636e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002", "method": "VIMTS\u5c06\u4e0d\u89c4\u5219\u65f6\u95f4\u5e8f\u5217\u5206\u5272\u4e3a\u7b49\u95f4\u9694\u7279\u5f81\u5757\uff0c\u5229\u7528\u5b66\u4e60\u5230\u7684\u8de8\u901a\u9053\u4f9d\u8d56\u8865\u5168\u7f3a\u5931\u6570\u636e\uff0c\u5e76\u501f\u52a9\u89c6\u89c9MAE\u5904\u7406\u7a00\u758f\u591a\u901a\u9053\u6570\u636e\u7684\u80fd\u529b\u8fdb\u884c\u5757\u91cd\u6784\uff0c\u518d\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u6280\u672f\u751f\u6210\u9884\u6d4b\u3002\u540c\u65f6\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u4f18\u5316\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVIMTS\u5728\u6027\u80fd\u548c\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u52a8\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u66f4\u5e7f\u6cdb\u65f6\u5e8f\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "VIMTS\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9MAE\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u89c4\u5219\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "keywords": "\u4e0d\u89c4\u5219\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u3001\u89c6\u89c9MAE\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u65f6\u5e8f\u9884\u6d4b\u3001\u7f3a\u5931\u6570\u636e\u8865\u5168"}}
{"id": "2505.23042", "pdf": "https://arxiv.org/pdf/2505.23042", "abs": "https://arxiv.org/abs/2505.23042", "authors": ["Siwen Wang", "Shitou Zhang", "Wan-Lin Chen", "Dung Truong", "Tzyy-Ping Jung"], "title": "From Theory to Application: Fine-Tuning Large EEG Model with Real-World Stress Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Language Models have inspired the development of\nfoundation models across various domains. In this study, we evaluate the\nefficacy of Large EEG Models (LEMs) by fine-tuning LaBraM, a state-of-the-art\nfoundation EEG model, on a real-world stress classification dataset collected\nin a graduate classroom. Unlike previous studies that primarily evaluate LEMs\nusing data from controlled clinical settings, our work assesses their\napplicability to real-world environments. We train a binary classifier that\ndistinguishes between normal and elevated stress states using resting-state EEG\ndata recorded from 18 graduate students during a class session. The\nbest-performing fine-tuned model achieves a balanced accuracy of 90.47% with a\n5-second window, significantly outperforming traditional stress classifiers in\nboth accuracy and inference efficiency. We further evaluate the robustness of\nthe fine-tuned LEM under random data shuffling and reduced channel counts.\nThese results demonstrate the capability of LEMs to effectively process\nreal-world EEG data and highlight their potential to revolutionize\nbrain-computer interface applications by shifting the focus from model-centric\nto data-centric design.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u4f7f\u7528LaBraM\u6a21\u578b\u5fae\u8c03\u8fdb\u884c\u538b\u529b\u5206\u7c7b\uff0c\u8bc1\u660e\u4e86\u5927\u578bEEG\u6a21\u578b\uff08LEMs\uff09\u7684\u5b9e\u7528\u6027\uff0c\u5176\u5206\u7c7b\u51c6\u786e\u7387\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5927\u578bEEG\u6a21\u578b\uff08LEMs\uff09\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u800c\u975e\u4ec5\u5c40\u9650\u4e8e\u4e34\u5e8a\u63a7\u5236\u73af\u5883\u3002", "method": "\u4f7f\u7528LaBraM\u6a21\u578b\u572818\u540d\u7814\u7a76\u751f\u7684\u9759\u606f\u6001EEG\u6570\u636e\u4e0a\u5fae\u8c03\uff0c\u8bad\u7ec3\u4e00\u4e2a\u4e8c\u5206\u7c7b\u5668\u533a\u5206\u6b63\u5e38\u548c\u5347\u9ad8\u538b\u529b\u72b6\u6001\u3002", "result": "\u6700\u4f73\u6a21\u578b\u7684\u5e73\u8861\u51c6\u786e\u7387\u8fbe\u523090.47%\uff085\u79d2\u7a97\u53e3\uff09\uff0c\u5728\u51c6\u786e\u6027\u548c\u63a8\u7406\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u5177\u6709\u6570\u636e\u968f\u673a\u6df7\u6d17\u548c\u901a\u9053\u51cf\u5c11\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "LEMs\u80fd\u591f\u9ad8\u6548\u5904\u7406\u771f\u5b9eEEG\u6570\u636e\uff0c\u4e3a\u8111\u673a\u63a5\u53e3\u5e94\u7528\u4ece\u6a21\u578b\u4e2d\u5fc3\u8f6c\u5411\u6570\u636e\u4e2d\u5fc3\u7684\u9769\u65b0\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "keywords": "\u5927\u578bEEG\u6a21\u578b,LaBraM,\u538b\u529b\u5206\u7c7b,\u771f\u5b9e\u4e16\u754cEEG,\u8111\u673a\u63a5\u53e3"}}
{"id": "2505.23126", "pdf": "https://arxiv.org/pdf/2505.23126", "abs": "https://arxiv.org/abs/2505.23126", "authors": ["Atharva Naik", "Darsh Agrawal", "Manav Kapadnis", "Yuwei An", "Yash Mathur", "Carolyn Rose", "David Mortensen"], "title": "PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark inspired by Historical Linguistics", "categories": ["cs.CL"], "comment": null, "summary": "Recently, long chain of thought (LCoT), Large Language Models (LLMs), have\ntaken the machine learning world by storm with their breathtaking reasoning\ncapabilities. However, are the abstract reasoning abilities of these models\ngeneral enough for problems of practical importance? Unlike past work, which\nhas focused mainly on math, coding, and data wrangling, we focus on a\nhistorical linguistics-inspired inductive reasoning problem, formulated as\nProgramming by Examples. We develop a fully automated pipeline for dynamically\ngenerating a benchmark for this task with controllable difficulty in order to\ntackle scalability and contamination issues to which many reasoning benchmarks\nare subject. Using our pipeline, we generate a test set with nearly 1k\ninstances that is challenging for all state-of-the-art reasoning LLMs, with the\nbest model (Claude-3.7-Sonnet) achieving a mere 54% pass rate, demonstrating\nthat LCoT LLMs still struggle with a class or reasoning that is ubiquitous in\nhistorical linguistics as well as many other domains.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u957f\u94fe\u601d\u7ef4\uff08LCoT\uff09\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5386\u53f2\u8bed\u8a00\u5b66\u542f\u53d1\u7684\u5f52\u7eb3\u63a8\u7406\u95ee\u9898\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u80fd\u529b\u6709\u9650\uff0c\u6700\u4f73\u6a21\u578b\u4ec5\u8fbe\u523054%\u901a\u8fc7\u7387\u3002", "motivation": "\u9a8c\u8bc1LLMs\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u4e4b\u5916\u7684\u5b9e\u7528\u9886\u57df\uff08\u5982\u5386\u53f2\u8bed\u8a00\u5b66\uff09\u7684\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u662f\u5426\u666e\u9002\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u52a8\u6001\u751f\u6210\u53ef\u63a7\u96be\u5ea6\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08\u542b\u8fd11k\u5b9e\u4f8b\uff09\uff0c\u89e3\u51b3\u73b0\u6709\u63a8\u7406\u57fa\u51c6\u7684\u6269\u5c55\u6027\u548c\u6570\u636e\u6c61\u67d3\u95ee\u9898\u3002", "result": "\u5f53\u524d\u6700\u4f18\u6a21\u578b\uff08Claude-3.7-Sonnet\uff09\u901a\u8fc7\u7387\u4ec554%\uff0c\u8868\u660eLLMs\u5728\u5386\u53f2\u8bed\u8a00\u5b66\u7b49\u9886\u57df\u7684\u63a8\u7406\u4ecd\u5b58\u5728\u663e\u8457\u6311\u6218\u3002", "conclusion": "LCoT LLMs\u5728\u5386\u53f2\u8bed\u8a00\u5b66\u7c7b\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002", "keywords": "\u957f\u94fe\u601d\u7ef4, \u5927\u8bed\u8a00\u6a21\u578b, \u5386\u53f2\u8bed\u8a00\u5b66, \u5f52\u7eb3\u63a8\u7406, \u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2505.22818", "pdf": "https://arxiv.org/pdf/2505.22818", "abs": "https://arxiv.org/abs/2505.22818", "authors": ["Linghan Zhong", "Samuel Yuan", "Jiyang Zhang", "Yu Liu", "Pengyu Nie", "Junyi Jessy Li", "Milos Gligoric"], "title": "A Tool for Generating Exceptional Behavior Tests With Large Language Models", "categories": ["cs.SE", "cs.AI"], "comment": "FSE 2025 Demo (Camera Ready)", "summary": "Exceptional behavior tests (EBTs) are crucial in software development for\nverifying that code correctly handles unwanted events and throws appropriate\nexceptions. However, prior research has shown that developers often prioritize\ntesting \"happy paths\", e.g., paths without unwanted events over exceptional\nscenarios. We present exLong, a framework that automatically generates EBTs to\naddress this gap. exLong leverages a large language model (LLM) fine-tuned from\nCodeLlama and incorporates reasoning about exception-throwing traces,\nconditional expressions that guard throw statements, and non-exceptional\nbehavior tests that execute similar traces. Our demonstration video illustrates\nhow exLong can effectively assist developers in creating comprehensive EBTs for\ntheir project (available at https://youtu.be/Jro8kMgplZk).", "AI": {"tldr": "exLong\u662f\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u5f02\u5e38\u884c\u4e3a\u6d4b\u8bd5\uff08EBTs\uff09\u7684\u6846\u67b6\uff0c\u57fa\u4e8e\u5fae\u8c03\u540e\u7684CodeLlama\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5f00\u53d1\u8005\u4f18\u5148\u6d4b\u8bd5\u201c\u6b63\u5e38\u8def\u5f84\u201d\u800c\u5ffd\u7565\u5f02\u5e38\u573a\u666f\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u5f00\u53d1\u8005\u901a\u5e38\u4f18\u5148\u6d4b\u8bd5\u201c\u6b63\u5e38\u8def\u5f84\u201d\uff0c\u5bfc\u81f4\u5f02\u5e38\u573a\u666f\u6d4b\u8bd5\u4e0d\u8db3\uff0cexLong\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u751f\u6210EBTs\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "exLong\u5229\u7528\u5fae\u8c03\u7684CodeLlama\u6a21\u578b\uff0c\u5206\u6790\u5f02\u5e38\u629b\u51fa\u8f68\u8ff9\u3001\u4fdd\u62a4\u629b\u51fa\u8bed\u53e5\u7684\u6761\u4ef6\u8868\u8fbe\u5f0f\uff0c\u4ee5\u53ca\u6267\u884c\u7c7b\u4f3c\u8f68\u8ff9\u7684\u975e\u5f02\u5e38\u884c\u4e3a\u6d4b\u8bd5\u3002", "result": "exLong\u80fd\u6709\u6548\u8f85\u52a9\u5f00\u53d1\u8005\u751f\u6210\u5168\u9762\u7684EBTs\uff0c\u6f14\u793a\u89c6\u9891\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "conclusion": "exLong\u901a\u8fc7\u81ea\u52a8\u5316\u751f\u6210EBTs\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u573a\u666f\u6d4b\u8bd5\u7684\u8986\u76d6\u7387\u548c\u6548\u7387\u3002", "keywords": "\u5f02\u5e38\u884c\u4e3a\u6d4b\u8bd5\u3001CodeLlama\u3001\u81ea\u52a8\u5316\u6d4b\u8bd5\u3001\u8f6f\u4ef6\u5f00\u53d1"}}
{"id": "2505.23048", "pdf": "https://arxiv.org/pdf/2505.23048", "abs": "https://arxiv.org/abs/2505.23048", "authors": ["Tianci Bu", "Le Zhou", "Wenchuan Yang", "Jianhong Mou", "Kang Yang", "Suoyi Tan", "Feng Yao", "Jingyuan Wang", "Xin Lu"], "title": "ProDiff: Prototype-Guided Diffusion for Minimal Information Trajectory Imputation", "categories": ["cs.LG"], "comment": null, "summary": "Trajectory data is crucial for various applications but often suffers from\nincompleteness due to device limitations and diverse collection scenarios.\nExisting imputation methods rely on sparse trajectory or travel information,\nsuch as velocity, to infer missing points. However, these approaches assume\nthat sparse trajectories retain essential behavioral patterns, which place\nsignificant demands on data acquisition and overlook the potential of\nlarge-scale human trajectory embeddings. To address this, we propose ProDiff, a\ntrajectory imputation framework that uses only two endpoints as minimal\ninformation. It integrates prototype learning to embed human movement patterns\nand a denoising diffusion probabilistic model for robust spatiotemporal\nreconstruction. Joint training with a tailored loss function ensures effective\nimputation. ProDiff outperforms state-of-the-art methods, improving accuracy by\n6.28\\% on FourSquare and 2.52\\% on WuXi. Further analysis shows a 0.927\ncorrelation between generated and real trajectories, demonstrating the\neffectiveness of our approach.", "AI": {"tldr": "ProDiff is a trajectory imputation framework that uses only two endpoints and integrates prototype learning with a denoising diffusion model for robust reconstruction, outperforming existing methods.", "motivation": "Existing methods rely on sparse trajectory data but may not retain essential patterns, while ProDiff leverages minimal endpoints and large-scale embeddings for better accuracy.", "method": "ProDiff uses prototype learning to embed human movement patterns and a denoising diffusion model for spatiotemporal reconstruction, trained with a tailored loss function.", "result": "ProDiff improves accuracy by 6.28% on FourSquare and 2.52% on WuXi, with a 0.927 correlation between generated and real trajectories.", "conclusion": "ProDiff demonstrates superior performance in trajectory imputation by effectively leveraging minimal input and advanced modeling techniques.", "keywords": "trajectory imputation, prototype learning, denoising diffusion model, spatiotemporal reconstruction, human movement patterns"}}
{"id": "2505.23140", "pdf": "https://arxiv.org/pdf/2505.23140", "abs": "https://arxiv.org/abs/2505.23140", "authors": ["Qiuyu Ding", "Zhiqiang Cao", "Hailong Cao", "Tiejun Zhao"], "title": "Enhancing Large Language Models'Machine Translation via Dynamic Focus Anchoring", "categories": ["cs.CL"], "comment": null, "summary": "Large language models have demonstrated exceptional performance across\nmultiple crosslingual NLP tasks, including machine translation (MT). However,\npersistent challenges remain in addressing context-sensitive units (CSUs), such\nas polysemous words. These CSUs not only affect the local translation accuracy\nof LLMs, but also affect LLMs' understanding capability for sentences and\ntasks, and even lead to translation failure. To address this problem, we\npropose a simple but effective method to enhance LLMs' MT capabilities by\nacquiring CSUs and applying semantic focus. Specifically, we dynamically\nanalyze and identify translation challenges, then incorporate them into LLMs in\na structured manner to mitigate mistranslations or misunderstandings of CSUs\ncaused by information flattening. Efficiently activate LLMs to identify and\napply relevant knowledge from its vast data pool in this way, ensuring more\naccurate translations for translating difficult terms. On a benchmark dataset\nof MT, our proposed method achieved competitive performance compared to\nmultiple existing open-sourced MT baseline models. It demonstrates\neffectiveness and robustness across multiple language pairs, including both\nsimilar language pairs and distant language pairs. Notably, the proposed method\nrequires no additional model training and enhances LLMs' performance across\nmultiple NLP tasks with minimal resource consumption.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u83b7\u53d6\u4e0a\u4e0b\u6587\u654f\u611f\u5355\u5143(CSU)\u548c\u5e94\u7528\u8bed\u4e49\u7126\u70b9\u6765\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u673a\u5668\u7ffb\u8bd1\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5728\u591a\u8bed\u8a00\u5bf9\u4e0a\u5b9e\u73b0\u7ade\u4e89\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4e0a\u4e0b\u6587\u654f\u611f\u5355\u5143(CSU)\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u5bfc\u81f4\u7684\u8bef\u8bd1\u548c\u8bef\u89e3\u95ee\u9898\uff0c\u63d0\u5347LLM\u7684\u7ffb\u8bd1\u51c6\u786e\u6027\u548c\u7406\u89e3\u80fd\u529b\u3002", "method": "\u52a8\u6001\u5206\u6790\u5e76\u8bc6\u522b\u7ffb\u8bd1\u96be\u70b9\uff0c\u4ee5\u7ed3\u6784\u5316\u65b9\u5f0f\u5c06CSU\u878d\u5165LLM\uff0c\u901a\u8fc7\u8bed\u4e49\u7126\u70b9\u6fc0\u6d3b\u6a21\u578b\u76f8\u5173\u77e5\u8bc6\u5e93\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u591a\u4e2a\u5f00\u6e90\u57fa\u7ebf\u6a21\u578b\uff0c\u652f\u6301\u76f8\u4f3c\u548c\u8fdc\u8ddd\u79bb\u8bed\u8a00\u5bf9\u7684\u7ffb\u8bd1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u8d44\u6e90\u6d88\u8017\u4f4e\uff0c\u80fd\u6709\u6548\u63d0\u5347LLM\u5728\u591a\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u673a\u5668\u7ffb\u8bd1, \u4e0a\u4e0b\u6587\u654f\u611f\u5355\u5143, \u8bed\u4e49\u7126\u70b9, \u8de8\u8bed\u8a00\u4efb\u52a1"}}
{"id": "2505.23049", "pdf": "https://arxiv.org/pdf/2505.23049", "abs": "https://arxiv.org/abs/2505.23049", "authors": ["Tianteng Gu", "Bei Liu", "Bo Xiao", "Ke Zeng", "Jiacheng Liu", "Yanmin Qian"], "title": "DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Pruning is a widely used technique to compress large language models (LLMs)\nby removing unimportant weights, but it often suffers from significant\nperformance degradation - especially under semi-structured sparsity\nconstraints. Existing pruning methods primarily focus on estimating the\nimportance of individual weights, which limits their ability to preserve\ncritical capabilities of the model. In this work, we propose a new perspective:\nrather than merely selecting which weights to prune, we first redistribute\nparameter importance to make the model inherently more amenable to pruning. By\nminimizing the information entropy of normalized importance scores, our\napproach concentrates importance onto a smaller subset of weights, thereby\nenhancing pruning robustness. We instantiate this idea through DenoiseRotator,\nwhich applies learnable orthogonal transformations to the model's weight\nmatrices. Our method is model-agnostic and can be seamlessly integrated with\nexisting pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated\non LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4\nsemi-structured sparsity, DenoiseRotator consistently improves perplexity and\nzero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4\nsemi-structured sparsity, DenoiseRotator reduces the perplexity gap to the\ndense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are\navailable at https://github.com/Axel-gu/DenoiseRotator.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDenoiseRotator\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u5206\u914d\u53c2\u6570\u91cd\u8981\u6027\u6765\u589e\u5f3aLLMs\u7684\u526a\u679d\u9c81\u68d2\u6027\uff0c\u663e\u8457\u51cf\u5c11\u6027\u80fd\u635f\u5931\u3002", "motivation": "\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u6743\u91cd\u7684\u91cd\u8981\u6027\u8bc4\u4f30\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5173\u952e\u80fd\u529b\u7684\u4fdd\u7559\uff0c\u5c24\u5176\u662f\u5728\u534a\u7ed3\u6784\u5316\u7a00\u758f\u7ea6\u675f\u4e0b\u6027\u80fd\u4e0b\u964d\u663e\u8457\u3002", "method": "\u901a\u8fc7\u6700\u5c0f\u5316\u5f52\u4e00\u5316\u91cd\u8981\u6027\u5206\u6570\u7684\u4fe1\u606f\u71b5\uff0c\u5c06\u91cd\u8981\u6027\u96c6\u4e2d\u5230\u66f4\u5c0f\u7684\u6743\u91cd\u5b50\u96c6\uff0c\u5e76\u7ed3\u5408\u53ef\u5b66\u4e60\u6b63\u4ea4\u53d8\u6362\uff08DenoiseRotator\uff09\u5b9e\u73b0\u3002", "result": "\u5728LLaMA3\u3001Qwen2.5\u548cMistral\u6a21\u578b\u4e0a\uff0c50%\u975e\u7ed3\u6784\u5316\u548c2:4\u534a\u7ed3\u6784\u5316\u7a00\u758f\u4e0b\uff0cDenoiseRotator\u663e\u8457\u964d\u4f4e\u4e86\u56f0\u60d1\u5ea6\u5dee\u8ddd\uff08\u5982LLaMA3-70B\u51cf\u5c1158%\uff09\u3002", "conclusion": "DenoiseRotator\u662f\u6a21\u578b\u65e0\u5173\u7684\uff0c\u53ef\u4e0e\u73b0\u6709\u526a\u679d\u6280\u672f\u65e0\u7f1d\u96c6\u6210\uff0c\u6709\u6548\u63d0\u5347\u526a\u679d\u540e\u7684\u6a21\u578b\u6027\u80fd\u3002", "keywords": "\u526a\u679d\u3001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3001DenoiseRotator\u3001\u7a00\u758f\u6027\u3001\u4fe1\u606f\u71b5"}}
{"id": "2505.23146", "pdf": "https://arxiv.org/pdf/2505.23146", "abs": "https://arxiv.org/abs/2505.23146", "authors": ["Qiuyu Ding", "Zhiqiang Cao", "Hailong Cao", "Tiejun Zhao"], "title": "Cross-Domain Bilingual Lexicon Induction via Pretrained Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Bilingual Lexicon Induction (BLI) is generally based on common domain data to\nobtain monolingual word embedding, and by aligning the monolingual word\nembeddings to obtain the cross-lingual embeddings which are used to get the\nword translation pairs. In this paper, we propose a new task of BLI, which is\nto use the monolingual corpus of the general domain and target domain to\nextract domain-specific bilingual dictionaries. Motivated by the ability of\nPre-trained models, we propose a method to get better word embeddings that\nbuild on the recent work on BLI. This way, we introduce the Code Switch(Qin et\nal., 2020) firstly in the cross-domain BLI task, which can match differit is\nyet to be seen whether these methods are suitable for bilingual lexicon\nextraction in professional fields. As we can see in table 1, the classic and\nefficient BLI approach, Muse and Vecmap, perform much worse on the Medical\ndataset than on the Wiki dataset. On one hand, the specialized domain data set\nis relatively smaller compared to the generic domain data set generally, and\nspecialized words have a lower frequency, which will directly affect the\ntranslation quality of bilingual dictionaries. On the other hand, static word\nembeddings are widely used for BLI, however, in some specific fields, the\nmeaning of words is greatly influenced by context, in this case, using only\nstatic word embeddings may lead to greater bias. ent strategies in different\ncontexts, making the model more suitable for this task. Experimental results\nshow that our method can improve performances over robust BLI baselines on\nthree specific domains by averagely improving 0.78 points.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u6539\u8fdb\u53cc\u8bed\u8bcd\u5178\u6784\u5efa\u7684\u65b9\u6cd5\uff0c\u5728\u4e13\u4e1a\u9886\u57df\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u597d\u7684\u6548\u679c\uff0c\u5e73\u5747\u63d0\u5347\u4e860.78\u5206\u3002", "motivation": "\u4f20\u7edf\u53cc\u8bed\u8bcd\u5178\u6784\u5efa\u65b9\u6cd5\uff08BLI\uff09\u5728\u901a\u7528\u9886\u57df\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4e13\u4e1a\u9886\u57df\uff08\u5982\u533b\u5b66\uff09\u56e0\u6570\u636e\u91cf\u5c0f\u548c\u8bcd\u9891\u4f4e\u800c\u6548\u679c\u4e0d\u4f73\u3002\u9884\u8bad\u7ec3\u6a21\u578b\u80fd\u66f4\u597d\u6355\u6349\u4e0a\u4e0b\u6587\uff0c\u53ef\u80fd\u66f4\u9002\u5408\u4e13\u4e1a\u9886\u57df\u8bcd\u5178\u6784\u5efa\u3002", "method": "\u7ed3\u5408\u901a\u7528\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u5355\u8bed\u8bed\u6599\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u4f18\u5316\u8bcd\u5d4c\u5165\uff0c\u9996\u6b21\u5728\u8de8\u9886\u57dfBLI\u4efb\u52a1\u4e2d\u5f15\u5165Code Switch\u7b56\u7565\uff0c\u4ee5\u9002\u914d\u4e0d\u540c\u4e0a\u4e0b\u6587\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u4e13\u4e1a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u53470.78\u5206\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff08\u5982Muse\u548cVecmap\uff09\u3002", "conclusion": "\u9884\u8bad\u7ec3\u6a21\u578b\u80fd\u6709\u6548\u63d0\u5347\u4e13\u4e1a\u9886\u57df\u53cc\u8bed\u8bcd\u5178\u6784\u5efa\u6548\u679c\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u7f3a\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u9886\u57df\u3002", "keywords": "\u53cc\u8bed\u8bcd\u5178\u6784\u5efa\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u4e13\u4e1a\u9886\u57df\u3001\u8bcd\u5d4c\u5165\u3001Code Switch"}}
{"id": "2505.23055", "pdf": "https://arxiv.org/pdf/2505.23055", "abs": "https://arxiv.org/abs/2505.23055", "authors": ["Zhen Xiang", "Aliyah R. Hsu", "Austin V. Zane", "Aaron E. Kornblith", "Margaret J. Lin-Martore", "Jasmanpreet C. Kaur", "Vasuda M. Dokiparthi", "Bo Li", "Bin Yu"], "title": "CDR-Agent: Intelligent Selection and Execution of Clinical Decision Rules Using Large Language Model Agents", "categories": ["cs.LG"], "comment": null, "summary": "Clinical decision-making is inherently complex and fast-paced, particularly\nin emergency departments (EDs) where critical, rapid and high-stakes decisions\nare made. Clinical Decision Rules (CDRs) are standardized evidence-based tools\nthat combine signs, symptoms, and clinical variables into decision trees to\nmake consistent and accurate diagnoses. CDR usage is often hindered by the\nclinician's cognitive load, limiting their ability to quickly recall and apply\nthe appropriate rules. We introduce CDR-Agent, a novel LLM-based system\ndesigned to enhance ED decision-making by autonomously identifying and applying\nthe most appropriate CDRs based on unstructured clinical notes. To validate\nCDR-Agent, we curated two novel ED datasets: synthetic and CDR-Bench, although\nCDR-Agent is applicable to non ED clinics. CDR-Agent achieves a 56.3\\%\n(synthetic) and 8.7\\% (CDR-Bench) accuracy gain relative to the standalone LLM\nbaseline in CDR selection. Moreover, CDR-Agent significantly reduces\ncomputational overhead. Using these datasets, we demonstrated that CDR-Agent\nnot only selects relevant CDRs efficiently, but makes cautious yet effective\nimaging decisions by minimizing unnecessary interventions while successfully\nidentifying most positively diagnosed cases, outperforming traditional LLM\nprompting approaches. Code for our work can be found at:\nhttps://github.com/zhenxianglance/medagent-cdr-agent", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u7cfb\u7edfCDR-Agent\uff0c\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u8bc6\u522b\u548c\u5e94\u7528\u6700\u5408\u9002\u7684\u4e34\u5e8a\u51b3\u7b56\u89c4\u5219\uff08CDRs\uff09\u6765\u63d0\u5347\u6025\u8bca\u79d1\u7684\u51b3\u7b56\u6548\u7387\u3002\u9a8c\u8bc1\u7ed3\u679c\u663e\u793a\uff0cCDR-Agent\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u6025\u8bca\u79d1\u7684\u4e34\u5e8a\u51b3\u7b56\u590d\u6742\u4e14\u5feb\u901f\uff0c\u4f20\u7edfCDRs\u7684\u4f7f\u7528\u5e38\u56e0\u533b\u751f\u7684\u8ba4\u77e5\u8d1f\u8377\u53d7\u9650\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7AI\u7cfb\u7edf\u7b80\u5316CDRs\u7684\u8bc6\u522b\u548c\u5e94\u7528\uff0c\u63d0\u5347\u51b3\u7b56\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86LLM-based\u7cfb\u7edfCDR-Agent\uff0c\u5229\u7528\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u7b14\u8bb0\u81ea\u52a8\u9009\u62e9\u548c\u5e94\u7528CDRs\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u548cCDR-Bench\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "CDR-Agent\u5728CDR\u9009\u62e9\u4e0a\u7684\u51c6\u786e\u7387\u5206\u522b\u63d0\u5347\u4e8656.3%\uff08\u5408\u6210\u6570\u636e\uff09\u548c8.7%\uff08CDR-Bench\uff09\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "CDR-Agent\u5728\u6025\u8bca\u79d1\u51b3\u7b56\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u9ad8\u6548\u9009\u62e9CDRs\u5e76\u51cf\u5c11\u4e0d\u5fc5\u8981\u5e72\u9884\uff0c\u4f18\u4e8e\u4f20\u7edfLLM\u65b9\u6cd5\u3002", "keywords": "\u4e34\u5e8a\u51b3\u7b56\u89c4\u5219, LLM, \u6025\u8bca\u79d1, \u4eba\u5de5\u667a\u80fd, \u533b\u7597\u51b3\u7b56"}}
{"id": "2505.23166", "pdf": "https://arxiv.org/pdf/2505.23166", "abs": "https://arxiv.org/abs/2505.23166", "authors": ["Li Lucy", "Camilla Griffiths", "Sarah Levine", "Jennifer L. Eberhardt", "Dorottya Demszky", "David Bamman"], "title": "Tell, Don't Show: Leveraging Language Models' Abstractive Retellings to Model Literary Themes", "categories": ["cs.CL"], "comment": "26 pages, 7 figures, Findings of ACL 2025", "summary": "Conventional bag-of-words approaches for topic modeling, like latent\nDirichlet allocation (LDA), struggle with literary text. Literature challenges\nlexical methods because narrative language focuses on immersive sensory details\ninstead of abstractive description or exposition: writers are advised to \"show,\ndon't tell.\" We propose Retell, a simple, accessible topic modeling approach\nfor literature. Here, we prompt resource-efficient, generative language models\n(LMs) to tell what passages show, thereby translating narratives' surface forms\ninto higher-level concepts and themes. By running LDA on LMs' retellings of\npassages, we can obtain more precise and informative topics than by running LDA\nalone or by directly asking LMs to list topics. To investigate the potential of\nour method for cultural analytics, we compare our method's outputs to\nexpert-guided annotations in a case study on racial/cultural identity in high\nschool English language arts books.", "AI": {"tldr": "Retell\u65b9\u6cd5\u5229\u7528\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u5c06\u6587\u5b66\u53d9\u4e8b\u8f6c\u6362\u4e3a\u9ad8\u5c42\u6b21\u6982\u5ff5\uff0c\u518d\u901a\u8fc7LDA\u4e3b\u9898\u5efa\u6a21\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u7cbe\u51c6\uff0c\u5e94\u7528\u4e8e\u6587\u5316\u5206\u6790\u6848\u4f8b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u8bcd\u888b\u65b9\u6cd5\uff08\u5982LDA\uff09\u5728\u5904\u7406\u6587\u5b66\u6587\u672c\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u5176\u6ce8\u91cd\u611f\u5b98\u7ec6\u8282\u800c\u975e\u62bd\u8c61\u63cf\u8ff0\u3002\u9700\u4e00\u79cd\u80fd\u6355\u6349\u53d9\u4e8b\u6df1\u5c42\u4e3b\u9898\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u5bf9\u6bb5\u843d\u8fdb\u884c\u2018\u91cd\u8ff0\u2019\uff0c\u8f6c\u5316\u4e3a\u6982\u5ff5\u6027\u63cf\u8ff0\uff0c\u518d\u5e94\u7528LDA\u8fdb\u884c\u4e3b\u9898\u5efa\u6a21\u3002", "result": "\u76f8\u6bd4\u76f4\u63a5LDA\u6216\u6a21\u578b\u751f\u6210\u4e3b\u9898\uff0cRetell\u65b9\u6cd5\u751f\u6210\u7684\u4e3b\u9898\u66f4\u7cbe\u786e\u4e14\u4fe1\u606f\u4e30\u5bcc\uff0c\u5728\u6587\u5316\u8eab\u4efd\u6848\u4f8b\u5206\u6790\u4e2d\u4e0e\u4e13\u5bb6\u6807\u6ce8\u4e00\u81f4\u3002", "conclusion": "Retell\u4e3a\u6587\u5b66\u4e3b\u9898\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u6587\u5316\u5206\u6790\u4e2d\u6f5c\u529b\u663e\u8457\u3002", "keywords": "\u4e3b\u9898\u5efa\u6a21, \u6587\u5b66\u5206\u6790, \u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b, \u6587\u5316\u5206\u6790, LDA"}}
{"id": "2505.23061", "pdf": "https://arxiv.org/pdf/2505.23061", "abs": "https://arxiv.org/abs/2505.23061", "authors": ["Tarun Suresh", "Debangshu Banerjee", "Shubham Ugare", "Sasa Misailovic", "Gagandeep Singh"], "title": "DINGO: Constrained Inference for Diffusion LLMs", "categories": ["cs.LG", "cs.PL", "cs.SE"], "comment": "DINGO an algorithm to provably apply constraints to diffusion LLM\n  generations", "summary": "Diffusion LLMs have emerged as a promising alternative to conventional\nautoregressive LLMs, offering significant potential for improved runtime\nefficiency. However, existing diffusion models lack the ability to provably\nenforce user-specified formal constraints, such as regular expressions, which\nmakes them unreliable for tasks that require structured outputs, such as\nfixed-schema JSON generation. Unlike autoregressive models that generate tokens\nsequentially, diffusion LLMs predict a block of tokens in parallel. This\nparallelism makes traditional constrained decoding algorithms, which are\ndesigned for sequential token prediction, ineffective at preserving the true\noutput distribution. To address this limitation, we propose DINGO, a dynamic\nprogramming-based constrained decoding strategy that is both efficient and\nprovably distribution-preserving. DINGO enables sampling of output strings with\nthe highest probability under the model's predicted distribution, while\nstrictly satisfying any user-specified regular expression. On standard symbolic\nmath and JSON generation benchmarks, DINGO achieves up to a 68 percentage point\nimprovement over unconstrained inference", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DINGO\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u89e3\u51b3\u6269\u6563LLM\u5728\u5e76\u884c\u751f\u6210\u65f6\u96be\u4ee5\u6ee1\u8db3\u5f62\u5f0f\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u6269\u6563LLM\u867d\u5728\u8fd0\u884c\u65f6\u6548\u7387\u4e0a\u6709\u4f18\u52bf\uff0c\u4f46\u65e0\u6cd5\u53ef\u9760\u6ee1\u8db3\u7528\u6237\u6307\u5b9a\u7684\u5f62\u5f0f\u7ea6\u675f\uff08\u5982\u6b63\u5219\u8868\u8fbe\u5f0f\uff09\uff0c\u9650\u5236\u4e86\u5176\u5728\u7ed3\u6784\u5316\u8f93\u51fa\u4efb\u52a1\uff08\u5982JSON\u751f\u6210\uff09\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faDINGO\uff0c\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u89c4\u5212\u7684\u7ea6\u675f\u89e3\u7801\u7b56\u7565\uff0c\u9ad8\u6548\u4e14\u80fd\u4fdd\u6301\u8f93\u51fa\u5206\u5e03\u7684\u771f\u5b9e\u6027\uff0c\u652f\u6301\u5728\u6ee1\u8db3\u6b63\u5219\u8868\u8fbe\u5f0f\u7ea6\u675f\u4e0b\u91c7\u6837\u9ad8\u6982\u7387\u8f93\u51fa\u5b57\u7b26\u4e32\u3002", "result": "\u5728\u7b26\u53f7\u6570\u5b66\u548cJSON\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDINGO\u5bf9\u6bd4\u65e0\u7ea6\u675f\u63a8\u7406\u6700\u9ad8\u63d0\u5347\u4e8668\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "DINGO\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563LLM\u7684\u7ea6\u675f\u89e3\u7801\u95ee\u9898\uff0c\u4e3a\u7ed3\u6784\u5316\u8f93\u51fa\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u6269\u6563LLM, \u7ea6\u675f\u89e3\u7801, \u52a8\u6001\u89c4\u5212, \u7ed3\u6784\u5316\u8f93\u51fa, \u6b63\u5219\u8868\u8fbe\u5f0f"}}
{"id": "2505.23170", "pdf": "https://arxiv.org/pdf/2505.23170", "abs": "https://arxiv.org/abs/2505.23170", "authors": ["Jian Zhu", "Farhan Samir", "Eleanor Chodroff", "David R. Mortensen"], "title": "ZIPA: A family of efficient models for multilingual phone recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "ACL 2025 Main", "summary": "We present ZIPA, a family of efficient speech models that advances the\nstate-of-the-art performance of crosslinguistic phone recognition. We first\ncurated IPAPack++, a large-scale multilingual speech corpus with 17,132 hours\nof normalized phone transcriptions and a novel evaluation set capturing unseen\nlanguages and sociophonetic variation. With the large-scale training data,\nZIPA, including transducer (ZIPA-T) and CTC-based (ZIPA-CR) variants, leverage\nthe efficient Zipformer backbones and outperform existing phone recognition\nsystems with much fewer parameters. Further scaling via noisy student training\non 11,000 hours of pseudo-labeled multilingual data yields further improvement.\nWhile ZIPA achieves strong performance on benchmarks, error analysis reveals\npersistent limitations in modeling sociophonetic diversity, underscoring\nchallenges for future research.", "AI": {"tldr": "ZIPA\u662f\u4e00\u7cfb\u5217\u9ad8\u6548\u7684\u8bed\u97f3\u6a21\u578b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u548c\u9ad8\u6548\u67b6\u6784\u63d0\u5347\u8de8\u8bed\u8a00\u97f3\u7d20\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u63a8\u52a8\u8de8\u8bed\u8a00\u97f3\u7d20\u8bc6\u522b\u7684\u524d\u6cbf\u6027\u80fd\uff0c\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u548c\u591a\u8bed\u8a00\u652f\u6301\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528IPAPack++\u6570\u636e\u96c6\u548cZipformer\u67b6\u6784\uff0c\u63d0\u51faZIPA-T\u548cZIPA-CR\u53d8\u4f53\uff0c\u5e76\u91c7\u7528\u566a\u58f0\u5b66\u751f\u8bad\u7ec3\u8fdb\u884c\u6269\u5c55\u3002", "result": "ZIPA\u5728\u53c2\u6570\u66f4\u5c11\u7684\u60c5\u51b5\u4e0b\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u7cfb\u7edf\uff0c\u4f46\u5206\u6790\u663e\u793a\u5176\u5728\u793e\u4f1a\u8bed\u97f3\u591a\u6837\u6027\u5efa\u6a21\u4e0a\u4ecd\u6709\u5c40\u9650\u3002", "conclusion": "ZIPA\u5728\u8de8\u8bed\u8a00\u97f3\u7d20\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u6539\u8fdb\u793e\u4f1a\u8bed\u97f3\u591a\u6837\u6027\u5efa\u6a21\u3002", "keywords": "ZIPA, \u8de8\u8bed\u8a00\u97f3\u7d20\u8bc6\u522b, IPAPack++, Zipformer, \u566a\u58f0\u5b66\u751f\u8bad\u7ec3"}}
{"id": "2505.22831", "pdf": "https://arxiv.org/pdf/2505.22831", "abs": "https://arxiv.org/abs/2505.22831", "authors": ["Peiling Jiang", "Haijun Xia"], "title": "Orca: Browsing at Scale Through User-Driven and AI-Facilitated Orchestration Across Malleable Webpages", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Web-based activities are fundamentally distributed across webpages. However,\nconventional browsers with stacks of tabs fail to support operating and\nsynthesizing large volumes of information across pages. While recent AI systems\nenable fully automated web browsing and information synthesis, they often\ndiminish user agency and hinder contextual understanding. Therefore, we explore\nhow AI could instead augment users' interactions with content across webpages\nand mitigate cognitive and manual efforts. Through literature on information\ntasks and web browsing challenges, and an iterative design process, we present\na rich set of novel interactions with our prototype web browser, Orca.\nLeveraging AI, Orca supports user-driven exploration, operation, organization,\nand synthesis of web content at scale. To enable browsing at scale, webpages\nare treated as malleable materials that humans and AI can collaboratively\nmanipulate and compose into a malleable, dynamic, and browser-level workspace.\nOur evaluation revealed an increased \"appetite\" for information foraging,\nenhanced user control, and more flexibility in sensemaking across a broader\ninformation landscape on the web.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6d4f\u89c8\u5668Orca\uff0c\u901a\u8fc7AI\u8f85\u52a9\u7528\u6237\u5728\u5927\u89c4\u6a21\u7f51\u9875\u4e2d\u63a2\u7d22\u3001\u64cd\u4f5c\u548c\u7ec4\u7ec7\u5185\u5bb9\uff0c\u63d0\u5347\u4fe1\u606f\u5904\u7406\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u4f20\u7edf\u6d4f\u89c8\u5668\u5728\u591a\u6807\u7b7e\u9875\u73af\u5883\u4e0b\u96be\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u4fe1\u606f\u7684\u64cd\u4f5c\u548c\u5408\u6210\uff0c\u800c\u73b0\u6709\u7684AI\u7cfb\u7edf\u867d\u80fd\u81ea\u52a8\u5316\u6d4f\u89c8\u4f46\u524a\u5f31\u4e86\u7528\u6237\u63a7\u5236\u529b\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u7528AI\u589e\u5f3a\u7528\u6237\u5728\u591a\u7f51\u9875\u95f4\u7684\u4ea4\u4e92\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u8fed\u4ee3\u8bbe\u8ba1\u8fc7\u7a0b\uff0c\u5f00\u53d1\u4e86\u539f\u578b\u6d4f\u89c8\u5668Orca\uff0c\u5c06\u7f51\u9875\u89c6\u4e3a\u53ef\u5851\u6750\u6599\uff0c\u652f\u6301\u7528\u6237\u4e0eAI\u534f\u4f5c\u64cd\u4f5c\u548c\u5408\u6210\u5185\u5bb9\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0cOrca\u63d0\u9ad8\u4e86\u7528\u6237\u7684\u4fe1\u606f\u83b7\u53d6\u610f\u613f\u3001\u63a7\u5236\u529b\u53ca\u5728\u66f4\u5e7f\u6cdb\u4fe1\u606f\u73af\u5883\u4e2d\u7684\u7075\u6d3b\u6027\u3002", "conclusion": "Orca\u8bc1\u660e\u4e86AI\u8f85\u52a9\u7684\u534f\u4f5c\u5f0f\u6d4f\u89c8\u5668\u8bbe\u8ba1\u80fd\u6709\u6548\u63d0\u5347\u7528\u6237\u5728\u5927\u89c4\u6a21\u4fe1\u606f\u5904\u7406\u4e2d\u7684\u80fd\u529b\u548c\u4f53\u9a8c\u3002", "keywords": "AI\u8f85\u52a9\u6d4f\u89c8, \u7528\u6237\u534f\u4f5c, \u4fe1\u606f\u5408\u6210, Orca\u6d4f\u89c8\u5668"}}
{"id": "2505.23062", "pdf": "https://arxiv.org/pdf/2505.23062", "abs": "https://arxiv.org/abs/2505.23062", "authors": ["Lingkai Kong", "Haichuan Wang", "Tonghan Wang", "Guojun Xiong", "Milind Tambe"], "title": "Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Incorporating pre-collected offline data from a source environment can\nsignificantly improve the sample efficiency of reinforcement learning (RL), but\nthis benefit is often challenged by discrepancies between the transition\ndynamics of the source and target environments. Existing methods typically\naddress this issue by penalizing or filtering out source transitions in high\ndynamics-gap regions. However, their estimation of the dynamics gap often\nrelies on KL divergence or mutual information, which can be ill-defined when\nthe source and target dynamics have disjoint support. To overcome these\nlimitations, we propose CompFlow, a method grounded in the theoretical\nconnection between flow matching and optimal transport. Specifically, we model\nthe target dynamics as a conditional flow built upon the output distribution of\nthe source-domain flow, rather than learning it directly from a Gaussian prior.\nThis composite structure offers two key advantages: (1) improved generalization\nfor learning target dynamics, and (2) a principled estimation of the dynamics\ngap via the Wasserstein distance between source and target transitions.\nLeveraging our principled estimation of the dynamics gap, we further introduce\nan optimistic active data collection strategy that prioritizes exploration in\nregions of high dynamics gap, and theoretically prove that it reduces the\nperformance disparity with the optimal policy. Empirically, CompFlow\noutperforms strong baselines across several RL benchmarks with shifted\ndynamics.", "AI": {"tldr": "CompFlow\u5229\u7528\u6d41\u5339\u914d\u548c\u6700\u4f18\u4f20\u8f93\u7684\u7406\u8bba\uff0c\u901a\u8fc7\u5efa\u7acb\u6e90-\u76ee\u6807\u52a8\u6001\u7684\u590d\u5408\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u6e90\u4e0e\u76ee\u6807\u73af\u5883\u52a8\u6001\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5e76\u4f18\u5316\u6570\u636e\u6536\u96c6\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7KL\u6563\u5ea6\u6216\u4e92\u4fe1\u606f\u4f30\u8ba1\u52a8\u6001\u5dee\u5f02\uff0c\u4f46\u5728\u6e90\u4e0e\u76ee\u6807\u52a8\u6001\u4e0d\u91cd\u53e0\u65f6\u53ef\u80fd\u5931\u6548\u3002", "method": "CompFlow\u901a\u8fc7\u5efa\u6a21\u76ee\u6807\u52a8\u6001\u4e3a\u6e90\u52a8\u6001\u6d41\u7684\u6761\u4ef6\u8f93\u51fa\uff0c\u5229\u7528Wasserstein\u8ddd\u79bb\u4f30\u8ba1\u5dee\u5f02\uff0c\u5e76\u5f15\u5165\u4e50\u89c2\u4e3b\u52a8\u6570\u636e\u6536\u96c6\u7b56\u7565\u3002", "result": "CompFlow\u5728\u52a8\u6001\u504f\u79fb\u7684RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CompFlow\u63d0\u4f9b\u4e86\u52a8\u6001\u5dee\u5f02\u7684\u7406\u8bba\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u6570\u636e\u6536\u96c6\u63d0\u5347\u6027\u80fd\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60,\u52a8\u6001\u5dee\u5f02,\u6d41\u5339\u914d,\u6700\u4f18\u4f20\u8f93,Wasserstein\u8ddd\u79bb"}}
{"id": "2505.23174", "pdf": "https://arxiv.org/pdf/2505.23174", "abs": "https://arxiv.org/abs/2505.23174", "authors": ["Naman Ahuja", "Fenil Bardoliya", "Chitta Baral", "Vivek Gupta"], "title": "Map&Make: Schema Guided Text to Table Generation", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Transforming dense, detailed, unstructured text into an interpretable and\nsummarised table, also colloquially known as Text-to-Table generation, is an\nessential task for information retrieval. Current methods, however, miss out on\nhow and what complex information to extract; they also lack the ability to\ninfer data from the text. In this paper, we introduce a versatile approach,\nMap&Make, which \"dissects\" text into propositional atomic statements. This\nfacilitates granular decomposition to extract the latent schema. The schema is\nthen used to populate the tables that capture the qualitative nuances and the\nquantitative facts in the original text. Our approach is tested against two\nchallenging datasets, Rotowire, renowned for its complex and multi-table\nschema, and Livesum, which demands numerical aggregation. By carefully\nidentifying and correcting hallucination errors in Rotowire, we aim to achieve\na cleaner and more reliable benchmark. We evaluate our method rigorously on a\ncomprehensive suite of comparative and referenceless metrics. Our findings\ndemonstrate significant improvement results across both datasets with better\ninterpretability in Text-to-Table generation. Moreover, through detailed\nablation studies and analyses, we investigate the factors contributing to\nsuperior performance and validate the practicality of our framework in\nstructured summarization tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMap&Make\u7684\u6587\u672c\u5230\u8868\u683c\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6587\u672c\u5206\u89e3\u4e3a\u539f\u5b50\u547d\u9898\u4ee5\u63d0\u53d6\u6f5c\u5728\u6a21\u5f0f\uff0c\u5e76\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u8868\u683c\u751f\u6210\u65b9\u6cd5\u5728\u63d0\u53d6\u590d\u6742\u4fe1\u606f\u53ca\u63a8\u65ad\u6570\u636e\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cMap&Make\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u5c06\u6587\u672c\u5206\u89e3\u4e3a\u539f\u5b50\u547d\u9898\u4ee5\u63d0\u53d6\u6a21\u5f0f\uff0c\u5e76\u586b\u5145\u8868\u683c\u4ee5\u6355\u6349\u539f\u6587\u7684\u8d28\u6027\u548c\u91cf\u5316\u4fe1\u606f\u3002", "result": "\u5728Rotowire\u548cLivesum\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8868\u683c\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660eMap&Make\u5728\u7ed3\u6784\u5316\u6458\u8981\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u8d8a\uff0c\u4e14\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "keywords": "\u6587\u672c\u5230\u8868\u683c\u3001\u4fe1\u606f\u63d0\u53d6\u3001\u7ed3\u6784\u5316\u6458\u8981\u3001Map&Make"}}
{"id": "2505.23063", "pdf": "https://arxiv.org/pdf/2505.23063", "abs": "https://arxiv.org/abs/2505.23063", "authors": ["Denis Mamba Kabala", "Adel Hafiane", "Laurent Bobelin", "Raphael Canals"], "title": "Loss-Guided Model Sharing and Local Learning Correction in Decentralized Federated Learning for Crop Disease Classification", "categories": ["cs.LG"], "comment": null, "summary": "Crop disease detection and classification is a critical challenge in\nagriculture, with major implications for productivity, food security, and\nenvironmental sustainability. While deep learning models such as CNN and ViT\nhave shown excellent performance in classifying plant diseases from images,\ntheir large-scale deployment is often limited by data privacy concerns.\nFederated Learning (FL) addresses this issue, but centralized FL remains\nvulnerable to single-point failures and scalability limits. In this paper, we\nintroduce a novel Decentralized Federated Learning (DFL) framework that uses\nvalidation loss (Loss_val) both to guide model sharing between peers and to\ncorrect local training via an adaptive loss function controlled by weighting\nparameter. We conduct extensive experiments using PlantVillage datasets with\nthree deep learning architectures (ResNet50, VGG16, and ViT_B16), analyzing the\nimpact of weighting parameter, the number of shared models, the number of\nclients, and the use of Loss_val versus Loss_train of other clients. Results\ndemonstrate that our DFL approach not only improves accuracy and convergence\nspeed, but also ensures better generalization and robustness across\nheterogeneous data environments making it particularly well-suited for\nprivacy-preserving agricultural applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\uff08DFL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4f5c\u7269\u75c5\u5bb3\u5206\u7c7b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u635f\u5931\u51fd\u6570\u548c\u9a8c\u8bc1\u635f\u5931\u4f18\u5316\u6a21\u578b\u5171\u4eab\u4e0e\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u9690\u79c1\u548c\u5355\u70b9\u6545\u969c\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f5c\u7269\u75c5\u5bb3\u68c0\u6d4b\u5bf9\u519c\u4e1a\u751f\u4ea7\u529b\u3001\u98df\u54c1\u5b89\u5168\u548c\u53ef\u6301\u7eed\u6027\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1CNN\u548cViT\u7b49\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u75c5\u5bb3\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5927\u89c4\u6a21\u90e8\u7f72\u53d7\u9650\u4e8e\u6570\u636e\u9690\u79c1\u95ee\u9898\u3002\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u867d\u7f13\u89e3\u4e86\u9690\u79c1\u95ee\u9898\uff0c\u4f46\u96c6\u4e2d\u5f0fFL\u5b58\u5728\u5355\u70b9\u6545\u969c\u548c\u53ef\u6269\u5c55\u6027\u9650\u5236\u3002", "method": "\u63d0\u51fa\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\uff08DFL\uff09\u6846\u67b6\uff0c\u5229\u7528\u9a8c\u8bc1\u635f\u5931\uff08Loss_val\uff09\u6307\u5bfc\u6a21\u578b\u5171\u4eab\u5e76\u901a\u8fc7\u52a0\u6743\u53c2\u6570\u63a7\u5236\u7684\u81ea\u9002\u5e94\u635f\u5931\u51fd\u6570\u4fee\u6b63\u672c\u5730\u8bad\u7ec3\u3002\u5b9e\u9a8c\u4f7f\u7528PlantVillage\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u4e86ResNet50\u3001VGG16\u548cViT_B16\u4e09\u79cd\u67b6\u6784\uff0c\u5206\u6790\u52a0\u6743\u53c2\u6570\u3001\u5171\u4eab\u6a21\u578b\u6570\u91cf\u3001\u5ba2\u6237\u7aef\u6570\u91cf\u53caLoss_val\u4e0eLoss_train\u7684\u5f71\u54cd\u3002", "result": "DFL\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u548c\u6536\u655b\u901f\u5ea6\uff0c\u8fd8\u589e\u5f3a\u4e86\u5f02\u6784\u6570\u636e\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u9002\u5408\u9690\u79c1\u4fdd\u62a4\u7684\u519c\u4e1a\u5e94\u7528\u3002", "conclusion": "DFL\u6846\u67b6\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f5c\u7269\u75c5\u5bb3\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u4e3a\u519c\u4e1a\u9886\u57df\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u4f5c\u7269\u75c5\u5bb3\u5206\u7c7b, \u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60, \u9690\u79c1\u4fdd\u62a4, \u6df1\u5ea6\u5b66\u4e60, \u519c\u4e1a\u5e94\u7528"}}
{"id": "2505.23177", "pdf": "https://arxiv.org/pdf/2505.23177", "abs": "https://arxiv.org/abs/2505.23177", "authors": ["Wenjing Xing", "Wenke Lu", "Yeheng Duan", "Bing Zhao", "Zhenghui kang", "Yaolong Wang", "Kai Gao", "Lei Qiao"], "title": "Infinite-Instruct: Synthesizing Scaling Code instruction Data with Bidirectional Synthesis and Static Verification", "categories": ["cs.CL"], "comment": null, "summary": "Traditional code instruction data synthesis methods suffer from limited\ndiversity and poor logic. We introduce Infinite-Instruct, an automated\nframework for synthesizing high-quality question-answer pairs, designed to\nenhance the code generation capabilities of large language models (LLMs). The\nframework focuses on improving the internal logic of synthesized problems and\nthe quality of synthesized code. First, \"Reverse Construction\" transforms code\nsnippets into diverse programming problems. Then, through \"Backfeeding\nConstruction,\" keywords in programming problems are structured into a knowledge\ngraph to reconstruct them into programming problems with stronger internal\nlogic. Finally, a cross-lingual static code analysis pipeline filters invalid\nsamples to ensure data quality. Experiments show that on mainstream code\ngeneration benchmarks, our fine-tuned models achieve an average performance\nimprovement of 21.70% on 7B-parameter models and 36.95% on 32B-parameter\nmodels. Using less than one-tenth of the instruction fine-tuning data, we\nachieved performance comparable to the Qwen-2.5-Coder-Instruct.\nInfinite-Instruct provides a scalable solution for LLM training in programming.\nWe open-source the datasets used in the experiments, including both unfiltered\nversions and filtered versions via static analysis. The data are available at\nhttps://github.com/xingwenjing417/Infinite-Instruct-dataset", "AI": {"tldr": "Infinite-Instruct\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u2018\u9006\u5411\u6784\u5efa\u2019\u548c\u2018\u53cd\u9988\u6784\u5efa\u2019\u65b9\u6cd5\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u4ee3\u7801\u95ee\u7b54\u5bf9\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u6307\u4ee4\u6570\u636e\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u591a\u6837\u6027\u548c\u903b\u8f91\u6027\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u63d0\u5347\u5408\u6210\u95ee\u9898\u5185\u90e8\u903b\u8f91\u548c\u4ee3\u7801\u8d28\u91cf\u7684\u6846\u67b6\u3002", "method": "\u91c7\u7528\u2018\u9006\u5411\u6784\u5efa\u2019\u5c06\u4ee3\u7801\u7247\u6bb5\u8f6c\u5316\u4e3a\u591a\u6837\u5316\u7f16\u7a0b\u95ee\u9898\uff0c\u901a\u8fc7\u2018\u53cd\u9988\u6784\u5efa\u2019\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u95ee\u9898\u903b\u8f91\uff0c\u5e76\u5229\u7528\u8de8\u8bed\u8a00\u9759\u6001\u4ee3\u7801\u5206\u6790\u8fc7\u6ee4\u65e0\u6548\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u4e3b\u6d41\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c7B\u53c2\u6570\u6a21\u578b\u5e73\u5747\u6027\u80fd\u63d0\u534721.70%\uff0c32B\u53c2\u6570\u6a21\u578b\u63d0\u534736.95%\uff0c\u4e14\u6570\u636e\u91cf\u4ec5\u4e3aQwen-2.5-Coder-Instruct\u7684\u5341\u5206\u4e4b\u4e00\u3002", "conclusion": "Infinite-Instruct\u4e3a\u7f16\u7a0b\u9886\u57df\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u4e86\u5b9e\u9a8c\u6570\u636e\u96c6\u3002", "keywords": "\u4ee3\u7801\u751f\u6210\u3001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u6570\u636e\u5408\u6210\u3001\u77e5\u8bc6\u56fe\u8c31\u3001\u9759\u6001\u5206\u6790"}}
{"id": "2505.22843", "pdf": "https://arxiv.org/pdf/2505.22843", "abs": "https://arxiv.org/abs/2505.22843", "authors": ["Alexander Herzog", "Aliai Eusebi", "Lorenzo Cavallaro"], "title": "Aurora: Are Android Malware Classifiers Reliable under Distribution Shift?", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The performance figures of modern drift-adaptive malware classifiers appear\npromising, but does this translate to genuine operational reliability? The\nstandard evaluation paradigm primarily focuses on baseline performance metrics,\nneglecting confidence-error alignment and operational stability. While\nTESSERACT established the importance of temporal evaluation, we take a\ncomplementary direction by investigating whether malware classifiers maintain\nreliable confidence estimates under distribution shifts and exploring the\ntensions between scientific advancement and practical impacts when they do not.\nWe propose AURORA, a framework to evaluate malware classifiers based on their\nconfidence quality and operational resilience. AURORA subjects the confidence\nprofile of a given model to verification to assess the reliability of its\nestimates. Unreliable confidence estimates erode operational trust, waste\nvaluable annotation budget on non-informative samples for active learning, and\nleave error-prone instances undetected in selective classification. AURORA is\nfurther complemented by a set of metrics designed to go beyond point-in-time\nperformance, striving towards a more holistic assessment of operational\nstability throughout temporal evaluation periods. The fragility we observe in\nstate-of-the-art frameworks across datasets of varying drift severity suggests\nthe need for a return to the whiteboard.", "AI": {"tldr": "\u8bba\u6587\u8d28\u7591\u73b0\u6709\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u5668\u7684\u64cd\u4f5c\u53ef\u9760\u6027\uff0c\u5e76\u63d0\u51faAURORA\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u4f30\u7f6e\u4fe1\u5ea6\u8d28\u91cf\u548c\u64cd\u4f5c\u5f39\u6027\u6765\u66f4\u5168\u9762\u5730\u6d4b\u8bd5\u5206\u7c7b\u5668\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u7f6e\u4fe1\u5ea6\u4e0d\u53ef\u9760\uff0c\u9700\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u5668\u867d\u7136\u5728\u6027\u80fd\u6307\u6807\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5ffd\u89c6\u4e86\u7f6e\u4fe1\u5ea6\u8bef\u5dee\u5bf9\u9f50\u548c\u64cd\u4f5c\u7a33\u5b9a\u6027\u3002\u4f5c\u8005\u65e8\u5728\u8bc4\u4f30\u5206\u7c7b\u5668\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u662f\u5426\u4fdd\u6301\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u5e76\u63a2\u8ba8\u79d1\u5b66\u8fdb\u6b65\u4e0e\u5b9e\u9645\u5f71\u54cd\u7684\u51b2\u7a81\u3002", "method": "\u63d0\u51faAURORA\u6846\u67b6\uff0c\u901a\u8fc7\u9a8c\u8bc1\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u5256\u9762\u6765\u8bc4\u4f30\u5176\u4f30\u8ba1\u7684\u53ef\u9760\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u7ec4\u8d85\u8d8a\u5373\u65f6\u6027\u80fd\u7684\u6307\u6807\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u64cd\u4f5c\u7a33\u5b9a\u6027\u8bc4\u4f30\u3002", "result": "\u5728\u591a\u79cd\u6f02\u79fb\u5f3a\u5ea6\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u73b0\u6709\u6700\u5148\u8fdb\u6846\u67b6\u7684\u8106\u5f31\u6027\u8868\u660e\uff0c\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u4ee5\u63d0\u9ad8\u7f6e\u4fe1\u5ea6\u53ef\u9760\u6027\u548c\u64cd\u4f5c\u5f39\u6027\u3002", "conclusion": "AURORA\u6846\u67b6\u4e3a\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u5668\u7684\u64cd\u4f5c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6280\u672f\u7684\u4e0d\u8db3\uff0c\u5e76\u547c\u5401\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "keywords": "\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u5668\uff0c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u64cd\u4f5c\u53ef\u9760\u6027\uff0c\u5206\u5e03\u6f02\u79fb\uff0cAURORA\u6846\u67b6"}}
{"id": "2505.23071", "pdf": "https://arxiv.org/pdf/2505.23071", "abs": "https://arxiv.org/abs/2505.23071", "authors": ["Peizheng Guo", "Jingyao Wang", "Huijie Guo", "Jiangmeng Li", "Chuxiong Sun", "Changwen Zheng", "Wenwen Qiang"], "title": "Multi-Modal Learning with Bayesian-Oriented Gradient Calibration", "categories": ["cs.LG"], "comment": null, "summary": "Multi-Modal Learning (MML) integrates information from diverse modalities to\nimprove predictive accuracy. However, existing methods mainly aggregate\ngradients with fixed weights and treat all dimensions equally, overlooking the\nintrinsic gradient uncertainty of each modality. This may lead to (i) excessive\nupdates in sensitive dimensions, degrading performance, and (ii) insufficient\nupdates in less sensitive dimensions, hindering learning. To address this\nissue, we propose BOGC-MML, a Bayesian-Oriented Gradient Calibration method for\nMML to explicitly model the gradient uncertainty and guide the model\noptimization towards the optimal direction. Specifically, we first model each\nmodality's gradient as a random variable and derive its probability\ndistribution, capturing the full uncertainty in the gradient space. Then, we\npropose an effective method that converts the precision (inverse variance) of\neach gradient distribution into a scalar evidence. This evidence quantifies the\nconfidence of each modality in every gradient dimension. Using these evidences,\nwe explicitly quantify per-dimension uncertainties and fuse them via a reduced\nDempster-Shafer rule. The resulting uncertainty-weighted aggregation produces a\ncalibrated update direction that balances sensitivity and conservatism across\ndimensions. Extensive experiments on multiple benchmark datasets demonstrate\nthe effectiveness and advantages of the proposed method.", "AI": {"tldr": "BOGC-MML proposes a Bayesian-Oriented Gradient Calibration method for multi-modal learning, addressing gradient uncertainty by modeling distributions and weighting updates with quantified confidence, improving predictive accuracy.", "motivation": "Existing multi-modal learning methods aggregate gradients with fixed weights, ignoring gradient uncertainty, which can lead to imbalanced updates and degraded performance.", "method": "The method models each modality's gradient distribution, converts gradient precision into evidence, quantifies per-dimension uncertainties, and fuses them with a Dempster-Shafer rule for calibrated updates.", "result": "Experiments on benchmark datasets show that BOGC-MML effectively balances gradient updates and enhances model performance.", "conclusion": "BOGC-MML successfully addresses gradient uncertainty in multi-modal learning, providing a more balanced and effective optimization approach.", "keywords": "multi-modal learning, gradient uncertainty, Bayesian, calibration, Dempster-Shafer rule"}}
{"id": "2505.23183", "pdf": "https://arxiv.org/pdf/2505.23183", "abs": "https://arxiv.org/abs/2505.23183", "authors": ["Gabriele Sarti", "Vil\u00e9m Zouhar", "Malvina Nissim", "Arianna Bisazza"], "title": "Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Under review. Code:\n  https://github.com/gsarti/labl/tree/main/examples/unsup_wqe Metrics:\n  https://huggingface.co/datasets/gsarti/unsup_wqe_metrics", "summary": "Word-level quality estimation (WQE) aims to automatically identify\nfine-grained error spans in machine-translated outputs and has found many uses,\nincluding assisting translators during post-editing. Modern WQE techniques are\noften expensive, involving prompting of large language models or ad-hoc\ntraining on large amounts of human-labeled data. In this work, we investigate\nefficient alternatives exploiting recent advances in language model\ninterpretability and uncertainty quantification to identify translation errors\nfrom the inner workings of translation models. In our evaluation spanning 14\nmetrics across 12 translation directions, we quantify the impact of human label\nvariation on metric performance by using multiple sets of human labels. Our\nresults highlight the untapped potential of unsupervised metrics, the\nshortcomings of supervised methods when faced with label uncertainty, and the\nbrittleness of single-annotator evaluation practices.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u8bed\u8a00\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u9ad8\u6548\u8bc6\u522b\u7ffb\u8bd1\u9519\u8bef\u7684\u65b9\u6cd5\uff0c\u5bf9\u6bd4\u4e86\u65e0\u76d1\u7763\u548c\u76d1\u7763\u5ea6\u91cf\u5728\u591a\u79cd\u7ffb\u8bd1\u65b9\u5411\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u4eba\u5de5\u6807\u6ce8\u53d8\u5f02\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u8bcd\u7ea7\u8d28\u91cf\u4f30\u8ba1\uff08WQE\uff09\u65b9\u6cd5\u901a\u5e38\u6210\u672c\u9ad8\u6602\uff0c\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6216\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\uff0c\u7ed3\u540812\u79cd\u7ffb\u8bd1\u65b9\u5411\u548c14\u9879\u5ea6\u91cf\u6807\u51c6\uff0c\u8bc4\u4f30\u65e0\u76d1\u7763\u6307\u6807\u548c\u76d1\u7763\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\u65e0\u76d1\u7763\u6307\u6807\u6f5c\u529b\u672a\u88ab\u5145\u5206\u6316\u6398\uff0c\u76d1\u7763\u65b9\u6cd5\u5728\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5355\u4e00\u6807\u6ce8\u8005\u8bc4\u4f30\u5b9e\u8df5\u5b58\u5728\u8106\u5f31\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u65e0\u76d1\u7763\u6307\u6807\u7684\u6f5c\u529b\uff0c\u5e76\u6307\u51fa\u9700\u8981\u6539\u8fdb\u76d1\u7763\u65b9\u6cd5\u4ee5\u5e94\u5bf9\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\uff0c\u540c\u65f6\u547c\u5401\u66f4\u7a33\u5065\u7684\u8bc4\u4f30\u5b9e\u8df5\u3002", "keywords": "\u8bcd\u7ea7\u8d28\u91cf\u4f30\u8ba1, \u8bed\u8a00\u6a21\u578b\u53ef\u89e3\u91ca\u6027, \u4e0d\u786e\u5b9a\u6027\u91cf\u5316, \u65e0\u76d1\u7763\u5ea6\u91cf, \u4eba\u5de5\u6807\u6ce8\u53d8\u5f02"}}
{"id": "2505.22845", "pdf": "https://arxiv.org/pdf/2505.22845", "abs": "https://arxiv.org/abs/2505.22845", "authors": ["Sandra H\u00f6ltervennhoff", "Jonas Ricker", "Maike M. Raphael", "Charlotte Schwedes", "Rebecca Weil", "Asja Fischer", "Thorsten Holz", "Lea Sch\u00f6nherr", "Sascha Fahl"], "title": "Security Benefits and Side Effects of Labeling AI-Generated Images", "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.SI"], "comment": null, "summary": "Generative artificial intelligence is developing rapidly, impacting humans'\ninteraction with information and digital media. It is increasingly used to\ncreate deceptively realistic misinformation, so lawmakers have imposed\nregulations requiring the disclosure of AI-generated content. However, only\nlittle is known about whether these labels reduce the risks of AI-generated\nmisinformation.\n  Our work addresses this research gap. Focusing on AI-generated images, we\nstudy the implications of labels, including the possibility of mislabeling.\nAssuming that simplicity, transparency, and trust are likely to impact the\nsuccessful adoption of such labels, we first qualitatively explore users'\nopinions and expectations of AI labeling using five focus groups. Second, we\nconduct a pre-registered online survey with over 1300 U.S. and EU participants\nto quantitatively assess the effect of AI labels on users' ability to recognize\nmisinformation containing either human-made or AI-generated images. Our focus\ngroups illustrate that, while participants have concerns about the practical\nimplementation of labeling, they consider it helpful in identifying\nAI-generated images and avoiding deception. However, considering security\nbenefits, our survey revealed an ambiguous picture, suggesting that users might\nover-rely on labels. While inaccurate claims supported by labeled AI-generated\nimages were rated less credible than those with unlabeled AI-images, the belief\nin accurate claims also decreased when accompanied by a labeled AI-generated\nimage. Moreover, we find the undesired side effect that human-made images\nconveying inaccurate claims were perceived as more credible in the presence of\nlabels.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0cAI\u751f\u6210\u5185\u5bb9\u7684\u6807\u7b7e\u5728\u5e2e\u52a9\u8bc6\u522b\u865a\u5047\u4fe1\u606f\u65b9\u9762\u6709\u4e00\u5b9a\u6548\u679c\uff0c\u4f46\u5b58\u5728\u7528\u6237\u8fc7\u5ea6\u4f9d\u8d56\u6807\u7b7e\u7684\u95ee\u9898\uff0c\u4e14\u6807\u7b7e\u53ef\u80fd\u610f\u5916\u63d0\u9ad8\u4eba\u7c7b\u5236\u4f5c\u865a\u5047\u4fe1\u606f\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u63a2\u8ba8AI\u751f\u6210\u5185\u5bb9\u7684\u6807\u7b7e\u662f\u5426\u80fd\u6709\u6548\u964d\u4f4e\u865a\u5047\u4fe1\u606f\u7684\u98ce\u9669\uff0c\u5e76\u7814\u7a76\u6807\u7b7e\u53ef\u80fd\u5e26\u6765\u7684\u526f\u4f5c\u7528\u3002", "method": "\u4f7f\u7528\u4e94\u4e2a\u7126\u70b9\u5c0f\u7ec4\u8fdb\u884c\u5b9a\u6027\u7814\u7a76\uff0c\u4e86\u89e3\u7528\u6237\u5bf9AI\u6807\u7b7e\u7684\u770b\u6cd5\uff1b\u968f\u540e\u8fdb\u884c\u9884\u6ce8\u518c\u7684\u5728\u7ebf\u8c03\u67e5\uff0c\u91cf\u5316\u8bc4\u4f30\u6807\u7b7e\u5bf91300\u591a\u540d\u6b27\u7f8e\u7528\u6237\u8bc6\u522b\u865a\u5047\u4fe1\u606f\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u6807\u7b7e\u6709\u52a9\u4e8e\u8bc6\u522bAI\u751f\u6210\u7684\u865a\u5047\u4fe1\u606f\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u7528\u6237\u8fc7\u5ea6\u4f9d\u8d56\u6807\u7b7e\uff0c\u540c\u65f6\u610f\u5916\u63d0\u9ad8\u4e86\u4eba\u7c7b\u5236\u4f5c\u865a\u5047\u4fe1\u606f\u7684\u53ef\u4fe1\u5ea6\u3002", "conclusion": "AI\u6807\u7b7e\u867d\u6709\u52a9\u4e8e\u8bc6\u522b\u865a\u5047\u4fe1\u606f\uff0c\u4f46\u9700\u8c28\u614e\u8bbe\u8ba1\u4ee5\u907f\u514d\u526f\u4f5c\u7528\u3002", "keywords": "AI\u751f\u6210\u5185\u5bb9, \u865a\u5047\u4fe1\u606f, \u6807\u7b7e, \u53ef\u4fe1\u5ea6, \u7528\u6237\u884c\u4e3a"}}
{"id": "2505.23084", "pdf": "https://arxiv.org/pdf/2505.23084", "abs": "https://arxiv.org/abs/2505.23084", "authors": ["Chang Yu", "Fang Liu", "Jie Zhu", "Shaobo Guo", "Yifan Gao", "Zhongheng Yang", "Meiwei Liu", "Qianwen Xing"], "title": "Gradient Boosting Decision Tree with LSTM for Investment Prediction", "categories": ["cs.LG"], "comment": "This paper have been accepted by IEEE confulence", "summary": "This paper proposes a hybrid framework combining LSTM (Long Short-Term\nMemory) networks with LightGBM and CatBoost for stock price prediction. The\nframework processes time-series financial data and evaluates performance using\nseven models: Artificial Neural Networks (ANNs), Convolutional Neural Networks\n(CNNs), Bidirectional LSTM (BiLSTM), vanilla LSTM, XGBoost, LightGBM, and\nstandard Neural Networks (NNs). Key metrics, including MAE, R-squared, MSE, and\nRMSE, are used to establish benchmarks across different time scales.\n  Building on these benchmarks, we develop an ensemble model that combines the\nstrengths of sequential and tree-based approaches. Experimental results show\nthat the proposed framework improves accuracy by 10 to 15 percent compared to\nindividual models and reduces error during market changes. This study\nhighlights the potential of ensemble methods for financial forecasting and\nprovides a flexible design for integrating new machine learning techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LSTM\u3001LightGBM\u548cCatBoost\u7684\u6df7\u5408\u6846\u67b6\u7528\u4e8e\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\uff0c\u901a\u8fc7\u4e03\u79cd\u6a21\u578b\u8bc4\u4f30\u6027\u80fd\uff0c\u7ed3\u679c\u663e\u793a\u76f8\u8f83\u4e8e\u5355\u4e00\u6a21\u578b\uff0c\u8be5\u6846\u67b6\u63d0\u9ad8\u4e8610%\u81f315%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u5e02\u573a\u53d8\u5316\u65f6\u51cf\u5c11\u8bef\u5dee\uff0c\u63a2\u7d22\u65f6\u5e8f\u6570\u636e\u548c\u6811\u6a21\u578b\u7684\u7ed3\u5408\u6f5c\u529b\u3002", "method": "\u91c7\u7528LSTM\u3001LightGBM\u548cCatBoost\u6784\u5efa\u6df7\u5408\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86\u5305\u62ecANN\u3001CNN\u3001BiLSTM\u7b49\u4e03\u79cd\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6df7\u5408\u6846\u67b6\u6bd4\u5355\u4e00\u6a21\u578b\u63d0\u9ad8\u4e8610%\u81f315%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u964d\u4f4e\u4e86\u5e02\u573a\u53d8\u5316\u65f6\u7684\u8bef\u5dee\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u96c6\u6210\u65b9\u6cd5\u5728\u91d1\u878d\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u8bbe\u8ba1\u4ee5\u6574\u5408\u65b0\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\u3002", "keywords": "\u80a1\u7968\u9884\u6d4b, LSTM, LightGBM, CatBoost, \u96c6\u6210\u5b66\u4e60"}}
{"id": "2505.23187", "pdf": "https://arxiv.org/pdf/2505.23187", "abs": "https://arxiv.org/abs/2505.23187", "authors": ["Yilong Li", "Chen Qian", "Yu Xia", "Ruijie Shi", "Yufan Dang", "Zihao Xie", "Ziming You", "Weize Chen", "Cheng Yang", "Weichuan Liu", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "title": "Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "Work in Progress", "summary": "Large Language Model-based multi-agent systems (MAS) have shown remarkable\nprogress in solving complex tasks through collaborative reasoning and\ninter-agent critique. However, existing approaches typically treat each task in\nisolation, resulting in redundant computations and limited generalization\nacross structurally similar tasks. To address this, we introduce multi-agent\ncross-task experiential learning (MAEL), a novel framework that endows\nLLM-driven agents with explicit cross-task learning and experience\naccumulation. We model the task-solving workflow on a graph-structured\nmulti-agent collaboration network, where agents propagate information and\ncoordinate via explicit connectivity. During the experiential learning phase,\nwe quantify the quality for each step in the task-solving workflow and store\nthe resulting rewards along with the corresponding inputs and outputs into each\nagent's individual experience pool. During inference, agents retrieve\nhigh-reward, task-relevant experiences as few-shot examples to enhance the\neffectiveness of each reasoning step, thereby enabling more accurate and\nefficient multi-agent collaboration. Experimental results on diverse datasets\ndemonstrate that MAEL empowers agents to learn from prior task experiences\neffectively-achieving faster convergence and producing higher-quality solutions\non current tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMAEL\u7684\u591a\u667a\u80fd\u4f53\u8de8\u4efb\u52a1\u7ecf\u9a8c\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u8de8\u4efb\u52a1\u5b66\u4e60\u548c\u7ecf\u9a8c\u79ef\u7d2f\u63d0\u5347LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5904\u7406\u4efb\u52a1\u65f6\u901a\u5e38\u5b64\u7acb\u5bf9\u5f85\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5197\u4f59\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u7ed3\u6784\u76f8\u4f3c\u7684\u4efb\u52a1\uff0cMAEL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7f51\u7edc\u5efa\u6a21\u4efb\u52a1\u6d41\u7a0b\uff0c\u901a\u8fc7\u91cf\u5316\u5de5\u4f5c\u6d41\u6b65\u9aa4\u8d28\u91cf\u5e76\u5b58\u50a8\u5956\u52b1\u3001\u8f93\u5165\u8f93\u51fa\u5230\u4e2a\u4f53\u7ecf\u9a8c\u6c60\uff0c\u63a8\u7406\u65f6\u68c0\u7d22\u9ad8\u5956\u52b1\u7ecf\u9a8c\u4f5c\u4e3a\u5c11\u91cf\u793a\u4f8b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMAEL\u80fd\u6709\u6548\u5229\u7528\u5148\u9a8c\u4efb\u52a1\u7ecf\u9a8c\uff0c\u52a0\u901f\u6536\u655b\u5e76\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "MAEL\u6846\u67b6\u901a\u8fc7\u8de8\u4efb\u52a1\u7ecf\u9a8c\u5b66\u4e60\u663e\u8457\u63d0\u5347\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "keywords": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf, LLM, \u8de8\u4efb\u52a1\u5b66\u4e60, \u7ecf\u9a8c\u79ef\u7d2f, \u56fe\u7ed3\u6784\u7f51\u7edc"}}
{"id": "2505.23086", "pdf": "https://arxiv.org/pdf/2505.23086", "abs": "https://arxiv.org/abs/2505.23086", "authors": ["Junyi An", "Xinyu Lu", "Chao Qu", "Yunfei Shi", "Peijia Lin", "Qianwei Tang", "Licheng Xu", "Fenglei Cao", "Yuan Qi"], "title": "Equivariant Spherical Transformer for Efficient Molecular Modeling", "categories": ["cs.LG", "cs.AI"], "comment": "24 pages, 3 figures", "summary": "SE(3)-equivariant Graph Neural Networks (GNNs) have significantly advanced\nmolecular system modeling by employing group representations. However, their\nmessage passing processes, which rely on tensor product-based convolutions, are\nlimited by insufficient non-linearity and incomplete group representations,\nthereby restricting expressiveness. To overcome these limitations, we introduce\nthe Equivariant Spherical Transformer (EST), a novel framework that leverages a\nTransformer structure within the spatial domain of group representations after\nFourier transform. We theoretically and empirically demonstrate that EST can\nencompass the function space of tensor products while achieving superior\nexpressiveness. Furthermore, EST's equivariant inductive bias is guaranteed\nthrough a uniform sampling strategy for the Fourier transform. Our experiments\ndemonstrate state-of-the-art performance by EST on various molecular\nbenchmarks, including OC20 and QM9.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86\u201c\u7b49\u53d8\u7403\u9762\u53d8\u6362\u5668\uff08EST\uff09\u201d\u8fd9\u4e00\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u514b\u670d\u73b0\u6709SE(3)-\u7b49\u53d8\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u975e\u7ebf\u6027\u548c\u7fa4\u8868\u793a\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u7ed3\u5408Transformer\u4e0e\u5085\u91cc\u53f6\u53d8\u6362\u63d0\u5347\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u5206\u5b50\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709SE(3)-\u7b49\u53d8GNNs\u7531\u4e8e\u57fa\u4e8e\u5f20\u91cf\u79ef\u7684\u5377\u79ef\u8fc7\u7a0b\u7f3a\u4e4f\u8db3\u591f\u7684\u975e\u7ebf\u6027\u548c\u5b8c\u6574\u7684\u7fa4\u8868\u793a\uff0c\u9650\u5236\u4e86\u5176\u5728\u5206\u5b50\u7cfb\u7edf\u5efa\u6a21\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u201c\u7b49\u53d8\u7403\u9762\u53d8\u6362\u5668\uff08EST\uff09\u201d\uff0c\u5728\u7fa4\u8868\u793a\u7684\u7a7a\u95f4\u57df\u4e2d\u7ed3\u5408Transformer\u7ed3\u6784\u4e0e\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u5e76\u901a\u8fc7\u5747\u5300\u91c7\u6837\u7b56\u7565\u4fdd\u6301\u7b49\u53d8\u5f52\u7eb3\u504f\u7f6e\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u8868\u660e\uff0cEST\u80fd\u591f\u8986\u76d6\u5f20\u91cf\u79ef\u7684\u51fd\u6570\u7a7a\u95f4\uff0c\u540c\u65f6\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u8868\u8fbe\u529b\uff0c\u5728OC20\u548cQM9\u7b49\u5206\u5b50\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "EST\u6846\u67b6\u901a\u8fc7\u5f15\u5165Transformer\u4e0e\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u6709\u6548\u63d0\u5347\u4e86SE(3)-\u7b49\u53d8\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4e3a\u5206\u5b50\u7cfb\u7edf\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u7b49\u53d8\u7403\u9762\u53d8\u6362\u5668\uff08EST\uff09\uff0cSE(3)-\u7b49\u53d8\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u5206\u5b50\u5efa\u6a21"}}
{"id": "2505.23191", "pdf": "https://arxiv.org/pdf/2505.23191", "abs": "https://arxiv.org/abs/2505.23191", "authors": ["Jinglong Gao", "Xiao Ding", "Lingxiao Zou", "Bibo Cai", "Bing Qin", "Ting Liu"], "title": "ExpeTrans: LLMs Are Experiential Transfer Learners", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 12 figs/tables", "summary": "Recent studies provide large language models (LLMs) with textual task-solving\nexperiences via prompts to improve their performance. However, previous methods\nrely on substantial human labor or time to gather such experiences for each\ntask, which is impractical given the growing variety of task types in user\nqueries to LLMs. To address this issue, we design an autonomous experience\ntransfer framework to explore whether LLMs can mimic human cognitive\nintelligence to autonomously transfer experience from existing source tasks to\nnewly encountered target tasks. This not only allows the acquisition of\nexperience without extensive costs of previous methods, but also offers a novel\npath for the generalization of LLMs. Experimental results on 13 datasets\ndemonstrate that our framework effectively improves the performance of LLMs.\nFurthermore, we provide a detailed analysis of each module in the framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4e3b\u7ecf\u9a8c\u8f6c\u79fb\u6846\u67b6\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u4ece\u73b0\u6709\u4efb\u52a1\u4e2d\u81ea\u52a8\u8fc1\u79fb\u7ecf\u9a8c\u5230\u65b0\u4efb\u52a1\uff0c\u65e0\u9700\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u6216\u65f6\u95f4\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6216\u65f6\u95f4\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63a2\u7d22LLMs\u80fd\u5426\u6a21\u4eff\u4eba\u7c7b\u8ba4\u77e5\u80fd\u529b\u81ea\u4e3b\u8fc1\u79fb\u7ecf\u9a8c\u3002", "method": "\u8bbe\u8ba1\u4e86\u81ea\u4e3b\u7ecf\u9a8c\u8f6c\u79fb\u6846\u67b6\uff0c\u5b9e\u9a8c\u572813\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86LLMs\u7684\u6027\u80fd\uff0c\u5e76\u5bf9\u6846\u67b6\u5404\u6a21\u5757\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u964d\u4f4e\u4e86\u7ecf\u9a8c\u83b7\u53d6\u6210\u672c\uff0c\u8fd8\u4e3aLLMs\u7684\u6cdb\u5316\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u7ecf\u9a8c\u8f6c\u79fb\u3001\u81ea\u4e3b\u6846\u67b6\u3001\u6cdb\u5316"}}
{"id": "2505.22852", "pdf": "https://arxiv.org/pdf/2505.22852", "abs": "https://arxiv.org/abs/2505.22852", "authors": ["Krti Tallam", "Emma Miller"], "title": "Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "CaMeL (Capabilities for Machine Learning) introduces a capability-based\nsandbox to mitigate prompt injection attacks in large language model (LLM)\nagents. While effective, CaMeL assumes a trusted user prompt, omits\nside-channel concerns, and incurs performance tradeoffs due to its dual-LLM\ndesign. This response identifies these issues and proposes engineering\nimprovements to expand CaMeL's threat coverage and operational usability. We\nintroduce: (1) prompt screening for initial inputs, (2) output auditing to\ndetect instruction leakage, (3) a tiered-risk access model to balance usability\nand control, and (4) a verified intermediate language for formal guarantees.\nTogether, these upgrades align CaMeL with best practices in enterprise security\nand support scalable deployment.", "AI": {"tldr": "CaMeL\u901a\u8fc7\u57fa\u4e8e\u80fd\u529b\u7684\u6c99\u76d2\u7f13\u89e3LLM\u4ee3\u7406\u4e2d\u7684\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u4f46\u5b58\u5728\u4fe1\u4efb\u5047\u8bbe\u3001\u4fa7\u4fe1\u9053\u95ee\u9898\u548c\u6027\u80fd\u6298\u8877\u7684\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51fa\u6539\u8fdb\uff1a\u63d0\u793a\u7b5b\u9009\u3001\u8f93\u51fa\u5ba1\u8ba1\u3001\u5206\u5c42\u98ce\u9669\u8bbf\u95ee\u6a21\u578b\u548c\u9a8c\u8bc1\u4e2d\u95f4\u8bed\u8a00\uff0c\u4ee5\u589e\u5f3a\u5b89\u5168\u6027\u548c\u53ef\u7528\u6027\u3002", "motivation": "\u9488\u5bf9CaMeL\u73b0\u6709\u8bbe\u8ba1\u4e2d\u5047\u8bbe\u53ef\u4fe1\u7528\u6237\u63d0\u793a\u3001\u5ffd\u7565\u4fa7\u4fe1\u9053\u95ee\u9898\u53ca\u53ccLLM\u8bbe\u8ba1\u7684\u6027\u80fd\u6298\u8877\uff0c\u63d0\u51fa\u5de5\u7a0b\u6539\u8fdb\u4ee5\u6269\u5c55\u5a01\u80c1\u8986\u76d6\u8303\u56f4\u548c\u64cd\u4f5c\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u56db\u79cd\u65b9\u6cd5\uff1a1\uff09\u521d\u59cb\u8f93\u5165\u7684\u63d0\u793a\u7b5b\u9009\uff0c2\uff09\u68c0\u6d4b\u6307\u4ee4\u6cc4\u6f0f\u7684\u8f93\u51fa\u5ba1\u8ba1\uff0c3\uff09\u5e73\u8861\u53ef\u7528\u6027\u4e0e\u63a7\u5236\u7684\u5206\u5c42\u98ce\u9669\u8bbf\u95ee\u6a21\u578b\uff0c4\uff09\u63d0\u4f9b\u5f62\u5f0f\u5316\u4fdd\u8bc1\u7684\u9a8c\u8bc1\u4e2d\u95f4\u8bed\u8a00\u3002", "result": "\u6539\u8fdb\u540e\u7684CaMeL\u66f4\u7b26\u5408\u4f01\u4e1a\u5b89\u5168\u6700\u4f73\u5b9e\u8df5\uff0c\u652f\u6301\u53ef\u6269\u5c55\u90e8\u7f72\u3002", "conclusion": "\u63d0\u51fa\u7684\u56db\u9879\u5347\u7ea7\u63aa\u65bd\u6709\u6548\u589e\u5f3a\u4e86CaMeL\u7684\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u4f01\u4e1a\u7ea7\u5e94\u7528\u3002", "keywords": "CaMeL, LLM\u4ee3\u7406, \u63d0\u793a\u6ce8\u5165\u653b\u51fb, \u4f01\u4e1a\u5b89\u5168, \u5f62\u5f0f\u5316\u4fdd\u8bc1"}}
{"id": "2505.23094", "pdf": "https://arxiv.org/pdf/2505.23094", "abs": "https://arxiv.org/abs/2505.23094", "authors": ["Chongjie Si", "Zhiyi Shi", "Yadao Wang", "Xiaokang Yang", "Susanto Rahardja", "Wei Shen"], "title": "MAP: Revisiting Weight Decomposition for Low-Rank Adaptation", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The rapid development of large language models has revolutionized natural\nlanguage processing, but their fine-tuning remains computationally expensive,\nhindering broad deployment. Parameter-efficient fine-tuning (PEFT) methods,\nsuch as LoRA, have emerged as solutions. Recent work like DoRA attempts to\nfurther decompose weight adaptation into direction and magnitude components.\nHowever, existing formulations often define direction heuristically at the\ncolumn level, lacking a principled geometric foundation. In this paper, we\npropose MAP, a novel framework that reformulates weight matrices as\nhigh-dimensional vectors and decouples their adaptation into direction and\nmagnitude in a rigorous manner. MAP normalizes the pre-trained weights, learns\na directional update, and introduces two scalar coefficients to independently\nscale the magnitude of the base and update vectors. This design enables more\ninterpretable and flexible adaptation, and can be seamlessly integrated into\nexisting PEFT methods. Extensive experiments show that MAP significantly\nimproves performance when coupling with existing methods, offering a simple yet\npowerful enhancement to existing PEFT methods. Given the universality and\nsimplicity of MAP, we hope it can serve as a default setting for designing\nfuture PEFT methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u6846\u67b6MAP\uff0c\u901a\u8fc7\u5c06\u6743\u91cd\u77e9\u9635\u91cd\u65b0\u8868\u8ff0\u4e3a\u9ad8\u7ef4\u5411\u91cf\u5e76\u4e25\u8c28\u5730\u89e3\u8026\u65b9\u5411\u548c\u5e45\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684PEFT\u65b9\u6cd5\uff08\u5982LoRA\u548cDoRA\uff09\u5728\u65b9\u5411\u5b9a\u4e49\u4e0a\u7f3a\u4e4f\u51e0\u4f55\u57fa\u7840\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u66f4\u4e25\u8c28\u4e14\u7075\u6d3b\u7684\u65b9\u5411\u548c\u5e45\u5ea6\u89e3\u8026\u6846\u67b6\u3002", "method": "MAP\u5c06\u9884\u8bad\u7ec3\u6743\u91cd\u5f52\u4e00\u5316\uff0c\u5b66\u4e60\u65b9\u5411\u6027\u66f4\u65b0\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u6807\u91cf\u7cfb\u6570\u72ec\u7acb\u8c03\u6574\u57fa\u7840\u5411\u91cf\u548c\u66f4\u65b0\u5411\u91cf\u7684\u5e45\u5ea6\uff0c\u4ece\u800c\u589e\u5f3a\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMAP\u4e0e\u73b0\u6709PEFT\u65b9\u6cd5\u7ed3\u5408\u65f6\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u672a\u6765PEFT\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u7b80\u5355\u7684\u589e\u5f3a\u65b9\u6848\u3002", "conclusion": "MAP\u56e0\u5176\u666e\u9002\u6027\u548c\u7b80\u6d01\u6027\uff0c\u6709\u671b\u6210\u4e3a\u672a\u6765PEFT\u65b9\u6cd5\u8bbe\u8ba1\u7684\u9ed8\u8ba4\u914d\u7f6e\u3002", "keywords": "\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u3001MAP\u6846\u67b6\u3001\u65b9\u5411\u4e0e\u5e45\u5ea6\u89e3\u8026\u3001\u6743\u91cd\u9002\u5e94\u3001\u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.23224", "pdf": "https://arxiv.org/pdf/2505.23224", "abs": "https://arxiv.org/abs/2505.23224", "authors": ["Zhitao He", "Sandeep Polisetty", "Zhiyuan Fan", "Yuchen Huang", "Shujin Wu", "Yi R.", "Fung"], "title": "MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "In recent years, multimodal large language models (MLLMs) have made\nsignificant progress but continue to face inherent challenges in multimodal\nreasoning, which requires multi-level (e.g., perception, reasoning) and\nmulti-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior\nwork on estimating model confidence tends to focus on the overall response for\ntraining and calibration, but fails to assess confidence in each reasoning\nstep, leading to undesirable hallucination snowballing. In this work, we\npresent MMBoundary, a novel framework that advances the knowledge boundary\nawareness of MLLMs through reasoning step confidence calibration. To achieve\nthis, we propose to incorporate complementary textual and cross-modal\nself-rewarding signals to estimate confidence at each step of the MLLM\nreasoning process. In addition to supervised fine-tuning MLLM on this set of\nself-rewarded confidence estimation signal for initial confidence expression\nwarm-up, we introduce a reinforcement learning stage with multiple reward\nfunctions for further aligning model knowledge and calibrating confidence at\neach reasoning step, enhancing reasoning chain self-correction. Empirical\nresults show that MMBoundary significantly outperforms existing methods across\ndiverse domain datasets and metrics, achieving an average of 7.5% reduction in\nmultimodal confidence calibration errors and up to 8.3% improvement in task\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MMBoundary\u6846\u67b6\uff0c\u901a\u8fc7\u6821\u51c6\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u63a8\u7406\u6b65\u9aa4\u7f6e\u4fe1\u5ea6\uff0c\u63d0\u5347\u5176\u77e5\u8bc6\u8fb9\u754c\u610f\u8bc6\uff0c\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\u3002\u7ed3\u5408\u6587\u672c\u548c\u8de8\u6a21\u6001\u81ea\u5956\u52b1\u4fe1\u53f7\uff0c\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u6539\u8fdb\u63a8\u7406\u94fe\u81ea\u6211\u4fee\u6b63\uff0c\u5b9e\u9a8c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u7ea7\u3001\u591a\u7c92\u5ea6\u63a8\u7406\u4e2d\u5b58\u5728\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u5e7b\u89c9\u7d2f\u79ef\u3002\u9700\u8981\u9488\u5bf9\u63a8\u7406\u6b65\u9aa4\u7684\u7ec6\u7c92\u5ea6\u7f6e\u4fe1\u5ea6\u6821\u51c6\u4ee5\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u3002", "method": "1. \u63d0\u51faMMBoundary\u6846\u67b6\uff0c\u7ed3\u5408\u6587\u672c\u548c\u8de8\u6a21\u6001\u81ea\u5956\u52b1\u4fe1\u53f7\u8bc4\u4f30\u63a8\u7406\u6b65\u9aa4\u7f6e\u4fe1\u5ea6\uff1b2. \u76d1\u7763\u5fae\u8c03\u521d\u59cb\u7f6e\u4fe1\u5ea6\u8868\u8fbe\uff1b3. \u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u901a\u8fc7\u591a\u5956\u52b1\u51fd\u6570\u6821\u51c6\u63a8\u7406\u6b65\u9aa4\u7f6e\u4fe1\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMMBoundary\u5728\u591a\u9886\u57df\u6570\u636e\u96c6\u548c\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u51cf\u5c117.5%\u7684\u591a\u6a21\u6001\u7f6e\u4fe1\u5ea6\u6821\u51c6\u8bef\u5dee\uff0c\u4efb\u52a1\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe8.3%\u3002", "conclusion": "\u901a\u8fc7\u7ec6\u7c92\u5ea6\u63a8\u7406\u6b65\u9aa4\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0cMMBoundary\u6709\u6548\u63d0\u5347MLLMs\u7684\u63a8\u7406\u94fe\u81ea\u6211\u4fee\u6b63\u80fd\u529b\uff0c\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\uff0c\u63a8\u52a8\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, \u7f6e\u4fe1\u5ea6\u6821\u51c6, \u63a8\u7406\u94fe, \u81ea\u6211\u5956\u52b1, \u5f3a\u5316\u5b66\u4e60"}}
{"id": "2505.22857", "pdf": "https://arxiv.org/pdf/2505.22857", "abs": "https://arxiv.org/abs/2505.22857", "authors": ["Vladimir Bataev", "Andrei Andrusenko", "Lilit Grigoryan", "Aleksandr Laptev", "Vitaly Lavrukhin", "Boris Ginsburg"], "title": "NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy ASR Decoding", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "Statistical n-gram language models are widely used for context-biasing tasks\nin Automatic Speech Recognition (ASR). However, existing implementations lack\ncomputational efficiency due to poor parallelization, making context-biasing\nless appealing for industrial use. This work rethinks data structures for\nstatistical n-gram language models to enable fast and parallel operations for\nGPU-optimized inference. Our approach, named NGPU-LM, introduces customizable\ngreedy decoding for all major ASR model types - including transducers,\nattention encoder-decoder models, and CTC - with less than 7% computational\noverhead. The proposed approach can eliminate more than 50% of the accuracy gap\nbetween greedy and beam search for out-of-domain scenarios while avoiding\nsignificant slowdown caused by beam search. The implementation of the proposed\nNGPU-LM is open-sourced.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNGPU-LM\u7684\u9ad8\u6548\u5e76\u884c\u7edf\u8ba1n-gram\u8bed\u8a00\u6a21\u578b\uff0c\u4f18\u5316GPU\u63a8\u7406\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7edf\u8ba1n-gram\u8bed\u8a00\u6a21\u578b\u5728\u5e76\u884c\u5316\u4e0a\u6548\u7387\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91cd\u65b0\u8bbe\u8ba1\u6570\u636e\u7ed3\u6784\uff0c\u652f\u6301\u5feb\u901f\u5e76\u884c\u64cd\u4f5c\uff0c\u5e76\u5f15\u5165\u53ef\u5b9a\u5236\u8d2a\u5a6a\u89e3\u7801\uff0c\u9002\u7528\u4e8e\u591a\u79cdASR\u6a21\u578b\u7c7b\u578b\u3002", "result": "\u8ba1\u7b97\u5f00\u9500\u4f4e\u4e8e7%\uff0c\u5728\u57df\u5916\u573a\u666f\u4e2d\u7f29\u5c0f\u4e86\u8d2a\u5a6a\u641c\u7d22\u4e0e\u675f\u641c\u7d2250%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u5dee\u8ddd\u3002", "conclusion": "NGPU-LM\u5728\u63d0\u5347\u6548\u7387\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u573a\u666f\u3002", "keywords": "\u7edf\u8ba1n-gram\u8bed\u8a00\u6a21\u578b, GPU\u4f18\u5316, ASR, \u5e76\u884c\u8ba1\u7b97, \u8d2a\u5a6a\u89e3\u7801"}}
{"id": "2505.23098", "pdf": "https://arxiv.org/pdf/2505.23098", "abs": "https://arxiv.org/abs/2505.23098", "authors": ["Kuan Xu", "Zhiguang Cao", "Chenlong Zheng", "Linong Liu"], "title": "Learning to Search for Vehicle Routing with Multiple Time Windows", "categories": ["cs.LG"], "comment": null, "summary": "In this study, we propose a reinforcement learning-based adaptive variable\nneighborhood search (RL-AVNS) method designed for effectively solving the\nVehicle Routing Problem with Multiple Time Windows (VRPMTW). Unlike traditional\nadaptive approaches that rely solely on historical operator performance, our\nmethod integrates a reinforcement learning framework to dynamically select\nneighborhood operators based on real-time solution states and learned\nexperience. We introduce a fitness metric that quantifies customers' temporal\nflexibility to improve the shaking phase, and employ a transformer-based neural\npolicy network to intelligently guide operator selection during the local\nsearch. Extensive computational experiments are conducted on realistic\nscenarios derived from the replenishment of unmanned vending machines,\ncharacterized by multiple clustered replenishment windows. Results demonstrate\nthat RL-AVNS significantly outperforms traditional variable neighborhood search\n(VNS), adaptive VNS (AVNS), and state-of-the-art learning-based heuristics,\nachieving substantial improvements in solution quality and computational\nefficiency across various instance scales and time window complexities.\nParticularly notable is the algorithm's capability to generalize effectively to\nproblem instances not encountered during training, underscoring its practical\nutility for complex logistics scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u53d8\u90bb\u57df\u641c\u7d22\u65b9\u6cd5\uff08RL-AVNS\uff09\uff0c\u7528\u4e8e\u9ad8\u6548\u89e3\u51b3\u5e26\u591a\u65f6\u95f4\u7a97\u7684\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08VRPMTW\uff09\uff0c\u901a\u8fc7\u5b9e\u65f6\u72b6\u6001\u548c\u5b66\u4e60\u7ecf\u9a8c\u52a8\u6001\u9009\u62e9\u90bb\u57df\u7b97\u5b50\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u81ea\u9002\u5e94\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5386\u53f2\u7b97\u5b50\u6027\u80fd\uff0c\u96be\u4ee5\u52a8\u6001\u9002\u5e94\u590d\u6742\u95ee\u9898\u573a\u666f\uff0c\u5e0c\u671b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u63d0\u5347\u89e3\u7684\u5b9e\u65f6\u6027\u548c\u667a\u80fd\u5316\u3002", "method": "\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u53d8\u90bb\u57df\u641c\u7d22\uff0c\u5f15\u5165\u65f6\u95f4\u7075\u6d3b\u6027\u6307\u6807\u4f18\u5316\u632f\u52a8\u9636\u6bb5\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u7b56\u7565\u7f51\u7edc\u6307\u5bfc\u5c40\u90e8\u641c\u7d22\u4e2d\u7684\u7b97\u5b50\u9009\u62e9\u3002", "result": "\u5728\u65e0\u4eba\u552e\u8d27\u673a\u8865\u8d27\u573a\u666f\u7684\u5b9e\u9a8c\u4e2d\uff0cRL-AVNS\u5728\u89e3\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edfVNS\u3001AVNS\u53ca\u73b0\u6709\u5b66\u4e60\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4e14\u80fd\u6cdb\u5316\u81f3\u672a\u8bad\u7ec3\u7684\u95ee\u9898\u5b9e\u4f8b\u3002", "conclusion": "RL-AVNS\u4e3a\u590d\u6742\u7269\u6d41\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60\uff0c\u81ea\u9002\u5e94\u53d8\u90bb\u57df\u641c\u7d22\uff0c\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff0c\u591a\u65f6\u95f4\u7a97\uff0cTransformer\u7b56\u7565\u7f51\u7edc"}}
{"id": "2505.23229", "pdf": "https://arxiv.org/pdf/2505.23229", "abs": "https://arxiv.org/abs/2505.23229", "authors": ["Hao Lu", "Yanchi Gu", "Haoyuan Huang", "Yulin Zhou", "Ningxin Zhu", "Chen Li"], "title": "MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "50 pages, 3 figures", "summary": "The integration of Monte Carlo Tree Search (MCTS) with Large Language Models\n(LLMs) has demonstrated significant success in structured, problem-oriented\ntasks. However, applying these methods to open-ended dialogues, such as those\nin psychological counseling, presents unique challenges. Unlike tasks with\nobjective correctness, success in therapeutic conversations depends on\nsubjective factors like empathetic engagement, ethical adherence, and alignment\nwith human preferences, for which strict \"correctness\" criteria are\nill-defined. Existing result-oriented MCTS approaches can therefore produce\nmisaligned responses. To address this, we introduce MCTSr-Zero, an MCTS\nframework designed for open-ended, human-centric dialogues. Its core innovation\nis \"domain alignment\", which shifts the MCTS search objective from predefined\nend-states towards conversational trajectories that conform to target domain\nprinciples (e.g., empathy in counseling). Furthermore, MCTSr-Zero incorporates\n\"Regeneration\" and \"Meta-Prompt Adaptation\" mechanisms to substantially broaden\nexploration by allowing the MCTS to consider fundamentally different initial\ndialogue strategies. We evaluate MCTSr-Zero in psychological counseling by\ngenerating multi-turn dialogue data, which is used to fine-tune an LLM, PsyLLM.\nWe also introduce PsyEval, a benchmark for assessing multi-turn psychological\ncounseling dialogues. Experiments demonstrate that PsyLLM achieves\nstate-of-the-art performance on PsyEval and other relevant metrics, validating\nMCTSr-Zero's effectiveness in generating high-quality, principle-aligned\nconversational data for human-centric domains and addressing the LLM challenge\nof consistently adhering to complex psychological standards.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MCTSr-Zero\u6846\u67b6\uff0c\u901a\u8fc7\u9886\u57df\u5bf9\u9f50\u3001\u518d\u751f\u548c\u5143\u63d0\u793a\u9002\u5e94\u673a\u5236\u6539\u8fdbMCTS\uff0c\u4ee5\u751f\u6210\u7b26\u5408\u5fc3\u7406\u6cbb\u7597\u9886\u57df\u539f\u5219\u7684\u5bf9\u8bdd\u6570\u636e\uff0c\u5e76\u5728\u5fc3\u7406\u8f85\u5bfc\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709MCTS\u65b9\u6cd5\u5728\u5f00\u653e\u5f0f\u5bf9\u8bdd\uff08\u5982\u5fc3\u7406\u8f85\u5bfc\uff09\u4e2d\u56e0\u7f3a\u4e4f\u660e\u786e\u6b63\u786e\u6027\u6807\u51c6\u800c\u6613\u4ea7\u751f\u4e0d\u6070\u5f53\u56de\u5e94\uff0c\u9700\u4e00\u79cd\u80fd\u9002\u5e94\u4e3b\u89c2\u6027\u9700\u6c42\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMCTSr-Zero\u6846\u67b6\uff0c\u5f15\u5165\u9886\u57df\u5bf9\u9f50\uff08\u76ee\u6807\u8f6c\u5411\u7b26\u5408\u9886\u57df\u539f\u5219\u7684\u5bf9\u8bdd\u8f68\u8ff9\uff09\u3001\u518d\u751f\u548c\u5143\u63d0\u793a\u9002\u5e94\u673a\u5236\uff08\u62d3\u5bbd\u7b56\u7565\u63a2\u7d22\uff09\u3002", "result": "\u751f\u6210\u7684PsyLLM\u6a21\u578b\u5728PsyEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86MCTSr-Zero\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u539f\u5219\u5bf9\u9f50\u6570\u636e\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "MCTSr-Zero\u6210\u529f\u89e3\u51b3\u4e86LLM\u5728\u590d\u6742\u5fc3\u7406\u6807\u51c6\u4e0b\u4fdd\u6301\u4e00\u81f4\u6027\u7684\u6311\u6218\uff0c\u9002\u7528\u4e8e\u4eba\u672c\u9886\u57df\u5bf9\u8bdd\u751f\u6210\u3002", "keywords": "\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5fc3\u7406\u8f85\u5bfc\uff0c\u5f00\u653e\u5f0f\u5bf9\u8bdd\uff0c\u9886\u57df\u5bf9\u9f50"}}
{"id": "2505.22860", "pdf": "https://arxiv.org/pdf/2505.22860", "abs": "https://arxiv.org/abs/2505.22860", "authors": ["Bargav Jayaraman", "Virendra J. Marathe", "Hamid Mozaffari", "William F. Shen", "Krishnaram Kenthapadi"], "title": "Permissioned LLMs: Enforcing Access Control in Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "In enterprise settings, organizational data is segregated, siloed and\ncarefully protected by elaborate access control frameworks. These access\ncontrol structures can completely break down if an LLM fine-tuned on the siloed\ndata serves requests, for downstream tasks, from individuals with disparate\naccess privileges. We propose Permissioned LLMs (PermLLM), a new class of LLMs\nthat superimpose the organizational data access control structures on query\nresponses they generate. We formalize abstractions underpinning the means to\ndetermine whether access control enforcement happens correctly over LLM query\nresponses. Our formalism introduces the notion of a relevant response that can\nbe used to prove whether a PermLLM mechanism has been implemented correctly. We\nalso introduce a novel metric, called access advantage, to empirically evaluate\nthe efficacy of a PermLLM mechanism. We introduce three novel PermLLM\nmechanisms that build on Parameter Efficient Fine-Tuning to achieve the desired\naccess control. We furthermore present two instantiations of access\nadvantage--(i) Domain Distinguishability Index (DDI) based on Membership\nInference Attacks, and (ii) Utility Gap Index (UGI) based on LLM utility\nevaluation. We demonstrate the efficacy of our PermLLM mechanisms through\nextensive experiments on four public datasets (GPQA, RCV1, SimpleQA, and WMDP),\nin addition to evaluating the validity of DDI and UGI metrics themselves for\nquantifying access control in LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPermissioned LLMs\uff08PermLLM\uff09\u7684\u65b0\u578bLLM\uff0c\u901a\u8fc7\u5728\u67e5\u8be2\u54cd\u5e94\u4e0a\u53e0\u52a0\u7ec4\u7ec7\u6570\u636e\u8bbf\u95ee\u63a7\u5236\u7ed3\u6784\u6765\u89e3\u51b3\u4f01\u4e1a\u73af\u5883\u4e2d\u6570\u636e\u9694\u79bb\u548c\u8bbf\u95ee\u63a7\u5236\u95ee\u9898\u3002", "motivation": "\u4f01\u4e1a\u73af\u5883\u4e2d\uff0c\u6570\u636e\u9694\u79bb\u548c\u4fdd\u62a4\u5bfc\u81f4\u8bbf\u95ee\u63a7\u5236\u7ed3\u6784\u5728LLM\u5904\u7406\u4e0b\u6e38\u4efb\u52a1\u65f6\u53ef\u80fd\u5931\u6548\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u786e\u4fdd\u8bbf\u95ee\u63a7\u5236\u7684\u6b63\u786e\u5b9e\u65bd\u3002", "method": "\u63d0\u51fa\u4e86PermLLM\u673a\u5236\uff0c\u57fa\u4e8e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08Parameter Efficient Fine-Tuning\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u76f8\u5173\u54cd\u5e94\u548c\u8bbf\u95ee\u4f18\u52bf\uff08access advantage\uff09\u7684\u5f62\u5f0f\u5316\u6982\u5ff5\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\uff08GPQA\u3001RCV1\u3001SimpleQA\u548cWMDP\uff09\u4e0a\u9a8c\u8bc1\u4e86PermLLM\u673a\u5236\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u91cf\u5316\u8bbf\u95ee\u63a7\u5236\u7684\u6307\u6807\uff1aDDI\u548cUGI\u3002", "conclusion": "PermLLM\u80fd\u591f\u6709\u6548\u5b9e\u65bd\u8bbf\u95ee\u63a7\u5236\uff0c\u540c\u65f6DDI\u548cUGI\u4f5c\u4e3a\u91cf\u5316\u6307\u6807\u662f\u53ef\u9760\u7684\u3002", "keywords": "Permissioned LLMs, access control, Parameter Efficient Fine-Tuning, relevant response, access advantage"}}
{"id": "2505.23099", "pdf": "https://arxiv.org/pdf/2505.23099", "abs": "https://arxiv.org/abs/2505.23099", "authors": ["Chongjie Si", "Xuankun Yang", "Muqing Liu", "Yadao Wang", "Xiaokang Yang", "Wenbo Su", "Bo Zheng", "Wei Shen"], "title": "Weight Spectra Induced Efficient Model Adaptation", "categories": ["cs.LG"], "comment": null, "summary": "Large-scale foundation models have demonstrated remarkable versatility across\na wide range of downstream tasks. However, fully fine-tuning these models\nincurs prohibitive computational costs, motivating the development of\nParameter-Efficient Fine-Tuning (PEFT) methods such as LoRA, which introduces\nlow-rank updates to pre-trained weights. Despite their empirical success, the\nunderlying mechanisms by which PEFT modifies model parameters remain\nunderexplored. In this work, we present a systematic investigation into the\nstructural changes of weight matrices during fully fine-tuning. Through\nsingular value decomposition (SVD), we reveal that fine-tuning predominantly\namplifies the top singular values while leaving the remainder largely intact,\nsuggesting that task-specific knowledge is injected into a low-dimensional\nsubspace. Furthermore, we find that the dominant singular vectors are\nreoriented in task-specific directions, whereas the non-dominant subspace\nremains stable. Building on these insights, we propose a novel method that\nleverages learnable rescaling of top singular directions, enabling precise\nmodulation of the most influential components without disrupting the global\nstructure. Our approach achieves consistent improvements over strong baselines\nacross multiple tasks, highlighting the efficacy of structurally informed\nfine-tuning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u5168\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u6743\u91cd\u77e9\u9635\u7684\u7ed3\u6784\u53d8\u5316\uff0c\u63ed\u793a\u4e86\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u901a\u8fc7\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u6ce8\u5165\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8eSVD\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u5168\u5fae\u8c03\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u7684\u5e95\u5c42\u673a\u5236\uff0c\u5c24\u5176\u662fLoRA\u7b49\u65b9\u6cd5\u5982\u4f55\u4fee\u6539\u6a21\u578b\u53c2\u6570\u3002", "method": "\u901a\u8fc7\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\u5206\u6790\u6743\u91cd\u77e9\u9635\u7684\u7ed3\u6784\u53d8\u5316\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5b66\u4e60\u7f29\u653e\u9876\u90e8\u5947\u5f02\u65b9\u5411\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u7cbe\u786e\u8c03\u63a7\u6700\u5177\u5f71\u54cd\u529b\u7684\u7ec4\u4ef6\u3002", "result": "\u5728\u591a\u9879\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86\u7ed3\u6784\u611f\u77e5\u5fae\u8c03\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u4e3b\u8981\u901a\u8fc7\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u6ce8\u5165\uff0c\u4e14\u9876\u90e8\u5947\u5f02\u65b9\u5411\u7684\u53ef\u5b66\u4e60\u7f29\u653e\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5fae\u8c03\u7b56\u7565\u3002", "keywords": "\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b, \u53c2\u6570\u9ad8\u6548\u5fae\u8c03, \u5947\u5f02\u503c\u5206\u89e3, \u4f4e\u7ef4\u5b50\u7a7a\u95f4, \u7ed3\u6784\u611f\u77e5"}}
{"id": "2505.23242", "pdf": "https://arxiv.org/pdf/2505.23242", "abs": "https://arxiv.org/abs/2505.23242", "authors": ["Jingxuan Wei", "Nan Xu", "Junnan Zhu", "Yanni Hao", "Gaowei Wu", "Bihui Yu", "Lei Wang"], "title": "ChartMind: A Comprehensive Benchmark for Complex Real-world Multimodal Chart Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Chart question answering (CQA) has become a critical multimodal task for\nevaluating the reasoning capabilities of vision-language models. While early\napproaches have shown promising performance by focusing on visual features or\nleveraging large-scale pre-training, most existing evaluations rely on rigid\noutput formats and objective metrics, thus ignoring the complex, real-world\ndemands of practical chart analysis. In this paper, we introduce ChartMind, a\nnew benchmark designed for complex CQA tasks in real-world settings. ChartMind\ncovers seven task categories, incorporates multilingual contexts, supports\nopen-domain textual outputs, and accommodates diverse chart formats, bridging\nthe gap between real-world applications and traditional academic benchmarks.\nFurthermore, we propose a context-aware yet model-agnostic framework, ChartLLM,\nthat focuses on extracting key contextual elements, reducing noise, and\nenhancing the reasoning accuracy of multimodal large language models. Extensive\nevaluations on ChartMind and three representative public benchmarks with 14\nmainstream multimodal models show our framework significantly outperforms the\nprevious three common CQA paradigms: instruction-following, OCR-enhanced, and\nchain-of-thought, highlighting the importance of flexible chart understanding\nfor real-world CQA. These findings suggest new directions for developing more\nrobust chart reasoning in future research.", "AI": {"tldr": "ChartMind \u662f\u4e00\u4e2a\u4e3a\u590d\u6742\u56fe\u8868\u95ee\u7b54\uff08CQA\uff09\u4efb\u52a1\u8bbe\u8ba1\u7684\u65b0\u57fa\u51c6\uff0c\u652f\u6301\u591a\u8bed\u8a00\u548c\u5f00\u653e\u5f0f\u8f93\u51fa\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6 ChartLLM\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709 CQA \u8bc4\u4f30\u8fc7\u4e8e\u4f9d\u8d56\u56fa\u5b9a\u8f93\u51fa\u683c\u5f0f\u548c\u5ba2\u89c2\u6307\u6807\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u56fe\u8868\u5206\u6790\u7684\u590d\u6742\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86 ChartMind \u57fa\u51c6\u548c ChartLLM \u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u63d0\u53d6\u5173\u952e\u4e0a\u4e0b\u6587\u5143\u7d20\u548c\u589e\u5f3a\u591a\u6a21\u6001\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728 ChartMind \u548c\u4e09\u4e2a\u516c\u5f00\u57fa\u51c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cChartLLM \u663e\u8457\u4f18\u4e8e\u6307\u4ee4\u8ddf\u968f\u3001OCR\u589e\u5f3a\u548c\u601d\u7ef4\u94fe\u8fd9\u4e09\u79cd\u5e38\u89c1 CQA \u8303\u5f0f\u3002", "conclusion": "\u7075\u6d3b\u7684\u56fe\u8868\u7406\u89e3\u5bf9\u5b9e\u9645 CQA \u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u56fe\u8868\u95ee\u7b54\uff0c\u591a\u6a21\u6001\uff0c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6a21\u578b\u65e0\u5173\uff0c\u63a8\u7406\u80fd\u529b"}}
{"id": "2505.22865", "pdf": "https://arxiv.org/pdf/2505.22865", "abs": "https://arxiv.org/abs/2505.22865", "authors": ["Susan Liang", "Dejan Markovic", "Israel D. Gebru", "Steven Krenn", "Todd Keebler", "Jacob Sandakly", "Frank Yu", "Samuel Hassel", "Chenliang Xu", "Alexander Richard"], "title": "BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural Speech Synthesis with Flow Matching Models", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "ICML 2025, 18 pages", "summary": "Binaural rendering aims to synthesize binaural audio that mimics natural\nhearing based on a mono audio and the locations of the speaker and listener.\nAlthough many methods have been proposed to solve this problem, they struggle\nwith rendering quality and streamable inference. Synthesizing high-quality\nbinaural audio that is indistinguishable from real-world recordings requires\nprecise modeling of binaural cues, room reverb, and ambient sounds.\nAdditionally, real-world applications demand streaming inference. To address\nthese challenges, we propose a flow matching based streaming binaural speech\nsynthesis framework called BinauralFlow. We consider binaural rendering to be a\ngeneration problem rather than a regression problem and design a conditional\nflow matching model to render high-quality audio. Moreover, we design a causal\nU-Net architecture that estimates the current audio frame solely based on past\ninformation to tailor generative models for streaming inference. Finally, we\nintroduce a continuous inference pipeline incorporating streaming STFT/ISTFT\noperations, a buffer bank, a midpoint solver, and an early skip schedule to\nimprove rendering continuity and speed. Quantitative and qualitative\nevaluations demonstrate the superiority of our method over SOTA approaches. A\nperceptual study further reveals that our model is nearly indistinguishable\nfrom real-world recordings, with a $42\\%$ confusion rate.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBinauralFlow\u7684\u6d41\u5f0f\u53cc\u8033\u8bed\u97f3\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6761\u4ef6\u6d41\u5339\u914d\u6a21\u578b\u548c\u56e0\u679cU-Net\u67b6\u6784\u89e3\u51b3\u4e86\u4f20\u7edf\u53cc\u8033\u6e32\u67d3\u5728\u8d28\u91cf\u548c\u5b9e\u65f6\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u4e14\u53ef\u5b9e\u65f6\u63a8\u7406\u7684\u97f3\u9891\u5408\u6210\u3002", "motivation": "\u4f20\u7edf\u53cc\u8033\u6e32\u67d3\u65b9\u6cd5\u5728\u97f3\u9891\u8d28\u91cf\u548c\u5b9e\u65f6\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u89e3\u51b3\u7cbe\u786e\u5efa\u6a21\u53cc\u8033\u7ebf\u7d22\u3001\u623f\u95f4\u6df7\u54cd\u548c\u73af\u5883\u58f0\u97f3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u6761\u4ef6\u751f\u6210\u6a21\u578b\u548c\u56e0\u679cU-Net\u67b6\u6784\uff0c\u8bbe\u8ba1\u8fde\u7eed\u63a8\u7406\u6d41\u7a0b\uff08\u6d41\u5f0fSTFT/ISTFT\u64cd\u4f5c\u3001\u7f13\u51b2\u533a\u3001\u4e2d\u70b9\u6c42\u89e3\u5668\u548c\u63d0\u524d\u8df3\u8fc7\u673a\u5236\uff09\u4ee5\u63d0\u9ad8\u6e32\u67d3\u8fde\u7eed\u6027\u548c\u901f\u5ea6\u3002", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c perceptual study \u663e\u793a\u6a21\u578b\u5408\u6210\u97f3\u9891\u4e0e\u771f\u5b9e\u5f55\u97f3\u7684\u6df7\u6dc6\u7387\u9ad8\u8fbe42%\u3002", "conclusion": "BinauralFlow\u901a\u8fc7\u751f\u6210\u5f0f\u65b9\u6cd5\u7ed3\u5408\u6d41\u5f0f\u63a8\u7406\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53cc\u8033\u6e32\u67d3\u7684\u8d28\u91cf\u548c\u5b9e\u65f6\u6027\uff0c\u63a5\u8fd1\u771f\u5b9e\u5f55\u97f3\u6548\u679c\u3002", "keywords": "\u53cc\u8033\u6e32\u67d3, \u6d41\u5f0f\u63a8\u7406, \u6761\u4ef6\u6d41\u5339\u914d, \u56e0\u679cU-Net, \u97f3\u9891\u5408\u6210"}}
{"id": "2505.23105", "pdf": "https://arxiv.org/pdf/2505.23105", "abs": "https://arxiv.org/abs/2505.23105", "authors": ["Abhishek Vijaya Kumar", "Eric Ding", "Arjun Devraj", "Darius Bunandar", "Rachee Singh"], "title": "LUMION: Fast Fault Recovery for ML Jobs Using Programmable Optical Fabrics", "categories": ["cs.LG", "cs.NI"], "comment": null, "summary": "When accelerators fail in modern ML datacenters, operators migrate the\naffected ML training or inference jobs to entirely new racks. This approach,\nwhile preserving network performance, is highly inefficient, requiring\ndatacenters to reserve full racks of idle accelerators for fault tolerance. In\nthis paper, we address this resource inefficiency by introducing LUMION, a\nnovel reconfigurable optical fabric for connecting accelerators within a\ndatacenter rack. Instead of migrating entire ML jobs, LUMION dynamically\nintegrates spare accelerators into ongoing workloads as failures occur, thereby\nmaintaining consistent performance without costly migrations. We show the\nbenefits of LUMION by building an end-to-end hardware prototype. Our\nexperiments fine-tune Llama 3.2 and show that LUMION swaps a failed GPU with a\nhealthy one and restarts the ML job within ~ 1 second of the failure. LUMION\nachieves higher inter-GPU bandwidth compared to traditional electrical racks\nafter replacing failed accelerators with spare ones, leading to nearly 2X\nimprovement in fine-tuning throughput.", "AI": {"tldr": "LUMION is a reconfigurable optical fabric that dynamically integrates spare accelerators to replace failed ones in ML datacenters, avoiding costly migrations and improving performance.", "motivation": "Traditional methods of migrating entire ML jobs to new racks during accelerator failures are inefficient and require idle racks. LUMION aims to address this by dynamically utilizing spare accelerators.", "method": "LUMION uses a reconfigurable optical fabric to swap failed GPUs with healthy ones within the same rack, enabling rapid recovery and maintaining high bandwidth.", "result": "LUMION reduces recovery time to ~1 second and nearly doubles fine-tuning throughput by improving inter-GPU bandwidth.", "conclusion": "LUMION offers an efficient solution for accelerator failure recovery in ML datacenters, enhancing performance and resource utilization.", "keywords": "LUMION, optical fabric, ML datacenters, GPU failure, fine-tuning throughput"}}
{"id": "2505.23252", "pdf": "https://arxiv.org/pdf/2505.23252", "abs": "https://arxiv.org/abs/2505.23252", "authors": ["Bing Ma", "Hai Zhuge"], "title": "Automatic Construction of Multiple Classification Dimensions for Managing Approaches in Scientific Papers", "categories": ["cs.CL"], "comment": "26 pages, 9 figures", "summary": "Approaches form the foundation for conducting scientific research. Querying\napproaches from a vast body of scientific papers is extremely time-consuming,\nand without a well-organized management framework, researchers may face\nsignificant challenges in querying and utilizing relevant approaches.\nConstructing multiple dimensions on approaches and managing them from these\ndimensions can provide an efficient solution. Firstly, this paper identifies\napproach patterns using a top-down way, refining the patterns through four\ndistinct linguistic levels: semantic level, discourse level, syntactic level,\nand lexical level. Approaches in scientific papers are extracted based on\napproach patterns. Additionally, five dimensions for categorizing approaches\nare identified using these patterns. This paper proposes using tree structure\nto represent step and measuring the similarity between different steps with a\ntree-structure-based similarity measure that focuses on syntactic-level\nsimilarities. A collection similarity measure is proposed to compute the\nsimilarity between approaches. A bottom-up clustering algorithm is proposed to\nconstruct class trees for approach components within each dimension by merging\neach approach component or class with its most similar approach component or\nclass in each iteration. The class labels generated during the clustering\nprocess indicate the common semantics of the step components within the\napproach components in each class and are used to manage the approaches within\nthe class. The class trees of the five dimensions collectively form a\nmulti-dimensional approach space. The application of approach queries on the\nmulti-dimensional approach space demonstrates that querying within this space\nensures strong relevance between user queries and results and rapidly reduces\nsearch space through a class-based query mechanism.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ef4\u5ea6\u7ba1\u7406\u79d1\u7814\u65b9\u6cd5\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u8bed\u4e49\u3001\u8bed\u7bc7\u3001\u53e5\u6cd5\u548c\u8bcd\u6c47\u56db\u4e2a\u5c42\u6b21\u8bc6\u522b\u65b9\u6cd5\u6a21\u5f0f\uff0c\u5e76\u57fa\u4e8e\u6811\u7ed3\u6784\u76f8\u4f3c\u6027\u5ea6\u91cf\u8fdb\u884c\u5206\u7c7b\u4e0e\u805a\u7c7b\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u65b9\u6cd5\u67e5\u8be2\u7a7a\u95f4\u3002", "motivation": "\u7531\u4e8e\u79d1\u5b66\u6587\u732e\u4e2d\u65b9\u6cd5\u67e5\u8be2\u8017\u65f6\u4e14\u7f3a\u4e4f\u7ec4\u7ec7\u7ba1\u7406\u6846\u67b6\uff0c\u7814\u7a76\u8005\u96be\u4ee5\u9ad8\u6548\u83b7\u53d6\u548c\u5229\u7528\u76f8\u5173\u65b9\u6cd5\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u57fa\u4e8e\u591a\u7ef4\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u81ea\u4e0a\u800c\u4e0b\u8bc6\u522b\u65b9\u6cd5\u6a21\u5f0f\uff0c\u7ed3\u5408\u56db\u79cd\u8bed\u8a00\u5c42\u6b21\u63d0\u53d6\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6811\u7ed3\u6784\u76f8\u4f3c\u6027\u548c\u805a\u7c7b\u7b97\u6cd5\u6784\u5efa\u591a\u7ef4\u5ea6\u65b9\u6cd5\u7a7a\u95f4\u3002", "result": "\u591a\u7ef4\u5ea6\u65b9\u6cd5\u7a7a\u95f4\u7684\u5e94\u7528\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u786e\u4fdd\u67e5\u8be2\u7ed3\u679c\u7684\u9ad8\u76f8\u5173\u6027\uff0c\u5e76\u901a\u8fc7\u7c7b\u5c42\u6b21\u7ed3\u6784\u5feb\u901f\u7f29\u5c0f\u641c\u7d22\u8303\u56f4\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u79d1\u7814\u65b9\u6cd5\u7684\u7ba1\u7406\u548c\u67e5\u8be2\u6548\u7387\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5de5\u5177\u3002", "keywords": "\u65b9\u6cd5\u6a21\u5f0f\u3001\u591a\u7ef4\u5ea6\u7ba1\u7406\u3001\u6811\u7ed3\u6784\u76f8\u4f3c\u6027\u3001\u805a\u7c7b\u7b97\u6cd5\u3001\u65b9\u6cd5\u67e5\u8be2"}}
{"id": "2505.23106", "pdf": "https://arxiv.org/pdf/2505.23106", "abs": "https://arxiv.org/abs/2505.23106", "authors": ["Ning Liu", "Yue Yu"], "title": "Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "Attention mechanisms have emerged as transformative tools in core AI domains\nsuch as natural language processing and computer vision. Yet, their largely\nuntapped potential for modeling intricate physical systems presents a\ncompelling frontier. Learning such systems often entails discovering operators\nthat map between functional spaces using limited instances of function pairs --\na task commonly framed as a severely ill-posed inverse PDE problem. In this\nwork, we introduce Neural Interpretable PDEs (NIPS), a novel neural operator\narchitecture that builds upon and enhances Nonlocal Attention Operators (NAO)\nin both predictive accuracy and computational efficiency. NIPS employs a linear\nattention mechanism to enable scalable learning and integrates a learnable\nkernel network that acts as a channel-independent convolution in Fourier space.\nAs a consequence, NIPS eliminates the need to explicitly compute and store\nlarge pairwise interactions, effectively amortizing the cost of handling\nspatial interactions into the Fourier transform. Empirical evaluations\ndemonstrate that NIPS consistently surpasses NAO and other baselines across\ndiverse benchmarks, heralding a substantial leap in scalable, interpretable,\nand efficient physics learning. Our code and data accompanying this paper are\navailable at https://github.com/fishmoon1234/Nonlocal-Attention-Operator.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNIPS\u7684\u65b0\u578b\u795e\u7ecf\u7b97\u5b50\u67b6\u6784\uff0c\u901a\u8fc7\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u548c\u53ef\u5b66\u4e60\u6838\u7f51\u7edc\u63d0\u5347\u7269\u7406\u7cfb\u7edf\u5efa\u6a21\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u63a2\u7d22\u6ce8\u610f\u529b\u673a\u5236\u5728\u590d\u6742\u7269\u7406\u7cfb\u7edf\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u7a7a\u95f4\u4ea4\u4e92\u65f6\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u548c\u53ef\u5b66\u4e60\u6838\u7f51\u7edc\uff0c\u5728\u5085\u91cc\u53f6\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u9010\u901a\u9053\u72ec\u7acb\u5377\u79ef\uff0c\u907f\u514d\u663e\u5f0f\u8ba1\u7b97\u548c\u5b58\u50a8\u5927\u89c4\u6a21\u4ea4\u4e92\u3002", "result": "NIPS\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8eNAO\u53ca\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7269\u7406\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002", "conclusion": "NIPS\u4e3a\u7269\u7406\u7cfb\u7edf\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65b0\u65b9\u6cd5\uff0c\u5c55\u73b0\u4e86\u6ce8\u610f\u529b\u673a\u5236\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u524d\u666f\u3002", "keywords": "\u6ce8\u610f\u529b\u673a\u5236, \u7269\u7406\u7cfb\u7edf\u5efa\u6a21, \u795e\u7ecf\u7b97\u5b50, \u5085\u91cc\u53f6\u53d8\u6362, \u53ef\u5b66\u4e60\u6838\u7f51\u7edc"}}
{"id": "2505.23276", "pdf": "https://arxiv.org/pdf/2505.23276", "abs": "https://arxiv.org/abs/2505.23276", "authors": ["Maged S. Al-Shaibani", "Moataz Ahmed"], "title": "The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved unprecedented capabilities in\ngenerating human-like text, posing subtle yet significant challenges for\ninformation integrity across critical domains, including education, social\nmedia, and academia, enabling sophisticated misinformation campaigns,\ncompromising healthcare guidance, and facilitating targeted propaganda. This\nchallenge becomes severe, particularly in under-explored and low-resource\nlanguages like Arabic. This paper presents a comprehensive investigation of\nArabic machine-generated text, examining multiple generation strategies\n(generation from the title only, content-aware generation, and text refinement)\nacross diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic,\nand social media domains. Our stylometric analysis reveals distinctive\nlinguistic patterns differentiating human-written from machine-generated Arabic\ntext across these varied contexts. Despite their human-like qualities, we\ndemonstrate that LLMs produce detectable signatures in their Arabic outputs,\nwith domain-specific characteristics that vary significantly between different\ncontexts. Based on these insights, we developed BERT-based detection models\nthat achieved exceptional performance in formal contexts (up to 99.9\\%\nF1-score) with strong precision across model architectures. Our cross-domain\nanalysis confirms generalization challenges previously reported in the\nliterature. To the best of our knowledge, this work represents the most\ncomprehensive investigation of Arabic machine-generated text to date, uniquely\ncombining multiple prompt generation methods, diverse model architectures, and\nin-depth stylometric analysis across varied textual domains, establishing a\nfoundation for developing robust, linguistically-informed detection systems\nessential for preserving information integrity in Arabic-language contexts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u963f\u62c9\u4f2f\u8bed\u673a\u5668\u751f\u6210\u6587\u672c\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u63ed\u793a\u4e86LLMs\u5728\u751f\u6210\u963f\u62c9\u4f2f\u8bed\u6587\u672c\u65f6\u7684\u53ef\u68c0\u6d4b\u7279\u5f81\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8eBERT\u7684\u9ad8\u6548\u68c0\u6d4b\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u963f\u62c9\u4f2f\u8bed\uff09\u4e2d\u751f\u6210\u6587\u672c\u65f6\u5bf9\u4fe1\u606f\u5b8c\u6574\u6027\u7684\u6f5c\u5728\u5a01\u80c1\uff0c\u5c24\u5176\u662f\u5728\u6559\u80b2\u3001\u793e\u4ea4\u5a92\u4f53\u548c\u5b66\u672f\u9886\u57df\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u751f\u6210\u7b56\u7565\uff08\u6807\u9898\u751f\u6210\u3001\u5185\u5bb9\u611f\u77e5\u751f\u6210\u3001\u6587\u672c\u4f18\u5316\uff09\u548c\u6a21\u578b\u67b6\u6784\uff08ALLaM\u3001Jais\u3001Llama\u3001GPT-4\uff09\uff0c\u7ed3\u5408\u8bed\u8a00\u5b66\u5206\u6790\u548cBERT\u68c0\u6d4b\u6a21\u578b\u3002", "result": "\u53d1\u73b0LLMs\u751f\u6210\u7684\u963f\u62c9\u4f2f\u8bed\u6587\u672c\u5177\u6709\u53ef\u68c0\u6d4b\u7684\u7279\u5f81\uff0c\u68c0\u6d4b\u6a21\u578b\u5728\u6b63\u5f0f\u8bed\u5883\u4e2dF1-score\u9ad8\u8fbe99.9%\u3002", "conclusion": "\u4e3a\u963f\u62c9\u4f2f\u8bed\u4fe1\u606f\u5b8c\u6574\u6027\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u8de8\u9886\u57df\u68c0\u6d4b\u7684\u6311\u6218\u548c\u6f5c\u529b\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u963f\u62c9\u4f2f\u8bed\u3001\u673a\u5668\u751f\u6210\u6587\u672c\u3001\u68c0\u6d4b\u6a21\u578b\u3001\u4fe1\u606f\u5b8c\u6574\u6027"}}
{"id": "2505.22878", "pdf": "https://arxiv.org/pdf/2505.22878", "abs": "https://arxiv.org/abs/2505.22878", "authors": ["Shams Tarek", "Dipayan Saha", "Sujan Kumar Saha", "Farimah Farahmandi"], "title": "BugWhisperer: Fine-Tuning LLMs for SoC Hardware Vulnerability Detection", "categories": ["cs.CR", "cs.AI"], "comment": "This paper was presented at IEEE VLSI Test Symposium (VTS) 2025", "summary": "The current landscape of system-on-chips (SoCs) security verification faces\nchallenges due to manual, labor-intensive, and inflexible methodologies. These\nissues limit the scalability and effectiveness of security protocols, making\nbug detection at the Register-Transfer Level (RTL) difficult. This paper\nproposes a new framework named BugWhisperer that utilizes a specialized,\nfine-tuned Large Language Model (LLM) to address these challenges. By enhancing\nthe LLM's hardware security knowledge and leveraging its capabilities for text\ninference and knowledge transfer, this approach automates and improves the\nadaptability and reusability of the verification process. We introduce an\nopen-source, fine-tuned LLM specifically designed for detecting security\nvulnerabilities in SoC designs. Our findings demonstrate that this tailored LLM\neffectively enhances the efficiency and flexibility of the security\nverification process. Additionally, we introduce a comprehensive hardware\nvulnerability database that supports this work and will further assist the\nresearch community in enhancing the security verification process.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBugWhisperer\u6846\u67b6\uff0c\u901a\u8fc7\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u5316SoC\u5b89\u5168\u9a8c\u8bc1\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u7075\u6d3b\u6027\uff0c\u5e76\u5f00\u6e90\u4e86\u4e13\u7528LLM\u548c\u786c\u4ef6\u6f0f\u6d1e\u6570\u636e\u5e93\u3002", "motivation": "\u5f53\u524dSoC\u5b89\u5168\u9a8c\u8bc1\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u4e14\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u96be\u4ee5\u5728RTL\u7ea7\u522b\u6709\u6548\u68c0\u6d4b\u6f0f\u6d1e\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u540d\u4e3aBugWhisperer\u7684\u6846\u67b6\uff0c\u5229\u7528\u4e13\u4e3a\u786c\u4ef6\u5b89\u5168\u5fae\u8c03\u7684LLM\uff0c\u7ed3\u5408\u6587\u672c\u63a8\u7406\u548c\u77e5\u8bc6\u8fc1\u79fb\u80fd\u529b\uff0c\u81ea\u52a8\u5316\u9a8c\u8bc1\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5b9a\u5236\u5316LLM\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u9a8c\u8bc1\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u914d\u5957\u7684\u786c\u4ef6\u6f0f\u6d1e\u6570\u636e\u5e93\u3002", "conclusion": "BugWhisperer\u901a\u8fc7LLM\u6280\u672f\u89e3\u51b3\u4e86SoC\u5b89\u5168\u9a8c\u8bc1\u7684\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\u95ee\u9898\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "keywords": "SoC\u5b89\u5168\u9a8c\u8bc1, \u5927\u8bed\u8a00\u6a21\u578b, BugWhisperer, RTL\u6f0f\u6d1e\u68c0\u6d4b, \u786c\u4ef6\u6f0f\u6d1e\u6570\u636e\u5e93"}}
{"id": "2505.23116", "pdf": "https://arxiv.org/pdf/2505.23116", "abs": "https://arxiv.org/abs/2505.23116", "authors": ["Pengfei Zhou", "Yunlong Liu", "Junli Liang", "Qi Song", "Xiangyang Li"], "title": "CrossLinear: Plug-and-Play Cross-Correlation Embedding for Time Series Forecasting with Exogenous Variables", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series forecasting with exogenous variables is a critical emerging\nparadigm that presents unique challenges in modeling dependencies between\nvariables. Traditional models often struggle to differentiate between\nendogenous and exogenous variables, leading to inefficiencies and overfitting.\nIn this paper, we introduce CrossLinear, a novel Linear-based forecasting model\nthat addresses these challenges by incorporating a plug-and-play\ncross-correlation embedding module. This lightweight module captures the\ndependencies between variables with minimal computational cost and seamlessly\nintegrates into existing neural networks. Specifically, it captures\ntime-invariant and direct variable dependencies while disregarding time-varying\nor indirect dependencies, thereby mitigating the risk of overfitting in\ndependency modeling and contributing to consistent performance improvements.\nFurthermore, CrossLinear employs patch-wise processing and a global linear head\nto effectively capture both short-term and long-term temporal dependencies,\nfurther improving its forecasting precision. Extensive experiments on 12\nreal-world datasets demonstrate that CrossLinear achieves superior performance\nin both short-term and long-term forecasting tasks. The ablation study\nunderscores the effectiveness of the cross-correlation embedding module.\nAdditionally, the generalizability of this module makes it a valuable plug-in\nfor various forecasting tasks across different domains. Codes are available at\nhttps://github.com/mumiao2000/CrossLinear.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCrossLinear\u7684\u65b0\u578b\u7ebf\u6027\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u4ea4\u53c9\u76f8\u5173\u5d4c\u5165\u6a21\u5757\uff0c\u6709\u6548\u533a\u5206\u5185\u6e90\u548c\u5916\u6e90\u53d8\u91cf\uff0c\u51cf\u5c11\u8fc7\u62df\u5408\u5e76\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u5728\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65f6\u96be\u4ee5\u533a\u5206\u5185\u6e90\u548c\u5916\u6e90\u53d8\u91cf\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u8fc7\u62df\u5408\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bba\u6587\u91c7\u7528\u4e86CrossLinear\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u4ea4\u53c9\u76f8\u5173\u5d4c\u5165\u6a21\u5757\uff0c\u7528\u4e8e\u6355\u6349\u53d8\u91cf\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u5206\u5757\u5904\u7406\u548c\u5168\u5c40\u7ebf\u6027\u5934\u6765\u4f18\u5316\u77ed\u671f\u548c\u957f\u671f\u9884\u6d4b\u3002", "result": "\u572812\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCrossLinear\u5728\u77ed\u671f\u548c\u957f\u671f\u9884\u6d4b\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u4ea4\u53c9\u76f8\u5173\u5d4c\u5165\u6a21\u5757\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684CrossLinear\u6a21\u578b\u548c\u4ea4\u53c9\u76f8\u5173\u5d4c\u5165\u6a21\u5757\u4e0d\u4ec5\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u8fd8\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u4e0d\u540c\u9886\u57df\u7684\u9884\u6d4b\u4efb\u52a1\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b, \u5916\u6e90\u53d8\u91cf, CrossLinear, \u4ea4\u53c9\u76f8\u5173\u5d4c\u5165, \u8fc7\u62df\u5408"}}
{"id": "2505.23277", "pdf": "https://arxiv.org/pdf/2505.23277", "abs": "https://arxiv.org/abs/2505.23277", "authors": ["Yong Zhang", "Yanwen Huang", "Ning Cheng", "Yang Guo", "Yun Zhu", "Yanmeng Wang", "Shaojun Wang", "Jing Xiao"], "title": "Sentinel: Attention Probing of Proxy Models for LLM Context Compression with an Understanding Perspective", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint. 17 pages including appendix", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external context, but retrieved passages are often lengthy, noisy, or\nexceed input limits. Existing compression methods typically require supervised\ntraining of dedicated compression models, increasing cost and reducing\nportability. We propose Sentinel, a lightweight sentence-level compression\nframework that reframes context filtering as an attention-based understanding\ntask. Rather than training a compression model, Sentinel probes decoder\nattention from an off-the-shelf 0.5B proxy LLM using a lightweight classifier\nto identify sentence relevance. Empirically, we find that query-context\nrelevance estimation is consistent across model scales, with 0.5B proxies\nclosely matching the behaviors of larger models. On the LongBench benchmark,\nSentinel achieves up to 5$\\times$ compression while matching the QA performance\nof 7B-scale compression systems. Our results suggest that probing native\nattention signals enables fast, effective, and question-aware context\ncompression. Code available at: https://github.com/yzhangchuck/Sentinel.", "AI": {"tldr": "Sentinel \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u53e5\u5b50\u7ea7\u522b\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u73b0\u6210\u7684\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u7801\u5668\u6ce8\u610f\u529b\u6765\u8bc6\u522b\u53e5\u5b50\u76f8\u5173\u6027\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u4fbf\u643a\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e2d\u68c0\u7d22\u5230\u7684\u7247\u6bb5\u901a\u5e38\u5197\u957f\u3001\u5608\u6742\u6216\u8d85\u51fa\u8f93\u5165\u9650\u5236\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u9700\u8981\u8bad\u7ec3\u4e13\u7528\u6a21\u578b\uff0c\u6210\u672c\u9ad8\u4e14\u4e0d\u4fbf\u4e8e\u79fb\u690d\u3002", "method": "Sentinel \u5c06\u4e0a\u4e0b\u6587\u8fc7\u6ee4\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7406\u89e3\u4efb\u52a1\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u63a2\u6d4b\u73b0\u6210\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u7801\u5668\u6ce8\u610f\u529b\u6765\u8bc6\u522b\u53e5\u5b50\u76f8\u5173\u6027\u3002", "result": "\u5728 LongBench \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSentinel \u5b9e\u73b0\u4e86 5 \u500d\u7684\u538b\u7f29\uff0c\u540c\u65f6 QA \u6027\u80fd\u4e0e 7B \u89c4\u6a21\u7684\u538b\u7f29\u7cfb\u7edf\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u63a2\u6d4b\u539f\u751f\u6ce8\u610f\u529b\u4fe1\u53f7\u53ef\u4ee5\u5b9e\u73b0\u5feb\u901f\u3001\u6709\u6548\u4e14\u95ee\u9898\u611f\u77e5\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u3002", "keywords": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210, \u538b\u7f29, \u6ce8\u610f\u529b\u673a\u5236, \u4e0a\u4e0b\u6587\u8fc7\u6ee4, \u8f7b\u91cf\u7ea7"}}
{"id": "2505.22880", "pdf": "https://arxiv.org/pdf/2505.22880", "abs": "https://arxiv.org/abs/2505.22880", "authors": ["Xiaoyang Zhan", "Shixin Zhou", "Qianqian Yang", "Yixuan Zhao", "Hao Liu", "Srinivas Chowdary Ramineni", "Kenji Shimada"], "title": "Semantic Exploration and Dense Mapping of Complex Environments using Ground Robots Equipped with LiDAR and Panoramic Camera", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "This paper presents a system for autonomous semantic exploration and dense\nsemantic target mapping of a complex unknown environment using a ground robot\nequipped with a LiDAR-panoramic camera suite. Existing approaches often\nstruggle to balance collecting high-quality observations from multiple view\nangles and avoiding unnecessary repetitive traversal. To fill this gap, we\npropose a complete system combining mapping and planning. We first redefine the\ntask as completing both geometric coverage and semantic viewpoint observation.\nWe then manage semantic and geometric viewpoints separately and propose a novel\nPriority-driven Decoupled Local Sampler to generate local viewpoint sets. This\nenables explicit multi-view semantic inspection and voxel coverage without\nunnecessary repetition. Building on this, we develop a hierarchical planner to\nensure efficient global coverage. In addition, we propose a Safe Aggressive\nExploration State Machine, which allows aggressive exploration behavior while\nensuring the robot's safety. Our system includes a plug-and-play semantic\ntarget mapping module that integrates seamlessly with state-of-the-art SLAM\nalgorithms for pointcloud-level dense semantic target mapping. We validate our\napproach through extensive experiments in both realistic simulations and\ncomplex real-world environments. Simulation results show that our planner\nachieves faster exploration and shorter travel distances while guaranteeing a\nspecified number of multi-view inspections. Real-world experiments further\nconfirm the system's effectiveness in achieving accurate dense semantic object\nmapping of unstructured environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u590d\u6742\u672a\u77e5\u73af\u5883\u7684\u81ea\u4e3b\u8bed\u4e49\u63a2\u7d22\u548c\u5bc6\u96c6\u8bed\u4e49\u76ee\u6807\u6620\u5c04\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u548c\u8bed\u4e49\u89c6\u89d2\u7684\u5206\u79bb\u7ba1\u7406\u4ee5\u53ca\u65b0\u9896\u7684\u4f18\u5148\u7ea7\u9a71\u52a8\u89e3\u8026\u5c40\u90e8\u91c7\u6837\u5668\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u63a2\u7d22\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5e73\u8861\u591a\u89c6\u89d2\u9ad8\u8d28\u91cf\u89c2\u6d4b\u548c\u907f\u514d\u91cd\u590d\u904d\u5386\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5b8c\u6210\u51e0\u4f55\u8986\u76d6\u548c\u8bed\u4e49\u89c6\u89d2\u89c2\u6d4b\u7684\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6620\u5c04\u548c\u89c4\u5212\u7684\u5b8c\u6574\u7cfb\u7edf\uff0c\u5305\u62ec\u4f18\u5148\u7ea7\u9a71\u52a8\u89e3\u8026\u5c40\u90e8\u91c7\u6837\u5668\u751f\u6210\u5c40\u90e8\u89c6\u89d2\u96c6\uff0c\u4ee5\u53ca\u5206\u5c42\u89c4\u5212\u5668\u548c\u5b89\u5168\u6fc0\u8fdb\u63a2\u7d22\u72b6\u6001\u673a\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u5feb\u901f\u63a2\u7d22\u3001\u77ed\u8def\u5f84\u548c\u7cbe\u786e\u5bc6\u96c6\u8bed\u4e49\u76ee\u6807\u6620\u5c04\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8bed\u4e49\u63a2\u7d22\u548c\u76ee\u6807\u6620\u5c04\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "keywords": "\u81ea\u4e3b\u63a2\u7d22, \u8bed\u4e49\u6620\u5c04, \u5730\u9762\u673a\u5668\u4eba, LiDAR, \u591a\u89c6\u89d2\u89c2\u6d4b"}}
{"id": "2505.23117", "pdf": "https://arxiv.org/pdf/2505.23117", "abs": "https://arxiv.org/abs/2505.23117", "authors": ["Yuatyong Chaichana", "Thanapat Trachu", "Peerat Limkonchotiwat", "Konpat Preechakul", "Tirasan Khandhawit", "Ekapol Chuangsuwanich"], "title": "Decom-Renorm-Merge: Model Merging on the Right Space Improves Multitasking", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In the era of large-scale training, model merging has evolved into a tool for\ncreating multitasking models efficiently. It enables the knowledge of models to\nbe fused, without the need for heavy computation as required in traditional\nmultitask learning. Existing merging methods often assume that entries at\nidentical positions in weight matrices serve the same function, enabling\nstraightforward entry-wise comparison and merging. However, this assumption\noverlooks the complexity of finetuned neural networks, where neurons may\ndevelop distinct feature compositions, making direct entry-wise merging\nproblematic. We present Decom-Renorm-Merge (DRM), a simple yet effective\napproach that leverages Singular Value Decomposition to decompose and\ncoordinate weight matrices into an aligned joint space, where entry-wise\nmerging becomes possible. We showcase the effectiveness of DRM across various\nsettings ranging from smaller encoder-based such as ViT and DeBERTa,\nencoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B.\nOur experimental results show that DRM outperforms several state-of-the-art\nmerging techniques across full finetuning and low-rank adaptation settings.\nMoreover, our analysis reveals renormalization as the crucial component for\ncreating a robust and even joint space for merging, significantly contributing\nto the method's performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDecom-Renorm-Merge\uff08DRM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5947\u5f02\u503c\u5206\u89e3\u5bf9\u9f50\u6743\u91cd\u77e9\u9635\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u5ffd\u7565\u5fae\u8c03\u795e\u7ecf\u7f51\u7edc\u590d\u6742\u6027\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u5047\u8bbe\u6743\u91cd\u77e9\u9635\u4e2d\u76f8\u540c\u4f4d\u7f6e\u7684\u6761\u76ee\u529f\u80fd\u76f8\u540c\uff0c\u5ffd\u7565\u4e86\u5fae\u8c03\u540e\u795e\u7ecf\u7f51\u7edc\u7684\u590d\u6742\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u534f\u8c03\u4e0d\u540c\u7279\u5f81\u7ec4\u5408\u7684\u5408\u5e76\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDRM\u65b9\u6cd5\uff0c\u5229\u7528\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\u5206\u89e3\u548c\u534f\u8c03\u6743\u91cd\u77e9\u9635\u5230\u4e00\u4e2a\u5bf9\u9f50\u7684\u8054\u5408\u7a7a\u95f4\uff0c\u4ece\u800c\u652f\u6301\u6761\u76ee\u7ea7\u5408\u5e76\u3002", "result": "DRM\u5728\u591a\u79cd\u6a21\u578b\uff08\u5982ViT\u3001DeBERTa\u3001T5\u3001Llama3.1-8B\uff09\u4e0a\u4f18\u4e8e\u73b0\u6709\u5408\u5e76\u6280\u672f\uff0c\u7279\u522b\u662f\u5728\u5b8c\u6574\u5fae\u8c03\u548c\u4f4e\u79e9\u9002\u5e94\u8bbe\u7f6e\u4e2d\u3002", "conclusion": "DRM\u901a\u8fc7\u91cd\u65b0\u5f52\u4e00\u5316\u521b\u5efa\u4e86\u4e00\u4e2a\u9c81\u68d2\u4e14\u5747\u5300\u7684\u8054\u5408\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u5e76\u6027\u80fd\u3002", "keywords": "\u6a21\u578b\u5408\u5e76, \u5947\u5f02\u503c\u5206\u89e3, \u591a\u4efb\u52a1\u5b66\u4e60, \u795e\u7ecf\u7f51\u7edc\u5fae\u8c03, DRM"}}
{"id": "2505.23291", "pdf": "https://arxiv.org/pdf/2505.23291", "abs": "https://arxiv.org/abs/2505.23291", "authors": ["Xinye Li", "Zunwen Zheng", "Qian Zhang", "Dekai Zhuang", "Jiabao Kang", "Liyan Xu", "Qingbin Liu", "Xi Chen", "Zhiying Tu", "Dianhui Chu", "Dianbo Sui"], "title": "ScEdit: Script-based Assessment of Knowledge Editing", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Knowledge Editing (KE) has gained increasing attention, yet current KE tasks\nremain relatively simple. Under current evaluation frameworks, many editing\nmethods achieve exceptionally high scores, sometimes nearing perfection.\nHowever, few studies integrate KE into real-world application scenarios (e.g.,\nrecent interest in LLM-as-agent). To support our analysis, we introduce a novel\nscript-based benchmark -- ScEdit (Script-based Knowledge Editing Benchmark) --\nwhich encompasses both counterfactual and temporal edits. We integrate\ntoken-level and text-level evaluation methods, comprehensively analyzing\nexisting KE techniques. The benchmark extends traditional fact-based\n(\"What\"-type question) evaluation to action-based (\"How\"-type question)\nevaluation. We observe that all KE methods exhibit a drop in performance on\nestablished metrics and face challenges on text-level metrics, indicating a\nchallenging task. Our benchmark is available at\nhttps://github.com/asdfo123/ScEdit.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u4e8e\u811a\u672c\u7684\u77e5\u8bc6\u7f16\u8f91\u57fa\u51c6ScEdit\uff0c\u7528\u4e8e\u8bc4\u4f30\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u53cd\u4e8b\u5b9e\u548c\u65f6\u95f4\u7f16\u8f91\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u77e5\u8bc6\u7f16\u8f91\u4efb\u52a1\u8fc7\u4e8e\u7b80\u5355\uff0c\u4e14\u5728\u771f\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\u7f3a\u4e4f\u9a8c\u8bc1\u3002", "method": "\u5f15\u5165ScEdit\u57fa\u51c6\uff0c\u6574\u5408\u4e86\u6807\u8bb0\u7ea7\u548c\u6587\u672c\u7ea7\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u7684\u4e8b\u5b9e\u578b\u8bc4\u4f30\u5230\u57fa\u4e8e\u52a8\u4f5c\u7684\u8bc4\u4f30\u3002", "result": "\u6240\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u5747\u6709\u6240\u4e0b\u964d\uff0c\u4e14\u5728\u6587\u672c\u7ea7\u6307\u6807\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "ScEdit\u57fa\u51c6\u63ed\u793a\u4e86\u77e5\u8bc6\u7f16\u8f91\u7684\u6311\u6218\u6027\uff0c\u5e76\u4e3a\u5176\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "keywords": "\u77e5\u8bc6\u7f16\u8f91, \u57fa\u51c6\u6d4b\u8bd5, ScEdit, \u53cd\u4e8b\u5b9e\u7f16\u8f91, \u65f6\u95f4\u7f16\u8f91"}}
{"id": "2505.22889", "pdf": "https://arxiv.org/pdf/2505.22889", "abs": "https://arxiv.org/abs/2505.22889", "authors": ["Hamidreza Montazeri Hedesh", "Moh Kamalul Wafi", "Milad Siami"], "title": "Local Stability and Region of Attraction Analysis for Neural Network Feedback Systems under Positivity Constraints", "categories": ["eess.SY", "cs.AI", "cs.SY", "93D09, 93D20, 93C10, 68T07", "B.1.3; G.1; I.2; I.2.3; I.2.8; I.2.1; J.2"], "comment": "Submitted to 64th IEEE Conference on Decision and Control 2025 - Rio\n  de Janeiro, Brazil", "summary": "We study the local stability of nonlinear systems in the Lur'e form with\nstatic nonlinear feedback realized by feedforward neural networks (FFNNs). By\nleveraging positivity system constraints, we employ a localized variant of the\nAizerman conjecture, which provides sufficient conditions for exponential\nstability of trajectories confined to a compact set. Using this foundation, we\ndevelop two distinct methods for estimating the Region of Attraction (ROA): (i)\na less conservative Lyapunov-based approach that constructs invariant sublevel\nsets of a quadratic function satisfying a linear matrix inequality (LMI), and\n(ii) a novel technique for computing tight local sector bounds for FFNNs via\nlayer-wise propagation of linear relaxations. These bounds are integrated into\nthe localized Aizerman framework to certify local exponential stability.\nNumerical results demonstrate substantial improvements over existing integral\nquadratic constraint-based approaches in both ROA size and scalability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5177\u6709\u9759\u6001\u975e\u7ebf\u6027\u53cd\u9988\u7684Lur'e\u578b\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u5c40\u90e8\u7a33\u5b9a\u6027\uff0c\u5229\u7528Aizerman\u731c\u60f3\u7684\u5c40\u90e8\u53d8\u4f53\u548c\u4e24\u79cd\u65b9\u6cd5\uff08Lyapunov\u65b9\u6cd5\u548c\u5c40\u90e8\u6247\u533a\u8fb9\u754c\u6280\u672f\uff09\u4f30\u8ba1\u5438\u5f15\u533a\u57df\uff08ROA\uff09\uff0c\u6570\u503c\u7ed3\u679c\u663e\u793a\u5728ROA\u5927\u5c0f\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u5177\u6709\u9759\u6001\u975e\u7ebf\u6027\u53cd\u9988\u7684Lur'e\u578b\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u5c40\u90e8\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff08FFNNs\uff09\u5b9e\u73b0\u975e\u7ebf\u6027\u53cd\u9988\u65f6\uff0c\u5982\u4f55\u6709\u6548\u4f30\u8ba1\u5438\u5f15\u533a\u57df\uff08ROA\uff09\u3002", "method": "\u8bba\u6587\u91c7\u7528\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u4e00\u662f\u57fa\u4e8eLyapunov\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u9020\u6ee1\u8db3\u7ebf\u6027\u77e9\u9635\u4e0d\u7b49\u5f0f\uff08LMI\uff09\u7684\u4e8c\u6b21\u51fd\u6570\u7684\u5b50\u6c34\u5e73\u96c6\uff1b\u4e8c\u662f\u901a\u8fc7\u9010\u5c42\u4f20\u64ad\u7ebf\u6027\u677e\u5f1b\u8ba1\u7b97FFNNs\u7684\u7d27\u5c40\u90e8\u6247\u533a\u8fb9\u754c\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u5c40\u90e8Aizerman\u6846\u67b6\u4e2d\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u79ef\u5206\u4e8c\u6b21\u7ea6\u675f\uff08IQC\uff09\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728ROA\u5927\u5c0f\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u8868\u73b0\u51fa\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "\u8bba\u6587\u7684\u7ed3\u8bba\u8868\u660e\uff0c\u63d0\u51fa\u7684\u4e24\u79cd\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f30\u8ba1\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u5438\u5f15\u533a\u57df\uff0c\u5e76\u4e14\u5728FFNNs\u5b9e\u73b0\u975e\u7ebf\u6027\u53cd\u9988\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u4e86\u5c40\u90e8\u7a33\u5b9a\u6027\u7684\u9a8c\u8bc1\u80fd\u529b\u3002", "keywords": "\u975e\u7ebf\u6027\u7cfb\u7edf\uff0cLur'e\u5f62\u5f0f\uff0c\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff08FFNNs\uff09\uff0c\u5c40\u90e8\u7a33\u5b9a\u6027\uff0cAizerman\u731c\u60f3\uff0c\u5438\u5f15\u533a\u57df\uff08ROA\uff09\uff0c\u7ebf\u6027\u77e9\u9635\u4e0d\u7b49\u5f0f\uff08LMI\uff09\uff0c\u5c40\u90e8\u6247\u533a\u8fb9\u754c"}}
{"id": "2505.23131", "pdf": "https://arxiv.org/pdf/2505.23131", "abs": "https://arxiv.org/abs/2505.23131", "authors": ["Xinyu Yao", "Daniel Bourgeois", "Abhinav Jain", "Yuxin Tang", "Jiawen Yao", "Zhimin Ding", "Arlei Silva", "Chris Jermaine"], "title": "DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous Dataflow Graphs", "categories": ["cs.LG", "cs.DC"], "comment": "32 pages, 19 figures", "summary": "We study the problem of assigning operations in a dataflow graph to devices\nto minimize execution time in a work-conserving system, with emphasis on\ncomplex machine learning workloads. Prior learning-based methods often struggle\ndue to three key limitations: (1) reliance on bulk-synchronous systems like\nTensorFlow, which under-utilize devices due to barrier synchronization; (2)\nlack of awareness of the scheduling mechanism of underlying systems when\ndesigning learning-based methods; and (3) exclusive dependence on reinforcement\nlearning, ignoring the structure of effective heuristics designed by experts.\nIn this paper, we propose \\textsc{Doppler}, a three-stage framework for\ntraining dual-policy networks consisting of 1) a $\\mathsf{SEL}$ policy for\nselecting operations and 2) a $\\mathsf{PLC}$ policy for placing chosen\noperations on devices. Our experiments show that \\textsc{Doppler} outperforms\nall baseline methods across tasks by reducing system execution time and\nadditionally demonstrates sampling efficiency by reducing per-episode training\ntime.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aDoppler\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u6570\u636e\u6d41\u56fe\u4e2d\u64cd\u4f5c\u5206\u914d\u5230\u8bbe\u5907\u7684\u8fc7\u7a0b\uff0c\u4ee5\u51cf\u5c11\u6267\u884c\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u4f9d\u8d56\u6279\u91cf\u540c\u6b65\u7cfb\u7edf\u3001\u7f3a\u4e4f\u5bf9\u5e95\u5c42\u7cfb\u7edf\u8c03\u5ea6\u673a\u5236\u7684\u4e86\u89e3\uff0c\u4ee5\u53ca\u5ffd\u7565\u4e13\u5bb6\u8bbe\u8ba1\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "method": "Doppler\u6846\u67b6\u5305\u62ec\u53cc\u7b56\u7565\u7f51\u7edc\uff1a\u4e00\u4e2a\u7528\u4e8e\u9009\u62e9\u64cd\u4f5c\uff08SEL\u7b56\u7565\uff09\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u5c06\u64cd\u4f5c\u5206\u914d\u5230\u8bbe\u5907\uff08PLC\u7b56\u7565\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cDoppler\u5728\u51cf\u5c11\u7cfb\u7edf\u6267\u884c\u65f6\u95f4\u548c\u6bcf\u8f6e\u8bad\u7ec3\u65f6\u95f4\u4e0a\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Doppler\u901a\u8fc7\u7ed3\u5408\u5b66\u4e60\u65b9\u6cd5\u548c\u4e13\u5bb6\u542f\u53d1\u5f0f\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6267\u884c\u6548\u7387\u3002", "keywords": "\u6570\u636e\u6d41\u56fe\u3001\u8bbe\u5907\u5206\u914d\u3001\u673a\u5668\u5b66\u4e60\u3001\u8c03\u5ea6\u4f18\u5316\u3001\u53cc\u7b56\u7565\u7f51\u7edc"}}
{"id": "2505.23295", "pdf": "https://arxiv.org/pdf/2505.23295", "abs": "https://arxiv.org/abs/2505.23295", "authors": ["James Xu Zhao", "Jimmy Z. J. Liu", "Bryan Hooi", "See-Kiong Ng"], "title": "How Does Response Length Affect Long-Form Factuality", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Findings. 24 pages, 10 figures, 18 tables. Code available at\n  https://github.com/XuZhao0/length-bias-factuality", "summary": "Large language models (LLMs) are widely used for long-form text generation.\nHowever, factual errors in the responses would undermine their reliability.\nDespite growing attention to LLM factuality, the effect of response length on\nfactuality remains underexplored. In this work, we systematically investigate\nthis relationship by first introducing an automatic and bi-level long-form\nfactuality evaluation framework, which achieves high agreement with human\nannotations while being cost-effective. Using this framework, we conduct\ncontrolled experiments and find that longer responses exhibit lower factual\nprecision, confirming the presence of length bias. To explain this phenomenon,\nwe empirically examine three hypotheses: error propagation, long context, and\nfacts exhaustion. Our results reveal that facts exhaustion, where the model\ngradually exhausts more reliable knowledge, is the primary cause of factual\ndegradation, rather than the other two hypotheses.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u957f\u6587\u672c\u65f6\u7684\u51c6\u786e\u6027\uff0c\u53d1\u73b0\u968f\u7740\u6587\u672c\u957f\u5ea6\u589e\u52a0\uff0c\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\u9a8c\u8bc1\u8fd9\u4e00\u73b0\u8c61\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u77e5\u8bc6\u8017\u5c3d\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u957f\u6587\u672c\u751f\u6210\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u4e8b\u5b9e\u9519\u8bef\u4f1a\u5f71\u54cd\u53ef\u9760\u6027\uff0c\u800c\u6587\u672c\u957f\u5ea6\u5bf9\u51c6\u786e\u6027\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u81ea\u52a8\u7684\u53cc\u5c42\u957f\u6587\u672c\u4e8b\u5b9e\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u9a8c\u8bc1\u957f\u5ea6\u4e0e\u51c6\u786e\u6027\u7684\u5173\u7cfb\uff0c\u68c0\u9a8c\u4e09\u79cd\u5047\u8bbe\uff08\u9519\u8bef\u4f20\u64ad\u3001\u957f\u4e0a\u4e0b\u6587\u3001\u77e5\u8bc6\u8017\u5c3d\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6587\u672c\u8d8a\u957f\uff0c\u4e8b\u5b9e\u51c6\u786e\u6027\u8d8a\u4f4e\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u77e5\u8bc6\u8017\u5c3d\uff0c\u800c\u975e\u9519\u8bef\u4f20\u64ad\u6216\u957f\u4e0a\u4e0b\u6587\u3002", "conclusion": "\u957f\u6587\u672c\u751f\u6210\u4e2d\u7684\u4e8b\u5b9e\u6027\u4e0b\u964d\u4e3b\u8981\u7531\u77e5\u8bc6\u8017\u5c3d\u5f15\u8d77\uff0c\u5efa\u8bae\u672a\u6765\u5173\u6ce8\u5982\u4f55\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u4e8b\u5b9e\u6027\u8bc4\u4f30, \u957f\u6587\u672c\u751f\u6210, \u957f\u5ea6\u504f\u5dee, \u77e5\u8bc6\u8017\u5c3d"}}
{"id": "2505.23135", "pdf": "https://arxiv.org/pdf/2505.23135", "abs": "https://arxiv.org/abs/2505.23135", "authors": ["Zhe Ye", "Zhengxu Yan", "Jingxuan He", "Timothe Kasriel", "Kaiyu Yang", "Dawn Song"], "title": "VERINA: Benchmarking Verifiable Code Generation", "categories": ["cs.LG", "cs.AI", "cs.LO", "cs.PL", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) are increasingly integrated in software\ndevelopment, but ensuring correctness in LLM-generated code remains challenging\nand often requires costly manual review. Verifiable code generation -- jointly\ngenerating code, specifications, and proofs of code-specification alignment --\noffers a promising path to address this limitation and further unleash LLMs'\nbenefits in coding. Yet, there exists a significant gap in evaluation: current\nbenchmarks often lack support for end-to-end verifiable code generation. In\nthis paper, we introduce Verina (Verifiable Code Generation Arena), a\nhigh-quality benchmark enabling a comprehensive and modular evaluation of code,\nspecification, and proof generation as well as their compositions. Verina\nconsists of 189 manually curated coding tasks in Lean, with detailed problem\ndescriptions, reference implementations, formal specifications, and extensive\ntest suites. Our extensive evaluation of state-of-the-art LLMs reveals\nsignificant challenges in verifiable code generation, especially in proof\ngeneration, underscoring the need for improving LLM-based theorem provers in\nverification domains. The best model, OpenAI o4-mini, generates only 61.4%\ncorrect code, 51.0% sound and complete specifications, and 3.6% successful\nproofs, with one trial per task. We hope Verina will catalyze progress in\nverifiable code generation by providing a rigorous and comprehensive benchmark.\nWe release our dataset on https://huggingface.co/datasets/sunblaze-ucb/verina\nand our evaluation code on https://github.com/sunblaze-ucb/verina.", "AI": {"tldr": "Verina\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u53ef\u9a8c\u8bc1\u4ee3\u7801\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u57fa\u51c6\uff0c\u5305\u542b189\u4e2a\u624b\u52a8\u6574\u7406\u7684Lean\u7f16\u7a0b\u4efb\u52a1\u3002\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u9a8c\u8bc1\u4ee3\u7801\u751f\u6210\uff08\u5c24\u5176\u662f\u8bc1\u660e\u751f\u6210\uff09\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u4f73\u6a21\u578b\uff08OpenAI o4-mini\uff09\u7684\u4ee3\u7801\u6b63\u786e\u7387\u4ec5\u4e3a61.4%\uff0c\u8bc1\u660e\u6210\u529f\u7387\u4ec5\u67093.6%\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4ee3\u7801\u65f6\u7f3a\u4e4f\u6b63\u786e\u6027\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u53ef\u9a8c\u8bc1\u4ee3\u7801\u751f\u6210\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86Verina\u57fa\u51c6\uff0c\u5305\u542b189\u4e2aLean\u7f16\u7a0b\u4efb\u52a1\uff0c\u6db5\u76d6\u4ee3\u7801\u3001\u89c4\u8303\u548c\u8bc1\u660e\u751f\u6210\u7684\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u9a8c\u8bc1\u4ee3\u7801\u751f\u6210\uff08\u5c24\u5176\u662f\u8bc1\u660e\u751f\u6210\uff09\u65b9\u9762\u8868\u73b0\u8f83\u5dee\uff0c\u6700\u4f73\u6a21\u578b\u7684\u8bc1\u660e\u6210\u529f\u7387\u4ec53.6%\u3002", "conclusion": "Verina\u4e3a\u53ef\u9a8c\u8bc1\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u4e25\u683c\u4e14\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u53ef\u9a8c\u8bc1\u4ee3\u7801\u751f\u6210,Lean,\u57fa\u51c6,\u5b9a\u7406\u8bc1\u660e"}}
{"id": "2505.23297", "pdf": "https://arxiv.org/pdf/2505.23297", "abs": "https://arxiv.org/abs/2505.23297", "authors": ["Daryna Dementieva", "Nikolay Babakov", "Alexander Fraser"], "title": "EmoBench-UA: A Benchmark Dataset for Emotion Detection in Ukrainian", "categories": ["cs.CL"], "comment": null, "summary": "While Ukrainian NLP has seen progress in many texts processing tasks, emotion\nclassification remains an underexplored area with no publicly available\nbenchmark to date. In this work, we introduce EmoBench-UA, the first annotated\ndataset for emotion detection in Ukrainian texts. Our annotation schema is\nadapted from the previous English-centric works on emotion detection (Mohammad\net al., 2018; Mohammad, 2022) guidelines. The dataset was created through\ncrowdsourcing using the Toloka.ai platform ensuring high-quality of the\nannotation process. Then, we evaluate a range of approaches on the collected\ndataset, starting from linguistic-based baselines, synthetic data translated\nfrom English, to large language models (LLMs). Our findings highlight the\nchallenges of emotion classification in non-mainstream languages like Ukrainian\nand emphasize the need for further development of Ukrainian-specific models and\ntraining resources.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86EmoBench-UA\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u4e4c\u514b\u5170\u8bed\u6587\u672c\u60c5\u611f\u68c0\u6d4b\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u65b9\u6cd5\u8bc4\u4f30\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u4e4c\u514b\u5170\u8bedNLP\u5728\u60c5\u611f\u5206\u7c7b\u9886\u57df\u7f3a\u4e4f\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7814\u7a76\u8005\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u5e76\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u8d44\u6e90\u3002", "method": "\u901a\u8fc7\u4f17\u5305\u5e73\u53f0Toloka.ai\u521b\u5efa\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u5305\u62ec\u8bed\u8a00\u5b66\u57fa\u7ebf\u3001\u82f1\u8bed\u7ffb\u8bd1\u5408\u6210\u6570\u636e\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5185\u7684\u591a\u79cd\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u4e4c\u514b\u5170\u8bed\u7b49\u975e\u4e3b\u6d41\u8bed\u8a00\u5728\u60c5\u611f\u5206\u7c7b\u4e0a\u7684\u6311\u6218\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f00\u53d1\u4e4c\u514b\u5170\u8bed\u4e13\u7528\u6a21\u578b\u548c\u8bad\u7ec3\u8d44\u6e90\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "EmoBench-UA\u4e3a\u4e4c\u514b\u5170\u8bed\u60c5\u611f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9996\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u547c\u5401\u66f4\u591a\u9488\u5bf9\u4e4c\u514b\u5170\u8bed\u7684NLP\u7814\u7a76\u3002", "keywords": "\u4e4c\u514b\u5170\u8bedNLP, \u60c5\u611f\u68c0\u6d4b, \u6570\u636e\u96c6, \u4f17\u5305\u6807\u6ce8, \u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.22906", "pdf": "https://arxiv.org/pdf/2505.22906", "abs": "https://arxiv.org/abs/2505.22906", "authors": ["Emmanuel Anaya Gonz\u00e1lez", "Raven Rothkopf", "Sorin Lerner", "Nadia Polikarpova"], "title": "HiLDe: Intentional Code Generation via Human-in-the-Loop Decoding", "categories": ["cs.HC", "cs.AI", "cs.PL"], "comment": "10 pages, 6 figures", "summary": "While AI programming tools hold the promise of increasing programmers'\ncapabilities and productivity to a remarkable degree, they often exclude users\nfrom essential decision-making processes, causing many to effectively \"turn off\ntheir brains\" and over-rely on solutions provided by these systems. These\nbehaviors can have severe consequences in critical domains, like software\nsecurity. We propose Human-in-the-loop Decoding, a novel interaction technique\nthat allows users to observe and directly influence LLM decisions during code\ngeneration, in order to align the model's output with their personal\nrequirements. We implement this technique in HiLDe, a code completion assistant\nthat highlights critical decisions made by the LLM and provides local\nalternatives for the user to explore. In a within-subjects study (N=18) on\nsecurity-related tasks, we found that HiLDe led participants to generate\nsignificantly fewer vulnerabilities and better align code generation with their\ngoals compared to a traditional code completion assistant.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHuman-in-the-loop Decoding\u7684\u4ea4\u4e92\u6280\u672f\uff0c\u901a\u8fc7\u8ba9\u7528\u6237\u89c2\u5bdf\u548c\u5f71\u54cdLLM\u5728\u4ee3\u7801\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u51b3\u7b56\uff0c\u4ee5\u51cf\u5c11\u8fc7\u5ea6\u4f9d\u8d56AI\u5de5\u5177\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8f6f\u4ef6\u5b89\u5168\u9886\u57df\u3002", "motivation": "AI\u7f16\u7a0b\u5de5\u5177\u867d\u80fd\u63d0\u5347\u6548\u7387\u548c\u80fd\u529b\uff0c\u4f46\u5e38\u5bfc\u81f4\u7528\u6237\u8fc7\u5ea6\u4f9d\u8d56\u800c\u5ffd\u89c6\u5173\u952e\u51b3\u7b56\uff0c\u5c24\u5176\u5728\u5b89\u5168\u9886\u57df\u53ef\u80fd\u5e26\u6765\u4e25\u91cd\u540e\u679c\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u65b0\u4ea4\u4e92\u65b9\u5f0f\u4ee5\u589e\u5f3a\u7528\u6237\u5bf9LLM\u8f93\u51fa\u7684\u63a7\u5236\u3002", "method": "\u63d0\u51fa\u4e86Human-in-the-loop Decoding\u6280\u672f\uff0c\u5e76\u5728HiLDe\u5de5\u5177\u4e2d\u5b9e\u73b0\u3002\u8be5\u5de5\u5177\u9ad8\u4eaeLLM\u7684\u5173\u952e\u51b3\u7b56\u5e76\u63d0\u4f9b\u672c\u5730\u66ff\u4ee3\u65b9\u6848\u4f9b\u7528\u6237\u9009\u62e9\u3002", "result": "\u572818\u4eba\u53c2\u4e0e\u7684\u5b9e\u9a8c\u4e2d\u53d1\u73b0\uff0c\u4e0e\u4f20\u7edf\u4ee3\u7801\u8865\u5168\u5de5\u5177\u76f8\u6bd4\uff0cHiLDe\u663e\u8457\u51cf\u5c11\u4e86\u751f\u6210\u4ee3\u7801\u4e2d\u7684\u6f0f\u6d1e\uff0c\u5e76\u66f4\u597d\u5730\u6ee1\u8db3\u4e86\u7528\u6237\u76ee\u6807\u3002", "conclusion": "Human-in-the-loop Decoding\u6280\u672f\u6709\u6548\u63d0\u5347\u4e86\u7528\u6237\u5bf9AI\u751f\u6210\u4ee3\u7801\u7684\u63a7\u5236\uff0c\u51cf\u5c11\u4e86\u5b89\u5168\u9690\u60a3\u3002", "keywords": "AI\u7f16\u7a0b\u5de5\u5177, Human-in-the-loop, \u4ee3\u7801\u751f\u6210, \u8f6f\u4ef6\u5b89\u5168, LLM\u4ea4\u4e92"}}
{"id": "2505.23150", "pdf": "https://arxiv.org/pdf/2505.23150", "abs": "https://arxiv.org/abs/2505.23150", "authors": ["Michal Nauman", "Marek Cygan", "Carmelo Sferrazza", "Aviral Kumar", "Pieter Abbeel"], "title": "Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners", "categories": ["cs.LG"], "comment": "preprint", "summary": "Recent advances in language modeling and vision stem from training large\nmodels on diverse, multi-task data. This paradigm has had limited impact in\nvalue-based reinforcement learning (RL), where improvements are often driven by\nsmall models trained in a single-task context. This is because in multi-task RL\nsparse rewards and gradient conflicts make optimization of temporal difference\nbrittle. Practical workflows for generalist policies therefore avoid online\ntraining, instead cloning expert trajectories or distilling collections of\nsingle-task policies into one agent. In this work, we show that the use of\nhigh-capacity value models trained via cross-entropy and conditioned on\nlearnable task embeddings addresses the problem of task interference in online\nRL, allowing for robust and scalable multi-task training. We test our approach\non 7 multi-task benchmarks with over 280 unique tasks, spanning high\ndegree-of-freedom humanoid control and discrete vision-based RL. We find that,\ndespite its simplicity, the proposed approach leads to state-of-the-art single\nand multi-task performance, as well as sample-efficient transfer to new tasks.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8e\u4ea4\u53c9\u71b5\u7684\u9ad8\u5bb9\u91cf\u4ef7\u503c\u6a21\u578b\u548c\u53ef\u5b66\u4e60\u4efb\u52a1\u5d4c\u5165\uff0c\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4efb\u52a1\u5e72\u6270\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u5728\u7ebf\u8bad\u7ec3\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u56e0\u7a00\u758f\u5956\u52b1\u548c\u68af\u5ea6\u51b2\u7a81\u5bfc\u81f4\u4f18\u5316\u56f0\u96be\uff0c\u9650\u5236\u4e86\u901a\u7528\u7b56\u7565\u7684\u5728\u7ebf\u8bad\u7ec3\uff0c\u800c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9ad8\u5bb9\u91cf\u4ef7\u503c\u6a21\u578b\uff0c\u901a\u8fc7\u4ea4\u53c9\u71b5\u8bad\u7ec3\u5e76\u7ed3\u5408\u53ef\u5b66\u4e60\u4efb\u52a1\u5d4c\u5165\uff0c\u4ee5\u51cf\u5c11\u4efb\u52a1\u5e72\u6270\u3002", "result": "\u57287\u4e2a\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\uff08\u542b280\u591a\u4e2a\u4efb\u52a1\uff09\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f18\u7684\u5355\u4efb\u52a1\u548c\u591a\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u9ad8\u6548\u7684\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u4e3a\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4efb\u52a1\u5e72\u6270\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60,\u591a\u4efb\u52a1\u5b66\u4e60,\u4ef7\u503c\u6a21\u578b,\u4ea4\u53c9\u71b5,\u4efb\u52a1\u5d4c\u5165"}}
{"id": "2505.23299", "pdf": "https://arxiv.org/pdf/2505.23299", "abs": "https://arxiv.org/abs/2505.23299", "authors": ["Julia Belikova", "Konstantin Polev", "Rauf Parchiev", "Dmitry Simakov"], "title": "Data-efficient Meta-models for Evaluation of Context-based Questions and Answers in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems\nare increasingly deployed in industry applications, yet their reliability\nremains hampered by challenges in detecting hallucinations. While supervised\nstate-of-the-art (SOTA) methods that leverage LLM hidden states -- such as\nactivation tracing and representation analysis -- show promise, their\ndependence on extensively annotated datasets limits scalability in real-world\napplications. This paper addresses the critical bottleneck of data annotation\nby investigating the feasibility of reducing training data requirements for two\nSOTA hallucination detection frameworks: Lookback Lens, which analyzes\nattention head dynamics, and probing-based approaches, which decode internal\nmodel representations. We propose a methodology combining efficient\nclassification algorithms with dimensionality reduction techniques to minimize\nsample size demands while maintaining competitive performance. Evaluations on\nstandardized question-answering RAG benchmarks show that our approach achieves\nperformance comparable to strong proprietary LLM-based baselines with only 250\ntraining samples. These results highlight the potential of lightweight,\ndata-efficient paradigms for industrial deployment, particularly in\nannotation-constrained scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7ed3\u5408\u9ad8\u6548\u5206\u7c7b\u7b97\u6cd5\u4e0e\u964d\u7ef4\u6280\u672f\u7684\u65b9\u6cd5\uff0c\u51cf\u5c11\u5e7b\u89c9\u68c0\u6d4b\u8bad\u7ec3\u6570\u636e\u9700\u6c42\uff0c\u4ec5\u9700250\u4e2a\u6837\u672c\u5373\u8fbe\u5230\u4e0e\u73b0\u6709\u57fa\u51c6\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e9f\u9700\u89e3\u51b3\u6570\u636e\u6807\u6ce8\u7684\u74f6\u9888\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u9ad8\u6548\u5206\u7c7b\u7b97\u6cd5\u548c\u964d\u7ef4\u6280\u672f\uff0c\u4f18\u5316Lookback Lens\u548cprobing-based\u4e24\u79cdSOTA\u6846\u67b6\u7684\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u3002", "result": "\u5728\u6807\u51c6\u95ee\u7b54RAG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u7528250\u4e2a\u8bad\u7ec3\u6837\u672c\u5373\u5b9e\u73b0\u4e86\u4e0e\u5f3a\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u3001\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u5728\u5de5\u4e1a\u90e8\u7f72\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u6807\u6ce8\u6570\u636e\u53d7\u9650\u7684\u573a\u666f\u4e0b\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u68c0\u7d22\u589e\u5f3a\u751f\u6210, \u5e7b\u89c9\u68c0\u6d4b, \u6570\u636e\u9ad8\u6548, \u964d\u7ef4\u6280\u672f"}}
{"id": "2505.22909", "pdf": "https://arxiv.org/pdf/2505.22909", "abs": "https://arxiv.org/abs/2505.22909", "authors": ["Cristian Chica", "Yinglong Guo", "Gilad Lerman"], "title": "Learning to Charge More: A Theoretical Study of Collusion by Q-Learning Agents", "categories": ["econ.GN", "cs.AI", "cs.GT", "q-fin.EC"], "comment": null, "summary": "There is growing experimental evidence that $Q$-learning agents may learn to\ncharge supracompetitive prices. We provide the first theoretical explanation\nfor this behavior in infinite repeated games. Firms update their pricing\npolicies based solely on observed profits, without computing equilibrium\nstrategies. We show that when the game admits both a one-stage Nash equilibrium\nprice and a collusive-enabling price, and when the $Q$-function satisfies\ncertain inequalities at the end of experimentation, firms learn to consistently\ncharge supracompetitive prices. We introduce a new class of one-memory subgame\nperfect equilibria (SPEs) and provide conditions under which learned behavior\nis supported by naive collusion, grim trigger policies, or increasing\nstrategies. Naive collusion does not constitute an SPE unless the\ncollusive-enabling price is a one-stage Nash equilibrium, whereas grim trigger\npolicies can.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u65e0\u9650\u91cd\u590d\u535a\u5f08\u4e2d\uff0cQ\u5b66\u4e60\u667a\u80fd\u4f53\u53ef\u80fd\u5b66\u4f1a\u6536\u53d6\u8d85\u9ad8\u7ade\u4e89\u4ef7\u683c\uff0c\u5176\u884c\u4e3a\u53d7\u4e00\u7c7b\u65b0\u7684\u5355\u8bb0\u5fc6\u5b50\u535a\u5f08\u5b8c\u7f8e\u5747\u8861\u652f\u6301\u3002", "motivation": "\u89e3\u91caQ\u5b66\u4e60\u667a\u80fd\u4f53\u5728\u65e0\u9650\u91cd\u590d\u535a\u5f08\u4e2d\u5982\u4f55\u5b66\u4f1a\u6536\u53d6\u8d85\u9ad8\u7ade\u4e89\u4ef7\u683c\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5206\u6790Q\u51fd\u6570\u5728\u5b9e\u9a8c\u7ed3\u675f\u65f6\u7684\u7279\u5b9a\u4e0d\u7b49\u5f0f\u6761\u4ef6\uff0c\u4ee5\u53ca\u5f15\u5165\u5355\u8bb0\u5fc6\u5b50\u535a\u5f08\u5b8c\u7f8e\u5747\u8861\u7c7b\u3002", "result": "\u667a\u80fd\u4f53\u5728\u6ee1\u8db3\u6761\u4ef6\u4e0b\u5b66\u4f1a\u6301\u7eed\u6536\u53d6\u8d85\u9ad8\u7ade\u4e89\u4ef7\u683c\uff0c\u884c\u4e3a\u53d7\u5929\u771f\u5408\u8c0b\u3001\u6b8b\u9177\u89e6\u53d1\u7b56\u7565\u6216\u9012\u589e\u7b56\u7565\u652f\u6301\u3002", "conclusion": "\u5929\u771f\u5408\u8c0b\u4ec5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u6784\u6210\u5b50\u535a\u5f08\u5b8c\u7f8e\u5747\u8861\uff0c\u800c\u6b8b\u9177\u89e6\u53d1\u7b56\u7565\u53ef\u4ee5\u3002", "keywords": "Q\u5b66\u4e60, \u65e0\u9650\u91cd\u590d\u535a\u5f08, \u8d85\u9ad8\u7ade\u4e89\u4ef7\u683c, \u5b50\u535a\u5f08\u5b8c\u7f8e\u5747\u8861"}}
{"id": "2505.23165", "pdf": "https://arxiv.org/pdf/2505.23165", "abs": "https://arxiv.org/abs/2505.23165", "authors": ["Le Yang", "Vincent Y. F. Tan", "Wang Chi Cheung"], "title": "Best Arm Identification with Possibly Biased Offline Data", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": "Accepted to UAI 2025", "summary": "We study the best arm identification (BAI) problem with potentially biased\noffline data in the fixed confidence setting, which commonly arises in\nreal-world scenarios such as clinical trials. We prove an impossibility result\nfor adaptive algorithms without prior knowledge of the bias bound between\nonline and offline distributions. To address this, we propose the LUCB-H\nalgorithm, which introduces adaptive confidence bounds by incorporating an\nauxiliary bias correction to balance offline and online data within the LUCB\nframework. Theoretical analysis shows that LUCB-H matches the sample complexity\nof standard LUCB when offline data is misleading and significantly outperforms\nit when offline data is helpful. We also derive an instance-dependent lower\nbound that matches the upper bound of LUCB-H in certain scenarios. Numerical\nexperiments further demonstrate the robustness and adaptability of LUCB-H in\neffectively incorporating offline data.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5e26\u6709\u6f5c\u5728\u504f\u89c1\u7684\u79bb\u7ebf\u6570\u636e\u5728\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u8bbe\u7f6e\u4e0b\u7684\u6700\u4f73\u81c2\u8bc6\u522b\uff08BAI\uff09\u95ee\u9898\uff0c\u5e76\u63d0\u51faLUCB-H\u7b97\u6cd5\u6765\u5e73\u8861\u79bb\u7ebf\u548c\u5728\u7ebf\u6570\u636e\u3002\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u4f18\u8d8a\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u5982\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u5b58\u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u6570\u636e\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u81ea\u9002\u5e94\u7b97\u6cd5\u5728\u65e0\u504f\u89c1\u8fb9\u754c\u5148\u9a8c\u77e5\u8bc6\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faLUCB-H\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u504f\u89c1\u6821\u6b63\u548c\u81ea\u9002\u5e94\u7f6e\u4fe1\u8fb9\u754c\uff0c\u5728LUCB\u6846\u67b6\u5185\u5e73\u8861\u79bb\u7ebf\u548c\u5728\u7ebf\u6570\u636e\u3002", "result": "\u7406\u8bba\u8bc1\u660eLUCB-H\u5728\u79bb\u7ebf\u6570\u636e\u8bef\u5bfc\u65f6\u4e0e\u6807\u51c6LUCB\u6837\u672c\u590d\u6742\u5ea6\u76f8\u5f53\uff0c\u79bb\u7ebf\u6570\u636e\u6709\u76ca\u65f6\u663e\u8457\u4f18\u4e8e\u6807\u51c6LUCB\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "LUCB-H\u7b97\u6cd5\u80fd\u6709\u6548\u7ed3\u5408\u79bb\u7ebf\u6570\u636e\uff0c\u9002\u5e94\u4e0d\u540c\u573a\u666f\uff0c\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u3002", "keywords": "\u6700\u4f73\u81c2\u8bc6\u522b\u3001\u79bb\u7ebf\u6570\u636e\u3001\u81ea\u9002\u5e94\u7b97\u6cd5\u3001LUCB-H\u3001\u6837\u672c\u590d\u6742\u5ea6"}}
{"id": "2505.23304", "pdf": "https://arxiv.org/pdf/2505.23304", "abs": "https://arxiv.org/abs/2505.23304", "authors": ["Yi Luo", "Qiwen Wang", "Junqi Yang", "Luyao Tang", "Zhenghao Lin", "Zhenzhe Ying", "Weiqiang Wang", "Chen Lin"], "title": "Generalized Category Discovery in Event-Centric Contexts: Latent Pattern Mining with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Generalized Category Discovery (GCD) aims to classify both known and novel\ncategories using partially labeled data that contains only known classes.\nDespite achieving strong performance on existing benchmarks, current textual\nGCD methods lack sufficient validation in realistic settings. We introduce\nEvent-Centric GCD (EC-GCD), characterized by long, complex narratives and\nhighly imbalanced class distributions, posing two main challenges: (1)\ndivergent clustering versus classification groupings caused by subjective\ncriteria, and (2) Unfair alignment for minority classes. To tackle these, we\npropose PaMA, a framework leveraging LLMs to extract and refine event patterns\nfor improved cluster-class alignment. Additionally, a ranking-filtering-mining\npipeline ensures balanced representation of prototypes across imbalanced\ncategories. Evaluations on two EC-GCD benchmarks, including a newly constructed\nScam Report dataset, demonstrate that PaMA outperforms prior methods with up to\n12.58% H-score gains, while maintaining strong generalization on base GCD\ndatasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Event-Centric GCD (EC-GCD)\u95ee\u9898\uff0c\u9488\u5bf9\u957f\u6587\u672c\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6311\u6218\uff0c\u63d0\u51faPaMA\u6846\u67b6\uff0c\u5229\u7528LLMs\u63d0\u53d6\u4e8b\u4ef6\u6a21\u5f0f\u5e76\u901a\u8fc7\u6392\u540d-\u8fc7\u6ee4-\u6316\u6398\u6d41\u7a0b\u4f18\u5316\u539f\u578b\u8868\u793a\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u6587\u672cGCD\u65b9\u6cd5\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u957f\u6587\u672c\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faPaMA\u6846\u67b6\uff0c\u5229\u7528LLMs\u63d0\u53d6\u548c\u4f18\u5316\u4e8b\u4ef6\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u6392\u540d-\u8fc7\u6ee4-\u6316\u6398\u6d41\u7a0b\u5e73\u8861\u539f\u578b\u8868\u793a\u3002", "result": "\u5728EC-GCD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPaMA\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u63d0\u5347\u9ad8\u8fbe12.58%\u7684H-score\uff0c\u540c\u65f6\u5728\u57fa\u7840GCD\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PaMA\u6210\u529f\u89e3\u51b3\u4e86EC-GCD\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u5904\u7406\u957f\u6587\u672c\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "GCD, EC-GCD, LLMs, \u4e8b\u4ef6\u6a21\u5f0f, \u7c7b\u522b\u4e0d\u5e73\u8861"}}
{"id": "2505.23173", "pdf": "https://arxiv.org/pdf/2505.23173", "abs": "https://arxiv.org/abs/2505.23173", "authors": ["Shohei Enomoto"], "title": "Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Deep learning models often struggle to maintain performance when deployed on\ndata distributions different from their training data, particularly in\nreal-world applications where environmental conditions frequently change. While\nMulti-source Domain Generalization (MDG) has shown promise in addressing this\nchallenge by leveraging multiple source domains during training, its practical\napplication is limited by the significant costs and difficulties associated\nwith creating multi-domain datasets. To address this limitation, we propose\nPseudo Multi-source Domain Generalization (PMDG), a novel framework that\nenables the application of sophisticated MDG algorithms in more practical\nSingle-source Domain Generalization (SDG) settings. PMDG generates multiple\npseudo-domains from a single source domain through style transfer and data\naugmentation techniques, creating a synthetic multi-domain dataset that can be\nused with existing MDG algorithms. Through extensive experiments with\nPseudoDomainBed, our modified version of the DomainBed benchmark, we analyze\nthe effectiveness of PMDG across multiple datasets and architectures. Our\nanalysis reveals several key findings, including a positive correlation between\nMDG and PMDG performance and the potential of pseudo-domains to match or exceed\nactual multi-domain performance with sufficient data. These comprehensive\nempirical results provide valuable insights for future research in domain\ngeneralization. Our code is available at\nhttps://github.com/s-enmt/PseudoDomainBed.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPMDG\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u98ce\u683c\u8fc1\u79fb\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\u4ece\u5355\u4e00\u6e90\u57df\u751f\u6210\u591a\u4e2a\u4f2a\u57df\uff0c\u4f7fMDG\u7b97\u6cd5\u80fd\u5728\u66f4\u5b9e\u9645\u7684SDG\u8bbe\u7f6e\u4e2d\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6e90\u57df\u6cdb\u5316\u65b9\u6cd5\u9700\u8981\u591a\u57df\u6570\u636e\u96c6\uff0c\u4f46\u73b0\u5b9e\u4e2d\u6784\u5efa\u8fd9\u6837\u7684\u6570\u636e\u96c6\u6210\u672c\u9ad8\u4e14\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86PMDG\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5355\u4e00\u6e90\u57df\u4e0b\u7684\u57df\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u91c7\u7528\u98ce\u683c\u8fc1\u79fb\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\u4ece\u5355\u4e00\u6e90\u57df\u751f\u6210\u591a\u4e2a\u4f2a\u57df\uff0c\u6784\u6210\u5408\u6210\u591a\u57df\u6570\u636e\u96c6\uff0c\u4ece\u800c\u5e94\u7528\u73b0\u6709\u7684MDG\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPMDG\u6027\u80fd\u4e0eMDG\u6b63\u76f8\u5173\uff0c\u4e14\u4f2a\u57df\u5728\u6570\u636e\u5145\u8db3\u65f6\u53ef\u5339\u654c\u6216\u8d85\u8d8a\u5b9e\u9645\u591a\u57df\u6027\u80fd\u3002", "conclusion": "PMDG\u4e3a\u5355\u4e00\u6e90\u57df\u4e0b\u7684\u57df\u6cdb\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u57df\u6cdb\u5316, \u5355\u6e90\u57df, \u4f2a\u57df, \u98ce\u683c\u8fc1\u79fb, \u6570\u636e\u589e\u5f3a"}}
{"id": "2505.23315", "pdf": "https://arxiv.org/pdf/2505.23315", "abs": "https://arxiv.org/abs/2505.23315", "authors": ["Abhirup Chakravarty", "Mark Brenchley", "Trevor Breakspear", "Ian Lewin", "Yan Huang"], "title": "Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This is the preprint version of our paper accepted to ACL 2025\n  (Industry Track). The DOI will be added once available", "summary": "A key ethical challenge in Automated Essay Scoring (AES) is ensuring that\nscores are only released when they meet high reliability standards. Confidence\nmodelling addresses this by assigning a reliability estimate measure, in the\nform of a confidence score, to each automated score. In this study, we frame\nconfidence estimation as a classification task: predicting whether an\nAES-generated score correctly places a candidate in the appropriate CEFR level.\nWhile this is a binary decision, we leverage the inherent granularity of the\nscoring domain in two ways. First, we reformulate the task as an n-ary\nclassification problem using score binning. Second, we introduce a set of novel\nKernel Weighted Ordinal Categorical Cross Entropy (KWOCCE) loss functions that\nincorporate the ordinal structure of CEFR labels. Our best-performing model\nachieves an F1 score of 0.97, and enables the system to release 47% of scores\nwith 100% CEFR agreement and 99% with at least 95% CEFR agreement -compared to\napproximately 92% (approx.) CEFR agreement from the standalone AES model where\nwe release all AM predicted scores.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\uff08AES\uff09\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u5efa\u6a21\uff0c\u901a\u8fc7\u5206\u7c7b\u4efb\u52a1\u9884\u6d4b\u8bc4\u5206\u662f\u5426\u7b26\u5408CEFR\u7b49\u7ea7\uff0c\u4f7f\u7528\u5f97\u5206\u5206\u7bb1\u548c\u65b0\u578b\u635f\u5931\u51fd\u6570\u63d0\u5347\u6027\u80fd\uff0c\u6a21\u578bF1\u5206\u6570\u8fbe0.97\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc4\u5206\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3AES\u8bc4\u5206\u53ef\u9760\u6027\u95ee\u9898\uff0c\u786e\u4fdd\u8bc4\u5206\u4ec5\u5728\u8fbe\u5230\u9ad8\u6807\u51c6\u65f6\u53d1\u5e03\uff0c\u63d0\u5347\u81ea\u52a8\u8bc4\u5206\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u5c06\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u89c6\u4e3a\u5206\u7c7b\u4efb\u52a1\uff0c\u91c7\u7528\u5f97\u5206\u5206\u7bb1\u5c06\u5176\u8f6c\u5316\u4e3a\u591a\u5206\u7c7b\u95ee\u9898\uff0c\u5e76\u5f15\u5165KWOCCE\u635f\u5931\u51fd\u6570\u4ee5\u5229\u7528CEFR\u6807\u7b7e\u7684\u987a\u5e8f\u7ed3\u6784\u3002", "result": "\u6700\u4f73\u6a21\u578bF1\u5206\u6570\u4e3a0.97\uff0c47%\u7684\u8bc4\u5206\u8fbe\u5230100% CEFR\u4e00\u81f4\u6027\uff0c99%\u7684\u8bc4\u5206\u8fbe\u5230\u81f3\u5c1195%\u4e00\u81f4\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528AES\u6a21\u578b\u65f6\u768492%\u4e00\u81f4\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7f6e\u4fe1\u5ea6\u5efa\u6a21\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86AES\u8bc4\u5206\u7684\u53ef\u9760\u6027\u548c\u9009\u62e9\u6027\u53d1\u5e03\u80fd\u529b\u3002", "keywords": "\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206, \u7f6e\u4fe1\u5ea6\u5efa\u6a21, CEFR, KWOCCE, \u5206\u7c7b\u4efb\u52a1"}}
{"id": "2505.23176", "pdf": "https://arxiv.org/pdf/2505.23176", "abs": "https://arxiv.org/abs/2505.23176", "authors": ["Shiwei Li", "Xiandi Luo", "Haozhao Wang", "Xing Tang", "Shijie Xu", "Weihong Luo", "Yuhua Li", "Xiuqiang He", "Ruixuan Li"], "title": "The Panaceas for Improving Low-Rank Decomposition in Communication-Efficient Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": "Accepted by ICML 2025", "summary": "To improve the training efficiency of federated learning (FL), previous\nresearch has employed low-rank decomposition techniques to reduce communication\noverhead. In this paper, we seek to enhance the performance of these low-rank\ndecomposition methods. Specifically, we focus on three key issues related to\ndecomposition in FL: what to decompose, how to decompose, and how to aggregate.\nSubsequently, we introduce three novel techniques: Model Update Decomposition\n(MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware\nDecomposition (AAD), each targeting a specific issue. These techniques are\ncomplementary and can be applied simultaneously to achieve optimal performance.\nAdditionally, we provide a rigorous theoretical analysis to ensure the\nconvergence of the proposed MUD. Extensive experimental results show that our\napproach achieves faster convergence and superior accuracy compared to relevant\nbaseline methods. The code is available at\nhttps://github.com/Leopold1423/fedmud-icml25.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e09\u79cd\u65b0\u6280\u672f\uff08MUD\u3001BKD\u3001AAD\uff09\u4ee5\u4f18\u5316\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u4f4e\u79e9\u5206\u89e3\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u8054\u90a6\u5b66\u4e60\u7684\u8bad\u7ec3\u6548\u7387\uff0c\u73b0\u6709\u7814\u7a76\u91c7\u7528\u4f4e\u79e9\u5206\u89e3\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u4f46\u6027\u80fd\u4ecd\u6709\u4f18\u5316\u7a7a\u95f4\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5206\u89e3\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff08\u5206\u89e3\u5185\u5bb9\u3001\u65b9\u5f0f\u53ca\u805a\u5408\uff09\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u6280\u672f\uff1a\u6a21\u578b\u66f4\u65b0\u5206\u89e3\uff08MUD\uff09\u3001\u5206\u5757Kronecker\u5206\u89e3\uff08BKD\uff09\u3001\u805a\u5408\u611f\u77e5\u5206\u89e3\uff08AAD\uff09\uff0c\u5206\u522b\u9488\u5bf9\u5206\u89e3\u7684\u4e09\u5927\u95ee\u9898\uff0c\u5e76\u53ef\u534f\u540c\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6536\u655b\u66f4\u5feb\u4e14\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u7406\u8bba\u5206\u6790\u786e\u4fdd\u4e86MUD\u7684\u6536\u655b\u6027\u3002", "conclusion": "\u901a\u8fc7\u6280\u672f\u521b\u65b0\u548c\u7406\u8bba\u9a8c\u8bc1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u4f4e\u79e9\u5206\u89e3\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60, \u4f4e\u79e9\u5206\u89e3, \u6a21\u578b\u66f4\u65b0\u5206\u89e3, Kronecker\u5206\u89e3, \u805a\u5408\u611f\u77e5"}}
{"id": "2505.23316", "pdf": "https://arxiv.org/pdf/2505.23316", "abs": "https://arxiv.org/abs/2505.23316", "authors": ["Kaiyang Guo", "Yinchuan Li", "Zhitang Chen"], "title": "Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO", "categories": ["cs.CL"], "comment": null, "summary": "Direct alignment methods typically optimize large language models (LLMs) by\ncontrasting the likelihoods of preferred versus dispreferred responses. While\neffective in steering LLMs to match relative preference, these methods are\nfrequently noted for decreasing the absolute likelihoods of example responses.\nAs a result, aligned models tend to generate outputs that deviate from the\nexpected patterns, exhibiting reward-hacking effect even without a reward\nmodel. This undesired consequence exposes a fundamental limitation in\ncontrastive alignment, which we characterize as likelihood underdetermination.\nIn this work, we revisit direct preference optimization (DPO) -- the seminal\ndirect alignment method -- and demonstrate that its loss theoretically admits a\ndecomposed reformulation. The reformulated loss not only broadens applicability\nto a wider range of feedback types, but also provides novel insights into the\nunderlying cause of likelihood underdetermination. Specifically, the standard\nDPO implementation implicitly oversimplifies a regularizer in the reformulated\nloss, and reinstating its complete version effectively resolves the\nunderdetermination issue. Leveraging these findings, we introduce PRoximalized\nPReference Optimization (PRO), a unified method to align with diverse feeback\ntypes, eliminating likelihood underdetermination through an efficient\napproximation of the complete regularizer. Comprehensive experiments show the\nsuperiority of PRO over existing methods in scenarios involving pairwise,\nbinary and scalar feedback.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPRO\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u76f4\u63a5\u504f\u597d\u4f18\u5316\u4e2d\u7684\u4f3c\u7136\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u53cd\u9988\u7c7b\u578b\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u76f4\u63a5\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982DPO\uff09\u5728\u4f18\u5316LLMs\u65f6\uff0c\u867d\u7136\u80fd\u5339\u914d\u76f8\u5bf9\u504f\u597d\uff0c\u4f46\u4f1a\u964d\u4f4e\u54cd\u5e94\u7edd\u5bf9\u4f3c\u7136\uff0c\u5bfc\u81f4\u5956\u52b1\u7834\u89e3\u7b49\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6839\u672c\u9650\u5236\u3002", "method": "\u901a\u8fc7\u5206\u89e3DPO\u635f\u5931\u51fd\u6570\uff0c\u63d0\u51faPRO\u65b9\u6cd5\uff0c\u6062\u590d\u5b8c\u6574\u6b63\u5219\u9879\u4ee5\u6d88\u9664\u4f3c\u7136\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u53cd\u9988\u7c7b\u578b\uff08\u5982\u6210\u5bf9\u3001\u4e8c\u5143\u548c\u6807\u91cf\u53cd\u9988\uff09\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660ePRO\u5728\u591a\u79cd\u53cd\u9988\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f3c\u7136\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u3002", "conclusion": "PRO\u4f5c\u4e3a\u4e00\u79cd\u7edf\u4e00\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u91cd\u6784\u548c\u6b63\u5219\u9879\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u4e0d\u540c\u53cd\u9988\u7c7b\u578b\u4e2d\u7684\u8868\u73b0\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u76f4\u63a5\u504f\u597d\u4f18\u5316, \u4f3c\u7136\u4e0d\u786e\u5b9a\u6027, \u5956\u52b1\u7834\u89e3, PRO\u65b9\u6cd5"}}
{"id": "2505.22939", "pdf": "https://arxiv.org/pdf/2505.22939", "abs": "https://arxiv.org/abs/2505.22939", "authors": ["Niclas Boehmer", "Sara Fish", "Ariel D. Procaccia"], "title": "Generative Social Choice: The Next Generation", "categories": ["cs.GT", "cs.AI", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "A key task in certain democratic processes is to produce a concise slate of\nstatements that proportionally represents the full spectrum of user opinions.\nThis task is similar to committee elections, but unlike traditional settings,\nthe candidate set comprises all possible statements of varying lengths, and so\nit can only be accessed through specific queries. Combining social choice and\nlarge language models, prior work has approached this challenge through a\nframework of generative social choice. We extend the framework in two\nfundamental ways, providing theoretical guarantees even in the face of\napproximately optimal queries and a budget limit on the overall length of the\nslate. Using GPT-4o to implement queries, we showcase our approach on datasets\nrelated to city improvement measures and drug reviews, demonstrating its\neffectiveness in generating representative slates from unstructured user\nopinions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6269\u5c55\u751f\u6210\u5f0f\u793e\u4f1a\u9009\u62e9\u6846\u67b6\uff0c\u7ed3\u5408\u793e\u4f1a\u9009\u62e9\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4ece\u975e\u7ed3\u6784\u5316\u7528\u6237\u610f\u89c1\u4e2d\u751f\u6210\u4ee3\u8868\u6027\u6458\u8981\u7684\u4efb\u52a1\uff0c\u5373\u4f7f\u5b58\u5728\u8fd1\u4f3c\u6700\u4f18\u67e5\u8be2\u548c\u9884\u7b97\u9650\u5236\uff0c\u4ecd\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u6c11\u4e3b\u8fc7\u7a0b\u4e2d\u9700\u8981\u4ece\u5927\u91cf\u975e\u7ed3\u6784\u5316\u7684\u7528\u6237\u610f\u89c1\u4e2d\u751f\u6210\u6bd4\u4f8b\u5747\u8861\u7684\u4ee3\u8868\u6027\u6458\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u5019\u9009\u96c6\u5408\u7684\u52a8\u6001\u6027\u548c\u957f\u5ea6\u9650\u5236\u3002", "method": "\u6269\u5c55\u751f\u6210\u5f0f\u793e\u4f1a\u9009\u62e9\u6846\u67b6\uff0c\u5f15\u5165\u8fd1\u4f3c\u6700\u4f18\u67e5\u8be2\u548c\u9884\u7b97\u9650\u5236\u7684\u7406\u8bba\u5206\u6790\uff0c\u5e76\u4f7f\u7528GPT-4o\u5b9e\u73b0\u67e5\u8be2\uff0c\u5e94\u7528\u4e8e\u57ce\u5e02\u6539\u8fdb\u63aa\u65bd\u548c\u836f\u7269\u8bc4\u4ef7\u6570\u636e\u96c6\u3002", "result": "\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u4ee3\u8868\u6027\u6458\u8981\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u57ce\u5e02\u6539\u8fdb\u548c\u836f\u7269\u8bc4\u4ef7\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6269\u5c55\u540e\u7684\u6846\u67b6\u5728\u4fdd\u8bc1\u7406\u8bba\u4e25\u8c28\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u9645\u89e3\u51b3\u4e86\u4ece\u975e\u7ed3\u6784\u5316\u610f\u89c1\u4e2d\u751f\u6210\u4ee3\u8868\u6027\u6458\u8981\u7684\u6311\u6218\u3002", "keywords": "\u751f\u6210\u5f0f\u793e\u4f1a\u9009\u62e9\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u6c11\u4e3b\u8fc7\u7a0b\u3001\u6bd4\u4f8b\u4ee3\u8868\u6027\u3001\u975e\u7ed3\u6784\u5316\u610f\u89c1"}}
{"id": "2505.23181", "pdf": "https://arxiv.org/pdf/2505.23181", "abs": "https://arxiv.org/abs/2505.23181", "authors": ["Tian Tian", "Chunyan Miao", "Hangwei Qian"], "title": "FreRA: A Frequency-Refined Augmentation for Contrastive Learning on Time Series Classification", "categories": ["cs.LG", "cs.AI", "I.2.6"], "comment": "KDD 2025", "summary": "Contrastive learning has emerged as a competent approach for unsupervised\nrepresentation learning. However, the design of an optimal augmentation\nstrategy, although crucial for contrastive learning, is less explored for time\nseries classification tasks. Existing predefined time-domain augmentation\nmethods are primarily adopted from vision and are not specific to time series\ndata. Consequently, this cross-modality incompatibility may distort the\nsemantically relevant information of time series by introducing mismatched\npatterns into the data. To address this limitation, we present a novel\nperspective from the frequency domain and identify three advantages for\ndownstream classification: global, independent, and compact. To fully utilize\nthe three properties, we propose the lightweight yet effective Frequency\nRefined Augmentation (FreRA) tailored for time series contrastive learning on\nclassification tasks, which can be seamlessly integrated with contrastive\nlearning frameworks in a plug-and-play manner. Specifically, FreRA\nautomatically separates critical and unimportant frequency components.\nAccordingly, we propose semantic-aware Identity Modification and\nsemantic-agnostic Self-adaptive Modification to protect semantically relevant\ninformation in the critical frequency components and infuse variance into the\nunimportant ones respectively. Theoretically, we prove that FreRA generates\nsemantic-preserving views. Empirically, we conduct extensive experiments on two\nbenchmark datasets, including UCR and UEA archives, as well as five large-scale\ndatasets on diverse applications. FreRA consistently outperforms ten leading\nbaselines on time series classification, anomaly detection, and transfer\nlearning tasks, demonstrating superior capabilities in contrastive\nrepresentation learning and generalization in transfer learning scenarios\nacross diverse datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u7684\u9891\u7387\u57df\u589e\u5f3a\u65b9\u6cd5\uff08FreRA\uff09\uff0c\u901a\u8fc7\u81ea\u52a8\u5206\u79bb\u5173\u952e\u548c\u975e\u5173\u952e\u9891\u7387\u6210\u5206\uff0c\u4f18\u5316\u5bf9\u6bd4\u5b66\u4e60\u7684\u8868\u793a\u6548\u679c\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u5bf9\u6bd4\u5b66\u4e60\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u4e3b\u8981\u4ece\u89c6\u89c9\u9886\u57df\u501f\u9274\uff0c\u53ef\u80fd\u7834\u574f\u65f6\u95f4\u5e8f\u5217\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9002\u5e94\u65f6\u95f4\u5e8f\u5217\u7279\u6027\u7684\u589e\u5f3a\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u9891\u7387\u7ec6\u5316\u589e\u5f3a\uff08FreRA\uff09\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u7684\u8eab\u4efd\u4fee\u6539\u548c\u8bed\u4e49\u65e0\u5173\u7684\u81ea\u9002\u5e94\u4fee\u6539\uff0c\u4fdd\u62a4\u5173\u952e\u9891\u7387\u6210\u5206\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u540c\u65f6\u589e\u52a0\u975e\u5173\u952e\u6210\u5206\u7684\u591a\u6837\u6027\u3002", "result": "\u5728UCR\u3001UEA\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u53ca\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cFreRA\u4f18\u4e8e10\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5206\u7c7b\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u8fc1\u79fb\u5b66\u4e60\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u9891\u7387\u57df\u589e\u5f3a\uff08FreRA\uff09\u80fd\u6709\u6548\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u5bf9\u6bd4\u5b66\u4e60\u7684\u8868\u793a\u80fd\u529b\u548c\u6cdb\u5316\u6027\uff0c\u9002\u914d\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "keywords": "\u5bf9\u6bd4\u5b66\u4e60,\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b,\u9891\u7387\u57df\u589e\u5f3a,\u8bed\u4e49\u4fdd\u6301,\u8fc1\u79fb\u5b66\u4e60"}}
{"id": "2505.23323", "pdf": "https://arxiv.org/pdf/2505.23323", "abs": "https://arxiv.org/abs/2505.23323", "authors": ["Harish Tayyar Madabushi", "Melissa Torgbi", "Claire Bonial"], "title": "Neither Stochastic Parroting nor AGI: LLMs Solve Tasks through Context-Directed Extrapolation from Training Data Priors", "categories": ["cs.CL"], "comment": null, "summary": "In this position paper we raise critical awareness of a realistic view of LLM\ncapabilities that eschews extreme alternative views that LLMs are either\n\"stochastic parrots\" or in possession of \"emergent\" advanced reasoning\ncapabilities, which, due to their unpredictable emergence, constitute an\nexistential threat. Our middle-ground view is that LLMs extrapolate from priors\nfrom their training data, and that a mechanism akin to in-context learning\nenables the targeting of the appropriate information from which to extrapolate.\nWe call this \"context-directed extrapolation.\" Under this view, substantiated\nthough existing literature, while reasoning capabilities go well beyond\nstochastic parroting, such capabilities are predictable, controllable, not\nindicative of advanced reasoning akin to high-level cognitive capabilities in\nhumans, and not infinitely scalable with additional training. As a result,\nfears of uncontrollable emergence of agency are allayed, while research\nadvances are appropriately refocused on the processes of context-directed\nextrapolation and how this interacts with training data to produce valuable\ncapabilities in LLMs. Future work can therefore explore alternative augmenting\ntechniques that do not rely on inherent advanced reasoning in LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ecb\u4e8e'\u968f\u673a\u9e66\u9e49'\u4e0e'\u5177\u5907\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b'\u4e4b\u95f4\u7684\u89c2\u70b9\uff0c\u8ba4\u4e3aLLMs\u901a\u8fc7'\u4e0a\u4e0b\u6587\u5f15\u5bfc\u5916\u63a8'\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u63d0\u53d6\u4fe1\u606f\uff0c\u5176\u80fd\u529b\u662f\u53ef\u9884\u6d4b\u3001\u53ef\u63a7\u7684\uff0c\u4e14\u4e0d\u4f9d\u8d56\u65e0\u9650\u6269\u5c55\u7684\u8bad\u7ec3\u89c4\u6a21\u3002", "motivation": "\u9488\u5bf9\u5f53\u524d\u5bf9LLM\u80fd\u529b\u7684\u6781\u7aef\u89c2\u70b9\uff08\u8981\u4e48\u662f'\u968f\u673a\u9e66\u9e49'\uff0c\u8981\u4e48\u5177\u5907\u4e0d\u53ef\u63a7\u7684'\u6d8c\u73b0'\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\uff09\uff0c\u672c\u6587\u8bd5\u56fe\u63d0\u4f9b\u4e00\u4e2a\u66f4\u73b0\u5b9e\u7684\u4e2d\u95f4\u7acb\u573a\uff0c\u4ee5\u6d88\u9664\u5bf9LLM\u4e0d\u53ef\u63a7\u80fd\u529b\u7684\u62c5\u5fe7\uff0c\u5e76\u91cd\u65b0\u805a\u7126\u7814\u7a76\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u63d0\u51fa'\u4e0a\u4e0b\u6587\u5f15\u5bfc\u5916\u63a8'\uff08context-directed extrapolation\uff09\u673a\u5236\uff0c\u89e3\u91caLLMs\u5982\u4f55\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u5b9a\u5411\u63d0\u53d6\u4fe1\u606f\u5e76\u5916\u63a8\uff0c\u800c\u975e\u4f9d\u8d56\u4e0d\u53ef\u9884\u6d4b\u7684'\u6d8c\u73b0'\u80fd\u529b\u3002", "result": "\u8bba\u8bc1LLMs\u7684\u80fd\u529b\u867d\u8d85\u8d8a\u968f\u673a\u91cd\u590d\uff0c\u4f46\u4ecd\u53ef\u9884\u6d4b\u3001\u53ef\u63a7\uff0c\u4e14\u4e0d\u4e0e\u4eba\u7c7b\u9ad8\u7ea7\u8ba4\u77e5\u80fd\u529b\u7b49\u540c\uff1b\u540c\u65f6\u6307\u51fa\u5176\u80fd\u529b\u65e0\u6cd5\u901a\u8fc7\u65e0\u9650\u8bad\u7ec3\u65e0\u9650\u6269\u5c55\u3002", "conclusion": "\u547c\u5401\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u4e0a\u4e0b\u6587\u5f15\u5bfc\u5916\u63a8\u4e0e\u8bad\u7ec3\u6570\u636e\u7684\u4ea4\u4e92\uff0c\u4ee5\u53ca\u4e0d\u4f9d\u8d56LLMs\u56fa\u6709\u63a8\u7406\u80fd\u529b\u7684\u589e\u5f3a\u6280\u672f\u3002", "keywords": "LLMs, \u4e0a\u4e0b\u6587\u5f15\u5bfc\u5916\u63a8, \u968f\u673a\u9e66\u9e49, \u6d8c\u73b0\u80fd\u529b, \u53ef\u63a7\u6027"}}
{"id": "2505.23182", "pdf": "https://arxiv.org/pdf/2505.23182", "abs": "https://arxiv.org/abs/2505.23182", "authors": ["Srijith Nair", "Michael Lin", "Amirreza Talebi", "Peizhong Ju", "Elizabeth Bentley", "Jia Liu"], "title": "FSL-SAGE: Accelerating Federated Split Learning via Smashed Activation Gradient Estimation", "categories": ["cs.LG"], "comment": "22 pages, 14 figures, Accepted at ICML 2025 as poster", "summary": "Collaborative training methods like Federated Learning (FL) and Split\nLearning (SL) enable distributed machine learning without sharing raw data.\nHowever, FL assumes clients can train entire models, which is infeasible for\nlarge-scale models. In contrast, while SL alleviates the client memory\nconstraint in FL by offloading most training to the server, it increases\nnetwork latency due to its sequential nature. Other methods address the\nconundrum by using local loss functions for parallel client-side training to\nimprove efficiency, but they lack server feedback and potentially suffer poor\naccuracy. We propose FSL-SAGE (Federated Split Learning via Smashed Activation\nGradient Estimation), a new federated split learning algorithm that estimates\nserver-side gradient feedback via auxiliary models. These auxiliary models\nperiodically adapt to emulate server behavior on local datasets. We show that\nFSL-SAGE achieves a convergence rate of $\\mathcal{O}(1/\\sqrt{T})$, where $T$ is\nthe number of communication rounds. This result matches FedAvg, while\nsignificantly reducing communication costs and client memory requirements. Our\nempirical results also verify that it outperforms existing state-of-the-art FSL\nmethods, offering both communication efficiency and accuracy.", "AI": {"tldr": "FSL-SAGE\u662f\u4e00\u79cd\u65b0\u578b\u7684\u8054\u5408\u5206\u5272\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f85\u52a9\u6a21\u578b\u4f30\u8ba1\u670d\u52a1\u5668\u7aef\u68af\u5ea6\u53cd\u9988\uff0c\u517c\u987e\u4e86\u901a\u4fe1\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u8054\u5408\u5b66\u4e60\u65b9\u6cd5\uff08\u5982FL\u548cSL\uff09\u5b58\u5728\u5ba2\u6237\u7aef\u5185\u5b58\u9650\u5236\u6216\u7f51\u7edc\u5ef6\u8fdf\u95ee\u9898\uff0c\u7f3a\u4e4f\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u800c\u73b0\u6709\u5e76\u884c\u8bad\u7ec3\u65b9\u6cd5\u53c8\u7f3a\u4e4f\u670d\u52a1\u5668\u53cd\u9988\uff0c\u53ef\u80fd\u5bfc\u81f4\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u63d0\u51faFSL-SAGE\u7b97\u6cd5\uff0c\u901a\u8fc7\u8f85\u52a9\u6a21\u578b\u5468\u671f\u6027\u6a21\u62df\u670d\u52a1\u5668\u884c\u4e3a\uff0c\u4f30\u8ba1\u670d\u52a1\u5668\u7aef\u68af\u5ea6\u53cd\u9988\uff0c\u5b9e\u73b0\u5e76\u884c\u5ba2\u6237\u7aef\u8bad\u7ec3\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86FSL-SAGE\u7684\u6536\u655b\u901f\u5ea6\u4e3a$\u213b$\u213b1/\u213bT$\u213b$\uff0c\u5176\u901a\u4fe1\u6210\u672c\u548c\u5ba2\u6237\u7aef\u5185\u5b58\u9700\u6c42\u663e\u8457\u964d\u4f4e\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u4e8e\u73b0\u6709FSL\u65b9\u6cd5\u3002", "conclusion": "FSL-SAGE\u5728\u4fdd\u8bc1\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u548c\u5185\u5b58\u5f00\u9500\uff0c\u4e3a\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u8054\u5408\u5b66\u4e60,\u5206\u5272\u5b66\u4e60,\u68af\u5ea6\u4f30\u8ba1,\u901a\u4fe1\u6548\u7387,\u8f85\u52a9\u6a21\u578b"}}
{"id": "2505.23363", "pdf": "https://arxiv.org/pdf/2505.23363", "abs": "https://arxiv.org/abs/2505.23363", "authors": ["Hongzhan Chen", "Tao Yang", "Shiping Gao", "Ruijun Chen", "Xiaojun Quan", "Hongtao Tian", "Ting Yao"], "title": "Discriminative Policy Optimization for Token-Level Reward Models", "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "Process reward models (PRMs) provide more nuanced supervision compared to\noutcome reward models (ORMs) for optimizing policy models, positioning them as\na promising approach to enhancing the capabilities of LLMs in complex reasoning\ntasks. Recent efforts have advanced PRMs from step-level to token-level\ngranularity by integrating reward modeling into the training of generative\nmodels, with reward scores derived from token generation probabilities.\nHowever, the conflict between generative language modeling and reward modeling\nmay introduce instability and lead to inaccurate credit assignments. To address\nthis challenge, we revisit token-level reward assignment by decoupling reward\nmodeling from language generation and derive a token-level reward model through\nthe optimization of a discriminative policy, termed the Q-function Reward Model\n(Q-RM). We theoretically demonstrate that Q-RM explicitly learns token-level\nQ-functions from preference data without relying on fine-grained annotations.\nIn our experiments, Q-RM consistently outperforms all baseline methods across\nvarious benchmarks. For example, when integrated into PPO/REINFORCE algorithms,\nQ-RM enhances the average Pass@1 score by 5.85/4.70 points on mathematical\nreasoning tasks compared to the ORM baseline, and by 4.56/5.73 points compared\nto the token-level PRM counterpart. Moreover, reinforcement learning with Q-RM\nsignificantly enhances training efficiency, achieving convergence 12 times\nfaster than ORM on GSM8K and 11 times faster than step-level PRM on MATH. Code\nand data are available at https://github.com/homzer/Q-RM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aQ-RM\u7684\u4ee4\u724c\u7ea7\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u8026\u5956\u52b1\u5efa\u6a21\u4e0e\u8bed\u8a00\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7b56\u7565\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(PRMs)\u4e2d\u751f\u6210\u8bed\u8a00\u5efa\u6a21\u4e0e\u5956\u52b1\u5efa\u6a21\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u907f\u514d\u4e0d\u7a33\u5b9a\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\u3002", "method": "\u63d0\u51faQ-RM\u6a21\u578b\uff0c\u901a\u8fc7\u4f18\u5316\u5224\u522b\u7b56\u7565\u6765\u5b66\u4e60\u4ee4\u724c\u7ea7Q\u51fd\u6570\uff0c\u4e0d\u4f9d\u8d56\u7ec6\u7c92\u5ea6\u6807\u6ce8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cQ-RM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "conclusion": "Q-RM\u6709\u6548\u89e3\u51b3\u4e86PRMs\u4e2d\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "Q-RM, \u8fc7\u7a0b\u5956\u52b1\u6a21\u578b, \u4ee4\u724c\u7ea7\u5956\u52b1, \u5f3a\u5316\u5b66\u4e60, \u590d\u6742\u63a8\u7406"}}
{"id": "2505.23184", "pdf": "https://arxiv.org/pdf/2505.23184", "abs": "https://arxiv.org/abs/2505.23184", "authors": ["Hongcan Guo", "Guoshun Nan", "Yuan Yang", "Diyang Zhang", "Haotian Li", "Zhican Chen", "Qinchuan Zhou", "Yuhan Ran", "Xinye Cao", "Sicong Leng", "Xiaofeng Tao", "Xudong Jiang"], "title": "Two Is Better Than One: Rotations Scale LoRAs", "categories": ["cs.LG", "cs.SE", "68T50", "I.2.6"], "comment": "27pages, 16figures", "summary": "Scaling Low-Rank Adaptation (LoRA)-based Mixture-of-Experts (MoE) facilitates\nlarge language models (LLMs) to efficiently adapt to diverse tasks. However,\ntraditional gating mechanisms that route inputs to the best experts may\nfundamentally hinder LLMs' scalability, leading to poor generalization and\nunderfitting issues. We identify that the root cause lies in the restricted\nexpressiveness of existing weighted-sum mechanisms, both within and outside the\nconvex cone of LoRA representations. This motivates us to propose RadarGate, a\nnovel geometrically inspired gating method that introduces rotational\noperations of LoRAs representations to boost the expressiveness and facilitate\nricher feature interactions among multiple LoRAs for scalable LLMs.\nSpecifically, we first fuse each LoRA representation to other LoRAs using a\nlearnable component and then feed the output to a rotation matrix. This matrix\ninvolves learnable parameters that define the relative angular relationship\nbetween LoRA representations. Such a simple yet effective mechanism provides an\nextra degree of freedom, facilitating the learning of cross-LoRA synergies and\nproperly tracking the challenging poor generalization and underfitting issues\nas the number of LoRA grows. Extensive experiments on 6 public benchmarks\nacross 21 tasks show the effectiveness of our RadarGate for scaling LoRAs. We\nalso provide valuable insights, revealing that the rotations to each pair of\nrepresentations are contrastive, encouraging closer alignment of semantically\nsimilar representations during geometrical transformation while pushing\ndistance ones further apart. We will release our code to the community.", "AI": {"tldr": "RadarGate\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u65cb\u8f6c\u7684\u65b0\u578b\u95e8\u63a7\u673a\u5236\uff0c\u901a\u8fc7\u589e\u5f3aLoRA\u8868\u793a\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u52a0\u6743\u548c\u673a\u5236\u5728LLM\u6269\u5c55\u4e2d\u7684\u6cdb\u5316\u548c\u6b20\u62df\u5408\u95ee\u9898\u3002\u5b9e\u9a8c\u57286\u4e2a\u516c\u5f00\u57fa\u51c6\u768421\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u95e8\u63a7\u673a\u5236\uff08\u5982\u52a0\u6743\u548c\uff09\u9650\u5236\u4e86LoRA\u8868\u793a\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5bfc\u81f4LLM\u6269\u5c55\u65f6\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u6b20\u62df\u5408\u95ee\u9898\u3002RadarGate\u901a\u8fc7\u51e0\u4f55\u65cb\u8f6c\u64cd\u4f5c\u63d0\u5347\u8868\u793a\u4ea4\u4e92\u7684\u4e30\u5bcc\u6027\u3002", "method": "1. \u4f7f\u7528\u53ef\u5b66\u4e60\u7ec4\u4ef6\u878d\u5408\u5404LoRA\u8868\u793a\uff1b2. \u901a\u8fc7\u65cb\u8f6c\u77e9\u9635\u5b9a\u4e49LoRA\u8868\u793a\u95f4\u7684\u89d2\u5ea6\u5173\u7cfb\uff0c\u589e\u5f3a\u8de8LoRA\u534f\u540c\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u57286\u4e2a\u57fa\u51c6\u768421\u4e2a\u4efb\u52a1\u4e2d\uff0cRadarGate\u663e\u8457\u6539\u5584\u4e86LoRA\u7684\u6269\u5c55\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u65cb\u8f6c\u64cd\u4f5c\u5bf9\u6bd4\u5b66\u4e60\u7684\u7279\u6027\u3002", "conclusion": "RadarGate\u901a\u8fc7\u51e0\u4f55\u65cb\u8f6c\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86LoRA\u6269\u5c55\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3aLLM\u7684\u89c4\u6a21\u5316\u9002\u914d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "LoRA, Mixture-of-Experts, \u95e8\u63a7\u673a\u5236, \u51e0\u4f55\u65cb\u8f6c, \u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.23368", "pdf": "https://arxiv.org/pdf/2505.23368", "abs": "https://arxiv.org/abs/2505.23368", "authors": ["Beiduo Chen", "Yang Janet Liu", "Anna Korhonen", "Barbara Plank"], "title": "Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation", "categories": ["cs.CL"], "comment": "22 pages, 7 figures", "summary": "The recent rise of reasoning-tuned Large Language Models (LLMs)--which\ngenerate chains of thought (CoTs) before giving the final answer--has attracted\nsignificant attention and offers new opportunities for gaining insights into\nhuman label variation, which refers to plausible differences in how multiple\nannotators label the same data instance. Prior work has shown that\nLLM-generated explanations can help align model predictions with human label\ndistributions, but typically adopt a reverse paradigm: producing explanations\nbased on given answers. In contrast, CoTs provide a forward reasoning path that\nmay implicitly embed rationales for each answer option, before generating the\nanswers. We thus propose a novel LLM-based pipeline enriched with\nlinguistically-grounded discourse segmenters to extract supporting and opposing\nstatements for each answer option from CoTs with improved accuracy. We also\npropose a rank-based HLV evaluation framework that prioritizes the ranking of\nanswers over exact scores, which instead favor direct comparison of label\ndistributions. Our method outperforms a direct generation method as well as\nbaselines on three datasets, and shows better alignment of ranking methods with\nhumans, highlighting the effectiveness of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406\u8c03\u4f18\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6d41\u7a0b\uff0c\u7ed3\u5408\u8bed\u8a00\u5b66\u9a71\u52a8\u7684\u6587\u672c\u5206\u5272\u6280\u672f\uff0c\u4ece\u601d\u7ef4\u94fe\uff08CoTs\uff09\u4e2d\u63d0\u53d6\u652f\u6301\u6216\u53cd\u5bf9\u6bcf\u4e2a\u7b54\u6848\u9009\u9879\u7684\u9648\u8ff0\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002\u540c\u65f6\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6392\u540d\u7684HLV\u8bc4\u4f30\u6846\u67b6\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u548c\u57fa\u51c6\uff0c\u5e76\u66f4\u7b26\u5408\u4eba\u7c7b\u8bc4\u4f30\u3002", "motivation": "\u4eba\u7c7b\u6807\u6ce8\u8005\u5bf9\u4e8e\u540c\u4e00\u6570\u636e\u5b9e\u4f8b\u53ef\u80fd\u5b58\u5728\u591a\u79cd\u5408\u7406\u7684\u6807\u6ce8\u5dee\u5f02\uff08HLV\uff09\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u8868\u660eLLM\u751f\u6210\u7684\u89e3\u91ca\u53ef\u4ee5\u5e2e\u52a9\u6a21\u578b\u9884\u6d4b\u4e0e\u4eba\u7c7b\u6807\u6ce8\u5206\u5e03\u5bf9\u9f50\uff0c\u4f46\u901a\u5e38\u91c7\u7528\u53cd\u5411\u8303\u5f0f\uff08\u57fa\u4e8e\u7ed9\u5b9a\u7b54\u6848\u751f\u6210\u89e3\u91ca\uff09\u3002\u800c\u601d\u7ef4\u94fe\uff08CoTs\uff09\u63d0\u4f9b\u4e86\u524d\u5411\u63a8\u7406\u8def\u5f84\uff0c\u53ef\u80fd\u9690\u542b\u6bcf\u4e2a\u7b54\u6848\u9009\u9879\u7684\u5408\u7406\u6027\uff0c\u56e0\u6b64\u7814\u7a76\u5982\u4f55\u5229\u7528CoTs\u6539\u8fdbHLV\u8bc4\u4f30\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u63d0\u51fa\u4e00\u79cdLLM\u6d41\u7a0b\uff0c\u7ed3\u5408\u8bed\u8a00\u5b66\u9a71\u52a8\u7684\u6587\u672c\u5206\u5272\u5668\uff0c\u4eceCoTs\u4e2d\u63d0\u53d6\u652f\u6301\u548c\u53cd\u5bf9\u6bcf\u4e2a\u7b54\u6848\u9009\u9879\u7684\u9648\u8ff0\u3002\u540c\u65f6\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u6392\u540d\u7684HLV\u8bc4\u4f30\u6846\u67b6\uff0c\u66f4\u6ce8\u91cd\u7b54\u6848\u7684\u6392\u5e8f\u800c\u975e\u7cbe\u786e\u8bc4\u5206\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u76f4\u63a5\u751f\u6210\u65b9\u6cd5\u548c\u57fa\u51c6\u6a21\u578b\uff0c\u4e14\u5176\u6392\u540d\u65b9\u6cd5\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u5951\u5408\u5ea6\u66f4\u9ad8\u3002", "conclusion": "\u901a\u8fc7\u8bed\u8a00\u5b66\u9a71\u52a8\u7684\u6587\u672c\u5206\u5272\u548c\u57fa\u4e8e\u6392\u540d\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLM\u5728HLV\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u66f4\u597d\u5730\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u5bf9\u9f50\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3001\u601d\u7ef4\u94fe\uff08CoTs\uff09\u3001\u4eba\u7c7b\u6807\u6ce8\u5dee\u5f02\uff08HLV\uff09\u3001\u8bed\u8a00\u5b66\u9a71\u52a8\u7684\u6587\u672c\u5206\u5272\u3001\u6392\u540d\u6846\u67b6"}}
{"id": "2505.22944", "pdf": "https://arxiv.org/pdf/2505.22944", "abs": "https://arxiv.org/abs/2505.22944", "authors": ["Angtian Wang", "Haibin Huang", "Jacob Zhiyuan Fang", "Yiding Yang", "Chongyang Ma"], "title": "ATI: Any Trajectory Instruction for Controllable Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a unified framework for motion control in video generation that\nseamlessly integrates camera movement, object-level translation, and\nfine-grained local motion using trajectory-based inputs. In contrast to prior\nmethods that address these motion types through separate modules or\ntask-specific designs, our approach offers a cohesive solution by projecting\nuser-defined trajectories into the latent space of pre-trained image-to-video\ngeneration models via a lightweight motion injector. Users can specify\nkeypoints and their motion paths to control localized deformations, entire\nobject motion, virtual camera dynamics, or combinations of these. The injected\ntrajectory signals guide the generative process to produce temporally\nconsistent and semantically aligned motion sequences. Our framework\ndemonstrates superior performance across multiple video motion control tasks,\nincluding stylized motion effects (e.g., motion brushes), dynamic viewpoint\nchanges, and precise local motion manipulation. Experiments show that our\nmethod provides significantly better controllability and visual quality\ncompared to prior approaches and commercial solutions, while remaining broadly\ncompatible with various state-of-the-art video generation backbones. Project\npage: https://anytraj.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u9891\u751f\u6210\u8fd0\u52a8\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u7528\u6237\u5b9a\u4e49\u7684\u8f68\u8ff9\u8f93\u5165\u7efc\u5408\u63a7\u5236\u76f8\u673a\u8fd0\u52a8\u3001\u7269\u4f53\u5e73\u79fb\u548c\u5c40\u90e8\u8fd0\u52a8\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u72ec\u7acb\u6a21\u5757\u6216\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u8fd0\u52a8\u63a7\u5236\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6027\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8fd0\u52a8\u6ce8\u5165\u5668\u5c06\u7528\u6237\u5b9a\u4e49\u7684\u8f68\u8ff9\u6295\u5c04\u5230\u9884\u8bad\u7ec3\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5b9e\u73b0\u8fd0\u52a8\u63a7\u5236\uff0c\u652f\u6301\u5173\u952e\u70b9\u548c\u8def\u5f84\u6307\u5b9a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u89c6\u9891\u8fd0\u52a8\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ec\u98ce\u683c\u5316\u8fd0\u52a8\u6548\u679c\u3001\u52a8\u6001\u89c6\u89d2\u53d8\u5316\u548c\u5c40\u90e8\u8fd0\u52a8\u7cbe\u786e\u64cd\u63a7\uff0c\u53ef\u63a7\u6027\u548c\u89c6\u89c9\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u53ca\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u8f68\u8ff9\u6ce8\u5165\u5b9e\u73b0\u591a\u7c7b\u578b\u8fd0\u52a8\u63a7\u5236\uff0c\u517c\u5bb9\u6027\u5f3a\uff0c\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u548c\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u89c6\u9891\u751f\u6210\u3001\u8fd0\u52a8\u63a7\u5236\u3001\u8f68\u8ff9\u6ce8\u5165\u3001\u7edf\u4e00\u6846\u67b6\u3001\u6f5c\u5728\u7a7a\u95f4"}}
{"id": "2505.23185", "pdf": "https://arxiv.org/pdf/2505.23185", "abs": "https://arxiv.org/abs/2505.23185", "authors": ["Shahaf E. Finder", "Ron Shapira Weber", "Moshe Eliasof", "Oren Freifeld", "Eran Treister"], "title": "Improving the Effective Receptive Field of Message-Passing Neural Networks", "categories": ["cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Message-Passing Neural Networks (MPNNs) have become a cornerstone for\nprocessing and analyzing graph-structured data. However, their effectiveness is\noften hindered by phenomena such as over-squashing, where long-range\ndependencies or interactions are inadequately captured and expressed in the\nMPNN output. This limitation mirrors the challenges of the Effective Receptive\nField (ERF) in Convolutional Neural Networks (CNNs), where the theoretical\nreceptive field is underutilized in practice. In this work, we show and\ntheoretically explain the limited ERF problem in MPNNs. Furthermore, inspired\nby recent advances in ERF augmentation for CNNs, we propose an Interleaved\nMultiscale Message-Passing Neural Networks (IM-MPNN) architecture to address\nthese problems in MPNNs. Our method incorporates a hierarchical coarsening of\nthe graph, enabling message-passing across multiscale representations and\nfacilitating long-range interactions without excessive depth or\nparameterization. Through extensive evaluations on benchmarks such as the\nLong-Range Graph Benchmark (LRGB), we demonstrate substantial improvements over\nbaseline MPNNs in capturing long-range dependencies while maintaining\ncomputational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86IM-MPNN\u67b6\u6784\u89e3\u51b3MPNNs\u4e2d\u957f\u7a0b\u4f9d\u8d56\u6355\u6349\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u56fe\u8868\u793a\u589e\u5f3a\u6d88\u606f\u4f20\u9012\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "MPNNs\u5728\u5904\u7406\u56fe\u7ed3\u6784\u6570\u636e\u65f6\uff0c\u5e38\u56e0\u8fc7\u538b\u7f29\u73b0\u8c61\u65e0\u6cd5\u6709\u6548\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\uff0c\u7c7b\u4f3c\u4e8eCNNs\u4e2d\u7684\u6709\u6548\u611f\u53d7\u91ce\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u5c3a\u5ea6\u56fe\u8868\u793a\u65b9\u6cd5\uff08IM-MPNN\uff09\uff0c\u901a\u8fc7\u5728\u591a\u5c3a\u5ea6\u56fe\u4e0a\u8fdb\u884c\u6d88\u606f\u4f20\u9012\uff0c\u589e\u5f3a\u957f\u7a0b\u4ea4\u4e92\u3002", "result": "\u5728LRGB\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cIM-MPNN\u663e\u8457\u63d0\u5347\u4e86\u957f\u7a0b\u4f9d\u8d56\u6355\u6349\u80fd\u529b\u5e76\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "IM-MPNN\u6709\u6548\u89e3\u51b3\u4e86MPNNs\u4e2d\u7684\u957f\u7a0b\u4f9d\u8d56\u95ee\u9898\uff0c\u4e3a\u56fe\u7ed3\u6784\u6570\u636e\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "MPNNs, \u957f\u7a0b\u4f9d\u8d56, \u591a\u5c3a\u5ea6\u56fe\u8868\u793a, IM-MPNN"}}
{"id": "2505.23404", "pdf": "https://arxiv.org/pdf/2505.23404", "abs": "https://arxiv.org/abs/2505.23404", "authors": ["Mingyu Yu", "Wei Wang", "Yanjie Wei", "Sujuan Qin"], "title": "Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Adversarial attacks on Large Language Models (LLMs) via jailbreaking\ntechniques-methods that circumvent their built-in safety and ethical\nconstraints-have emerged as a critical challenge in AI security. These attacks\ncompromise the reliability of LLMs by exploiting inherent weaknesses in their\ncomprehension capabilities. This paper investigates the efficacy of\njailbreaking strategies that are specifically adapted to the diverse levels of\nunderstanding exhibited by different LLMs. We propose the Adaptive Jailbreaking\nStrategies Based on the Semantic Understanding Capabilities of Large Language\nModels, a novel framework that classifies LLMs into Type I and Type II\ncategories according to their semantic comprehension abilities. For each\ncategory, we design tailored jailbreaking strategies aimed at leveraging their\nvulnerabilities to facilitate successful attacks. Extensive experiments\nconducted on multiple LLMs demonstrate that our adaptive strategy markedly\nimproves the success rate of jailbreaking. Notably, our approach achieves an\nexceptional 98.9% success rate in jailbreaking GPT-4o(29 May 2025 release)", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u9002\u5e94\u6027\u8d8a\u72f1\u7b56\u7565\uff0c\u6839\u636e\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u5c06\u5176\u5206\u7c7b\uff0c\u5e76\u4e3a\u6bcf\u7c7b\u8bbe\u8ba1\u5b9a\u5236\u5316\u653b\u51fb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8d8a\u72f1\u6210\u529f\u7387\uff0c\u5c24\u5176\u5728GPT-4o\u4e0a\u8fbe\u523098.9%\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u5bf9\u6297\u6027\u653b\u51fb\uff08\u5982\u8d8a\u72f1\u6280\u672f\uff09\u5bf9LLMs\u5b89\u5168\u6027\u7684\u5a01\u80c1\uff0c\u65e8\u5728\u901a\u8fc7\u5229\u7528\u6a21\u578b\u7406\u89e3\u80fd\u529b\u7684\u5f31\u70b9\uff0c\u5f00\u53d1\u66f4\u6709\u6548\u7684\u653b\u51fb\u7b56\u7565\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5c06LLMs\u6309\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u5206\u4e3aType I\u548cType II\u4e24\u7c7b\uff0c\u5e76\u4e3a\u6bcf\u7c7b\u8bbe\u8ba1\u5b9a\u5236\u5316\u8d8a\u72f1\u7b56\u7565\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u9002\u5e94\u6027\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u8d8a\u72f1\u6210\u529f\u7387\uff0c\u5c24\u5176\u662f\u5728GPT-4o\u4e0a\u8fbe\u523098.9%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0c\u57fa\u4e8e\u6a21\u578b\u7406\u89e3\u80fd\u529b\u7684\u5b9a\u5236\u5316\u8d8a\u72f1\u7b56\u7565\u5bf9\u653b\u51fbLLMs\u975e\u5e38\u6709\u6548\uff0c\u63ed\u793a\u4e86\u5176\u5b89\u5168\u6f0f\u6d1e\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5bf9\u6297\u6027\u653b\u51fb, \u8d8a\u72f1\u6280\u672f, \u8bed\u4e49\u7406\u89e3, AI\u5b89\u5168"}}
{"id": "2505.23190", "pdf": "https://arxiv.org/pdf/2505.23190", "abs": "https://arxiv.org/abs/2505.23190", "authors": ["Yekun Zhu", "Min Tang", "Zheng Ma"], "title": "DeepRTE: Pre-trained Attention-based Neural Network for Radiative Tranfer", "categories": ["cs.LG"], "comment": null, "summary": "In this study, we propose a novel neural network approach, termed DeepRTE, to\naddress the steady-state Radiative Transfer Equation (RTE). The RTE is a\ndifferential-integral equation that governs the propagation of radiation\nthrough a participating medium, with applications spanning diverse domains such\nas neutron transport, atmospheric radiative transfer, heat transfer, and\noptical imaging. Our proposed DeepRTE framework leverages pre-trained\nattention-based neural networks to solve the RTE with high accuracy and\ncomputational efficiency. The efficacy of the proposed approach is\nsubstantiated through comprehensive numerical experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeepRTE\u7684\u65b0\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u7a33\u6001\u8f90\u5c04\u4f20\u8f93\u65b9\u7a0b\uff08RTE\uff09\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6ce8\u610f\u529b\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4e0e\u9ad8\u6548\u8ba1\u7b97\u3002", "motivation": "RTE\u5728\u591a\u4e2a\u9886\u57df\uff08\u5982\u4e2d\u5b50\u4f20\u8f93\u3001\u5927\u6c14\u8f90\u5c04\u4f20\u8f93\u7b49\uff09\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u4e14\u6548\u7387\u4f4e\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86DeepRTE\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6ce8\u610f\u529b\u795e\u7ecf\u7f51\u7edc\u6765\u6c42\u89e3RTE\u3002", "result": "\u901a\u8fc7\u5168\u9762\u7684\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "DeepRTE\u5728\u6c42\u89e3RTE\u65f6\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "DeepRTE, \u8f90\u5c04\u4f20\u8f93\u65b9\u7a0b, \u795e\u7ecf\u7f51\u7edc, \u6ce8\u610f\u529b\u673a\u5236, \u6570\u503c\u5b9e\u9a8c"}}
{"id": "2505.23410", "pdf": "https://arxiv.org/pdf/2505.23410", "abs": "https://arxiv.org/abs/2505.23410", "authors": ["Xuan Gong", "Hanbo Huang", "Shiyu Liang"], "title": "From Parameters to Prompts: Understanding and Mitigating the Factuality Gap between Fine-Tuned LLMs", "categories": ["cs.CL"], "comment": "The code of this paper will be released soon", "summary": "Factual knowledge extraction aims to explicitly extract knowledge\nparameterized in pre-trained language models for application in downstream\ntasks. While prior work has been investigating the impact of supervised\nfine-tuning data on the factuality of large language models (LLMs), its\nmechanism remains poorly understood. We revisit this impact through systematic\nexperiments, with a particular focus on the factuality gap that arises when\nfine-tuning on known versus unknown knowledge. Our findings show that this gap\ncan be mitigated at the inference stage, either under out-of-distribution (OOD)\nsettings or by using appropriate in-context learning (ICL) prompts (i.e.,\nfew-shot learning and Chain of Thought (CoT)). We prove this phenomenon\ntheoretically from the perspective of knowledge graphs, showing that the\ntest-time prompt may diminish or even overshadow the impact of fine-tuning data\nand play a dominant role in knowledge extraction. Ultimately, our results shed\nlight on the interaction between finetuning data and test-time prompt,\ndemonstrating that ICL can effectively compensate for shortcomings in\nfine-tuning data, and highlighting the need to reconsider the use of ICL\nprompting as a means to evaluate the effectiveness of fine-tuning data\nselection methods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u76d1\u7763\u5fae\u8c03\u6570\u636e\u5bf9\u4e8b\u5b9e\u6027\u77e5\u8bc6\u63d0\u53d6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u901a\u8fc7\u9002\u5f53\u7684\u63a8\u7406\u9636\u6bb5\u63d0\u793a\uff08\u5982\u5c11\u6837\u672c\u5b66\u4e60\u548c\u601d\u7ef4\u94fe\u63d0\u793a\uff09\u53ef\u4ee5\u7f13\u89e3\u5df2\u77e5\u4e0e\u672a\u77e5\u77e5\u8bc6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u7406\u8bba\u8bc1\u660e\u4e86\u63d0\u793a\u5bf9\u77e5\u8bc6\u63d0\u53d6\u7684\u4e3b\u5bfc\u4f5c\u7528\u3002", "motivation": "\u63a2\u8ba8\u76d1\u7763\u5fae\u8c03\u6570\u636e\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e8b\u5b9e\u6027\u7684\u5f71\u54cd\u673a\u5236\uff0c\u5c24\u5176\u662f\u5df2\u77e5\u4e0e\u672a\u77e5\u77e5\u8bc6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u63a8\u7406\u9636\u6bb5\u65b9\u6cd5\uff08\u5982\u5c11\u6837\u672c\u5b66\u4e60\u548c\u601d\u7ef4\u94fe\u63d0\u793a\uff09\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u7814\u7a76\u5df2\u77e5\u4e0e\u672a\u77e5\u77e5\u8bc6\u5728\u5fae\u8c03\u65f6\u7684\u5dee\u8ddd\uff0c\u5e76\u7406\u8bba\u5206\u6790\u77e5\u8bc6\u56fe\u89c6\u89d2\u4e0b\u63d0\u793a\u5bf9\u77e5\u8bc6\u63d0\u53d6\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u63a8\u7406\u9636\u6bb5\u7684\u63d0\u793a\uff08\u5982\u5c11\u6837\u672c\u5b66\u4e60\u548c\u601d\u7ef4\u94fe\uff09\u53ef\u4ee5\u5f25\u8865\u5fae\u8c03\u6570\u636e\u7684\u4e0d\u8db3\uff0c\u751a\u81f3\u5728\u77e5\u8bc6\u63d0\u53d6\u4e2d\u8d77\u5230\u4e3b\u5bfc\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u63d0\u793a\u53ef\u4ee5\u6709\u6548\u5f25\u8865\u5fae\u8c03\u6570\u636e\u7684\u7f3a\u9677\uff0c\u63d0\u793a\u5728\u77e5\u8bc6\u63d0\u53d6\u4e2d\u7684\u91cd\u8981\u6027\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u3002", "keywords": "\u77e5\u8bc6\u63d0\u53d6,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u76d1\u7763\u5fae\u8c03,\u4e0a\u4e0b\u6587\u5b66\u4e60,\u601d\u7ef4\u94fe\u63d0\u793a"}}
{"id": "2505.23194", "pdf": "https://arxiv.org/pdf/2505.23194", "abs": "https://arxiv.org/abs/2505.23194", "authors": ["Shiwei Li", "Xiandi Luo", "Xing Tang", "Haozhao Wang", "Hao Chen", "Weihong Luo", "Yuhua Li", "Xiuqiang He", "Ruixuan Li"], "title": "Beyond Zero Initialization: Investigating the Impact of Non-Zero Initialization on LoRA Fine-Tuning Dynamics", "categories": ["cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Low-rank adaptation (LoRA) is a widely used parameter-efficient fine-tuning\nmethod. In standard LoRA layers, one of the matrices, $A$ or $B$, is\ninitialized to zero, ensuring that fine-tuning starts from the pretrained\nmodel. However, there is no theoretical support for this practice. In this\npaper, we investigate the impact of non-zero initialization on LoRA's\nfine-tuning dynamics from an infinite-width perspective. Our analysis reveals\nthat, compared to zero initialization, simultaneously initializing $A$ and $B$\nto non-zero values improves LoRA's robustness to suboptimal learning rates,\nparticularly smaller ones. Further analysis indicates that although the\nnon-zero initialization of $AB$ introduces random noise into the pretrained\nweight, it generally does not affect fine-tuning performance. In other words,\nfine-tuning does not need to strictly start from the pretrained model. The\nvalidity of our findings is confirmed through extensive experiments across\nvarious models and datasets. The code is available at\nhttps://github.com/Leopold1423/non_zero_lora-icml25.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5728LoRA\u5fae\u8c03\u4e2d\uff0c\u975e\u96f6\u521d\u59cb\u5316\u77e9\u9635A\u548cB\u6bd4\u96f6\u521d\u59cb\u5316\u66f4\u4f18\uff0c\u80fd\u63d0\u5347\u5bf9\u6b21\u4f18\u5b66\u4e60\u7387\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "motivation": "\u6807\u51c6LoRA\u5c42\u4e2d\u77e9\u9635A\u6216B\u521d\u59cb\u5316\u4e3a\u96f6\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\uff0c\u672c\u7814\u7a76\u63a2\u8ba8\u975e\u96f6\u521d\u59cb\u5316\u5bf9\u5fae\u8c03\u52a8\u6001\u7684\u5f71\u54cd\u3002", "method": "\u4ece\u65e0\u9650\u5bbd\u5ea6\u7684\u89d2\u5ea6\u5206\u6790\u975e\u96f6\u521d\u59cb\u5316\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u975e\u96f6\u521d\u59cb\u5316\u63d0\u5347\u4e86\u5bf9\u6b21\u4f18\u5b66\u4e60\u7387\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u968f\u673a\u566a\u58f0\u4e0d\u5f71\u54cd\u5fae\u8c03\u6027\u80fd\u3002", "conclusion": "\u5fae\u8c03\u65e0\u9700\u4e25\u683c\u4ece\u9884\u8bad\u7ec3\u6a21\u578b\u5f00\u59cb\uff0c\u975e\u96f6\u521d\u59cb\u5316\u662f\u53ef\u884c\u7684\u3002", "keywords": "LoRA, \u53c2\u6570\u9ad8\u6548\u5fae\u8c03, \u521d\u59cb\u5316, \u9c81\u68d2\u6027, \u5b66\u4e60\u7387"}}
{"id": "2505.23420", "pdf": "https://arxiv.org/pdf/2505.23420", "abs": "https://arxiv.org/abs/2505.23420", "authors": ["Marco Gaido", "Sara Papi", "Luisa Bentivogli", "Alessio Brutti", "Mauro Cettolo", "Roberto Gretter", "Marco Matassoni", "Mohamed Nabih", "Matteo Negri"], "title": "The Warmup Dilemma: How Learning Rate Strategies Impact Speech-to-Text Model Convergence", "categories": ["cs.CL"], "comment": "Accepted to IWSLT 2025", "summary": "Training large-scale models presents challenges not only in terms of resource\nrequirements but also in terms of their convergence. For this reason, the\nlearning rate (LR) is often decreased when the size of a model is increased.\nSuch a simple solution is not enough in the case of speech-to-text (S2T)\ntrainings, where evolved and more complex variants of the Transformer\narchitecture -- e.g., Conformer or Branchformer -- are used in light of their\nbetter performance. As a workaround, OWSM designed a double linear warmup of\nthe LR, increasing it to a very small value in the first phase before updating\nit to a higher value in the second phase. While this solution worked well in\npractice, it was not compared with alternative solutions, nor was the impact on\nthe final performance of different LR warmup schedules studied. This paper\nfills this gap, revealing that i) large-scale S2T trainings demand a\nsub-exponential LR warmup, and ii) a higher LR in the warmup phase accelerates\ninitial convergence, but it does not boost final performance.", "AI": {"tldr": "The paper explores learning rate (LR) warmup schedules for large-scale speech-to-text models, finding that a sub-exponential warmup is optimal and that higher initial LRs speed up early convergence but not final performance.", "motivation": "To address the challenges of training large-scale speech-to-text models, especially the need for effective LR warmup schedules, as simple LR reduction is insufficient for complex architectures like Conformer or Branchformer.", "method": "The study compares different LR warmup schedules, including a double linear warmup, and analyzes their impact on model convergence and final performance.", "result": "It reveals that sub-exponential LR warmup is best for large-scale S2T training, and while higher initial LRs accelerate early convergence, they do not improve final model performance.", "conclusion": "Optimal LR warmup strategies, specifically sub-exponential schedules, are crucial for efficient training of large-scale speech-to-text models without compromising final accuracy.", "keywords": "learning rate, speech-to-text, large-scale training, warmup schedules, Transformer architectures"}}
{"id": "2505.23195", "pdf": "https://arxiv.org/pdf/2505.23195", "abs": "https://arxiv.org/abs/2505.23195", "authors": ["Lifan Zhao", "Yanyan Shen", "Zhaoyang Liu", "Xue Wang", "Jiaji Deng"], "title": "Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning", "categories": ["cs.LG", "cs.AI"], "comment": "Manuscript with fixed typos and figures", "summary": "Scaling laws motivate the development of Time Series Foundation Models\n(TSFMs) that pre-train vast parameters and achieve remarkable zero-shot\nforecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot\nconsistently outperform smaller, specialized models trained on full-shot\ndownstream data. A key question is how to realize effective adaptation of TSFMs\nfor a target forecasting task. Through empirical studies on various TSFMs, the\npre-trained models often exhibit inherent sparsity and redundancy in\ncomputation, suggesting that TSFMs have learned to activate task-relevant\nnetwork substructures to accommodate diverse forecasting tasks. To preserve\nthis valuable prior knowledge, we propose a structured pruning method to\nregularize the subsequent fine-tuning process by focusing it on a more relevant\nand compact parameter space. Extensive experiments on seven TSFMs and six\nbenchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly\nimproves forecasting performance compared to fine-tuning original models. This\n\"prune-then-finetune\" paradigm often enables TSFMs to achieve state-of-the-art\nperformance and surpass strong specialized baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\u7684\u6709\u6548\u9002\u5e94\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u66f4\u76f8\u5173\u548c\u7d27\u51d1\u7684\u53c2\u6570\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1TSFMs\u5728\u96f6\u6837\u672c\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5fae\u8c03\u540e\u4ecd\u65e0\u6cd5\u7a33\u5b9a\u8d85\u8d8a\u5c0f\u578b\u4e13\u4e1a\u5316\u6a21\u578b\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u6709\u6548\u9002\u5e94TSFMs\u4ee5\u63d0\u5347\u76ee\u6807\u9884\u6d4b\u4efb\u52a1\u6027\u80fd\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0TSFMs\u5177\u6709\u8ba1\u7b97\u7a00\u758f\u6027\u548c\u5197\u4f59\u6027\uff0c\u63d0\u51fa\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff0c\u4ee5\u4fdd\u7559\u5148\u9a8c\u77e5\u8bc6\u5e76\u96c6\u4e2d\u5728\u66f4\u76f8\u5173\u7684\u53c2\u6570\u7a7a\u95f4\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u8fc7\u526a\u679d\u548c\u5fae\u8c03\u7684\u8f83\u5c0fTSFM\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u539f\u59cb\u6a21\u578b\uff0c\u751a\u81f3\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u8d85\u8d8a\u4e13\u4e1a\u57fa\u7ebf\u3002", "conclusion": "\u201c\u5148\u526a\u679d\u540e\u5fae\u8c03\u201d\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347TSFMs\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u4f7f\u5176\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4f20\u7edf\u4e13\u4e1a\u6a21\u578b\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFM\uff09, \u7ed3\u6784\u5316\u526a\u679d, \u5fae\u8c03, \u9884\u6d4b\u6027\u80fd, \u7a00\u758f\u6027"}}
{"id": "2505.23461", "pdf": "https://arxiv.org/pdf/2505.23461", "abs": "https://arxiv.org/abs/2505.23461", "authors": ["Chuanyuan Tan", "Wenbiao Shao", "Hao Xiong", "Tong Zhu", "Zhenhua Liu", "Kai Shi", "Wenliang Chen"], "title": "UAQFact: Evaluating Factual Knowledge Utilization of LLMs on Unanswerable Questions", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Handling unanswerable questions (UAQ) is crucial for LLMs, as it helps\nprevent misleading responses in complex situations. While previous studies have\nbuilt several datasets to assess LLMs' performance on UAQ, these datasets lack\nfactual knowledge support, which limits the evaluation of LLMs' ability to\nutilize their factual knowledge when handling UAQ. To address the limitation,\nwe introduce a new unanswerable question dataset UAQFact, a bilingual dataset\nwith auxiliary factual knowledge created from a Knowledge Graph. Based on\nUAQFact, we further define two new tasks to measure LLMs' ability to utilize\ninternal and external factual knowledge, respectively. Our experimental results\nacross multiple LLM series show that UAQFact presents significant challenges,\nas LLMs do not consistently perform well even when they have factual knowledge\nstored. Additionally, we find that incorporating external knowledge may enhance\nperformance, but LLMs still cannot make full use of the knowledge which may\nresult in incorrect responses.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u65b0\u6570\u636e\u96c6UAQFact\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u4e0d\u53ef\u7b54\u95ee\u9898\u65f6\u5229\u7528\u4e8b\u5b9e\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u4e0d\u53ef\u7b54\u95ee\u9898\u7684\u6570\u636e\u96c6\u7f3a\u4e4f\u4e8b\u5b9e\u77e5\u8bc6\u652f\u6301\uff0c\u9650\u5236\u4e86\u5bf9\u5176\u5229\u7528\u77e5\u8bc6\u80fd\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u53cc\u8bed\u6570\u636e\u96c6UAQFact\uff0c\u5e76\u5b9a\u4e49\u4e24\u9879\u65b0\u4efb\u52a1\u5206\u522b\u8bc4\u4f30\u6a21\u578b\u5229\u7528\u5185\u90e8\u548c\u5916\u90e8\u77e5\u8bc6\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5177\u5907\u76f8\u5173\u77e5\u8bc6\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728UAQFact\u4e0a\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u5916\u90e8\u77e5\u8bc6\u53ef\u63d0\u5347\u6027\u80fd\u4f46\u672a\u5145\u5206\u5229\u7528\u3002", "conclusion": "UAQFact\u4e3a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u5229\u7528\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "keywords": "\u4e0d\u53ef\u7b54\u95ee\u9898\u3001\u4e8b\u5b9e\u77e5\u8bc6\u3001\u77e5\u8bc6\u56fe\u8c31\u3001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u8bc4\u4f30"}}
{"id": "2505.23223", "pdf": "https://arxiv.org/pdf/2505.23223", "abs": "https://arxiv.org/abs/2505.23223", "authors": ["Xingyuan Pan", "Chenlu Ye", "Joseph Melkonian", "Jiaqi W. Ma", "Tong Zhang"], "title": "Daunce: Data Attribution through Uncertainty Estimation", "categories": ["cs.LG"], "comment": null, "summary": "Training data attribution (TDA) methods aim to identify which training\nexamples influence a model's predictions on specific test data most. By\nquantifying these influences, TDA supports critical applications such as data\ndebugging, curation, and valuation. Gradient-based TDA methods rely on\ngradients and second-order information, limiting their applicability at scale.\nWhile recent random projection-based methods improve scalability, they often\nsuffer from degraded attribution accuracy. Motivated by connections between\nuncertainty and influence functions, we introduce Daunce - a simple yet\neffective data attribution approach through uncertainty estimation. Our method\noperates by fine-tuning a collection of perturbed models and computing the\ncovariance of per-example losses across these models as the attribution score.\nDaunce is scalable to large language models (LLMs) and achieves more accurate\nattribution compared to existing TDA methods. We validate Daunce on tasks\nranging from vision tasks to LLM fine-tuning, and further demonstrate its\ncompatibility with black-box model access. Applied to OpenAI's GPT models, our\nmethod achieves, to our knowledge, the first instance of data attribution on\nproprietary LLMs.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Daunce\u65b9\u6cd5\uff0c\u4e00\u79cd\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5b9e\u73b0\u6570\u636e\u5f52\u56e0\u7684\u7b80\u5355\u6709\u6548\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u89c6\u89c9\u4efb\u52a1\u548cLLM\u5fae\u8c03\u4e2d\u7684\u51c6\u786e\u6027\u548c\u9002\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u68af\u5ea6\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u4e0a\u5b58\u5728\u9650\u5236\uff0c\u800c\u968f\u673a\u6295\u5f71\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u4e0a\u6709\u6240\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u548c\u5f71\u54cd\u51fd\u6570\u7684\u6982\u5ff5\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u3002", "method": "Daunce\u65b9\u6cd5\u901a\u8fc7\u5fae\u8c03\u4e00\u7ec4\u6270\u52a8\u6a21\u578b\uff0c\u8ba1\u7b97\u8fd9\u4e9b\u6a21\u578b\u4e0a\u6bcf\u4e2a\u6837\u672c\u635f\u5931\u7684\u534f\u65b9\u5dee\u4f5c\u4e3a\u5f52\u56e0\u5206\u6570\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u8bad\u7ec3\u6570\u636e\u5f52\u56e0\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86Daunce\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT\uff09\u4e0a\u7684\u6709\u6548\u6027\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5bf9\u4e13\u6709LLM\u7684\u6570\u636e\u5f52\u56e0\uff0c\u4e14\u5728\u5f52\u56e0\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Daunce\u662f\u4e00\u79cd\u7b80\u5355\u3001\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\uff0c\u5305\u62ec\u9ed1\u76d2\u6a21\u578b\u3002", "keywords": "data attribution, uncertainty estimation, large language models, influence functions, model fine-tuning"}}
{"id": "2505.23477", "pdf": "https://arxiv.org/pdf/2505.23477", "abs": "https://arxiv.org/abs/2505.23477", "authors": ["Krithik Vishwanath", "Anton Alyakin", "Mrigayu Ghosh", "Jin Vivian Lee", "Daniel Alexander Alber", "Karl L. Sangwon", "Douglas Kondziolka", "Eric Karl Oermann"], "title": "Evaluating the performance and fragility of large language models on the self-assessment for neurological surgeons", "categories": ["cs.CL"], "comment": "22 pages, 3 main figures, 3 supplemental figures", "summary": "The Congress of Neurological Surgeons Self-Assessment for Neurological\nSurgeons (CNS-SANS) questions are widely used by neurosurgical residents to\nprepare for written board examinations. Recently, these questions have also\nserved as benchmarks for evaluating large language models' (LLMs) neurosurgical\nknowledge. This study aims to assess the performance of state-of-the-art LLMs\non neurosurgery board-like questions and to evaluate their robustness to the\ninclusion of distractor statements. A comprehensive evaluation was conducted\nusing 28 large language models. These models were tested on 2,904 neurosurgery\nboard examination questions derived from the CNS-SANS. Additionally, the study\nintroduced a distraction framework to assess the fragility of these models. The\nframework incorporated simple, irrelevant distractor statements containing\npolysemous words with clinical meanings used in non-clinical contexts to\ndetermine the extent to which such distractions degrade model performance on\nstandard medical benchmarks. 6 of the 28 tested LLMs achieved board-passing\noutcomes, with the top-performing models scoring over 15.7% above the passing\nthreshold. When exposed to distractions, accuracy across various model\narchitectures was significantly reduced-by as much as 20.4%-with one model\nfailing that had previously passed. Both general-purpose and medical\nopen-source models experienced greater performance declines compared to\nproprietary variants when subjected to the added distractors. While current\nLLMs demonstrate an impressive ability to answer neurosurgery board-like exam\nquestions, their performance is markedly vulnerable to extraneous, distracting\ninformation. These findings underscore the critical need for developing novel\nmitigation strategies aimed at bolstering LLM resilience against in-text\ndistractions, particularly for safe and effective clinical deployment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e8628\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u795e\u7ecf\u5916\u79d1\u8003\u8bd5\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b06\u4e2a\u6a21\u578b\u8fbe\u5230\u4e86\u53ca\u683c\u6c34\u5e73\uff0c\u4f46\u52a0\u5165\u5e72\u6270\u4fe1\u606f\u540e\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u795e\u7ecf\u5916\u79d1\u8003\u8bd5\u9898\u4e0a\u7684\u8868\u73b0\u53ca\u5176\u5bf9\u5e72\u6270\u4fe1\u606f\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u4e34\u5e8a\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u6d4b\u8bd528\u4e2aLLM\u57282,904\u9053\u795e\u7ecf\u5916\u79d1\u8003\u8bd5\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5f15\u5165\u5305\u542b\u65e0\u5173\u5e72\u6270\u7684\u6846\u67b6\u5206\u6790\u6a21\u578b\u8106\u5f31\u6027\u3002", "result": "6\u4e2a\u6a21\u578b\u8fbe\u5230\u53ca\u683c\u6c34\u5e73\uff0c\u4f46\u5e72\u6270\u4fe1\u606f\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe20.4%\u3002\u5f00\u6e90\u6a21\u578b\u6bd4\u4e13\u6709\u6a21\u578b\u66f4\u6613\u53d7\u5f71\u54cd\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u795e\u7ecf\u5916\u79d1\u8003\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6613\u53d7\u5e72\u6270\u4fe1\u606f\u5f71\u54cd\uff0c\u9700\u5f00\u53d1\u65b0\u7b56\u7565\u63d0\u5347\u9c81\u68d2\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u795e\u7ecf\u5916\u79d1,\u8003\u8bd5\u9898,\u5e72\u6270\u4fe1\u606f,\u9c81\u68d2\u6027"}}
{"id": "2505.22976", "pdf": "https://arxiv.org/pdf/2505.22976", "abs": "https://arxiv.org/abs/2505.22976", "authors": ["Kewei Lian", "Shaofei Cai", "Yilun Du", "Yitao Liang"], "title": "Toward Memory-Aided World Models: Benchmarking via Spatial Consistency", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The ability to simulate the world in a spatially consistent manner is a\ncrucial requirements for effective world models. Such a model enables\nhigh-quality visual generation, and also ensures the reliability of world\nmodels for downstream tasks such as simulation and planning. Designing a memory\nmodule is a crucial component for addressing spatial consistency: such a model\nmust not only retain long-horizon observational information, but also enables\nthe construction of explicit or implicit internal spatial representations.\nHowever, there are no dataset designed to promote the development of memory\nmodules by explicitly enforcing spatial consistency constraints. Furthermore,\nmost existing benchmarks primarily emphasize visual coherence or generation\nquality, neglecting the requirement of long-range spatial consistency. To\nbridge this gap, we construct a dataset and corresponding benchmark by sampling\n150 distinct locations within the open-world environment of Minecraft,\ncollecting about 250 hours (20 million frames) of loop-based navigation videos\nwith actions. Our dataset follows a curriculum design of sequence lengths,\nallowing models to learn spatial consistency on increasingly complex navigation\ntrajectories. Furthermore, our data collection pipeline is easily extensible to\nnew Minecraft environments and modules. Four representative world model\nbaselines are evaluated on our benchmark. Dataset, benchmark, and code are\nopen-sourced to support future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u7a7a\u95f4\u4e00\u81f4\u6027\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u4fc3\u8fdb\u4e16\u754c\u6a21\u578b\u4e2d\u7684\u8bb0\u5fc6\u6a21\u5757\u5f00\u53d1\uff0c\u7279\u522b\u5173\u6ce8\u957f\u671f\u7a7a\u95f4\u4e00\u81f4\u6027\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u5bf9\u7a7a\u95f4\u4e00\u81f4\u6027\u7684\u660e\u786e\u8981\u6c42\uff0c\u5927\u591a\u6570\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u5173\u6ce8\u89c6\u89c9\u4e00\u81f4\u6027\u6216\u751f\u6210\u8d28\u91cf\uff0c\u5ffd\u89c6\u957f\u671f\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "method": "\u901a\u8fc7\u5728Minecraft\u5f00\u653e\u4e16\u754c\u4e2d\u91c7\u6837150\u4e2a\u4e0d\u540c\u4f4d\u7f6e\uff0c\u6536\u96c6250\u5c0f\u65f6\uff082000\u4e07\u5e27\uff09\u7684\u5faa\u73af\u5bfc\u822a\u89c6\u9891\u6570\u636e\uff0c\u5e76\u91c7\u7528\u8bfe\u7a0b\u8bbe\u8ba1\u9010\u6b65\u589e\u52a0\u5e8f\u5217\u957f\u5ea6\u3002", "result": "\u6784\u5efa\u4e86\u53ef\u6269\u5c55\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u56db\u79cd\u4ee3\u8868\u6027\u4e16\u754c\u6a21\u578b\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u586b\u8865\u4e86\u7a7a\u95f4\u4e00\u81f4\u6027\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "keywords": "\u4e16\u754c\u6a21\u578b\u3001\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u8bb0\u5fc6\u6a21\u5757\u3001Minecraft\u3001\u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2505.23225", "pdf": "https://arxiv.org/pdf/2505.23225", "abs": "https://arxiv.org/abs/2505.23225", "authors": ["Fabiano Veglianti", "Flavio Giorgi", "Fabrizio Silvestri", "Gabriele Tolomei"], "title": "Generalizability vs. Counterfactual Explainability Trade-Off", "categories": ["cs.LG"], "comment": "9 pages, 4 figures, plus appendix. arXiv admin note: text overlap\n  with arXiv:2502.09193", "summary": "In this work, we investigate the relationship between model generalization\nand counterfactual explainability in supervised learning. We introduce the\nnotion of $\\varepsilon$-valid counterfactual probability ($\\varepsilon$-VCP) --\nthe probability of finding perturbations of a data point within its\n$\\varepsilon$-neighborhood that result in a label change. We provide a\ntheoretical analysis of $\\varepsilon$-VCP in relation to the geometry of the\nmodel's decision boundary, showing that $\\varepsilon$-VCP tends to increase\nwith model overfitting. Our findings establish a rigorous connection between\npoor generalization and the ease of counterfactual generation, revealing an\ninherent trade-off between generalization and counterfactual explainability.\nEmpirical results validate our theory, suggesting $\\varepsilon$-VCP as a\npractical proxy for quantitatively characterizing overfitting.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6a21\u578b\u6cdb\u5316\u4e0e\u53cd\u4e8b\u5b9e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5f15\u5165\u03b5-VCP\u6982\u5ff5\uff0c\u7406\u8bba\u5206\u6790\u4e86\u5176\u4e0e\u51b3\u7b56\u8fb9\u754c\u51e0\u4f55\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u03b5-VCP\u968f\u6a21\u578b\u8fc7\u62df\u5408\u589e\u52a0\uff0c\u63ed\u793a\u4e86\u6cdb\u5316\u4e0e\u53cd\u4e8b\u5b9e\u53ef\u89e3\u91ca\u6027\u7684\u6743\u8861\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u3002", "motivation": "\u7814\u7a76\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0e\u53cd\u4e8b\u5b9e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u65e8\u5728\u7406\u89e3\u8fc7\u62df\u5408\u5982\u4f55\u5f71\u54cd\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u96be\u6613\u7a0b\u5ea6\u3002", "method": "\u5f15\u5165\u03b5-VCP\uff08\u03b5-\u6709\u6548\u53cd\u4e8b\u5b9e\u6982\u7387\uff09\u6982\u5ff5\uff0c\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u5176\u4e0e\u6a21\u578b\u51b3\u7b56\u8fb9\u754c\u51e0\u4f55\u7684\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u3002", "result": "\u03b5-VCP\u968f\u6a21\u578b\u8fc7\u62df\u5408\u589e\u52a0\uff0c\u8868\u660e\u6cdb\u5316\u80fd\u529b\u4e0e\u53cd\u4e8b\u5b9e\u53ef\u89e3\u91ca\u6027\u5b58\u5728\u6743\u8861\u5173\u7cfb\uff0c\u03b5-VCP\u53ef\u4f5c\u4e3a\u91cf\u5316\u8fc7\u62df\u5408\u7684\u5b9e\u7528\u6307\u6807\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u8fc7\u62df\u5408\u4e0e\u53cd\u4e8b\u5b9e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u5b58\u5728\u5185\u5728\u77db\u76fe\uff0c\u4e3a\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u5b9a\u91cf\u89c6\u89d2\u3002", "keywords": "\u6a21\u578b\u6cdb\u5316\u3001\u53cd\u4e8b\u5b9e\u53ef\u89e3\u91ca\u6027\u3001\u03b5-VCP\u3001\u51b3\u7b56\u8fb9\u754c\u3001\u8fc7\u62df\u5408"}}
{"id": "2505.23480", "pdf": "https://arxiv.org/pdf/2505.23480", "abs": "https://arxiv.org/abs/2505.23480", "authors": ["Keqin Peng", "Liang Ding", "Yuanxin Ouyang", "Meng Fang", "Dacheng Tao"], "title": "Revisiting Overthinking in Long Chain-of-Thought from the Perspective of Self-Doubt", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning Large Language Models (RLLMs) have demonstrated impressive\nperformance on complex tasks, largely due to the adoption of Long\nChain-of-Thought (Long CoT) reasoning. However, they often exhibit overthinking\n-- performing unnecessary reasoning steps even after arriving at the correct\nanswer. Prior work has largely focused on qualitative analyses of overthinking\nthrough sample-based observations of long CoTs. In contrast, we present a\nquantitative analysis of overthinking from the perspective of self-doubt,\ncharacterized by excessive token usage devoted to re-verifying already-correct\nanswer. We find that self-doubt significantly contributes to overthinking. In\nresponse, we introduce a simple and effective prompting method to reduce the\nmodel's over-reliance on input questions, thereby avoiding self-doubt.\nSpecifically, we first prompt the model to question the validity of the input\nquestion, and then respond concisely based on the outcome of that evaluation.\nExperiments on three mathematical reasoning tasks and four datasets with\nmissing premises demonstrate that our method substantially reduces answer\nlength and yields significant improvements across nearly all datasets upon 4\nwidely-used RLLMs. Further analysis demonstrates that our method effectively\nminimizes the number of reasoning steps and reduces self-doubt.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\uff08RLLM\uff09\u5728\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u7684\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\uff08\u5982\u81ea\u6211\u6000\u7591\u5bfc\u81f4\u7684\u591a\u4f59\u63a8\u7406\u6b65\u9aa4\uff09\uff0c\u4ece\u800c\u63d0\u5347\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "RLLMs\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5e38\u56e0\u2018\u8fc7\u5ea6\u601d\u8003\u2019\uff08\u5982\u81ea\u6211\u6000\u7591\uff09\u800c\u8fdb\u884c\u4e0d\u5fc5\u8981\u7684\u63a8\u7406\u6b65\u9aa4\u3002\u6b64\u524d\u7814\u7a76\u591a\u4e3a\u5b9a\u6027\u5206\u6790\uff0c\u672c\u6587\u5219\u4ece\u81ea\u6211\u6000\u7591\u7684\u5b9a\u91cf\u89d2\u5ea6\u5207\u5165\uff0c\u65e8\u5728\u51cf\u5c11\u8fd9\u79cd\u4f4e\u6548\u73b0\u8c61\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u7684\u63d0\u793a\u65b9\u6cd5\uff1a\u5148\u8ba9\u6a21\u578b\u8bc4\u4f30\u8f93\u5165\u95ee\u9898\u7684\u6709\u6548\u6027\uff0c\u518d\u6839\u636e\u8bc4\u4f30\u7ed3\u679c\u7b80\u6d01\u56de\u7b54\u3002\u6b64\u65b9\u6cd5\u65e8\u5728\u51cf\u5c11\u6a21\u578b\u5bf9\u95ee\u9898\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u4ece\u800c\u907f\u514d\u81ea\u6211\u6000\u7591\u3002", "result": "\u57283\u4e2a\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u548c4\u4e2a\u7f3a\u5931\u524d\u63d0\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u7f29\u77ed\u4e86\u7b54\u6848\u957f\u5ea6\uff0c\u5e76\u57284\u79cdRLLMs\u4e0a\u51e0\u4e4e\u5168\u90e8\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u63a8\u7406\u6b65\u9aa4\u548c\u81ea\u6211\u6000\u7591\u3002", "conclusion": "\u63d0\u51fa\u7684\u63d0\u793a\u65b9\u6cd5\u901a\u8fc7\u6291\u5236\u81ea\u6211\u6000\u7591\u6709\u6548\u51cf\u5c11\u4e86\u8fc7\u5ea6\u601d\u8003\uff0c\u63d0\u5347\u4e86RLLMs\u7684\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u957f\u94fe\u601d\u7ef4\u63a8\u7406, \u8fc7\u5ea6\u601d\u8003, \u81ea\u6211\u6000\u7591, \u63d0\u793a\u65b9\u6cd5"}}
{"id": "2505.23228", "pdf": "https://arxiv.org/pdf/2505.23228", "abs": "https://arxiv.org/abs/2505.23228", "authors": ["Wanfu Gao", "Jun Gao", "Qingqi Han", "Hanlin Pan", "Kunpeng Liu"], "title": "Graph Random Walk with Feature-Label Space Alignment: A Multi-Label Feature Selection Method", "categories": ["cs.LG"], "comment": null, "summary": "The rapid growth in feature dimension may introduce implicit associations\nbetween features and labels in multi-label datasets, making the relationships\nbetween features and labels increasingly complex. Moreover, existing methods\noften adopt low-dimensional linear decomposition to explore the associations\nbetween features and labels. However, linear decomposition struggles to capture\ncomplex nonlinear associations and may lead to misalignment between the feature\nspace and the label space. To address these two critical challenges, we propose\ninnovative solutions. First, we design a random walk graph that integrates\nfeature-feature, label-label, and feature-label relationships to accurately\ncapture nonlinear and implicit indirect associations, while optimizing the\nlatent representations of associations between features and labels after\nlow-rank decomposition. Second, we align the variable spaces by leveraging\nlow-dimensional representation coefficients, while preserving the manifold\nstructure between the original high-dimensional multi-label data and the\nlow-dimensional representation space. Extensive experiments and ablation\nstudies conducted on seven benchmark datasets and three representative datasets\nusing various evaluation metrics demonstrate the superiority of the proposed\nmethod\\footnote{Code: https://github.com/Heilong623/-GRW-}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u968f\u673a\u6e38\u8d70\u56fe\u6574\u5408\u7279\u5f81-\u7279\u5f81\u3001\u6807\u7b7e-\u6807\u7b7e\u53ca\u7279\u5f81-\u6807\u7b7e\u5173\u7cfb\u7684\u65b9\u6cd5\uff0c\u4ee5\u6355\u6349\u975e\u7ebf\u6027\u5173\u8054\uff0c\u5e76\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3\u4f18\u5316\u6f5c\u5728\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u7279\u5f81\u4e0e\u6807\u7b7e\u95f4\u590d\u6742\u975e\u7ebf\u6027\u5173\u8054\u7684\u6311\u6218\u3002", "motivation": "\u7531\u4e8e\u591a\u6807\u7b7e\u6570\u636e\u96c6\u4e2d\u7279\u5f81\u4e0e\u6807\u7b7e\u95f4\u9690\u542b\u5173\u8054\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u73b0\u6709\u7ebf\u6027\u5206\u89e3\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u975e\u7ebf\u6027\u5173\u8054\uff0c\u5bfc\u81f4\u7279\u5f81\u7a7a\u95f4\u4e0e\u6807\u7b7e\u7a7a\u95f4\u4e0d\u5bf9\u9f50\u3002", "method": "\u8bbe\u8ba1\u4e86\u968f\u673a\u6e38\u8d70\u56fe\u6574\u5408\u591a\u7c7b\u5173\u7cfb\uff0c\u6355\u6349\u975e\u7ebf\u6027\u95f4\u63a5\u5173\u8054\uff1b\u5229\u7528\u4f4e\u79e9\u5206\u89e3\u4f18\u5316\u6f5c\u5728\u8868\u793a\uff0c\u540c\u65f6\u901a\u8fc7\u4f4e\u7ef4\u8868\u793a\u7cfb\u6570\u5bf9\u9f50\u53d8\u91cf\u7a7a\u95f4\u5e76\u4fdd\u6301\u6d41\u5f62\u7ed3\u6784\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u4ee3\u8868\u6027\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u8bc4\u4ef7\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6807\u7b7e\u6570\u636e\u4e2d\u7279\u5f81\u4e0e\u6807\u7b7e\u95f4\u7684\u975e\u7ebf\u6027\u5173\u8054\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "keywords": "\u591a\u6807\u7b7e\u5b66\u4e60, \u975e\u7ebf\u6027\u5173\u8054, \u968f\u673a\u6e38\u8d70\u56fe, \u4f4e\u79e9\u5206\u89e3, \u6d41\u5f62\u7ed3\u6784"}}
{"id": "2505.23494", "pdf": "https://arxiv.org/pdf/2505.23494", "abs": "https://arxiv.org/abs/2505.23494", "authors": ["Nicol Visser", "Herman Kamper"], "title": "Spoken Language Modeling with Duration-Penalized Self-Supervised Units", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Spoken language models (SLMs) operate on acoustic units obtained by\ndiscretizing self-supervised speech representations. Although the\ncharacteristics of these units directly affect performance, the interaction\nbetween codebook size and unit coarseness (i.e., duration) remains unexplored.\nWe investigate SLM performance as we vary codebook size and unit coarseness\nusing the simple duration-penalized dynamic programming (DPDP) method. New\nanalyses are performed across different linguistic levels. At the phone and\nword levels, coarseness provides little benefit, as long as the codebook size\nis chosen appropriately. However, when producing whole sentences in a\nresynthesis task, SLMs perform better with coarser units. In lexical and\nsyntactic language modeling tasks, coarser units also give higher accuracies at\nlower bitrates. We therefore show that coarser units aren't always better, but\nthat DPDP is a simple and efficient way to obtain coarser units for the tasks\nwhere they are beneficial.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u4e2d\u7801\u672c\u5927\u5c0f\u548c\u5355\u5143\u7c97\u7cd9\u5ea6\uff08\u5373\u6301\u7eed\u65f6\u95f4\uff09\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7c97\u7cd9\u5ea6\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u4e0d\u540c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5DPDP\u6765\u4f18\u5316\u5355\u5143\u9009\u62e9\u3002", "motivation": "\u63a2\u7d22\u7801\u672c\u5927\u5c0f\u548c\u8bed\u97f3\u5355\u5143\u7c97\u7cd9\u5ea6\u5982\u4f55\u5f71\u54cdSLM\u6027\u80fd\uff0c\u4ee5\u4f18\u5316\u8bed\u97f3\u8868\u793a\u7684\u9009\u62e9\u3002", "method": "\u4f7f\u7528\u6301\u7eed\u65f6\u95f4\u60e9\u7f5a\u7684\u52a8\u6001\u89c4\u5212\uff08DPDP\uff09\u65b9\u6cd5\u8c03\u6574\u7801\u672c\u5927\u5c0f\u548c\u5355\u5143\u7c97\u7cd9\u5ea6\uff0c\u5e76\u5728\u4e0d\u540c\u8bed\u8a00\u5c42\u6b21\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5728\u7535\u8bdd\u548c\u5355\u8bcd\u7ea7\u522b\uff0c\u9002\u5f53\u9009\u62e9\u7801\u672c\u5927\u5c0f\u5373\u53ef\uff0c\u7c97\u7cd9\u5ea6\u5e2e\u52a9\u4e0d\u5927\uff1b\u800c\u5728\u53e5\u5b50\u91cd\u6784\u4efb\u52a1\u4e2d\uff0c\u7c97\u7cd9\u5355\u5143\u8868\u73b0\u66f4\u597d\u3002\u8bcd\u6c47\u548c\u53e5\u6cd5\u4efb\u52a1\u4e2d\uff0c\u7c97\u7cd9\u5355\u5143\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u51c6\u786e\u7387\u66f4\u9ad8\u3002", "conclusion": "\u7c97\u7cd9\u5355\u5143\u5e76\u975e\u603b\u662f\u66f4\u597d\uff0c\u4f46DPDP\u662f\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u5728\u9700\u8981\u7c97\u7cd9\u5355\u5143\u7684\u4efb\u52a1\u4e2d\u6709\u6548\u4f18\u5316\u3002", "keywords": "\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u3001\u7801\u672c\u5927\u5c0f\u3001\u5355\u5143\u7c97\u7cd9\u5ea6\u3001DPDP\u3001\u8bed\u97f3\u8868\u793a"}}
{"id": "2505.23244", "pdf": "https://arxiv.org/pdf/2505.23244", "abs": "https://arxiv.org/abs/2505.23244", "authors": ["Emo Todorov"], "title": "Equivalence of stochastic and deterministic policy gradients", "categories": ["cs.LG"], "comment": null, "summary": "Policy gradients in continuous control have been derived for both stochastic\nand deterministic policies. Here we study the relationship between the two. In\na widely-used family of MDPs involving Gaussian control noise and quadratic\ncontrol costs, we show that the stochastic and deterministic policy gradients,\nnatural gradients, and state value functions are identical; while the\nstate-control value functions are different. We then develop a general\nprocedure for constructing an MDP with deterministic policy that is equivalent\nto a given MDP with stochastic policy. The controls of this new MDP are the\nsufficient statistics of the stochastic policy in the original MDP. Our results\nsuggest that policy gradient methods can be unified by approximating state\nvalue functions rather than state-control value functions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8fde\u7eed\u63a7\u5236\u4e2d\u968f\u673a\u7b56\u7565\u4e0e\u786e\u5b9a\u6027\u7b56\u7565\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u4e24\u8005\u5728\u7279\u5b9aMDP\u5bb6\u65cf\u4e2d\u5177\u6709\u76f8\u540c\u7684\u68af\u5ea6\u3001\u81ea\u7136\u68af\u5ea6\u548c\u72b6\u6001\u4ef7\u503c\u51fd\u6570\uff0c\u4f46\u72b6\u6001\u63a7\u5236\u4ef7\u503c\u51fd\u6570\u4e0d\u540c\u3002", "motivation": "\u63a2\u8ba8\u968f\u673a\u7b56\u7565\u4e0e\u786e\u5b9a\u6027\u7b56\u7565\u5728\u8fde\u7eed\u63a7\u5236\u4e2d\u7684\u5173\u7cfb\uff0c\u4ee5\u7edf\u4e00\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u3002", "method": "\u5728\u6d89\u53ca\u9ad8\u65af\u63a7\u5236\u566a\u58f0\u548c\u4e8c\u6b21\u63a7\u5236\u6210\u672c\u7684MDP\u5bb6\u65cf\u4e2d\u5206\u6790\u4e24\u79cd\u7b56\u7565\u7684\u68af\u5ea6\u3001\u81ea\u7136\u68af\u5ea6\u548c\u4ef7\u503c\u51fd\u6570\uff0c\u5e76\u63d0\u51fa\u6784\u5efa\u7b49\u6548MDP\u7684\u4e00\u822c\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u968f\u673a\u548c\u786e\u5b9a\u6027\u7b56\u7565\u7684\u68af\u5ea6\u3001\u81ea\u7136\u68af\u5ea6\u548c\u72b6\u6001\u4ef7\u503c\u51fd\u6570\u76f8\u540c\uff0c\u4f46\u72b6\u6001\u63a7\u5236\u4ef7\u503c\u51fd\u6570\u4e0d\u540c\u3002", "conclusion": "\u901a\u8fc7\u8fd1\u4f3c\u72b6\u6001\u4ef7\u503c\u51fd\u6570\u800c\u975e\u72b6\u6001\u63a7\u5236\u4ef7\u503c\u51fd\u6570\uff0c\u53ef\u4ee5\u7edf\u4e00\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u3002", "keywords": "\u7b56\u7565\u68af\u5ea6,\u8fde\u7eed\u63a7\u5236,\u968f\u673a\u7b56\u7565,\u786e\u5b9a\u6027\u7b56\u7565,MDP"}}
{"id": "2505.23495", "pdf": "https://arxiv.org/pdf/2505.23495", "abs": "https://arxiv.org/abs/2505.23495", "authors": ["Liangliang Zhang", "Zhuorui Jiang", "Hongliang Chi", "Haoyang Chen", "Mohammed Elkoumy", "Fali Wang", "Qiong Wu", "Zhengyi Zhou", "Shirui Pan", "Suhang Wang", "Yao Ma"], "title": "Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages", "summary": "Knowledge Graph Question Answering (KGQA) systems rely on high-quality\nbenchmarks to evaluate complex multi-hop reasoning. However, despite their\nwidespread use, popular datasets such as WebQSP and CWQ suffer from critical\nquality issues, including inaccurate or incomplete ground-truth annotations,\npoorly constructed questions that are ambiguous, trivial, or unanswerable, and\noutdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA\ndatasets, including WebQSP and CWQ, we find that the average factual\ncorrectness rate is only 57 %. To address these issues, we introduce KGQAGen,\nan LLM-in-the-loop framework that systematically resolves these pitfalls.\nKGQAGen combines structured knowledge grounding, LLM-guided generation, and\nsymbolic verification to produce challenging and verifiable QA instances. Using\nKGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in\nWikidata, and evaluate a diverse set of KG-RAG models. Experimental results\ndemonstrate that even state-of-the-art systems struggle on this benchmark,\nhighlighting its ability to expose limitations of existing models. Our findings\nadvocate for more rigorous benchmark construction and position KGQAGen as a\nscalable framework for advancing KGQA evaluation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86KGQAGen\uff0c\u4e00\u4e2a\u7ed3\u5408LLM\u548c\u7b26\u53f7\u9a8c\u8bc1\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u57fa\u51c6\uff0c\u5e76\u6784\u5efa\u4e86KGQAGen-10k\u6570\u636e\u96c6\uff0c\u5c55\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u65b0\u57fa\u51c6\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u57fa\u51c6\uff08\u5982WebQSP\u548cCWQ\uff09\u5b58\u5728\u6807\u6ce8\u4e0d\u51c6\u786e\u3001\u95ee\u9898\u6a21\u7cca\u6216\u65e0\u6cd5\u56de\u7b54\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86KGQA\u7cfb\u7edf\u7684\u8bc4\u4f30\u548c\u53d1\u5c55\u3002", "method": "\u63d0\u51faKGQAGen\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u77e5\u8bc6\u3001LLM\u751f\u6210\u548c\u7b26\u53f7\u9a8c\u8bc1\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u95ee\u7b54\u6837\u672c\u3002", "result": "\u6784\u5efaKGQAGen-10k\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8868\u660e\u5373\u4f7f\u6700\u5148\u8fdb\u7684KGQA\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "KGQAGen\u4e3aKGQA\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9ad8\u8d28\u91cf\u57fa\u51c6\u751f\u6210\u6846\u67b6\uff0c\u547c\u5401\u66f4\u4e25\u683c\u7684\u57fa\u51c6\u6784\u5efa\u6807\u51c6\u3002", "keywords": "Knowledge Graph Question Answering, benchmark quality, LLM, symbolic verification, KGQAGen"}}
{"id": "2505.23246", "pdf": "https://arxiv.org/pdf/2505.23246", "abs": "https://arxiv.org/abs/2505.23246", "authors": ["Honoka Anada", "Tatsuya Kaneko", "Shinya Takamaeda-Yamazaki"], "title": "Measuring Participant Contributions in Decentralized Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning (FL) enables multiple clients to collaboratively train\nmodels without sharing their data. Measuring participant contributions in FL is\ncrucial for incentivizing clients and ensuring transparency. While various\nmethods have been proposed for contribution measurement, they are designed\nexclusively for centralized federated learning (CFL), where a central server\ncollects and aggregates client models, along with evaluating their\ncontributions. Meanwhile, decentralized federated learning (DFL), in which\nclients exchange models directly without a central server, has gained\nsignificant attention for mitigating communication bottlenecks and eliminating\na single point of failure. However, applying existing contribution measurement\nmethods to DFL is challenging due to the presence of multiple global models and\nthe absence of a central server. In this study, we present novel methodologies\nfor measuring participant contributions in DFL. We first propose DFL-Shapley,\nan extension of the Shapley value tailored for DFL, adapting this widely used\nCFL metric to decentralized settings. Given the impracticality of computing the\nideal DFL-Shapley in real-world systems, we introduce DFL-MR, a computable\napproximation that estimates overall contributions by accumulating round-wise\nShapley values. We evaluate DFL-Shapley and DFL-MR across various FL scenarios\nand compare them with existing CFL metrics. The experimental results confirm\nDFL-Shapley as a valid ground-truth metric and demonstrate DFL-MR's proximity\nto DFL-Shapley across various settings, highlighting their effectiveness as\ncontribution metrics in DFL.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u8861\u91cf\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\uff08DFL\uff09\u4e2d\u53c2\u4e0e\u8005\u7684\u8d21\u732e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\uff08CFL\uff09\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u53c2\u4e0e\u8005\u8d21\u732e\u8861\u91cf\u65b9\u6cd5\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86DFL-Shapley\uff08\u57fa\u4e8eShapley\u503c\u7684\u6269\u5c55\uff09\u548cDFL-MR\uff08\u53ef\u8ba1\u7b97\u7684\u8fd1\u4f3c\u65b9\u6cd5\uff09\u6765\u8861\u91cf\u8d21\u732e\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DFL-Shapley\u7684\u6709\u6548\u6027\uff0c\u5e76\u663e\u793aDFL-MR\u80fd\u591f\u63a5\u8fd1DFL-Shapley\u7684\u51c6\u786e\u6027\u3002", "conclusion": "DFL-Shapley\u548cDFL-MR\u4e3aDFL\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8d21\u732e\u8861\u91cf\u65b9\u6cd5\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60,\u53bb\u4e2d\u5fc3\u5316,\u8d21\u732e\u8861\u91cf,Shapley\u503c"}}
{"id": "2505.23538", "pdf": "https://arxiv.org/pdf/2505.23538", "abs": "https://arxiv.org/abs/2505.23538", "authors": ["Nawar Turk", "Eeham Khan", "Leila Kosseim"], "title": "CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental Promise Verification", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to SemEval-2025 Task 6 (ACL 2025)", "summary": "This paper presents our approach to the SemEval-2025 Task~6 (PromiseEval),\nwhich focuses on verifying promises in corporate ESG (Environmental, Social,\nand Governance) reports. We explore three model architectures to address the\nfour subtasks of promise identification, supporting evidence assessment,\nclarity evaluation, and verification timing. Our first model utilizes ESG-BERT\nwith task-specific classifier heads, while our second model enhances this\narchitecture with linguistic features tailored for each subtask. Our third\napproach implements a combined subtask model with attention-based sequence\npooling, transformer representations augmented with document metadata, and\nmulti-objective learning. Experiments on the English portion of the ML-Promise\ndataset demonstrate progressive improvement across our models, with our\ncombined subtask approach achieving a leaderboard score of 0.5268,\noutperforming the provided baseline of 0.5227. Our work highlights the\neffectiveness of linguistic feature extraction, attention pooling, and\nmulti-objective learning in promise verification tasks, despite challenges\nposed by class imbalance and limited training data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u9488\u5bf9SemEval-2025 Task 6\uff08PromiseEval\uff09\u7684\u4e09\u79cd\u6a21\u578b\u67b6\u6784\uff0c\u7528\u4e8e\u9a8c\u8bc1\u516c\u53f8ESG\u62a5\u544a\u4e2d\u7684\u627f\u8bfa\uff0c\u5e76\u5728ML-Promise\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u9010\u6b65\u6539\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u516c\u53f8ESG\u62a5\u544a\u4e2d\u7684\u627f\u8bfa\u9a8c\u8bc1\u5bf9\u900f\u660e\u5ea6\u548c\u95ee\u8d23\u5236\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u8bc6\u522b\u3001\u8bc4\u4f30\u3001\u6e05\u6670\u5ea6\u548c\u9a8c\u8bc1\u65f6\u95f4\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528ESG-BERT\u3001\u5e26\u8bed\u8a00\u7279\u5f81\u7684\u589e\u5f3a\u6a21\u578b\u4ee5\u53ca\u7ed3\u5408\u6ce8\u610f\u529b\u6c60\u5316\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u7efc\u5408\u5b50\u4efb\u52a1\u6a21\u578b\u3002", "result": "\u7efc\u5408\u5b50\u4efb\u52a1\u6a21\u578b\u53d6\u5f970.5268\u7684\u9886\u5148\u5206\u6570\uff0c\u4f18\u4e8e\u57fa\u7ebf0.5227\u3002", "conclusion": "\u8bed\u8a00\u7279\u5f81\u63d0\u53d6\u3001\u6ce8\u610f\u529b\u6c60\u5316\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u53ef\u6709\u6548\u63d0\u5347\u627f\u8bfa\u9a8c\u8bc1\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u5c3d\u7ba1\u9762\u4e34\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u6311\u6218\u3002", "keywords": "PromiseEval, ESG, \u627f\u8bfa\u9a8c\u8bc1, BERT, \u591a\u4efb\u52a1\u5b66\u4e60"}}
{"id": "2505.23247", "pdf": "https://arxiv.org/pdf/2505.23247", "abs": "https://arxiv.org/abs/2505.23247", "authors": ["Zonglin Yang", "Zhexuan Gu", "Houduo Qi", "Yancheng Yuan"], "title": "Accelerating RLHF Training with Reward Variance Increase", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) is an essential technique\nfor ensuring that large language models (LLMs) are aligned with human values\nand preferences during the post-training phase. As an effective RLHF approach,\ngroup relative policy optimization (GRPO) has demonstrated success in many\nLLM-based applications. However, efficient GRPO-based RLHF training remains a\nchallenge. Recent studies reveal that a higher reward variance of the initial\npolicy model leads to faster RLHF training. Inspired by this finding, we\npropose a practical reward adjustment model to accelerate RLHF training by\nprovably increasing the reward variance and preserving the relative preferences\nand reward expectation. Our reward adjustment method inherently poses a\nnonconvex optimization problem, which is NP-hard to solve in general. To\novercome the computational challenges, we design a novel $O(n \\log n)$\nalgorithm to find a global solution of the nonconvex reward adjustment model by\nexplicitly characterizing the extreme points of the feasible set. As an\nimportant application, we naturally integrate this reward adjustment model into\nthe GRPO algorithm, leading to a more efficient GRPO with reward variance\nincrease (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we\nprovide an indirect explanation for the empirical effectiveness of GRPO with\nrule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment\nresults demonstrate that the GRPOVI algorithm can significantly improve the\nRLHF training efficiency compared to the original GRPO algorithm.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5956\u52b1\u8c03\u6574\u6a21\u578b\uff0c\u901a\u8fc7\u63d0\u9ad8\u5956\u52b1\u65b9\u5dee\u6765\u52a0\u901fRLHF\u8bad\u7ec3\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9ad8\u6548\u7b97\u6cd5\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u6700\u7ec8\u63d0\u51faGRPOVI\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u5f53\u524dGRPO-based RLHF\u8bad\u7ec3\u6548\u7387\u4e0d\u8db3\uff0c\u7814\u7a76\u53d1\u73b0\u521d\u59cb\u653f\u7b56\u6a21\u578b\u7684\u5956\u52b1\u65b9\u5dee\u8f83\u9ad8\u53ef\u52a0\u901f\u8bad\u7ec3\uff0c\u56e0\u6b64\u63d0\u51fa\u4e00\u79cd\u5956\u52b1\u8c03\u6574\u65b9\u6cd5\u6765\u63d0\u9ad8\u65b9\u5dee\u5e76\u4fdd\u6301\u504f\u597d\u548c\u671f\u671b\u4e0d\u53d8\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u975e\u51f8\u4f18\u5316\u6a21\u578b\uff0c\u901a\u8fc7\u660e\u786e\u8868\u5f81\u53ef\u884c\u96c6\u7684\u6781\u503c\u70b9\uff0c\u63d0\u51faO(n log n)\u7b97\u6cd5\u6c42\u89e3\u5168\u5c40\u6700\u4f18\u89e3\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230GRPO\u7b97\u6cd5\u4e2d\uff0c\u5f62\u6210GRPOVI\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eGRPOVI\u7b97\u6cd5\u6bd4\u539f\u59cbGRPO\u7b97\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86RLHF\u8bad\u7ec3\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u5956\u52b1\u8c03\u6574\u65b9\u6cd5\u548cGRPOVI\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86RLHF\u8bad\u7ec3\u6548\u7387\u95ee\u9898\uff0c\u5e76\u4e3a\u89c4\u5219\u5956\u52b1\u7684GRPO\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u89e3\u91ca\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, RLHF, GRPO, \u5956\u52b1\u65b9\u5dee, \u975e\u51f8\u4f18\u5316"}}
{"id": "2505.23540", "pdf": "https://arxiv.org/pdf/2505.23540", "abs": "https://arxiv.org/abs/2505.23540", "authors": ["Yunqiao Yang", "Houxing Ren", "Zimu Lu", "Ke Wang", "Weikang Shi", "Aojun Zhou", "Junting Pan", "Mingjie Zhan", "Hongsheng Li"], "title": "Probability-Consistent Preference Optimization for Enhanced LLM Reasoning", "categories": ["cs.CL"], "comment": "14 pages, to be published in ACL 2025 findings", "summary": "Recent advances in preference optimization have demonstrated significant\npotential for improving mathematical reasoning capabilities in large language\nmodels (LLMs). While current approaches leverage high-quality pairwise\npreference data through outcome-based criteria like answer correctness or\nconsistency, they fundamentally neglect the internal logical coherence of\nresponses. To overcome this, we propose Probability-Consistent Preference\nOptimization (PCPO), a novel framework that establishes dual quantitative\nmetrics for preference selection: (1) surface-level answer correctness and (2)\nintrinsic token-level probability consistency across responses. Extensive\nexperiments show that our PCPO consistently outperforms existing outcome-only\ncriterion approaches across a diverse range of LLMs and benchmarks. Our code is\npublicly available at https://github.com/YunqiaoYang/PCPO.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u6846\u67b6PCPO\uff0c\u901a\u8fc7\u7ed3\u5408\u8868\u9762\u7b54\u6848\u6b63\u786e\u6027\u548c\u5185\u90e8\u6982\u7387\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u4e8e\u7ed3\u679c\u5bfc\u5411\u7684\u504f\u597d\u6570\u636e\uff08\u5982\u7b54\u6848\u6b63\u786e\u6027\u6216\u4e00\u81f4\u6027\uff09\uff0c\u5ffd\u89c6\u4e86\u54cd\u5e94\u5185\u90e8\u7684\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u4f5c\u8005\u63d0\u51faPCPO\u4ee5\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\u3002", "method": "\u63d0\u51faPCPO\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u91cd\u5b9a\u91cf\u6307\u6807\uff08\u8868\u9762\u7b54\u6848\u6b63\u786e\u6027\u548c\u5185\u5728token\u7ea7\u6982\u7387\u4e00\u81f4\u6027\uff09\u8fdb\u884c\u504f\u597d\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPCPO\u5728\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u7ed3\u679c\u6307\u6807\u7684\u65b9\u6cd5\u3002", "conclusion": "PCPO\u901a\u8fc7\u7ed3\u5408\u5916\u90e8\u7ed3\u679c\u548c\u5185\u90e8\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u504f\u597d\u4f18\u5316, \u6570\u5b66\u63a8\u7406, PCPO, \u6982\u7387\u4e00\u81f4\u6027"}}
{"id": "2505.23264", "pdf": "https://arxiv.org/pdf/2505.23264", "abs": "https://arxiv.org/abs/2505.23264", "authors": ["Fangyikang Wang", "Hubery Yin", "Shaobin Zhuang", "Huminhao Zhu", "Yinan Li", "Lei Qian", "Chao Zhang", "Hanbin Zhao", "Hui Qian", "Chen Li"], "title": "Efficiently Access Diffusion Fisher: Within the Outer Product Span Space", "categories": ["cs.LG"], "comment": null, "summary": "Recent Diffusion models (DMs) advancements have explored incorporating the\nsecond-order diffusion Fisher information (DF), defined as the negative Hessian\nof log density, into various downstream tasks and theoretical analysis.\nHowever, current practices typically approximate the diffusion Fisher by\napplying auto-differentiation to the learned score network. This black-box\nmethod, though straightforward, lacks any accuracy guarantee and is\ntime-consuming. In this paper, we show that the diffusion Fisher actually\nresides within a space spanned by the outer products of score and initial data.\nBased on the outer-product structure, we develop two efficient approximation\nalgorithms to access the trace and matrix-vector multiplication of DF,\nrespectively. These algorithms bypass the auto-differentiation operations with\ntime-efficient vector-product calculations. Furthermore, we establish the\napproximation error bounds for the proposed algorithms. Experiments in\nlikelihood evaluation and adjoint optimization demonstrate the superior\naccuracy and reduced computational cost of our proposed algorithms.\nAdditionally, based on the novel outer-product formulation of DF, we design the\nfirst numerical verification experiment for the optimal transport property of\nthe general PF-ODE deduced map.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f97\u5206\u548c\u521d\u59cb\u6570\u636e\u7684\u5916\u79ef\u7ed3\u6784\u6765\u9ad8\u6548\u8fd1\u4f3c\u6269\u6563Fisher\u4fe1\u606f\uff08DF\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u81ea\u52a8\u5fae\u5206\u7684\u8017\u65f6\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u901a\u8fc7\u81ea\u52a8\u5fae\u5206\u8fd1\u4f3c\u6269\u6563Fisher\u4fe1\u606f\u7684\u65b9\u6cd5\u7f3a\u4e4f\u51c6\u786e\u6027\u4fdd\u8bc1\u4e14\u8ba1\u7b97\u8017\u65f6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u5f97\u5206\u548c\u521d\u59cb\u6570\u636e\u7684\u5916\u79ef\u7ed3\u6784\uff0c\u5f00\u53d1\u4e86\u4e24\u79cd\u9ad8\u6548\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5206\u522b\u7528\u4e8e\u8ba1\u7b97DF\u7684\u8ff9\u548c\u77e9\u9635-\u5411\u91cf\u4e58\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u5728\u4f3c\u7136\u8bc4\u4f30\u548c\u4f34\u968f\u4f18\u5316\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u4e0d\u4ec5\u9ad8\u6548\u4e14\u51c6\u786e\uff0c\u8fd8\u9996\u6b21\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PF-ODE\u63a8\u5bfc\u56fe\u7684\u6700\u4f18\u4f20\u8f93\u6027\u8d28\u3002", "keywords": "\u6269\u6563\u6a21\u578b\uff0cFisher\u4fe1\u606f\uff0c\u5916\u79ef\u7ed3\u6784\uff0c\u9ad8\u6548\u8fd1\u4f3c\uff0c\u6700\u4f18\u4f20\u8f93"}}
{"id": "2505.23548", "pdf": "https://arxiv.org/pdf/2505.23548", "abs": "https://arxiv.org/abs/2505.23548", "authors": ["Yuri Balashov"], "title": "Translation in the Wild", "categories": ["cs.CL"], "comment": "4 figures", "summary": "Large Language Models (LLMs) excel in translation among other things,\ndemonstrating competitive performance for many language pairs in zero- and\nfew-shot settings. But unlike dedicated neural machine translation models, LLMs\nare not trained on any translation-related objective. What explains their\nremarkable translation abilities? Are these abilities grounded in \"incidental\nbilingualism\" (Briakou et al. 2023) in training data? Does instruction tuning\ncontribute to it? Are LLMs capable of aligning and leveraging semantically\nidentical or similar monolingual contents from different corners of the\ninternet that are unlikely to fit in a single context window? I offer some\nreflections on this topic, informed by recent studies and growing user\nexperience. My working hypothesis is that LLMs' translation abilities originate\nin two different types of pre-training data that may be internalized by the\nmodels in different ways. I discuss the prospects for testing the \"duality\"\nhypothesis empirically and its implications for reconceptualizing translation,\nhuman and machine, in the age of deep learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u6765\u6e90\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u9884\u8bad\u7ec3\u6570\u636e\u7c7b\u578b\u7684\u5047\u8bbe\uff0c\u5e76\u8ba8\u8bba\u4e86\u6d4b\u8bd5\u8be5\u5047\u8bbe\u7684\u65b9\u6cd5\u53ca\u5176\u5bf9\u7ffb\u8bd1\u6982\u5ff5\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u91caLLMs\u5728\u6ca1\u6709\u4e13\u95e8\u7ffb\u8bd1\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5c55\u73b0\u51fa\u8272\u7ffb\u8bd1\u80fd\u529b\u7684\u73b0\u8c61\uff0c\u63a2\u8ba8\u5176\u80cc\u540e\u7684\u6570\u636e\u673a\u5236\u548c\u80fd\u529b\u6765\u6e90\u3002", "method": "\u901a\u8fc7\u56de\u987e\u8fd1\u671f\u7814\u7a76\u548c\u7528\u6237\u7ecf\u9a8c\uff0c\u63d0\u51fa\u5e76\u8ba8\u8bba\u4e86\u201c\u53cc\u91cd\u6027\u201d\u5047\u8bbe\uff0c\u5373LLMs\u7684\u7ffb\u8bd1\u80fd\u529b\u53ef\u80fd\u6e90\u81ea\u4e24\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u9884\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u8bba\u6587\u63d0\u51faLLMs\u7684\u7ffb\u8bd1\u80fd\u529b\u53ef\u80fd\u6765\u6e90\u4e8e\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u5076\u7136\u53cc\u8bed\u73b0\u8c61\u548c\u6307\u4ee4\u8c03\u4f18\uff0c\u5e76\u63a2\u8ba8\u4e86\u6d4b\u8bd5\u8fd9\u4e00\u5047\u8bbe\u7684\u5b9e\u8bc1\u65b9\u6cd5\u3002", "conclusion": "LLMs\u7684\u7ffb\u8bd1\u80fd\u529b\u7814\u7a76\u6709\u52a9\u4e8e\u91cd\u65b0\u5b9a\u4e49\u6df1\u5ea6\u5b66\u4e60\u65f6\u4ee3\u7684\u7ffb\u8bd1\u6982\u5ff5\uff0c\u4e3a\u4eba\u7c7b\u548c\u673a\u5668\u7ffb\u8bd1\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u673a\u5668\u7ffb\u8bd1, \u9884\u8bad\u7ec3\u6570\u636e, \u6307\u4ee4\u8c03\u4f18, \u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2505.23008", "pdf": "https://arxiv.org/pdf/2505.23008", "abs": "https://arxiv.org/abs/2505.23008", "authors": ["Jonathan Li", "Zoltan Csaki", "Nidhi Hiremath", "Etash Guha", "Fenglu Hong", "Edward Ma", "Urmish Thakker"], "title": "Synthetic Document Question Answering in Hungarian", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Modern VLMs have achieved near-saturation accuracy in English document visual\nquestion-answering (VQA). However, this task remains challenging in lower\nresource languages due to a dearth of suitable training and evaluation data. In\nthis paper we present scalable methods for curating such datasets by focusing\non Hungarian, approximately the 17th highest resource language on the internet.\nSpecifically, we present HuDocVQA and HuDocVQA-manual, document VQA datasets\nthat modern VLMs significantly underperform on compared to English DocVQA.\nHuDocVQA-manual is a small manually curated dataset based on Hungarian\ndocuments from Common Crawl, while HuDocVQA is a larger synthetically generated\nVQA data set from the same source. We apply multiple rounds of quality\nfiltering and deduplication to HuDocVQA in order to match human-level quality\nin this dataset. We also present HuCCPDF, a dataset of 117k pages from\nHungarian Common Crawl PDFs along with their transcriptions, which can be used\nfor training a model for Hungarian OCR. To validate the quality of our\ndatasets, we show how finetuning on a mixture of these datasets can improve\naccuracy on HuDocVQA for Llama 3.2 11B Instruct by +7.2%. Our datasets and code\nwill be released to the public to foster further research in multilingual\nDocVQA.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e3a\u5308\u7259\u5229\u8bed\u6784\u5efa\u7684\u6587\u6863\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\uff08HuDocVQA\u548cHuDocVQA-manual\uff09\u53ca\u5308\u7259\u5229\u8bedOCR\u8bad\u7ec3\u6570\u636e\u96c6\uff08HuCCPDF\uff09\uff0c\u901a\u8fc7\u7cbe\u7ec6\u8fc7\u6ee4\u548c\u6df7\u5408\u8bad\u7ec3\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u9274\u4e8e\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u82f1\u8bed\u6587\u6863\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5308\u7259\u5229\u8bed\uff09\u4e2d\u56e0\u6570\u636e\u7a00\u7f3a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u63a8\u52a8\u591a\u8bed\u8a00\u6587\u6863VQA\u7814\u7a76\u3002", "method": "\u4eceCommon Crawl\u4e2d\u63d0\u53d6\u5308\u7259\u5229\u8bed\u6587\u6863\uff0c\u4eba\u5de5\u6807\u6ce8\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\uff08HuDocVQA-manual\uff09\u5e76\u5408\u6210\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08HuDocVQA\uff09\uff0c\u540c\u65f6\u6784\u5efaOCR\u8bad\u7ec3\u6570\u636e\u96c6\uff08HuCCPDF\uff09\uff1b\u901a\u8fc7\u591a\u8f6e\u8d28\u91cf\u8fc7\u6ee4\u548c\u53bb\u91cd\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u3002", "result": "\u57fa\u4e8eLlama 3.2 11B Instruct\u6a21\u578b\uff0c\u6df7\u5408\u8bad\u7ec3\u6570\u636e\u540e\uff0c\u5728HuDocVQA\u4e0a\u7684\u51c6\u786e\u7387\u63d0\u5347\u4e867.2%\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u96c6\u586b\u8865\u4e86\u5308\u7259\u5229\u8bed\u6587\u6863VQA\u7684\u6570\u636e\u7a7a\u767d\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u8d28\u91cf\u548c\u8bad\u7ec3\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u516c\u5f00\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u591a\u8bed\u8a00\u7814\u7a76\u3002", "keywords": "\u6587\u6863\u89c6\u89c9\u95ee\u7b54, \u5308\u7259\u5229\u8bed, \u4f4e\u8d44\u6e90\u8bed\u8a00, \u6570\u636e\u96c6\u6784\u5efa, OCR"}}
{"id": "2505.23270", "pdf": "https://arxiv.org/pdf/2505.23270", "abs": "https://arxiv.org/abs/2505.23270", "authors": ["Haokun Chen", "Yueqi Zhang", "Yuan Bi", "Yao Zhang", "Tong Liu", "Jinhe Bi", "Jian Lan", "Jindong Gu", "Claudia Grosser", "Denis Krompass", "Nassir Navab", "Volker Tresp"], "title": "Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "In recent years, Large Language Models (LLMs) have achieved remarkable\nadvancements, drawing significant attention from the research community. Their\ncapabilities are largely attributed to large-scale architectures, which require\nextensive training on massive datasets. However, such datasets often contain\nsensitive or copyrighted content sourced from the public internet, raising\nconcerns about data privacy and ownership. Regulatory frameworks, such as the\nGeneral Data Protection Regulation (GDPR), grant individuals the right to\nrequest the removal of such sensitive information. This has motivated the\ndevelopment of machine unlearning algorithms that aim to remove specific\nknowledge from models without the need for costly retraining. Despite these\nadvancements, evaluating the efficacy of unlearning algorithms remains a\nchallenge due to the inherent complexity and generative nature of LLMs. In this\nwork, we introduce a comprehensive auditing framework for unlearning\nevaluation, comprising three benchmark datasets, six unlearning algorithms, and\nfive prompt-based auditing methods. By using various auditing algorithms, we\nevaluate the effectiveness and robustness of different unlearning strategies.\nTo explore alternatives beyond prompt-based auditing, we propose a novel\ntechnique that leverages intermediate activation perturbations, addressing the\nlimitations of auditing methods that rely solely on model inputs and outputs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u673a\u5668\u9057\u5fd8\u7b97\u6cd5\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u6570\u636e\u96c6\u3001\u7b97\u6cd5\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u5ba1\u8ba1\u65b9\u6cd5\uff0c\u5e76\u63a2\u7d22\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e2d\u95f4\u6fc0\u6d3b\u6270\u52a8\u7684\u65b0\u6280\u672f\u4ee5\u63d0\u9ad8\u8bc4\u4f30\u6548\u679c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u9690\u79c1\u548c\u7248\u6743\u95ee\u9898\u4fc3\u4f7f\u4e86\u673a\u5668\u9057\u5fd8\u7b97\u6cd5\u7684\u53d1\u5c55\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u8005\u8bbe\u8ba1\u4e86\u5305\u542b\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u3001\u516d\u79cd\u9057\u5fd8\u7b97\u6cd5\u548c\u4e94\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u5ba1\u8ba1\u65b9\u6cd5\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e2d\u95f4\u6fc0\u6d3b\u6270\u52a8\u7684\u65b0\u6280\u672f\u3002", "result": "\u901a\u8fc7\u591a\u79cd\u5ba1\u8ba1\u7b97\u6cd5\u8bc4\u4f30\u4e86\u4e0d\u540c\u9057\u5fd8\u7b56\u7565\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9a8c\u8bc1\u4e86\u65b0\u6280\u672f\u7684\u6f5c\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u6280\u672f\u4e3a\u673a\u5668\u9057\u5fd8\u7b97\u6cd5\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u5de5\u5177\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u673a\u5668\u9057\u5fd8, \u6570\u636e\u9690\u79c1, \u5ba1\u8ba1\u6846\u67b6, \u4e2d\u95f4\u6fc0\u6d3b\u6270\u52a8"}}
{"id": "2505.23556", "pdf": "https://arxiv.org/pdf/2505.23556", "abs": "https://arxiv.org/abs/2505.23556", "authors": ["Wei Jie Yeo", "Nirmalendu Prakash", "Clement Neo", "Roy Ka-Wei Lee", "Erik Cambria", "Ranjan Satapathy"], "title": "Understanding Refusal in Language Models with Sparse Autoencoders", "categories": ["cs.CL"], "comment": null, "summary": "Refusal is a key safety behavior in aligned language models, yet the internal\nmechanisms driving refusals remain opaque. In this work, we conduct a\nmechanistic study of refusal in instruction-tuned LLMs using sparse\nautoencoders to identify latent features that causally mediate refusal\nbehaviors. We apply our method to two open-source chat models and intervene on\nrefusal-related features to assess their influence on generation, validating\ntheir behavioral impact across multiple harmful datasets. This enables a\nfine-grained inspection of how refusal manifests at the activation level and\naddresses key research questions such as investigating upstream-downstream\nlatent relationship and understanding the mechanisms of adversarial\njailbreaking techniques. We also establish the usefulness of refusal features\nin enhancing generalization for linear probes to out-of-distribution\nadversarial samples in classification tasks. We open source our code in\nhttps://github.com/wj210/refusal_sae.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u6a21\u578b\u4e2d\u62d2\u7edd\u884c\u4e3a\u7684\u5185\u5728\u673a\u5236\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bc6\u522b\u5f71\u54cd\u62d2\u7edd\u884c\u4e3a\u7684\u6f5c\u5728\u7279\u5f81\uff0c\u5e76\u5728\u591a\u4e2a\u6709\u5bb3\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u5f71\u54cd\u3002", "motivation": "\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u62d2\u7edd\u884c\u4e3a\u7684\u5185\u5728\u673a\u5236\uff0c\u63a2\u7a76\u5176\u5982\u4f55\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u88ab\u6fc0\u6d3b\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5e72\u9884\u5b9e\u9a8c\u9a8c\u8bc1\u8fd9\u4e9b\u7279\u5f81\u5bf9\u62d2\u7edd\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u62d2\u7edd\u884c\u4e3a\u7531\u7279\u5b9a\u6f5c\u5728\u7279\u5f81\u4ecb\u5bfc\uff0c\u4e14\u8fd9\u4e9b\u7279\u5f81\u53ef\u7528\u4e8e\u63d0\u5347\u5206\u7c7b\u4efb\u52a1\u4e2d\u5bf9\u5bf9\u6297\u6837\u672c\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u673a\u5236\u5316\u7814\u7a76\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u4e2d\u62d2\u7edd\u884c\u4e3a\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u4e3a\u6539\u8fdb\u6a21\u578b\u5b89\u5168\u6027\u548c\u5bf9\u6297\u653b\u51fb\u9632\u5fa1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b, \u62d2\u7edd\u884c\u4e3a, \u7a00\u758f\u81ea\u7f16\u7801\u5668, \u5bf9\u6297\u6837\u672c, \u5b89\u5168\u6027"}}
{"id": "2505.23285", "pdf": "https://arxiv.org/pdf/2505.23285", "abs": "https://arxiv.org/abs/2505.23285", "authors": ["Muhammad Shafi", "Syed Mohsin Bokhari"], "title": "Comparative Analysis of the Land Use and Land Cover Changes in Different Governorates of Oman using Spatiotemporal Multi-spectral Satellite Data", "categories": ["cs.LG"], "comment": null, "summary": "Land cover and land use (LULC) changes are key applications of satellite\nimagery, and they have critical roles in resource management, urbanization,\nprotection of soils and the environment, and enhancing sustainable development.\nThe literature has heavily utilized multispectral spatiotemporal satellite data\nalongside advanced machine learning algorithms to monitor and predict LULC\nchanges. This study analyzes and compares LULC changes across various\ngovernorates (provinces) of the Sultanate of Oman from 2016 to 2021 using\nannual time steps. For the chosen region, multispectral spatiotemporal data\nwere acquired from the open-source Sentinel-2 satellite dataset. Supervised\nmachine learning algorithms were used to train and classify different land\ncovers, such as water bodies, crops, urban, etc. The constructed model was\nsubsequently applied within the study region, allowing for an effective\ncomparative evaluation of LULC changes within the given timeframe.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5229\u7528Sentinel-2\u536b\u661f\u591a\u5149\u8c31\u65f6\u7a7a\u6570\u636e\u548c\u76d1\u7763\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u5206\u6790\u4e862016\u81f32021\u5e74\u963f\u66fc\u82cf\u4e39\u56fd\u5404\u7701\u7684\u571f\u5730\u8986\u76d6\u4e0e\u571f\u5730\u5229\u7528\u53d8\u5316\uff0c\u5e76\u8fdb\u884c\u4e86\u6bd4\u8f83\u8bc4\u4f30\u3002", "motivation": "\u571f\u5730\u8986\u76d6\u4e0e\u571f\u5730\u5229\u7528\u53d8\u5316\u5bf9\u8d44\u6e90\u7ba1\u7406\u3001\u57ce\u5e02\u5316\u3001\u73af\u5883\u4fdd\u62a4\u548c\u53ef\u6301\u7eed\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u901a\u8fc7\u536b\u661f\u5f71\u50cf\u548c\u673a\u5668\u5b66\u4e60\u8fdb\u884c\u76d1\u6d4b\u4e0e\u9884\u6d4b\u3002", "method": "\u7814\u7a76\u4f7f\u7528Sentinel-2\u536b\u661f\u7684\u591a\u5149\u8c31\u65f6\u7a7a\u6570\u636e\uff0c\u7ed3\u5408\u76d1\u7763\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u8bad\u7ec3\u5e76\u5206\u7c7b\u4e86\u6c34\u4f53\u3001\u4f5c\u7269\u3001\u57ce\u5e02\u7b49\u4e0d\u540c\u571f\u5730\u8986\u76d6\u7c7b\u578b\u3002", "result": "\u6a21\u578b\u5728\u7814\u7a76\u533a\u57df\u5185\u6210\u529f\u5e94\u7528\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6307\u5b9a\u65f6\u95f4\u6bb5\u5185\u571f\u5730\u8986\u76d6\u4e0e\u571f\u5730\u5229\u7528\u53d8\u5316\u7684\u6709\u6548\u6bd4\u8f83\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u591a\u5149\u8c31\u536b\u661f\u6570\u636e\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728\u76d1\u6d4b\u571f\u5730\u8986\u76d6\u4e0e\u571f\u5730\u5229\u7528\u53d8\u5316\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "keywords": "\u571f\u5730\u8986\u76d6\u4e0e\u571f\u5730\u5229\u7528\u53d8\u5316, Sentinel-2, \u673a\u5668\u5b66\u4e60, \u591a\u5149\u8c31\u6570\u636e, \u963f\u66fc\u82cf\u4e39\u56fd"}}
{"id": "2505.23570", "pdf": "https://arxiv.org/pdf/2505.23570", "abs": "https://arxiv.org/abs/2505.23570", "authors": ["Leonardo La Rocca", "Francesco Corso", "Francesco Pierri"], "title": "Evaluating AI capabilities in detecting conspiracy theories on YouTube", "categories": ["cs.CL", "cs.CY", "cs.SI"], "comment": "Submitted for review to OSNEM Special Issue of April 2025", "summary": "As a leading online platform with a vast global audience, YouTube's extensive\nreach also makes it susceptible to hosting harmful content, including\ndisinformation and conspiracy theories. This study explores the use of\nopen-weight Large Language Models (LLMs), both text-only and multimodal, for\nidentifying conspiracy theory videos shared on YouTube. Leveraging a labeled\ndataset of thousands of videos, we evaluate a variety of LLMs in a zero-shot\nsetting and compare their performance to a fine-tuned RoBERTa baseline. Results\nshow that text-based LLMs achieve high recall but lower precision, leading to\nincreased false positives. Multimodal models lag behind their text-only\ncounterparts, indicating limited benefits from visual data integration. To\nassess real-world applicability, we evaluate the most accurate models on an\nunlabeled dataset, finding that RoBERTa achieves performance close to LLMs with\na larger number of parameters. Our work highlights the strengths and\nlimitations of current LLM-based approaches for online harmful content\ndetection, emphasizing the need for more precise and robust systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bc6\u522bYouTube\u4e0a\u7684\u9634\u8c0b\u8bba\u89c6\u9891\uff0c\u53d1\u73b0\u6587\u672c\u6a21\u578b\u53ec\u56de\u7387\u9ad8\u4f46\u7cbe\u5ea6\u4f4e\uff0c\u591a\u6a21\u6001\u6a21\u578b\u8868\u73b0\u4e0d\u5982\u7eaf\u6587\u672c\u6a21\u578b\uff0cRoBERTa\u7684\u6027\u80fd\u63a5\u8fd1\u66f4\u5927\u53c2\u6570\u7684LLMs\u3002", "motivation": "YouTube\u4f5c\u4e3a\u5168\u7403\u9886\u5148\u7684\u5728\u7ebf\u5e73\u53f0\uff0c\u6613\u4f20\u64ad\u6709\u5bb3\u5185\u5bb9\uff08\u5982\u9634\u8c0b\u8bba\uff09\u3002\u7814\u7a76\u5e0c\u671b\u901a\u8fc7LLMs\u63d0\u5347\u5bf9\u8fd9\u7c7b\u5185\u5bb9\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u5229\u7528\u6807\u6ce8\u6570\u636e\u96c6\u8fdb\u884c\u96f6\u6837\u672c\u6d4b\u8bd5\uff0c\u5bf9\u6bd4\u591a\u79cdLLMs\u4e0e\u5fae\u8c03RoBERTa\u7684\u8868\u73b0\uff0c\u5e76\u5728\u672a\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5b9e\u7528\u6027\u3002", "result": "\u6587\u672cLLMs\u53ec\u56de\u7387\u9ad8\u4f46\u7cbe\u5ea6\u4f4e\uff1b\u591a\u6a21\u6001\u6a21\u578b\u8868\u73b0\u8f83\u5dee\uff1bRoBERTa\u6027\u80fd\u63a5\u8fd1\u66f4\u5927\u53c2\u6570\u7684LLMs\u3002", "conclusion": "\u5f53\u524dLLMs\u5728\u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b\u4e2d\u6709\u6f5c\u529b\u4f46\u5c40\u9650\u6027\u660e\u663e\uff0c\u9700\u66f4\u7cbe\u786e\u548c\u9c81\u68d2\u7684\u7cfb\u7edf\u3002", "keywords": "YouTube, LLMs, \u9634\u8c0b\u8bba, \u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b, RoBERTa"}}
{"id": "2505.23020", "pdf": "https://arxiv.org/pdf/2505.23020", "abs": "https://arxiv.org/abs/2505.23020", "authors": ["Jinchuan Zhang", "Lu Yin", "Yan Zhou", "Songlin Hu"], "title": "AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Submitted to ACL 2025", "summary": "The acquisition of agentic capabilities has transformed LLMs from \"knowledge\nproviders\" to \"action executors\", a trend that while expanding LLMs' capability\nboundaries, significantly increases their susceptibility to malicious use.\nPrevious work has shown that current LLM-based agents execute numerous\nmalicious tasks even without being attacked, indicating a deficiency in agentic\nuse safety alignment during the post-training phase. To address this gap, we\npropose AgentAlign, a novel framework that leverages abstract behavior chains\nas a medium for safety alignment data synthesis. By instantiating these\nbehavior chains in simulated environments with diverse tool instances, our\nframework enables the generation of highly authentic and executable\ninstructions while capturing complex multi-step dynamics. The framework further\nensures model utility by proportionally synthesizing benign instructions\nthrough non-malicious interpretations of behavior chains, precisely calibrating\nthe boundary between helpfulness and harmlessness. Evaluation results on\nAgentHarm demonstrate that fine-tuning three families of open-source models\nusing our method substantially improves their safety (35.8% to 79.5%\nimprovement) while minimally impacting or even positively enhancing their\nhelpfulness, outperforming various prompting methods. The dataset and code have\nboth been open-sourced.", "AI": {"tldr": "\u63d0\u51fa\u4e86AgentAlign\u6846\u67b6\uff0c\u901a\u8fc7\u62bd\u8c61\u884c\u4e3a\u94fe\u751f\u6210\u5b89\u5168\u5bf9\u9f50\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u5b89\u5168\u6027\uff08\u63d0\u534735.8%\u81f379.5%\uff09\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u6a21\u578b\u5b9e\u7528\u6027\u7684\u5f71\u54cd\u3002", "motivation": "LLM\u4ece\u201c\u77e5\u8bc6\u63d0\u4f9b\u8005\u201d\u8f6c\u53d8\u4e3a\u201c\u884c\u52a8\u6267\u884c\u8005\u201d\u540e\uff0c\u6613\u53d7\u6076\u610f\u5229\u7528\uff0c\u76ee\u524d\u7f3a\u4e4f\u540e\u8bad\u7ec3\u9636\u6bb5\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u62bd\u8c61\u884c\u4e3a\u94fe\u5728\u6a21\u62df\u73af\u5883\u4e2d\u751f\u6210\u5b89\u5168\u5bf9\u9f50\u6570\u636e\uff0c\u517c\u987e\u771f\u5b9e\u6027\u548c\u590d\u6742\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u826f\u6027\u6307\u4ee4\u6821\u51c6\u5b89\u5168\u8fb9\u754c\u3002", "result": "\u5728AgentHarm\u8bc4\u4f30\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u5f00\u6e90\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0835.8%-79.5%\uff09\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u6216\u63d0\u5347\u6a21\u578b\u5b9e\u7528\u6027\u3002", "conclusion": "AgentAlign\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u4ee3\u7406\u80fd\u529b\u4e0b\u7684\u5b89\u5168\u5bf9\u9f50\u95ee\u9898\uff0c\u5e73\u8861\u4e86\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u3002", "keywords": "LLM\u3001\u5b89\u5168\u5bf9\u9f50\u3001\u884c\u4e3a\u94fe\u3001AgentAlign"}}
{"id": "2505.23309", "pdf": "https://arxiv.org/pdf/2505.23309", "abs": "https://arxiv.org/abs/2505.23309", "authors": ["Yixin Ren", "Chenghou Jin", "Yewei Xia", "Li Ke", "Longtao Huang", "Hui Xue", "Hao Zhang", "Jihong Guan", "Shuigeng Zhou"], "title": "Score-based Generative Modeling for Conditional Independence Testing", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by KDD2025", "summary": "Determining conditional independence (CI) relationships between random\nvariables is a fundamental yet challenging task in machine learning and\nstatistics, especially in high-dimensional settings. Existing generative\nmodel-based CI testing methods, such as those utilizing generative adversarial\nnetworks (GANs), often struggle with undesirable modeling of conditional\ndistributions and training instability, resulting in subpar performance. To\naddress these issues, we propose a novel CI testing method via score-based\ngenerative modeling, which achieves precise Type I error control and strong\ntesting power. Concretely, we first employ a sliced conditional score matching\nscheme to accurately estimate conditional score and use Langevin dynamics\nconditional sampling to generate null hypothesis samples, ensuring precise Type\nI error control. Then, we incorporate a goodness-of-fit stage into the method\nto verify generated samples and enhance interpretability in practice. We\ntheoretically establish the error bound of conditional distributions modeled by\nscore-based generative models and prove the validity of our CI tests. Extensive\nexperiments on both synthetic and real-world datasets show that our method\nsignificantly outperforms existing state-of-the-art methods, providing a\npromising way to revitalize generative model-based CI testing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u6570\u751f\u6210\u5efa\u6a21\u7684\u65b0\u578b\u6761\u4ef6\u72ec\u7acb\u6027\uff08CI\uff09\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u751f\u6210\u6a21\u578b\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\uff0c\u786e\u5b9a\u968f\u673a\u53d8\u91cf\u4e4b\u95f4\u7684\u6761\u4ef6\u72ec\u7acb\u6027\u5173\u7cfb\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u65b9\u6cd5\uff08\u5982GANs\uff09\u5728\u5efa\u6a21\u6761\u4ef6\u5206\u5e03\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u6027\u80fd\u8f83\u5dee\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5206\u6570\u751f\u6210\u5efa\u6a21\u7684\u65b9\u6cd5\uff0c\u9996\u5148\u4f7f\u7528\u5207\u7247\u6761\u4ef6\u5206\u6570\u5339\u914d\u65b9\u6848\u51c6\u786e\u4f30\u8ba1\u6761\u4ef6\u5206\u6570\uff0c\u5e76\u7528Langevin\u52a8\u529b\u5b66\u6761\u4ef6\u91c7\u6837\u751f\u6210\u96f6\u5047\u8bbe\u6837\u672c\u4ee5\u786e\u4fdd\u7cbe\u786e\u7684Type I\u9519\u8bef\u63a7\u5236\uff1b\u968f\u540e\u52a0\u5165\u62df\u5408\u4f18\u5ea6\u9636\u6bb5\u9a8c\u8bc1\u751f\u6210\u7684\u6837\u672c\uff0c\u63d0\u5347\u5b9e\u9645\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u7406\u8bba\u4e0a\u7684\u6761\u4ef6\u5206\u5e03\u8bef\u5dee\u754c\u9650\uff0c\u5e76\u8bc1\u660e\u4e86CI\u6d4b\u8bd5\u7684\u6709\u6548\u6027\u3002\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684CI\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "keywords": "\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\u3001\u5206\u6570\u751f\u6210\u5efa\u6a21\u3001Type I\u9519\u8bef\u63a7\u5236\u3001Langevin\u52a8\u529b\u5b66\u3001\u62df\u5408\u4f18\u5ea6"}}
{"id": "2505.23604", "pdf": "https://arxiv.org/pdf/2505.23604", "abs": "https://arxiv.org/abs/2505.23604", "authors": ["Guangtao Zeng", "Maohao Shen", "Delin Chen", "Zhenting Qi", "Subhro Das", "Dan Gutfreund", "David Cox", "Gregory Wornell", "Wei Lu", "Zhang-Wei Hong", "Chuang Gan"], "title": "Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": null, "summary": "Language models (LMs) perform well on standardized coding benchmarks but\nstruggle with real-world software engineering tasks such as resolving GitHub\nissues in SWE-Bench, especially when model parameters are less than 100B. While\nsmaller models are preferable in practice due to their lower computational\ncost, improving their performance remains challenging. Existing approaches\nprimarily rely on supervised fine-tuning (SFT) with high-quality data, which is\nexpensive to curate at scale. An alternative is test-time scaling: generating\nmultiple outputs, scoring them using a verifier, and selecting the best one.\nAlthough effective, this strategy often requires excessive sampling and costly\nscoring, limiting its practical application. We propose Evolutionary Test-Time\nScaling (EvoScale), a sample-efficient method that treats generation as an\nevolutionary process. By iteratively refining outputs via selection and\nmutation, EvoScale shifts the output distribution toward higher-scoring\nregions, reducing the number of samples needed to find correct solutions. To\nreduce the overhead from repeatedly sampling and selection, we train the model\nto self-evolve using reinforcement learning (RL). Rather than relying on\nexternal verifiers at inference time, the model learns to self-improve the\nscores of its own generations across iterations. Evaluated on\nSWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or\nexceed the performance of models with over 100B parameters while using a few\nsamples. Code, data, and models will be fully open-sourced.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEvoScale\u7684\u8fdb\u5316\u5f0f\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u548c\u81ea\u6211\u8fdb\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u76d1\u7763\u5fae\u8c03\u6216\u6d4b\u8bd5\u65f6\u6269\u5c55\uff09\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\u3002", "method": "\u91c7\u7528\u8fdb\u5316\u5f0f\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08EvoScale\uff09\uff0c\u7ed3\u5408\u9009\u62e9\u3001\u7a81\u53d8\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u8ba9\u6a21\u578b\u80fd\u591f\u81ea\u6211\u8fdb\u5316\uff0c\u51cf\u5c11\u751f\u6210\u6837\u672c\u6570\u91cf\u3002", "result": "EvoScale\u4f7f32B\u53c2\u6570\u7684Satori-SWE-32B\u6a21\u578b\u5728SWE-Bench-Verified\u4e0a\u6027\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7100B\u4ee5\u4e0a\u53c2\u6570\u6a21\u578b\uff0c\u4e14\u6837\u672c\u4f7f\u7528\u91cf\u5c11\u3002", "conclusion": "EvoScale\u4e3a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u4f18\u5316\u8def\u5f84\uff0c\u663e\u8457\u63d0\u5347\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b\u3001\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u3001\u8fdb\u5316\u5f0f\u4f18\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u6d4b\u8bd5\u65f6\u6269\u5c55"}}
{"id": "2505.23320", "pdf": "https://arxiv.org/pdf/2505.23320", "abs": "https://arxiv.org/abs/2505.23320", "authors": ["Connor Cooper", "Geoffrey I. Webb", "Daniel F. Schmidt"], "title": "Efficient Parameter Estimation for Bayesian Network Classifiers using Hierarchical Linear Smoothing", "categories": ["cs.LG", "stat.ML"], "comment": "20 pages, 9 figures", "summary": "Bayesian network classifiers (BNCs) possess a number of properties desirable\nfor a modern classifier: They are easily interpretable, highly scalable, and\noffer adaptable complexity. However, traditional methods for learning BNCs have\nhistorically underperformed when compared to leading classification methods\nsuch as random forests. Recent parameter smoothing techniques using\nhierarchical Dirichlet processes (HDPs) have enabled BNCs to achieve\nperformance competitive with random forests on categorical data, but these\ntechniques are relatively inflexible, and require a complicated, specialized\nsampling process. In this paper, we introduce a novel method for parameter\nestimation that uses a log-linear regression to approximate the behaviour of\nHDPs. As a linear model, our method is remarkably flexible and simple to\ninterpret, and can leverage the vast literature on learning linear models. Our\nexperiments show that our method can outperform HDP smoothing while being\norders of magnitude faster, remaining competitive with random forests on\ncategorical data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6570\u7ebf\u6027\u56de\u5f52\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fd1\u4f3c\u5c42\u6b21\u72c4\u5229\u514b\u96f7\u8fc7\u7a0b\uff08HDPs\uff09\u7684\u884c\u4e3a\uff0c\u4ee5\u63d0\u9ad8\u8d1d\u53f6\u65af\u7f51\u7edc\u5206\u7c7b\u5668\uff08BNCs\uff09\u7684\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u7b80\u5355\u3001\u7075\u6d3b\u4e14\u9ad8\u6548\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8eHDP\u5e73\u6ed1\u65b9\u6cd5\uff0c\u5e76\u4e0e\u968f\u673a\u68ee\u6797\u7ade\u4e89\u3002", "motivation": "\u4f20\u7edf\u7684BNCs\u5b66\u4e60\u65b9\u6cd5\u6027\u80fd\u8f83\u5dee\uff0c\u800cHDPs\u5e73\u6ed1\u6280\u672f\u867d\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u590d\u6742\u4e14\u4e0d\u7075\u6d3b\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5bf9\u6570\u7ebf\u6027\u56de\u5f52\u8fd1\u4f3cHDPs\u7684\u884c\u4e3a\uff0c\u5229\u7528\u7ebf\u6027\u6a21\u578b\u7684\u7075\u6d3b\u6027\u548c\u7b80\u5355\u6027\u6765\u4f18\u5316\u53c2\u6570\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8eHDP\u5e73\u6ed1\uff0c\u4e14\u901f\u5ea6\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u4e0e\u968f\u673a\u68ee\u6797\u5728\u5206\u7c7b\u6570\u636e\u4e0a\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "\u5bf9\u6570\u7ebf\u6027\u56de\u5f52\u65b9\u6cd5\u4e3aBNCs\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u53c2\u6570\u4f30\u8ba1\u65b9\u6848\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfHDP\u5e73\u6ed1\u65b9\u6cd5\u3002", "keywords": "\u8d1d\u53f6\u65af\u7f51\u7edc\u5206\u7c7b\u5668\u3001\u5c42\u6b21\u72c4\u5229\u514b\u96f7\u8fc7\u7a0b\u3001\u5bf9\u6570\u7ebf\u6027\u56de\u5f52\u3001\u53c2\u6570\u4f30\u8ba1\u3001\u5206\u7c7b\u6027\u80fd"}}
{"id": "2505.23621", "pdf": "https://arxiv.org/pdf/2505.23621", "abs": "https://arxiv.org/abs/2505.23621", "authors": ["Zheyuan Yang", "Lyuhao Chen", "Arman Cohan", "Yilun Zhao"], "title": "Table-R1: Inference-Time Scaling for Table Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "In this work, we present the first study to explore inference-time scaling on\ntable reasoning tasks. We develop and evaluate two post-training strategies to\nenable inference-time scaling: distillation from frontier model reasoning\ntraces and reinforcement learning with verifiable rewards (RLVR). For\ndistillation, we introduce a large-scale dataset of reasoning traces generated\nby DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For\nRLVR, we propose task-specific verifiable reward functions and apply the GRPO\nalgorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series\nmodels across diverse table reasoning tasks, including short-form QA, fact\nverification, and free-form QA. Notably, the Table-R1-Zero model matches or\nexceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a\n7B-parameter LLM. It also demonstrates strong generalization to out-of-domain\ndatasets. Extensive ablation and qualitative analyses reveal the benefits of\ninstruction tuning, model architecture choices, and cross-task generalization,\nas well as emergence of essential table reasoning skills during RL training.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u63a2\u7d22\u4e86\u8868\u683c\u63a8\u7406\u4efb\u52a1\u7684\u63a8\u65ad\u65f6\u6269\u5c55\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u540e\u8bad\u7ec3\u7b56\u7565\uff1a\u84b8\u998f\u524d\u6cbf\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\u548c\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u3002\u901a\u8fc7\u8fd9\u4e24\u79cd\u65b9\u6cd5\uff0c\u8bad\u7ec3\u51fa\u7684Table-R1\u7cfb\u5217\u6a21\u578b\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8aGPT-4.1\u548cDeepSeek-R1\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63d0\u5347\u8868\u683c\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u63a2\u7d22\u901a\u8fc7\u540e\u8bad\u7ec3\u7b56\u7565\uff08\u5982\u84b8\u998f\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u5728\u5c0f\u89c4\u6a21\u6a21\u578b\u4e0a\u5b9e\u73b0\u63a5\u8fd1\u6216\u8d85\u8d8a\u524d\u6cbf\u6a21\u578b\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u65b9\u6cd5\uff1a1) \u57fa\u4e8eDeepSeek-R1\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\u8fdb\u884c\u84b8\u998f\u8bad\u7ec3\uff08Table-R1-SFT\uff09\uff1b2) \u63d0\u51fa\u53ef\u9a8c\u8bc1\u5956\u52b1\u51fd\u6570\u5e76\u5e94\u7528GRPO\u7b97\u6cd5\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff08Table-R1-Zero\uff09\u3002", "result": "Table-R1-Zero\u6a21\u578b\u5728\u591a\u4e2a\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5339\u914d\u6216\u8d85\u8d8aGPT-4.1\u548cDeepSeek-R1\uff0c\u4e14\u4f7f\u7528\u4e86\u4ec57B\u53c2\u6570\u7684\u8f7b\u91cf\u6a21\u578b\uff0c\u5e76\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6307\u4ee4\u5fae\u8c03\u3001\u6a21\u578b\u67b6\u6784\u9009\u62e9\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u5bf9\u63d0\u5347\u8868\u683c\u63a8\u7406\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u8fd8\u6d8c\u73b0\u4e86\u5173\u952e\u63a8\u7406\u6280\u80fd\u3002", "keywords": "\u8868\u683c\u63a8\u7406\uff0c\u63a8\u65ad\u65f6\u6269\u5c55\uff0c\u84b8\u998f\uff0c\u5f3a\u5316\u5b66\u4e60\uff0c\u53ef\u9a8c\u8bc1\u5956\u52b1"}}
{"id": "2505.23334", "pdf": "https://arxiv.org/pdf/2505.23334", "abs": "https://arxiv.org/abs/2505.23334", "authors": ["Tu Bui", "Mohamed Suliman", "Aparajita Haldar", "Mohammed Amer", "Serban Georgescu"], "title": "X2Graph for Cancer Subtyping Prediction on Biological Tabular Data", "categories": ["cs.LG"], "comment": "IEEE Engineering in Medicine and Biology Society (EMBC) 2025", "summary": "Despite the transformative impact of deep learning on text, audio, and image\ndatasets, its dominance in tabular data, especially in the medical domain where\ndata are often scarce, remains less clear. In this paper, we propose X2Graph, a\nnovel deep learning method that achieves strong performance on small biological\ntabular datasets. X2Graph leverages external knowledge about the relationships\nbetween table columns, such as gene interactions, to convert each sample into a\ngraph structure. This transformation enables the application of standard\nmessage passing algorithms for graph modeling. Our X2Graph method demonstrates\nsuperior performance compared to existing tree-based and deep learning methods\nacross three cancer subtyping datasets.", "AI": {"tldr": "X2Graph\u662f\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u9488\u5bf9\u5c0f\u578b\u751f\u7269\u8868\u683c\u6570\u636e\u8868\u73b0\u4f18\u5f02\uff0c\u5229\u7528\u5217\u95f4\u5173\u7cfb\u5c06\u6837\u672c\u8f6c\u6362\u4e3a\u56fe\u7ed3\u6784\uff0c\u5e94\u7528\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\u8fdb\u884c\u56fe\u5efa\u6a21\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u6587\u672c\u3001\u97f3\u9891\u548c\u56fe\u50cf\u6570\u636e\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5728\u533b\u5b66\u9886\u57df\u7b49\u6570\u636e\u7a00\u7f3a\u7684\u8868\u683c\u6570\u636e\u4e2d\u6548\u679c\u5c1a\u4e0d\u660e\u786e\u3002\u56e0\u6b64\uff0c\u63d0\u51faX2Graph\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "X2Graph\u901a\u8fc7\u5229\u7528\u5217\u95f4\u5173\u7cfb\uff08\u5982\u57fa\u56e0\u76f8\u4e92\u4f5c\u7528\uff09\u5c06\u8868\u683c\u6570\u636e\u8f6c\u6362\u4e3a\u56fe\u7ed3\u6784\uff0c\u4ece\u800c\u5e94\u7528\u6807\u51c6\u7684\u56fe\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\u8fdb\u884c\u5efa\u6a21\u3002", "result": "X2Graph\u5728\u4e09\u79cd\u764c\u75c7\u4e9a\u578b\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u6811\u57fa\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "X2Graph\u5728\u5c0f\u578b\u751f\u7269\u8868\u683c\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u8f6c\u6362\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "keywords": "X2Graph, \u6df1\u5ea6\u5b66\u4e60, \u8868\u683c\u6570\u636e, \u56fe\u7ed3\u6784, \u764c\u75c7\u4e9a\u578b\u5206\u7c7b"}}
{"id": "2505.23623", "pdf": "https://arxiv.org/pdf/2505.23623", "abs": "https://arxiv.org/abs/2505.23623", "authors": ["Jiaoda Li", "Ryan Cotterell"], "title": "Characterizing the Expressivity of Transformer Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Transformer-based language models (LMs) have achieved widespread empirical\nsuccess, but their theoretical expressive power remains only partially\nunderstood. Prior work often relies on idealized models with assumptions --\nsuch as arbitrary numerical precision and hard attention -- that diverge from\nreal-world transformers. In this work, we provide an exact characterization of\nfixed-precision transformers with strict future masking and soft attention, an\nidealization that more closely mirrors practical implementations. We show that\nthese models are precisely as expressive as a specific fragment of linear\ntemporal logic that includes only a single temporal operator: the past\noperator. We further relate this logic to established classes in formal\nlanguage theory, automata theory, and algebra, yielding a rich and unified\ntheoretical framework for understanding transformer expressivity. Finally, we\npresent empirical results that align closely with our theory: transformers\ntrained on languages within their theoretical capacity generalize perfectly\nover lengths, while they consistently fail to generalize on languages beyond\nit.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cbe\u786e\u63cf\u8ff0\u4e86\u5177\u6709\u56fa\u5b9a\u7cbe\u5ea6\u3001\u4e25\u683c\u672a\u6765\u906e\u853d\u548c\u8f6f\u6ce8\u610f\u529b\u7684Transformer\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u7b49\u540c\u4e8e\u4e00\u79cd\u53ea\u5305\u542b\u8fc7\u53bb\u7b97\u5b50\u7684\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u7247\u6bb5\uff0c\u5e76\u4e0e\u5f62\u5f0f\u8bed\u8a00\u7406\u8bba\u3001\u81ea\u52a8\u673a\u7406\u8bba\u548c\u4ee3\u6570\u4e2d\u7684\u5df2\u77e5\u7c7b\u522b\u5173\u8054\uff0c\u5efa\u7acb\u4e86\u7406\u89e3Transformer\u8868\u8fbe\u80fd\u529b\u7684\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u3002\u5b9e\u8bc1\u7ed3\u679c\u4e0e\u7406\u8bba\u4e00\u81f4\u3002", "motivation": "\u5c3d\u7ba1Transformer\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u4e2d\u5e7f\u6cdb\u6210\u529f\uff0c\u4f46\u5176\u7406\u8bba\u8868\u8fbe\u80fd\u529b\u5c1a\u672a\u5b8c\u5168\u7406\u89e3\u3002\u4e4b\u524d\u7684\u7814\u7a76\u5f80\u5f80\u4f9d\u8d56\u4e0e\u73b0\u5b9eTransformer\u5b9e\u73b0\u5dee\u5f02\u8f83\u5927\u7684\u7406\u60f3\u5316\u5047\u8bbe\u3002\u672c\u7814\u7a76\u65e8\u5728\u66f4\u63a5\u8fd1\u5b9e\u9645\u5b9e\u73b0\u6761\u4ef6\u4e0b\u7cbe\u786e\u63cf\u8ff0Transformer\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u56fa\u5b9a\u7cbe\u5ea6\u3001\u4e25\u683c\u672a\u6765\u906e\u853d\u548c\u8f6f\u6ce8\u610f\u529b\u673a\u5236\u7684Transformer\uff0c\u5c06\u5176\u8868\u8fbe\u80fd\u529b\u7cbe\u786e\u5bf9\u5e94\u4e8e\u4e00\u79cd\u4ec5\u5305\u542b\u8fc7\u53bb\u7b97\u5b50\u7684\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u7247\u6bb5\uff0c\u5e76\u4e0e\u5f62\u5f0f\u8bed\u8a00\u7406\u8bba\u3001\u81ea\u52a8\u673a\u7406\u8bba\u548c\u4ee3\u6570\u4e2d\u7684\u5df2\u77e5\u7c7b\u522b\u5173\u8054\u3002", "result": "\u5efa\u7acb\u4e86Transformer\u8868\u8fbe\u80fd\u529b\u4e0e\u7279\u5b9a\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u7684\u7cbe\u786e\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7406\u8bba\u6846\u67b6\u7684\u6b63\u786e\u6027\u3002\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cTransformer\u5728\u7406\u8bba\u80fd\u529b\u8303\u56f4\u5185\u7684\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u5b8c\u7f8e\uff0c\u800c\u8d85\u51fa\u80fd\u529b\u8303\u56f4\u7684\u4efb\u52a1\u5219\u5931\u8d25\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3Transformer\u7684\u8868\u8fbe\u80fd\u529b\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u4e14\u7406\u8bba\u4e0e\u5b9e\u8bc1\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\u3002\u8fd9\u5bf9\u4e8e\u6539\u8fdb\u548c\u4f18\u5316\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u8bbe\u8ba1\u5177\u6709\u91cd\u8981\u6307\u5bfc\u610f\u4e49\u3002", "keywords": "Transformer\u3001\u8868\u8fbe\u80fd\u529b\u3001\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u3001\u81ea\u52a8\u673a\u7406\u8bba\u3001\u5f62\u5f0f\u8bed\u8a00\u7406\u8bba"}}
{"id": "2505.23337", "pdf": "https://arxiv.org/pdf/2505.23337", "abs": "https://arxiv.org/abs/2505.23337", "authors": ["Chetan Verma", "Aditya Srinivas Timmaraju", "Cho Jui-Hsieh", "Suyash Damle", "Ngot Bui", "Yang Zhang", "Wen Chen", "Xin Liu", "Prateek Jain", "Inderjit S Dhillon"], "title": "Matryoshka Model Learning for Improved Elastic Student Models", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages, 5 figures, Accepted at KDD 2025", "summary": "Industry-grade ML models are carefully designed to meet rapidly evolving\nserving constraints, which requires significant resources for model\ndevelopment. In this paper, we propose MatTA, a framework for training multiple\naccurate Student models using a novel Teacher-TA-Student recipe. TA models are\nlarger versions of the Student models with higher capacity, and thus allow\nStudent models to better relate to the Teacher model and also bring in more\ndomain-specific expertise. Furthermore, multiple accurate Student models can be\nextracted from the TA model. Therefore, despite only one training run, our\nmethodology provides multiple servable options to trade off accuracy for lower\nserving cost. We demonstrate the proposed method, MatTA, on proprietary\ndatasets and models. Its practical efficacy is underscored by live A/B tests\nwithin a production ML system, demonstrating 20% improvement on a key metric.\nWe also demonstrate our method on GPT-2 Medium, a public model, and achieve\nrelative improvements of over 24% on SAT Math and over 10% on the LAMBADA\nbenchmark.", "AI": {"tldr": "MatTA \u6846\u67b6\u901a\u8fc7\u65b0\u9896\u7684 Teacher-TA-Student \u65b9\u6cd5\u8bad\u7ec3\u591a\u4e2a\u51c6\u786e\u7684\u5b66\u751f\u6a21\u578b\uff0c\u5229\u7528\u66f4\u9ad8\u5bb9\u91cf\u7684 TA \u6a21\u578b\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db3\u5feb\u901f\u53d8\u5316\u7684\u670d\u52a1\u7ea6\u675f\uff0c\u5e76\u51cf\u5c11\u6a21\u578b\u5f00\u53d1\u8d44\u6e90\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\u3002", "method": "\u4f7f\u7528 Teacher-TA-Student \u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u5bb9\u91cf\u7684 TA \u6a21\u578b\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002", "result": "\u5728\u751f\u4ea7 ML \u7cfb\u7edf\u4e2d\uff0c\u5173\u952e\u6307\u6807\u63d0\u5347 20%\uff1b\u5728 GPT-2 Medium \u4e0a\uff0cSAT Math \u548c LAMBADA \u57fa\u51c6\u5206\u522b\u63d0\u5347 24% \u548c 10%\u3002", "conclusion": "MatTA \u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u80fd\u591f\u7075\u6d3b\u6743\u8861\u51c6\u786e\u6027\u4e0e\u670d\u52a1\u6210\u672c\u3002", "keywords": "MatTA, Teacher-TA-Student, \u6a21\u578b\u8bad\u7ec3, \u751f\u4ea7\u73af\u5883, \u6027\u80fd\u63d0\u5347"}}
{"id": "2505.23628", "pdf": "https://arxiv.org/pdf/2505.23628", "abs": "https://arxiv.org/abs/2505.23628", "authors": ["Jiaxin Bai", "Wei Fan", "Qi Hu", "Qing Zong", "Chunyang Li", "Hong Ting Tsang", "Hongyu Luo", "Yauwai Yim", "Haoyu Huang", "Xiao Zhou", "Feng Qin", "Tianshi Zheng", "Xi Peng", "Xin Yao", "Huiwen Yang", "Leijie Wu", "Yi Ji", "Gong Zhang", "Renhai Chen", "Yangqiu Song"], "title": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, preprint, code:\n  https://github.com/HKUST-KnowComp/AutoSchemaKG", "summary": "We present AutoSchemaKG, a framework for fully autonomous knowledge graph\nconstruction that eliminates the need for predefined schemas. Our system\nleverages large language models to simultaneously extract knowledge triples and\ninduce comprehensive schemas directly from text, modeling both entities and\nevents while employing conceptualization to organize instances into semantic\ncategories. Processing over 50 million documents, we construct ATLAS (Automated\nTriple Linking And Schema induction), a family of knowledge graphs with 900+\nmillion nodes and 5.9 billion edges. This approach outperforms state-of-the-art\nbaselines on multi-hop QA tasks and enhances LLM factuality. Notably, our\nschema induction achieves 95\\% semantic alignment with human-crafted schemas\nwith zero manual intervention, demonstrating that billion-scale knowledge\ngraphs with dynamically induced schemas can effectively complement parametric\nknowledge in large language models.", "AI": {"tldr": "AutoSchemaKG \u662f\u4e00\u4e2a\u5b8c\u5168\u81ea\u4e3b\u7684\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u6846\u67b6\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u6a21\u5f0f\u3002\u5b83\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u6587\u672c\u4e2d\u63d0\u53d6\u77e5\u8bc6\u4e09\u5143\u7ec4\u5e76\u5f52\u7eb3\u5168\u9762\u6a21\u5f0f\uff0c\u6784\u5efa\u4e86\u5305\u542b\u8d85\u8fc7 9 \u4ebf\u4e2a\u8282\u70b9\u548c 59 \u4ebf\u6761\u8fb9\u7684\u77e5\u8bc6\u56fe\u8c31 ATLAS\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u7684\u8868\u73b0\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u6027\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u4f9d\u8d56\u9884\u5b9a\u4e49\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002AutoSchemaKG \u65e8\u5728\u901a\u8fc7\u52a8\u6001\u6a21\u5f0f\u5f52\u7eb3\uff0c\u5b9e\u73b0\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u5927\u89c4\u6a21\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u3002", "method": "\u501f\u52a9\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u63d0\u53d6\u77e5\u8bc6\u4e09\u5143\u7ec4\u5e76\u5f52\u7eb3\u6a21\u5f0f\uff0c\u901a\u8fc7\u6982\u5ff5\u5316\u5c06\u5b9e\u4f8b\u7ec4\u7ec7\u5230\u8bed\u4e49\u7c7b\u522b\u4e2d\u3002", "result": "\u6784\u5efa\u4e86 ATLAS \u77e5\u8bc6\u56fe\u8c31\uff08900+ \u767e\u4e07\u8282\u70b9\uff0c5.9 \u5341\u4ebf\u8fb9\uff09\uff0c\u5728\u6a21\u5f0f\u5f52\u7eb3\u4e0a\u4e0e\u4eba\u5de5\u6a21\u5f0f\u8bed\u4e49\u5bf9\u9f50\u8fbe 95%\u3002\u5728\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u52a8\u6001\u6a21\u5f0f\u5f52\u7eb3\u6280\u672f\u80fd\u6709\u6548\u6784\u5efa\u5927\u89c4\u6a21\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u6027\u3002", "keywords": "\u77e5\u8bc6\u56fe\u8c31, \u6a21\u5f0f\u5f52\u7eb3, \u5927\u8bed\u8a00\u6a21\u578b, \u81ea\u4e3b\u6784\u5efa, ATLAS"}}
{"id": "2505.23345", "pdf": "https://arxiv.org/pdf/2505.23345", "abs": "https://arxiv.org/abs/2505.23345", "authors": ["Yang Liu", "Deyu Bo", "Wenxuan Cao", "Yuan Fang", "Yawen Li", "Chuan Shi"], "title": "Graph Positional Autoencoders as Self-supervised Learners", "categories": ["cs.LG"], "comment": "12 pages, 3 figures, Accepted at KDD 2025", "summary": "Graph self-supervised learning seeks to learn effective graph representations\nwithout relying on labeled data. Among various approaches, graph autoencoders\n(GAEs) have gained significant attention for their efficiency and scalability.\nTypically, GAEs take incomplete graphs as input and predict missing elements,\nsuch as masked nodes or edges. While effective, our experimental investigation\nreveals that traditional node or edge masking paradigms primarily capture\nlow-frequency signals in the graph and fail to learn the expressive structural\ninformation. To address these issues, we propose Graph Positional Autoencoders\n(GraphPAE), which employs a dual-path architecture to reconstruct both node\nfeatures and positions. Specifically, the feature path uses positional encoding\nto enhance the message-passing processing, improving GAE's ability to predict\nthe corrupted information. The position path, on the other hand, leverages node\nrepresentations to refine positions and approximate eigenvectors, thereby\nenabling the encoder to learn diverse frequency information. We conduct\nextensive experiments to verify the effectiveness of GraphPAE, including\nheterophilic node classification, graph property prediction, and transfer\nlearning. The results demonstrate that GraphPAE achieves state-of-the-art\nperformance and consistently outperforms baselines by a large margin.", "AI": {"tldr": "GraphPAE\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8def\u5f84\u56fe\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u540c\u65f6\u91cd\u6784\u8282\u70b9\u7279\u5f81\u548c\u4f4d\u7f6e\u6765\u6355\u6349\u591a\u6837\u5316\u7684\u9891\u7387\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u56fe\u81ea\u7f16\u7801\u5668\uff08GAEs\uff09\u901a\u8fc7\u8282\u70b9\u6216\u8fb9\u63a9\u7801\u4e3b\u8981\u6355\u83b7\u4f4e\u9891\u4fe1\u53f7\uff0c\u4f46\u5ffd\u7565\u4e86\u8868\u8fbe\u6027\u7ed3\u6784\u4fe1\u606f\u3002\u4e3a\u5f25\u8865\u8fd9\u4e00\u7f3a\u9677\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "GraphPAE\u91c7\u7528\u53cc\u8def\u5f84\u67b6\u6784\uff1a\u7279\u5f81\u8def\u5f84\u901a\u8fc7\u4f4d\u7f6e\u7f16\u7801\u589e\u5f3a\u6d88\u606f\u4f20\u9012\uff1b\u4f4d\u7f6e\u8def\u5f84\u5229\u7528\u8282\u70b9\u8868\u793a\u4f18\u5316\u4f4d\u7f6e\u5e76\u903c\u8fd1\u7279\u5f81\u5411\u91cf\uff0c\u5b66\u4e60\u591a\u6837\u5316\u9891\u7387\u4fe1\u606f\u3002", "result": "\u5728\u5f02\u8d28\u6027\u8282\u70b9\u5206\u7c7b\u3001\u56fe\u5c5e\u6027\u9884\u6d4b\u548c\u8fc1\u79fb\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0cGraphPAE\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "GraphPAE\u901a\u8fc7\u540c\u65f6\u5b66\u4e60\u7279\u5f81\u548c\u4f4d\u7f6e\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u56fe\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u8868\u8fbe\u80fd\u529b\u548c\u6027\u80fd\u3002", "keywords": "\u56fe\u81ea\u7f16\u7801\u5668, \u81ea\u76d1\u7763\u5b66\u4e60, \u4f4d\u7f6e\u7f16\u7801, \u9891\u7387\u4fe1\u606f, \u53cc\u8def\u5f84\u67b6\u6784"}}
{"id": "2505.23630", "pdf": "https://arxiv.org/pdf/2505.23630", "abs": "https://arxiv.org/abs/2505.23630", "authors": ["Enzo Doyen", "Amalia Todirascu"], "title": "GeNRe: A French Gender-Neutral Rewriting System Using Collective Nouns", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings; 9 pages, 2 figures", "summary": "A significant portion of the textual data used in the field of Natural\nLanguage Processing (NLP) exhibits gender biases, particularly due to the use\nof masculine generics (masculine words that are supposed to refer to mixed\ngroups of men and women), which can perpetuate and amplify stereotypes. Gender\nrewriting, an NLP task that involves automatically detecting and replacing\ngendered forms with neutral or opposite forms (e.g., from masculine to\nfeminine), can be employed to mitigate these biases. While such systems have\nbeen developed in a number of languages (English, Arabic, Portuguese, German,\nFrench), automatic use of gender neutralization techniques (as opposed to\ninclusive or gender-switching techniques) has only been studied for English.\nThis paper presents GeNRe, the very first French gender-neutral rewriting\nsystem using collective nouns, which are gender-fixed in French. We introduce a\nrule-based system (RBS) tailored for the French language alongside two\nfine-tuned language models trained on data generated by our RBS. We also\nexplore the use of instruct-based models to enhance the performance of our\nother systems and find that Claude 3 Opus combined with our dictionary achieves\nresults close to our RBS. Through this contribution, we hope to promote the\nadvancement of gender bias mitigation techniques in NLP for French.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86GeNRe\uff0c\u9996\u4e2a\u9488\u5bf9\u6cd5\u8bed\u7684\u6027\u522b\u4e2d\u6027\u91cd\u5199\u7cfb\u7edf\uff0c\u5229\u7528\u96c6\u4f53\u540d\u8bcd\u89e3\u51b3\u6027\u522b\u504f\u89c1\u95ee\u9898\uff0c\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\uff08RBS\uff09\u548c\u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6307\u4ee4\u589e\u5f3a\u6a21\u578b\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3NLP\u9886\u57df\u4e2d\u6cd5\u8bed\u6587\u672c\u56e0\u4f7f\u7528\u7537\u6027\u901a\u7528\u8bcd\u5bfc\u81f4\u7684\u6027\u522b\u504f\u89c1\u95ee\u9898\uff0c\u63a8\u52a8\u6027\u522b\u504f\u89c1\u7f13\u89e3\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\uff08RBS\uff09\u548c\u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408\u6307\u4ee4\u589e\u5f3a\u6a21\u578b\uff08\u5982Claude 3 Opus\uff09\u4f18\u5316\u6027\u80fd\u3002", "result": "RBS\u8868\u73b0\u4f18\u5f02\uff0c\u6307\u4ee4\u589e\u5f3a\u6a21\u578b\u7ed3\u5408\u5b57\u5178\u7684\u6548\u679c\u63a5\u8fd1RBS\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "GeNRe\u7cfb\u7edf\u4e3a\u6cd5\u8bed\u6027\u522b\u4e2d\u6027\u91cd\u5199\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u63a8\u52a8\u4e86NLP\u4e2d\u6027\u522b\u504f\u89c1\u7f13\u89e3\u6280\u672f\u7684\u8fdb\u6b65\u3002", "keywords": "\u6027\u522b\u504f\u89c1, \u6cd5\u8bed, \u4e2d\u6027\u91cd\u5199, \u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf, \u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.23043", "pdf": "https://arxiv.org/pdf/2505.23043", "abs": "https://arxiv.org/abs/2505.23043", "authors": ["Jihai Zhang", "Tianle Li", "Linjie Li", "Zhengyuan Yang", "Yu Cheng"], "title": "Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in unified vision-language models (VLMs), which integrate\nboth visual understanding and generation capabilities, have attracted\nsignificant attention. The underlying hypothesis is that a unified architecture\nwith mixed training on both understanding and generation tasks can enable\nmutual enhancement between understanding and generation. However, this\nhypothesis remains underexplored in prior works on unified VLMs. To address\nthis gap, this paper systematically investigates the generalization across\nunderstanding and generation tasks in unified VLMs. Specifically, we design a\ndataset closely aligned with real-world scenarios to facilitate extensive\nexperiments and quantitative evaluations. We evaluate multiple unified VLM\narchitectures to validate our findings. Our key findings are as follows. First,\nunified VLMs trained with mixed data exhibit mutual benefits in understanding\nand generation tasks across various architectures, and this mutual benefits can\nscale up with increased data. Second, better alignment between multimodal input\nand output spaces will lead to better generalization. Third, the knowledge\nacquired during generation tasks can transfer to understanding tasks, and this\ncross-task generalization occurs within the base language model, beyond\nmodality adapters. Our findings underscore the critical necessity of unifying\nunderstanding and generation in VLMs, offering valuable insights for the design\nand optimization of unified VLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7edf\u4e00\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u4e4b\u95f4\u7684\u76f8\u4e92\u589e\u5f3a\u6548\u679c\uff0c\u53d1\u73b0\u6df7\u5408\u8bad\u7ec3\u80fd\u5e26\u6765\u4efb\u52a1\u95f4\u4e92\u76ca\uff0c\u4e14\u6570\u636e\u91cf\u589e\u52a0\u6548\u679c\u66f4\u4f73\uff0c\u591a\u6a21\u6001\u8f93\u5165\u8f93\u51fa\u7a7a\u95f4\u5bf9\u9f50\u6709\u52a9\u4e8e\u6cdb\u5316\uff0c\u77e5\u8bc6\u53ef\u5728\u751f\u6210\u4e0e\u7406\u89e3\u4efb\u52a1\u95f4\u8fc1\u79fb\u3002", "motivation": "\u63a2\u7d22\u7edf\u4e00VLMs\u4e2d\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u662f\u5426\u76f8\u4e92\u589e\u5f3a\u7684\u5047\u8bbe\uff0c\u8865\u8db3\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u4e3aVLMs\u7684\u8bbe\u8ba1\u4f18\u5316\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u8bbe\u8ba1\u8d34\u8fd1\u771f\u5b9e\u573a\u666f\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u591a\u79cd\u7edf\u4e00VLM\u67b6\u6784\uff0c\u901a\u8fc7\u6df7\u5408\u6570\u636e\u8bad\u7ec3\u53ca\u5b9a\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e92\u76ca\u6027\u3001\u6cdb\u5316\u80fd\u529b\u4e0e\u77e5\u8bc6\u8fc1\u79fb\u6548\u679c\u3002", "result": "\u53d1\u73b0\u6df7\u5408\u8bad\u7ec3\u80fd\u5e26\u6765\u4efb\u52a1\u95f4\u4e92\u76ca\u4e14\u968f\u6570\u636e\u91cf\u589e\u52a0\u66f4\u663e\u8457\uff0c\u591a\u6a21\u6001\u7a7a\u95f4\u5bf9\u9f50\u63d0\u5347\u6cdb\u5316\uff0c\u751f\u6210\u4efb\u52a1\u77e5\u8bc6\u53ef\u8fc1\u79fb\u81f3\u7406\u89e3\u4efb\u52a1\u3002", "conclusion": "\u7edf\u4e00\u7406\u89e3\u4e0e\u751f\u6210\u5bf9VLMs\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u6210\u679c\u4e3aVLM\u8bbe\u8ba1\u4e0e\u4f18\u5316\u63d0\u4f9b\u4e86\u5173\u952e\u6d1e\u89c1\u3002", "keywords": "vision-language models, unified architecture, understanding-generation mutual enhancement, multimodal alignment, cross-task generalization"}}
{"id": "2505.23347", "pdf": "https://arxiv.org/pdf/2505.23347", "abs": "https://arxiv.org/abs/2505.23347", "authors": ["Yuting Li", "Shaoyuan Huang", "Tengwen Zhang", "Cheng Zhang", "Xiaofei Wang", "Victor C. M. Leung"], "title": "Sentinel: Scheduling Live Streams with Proactive Anomaly Detection in Crowdsourced Cloud-Edge Platforms", "categories": ["cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2402.14619", "summary": "With the rapid growth of live streaming services, Crowdsourced Cloud-edge\nservice Platforms (CCPs) are playing an increasingly important role in meeting\nthe increasing demand. Although stream scheduling plays a critical role in\noptimizing CCPs' revenue, most optimization strategies struggle to achieve\npractical results due to various anomalies in unstable CCPs. Additionally, the\nsubstantial scale of CCPs magnifies the difficulties of anomaly detection in\ntime-sensitive scheduling. To tackle these challenges, this paper proposes\nSentinel, a proactive anomaly detection-based scheduling framework. Sentinel\nmodels the scheduling process as a two-stage Pre-Post-Scheduling paradigm: in\nthe pre-scheduling stage, Sentinel conducts anomaly detection and constructs a\nstrategy pool; in the post-scheduling stage, upon request arrival, it triggers\nan appropriate scheduling based on a pre-generated strategy to implement the\nscheduling process. Extensive experiments on realistic datasets show that\nSentinel significantly reduces anomaly frequency by 70%, improves revenue by\n74%, and doubles the scheduling speed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSentinel\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8c03\u5ea6\uff08\u9884\u8c03\u5ea6\u548c\u540e\u8c03\u5ea6\uff09\u4e3b\u52a8\u68c0\u6d4b\u5f02\u5e38\u5e76\u4f18\u5316\u6d41\u8c03\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u5f02\u5e38\u9891\u7387\u3001\u63d0\u9ad8\u6536\u5165\u53ca\u8c03\u5ea6\u901f\u5ea6\u3002", "motivation": "\u76f4\u64ad\u6d41\u670d\u52a1\u7684\u5feb\u901f\u589e\u957f\u4f7f\u5f97\u4f17\u5305\u4e91\u8fb9\u5e73\u53f0\uff08CCPs\uff09\u9700\u6c42\u6fc0\u589e\uff0c\u4f46\u73b0\u6709\u8c03\u5ea6\u7b56\u7565\u56e0\u5f02\u5e38\u68c0\u6d4b\u56f0\u96be\u96be\u4ee5\u5b9e\u73b0\u4f18\u5316\u6548\u679c\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSentinel\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8c03\u5ea6\u6a21\u5f0f\uff1a\u9884\u8c03\u5ea6\u9636\u6bb5\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\u5e76\u6784\u5efa\u7b56\u7565\u6c60\uff1b\u540e\u8c03\u5ea6\u9636\u6bb5\u6839\u636e\u8bf7\u6c42\u89e6\u53d1\u9884\u751f\u6210\u7b56\u7565\u5b8c\u6210\u8c03\u5ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793aSentinel\u5c06\u5f02\u5e38\u9891\u7387\u964d\u4f4e70%\uff0c\u6536\u5165\u63d0\u534774%\uff0c\u8c03\u5ea6\u901f\u5ea6\u7ffb\u500d\u3002", "conclusion": "Sentinel\u901a\u8fc7\u4e3b\u52a8\u5f02\u5e38\u68c0\u6d4b\u4e0e\u4e24\u9636\u6bb5\u8c03\u5ea6\uff0c\u663e\u8457\u63d0\u5347CCPs\u7684\u7a33\u5b9a\u6027\u548c\u6536\u76ca\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5b9e\u65f6\u8c03\u5ea6\u573a\u666f\u3002", "keywords": "\u4f17\u5305\u4e91\u8fb9\u5e73\u53f0\uff08CCPs\uff09\u3001\u6d41\u8c03\u5ea6\u3001\u5f02\u5e38\u68c0\u6d4b\u3001\u4e24\u9636\u6bb5\u8c03\u5ea6\u3001Sentinel\u6846\u67b6"}}
{"id": "2505.23646", "pdf": "https://arxiv.org/pdf/2505.23646", "abs": "https://arxiv.org/abs/2505.23646", "authors": ["Zijun Yao", "Yantao Liu", "Yanxu Chen", "Jianhui Chen", "Junfeng Fang", "Lei Hou", "Juanzi Li", "Tat-Seng Chua"], "title": "Are Reasoning Models More Prone to Hallucination?", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recently evolved large reasoning models (LRMs) show powerful performance in\nsolving complex tasks with long chain-of-thought (CoT) reasoning capability. As\nthese LRMs are mostly developed by post-training on formal reasoning tasks,\nwhether they generalize the reasoning capability to help reduce hallucination\nin fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1\nreports increased performance on SimpleQA, a fact-seeking benchmark, while\nOpenAI-o3 observes even severer hallucination. This discrepancy naturally\nraises the following research question: Are reasoning models more prone to\nhallucination? This paper addresses the question from three perspectives. (1)\nWe first conduct a holistic evaluation for the hallucination in LRMs. Our\nanalysis reveals that LRMs undergo a full post-training pipeline with cold\nstart supervised fine-tuning (SFT) and verifiable reward RL generally alleviate\ntheir hallucination. In contrast, both distillation alone and RL training\nwithout cold start fine-tuning introduce more nuanced hallucinations. (2) To\nexplore why different post-training pipelines alters the impact on\nhallucination in LRMs, we conduct behavior analysis. We characterize two\ncritical cognitive behaviors that directly affect the factuality of a LRM: Flaw\nRepetition, where the surface-level reasoning attempts repeatedly follow the\nsame underlying flawed logic, and Think-Answer Mismatch, where the final answer\nfails to faithfully match the previous CoT process. (3) Further, we investigate\nthe mechanism behind the hallucination of LRMs from the perspective of model\nuncertainty. We find that increased hallucination of LRMs is usually associated\nwith the misalignment between model uncertainty and factual accuracy. Our work\nprovides an initial understanding of the hallucination in LRMs.", "AI": {"tldr": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u867d\u7136\u80fd\u89e3\u51b3\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u5176\u5728\u4e8b\u5b9e\u67e5\u8be2\u4efb\u52a1\u4e2d\u662f\u5426\u51cf\u5c11\u5e7b\u89c9\u4ecd\u5b58\u5728\u4e89\u8bae\u3002\u672c\u6587\u901a\u8fc7\u5168\u9762\u8bc4\u4f30\u3001\u884c\u4e3a\u5206\u6790\u548c\u673a\u5236\u63a2\u8ba8\uff0c\u53d1\u73b0\u51b7\u542f\u52a8\u76d1\u7763\u5fae\u8c03\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u80fd\u51cf\u8f7b\u5e7b\u89c9\uff0c\u5e76\u63d0\u51fa\u91cd\u590d\u9519\u8bef\u4e0e\u601d\u7ef4\u7b54\u6848\u4e0d\u5339\u914d\u662f\u5f71\u54cd\u4e8b\u5b9e\u6027\u7684\u5173\u952e\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u4e8b\u5b9e\u67e5\u8be2\u4efb\u52a1\u4e2d\u662f\u5426\u56e0\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u800c\u52a0\u5267\u5e7b\u89c9\u73b0\u8c61\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u8bad\u7ec3\u6d41\u7a0b\u5bf9\u5e7b\u89c9\u7684\u5f71\u54cd\u673a\u5236\u3002", "method": "\u901a\u8fc7\uff081\uff09\u5168\u9762\u8bc4\u4f30\u4e0d\u540c\u8bad\u7ec3\u6d41\u7a0b\u5bf9\u5e7b\u89c9\u7684\u5f71\u54cd\uff0c\uff082\uff09\u884c\u4e3a\u5206\u6790\uff08\u5982\u91cd\u590d\u9519\u8bef\u548c\u601d\u7ef4\u7b54\u6848\u4e0d\u5339\u914d\uff09\uff0c\uff083\uff09\u4ece\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u89d2\u5ea6\u7814\u7a76\u5e7b\u89c9\u673a\u5236\u3002", "result": "\u51b7\u542f\u52a8\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u80fd\u51cf\u8f7b\u5e7b\u89c9\uff0c\u800c\u4ec5\u84b8\u998f\u6216\u65e0\u51b7\u542f\u52a8\u7684RL\u8bad\u7ec3\u4f1a\u52a0\u5267\u5e7b\u89c9\u3002\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0e\u4e8b\u5b9e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u9519\u914d\u662f\u5e7b\u89c9\u589e\u52a0\u7684\u539f\u56e0\u3002", "conclusion": "\u672c\u6587\u521d\u6b65\u63ed\u793a\u4e86LRMs\u5e7b\u89c9\u7684\u6210\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u7f13\u89e3\u5e7b\u89c9\u7684\u4f18\u5316\u65b9\u5411\uff0c\u5982\u6539\u8fdb\u8bad\u7ec3\u6d41\u7a0b\u548c\u8ba4\u77e5\u884c\u4e3a\u5bf9\u9f50\u3002", "keywords": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u3001\u5e7b\u89c9\u3001\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u8ba4\u77e5\u884c\u4e3a\u3001\u6a21\u578b\u4e0d\u786e\u5b9a\u6027"}}
{"id": "2505.23045", "pdf": "https://arxiv.org/pdf/2505.23045", "abs": "https://arxiv.org/abs/2505.23045", "authors": ["Chuanhao Li", "Wenbo Ye", "Zhen Li", "Yuwei Wu", "Yunde Jia"], "title": "Multi-Sourced Compositional Generalization in Visual Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Compositional generalization is the ability of generalizing novel\ncompositions from seen primitives, and has received much attention in\nvision-and-language (V\\&L) recently. Due to the multi-modal nature of V\\&L\ntasks, the primitives composing compositions source from different modalities,\nresulting in multi-sourced novel compositions. However, the generalization\nability over multi-sourced novel compositions, \\textit{i.e.}, multi-sourced\ncompositional generalization (MSCG) remains unexplored. In this paper, we\nexplore MSCG in the context of visual question answering (VQA), and propose a\nretrieval-augmented training framework to enhance the MSCG ability of VQA\nmodels by learning unified representations for primitives from different\nmodalities. Specifically, semantically equivalent primitives are retrieved for\neach primitive in the training samples, and the retrieved features are\naggregated with the original primitive to refine the model. This process helps\nthe model learn consistent representations for the same semantic primitives\nacross different modalities. To evaluate the MSCG ability of VQA models, we\nconstruct a new GQA-MSCG dataset based on the GQA dataset, in which samples\ninclude three types of novel compositions composed of primitives from different\nmodalities. Experimental results demonstrate the effectiveness of the proposed\nframework. We release GQA-MSCG at https://github.com/NeverMoreLCH/MSCG.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u89c6\u89c9\u4e0e\u8bed\u8a00\u4efb\u52a1\u4e2d\u591a\u6e90\u7ec4\u5408\u6cdb\u5316\uff08MSCG\uff09\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u7d22\u589e\u5f3a\u7684\u8bad\u7ec3\u6846\u67b6\u6765\u63d0\u9ad8\u89c6\u89c9\u95ee\u7b54\u6a21\u578b\u7684MSCG\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u6784\u5efa\u65b0\u6570\u636e\u96c6GQA-MSCG\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u7531\u4e8e\u89c6\u89c9\u4e0e\u8bed\u8a00\u4efb\u52a1\u7684\u591a\u6a21\u6001\u7279\u6027\uff0c\u4e0d\u540c\u6a21\u6001\u7684\u57fa\u5143\u7ec4\u5408\u53ef\u80fd\u5bfc\u81f4\u591a\u6e90\u65b0\u7ec4\u5408\u7684\u6cdb\u5316\u80fd\u529b\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u68c0\u7d22\u8bed\u4e49\u7b49\u6548\u7684\u57fa\u5143\u5e76\u805a\u5408\u5176\u7279\u5f81\uff0c\u5b66\u4e60\u8de8\u6a21\u6001\u7684\u7edf\u4e00\u8868\u793a\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u7684\u591a\u6e90\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u68c0\u7d22\u589e\u5f3a\u8bad\u7ec3\u6846\u67b6\u80fd\u6709\u6548\u63d0\u9ad8\u89c6\u89c9\u95ee\u7b54\u6a21\u578b\u7684MSCG\u80fd\u529b\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u6e90\u7ec4\u5408\u6cdb\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u53d1\u5e03\u4e86GQA-MSCG\u6570\u636e\u96c6\u4ee5\u4f9b\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "keywords": "\u7ec4\u5408\u6cdb\u5316\u3001\u89c6\u89c9\u4e0e\u8bed\u8a00\u3001\u591a\u6e90\u7ec4\u5408\u3001\u68c0\u7d22\u589e\u5f3a\u8bad\u7ec3\u3001\u89c6\u89c9\u95ee\u7b54"}}
{"id": "2505.23349", "pdf": "https://arxiv.org/pdf/2505.23349", "abs": "https://arxiv.org/abs/2505.23349", "authors": ["Sheng Ouyang", "Yulan Hu", "Ge Chen", "Qingyang Li", "Fuzheng Zhang", "Yong Liu"], "title": "Towards Reward Fairness in RLHF: From a Resource Allocation Perspective", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ACL 2025", "summary": "Rewards serve as proxies for human preferences and play a crucial role in\nReinforcement Learning from Human Feedback (RLHF). However, if these rewards\nare inherently imperfect, exhibiting various biases, they can adversely affect\nthe alignment of large language models (LLMs). In this paper, we collectively\ndefine the various biases present in rewards as the problem of reward\nunfairness. We propose a bias-agnostic method to address the issue of reward\nfairness from a resource allocation perspective, without specifically designing\nfor each type of bias, yet effectively mitigating them. Specifically, we model\npreference learning as a resource allocation problem, treating rewards as\nresources to be allocated while considering the trade-off between utility and\nfairness in their distribution. We propose two methods, Fairness Regularization\nand Fairness Coefficient, to achieve fairness in rewards. We apply our methods\nin both verification and reinforcement learning scenarios to obtain a fairness\nreward model and a policy model, respectively. Experiments conducted in these\nscenarios demonstrate that our approach aligns LLMs with human preferences in a\nmore fair manner.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u5956\u52b1\u4e0d\u516c\u5e73\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u504f\u597d\u5b66\u4e60\u5efa\u6a21\u4e3a\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5229\u7528\u516c\u5e73\u6027\u6b63\u5219\u5316\u548c\u516c\u5e73\u7cfb\u6570\u6765\u4f18\u5316\u5956\u52b1\u5206\u914d\uff0c\u4ece\u800c\u66f4\u516c\u5e73\u5730\u5bf9\u9f50\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u4eba\u7c7b\u504f\u597d\u3002", "motivation": "\u5956\u52b1\u5728\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u4e2d\u4f5c\u4e3a\u4eba\u7c7b\u504f\u597d\u7684\u4ee3\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u56fa\u6709\u7684\u4e0d\u5b8c\u7f8e\u548c\u504f\u5dee\u53ef\u80fd\u5f71\u54cdLLMs\u7684\u5bf9\u9f50\u6548\u679c\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5956\u52b1\u504f\u5dee\u95ee\u9898\uff0c\u5b9a\u4e49\u4e3a\u5956\u52b1\u4e0d\u516c\u5e73\u3002", "method": "\u8bba\u6587\u4ece\u8d44\u6e90\u5206\u914d\u89d2\u5ea6\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u516c\u5e73\u6027\u6b63\u5219\u5316\uff08Fairness Regularization\uff09\u548c\u516c\u5e73\u7cfb\u6570\uff08Fairness Coefficient\uff09\uff0c\u7528\u4e8e\u5728\u4e0d\u9488\u5bf9\u5177\u4f53\u504f\u5dee\u7c7b\u578b\u8bbe\u8ba1\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u5956\u52b1\u5206\u914d\uff0c\u5e73\u8861\u6548\u7528\u4e0e\u516c\u5e73\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9a8c\u8bc1\u548c\u5f3a\u5316\u5b66\u4e60\u573a\u666f\u4e2d\u5747\u80fd\u6709\u6548\u63d0\u5347\u5956\u52b1\u6a21\u578b\u7684\u516c\u5e73\u6027\uff0c\u4ece\u800c\u66f4\u516c\u5e73\u5730\u5bf9\u9f50LLMs\u4e0e\u4eba\u7c7b\u504f\u597d\u3002", "conclusion": "\u901a\u8fc7\u8d44\u6e90\u5206\u914d\u89c6\u89d2\u89e3\u51b3\u5956\u52b1\u4e0d\u516c\u5e73\u95ee\u9898\u662f\u6709\u6548\u7684\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u901a\u7528\u7684\u504f\u5dee\u7f13\u89e3\u65b9\u6cd5\u3002", "keywords": "\u5956\u52b1\u4e0d\u516c\u5e73, \u8d44\u6e90\u5206\u914d, \u516c\u5e73\u6027\u6b63\u5219\u5316, \u516c\u5e73\u7cfb\u6570, RLHF"}}
{"id": "2505.23654", "pdf": "https://arxiv.org/pdf/2505.23654", "abs": "https://arxiv.org/abs/2505.23654", "authors": ["Mohamed Elaraby", "Diane Litman"], "title": "ARC: Argument Representation and Coverage Analysis for Zero-Shot Long Document Summarization with Instruction Following LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Integrating structured information has long improved the quality of\nabstractive summarization, particularly in retaining salient content. In this\nwork, we focus on a specific form of structure: argument roles, which are\ncrucial for summarizing documents in high-stakes domains such as law. We\ninvestigate whether instruction-tuned large language models (LLMs) adequately\npreserve this information. To this end, we introduce Argument Representation\nCoverage (ARC), a framework for measuring how well LLM-generated summaries\ncapture salient arguments. Using ARC, we analyze summaries produced by three\nopen-weight LLMs in two domains where argument roles are central: long legal\nopinions and scientific articles. Our results show that while LLMs cover\nsalient argument roles to some extent, critical information is often omitted in\ngenerated summaries, particularly when arguments are sparsely distributed\nthroughout the input. Further, we use ARC to uncover behavioral patterns --\nspecifically, how the positional bias of LLM context windows and role-specific\npreferences impact the coverage of key arguments in generated summaries,\nemphasizing the need for more argument-aware summarization strategies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u6458\u8981\u65f6\u5982\u4f55\u4fdd\u7559\u8bba\u636e\u89d2\u8272\u7684\u4fe1\u606f\uff0c\u63d0\u51faArgument Representation Coverage\uff08ARC\uff09\u6846\u67b6\u6765\u8bc4\u4f30\uff0c\u53d1\u73b0LLMs\u5728\u6458\u8981\u4e2d\u9057\u6f0f\u5173\u952e\u8bba\u636e\u4fe1\u606f\uff0c\u547c\u5401\u5f00\u53d1\u66f4\u6ce8\u91cd\u8bba\u636e\u7684\u6458\u8981\u7b56\u7565\u3002", "motivation": "\u8bba\u636e\u89d2\u8272\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u6cd5\u5f8b\uff09\u7684\u6458\u8981\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5145\u5206\u4fdd\u7559\u8fd9\u4e9b\u4fe1\u606f\u3002", "method": "\u5f15\u5165ARC\u6846\u67b6\uff0c\u8bc4\u4f30\u4e09\u4e2a\u5f00\u6e90\u6027LLM\u5728\u6cd5\u5f8b\u610f\u89c1\u4e66\u548c\u79d1\u5b66\u8bba\u6587\u4e24\u79cd\u9886\u57df\u751f\u6210\u7684\u6458\u8981\u5bf9\u8bba\u636e\u89d2\u8272\u7684\u8986\u76d6\u60c5\u51b5\u3002", "result": "LLMs\u80fd\u90e8\u5206\u8986\u76d6\u91cd\u8981\u8bba\u636e\u89d2\u8272\uff0c\u4f46\u5f53\u8bba\u636e\u5206\u6563\u65f6\u6613\u9057\u6f0f\u5173\u952e\u4fe1\u606f\uff0c\u4e14\u6a21\u578b\u7684\u4f4d\u7f6e\u504f\u89c1\u5f71\u54cd\u8bba\u636e\u8986\u76d6\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u66f4\u6ce8\u91cd\u8bba\u636e\u7684\u6458\u8981\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3LLMs\u5728\u751f\u6210\u6458\u8981\u65f6\u7684\u5c40\u9650\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u8bba\u636e\u89d2\u8272, \u6458\u8981\u8d28\u91cf, \u7ed3\u6784\u4fe1\u606f, ARC\u6846\u67b6"}}
{"id": "2505.23053", "pdf": "https://arxiv.org/pdf/2505.23053", "abs": "https://arxiv.org/abs/2505.23053", "authors": ["Wei-Hsiang Huang", "Chen-Wei Ke", "Wei-Ning Chiu", "Yu-Xuan Su", "Chun-Chun Yang", "Chieh-Yuan Cheng", "Yun-Nung Chen", "Pu-Jen Cheng"], "title": "Augment or Not? A Comparative Study of Pure and Augmented Large Language Model Recommenders", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have introduced new paradigms for recommender\nsystems by enabling richer semantic understanding and incorporating implicit\nworld knowledge. In this study, we propose a systematic taxonomy that\nclassifies existing approaches into two categories: (1) Pure LLM Recommenders,\nwhich rely solely on LLMs, and (2) Augmented LLM Recommenders, which integrate\nadditional non-LLM techniques to enhance performance. This taxonomy provides a\nnovel lens through which to examine the evolving landscape of LLM-based\nrecommendation. To support fair comparison, we introduce a unified evaluation\nplatform that benchmarks representative models under consistent experimental\nsettings, highlighting key design choices that impact effectiveness. We\nconclude by discussing open challenges and outlining promising directions for\nfuture research. This work offers both a comprehensive overview and practical\nguidance for advancing next-generation LLM-powered recommender.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5206\u7c7b\u6cd5\uff0c\u5c06\u57fa\u4e8eLLM\u7684\u63a8\u8350\u7cfb\u7edf\u5206\u4e3a\u7eafLLM\u63a8\u8350\u548c\u589e\u5f3aLLM\u63a8\u8350\u4e24\u7c7b\uff0c\u5e76\u5f15\u5165\u7edf\u4e00\u7684\u8bc4\u4f30\u5e73\u53f0\u8fdb\u884c\u516c\u5e73\u6bd4\u8f83\uff0c\u603b\u7ed3\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u63a2\u7a76\u5982\u4f55\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u7684\u8bed\u4e49\u7406\u89e3\u548c\u4e16\u754c\u77e5\u8bc6\u878d\u5408\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5206\u7c7b\u6cd5\uff08\u7eafLLM\u63a8\u8350\u548c\u589e\u5f3aLLM\u63a8\u8350\uff09\uff0c\u5e76\u5f00\u53d1\u7edf\u4e00\u8bc4\u4f30\u5e73\u53f0\u8fdb\u884c\u6a21\u578b\u6bd4\u8f83\u3002", "result": "\u901a\u8fc7\u7edf\u4e00\u8bc4\u4f30\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u5bf9\u63a8\u8350\u6548\u679c\u7684\u5f71\u54cd\u3002", "conclusion": "\u603b\u7ed3\u4e86LLM\u63a8\u8350\u7cfb\u7edf\u7684\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u6307\u5bfc\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u63a8\u8350\u7cfb\u7edf, \u5206\u7c7b\u6cd5, \u8bc4\u4f30\u5e73\u53f0"}}
{"id": "2505.23355", "pdf": "https://arxiv.org/pdf/2505.23355", "abs": "https://arxiv.org/abs/2505.23355", "authors": ["Maxiu Xiao", "Jianglin Lan", "Jingxing Yu", "Eldert van Henten", "Congcong Sun"], "title": "Grower-in-the-Loop Interactive Reinforcement Learning for Greenhouse Climate Control", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Climate control is crucial for greenhouse production as it directly affects\ncrop growth and resource use. Reinforcement learning (RL) has received\nincreasing attention in this field, but still faces challenges, including\nlimited training efficiency and high reliance on initial learning conditions.\nInteractive RL, which combines human (grower) input with the RL agent's\nlearning, offers a potential solution to overcome these challenges. However,\ninteractive RL has not yet been applied to greenhouse climate control and may\nface challenges related to imperfect inputs. Therefore, this paper aims to\nexplore the possibility and performance of applying interactive RL with\nimperfect inputs into greenhouse climate control, by: (1) developing three\nrepresentative interactive RL algorithms tailored for greenhouse climate\ncontrol (reward shaping, policy shaping and control sharing); (2) analyzing how\ninput characteristics are often contradicting, and how the trade-offs between\nthem make grower's inputs difficult to perfect; (3) proposing a neural\nnetwork-based approach to enhance the robustness of interactive RL agents under\nlimited input availability; (4) conducting a comprehensive evaluation of the\nthree interactive RL algorithms with imperfect inputs in a simulated greenhouse\nenvironment. The demonstration shows that interactive RL incorporating\nimperfect grower inputs has the potential to improve the performance of the RL\nagent. RL algorithms that influence action selection, such as policy shaping\nand control sharing, perform better when dealing with imperfect inputs,\nachieving 8.4% and 6.8% improvement in profit, respectively. In contrast,\nreward shaping, an algorithm that manipulates the reward function, is sensitive\nto imperfect inputs and leads to a 9.4% decrease in profit. This highlights the\nimportance of selecting an appropriate mechanism when incorporating imperfect\ninputs.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4ea4\u4e92\u5f0f\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u6e29\u5ba4\u6c14\u5019\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u4e09\u79cd\u7b97\u6cd5\u5728\u8f93\u5165\u4e0d\u5b8c\u7f8e\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f71\u54cd\u52a8\u4f5c\u9009\u62e9\u7684\u7b97\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u6e29\u5ba4\u6c14\u5019\u63a7\u5236\u5bf9\u4f5c\u7269\u751f\u957f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6548\u7387\u4f4e\u4e14\u4f9d\u8d56\u521d\u59cb\u6761\u4ef6\u3002\u4ea4\u4e92\u5f0fRL\u7ed3\u5408\u79cd\u690d\u8005\u8f93\u5165\uff0c\u53ef\u80fd\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u8f93\u5165\u4e0d\u5b8c\u7f8e\u7684\u6311\u6218\u5c1a\u672a\u7814\u7a76\u3002", "method": "1. \u5f00\u53d1\u4e09\u79cd\u4ea4\u4e92\u5f0fRL\u7b97\u6cd5\uff08\u5956\u52b1\u5851\u9020\u3001\u7b56\u7565\u5851\u9020\u548c\u63a7\u5236\u5171\u4eab\uff09\u30022. \u5206\u6790\u8f93\u5165\u77db\u76fe\u6027\u548c\u6743\u8861\u30023. \u63d0\u51fa\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u589e\u5f3a\u9c81\u68d2\u6027\u30024. \u5728\u6a21\u62df\u73af\u5883\u4e2d\u5168\u9762\u8bc4\u4f30\u7b97\u6cd5\u8868\u73b0\u3002", "result": "\u7b56\u7565\u5851\u9020\u548c\u63a7\u5236\u5171\u4eab\u5206\u522b\u63d0\u5347\u5229\u6da68.4%\u548c6.8%\uff0c\u800c\u5956\u52b1\u5851\u9020\u53d7\u8f93\u5165\u4e0d\u5b8c\u7f8e\u5f71\u54cd\u5bfc\u81f4\u5229\u6da6\u4e0b\u964d9.4%\u3002", "conclusion": "\u4ea4\u4e92\u5f0fRL\u7ed3\u5408\u4e0d\u5b8c\u7f8e\u8f93\u5165\u53ef\u63d0\u5347RL\u4ee3\u7406\u6027\u80fd\uff0c\u4f46\u9700\u9009\u62e9\u5408\u9002\u673a\u5236\u4ee5\u907f\u514d\u8d1f\u9762\u5f71\u54cd\u3002", "keywords": "\u6e29\u5ba4\u6c14\u5019\u63a7\u5236, \u5f3a\u5316\u5b66\u4e60, \u4ea4\u4e92\u5f0fRL, \u4e0d\u5b8c\u7f8e\u8f93\u5165, \u795e\u7ecf\u7f51\u7edc"}}
{"id": "2505.23657", "pdf": "https://arxiv.org/pdf/2505.23657", "abs": "https://arxiv.org/abs/2505.23657", "authors": ["Hongxiang Zhang", "Hao Chen", "Tianyi Zhang", "Muhao Chen"], "title": "Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent decoding methods improve the factuality of large language\nmodels~(LLMs) by refining how the next token is selected during generation.\nThese methods typically operate at the token level, leveraging internal\nrepresentations to suppress superficial patterns. Nevertheless, LLMs remain\nprone to hallucinations, especially over longer contexts. In this paper, we\npropose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy\nthat actively decides when to apply contrasting layers during generation. By\ncasting decoding as a sequential decision-making problem, ActLCD employs a\nreinforcement learning policy guided by a reward-aware classifier to optimize\nfactuality beyond the token level. Our experiments demonstrate that ActLCD\nsurpasses state-of-the-art methods across five benchmarks, showcasing its\neffectiveness in mitigating hallucinations in diverse generation scenarios.", "AI": {"tldr": "ActLCD\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4f18\u5316\u89e3\u7801\u8fc7\u7a0b\uff0c\u6709\u6548\u51cf\u5c11LLMs\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u5728token\u7ea7\u522b\u4f18\u5316\u751f\u6210\u7684\u6b63\u786e\u6027\uff0c\u4f46LLMs\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u4ecd\u6613\u51fa\u73b0\u5e7b\u89c9\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u7801\u7b56\u7565\u3002", "method": "\u63d0\u51faActive Layer-Contrastive Decoding (ActLCD)\uff0c\u5c06\u89e3\u7801\u5efa\u6a21\u4e3a\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u548c\u5956\u52b1\u611f\u77e5\u5206\u7c7b\u5668\u52a8\u6001\u8c03\u6574\u5bf9\u6bd4\u5c42\u5e94\u7528\u65f6\u673a\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u591a\u6837\u751f\u6210\u573a\u666f\u4e2d\u7684\u5e7b\u89c9\u3002", "conclusion": "ActLCD\u901a\u8fc7\u52a8\u6001\u5c42\u5bf9\u6bd4\u89e3\u7801\u6709\u6548\u63d0\u5347\u4e86LLMs\u751f\u6210\u5185\u5bb9\u7684\u771f\u5b9e\u6027\uff0c\u5c24\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "keywords": "LLMs, \u89e3\u7801\u7b56\u7565, \u5f3a\u5316\u5b66\u4e60, \u5e8f\u5217\u51b3\u7b56, \u5e7b\u89c9\u7f13\u89e3"}}
{"id": "2505.23059", "pdf": "https://arxiv.org/pdf/2505.23059", "abs": "https://arxiv.org/abs/2505.23059", "authors": ["Dohyeon Lee", "Yeonseok Jeong", "Seung-won Hwang"], "title": "From Token to Action: State Machine Reasoning to Mitigate Overthinking in Information Retrieval", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting enables complex reasoning in large language\nmodels (LLMs), including applications in information retrieval (IR). However,\nit often leads to overthinking, where models produce excessively long and\nsemantically redundant traces with little or no benefit. We identify two key\nchallenges in IR: redundant trajectories that revisit similar states and\nmisguided reasoning that diverges from user intent. To address these, we\npropose State Machine Reasoning (SMR), a transition-based reasoning framework\ncomposed of discrete actions (Refine, Rerank, Stop) that support early stopping\nand fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show\nthat SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token\nusage by 74.4%. It generalizes across LLMs and retrievers without requiring\ntask-specific tuning, offering a practical alternative to conventional CoT\nreasoning. The code and details are available at https://github.com/ldilab/SMR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86State Machine Reasoning (SMR)\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86Chain-of-Thought (CoT)\u63a8\u7406\u4e2d\u8fc7\u5ea6\u601d\u8003\u548c\u8bed\u4e49\u5197\u4f59\u7684\u95ee\u9898\uff0c\u5728\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u9488\u5bf9Chain-of-Thought (CoT)\u63d0\u793a\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u5bfc\u81f4\u7684\u8fc7\u5ea6\u601d\u8003\u548c\u5197\u4f59\u8f68\u8ff9\u95ee\u9898\uff0c\u4f5c\u8005\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u63a8\u7406\u6846\u67b6\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86State Machine Reasoning (SMR)\uff0c\u4e00\u4e2a\u57fa\u4e8e\u79bb\u6563\u52a8\u4f5c\uff08Refine\u3001Rerank\u3001Stop\uff09\u7684\u8fc7\u6e21\u63a8\u7406\u6846\u67b6\uff0c\u652f\u6301\u65e9\u671f\u505c\u6b62\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "result": "\u5728BEIR\u548cBRIGHT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSMR\u5c06\u68c0\u7d22\u6027\u80fd\uff08nDCG@10\uff09\u63d0\u9ad8\u4e863.4%\uff0c\u540c\u65f6\u51cf\u5c11\u4e8674.4%\u7684\u4ee4\u724c\u4f7f\u7528\u91cf\u3002", "conclusion": "SMR\u662f\u4e00\u79cd\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8c03\u4f18\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u4e3a\u4f20\u7edfCoT\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\u3002", "keywords": "Chain-of-Thought, State Machine Reasoning, information retrieval, large language models"}}
{"id": "2505.23369", "pdf": "https://arxiv.org/pdf/2505.23369", "abs": "https://arxiv.org/abs/2505.23369", "authors": ["Mannmohan Muthuraman"], "title": "Dynamic Spectral Backpropagation for Efficient Neural Network Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Dynamic Spectral Backpropagation (DSBP) enhances neural network training\nunder resource constraints by projecting gradients onto principal eigenvectors,\nreducing complexity and promoting flat minima. Five extensions are proposed,\ndynamic spectral inference, spectral architecture optimization, spectral meta\nlearning, spectral transfer regularization, and Lie algebra inspired dynamics,\nto address challenges in robustness, fewshot learning, and hardware efficiency.\nSupported by a third order stochastic differential equation (SDE) and a PAC\nBayes limit, DSBP outperforms Sharpness Aware Minimization (SAM), Low Rank\nAdaptation (LoRA), and Model Agnostic Meta Learning (MAML) on CIFAR 10, Fashion\nMNIST, MedMNIST, and Tiny ImageNet, as demonstrated through extensive\nexperiments and visualizations. Future work focuses on scalability, bias\nmitigation, and ethical considerations.", "AI": {"tldr": "Dynamic Spectral Backpropagation (DSBP) \u901a\u8fc7\u5c06\u68af\u5ea6\u6295\u5f71\u5230\u4e3b\u7279\u5f81\u5411\u91cf\u4e0a\uff0c\u964d\u4f4e\u590d\u6742\u5ea6\u5e76\u4fc3\u8fdb\u5e73\u5766\u6700\u5c0f\u503c\uff0c\u4ece\u800c\u5728\u8d44\u6e90\u53d7\u9650\u60c5\u51b5\u4e0b\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u3002\u63d0\u51fa\u7684\u4e94\u79cd\u6269\u5c55\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u9ad8\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\uff08\u5982 SAM\u3001LoRA\u3001MAML\uff09\u5728\u5c11\u6837\u672c\u5b66\u4e60\u548c\u786c\u4ef6\u6548\u7387\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u52a8\u6001\u8c31\u53cd\u5411\u4f20\u64ad\uff08DSBP\uff09\uff0c\u7ed3\u5408\u52a8\u6001\u8c31\u63a8\u65ad\u3001\u8c31\u67b6\u6784\u4f18\u5316\u3001\u8c31\u5143\u5b66\u4e60\u3001\u8c31\u8fc1\u79fb\u6b63\u5219\u5316\u548c\u674e\u4ee3\u6570\u52a8\u6001\u4e94\u79cd\u6269\u5c55\u65b9\u6cd5\uff0c\u7406\u8bba\u652f\u6301\u4e3a\u4e09\u9636\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\u548c PAC-Bayes \u6781\u9650\u3002", "result": "\u5728 CIFAR-10\u3001Fashion MNIST\u3001MedMNIST \u548c Tiny ImageNet \u4e0a\uff0cDSBP \u7684\u8868\u73b0\u4f18\u4e8e SAM\u3001LoRA \u548c MAML\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u548c\u53ef\u89c6\u5316\u9a8c\u8bc1\u3002", "conclusion": "DSBP \u5728\u8d44\u6e90\u53d7\u9650\u548c\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u6269\u5c55\u6027\u3001\u504f\u5dee\u7f13\u89e3\u548c\u4f26\u7406\u95ee\u9898\u3002", "keywords": "Dynamic Spectral Backpropagation (DSBP), \u68af\u5ea6\u6295\u5f71, \u5e73\u5766\u6700\u5c0f\u503c, \u5c11\u6837\u672c\u5b66\u4e60, \u4e09\u9636\u968f\u673a\u5fae\u5206\u65b9\u7a0b (SDE)"}}
{"id": "2505.23662", "pdf": "https://arxiv.org/pdf/2505.23662", "abs": "https://arxiv.org/abs/2505.23662", "authors": ["Beong-woo Kwak", "Minju Kim", "Dongha Lim", "Hyungjoo Chae", "Dongjin Kang", "Sunghwan Kim", "Dongil Yang", "Jinyoung Yeo"], "title": "ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic Long-Term Interactions", "categories": ["cs.CL"], "comment": "Our code and data are available at\n  https://github.com/bwookwak/ToolHaystack", "summary": "Large language models (LLMs) have demonstrated strong capabilities in using\nexternal tools to address user inquiries. However, most existing evaluations\nassume tool use in short contexts, offering limited insight into model behavior\nduring realistic long-term interactions. To fill this gap, we introduce\nToolHaystack, a benchmark for testing the tool use capabilities in long-term\ninteractions. Each test instance in ToolHaystack includes multiple tasks\nexecution contexts and realistic noise within a continuous conversation,\nenabling assessment of how well models maintain context and handle various\ndisruptions. By applying this benchmark to 14 state-of-the-art LLMs, we find\nthat while current models perform well in standard multi-turn settings, they\noften significantly struggle in ToolHaystack, highlighting critical gaps in\ntheir long-term robustness not revealed by previous tool benchmarks.", "AI": {"tldr": "ToolHaystack \u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u5de5\u5177\u4f7f\u7528\u7684\u5c40\u9650\u6027\uff0c\u5c3d\u7ba1\u5b83\u4eec\u5728\u6807\u51c6\u591a\u8f6e\u8bbe\u7f6e\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u5927\u591a\u5047\u8bbe\u5de5\u5177\u5728\u77ed\u4e0a\u4e0b\u6587\u4e2d\u7684\u4f7f\u7528\uff0c\u65e0\u6cd5\u53cd\u6620\u6a21\u578b\u5728\u957f\u671f\u4e92\u52a8\u4e2d\u7684\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u5f15\u5165 ToolHaystack \u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5305\u542b\u591a\u4efb\u52a1\u6267\u884c\u4e0a\u4e0b\u6587\u548c\u771f\u5b9e\u566a\u58f0\u7684\u8fde\u7eed\u5bf9\u8bdd\uff0c\u8bc4\u4f30\u6a21\u578b\u7684\u60c5\u5883\u4fdd\u6301\u548c\u5e72\u6270\u5904\u7406\u80fd\u529b\u3002", "result": "\u6d4b\u8bd5 14 \u79cd\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u663e\u793a\uff0c\u867d\u7136\u5b83\u4eec\u5728\u6807\u51c6\u591a\u8f6e\u8bbe\u7f6e\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728 ToolHaystack \u4e2d\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u7a33\u5065\u6027\u5b58\u5728\u5173\u952e\u7f3a\u9677\uff0c\u4f20\u7edf\u5de5\u5177\u57fa\u51c6\u672a\u80fd\u63ed\u793a\u8fd9\u4e00\u70b9\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u5de5\u5177\u4f7f\u7528,\u957f\u671f\u4ea4\u4e92,\u57fa\u51c6\u6d4b\u8bd5,\u7a33\u5065\u6027"}}
{"id": "2505.23378", "pdf": "https://arxiv.org/pdf/2505.23378", "abs": "https://arxiv.org/abs/2505.23378", "authors": ["Roseline Polle", "Agnes Norbury", "Alexandra Livia Georgescu", "Nicholas Cummins", "Stefano Goria"], "title": "Meta-Learning Approaches for Speaker-Dependent Voice Fatigue Models", "categories": ["cs.LG"], "comment": "5 pages, 3 figures. To appear at Interspeech 2025", "summary": "Speaker-dependent modelling can substantially improve performance in\nspeech-based health monitoring applications. While mixed-effect models are\ncommonly used for such speaker adaptation, they require computationally\nexpensive retraining for each new observation, making them impractical in a\nproduction environment. We reformulate this task as a meta-learning problem and\nexplore three approaches of increasing complexity: ensemble-based distance\nmodels, prototypical networks, and transformer-based sequence models. Using\npre-trained speech embeddings, we evaluate these methods on a large\nlongitudinal dataset of shift workers (N=1,185, 10,286 recordings), predicting\ntime since sleep from speech as a function of fatigue, a symptom commonly\nassociated with ill-health. Our results demonstrate that all meta-learning\napproaches tested outperformed both cross-sectional and conventional\nmixed-effects models, with a transformer-based method achieving the strongest\nperformance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u5143\u5b66\u4e60\u65b9\u6cd5\u6539\u8fdb\u8bed\u97f3\u5065\u5eb7\u76d1\u6d4b\u4e2d\u7684\u8bf4\u8bdd\u4eba\u4f9d\u8d56\u5efa\u6a21\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u65b9\u6cd5\u5e76\u9a8c\u8bc1\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u8bf4\u8bdd\u4eba\u4f9d\u8d56\u5efa\u6a21\u53ef\u4ee5\u63d0\u5347\u8bed\u97f3\u5065\u5eb7\u76d1\u6d4b\u7684\u6027\u80fd\uff0c\u4f46\u4f20\u7edf\u6df7\u5408\u6548\u5e94\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5c06\u4efb\u52a1\u91cd\u65b0\u5efa\u6a21\u4e3a\u5143\u5b66\u4e60\u95ee\u9898\uff0c\u6d4b\u8bd5\u4e86\u96c6\u6210\u8ddd\u79bb\u6a21\u578b\u3001\u539f\u578b\u7f51\u7edc\u548c\u57fa\u4e8eTransformer\u7684\u5e8f\u5217\u6a21\u578b\u3002", "result": "\u5728\u9884\u6d4b\u75b2\u52b3\u76f8\u5173\u7761\u7720\u65f6\u95f4\u7684\u4efb\u52a1\u4e2d\uff0c\u6240\u6709\u5143\u5b66\u4e60\u65b9\u6cd5\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0cTransformer\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u5143\u5b66\u4e60\u65b9\u6cd5\u5728\u8bed\u97f3\u5065\u5eb7\u76d1\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662fTransformer\u6a21\u578b\u3002", "keywords": "\u8bed\u97f3\u5065\u5eb7\u76d1\u6d4b\u3001\u5143\u5b66\u4e60\u3001\u6df7\u5408\u6548\u5e94\u6a21\u578b\u3001Transformer\u3001\u8bf4\u8bdd\u4eba\u4f9d\u8d56\u5efa\u6a21"}}
{"id": "2505.23666", "pdf": "https://arxiv.org/pdf/2505.23666", "abs": "https://arxiv.org/abs/2505.23666", "authors": ["Luke McDermott", "Robert W. Heath Jr.", "Rahul Parhi"], "title": "LoLA: Low-Rank Linear Attention With Sparse Caching", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Transformer-based large language models suffer from quadratic complexity at\ninference on long sequences. Linear attention methods are efficient\nalternatives, however, they fail to provide an accurate approximation of\nsoftmax attention. By additionally incorporating sliding window attention into\neach linear attention head, this gap can be closed for short context-length\ntasks. Unfortunately, these approaches cannot recall important information from\nlong contexts due to \"memory collisions\". In this paper , we propose LoLA:\nLow-rank Linear Attention with sparse caching. LoLA separately stores\nadditional key-value pairs that would otherwise interfere with past associative\nmemories. Moreover, LoLA further closes the gap between linear attention models\nand transformers by distributing past key-value pairs into three forms of\nmemory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. As an inference-only strategy, LoLA enables\npass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks\nfrom RULER. It boosts the accuracy of the base subquadratic model from 0.6% to\n97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1\n8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning\ntasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an\nextremely lightweight approach: Nearly all of our results can be reproduced on\na single consumer GPU.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLoLA\uff0c\u4e00\u79cd\u4f4e\u79e9\u7ebf\u6027\u6ce8\u610f\u529b\u4e0e\u7a00\u758f\u7f13\u5b58\u7684\u7ed3\u5408\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u4e0b\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u8bb0\u5fc6\u51b2\u7a81\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfTransformer\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u56e0\u4e8c\u6b21\u590d\u6742\u5ea6\u5728\u957f\u5e8f\u5217\u63a8\u7406\u4e0a\u6548\u7387\u4f4e\u4e0b\uff0c\u7ebf\u6027\u6ce8\u610f\u529b\u867d\u9ad8\u6548\u4f46\u8fd1\u4f3csoftmax\u6ce8\u610f\u529b\u4e0d\u51c6\u786e\uff1b\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u867d\u80fd\u5f25\u8865\u77ed\u4e0a\u4e0b\u6587\u4efb\u52a1\uff0c\u4f46\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u4ecd\u5b58\u5728\u201c\u8bb0\u5fc6\u51b2\u7a81\u201d\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u3002", "method": "\u63d0\u51faLoLA\u65b9\u6cd5\uff0c\u5c06\u8fc7\u53bb\u7684\u952e\u503c\u5bf9\u5206\u6563\u5b58\u50a8\u5230\u4e09\u79cd\u5f62\u5f0f\u7684\u5185\u5b58\u4e2d\uff1a(i)\u5c40\u90e8\u6ed1\u52a8\u7a97\u53e3\u4e2d\u7684\u8fd1\u671f\u952e\u503c\u5bf9\uff1b(ii)\u7a00\u758f\u5168\u5c40\u7f13\u5b58\u4e2d\u7684\u96be\u8bb0\u5fc6\u952e\u503c\u5bf9\uff1b(iii)\u7ebf\u6027\u6ce8\u610f\u529b\u5faa\u73af\u9690\u85cf\u72b6\u6001\u4e2d\u7684\u901a\u7528\u952e\u503c\u5bf9\u3002", "result": "LoLA\u5728RULER\u76848K\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u201c\u5927\u6d77\u635e\u9488\u201d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8697.4%\u7684\u51c6\u786e\u7387\uff08\u4ece0.6%\u63d0\u5347\uff09\uff0c\u7f13\u5b58\u6bd4Llama-3.1 8B\u5c0f4.6\u500d\uff0c\u5e76\u57281B\u548c8B\u53c2\u6570\u7684\u6b21\u4e8c\u6b21\u6a21\u578b\u4e2d\u5c55\u73b0\u51fa\u8272\u7684\u96f6\u6837\u672c\u5e38\u8bc6\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "LoLA\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u5f25\u8865\u7ebf\u6027\u6ce8\u610f\u529b\u4e0eTransformer\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4e14\u53ef\u5728\u5355\u5f20\u6d88\u8d39\u7ea7GPU\u4e0a\u590d\u73b0\u7ed3\u679c\u3002", "keywords": "LoLA, \u4f4e\u79e9\u7ebf\u6027\u6ce8\u610f\u529b, \u7a00\u758f\u7f13\u5b58, \u957f\u5e8f\u5217\u63a8\u7406, \u8bb0\u5fc6\u51b2\u7a81"}}
{"id": "2505.23066", "pdf": "https://arxiv.org/pdf/2505.23066", "abs": "https://arxiv.org/abs/2505.23066", "authors": ["Shuyin Xia", "Xiaojiang Tian", "Suzhen Yuan", "Jeremiah D. Deng"], "title": "Efficient Quantum Approximate $k$NN Algorithm via Granular-Ball Computing", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": "8 pages; 7 figure; accepted by IJCAI 2025", "summary": "High time complexity is one of the biggest challenges faced by $k$-Nearest\nNeighbors ($k$NN). Although current classical and quantum $k$NN algorithms have\nmade some improvements, they still have a speed bottleneck when facing large\namounts of data. To address this issue, we propose an innovative algorithm\ncalled Granular-Ball based Quantum $k$NN(GB-Q$k$NN). This approach achieves\nhigher efficiency by first employing granular-balls, which reduces the data\nsize needed to processed. The search process is then accelerated by adopting a\nHierarchical Navigable Small World (HNSW) method. Moreover, we optimize the\ntime-consuming steps, such as distance calculation, of the HNSW via\nquantization, further reducing the time complexity of the construct and search\nprocess. By combining the use of granular-balls and quantization of the HNSW\nmethod, our approach manages to take advantage of these treatments and\nsignificantly reduces the time complexity of the $k$NN-like algorithms, as\nrevealed by a comprehensive complexity analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGB-QkNN\u7684\u521b\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7c92\u5ea6\u7403\u548cHNSW\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86kNN\u7b97\u6cd5\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "motivation": "\u89e3\u51b3k\u8fd1\u90bb\u7b97\u6cd5\u5728\u9762\u5bf9\u5927\u91cf\u6570\u636e\u65f6\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u5347\u7b97\u6cd5\u6548\u7387\u3002", "method": "\u4f7f\u7528\u7c92\u5ea6\u7403\u9884\u5904\u7406\u51cf\u5c11\u6570\u636e\u89c4\u6a21\uff0c\u91c7\u7528HNSW\u65b9\u6cd5\u52a0\u901f\u641c\u7d22\uff0c\u5e76\u901a\u8fc7\u91cf\u5b50\u5316\u4f18\u5316\u8ddd\u79bb\u8ba1\u7b97\u7b49\u8017\u65f6\u6b65\u9aa4\u3002", "result": "\u7b97\u6cd5\u663e\u8457\u964d\u4f4e\u4e86kNN\u7c7b\u7b97\u6cd5\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "conclusion": "GB-QkNN\u901a\u8fc7\u7c92\u5ea6\u7403\u548c\u91cf\u5b50\u5316HNSW\u65b9\u6cd5\u7684\u7ed3\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86kNN\u7b97\u6cd5\u7684\u6548\u7387\u3002", "keywords": "k-Nearest Neighbors, \u91cf\u5b50\u7b97\u6cd5, \u7c92\u5ea6\u7403, HNSW, \u65f6\u95f4\u590d\u6742\u5ea6"}}
{"id": "2505.23383", "pdf": "https://arxiv.org/pdf/2505.23383", "abs": "https://arxiv.org/abs/2505.23383", "authors": ["Ahmad Anaqreh", "Shih-Kai Chou", "Mihael Mohor\u010di\u010d", "Carolina Fortuna"], "title": "Automated Modeling Method for Pathloss Model Discovery", "categories": ["cs.LG"], "comment": null, "summary": "Modeling propagation is the cornerstone for designing and optimizing\nnext-generation wireless systems, with a particular emphasis on 5G and beyond\nera. Traditional modeling methods have long relied on statistic-based\ntechniques to characterize propagation behavior across different environments.\nWith the expansion of wireless communication systems, there is a growing demand\nfor methods that guarantee the accuracy and interoperability of modeling.\nArtificial intelligence (AI)-based techniques, in particular, are increasingly\nbeing adopted to overcome this challenge, although the interpretability is not\nassured with most of these methods. Inspired by recent advancements in AI, this\npaper proposes a novel approach that accelerates the discovery of path loss\nmodels while maintaining interpretability. The proposed method automates the\nmodel formulation, evaluation, and refinement, facilitating model discovery. We\nevaluate two techniques: one based on Deep Symbolic Regression, offering full\ninterpretability, and the second based on Kolmogorov-Arnold Networks, providing\ntwo levels of interpretability. Both approaches are evaluated on two synthetic\nand two real-world datasets. Our results show that Kolmogorov-Arnold Networks\nachieve R^2 values close to 1 with minimal prediction error, while Deep\nSymbolic Regression generates compact models with moderate accuracy. Moreover,\non the selected examples, we demonstrate that automated methods outperform\ntraditional methods, achieving up to 75% reduction in prediction errors,\noffering accurate and explainable solutions with potential to increase the\nefficiency of discovering next-generation path loss models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u65b0\u578b\u8def\u5f84\u635f\u8017\u5efa\u6a21\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u6df1\u5ea6\u7b26\u53f7\u56de\u5f52\u548cKolmogorov-Arnold\u7f51\u7edc\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u5efa\u6a21\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\u57285G\u53ca\u672a\u6765\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u96be\u4ee5\u517c\u987e\u5efa\u6a21\u7cbe\u5ea6\u4e0e\u4e92\u64cd\u4f5c\u6027\uff0c\u800c\u73b0\u6709AI\u65b9\u6cd5\u53c8\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u7b26\u53f7\u56de\u5f52\uff08\u5b8c\u5168\u53ef\u89e3\u91ca\uff09\u548cKolmogorov-Arnold\u7f51\u7edc\uff08\u4e24\u7ea7\u53ef\u89e3\u91ca\uff09\u81ea\u52a8\u5316\u6a21\u578b\u6784\u5efa\u4e0e\u4f18\u5316\uff0c\u5e76\u5728\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u3002", "result": "Kolmogorov-Arnold\u7f51\u7edc\u7684R\u00b2\u63a5\u8fd11\u4e14\u8bef\u5dee\u6781\u5c0f\uff1b\u6df1\u5ea6\u7b26\u53f7\u56de\u5f52\u6a21\u578b\u7b80\u6d01\u4f46\u7cbe\u5ea6\u4e2d\u7b49\u3002\u81ea\u52a8\u5316\u65b9\u6cd5\u8f83\u4f20\u7edf\u65b9\u6cd5\u51cf\u5c1175%\u9884\u6d4b\u8bef\u5dee\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u9ad8\u6548\u53d1\u73b0\u53ef\u89e3\u91ca\u7684\u4e0b\u4e00\u4ee3\u8def\u5f84\u635f\u8017\u6a21\u578b\uff0c\u517c\u5177\u51c6\u786e\u6027\u4e0e\u5b9e\u7528\u6027\u3002", "keywords": "\u8def\u5f84\u635f\u8017\u5efa\u6a21, \u4eba\u5de5\u667a\u80fd, \u53ef\u89e3\u91ca\u6027, \u6df1\u5ea6\u7b26\u53f7\u56de\u5f52, Kolmogorov-Arnold\u7f51\u7edc"}}
{"id": "2505.23688", "pdf": "https://arxiv.org/pdf/2505.23688", "abs": "https://arxiv.org/abs/2505.23688", "authors": ["James Tanner", "Morgan Sonderegger", "Jane Stuart-Smith", "Jeff Mielke", "Tyler Kendall"], "title": "Automatic classification of stop realisation with wav2vec2.0", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted for Interspeech 2025. 5 pages, 3 figures", "summary": "Modern phonetic research regularly makes use of automatic tools for the\nannotation of speech data, however few tools exist for the annotation of many\nvariable phonetic phenomena. At the same time, pre-trained self-supervised\nmodels, such as wav2vec2.0, have been shown to perform well at speech\nclassification tasks and latently encode fine-grained phonetic information. We\ndemonstrate that wav2vec2.0 models can be trained to automatically classify\nstop burst presence with high accuracy in both English and Japanese, robust\nacross both finely-curated and unprepared speech corpora. Patterns of\nvariability in stop realisation are replicated with the automatic annotations,\nand closely follow those of manual annotations. These results demonstrate the\npotential of pre-trained speech models as tools for the automatic annotation\nand processing of speech corpus data, enabling researchers to `scale-up' the\nscope of phonetic research with relative ease.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u9884\u8bad\u7ec3\u7684wav2vec2.0\u6a21\u578b\u9ad8\u51c6\u786e\u5ea6\u81ea\u52a8\u5206\u7c7b\u82f1\u8bed\u548c\u65e5\u8bed\u4e2d\u7684\u7206\u7834\u97f3\u5b58\u5728\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8bed\u97f3\u6570\u636e\u6807\u6ce8\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u7814\u7a76\u4e2d\u7f3a\u4e4f\u5bf9\u591a\u53d8\u8bed\u97f3\u73b0\u8c61\u7684\u81ea\u52a8\u6807\u6ce8\u5de5\u5177\uff0c\u800c\u9884\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u6a21\u578b\uff08\u5982wav2vec2.0\uff09\u5728\u8bed\u97f3\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u7f16\u7801\u7ec6\u7c92\u5ea6\u7684\u8bed\u97f3\u4fe1\u606f\u3002", "method": "\u4f7f\u7528wav2vec2.0\u6a21\u578b\u8bad\u7ec3\u81ea\u52a8\u5206\u7c7b\u7206\u7834\u97f3\u5b58\u5728\uff0c\u6d4b\u8bd5\u5176\u5728\u82f1\u8bed\u548c\u65e5\u8bed\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5bf9\u6bd4\u624b\u5de5\u6807\u6ce8\u7ed3\u679c\u3002", "result": "\u6a21\u578b\u5728\u7cbe\u5fc3\u51c6\u5907\u548c\u672a\u7ecf\u51c6\u5907\u7684\u8bed\u97f3\u5e93\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u81ea\u52a8\u6807\u6ce8\u7ed3\u679c\u4e0e\u4eba\u5de5\u6807\u6ce8\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u91cd\u590d\u4e86\u7206\u7834\u97f3\u5b9e\u73b0\u7684\u53d8\u5f02\u6027\u6a21\u5f0f\u3002", "conclusion": "\u9884\u8bad\u7ec3\u8bed\u97f3\u6a21\u578b\u5177\u5907\u81ea\u52a8\u6807\u6ce8\u8bed\u97f3\u6570\u636e\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u7b80\u5316\u5e76\u6269\u5927\u8bed\u97f3\u7814\u7a76\u7684\u8303\u56f4\u3002", "keywords": "\u8bed\u97f3\u6807\u6ce8, \u9884\u8bad\u7ec3\u6a21\u578b, wav2vec2.0, \u7206\u7834\u97f3\u5206\u7c7b, \u8bed\u97f3\u7814\u7a76"}}
{"id": "2505.23412", "pdf": "https://arxiv.org/pdf/2505.23412", "abs": "https://arxiv.org/abs/2505.23412", "authors": ["Srishti Gupta", "Daniele Angioni", "Maura Pintor", "Ambra Demontis", "Lea Sch\u00f6nherr", "Battista Biggio", "Fabio Roli"], "title": "Buffer-free Class-Incremental Learning with Out-of-Distribution Detection", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Class-incremental learning (CIL) poses significant challenges in open-world\nscenarios, where models must not only learn new classes over time without\nforgetting previous ones but also handle inputs from unknown classes that a\nclosed-set model would misclassify. Recent works address both issues by\n(i)~training multi-head models using the task-incremental learning framework,\nand (ii) predicting the task identity employing out-of-distribution (OOD)\ndetectors. While effective, the latter mainly relies on joint training with a\nmemory buffer of past data, raising concerns around privacy, scalability, and\nincreased training time. In this paper, we present an in-depth analysis of\npost-hoc OOD detection methods and investigate their potential to eliminate the\nneed for a memory buffer. We uncover that these methods, when applied\nappropriately at inference time, can serve as a strong substitute for\nbuffer-based OOD detection. We show that this buffer-free approach achieves\ncomparable or superior performance to buffer-based methods both in terms of\nclass-incremental learning and the rejection of unknown samples. Experimental\nresults on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets support our findings,\noffering new insights into the design of efficient and privacy-preserving CIL\nsystems for open-world settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6df1\u5165\u5206\u6790\u4e86\u4e8b\u540eOOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8bc1\u660e\u5176\u5728\u65e0\u9700\u5185\u5b58\u7f13\u51b2\u533a\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u6709\u6548\u66ff\u4ee3\u57fa\u4e8e\u7f13\u51b2\u533a\u7684OOD\u68c0\u6d4b\uff0c\u5728\u7c7b\u589e\u91cf\u5b66\u4e60\u548c\u672a\u77e5\u6837\u672c\u62d2\u7edd\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9488\u5bf9\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5185\u5b58\u7f13\u51b2\u533a\u5e26\u6765\u9690\u79c1\u3001\u53ef\u6269\u5c55\u6027\u548c\u8bad\u7ec3\u65f6\u95f4\u589e\u52a0\u7684\u62c5\u5fe7\uff0c\u8bba\u6587\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u4e8b\u540eOOD\u68c0\u6d4b\u65b9\u6cd5\u6d88\u9664\u5bf9\u7f13\u51b2\u533a\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u5e76\u5206\u6790\u4e8b\u540eOOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u65f6\u9002\u5f53\u5e94\u7528\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u5176\u4f5c\u4e3a\u7f13\u51b2\u533a\u66ff\u4ee3\u65b9\u6848\u7684\u6f5c\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u65e0\u7f13\u51b2\u533a\u65b9\u6cd5\u5728CIFAR-10\u3001CIFAR-100\u548cTiny ImageNet\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0e\u57fa\u4e8e\u7f13\u51b2\u533a\u7684\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "\u7814\u7a76\u65b9\u6cd5\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u5f00\u653e\u4e16\u754c\u7c7b\u589e\u91cf\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u7c7b\u589e\u91cf\u5b66\u4e60, \u5f00\u653e\u4e16\u754c, OOD\u68c0\u6d4b, \u9690\u79c1\u4fdd\u62a4, \u65e0\u7f13\u51b2\u533a"}}
{"id": "2505.23689", "pdf": "https://arxiv.org/pdf/2505.23689", "abs": "https://arxiv.org/abs/2505.23689", "authors": ["Francesca Padovani", "Jaap Jumelet", "Yevgen Matusevych", "Arianna Bisazza"], "title": "Child-Directed Language Does Not Consistently Boost Syntax Learning in Language Models", "categories": ["cs.CL"], "comment": "21 pages, 4 figures, 4 tables", "summary": "Seminal work by Huebner et al. (2021) showed that language models (LMs)\ntrained on English Child-Directed Language (CDL) can reach similar syntactic\nabilities as LMs trained on much larger amounts of adult-directed written text,\nsuggesting that CDL could provide more effective LM training material than the\ncommonly used internet-crawled data. However, the generalizability of these\nresults across languages, model types, and evaluation settings remains unclear.\nWe test this by comparing models trained on CDL vs. Wikipedia across two LM\nobjectives (masked and causal), three languages (English, French, German), and\nthree syntactic minimal-pair benchmarks. Our results on these benchmarks show\ninconsistent benefits of CDL, which in most cases is outperformed by Wikipedia\nmodels. We then identify various shortcomings in previous benchmarks, and\nintroduce a novel testing methodology, FIT-CLAMS, which uses a\nfrequency-controlled design to enable balanced comparisons across training\ncorpora. Through minimal pair evaluations and regression analysis we show that\ntraining on CDL does not yield stronger generalizations for acquiring syntax\nand highlight the importance of controlling for frequency effects when\nevaluating syntactic ability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u513f\u7ae5\u5bfc\u5411\u8bed\u8a00\uff08CDL\uff09\u4e0e\u7ef4\u57fa\u767e\u79d1\u6570\u636e\u5728\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u65f6\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u53d1\u73b0\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u7ef4\u57fa\u767e\u79d1\u6a21\u578b\u4f18\u4e8eCDL\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5FIT-CLAMS\u4ee5\u63a7\u5236\u9891\u7387\u6548\u5e94\u3002", "motivation": "\u9a8c\u8bc1CDL\u5728\u591a\u8bed\u8a00\u3001\u6a21\u578b\u7c7b\u578b\u548c\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ee5\u786e\u5b9a\u5176\u662f\u5426\u6bd4\u4e92\u8054\u7f51\u6293\u53d6\u6570\u636e\u66f4\u6709\u6548\u3002", "method": "\u6bd4\u8f83CDL\u548c\u7ef4\u57fa\u767e\u79d1\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u4e24\u79cdLM\u76ee\u6807\uff08\u63a9\u7801\u548c\u56e0\u679c\uff09\u3001\u4e09\u79cd\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u6cd5\u8bed\u3001\u5fb7\u8bed\uff09\u548c\u4e09\u4e2a\u53e5\u6cd5\u6700\u5c0f\u5bf9\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5f15\u5165FIT-CLAMS\u65b9\u6cd5\u3002", "result": "CDL\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u5982\u7ef4\u57fa\u767e\u79d1\u6a21\u578b\uff0c\u4e14\u672a\u663e\u793a\u51fa\u66f4\u5f3a\u7684\u53e5\u6cd5\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8bad\u7ec3\u4e8eCDL\u7684\u8bed\u8a00\u6a21\u578b\u5e76\u672a\u5728\u53e5\u6cd5\u5b66\u4e60\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u9891\u7387\u6548\u5e94\u7684\u63a7\u5236\u5728\u53e5\u6cd5\u80fd\u529b\u8bc4\u4f30\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "keywords": "\u513f\u7ae5\u5bfc\u5411\u8bed\u8a00, \u8bed\u8a00\u6a21\u578b, \u53e5\u6cd5\u5b66\u4e60, \u9891\u7387\u6548\u5e94, FIT-CLAMS"}}
{"id": "2505.23085", "pdf": "https://arxiv.org/pdf/2505.23085", "abs": "https://arxiv.org/abs/2505.23085", "authors": ["Gwanghyun Kim", "Xueting Li", "Ye Yuan", "Koki Nagano", "Tianye Li", "Jan Kautz", "Se Young Chun", "Umar Iqbal"], "title": "GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "Project page: https://research.nvidia.com/labs/dair/geoman", "summary": "Estimating accurate and temporally consistent 3D human geometry from videos\nis a challenging problem in computer vision. Existing methods, primarily\noptimized for single images, often suffer from temporal inconsistencies and\nfail to capture fine-grained dynamic details. To address these limitations, we\npresent GeoMan, a novel architecture designed to produce accurate and\ntemporally consistent depth and normal estimations from monocular human videos.\nGeoMan addresses two key challenges: the scarcity of high-quality 4D training\ndata and the need for metric depth estimation to accurately model human size.\nTo overcome the first challenge, GeoMan employs an image-based model to\nestimate depth and normals for the first frame of a video, which then\nconditions a video diffusion model, reframing video geometry estimation task as\nan image-to-video generation problem. This design offloads the heavy lifting of\ngeometric estimation to the image model and simplifies the video model's role\nto focus on intricate details while using priors learned from large-scale video\ndatasets. Consequently, GeoMan improves temporal consistency and\ngeneralizability while requiring minimal 4D training data. To address the\nchallenge of accurate human size estimation, we introduce a root-relative depth\nrepresentation that retains critical human-scale details and is easier to be\nestimated from monocular inputs, overcoming the limitations of traditional\naffine-invariant and metric depth representations. GeoMan achieves\nstate-of-the-art performance in both qualitative and quantitative evaluations,\ndemonstrating its effectiveness in overcoming longstanding challenges in 3D\nhuman geometry estimation from videos.", "AI": {"tldr": "GeoMan\u662f\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u5c06\u89c6\u9891\u51e0\u4f55\u4f30\u8ba1\u4efb\u52a1\u91cd\u6784\u4e3a\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u56fe\u50cf\u4f18\u5316\u4e2d\u96be\u4ee5\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u6355\u6349\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4ece\u5355\u56fe\u50cf\u4f18\u5316\u65f6\uff0c\u5f80\u5f80\u65e0\u6cd5\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u4e14\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u52a8\u6001\u7ec6\u8282\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u53473D\u4eba\u4f53\u51e0\u4f55\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "method": "GeoMan\u91c7\u7528\u57fa\u4e8e\u56fe\u50cf\u7684\u6a21\u578b\u4f30\u8ba1\u9996\u5e27\u6df1\u5ea6\u548c\u6cd5\u7ebf\uff0c\u968f\u540e\u901a\u8fc7\u89c6\u9891\u6269\u6563\u6a21\u578b\u7ec6\u5316\uff0c\u5e76\u7ed3\u5408\u6839\u76f8\u5bf9\u6df1\u5ea6\u8868\u793a\u4ee5\u4fdd\u7559\u4eba\u4f53\u5c3a\u5bf8\u7ec6\u8282\u3002", "result": "GeoMan\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GeoMan\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u57284D\u6570\u636e\u7a00\u7f3a\u548c\u4eba\u4f53\u5c3a\u5bf8\u4f30\u8ba1\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u89c6\u9891\u4e2d\u76843D\u4eba\u4f53\u51e0\u4f55\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "3D human geometry, video depth estimation, temporal consistency, root-relative depth, monocular video"}}
{"id": "2505.23415", "pdf": "https://arxiv.org/pdf/2505.23415", "abs": "https://arxiv.org/abs/2505.23415", "authors": ["Gaspard Oliviers", "Mufeng Tang", "Rafal Bogacz"], "title": "Bidirectional predictive coding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Predictive coding (PC) is an influential computational model of visual\nlearning and inference in the brain. Classical PC was proposed as a top-down\ngenerative model, where the brain actively predicts upcoming visual inputs, and\ninference minimises the prediction errors. Recent studies have also shown that\nPC can be formulated as a discriminative model, where sensory inputs predict\nneural activities in a feedforward manner. However, experimental evidence\nsuggests that the brain employs both generative and discriminative inference,\nwhile unidirectional PC models show degraded performance in tasks requiring\nbidirectional processing. In this work, we propose bidirectional PC (bPC), a PC\nmodel that incorporates both generative and discriminative inference while\nmaintaining a biologically plausible circuit implementation. We show that bPC\nmatches or outperforms unidirectional models in their specialised generative or\ndiscriminative tasks, by developing an energy landscape that simultaneously\nsuits both tasks. We also demonstrate bPC's superior performance in two\nbiologically relevant tasks including multimodal learning and inference with\nmissing information, suggesting that bPC resembles biological visual inference\nmore closely.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5411\u9884\u6d4b\u7f16\u7801\uff08bPC\uff09\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u751f\u6210\u548c\u5224\u522b\u63a8\u7406\uff0c\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u5411\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u9884\u6d4b\u7f16\u7801\u6a21\u578b\u591a\u4e3a\u5355\u5411\uff08\u751f\u6210\u6216\u5224\u522b\uff09\uff0c\u800c\u5b9e\u9a8c\u8bc1\u636e\u8868\u660e\u5927\u8111\u9700\u53cc\u5411\u5904\u7406\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u66f4\u751f\u7269\u5408\u7406\u7684\u53cc\u5411\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u53cc\u5411\u9884\u6d4b\u7f16\u7801\uff08bPC\uff09\uff0c\u901a\u8fc7\u6784\u5efa\u540c\u65f6\u9002\u5408\u751f\u6210\u548c\u5224\u522b\u4efb\u52a1\u7684\u80fd\u91cf\u666f\u89c2\uff0c\u5b9e\u73b0\u53cc\u5411\u63a8\u7406\u3002", "result": "bPC\u5728\u751f\u6210\u548c\u5224\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u5411\u6a21\u578b\uff0c\u5728\u591a\u6a21\u6001\u5b66\u4e60\u548c\u4fe1\u606f\u7f3a\u5931\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u66f4\u5f3a\u6027\u80fd\u3002", "conclusion": "bPC\u66f4\u63a5\u8fd1\u751f\u7269\u89c6\u89c9\u63a8\u7406\u673a\u5236\uff0c\u4e3a\u7406\u89e3\u5927\u8111\u89c6\u89c9\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u8ba1\u7b97\u6a21\u578b\u3002", "keywords": "\u9884\u6d4b\u7f16\u7801\uff0c\u751f\u6210\u6a21\u578b\uff0c\u5224\u522b\u6a21\u578b\uff0c\u53cc\u5411\u63a8\u7406\uff0c\u89c6\u89c9\u5b66\u4e60"}}
{"id": "2505.23701", "pdf": "https://arxiv.org/pdf/2505.23701", "abs": "https://arxiv.org/abs/2505.23701", "authors": ["Ziling Cheng", "Meng Cao", "Leila Pishdad", "Yanshuai Cao", "Jackie Chi Kit Cheung"], "title": "Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation", "categories": ["cs.CL"], "comment": null, "summary": "Final-answer-based metrics are commonly used for evaluating large language\nmodels (LLMs) on math word problems, often taken as proxies for reasoning\nability. However, such metrics conflate two distinct sub-skills: abstract\nformulation (capturing mathematical relationships using expressions) and\narithmetic computation (executing the calculations). Through a disentangled\nevaluation on GSM8K and SVAMP, we find that the final-answer accuracy of\nLlama-3 and Qwen2.5 (1B-32B) without CoT is overwhelmingly bottlenecked by the\narithmetic computation step and not by the abstract formulation step. Contrary\nto the common belief, we show that CoT primarily aids in computation, with\nlimited impact on abstract formulation. Mechanistically, we show that these two\nskills are composed conjunctively even in a single forward pass without any\nreasoning steps via an abstract-then-compute mechanism: models first capture\nproblem abstractions, then handle computation. Causal patching confirms these\nabstractions are present, transferable, composable, and precede computation.\nThese behavioural and mechanistic findings highlight the need for disentangled\nevaluation to accurately assess LLM reasoning and to guide future improvements.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\uff0c\u57fa\u4e8e\u6700\u7ec8\u7b54\u6848\u7684\u6307\u6807\u5728\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6570\u5b66\u89e3\u9898\u80fd\u529b\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u5176\u6df7\u6dc6\u4e86\u62bd\u8c61\u8868\u8fbe\u548c\u7b97\u672f\u8ba1\u7b97\u4e24\u4e2a\u5b50\u6280\u80fd\u3002\u7814\u7a76\u53d1\u73b0\uff0cLLM\u7684\u74f6\u9888\u4e3b\u8981\u5728\u7b97\u672f\u8ba1\u7b97\u800c\u975e\u62bd\u8c61\u8868\u8fbe\uff0c\u4e14\u601d\u7ef4\u94fe\uff08CoT\uff09\u5bf9\u8ba1\u7b97\u7684\u5e2e\u52a9\u5927\u4e8e\u62bd\u8c61\u8868\u8fbe\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30LLM\u6570\u5b66\u89e3\u9898\u80fd\u529b\u7684\u6307\u6807\uff08\u5982\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\uff09\u65e0\u6cd5\u533a\u5206\u62bd\u8c61\u8868\u8fbe\u4e0e\u7b97\u672f\u8ba1\u7b97\uff0c\u53ef\u80fd\u5bfc\u81f4\u5bf9\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u8bef\u5224\u3002\u7814\u7a76\u65e8\u5728\u5206\u89e3\u8fd9\u4e24\u9879\u5b50\u6280\u80fd\uff0c\u66f4\u7cbe\u51c6\u5730\u8bc4\u4f30LLM\u6027\u80fd\u3002", "method": "\u5728GSM8K\u548cSVAMP\u6570\u636e\u96c6\u4e0a\u5bf9Llama-3\u548cQwen2.5\uff081B-32B\uff09\u8fdb\u884c\u5206\u79bb\u8bc4\u4f30\uff0c\u5bf9\u6bd4\u6709\u65e0\u601d\u7ef4\u94fe\uff08CoT\uff09\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u56e0\u679c\u4fee\u8865\u9a8c\u8bc1\u62bd\u8c61\u8868\u8fbe\u7684\u5b58\u5728\u4e0e\u4f5c\u7528\u673a\u5236\u3002", "result": "\u53d1\u73b0LLM\u7684\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\u4e3b\u8981\u53d7\u7b97\u672f\u8ba1\u7b97\u9650\u5236\uff0c\u800c\u975e\u62bd\u8c61\u8868\u8fbe\uff1bCoT\u5bf9\u8ba1\u7b97\u7684\u63d0\u5347\u663e\u8457\uff0c\u4f46\u5bf9\u62bd\u8c61\u8868\u8fbe\u5f71\u54cd\u6709\u9650\u3002\u6a21\u578b\u901a\u8fc7\u2018\u5148\u62bd\u8c61\u540e\u8ba1\u7b97\u2019\u673a\u5236\u5728\u5355\u6b21\u524d\u5411\u4f20\u9012\u4e2d\u5b8c\u6210\u63a8\u7406\u3002", "conclusion": "\u9700\u89e3\u8026\u8bc4\u4f30\u6307\u6807\u4ee5\u51c6\u786e\u8861\u91cfLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u6307\u5bfc\u672a\u6765\u6539\u8fdb\u3002\u62bd\u8c61\u8868\u8fbe\u4e0e\u8ba1\u7b97\u6280\u80fd\u7684\u53ef\u7ec4\u5408\u6027\u4e0e\u65f6\u5e8f\u6027\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u6570\u5b66\u89e3\u9898, \u62bd\u8c61\u8868\u8fbe, \u7b97\u672f\u8ba1\u7b97, \u601d\u7ef4\u94fe, \u5206\u89e3\u8bc4\u4f30"}}
{"id": "2505.23421", "pdf": "https://arxiv.org/pdf/2505.23421", "abs": "https://arxiv.org/abs/2505.23421", "authors": ["Zheming Zhang", "Yan Jiang", "Qingshan Li", "Ai Han"], "title": "OTPTO: Joint Product Selection and Inventory Optimization in Fresh E-commerce Front-End Warehouses", "categories": ["cs.LG"], "comment": "18 pages, 9 figures", "summary": "In China's competitive fresh e-commerce market, optimizing operational\nstrategies, especially inventory management in front-end warehouses, is key to\nenhance customer satisfaction and to gain a competitive edge. Front-end\nwarehouses are placed in residential areas to ensure the timely delivery of\nfresh goods and are usually in small size. This brings the challenge of\ndeciding which goods to stock and in what quantities, taking into account\ncapacity constraints. To address this issue, traditional predict-then-optimize\n(PTO) methods that predict sales and then decide on inventory often don't align\nprediction with inventory goals, as well as fail to prioritize consumer\nsatisfaction. This paper proposes a multi-task\nOptimize-then-Predict-then-Optimize (OTPTO) approach that jointly optimizes\nproduct selection and inventory management, aiming to increase consumer\nsatisfaction by maximizing the full order fulfillment rate. Our method employs\na 0-1 mixed integer programming model OM1 to determine historically optimal\ninventory levels, and then uses a product selection model PM1 and the stocking\nmodel PM2 for prediction. The combined results are further refined through a\npost-processing algorithm OM2. Experimental results from JD.com's 7Fresh\nplatform demonstrate the robustness and significant advantages of our OTPTO\nmethod. Compared to the PTO approach, our OTPTO method substantially enhances\nthe full order fulfillment rate by 4.34% (a relative increase of 7.05%) and\nnarrows the gap to the optimal full order fulfillment rate by 5.27%. These\nfindings substantiate the efficacy of the OTPTO method in managing inventory at\nfront-end warehouses of fresh e-commerce platforms and provide valuable\ninsights for future research in this domain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u4f18\u5316-\u9884\u6d4b-\u518d\u4f18\u5316\uff08OTPTO\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e2d\u56fd\u751f\u9c9c\u7535\u5546\u524d\u7aef\u4ed3\u5e93\u7684\u5e93\u5b58\u7ba1\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba2\u5355\u6ee1\u8db3\u7387\u3002", "motivation": "\u5728\u7ade\u4e89\u6fc0\u70c8\u7684\u751f\u9c9c\u7535\u5546\u5e02\u573a\u4e2d\uff0c\u524d\u7aef\u4ed3\u5e93\u7684\u5e93\u5b58\u7ba1\u7406\u662f\u5173\u952e\u6311\u6218\u3002\u4f20\u7edf\u9884\u6d4b\u518d\u4f18\u5316\uff08PTO\uff09\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u7ed3\u5408\u9884\u6d4b\u4e0e\u5e93\u5b58\u76ee\u6807\uff0c\u4e14\u5ffd\u89c6\u4e86\u6d88\u8d39\u8005\u6ee1\u610f\u5ea6\u3002", "method": "\u91c7\u7528OTPTO\u65b9\u6cd5\uff0c\u7ed3\u54080-1\u6df7\u5408\u6574\u6570\u89c4\u5212\u6a21\u578b\uff08OM1\uff09\u786e\u5b9a\u5386\u53f2\u6700\u4f18\u5e93\u5b58\uff0c\u518d\u901a\u8fc7\u4ea7\u54c1\u9009\u62e9\u6a21\u578b\uff08PM1\uff09\u548c\u5e93\u5b58\u6a21\u578b\uff08PM2\uff09\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u540e\u5904\u7406\u7b97\u6cd5\uff08OM2\uff09\u4f18\u5316\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOTPTO\u65b9\u6cd5\u5c06\u8ba2\u5355\u6ee1\u8db3\u7387\u63d0\u9ad8\u4e864.34%\uff08\u76f8\u5bf9\u63d0\u53477.05%\uff09\uff0c\u5e76\u5c06\u4e0e\u6700\u4f18\u8ba2\u5355\u6ee1\u8db3\u7387\u7684\u5dee\u8ddd\u7f29\u5c0f\u4e865.27%\u3002", "conclusion": "OTPTO\u65b9\u6cd5\u5728\u751f\u9c9c\u7535\u5546\u524d\u7aef\u4ed3\u5e93\u7ba1\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "keywords": "\u751f\u9c9c\u7535\u5546, \u524d\u7aef\u4ed3\u5e93, \u5e93\u5b58\u7ba1\u7406, OTPTO, \u8ba2\u5355\u6ee1\u8db3\u7387"}}
{"id": "2505.23713", "pdf": "https://arxiv.org/pdf/2505.23713", "abs": "https://arxiv.org/abs/2505.23713", "authors": ["Zixiang Xu", "Yanbo Wang", "Yue Huang", "Jiayi Ye", "Haomin Zhuang", "Zirui Song", "Lang Gao", "Chenxi Wang", "Zhaorun Chen", "Yujun Zhou", "Sixian Li", "Wang Pan", "Yue Zhao", "Jieyu Zhao", "Xiangliang Zhang", "Xiuying Chen"], "title": "SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": "Code available at https://github.com/xzx34/SocialMaze", "summary": "Large language models (LLMs) are increasingly applied to socially grounded\ntasks, such as online community moderation, media content analysis, and social\nreasoning games. Success in these contexts depends on a model's social\nreasoning ability - the capacity to interpret social contexts, infer others'\nmental states, and assess the truthfulness of presented information. However,\nthere is currently no systematic evaluation framework that comprehensively\nassesses the social reasoning capabilities of LLMs. Existing efforts often\noversimplify real-world scenarios and consist of tasks that are too basic to\nchallenge advanced models. To address this gap, we introduce SocialMaze, a new\nbenchmark specifically designed to evaluate social reasoning. SocialMaze\nsystematically incorporates three core challenges: deep reasoning, dynamic\ninteraction, and information uncertainty. It provides six diverse tasks across\nthree key settings: social reasoning games, daily-life interactions, and\ndigital community platforms. Both automated and human validation are used to\nensure data quality. Our evaluation reveals several key insights: models vary\nsubstantially in their ability to handle dynamic interactions and integrate\ntemporally evolving information; models with strong chain-of-thought reasoning\nperform better on tasks requiring deeper inference beyond surface-level cues;\nand model reasoning degrades significantly under uncertainty. Furthermore, we\nshow that targeted fine-tuning on curated reasoning examples can greatly\nimprove model performance in complex social scenarios. The dataset is publicly\navailable at: https://huggingface.co/datasets/MBZUAI/SocialMaze", "AI": {"tldr": "SocialMaze\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u5168\u9762\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u793e\u4f1a\u63a8\u7406\u80fd\u529b\uff0c\u5f25\u8865\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u8fc7\u4e8e\u7b80\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u65e0\u6cd5\u5168\u9762\u6d4b\u8bd5LLMs\u5728\u590d\u6742\u793e\u4f1a\u573a\u666f\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0cSocialMaze\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "SocialMaze\u901a\u8fc7\u516d\u9879\u4efb\u52a1\u8bc4\u4f30\u4e09\u4e2a\u6838\u5fc3\u6311\u6218\uff1a\u6df1\u5ea6\u63a8\u7406\u3001\u52a8\u6001\u4e92\u52a8\u548c\u4fe1\u606f\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u7ed3\u5408\u81ea\u52a8\u5316\u548c\u4eba\u5de5\u9a8c\u8bc1\u6570\u636e\u8d28\u91cf\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\uff1a\u6a21\u578b\u5728\u5904\u7406\u52a8\u6001\u4e92\u52a8\u548c\u6574\u5408\u65f6\u95f4\u6f14\u5316\u4fe1\u606f\u65f6\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff1b\u5177\u5907\u94fe\u5f0f\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u5728\u6df1\u5c42\u63a8\u7406\u4efb\u52a1\u4e2d\u66f4\u4f18\uff1b\u4e0d\u786e\u5b9a\u6027\u4e0b\u6a21\u578b\u63a8\u7406\u80fd\u529b\u663e\u8457\u4e0b\u964d\uff1b\u9488\u5bf9\u6027\u5fae\u8c03\u53ef\u63d0\u5347\u8868\u73b0\u3002", "conclusion": "SocialMaze\u4e3aLLMs\u7684\u793e\u4f1a\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u590d\u6742\u793e\u4f1a\u573a\u666f\u4e2d\u7684\u8868\u73b0\u53ca\u6539\u8fdb\u6f5c\u529b\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u793e\u4f1a\u63a8\u7406, SocialMaze, \u52a8\u6001\u4e92\u52a8, \u4fe1\u606f\u4e0d\u786e\u5b9a\u6027"}}
{"id": "2505.23107", "pdf": "https://arxiv.org/pdf/2505.23107", "abs": "https://arxiv.org/abs/2505.23107", "authors": ["Pushapdeep Singh", "Jyoti Nigam", "Medicherla Vamsi Krishna", "Arnav Bhavsar", "Aditya Nigam"], "title": "EAD: An EEG Adapter for Automated Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "While electroencephalography (EEG) has been a popular modality for neural\ndecoding, it often involves task specific acquisition of the EEG data. This\nposes challenges for the development of a unified pipeline to learn embeddings\nfor various EEG signal classification, which is often involved in various\ndecoding tasks. Traditionally, EEG classification involves the step of signal\npreprocessing and the use of deep learning techniques, which are highly\ndependent on the number of EEG channels in each sample. However, the same\npipeline cannot be applied even if the EEG data is collected for the same\nexperiment but with different acquisition devices. This necessitates the\ndevelopment of a framework for learning EEG embeddings, which could be highly\nbeneficial for tasks involving multiple EEG samples for the same task but with\nvarying numbers of EEG channels. In this work, we propose EEG Adapter (EAD), a\nflexible framework compatible with any signal acquisition device. More\nspecifically, we leverage a recent EEG foundational model with significant\nadaptations to learn robust representations from the EEG data for the\nclassification task. We evaluate EAD on two publicly available datasets\nachieving state-of-the-art accuracies 99.33% and 92.31% on EEG-ImageNet and\nBrainLat respectively. This illustrates the effectiveness of the proposed\nframework across diverse EEG datasets containing two different perception\ntasks: stimulus and resting-state EEG signals. We also perform zero-shot EEG\nclassification on EEG-ImageNet task to demonstrate the generalization\ncapability of the proposed approach.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86EEG Adapter\uff08EAD\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u9002\u5e94\u4e0d\u540c\u8bbe\u5907\u91c7\u96c6\u7684EEG\u4fe1\u53f7\u7684\u7edf\u4e00\u5d4c\u5165\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u56e0\u901a\u9053\u6570\u4e0d\u540c\u5bfc\u81f4\u7684\u5206\u7c7b\u96be\u9898\u3002", "motivation": "EEG\u4fe1\u53f7\u5206\u7c7b\u901a\u5e38\u4f9d\u8d56\u7279\u5b9a\u4efb\u52a1\u7684\u6570\u636e\u91c7\u96c6\u548c\u901a\u9053\u6570\uff0c\u5bfc\u81f4\u4e0d\u540c\u8bbe\u5907\u7684\u6570\u636e\u65e0\u6cd5\u7edf\u4e00\u5904\u7406\uff1b\u9700\u5f00\u53d1\u517c\u5bb9\u591a\u8bbe\u5907\u7684\u5d4c\u5165\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u57fa\u4e8eEEG\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u9002\u914d\u6539\u9020\uff0c\u63d0\u51faEAD\u6846\u67b6\uff0c\u5b66\u4e60\u9c81\u68d2\u7684EEG\u8868\u5f81\uff0c\u652f\u6301\u4e0d\u540c\u901a\u9053\u6570\u7684\u6570\u636e\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u5728EEG-ImageNet\u548cBrainLat\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523099.33%\u548c92.31%\u7684\u51c6\u786e\u7387\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u523a\u6fc0\u548c\u9759\u606f\u6001EEG\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u53ca\u96f6\u6837\u672c\u5206\u7c7b\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "EAD\u6846\u67b6\u4e3a\u591a\u8bbe\u5907EEG\u6570\u636e\u5206\u7c7b\u63d0\u4f9b\u4e86\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u901a\u7528\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002", "keywords": "EEG\u89e3\u7801\u3001\u5d4c\u5165\u5b66\u4e60\u3001\u591a\u8bbe\u5907\u517c\u5bb9\u3001\u96f6\u6837\u672c\u5206\u7c7b\u3001\u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2505.23426", "pdf": "https://arxiv.org/pdf/2505.23426", "abs": "https://arxiv.org/abs/2505.23426", "authors": ["Yinuo Wang", "Mining Tan", "Wenjun Zou", "Haotian Lin", "Xujie Song", "Wenxuan Wang", "Tong Liu", "Likun Wang", "Guojian Zhan", "Tianze Zhu", "Shiqi Liu", "Jingliang Duan", "Shengbo Eben Li"], "title": "Enhanced DACER Algorithm with High Diffusion Efficiency", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Due to their expressive capacity, diffusion models have shown great promise\nin offline RL and imitation learning. Diffusion Actor-Critic with Entropy\nRegulator (DACER) extended this capability to online RL by using the reverse\ndiffusion process as a policy approximator, trained end-to-end with policy\ngradient methods, achieving strong performance. However, this comes at the cost\nof requiring many diffusion steps, which significantly hampers training\nefficiency, while directly reducing the steps leads to noticeable performance\ndegradation. Critically, the lack of inference efficiency becomes a significant\nbottleneck for applying diffusion policies in real-time online RL settings. To\nimprove training and inference efficiency while maintaining or even enhancing\nperformance, we propose a Q-gradient field objective as an auxiliary\noptimization target to guide the denoising process at each diffusion step.\nNonetheless, we observe that the independence of the Q-gradient field from the\ndiffusion time step negatively impacts the performance of the diffusion policy.\nTo address this, we introduce a temporal weighting mechanism that enables the\nmodel to efficiently eliminate large-scale noise in the early stages and refine\nactions in the later stages. Experimental results on MuJoCo benchmarks and\nseveral multimodal tasks demonstrate that the DACER2 algorithm achieves\nstate-of-the-art performance in most MuJoCo control tasks with only five\ndiffusion steps, while also exhibiting stronger multimodality compared to\nDACER.", "AI": {"tldr": "DACER2\u901a\u8fc7\u5f15\u5165Q\u68af\u5ea6\u573a\u76ee\u6807\u548c\u65f6\u95f4\u52a0\u6743\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u5728\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6216\u589e\u5f3a\u4e86\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5728\u7ebfRL\u4e2d\u56e0\u9700\u8981\u5927\u91cf\u6269\u6563\u6b65\u9aa4\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\uff0c\u76f4\u63a5\u51cf\u5c11\u6b65\u9aa4\u53c8\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faQ\u68af\u5ea6\u573a\u76ee\u6807\u4f5c\u4e3a\u8f85\u52a9\u4f18\u5316\u76ee\u6807\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u52a0\u6743\u673a\u5236\u6765\u6307\u5bfc\u6bcf\u4e2a\u6269\u6563\u6b65\u9aa4\u7684\u53bb\u566a\u8fc7\u7a0b\u3002", "result": "\u5728MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\uff0cDACER2\u4ec5\u9700\u4e94\u4e2a\u6269\u6563\u6b65\u9aa4\u5373\u53ef\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u591a\u6a21\u6001\u80fd\u529b\u3002", "conclusion": "DACER2\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u7b56\u7565\u7684\u6548\u7387\u4e0e\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5728\u7ebfRL\u573a\u666f\u3002", "keywords": "\u6269\u6563\u6a21\u578b, \u5728\u7ebf\u5f3a\u5316\u5b66\u4e60, DACER2, Q\u68af\u5ea6\u573a, \u65f6\u95f4\u52a0\u6743\u673a\u5236"}}
{"id": "2505.23714", "pdf": "https://arxiv.org/pdf/2505.23714", "abs": "https://arxiv.org/abs/2505.23714", "authors": ["Roksana Goworek", "Harpal Karlcut", "Muhammad Shezad", "Nijaguna Darshana", "Abhishek Mane", "Syam Bondada", "Raghav Sikka", "Ulvi Mammadov", "Rauf Allahverdiyev", "Sriram Purighella", "Paridhi Gupta", "Muhinyia Ndegwa", "Haim Dubossarsky"], "title": "SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 22 figures, submitted to SIGTYP 2025 workshop in ACL", "summary": "This paper addresses the critical need for high-quality evaluation datasets\nin low-resource languages to advance cross-lingual transfer. While\ncross-lingual transfer offers a key strategy for leveraging multilingual\npretraining to expand language technologies to understudied and typologically\ndiverse languages, its effectiveness is dependent on quality and suitable\nbenchmarks. We release new sense-annotated datasets of sentences containing\npolysemous words, spanning nine low-resource languages across diverse language\nfamilies and scripts. To facilitate dataset creation, the paper presents a\ndemonstrably beneficial semi-automatic annotation method. The utility of the\ndatasets is demonstrated through Word-in-Context (WiC) formatted experiments\nthat evaluate transfer on these low-resource languages. Results highlight the\nimportance of targeted dataset creation and evaluation for effective polysemy\ndisambiguation in low-resource settings and transfer studies. The released\ndatasets and code aim to support further research into fair, robust, and truly\nmultilingual NLP.", "AI": {"tldr": "\u672c\u6587\u89e3\u51b3\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u9ad8\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u96c6\u7684\u8feb\u5207\u9700\u6c42\uff0c\u63d0\u51fa\u4e86\u4e5d\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bed\u4e49\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u4e00\u79cd\u534a\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\uff0c\u901a\u8fc7WiC\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "motivation": "\u8de8\u8bed\u8a00\u8fc1\u79fb\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\uff0c\u4f46\u76ee\u524d\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4f\u6b64\u7c7b\u8d44\u6e90\uff0c\u9650\u5236\u4e86\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u7684\u5e94\u7528\u548c\u8bed\u8a00\u6280\u672f\u7684\u6269\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\uff0c\u521b\u5efa\u4e86\u5305\u542b\u591a\u4e49\u8bcd\u53e5\u5b50\u7684\u8bed\u4e49\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e5d\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u9488\u5bf9\u6027\u7684\u6570\u636e\u96c6\u521b\u5efa\u548c\u8bc4\u4f30\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u591a\u4e49\u8bcd\u6d88\u6b67\u548c\u8de8\u8bed\u8a00\u8fc1\u79fb\u7814\u7a76\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u53d1\u5e03\u7684\u6570\u636e\u96c6\u548c\u4ee3\u7801\u65e8\u5728\u652f\u6301\u66f4\u516c\u5e73\u3001\u7a33\u5065\u548c\u771f\u6b63\u591a\u8bed\u8a00\u7684NLP\u7814\u7a76\u3002", "keywords": "\u4f4e\u8d44\u6e90\u8bed\u8a00, \u8de8\u8bed\u8a00\u8fc1\u79fb, \u8bed\u4e49\u6807\u6ce8, \u591a\u4e49\u8bcd\u6d88\u6b67, WiC"}}
{"id": "2505.23427", "pdf": "https://arxiv.org/pdf/2505.23427", "abs": "https://arxiv.org/abs/2505.23427", "authors": ["Monika Gahalawat", "Maneesh Bilalpur", "Raul Fernandez Rojas", "Jeffrey F. Cohn", "Roland Goecke", "Ramanathan Subramanian"], "title": "On the Validity of Head Motion Patterns as Generalisable Depression Biomarkers", "categories": ["cs.LG"], "comment": null, "summary": "Depression is a debilitating mood disorder negatively impacting millions\nworldwide. While researchers have explored multiple verbal and non-verbal\nbehavioural cues for automated depression assessment, head motion has received\nlittle attention thus far. Further, the common practice of validating machine\nlearning models via a single dataset can limit model generalisability. This\nwork examines the effectiveness and generalisability of models utilising\nelementary head motion units, termed kinemes, for depression severity\nestimation. Specifically, we consider three depression datasets from different\nwestern cultures (German: AVEC2013, Australian: Blackdog and American: Pitt\ndatasets) with varied contextual and recording settings to investigate the\ngeneralisability of the derived kineme patterns via two methods: (i) k-fold\ncross-validation over individual/multiple datasets, and (ii) model reuse on\nother datasets. Evaluating classification and regression performance with\nclassical machine learning methods, our results show that: (1) head motion\npatterns are efficient biomarkers for estimating depression severity, achieving\nhighly competitive performance for both classification and regression tasks on\na variety of datasets, including achieving the second best Mean Absolute Error\n(MAE) on the AVEC2013 dataset, and (2) kineme-based features are more\ngeneralisable than (a) raw head motion descriptors for binary severity\nclassification, and (b) other visual behavioural cues for severity estimation\n(regression).", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u5934\u90e8\u8fd0\u52a8\u57fa\u7840\u5355\u5143\uff08kinemes\uff09\u4f5c\u4e3a\u6291\u90c1\u75c7\u4e25\u91cd\u7a0b\u5ea6\u8bc4\u4f30\u7684\u751f\u7269\u6807\u5fd7\uff0c\u901a\u8fc7\u591a\u6570\u636e\u96c6\u9a8c\u8bc1\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7ed3\u679c\u8868\u660e\uff0ckineme\u7279\u5f81\u5728\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u4e14\u6cdb\u5316\u6027\u66f4\u5f3a\u3002", "motivation": "\u6291\u90c1\u75c7\u4e25\u91cd\u5f71\u54cd\u5168\u7403\u6570\u767e\u4e07\u4eba\uff0c\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u5173\u6ce8\u5934\u90e8\u8fd0\u52a8\u7684\u8bc4\u4f30\u4f5c\u7528\uff0c\u4e14\u5355\u4e00\u6570\u636e\u96c6\u9a8c\u8bc1\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u6027\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22kinemes\u7684\u6709\u6548\u6027\u548c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e09\u4e2a\u897f\u65b9\u6587\u5316\u80cc\u666f\u7684\u6291\u90c1\u75c7\u6570\u636e\u96c6\uff08\u5fb7\u56fdAVEC2013\u3001\u6fb3\u5927\u5229\u4e9aBlackdog\u3001\u7f8e\u56fdPitt\uff09\uff0c\u901a\u8fc7\u4e24\u79cd\u65b9\u6cd5\u9a8c\u8bc1\uff1a(1) \u5355/\u591a\u6570\u636e\u96c6\u7684k\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff0c(2) \u6570\u636e\u96c6\u95f4\u7684\u6a21\u578b\u590d\u7528\uff0c\u8bc4\u4f30\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u5206\u7c7b\u548c\u56de\u5f52\u6027\u80fd\u3002", "result": "\u5934\u90e8\u8fd0\u52a8\u6a21\u5f0f\u662f\u6709\u6548\u7684\u6291\u90c1\u75c7\u4e25\u91cd\u7a0b\u5ea6\u751f\u7269\u6807\u5fd7\uff1a(1) \u5728\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5982AVEC2013\u6570\u636e\u96c6\u7684MAE\u7b2c\u4e8c\u4f18\uff1b(2) kineme\u7279\u5f81\u6bd4\u539f\u59cb\u5934\u90e8\u8fd0\u52a8\u63cf\u8ff0\u7b26\uff08\u5206\u7c7b\uff09\u548c\u5176\u4ed6\u89c6\u89c9\u884c\u4e3a\u7ebf\u7d22\uff08\u56de\u5f52\uff09\u66f4\u5177\u6cdb\u5316\u6027\u3002", "conclusion": "kinemes\u4f5c\u4e3a\u8de8\u6587\u5316\u6570\u636e\u96c6\u7684\u5934\u90e8\u8fd0\u52a8\u7279\u5f81\uff0c\u80fd\u663e\u8457\u63d0\u5347\u6291\u90c1\u75c7\u8bc4\u4f30\u7684\u6cdb\u5316\u6027\u548c\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u5316\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u6291\u90c1\u75c7\u8bc4\u4f30\u3001\u5934\u90e8\u8fd0\u52a8\u3001kinemes\u3001\u673a\u5668\u5b66\u4e60\u3001\u6cdb\u5316\u6027"}}
{"id": "2505.23715", "pdf": "https://arxiv.org/pdf/2505.23715", "abs": "https://arxiv.org/abs/2505.23715", "authors": ["Jinzhe Li", "Gengxu Li", "Yi Chang", "Yuan Wu"], "title": "Don't Take the Premise for Granted: Evaluating the Premise Critique Ability of Large Language Models", "categories": ["cs.CL"], "comment": "31 pages,13 figures,15 tables", "summary": "Large language models (LLMs) have witnessed rapid advancements, demonstrating\nremarkable capabilities. However, a notable vulnerability persists: LLMs often\nuncritically accept flawed or contradictory premises, leading to inefficient\nreasoning and unreliable outputs. This emphasizes the significance of\npossessing the \\textbf{Premise Critique Ability} for LLMs, defined as the\ncapacity to proactively identify and articulate errors in input premises. Most\nexisting studies assess LLMs' reasoning ability in ideal settings, largely\nignoring their vulnerabilities when faced with flawed premises. Thus, we\nintroduce the \\textbf{Premise Critique Bench (PCBench)}, designed by\nincorporating four error types across three difficulty levels, paired with\nmulti-faceted evaluation metrics. We conducted systematic evaluations of 15\nrepresentative LLMs. Our findings reveal: (1) Most models rely heavily on\nexplicit prompts to detect errors, with limited autonomous critique; (2)\nPremise critique ability depends on question difficulty and error type, with\ndirect contradictions being easier to detect than complex or procedural errors;\n(3) Reasoning ability does not consistently correlate with the premise critique\nability; (4) Flawed premises trigger overthinking in reasoning models, markedly\nlengthening responses due to repeated attempts at resolving conflicts. These\ninsights underscore the urgent need to enhance LLMs' proactive evaluation of\ninput validity, positioning premise critique as a foundational capability for\ndeveloping reliable, human-centric systems. The code is available at\nhttps://github.com/MLGroupJLU/Premise_Critique.", "AI": {"tldr": "\u6458\u8981\u603b\u7ed3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u73b0\u6709\u95ee\u9898\u2014\u2014\u7f3a\u4e4f\u5bf9\u9519\u8bef\u524d\u63d0\u7684\u6279\u5224\u80fd\u529b,\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6PCBench,\u5e76\u5bf915\u79cd\u4ee3\u8868\u6027LLMs\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30,\u53d1\u73b0\u5176\u6279\u5224\u80fd\u529b\u4e0e\u63a8\u7406\u80fd\u529b\u5e76\u4e0d\u603b\u76f8\u5173,\u4e14\u9519\u8bef\u524d\u63d0\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u601d\u8003\u3002\u5f3a\u8c03\u4e86\u63d0\u5347LLMs\u524d\u63d0\u6279\u5224\u80fd\u529b\u7684\u5fc5\u8981\u6027\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8LLMs\u5728\u7406\u60f3\u73af\u5883\u4e2d\u7684\u63a8\u7406\u80fd\u529b,\u800c\u5ffd\u7565\u4e86\u5b83\u4eec\u5728\u9762\u5bf9\u9519\u8bef\u524d\u63d0\u65f6\u7684\u8106\u5f31\u6027\u3002\u56e0\u6b64,\u9700\u8981\u8bc4\u4f30\u548c\u63d0\u5347LLMs\u7684\u524d\u63d0\u6279\u5224\u80fd\u529b,\u4ee5\u589e\u5f3a\u5176\u53ef\u9760\u6027\u548c\u4ee5\u4eba\u4e3a\u672c\u7684\u7279\u6027\u3002", "method": "\u5f15\u5165PCBench\u57fa\u51c6,\u5305\u542b\u56db\u79cd\u9519\u8bef\u7c7b\u578b\u548c\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b,\u5e76\u91c7\u7528\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6307\u6807\u3002\u5bf915\u79cd\u4ee3\u8868\u6027LLMs\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "(1)\u591a\u6570\u6a21\u578b\u9700\u4f9d\u8d56\u663e\u5f0f\u63d0\u793a\u624d\u80fd\u68c0\u6d4b\u9519\u8bef,\u81ea\u4e3b\u6279\u5224\u80fd\u529b\u6709\u9650;(2)\u524d\u63d0\u6279\u5224\u80fd\u529b\u53d7\u95ee\u9898\u548c\u9519\u8bef\u7c7b\u578b\u5f71\u54cd;(3)\u63a8\u7406\u80fd\u529b\u4e0e\u524d\u63d0\u6279\u5224\u80fd\u529b\u65e0\u7a33\u5b9a\u76f8\u5173\u6027;(4)\u9519\u8bef\u524d\u63d0\u4f1a\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u601d\u8003,\u663e\u8457\u5ef6\u957f\u54cd\u5e94\u65f6\u95f4\u3002", "conclusion": "\u63d0\u5347LLMs\u5bf9\u8f93\u5165\u6709\u6548\u6027\u7684\u4e3b\u52a8\u8bc4\u4f30\u80fd\u529b\u81f3\u5173\u91cd\u8981,\u524d\u63d0\u6279\u5224\u80fd\u529b\u5e94\u4f5c\u4e3a\u5f00\u53d1\u53ef\u9760\u3001\u4ee5\u4eba\u4e3a\u672c\u7cfb\u7edf\u7684\u57fa\u7840\u80fd\u529b\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u524d\u63d0\u6279\u5224,PCBench,\u8bc4\u4f30\u57fa\u51c6,\u63a8\u7406\u80fd\u529b"}}
{"id": "2505.23433", "pdf": "https://arxiv.org/pdf/2505.23433", "abs": "https://arxiv.org/abs/2505.23433", "authors": ["Jian Yao", "Ran Cheng", "Xingyu Wu", "Jibin Wu", "Kay Chen Tan"], "title": "Diversity-Aware Policy Optimization for Large Language Model Reasoning", "categories": ["cs.LG"], "comment": null, "summary": "The reasoning capabilities of large language models (LLMs) have advanced\nrapidly, particularly following the release of DeepSeek R1, which has inspired\na surge of research into data quality and reinforcement learning (RL)\nalgorithms. Despite the pivotal role diversity plays in RL, its influence on\nLLM reasoning remains largely underexplored. To bridge this gap, this work\npresents a systematic investigation into the impact of diversity in RL-based\ntraining for LLM reasoning, and proposes a novel diversity-aware policy\noptimization method. Across evaluations on 12 LLMs, we observe a strong\npositive correlation between the solution diversity and Potential at k (a novel\nmetric quantifying an LLM's reasoning potential) in high-performing models.\nThis finding motivates our method to explicitly promote diversity during RL\ntraining. Specifically, we design a token-level diversity and reformulate it\ninto a practical objective, then we selectively apply it to positive samples.\nIntegrated into the R1-zero training framework, our method achieves a 3.5\npercent average improvement across four mathematical reasoning benchmarks,\nwhile generating more diverse and robust solutions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u591a\u6837\u6027\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u591a\u6837\u6027\u611f\u77e5\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u5347\u4e863.5%\uff0c\u540c\u65f6\u751f\u6210\u66f4\u591a\u6837\u5316\u548c\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6837\u6027\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5176\u5bf9LLM\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u7814\u7a76\u591a\u6837\u6027\u5728RL\u8bad\u7ec3\u4e2d\u5bf9LLM\u63a8\u7406\u7684\u4fc3\u8fdb\u4f5c\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u4ee4\u724c\u7ea7\u522b\u7684\u591a\u6837\u6027\u5ea6\u91cf\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u5b9e\u9645\u76ee\u6807\uff0c\u5e76\u9009\u62e9\u6027\u5730\u5e94\u7528\u4e8e\u6b63\u6837\u672c\u3002\u8be5\u65b9\u6cd5\u96c6\u6210\u5230R1-zero\u8bad\u7ec3\u6846\u67b6\u4e2d\u3002", "result": "\u572812\u4e2aLLM\u7684\u8bc4\u4f30\u4e2d\uff0c\u9ad8\u8868\u73b0\u6a21\u578b\u7684\u63a8\u7406\u591a\u6837\u6027\u4e0e\u63a8\u7406\u6f5c\u529b\u5448\u6b63\u76f8\u5173\u3002\u8be5\u65b9\u6cd5\u5728\u56db\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u53473.5%\uff0c\u4e14\u751f\u6210\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u591a\u6837\u6027\u5bf9LLM\u63a8\u7406\u6709\u663e\u8457\u5f71\u54cd\uff0c\u63d0\u51fa\u7684\u591a\u6837\u6027\u611f\u77e5\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u548c\u89e3\u51b3\u65b9\u6848\u7684\u591a\u6837\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5f3a\u5316\u5b66\u4e60, \u63a8\u7406\u80fd\u529b, \u591a\u6837\u6027, R1-zero"}}
{"id": "2505.23722", "pdf": "https://arxiv.org/pdf/2505.23722", "abs": "https://arxiv.org/abs/2505.23722", "authors": ["Fan Bai", "Hamid Hassanzadeh", "Ardavan Saeedi", "Mark Dredze"], "title": "Label-Guided In-Context Learning for Named Entity Recognition", "categories": ["cs.CL"], "comment": "Preprint", "summary": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks using only a few demonstrations. In Named Entity Recognition (NER),\ndemonstrations are typically selected based on semantic similarity to the test\ninstance, ignoring training labels and resulting in suboptimal performance. We\nintroduce DEER, a new method that leverages training labels through token-level\nstatistics to improve ICL performance. DEER first enhances example selection\nwith a label-guided, token-based retriever that prioritizes tokens most\ninformative for entity recognition. It then prompts the LLM to revisit\nerror-prone tokens, which are also identified using label statistics, and make\ntargeted corrections. Evaluated on five NER datasets using four different LLMs,\nDEER consistently outperforms existing ICL methods and approaches the\nperformance of supervised fine-tuning. Further analysis shows its effectiveness\non both seen and unseen entities and its robustness in low-resource settings.", "AI": {"tldr": "DEER\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u8bad\u7ec3\u6807\u7b7e\u7684token\u7ea7\u7edf\u8ba1\u4fe1\u606f\u4f18\u5316ICL\u793a\u4f8b\u9009\u62e9\u548c\u7ea0\u6b63\u9519\u8beftoken\uff0c\u663e\u8457\u63d0\u5347\u4e86NER\u4efb\u52a1\u6027\u80fd\uff0c\u63a5\u8fd1\u76d1\u7763\u5fae\u8c03\u6c34\u5e73\u3002", "motivation": "\u4f20\u7edfICL\u5728NER\u4efb\u52a1\u4e2d\u4ec5\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u9009\u62e9\u793a\u4f8b\uff0c\u5ffd\u7565\u8bad\u7ec3\u6807\u7b7e\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002DEER\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u5229\u7528\u6807\u7b7e\u7edf\u8ba1\u4fe1\u606f\u6539\u8fdb\u793a\u4f8b\u9009\u62e9\u548c\u9519\u8bef\u7ea0\u6b63\u3002", "method": "1. \u4f7f\u7528\u6807\u7b7e\u5f15\u5bfc\u7684token\u68c0\u7d22\u5668\u4f18\u5316\u793a\u4f8b\u9009\u62e9\uff1b2. \u901a\u8fc7\u6807\u7b7e\u7edf\u8ba1\u8bc6\u522b\u6613\u9519token\u5e76\u63d0\u793aLLM\u9488\u5bf9\u6027\u4fee\u6b63\u3002", "result": "\u57285\u4e2aNER\u6570\u636e\u96c6\u548c4\u79cdLLM\u4e0a\uff0cDEER\u5747\u4f18\u4e8e\u73b0\u6709ICL\u65b9\u6cd5\uff0c\u63a5\u8fd1\u76d1\u7763\u5fae\u8c03\u6027\u80fd\uff0c\u4e14\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "DEER\u9a8c\u8bc1\u4e86\u6807\u7b7e\u7edf\u8ba1\u4fe1\u606f\u5728ICL\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86NER\u4efb\u52a1\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "keywords": "in-context learning, named entity recognition, token-level statistics, label-guided retrieval, low-resource NER"}}
{"id": "2505.23437", "pdf": "https://arxiv.org/pdf/2505.23437", "abs": "https://arxiv.org/abs/2505.23437", "authors": ["Antonio Ferrara", "Andrea Pugnana", "Francesco Bonchi", "Salvatore Ruggieri"], "title": "Bounded-Abstention Pairwise Learning to Rank", "categories": ["cs.LG", "cs.AI", "cs.IR"], "comment": null, "summary": "Ranking systems influence decision-making in high-stakes domains like health,\neducation, and employment, where they can have substantial economic and social\nimpacts. This makes the integration of safety mechanisms essential. One such\nmechanism is $\\textit{abstention}$, which enables algorithmic decision-making\nsystem to defer uncertain or low-confidence decisions to human experts. While\nabstention have been predominantly explored in the context of classification\ntasks, its application to other machine learning paradigms remains\nunderexplored. In this paper, we introduce a novel method for abstention in\npairwise learning-to-rank tasks. Our approach is based on thresholding the\nranker's conditional risk: the system abstains from making a decision when the\nestimated risk exceeds a predefined threshold. Our contributions are threefold:\na theoretical characterization of the optimal abstention strategy, a\nmodel-agnostic, plug-in algorithm for constructing abstaining ranking models,\nand a comprehensive empirical evaluations across multiple datasets,\ndemonstrating the effectiveness of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5728\u6210\u5bf9\u5b66\u4e60\u6392\u5e8f\u4efb\u52a1\u4e2d\u5f15\u5165\u5f03\u6743\u673a\u5236\uff0c\u57fa\u4e8e\u5bf9\u6392\u540d\u5668\u6761\u4ef6\u98ce\u9669\u7684\u9608\u503c\u8bbe\u5b9a\uff0c\u5f53\u98ce\u9669\u8d85\u9884\u8bbe\u9608\u503c\u65f6\u5f03\u6743\uff0c\u907f\u514d\u4f4e\u7f6e\u4fe1\u5ea6\u51b3\u7b56\u3002", "motivation": "\u6392\u540d\u7cfb\u7edf\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u5065\u5eb7\u3001\u6559\u80b2\u548c\u5c31\u4e1a\uff09\u4e2d\u53d1\u6325\u91cd\u5927\u7ecf\u6d4e\u548c\u793e\u4f1a\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u5b89\u5168\u673a\u5236\u3002\u5f03\u6743\u4f5c\u4e3a\u4e00\u79cd\u673a\u5236\uff0c\u53ef\u8ba9\u7b97\u6cd5\u5728\u4e0d\u786e\u5b9a\u6216\u4f4e\u7f6e\u4fe1\u5ea6\u65f6\u4ea4\u7531\u4eba\u7c7b\u4e13\u5bb6\u51b3\u7b56\u3002", "method": "\u901a\u8fc7\u9608\u503c\u8bbe\u5b9a\u6392\u540d\u5668\u7684\u6761\u4ef6\u98ce\u9669\uff0c\u5f53\u4f30\u8ba1\u98ce\u9669\u8d85\u8fc7\u9884\u8bbe\u9608\u503c\u65f6\u7cfb\u7edf\u5f03\u6743\u3002\u63d0\u51fa\u7406\u8bba\u6700\u4f18\u5f03\u6743\u7b56\u7565\u3001\u6a21\u578b\u65e0\u5173\u7684\u63d2\u4ef6\u7b97\u6cd5\uff0c\u5e76\u8fdb\u884c\u591a\u6570\u636e\u96c6\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6709\u6548\uff0c\u80fd\u591f\u901a\u8fc7\u5f03\u6743\u673a\u5236\u63d0\u5347\u51b3\u7b56\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6392\u5e8f\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f03\u6743\u7b56\u7565\uff0c\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u7ed3\u679c\u5747\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u548c\u4f18\u52bf\u3002", "keywords": "\u6392\u540d\u7cfb\u7edf, \u5f03\u6743\u673a\u5236, \u6210\u5bf9\u5b66\u4e60\u6392\u5e8f, \u6761\u4ef6\u98ce\u9669\u9608\u503c, \u6a21\u578b\u65e0\u5173\u7b97\u6cd5"}}
{"id": "2505.23723", "pdf": "https://arxiv.org/pdf/2505.23723", "abs": "https://arxiv.org/abs/2505.23723", "authors": ["Zexi Liu", "Jingyi Chai", "Xinyu Zhu", "Shuo Tang", "Rui Ye", "Bo Zhang", "Lei Bai", "Siheng Chen"], "title": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The emergence of large language model (LLM)-based agents has significantly\nadvanced the development of autonomous machine learning (ML) engineering.\nHowever, most existing approaches rely heavily on manual prompt engineering,\nfailing to adapt and optimize based on diverse experimental experiences.\nFocusing on this, for the first time, we explore the paradigm of learning-based\nagentic ML, where an LLM agent learns through interactive experimentation on ML\ntasks using online reinforcement learning (RL). To realize this, we propose a\nnovel agentic ML training framework with three key components: (1)\nexploration-enriched fine-tuning, which enables LLM agents to generate diverse\nactions for enhanced RL exploration; (2) step-wise RL, which enables training\non a single action step, accelerating experience collection and improving\ntraining efficiency; (3) an agentic ML-specific reward module, which unifies\nvaried ML feedback signals into consistent rewards for RL optimization.\nLeveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM\nfor autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our\n7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it\nachieves continuous performance improvements and demonstrates exceptional\ncross-task generalization capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u8303\u5f0f\u7684\u81ea\u4e3b\u673a\u5668\u5b66\u4e60\u4ee3\u7406\uff08ML-Agent\uff09\uff0c\u5229\u7528\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4f18\u5316LLM\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u66f4\u5927\u89c4\u6a21\u4ee3\u7406\u7684\u8868\u73b0\uff0c\u5e76\u5c55\u793a\u4e86\u4f18\u5f02\u7684\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u6280\u672f\u4f9d\u8d56\u624b\u52a8\u63d0\u793a\u5de5\u7a0b\uff0c\u7f3a\u4e4f\u6839\u636e\u5b9e\u9a8c\u7ecf\u9a8c\u52a8\u6001\u4f18\u5316\u7684\u80fd\u529b\u3002\u8bba\u6587\u9996\u6b21\u63a2\u7d22\u5b66\u4e60\u578b\u4ee3\u7406\u8303\u5f0f\uff0c\u65e8\u5728\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4ee3\u7406\u7684\u81ea\u4e3b\u5b66\u4e60\u548c\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\u7684\u8bad\u7ec3\u6846\u67b6\uff1a\u63a2\u7d22\u589e\u5f3a\u5fae\u8c03\uff08\u63d0\u5347RL\u63a2\u7d22\u80fd\u529b\uff09\u3001\u5206\u6b65RL\uff08\u52a0\u901f\u7ecf\u9a8c\u6536\u96c6\uff09\u548c\u4ee3\u7406\u7279\u5b9a\u5956\u52b1\u6a21\u5757\uff08\u7edf\u4e00\u4f18\u5316\u53cd\u9988\u4fe1\u53f7\uff09\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\u8bad\u7ec3\u4e867B\u89c4\u6a21\u7684ML-Agent\uff08Qwen-2.5\u9a71\u52a8\uff09\u3002", "result": "\u5c3d\u7ba1\u4ec5\u8bad\u7ec3\u4e869\u4e2aML\u4efb\u52a1\uff0c7B\u7684ML-Agent\u6027\u80fd\u8d85\u8d8a671B\u7684DeepSeek-R1\u4ee3\u7406\uff0c\u4e14\u80fd\u6301\u7eed\u63d0\u5347\u5e76\u5c55\u793a\u51fa\u8272\u7684\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5b66\u4e60\u578b\u4ee3\u7406\u8303\u5f0f\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u63d0\u5347LLM\u4ee3\u7406\u7684\u81ea\u4e3b\u6027\u548c\u6027\u80fd\uff0c\u5c0f\u89c4\u6a21\u6a21\u578b\u4e5f\u80fd\u5b9e\u73b0\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\u7684\u6210\u679c\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3001\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u3001\u81ea\u4e3b\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u3001\u4ee3\u7406\uff08Agent\uff09\u3001\u8de8\u4efb\u52a1\u6cdb\u5316"}}
{"id": "2505.23442", "pdf": "https://arxiv.org/pdf/2505.23442", "abs": "https://arxiv.org/abs/2505.23442", "authors": ["Linyu Li", "Zhi Jin", "Yuanpeng He", "Dongming Jin", "Haoran Duan", "Zhengwei Tao", "Xuan Zhang", "Jiandong Li"], "title": "Rethinking Regularization Methods for Knowledge Graph Completion", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Knowledge graph completion (KGC) has attracted considerable attention in\nrecent years because it is critical to improving the quality of knowledge\ngraphs. Researchers have continuously explored various models. However, most\nprevious efforts have neglected to take advantage of regularization from a\ndeeper perspective and therefore have not been used to their full potential.\nThis paper rethinks the application of regularization methods in KGC. Through\nextensive empirical studies on various KGC models, we find that carefully\ndesigned regularization not only alleviates overfitting and reduces variance\nbut also enables these models to break through the upper bounds of their\noriginal performance. Furthermore, we introduce a novel sparse-regularization\nmethod that embeds the concept of rank-based selective sparsity into the KGC\nregularizer. The core idea is to selectively penalize those components with\nsignificant features in the embedding vector, thus effectively ignoring many\ncomponents that contribute little and may only represent noise. Various\ncomparative experiments on multiple datasets and multiple models show that the\nSPR regularization method is better than other regularization methods and can\nenable the KGC model to further break through the performance margin.", "AI": {"tldr": "\u672c\u6587\u603b\u7ed3\u4e86\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\uff08KGC\uff09\u4e2d\u6b63\u5219\u5316\u65b9\u6cd5\u7684\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u7a00\u758f\u6b63\u5219\u5316\u65b9\u6cd5\uff08SPR\uff09\uff0c\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709KGC\u7814\u7a76\u591a\u5ffd\u89c6\u6b63\u5219\u5316\u7684\u6df1\u5c42\u6f5c\u529b\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u6b63\u5219\u5316\u5bf9KGC\u6027\u80fd\u7684\u5f71\u54cd\u5e76\u63d0\u51fa\u6539\u8fdb\u3002", "method": "\u63d0\u51faSPR\u7a00\u758f\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u9009\u62e9\u6027\u5730\u60e9\u7f5a\u5d4c\u5165\u5411\u91cf\u4e2d\u663e\u8457\u7279\u5f81\uff0c\u51cf\u5c11\u566a\u58f0\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSPR\u4f18\u4e8e\u5176\u4ed6\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u7a81\u7834\u6a21\u578b\u539f\u6709\u6548\u80fd\u4e0a\u9650\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u5347KGC\u6a21\u578b\u7684\u6027\u80fd\u3002", "keywords": "\u77e5\u8bc6\u56fe\u8c31\u8865\u5168, \u6b63\u5219\u5316, \u7a00\u758f\u6027, \u5d4c\u5165\u5411\u91cf, SPR"}}
{"id": "2505.23729", "pdf": "https://arxiv.org/pdf/2505.23729", "abs": "https://arxiv.org/abs/2505.23729", "authors": ["Mohamad Chehade", "Soumya Suvra Ghosal", "Souradip Chakraborty", "Avinash Reddy", "Dinesh Manocha", "Hao Zhu", "Amrit Singh Bedi"], "title": "Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ICML 2025", "summary": "Aligning large language models with humans is challenging due to the\ninherently multifaceted nature of preference feedback. While existing\napproaches typically frame this as a multi-objective optimization problem, they\noften overlook how humans actually make decisions. Research on bounded\nrationality suggests that human decision making follows satisficing\nstrategies-optimizing primary objectives while ensuring others meet acceptable\nthresholds. To bridge this gap and operationalize the notion of satisficing\nalignment, we propose SITAlign: an inference time framework that addresses the\nmultifaceted nature of alignment by maximizing a primary objective while\nsatisfying threshold-based constraints on secondary criteria. We provide\ntheoretical insights by deriving sub-optimality bounds of our satisficing based\ninference alignment approach. We empirically validate SITAlign's performance\nthrough extensive experimentation on multiple benchmarks. For instance, on the\nPKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while\nensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art\nmulti objective decoding strategy by a margin of 22.3% in terms of GPT-4\nwin-tie rate for helpfulness reward while adhering to the threshold on\nharmlessness.", "AI": {"tldr": "SITAlign\u662f\u4e00\u4e2a\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u4e3b\u8981\u76ee\u6807\u5e76\u6ee1\u8db3\u6b21\u8981\u6807\u51c6\u7684\u9608\u503c\u7ea6\u675f\u6765\u89e3\u51b3\u5bf9\u9f50\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u4eba\u7c7b\u504f\u597d\u53cd\u9988\u89c6\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u4f46\u5ffd\u89c6\u4e86\u4eba\u7c7b\u51b3\u7b56\u7684\u5b9e\u9645\u65b9\u5f0f\uff08\u6ee1\u610f\u7b56\u7565\uff09\u3002", "method": "\u63d0\u51faSITAlign\u6846\u67b6\uff0c\u5728\u63a8\u7406\u65f6\u6700\u5927\u5316\u4e3b\u8981\u76ee\u6807\u7684\u540c\u65f6\uff0c\u6ee1\u8db3\u6b21\u8981\u6807\u51c6\u7684\u9608\u503c\u7ea6\u675f\uff0c\u5e76\u63a8\u5bfc\u5176\u7406\u8bba\u6b21\u4f18\u6027\u754c\u9650\u3002", "result": "\u5728PKU-SafeRLHF\u6570\u636e\u96c6\u4e0a\uff0cSITAlign\u5728\u4fdd\u6301\u65e0\u5bb3\u6027\u9608\u503c\u7684\u540c\u65f6\uff0cGPT-4\u80dc\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad822.3%\u3002", "conclusion": "SITAlign\u901a\u8fc7\u6ee1\u610f\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u591a\u9762\u5411\u5bf9\u9f50\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u5bf9\u9f50, \u6ee1\u610f\u7b56\u7565, SITAlign, \u591a\u76ee\u6807\u4f18\u5316"}}
{"id": "2505.23132", "pdf": "https://arxiv.org/pdf/2505.23132", "abs": "https://arxiv.org/abs/2505.23132", "authors": ["Seung Gyu Jeong", "Seong Eun Kim"], "title": "Patient Domain Supervised Contrastive Learning for Lung Sound Classification Using Mobile Phone", "categories": ["cs.SD", "cs.AI"], "comment": "ITS-CSCC 2024", "summary": "Auscultation is crucial for diagnosing lung diseases. The COVID-19 pandemic\nhas revealed the limitations of traditional, in-person lung sound assessments.\nTo overcome these issues, advancements in digital stethoscopes and artificial\nintelligence (AI) have led to the development of new diagnostic methods. In\nthis context, our study aims to use smartphone microphones to record and\nanalyze lung sounds. We faced two major challenges: the difference in audio\nstyle between electronic stethoscopes and smartphone microphones, and the\nvariability among patients. To address these challenges, we developed a method\ncalled Patient Domain Supervised Contrastive Learning (PD-SCL). By integrating\nthis method with the Audio Spectrogram Transformer (AST) model, we\nsignificantly improved its performance by 2.4\\% compared to the original AST\nmodel. This progress demonstrates that smartphones can effectively diagnose\nlung sounds, addressing inconsistencies in patient data and showing potential\nfor broad use beyond traditional clinical settings. Our research contributes to\nmaking lung disease detection more accessible in the post-COVID-19 world.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPD-SCL\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408AST\u6a21\u578b\uff0c\u5229\u7528\u667a\u80fd\u624b\u673a\u9ea6\u514b\u98ce\u5206\u6790\u80ba\u90e8\u58f0\u97f3\uff0c\u6027\u80fd\u63d0\u53472.4%\uff0c\u5c55\u793a\u4e86\u624b\u673a\u5728\u80ba\u90e8\u75be\u75c5\u8bca\u65ad\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u80ba\u90e8\u542c\u8bca\u5728COVID-19\u671f\u95f4\u66b4\u9732\u4e86\u5c40\u9650\u6027\uff0c\u4fc3\u4f7f\u7814\u7a76\u901a\u8fc7\u667a\u80fd\u624b\u673a\u548cAI\u6280\u672f\u5f00\u53d1\u66f4\u4fbf\u6377\u7684\u8bca\u65ad\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u60a3\u8005\u9886\u57df\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\uff08PD-SCL\uff09\uff0c\u7ed3\u5408\u97f3\u9891\u9891\u8c31\u53d8\u6362\u5668\uff08AST\uff09\u6a21\u578b\uff0c\u4f18\u5316\u667a\u80fd\u624b\u673a\u8bb0\u5f55\u7684\u80ba\u90e8\u58f0\u97f3\u5206\u6790\u3002", "result": "PD-SCL\u4e0eAST\u7ed3\u5408\uff0c\u6027\u80fd\u6bd4\u539f\u59cbAST\u63d0\u53472.4%\uff0c\u9a8c\u8bc1\u4e86\u667a\u80fd\u624b\u673a\u8bca\u65ad\u80ba\u90e8\u58f0\u97f3\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u667a\u80fd\u624b\u673a\u80fd\u6709\u6548\u8bca\u65ad\u80ba\u90e8\u58f0\u97f3\uff0c\u4e3a\u540e\u75ab\u60c5\u65f6\u4ee3\u80ba\u90e8\u75be\u75c5\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u5de5\u5177\u3002", "keywords": "\u80ba\u90e8\u542c\u8bca, \u667a\u80fd\u624b\u673a, AI, PD-SCL, AST"}}
{"id": "2505.23443", "pdf": "https://arxiv.org/pdf/2505.23443", "abs": "https://arxiv.org/abs/2505.23443", "authors": ["Benyamin Trachtenberg", "Nir Rosenfeld"], "title": "Strategic Classification with Non-Linear Classifiers", "categories": ["cs.LG"], "comment": null, "summary": "In strategic classification, the standard supervised learning setting is\nextended to support the notion of strategic user behavior in the form of costly\nfeature manipulations made in response to a classifier. While standard learning\nsupports a broad range of model classes, the study of strategic classification\nhas, so far, been dedicated mostly to linear classifiers. This work aims to\nexpand the horizon by exploring how strategic behavior manifests under\nnon-linear classifiers and what this implies for learning. We take a bottom-up\napproach showing how non-linearity affects decision boundary points, classifier\nexpressivity, and model classes complexity. A key finding is that universal\napproximators (e.g., neural nets) are no longer universal once the environment\nis strategic. We demonstrate empirically how this can create performance gaps\neven on an unrestricted model class.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u6218\u7565\u5206\u7c7b\u4e2d\u975e\u7ebf\u6027\u5206\u7c7b\u5668\u5bf9\u7528\u6237\u7b56\u7565\u884c\u4e3a\u7684\u5f71\u54cd\u53ca\u5176\u5bf9\u5b66\u4e60\u7684\u542f\u793a\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u6269\u5c55\u6218\u7565\u5206\u7c7b\u7684\u7814\u7a76\u8303\u56f4\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u63a2\u7d22\u975e\u7ebf\u6027\u5206\u7c7b\u5668\u5728\u6218\u7565\u73af\u5883\u4e0b\u7684\u8868\u73b0\u548c\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u81ea\u5e95\u5411\u4e0a\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u975e\u7ebf\u6027\u5982\u4f55\u5f71\u54cd\u51b3\u7b56\u8fb9\u754c\u70b9\u3001\u5206\u7c7b\u5668\u8868\u8fbe\u80fd\u529b\u53ca\u6a21\u578b\u7c7b\u7684\u590d\u6742\u5ea6\u3002", "result": "\u5173\u952e\u53d1\u73b0\u662f\uff0c\u5728\u6218\u7565\u73af\u5883\u4e0b\uff0c\u901a\u7528\u903c\u8fd1\u5668\uff08\u5982\u795e\u7ecf\u7f51\u7edc\uff09\u4e0d\u518d\u5177\u5907\u901a\u7528\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u975e\u7ebf\u6027\u5206\u7c7b\u5668\u5728\u6218\u7565\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u4e0e\u6807\u51c6\u5b66\u4e60\u73af\u5883\u4e0d\u540c\uff0c\u5bf9\u6a21\u578b\u9009\u62e9\u548c\u8bbe\u8ba1\u63d0\u51fa\u4e86\u65b0\u6311\u6218\u3002", "keywords": "\u6218\u7565\u5206\u7c7b, \u975e\u7ebf\u6027\u5206\u7c7b\u5668, \u6218\u7565\u884c\u4e3a, \u673a\u5668\u5b66\u4e60, \u6a21\u578b\u590d\u6742\u5ea6"}}
{"id": "2505.23735", "pdf": "https://arxiv.org/pdf/2505.23735", "abs": "https://arxiv.org/abs/2505.23735", "authors": ["Ali Behrouz", "Zeman Li", "Praneeth Kacham", "Majid Daliri", "Yuan Deng", "Peilin Zhong", "Meisam Razaviyayn", "Vahab Mirrokni"], "title": "ATLAS: Learning to Optimally Memorize the Context at Test Time", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transformers have been established as the most popular backbones in sequence\nmodeling, mainly due to their effectiveness in in-context retrieval tasks and\nthe ability to learn at scale. Their quadratic memory and time complexity,\nhowever, bound their applicability in longer sequences and so has motivated\nresearchers to explore effective alternative architectures such as modern\nrecurrent neural networks (a.k.a long-term recurrent memory module). Despite\ntheir recent success in diverse downstream tasks, they struggle in tasks that\nrequires long context understanding and extrapolation to longer sequences. We\nobserve that these shortcomings come from three disjoint aspects in their\ndesign: (1) limited memory capacity that is bounded by the architecture of\nmemory and feature mapping of the input; (2) online nature of update, i.e.,\noptimizing the memory only with respect to the last input; and (3) less\nexpressive management of their fixed-size memory. To enhance all these three\naspects, we present ATLAS, a long-term memory module with high capacity that\nlearns to memorize the context by optimizing the memory based on the current\nand past tokens, overcoming the online nature of long-term memory models.\nBuilding on this insight, we present a new family of Transformer-like\narchitectures, called DeepTransformers, that are strict generalizations of the\noriginal Transformer architecture. Our experimental results on language\nmodeling, common-sense reasoning, recall-intensive, and long-context\nunderstanding tasks show that ATLAS surpasses the performance of Transformers\nand recent linear recurrent models. ATLAS further improves the long context\nperformance of Titans, achieving +80\\% accuracy in 10M context length of\nBABILong benchmark.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ATLAS\uff0c\u4e00\u79cd\u9ad8\u5bb9\u91cf\u7684\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\uff0c\u901a\u8fc7\u57fa\u4e8e\u5f53\u524d\u548c\u8fc7\u53bb\u7684\u4ee4\u724c\u4f18\u5316\u8bb0\u5fc6\uff0c\u6539\u8fdb\u73b0\u4ee3\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7684\u4e09\u4e2a\u8bbe\u8ba1\u7f3a\u9677\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51faDeepTransformers\u67b6\u6784\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u4f18\u4e8eTransformer\u548c\u7ebf\u6027\u5faa\u73af\u6a21\u578b\u3002", "motivation": "Transformer\u7684\u4e8c\u6b21\u65b9\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5176\u5728\u957f\u5e8f\u5217\u4e2d\u7684\u5e94\u7528\uff0c\u800c\u73b0\u4ee3\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u5916\u63a8\u4efb\u52a1\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86ATLAS\u6a21\u5757\uff0c\u901a\u8fc7\u63d0\u9ad8\u8bb0\u5fc6\u5bb9\u91cf\u3001\u4f18\u5316\u8bb0\u5fc6\u66f4\u65b0\u673a\u5236\u548c\u589e\u5f3a\u56fa\u5b9a\u5927\u5c0f\u8bb0\u5fc6\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u514b\u670d\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7684\u7f3a\u9677\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51faDeepTransformers\u67b6\u6784\u3002", "result": "ATLAS\u5728\u591a\u4efb\u52a1\uff08\u5982\u8bed\u8a00\u5efa\u6a21\u3001\u5e38\u8bc6\u63a8\u7406\u3001\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\uff09\u4e2d\u4f18\u4e8eTransformer\u548c\u7ebf\u6027\u5faa\u73af\u6a21\u578b\uff0c\u5c24\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u5347\u663e\u8457\uff08BABILong\u57fa\u51c6\u4e2d\u51c6\u786e\u7387\u63d0\u534780%\uff09\u3002", "conclusion": "ATLAS\u548cDeepTransformers\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u9ad8\u6548\u67b6\u6784\u3002", "keywords": "Transformer, \u5faa\u73af\u795e\u7ecf\u7f51\u7edc, \u957f\u671f\u8bb0\u5fc6, DeepTransformers, \u957f\u5e8f\u5217\u5efa\u6a21"}}
{"id": "2505.23134", "pdf": "https://arxiv.org/pdf/2505.23134", "abs": "https://arxiv.org/abs/2505.23134", "authors": ["Tongtong Su", "Chengyu Wang", "Jun Huang", "Dongming Lu"], "title": "Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Appearance editing according to user needs is a pivotal task in video\nediting. Existing text-guided methods often lead to ambiguities regarding user\nintentions and restrict fine-grained control over editing specific aspects of\nobjects. To overcome these limitations, this paper introduces a novel approach\nnamed {Zero-to-Hero}, which focuses on reference-based video editing that\ndisentangles the editing process into two distinct problems. It achieves this\nby first editing an anchor frame to satisfy user requirements as a reference\nimage and then consistently propagating its appearance across other frames. We\nleverage correspondence within the original frames to guide the attention\nmechanism, which is more robust than previously proposed optical flow or\ntemporal modules in memory-friendly video generative models, especially when\ndealing with objects exhibiting large motions. It offers a solid ZERO-shot\ninitialization that ensures both accuracy and temporal consistency. However,\nintervention in the attention mechanism results in compounded imaging\ndegradation with over-saturated colors and unknown blurring issues. Starting\nfrom Zero-Stage, our Hero-Stage Holistically learns a conditional generative\nmodel for vidEo RestOration. To accurately evaluate the consistency of the\nappearance, we construct a set of videos with multiple appearances using\nBlender, enabling a fine-grained and deterministic evaluation. Our method\noutperforms the best-performing baseline with a PSNR improvement of 2.6 dB. The\nproject page is at https://github.com/Tonniia/Zero2Hero.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u540d\u4e3a{Zero-to-Hero}\u7684\u53c2\u8003\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u7f16\u8f91\u8fc7\u7a0b\u4e3a\u951a\u5e27\u7f16\u8f91\u548c\u4e00\u81f4\u6027\u4f20\u64ad\uff0c\u5e76\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u548c\u751f\u6210\u6a21\u578b\u63d0\u5347\u6548\u679c\uff0cPSNR\u63d0\u9ad8\u4e862.6 dB\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u5f15\u5bfc\u89c6\u9891\u7f16\u8f91\u4e2d\u7528\u6237\u610f\u56fe\u4e0d\u660e\u786e\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5206\u9636\u6bb5\u65b9\u6cd5\uff1a\u96f6\u9636\u6bb5\u521d\u59cb\u5316\u951a\u5e27\u5e76\u4f20\u64ad\u5916\u89c2\uff0c\u82f1\u96c4\u9636\u6bb5\u901a\u8fc7\u6761\u4ef6\u751f\u6210\u6a21\u578b\u4fee\u590d\u753b\u9762\u9000\u5316\u95ee\u9898\u3002", "result": "\u5728PSNR\u4e0a\u6bd4\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e862.6 dB\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684{Zero-to-Hero}\u65b9\u6cd5\u5728\u89c6\u9891\u7f16\u8f91\u4e2d\u5b9e\u73b0\u4e86\u7528\u6237\u610f\u56fe\u660e\u786e\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u6548\u679c\u663e\u8457\u3002", "keywords": "\u89c6\u9891\u7f16\u8f91, \u53c2\u8003\u56fe\u50cf, \u6ce8\u610f\u529b\u673a\u5236, \u6761\u4ef6\u751f\u6210\u6a21\u578b, PSNR"}}
{"id": "2505.23448", "pdf": "https://arxiv.org/pdf/2505.23448", "abs": "https://arxiv.org/abs/2505.23448", "authors": ["Pirzada Suhail", "Rehna Afroz", "Amit Sethi"], "title": "Network Inversion for Uncertainty-Aware Out-of-Distribution Detection", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Out-of-distribution (OOD) detection and uncertainty estimation (UE) are\ncritical components for building safe machine learning systems, especially in\nreal-world scenarios where unexpected inputs are inevitable. In this work, we\npropose a novel framework that combines network inversion with classifier\ntraining to simultaneously address both OOD detection and uncertainty\nestimation. For a standard n-class classification task, we extend the\nclassifier to an (n+1)-class model by introducing a \"garbage\" class, initially\npopulated with random gaussian noise to represent outlier inputs. After each\ntraining epoch, we use network inversion to reconstruct input images\ncorresponding to all output classes that initially appear as noisy and\nincoherent and are therefore excluded to the garbage class for retraining the\nclassifier. This cycle of training, inversion, and exclusion continues\niteratively till the inverted samples begin to resemble the in-distribution\ndata more closely, suggesting that the classifier has learned to carve out\nmeaningful decision boundaries while sanitising the class manifolds by pushing\nOOD content into the garbage class. During inference, this training scheme\nenables the model to effectively detect and reject OOD samples by classifying\nthem into the garbage class. Furthermore, the confidence scores associated with\neach prediction can be used to estimate uncertainty for both in-distribution\nand OOD inputs. Our approach is scalable, interpretable, and does not require\naccess to external OOD datasets or post-hoc calibration techniques while\nproviding a unified solution to the dual challenges of OOD detection and\nuncertainty estimation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7f51\u7edc\u53cd\u6f14\u4e0e\u5206\u7c7b\u5668\u8bad\u7ec3\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u540c\u65f6\u89e3\u51b3\u5206\u5e03\u5916\u68c0\u6d4b\uff08OOD\uff09\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff08UE\uff09\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u5783\u573e\u7c7b\u5e76\u901a\u8fc7\u8fed\u4ee3\u8bad\u7ec3\u4f18\u5316\u6a21\u578b\u3002", "motivation": "\u5206\u5e03\u5916\u68c0\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5bf9\u4e8e\u6784\u5efa\u5b89\u5168\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u9047\u5230\u610f\u5916\u8f93\u5165\u65f6\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u5916\u90e8OOD\u6570\u636e\u96c6\u6216\u540e\u6821\u51c6\u6280\u672f\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6269\u5c55n\u7c7b\u5206\u7c7b\u5668\u4e3a(n+1)\u7c7b\u6a21\u578b\uff0c\u5f15\u5165\u5783\u573e\u7c7b\uff0c\u5e76\u4f7f\u7528\u7f51\u7edc\u53cd\u6f14\u8fed\u4ee3\u4f18\u5316\u5206\u7c7b\u5668\uff0c\u5c06OOD\u6837\u672c\u63a8\u5165\u5783\u573e\u7c7b\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u5728\u63a8\u7406\u9636\u6bb5\u6709\u6548\u68c0\u6d4b\u548c\u62d2\u65a5OOD\u6837\u672c\uff0c\u540c\u65f6\u4e3a\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u8f93\u5165\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u65e0\u9700\u989d\u5916OOD\u6570\u636e\u6216\u540e\u5904\u7406\u6280\u672f\uff0c\u5b9e\u73b0\u4e86OOD\u68c0\u6d4b\u4e0eUE\u7684\u7edf\u4e00\u89e3\u51b3\u3002", "keywords": "\u5206\u5e03\u5916\u68c0\u6d4b\uff08OOD\uff09, \u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff08UE\uff09, \u7f51\u7edc\u53cd\u6f14, \u5783\u573e\u7c7b, \u673a\u5668\u5b66\u4e60\u5b89\u5168\u6027"}}
{"id": "2505.23754", "pdf": "https://arxiv.org/pdf/2505.23754", "abs": "https://arxiv.org/abs/2505.23754", "authors": ["Ziyin Zhang", "Jiahao Xu", "Zhiwei He", "Tian Liang", "Qiuzhi Liu", "Yansi Li", "Linfeng Song", "Zhengwen Liang", "Zhuosheng Zhang", "Rui Wang", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Theorem proving serves as a major testbed for evaluating complex reasoning\nabilities in large language models (LLMs). However, traditional automated\ntheorem proving (ATP) approaches rely heavily on formal proof systems that\npoorly align with LLMs' strength derived from informal, natural language\nknowledge acquired during pre-training. In this work, we propose DeepTheorem, a\ncomprehensive informal theorem-proving framework exploiting natural language to\nenhance LLM mathematical reasoning. DeepTheorem includes a large-scale\nbenchmark dataset consisting of 121K high-quality IMO-level informal theorems\nand proofs spanning diverse mathematical domains, rigorously annotated for\ncorrectness, difficulty, and topic categories, accompanied by systematically\nconstructed verifiable theorem variants. We devise a novel reinforcement\nlearning strategy (RL-Zero) explicitly tailored to informal theorem proving,\nleveraging the verified theorem variants to incentivize robust mathematical\ninference. Additionally, we propose comprehensive outcome and process\nevaluation metrics examining proof correctness and the quality of reasoning\nsteps. Extensive experimental analyses demonstrate DeepTheorem significantly\nimproves LLM theorem-proving performance compared to existing datasets and\nsupervised fine-tuning protocols, achieving state-of-the-art accuracy and\nreasoning quality. Our findings highlight DeepTheorem's potential to\nfundamentally advance automated informal theorem proving and mathematical\nexploration.", "AI": {"tldr": "DeepTheorem\u662f\u4e00\u4e2a\u5229\u7528\u81ea\u7136\u8bed\u8a00\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u6570\u5b66\u63a8\u7406\u7684\u975e\u6b63\u5f0f\u5b9a\u7406\u8bc1\u660e\u6846\u67b6\uff0c\u5305\u62ec\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u7406\u8bc1\u660e\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u65b9\u6cd5\u4f9d\u8d56\u5f62\u5f0f\u5316\u7cfb\u7edf\uff0c\u4e0eLLMs\u7684\u81ea\u7136\u8bed\u8a00\u77e5\u8bc6\u4e0d\u5339\u914d\u3002DeepTheorem\u65e8\u5728\u5229\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u5347LLMs\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faDeepTheorem\u6846\u67b6\uff0c\u5305\u542b12.1\u4e07\u9ad8\u8d28\u91cfIMO\u7ea7\u975e\u6b63\u5f0f\u5b9a\u7406\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u5f3a\u5316\u5b66\u4e60\u7b56\u7565RL-Zero\uff0c\u5e76\u91c7\u7528\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDeepTheorem\u5728\u5b9a\u7406\u8bc1\u660e\u6027\u80fd\u548c\u63a8\u7406\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "DeepTheorem\u6709\u6f5c\u529b\u63a8\u52a8\u975e\u6b63\u5f0f\u5b9a\u7406\u8bc1\u660e\u548c\u6570\u5b66\u63a2\u7d22\u7684\u8fdb\u6b65\u3002", "keywords": "\u5b9a\u7406\u8bc1\u660e, \u5927\u8bed\u8a00\u6a21\u578b, \u6570\u5b66\u63a8\u7406, \u5f3a\u5316\u5b66\u4e60, \u6570\u636e\u96c6"}}
{"id": "2505.23458", "pdf": "https://arxiv.org/pdf/2505.23458", "abs": "https://arxiv.org/abs/2505.23458", "authors": ["Kevin Frans", "Seohong Park", "Pieter Abbeel", "Sergey Levine"], "title": "Diffusion Guidance Is a Controllable Policy Improvement Operator", "categories": ["cs.LG"], "comment": null, "summary": "At the core of reinforcement learning is the idea of learning beyond the\nperformance in the data. However, scaling such systems has proven notoriously\ntricky. In contrast, techniques from generative modeling have proven remarkably\nscalable and are simple to train. In this work, we combine these strengths, by\nderiving a direct relation between policy improvement and guidance of diffusion\nmodels. The resulting framework, CFGRL, is trained with the simplicity of\nsupervised learning, yet can further improve on the policies in the data. On\noffline RL tasks, we observe a reliable trend -- increased guidance weighting\nleads to increased performance. Of particular importance, CFGRL can operate\nwithout explicitly learning a value function, allowing us to generalize simple\nsupervised methods (e.g., goal-conditioned behavioral cloning) to further\nprioritize optimality, gaining performance for \"free\" across the board.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86CFGRL\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7b56\u7565\u6539\u8fdb\u4e0e\u6269\u6563\u6a21\u578b\u7684\u5f15\u5bfc\u76f8\u7ed3\u5408\uff0c\u5728\u7b80\u5316\u8bad\u7ec3\u7684\u540c\u65f6\u63d0\u5347\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e14\u5728\u65e0\u9700\u663e\u5f0f\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u96be\u4ee5\u6269\u5c55\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u7b80\u5316\u8bad\u7ec3\u6d41\u7a0b\u5e76\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51faCFGRL\u6846\u67b6\uff0c\u5c06\u7b56\u7565\u6539\u8fdb\u4e0e\u6269\u6563\u6a21\u578b\u5f15\u5bfc\u76f4\u63a5\u5173\u8054\uff0c\u5229\u7528\u76d1\u7763\u5b66\u4e60\u7684\u7b80\u5355\u6027\u8bad\u7ec3\uff0c\u5e76\u5728\u6570\u636e\u4e2d\u7684\u7b56\u7565\u57fa\u7840\u4e0a\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u589e\u52a0\u5f15\u5bfc\u6743\u91cd\u53ef\u63d0\u5347\u6027\u80fd\uff1bCFGRL\u65e0\u9700\u663e\u5f0f\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\uff0c\u5373\u53ef\u901a\u8fc7\u76d1\u7763\u65b9\u6cd5\uff08\u5982\u76ee\u6807\u6761\u4ef6\u884c\u4e3a\u514b\u9686\uff09\u5b9e\u73b0\u4f18\u5316\u3002", "conclusion": "CFGRL\u6210\u529f\u7ed3\u5408\u4e86\u5f3a\u5316\u5b66\u4e60\u548c\u751f\u6210\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u7b80\u5316\u4e86\u8bad\u7ec3\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u6269\u5c55\u4e86\u5e94\u7528\u573a\u666f\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60,\u6269\u6563\u6a21\u578b,\u7b56\u7565\u6539\u8fdb,CFGRL,\u79bb\u7ebf\u5b66\u4e60"}}
{"id": "2505.23759", "pdf": "https://arxiv.org/pdf/2505.23759", "abs": "https://arxiv.org/abs/2505.23759", "authors": ["Heekyung Lee", "Jiaxin Ge", "Tsung-Han Wu", "Minwoo Kang", "Trevor Darrell", "David M. Chan"], "title": "Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Rebus puzzles, visual riddles that encode language through imagery, spatial\narrangement, and symbolic substitution, pose a unique challenge to current\nvision-language models (VLMs). Unlike traditional image captioning or question\nanswering tasks, rebus solving requires multi-modal abstraction, symbolic\nreasoning, and a grasp of cultural, phonetic and linguistic puns. In this\npaper, we investigate the capacity of contemporary VLMs to interpret and solve\nrebus puzzles by constructing a hand-generated and annotated benchmark of\ndiverse English-language rebus puzzles, ranging from simple pictographic\nsubstitutions to spatially-dependent cues (\"head\" over \"heels\"). We analyze how\ndifferent VLMs perform, and our findings reveal that while VLMs exhibit some\nsurprising capabilities in decoding simple visual clues, they struggle\nsignificantly with tasks requiring abstract reasoning, lateral thinking, and\nunderstanding visual metaphors.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u89e3\u51b3\u753b\u8c1c\uff08rebus puzzles\uff09\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u80fd\u5904\u7406\u7b80\u5355\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u4f46\u5728\u9700\u8981\u62bd\u8c61\u63a8\u7406\u548c\u89c6\u89c9\u9690\u55bb\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u753b\u8c1c\u4f5c\u4e3a\u4e00\u79cd\u591a\u6a21\u6001\u8bed\u8a00\u7f16\u7801\u7684\u89c6\u89c9\u8c1c\u9898\uff0c\u5bf9\u73b0\u6709VLM\u7684\u62bd\u8c61\u63a8\u7406\u548c\u6587\u5316\u3001\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u63d0\u51fa\u4e86\u72ec\u7279\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30VLM\u5728\u6b64\u7c7b\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u624b\u5de5\u751f\u6210\u4e14\u591a\u6837\u5316\u7684\u753b\u8c1c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u4ece\u7b80\u5355\u56fe\u50cf\u66ff\u6362\u5230\u7a7a\u95f4\u4f9d\u8d56\u7ebf\u7d22\u7684\u8c1c\u9898\uff0c\u5206\u6790\u4e0d\u540cVLM\u7684\u8868\u73b0\u3002", "result": "VLMs\u80fd\u89e3\u7801\u7b80\u5355\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u4f46\u5728\u9700\u8981\u62bd\u8c61\u63a8\u7406\u3001\u6a2a\u5411\u601d\u7ef4\u548c\u89c6\u89c9\u9690\u55bb\u7406\u89e3\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u4e0d\u8db3\u3002", "conclusion": "\u5f53\u524dVLM\u5728\u5904\u7406\u590d\u6742\u753b\u8c1c\u65f6\u4ecd\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u591a\u6a21\u6001\u62bd\u8c61\u4e0e\u6587\u5316\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u753b\u8c1c, \u591a\u6a21\u6001\u63a8\u7406, \u62bd\u8c61\u63a8\u7406, \u89c6\u89c9\u9690\u55bb"}}
{"id": "2505.23145", "pdf": "https://arxiv.org/pdf/2505.23145", "abs": "https://arxiv.org/abs/2505.23145", "authors": ["Jeongsol Kim", "Yeobin Hong", "Jong Chul Ye"], "title": "FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent inversion-free, flow-based image editing methods such as FlowEdit\nleverages a pre-trained noise-to-image flow model such as Stable Diffusion 3,\nenabling text-driven manipulation by solving an ordinary differential equation\n(ODE). While the lack of exact latent inversion is a core advantage of these\nmethods, it often results in unstable editing trajectories and poor source\nconsistency. To address this limitation, we propose FlowAlign, a novel\ninversion-free flow-based framework for consistent image editing with\nprincipled trajectory control. FlowAlign introduces a flow-matching loss as a\nregularization mechanism to promote smoother and more stable trajectories\nduring the editing process. Notably, the flow-matching loss is shown to\nexplicitly balance semantic alignment with the edit prompt and structural\nconsistency with the source image along the trajectory. Furthermore, FlowAlign\nnaturally supports reverse editing by simply reversing the ODE trajectory,\nhighlighting the reversible and consistent nature of the transformation.\nExtensive experiments demonstrate that FlowAlign outperforms existing methods\nin both source preservation and editing controllability.", "AI": {"tldr": "FlowAlign\u901a\u8fc7\u5f15\u5165\u6d41\u5339\u914d\u635f\u5931\u4f5c\u4e3a\u6b63\u5219\u5316\u673a\u5236\uff0c\u89e3\u51b3\u4e86FlowEdit\u7b49\u65e0\u53cd\u8f6c\u6d41\u7f16\u8f91\u65b9\u6cd5\u4e2d\u8f68\u8ff9\u4e0d\u7a33\u5b9a\u548c\u6e90\u4e00\u81f4\u6027\u5dee\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8f91\u6548\u679c\u3002", "motivation": "\u9488\u5bf9FlowEdit\u7b49\u65e0\u53cd\u8f6c\u6d41\u7f16\u8f91\u65b9\u6cd5\u5728\u8f68\u8ff9\u63a7\u5236\u548c\u6e90\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3\uff0cFlowAlign\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u7a33\u5b9a\u3001\u4e00\u81f4\u7684\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\u3002", "method": "FlowAlign\u91c7\u7528\u6d41\u5339\u914d\u635f\u5931\u4f5c\u4e3a\u6b63\u5219\u5316\u673a\u5236\uff0c\u5e73\u8861\u8bed\u4e49\u5bf9\u9f50\u4e0e\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u652f\u6301\u53cd\u5411\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cFlowAlign\u5728\u6e90\u4fdd\u6301\u548c\u7f16\u8f91\u53ef\u63a7\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FlowAlign\u4e3a\u65e0\u53cd\u8f6c\u6d41\u7f16\u8f91\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9006\u3001\u4e00\u81f4\u4e14\u53ef\u63a7\u7684\u65b0\u6846\u67b6\u3002", "keywords": "FlowAlign, flow-based editing, inversion-free, image manipulation, ODE"}}
{"id": "2505.23459", "pdf": "https://arxiv.org/pdf/2505.23459", "abs": "https://arxiv.org/abs/2505.23459", "authors": ["Safwan Labbi", "Paul Mangold", "Daniil Tiapkin", "Eric Moulines"], "title": "On Global Convergence Rates for Federated Policy Gradient under Heterogeneous Environment", "categories": ["cs.LG"], "comment": "Preprint", "summary": "Ensuring convergence of policy gradient methods in federated reinforcement\nlearning (FRL) under environment heterogeneity remains a major challenge. In\nthis work, we first establish that heterogeneity, perhaps counter-intuitively,\ncan necessitate optimal policies to be non-deterministic or even time-varying,\neven in tabular environments. Subsequently, we prove global convergence results\nfor federated policy gradient (FedPG) algorithms employing local updates, under\na {\\L}ojasiewicz condition that holds only for each individual agent, in both\nentropy-regularized and non-regularized scenarios. Crucially, our theoretical\nanalysis shows that FedPG attains linear speed-up with respect to the number of\nagents, a property central to efficient federated learning. Leveraging insights\nfrom our theoretical findings, we introduce b-RS-FedPG, a novel policy gradient\nmethod that employs a carefully constructed softmax-inspired parameterization\ncoupled with an appropriate regularization scheme. We further demonstrate\nexplicit convergence rates for b-RS-FedPG toward near-optimal stationary\npolicies. Finally, we demonstrate that empirically both FedPG and b-RS-FedPG\nconsistently outperform federated Q-learning on heterogeneous settings.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7684\u6536\u655b\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5b-RS-FedPG\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u4e8e\u8054\u90a6Q\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u5728\u73af\u5883\u5f02\u8d28\u6027\u4e0b\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u6536\u655b\u6027\u96be\u9898\u3002", "method": "\u63d0\u51fa\u4e86Federated Policy Gradient (FedPG) \u65b9\u6cd5\u548c\u65b0\u578b\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5b-RS-FedPG\uff0c\u91c7\u7528\u5c40\u90e8\u66f4\u65b0\u548c\u9002\u5f53\u7684\u6b63\u5219\u5316\u65b9\u6848\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86FedPG\u7684\u5168\u5c40\u6536\u655b\u6027\uff0c\u5b9e\u9a8c\u663e\u793aFedPG\u548cb-RS-FedPG\u5728\u5f02\u8d28\u6027\u73af\u5883\u4e0b\u4f18\u4e8e\u8054\u90a6Q\u5b66\u4e60\u3002", "conclusion": "FedPG\u548cb-RS-FedPG\u5728\u5f02\u8d28\u6027\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u7ebf\u6027\u52a0\u901f\u7279\u6027\u3002", "keywords": "\u8054\u90a6\u5f3a\u5316\u5b66\u4e60, \u7b56\u7565\u68af\u5ea6, \u5f02\u8d28\u6027, \u6536\u655b\u6027"}}
{"id": "2505.23765", "pdf": "https://arxiv.org/pdf/2505.23765", "abs": "https://arxiv.org/abs/2505.23765", "authors": ["Wentao Zhang", "Woojeong Kim", "Yuntian Deng"], "title": "From Chat Logs to Collective Insights: Aggregative Question Answering", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Conversational agents powered by large language models (LLMs) are rapidly\nbecoming integral to our daily interactions, generating unprecedented amounts\nof conversational data. Such datasets offer a powerful lens into societal\ninterests, trending topics, and collective concerns. Yet, existing approaches\ntypically treat these interactions as independent and miss critical insights\nthat could emerge from aggregating and reasoning across large-scale\nconversation logs. In this paper, we introduce Aggregative Question Answering,\na novel task requiring models to reason explicitly over thousands of\nuser-chatbot interactions to answer aggregative queries, such as identifying\nemerging concerns among specific demographics. To enable research in this\ndirection, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative\nquestions derived from 182,330 real-world chatbot conversations. Experiments\nshow that existing methods either struggle to reason effectively or incur\nprohibitive computational costs, underscoring the need for new approaches\ncapable of extracting collective insights from large-scale conversational data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u805a\u5408\u5f0f\u95ee\u7b54\uff08Aggregative Question Answering\uff09\u8fd9\u4e00\u65b0\u4efb\u52a1\uff0c\u65e8\u5728\u901a\u8fc7\u5206\u6790\u5927\u89c4\u6a21\u7528\u6237\u4e0e\u804a\u5929\u673a\u5668\u4eba\u4ea4\u4e92\u6570\u636e\u6765\u56de\u7b54\u805a\u5408\u5f0f\u67e5\u8be2\uff0c\u5e76\u63d0\u51fa\u4e86WildChat-AQA\u57fa\u51c6\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u6b64\u7c7b\u4efb\u52a1\uff0c\u7a81\u51fa\u4e86\u65b0\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u5c06\u7528\u6237\u4e0e\u804a\u5929\u673a\u5668\u4eba\u7684\u4ea4\u4e92\u89c6\u4e3a\u72ec\u7acb\u7684\uff0c\u7f3a\u4e4f\u5bf9\u5927\u89c4\u6a21\u5bf9\u8bdd\u6570\u636e\u7684\u805a\u5408\u5206\u6790\uff0c\u4ece\u800c\u9519\u5931\u6f5c\u5728\u7684\u96c6\u4f53\u6d1e\u5bdf\u529b\u3002", "method": "\u63d0\u51fa\u805a\u5408\u5f0f\u95ee\u7b54\u4efb\u52a1\uff0c\u5e76\u6784\u5efaWildChat-AQA\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b6,027\u4e2a\u805a\u5408\u95ee\u9898\uff0c\u6765\u6e90\u4e8e182,330\u6761\u771f\u5b9e\u804a\u5929\u8bb0\u5f55\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u65b0\u65b9\u6cd5\uff0c\u4ee5\u4ece\u5927\u89c4\u6a21\u5bf9\u8bdd\u6570\u636e\u4e2d\u63d0\u53d6\u96c6\u4f53\u6d1e\u5bdf\u3002", "keywords": "\u805a\u5408\u5f0f\u95ee\u7b54\u3001\u5927\u89c4\u6a21\u5bf9\u8bdd\u6570\u636e\u3001LLM\u3001WildChat-AQA\u3001\u96c6\u4f53\u6d1e\u5bdf"}}
{"id": "2505.23161", "pdf": "https://arxiv.org/pdf/2505.23161", "abs": "https://arxiv.org/abs/2505.23161", "authors": ["Antonio D'Orazio", "Maria Rosaria Briglia", "Donato Crisostomi", "Dario Loi", "Emanuele Rodol\u00e0", "Iacopo Masi"], "title": "Implicit Inversion turns CLIP into a Decoder", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "CLIP is a discriminative model trained to align images and text in a shared\nembedding space. Due to its multimodal structure, it serves as the backbone of\nmany generative pipelines, where a decoder is trained to map from the shared\nspace back to images. In this work, we show that image synthesis is\nnevertheless possible using CLIP alone -- without any decoder, training, or\nfine-tuning. Our approach optimizes a frequency-aware implicit neural\nrepresentation that encourages coarse-to-fine generation by stratifying\nfrequencies across network layers. To stabilize this inverse mapping, we\nintroduce adversarially robust initialization, a lightweight Orthogonal\nProcrustes projection to align local text and image embeddings, and a blending\nloss that anchors outputs to natural image statistics. Without altering CLIP's\nweights, this framework unlocks capabilities such as text-to-image generation,\nstyle transfer, and image reconstruction. These findings suggest that\ndiscriminative models may hold untapped generative potential, hidden in plain\nsight.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u89e3\u7801\u5668\u3001\u8bad\u7ec3\u6216\u5fae\u8c03\u7684CLIP\u5355\u72ec\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u9891\u7387\u611f\u77e5\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u5b9e\u73b0\u751f\u6210\u3002", "motivation": "\u63a2\u7d22\u5224\u522b\u5f0f\u6a21\u578b\uff08\u5982CLIP\uff09\u6f5c\u5728\u7684\u751f\u6210\u80fd\u529b\uff0c\u63ed\u793a\u5176\u9690\u85cf\u7684\u751f\u6210\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u9891\u7387\u611f\u77e5\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff0c\u7ed3\u5408\u5bf9\u6297\u6027\u9c81\u68d2\u521d\u59cb\u5316\u3001\u6b63\u4ea4\u6295\u5f71\u548c\u6df7\u5408\u635f\u5931\uff0c\u5b9e\u73b0\u56fe\u50cf\u5408\u6210\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3001\u98ce\u683c\u8fc1\u79fb\u548c\u56fe\u50cf\u91cd\u5efa\uff0c\u65e0\u9700\u4fee\u6539CLIP\u6743\u91cd\u3002", "conclusion": "\u5224\u522b\u5f0f\u6a21\u578b\u53ef\u80fd\u5177\u5907\u672a\u88ab\u5f00\u53d1\u7684\u751f\u6210\u6f5c\u529b\uff0c\u4e3a\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "CLIP, \u56fe\u50cf\u5408\u6210, \u9690\u5f0f\u795e\u7ecf\u8868\u793a, \u5224\u522b\u5f0f\u6a21\u578b, \u751f\u6210\u6f5c\u529b"}}
{"id": "2505.23470", "pdf": "https://arxiv.org/pdf/2505.23470", "abs": "https://arxiv.org/abs/2505.23470", "authors": ["Chenjie Li", "Amir Gilad", "Boris Glavic", "Zhengjie Miao", "Sudeepa Roy"], "title": "Refining Labeling Functions with Limited Labeled Data", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": "techreport", "summary": "Programmatic weak supervision (PWS) significantly reduces human effort for\nlabeling data by combining the outputs of user-provided labeling functions\n(LFs) on unlabeled datapoints. However, the quality of the generated labels\ndepends directly on the accuracy of the LFs. In this work, we study the problem\nof fixing LFs based on a small set of labeled examples. Towards this goal, we\ndevelop novel techniques for repairing a set of LFs by minimally changing their\nresults on the labeled examples such that the fixed LFs ensure that (i) there\nis sufficient evidence for the correct label of each labeled datapoint and (ii)\nthe accuracy of each repaired LF is sufficiently high. We model LFs as\nconditional rules which enables us to refine them, i.e., to selectively change\ntheir output for some inputs. We demonstrate experimentally that our system\nimproves the quality of LFs based on surprisingly small sets of labeled\ndatapoints.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u4fee\u590d\u6807\u7b7e\u51fd\u6570\uff08LFs\uff09\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6700\u5c0f\u5316\u4fee\u6539LFs\u8f93\u51fa\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u6807\u7b7e\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7a0b\u5e8f\u5316\u5f31\u76d1\u7763\uff08PWS\uff09\u4f9d\u8d56\u7528\u6237\u63d0\u4f9b\u7684LFs\u751f\u6210\u6807\u7b7e\uff0c\u4f46LFs\u7684\u51c6\u786e\u6027\u76f4\u63a5\u5f71\u54cd\u6807\u7b7e\u8d28\u91cf\u3002\u5f53\u4ec5\u6709\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u65f6\uff0c\u5982\u4f55\u9ad8\u6548\u4fee\u590dLFs\u662f\u4e00\u5927\u6311\u6218\u3002", "method": "\u5c06LFs\u5efa\u6a21\u4e3a\u6761\u4ef6\u89c4\u5219\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u4fee\u6539\u8f93\u5165\u8f93\u51fa\u5bf9\u6765\u6700\u5c0f\u5316\u53d8\u66f4\uff0c\u786e\u4fdd\u4fee\u590d\u540e\u7684LFs\u65e2\u652f\u6301\u6807\u6ce8\u6570\u636e\u7684\u6b63\u786e\u6807\u7b7e\uff0c\u53c8\u4fdd\u6301\u8db3\u591f\u9ad8\u7684\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u6781\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u4e0b\u663e\u8457\u63d0\u5347LFs\u7684\u8d28\u91cf\u3002", "conclusion": "\u901a\u8fc7\u6761\u4ef6\u89c4\u5219\u7684\u7ec6\u7c92\u5ea6\u4fee\u590d\uff0c\u5b9e\u73b0\u4e86LFs\u5728\u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u4e0b\u7684\u9ad8\u6548\u4f18\u5316\uff0c\u4e3a\u5f31\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u7a0b\u5e8f\u5316\u5f31\u76d1\u7763, \u6807\u7b7e\u51fd\u6570\u4fee\u590d, \u6761\u4ef6\u89c4\u5219, \u5c0f\u6837\u672c\u6807\u6ce8"}}
{"id": "2505.22654", "pdf": "https://arxiv.org/pdf/2505.22654", "abs": "https://arxiv.org/abs/2505.22654", "authors": ["Ce Zhang", "Kaixin Ma", "Tianqing Fang", "Wenhao Yu", "Hongming Zhang", "Zhisong Zhang", "Yaqi Xie", "Katia Sycara", "Haitao Mi", "Dong Yu"], "title": "VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent Large Vision-Language Models (LVLMs) have advanced multi-modal\nunderstanding by incorporating finer-grained visual perception and encoding.\nHowever, such methods incur significant computational costs due to longer\nvisual token sequences, posing challenges for real-time deployment. To mitigate\nthis, prior studies have explored pruning unimportant visual tokens either at\nthe output layer of the visual encoder or at the early layers of the language\nmodel. In this work, we revisit these design choices and reassess their\neffectiveness through comprehensive empirical studies of how visual tokens are\nprocessed throughout the visual encoding and language decoding stages. Guided\nby these insights, we propose VScan, a two-stage visual token reduction\nframework that addresses token redundancy by: (1) integrating complementary\nglobal and local scans with token merging during visual encoding, and (2)\nintroducing pruning at intermediate layers of the language model. Extensive\nexperimental results across four LVLMs validate the effectiveness of VScan in\naccelerating inference and demonstrate its superior performance over current\nstate-of-the-arts on sixteen benchmarks. Notably, when applied to\nLLaVA-NeXT-7B, VScan achieves a 2.91$\\times$ speedup in prefilling and a\n10$\\times$ reduction in FLOPs, while retaining 95.4% of the original\nperformance.", "AI": {"tldr": "VScan\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u89c6\u89c9\u4ee4\u724c\u7f29\u51cf\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5168\u5c40\u548c\u5c40\u90e8\u626b\u63cf\u53ca\u4e2d\u95f4\u5c42\u526a\u679d\uff0c\u663e\u8457\u52a0\u901f\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u56e0\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9\u611f\u77e5\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5b9e\u65f6\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\u5728\u4e0d\u663e\u8457\u635f\u5931\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51faVScan\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u7f16\u7801\u9636\u6bb5\u7684\u5168\u5c40\u4e0e\u5c40\u90e8\u626b\u63cf\u4ee4\u724c\u5408\u5e76\uff0c\u4ee5\u53ca\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u95f4\u5c42\u5f15\u5165\u526a\u679d\u3002", "result": "\u5728\u56db\u4e2aLVLM\u4e0a\u9a8c\u8bc1\uff0cVScan\u5b9e\u73b0\u4e86\u63a8\u7406\u52a0\u901f\uff082.91\u500d\u9884\u586b\u5145\u52a0\u901f\uff0c10\u500dFLOPs\u51cf\u5c11\uff09\uff0c\u5e76\u572816\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4fdd\u755995.4%\u7684\u539f\u59cb\u6027\u80fd\u3002", "conclusion": "VScan\u901a\u8fc7\u4e24\u9636\u6bb5\u4ee4\u724c\u7f29\u51cf\u663e\u8457\u63d0\u5347LVLM\u7684\u63a8\u7406\u6548\u7387\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u4ee4\u724c\u7f29\u51cf\u3001\u63a8\u7406\u52a0\u901f\u3001VScan\u3001\u8ba1\u7b97\u6548\u7387"}}
{"id": "2505.23489", "pdf": "https://arxiv.org/pdf/2505.23489", "abs": "https://arxiv.org/abs/2505.23489", "authors": ["Ildus Sadrtdinov", "Ivan Klimov", "Ekaterina Lobacheva", "Dmitry Vetrov"], "title": "SGD as Free Energy Minimization: A Thermodynamic View on Neural Network Training", "categories": ["cs.LG"], "comment": "First two authors contributed equally", "summary": "We present a thermodynamic interpretation of the stationary behavior of\nstochastic gradient descent (SGD) under fixed learning rates (LRs) in neural\nnetwork training. We show that SGD implicitly minimizes a free energy function\n$F=U-TS$, balancing training loss $U$ and the entropy of the weights\ndistribution $S$, with temperature $T$ determined by the LR. This perspective\noffers a new lens on why high LRs prevent training from converging to the loss\nminima and how different LRs lead to stabilization at different loss levels. We\nempirically validate the free energy framework on both underparameterized (UP)\nand overparameterized (OP) models. UP models consistently follow free energy\nminimization, with temperature increasing monotonically with LR, while for OP\nmodels, the temperature effectively drops to zero at low LRs, causing SGD to\nminimize the loss directly and converge to an optimum. We attribute this\nmismatch to differences in the signal-to-noise ratio of stochastic gradients\nnear optima, supported by both a toy example and neural network experiments.", "AI": {"tldr": "\u901a\u8fc7\u70ed\u529b\u5b66\u6846\u67b6\u89e3\u91caSGD\u56fa\u5b9a\u5b66\u4e60\u7387\u4e0b\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u5176\u9690\u542b\u6700\u5c0f\u5316\u81ea\u7531\u80fd\uff0c\u5e73\u8861\u8bad\u7ec3\u635f\u5931\u548c\u6743\u91cd\u5206\u5e03\u71b5\uff0c\u4e14\u5b66\u4e60\u7387\u51b3\u5b9a\u2018\u6e29\u5ea6\u2019\u3002", "motivation": "\u63ed\u793aSGD\u5728\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684\u884c\u4e3a\u673a\u5236\uff0c\u7279\u522b\u662f\u9ad8\u5b66\u4e60\u7387\u963b\u788d\u6536\u655b\u7684\u539f\u56e0\uff0c\u4ee5\u53ca\u5b66\u4e60\u7387\u5982\u4f55\u5f71\u54cd\u635f\u5931\u51fd\u6570\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u70ed\u529b\u5b66\u89c6\u89d2\uff08\u81ea\u7531\u80fd\u6700\u5c0f\u5316\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\uff0c\u5305\u62ec\u6b20\u53c2\u6570\u5316\uff08UP\uff09\u548c\u8fc7\u53c2\u6570\u5316\uff08OP\uff09\u6a21\u578b\u7684\u884c\u4e3a\u5dee\u5f02\u3002", "result": "UP\u6a21\u578b\u4e2d\u2018\u6e29\u5ea6\u2019\u968f\u5b66\u4e60\u7387\u5355\u8c03\u4e0a\u5347\uff0c\u9075\u5faa\u81ea\u7531\u80fd\u6700\u5c0f\u5316\uff1bOP\u6a21\u578b\u5728\u4f4e\u5b66\u4e60\u7387\u65f6\u2018\u6e29\u5ea6\u2019\u8d8b\u96f6\uff0c\u76f4\u63a5\u6700\u5c0f\u5316\u635f\u5931\u81f3\u6700\u4f18\u3002", "conclusion": "SGD\u884c\u4e3a\u53ef\u901a\u8fc7\u81ea\u7531\u80fd\u6846\u67b6\u7edf\u4e00\u89e3\u91ca\uff0cUP\u4e0eOP\u6a21\u578b\u7684\u5dee\u5f02\u6e90\u4e8e\u968f\u673a\u68af\u5ea6\u4fe1\u566a\u6bd4\u7684\u4e0d\u540c\u3002", "keywords": "SGD, \u70ed\u529b\u5b66, \u81ea\u7531\u80fd, \u5b66\u4e60\u7387, \u795e\u7ecf\u7f51\u7edc"}}
{"id": "2505.23496", "pdf": "https://arxiv.org/pdf/2505.23496", "abs": "https://arxiv.org/abs/2505.23496", "authors": ["Sabina J. Sloman", "Michele Caprio", "Samuel Kaski"], "title": "Epistemic Errors of Imperfect Multitask Learners When Distributions Shift", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "When data are noisy, a statistical learner's goal is to resolve epistemic\nuncertainty about the data it will encounter at test-time, i.e., to identify\nthe distribution of test (target) data. Many real-world learning settings\nintroduce sources of epistemic uncertainty that can not be resolved on the\nbasis of training (source) data alone: The source data may arise from multiple\ntasks (multitask learning), the target data may differ systematically from the\nsource data tasks (distribution shift), and/or the learner may not arrive at an\naccurate characterization of the source data (imperfect learning). We introduce\na principled definition of epistemic error, and provide a generic,\ndecompositional epistemic error bound. Our error bound is the first to (i)\nconsider epistemic error specifically, (ii) accommodate all the sources of\nepistemic uncertainty above, and (iii) separately attribute the error to each\nof multiple aspects of the learning procedure and environment. As corollaries\nof the generic result, we provide (i) epistemic error bounds specialized to the\nsettings of Bayesian transfer learning and distribution shift within\n$\\epsilon$-neighborhoods, and (ii) a set of corresponding generalization\nbounds. Finally, we provide a novel definition of negative transfer, and\nvalidate its insights in a synthetic experimental setting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u8ba4\u77e5\u8bef\u5dee\u5b9a\u4e49\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u3001\u53ef\u5206\u89e3\u7684\u8ba4\u77e5\u8bef\u5dee\u754c\u9650\u3002\u8be5\u754c\u9650\u9996\u6b21\uff08i\uff09\u4e13\u95e8\u8003\u8651\u8ba4\u77e5\u8bef\u5dee\uff0c\uff08ii\uff09\u6db5\u76d6\u591a\u79cd\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\uff0c\uff08iii\uff09\u5c06\u8bef\u5dee\u5f52\u56e0\u4e8e\u5b66\u4e60\u8fc7\u7a0b\u548c\u73af\u5883\u7684\u4e0d\u540c\u65b9\u9762\u3002", "motivation": "\u89e3\u51b3\u566a\u58f0\u6570\u636e\u4e0b\u7edf\u8ba1\u5b66\u4e60\u8005\u5728\u6d4b\u8bd5\u65f6\u9762\u4e34\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5305\u62ec\u591a\u4efb\u52a1\u5b66\u4e60\u3001\u5206\u5e03\u504f\u79fb\u548c\u4e0d\u5b8c\u5584\u5b66\u4e60\u7b49\u73b0\u5b9e\u573a\u666f\u3002", "method": "\u5f15\u5165\u8ba4\u77e5\u8bef\u5dee\u7684\u660e\u786e\u5b9a\u4e49\uff0c\u63d0\u51fa\u901a\u7528\u7684\u5206\u89e3\u5f0f\u8ba4\u77e5\u8bef\u5dee\u754c\u9650\uff0c\u5e76\u9488\u5bf9\u8d1d\u53f6\u65af\u8fc1\u79fb\u5b66\u4e60\u548c\u5206\u5e03\u504f\u79fb\u573a\u666f\u8fdb\u884c\u7279\u5316\u5206\u6790\u3002", "result": "\u63d0\u51fa\u4e86\u9996\u4e2a\u6db5\u76d6\u591a\u79cd\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u7684\u8bef\u5dee\u754c\u9650\uff0c\u5e76\u5728\u5408\u6210\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u8d1f\u8fc1\u79fb\u7684\u65b0\u5b9a\u4e49\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u8ba4\u77e5\u8bef\u5dee\u548c\u8d1f\u8fc1\u79fb\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u9002\u7528\u4e8e\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u5206\u5e03\u504f\u79fb\u7b49\u590d\u6742\u573a\u666f\u3002", "keywords": "\u8ba4\u77e5\u8bef\u5dee\u3001\u591a\u4efb\u52a1\u5b66\u4e60\u3001\u5206\u5e03\u504f\u79fb\u3001\u8d1d\u53f6\u65af\u8fc1\u79fb\u5b66\u4e60\u3001\u8d1f\u8fc1\u79fb"}}
{"id": "2505.23506", "pdf": "https://arxiv.org/pdf/2505.23506", "abs": "https://arxiv.org/abs/2505.23506", "authors": ["Sebasti\u00e1n Jim\u00e9nez", "Mira J\u00fcrgens", "Willem Waegeman"], "title": "Why Machine Learning Models Fail to Fully Capture Epistemic Uncertainty", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "In recent years various supervised learning methods that disentangle\naleatoric and epistemic uncertainty based on second-order distributions have\nbeen proposed. We argue that these methods fail to capture critical components\nof epistemic uncertainty, particularly due to the often-neglected component of\nmodel bias. To show this, we make use of a more fine-grained taxonomy of\nepistemic uncertainty sources in machine learning models, and analyse how the\nclassical bias-variance decomposition of the expected prediction error can be\ndecomposed into different parts reflecting these uncertainties. By using a\nsimulation-based evaluation protocol which encompasses epistemic uncertainty\ndue to both procedural- and data-driven uncertainty components, we illustrate\nthat current methods rarely capture the full spectrum of epistemic uncertainty.\nThrough theoretical insights and synthetic experiments, we show that high model\nbias can lead to misleadingly low estimates of epistemic uncertainty, and\ncommon second-order uncertainty quantification methods systematically blur\nbias-induced errors into aleatoric estimates, thereby underrepresenting\nepistemic uncertainty. Our findings underscore that meaningful aleatoric\nestimates are feasible only if all relevant sources of epistemic uncertainty\nare properly represented.", "AI": {"tldr": "\u5f53\u524d\u57fa\u4e8e\u4e8c\u9636\u5206\u5e03\u7684\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6355\u6349\u6a21\u578b\u504f\u5dee\u7b49\u5173\u952e\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u6210\u5206\uff0c\u5bfc\u81f4\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u88ab\u4f4e\u4f30\u3002", "motivation": "\u63ed\u793a\u73b0\u6709\u65b9\u6cd5\u5728\u91cf\u5316\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u65f6\u5ffd\u7565\u6a21\u578b\u504f\u5dee\u7684\u95ee\u9898\uff0c\u5f3a\u8c03\u5168\u9762\u8bc4\u4f30\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u7684\u91cd\u8981\u6027\u3002", "method": "\u901a\u8fc7\u66f4\u7ec6\u7c92\u5ea6\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u5206\u7c7b\u548c\u504f\u5dee-\u65b9\u5dee\u5206\u89e3\uff0c\u7ed3\u5408\u4eff\u771f\u5b9e\u9a8c\u8bc4\u4f30\u73b0\u6709\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u9ad8\u6a21\u578b\u504f\u5dee\u4f1a\u8bef\u5bfc\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u5c06\u504f\u5dee\u76f8\u5173\u9519\u8bef\u5f52\u4e3a\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u53ea\u6709\u5728\u5145\u5206\u8868\u793a\u6240\u6709\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u540e\uff0c\u624d\u80fd\u83b7\u5f97\u6709\u610f\u4e49\u7684\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "keywords": "\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3001\u6a21\u578b\u504f\u5dee\u3001\u4e8c\u9636\u5206\u5e03\u3001\u504f\u5dee-\u65b9\u5dee\u5206\u89e3\u3001\u4eff\u771f\u5b9e\u9a8c"}}
{"id": "2505.22793", "pdf": "https://arxiv.org/pdf/2505.22793", "abs": "https://arxiv.org/abs/2505.22793", "authors": ["Srishti Yadav", "Lauren Tilton", "Maria Antoniak", "Taylor Arnold", "Jiaang Li", "Siddhesh Milind Pawar", "Antonia Karamolegkou", "Stella Frank", "Zhaochong An", "Negar Rostamzadeh", "Daniel Hershcovich", "Serge Belongie", "Ekaterina Shutova"], "title": "Cultural Evaluations of Vision-Language Models Have a Lot to Learn from Cultural Theory", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Modern vision-language models (VLMs) often fail at cultural competency\nevaluations and benchmarks. Given the diversity of applications built upon\nVLMs, there is renewed interest in understanding how they encode cultural\nnuances. While individual aspects of this problem have been studied, we still\nlack a comprehensive framework for systematically identifying and annotating\nthe nuanced cultural dimensions present in images for VLMs. This position paper\nargues that foundational methodologies from visual culture studies (cultural\nstudies, semiotics, and visual studies) are necessary for cultural analysis of\nimages. Building upon this review, we propose a set of five frameworks,\ncorresponding to cultural dimensions, that must be considered for a more\ncomplete analysis of the cultural competencies of VLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u6587\u5316\u80fd\u529b\u8bc4\u4f30\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u547c\u5401\u5229\u7528\u89c6\u89c9\u6587\u5316\u7814\u7a76\u65b9\u6cd5\u6784\u5efa\u7cfb\u7edf\u6027\u6846\u67b6\u4ee5\u5206\u6790\u56fe\u50cf\u4e2d\u7684\u6587\u5316\u7ef4\u5ea6\u3002", "motivation": "\u5f53\u524dVLM\u7f3a\u4e4f\u5bf9\u6587\u5316\u7ec6\u5fae\u5dee\u5f02\u7684\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u5e94\u7528\u591a\u6837\u6027\u7684\u589e\u52a0\u4f7f\u5f97\u7406\u89e3\u5176\u6587\u5316\u7f16\u7801\u80fd\u529b\u53d8\u5f97\u8feb\u5207\u3002", "method": "\u7ed3\u5408\u6587\u5316\u7814\u7a76\u3001\u7b26\u53f7\u5b66\u548c\u89c6\u89c9\u7814\u7a76\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63d0\u51fa\u4e94\u79cd\u6587\u5316\u7ef4\u5ea6\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u5957\u7cfb\u7edf\u6027\u5206\u6790VLM\u6587\u5316\u80fd\u529b\u7684\u6846\u67b6\uff0c\u6db5\u76d6\u4e94\u4e2a\u5173\u952e\u6587\u5316\u7ef4\u5ea6\u3002", "conclusion": "\u9700\u8981\u4ece\u89c6\u89c9\u6587\u5316\u7814\u7a76\u7684\u89d2\u5ea6\u6784\u5efa\u66f4\u5168\u9762\u7684\u5206\u6790\u6846\u67b6\uff0c\u4ee5\u63d0\u5347VLM\u7684\u6587\u5316\u80fd\u529b\u8bc4\u4f30\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u6587\u5316\u80fd\u529b, \u89c6\u89c9\u6587\u5316\u7814\u7a76, \u7b26\u53f7\u5b66, \u6587\u5316\u7ef4\u5ea6"}}
{"id": "2505.23520", "pdf": "https://arxiv.org/pdf/2505.23520", "abs": "https://arxiv.org/abs/2505.23520", "authors": ["Yu Zhang", "Dong Guo", "Fang Wu", "Guoliang Zhu", "Dian Ding", "Yiming Zhang"], "title": "AnchorAttention: Difference-Aware Sparse Attention with Stripe Granularity", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates.", "AI": {"tldr": "AnchorAttention\u662f\u4e00\u79cd\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u66f4\u7ec6\u7c92\u5ea6\u7684\u6761\u7eb9\u7a00\u758f\u8bc6\u522b\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u9002\u5e94\uff0c\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9884\u586b\u5145\u9636\u6bb5\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u52a8\u6001\u6a21\u5f0f\u5339\u914d\u548c\u5757\u7a00\u758f\u65b9\u6cd5\u56e0\u4f9d\u8d56\u5c40\u90e8\u4fe1\u606f\u548c\u7c97\u7c92\u5ea6\u5757\u5bfc\u81f4\u7684\u8ba1\u7b97\u6548\u7387\u4e0e\u51c6\u786e\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faAnchorAttention\uff0c\u5305\u542b\u57fa\u4e8e\u6a21\u5f0f\u7684\u951a\u8ba1\u7b97\u3001\u5dee\u5f02\u611f\u77e5\u6761\u7eb9\u7a00\u758f\u8bc6\u522b\u548c\u7ec6\u7c92\u5ea6\u7a00\u758f\u8ba1\u7b97\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\u3002", "result": "\u5728128k\u6587\u672c\u957f\u5ea6\u4e0b\u5b9e\u73b01.44\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u66f4\u9ad8\u7684\u53ec\u56de\u7387\u3002", "conclusion": "AnchorAttention\u5728\u7ec6\u7c92\u5ea6\u7a00\u758f\u7b56\u7565\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u6ce8\u610f\u529b\u673a\u5236,\u7a00\u758f\u8ba1\u7b97,\u5168\u5c40\u4e0a\u4e0b\u6587,\u8ba1\u7b97\u6548\u7387"}}
{"id": "2505.23523", "pdf": "https://arxiv.org/pdf/2505.23523", "abs": "https://arxiv.org/abs/2505.23523", "authors": ["Arjun Devraj", "Eric Ding", "Abhishek Vijaya Kumar", "Robert Kleinberg", "Rachee Singh"], "title": "Accelerating AllReduce with a Persistent Straggler", "categories": ["cs.LG", "cs.DC"], "comment": "23 pages, 11 figures", "summary": "Distributed machine learning workloads use data and tensor parallelism for\ntraining and inference, both of which rely on the AllReduce collective to\nsynchronize gradients or activations. However, bulk-synchronous AllReduce\nalgorithms can be delayed by a persistent straggler that is slower to reach the\nsynchronization barrier required to begin the collective. To address this\nchallenge, we propose StragglAR: an AllReduce algorithm that accelerates\ndistributed training and inference in the presence of persistent stragglers.\nStragglAR implements a ReduceScatter among the remaining GPUs during the\nstraggler-induced delay, and then executes a novel collective algorithm to\ncomplete the AllReduce once the straggler reaches the synchronization barrier.\nStragglAR achieves a 2x theoretical speedup over popular bandwidth-efficient\nAllReduce algorithms (e.g., Ring) for large GPU clusters with persistent\nstragglers. On an 8-GPU server, our implementation of StragglAR yields a 22%\nspeedup over state-of-the-art AllReduce algorithms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86StragglAR\u7b97\u6cd5\uff0c\u901a\u8fc7\u89e3\u51b3\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u4e2d\u6301\u4e45\u6027\u6162\u8282\u70b9\u5bfc\u81f4\u7684AllReduce\u540c\u6b65\u5ef6\u8fdf\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684AllReduce\u7b97\u6cd5\u5728\u9762\u5bf9\u6301\u4e45\u6027\u6162\u8282\u70b9\u65f6\u4f1a\u5bfc\u81f4\u540c\u6b65\u5ef6\u8fdf\uff0c\u5f71\u54cd\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u7684\u6548\u7387\u3002", "method": "StragglAR\u7b97\u6cd5\u5728\u6162\u8282\u70b9\u5ef6\u8fdf\u671f\u95f4\u6267\u884cReduceScatter\u64cd\u4f5c\uff0c\u968f\u540e\u91c7\u7528\u65b0\u578b\u96c6\u4f53\u7b97\u6cd5\u5b8c\u6210AllReduce\u3002", "result": "\u7406\u8bba\u901f\u5ea6\u63d0\u53472\u500d\uff0c8-GPU\u670d\u52a1\u5668\u4e0a\u5b9e\u73b022%\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "StragglAR\u6709\u6548\u89e3\u51b3\u4e86\u6301\u4e45\u6027\u6162\u8282\u70b9\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u6027\u80fd\u3002", "keywords": "\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60, AllReduce, \u6301\u4e45\u6027\u6162\u8282\u70b9, StragglAR, \u8bad\u7ec3\u52a0\u901f"}}
{"id": "2505.22863", "pdf": "https://arxiv.org/pdf/2505.22863", "abs": "https://arxiv.org/abs/2505.22863", "authors": ["Yupei Li", "Shuaijie Shao", "Manuel Milling", "Bj\u00f6rn W. Schuller"], "title": "Large Language Models for Depression Recognition in Spoken Language Integrating Psychological Knowledge", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Depression is a growing concern gaining attention in both public discourse\nand AI research. While deep neural networks (DNNs) have been used for\nrecognition, they still lack real-world effectiveness. Large language models\n(LLMs) show strong potential but require domain-specific fine-tuning and\nstruggle with non-textual cues. Since depression is often expressed through\nvocal tone and behaviour rather than explicit text, relying on language alone\nis insufficient. Diagnostic accuracy also suffers without incorporating\npsychological expertise. To address these limitations, we present, to the best\nof our knowledge, the first application of LLMs to multimodal depression\ndetection using the DAIC-WOZ dataset. We extract the audio features using the\npre-trained model Wav2Vec, and mapped it to text-based LLMs for further\nprocessing. We also propose a novel strategy for incorporating psychological\nknowledge into LLMs to enhance diagnostic performance, specifically using a\nquestion and answer set to grant authorised knowledge to LLMs. Our approach\nyields a notable improvement in both Mean Absolute Error (MAE) and Root Mean\nSquare Error (RMSE) compared to a base score proposed by the related original\npaper. The codes are available at\nhttps://github.com/myxp-lyp/Depression-detection.git", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u97f3\u9891\u7279\u5f81\u548c\u5fc3\u7406\u5b66\u77e5\u8bc6\u7684LLM\u591a\u6a21\u6001\u6291\u90c1\u75c7\u68c0\u6d4b\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86MAE\u548cRMSE\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709DNN\u548cLLM\u5728\u6291\u90c1\u75c7\u8bc6\u522b\u4e2d\u5b58\u5728\u5c40\u9650\uff0c\u5982\u7f3a\u4e4f\u975e\u6587\u672c\u7ebf\u7d22\u548c\u5fc3\u7406\u5b66\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u4f7f\u7528Wav2Vec\u63d0\u53d6\u97f3\u9891\u7279\u5f81\uff0c\u7ed3\u5408\u6587\u672cLLM\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u95ee\u7b54\u5f62\u5f0f\u5f15\u5165\u5fc3\u7406\u5b66\u77e5\u8bc6\u3002", "result": "\u5728DAIC-WOZ\u6570\u636e\u96c6\u4e0a\uff0cMAE\u548cRMSE\u6bd4\u57fa\u7ebf\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u591a\u6a21\u6001\u7ed3\u5408\u5fc3\u7406\u5b66\u77e5\u8bc6\u80fd\u6709\u6548\u63d0\u5347\u6291\u90c1\u75c7\u68c0\u6d4b\u6027\u80fd\u3002", "keywords": "\u6291\u90c1\u75c7\u68c0\u6d4b,\u591a\u6a21\u6001,LLM,Wav2Vec,\u5fc3\u7406\u5b66\u77e5\u8bc6"}}
{"id": "2505.23192", "pdf": "https://arxiv.org/pdf/2505.23192", "abs": "https://arxiv.org/abs/2505.23192", "authors": ["Run Hao", "Peng Ying"], "title": "Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "9 pages", "summary": "The rise of text-to-image (T2I) models has enabled the synthesis of\nphotorealistic human portraits, raising serious concerns about identity misuse\nand the robustness of AIGC detectors. In this work, we propose an automated\nadversarial prompt generation framework that leverages a grammar tree structure\nand a variant of the Monte Carlo tree search algorithm to systematically\nexplore the semantic prompt space. Our method generates diverse, controllable\nprompts that consistently evade both open-source and commercial AIGC detectors.\nExtensive experiments across multiple T2I models validate its effectiveness,\nand the approach ranked first in a real-world adversarial AIGC detection\ncompetition. Beyond attack scenarios, our method can also be used to construct\nhigh-quality adversarial datasets, providing valuable resources for training\nand evaluating more robust AIGC detection and defense systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u5316\u5bf9\u6297\u63d0\u793a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u6cd5\u6811\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u751f\u6210\u591a\u6837\u5316\u3001\u53ef\u63a7\u7684\u63d0\u793a\uff0c\u80fd\u6709\u6548\u7ed5\u8fc7AIGC\u68c0\u6d4b\u5668\u3002", "motivation": "\u968f\u7740\u6587\u672c\u751f\u6210\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u7684\u666e\u53ca\uff0c\u771f\u5b9e\u8096\u50cf\u5408\u6210\u5f15\u53d1\u8eab\u4efd\u6ee5\u7528\u548cAIGC\u68c0\u6d4b\u5668\u9c81\u68d2\u6027\u62c5\u5fe7\u3002", "method": "\u5229\u7528\u8bed\u6cd5\u6811\u7ed3\u6784\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7b97\u6cd5\u53d8\u4f53\uff0c\u7cfb\u7edf\u63a2\u7d22\u8bed\u4e49\u63d0\u793a\u7a7a\u95f4\uff0c\u751f\u6210\u5bf9\u6297\u63d0\u793a\u3002", "result": "\u6846\u67b6\u5728\u591a\u4e2aT2I\u6a21\u578b\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u5e76\u5728\u771f\u5b9e\u5bf9\u6297\u7ade\u8d5b\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u8fd8\u80fd\u6784\u5efa\u9ad8\u8d28\u91cf\u5bf9\u6297\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u7528\u4e8e\u653b\u51fb\u573a\u666f\uff0c\u8fd8\u53ef\u63d0\u5347AIGC\u68c0\u6d4b\u548c\u9632\u5fa1\u7cfb\u7edf\u7684\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u8d44\u6e90\u3002", "keywords": "\u6587\u672c\u751f\u6210\u56fe\u50cf\uff0c\u5bf9\u6297\u63d0\u793a\uff0c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0cAIGC\u68c0\u6d4b"}}
{"id": "2505.23527", "pdf": "https://arxiv.org/pdf/2505.23527", "abs": "https://arxiv.org/abs/2505.23527", "authors": ["Raj Ghugare", "Benjamin Eysenbach"], "title": "Normalizing Flows are Capable Models for RL", "categories": ["cs.LG"], "comment": "Project page with code - https://rajghugare19.github.io/nf4rl/", "summary": "Modern reinforcement learning (RL) algorithms have found success by using\npowerful probabilistic models, such as transformers, energy-based models, and\ndiffusion/flow-based models. To this end, RL researchers often choose to pay\nthe price of accommodating these models into their algorithms -- diffusion\nmodels are expressive, but are computationally intensive due to their reliance\non solving differential equations, while autoregressive transformer models are\nscalable but typically require learning discrete representations. Normalizing\nflows (NFs), by contrast, seem to provide an appealing alternative, as they\nenable likelihoods and sampling without solving differential equations or\nautoregressive architectures. However, their potential in RL has received\nlimited attention, partly due to the prevailing belief that normalizing flows\nlack sufficient expressivity. We show that this is not the case. Building on\nrecent work in NFs, we propose a single NF architecture which integrates\nseamlessly into RL algorithms, serving as a policy, Q-function, and occupancy\nmeasure. Our approach leads to much simpler algorithms, and achieves higher\nperformance in imitation learning, offline, goal conditioned RL and\nunsupervised RL.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5f52\u4e00\u5316\u6d41\uff08NFs\uff09\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u8bc1\u660e\u5176\u8868\u8fbe\u529b\u8db3\u591f\u4e14\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5982\u6269\u6563\u6a21\u578b\u548c\u81ea\u56de\u5f52\u53d8\u6362\u5668\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bNF\u67b6\u6784\uff0c\u7b80\u5316\u7b97\u6cd5\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684RL\u7b97\u6cd5\u4f9d\u8d56\u590d\u6742\u6a21\u578b\uff08\u5982\u6269\u6563\u6a21\u578b\u6216\u81ea\u56de\u5f52\u53d8\u6362\u5668\uff09\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u9700\u8981\u79bb\u6563\u8868\u793a\u3002\u5f52\u4e00\u5316\u6d41\uff08NFs\uff09\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u5728RL\u4e2d\u7684\u6f5c\u529b\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u5f52\u4e00\u5316\u6d41\uff08NF\uff09\u67b6\u6784\uff0c\u65e0\u7f1d\u96c6\u6210\u5230RL\u7b97\u6cd5\u4e2d\uff0c\u7528\u4e8e\u8868\u793a\u7b56\u7565\u3001Q\u51fd\u6570\u548c\u5360\u7528\u5ea6\u91cf\uff0c\u907f\u514d\u4e86\u590d\u6742\u5fae\u5206\u65b9\u7a0b\u6216\u81ea\u56de\u5f52\u7ed3\u6784\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002", "result": "\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86RL\u7b97\u6cd5\u7ed3\u6784\uff0c\u5728\u6a21\u4eff\u5b66\u4e60\u3001\u79bb\u7ebfRL\u3001\u76ee\u6807\u6761\u4ef6RL\u548c\u65e0\u76d1\u7763RL\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u5f52\u4e00\u5316\u6d41\u5728RL\u4e2d\u5177\u6709\u5f3a\u5927\u8868\u8fbe\u529b\uff0c\u5176\u9ad8\u6548\u6027\u4e0e\u7b80\u6d01\u6027\u4e3a\u672a\u6765RL\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u5f52\u4e00\u5316\u6d41, \u6269\u6563\u6a21\u578b, \u81ea\u56de\u5f52\u53d8\u6362\u5668, \u6a21\u4eff\u5b66\u4e60"}}
{"id": "2505.22907", "pdf": "https://arxiv.org/pdf/2505.22907", "abs": "https://arxiv.org/abs/2505.22907", "authors": ["Rachel Katharine Sterken", "James Ravi Kirkpatrick"], "title": "Conversational Alignment with Artificial Intelligence in Context", "categories": ["cs.CY", "cs.CL"], "comment": "20 pages, to be published in Philosophical Perspectives", "summary": "The development of sophisticated artificial intelligence (AI) conversational\nagents based on large language models raises important questions about the\nrelationship between human norms, values, and practices and AI design and\nperformance. This article explores what it means for AI agents to be\nconversationally aligned to human communicative norms and practices for\nhandling context and common ground and proposes a new framework for evaluating\ndevelopers' design choices. We begin by drawing on the philosophical and\nlinguistic literature on conversational pragmatics to motivate a set of\ndesiderata, which we call the CONTEXT-ALIGN framework, for conversational\nalignment with human communicative practices. We then suggest that current\nlarge language model (LLM) architectures, constraints, and affordances may\nimpose fundamental limitations on achieving full conversational alignment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aCONTEXT-ALIGN\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5bf9\u8bdd\u4ee3\u7406\u5bf9\u4eba\u7c7b\u4ea4\u6d41\u89c4\u8303\u7684\u9002\u914d\u7a0b\u5ea6\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8fd9\u65b9\u9762\u7684\u6839\u672c\u6027\u9650\u5236\u3002", "motivation": "\u63a2\u8ba8AI\u5bf9\u8bdd\u4ee3\u7406\u5982\u4f55\u66f4\u597d\u5730\u9075\u5faa\u4eba\u7c7b\u4ea4\u6d41\u89c4\u8303\uff0c\u5c24\u5176\u662f\u5904\u7406\u4e0a\u4e0b\u6587\u548c\u5171\u6709\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u5347\u5bf9\u8bdd\u8d28\u91cf\u3002", "method": "\u57fa\u4e8e\u54f2\u5b66\u548c\u8bed\u8a00\u5b66\u4e2d\u7684\u5bf9\u8bdd\u8bed\u7528\u5b66\u7406\u8bba\uff0c\u63d0\u51faCONTEXT-ALIGN\u6846\u67b6\uff0c\u5206\u6790LLM\u67b6\u6784\u5bf9\u5b9e\u73b0\u5bf9\u8bdd\u5bf9\u9f50\u7684\u6f5c\u5728\u9650\u5236\u3002", "result": "\u5f53\u524dLLM\u5728\u5b8c\u5168\u5b9e\u73b0\u5bf9\u8bdd\u5bf9\u9f50\u4e0a\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff0c\u9700\u66f4\u6df1\u5165\u7684\u8bbe\u8ba1\u6539\u8fdb\u3002", "conclusion": "CONTEXT-ALIGN\u6846\u67b6\u4e3aAI\u5bf9\u8bdd\u4ee3\u7406\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u7a81\u7834\u73b0\u6709\u6280\u672f\u9650\u5236\u3002", "keywords": "AI\u5bf9\u8bdd\u4ee3\u7406,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u5bf9\u8bdd\u5bf9\u9f50,\u8bed\u7528\u5b66,CONTEXT-ALIGN"}}
{"id": "2505.23528", "pdf": "https://arxiv.org/pdf/2505.23528", "abs": "https://arxiv.org/abs/2505.23528", "authors": ["Maria Eleftheria Vlontzou", "Maria Athanasiou", "Christos Davatzikos", "Konstantina S. Nikita"], "title": "Comparative assessment of fairness definitions and bias mitigation strategies in machine learning-based diagnosis of Alzheimer's disease from MR images", "categories": ["cs.LG"], "comment": "(C) 2025 IEEE Paper accepted at IEEE Engineering in Medicine and\n  Biology Society Conference, 2025", "summary": "The present study performs a comprehensive fairness analysis of machine\nlearning (ML) models for the diagnosis of Mild Cognitive Impairment (MCI) and\nAlzheimer's disease (AD) from MRI-derived neuroimaging features. Biases\nassociated with age, race, and gender in a multi-cohort dataset, as well as the\ninfluence of proxy features encoding these sensitive attributes, are\ninvestigated. The reliability of various fairness definitions and metrics in\nthe identification of such biases is also assessed. Based on the most\nappropriate fairness measures, a comparative analysis of widely used\npre-processing, in-processing, and post-processing bias mitigation strategies\nis performed. Moreover, a novel composite measure is introduced to quantify the\ntrade-off between fairness and performance by considering the F1-score and the\nequalized odds ratio, making it appropriate for medical diagnostic\napplications. The obtained results reveal the existence of biases related to\nage and race, while no significant gender bias is observed. The deployed\nmitigation strategies yield varying improvements in terms of fairness across\nthe different sensitive attributes and studied subproblems. For race and\ngender, Reject Option Classification improves equalized odds by 46% and 57%,\nrespectively, and achieves harmonic mean scores of 0.75 and 0.80 in the MCI\nversus AD subproblem, whereas for age, in the same subproblem, adversarial\ndebiasing yields the highest equalized odds improvement of 40% with a harmonic\nmean score of 0.69. Insights are provided into how variations in AD\nneuropathology and risk factors, associated with demographic characteristics,\ninfluence model fairness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u8bca\u65ad\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\uff08MCI\uff09\u548c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u4e2d\u7684\u516c\u5e73\u6027\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u8bc4\u4f30\u4e86\u5e74\u9f84\u3001\u79cd\u65cf\u548c\u6027\u522b\u7b49\u56e0\u7d20\u5bf9\u6a21\u578b\u7684\u6f5c\u5728\u504f\u89c1\u53ca\u5176\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u7684\u52a8\u673a\u662f\u63a2\u8ba8\u533b\u7597AI\u6a21\u578b\u4e2d\u6f5c\u5728\u7684\u504f\u89c1\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u57fa\u4e8eMRI\u5f71\u50cf\u8bca\u65adMCI\u548cAD\u65f6\uff0c\u5e74\u9f84\u3001\u79cd\u65cf\u548c\u6027\u522b\u7b49\u56e0\u7d20\u53ef\u80fd\u5bf9\u6a21\u578b\u7684\u516c\u5e73\u6027\u548c\u6027\u80fd\u4ea7\u751f\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u591a\u961f\u5217\u6570\u636e\u96c6\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u7684\u516c\u5e73\u6027\u5b9a\u4e49\u548c\u6307\u6807\uff0c\u5e76\u6bd4\u8f83\u4e86\u9884\u5904\u7406\u3001\u5904\u7406\u4e2d\u548c\u5904\u7406\u540e\u7684\u504f\u89c1\u7f13\u89e3\u7b56\u7565\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u590d\u5408\u6307\u6807\u6765\u8861\u91cf\u516c\u5e73\u6027\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5e74\u9f84\u548c\u79cd\u65cf\u76f8\u5173\u7684\u504f\u89c1\u5b58\u5728\uff0c\u800c\u6027\u522b\u504f\u89c1\u4e0d\u663e\u8457\u3002\u4e0d\u540c\u7684\u504f\u89c1\u7f13\u89e3\u7b56\u7565\u5728\u4e0d\u540c\u654f\u611f\u5c5e\u6027\u4e0a\u6548\u679c\u5404\u5f02\uff0c\u5982\u62d2\u7edd\u9009\u9879\u5206\u7c7b\u5bf9\u79cd\u65cf\u548c\u6027\u522b\u504f\u89c1\u7684\u6539\u5584\u6548\u679c\u5c24\u4e3a\u660e\u663e\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u533b\u7597\u8bca\u65adAI\u4e2d\u8003\u8651\u516c\u5e73\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u5982\u4f55\u6839\u636e\u4e0d\u540c\u4eba\u53e3\u7279\u5f81\u8c03\u6574\u6a21\u578b\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "keywords": "\u516c\u5e73\u6027\u5206\u6790\uff0c\u673a\u5668\u5b66\u4e60\uff0c\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\uff0c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff0c\u504f\u89c1\u7f13\u89e3"}}
{"id": "2505.23214", "pdf": "https://arxiv.org/pdf/2505.23214", "abs": "https://arxiv.org/abs/2505.23214", "authors": ["Wenhao Xu", "Shuchen Zheng", "Changwei Wang", "Zherui Zhang", "Chuan Ren", "Rongtao Xu", "Shibiao Xu"], "title": "SAMamba: Adaptive State Space Modeling with Hierarchical Vision for Infrared Small Target Detection", "categories": ["cs.CV", "cs.AI"], "comment": "Information Fusion 2025", "summary": "Infrared small target detection (ISTD) is vital for long-range surveillance\nin military, maritime, and early warning applications. ISTD is challenged by\ntargets occupying less than 0.15% of the image and low distinguishability from\ncomplex backgrounds. Existing deep learning methods often suffer from\ninformation loss during downsampling and inefficient global context modeling.\nThis paper presents SAMamba, a novel framework integrating SAM2's hierarchical\nfeature learning with Mamba's selective sequence modeling. Key innovations\ninclude: (1) A Feature Selection Adapter (FS-Adapter) for efficient\nnatural-to-infrared domain adaptation via dual-stage selection (token-level\nwith a learnable task embedding and channel-wise adaptive transformations); (2)\nA Cross-Channel State-Space Interaction (CSI) module for efficient global\ncontext modeling with linear complexity using selective state space modeling;\nand (3) A Detail-Preserving Contextual Fusion (DPCF) module that adaptively\ncombines multi-scale features with a gating mechanism to balance\nhigh-resolution and low-resolution feature contributions. SAMamba addresses\ncore ISTD challenges by bridging the domain gap, maintaining fine-grained\ndetails, and efficiently modeling long-range dependencies. Experiments on\nNUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets show SAMamba significantly\noutperforms state-of-the-art methods, especially in challenging scenarios with\nheterogeneous backgrounds and varying target scales. Code:\nhttps://github.com/zhengshuchen/SAMamba.", "AI": {"tldr": "SAMamba \u662f\u4e00\u79cd\u65b0\u578b\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u4e86 SAM2 \u7684\u5206\u5c42\u7279\u5f81\u5b66\u4e60\u548c Mamba \u7684\u9009\u62e9\u6027\u5e8f\u5217\u5efa\u6a21\uff0c\u901a\u8fc7 FS-Adapter\u3001CSI \u6a21\u5757\u548c DPCF \u6a21\u5757\u89e3\u51b3\u9886\u57df\u9002\u5e94\u3001\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u5728\u519b\u4e8b\u3001\u6d77\u4e8b\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002\u9488\u5bf9\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86 SAMamba \u6846\u67b6\u3002", "method": "SAMamba \u96c6\u6210\u4e86 SAM2 \u7684\u5206\u5c42\u7279\u5f81\u5b66\u4e60\u548c Mamba \u7684\u9009\u62e9\u6027\u5e8f\u5217\u5efa\u6a21\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1aFS-Adapter \u7528\u4e8e\u9886\u57df\u9002\u5e94\uff0cCSI \u6a21\u5757\u7528\u4e8e\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0cDPCF \u6a21\u5757\u7528\u4e8e\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728 NUAA-SIRST\u3001IRSTD-1k \u548c NUDT-SIRST \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSAMamba \u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u590d\u6742\u80cc\u666f\u548c\u591a\u5c3a\u5ea6\u76ee\u6807\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "SAMamba \u901a\u8fc7\u521b\u65b0\u6a21\u5757\u6709\u6548\u89e3\u51b3\u4e86\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b, \u9886\u57df\u9002\u5e94, \u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21, \u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408, \u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2505.23529", "pdf": "https://arxiv.org/pdf/2505.23529", "abs": "https://arxiv.org/abs/2505.23529", "authors": ["Shifeng Xie", "Aref Einizade", "Jhony H. Giraldo"], "title": "Subgraph Gaussian Embedding Contrast for Self-Supervised Graph Representation Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Representation Learning (GRL) is a fundamental task in machine\nlearning, aiming to encode high-dimensional graph-structured data into\nlow-dimensional vectors. Self-Supervised Learning (SSL) methods are widely used\nin GRL because they can avoid expensive human annotation. In this work, we\npropose a novel Subgraph Gaussian Embedding Contrast (SubGEC) method. Our\napproach introduces a subgraph Gaussian embedding module, which adaptively maps\nsubgraphs to a structured Gaussian space, ensuring the preservation of input\nsubgraph characteristics while generating subgraphs with a controlled\ndistribution. We then employ optimal transport distances, more precisely the\nWasserstein and Gromov-Wasserstein distances, to effectively measure the\nsimilarity between subgraphs, enhancing the robustness of the contrastive\nlearning process. Extensive experiments across multiple benchmarks demonstrate\nthat \\method~outperforms or presents competitive performance against\nstate-of-the-art approaches. Our findings provide insights into the design of\nSSL methods for GRL, emphasizing the importance of the distribution of the\ngenerated contrastive pairs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aSubGEC\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b50\u56fe\u9ad8\u65af\u5d4c\u5165\u548c\u6700\u4f18\u8fd0\u8f93\u8ddd\u79bb\u63d0\u5347\u56fe\u8868\u793a\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u81ea\u76d1\u7763\u5b66\u4e60\u53ef\u907f\u514d\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5b50\u56fe\u7279\u5f81\u4fdd\u7559\u548c\u5bf9\u6bd4\u5b66\u4e60\u9c81\u68d2\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u5b50\u56fe\u9ad8\u65af\u5d4c\u5165\u6a21\u5757\u548cWasserstein/Gromov-Wasserstein\u8ddd\u79bb\u6765\u5ea6\u91cf\u5b50\u56fe\u76f8\u4f3c\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u5bf9\u6bd4\u5bf9\u5206\u5e03\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u81ea\u76d1\u7763\u56fe\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u56fe\u8868\u793a\u5b66\u4e60, \u81ea\u76d1\u7763\u5b66\u4e60, \u9ad8\u65af\u5d4c\u5165, \u6700\u4f18\u8fd0\u8f93"}}
{"id": "2505.23537", "pdf": "https://arxiv.org/pdf/2505.23537", "abs": "https://arxiv.org/abs/2505.23537", "authors": ["Giorgos Iacovides", "Wuyang Zhou", "Chao Li", "Qibin Zhao", "Danilo Mandic"], "title": "Domain-Aware Tensor Network Structure Search", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Tensor networks (TNs) provide efficient representations of high-dimensional\ndata, yet identification of the optimal TN structures, the so called tensor\nnetwork structure search (TN-SS) problem, remains a challenge. Current\nstate-of-the-art (SOTA) algorithms are computationally expensive as they\nrequire extensive function evaluations, which is prohibitive for real-world\napplications. In addition, existing methods ignore valuable domain information\ninherent in real-world tensor data and lack transparency in their identified TN\nstructures. To this end, we propose a novel TN-SS framework, termed the tnLLM,\nwhich incorporates domain information about the data and harnesses the\nreasoning capabilities of large language models (LLMs) to directly predict\nsuitable TN structures. The proposed framework involves a domain-aware\nprompting pipeline which instructs the LLM to infer suitable TN structures\nbased on the real-world relationships between tensor modes. In this way, our\napproach is capable of not only iteratively optimizing the objective function,\nbut also generating domain-aware explanations for the identified structures.\nExperimental results demonstrate that tnLLM achieves comparable TN-SS objective\nfunction values with much fewer function evaluations compared to SOTA\nalgorithms. Furthermore, we demonstrate that the LLM-enabled domain information\ncan be used to find good initializations in the search space for sampling-based\nSOTA methods to accelerate their convergence while preserving theoretical\nperformance guarantees.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3atnLLM\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7ed3\u5408\u9886\u57df\u4fe1\u606f\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\uff0c\u76f4\u63a5\u9884\u6d4b\u4f18\u5316\u7684\u5f20\u91cf\u7f51\u7edc\u7ed3\u6784\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u7ed3\u6784\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u5f20\u91cf\u7f51\u7edc\u7ed3\u6784\u641c\u7d22\uff08TN-SS\uff09\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5ffd\u89c6\u9886\u57df\u4fe1\u606f\u4e14\u7f3a\u4e4f\u7ed3\u6784\u900f\u660e\u6027\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fatnLLM\u6846\u67b6\uff0c\u901a\u8fc7\u9886\u57df\u611f\u77e5\u63d0\u793a\u7ba1\u9053\u5f15\u5bfcLLM\u57fa\u4e8e\u5f20\u91cf\u6a21\u5f0f\u95f4\u7684\u5b9e\u9645\u5173\u7cfb\u63a8\u65ad\u7ed3\u6784\uff0c\u5e76\u751f\u6210\u9886\u57df\u611f\u77e5\u89e3\u91ca\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0ctnLLM\u4ee5\u66f4\u5c11\u8ba1\u7b97\u8fbe\u5230\u4e0eSOTA\u76f8\u8fd1\u7684\u7ed3\u679c\uff0c\u4e14\u80fd\u4e3a\u91c7\u6837\u7c7b\u65b9\u6cd5\u63d0\u4f9b\u4f18\u8d28\u521d\u59cb\u5316\u4ee5\u52a0\u901f\u6536\u655b\u3002", "conclusion": "tnLLM\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u4e0eLLM\u63a8\u7406\uff0c\u9ad8\u6548\u89e3\u51b3\u4e86TN-SS\u95ee\u9898\uff0c\u517c\u5177\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "keywords": "\u5f20\u91cf\u7f51\u7edc\u3001\u7ed3\u6784\u641c\u7d22\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u9886\u57df\u4fe1\u606f\u3001\u8ba1\u7b97\u6548\u7387"}}
{"id": "2505.23239", "pdf": "https://arxiv.org/pdf/2505.23239", "abs": "https://arxiv.org/abs/2505.23239", "authors": ["Lingkai Meng", "Yu Shao", "Long Yuan", "Longbin Lai", "Peng Cheng", "Wenyuan Yu", "Wenjie Zhang", "Xuemin Lin", "Jingren Zhou"], "title": "OSS-UAgent: An Agent-based Usability Evaluation Framework for Open Source Software", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Usability evaluation is critical to the impact and adoption of open source\nsoftware (OSS), yet traditional methods relying on human evaluators suffer from\nhigh costs and limited scalability. To address these limitations, we introduce\nOSS-UAgent, an automated, configurable, and interactive agent-based usability\nevaluation framework specifically designed for open source software. Our\nframework employs intelligent agents powered by large language models (LLMs) to\nsimulate developers performing programming tasks across various experience\nlevels (from Junior to Expert). By dynamically constructing platform-specific\nknowledge bases, OSS-UAgent ensures accurate and context-aware code generation.\nThe generated code is automatically evaluated across multiple dimensions,\nincluding compliance, correctness, and readability, providing a comprehensive\nmeasure of the software's usability. Additionally, our demonstration showcases\nOSS-UAgent's practical application in evaluating graph analytics platforms,\nhighlighting its effectiveness in automating usability evaluation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86OSS-UAgent\uff0c\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u3001\u53ef\u914d\u7f6e\u7684\u3001\u57fa\u4e8e\u4ea4\u4e92\u5f0f\u4ee3\u7406\u7684\u5f00\u6e90\u8f6f\u4ef6\u53ef\u7528\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6a21\u62df\u4e0d\u540c\u7ecf\u9a8c\u6c34\u5e73\u7684\u5f00\u53d1\u8005\u6267\u884c\u7f16\u7a0b\u4efb\u52a1\uff0c\u4ee5\u964d\u4f4e\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u7684\u6210\u672c\u548c\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5f00\u6e90\u8f6f\u4ef6\u7684\u53ef\u7528\u6027\u8bc4\u4f30\u5bf9\u63a8\u5e7f\u548c\u91c7\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bc4\u4f30\uff0c\u6210\u672c\u9ad8\u4e14\u53ef\u6269\u5c55\u6027\u5dee\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4ee3\u7406\uff0c\u6a21\u62df\u4e0d\u540c\u7ecf\u9a8c\u6c34\u5e73\u7684\u5f00\u53d1\u8005\u6267\u884c\u4efb\u52a1\uff0c\u52a8\u6001\u6784\u5efa\u5e73\u53f0\u7279\u5b9a\u77e5\u8bc6\u5e93\uff0c\u5e76\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "OSS-UAgent\u5728\u8bc4\u4f30\u56fe\u5206\u6790\u5e73\u53f0\u65f6\u8868\u73b0\u51fa\u9ad8\u6548\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u7efc\u5408\u8bc4\u4ef7\u4e86\u4ee3\u7801\u7684\u5408\u89c4\u6027\u3001\u6b63\u786e\u6027\u548c\u53ef\u8bfb\u6027\u3002", "conclusion": "OSS-UAgent\u4e3a\u5f00\u6e90\u8f6f\u4ef6\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u53ef\u7528\u6027\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u6210\u672c\u548c\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "keywords": "\u5f00\u6e90\u8f6f\u4ef6, \u53ef\u7528\u6027\u8bc4\u4f30, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u667a\u80fd\u4ee3\u7406, \u81ea\u52a8\u5316"}}
{"id": "2505.23552", "pdf": "https://arxiv.org/pdf/2505.23552", "abs": "https://arxiv.org/abs/2505.23552", "authors": ["Alex Adams"], "title": "Comparing the Moore-Penrose Pseudoinverse and Gradient Descent for Solving Linear Regression Problems: A Performance Analysis", "categories": ["cs.LG"], "comment": null, "summary": "This paper investigates the comparative performance of two fundamental\napproaches to solving linear regression problems: the closed-form Moore-Penrose\npseudoinverse and the iterative gradient descent method. Linear regression is a\ncornerstone of predictive modeling, and the choice of solver can significantly\nimpact efficiency and accuracy. I review and discuss the theoretical\nunderpinnings of both methods, analyze their computational complexity, and\nevaluate their empirical behavior on synthetic datasets with controlled\ncharacteristics, as well as on established real-world datasets. My results\ndelineate the conditions under which each method excels in terms of\ncomputational time, numerical stability, and predictive accuracy. This work\naims to provide practical guidance for researchers and practitioners in machine\nlearning when selecting between direct, exact solutions and iterative,\napproximate solutions for linear regression tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6bd4\u8f83\u4e86\u4e24\u79cd\u7ebf\u6027\u56de\u5f52\u6c42\u89e3\u65b9\u6cd5\u7684\u6027\u80fd\uff1a\u95ed\u5f0fMoore-Penrose\u4f2a\u9006\u548c\u8fed\u4ee3\u68af\u5ea6\u4e0b\u964d\u6cd5\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u7406\u8bba\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5b9e\u9645\u8868\u73b0\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f7f\u7528\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u673a\u5668\u5b66\u4e60\u7684\u5b9e\u8df5\u8005\u63d0\u4f9b\u9009\u62e9\u7ebf\u6027\u56de\u5f52\u6c42\u89e3\u65b9\u6cd5\u7684\u4f9d\u636e\uff0c\u6bd4\u8f83\u95ed\u5f0f\u89e3\u548c\u8fed\u4ee3\u89e3\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u4f18\u52a3\u3002", "method": "\u901a\u8fc7\u5bf9\u4e24\u79cd\u65b9\u6cd5\u7684\u7406\u8bba\u5206\u6790\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u8bc4\u4f30\uff0c\u4ee5\u53ca\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u6d4b\u8bd5\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6bcf\u79cd\u65b9\u6cd5\u5728\u8ba1\u7b97\u65f6\u95f4\u3001\u6570\u503c\u7a33\u5b9a\u6027\u548c\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u5404\u6709\u4f18\u52bf\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u6570\u636e\u7279\u5f81\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u5e94\u6839\u636e\u6570\u636e\u89c4\u6a21\u548c\u7279\u5f81\u9009\u62e9\u5408\u9002\u7684\u65b9\u6cd5\uff0c\u95ed\u5f0f\u89e3\u9002\u5408\u5c0f\u89c4\u6a21\u6570\u636e\uff0c\u800c\u68af\u5ea6\u4e0b\u964d\u66f4\u9002\u5408\u5927\u89c4\u6a21\u95ee\u9898\u3002", "keywords": "\u7ebf\u6027\u56de\u5f52, Moore-Penrose\u4f2a\u9006, \u68af\u5ea6\u4e0b\u964d, \u8ba1\u7b97\u590d\u6742\u5ea6, \u6570\u503c\u7a33\u5b9a\u6027"}}
{"id": "2505.23039", "pdf": "https://arxiv.org/pdf/2505.23039", "abs": "https://arxiv.org/abs/2505.23039", "authors": ["Kapil Vaidya", "Jialin Ding", "Sebastian Kosak", "David Kernert", "Chuan Lei", "Xiao Qin", "Abhinav Tripathy", "Ramesh Balan", "Balakrishnan Narayanaswamy", "Tim Kraska"], "title": "TailorSQL: An NL2SQL System Tailored to Your Query Workload", "categories": ["cs.DB", "cs.CL"], "comment": null, "summary": "NL2SQL (natural language to SQL) translates natural language questions into\nSQL queries, thereby making structured data accessible to non-technical users,\nserving as the foundation for intelligent data applications. State-of-the-art\nNL2SQL techniques typically perform translation by retrieving database-specific\ninformation, such as the database schema, and invoking a pre-trained large\nlanguage model (LLM) using the question and retrieved information to generate\nthe SQL query.\n  However, existing NL2SQL techniques miss a key opportunity which is present\nin real-world settings: NL2SQL is typically applied on existing databases which\nhave already served many SQL queries in the past. The past query workload\nimplicitly contains information which is helpful for accurate NL2SQL\ntranslation and is not apparent from the database schema alone, such as common\njoin paths and the semantics of obscurely-named tables and columns. We\nintroduce TailorSQL, a NL2SQL system that takes advantage of information in the\npast query workload to improve both the accuracy and latency of translating\nnatural language questions into SQL. By specializing to a given workload,\nTailorSQL achieves up to 2$\\times$ improvement in execution accuracy on\nstandardized benchmarks.", "AI": {"tldr": "TailorSQL \u662f\u4e00\u79cd\u5229\u7528\u5386\u53f2\u67e5\u8be2\u8d1f\u8f7d\u4fe1\u606f\u7684 NL2SQL \u7cfb\u7edf\uff0c\u901a\u8fc7\u6539\u8fdb\u81ea\u7136\u8bed\u8a00\u5230 SQL \u7684\u7ffb\u8bd1\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u4e86\u6807\u51c6\u5316\u57fa\u51c6\u4e0a\u6267\u884c\u51c6\u786e\u6027\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684 NL2SQL \u6280\u672f\u901a\u5e38\u5ffd\u7565\u4e86\u771f\u5b9e\u573a\u666f\u4e2d\u5df2\u6709\u7684\u5386\u53f2\u67e5\u8be2\u8d1f\u8f7d\u4fe1\u606f\uff0c\u8fd9\u4e9b\u4fe1\u606f\u5bf9\u4e8e\u63d0\u9ad8\u7ffb\u8bd1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u5177\u6709\u6f5c\u5728\u4ef7\u503c\u3002", "method": "TailorSQL \u901a\u8fc7\u5229\u7528\u5386\u53f2\u67e5\u8be2\u8d1f\u8f7d\u4e2d\u7684\u4fe1\u606f\uff08\u5982\u5e38\u89c1\u8fde\u63a5\u8def\u5f84\u548c\u8868\u5217\u540d\u7684\u8bed\u4e49\uff09\u6765\u4f18\u5316\u7ffb\u8bd1\u8fc7\u7a0b\uff0c\u800c\u4e0d\u53ea\u662f\u4f9d\u8d56\u4e8e\u6570\u636e\u5e93\u6a21\u5f0f\u3002", "result": "TailorSQL \u5728\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6267\u884c\u51c6\u786e\u6027\u9ad8\u8fbe 2 \u500d\u7684\u63d0\u5347\u3002", "conclusion": "\u5229\u7528\u5386\u53f2\u67e5\u8be2\u8d1f\u8f7d\u4fe1\u606f\u53ef\u4ee5\u663e\u8457\u63d0\u5347 NL2SQL \u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u7279\u5b9a\u5de5\u4f5c\u8d1f\u8f7d\u65f6\u3002", "keywords": "NL2SQL, TailorSQL, \u81ea\u7136\u8bed\u8a00\u5904\u7406, SQL \u67e5\u8be2, \u5de5\u4f5c\u91cf\u4f18\u5316"}}
{"id": "2505.23555", "pdf": "https://arxiv.org/pdf/2505.23555", "abs": "https://arxiv.org/abs/2505.23555", "authors": ["Yanzhao Hou", "Jiaxiang Geng", "Boyu Li", "Xiaofeng Tao", "Juncheng Wang", "Xiaodong Xu", "Bing Luo"], "title": "Adaptive Federated LoRA in Heterogeneous Wireless Networks with Independent Sampling", "categories": ["cs.LG"], "comment": "13 pages, Submitted to IEEE Journal on Selected Areas in\n  Communications (JSAC)", "summary": "Federated LoRA has emerged as a promising technique for efficiently\nfine-tuning large language models (LLMs) on distributed devices by reducing the\nnumber of trainable parameters. However, existing approaches often inadequately\noverlook the theoretical and practical implications of system and data\nheterogeneity, thereby failing to optimize the overall training efficiency,\nparticularly in terms of wall-clock time. In this paper, we propose an adaptive\nfederated LoRA strategy with independent client sampling to minimize the\nconvergence wall-clock time of federated fine-tuning under both computation and\ncommunication heterogeneity. We first derive a new convergence bound for\nfederated LoRA with arbitrary and independent client sampling, notably without\nrequiring the stringent bounded gradient assumption. Then, we introduce an\nadaptive bandwidth allocation scheme that accounts for heterogeneous client\nresources and system bandwidth constraints. Based on the derived theory, we\nformulate and solve a non-convex optimization problem to jointly determine the\nLoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock\nconvergence time. An efficient and low-complexity algorithm is developed to\napproximate the solution. Finally, extensive experiments demonstrate that our\napproach significantly reduces wall-clock training time compared to\nstate-of-the-art methods across various models and datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u8054\u90a6LoRA\u7b56\u7565\uff0c\u901a\u8fc7\u72ec\u7acb\u5ba2\u6237\u7aef\u91c7\u6837\u548c\u8d44\u6e90\u5206\u914d\u4f18\u5316\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8054\u90a6\u5fae\u8c03\u4e2d\u7684\u6536\u655b\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u8054\u90a6LoRA\u65b9\u6cd5\u5ffd\u89c6\u7cfb\u7edf\u548c\u6570\u636e\u5f02\u6784\u6027\u5bf9\u8bad\u7ec3\u6548\u7387\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u5b9e\u9645\u65f6\u95f4\u4e0a\u7684\u4f18\u5316\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u8054\u90a6LoRA\u7b56\u7565\uff0c\u7ed3\u5408\u72ec\u7acb\u5ba2\u6237\u7aef\u91c7\u6837\u548c\u5e26\u5bbd\u5206\u914d\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u975e\u51f8\u4f18\u5316\u8054\u5408\u786e\u5b9aLoRA\u53c2\u6570\u548c\u91c7\u6837\u6982\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u663e\u8457\u7f29\u77ed\u8bad\u7ec3\u65f6\u95f4\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u81ea\u9002\u5e94\u8054\u90a6LoRA\u7b56\u7565\u80fd\u6709\u6548\u5e94\u5bf9\u5f02\u6784\u8d44\u6e90\u73af\u5883\uff0c\u4f18\u5316\u8054\u90a6\u5b66\u4e60\u7684\u6548\u7387\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60, LoRA, \u5f02\u6784\u6027, \u81ea\u9002\u5e94\u4f18\u5316, \u5ba2\u6237\u7aef\u91c7\u6837"}}
{"id": "2505.23250", "pdf": "https://arxiv.org/pdf/2505.23250", "abs": "https://arxiv.org/abs/2505.23250", "authors": ["Pascal J. Sager", "Ashwini Kamaraj", "Benjamin F. Grewe", "Thilo Stadelmann"], "title": "Deep Retrieval at CheckThat! 2025: Identifying Scientific Papers from Implicit Social Media Mentions via Hybrid Retrieval and Re-Ranking", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "We present the methodology and results of the Deep Retrieval team for subtask\n4b of the CLEF CheckThat! 2025 competition, which focuses on retrieving\nrelevant scientific literature for given social media posts. To address this\ntask, we propose a hybrid retrieval pipeline that combines lexical precision,\nsemantic generalization, and deep contextual re-ranking, enabling robust\nretrieval that bridges the informal-to-formal language gap. Specifically, we\ncombine BM25-based keyword matching with a FAISS vector store using a\nfine-tuned INF-Retriever-v1 model for dense semantic retrieval. BM25 returns\nthe top 30 candidates, and semantic search yields 100 candidates, which are\nthen merged and re-ranked via a large language model (LLM)-based cross-encoder.\n  Our approach achieves a mean reciprocal rank at 5 (MRR@5) of 76.46% on the\ndevelopment set and 66.43% on the hidden test set, securing the 1st position on\nthe development leaderboard and ranking 3rd on the test leaderboard (out of 31\nteams), with a relative performance gap of only 2 percentage points compared to\nthe top-ranked system. We achieve this strong performance by running\nopen-source models locally and without external training data, highlighting the\neffectiveness of a carefully designed and fine-tuned retrieval pipeline.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u68c0\u7d22\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u8bcd\u6cd5\u7cbe\u786e\u5339\u914d\u3001\u8bed\u4e49\u6cdb\u5316\u548c\u6df1\u5ea6\u4e0a\u4e0b\u6587\u91cd\u6392\u5e8f\uff0c\u7528\u4e8e\u4e3a\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u68c0\u7d22\u76f8\u5173\u79d1\u5b66\u6587\u732e\u3002\u8be5\u65b9\u6cd5\u5728\u5f00\u53d1\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6392\u540d\u9760\u524d\u3002", "motivation": "\u89e3\u51b3\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u4e0e\u79d1\u5b66\u6587\u732e\u4e4b\u95f4\u975e\u6b63\u5f0f\u5230\u6b63\u5f0f\u8bed\u8a00\u7684\u68c0\u7d22\u9e3f\u6c9f\uff0c\u63d0\u5347\u68c0\u7d22\u6548\u679c\u3002", "method": "\u7ed3\u5408BM25\u5173\u952e\u8bcd\u5339\u914d\u548cFAISS\u5411\u91cf\u5b58\u50a8\uff08\u57fa\u4e8e\u5fae\u8c03\u7684INF-Retriever-v1\uff09\u8fdb\u884c\u5bc6\u96c6\u8bed\u4e49\u68c0\u7d22\uff0c\u518d\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u4ea4\u53c9\u7f16\u7801\u5668\u91cd\u6392\u5e8f\u3002", "result": "\u5f00\u53d1\u96c6MRR@5\u4e3a76.46%\uff0c\u6d4b\u8bd5\u96c6\u4e3a66.43%\uff0c\u5728\u5f00\u53d1\u6392\u884c\u699c\u6392\u540d\u7b2c\u4e00\uff0c\u6d4b\u8bd5\u6392\u884c\u699c\u6392\u540d\u7b2c\u4e09\uff08\u517131\u4e2a\u56e2\u961f\uff09\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u548c\u5fae\u8c03\u7684\u672c\u5730\u5f00\u6e90\u6a21\u578b\u68c0\u7d22\u6d41\u7a0b\uff0c\u65e0\u9700\u5916\u90e8\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u68c0\u7d22\u3002", "keywords": "\u6df7\u5408\u68c0\u7d22\u3001\u793e\u4ea4\u5a92\u4f53\u3001\u79d1\u5b66\u6587\u732e\u3001BM25\u3001FAISS\u3001LLM\u3001CLEF"}}
{"id": "2505.23564", "pdf": "https://arxiv.org/pdf/2505.23564", "abs": "https://arxiv.org/abs/2505.23564", "authors": ["Yiran Guo", "Lijie Xu", "Jie Liu", "Dan Ye", "Shuang Qiu"], "title": "Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Enhancing the reasoning capabilities of large language models effectively\nusing reinforcement learning (RL) remains a crucial challenge. Existing\napproaches primarily adopt two contrasting advantage estimation granularities:\nToken-level methods (e.g., PPO) aim to provide the fine-grained advantage\nsignals but suffer from inaccurate estimation due to difficulties in training\nan accurate critic model. On the other extreme, trajectory-level methods (e.g.,\nGRPO) solely rely on a coarse-grained advantage signal from the final reward,\nleading to imprecise credit assignment. To address these limitations, we\npropose Segment Policy Optimization (SPO), a novel RL framework that leverages\nsegment-level advantage estimation at an intermediate granularity, achieving a\nbetter balance by offering more precise credit assignment than trajectory-level\nmethods and requiring fewer estimation points than token-level methods,\nenabling accurate advantage estimation based on Monte Carlo (MC) without a\ncritic model. SPO features three components with novel strategies: (1) flexible\nsegment partition; (2) accurate segment advantage estimation; and (3) policy\noptimization using segment advantages, including a novel probability-mask\nstrategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain\nfor short chain-of-thought (CoT), featuring novel cutpoint-based partition and\nchain-based advantage estimation, achieving $6$-$12$ percentage point\nimprovements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT,\nfeaturing novel tree-based advantage estimation, which significantly reduces\nthe cost of MC estimation, achieving $7$-$11$ percentage point improvements\nover GRPO on MATH500 under 2K and 4K context evaluation. We make our code\npublicly available at https://github.com/AIFrameResearch/SPO.", "AI": {"tldr": "\u63d0\u51fa\u4e86Segment Policy Optimization (SPO)\uff0c\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e2d\u7b49\u7c92\u5ea6\u7684\u5206\u6bb5\u7ea7\u4f18\u52bf\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4f18\u52bf\u4f30\u8ba1\u7c92\u5ea6\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u4e24\u79cd\u5177\u4f53\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4f18\u52bf\u4f30\u8ba1\u7c92\u5ea6\u4e0a\u7684\u4e0d\u8db3\uff08\u7ec6\u7c92\u5ea6\u4f30\u8ba1\u4e0d\u51c6\u786e\uff0c\u7c97\u7c92\u5ea6\u4f30\u8ba1\u4e0d\u7cbe\u786e\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2d\u7b49\u7c92\u5ea6\u7684\u5206\u6bb5\u7ea7\u4f18\u52bf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u5e73\u8861\u7cbe\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86SPO\u6846\u67b6\uff0c\u5305\u62ec\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u7075\u6d3b\u7684\u5206\u6bb5\u5212\u5206\u7b56\u7565\uff1b(2) \u7cbe\u786e\u7684\u5206\u6bb5\u4f18\u52bf\u4f30\u8ba1\uff1b(3) \u4f7f\u7528\u5206\u6bb5\u4f18\u52bf\u8fdb\u884c\u7b56\u7565\u4f18\u5316\uff08\u542b\u65b0\u7684\u6982\u7387\u63a9\u7801\u7b56\u7565\uff09\u3002\u5e76\u9488\u5bf9\u77ed\u94fe\u548c\u957f\u94fe\u601d\u7ef4\uff08CoT\uff09\u573a\u666f\u5206\u522b\u8bbe\u8ba1\u4e86SPO-chain\u548cSPO-tree\u3002", "result": "\u5728GSM8K\u6570\u636e\u96c6\u4e0a\uff0cSPO-chain\u6bd4PPO\u548cGRPO\u51c6\u786e\u7387\u63d0\u9ad86-12\u4e2a\u767e\u5206\u70b9\uff1b\u5728MATH500\u6570\u636e\u96c6\u4e0a\uff0cSPO-tree\u57282K\u548c4K\u4e0a\u4e0b\u6587\u8bc4\u4f30\u4e2d\u6bd4GRPO\u63d0\u9ad87-11\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "SPO\u901a\u8fc7\u5206\u6bb5\u7ea7\u4f18\u52bf\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60\uff0c\u4f18\u52bf\u4f30\u8ba1\uff0c\u5206\u6bb5\u7b56\u7565\u4f18\u5316\uff0c\u601d\u7ef4\u94fe\uff0c\u8bed\u8a00\u6a21\u578b\u63a8\u7406"}}
{"id": "2505.23266", "pdf": "https://arxiv.org/pdf/2505.23266", "abs": "https://arxiv.org/abs/2505.23266", "authors": ["Chunlong Xie", "Jialing He", "Shangwei Guo", "Jiacheng Wang", "Shudong Zhang", "Tianwei Zhang", "Tao Xiang"], "title": "Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": "Under review", "summary": "We present Adversarial Object Fusion (AdvOF), a novel attack framework\ntargeting vision-and-language navigation (VLN) agents in service-oriented\nenvironments by generating adversarial 3D objects. While foundational models\nlike Large Language Models (LLMs) and Vision Language Models (VLMs) have\nenhanced service-oriented navigation systems through improved perception and\ndecision-making, their integration introduces vulnerabilities in\nmission-critical service workflows. Existing adversarial attacks fail to\naddress service computing contexts, where reliability and quality-of-service\n(QoS) are paramount. We utilize AdvOF to investigate and explore the impact of\nadversarial environments on the VLM-based perception module of VLN agents. In\nparticular, AdvOF first precisely aggregates and aligns the victim object\npositions in both 2D and 3D space, defining and rendering adversarial objects.\nThen, we collaboratively optimize the adversarial object with regularization\nbetween the adversarial and victim object across physical properties and VLM\nperceptions. Through assigning importance weights to varying views, the\noptimization is processed stably and multi-viewedly by iterative fusions from\nlocal updates and justifications. Our extensive evaluations demonstrate AdvOF\ncan effectively degrade agent performance under adversarial conditions while\nmaintaining minimal interference with normal navigation tasks. This work\nadvances the understanding of service security in VLM-powered navigation\nsystems, providing computational foundations for robust service composition in\nphysical-world deployments.", "AI": {"tldr": "AdvOF\u662f\u4e00\u79cd\u65b0\u7684\u653b\u51fb\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u670d\u52a1\u5bfc\u5411\u73af\u5883\u4e2d\u9488\u5bf9\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u4ee3\u7406\u751f\u6210\u5bf9\u6297\u60273D\u7269\u4f53\uff0c\u63ed\u793a\u4e86VLN\u4ee3\u7406\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u6574\u5408\u63d0\u5347\u4e86\u670d\u52a1\u5bfc\u5411\u5bfc\u822a\u7cfb\u7edf\u7684\u611f\u77e5\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u5728\u5173\u952e\u4efb\u52a1\u670d\u52a1\u6d41\u7a0b\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u672a\u80fd\u89e3\u51b3\u670d\u52a1\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u548c\u670d\u52a1\u8d28\u91cf\u9700\u6c42\u3002", "method": "AdvOF\u901a\u8fc7\u7cbe\u786e\u805a\u5408\u548c\u5bf9\u9f50\u76ee\u6807\u7269\u4f53\u57282D\u548c3D\u7a7a\u95f4\u4e2d\u7684\u4f4d\u7f6e\uff0c\u5b9a\u4e49\u5e76\u6e32\u67d3\u5bf9\u6297\u6027\u7269\u4f53\uff0c\u901a\u8fc7\u8de8\u7269\u7406\u5c5e\u6027\u548cVLM\u611f\u77e5\u7684\u534f\u540c\u4f18\u5316\uff0c\u7a33\u5b9a\u5904\u7406\u591a\u89c6\u56fe\u8fed\u4ee3\u878d\u5408\u7684\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cAdvOF\u80fd\u6709\u6548\u964d\u4f4e\u4ee3\u7406\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u540c\u65f6\u5bf9\u6b63\u5e38\u5bfc\u822a\u4efb\u52a1\u7684\u5e72\u6270\u6700\u5c0f\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u5bf9VLM\u9a71\u52a8\u5bfc\u822a\u7cfb\u7edf\u670d\u52a1\u5b89\u5168\u7684\u7406\u89e3\uff0c\u4e3a\u7269\u7406\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u7a33\u5065\u670d\u52a1\u7ec4\u5408\u63d0\u4f9b\u4e86\u8ba1\u7b97\u57fa\u7840\u3002", "keywords": "\u5bf9\u6297\u6027\u653b\u51fb,\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a,3D\u7269\u4f53\u751f\u6210,\u670d\u52a1\u5b89\u5168,\u7a33\u5065\u6027"}}
{"id": "2505.23565", "pdf": "https://arxiv.org/pdf/2505.23565", "abs": "https://arxiv.org/abs/2505.23565", "authors": ["Jiashuo Liu", "Tianyu Wang", "Henry Lam", "Hongseok Namkoong", "Jose Blanchet"], "title": "DRO: A Python Library for Distributionally Robust Optimization in Machine Learning", "categories": ["cs.LG", "cs.MS", "cs.NA", "math.NA"], "comment": null, "summary": "We introduce dro, an open-source Python library for distributionally robust\noptimization (DRO) for regression and classification problems. The library\nimplements 14 DRO formulations and 9 backbone models, enabling 79 distinct DRO\nmethods. Furthermore, dro is compatible with both scikit-learn and PyTorch.\nThrough vectorization and optimization approximation techniques, dro reduces\nruntime by 10x to over 1000x compared to baseline implementations on\nlarge-scale datasets. Comprehensive documentation is available at\nhttps://python-dro.org.", "AI": {"tldr": "dro is an open-source Python library for distributionally robust optimization (DRO) in regression and classification, offering multiple formulations and models with significant runtime improvements.", "motivation": "To provide a versatile and efficient tool for distributionally robust optimization, addressing the need for robustness in machine learning models against distribution shifts.", "method": "The library implements 14 DRO formulations and 9 backbone models, supporting both scikit-learn and PyTorch, with optimizations for runtime.", "result": "Achieves 10x to over 1000x runtime reduction compared to baseline implementations on large-scale datasets.", "conclusion": "dro is a comprehensive and efficient solution for DRO, compatible with popular ML frameworks and well-documented.", "keywords": "distributionally robust optimization, Python library, regression, classification, scikit-learn, PyTorch"}}
{"id": "2505.23267", "pdf": "https://arxiv.org/pdf/2505.23267", "abs": "https://arxiv.org/abs/2505.23267", "authors": ["Jianlin Ye", "Savvas Papaioannou", "Panayiotis Kolios"], "title": "VLM-RRT: Vision Language Model Guided RRT Search for Autonomous UAV Navigation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Path planning is a fundamental capability of autonomous Unmanned Aerial\nVehicles (UAVs), enabling them to efficiently navigate toward a target region\nor explore complex environments while avoiding obstacles. Traditional\npathplanning methods, such as Rapidly-exploring Random Trees (RRT), have proven\neffective but often encounter significant challenges. These include high search\nspace complexity, suboptimal path quality, and slow convergence, issues that\nare particularly problematic in high-stakes applications like disaster\nresponse, where rapid and efficient planning is critical. To address these\nlimitations and enhance path-planning efficiency, we propose Vision Language\nModel RRT (VLM-RRT), a hybrid approach that integrates the pattern recognition\ncapabilities of Vision Language Models (VLMs) with the path-planning strengths\nof RRT. By leveraging VLMs to provide initial directional guidance based on\nenvironmental snapshots, our method biases sampling toward regions more likely\nto contain feasible paths, significantly improving sampling efficiency and path\nquality. Extensive quantitative and qualitative experiments with various\nstate-of-the-art VLMs demonstrate the effectiveness of this proposed approach.", "AI": {"tldr": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5VLM-RRT\uff0c\u7ed3\u5408\u4e86RLT\u548cVLMs\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5982RRT\u5728\u9ad8\u590d\u6742\u5ea6\u641c\u7d22\u7a7a\u95f4\u3001\u8def\u5f84\u8d28\u91cf\u4e0d\u4f73\u548c\u6536\u655b\u901f\u5ea6\u6162\u7b49\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0c\u5c24\u5176\u5728\u707e\u5bb3\u54cd\u5e94\u7b49\u5173\u952e\u5e94\u7528\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u63d0\u51faVLM-RRT\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6a21\u5f0f\u8bc6\u522b\u80fd\u529b\u4e3aRRT\u63d0\u4f9b\u521d\u59cb\u65b9\u5411\u5f15\u5bfc\uff0c\u4ece\u800c\u4f18\u5316\u91c7\u6837\u6548\u7387\u4e0e\u8def\u5f84\u8d28\u91cf\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cVLM-RRT\u5728\u5404\u79cd\u524d\u6cbf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0b\u5747\u8868\u73b0\u51fa\u663e\u8457\u6548\u679c\u3002", "conclusion": "VLM-RRT\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e2d\u8def\u5f84\u89c4\u5212\u7684\u6548\u7387\u548c\u9002\u7528\u6027\u3002", "keywords": "\u8def\u5f84\u89c4\u5212,\u65e0\u4eba\u673a,RRT,\u89c6\u89c9\u8bed\u8a00\u6a21\u578b,VLM-RRT"}}
{"id": "2505.23569", "pdf": "https://arxiv.org/pdf/2505.23569", "abs": "https://arxiv.org/abs/2505.23569", "authors": ["Samo Hromadka", "Kai Biegun", "Lior Fox", "James Heald", "Maneesh Sahani"], "title": "Maximum Likelihood Learning of Latent Dynamics Without Reconstruction", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We introduce a novel unsupervised learning method for time series data with\nlatent dynamical structure: the recognition-parametrized Gaussian state space\nmodel (RP-GSSM). The RP-GSSM is a probabilistic model that learns Markovian\nGaussian latents explaining statistical dependence between observations at\ndifferent time steps, combining the intuition of contrastive methods with the\nflexible tools of probabilistic generative models. Unlike contrastive\napproaches, the RP-GSSM is a valid probabilistic model learned via maximum\nlikelihood. Unlike generative approaches, the RP-GSSM has no need for an\nexplicit network mapping from latents to observations, allowing it to focus\nmodel capacity on inference of latents. The model is both tractable and\nexpressive: it admits exact inference thanks to its jointly Gaussian latent\nprior, while maintaining expressivity with an arbitrarily nonlinear neural\nnetwork link between observations and latents. These qualities allow the\nRP-GSSM to learn task-relevant latents without ad-hoc regularization, auxiliary\nlosses, or optimizer scheduling. We show how this approach outperforms\nalternatives on problems that include learning nonlinear stochastic dynamics\nfrom video, with or without background distractors. Our results position the\nRP-GSSM as a useful foundation model for a variety of downstream applications.", "AI": {"tldr": "RP-GSSM\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u5b66\u4e60\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u5bf9\u6bd4\u65b9\u6cd5\u7684\u76f4\u89c9\u548c\u6982\u7387\u751f\u6210\u6a21\u578b\u7684\u7075\u6d3b\u6027\uff0c\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u5b66\u4e60\u9ad8\u65af\u6f5c\u5728\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u65e0\u9700\u663e\u5f0f\u7f51\u7edc\u6620\u5c04\uff0c\u4e14\u80fd\u51c6\u786e\u63a8\u65ad\u6f5c\u5728\u53d8\u91cf\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u6f5c\u5728\u52a8\u6001\u7ed3\u6784\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u95ee\u9898\uff0c\u7ed3\u5408\u5bf9\u6bd4\u65b9\u6cd5\u548c\u751f\u6210\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u63d0\u51faRP-GSSM\u6a21\u578b\u3002", "method": "RP-GSSM\u662f\u4e00\u79cd\u6982\u7387\u6a21\u578b\uff0c\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u5b66\u4e60\u9ad8\u65af\u6f5c\u5728\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5229\u7528\u975e\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc\u94fe\u63a5\u89c2\u6d4b\u4e0e\u6f5c\u5728\u53d8\u91cf\uff0c\u65e0\u9700\u663e\u5f0f\u6620\u5c04\u3002", "result": "RP-GSSM\u5728\u975e\u7ebf\u6027\u968f\u673a\u52a8\u6001\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u89c6\u9891\u6570\u636e\uff08\u542b\u6216\u4e0d\u542b\u80cc\u666f\u5e72\u6270\uff09\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "RP-GSSM\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u5e94\u7528\u3002", "keywords": "\u65e0\u76d1\u7763\u5b66\u4e60,\u65f6\u95f4\u5e8f\u5217,\u9ad8\u65af\u72b6\u6001\u7a7a\u95f4\u6a21\u578b,\u6982\u7387\u751f\u6210\u6a21\u578b"}}
{"id": "2505.23268", "pdf": "https://arxiv.org/pdf/2505.23268", "abs": "https://arxiv.org/abs/2505.23268", "authors": ["Spyros Barbakos", "Charalampos Antoniadis", "Gerasimos Potamianos", "Gianluca Setti"], "title": "Unsupervised Transcript-assisted Video Summarization and Highlight Detection", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Video consumption is a key part of daily life, but watching entire videos can\nbe tedious. To address this, researchers have explored video summarization and\nhighlight detection to identify key video segments. While some works combine\nvideo frames and transcripts, and others tackle video summarization and\nhighlight detection using Reinforcement Learning (RL), no existing work, to the\nbest of our knowledge, integrates both modalities within an RL framework. In\nthis paper, we propose a multimodal pipeline that leverages video frames and\ntheir corresponding transcripts to generate a more condensed version of the\nvideo and detect highlights using a modality fusion mechanism. The pipeline is\ntrained within an RL framework, which rewards the model for generating diverse\nand representative summaries while ensuring the inclusion of video segments\nwith meaningful transcript content. The unsupervised nature of the training\nallows for learning from large-scale unannotated datasets, overcoming the\nchallenge posed by the limited size of existing annotated datasets. Our\nexperiments show that using the transcript in video summarization and highlight\ndetection achieves superior results compared to relying solely on the visual\ncontent of the video.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u9891\u5e27\u548c\u6587\u672c\u8f6c\u5f55\u7684\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u9891\u6458\u8981\u548c\u9ad8\u4eae\u68c0\u6d4b\uff0c\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u89c6\u89c9\u5185\u5bb9\u7684\u65b9\u6cd5\u3002", "motivation": "\u89c6\u9891\u6d88\u8d39\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5b8c\u6574\u89c2\u770b\u8017\u65f6\uff1b\u73b0\u6709\u65b9\u6cd5\u591a\u672a\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u89c6\u9891\u5e27\u4e0e\u8f6c\u5f55\uff09\u4e8e\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u7ba1\u9053\u6574\u5408\u89c6\u9891\u5e27\u548c\u8f6c\u5f55\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u6a21\u578b\uff0c\u5956\u52b1\u751f\u6210\u591a\u6837\u4e14\u5177\u4ee3\u8868\u6027\u7684\u6458\u8981\uff0c\u5e76\u786e\u4fdd\u5305\u542b\u6709\u610f\u4e49\u6587\u672c\u5185\u5bb9\u7684\u89c6\u9891\u7247\u6bb5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u8f6c\u5f55\u7684\u89c6\u9891\u6458\u8981\u548c\u9ad8\u4eae\u68c0\u6d4b\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u89c6\u89c9\u5185\u5bb9\u7684\u65b9\u6cd5\u3002", "conclusion": "\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u6458\u8981\u548c\u9ad8\u4eae\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u4e14\u80fd\u5229\u7528\u5927\u89c4\u6a21\u672a\u6807\u6ce8\u6570\u636e\u514b\u670d\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "keywords": "\u89c6\u9891\u6458\u8981\u3001\u9ad8\u4eae\u68c0\u6d4b\u3001\u591a\u6a21\u6001\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u65e0\u76d1\u7763\u8bad\u7ec3"}}
{"id": "2505.23579", "pdf": "https://arxiv.org/pdf/2505.23579", "abs": "https://arxiv.org/abs/2505.23579", "authors": ["Adibvafa Fallahpour", "Andrew Magnuson", "Purav Gupta", "Shihao Ma", "Jack Naimer", "Arnav Shah", "Haonan Duan", "Omar Ibrahim", "Hani Goodarzi", "Chris J. Maddison", "Bo Wang"], "title": "BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model", "categories": ["cs.LG"], "comment": "16 pages, 3 figures, 2 tables", "summary": "Unlocking deep, interpretable biological reasoning from complex genomic data\nis a major AI challenge hindering scientific discovery. Current DNA foundation\nmodels, despite strong sequence representation, struggle with multi-step\nreasoning and lack inherent transparent, biologically intuitive explanations.\nWe introduce BioReason, a pioneering architecture that, for the first time,\ndeeply integrates a DNA foundation model with a Large Language Model (LLM).\nThis novel connection enables the LLM to directly process and reason with\ngenomic information as a fundamental input, fostering a new form of multimodal\nbiological understanding. BioReason's sophisticated multi-step reasoning is\ndeveloped through supervised fine-tuning and targeted reinforcement learning,\nguiding the system to generate logical, biologically coherent deductions. On\nbiological reasoning benchmarks including KEGG-based disease pathway prediction\n- where accuracy improves from 88% to 97% - and variant effect prediction,\nBioReason demonstrates an average 15% performance gain over strong\nsingle-modality baselines. BioReason reasons over unseen biological entities\nand articulates decision-making through interpretable, step-by-step biological\ntraces, offering a transformative approach for AI in biology that enables\ndeeper mechanistic insights and accelerates testable hypothesis generation from\ngenomic data. Data, code, and checkpoints are publicly available at\nhttps://github.com/bowang-lab/BioReason", "AI": {"tldr": "BioReason\u662f\u4e00\u79cd\u7ed3\u5408DNA\u57fa\u7840\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u57fa\u56e0\u7ec4\u6570\u636e\u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\u548c\u900f\u660e\u89e3\u91ca\u6027\uff0c\u5728\u75be\u75c5\u901a\u8def\u9884\u6d4b\u7b49\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u534715%\u3002", "motivation": "\u5f53\u524dDNA\u57fa\u7840\u6a21\u578b\u5728\u591a\u6b65\u63a8\u7406\u548c\u900f\u660e\u89e3\u91ca\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u963b\u788d\u4e86\u79d1\u5b66\u53d1\u73b0\u3002", "method": "\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5c06DNA\u57fa\u7840\u6a21\u578b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6df1\u5ea6\u878d\u5408\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u751f\u7269\u7406\u89e3\u3002", "result": "\u5728KEGG\u75be\u75c5\u901a\u8def\u9884\u6d4b\u7b49\u4efb\u52a1\u4e2d\uff0c\u51c6\u786e\u7387\u4ece88%\u63d0\u5347\u81f397%\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u534715%\u3002", "conclusion": "BioReason\u4e3a\u751f\u7269\u5b66AI\u63d0\u4f9b\u4e86\u66f4\u6df1\u7684\u673a\u5236\u6d1e\u5bdf\u548c\u53ef\u89e3\u91ca\u7684\u5047\u8bbe\u751f\u6210\u80fd\u529b\u3002", "keywords": "BioReason, DNA\u57fa\u7840\u6a21\u578b, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u591a\u6b65\u63a8\u7406, \u57fa\u56e0\u7ec4\u6570\u636e"}}
{"id": "2505.23583", "pdf": "https://arxiv.org/pdf/2505.23583", "abs": "https://arxiv.org/abs/2505.23583", "authors": ["Zhiding Liu", "Mingyue Cheng", "Guanhao Zhao", "Jiqian Yang", "Qi Liu", "Enhong Chen"], "title": "Improving Time Series Forecasting via Instance-aware Post-hoc Revision", "categories": ["cs.LG"], "comment": null, "summary": "Time series forecasting plays a vital role in various real-world applications\nand has attracted significant attention in recent decades. While recent methods\nhave achieved remarkable accuracy by incorporating advanced inductive biases\nand training strategies, we observe that instance-level variations remain a\nsignificant challenge. These variations--stemming from distribution shifts,\nmissing data, and long-tail patterns--often lead to suboptimal forecasts for\nspecific instances, even when overall performance appears strong. To address\nthis issue, we propose a model-agnostic framework, PIR, designed to enhance\nforecasting performance through Post-forecasting Identification and Revision.\nSpecifically, PIR first identifies biased forecasting instances by estimating\ntheir accuracy. Based on this, the framework revises the forecasts using\ncontextual information, including covariates and historical time series, from\nboth local and global perspectives in a post-processing fashion. Extensive\nexperiments on real-world datasets with mainstream forecasting models\ndemonstrate that PIR effectively mitigates instance-level errors and\nsignificantly improves forecasting reliability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6PIR\uff0c\u901a\u8fc7\u540e\u9884\u6d4b\u8bc6\u522b\u548c\u4fee\u6b63\u6765\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\uff0c\u89e3\u51b3\u5b9e\u4f8b\u7ea7\u53d8\u5316\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5728\u5b9e\u4f8b\u7ea7\u53d8\u5316\uff08\u5982\u5206\u5e03\u504f\u79fb\u3001\u7f3a\u5931\u6570\u636e\u3001\u957f\u5c3e\u6a21\u5f0f\uff09\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0cPIR\u65e8\u5728\u901a\u8fc7\u540e\u5904\u7406\u63d0\u5347\u9884\u6d4b\u53ef\u9760\u6027\u3002", "method": "PIR\u6846\u67b6\u901a\u8fc7\u540e\u9884\u6d4b\u8bc6\u522b\uff08\u57fa\u4e8e\u51c6\u786e\u6027\u4f30\u8ba1\uff09\u548c\u4fee\u6b63\uff08\u5229\u7528\u5c40\u90e8\u4e0e\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\uff09\u6765\u4f18\u5316\u9884\u6d4b\u7ed3\u679c\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cPIR\u663e\u8457\u51cf\u5c11\u4e86\u5b9e\u4f8b\u7ea7\u8bef\u5dee\u5e76\u63d0\u5347\u4e86\u9884\u6d4b\u53ef\u9760\u6027\u3002", "conclusion": "PIR\u662f\u4e00\u79cd\u6709\u6548\u7684\u540e\u5904\u7406\u6846\u67b6\uff0c\u80fd\u591f\u63d0\u5347\u591a\u79cd\u4e3b\u6d41\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b,\u5b9e\u4f8b\u7ea7\u53d8\u5316,\u540e\u5904\u7406,\u6a21\u578b\u65e0\u5173,\u9884\u6d4b\u4fee\u6b63"}}
{"id": "2505.23339", "pdf": "https://arxiv.org/pdf/2505.23339", "abs": "https://arxiv.org/abs/2505.23339", "authors": ["Maya Dewhurst", "Jack Collins", "Justin J. H. Lo", "Roy Alderton", "Sam Kirkham"], "title": "Nosey: Open-source hardware for acoustic nasalance", "categories": ["cs.SD", "cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "We introduce Nosey (Nasalance Open Source Estimation sYstem), a low-cost,\ncustomizable, 3D-printed system for recording acoustic nasalance data that we\nhave made available as open-source hardware\n(http://github.com/phoneticslab/nosey). We first outline the motivations and\ndesign principles behind our hardware nasalance system, and then present a\ncomparison between Nosey and a commercial nasalance device. Nosey shows\nconsistently higher nasalance scores than the commercial device, but the\nmagnitude of contrast between phonological environments is comparable between\nsystems. We also review ways of customizing the hardware to facilitate testing,\nsuch as comparison of microphones and different construction materials. We\nconclude that Nosey is a flexible and cost-effective alternative to commercial\nnasometry devices and propose some methodological considerations for its use in\ndata collection.", "AI": {"tldr": "Nosey\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u53ef\u5b9a\u5236\u7684\u5f00\u6e90\u786c\u4ef6\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bb0\u5f55\u58f0\u5b66\u9f3b\u97f3\u6570\u636e\uff0c\u76f8\u6bd4\u5546\u7528\u8bbe\u5907\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8bbe\u8ba1\u4f4e\u6210\u672c\u3001\u5f00\u6e90\u7684\u9f3b\u97f3\u6d4b\u91cf\u8bbe\u5907\uff0c\u4ee5\u66ff\u4ee3\u6602\u8d35\u7684\u5546\u7528\u9f3b\u6d4b\u8ba1\u3002", "method": "\u5f00\u53d13D\u6253\u5370\u7684\u786c\u4ef6\u7cfb\u7edfNosey\uff0c\u5e76\u5bf9\u6bd4\u5176\u4e0e\u5546\u7528\u8bbe\u5907\u7684\u6027\u80fd\u3002", "result": "Nosey\u663e\u793a\u66f4\u9ad8\u7684\u9f3b\u97f3\u5f97\u5206\uff0c\u4f46\u4e0d\u540c\u7cfb\u7edf\u95f4\u7684\u97f3\u7cfb\u73af\u5883\u5bf9\u6bd4\u5e45\u5ea6\u76f8\u4f3c\u3002", "conclusion": "Nosey\u662f\u5546\u7528\u9f3b\u6d4b\u8ba1\u7684\u7075\u6d3b\u7ecf\u6d4e\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u5408\u6570\u636e\u6536\u96c6\u3002", "keywords": "Nosey, \u9f3b\u97f3\u6d4b\u91cf, \u5f00\u6e90\u786c\u4ef6, 3D\u6253\u5370, \u4f4e\u6210\u672c"}}
{"id": "2505.23585", "pdf": "https://arxiv.org/pdf/2505.23585", "abs": "https://arxiv.org/abs/2505.23585", "authors": ["Yaru Hao", "Li Dong", "Xun Wu", "Shaohan Huang", "Zewen Chi", "Furu Wei"], "title": "On-Policy RL with Optimal Reward Baseline", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reinforcement learning algorithms are fundamental to align large language\nmodels with human preferences and to enhance their reasoning capabilities.\nHowever, current reinforcement learning algorithms often suffer from training\ninstability due to loose on-policy constraints and computational inefficiency\ndue to auxiliary models. In this work, we propose On-Policy RL with Optimal\nreward baseline (OPO), a novel and simplified reinforcement learning algorithm\ndesigned to address these challenges. OPO emphasizes the importance of exact\non-policy training, which empirically stabilizes the training process and\nenhances exploration. Moreover, OPO introduces the optimal reward baseline that\ntheoretically minimizes gradient variance. We evaluate OPO on mathematical\nreasoning benchmarks. The results demonstrate its superior performance and\ntraining stability without additional models or regularization terms.\nFurthermore, OPO achieves lower policy shifts and higher output entropy,\nencouraging more diverse and less repetitive responses. These results highlight\nOPO as a promising direction for stable and effective reinforcement learning in\nlarge language model alignment and reasoning tasks. The implementation is\nprovided at https://github.com/microsoft/LMOps/tree/main/opo.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5OPO\uff0c\u901a\u8fc7\u4e25\u683c\u7684\u540c\u7b56\u7565\u8bad\u7ec3\u548c\u6700\u4f18\u5956\u52b1\u57fa\u7ebf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7b97\u6cd5\u7684\u4e0d\u7a33\u5b9a\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u517c\u5177\u7a33\u5b9a\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u5b58\u5728\u7a33\u5b9a\u6027\u4e0d\u8db3\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u6d01\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faOPO\u7b97\u6cd5\uff0c\u5f3a\u8c03\u4e25\u683c\u7684\u540c\u7b56\u7565\u8bad\u7ec3\u548c\u7406\u8bba\u6700\u5c0f\u5316\u68af\u5ea6\u65b9\u5dee\u7684\u6700\u4f18\u5956\u52b1\u57fa\u7ebf\u3002", "result": "OPO\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u66f4\u4f4e\u7684\u7b56\u7565\u6f02\u79fb\u548c\u66f4\u9ad8\u7684\u8f93\u51fa\u71b5\uff0c\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u7ed3\u679c\u3002", "conclusion": "OPO\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u548c\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5b9a\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u5411\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60,\u5927\u8bed\u8a00\u6a21\u578b,\u540c\u7b56\u7565\u8bad\u7ec3,\u6700\u4f18\u5956\u52b1\u57fa\u7ebf,\u6570\u5b66\u63a8\u7406"}}
{"id": "2505.23419", "pdf": "https://arxiv.org/pdf/2505.23419", "abs": "https://arxiv.org/abs/2505.23419", "authors": ["Linghao Zhang", "Shilin He", "Chaoyun Zhang", "Yu Kang", "Bowen Li", "Chengxing Xie", "Junhao Wang", "Maoquan Wang", "Yufan Huang", "Shengyu Fu", "Elsie Nallipogu", "Qingwei Lin", "Yingnong Dang", "Saravan Rajmohan", "Dongmei Zhang"], "title": "SWE-bench Goes Live!", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "Homepage: \\url{https://swe-bench-live.github.io/}, Code:\n  \\url{https://github.com/SWE-bench-Live}, Dataset:\n  \\url{https://huggingface.co/SWE-bench-Live}", "summary": "The issue-resolving task, where a model generates patches to fix real-world\nbugs, has emerged as a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs). While SWE-bench and its variants have become\nstandard in this domain, they suffer from key limitations: they have not been\nupdated since their initial releases, cover a narrow set of repositories, and\ndepend heavily on manual effort for instance construction and environment\nsetup. These factors hinder scalability and introduce risks of overfitting and\ndata contamination. In this work, we present \\textbf{SWE-bench-Live}, a\n\\textit{live-updatable} benchmark designed to overcome these challenges. Our\ninitial release consists of 1,319 tasks derived from real GitHub issues created\nsince 2024, spanning 93 repositories. Each task is accompanied by a dedicated\nDocker image to ensure reproducible execution. Central to our benchmark is\n\\method, an automated curation pipeline that streamlines the entire process\nfrom instance creation to environment setup, removing manual bottlenecks and\nenabling scalability and continuous updates. We evaluate a range of\nstate-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a\nsubstantial performance gap compared to static benchmarks like SWE-bench, even\nunder controlled evaluation conditions. To better understand this discrepancy,\nwe perform detailed analyses across repository origin, issue recency, and task\ndifficulty. By providing a fresh, diverse, and executable benchmark grounded in\nlive repository activity, SWE-bench-Live facilitates rigorous,\ncontamination-resistant evaluation of LLMs and agents in dynamic, real-world\nsoftware development settings.", "AI": {"tldr": "SWE-bench-Live\u662f\u4e00\u4e2a\u52a8\u6001\u66f4\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709SWE-bench\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u591a\u6837\u5316\u7684\u5b9e\u65f6GitHub\u95ee\u9898\u4efb\u52a1\u548c\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u8f6f\u4ef6\u4fee\u590d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684SWE-bench\u53ca\u5176\u53d8\u4f53\u5b58\u5728\u66f4\u65b0\u4e0d\u53ca\u65f6\u3001\u8986\u76d6\u8303\u56f4\u7a84\u3001\u4f9d\u8d56\u4eba\u5de5\u7b49\u9650\u5236\uff0c\u5f71\u54cd\u5176\u6269\u5c55\u6027\u548c\u6297\u8fc7\u62df\u5408\u80fd\u529b\u3002", "method": "\u63d0\u51faSWE-bench-Live\uff0c\u91c7\u7528\u81ea\u52a8\u5316\u7ba1\u9053\uff08\\method\uff09\u4ece\u5b9e\u65f6GitHub\u95ee\u9898\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u63d0\u4f9b\u4e13\u7528\u7684Docker\u955c\u50cf\u4ee5\u786e\u4fdd\u53ef\u91cd\u590d\u6027\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0c\u5f53\u524d\u5148\u8fdb\u4ee3\u7406\u6846\u67b6\u548cLLM\u5728\u52a8\u6001\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u4e0e\u9759\u6001\u57fa\u51c6\uff08\u5982SWE-bench\uff09\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "SWE-bench-Live\u901a\u8fc7\u63d0\u4f9b\u52a8\u6001\u3001\u591a\u6837\u4e14\u53ef\u6267\u884c\u7684\u57fa\u51c6\uff0c\u652f\u6301\u5bf9LLM\u548c\u4ee3\u7406\u5728\u771f\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u73af\u5883\u4e2d\u7684\u4e25\u683c\u8bc4\u4f30\u3002", "keywords": "SWE-bench-Live, LLM\u8bc4\u4f30, \u8f6f\u4ef6\u4fee\u590d, \u52a8\u6001\u57fa\u51c6, GitHub\u95ee\u9898"}}
{"id": "2505.23588", "pdf": "https://arxiv.org/pdf/2505.23588", "abs": "https://arxiv.org/abs/2505.23588", "authors": ["Mrinmay Sen", "Sidhant R Nair", "C Krishna Mohan"], "title": "Accelerated Training of Federated Learning via Second-Order Methods", "categories": ["cs.LG", "cs.DC", "68Q25, 68T05, 90C06, 90C25, 90C30", "I.2.6; G.1.6; C.2.4; C.4"], "comment": "17 pages, 1 figure, 4 tables, submitted to IEEE Transactions on\n  Pattern Analysis and Machine Intelligence (T-PAMI)", "summary": "This paper explores second-order optimization methods in Federated Learning\n(FL), addressing the critical challenges of slow convergence and the excessive\ncommunication rounds required to achieve optimal performance from the global\nmodel. While existing surveys in FL primarily focus on challenges related to\nstatistical and device label heterogeneity, as well as privacy and security\nconcerns in first-order FL methods, less attention has been given to the issue\nof slow model training. This slow training often leads to the need for\nexcessive communication rounds or increased communication costs, particularly\nwhen data across clients are highly heterogeneous. In this paper, we examine\nvarious FL methods that leverage second-order optimization to accelerate the\ntraining process. We provide a comprehensive categorization of state-of-the-art\nsecond-order FL methods and compare their performance based on convergence\nspeed, computational cost, memory usage, transmission overhead, and\ngeneralization of the global model. Our findings show the potential of\nincorporating Hessian curvature through second-order optimization into FL and\nhighlight key challenges, such as the efficient utilization of Hessian and its\ninverse in FL. This work lays the groundwork for future research aimed at\ndeveloping scalable and efficient federated optimization methods for improving\nthe training of the global model in FL.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u8054\u90a6\u5b66\u4e60(FL)\u4e2d\u7684\u4e8c\u9636\u4f18\u5316\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u6536\u655b\u6162\u548c\u901a\u4fe1\u8f6e\u6b21\u8fc7\u591a\u7684\u95ee\u9898\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7edf\u8ba1\u548c\u8bbe\u5907\u6807\u7b7e\u5f02\u8d28\u6027\u4ee5\u53ca\u9690\u79c1\u5b89\u5168\uff0c\u800c\u5bf9\u8bad\u7ec3\u901f\u5ea6\u5173\u6ce8\u8f83\u5c11\u3002\u6587\u7ae0\u5206\u7c7b\u6bd4\u8f83\u4e86\u591a\u79cd\u4e8c\u9636FL\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u5176\u52a0\u901f\u8bad\u7ec3\u7684\u6f5c\u529b\u53ca\u6311\u6218\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u8bad\u7ec3\u901f\u5ea6\u6162\u548c\u901a\u4fe1\u6210\u672c\u9ad8\u662f\u4e3b\u8981\u6311\u6218\uff0c\u5c24\u5176\u662f\u6570\u636e\u5f02\u8d28\u6027\u4e25\u91cd\u65f6\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u9690\u79c1\u548c\u7edf\u8ba1\u5f02\u8d28\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u4f18\u5316\u65b9\u6cd5\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u672c\u6587\u63a2\u8ba8\u4e8c\u9636\u4f18\u5316\u7684\u6f5c\u529b\u3002", "method": "\u6587\u7ae0\u7cfb\u7edf\u5206\u7c7b\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u4f7f\u7528\u4e8c\u9636\u4f18\u5316\u7684FL\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u5b83\u4eec\u7684\u6536\u655b\u901f\u5ea6\u3001\u8ba1\u7b97\u6210\u672c\u3001\u5185\u5b58\u5360\u7528\u3001\u4f20\u8f93\u5f00\u9500\u548c\u5168\u5c40\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e8c\u9636\u4f18\u5316\u80fd\u6709\u6548\u52a0\u901fFL\u8bad\u7ec3\uff0c\u4f46\u4e5f\u9762\u4e34Hessian\u77e9\u9635\u53ca\u5176\u9006\u7684\u9ad8\u6548\u5229\u7528\u7b49\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3a\u672a\u6765\u5f00\u53d1\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u8054\u90a6\u4f18\u5316\u65b9\u6cd5\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u4e8c\u9636\u4f18\u5316\u5728FL\u4e2d\u7684\u91cd\u8981\u6027\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60, \u4e8c\u9636\u4f18\u5316, \u6536\u655b\u901f\u5ea6, \u901a\u4fe1\u6210\u672c, Hessian\u77e9\u9635"}}
{"id": "2505.23292", "pdf": "https://arxiv.org/pdf/2505.23292", "abs": "https://arxiv.org/abs/2505.23292", "authors": ["Evangelos Charalampakis", "Vasileios Mygdalis", "Ioannis Pitas"], "title": "Federated Unsupervised Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This work explores the application of Federated Learning (FL) in Unsupervised\nSemantic image Segmentation (USS). Recent USS methods extract pixel-level\nfeatures using frozen visual foundation models and refine them through\nself-supervised objectives that encourage semantic grouping. These features are\nthen grouped to semantic clusters to produce segmentation masks. Extending\nthese ideas to federated settings requires feature representation and cluster\ncentroid alignment across distributed clients -- an inherently difficult task\nunder heterogeneous data distributions in the absence of supervision. To\naddress this, we propose FUSS Federated Unsupervised image Semantic\nSegmentation) which is, to our knowledge, the first framework to enable fully\ndecentralized, label-free semantic segmentation training. FUSS introduces novel\nfederation strategies that promote global consistency in feature and prototype\nspace, jointly optimizing local segmentation heads and shared semantic\ncentroids. Experiments on both benchmark and real-world datasets, including\nbinary and multi-class segmentation tasks, show that FUSS consistently\noutperforms local-only client trainings as well as extensions of classical FL\nalgorithms under varying client data distributions. To support reproducibility,\nfull code will be released upon manuscript acceptance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86FUSS\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u3001\u65e0\u6807\u7b7e\u7684\u8054\u90a6\u5b66\u4e60\u8bed\u4e49\u56fe\u50cf\u5206\u5272\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u672c\u5730\u5206\u5272\u5934\u548c\u5171\u4eab\u8bed\u4e49\u4e2d\u5fc3\uff0c\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u5ba2\u6237\u7aef\u7279\u5f81\u548c\u805a\u7c7b\u4e2d\u5fc3\u5bf9\u9f50\u7684\u96be\u9898\u3002", "motivation": "\u63a2\u7a76\u8054\u90a6\u5b66\u4e60\u5728\u65e0\u76d1\u7763\u8bed\u4e49\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u5206\u5e03\u5f0f\u5ba2\u6237\u7aef\u5728\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e0b\u7279\u5f81\u8868\u793a\u548c\u805a\u7c7b\u4e2d\u5fc3\u5bf9\u9f50\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faFUSS\u6846\u67b6\uff0c\u5f15\u5165\u65b0\u7684\u8054\u90a6\u7b56\u7565\uff0c\u4fc3\u8fdb\u7279\u5f81\u548c\u539f\u578b\u7a7a\u95f4\u7684\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u8054\u5408\u4f18\u5316\u672c\u5730\u5206\u5272\u5934\u548c\u5171\u4eab\u8bed\u4e49\u4e2d\u5fc3\u3002", "result": "FUSS\u5728\u57fa\u51c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff08\u5305\u62ec\u4e8c\u5206\u7c7b\u548c\u591a\u5206\u7c7b\u4efb\u52a1\uff09\u4e00\u81f4\u4f18\u4e8e\u4ec5\u672c\u5730\u8bad\u7ec3\u548c\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u6269\u5c55\u3002", "conclusion": "FUSS\u8bc1\u660e\u4e86\u5728\u65e0\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u4e2d\u5b9e\u73b0\u8bed\u4e49\u5206\u5272\u7684\u53ef\u884c\u6027\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60, \u65e0\u76d1\u7763\u8bed\u4e49\u5206\u5272, \u7279\u5f81\u5bf9\u9f50, \u539f\u578b\u7a7a\u95f4, \u5206\u5e03\u5f0f\u8bad\u7ec3"}}
{"id": "2505.23593", "pdf": "https://arxiv.org/pdf/2505.23593", "abs": "https://arxiv.org/abs/2505.23593", "authors": ["Nikita Agrawal", "Simon Mertel", "Ruben Mayer"], "title": "Position: Federated Foundation Language Model Post-Training Should Focus on Open-Source Models", "categories": ["cs.LG"], "comment": null, "summary": "Post-training of foundation language models has emerged as a promising\nresearch domain in federated learning (FL) with the goal to enable\nprivacy-preserving model improvements and adaptations to user's downstream\ntasks. Recent advances in this area adopt centralized post-training approaches\nthat build upon black-box foundation language models where there is no access\nto model weights and architecture details. Although the use of black-box models\nhas been successful in centralized post-training, their blind replication in FL\nraises several concerns. Our position is that using black-box models in FL\ncontradicts the core principles of federation such as data privacy and\nautonomy. In this position paper, we critically analyze the usage of black-box\nmodels in federated post-training, and provide a detailed account of various\naspects of openness and their implications for FL.", "AI": {"tldr": "\u8be5\u7acb\u573a\u8bba\u6587\u6279\u5224\u6027\u5206\u6790\u4e86\u5728\u9ed1\u76d2\u6a21\u578b\u4e2d\u5e94\u7528\u8054\u90a6\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u4e0e\u8054\u90a6\u5b66\u4e60\u6838\u5fc3\u539f\u5219\uff08\u5982\u6570\u636e\u9690\u79c1\u548c\u81ea\u4e3b\u6027\uff09\u7684\u77db\u76fe\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u5728\u4fdd\u62a4\u9690\u79c1\u548c\u81ea\u4e3b\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5229\u7528\u9ed1\u76d2\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8054\u90a6\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u3002", "method": "\u901a\u8fc7\u6279\u5224\u6027\u5206\u6790\u9ed1\u76d2\u6a21\u578b\u5728\u8054\u90a6\u540e\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\uff0c\u8be6\u7ec6\u8ba8\u8bba\u4e86\u5f00\u653e\u6027\u7684\u591a\u4e2a\u65b9\u9762\u53ca\u5176\u5bf9\u8054\u90a6\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "result": "\u6307\u51fa\u7b80\u5355\u5730\u590d\u5236\u96c6\u4e2d\u5f0f\u540e\u8bad\u7ec3\u4e2d\u9ed1\u76d2\u6a21\u578b\u7684\u4f7f\u7528\u5230\u8054\u90a6\u5b66\u4e60\u4e2d\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff0c\u53ef\u80fd\u8fdd\u80cc\u8054\u90a6\u5b66\u4e60\u7684\u6838\u5fc3\u539f\u5219\u3002", "conclusion": "\u5efa\u8bae\u91cd\u65b0\u5ba1\u89c6\u9ed1\u76d2\u6a21\u578b\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u786e\u4fdd\u5176\u7b26\u5408\u9690\u79c1\u4fdd\u62a4\u548c\u6570\u636e\u81ea\u4e3b\u6027\u7684\u8981\u6c42\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60\uff0c\u540e\u8bad\u7ec3\uff0c\u9ed1\u76d2\u6a21\u578b\uff0c\u6570\u636e\u9690\u79c1\uff0c\u81ea\u4e3b\u6027"}}
{"id": "2505.23598", "pdf": "https://arxiv.org/pdf/2505.23598", "abs": "https://arxiv.org/abs/2505.23598", "authors": ["Radzim Sendyka", "Christian Cabrera", "Andrei Paleyes", "Diana Robinson", "Neil Lawrence"], "title": "LLM Performance for Code Generation on Noisy Tasks", "categories": ["cs.LG", "cs.SE"], "comment": null, "summary": "This paper investigates the ability of large language models (LLMs) to\nrecognise and solve tasks which have been obfuscated beyond recognition.\nFocusing on competitive programming and benchmark tasks (LeetCode and MATH), we\ncompare performance across multiple models and obfuscation methods, such as\nnoise and redaction. We demonstrate that all evaluated LLMs can solve tasks\nobfuscated to a level where the text would be unintelligible to human readers,\nand does not contain key pieces of instruction or context. We introduce the\nconcept of eager pattern matching to describe this behaviour, which is not\nobserved in tasks published after the models' knowledge cutoff date, indicating\nstrong memorisation or overfitting to training data, rather than legitimate\nreasoning about the presented problem. We report empirical evidence of distinct\nperformance decay patterns between contaminated and unseen datasets. We discuss\nthe implications for benchmarking and evaluations of model behaviour, arguing\nfor caution when designing experiments using standard datasets. We also propose\nmeasuring the decay of performance under obfuscation as a possible strategy for\ndetecting dataset contamination and highlighting potential safety risks and\ninterpretability issues for automated software systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u6a21\u7cca\u5316\u4efb\u52a1\u7684\u8bc6\u522b\u4e0e\u89e3\u51b3\u80fd\u529b\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u4eba\u7c7b\u65e0\u6cd5\u7406\u89e3\u7684\u6587\u672c\uff0c\u6a21\u578b\u4ecd\u80fd\u5b8c\u6210\u4efb\u52a1\uff0c\u8868\u660e\u5b58\u5728\u8fc7\u5ea6\u4f9d\u8d56\u8bb0\u5fc6\u800c\u975e\u63a8\u7406\u7684\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u9a8c\u8bc1LLMs\u662f\u5426\u771f\u6b63\u5177\u5907\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u7684\u8bb0\u5fc6\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u6a21\u7cca\u5316\u4efb\u52a1\u65f6\u7684\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u4e0d\u540cLLMs\u5728\u7ade\u4e89\u6027\u7f16\u7a0b\u548c\u57fa\u51c6\u4efb\u52a1\uff08\u5982LeetCode\u548cMATH\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u566a\u58f0\u548c\u5220\u51cf\u7b49\u65b9\u6cd5\u6a21\u7cca\u5316\u4efb\u52a1\u6587\u672c\u3002", "result": "\u6240\u6709\u6d4b\u8bd5\u7684LLMs\u5747\u80fd\u89e3\u51b3\u9ad8\u5ea6\u6a21\u7cca\u5316\u7684\u4efb\u52a1\uff0c\u4f46\u8868\u73b0\u51fa\u660e\u663e\u7684\u8bb0\u5fc6\u4f9d\u8d56\u884c\u4e3a\uff08\u5373\u2018\u6025\u5207\u6a21\u5f0f\u5339\u914d\u2019\uff09\uff0c\u5bf9\u65b0\u6570\u636e\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u8c28\u614e\u8bbe\u8ba1\u5b9e\u9a8c\u57fa\u51c6\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u6a21\u7cca\u5316\u6027\u80fd\u8870\u51cf\u68c0\u6d4b\u6570\u636e\u96c6\u6c61\u67d3\uff0c\u63d0\u793a\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u5b89\u5168\u4e0e\u53ef\u89e3\u91ca\u6027\u98ce\u9669\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u4efb\u52a1\u6a21\u7cca\u5316, \u8bb0\u5fc6\u4f9d\u8d56, \u6570\u636e\u96c6\u6c61\u67d3, \u8bc4\u4f30\u65b9\u6cd5"}}
{"id": "2505.23493", "pdf": "https://arxiv.org/pdf/2505.23493", "abs": "https://arxiv.org/abs/2505.23493", "authors": ["Kaijie Chen", "Zihao Lin", "Zhiyang Xu", "Ying Shen", "Yuguang Yao", "Joy Rimchala", "Jiaxin Zhang", "Lifu Huang"], "title": "R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation", "categories": ["cs.CV", "cs.CL"], "comment": "Project Page: https://r2i-bench.github.io", "summary": "Reasoning is a fundamental capability often required in real-world\ntext-to-image (T2I) generation, e.g., generating ``a bitten apple that has been\nleft in the air for more than a week`` necessitates understanding temporal\ndecay and commonsense concepts. While recent T2I models have made impressive\nprogress in producing photorealistic images, their reasoning capability remains\nunderdeveloped and insufficiently evaluated. To bridge this gap, we introduce\nR2I-Bench, a comprehensive benchmark specifically designed to rigorously assess\nreasoning-driven T2I generation. R2I-Bench comprises meticulously curated data\ninstances, spanning core reasoning categories, including commonsense,\nmathematical, logical, compositional, numerical, causal, and concept mixing. To\nfacilitate fine-grained evaluation, we design R2IScore, a QA-style metric based\non instance-specific, reasoning-oriented evaluation questions that assess three\ncritical dimensions: text-image alignment, reasoning accuracy, and image\nquality. Extensive experiments with 16 representative T2I models, including a\nstrong pipeline-based framework that decouples reasoning and generation using\nthe state-of-the-art language and image generation models, demonstrate\nconsistently limited reasoning performance, highlighting the need for more\nrobust, reasoning-aware architectures in the next generation of T2I systems.\nProject Page: https://r2i-bench.github.io", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86R2I-Bench\u57fa\u51c6\u548cR2IScore\u8bc4\u5206\u6807\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u63a8\u7406\u6027\u80fd\u4e0a\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709T2I\u6a21\u578b\u5728\u751f\u6210\u903c\u771f\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u4ecd\u663e\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u8bbe\u8ba1\u4e86R2I-Bench\u57fa\u51c6\uff0c\u5305\u542b\u591a\u4e2a\u63a8\u7406\u7c7b\u522b\u7684\u6570\u636e\u5b9e\u4f8b\uff0c\u5e76\u5f00\u53d1\u4e86R2IScore\u8bc4\u5206\u6807\u51c6\uff0c\u57fa\u4e8e\u95ee\u7b54\u5f0f\u95ee\u9898\u8bc4\u4f30\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u3001\u63a8\u7406\u51c6\u786e\u6027\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c16\u79cd\u4ee3\u8868\u6027T2I\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u666e\u904d\u6709\u9650\uff0c\u51f8\u663e\u4e86\u4e0b\u4e00\u4ee3T2I\u7cfb\u7edf\u9700\u8981\u66f4\u5f3a\u5927\u7684\u63a8\u7406\u611f\u77e5\u67b6\u6784\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u66f4\u5177\u9c81\u68d2\u6027\u548c\u63a8\u7406\u80fd\u529b\u7684T2I\u67b6\u6784\u3002", "keywords": "\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210, \u63a8\u7406\u80fd\u529b, \u57fa\u51c6\u8bc4\u4f30, R2I-Bench, R2IScore"}}
{"id": "2505.23308", "pdf": "https://arxiv.org/pdf/2505.23308", "abs": "https://arxiv.org/abs/2505.23308", "authors": ["Nimrod Shabtay", "Zvi Kons", "Avihu Dekel", "Hagai Aronowitz", "Ron Hoory", "Assaf Arbelle"], "title": "Spoken question answering for visual queries", "categories": ["eess.AS", "cs.AI", "eess.IV"], "comment": "Accepted for Interspeech 2025 (with additional results)", "summary": "Question answering (QA) systems are designed to answer natural language\nquestions. Visual QA (VQA) and Spoken QA (SQA) systems extend the textual QA\nsystem to accept visual and spoken input respectively.\n  This work aims to create a system that enables user interaction through both\nspeech and images. That is achieved through the fusion of text, speech, and\nimage modalities to tackle the task of spoken VQA (SVQA). The resulting\nmulti-modal model has textual, visual, and spoken inputs and can answer spoken\nquestions on images.\n  Training and evaluating SVQA models requires a dataset for all three\nmodalities, but no such dataset currently exists. We address this problem by\nsynthesizing VQA datasets using two zero-shot TTS models. Our initial findings\nindicate that a model trained only with synthesized speech nearly reaches the\nperformance of the upper-bounding model trained on textual QAs. In addition, we\nshow that the choice of the TTS model has a minor impact on accuracy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6587\u672c\u3001\u8bed\u97f3\u548c\u56fe\u50cf\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u7528\u4e8e\u5904\u7406\u8bed\u97f3\u89c6\u89c9\u95ee\u7b54\uff08SVQA\uff09\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u7684\u8bed\u97f3\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u5408\u6210\u8bed\u97f3\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u63a5\u8fd1\u7eaf\u6587\u672c\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u4e14TTS\u6a21\u578b\u7684\u9009\u62e9\u5bf9\u51c6\u786e\u6027\u5f71\u54cd\u8f83\u5c0f\u3002", "motivation": "\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u80fd\u591f\u901a\u8fc7\u8bed\u97f3\u548c\u56fe\u50cf\u8fdb\u884c\u7528\u6237\u4ea4\u4e92\u7684\u7cfb\u7edf\uff0c\u7ed3\u5408\u6587\u672c\u3001\u8bed\u97f3\u548c\u56fe\u50cf\u4e09\u79cd\u6a21\u6001\u6765\u89e3\u51b3\u8bed\u97f3\u89c6\u89c9\u95ee\u7b54\uff08SVQA\uff09\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u878d\u5408\u6587\u672c\u3001\u8bed\u97f3\u548c\u56fe\u50cf\u6a21\u6001\u6765\u6784\u5efa\u591a\u6a21\u6001\u6a21\u578b\u3002\u4f7f\u7528\u96f6\u6837\u672cTTS\u6a21\u578b\u5408\u6210\u8bed\u97f3\u6570\u636e\u96c6\u4ee5\u8bad\u7ec3\u548c\u8bc4\u4f30SVQA\u6a21\u578b\u3002", "result": "\u4ec5\u4f7f\u7528\u5408\u6210\u8bed\u97f3\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u63a5\u8fd1\u7eaf\u6587\u672c\u8bad\u7ec3\u7684\u6a21\u578b\uff0cTTS\u6a21\u578b\u7684\u9009\u62e9\u5bf9\u51c6\u786e\u6027\u5f71\u54cd\u8f83\u5c0f\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u591a\u6a21\u6001\u878d\u5408\u53ef\u4ee5\u6709\u6548\u5904\u7406SVQA\u4efb\u52a1\uff0c\u5408\u6210\u8bed\u97f3\u6570\u636e\u96c6\u662f\u4e00\u79cd\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\u3002TTS\u6a21\u578b\u7684\u9009\u62e9\u5bf9\u6027\u80fd\u5f71\u54cd\u6709\u9650\u3002", "keywords": "\u8bed\u97f3\u89c6\u89c9\u95ee\u7b54\uff08SVQA\uff09\u3001\u591a\u6a21\u6001\u878d\u5408\u3001TTS\u6a21\u578b\u3001\u5408\u6210\u8bed\u97f3\u6570\u636e\u96c6"}}
{"id": "2505.23599", "pdf": "https://arxiv.org/pdf/2505.23599", "abs": "https://arxiv.org/abs/2505.23599", "authors": ["Eitan Levin", "Yuxin Ma", "Mateo D\u00edaz", "Soledad Villar"], "title": "On Transferring Transferability: Towards a Theory for Size Generalization", "categories": ["cs.LG", "math.RT", "math.ST", "stat.ML", "stat.TH"], "comment": "69 pages, 8 figures", "summary": "Many modern learning tasks require models that can take inputs of varying\nsizes. Consequently, dimension-independent architectures have been proposed for\ndomains where the inputs are graphs, sets, and point clouds. Recent work on\ngraph neural networks has explored whether a model trained on low-dimensional\ndata can transfer its performance to higher-dimensional inputs. We extend this\nbody of work by introducing a general framework for transferability across\ndimensions. We show that transferability corresponds precisely to continuity in\na limit space formed by identifying small problem instances with equivalent\nlarge ones. This identification is driven by the data and the learning task. We\ninstantiate our framework on existing architectures, and implement the\nnecessary changes to ensure their transferability. Finally, we provide design\nprinciples for designing new transferable models. Numerical experiments support\nour findings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u7ef4\u5ea6\u72ec\u7acb\u67b6\u6784\u5728\u4e0d\u540c\u5c3a\u5bf8\u8f93\u5165\u95f4\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u4ee3\u5b66\u4e60\u4efb\u52a1\u4e2d\u6a21\u578b\u9700\u8981\u9002\u5e94\u4e0d\u540c\u5c3a\u5bf8\u8f93\u5165\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u5728\u4f4e\u7ef4\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u5426\u8fc1\u79fb\u5230\u9ad8\u7ef4\u8f93\u5165\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u5c06\u53ef\u8fc1\u79fb\u6027\u4e0e\u6781\u9650\u7a7a\u95f4\u4e2d\u7684\u8fde\u7eed\u6027\u5bf9\u5e94\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u548c\u5b66\u4e60\u4efb\u52a1\u9a71\u52a8\u5c0f\u95ee\u9898\u5b9e\u4f8b\u4e0e\u5927\u5b9e\u4f8b\u7684\u7b49\u4ef7\u8bc6\u522b\u3002", "result": "\u5c55\u793a\u4e86\u53ef\u8fc1\u79fb\u6027\u5728\u7406\u8bba\u4e0a\u7684\u7cbe\u786e\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u5728\u73b0\u6709\u67b6\u6784\u4e0a\u5b9e\u4f8b\u5316\uff0c\u652f\u6301\u4e86\u8bbe\u8ba1\u65b0\u53ef\u8fc1\u79fb\u6a21\u578b\u7684\u539f\u5219\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u9a8c\u8bc1\u4e86\u7ef4\u5ea6\u95f4\u6a21\u578b\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u539f\u5219\u3002", "keywords": "\u7ef4\u5ea6\u72ec\u7acb\u67b6\u6784, \u53ef\u8fc1\u79fb\u6027, \u56fe\u795e\u7ecf\u7f51\u7edc, \u8f93\u5165\u5c3a\u5bf8\u53d8\u5316, \u6781\u9650\u7a7a\u95f4"}}
{"id": "2505.23500", "pdf": "https://arxiv.org/pdf/2505.23500", "abs": "https://arxiv.org/abs/2505.23500", "authors": ["Eva Mart\u00edn del Pico", "Josep Llu\u00eds Gelp\u00ed", "Salvador Capella-Guti\u00e9rrez"], "title": "Identity resolution of software metadata using Large Language Models", "categories": ["cs.SE", "cs.CL", "cs.DL"], "comment": null, "summary": "Software is an essential component of research. However, little attention has\nbeen paid to it compared with that paid to research data. Recently, there has\nbeen an increase in efforts to acknowledge and highlight the importance of\nsoftware in research activities.\n  Structured metadata from platforms like bio.tools, Bioconductor, and Galaxy\nToolShed offers valuable insights into research software in the Life Sciences.\nAlthough originally intended to support discovery and integration, this\nmetadata can be repurposed for large-scale analysis of software practices.\nHowever, its quality and completeness vary across platforms, reflecting diverse\ndocumentation practices.\n  To gain a comprehensive view of software development and sustainability,\nconsolidating this metadata is necessary, but requires robust mechanisms to\naddress its heterogeneity and scale.\n  This article presents an evaluation of instruction-tuned large language\nmodels for the task of software metadata identity resolution, a critical step\nin assembling a cohesive collection of research software. Such a collection is\nthe reference component for the Software Observatory at OpenEBench, a platform\nthat aggregates metadata to monitor the FAIRness of research software in the\nLife Sciences.\n  We benchmarked multiple models against a human-annotated gold standard,\nexamined their behavior on ambiguous cases, and introduced an agreement-based\nproxy for high-confidence automated decisions. The proxy achieved high\nprecision and statistical robustness, while also highlighting the limitations\nof current models and the broader challenges of automating semantic judgment in\nFAIR-aligned software metadata across registries and repositories.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u7814\u7a76\u8f6f\u4ef6\u7684\u91cd\u8981\u6027\u53ca\u5176\u5728\u751f\u547d\u79d1\u5b66\u4e2d\u7684\u5143\u6570\u636e\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u5229\u7528\u6307\u4ee4\u8c03\u6574\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8f6f\u4ef6\u5143\u6570\u636e\u8eab\u4efd\u89e3\u6790\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u9ec4\u91d1\u6807\u51c6\u5bf9\u6bd4\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u7cbe\u5ea6\u548c\u7edf\u8ba1\u7a33\u5065\u6027\u3002", "motivation": "\u7814\u7a76\u8f6f\u4ef6\u5728\u79d1\u7814\u6d3b\u52a8\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u4f46\u5176\u91cd\u8981\u6027\u672a\u5f97\u5230\u8db3\u591f\u5173\u6ce8\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6574\u5408\u548c\u5206\u6790\u4e0d\u540c\u5e73\u53f0\u7684\u8f6f\u4ef6\u5143\u6570\u636e\uff0c\u63d0\u5347\u8f6f\u4ef6\u7684\u53ef\u53d1\u73b0\u6027\u548c\u53ef\u6301\u7eed\u6027\u3002", "method": "\u91c7\u7528\u6307\u4ee4\u8c03\u6574\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8f6f\u4ef6\u5143\u6570\u636e\u8eab\u4efd\u89e3\u6790\uff0c\u5e76\u901a\u8fc7\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u9ec4\u91d1\u6807\u51c6\u5bf9\u6bd4\uff0c\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u63d0\u51fa\u7684\u57fa\u4e8e\u4e00\u81f4\u6027\u7684\u4ee3\u7406\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u7edf\u8ba1\u7a33\u5065\u6027\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u53ca\u8de8\u6ce8\u518c\u8868\u548c\u5b58\u50a8\u5e93\u81ea\u52a8\u5316\u8bed\u4e49\u5224\u65ad\u7684\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5143\u6570\u636e\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u5e94\u5bf9\u5f02\u6784\u6027\u548c\u89c4\u6a21\u5316\u7684\u6311\u6218\u3002", "keywords": "\u7814\u7a76\u8f6f\u4ef6, \u5143\u6570\u636e\u5206\u6790, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u8eab\u4efd\u89e3\u6790, FAIR\u539f\u5219"}}
{"id": "2505.23606", "pdf": "https://arxiv.org/pdf/2505.23606", "abs": "https://arxiv.org/abs/2505.23606", "authors": ["Qingyu Shi", "Jinbin Bai", "Zhuoran Zhao", "Wenhao Chai", "Kaidong Yu", "Jianzong Wu", "Shuangyong Song", "Yunhai Tong", "Xiangtai Li", "Xuelong Li", "Shuicheng Yan"], "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model", "categories": ["cs.LG", "cs.CV"], "comment": "The code and model are available at\n  https://github.com/M-E-AGI-Lab/Muddit", "summary": "Unified generation models aim to handle diverse tasks across modalities --\nsuch as text generation, image generation, and vision-language reasoning --\nwithin a single architecture and decoding paradigm. Autoregressive unified\nmodels suffer from slow inference due to sequential decoding, and\nnon-autoregressive unified models suffer from weak generalization due to\nlimited pretrained backbones. We introduce Muddit, a unified discrete diffusion\ntransformer that enables fast and parallel generation across both text and\nimage modalities. Unlike prior unified diffusion models trained from scratch,\nMuddit integrates strong visual priors from a pretrained text-to-image backbone\nwith a lightweight text decoder, enabling flexible and high-quality multimodal\ngeneration under a unified architecture. Empirical results show that Muddit\nachieves competitive or superior performance compared to significantly larger\nautoregressive models in both quality and efficiency. The work highlights the\npotential of purely discrete diffusion, when equipped with strong visual\npriors, as a scalable and effective backbone for unified generation.", "AI": {"tldr": "Muddit\u662f\u4e00\u79cd\u7edf\u4e00\u7684\u79bb\u6563\u6269\u6563\u53d8\u6362\u5668\uff0c\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u9aa8\u5e72\u548c\u8f7b\u91cf\u7ea7\u6587\u672c\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u5e76\u884c\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7edf\u4e00\u6a21\u578b\u5728\u63a8\u7406\u901f\u5ea6\u6216\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u89e3\u51b3\u81ea\u56de\u5f52\u7edf\u4e00\u6a21\u578b\u63a8\u7406\u6162\u548c\u975e\u81ea\u56de\u5f52\u7edf\u4e00\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5f31\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u8de8\u6a21\u6001\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u9aa8\u5e72\u4e0e\u8f7b\u91cf\u7ea7\u6587\u672c\u89e3\u7801\u5668\uff0c\u91c7\u7528\u79bb\u6563\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u5b9e\u73b0\u5e76\u884c\u751f\u6210\u3002", "result": "\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u6216\u5339\u914d\u66f4\u5927\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u79bb\u6563\u6269\u6563\u4f5c\u4e3a\u7edf\u4e00\u751f\u6210\u9aa8\u5e72\u7684\u6f5c\u529b\u3002", "conclusion": "\u79bb\u6563\u6269\u6563\u7ed3\u5408\u5f3a\u89c6\u89c9\u5148\u9a8c\u662f\u7edf\u4e00\u751f\u6210\u4efb\u52a1\u7684\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u7edf\u4e00\u751f\u6210\u6a21\u578b\uff0c\u79bb\u6563\u6269\u6563\uff0c\u591a\u6a21\u6001\u751f\u6210\uff0c\u5e76\u884c\u63a8\u7406"}}
{"id": "2505.23313", "pdf": "https://arxiv.org/pdf/2505.23313", "abs": "https://arxiv.org/abs/2505.23313", "authors": ["Weizhe Kong", "Xiao Wang", "Ruichong Gao", "Chenglong Li", "Yu Zhang", "Xing Yang", "Yaowei Wang", "Jin Tang"], "title": "Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Pedestrian Attribute Recognition (PAR) is an indispensable task in\nhuman-centered research and has made great progress in recent years with the\ndevelopment of deep neural networks. However, the potential vulnerability and\nanti-interference ability have still not been fully explored. To bridge this\ngap, this paper proposes the first adversarial attack and defense framework for\npedestrian attribute recognition. Specifically, we exploit both global- and\npatch-level attacks on the pedestrian images, based on the pre-trained\nCLIP-based PAR framework. It first divides the input pedestrian image into\nnon-overlapping patches and embeds them into feature embeddings using a\nprojection layer. Meanwhile, the attribute set is expanded into sentences using\nprompts and embedded into attribute features using a pre-trained CLIP text\nencoder. A multi-modal Transformer is adopted to fuse the obtained vision and\ntext tokens, and a feed-forward network is utilized for attribute recognition.\nBased on the aforementioned PAR framework, we adopt the adversarial semantic\nand label-perturbation to generate the adversarial noise, termed ASL-PAR. We\nalso design a semantic offset defense strategy to suppress the influence of\nadversarial attacks. Extensive experiments conducted on both digital domains\n(i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the\neffectiveness of our proposed adversarial attack and defense strategies for the\npedestrian attribute recognition. The source code of this paper will be\nreleased on https://github.com/Event-AHU/OpenPAR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\uff08PAR\uff09\u7684\u5bf9\u6297\u653b\u51fb\u4e0e\u9632\u5fa1\u6846\u67b6ASL-PAR\uff0c\u57fa\u4e8eCLIP\u6a21\u578b\u5b9e\u73b0\u591a\u6a21\u6001\u878d\u5408\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u8ba8\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6f5c\u5728\u8106\u5f31\u6027\u548c\u6297\u5e72\u6270\u80fd\u529b\u5c1a\u672a\u5145\u5206\u7814\u7a76\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684CLIP\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u653b\u51fb\u751f\u6210\u5bf9\u6297\u566a\u58f0\uff0c\u5e76\u8bbe\u8ba1\u8bed\u4e49\u504f\u79fb\u9632\u5fa1\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u63d0\u51fa\u7684\u5bf9\u6297\u653b\u51fb\u4e0e\u9632\u5fa1\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684ASL-PAR\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u3002", "keywords": "\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\u3001\u5bf9\u6297\u653b\u51fb\u3001\u9632\u5fa1\u7b56\u7565\u3001CLIP\u6a21\u578b\u3001\u591a\u6a21\u6001\u878d\u5408"}}
{"id": "2505.23607", "pdf": "https://arxiv.org/pdf/2505.23607", "abs": "https://arxiv.org/abs/2505.23607", "authors": ["Carolina Fortuna", "Gregor Cerar", "Blaz Bertalanic", "Andrej Campa", "Mihael Mohorcic"], "title": "Data Model Design for Explainable Machine Learning-based Electricity Applications", "categories": ["cs.LG"], "comment": null, "summary": "The transition from traditional power grids to smart grids, significant\nincrease in the use of renewable energy sources, and soaring electricity prices\nhas triggered a digital transformation of the energy infrastructure that\nenables new, data driven, applications often supported by machine learning\nmodels. However, the majority of the developed machine learning models rely on\nunivariate data. To date, a structured study considering the role meta-data and\nadditional measurements resulting in multivariate data is missing. In this\npaper we propose a taxonomy that identifies and structures various types of\ndata related to energy applications. The taxonomy can be used to guide\napplication specific data model development for training machine learning\nmodels. Focusing on a household electricity forecasting application, we\nvalidate the effectiveness of the proposed taxonomy in guiding the selection of\nthe features for various types of models. As such, we study of the effect of\ndomain, contextual and behavioral features on the forecasting accuracy of four\ninterpretable machine learning techniques and three openly available datasets.\nFinally, using a feature importance techniques, we explain individual feature\ncontributions to the forecasting accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u80fd\u6e90\u5e94\u7528\u4e2d\u6570\u636e\u7c7b\u578b\u7684\u5206\u7c7b\u6cd5\uff0c\u6307\u5bfc\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f00\u53d1\uff0c\u5e76\u901a\u8fc7\u5bb6\u5ead\u7528\u7535\u9884\u6d4b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u7535\u7f51\u5411\u667a\u80fd\u7535\u7f51\u7684\u8fc7\u6e21\u3001\u53ef\u518d\u751f\u80fd\u6e90\u4f7f\u7528\u7684\u589e\u52a0\u4ee5\u53ca\u7535\u4ef7\u4e0a\u6da8\u63a8\u52a8\u4e86\u80fd\u6e90\u57fa\u7840\u8bbe\u65bd\u7684\u6570\u5b57\u5316\u8f6c\u578b\uff0c\u4f46\u76ee\u524d\u5927\u591a\u6570\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f9d\u8d56\u5355\u53d8\u91cf\u6570\u636e\uff0c\u7f3a\u4e4f\u5bf9\u591a\u5143\u6570\u636e\u7684\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u7ed3\u6784\u5316\u80fd\u6e90\u5e94\u7528\u4e2d\u7684\u5404\u7c7b\u6570\u636e\uff0c\u5e76\u5728\u5bb6\u5ead\u7528\u7535\u9884\u6d4b\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u7814\u7a76\u4e86\u9886\u57df\u3001\u4e0a\u4e0b\u6587\u548c\u884c\u4e3a\u7279\u5f81\u5bf9\u56db\u79cd\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u6280\u672f\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u901a\u8fc7\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u5206\u7c7b\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5229\u7528\u7279\u5f81\u91cd\u8981\u6027\u6280\u672f\u89e3\u91ca\u4e86\u5404\u7279\u5f81\u5bf9\u9884\u6d4b\u51c6\u786e\u6027\u7684\u8d21\u732e\u3002", "conclusion": "\u8bba\u6587\u7684\u5206\u7c7b\u6cd5\u4e3a\u80fd\u6e90\u5e94\u7528\u4e2d\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u5143\u6570\u636e\u5bf9\u9884\u6d4b\u51c6\u786e\u6027\u7684\u91cd\u8981\u6027\u3002", "keywords": "\u667a\u80fd\u7535\u7f51,\u673a\u5668\u5b66\u4e60,\u591a\u5143\u6570\u636e,\u5bb6\u5ead\u7528\u7535\u9884\u6d4b,\u7279\u5f81\u9009\u62e9"}}
{"id": "2505.23609", "pdf": "https://arxiv.org/pdf/2505.23609", "abs": "https://arxiv.org/abs/2505.23609", "authors": ["Armando Bellante", "Martin Pl\u00e1vala", "Alessandro Luongo"], "title": "The Generalized Skew Spectrum of Graphs", "categories": ["cs.LG", "cs.DS", "math.GR", "math.RT"], "comment": null, "summary": "This paper proposes a family of permutation-invariant graph embeddings,\ngeneralizing the Skew Spectrum of graphs of Kondor & Borgwardt (2008). Grounded\nin group theory and harmonic analysis, our method introduces a new class of\ngraph invariants that are isomorphism-invariant and capable of embedding richer\ngraph structures - including attributed graphs, multilayer graphs, and\nhypergraphs - which the Skew Spectrum could not handle. Our generalization\nfurther defines a family of functions that enables a trade-off between\ncomputational complexity and expressivity. By applying\ngeneralization-preserving heuristics to this family, we improve the Skew\nSpectrum's expressivity at the same computational cost. We formally prove the\ninvariance of our generalization, demonstrate its improved expressiveness\nthrough experiments, and discuss its efficient computation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u7684\u7f6e\u6362\u4e0d\u53d8\u56fe\u5d4c\u5165\u65b9\u6cd5\uff0c\u8d85\u8d8a\u4e86Kondor & Borgwardt (2008)\u7684Skew Spectrum\uff0c\u9002\u7528\u4e8e\u5c5e\u6027\u56fe\u3001\u591a\u5c42\u56fe\u548c\u8d85\u56fe\uff0c\u5e76\u5728\u8ba1\u7b97\u590d\u6742\u6027\u548c\u8868\u8fbe\u80fd\u529b\u4e4b\u95f4\u63d0\u4f9b\u6743\u8861\u3002", "motivation": "\u73b0\u6709Skew Spectrum\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u5c5e\u6027\u56fe\u3001\u591a\u5c42\u56fe\u7b49\u590d\u6742\u56fe\u7ed3\u6784\uff0c\u4e14\u7f3a\u4e4f\u8ba1\u7b97\u6548\u7387\u4e0e\u8868\u8fbe\u80fd\u529b\u7684\u5e73\u8861\u3002\u8bba\u6587\u65e8\u5728\u63d0\u51fa\u66f4\u901a\u7528\u7684\u4e0d\u53d8\u56fe\u5d4c\u5165\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u7fa4\u8bba\u548c\u8c10\u6ce2\u5206\u6790\uff0c\u63d0\u51fa\u65b0\u7684\u56fe\u4e0d\u53d8\u91cf\u65cf\uff0c\u652f\u6301\u7f6e\u6362\u4e0d\u53d8\u6027\uff0c\u5e76\u901a\u8fc7\u6cdb\u5316\u4fdd\u7559\u542f\u53d1\u5f0f\u65b9\u6cd5\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u8868\u8fbe\u529b\uff0c\u5e76\u5f62\u5f0f\u5316\u8bc1\u660e\u4e86\u5176\u4e0d\u53d8\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u590d\u6742\u56fe\u7ed3\u6784\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u4e14\u9ad8\u6548\u7684\u4e0d\u53d8\u5d4c\u5165\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86Skew Spectrum\u7684\u5e94\u7528\u8303\u56f4\u3002", "keywords": "\u56fe\u5d4c\u5165\u3001\u7f6e\u6362\u4e0d\u53d8\u6027\u3001Skew Spectrum\u3001\u7fa4\u8bba\u3001\u8c10\u6ce2\u5206\u6790"}}
{"id": "2505.23331", "pdf": "https://arxiv.org/pdf/2505.23331", "abs": "https://arxiv.org/abs/2505.23331", "authors": ["Matteo Gallici", "Haitz S\u00e1ez de Oc\u00e1riz Borde"], "title": "Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Fine-tuning pre-trained generative models with Reinforcement Learning (RL)\nhas emerged as an effective approach for aligning outputs more closely with\nnuanced human preferences. In this paper, we investigate the application of\nGroup Relative Policy Optimization (GRPO) to fine-tune next-scale visual\nautoregressive (VAR) models. Our empirical results demonstrate that this\napproach enables alignment to intricate reward signals derived from aesthetic\npredictors and CLIP embeddings, significantly enhancing image quality and\nenabling precise control over the generation style. Interestingly, by\nleveraging CLIP, our method can help VAR models generalize beyond their initial\nImageNet distribution: through RL-driven exploration, these models can generate\nimages aligned with prompts referencing image styles that were absent during\npre-training. In summary, we show that RL-based fine-tuning is both efficient\nand effective for VAR models, benefiting particularly from their fast inference\nspeeds, which are advantageous for online sampling, an aspect that poses\nsignificant challenges for diffusion-based alternatives.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u751f\u6210\u6a21\u578b\uff0c\u7279\u522b\u662f\u5e94\u7528Group Relative Policy Optimization\uff08GRPO\uff09\u6765\u4f18\u5316\u89c6\u89c9\u81ea\u56de\u5f52\uff08VAR\uff09\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u5e76\u63a7\u5236\u751f\u6210\u98ce\u683c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u901a\u8fc7RL\u5fae\u8c03\u65b9\u6cd5\u4f7fVAR\u6a21\u578b\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\uff0c\u5c24\u5176\u662f\u9762\u5bf9\u590d\u6742\u7684\u5956\u52b1\u4fe1\u53f7\uff08\u5982\u7f8e\u5b66\u9884\u6d4b\u5668\u548cCLIP\u5d4c\u5165\uff09\u65f6\u3002", "method": "\u91c7\u7528GRPO\u65b9\u6cd5\u5fae\u8c03VAR\u6a21\u578b\uff0c\u5229\u7528CLIP\u5d4c\u5165\u548c\u7f8e\u5b66\u9884\u6d4b\u5668\u751f\u6210\u7684\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u8ba9\u6a21\u578b\u751f\u6210\u8d85\u51fa\u521d\u59cbImageNet\u5206\u5e03\u7684\u98ce\u683c\u5316\u56fe\u50cf\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cRL\u5fae\u8c03\u5bf9VAR\u6a21\u578b\u65e2\u9ad8\u6548\u53c8\u6709\u6548\uff0c\u5c24\u5176\u9002\u5408\u5728\u7ebf\u91c7\u6837\u573a\u666f\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, GRPO, \u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b, CLIP, \u56fe\u50cf\u751f\u6210"}}
{"id": "2505.23614", "pdf": "https://arxiv.org/pdf/2505.23614", "abs": "https://arxiv.org/abs/2505.23614", "authors": ["Xiangcheng Zhang", "Haowei Lin", "Haotian Ye", "James Zou", "Jianzhu Ma", "Yitao Liang", "Yilun Du"], "title": "Inference-time Scaling of Diffusion Models through Classical Search", "categories": ["cs.LG", "stat.ML"], "comment": "Website at https://diffusion-inference-scaling.github.io/", "summary": "Classical search algorithms have long underpinned modern artificial\nintelligence. In this work, we tackle the challenge of inference-time control\nin diffusion models -- adapting generated outputs to meet diverse test-time\nobjectives -- using principles from classical search. We propose a general\nframework that orchestrates local and global search to efficiently navigate the\ngenerative space. It employs a theoretically grounded local search via annealed\nLangevin MCMC and performs compute-efficient global exploration using\nbreadth-first and depth-first tree search. We evaluate our approach on a range\nof challenging domains, including planning, offline reinforcement learning, and\nimage generation. Across all tasks, we observe significant gains in both\nperformance and efficiency. These results show that classical search provides a\nprincipled and practical foundation for inference-time scaling in diffusion\nmodels. Project page at diffusion-inference-scaling.github.io.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7ecf\u5178\u641c\u7d22\u539f\u5219\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u63a8\u7406\u65f6\u63a7\u5236\uff0c\u901a\u8fc7\u5c40\u90e8\u548c\u5168\u5c40\u641c\u7d22\u9ad8\u6548\u5bfc\u822a\u751f\u6210\u7a7a\u95f4\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5229\u7528\u7ecf\u5178\u641c\u7d22\u7b97\u6cd5\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u63a8\u7406\u65f6\u63a7\u5236\u751f\u6210\u7684\u6311\u6218\uff0c\u4ee5\u6ee1\u8db3\u591a\u6837\u5316\u7684\u6d4b\u8bd5\u65f6\u76ee\u6807\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u9000\u706b\u7684Langevin MCMC\u8fdb\u884c\u5c40\u90e8\u641c\u7d22\uff0c\u5e76\u5229\u7528\u5e7f\u5ea6\u4f18\u5148\u548c\u6df1\u5ea6\u4f18\u5148\u6811\u641c\u7d22\u8fdb\u884c\u5168\u5c40\u63a2\u7d22\u3002", "result": "\u5728\u89c4\u5212\u3001\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u56fe\u50cf\u751f\u6210\u7b49\u591a\u4e2a\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0e\u6548\u7387\u4e0a\u5747\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\u7ecf\u5178\u641c\u7d22\u4e3a\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\u65f6\u6269\u5c55\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u57fa\u7840\u3002", "keywords": "\u6269\u6563\u6a21\u578b, \u63a8\u7406\u65f6\u63a7\u5236, \u7ecf\u5178\u641c\u7d22, Langevin MCMC, \u6811\u641c\u7d22"}}
{"id": "2505.23590", "pdf": "https://arxiv.org/pdf/2505.23590", "abs": "https://arxiv.org/abs/2505.23590", "authors": ["Zifu Wang", "Junyi Zhu", "Bo Tang", "Zhiyu Li", "Feiyu Xiong", "Jiaqian Yu", "Matthew B. Blaschko"], "title": "Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The application of rule-based reinforcement learning (RL) to multimodal large\nlanguage models (MLLMs) introduces unique challenges and potential deviations\nfrom findings in text-only domains, particularly for perception-heavy tasks.\nThis paper provides a comprehensive study of rule-based visual RL using jigsaw\npuzzles as a structured experimental framework, revealing several key findings.\n\\textit{Firstly,} we find that MLLMs, initially performing near to random\nguessing on simple puzzles, achieve near-perfect accuracy and generalize to\ncomplex, unseen configurations through fine-tuning. \\textit{Secondly,} training\non jigsaw puzzles can induce generalization to other visual tasks, with\neffectiveness tied to specific task configurations. \\textit{Thirdly,} MLLMs can\nlearn and generalize with or without explicit reasoning, though open-source\nmodels often favor direct answering. Consequently, even when trained for\nstep-by-step reasoning, they can ignore the thinking process in deriving the\nfinal answer. \\textit{Fourthly,} we observe that complex reasoning patterns\nappear to be pre-existing rather than emergent, with their frequency increasing\nalongside training and task difficulty. \\textit{Finally,} our results\ndemonstrate that RL exhibits more effective generalization than Supervised\nFine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL\noptimization. Although these observations are based on jigsaw puzzles and may\nvary across other visual tasks, this research contributes a valuable piece of\njigsaw to the larger puzzle of collective understanding rule-based visual RL\nand its potential in multimodal learning. The code is available at:\n\\href{https://github.com/zifuwanggg/Jigsaw-R1}{https://github.com/zifuwanggg/Jigsaw-R1}.", "AI": {"tldr": "\u7814\u7a76\u4e86\u57fa\u4e8e\u89c4\u5219\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u901a\u8fc7\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u5347\u62fc\u56fe\u4efb\u52a1\u7684\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u5176\u4ed6\u89c6\u89c9\u4efb\u52a1\uff0c\u4f46\u63a8\u7406\u80fd\u529b\u53ef\u80fd\u9690\u85cf\u3002", "motivation": "\u63a2\u7d22\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u62fc\u56fe\u4efb\u52a1\u4e2d\u7684\u5b66\u4e60\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u62fc\u56fe\u4f5c\u4e3a\u5b9e\u9a8c\u6846\u67b6\uff0c\u5bf9\u6bd4\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e0e\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u7684\u6548\u679c\u3002", "result": "\u5fae\u8c03\u540e\u6a21\u578b\u51c6\u786e\u6027\u63a5\u8fd1\u5b8c\u7f8e\uff0c\u80fd\u6cdb\u5316\u81f3\u590d\u6742\u4efb\u52a1\uff1bRL\u6bd4SFT\u6cdb\u5316\u6548\u679c\u66f4\u597d\u3002", "conclusion": "\u62fc\u56fe\u4efb\u52a1\u4e3a\u7406\u89e3\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u89c4\u5219\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "keywords": "\u89c4\u5219\u5f3a\u5316\u5b66\u4e60, \u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, \u62fc\u56fe\u4efb\u52a1, \u6cdb\u5316\u80fd\u529b, \u76d1\u7763\u5fae\u8c03"}}
{"id": "2505.23615", "pdf": "https://arxiv.org/pdf/2505.23615", "abs": "https://arxiv.org/abs/2505.23615", "authors": ["Chang Yue", "Niraj K. Jha"], "title": "Learning Interpretable Differentiable Logic Networks for Tabular Regression", "categories": ["cs.LG"], "comment": null, "summary": "Neural networks (NNs) achieve outstanding performance in many domains;\nhowever, their decision processes are often opaque and their inference can be\ncomputationally expensive in resource-constrained environments. We recently\nproposed Differentiable Logic Networks (DLNs) to address these issues for\ntabular classification based on relaxing discrete logic into a differentiable\nform, thereby enabling gradient-based learning of networks built from binary\nlogic operations. DLNs offer interpretable reasoning and substantially lower\ninference cost.\n  We extend the DLN framework to supervised tabular regression. Specifically,\nwe redesign the final output layer to support continuous targets and unify the\noriginal two-phase training procedure into a single differentiable stage. We\nevaluate the resulting model on 15 public regression benchmarks, comparing it\nwith modern neural networks and classical regression baselines. Regression DLNs\nmatch or exceed baseline accuracy while preserving interpretability and fast\ninference. Our results show that DLNs are a viable, cost-effective alternative\nfor regression tasks, especially where model transparency and computational\nefficiency are important.", "AI": {"tldr": "\u8bba\u6587\u6269\u5c55\u4e86Differentiable Logic Networks (DLNs)\u6846\u67b6\uff0c\u5e94\u7528\u4e8e\u76d1\u7763\u8868\u683c\u56de\u5f52\u4efb\u52a1\uff0c\u901a\u8fc7\u6539\u8fdb\u8f93\u51fa\u5c42\u548c\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u548c\u9ad8\u6548\u63a8\u7406\u7684\u540c\u65f6\uff0c\u6027\u80fd\u4e0e\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u7684\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\u4e14\u63a8\u7406\u6210\u672c\u9ad8\uff0cDLNs\u901a\u8fc7\u53ef\u5fae\u5206\u903b\u8f91\u8fd0\u7b97\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u548c\u4f4e\u6210\u672c\u63a8\u7406\uff0c\u4f46\u539f\u6846\u67b6\u4ec5\u9002\u7528\u4e8e\u5206\u7c7b\u4efb\u52a1\u3002\u672c\u6587\u65e8\u5728\u5c06\u5176\u6269\u5c55\u5230\u56de\u5f52\u4efb\u52a1\uff0c\u89e3\u51b3\u7c7b\u4f3c\u95ee\u9898\u3002", "method": "\u6539\u8fdbDLN\u6846\u67b6\u7684\u8f93\u51fa\u5c42\u4ee5\u652f\u6301\u8fde\u7eed\u76ee\u6807\u53d8\u91cf\uff0c\u5e76\u5c06\u539f\u59cb\u4e24\u9636\u6bb5\u8bad\u7ec3\u7edf\u4e00\u4e3a\u5355\u9636\u6bb5\u53ef\u5fae\u5206\u8fc7\u7a0b\uff0c\u5e94\u7528\u4e8e15\u4e2a\u516c\u5171\u56de\u5f52\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u56de\u5f52DLNs\u5728\u7cbe\u5ea6\u4e0a\u4e0e\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u548c\u7ecf\u5178\u56de\u5f52\u57fa\u7ebf\u76f8\u5f53\u6216\u66f4\u9ad8\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u548c\u5feb\u901f\u63a8\u7406\u3002", "conclusion": "DLNs\u662f\u56de\u5f52\u4efb\u52a1\u4e2d\u900f\u660e\u4e14\u9ad8\u6548\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9700\u8981\u6a21\u578b\u900f\u660e\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u7684\u573a\u666f\u3002", "keywords": "Differentiable Logic Networks, DLNs, \u76d1\u7763\u56de\u5f52, \u53ef\u89e3\u91ca\u6027, \u9ad8\u6548\u63a8\u7406"}}
{"id": "2505.23631", "pdf": "https://arxiv.org/pdf/2505.23631", "abs": "https://arxiv.org/abs/2505.23631", "authors": ["Boning Zhao"], "title": "Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "7 pages, 6 figures. Under review", "summary": "Assessing student depression in sensitive environments like special education\nis challenging. Standardized questionnaires may not fully reflect students'\ntrue situations. Furthermore, automated methods often falter with rich student\nnarratives, lacking the crucial, individualized insights stemming from\nteachers' empathetic connections with students. Existing methods often fail to\naddress this ambiguity or effectively integrate educator understanding. To\naddress these limitations by fostering a synergistic human-AI collaboration,\nthis paper introduces Human Empathy as Encoder (HEAE), a novel, human-centered\nAI framework for transparent and socially responsible depression severity\nassessment. Our approach uniquely integrates student narrative text with a\nteacher-derived, 9-dimensional \"Empathy Vector\" (EV), its dimensions guided by\nthe PHQ-9 framework,to explicitly translate tacit empathetic insight into a\nstructured AI input enhancing rather than replacing human judgment. Rigorous\nexperiments optimized the multimodal fusion, text representation, and\nclassification architecture, achieving 82.74% accuracy for 7-level severity\nclassification. This work demonstrates a path toward more responsible and\nethical affective computing by structurally embedding human empathy", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86HEAE\u6846\u67b6\uff0c\u7ed3\u5408\u5b66\u751f\u53d9\u8ff0\u548c\u6559\u5e08\u5171\u60c5\u8bc4\u4f30\u6291\u90c1\u4e25\u91cd\u7a0b\u5ea6\uff0c\u51c6\u786e\u7387\u8fbe82.74%\uff0c\u5f3a\u8c03\u4eba\u673a\u534f\u4f5c\u7684\u4f26\u7406\u610f\u4e49\u3002", "motivation": "\u7279\u6b8a\u6559\u80b2\u73af\u5883\u4e0b\u5b66\u751f\u6291\u90c1\u8bc4\u4f30\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u4e2a\u4f53\u5316\u5171\u60c5\u6d1e\u5bdf\uff0c\u9700\u900f\u660e\u4e14\u8d1f\u8d23\u4efb\u7684\u4eba\u673a\u534f\u4f5c\u65b9\u6848\u3002", "method": "\u63d0\u51faHEAE\u6846\u67b6\uff0c\u6574\u5408\u5b66\u751f\u53d9\u8ff0\u6587\u672c\u4e0e\u6559\u5e08\u6d3e\u751f\u76849\u7ef4'\u5171\u60c5\u5411\u91cf'\uff08\u57fa\u4e8ePHQ-9\u6846\u67b6\uff09\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u4f18\u5316\u5206\u7c7b\u67b6\u6784\u3002", "result": "\u5728\u591a\u6a21\u6001\u878d\u5408\u3001\u6587\u672c\u8868\u793a\u548c\u5206\u7c7b\u67b6\u6784\u4f18\u5316\u540e\uff0c7\u7ea7\u6291\u90c1\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u51c6\u786e\u7387\u8fbe82.74%\u3002", "conclusion": "HEAE\u5728\u60c5\u611f\u8ba1\u7b97\u4e2d\u5d4c\u5165\u4eba\u7c7b\u5171\u60c5\uff0c\u4e3a\u4f26\u7406\u5316\u4eba\u673a\u534f\u540c\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u589e\u5f3a\u800c\u975e\u66ff\u4ee3\u4eba\u5de5\u5224\u65ad\u3002", "keywords": "\u6291\u90c1\u8bc4\u4f30, \u4eba\u673a\u534f\u4f5c, \u5171\u60c5\u5411\u91cf, HEAE\u6846\u67b6, \u7279\u6b8a\u6559\u80b2"}}
{"id": "2505.23627", "pdf": "https://arxiv.org/pdf/2505.23627", "abs": "https://arxiv.org/abs/2505.23627", "authors": ["Griffin Dietz Smith", "Dianna Yee", "Jennifer King Chen", "Leah Findlater"], "title": "Prompting Whisper for Improved Verbatim Transcription and End-to-end Miscue Detection", "categories": ["cs.LG"], "comment": "Interspeech 2025", "summary": "Identifying mistakes (i.e., miscues) made while reading aloud is commonly\napproached post-hoc by comparing automatic speech recognition (ASR)\ntranscriptions to the target reading text. However, post-hoc methods perform\npoorly when ASR inaccurately transcribes verbatim speech. To improve on current\nmethods for reading error annotation, we propose a novel end-to-end\narchitecture that incorporates the target reading text via prompting and is\ntrained for both improved verbatim transcription and direct miscue detection.\nOur contributions include: first, demonstrating that incorporating reading text\nthrough prompting benefits verbatim transcription performance over fine-tuning,\nand second, showing that it is feasible to augment speech recognition tasks for\nend-to-end miscue detection. We conducted two case studies -- children's\nread-aloud and adult atypical speech -- and found that our proposed strategies\nimprove verbatim transcription and miscue detection compared to current\nstate-of-the-art.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u76ee\u6807\u9605\u8bfb\u6587\u672c\u7684\u7aef\u5230\u7aef\u67b6\u6784\uff0c\u901a\u8fc7\u63d0\u793a\u6539\u8fdb\u9010\u5b57\u8f6c\u5f55\u548c\u76f4\u63a5\u9519\u8bef\u68c0\u6d4b\uff0c\u4ece\u800c\u63d0\u9ad8\u6717\u8bfb\u9519\u8bef\u6807\u6ce8\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982ASR\u8f6c\u5f55\u540e\u6bd4\u8f83\uff09\u5728\u8f6c\u5f55\u4e0d\u51c6\u786e\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u6539\u8fdb\u6717\u8bfb\u9519\u8bef\u6807\u6ce8\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7aef\u5230\u7aef\u67b6\u6784\uff0c\u7ed3\u5408\u76ee\u6807\u6587\u672c\u63d0\u793a\uff0c\u540c\u65f6\u4f18\u5316\u9010\u5b57\u8f6c\u5f55\u548c\u9519\u8bef\u68c0\u6d4b\u3002", "result": "\u5728\u513f\u7ae5\u6717\u8bfb\u548c\u6210\u4eba\u975e\u5178\u578b\u8bed\u97f3\u6848\u4f8b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u8f6c\u5f55\u548c\u9519\u8bef\u68c0\u6d4b\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u6280\u672f\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u76ee\u6807\u6587\u672c\u63d0\u793a\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u53ef\u663e\u8457\u63d0\u5347\u6717\u8bfb\u9519\u8bef\u68c0\u6d4b\u6027\u80fd\u3002", "keywords": "\u6717\u8bfb\u9519\u8bef\u3001ASR\u3001\u7aef\u5230\u7aef\u8bad\u7ec3\u3001\u63d0\u793a\u3001\u8f6c\u5f55\u4f18\u5316"}}
{"id": "2505.23671", "pdf": "https://arxiv.org/pdf/2505.23671", "abs": "https://arxiv.org/abs/2505.23671", "authors": ["Manish Shetty", "Naman Jain", "Jinjian Liu", "Vijay Kethanaboyina", "Koushik Sen", "Ion Stoica"], "title": "GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Website: https://gso-bench.github.io/", "summary": "Developing high-performance software is a complex task that requires\nspecialized expertise. We introduce GSO, a benchmark for evaluating language\nmodels' capabilities in developing high-performance software. We develop an\nautomated pipeline that generates and executes performance tests to analyze\nrepository commit histories to identify 102 challenging optimization tasks\nacross 10 codebases, spanning diverse domains and programming languages. An\nagent is provided with a codebase and performance test as a precise\nspecification, and tasked to improve the runtime efficiency, which is measured\nagainst the expert developer optimization. Our quantitative evaluation reveals\nthat leading SWE-Agents struggle significantly, achieving less than 5% success\nrate, with limited improvements even with inference-time scaling. Our\nqualitative analysis identifies key failure modes, including difficulties with\nlow-level languages, practicing lazy optimization strategies, and challenges in\naccurately localizing bottlenecks. We release the code and artifacts of our\nbenchmark along with agent trajectories to enable future research.", "AI": {"tldr": "GSO benchmark evaluates language models' ability to optimize high-performance software, revealing low success rates (5%) and identifying key failure modes like low-level language challenges and bottleneck localization issues.", "motivation": "Assessing language models' capabilities in high-performance software development, given its complexity and the need for specialized expertise.", "method": "Automated pipeline generates performance tests from repository commit histories, identifying 102 optimization tasks across 10 diverse codebases. Agents improve runtime efficiency against expert benchmarks.", "result": "Leading SWE-Agents achieve <5% success rate, with minimal improvements despite inference-time scaling. Common failure modes include low-level language barriers and poor bottleneck identification.", "conclusion": "Current models struggle with high-performance optimization tasks, highlighting limitations in low-level coding and optimization strategy. Benchmark resources are released for future research.", "keywords": "GSO benchmark, high-performance software, language models, optimization tasks, SWE-Agents"}}
{"id": "2505.23352", "pdf": "https://arxiv.org/pdf/2505.23352", "abs": "https://arxiv.org/abs/2505.23352", "authors": ["Xu Shen", "Yixin Liu", "Yiwei Dai", "Yili Wang", "Rui Miao", "Yue Tan", "Shirui Pan", "Xin Wang"], "title": "Understanding the Information Propagation Effects of Communication Topologies in LLM-based Multi-Agent Systems", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "The communication topology in large language model-based multi-agent systems\nfundamentally governs inter-agent collaboration patterns, critically shaping\nboth the efficiency and effectiveness of collective decision-making. While\nrecent studies for communication topology automated design tend to construct\nsparse structures for efficiency, they often overlook why and when sparse and\ndense topologies help or hinder collaboration. In this paper, we present a\ncausal framework to analyze how agent outputs, whether correct or erroneous,\npropagate under topologies with varying sparsity. Our empirical studies reveal\nthat moderately sparse topologies, which effectively suppress error propagation\nwhile preserving beneficial information diffusion, typically achieve optimal\ntask performance. Guided by this insight, we propose a novel topology design\napproach, EIB-leanrner, that balances error suppression and beneficial\ninformation propagation by fusing connectivity patterns from both dense and\nsparse graphs. Extensive experiments show the superior effectiveness,\ncommunication cost, and robustness of EIB-leanrner.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u56e0\u679c\u6846\u67b6\u5206\u6790\u4e0d\u540c\u7a00\u758f\u5ea6\u7684\u901a\u4fe1\u62d3\u6251\u5982\u4f55\u5f71\u54cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u534f\u4f5c\uff0c\u53d1\u73b0\u9002\u5ea6\u7a00\u758f\u7684\u62d3\u6251\u80fd\u6291\u5236\u9519\u8bef\u4f20\u64ad\u540c\u65f6\u4fdd\u7559\u6709\u76ca\u4fe1\u606f\u6269\u6563\uff1b\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u65b0\u65b9\u6cd5EIB-leanrner\uff0c\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u503e\u5411\u4e8e\u8bbe\u8ba1\u7a00\u758f\u7684\u901a\u4fe1\u62d3\u6251\u4ee5\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u5ffd\u7565\u4e86\u7a00\u758f\u4e0e\u5bc6\u96c6\u62d3\u6251\u4f55\u65f6\u5bf9\u534f\u4f5c\u6709\u76ca\u6216\u6709\u5bb3\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "\u63d0\u51fa\u56e0\u679c\u6846\u67b6\u5206\u6790\u62d3\u6251\u7a00\u758f\u5ea6\u5bf9\u4fe1\u606f\u4f20\u64ad\u7684\u5f71\u54cd\uff0c\u5e76\u8bbe\u8ba1EIB-leanrner\u65b9\u6cd5\uff0c\u878d\u5408\u5bc6\u96c6\u4e0e\u7a00\u758f\u56fe\u7684\u8fde\u63a5\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEIB-leanrner\u5728\u4efb\u52a1\u6027\u80fd\u3001\u901a\u4fe1\u6210\u672c\u548c\u9c81\u68d2\u6027\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u9002\u5ea6\u7a00\u758f\u7684\u62d3\u6251\u80fd\u6700\u4f73\u5e73\u8861\u9519\u8bef\u6291\u5236\u4e0e\u4fe1\u606f\u4f20\u64ad\uff0cEIB-leanrner\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u8bbe\u8ba1\u539f\u5219\u7684\u6709\u6548\u6027\u3002", "keywords": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf,\u901a\u4fe1\u62d3\u6251,\u56e0\u679c\u5206\u6790,\u9519\u8bef\u4f20\u64ad,EIB-leanrner"}}
{"id": "2505.23634", "pdf": "https://arxiv.org/pdf/2505.23634", "abs": "https://arxiv.org/abs/2505.23634", "authors": ["John Halloran"], "title": "MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment", "categories": ["cs.LG", "cs.CR"], "comment": "27 pages, 19 figures, 4 tables", "summary": "The model context protocol (MCP) has been widely adapted as an open standard\nenabling the seamless integration of generative AI agents. However, recent work\nhas shown the MCP is susceptible to retrieval-based \"falsely benign\" attacks\n(FBAs), allowing malicious system access and credential theft, but requiring\nthat users download compromised files directly to their systems. Herein, we\nshow that the threat model of MCP-based attacks is significantly broader than\npreviously thought, i.e., attackers need only post malicious content online to\ndeceive MCP agents into carrying out their attacks on unsuspecting victims'\nsystems.\n  To improve alignment guardrails against such attacks, we introduce a new MCP\ndataset of FBAs and (truly) benign samples to explore the effectiveness of\ndirect preference optimization (DPO) for the refusal training of large language\nmodels (LLMs). While DPO improves model guardrails against such attacks, we\nshow that the efficacy of refusal learning varies drastically depending on the\nmodel's original post-training alignment scheme--e.g., GRPO-based LLMs learn to\nrefuse extremely poorly. Thus, to further improve FBA refusals, we introduce\nRetrieval Augmented Generation for Preference alignment (RAG-Pref), a novel\npreference alignment strategy based on RAG. We show that RAG-Pref significantly\nimproves the ability of LLMs to refuse FBAs, particularly when combined with\nDPO alignment, thus drastically improving guardrails against MCP-based attacks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86MCP\u534f\u8bae\u5b58\u5728\u66f4\u5e7f\u6cdb\u7684\u5a01\u80c1\u6a21\u578b\uff0c\u5373\u53ea\u9700\u5728\u7ebf\u53d1\u5e03\u6076\u610f\u5185\u5bb9\u5373\u53ef\u6b3a\u9a97MCP\u4ee3\u7406\u6267\u884c\u653b\u51fb\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7DPO\u548cRAG-Pref\u65b9\u6cd5\u63d0\u5347LLM\u5bf9FBA\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\u3002", "motivation": "MCP\u534f\u8bae\u4f5c\u4e3a\u5f00\u653e\u6807\u51c6\u867d\u5e7f\u6cdb\u7528\u4e8e\u751f\u6210\u5f0fAI\u4ee3\u7406\u96c6\u6210\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u53d1\u73b0\u5176\u6613\u53d7FBA\u653b\u51fb\u3002\u4f20\u7edf\u653b\u51fb\u9700\u7528\u6237\u76f4\u63a5\u4e0b\u8f7d\u6076\u610f\u6587\u4ef6\uff0c\u800c\u672c\u6587\u53d1\u73b0\u653b\u51fb\u8005\u4ec5\u9700\u5728\u7ebf\u53d1\u5e03\u5185\u5bb9\u5373\u53ef\u6b3a\u9a97\u4ee3\u7406\uff0c\u5a01\u80c1\u8303\u56f4\u8fdc\u8d85\u9884\u671f\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u65b0\u7684MCP\u6570\u636e\u96c6\uff08\u5305\u542bFBA\u548c\u826f\u6027\u6837\u672c\uff09\uff0c\u91c7\u7528DPO\u8bad\u7ec3LLM\u7684\u62d2\u7edd\u80fd\u529b\uff0c\u5e76\u63d0\u51faRAG-Pref\u65b9\u6cd5\uff08\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e0e\u504f\u597d\u5bf9\u9f50\uff09\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u9632\u5fa1\u8868\u73b0\u3002", "result": "DPO\u80fd\u63d0\u5347\u6a21\u578b\u9632\u5fa1\u80fd\u529b\uff0c\u4f46\u6548\u679c\u53d7\u521d\u59cb\u5bf9\u9f50\u65b9\u6848\u5f71\u54cd\uff08\u5982GRPO\u6a21\u578b\u8868\u73b0\u6781\u5dee\uff09\u3002RAG-Pref\u663e\u8457\u589e\u5f3a\u4e86LLM\u62d2\u7eddFBA\u7684\u80fd\u529b\uff0c\u5c24\u5176\u4e0eDPO\u7ed3\u5408\u65f6\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "RAG-Pref\u4e0eDPO\u7684\u7ed3\u5408\u663e\u8457\u5f3a\u5316\u4e86\u9488\u5bf9MCP\u653b\u51fb\u7684\u9632\u62a4\u673a\u5236\uff0c\u4e3a\u751f\u6210\u5f0fAI\u4ee3\u7406\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "MCP, FBA, DPO, RAG-Pref, LLMs, \u5b89\u5168\u6027"}}
{"id": "2505.23693", "pdf": "https://arxiv.org/pdf/2505.23693", "abs": "https://arxiv.org/abs/2505.23693", "authors": ["Tingyu Song", "Tongyan Hu", "Guo Gan", "Yilun Zhao"], "title": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "ACL 2025 Main", "summary": "MLLMs have been widely studied for video question answering recently.\nHowever, most existing assessments focus on natural videos, overlooking\nsynthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in\nvideo generation rely on MLLMs to evaluate the quality of generated videos, but\nthe capabilities of MLLMs on interpreting AIGC videos remain largely\nunderexplored. To address this, we propose a new benchmark, VF-Eval, which\nintroduces four tasks-coherence validation, error awareness, error type\ndetection, and reasoning evaluation-to comprehensively evaluate the abilities\nof MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that\neven the best-performing model, GPT-4.1, struggles to achieve consistently good\nperformance across all tasks. This highlights the challenging nature of our\nbenchmark. Additionally, to investigate the practical applications of VF-Eval\nin improving video generation, we conduct an experiment, RePrompt,\ndemonstrating that aligning MLLMs more closely with human feedback can benefit\nvideo generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6VF-Eval\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728AI\u751f\u6210\u5185\u5bb9\uff08AIGC\uff09\u89c6\u9891\u4e0a\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7RePrompt\u5b9e\u9a8c\u6539\u8fdb\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u591a\u805a\u7126\u4e8e\u81ea\u7136\u89c6\u9891\uff0c\u5ffd\u7565\u4e86AIGC\u89c6\u9891\u7684\u8bc4\u4f30\u9700\u6c42\uff0c\u540c\u65f6\u89c6\u9891\u751f\u6210\u9886\u57df\u4f9d\u8d56MLLMs\u8bc4\u4f30\u8d28\u91cf\uff0c\u4f46\u5176\u5bf9AIGC\u89c6\u9891\u7684\u7406\u89e3\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faVF-Eval\u57fa\u51c6\uff0c\u5305\u542b\u4e00\u81f4\u6027\u9a8c\u8bc1\u3001\u9519\u8bef\u611f\u77e5\u3001\u9519\u8bef\u7c7b\u578b\u68c0\u6d4b\u548c\u63a8\u7406\u8bc4\u4f30\u56db\u9879\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e8613\u79cd\u524d\u6cbfMLLMs\uff0c\u5e76\u901a\u8fc7RePrompt\u5b9e\u9a8c\u63a2\u7d22\u5176\u5e94\u7528\u3002", "result": "\u5373\u4f7f\u8868\u73b0\u6700\u4f73\u7684GPT-4.1\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u8868\u73b0\u4e5f\u4e0d\u4e00\u81f4\uff0c\u663e\u793a\u4e86\u57fa\u51c6\u7684\u6311\u6218\u6027\uff1bRePrompt\u5b9e\u9a8c\u8868\u660eMLLMs\u4e0e\u4eba\u7c7b\u53cd\u9988\u5bf9\u9f50\u53ef\u6539\u5584\u89c6\u9891\u751f\u6210\u3002", "conclusion": "VF-Eval\u6709\u6548\u63ed\u793a\u4e86MLLMs\u5728AIGC\u89c6\u9891\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7RePrompt\u9a8c\u8bc1\u4e86\u6539\u8fdb\u6f5c\u529b\u3002", "keywords": "MLLMs, AIGC, VF-Eval, RePrompt, video generation"}}
{"id": "2505.23353", "pdf": "https://arxiv.org/pdf/2505.23353", "abs": "https://arxiv.org/abs/2505.23353", "authors": ["Alexandra G. Roberts", "Ha M. Luu", "Mert \u015ei\u015fman", "Alexey V. Dimov", "Ceren Tozlu", "Ilhami Kovanlikaya", "Susan A. Gauthier", "Thanh D. Nguyen", "Yi Wang"], "title": "Synthetic Generation and Latent Projection Denoising of Rim Lesions in Multiple Sclerosis", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted full paper in Synthetic Data @ CVPR 2025 12 pages, 10\n  figures", "summary": "Quantitative susceptibility maps from magnetic resonance images can provide\nboth prognostic and diagnostic information in multiple sclerosis, a\nneurodegenerative disease characterized by the formation of lesions in white\nmatter brain tissue. In particular, susceptibility maps provide adequate\ncontrast to distinguish between \"rim\" lesions, surrounded by deposited\nparamagnetic iron, and \"non-rim\" lesion types. These paramagnetic rim lesions\n(PRLs) are an emerging biomarker in multiple sclerosis. Much effort has been\ndevoted to both detection and segmentation of such lesions to monitor\nlongitudinal change. As paramagnetic rim lesions are rare, addressing this\nproblem requires confronting the class imbalance between rim and non-rim\nlesions. We produce synthetic quantitative susceptibility maps of paramagnetic\nrim lesions and show that inclusion of such synthetic data improves classifier\nperformance and provide a multi-channel extension to generate accompanying\ncontrasts and probabilistic segmentation maps. We exploit the projection\ncapability of our trained generative network to demonstrate a novel denoising\napproach that allows us to train on ambiguous rim cases and substantially\nincrease the minority class. We show that both synthetic lesion synthesis and\nour proposed rim lesion label denoising method best approximate the unseen rim\nlesion distribution and improve detection in a clinically interpretable manner.\nWe release our code and generated data at https://github.com/agr78/PRLx-GAN\nupon publication.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u751f\u6210\u5408\u6210\u5b9a\u91cf\u78c1\u5316\u7387\u56fe\uff08QSM\uff09\u6765\u89e3\u51b3\u591a\u53d1\u6027\u786c\u5316\u75c7\u4e2d\u7f55\u89c1\u78c1\u6027\u8fb9\u7f18\u75c5\u7076\uff08PRLs\uff09\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53bb\u566a\u65b9\u6cd5\u548c\u591a\u901a\u9053\u6269\u5c55\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u5668\u6027\u80fd\u3002", "motivation": "\u591a\u53d1\u6027\u786c\u5316\u75c7\u4e2d\u78c1\u6027\u8fb9\u7f18\u75c5\u7076\uff08PRLs\uff09\u662f\u4e00\u79cd\u65b0\u5174\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4f46\u5176\u7f55\u89c1\u6027\u5bfc\u81f4\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5f71\u54cd\u68c0\u6d4b\u548c\u5206\u5272\u7684\u51c6\u786e\u6027\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u65b0\u6280\u672f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5408\u6210QSM\u56fe\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u591a\u901a\u9053\u6269\u5c55\u751f\u6210\u4f34\u968f\u7684\u5bf9\u6bd4\u5ea6\u548c\u6982\u7387\u5206\u5272\u56fe\u3002\u6b64\u5916\uff0c\u5229\u7528\u751f\u6210\u7f51\u7edc\u7684\u6295\u5f71\u80fd\u529b\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u53bb\u566a\u65b9\u6cd5\uff0c\u4ee5\u8bad\u7ec3\u6a21\u7cca\u8fb9\u7f18\u75c5\u4f8b\u5e76\u589e\u52a0\u5c11\u6570\u7c7b\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5408\u6210\u6570\u636e\u7684\u5f15\u5165\u548c\u65b0\u7684\u53bb\u566a\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u903c\u8fd1\u771f\u5b9ePRLs\u5206\u5e03\uff0c\u5e76\u4ee5\u4e34\u5e8a\u53ef\u89e3\u91ca\u7684\u65b9\u5f0f\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u53bb\u566a\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86PRLs\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u4e34\u5e8a\u7ed3\u679c\u3002", "keywords": "\u5b9a\u91cf\u78c1\u5316\u7387\u56fe\uff08QSM\uff09, \u591a\u53d1\u6027\u786c\u5316\u75c7, \u78c1\u6027\u8fb9\u7f18\u75c5\u7076\uff08PRLs\uff09, \u7c7b\u522b\u4e0d\u5e73\u8861, \u5408\u6210\u6570\u636e, \u53bb\u566a"}}
{"id": "2505.23640", "pdf": "https://arxiv.org/pdf/2505.23640", "abs": "https://arxiv.org/abs/2505.23640", "authors": ["Yilin Xie", "Shiqiang Zhang", "Jixiang Qing", "Ruth Misener", "Calvin Tsay"], "title": "Global optimization of graph acquisition functions for neural architecture search", "categories": ["cs.LG", "math.OC"], "comment": "19 pages, 6 figures, 3 tables", "summary": "Graph Bayesian optimization (BO) has shown potential as a powerful and\ndata-efficient tool for neural architecture search (NAS). Most existing graph\nBO works focus on developing graph surrogates models, i.e., metrics of networks\nand/or different kernels to quantify the similarity between networks. However,\nthe acquisition optimization, as a discrete optimization task over graph\nstructures, is not well studied due to the complexity of formulating the graph\nsearch space and acquisition functions. This paper presents explicit\noptimization formulations for graph input space including properties such as\nreachability and shortest paths, which are used later to formulate graph\nkernels and the acquisition function. We theoretically prove that the proposed\nencoding is an equivalent representation of the graph space and provide\nrestrictions for the NAS domain with either node or edge labels. Numerical\nresults over several NAS benchmarks show that our method efficiently finds the\noptimal architecture for most cases, highlighting its efficacy.", "AI": {"tldr": "Graph Bayesian optimization (BO) for neural architecture search (NAS) improves acquisition optimization via explicit graph space formulations, proven effective in NAS benchmarks.", "motivation": "Existing graph BO works focus on surrogate models but neglect discrete optimization over graph structures. This paper addresses the complexity of graph search space and acquisition functions.", "method": "Proposes explicit optimization formulations for graph input space (e.g., reachability, shortest paths) to derive graph kernels and acquisition functions, with theoretical proof of equivalence to graph space.", "result": "Numerical results show the method efficiently finds optimal architectures in most NAS benchmarks.", "conclusion": "The proposed approach effectively addresses challenges in graph BO and demonstrates strong performance in NAS.", "keywords": "Graph Bayesian optimization, neural architecture search, acquisition function, graph kernels"}}
{"id": "2505.23761", "pdf": "https://arxiv.org/pdf/2505.23761", "abs": "https://arxiv.org/abs/2505.23761", "authors": ["Yunjae Won", "Hyunji Lee", "Hyeonbin Hwang", "Minjoon Seo"], "title": "Differential Information: An Information-Theoretic Perspective on Preference Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "41 pages, 13 figures; due to the 1,920-character limitation imposed\n  on the abstract field by arXiv, the abstract included on the arXiv page is\n  slightly abbreviated compared to the version presented in the PDF", "summary": "Direct Preference Optimization (DPO) has become a standard technique for\naligning language models with human preferences in a supervised manner. Despite\nits empirical success, the theoretical justification behind its log-ratio\nreward parameterization remains incomplete. In this work, we address this gap\nby utilizing the Differential Information Distribution (DID): a distribution\nover token sequences that captures the information gained during policy\nupdates. First, we show that when preference labels encode the differential\ninformation required to transform a reference policy into a target policy, the\nlog-ratio reward in DPO emerges as the uniquely optimal form for learning the\ntarget policy via preference optimization. This result naturally yields a\nclosed-form expression for the optimal sampling distribution over rejected\nresponses. Second, we find that the condition for preferences to encode\ndifferential information is fundamentally linked to an implicit assumption\nregarding log-margin ordered policies-an inductive bias widely used in\npreference optimization yet previously unrecognized. Finally, by analyzing the\nentropy of the DID, we characterize how learning low-entropy differential\ninformation reinforces the policy distribution, while high-entropy differential\ninformation induces a smoothing effect, which explains the log-likelihood\ndisplacement phenomenon. We validate our theoretical findings in synthetic\nexperiments and extend them to real-world instruction-following datasets. Our\nresults suggest that learning high-entropy differential information is crucial\nfor general instruction-following, while learning low-entropy differential\ninformation benefits knowledge-intensive question answering. Overall, our work\npresents a unifying perspective on the DPO objective, the structure of\npreference data, and resulting policy behaviors through the lens of\ndifferential information.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5fae\u5206\u4fe1\u606f\u5206\u5e03\uff08DID\uff09\u7406\u8bba\u586b\u8865\u4e86\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u7684log-ratio\u5956\u52b1\u53c2\u6570\u5316\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u5206\u6790\u4e86\u504f\u597d\u6807\u7b7e\u5982\u4f55\u7f16\u7801\u4fe1\u606f\u4ee5\u4f18\u5316\u76ee\u6807\u7b56\u7565\uff0c\u5e76\u63a2\u8ba8\u4e86\u9ad8\u4f4e\u71b5\u4fe1\u606f\u5bf9\u7b56\u7565\u5206\u5e03\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4e3aDPO\u4e2dlog-ratio\u5956\u52b1\u53c2\u6570\u5316\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u901a\u8fc7DID\u6846\u67b6\u89e3\u91ca\u504f\u597d\u4f18\u5316\u4e2d\u7684\u7b56\u7565\u884c\u4e3a\u548c\u4fe1\u606f\u5206\u5e03\u7684\u5f71\u54cd\u3002", "method": "\u5229\u7528\u5fae\u5206\u4fe1\u606f\u5206\u5e03\uff08DID\uff09\u5206\u6790\u504f\u597d\u6807\u7b7e\u548c\u4fe1\u606f\u589e\u76ca\uff0c\u63a8\u5bfc\u51falog-ratio\u5956\u52b1\u7684\u552f\u4e00\u6700\u4f18\u5f62\u5f0f\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u5b9e\u9a8c\u548c\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\u7406\u8bba\u3002", "result": "\u8bc1\u660e\u4e86\u4f4e\u71b5\u4fe1\u606f\u5f3a\u5316\u7b56\u7565\u5206\u5e03\uff0c\u9ad8\u71b5\u4fe1\u606f\u5e73\u6ed1\u7b56\u7565\u5206\u5e03\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d1\u73b0\u9ad8\u71b5\u4fe1\u606f\u5bf9\u901a\u7528\u6307\u4ee4\u66f4\u6709\u6548\uff0c\u4f4e\u71b5\u4fe1\u606f\u5bf9\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u66f4\u6709\u5229\u3002", "conclusion": "\u7814\u7a76\u4e3aDPO\u76ee\u6807\u3001\u504f\u597d\u6570\u636e\u7ed3\u6784\u548c\u7b56\u7565\u884c\u4e3a\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u89c6\u89d2\uff0c\u5f3a\u8c03\u4e86\u5fae\u5206\u4fe1\u606f\u5728\u504f\u597d\u4f18\u5316\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "keywords": "\u76f4\u63a5\u504f\u597d\u4f18\u5316, log-ratio\u5956\u52b1, \u5fae\u5206\u4fe1\u606f\u5206\u5e03, \u7b56\u7565\u884c\u4e3a, \u4fe1\u606f\u71b5"}}
{"id": "2505.23354", "pdf": "https://arxiv.org/pdf/2505.23354", "abs": "https://arxiv.org/abs/2505.23354", "authors": ["Meital Bojan", "Sanketh Vedula", "Advaith Maddipatla", "Nadav Bojan Sellam", "Federico Napoli", "Paul Schanda", "Alex M. Bronstein"], "title": "Representing local protein environments with atomistic foundation models", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "The local structure of a protein strongly impacts its function and\ninteractions with other molecules. Therefore, a concise, informative\nrepresentation of a local protein environment is essential for modeling and\ndesigning proteins and biomolecular interactions. However, these environments'\nextensive structural and chemical variability makes them challenging to model,\nand such representations remain under-explored. In this work, we propose a\nnovel representation for a local protein environment derived from the\nintermediate features of atomistic foundation models (AFMs). We demonstrate\nthat this embedding effectively captures both local structure (e.g., secondary\nmotifs), and chemical features (e.g., amino-acid identity and protonation\nstate). We further show that the AFM-derived representation space exhibits\nmeaningful structure, enabling the construction of data-driven priors over the\ndistribution of biomolecular environments. Finally, in the context of\nbiomolecular NMR spectroscopy, we demonstrate that the proposed representations\nenable a first-of-its-kind physics-informed chemical shift predictor that\nachieves state-of-the-art accuracy. Our results demonstrate the surprising\neffectiveness of atomistic foundation models and their emergent representations\nfor protein modeling beyond traditional molecular simulations. We believe this\nwill open new lines of work in constructing effective functional\nrepresentations for protein environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u539f\u5b50\u57fa\u7840\u6a21\u578b\uff08AFM\uff09\u4e2d\u95f4\u7279\u5f81\u7684\u86cb\u767d\u8d28\u5c40\u90e8\u73af\u5883\u8868\u793a\u65b9\u6cd5\uff0c\u80fd\u591f\u6355\u6349\u5c40\u90e8\u7ed3\u6784\u548c\u5316\u5b66\u7279\u5f81\uff0c\u5e76\u5728\u751f\u7269\u5206\u5b50NMR\u5149\u8c31\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5316\u5b66\u4f4d\u79fb\u9884\u6d4b\u3002", "motivation": "\u86cb\u767d\u8d28\u7684\u5c40\u90e8\u7ed3\u6784\u5bf9\u5176\u529f\u80fd\u53ca\u4e0e\u5176\u4ed6\u5206\u5b50\u7684\u76f8\u4e92\u4f5c\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f53\u524d\u5bf9\u8fd9\u4e9b\u73af\u5883\u7684\u8868\u793a\u4ecd\u6709\u9650\u4e14\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5229\u7528\u539f\u5b50\u57fa\u7840\u6a21\u578b\u7684\u4e2d\u95f4\u7279\u5f81\u751f\u6210\u86cb\u767d\u8d28\u5c40\u90e8\u73af\u5883\u7684\u8868\u793a\uff0c\u6355\u6349\u7ed3\u6784\u548c\u5316\u5b66\u7279\u5f81\u3002", "result": "AFM\u751f\u6210\u7684\u8868\u793a\u7a7a\u95f4\u5177\u6709\u6709\u610f\u4e49\u7684\u7ed3\u6784\uff0c\u80fd\u591f\u6784\u5efa\u751f\u7269\u5206\u5b50\u73af\u5883\u5206\u5e03\u7684\u6570\u636e\u9a71\u52a8\u5148\u9a8c\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5316\u5b66\u4f4d\u79fb\u9884\u6d4b\u3002", "conclusion": "\u539f\u5b50\u57fa\u7840\u6a21\u578b\u53ca\u5176\u6d8c\u73b0\u8868\u793a\u5728\u86cb\u767d\u8d28\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u60ca\u4eba\u7684\u6709\u6548\u6027\uff0c\u4e3a\u86cb\u767d\u8d28\u73af\u5883\u7684\u51fd\u6570\u8868\u793a\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u86cb\u767d\u8d28\u5c40\u90e8\u73af\u5883, \u539f\u5b50\u57fa\u7840\u6a21\u578b, \u5316\u5b66\u4f4d\u79fb\u9884\u6d4b, \u751f\u7269\u5206\u5b50NMR, \u6570\u636e\u9a71\u52a8\u5148\u9a8c"}}
{"id": "2505.23648", "pdf": "https://arxiv.org/pdf/2505.23648", "abs": "https://arxiv.org/abs/2505.23648", "authors": ["Halil Alperen Gozeten", "M. Emrullah Ildiz", "Xuechen Zhang", "Hrayr Harutyunyan", "Ankit Singh Rawat", "Samet Oymak"], "title": "Continuous Chain of Thought Enables Parallel Exploration and Reasoning", "categories": ["cs.LG"], "comment": null, "summary": "Current language models generate chain-of-thought traces by autoregressively\nsampling tokens from a finite vocabulary. While this discrete sampling has\nachieved remarkable success, conducting chain-of-thought with\ncontinuously-valued tokens (CoT2) offers a richer and more expressive\nalternative. Our work examines the benefits of CoT2 through logical reasoning\ntasks that inherently require search capabilities and provide optimization and\nexploration methods for CoT2. Theoretically, we show that CoT2 allows the model\nto track multiple traces in parallel and quantify its benefits for inference\nefficiency. Notably, one layer transformer equipped with CoT2 can provably\nsolve the combinatorial \"subset sum problem\" given sufficient embedding\ndimension. These insights lead to a novel and effective supervision strategy\nwhere we match the softmax outputs to the empirical token distributions of a\nset of target traces. Complementing this, we introduce sampling strategies that\nunlock policy optimization and self-improvement for CoT2. Our first strategy\nsamples and composes $K$ discrete tokens at each decoding step to control the\nlevel of parallelism, and reduces to standard CoT when $K=1$. Our second\nstrategy relies on continuous exploration over the probability simplex.\nExperiments confirm that policy optimization with CoT2 indeed improves the\nperformance of the model beyond its initial discrete or continuous supervision.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u8fde\u7eed\u503c\u6807\u8bb0\uff08CoT2\uff09\u8fdb\u884c\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u903b\u8f91\u4efb\u52a1\u9a8c\u8bc1\u5176\u8868\u8fbe\u80fd\u529b\u548c\u6548\u7387\u4f18\u52bf\uff0c\u5e76\u5c55\u73b0\u4e86\u76d1\u7763\u548c\u91c7\u6837\u7b56\u7565\u7684\u4f18\u5316\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u8fde\u7eed\u503c\u6807\u8bb0\u5728\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u76f8\u8f83\u4e8e\u79bb\u6563\u91c7\u6837\uff0c\u5b83\u80fd\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u8868\u8fbe\u548c\u5e76\u884c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u7406\u8bba\u5206\u6790CoT2\u7684\u5e76\u884c\u63a8\u7406\u4f18\u52bf\uff0c\u63d0\u51fa\u5339\u914d\u8f6f\u8f93\u51fa\u4e0e\u76ee\u6807\u5206\u5e03\u7684\u65b0\u76d1\u7763\u7b56\u7565\uff0c\u4ee5\u53ca\u4e24\u79cd\u91c7\u6837\u7b56\u7565\uff08K\u6807\u8bb0\u91c7\u6837\u548c\u8fde\u7eed\u63a2\u7d22\uff09\u7528\u4e8e\u4f18\u5316\u548c\u81ea\u6539\u8fdb\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cCoT2\u5728\u7ec4\u5408\u95ee\u9898\uff08\u5982\u5b50\u96c6\u548c\u95ee\u9898\uff09\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u4e14\u4f18\u5316\u540e\u7684\u6a21\u578b\u6027\u80fd\u8d85\u8fc7\u521d\u59cb\u79bb\u6563\u6216\u8fde\u7eed\u76d1\u7763\u3002", "conclusion": "\u7ed3\u8bba\u662fCoT2\u4e3a\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u6846\u67b6\uff0c\u5176\u76d1\u7763\u548c\u91c7\u6837\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "keywords": "\u94fe\u5f0f\u601d\u7ef4\u3001\u8fde\u7eed\u503c\u6807\u8bb0\u3001\u903b\u8f91\u63a8\u7406\u3001\u5e76\u884c\u63a8\u7406\u3001\u76d1\u7763\u7b56\u7565\u3001\u91c7\u6837\u4f18\u5316"}}
{"id": "2505.23367", "pdf": "https://arxiv.org/pdf/2505.23367", "abs": "https://arxiv.org/abs/2505.23367", "authors": ["Jeonghyeok Do", "Sungpyo Kim", "Geunhyuk Youk", "Jaehyup Lee", "Munchurl Kim"], "title": "PAN-Crafter: Learning Modality-Consistent Alignment for PAN-Sharpening", "categories": ["cs.CV", "cs.AI"], "comment": "Please visit our project page\n  https://kaist-viclab.github.io/PAN-Crafter_site", "summary": "PAN-sharpening aims to fuse high-resolution panchromatic (PAN) images with\nlow-resolution multi-spectral (MS) images to generate high-resolution\nmulti-spectral (HRMS) outputs. However, cross-modality misalignment -- caused\nby sensor placement, acquisition timing, and resolution disparity -- induces a\nfundamental challenge. Conventional deep learning methods assume perfect\npixel-wise alignment and rely on per-pixel reconstruction losses, leading to\nspectral distortion, double edges, and blurring when misalignment is present.\nTo address this, we propose PAN-Crafter, a modality-consistent alignment\nframework that explicitly mitigates the misalignment gap between PAN and MS\nmodalities. At its core, Modality-Adaptive Reconstruction (MARs) enables a\nsingle network to jointly reconstruct HRMS and PAN images, leveraging PAN's\nhigh-frequency details as auxiliary self-supervision. Additionally, we\nintroduce Cross-Modality Alignment-Aware Attention (CM3A), a novel mechanism\nthat bidirectionally aligns MS texture to PAN structure and vice versa,\nenabling adaptive feature refinement across modalities. Extensive experiments\non multiple benchmark datasets demonstrate that our PAN-Crafter outperforms the\nmost recent state-of-the-art method in all metrics, even with 50.11$\\times$\nfaster inference time and 0.63$\\times$ the memory size. Furthermore, it\ndemonstrates strong generalization performance on unseen satellite datasets,\nshowing its robustness across different conditions.", "AI": {"tldr": "PAN-sharpening\u8bba\u6587\u63d0\u51faPAN-Crafter\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u6001\u4e00\u81f4\u6027\u5bf9\u9f50\u89e3\u51b3PAN\u4e0eMS\u56fe\u50cf\u7684\u591a\u6a21\u6001\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347HRMS\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u4e0d\u5bf9\u9f50\u60c5\u51b5\u4e0b\u5bfc\u81f4\u7684\u9891\u8c31\u5931\u771f\u3001\u53cc\u8fb9\u7f18\u548c\u6a21\u7cca\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6a21\u6001\u81ea\u9002\u5e94\u91cd\u5efa\uff08MARs\uff09\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u611f\u77e5\u6ce8\u610f\u529b\uff08CM3A\uff09\u673a\u5236\uff0c\u8054\u5408\u91cd\u5efaHRMS\u548cPAN\u56fe\u50cf\uff0c\u5e76\u53cc\u5411\u5bf9\u9f50\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u534750.11\u500d\uff0c\u5185\u5b58\u5360\u7528\u51cf\u5c110.63\u500d\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "conclusion": "PAN-Crafter\u5728\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u6548\u7387\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u536b\u661f\u6570\u636e\u96c6\u3002", "keywords": "PAN-sharpening, cross-modality alignment, deep learning, satellite imagery"}}
{"id": "2505.23651", "pdf": "https://arxiv.org/pdf/2505.23651", "abs": "https://arxiv.org/abs/2505.23651", "authors": ["Juncheol Shin", "Minsang Seok", "Seonggon Kim", "Eunhyeok Park"], "title": "Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation", "categories": ["cs.LG", "cs.CV"], "comment": "ICML 2025. Code: https://github.com/ewsn1593/HDRQ", "summary": "Model merging has emerged as a powerful technique for combining task-specific\nweights, achieving superior performance in multi-target domain adaptation.\nHowever, when applied to practical scenarios, such as quantized models, new\nchallenges arise. In practical scenarios, quantization is often applied to\ntarget-specific data, but this process restricts the domain of interest and\nintroduces discretization effects, making model merging highly non-trivial. In\nthis study, we analyze the impact of quantization on model merging through the\nlens of error barriers. Leveraging these insights, we propose a novel\npost-training quantization, HDRQ - Hessian and distant regularizing\nquantization - that is designed to consider model merging for multi-target\ndomain adaptation. Our approach ensures that the quantization process incurs\nminimal deviation from the source pre-trained model while flattening the loss\nsurface to facilitate smooth model merging. To our knowledge, this is the first\nstudy on this challenge, and extensive experiments confirm its effectiveness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u91cf\u5316\u5bf9\u6a21\u578b\u5408\u5e76\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5HDRQ\uff0c\u65e8\u5728\u4f18\u5316\u591a\u76ee\u6807\u57df\u9002\u5e94\u7684\u6a21\u578b\u5408\u5e76\u6548\u679c\u3002", "motivation": "\u5b9e\u9645\u573a\u666f\u4e2d\u91cf\u5316\u4f1a\u9650\u5236\u5174\u8da3\u57df\u5e76\u5f15\u5165\u79bb\u6563\u5316\u6548\u5e94\uff0c\u4f7f\u6a21\u578b\u5408\u5e76\u53d8\u5f97\u590d\u6742\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u517c\u987e\u5408\u5e76\u6548\u679c\u7684\u65b0\u578b\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5206\u6790\u91cf\u5316\u5bf9\u6a21\u578b\u5408\u5e76\u7684\u8bef\u5dee\u5c4f\u969c\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86HDRQ\u65b9\u6cd5\uff0c\u7ed3\u5408Hessian\u548c\u8ddd\u79bb\u6b63\u5219\u5316\u8fdb\u884c\u91cf\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eHDRQ\u5728\u91cf\u5316\u65f6\u6700\u5c0f\u5316\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u504f\u79bb\uff0c\u540c\u65f6\u5e73\u6ed1\u635f\u5931\u8868\u9762\uff0c\u6709\u6548\u4fc3\u8fdb\u6a21\u578b\u5408\u5e76\u3002", "conclusion": "HDRQ\u4e3a\u91cf\u5316\u6a21\u578b\u5728\u591a\u76ee\u6807\u57df\u9002\u5e94\u4e2d\u7684\u5408\u5e76\u63d0\u4f9b\u4e86\u9996\u4e2a\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "keywords": "\u6a21\u578b\u5408\u5e76, \u91cf\u5316, \u591a\u76ee\u6807\u57df\u9002\u5e94, HDRQ, \u540e\u8bad\u7ec3\u91cf\u5316"}}
{"id": "2505.23764", "pdf": "https://arxiv.org/pdf/2505.23764", "abs": "https://arxiv.org/abs/2505.23764", "authors": ["Sihan Yang", "Runsen Xu", "Yiman Xie", "Sizhe Yang", "Mo Li", "Jingli Lin", "Chenming Zhu", "Xiaochen Chen", "Haodong Duan", "Xiangyu Yue", "Dahua Lin", "Tai Wang", "Jiangmiao Pang"], "title": "MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence", "categories": ["cs.CV", "cs.CL"], "comment": "34 pages. A comprehensive, fully human-curated, multi-image-based\n  spatial intelligence benchmark with reasoning annotation for MLLMs. Project\n  page: https://runsenxu.com/projects/MMSI_Bench", "summary": "Spatial intelligence is essential for multimodal large language models\n(MLLMs) operating in the complex physical world. Existing benchmarks, however,\nprobe only single-image relations and thus fail to assess the multi-image\nspatial reasoning that real-world deployments demand. We introduce MMSI-Bench,\na VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision\nresearchers spent more than 300 hours meticulously crafting 1,000 challenging,\nunambiguous multiple-choice questions from over 120,000 images, each paired\nwith carefully designed distractors and a step-by-step reasoning process. We\nconduct extensive experiments and thoroughly evaluate 34 open-source and\nproprietary MLLMs, observing a wide gap: the strongest open-source model\nattains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while\nhumans score 97%. These results underscore the challenging nature of MMSI-Bench\nand the substantial headroom for future research. Leveraging the annotated\nreasoning processes, we also provide an automated error analysis pipeline that\ndiagnoses four dominant failure modes, including (1) grounding errors, (2)\noverlap-matching and scene-reconstruction errors, (3) situation-transformation\nreasoning errors, and (4) spatial-logic errors, offering valuable insights for\nadvancing multi-image spatial intelligence. Project page:\nhttps://runsenxu.com/projects/MMSI_Bench .", "AI": {"tldr": "MMSI-Bench is a new benchmark for multi-image spatial intelligence in MLLMs, highlighting a significant performance gap between models and humans.", "motivation": "Existing benchmarks focus on single-image relations, failing to address the multi-image spatial reasoning needed for real-world applications.", "method": "Developed a VQA benchmark with 1,000 challenging questions from 120,000+ images, evaluated 34 MLLMs, and provided an error analysis pipeline.", "result": "Top open-source model scored 30% accuracy, proprietary model 40%, while humans achieved 97%.", "conclusion": "MMSI-Bench is challenging and reveals substantial room for improvement in multi-image spatial intelligence.", "keywords": "spatial intelligence, MLLMs, VQA benchmark, multi-image reasoning"}}
{"id": "2505.23653", "pdf": "https://arxiv.org/pdf/2505.23653", "abs": "https://arxiv.org/abs/2505.23653", "authors": ["Jiaran Ye", "Zijun Yao", "Zhidian Huang", "Liangming Pan", "Jinxin Liu", "Yushi Bai", "Amy Xin", "Liu Weichuan", "Xiaoyin Che", "Lei Hou", "Juanzi Li"], "title": "How does Transformer Learn Implicit Reasoning?", "categories": ["cs.LG"], "comment": null, "summary": "Recent work suggests that large language models (LLMs) can perform multi-hop\nreasoning implicitly -- producing correct answers without explicitly\nverbalizing intermediate steps -- but the underlying mechanisms remain poorly\nunderstood. In this paper, we study how such implicit reasoning emerges by\ntraining transformers from scratch in a controlled symbolic environment. Our\nanalysis reveals a three-stage developmental trajectory: early memorization,\nfollowed by in-distribution generalization, and eventually cross-distribution\ngeneralization. We find that training with atomic triples is not necessary but\naccelerates learning, and that second-hop generalization relies on query-level\nexposure to specific compositional structures. To interpret these behaviors, we\nintroduce two diagnostic tools: cross-query semantic patching, which identifies\nsemantically reusable intermediate representations, and a cosine-based\nrepresentational lens, which reveals that successful reasoning correlates with\nthe cosine-base clustering in hidden space. This clustering phenomenon in turn\nprovides a coherent explanation for the behavioral dynamics observed across\ntraining, linking representational structure to reasoning capability. These\nfindings provide new insights into the interpretability of implicit multi-hop\nreasoning in LLMs, helping to clarify how complex reasoning processes unfold\ninternally and offering pathways to enhance the transparency of such models.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u901a\u8fc7\u53d7\u63a7\u7b26\u53f7\u73af\u5883\u4e2d\u7684\u8bad\u7ec3\u5b9e\u73b0\u9690\u5f0f\u591a\u8df3\u63a8\u7406\uff0c\u63ed\u793a\u4e86\u4e09\u4e2a\u9636\u6bb5\u7684\u5b66\u4e60\u8f68\u8ff9\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u8bca\u65ad\u5de5\u5177\u6765\u89e3\u91ca\u8fd9\u4e9b\u884c\u4e3a\u3002", "motivation": "\u63a2\u7a76LLMs\u9690\u5f0f\u591a\u8df3\u63a8\u7406\u7684\u5e95\u5c42\u673a\u5236\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u3002", "method": "\u901a\u8fc7\u5728\u53d7\u63a7\u7b26\u53f7\u73af\u5883\u4e2d\u4ece\u5934\u8bad\u7ec3Transformer\uff0c\u5206\u6790\u5b66\u4e60\u8f68\u8ff9\uff0c\u5e76\u63d0\u51fa\u8de8\u67e5\u8be2\u8bed\u4e49\u4fee\u8865\u548c\u4f59\u5f26\u57fa\u8868\u793a\u5de5\u5177\u3002", "result": "\u53d1\u73b0\u9690\u5f0f\u63a8\u7406\u7684\u4e09\u4e2a\u9636\u6bb5\uff1a\u65e9\u671f\u8bb0\u5fc6\u3001\u5206\u5e03\u5185\u6cdb\u5316\u548c\u8de8\u5206\u5e03\u6cdb\u5316\uff1b\u539f\u5b50\u4e09\u5143\u7ec4\u8bad\u7ec3\u52a0\u901f\u5b66\u4e60\uff0c\u7279\u5b9a\u7ec4\u5408\u7ed3\u6784\u7684\u67e5\u8be2\u7ea7\u66b4\u9732\u662f\u5173\u952e\u3002", "conclusion": "\u7814\u7a76\u4e3aLLMs\u7684\u9690\u5f0f\u591a\u8df3\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u94fe\u63a5\u4e86\u8868\u793a\u7ed3\u6784\u4e0e\u63a8\u7406\u80fd\u529b\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u9690\u5f0f\u591a\u8df3\u63a8\u7406,\u8868\u793a\u5b66\u4e60,\u53ef\u89e3\u91ca\u6027,Transformer"}}
{"id": "2505.23386", "pdf": "https://arxiv.org/pdf/2505.23386", "abs": "https://arxiv.org/abs/2505.23386", "authors": ["Han Bao", "Qinying Wang", "Zhi Chen", "Qingming Li", "Xuhong Zhang", "Changjiang Li", "Zonghui Wang", "Shouling Ji", "Wenzhi Chen"], "title": "VModA: An Effective Framework for Adaptive NSFW Image Moderation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Not Safe/Suitable for Work (NSFW) content is rampant on social networks and\nposes serious harm to citizens, especially minors. Current detection methods\nmainly rely on deep learning-based image recognition and classification.\nHowever, NSFW images are now presented in increasingly sophisticated ways,\noften using image details and complex semantics to obscure their true nature or\nattract more views. Although still understandable to humans, these images often\nevade existing detection methods, posing a significant threat. Further\ncomplicating the issue, varying regulations across platforms and regions create\nadditional challenges for effective moderation, leading to detection bias and\nreduced accuracy. To address this, we propose VModA, a general and effective\nframework that adapts to diverse moderation rules and handles complex,\nsemantically rich NSFW content across categories. Experimental results show\nthat VModA significantly outperforms existing methods, achieving up to a 54.3%\naccuracy improvement across NSFW types, including those with complex semantics.\nFurther experiments demonstrate that our method exhibits strong adaptability\nacross categories, scenarios, and base VLMs. We also identified inconsistent\nand controversial label samples in public NSFW benchmark datasets, re-annotated\nthem, and submitted corrections to the original maintainers. Two datasets have\nconfirmed the updates so far. Additionally, we evaluate VModA in real-world\nscenarios to demonstrate its practical effectiveness.", "AI": {"tldr": "VModA\u6846\u67b6\u63d0\u5347\u4e86NSFW\u5185\u5bb9\u7684\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u9002\u5e94\u591a\u6837\u5316\u5ba1\u6838\u89c4\u5219\uff0c\u4e14\u5728\u4e0d\u540c\u7c7b\u522b\u3001\u573a\u666f\u548cVLMs\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "NSFW\u5185\u5bb9\u5728\u793e\u4ea4\u5a92\u4f53\u6cdb\u6ee5\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u8bed\u4e49\u548c\u591a\u6837\u5316\u5ba1\u6838\u89c4\u5219\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faVModA\u6846\u67b6\uff0c\u9002\u5e94\u591a\u6837\u5316\u5ba1\u6838\u89c4\u5219\uff0c\u5904\u7406\u8bed\u4e49\u590d\u6742\u7684NSFW\u5185\u5bb9\u3002", "result": "VModA\u5728\u591a\u4e2aNSFW\u7c7b\u578b\u4e2d\u51c6\u786e\u7387\u63d0\u534754.3%\uff0c\u5e76\u663e\u793a\u4e86\u8de8\u7c7b\u522b\u548c\u573a\u666f\u7684\u5f3a\u9002\u5e94\u6027\u3002", "conclusion": "VModA\u662f\u4e00\u79cd\u901a\u7528\u4e14\u6709\u6548\u7684NSFW\u5185\u5bb9\u68c0\u6d4b\u6846\u67b6\uff0c\u5b9e\u9645\u5e94\u7528\u8868\u73b0\u4f18\u5f02\u3002", "keywords": "NSFW\u68c0\u6d4b,\u8bed\u4e49\u590d\u6742\u5185\u5bb9,\u591a\u6837\u5316\u5ba1\u6838\u89c4\u5219,VModA\u6846\u67b6"}}
{"id": "2505.23663", "pdf": "https://arxiv.org/pdf/2505.23663", "abs": "https://arxiv.org/abs/2505.23663", "authors": ["Niklas Freymuth", "Tobias W\u00fcrth", "Nicolas Schreiber", "Balazs Gyenes", "Andreas Boltres", "Johannes Mitsch", "Aleksandar Taranovic", "Tai Hoang", "Philipp Dahlinger", "Philipp Becker", "Luise K\u00e4rger", "Gerhard Neumann"], "title": "AMBER: Adaptive Mesh Generation by Iterative Mesh Resolution Prediction", "categories": ["cs.LG", "cs.CG"], "comment": null, "summary": "The cost and accuracy of simulating complex physical systems using the Finite\nElement Method (FEM) scales with the resolution of the underlying mesh.\nAdaptive meshes improve computational efficiency by refining resolution in\ncritical regions, but typically require task-specific heuristics or cumbersome\nmanual design by a human expert. We propose Adaptive Meshing By Expert\nReconstruction (AMBER), a supervised learning approach to mesh adaptation.\nStarting from a coarse mesh, AMBER iteratively predicts the sizing field, i.e.,\na function mapping from the geometry to the local element size of the target\nmesh, and uses this prediction to produce a new intermediate mesh using an\nout-of-the-box mesh generator. This process is enabled through a hierarchical\ngraph neural network, and relies on data augmentation by automatically\nprojecting expert labels onto AMBER-generated data during training. We evaluate\nAMBER on 2D and 3D datasets, including classical physics problems, mechanical\ncomponents, and real-world industrial designs with human expert meshes. AMBER\ngeneralizes to unseen geometries and consistently outperforms multiple recent\nbaselines, including ones using Graph and Convolutional Neural Networks, and\nReinforcement Learning-based approaches.", "AI": {"tldr": "AMBER\u662f\u4e00\u79cd\u901a\u8fc7\u4e13\u5bb6\u91cd\u5efa\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u6709\u9650\u5143\u65b9\u6cd5\u4e2d\u7684\u81ea\u9002\u5e94\u7f51\u683c\u751f\u6210\uff0c\u901a\u8fc7\u8fed\u4ee3\u9884\u6d4b\u5c3a\u5bf8\u573a\u5e76\u751f\u6210\u4e2d\u95f4\u7f51\u683c\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63d0\u9ad8\u6709\u9650\u5143\u65b9\u6cd5\u6a21\u62df\u4e2d\u81ea\u9002\u5e94\u7f51\u683c\u7684\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u5bf9\u4efb\u52a1\u7279\u5b9a\u542f\u53d1\u5f0f\u6216\u4e13\u5bb6\u624b\u52a8\u8bbe\u8ba1\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u56fe\u795e\u7ecf\u7f51\u7edc\u8fed\u4ee3\u9884\u6d4b\u5c3a\u5bf8\u573a\uff0c\u5e76\u5229\u7528\u73b0\u6210\u7684\u7f51\u683c\u751f\u6210\u5668\u751f\u6210\u4e2d\u95f4\u7f51\u683c\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u81ea\u52a8\u6295\u5f71\u4e13\u5bb6\u6807\u7b7e\u8fdb\u884c\u6570\u636e\u589e\u5f3a\u3002", "result": "\u57282D\u548c3D\u6570\u636e\u96c6\u4e0a\uff0cAMBER\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5305\u62ec\u56fe\u795e\u7ecf\u7f51\u7edc\u3001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "conclusion": "AMBER\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u5e76\u5728\u81ea\u9002\u5e94\u7f51\u683c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "keywords": "\u81ea\u9002\u5e94\u7f51\u683c,\u6709\u9650\u5143\u65b9\u6cd5,\u56fe\u795e\u7ecf\u7f51\u7edc,\u6570\u636e\u589e\u5f3a,\u4e13\u5bb6\u91cd\u5efa"}}
{"id": "2505.23387", "pdf": "https://arxiv.org/pdf/2505.23387", "abs": "https://arxiv.org/abs/2505.23387", "authors": ["Mingzhe Du", "Luu Tuan Tuan", "Yue Liu", "Yuhao Qing", "Dong Huang", "Xinyi He", "Qian Liu", "Zejun Ma", "See-kiong Ng"], "title": "Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) generate functionally correct solutions but\noften fall short in code efficiency, a critical bottleneck for real-world\ndeployment. In this paper, we introduce a novel test-time iterative\noptimization framework to address this, employing a closed-loop system where\nLLMs iteratively refine code based on empirical performance feedback from an\nexecution sandbox. We explore three training strategies: Supervised Fine-Tuning\n(SFT), Direct Preference Optimization (DPO), and Group Relative Policy\nOptimization~(GRPO). Experiments on our Venus dataset and the APPS benchmark\nshow that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO,\nusing reinforcement learning (RL) with execution feedback, continuously\noptimizes code performance, significantly boosting both pass@1 (from 47% to\n62%) and the likelihood of outperforming human submissions in efficiency (from\n31% to 45%). Our work demonstrates effective test-time code efficiency\nimprovement and critically reveals the power of RL in teaching LLMs to truly\nself-improve code efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4b\u8bd5\u65f6\u8fed\u4ee3\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u95ed\u73af\u7cfb\u7edf\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u57fa\u4e8e\u6267\u884c\u6c99\u7bb1\u7684\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\u4ee3\u7801\u6548\u7387\u3002\u4e09\u79cd\u8bad\u7ec3\u7b56\u7565\u4e2d\uff0cGRPO\uff08\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff09\u8868\u73b0\u6700\u4f73\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1LLMs\u80fd\u751f\u6210\u529f\u80fd\u6b63\u786e\u7684\u4ee3\u7801\uff0c\u4f46\u5176\u4ee3\u7801\u6548\u7387\u4e0d\u8db3\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u6d4b\u8bd5\u65f6\u4f18\u5316\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u95ed\u73af\u7cfb\u7edf\uff0cLLMs\u6839\u636e\u6267\u884c\u6c99\u7bb1\u7684\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\u4ee3\u7801\u3002\u6bd4\u8f83\u4e86\u4e09\u79cd\u8bad\u7ec3\u7b56\u7565\uff1aSFT\u3001DPO\u548cGRPO\uff08\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff09\u3002", "result": "GRPO\u663e\u8457\u63d0\u5347\u4e86pass@1\uff08\u4ece47%\u523062%\uff09\u548c\u6548\u7387\u8d85\u8d8a\u4eba\u7c7b\u63d0\u4ea4\u7684\u6982\u7387\uff08\u4ece31%\u523045%\uff09\uff0c\u8fdc\u4f18\u4e8eSFT\u548cDPO\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347LLMs\u4ee3\u7801\u6548\u7387\u65b9\u9762\u6548\u679c\u663e\u8457\uff0c\u9a8c\u8bc1\u4e86\u6d4b\u8bd5\u65f6\u4f18\u5316\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u4ee3\u7801\u6548\u7387\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u6d4b\u8bd5\u65f6\u4f18\u5316\u3001GRPO"}}
{"id": "2505.23673", "pdf": "https://arxiv.org/pdf/2505.23673", "abs": "https://arxiv.org/abs/2505.23673", "authors": ["Aya Kayal", "Sattar Vakili", "Laura Toni", "Da-shan Shiu", "Alberto Bernacchia"], "title": "Bayesian Optimization from Human Feedback: Near-Optimal Regret Bounds", "categories": ["cs.LG"], "comment": null, "summary": "Bayesian optimization (BO) with preference-based feedback has recently\ngarnered significant attention due to its emerging applications. We refer to\nthis problem as Bayesian Optimization from Human Feedback (BOHF), which differs\nfrom conventional BO by learning the best actions from a reduced feedback\nmodel, where only the preference between two actions is revealed to the learner\nat each time step. The objective is to identify the best action using a limited\nnumber of preference queries, typically obtained through costly human feedback.\nExisting work, which adopts the Bradley-Terry-Luce (BTL) feedback model,\nprovides regret bounds for the performance of several algorithms. In this work,\nwithin the same framework we develop tighter performance guarantees.\nSpecifically, we derive regret bounds of\n$\\tilde{\\mathcal{O}}(\\sqrt{\\Gamma(T)T})$, where $\\Gamma(T)$ represents the\nmaximum information gain$\\unicode{x2014}$a kernel-specific complexity\nterm$\\unicode{x2014}$and $T$ is the number of queries. Our results\nsignificantly improve upon existing bounds. Notably, for common kernels, we\nshow that the order-optimal sample complexities of conventional\nBO$\\unicode{x2014}$achieved with richer feedback models$\\unicode{x2014}$are\nrecovered. In other words, the same number of preferential samples as\nscalar-valued samples is sufficient to find a nearly optimal solution.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5728\u8d1d\u53f6\u65af\u4f18\u5316\uff08BO\uff09\u6846\u67b6\u4e0b\uff0c\u901a\u8fc7\u4eba\u7c7b\u504f\u597d\u53cd\u9988\uff08BOHF\uff09\u5b66\u4e60\u6700\u4f18\u884c\u52a8\u7684\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u5e03\u62c9\u5fb7\u5229-\u7279\u91cc-\u5362\u65af\uff08BTL\uff09\u6a21\u578b\u7684\u6027\u80fd\u754c\u9650\uff0c\u5c55\u793a\u4e86\u4e0e\u6807\u91cf\u53cd\u9988\u76f8\u4f3c\u7684\u6837\u672c\u6548\u7387\u3002", "motivation": "\u5728\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\uff0c\u4eba\u7c7b\u53cd\u9988\u6210\u672c\u9ad8\u4e14\u4fe1\u606f\u6709\u9650\uff08\u4ec5\u504f\u597d\u5bf9\u6bd4\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u754c\u9650\u4e0d\u591f\u7d27\u3002\u672c\u6587\u65e8\u5728\u63d0\u5347\u7406\u8bba\u4fdd\u8bc1\uff0c\u8bc1\u660e\u5728\u504f\u597d\u53cd\u9988\u4e0b\u4ecd\u53ef\u9ad8\u6548\u5b66\u4e60\u6700\u4f18\u884c\u52a8\u3002", "method": "\u91c7\u7528\u4e0e\u73b0\u6709\u5de5\u4f5c\u76f8\u540c\u7684\u6846\u67b6\uff08BTL\u6a21\u578b\uff09\uff0c\u901a\u8fc7\u5206\u6790\u4fe1\u606f\u589e\u76ca\u4e0e\u67e5\u8be2\u6b21\u6570\u7684\u5173\u7cfb\uff0c\u63a8\u5bfc\u51fa\u65b0\u7684\u9057\u61be\u754c\u9650$\tilde{\\mathcal{O}}(\\sqrt{\\Gamma(T)T})$\u3002", "result": "\u65b0\u754c\u9650\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7ed3\u679c\uff0c\u5e38\u89c1\u6838\u51fd\u6570\u4e0b\u4e0e\u6807\u91cf\u53cd\u9988\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e00\u81f4\uff0c\u8bc1\u660e\u504f\u597d\u53cd\u9988\u80fd\u8fbe\u5230\u4e0e\u4f20\u7edfBO\u76f8\u8fd1\u7684\u6548\u7387\u3002", "conclusion": "\u504f\u597d\u53cd\u9988\u867d\u4fe1\u606f\u6709\u9650\uff0c\u4f46\u5728\u7406\u8bba\u4fdd\u8bc1\u4e0b\u4e0e\u6807\u91cf\u53cd\u9988\u540c\u6837\u9ad8\u6548\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\uff08\u5982\u9ad8\u6210\u672c\u4eba\u7c7b\u53cd\u9988\u573a\u666f\uff09\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "keywords": "\u8d1d\u53f6\u65af\u4f18\u5316, \u4eba\u7c7b\u53cd\u9988, \u504f\u597d\u5b66\u4e60, \u5e03\u62c9\u5fb7\u5229-\u7279\u91cc-\u5362\u65af\u6a21\u578b, \u9057\u61be\u754c\u9650"}}
{"id": "2505.23406", "pdf": "https://arxiv.org/pdf/2505.23406", "abs": "https://arxiv.org/abs/2505.23406", "authors": ["Binyamin Manela", "Sharon Gannot", "Ethan Fetyaya"], "title": "Video Editing for Audio-Visual Dubbing", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Visual dubbing, the synchronization of facial movements with new speech, is\ncrucial for making content accessible across different languages, enabling\nbroader global reach. However, current methods face significant limitations.\nExisting approaches often generate talking faces, hindering seamless\nintegration into original scenes, or employ inpainting techniques that discard\nvital visual information like partial occlusions and lighting variations. This\nwork introduces EdiDub, a novel framework that reformulates visual dubbing as a\ncontent-aware editing task. EdiDub preserves the original video context by\nutilizing a specialized conditioning scheme to ensure faithful and accurate\nmodifications rather than mere copying. On multiple benchmarks, including a\nchallenging occluded-lip dataset, EdiDub significantly improves identity\npreservation and synchronization. Human evaluations further confirm its\nsuperiority, achieving higher synchronization and visual naturalness scores\ncompared to the leading methods. These results demonstrate that our\ncontent-aware editing approach outperforms traditional generation or\ninpainting, particularly in maintaining complex visual elements while ensuring\naccurate lip synchronization.", "AI": {"tldr": "EdiDub, \u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u5c06\u89c6\u89c9\u914d\u97f3\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5185\u5bb9\u611f\u77e5\u7f16\u8f91\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8eab\u4efd\u4fdd\u5b58\u548c\u540c\u6b65\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u914d\u97f3\u65b9\u6cd5\u5728\u4fdd\u6301\u539f\u59cb\u573a\u666f\u65e0\u7f1d\u96c6\u6210\u548c\u5904\u7406\u906e\u6321\u3001\u5149\u7167\u53d8\u5316\u7b49\u590d\u6742\u89c6\u89c9\u4fe1\u606f\u65f6\u5b58\u5728\u5c40\u9650\u3002", "method": "EdiDub \u901a\u8fc7\u5185\u5bb9\u611f\u77e5\u7f16\u8f91\u4efb\u52a1\u548c\u4e13\u7528\u6761\u4ef6\u65b9\u6848\uff0c\u4fdd\u7559\u539f\u59cb\u89c6\u9891\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u7cbe\u51c6\u4fee\u6539\u800c\u975e\u7b80\u5355\u590d\u5236\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5305\u62ec\u5177\u6709\u6311\u6218\u6027\u7684\u906e\u6321\u5634\u5507\u6570\u636e\u96c6\uff0cEdiDub \u663e\u8457\u63d0\u5347\u4e86\u8eab\u4efd\u4fdd\u5b58\u548c\u540c\u6b65\u6548\u679c\uff0c\u5e76\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u83b7\u5f97\u66f4\u9ad8\u7684\u540c\u6b65\u548c\u89c6\u89c9\u81ea\u7136\u5ea6\u8bc4\u5206\u3002", "conclusion": "\u5185\u5bb9\u611f\u77e5\u7f16\u8f91\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u7684\u751f\u6210\u6216\u4fee\u590d\u6280\u672f\uff0c\u7279\u522b\u662f\u5728\u4fdd\u6301\u590d\u6742\u89c6\u89c9\u5143\u7d20\u548c\u51c6\u786e\u5507\u540c\u6b65\u65b9\u9762\u3002", "keywords": "\u89c6\u89c9\u914d\u97f3, \u5185\u5bb9\u611f\u77e5\u7f16\u8f91, \u5507\u540c\u6b65, \u8eab\u4efd\u4fdd\u5b58, EdiDub"}}
{"id": "2505.23681", "pdf": "https://arxiv.org/pdf/2505.23681", "abs": "https://arxiv.org/abs/2505.23681", "authors": ["Bo Zhao", "Nima Dehmamy", "Robin Walters", "Rose Yu"], "title": "Understanding Mode Connectivity via Parameter Space Symmetry", "categories": ["cs.LG"], "comment": "20 pages, 4 figures, ICML 2025", "summary": "Neural network minima are often connected by curves along which train and\ntest loss remain nearly constant, a phenomenon known as mode connectivity.\nWhile this property has enabled applications such as model merging and\nfine-tuning, its theoretical explanation remains unclear. We propose a new\napproach to exploring the connectedness of minima using parameter space\nsymmetry. By linking the topology of symmetry groups to that of the minima, we\nderive the number of connected components of the minima of linear networks and\nshow that skip connections reduce this number. We then examine when mode\nconnectivity and linear mode connectivity hold or fail, using parameter\nsymmetries which account for a significant part of the minimum. Finally, we\nprovide explicit expressions for connecting curves in the minima induced by\nsymmetry. Using the curvature of these curves, we derive conditions under which\nlinear mode connectivity approximately holds. Our findings highlight the role\nof continuous symmetries in understanding the neural network loss landscape.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u7f51\u7edc\u6781\u5c0f\u503c\u4e4b\u95f4\u7684\u8fde\u901a\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u53c2\u6570\u7a7a\u95f4\u5bf9\u79f0\u6027\u63a2\u7d22\u6781\u5c0f\u503c\u8fde\u901a\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u8fde\u7eed\u5bf9\u79f0\u6027\u5728\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u635f\u5931\u666f\u89c2\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u6781\u5c0f\u503c\u901a\u8fc7\u66f2\u7ebf\u8fde\u63a5\u7684\u7269\u7406\u73b0\u8c61\u7684\u52a8\u673a\u5728\u4e8e\u901a\u8fc7\u53c2\u6570\u5bf9\u79f0\u6027\u63a2\u7d22\u6781\u5c0f\u503c\u4e4b\u95f4\u7684\u8fde\u901a\u7279\u6027\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u5c06\u5bf9\u79f0\u7fa4\u7684\u62d3\u6251\u7ed3\u6784\u8054\u7cfb\u5230\u6781\u5c0f\u503c\u7684\u62d3\u6251\u7ed3\u6784\u4e0a\u8fdb\u884c\u5206\u6790\uff0c\u5e76\u7814\u7a76\u4e86\u7ebf\u578b\u7f51\u7edc\u6781\u5c0f\u503c\u8fde\u901a\u7ec4\u4ef6\u7684\u6570\u91cf\u53ca\u8df3\u8dc3\u8fde\u63a5\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8df3\u8dc3\u8fde\u63a5\u80fd\u51cf\u5c11\u8fde\u901a\u7ec4\u4ef6\u7684\u6570\u91cf\uff0c\u5e76\u4e14\u5f53\u6a21\u5f0f\u8fde\u901a\u6027\u548c\u7ebf\u6027\u6a21\u5f0f\u8fde\u901a\u6027\u6210\u7acb\u6216\u5931\u8d25\u65f6\u5229\u7528\u53c2\u6570\u5bf9\u79f0\u6027\u8fdb\u884c\u89e3\u91ca\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8fde\u7eed\u5bf9\u79f0\u6027\u5bf9\u4e8e\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u635f\u5931\u666f\u89c2\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u7531\u5bf9\u79f0\u6027\u5bfc\u51fa\u7684\u8fde\u63a5\u66f2\u7ebf\u7684\u660e\u786e\u8868\u8fbe\u5f0f\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc, \u6781\u5c0f\u503c\u8fde\u901a\u6027, \u53c2\u6570\u5bf9\u79f0\u6027, \u635f\u5931\u666f\u89c2, \u8df3\u8dc3\u8fde\u63a5"}}
{"id": "2505.23683", "pdf": "https://arxiv.org/pdf/2505.23683", "abs": "https://arxiv.org/abs/2505.23683", "authors": ["Zixuan Wang", "Eshaan Nichani", "Alberto Bietti", "Alex Damian", "Daniel Hsu", "Jason D. Lee", "Denny Wu"], "title": "Learning Compositional Functions with Transformers from Easy-to-Hard Data", "categories": ["cs.LG"], "comment": "COLT 2025", "summary": "Transformer-based language models have demonstrated impressive capabilities\nacross a range of complex reasoning tasks. Prior theoretical work exploring the\nexpressive power of transformers has shown that they can efficiently perform\nmulti-step reasoning tasks involving parallelizable computations. However, the\nlearnability of such constructions, particularly the conditions on the data\ndistribution that enable efficient learning via gradient-based optimization,\nremains an open question. Towards answering this question, in this work we\nstudy the learnability of the $k$-fold composition task, which requires\ncomputing an interleaved composition of $k$ input permutations and $k$ hidden\npermutations, and can be expressed by a transformer with $O(\\log k)$ layers. On\nthe negative front, we prove a Statistical Query (SQ) lower bound showing that\nany SQ learner that makes only polynomially-many queries to an SQ oracle for\nthe $k$-fold composition task distribution must have sample size exponential in\n$k$, thus establishing a statistical-computational gap. On the other hand, we\nshow that this function class can be efficiently learned, with runtime and\nsample complexity polynomial in $k$, by gradient descent on an $O(\\log\nk)$-depth transformer via two different curriculum learning strategies: one in\nwhich data consists of $k'$-fold composition functions with $k' \\le k$\npresented in increasing difficulty, and another in which all such data is\npresented simultaneously. Our work sheds light on the necessity and sufficiency\nof having both easy and hard examples in the data distribution for transformers\nto learn complex compositional tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86Transformer\u5728\u590d\u6742\u7ec4\u5408\u4efb\u52a1\u4e2d\u7684\u53ef\u5b66\u4e60\u6027\uff0c\u5c24\u5176\u662fk-fold\u7ec4\u5408\u4efb\u52a1\u7684\u7edf\u8ba1-\u8ba1\u7b97\u5dee\u8ddd\u53ca\u5176\u9ad8\u6548\u5b66\u4e60\u6761\u4ef6\u3002", "motivation": "\u65e8\u5728\u7406\u89e3\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5\u5728\u4f55\u79cd\u6570\u636e\u5206\u5e03\u6761\u4ef6\u4e0b\u80fd\u9ad8\u6548\u5b66\u4e60\u590d\u6742\u7684\u7ec4\u5408\u4efb\u52a1\uff0c\u586b\u8865\u4e86\u7406\u8bba\u4e0a\u7684\u7a7a\u767d\u3002", "method": "\u5206\u6790\u4e86k-fold\u7ec4\u5408\u4efb\u52a1\u7684\u7edf\u8ba1\u67e5\u8be2\u4e0b\u754c\uff0c\u5e76\u901a\u8fc7\u4e24\u79cd\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff08\u9010\u6b65\u589e\u52a0\u96be\u5ea6\u548c\u540c\u65f6\u5448\u73b0\u6240\u6709\u6570\u636e\uff09\u6765\u8bad\u7ec3Transformer\u3002", "result": "\u8bc1\u660e\u4e86\u7edf\u8ba1\u67e5\u8be2\u5b66\u4e60\u5668\u9700\u8981\u6307\u6570\u7ea7\u6837\u672c\uff0c\u800c\u68af\u5ea6\u4e0b\u964d\u6cd5\u53ef\u5728\u591a\u9879\u5f0f\u7684\u8fd0\u884c\u65f6\u548c\u6837\u672c\u590d\u6742\u5ea6\u4e0b\u9ad8\u6548\u5b66\u4e60\u8be5\u4efb\u52a1\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6570\u636e\u5206\u5e03\u4e2d\u6613\u96be\u6837\u672c\u7684\u6df7\u5408\u5bf9Transformer\u5b66\u4e60\u590d\u6742\u7ec4\u5408\u4efb\u52a1\u7684\u5fc5\u8981\u6027\u548c\u5145\u5206\u6027\u3002", "keywords": "Transformer, \u53ef\u5b66\u4e60\u6027, \u7edf\u8ba1\u67e5\u8be2, \u68af\u5ea6\u4e0b\u964d, \u8bfe\u7a0b\u5b66\u4e60"}}
{"id": "2505.23696", "pdf": "https://arxiv.org/pdf/2505.23696", "abs": "https://arxiv.org/abs/2505.23696", "authors": ["Hiroshi Kera", "Nico Pelleriti", "Yuki Ishihara", "Max Zimmer", "Sebastian Pokutta"], "title": "Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms", "categories": ["cs.LG", "cs.SC"], "comment": "13+19 pages (3+9 figures, 2+7 tables)", "summary": "Solving systems of polynomial equations, particularly those with finitely\nmany solutions, is a crucial challenge across many scientific fields.\nTraditional methods like Gr\\\"obner and Border bases are fundamental but suffer\nfrom high computational costs, which have motivated recent Deep Learning\napproaches to improve efficiency, albeit at the expense of output correctness.\nIn this work, we introduce the Oracle Border Basis Algorithm, the first Deep\nLearning approach that accelerates Border basis computation while maintaining\noutput guarantees. To this end, we design and train a Transformer-based oracle\nthat identifies and eliminates computationally expensive reduction steps, which\nwe find to dominate the algorithm's runtime. By selectively invoking this\noracle during critical phases of computation, we achieve substantial speedup\nfactors of up to 3.5x compared to the base algorithm, without compromising the\ncorrectness of results. To generate the training data, we develop a sampling\nmethod and provide the first sampling theorem for border bases. We construct a\ntokenization and embedding scheme tailored to monomial-centered algebraic\ncomputations, resulting in a compact and expressive input representation, which\nreduces the number of tokens to encode an $n$-variate polynomial by a factor of\n$O(n)$. Our learning approach is data efficient, stable, and a practical\nenhancement to traditional computer algebra algorithms and symbolic\ncomputation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684Oracle Border Basis\u7b97\u6cd5\uff0c\u901a\u8fc7Transformer\u9884\u6d4b\u5e76\u8df3\u8fc7\u8ba1\u7b97\u6602\u8d35\u7684\u6b65\u9aa4\uff0c\u5c06\u901f\u5ea6\u63d0\u53473.5\u500d\u4e14\u4e0d\u727a\u7272\u6b63\u786e\u6027\uff0c\u540c\u65f6\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7684\u6570\u636e\u91c7\u6837\u4e0e\u5d4c\u5165\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u591a\u9879\u5f0f\u65b9\u7a0b\u7ec4\u6c42\u89e3\u65b9\u6cd5\uff08\u5982Gr\u00f6bner\u57fa\u548cBorder\u57fa\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u63d0\u901f\u5374\u65e0\u6cd5\u4fdd\u8bc1\u7ed3\u679c\u6b63\u786e\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u8ba1\u7b97\u52a0\u901f\u3002", "method": "\u91c7\u7528Transformer\u6a21\u578b\u4f5c\u4e3a\u9884\u6d4bOracle\uff0c\u5728Border\u57fa\u8ba1\u7b97\u4e2d\u52a8\u6001\u8df3\u8fc7\u5197\u4f59\u6b65\u9aa4\uff1b\u63d0\u51fa\u91c7\u6837\u5b9a\u7406\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u8bbe\u8ba1\u7d27\u51d1\u7684\u5355\u9879\u5f0f\u5d4c\u5165\u8868\u793a\u964d\u4f4e\u8f93\u5165\u590d\u6742\u5ea6\u3002", "result": "\u76f8\u6bd4\u57fa\u7840\u7b97\u6cd5\u901f\u5ea6\u63d0\u5347\u8fbe3.5\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u7ed3\u679c\u6b63\u786e\u6027\uff1b\u6570\u636e\u6548\u7387\u548c\u7a33\u5b9a\u6027\u4f18\u4e8e\u4f20\u7edf\u7b26\u53f7\u8ba1\u7b97\u65b9\u6cd5\u3002", "conclusion": "Oracle Border Basis\u7b97\u6cd5\u8bc1\u660e\u4e86\u6df1\u5ea6\u5b66\u4e60\u53ef\u65e0\u7f1d\u589e\u5f3a\u4f20\u7edf\u7b26\u53f7\u8ba1\u7b97\uff0c\u4e3a\u53ef\u9760\u9ad8\u6548\u7684\u4ee3\u6570\u6c42\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u591a\u9879\u5f0f\u65b9\u7a0b\u7ec4, Border\u57fa, \u6df1\u5ea6\u5b66\u4e60, Transformer, \u8ba1\u7b97\u52a0\u901f"}}
{"id": "2505.23417", "pdf": "https://arxiv.org/pdf/2505.23417", "abs": "https://arxiv.org/abs/2505.23417", "authors": ["Danilo Ribeiro", "Thayssa Rocha", "Gustavo Pinto", "Bruno Cartaxo", "Marcelo Amaral", "Nicole Davila", "Ana Camargo"], "title": "Toward Effective AI Governance: A Review of Principles", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Artificial Intelligence (AI) governance is the practice of establishing\nframeworks, policies, and procedures to ensure the responsible, ethical, and\nsafe development and deployment of AI systems. Although AI governance is a core\npillar of Responsible AI, current literature still lacks synthesis across such\ngovernance frameworks and practices. Objective: To identify which frameworks,\nprinciples, mechanisms, and stakeholder roles are emphasized in secondary\nliterature on AI governance. Method: We conducted a rapid tertiary review of\nnine peer-reviewed secondary studies from IEEE and ACM (20202024), using\nstructured inclusion criteria and thematic semantic synthesis. Results: The\nmost cited frameworks include the EU AI Act and NIST RMF; transparency and\naccountability are the most common principles. Few reviews detail actionable\ngovernance mechanisms or stakeholder strategies. Conclusion: The review\nconsolidates key directions in AI governance and highlights gaps in empirical\nvalidation and inclusivity. Findings inform both academic inquiry and practical\nadoption in organizations.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u5feb\u901f\u4e09\u7ea7\u7efc\u8ff0\u5206\u6790\u4e86\u4e5d\u7bc7\u5173\u4e8eAI\u6cbb\u7406\u7684\u4e8c\u6b21\u7814\u7a76\uff0c\u603b\u7ed3\u4e86\u4e3b\u8981\u6846\u67b6\u548c\u539f\u5219\uff0c\u5e76\u6307\u51fa\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u5173\u4e8eAI\u6cbb\u7406\u7684\u6587\u732e\u7f3a\u4e4f\u5bf9\u5404\u79cd\u6cbb\u7406\u6846\u67b6\u548c\u5b9e\u8df5\u7684\u7efc\u5408\u5206\u6790\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u5730\u68b3\u7406\u73b0\u6709\u7814\u7a76\u3002", "method": "\u91c7\u7528\u5feb\u901f\u4e09\u7ea7\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5bf92020\u81f32024\u5e74\u95f4IEEE\u548cACM\u7684\u4e5d\u7bc7\u540c\u884c\u8bc4\u5ba1\u4e8c\u6b21\u7814\u7a76\u8fdb\u884c\u4e86\u4e3b\u9898\u8bed\u4e49\u7efc\u5408\u5206\u6790\u3002", "result": "\u6700\u5e38\u88ab\u5f15\u7528\u7684\u6846\u67b6\u5305\u62ec\u6b27\u76dfAI\u6cd5\u6848\u548cNIST RMF\uff1b\u900f\u660e\u5ea6\u548c\u95ee\u8d23\u5236\u662f\u6700\u5e38\u89c1\u7684\u539f\u5219\uff0c\u4f46\u7814\u7a76\u4e2d\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u7684\u6cbb\u7406\u673a\u5236\u6216\u5229\u76ca\u76f8\u5173\u8005\u7b56\u7565\u3002", "conclusion": "\u7efc\u8ff0\u6574\u5408\u4e86AI\u6cbb\u7406\u7684\u5173\u952e\u65b9\u5411\uff0c\u5e76\u6307\u51fa\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\u548c\u5305\u5bb9\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u5b66\u672f\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "keywords": "AI\u6cbb\u7406, \u6846\u67b6, \u539f\u5219, \u900f\u660e\u5ea6, \u95ee\u8d23\u5236"}}
{"id": "2505.23700", "pdf": "https://arxiv.org/pdf/2505.23700", "abs": "https://arxiv.org/abs/2505.23700", "authors": ["Oleksii Furman", "Ulvi Movsum-zada", "Patryk Marszalek", "Maciej Zi\u0119ba", "Marek \u015amieja"], "title": "DiCoFlex: Model-agnostic diverse counterfactuals with flexible control", "categories": ["cs.LG"], "comment": null, "summary": "Counterfactual explanations play a pivotal role in explainable artificial\nintelligence (XAI) by offering intuitive, human-understandable alternatives\nthat elucidate machine learning model decisions. Despite their significance,\nexisting methods for generating counterfactuals often require constant access\nto the predictive model, involve computationally intensive optimization for\neach instance and lack the flexibility to adapt to new user-defined constraints\nwithout retraining. In this paper, we propose DiCoFlex, a novel model-agnostic,\nconditional generative framework that produces multiple diverse counterfactuals\nin a single forward pass. Leveraging conditional normalizing flows trained\nsolely on labeled data, DiCoFlex addresses key limitations by enabling\nreal-time user-driven customization of constraints such as sparsity and\nactionability at inference time. Extensive experiments on standard benchmark\ndatasets show that DiCoFlex outperforms existing methods in terms of validity,\ndiversity, proximity, and constraint adherence, making it a practical and\nscalable solution for counterfactual generation in sensitive decision-making\ndomains.", "AI": {"tldr": "DiCoFlex\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6761\u4ef6\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u751f\u6210\u591a\u6837\u5316\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6301\u7eed\u8bbf\u95ee\u6a21\u578b\u3001\u8ba1\u7b97\u5bc6\u96c6\u548c\u7f3a\u4e4f\u7075\u6d3b\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u9700\u8981\u6301\u7eed\u8bbf\u95ee\u6a21\u578b\u3001\u8ba1\u7b97\u5bc6\u96c6\u4e14\u65e0\u6cd5\u7075\u6d3b\u9002\u5e94\u7528\u6237\u5b9a\u4e49\u7684\u65b0\u7ea6\u675f\uff0cDiCoFlex\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528\u6761\u4ef6\u5f52\u4e00\u5316\u6d41\uff0c\u8bad\u7ec3\u4ec5\u57fa\u4e8e\u6807\u6ce8\u6570\u636e\uff0c\u63d0\u4f9b\u5b9e\u65f6\u7528\u6237\u9a71\u52a8\u7684\u7ea6\u675f\u5b9a\u5236\uff08\u5982\u7a00\u758f\u6027\u548c\u53ef\u64cd\u4f5c\u6027\uff09\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cDiCoFlex\u5728\u6709\u6548\u6027\u3001\u591a\u6837\u6027\u3001\u63a5\u8fd1\u6027\u548c\u7ea6\u675f\u9075\u5faa\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DiCoFlex\u4e3a\u654f\u611f\u51b3\u7b56\u9886\u57df\u7684\u53cd\u4e8b\u5b9e\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u53cd\u4e8b\u5b9e\u89e3\u91ca, \u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd, \u6761\u4ef6\u751f\u6210\u6a21\u578b, \u5f52\u4e00\u5316\u6d41"}}
{"id": "2505.23702", "pdf": "https://arxiv.org/pdf/2505.23702", "abs": "https://arxiv.org/abs/2505.23702", "authors": ["Nathan Lichtl\u00e9", "Alexi Canesse", "Zhe Fu", "Hossein Nick Zinat Matin", "Maria Laura Delle Monache", "Alexandre M. Bayen"], "title": "(U)NFV: Supervised and Unsupervised Neural Finite Volume Methods for Solving Hyperbolic PDEs", "categories": ["cs.LG", "cs.NA", "math.NA", "I.2.6; G.1.8"], "comment": null, "summary": "We introduce (U)NFV, a modular neural network architecture that generalizes\nclassical finite volume (FV) methods for solving hyperbolic conservation laws.\nHyperbolic partial differential equations (PDEs) are challenging to solve,\nparticularly conservation laws whose physically relevant solutions contain\nshocks and discontinuities. FV methods are widely used for their mathematical\nproperties: convergence to entropy solutions, flow conservation, or total\nvariation diminishing, but often lack accuracy and flexibility in complex\nsettings. Neural Finite Volume addresses these limitations by learning update\nrules over extended spatial and temporal stencils while preserving conservation\nstructure. It supports both supervised training on solution data (NFV) and\nunsupervised training via weak-form residual loss (UNFV). Applied to\nfirst-order conservation laws, (U)NFV achieves up to 10x lower error than\nGodunov's method, outperforms ENO/WENO, and rivals discontinuous Galerkin\nsolvers with far less complexity. On traffic modeling problems, both from PDEs\nand from experimental highway data, (U)NFV captures nonlinear wave dynamics\nwith significantly higher fidelity and scalability than traditional FV\napproaches.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a(U)NFV\u7684\u6a21\u5757\u5316\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u53cc\u66f2\u5b88\u6052\u5b9a\u5f8b\uff0c\u76f8\u6bd4\u4f20\u7edf\u6709\u9650\u4f53\u79ef\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7cbe\u5ea6\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u53cc\u66f2\u504f\u5fae\u5206\u65b9\u7a0b\uff08\u7279\u522b\u662f\u5b88\u6052\u5b9a\u5f8b\uff09\u7684\u6c42\u89e3\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u6709\u9650\u4f53\u79ef\u65b9\u6cd5\u867d\u7136\u6570\u5b66\u6027\u8d28\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u573a\u666f\u4e0b\u7cbe\u5ea6\u548c\u7075\u6d3b\u6027\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u65f6\u7a7a\u6a21\u677f\u5b66\u4e60\u66f4\u65b0\u89c4\u5219\uff0c\u540c\u65f6\u4fdd\u6301\u5b88\u6052\u7ed3\u6784\uff0c\u652f\u6301\u57fa\u4e8e\u89e3\u6570\u636e\u7684\u76d1\u7763\u8bad\u7ec3\uff08NFV\uff09\u548c\u5f31\u5f62\u5f0f\u6b8b\u5dee\u635f\u5931\u7684\u65e0\u76d1\u7763\u8bad\u7ec3\uff08UNFV\uff09\u3002", "result": "(U)NFV\u5728\u4e00\u9636\u5b88\u6052\u5b9a\u5f8b\u4e0a\u7684\u8bef\u5dee\u6bd4Godunov\u65b9\u6cd5\u4f4e10\u500d\uff0c\u4f18\u4e8eENO/WENO\uff0c\u4e0e\u95f4\u65adGalerkin\u6c42\u89e3\u5668\u76f8\u5f53\u4f46\u590d\u6742\u5ea6\u66f4\u4f4e\uff1b\u5728\u4ea4\u901a\u5efa\u6a21\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfFV\u65b9\u6cd5\u3002", "conclusion": "(U)NFV\u5728\u53cc\u66f2\u5b88\u6052\u5b9a\u5f8b\u6c42\u89e3\u4e2d\u5c55\u73b0\u51fa\u9ad8\u4fdd\u771f\u5ea6\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u590d\u6742\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc, \u6709\u9650\u4f53\u79ef\u65b9\u6cd5, \u53cc\u66f2\u5b88\u6052\u5b9a\u5f8b, \u5f31\u5f62\u5f0f\u6b8b\u5dee, \u4ea4\u901a\u5efa\u6a21"}}
{"id": "2505.23422", "pdf": "https://arxiv.org/pdf/2505.23422", "abs": "https://arxiv.org/abs/2505.23422", "authors": ["Tobias Lindenbauer", "Georg Groh", "Hinrich Sch\u00fctze"], "title": "From Knowledge to Noise: CTIM-Rover and the Pitfalls of Episodic Memory in Software Engineering Agents", "categories": ["cs.SE", "cs.AI"], "comment": "Short Paper, REALM '25 camera-ready", "summary": "We introduce CTIM-Rover, an AI agent for Software Engineering (SE) built on\ntop of AutoCodeRover (Zhang et al., 2024) that extends agentic reasoning\nframeworks with an episodic memory, more specifically, a general and\nrepository-level Cross-Task-Instance Memory (CTIM). While existing open-source\nSE agents mostly rely on ReAct (Yao et al., 2023b), Reflexion (Shinn et al.,\n2023), or Code-Act (Wang et al., 2024), all of these reasoning and planning\nframeworks inefficiently discard their long-term memory after a single task\ninstance. As repository-level understanding is pivotal for identifying all\nlocations requiring a patch for fixing a bug, we hypothesize that SE is\nparticularly well positioned to benefit from CTIM. For this, we build on the\nExperiential Learning (EL) approach ExpeL (Zhao et al., 2024), proposing a\nMixture-Of-Experts (MoEs) inspired approach to create both a general-purpose\nand repository-level CTIM. We find that CTIM-Rover does not outperform\nAutoCodeRover in any configuration and thus conclude that neither ExpeL nor\nDoT-Bank (Lingam et al., 2024) scale to real-world SE problems. Our analysis\nindicates noise introduced by distracting CTIM items or exemplar trajectories\nas the likely source of the performance degradation.", "AI": {"tldr": "CTIM-Rover\u662f\u4e00\u4e2a\u57fa\u4e8eAutoCodeRover\u7684AI\u4ee3\u7406\uff0c\u5f15\u5165\u4e86\u8de8\u4efb\u52a1\u5b9e\u4f8b\u8bb0\u5fc6\uff08CTIM\uff09\u4ee5\u63d0\u9ad8\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u957f\u671f\u8bb0\u5fc6\u80fd\u529b\uff0c\u4f46\u5b9e\u9645\u8868\u73b0\u672a\u8d85\u8d8aAutoCodeRover\uff0c\u53ef\u80fd\u56e0\u8bb0\u5fc6\u566a\u97f3\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u4f9d\u8d56\u7684\u63a8\u7406\u6846\u67b6\uff08\u5982ReAct\u3001Reflexion\u3001Code-Act\uff09\u5728\u4efb\u52a1\u5b8c\u6210\u540e\u4e22\u5f03\u957f\u671f\u8bb0\u5fc6\uff0c\u800c\u4ed3\u5e93\u7ea7\u7406\u89e3\u5bf9\u4fee\u590d\u6f0f\u6d1e\u5c24\u4e3a\u91cd\u8981\uff0c\u56e0\u6b64\u63a2\u7d22CTIM\u7684\u6f5c\u529b\u3002", "method": "\u57fa\u4e8eExpeL\u7684\u4f53\u9a8c\u5b66\u4e60\u65b9\u6cd5\uff0c\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoEs\uff09\u65b9\u6cd5\u6784\u5efa\u901a\u7528\u53ca\u4ed3\u5e93\u7ea7CTIM\uff0c\u6269\u5c55AutoCodeRover\u7684\u4ee3\u7406\u63a8\u7406\u6846\u67b6\u3002", "result": "CTIM-Rover\u5728\u6240\u6709\u914d\u7f6e\u4e2d\u5747\u672a\u8d85\u8d8aAutoCodeRover\uff0c\u6027\u80fd\u4e0b\u964d\u53ef\u80fd\u6e90\u4e8eCTIM\u9879\u76ee\u6216\u8f68\u8ff9\u793a\u4f8b\u5f15\u5165\u7684\u566a\u97f3\u3002", "conclusion": "ExpeL\u548cDoT-Bank\u672a\u80fd\u6709\u6548\u6269\u5c55\u81f3\u5b9e\u9645\u8f6f\u4ef6\u5de5\u7a0b\u95ee\u9898\uff0cCTIM\u7684\u5b9e\u7528\u6027\u53d7\u9650\u3002", "keywords": "CTIM-Rover, AutoCodeRover, \u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406, \u8de8\u4efb\u52a1\u5b9e\u4f8b\u8bb0\u5fc6, \u4f53\u9a8c\u5b66\u4e60"}}
{"id": "2505.23705", "pdf": "https://arxiv.org/pdf/2505.23705", "abs": "https://arxiv.org/abs/2505.23705", "authors": ["Danny Driess", "Jost Tobias Springenberg", "Brian Ichter", "Lili Yu", "Adrian Li-Bell", "Karl Pertsch", "Allen Z. Ren", "Homer Walke", "Quan Vuong", "Lucy Xiaoyang Shi", "Sergey Levine"], "title": "Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Vision-language-action (VLA) models provide a powerful approach to training\ncontrol policies for physical systems, such as robots, by combining end-to-end\nlearning with transfer of semantic knowledge from web-scale vision-language\nmodel (VLM) training. However, the constraints of real-time control are often\nat odds with the design of VLMs: the most powerful VLMs have tens or hundreds\nof billions of parameters, presenting an obstacle to real-time inference, and\noperate on discrete tokens rather than the continuous-valued outputs that are\nrequired for controlling robots. To address this challenge, recent VLA models\nhave used specialized modules for efficient continuous control, such as action\nexperts or continuous output heads, which typically require adding new\nuntrained parameters to the pretrained VLM backbone. While these modules\nimprove real-time and control capabilities, it remains an open question whether\nthey preserve or degrade the semantic knowledge contained in the pretrained\nVLM, and what effect they have on the VLA training dynamics. In this paper, we\nstudy this question in the context of VLAs that include a continuous diffusion\nor flow matching action expert, showing that naively including such experts\nsignificantly harms both training speed and knowledge transfer. We provide an\nextensive analysis of various design choices, their impact on performance and\nknowledge transfer, and propose a technique for insulating the VLM backbone\nduring VLA training that mitigates this issue. Videos are available at\nhttps://pi.website/research/knowledge_insulation.", "AI": {"tldr": "The paper explores how integrating continuous control modules in Vision-Language-Action (VLA) models affects semantic knowledge transfer and training dynamics, finding that naive implementations hinder performance and proposing a solution to mitigate this issue.", "motivation": "To address the conflict between powerful vision-language models (VLMs) with large parameter counts and the need for real-time continuous control in robotics, this paper investigates whether specialized control modules degrade the VLM's semantic knowledge and training efficiency.", "method": "The study evaluates VLAs incorporating continuous diffusion or flow matching action experts, analyzing design choices and their impacts. It introduces a technique to insulate the VLM backbone during training to preserve knowledge transfer.", "result": "Naively adding continuous control experts harms training speed and knowledge transfer, but the proposed insulation technique mitigates these negative effects.", "conclusion": "Preserving semantic knowledge in VLAs requires careful design of control modules, and the proposed insulation approach effectively maintains performance while enabling real-time control.", "keywords": "Vision-language-action models, continuous control, knowledge transfer, robotics, diffusion models"}}
{"id": "2505.23719", "pdf": "https://arxiv.org/pdf/2505.23719", "abs": "https://arxiv.org/abs/2505.23719", "authors": ["Andreas Auer", "Patrick Podest", "Daniel Klotz", "Sebastian B\u00f6ck", "G\u00fcnter Klambauer", "Sepp Hochreiter"], "title": "TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning", "categories": ["cs.LG"], "comment": null, "summary": "In-context learning, the ability of large language models to perform tasks\nusing only examples provided in the prompt, has recently been adapted for time\nseries forecasting. This paradigm enables zero-shot prediction, where past\nvalues serve as context for forecasting future values, making powerful\nforecasting tools accessible to non-experts and increasing the performance when\ntraining data are scarce. Most existing zero-shot forecasting approaches rely\non transformer architectures, which, despite their success in language, often\nfall short of expectations in time series forecasting, where recurrent models\nlike LSTMs frequently have the edge. Conversely, while LSTMs are well-suited\nfor time series modeling due to their state-tracking capabilities, they lack\nstrong in-context learning abilities. We introduce TiRex that closes this gap\nby leveraging xLSTM, an enhanced LSTM with competitive in-context learning\nskills. Unlike transformers, state-space models, or parallelizable RNNs such as\nRWKV, TiRex retains state-tracking, a critical property for long-horizon\nforecasting. To further facilitate its state-tracking ability, we propose a\ntraining-time masking strategy called CPM. TiRex sets a new state of the art in\nzero-shot time series forecasting on the HuggingFace benchmarks GiftEval and\nChronos-ZS, outperforming significantly larger models including TabPFN-TS\n(Prior Labs), Chronos Bolt (Amazon), TimesFM (Google), and Moirai (Salesforce)\nacross both short- and long-term forecasts.", "AI": {"tldr": "TiRex\u5229\u7528xLSTM\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u96f6\u6837\u672c\u9884\u6d4b\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u5927\u578b\u6a21\u578b\u3002", "motivation": "\u586b\u8865LSTM\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u7684\u7a7a\u767d\uff0c\u63d0\u5347\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u7ed3\u5408xLSTM\u548c\u6539\u8fdb\u7684\u8bad\u7ec3\u63a9\u7801\u7b56\u7565CPM\uff0c\u4fdd\u6301\u72b6\u6001\u8ddf\u8e2a\u80fd\u529b\u3002", "result": "\u5728HuggingFace\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u591a\u4e2a\u5927\u578b\u6a21\u578b\uff0c\u5305\u62ecGoogle\u548cSalesforce\u7684\u6a21\u578b\u3002", "conclusion": "TiRex\u5728\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5c24\u5176\u9002\u5408\u6570\u636e\u7a00\u7f3a\u573a\u666f\u3002", "keywords": "TiRex, xLSTM, \u96f6\u6837\u672c\u9884\u6d4b, \u65f6\u95f4\u5e8f\u5217, CPM"}}
{"id": "2505.23720", "pdf": "https://arxiv.org/pdf/2505.23720", "abs": "https://arxiv.org/abs/2505.23720", "authors": ["Arun Verma", "Indrajit Saha", "Makoto Yokoo", "Bryan Kian Hsiang Low"], "title": "COBRA: Contextual Bandit Algorithm for Ensuring Truthful Strategic Agents", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "This paper proposes a contextual bandit algorithm that prevents\n  strategic agents from misreporting while having approximate incentive\n  compatibility and a sub-linear regret guarantee", "summary": "This paper considers a contextual bandit problem involving multiple agents,\nwhere a learner sequentially observes the contexts and the agent's reported\narms, and then selects the arm that maximizes the system's overall reward.\nExisting work in contextual bandits assumes that agents truthfully report their\narms, which is unrealistic in many real-life applications. For instance,\nconsider an online platform with multiple sellers; some sellers may\nmisrepresent product quality to gain an advantage, such as having the platform\npreferentially recommend their products to online users. To address this\nchallenge, we propose an algorithm, COBRA, for contextual bandit problems\ninvolving strategic agents that disincentivize their strategic behavior without\nusing any monetary incentives, while having incentive compatibility and a\nsub-linear regret guarantee. Our experimental results also validate the\ndifferent performance aspects of our proposed algorithm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e0b\u7684\u4e0a\u4e0b\u6587\u591a\u81c2\u8d4c\u535a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCOBRA\u7684\u7b97\u6cd5\uff0c\u65e8\u5728\u65e0\u9700\u8d27\u5e01\u6fc0\u52b1\u7684\u60c5\u51b5\u4e0b\u6291\u5236\u667a\u80fd\u4f53\u7684\u7b56\u7565\u6027\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u8bc1\u6fc0\u52b1\u517c\u5bb9\u6027\u548c\u6b21\u7ebf\u6027\u9057\u61be\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u667a\u80fd\u4f53\u53ef\u80fd\u4e0d\u8bda\u5b9e\u62a5\u544a\u5176\u884c\u4e3a\uff08\u5982\u5728\u7ebf\u5e73\u53f0\u4e2d\u5356\u5bb6\u865a\u62a5\u4ea7\u54c1\u8d28\u91cf\uff09\uff0c\u73b0\u6709\u7814\u7a76\u5047\u8bbe\u667a\u80fd\u4f53\u8bda\u5b9e\u62a5\u544a\u7684\u5c40\u9650\u6027\u4fc3\u4f7f\u4f5c\u8005\u63d0\u51fa\u65b0\u7b97\u6cd5\u3002", "method": "\u63d0\u51faCOBRA\u7b97\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u673a\u5236\u5728\u4e0d\u4f7f\u7528\u8d27\u5e01\u6fc0\u52b1\u7684\u60c5\u51b5\u4e0b\uff0c\u6291\u5236\u667a\u80fd\u4f53\u7684\u7b56\u7565\u6027\u884c\u4e3a\uff0c\u786e\u4fdd\u6fc0\u52b1\u517c\u5bb9\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86COBRA\u7b97\u6cd5\u5728\u6291\u5236\u7b56\u7565\u6027\u884c\u4e3a\u548c\u5b9e\u73b0\u6b21\u7ebf\u6027\u9057\u61be\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "COBRA\u7b97\u6cd5\u4e3a\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u8d4c\u535a\u95ee\u9898\u4e2d\u7684\u7b56\u7565\u6027\u884c\u4e3a\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u4e0a\u4e0b\u6587\u591a\u81c2\u8d4c\u535a, \u591a\u667a\u80fd\u4f53, \u6fc0\u52b1\u517c\u5bb9\u6027, \u7b56\u7565\u6027\u884c\u4e3a, COBRA"}}
{"id": "2505.23444", "pdf": "https://arxiv.org/pdf/2505.23444", "abs": "https://arxiv.org/abs/2505.23444", "authors": ["Runmin Jiang", "Genpei Zhang", "Yuntian Yang", "Siqi Wu", "Yuheng Zhang", "Wanyue Feng", "Yizhou Zhao", "Xi Xiao", "Xiao Wang", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of\nmacromolecules, but developing robust models for downstream analysis is\nhindered by the scarcity of high-quality annotated data. While synthetic data\ngeneration has emerged as a potential solution, existing methods often fail to\ncapture both the structural diversity of biological specimens and the complex,\nspatially varying noise inherent in cryo-EM imaging. To overcome these\nlimitations, we propose CryoCCD, a synthesis framework that integrates\nbiophysical modeling with generative techniques. Specifically, CryoCCD produces\nmulti-scale cryo-EM micrographs that reflect realistic biophysical variability\nthrough compositional heterogeneity, cellular context, and physics-informed\nimaging. To generate realistic noise, we employ a conditional diffusion model,\nenhanced by cycle consistency to preserve structural fidelity and mask-aware\ncontrastive learning to capture spatially adaptive noise patterns. Extensive\nexperiments show that CryoCCD generates structurally accurate micrographs and\nenhances performance in downstream tasks, outperforming state-of-the-art\nbaselines in both particle picking and reconstruction.", "AI": {"tldr": "CryoCCD\u662f\u4e00\u4e2a\u7ed3\u5408\u751f\u7269\u7269\u7406\u5efa\u6a21\u4e0e\u751f\u6210\u6280\u672f\u7684\u5408\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u51b7\u51bb\u7535\u955c\u663e\u5fae\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u751f\u7269\u6807\u672c\u7ed3\u6784\u591a\u6837\u6027\u548c\u590d\u6742\u566a\u58f0\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u51b7\u51bb\u7535\u955c\uff08cryo-EM\uff09\u867d\u80fd\u5b9e\u73b0\u8fd1\u539f\u5b50\u5206\u8fa8\u7387\u6210\u50cf\uff0c\u4f46\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7684\u7a00\u7f3a\u963b\u788d\u4e86\u7a33\u5065\u6a21\u578b\u7684\u5f00\u53d1\u3002\u73b0\u6709\u5408\u6210\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u6a21\u62df\u751f\u7269\u6807\u672c\u7684\u7ed3\u6784\u591a\u6837\u6027\u548c\u590d\u6742\u566a\u58f0\u3002", "method": "\u63d0\u51faCryoCCD\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u7269\u7269\u7406\u5efa\u6a21\u751f\u6210\u591a\u5c3a\u5ea6\u51b7\u51bb\u7535\u955c\u56fe\u50cf\uff0c\u5e76\u91c7\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u6a21\u62df\u771f\u5b9e\u566a\u58f0\uff0c\u7ed3\u5408\u5faa\u73af\u4e00\u81f4\u6027\u548c\u63a9\u819c\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u566a\u58f0\u6a21\u5f0f\u9002\u5e94\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCryoCCD\u751f\u6210\u7684\u56fe\u50cf\u7ed3\u6784\u51c6\u786e\uff0c\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u7c92\u5b50\u62fe\u53d6\u548c\u91cd\u5efa\uff09\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CryoCCD\u663e\u8457\u63d0\u5347\u4e86\u51b7\u51bb\u7535\u955c\u56fe\u50cf\u5408\u6210\u7684\u771f\u5b9e\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u51b7\u51bb\u7535\u955c\u3001\u5408\u6210\u6570\u636e\u751f\u6210\u3001\u751f\u7269\u7269\u7406\u5efa\u6a21\u3001\u6761\u4ef6\u6269\u6563\u6a21\u578b\u3001\u566a\u58f0\u6a21\u62df"}}
{"id": "2505.23721", "pdf": "https://arxiv.org/pdf/2505.23721", "abs": "https://arxiv.org/abs/2505.23721", "authors": ["Sean Current", "Ziqi Chen", "Daniel Adu-Ampratwum", "Xia Ning", "Srinivasan Parthasarathy"], "title": "DiffER: Categorical Diffusion for Chemical Retrosynthesis", "categories": ["cs.LG"], "comment": "25 pages, 3 figures, 3 tables", "summary": "Methods for automatic chemical retrosynthesis have found recent success\nthrough the application of models traditionally built for natural language\nprocessing, primarily through transformer neural networks. These models have\ndemonstrated significant ability to translate between the SMILES encodings of\nchemical products and reactants, but are constrained as a result of their\nautoregressive nature. We propose DiffER, an alternative template-free method\nfor retrosynthesis prediction in the form of categorical diffusion, which\nallows the entire output SMILES sequence to be predicted in unison. We\nconstruct an ensemble of diffusion models which achieves state-of-the-art\nperformance for top-1 accuracy and competitive performance for top-3, top-5,\nand top-10 accuracy among template-free methods. We prove that DiffER is a\nstrong baseline for a new class of template-free model, capable of learning a\nvariety of synthetic techniques used in laboratory settings and outperforming a\nvariety of other template-free methods on top-k accuracy metrics. By\nconstructing an ensemble of categorical diffusion models with a novel length\nprediction component with variance, our method is able to approximately sample\nfrom the posterior distribution of reactants, producing results with strong\nmetrics of confidence and likelihood. Furthermore, our analyses demonstrate\nthat accurate prediction of the SMILES sequence length is key to further\nboosting the performance of categorical diffusion models.", "AI": {"tldr": "DiffER\u662f\u4e00\u79cd\u57fa\u4e8e\u5206\u7c7b\u6269\u6563\u7684\u65e0\u6a21\u677f\u5316\u5b66\u9006\u5408\u6210\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u6269\u6563\u6a21\u578b\u548c\u957f\u5ea6\u9884\u6d4b\u7ec4\u4ef6\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728\u5316\u5b66\u9006\u5408\u6210\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u81ea\u56de\u5f52\u6027\u8d28\u9650\u5236\u4e86\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDiffER\uff0c\u4e00\u79cd\u57fa\u4e8e\u5206\u7c7b\u6269\u6563\u7684\u65e0\u6a21\u677f\u65b9\u6cd5\uff0c\u96c6\u6210\u6269\u6563\u6a21\u578b\u5e76\u7ed3\u5408\u521b\u65b0\u7684\u957f\u5ea6\u9884\u6d4b\u7ec4\u4ef6\uff0c\u4ee5\u540c\u6b65\u9884\u6d4bSMILES\u5e8f\u5217\u3002", "result": "DiffER\u5728top-1\u51c6\u786e\u7387\u4e0a\u8fbe\u5230\u6700\u4f18\uff0c\u5728top-3\u3001top-5\u548ctop-10\u4e0a\u4e5f\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u5e76\u80fd\u5b66\u4e60\u591a\u79cd\u5b9e\u9a8c\u5ba4\u5408\u6210\u6280\u672f\u3002", "conclusion": "DiffER\u4f5c\u4e3a\u65e0\u6a21\u677f\u6a21\u578b\u7684\u65b0\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86SMILES\u5e8f\u5217\u957f\u5ea6\u9884\u6d4b\u5bf9\u63d0\u5347\u6027\u80fd\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u5c55\u793a\u4e86\u540e\u9a8c\u91c7\u6837\u80fd\u529b\u3002", "keywords": "\u5316\u5b66\u9006\u5408\u6210, \u6269\u6563\u6a21\u578b, \u65e0\u6a21\u677f\u65b9\u6cd5, SMILES, \u957f\u5ea6\u9884\u6d4b"}}
{"id": "2505.23454", "pdf": "https://arxiv.org/pdf/2505.23454", "abs": "https://arxiv.org/abs/2505.23454", "authors": ["Yanbin Wang", "Xingyu Chen", "Yumiao Wang", "Xiang Wang", "Chuanfei Zang", "Guolong Cui", "Jiahuan Liu"], "title": "LCB-CV-UNet: Enhanced Detector for High Dynamic Range Radar Signals", "categories": ["eess.SP", "cs.AI"], "comment": "5 pages, 4 figures. Accepted to IEEE IGARSS 2025", "summary": "We propose the LCB-CV-UNet to tackle performance degradation caused by High\nDynamic Range (HDR) radar signals. Initially, a hardware-efficient,\nplug-and-play module named Logarithmic Connect Block (LCB) is proposed as a\nphase coherence preserving solution to address the inherent challenges in\nhandling HDR features. Then, we propose the Dual Hybrid Dataset Construction\nmethod to generate a semi-synthetic dataset, approximating typical HDR signal\nscenarios with adjustable target distributions. Simulation results show about\n1% total detection probability improvement with under 0.9% computational\ncomplexity added compared with the baseline. Furthermore, it excels 5% over the\nbaseline at the range in 11-13 dB signal-to-noise ratio typical for urban\ntargets. Finally, the real experiment validates the practicality of our model.", "AI": {"tldr": "\u63d0\u51faLCB-CV-UNet\u89e3\u51b3HDR\u96f7\u8fbe\u4fe1\u53f7\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u7ed3\u5408Logarithmic Connect Block\u6a21\u5757\u548c\u53cc\u6df7\u5408\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\uff0c\u63d0\u5347\u68c0\u6d4b\u6982\u7387\u5e76\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u9ad8\u52a8\u6001\u8303\u56f4\uff08HDR\uff09\u96f7\u8fbe\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u5c24\u5176\u9488\u5bf9\u57ce\u5e02\u76ee\u6807\u7684\u5178\u578b\u4fe1\u566a\u6bd4\u8303\u56f4\u3002", "method": "\u5f15\u5165Logarithmic Connect Block\uff08LCB\uff09\u6a21\u5757\u4fdd\u6301\u76f8\u4f4d\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u51fa\u53cc\u6df7\u5408\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\u751f\u6210\u534a\u5408\u6210\u6570\u636e\u96c6\u3002", "result": "\u4eff\u771f\u663e\u793a\u68c0\u6d4b\u6982\u7387\u63d0\u5347\u7ea61%\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4ec5\u589e\u52a00.9%\uff0c\u4e14\u572811-13 dB\u4fe1\u566a\u6bd4\u8303\u56f4\u5185\u4f18\u4e8e\u57fa\u7ebf5%\u3002\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u5b9e\u7528\u6027\u3002", "conclusion": "LCB-CV-UNet\u6709\u6548\u89e3\u51b3\u4e86HDR\u4fe1\u53f7\u5904\u7406\u6311\u6218\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002", "keywords": "LCB-CV-UNet, HDR\u96f7\u8fbe\u4fe1\u53f7, Logarithmic Connect Block, \u53cc\u6df7\u5408\u6570\u636e\u96c6, \u68c0\u6d4b\u6982\u7387"}}
{"id": "2505.23724", "pdf": "https://arxiv.org/pdf/2505.23724", "abs": "https://arxiv.org/abs/2505.23724", "authors": ["Minrui Luo", "Fuhang Kuang", "Yu Wang", "Zirui Liu", "Tianxing He"], "title": "SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), are indispensable for efficiently customizing Large Language\nModels (LLMs). However, vanilla LoRA suffers from slow convergence speed and\nknowledge forgetting problems. Recent studies have leveraged the power of\ndesigned LoRA initialization, to enhance the fine-tuning efficiency, or to\npreserve knowledge in the pre-trained LLM. However, none of these works can\naddress the two cases at the same time. To this end, we introduce\nSubspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework\nengineered to navigate the trade-off between efficient fine-tuning and\nknowledge preservation. We achieve this by constraining the output of trainable\nLoRA adapters in a low-rank subspace, where the context information of\nfine-tuning data is most preserved while the context information of preserved\nknowledge is least retained, in a balanced way. Such constraint enables the\ntrainable weights to primarily focus on the main features of fine-tuning data\nwhile avoiding damaging the preserved knowledge features. We provide\ntheoretical analysis on our method, and conduct extensive experiments including\nsafety preservation and world knowledge preservation, on various downstream\ntasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning\nperformance while markedly diminishing knowledge forgetting, surpassing\ncontemporary LoRA initialization methods.", "AI": {"tldr": "SC-LoRA \u662f\u4e00\u79cd\u65b0\u578b\u7684 LoRA \u521d\u59cb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7ea6\u675f\u53ef\u8bad\u7ec3\u9002\u914d\u5668\u7684\u8f93\u51fa\u5728\u4f4e\u79e9\u5b50\u7a7a\u95f4\u4e2d\uff0c\u5e73\u8861\u4e86\u9ad8\u6548\u5fae\u8c03\u4e0e\u77e5\u8bc6\u4fdd\u7559\u7684\u6743\u8861\u3002", "motivation": "\u4f20\u7edf LoRA \u65b9\u6cd5\u5b58\u5728\u6536\u655b\u901f\u5ea6\u6162\u548c\u77e5\u8bc6\u9057\u5fd8\u95ee\u9898\uff0c\u73b0\u6709\u6539\u8fdb\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u89e3\u51b3\u9ad8\u6548\u5fae\u8c03\u548c\u77e5\u8bc6\u4fdd\u7559\uff0cSC-LoRA \u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7ea6\u675f LoRA \u9002\u914d\u5668\u8f93\u51fa\u5728\u7279\u5b9a\u4f4e\u79e9\u5b50\u7a7a\u95f4\uff0c\u4f18\u5316\u5fae\u8c03\u6570\u636e\u7684\u4e3b\u8981\u7279\u5f81\u800c\u4e0d\u635f\u5bb3\u4fdd\u7559\u7684\u77e5\u8bc6\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSC-LoRA \u5728\u5fae\u8c03\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709 LoRA \u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u77e5\u8bc6\u9057\u5fd8\u3002", "conclusion": "SC-LoRA \u9ad8\u6548\u5e73\u8861\u4e86\u5fae\u8c03\u6027\u80fd\u548c\u77e5\u8bc6\u4fdd\u7559\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b9a\u5236\u5316\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u53c2\u6570\u9ad8\u6548\u5fae\u8c03, LoRA, \u77e5\u8bc6\u4fdd\u7559, \u4f4e\u79e9\u5b50\u7a7a\u95f4, SC-LoRA"}}
{"id": "2505.23503", "pdf": "https://arxiv.org/pdf/2505.23503", "abs": "https://arxiv.org/abs/2505.23503", "authors": ["Shibbir Ahmed", "Shahnewaz Karim Sakib", "Anindya Bijoy Das"], "title": "Can Large Language Models Challenge CNNS in Medical Image Analysis?", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "This study presents a multimodal AI framework designed for precisely\nclassifying medical diagnostic images. Utilizing publicly available datasets,\nthe proposed system compares the strengths of convolutional neural networks\n(CNNs) and different large language models (LLMs). This in-depth comparative\nanalysis highlights key differences in diagnostic performance, execution\nefficiency, and environmental impacts. Model evaluation was based on accuracy,\nF1-score, average execution time, average energy consumption, and estimated\n$CO_2$ emission. The findings indicate that although CNN-based models can\noutperform various multimodal techniques that incorporate both images and\ncontextual information, applying additional filtering on top of LLMs can lead\nto substantial performance gains. These findings highlight the transformative\npotential of multimodal AI systems to enhance the reliability, efficiency, and\nscalability of medical diagnostics in clinical settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u591a\u6a21\u6001AI\u6846\u67b6\u7528\u4e8e\u7cbe\u786e\u5206\u7c7b\u533b\u5b66\u8bca\u65ad\u56fe\u50cf\uff0c\u6bd4\u8f83\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u989d\u5916\u8fc7\u6ee4\u53ef\u63d0\u5347LLM\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u591a\u6a21\u6001AI\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u6f5c\u529b\uff0c\u63d0\u5347\u8bca\u65ad\u7684\u53ef\u9760\u6027\u3001\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u6570\u636e\u96c6\uff0c\u6bd4\u8f83CNN\u548cLLM\u5728\u51c6\u786e\u6027\u3001F1\u5206\u6570\u3001\u6267\u884c\u65f6\u95f4\u3001\u80fd\u8017\u548cCO2\u6392\u653e\u4e0a\u7684\u5dee\u5f02\u3002", "result": "CNN\u5728\u67d0\u4e9b\u65b9\u9762\u4f18\u4e8e\u591a\u6a21\u6001\u6280\u672f\uff0c\u4f46LLM\u7ecf\u8fc7\u989d\u5916\u8fc7\u6ee4\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u591a\u6a21\u6001AI\u7cfb\u7edf\u5177\u6709\u53d8\u9769\u533b\u5b66\u8bca\u65ad\u7684\u6f5c\u529b\u3002", "keywords": "\u591a\u6a21\u6001AI, CNN, LLM, \u533b\u5b66\u8bca\u65ad, \u80fd\u6548"}}
{"id": "2505.23725", "pdf": "https://arxiv.org/pdf/2505.23725", "abs": "https://arxiv.org/abs/2505.23725", "authors": ["Benjamin Th\u00e9rien", "Xiaolong Huang", "Irina Rish", "Eugene Belilovsky"], "title": "MuLoCo: Muon is a practical inner optimizer for DiLoCo", "categories": ["cs.LG"], "comment": null, "summary": "DiLoCo is a powerful framework for training large language models (LLMs)\nunder networking constraints with advantages for increasing parallelism and\naccelerator utilization in data center settings. Despite significantly reducing\ncommunication frequency, however, DiLoCo's communication steps still involve\nall-reducing a complete copy of the model's parameters. While existing works\nhave explored ways to reduce communication in DiLoCo, the role of error\nfeedback accumulators and the effect of the inner-optimizer on compressibility\nremain under-explored. In this work, we investigate the effectiveness of\nstandard compression methods including Top-k sparsification and quantization\nfor reducing the communication overhead of DiLoCo when paired with two local\noptimizers (AdamW and Muon). Our experiments pre-training decoder-only\ntransformer language models (LMs) reveal that leveraging Muon as the inner\noptimizer for DiLoCo along with an error-feedback accumulator allows to\naggressively compress the communicated delta to 2-bits with next to no\nperformance degradation. Crucially, MuLoCo (Muon inner optimizer DiLoCo)\nsignificantly outperforms DiLoCo while communicating 8X less and having\nidentical memory complexity.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u63d0\u5230DiLoCo\u662f\u4e00\u79cd\u5728\u7f51\u7edc\u7ea6\u675f\u4e0b\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5f3a\u5927\u6846\u67b6\uff0c\u4f46\u4ecd\u6709\u901a\u4fe1\u6548\u7387\u95ee\u9898\u3002\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u538b\u7f29\u65b9\u6cd5\uff08\u5982Top-k\u7a00\u758f\u5316\u548c\u91cf\u5316\uff09\u51cf\u5c11DiLoCo\u901a\u4fe1\u5f00\u9500\uff0c\u7ed3\u5408Muon\u4f18\u5316\u5668\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "DiLoCo\u5728\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u901a\u4fe1\u6b65\u9aa4\u4ecd\u5b58\u5728\u6548\u7387\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5bf9\u8bef\u5dee\u53cd\u9988\u7d2f\u52a0\u5668\u548c\u4f18\u5316\u5668\u5bf9\u538b\u7f29\u6027\u7684\u5f71\u54cd\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u7814\u7a76\u91c7\u7528\u6807\u51c6\u538b\u7f29\u65b9\u6cd5\uff08Top-k\u7a00\u758f\u5316\u548c\u91cf\u5316\uff09\u7ed3\u5408\u4e24\u79cd\u672c\u5730\u4f18\u5316\u5668\uff08AdamW\u548cMuon\uff09\uff0c\u5e76\u901a\u8fc7\u8bef\u5dee\u53cd\u9988\u7d2f\u52a0\u5668\u8bc4\u4f30\u901a\u4fe1\u538b\u7f29\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cMuon\u4f18\u5316\u5668\u7ed3\u5408\u8bef\u5dee\u53cd\u9988\u7d2f\u52a0\u5668\u53ef\u5c06\u901a\u4fe1\u6570\u636e\u538b\u7f29\u81f32\u4f4d\uff0c\u6027\u80fd\u51e0\u4e4e\u65e0\u635f\uff0c\u4e14MuLoCo\uff08Muon\u4f18\u5316\u5668\u7248\u7684DiLoCo\uff09\u5728\u901a\u4fe1\u91cf\u51cf\u5c118\u500d\u7684\u540c\u65f6\u663e\u8457\u4f18\u4e8e\u539f\u7248\uff0c\u5185\u5b58\u590d\u6742\u5ea6\u76f8\u540c\u3002", "conclusion": "DiLoCo\u7ed3\u5408Muon\u4f18\u5316\u5668\u548c\u538b\u7f29\u6280\u672f\u53ef\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u4e3a\u7f51\u7edc\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5927\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "DiLoCo, LLM, \u901a\u4fe1\u6548\u7387, Top-k\u7a00\u758f\u5316, \u91cf\u5316, Muon\u4f18\u5316\u5668, \u8bef\u5dee\u53cd\u9988\u7d2f\u52a0\u5668"}}
{"id": "2505.23508", "pdf": "https://arxiv.org/pdf/2505.23508", "abs": "https://arxiv.org/abs/2505.23508", "authors": ["Rebecca Ramnauth", "Dra\u017een Br\u0161\u010di\u0107", "Brian Scassellati"], "title": "A Robot-Assisted Approach to Small Talk Training for Adults with ASD", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted for publication in Robotics: Science and Systems (RSS) 2025,\n  14 pages, 4 figures,", "summary": "From dating to job interviews, making new friends or simply chatting with the\ncashier at checkout, engaging in small talk is a vital, everyday social skill.\nFor adults with Autism Spectrum Disorder (ASD), small talk can be particularly\nchallenging, yet it is essential for social integration, building\nrelationships, and accessing professional opportunities. In this study, we\npresent our development and evaluation of an in-home autonomous robot system\nthat allows users to practice small talk. Results from the week-long study show\nthat adults with ASD enjoyed the training, made notable progress in initiating\nconversations and improving eye contact, and viewed the system as a valuable\ntool for enhancing their conversational skills.", "AI": {"tldr": "\u7814\u53d1\u4e86\u4e00\u6b3e\u5bb6\u7528\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5e2e\u52a9\u81ea\u95ed\u75c7\u8c31\u7cfb\u969c\u788d\uff08ASD\uff09\u6210\u5e74\u4eba\u7ec3\u4e60\u95f2\u804a\uff0c\u7ed3\u679c\u663e\u793a\u53c2\u4e0e\u8005\u4eab\u53d7\u8bad\u7ec3\u5e76\u5728\u793e\u4ea4\u6280\u80fd\u4e0a\u6709\u6240\u8fdb\u6b65\u3002", "motivation": "\u95f2\u804a\u5bf9ASD\u6210\u5e74\u4eba\u800c\u8a00\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u5bf9\u793e\u4ea4\u878d\u5165\u3001\u5efa\u7acb\u5173\u7cfb\u548c\u804c\u4e1a\u673a\u4f1a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u548c\u8bc4\u4f30\u4e86\u4e00\u6b3e\u5bb6\u7528\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8eASD\u6210\u5e74\u4eba\u7ec3\u4e60\u95f2\u804a\u3002", "result": "\u53c2\u4e0e\u8005\u5728\u4e3a\u671f\u4e00\u5468\u7684\u8bad\u7ec3\u4e2d\u4eab\u53d7\u8fc7\u7a0b\uff0c\u5728\u53d1\u8d77\u5bf9\u8bdd\u548c\u773c\u795e\u4ea4\u6d41\u65b9\u9762\u6709\u663e\u8457\u8fdb\u6b65\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u88ab\u89c6\u4e3a\u63d0\u5347ASD\u6210\u5e74\u4eba\u4ea4\u6d41\u6280\u80fd\u7684\u6709\u6548\u5de5\u5177\u3002", "keywords": "\u81ea\u95ed\u75c7\u8c31\u7cfb\u969c\u788d\uff0c\u95f2\u804a\uff0c\u81ea\u4e3b\u673a\u5668\u4eba\uff0c\u793e\u4ea4\u6280\u80fd\uff0c\u5bb6\u7528\u7cfb\u7edf"}}
{"id": "2505.23732", "pdf": "https://arxiv.org/pdf/2505.23732", "abs": "https://arxiv.org/abs/2505.23732", "authors": ["Shreeram Suresh Chandra", "Lucas Goncalves", "Junchen Lu", "Carlos Busso", "Berrak Sisman"], "title": "EmotionRankCLAP: Bridging Natural Language Speaking Styles and Ordinal Speech Emotion via Rank-N-Contrast", "categories": ["cs.LG"], "comment": "Accepted at Interspeech 2025", "summary": "Current emotion-based contrastive language-audio pretraining (CLAP) methods\ntypically learn by na\\\"ively aligning audio samples with corresponding text\nprompts. Consequently, this approach fails to capture the ordinal nature of\nemotions, hindering inter-emotion understanding and often resulting in a wide\nmodality gap between the audio and text embeddings due to insufficient\nalignment. To handle these drawbacks, we introduce EmotionRankCLAP, a\nsupervised contrastive learning approach that uses dimensional attributes of\nemotional speech and natural language prompts to jointly capture fine-grained\nemotion variations and improve cross-modal alignment. Our approach utilizes a\nRank-N-Contrast objective to learn ordered relationships by contrasting samples\nbased on their rankings in the valence-arousal space. EmotionRankCLAP\noutperforms existing emotion-CLAP methods in modeling emotion ordinality across\nmodalities, measured via a cross-modal retrieval task.", "AI": {"tldr": "\u5f53\u524d\u57fa\u4e8e\u60c5\u611f\u7684CLAP\u65b9\u6cd5\u901a\u8fc7\u7b80\u5355\u5bf9\u9f50\u97f3\u9891\u4e0e\u6587\u672c\u63d0\u793a\u5b66\u4e60\uff0c\u5ffd\u7565\u4e86\u60c5\u611f\u7684\u5e8f\u6570\u6027\u3002EmotionRankCLAP\u5229\u7528\u60c5\u611f\u7ef4\u5ea6\u5c5e\u6027\u6539\u8fdb\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u6355\u83b7\u7ec6\u7c92\u5ea6\u60c5\u611f\u53d8\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u60c5\u611f\u7684\u5e8f\u6570\u6027\uff0c\u5bfc\u81f4\u6a21\u6001\u95f4\u5dee\u8ddd\u5927\u3002", "method": "\u63d0\u51faEmotionRankCLAP\uff0c\u7ed3\u5408\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u548cRank-N-Contrast\u76ee\u6807\uff0c\u5229\u7528\u60c5\u611f\u7ef4\u5ea6\u5c5e\u6027\u6539\u8fdb\u5bf9\u9f50\u3002", "result": "\u5728\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0cEmotionRankCLAP\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EmotionRankCLAP\u80fd\u6709\u6548\u5efa\u6a21\u60c5\u611f\u5e8f\u6570\u6027\u5e76\u63d0\u5347\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "keywords": "\u60c5\u611fCLAP\uff0c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0cRank-N-Contrast\uff0c\u60c5\u611f\u5e8f\u6570\u6027"}}
{"id": "2505.23749", "pdf": "https://arxiv.org/pdf/2505.23749", "abs": "https://arxiv.org/abs/2505.23749", "authors": ["Paul G\u00f6lz", "Nika Haghtalab", "Kunhe Yang"], "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "categories": ["cs.LG", "cs.GT"], "comment": null, "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u793e\u4f1a\u9009\u62e9\u7406\u8bba\u548c\u4e2a\u4f53Bradley-Terry\u6a21\u578b\u8bc4\u4f30\u7528\u6237\u591a\u6837\u6027\u504f\u597d\uff0c\u5bf9\u6bd4\u73b0\u6709\u65b9\u6cd5\uff08\u5982RLHF\u548cDPO\uff09\u5728\u6548\u7528\u635f\u5931\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u5bf9\u9f50\u65b9\u6cd5\u5047\u8bbe\u5355\u4e00\u504f\u597d\u6a21\u578b\uff0c\u4f46\u5b9e\u9645\u7528\u6237\u504f\u597d\u591a\u6837\uff0c\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u6ee1\u8db3\u7528\u6237\u5e73\u5747\u9700\u6c42\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u9002\u5e94\u591a\u5143\u504f\u597d\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u793e\u4f1a\u9009\u62e9\u7406\u8bba\u548c\u4e2a\u4f53Bradley-Terry\u6a21\u578b\uff0c\u5b9a\u4e49\u5bf9\u9f50\u65b9\u6cd5\u7684\u201c\u5931\u771f\u201d\u6982\u5ff5\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u65b9\u6cd5\u5728\u591a\u5143\u504f\u597d\u4e0b\u7684\u6548\u7528\u635f\u5931\u3002", "result": "Nash Learning from Human Feedback\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f18\uff0c\u5931\u771f\u7387\u4e3a$(\\frac{1}{2} + o(1)) \\cdot \\beta$\uff0c\u800cRLHF\u548cDPO\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5931\u771f\u7387\u663e\u8457\u66f4\u9ad8\u3002", "conclusion": "\u65b0\u7684\u5bf9\u9f50\u65b9\u6cd5\u5728\u591a\u5143\u504f\u597d\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u4e3a\u5bf9\u9f50\u6280\u672f\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u4f9d\u636e\u3002", "keywords": "\u5bf9\u9f50\u65b9\u6cd5, \u793e\u4f1a\u9009\u62e9\u7406\u8bba, Bradley-Terry\u6a21\u578b, \u5931\u771f, \u591a\u5143\u504f\u597d"}}
{"id": "2505.23751", "pdf": "https://arxiv.org/pdf/2505.23751", "abs": "https://arxiv.org/abs/2505.23751", "authors": ["Declan Kutscher", "David M. Chan", "Yutong Bai", "Trevor Darrell", "Ritwik Gupta"], "title": "REOrdering Patches Improves Vision Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Sequence models such as transformers require inputs to be represented as\none-dimensional sequences. In vision, this typically involves flattening images\nusing a fixed row-major (raster-scan) order. While full self-attention is\npermutation-equivariant, modern long-sequence transformers increasingly rely on\narchitectural approximations that break this invariance and introduce\nsensitivity to patch ordering. We show that patch order significantly affects\nmodel performance in such settings, with simple alternatives like column-major\nor Hilbert curves yielding notable accuracy shifts. Motivated by this, we\npropose REOrder, a two-stage framework for discovering task-optimal patch\norderings. First, we derive an information-theoretic prior by evaluating the\ncompressibility of various patch sequences. Then, we learn a policy over\npermutations by optimizing a Plackett-Luce policy using REINFORCE. This\napproach enables efficient learning in a combinatorial permutation space.\nREOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to\n3.01% and Functional Map of the World by 13.35%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aREOrder\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u56fe\u50cf\u8865\u4e01\u7684\u987a\u5e8f\u6765\u63d0\u5347\u5e8f\u5217\u6a21\u578b\uff08\u5982\u53d8\u6362\u5668\uff09\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5e8f\u5217\u6a21\u578b\u5728\u5904\u7406\u56fe\u50cf\u65f6\u901a\u5e38\u91c7\u7528\u56fa\u5b9a\u7684\u884c\u4f18\u5148\u987a\u5e8f\uff0c\u800c\u4e0d\u540c\u7684\u8865\u4e01\u987a\u5e8f\u53ef\u80fd\u663e\u8457\u5f71\u54cd\u6027\u80fd\u3002\u4f5c\u8005\u65e8\u5728\u63a2\u7d22\u4efb\u52a1\u6700\u4f18\u7684\u8865\u4e01\u6392\u5e8f\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u9996\u5148\u57fa\u4e8e\u4fe1\u606f\u8bba\u8bc4\u4f30\u8865\u4e01\u5e8f\u5217\u7684\u53ef\u538b\u7f29\u6027\uff1b\u7136\u540e\u901a\u8fc7REINFORCE\u4f18\u5316Plackett-Luce\u7b56\u7565\uff0c\u5b66\u4e60\u8865\u4e01\u6392\u5217\u7b56\u7565\u3002", "result": "\u5728ImageNet-1K\u4e0a\uff0cREOrder\u6bd4\u884c\u4f18\u5148\u6392\u5e8f\u7684Top-1\u51c6\u786e\u7387\u63d0\u9ad83.01%\uff0c\u5728Functional Map of the World\u4e0a\u63d0\u534713.35%\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u8865\u4e01\u987a\u5e8f\uff0cREOrder\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u8865\u4e01\u987a\u5e8f\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\u3002", "keywords": "\u5e8f\u5217\u6a21\u578b, \u8865\u4e01\u987a\u5e8f, \u53d8\u6362\u5668, \u4fe1\u606f\u8bba, REINFORCE"}}
{"id": "2505.23554", "pdf": "https://arxiv.org/pdf/2505.23554", "abs": "https://arxiv.org/abs/2505.23554", "authors": ["Hayden Moore", "Sirui Qi", "Ninad Hogade", "Dejan Milojicic", "Cullen Bash", "Sudeep Pasricha"], "title": "Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in Geo-Distributed Cloud Datacenters", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": null, "summary": "In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and\nGemini have been widely adopted in different areas. As the use of LLMs\ncontinues to grow, many efforts have focused on reducing the massive training\noverheads of these models. But it is the environmental impact of handling user\nrequests to LLMs that is increasingly becoming a concern. Recent studies\nestimate that the costs of operating LLMs in their inference phase can exceed\ntraining costs by 25x per year. As LLMs are queried incessantly, the cumulative\ncarbon footprint for the operational phase has been shown to far exceed the\nfootprint during the training phase. Further, estimates indicate that 500 ml of\nfresh water is expended for every 20-50 requests to LLMs during inference. To\naddress these important sustainability issues with LLMs, we propose a novel\nframework called SLIT to co-optimize LLM quality of service (time-to-first\ntoken), carbon emissions, water usage, and energy costs. The framework utilizes\na machine learning (ML) based metaheuristic to enhance the sustainability of\nLLM hosting across geo-distributed cloud datacenters. Such a framework will\nbecome increasingly vital as LLMs proliferate.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSLIT\u7684\u65b0\u578b\u6846\u67b6\uff0c\u65e8\u5728\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u670d\u52a1\u8d28\u91cf\u3001\u78b3\u6392\u653e\u3001\u6c34\u8d44\u6e90\u6d88\u8017\u548c\u80fd\u6e90\u6210\u672c\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5\u63d0\u5347LLM\u6258\u7ba1\u7684\u5730\u7406\u5206\u5e03\u5f0f\u4e91\u6570\u636e\u4e2d\u5fc3\u7684\u53ef\u6301\u7eed\u6027\u3002", "motivation": "\u968f\u7740LLM\uff08\u5982ChatGPT\u3001CoPilot\u548cGemini\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u63a8\u7406\u9636\u6bb5\u7684\u6210\u672c\uff08\u5305\u62ec\u78b3\u8db3\u8ff9\u548c\u6c34\u8d44\u6e90\u6d88\u8017\uff09\u5df2\u8fdc\u8d85\u8bad\u7ec3\u9636\u6bb5\uff0c\u5f15\u53d1\u4e86\u5bf9\u73af\u5883\u53ef\u6301\u7eed\u6027\u7684\u62c5\u5fe7\u3002", "method": "\u63d0\u51faSLIT\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5728\u5730\u7406\u5206\u5e03\u5f0f\u4e91\u6570\u636e\u4e2d\u5fc3\u4e2d\u4f18\u5316LLM\u7684\u670d\u52a1\u8d28\u91cf\u3001\u78b3\u6392\u653e\u3001\u6c34\u6d88\u8017\u548c\u80fd\u6e90\u6210\u672c\u3002", "result": "SLIT\u6846\u67b6\u80fd\u591f\u6709\u6548\u51cf\u5c11LLM\u63a8\u7406\u9636\u6bb5\u7684\u78b3\u8db3\u8ff9\u548c\u6c34\u8d44\u6e90\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u670d\u52a1\u8d28\u91cf\u3002", "conclusion": "\u968f\u7740LLM\u7684\u666e\u53ca\uff0cSLIT\u8fd9\u7c7b\u53ef\u6301\u7eed\u6027\u4f18\u5316\u6846\u67b6\u5c06\u53d8\u5f97\u6108\u53d1\u91cd\u8981\uff0c\u5176\u65b9\u6cd5\u4e3a\u51cf\u5c11LLM\u7684\u73af\u5883\u5f71\u54cd\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u53ef\u6301\u7eed\u6027\uff0c\u78b3\u8db3\u8ff9\uff0c\u6c34\u8d44\u6e90\u6d88\u8017\uff0c\u673a\u5668\u5b66\u4e60\u5143\u542f\u53d1\u5f0f"}}
{"id": "2505.23760", "pdf": "https://arxiv.org/pdf/2505.23760", "abs": "https://arxiv.org/abs/2505.23760", "authors": ["Amber Yijia Zheng", "Cedar Site Bai", "Brian Bullins", "Raymond A. Yeh"], "title": "Model Immunization from a Condition Number Perspective", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Model immunization aims to pre-train models that are difficult to fine-tune\non harmful tasks while retaining their utility on other non-harmful tasks.\nThough prior work has shown empirical evidence for immunizing text-to-image\nmodels, the key understanding of when immunization is possible and a precise\ndefinition of an immunized model remain unclear. In this work, we propose a\nframework, based on the condition number of a Hessian matrix, to analyze model\nimmunization for linear models. Building on this framework, we design an\nalgorithm with regularization terms to control the resulting condition numbers\nafter pre-training. Empirical results on linear models and non-linear deep-nets\ndemonstrate the effectiveness of the proposed algorithm on model immunization.\nThe code is available at\nhttps://github.com/amberyzheng/model-immunization-cond-num.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHessian\u77e9\u9635\u6761\u4ef6\u6570\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u7ebf\u6027\u6a21\u578b\u7684\u514d\u75ab\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u901a\u8fc7\u6b63\u5219\u5316\u63a7\u5236\u6761\u4ef6\u6570\u7684\u7b97\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u6a21\u578b\u514d\u75ab\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u5bf9\u6a21\u578b\u514d\u75ab\u7684\u5b9a\u4e49\u548c\u53ef\u80fd\u6027\u7f3a\u4e4f\u6e05\u6670\u7406\u89e3\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u7b97\u6cd5\u3002", "method": "\u57fa\u4e8eHessian\u77e9\u9635\u6761\u4ef6\u6570\u7684\u7406\u8bba\u5206\u6790\uff0c\u8bbe\u8ba1\u4e86\u901a\u8fc7\u6b63\u5219\u5316\u63a7\u5236\u6761\u4ef6\u6570\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u7ebf\u6027\u6a21\u578b\u548c\u6df1\u5ea6\u975e\u7ebf\u6027\u7f51\u7edc\u4e2d\u9a8c\u8bc1\u3002", "result": "\u7b97\u6cd5\u5728\u7ebf\u6027\u6a21\u578b\u548c\u975e\u7ebf\u6027\u6df1\u5ea6\u7f51\u7edc\u4e2d\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u6a21\u578b\u514d\u75ab\u7684\u76ee\u6807\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u7b97\u6cd5\u4e3a\u6a21\u578b\u514d\u75ab\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "keywords": "\u6a21\u578b\u514d\u75ab\uff0cHessian\u77e9\u9635\uff0c\u6761\u4ef6\u6570\uff0c\u6b63\u5219\u5316\uff0c\u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2505.23576", "pdf": "https://arxiv.org/pdf/2505.23576", "abs": "https://arxiv.org/abs/2505.23576", "authors": ["Jane Cleland-Huang", "Pedro Antonio Alarcon Granadeno", "Arturo Miguel Russell Bernal", "Demetrius Hernandez", "Michael Murphy", "Maureen Petterson", "Walter Scheirer"], "title": "Cognitive Guardrails for Open-World Decision Making in Autonomous Drone Swarms", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "16 pages, 8 figures", "summary": "Small Uncrewed Aerial Systems (sUAS) are increasingly deployed as autonomous\nswarms in search-and-rescue and other disaster-response scenarios. In these\nsettings, they use computer vision (CV) to detect objects of interest and\nautonomously adapt their missions. However, traditional CV systems often\nstruggle to recognize unfamiliar objects in open-world environments or to infer\ntheir relevance for mission planning. To address this, we incorporate large\nlanguage models (LLMs) to reason about detected objects and their implications.\nWhile LLMs can offer valuable insights, they are also prone to hallucinations\nand may produce incorrect, misleading, or unsafe recommendations. To ensure\nsafe and sensible decision-making under uncertainty, high-level decisions must\nbe governed by cognitive guardrails. This article presents the design,\nsimulation, and real-world integration of these guardrails for sUAS swarms in\nsearch-and-rescue missions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u5c0f\u578b\u65e0\u4eba\u98de\u884c\u7cfb\u7edf\uff08sUAS\uff09\u7fa4\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u641c\u7d22\u6551\u63f4\u4efb\u52a1\u4e2d\u7684\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u8ba4\u77e5\u62a4\u680f\u786e\u4fdd\u51b3\u7b56\u5b89\u5168\u548c\u5408\u7406\u3002", "motivation": "\u4f20\u7edf\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u5728\u5f00\u653e\u73af\u5883\u4e2d\u96be\u4ee5\u8bc6\u522b\u964c\u751f\u5bf9\u8c61\u6216\u63a8\u65ad\u5176\u5bf9\u4efb\u52a1\u89c4\u5212\u7684\u76f8\u5173\u6027\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u80fd\u63d0\u4f9b\u89c1\u89e3\uff0c\u4f46\u6613\u4ea7\u751f\u5e7b\u89c9\u6216\u4e0d\u5b89\u5168\u5efa\u8bae\u3002\u56e0\u6b64\uff0c\u9700\u8bbe\u8ba1\u8ba4\u77e5\u62a4\u680f\u4ee5\u786e\u4fdd\u51b3\u7b56\u5b89\u5168\u3002", "method": "\u901a\u8fc7\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u96c6\u6210\u5230sUAS\u7fa4\u4e2d\uff0c\u8bbe\u8ba1\u8ba4\u77e5\u62a4\u680f\u6765\u6307\u5bfc\u51b3\u7b56\uff0c\u5e76\u5728\u6a21\u62df\u548c\u5b9e\u9645\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u8ba4\u77e5\u62a4\u680f\u7684\u8bbe\u8ba1\u3001\u6a21\u62df\u4e0e\u5b9e\u9645\u96c6\u6210\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728sUAS\u7fa4\u641c\u7d22\u6551\u63f4\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u5408LLMs\u548c\u8ba4\u77e5\u62a4\u680f\u7684\u65b9\u6cd5\u63d0\u5347\u4e86sUAS\u7fa4\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\uff0c\u5e76\u786e\u4fdd\u5176\u51b3\u7b56\u7684\u5b89\u5168\u6027\u548c\u5408\u7406\u6027\u3002", "keywords": "\u5c0f\u578b\u65e0\u4eba\u98de\u884c\u7cfb\u7edf\uff08sUAS\uff09\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u8ba4\u77e5\u62a4\u680f\uff0c\u641c\u7d22\u6551\u63f4\uff0c\u81ea\u4e3b\u51b3\u7b56"}}
{"id": "2505.11047", "pdf": "https://arxiv.org/pdf/2505.11047", "abs": "https://arxiv.org/abs/2505.11047", "authors": ["Arghya Mallick", "Georgios Pantazis", "Mohammad Khosravi", "Peyman Mohajerin Esfahani", "Sergio Grammatico"], "title": "User-centric Vehicle-to-Grid Optimization with an Input Convex Neural Network-based Battery Degradation Model", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "We propose a data-driven, user-centric vehicle-to-grid (V2G) methodology\nbased on multi-objective optimization to balance battery degradation and V2G\nrevenue according to EV user preference. Given the lack of accurate and\ngeneralizable battery degradation models, we leverage input convex neural\nnetworks (ICNNs) to develop a data-driven degradation model trained on\nextensive experimental datasets. This approach enables our model to capture\nnonconvex dependencies on battery temperature and time while maintaining\nconvexity with respect to the charging rate. Such a partial convexity property\nensures that the second stage of our methodology remains computationally\nefficient. In the second stage, we integrate our data-driven degradation model\ninto a multi-objective optimization framework to generate an optimal smart\ncharging profile for each EV. This profile effectively balances the trade-off\nbetween financial benefits from V2G participation and battery degradation,\ncontrolled by a hyperparameter reflecting the user prioritization of battery\nhealth. Numerical simulations show the high accuracy of the ICNN model in\npredicting battery degradation for unseen data. Finally, we present a trade-off\ncurve illustrating financial benefits from V2G versus losses from battery\nhealth degradation based on user preferences and showcase smart charging\nstrategies under realistic scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u76ee\u6807\u4f18\u5316\u7684\u6570\u636e\u9a71\u52a8\u3001\u7528\u6237\u4e2d\u5fc3\u7684V2G\u65b9\u6cd5\uff0c\u5e73\u8861\u7535\u6c60\u635f\u8017\u4e0eV2G\u6536\u76ca\u3002\u901a\u8fc7\u8f93\u5165\u51f8\u6027\u795e\u7ecf\u7f51\u7edc\uff08ICNNs\uff09\u5efa\u7acb\u6570\u636e\u9a71\u52a8\u7684\u7535\u6c60\u635f\u8017\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\u751f\u6210\u6700\u4f18\u5145\u7535\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u7528\u6237\u504f\u597d\u4e0b\u7684\u6536\u76ca\u4e0e\u635f\u8017\u6743\u8861\u66f2\u7ebf\u3002", "motivation": "\u73b0\u6709\u7535\u6c60\u635f\u8017\u6a21\u578b\u7f3a\u4e4f\u51c6\u786e\u6027\u548c\u901a\u7528\u6027\uff0c\u96be\u4ee5\u6ee1\u8db3\u7528\u6237\u5bf9\u7535\u6c60\u5065\u5eb7\u4e0eV2G\u6536\u76ca\u7684\u6743\u8861\u9700\u6c42\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u7075\u6d3b\u9002\u5e94\u7528\u6237\u504f\u597d\u3002", "method": "1. \u4f7f\u7528ICNNs\u6784\u5efa\u6570\u636e\u9a71\u52a8\u7684\u7535\u6c60\u635f\u8017\u6a21\u578b\uff0c\u4fdd\u6301\u5bf9\u5145\u7535\u901f\u7387\u7684\u51f8\u6027\u4ee5\u4fdd\u8bc1\u8ba1\u7b97\u6548\u7387\uff1b2. \u5c06\u8be5\u6a21\u578b\u878d\u5165\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\uff0c\u751f\u6210\u517c\u987eV2G\u6536\u76ca\u548c\u7535\u6c60\u5065\u5eb7\u7684\u667a\u80fd\u5145\u7535\u7b56\u7565\u3002", "result": "ICNN\u6a21\u578b\u5728\u9884\u6d4b\u672a\u77e5\u6570\u636e\u65f6\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u3002\u8bba\u6587\u5c55\u793a\u4e86\u57fa\u4e8e\u7528\u6237\u504f\u597d\u7684V2G\u6536\u76ca\u4e0e\u7535\u6c60\u635f\u8017\u4e4b\u95f4\u7684\u6743\u8861\u66f2\u7ebf\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u573a\u666f\u9a8c\u8bc1\u4e86\u667a\u80fd\u5145\u7535\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u7528\u6237\u4e2d\u5fc3\u7684V2G\u4f18\u5316\uff0c\u5e73\u8861\u4e86\u7ecf\u6d4e\u6536\u76ca\u4e0e\u7535\u6c60\u5065\u5eb7\uff0c\u540c\u65f6\u8bc1\u660e\u4e86ICNN\u5728\u590d\u6742\u975e\u7ebf\u6027\u5173\u7cfb\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "V2G\uff0c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u7535\u6c60\u635f\u8017\uff0cICNN\uff0c\u7528\u6237\u504f\u597d"}}
{"id": "2505.23580", "pdf": "https://arxiv.org/pdf/2505.23580", "abs": "https://arxiv.org/abs/2505.23580", "authors": ["Ramit Aditya", "Razvan Bunescu", "Smita Nannaware", "Erfan Al-Hossami"], "title": "Engineering Serendipity through Recommendations of Items with Atypical Aspects", "categories": ["cs.IR", "cs.AI"], "comment": "25 pages of content + references and appendix. arXiv admin note: text\n  overlap with arXiv:2311.02702", "summary": "A restaurant dinner or a hotel stay may lead to memorable experiences when\nguests encounter unexpected aspects that also match their interests. For\nexample, an origami-making station in the waiting area of a restaurant may be\nboth surprising and enjoyable for a customer who is passionate about paper\ncrafts. Similarly, an exhibit of 18th century harpsichords would be atypical\nfor a hotel lobby and likely pique the interest of a guest who has a passion\nfor Baroque music. Motivated by this insight, in this paper we introduce the\nnew task of engineering serendipity through recommendations of items with\natypical aspects. We describe an LLM-based system pipeline that extracts\natypical aspects from item reviews, then estimates and aggregates their\nuser-specific utility in a measure of serendipity potential that is used to\nrerank a list of items recommended to the user. To facilitate system\ndevelopment and evaluation, we introduce a dataset of Yelp reviews that are\nmanually annotated with atypical aspects and a dataset of artificially\ngenerated user profiles, together with crowdsourced annotations of user-aspect\nutility values. Furthermore, we introduce a custom procedure for dynamic\nselection of in-context learning examples, which is shown to improve LLM-based\njudgments of atypicality and utility. Experimental evaluations show that\nserendipity-based rankings generated by the system are highly correlated with\nground truth rankings for which serendipity scores are computed from manual\nannotations of atypical aspects and their user-dependent utility. Overall, we\nhope that the new recommendation task and the associated system presented in\nthis paper catalyze further research into recommendation approaches that go\nbeyond accuracy in their pursuit of enhanced user satisfaction.\n  The datasets and the code are made publicly available at\nhttps://github.com/ramituncc49er/ATARS .", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u63a8\u8350\u5177\u6709\u975e\u5178\u578b\u7279\u5f81\u7684\u7269\u54c1\u6765\u8bbe\u8ba1\u201c\u610f\u5916\u4e4b\u559c\u201d\u7684\u65b0\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\uff0c\u4ece\u8bc4\u8bba\u4e2d\u63d0\u53d6\u975e\u5178\u578b\u7279\u5f81\u5e76\u8ba1\u7b97\u7528\u6237\u7684\u6f5c\u5728\u60ca\u559c\u503c\uff0c\u7528\u4e8e\u91cd\u65b0\u6392\u5e8f\u63a8\u8350\u5217\u8868\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u751f\u6210\u7684\u60ca\u559c\u6392\u540d\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u771f\u5b9e\u6392\u540d\u9ad8\u5ea6\u76f8\u5173\u3002", "motivation": "\u901a\u8fc7\u63a8\u8350\u7b26\u5408\u7528\u6237\u5174\u8da3\u7684\u975e\u5178\u578b\u7269\u54c1\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u4e2d\u7684\u610f\u5916\u60ca\u559c\u611f\uff0c\u5982\u9910\u5385\u4e2d\u7684\u6298\u7eb8\u6d3b\u52a8\u5bf9\u559c\u6b22\u624b\u5de5\u7684\u987e\u5ba2\u3002", "method": "\u4f7f\u7528LLM\u4ece\u8bc4\u8bba\u4e2d\u63d0\u53d6\u975e\u5178\u578b\u7279\u5f81\uff0c\u4f30\u8ba1\u7528\u6237\u7279\u5b9a\u6548\u7528\u5e76\u805a\u5408\u4e3a\u60ca\u559c\u6f5c\u529b\u503c\uff0c\u7528\u4e8e\u91cd\u65b0\u6392\u5e8f\u63a8\u8350\u5217\u8868\u3002\u8fd8\u63d0\u51fa\u52a8\u6001\u9009\u62e9\u4e0a\u4e0b\u6587\u5b66\u4e60\u793a\u4f8b\u7684\u5b9a\u5236\u65b9\u6cd5\u3002", "result": "\u7cfb\u7edf\u751f\u6210\u7684\u60ca\u559c\u6392\u540d\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u771f\u5b9e\u6392\u540d\u9ad8\u5ea6\u76f8\u5173\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8d85\u8d8a\u51c6\u786e\u6027\u7684\u65b0\u65b9\u5411\uff0c\u65e8\u5728\u901a\u8fc7\u60ca\u559c\u611f\u63d0\u5347\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "keywords": "\u63a8\u8350\u7cfb\u7edf, \u610f\u5916\u4e4b\u559c, LLM, \u975e\u5178\u578b\u7279\u5f81, \u7528\u6237\u6ee1\u610f\u5ea6"}}
{"id": "2505.22670", "pdf": "https://arxiv.org/pdf/2505.22670", "abs": "https://arxiv.org/abs/2505.22670", "authors": ["Jin Han", "Xin-Zheng Lu", "Jia-Rui Lin"], "title": "Unified Network-Based Representation of BIM Models for Embedding Semantic, Spatial, and Topological Data", "categories": ["cs.CE", "cs.LG"], "comment": null, "summary": "Building Information Modeling (BIM) has revolutionized the construction\nindustry by providing a comprehensive digital representation of building\nstructures throughout their lifecycle. However, existing research lacks\neffective methods for capturing the complex spatial and topological\nrelationships between components in BIM models, which are essential for\nunderstanding design patterns and enhancing decision-making. This study\nproposes a unified network-based representation method that integrates the\n\"semantic-spatial-topological\" multi-dimensional design features of BIM models.\nBy extending the IFC (Industry Foundation Classes) standard, we introduce local\nspatial relationships and topological connections between components to enrich\nthe network structure. This representation method enables a more detailed\nunderstanding of component interactions, dependencies, and implicit design\npatterns, effectively capturing the semantic, topological, and spatial\nrelationships in BIM, and holds significant potential for the representation\nand learning of design patterns.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f51\u7edc\u7684\u7edf\u4e00\u8868\u793a\u65b9\u6cd5\uff0c\u6574\u5408BIM\u6a21\u578b\u7684\u591a\u7ef4\u5ea6\u8bbe\u8ba1\u7279\u5f81\uff0c\u4ee5\u589e\u5f3a\u7a7a\u95f4\u4e0e\u62d3\u6251\u5173\u7cfb\u7684\u6355\u6349\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u6709\u6548\u65b9\u6cd5\u6355\u6349BIM\u6a21\u578b\u4e2d\u590d\u6742\u7684\u7a7a\u95f4\u4e0e\u62d3\u6251\u5173\u7cfb\uff0c\u8fd9\u5bf9\u7406\u89e3\u8bbe\u8ba1\u6a21\u5f0f\u548c\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6269\u5c55IFC\u6807\u51c6\uff0c\u5f15\u5165\u5c40\u90e8\u7a7a\u95f4\u5173\u7cfb\u4e0e\u62d3\u6251\u8fde\u63a5\uff0c\u6784\u5efa\u7f51\u7edc\u7ed3\u6784\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u66f4\u7ec6\u81f4\u5730\u7406\u89e3\u7ec4\u4ef6\u4ea4\u4e92\u3001\u4f9d\u8d56\u548c\u9690\u542b\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347BIM\u8bed\u4e49\u3001\u62d3\u6251\u4e0e\u7a7a\u95f4\u5173\u7cfb\u7684\u8868\u793a\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8bbe\u8ba1\u6a21\u5f0f\u7684\u8868\u793a\u4e0e\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5177\u6709\u91cd\u8981\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "BIM, \u7a7a\u95f4\u5173\u7cfb, \u62d3\u6251\u8fde\u63a5, IFC\u6807\u51c6, \u8bbe\u8ba1\u6a21\u5f0f"}}
{"id": "2505.23584", "pdf": "https://arxiv.org/pdf/2505.23584", "abs": "https://arxiv.org/abs/2505.23584", "authors": ["Sumbal Malik", "Majid Khonji", "Khaled Elbassioni", "Jorge Dias"], "title": "Collaborative Last-Mile Delivery: A Multi-Platform Vehicle Routing Problem With En-route Charging", "categories": ["cs.MA", "cs.AI", "cs.RO"], "comment": null, "summary": "The rapid growth of e-commerce and the increasing demand for timely,\ncost-effective last-mile delivery have increased interest in collaborative\nlogistics. This research introduces a novel collaborative synchronized\nmulti-platform vehicle routing problem with drones and robots (VRP-DR), where a\nfleet of $\\mathcal{M}$ trucks, $\\mathcal{N}$ drones and $\\mathcal{K}$ robots,\ncooperatively delivers parcels. Trucks serve as mobile platforms, enabling the\nlaunching, retrieving, and en-route charging of drones and robots, thereby\naddressing critical limitations such as restricted payload capacities, limited\nrange, and battery constraints. The VRP-DR incorporates five realistic\nfeatures: (1) multi-visit service per trip, (2) multi-trip operations, (3)\nflexible docking, allowing returns to the same or different trucks (4) cyclic\nand acyclic operations, enabling return to the same or different nodes; and (5)\nen-route charging, enabling drones and robots to recharge while being\ntransported on the truck, maximizing operational efficiency by utilizing idle\ntransit time. The VRP-DR is formulated as a mixed-integer linear program (MILP)\nto minimize both operational costs and makespan. To overcome the computational\nchallenges of solving large-scale instances, a scalable heuristic algorithm,\nFINDER (Flexible INtegrated Delivery with Energy Recharge), is developed, to\nprovide efficient, near-optimal solutions. Numerical experiments across various\ninstance sizes evaluate the performance of the MILP and heuristic approaches in\nterms of solution quality and computation time. The results demonstrate\nsignificant time savings of the combined delivery mode over the truck-only mode\nand substantial cost reductions from enabling multi-visits. The study also\nprovides insights into the effects of en-route charging, docking flexibility,\ndrone count, speed, and payload capacity on system performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5e73\u53f0\u8f66\u8f86\u3001\u65e0\u4eba\u673a\u548c\u673a\u5668\u4eba\u534f\u540c\u914d\u9001\u7684VRP-DR\u95ee\u9898\uff0c\u901a\u8fc7\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u548c\u542f\u53d1\u5f0f\u7b97\u6cd5\u4f18\u5316\u6210\u672c\u548c\u6548\u7387\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u7535\u5b50\u5546\u52a1\u7684\u5feb\u901f\u589e\u957f\u548c\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u7684\u6548\u7387\u9700\u6c42\u63a8\u52a8\u4e86\u591a\u5e73\u53f0\u534f\u540c\u914d\u9001\u7684\u7814\u7a76\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u548c\u542f\u53d1\u5f0f\u7b97\u6cd5FINDER\u89e3\u51b3VRP-DR\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u534f\u540c\u914d\u9001\u6a21\u5f0f\u5728\u65f6\u95f4\u548c\u6210\u672c\u4e0a\u663e\u8457\u4f18\u4e8e\u7eaf\u5361\u8f66\u914d\u9001\uff0c\u5e76\u5206\u6790\u4e86\u5145\u7535\u3001\u7075\u6d3b\u6027\u7b49\u56e0\u7d20\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "VRP-DR\u6a21\u578b\u548cFINDER\u7b97\u6cd5\u5728\u591a\u5e73\u53f0\u534f\u540c\u914d\u9001\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u548c\u8fd1\u4f18\u7684\u6027\u80fd\u3002", "keywords": "\u534f\u540c\u914d\u9001\u3001\u65e0\u4eba\u673a\u3001\u673a\u5668\u4eba\u3001\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u3001\u542f\u53d1\u5f0f\u7b97\u6cd5"}}
{"id": "2505.22678", "pdf": "https://arxiv.org/pdf/2505.22678", "abs": "https://arxiv.org/abs/2505.22678", "authors": ["Jiahao Yang", "Ran Fang", "Ming Zhang", "Jun Zhou"], "title": "An Efficient deep learning model to Predict Stock Price Movement Based on Limit Order Book", "categories": ["q-fin.TR", "cs.LG"], "comment": null, "summary": "In high-frequency trading (HFT), leveraging limit order books (LOB) to model\nstock price movements is crucial for achieving profitable outcomes. However,\nthis task is challenging due to the high-dimensional and volatile nature of the\noriginal data. Even recent deep learning models often struggle to capture price\nmovement patterns effectively, particularly without well-designed features. We\nobserved that raw LOB data exhibits inherent symmetry between the ask and bid\nsides, and the bid-ask differences demonstrate greater stability and lower\ncomplexity compared to the original data. Building on this insight, we propose\na novel approach in which leverages the Siamese architecture to enhance the\nperformance of existing deep learning models. The core idea involves processing\nthe ask and bid sides separately using the same module with shared parameters.\nWe applied our Siamese-based methods to several widely used strong baselines\nand validated their effectiveness using data from 14 military industry stocks\nin the Chinese A-share market. Furthermore, we integrated multi-head attention\n(MHA) mechanisms with the Long Short-Term Memory (LSTM) module to investigate\nits role in modeling stock price movements. Our experiments used raw data and\nwidely used Order Flow Imbalance (OFI) features as input with some strong\nbaseline models. The results show that our method improves the performance of\nstrong baselines in over 75$% of cases, excluding the Multi-Layer Perception\n(MLP) baseline, which performed poorly and is not considered practical.\nFurthermore, we found that Multi-Head Attention can enhance model performance,\nparticularly over shorter forecasting horizons.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684Siamese\u67b6\u6784\u65b9\u6cd5\uff0c\u5229\u7528\u9650\u4ef7\u8ba2\u5355\u7c3f\uff08LOB\uff09\u6570\u636e\u7684\u5bf9\u79f0\u6027\uff0c\u7ed3\u5408\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u539f\u59cbLOB\u6570\u636e\u9ad8\u7ef4\u4e14\u6ce2\u52a8\u6027\u5f3a\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u6709\u6548\u6355\u6349\u4ef7\u683c\u53d8\u52a8\u6a21\u5f0f\uff0c\u800c\u6570\u636e\u7684\u5bf9\u79f0\u6027\u548c\u7a33\u5b9a\u6027\u4e3a\u6539\u8fdb\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002", "method": "\u91c7\u7528Siamese\u67b6\u6784\u5206\u522b\u5904\u7406\u8ba2\u5355\u7c3f\u7684\u4e70\u5356\u4e24\u4fa7\uff0c\u7ed3\u5408\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff08MHA\uff09\u548cLSTM\u6a21\u5757\uff0c\u5e76\u5728\u591a\u79cd\u57fa\u7ebf\u6a21\u578b\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u8be5\u65b9\u6cd5\u572875%\u4ee5\u4e0a\u7684\u6848\u4f8b\u4e2d\u63d0\u5347\u4e86\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5728\u77ed\u671f\u9884\u6d4b\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "Siamese\u67b6\u6784\u548c\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u7ed3\u5408\u6709\u6548\u63d0\u5347\u4e86\u9650\u4ef7\u8ba2\u5355\u7c3f\u6570\u636e\u5efa\u6a21\u7684\u6027\u80fd\u3002", "keywords": "\u9ad8\u9891\u4ea4\u6613\uff08HFT\uff09\u3001\u9650\u4ef7\u8ba2\u5355\u7c3f\uff08LOB\uff09\u3001Siamese\u67b6\u6784\u3001\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff08MHA\uff09\u3001LSTM"}}
{"id": "2505.23595", "pdf": "https://arxiv.org/pdf/2505.23595", "abs": "https://arxiv.org/abs/2505.23595", "authors": ["Youssef Mohamed", "Noran Mohamed", "Khaled Abouhashad", "Feilong Tang", "Sara Atito", "Shoaib Jameel", "Imran Razzak", "Ahmed B. Zaky"], "title": "DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task Learning in Chest X-ray Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "While Multi-Task Learning (MTL) offers inherent advantages in complex domains\nsuch as medical imaging by enabling shared representation learning, effectively\nbalancing task contributions remains a significant challenge. This paper\naddresses this critical issue by introducing DeepChest, a novel,\ncomputationally efficient and effective dynamic task-weighting framework\nspecifically designed for multi-label chest X-ray (CXR) classification. Unlike\nexisting heuristic or gradient-based methods that often incur substantial\noverhead, DeepChest leverages a performance-driven weighting mechanism based on\neffective analysis of task-specific loss trends. Given a network architecture\n(e.g., ResNet18), our model-agnostic approach adaptively adjusts task\nimportance without requiring gradient access, thereby significantly reducing\nmemory usage and achieving a threefold increase in training speed. It can be\neasily applied to improve various state-of-the-art methods. Extensive\nexperiments on a large-scale CXR dataset demonstrate that DeepChest not only\noutperforms state-of-the-art MTL methods by 7% in overall accuracy but also\nyields substantial reductions in individual task losses, indicating improved\ngeneralization and effective mitigation of negative transfer. The efficiency\nand performance gains of DeepChest pave the way for more practical and robust\ndeployment of deep learning in critical medical diagnostic applications. The\ncode is publicly available at https://github.com/youssefkhalil320/DeepChest-MTL", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DeepChest\uff0c\u4e00\u79cd\u9ad8\u6548\u52a8\u6001\u4efb\u52a1\u52a0\u6743\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6807\u7b7e\u80f8\u90e8X\u5149\u5206\u7c7b\uff0c\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u4efb\u52a1\u8d21\u732e\u5e73\u8861\u7684\u96be\u9898\u3002", "motivation": "\u591a\u4efb\u52a1\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u7b49\u590d\u6742\u9886\u57df\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u4efb\u52a1\u8d21\u732e\u7684\u5e73\u8861\u4ecd\u662f\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u4efb\u52a1\u7279\u5b9a\u635f\u5931\u8d8b\u52bf\u5206\u6790\u7684\u6027\u80fd\u9a71\u52a8\u52a0\u6743\u673a\u5236\uff0c\u65e0\u9700\u68af\u5ea6\u8bbf\u95ee\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u4efb\u52a1\u91cd\u8981\u6027\u3002", "result": "DeepChest\u5728\u591a\u6807\u7b7eCXR\u5206\u7c7b\u4e2d\u9886\u5148\u73b0\u6709\u65b9\u6cd57%\u7684\u6574\u4f53\u51c6\u786e\u7387\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "DeepChest\u4e3a\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u591a\u4efb\u52a1\u5b66\u4e60, \u80f8\u90e8X\u5149\u5206\u7c7b, \u52a8\u6001\u4efb\u52a1\u52a0\u6743, \u533b\u5b66\u5f71\u50cf"}}
{"id": "2505.22684", "pdf": "https://arxiv.org/pdf/2505.22684", "abs": "https://arxiv.org/abs/2505.22684", "authors": ["Yufeng Wang", "Yiguang Bai", "Tianqing Zhu", "Ismail Ben Ayed", "Jing Yuan"], "title": "Recovering Fairness Directly from Modularity: a New Way for Fair Community Partitioning", "categories": ["cs.SI", "cs.LG"], "comment": "17pages, 5 figures", "summary": "Community partitioning is crucial in network analysis, with modularity\noptimization being the prevailing technique. However, traditional\nmodularity-based methods often overlook fairness, a critical aspect in\nreal-world applications. To address this, we introduce protected group networks\nand propose a novel fairness-modularity metric. This metric extends traditional\nmodularity by explicitly incorporating fairness, and we prove that minimizing\nit yields naturally fair partitions for protected groups while maintaining\ntheoretical soundness. We develop a general optimization framework for fairness\npartitioning and design the efficient Fair Fast Newman (FairFN) algorithm,\nenhancing the Fast Newman (FN) method to optimize both modularity and fairness.\nExperiments show FairFN achieves significantly improved fairness and\nhigh-quality partitions compared to state-of-the-art methods, especially on\nunbalanced datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u516c\u5e73\u6027-\u6a21\u5757\u5316\u5ea6\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4f20\u7edf\u6a21\u5757\u5316\u4e2d\u663e\u5f0f\u52a0\u5165\u516c\u5e73\u6027\uff0c\u89e3\u51b3\u4e86\u4fdd\u62a4\u7fa4\u4f53\u7f51\u7edc\u7684\u516c\u5e73\u5206\u533a\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u7684FairFN\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u6a21\u5757\u5316\u4f18\u5316\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u516c\u5e73\u6027\uff0c\u800c\u516c\u5e73\u6027\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u4fdd\u62a4\u7fa4\u4f53\u7f51\u7edc\uff0c\u63d0\u51fa\u516c\u5e73\u6027-\u6a21\u5757\u5316\u5ea6\u91cf\uff0c\u5f00\u53d1Fair Fast Newman (FairFN)\u7b97\u6cd5\uff0c\u4f18\u5316\u6a21\u5757\u5316\u548c\u516c\u5e73\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFairFN\u5728\u516c\u5e73\u6027\u548c\u5206\u533a\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "FairFN\u7b97\u6cd5\u80fd\u6709\u6548\u5e73\u8861\u6a21\u5757\u5316\u548c\u516c\u5e73\u6027\uff0c\u9002\u7528\u4e8e\u4fdd\u62a4\u7fa4\u4f53\u7684\u7f51\u7edc\u5206\u533a\u3002", "keywords": "\u793e\u533a\u5206\u533a, \u6a21\u5757\u5316\u4f18\u5316, \u516c\u5e73\u6027, FairFN\u7b97\u6cd5, \u4fdd\u62a4\u7fa4\u4f53\u7f51\u7edc"}}
{"id": "2505.22688", "pdf": "https://arxiv.org/pdf/2505.22688", "abs": "https://arxiv.org/abs/2505.22688", "authors": ["Palur Venkata Raghuvamsi", "Siyuan Brandon Loh", "Prasanta Bhattacharya", "Joses Ho", "Raphael Lee Tze Chuen", "Alvin X. Han", "Sebastian Maurer-Stroh"], "title": "Investigating the effectiveness of multimodal data in forecasting SARS-COV-2 case surges", "categories": ["q-bio.QM", "cs.LG", "stat.ML"], "comment": null, "summary": "The COVID-19 pandemic response relied heavily on statistical and machine\nlearning models to predict key outcomes such as case prevalence and fatality\nrates. These predictions were instrumental in enabling timely public health\ninterventions that helped break transmission cycles. While most existing models\nare grounded in traditional epidemiological data, the potential of alternative\ndatasets, such as those derived from genomic information and human behavior,\nremains underexplored. In the current study, we investigated the usefulness of\ndiverse modalities of feature sets in predicting case surges. Our results\nhighlight the relative effectiveness of biological (e.g., mutations), public\nhealth (e.g., case counts, policy interventions) and human behavioral features\n(e.g., mobility and social media conversations) in predicting country-level\ncase surges. Importantly, we uncover considerable heterogeneity in predictive\nperformance across countries and feature modalities, suggesting that surge\nprediction models may need to be tailored to specific national contexts and\npandemic phases. Overall, our work highlights the value of integrating\nalternative data sources into existing disease surveillance frameworks to\nenhance the prediction of pandemic dynamics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u591a\u79cd\u7279\u5f81\u96c6\uff08\u5982\u751f\u7269\u3001\u516c\u5171\u536b\u751f\u548c\u4eba\u7c7b\u884c\u4e3a\u6570\u636e\uff09\u9884\u6d4bCOVID-19\u75c5\u4f8b\u6fc0\u589e\u7684\u6548\u679c\uff0c\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u56fd\u5bb6\u548c\u7279\u5f81\u6a21\u6001\u7684\u9884\u6d4b\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u9700\u6839\u636e\u56fd\u60c5\u548c\u75ab\u60c5\u9636\u6bb5\u5b9a\u5236\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u75ab\u60c5\u9884\u6d4b\u6a21\u578b\u591a\u57fa\u4e8e\u4f20\u7edf\u6d41\u884c\u75c5\u5b66\u6570\u636e\uff0c\u800c\u751f\u7269\u4fe1\u606f\u548c\u4eba\u7c7b\u884c\u4e3a\u7b49\u66ff\u4ee3\u6570\u636e\u6e90\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u8fd9\u4e9b\u66ff\u4ee3\u6570\u636e\u5bf9\u75c5\u4f8b\u6fc0\u589e\u9884\u6d4b\u7684\u4ef7\u503c\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u751f\u7269\u7279\u5f81\uff08\u5982\u75c5\u6bd2\u7a81\u53d8\uff09\u3001\u516c\u5171\u536b\u751f\u6570\u636e\uff08\u5982\u75c5\u4f8b\u6570\u548c\u653f\u7b56\u5e72\u9884\uff09\u548c\u4eba\u7c7b\u884c\u4e3a\u7279\u5f81\uff08\u5982\u79fb\u52a8\u6027\u548c\u793e\u4ea4\u5a92\u4f53\u8ba8\u8bba\uff09\u5728\u9884\u6d4b\u56fd\u5bb6\u5c42\u9762\u75c5\u4f8b\u6fc0\u589e\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u7279\u5f81\u6a21\u6001\u548c\u56fd\u5bb6\u95f4\u7684\u9884\u6d4b\u6548\u679c\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660e\u9700\u9488\u5bf9\u7279\u5b9a\u56fd\u5bb6\u548c\u75ab\u60c5\u9636\u6bb5\u4f18\u5316\u6a21\u578b\u3002\u751f\u7269\u548c\u4eba\u7c7b\u884c\u4e3a\u6570\u636e\u7684\u7efc\u5408\u4f7f\u7528\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u6574\u5408\u66ff\u4ee3\u6570\u636e\u6e90\u53ef\u4f18\u5316\u73b0\u6709\u75be\u75c5\u76d1\u6d4b\u6846\u67b6\uff0c\u63d0\u9ad8\u75ab\u60c5\u52a8\u6001\u9884\u6d4b\u80fd\u529b\uff0c\u672a\u6765\u6a21\u578b\u5e94\u6ce8\u91cd\u5b9a\u5236\u5316\u548c\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u3002", "keywords": "COVID-19\u3001\u75c5\u4f8b\u6fc0\u589e\u9884\u6d4b\u3001\u673a\u5668\u5b66\u4e60\u3001\u591a\u6a21\u6001\u6570\u636e\u3001\u516c\u5171\u536b\u751f"}}
{"id": "2505.23617", "pdf": "https://arxiv.org/pdf/2505.23617", "abs": "https://arxiv.org/abs/2505.23617", "authors": ["Chenhao Zheng", "Jieyu Zhang", "Mohammadreza Salehi", "Ziqi Gao", "Vishnu Iyengar", "Norimasa Kobori", "Quan Kong", "Ranjay Krishna"], "title": "One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "comment": null, "summary": "Effective video tokenization is critical for scaling transformer models for\nlong videos. Current approaches tokenize videos using space-time patches,\nleading to excessive tokens and computational inefficiencies. The best token\nreduction strategies degrade performance and barely reduce the number of tokens\nwhen the camera moves. We introduce grounded video tokenization, a paradigm\nthat organizes tokens based on panoptic sub-object trajectories rather than\nfixed patches. Our method aligns with fundamental perceptual principles,\nensuring that tokenization reflects scene complexity rather than video\nduration. We propose TrajViT, a video encoder that extracts object trajectories\nand converts them into semantically meaningful tokens, significantly reducing\nredundancy while maintaining temporal coherence. Trained with contrastive\nlearning, TrajViT significantly outperforms space-time ViT (ViT3D) across\nmultiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a\nlarge margin of 6% top-5 recall in average at video-text retrieval task with\n10x token deduction. We also show TrajViT as a stronger model than ViT3D for\nbeing the video encoder for modern VideoLLM, obtaining an average of 5.2%\nperformance improvement across 6 VideoQA benchmarks while having 4x faster\ntraining time and 18x less inference FLOPs. TrajViT is the first efficient\nencoder to consistently outperform ViT3D across diverse video analysis tasks,\nmaking it a robust and scalable solution.", "AI": {"tldr": "TrajViT\u63d0\u51fa\u7684\u57fa\u4e8e\u7269\u4f53\u8f68\u8ff9\u7684tokenization\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u5197\u4f59token\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u5927\u5e45\u4f18\u4e8e\u4f20\u7edfViT3D\u3002", "motivation": "\u73b0\u6709\u89c6\u9891tokenization\u65b9\u6cd5\u901a\u8fc7\u65f6\u7a7a\u5206\u5757\u751f\u6210\u8fc7\u591a\u5197\u4f59token\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u4e14\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fagrounded video tokenization\uff0c\u57fa\u4e8e\u7269\u4f53\u8f68\u8ff9\u751f\u6210\u8bed\u4e49token\uff1b\u4f7f\u7528TrajViT\u7f16\u7801\u5668\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "TrajViT\u5728\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\u4e2dTop-5\u53ec\u56de\u7387\u6bd4ViT3D\u9ad86%\uff0ctoken\u51cf\u5c1110\u500d\uff1b\u4f5c\u4e3aVideoLLM\u7f16\u7801\u5668\u65f6\u6027\u80fd\u63d0\u53475.2%\uff0c\u8bad\u7ec3\u5feb4\u500d\uff0c\u63a8\u7406FLOPs\u5c1118\u500d\u3002", "conclusion": "TrajViT\u662f\u9996\u4e2a\u5728\u591a\u6837\u89c6\u9891\u4efb\u52a1\u4e2d\u9ad8\u6548\u8d85\u8d8aViT3D\u7684\u7f16\u7801\u5668\uff0c\u5177\u5907\u5f3a\u6269\u5c55\u6027\u3002", "keywords": "video tokenization, TrajViT, object trajectory, contrastive learning, VideoLLM"}}
{"id": "2505.22743", "pdf": "https://arxiv.org/pdf/2505.22743", "abs": "https://arxiv.org/abs/2505.22743", "authors": ["Sitan Chen", "Weiyuan Gong", "Jonas Haferkamp", "Yihui Quek"], "title": "Information-Computation Gaps in Quantum Learning via Low-Degree Likelihood", "categories": ["quant-ph", "cs.CC", "cs.DS", "cs.LG"], "comment": "88 pages, 2 figures", "summary": "In a variety of physically relevant settings for learning from quantum data,\ndesigning protocols that can computationally efficiently extract information\nremains largely an art, and there are important cases where we believe this to\nbe impossible, that is, where there is an information-computation gap. While\nthere is a large array of tools in the classical literature for giving evidence\nfor average-case hardness of statistical inference problems, the corresponding\ntools in the quantum literature are far more limited. One such framework in the\nclassical literature, the low-degree method, makes predictions about hardness\nof inference problems based on the failure of estimators given by low-degree\npolynomials. In this work, we extend this framework to the quantum setting.\n  We establish a general connection between state designs and low-degree\nhardness. We use this to obtain the first information-computation gaps for\nlearning Gibbs states of random, sparse, non-local Hamiltonians. We also use it\nto prove hardness for learning random shallow quantum circuit states in a\nchallenging model where states can be measured in adaptively chosen bases. To\nour knowledge, the ability to model adaptivity within the low-degree framework\nwas open even in classical settings. In addition, we also obtain a low-degree\nhardness result for quantum error mitigation against strategies with\nsingle-qubit measurements.\n  We define a new quantum generalization of the planted biclique problem and\nidentify the threshold at which this problem becomes computationally hard for\nprotocols that perform local measurements. Interestingly, the complexity\nlandscape for this problem shifts when going from local measurements to more\nentangled single-copy measurements.\n  We show average-case hardness for the \"standard\" variant of Learning\nStabilizers with Noise and for agnostically learning product states.", "AI": {"tldr": "\u8bba\u6587\u6269\u5c55\u4e86\u4f4e\u9636\u65b9\u6cd5\u6846\u67b6\u81f3\u91cf\u5b50\u8bbe\u7f6e\uff0c\u5efa\u7acb\u4e86\u72b6\u6001\u8bbe\u8ba1\u4e0e\u4f4e\u9636\u56f0\u96be\u6027\u7684\u8054\u7cfb\uff0c\u9996\u6b21\u8bc1\u660e\u4e86\u968f\u673a\u7a00\u758f\u975e\u5c40\u90e8\u54c8\u5bc6\u987f\u91cfGibbs\u6001\u5b66\u4e60\u7684\u4fe1\u606f-\u8ba1\u7b97\u9e3f\u6c9f\uff0c\u5e76\u8bc1\u660e\u4e86\u968f\u673a\u6d45\u5c42\u91cf\u5b50\u7535\u8def\u6001\u5b66\u4e60\u7684\u56f0\u96be\u6027\u3002", "motivation": "\u91cf\u5b50\u6570\u636e\u5b66\u4e60\u4e2d\uff0c\u4fe1\u606f-\u8ba1\u7b97\u9e3f\u6c9f\u7684\u5b58\u5728\u4f7f\u5f97\u8bbe\u8ba1\u9ad8\u6548\u534f\u8bae\u6210\u4e3a\u6311\u6218\u3002\u73b0\u6709\u91cf\u5b50\u6587\u732e\u4e2d\u7f3a\u4e4f\u7c7b\u4f3c\u7ecf\u5178\u6587\u732e\u4e2d\u7684\u5de5\u5177\u6765\u8bc1\u660e\u7edf\u8ba1\u63a8\u65ad\u95ee\u9898\u7684\u5e73\u5747\u60c5\u51b5\u56f0\u96be\u6027\u3002", "method": "\u6269\u5c55\u7ecf\u5178\u4f4e\u9636\u65b9\u6cd5\u6846\u67b6\u81f3\u91cf\u5b50\u8bbe\u7f6e\uff0c\u5229\u7528\u72b6\u6001\u8bbe\u8ba1\u4e0e\u4f4e\u9636\u591a\u9879\u5f0f\u4f30\u8ba1\u5668\u7684\u5931\u6548\u5efa\u7acb\u8054\u7cfb\uff0c\u5206\u6790Gibbs\u6001\u3001\u968f\u673a\u6d45\u5c42\u7535\u8def\u6001\u53ca\u91cf\u5b50\u8bef\u5dee\u7f13\u89e3\u7684\u56f0\u96be\u6027\u3002", "result": "\u6210\u529f\u8bc1\u660e\u4e86\u968f\u673a\u7a00\u758f\u975e\u5c40\u90e8\u54c8\u5bc6\u987f\u91cf\u7684Gibbs\u6001\u5b66\u4e60\u3001\u81ea\u9002\u5e94\u6d4b\u91cf\u6a21\u578b\u4e0b\u7684\u968f\u673a\u6d45\u5c42\u7535\u8def\u6001\u5b66\u4e60\u53ca\u91cf\u5b50\u8bef\u5dee\u7f13\u89e3\u7684\u5355\u6bd4\u7279\u6d4b\u91cf\u7b56\u7565\u5b58\u5728\u4fe1\u606f-\u8ba1\u7b97\u9e3f\u6c9f\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u586b\u8865\u4e86\u91cf\u5b50\u5b66\u4e60\u9886\u57df\u4e2d\u4f4e\u9636\u65b9\u6cd5\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u4e3a\u91cf\u5b50\u6001\u5b66\u4e60\u7684\u8ba1\u7b97\u56f0\u96be\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "keywords": "\u91cf\u5b50\u5b66\u4e60, \u4fe1\u606f-\u8ba1\u7b97\u9e3f\u6c9f, \u4f4e\u9636\u65b9\u6cd5, Gibbs\u6001, \u968f\u673a\u6d45\u5c42\u91cf\u5b50\u7535\u8def"}}
{"id": "2505.23624", "pdf": "https://arxiv.org/pdf/2505.23624", "abs": "https://arxiv.org/abs/2505.23624", "authors": ["Giacomo Bergami", "Emma Packer", "Kirsty Scott", "Silvia Del Din"], "title": "Towards Explainable Sequential Learning", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "This paper offers a hybrid explainable temporal data processing pipeline,\nDataFul Explainable MultivariatE coRrelatIonal Temporal Artificial inTElligence\n(EMeriTAte+DF), bridging numerical-driven temporal data classification with an\nevent-based one through verified artificial intelligence principles, enabling\nhuman-explainable results. This was possible through a preliminary a posteriori\nexplainable phase describing the numerical input data in terms of concurrent\nconstituents with numerical payloads. This further required extending the\nevent-based literature to design specification mining algorithms supporting\nconcurrent constituents. Our previous and current solutions outperform\nstate-of-the-art solutions for multivariate time series classifications, thus\nshowcasing the effectiveness of the proposed methodology.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u6570\u636e\u5904\u7406\u6d41\u7a0bEMeriTAte+DF\uff0c\u7ed3\u5408\u6570\u503c\u548c\u4e8b\u4ef6\u9a71\u52a8\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684AI\u539f\u5219\u5b9e\u73b0\u53ef\u89e3\u91ca\u7ed3\u679c\uff0c\u5e76\u5728\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u6570\u636e\u5904\u7406\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u548c\u5e76\u53d1\u5904\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u63d0\u5347\u5206\u7c7b\u6548\u679c\u5e76\u589e\u5f3a\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e86EMeriTAte+DF\u6d41\u7a0b\uff0c\u878d\u5408\u6570\u503c\u548c\u4e8b\u4ef6\u9a71\u52a8\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u6269\u5c55\u4e8b\u4ef6\u9a71\u52a8\u6587\u732e\u4ee5\u652f\u6301\u5e76\u53d1\u6210\u5206\uff0c\u5e76\u901a\u8fc7\u540e\u9a8c\u53ef\u89e3\u91ca\u9636\u6bb5\u63cf\u8ff0\u8f93\u5165\u6570\u636e\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\u3002", "conclusion": "\u6df7\u5408\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u6570\u636e\u5904\u7406\u65b9\u6cd5\u4e0d\u4ec5\u6709\u6548\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u8fd8\u589e\u5f3a\u4e86\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\u3002", "keywords": "\u53ef\u89e3\u91caAI, \u65f6\u95f4\u5e8f\u5217\u5206\u7c7b, \u6df7\u5408\u65b9\u6cd5, \u4e8b\u4ef6\u9a71\u52a8, \u591a\u53d8\u91cf\u5206\u6790"}}
{"id": "2505.22746", "pdf": "https://arxiv.org/pdf/2505.22746", "abs": "https://arxiv.org/abs/2505.22746", "authors": ["Jose Guadalupe Hernandez", "Attri Ghosh", "Philip J. Freda", "Yufei Meng", "Nicholas Matsumoto", "Jason H. Moore"], "title": "StarBASE-GP: Biologically-Guided Automated Machine Learning for Genotype-to-Phenotype Association Analysis", "categories": ["cs.NE", "cs.LG"], "comment": null, "summary": "We present the Star-Based Automated Single-locus and Epistasis analysis tool\n- Genetic Programming (StarBASE-GP), an automated framework for discovering\nmeaningful genetic variants associated with phenotypic variation in large-scale\ngenomic datasets. StarBASE-GP uses a genetic programming-based multi-objective\noptimization strategy to evolve machine learning pipelines that simultaneously\nmaximize explanatory power (r2) and minimize pipeline complexity. Biological\ndomain knowledge is integrated at multiple stages, including the use of nine\ninheritance encoding strategies to model deviations from additivity, a custom\nlinkage disequilibrium pruning node that minimizes redundancy among features,\nand a dynamic variant recommendation system that prioritizes informative\ncandidates for pipeline inclusion. We evaluate StarBASE-GP on a cohort of\nRattus norvegicus (brown rat) to identify variants associated with body mass\nindex, benchmarking its performance against a random baseline and a\nbiologically naive version of the tool. StarBASE-GP consistently evolves Pareto\nfronts with superior performance, yielding higher accuracy in identifying both\nground truth and novel quantitative trait loci, highlighting relevant targets\nfor future validation. By incorporating evolutionary search and relevant\nbiological theory into a flexible automated machine learning framework,\nStarBASE-GP demonstrates robust potential for advancing variant discovery in\ncomplex traits.", "AI": {"tldr": "StarBASE-GP\u662f\u4e00\u79cd\u57fa\u4e8e\u9057\u4f20\u7f16\u7a0b\u7684\u591a\u76ee\u6807\u4f18\u5316\u5de5\u5177\uff0c\u7528\u4e8e\u9ad8\u6548\u8bc6\u522b\u57fa\u56e0\u7ec4\u6570\u636e\u4e2d\u4e0e\u8868\u578b\u76f8\u5173\u7684\u53d8\u5f02\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u57fa\u56e0\u7ec4\u6570\u636e\u4e2d\u590d\u6742\u6027\u72b6\u76f8\u5173\u53d8\u5f02\u7684\u81ea\u52a8\u5316\u53d1\u73b0\u96be\u9898\uff0c\u5e76\u6574\u5408\u751f\u7269\u5b66\u77e5\u8bc6\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u9057\u4f20\u7f16\u7a0b\u591a\u76ee\u6807\u4f18\u5316\u7b56\u7565\uff0c\u7ed3\u5408\u591a\u79cd\u751f\u7269\u5b66\u9886\u57df\u77e5\u8bc6\uff08\u5982\u9057\u4f20\u7f16\u7801\u7b56\u7565\u3001\u8fde\u9501\u4e0d\u5e73\u8861\u4fee\u526a\uff09\u548c\u52a8\u6001\u53d8\u5f02\u63a8\u8350\u7cfb\u7edf\u3002", "result": "\u5728\u8910\u5bb6\u9f20\u7fa4\u4f53\u4e2d\u8868\u73b0\u4f18\u4e8e\u968f\u673a\u57fa\u51c6\u548c\u751f\u7269\u5b66\u65e0\u77e5\u7248\u672c\uff0c\u80fd\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u5df2\u77e5\u548c\u65b0\u6570\u91cf\u6027\u72b6\u4f4d\u70b9\u3002", "conclusion": "StarBASE-GP\u7ed3\u5408\u8fdb\u5316\u641c\u7d22\u4e0e\u751f\u7269\u5b66\u7406\u8bba\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u6027\u72b6\u53d8\u5f02\u53d1\u73b0\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002", "keywords": "\u9057\u4f20\u7f16\u7a0b,\u591a\u76ee\u6807\u4f18\u5316,\u57fa\u56e0\u7ec4\u5b66,\u6570\u91cf\u6027\u72b6\u4f4d\u70b9,\u53d8\u5f02\u53d1\u73b0"}}
{"id": "2505.23637", "pdf": "https://arxiv.org/pdf/2505.23637", "abs": "https://arxiv.org/abs/2505.23637", "authors": ["Dashti A. Ali", "Richard K. G. Do", "William R. Jarnagin", "Aras T. Asaad", "Amber L. Simpson"], "title": "Comparing the Effects of Persistence Barcodes Aggregation and Feature Concatenation on Medical Imaging", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 8 figures", "summary": "In medical image analysis, feature engineering plays an important role in the\ndesign and performance of machine learning models. Persistent homology (PH),\nfrom the field of topological data analysis (TDA), demonstrates robustness and\nstability to data perturbations and addresses the limitation from traditional\nfeature extraction approaches where a small change in input results in a large\nchange in feature representation. Using PH, we store persistent topological and\ngeometrical features in the form of the persistence barcode whereby large bars\nrepresent global topological features and small bars encapsulate geometrical\ninformation of the data. When multiple barcodes are computed from 2D or 3D\nmedical images, two approaches can be used to construct the final topological\nfeature vector in each dimension: aggregating persistence barcodes followed by\nfeaturization or concatenating topological feature vectors derived from each\nbarcode. In this study, we conduct a comprehensive analysis across diverse\nmedical imaging datasets to compare the effects of the two aforementioned\napproaches on the performance of classification models. The results of this\nanalysis indicate that feature concatenation preserves detailed topological\ninformation from individual barcodes, yields better classification performance\nand is therefore a preferred approach when conducting similar experiments.", "AI": {"tldr": "The paper compares two methods for constructing topological feature vectors from medical images using persistent homology, concluding that feature concatenation outperforms barcode aggregation in preserving detailed information and improving classification performance.", "motivation": "Traditional feature extraction in medical image analysis is sensitive to data perturbations. Persistent homology offers a robust alternative, but the best method to construct topological feature vectors from multiple barcodes is unclear.", "method": "The study compares two approaches: aggregating persistence barcodes before featurization vs. concatenating feature vectors from individual barcodes, tested across diverse medical imaging datasets.", "result": "Feature concatenation preserves more detailed topological information and leads to better classification performance compared to barcode aggregation.", "conclusion": "For similar experiments, concatenating topological feature vectors from individual barcodes is the preferred approach due to its superior performance.", "keywords": "medical image analysis, persistent homology, topological data analysis, feature engineering, classification performance"}}
{"id": "2505.22760", "pdf": "https://arxiv.org/pdf/2505.22760", "abs": "https://arxiv.org/abs/2505.22760", "authors": ["Razvan-Andrei Lascu", "Mateusz B. Majka"], "title": "Non-convex entropic mean-field optimization via Best Response flow", "categories": ["math.OC", "cs.LG", "math.PR"], "comment": "40 pages", "summary": "We study the problem of minimizing non-convex functionals on the space of\nprobability measures, regularized by the relative entropy (KL divergence) with\nrespect to a fixed reference measure, as well as the corresponding problem of\nsolving entropy-regularized non-convex-non-concave min-max problems. We utilize\nthe Best Response flow (also known in the literature as the fictitious play\nflow) and study how its convergence is influenced by the relation between the\ndegree of non-convexity of the functional under consideration, the\nregularization parameter and the tail behaviour of the reference measure. In\nparticular, we demonstrate how to choose the regularizer, given the non-convex\nfunctional, so that the Best Response operator becomes a contraction with\nrespect to the $L^1$-Wasserstein distance, which then ensures the existence of\nits unique fixed point, which is then shown to be the unique global minimizer\nfor our optimization problem. This extends recent results where the Best\nResponse flow was applied to solve convex optimization problems regularized by\nthe relative entropy with respect to arbitrary reference measures, and with\narbitrary values of the regularization parameter. Our results explain precisely\nhow the assumption of convexity can be relaxed, at the expense of making a\nspecific choice of the regularizer. Additionally, we demonstrate how these\nresults can be applied in reinforcement learning in the context of policy\noptimization for Markov Decision Processes and Markov games with softmax\nparametrized policies in the mean-field regime.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u76f8\u5bf9\u71b5\uff08KL\u6563\u5ea6\uff09\u6b63\u5219\u5316\u7684\u6982\u7387\u6d4b\u5ea6\u7a7a\u95f4\u4e0a\u7684\u975e\u51f8\u6cdb\u51fd\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u4ee5\u53ca\u71b5\u6b63\u5219\u5316\u7684\u975e\u51f8-\u975e\u51f9\u6781\u5c0f\u6781\u5927\u95ee\u9898\u7684\u89e3\u6cd5\u3002\u901a\u8fc7Best Response\u6d41\uff0c\u5206\u6790\u4e86\u5176\u6536\u655b\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u9009\u62e9\u6b63\u5219\u5316\u5668\u548c\u975e\u51f8\u6cdb\u51fd\u7684\u5173\u7cfb\u4ee5\u786e\u4fdd\u5168\u5c40\u6700\u4f18\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u71b5\u6b63\u5219\u5316\u65b9\u6cd5\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u5982\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u548c\u9a6c\u5c14\u53ef\u592b\u6e38\u620f\u7684\u7b56\u7565\u4f18\u5316\u3002", "method": "\u5229\u7528Best Response\u6d41\uff08\u4e5f\u79f0\u4e3a\u865a\u6784\u535a\u5f08\u6d41\uff09\uff0c\u7814\u7a76\u5176\u6536\u655b\u6027\u4e0e\u975e\u51f8\u6027\u3001\u6b63\u5219\u5316\u53c2\u6570\u53ca\u53c2\u8003\u6d4b\u5ea6\u5c3e\u90e8\u884c\u4e3a\u7684\u5173\u7cfb\u3002\u901a\u8fc7\u9009\u62e9\u7279\u5b9a\u7684\u6b63\u5219\u5316\u5668\uff0c\u4f7f\u5f97Best Response\u7b97\u5b50\u6210\u4e3a\u5173\u4e8e$L^1$-Wasserstein\u8ddd\u79bb\u7684\u538b\u7f29\u6620\u5c04\u3002", "result": "\u8bc1\u660e\u4e86\u901a\u8fc7\u7279\u5b9a\u9009\u62e9\u7684\u6b63\u5219\u5316\u5668\uff0cBest Response\u7b97\u5b50\u5b58\u5728\u552f\u4e00\u4e0d\u52a8\u70b9\uff0c\u4e14\u8be5\u70b9\u662f\u4f18\u5316\u95ee\u9898\u7684\u5168\u5c40\u6700\u5c0f\u5316\u70b9\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u7279\u5b9a\u6b63\u5219\u5316\u5668\u7684\u9009\u62e9\uff0c\u5c06Best Response\u6d41\u5e94\u7528\u4e8e\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u4e86\u5176\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002", "keywords": "\u975e\u51f8\u4f18\u5316, \u71b5\u6b63\u5219\u5316, Best Response\u6d41, \u5f3a\u5316\u5b66\u4e60, \u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b"}}
{"id": "2505.23643", "pdf": "https://arxiv.org/pdf/2505.23643", "abs": "https://arxiv.org/abs/2505.23643", "authors": ["Manuel Costa", "Boris K\u00f6pf", "Aashish Kolluri", "Andrew Paverd", "Mark Russinovich", "Ahmed Salem", "Shruti Tople", "Lukas Wutschitz", "Santiago Zanella-B\u00e9guelin"], "title": "Securing AI Agents with Information-Flow Control", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "As AI agents become increasingly autonomous and capable, ensuring their\nsecurity against vulnerabilities such as prompt injection becomes critical.\nThis paper explores the use of information-flow control (IFC) to provide\nsecurity guarantees for AI agents. We present a formal model to reason about\nthe security and expressiveness of agent planners. Using this model, we\ncharacterize the class of properties enforceable by dynamic taint-tracking and\nconstruct a taxonomy of tasks to evaluate security and utility trade-offs of\nplanner designs. Informed by this exploration, we present Fides, a planner that\ntracks confidentiality and integrity labels, deterministically enforces\nsecurity policies, and introduces novel primitives for selectively hiding\ninformation. Its evaluation in AgentDojo demonstrates that this approach\nbroadens the range of tasks that can be securely accomplished. A tutorial to\nwalk readers through the the concepts introduced in the paper can be found at\nhttps://github.com/microsoft/fides", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u4fe1\u606f\u6d41\u63a7\u5236\uff08IFC\uff09\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u5b89\u5168\u4fdd\u969c\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f62\u5f0f\u5316\u6a21\u578b\u6765\u8bc4\u4f30\u4ee3\u7406\u8ba1\u5212\u5668\u7684\u5b89\u5168\u6027\u548c\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86Fides\u8ba1\u5212\u5668\uff0c\u5176\u8bc4\u4f30\u8868\u660e\u80fd\u5b89\u5168\u5b8c\u6210\u66f4\u591a\u4efb\u52a1\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u7684\u81ea\u4e3b\u6027\u548c\u80fd\u529b\u589e\u5f3a\uff0c\u786e\u4fdd\u5176\u514d\u53d7\u63d0\u793a\u6ce8\u5165\u7b49\u6f0f\u6d1e\u653b\u51fb\u7684\u5b89\u5168\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u7528\u4e8e\u52a8\u6001\u6c61\u70b9\u8ddf\u8e2a\u7684\u53ef\u6267\u884c\u7279\u6027\u5206\u7c7b\uff0c\u5e76\u6784\u5efa\u4efb\u52a1\u5206\u7c7b\u4ee5\u8bc4\u4f30\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u7684\u6743\u8861\u3002\u8bbe\u8ba1\u4e86Fides\u8ba1\u5212\u5668\uff0c\u8ddf\u8e2a\u673a\u5bc6\u6027\u548c\u5b8c\u6574\u6027\u6807\u7b7e\uff0c\u5f3a\u5236\u6267\u884c\u5b89\u5168\u7b56\u7565\u3002", "result": "Fides\u5728AgentDojo\u4e2d\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u53ef\u5b89\u5168\u5b8c\u6210\u7684\u4efb\u52a1\u8303\u56f4\u3002", "conclusion": "\u4fe1\u606f\u6d41\u63a7\u5236\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b89\u5168\u4fdd\u969c\uff0cFides\u8ba1\u5212\u5668\u901a\u8fc7\u65b0\u9896\u7684\u539f\u8bed\u5b9e\u73b0\u4e86\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u7684\u5e73\u8861\u3002", "keywords": "AI\u4ee3\u7406\uff0c\u4fe1\u606f\u6d41\u63a7\u5236\uff0c\u52a8\u6001\u6c61\u70b9\u8ddf\u8e2a\uff0c\u5b89\u5168\u6027\uff0cFides\u8ba1\u5212\u5668"}}
{"id": "2505.23655", "pdf": "https://arxiv.org/pdf/2505.23655", "abs": "https://arxiv.org/abs/2505.23655", "authors": ["Peter David Fagan"], "title": "Keyed Chaotic Tensor Transformations for Secure And Attributable Neural Inference", "categories": ["cs.CR", "cs.AI", "94A60, 37N25, 68T05", "D.4.6"], "comment": "8 pages", "summary": "This work introduces a novel framework for secure and privacy-preserving\nneural network inference based on keyed chaotic dynamical transformations. The\nproposed method applies a deterministic, cryptographically seeded chaotic\nsystem to tensors, producing non-invertible, user-specific transformations that\nenable authenticated inference, tensor-level watermarking, and data\nattribution. This framework offers a scalable and lightweight alternative to\nconventional cryptographic techniques, and establishes a new direction for\ntensor-level security in AI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bc6\u94a5\u6df7\u6c8c\u52a8\u529b\u53d8\u6362\u7684\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u6846\u67b6\uff0c\u5b9e\u73b0\u5b89\u5168\u9690\u79c1\u4fdd\u62a4\uff0c\u652f\u6301\u8ba4\u8bc1\u63a8\u7406\u3001\u5f20\u91cf\u6c34\u5370\u548c\u6570\u636e\u5f52\u5c5e\uff0c\u662f\u4f20\u7edf\u52a0\u5bc6\u6280\u672f\u7684\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u52a0\u5bc6\u6280\u672f\u5728\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u4e2d\u7684\u6548\u7387\u548c\u6269\u5c55\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u6df7\u6c8c\u7cfb\u7edf\u4e3aAI\u7cfb\u7edf\u63d0\u4f9b\u5f20\u91cf\u7ea7\u522b\u7684\u5b89\u5168\u4fdd\u969c\u3002", "method": "\u4f7f\u7528\u786e\u5b9a\u6027\u3001\u5bc6\u7801\u5b66\u79cd\u5b50\u7684\u6df7\u6c8c\u7cfb\u7edf\u5bf9\u5f20\u91cf\u8fdb\u884c\u4e0d\u53ef\u9006\u53d8\u6362\uff0c\u652f\u6301\u7528\u6237\u7279\u5b9a\u64cd\u4f5c\u3002", "result": "\u5b9e\u73b0\u4e86\u8ba4\u8bc1\u63a8\u7406\u3001\u5f20\u91cf\u6c34\u5370\u548c\u6570\u636e\u5f52\u5c5e\u529f\u80fd\uff0c\u6846\u67b6\u8f7b\u91cf\u4e14\u53ef\u6269\u5c55\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u7cfb\u7edf\u5f20\u91cf\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u662f\u4f20\u7edf\u52a0\u5bc6\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "keywords": "\u6df7\u6c8c\u7cfb\u7edf\u3001\u9690\u79c1\u4fdd\u62a4\u3001\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u3001\u5f20\u91cf\u5b89\u5168\u3001\u6c34\u5370"}}
{"id": "2505.22781", "pdf": "https://arxiv.org/pdf/2505.22781", "abs": "https://arxiv.org/abs/2505.22781", "authors": ["Antonio Ocello", "Daniil Tiapkin", "Lorenzo Mancini", "Mathieu Lauri\u00e8re", "Eric Moulines"], "title": "Finite-Sample Convergence Bounds for Trust Region Policy Optimization in Mean-Field Games", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "We introduce Mean-Field Trust Region Policy Optimization (MF-TRPO), a novel\nalgorithm designed to compute approximate Nash equilibria for ergodic\nMean-Field Games (MFG) in finite state-action spaces. Building on the\nwell-established performance of TRPO in the reinforcement learning (RL)\nsetting, we extend its methodology to the MFG framework, leveraging its\nstability and robustness in policy optimization. Under standard assumptions in\nthe MFG literature, we provide a rigorous analysis of MF-TRPO, establishing\ntheoretical guarantees on its convergence. Our results cover both the exact\nformulation of the algorithm and its sample-based counterpart, where we derive\nhigh-probability guarantees and finite sample complexity. This work advances\nMFG optimization by bridging RL techniques with mean-field decision-making,\noffering a theoretically grounded approach to solving complex multi-agent\nproblems.", "AI": {"tldr": "MF-TRPO\u662f\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u6709\u9650\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u5747\u503c\u573a\u535a\u5f08\u7684\u8fd1\u4f3c\u7eb3\u4ec0\u5747\u8861\uff0c\u7ed3\u5408\u4e86TRPO\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u3002", "motivation": "\u5c06\u5f3a\u5316\u5b66\u4e60\u4e2dTRPO\u7684\u6210\u529f\u6269\u5c55\u5230\u5747\u503c\u573a\u535a\u5f08\u6846\u67b6\uff0c\u5229\u7528\u5176\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\uff0c\u586b\u8865\u591a\u667a\u80fd\u4f53\u590d\u6742\u95ee\u9898\u6c42\u89e3\u7684\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u6269\u5c55TRPO\u65b9\u6cd5\u81f3\u5747\u503c\u573a\u535a\u5f08\u6846\u67b6\uff0c\u5206\u6790\u5176\u6536\u655b\u6027\u5e76\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\uff0c\u5305\u62ec\u7cbe\u786e\u5f62\u5f0f\u548c\u6837\u672c\u57fa\u5f62\u5f0f\u7684\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86MF-TRPO\u7684\u6536\u655b\u6027\uff0c\u5e76\u9488\u5bf9\u6837\u672c\u57fa\u5f62\u5f0f\u63d0\u4f9b\u4e86\u9ad8\u6982\u7387\u4fdd\u8bc1\u548c\u6709\u9650\u6837\u672c\u590d\u6742\u5ea6\u3002", "conclusion": "MF-TRPO\u901a\u8fc7\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u548c\u5747\u503c\u573a\u51b3\u7b56\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u591a\u667a\u80fd\u4f53\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "keywords": "\u5747\u503c\u573a\u535a\u5f08\uff0c\u5f3a\u5316\u5b66\u4e60\uff0c\u7b56\u7565\u4f18\u5316\uff0c\u7eb3\u4ec0\u5747\u8861\uff0c\u6536\u655b\u6027\u5206\u6790"}}
{"id": "2505.22783", "pdf": "https://arxiv.org/pdf/2505.22783", "abs": "https://arxiv.org/abs/2505.22783", "authors": ["Charles E. Thornton", "Jamie Sloop", "Samuel Brown", "Aaron Orndorff", "William C. Headley", "Stephen Young"], "title": "Temporal Convolutional Autoencoder for Interference Mitigation in FMCW Radar Altimeters", "categories": ["eess.SP", "cs.LG"], "comment": "10 pages, 10 figures", "summary": "We investigate the end-to-end altitude estimation performance of a\nconvolutional autoencoder-based interference mitigation approach for\nfrequency-modulated continuous-wave (FMCW) radar altimeters. Specifically, we\nshow that a Temporal Convolutional Network (TCN) autoencoder effectively\nexploits temporal correlations in the received signal, providing superior\ninterference suppression compared to a Least Mean Squares (LMS) adaptive\nfilter. Unlike existing approaches, the present method operates directly on the\nreceived FMCW signal. Additionally, we identify key challenges in applying deep\nlearning to wideband FMCW interference mitigation and outline directions for\nfuture research to enhance real-time feasibility and generalization to\narbitrary interference conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8eFMCW\u96f7\u8fbe\u9ad8\u5ea6\u8ba1\u5e72\u6270\u6291\u5236\u7684\u5377\u79ef\u81ea\u7f16\u7801\u5668\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\uff08TCN\uff09\u5229\u7528\u4fe1\u53f7\u7684\u65f6\u5e8f\u76f8\u5173\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u6700\u5c0f\u5747\u65b9\uff08LMS\uff09\u81ea\u9002\u5e94\u6ee4\u6ce2\u5668\u3002\u8be5\u65b9\u6cd5\u76f4\u63a5\u5904\u7406\u63a5\u6536\u4fe1\u53f7\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u7814\u7a76FMCW\u96f7\u8fbe\u9ad8\u5ea6\u8ba1\u7684\u7aef\u5230\u7aef\u9ad8\u5ea6\u4f30\u8ba1\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5e72\u6270\u6291\u5236\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5982LMS\u6ee4\u6ce2\u5668\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\uff08TCN\uff09\u81ea\u7f16\u7801\u5668\u7684\u65b9\u6cd5\uff0c\u76f4\u63a5\u5904\u7406\u63a5\u6536\u5230\u7684FMCW\u4fe1\u53f7\uff0c\u5229\u7528\u65f6\u95f4\u76f8\u5173\u6027\u8fdb\u884c\u5e72\u6270\u6291\u5236\u3002", "result": "TCN\u81ea\u7f16\u7801\u5668\u7684\u5e72\u6270\u6291\u5236\u6548\u679c\u4f18\u4e8eLMS\u6ee4\u6ce2\u5668\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728FMCW\u96f7\u8fbe\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u5728FMCW\u5e72\u6270\u6291\u5236\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u89e3\u51b3\u5b9e\u65f6\u6027\u548c\u6cdb\u5316\u6027\u95ee\u9898\uff0c\u672a\u6765\u7814\u7a76\u9700\u805a\u7126\u4e8e\u6b64\u3002", "keywords": "FMCW\u96f7\u8fbe\u3001\u81ea\u7f16\u7801\u5668\u3001\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\u3001\u5e72\u6270\u6291\u5236\u3001\u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2505.22805", "pdf": "https://arxiv.org/pdf/2505.22805", "abs": "https://arxiv.org/abs/2505.22805", "authors": ["Siddharth Ancha", "Sunshine Jiang", "Travis Manderson", "Laura Brandt", "Yilun Du", "Philip R. Osteen", "Nicholas Roy"], "title": "Anomalies by Synthesis: Anomaly Detection using Generative Diffusion Models for Off-Road Navigation", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Presented at ICRA 2025", "summary": "In order to navigate safely and reliably in off-road and unstructured\nenvironments, robots must detect anomalies that are out-of-distribution (OOD)\nwith respect to the training data. We present an analysis-by-synthesis approach\nfor pixel-wise anomaly detection without making any assumptions about the\nnature of OOD data. Given an input image, we use a generative diffusion model\nto synthesize an edited image that removes anomalies while keeping the\nremaining image unchanged. Then, we formulate anomaly detection as analyzing\nwhich image segments were modified by the diffusion model. We propose a novel\ninference approach for guided diffusion by analyzing the ideal guidance\ngradient and deriving a principled approximation that bootstraps the diffusion\nmodel to predict guidance gradients. Our editing technique is purely test-time\nthat can be integrated into existing workflows without the need for retraining\nor fine-tuning. Finally, we use a combination of vision-language foundation\nmodels to compare pixels in a learned feature space and detect semantically\nmeaningful edits, enabling accurate anomaly detection for off-road navigation.\nProject website: https://siddancha.github.io/anomalies-by-diffusion-synthesis/", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u6790-\u5408\u6210\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u751f\u6210\u6269\u6563\u6a21\u578b\u68c0\u6d4b\u56fe\u50cf\u4e2d\u7684\u5f02\u5e38\u533a\u57df\uff0c\u65e0\u9700\u5bf9\u5f02\u5e38\u6570\u636e\u7684\u6027\u8d28\u505a\u4efb\u4f55\u5047\u8bbe\uff0c\u9002\u7528\u4e8e\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u5bfc\u822a\u3002", "motivation": "\u4e3a\u4e86\u5728\u975e\u7ed3\u6784\u5316\u548c\u8d8a\u91ce\u73af\u5883\u4e2d\u5b89\u5168\u53ef\u9760\u5730\u5bfc\u822a\uff0c\u673a\u5668\u4eba\u9700\u8981\u68c0\u6d4b\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4e0d\u7b26\u7684\u5f02\u5e38\u70b9\u3002", "method": "\u5229\u7528\u751f\u6210\u6269\u6563\u6a21\u578b\u5bf9\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u7f16\u8f91\uff0c\u79fb\u9664\u5f02\u5e38\u533a\u57df\u5e76\u4fdd\u6301\u5176\u4ed6\u90e8\u5206\u4e0d\u53d8\uff1b\u901a\u8fc7\u5206\u6790\u6269\u6563\u6a21\u578b\u7684\u4fee\u6539\u90e8\u5206\u5b9e\u73b0\u5f02\u5e38\u68c0\u6d4b\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u5206\u6790\u7406\u60f3\u68af\u5ea6\u5e76\u63a8\u5bfc\u8fd1\u4f3c\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u6bd4\u8f83\u50cf\u7d20\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u8bed\u4e49\u5f02\u5e38\u68c0\u6d4b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8d8a\u91ce\u73af\u5883\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "keywords": "\u5f02\u5e38\u68c0\u6d4b, \u6269\u6563\u6a21\u578b, \u673a\u5668\u5bfc\u822a, \u975e\u7ed3\u6784\u5316\u73af\u5883, \u89c6\u89c9-\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.22807", "pdf": "https://arxiv.org/pdf/2505.22807", "abs": "https://arxiv.org/abs/2505.22807", "authors": ["John C. Duchi"], "title": "Distribution free M-estimation", "categories": ["math.ST", "cs.IT", "cs.LG", "math.IT", "stat.TH"], "comment": "26 pages", "summary": "The basic question of delineating those statistical problems that are\nsolvable without making any assumptions on the underlying data distribution has\nlong animated statistics and learning theory. This paper characterizes when a\n(univariate) convex M-estimation or stochastic optimization problem is solvable\nin such an assumption-free setting, providing a precise dividing line between\nsolvable and unsolvable problems. The conditions we identify show, perhaps\nsurprisingly, that Lipschitz continuity of the loss being minimized is not\nnecessary for distribution free minimization, and they are also distinct from\nclassical characterizations of learnability in machine learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5728\u65e0\u9700\u4efb\u4f55\u6570\u636e\u5206\u5e03\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\uff0c\u51f8M\u4f30\u8ba1\u6216\u968f\u673a\u4f18\u5316\u95ee\u9898\u53ef\u89e3\u7684\u7cbe\u786e\u6761\u4ef6\u3002", "motivation": "\u63a2\u8ba8\u65e0\u6570\u636e\u5206\u5e03\u5047\u8bbe\u7684\u7edf\u8ba1\u95ee\u9898\u53ef\u89e3\u6027\uff0c\u4e3a\u7edf\u8ba1\u5b66\u548c\u5b66\u4e60\u7406\u8bba\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u51f8M\u4f30\u8ba1\u6216\u968f\u673a\u4f18\u5316\u95ee\u9898\u7684\u5206\u6790\uff0c\u786e\u5b9a\u53ef\u89e3\u4e0e\u4e0d\u53ef\u89e3\u95ee\u9898\u7684\u5206\u754c\u7ebf\u3002", "result": "\u53d1\u73b0Lipshitz\u8fde\u7eed\u5e76\u975e\u5206\u5e03\u81ea\u7531\u4f18\u5316\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u4e14\u4e0d\u540c\u4e8e\u673a\u5668\u5b66\u4e60\u4e2d\u7ecf\u5178\u7684\u53ef\u5b66\u4e60\u6027\u6761\u4ef6\u3002", "conclusion": "\u63d0\u51fa\u4e86\u65e0\u5206\u5e03\u5047\u8bbe\u4e0b\u7684\u53ef\u89e3\u6027\u6761\u4ef6\uff0c\u6269\u5c55\u4e86\u6b64\u7c7b\u95ee\u9898\u7684\u7406\u8bba\u8fb9\u754c\u3002", "keywords": "\u5206\u5e03\u81ea\u7531\u4f18\u5316,\u51f8M\u4f30\u8ba1,\u968f\u673a\u4f18\u5316,\u53ef\u5b66\u4e60\u6027,Lipschitz\u8fde\u7eed"}}
{"id": "2505.23704", "pdf": "https://arxiv.org/pdf/2505.23704", "abs": "https://arxiv.org/abs/2505.23704", "authors": ["Mohamad Alansari", "Sajid Javed", "Iyyakutti Iyappan Ganapathi", "Sara Alansari", "Muzammal Naseer"], "title": "CLDTracker: A Comprehensive Language Description for Visual Tracking", "categories": ["cs.CV", "cs.AI"], "comment": "47 pages, 9 figures, Information Fusion Journal", "summary": "VOT remains a fundamental yet challenging task in computer vision due to\ndynamic appearance changes, occlusions, and background clutter. Traditional\ntrackers, relying primarily on visual cues, often struggle in such complex\nscenarios. Recent advancements in VLMs have shown promise in semantic\nunderstanding for tasks like open-vocabulary detection and image captioning,\nsuggesting their potential for VOT. However, the direct application of VLMs to\nVOT is hindered by critical limitations: the absence of a rich and\ncomprehensive textual representation that semantically captures the target\nobject's nuances, limiting the effective use of language information;\ninefficient fusion mechanisms that fail to optimally integrate visual and\ntextual features, preventing a holistic understanding of the target; and a lack\nof temporal modeling of the target's evolving appearance in the language\ndomain, leading to a disconnect between the initial description and the\nobject's subsequent visual changes. To bridge these gaps and unlock the full\npotential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive\nLanguage Description framework for robust visual Tracking. Our tracker\nintroduces a dual-branch architecture consisting of a textual and a visual\nbranch. In the textual branch, we construct a rich bag of textual descriptions\nderived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with\nsemantic and contextual cues to address the lack of rich textual\nrepresentation. Experiments on six standard VOT benchmarks demonstrate that\nCLDTracker achieves SOTA performance, validating the effectiveness of\nleveraging robust and temporally-adaptive vision-language representations for\ntracking. Code and models are publicly available at:\nhttps://github.com/HamadYA/CLDTracker", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5168\u9762\u8bed\u8a00\u63cf\u8ff0\u7684\u89c6\u89c9\u8ffd\u8e2a\u6846\u67b6CLDTracker\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u7684\u53cc\u5206\u652f\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8ffd\u8e2a\u5668\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u516d\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8ffd\u8e2a\u5668\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u7ebf\u7d22\uff0c\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u5916\u89c2\u53d8\u5316\u3001\u906e\u6321\u548c\u80cc\u666f\u5e72\u6270\u7b49\u590d\u6742\u573a\u666f\u3002\u8fd1\u671f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u8fdb\u5c55\u5c55\u793a\u4e86\u5176\u5728\u7406\u89e3\u8bed\u4e49\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u89c6\u89c9\u8ffd\u8e2a\uff08VOT\uff09\u4ecd\u5b58\u5728\u7f3a\u4e4f\u4e30\u5bcc\u6587\u672c\u8868\u793a\u3001\u89c6\u89c9\u4e0e\u6587\u672c\u7279\u5f81\u878d\u5408\u6548\u7387\u4f4e\u4ee5\u53ca\u5bf9\u76ee\u6807\u5916\u89c2\u7684\u65f6\u95f4\u6f14\u5316\u5efa\u6a21\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86CLDTracker\uff0c\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\uff08\u6587\u672c\u5206\u652f\u548c\u89c6\u89c9\u5206\u652f\uff09\uff0c\u5176\u4e2d\u6587\u672c\u5206\u652f\u5229\u7528CLIP\u548cGPT-4V\u7b49\u5f3a\u5927\u7684VLM\u751f\u6210\u4e30\u5bcc\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u7ed3\u5408\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ee5\u5f25\u8865\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "result": "\u5728\u516d\u4e2a\u6807\u51c6VOT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86SOTA\uff08State-of-the-Art\uff09\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u8868\u793a\u7684\u8ffd\u8e2a\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "CLDTracker\u901a\u8fc7\u6709\u6548\u878d\u5408\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u76ee\u6807\u5916\u89c2\u7684\u65f6\u95f4\u6f14\u5316\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u89c6\u89c9\u8ffd\u8e2a\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "keywords": "\u89c6\u89c9\u76ee\u6807\u8ffd\u8e2a\uff08VOT\uff09\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u3001CLDTracker\u3001\u53cc\u5206\u652f\u67b6\u6784\u3001\u8bed\u4e49\u7406\u89e3"}}
{"id": "2505.22811", "pdf": "https://arxiv.org/pdf/2505.22811", "abs": "https://arxiv.org/abs/2505.22811", "authors": ["Ba-Hien Tran", "Van Minh Nguyen"], "title": "Highly Efficient and Effective LLMs with Multi-Boolean Architectures", "categories": ["stat.ML", "cs.LG"], "comment": "Under Review", "summary": "Weight binarization has emerged as a promising strategy to drastically reduce\nthe complexity of large language models (LLMs). It is mainly classified into\ntwo approaches: post-training binarization and finetuning with training-aware\nbinarization methods. The first approach, while having low complexity, leads to\nsignificant loss of information from the original LLMs, resulting in poor\nperformance. The second approach, on the other hand, relies heavily on\nfull-precision latent weights for gradient approximation of binary weights,\nwhich not only remains suboptimal but also introduces substantial complexity.\nIn this paper, we introduce a novel framework that effectively transforms LLMs\ninto multi-kernel Boolean parameters, for the first time, finetunes them\ndirectly in the Boolean domain, eliminating the need for expensive latent\nweights. This significantly reduces complexity during both finetuning and\ninference. Through extensive and insightful experiments across a wide range of\nLLMs, we demonstrate that our method outperforms recent ultra low-bit\nquantization and binarization methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f6c\u6362\u4e3a\u591a\u6838\u5e03\u5c14\u53c2\u6570\uff0c\u5e76\u5728\u5e03\u5c14\u57df\u76f4\u63a5\u5fae\u8c03\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5fae\u8c03\u548c\u63a8\u7406\u7684\u590d\u6742\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6743\u91cd\u4e8c\u503c\u5316\u65b9\u6cd5\uff08\u540e\u8bad\u7ec3\u4e8c\u503c\u5316\u548c\u8bad\u7ec3\u611f\u77e5\u4e8c\u503c\u5316\uff09\u8981\u4e48\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\uff0c\u8981\u4e48\u4f9d\u8d56\u9ad8\u590d\u6742\u5ea6\u7684\u5168\u7cbe\u5ea6\u6f5c\u5728\u6743\u91cd\uff0c\u63d0\u51fa\u65b0\u65b9\u6cd5\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06LLM\u8f6c\u6362\u4e3a\u591a\u6838\u5e03\u5c14\u53c2\u6570\u5e76\u76f4\u63a5\u5728\u5e03\u5c14\u57df\u8fdb\u884c\u5fae\u8c03\uff0c\u907f\u514d\u4e86\u6602\u8d35\u6f5c\u5728\u6743\u91cd\u7684\u4f7f\u7528\u3002", "result": "\u5728\u591a\u79cdLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u8fd1\u671f\u8d85\u4f4e\u4f4d\u91cf\u5316\u548c\u4e8c\u503c\u5316\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\u5e03\u5c14\u57df\u76f4\u63a5\u5fae\u8c03\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u590d\u6742\u6027\u548c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u6743\u91cd\u4e8c\u503c\u5316\u3001\u5e03\u5c14\u53c2\u6570\u3001\u5fae\u8c03\u3001\u590d\u6742\u6027\u964d\u4f4e"}}
{"id": "2505.23706", "pdf": "https://arxiv.org/pdf/2505.23706", "abs": "https://arxiv.org/abs/2505.23706", "authors": ["Utku Demir", "Yalin E. Sagduyu", "Tugba Erpek", "Hossein Jafari", "Sastry Kompella", "Mengran Xue"], "title": "Distributed Federated Learning for Vehicular Network Security: Anomaly Detection Benefits and Multi-Domain Attack Threats", "categories": ["cs.NI", "cs.AI", "cs.DC", "cs.IT", "eess.SP", "math.IT"], "comment": null, "summary": "In connected and autonomous vehicles, machine learning for safety message\nclassification has become critical for detecting malicious or anomalous\nbehavior. However, conventional approaches that rely on centralized data\ncollection or purely local training face limitations due to the large scale,\nhigh mobility, and heterogeneous data distributions inherent in inter-vehicle\nnetworks. To overcome these challenges, this paper explores Distributed\nFederated Learning (DFL), whereby vehicles collaboratively train deep learning\nmodels by exchanging model updates among one-hop neighbors and propagating\nmodels over multiple hops. Using the Vehicular Reference Misbehavior (VeReMi)\nExtension Dataset, we show that DFL can significantly improve classification\naccuracy across all vehicles compared to learning strictly with local data.\nNotably, vehicles with low individual accuracy see substantial accuracy gains\nthrough DFL, illustrating the benefit of knowledge sharing across the network.\nWe further show that local training data size and time-varying network\nconnectivity correlate strongly with the model's overall accuracy. We\ninvestigate DFL's resilience and vulnerabilities under attacks in multiple\ndomains, namely wireless jamming and training data poisoning attacks. Our\nresults reveal important insights into the vulnerabilities of DFL when\nconfronted with multi-domain attacks, underlining the need for more robust\nstrategies to secure DFL in vehicular networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5206\u5e03\u5f0f\u8054\u90a6\u5b66\u4e60\uff08DFL\uff09\u5728\u8f66\u8054\u7f51\u4e2d\u7528\u4e8e\u5b89\u5168\u6d88\u606f\u5206\u7c7b\u7684\u6548\u679c\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0cDFL\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u5e76\u589e\u5f3a\u4e86\u77e5\u8bc6\u5171\u4eab\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u5176\u5728\u591a\u57df\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u89e3\u51b3\u8f66\u8054\u7f51\u4e2d\u56e0\u89c4\u6a21\u5927\u3001\u9ad8\u79fb\u52a8\u6027\u548c\u5f02\u6784\u6570\u636e\u5206\u5e03\u5bfc\u81f4\u7684\u96c6\u4e2d\u5f0f\u6216\u7eaf\u672c\u5730\u5b66\u4e60\u65b9\u6cd5\u5c40\u9650\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u8054\u90a6\u5b66\u4e60\uff08DFL\uff09\uff0c\u8f66\u8f86\u901a\u8fc7\u4e00\u8df3\u90bb\u5c45\u4ea4\u6362\u6a21\u578b\u66f4\u65b0\u5e76\u5728\u591a\u8df3\u4e2d\u4f20\u64ad\u6a21\u578b\u8fdb\u884c\u534f\u4f5c\u8bad\u7ec3\u3002", "result": "DFL\u663e\u8457\u63d0\u5347\u4e86\u6240\u6709\u8f66\u8f86\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5c24\u5176\u662f\u4f4e\u51c6\u786e\u7387\u8f66\u8f86\uff1b\u540c\u65f6\u63ed\u793a\u4e86\u6570\u636e\u5927\u5c0f\u548c\u7f51\u7edc\u52a8\u6001\u6027\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u53ca\u5176\u5728\u591a\u57df\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\u3002", "conclusion": "DFL\u5728\u8f66\u8054\u7f51\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u66f4\u9c81\u68d2\u7684\u7b56\u7565\u4ee5\u5e94\u5bf9\u591a\u57df\u653b\u51fb\u3002", "keywords": "\u5206\u5e03\u5f0f\u8054\u90a6\u5b66\u4e60, \u8f66\u8054\u7f51, \u5b89\u5168\u6d88\u606f\u5206\u7c7b, \u591a\u57df\u653b\u51fb, \u6a21\u578b\u66f4\u65b0"}}
{"id": "2505.23709", "pdf": "https://arxiv.org/pdf/2505.23709", "abs": "https://arxiv.org/abs/2505.23709", "authors": ["Dionysis Christopoulos", "Sotiris Spanos", "Eirini Baltzi", "Valsamis Ntouskos", "Konstantinos Karantzalos"], "title": "Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce SLIMP (Skin Lesion Image-Metadata Pre-training) for learning\nrich representations of skin lesions through a novel nested contrastive\nlearning approach that captures complex relationships between images and\nmetadata. Melanoma detection and skin lesion classification based solely on\nimages, pose significant challenges due to large variations in imaging\nconditions (lighting, color, resolution, distance, etc.) and lack of clinical\nand phenotypical context. Clinicians typically follow a holistic approach for\nassessing the risk level of the patient and for deciding which lesions may be\nmalignant and need to be excised, by considering the patient's medical history\nas well as the appearance of other lesions of the patient. Inspired by this,\nSLIMP combines the appearance and the metadata of individual skin lesions with\npatient-level metadata relating to their medical record and other clinically\nrelevant information. By fully exploiting all available data modalities\nthroughout the learning process, the proposed pre-training strategy improves\nperformance compared to other pre-training strategies on downstream skin\nlesions classification tasks highlighting the learned representations quality.", "AI": {"tldr": "SLIMP\u901a\u8fc7\u5d4c\u5957\u5bf9\u6bd4\u5b66\u4e60\u7ed3\u5408\u76ae\u80a4\u75c5\u53d8\u56fe\u50cf\u548c\u5143\u6570\u636e\uff0c\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4ec5\u4f9d\u8d56\u56fe\u50cf\u7684\u76ae\u80a4\u75c5\u53d8\u5206\u7c7b\u6311\u6218\uff0c\u6a21\u4eff\u533b\u751f\u7efc\u5408\u8003\u8651\u60a3\u8005\u75c5\u53f2\u548c\u75c5\u53d8\u5916\u89c2\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5d4c\u5957\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u7ed3\u5408\u56fe\u50cf\u3001\u4e2a\u4f53\u75c5\u53d8\u5143\u6570\u636e\u548c\u60a3\u8005\u7ea7\u5143\u6570\u636e\u3002", "result": "\u76f8\u6bd4\u5176\u4ed6\u9884\u8bad\u7ec3\u7b56\u7565\uff0cSLIMP\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "SLIMP\u901a\u8fc7\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u63d0\u5347\u8868\u5f81\u8d28\u91cf\uff0c\u6539\u5584\u5206\u7c7b\u6548\u679c\u3002", "keywords": "SLIMP, \u76ae\u80a4\u75c5\u53d8, \u5143\u6570\u636e, \u5bf9\u6bd4\u5b66\u4e60, \u5206\u7c7b"}}
{"id": "2505.23710", "pdf": "https://arxiv.org/pdf/2505.23710", "abs": "https://arxiv.org/abs/2505.23710", "authors": ["Zeinab Nezami", "Syed Danial Ali Shah", "Maryam Hafeez", "Karim Djemame", "Syed Ali Raza Zaidi"], "title": "From Connectivity to Autonomy: The Dawn of Self-Evolving Communication Systems", "categories": ["eess.SY", "cs.AI", "cs.DC", "cs.ET", "cs.SY"], "comment": null, "summary": "This paper envisions 6G as a self-evolving telecom ecosystem, where AI-driven\nintelligence enables dynamic adaptation beyond static connectivity. We explore\nthe key enablers of autonomous communication systems, spanning reconfigurable\ninfrastructure, adaptive middleware, and intelligent network functions,\nalongside multi-agent collaboration for distributed decision-making. We explore\nhow these methodologies align with emerging industrial IoT frameworks, ensuring\nseamless integration within digital manufacturing processes. Our findings\nemphasize the potential for improved real-time decision-making, optimizing\nefficiency, and reducing latency in networked control systems. The discussion\naddresses ethical challenges, research directions, and standardization efforts,\nconcluding with a technology stack roadmap to guide future developments. By\nleveraging state-of-the-art 6G network management techniques, this research\ncontributes to the next generation of intelligent automation solutions,\nbridging the gap between theoretical advancements and real-world industrial\napplications.", "AI": {"tldr": "\u672c\u6587\u5c55\u671b6G\u4f5c\u4e3a\u81ea\u6f14\u8fdb\u7684\u7535\u4fe1\u751f\u6001\u7cfb\u7edf\uff0cAI\u9a71\u52a8\u667a\u80fd\u5b9e\u73b0\u52a8\u6001\u9002\u5e94\uff0c\u63a2\u7d22\u81ea\u4e3b\u901a\u4fe1\u7cfb\u7edf\u7684\u5173\u952e\u4f7f\u80fd\u6280\u672f\uff0c\u5e76\u4e0e\u5de5\u4e1a\u7269\u8054\u7f51\u6846\u67b6\u7ed3\u5408\uff0c\u5f3a\u8c03\u5b9e\u65f6\u51b3\u7b56\u4f18\u5316\u53ca\u5ef6\u8fdf\u964d\u4f4e\u3002", "motivation": "\u63a8\u52a86G\u6280\u672f\u5411\u81ea\u6f14\u8fdb\u7535\u4fe1\u751f\u6001\u53d1\u5c55\uff0c\u5229\u7528AI\u667a\u80fd\u5b9e\u73b0\u52a8\u6001\u9002\u5e94\u6027\uff0c\u63d0\u5347\u5de5\u4e1a\u81ea\u52a8\u5316\u4e2d\u7684\u5b9e\u65f6\u51b3\u7b56\u80fd\u529b\u4e0e\u6548\u7387\u3002", "method": "\u7814\u7a76\u53ef\u91cd\u6784\u57fa\u7840\u8bbe\u65bd\u3001\u81ea\u9002\u5e94\u4e2d\u95f4\u4ef6\u3001\u667a\u80fd\u7f51\u7edc\u529f\u80fd\u53ca\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u5206\u5e03\u5f0f\u51b3\u7b56\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u6280\u672f\u80fd\u4f18\u5316\u5b9e\u65f6\u51b3\u7b56\uff0c\u63d0\u9ad8\u6548\u7387\u5e76\u964d\u4f4e\u7f51\u7edc\u63a7\u5236\u7cfb\u7edf\u7684\u5ef6\u8fdf\u3002", "conclusion": "\u901a\u8fc76G\u7f51\u7edc\u7ba1\u7406\u6280\u672f\uff0c\u672c\u7814\u7a76\u4e3a\u667a\u80fd\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u7406\u8bba\u5230\u5de5\u4e1a\u5e94\u7528\u7684\u6865\u6881\uff0c\u5e76\u63d0\u51fa\u4e86\u6280\u672f\u6808\u8def\u7ebf\u56fe\u3002", "keywords": "6G, AI\u9a71\u52a8, \u81ea\u6f14\u8fdb, \u5de5\u4e1a\u7269\u8054\u7f51, \u5b9e\u65f6\u51b3\u7b56"}}
{"id": "2505.22868", "pdf": "https://arxiv.org/pdf/2505.22868", "abs": "https://arxiv.org/abs/2505.22868", "authors": ["Md Hasibul Amin", "Mohammadreza Mohammadi", "Jason D. Bakos", "Ramtin Zand"], "title": "CrossNAS: A Cross-Layer Neural Architecture Search Framework for PIM Systems", "categories": ["cs.ET", "cs.AR", "cs.LG"], "comment": null, "summary": "In this paper, we propose the CrossNAS framework, an automated approach for\nexploring a vast, multidimensional search space that spans various design\nabstraction layers-circuits, architecture, and systems-to optimize the\ndeployment of machine learning workloads on analog processing-in-memory (PIM)\nsystems. CrossNAS leverages the single-path one-shot weight-sharing strategy\ncombined with the evolutionary search for the first time in the context of PIM\nsystem mapping and optimization. CrossNAS sets a new benchmark for PIM neural\narchitecture search (NAS), outperforming previous methods in both accuracy and\nenergy efficiency while maintaining comparable or shorter search times.", "AI": {"tldr": "CrossNAS is a novel automated framework for optimizing machine learning workloads on analog PIM systems, combining weight-sharing and evolutionary search to achieve superior accuracy and energy efficiency.", "motivation": "To address the challenge of efficiently deploying ML workloads on analog PIM systems by exploring a multidimensional search space across circuits, architecture, and systems.", "method": "Uses single-path one-shot weight-sharing strategy with evolutionary search for PIM system mapping and optimization.", "result": "Outperforms previous methods in accuracy and energy efficiency while maintaining comparable or shorter search times.", "conclusion": "CrossNAS sets a new benchmark for PIM neural architecture search.", "keywords": "CrossNAS, analog PIM, neural architecture search, evolutionary search, weight-sharing"}}
{"id": "2505.22869", "pdf": "https://arxiv.org/pdf/2505.22869", "abs": "https://arxiv.org/abs/2505.22869", "authors": ["Junbo Yin", "Chao Zha", "Wenjia He", "Chencheng Xu", "Xin Gao"], "title": "CFP-Gen: Combinatorial Functional Protein Generation via Diffusion Language Models", "categories": ["cs.CV", "cs.LG", "q-bio.BM"], "comment": "Accepted at ICML 2025. Code is available at\n  https://github.com/yinjunbo/cfpgen", "summary": "Existing PLMs generate protein sequences based on a single-condition\nconstraint from a specific modality, struggling to simultaneously satisfy\nmultiple constraints across different modalities. In this work, we introduce\nCFP-Gen, a novel diffusion language model for Combinatorial Functional Protein\nGENeration. CFP-Gen facilitates the de novo protein design by integrating\nmultimodal conditions with functional, sequence, and structural constraints.\nSpecifically, an Annotation-Guided Feature Modulation (AGFM) module is\nintroduced to dynamically adjust the protein feature distribution based on\ncomposable functional annotations, e.g., GO terms, IPR domains and EC numbers.\nMeanwhile, the Residue-Controlled Functional Encoding (RCFE) module captures\nresidue-wise interaction to ensure more precise control. Additionally,\noff-the-shelf 3D structure encoders can be seamlessly integrated to impose\ngeometric constraints. We demonstrate that CFP-Gen enables high-throughput\ngeneration of novel proteins with functionality comparable to natural proteins,\nwhile achieving a high success rate in designing multifunctional proteins. Code\nand data available at https://github.com/yinjunbo/cfpgen.", "AI": {"tldr": "\u63d0\u51fa\u4e86CFP-Gen\uff0c\u4e00\u79cd\u65b0\u578b\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u7ec4\u5408\u529f\u80fd\u6027\u86cb\u767d\u8d28\u751f\u6210\u3002\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u6761\u4ef6\u53ca\u529f\u80fd\u3001\u5e8f\u5217\u548c\u7ed3\u6784\u7ea6\u675f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709PLM\u5728\u591a\u6a21\u6001\u6761\u4ef6\u4e0b\u751f\u6210\u86cb\u767d\u8d28\u5e8f\u5217\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709PLM\u5728\u751f\u6210\u86cb\u767d\u8d28\u5e8f\u5217\u65f6\u53ea\u80fd\u5904\u7406\u5355\u4e00\u6a21\u6001\u6761\u4ef6\uff0c\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u591a\u6a21\u6001\u7684\u4e0d\u540c\u7ea6\u675f\uff0c\u5236\u7ea6\u4e86\u591a\u529f\u80fd\u86cb\u767d\u8d28\u7684\u8bbe\u8ba1\u3002", "method": "\u63d0\u51fa\u4e86CFP-Gen\u6a21\u578b\uff0c\u5305\u62ecAnnotation-Guided Feature Modulation (AGFM)\u6a21\u5757\u52a8\u6001\u8c03\u6574\u7279\u5f81\u5206\u5e03\uff0cResidue-Controlled Functional Encoding (RCFE)\u6a21\u5757\u6355\u83b7\u6b8b\u57fa\u4ea4\u4e92\uff0c\u4ee5\u53ca\u6574\u54083D\u7ed3\u6784\u7f16\u7801\u5668\u65bd\u52a0\u51e0\u4f55\u7ea6\u675f\u3002", "result": "CFP-Gen\u80fd\u591f\u9ad8\u6548\u751f\u6210\u529f\u80fd\u4e0e\u5929\u7136\u86cb\u767d\u8d28\u76f8\u5f53\u7684\u65b0\u86cb\u767d\u8d28\uff0c\u5e76\u6210\u529f\u8bbe\u8ba1\u591a\u529f\u80fd\u86cb\u767d\u8d28\u3002", "conclusion": "CFP-Gen\u4e3a\u7ec4\u5408\u6027\u591a\u6a21\u6001\u7ea6\u675f\u4e0b\u7684\u86cb\u767d\u8d28\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7cbe\u51c6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u86cb\u767d\u8d28\u751f\u6210\u3001\u6269\u6563\u6a21\u578b\u3001\u591a\u529f\u80fd\u86cb\u767d\u8d28\u3001\u591a\u6a21\u6001\u7ea6\u675f\u3001CFP-Gen"}}
{"id": "2505.22914", "pdf": "https://arxiv.org/pdf/2505.22914", "abs": "https://arxiv.org/abs/2505.22914", "authors": ["Maksim Kolodiazhnyi", "Denis Tarasov", "Dmitrii Zhemchuzhnikov", "Alexander Nikulin", "Ilya Zisman", "Anna Vorontsova", "Anton Konushin", "Vladislav Kurenkov", "Danila Rukhovich"], "title": "cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Computer-Aided Design (CAD) plays a central role in engineering and\nmanufacturing, making it possible to create precise and editable 3D models.\nUsing a variety of sensor or user-provided data as inputs for CAD\nreconstruction can democratize access to design applications. However, existing\nmethods typically focus on a single input modality, such as point clouds,\nimages, or text, which limits their generalizability and robustness. Leveraging\nrecent advances in vision-language models (VLM), we propose a multi-modal CAD\nreconstruction model that simultaneously processes all three input modalities.\nInspired by large language model (LLM) training paradigms, we adopt a two-stage\npipeline: supervised fine-tuning (SFT) on large-scale procedurally generated\ndata, followed by reinforcement learning (RL) fine-tuning using online\nfeedback, obtained programatically. Furthermore, we are the first to explore RL\nfine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such\nas Group Relative Preference Optimization (GRPO) outperform offline\nalternatives. In the DeepCAD benchmark, our SFT model outperforms existing\nsingle-modal approaches in all three input modalities simultaneously. More\nimportantly, after RL fine-tuning, cadrille sets new state-of-the-art on three\nchallenging datasets, including a real-world one.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001CAD\u91cd\u5efa\u6a21\u578b\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u540c\u65f6\u5904\u7406\u70b9\u4e91\u3001\u56fe\u50cf\u548c\u6587\u672c\u8f93\u5165\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u76d1\u7763\u5fae\u8c03+\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff09\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709CAD\u91cd\u5efa\u65b9\u6cd5\u901a\u5e38\u4ec5\u652f\u6301\u5355\u4e00\u8f93\u5165\u6a21\u6001\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\uff1b\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u8f93\u5165\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1) \u5728\u5927\u89c4\u6a21\u7a0b\u5e8f\u751f\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff1b2) \u901a\u8fc7\u5728\u7ebf\u53cd\u9988\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff08RL\uff09\uff0c\u63a2\u7d22\u4e86Group Relative Preference Optimization\uff08GRPO\uff09\u7b49\u7b97\u6cd5\u3002", "result": "\u5728DeepCAD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSFT\u6a21\u578b\u5728\u4e09\u79cd\u8f93\u5165\u6a21\u6001\u4e0a\u5747\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\uff1bRL\u5fae\u8c03\u540e\uff0c\u6a21\u578b\u5728\u4e09\u4e2a\u6570\u636e\u96c6\uff08\u5305\u62ec\u771f\u5b9e\u573a\u666f\uff09\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u591a\u6a21\u6001\u8f93\u5165\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86CAD\u91cd\u5efa\u7684\u6cdb\u5316\u6027\u548c\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "CAD\u91cd\u5efa, \u591a\u6a21\u6001, \u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u5f3a\u5316\u5b66\u4e60, Group Relative Preference Optimization"}}
{"id": "2505.23733", "pdf": "https://arxiv.org/pdf/2505.23733", "abs": "https://arxiv.org/abs/2505.23733", "authors": ["Truong", "Luu", "Binny M. Samuel"], "title": "Exposing the Impact of GenAI for Cybercrime: An Investigation into the Dark Side", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "In recent years, the rapid advancement and democratization of generative AI\nmodels have sparked significant debate over safety, ethical risks, and dual-use\nconcerns, particularly in the context of cybersecurity. While anecdotally\nknown, this paper provides empirical evidence regarding generative AI's\nassociation with malicious internet-related activities and cybercrime by\nexamining the phenomenon through psychological frameworks of technological\namplification and affordance theory. Using a quasi-experimental design with\ninterrupted time series analysis, we analyze two datasets, one general and one\ncryptocurrency-focused, to empirically assess generative AI's role in\ncybercrime. The findings contribute to ongoing discussions about AI governance\nby balancing control and fostering innovation, underscoring the need for\nstrategies to guide policymakers, inform AI developers and cybersecurity\nprofessionals, and educate the public to maximize AI's benefits while\nmitigating its risks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u751f\u6210\u5f0fAI\u4e0e\u7f51\u7edc\u72af\u7f6a\u7684\u5173\u8054\uff0c\u901a\u8fc7\u5fc3\u7406\u5b66\u6846\u67b6\u548c\u51c6\u5b9e\u9a8c\u8bbe\u8ba1\u63d0\u4f9b\u5b9e\u8bc1\u8bc1\u636e\uff0c\u5f3a\u8c03AI\u6cbb\u7406\u9700\u5e73\u8861\u63a7\u5236\u4e0e\u521b\u65b0\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u6f5c\u5728\u98ce\u9669\u548c\u53cc\u7528\u9014\u95ee\u9898\uff0c\u586b\u8865\u76f8\u5173\u5b9e\u8bc1\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u51c6\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u4e2d\u65ad\u65f6\u95f4\u5e8f\u5217\u5206\u6790\uff0c\u5206\u6790\u4e24\u4e2a\u6570\u636e\u96c6\uff08\u901a\u7528\u548c\u52a0\u5bc6\u8d27\u5e01\u76f8\u5173\uff09\u3002", "result": "\u5b9e\u8bc1\u8868\u660e\u751f\u6210\u5f0fAI\u4e0e\u6076\u610f\u7f51\u7edc\u6d3b\u52a8\u76f8\u5173\uff0c\u4e3aAI\u6cbb\u7406\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\u3002", "conclusion": "\u9700\u5236\u5b9a\u7b56\u7565\u6307\u5bfc\u653f\u7b56\u5236\u5b9a\u8005\u3001\u5f00\u53d1\u8005\u548c\u516c\u4f17\uff0c\u4ee5\u6700\u5927\u5316AI\u6548\u76ca\u5e76\u964d\u4f4e\u98ce\u9669\u3002", "keywords": "\u751f\u6210\u5f0fAI\u3001\u7f51\u7edc\u5b89\u5168\u3001\u5b9e\u8bc1\u7814\u7a76\u3001AI\u6cbb\u7406\u3001\u5fc3\u7406\u6846\u67b6"}}
{"id": "2505.23742", "pdf": "https://arxiv.org/pdf/2505.23742", "abs": "https://arxiv.org/abs/2505.23742", "authors": ["Yufan Deng", "Xun Guo", "Yuanyang Yin", "Jacob Zhiyuan Fang", "Yiding Yang", "Yizhi Wang", "Shenghai Yuan", "Angtian Wang", "Bo Liu", "Haibin Huang", "Chongyang Ma"], "title": "MAGREF: Masked Guidance for Any-Reference Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Project website: https://magref-video.github.io/magref.github.io/", "summary": "Video generation has made substantial strides with the emergence of deep\ngenerative models, especially diffusion-based approaches. However, video\ngeneration based on multiple reference subjects still faces significant\nchallenges in maintaining multi-subject consistency and ensuring high\ngeneration quality. In this paper, we propose MAGREF, a unified framework for\nany-reference video generation that introduces masked guidance to enable\ncoherent multi-subject video synthesis conditioned on diverse reference images\nand a textual prompt. Specifically, we propose (1) a region-aware dynamic\nmasking mechanism that enables a single model to flexibly handle various\nsubject inference, including humans, objects, and backgrounds, without\narchitectural changes, and (2) a pixel-wise channel concatenation mechanism\nthat operates on the channel dimension to better preserve appearance features.\nOur model delivers state-of-the-art video generation quality, generalizing from\nsingle-subject training to complex multi-subject scenarios with coherent\nsynthesis and precise control over individual subjects, outperforming existing\nopen-source and commercial baselines. To facilitate evaluation, we also\nintroduce a comprehensive multi-subject video benchmark. Extensive experiments\ndemonstrate the effectiveness of our approach, paving the way for scalable,\ncontrollable, and high-fidelity multi-subject video synthesis. Code and model\ncan be found at: https://github.com/MAGREF-Video/MAGREF", "AI": {"tldr": "MAGREF\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u53c2\u8003\u89c6\u9891\u751f\u6210\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7masked guidance\u5b9e\u73b0\u9ad8\u8d28\u91cf\u591a\u4e3b\u4f53\u89c6\u9891\u5408\u6210\u3002", "motivation": "\u89e3\u51b3\u591a\u4e3b\u4f53\u89c6\u9891\u751f\u6210\u4e2d\u4e00\u81f4\u6027\u548c\u8d28\u91cf\u7684\u95ee\u9898\u3002", "method": "1. \u533a\u57df\u611f\u77e5\u52a8\u6001\u63a9\u7801\u673a\u5236\uff1b2. \u50cf\u7d20\u7ea7\u901a\u9053\u7ea7\u8054\u673a\u5236\u3002", "result": "\u5728\u89c6\u9891\u751f\u6210\u8d28\u91cf\u548c\u591a\u4e3b\u4f53\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "conclusion": "MAGREF\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u63a7\u7684\u591a\u4e3b\u4f53\u89c6\u9891\u5408\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u89c6\u9891\u751f\u6210, \u591a\u4e3b\u4f53\u4e00\u81f4\u6027, \u6269\u6563\u6a21\u578b, masked guidance"}}
{"id": "2505.23744", "pdf": "https://arxiv.org/pdf/2505.23744", "abs": "https://arxiv.org/abs/2505.23744", "authors": ["Qiang Wang", "Xiang Song", "Yuhang He", "Jizhou Han", "Chenhao Ding", "Xinyuan Gao", "Yihong Gong"], "title": "Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at CVPR 2025", "summary": "Deep neural networks (DNNs) often underperform in real-world, dynamic\nsettings where data distributions change over time. Domain Incremental Learning\n(DIL) offers a solution by enabling continual model adaptation, with\nParameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce\nknowledge conflicts. However, existing PIDIL methods struggle with parameter\nselection accuracy, especially as the number of domains and corresponding\nclasses grows. To address this, we propose SOYO, a lightweight framework that\nimproves domain selection in PIDIL. SOYO introduces a Gaussian Mixture\nCompressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior\ndomain data efficiently, while a Multi-level Domain Feature Fusion Network\n(MDFN) enhances domain feature extraction. Our framework supports multiple\nParameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks\nsuch as image classification, object detection, and speech enhancement.\nExperimental results on six benchmarks demonstrate SOYO's consistent\nsuperiority over existing baselines, showcasing its robustness and adaptability\nin complex, evolving environments. The codes will be released in\nhttps://github.com/qwangcv/SOYO.", "AI": {"tldr": "SOYO\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u65af\u6df7\u5408\u538b\u7f29\u5668\uff08GMC\uff09\u548c\u57df\u7279\u5f81\u91cd\u91c7\u6837\u5668\uff08DFR\uff09\u4f18\u5316\u53c2\u6570\u9694\u79bb\u589e\u91cf\u5b66\u4e60\uff08PIDIL\uff09\uff0c\u5728\u591a\u9886\u57df\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u5728\u52a8\u6001\u6570\u636e\u5206\u5e03\u73af\u5883\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5c24\u5176\u662f\u53c2\u6570\u9694\u79bb\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u5728\u53c2\u6570\u9009\u62e9\u51c6\u786e\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSOYO\u6846\u67b6\uff0c\u5305\u542bGMC\u3001DFR\u548c\u591a\u5c42\u6b21\u57df\u7279\u5f81\u878d\u5408\u7f51\u7edc\uff08MDFN\uff09\uff0c\u652f\u6301\u591a\u79cd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSOYO\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u73b0\u5176\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "SOYO\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "keywords": "\u589e\u91cf\u5b66\u4e60, \u57df\u9002\u5e94, \u9ad8\u65af\u6df7\u5408\u6a21\u578b, \u7279\u5f81\u878d\u5408"}}
{"id": "2505.23745", "pdf": "https://arxiv.org/pdf/2505.23745", "abs": "https://arxiv.org/abs/2505.23745", "authors": ["Hao Dong", "Moru Liu", "Jian Liang", "Eleni Chatzi", "Olga Fink"], "title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated strong capabilities in\naligning visual and textual modalities, enabling a wide range of applications\nin multimodal understanding and generation. While they excel in zero-shot and\ntransfer learning scenarios, VLMs remain susceptible to misclassification,\noften yielding confident yet incorrect predictions. This limitation poses a\nsignificant risk in safety-critical domains, where erroneous predictions can\nlead to severe consequences. In this work, we introduce TrustVLM, a\ntraining-free framework designed to address the critical challenge of\nestimating when VLM's predictions can be trusted. Motivated by the observed\nmodality gap in VLMs and the insight that certain concepts are more distinctly\nrepresented in the image embedding space, we propose a novel confidence-scoring\nfunction that leverages this space to improve misclassification detection. We\nrigorously evaluate our approach across 17 diverse datasets, employing 4\narchitectures and 2 VLMs, and demonstrate state-of-the-art performance, with\nimprovements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95\ncompared to existing baselines. By improving the reliability of the model\nwithout requiring retraining, TrustVLM paves the way for safer deployment of\nVLMs in real-world applications. The code will be available at\nhttps://github.com/EPFL-IMOS/TrustVLM.", "AI": {"tldr": "TrustVLM\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u56fe\u50cf\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u6a21\u6001\u5dee\u5f02\u6765\u63d0\u9ad8Vision-Language Models (VLMs)\u7684\u9884\u6d4b\u53ef\u9760\u6027\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u9519\u8bef\u5206\u7c7b\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VLMs\u5728\u96f6\u6837\u672c\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u5bb9\u6613\u4ea7\u751f\u81ea\u4fe1\u7684\u9519\u8bef\u9884\u6d4b\uff0c\u8fd9\u5e26\u6765\u4e86\u4e25\u91cd\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u4f30\u8ba1VLM\u9884\u6d4b\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "TrustVLM\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5206\u6790VLMs\u4e2d\u7684\u6a21\u6001\u5dee\u8ddd\u548c\u56fe\u50cf\u5d4c\u5165\u7a7a\u95f4\u4e2d\u6982\u5ff5\u7684\u533a\u5206\u5ea6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u51fd\u6570\u3002", "result": "\u572817\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u30014\u79cd\u67b6\u6784\u548c2\u79cdVLMs\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTrustVLM\u5728AURC\u3001AUROC\u548cFPR95\u7b49\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u6700\u9ad8\u63d0\u534751.87%\u30019.14%\u548c32.42%\u3002", "conclusion": "TrustVLM\u901a\u8fc7\u6539\u8fdb\u6a21\u578b\u53ef\u9760\u6027\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u4e3aVLMs\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\u3002", "keywords": "Vision-Language Models, TrustVLM, \u7f6e\u4fe1\u5ea6\u8bc4\u5206, \u6a21\u6001\u5dee\u8ddd, \u9519\u8bef\u5206\u7c7b\u68c0\u6d4b"}}
{"id": "2505.23747", "pdf": "https://arxiv.org/pdf/2505.23747", "abs": "https://arxiv.org/abs/2505.23747", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.6; I.2"], "comment": "21 pages", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "AI": {"tldr": "Spatial-MLLM\u901a\u8fc7\u53cc\u7f16\u7801\u5668\u67b6\u6784\u548c\u7a7a\u95f4\u611f\u77e5\u5e27\u91c7\u6837\u7b56\u7565\uff0c\u4ec5\u57fa\u4e8e2D\u8f93\u5165\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u5404\u7c7b\u89c6\u89c9\u7a7a\u95f4\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D MLLMs\u4f9d\u8d56\u989d\u59163D\u62162.5D\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5728\u7eaf2D\u8f93\u5165\u573a\u666f\u7684\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u989d\u5916\u6570\u636e\u7684\u7a7a\u95f4\u63a8\u7406\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff08\u8bed\u4e49\u7f16\u7801\u5668+\u7a7a\u95f4\u7f16\u7801\u5668\uff09\u548c\u7a7a\u95f4\u611f\u77e5\u5e27\u91c7\u6837\u7b56\u7565\uff0c\u7ed3\u5408\u89c6\u89c9\u51e0\u4f55\u6a21\u578b\u7684\u7ed3\u6784\u5148\u9a8c\u3002", "result": "\u5728\u591a\u79cd\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u89c6\u89c9\u7a7a\u95f4\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "Spatial-MLLM\u4e3a\u7eaf2D\u8f93\u5165\u573a\u666f\u7684\u7a7a\u95f4\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u6570\u636e\u96c6\u548c\u6a21\u578b\u3002", "keywords": "Multimodal Large Language Models, \u7a7a\u95f4\u63a8\u7406, \u53cc\u7f16\u7801\u5668, \u89c6\u89c9\u51e0\u4f55\u6a21\u578b"}}
{"id": "2505.22974", "pdf": "https://arxiv.org/pdf/2505.22974", "abs": "https://arxiv.org/abs/2505.22974", "authors": ["Yuntao Ma", "Andrei Cramariuc", "Farbod Farshidian", "Marco Hutter"], "title": "Learning coordinated badminton skills for legged manipulators", "categories": ["cs.RO", "cs.LG", "68T40, 93C85", "I.2.9; I.2.6; I.2.8"], "comment": "Science Robotics DOI: 10.1126/scirobotics.adu3922", "summary": "Coordinating the motion between lower and upper limbs and aligning limb\ncontrol with perception are substantial challenges in robotics, particularly in\ndynamic environments. To this end, we introduce an approach for enabling legged\nmobile manipulators to play badminton, a task that requires precise\ncoordination of perception, locomotion, and arm swinging. We propose a unified\nreinforcement learning-based control policy for whole-body visuomotor skills\ninvolving all degrees of freedom to achieve effective shuttlecock tracking and\nstriking. This policy is informed by a perception noise model that utilizes\nreal-world camera data, allowing for consistent perception error levels between\nsimulation and deployment and encouraging learned active perception behaviors.\nOur method includes a shuttlecock prediction model, constrained reinforcement\nlearning for robust motion control, and integrated system identification\ntechniques to enhance deployment readiness. Extensive experimental results in a\nvariety of environments validate the robot's capability to predict shuttlecock\ntrajectories, navigate the service area effectively, and execute precise\nstrikes against human players, demonstrating the feasibility of using legged\nmobile manipulators in complex and dynamic sports scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5168\u8eab\u63a7\u5236\u7b56\u7565\uff0c\u4f7f\u817f\u5f0f\u79fb\u52a8\u673a\u68b0\u81c2\u80fd\u591f\u5b8c\u6210\u590d\u6742\u7684\u7fbd\u6bdb\u7403\u8fd0\u52a8\u4efb\u52a1\uff0c\u5305\u62ec\u9884\u6d4b\u7fbd\u6bdb\u7403\u8f68\u8ff9\u3001\u7cbe\u51c6\u51fb\u7403\u548c\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u534f\u8c03\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u534f\u8c03\u80a2\u4f53\u8fd0\u52a8\u548c\u611f\u77e5\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u4efb\u52a1\u5982\u7fbd\u6bdb\u7403\u8fd0\u52a8\u4e2d\uff0c\u9700\u8981\u7cbe\u786e\u6574\u5408\u611f\u77e5\u3001\u8fd0\u52a8\u63a7\u5236\u548c\u624b\u81c2\u52a8\u4f5c\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u7b56\u7565\uff0c\u7ed3\u5408\u611f\u77e5\u566a\u58f0\u6a21\u578b\u3001\u7fbd\u6bdb\u7403\u8f68\u8ff9\u9884\u6d4b\u3001\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u548c\u7cfb\u7edf\u8fa8\u8bc6\u6280\u672f\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u7684\u5168\u8eab\u89c6\u89c9\u8fd0\u52a8\u6280\u80fd\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u673a\u5668\u4eba\u5728\u591a\u79cd\u73af\u5883\u4e2d\u80fd\u6709\u6548\u9884\u6d4b\u7fbd\u6bdb\u7403\u8f68\u8ff9\u3001\u7075\u6d3b\u79fb\u52a8\u5e76\u7cbe\u51c6\u51fb\u7403\uff0c\u5c55\u793a\u4e86\u817f\u5f0f\u79fb\u52a8\u673a\u68b0\u81c2\u5728\u52a8\u6001\u8fd0\u52a8\u573a\u666f\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u817f\u5f0f\u79fb\u52a8\u673a\u68b0\u81c2\u5728\u590d\u6742\u52a8\u6001\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u548c\u611f\u77e5\u6574\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u817f\u5f0f\u79fb\u52a8\u673a\u68b0\u81c2, \u5f3a\u5316\u5b66\u4e60, \u89c6\u89c9\u8fd0\u52a8\u6280\u80fd, \u7fbd\u6bdb\u7403, \u52a8\u6001\u73af\u5883"}}
{"id": "2505.22997", "pdf": "https://arxiv.org/pdf/2505.22997", "abs": "https://arxiv.org/abs/2505.22997", "authors": ["Agnideep Aich", "Ashit Baran Aich", "Bruce Wade"], "title": "Theoretical Foundations of the Deep Copula Classifier: A Generative Approach to Modeling Dependent Features", "categories": ["stat.ML", "cs.LG", "62H30, 68T07, 62C12, 62G05"], "comment": "Submitted", "summary": "Traditional classifiers often assume feature independence or rely on overly\nsimplistic relationships, leading to poor performance in settings where\nreal-world dependencies matter. We introduce the Deep Copula Classifier (DCC),\na generative model that separates the learning of each feature's marginal\ndistribution from the modeling of their joint dependence structure via neural\nnetwork-parameterized copulas. For each class, lightweight neural networks are\nused to flexibly and adaptively capture feature interactions, making DCC\nparticularly effective when classification is driven by complex dependencies.\nWe establish that DCC converges to the Bayes-optimal classifier under standard\nconditions and provide explicit convergence rates of O(n^{-r/(2r + d)}) for\nr-smooth copula densities. Beyond theoretical guarantees, we outline several\npractical extensions, including high-dimensional scalability through vine and\nfactor copula architectures, semi-supervised learning via entropy\nregularization, and online adaptation using streaming gradient methods. By\nunifying statistical rigor with the representational power of neural networks,\nDCC offers a mathematically grounded and interpretable framework for\ndependency-aware classification.", "AI": {"tldr": "Deep Copula Classifier (DCC) \u662f\u4e00\u79cd\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316 copula \u5206\u79bb\u7279\u5f81\u8fb9\u7f18\u5206\u5e03\u548c\u8054\u5408\u4f9d\u8d56\u7ed3\u6784\u7684\u5b66\u4e60\uff0c\u9002\u5408\u73b0\u5b9e\u4e16\u754c\u4f9d\u8d56\u5173\u7cfb\u590d\u6742\u7684\u5206\u7c7b\u573a\u666f\u3002", "motivation": "\u4f20\u7edf\u5206\u7c7b\u5668\u5e38\u5047\u8bbe\u7279\u5f81\u72ec\u7acb\u6216\u4f9d\u8d56\u8fc7\u4e8e\u7b80\u5355\u7684\u5173\u7cfb\uff0c\u5bfc\u81f4\u5728\u73b0\u5b9e\u4f9d\u8d56\u5173\u7cfb\u590d\u6742\u65f6\u8868\u73b0\u4e0d\u4f73\uff0cDCC \u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DCC \u4f7f\u7528\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u7075\u6d3b\u6355\u6349\u7279\u5f81\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u5b66\u4e60\u6bcf\u4e2a\u7c7b\u522b\u7684\u8fb9\u7f18\u5206\u5e03\u548c\u4f9d\u8d56\u7ed3\u6784\u3002", "result": "DCC \u5728\u6807\u51c6\u6761\u4ef6\u4e0b\u6536\u655b\u5230\u8d1d\u53f6\u65af\u6700\u4f18\u5206\u7c7b\u5668\uff0c\u5e76\u9488\u5bf9\u5149\u6ed1 copula \u5bc6\u5ea6\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u6536\u655b\u901f\u7387\uff0c\u540c\u65f6\u652f\u6301\u9ad8\u7ef4\u53ef\u6269\u5c55\u6027\u548c\u534a\u76d1\u7763\u5b66\u4e60\u3002", "conclusion": "DCC \u901a\u8fc7\u7ed3\u5408\u7edf\u8ba1\u4e25\u8c28\u6027\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4e3a\u4f9d\u8d56\u611f\u77e5\u5206\u7c7b\u63d0\u4f9b\u4e86\u6570\u5b66\u4e0a\u53ef\u89e3\u91ca\u7684\u6846\u67b6\u3002", "keywords": "Deep Copula Classifier, \u795e\u7ecf\u7f51\u7edc, \u4f9d\u8d56\u5efa\u6a21, \u8d1d\u53f6\u65af\u6700\u4f18\u5206\u7c7b\u5668, \u9ad8\u7ef4\u53ef\u6269\u5c55\u6027"}}
{"id": "2505.23056", "pdf": "https://arxiv.org/pdf/2505.23056", "abs": "https://arxiv.org/abs/2505.23056", "authors": ["Zijian Liu", "Zhengyuan Zhou"], "title": "Improved Last-Iterate Convergence of Shuffling Gradient Methods for Nonsmooth Convex Optimization", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": "ICML 2025", "summary": "We study the convergence of the shuffling gradient method, a popular\nalgorithm employed to minimize the finite-sum function with regularization, in\nwhich functions are passed to apply (Proximal) Gradient Descent (GD) one by one\nwhose order is determined by a permutation on the indices of functions. In\ncontrast to its easy implementation and effective performance in practice, the\ntheoretical understanding remains limited. A recent advance by (Liu & Zhou,\n2024b) establishes the first last-iterate convergence results under various\nsettings, especially proving the optimal rates for smooth (strongly) convex\noptimization. However, their bounds for nonsmooth (strongly) convex functions\nare only as fast as Proximal GD. In this work, we provide the first improved\nlast-iterate analysis for the nonsmooth case demonstrating that the widely used\nRandom Reshuffle ($\\textsf{RR}$) and Single Shuffle ($\\textsf{SS}$) strategies\nare both provably faster than Proximal GD, reflecting the benefit of\nrandomness. As an important implication, we give the first (nearly) optimal\nconvergence result for the suffix average under the $\\textsf{RR}$ sampling\nscheme in the general convex case, matching the lower bound shown by (Koren et\nal., 2022).", "AI": {"tldr": "\u8be5\u8bba\u6587\u6539\u8fdb\u4e86\u968f\u673a\u6d17\u724c\u68af\u5ea6\u65b9\u6cd5\u5728\u975e\u5149\u6ed1\u51f8\u4f18\u5316\u4e2d\u7684\u6536\u655b\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u968f\u673a\u6d17\u724c\u548c\u5355\u6b21\u6d17\u724c\u7b56\u7565\u6bd4\u8fd1\u7aef\u68af\u5ea6\u4e0b\u964d\u66f4\u5feb\uff0c\u9996\u6b21\u5728\u4e00\u822c\u51f8\u60c5\u51b5\u4e0b\u7ed9\u51fa\uff08\u8fd1\u4e4e\uff09\u6700\u4f18\u7684\u540e\u7f00\u5e73\u5747\u6536\u655b\u7ed3\u679c\u3002", "motivation": "\u5c3d\u7ba1\u968f\u673a\u68af\u5ea6\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u7406\u8bba\u7406\u89e3\u4ecd\u6709\u9650\u3002\u6700\u8fd1\u7684\u5de5\u4f5c\u5728\u5149\u6ed1\u51f8\u4f18\u5316\u4e2d\u53d6\u5f97\u4e86\u6700\u4f18\u6536\u655b\u7387\uff0c\u4f46\u5728\u975e\u5149\u6ed1\u51f8\u51fd\u6570\u4e0a\u7684\u6536\u655b\u901f\u5ea6\u4ec5\u4e0e\u8fd1\u7aef\u68af\u5ea6\u4e0b\u964d\u76f8\u5f53\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u968f\u673a\u6d17\u724c\uff08RR\uff09\u548c\u5355\u6b21\u6d17\u724c\uff08SS\uff09\u7b56\u7565\uff0c\u6539\u8fdb\u4e86\u975e\u5149\u6ed1\uff08\u5f3a\uff09\u51f8\u51fd\u6570\u7684\u6700\u540e\u4e00\u6b21\u8fed\u4ee3\u6536\u655b\u901f\u5ea6\u3002", "result": "\u8bc1\u660e\u4e86RR\u548cSS\u7b56\u7565\u5728\u975e\u5149\u6ed1\u60c5\u51b5\u4e0b\u6bd4\u8fd1\u7aef\u68af\u5ea6\u4e0b\u964d\u66f4\u5feb\uff0c\u5e76\u5728\u4e00\u822c\u51f8\u60c5\u51b5\u4e0b\u7ed9\u51fa\u4e86\uff08\u8fd1\u4e4e\uff09\u6700\u4f18\u7684\u540e\u7f00\u5e73\u5747\u6536\u655b\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u968f\u673a\u6d17\u724c\u68af\u5ea6\u65b9\u6cd5\u5728\u975e\u5149\u6ed1\u51f8\u51fd\u6570\u4e2d\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u5c55\u793a\u4e86\u968f\u673a\u6027\u7684\u4f18\u52bf\uff0c\u5e76\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u652f\u6301\u3002", "keywords": "\u968f\u673a\u6d17\u724c\u68af\u5ea6\u65b9\u6cd5, \u975e\u5149\u6ed1\u4f18\u5316, \u6536\u655b\u5206\u6790, \u8fd1\u7aef\u68af\u5ea6\u4e0b\u964d, \u968f\u673a\u6027"}}
{"id": "2505.23081", "pdf": "https://arxiv.org/pdf/2505.23081", "abs": "https://arxiv.org/abs/2505.23081", "authors": ["Wenzhi Gao", "Ya-Chi Chu", "Yinyu Ye", "Madeleine Udell"], "title": "Gradient Methods with Online Scaling Part I. Theoretical Foundations", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": "Extension of arXiv:2411.01803 and arXiv:2502.11229", "summary": "This paper establishes the theoretical foundations of the online scaled\ngradient methods (OSGM), a framework that utilizes online learning to adapt\nstepsizes and provably accelerate first-order methods. OSGM quantifies the\neffectiveness of a stepsize by a feedback function motivated from a convergence\nmeasure and uses the feedback to adjust the stepsize through an online learning\nalgorithm. Consequently, instantiations of OSGM achieve convergence rates that\nare asymptotically no worse than the optimal stepsize. OSGM yields desirable\nconvergence guarantees on smooth convex problems, including 1)\ntrajectory-dependent global convergence on smooth convex objectives; 2) an\nimproved complexity result on smooth strongly convex problems, and 3) local\nsuperlinear convergence. Notably, OSGM constitutes a new family of first-order\nmethods with non-asymptotic superlinear convergence, joining the celebrated\nquasi-Newton methods. Finally, OSGM explains the empirical success of the\npopular hypergradient-descent heuristic in optimization for machine learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5728\u7ebf\u7f29\u653e\u68af\u5ea6\u65b9\u6cd5\uff08OSGM\uff09\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u81ea\u9002\u5e94\u6b65\u957f\u4ee5\u52a0\u901f\u4e00\u9636\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6536\u655b\u6027\u4f18\u4e8e\u56fa\u5b9a\u6b65\u957f\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u4f20\u7edf\u4e00\u9636\u4f18\u5316\u65b9\u6cd5\u4e2d\u6b65\u957f\u9009\u62e9\u7684\u6548\u7387\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u673a\u5668\u5b66\u4e60\u548c\u4f18\u5316\u4efb\u52a1\u4e2d\u5982\u4f55\u81ea\u9002\u5e94\u8c03\u6574\u6b65\u957f\u4ee5\u63d0\u9ad8\u6536\u655b\u901f\u5ea6\u3002", "method": "OSGM\u901a\u8fc7\u53cd\u9988\u51fd\u6570\u91cf\u5316\u6b65\u957f\u7684\u6709\u6548\u6027\uff0c\u5e76\u5229\u7528\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u52a8\u6001\u8c03\u6574\u6b65\u957f\uff0c\u5176\u6846\u67b6\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7684\u6536\u655b\u6d4b\u5ea6\u3002", "result": "OSGM\u5728\u5149\u6ed1\u51f8\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6536\u655b\u6027\uff0c\u5305\u62ec\u8f68\u8ff9\u4f9d\u8d56\u7684\u5168\u5c40\u6536\u655b\u3001\u5f3a\u51f8\u95ee\u9898\u7684\u6539\u8fdb\u590d\u6742\u5ea6\uff0c\u4ee5\u53ca\u5c40\u90e8\u8d85\u7ebf\u6027\u6536\u655b\uff0c\u5e76\u89e3\u91ca\u4e86\u8d85\u68af\u5ea6\u4e0b\u964d\u7684\u5b9e\u8bc1\u6210\u529f\u3002", "conclusion": "OSGM\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u4f18\u4e8e\u56fa\u5b9a\u6b65\u957f\u7684\u6536\u655b\u4fdd\u8bc1\uff0c\u8fd8\u6784\u6210\u4e86\u975e\u6e10\u8fd1\u8d85\u7ebf\u6027\u6536\u655b\u7684\u65b0\u4e00\u65cf\u4e00\u9636\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u4f18\u5316\u7b97\u6cd5\u7684\u7406\u8bba\u8fb9\u754c\u3002", "keywords": "\u4f18\u5316\u65b9\u6cd5, \u5728\u7ebf\u5b66\u4e60, \u6b65\u957f\u81ea\u9002\u5e94, \u6536\u655b\u6027, \u8d85\u7ebf\u6027\u6536\u655b"}}
{"id": "2505.23124", "pdf": "https://arxiv.org/pdf/2505.23124", "abs": "https://arxiv.org/abs/2505.23124", "authors": ["Junyan Liu", "Arnab Maiti", "Artin Tajdini", "Kevin Jamieson", "Lillian J. Ratliff"], "title": "Learning to Incentivize in Repeated Principal-Agent Problems with Adversarial Agent Arrivals", "categories": ["cs.GT", "cs.LG"], "comment": "To appear at ICML 2025", "summary": "We initiate the study of a repeated principal-agent problem over a finite\nhorizon $T$, where a principal sequentially interacts with $K\\geq 2$ types of\nagents arriving in an adversarial order. At each round, the principal\nstrategically chooses one of the $N$ arms to incentivize for an arriving agent\nof unknown type. The agent then chooses an arm based on its own utility and the\nprovided incentive, and the principal receives a corresponding reward. The\nobjective is to minimize regret against the best incentive in hindsight.\nWithout prior knowledge of agent behavior, we show that the problem becomes\nintractable, leading to linear regret. We analyze two key settings where\nsublinear regret is achievable. In the first setting, the principal knows the\narm each agent type would select greedily for any given incentive. Under this\nsetting, we propose an algorithm that achieves a regret bound of\n$O(\\min\\{\\sqrt{KT\\log N},K\\sqrt{T}\\})$ and provide a matching lower bound up to\na $\\log K$ factor. In the second setting, an agent's response varies smoothly\nwith the incentive and is governed by a Lipschitz constant $L\\geq 1$. Under\nthis setting, we show that there is an algorithm with a regret bound of\n$\\tilde{O}((LN)^{1/3}T^{2/3})$ and establish a matching lower bound up to\nlogarithmic factors. Finally, we extend our algorithmic results for both\nsettings by allowing the principal to incentivize multiple arms simultaneously\nin each round.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u6709\u9650\u65f6\u95f4T\u5185\u91cd\u590d\u7684\u59d4\u6258\u4eba-\u4ee3\u7406\u4eba\u95ee\u9898\uff0c\u59d4\u6258\u4eba\u9700\u5728\u672a\u77e5\u4ee3\u7406\u4eba\u7c7b\u578b\u4e0b\u9009\u62e9\u6fc0\u52b1\u63aa\u65bd\u4ee5\u6700\u5c0f\u5316\u540e\u6094\u3002\u5206\u6790\u4e86\u4e24\u79cd\u53ef\u5b9e\u73b0\u6b21\u7ebf\u6027\u540e\u6094\u7684\u8bbe\u5b9a\uff0c\u5e76\u6269\u5c55\u81f3\u591a\u81c2\u6fc0\u52b1\u3002", "motivation": "\u63a2\u8ba8\u5728\u5bf9\u6297\u6027\u987a\u5e8f\u4e0b\uff0c\u59d4\u6258\u4eba\u5982\u4f55\u901a\u8fc7\u6fc0\u52b1\u7b56\u7565\u4e0e\u4e0d\u540c\u7c7b\u578b\u4ee3\u7406\u4eba\u4ea4\u4e92\u4ee5\u5b9e\u73b0\u6700\u5c0f\u540e\u6094\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u8bbe\u5b9a\u4e0b\u7684\u7b97\u6cd5\uff1a\u4e00\u662f\u5df2\u77e5\u4ee3\u7406\u4eba\u8d2a\u5a6a\u9009\u62e9\u884c\u4e3a\u7684\u7b97\u6cd5\uff0c\u4e8c\u662f\u4ee3\u7406\u4eba\u54cd\u5e94\u5e73\u6ed1\u53d8\u5316\u65f6\u7684\u7b97\u6cd5\u3002", "result": "\u5728\u4e24\u79cd\u8bbe\u5b9a\u4e0b\u5747\u5b9e\u73b0\u4e86\u6b21\u7ebf\u6027\u540e\u6094\uff0c\u5e76\u7ed9\u51fa\u4e86\u5339\u914d\u7684\u4e0b\u754c\u3002", "conclusion": "\u5373\u4f7f\u5728\u65e0\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u7279\u5b9a\u8bbe\u5b9a\u4ecd\u80fd\u5b9e\u73b0\u6b21\u7ebf\u6027\u540e\u6094\uff0c\u4e14\u53ef\u6269\u5c55\u81f3\u591a\u81c2\u6fc0\u52b1\u3002", "keywords": "\u91cd\u590d\u59d4\u6258\u4eba-\u4ee3\u7406\u4eba\u95ee\u9898, \u6fc0\u52b1\u7b56\u7565, \u540e\u6094\u6700\u5c0f\u5316, \u5bf9\u6297\u6027\u987a\u5e8f, \u6b21\u7ebf\u6027\u540e\u6094"}}
{"id": "2505.23160", "pdf": "https://arxiv.org/pdf/2505.23160", "abs": "https://arxiv.org/abs/2505.23160", "authors": ["Lorenzo Marinucci", "Claudio Battiloro", "Paolo Di Lorenzo"], "title": "Topological Adaptive Least Mean Squares Algorithms over Simplicial Complexes", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "This paper introduces a novel adaptive framework for processing dynamic flow\nsignals over simplicial complexes, extending classical least-mean-squares (LMS)\nmethods to high-order topological domains. Building on discrete Hodge theory,\nwe present a topological LMS algorithm that efficiently processes streaming\nsignals observed over time-varying edge subsets. We provide a detailed\nstochastic analysis of the algorithm, deriving its stability conditions,\nsteady-state mean-square-error, and convergence speed, while exploring the\nimpact of edge sampling on performance. We also propose strategies to design\noptimal edge sampling probabilities, minimizing rate while ensuring desired\nestimation accuracy. Assuming partial knowledge of the complex structure (e.g.,\nthe underlying graph), we introduce an adaptive topology inference method that\nintegrates with the proposed LMS framework. Additionally, we propose a\ndistributed version of the algorithm and analyze its stability and\nmean-square-error properties. Empirical results on synthetic and real-world\ntraffic data demonstrate that our approach, in both centralized and distributed\nsettings, outperforms graph-based LMS methods by leveraging higher-order\ntopological features.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u57fa\u4e8e\u5355\u7eaf\u590d\u5f62\u7684\u52a8\u6001\u6d41\u4fe1\u53f7\uff0c\u5c06\u7ecf\u5178\u7684\u6700\u5c0f\u5747\u65b9\uff08LMS\uff09\u65b9\u6cd5\u6269\u5c55\u5230\u9ad8\u9636\u62d3\u6251\u9886\u57df\u3002", "motivation": "\u4f20\u7edf\u56fe\u4fe1\u53f7\u5904\u7406\u901a\u5e38\u4ec5\u4f9d\u8d56\u4f4e\u9636\u62d3\u6251\u4fe1\u606f\uff0c\u800c\u5ffd\u7565\u4e86\u9ad8\u9636\u7ed3\u6784\u7684\u6f5c\u5728\u4ef7\u503c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u5355\u7eaf\u590d\u5f62\u548c\u79bb\u6563Hodge\u7406\u8bba\uff0c\u6269\u5c55LMS\u7b97\u6cd5\u4ee5\u5229\u7528\u9ad8\u9636\u62d3\u6251\u7279\u5f81\uff0c\u4ece\u800c\u63d0\u9ad8\u52a8\u6001\u6d41\u4fe1\u53f7\u5904\u7406\u7684\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u79bb\u6563Hodge\u7406\u8bba\uff0c\u63d0\u51fa\u4e86\u62d3\u6251LMS\u7b97\u6cd5\uff0c\u652f\u6301\u65f6\u53d8\u8fb9\u5b50\u96c6\u4e0a\u7684\u6d41\u4fe1\u53f7\u5904\u7406\u3002\u65b9\u6cd5\u5305\u62ec\u8be6\u7ec6\u968f\u673a\u5206\u6790\u3001\u7a33\u5b9a\u6027\u4e0e\u6536\u655b\u6027\u8bc1\u660e\u3001\u6700\u4f18\u8fb9\u91c7\u6837\u6982\u7387\u8bbe\u8ba1\u3001\u81ea\u9002\u5e94\u62d3\u6251\u63a8\u7406\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5206\u5e03\u5f0f\u5b9e\u73b0\u3002", "result": "\u7b97\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u4ea4\u901a\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u7248\u672c\u5747\u4f18\u4e8e\u4f20\u7edf\u56feLMS\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u9ad8\u9636\u62d3\u6251\u7279\u5f81\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u9ad8\u9636\u62d3\u6251\u7ed3\u6784\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u53f7\u5904\u7406\u7684\u6027\u80fd\uff0c\u9002\u5e94\u6027\u5f3a\u4e14\u53ef\u6269\u5c55\u81f3\u5206\u5e03\u5f0f\u573a\u666f\u3002", "keywords": "\u5355\u7eaf\u590d\u5f62, \u79bb\u6563Hodge\u7406\u8bba, \u81ea\u9002\u5e94\u6ee4\u6ce2, \u5206\u5e03\u5f0f\u7b97\u6cd5, \u62d3\u6251\u4fe1\u53f7\u5904\u7406"}}
{"id": "2505.23196", "pdf": "https://arxiv.org/pdf/2505.23196", "abs": "https://arxiv.org/abs/2505.23196", "authors": ["Eshant English", "Christoph Lippert"], "title": "JAPAN: Joint Adaptive Prediction Areas with Normalising-Flows", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Conformal prediction provides a model-agnostic framework for uncertainty\nquantification with finite-sample validity guarantees, making it an attractive\ntool for constructing reliable prediction sets. However, existing approaches\ncommonly rely on residual-based conformity scores, which impose geometric\nconstraints and struggle when the underlying distribution is multimodal. In\nparticular, they tend to produce overly conservative prediction areas centred\naround the mean, often failing to capture the true shape of complex predictive\ndistributions. In this work, we introduce JAPAN (Joint Adaptive Prediction\nAreas with Normalising-Flows), a conformal prediction framework that uses\ndensity-based conformity scores. By leveraging flow-based models, JAPAN\nestimates the (predictive) density and constructs prediction areas by\nthresholding on the estimated density scores, enabling compact, potentially\ndisjoint, and context-adaptive regions that retain finite-sample coverage\nguarantees. We theoretically motivate the efficiency of JAPAN and empirically\nvalidate it across multivariate regression and forecasting tasks, demonstrating\ngood calibration and tighter prediction areas compared to existing baselines.\nWe also provide several \\emph{extensions} adding flexibility to our proposed\nframework.", "AI": {"tldr": "JAPAN\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bc6\u5ea6\u7684\u9002\u5e94\u9884\u6d4b\u533a\u57df\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6b8b\u5dee\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u5206\u5e03\u4e0b\u7684\u4fdd\u5b88\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6b8b\u5dee\u7684\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u5206\u5e03\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4ea7\u751f\u8fc7\u4e8e\u4fdd\u5b88\u7684\u9884\u6d4b\u533a\u57df\uff0c\u65e0\u6cd5\u6355\u6349\u590d\u6742\u5206\u5e03\u7684\u5f62\u6001\u3002", "method": "\u5229\u7528\u5f52\u4e00\u5316\u6d41\u6a21\u578b\u4f30\u8ba1\u5bc6\u5ea6\uff0c\u901a\u8fc7\u9608\u503c\u5316\u5bc6\u5ea6\u5206\u6570\u6784\u5efa\u7d27\u51d1\u4e14\u9002\u5e94\u6027\u7684\u9884\u6d4b\u533a\u57df\u3002", "result": "JAPAN\u5728\u591a\u5143\u56de\u5f52\u548c\u9884\u6d4b\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6821\u51c6\u6027\u548c\u66f4\u7d27\u5bc6\u7684\u9884\u6d4b\u533a\u57df\u3002", "conclusion": "JAPAN\u901a\u8fc7\u5bc6\u5ea6\u5206\u6570\u63d0\u5347\u4e86\u9884\u6d4b\u533a\u57df\u7684\u6548\u7387\u548c\u7075\u6d3b\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5206\u5e03\u3002", "keywords": "conformal prediction, uncertainty quantification, normalizing flows, density estimation, multimodal distributions"}}
{"id": "2505.23207", "pdf": "https://arxiv.org/pdf/2505.23207", "abs": "https://arxiv.org/abs/2505.23207", "authors": ["Zhaokai Sun", "Li Zhang", "Qing Wang", "Pan Zhou", "Lei Xie"], "title": "Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": null, "summary": "Overlapping Speech Detection (OSD) aims to identify regions where multiple\nspeakers overlap in a conversation, a critical challenge in multi-party speech\nprocessing. This work proposes a speaker-aware progressive OSD model that\nleverages a progressive training strategy to enhance the correlation between\nsubtasks such as voice activity detection (VAD) and overlap detection. To\nimprove acoustic representation, we explore the effectiveness of\nstate-of-the-art self-supervised learning (SSL) models, including WavLM and\nwav2vec 2.0, while incorporating a speaker attention module to enrich features\nwith frame-level speaker information. Experimental results show that the\nproposed method achieves state-of-the-art performance, with an F1 score of\n82.76\\% on the AMI test set, demonstrating its robustness and effectiveness in\nOSD.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u7684\u8bf4\u8bdd\u4eba\u611f\u77e5\u91cd\u53e0\u8bed\u97f3\u68c0\u6d4b\u6a21\u578b\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8bf4\u8bdd\u4eba\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u91cd\u53e0\u8bed\u97f3\u68c0\u6d4b\uff08OSD\uff09\u662f\u591a\u65b9\u8bed\u97f3\u5904\u7406\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5b50\u4efb\u52a1\u5173\u8054\u6027\u548c\u58f0\u5b66\u8868\u5f81\u4e0a\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u8054\u5408\u4f18\u5316\u5b50\u4efb\u52a1\uff08\u5982\u8bed\u97f3\u6d3b\u52a8\u68c0\u6d4b\u548c\u91cd\u53e0\u68c0\u6d4b\uff09\uff0c\u5e76\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\uff08\u5982WavLM\u3001wav2vec 2.0\uff09\u4e0e\u8bf4\u8bdd\u4eba\u6ce8\u610f\u529b\u6a21\u5757\u589e\u5f3a\u58f0\u5b66\u7279\u5f81\u3002", "result": "\u5728AMI\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523082.76%\u7684F1\u5206\u6570\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u548c\u8bf4\u8bdd\u4eba\u4fe1\u606f\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86OSD\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u91cd\u53e0\u8bed\u97f3\u68c0\u6d4b,\u6e10\u8fdb\u5f0f\u8bad\u7ec3,\u81ea\u76d1\u7763\u5b66\u4e60,\u8bf4\u8bdd\u4eba\u6ce8\u610f\u529b,\u8bed\u97f3\u6d3b\u52a8\u68c0\u6d4b"}}
{"id": "2505.23215", "pdf": "https://arxiv.org/pdf/2505.23215", "abs": "https://arxiv.org/abs/2505.23215", "authors": ["T. Jahn", "J. Chemseddine", "P. Hagemann", "C. Wald", "G. Steidl"], "title": "Trajectory Generator Matching for Time Series", "categories": ["math.NA", "cs.LG", "cs.NA"], "comment": null, "summary": "Accurately modeling time-continuous stochastic processes from irregular\nobservations remains a significant challenge. In this paper, we leverage ideas\nfrom generative modeling of image data to push the boundary of time series\ngeneration. For this, we find new generators of SDEs and jump processes,\ninspired by trajectory flow matching, that have the marginal distributions of\nthe time series of interest. Specifically, we can handle discontinuities of the\nunderlying processes by parameterizing the jump kernel densities by scaled\nGaussians that allow for closed form formulas of the corresponding\nKullback-Leibler divergence in the loss. Unlike most other approaches, we are\nable to handle irregularly sampled time series.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u751f\u6210\u6a21\u578b\u601d\u60f3\u5904\u7406\u4e0d\u89c4\u5219\u91c7\u6837\u65f6\u95f4\u5e8f\u5217\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b0\u7684SDE\u548c\u8df3\u8dc3\u8fc7\u7a0b\u751f\u6210\u5668\uff0c\u7ed3\u5408\u8f68\u8ff9\u6d41\u5339\u914d\u548c\u53c2\u6570\u5316\u8df3\u8dc3\u6838\u5bc6\u5ea6\uff0c\u89e3\u51b3\u4e86\u65f6\u95f4\u8fde\u7eed\u968f\u673a\u8fc7\u7a0b\u7684\u5efa\u6a21\u6311\u6218\u3002", "motivation": "\u51c6\u786e\u5efa\u6a21\u4e0d\u89c4\u5219\u89c2\u6d4b\u4e0b\u7684\u65f6\u95f4\u8fde\u7eed\u968f\u673a\u8fc7\u7a0b\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u9700\u8981\u521b\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u56fe\u50cf\u6570\u636e\u751f\u6210\u6a21\u578b\u7684\u601d\u8def\uff0c\u63d0\u51fa\u65b0\u7684SDE\u548c\u8df3\u8dc3\u8fc7\u7a0b\u751f\u6210\u5668\uff0c\u7ed3\u5408\u8f68\u8ff9\u6d41\u5339\u914d\u548c\u53c2\u6570\u5316\u8df3\u8dc3\u6838\u5bc6\u5ea6\uff08\u4f7f\u7528\u7f29\u653e\u9ad8\u65af\u51fd\u6570\uff09\uff0c\u4ee5\u5904\u7406\u5e95\u5c42\u8fc7\u7a0b\u7684\u95f4\u65ad\u6027\u3002", "result": "\u80fd\u591f\u5904\u7406\u4e0d\u89c4\u5219\u91c7\u6837\u7684\u65f6\u95f4\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u95ed\u5f0fKL\u6563\u5ea6\u635f\u5931\u516c\u5f0f\u4f18\u5316\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u9886\u57df\u7a81\u7834\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u4e0d\u89c4\u5219\u89c2\u6d4b\u6570\u636e\u7684\u80fd\u529b\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217\u751f\u6210, \u968f\u673a\u5fae\u5206\u65b9\u7a0b, \u8df3\u8dc3\u8fc7\u7a0b, \u751f\u6210\u6a21\u578b, \u8f68\u8ff9\u6d41\u5339\u914d"}}
{"id": "2505.23240", "pdf": "https://arxiv.org/pdf/2505.23240", "abs": "https://arxiv.org/abs/2505.23240", "authors": ["Hemant Tyagi"], "title": "Joint estimation of smooth graph signals from partial linear measurements", "categories": ["math.ST", "cs.LG", "stat.ML", "stat.TH"], "comment": "29 pages, 2 figures", "summary": "Given an undirected and connected graph $G$ on $T$ vertices, suppose each\nvertex $t$ has a latent signal $x_t \\in \\mathbb{R}^n$ associated to it. Given\npartial linear measurements of the signals, for a potentially small subset of\nthe vertices, our goal is to estimate $x_t$'s. Assuming that the signals are\nsmooth w.r.t $G$, in the sense that the quadratic variation of the signals over\nthe graph is small, we obtain non-asymptotic bounds on the mean squared error\nfor jointly recovering $x_t$'s, for the smoothness penalized least squares\nestimator. In particular, this implies for certain choices of $G$ that this\nestimator is weakly consistent (as $T \\rightarrow \\infty$) under potentially\nvery stringent sampling, where only one coordinate is measured per vertex for a\nvanishingly small fraction of the vertices. The results are extended to a\n``multi-layer'' ranking problem where $x_t$ corresponds to the latent strengths\nof a collection of $n$ items, and noisy pairwise difference measurements are\nobtained at each ``layer'' $t$ via a measurement graph $G_t$. Weak consistency\nis established for certain choices of $G$ even when the individual $G_t$'s are\nvery sparse and disconnected.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5728\u56fe\u7ed3\u6784\u4e0a\u57fa\u4e8e\u90e8\u5206\u7ebf\u6027\u6d4b\u91cf\u4f30\u8ba1\u9876\u70b9\u4fe1\u53f7\uff0c\u7ed3\u679c\u8868\u660e\u5e73\u6ed1\u60e9\u7f5a\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\u5668\u5728\u4e25\u683c\u91c7\u6837\u6761\u4ef6\u4e0b\u4ecd\u80fd\u4fdd\u6301\u5f31\u4e00\u81f4\u6027\u3002", "motivation": "\u7814\u7a76\u7684\u76ee\u6807\u662f\u901a\u8fc7\u90e8\u5206\u6d4b\u91cf\u6570\u636e\u6062\u590d\u56fe\u9876\u70b9\u4e0a\u7684\u6f5c\u5728\u4fe1\u53f7\uff0c\u7279\u522b\u662f\u5728\u4fe1\u53f7\u5e73\u6ed1\u6027\u5047\u8bbe\u4e0b\uff0c\u63a2\u7d22\u5373\u4f7f\u91c7\u6837\u6781\u5176\u7a00\u758f\u65f6\u7684\u4f30\u8ba1\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u5e73\u6ed1\u60e9\u7f5a\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u4fe1\u53f7\u7684\u4e8c\u6b21\u53d8\u5dee\uff0c\u7ed3\u5408\u56fe\u7684\u62d3\u6251\u7ed3\u6784\u6765\u8054\u5408\u6062\u590d\u4fe1\u53f7\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u56fe\u7ed3\u6784\u4e0b\uff0c\u5373\u4f7f\u6781\u7aef\u7a00\u758f\u91c7\u6837\uff08\u6bcf\u4e2a\u9876\u70b9\u4ec5\u6d4b\u91cf\u4e00\u4e2a\u5750\u6807\u4e14\u91c7\u6837\u6bd4\u4f8b\u8d8b\u8fd1\u4e8e\u96f6\uff09\uff0c\u4f30\u8ba1\u5668\u4ecd\u80fd\u4fdd\u6301\u5f31\u4e00\u81f4\u6027\u3002", "conclusion": "\u5e73\u6ed1\u6027\u5047\u8bbe\u4e0b\uff0c\u5373\u4f7f\u91c7\u6837\u6781\u5176\u7a00\u758f\uff0c\u8be5\u65b9\u6cd5\u4ecd\u80fd\u6709\u6548\u6062\u590d\u4fe1\u53f7\uff0c\u4e14\u7ed3\u679c\u53ef\u6269\u5c55\u5230\u591a\u5c42\u6392\u540d\u95ee\u9898\u3002", "keywords": "\u56fe\u4fe1\u53f7\u5904\u7406\u3001\u5e73\u6ed1\u60e9\u7f5a\u6700\u5c0f\u4e8c\u4e58\u3001\u5f31\u4e00\u81f4\u6027\u3001\u7a00\u758f\u91c7\u6837\u3001\u591a\u5c42\u6392\u540d"}}
{"id": "2505.23260", "pdf": "https://arxiv.org/pdf/2505.23260", "abs": "https://arxiv.org/abs/2505.23260", "authors": ["Budhaditya Halder", "Shubhayan Pan", "Koulik Khamaru"], "title": "Stable Thompson Sampling: Valid Inference via Variance Inflation", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We consider the problem of statistical inference when the data is collected\nvia a Thompson Sampling-type algorithm. While Thompson Sampling (TS) is known\nto be both asymptotically optimal and empirically effective, its adaptive\nsampling scheme poses challenges for constructing confidence intervals for\nmodel parameters. We propose and analyze a variant of TS, called Stable\nThompson Sampling, in which the posterior variance is inflated by a logarithmic\nfactor. We show that this modification leads to asymptotically normal estimates\nof the arm means, despite the non-i.i.d. nature of the data. Importantly, this\nstatistical benefit comes at a modest cost: the variance inflation increases\nregret by only a logarithmic factor compared to standard TS. Our results reveal\na principled trade-off: by paying a small price in regret, one can enable valid\nstatistical inference for adaptive decision-making algorithms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u7a33\u5b9a\u6c64\u666e\u68ee\u91c7\u6837\uff08Stable Thompson Sampling\uff09\uff0c\u901a\u8fc7\u540e\u9a8c\u65b9\u5dee\u5bf9\u6570\u81a8\u80c0\u89e3\u51b3\u4e86\u6807\u51c6TS\u5728\u975ei.i.d.\u6570\u636e\u4e0b\u7f6e\u4fe1\u533a\u95f4\u6784\u5efa\u7684\u96be\u9898\uff0c\u5b9e\u73b0\u4e86\u6e10\u8fd1\u6b63\u6001\u4f30\u8ba1\uff0c\u4e14\u4ec5\u4ee5\u5bf9\u6570\u7ea7\u589e\u52a0\u9057\u61be\u4e3a\u4ee3\u4ef7\u3002", "motivation": "\u6807\u51c6\u6c64\u666e\u68ee\u91c7\u6837\uff08TS\uff09\u7684\u81ea\u9002\u5e94\u91c7\u6837\u673a\u5236\u867d\u5728\u6e10\u8fdb\u6700\u4f18\u548c\u5b9e\u8bc1\u6709\u6548\uff0c\u4f46\u56e0\u6570\u636e\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7279\u6027\uff0c\u96be\u4ee5\u6784\u5efa\u6a21\u578b\u53c2\u6570\u7684\u7f6e\u4fe1\u533a\u95f4\u3002\u9700\u6539\u826fTS\u4ee5\u652f\u6301\u7edf\u8ba1\u63a8\u65ad\u3002", "method": "\u63d0\u51fa\u7a33\u5b9a\u6c64\u666e\u68ee\u91c7\u6837\uff1a\u5bf9\u540e\u9a8c\u65b9\u5dee\u65bd\u52a0\u5bf9\u6570\u81a8\u80c0\u56e0\u5b50\uff0c\u8c03\u6574\u91c7\u6837\u7b56\u7565\uff0c\u4fdd\u8bc1\u6570\u636e\u867d\u975ei.i.d.\u4ecd\u80fd\u83b7\u5f97\u6e10\u8fd1\u6b63\u6001\u7684\u81c2\u5747\u503c\u4f30\u8ba1\u3002", "result": "\u6539\u9020\u540e\u7684\u7b97\u6cd5\u5728\u4fdd\u6301TS\u6838\u5fc3\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u4f30\u8ba1\u7684\u6e10\u8fd1\u6b63\u6001\u6027\uff0c\u9057\u61be\uff08regret\uff09\u4ec5\u6bd4\u6807\u51c6TS\u589e\u52a0\u5bf9\u6570\u7ea7\uff0c\u5c55\u73b0\u4e86\u7edf\u8ba1\u6709\u6548\u6027\u4e0e\u51b3\u7b56\u6548\u7387\u7684\u5e73\u8861\u3002", "conclusion": "\u901a\u8fc7\u5fae\u5c0f\u9057\u61be\u4ee3\u4ef7\u6362\u53d6\u7edf\u8ba1\u63a8\u65ad\u80fd\u529b\uff0c\u4e3a\u81ea\u9002\u5e94\u51b3\u7b56\u7b97\u6cd5\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u7edf\u8ba1\u57fa\u7840\u3002", "keywords": "Thompson Sampling, \u7edf\u8ba1\u63a8\u65ad, \u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e, \u6e10\u8fd1\u6b63\u6001\u6027, \u9057\u61be\u754c"}}
{"id": "2505.23271", "pdf": "https://arxiv.org/pdf/2505.23271", "abs": "https://arxiv.org/abs/2505.23271", "authors": ["Mao-Lin Luo", "Zi-Hao Zhou", "Tong Wei", "Min-Ling Zhang"], "title": "LADA: Scalable Label-Specific CLIP Adapter for Continual Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Continual learning with vision-language models like CLIP offers a pathway\ntoward scalable machine learning systems by leveraging its transferable\nrepresentations. Existing CLIP-based methods adapt the pre-trained image\nencoder by adding multiple sets of learnable parameters, with each task using a\npartial set of parameters. This requires selecting the expected parameters for\ninput images during inference, which is prone to error that degrades\nperformance. To address this problem, we introduce LADA (Label-specific\nADApter). Instead of partitioning parameters across tasks, LADA appends\nlightweight, label-specific memory units to the frozen CLIP image encoder,\nenabling discriminative feature generation by aggregating task-agnostic\nknowledge. To prevent catastrophic forgetting, LADA employs feature\ndistillation for seen classes, preventing their features from being interfered\nwith by new classes. Positioned after the image encoder, LADA prevents gradient\nflow to the frozen CLIP parameters, ensuring efficient training. Extensive\nresults show that LADA achieves state-of-the-art performance in continual\nlearning settings. The implementation code is available at\nhttps://github.com/MaolinLuo/LADA.", "AI": {"tldr": "LADA\u662f\u4e00\u4e2a\u57fa\u4e8eCLIP\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u8f7b\u91cf\u7ea7\u6807\u7b7e\u7279\u5b9a\u8bb0\u5fc6\u5355\u5143\u6765\u9632\u6b62\u6027\u80fd\u9000\u5316\u3002", "motivation": "\u73b0\u6709CLIP\u57fa\u65b9\u6cd5\u901a\u8fc7\u6dfb\u52a0\u591a\u7ec4\u53ef\u5b66\u4e60\u53c2\u6570\u5bfc\u81f4\u63a8\u7406\u65f6\u53c2\u6570\u9009\u62e9\u5bb9\u6613\u51fa\u9519\uff0c\u5f71\u54cd\u6027\u80fd\u3002", "method": "LADA\u5728\u51bb\u7ed3\u7684CLIP\u56fe\u50cf\u7f16\u7801\u5668\u540e\u9644\u52a0\u8f7b\u91cf\u7ea7\u6807\u7b7e\u7279\u5b9a\u8bb0\u5fc6\u5355\u5143\uff0c\u5229\u7528\u7279\u5f81\u84b8\u998f\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "LADA\u5728\u6301\u7eed\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "LADA\u901a\u8fc7\u907f\u514d\u68af\u5ea6\u6d41\u5230CLIP\u53c2\u6570\u5e76\u805a\u5408\u4efb\u52a1\u65e0\u5173\u77e5\u8bc6\uff0c\u6709\u6548\u63d0\u5347\u6301\u7eed\u5b66\u4e60\u6548\u679c\u3002", "keywords": "\u6301\u7eed\u5b66\u4e60, CLIP, LADA, \u89c6\u89c9\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.23305", "pdf": "https://arxiv.org/pdf/2505.23305", "abs": "https://arxiv.org/abs/2505.23305", "authors": ["Yunkee Chae", "Kyogu Lee"], "title": "MGE-LDM: Joint Latent Diffusion for Simultaneous Music Generation and Source Extraction", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "27 pages, 4 figures", "summary": "We present MGE-LDM, a unified latent diffusion framework for simultaneous\nmusic generation, source imputation, and query-driven source separation. Unlike\nprior approaches constrained to fixed instrument classes, MGE-LDM learns a\njoint distribution over full mixtures, submixtures, and individual stems within\na single compact latent diffusion model. At inference, MGE-LDM enables (1)\ncomplete mixture generation, (2) partial generation (i.e., source imputation),\nand (3) text-conditioned extraction of arbitrary sources. By formulating both\nseparation and imputation as conditional inpainting tasks in the latent space,\nour approach supports flexible, class-agnostic manipulation of arbitrary\ninstrument sources. Notably, MGE-LDM can be trained jointly across\nheterogeneous multi-track datasets (e.g., Slakh2100, MUSDB18, MoisesDB) without\nrelying on predefined instrument categories. Audio samples are available at our\nproject page: https://yoongi43.github.io/MGELDM_Samples/.", "AI": {"tldr": "MGE-LDM\u662f\u4e00\u79cd\u7edf\u4e00\u7684\u6f5c\u5728\u6269\u6563\u6846\u67b6\uff0c\u53ef\u540c\u65f6\u8fdb\u884c\u97f3\u4e50\u751f\u6210\u3001\u6e90\u8865\u5168\u548c\u67e5\u8be2\u9a71\u52a8\u6e90\u5206\u79bb\u3002\u5b83\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u7684\u6761\u4ef6\u4fee\u590d\u4efb\u52a1\uff0c\u652f\u6301\u5bf9\u4efb\u610f\u4e50\u5668\u6e90\u7684\u7075\u6d3b\u64cd\u4f5c\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u4e50\u5668\u7c7b\u522b\u3002", "motivation": "\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u56fa\u5b9a\u4e50\u5668\u7c7b\u522b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u4e2a\u80fd\u5b66\u4e60\u5b8c\u6574\u6df7\u97f3\u3001\u5b50\u6df7\u97f3\u53ca\u5355\u72ec\u97f3\u8f68\u8054\u5408\u5206\u5e03\u7684\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u66f4\u7075\u6d3b\u7684\u97f3\u4e50\u64cd\u4f5c\u3002", "method": "MGE-LDM\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5c06\u5206\u79bb\u548c\u8865\u5168\u4efb\u52a1\u5efa\u6a21\u4e3a\u6f5c\u5728\u7a7a\u95f4\u7684\u6761\u4ef6\u4fee\u590d\uff0c\u53ef\u4ee5\u5728\u5f02\u6784\u591a\u8f68\u6570\u636e\u96c6\u4e0a\u8054\u5408\u8bad\u7ec3\u3002", "result": "MGE-LDM\u652f\u6301\u5b8c\u6574\u7684\u6df7\u97f3\u751f\u6210\u3001\u90e8\u5206\u751f\u6210\uff08\u6e90\u8865\u5168\uff09\u548c\u57fa\u4e8e\u6587\u672c\u7684\u4efb\u610f\u6e90\u63d0\u53d6\uff0c\u4e14\u65e0\u9700\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u4e50\u5668\u7c7b\u522b\u3002", "conclusion": "MGE-LDM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u7075\u6d3b\u5904\u7406\u97f3\u4e50\u751f\u6210\u3001\u8865\u5168\u548c\u5206\u79bb\u4efb\u52a1\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9650\u5236\u3002", "keywords": "\u97f3\u4e50\u751f\u6210\uff0c\u6e90\u5206\u79bb\uff0c\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u6761\u4ef6\u4fee\u590d\uff0c\u591a\u8f68\u6570\u636e\u96c6"}}
{"id": "2505.23344", "pdf": "https://arxiv.org/pdf/2505.23344", "abs": "https://arxiv.org/abs/2505.23344", "authors": ["Jakub Martinka", "Lina Zhang", "Yi-Fan Hou", "Miko\u0142aj Martyka", "Ji\u0159\u00ed Pittner", "Mario Barbatti", "Pavlo O. Dral"], "title": "A Descriptor Is All You Need: Accurate Machine Learning of Nonadiabatic Coupling Vectors", "categories": ["physics.comp-ph", "cs.LG", "physics.chem-ph"], "comment": null, "summary": "Nonadiabatic couplings (NACs) play a crucial role in modeling photochemical\nand photophysical processes with methods such as the widely used\nfewest-switches surface hopping (FSSH). There is therefore a strong incentive\nto machine learn NACs for accelerating simulations. However, this is\nchallenging due to NACs' vectorial, double-valued character and the singularity\nnear a conical intersection seam. For the first time, we design NAC-specific\ndescriptors based on our domain expertise and show that they allow learning\nNACs with never-before-reported accuracy of $R^2$ exceeding 0.99. The key to\nsuccess is also our new ML phase-correction procedure. We demonstrate the\nefficiency and robustness of our approach on a prototypical example of fully\nML-driven FSSH simulations of fulvene targeting the SA-2-CASSCF(6,6) electronic\nstructure level. This ML-FSSH dynamics leads to an accurate description of\n$S_1$ decay while reducing error bars by allowing the execution of a large\nensemble of trajectories. Our implementations are available in open-source\nMLatom.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u8bbe\u8ba1\u4e86\u9488\u5bf9\u975e\u7edd\u70ed\u8026\u5408\uff08NAC\uff09\u7684\u673a\u5668\u5b66\u4e60\u63cf\u8ff0\u7b26\uff0c\u5e76\u7ed3\u5408\u65b0\u7684\u76f8\u4f4d\u6821\u6b63\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u9ad8\u7cbe\u5ea6\uff08R\u00b2>0.99\uff09\uff0c\u6210\u529f\u5e94\u7528\u4e8eML-FSSH\u52a8\u529b\u5b66\u6a21\u62df\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bef\u5dee\u3002", "motivation": "\u975e\u7edd\u70ed\u8026\u5408\uff08NAC\uff09\u5728\u5149\u5316\u5b66\u548c\u5149\u7269\u7406\u8fc7\u7a0b\u7684\u6a21\u62df\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5b66\u4e60NAC\u9762\u4e34\u5411\u91cf\u6027\u3001\u53cc\u503c\u6027\u548c\u9525\u5f62\u4ea4\u53c9\u70b9\u9644\u8fd1\u5947\u5f02\u6027\u7684\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u9886\u57df\u77e5\u8bc6\u8bbe\u8ba1\u4e86NAC\u7279\u5b9a\u63cf\u8ff0\u7b26\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u673a\u5668\u5b66\u4e60\u76f8\u4f4d\u6821\u6b63\u65b9\u6cd5\uff0c\u7ed3\u5408FSSH\u6a21\u62df\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "\u5728fulvene\u7684SA-2-CASSCF(6,6)\u7535\u5b50\u7ed3\u6784\u6c34\u5e73\u4e0a\uff0cML-FSSH\u52a8\u529b\u5b66\u6a21\u62df\u51c6\u786e\u63cf\u8ff0\u4e86S\u2081\u8870\u51cf\uff0c\u4e14\u901a\u8fc7\u5927\u91cf\u8f68\u8ff9\u964d\u4f4e\u4e86\u8bef\u5dee\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86NAC\u673a\u5668\u5b66\u4e60\u7cbe\u5ea6\uff0c\u4e3a\u5149\u5316\u5b66\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u975e\u7edd\u70ed\u8026\u5408, \u673a\u5668\u5b66\u4e60, FSSH, \u5149\u5316\u5b66\u6a21\u62df, \u9525\u5f62\u4ea4\u53c9"}}
{"id": "2505.23389", "pdf": "https://arxiv.org/pdf/2505.23389", "abs": "https://arxiv.org/abs/2505.23389", "authors": ["Ivana Nikoloska", "Hamdi Joudeh", "Ruud van Sloun", "Osvaldo Simeone"], "title": "Dynamic Estimation Loss Control in Variational Quantum Sensing via Online Conformal Inference", "categories": ["quant-ph", "cs.IT", "cs.LG", "eess.SP", "math.IT"], "comment": null, "summary": "Quantum sensing exploits non-classical effects to overcome limitations of\nclassical sensors, with applications ranging from gravitational-wave detection\nto nanoscale imaging. However, practical quantum sensors built on noisy\nintermediate-scale quantum (NISQ) devices face significant noise and sampling\nconstraints, and current variational quantum sensing (VQS) methods lack\nrigorous performance guarantees. This paper proposes an online control\nframework for VQS that dynamically updates the variational parameters while\nproviding deterministic error bars on the estimates. By leveraging online\nconformal inference techniques, the approach produces sequential estimation\nsets with a guaranteed long-term risk level. Experiments on a quantum\nmagnetometry task confirm that the proposed dynamic VQS approach maintains the\nrequired reliability over time, while still yielding precise estimates. The\nresults demonstrate the practical benefits of combining variational quantum\nalgorithms with online conformal inference to achieve reliable quantum sensing\non NISQ devices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u53d8\u5206\u91cf\u5b50\u4f20\u611f\uff08VQS\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u5728\u7ebf\u5171\u5f62\u63a8\u65ad\u6280\u672f\uff0c\u4e3aNISQ\u8bbe\u5907\u63d0\u4f9b\u786e\u5b9a\u6027\u8bef\u5dee\u8fb9\u754c\u548c\u957f\u671f\u98ce\u9669\u4fdd\u969c\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u53d8\u5206\u91cf\u5b50\u4f20\u611f\u65b9\u6cd5\u5728\u566a\u58f0\u4e2d\u5c3a\u5ea6\u91cf\u5b50\uff08NISQ\uff09\u8bbe\u5907\u4e0a\u7f3a\u4e4f\u4e25\u683c\u6027\u80fd\u4fdd\u8bc1\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u5728\u7ebf\u5171\u5f62\u63a8\u65ad\u6280\u672f\u52a8\u6001\u66f4\u65b0\u53d8\u5206\u53c2\u6570\uff0c\u751f\u6210\u5177\u6709\u786e\u5b9a\u8bef\u5dee\u8fb9\u754c\u7684\u987a\u5e8f\u4f30\u8ba1\u96c6\u3002", "result": "\u5728\u91cf\u5b50\u78c1\u529b\u6d4b\u91cf\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u52a8\u6001VQS\u65b9\u6cd5\u7684\u53ef\u9760\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f30\u8ba1\u7cbe\u5ea6\u3002", "conclusion": "\u52a8\u6001VQS\u4e0e\u5728\u7ebf\u5171\u5f62\u63a8\u65ad\u7ed3\u5408\uff0c\u4e3aNISQ\u8bbe\u5907\u4e0a\u7684\u53ef\u9760\u91cf\u5b50\u4f20\u611f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u91cf\u5b50\u4f20\u611f, \u53d8\u5206\u91cf\u5b50\u7b97\u6cd5, \u5728\u7ebf\u5171\u5f62\u63a8\u65ad, NISQ\u8bbe\u5907"}}
{"id": "2505.23416", "pdf": "https://arxiv.org/pdf/2505.23416", "abs": "https://arxiv.org/abs/2505.23416", "authors": ["Jang-Hyun Kim", "Jinuk Kim", "Sangwoo Kwon", "Jae W. Lee", "Sangdoo Yun", "Hyun Oh Song"], "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction", "categories": ["cs.DB", "cs.LG"], "comment": "preprint", "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios.", "AI": {"tldr": "KVzip\u662f\u4e00\u79cd\u67e5\u8be2\u65e0\u5173\u7684KV\u7f13\u5b58\u9a71\u9010\u65b9\u6cd5\uff0c\u901a\u8fc7\u538b\u7f29\u548c\u91cd\u7528KV\u7f13\u5b58\u51cf\u5c11\u5185\u5b58\u5f00\u9500\u548c\u6ce8\u610f\u529b\u5ef6\u8fdf\uff0c\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\u3002", "motivation": "\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\uff0c\u57fa\u4e8eTransformer\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684KV\u7f13\u5b58\u5360\u7528\u5927\u91cf\u5185\u5b58\uff0c\u5bfc\u81f4\u5ef6\u8fdf\u589e\u52a0\uff0c\u73b0\u6709\u67e5\u8be2\u611f\u77e5\u65b9\u6cd5\u5728\u591a\u67e5\u8be2\u573a\u666f\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "method": "KVzip\u901a\u8fc7LLM\u91cf\u5316KV\u5bf9\u7684\u91cd\u8981\u6027\uff0c\u91cd\u6784\u539f\u59cb\u4e0a\u4e0b\u6587\u5e76\u9a71\u9010\u4f4e\u91cd\u8981\u6027\u7f13\u5b58\uff0c\u5b9e\u73b0\u67e5\u8be2\u65e0\u5173\u7684\u9ad8\u6548\u538b\u7f29\u3002", "result": "\u5b9e\u9a8c\u8868\u660eKVzip\u5c06KV\u7f13\u5b58\u5927\u5c0f\u51cf\u5c113-4\u500d\uff0cFlashAttention\u89e3\u7801\u5ef6\u8fdf\u964d\u4f4e\u7ea62\u500d\uff0c\u5728\u95ee\u7b54\u3001\u68c0\u7d22\u7b49\u4efb\u52a1\u4e2d\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "KVzip\u5728\u957f\u4e0a\u4e0b\u6587\u548c\u591a\u67e5\u8be2\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "keywords": "KV\u7f13\u5b58\u3001\u538b\u7f29\u3001\u67e5\u8be2\u65e0\u5173\u3001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u5185\u5b58\u4f18\u5316"}}
{"id": "2505.23431", "pdf": "https://arxiv.org/pdf/2505.23431", "abs": "https://arxiv.org/abs/2505.23431", "authors": ["Amer Krivo\u0161ija", "Alexander Munteanu", "Andr\u00e9 Nusser", "Chris Schwiegelshohn"], "title": "Improved Learning via k-DTW: A Novel Dissimilarity Measure for Curves", "categories": ["cs.DS", "cs.CG", "cs.LG", "stat.ML"], "comment": "ICML 2025", "summary": "This paper introduces $k$-Dynamic Time Warping ($k$-DTW), a novel\ndissimilarity measure for polygonal curves. $k$-DTW has stronger metric\nproperties than Dynamic Time Warping (DTW) and is more robust to outliers than\nthe Fr\\'{e}chet distance, which are the two gold standards of dissimilarity\nmeasures for polygonal curves. We show interesting properties of $k$-DTW and\ngive an exact algorithm as well as a $(1+\\varepsilon)$-approximation algorithm\nfor $k$-DTW by a parametric search for the $k$-th largest matched distance. We\nprove the first dimension-free learning bounds for curves and further learning\ntheoretic results. $k$-DTW not only admits smaller sample size than DTW for the\nproblem of learning the median of curves, where some factors depending on the\ncurves' complexity $m$ are replaced by $k$, but we also show a surprising\nseparation on the associated Rademacher and Gaussian complexities: $k$-DTW\nadmits strictly smaller bounds than DTW, by a factor $\\tilde\\Omega(\\sqrt{m})$\nwhen $k\\ll m$. We complement our theoretical findings with an experimental\nillustration of the benefits of using $k$-DTW for clustering and nearest\nneighbor classification.", "AI": {"tldr": "\u8bba\u6587\u5f15\u5165\u4e86$k$-DTW\uff0c\u4e00\u79cd\u65b0\u7684\u591a\u8fb9\u5f62\u66f2\u7ebf\u5dee\u5f02\u5ea6\u91cf\u65b9\u6cd5\uff0c\u6bd4DTW\u66f4\u5177\u5ea6\u91cf\u7279\u6027\u4e14\u6bd4Fr&#xe9;chet\u8ddd\u79bb\u66f4\u80fd\u62b5\u6297\u5f02\u5e38\u503c\uff0c\u63d0\u4f9b\u4e86\u7cbe\u786e\u548c\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5c55\u793a\u4e86\u5176\u4f18\u52bf\u3002", "motivation": "\u4e3a\u89e3\u51b3\u73b0\u6709\u5dee\u5f02\u5ea6\u91cf\u65b9\u6cd5\uff08\u5982DTW\u548cFrechet\u8ddd\u79bb\uff09\u5728\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u66f4\u5177\u9002\u5e94\u6027\u548c\u6027\u80fd\u7684$k$-DTW\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa$k$-DTW\u7b97\u6cd5\uff0c\u5305\u62ec\u7cbe\u786e\u7b97\u6cd5\u548c$(1+/epsilon)$-\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u641c\u7d22\u5904\u7406$k$\u6700\u5927\u5339\u914d\u8ddd\u79bb\uff0c\u5e76\u83b7\u5f97\u7ef4\u5ea6\u65e0\u5173\u7684\u5b66\u4e60\u8fb9\u754c\u3002", "result": "$k$-DTW\u5728\u66f2\u7ebf\u5b66\u4e60\u4e2d\u6837\u672c\u91cf\u66f4\u5c0f\uff0cRademacher\u548cGaussian\u590d\u6742\u5ea6\u4e25\u683c\u4f4e\u4e8eDTW\uff0c\u56e0\u5b50\u4e3a$/tildeOmega(/sqrt{m})$\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u805a\u7c7b\u548c\u5206\u7c7b\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "$k$-DTW\u5728\u591a\u8fb9\u5f62\u66f2\u7ebf\u5dee\u5f02\u5ea6\u91cf\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u7406\u8bba\u548c\u5b9e\u9645\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "keywords": "\u52a8\u6001\u65f6\u95f4\u89c4\u6574, \u591a\u8fb9\u5f62\u66f2\u7ebf, \u5ea6\u91cf\u5b66\u4e60, \u9c81\u68d2\u6027, \u8fd1\u4f3c\u7b97\u6cd5"}}
{"id": "2505.23445", "pdf": "https://arxiv.org/pdf/2505.23445", "abs": "https://arxiv.org/abs/2505.23445", "authors": ["Adrien Majka", "El-Mahdi El-Mhamdi"], "title": "The Strong, Weak and Benign Goodhart's law. An independence-free and paradigm-agnostic formalisation", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": "32 pages, 1 figure", "summary": "Goodhart's law is a famous adage in policy-making that states that ``When a\nmeasure becomes a target, it ceases to be a good measure''. As machine learning\nmodels and the optimisation capacity to train them grow, growing empirical\nevidence reinforced the belief in the validity of this law without however\nbeing formalised. Recently, a few attempts were made to formalise Goodhart's\nlaw, either by categorising variants of it, or by looking at how optimising a\nproxy metric affects the optimisation of an intended goal. In this work, we\nalleviate the simplifying independence assumption, made in previous works, and\nthe assumption on the learning paradigm made in most of them, to study the\neffect of the coupling between the proxy metric and the intended goal on\nGoodhart's law. Our results show that in the case of light tailed goal and\nlight tailed discrepancy, dependence does not change the nature of Goodhart's\neffect. However, in the light tailed goal and heavy tailed discrepancy case, we\nexhibit an example where over-optimisation occurs at a rate inversely\nproportional to the heavy tailedness of the discrepancy between the goal and\nthe metric. %", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86Goodhart\u5b9a\u5f8b\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5f62\u5f0f\u5316\uff0c\u7279\u522b\u662f\u4ee3\u7406\u6307\u6807\u4e0e\u76ee\u6807\u4e4b\u95f4\u7684\u8026\u5408\u5173\u7cfb\u5bf9\u5b9a\u5f8b\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4f9d\u8d56\u6027\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4e0d\u4f1a\u6539\u53d8\u6548\u5e94\uff0c\u4f46\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u4f18\u5316\u3002", "motivation": "Goodhart\u5b9a\u5f8b\u5728\u653f\u7b56\u5236\u5b9a\u4e2d\u5e7f\u4e3a\u4eba\u77e5\uff0c\u4f46\u7f3a\u4e4f\u5f62\u5f0f\u5316\u7684\u7406\u8bba\u652f\u6301\u3002\u968f\u7740\u673a\u5668\u5b66\u4e60\u7684\u53d1\u5c55\uff0c\u8fd9\u4e00\u73b0\u8c61\u5728\u4f18\u5316\u4ee3\u7406\u6307\u6807\u65f6\u5c24\u4e3a\u7a81\u51fa\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u653e\u677e\u72ec\u7acb\u6027\u5047\u8bbe\uff0c\u66f4\u6df1\u5165\u5730\u7406\u89e3\u4ee3\u7406\u6307\u6807\u4e0e\u76ee\u6807\u7684\u8026\u5408\u5982\u4f55\u5f71\u54cd\u8be5\u5b9a\u5f8b\u3002", "method": "\u7814\u7a76\u653e\u677e\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u7684\u72ec\u7acb\u6027\u5047\u8bbe\u548c\u5b66\u4e60\u8303\u5f0f\u5047\u8bbe\uff0c\u5206\u6790\u4ee3\u7406\u6307\u6807\u4e0e\u76ee\u6807\u4e4b\u95f4\u7684\u8026\u5408\u6548\u5e94\u3002\u901a\u8fc7\u4e0d\u540c\u5c3e\u90e8\u7279\u5f81\uff08\u8f7b\u5c3e\u548c\u91cd\u5c3e\uff09\u7684\u7ec4\u5408\uff0c\u63a2\u8ba8\u4e86\u4f9d\u8d56\u6027\u5bf9Goodhart\u6548\u5e94\u7684\u5f71\u54cd\u3002", "result": "\u5728\u8f7b\u5c3e\u76ee\u6807\u548c\u8f7b\u5c3e\u5dee\u5f02\u7684\u60c5\u5883\u4e0b\uff0c\u4f9d\u8d56\u6027\u4e0d\u4f1a\u6539\u53d8Goodhart\u6548\u5e94\u7684\u672c\u8d28\uff1b\u4f46\u5728\u8f7b\u5c3e\u76ee\u6807\u548c\u91cd\u5c3e\u5dee\u5f02\u7684\u60c5\u5883\u4e2d\uff0c\u8fc7\u5ea6\u4f18\u5316\u7684\u901f\u7387\u4e0e\u5dee\u5f02\u7684\u91cd\u5c3e\u6027\u6210\u53cd\u6bd4\u3002", "conclusion": "\u4ee3\u7406\u6307\u6807\u4e0e\u76ee\u6807\u7684\u8026\u5408\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u5bf9Goodhart\u6548\u5e94\u6709\u4e0d\u540c\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u91cd\u5c3e\u5dee\u5f02\u60c5\u51b5\u4e0b\uff0c\u53ef\u80fd\u5bfc\u81f4\u663e\u8457\u7684\u8fc7\u5ea6\u4f18\u5316\u95ee\u9898\u3002", "keywords": "Goodhart\u5b9a\u5f8b, \u673a\u5668\u5b66\u4e60, \u4ee3\u7406\u6307\u6807, \u4f18\u5316, \u91cd\u5c3e\u5206\u5e03"}}
{"id": "2505.23475", "pdf": "https://arxiv.org/pdf/2505.23475", "abs": "https://arxiv.org/abs/2505.23475", "authors": ["Ron Shapira Weber", "Shahar Ben Ishay", "Andrey Lavrinenko", "Shahaf E. Finder", "Oren Freifeld"], "title": "TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning", "categories": ["cs.CV", "cs.LG"], "comment": "ICML 2025", "summary": "Fast and scalable alignment of time series is a fundamental challenge in many\ndomains. The standard solution, Dynamic Time Warping (DTW), struggles with poor\nscalability and sensitivity to noise. We introduce TimePoint, a self-supervised\nmethod that dramatically accelerates DTW-based alignment while typically\nimproving alignment accuracy by learning keypoints and descriptors from\nsynthetic data. Inspired by 2D keypoint detection but carefully adapted to the\nunique challenges of 1D signals, TimePoint leverages efficient 1D\ndiffeomorphisms, which effectively model nonlinear time warping, to generate\nrealistic training data. This approach, along with fully convolutional and\nwavelet convolutional architectures, enables the extraction of informative\nkeypoints and descriptors. Applying DTW to these sparse representations yield\nmajor speedups and typically higher alignment accuracy than standard DTW\napplied to the full signals. TimePoint demonstrates strong generalization to\nreal-world time series when trained solely on synthetic data, and further\nimproves with fine-tuning on real data. Extensive experiments demonstrate that\nTimePoint consistently achieves faster and more accurate alignments than\nstandard DTW, making it a scalable solution for time-series analysis. Our code\nis available at https://github.com/BGU-CS-VIL/TimePoint", "AI": {"tldr": "TimePoint\u662f\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u5229\u7528\u5408\u6210\u6570\u636e\u5b66\u4e60\u5173\u952e\u70b9\u548c\u63cf\u8ff0\u7b26\uff0c\u663e\u8457\u52a0\u901fDTW\u5bf9\u9f50\u4e14\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edfDTW\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u566a\u58f0\u654f\u611f\u6027\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u54081D\u5fae\u5206\u540c\u80da\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u91c7\u7528\u5168\u5377\u79ef\u548c\u5c0f\u6ce2\u5377\u79ef\u67b6\u6784\u63d0\u53d6\u5173\u952e\u70b9\u4e0e\u63cf\u8ff0\u7b26\uff0c\u518d\u5e94\u7528DTW\u7a00\u758f\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTimePoint\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edfDTW\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "TimePoint\u4e3a\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217\u5bf9\u9f50, DTW, \u81ea\u76d1\u7763\u5b66\u4e60, \u5173\u952e\u70b9\u68c0\u6d4b, 1D\u4fe1\u53f7"}}
{"id": "2505.23509", "pdf": "https://arxiv.org/pdf/2505.23509", "abs": "https://arxiv.org/abs/2505.23509", "authors": ["Andrew Chang", "Yike Li", "Iran R. Roman", "David Poeppel"], "title": "Spectrotemporal Modulation: Efficient and Interpretable Feature Representation for Classifying Speech, Music, and Environmental Sounds", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "Interspeech 2025", "summary": "Audio DNNs have demonstrated impressive performance on various machine\nlistening tasks; however, most of their representations are computationally\ncostly and uninterpretable, leaving room for optimization. Here, we propose a\nnovel approach centered on spectrotemporal modulation (STM) features, a signal\nprocessing method that mimics the neurophysiological representation in the\nhuman auditory cortex. The classification performance of our STM-based model,\nwithout any pretraining, is comparable to that of pretrained audio DNNs across\ndiverse naturalistic speech, music, and environmental sounds, which are\nessential categories for both human cognition and machine perception. These\nresults show that STM is an efficient and interpretable feature representation\nfor audio classification, advancing the development of machine listening and\nunlocking exciting new possibilities for basic understanding of speech and\nauditory sciences, as well as developing audio BCI and cognitive computing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u8c31\u65f6\u57df\u8c03\u5236\uff08STM\uff09\u7279\u5f81\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u97f3\u9891\u5206\u7c7b\uff0c\u5176\u6027\u80fd\u4e0e\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u76f8\u5f53\uff0c\u4e14\u5177\u6709\u9ad8\u6548\u548c\u53ef\u89e3\u91ca\u7684\u4f18\u52bf\u3002", "motivation": "\u5f53\u524d\u97f3\u9891\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4e0d\u53ef\u89e3\u91ca\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u53ef\u7406\u89e3\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u9891\u8c31\u65f6\u57df\u8c03\u5236\uff08STM\uff09\u7279\u5f81\uff0c\u6a21\u4eff\u4eba\u7c7b\u542c\u89c9\u76ae\u5c42\u7684\u795e\u7ecf\u751f\u7406\u5b66\u8868\u793a\uff0c\u6784\u5efa\u5206\u7c7b\u6a21\u578b\u3002", "result": "STM\u6a21\u578b\u5728\u672a\u9884\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u81ea\u7136\u8bed\u97f3\u3001\u97f3\u4e50\u548c\u73af\u5883\u58f0\u97f3\u7684\u5206\u7c7b\u6027\u80fd\u4e0e\u9884\u8bad\u7ec3DNNs\u76f8\u5f53\u3002", "conclusion": "STM\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u97f3\u9891\u7279\u5f81\u8868\u793a\u65b9\u6cd5\uff0c\u4e3a\u673a\u5668\u542c\u89c9\u548c\u8bed\u97f3\u79d1\u5b66\u7684\u57fa\u7840\u7814\u7a76\u4ee5\u53ca\u8111\u673a\u63a5\u53e3\u548c\u8ba4\u77e5\u8ba1\u7b97\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002", "keywords": "\u9891\u8c31\u65f6\u57df\u8c03\u5236\uff08STM\uff09\u3001\u97f3\u9891\u5206\u7c7b\u3001\u53ef\u89e3\u91ca\u6027\u3001\u673a\u5668\u542c\u89c9\u3001\u8111\u673a\u63a5\u53e3"}}
{"id": "2505.23515", "pdf": "https://arxiv.org/pdf/2505.23515", "abs": "https://arxiv.org/abs/2505.23515", "authors": ["Sanberk Serbest", "Tijana Stojkovic", "Milos Cernak", "Andrew Harper"], "title": "DeepFilterGAN: A Full-band Real-time Speech Enhancement System with GAN-based Stochastic Regeneration", "categories": ["eess.AS", "cs.LG", "eess.SP"], "comment": "Accepted to Interspeech 2025", "summary": "In this work, we propose a full-band real-time speech enhancement system with\nGAN-based stochastic regeneration. Predictive models focus on estimating the\nmean of the target distribution, whereas generative models aim to learn the\nfull distribution. This behavior of predictive models may lead to\nover-suppression, i.e. the removal of speech content. In the literature, it was\nshown that combining a predictive model with a generative one within the\nstochastic regeneration framework can reduce the distortion in the output. We\nuse this framework to obtain a real-time speech enhancement system. With 3.58M\nparameters and a low latency, our system is designed for real-time streaming\nwith a lightweight architecture. Experiments show that our system improves over\nthe first stage in terms of NISQA-MOS metric. Finally, through an ablation\nstudy, we show the importance of noisy conditioning in our system. We\nparticipated in 2025 Urgent Challenge with our model and later made further\nimprovements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGAN\u968f\u673a\u518d\u751f\u7684\u5b9e\u65f6\u5168\u9891\u5e26\u8bed\u97f3\u589e\u5f3a\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u9884\u6d4b\u6a21\u578b\u4e0e\u751f\u6210\u6a21\u578b\u51cf\u5c11\u5931\u771f\uff0c\u5b9e\u9a8c\u663e\u793a\u7cfb\u7edf\u5728NISQA-MOS\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9884\u6d4b\u6a21\u578b\u53ef\u80fd\u56e0\u8fc7\u5ea6\u6291\u5236\u5bfc\u81f4\u8bed\u97f3\u5185\u5bb9\u4e22\u5931\uff0c\u7ed3\u5408\u751f\u6210\u6a21\u578b\u5b66\u4e60\u5b8c\u6574\u5206\u5e03\u4ee5\u51cf\u5c11\u5931\u771f\u3002", "method": "\u57fa\u4e8eGAN\u7684\u968f\u673a\u518d\u751f\u6846\u67b6\u6784\u5efa\u8f7b\u91cf\u7ea7\u5b9e\u65f6\u7cfb\u7edf\uff0c\u53c2\u6570\u91cf\u4e3a3.58M\u4e14\u4f4e\u5ef6\u8fdf\u3002", "result": "\u7cfb\u7edf\u5728NISQA-MOS\u6307\u6807\u4e0a\u4f18\u4e8e\u521d\u59cb\u9636\u6bb5\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u566a\u58f0\u6761\u4ef6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u8f7b\u91cf\u6846\u67b6\u5728\u5b9e\u65f6\u8bed\u97f3\u589e\u5f3a\u4e2d\u8868\u73b0\u9ad8\u6548\uff0c\u8fdb\u4e00\u6b65\u6539\u8fdb\u540e\u53c2\u4e0e2025 Urgent Challenge\u3002", "keywords": "\u8bed\u97f3\u589e\u5f3a, GAN, \u968f\u673a\u518d\u751f, \u5b9e\u65f6\u7cfb\u7edf, \u4f4e\u5ef6\u8fdf"}}
{"id": "2505.23522", "pdf": "https://arxiv.org/pdf/2505.23522", "abs": "https://arxiv.org/abs/2505.23522", "authors": ["Fengxiang Wang", "Mingshuo Chen", "Xuming He", "YiFan Zhang", "Feng Liu", "Zijie Guo", "Zhenghao Hu", "Jiong Wang", "Jingyi Xu", "Zhangrui Li", "Fenghua Ling", "Ben Fei", "Weijia Li", "Long Lan", "Wenjing Yang", "Wenlong Zhang", "Lei Bai"], "title": "OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Existing benchmarks for Earth science multimodal learning exhibit critical\nlimitations in systematic coverage of geosystem components and cross-sphere\ninteractions, often constrained to isolated subsystems (only in\nHuman-activities sphere or atmosphere) with limited evaluation dimensions (less\nthan 16 tasks). To address these gaps, we introduce OmniEarth-Bench, the first\ncomprehensive multimodal benchmark spanning all six Earth science spheres\n(atmosphere, lithosphere, Oceansphere, cryosphere, biosphere and\nHuman-activities sphere) and cross-spheres with one hundred expert-curated\nevaluation dimensions. Leveraging observational data from satellite sensors and\nin-situ measurements, OmniEarth-Bench integrates 29,779 annotations across four\ntiers: perception, general reasoning, scientific knowledge reasoning and\nchain-of-thought (CoT) reasoning. This involves the efforts of 2-5 experts per\nsphere to establish authoritative evaluation dimensions and curate relevant\nobservational datasets, 40 crowd-sourcing annotators to assist experts for\nannotations, and finally, OmniEarth-Bench is validated via hybrid expert-crowd\nworkflows to reduce label ambiguity. Experiments on 9 state-of-the-art MLLMs\nreveal that even the most advanced models struggle with our benchmarks, where\nnone of them reach 35\\% accuracy. Especially, in some cross-spheres tasks, the\nperformance of leading models like GPT-4o drops to 0.0\\%. OmniEarth-Bench sets\na new standard for geosystem-aware AI, advancing both scientific discovery and\npractical applications in environmental monitoring and disaster prediction. The\ndataset, source code, and trained models were released.", "AI": {"tldr": "OmniEarth-Bench\u662f\u4e00\u4e2a\u5168\u9762\u7684\u5730\u7403\u79d1\u5b66\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u6db5\u76d6\u516d\u4e2a\u5730\u7403\u79d1\u5b66\u9886\u57df\u548c\u8de8\u9886\u57df\u4ea4\u4e92\uff0c\u5305\u542b100\u4e2a\u4e13\u5bb6\u7b56\u5212\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u65e8\u5728\u586b\u8865\u73b0\u6709\u57fa\u51c6\u5728\u7cfb\u7edf\u548c\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u5730\u7403\u79d1\u5b66\u591a\u6a21\u6001\u5b66\u4e60\u57fa\u51c6\u5728\u8986\u76d6\u5730\u7403\u7cfb\u7edf\u7ec4\u4ef6\u548c\u8de8\u9886\u57df\u4ea4\u4e92\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u4e0d\u8db3\uff0c\u4ec5\u6d89\u53ca\u5b64\u7acb\u5b50\u7cfb\u7edf\u4e14\u8bc4\u4f30\u7ef4\u5ea6\u6709\u9650\uff0cOmniEarth-Bench\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5dee\u8ddd\u3002", "method": "\u6574\u5408\u536b\u661f\u4f20\u611f\u5668\u548c\u73b0\u573a\u89c2\u6d4b\u6570\u636e\uff0c\u6db5\u76d6\u611f\u77e5\u3001\u901a\u7528\u63a8\u7406\u3001\u79d1\u5b66\u77e5\u8bc6\u63a8\u7406\u548c\u94fe\u5f0f\u63a8\u7406\u56db\u4e2a\u5c42\u6b21\uff0c\u7531\u4e13\u5bb6\u548c\u4f17\u5305\u6807\u6ce8\u8005\u5171\u540c\u5b8c\u621029,779\u4e2a\u6807\u6ce8\u3002", "result": "\u57289\u4e2a\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e0a\u6d4b\u8bd5\uff0c\u5373\u4f7f\u662f\u6027\u80fd\u6700\u4f73\u7684\u6a21\u578b\u4e5f\u8868\u73b0\u4e0d\u4f73\uff0c\u51c6\u786e\u7387\u4e0d\u8db335%\uff0c\u67d0\u4e9b\u8de8\u9886\u57df\u4efb\u52a1\u4e2dGPT-4o\u7684\u51c6\u786e\u7387\u964d\u81f30.0%\u3002", "conclusion": "OmniEarth-Bench\u4e3a\u5730\u5b66\u611f\u77e5AI\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u63a8\u52a8\u4e86\u79d1\u5b66\u53d1\u73b0\u53ca\u73af\u5883\u76d1\u6d4b\u548c\u707e\u5bb3\u9884\u6d4b\u7684\u5b9e\u9645\u5e94\u7528\u3002", "keywords": "\u5730\u7403\u79d1\u5b66, \u591a\u6a21\u6001\u5b66\u4e60, \u57fa\u51c6\u6d4b\u8bd5, \u8de8\u9886\u57df\u4ea4\u4e92, AI\u8bc4\u4f30"}}
{"id": "2505.23557", "pdf": "https://arxiv.org/pdf/2505.23557", "abs": "https://arxiv.org/abs/2505.23557", "authors": ["Marc Jourdan", "Gizem Y\u00fcce", "Nicolas Flammarion"], "title": "Learning Parametric Distributions from Samples and Preferences", "categories": ["stat.ML", "cs.LG"], "comment": "28 pages, 8 figures. To be published in the Forty-Second\n  International Conference on Machine Learning", "summary": "Recent advances in language modeling have underscored the role of preference\nfeedback in enhancing model performance. This paper investigates the conditions\nunder which preference feedback improves parameter estimation in classes of\ncontinuous parametric distributions. In our framework, the learner observes\npairs of samples from an unknown distribution along with their relative\npreferences depending on the same unknown parameter. We show that\npreference-based M-estimators achieve a better asymptotic variance than\nsample-only M-estimators, further improved by deterministic preferences.\nLeveraging the hard constraints revealed by deterministic preferences, we\npropose an estimator achieving an estimation error scaling of\n$\\mathcal{O}(1/n)$ -- a significant improvement over the $\\Theta(1/\\sqrt{n})$\nrate attainable with samples alone. Next, we establish a lower bound that\nmatches this accelerated rate; up to dimension and problem-dependent constants.\nWhile the assumptions underpinning our analysis are restrictive, they are\nsatisfied by notable cases such as Gaussian or Laplace distributions for\npreferences based on the log-probability reward.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u504f\u597d\u53cd\u9988\u5728\u8fde\u7eed\u53c2\u6570\u5206\u5e03\u7c7b\u4e2d\u63d0\u5347\u53c2\u6570\u4f30\u8ba1\u6027\u80fd\u7684\u6761\u4ef6\uff0c\u5c55\u793a\u4e86\u57fa\u4e8e\u504f\u597d\u7684M\u4f30\u8ba1\u6bd4\u4ec5\u4f9d\u8d56\u6837\u672c\u7684M\u4f30\u8ba1\u5177\u6709\u66f4\u4f18\u7684\u6e10\u8fd1\u65b9\u5dee\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f30\u8ba1\u5668\uff0c\u5176\u8bef\u5dee\u6536\u655b\u901f\u5ea6\u4e3aO(1/n)\u3002", "motivation": "\u63a2\u8ba8\u504f\u597d\u53cd\u9988\u5982\u4f55\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u4f18\u5316\u53c2\u6570\u4f30\u8ba1\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u8fde\u7eed\u53c2\u6570\u5206\u5e03\u7c7b\u4e2d\u3002", "method": "\u901a\u8fc7\u504f\u597d\u9a71\u52a8\u7684M\u4f30\u8ba1\u6846\u67b6\uff0c\u5206\u6790\u6837\u672c\u5bf9\u53ca\u5176\u76f8\u5bf9\u504f\u597d\u5bf9\u53c2\u6570\u4f30\u8ba1\u7684\u5f71\u54cd\uff0c\u7279\u522b\u5173\u6ce8\u786e\u5b9a\u6027\u504f\u597d\u63ed\u793a\u7684\u786c\u7ea6\u675f\u6761\u4ef6\u3002", "result": "\u63d0\u51fa\u7684\u4f30\u8ba1\u5668\u5b9e\u73b0\u4e86O(1/n)\u7684\u8bef\u5dee\u6536\u655b\u901f\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u6837\u672c\u65f6\u7684\u0398(1/\u221an)\u901f\u5ea6\uff0c\u5e76\u5efa\u7acb\u4e86\u5339\u914d\u8be5\u52a0\u901f\u901f\u5ea6\u7684\u4e0b\u754c\u3002", "conclusion": "\u5728\u7279\u5b9a\u5047\u8bbe\u4e0b\uff08\u5982\u9ad8\u65af\u6216\u62c9\u666e\u62c9\u65af\u5206\u5e03\uff09\uff0c\u504f\u597d\u53cd\u9988\u80fd\u663e\u8457\u63d0\u5347\u53c2\u6570\u4f30\u8ba1\u6548\u7387\uff0c\u4f46\u5047\u8bbe\u6761\u4ef6\u8f83\u4e3a\u4e25\u683c\u3002", "keywords": "\u504f\u597d\u53cd\u9988, M\u4f30\u8ba1, \u53c2\u6570\u4f30\u8ba1, \u6e10\u8fd1\u65b9\u5dee, \u6536\u655b\u901f\u5ea6"}}
{"id": "2505.23558", "pdf": "https://arxiv.org/pdf/2505.23558", "abs": "https://arxiv.org/abs/2505.23558", "authors": ["Xu Chu", "Xinrong Chen", "Guanyu Wang", "Zhijie Tan", "Kui Huang", "Wenyu Lv", "Tong Mo", "Weiping Li"], "title": "Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Inference time scaling drives extended reasoning to enhance the performance\nof Vision-Language Models (VLMs), thus forming powerful Vision-Language\nReasoning Models (VLRMs). However, long reasoning dilutes visual tokens,\ncausing visual information to receive less attention and may trigger\nhallucinations. Although introducing text-only reflection processes shows\npromise in language models, we demonstrate that it is insufficient to suppress\nhallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain\n(Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a\nvision-text reflection process that guides the model to re-attention visual\ninformation during reasoning. We first propose a reinforcement learning method\nBalanced Reflective Policy Optimization (BRPO), which guides the model to\ndecide when to generate vision-text reflection on its own and balance the\nnumber and length of reflections. Then, we formally prove that VLRMs lose\nattention to visual tokens as reasoning progresses, and demonstrate that\nsupplementing visual information during reflection enhances visual attention.\nTherefore, during training and inference, Visual Token COPY and Visual Token\nROUTE are introduced to force the model to re-attention visual information at\nthe visual level, addressing the limitations of text-only reflection.\nExperiments on multiple visual QA datasets and hallucination metrics indicate\nthat Qwen-LA achieves leading accuracy performance while reducing\nhallucinations. Our code is available at:\nhttps://github.com/Liar406/Look_Again.", "AI": {"tldr": "Qwen-LA\u662f\u4e00\u79cd\u65b0\u578b\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u6a21\u578b(VLRM)\uff0c\u901a\u8fc7\u5f15\u5165\u89c6\u89c9-\u6587\u672c\u53cd\u601d\u8fc7\u7a0b\u6765\u51cf\u5c11\u5e7b\u89c9\uff0c\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u53cd\u601d\u7b56\u7565\uff0c\u63d0\u5347\u89c6\u89c9\u6ce8\u610f\u529b\u3002", "motivation": "\u957f\u63a8\u7406\u8fc7\u7a0b\u4f1a\u7a00\u91ca\u89c6\u89c9\u6807\u8bb0\uff0c\u5bfc\u81f4\u89c6\u89c9\u4fe1\u606f\u88ab\u5ffd\u89c6\u5e76\u53ef\u80fd\u5f15\u53d1\u5e7b\u89c9\uff0c\u4ec5\u4f9d\u9760\u6587\u672c\u53cd\u601d\u4e0d\u8db3\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5e73\u8861\u53cd\u5c04\u7b56\u7565\u4f18\u5316(BRPO)\u548c\u89c6\u89c9\u6807\u8bb0COPY/ROUTE\u673a\u5236\uff0c\u5f3a\u5236\u6a21\u578b\u5728\u63a8\u7406\u4e2d\u91cd\u65b0\u5173\u6ce8\u89c6\u89c9\u4fe1\u606f\u3002", "result": "Qwen-LA\u5728\u591a\u4e2a\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\u548c\u5e7b\u89c9\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u9886\u5148\u7684\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u5e7b\u89c9\u3002", "conclusion": "\u89c6\u89c9-\u6587\u672c\u53cd\u601d\u548c\u89c6\u89c9\u6807\u8bb0\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u6ce8\u610f\u529b\u5e76\u51cf\u5c11\u5e7b\u89c9\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u6a21\u578b, \u5e7b\u89c9\u6291\u5236, \u5f3a\u5316\u5b66\u4e60, \u89c6\u89c9\u6ce8\u610f\u529b"}}
{"id": "2505.23587", "pdf": "https://arxiv.org/pdf/2505.23587", "abs": "https://arxiv.org/abs/2505.23587", "authors": ["Christian Schmidt", "Heinrich Martin Overhoff"], "title": "PCA for Enhanced Cross-Dataset Generalizability in Breast Ultrasound Tumor Segmentation", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "In medical image segmentation, limited external validity remains a critical\nobstacle when models are deployed across unseen datasets, an issue particularly\npronounced in the ultrasound image domain. Existing solutions-such as domain\nadaptation and GAN-based style transfer-while promising, often fall short in\nthe medical domain where datasets are typically small and diverse. This paper\npresents a novel application of principal component analysis (PCA) to address\nthis limitation. PCA preprocessing reduces noise and emphasizes essential\nfeatures by retaining approximately 90\\% of the dataset variance. We evaluate\nour approach across six diverse breast tumor ultrasound datasets comprising\n3,983 B-mode images and corresponding expert tumor segmentation masks. For each\ndataset, a corresponding dimensionality reduced PCA-dataset is created and\nU-Net-based segmentation models are trained on each of the twelve datasets.\nEach model trained on an original dataset was inferenced on the remaining five\nout-of-domain original datasets (baseline results), while each model trained on\na PCA dataset was inferenced on five out-of-domain PCA datasets. Our\nexperimental results indicate that using PCA reconstructed datasets, instead of\noriginal images, improves the model's recall and Dice scores, particularly for\nmodel-dataset pairs where baseline performance was lowest, achieving\nstatistically significant gains in recall (0.57 $\\pm$ 0.07 vs. 0.70 $\\pm$ 0.05,\n$p = 0.0004$) and Dice scores (0.50 $\\pm$ 0.06 vs. 0.58 $\\pm$ 0.06, $p =\n0.03$). Our method reduced the decline in recall values due to external\nvalidation by $33\\%$. These findings underscore the potential of PCA\nreconstruction as a safeguard to mitigate declines in segmentation performance,\nespecially in challenging cases, with implications for enhancing external\nvalidity in real-world medical applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u4e3b\u6210\u5206\u5206\u6790(PCA)\u9884\u5904\u7406\u533b\u5b66\u8d85\u58f0\u56fe\u50cf\u4ee5\u63d0\u5347\u5206\u5272\u6a21\u578b\u7684\u5916\u90e8\u6548\u5ea6\uff0c\u5b9e\u9a8c\u8868\u660ePCA\u80fd\u663e\u8457\u63d0\u9ad8\u53ec\u56de\u7387\u548cDice\u5206\u6570\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u8de8\u6570\u636e\u96c6\u90e8\u7f72\u65f6\u5916\u90e8\u6548\u5ea6\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u91cf\u5c0f\u3001\u591a\u6837\u6027\u9ad8\u7684\u533b\u5b66\u9886\u57df\u6548\u679c\u6709\u9650\u3002", "method": "\u5229\u7528PCA\u9884\u5904\u7406\u56fe\u50cf\uff0c\u4fdd\u755990%\u65b9\u5dee\u4ee5\u964d\u566a\u5e76\u5f3a\u5316\u5173\u952e\u7279\u5f81\uff0c\u57fa\u4e8eU-Net\u57286\u4e2a\u4e73\u817a\u80bf\u7624\u8d85\u58f0\u6570\u636e\u96c6\u4e0a\u5bf9\u6bd4\u539f\u59cb\u6570\u636e\u4e0ePCA\u6570\u636e\u7684\u6a21\u578b\u8868\u73b0\u3002", "result": "PCA\u91cd\u5efa\u6570\u636e\u4f7f\u53ec\u56de\u7387(0.70\u00b10.05 vs. 0.57\u00b10.07)\u548cDice\u5206\u6570(0.58\u00b10.06 vs. 0.50\u00b10.06)\u663e\u8457\u63d0\u5347\uff0c\u5916\u90e8\u9a8c\u8bc1\u5bfc\u81f4\u7684\u53ec\u56de\u7387\u4e0b\u964d\u51cf\u5c1133%\u3002", "conclusion": "PCA\u91cd\u5efa\u80fd\u6709\u6548\u7f13\u89e3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u4e0b\u964d\uff0c\u63d0\u5347\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5916\u90e8\u6548\u5ea6\u3002", "keywords": "\u533b\u5b66\u56fe\u50cf\u5206\u5272,\u4e3b\u6210\u5206\u5206\u6790,\u5916\u90e8\u6548\u5ea6,\u8d85\u58f0\u56fe\u50cf,U-Net"}}
{"id": "2505.23594", "pdf": "https://arxiv.org/pdf/2505.23594", "abs": "https://arxiv.org/abs/2505.23594", "authors": ["Xi Chen", "Soham Jana", "Christopher A. Metzler", "Arian Maleki", "Shirin Jalali"], "title": "Multilook Coherent Imaging: Theoretical Guarantees and Algorithms", "categories": ["stat.ML", "cs.LG", "eess.IV"], "comment": "29 pages, 4 figures, 3 tables. arXiv admin note: substantial text\n  overlap with arXiv:2402.15635", "summary": "Multilook coherent imaging is a widely used technique in applications such as\ndigital holography, ultrasound imaging, and synthetic aperture radar. A central\nchallenge in these systems is the presence of multiplicative noise, commonly\nknown as speckle, which degrades image quality. Despite the widespread use of\ncoherent imaging systems, their theoretical foundations remain relatively\nunderexplored. In this paper, we study both the theoretical and algorithmic\naspects of likelihood-based approaches for multilook coherent imaging,\nproviding a rigorous framework for analysis and method development. Our\ntheoretical contributions include establishing the first theoretical upper\nbound on the Mean Squared Error (MSE) of the maximum likelihood estimator under\nthe deep image prior hypothesis. Our results capture the dependence of MSE on\nthe number of parameters in the deep image prior, the number of looks, the\nsignal dimension, and the number of measurements per look. On the algorithmic\nside, we employ projected gradient descent (PGD) as an efficient method for\ncomputing the maximum likelihood solution. Furthermore, we introduce two key\nideas to enhance the practical performance of PGD. First, we incorporate the\nNewton-Schulz algorithm to compute matrix inverses within the PGD iterations,\nsignificantly reducing computational complexity. Second, we develop a bagging\nstrategy to mitigate projection errors introduced during PGD updates. We\ndemonstrate that combining these techniques with PGD yields state-of-the-art\nperformance. Our code is available at\nhttps://github.com/Computational-Imaging-RU/Bagged-DIP-Speckle.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u89c6\u76f8\u5e72\u6210\u50cf\u4e2d\u7684\u4f3c\u7136\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u7406\u8bba\u4e0a\u754c\uff0c\u5e76\u91c7\u7528\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\uff08PGD\uff09\u53ca\u5176\u6539\u8fdb\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u6548\u6c42\u89e3\u3002", "motivation": "\u591a\u89c6\u76f8\u5e72\u6210\u50cf\uff08\u5982\u6570\u5b57\u5168\u606f\u3001\u8d85\u58f0\u6210\u50cf\u548c\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff09\u4e2d\u666e\u904d\u5b58\u5728\u4e58\u6027\u566a\u58f0\uff08\u6563\u6591\uff09\uff0c\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\u3002\u76ee\u524d\u5176\u7406\u8bba\u57fa\u7840\u7814\u7a76\u4e0d\u8db3\uff0c\u4f5c\u8005\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "1. \u5efa\u7acb\u4e86\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5728\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\u5047\u8bbe\u4e0b\u7684MSE\u7406\u8bba\u4e0a\u754c\uff1b2. \u4f7f\u7528PGD\u8ba1\u7b97\u6700\u5927\u4f3c\u7136\u89e3\uff1b3. \u5f15\u5165\u725b\u987f-\u8212\u5c14\u8328\u7b97\u6cd5\u964d\u4f4e\u77e9\u9635\u6c42\u9006\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b4. \u63d0\u51fa\u88c5\u888b\u7b56\u7565\u51cf\u5c11\u6295\u5f71\u8bef\u5dee\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728MSE\u7406\u8bba\u4e0a\u754c\u548c\u7b97\u6cd5\u6548\u7387\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u7ed3\u5408\u6539\u8fdbPGD\u5b9e\u73b0\u4e86\u5f53\u524d\u6700\u4f18\u6027\u80fd\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "\u8bba\u6587\u4e3a\u591a\u89c6\u76f8\u5e72\u6210\u50cf\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u7b97\u6cd5\u6846\u67b6\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u7b97\u6cd5\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "keywords": "\u591a\u89c6\u76f8\u5e72\u6210\u50cf, \u6563\u6591\u566a\u58f0, \u6700\u5927\u4f3c\u7136\u4f30\u8ba1, \u6295\u5f71\u68af\u5ea6\u4e0b\u964d, \u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c"}}
{"id": "2505.23619", "pdf": "https://arxiv.org/pdf/2505.23619", "abs": "https://arxiv.org/abs/2505.23619", "authors": ["Neta Glazer", "David Chernin", "Idan Achituve", "Sharon Gannot", "Ethan Fetaya"], "title": "Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": null, "summary": "Recent advancements in Text-to-Speech (TTS) models, particularly in voice\ncloning, have intensified the demand for adaptable and efficient deepfake\ndetection methods. As TTS systems continue to evolve, detection models must be\nable to efficiently adapt to previously unseen generation models with minimal\ndata. This paper introduces ADD-GP, a few-shot adaptive framework based on a\nGaussian Process (GP) classifier for Audio Deepfake Detection (ADD). We show\nhow the combination of a powerful deep embedding model with the Gaussian\nprocesses flexibility can achieve strong performance and adaptability.\nAdditionally, we show this approach can also be used for personalized\ndetection, with greater robustness to new TTS models and one-shot adaptability.\nTo support our evaluation, a benchmark dataset is constructed for this task\nusing new state-of-the-art voice cloning models.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u7684\u5c11\u6837\u672c\u81ea\u9002\u5e94\u6846\u67b6ADD-GP\uff0c\u7528\u4e8e\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\uff08ADD\uff09\uff0c\u80fd\u591f\u9ad8\u6548\u9002\u5e94\u65b0\u6a21\u578b\u5e76\u652f\u6301\u4e2a\u6027\u5316\u68c0\u6d4b\u3002", "motivation": "\u968f\u7740TTS\u6280\u672f\u7684\u53d1\u5c55\uff0c\u68c0\u6d4b\u6a21\u578b\u9700\u8981\u9ad8\u6548\u9002\u5e94\u65b0\u751f\u6210\u7684\u8bed\u97f3\u6a21\u578b\uff0c\u4e14\u9700\u6570\u636e\u91cf\u6700\u5c0f\u5316\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5d4c\u5165\u6a21\u578b\u4e0e\u9ad8\u65af\u8fc7\u7a0b\u5206\u7c7b\u5668\u7684\u7075\u6d3b\u6027\uff0c\u6784\u5efaADD-GP\u6846\u67b6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0e\u9002\u5e94\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u652f\u6301\u5bf9\u65b0TTS\u6a21\u578b\u7684\u4e00\u53d1\u9002\u5e94\u3002", "conclusion": "ADD-GP\u4e3a\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "Text-to-Speech, voice cloning, deepfake detection, Gaussian Process, few-shot learning"}}
{"id": "2505.23620", "pdf": "https://arxiv.org/pdf/2505.23620", "abs": "https://arxiv.org/abs/2505.23620", "authors": ["Jiayuan Ye", "Vitaly Feldman", "Kunal Talwar"], "title": "Instance-Optimality for Private KL Distribution Estimation", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We study the fundamental problem of estimating an unknown discrete\ndistribution $p$ over $d$ symbols, given $n$ i.i.d. samples from the\ndistribution. We are interested in minimizing the KL divergence between the\ntrue distribution and the algorithm's estimate. We first construct minimax\noptimal private estimators. Minimax optimality however fails to shed light on\nan algorithm's performance on individual (non-worst-case) instances $p$ and\nsimple minimax-optimal DP estimators can have poor empirical performance on\nreal distributions. We then study this problem from an instance-optimality\nviewpoint, where the algorithm's error on $p$ is compared to the minimum\nachievable estimation error over a small local neighborhood of $p$. Under\nnatural notions of local neighborhood, we propose algorithms that achieve\ninstance-optimality up to constant factors, with and without a differential\nprivacy constraint. Our upper bounds rely on (private) variants of the\nGood-Turing estimator. Our lower bounds use additive local neighborhoods that\nmore precisely captures the hardness of distribution estimation in KL\ndivergence, compared to ones considered in prior works.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u79bb\u6563\u5206\u5e03\u7684KL\u6563\u5ea6\u4f30\u8ba1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5b9e\u4f8b\u6700\u4f18\u6027\u548c\u5dee\u5206\u9690\u79c1\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u79bb\u6563\u5206\u5e03\u4f30\u8ba1\u7684\u6700\u5c0f\u5316KL\u6563\u5ea6\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5dee\u5206\u9690\u79c1\u7ea6\u675f\u4e0b\uff0c\u4ee5\u5f25\u8865\u4f20\u7edf\u6781\u5c0f\u6781\u5927\u6700\u4f18\u65b9\u6cd5\u5728\u5177\u4f53\u5b9e\u4f8b\u8868\u73b0\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\uff08\u9690\u79c1\u4fdd\u62a4\u7684\uff09Good-Turing\u4f30\u8ba1\u5668\u53d8\u4f53\uff0c\u5e76\u5728\u5c40\u90e8\u90bb\u57df\u4e0b\u5b9a\u4e49\u4e86\u5b9e\u4f8b\u6700\u4f18\u6027\u6807\u51c6\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5b9e\u4f8b\u6700\u4f18\u6027\u4e0a\u8fbe\u5230\u5e38\u6570\u500d\u8bef\u5dee\uff0c\u4e14\u9002\u7528\u4e8e\u5dee\u5206\u9690\u79c1\u548c\u975e\u9690\u79c1\u573a\u666f\u3002", "conclusion": "\u901a\u8fc7\u5c40\u90e8\u90bb\u57df\u5b9a\u4e49\u548c\u9690\u79c1\u4fdd\u62a4\u53d8\u4f53\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u5206\u5e03\u4f30\u8ba1\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "keywords": "\u79bb\u6563\u5206\u5e03\u4f30\u8ba1, KL\u6563\u5ea6, \u5b9e\u4f8b\u6700\u4f18\u6027, \u5dee\u5206\u9690\u79c1, Good-Turing\u4f30\u8ba1\u5668"}}
{"id": "2505.23652", "pdf": "https://arxiv.org/pdf/2505.23652", "abs": "https://arxiv.org/abs/2505.23652", "authors": ["Yuehaw Khoo", "Mathias Oster", "Yifan Peng"], "title": "Optimization-Free Diffusion Model -- A Perturbation Theory Approach", "categories": ["math.NA", "cs.LG", "cs.NA"], "comment": "36 pages, 6 figures", "summary": "Diffusion models have emerged as a powerful framework in generative modeling,\ntypically relying on optimizing neural networks to estimate the score function\nvia forward SDE simulations. In this work, we propose an alternative method\nthat is both optimization-free and forward SDE-free. By expanding the score\nfunction in a sparse set of eigenbasis of the backward Kolmogorov operator\nassociated with the diffusion process, we reformulate score estimation as the\nsolution to a linear system, avoiding iterative optimization and time-dependent\nsample generation. We analyze the approximation error using perturbation theory\nand demonstrate the effectiveness of our method on high-dimensional Boltzmann\ndistributions and real-world datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4f18\u5316\u548c\u524d\u5411SDE\u7684\u6269\u6563\u6a21\u578b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u7279\u5f81\u57fa\u5c55\u5f00\u5206\u6570\u51fd\u6570\uff0c\u907f\u514d\u4e86\u8fed\u4ee3\u4f18\u5316\u548c\u65f6\u53d8\u6837\u672c\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u4f9d\u8d56\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u548c\u6b63\u5411SDE\u6a21\u62df\uff0c\u8ba1\u7b97\u590d\u6742\u4e14\u8017\u65f6\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u5206\u6570\u51fd\u6570\u5c55\u5f00\u5230\u6269\u6563\u8fc7\u7a0b\u5bf9\u5e94\u7684\u540e\u5411Kolmogorov\u7b97\u5b50\u7684\u7a00\u758f\u7279\u5f81\u57fa\u4e2d\uff0c\u5c06\u5206\u6570\u4f30\u8ba1\u8f6c\u5316\u4e3a\u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3\u95ee\u9898\uff0c\u907f\u514d\u4e86\u4f18\u5316\u548c\u65f6\u53d8\u6837\u672c\u751f\u6210\u3002", "result": "\u901a\u8fc7\u6270\u52a8\u7406\u8bba\u5206\u6790\u8fd1\u4f3c\u8bef\u5dee\uff0c\u5e76\u5728\u9ad8\u7ef4Boltzmann\u5206\u5e03\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8ba1\u7b97\u74f6\u9888\u3002", "keywords": "\u6269\u6563\u6a21\u578b, \u5206\u6570\u51fd\u6570, Kolmogorov\u7b97\u5b50, \u7a00\u758f\u7279\u5f81\u57fa, \u7ebf\u6027\u7cfb\u7edf"}}
{"id": "2505.23658", "pdf": "https://arxiv.org/pdf/2505.23658", "abs": "https://arxiv.org/abs/2505.23658", "authors": ["Haim Kaplan", "Yishay Mansour", "Kobbi Nissim", "Uri Stemmer"], "title": "Bayesian Perspective on Memorization and Reconstruction", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "We introduce a new Bayesian perspective on the concept of data\nreconstruction, and leverage this viewpoint to propose a new security\ndefinition that, in certain settings, provably prevents reconstruction attacks.\nWe use our paradigm to shed new light on one of the most notorious attacks in\nthe privacy and memorization literature - fingerprinting code attacks (FPC). We\nargue that these attacks are really a form of membership inference attacks,\nrather than reconstruction attacks. Furthermore, we show that if the goal is\nsolely to prevent reconstruction (but not membership inference), then in some\ncases the impossibility results derived from FPC no longer apply.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8d1d\u53f6\u65af\u7684\u65b0\u89c6\u89d2\u6765\u5b9a\u4e49\u6570\u636e\u91cd\u6784\u5b89\u5168\u6027\uff0c\u5e76\u8bc1\u660e\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u4ee5\u9632\u6b62\u91cd\u6784\u653b\u51fb\uff0c\u540c\u65f6\u6307\u51fa\u6307\u7eb9\u7801\u653b\u51fb\u672c\u8d28\u4e0a\u662f\u4e00\u79cd\u6210\u5458\u63a8\u7406\u653b\u51fb\u800c\u975e\u91cd\u6784\u653b\u51fb\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5b9a\u4e49\u5bf9\u91cd\u6784\u653b\u51fb\u7684\u9632\u62a4\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u6307\u7eb9\u7801\u653b\u51fb\uff08FPC\uff09\u5e38\u88ab\u8bef\u8ba4\u4e3a\u662f\u91cd\u6784\u653b\u51fb\uff0c\u800c\u5b9e\u9645\u4e0a\u5c5e\u4e8e\u6210\u5458\u63a8\u7406\u653b\u51fb\u3002\u8bba\u6587\u65e8\u5728\u6f84\u6e05\u8fd9\u4e00\u70b9\uff0c\u5e76\u63d0\u51fa\u66f4\u6709\u6548\u7684\u9632\u62a4\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u91c7\u7528\u8d1d\u53f6\u65af\u89c6\u89d2\u91cd\u65b0\u5b9a\u4e49\u6570\u636e\u91cd\u6784\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u65b0\u7684\u5b89\u5168\u5b9a\u4e49\uff0c\u7528\u4e8e\u5206\u6790FPC\u653b\u51fb\u7684\u672c\u8d28\u53ca\u5176\u5c40\u9650\u6027\u3002", "result": "\u7814\u7a76\u8bc1\u660e\uff0c\u82e5\u76ee\u6807\u4ec5\u662f\u9632\u6b62\u91cd\u6784\u653b\u51fb\u800c\u975e\u6210\u5458\u63a8\u7406\u653b\u51fb\uff0c\u67d0\u4e9b\u57fa\u4e8eFPC\u7684\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\u4e0d\u518d\u9002\u7528\u3002", "conclusion": "\u8bba\u6587\u4e3a\u6570\u636e\u91cd\u6784\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u533a\u5206\u4e86\u91cd\u6784\u653b\u51fb\u4e0e\u6210\u5458\u63a8\u7406\u653b\u51fb\uff0c\u5e76\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u7a81\u7834\u4e86\u539f\u6709\u7406\u8bba\u9650\u5236\u3002", "keywords": "\u6570\u636e\u91cd\u6784\u3001\u8d1d\u53f6\u65af\u89c6\u89d2\u3001\u6307\u7eb9\u7801\u653b\u51fb\u3001\u6210\u5458\u63a8\u7406\u653b\u51fb\u3001\u5b89\u5168\u5b9a\u4e49"}}
{"id": "2505.23692", "pdf": "https://arxiv.org/pdf/2505.23692", "abs": "https://arxiv.org/abs/2505.23692", "authors": ["Jingyun Yang", "Isabella Huang", "Brandon Vu", "Max Bajracharya", "Rika Antonova", "Jeannette Bohg"], "title": "Mobi-$\u03c0$: Mobilizing Your Robot Learning Policy", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Project website: https://mobipi.github.io/", "summary": "Learned visuomotor policies are capable of performing increasingly complex\nmanipulation tasks. However, most of these policies are trained on data\ncollected from limited robot positions and camera viewpoints. This leads to\npoor generalization to novel robot positions, which limits the use of these\npolicies on mobile platforms, especially for precise tasks like pressing\nbuttons or turning faucets. In this work, we formulate the policy mobilization\nproblem: find a mobile robot base pose in a novel environment that is in\ndistribution with respect to a manipulation policy trained on a limited set of\ncamera viewpoints. Compared to retraining the policy itself to be more robust\nto unseen robot base pose initializations, policy mobilization decouples\nnavigation from manipulation and thus does not require additional\ndemonstrations. Crucially, this problem formulation complements existing\nefforts to improve manipulation policy robustness to novel viewpoints and\nremains compatible with them. To study policy mobilization, we introduce the\nMobi-$\\pi$ framework, which includes: (1) metrics that quantify the difficulty\nof mobilizing a given policy, (2) a suite of simulated mobile manipulation\ntasks based on RoboCasa to evaluate policy mobilization, (3) visualization\ntools for analysis, and (4) several baseline methods. We also propose a novel\napproach that bridges navigation and manipulation by optimizing the robot's\nbase pose to align with an in-distribution base pose for a learned policy. Our\napproach utilizes 3D Gaussian Splatting for novel view synthesis, a score\nfunction to evaluate pose suitability, and sampling-based optimization to\nidentify optimal robot poses. We show that our approach outperforms baselines\nin both simulation and real-world environments, demonstrating its effectiveness\nfor policy mobilization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u7b56\u7565\u79fb\u52a8\u5316\u95ee\u9898\uff0c\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u673a\u5668\u4eba\u57fa\u5ea7\u4f4d\u59ff\u6765\u63d0\u5347\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5728\u65b0\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ece\u800c\u907f\u514d\u91cd\u65b0\u8bad\u7ec3\u7b56\u7565\u3002\u4f5c\u8005\u63d0\u51faMobi-\u03c0\u6846\u67b6\u5e76\u5c55\u793a\u4e86\u5176\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5728\u8bad\u7ec3\u65f6\u53d7\u9650\u4e8e\u7279\u5b9a\u673a\u5668\u4eba\u4f4d\u59ff\u548c\u6444\u50cf\u5934\u89c6\u89d2\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u79fb\u52a8\u5e73\u53f0\u4e0a\u6267\u884c\u7cbe\u786e\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u57fa\u5ea7\u4f4d\u59ff\u800c\u975e\u91cd\u65b0\u8bad\u7ec3\u7b56\u7565\u6765\u89e3\u51b3\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51faMobi-\u03c0\u6846\u67b6\uff0c\u5305\u62ec\u56db\u9879\u5185\u5bb9\uff1a\uff081\uff09\u91cf\u5316\u7b56\u7565\u79fb\u52a8\u96be\u5ea6\u7684\u6307\u6807\uff1b\uff082\uff09\u57fa\u4e8eRoboCasa\u7684\u6a21\u62df\u4efb\u52a1\u5957\u4ef6\uff1b\uff083\uff09\u53ef\u89c6\u5316\u5206\u6790\u5de5\u5177\uff1b\uff084\uff09\u57fa\u7ebf\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc73D\u9ad8\u65af\u6cfc\u6e85\u3001\u8bc4\u5206\u51fd\u6570\u548c\u91c7\u6837\u4f18\u5316\u6765\u4f18\u5316\u57fa\u5ea7\u4f4d\u59ff\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\uff0c\u65b0\u65b9\u6cd5\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u7b56\u7565\u79fb\u52a8\u5316\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7b56\u7565\u79fb\u52a8\u5316\u901a\u8fc7\u4f18\u5316\u57fa\u5ea7\u4f4d\u59ff\u89e3\u51b3\u4e86\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4e14\u4e0e\u73b0\u6709\u63d0\u5347\u7b56\u7565\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u517c\u5bb9\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565,\u7b56\u7565\u79fb\u52a8\u5316,\u673a\u5668\u4eba\u5bfc\u822a,3D\u9ad8\u65af\u6cfc\u6e85,\u4f18\u5316"}}
{"id": "2505.23737", "pdf": "https://arxiv.org/pdf/2505.23737", "abs": "https://arxiv.org/abs/2505.23737", "authors": ["Wei Shen", "Ruichuan Huang", "Minhui Huang", "Cong Shen", "Jiawei Zhang"], "title": "On the Convergence Analysis of Muon", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT", "math.OC"], "comment": null, "summary": "The majority of parameters in neural networks are naturally represented as\nmatrices. However, most commonly used optimizers treat these matrix parameters\nas flattened vectors during optimization, potentially overlooking their\ninherent structural properties. Recently, an optimizer called Muon has been\nproposed, specifically designed to optimize matrix-structured parameters.\nExtensive empirical evidence shows that Muon can significantly outperform\ntraditional optimizers when training neural networks. Nonetheless, the\ntheoretical understanding of Muon's convergence behavior and the reasons behind\nits superior performance remain limited. In this work, we present a\ncomprehensive convergence rate analysis of Muon and its comparison with\nGradient Descent (GD). We further characterize the conditions under which Muon\ncan outperform GD. Our theoretical results reveal that Muon can benefit from\nthe low-rank and approximate blockwise diagonal structure of Hessian matrices\n-- phenomena widely observed in practical neural network training. Our\nexperimental results support and corroborate the theoretical findings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86Muon\u4f18\u5316\u5668\u7684\u6536\u655b\u6027\u80fd\u53ca\u5176\u4f18\u4e8e\u4f20\u7edf\u68af\u5ea6\u4e0b\u964d\u6cd5\uff08GD\uff09\u7684\u7406\u8bba\u539f\u56e0\uff0c\u63ed\u793a\u4e86Muon\u5982\u4f55\u5229\u7528Hessian\u77e9\u9635\u7684\u4f4e\u79e9\u548c\u5757\u5bf9\u89d2\u7ed3\u6784\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u5927\u591a\u6570\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u4ee5\u77e9\u9635\u5f62\u5f0f\u5b58\u5728\uff0c\u4f46\u4f20\u7edf\u4f18\u5316\u5668\u5c06\u5176\u89c6\u4e3a\u5411\u91cf\u5904\u7406\uff0c\u5ffd\u7565\u4e86\u5176\u7ed3\u6784\u7279\u6027\u3002Muon\u4f18\u5316\u5668\u867d\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u6536\u655b\u884c\u4e3a\u7684\u7406\u8bba\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790Muon\u7684\u6536\u655b\u901f\u5ea6\uff0c\u5bf9\u6bd4GD\uff0c\u5e76\u63a2\u8ba8Muon\u4f18\u4e8eGD\u7684\u6761\u4ef6\u3002", "result": "\u7406\u8bba\u53d1\u73b0Muon\u80fd\u5229\u7528Hessian\u77e9\u9635\u7684\u4f4e\u79e9\u548c\u8fd1\u4f3c\u5757\u5bf9\u89d2\u7ed3\u6784\u53d6\u5f97\u4f18\u52bf\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u7ed3\u8bba\u3002", "conclusion": "Muon\u901a\u8fc7\u77e9\u9635\u53c2\u6570\u7684\u7ed3\u6784\u7279\u6027\u5b9e\u73b0\u4f18\u4e8eGD\u7684\u4f18\u5316\u6548\u679c\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002", "keywords": ""}}
