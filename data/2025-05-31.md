<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 112]
- [cs.LG](#cs.LG) [Total: 163]
- [cs.AI](#cs.AI) [Total: 37]
- [eess.SY](#eess.SY) [Total: 3]
- [cs.MA](#cs.MA) [Total: 3]
- [math.OC](#math.OC) [Total: 3]
- [eess.IV](#eess.IV) [Total: 4]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 7]
- [cs.DB](#cs.DB) [Total: 3]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.HC](#cs.HC) [Total: 5]
- [eess.AS](#eess.AS) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [math.ST](#math.ST) [Total: 2]
- [cs.ET](#cs.ET) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.RO](#cs.RO) [Total: 7]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.CV](#cs.CV) [Total: 43]
- [stat.ML](#stat.ML) [Total: 10]
- [cs.NI](#cs.NI) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Training Language Models to Generate Quality Code with Program Analysis Feedback](https://arxiv.org/abs/2505.22704)
*Feng Yao,Zilong Wang,Liyuan Liu,Junxia Cui,Li Zhong,Xiaohan Fu,Haohui Mai,Vish Krishnan,Jianfeng Gao,Jingbo Shang*

Key words: LLMs, code generation, reinforcement learning, program analysis, unit tests

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: REAL is a reinforcement learning framework that improves code quality from large language models (LLMs) by using automated program analysis and unit tests, outperforming existing methods in functionality and quality.

Motivation: Existing methods for ensuring code quality in LLM-generated code rely on labor-intensive annotations or brittle heuristics, limiting scalability. REAL aims to bridge the gap between rapid prototyping and production-ready code.

Method: REAL integrates two automated feedback signals: program analysis for security/maintainability defects and unit tests for functional correctness, without needing manual annotations.

Result: Experiments show REAL outperforms state-of-the-art methods in functionality and code quality across multiple datasets and model scales.

Conclusion: REAL enables LLMs to generate high-quality, production-ready code, combining speed and quality effectively.

Abstract: Code generation with large language models (LLMs), often termed vibe coding,
is increasingly adopted in production but fails to ensure code quality,
particularly in security (e.g., SQL injection vulnerabilities) and
maintainability (e.g., missing type annotations). Existing methods, such as
supervised fine-tuning and rule-based post-processing, rely on labor-intensive
annotations or brittle heuristics, limiting their scalability and
effectiveness. We propose REAL, a reinforcement learning framework that
incentivizes LLMs to generate production-quality code using program
analysis-guided feedback. Specifically, REAL integrates two automated signals:
(1) program analysis detecting security or maintainability defects and (2) unit
tests ensuring functional correctness. Unlike prior work, our framework is
prompt-agnostic and reference-free, enabling scalable supervision without
manual intervention. Experiments across multiple datasets and model scales
demonstrate that REAL outperforms state-of-the-art methods in simultaneous
assessments of functionality and code quality. Our work bridges the gap between
rapid prototyping and production-ready code, enabling LLMs to deliver both
speed and quality.

</details>


### [2] [Climate Finance Bench](https://arxiv.org/abs/2505.22752)
*Rafik Mankour,Yassine Chafai,Hamada Saleh,Ghassen Ben Hassine,Thibaud Barreau,Peter Tankov*

Key words: Climate Finance, Question Answering, Large Language Models, RAG, Carbon Reporting

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文介绍了Climate Finance Bench——一个基于大型语言模型的企业气候变化披露问答开放基准测试，包含33份可持续发展报告和330个专家验证的问答对，探讨了RAG方法的性能瓶颈，并强调了透明碳报告的重要性。

Motivation: 研究旨在通过构建一个针对企业气候变化披露的问答基准测试，推动大型语言模型在气候金融领域的应用，并解决现有方法在检索和生成环节的瓶颈问题。

Method: 基于33份跨11个GICS行业的可持续发展报告，标注了330个涵盖提取、数值推理和逻辑推理的问答对，并比较多种RAG（检索增强生成）方法。

Result: 研究发现检索模块能否定位到包含答案的段落是性能的主要瓶颈，同时提倡在气候AI应用中采用透明碳报告（如权重量化技术）。

Conclusion: 论文提出了一个实用的气候金融问答基准，揭示了RAG方法的性能挑战，并呼吁在AI气候应用中关注碳足迹透明度。

Abstract: Climate Finance Bench introduces an open benchmark that targets
question-answering over corporate climate disclosures using Large Language
Models. We curate 33 recent sustainability reports in English drawn from
companies across all 11 GICS sectors and annotate 330 expert-validated
question-answer pairs that span pure extraction, numerical reasoning, and
logical reasoning. Building on this dataset, we propose a comparison of RAG
(retrieval-augmented generation) approaches. We show that the retriever's
ability to locate passages that actually contain the answer is the chief
performance bottleneck. We further argue for transparent carbon reporting in
AI-for-climate applications, highlighting advantages of techniques such as
Weight Quantization.

</details>


### [3] [Pre-Training Curriculum for Multi-Token Prediction in Language Models](https://arxiv.org/abs/2505.22757)
*Ansar Aynetdinov,Alan Akbik*

Key words: 多令牌预测, 课程学习, 小语言模型, 自推测解码

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种课程学习策略，用于改进小语言模型（SLM）在多令牌预测（MTP）中的表现，探讨了正向和反向两种课程学习方式的效果。

Motivation: 由于小语言模型在多令牌预测（MTP）任务中表现不佳，作者希望通过课程学习策略来提升其性能。

Method: 采用两种课程学习策略：正向课程（从NTP逐步过渡到MTP）和反向课程（从MTP逐步过渡到NTP）。

Result: 正向课程显著提升了SLM在预训练中的MTP表现，并改善了生成质量，同时保留了自推测解码的优势；反向课程虽提升了NTP性能和输出质量，但未带来自推测解码的收益。

Conclusion: 正向课程更适合SLM在多令牌预测任务中的训练，平衡了性能和效率。

Abstract: Multi-token prediction (MTP) is a recently proposed pre-training objective
for language models. Rather than predicting only the next token (NTP), MTP
predicts the next $k$ tokens at each prediction step, using multiple prediction
heads. MTP has shown promise in improving downstream performance, inference
speed, and training efficiency, particularly for large models. However, prior
work has shown that smaller language models (SLMs) struggle with the MTP
objective. To address this, we propose a curriculum learning strategy for MTP
training, exploring two variants: a forward curriculum, which gradually
increases the complexity of the pre-training objective from NTP to MTP, and a
reverse curriculum, which does the opposite. Our experiments show that the
forward curriculum enables SLMs to better leverage the MTP objective during
pre-training, improving downstream NTP performance and generative output
quality, while retaining the benefits of self-speculative decoding. The reverse
curriculum achieves stronger NTP performance and output quality, but fails to
provide any self-speculative decoding benefits.

</details>


### [4] [FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian](https://arxiv.org/abs/2505.22759)
*Sara Papi,Marco Gaido,Luisa Bentivogli,Alessio Brutti,Mauro Cettolo,Roberto Gretter,Marco Matassoni,Mohamed Nabih,Matteo Negri*

Key words: speech foundation models, open science, FAMA, speech processing, reproducibility

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: FAMA introduces the first open science speech foundation models for English and Italian, trained on 150k+ hours of open-source speech data, and releases all artifacts under OS-compliant licenses.

Motivation: Addressing the reproducibility and fair evaluation challenges posed by closed-source speech foundation models by promoting openness in speech technology research.

Method: Developed FAMA, a family of open science speech foundation models trained on 150k+ hours of open-source speech data, including a new dataset of 16k hours of cleaned and pseudo-labeled speech.

Result: FAMA achieves competitive performance compared to existing speech foundation models while being up to 8 times faster.

Conclusion: FAMA successfully fills the gap in open science speech foundation models, providing competitive performance and promoting transparency in research.

Abstract: The development of speech foundation models (SFMs) like Whisper and
SeamlessM4T has significantly advanced the field of speech processing. However,
their closed nature--with inaccessible training data and code--poses major
reproducibility and fair evaluation challenges. While other domains have made
substantial progress toward open science by developing fully transparent models
trained on open-source (OS) code and data, similar efforts in speech remain
limited. To fill this gap, we introduce FAMA, the first family of open science
SFMs for English and Italian, trained on 150k+ hours of OS speech data.
Moreover, we present a new dataset containing 16k hours of cleaned and
pseudo-labeled speech for both languages. Results show that FAMA achieves
competitive performance compared to existing SFMs while being up to 8 times
faster. All artifacts, including code, datasets, and models, are released under
OS-compliant licenses, promoting openness in speech technology research.

</details>


### [5] [StressTest: Can YOUR Speech LM Handle the Stress?](https://arxiv.org/abs/2505.22765)
*Iddo Yosha,Gallil Maimon,Yossi Adi*

Key words: 句子重音,语音感知语言模型,StressTest,合成数据生成,Stress17k

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了StressTest基准来评估语音感知语言模型（SLMs）在句子重音区分上的表现，发现现有模型表现不佳。研究者提出了一种合成数据生成方法，创建了Stress17k数据集，并证明了其优化后的模型StresSLM在重音推理和检测任务上显著优于现有模型。

Motivation: 句子重音在语音中至关重要但被现有SLMs忽视，因此研究旨在填补这一空白，评估并提升模型对重音模式的识别能力。

Method: 引入StressTest基准，评估多个SLMs对重音模式的区分能力，并提出合成数据生成流程（Stress17k数据集）进行模型优化。

Result: 优化后的StresSLM在重音推理和检测任务上显著优于现有模型，且合成数据与真实录音对齐良好。

Conclusion: 合成数据能够有效提升SLMs对重音模式的理解能力，StresSLM展现了显著的性能提升。

Abstract: Sentence stress refers to emphasis, placed on specific words within a spoken
utterance to highlight or contrast an idea, or to introduce new information. It
is often used to imply an underlying intention that is not explicitly stated.
Recent advances in speech-aware language models (SLMs) have enabled direct
processing of audio, allowing models to bypass transcription and access the
full richness of the speech signal and perform audio reasoning tasks such as
spoken question answering. Despite the crucial role of sentence stress in
shaping meaning and speaker intent, it remains largely overlooked in evaluation
and development of such models. In this work, we address this gap by
introducing StressTest, a benchmark specifically designed to evaluate a model's
ability to distinguish between interpretations of spoken sentences based on the
stress pattern. We assess the performance of several leading SLMs and find
that, despite their overall capabilities, they perform poorly on such tasks. To
overcome this limitation, we propose a novel synthetic data generation
pipeline, and create Stress17k, a training set that simulates change of meaning
implied by stress variation. Then, we empirically show that optimizing models
with this synthetic dataset aligns well with real-world recordings and enables
effective finetuning of SLMs. Results suggest, that our finetuned model,
StresSLM, significantly outperforms existing models on both sentence stress
reasoning and detection tasks. Code, models, data, and audio samples -
pages.cs.huji.ac.il/adiyoss-lab/stresstest.

</details>


### [6] [Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems](https://arxiv.org/abs/2505.22771)
*Christopher Ormerod*

Key words: 自动作文评分,反馈注释,语言模型,PERSUADE语料库

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 通过将反馈导向的注释整合到自动作文评分（AES）流程中，可以提高评分的准确性。

Motivation: 提升自动作文评分的准确性，通过使用反馈驱动的注释来改善评分模型的表现。

Method: 使用两种反馈驱动的注释类型：拼写和语法错误的标记，以及论证组件的标注。两种大型语言模型（LLM）分别用于生成这些注释。

Result: 整合注释后，基于编码器的大型语言模型在分类任务中表现有所提升。

Conclusion: 通过引入反馈驱动的注释，自动作文评分的准确性得到显著提高。

Abstract: This study illustrates how incorporating feedback-oriented annotations into
the scoring pipeline can enhance the accuracy of automated essay scoring (AES).
This approach is demonstrated with the Persuasive Essays for Rating, Selecting,
and Understanding Argumentative and Discourse Elements (PERSUADE) corpus. We
integrate two types of feedback-driven annotations: those that identify
spelling and grammatical errors, and those that highlight argumentative
components. To illustrate how this method could be applied in real-world
scenarios, we employ two LLMs to generate annotations -- a generative language
model used for spell-correction and an encoder-based token classifier trained
to identify and mark argumentative elements. By incorporating annotations into
the scoring process, we demonstrate improvements in performance using
encoder-based large language models fine-tuned as classifiers.

</details>


### [7] [Counting trees: A treebank-driven exploration of syntactic variation in speech and writing across languages](https://arxiv.org/abs/2505.22774)
*Kaja Dobrovoljc*

Key words: 句法比较，依存树库，口语与书面语，跨语言分析，语法使用

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于依存树库的新方法，比较口语和书面语中的句法结构，发现两者在句法多样性、分布及特征上存在显著差异。

Motivation: 研究动机在于探索口语和书面语在句法结构上的差异，尤其是受实时互动与精心写作需求影响的句法特性差异。

Method: 采用自下而上的完全归纳方法，从英语和斯洛文尼亚语的依存树库中提取去词汇化的依存子树，分析其规模、多样性和分布。

Result: 口语语料库中的句法结构更少且多样性更低，与书面语的句法库重叠有限，表明模态特异性偏好。关键性分析揭示了与互动性和经济性相关的口语特性结构。

Conclusion: 该可扩展、语言无关的框架为跨语料库的句法变异性研究提供了通用方法，为基于数据的语法理论奠定了基础。

Abstract: This paper presents a novel treebank-driven approach to comparing syntactic
structures in speech and writing using dependency-parsed corpora. Adopting a
fully inductive, bottom-up method, we define syntactic structures as
delexicalized dependency (sub)trees and extract them from spoken and written
Universal Dependencies (UD) treebanks in two syntactically distinct languages,
English and Slovenian. For each corpus, we analyze the size, diversity, and
distribution of syntactic inventories, their overlap across modalities, and the
structures most characteristic of speech. Results show that, across both
languages, spoken corpora contain fewer and less diverse syntactic structures
than their written counterparts, with consistent cross-linguistic preferences
for certain structural types across modalities. Strikingly, the overlap between
spoken and written syntactic inventories is very limited: most structures
attested in speech do not occur in writing, pointing to modality-specific
preferences in syntactic organization that reflect the distinct demands of
real-time interaction and elaborated writing. This contrast is further
supported by a keyness analysis of the most frequent speech-specific
structures, which highlights patterns associated with interactivity,
context-grounding, and economy of expression. We argue that this scalable,
language-independent framework offers a useful general method for
systematically studying syntactic variation across corpora, laying the
groundwork for more comprehensive data-driven theories of grammar in use.

</details>


### [8] [MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Chatbots and Dialogue Evaluators](https://arxiv.org/abs/2505.22777)
*John Mendonça,Alon Lavie,Isabel Trancoso*

Key words: chatbot, LLM, evaluation benchmark, multilingual, automated framework

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: MEDAL框架通过多智能体自动化生成、评估和改进多语言开放域对话评测基准，发现当前LLMs在检测细腻问题上存在不足。

Motivation: 现有的聊天机器人评测数据集多为静态、过时且缺乏多语言覆盖，无法捕捉细微的语言和文化差异。

Method: 利用多个先进LLMs生成多语言对话，以GPT-4.1进行多维分析，并构建新的多语言基准，评估LLMs的对话评测能力。

Result: 发现LLMs在检测细腻问题（如同理心和推理）上表现不佳，并揭示跨语言性能差异。

Conclusion: MEDAL框架能生成更全面、多样化的评测基准，但当前LLMs的评测能力仍有提升空间。

Abstract: As the capabilities of chatbots and their underlying LLMs continue to
dramatically improve, evaluating their performance has increasingly become a
major blocker to their further development. A major challenge is the available
benchmarking datasets, which are largely static, outdated, and lacking in
multilingual coverage, limiting their ability to capture subtle linguistic and
cultural variations. This paper introduces MEDAL, an automated multi-agent
framework for generating, evaluating, and curating more representative and
diverse open-domain dialogue evaluation benchmarks. Our approach leverages
several state-of-the-art LLMs to generate user-chatbot multilingual dialogues,
conditioned on varied seed contexts. A strong LLM (GPT-4.1) is then used for a
multidimensional analysis of the performance of the chatbots, uncovering
noticeable cross-lingual performance differences. Guided by this large-scale
evaluation, we curate a new meta-evaluation multilingual benchmark and
human-annotate samples with nuanced quality judgments. This benchmark is then
used to assess the ability of several reasoning and non-reasoning LLMs to act
as evaluators of open-domain dialogues. We find that current LLMs struggle to
detect nuanced issues, particularly those involving empathy and reasoning.

</details>


### [9] [Can Large Language Models Match the Conclusions of Systematic Reviews?](https://arxiv.org/abs/2505.22787)
*Christopher Polzak,Alejandro Lozano,Min Woo Sun,James Burgess,Yuhui Zhang,Kevin Wu,Serena Yeung-Levy*

Key words: 系统综述, 大语言模型, MedEvidence, 临床决策, 自动化评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LLMs目前尚无法完全匹配专家撰写的系统综述结论，性能受上下文长度影响且缺乏科学怀疑态度。

Motivation: 评估LLMs在自动化系统综述生成中与专家结论的一致性，填补现有研究空白。

Method: 基于MedEvidence基准（100篇系统综述及其对应研究），测试24种不同规模与类型的LLM。

Result: LLMs表现受限于上下文长度，普遍过度自信，且对低质量研究缺乏怀疑；模型规模与微调未显著提升效果。

Conclusion: LLMs需进一步改进才能可靠替代专家主导的系统综述，当前部署仍需谨慎。

Abstract: Systematic reviews (SR), in which experts summarize and analyze evidence
across individual studies to provide insights on a specialized topic, are a
cornerstone for evidence-based clinical decision-making, research, and policy.
Given the exponential growth of scientific articles, there is growing interest
in using large language models (LLMs) to automate SR generation. However, the
ability of LLMs to critically assess evidence and reason across multiple
documents to provide recommendations at the same proficiency as domain experts
remains poorly characterized. We therefore ask: Can LLMs match the conclusions
of systematic reviews written by clinical experts when given access to the same
studies? To explore this question, we present MedEvidence, a benchmark pairing
findings from 100 SRs with the studies they are based on. We benchmark 24 LLMs
on MedEvidence, including reasoning, non-reasoning, medical specialist, and
models across varying sizes (from 7B-700B). Through our systematic evaluation,
we find that reasoning does not necessarily improve performance, larger models
do not consistently yield greater gains, and knowledge-based fine-tuning
degrades accuracy on MedEvidence. Instead, most models exhibit similar
behavior: performance tends to degrade as token length increases, their
responses show overconfidence, and, contrary to human experts, all models show
a lack of scientific skepticism toward low-quality findings. These results
suggest that more work is still required before LLMs can reliably match the
observations from expert-conducted SRs, even though these systems are already
deployed and being used by clinicians. We release our codebase and benchmark to
the broader research community to further investigate LLM-based SR systems.

</details>


### [10] [Towards a More Generalized Approach in Open Relation Extraction](https://arxiv.org/abs/2505.22801)
*Qing Wang,Yuepei Li,Qiao Qiao,Kang Zhou,Qi Li*

Key words: Open Relation Extraction, MixORE, relation classification, clustering, generalized setting

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种广义OpenRE框架MixORE，通过两阶段的关系分类和聚类方法，在已知和未知关系混合的数据中取得了优于基线模型的效果。

Motivation: 现实场景中，未标记数据通常是已知和未知关系的混合，而传统OpenRE方法假设数据仅包含未知关系或已预先划分，无法处理这种混合情况。

Method: 提出了MixORE框架，结合关系分类（处理已知关系）和聚类（识别未知关系），在混合数据中联合学习。

Result: 在三个基准数据集上，MixORE在已知关系分类和未知关系聚类任务中均优于竞争基线方法。

Conclusion: MixORE为广义OpenRE研究及实际应用提供了有效解决方案。

Abstract: Open Relation Extraction (OpenRE) seeks to identify and extract novel
relational facts between named entities from unlabeled data without pre-defined
relation schemas. Traditional OpenRE methods typically assume that the
unlabeled data consists solely of novel relations or is pre-divided into known
and novel instances. However, in real-world scenarios, novel relations are
arbitrarily distributed. In this paper, we propose a generalized OpenRE setting
that considers unlabeled data as a mixture of both known and novel instances.
To address this, we propose MixORE, a two-phase framework that integrates
relation classification and clustering to jointly learn known and novel
relations. Experiments on three benchmark datasets demonstrate that MixORE
consistently outperforms competitive baselines in known relation classification
and novel relation clustering. Our findings contribute to the advancement of
generalized OpenRE research and real-world applications.

</details>


### [11] [First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay](https://arxiv.org/abs/2505.22809)
*Andrew Zhu,Evan Osgood,Chris Callison-Burch*

Key words: 偷听智能体, 多模态模型, 龙与地下城, 音频-语言模型, 人机交互

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了'偷听智能体'的新范式，即LLM智能体通过监听人类对话提供后台任务支持或建议，并以《龙与地下城》游戏为例展示了其应用效果。

Motivation: 现有对话型LLM智能体需直接与用户交互，而本文探索不主动参与对话、仅通过监听提供辅助的'偷听智能体'模式。

Method: 利用大型多模态音频-语言模型作为偷听智能体，通过人类评估检验其对《龙与地下城》地下城主持人的帮助效果。

Result: 研究发现某些大型音频-语言模型能通过隐式音频线索完成偷听任务，并开源了支持代码库。

Conclusion: 偷听智能体范式具有潜在应用价值，多模态模型在此任务中展现出涌现能力。

Abstract: Much work has been done on conversational LLM agents which directly assist
human users with tasks. We present an alternative paradigm for interacting with
LLM agents, which we call "overhearing agents". These overhearing agents do not
actively participate in conversation -- instead, they "listen in" on
human-to-human conversations and perform background tasks or provide
suggestions to assist the user. In this work, we explore the overhearing agents
paradigm through the lens of Dungeons & Dragons gameplay. We present an
in-depth study using large multimodal audio-language models as overhearing
agents to assist a Dungeon Master. We perform a human evaluation to examine the
helpfulness of such agents and find that some large audio-language models have
the emergent ability to perform overhearing agent tasks using implicit audio
cues. Finally, we release Python libraries and our project code to support
further research into the overhearing agents paradigm at
https://github.com/zhudotexe/overhearing_agents.

</details>


### [12] [Self-Critique and Refinement for Faithful Natural Language Explanations](https://arxiv.org/abs/2505.22823)
*Yingming Wang,Pepa Atanasova*

Key words: 大语言模型,自然语言解释,自我优化,忠实度,特征归因

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了SR-NLE框架，通过自我批评和迭代优化提升大语言模型生成的自然语言解释的忠实度，无需外部监督。

Motivation: 现有的自然语言解释（NLEs）往往无法忠实反映模型的真实推理过程，作者希望通过自我优化机制解决这一问题。

Method: 引入SR-NLE框架，结合自然语言自我反馈和基于特征归因的反馈机制，迭代优化模型生成的解释。

Result: 在三个数据集和四种先进LLMs上实验显示，SR-NLE将不忠实率从54.81%降至36.02%，绝对减少18.79%。

Conclusion: 研究表明，LLMs能够通过适当反馈机制优化解释，更准确地反映其推理过程，无需额外训练。

Abstract: With the rapid development of large language models (LLMs), natural language
explanations (NLEs) have become increasingly important for understanding model
predictions. However, these explanations often fail to faithfully represent the
model's actual reasoning process. While existing work has demonstrated that
LLMs can self-critique and refine their initial outputs for various tasks, this
capability remains unexplored for improving explanation faithfulness. To
address this gap, we introduce Self-critique and Refinement for Natural
Language Explanations (SR-NLE), a framework that enables models to improve the
faithfulness of their own explanations -- specifically, post-hoc NLEs --
through an iterative critique and refinement process without external
supervision. Our framework leverages different feedback mechanisms to guide the
refinement process, including natural language self-feedback and, notably, a
novel feedback approach based on feature attribution that highlights important
input words. Our experiments across three datasets and four state-of-the-art
LLMs demonstrate that SR-NLE significantly reduces unfaithfulness rates, with
our best method achieving an average unfaithfulness rate of 36.02%, compared to
54.81% for baseline -- an absolute reduction of 18.79%. These findings reveal
that the investigated LLMs can indeed refine their explanations to better
reflect their actual reasoning process, requiring only appropriate guidance
through feedback without additional training or fine-tuning.

</details>


### [13] [What Has Been Lost with Synthetic Evaluation?](https://arxiv.org/abs/2505.22830)
*Alexander Gill,Abhilasha Ravichander,Ana Marasović*

Key words: 大型语言模型、评估基准、数据生成、阅读理解、众包

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究表明，使用大型语言模型（LLMs）生成评估基准虽然成本低且有效，但生成的基准对LLMs本身的挑战性低于人工众包数据，需重新评估其直接应用。

Motivation: 探讨LLMs是否能满足生成高质量评估基准的需求，比较其与人工众包数据的有效性和难度。

Method: 通过两个案例研究（CondaQA和DROP数据集），分析LLMs生成的基准在有效性和难度上的表现。

Result: LLMs生成的基准成本低且符合标注指南，但对LLMs的挑战性低于人工数据。

Conclusion: 需重新评估直接使用LLMs生成评估基准的可行性，因其可能丧失某些挑战性。

Abstract: Large language models (LLMs) are increasingly used for data generation.
However, creating evaluation benchmarks raises the bar for this emerging
paradigm. Benchmarks must target specific phenomena, penalize exploiting
shortcuts, and be challenging. Through two case studies, we investigate whether
LLMs can meet these demands by generating reasoning over-text benchmarks and
comparing them to those created through careful crowdsourcing. Specifically, we
evaluate both the validity and difficulty of LLM-generated versions of two
high-quality reading comprehension datasets: CondaQA, which evaluates reasoning
about negation, and DROP, which targets reasoning about quantities. We find
that prompting LLMs can produce variants of these datasets that are often valid
according to the annotation guidelines, at a fraction of the cost of the
original crowdsourcing effort. However, we show that they are less challenging
for LLMs than their human-authored counterparts. This finding sheds light on
what may have been lost by generating evaluation data with LLMs, and calls for
critically reassessing the immediate use of this increasingly prevalent
approach to benchmark creation.

</details>


### [14] [Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation](https://arxiv.org/abs/2505.22842)
*Arthur S. Bianchessi,Rodrigo C. Barros,Lucas S. Kupssinskü*

Key words: 位置编码, 贝叶斯注意力机制, 长上下文泛化, 语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文提出了贝叶斯注意力机制（BAM），一种将位置编码建模为先验的理论框架，统一了现有方法并显著提升长上下文泛化能力。

Motivation: 现有位置编码方法缺乏理论清晰性且评估指标有限，难以支持其外推能力的主张。

Method: BAM将位置编码作为概率模型中的先验，提出了广义高斯位置先验。

Result: BAM在500倍训练上下文长度下实现了准确信息检索，优于现有方法，同时保持了相当的困惑度且参数增加极少。

Conclusion: BAM为位置编码提供了理论支持，显著提升了长上下文任务中的表现。

Abstract: Transformer-based language models rely on positional encoding (PE) to handle
token order and support context length extrapolation. However, existing PE
methods lack theoretical clarity and rely on limited evaluation metrics to
substantiate their extrapolation claims. We propose the Bayesian Attention
Mechanism (BAM), a theoretical framework that formulates positional encoding as
a prior within a probabilistic model. BAM unifies existing methods (e.g., NoPE
and ALiBi) and motivates a new Generalized Gaussian positional prior that
substantially improves long-context generalization. Empirically, BAM enables
accurate information retrieval at $500\times$ the training context length,
outperforming previous state-of-the-art context length generalization in long
context retrieval accuracy while maintaining comparable perplexity and
introducing minimal additional parameters.

</details>


### [15] [LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference](https://arxiv.org/abs/2505.22848)
*Pingjun Hong,Beiduo Chen,Siyao Peng,Marie-Catherine de Marneffe,Barbara Plank*

Key words: Human Label Variation, NLI, free-text explanations, LITEX, explanation generation

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了LITEX，一种用于分类自由文本解释的语言学分类法，以捕捉NLI任务中的标签内变异，并验证其在解释生成中的有效性。

Motivation: 研究NLI任务中标注者在相同标签下提供不同推理的原因，填补了标签内变异研究的空白，并探索如何通过分类法提升解释生成的质量。

Method: 引入LITEX分类法，对e-SNLI数据集子集进行标注，验证分类法的可靠性，并评估其在解释生成中的应用效果。

Result: LITEX生成的解释在语言学上更接近人类解释，优于仅基于标签或高亮的方法，有效捕捉了标签内变异。

Conclusion: LITEX不仅能系统化理解NLI中的标签内变异，还为模型生成解释提供了更接近人类推理的方法。

Abstract: There is increasing evidence of Human Label Variation (HLV) in Natural
Language Inference (NLI), where annotators assign different labels to the same
premise-hypothesis pair. However, within-label variation--cases where
annotators agree on the same label but provide divergent reasoning--poses an
additional and mostly overlooked challenge. Several NLI datasets contain
highlighted words in the NLI item as explanations, but the same spans on the
NLI item can be highlighted for different reasons, as evidenced by free-text
explanations, which offer a window into annotators' reasoning. To
systematically understand this problem and gain insight into the rationales
behind NLI labels, we introduce LITEX, a linguistically-informed taxonomy for
categorizing free-text explanations. Using this taxonomy, we annotate a subset
of the e-SNLI dataset, validate the taxonomy's reliability, and analyze how it
aligns with NLI labels, highlights, and explanations. We further assess the
taxonomy's usefulness in explanation generation, demonstrating that
conditioning generation on LITEX yields explanations that are linguistically
closer to human explanations than those generated using only labels or
highlights. Our approach thus not only captures within-label variation but also
shows how taxonomy-guided generation for reasoning can bridge the gap between
human and model explanations more effectively than existing strategies.

</details>


### [16] [GateNLP at SemEval-2025 Task 10: Hierarchical Three-Step Prompting for Multilingual Narrative Classification](https://arxiv.org/abs/2505.22867)
*Iknoor Singh,Carolina Scarton,Kalina Bontcheva*

Key words: 叙事分类,多语言处理,大语言模型,提示策略

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一个名为H3Prompt的分层三步提示方法，用于多语言新闻叙事分类，并在国际比赛中取得了最佳成绩。

Motivation: 随着在线新闻和错误信息的扩散，自动数据分析方法变得尤为重要，叙事分类成为关键任务。

Method: 采用分层三步提示策略，通过大语言模型先分类文章领域，再识别主要叙事，最后分配子叙事。

Result: 该方法在28个参赛团队中，英语测试集上排名第一。

Conclusion: H3Prompt在多语言叙事分类任务中表现出色，具有实际应用潜力。

Abstract: The proliferation of online news and the increasing spread of misinformation
necessitate robust methods for automatic data analysis. Narrative
classification is emerging as a important task, since identifying what is being
said online is critical for fact-checkers, policy markers and other
professionals working on information studies. This paper presents our approach
to SemEval 2025 Task 10 Subtask 2, which aims to classify news articles into a
pre-defined two-level taxonomy of main narratives and sub-narratives across
multiple languages.
  We propose Hierarchical Three-Step Prompting (H3Prompt) for multilingual
narrative classification. Our methodology follows a three-step Large Language
Model (LLM) prompting strategy, where the model first categorises an article
into one of two domains (Ukraine-Russia War or Climate Change), then identifies
the most relevant main narratives, and finally assigns sub-narratives. Our
approach secured the top position on the English test set among 28 competing
teams worldwide. The code is available at https://github.com/GateNLP/H3Prompt.

</details>


### [17] [When Models Reason in Your Language: Controlling Thinking Trace Language Comes at the Cost of Accuracy](https://arxiv.org/abs/2505.22888)
*Jirui Qi,Shan Chen,Zidi Xiong,Raquel Fernández,Danielle S. Bitterman,Arianna Bisazza*

Key words: 大型推理模型, 多语言推理, 提示干预, 后训练, XReasoning基准

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文评估了当前大型推理模型（LRMs）在多语言推理任务中的表现，发现其存在明显不足，尤其是推理过程常回归英语或支离破碎。通过提示干预和少量数据的后训练可以部分改善，但仍需进一步研究。

Motivation: 研究动机在于探讨LRMs在多语言环境下的实际应用能力，强调用户需要以母语理解的推理痕迹以实现有效监督。

Method: 论文方法包括对两种领先LRMs模型在XReasoning基准上的全面评估，以及通过提示干预和针对性后训练（仅100个示例）来优化多语言推理能力。

Result: 结果表明，当前LRMs的多语言推理能力有限，强制用用户语言推理会降低答案准确性，但后训练可部分缓解这一问题。

Conclusion: 结论指出LRMs在多语言推理上仍有显著差距，未来需优化模型设计以平衡准确性与可读性。

Abstract: Recent Large Reasoning Models (LRMs) with thinking traces have shown strong
performance on English reasoning tasks. However, their ability to think in
other languages is less studied. This capability is as important as answer
accuracy for real world applications because users may find the reasoning trace
useful for oversight only when it is expressed in their own language. We
comprehensively evaluate two leading families of LRMs on our XReasoning
benchmark and find that even the most advanced models often revert to English
or produce fragmented reasoning in other languages, revealing a substantial gap
in multilingual reasoning. Prompt based interventions that force models to
reason in the users language improve readability and oversight but reduce
answer accuracy, exposing an important trade off. We further show that targeted
post training on just 100 examples mitigates this mismatch, though some
accuracy loss remains. Our results highlight the limited multilingual reasoning
capabilities of current LRMs and outline directions for future work. Code and
data are available at https://github.com/Betswish/mCoT-XReasoning.

</details>


### [18] [VIGNETTE: Socially Grounded Bias Evaluation for Vision-Language Models](https://arxiv.org/abs/2505.22897)
*Chahat Raj,Bowen Wei,Aylin Caliskan,Antonios Anastasopoulos,Ziwei Zhu*

Key words: 视觉语言模型, 偏见评估, 社会刻板印象, VIGNETTE基准

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了VIGNETTE基准，用于评估视觉语言模型（VLMs）中的偏见，涵盖事实性、感知、刻板印象和决策四个方向，揭示了VLMs如何从视觉身份线索中推断社会刻板印象。

Motivation: 现有研究多关注文本模型中的偏见，而视觉语言模型的偏见研究较少，尤其是广泛社会刻板印象及其潜在危害。

Method: 引入VIGNETTE基准，包含3000多万张图像，通过问答框架评估VLMs在四个方向上的偏见。

Result: 研究发现VLMs在身份解释中存在多层面刻板印象，并从视觉线索中推断社会等级。

Conclusion: VLMs能够从输入中构建社会意义，但隐含微妙且多面的刻板印象模式。

Abstract: While bias in large language models (LLMs) is well-studied, similar concerns
in vision-language models (VLMs) have received comparatively less attention.
Existing VLM bias studies often focus on portrait-style images and
gender-occupation associations, overlooking broader and more complex social
stereotypes and their implied harm. This work introduces VIGNETTE, a
large-scale VQA benchmark with 30M+ images for evaluating bias in VLMs through
a question-answering framework spanning four directions: factuality,
perception, stereotyping, and decision making. Beyond narrowly-centered
studies, we assess how VLMs interpret identities in contextualized settings,
revealing how models make trait and capability assumptions and exhibit patterns
of discrimination. Drawing from social psychology, we examine how VLMs connect
visual identity cues to trait and role-based inferences, encoding social
hierarchies, through biased selections. Our findings uncover subtle,
multifaceted, and surprising stereotypical patterns, offering insights into how
VLMs construct social meaning from inputs.

</details>


### [19] [Talent or Luck? Evaluating Attribution Bias in Large Language Models](https://arxiv.org/abs/2505.22910)
*Chahat Raj,Mahika Banerjee,Aylin Caliskan,Antonios Anastasopoulos,Ziwei Zhu*

Key words: 归因理论, LLM, 偏见评估, 认知框架, 公平性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨学生考试失败时归因于努力还是考试难度，提出基于归因理论的认知框架评估LLM中因推理差异导致的群体偏见。

Motivation: 研究LLM如何根据人口统计特征归因事件结果，这对公平性有重要意义，现有研究多关注表面偏见，缺乏认知层面的分析。

Method: 提出基于归因理论的认知框架，评估模型在归因推理中对不同群体的偏见。

Result: 揭示了LLM在归因推理中存在基于人口统计的偏见，其认知机制强化了不公平性。

Conclusion: 认知框架能有效识别LLM的深层偏见，为公平性改进提供理论基础。

Abstract: When a student fails an exam, do we tend to blame their effort or the test's
difficulty? Attribution, defined as how reasons are assigned to event outcomes,
shapes perceptions, reinforces stereotypes, and influences decisions.
Attribution Theory in social psychology explains how humans assign
responsibility for events using implicit cognition, attributing causes to
internal (e.g., effort, ability) or external (e.g., task difficulty, luck)
factors. LLMs' attribution of event outcomes based on demographics carries
important fairness implications. Most works exploring social biases in LLMs
focus on surface-level associations or isolated stereotypes. This work proposes
a cognitively grounded bias evaluation framework to identify how models'
reasoning disparities channelize biases toward demographic groups.

</details>


### [20] [ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the Emergency Room](https://arxiv.org/abs/2505.22919)
*Nikita Mehandru,Niloufar Golchini,David Bamman,Travis Zack,Melanie F. Molina,Ahmed Alaa*

Key words: 大语言模型, 临床推理, 急诊室, ER-Reason基准

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了ER-Reason基准，用于评估大语言模型在急诊室临床决策中的推理能力，发现现有模型与临床医生的推理存在差距。

Motivation: 现有的医学问答任务基准未能全面捕捉临床决策的真实推理过程，尤其是在高风险的急诊室环境下。

Method: 构建了包含3,984名患者数据的ER-Reason基准，设计多阶段任务评估临床推理能力，并收集了医生的完整推理理由作为参考。

Result: 评估显示，先进的大语言模型在急诊室决策中的推理能力与临床医生存在明显差距。

Conclusion: 未来研究需要进一步缩小大语言模型与临床医生在决策推理上的差距。

Abstract: Large language models (LLMs) have been extensively evaluated on medical
question answering tasks based on licensing exams. However, real-world
evaluations often depend on costly human annotators, and existing benchmarks
tend to focus on isolated tasks that rarely capture the clinical reasoning or
full workflow underlying medical decisions. In this paper, we introduce
ER-Reason, a benchmark designed to evaluate LLM-based clinical reasoning and
decision-making in the emergency room (ER)--a high-stakes setting where
clinicians make rapid, consequential decisions across diverse patient
presentations and medical specialties under time pressure. ER-Reason includes
data from 3,984 patients, encompassing 25,174 de-identified longitudinal
clinical notes spanning discharge summaries, progress notes, history and
physical exams, consults, echocardiography reports, imaging notes, and ER
provider documentation. The benchmark includes evaluation tasks that span key
stages of the ER workflow: triage intake, initial assessment, treatment
selection, disposition planning, and final diagnosis--each structured to
reflect core clinical reasoning processes such as differential diagnosis via
rule-out reasoning. We also collected 72 full physician-authored rationales
explaining reasoning processes that mimic the teaching process used in
residency training, and are typically absent from ER documentation. Evaluations
of state-of-the-art LLMs on ER-Reason reveal a gap between LLM-generated and
clinician-authored clinical reasoning for ER decisions, highlighting the need
for future research to bridge this divide.

</details>


### [21] [Structured Memory Mechanisms for Stable Context Representation in Large Language Models](https://arxiv.org/abs/2505.22921)
*Yue Xing,Tao Yang,Yijiashun Qi,Minggu Wei,Yu Cheng,Honghui Xin*

Key words: 长期记忆,语言模型,语义保留,动态更新,联合训练

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种配备长期记忆机制的模型架构，通过显式记忆单元、门控写入机制和基于注意力的读取模块，改进语言模型对长期语义信息的保留和检索。

Motivation: 解决大型语言模型在理解长期上下文时的局限性，如上下文丢失和语义漂移。

Method: 集成显式记忆单元、门控写入和注意力读取模块，引入动态遗忘功能，并设计联合训练目标。

Result: 模型在文本生成一致性、多轮问答稳定性和跨上下文推理准确性上表现优异，尤其在长文本任务和复杂问答中展示了强语义保留能力。

Conclusion: 记忆机制在语言理解中起关键作用，提出的方法在架构设计和性能效果上均可行有效。

Abstract: This paper addresses the limitations of large language models in
understanding long-term context. It proposes a model architecture equipped with
a long-term memory mechanism to improve the retention and retrieval of semantic
information across paragraphs and dialogue turns. The model integrates explicit
memory units, gated writing mechanisms, and attention-based reading modules. A
forgetting function is introduced to enable dynamic updates of memory content,
enhancing the model's ability to manage historical information. To further
improve the effectiveness of memory operations, the study designs a joint
training objective. This combines the main task loss with constraints on memory
writing and forgetting. It guides the model to learn better memory strategies
during task execution. Systematic evaluation across multiple subtasks shows
that the model achieves clear advantages in text generation consistency,
stability in multi-turn question answering, and accuracy in cross-context
reasoning. In particular, the model demonstrates strong semantic retention and
contextual coherence in long-text tasks and complex question answering
scenarios. It effectively mitigates the context loss and semantic drift
problems commonly faced by traditional language models when handling long-term
dependencies. The experiments also include analysis of different memory
structures, capacity sizes, and control strategies. These results further
confirm the critical role of memory mechanisms in language understanding. They
demonstrate the feasibility and effectiveness of the proposed approach in both
architectural design and performance outcomes.

</details>


### [22] [Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging](https://arxiv.org/abs/2505.22934)
*Haobo Zhang,Jiayu Zhou*

Key words: 模型合并, 低秩适应, 多任务学习, 正交子空间

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种名为OSRM的方法，用于解决低秩适应（LoRA）微调模型在合并时性能下降的问题，通过正交子空间约束减少任务间的干扰，并在实验中验证了其有效性。

Motivation: 现有的模型合并方法在用于LoRA微调的模型时性能下降严重，这源于模型参数与数据分布之间未被充分考虑的相互作用。为了提高模型合并的效果，研究者提出了OSRM方法。

Method: 提出Orthogonal Subspaces for Robust model Merging (OSRM)，在微调前约束LoRA子空间，确保不同任务的更新不会互相干扰，同时兼容现有合并算法。

Result: 在8个数据集上的实验表明，OSRM不仅提升了合并性能，还保持了单任务的准确性，且对合并超参数更具鲁棒性。

Conclusion: 研究强调了数据-参数交互在模型合并中的重要性，并提供了一种即插即用的LoRA模型合并方案。

Abstract: Fine-tuning large language models (LMs) for individual tasks yields strong
performance but is expensive for deployment and storage. Recent works explore
model merging to combine multiple task-specific models into a single multi-task
model without additional training. However, existing merging methods often fail
for models fine-tuned with low-rank adaptation (LoRA), due to significant
performance degradation. In this paper, we show that this issue arises from a
previously overlooked interplay between model parameters and data
distributions. We propose Orthogonal Subspaces for Robust model Merging (OSRM)
to constrain the LoRA subspace *prior* to fine-tuning, ensuring that updates
relevant to one task do not adversely shift outputs for others. Our approach
can seamlessly integrate with most existing merging algorithms, reducing the
unintended interference among tasks. Extensive experiments on eight datasets,
tested with three widely used LMs and two large LMs, demonstrate that our
method not only boosts merging performance but also preserves single-task
accuracy. Furthermore, our approach exhibits greater robustness to the
hyperparameters of merging. These results highlight the importance of
data-parameter interaction in model merging and offer a plug-and-play solution
for merging LoRA models.

</details>


### [23] [Improving QA Efficiency with DistilBERT: Fine-Tuning and Inference on mobile Intel CPUs](https://arxiv.org/abs/2505.22937)
*Ngeyen Yinkfu*

Key words: Transformer, DistilBERT, 问答模型, SQuAD, 数据增强, CPU推理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究提出了一种基于Transformer的高效问答模型，适用于13代Intel i7-1355U CPU，通过数据增强和DistilBERT微调，在SQuAD v1.1上取得F1分数0.6536，推理时间0.1208秒/问题，优于规则基线和完整BERT模型。

Motivation: 针对资源受限系统（如CPU部署）设计高效的问答模型，平衡计算效率和准确性，适用于实时应用场景。

Method: 结合探索性数据分析、数据增强和DistilBERT架构微调，系统评估数据增强策略与超参数配置。

Result: 验证F1分数0.6536，推理时间0.1208秒/问题，显著优于规则基线（F1:0.3124）和完整BERT模型。

Conclusion: 该模型在计算效率和精度间取得良好平衡，适合资源受限系统，同时提供了针对CPU优化的Transformer模型实用见解。

Abstract: This study presents an efficient transformer-based question-answering (QA)
model optimized for deployment on a 13th Gen Intel i7-1355U CPU, using the
Stanford Question Answering Dataset (SQuAD) v1.1. Leveraging exploratory data
analysis, data augmentation, and fine-tuning of a DistilBERT architecture, the
model achieves a validation F1 score of 0.6536 with an average inference time
of 0.1208 seconds per question. Compared to a rule-based baseline (F1: 0.3124)
and full BERT-based models, our approach offers a favorable trade-off between
accuracy and computational efficiency. This makes it well-suited for real-time
applications on resource-constrained systems. The study includes systematic
evaluation of data augmentation strategies and hyperparameter configurations,
providing practical insights into optimizing transformer models for CPU-based
inference.

</details>


### [24] [WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning](https://arxiv.org/abs/2505.22942)
*Yuchen Zhuang,Di Jin,Jiaao Chen,Wenqi Shi,Hanrui Wang,Chao Zhang*

Key words: 大语言模型、网络代理、强化学习、业务导向任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了WorkForceAgent-R1，一种基于LLM的网络代理，通过R1风格强化学习框架提升业务导向网络任务的单步推理能力，显著超越SFT基线。

Motivation: 现有基于SFT的网络代理在处理动态网络交互时泛化性和鲁棒性不足，需增强推理能力。

Method: 采用基于规则的R1强化学习框架和结构化奖励函数，隐式学习中间推理。

Result: 在WorkArena基准测试中，WorkForceAgent-R1比SFT基线提升10.26-16.59%，接近GPT-4O性能。

Conclusion: WorkForceAgent-R1有效提升了业务导向网络任务的推理能力和性能。

Abstract: Large language models (LLMs)-empowered web agents enables automating complex,
real-time web navigation tasks in enterprise environments. However, existing
web agents relying on supervised fine-tuning (SFT) often struggle with
generalization and robustness due to insufficient reasoning capabilities when
handling the inherently dynamic nature of web interactions. In this study, we
introduce WorkForceAgent-R1, an LLM-based web agent trained using a rule-based
R1-style reinforcement learning framework designed explicitly to enhance
single-step reasoning and planning for business-oriented web navigation tasks.
We employ a structured reward function that evaluates both adherence to output
formats and correctness of actions, enabling WorkForceAgent-R1 to implicitly
learn robust intermediate reasoning without explicit annotations or extensive
expert demonstrations. Extensive experiments on the WorkArena benchmark
demonstrate that WorkForceAgent-R1 substantially outperforms SFT baselines by
10.26-16.59%, achieving competitive performance relative to proprietary
LLM-based agents (gpt-4o) in workplace-oriented web navigation tasks.

</details>


### [25] [Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates](https://arxiv.org/abs/2505.22943)
*Jaewoo Ahn,Heeseung Yun,Dayoon Ko,Gunhee Kim*

Key words: 多模态表示，组合性漏洞，对抗样本，自训练，零样本学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种多模态对抗组合性（MAC）基准，通过生成欺骗性文本样本揭示预训练多模态表示（如CLIP）的组合性漏洞，并提出一种自训练方法提升零样本性能。结果显示该方法在多种模态上表现优越。

Motivation: 预训练多模态表示（如CLIP）存在组合性漏洞，导致反直觉判断，需要一种方法揭示并改进这些漏洞。

Method: 引入MAC基准，利用大语言模型（LLMs）生成欺骗性文本样本，并采用自训练方法，结合拒绝采样微调和多样性促进过滤。

Result: 使用较小语言模型（如Llama-3.1-8B），该方法在图像、视频和音频等多种模态上显著提升攻击成功率和样本多样性。

Conclusion: MAC基准及自训练方法有效揭示了多模态表示的组合性漏洞，并在零样本任务中表现优越。

Abstract: While pre-trained multimodal representations (e.g., CLIP) have shown
impressive capabilities, they exhibit significant compositional vulnerabilities
leading to counterintuitive judgments. We introduce Multimodal Adversarial
Compositionality (MAC), a benchmark that leverages large language models (LLMs)
to generate deceptive text samples to exploit these vulnerabilities across
different modalities and evaluates them through both sample-wise attack success
rate and group-wise entropy-based diversity. To improve zero-shot methods, we
propose a self-training approach that leverages rejection-sampling fine-tuning
with diversity-promoting filtering, which enhances both attack success rate and
sample diversity. Using smaller language models like Llama-3.1-8B, our approach
demonstrates superior performance in revealing compositional vulnerabilities
across various multimodal representations, including images, videos, and
audios.

</details>


### [26] [OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature](https://arxiv.org/abs/2505.22945)
*Alisha Srivastava,Emir Korukluoglu,Minh Nhat Le,Duyen Tran,Chau Minh Pham,Marzena Karpinska,Mohit Iyyer*

Key words: 大语言模型, 跨语言记忆, 多语言能力, 机器翻译, OWL数据集

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了多语言大语言模型（LLMs）的跨语言记忆能力，发现即使对于未见过的翻译文本，模型仍能回忆起相关内容。

Motivation: 探索LLMs在非英语语言中的记忆能力及其跨语言迁移的可能性。

Method: 使用OWL数据集（包含10种语言的31.5K对齐文本），通过三种任务评估模型记忆能力：直接探测、名称填空和前缀生成。

Result: 发现LLMs跨语言记忆能力显著，如GPT-4o在新翻译文本中作者和标题识别准确率达69%，扰动仅略微降低性能。

Conclusion: LLMs展现出较强的跨语言记忆能力，为模型差异提供了新见解。

Abstract: Large language models (LLMs) are known to memorize and recall English text
from their pretraining data. However, the extent to which this ability
generalizes to non-English languages or transfers across languages remains
unclear. This paper investigates multilingual and cross-lingual memorization in
LLMs, probing if memorized content in one language (e.g., English) can be
recalled when presented in translation. To do so, we introduce OWL, a dataset
of 31.5K aligned excerpts from 20 books in ten languages, including English
originals, official translations (Vietnamese, Spanish, Turkish), and new
translations in six low-resource languages (Sesotho, Yoruba, Maithili,
Malagasy, Setswana, Tahitian). We evaluate memorization across model families
and sizes through three tasks: (1) direct probing, which asks the model to
identify a book's title and author; (2) name cloze, which requires predicting
masked character names; and (3) prefix probing, which involves generating
continuations. We find that LLMs consistently recall content across languages,
even for texts without direct translation in pretraining data. GPT-4o, for
example, identifies authors and titles 69% of the time and masked entities 6%
of the time in newly translated excerpts. Perturbations (e.g., masking
characters, shuffling words) modestly reduce direct probing accuracy (7% drop
for shuffled official translations). Our results highlight the extent of
cross-lingual memorization and provide insights on the differences between the
models.

</details>


### [27] [NegVQA: Can Vision Language Models Understand Negation?](https://arxiv.org/abs/2505.22946)
*Yuhui Zhang,Yuchang Su,Yiming Liu,Serena Yeung-Levy*

Key words: Negation, Vision Language Models, VQA benchmark, NegVQA, scaling trend

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了NegVQA，一个针对视觉语言模型（VLMs）理解否定能力的评测基准，包含7,379个二选一问题。评测发现现有VLM在否定问题上表现显著下降，并揭示了模型规模与性能的U型关系。

Motivation: 由于否定是语言中的核心现象且可能完全逆转句子含义，而当前视觉语言模型（VLMs）在高风险应用中日益广泛，评估其理解否定能力至关重要。

Method: 通过利用大语言模型生成现有VQA数据集中问题的否定版本，构建NegVQA基准，涵盖多样化的否定场景与图像-问题分布，并对20个前沿VLM进行评测。

Result: 发现VLM在否定问题上表现显著下降，且模型规模与性能呈U型关系（增大规模先降低性能后提升）。

Conclusion: NegVQA揭示了VLM在否定理解上的关键缺陷，为未来模型开发提供了洞察。

Abstract: Negation is a fundamental linguistic phenomenon that can entirely reverse the
meaning of a sentence. As vision language models (VLMs) continue to advance and
are deployed in high-stakes applications, assessing their ability to comprehend
negation becomes essential. To address this, we introduce NegVQA, a visual
question answering (VQA) benchmark consisting of 7,379 two-choice questions
covering diverse negation scenarios and image-question distributions. We
construct NegVQA by leveraging large language models to generate negated
versions of questions from existing VQA datasets. Evaluating 20
state-of-the-art VLMs across seven model families, we find that these models
struggle significantly with negation, exhibiting a substantial performance drop
compared to their responses to the original questions. Furthermore, we uncover
a U-shaped scaling trend, where increasing model size initially degrades
performance on NegVQA before leading to improvements. Our benchmark reveals
critical gaps in VLMs' negation understanding and offers insights into future
VLM development. Project page available at
https://yuhui-zh15.github.io/NegVQA/.

</details>


### [28] [StrucSum: Graph-Structured Reasoning for Long Document Extractive Summarization with LLMs](https://arxiv.org/abs/2505.22950)
*Haohan Yuan,Sukhwa Hong,Haopeng Zhang*

Key words: 大语言模型、零样本摘要、结构化提示、句子级图、事实一致性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: StrucSum是一种无需训练的提示框架，通过句子级图结构增强LLM推理能力，显著提升零样本摘要质量和事实一致性，优于无监督基线和普通提示方法。

Motivation: 现有LLM在零样本摘要任务中难以建模文档结构和识别长文本关键信息，为解决这一问题，作者提出了结构化提示方法。

Method: StrucSum通过三种策略注入结构信号：邻居感知提示（NAP）提取局部上下文、中心性感知提示（CAP）估计重要性、中心性引导掩码（CGM）实现高效输入压缩。

Result: 在ArXiv、PubMed和Multi-News数据集上，StrucSum显著提升摘要质量（如FactCC和SummaC指标分别提高19.2和9.7分），且无需训练或任务特定调优。

Conclusion: 结构化提示是一种简单有效的零样本抽取式摘要方法，可增强LLM对文档结构和重要信息的建模能力。

Abstract: Large language models (LLMs) have shown strong performance in zero-shot
summarization, but often struggle to model document structure and identify
salient information in long texts. In this work, we introduce StrucSum, a
training-free prompting framework that enhances LLM reasoning through
sentence-level graph structures. StrucSum injects structural signals into
prompts via three targeted strategies: Neighbor-Aware Prompting (NAP) for local
context, Centrality-Aware Prompting (CAP) for importance estimation, and
Centrality-Guided Masking (CGM) for efficient input reduction. Experiments on
ArXiv, PubMed, and Multi-News demonstrate that StrucSum consistently improves
both summary quality and factual consistency over unsupervised baselines and
vanilla prompting. Notably, on ArXiv, it boosts FactCC and SummaC by 19.2 and
9.7 points, indicating stronger alignment between summaries and source content.
These findings suggest that structure-aware prompting is a simple yet effective
approach for zero-shot extractive summarization with LLMs, without any training
or task-specific tuning.

</details>


### [29] [LLMs for Argument Mining: Detection, Extraction, and Relationship Classification of pre-defined Arguments in Online Comments](https://arxiv.org/abs/2505.22956)
*Matteo Guida,Yulia Otmakhova,Eduard Hovy,Lea Frermann*

Key words: 大语言模型, 论点挖掘, 公共讨论, 争议话题, 自动化分析

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究评估了四种先进大语言模型（LLM）在分析公共讨论中的论点挖掘任务上的表现，结果显示其总体表现良好，但在处理长、复杂及情感化评论时有系统性问题。

Motivation: 探讨LLM在争议话题（如堕胎）的公共讨论中挖掘预定义论点的能力，填补现有研究空白。

Method: 使用三种论点挖掘任务和六种争议主题的2000多条评论数据集，评估四种LLM的表现。

Result: 大型和微调后的LLM表现优异，但存在环境成本；对长文本、 nuanced（微妙）评论和情感化语言处理不足。

Conclusion: LLM在自动化论点分析中前景与局限并存，需改进对复杂语言的处理能力。

Abstract: Automated large-scale analysis of public discussions around contested issues
like abortion requires detecting and understanding the use of arguments. While
Large Language Models (LLMs) have shown promise in language processing tasks,
their performance in mining topic-specific, pre-defined arguments in online
comments remains underexplored. We evaluate four state-of-the-art LLMs on three
argument mining tasks using datasets comprising over 2,000 opinion comments
across six polarizing topics. Quantitative evaluation suggests an overall
strong performance across the three tasks, especially for large and fine-tuned
LLMs, albeit at a significant environmental cost. However, a detailed error
analysis revealed systematic shortcomings on long and nuanced comments and
emotionally charged language, raising concerns for downstream applications like
content moderation or opinion analysis. Our results highlight both the promise
and current limitations of LLMs for automated argument analysis in online
comments.

</details>


### [30] [LLM-based HSE Compliance Assessment: Benchmark, Performance, and Advancements](https://arxiv.org/abs/2505.22959)
*Jianwei Wang,Mengqi Wang,Yinsi Zhou,Zhenchang Xing,Qing Liu,Xiwei Xu,Wenjie Zhang,Liming Zhu*

Key words: HSE合规性、大型语言模型、基准数据集、法律推理、专家推理（RoE）

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了HSE-Bench，一个评估大型语言模型（LLM）在健康、安全与环境（HSE）合规性评估领域能力的首个基准数据集，提出了专家推理（RoE）提示技术以改进LLM的推理能力。

Motivation: HSE合规性评估需要动态实时决策，但当前LLM在HSE领域知识和结构化法律推理方面能力不足，缺乏系统性的法律推理。

Method: 构建包含1000多个问题的HSE-Bench数据集，基于IRAC（问题识别、规则召回、规则应用、规则结论）评估LLM表现，并提出RoE提示技术。

Result: 当前LLM表现良好，但更多依赖语义匹配而非基于HSE上下文的推理，RoE技术能显著提升其推理能力。

Conclusion: 研究揭示了LLM在HSE合规性推理中的不足，RoE技术为未来研究提供了改进方向。

Abstract: Health, Safety, and Environment (HSE) compliance assessment demands dynamic
real-time decision-making under complicated regulations and complex
human-machine-environment interactions. While large language models (LLMs) hold
significant potential for decision intelligence and contextual dialogue, their
capacity for domain-specific knowledge in HSE and structured legal reasoning
remains underexplored. We introduce HSE-Bench, the first benchmark dataset
designed to evaluate the HSE compliance assessment capabilities of LLM.
HSE-Bench comprises over 1,000 manually curated questions drawn from
regulations, court cases, safety exams, and fieldwork videos, and integrates a
reasoning flow based on Issue spotting, rule Recall, rule Application, and rule
Conclusion (IRAC) to assess the holistic reasoning pipeline. We conduct
extensive evaluations on different prompting strategies and more than 10 LLMs,
including foundation models, reasoning models and multimodal vision models. The
results show that, although current LLMs achieve good performance, their
capabilities largely rely on semantic matching rather than principled reasoning
grounded in the underlying HSE compliance context. Moreover, their native
reasoning trace lacks the systematic legal reasoning required for rigorous HSE
compliance assessment. To alleviate these, we propose a new prompting
technique, Reasoning of Expert (RoE), which guides LLMs to simulate the
reasoning process of different experts for compliance assessment and reach a
more accurate unified decision. We hope our study highlights reasoning gaps in
LLMs for HSE compliance and inspires further research on related tasks.

</details>


### [31] [ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind](https://arxiv.org/abs/2505.22961)
*Peixuan Han,Zijia Liu,Jiaxuan You*

Key words: 大型语言模型, 心理理论, 强化学习, 说服力

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种名为ToMAP的新方法，通过结合两个心理理论模块增强语言模型的说服力，使其能更好地理解和分析对手的心理状态，从而生成更有效的论证。实验表明，ToMAP在参数较少的情况下优于更大的基线模型。

Motivation: 现有的大型语言模型在说服任务中缺乏对对手心理状态的动态建模能力，导致论证多样性和对手意识不足。为此，作者旨在开发一种增强对手意识的方法。

Method: 论文提出了ToMAP方法，通过心理理论模块预测对手立场，并结合强化学习训练模型生成更有效的论证。

Result: 实验结果显示，ToMAP在多项指标上优于GPT-4o等基线模型，论证多样性和逻辑性显著提升。

Conclusion: ToMAP通过增强对手意识，显著提升了语言模型的说服力，为开发更强大的说服代理提供了新思路。

Abstract: Large language models (LLMs) have shown promising potential in persuasion,
but existing works on training LLM persuaders are still preliminary. Notably,
while humans are skilled in modeling their opponent's thoughts and opinions
proactively and dynamically, current LLMs struggle with such Theory of Mind
(ToM) reasoning, resulting in limited diversity and opponent awareness. To
address this limitation, we introduce Theory of Mind Augmented Persuader
(ToMAP), a novel approach for building more flexible persuader agents by
incorporating two theory of mind modules that enhance the persuader's awareness
and analysis of the opponent's mental state. Specifically, we begin by
prompting the persuader to consider possible objections to the target central
claim, and then use a text encoder paired with a trained MLP classifier to
predict the opponent's current stance on these counterclaims. Our carefully
designed reinforcement learning schema enables the persuader learns how to
analyze opponent-related information and utilize it to generate more effective
arguments. Experiments show that the ToMAP persuader, while containing only 3B
parameters, outperforms much larger baselines, like GPT-4o, with a relative
gain of 39.4% across multiple persuadee models and diverse corpora. Notably,
ToMAP exhibits complex reasoning chains and reduced repetition during training,
which leads to more diverse and effective arguments. The opponent-aware feature
of ToMAP also makes it suitable for long conversations and enables it to employ
more logical and opponent-aware strategies. These results underscore our
method's effectiveness and highlight its potential for developing more
persuasive language agents. Code is available at:
https://github.com/ulab-uiuc/ToMAP.

</details>


### [32] [Exploring Scaling Laws for EHR Foundation Models](https://arxiv.org/abs/2505.22964)
*Sheng Zhang,Qin Liu,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon*

Key words: scaling laws, EHR, transformer, MIMIC-IV, clinical utility

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究首次探讨了电子健康记录（EHR）基础模型的规模定律，通过训练不同规模的Transformer模型并分析其性能，发现EHR模型表现出与大型语言模型类似的规模扩展性，为临床预测任务和个性化医疗提供了资源高效训练的指导。

Motivation: 探究EHR数据是否具有与语言模型类似的规模扩展性，以指导资源高效的EHR基础模型训练和临床应用的开发。

Method: 使用Transformer架构在MIMIC-IV数据库中的患者时间线数据上进行训练，比较不同模型规模与计算预算下的性能表现。

Result: 发现了EHR模型的抛物线IsoFLOPs曲线及计算、参数量与数据规模之间的幂律关系，表明其扩展性与LLMs类似。

Conclusion: EHR基础模型展示出类似LLMs的规模扩展性，为临床预测和个性化医疗模型的高效开发提供了理论基础。

Abstract: The emergence of scaling laws has profoundly shaped the development of large
language models (LLMs), enabling predictable performance gains through
systematic increases in model size, dataset volume, and compute. Yet, these
principles remain largely unexplored in the context of electronic health
records (EHRs) -- a rich, sequential, and globally abundant data source that
differs structurally from natural language. In this work, we present the first
empirical investigation of scaling laws for EHR foundation models. By training
transformer architectures on patient timeline data from the MIMIC-IV database
across varying model sizes and compute budgets, we identify consistent scaling
patterns, including parabolic IsoFLOPs curves and power-law relationships
between compute, model parameters, data size, and clinical utility. These
findings demonstrate that EHR models exhibit scaling behavior analogous to
LLMs, offering predictive insights into resource-efficient training strategies.
Our results lay the groundwork for developing powerful EHR foundation models
capable of transforming clinical prediction tasks and advancing personalized
healthcare.

</details>


### [33] [Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation](https://arxiv.org/abs/2505.22993)
*Hoang Pham,Thanh-Do Nguyen,Khac-Hoai Nam Bui*

Key words: claim verification, LLMs, graph representation, entity disambiguation, fact-checking

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一个名为VeGraph的新框架，利用LLM智能体进行复杂声明验证，通过图表示、实体消歧和验证三步骤提升准确性和可解释性。

Motivation: 解决复杂声明的验证挑战，特别是在LLM时代中语义结构和模糊实体的问题。

Method: VeGraph框架分三个阶段：图表示（分解声明为三元组图）、实体消歧（与知识库交互解决模糊实体）、验证（完成事实核查）。

Result: 在HoVer和FEVEROUS基准测试中表现优异，验证了框架的有效性。

Conclusion: VeGraph通过图结构和LLM推理能力，显著提升了复杂声明验证的性能和可解释性。

Abstract: Claim verification is a long-standing and challenging task that demands not
only high accuracy but also explainability of the verification process. This
task becomes an emerging research issue in the era of large language models
(LLMs) since real-world claims are often complex, featuring intricate semantic
structures or obfuscated entities. Traditional approaches typically address
this by decomposing claims into sub-claims and querying a knowledge base to
resolve hidden or ambiguous entities. However, the absence of effective
disambiguation strategies for these entities can compromise the entire
verification process. To address these challenges, we propose
Verify-in-the-Graph (VeGraph), a novel framework leveraging the reasoning and
comprehension abilities of LLM agents. VeGraph operates in three phases: (1)
Graph Representation - an input claim is decomposed into structured triplets,
forming a graph-based representation that integrates both structured and
unstructured information; (2) Entity Disambiguation -VeGraph iteratively
interacts with the knowledge base to resolve ambiguous entities within the
graph for deeper sub-claim verification; and (3) Verification - remaining
triplets are verified to complete the fact-checking process. Experiments using
Meta-Llama-3-70B (instruct version) show that VeGraph achieves competitive
performance compared to baselines on two benchmarks HoVer and FEVEROUS,
effectively addressing claim verification challenges. Our source code and data
are available for further exploitation.

</details>


### [34] [DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors](https://arxiv.org/abs/2505.23001)
*Yize Cheng,Wenxiao Wang,Mazda Moayeri,Soheil Feizi*

Key words: 基准测试污染、后门攻击、误报率、模型检测、DyePack

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DyePack框架利用后门攻击技术，无需模型内部信息即可检测是否在训练中使用了基准测试集，确保低误报率且高效识别污染模型。

Motivation: 开放基准测试易受测试集污染的滥用，需一种无需接触模型内部即可准确检测污染的方法。

Method: DyePack通过混入带后门的测试样本，结合多后门随机目标设计，实现可计算的极低误报率检测。

Result: 在多项选择任务（如MMLU-Pro、Big-Bench-Hard）及开放生成任务（如Alpaca）中，误报率分别低至0.000073%、0.000017%和0.127%，且成功识别所有污染模型。

Conclusion: DyePack为基准测试污染提供了可验证且高精度的解决方案，尤其适用于多场景任务。

Abstract: Open benchmarks are essential for evaluating and advancing large language
models, offering reproducibility and transparency. However, their accessibility
makes them likely targets of test set contamination. In this work, we introduce
DyePack, a framework that leverages backdoor attacks to identify models that
used benchmark test sets during training, without requiring access to the loss,
logits, or any internal details of the model. Like how banks mix dye packs with
their money to mark robbers, DyePack mixes backdoor samples with the test data
to flag models that trained on it. We propose a principled design incorporating
multiple backdoors with stochastic targets, enabling exact false positive rate
(FPR) computation when flagging every model. This provably prevents false
accusations while providing strong evidence for every detected case of
contamination. We evaluate DyePack on five models across three datasets,
covering both multiple-choice and open-ended generation tasks. For
multiple-choice questions, it successfully detects all contaminated models with
guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard
using eight backdoors. For open-ended generation tasks, it generalizes well and
identifies all contaminated models on Alpaca with a guaranteed false positive
rate of just 0.127% using six backdoors.

</details>


### [35] [A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs](https://arxiv.org/abs/2505.23006)
*Chiwan Park,Wonjun Jang,Daeryong Kim,Aelim Ahn,Kichang Yang,Woosung Hwang,Jihyeon Roh,Hyerin Park,Hyosun Wang,Min Seok Kim,Jihoon Kang*

Key words: Large Language Models, Industrial Application, Conversational Agent, E-commerce

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文探讨了如何将先进的大型语言模型（LLMs）应用于工业环境中的挑战，特别是在电子商务领域的对话代理中，提出了一种兼顾灵活对话能力和严格服务约束的解决方案。

Motivation: 面对工业环境中严格的服务约束与LLMs的灵活性之间的冲突，如何有效应用SOTA研究成为关键挑战。

Method: 通过案例研究，详细展示了在电子商务领域的对话代理实现流程及优化策略。

Result: 提出了一种可扩展、可控且可靠的AI驱动代理开发框架，为学术研究与实际应用的桥梁提供见解。

Conclusion: 研究证实了所提出框架的有效性，为LLMs在工业环境中的应用提供了新的实践路径。

Abstract: The advancement of Large Language Models (LLMs) has led to significant
improvements in various service domains, including search, recommendation, and
chatbot applications. However, applying state-of-the-art (SOTA) research to
industrial settings presents challenges, as it requires maintaining flexible
conversational abilities while also strictly complying with service-specific
constraints. This can be seen as two conflicting requirements due to the
probabilistic nature of LLMs. In this paper, we propose our approach to
addressing this challenge and detail the strategies we employed to overcome
their inherent limitations in real-world applications. We conduct a practical
case study of a conversational agent designed for the e-commerce domain,
detailing our implementation workflow and optimizations. Our findings provide
insights into bridging the gap between academic research and real-world
application, introducing a framework for developing scalable, controllable, and
reliable AI-driven agents.

</details>


### [36] [Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models](https://arxiv.org/abs/2505.23015)
*Jinwen Chen,Hainan Zhang,Fei Sun,Qinnan Zhang,Sijia Wen,Ziwei Wang,Zhiming Zheng*

Key words: 后门检测、LLM、TF-IDF聚类、参考过滤、安全风险

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于参考过滤和TF-IDF聚类的隐蔽后门样本检测方法（RFTC），用于识别LLM中的恶意样本，实验证明其在检测效果和模型性能上优于基线。

Motivation: 当前主流检测方法无法适用于生成任务或可能降低生成性能，导致隐蔽后门样本的高效清除问题亟待解决。

Method: 通过参考模型输出对比筛选可疑样本，再利用TF-IDF聚类基于类内距离识别真实后门样本。

Result: 在两个机器翻译数据集和一个问答数据集上，RFTC在检测效果和模型性能上优于基线方法。

Conclusion: RFTC能有效识别隐蔽后门样本，且参考过滤机制在不同参考模型下均表现稳定。

Abstract: Fine-tuning LLMs with datasets containing stealthy backdoors from publishers
poses security risks to downstream applications. Mainstream detection methods
either identify poisoned samples by analyzing the prediction probability of
poisoned classification models or rely on the rewriting model to eliminate the
stealthy triggers. However, the former cannot be applied to generation tasks,
while the latter may degrade generation performance and introduce new triggers.
Therefore, efficiently eliminating stealthy poisoned samples for LLMs remains
an urgent problem. We observe that after applying TF-IDF clustering to the
sample response, there are notable differences in the intra-class distances
between clean and poisoned samples. Poisoned samples tend to cluster closely
because of their specific malicious outputs, whereas clean samples are more
scattered due to their more varied responses. Thus, in this paper, we propose a
stealthy backdoor sample detection method based on Reference-Filtration and
Tfidf-Clustering mechanisms (RFTC). Specifically, we first compare the sample
response with the reference model's outputs and consider the sample suspicious
if there's a significant discrepancy. And then we perform TF-IDF clustering on
these suspicious samples to identify the true poisoned samples based on the
intra-class distance. Experiments on two machine translation datasets and one
QA dataset demonstrate that RFTC outperforms baselines in backdoor detection
and model performance. Further analysis of different reference models also
confirms the effectiveness of our Reference-Filtration.

</details>


### [37] [Context Robust Knowledge Editing for Language Models](https://arxiv.org/abs/2505.23026)
*Haewon Park,Gyubin Choi,Minjun Kim,Yohan Jo*

Key words: 知识编辑,上下文鲁棒性,CHED基准,CoRE方法,隐藏状态方差

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: CHED评估了知识编辑（KE）方法的上下文鲁棒性，发现现有方法在前置上下文存在时常失效。CoRE方法通过最小化隐藏状态的上下文敏感方差提升了成功率。

Motivation: 当前KE评估缺乏对前置上下文的考虑，导致实际应用中编辑效果不佳，需开发更健壮的评估和方法。

Method: 提出CHED基准和CoRE方法，后者通过优化隐藏状态方差增强上下文鲁棒性。

Result: CoRE在前置上下文存在时显著提升编辑成功率，且不影响模型整体性能。

Conclusion: 上下文鲁棒性是KE的关键挑战，CoRE为改进方向提供了有效解决方案。

Abstract: Knowledge editing (KE) methods offer an efficient way to modify knowledge in
large language models. Current KE evaluations typically assess editing success
by considering only the edited knowledge without any preceding contexts. In
real-world applications, however, preceding contexts often trigger the
retrieval of the original knowledge and undermine the intended edit. To address
this issue, we develop CHED -- a benchmark designed to evaluate the context
robustness of KE methods. Evaluations on CHED show that they often fail when
preceding contexts are present. To mitigate this shortcoming, we introduce
CoRE, a KE method designed to strengthen context robustness by minimizing
context-sensitive variance in hidden states of the model for edited knowledge.
This method not only improves the editing success rate in situations where a
preceding context is present but also preserves the overall capabilities of the
model. We provide an in-depth analysis of the differing impacts of preceding
contexts when introduced as user utterances versus assistant responses, and we
dissect attention-score patterns to assess how specific tokens influence
editing success.

</details>


### [38] [Uncovering Visual-Semantic Psycholinguistic Properties from the Distributional Structure of Text Embedding Spac](https://arxiv.org/abs/2505.23029)
*Si Wu,Sebastian Bruch*

Key words: imageability, concreteness, psycholinguistics, NSM, unsupervised learning

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 通过无监督方法Neighborhood Stability Measure (NSM)衡量语义嵌入空间中单词邻域的峰值度，以预测图像性和具体性，效果优于现有方法。

Motivation: 本文基于图像数据集中的文本信号足够预测图像性和具体性的假设，探索语义嵌入空间中的峰值度作为这两种属性的指标。

Method: 提出无监督、无分布限制的NSM方法，量化语义嵌入空间中单词邻域的峰值度。

Result: 实验表明，NSM与真实评分的相关性高于其他无监督方法，且能有效分类图像性和具体性。

Conclusion: 文本信号本身即可有效预测图像性和具体性，NSM是一种高效的预测工具。

Abstract: Imageability (potential of text to evoke a mental image) and concreteness
(perceptibility of text) are two psycholinguistic properties that link visual
and semantic spaces. It is little surprise that computational methods that
estimate them do so using parallel visual and semantic spaces, such as
collections of image-caption pairs or multi-modal models. In this paper, we
work on the supposition that text itself in an image-caption dataset offers
sufficient signals to accurately estimate these properties. We hypothesize, in
particular, that the peakedness of the neighborhood of a word in the semantic
embedding space reflects its degree of imageability and concreteness. We then
propose an unsupervised, distribution-free measure, which we call Neighborhood
Stability Measure (NSM), that quantifies the sharpness of peaks. Extensive
experiments show that NSM correlates more strongly with ground-truth ratings
than existing unsupervised methods, and is a strong predictor of these
properties for classification. Our code and data are available on GitHub
(https://github.com/Artificial-Memory-Lab/imageability).

</details>


### [39] [Can Modern NLP Systems Reliably Annotate Chest Radiography Exams? A Pre-Purchase Evaluation and Comparative Study of Solutions from AWS, Google, Azure, John Snow Labs, and Open-Source Models on an Independent Pediatric Dataset](https://arxiv.org/abs/2505.23030)
*Shruti Hegde,Mabon Manoj Ninan,Jonathan R. Dillman,Shireen Hayatghaibi,Lynn Babcock,Elanchezhian Somasundaram*

Key words: 临床NLP, 儿科CXR报告, 实体提取, 断言检测, 性能评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究比较了四种商业临床NLP系统及两款专用胸部放射报告标注工具在儿科CXR报告中的实体提取和断言检测性能，发现各工具表现差异显著，强调了部署前验证的重要性。

Motivation: 探讨通用临床NLP工具在儿科CXR报告标注任务中的表现，填补特定任务独立评估的空白。

Method: 分析95,008份儿科CXR报告，使用四种NLP系统和两款专用工具提取实体及断言状态，通过Fleiss Kappa和准确率评估性能。

Result: 各工具在实体提取数量和断言准确率上差异显著，SparkNLP表现最佳（76%），Amazon Comprehend Medical最低（50%），专用工具准确率为56%。

Conclusion: 临床NLP工具性能差异显著，部署前需严格验证和审查。

Abstract: General-purpose clinical natural language processing (NLP) tools are
increasingly used for the automatic labeling of clinical reports. However,
independent evaluations for specific tasks, such as pediatric chest radiograph
(CXR) report labeling, are limited. This study compares four commercial
clinical NLP systems - Amazon Comprehend Medical (AWS), Google Healthcare NLP
(GC), Azure Clinical NLP (AZ), and SparkNLP (SP) - for entity extraction and
assertion detection in pediatric CXR reports. Additionally, CheXpert and
CheXbert, two dedicated chest radiograph report labelers, were evaluated on the
same task using CheXpert-defined labels. We analyzed 95,008 pediatric CXR
reports from a large academic pediatric hospital. Entities and assertion
statuses (positive, negative, uncertain) from the findings and impression
sections were extracted by the NLP systems, with impression section entities
mapped to 12 disease categories and a No Findings category. CheXpert and
CheXbert extracted the same 13 categories. Outputs were compared using Fleiss
Kappa and accuracy against a consensus pseudo-ground truth. Significant
differences were found in the number of extracted entities and assertion
distributions across NLP systems. SP extracted 49,688 unique entities, GC
16,477, AZ 31,543, and AWS 27,216. Assertion accuracy across models averaged
around 62%, with SP highest (76%) and AWS lowest (50%). CheXpert and CheXbert
achieved 56% accuracy. Considerable variability in performance highlights the
need for careful validation and review before deploying NLP tools for clinical
report labeling.

</details>


### [40] [Machine-Facing English: Defining a Hybrid Register Shaped by Human-AI Discourse](https://arxiv.org/abs/2505.23035)
*Hyunwoo Kim,Hanau Yi*

Key words: 机器面向英语，人机交互，语体理论，自然语言处理，语言演化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究探讨了机器面向英语（MFE）作为一种新兴语体的特征及其在人机互动中的演化，分析了其如何通过语法僵化、语用简化和超显性表达提高机器解析效率，但也牺牲了语言的流畅性和表达多样性。

Motivation: 随着AI交互的普及，日常语言逐渐适应机器需求，形成MFE，研究旨在揭示这种语言现象的演化过程及其对人机交互和语言教学的影响。

Method: 基于双语（韩语/英语）有声与文本产品测试的定性观察，结合自然语言声明性提示（NLD-P）的人工管理，通过主题分析识别出MFE的五种常见特征。

Result: 研究发现，MFE通过冗余清晰度、指令性语法、受限词汇、扁平语调和单一意图结构提高了机器执行准确性，但压缩了表达的丰富性。

Conclusion: MFE的演化揭示了沟通效率与语言丰富性之间的张力，为会话界面设计和多语言用户教学提出了挑战，并强调未来需要实证验证和方法论的完善。

Abstract: Machine-Facing English (MFE) is an emergent register shaped by the adaptation
of everyday language to the expanding presence of AI interlocutors. Drawing on
register theory (Halliday 1985, 2006), enregisterment (Agha 2003), audience
design (Bell 1984), and interactional pragmatics (Giles & Ogay 2007), this
study traces how sustained human-AI interaction normalizes syntactic rigidity,
pragmatic simplification, and hyper-explicit phrasing - features that enhance
machine parseability at the expense of natural fluency. Our analysis is
grounded in qualitative observations from bilingual (Korean/English) voice- and
text-based product testing sessions, with reflexive drafting conducted using
Natural Language Declarative Prompting (NLD-P) under human curation. Thematic
analysis identifies five recurrent traits - redundant clarity, directive
syntax, controlled vocabulary, flattened prosody, and single-intent structuring
- that improve execution accuracy but compress expressive range. MFE's
evolution highlights a persistent tension between communicative efficiency and
linguistic richness, raising design challenges for conversational interfaces
and pedagogical considerations for multilingual users. We conclude by
underscoring the need for comprehensive methodological exposition and future
empirical validation.

</details>


### [41] [Improving Multilingual Social Media Insights: Aspect-based Comment Analysis](https://arxiv.org/abs/2505.23037)
*Longyin Zhang,Bowei Zou,Ai Ti Aw*

Key words: 社交媒体评论, 方面术语生成, 多语言大模型, DPO, CAT-G测试集

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种通过细粒度识别和生成评论方面术语的方法，利用多语言大模型和DPO优化，提升社交媒体评论的理解能力，并贡献了首个多语言CAT-G测试集。

Motivation: 社交媒体评论的自由性和多样性给NLP任务带来挑战，缺乏细粒度方面术语识别的方法阻碍了评论聚類、总结和意见分析的效果提升。

Method: 提出CAT-G方法，结合多语言大模型的监督微调及DPO对齐，从个体评论中生成方面术语以引导模型注意力。

Result: 方法显著提升了两个NLP任务中对社交媒体讨论的理解，并建立了覆盖四种语言的首个多语言CAT-G测试集。

Conclusion: 细粒度方面术语生成和多语言测试集的结合，为跨语言NLP任务提供了新的研究基准和改进方向。

Abstract: The inherent nature of social media posts, characterized by the freedom of
language use with a disjointed array of diverse opinions and topics, poses
significant challenges to downstream NLP tasks such as comment clustering,
comment summarization, and social media opinion analysis. To address this, we
propose a granular level of identifying and generating aspect terms from
individual comments to guide model attention. Specifically, we leverage
multilingual large language models with supervised fine-tuning for comment
aspect term generation (CAT-G), further aligning the model's predictions with
human expectations through DPO. We demonstrate the effectiveness of our method
in enhancing the comprehension of social media discourse on two NLP tasks.
Moreover, this paper contributes the first multilingual CAT-G test set on
English, Chinese, Malay, and Bahasa Indonesian. As LLM capabilities vary among
languages, this test set allows for a comparative analysis of performance
across languages with varying levels of LLM proficiency.

</details>


### [42] [EL4NER: Ensemble Learning for Named Entity Recognition via Multiple Small-Parameter Large Language Models](https://arxiv.org/abs/2505.23038)
*Yuzhen Xiao,Jiahe Song,Yongxin Xu,Ruizhe Zhang,Yiqi Xiao,Xin Lu,Runchuan Zhu,Bowen Jiang,Junfeng Zhao*

Key words: In-Context Learning, Named Entity Recognition, Ensemble Learning, Small-Parameter LLMs, Parameter Efficiency

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为EL4NER的集成学习方法，通过结合多个开源小参数LLM的ICL输出，以更低的部署和推理成本提升NER任务性能，并在多个数据集上超越大多数闭源大参数LLM方法，甚至达到SOTA性能。

Motivation: 为解决基于大参数LLM的ICL方法在NER任务中存在的计算资源消耗大、API成本高、数据隐私和社区协作限制等问题，提出一种更高效的小参数模型集成方案。

Method: 1. 设计基于任务分解的多阶段集成学习流程；2. 提出适用于NER任务的span级句子相似性算法以优化ICL示例检索；3. 引入自验证机制减少集成过程中的噪声。

Result: 实验表明，EL4NER以更低参数量和成本超越多数闭源大参数LLM方法，部分数据集上达到ICL-based SOTA性能。

Conclusion: EL4NER证明了在ICL范式下使用开源小参数LLM进行NER任务的可行性，并展现了参数高效性。

Abstract: In-Context Learning (ICL) technique based on Large Language Models (LLMs) has
gained prominence in Named Entity Recognition (NER) tasks for its lower
computing resource consumption, less manual labeling overhead, and stronger
generalizability. Nevertheless, most ICL-based NER methods depend on
large-parameter LLMs: the open-source models demand substantial computational
resources for deployment and inference, while the closed-source ones incur high
API costs, raise data-privacy concerns, and hinder community collaboration. To
address this question, we propose an Ensemble Learning Method for Named Entity
Recognition (EL4NER), which aims at aggregating the ICL outputs of multiple
open-source, small-parameter LLMs to enhance overall performance in NER tasks
at less deployment and inference cost. Specifically, our method comprises three
key components. First, we design a task decomposition-based pipeline that
facilitates deep, multi-stage ensemble learning. Second, we introduce a novel
span-level sentence similarity algorithm to establish an ICL demonstration
retrieval mechanism better suited for NER tasks. Third, we incorporate a
self-validation mechanism to mitigate the noise introduced during the ensemble
process. We evaluated EL4NER on multiple widely adopted NER datasets from
diverse domains. Our experimental results indicate that EL4NER surpasses most
closed-source, large-parameter LLM-based methods at a lower parameter cost and
even attains state-of-the-art (SOTA) performance among ICL-based methods on
certain datasets. These results show the parameter efficiency of EL4NER and
underscore the feasibility of employing open-source, small-parameter LLMs
within the ICL paradigm for NER tasks.

</details>


### [43] [Query Routing for Retrieval-Augmented Language Models](https://arxiv.org/abs/2505.23052)
*Jiarui Zhang,Xiangyu Liu,Yong Hu,Chaoyue Niu,Fan Wu,Guihai Chen*

Key words: Retrieval-Augmented Generation, Large Language Models, routing, document embeddings, contrastive learning

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: RAGRouter improves response quality in RAG tasks by dynamically routing queries to the best LLM based on document embeddings, outperforming existing methods by 3.29%-9.33%.

Motivation: Existing routing methods for RAG scenarios rely on static knowledge representations, leading to suboptimal performance due to dynamic document influence.

Method: Proposes RAGRouter, a routing framework integrating document embeddings and RAG capability embeddings via contrastive learning to dynamically select the best LLM.

Result: Outperforms the best individual LLM by 3.61% and existing routing methods by 3.29%-9.33% on diverse tasks, achieving efficient performance-latency trade-offs.

Conclusion: RAGRouter effectively addresses the dynamic influence of retrieved documents in RAG scenarios, enhancing routing performance and efficiency.

Abstract: Retrieval-Augmented Generation (RAG) significantly improves the performance
of Large Language Models (LLMs) on knowledge-intensive tasks. However, varying
response quality across LLMs under RAG necessitates intelligent routing
mechanisms, which select the most suitable model for each query from multiple
retrieval-augmented LLMs via a dedicated router model. We observe that external
documents dynamically affect LLMs' ability to answer queries, while existing
routing methods, which rely on static parametric knowledge representations,
exhibit suboptimal performance in RAG scenarios. To address this, we formally
define the new retrieval-augmented LLM routing problem, incorporating the
influence of retrieved documents into the routing framework. We propose
RAGRouter, a RAG-aware routing design, which leverages document embeddings and
RAG capability embeddings with contrastive learning to capture knowledge
representation shifts and enable informed routing decisions. Extensive
experiments on diverse knowledge-intensive tasks and retrieval settings show
that RAGRouter outperforms the best individual LLM by 3.61% on average and
existing routing methods by 3.29%-9.33%. With an extended score-threshold-based
mechanism, it also achieves strong performance-efficiency trade-offs under
low-latency constraints.

</details>


### [44] [Self-Correcting Code Generation Using Small Language Models](https://arxiv.org/abs/2505.23060)
*Jeonghun Cho,Deokhyung Kang,Hyounghun Kim,Gary Geunbae Lee*

Key words: 自我修正、代码生成、小型语言模型、强化学习、多轮修正

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 小型语言模型在自我修正代码生成方面表现不佳，研究提出CoCoS方法，通过多轮修正提升其能力，显著优于基线。

Motivation: 探索小型语言模型是否具备通过自我反思修正代码的能力，发现其表现不足，需开发新方法提升。

Method: 提出CoCoS方法，采用在线强化学习目标，设计累积奖励函数和细粒度奖励，优化多轮修正过程。

Result: 在1B规模模型上，CoCoS在MBPP和HumanEval数据集上分别提升35.8%和27.7%。

Conclusion: CoCoS有效增强小型模型的自我修正能力，为代码生成任务提供新解决方案。

Abstract: Self-correction has demonstrated potential in code generation by allowing
language models to revise and improve their outputs through successive
refinement. Recent studies have explored prompting-based strategies that
incorporate verification or feedback loops using proprietary models, as well as
training-based methods that leverage their strong reasoning capabilities.
However, whether smaller models possess the capacity to effectively guide their
outputs through self-reflection remains unexplored. Our findings reveal that
smaller models struggle to exhibit reflective revision behavior across both
self-correction paradigms. In response, we introduce CoCoS, an approach
designed to enhance the ability of small language models for multi-turn code
correction. Specifically, we propose an online reinforcement learning objective
that trains the model to confidently maintain correct outputs while
progressively correcting incorrect outputs as turns proceed. Our approach
features an accumulated reward function that aggregates rewards across the
entire trajectory and a fine-grained reward better suited to multi-turn
correction scenarios. This facilitates the model in enhancing initial response
quality while achieving substantial improvements through self-correction. With
1B-scale models, CoCoS achieves improvements of 35.8% on the MBPP and 27.7% on
HumanEval compared to the baselines.

</details>


### [45] [SNS-Bench-VL: Benchmarking Multimodal Large Language Models in Social Networking Services](https://arxiv.org/abs/2505.23065)
*Hongcheng Guo,Zheyong Xie,Shaosheng Cao,Boyang Wang,Weiting Liu,Anjie Le,Lei Li,Zhoujun Li*

Key words: 多模态基准测试、视觉-语言模型、社交媒体、大语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出SNS-Bench-VL，一个用于评估视觉-语言大模型在真实社交媒体场景中性能的多模态基准测试，涵盖8种任务和4001个问答对，测试了25种模型，揭示了多模态社交语境理解中的挑战。

Motivation: 随着社交媒体中视觉与文本内容的融合日益增多，评估大语言模型的多模态能力对提升用户体验、内容理解和平台智能化至关重要。现有基准测试主要关注纯文本任务，缺乏对现代社交媒体生态中多模态语境的覆盖。

Method: 引入了SNS-Bench-VL基准测试，包含8种多模态任务（如笔记理解、用户参与分析和个性化推荐）和4001个问答对。测试了25种前沿多模态大语言模型。

Result: 研究结果突显了在多模态社交语境理解中的持续挑战。

Conclusion: 期望SNS-Bench-VL能推动未来研究，以发展更鲁棒、情境感知且人类对齐的多模态智能，服务于下一代社交媒体服务。

Abstract: With the increasing integration of visual and textual content in Social
Networking Services (SNS), evaluating the multimodal capabilities of Large
Language Models (LLMs) is crucial for enhancing user experience, content
understanding, and platform intelligence. Existing benchmarks primarily focus
on text-centric tasks, lacking coverage of the multimodal contexts prevalent in
modern SNS ecosystems. In this paper, we introduce SNS-Bench-VL, a
comprehensive multimodal benchmark designed to assess the performance of
Vision-Language LLMs in real-world social media scenarios. SNS-Bench-VL
incorporates images and text across 8 multimodal tasks, including note
comprehension, user engagement analysis, information retrieval, and
personalized recommendation. It comprises 4,001 carefully curated multimodal
question-answer pairs, covering single-choice, multiple-choice, and open-ended
tasks. We evaluate over 25 state-of-the-art multimodal LLMs, analyzing their
performance across tasks. Our findings highlight persistent challenges in
multimodal social context comprehension. We hope SNS-Bench-VL will inspire
future research towards robust, context-aware, and human-aligned multimodal
intelligence for next-generation social networking services.

</details>


### [46] [Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport](https://arxiv.org/abs/2505.23078)
*Yuu Jinnai*

Key words: 文档级文本生成, 最小贝叶斯风险解码, Wasserstein距离, 机器翻译, 文本简化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了将最小贝叶斯风险（MBR）解码适配到文档级文本生成任务中，提出MBR-OT方法，利用Wasserstein距离计算文档效用，提升生成质量。

Motivation: 文档级文本生成任务因需理解更长上下文而更具挑战性，现有MBR解码的效用函数主要针对句子级任务，文档级效果有限。

Method: 提出MBR-OT，通过Wasserstein距离结合句子级效用函数计算文档效用，优化候选输出的选择。

Result: 实验表明MBR-OT在文档级机器翻译、文本简化和密集图像描述任务中优于标准MBR。

Conclusion: MBR-OT有效扩展了MBR解码的应用范围，提升了文档级文本生成的性能。

Abstract: Document-level text generation tasks are known to be more difficult than
sentence-level text generation tasks as they require the understanding of
longer context to generate high-quality texts. In this paper, we investigate
the adaption of Minimum Bayes Risk (MBR) decoding for document-level text
generation tasks. MBR decoding makes use of a utility function to estimate the
output with the highest expected utility from a set of candidate outputs.
Although MBR decoding is shown to be effective in a wide range of
sentence-level text generation tasks, its performance on document-level text
generation tasks is limited as many of the utility functions are designed for
evaluating the utility of sentences. To this end, we propose MBR-OT, a variant
of MBR decoding using Wasserstein distance to compute the utility of a document
using a sentence-level utility function. The experimental result shows that the
performance of MBR-OT outperforms that of the standard MBR in document-level
machine translation, text simplification, and dense image captioning tasks. Our
code is available at https://github.com/jinnaiyuu/mbr-optimal-transport

</details>


### [47] [Generating Diverse Training Samples for Relation Extraction with Large Language Models](https://arxiv.org/abs/2505.23108)
*Zexuan Li,Hongliang Dai,Piji Li*

Key words: 大语言模型、关系抽取、训练样本生成、多样性、直接偏好优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了如何利用大语言模型（LLMs）生成多样化的训练样本以改进关系抽取（RE）任务，提出了通过上下文学习提示和直接偏好优化（DPO）微调LLMs的方法，实验证明了其有效性。

Motivation: 直接通过LLMs生成的训练样本在关系抽取任务中可能结构相似、表达单一，因此研究如何提升生成样本的多样性同时保持正确性。

Method: 尝试通过上下文学习提示直接生成多样化样本，并提出了基于直接偏好优化的LLMs微调方法。

Result: 实验表明这两种方法均能提升训练数据的质量，且用生成样本训练的非LLM模型在RE任务上表现优于直接使用LLMs。

Conclusion: 通过多样化生成训练样本的方法能有效改进关系抽取任务，为非LLM模型提供了更好的训练基础。

Abstract: Using Large Language Models (LLMs) to generate training data can potentially
be a preferable way to improve zero or few-shot NLP tasks. However, many
problems remain to be investigated for this direction. For the task of Relation
Extraction (RE), we find that samples generated by directly prompting LLMs may
easily have high structural similarities with each other. They tend to use a
limited variety of phrasing while expressing the relation between a pair of
entities. Therefore, in this paper, we study how to effectively improve the
diversity of the training samples generated with LLMs for RE, while also
maintaining their correctness. We first try to make the LLMs produce dissimilar
samples by directly giving instructions in In-Context Learning (ICL) prompts.
Then, we propose an approach to fine-tune LLMs for diversity training sample
generation through Direct Preference Optimization (DPO). Our experiments on
commonly used RE datasets show that both attempts can improve the quality of
the generated training data. We also find that comparing with directly
performing RE with an LLM, training a non-LLM RE model with its generated
samples may lead to better performance.

</details>


### [48] [Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data](https://arxiv.org/abs/2505.23114)
*Seohyeong Lee,Eunwon Kim,Hwaran Lee,Buru Chang*

Key words: LLM对齐, 人类偏好数据, GPT-4o, 数据效率, Alignment Data Map

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种名为Alignment Data Map的工具，通过GPT-4o辅助分析和诊断偏好数据，提高LLM对齐人类价值观的效率。仅使用33%的高质量数据即可达到或超越全数据集的性能。

Motivation: 收集人类偏好数据成本高且低效，限制了LLM与人类价值观对齐的可扩展性。

Method: 使用GPT-4o作为代理计算对齐分数，并基于均值和方差构建Alignment Data Map，筛选高质量数据。

Result: 仅需33%的高均值、低方差数据即可实现与全数据集相当或更好的性能，并能有效识别低质量或错误标注样本。

Conclusion: Alignment Data Map显著提升了数据收集效率，且无需显式标注即可识别高质量样本。

Abstract: Human preference data plays a critical role in aligning large language models
(LLMs) with human values. However, collecting such data is often expensive and
inefficient, posing a significant scalability challenge. To address this, we
introduce Alignment Data Map, a GPT-4o-assisted tool for analyzing and
diagnosing preference data. Using GPT-4o as a proxy for LLM alignment, we
compute alignment scores for LLM-generated responses to instructions from
existing preference datasets. These scores are then used to construct an
Alignment Data Map based on their mean and variance. Our experiments show that
using only 33 percent of the data, specifically samples in the high-mean,
low-variance region, achieves performance comparable to or better than using
the entire dataset. This finding suggests that the Alignment Data Map can
significantly improve data collection efficiency by identifying high-quality
samples for LLM alignment without requiring explicit annotations. Moreover, the
Alignment Data Map can diagnose existing preference datasets. Our analysis
shows that it effectively detects low-impact or potentially misannotated
samples. Source code is available online.

</details>


### [49] [Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios](https://arxiv.org/abs/2505.23118)
*Linjie Mu,Zhongzhen Huang,Yakun Zhu,Xiangyu Zhao,Shaoting Zhang,Xiaofan Zhang*

Key words: 多模态推理,医学领域,两阶段训练,MedE$^2$,模型增强

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了名为MedE$^2$的两阶段后训练流程，旨在增强医学领域的多模态推理能力。通过文本数据微调和多模态医学案例增强，实验证明其显著提升了模型性能。

Motivation: 尽管多模态推理模型在数学和科学领域取得了成功，但其在医学领域的应用仍不足。因此，研究旨在填补这一空白，提升医学多模态推理能力。

Method: 使用两阶段训练：第一阶段用2000个文本样本微调模型以激发推理行为；第二阶段用1500个多模态医学案例增强推理能力，并与提出的推理偏好对齐。

Result: MedE$^2$显著提升了多模态医学模型的推理性能，在多个基准测试中超越基线模型，且在更大模型和推理时扩展下依然表现稳健。

Conclusion: MedE$^2$是一种有效的医学多模态推理增强方法，具有实际应用价值。

Abstract: Effective clinical decision-making depends on iterative, multimodal reasoning
across diverse sources of evidence. The recent emergence of multimodal
reasoning models has significantly transformed the landscape of solving complex
tasks. Although such models have achieved notable success in mathematics and
science, their application to medical domains remains underexplored. In this
work, we propose \textit{MedE$^2$}, a two-stage post-training pipeline that
elicits and then enhances multimodal reasoning for medical domains. In Stage-I,
we fine-tune models using 2,000 text-only data samples containing precisely
orchestrated reasoning demonstrations to elicit reasoning behaviors. In
Stage-II, we further enhance the model's reasoning capabilities using 1,500
rigorously curated multimodal medical cases, aligning model reasoning outputs
with our proposed multimodal medical reasoning preference. Extensive
experiments demonstrate the efficacy and reliability of \textit{MedE$^2$} in
improving the reasoning performance of medical multimodal models. Notably,
models trained with \textit{MedE$^2$} consistently outperform baselines across
multiple medical multimodal benchmarks. Additional validation on larger models
and under inference-time scaling further confirms the robustness and practical
utility of our approach.

</details>


### [50] [ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations](https://arxiv.org/abs/2505.23121)
*Yiming Lei,Zhizheng Yang,Zeming Liu,Haitao Leng,Shaoguo Liu,Tingting Gao,Qingjie Liu,Yunhong Wang*

Key words: 多模态大语言模型, 多轮对话, ContextQFormer, TMDialog

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出ContextQFormer模块和TMDialog数据集，解决多模态大语言模型在长上下文多轮对话中的弱交互问题，实验显示性能提升2%-4%。

Motivation: 现有开源多模态模型在多轮交互（尤其是长上下文）中表现较弱，需改进。

Method: 引入ContextQFormer模块增强上下文表示，并构建TMDialog数据集用于训练与评估。

Result: ContextQFormer在TMDialog上比基线模型可用率提升2%-4%。

Conclusion: ContextQFormer和TMDialog有效提升多模态多轮对话能力，支持未来研究。

Abstract: Multi-modal large language models have demonstrated remarkable zero-shot
abilities and powerful image-understanding capabilities. However, the existing
open-source multi-modal models suffer from the weak capability of multi-turn
interaction, especially for long contexts. To address the issue, we first
introduce a context modeling module, termed ContextQFormer, which utilizes a
memory block to enhance the presentation of contextual information.
Furthermore, to facilitate further research, we carefully build a new
multi-turn multi-modal dialogue dataset (TMDialog) for pre-training,
instruction-tuning, and evaluation, which will be open-sourced lately. Compared
with other multi-modal dialogue datasets, TMDialog contains longer
conversations, which supports the research of multi-turn multi-modal dialogue.
In addition, ContextQFormer is compared with three baselines on TMDialog and
experimental results illustrate that ContextQFormer achieves an improvement of
2%-4% in available rate over baselines.

</details>


### [51] [PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark inspired by Historical Linguistics](https://arxiv.org/abs/2505.23126)
*Atharva Naik,Darsh Agrawal,Manav Kapadnis,Yuwei An,Yash Mathur,Carolyn Rose,David Mortensen*

Key words: 长链条思维（LCoT）、大语言模型（LLMs）、历史语言学、归纳推理、编程示例

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了长链条思维（LCoT）大语言模型（LLMs）在历史语言学启发式归纳推理任务中的表现，发现即使最先进模型在近千个测试实例中仅达54%通过率，表明其推理能力仍有限。

Motivation: 研究动机是验证LLMs的抽象推理能力是否足以应对实际问题，尤其是历史语言学启发式归纳推理这一未被充分探索的领域。

Method: 采用了完全自动化流水线动态生成可控难度的基准测试，以解决现有推理基准的可扩展性和污染问题。

Result: 生成包含近千实例的测试集，挑战了所有前沿LLMs，最佳模型（Claude-3.7-Sonnet）仅达54%通过率。

Conclusion: 结论指出，尽管LLMs在数学、编程等领域表现优异，但在历史语言学等领域的推理仍存在局限。

Abstract: Recently, long chain of thought (LCoT), Large Language Models (LLMs), have
taken the machine learning world by storm with their breathtaking reasoning
capabilities. However, are the abstract reasoning abilities of these models
general enough for problems of practical importance? Unlike past work, which
has focused mainly on math, coding, and data wrangling, we focus on a
historical linguistics-inspired inductive reasoning problem, formulated as
Programming by Examples. We develop a fully automated pipeline for dynamically
generating a benchmark for this task with controllable difficulty in order to
tackle scalability and contamination issues to which many reasoning benchmarks
are subject. Using our pipeline, we generate a test set with nearly 1k
instances that is challenging for all state-of-the-art reasoning LLMs, with the
best model (Claude-3.7-Sonnet) achieving a mere 54% pass rate, demonstrating
that LCoT LLMs still struggle with a class or reasoning that is ubiquitous in
historical linguistics as well as many other domains.

</details>


### [52] [Enhancing Large Language Models'Machine Translation via Dynamic Focus Anchoring](https://arxiv.org/abs/2505.23140)
*Qiuyu Ding,Zhiqiang Cao,Hailong Cao,Tiejun Zhao*

Key words: 大语言模型, 机器翻译, 上下文敏感单元, 语义聚焦, 跨语言任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种增强大语言模型（LLMs）跨语言翻译能力的方法，通过动态识别上下文敏感单元（CSUs）并应用语义聚焦，显著提升翻译准确性和鲁棒性。

Motivation: 针对大语言模型在翻译上下文敏感单元（如多义词）时的挑战，这些挑战不仅影响局部翻译准确性，还可能导致整个句子或任务理解的失败。

Method: 动态分析和识别翻译难点，以结构化方式将CSUs融入LLMs，避免信息扁平化导致的误译或误解，从而高效激活模型相关知识。

Result: 在机器翻译基准数据集上，该方法表现优于多个开源基线模型，对相似和远距离语言对均有效且无需额外训练。

Conclusion: 该方法简单有效，资源消耗低，能显著提升LLMs在多语言任务中的性能。

Abstract: Large language models have demonstrated exceptional performance across
multiple crosslingual NLP tasks, including machine translation (MT). However,
persistent challenges remain in addressing context-sensitive units (CSUs), such
as polysemous words. These CSUs not only affect the local translation accuracy
of LLMs, but also affect LLMs' understanding capability for sentences and
tasks, and even lead to translation failure. To address this problem, we
propose a simple but effective method to enhance LLMs' MT capabilities by
acquiring CSUs and applying semantic focus. Specifically, we dynamically
analyze and identify translation challenges, then incorporate them into LLMs in
a structured manner to mitigate mistranslations or misunderstandings of CSUs
caused by information flattening. Efficiently activate LLMs to identify and
apply relevant knowledge from its vast data pool in this way, ensuring more
accurate translations for translating difficult terms. On a benchmark dataset
of MT, our proposed method achieved competitive performance compared to
multiple existing open-sourced MT baseline models. It demonstrates
effectiveness and robustness across multiple language pairs, including both
similar language pairs and distant language pairs. Notably, the proposed method
requires no additional model training and enhances LLMs' performance across
multiple NLP tasks with minimal resource consumption.

</details>


### [53] [Cross-Domain Bilingual Lexicon Induction via Pretrained Language Models](https://arxiv.org/abs/2505.23146)
*Qiuyu Ding,Zhiqiang Cao,Hailong Cao,Tiejun Zhao*

Key words: 双语词典归纳（BLI）, 预训练模型, Code Switch, 专业领域, 词嵌入

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种新的双语词典归纳（BLI）任务，利用通用领域和目标领域的单语语料库生成领域特定的双语词典。通过结合预训练模型的能力，改进了传统BLI方法在专业领域的表现。实验结果显示，该方法在三个特定领域的BLI任务中平均提升了0.78分。

Motivation: 由于专业领域数据集规模较小且专业词汇频率较低，传统BLI方法在专业领域的表现较差。此外，静态词嵌入无法充分捕捉上下文对词义的影响，导致翻译质量下降。因此，作者提出了一种更适应专业领域需求的新方法。

Method: 结合预训练模型的能力，提出了一种改进的词嵌入生成方法，并在跨领域BLI任务中首次引入了Code Switch（Qin et al., 2020）技术，以匹配不同上下文中的词义。

Result: 实验结果显示，该方法在三个特定领域的BLI任务中平均提升了0.78分的性能表现。

Conclusion: 该方法显著提升了BLI在专业领域的性能，尤其是在处理小规模和低频专业词汇时表现更优。

Abstract: Bilingual Lexicon Induction (BLI) is generally based on common domain data to
obtain monolingual word embedding, and by aligning the monolingual word
embeddings to obtain the cross-lingual embeddings which are used to get the
word translation pairs. In this paper, we propose a new task of BLI, which is
to use the monolingual corpus of the general domain and target domain to
extract domain-specific bilingual dictionaries. Motivated by the ability of
Pre-trained models, we propose a method to get better word embeddings that
build on the recent work on BLI. This way, we introduce the Code Switch(Qin et
al., 2020) firstly in the cross-domain BLI task, which can match differit is
yet to be seen whether these methods are suitable for bilingual lexicon
extraction in professional fields. As we can see in table 1, the classic and
efficient BLI approach, Muse and Vecmap, perform much worse on the Medical
dataset than on the Wiki dataset. On one hand, the specialized domain data set
is relatively smaller compared to the generic domain data set generally, and
specialized words have a lower frequency, which will directly affect the
translation quality of bilingual dictionaries. On the other hand, static word
embeddings are widely used for BLI, however, in some specific fields, the
meaning of words is greatly influenced by context, in this case, using only
static word embeddings may lead to greater bias. ent strategies in different
contexts, making the model more suitable for this task. Experimental results
show that our method can improve performances over robust BLI baselines on
three specific domains by averagely improving 0.78 points.

</details>


### [54] [Tell, Don't Show: Leveraging Language Models' Abstractive Retellings to Model Literary Themes](https://arxiv.org/abs/2505.23166)
*Li Lucy,Camilla Griffiths,Sarah Levine,Jennifer L. Eberhardt,Dorottya Demszky,David Bamman*

Key words: 主题建模, 文学文本, 生成式语言模型, LDA, 文化分析

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 传统主题建模方法如LDA在处理文学文本时表现不佳，本文提出Retell方法，通过生成式语言模型（LM）将叙事内容转化为高层概念，再结合LDA生成更精确的主题。

Motivation: 解决传统基于词袋方法的主题建模（如LDA）在分析文学文本时的局限性，尤其是叙事语言偏向感官细节而非抽象描述的问题。

Method: 利用生成式语言模型（LM）对文本进行重述（retelling），将叙事转化为高层概念，随后对重述内容应用LDA进行主题建模。

Result: 相比单独使用LDA或直接要求LM列出主题，Retell方法生成的主题更精确、信息量更大，并在关于种族/文化身份的高中英语教材案例中表现出与专家标注的一致性。

Conclusion: Retell方法为文学文本的主题建模提供了更有效的工具，适用于文化分析等领域。

Abstract: Conventional bag-of-words approaches for topic modeling, like latent
Dirichlet allocation (LDA), struggle with literary text. Literature challenges
lexical methods because narrative language focuses on immersive sensory details
instead of abstractive description or exposition: writers are advised to "show,
don't tell." We propose Retell, a simple, accessible topic modeling approach
for literature. Here, we prompt resource-efficient, generative language models
(LMs) to tell what passages show, thereby translating narratives' surface forms
into higher-level concepts and themes. By running LDA on LMs' retellings of
passages, we can obtain more precise and informative topics than by running LDA
alone or by directly asking LMs to list topics. To investigate the potential of
our method for cultural analytics, we compare our method's outputs to
expert-guided annotations in a case study on racial/cultural identity in high
school English language arts books.

</details>


### [55] [ZIPA: A family of efficient models for multilingual phone recognition](https://arxiv.org/abs/2505.23170)
*Jian Zhu,Farhan Samir,Eleanor Chodroff,David R. Mortensen*

Key words: ZIPA, 多语言音素识别, IPAPack++, Zipformer, 噪声学生训练

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ZIPA 是一系列高效的语音模型，在多语言音素识别任务中取得了最先进的性能。通过构建大语料库 IPAPack++ 和改进模型架构（Zipformer 结合 CTC 或 Transducer），ZIPA 在参数更少的情况下优于现有系统。进一步的伪标注数据训练带来额外提升，但社会语音多样性的建模仍是挑战。

Motivation: 推动多语言音素识别任务的性能边界，同时减少模型参数以提升效率，并探索社会语音多样性建模的难点。

Method: 1. 构建 IPAPack++ 语料库（17,132 小时标注数据）；2. 设计基于 Zipformer 的 ZIPA-T（Transducer）和 ZIPA-CR（CTC）模型；3. 利用 11,000 小时伪标注数据通过噪声学生训练扩展模型规模。

Result: ZIPA 在参数更少的情况下超越现有音素识别系统，且伪标注数据训练进一步提升了性能。错误分析显示对社会语音多样性的建模仍不足。

Conclusion: ZIPA 在性能和效率上设定了新基准，但社会语音变体的识别仍需未来研究突破。

Abstract: We present ZIPA, a family of efficient speech models that advances the
state-of-the-art performance of crosslinguistic phone recognition. We first
curated IPAPack++, a large-scale multilingual speech corpus with 17,132 hours
of normalized phone transcriptions and a novel evaluation set capturing unseen
languages and sociophonetic variation. With the large-scale training data,
ZIPA, including transducer (ZIPA-T) and CTC-based (ZIPA-CR) variants, leverage
the efficient Zipformer backbones and outperform existing phone recognition
systems with much fewer parameters. Further scaling via noisy student training
on 11,000 hours of pseudo-labeled multilingual data yields further improvement.
While ZIPA achieves strong performance on benchmarks, error analysis reveals
persistent limitations in modeling sociophonetic diversity, underscoring
challenges for future research.

</details>


### [56] [Map&Make: Schema Guided Text to Table Generation](https://arxiv.org/abs/2505.23174)
*Naman Ahuja,Fenil Bardoliya,Chitta Baral,Vivek Gupta*

Key words: Text-to-Table, 信息抽取, 结构化摘要, Map&Make, 模式提取

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 文章介绍了一种名为Map&Make的新方法，用于将复杂文本转换为可解释的表格（Text-to-Table生成），通过分解文本为原子命题并提取潜在模式，显著提升了两大数据集的生成效果和可解释性。

Motivation: 当前Text-to-Table生成方法在提取复杂信息和推断数据方面能力不足，未能有效捕捉文本中的定性与定量信息。

Method: 提出Map&Make方法，将文本分解为命题原子语句，提取潜在模式并填充表格，同时通过纠正幻觉误差提升可靠性。

Result: 在Rotowire和Livesum数据集上表现显著优于现有方法，生成表格的可解释性和准确性均有提升。

Conclusion: Map&Make框架通过细粒度分析和模式提取，为结构化摘要任务提供了实用且高效的解决方案。

Abstract: Transforming dense, detailed, unstructured text into an interpretable and
summarised table, also colloquially known as Text-to-Table generation, is an
essential task for information retrieval. Current methods, however, miss out on
how and what complex information to extract; they also lack the ability to
infer data from the text. In this paper, we introduce a versatile approach,
Map&Make, which "dissects" text into propositional atomic statements. This
facilitates granular decomposition to extract the latent schema. The schema is
then used to populate the tables that capture the qualitative nuances and the
quantitative facts in the original text. Our approach is tested against two
challenging datasets, Rotowire, renowned for its complex and multi-table
schema, and Livesum, which demands numerical aggregation. By carefully
identifying and correcting hallucination errors in Rotowire, we aim to achieve
a cleaner and more reliable benchmark. We evaluate our method rigorously on a
comprehensive suite of comparative and referenceless metrics. Our findings
demonstrate significant improvement results across both datasets with better
interpretability in Text-to-Table generation. Moreover, through detailed
ablation studies and analyses, we investigate the factors contributing to
superior performance and validate the practicality of our framework in
structured summarization tasks.

</details>


### [57] [Infinite-Instruct: Synthesizing Scaling Code instruction Data with Bidirectional Synthesis and Static Verification](https://arxiv.org/abs/2505.23177)
*Wenjing Xing,Wenke Lu,Yeheng Duan,Bing Zhao,Zhenghui kang,Yaolong Wang,Kai Gao,Lei Qiao*

Key words: 代码生成、LLM、数据合成、逆向构建、静态分析

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Infinite-Instruct，一种自动生成高质量代码问答对的框架，提升了LLM生成代码的逻辑性和多样性，实验显示性能显著提升。

Motivation: 解决传统方法在代码指令数据合成中多样性不足和逻辑性差的问题。

Method: 结合逆向构建和反馈构建，形成知识图谱，并通过跨语言静态分析确保数据质量。

Result: 7B和32B参数模型分别提升21.70%和36.95%性能，用更少数据达到竞争水平。

Conclusion: 为编程领域的LLM训练提供了可扩展方案，开源了实验数据。

Abstract: Traditional code instruction data synthesis methods suffer from limited
diversity and poor logic. We introduce Infinite-Instruct, an automated
framework for synthesizing high-quality question-answer pairs, designed to
enhance the code generation capabilities of large language models (LLMs). The
framework focuses on improving the internal logic of synthesized problems and
the quality of synthesized code. First, "Reverse Construction" transforms code
snippets into diverse programming problems. Then, through "Backfeeding
Construction," keywords in programming problems are structured into a knowledge
graph to reconstruct them into programming problems with stronger internal
logic. Finally, a cross-lingual static code analysis pipeline filters invalid
samples to ensure data quality. Experiments show that on mainstream code
generation benchmarks, our fine-tuned models achieve an average performance
improvement of 21.70% on 7B-parameter models and 36.95% on 32B-parameter
models. Using less than one-tenth of the instruction fine-tuning data, we
achieved performance comparable to the Qwen-2.5-Coder-Instruct.
Infinite-Instruct provides a scalable solution for LLM training in programming.
We open-source the datasets used in the experiments, including both unfiltered
versions and filtered versions via static analysis. The data are available at
https://github.com/xingwenjing417/Infinite-Instruct-dataset

</details>


### [58] [Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement](https://arxiv.org/abs/2505.23183)
*Gabriele Sarti,Vilém Zouhar,Malvina Nissim,Arianna Bisazza*

Key words: 词级质量估计, 机器翻译, 无监督指标, 语言模型, 不确定性量化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本研究探讨了利用语言模型可解释性和不确定性量化来高效识别机器翻译错误的方法，避免了传统方法的高成本问题。在12种翻译方向上评估14种指标，揭示了无监督指标的潜力和监督方法在标签不确定性下的局限性。

Motivation: 传统的词级质量估计（WQE）方法成本高昂，本研究旨在通过语言模型的可解释性和不确定性量化，提出更高效的替代方案。

Method: 利用语言模型的内部工作机制和不确定性量化技术，结合14种指标在12种翻译方向上进行评估。

Result: 研究发现无监督指标具有未开发的潜力，而监督方法在标签不确定性下表现不佳。

Conclusion: 研究强调了无监督方法的潜力以及单标注评估实践的脆弱性。

Abstract: Word-level quality estimation (WQE) aims to automatically identify
fine-grained error spans in machine-translated outputs and has found many uses,
including assisting translators during post-editing. Modern WQE techniques are
often expensive, involving prompting of large language models or ad-hoc
training on large amounts of human-labeled data. In this work, we investigate
efficient alternatives exploiting recent advances in language model
interpretability and uncertainty quantification to identify translation errors
from the inner workings of translation models. In our evaluation spanning 14
metrics across 12 translation directions, we quantify the impact of human label
variation on metric performance by using multiple sets of human labels. Our
results highlight the untapped potential of unsupervised metrics, the
shortcomings of supervised methods when faced with label uncertainty, and the
brittleness of single-annotator evaluation practices.

</details>


### [59] [Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration](https://arxiv.org/abs/2505.23187)
*Yilong Li,Chen Qian,Yu Xia,Ruijie Shi,Yufan Dang,Zihao Xie,Ziming You,Weize Chen,Cheng Yang,Weichuan Liu,Ye Tian,Xuantang Xiong,Lei Han,Zhiyuan Liu,Maosong Sun*

Key words: Large Language Model, multi-agent systems, experiential learning, cross-task learning, collaboration network

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: MAEL框架通过跨任务经验学习提升LLM多智能体系统的协作效率，减少冗余计算并增强相似任务的泛化能力。

Motivation: 现有方法通常孤立处理任务，导致计算冗余和泛化能力有限，需要一种新的框架来支持跨任务学习和经验积累。

Method: 基于图结构的多智能体协作网络设计，通过量化任务解决流程中各步骤质量并存储为经验，推理时检索高奖励经验作为示例。

Result: 实验证明MAEL能有效学习先前任务经验，实现更快收敛和更高质量的解决方案。

Conclusion: MAEL通过经验积累和检索提升了多智能体协作的准确性和效率，尤其在相似任务中表现显著。

Abstract: Large Language Model-based multi-agent systems (MAS) have shown remarkable
progress in solving complex tasks through collaborative reasoning and
inter-agent critique. However, existing approaches typically treat each task in
isolation, resulting in redundant computations and limited generalization
across structurally similar tasks. To address this, we introduce multi-agent
cross-task experiential learning (MAEL), a novel framework that endows
LLM-driven agents with explicit cross-task learning and experience
accumulation. We model the task-solving workflow on a graph-structured
multi-agent collaboration network, where agents propagate information and
coordinate via explicit connectivity. During the experiential learning phase,
we quantify the quality for each step in the task-solving workflow and store
the resulting rewards along with the corresponding inputs and outputs into each
agent's individual experience pool. During inference, agents retrieve
high-reward, task-relevant experiences as few-shot examples to enhance the
effectiveness of each reasoning step, thereby enabling more accurate and
efficient multi-agent collaboration. Experimental results on diverse datasets
demonstrate that MAEL empowers agents to learn from prior task experiences
effectively-achieving faster convergence and producing higher-quality solutions
on current tasks.

</details>


### [60] [ExpeTrans: LLMs Are Experiential Transfer Learners](https://arxiv.org/abs/2505.23191)
*Jinglong Gao,Xiao Ding,Lingxiao Zou,Bibo Cai,Bing Qin,Ting Liu*

Key words: 大语言模型（LLMs）、经验迁移、认知智能、任务泛化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究提出了一种自主经验转移框架，通过让大语言模型（LLMs）模仿人类认知智能，将现有任务的经验迁移到新任务中，以减少人工和时间成本。实验表明，该方法能显著提升LLMs的性能。

Motivation: 传统的文本任务解决经验收集方法依赖大量人工或时间，无法应对LLMs中日益多样化的任务类型。本文旨在探索LLMs是否能够自主迁移经验。

Method: 设计了一个自主经验转移框架，允许LLMs将已有任务的经验应用到新任务上，具体实现包括经验提取和迁移模块。

Result: 在13个数据集上的实验验证了该框架的有效性，成功提升了LLMs的任务解决能力。

Conclusion: 自主经验转移框架为LLMs的泛化提供了新路径，同时显著降低了获取经验的成本。

Abstract: Recent studies provide large language models (LLMs) with textual task-solving
experiences via prompts to improve their performance. However, previous methods
rely on substantial human labor or time to gather such experiences for each
task, which is impractical given the growing variety of task types in user
queries to LLMs. To address this issue, we design an autonomous experience
transfer framework to explore whether LLMs can mimic human cognitive
intelligence to autonomously transfer experience from existing source tasks to
newly encountered target tasks. This not only allows the acquisition of
experience without extensive costs of previous methods, but also offers a novel
path for the generalization of LLMs. Experimental results on 13 datasets
demonstrate that our framework effectively improves the performance of LLMs.
Furthermore, we provide a detailed analysis of each module in the framework.

</details>


### [61] [MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration](https://arxiv.org/abs/2505.23224)
*Zhitao He,Sandeep Polisetty,Zhiyuan Fan,Yuchen Huang,Shujin Wu,Yi R.,Fung*

Key words: 多模态大语言模型，置信度校准，推理步骤，自奖励信号，强化学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一个名为MMBoundary的新框架，通过校准多模态大语言模型（MLLMs）在推理步骤中的置信度，减少幻觉传播并提升推理性能。

Motivation: 现有的置信度评估方法通常关注整体响应，而忽略了推理步骤中的置信度，导致幻觉传播问题。

Method: 结合文本和跨模态自奖励信号估计推理步骤的置信度，并通过监督微调和强化学习阶段进一步校准置信度。

Result: MMBoundary在多样数据集和指标上显著优于现有方法，平均减少7.5%的校准误差，任务性能提升8.3%。

Conclusion: MMBoundary有效提升了MLLMs的推理能力，解决了置信度校准问题。

Abstract: In recent years, multimodal large language models (MLLMs) have made
significant progress but continue to face inherent challenges in multimodal
reasoning, which requires multi-level (e.g., perception, reasoning) and
multi-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior
work on estimating model confidence tends to focus on the overall response for
training and calibration, but fails to assess confidence in each reasoning
step, leading to undesirable hallucination snowballing. In this work, we
present MMBoundary, a novel framework that advances the knowledge boundary
awareness of MLLMs through reasoning step confidence calibration. To achieve
this, we propose to incorporate complementary textual and cross-modal
self-rewarding signals to estimate confidence at each step of the MLLM
reasoning process. In addition to supervised fine-tuning MLLM on this set of
self-rewarded confidence estimation signal for initial confidence expression
warm-up, we introduce a reinforcement learning stage with multiple reward
functions for further aligning model knowledge and calibrating confidence at
each reasoning step, enhancing reasoning chain self-correction. Empirical
results show that MMBoundary significantly outperforms existing methods across
diverse domain datasets and metrics, achieving an average of 7.5% reduction in
multimodal confidence calibration errors and up to 8.3% improvement in task
performance.

</details>


### [62] [MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration](https://arxiv.org/abs/2505.23229)
*Hao Lu,Yanchi Gu,Haoyuan Huang,Yulin Zhou,Ningxin Zhu,Chen Li*

Key words: Monte Carlo Tree Search, Large Language Models, psychological counseling, domain alignment, PsyLLM, PsyEval

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: MCTSr-Zero combines Monte Carlo Tree Search (MCTS) with Large Language Models (LLMs) to improve open-ended dialogue performance, especially in psychological counseling, addressing challenges like empathy and ethical alignment.

Motivation: Existing MCTS methods in dialogues fail to handle subjective success factors (e.g., empathy, ethics) crucial in psychological counseling, requiring a new approach.

Method: Proposed MCTSr-Zero framework introduces 'domain alignment' for subjective targets, along with 'Regeneration' and 'Meta-Prompt Adaptation' to explore diverse strategies.

Result: Fine-tuned LLM (PsyLLM) achieves SOTA on PsyEval benchmark, proving MCTSr-Zero generates high-quality, principle-aligned conversational data.

Conclusion: MCTSr-Zero effectively addresses LLMs' limitations in adhering to complex psychological standards, validating its use in human-centric domains.

Abstract: The integration of Monte Carlo Tree Search (MCTS) with Large Language Models
(LLMs) has demonstrated significant success in structured, problem-oriented
tasks. However, applying these methods to open-ended dialogues, such as those
in psychological counseling, presents unique challenges. Unlike tasks with
objective correctness, success in therapeutic conversations depends on
subjective factors like empathetic engagement, ethical adherence, and alignment
with human preferences, for which strict "correctness" criteria are
ill-defined. Existing result-oriented MCTS approaches can therefore produce
misaligned responses. To address this, we introduce MCTSr-Zero, an MCTS
framework designed for open-ended, human-centric dialogues. Its core innovation
is "domain alignment", which shifts the MCTS search objective from predefined
end-states towards conversational trajectories that conform to target domain
principles (e.g., empathy in counseling). Furthermore, MCTSr-Zero incorporates
"Regeneration" and "Meta-Prompt Adaptation" mechanisms to substantially broaden
exploration by allowing the MCTS to consider fundamentally different initial
dialogue strategies. We evaluate MCTSr-Zero in psychological counseling by
generating multi-turn dialogue data, which is used to fine-tune an LLM, PsyLLM.
We also introduce PsyEval, a benchmark for assessing multi-turn psychological
counseling dialogues. Experiments demonstrate that PsyLLM achieves
state-of-the-art performance on PsyEval and other relevant metrics, validating
MCTSr-Zero's effectiveness in generating high-quality, principle-aligned
conversational data for human-centric domains and addressing the LLM challenge
of consistently adhering to complex psychological standards.

</details>


### [63] [ChartMind: A Comprehensive Benchmark for Complex Real-world Multimodal Chart Question Answering](https://arxiv.org/abs/2505.23242)
*Jingxuan Wei,Nan Xu,Junnan Zhu,Yanni Hao,Gaowei Wu,Bihui Yu,Lei Wang*

Key words: 图表问答, 多模态模型, 基准测试, 上下文感知, 推理准确性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ChartMind是一个专为复杂图表问答任务设计的新基准，支持多语言环境和多样化图表格式。ChartLLM框架通过提取关键上下文元素和降噪，显著提升了多模态模型的推理准确性。

Motivation: 现有的图表问答评估方法过于依赖固定输出格式和客观指标，忽略了实际应用中的复杂需求。因此，旨在弥合理论与实际应用之间的差距。

Method: 提出了ChartMind基准和ChartLLM框架，专注于提取关键上下文元素、减少噪音并提升推理准确性。

Result: 在ChartMind和三个主流基准上的评估显示，ChartLLM显著优于常见的三种图表问答范式。

Conclusion: 灵活的图表理解对实际应用至关重要，为未来的图表推理研究指明了新方向。

Abstract: Chart question answering (CQA) has become a critical multimodal task for
evaluating the reasoning capabilities of vision-language models. While early
approaches have shown promising performance by focusing on visual features or
leveraging large-scale pre-training, most existing evaluations rely on rigid
output formats and objective metrics, thus ignoring the complex, real-world
demands of practical chart analysis. In this paper, we introduce ChartMind, a
new benchmark designed for complex CQA tasks in real-world settings. ChartMind
covers seven task categories, incorporates multilingual contexts, supports
open-domain textual outputs, and accommodates diverse chart formats, bridging
the gap between real-world applications and traditional academic benchmarks.
Furthermore, we propose a context-aware yet model-agnostic framework, ChartLLM,
that focuses on extracting key contextual elements, reducing noise, and
enhancing the reasoning accuracy of multimodal large language models. Extensive
evaluations on ChartMind and three representative public benchmarks with 14
mainstream multimodal models show our framework significantly outperforms the
previous three common CQA paradigms: instruction-following, OCR-enhanced, and
chain-of-thought, highlighting the importance of flexible chart understanding
for real-world CQA. These findings suggest new directions for developing more
robust chart reasoning in future research.

</details>


### [64] [Automatic Construction of Multiple Classification Dimensions for Managing Approaches in Scientific Papers](https://arxiv.org/abs/2505.23252)
*Bing Ma,Hai Zhuge*

Key words: 方法模式,多维度管理,树结构,相似性度量,聚类算法

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一个多维度方法管理框架，通过从语义、话语、句法和词汇层面识别方法模式，并利用树结构表示和相似性度量，实现了科学论文中方法的高效查询与管理。

Motivation: 由于科学论文数量庞大，查询和利用其中的方法耗时且困难，缺乏系统化管理框架。该研究旨在通过多维度分类和相似性度量，提升方法查询的效率和相关性。

Method: 通过自上而下方式从四个语言层面识别方法模式，利用树结构表示方法的步骤并计算相似性。提出基于集合相似性度量的聚类算法，构建多维度方法空间。

Result: 在多维度方法空间中进行查询，结果显示查询结果与用户需求高度相关，并通过基于类别的查询机制快速缩小搜索范围。

Conclusion: 该研究提出的多维度方法管理框架有效提升了科学方法的查询效率和管理能力。

Abstract: Approaches form the foundation for conducting scientific research. Querying
approaches from a vast body of scientific papers is extremely time-consuming,
and without a well-organized management framework, researchers may face
significant challenges in querying and utilizing relevant approaches.
Constructing multiple dimensions on approaches and managing them from these
dimensions can provide an efficient solution. Firstly, this paper identifies
approach patterns using a top-down way, refining the patterns through four
distinct linguistic levels: semantic level, discourse level, syntactic level,
and lexical level. Approaches in scientific papers are extracted based on
approach patterns. Additionally, five dimensions for categorizing approaches
are identified using these patterns. This paper proposes using tree structure
to represent step and measuring the similarity between different steps with a
tree-structure-based similarity measure that focuses on syntactic-level
similarities. A collection similarity measure is proposed to compute the
similarity between approaches. A bottom-up clustering algorithm is proposed to
construct class trees for approach components within each dimension by merging
each approach component or class with its most similar approach component or
class in each iteration. The class labels generated during the clustering
process indicate the common semantics of the step components within the
approach components in each class and are used to manage the approaches within
the class. The class trees of the five dimensions collectively form a
multi-dimensional approach space. The application of approach queries on the
multi-dimensional approach space demonstrates that querying within this space
ensures strong relevance between user queries and results and rapidly reduces
search space through a class-based query mechanism.

</details>


### [65] [The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text](https://arxiv.org/abs/2505.23276)
*Maged S. Al-Shaibani,Moataz Ahmed*

Key words: 大型语言模型, 阿拉伯语, 机器生成文本, 检测模型, 信息完整性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLMs）在阿拉伯语中生成的文本，通过多种生成策略和模型架构的全面分析，揭示了机器生成文本的可检测特征，并开发了高效的BERT检测模型。

Motivation: 由于LLMs在教育和社交媒体等关键领域的信息完整性挑战，尤其是在低资源语言（如阿拉伯语）中的影响尚未深入研究，论文旨在填补这一空白并开发有效的检测方法。

Method: 采用多种生成策略（仅标题生成、内容感知生成和文本精炼）和模型架构（ALLaM、Jais、Llama和GPT-4）生成阿拉伯语文本，并进行语言学和风格分析。基于分析结果开发BERT检测模型。

Result: 研究发现机器生成的阿拉伯语文本具有可检测的语言特征，BERT模型在正式语境中表现优异（最高99.9% F1分数）。但在跨领域泛化方面仍面临挑战。

Conclusion: 论文为阿拉伯语机器生成文本的检测奠定了理论基础，并为开发语言感知的检测系统提供了实用方法。

Abstract: Large Language Models (LLMs) have achieved unprecedented capabilities in
generating human-like text, posing subtle yet significant challenges for
information integrity across critical domains, including education, social
media, and academia, enabling sophisticated misinformation campaigns,
compromising healthcare guidance, and facilitating targeted propaganda. This
challenge becomes severe, particularly in under-explored and low-resource
languages like Arabic. This paper presents a comprehensive investigation of
Arabic machine-generated text, examining multiple generation strategies
(generation from the title only, content-aware generation, and text refinement)
across diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic,
and social media domains. Our stylometric analysis reveals distinctive
linguistic patterns differentiating human-written from machine-generated Arabic
text across these varied contexts. Despite their human-like qualities, we
demonstrate that LLMs produce detectable signatures in their Arabic outputs,
with domain-specific characteristics that vary significantly between different
contexts. Based on these insights, we developed BERT-based detection models
that achieved exceptional performance in formal contexts (up to 99.9\%
F1-score) with strong precision across model architectures. Our cross-domain
analysis confirms generalization challenges previously reported in the
literature. To the best of our knowledge, this work represents the most
comprehensive investigation of Arabic machine-generated text to date, uniquely
combining multiple prompt generation methods, diverse model architectures, and
in-depth stylometric analysis across varied textual domains, establishing a
foundation for developing robust, linguistically-informed detection systems
essential for preserving information integrity in Arabic-language contexts.

</details>


### [66] [Sentinel: Attention Probing of Proxy Models for LLM Context Compression with an Understanding Perspective](https://arxiv.org/abs/2505.23277)
*Yong Zhang,Yanwen Huang,Ning Cheng,Yang Guo,Yun Zhu,Yanmeng Wang,Shaojun Wang,Jing Xiao*

Key words: 检索增强生成 (RAG), 句子级压缩, 注意力机制, 轻量级框架

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Sentinel 是一种轻量级的句子级压缩框架，通过利用现成的 LLM 的注意力机制来高效过滤上下文，无需额外训练压缩模型，在 LongBench 基准上实现了 5 倍的压缩，同时保持了 QA 性能。

Motivation: 现有检索增强生成 (RAG) 方法中，检索到的上下文往往冗长、嘈杂或超出输入限制，而传统压缩方法需要训练专用模型，成本高且可移植性差。

Method: Sentinel 利用一个现成的 0.5B LLM 的解码器注意力信号，通过轻量级分类器识别句子相关性，避免了额外训练压缩模型。

Result: 在 LongBench 基准测试中，Sentinel 实现了高达 5 倍的压缩效果，且 QA 性能与 7B 规模的压缩系统相当。

Conclusion: 通过探测 LLM 的原生注意力信号可以实现高效、快速且问题感知的上下文压缩，无需训练专用模型。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with external context, but retrieved passages are often lengthy, noisy, or
exceed input limits. Existing compression methods typically require supervised
training of dedicated compression models, increasing cost and reducing
portability. We propose Sentinel, a lightweight sentence-level compression
framework that reframes context filtering as an attention-based understanding
task. Rather than training a compression model, Sentinel probes decoder
attention from an off-the-shelf 0.5B proxy LLM using a lightweight classifier
to identify sentence relevance. Empirically, we find that query-context
relevance estimation is consistent across model scales, with 0.5B proxies
closely matching the behaviors of larger models. On the LongBench benchmark,
Sentinel achieves up to 5$\times$ compression while matching the QA performance
of 7B-scale compression systems. Our results suggest that probing native
attention signals enables fast, effective, and question-aware context
compression. Code available at: https://github.com/yzhangchuck/Sentinel.

</details>


### [67] [ScEdit: Script-based Assessment of Knowledge Editing](https://arxiv.org/abs/2505.23291)
*Xinye Li,Zunwen Zheng,Qian Zhang,Dekai Zhuang,Jiabao Kang,Liyan Xu,Qingbin Liu,Xi Chen,Zhiying Tu,Dianhui Chu,Dianbo Sui*

Key words: 知识编辑(KE), 基准测试, 反事实编辑, 时间性编辑, LLM-as-agent

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文引入了一个基于脚本的新基准ScEdit，用于评估知识编辑(KE)方法在反事实和时间性编辑中的表现，发现现有方法在文本级指标上面临挑战。

Motivation: 当前的知识编辑任务过于简单，且缺乏与实际应用场景（如LLM-as-agent）的结合，因此需要一个更全面的评估框架。

Method: 提出了ScEdit基准，结合了反事实和时间性编辑，采用标记级和文本级评估方法，扩展了传统的事实型评估到行动型评估。

Result: 所有KE方法在现有指标上表现下降，尤其在文本级指标上面临显著挑战。

Conclusion: ScEdit基准揭示了知识编辑任务的复杂性，为未来研究提供了更全面的评估标准。

Abstract: Knowledge Editing (KE) has gained increasing attention, yet current KE tasks
remain relatively simple. Under current evaluation frameworks, many editing
methods achieve exceptionally high scores, sometimes nearing perfection.
However, few studies integrate KE into real-world application scenarios (e.g.,
recent interest in LLM-as-agent). To support our analysis, we introduce a novel
script-based benchmark -- ScEdit (Script-based Knowledge Editing Benchmark) --
which encompasses both counterfactual and temporal edits. We integrate
token-level and text-level evaluation methods, comprehensively analyzing
existing KE techniques. The benchmark extends traditional fact-based
("What"-type question) evaluation to action-based ("How"-type question)
evaluation. We observe that all KE methods exhibit a drop in performance on
established metrics and face challenges on text-level metrics, indicating a
challenging task. Our benchmark is available at
https://github.com/asdfo123/ScEdit.

</details>


### [68] [How Does Response Length Affect Long-Form Factuality](https://arxiv.org/abs/2505.23295)
*James Xu Zhao,Jimmy Z. J. Liu,Bryan Hooi,See-Kiong Ng*

Key words: 大型语言模型（LLMs）、事实性、长文本生成、评估框架、知识耗尽

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLMs）生成长文本时更容易出现事实性错误，且响应长度越长，事实性精度越低，主要原因是知识耗尽。

Motivation: 现有研究关注LLM的事实性，但响应长度对事实性的影响尚未充分探讨，因此需系统性研究。

Method: 提出一种自动化的双向长文本事实性评估框架，并通过实验验证响应长度与事实性精度的关系。

Result: 实验表明，响应长度越长，LLM的事实性精度越低，且知识耗尽是主要原因。

Conclusion: 长文本生成中的事实性下降主要由知识耗尽导致，而非错误传播或长上下文。

Abstract: Large language models (LLMs) are widely used for long-form text generation.
However, factual errors in the responses would undermine their reliability.
Despite growing attention to LLM factuality, the effect of response length on
factuality remains underexplored. In this work, we systematically investigate
this relationship by first introducing an automatic and bi-level long-form
factuality evaluation framework, which achieves high agreement with human
annotations while being cost-effective. Using this framework, we conduct
controlled experiments and find that longer responses exhibit lower factual
precision, confirming the presence of length bias. To explain this phenomenon,
we empirically examine three hypotheses: error propagation, long context, and
facts exhaustion. Our results reveal that facts exhaustion, where the model
gradually exhausts more reliable knowledge, is the primary cause of factual
degradation, rather than the other two hypotheses.

</details>


### [69] [EmoBench-UA: A Benchmark Dataset for Emotion Detection in Ukrainian](https://arxiv.org/abs/2505.23297)
*Daryna Dementieva,Nikolay Babakov,Alexander Fraser*

Key words: 乌克兰语,情感检测,标注数据集,众包,大型语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文介绍了EmoBench-UA，首个针对乌克兰语文本情感检测的标注数据集，并评估了多种方法，强调了乌克兰语情感检测的挑战和需求。

Motivation: 乌克兰语在NLP领域的情感分类研究不足，缺乏公开可用的基准数据集，因此作者创建了EmoBench-UA以填补这一空白。

Method: 数据通过Toloka.ai平台众包标注，标注标准基于英语情感检测研究，同时评估了语言学基线、合成数据和大型语言模型（LLMs）等方法。

Result: 研究突出了乌克兰语等非主流语言在情感分类中的挑战，表明需要更多乌克兰语特定的模型和训练资源。

Conclusion: EmoBench-UA为乌克兰语情感检测提供了首个基准数据集，揭示了进一步开发乌克兰语NLP资源的必要性。

Abstract: While Ukrainian NLP has seen progress in many texts processing tasks, emotion
classification remains an underexplored area with no publicly available
benchmark to date. In this work, we introduce EmoBench-UA, the first annotated
dataset for emotion detection in Ukrainian texts. Our annotation schema is
adapted from the previous English-centric works on emotion detection (Mohammad
et al., 2018; Mohammad, 2022) guidelines. The dataset was created through
crowdsourcing using the Toloka.ai platform ensuring high-quality of the
annotation process. Then, we evaluate a range of approaches on the collected
dataset, starting from linguistic-based baselines, synthetic data translated
from English, to large language models (LLMs). Our findings highlight the
challenges of emotion classification in non-mainstream languages like Ukrainian
and emphasize the need for further development of Ukrainian-specific models and
training resources.

</details>


### [70] [Data-efficient Meta-models for Evaluation of Context-based Questions and Answers in LLMs](https://arxiv.org/abs/2505.23299)
*Julia Belikova,Konstantin Polev,Rauf Parchiev,Dmitry Simakov*

Key words: LLMs, RAG, 幻觉检测, 数据效率, 降维

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了如何通过结合高效分类算法和降维技术，减少对标注数据的依赖，从而提升LLMs和RAG系统中的幻觉检测性能，仅需250个训练样本即可达到与基线相当的效果。

Motivation: 解决LLMs和RAG系统在工业应用中因标注数据不足而受限的幻觉检测问题。

Method: 结合高效分类算法和降维技术，减少训练数据需求，应用于Lookback Lens和基于探针的方法。

Result: 在标准问答RAG基准测试中，仅需250个训练样本即可达到与基线相当的性能。

Conclusion: 轻量级、数据高效的范式在工业部署中具有潜力，特别适用于标注受限场景。

Abstract: Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems
are increasingly deployed in industry applications, yet their reliability
remains hampered by challenges in detecting hallucinations. While supervised
state-of-the-art (SOTA) methods that leverage LLM hidden states -- such as
activation tracing and representation analysis -- show promise, their
dependence on extensively annotated datasets limits scalability in real-world
applications. This paper addresses the critical bottleneck of data annotation
by investigating the feasibility of reducing training data requirements for two
SOTA hallucination detection frameworks: Lookback Lens, which analyzes
attention head dynamics, and probing-based approaches, which decode internal
model representations. We propose a methodology combining efficient
classification algorithms with dimensionality reduction techniques to minimize
sample size demands while maintaining competitive performance. Evaluations on
standardized question-answering RAG benchmarks show that our approach achieves
performance comparable to strong proprietary LLM-based baselines with only 250
training samples. These results highlight the potential of lightweight,
data-efficient paradigms for industrial deployment, particularly in
annotation-constrained scenarios.

</details>


### [71] [Generalized Category Discovery in Event-Centric Contexts: Latent Pattern Mining with LLMs](https://arxiv.org/abs/2505.23304)
*Yi Luo,Qiwen Wang,Junqi Yang,Luyao Tang,Zhenghao Lin,Zhenzhe Ying,Weiqiang Wang,Chen Lin*

Key words: 通用类别发现（GCD）、事件中心GCD（EC-GCD）、PaMA框架、LLMs、聚类与分类对齐

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为EC-GCD的事件中心通用类别发现方法，解决了现有文本GCD方法在现实场景中的验证不足问题。通过PaMA框架利用LLM提取和优化事件模式，改善了聚类与分类的对齐，并通过排名-过滤-挖掘流程平衡了不均衡类别的原型表示。在包括新构建的Scam Report数据集在内的两个EC-GCD基准测试中，PaMA性能优于现有方法，H-score提升高达12.58%。

Motivation: 现有文本GCD方法在现实场景中的验证不足，尤其是在处理长、复杂叙事和高不均衡类别分布时面临挑战。

Method: 提出了PaMA框架，利用LLMs提取和优化事件模式，并通过排名-过滤-挖掘流程平衡不均衡类别的原型表示。

Result: 在EC-GCD基准测试中，PaMA的H-score比现有方法提升高达12.58%，同时在基础GCD数据集上保持了强泛化能力。

Conclusion: PaMA框架有效解决了EC-GCD中的聚类与分类对齐问题及类别不均衡问题，显著提升了性能。

Abstract: Generalized Category Discovery (GCD) aims to classify both known and novel
categories using partially labeled data that contains only known classes.
Despite achieving strong performance on existing benchmarks, current textual
GCD methods lack sufficient validation in realistic settings. We introduce
Event-Centric GCD (EC-GCD), characterized by long, complex narratives and
highly imbalanced class distributions, posing two main challenges: (1)
divergent clustering versus classification groupings caused by subjective
criteria, and (2) Unfair alignment for minority classes. To tackle these, we
propose PaMA, a framework leveraging LLMs to extract and refine event patterns
for improved cluster-class alignment. Additionally, a ranking-filtering-mining
pipeline ensures balanced representation of prototypes across imbalanced
categories. Evaluations on two EC-GCD benchmarks, including a newly constructed
Scam Report dataset, demonstrate that PaMA outperforms prior methods with up to
12.58% H-score gains, while maintaining strong generalization on base GCD
datasets.

</details>


### [72] [Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments](https://arxiv.org/abs/2505.23315)
*Abhirup Chakravarty,Mark Brenchley,Trevor Breakspear,Ian Lewin,Yan Huang*

Key words: 自动作文评分, 信心建模, CEFR, KWOCCE

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究通过将信心估计建模为分类任务，结合新的损失函数KWOCCE，显著提高了自动作文评分系统的可靠性和评分发布标准。

Motivation: 解决自动作文评分（AES）中评分可靠性不足的问题，确保仅在评分达到高可靠性标准时才发布。

Method: 将信心估计重构为n元分类问题，利用评分分箱和引入KWOCCE损失函数，利用CEFR标签的有序结构。

Result: 最佳模型F1分数为0.97，使系统能够发布47%的评分（100% CEFR一致）和99%的评分（至少95% CEFR一致），相比之下单独AES模型的约92%一致率。

Conclusion: 通过信心建模和新的损失函数，显著提升了AES评分的可靠性和可发布性。

Abstract: A key ethical challenge in Automated Essay Scoring (AES) is ensuring that
scores are only released when they meet high reliability standards. Confidence
modelling addresses this by assigning a reliability estimate measure, in the
form of a confidence score, to each automated score. In this study, we frame
confidence estimation as a classification task: predicting whether an
AES-generated score correctly places a candidate in the appropriate CEFR level.
While this is a binary decision, we leverage the inherent granularity of the
scoring domain in two ways. First, we reformulate the task as an n-ary
classification problem using score binning. Second, we introduce a set of novel
Kernel Weighted Ordinal Categorical Cross Entropy (KWOCCE) loss functions that
incorporate the ordinal structure of CEFR labels. Our best-performing model
achieves an F1 score of 0.97, and enables the system to release 47% of scores
with 100% CEFR agreement and 99% with at least 95% CEFR agreement -compared to
approximately 92% (approx.) CEFR agreement from the standalone AES model where
we release all AM predicted scores.

</details>


### [73] [Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO](https://arxiv.org/abs/2505.23316)
*Kaiyang Guo,Yinchuan Li,Zhitang Chen*

Key words: 大语言模型, 直接偏好优化, 似然不确定性, 奖励破解, 近端偏好优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了PRO方法，解决了直接偏好优化（DPO）中似然不确定性导致模型输出偏离预期的问题，并通过理论分解和实验验证了其优越性。

Motivation: 当前直接对齐方法（如DPO）通过对比偏好和非偏好响应的似然来优化大语言模型，但会降低绝对似然，导致输出偏离预期（奖励破解效应），暴露了对比对齐的根本局限性——似然不确定性。

Method: 首先理论分解DPO损失函数，发现其隐含简化了一个正则项。通过恢复完整正则项，提出PRO方法（近端偏好优化），支持多样反馈类型并高效逼近完整正则项。

Result: 实验表明，PRO在成对、二元和标量反馈场景中均优于现有方法，有效解决了似然不确定性问题。

Conclusion: PRO通过理论创新和正则项完整化，统一了对齐多样反馈的框架，显著提升模型对齐效果。

Abstract: Direct alignment methods typically optimize large language models (LLMs) by
contrasting the likelihoods of preferred versus dispreferred responses. While
effective in steering LLMs to match relative preference, these methods are
frequently noted for decreasing the absolute likelihoods of example responses.
As a result, aligned models tend to generate outputs that deviate from the
expected patterns, exhibiting reward-hacking effect even without a reward
model. This undesired consequence exposes a fundamental limitation in
contrastive alignment, which we characterize as likelihood underdetermination.
In this work, we revisit direct preference optimization (DPO) -- the seminal
direct alignment method -- and demonstrate that its loss theoretically admits a
decomposed reformulation. The reformulated loss not only broadens applicability
to a wider range of feedback types, but also provides novel insights into the
underlying cause of likelihood underdetermination. Specifically, the standard
DPO implementation implicitly oversimplifies a regularizer in the reformulated
loss, and reinstating its complete version effectively resolves the
underdetermination issue. Leveraging these findings, we introduce PRoximalized
PReference Optimization (PRO), a unified method to align with diverse feeback
types, eliminating likelihood underdetermination through an efficient
approximation of the complete regularizer. Comprehensive experiments show the
superiority of PRO over existing methods in scenarios involving pairwise,
binary and scalar feedback.

</details>


### [74] [Neither Stochastic Parroting nor AGI: LLMs Solve Tasks through Context-Directed Extrapolation from Training Data Priors](https://arxiv.org/abs/2505.23323)
*Harish Tayyar Madabushi,Melissa Torgbi,Claire Bonial*

Key words: 大型语言模型, 上下文导向外推, 涌现能力, 认知能力, 训练数据

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该立场论文提出了一种关于大型语言模型（LLM）能力的中间观点，避免了极端看法。作者认为LLM通过上下文学外推（context-directed extrapolation）从训练数据中提取信息，而非具备不可预测的涌现能力。

Motivation: 论文旨在纠正对LLM能力的两种极端看法（‘随机鹦鹉’或‘涌现高级推理’），提倡一种更现实的中间观点，以缓解对不可控涌现能力的担忧，并重新聚焦研究重点。

Method: 通过文献综述和理论分析，提出‘上下文导向外推’机制，解释LLM如何从训练数据中提取并利用信息。

Result: 论文证明LLM的能力超出随机模仿，但可预测、可控，且不体现人类高级认知能力。其能力受限于训练数据，无法通过无限扩展训练无限提升。

Conclusion: 研究表明LLM的‘涌现’能力可控，未来应研究如何优化上下文导向外推及训练数据的交互，而非依赖LLM的固有推理能力。

Abstract: In this position paper we raise critical awareness of a realistic view of LLM
capabilities that eschews extreme alternative views that LLMs are either
"stochastic parrots" or in possession of "emergent" advanced reasoning
capabilities, which, due to their unpredictable emergence, constitute an
existential threat. Our middle-ground view is that LLMs extrapolate from priors
from their training data, and that a mechanism akin to in-context learning
enables the targeting of the appropriate information from which to extrapolate.
We call this "context-directed extrapolation." Under this view, substantiated
though existing literature, while reasoning capabilities go well beyond
stochastic parroting, such capabilities are predictable, controllable, not
indicative of advanced reasoning akin to high-level cognitive capabilities in
humans, and not infinitely scalable with additional training. As a result,
fears of uncontrollable emergence of agency are allayed, while research
advances are appropriately refocused on the processes of context-directed
extrapolation and how this interacts with training data to produce valuable
capabilities in LLMs. Future work can therefore explore alternative augmenting
techniques that do not rely on inherent advanced reasoning in LLMs.

</details>


### [75] [Discriminative Policy Optimization for Token-Level Reward Models](https://arxiv.org/abs/2505.23363)
*Hongzhan Chen,Tao Yang,Shiping Gao,Ruijun Chen,Xiaojun Quan,Hongtao Tian,Ting Yao*

Key words: Q-RM, token-level reward, discriminative policy, mathematical reasoning

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Q-RM通过解耦奖励建模与语言生成，优化判别策略学习token级Q函数，显著提升数学推理任务的性能与训练效率。

Motivation: 现有PRMs在token级粒度上存在生成与奖励模型冲突问题，导致不稳定和信用分配不准确。

Method: 提出Q-RM，通过优化判别策略从偏好数据中学习token级Q函数，避免依赖细粒度标注。

Result: Q-RM在多个基准测试中表现最佳，数学推理任务Pass@1分数提升显著，训练效率提高10倍以上。

Conclusion: Q-RM有效解决了PRMs的冲突问题，为复杂推理任务的LLM优化提供了新方向。

Abstract: Process reward models (PRMs) provide more nuanced supervision compared to
outcome reward models (ORMs) for optimizing policy models, positioning them as
a promising approach to enhancing the capabilities of LLMs in complex reasoning
tasks. Recent efforts have advanced PRMs from step-level to token-level
granularity by integrating reward modeling into the training of generative
models, with reward scores derived from token generation probabilities.
However, the conflict between generative language modeling and reward modeling
may introduce instability and lead to inaccurate credit assignments. To address
this challenge, we revisit token-level reward assignment by decoupling reward
modeling from language generation and derive a token-level reward model through
the optimization of a discriminative policy, termed the Q-function Reward Model
(Q-RM). We theoretically demonstrate that Q-RM explicitly learns token-level
Q-functions from preference data without relying on fine-grained annotations.
In our experiments, Q-RM consistently outperforms all baseline methods across
various benchmarks. For example, when integrated into PPO/REINFORCE algorithms,
Q-RM enhances the average Pass@1 score by 5.85/4.70 points on mathematical
reasoning tasks compared to the ORM baseline, and by 4.56/5.73 points compared
to the token-level PRM counterpart. Moreover, reinforcement learning with Q-RM
significantly enhances training efficiency, achieving convergence 12 times
faster than ORM on GSM8K and 11 times faster than step-level PRM on MATH. Code
and data are available at https://github.com/homzer/Q-RM.

</details>


### [76] [Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation](https://arxiv.org/abs/2505.23368)
*Beiduo Chen,Yang Janet Liu,Anna Korhonen,Barbara Plank*

Key words: LLM, 推理链(CoT), 标签变异(HLV), 语言学分割, 排名评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于LLM的新方法，通过CoT提取支持与反对陈述，并引入基于排名的HLV评估框架，显著优于现有方法。

Motivation: 现有方法通常基于给定答案生成解释，而CoT提供的前向推理路径隐含了每个选项的理性依据。本文旨在通过这一优势改进对齐人类标签分布的任务。

Method: 提出了一种结合语言学基础的语段分割器的LLM流程，从CoT中更准确地提取支持与反对陈述；并设计了一种基于排名的HLV评估框架。

Result: 在三个数据集上表现优于直接生成方法和基线，且排名方法更符合人类判断。

Conclusion: 该方法有效利用了CoT的前向推理特性，显著提升了与人类标签分布的对应性。

Abstract: The recent rise of reasoning-tuned Large Language Models (LLMs)--which
generate chains of thought (CoTs) before giving the final answer--has attracted
significant attention and offers new opportunities for gaining insights into
human label variation, which refers to plausible differences in how multiple
annotators label the same data instance. Prior work has shown that
LLM-generated explanations can help align model predictions with human label
distributions, but typically adopt a reverse paradigm: producing explanations
based on given answers. In contrast, CoTs provide a forward reasoning path that
may implicitly embed rationales for each answer option, before generating the
answers. We thus propose a novel LLM-based pipeline enriched with
linguistically-grounded discourse segmenters to extract supporting and opposing
statements for each answer option from CoTs with improved accuracy. We also
propose a rank-based HLV evaluation framework that prioritizes the ranking of
answers over exact scores, which instead favor direct comparison of label
distributions. Our method outperforms a direct generation method as well as
baselines on three datasets, and shows better alignment of ranking methods with
humans, highlighting the effectiveness of our approach.

</details>


### [77] [Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models](https://arxiv.org/abs/2505.23404)
*Mingyu Yu,Wei Wang,Yanjie Wei,Sujuan Qin*

Key words: 大型语言模型，对抗攻击，越狱技术，语义理解，AI安全

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于大型语言模型语义理解能力的自适应越狱策略框架，显著提高了攻击成功率达98.9%。

Motivation: 解决AI安全中通过越狱技术绕过大型语言模型内置安全约束的对抗攻击问题，研究不同LLMs的理解能力差异如何影响越狱效果。

Method: 提出分类Type I和Type II LLMs的框架，并根据其语义理解能力设计定制化越狱策略。

Result: 在多LLMs实验中，自适应策略显著提升越狱成功率，特别在GPT-4o上达到98.9%的成功率。

Conclusion: 自适应越狱策略针对LLMs的理解能力差异，有效利用其弱点，显著提升了攻击成功率。

Abstract: Adversarial attacks on Large Language Models (LLMs) via jailbreaking
techniques-methods that circumvent their built-in safety and ethical
constraints-have emerged as a critical challenge in AI security. These attacks
compromise the reliability of LLMs by exploiting inherent weaknesses in their
comprehension capabilities. This paper investigates the efficacy of
jailbreaking strategies that are specifically adapted to the diverse levels of
understanding exhibited by different LLMs. We propose the Adaptive Jailbreaking
Strategies Based on the Semantic Understanding Capabilities of Large Language
Models, a novel framework that classifies LLMs into Type I and Type II
categories according to their semantic comprehension abilities. For each
category, we design tailored jailbreaking strategies aimed at leveraging their
vulnerabilities to facilitate successful attacks. Extensive experiments
conducted on multiple LLMs demonstrate that our adaptive strategy markedly
improves the success rate of jailbreaking. Notably, our approach achieves an
exceptional 98.9% success rate in jailbreaking GPT-4o(29 May 2025 release)

</details>


### [78] [From Parameters to Prompts: Understanding and Mitigating the Factuality Gap between Fine-Tuned LLMs](https://arxiv.org/abs/2505.23410)
*Xuan Gong,Hanbo Huang,Shiyu Liang*

Key words: 知识提取, 微调数据, 事实性差距, ICL, 知识图谱

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了微调数据与测试时提示对LLMs事实性提取的影响，发现ICL提示可弥补微调数据的不足，并提出了评估微调数据选择方法的新思路。

Motivation: 研究旨在明确微调数据对LLMs事实性提取的影响机制，尤其是已知与未知知识间的事实性差距问题。

Method: 通过系统实验，分析微调数据与测试时提示（如OOD设置和ICL）对事实性提取的交互作用，并从知识图谱角度进行理论证明。

Result: 发现测试时提示可显著弥补微调数据的不足，ICL（如少样本学习和CoT）能有效缩小事实性差距。

Conclusion: 研究表明ICL提示在知识提取中起主导作用，强调了重新评估ICL在微调数据选择方法有效性中的作用。

Abstract: Factual knowledge extraction aims to explicitly extract knowledge
parameterized in pre-trained language models for application in downstream
tasks. While prior work has been investigating the impact of supervised
fine-tuning data on the factuality of large language models (LLMs), its
mechanism remains poorly understood. We revisit this impact through systematic
experiments, with a particular focus on the factuality gap that arises when
fine-tuning on known versus unknown knowledge. Our findings show that this gap
can be mitigated at the inference stage, either under out-of-distribution (OOD)
settings or by using appropriate in-context learning (ICL) prompts (i.e.,
few-shot learning and Chain of Thought (CoT)). We prove this phenomenon
theoretically from the perspective of knowledge graphs, showing that the
test-time prompt may diminish or even overshadow the impact of fine-tuning data
and play a dominant role in knowledge extraction. Ultimately, our results shed
light on the interaction between finetuning data and test-time prompt,
demonstrating that ICL can effectively compensate for shortcomings in
fine-tuning data, and highlighting the need to reconsider the use of ICL
prompting as a means to evaluate the effectiveness of fine-tuning data
selection methods.

</details>


### [79] [The Warmup Dilemma: How Learning Rate Strategies Impact Speech-to-Text Model Convergence](https://arxiv.org/abs/2505.23420)
*Marco Gaido,Sara Papi,Luisa Bentivogli,Alessio Brutti,Mauro Cettolo,Roberto Gretter,Marco Matassoni,Mohamed Nabih,Matteo Negri*

Key words: 大规模训练, 语音到文本, 学习率预热, Transformer架构, 收敛性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究填补了大规模语音到文本（S2T）训练中学习率（LR）预热调度的空白，发现需要次指数型LR预热，且高初始LR能加速收敛但对最终性能无提升。

Motivation: 研究动机在于解决大规模模型训练中学习率调整对收敛性和性能的影响，尤其是在复杂Transformer架构（如Conformer或Branchformer）下，现有简单的LR下调方法不足以应对。

Method: 方法上，该研究对比了不同的LR预热调度策略，特别分析了OWSMs的双线性预热方案，进而提出了次指数型LR预热方法，并探讨了初始高LR对训练的影响。

Result: 实验结果表明，大规模S2T训练需要次指数型LR预热；更高的初始LR可以加速初期收敛，但对最终模型性能没有显著提升。

Conclusion: 结论指出，次指数型LR预热更适合大规模S2T训练，而初始高LR的作用仅限于加快早期收敛。

Abstract: Training large-scale models presents challenges not only in terms of resource
requirements but also in terms of their convergence. For this reason, the
learning rate (LR) is often decreased when the size of a model is increased.
Such a simple solution is not enough in the case of speech-to-text (S2T)
trainings, where evolved and more complex variants of the Transformer
architecture -- e.g., Conformer or Branchformer -- are used in light of their
better performance. As a workaround, OWSM designed a double linear warmup of
the LR, increasing it to a very small value in the first phase before updating
it to a higher value in the second phase. While this solution worked well in
practice, it was not compared with alternative solutions, nor was the impact on
the final performance of different LR warmup schedules studied. This paper
fills this gap, revealing that i) large-scale S2T trainings demand a
sub-exponential LR warmup, and ii) a higher LR in the warmup phase accelerates
initial convergence, but it does not boost final performance.

</details>


### [80] [UAQFact: Evaluating Factual Knowledge Utilization of LLMs on Unanswerable Questions](https://arxiv.org/abs/2505.23461)
*Chuanyuan Tan,Wenbiao Shao,Hao Xiong,Tong Zhu,Zhenhua Liu,Kai Shi,Wenliang Chen*

Key words: 不可回答问题、LLM、事实知识、数据集、知识图谱

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一个双语不可回答问题数据集UAQFact，旨在评估LLM在处理不可回答问题时利用内部和外部事实知识的能力。实验表明，即使具备相关知识，LLM表现仍不稳定，且外部知识可能提升效果但不充分。

Motivation: 现有数据集缺乏事实知识支持，无法全面评估LLM在处理不可回答问题时的知识利用能力。

Method: 基于知识图谱构建双语数据集UAQFact，并定义两任务分别测试LLM对内部和外部知识的利用能力。

Result: 实验显示LLM在UAQFact上表现不稳定，外部知识可能提升效果，但LLM仍无法充分利用知识。

Conclusion: UAQFact对LLM提出新挑战，未来需优化知识整合机制以提高不可回答问题处理能力。

Abstract: Handling unanswerable questions (UAQ) is crucial for LLMs, as it helps
prevent misleading responses in complex situations. While previous studies have
built several datasets to assess LLMs' performance on UAQ, these datasets lack
factual knowledge support, which limits the evaluation of LLMs' ability to
utilize their factual knowledge when handling UAQ. To address the limitation,
we introduce a new unanswerable question dataset UAQFact, a bilingual dataset
with auxiliary factual knowledge created from a Knowledge Graph. Based on
UAQFact, we further define two new tasks to measure LLMs' ability to utilize
internal and external factual knowledge, respectively. Our experimental results
across multiple LLM series show that UAQFact presents significant challenges,
as LLMs do not consistently perform well even when they have factual knowledge
stored. Additionally, we find that incorporating external knowledge may enhance
performance, but LLMs still cannot make full use of the knowledge which may
result in incorrect responses.

</details>


### [81] [Evaluating the performance and fragility of large language models on the self-assessment for neurological surgeons](https://arxiv.org/abs/2505.23477)
*Krithik Vishwanath,Anton Alyakin,Mrigayu Ghosh,Jin Vivian Lee,Daniel Alexander Alber,Karl L. Sangwon,Douglas Kondziolka,Eric Karl Oermann*

Key words: 大语言模型,神经外科考试,干扰信息,鲁棒性,临床部署

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究表明，现有的大语言模型（LLMs）能够通过神经外科委员会考试，但在有干扰信息的情况下表现显著下降。这一发现强调了增强模型抗干扰能力的必要性。

Motivation: 评估当前最先进的大语言模型在神经外科委员会考试中的表现，以及评估其对干扰信息的鲁棒性。

Method: 使用28个大语言模型评估2,904个来自CNS-SANS的神经外科考试题目，并引入干扰框架以测试模型对干扰信息的敏感性。

Result: 在28个模型中，6个达到了及格标准，其中表现最好的模型高出及格线15.7%。干扰信息使模型准确率大幅下降，降幅最高达20.4%。

Conclusion: 大语言模型虽然能够回答神经外科考试题目，但其表现易受无关信息干扰，因此需要开发新策略以增强其在临床部署中的稳健性。

Abstract: The Congress of Neurological Surgeons Self-Assessment for Neurological
Surgeons (CNS-SANS) questions are widely used by neurosurgical residents to
prepare for written board examinations. Recently, these questions have also
served as benchmarks for evaluating large language models' (LLMs) neurosurgical
knowledge. This study aims to assess the performance of state-of-the-art LLMs
on neurosurgery board-like questions and to evaluate their robustness to the
inclusion of distractor statements. A comprehensive evaluation was conducted
using 28 large language models. These models were tested on 2,904 neurosurgery
board examination questions derived from the CNS-SANS. Additionally, the study
introduced a distraction framework to assess the fragility of these models. The
framework incorporated simple, irrelevant distractor statements containing
polysemous words with clinical meanings used in non-clinical contexts to
determine the extent to which such distractions degrade model performance on
standard medical benchmarks. 6 of the 28 tested LLMs achieved board-passing
outcomes, with the top-performing models scoring over 15.7% above the passing
threshold. When exposed to distractions, accuracy across various model
architectures was significantly reduced-by as much as 20.4%-with one model
failing that had previously passed. Both general-purpose and medical
open-source models experienced greater performance declines compared to
proprietary variants when subjected to the added distractors. While current
LLMs demonstrate an impressive ability to answer neurosurgery board-like exam
questions, their performance is markedly vulnerable to extraneous, distracting
information. These findings underscore the critical need for developing novel
mitigation strategies aimed at bolstering LLM resilience against in-text
distractions, particularly for safe and effective clinical deployment.

</details>


### [82] [Revisiting Overthinking in Long Chain-of-Thought from the Perspective of Self-Doubt](https://arxiv.org/abs/2505.23480)
*Keqin Peng,Liang Ding,Yuanxin Ouyang,Meng Fang,Dacheng Tao*

Key words: 大型语言模型、长链推理、过度思考、自我怀疑、提示方法

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: RLLMs通过长链推理表现出色，但存在过度思考问题。本文从自我怀疑角度定量分析过度思考，提出一种简单有效的提示方法，减少模型对输入问题的依赖，从而避免自我怀疑。实验证明该方法显著缩短答案长度并提升性能。

Motivation: 大型语言模型在复杂任务中表现优秀，但常因长链推理产生过度思考现象，即在不必要的情况下继续推理。此前研究多基于定性分析，本文则从自我怀疑角度进行定量分析，并提出解决方案。

Method: 提出一种提示方法：首先让模型质疑输入问题的有效性，然后根据评估结果简洁回答。实验在三个数学推理任务和四个缺失前提的数据集上进行。

Result: 该方法显著减少了答案长度，并在四个广泛使用的RLLMs上对所有数据集都带来显著改进。同时有效减少了推理步骤和自我怀疑现象。

Conclusion: 通过减少模型对输入问题的依赖，可以有效缓解过度思考问题，提升模型效率和性能。

Abstract: Reasoning Large Language Models (RLLMs) have demonstrated impressive
performance on complex tasks, largely due to the adoption of Long
Chain-of-Thought (Long CoT) reasoning. However, they often exhibit overthinking
-- performing unnecessary reasoning steps even after arriving at the correct
answer. Prior work has largely focused on qualitative analyses of overthinking
through sample-based observations of long CoTs. In contrast, we present a
quantitative analysis of overthinking from the perspective of self-doubt,
characterized by excessive token usage devoted to re-verifying already-correct
answer. We find that self-doubt significantly contributes to overthinking. In
response, we introduce a simple and effective prompting method to reduce the
model's over-reliance on input questions, thereby avoiding self-doubt.
Specifically, we first prompt the model to question the validity of the input
question, and then respond concisely based on the outcome of that evaluation.
Experiments on three mathematical reasoning tasks and four datasets with
missing premises demonstrate that our method substantially reduces answer
length and yields significant improvements across nearly all datasets upon 4
widely-used RLLMs. Further analysis demonstrates that our method effectively
minimizes the number of reasoning steps and reduces self-doubt.

</details>


### [83] [Spoken Language Modeling with Duration-Penalized Self-Supervised Units](https://arxiv.org/abs/2505.23494)
*Nicol Visser,Herman Kamper*

Key words: 语音语言模型, 码本大小, 单元粗糙度, DPDP, 语音合成

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探索了语音语言模型（SLMs）中码本大小和单元粗糙度（即持续时间）对性能的影响，表明在特定任务中较粗糙的单元能提升模型性能。

Motivation: 研究SLMs中码本大小和单元粗糙度的交互作用及其对性能的影响，以优化模型表现。

Method: 通过简单的时长惩罚动态规划（DPDP）方法调整码本大小和单元粗糙度，并分析不同语言层次的表现。

Result: 在句子重合成任务中，粗糙单元表现更好；在词汇和句法语言建模任务中，粗糙单元在低比特率下准确性更高。

Conclusion: 粗糙单元并非总是更好，但DPDP是一种简单高效的方法，可在需要粗糙单元的任务中获得性能提升。

Abstract: Spoken language models (SLMs) operate on acoustic units obtained by
discretizing self-supervised speech representations. Although the
characteristics of these units directly affect performance, the interaction
between codebook size and unit coarseness (i.e., duration) remains unexplored.
We investigate SLM performance as we vary codebook size and unit coarseness
using the simple duration-penalized dynamic programming (DPDP) method. New
analyses are performed across different linguistic levels. At the phone and
word levels, coarseness provides little benefit, as long as the codebook size
is chosen appropriately. However, when producing whole sentences in a
resynthesis task, SLMs perform better with coarser units. In lexical and
syntactic language modeling tasks, coarser units also give higher accuracies at
lower bitrates. We therefore show that coarser units aren't always better, but
that DPDP is a simple and efficient way to obtain coarser units for the tasks
where they are beneficial.

</details>


### [84] [Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking](https://arxiv.org/abs/2505.23495)
*Liangliang Zhang,Zhuorui Jiang,Hongliang Chi,Haoyang Chen,Mohammed Elkoumy,Fali Wang,Qiong Wu,Zhengyi Zhou,Shirui Pan,Suhang Wang,Yao Ma*

Key words: Knowledge Graph Question Answering, Benchmark, LLM, Wikidata

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: KGQAGen is a framework introduced to improve the quality of Knowledge Graph Question Answering benchmarks, addressing issues like inaccuracies and ambiguities in existing datasets.

Motivation: Existing KGQA datasets often contain inaccuracies, ambiguities, or outdated knowledge, limiting their reliability for evaluating complex reasoning.

Method: KGQAGen uses structured knowledge grounding, LLM-guided generation, and symbolic verification to create high-quality QA instances.

Result: KGQAGen-10k, a new benchmark, reveals limitations in state-of-the-art KGQA systems, showing its effectiveness.

Conclusion: The paper advocates for stricter benchmark standards and presents KGQAGen as a scalable solution for better KGQA evaluation.

Abstract: Knowledge Graph Question Answering (KGQA) systems rely on high-quality
benchmarks to evaluate complex multi-hop reasoning. However, despite their
widespread use, popular datasets such as WebQSP and CWQ suffer from critical
quality issues, including inaccurate or incomplete ground-truth annotations,
poorly constructed questions that are ambiguous, trivial, or unanswerable, and
outdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA
datasets, including WebQSP and CWQ, we find that the average factual
correctness rate is only 57 %. To address these issues, we introduce KGQAGen,
an LLM-in-the-loop framework that systematically resolves these pitfalls.
KGQAGen combines structured knowledge grounding, LLM-guided generation, and
symbolic verification to produce challenging and verifiable QA instances. Using
KGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in
Wikidata, and evaluate a diverse set of KG-RAG models. Experimental results
demonstrate that even state-of-the-art systems struggle on this benchmark,
highlighting its ability to expose limitations of existing models. Our findings
advocate for more rigorous benchmark construction and position KGQAGen as a
scalable framework for advancing KGQA evaluation.

</details>


### [85] [CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental Promise Verification](https://arxiv.org/abs/2505.23538)
*Nawar Turk,Eeham Khan,Leila Kosseim*

Key words: ESG, 承诺验证, 模型架构, 多任务学习, 注意力机制

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了企业ESG报告中的承诺验证，提出了三种模型架构，并在ML-Promise数据集的英文部分取得进步，最终组合模型表现最佳。

Motivation: 企业ESG报告中的承诺验证是重要的，但面临数据不足和类别不平衡的挑战，因此需要有效的模型架构来提升验证效果。

Method: 采用三种模型架构：1) ESG-BERT加任务特定分类头；2) 加入语言学特征；3) 组合子任务模型，包括注意力池化、元数据增强的Transformer和多目标学习。

Result: 组合模型在ML-Promise数据集上得分0.5268，优于基线0.5227，证明语言学特征、注意力池化和多目标学习的有效性。

Conclusion: 尽管数据有限且类别不平衡，但提出的方法在承诺验证任务中表现优秀，尤其是组合模型架构。

Abstract: This paper presents our approach to the SemEval-2025 Task~6 (PromiseEval),
which focuses on verifying promises in corporate ESG (Environmental, Social,
and Governance) reports. We explore three model architectures to address the
four subtasks of promise identification, supporting evidence assessment,
clarity evaluation, and verification timing. Our first model utilizes ESG-BERT
with task-specific classifier heads, while our second model enhances this
architecture with linguistic features tailored for each subtask. Our third
approach implements a combined subtask model with attention-based sequence
pooling, transformer representations augmented with document metadata, and
multi-objective learning. Experiments on the English portion of the ML-Promise
dataset demonstrate progressive improvement across our models, with our
combined subtask approach achieving a leaderboard score of 0.5268,
outperforming the provided baseline of 0.5227. Our work highlights the
effectiveness of linguistic feature extraction, attention pooling, and
multi-objective learning in promise verification tasks, despite challenges
posed by class imbalance and limited training data.

</details>


### [86] [Probability-Consistent Preference Optimization for Enhanced LLM Reasoning](https://arxiv.org/abs/2505.23540)
*Yunqiao Yang,Houxing Ren,Zimu Lu,Ke Wang,Weikang Shi,Aojun Zhou,Junting Pan,Mingjie Zhan,Hongsheng Li*

Key words: 偏好优化, 大型语言模型, 数学推理, PCPO, 概率一致性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为PCPO的新框架，通过双重定量指标（答案正确性和内在概率一致性）优化语言模型的偏好，显著提升了性能。

Motivation: 现有方法仅基于答案正确性或一致性等结果标准，忽略了回答的内在逻辑连贯性，PCPO旨在解决这一问题。

Method: 提出概率一致性偏好优化（PCPO），通过表面级答案正确性和内在令牌级概率一致性双重指标选择偏好。

Result: 在多种大型语言模型和基准测试中，PCPO始终优于仅基于结果标准的现有方法。

Conclusion: PCPO通过结合双重指标，显著提升了语言模型的数学推理能力。

Abstract: Recent advances in preference optimization have demonstrated significant
potential for improving mathematical reasoning capabilities in large language
models (LLMs). While current approaches leverage high-quality pairwise
preference data through outcome-based criteria like answer correctness or
consistency, they fundamentally neglect the internal logical coherence of
responses. To overcome this, we propose Probability-Consistent Preference
Optimization (PCPO), a novel framework that establishes dual quantitative
metrics for preference selection: (1) surface-level answer correctness and (2)
intrinsic token-level probability consistency across responses. Extensive
experiments show that our PCPO consistently outperforms existing outcome-only
criterion approaches across a diverse range of LLMs and benchmarks. Our code is
publicly available at https://github.com/YunqiaoYang/PCPO.

</details>


### [87] [Translation in the Wild](https://arxiv.org/abs/2505.23548)
*Yuri Balashov*

Key words: 大型语言模型, 翻译能力, 预训练数据, 双重性假设, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LLMs展示出优秀的翻译能力，但缺乏翻译专用训练。作者探讨其能力来源，提出基于预训练数据的‘双重性’假设，并讨论实证测试的前景。

Motivation: 探究LLMs在未经特定翻译训练情况下表现出色的原因，研究其翻译能力的来源与机制。

Method: 通过文献回顾和用户经验分析，提出‘双重性’假设，并讨论实证验证的可能性。

Result: LLMs的翻译能力可能源于两种不同类型的预训练数据，并以不同方式内化。

Conclusion: LLMs的翻译能力挑战传统翻译概念，需重新思考人机翻译在深度学习时代的意义。

Abstract: Large Language Models (LLMs) excel in translation among other things,
demonstrating competitive performance for many language pairs in zero- and
few-shot settings. But unlike dedicated neural machine translation models, LLMs
are not trained on any translation-related objective. What explains their
remarkable translation abilities? Are these abilities grounded in "incidental
bilingualism" (Briakou et al. 2023) in training data? Does instruction tuning
contribute to it? Are LLMs capable of aligning and leveraging semantically
identical or similar monolingual contents from different corners of the
internet that are unlikely to fit in a single context window? I offer some
reflections on this topic, informed by recent studies and growing user
experience. My working hypothesis is that LLMs' translation abilities originate
in two different types of pre-training data that may be internalized by the
models in different ways. I discuss the prospects for testing the "duality"
hypothesis empirically and its implications for reconceptualizing translation,
human and machine, in the age of deep learning.

</details>


### [88] [Understanding Refusal in Language Models with Sparse Autoencoders](https://arxiv.org/abs/2505.23556)
*Wei Jie Yeo,Nirmalendu Prakash,Clement Neo,Roy Ka-Wei Lee,Erik Cambria,Ranjan Satapathy*

Key words: 拒绝行为, 语言模型, 稀疏自编码器, 干预实验, 对抗样本

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文通过稀疏自编码器研究了指令调优语言模型中的拒绝行为，识别了导致拒绝的潜在特征，并通过干预验证了这些特征的影响。

Motivation: 研究动机是揭示语言模型中拒绝行为的内在机制，尤其是在对齐模型中的安全行为。

Method: 使用稀疏自编码器识别潜在特征，并在两个开源聊天模型上进行干预实验，验证拒绝行为的影响。

Result: 研究结果包括拒绝特征的细化分析、上下游潜在关系的探究，以及拒绝特征在分类任务中对抗样本泛化的增强作用。

Conclusion: 研究表明拒绝特征的机制分析有助于理解语言模型的安全行为，并为对抗样本分类任务提供改进。

Abstract: Refusal is a key safety behavior in aligned language models, yet the internal
mechanisms driving refusals remain opaque. In this work, we conduct a
mechanistic study of refusal in instruction-tuned LLMs using sparse
autoencoders to identify latent features that causally mediate refusal
behaviors. We apply our method to two open-source chat models and intervene on
refusal-related features to assess their influence on generation, validating
their behavioral impact across multiple harmful datasets. This enables a
fine-grained inspection of how refusal manifests at the activation level and
addresses key research questions such as investigating upstream-downstream
latent relationship and understanding the mechanisms of adversarial
jailbreaking techniques. We also establish the usefulness of refusal features
in enhancing generalization for linear probes to out-of-distribution
adversarial samples in classification tasks. We open source our code in
https://github.com/wj210/refusal_sae.

</details>


### [89] [Evaluating AI capabilities in detecting conspiracy theories on YouTube](https://arxiv.org/abs/2505.23570)
*Leonardo La Rocca,Francesco Corso,Francesco Pierri*

Key words: YouTube, 阴谋论, 大语言模型, 多模态, RoBERTa

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 使用开源大语言模型（LLM）检测YouTube上的阴谋论内容，对比零样本性能与微调基线RoBERTa，发现文本模型召回率高但精度低，多模态模型表现不及纯文本模型。

Motivation: YouTube作为全球主流平台易滋生有害内容，研究旨在探索LLM在识别阴谋论视频中的实用性。

Method: 利用标注视频数据集，在零样本设置下评估多种LLM，并与微调RoBERTa基线对比。

Result: 文本LLM召回率高但假阳性多，多模态模型表现不佳；RoBERTa在未标注数据集上接近大参数LLM。

Conclusion: 当前LLM方法在有害内容检测中优劣并存，需更精确、鲁棒的系统。

Abstract: As a leading online platform with a vast global audience, YouTube's extensive
reach also makes it susceptible to hosting harmful content, including
disinformation and conspiracy theories. This study explores the use of
open-weight Large Language Models (LLMs), both text-only and multimodal, for
identifying conspiracy theory videos shared on YouTube. Leveraging a labeled
dataset of thousands of videos, we evaluate a variety of LLMs in a zero-shot
setting and compare their performance to a fine-tuned RoBERTa baseline. Results
show that text-based LLMs achieve high recall but lower precision, leading to
increased false positives. Multimodal models lag behind their text-only
counterparts, indicating limited benefits from visual data integration. To
assess real-world applicability, we evaluate the most accurate models on an
unlabeled dataset, finding that RoBERTa achieves performance close to LLMs with
a larger number of parameters. Our work highlights the strengths and
limitations of current LLM-based approaches for online harmful content
detection, emphasizing the need for more precise and robust systems.

</details>


### [90] [Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering](https://arxiv.org/abs/2505.23604)
*Guangtao Zeng,Maohao Shen,Delin Chen,Zhenting Qi,Subhro Das,Dan Gutfreund,David Cox,Gregory Wornell,Wei Lu,Zhang-Wei Hong,Chuang Gan*

Key words: 语言模型,测试时扩展,进化算法,强化学习,软件工程

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: EvoScale是一种样本高效的进化测试时扩展方法，通过迭代优化输出，使32B参数模型在SWE-Bench-Verified上性能媲美或超过100B参数模型。

Motivation: 尽管小型语言模型（<100B参数）在实际应用中计算成本更低，但其在解决真实软件工程问题（如GitHub问题）时表现不佳。现有方法依赖高成本的有监督微调或需大量采样的测试时扩展，亟需高效替代方案。

Method: 提出EvoScale：将生成过程视为进化，通过选择与变异迭代优化输出；同时使用强化学习训练模型自迭代改进，避免依赖外部验证器。

Result: 在SWE-Bench-Verified上，32B模型Satori-SWE-32B仅需少量样本即可达到或超越100B+参数模型的性能。

Conclusion: EvoScale显著提升小模型实用性，未来或将推动高效语言模型的广泛应用。

Abstract: Language models (LMs) perform well on standardized coding benchmarks but
struggle with real-world software engineering tasks such as resolving GitHub
issues in SWE-Bench, especially when model parameters are less than 100B. While
smaller models are preferable in practice due to their lower computational
cost, improving their performance remains challenging. Existing approaches
primarily rely on supervised fine-tuning (SFT) with high-quality data, which is
expensive to curate at scale. An alternative is test-time scaling: generating
multiple outputs, scoring them using a verifier, and selecting the best one.
Although effective, this strategy often requires excessive sampling and costly
scoring, limiting its practical application. We propose Evolutionary Test-Time
Scaling (EvoScale), a sample-efficient method that treats generation as an
evolutionary process. By iteratively refining outputs via selection and
mutation, EvoScale shifts the output distribution toward higher-scoring
regions, reducing the number of samples needed to find correct solutions. To
reduce the overhead from repeatedly sampling and selection, we train the model
to self-evolve using reinforcement learning (RL). Rather than relying on
external verifiers at inference time, the model learns to self-improve the
scores of its own generations across iterations. Evaluated on
SWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or
exceed the performance of models with over 100B parameters while using a few
samples. Code, data, and models will be fully open-sourced.

</details>


### [91] [Table-R1: Inference-Time Scaling for Table Reasoning](https://arxiv.org/abs/2505.23621)
*Zheyuan Yang,Lyuhao Chen,Arman Cohan,Yilun Zhao*

Key words: 推理时扩展、表格推理、蒸馏、强化学习、GRPO、泛化能力

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究提出了两种训练后策略（蒸馏和RLVR）来实现推理时扩展，在表格推理任务中表现优异，尤其Table-R1-Zero模型性能匹配或超越GPT-4.1和DeepSeek-R1，且仅使用7B参数的LLM。

Motivation: 探索推理时扩展在表格推理任务中的首次研究，提升模型性能并验证方法的有效性。

Method: 1. 蒸馏：利用Frontier模型生成的大规模推理轨迹数据集（DeepSeek-R1）微调LLMs，得到Table-R1-SFT模型；2. RLVR：提出可验证的奖励函数并应用GRPO算法训练Table-R1-Zero模型。

Result: Table-R1-Zero模型在表格推理任务（短问答、事实验证、自由问答）中表现优异，性能匹配或超越GPT-4.1和DeepSeek-R1，且仅需7B参数。模型还展现出强大的跨域泛化能力。

Conclusion: 指令微调、模型架构选择和跨任务泛化对提升表格推理能力至关重要，RL训练中涌现了关键推理技能。

Abstract: In this work, we present the first study to explore inference-time scaling on
table reasoning tasks. We develop and evaluate two post-training strategies to
enable inference-time scaling: distillation from frontier model reasoning
traces and reinforcement learning with verifiable rewards (RLVR). For
distillation, we introduce a large-scale dataset of reasoning traces generated
by DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For
RLVR, we propose task-specific verifiable reward functions and apply the GRPO
algorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series
models across diverse table reasoning tasks, including short-form QA, fact
verification, and free-form QA. Notably, the Table-R1-Zero model matches or
exceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a
7B-parameter LLM. It also demonstrates strong generalization to out-of-domain
datasets. Extensive ablation and qualitative analyses reveal the benefits of
instruction tuning, model architecture choices, and cross-task generalization,
as well as emergence of essential table reasoning skills during RL training.

</details>


### [92] [Characterizing the Expressivity of Transformer Language Models](https://arxiv.org/abs/2505.23623)
*Jiaoda Li,Ryan Cotterell*

Key words: Transformer, linear temporal logic, formal language theory, automata theory, expressivity

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究精确描述了固定精度、严格未来掩蔽和软注意力Transformer的表达能力，证明其等同于只包含过去算子的线性时序逻辑片段，进一步关联形式语言、自动机和代数理论，并通过实验验证了该理论框架。

Motivation: 理解Transformer语言模型的理论表达能力，填补理想化模型（如无限精度、硬注意力）与实际实现（有限精度、软注意力）之间的理论空白。

Method: 分析固定精度、严格未来掩蔽和软注意力的Transformer，将其表达能力映射为线性时序逻辑片段，并与形式语言理论、自动机理论和代数关联。

Result: 理论证明Transformer的表达能力等价于仅含过去算子的线性时序逻辑，实验显示模型在理论能力范围内能完美泛化，超出则失败。

Conclusion: 研究为Transformer的表达能力提供了统一的理论框架，验证了理论分析与实际表现的紧密关联。

Abstract: Transformer-based language models (LMs) have achieved widespread empirical
success, but their theoretical expressive power remains only partially
understood. Prior work often relies on idealized models with assumptions --
such as arbitrary numerical precision and hard attention -- that diverge from
real-world transformers. In this work, we provide an exact characterization of
fixed-precision transformers with strict future masking and soft attention, an
idealization that more closely mirrors practical implementations. We show that
these models are precisely as expressive as a specific fragment of linear
temporal logic that includes only a single temporal operator: the past
operator. We further relate this logic to established classes in formal
language theory, automata theory, and algebra, yielding a rich and unified
theoretical framework for understanding transformer expressivity. Finally, we
present empirical results that align closely with our theory: transformers
trained on languages within their theoretical capacity generalize perfectly
over lengths, while they consistently fail to generalize on languages beyond
it.

</details>


### [93] [AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora](https://arxiv.org/abs/2505.23628)
*Jiaxin Bai,Wei Fan,Qi Hu,Qing Zong,Chunyang Li,Hong Ting Tsang,Hongyu Luo,Yauwai Yim,Haoyu Huang,Xiao Zhou,Feng Qin,Tianshi Zheng,Xi Peng,Xin Yao,Huiwen Yang,Leijie Wu,Yi Ji,Gong Zhang,Renhai Chen,Yangqiu Song*

Key words: 知识图谱, 大语言模型, 模式诱导, 自动构建, ATLAS

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: AutoSchemaKG是一个无需预定义模式的自主知识图谱构建框架，利用大语言模型从文本中提取知识三元组并自动生成模式，构建了包含9亿节点和59亿边的ATLAS知识图谱，在多跳QA任务中优于现有基线，并显著提升了LLM的事实性。

Motivation: 传统的知识图谱构建依赖预定义模式，限制了灵活性和可扩展性。该研究旨在通过完全自动化的方式构建知识图谱，减少人工干预，同时提升知识图谱的覆盖率和准确性。

Method: 结合大语言模型，从文本中同时提取知识三元组和诱导模式，利用概念化技术将实例组织为语义类别，处理超过5000万文档。

Result: 构建的ATLAS知识图谱包含9亿节点和59亿边，在多跳QA任务中表现优于现有基线，模式诱导与人工构建模式的语义对齐率达95%。

Conclusion: 动态诱导的模式能有效补充大语言模型的参数知识，证明了十亿规模知识图谱的可行性和实用性。

Abstract: We present AutoSchemaKG, a framework for fully autonomous knowledge graph
construction that eliminates the need for predefined schemas. Our system
leverages large language models to simultaneously extract knowledge triples and
induce comprehensive schemas directly from text, modeling both entities and
events while employing conceptualization to organize instances into semantic
categories. Processing over 50 million documents, we construct ATLAS (Automated
Triple Linking And Schema induction), a family of knowledge graphs with 900+
million nodes and 5.9 billion edges. This approach outperforms state-of-the-art
baselines on multi-hop QA tasks and enhances LLM factuality. Notably, our
schema induction achieves 95\% semantic alignment with human-crafted schemas
with zero manual intervention, demonstrating that billion-scale knowledge
graphs with dynamically induced schemas can effectively complement parametric
knowledge in large language models.

</details>


### [94] [GeNRe: A French Gender-Neutral Rewriting System Using Collective Nouns](https://arxiv.org/abs/2505.23630)
*Enzo Doyen,Amalia Todirascu*

Key words: NLP, 性别偏见, 法语重写系统, 规则系统, 语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出GeNRe，首个法语性别中性重写系统，使用集体名词（法语中性别固定的词）来减少性别偏见，结合规则系统和微调的语言模型，并探索指令模型提升性能。

Motivation: NLP中的文本数据存在性别偏见，尤其是法语中的阳性别通用词，这加剧了性别刻板印象。法语缺乏中性化技术的研究，因此需要开发针对性系统。

Method: 采用规则系统（RBS）生成数据，并微调两个语言模型；同时探索基于指令的模型（如Claude 3 Opus）与字典结合的效果。

Result: Claude 3 Opus结合字典的结果接近RBS性能，表明指令模型可有效辅助系统。

Conclusion: GeNRe是法语性别中性改写的重要一步，推动了法语NLP中性别偏见缓解技术的发展。

Abstract: A significant portion of the textual data used in the field of Natural
Language Processing (NLP) exhibits gender biases, particularly due to the use
of masculine generics (masculine words that are supposed to refer to mixed
groups of men and women), which can perpetuate and amplify stereotypes. Gender
rewriting, an NLP task that involves automatically detecting and replacing
gendered forms with neutral or opposite forms (e.g., from masculine to
feminine), can be employed to mitigate these biases. While such systems have
been developed in a number of languages (English, Arabic, Portuguese, German,
French), automatic use of gender neutralization techniques (as opposed to
inclusive or gender-switching techniques) has only been studied for English.
This paper presents GeNRe, the very first French gender-neutral rewriting
system using collective nouns, which are gender-fixed in French. We introduce a
rule-based system (RBS) tailored for the French language alongside two
fine-tuned language models trained on data generated by our RBS. We also
explore the use of instruct-based models to enhance the performance of our
other systems and find that Claude 3 Opus combined with our dictionary achieves
results close to our RBS. Through this contribution, we hope to promote the
advancement of gender bias mitigation techniques in NLP for French.

</details>


### [95] [Are Reasoning Models More Prone to Hallucination?](https://arxiv.org/abs/2505.23646)
*Zijun Yao,Yantao Liu,Yanxu Chen,Jianhui Chen,Junfeng Fang,Lei Hou,Juanzi Li,Tat-Seng Chua*

Key words: 大型推理模型, 幻觉, 后训练, 行为分析, 模型不确定性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨大型推理模型（LRMs）在事实寻求任务中幻觉问题的成因与缓解方法，通过全面评估、行为分析和不确定性机制研究，发现冷启动SFT和可验证奖励RL可减少幻觉，而单纯蒸馏和无冷启动的RL则加重幻觉。

Motivation: 研究动机在于明确大型推理模型（LRM）在事实寻求任务中的幻觉问题，澄清其在推理能力泛化中的争议（如DeepSeek-R1性能提升与OpenAI-o3幻觉加剧的差异），并探索缓解方法。

Method: 方法包括三个部分：(1) 对LRMs的幻觉进行全面评估；(2) 分析不同后训练流程对幻觉的影响（如Flaw Repetition和Think-Answer Mismatch行为）；(3) 从模型不确定性角度研究幻觉机制。

Result: 结果表明：(1) 冷启动SFT和可验证奖励RL能有效减少幻觉；(2) 蒸馏和无冷启动的RL会加重幻觉；(3) 模型不确定性与事实准确性的错配是幻觉加剧的关键因素。

Conclusion: 结论指出，LRMs的幻觉问题可通过优化训练流程（如冷启动SFT）和模型不确定性校准缓解，为未来研究提供了理论基础和实践方向。

Abstract: Recently evolved large reasoning models (LRMs) show powerful performance in
solving complex tasks with long chain-of-thought (CoT) reasoning capability. As
these LRMs are mostly developed by post-training on formal reasoning tasks,
whether they generalize the reasoning capability to help reduce hallucination
in fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1
reports increased performance on SimpleQA, a fact-seeking benchmark, while
OpenAI-o3 observes even severer hallucination. This discrepancy naturally
raises the following research question: Are reasoning models more prone to
hallucination? This paper addresses the question from three perspectives. (1)
We first conduct a holistic evaluation for the hallucination in LRMs. Our
analysis reveals that LRMs undergo a full post-training pipeline with cold
start supervised fine-tuning (SFT) and verifiable reward RL generally alleviate
their hallucination. In contrast, both distillation alone and RL training
without cold start fine-tuning introduce more nuanced hallucinations. (2) To
explore why different post-training pipelines alters the impact on
hallucination in LRMs, we conduct behavior analysis. We characterize two
critical cognitive behaviors that directly affect the factuality of a LRM: Flaw
Repetition, where the surface-level reasoning attempts repeatedly follow the
same underlying flawed logic, and Think-Answer Mismatch, where the final answer
fails to faithfully match the previous CoT process. (3) Further, we investigate
the mechanism behind the hallucination of LRMs from the perspective of model
uncertainty. We find that increased hallucination of LRMs is usually associated
with the misalignment between model uncertainty and factual accuracy. Our work
provides an initial understanding of the hallucination in LRMs.

</details>


### [96] [ARC: Argument Representation and Coverage Analysis for Zero-Shot Long Document Summarization with Instruction Following LLMs](https://arxiv.org/abs/2505.23654)
*Mohamed Elaraby,Diane Litman*

Key words: abstractive summarization, argument roles, LLMs, ARC framework, legal documents, scientific articles

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了指令调优的大语言模型（LLMs）在摘要生成中是否充分保留关键论证角色，并提出了一种衡量框架ARC。实验表明，LLMs在摘要中对关键论证的覆盖存在不足，尤其是在论证分散分布时。

Motivation: 在抽象摘要任务中，结构化信息（尤其是论证角色）对高门槛领域（如法律）至关重要。本文旨在评估LLMs是否充分保留这些关键信息。

Method: 引入Argument Representation Coverage (ARC)框架，用于量化LLM生成的摘要对关键论证角色的覆盖程度。分析了三个开放权重LLM在长法律意见书和科学文献中的摘要表现。

Result: LLMs对关键论证角色的覆盖程度有限，尤其是当论证分散时。LLM上下文窗口的位置偏见和角色偏好会影响摘要中的论证覆盖。

Conclusion: 当前LLMs在生成摘要时对论证角色的覆盖不足，需开发更注重论证结构的摘要策略。

Abstract: Integrating structured information has long improved the quality of
abstractive summarization, particularly in retaining salient content. In this
work, we focus on a specific form of structure: argument roles, which are
crucial for summarizing documents in high-stakes domains such as law. We
investigate whether instruction-tuned large language models (LLMs) adequately
preserve this information. To this end, we introduce Argument Representation
Coverage (ARC), a framework for measuring how well LLM-generated summaries
capture salient arguments. Using ARC, we analyze summaries produced by three
open-weight LLMs in two domains where argument roles are central: long legal
opinions and scientific articles. Our results show that while LLMs cover
salient argument roles to some extent, critical information is often omitted in
generated summaries, particularly when arguments are sparsely distributed
throughout the input. Further, we use ARC to uncover behavioral patterns --
specifically, how the positional bias of LLM context windows and role-specific
preferences impact the coverage of key arguments in generated summaries,
emphasizing the need for more argument-aware summarization strategies.

</details>


### [97] [Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation](https://arxiv.org/abs/2505.23657)
*Hongxiang Zhang,Hao Chen,Tianyi Zhang,Muhao Chen*

Key words: 大型语言模型,解码方法,强化学习,幻觉,事实性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为ActLCD的新解码方法，通过强化学习策略优化生成的事实性，有效减少幻觉。

Motivation: 尽管现有解码方法改善了大型语言模型的事实性，但在长上下文环境中仍易产生幻觉。

Method: 使用强化学习策略（ActLCD），通过奖励感知分类器动态决定对比层的应用时机。

Result: ActLCD在五个基准测试中超越现有方法，显著减少幻觉现象。

Conclusion: ActLCD是一种高效的方法，能提升生成内容的事实性，尤其在长文本场景中表现优异。

Abstract: Recent decoding methods improve the factuality of large language
models~(LLMs) by refining how the next token is selected during generation.
These methods typically operate at the token level, leveraging internal
representations to suppress superficial patterns. Nevertheless, LLMs remain
prone to hallucinations, especially over longer contexts. In this paper, we
propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy
that actively decides when to apply contrasting layers during generation. By
casting decoding as a sequential decision-making problem, ActLCD employs a
reinforcement learning policy guided by a reward-aware classifier to optimize
factuality beyond the token level. Our experiments demonstrate that ActLCD
surpasses state-of-the-art methods across five benchmarks, showcasing its
effectiveness in mitigating hallucinations in diverse generation scenarios.

</details>


### [98] [ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic Long-Term Interactions](https://arxiv.org/abs/2505.23662)
*Beong-woo Kwak,Minju Kim,Dongha Lim,Hyungjoo Chae,Dongjin Kang,Sunghwan Kim,Dongil Yang,Jinyoung Yeo*

Key words: 大型语言模型, 工具使用, 长期交互, 基准测试, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了ToolHaystack基准测试，用于评估大型语言模型在长期交互中的工具使用能力，发现现有模型在长期鲁棒性上存在明显不足。

Motivation: 现有的评估方法主要关注短上下文中的工具使用，缺乏对长期交互中模型行为的深入理解。

Method: 提出ToolHaystack基准，包含多任务执行上下文和实际噪声的连续对话，用于测试模型维护上下文和处理干扰的能力。

Result: 测试了14个先进LLM，发现它们在标准多轮设置中表现良好，但在ToolHaystack中明显表现不佳。

Conclusion: 当前模型在长期鲁棒性上存在显著缺陷，这是以往工具基准测试未能揭示的。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in using
external tools to address user inquiries. However, most existing evaluations
assume tool use in short contexts, offering limited insight into model behavior
during realistic long-term interactions. To fill this gap, we introduce
ToolHaystack, a benchmark for testing the tool use capabilities in long-term
interactions. Each test instance in ToolHaystack includes multiple tasks
execution contexts and realistic noise within a continuous conversation,
enabling assessment of how well models maintain context and handle various
disruptions. By applying this benchmark to 14 state-of-the-art LLMs, we find
that while current models perform well in standard multi-turn settings, they
often significantly struggle in ToolHaystack, highlighting critical gaps in
their long-term robustness not revealed by previous tool benchmarks.

</details>


### [99] [LoLA: Low-Rank Linear Attention With Sparse Caching](https://arxiv.org/abs/2505.23666)
*Luke McDermott,Robert W. Heath Jr.,Rahul Parhi*

Key words: LoLA、线性注意力、稀疏缓存、长序列处理、Transformer

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了LoLA方法，通过结合低秩线性注意力和稀疏缓存来提升长序列处理的效率与准确性，解决了线性注意力在长上下文中的性能瓶颈。

Motivation: Transformer模型在处理长序列时的二次复杂度问题以及线性注意力方法在长上下文任务中的局限性。

Method: 提出了LoLA方法，结合低秩线性注意力和稀疏缓存机制，将过去的键值对分为三种内存形式（本地滑动窗口、稀疏全局缓存、线性注意力的递归隐藏状态）。

Result: LoLA在8K长上下文任务上表现出色，将基础模型的准确率从0.6%提升至97.4%（4K长度），并在零样本常识推理任务中表现优异。

Conclusion: LoLA是一种轻量级高效方法，显著提升了线性注意力模型在长上下文任务中的性能。

Abstract: Transformer-based large language models suffer from quadratic complexity at
inference on long sequences. Linear attention methods are efficient
alternatives, however, they fail to provide an accurate approximation of
softmax attention. By additionally incorporating sliding window attention into
each linear attention head, this gap can be closed for short context-length
tasks. Unfortunately, these approaches cannot recall important information from
long contexts due to "memory collisions". In this paper , we propose LoLA:
Low-rank Linear Attention with sparse caching. LoLA separately stores
additional key-value pairs that would otherwise interfere with past associative
memories. Moreover, LoLA further closes the gap between linear attention models
and transformers by distributing past key-value pairs into three forms of
memory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize
pairs in a sparse, global cache; and (iii) generic pairs in the recurrent
hidden state of linear attention. As an inference-only strategy, LoLA enables
pass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks
from RULER. It boosts the accuracy of the base subquadratic model from 0.6% to
97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1
8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning
tasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an
extremely lightweight approach: Nearly all of our results can be reproduced on
a single consumer GPU.

</details>


### [100] [Automatic classification of stop realisation with wav2vec2.0](https://arxiv.org/abs/2505.23688)
*James Tanner,Morgan Sonderegger,Jane Stuart-Smith,Jeff Mielke,Tyler Kendall*

Key words: 语音标注, 自监督学习, wav2vec2.0, 爆破音分类, 跨语言研究

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 使用预训练的wav2vec2.0模型高效自动标注语音数据中的爆破音存在，适用于英语和日语，并在不同语料库中表现稳健。

Motivation: 解决语音数据中可变音位现象自动标注工具的缺乏问题，探索预训练自监督模型在语音分类任务中的应用潜力。

Method: 利用wav2vec2.0模型训练自动分类器，针对英语和日语中的爆破音存在进行标注。

Result: 模型在不同语料库中均能高精度分类爆破音，且自动标注结果与人工标注的变异性模式高度一致。

Conclusion: 预训练语音模型可高效扩展语音数据的自动标注能力，为音位研究提供便捷工具。

Abstract: Modern phonetic research regularly makes use of automatic tools for the
annotation of speech data, however few tools exist for the annotation of many
variable phonetic phenomena. At the same time, pre-trained self-supervised
models, such as wav2vec2.0, have been shown to perform well at speech
classification tasks and latently encode fine-grained phonetic information. We
demonstrate that wav2vec2.0 models can be trained to automatically classify
stop burst presence with high accuracy in both English and Japanese, robust
across both finely-curated and unprepared speech corpora. Patterns of
variability in stop realisation are replicated with the automatic annotations,
and closely follow those of manual annotations. These results demonstrate the
potential of pre-trained speech models as tools for the automatic annotation
and processing of speech corpus data, enabling researchers to `scale-up' the
scope of phonetic research with relative ease.

</details>


### [101] [Child-Directed Language Does Not Consistently Boost Syntax Learning in Language Models](https://arxiv.org/abs/2505.23689)
*Francesca Padovani,Jaap Jumelet,Yevgen Matusevych,Arianna Bisazza*

Key words: 语言模型、儿童导向语言、句法能力、频率效应、FIT-CLAMS

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究对比了基于儿童导向语言（CDL）和维基百科训练的模型在两种目标、三种语言和三种句法基准上的表现，发现CDL并未普遍优于维基百科，并提出了新的测试方法FIT-CLAMS以控制频率效应。

Motivation: 验证Huebner等人（2021）关于CDL训练语言模型（LM）在句法能力上可能优于互联网数据的结论是否具有跨语言、模型类型和评估环境的普适性。

Method: 对比CDL与维基百科训练的模型，涵盖两种LM目标（masked和causal）、三种语言（英语、法语、德语）及三个句法最小对基准，并设计FIT-CLAMS方法控制频率效应。

Result: CDL在大多数情况下表现不及维基百科模型，且未显示出更强的句法泛化能力。频率控制对句法能力评估至关重要。

Conclusion: CDL并非普遍优于传统训练数据，频率效应需在句法评估中严格控制。

Abstract: Seminal work by Huebner et al. (2021) showed that language models (LMs)
trained on English Child-Directed Language (CDL) can reach similar syntactic
abilities as LMs trained on much larger amounts of adult-directed written text,
suggesting that CDL could provide more effective LM training material than the
commonly used internet-crawled data. However, the generalizability of these
results across languages, model types, and evaluation settings remains unclear.
We test this by comparing models trained on CDL vs. Wikipedia across two LM
objectives (masked and causal), three languages (English, French, German), and
three syntactic minimal-pair benchmarks. Our results on these benchmarks show
inconsistent benefits of CDL, which in most cases is outperformed by Wikipedia
models. We then identify various shortcomings in previous benchmarks, and
introduce a novel testing methodology, FIT-CLAMS, which uses a
frequency-controlled design to enable balanced comparisons across training
corpora. Through minimal pair evaluations and regression analysis we show that
training on CDL does not yield stronger generalizations for acquiring syntax
and highlight the importance of controlling for frequency effects when
evaluating syntactic ability.

</details>


### [102] [Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation](https://arxiv.org/abs/2505.23701)
*Ziling Cheng,Meng Cao,Leila Pishdad,Yanshuai Cao,Jackie Chi Kit Cheung*

Key words: 大语言模型, 数学推理, 分解评估, Chain-of-Thought, 抽象建模, 算术计算

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究表明，传统基于最终答案的指标在评估大语言模型（LLM）数学能力时存在误导，算术计算（而非抽象建模）是瓶颈。此外，Chain-of-Thought（CoT）主要辅助计算而非抽象建模，且这两种能力在单次前向传播中通过“先抽象后计算”机制实现。

Motivation: 传统评估LLM数学能力的指标（如最终答案准确率）无法区分抽象建模和算术计算的能力，可能导致对模型推理能力的错误评估。本文旨在分解这两种子技能，探究CoT的实际作用及模型内部机制。

Method: 通过GSM8K和SVAMP数据集对Llama-3和Qwen2.5（1B-32B）进行分解评估，分析CoT的影响，并利用因果修补验证抽象建模的存在性、可迁移性和组合性。

Result: 发现模型最终答案的瓶颈主要在算术计算而非抽象建模；CoT主要提升计算能力；抽象建模和计算在单次前向传播中按“先抽象后计算”的机制联合运作。

Conclusion: 需采用分解评估方法以更准确衡量LLM推理能力，并为未来改进（如针对性优化计算或抽象能力）提供方向。

Abstract: Final-answer-based metrics are commonly used for evaluating large language
models (LLMs) on math word problems, often taken as proxies for reasoning
ability. However, such metrics conflate two distinct sub-skills: abstract
formulation (capturing mathematical relationships using expressions) and
arithmetic computation (executing the calculations). Through a disentangled
evaluation on GSM8K and SVAMP, we find that the final-answer accuracy of
Llama-3 and Qwen2.5 (1B-32B) without CoT is overwhelmingly bottlenecked by the
arithmetic computation step and not by the abstract formulation step. Contrary
to the common belief, we show that CoT primarily aids in computation, with
limited impact on abstract formulation. Mechanistically, we show that these two
skills are composed conjunctively even in a single forward pass without any
reasoning steps via an abstract-then-compute mechanism: models first capture
problem abstractions, then handle computation. Causal patching confirms these
abstractions are present, transferable, composable, and precede computation.
These behavioural and mechanistic findings highlight the need for disentangled
evaluation to accurately assess LLM reasoning and to guide future improvements.

</details>


### [103] [SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models](https://arxiv.org/abs/2505.23713)
*Zixiang Xu,Yanbo Wang,Yue Huang,Jiayi Ye,Haomin Zhuang,Zirui Song,Lang Gao,Chenxi Wang,Zhaorun Chen,Yujun Zhou,Sixian Li,Wang Pan,Yue Zhao,Jieyu Zhao,Xiangliang Zhang,Xiuying Chen*

Key words: Large Language Models, Social Reasoning, Benchmark, SocialMaze, Fine-tuning

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了SocialMaze基准，用于全面评估大型语言模型（LLMs）的社交推理能力，填补了现有评估标准的不足，并揭示了模型在动态交互和信息不确定性下的表现差异。

Motivation: 由于LLMs在社交任务中的应用日益广泛，但缺乏系统评估其社交推理能力的框架，作者设计了SocialMaze基准来解决这一问题。

Method: SocialMaze包含三类核心挑战（深度推理、动态交互和信息不确定性），并在三种关键场景（社交推理游戏、日常生活互动和数字社区平台）中设计了六项任务。采用了自动化和人工验证确保数据质量。

Result: 研究发现，模型在动态交互和整合时序信息方面表现差异显著；具备强链式思维推理能力的模型在需深度推断的任务中表现更优；而在不确定性下，模型推理能力显著下降。通过针对性微调，模型在复杂社交场景中的表现得到显著提升。

Conclusion: SocialMaze为评估LLMs的社交推理能力提供了系统框架，揭示了现有模型的局限性，并展示了通过微调改进的可能性。

Abstract: Large language models (LLMs) are increasingly applied to socially grounded
tasks, such as online community moderation, media content analysis, and social
reasoning games. Success in these contexts depends on a model's social
reasoning ability - the capacity to interpret social contexts, infer others'
mental states, and assess the truthfulness of presented information. However,
there is currently no systematic evaluation framework that comprehensively
assesses the social reasoning capabilities of LLMs. Existing efforts often
oversimplify real-world scenarios and consist of tasks that are too basic to
challenge advanced models. To address this gap, we introduce SocialMaze, a new
benchmark specifically designed to evaluate social reasoning. SocialMaze
systematically incorporates three core challenges: deep reasoning, dynamic
interaction, and information uncertainty. It provides six diverse tasks across
three key settings: social reasoning games, daily-life interactions, and
digital community platforms. Both automated and human validation are used to
ensure data quality. Our evaluation reveals several key insights: models vary
substantially in their ability to handle dynamic interactions and integrate
temporally evolving information; models with strong chain-of-thought reasoning
perform better on tasks requiring deeper inference beyond surface-level cues;
and model reasoning degrades significantly under uncertainty. Furthermore, we
show that targeted fine-tuning on curated reasoning examples can greatly
improve model performance in complex social scenarios. The dataset is publicly
available at: https://huggingface.co/datasets/MBZUAI/SocialMaze

</details>


### [104] [SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods](https://arxiv.org/abs/2505.23714)
*Roksana Goworek,Harpal Karlcut,Muhammad Shezad,Nijaguna Darshana,Abhishek Mane,Syam Bondada,Raghav Sikka,Ulvi Mammadov,Rauf Allahverdiyev,Sriram Purighella,Paridhi Gupta,Muhinyia Ndegwa,Haim Dubossarsky*

Key words: 低资源语言, 跨语言迁移, 多义词消歧, 数据集标注, WiC实验

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出针对低资源语言的高质量评估数据集，支持跨语言迁移研究，并发布了一个包含多义词的标注数据集，覆盖九种低资源语言，通过半自动标注方法提升效率，实验结果强调了针对性数据集对多义词消歧的重要性。

Motivation: 解决低资源语言在跨语言迁移研究中缺乏高质量评估数据集的问题，以支持更公平、健壮的多语言NLP研究。

Method: 采用半自动标注方法创建包含多义词的句子数据集，覆盖九种低资源语言的多样语系和文字，并通过WiC格式实验评估跨语言迁移效果。

Result: 实验结果证明了针对性数据集在多义词消歧和跨语言迁移中的有效性，强调了高质量数据集的重要性。

Conclusion: 论文发布的标注数据集和代码有助于推动低资源语言的多义词消歧研究和跨语言迁移技术的发展。

Abstract: This paper addresses the critical need for high-quality evaluation datasets
in low-resource languages to advance cross-lingual transfer. While
cross-lingual transfer offers a key strategy for leveraging multilingual
pretraining to expand language technologies to understudied and typologically
diverse languages, its effectiveness is dependent on quality and suitable
benchmarks. We release new sense-annotated datasets of sentences containing
polysemous words, spanning nine low-resource languages across diverse language
families and scripts. To facilitate dataset creation, the paper presents a
demonstrably beneficial semi-automatic annotation method. The utility of the
datasets is demonstrated through Word-in-Context (WiC) formatted experiments
that evaluate transfer on these low-resource languages. Results highlight the
importance of targeted dataset creation and evaluation for effective polysemy
disambiguation in low-resource settings and transfer studies. The released
datasets and code aim to support further research into fair, robust, and truly
multilingual NLP.

</details>


### [105] [Don't Take the Premise for Granted: Evaluating the Premise Critique Ability of Large Language Models](https://arxiv.org/abs/2505.23715)
*Jinzhe Li,Gengxu Li,Yi Chang,Yuan Wu*

Key words: Large Language Models, Premise Critique, PCBench, Reasoning, Reliability

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: PCBench评估了15种大语言模型（LLM）在识别和批评错误前提时的性能，发现大多数模型依赖显式提示，且能力受问题难度和错误类型影响，推理能力与批评能力不一致。

Motivation: LLM在存在缺陷或矛盾前提时表现不可靠，研究提出前提批评能力（Premise Critique Ability）以提升模型可靠性。

Method: 开发了PCBench基准，包含四种错误类型和三种难度级别，通过多维度指标评估15种LLM。

Result: 1) 模型依赖显式提示；2) 批评能力与难度和错误类型相关；3) 批评与推理能力无强关联；4) 错误前提导致模型过度思考，答案变长。

Conclusion: 增强LLM对输入有效性的主动评估是构建可靠系统的关键，前提批评能力需作为基础能力。

Abstract: Large language models (LLMs) have witnessed rapid advancements, demonstrating
remarkable capabilities. However, a notable vulnerability persists: LLMs often
uncritically accept flawed or contradictory premises, leading to inefficient
reasoning and unreliable outputs. This emphasizes the significance of
possessing the \textbf{Premise Critique Ability} for LLMs, defined as the
capacity to proactively identify and articulate errors in input premises. Most
existing studies assess LLMs' reasoning ability in ideal settings, largely
ignoring their vulnerabilities when faced with flawed premises. Thus, we
introduce the \textbf{Premise Critique Bench (PCBench)}, designed by
incorporating four error types across three difficulty levels, paired with
multi-faceted evaluation metrics. We conducted systematic evaluations of 15
representative LLMs. Our findings reveal: (1) Most models rely heavily on
explicit prompts to detect errors, with limited autonomous critique; (2)
Premise critique ability depends on question difficulty and error type, with
direct contradictions being easier to detect than complex or procedural errors;
(3) Reasoning ability does not consistently correlate with the premise critique
ability; (4) Flawed premises trigger overthinking in reasoning models, markedly
lengthening responses due to repeated attempts at resolving conflicts. These
insights underscore the urgent need to enhance LLMs' proactive evaluation of
input validity, positioning premise critique as a foundational capability for
developing reliable, human-centric systems. The code is available at
https://github.com/MLGroupJLU/Premise_Critique.

</details>


### [106] [Label-Guided In-Context Learning for Named Entity Recognition](https://arxiv.org/abs/2505.23722)
*Fan Bai,Hamid Hassanzadeh,Ardavan Saeedi,Mark Dredze*

Key words: in-context learning, named entity recognition, token-level statistics, label-guided retrieval

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DEER方法通过利用训练标签的token级统计来改进上下文学习（ICL）在命名实体识别（NER）中的性能，其表现优于现有ICL方法并接近监督微调的效果。

Motivation: 现有方法在选择演示示例时仅考虑语义相似性而忽略训练标签，导致ICL在NER任务中表现不佳。

Method: DEER通过标签引导的token检索器优化示例选择，并提示LLM重新审视易错token进行针对性修正。

Result: 在五个NER数据集和四个LLM上的实验表明，DEER性能优于现有ICL方法，且对未见实体和低资源场景表现稳健。

Conclusion: DEER显著提升ICL在NER任务中的性能，接近监督微调水平，展示了标签信息的重要性。

Abstract: In-context learning (ICL) enables large language models (LLMs) to perform new
tasks using only a few demonstrations. In Named Entity Recognition (NER),
demonstrations are typically selected based on semantic similarity to the test
instance, ignoring training labels and resulting in suboptimal performance. We
introduce DEER, a new method that leverages training labels through token-level
statistics to improve ICL performance. DEER first enhances example selection
with a label-guided, token-based retriever that prioritizes tokens most
informative for entity recognition. It then prompts the LLM to revisit
error-prone tokens, which are also identified using label statistics, and make
targeted corrections. Evaluated on five NER datasets using four different LLMs,
DEER consistently outperforms existing ICL methods and approaches the
performance of supervised fine-tuning. Further analysis shows its effectiveness
on both seen and unseen entities and its robustness in low-resource settings.

</details>


### [107] [ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2505.23723)
*Zexi Liu,Jingyi Chai,Xinyu Zhu,Shuo Tang,Rui Ye,Bo Zhang,Lei Bai,Siheng Chen*

Key words: 大语言模型, 代理训练, 强化学习, 机器学习工程

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于强化学习的LLM代理框架，通过交互式实验优化ML任务表现，显著提升了小规模模型在ML任务上的性能。

Motivation: 现有基于LLM的代理方法依赖手动提示工程，缺乏从多样化实验经验中学习和优化的能力，因此探索学习驱动的代理ML范式成为研究重点。

Method: 提出代理ML训练框架，包含探索增强微调、逐步RL和特定奖励模块三部分，通过Qwen-2.5基础模型训练ML-Agent。

Result: 仅训练9个ML任务后，7B规模的ML-Agent超越671B规模的DeepSeek-R1，并展现出持续改进和跨任务泛化能力。

Conclusion: 学习驱动的代理ML框架有效提升了LLM代理在ML任务中的自主性和性能。

Abstract: The emergence of large language model (LLM)-based agents has significantly
advanced the development of autonomous machine learning (ML) engineering.
However, most existing approaches rely heavily on manual prompt engineering,
failing to adapt and optimize based on diverse experimental experiences.
Focusing on this, for the first time, we explore the paradigm of learning-based
agentic ML, where an LLM agent learns through interactive experimentation on ML
tasks using online reinforcement learning (RL). To realize this, we propose a
novel agentic ML training framework with three key components: (1)
exploration-enriched fine-tuning, which enables LLM agents to generate diverse
actions for enhanced RL exploration; (2) step-wise RL, which enables training
on a single action step, accelerating experience collection and improving
training efficiency; (3) an agentic ML-specific reward module, which unifies
varied ML feedback signals into consistent rewards for RL optimization.
Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM
for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our
7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it
achieves continuous performance improvements and demonstrates exceptional
cross-task generalization capabilities.

</details>


### [108] [Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time](https://arxiv.org/abs/2505.23729)
*Mohamad Chehade,Soumya Suvra Ghosal,Souradip Chakraborty,Avinash Reddy,Dinesh Manocha,Hao Zhu,Amrit Singh Bedi*

Key words: 大型语言模型, 满意度对齐, 多目标优化, 有限理性, 阈值约束

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了SITAlign框架，通过满足次要标准的阈值约束来优化主要目标，解决了大型语言模型与人类偏好对齐的复杂性，并在实验中表现出色。

Motivation: 现有方法通常忽视了人类决策的实际方式，尤其是有限理性的决策策略。论文旨在通过满足次要标准的阈值约束来优化主要目标，以更贴近人类决策的满意度对齐。

Method: 论文提出了SITAlign框架，通过最大化主要目标函数的同时满足次要标准的阈值约束，实现多目标对齐。

Result: 在PKU-SafeRLHF数据集上，SITAlign在保持无害性阈值的同时，在帮助性奖励方面比现有多目标解码策略提高了22.3%的GPT-4胜平率。

Conclusion: SITAlign框架有效地解决了多目标对齐问题，理论分析和实验验证均证明了其优越性。

Abstract: Aligning large language models with humans is challenging due to the
inherently multifaceted nature of preference feedback. While existing
approaches typically frame this as a multi-objective optimization problem, they
often overlook how humans actually make decisions. Research on bounded
rationality suggests that human decision making follows satisficing
strategies-optimizing primary objectives while ensuring others meet acceptable
thresholds. To bridge this gap and operationalize the notion of satisficing
alignment, we propose SITAlign: an inference time framework that addresses the
multifaceted nature of alignment by maximizing a primary objective while
satisfying threshold-based constraints on secondary criteria. We provide
theoretical insights by deriving sub-optimality bounds of our satisficing based
inference alignment approach. We empirically validate SITAlign's performance
through extensive experimentation on multiple benchmarks. For instance, on the
PKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while
ensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art
multi objective decoding strategy by a margin of 22.3% in terms of GPT-4
win-tie rate for helpfulness reward while adhering to the threshold on
harmlessness.

</details>


### [109] [ATLAS: Learning to Optimally Memorize the Context at Test Time](https://arxiv.org/abs/2505.23735)
*Ali Behrouz,Zeman Li,Praneeth Kacham,Majid Daliri,Yuan Deng,Peilin Zhong,Meisam Razaviyayn,Vahab Mirrokni*

Key words: Transformers, 长期记忆, 循环神经网络, ATLAS, DeepTransformers

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了ATLAS，一个高容量的长期记忆模块，旨在解决现代循环神经网络在长上下文理解和序列外推任务中的局限性，通过优化内存设计和更新机制，显著提升了性能。

Motivation: Transformers在序列建模中表现优异，但其二次复杂度限制了长序列应用。现代循环神经网络虽有改进，但在长上下文理解和外推任务中仍有不足，这主要源于其内存容量、在线更新机制和内存管理的局限性。

Method: 提出了ATLAS，一个高容量的长期记忆模块，通过优化内存设计和基于当前及过去令牌的更新机制，解决了现有模型的局限性。并在此基础上提出了DeepTransformers架构。

Result: 在语言建模、常识推理、召回密集和长上下文理解任务中，ATLAS超越了Transformers和现有线性循环模型的性能，尤其在长上下文任务中表现显著提升。

Conclusion: ATLAS通过改进内存设计和更新机制，显著提升了长上下文理解和序列外推能力，为序列建模提供了新的解决方案。

Abstract: Transformers have been established as the most popular backbones in sequence
modeling, mainly due to their effectiveness in in-context retrieval tasks and
the ability to learn at scale. Their quadratic memory and time complexity,
however, bound their applicability in longer sequences and so has motivated
researchers to explore effective alternative architectures such as modern
recurrent neural networks (a.k.a long-term recurrent memory module). Despite
their recent success in diverse downstream tasks, they struggle in tasks that
requires long context understanding and extrapolation to longer sequences. We
observe that these shortcomings come from three disjoint aspects in their
design: (1) limited memory capacity that is bounded by the architecture of
memory and feature mapping of the input; (2) online nature of update, i.e.,
optimizing the memory only with respect to the last input; and (3) less
expressive management of their fixed-size memory. To enhance all these three
aspects, we present ATLAS, a long-term memory module with high capacity that
learns to memorize the context by optimizing the memory based on the current
and past tokens, overcoming the online nature of long-term memory models.
Building on this insight, we present a new family of Transformer-like
architectures, called DeepTransformers, that are strict generalizations of the
original Transformer architecture. Our experimental results on language
modeling, common-sense reasoning, recall-intensive, and long-context
understanding tasks show that ATLAS surpasses the performance of Transformers
and recent linear recurrent models. ATLAS further improves the long context
performance of Titans, achieving +80\% accuracy in 10M context length of
BABILong benchmark.

</details>


### [110] [DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning](https://arxiv.org/abs/2505.23754)
*Ziyin Zhang,Jiahao Xu,Zhiwei He,Tian Liang,Qiuzhi Liu,Yansi Li,Linfeng Song,Zhengwen Liang,Zhuosheng Zhang,Rui Wang,Zhaopeng Tu,Haitao Mi,Dong Yu*

Key words: theorem proving, large language models, reinforcement learning, DeepTheorem, natural language

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DeepTheorem is a framework enhancing LLM theorem-proving using natural language, featuring a large dataset and RL-Zero reinforcement learning, achieving state-of-the-art results.

Motivation: Traditional ATP methods poorly align with LLMs' natural language strengths, necessitating an informal theorem-proving framework.

Method: DeepTheorem includes a 121K theorem dataset, RL-Zero reinforcement learning, and comprehensive evaluation metrics.

Result: DeepTheorem significantly improves LLM theorem-proving performance, achieving top accuracy and reasoning quality.

Conclusion: DeepTheorem advances automated informal theorem proving and mathematical exploration.

Abstract: Theorem proving serves as a major testbed for evaluating complex reasoning
abilities in large language models (LLMs). However, traditional automated
theorem proving (ATP) approaches rely heavily on formal proof systems that
poorly align with LLMs' strength derived from informal, natural language
knowledge acquired during pre-training. In this work, we propose DeepTheorem, a
comprehensive informal theorem-proving framework exploiting natural language to
enhance LLM mathematical reasoning. DeepTheorem includes a large-scale
benchmark dataset consisting of 121K high-quality IMO-level informal theorems
and proofs spanning diverse mathematical domains, rigorously annotated for
correctness, difficulty, and topic categories, accompanied by systematically
constructed verifiable theorem variants. We devise a novel reinforcement
learning strategy (RL-Zero) explicitly tailored to informal theorem proving,
leveraging the verified theorem variants to incentivize robust mathematical
inference. Additionally, we propose comprehensive outcome and process
evaluation metrics examining proof correctness and the quality of reasoning
steps. Extensive experimental analyses demonstrate DeepTheorem significantly
improves LLM theorem-proving performance compared to existing datasets and
supervised fine-tuning protocols, achieving state-of-the-art accuracy and
reasoning quality. Our findings highlight DeepTheorem's potential to
fundamentally advance automated informal theorem proving and mathematical
exploration.

</details>


### [111] [Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint](https://arxiv.org/abs/2505.23759)
*Heekyung Lee,Jiaxin Ge,Tsung-Han Wu,Minwoo Kang,Trevor Darrell,David M. Chan*

Key words: 视觉语言模型, 谜语, 多模态推理, 视觉隐喻, 符号推理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了现有视觉语言模型（VLMs）在解决谜语（Rebus puzzles）上的能力，发现虽然模型能解码简单的视觉线索，但在抽象推理和视觉隐喻理解上表现不佳。

Motivation: 研究动机在于评估VLMs是否能理解复杂的语言与视觉结合的谜题，这些谜题需要多模态抽象、符号推理及文化和语言双关的掌握。

Method: 通过构建一个手工生成和标注的多样化谜题基准，测试不同VLMs的表现。

Result: 结果显示，VLMs能解码简单视觉线索，但在抽象推理、横向思维和视觉隐喻理解方面表现不佳。

Conclusion: 现代VLMs在复杂多模态任务（如谜语解决）中仍存在明显局限，需要进一步提升抽象推理和跨模态理解能力。

Abstract: Rebus puzzles, visual riddles that encode language through imagery, spatial
arrangement, and symbolic substitution, pose a unique challenge to current
vision-language models (VLMs). Unlike traditional image captioning or question
answering tasks, rebus solving requires multi-modal abstraction, symbolic
reasoning, and a grasp of cultural, phonetic and linguistic puns. In this
paper, we investigate the capacity of contemporary VLMs to interpret and solve
rebus puzzles by constructing a hand-generated and annotated benchmark of
diverse English-language rebus puzzles, ranging from simple pictographic
substitutions to spatially-dependent cues ("head" over "heels"). We analyze how
different VLMs perform, and our findings reveal that while VLMs exhibit some
surprising capabilities in decoding simple visual clues, they struggle
significantly with tasks requiring abstract reasoning, lateral thinking, and
understanding visual metaphors.

</details>


### [112] [From Chat Logs to Collective Insights: Aggregative Question Answering](https://arxiv.org/abs/2505.23765)
*Wentao Zhang,Woojeong Kim,Yuntian Deng*

Key words: 聚合问答、大型语言模型、对话数据、WildChat-AQA基准

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文提出了聚合问答（Aggregative Question Answering）任务，旨在通过大规模聊天数据回答聚合性问题，并构建了WildChat-AQA基准。现有方法在效果或计算成本上表现不佳，凸显了新方法的必要性。

Motivation: 现有的对话数据处理方法通常将交互视为独立事件，忽略了从大规模对话日志中聚合和推理的关键洞察。这篇论文旨在填补这一空白。

Method: 论文提出了聚合问答任务，并构建了WildChat-AQA基准，包含6,027个聚合性问题，源自182,330条真实聊天数据。

Result: 实验表明，现有方法要么推理效果不佳，要么计算成本过高。

Conclusion: 论文呼吁开发能够从大规模对话数据中提取集体洞察的新方法。

Abstract: Conversational agents powered by large language models (LLMs) are rapidly
becoming integral to our daily interactions, generating unprecedented amounts
of conversational data. Such datasets offer a powerful lens into societal
interests, trending topics, and collective concerns. Yet, existing approaches
typically treat these interactions as independent and miss critical insights
that could emerge from aggregating and reasoning across large-scale
conversation logs. In this paper, we introduce Aggregative Question Answering,
a novel task requiring models to reason explicitly over thousands of
user-chatbot interactions to answer aggregative queries, such as identifying
emerging concerns among specific demographics. To enable research in this
direction, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative
questions derived from 182,330 real-world chatbot conversations. Experiments
show that existing methods either struggle to reason effectively or incur
prohibitive computational costs, underscoring the need for new approaches
capable of extracting collective insights from large-scale conversational data.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [113] [Localized Weather Prediction Using Kolmogorov-Arnold Network-Based Models and Deep RNNs](https://arxiv.org/abs/2505.22686)
*Ange-Clement Akazan,Verlon Roel Mbingui,Gnankan Landry Regis N'guessan,Issa Karambal*

Key words: 气象预测, 热带非洲, 深度学习, LSTM, GRU, KAN, TKAN

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文对比了多种深度循环神经网络和Kolmogorov-Arnold模型在热带非洲城市（阿比让和基加利）的气象预测中的表现，并提出了两种改进的TKAN模型变体。结果表明，KAN在温度预测上表现优异，而TKAN变体在低降雨量区域的降水预测中表现最佳。传统RNN在气压预测上仍具有竞争力。

Motivation: 热带非洲的极端天气事件严重影响当地生活和经济发展，但现有预测方法难以应对其非线性的复杂天气模式，因此需要更高效的预测模型。

Method: 研究使用LSTM、GRU、BiLSTM、BiGRU、KAN和TKAN等模型，并引入两种TKAN变体（替换激活函数为GeLU和MiSH），基于2010-2024年的气象数据，评估其在温度、降水和气压预测中的表现。

Result: KAN在温度预测中表现最佳（R²高达0.9998），TKAN变体在低降雨量区域的降水预测中误差最小。传统RNN在气压预测上优于KAN类模型（R²≈0.83-0.86）。

Conclusion: 基于样条的神经网络架构（如KAN和TKAN）在气象预测中展现出高效性，特别是在数据有限的情况下。未来可进一步探索其应用潜力。

Abstract: Weather forecasting is crucial for managing risks and economic planning,
particularly in tropical Africa, where extreme events severely impact
livelihoods. Yet, existing forecasting methods often struggle with the region's
complex, non-linear weather patterns. This study benchmarks deep recurrent
neural networks such as $\texttt{LSTM, GRU, BiLSTM, BiGRU}$, and
Kolmogorov-Arnold-based models $(\texttt{KAN} and \texttt{TKAN})$ for daily
forecasting of temperature, precipitation, and pressure in two tropical cities:
Abidjan, Cote d'Ivoire (Ivory Coast) and Kigali (Rwanda). We further introduce
two customized variants of $ \texttt{TKAN}$ that replace its original
$\texttt{SiLU}$ activation function with $ \texttt{GeLU}$ and \texttt{MiSH},
respectively. Using station-level meteorological data spanning from 2010 to
2024, we evaluate all the models on standard regression metrics. $\texttt{KAN}$
achieves temperature prediction ($R^2=0.9986$ in Abidjan, $0.9998$ in Kigali,
$\texttt{MSE} < 0.0014~^\circ C ^2$), while $\texttt{TKAN}$ variants minimize
absolute errors for precipitation forecasting in low-rainfall regimes. The
customized $\texttt{TKAN}$ models demonstrate improvements over the standard
$\texttt{TKAN}$ across both datasets. Classical \texttt{RNNs} remain highly
competitive for atmospheric pressure ($R^2 \approx 0.83{-}0.86$), outperforming
$\texttt{KAN}$-based models in this task. These results highlight the potential
of spline-based neural architectures for efficient and data-efficient
forecasting.

</details>


### [114] [SlimLLM: Accurate Structured Pruning for Large Language Models](https://arxiv.org/abs/2505.22689)
*Jialong Guo,Xinghao Chen,Yehui Tang,Yunhe Wang*

Key words: 大语言模型, 结构化剪枝, SlimLLM, 通道剪枝, 注意力头剪枝

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: SlimLLM是一种针对大语言模型的结构化剪枝方法，通过整体评估通道或注意力头的重要性，结合线性回归策略和分层重要性比例，显著减少了计算成本并保持性能。

Motivation: 大语言模型的计算成本高昂，限制了其部署和应用。结构化剪枝是减少参数的有效方法，但需解决子模块重要性评估和性能损失最小化的关键问题。

Method: 提出SlimLLM方法：1) 整体评估通道或注意力头的重要性；2) 使用线性回归策略快速恢复性能；3) 提出分层重要性比例确定剪枝比例。

Result: 在LLaMA基准测试中，SlimLLM优于其他方法，达到了最先进的性能。

Conclusion: SlimLLM通过更全面的重要性评估和高效的性能恢复策略，为大语言模型的高效部署提供了可行的解决方案。

Abstract: Large language models(LLMs) have garnered significant attention and
demonstrated impressive capabilities in a wide range of applications. However,
due to their enormous computational costs, the deployment and application of
LLMs are often severely limited. To address this issue, structured pruning is
an effective solution to compress the parameters of LLMs. Determining the
importance of each sub-module in LLMs and minimizing performance loss are
critical issues that need to be carefully addressed in structured pruning. In
this paper, we propose an effective and fast structured pruning method named
SlimLLM for large language models. For channel and attention head pruning, we
evaluate the importance based on the entire channel or head, rather than merely
aggregating the importance of individual elements within a sub-module. This
approach enables a more holistic consideration of the interdependence among
elements within the sub-module. In addition, we design a simple linear
regression strategy for the output matrix to quickly recover performance. We
also propose layer-based importance ratio to determine the pruning ratio for
each layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other
methods and achieves state-of-the-art performance.

</details>


### [115] [MoRE: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning](https://arxiv.org/abs/2505.22694)
*Dacao Zhang,Kun Zhang,Shimao Chu,Le Wu,Xin Li,Si Wei*

Key words: 参数高效微调（PEFT）、大语言模型（LLM）、低秩自适应（LoRA）、多任务学习、MoRE

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为MoRE的新型参数高效微调方法，通过混合不同秩的LoRA专家来提升多任务场景下的适应性和效率，实验证明其性能显著优于传统LoRA方法。

Motivation: 现有的PEFT方法（如LoRA）在多任务场景中效率不高，通常需要为每个任务单独训练模块。本文旨在解决这一问题，提出更高效的多任务参数微调方法。

Method: 提出MoRE方法，将不同秩的LoRA模块（称为低秩专家）与不同任务对齐，并设计自适应秩选择器为任务选择合适专家。

Result: 实验表明，MoRE在多任务基准测试中显著优于传统LoRA及其变体，且不增加推理成本。

Conclusion: MoRE通过联合训练低秩专家，有效提升了多任务场景下的模型性能，同时保持了参数高效性。

Abstract: With the rapid development of Large Language Models (LLMs),
Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant
attention, which aims to achieve efficient fine-tuning of LLMs with fewer
parameters. As a representative PEFT method, Low-Rank Adaptation (LoRA)
introduces low-rank matrices to approximate the incremental tuning parameters
and achieves impressive performance over multiple scenarios. After that, plenty
of improvements have been proposed for further improvement. However, these
methods either focus on single-task scenarios or separately train multiple LoRA
modules for multi-task scenarios, limiting the efficiency and effectiveness of
LoRA in multi-task scenarios. To better adapt to multi-task fine-tuning, in
this paper, we propose a novel Mixture of Low-Rank Experts (MoRE) for
multi-task PEFT. Specifically, instead of using an individual LoRA for each
task, we align different ranks of LoRA module with different tasks, which we
named low-rank experts. Moreover, we design a novel adaptive rank selector to
select the appropriate expert for each task. By jointly training low-rank
experts, MoRE can enhance the adaptability and efficiency of LoRA in multi-task
scenarios. Finally, we conduct extensive experiments over multiple multi-task
benchmarks along with different LLMs to verify model performance. Experimental
results demonstrate that compared to traditional LoRA and its variants, MoRE
significantly improves the performance of LLMs in multi-task scenarios and
incurs no additional inference cost. We also release the model and code to
facilitate the community.

</details>


### [116] [LLM-ODDR: A Large Language Model Framework for Joint Order Dispatching and Driver Repositioning](https://arxiv.org/abs/2505.22695)
*Tengfei Lyu,Siyuan Feng,Hao Liu,Hai Yang*

Key words: 叫车平台，订单调度，司机重新定位，大型语言模型，公平性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出LLM-ODDR框架，利用大型语言模型优化叫车平台的订单调度和司机重新定位问题，平衡收益与公平性，并在实验中表现出色。

Motivation: 传统方法在叫车平台的订单调度和司机重新定位中忽视司机收入公平性和决策可解释性，需要更有效的动态优化方案。

Method: LLM-ODDR框架包含多目标订单价值评估、公平性调度和时空需求驱动的司机重新定位，并开发了专用模型JointDR-GPT。

Result: 在曼哈顿出租车数据上的实验显示，该方法在效率、异常适应性和可解释性上显著优于传统方法。

Conclusion: 这是首次将LLM应用于叫车ODDR任务，为智能交通系统中的语言模型集成提供了基础。

Abstract: Ride-hailing platforms face significant challenges in optimizing order
dispatching and driver repositioning operations in dynamic urban environments.
Traditional approaches based on combinatorial optimization, rule-based
heuristics, and reinforcement learning often overlook driver income fairness,
interpretability, and adaptability to real-world dynamics. To address these
gaps, we propose LLM-ODDR, a novel framework leveraging Large Language Models
(LLMs) for joint Order Dispatching and Driver Repositioning (ODDR) in
ride-hailing services. LLM-ODDR framework comprises three key components: (1)
Multi-objective-guided Order Value Refinement, which evaluates orders by
considering multiple objectives to determine their overall value; (2)
Fairness-aware Order Dispatching, which balances platform revenue with driver
income fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning,
which optimizes idle vehicle placement based on historical patterns and
projected supply. We also develop JointDR-GPT, a fine-tuned model optimized for
ODDR tasks with domain knowledge. Extensive experiments on real-world datasets
from Manhattan taxi operations demonstrate that our framework significantly
outperforms traditional methods in terms of effectiveness, adaptability to
anomalous conditions, and decision interpretability. To our knowledge, this is
the first exploration of LLMs as decision-making agents in ride-hailing ODDR
tasks, establishing foundational insights for integrating advanced language
models within intelligent transportation systems.

</details>


### [117] [When Does Neuroevolution Outcompete Reinforcement Learning in Transfer Learning Tasks?](https://arxiv.org/abs/2505.22696)
*Eleni Nisioti,Joachim Winther Pedersen,Erwan Plantec,Milton L. Montero,Sebastian Risi*

Key words: 神经进化, 强化学习, 迁移学习, 基准测试, 课程学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了神经进化（NE）在迁移学习中的能力，提出了两个基准测试，并证明NE在迁移学习方面优于强化学习（RL）。

Motivation: 探索神经进化（NE）在技能迁移中的潜力，解决强化学习（RL）在面对任务变化时的脆弱性问题。

Method: 引入两个基准测试：逻辑电路模拟（stepping gates）和机器人形态切换（ecorobot），并通过课程学习评估技能迁移能力。

Result: 神经进化方法在迁移能力上表现优于强化学习基线，展示了其在构建适应性更强智能体方面的潜力。

Conclusion: 神经进化可作为适应复杂现实问题的基础，未来挑战在于将其扩展到更复杂的场景。

Abstract: The ability to continuously and efficiently transfer skills across tasks is a
hallmark of biological intelligence and a long-standing goal in artificial
systems. Reinforcement learning (RL), a dominant paradigm for learning in
high-dimensional control tasks, is known to suffer from brittleness to task
variations and catastrophic forgetting. Neuroevolution (NE) has recently gained
attention for its robustness, scalability, and capacity to escape local optima.
In this paper, we investigate an understudied dimension of NE: its transfer
learning capabilities. To this end, we introduce two benchmarks: a) in stepping
gates, neural networks are tasked with emulating logic circuits, with designs
that emphasize modular repetition and variation b) ecorobot extends the Brax
physics engine with objects such as walls and obstacles and the ability to
easily switch between different robotic morphologies. Crucial in both
benchmarks is the presence of a curriculum that enables evaluating skill
transfer across tasks of increasing complexity. Our empirical analysis shows
that NE methods vary in their transfer abilities and frequently outperform RL
baselines. Our findings support the potential of NE as a foundation for
building more adaptable agents and highlight future challenges for scaling NE
to complex, real-world problems.

</details>


### [118] [Update Your Transformer to the Latest Release: Re-Basin of Task Vectors](https://arxiv.org/abs/2505.22697)
*Filippo Rinaldi,Giacomo Capitani,Lorenzo Bonicelli,Donato Crisostomi,Federico Bolelli,Elisa Ficarra,Emanuele Rodolà,Simone Calderara,Angelo Porrello*

Key words: 微调转移,预训练模型,无数据方法,Transformer模型,多头注意力

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了如何在不重新训练的情况下，将微调任务向量转移到新的预训练模型上，提出了一种基于权重排列的无数据方法。

Motivation: 当前，当预训练模型更新或重新训练时，基于其微调的专业模型会失效，需重新训练，这对效率和数据隐私有负面影响。

Method: 论文利用模型再均衡原则，提出基于权重排列的两级方法，特别针对Transformer模型的残差连接和多头注意力层进行优化。

Result: 通过视觉和文本任务的广泛实验，该方案成功实现了无需训练或数据的微调知识无缝转移。

Conclusion: 该方法为预训练模型更新后的微调任务向量转移提供了一种高效、无数据的解决方案，对实际应用具有重要意义。

Abstract: Foundation models serve as the backbone for numerous specialized models
developed through fine-tuning. However, when the underlying pretrained model is
updated or retrained (e.g., on larger and more curated datasets), the
fine-tuned model becomes obsolete, losing its utility and requiring retraining.
This raises the question: is it possible to transfer fine-tuning to a new
release of the model? In this work, we investigate how to transfer fine-tuning
to a new checkpoint without having to re-train, in a data-free manner. To do
so, we draw principles from model re-basin and provide a recipe based on weight
permutations to re-base the modifications made to the original base model,
often called task vector. In particular, our approach tailors model re-basin
for Transformer models, taking into account the challenges of residual
connections and multi-head attention layers. Specifically, we propose a
two-level method rooted in spectral theory, initially permuting the attention
heads and subsequently adjusting parameters within select pairs of heads.
Through extensive experiments on visual and textual tasks, we achieve the
seamless transfer of fine-tuned knowledge to new pre-trained backbones without
relying on a single training step or datapoint. Code is available at
https://github.com/aimagelab/TransFusion.

</details>


### [119] [Private Rate-Constrained Optimization with Applications to Fair Learning](https://arxiv.org/abs/2505.22703)
*Mohammad Yaghini,Tudor Cebere,Michael Menart,Aurélien Bellet,Nicolas Papernot*

Key words: 差分隐私, 公平性, 拉格朗日方法, SGDA, 机器学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了RaCO-DP，一种差分隐私版本的SGDA算法，用于解决带约束的机器学习问题，通过拉格朗日方法和私有直方图估计实现隐私与公平-效用权衡的优化。

Motivation: 许多可信机器学习问题需要在满足预测率约束（如公平性）的同时最小化模型误差，但传统差分隐私优化方法（如DP-SGD）因约束引入的样本间依赖而失效。

Method: 开发RaCO-DP，基于SGDA的拉格朗日方法，通过私有化估计小批量直方图来解决约束问题。

Result: 理论证明算法收敛性，实验表明在公平-效用权衡上优于现有隐私学习方法。

Conclusion: RaCO-DP有效解决了带约束的隐私优化问题，并在公平性与效用间取得更好平衡。

Abstract: Many problems in trustworthy ML can be formulated as minimization of the
model error under constraints on the prediction rates of the model for
suitably-chosen marginals, including most group fairness constraints
(demographic parity, equality of odds, etc.). In this work, we study such
constrained minimization problems under differential privacy (DP). Standard DP
optimization techniques like DP-SGD rely on the loss function's decomposability
into per-sample contributions. However, rate constraints introduce inter-sample
dependencies, violating the decomposability requirement. To address this, we
develop RaCO-DP, a DP variant of the Stochastic Gradient Descent-Ascent (SGDA)
algorithm which solves the Lagrangian formulation of rate constraint problems.
We demonstrate that the additional privacy cost of incorporating these
constraints reduces to privately estimating a histogram over the mini-batch at
each optimization step. We prove the convergence of our algorithm through a
novel analysis of SGDA that leverages the linear structure of the dual
parameter. Finally, empirical results on learning under group fairness
constraints demonstrate that our method Pareto-dominates existing private
learning approaches in fairness-utility trade-offs.

</details>


### [120] [FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference](https://arxiv.org/abs/2505.22758)
*Aniruddha Nrusimha,William Brandon,Mayank Mishra,Yikang Shen,Rameswar Panda,Jonathan Ragan-Kelley,Yoon Kim*

Key words: 大语言模型, 推理加速, 单批次推理, FlashFormer, 延迟敏感

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了FlashFormer，一个针对基于Transformer的大语言模型单批次推理加速的概念验证内核，相比现有最先进推理内核实现了显著加速。

Motivation: 现代大语言模型的规模和计算特性引发了对专门内核的兴趣，现有内核主要优化大型批次的训练和推理，但在内存带宽和内核启动开销显著的边缘部署和延迟敏感应用中，小批次推理仍很重要。

Method: 开发了FlashFormer，一个专门针对单批次推理优化的概念验证内核，适用于不同模型大小和量化设置。

Result: 在各种模型大小和量化设置下，FlashFormer相比现有最先进推理内核实现了显著加速。

Conclusion: FlashFormer证明了针对小批次推理优化的潜力，尤其在边缘部署和延迟敏感场景中具有重要应用价值。

Abstract: The size and compute characteristics of modern large language models have led
to an increased interest in developing specialized kernels tailored for
training and inference. Existing kernels primarily optimize for compute
utilization, targeting the large-batch training and inference settings.
However, low-batch inference, where memory bandwidth and kernel launch
overheads contribute are significant factors, remains important for many
applications of interest such as in edge deployment and latency-sensitive
applications. This paper describes FlashFormer, a proof-of-concept kernel for
accelerating single-batch inference for transformer-based large language
models. Across various model sizes and quantizations settings, we observe
nontrivial speedups compared to existing state-of-the-art inference kernels.

</details>


### [121] [Test-time augmentation improves efficiency in conformal prediction](https://arxiv.org/abs/2505.22764)
*Divya Shanmugam,Helen Lu,Swami Sankaranarayanan,John Guttag*

Key words: conformal classifiers, test-time augmentation, probabilistic guarantee, prediction sets, inductive biases

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Test-time augmentation (TTA) reduces set sizes in conformal classifiers, improving informativeness without retraining.

Motivation: Conformal classifiers often produce large, uninformative prediction sets, which TTA addresses by introducing inductive biases during inference.

Method: Uses TTA with conformal classifiers, compatible with any conformal score, no model retraining required.

Result: Reduces prediction set sizes by 10%-14% on average across multiple datasets, models, and conformal scoring methods.

Conclusion: TTA is a flexible, efficient, and effective addition to the conformal pipeline, especially under distribution shifts.

Abstract: A conformal classifier produces a set of predicted classes and provides a
probabilistic guarantee that the set includes the true class. Unfortunately, it
is often the case that conformal classifiers produce uninformatively large
sets. In this work, we show that test-time augmentation (TTA)--a technique that
introduces inductive biases during inference--reduces the size of the sets
produced by conformal classifiers. Our approach is flexible, computationally
efficient, and effective. It can be combined with any conformal score, requires
no model retraining, and reduces prediction set sizes by 10%-14% on average. We
conduct an evaluation of the approach spanning three datasets, three models,
two established conformal scoring methods, different guarantee strengths, and
several distribution shifts to show when and why test-time augmentation is a
useful addition to the conformal pipeline.

</details>


### [122] [Multivariate de Bruijn Graphs: A Symbolic Graph Framework for Time Series Forecasting](https://arxiv.org/abs/2505.22768)
*Mert Onur Cakiroglu,Idil Bilge Altun,Hasan Kurban,Elham Buxton,Mehmet Dalkilic*

Key words: 时间序列预测, 多变量德布鲁因图, 图注意力, CNN, 符号表示

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: DRAGON 是一种新型编码器，通过多变量德布鲁因图（MdBGs）将连续时间序列离散化并映射到固定图结构，结合图注意力机制和双分支架构增强CNN编码器的性能。

Motivation: 时间序列预测因时间异质性、高维度和缺乏符号结构而具有挑战性，DRAGON旨在通过符号表示与神经建模的结合来克服这些困难。

Method: 提出DRAGON编码器，利用MdBGs离散化连续序列并映射到固定图结构，结合图注意力机制动态恢复上下文，以双分支架构辅助CNN编码器。

Result: DRAGON通过引入符号和结构感知表示，增强了传统CNN编码器的性能。

Conclusion: DRAGON为时间序列预测提供了一种结合符号和神经建模的有效方法，代码已开源。

Abstract: Time series forecasting remains a challenging task for foundation models due
to temporal heterogeneity, high dimensionality, and the lack of inherent
symbolic structure. In this work, we propose DRAGON (Discrete Representation
and Augmented Graph encoding Over deBruijN Graphs), a novel encoder that
introduces Multivariate de Bruijn Graphs (MdBGs) to bridge the gap between
symbolic representations and neural modeling. DRAGON discretizes continuous
input sequences and maps them onto a fixed graph structure, enabling dynamic
context recovery via graph-based attention. Integrated as an auxiliary module
within a dual-branch architecture, DRAGON augments conventional CNN-based
encoders with symbolic, structure-aware representations. All code developed for
this study is available at:
https://github.com/KurbanIntelligenceLab/MultdBG-Time-Series-Library

</details>


### [123] [Calibrated Value-Aware Model Learning with Stochastic Environment Models](https://arxiv.org/abs/2505.22772)
*Claas Voelcker,Anastasiia Pedan,Arash Ahmadian,Romina Abachi,Igor Gilitschenski,Amir-massoud Farahmand*

Key words: 强化学习, 模型学习, 价值感知, MuZero损失, 损失校准

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文探讨了在基于模型的强化学习中，价值感知模型学习损失的校准问题，提出了对MuZero损失的修正方法，并分析了损失校准与模型架构及辅助损失的交互作用。

Motivation: 价值感知模型学习在强化学习中逐渐兴起，但MuZero损失等方法的理论分析不足，存在未校准问题，需要深入研究与改进。

Method: 通过理论分析价值感知模型学习损失（包括MuZero损失）的校准性，提出修正方法，并探讨损失校准、潜在模型架构及辅助损失的关系。

Result: 研究发现现有损失存在未校准问题，改进后的方法能恢复正确的模型和价值函数；同时，随机模型的校准学习比确定性模型更有优势。

Conclusion: 价值感知模型学习的校准性至关重要，修正后的损失能够提升模型准确性，随机模型的校准学习在预测价值时表现更好。

Abstract: The idea of value-aware model learning, that models should produce accurate
value estimates, has gained prominence in model-based reinforcement learning.
The MuZero loss, which penalizes a model's value function prediction compared
to the ground-truth value function, has been utilized in several prominent
empirical works in the literature. However, theoretical investigation into its
strengths and weaknesses is limited. In this paper, we analyze the family of
value-aware model learning losses, which includes the popular MuZero loss. We
show that these losses, as normally used, are uncalibrated surrogate losses,
which means that they do not always recover the correct model and value
function. Building on this insight, we propose corrections to solve this issue.
Furthermore, we investigate the interplay between the loss calibration, latent
model architectures, and auxiliary losses that are commonly employed when
training MuZero-style agents. We show that while deterministic models can be
sufficient to predict accurate values, learning calibrated stochastic models is
still advantageous.

</details>


### [124] [Machine Learning Models Have a Supply Chain Problem](https://arxiv.org/abs/2505.22778)
*Sarah Meiklejohn,Hayden Blauzvern,Mihai Maruseac,Spencer Schrock,Laurent Simon,Ilia Shumailov*

Key words: 机器学习, 供应链风险, 透明度, Sigstore, 模型安全

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了开放机器学习模型生态系统的供应链风险，并提出使用Sigstore增加透明度的解决方案。

Motivation: 开放的机器学习模型生态系统虽然便利，但存在供应链风险，如恶意替换或数据问题，需要透明化管理。

Method: 通过Sigstore工具，模型发布者可以签名模型并证明数据集属性，以增强透明度。

Result: 研究表明，当前开放ML模型生态系统存在风险，Sigstore可有效缓解这些风险。

Conclusion: 利用Sigstore等工具可以提升开放机器学习模型的透明度和安全性。

Abstract: Powerful machine learning (ML) models are now readily available online, which
creates exciting possibilities for users who lack the deep technical expertise
or substantial computing resources needed to develop them. On the other hand,
this type of open ecosystem comes with many risks. In this paper, we argue that
the current ecosystem for open ML models contains significant supply-chain
risks, some of which have been exploited already in real attacks. These include
an attacker replacing a model with something malicious (e.g., malware), or a
model being trained using a vulnerable version of a framework or on restricted
or poisoned data. We then explore how Sigstore, a solution designed to bring
transparency to open-source software supply chains, can be used to bring
transparency to open ML models, in terms of enabling model publishers to sign
their models and prove properties about the datasets they use.

</details>


### [125] [Navigating the Latent Space Dynamics of Neural Models](https://arxiv.org/abs/2505.22785)
*Marco Fumero,Luca Moschella,Emanuele Rodolà,Francesco Locatello*

Key words: 神经网络, 动态系统, 自编码器, 潜在流形, 向量场, 吸引点

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 将神经网络解释为作用于潜在流形上的动态系统，展示了自编码器通过隐式定义潜在向量场揭示训练过程中的归纳偏差和吸引点，提供了一种新的模型分析工具。

Motivation: 传统方法将神经网络视为高维数据的降维表示，本文试图从动态系统角度重新解释神经网络的行为，以揭示其潜在的结构和属性。

Method: 提出将自编码器的编码-解码映射迭代应用，隐式定义潜在流形上的向量场，无需额外训练，并利用该向量场分析模型特性和数据。

Result: 通过向量场表征模型发现归纳偏差导致的吸引点，可用于分析模型泛化与记忆机制、提取网络参数中的先验知识，并识别分布外样本。

Conclusion: 动态系统视角为神经网络分析提供了新工具，尤其在理解模型行为和数据处理方面具有潜力，适用于现实场景中的视觉基础模型。

Abstract: Neural networks transform high-dimensional data into compact, structured
representations, often modeled as elements of a lower dimensional latent space.
In this paper, we present an alternative interpretation of neural models as
dynamical systems acting on the latent manifold. Specifically, we show that
autoencoder models implicitly define a latent vector field on the manifold,
derived by iteratively applying the encoding-decoding map, without any
additional training. We observe that standard training procedures introduce
inductive biases that lead to the emergence of attractor points within this
vector field. Drawing on this insight, we propose to leverage the vector field
as a representation for the network, providing a novel tool to analyze the
properties of the model and the data. This representation enables to: (i)
analyze the generalization and memorization regimes of neural models, even
throughout training; (ii) extract prior knowledge encoded in the network's
parameters from the attractors, without requiring any input data; (iii)
identify out-of-distribution samples from their trajectories in the vector
field. We further validate our approach on vision foundation models, showcasing
the applicability and effectiveness of our method in real-world scenarios.

</details>


### [126] [Efficient Preimage Approximation for Neural Network Certification](https://arxiv.org/abs/2505.22798)
*Anton Björklund,Mykola Zaitsev,Marta Kwiatkowska*

Key words: 神经网络认证,补丁攻击,预像近似,PREMAP算法,自适应蒙特卡罗采样

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种改进的PREMAP算法，用于神经网络预像近似，以提升对补丁攻击的认证效率和可扩展性。

Motivation: 随着人工智能在安全和关键安全应用中的依赖增加，需要有效的神经网络认证方法，特别是针对补丁攻击等实际用例。

Method: 通过更紧凑的边界、自适应蒙特卡罗采样和改进的分支启发式方法，对PREMAP算法进行了优化。

Result: 在强化学习控制基准测试中效率提升至少一个数量级，并能扩展到之前不可行的卷积神经网络。

Conclusion: 改进的PREMAP算法展示了预像近似方法在可靠性和鲁棒性认证上的潜力。

Abstract: The growing reliance on artificial intelligence in safety- and
security-critical applications demands effective neural network certification.
A challenging real-world use case is certification against ``patch attacks'',
where adversarial patches or lighting conditions obscure parts of images, for
example traffic signs. One approach to certification, which also gives
quantitative coverage estimates, utilizes preimages of neural networks, i.e.,
the set of inputs that lead to a specified output. However, these preimage
approximation methods, including the state-of-the-art PREMAP algorithm,
struggle with scalability. This paper presents novel algorithmic improvements
to PREMAP involving tighter bounds, adaptive Monte Carlo sampling, and improved
branching heuristics. We demonstrate efficiency improvements of at least an
order of magnitude on reinforcement learning control benchmarks, and show that
our method scales to convolutional neural networks that were previously
infeasible. Our results demonstrate the potential of preimage approximation
methodology for reliability and robustness certification.

</details>


### [127] [CLUE: Neural Networks Calibration via Learning Uncertainty-Error alignment](https://arxiv.org/abs/2505.22803)
*Pedro Mendes,Paolo Romano,David Garlan*

Key words: 不确定性估计, 校准, 神经网络, 领域泛化, CLUE

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: CLUE通过训练期间明确对齐预测不确定性与观察误差，提出一种新的校准方法，无需后处理即可在不同领域中实现可扩展、可微分、泛化的不确定性估计。

Motivation: 现有校准技术依赖于后处理或粗粒度分箱方法，难以实现可扩展性、可微分性和领域泛化。

Method: 引入CLUE方法，通过联合优化预测性能和校准的新损失函数，使用不确定性和损失的摘要统计作为代理。

Result: 在视觉、回归和语言建模任务中，包括分布外和领域偏移场景，CLUE表现出优异的校准质量和竞争力预测性能。

Conclusion: CLUE是一种完全可微分、领域无关且兼容标准训练流程的方法，无需显著计算开销即可优于现有技术。

Abstract: Reliable uncertainty estimation is critical for deploying neural networks
(NNs) in real-world applications. While existing calibration techniques often
rely on post-hoc adjustments or coarse-grained binning methods, they remain
limited in scalability, differentiability, and generalization across domains.
In this work, we introduce CLUE (Calibration via Learning Uncertainty-Error
Alignment), a novel approach that explicitly aligns predicted uncertainty with
observed error during training, grounded in the principle that well-calibrated
models should produce uncertainty estimates that match their empirical loss.
CLUE adopts a novel loss function that jointly optimizes predictive performance
and calibration, using summary statistics of uncertainty and loss as proxies.
The proposed method is fully differentiable, domain-agnostic, and compatible
with standard training pipelines. Through extensive experiments on vision,
regression, and language modeling tasks, including out-of-distribution and
domain-shift scenarios, we demonstrate that CLUE achieves superior calibration
quality and competitive predictive performance with respect to state-of-the-art
approaches without imposing significant computational overhead.

</details>


### [128] [X-Factor: Quality Is a Dataset-Intrinsic Property](https://arxiv.org/abs/2505.22813)
*Josiah Couch,Miao Li,Rima Arnaout,Ramy Arnaout*

Key words: dataset quality, machine learning, classifier performance, model architecture, class balance

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究发现数据集质量是独立于模型架构、数据集大小和类别平衡的内在属性，且与分类器性能强相关。

Motivation: 探索数据集质量是否是独立于模型架构、数据集大小和类别平衡的内在属性，从而更全面理解影响分类器性能的因素。

Method: 通过创建数千个控制大小和类别平衡的数据集，并用多种模型架构（从随机森林到深度网络）训练分类器，分析性能相关性。

Result: 不同模型架构下，分类器性能与数据集子集强相关（R²=0.79），表明质量是数据集的内在属性。

Conclusion: 数据集质量是独立于其他因素的重要性能指标，且可能源于其组成类别的质量，为机器学习分类优化提供了新方向。

Abstract: In the universal quest to optimize machine-learning classifiers, three
factors -- model architecture, dataset size, and class balance -- have been
shown to influence test-time performance but do not fully account for it.
Previously, evidence was presented for an additional factor that can be
referred to as dataset quality, but it was unclear whether this was actually a
joint property of the dataset and the model architecture, or an intrinsic
property of the dataset itself. If quality is truly dataset-intrinsic and
independent of model architecture, dataset size, and class balance, then the
same datasets should perform better (or worse) regardless of these other
factors. To test this hypothesis, here we create thousands of datasets, each
controlled for size and class balance, and use them to train classifiers with a
wide range of architectures, from random forests and support-vector machines to
deep networks. We find that classifier performance correlates strongly by
subset across architectures ($R^2=0.79$), supporting quality as an intrinsic
property of datasets independent of dataset size and class balance and of model
architecture. Digging deeper, we find that dataset quality appears to be an
emergent property of something more fundamental: the quality of datasets'
constituent classes. Thus, quality joins size, class balance, and model
architecture as an independent correlate of performance and a separate target
for optimizing machine-learning-based classification.

</details>


### [129] [Preference Learning with Response Time](https://arxiv.org/abs/2505.22820)
*Ayush Sawarni,Sahasrajit Sarmasarkar,Vasilis Syrgkanis*

Key words: 偏好学习, 奖励模型, 响应时间, EZ模型, 样本效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种将响应时间数据整合到人类偏好学习框架中的新方法，通过二元选择和响应时间数据的结合，显著提升了奖励模型的学习效率和准确性。

Motivation: 现有的二元偏好数据未被充分利用，而用户决策过程中的响应时间信息蕴含了偏好强度的重要信息，因此研究如何整合这些信息以提高奖励模型的学习效果是一个重要课题。

Method: 利用证据积累漂移扩散模型（EZ）整合响应时间和二元选择数据，并开发了Neyman正交损失函数，以实现最优收敛速率。

Result: 实验证明，相较于传统的偏好学习方法，整合响应时间数据的模型显著降低了误差率，提升了样本效率，并在图像偏好学习任务中验证了这一效果。

Conclusion: 通过整合响应时间信息，可以显著提升奖励模型的学习效率和准确性，尤其在复杂或大规模场景下效果更为明显。

Abstract: This paper investigates the integration of response time data into human
preference learning frameworks for more effective reward model elicitation.
While binary preference data has become fundamental in fine-tuning foundation
models, generative AI systems, and other large-scale models, the valuable
temporal information inherent in user decision-making remains largely
unexploited. We propose novel methodologies to incorporate response time
information alongside binary choice data, leveraging the Evidence Accumulation
Drift Diffusion (EZ) model, under which response time is informative of the
preference strength. We develop Neyman-orthogonal loss functions that achieve
oracle convergence rates for reward model learning, matching the theoretical
optimal rates that would be attained if the expected response times for each
query were known a priori. Our theoretical analysis demonstrates that for
linear reward functions, conventional preference learning suffers from error
rates that scale exponentially with reward magnitude. In contrast, our response
time-augmented approach reduces this to polynomial scaling, representing a
significant improvement in sample efficiency. We extend these guarantees to
non-parametric reward function spaces, establishing convergence properties for
more complex, realistic reward models. Our extensive experiments validate our
theoretical findings in the context of preference learning over images.

</details>


### [130] [PGLearn -- An Open-Source Learning Toolkit for Optimal Power Flow](https://arxiv.org/abs/2505.22825)
*Michael Klamkin,Mathieu Tanneau,Pascal Van Hentenryck*

Key words: 机器学习, 最优潮流, 标准化数据集, 评估工具, 时间序列数据

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出PGLearn，一套标准化数据集和评估工具，旨在解决机器学习在最优潮流问题中缺乏统一数据集和评估标准的问题，推动该领域研究。

Motivation: 现代电网的能源生产波动性和规模扩大，促使利用机器学习近似或加速解决复杂优化问题，但缺乏标准化数据集和评估指标阻碍了进展。

Method: 开发PGLearn，包含代表真实运行条件的数据集，支持多种最优潮流公式，并提供训练、评估和基准测试工具。

Result: PGLearn首次包含大规模系统的时间序列数据，支持AC、DC和二阶锥公式，公开数据集和工具以促进公平比较。

Conclusion: PGLearn通过标准化数据集和评估指标，推动最优潮流问题中机器学习的研究和创新。

Abstract: Machine Learning (ML) techniques for Optimal Power Flow (OPF) problems have
recently garnered significant attention, reflecting a broader trend of
leveraging ML to approximate and/or accelerate the resolution of complex
optimization problems. These developments are necessitated by the increased
volatility and scale in energy production for modern and future grids. However,
progress in ML for OPF is hindered by the lack of standardized datasets and
evaluation metrics, from generating and solving OPF instances, to training and
benchmarking machine learning models. To address this challenge, this paper
introduces PGLearn, a comprehensive suite of standardized datasets and
evaluation tools for ML and OPF. PGLearn provides datasets that are
representative of real-life operating conditions, by explicitly capturing both
global and local variability in the data generation, and by, for the first
time, including time series data for several large-scale systems. In addition,
it supports multiple OPF formulations, including AC, DC, and second-order cone
formulations. Standardized datasets are made publicly available to democratize
access to this field, reduce the burden of data generation, and enable the fair
comparison of various methodologies. PGLearn also includes a robust toolkit for
training, evaluating, and benchmarking machine learning models for OPF, with
the goal of standardizing performance evaluation across the field. By promoting
open, standardized datasets and evaluation metrics, PGLearn aims at
democratizing and accelerating research and innovation in machine learning
applications for optimal power flow problems. Datasets are available for
download at https://www.huggingface.co/PGLearn.

</details>


### [131] [Bridging Distribution Shift and AI Safety: Conceptual and Methodological Synergies](https://arxiv.org/abs/2505.22829)
*Chenruo Liu,Kenan Tang,Yao Qin,Qi Lei*

Key words: 分布偏移, AI安全, 方法协同, 统一视角

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文通过分析分布偏移与AI安全的联系，建立了两类具体关联，提出了统一视角以促进两者的融合研究。

Motivation: 弥补现有研究对分布偏移与AI安全之间联系的不足，通过具体关联促进两者方法的相互适应。

Method: 综合分析分布偏移与AI安全的协同关系，建立两种具体关联并验证其方法适应性。

Result: 提出了两类分布偏移与AI安全问题的关联方法，并展示了其相互适应性。

Conclusion: 研究为分布偏移与AI安全的融合提供了统一视角，推动了领域间的深入合作。

Abstract: This paper bridges distribution shift and AI safety through a comprehensive
analysis of their conceptual and methodological synergies. While prior
discussions often focus on narrow cases or informal analogies, we establish two
types connections between specific causes of distribution shift and
fine-grained AI safety issues: (1) methods addressing a specific shift type can
help achieve corresponding safety goals, or (2) certain shifts and safety
issues can be formally reduced to each other, enabling mutual adaptation of
their methods. Our findings provide a unified perspective that encourages
fundamental integration between distribution shift and AI safety research.

</details>


### [132] [How Do Diffusion Models Improve Adversarial Robustness?](https://arxiv.org/abs/2505.22839)
*Liu Yuezhang,Xue-Xin Wei*

Key words: 扩散模型,对抗鲁棒性,输入空间压缩,净化,随机性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 扩散模型为何提升对抗鲁棒性？研究发现关键在于输入空间的压缩而非直觉的去噪，且内部随机性影响显著。

Motivation: 探究扩散模型提升对抗鲁棒性的具体机制，挑战了传统去噪直觉。

Method: 分析扩散模型的ℓ_p距离变化、随机性影响，并在固定随机性下评估鲁棒性。

Result: 固定随机性时鲁棒性提升仅24%，远低于此前报告的70%，且压缩率与鲁棒性增益强相关。

Conclusion: 输入空间压缩是扩散模型鲁棒性提升的关键机制，为设计更有效的对抗净化系统提供指导。

Abstract: Recent findings suggest that diffusion models significantly enhance empirical
adversarial robustness. While some intuitive explanations have been proposed,
the precise mechanisms underlying these improvements remain unclear. In this
work, we systematically investigate how and how well diffusion models improve
adversarial robustness. First, we observe that diffusion models intriguingly
increase, rather than decrease, the $\ell_p$ distance to clean
samples--challenging the intuition that purification denoises inputs closer to
the original data. Second, we find that the purified images are heavily
influenced by the internal randomness of diffusion models, where a compression
effect arises within each randomness configuration. Motivated by this
observation, we evaluate robustness under fixed randomness and find that the
improvement drops to approximately 24% on CIFAR-10--substantially lower than
prior reports approaching 70%. Importantly, we show that this remaining
robustness gain strongly correlates with the model's ability to compress the
input space, revealing the compression rate as a reliable robustness indicator
without requiring gradient-based analysis. Our findings provide novel insights
into the mechanisms underlying diffusion-based purification, and offer guidance
for developing more effective and principled adversarial purification systems.

</details>


### [133] [Development and Validation of SXI++ LNM Algorithm for Sepsis Prediction](https://arxiv.org/abs/2505.22840)
*Dharambir Mahto,Prashant Yadav,Mahesh Banavar,Jim Keany,Alan T Joseph,Srinivas Kilambi*

Key words: 脓毒症,机器学习,深度神经网络,SXI++ LNM,预测模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: SXI++ LNM是一种机器学习评分系统，通过结合多种算法和深度神经网络，显著提升了脓毒症的预测性能，表现出卓越的准确性和可靠性。

Motivation: 脓毒症是全球范围内的重大健康威胁，但由于其症状非特异性和病理复杂性，预测仍具挑战性。本研究旨在开发更稳健的预测模型以改善临床应用。

Method: 研究采用深度神经网络，通过多种数据集分布场景进行训练和测试，评估模型的预测性能，包括准确性、精确度和AUC等指标。

Result: SXI++ LNM在三种用例中表现优于现有技术，AUC达0.99，精确度和准确性均接近100%，展现出极高的可靠性。

Conclusion: SXI++ LNM作为一种先进的脓毒症预测工具，具备极高的临床实用价值，有望提升早期诊断和治疗效果。

Abstract: Sepsis is a life-threatening condition affecting over 48.9 million people
globally and causing 11 million deaths annually. Despite medical advancements,
predicting sepsis remains a challenge due to non-specific symptoms and complex
pathophysiology. The SXI++ LNM is a machine learning scoring system that
refines sepsis prediction by leveraging multiple algorithms and deep neural
networks. This study aims to improve robustness in clinical applications and
evaluates the predictive performance of the SXI++ LNM for sepsis prediction.
The model, utilizing a deep neural network, was trained and tested using
multiple scenarios with different dataset distributions. The model's
performance was assessed against unseen test data, and accuracy, precision, and
area under the curve (AUC) were calculated. THE SXI++ LNM outperformed the
state of the art in three use cases, achieving an AUC of 0.99 (95% CI:
0.98-1.00). The model demonstrated a precision of 99.9% (95% CI: 99.8-100.0)
and an accuracy of 99.99% (95% CI: 99.98-100.0), maintaining high reliability.

</details>


### [134] [Kernel-Smoothed Scores for Denoising Diffusion: A Bias-Variance Study](https://arxiv.org/abs/2505.22841)
*Franck Gabriel,François Ged,Maria Han Veiga,Emmanuel Schertzer*

Key words: 扩散模型,记忆化,核平滑,LED-KDE,泛化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了扩散模型中的记忆化问题，通过引入核平滑经验分数分析了偏差-方差权衡，并提出了LED-KDE框架以增强泛化能力。

Motivation: 扩散模型在高保真生成采样中表现出色，但存在记忆化风险，即反向时间SDE可能过度拟合训练数据。论文旨在通过分数正则化减少方差，避免记忆化。

Method: 引入核平滑经验分数，分析其偏差-方差权衡；提出LED-KDE框架，结合高斯核和分数空间核进行数据分布正则化。

Result: 核平滑分数显著减少方差，实验在合成和MNIST数据集上验证了方法的有效性，增强了模型的泛化能力。

Conclusion: 分数正则化通过类似增加训练数据集的效果，有效缓解记忆化问题，提升扩散模型的生成质量。

Abstract: Diffusion models now set the benchmark in high-fidelity generative sampling,
yet they can, in principle, be prone to memorization. In this case, their
learned score overfits the finite dataset so that the reverse-time SDE samples
are mostly training points. In this paper, we interpret the empirical score as
a noisy version of the true score and show that its covariance matrix is
asymptotically a re-weighted data PCA. In large dimension, the small time limit
makes the noise variance blow up while simultaneously reducing spatial
correlation. To reduce this variance, we introduce a kernel-smoothed empirical
score and analyze its bias-variance trade-off. We derive asymptotic bounds on
the Kullback-Leibler divergence between the true distribution and the one
generated by the modified reverse SDE. Regularization on the score has the same
effect as increasing the size of the training dataset, and thus helps prevent
memorization. A spectral decomposition of the forward diffusion suggests better
variance control under some regularity conditions of the true data
distribution. Reverse diffusion with kernel-smoothed empirical score can be
reformulated as a gradient descent drifted toward a Log-Exponential
Double-Kernel Density Estimator (LED-KDE). This perspective highlights two
regularization mechanisms taking place in denoising diffusions: an initial
Gaussian kernel first diffuses mass isotropically in the ambient space, while a
second kernel applied in score space concentrates and spreads that mass along
the data manifold. Hence, even a straightforward regularization-without any
learning-already mitigates memorization and enhances generalization.
Numerically, we illustrate our results with several experiments on synthetic
and MNIST datasets.

</details>


### [135] [RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for Rocq generation](https://arxiv.org/abs/2505.22846)
*Nikita Khramov,Andrei Kozyrev,Gleb Solovev,Anton Podkopaev*

Key words: 交互式定理证明, 生成式人工智能, Rocq证明, 自注意力嵌入模型, 多代理系统

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文结合生成式人工智能与交互式定理证明，提出了基于自注意力嵌入模型的Rocq证明生成方法，性能提升28%，并通过多阶段代理系统和多代理辩论验证了其高效性。

Motivation: 评估多种Rocq生成方法并探索改进路径，强调前提选择对生成证明的重要性，以提升生成式人工智能在定理证明中的应用效果。

Method: 采用基于自注意力嵌入模型的检索方法，设计多阶段代理系统，结合多代理辩论进行证明合成的规划阶段优化。

Result: 提出的方法使生成器性能相对提升28%，并通过消融研究验证了多代理辩论在规划阶段的有效性。

Conclusion: 该方法显著提升了Rocq证明生成的效率和准确性，为形式化验证领域提供了新思路。

Abstract: Interactive Theorem Proving was repeatedly shown to be fruitful combined with
Generative Artificial Intelligence. This paper assesses multiple approaches to
Rocq generation and illuminates potential avenues for improvement. We highlight
the importance of thorough premise selection for generating Rocq proofs and
propose a novel approach, leveraging retrieval via a self-attentive embedder
model. The evaluation of the designed approach shows up to 28% relative
increase of the generator's performance. We tackle the problem of writing Rocq
proofs using a multi-stage agentic system, tailored for formal verification,
and demonstrate its high effectiveness. We conduct an ablation study and show
the use of multi-agent debate on the planning stage of proof synthesis.

</details>


### [136] [Causal-PIK: Causality-based Physical Reasoning with a Physics-Informed Kernel](https://arxiv.org/abs/2505.22861)
*Carlota Parés-Morlans,Michelle Yi,Claire Chen,Sarah A. Wu,Rika Antonova,Tobias Gerstenberg,Jeannette Bohg*

Key words: 因果推理, 贝叶斯优化, 物理知识核, 复杂交互任务, PHYRE基准

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出Causal-PIK方法，利用贝叶斯优化和物理知识指导动作搜索，在复杂交互任务中表现优于现有方法，并在PHYRE等基准测试中验证其效率。

Motivation: 针对复杂交互任务中对象动态未知导致规划困难的问题，提出一种迭代优化方法，通过因果推理提升动作效率。

Method: Causal-PIK结合贝叶斯优化与物理知识核（Physics-Informed Kernel），通过因果交互推理指导高效动作搜索。

Result: 在Virtual Tools和PHYRE测试中，Causal-PIK优于现有方法，并且与人类解题者相比，在挑战性任务中表现更具竞争力。

Conclusion: Causal-PIK通过因果推理和物理知识提升了复杂任务的动作效率，尤其在动态未知的环境中表现突出。

Abstract: Tasks that involve complex interactions between objects with unknown dynamics
make planning before execution difficult. These tasks require agents to
iteratively improve their actions after actively exploring causes and effects
in the environment. For these type of tasks, we propose Causal-PIK, a method
that leverages Bayesian optimization to reason about causal interactions via a
Physics-Informed Kernel to help guide efficient search for the best next
action. Experimental results on Virtual Tools and PHYRE physical reasoning
benchmarks show that Causal-PIK outperforms state-of-the-art results, requiring
fewer actions to reach the goal. We also compare Causal-PIK to human studies,
including results from a new user study we conducted on the PHYRE benchmark. We
find that Causal-PIK remains competitive on tasks that are very challenging,
even for human problem-solvers.

</details>


### [137] [Scaling Offline RL via Efficient and Expressive Shortcut Models](https://arxiv.org/abs/2505.22866)
*Nicolas Espinosa-Dice,Yiyi Zhang,Yiding Chen,Bradley Guo,Owen Oertell,Gokul Swamy,Kiante Brantley,Wen Sun*

Key words: 离线强化学习, 扩散模型, 流模型, SORL, shortcut models

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: SORL是一种新的离线强化学习算法，通过引入shortcut models来简化训练和推理过程，克服了扩散和流模型在离线RL中的挑战，实现了高效的一阶段训练和灵活的推理扩展。

Motivation: 扩散和流模型在离线强化学习中应用困难，主要因为其迭代噪声采样过程使策略优化复杂。本文旨在通过引入shortcut models简化这一过程。

Method: 提出SORL算法，利用shortcut models简化训练（一阶段）和推理（结合Q函数验证的序列与并行扩展）。

Result: SORL在多种离线RL任务中表现优异，且测试计算资源增加时展现正向扩展性。

Conclusion: SORL通过创新生成模型设计，为离线RL提供了高效、可扩展的解决方案。

Abstract: Diffusion and flow models have emerged as powerful generative approaches
capable of modeling diverse and multimodal behavior. However, applying these
models to offline reinforcement learning (RL) remains challenging due to the
iterative nature of their noise sampling processes, making policy optimization
difficult. In this paper, we introduce Scalable Offline Reinforcement Learning
(SORL), a new offline RL algorithm that leverages shortcut models - a novel
class of generative models - to scale both training and inference. SORL's
policy can capture complex data distributions and can be trained simply and
efficiently in a one-stage training procedure. At test time, SORL introduces
both sequential and parallel inference scaling by using the learned Q-function
as a verifier. We demonstrate that SORL achieves strong performance across a
range of offline RL tasks and exhibits positive scaling behavior with increased
test-time compute. We release the code at
nico-espinosadice.github.io/projects/sorl.

</details>


### [138] [Smart Surrogate Losses for Contextual Stochastic Linear Optimization with Robust Constraints](https://arxiv.org/abs/2505.22881)
*Hyungki Im,Wyame Benslimane,Paul Grigas*

Key words: 随机优化,机器学习,约束不确定性,SPO-RC,重要性重加权

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种扩展的上下文随机线性优化方法SPO-RC，用于处理机器学习预测的不确定约束，并通过实验验证了其有效性。

Motivation: 现有的大多数随机线性优化研究未充分考虑由机器学习模型预测的不确定约束，因此本文旨在填补这一空白。

Method: 引入了SPO-RC损失函数及其凸代理SPO-RC+，并采用截断数据集和重要性重加权技术来处理样本选择偏差。

Result: 实验证明，SPO-RC+能有效处理约束不确定性，截断与重加权结合能进一步提升性能。

Conclusion: 该方法为约束不确定性提供了一种新颖且有效的解决方案。

Abstract: We study an extension of contextual stochastic linear optimization (CSLO)
that, in contrast to most of the existing literature, involves inequality
constraints that depend on uncertain parameters predicted by a machine learning
model. To handle the constraint uncertainty, we use contextual uncertainty sets
constructed via methods like conformal prediction. Given a contextual
uncertainty set method, we introduce the "Smart Predict-then-Optimize with
Robust Constraints" (SPO-RC) loss, a feasibility-sensitive adaptation of the
SPO loss that measures decision error of predicted objective parameters. We
also introduce a convex surrogate, SPO-RC+, and prove Fisher consistency with
SPO-RC. To enhance performance, we train on truncated datasets where true
constraint parameters lie within the uncertainty sets, and we correct the
induced sample selection bias using importance reweighting techniques. Through
experiments on fractional knapsack and alloy production problem instances, we
demonstrate that SPO-RC+ effectively handles uncertainty in constraints and
that combining truncation with importance reweighting can further improve
performance.

</details>


### [139] [On the Dynamic Regret of Following the Regularized Leader: Optimism with History Pruning](https://arxiv.org/abs/2505.22899)
*Naram Mhaisen,George Iosifidis*

Key words: FTRL, Online Convex Optimization, dynamic regret, optimism, pruning

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文重新审视了FTRL框架在动态环境中的表现，提出通过乐观组合未来成本和线性化历史成本的方法，实现了动态后悔界，并优化了更新策略。

Motivation: 研究FTRL框架在动态环境中的局限性，探索如何通过优化方法克服这些问题，以实现更好的动态后悔保证。

Method: 通过乐观组合未来成本和线性化历史成本，结合修剪技术，优化FTRL框架的更新策略。

Result: 证明FTRL可以恢复已知的动态后悔界，并提供了一种介于贪婪和敏捷更新之间的新方法，优化了后悔控制。

Conclusion: FTRL框架在动态环境中表现不佳的原因并非其懒惰投影风格，而是算法状态与迭代的脱节。通过修剪技术可以同步二者，提升性能。

Abstract: We revisit the Follow the Regularized Leader (FTRL) framework for Online
Convex Optimization (OCO) over compact sets, focusing on achieving dynamic
regret guarantees. Prior work has highlighted the framework's limitations in
dynamic environments due to its tendency to produce "lazy" iterates. However,
building on insights showing FTRL's ability to produce "agile" iterates, we
show that it can indeed recover known dynamic regret bounds through optimistic
composition of future costs and careful linearization of past costs, which can
lead to pruning some of them. This new analysis of FTRL against dynamic
comparators yields a principled way to interpolate between greedy and agile
updates and offers several benefits, including refined control over regret
terms, optimism without cyclic dependence, and the application of minimal
recursive regularization akin to AdaFTRL. More broadly, we show that it is not
the lazy projection style of FTRL that hinders (optimistic) dynamic regret, but
the decoupling of the algorithm's state (linearized history) from its iterates,
allowing the state to grow arbitrarily. Instead, pruning synchronizes these two
when necessary.

</details>


### [140] [Defining Foundation Models for Computational Science: A Call for Clarity and Rigor](https://arxiv.org/abs/2505.22904)
*Youngsoo Choi,Siu Wun Cheung,Youngkyu Kim,Ping-Hsuan Tsai,Alejandro N. Diaz,Ivan Zanardi,Seung Whan Chung,Dylan Matthew Copeland,Coleman Kendrick,William Anderson,Traian Iliescu,Matthias Heinkenschloss*

Key words: 基础模型, 计算科学, 数据驱动, 有限元方法, 人工智能

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文讨论了“基础模型”在计算科学中的应用缺乏统一定义的问题，提出了基于通用性、可重用性和可扩展性的正式定义，并介绍了数据驱动的有限元方法（DD-FEM）作为解决方案。

Motivation: 由于“基础模型”概念在计算科学中缺乏明确界定，作者旨在填补这一空白，提出正式定义并开发符合核心特性的方法。

Method: 提出数据驱动的有限元方法（DD-FEM），结合经典有限元方法的模块化结构与数据驱动的表征能力。

Result: DD-FEM能够解决基础模型在计算科学中的关键挑战，如可扩展性、适应性和物理一致性。

Conclusion: DD-FEM为计算科学中的基础模型提供了严格的基础，推动了传统数值方法与现代AI范式的融合。

Abstract: The widespread success of foundation models in natural language processing
and computer vision has inspired researchers to extend the concept to
scientific machine learning and computational science. However, this position
paper argues that as the term "foundation model" is an evolving concept, its
application in computational science is increasingly used without a universally
accepted definition, potentially creating confusion and diluting its precise
scientific meaning. In this paper, we address this gap by proposing a formal
definition of foundation models in computational science, grounded in the core
values of generality, reusability, and scalability. We articulate a set of
essential and desirable characteristics that such models must exhibit, drawing
parallels with traditional foundational methods, like the finite element and
finite volume methods. Furthermore, we introduce the Data-Driven Finite Element
Method (DD-FEM), a framework that fuses the modular structure of classical FEM
with the representational power of data-driven learning. We demonstrate how
DD-FEM addresses many of the key challenges in realizing foundation models for
computational science, including scalability, adaptability, and physics
consistency. By bridging traditional numerical methods with modern AI
paradigms, this work provides a rigorous foundation for evaluating and
developing novel approaches toward future foundation models in computational
science.

</details>


### [141] [Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference](https://arxiv.org/abs/2505.22913)
*Donghyeon Joo,Helya Hosseini,Ramyad Hadidi,Bahar Asgari*

Key words: KV缓存压缩、非结构化稀疏性、注意力内核、位图格式、解码性能

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文展示了非结构化稀疏性显著提升KV缓存压缩，支持70%稀疏度而不损失精度或需微调，通过定制注意力内核和位图稀疏格式实现45%的密集推理压缩和2.23倍的吞吐量提升。

Motivation: KV缓存大小是解码性能的主要瓶颈，因长上下文内存开销高，需高效压缩方案。

Method: 采用逐令牌幅值修剪策略，结合位图稀疏格式和定制注意力内核，直接处理压缩缓存。

Result: KV缓存压缩至密集推理的45%，上下文长度和吞吐量提升2.23倍。

Conclusion: 非结构化稀疏性结合定制硬件支持，能有效解决KV缓存内存瓶颈。

Abstract: We demonstrate that unstructured sparsity significantly improves KV cache
compression for LLMs, enabling sparsity levels up to 70% without compromising
accuracy or requiring fine-tuning. We conduct a systematic exploration of
pruning strategies and find per-token magnitude-based pruning as highly
effective for both Key and Value caches under unstructured sparsity, surpassing
prior structured pruning schemes. The Key cache benefits from prominent outlier
elements, while the Value cache surprisingly benefits from a simple
magnitude-based pruning despite its uniform distribution. KV cache size is the
major bottleneck in decode performance due to high memory overhead for large
context lengths. To address this, we use a bitmap-based sparse format and a
custom attention kernel capable of compressing and directly computing over
compressed caches pruned to arbitrary sparsity patterns, significantly
accelerating memory-bound operations in decode computations and thereby
compensating for the overhead of runtime pruning and compression. Our custom
attention kernel coupled with the bitmap-based format delivers substantial
compression of KV cache upto 45% of dense inference and thereby enables longer
context length and increased tokens/sec throughput of upto 2.23x compared to
dense inference. Our pruning mechanism and sparse attention kernel is available
at https://github.com/dhjoo98/mustafar.

</details>


### [142] [Scalable Parameter and Memory Efficient Pretraining for LLM: Recent Algorithmic Advances and Benchmarking](https://arxiv.org/abs/2505.22922)
*Athanasios Glentis,Jiaxiang Li,Qiulin Shang,Andi Han,Ioannis Tsaknakis,Quan Wei,Mingyi Hong*

Key words: 大型语言模型,高效预训练,低秩方法,权重重构动量重置

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文探讨了如何通过参数或内存高效方法来提升大规模语言模型预训练的效率，同时保持与全模型训练相当的性能，并提出了两种实用技术优化低秩方法。

Motivation: 随着大型语言模型参数量的激增，训练和微调所需的计算资源成为重大挑战，论文旨在解决高效预训练方法在性能上的差距问题。

Method: 论文首先综述了现有的高效预训练方法，并进行了基准评估；随后提出了权重重构和动量重置两种技术来优化低秩方法的性能。

Result: 通过合理选择优化器和超参数，全秩训练表现最佳；结合提出的技术，低秩方法在1B模型上能够超越现有高效算法（如GaLore和Fira）的困惑度，并减少25%内存使用。

Conclusion: 论文表明，优化低秩方法可以有效提升预训练效率并缩小性能差距，为未来高效预训练研究提供了实用方向。

Abstract: Fueled by their remarkable ability to tackle diverse tasks across multiple
domains, large language models (LLMs) have grown at an unprecedented rate, with
some recent models containing trillions of parameters. This growth is
accompanied by substantial computational challenges, particularly regarding the
memory and compute resources required for training and fine-tuning. Numerous
approaches have been explored to address these issues, such as LoRA. While
these methods are effective for fine-tuning, their application to pre-training
is significantly more challenging due to the need to learn vast datasets.
Motivated by this issue, we aim to address the following questions: Can
parameter- or memory-efficient methods enhance pre-training efficiency while
achieving performance comparable to full-model training? How can the
performance gap be narrowed? To this end, the contributions of this work are
the following. (1) We begin by conducting a comprehensive survey that
summarizes state-of-the-art methods for efficient pre-training. (2) We perform
a benchmark evaluation of several representative memory efficient pre-training
approaches to comprehensively evaluate their performance across model sizes. We
observe that with a proper choice of optimizer and hyperparameters, full-rank
training delivers the best performance, as expected. We also notice that
incorporating high-rank updates in low-rank approaches is the key to improving
their performance. (3) Finally, we propose two practical techniques, namely
weight refactorization and momentum reset, to enhance the performance of
efficient pre-training methods. We observe that applying these techniques to
the low-rank method (on a 1B model) can achieve a lower perplexity than popular
memory efficient algorithms such as GaLore and Fira, while simultaneously using
about 25% less memory.

</details>


### [143] [Is Noise Conditioning Necessary? A Unified Theory of Unconditional Graph Diffusion Models](https://arxiv.org/abs/2505.22935)
*Jipeng Li,Yanning Shen*

Key words: 图像扩散模型、噪声条件、隐式推断、去噪

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了图像扩散模型中显式噪声条件是否必要，提出无条件的GDM可通过隐式推断噪声取得同等或更好效果，并减少计算开销。

Motivation: 挑战显式噪声条件在图像扩散模型中的必要性，探讨无条件的GDM是否能通过隐式推断噪声实现高效的去噪过程。

Method: 构建基于Bernoulli边翻转的理论框架，并扩展到更复杂的结构-属性耦合噪声场景，在合成与真实数据集上进行实验。

Result: 无条件GDM性能与有条件模型相当或更优，同时减少了4-6%的参数与8-10%的计算时间。

Conclusion: 图像高维数据本身提供足够的去噪信息，简化GDM架构是可行的。

Abstract: Explicit noise-level conditioning is widely regarded as essential for the
effective operation of Graph Diffusion Models (GDMs). In this work, we
challenge this assumption by investigating whether denoisers can implicitly
infer noise levels directly from corrupted graph structures, potentially
eliminating the need for explicit noise conditioning. To this end, we develop a
theoretical framework centered on Bernoulli edge-flip corruptions and extend it
to encompass more complex scenarios involving coupled structure-attribute
noise. Extensive empirical evaluations on both synthetic and real-world graph
datasets, using models such as GDSS and DiGress, provide strong support for our
theoretical findings. Notably, unconditional GDMs achieve performance
comparable or superior to their conditioned counterparts, while also offering
reductions in parameters (4-6%) and computation time (8-10%). Our results
suggest that the high-dimensional nature of graph data itself often encodes
sufficient information for the denoising process, opening avenues for simpler,
more efficient GDM architectures.

</details>


### [144] [Directed Graph Grammars for Sequence-based Learning](https://arxiv.org/abs/2505.22949)
*Michael Sun,Orion Foo,Gang Liu,Wojciech Matusik,Jie Chen*

Key words: 有向无环图（DAG），文法，序列化，无损压缩，生成模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究提出了一种基于语法的有向无环图（DAG）序列化方法，通过将DAG视为无歧义文法的推导过程，实现紧凑且等效的序列表示。

Motivation: 由于DAG节点可以有多种拓扑顺序，现有的解码方法缺乏原则性，因此需要一种更有效和规范的表示方式。

Method: 采用基于语法的框架，将DAG转换为唯一的文法产生规则序列，实现无损压缩和序列化。

Result: 该方法能够用于图生成、属性预测和学习结构化数据的潜在空间表示。

Conclusion: 提出的方法不仅解决了DAG解码的挑战，还扩展了其在多种任务中的应用潜力。

Abstract: Directed acyclic graphs (DAGs) are a class of graphs commonly used in
practice, with examples that include electronic circuits, Bayesian networks,
and neural architectures. While many effective encoders exist for DAGs, it
remains challenging to decode them in a principled manner, because the nodes of
a DAG can have many different topological orders. In this work, we propose a
grammar-based approach to constructing a principled, compact and equivalent
sequential representation of a DAG. Specifically, we view a graph as
derivations over an unambiguous grammar, where the DAG corresponds to a unique
sequence of production rules. Equivalently, the procedure to construct such a
description can be viewed as a lossless compression of the data. Such a
representation has many uses, including building a generative model for graph
generation, learning a latent space for property prediction, and leveraging the
sequence representational continuity for Bayesian Optimization over structured
data. Code is available at https://github.com/shiningsunnyday/induction.

</details>


### [145] [MermaidFlow: Redefining Agentic Workflow Generation via Safety-Constrained Evolutionary Programming](https://arxiv.org/abs/2505.22967)
*Chengqi Zheng,Jianda Chen,Yueming Lyu,Wen Zheng Terence Ng,Haopeng Zhang,Yew-Soon Ong,Ivor Tsang,Haiyan Yin*

Key words: 自主代理推理, 工作流生成, 安全约束, 图进化, Mermaid

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: MermaidFlow 通过安全约束的图进化重构了自主代理推理的搜索空间，使用 Mermaid 作为一种可验证的中间表示，提高了工作流的成功率和执行效率。

Motivation: 现有的工作流生成方法由于不受约束的 LLM 驱动构造，经常产生脆弱且不可执行的计划，需要一种更可靠的方法来生成高质量的工作流。

Method: MermaidFlow 使用 Mermaid 作为可验证的中间表示，并设计了领域感知的进化算子（交叉、变异、插入和删除），以保持语义正确性并促进结构多样性。

Result: 实验结果显示，MermaidFlow 在代理推理基准测试中显著提高了成功率和收敛速度，无需修改任务设置或评估协议。

Conclusion: 安全约束的图进化为构建稳健且可解释的代理推理系统提供了可扩展的模块化基础。

Abstract: Despite the promise of autonomous agentic reasoning, existing workflow
generation methods frequently produce fragile, unexecutable plans due to
unconstrained LLM-driven construction. We introduce MermaidFlow, a framework
that redefines the agentic search space through safety-constrained graph
evolution. At its core, MermaidFlow represent workflows as a verifiable
intermediate representation using Mermaid, a structured and human-interpretable
graph language. We formulate domain-aware evolutionary operators, i.e.,
crossover, mutation, insertion, and deletion, to preserve semantic correctness
while promoting structural diversity, enabling efficient exploration of a
high-quality, statically verifiable workflow space. Without modifying task
settings or evaluation protocols, MermaidFlow achieves consistent improvements
in success rates and faster convergence to executable plans on the agent
reasoning benchmark. The experimental results demonstrate that
safety-constrained graph evolution offers a scalable, modular foundation for
robust and interpretable agentic reasoning systems.

</details>


### [146] [EquiReg: Equivariance Regularized Diffusion for Inverse Problems](https://arxiv.org/abs/2505.22973)
*Bahareh Tolooshams,Aditi Chandrashekar,Rayhan Zirvi,Abbas Mammadov,Jiachen Yao,Chuwei Wang,Anima Anandkumar*

Key words: 扩散模型,反向问题,图像修复,正则化,等变性误差

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了EquiReg框架，通过正则化扩散模型的后验采样来解决反向问题中的图像修复任务，提高了重建质量并优于现有方法。

Motivation: 现有的扩散模型在解决反向问题时，由于似然项难以处理而依赖高斯近似，导致偏离数据流形并产生不一致的重建结果。

Method: 提出EquiReg框架，通过重新加权扩散轨迹并惩罚偏离数据流形的样本，利用分布依赖的等变性误差来正则化采样过程。

Result: EquiReg在多种求解器中表现优异，在线性和非线性图像修复任务以及偏微分方程重建中均超越现有扩散模型。

Conclusion: EquiReg通过正则化采样显著提升了扩散模型在反向问题中的性能，验证了其通用性和有效性。

Abstract: Diffusion models represent the state-of-the-art for solving inverse problems
such as image restoration tasks. In the Bayesian framework, diffusion-based
inverse solvers incorporate a likelihood term to guide the prior sampling
process, generating data consistent with the posterior distribution. However,
due to the intractability of the likelihood term, many current methods rely on
isotropic Gaussian approximations, which lead to deviations from the data
manifold and result in inconsistent, unstable reconstructions. We propose
Equivariance Regularized (EquiReg) diffusion, a general framework for
regularizing posterior sampling in diffusion-based inverse problem solvers.
EquiReg enhances reconstructions by reweighting diffusion trajectories and
penalizing those that deviate from the data manifold. We define a new
distribution-dependent equivariance error, empirically identify functions that
exhibit low error for on-manifold samples and higher error for off-manifold
samples, and leverage these functions to regularize the diffusion sampling
process. When applied to a variety of solvers, EquiReg outperforms
state-of-the-art diffusion models in both linear and nonlinear image
restoration tasks, as well as in reconstructing partial differential equations.

</details>


### [147] [A Computational Approach to Improving Fairness in K-means Clustering](https://arxiv.org/abs/2505.22984)
*Guancheng Zhou,Haiping Xu,Hongkang Xu,Chenyu Li,Donghui Yan*

Key words: K-means聚类, 公平性, 两阶段优化, 敏感变量, 算法改进

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种两阶段优化方法，以解决K-means聚类算法在公平性问题上的缺陷，通过调整部分数据点的聚类成员资格来显著提升公平性，同时对聚类质量影响最小。

Motivation: K-means聚类可能在敏感变量（如性别或种族）上导致某些子群在聚类中分布不均，引发公平性问题。作者希望通过改进算法减少这种偏见和社会影响。

Method: 采用两阶段优化方法：先进行聚类，然后调整部分选定数据点的聚类成员。提出两种高效算法，分别关注聚类外最近点和高度“混合”的数据点。

Result: 在基准数据集上的实验表明，公平性显著提升，且对聚类质量影响极小。方法可扩展到多种聚类算法或公平性指标。

Conclusion: 所提方法能够有效提升K-means聚类的公平性，且可通过简单调整应用于更广泛的场景。

Abstract: The popular K-means clustering algorithm potentially suffers from a major
weakness for further analysis or interpretation. Some cluster may have
disproportionately more (or fewer) points from one of the subpopulations in
terms of some sensitive variable, e.g., gender or race. Such a fairness issue
may cause bias and unexpected social consequences. This work attempts to
improve the fairness of K-means clustering with a two-stage optimization
formulation--clustering first and then adjust cluster membership of a small
subset of selected data points. Two computationally efficient algorithms are
proposed in identifying those data points that are expensive for fairness, with
one focusing on nearest data points outside of a cluster and the other on
highly 'mixed' data points. Experiments on benchmark datasets show substantial
improvement on fairness with a minimal impact to clustering quality. The
proposed algorithms can be easily extended to a broad class of clustering
algorithms or fairness metrics.

</details>


### [148] [Knowledge Distillation for Reservoir-based Classifier: Human Activity Recognition](https://arxiv.org/abs/2505.22985)
*Masaharu Kagiyama,Tsuyoshi Okita*

Key words: human activity recognition, Echo State Network, knowledge distillation, edge computing, energy-efficient

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于Echo State Network (ESN)的新型模型PatchEchoClassifier，用于高效识别人类活动，通过知识蒸馏框架从高容量MLP-Mixer教师模型转移到轻量级学生模型，在多个人类活动识别数据集上实现超过80%准确率，计算成本显著降低。

Motivation: 开发一种适合边缘计算环境的实时、高能效人类活动识别方法。

Method: 引入PatchEchoClassifier模型，结合ESN和tokenizer提取补丁级特征，采用知识蒸馏从MLP-Mixer教师模型迁移知识到学生模型。

Result: 模型在多个数据集上达到超过80%的准确率，计算开销仅为DeepConvLSTM的六分之一。

Conclusion: PatchEchoClassifier是边缘计算环境中实时高效人类活动识别的有前景的解决方案。

Abstract: This paper aims to develop an energy-efficient classifier for time-series
data by introducing PatchEchoClassifier, a novel model that leverages a
reservoir-based mechanism known as the Echo State Network (ESN). The model is
designed for human activity recognition (HAR) using one-dimensional sensor
signals and incorporates a tokenizer to extract patch-level representations. To
train the model efficiently, we propose a knowledge distillation framework that
transfers knowledge from a high-capacity MLP-Mixer teacher to the lightweight
reservoir-based student model. Experimental evaluations on multiple HAR
datasets demonstrate that our model achieves over 80 percent accuracy while
significantly reducing computational cost. Notably, PatchEchoClassifier
requires only about one-sixth of the floating point operations (FLOPS) compared
to DeepConvLSTM, a widely used convolutional baseline. These results suggest
that PatchEchoClassifier is a promising solution for real-time and
energy-efficient human activity recognition in edge computing environments.

</details>


### [149] [Model-Preserving Adaptive Rounding](https://arxiv.org/abs/2505.22988)
*Albert Tseng,Zhaofeng Sun,Christopher De Sa*

Key words: 后训练量化,KL散度,Kronecker分解,Hessian矩阵,自适应舍入

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为YAQA的后训练量化算法，通过考虑全局KL散度的Hessian矩阵，改进了传统的局部最小化激活误差的方法，显著提升了模型压缩效果。

Motivation: 传统的后训练量化方法仅独立优化线性层的局部激活误差，忽略了后续层的影响，导致压缩模型与原始模型的输出分布仍有差距。本文旨在通过全局优化方法提升量化效果。

Method: YAQA算法采用Kronecker分解近似每层线性层的Hessian矩阵（针对全局KL散度），并设计了一种与量化器无关的舍入算法，具有理论保证且可扩展到百亿参数模型。

Result: 实验表明，YAQA在多种模型和量化器上，将原始模型的KL散度降低了约30%，并在下游任务中达到了SOTA性能。

Conclusion: 通过全局优化的Hessian矩阵近似和高效舍入算法，YAQA显著提升了后训练量化的效果，为大规模语言模型的高效压缩提供了新思路。

Abstract: The main goal of post-training quantization (PTQ) is to produced a compressed
model whose output distribution is as close to the original model's as
possible. To do this tractably, almost all LLM PTQ algorithms quantize linear
layers by independently minimizing the immediate activation error. However,
this localized objective ignores the effect of subsequent layers, so reducing
it does not necessarily give a closer model. In this work, we introduce Yet
Another Quantization Algorithm (YAQA), an adaptive rounding algorithm that uses
Kronecker-factored approximations of each linear layer's Hessian with respect
to the \textit{full model} KL divergence. YAQA consists of two components:
Kronecker-factored sketches of the full layerwise Hessian that can be tractably
computed for hundred-billion parameter LLMs, and a quantizer-independent
rounding algorithm that uses these sketches and comes with theoretical
guarantees. Across a wide range of models and quantizers, YAQA empirically
reduces the KL divergence to the original model by $\approx 30\%$ while
achieving state of the art performance on downstream tasks.

</details>


### [150] [Number of Clusters in a Dataset: A Regularized K-means Approach](https://arxiv.org/abs/2505.22991)
*Behzad Kamgar-Parsi,Behrooz Kamgar-Parsi*

Key words: 正则化k均值, 聚类数量, 超参数边界, 加性正则化, 乘性正则化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了正则化k均值算法中关键超参数λ的严格边界，假设聚类为理想状态。通过实验展示了加性和乘性正则化在减少多解歧义方面的效果。

Motivation: 在无标签数据集中确定有意义的聚类数量是重要问题，但目前缺乏设置正则化参数λ的指导原则。

Method: 假设聚类为理想状态（d维球体），推导λ的严格边界，并分析加性和乘性正则化的k均值算法。

Result: 实验表明加性正则化常导致多解，而加性与乘性正则化的共识可减少歧义。当聚类偏离理想假设时，算法仍表现良好。

Conclusion: 通过理论边界和实验验证，正则化k均值算法在确定聚类数量上具有潜力，尤其在减少多解方面。

Abstract: Finding the number of meaningful clusters in an unlabeled dataset is
important in many applications. Regularized k-means algorithm is a possible
approach frequently used to find the correct number of distinct clusters in
datasets. The most common formulation of the regularization function is the
additive linear term $\lambda k$, where $k$ is the number of clusters and
$\lambda$ a positive coefficient. Currently, there are no principled guidelines
for setting a value for the critical hyperparameter $\lambda$. In this paper,
we derive rigorous bounds for $\lambda$ assuming clusters are {\em ideal}.
Ideal clusters (defined as $d$-dimensional spheres with identical radii) are
close proxies for k-means clusters ($d$-dimensional spherically symmetric
distributions with identical standard deviations). Experiments show that the
k-means algorithm with additive regularizer often yields multiple solutions.
Thus, we also analyze k-means algorithm with multiplicative regularizer. The
consensus among k-means solutions with additive and multiplicative
regularizations reduces the ambiguity of multiple solutions in certain cases.
We also present selected experiments that demonstrate performance of the
regularized k-means algorithms as clusters deviate from the ideal assumption.

</details>


### [151] [Walking the Weight Manifold: a Topological Approach to Conditioning Inspired by Neuromodulation](https://arxiv.org/abs/2505.22994)
*Ari S. Benjamin,Kyle Daruwalla,Christian Pehle,Anthony M. Zador*

Key words: 神经调制,权重流形,任务共享,泛化能力,条件神经网络

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种受神经调节启发的权重学习方法，通过优化权重空间中的光滑流形来跨任务共享知识，优于传统的输入拼接方法。

Motivation: 现有的神经网络通常通过将任务上下文作为输入来学习相似任务，而大脑通过神经调节调制参数。论文受此启发，提出一种更高效的任务知识共享方法。

Method: 通过优化权重空间中的光滑流形（如直线、圆环等），将权重参数化为任务上下文的函数。训练时最小化损失函数并约束流形体积变化。

Result: 实验表明，该方法在多种流形拓扑结构（如直线、椭圆）下优于传统的输入拼接方法，并在分布外样本上表现更好。

Conclusion: 权重在低维流形上的调制是一种更有效和原理性的任务条件方法。

Abstract: One frequently wishes to learn a range of similar tasks as efficiently as
possible, re-using knowledge across tasks. In artificial neural networks, this
is typically accomplished by conditioning a network upon task context by
injecting context as input. Brains have a different strategy: the parameters
themselves are modulated as a function of various neuromodulators such as
serotonin. Here, we take inspiration from neuromodulation and propose to learn
weights which are smoothly parameterized functions of task context variables.
Rather than optimize a weight vector, i.e. a single point in weight space, we
optimize a smooth manifold in weight space with a predefined topology. To
accomplish this, we derive a formal treatment of optimization of manifolds as
the minimization of a loss functional subject to a constraint on volumetric
movement, analogous to gradient descent. During inference, conditioning selects
a single point on this manifold which serves as the effective weight matrix for
a particular sub-task. This strategy for conditioning has two main advantages.
First, the topology of the manifold (whether a line, circle, or torus) is a
convenient lever for inductive biases about the relationship between tasks.
Second, learning in one state smoothly affects the entire manifold, encouraging
generalization across states. To verify this, we train manifolds with several
topologies, including straight lines in weight space (for conditioning on e.g.
noise level in input data) and ellipses (for rotated images). Despite their
simplicity, these parameterizations outperform conditioning identical networks
by input concatenation and better generalize to out-of-distribution samples.
These results suggest that modulating weights over low-dimensional manifolds
offers a principled and effective alternative to traditional conditioning.

</details>


### [152] [LLM Agents for Bargaining with Utility-based Feedback](https://arxiv.org/abs/2505.22998)
*Jihwan Oh,Murad Aghazada,Se-Young Yun,Taehyeon Kim*

Key words: 谈判, 大语言模型, 效用理论, 对手感知推理, 反馈机制

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一个基于效用反馈的框架，包括新基准数据集BargainArena、人机对齐的评价指标和结构化反馈机制，显著提升了LLM在谈判中的表现。

Motivation: 现有基准未能捕捉真实谈判的复杂性，LLM在战略深度和适应复杂人类因素方面存在局限。

Method: 引入BargainArena数据集、基于效用理论的评价指标和结构化反馈机制，结合ICL提示。

Result: LLM的谈判策略常与人类偏好不一致，但结构化反馈机制显著提升了其表现和对手感知推理能力。

Conclusion: 提出的框架有效提升了LLM在真实谈判中的性能，促进了更深层次的战略推理。

Abstract: Bargaining, a critical aspect of real-world interactions, presents challenges
for large language models (LLMs) due to limitations in strategic depth and
adaptation to complex human factors. Existing benchmarks often fail to capture
this real-world complexity. To address this and enhance LLM capabilities in
realistic bargaining, we introduce a comprehensive framework centered on
utility-based feedback. Our contributions are threefold: (1) BargainArena, a
novel benchmark dataset with six intricate scenarios (e.g., deceptive
practices, monopolies) to facilitate diverse strategy modeling; (2)
human-aligned, economically-grounded evaluation metrics inspired by utility
theory, incorporating agent utility and negotiation power, which implicitly
reflect and promote opponent-aware reasoning (OAR); and (3) a structured
feedback mechanism enabling LLMs to iteratively refine their bargaining
strategies. This mechanism can positively collaborate with in-context learning
(ICL) prompts, including those explicitly designed to foster OAR. Experimental
results show that LLMs often exhibit negotiation strategies misaligned with
human preferences, and that our structured feedback mechanism significantly
improves their performance, yielding deeper strategic and opponent-aware
reasoning.

</details>


### [153] [Hybrid Cross-domain Robust Reinforcement Learning](https://arxiv.org/abs/2505.23003)
*Linh Le Pham Van,Minh Hoang Nguyen,Hung Le,Hung The Tran,Sunil Gupta*

Key words: 强化学习、鲁棒性、混合跨域、样本效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了HYDRO框架，首次将在线模拟器与离线数据集结合用于鲁棒强化学习，通过优化样本选择和降低模型性能差距，显著提升了样本效率和性能。

Motivation: 鲁棒强化学习在实际应用中面临环境不确定性，而传统离线方法需要大量数据集。模拟器虽可快速生成数据，但存在动力学失配问题。

Method: 提出HYDRO框架，利用在线模拟器补充离线数据，并通过不确定性过滤和优先级采样优化模拟器样本选择。

Result: 实验显示HYDRO在多种任务中优于现有方法，显著提升了样本效率和鲁棒性。

Conclusion: HYDRO为离线鲁棒强化学习提供了一种高效的解决方案，平衡了离线数据与模拟器的优势。

Abstract: Robust reinforcement learning (RL) aims to learn policies that remain
effective despite uncertainties in its environment, which frequently arise in
real-world applications due to variations in environment dynamics. The robust
RL methods learn a robust policy by maximizing value under the worst-case
models within a predefined uncertainty set. Offline robust RL algorithms are
particularly promising in scenarios where only a fixed dataset is available and
new data cannot be collected. However, these approaches often require extensive
offline data, and gathering such datasets for specific tasks in specific
environments can be both costly and time-consuming. Using an imperfect
simulator offers a faster, cheaper, and safer way to collect data for training,
but it can suffer from dynamics mismatch. In this paper, we introduce HYDRO,
the first Hybrid Cross-Domain Robust RL framework designed to address these
challenges. HYDRO utilizes an online simulator to complement the limited amount
of offline datasets in the non-trivial context of robust RL. By measuring and
minimizing performance gaps between the simulator and the worst-case models in
the uncertainty set, HYDRO employs novel uncertainty filtering and prioritized
sampling to select the most relevant and reliable simulator samples. Our
extensive experiments demonstrate HYDRO's superior performance over existing
methods across various tasks, underscoring its potential to improve sample
efficiency in offline robust RL.

</details>


### [154] [QLIP: A Dynamic Quadtree Vision Prior Enhances MLLM Performance Without Retraining](https://arxiv.org/abs/2505.23004)
*Kyle R. Chickering,Bangzheng Li,Muhao Chen*

Key words: QLIP, CLIP, 多模态大语言模型, 视觉编码器, 内容感知分块

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: QLIP是一种可无缝替代CLIP的视觉编码器，通过内容感知的图像四叉树结构解决了CLIP的局限性，无需重新训练即可提升多模态大语言模型的视觉理解能力。

Motivation: CLIP视觉编码器存在固定输入分辨率和无法区分不相似图像嵌入的限制，更换编码器通常需要高昂的计算成本。

Method: 提出QLIP，采用图像四叉树结构和内容感知的分块方法，替代CLIP的标准均匀网格分块。

Result: QLIP在LLaVA v1.5模型中显著提升了视觉问答准确率，尤其在V*基准上细粒度理解性能提升达13.6%。

Conclusion: QLIP作为CLIP的即插即用替代方案，有效解决了其局限性并提升了模型性能，无需完整模型重新训练。

Abstract: Multimodal Large Language Models (MLLMs) encode images into visual tokens,
aligning visual and textual signals within a shared latent space to facilitate
crossmodal representation learning. The CLIP model is a widely adopted
foundational vision language model whose vision encoder has played a critical
role in the development of MLLMs such as LLaVA. However, the CLIP vision
encoder suffers from notable limitations including being constrained to only
handling fixed input resolutions and a failure to produce separated embeddings
for dissimilar images. Replacing the vision encoder of an existing model
typically incurs substantial computational costs because such a change often
necessitates retraining the entire model pipeline.
  In this work, we identify two factors which underlie the limitations of the
CLIP vision encoder: mesoscopic bias and interpolation bias. To address these
issues, we propose QLIP, a drop-in replacement for CLIP that can be seamlessly
integrated with existing MLLMs with only a few lines of code and can enhance
both coarse-grained and fine-grained visual understanding, without re-training.
QLIP is designed around an image quadtree which replaces the standard uniform
grid patches with a novel content aware patchification. Our experimental
results demonstrate that QLIP improves the general visual question answering
accuracy of the LLaVA v1.5 model series across various model sizes--without
requiring retraining or fine-tuning of the full MLLM. Notably, QLIP boosts
detailed understanding performance on the challenging $V^{\ast}$ benchmark by
up to 13.6 percent.

</details>


### [155] [EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic, Expressiveness, and Linguistic Challenges Using Model-as-a-Judge](https://arxiv.org/abs/2505.23009)
*Ruskin Raj Manku,Yuzhi Tang,Xingjian Shi,Mu Li,Alex Smola*

Key words: TTS，基准测试，自动化评估，语音合成，模型评委

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文介绍了_EmergentTTS-Eval_，一个针对TTS模型的全面基准测试，覆盖六种复杂场景，并采用自动化生成与评估方法，展示了模型间的细粒度性能差异。

Motivation: 现有的TTS基准测试难以全面评估模型对复杂语义和细微文本的处理能力，因此需要更全面的评估框架。

Method: 通过从少量人工种子提示出发，利用LLM扩展生成多样化的测试用例，并采用模型作为评委（LALM）从多维度评估语音质量。

Result: 评估结果显示，模型评委方法能有效揭示TTS系统的性能差异，并与人类偏好高度一致。

Conclusion: _EmergentTTS-Eval_为TTS领域提供了可扩展且细粒度的评估工具，开源代码和数据集进一步促进了研究。

Abstract: Text-to-Speech (TTS) benchmarks often fail to capture how well models handle
nuanced and semantically complex text. Building on $\textit{EmergentTTS}$, we
introduce $\textit{EmergentTTS-Eval}$, a comprehensive benchmark covering six
challenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic
complexity, complex pronunciation (e.g. URLs, formulas), and questions.
Crucially, our framework automates both test-case generation and evaluation,
making the benchmark easily extensible. Starting from a small set of
human-written seed prompts, we iteratively extend them using LLMs to target
specific structural, phonetic and prosodic challenges, resulting in 1,645
diverse test cases. Moreover, we employ a model-as-a-judge approach, using a
Large Audio Language Model (LALM) to assess the speech across multiple
dimensions such as expressed emotion, prosodic, intonational, and pronunciation
accuracy. We evaluate state-of-the-art open-source and proprietary TTS systems,
such as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval,
demonstrating its ability to reveal fine-grained performance differences.
Results show that the model-as-a-judge approach offers robust TTS assessment
and a high correlation with human preferences. We open source the evaluation
$\href{https://github.com/boson-ai/EmergentTTS-Eval-public}{code}$ and the
$\href{https://huggingface.co/datasets/bosonai/EmergentTTS-Eval}{dataset}$.

</details>


### [156] [Scalable Complexity Control Facilitates Reasoning Ability of LLMs](https://arxiv.org/abs/2505.23013)
*Liangkai Hang,Junjie Yao,Zhiwei Bai,Tianyi Chen,Yang Chen,Rongjie Diao,Hezhou Li,Pengxiao Lin,Zhiwei Wang,Cheng Xu,Zhongwang Zhang,Zhangchen Zhou,Zhiyu Li,Zehao Lin,Kai Chen,Feiyu Xiong,Yaoyu Zhang,Weinan E,Hongkang Yang,Zhi-Qin John Xu*

Key words: 大型语言模型、模型复杂性、扩展规律、初始化率、权重衰减

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 通过调整初始化率和权重衰减系数控制模型复杂性，可以提升大型语言模型的扩展规律，从而在不同模型和数据规模下持续优化性能。

Motivation: 研究旨在通过更基础的方法可靠地增强大型语言模型的泛化能力，推动其持续发展。

Method: 采用模型复杂性控制策略，调整初始化率和权重衰减系数，并在不同规模模型和数据上进行测试。

Result: 实验证明恒定初始化率能让扩展规律在模型和数据规模上下降更快，2.4B参数模型在1T Token预训练数据上表现更优。

Conclusion: 复杂性控制是提升大型语言模型性能的一个有潜力方向。

Abstract: The reasoning ability of large language models (LLMs) has been rapidly
advancing in recent years, attracting interest in more fundamental approaches
that can reliably enhance their generalizability. This work demonstrates that
model complexity control, conveniently implementable by adjusting the
initialization rate and weight decay coefficient, improves the scaling law of
LLMs consistently over varying model sizes and data sizes. This gain is further
illustrated by comparing the benchmark performance of 2.4B models pretrained on
1T tokens with different complexity hyperparameters. Instead of fixing the
initialization std, we found that a constant initialization rate (the exponent
of std) enables the scaling law to descend faster in both model and data sizes.
These results indicate that complexity control is a promising direction for the
continual advancement of LLMs.

</details>


### [157] [Hyperbolic-PDE GNN: Spectral Graph Neural Networks in the Perspective of A System of Hyperbolic Partial Differential Equations](https://arxiv.org/abs/2505.23014)
*Juwei Yue,Haikuo Li,Jiawei Sheng,Xiaodong Li,Taoyu Su,Tingwen Liu,Li Guo*

Key words: 图神经网络,双曲偏微分方程,谱图神经网络,拓扑特征,消息传递

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 将图神经网络的消息传递机制建模为双曲偏微分方程系统，增强拓扑特征的显式提取，并与谱图神经网络结合，显著提升性能。

Motivation: 传统GNN在空间域学习节点特征，难以确保拓扑特征。本文旨在通过双曲PDE系统显式映射节点表示到解空间，提升解释性和拓扑特征提取能力。

Method: 将消息传递建模为双曲PDE系统，利用特征向量描述图拓扑结构，并通过多项式逼近任意滤波器函数。

Result: 实验表明，双曲PDE范式不仅具有强灵活性，还能显著提升多种谱GNN在不同图任务中的性能。

Conclusion: 通过双曲PDE系统实现消息传递的显式建模，不仅增强了拓扑特征提取和解释性，还为谱GNN提供了性能提升的新途径。

Abstract: Graph neural networks (GNNs) leverage message passing mechanisms to learn the
topological features of graph data. Traditional GNNs learns node features in a
spatial domain unrelated to the topology, which can hardly ensure topological
features. In this paper, we formulates message passing as a system of
hyperbolic partial differential equations (hyperbolic PDEs), constituting a
dynamical system that explicitly maps node representations into a particular
solution space. This solution space is spanned by a set of eigenvectors
describing the topological structure of graphs. Within this system, for any
moment in time, a node features can be decomposed into a superposition of the
basis of eigenvectors. This not only enhances the interpretability of message
passing but also enables the explicit extraction of fundamental characteristics
about the topological structure. Furthermore, by solving this system of
hyperbolic partial differential equations, we establish a connection with
spectral graph neural networks (spectral GNNs), serving as a message passing
enhancement paradigm for spectral GNNs.We further introduce polynomials to
approximate arbitrary filter functions. Extensive experiments demonstrate that
the paradigm of hyperbolic PDEs not only exhibits strong flexibility but also
significantly enhances the performance of various spectral GNNs across diverse
graph tasks.

</details>


### [158] [$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2505.23017)
*Xingjian Wu,Xiangfei Qiu,Hongfan Gao,Jilin Hu,Bin Yang,Chenjuan Guo*

Key words: 概率时间序列预测、长程预测、KoopmanNet、KalmanNet、VAE

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种名为$K^2$VAE的新型模型，用于解决长程概率时间序列预测（LPTSF）中非线性动态和误差积累的问题，并在实验中表现优于现有方法。

Motivation: 现有的时间序列预测方法在短期预测中表现出色，但在长程预测中由于非线性动态和误差积累的问题，性能和效率显著下降。本文旨在解决这些问题。

Method: 作者提出了$K^2$VAE模型，结合了KoopmanNet将非线性时间序列转为线性动态系统，并利用KalmanNet优化预测和不确定性建模。

Result: 实验结果表明，$K^2$VAE在短期和长期概率时间序列预测中均优于现有方法，提供了更高效和准确的解决方案。

Conclusion: $K^2$VAE通过将非线性动态转为线性系统和优化不确定性建模，显著提升了长程时间序列预测的性能和效率。

Abstract: Probabilistic Time Series Forecasting (PTSF) plays a crucial role in
decision-making across various fields, including economics, energy, and
transportation. Most existing methods excell at short-term forecasting, while
overlooking the hurdles of Long-term Probabilistic Time Series Forecasting
(LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have
a significant adverse effect on prediction accuracy, and make generative models
inefficient by increasing the cost of each iteration. To overcome these
limitations, we introduce $K^2$VAE, an efficient VAE-based generative model
that leverages a KoopmanNet to transform nonlinear time series into a linear
dynamical system, and devises a KalmanNet to refine predictions and model
uncertainty in such linear system, which reduces error accumulation in
long-term forecasting. Extensive experiments demonstrate that $K^2$VAE
outperforms state-of-the-art methods in both short- and long-term PTSF,
providing a more efficient and accurate solution.

</details>


### [159] [SCORPIO: Serving the Right Requests at the Right Time for Heterogeneous SLOs in LLM Inference](https://arxiv.org/abs/2505.23022)
*Yinghao Tang,Tingfeng Lan,Xiuqi Huang,Hui Lu,Wei Chen*

Key words: LLM, SLO, SCORPIO, TTFT, TPOT

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: SCORPIO是一个面向SLO的LLM服务系统，通过自适应调度和预测模块，显著提升了系统吞吐量和SLO达标率。

Motivation: 现有LLM服务系统过于关注吞吐量，忽略了TTFT和TPOT等SLO指标，导致SLO达成率不足。

Method: 提出了SCORPIO系统，包含TTFT Guard（基于截止时间重新排序和拒绝不可达请求）和TPOT Guard（基于VBS的准入控制和信用批处理机制）。

Result: 实验表明，SCORPIO将系统吞吐量提升14.4倍，SLO达标率提高46.5%。

Conclusion: SCORPIO通过自适应调度，成功解决了现有系统的SLO达成问题。

Abstract: Existing Large Language Model (LLM) serving systems prioritize maximum
throughput. They often neglect Service Level Objectives (SLOs) such as Time to
First Token (TTFT) and Time Per Output Token (TPOT), which leads to suboptimal
SLO attainment. This paper introduces SCORPIO, an SLO-oriented LLM serving
system designed to maximize system goodput and SLO attainment for workloads
with heterogeneous SLOs. Our core insight is to exploit SLO heterogeneity for
adaptive scheduling across admission control, queue management, and batch
selection. SCORPIO features a TTFT Guard, which employs least-deadline-first
reordering and rejects unattainable requests, and a TPOT Guard, which utilizes
a VBS-based admission control and a novel credit-based batching mechanism. Both
guards are supported by a predictive module. Evaluations demonstrate that
SCORPIO improves system goodput by up to 14.4X and SLO adherence by up to 46.5%
compared to state-of-the-art baselines.

</details>


### [160] [An Empirical Study of Federated Prompt Learning for Vision Language Model](https://arxiv.org/abs/2505.23024)
*Zhihao Wang,Wenke Huang,Tian Chen,Zekun Shi,Guancheng Wan,Yu Qiao,Bin Yang,Jian Wang,Bing Li,Mang Ye*

Key words: 视觉语言模型, 提示学习, 联邦学习, 数据异质性, 标签偏差, 领域偏移

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文探讨了在联邦学习环境下，语言提示学习（LPT）和视觉提示学习（VPT）在数据异质性挑战下的行为差异，并提出了优化策略，以促进视觉语言模型在隐私保护场景中的广泛应用。

Motivation: 研究动机在于探索视觉语言模型（VLM）在联邦学习（FL）场景中提示学习的应用潜力，特别是在数据异质性（如标签偏差和领域偏移）下的表现差异。

Method: 通过广泛的实验评估不同联邦学习和提示配置（如客户端规模、聚合策略和提示长度），并研究在标签偏差和领域偏移共存时的优化策略。

Result: 研究结果揭示了LPT和VPT在FL中的行为差异，并提出了在计算资源允许时联合使用两种提示类型的优化方案。

Conclusion: 该研究为联邦学习中提示学习的优化提供了实用见解，推动了视觉语言模型在隐私保护环境中的更广泛应用。

Abstract: The Vision Language Model (VLM) excels in aligning vision and language
representations, and prompt learning has emerged as a key technique for
adapting such models to downstream tasks. However, the application of prompt
learning with VLM in federated learning (\fl{}) scenarios remains
underexplored. This paper systematically investigates the behavioral
differences between language prompt learning (LPT) and vision prompt learning
(VPT) under data heterogeneity challenges, including label skew and domain
shift. We conduct extensive experiments to evaluate the impact of various \fl{}
and prompt configurations, such as client scale, aggregation strategies, and
prompt length, to assess the robustness of Federated Prompt Learning (FPL).
Furthermore, we explore strategies for enhancing prompt learning in complex
scenarios where label skew and domain shift coexist, including leveraging both
prompt types when computational resources allow. Our findings offer practical
insights into optimizing prompt learning in federated settings, contributing to
the broader deployment of VLMs in privacy-preserving environments.

</details>


### [161] [Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift](https://arxiv.org/abs/2505.23027)
*Minh Nguyen Nhat To,Paul F RWilson,Viet Nguyen,Mohamed Harmanani,Michael Cooper,Fahimeh Fooladgar,Purang Abolmaesumi,Parvin Mousavi,Rahul G. Krishnan*

Key words: 子群体偏移, 机器学习, 多样原型集成, 最差群体准确率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为DPE的方法，通过多样原型集成来应对子群体分布偏移问题，避免了传统方法对子群体数量与标注的依赖，并在多个数据集上取得了优于现有技术的表现。

Motivation: 子群体分布偏移会显著降低机器学习模型的性能，而现有方法依赖子群体的数量、性质和标注信息，这在现实数据中往往不可得。

Method: 使用多样原型分类器集成替代标准线性分类层，每个成员专注于不同特征和样本，以自适应捕捉子群体风险。

Result: 在九个真实数据集上的实验表明，DPE方法在多个领域和子群体偏移类型中，通常表现优于现有技术的‘最差群体准确率’。

Conclusion: DPE方法有效解决了子群体分布偏移问题，无需依赖子群体的先验信息，具有广泛适用性。

Abstract: The subpopulationtion shift, characterized by a disparity in subpopulation
distributibetween theween the training and target datasets, can significantly
degrade the performance of machine learning models. Current solutions to
subpopulation shift involve modifying empirical risk minimization with
re-weighting strategies to improve generalization. This strategy relies on
assumptions about the number and nature of subpopulations and annotations on
group membership, which are unavailable for many real-world datasets. Instead,
we propose using an ensemble of diverse classifiers to adaptively capture risk
associated with subpopulations. Given a feature extractor network, we replace
its standard linear classification layer with a mixture of prototypical
classifiers, where each member is trained to classify the data while focusing
on different features and samples from other members. In empirical evaluation
on nine real-world datasets, covering diverse domains and kinds of
subpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often
outperforms the prior state-of-the-art in worst-group accuracy. The code is
available at https://github.com/minhto2802/dpe4subpop

</details>


### [162] [Bayesian Neural Scaling Laws Extrapolation with Prior-Fitted Networks](https://arxiv.org/abs/2505.23032)
*Dongwoo Lee,Dong Bok Lee,Steven Adriaensen,Juho Lee,Sung Ju Hwang,Frank Hutter,Seon Joo Kim,Hae Beom Lee*

Key words: 深度学习,贝叶斯框架,神经缩放定律,不确定性量化,元学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于贝叶斯框架的Prior-data Fitted Networks（PFNs）方法，用于神经缩放定律的外推，通过设计先验分布生成合成函数，并在数据有限场景下验证了其优越性能。

Motivation: 现有方法多依赖点估计且缺乏不确定性量化，而实际决策（如计算资源投资）需要可靠的外推预测，因此提出贝叶斯框架以解决这一问题。

Method: 采用PFNs，设计先验分布生成合成函数，模拟真实神经缩放定律，实现元学习外推。

Result: 在真实神经缩放定律上的验证表明，该方法优于现有点估计和贝叶斯方法，尤其在数据有限场景（如贝叶斯主动学习）中表现突出。

Conclusion: 该方法提供了可靠性高且具有不确定性感知的外推能力，适用于实际应用中的决策问题。

Abstract: Scaling has been a major driver of recent advancements in deep learning.
Numerous empirical studies have found that scaling laws often follow the
power-law and proposed several variants of power-law functions to predict the
scaling behavior at larger scales. However, existing methods mostly rely on
point estimation and do not quantify uncertainty, which is crucial for
real-world applications involving decision-making problems such as determining
the expected performance improvements achievable by investing additional
computational resources. In this work, we explore a Bayesian framework based on
Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation.
Specifically, we design a prior distribution that enables the sampling of
infinitely many synthetic functions resembling real-world neural scaling laws,
allowing our PFN to meta-learn the extrapolation. We validate the effectiveness
of our approach on real-world neural scaling laws, comparing it against both
the existing point estimation methods and Bayesian approaches. Our method
demonstrates superior performance, particularly in data-limited scenarios such
as Bayesian active learning, underscoring its potential for reliable,
uncertainty-aware extrapolation in practical applications.

</details>


### [163] [From Theory to Application: Fine-Tuning Large EEG Model with Real-World Stress Data](https://arxiv.org/abs/2505.23042)
*Siwen Wang,Shitou Zhang,Wan-Lin Chen,Dung Truong,Tzyy-Ping Jung*

Key words: 大EEG模型（LEMs）、脑机接口、压力分类、真实环境EEG、LaBraM

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究通过微调先进的EEG基础模型LaBraM，在真实课堂环境中评估了大EEG模型（LEMs）的压力分类效果，实现了90.47%的平衡准确率，显著优于传统方法。

Motivation: 研究动机在于探索大EEG模型（LEMs）在真实环境中的适用性，而非仅限于临床控制场景，旨在推动脑机接口应用向数据为中心的设计转变。

Method: 研究采用LaBraM模型，在18名研究生的静息态EEG数据上微调二元分类器，区分正常与高压状态，并测试了数据随机打乱和通道减少的鲁棒性。

Result: 微调后的模型在5秒窗口下实现90.47%的平衡准确率，显著优于传统分类器，且在数据扰动下表现稳定。

Conclusion: LEMs能够高效处理真实环境EEG数据，为脑机接口应用从模型为中心转向数据为中心提供了可能。

Abstract: Recent advancements in Large Language Models have inspired the development of
foundation models across various domains. In this study, we evaluate the
efficacy of Large EEG Models (LEMs) by fine-tuning LaBraM, a state-of-the-art
foundation EEG model, on a real-world stress classification dataset collected
in a graduate classroom. Unlike previous studies that primarily evaluate LEMs
using data from controlled clinical settings, our work assesses their
applicability to real-world environments. We train a binary classifier that
distinguishes between normal and elevated stress states using resting-state EEG
data recorded from 18 graduate students during a class session. The
best-performing fine-tuned model achieves a balanced accuracy of 90.47% with a
5-second window, significantly outperforming traditional stress classifiers in
both accuracy and inference efficiency. We further evaluate the robustness of
the fine-tuned LEM under random data shuffling and reduced channel counts.
These results demonstrate the capability of LEMs to effectively process
real-world EEG data and highlight their potential to revolutionize
brain-computer interface applications by shifting the focus from model-centric
to data-centric design.

</details>


### [164] [ProDiff: Prototype-Guided Diffusion for Minimal Information Trajectory Imputation](https://arxiv.org/abs/2505.23048)
*Tianci Bu,Le Zhou,Wenchuan Yang,Jianhong Mou,Kang Yang,Suoyi Tan,Feng Yao,Jingyuan Wang,Xin Lu*

Key words: 轨迹补全、原型学习、扩散模型、时空重建、数据增强

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出ProDiff框架，仅用两个端点信息实现轨迹补全，结合原型学习和去噪扩散模型，效果优于现有方法。

Motivation: 现有轨迹补全方法依赖稀疏轨迹信息，假设数据保留关键行为模式，但对数据获取要求高且忽略大规模轨迹嵌入的潜力。ProDiff旨在以最小端点信息实现高效补全。

Method: 结合原型学习嵌入人类移动模式，利用去噪扩散概率模型进行时空重建，并通过定制损失函数联合训练。

Result: 在FourSquare和WuXi数据集上分别提升6.28%和2.52%的准确率，生成轨迹与真实轨迹相关性达0.927。

Conclusion: ProDiff通过最小化输入需求并有效利用轨迹嵌入，显著提升了轨迹补全的准确性和鲁棒性。

Abstract: Trajectory data is crucial for various applications but often suffers from
incompleteness due to device limitations and diverse collection scenarios.
Existing imputation methods rely on sparse trajectory or travel information,
such as velocity, to infer missing points. However, these approaches assume
that sparse trajectories retain essential behavioral patterns, which place
significant demands on data acquisition and overlook the potential of
large-scale human trajectory embeddings. To address this, we propose ProDiff, a
trajectory imputation framework that uses only two endpoints as minimal
information. It integrates prototype learning to embed human movement patterns
and a denoising diffusion probabilistic model for robust spatiotemporal
reconstruction. Joint training with a tailored loss function ensures effective
imputation. ProDiff outperforms state-of-the-art methods, improving accuracy by
6.28\% on FourSquare and 2.52\% on WuXi. Further analysis shows a 0.927
correlation between generated and real trajectories, demonstrating the
effectiveness of our approach.

</details>


### [165] [DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration](https://arxiv.org/abs/2505.23049)
*Tianteng Gu,Bei Liu,Bo Xiao,Ke Zeng,Jiacheng Liu,Yanmin Qian*

Key words: 剪枝、语言模型、半结构化稀疏、DenoiseRotator、正交变换

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出DenoiseRotator方法，通过重分布参数重要性增强剪枝鲁棒性，减少模型性能下降。

Motivation: 传统剪枝方法仅关注权重重要性估计，导致性能下降严重，尤其在半结构化稀疏约束下。

Method: 通过最小化归一化重要性分数的信息熵，集中重要性到更小子集，并结合可学习正交变换（DenoiseRotator）。

Result: 在LLaMA3等模型上，50%非结构化和2:4半结构化稀疏下，困惑度和零样本准确率显著提升。

Conclusion: DenoiseRotator提升剪枝鲁棒性，可无缝整合现有剪枝方法，减少性能退化。

Abstract: Pruning is a widely used technique to compress large language models (LLMs)
by removing unimportant weights, but it often suffers from significant
performance degradation - especially under semi-structured sparsity
constraints. Existing pruning methods primarily focus on estimating the
importance of individual weights, which limits their ability to preserve
critical capabilities of the model. In this work, we propose a new perspective:
rather than merely selecting which weights to prune, we first redistribute
parameter importance to make the model inherently more amenable to pruning. By
minimizing the information entropy of normalized importance scores, our
approach concentrates importance onto a smaller subset of weights, thereby
enhancing pruning robustness. We instantiate this idea through DenoiseRotator,
which applies learnable orthogonal transformations to the model's weight
matrices. Our method is model-agnostic and can be seamlessly integrated with
existing pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated
on LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4
semi-structured sparsity, DenoiseRotator consistently improves perplexity and
zero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4
semi-structured sparsity, DenoiseRotator reduces the perplexity gap to the
dense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are
available at https://github.com/Axel-gu/DenoiseRotator.

</details>


### [166] [CDR-Agent: Intelligent Selection and Execution of Clinical Decision Rules Using Large Language Model Agents](https://arxiv.org/abs/2505.23055)
*Zhen Xiang,Aliyah R. Hsu,Austin V. Zane,Aaron E. Kornblith,Margaret J. Lin-Martore,Jasmanpreet C. Kaur,Vasuda M. Dokiparthi,Bo Li,Bin Yu*

Key words: 临床决策规则, LLM, 急诊科, 自主决策, 医疗AI

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: CDR-Agent是一种基于LLM的系统，旨在通过自动识别和应用临床决策规则来提升急诊科的决策效率，相比传统LLM方法，其准确率和效率均有显著提升。

Motivation: 急诊科临床决策复杂且高风险，现有的临床决策规则（CDRs）因医生认知负荷难以快速应用。CDR-Agent旨在解决这一问题。

Method: 通过基于LLM的CDR-Agent系统，从非结构化临床笔记中自主识别并应用合适的CDRs，并使用合成和CDR-Bench数据集进行验证。

Result: CDR-Agent在CDR选择上相比基线LLM提升了56.3%（合成数据）和8.7%（CDR-Bench），并在减少计算开销和优化影像决策方面表现突出。

Conclusion: CDR-Agent在提升急诊决策效率和准确性上显示出潜力，尤其适用于高负载的临床环境。

Abstract: Clinical decision-making is inherently complex and fast-paced, particularly
in emergency departments (EDs) where critical, rapid and high-stakes decisions
are made. Clinical Decision Rules (CDRs) are standardized evidence-based tools
that combine signs, symptoms, and clinical variables into decision trees to
make consistent and accurate diagnoses. CDR usage is often hindered by the
clinician's cognitive load, limiting their ability to quickly recall and apply
the appropriate rules. We introduce CDR-Agent, a novel LLM-based system
designed to enhance ED decision-making by autonomously identifying and applying
the most appropriate CDRs based on unstructured clinical notes. To validate
CDR-Agent, we curated two novel ED datasets: synthetic and CDR-Bench, although
CDR-Agent is applicable to non ED clinics. CDR-Agent achieves a 56.3\%
(synthetic) and 8.7\% (CDR-Bench) accuracy gain relative to the standalone LLM
baseline in CDR selection. Moreover, CDR-Agent significantly reduces
computational overhead. Using these datasets, we demonstrated that CDR-Agent
not only selects relevant CDRs efficiently, but makes cautious yet effective
imaging decisions by minimizing unnecessary interventions while successfully
identifying most positively diagnosed cases, outperforming traditional LLM
prompting approaches. Code for our work can be found at:
https://github.com/zhenxianglance/medagent-cdr-agent

</details>


### [167] [DINGO: Constrained Inference for Diffusion LLMs](https://arxiv.org/abs/2505.23061)
*Tarun Suresh,Debangshu Banerjee,Shubham Ugare,Sasa Misailovic,Gagandeep Singh*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出DINGO方法，一种基于动态规划的受约束解码策略，用于解决扩散LLMs无法保证满足形式约束的问题。

Motivation: 扩散LLMs虽在运行时效率上优于自回归LLMs，但无法可靠满足用户指定的形式约束（如正则表达式），限制了其在结构化输出任务（如固定模式JSON生成）中的应用。

Method: 采用动态规划策略（DINGO）进行解码，确保在模型预测分布下严格满足任何用户指定的正则表达式，并高效保留真实输出分布。

Result: 在符号数学和JSON生成基准测试中，DINGO相比无约束推理性能提升高达68个百分点。

Conclusion: DINGO有效解决了扩散LLMs的约束解码问题，显著提升了其在结构化输出任务中的可靠性和性能。

Abstract: Diffusion LLMs have emerged as a promising alternative to conventional
autoregressive LLMs, offering significant potential for improved runtime
efficiency. However, existing diffusion models lack the ability to provably
enforce user-specified formal constraints, such as regular expressions, which
makes them unreliable for tasks that require structured outputs, such as
fixed-schema JSON generation. Unlike autoregressive models that generate tokens
sequentially, diffusion LLMs predict a block of tokens in parallel. This
parallelism makes traditional constrained decoding algorithms, which are
designed for sequential token prediction, ineffective at preserving the true
output distribution. To address this limitation, we propose DINGO, a dynamic
programming-based constrained decoding strategy that is both efficient and
provably distribution-preserving. DINGO enables sampling of output strings with
the highest probability under the model's predicted distribution, while
strictly satisfying any user-specified regular expression. On standard symbolic
math and JSON generation benchmarks, DINGO achieves up to a 68 percentage point
improvement over unconstrained inference

</details>


### [168] [Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data](https://arxiv.org/abs/2505.23062)
*Lingkai Kong,Haichuan Wang,Tonghan Wang,Guojun Xiong,Milind Tambe*

Key words: 强化学习，样本效率，最优传输，动态差异，流匹配

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: CompFlow 通过在源和目标环境之间引入基于流匹配和最优传输的理论框架来提高强化学习的样本效率，优于现有方法。

Motivation: 现有方法在处理源和目标环境动态差异时往往依赖于 KL 发散或互信息，而这些度量在不重叠支持的情况下可能存在问题，因此需要更稳健的解决方案。

Method: CompFlow 利用流匹配和最优传输的理论，将目标动态建模为基于源动态的条件流，通过 Wasserstein 距离估计动态差异。

Result: CompFlow 在几个动态变化的强化学习基准测试中表现优于基线方法。

Conclusion: CompFlow 提供了一种更稳健的方法来处理源和目标环境之间的动态差异，并通过乐观数据收集策略提高了学习效率。

Abstract: Incorporating pre-collected offline data from a source environment can
significantly improve the sample efficiency of reinforcement learning (RL), but
this benefit is often challenged by discrepancies between the transition
dynamics of the source and target environments. Existing methods typically
address this issue by penalizing or filtering out source transitions in high
dynamics-gap regions. However, their estimation of the dynamics gap often
relies on KL divergence or mutual information, which can be ill-defined when
the source and target dynamics have disjoint support. To overcome these
limitations, we propose CompFlow, a method grounded in the theoretical
connection between flow matching and optimal transport. Specifically, we model
the target dynamics as a conditional flow built upon the output distribution of
the source-domain flow, rather than learning it directly from a Gaussian prior.
This composite structure offers two key advantages: (1) improved generalization
for learning target dynamics, and (2) a principled estimation of the dynamics
gap via the Wasserstein distance between source and target transitions.
Leveraging our principled estimation of the dynamics gap, we further introduce
an optimistic active data collection strategy that prioritizes exploration in
regions of high dynamics gap, and theoretically prove that it reduces the
performance disparity with the optimal policy. Empirically, CompFlow
outperforms strong baselines across several RL benchmarks with shifted
dynamics.

</details>


### [169] [Loss-Guided Model Sharing and Local Learning Correction in Decentralized Federated Learning for Crop Disease Classification](https://arxiv.org/abs/2505.23063)
*Denis Mamba Kabala,Adel Hafiane,Laurent Bobelin,Raphael Canals*

Key words: 分散式联邦学习、作物病害分类、验证损失、深度学习、隐私保护

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新型的分散式联邦学习（DFL）框架，通过验证损失（Loss_val）指导模型共享并优化局部训练，提升了农业病害分类的准确性、收敛速度和鲁棒性。

Motivation: 农业中作物病害检测对生产力和食品安全至关重要，但现有深度学习模型因数据隐私问题难以大规模部署，且中心化联邦学习存在单点故障和扩展性限制。

Method: 采用分散式联邦学习框架，利用验证损失（Loss_val）控制模型共享和局部训练的损失函数权重参数，测试了三种深度学习架构（ResNet50、VGG16、ViT_B16）。

Result: 实验表明，DFL在PlantVillage数据集上提升了准确率和收敛速度，同时增强了模型在异构数据环境中的泛化能力和鲁棒性。

Conclusion: DFL框架在保护隐私的农业应用中表现优越，解决了中心化联邦学习的局限性。

Abstract: Crop disease detection and classification is a critical challenge in
agriculture, with major implications for productivity, food security, and
environmental sustainability. While deep learning models such as CNN and ViT
have shown excellent performance in classifying plant diseases from images,
their large-scale deployment is often limited by data privacy concerns.
Federated Learning (FL) addresses this issue, but centralized FL remains
vulnerable to single-point failures and scalability limits. In this paper, we
introduce a novel Decentralized Federated Learning (DFL) framework that uses
validation loss (Loss_val) both to guide model sharing between peers and to
correct local training via an adaptive loss function controlled by weighting
parameter. We conduct extensive experiments using PlantVillage datasets with
three deep learning architectures (ResNet50, VGG16, and ViT_B16), analyzing the
impact of weighting parameter, the number of shared models, the number of
clients, and the use of Loss_val versus Loss_train of other clients. Results
demonstrate that our DFL approach not only improves accuracy and convergence
speed, but also ensures better generalization and robustness across
heterogeneous data environments making it particularly well-suited for
privacy-preserving agricultural applications.

</details>


### [170] [Multi-Modal Learning with Bayesian-Oriented Gradient Calibration](https://arxiv.org/abs/2505.23071)
*Peizheng Guo,Jingyao Wang,Huijie Guo,Jiangmeng Li,Chuxiong Sun,Changwen Zheng,Wenwen Qiang*

Key words: 多模态学习, 梯度校准, 贝叶斯方法, Dempster-Shafer规则, 不确定性建模

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 摘要提出了一种基于贝叶斯导向的梯度校准方法（BOGC-MML），用于解决多模态学习中梯度不确定性的问题，通过量化模态置信度并加权聚合来优化模型。

Motivation: 现有多模态学习方法通常采用固定权重梯度聚合，忽略了模态的梯度不确定性，可能导致敏感维度过度更新或非敏感维度更新不足。

Method: 提出BOGC-MML方法，将模态梯度建模为随机变量并推导其概率分布，通过量化梯度分布的精度生成置信度证据，结合Dempster-Shafer规则加权聚合梯度。

Result: 在多个基准数据集上的实验验证了该方法的有效性和优势。

Conclusion: BOGC-MML通过显式建模梯度不确定性，实现了更平衡的梯度更新方向，提升了多模态学习性能。

Abstract: Multi-Modal Learning (MML) integrates information from diverse modalities to
improve predictive accuracy. However, existing methods mainly aggregate
gradients with fixed weights and treat all dimensions equally, overlooking the
intrinsic gradient uncertainty of each modality. This may lead to (i) excessive
updates in sensitive dimensions, degrading performance, and (ii) insufficient
updates in less sensitive dimensions, hindering learning. To address this
issue, we propose BOGC-MML, a Bayesian-Oriented Gradient Calibration method for
MML to explicitly model the gradient uncertainty and guide the model
optimization towards the optimal direction. Specifically, we first model each
modality's gradient as a random variable and derive its probability
distribution, capturing the full uncertainty in the gradient space. Then, we
propose an effective method that converts the precision (inverse variance) of
each gradient distribution into a scalar evidence. This evidence quantifies the
confidence of each modality in every gradient dimension. Using these evidences,
we explicitly quantify per-dimension uncertainties and fuse them via a reduced
Dempster-Shafer rule. The resulting uncertainty-weighted aggregation produces a
calibrated update direction that balances sensitivity and conservatism across
dimensions. Extensive experiments on multiple benchmark datasets demonstrate
the effectiveness and advantages of the proposed method.

</details>


### [171] [Gradient Boosting Decision Tree with LSTM for Investment Prediction](https://arxiv.org/abs/2505.23084)
*Chang Yu,Fang Liu,Jie Zhu,Shaobo Guo,Yifan Gao,Zhongheng Yang,Meiwei Liu,Qianwen Xing*

Key words: 股票预测,LSTM,LightGBM,CatBoost,集成学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出一种结合LSTM、LightGBM和CatBoost的混合框架，用于股票价格预测，并通过七种模型评估性能，最终通过集成模型提升准确率10%至15%。

Motivation: 为提高股票价格预测的准确性，研究结合了序列模型与树模型的长处，以应对市场变化的挑战。

Method: 使用LSTM网络结合LightGBM和CatBoost处理时间序列数据，并通过七种模型进行性能评估，最终构建集成模型。

Result: 实验表明，集成模型比单一模型准确率提高10%至15%，并有效降低市场波动时的误差。

Conclusion: 研究表明集成方法在金融预测中潜力巨大，并提供了灵活的设计以整合新技术。

Abstract: This paper proposes a hybrid framework combining LSTM (Long Short-Term
Memory) networks with LightGBM and CatBoost for stock price prediction. The
framework processes time-series financial data and evaluates performance using
seven models: Artificial Neural Networks (ANNs), Convolutional Neural Networks
(CNNs), Bidirectional LSTM (BiLSTM), vanilla LSTM, XGBoost, LightGBM, and
standard Neural Networks (NNs). Key metrics, including MAE, R-squared, MSE, and
RMSE, are used to establish benchmarks across different time scales.
  Building on these benchmarks, we develop an ensemble model that combines the
strengths of sequential and tree-based approaches. Experimental results show
that the proposed framework improves accuracy by 10 to 15 percent compared to
individual models and reduces error during market changes. This study
highlights the potential of ensemble methods for financial forecasting and
provides a flexible design for integrating new machine learning techniques.

</details>


### [172] [Equivariant Spherical Transformer for Efficient Molecular Modeling](https://arxiv.org/abs/2505.23086)
*Junyi An,Xinyu Lu,Chao Qu,Yunfei Shi,Peijia Lin,Qianwei Tang,Licheng Xu,Fenglei Cao,Yuan Qi*

Key words: 等变变换器、图神经网络、分子建模、SE(3)-等变性、傅里叶变换

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为“等变球面变换器（EST）”的新方法，通过结合Transformer架构和傅里叶变换后的群表示空间，解决了现有SE(3)-等变图神经网络表达能力的不足，并在多个分子数据集上取得最先进性能。

Motivation: SE(3)-等变图神经网络（GNNs）在分子系统建模中的应用受限于其张量积卷积的非线性不足和群表示不完整，表达力受限。为了解决这些问题，提出了一种新的方法来提升模型的表达能力。

Method: 提出了一种名为“等变球面变换器（EST）”的新型框架，利用Transformer结构在群表示的傅里叶变换后的空间域中进行建模。通过均匀采样的傅里叶变换策略确保等变性，同时提升网络的表现力。

Result: 实验证明，EST不仅可以覆盖张量积卷积的函数空间，还具有更强的表达能力，并在OC20和QM9等多个分子基准测试中实现了最先进的性能。

Conclusion: EST通过结合Transformer与傅里叶变换后的群表示，显著提升了SE(3)-等变模型的表达能力，为解决分子系统建模中的现有局限提供了有效工具。

Abstract: SE(3)-equivariant Graph Neural Networks (GNNs) have significantly advanced
molecular system modeling by employing group representations. However, their
message passing processes, which rely on tensor product-based convolutions, are
limited by insufficient non-linearity and incomplete group representations,
thereby restricting expressiveness. To overcome these limitations, we introduce
the Equivariant Spherical Transformer (EST), a novel framework that leverages a
Transformer structure within the spatial domain of group representations after
Fourier transform. We theoretically and empirically demonstrate that EST can
encompass the function space of tensor products while achieving superior
expressiveness. Furthermore, EST's equivariant inductive bias is guaranteed
through a uniform sampling strategy for the Fourier transform. Our experiments
demonstrate state-of-the-art performance by EST on various molecular
benchmarks, including OC20 and QM9.

</details>


### [173] [MAP: Revisiting Weight Decomposition for Low-Rank Adaptation](https://arxiv.org/abs/2505.23094)
*Chongjie Si,Zhiyi Shi,Yadao Wang,Xiaokang Yang,Susanto Rahardja,Wei Shen*

Key words: 参数高效微调，MAP，方向与幅度解耦，LoRA，权重适应

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为MAP的新型参数高效微调框架，通过解耦权重矩阵的方向和幅度适应，提升了现有方法的性能和可解释性。

Motivation: 当前参数高效微调（PEFT）方法（如LoRA）在方向定义上缺乏几何理论支持，限制了其灵活性和性能。

Method: MAP将权重矩阵重构为高维向量，并解耦其方向与幅度适应，通过归一化预训练权重、学习方向更新及引入标量系数独立调整幅度。

Result: 实验表明，MAP与现有PEFT方法结合后性能显著提升，且设计简单通用。

Conclusion: MAP可作为未来PEFT方法的默认设计，兼具可解释性与灵活性。

Abstract: The rapid development of large language models has revolutionized natural
language processing, but their fine-tuning remains computationally expensive,
hindering broad deployment. Parameter-efficient fine-tuning (PEFT) methods,
such as LoRA, have emerged as solutions. Recent work like DoRA attempts to
further decompose weight adaptation into direction and magnitude components.
However, existing formulations often define direction heuristically at the
column level, lacking a principled geometric foundation. In this paper, we
propose MAP, a novel framework that reformulates weight matrices as
high-dimensional vectors and decouples their adaptation into direction and
magnitude in a rigorous manner. MAP normalizes the pre-trained weights, learns
a directional update, and introduces two scalar coefficients to independently
scale the magnitude of the base and update vectors. This design enables more
interpretable and flexible adaptation, and can be seamlessly integrated into
existing PEFT methods. Extensive experiments show that MAP significantly
improves performance when coupling with existing methods, offering a simple yet
powerful enhancement to existing PEFT methods. Given the universality and
simplicity of MAP, we hope it can serve as a default setting for designing
future PEFT methods.

</details>


### [174] [Learning to Search for Vehicle Routing with Multiple Time Windows](https://arxiv.org/abs/2505.23098)
*Kuan Xu,Zhiguang Cao,Chenlong Zheng,Linong Liu*

Key words: 强化学习，自适应变量邻域搜索，车辆路径问题，多时间窗，Transformer

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 文章提出了一种基于强化学习的自适应变量邻域搜索方法（RL-AVNS），用于解决多时间窗车辆路径问题（VRPMTW），通过动态选择算子和改进解的适应性，显著优于传统方法。

Motivation: 传统自适应方法仅依赖历史算子性能，无法实时调整策略，而VRPMTW问题在复杂物流场景中需要更高的灵活性和效率。

Method: 结合强化学习框架动态选择算子，引入量化客户时间灵活性的指标改进邻域搜索，并使用基于Transformer的策略网络指导算子选择。

Result: RL-AVNS在解质量和计算效率上显著优于传统VNS、AVNS及最新学习启发式方法，并在未训练实例中展现了良好的泛化能力。

Conclusion: RL-AVNS为复杂物流问题提供了高效且通用的解决方案，尤其适用于多时间窗聚类场景。

Abstract: In this study, we propose a reinforcement learning-based adaptive variable
neighborhood search (RL-AVNS) method designed for effectively solving the
Vehicle Routing Problem with Multiple Time Windows (VRPMTW). Unlike traditional
adaptive approaches that rely solely on historical operator performance, our
method integrates a reinforcement learning framework to dynamically select
neighborhood operators based on real-time solution states and learned
experience. We introduce a fitness metric that quantifies customers' temporal
flexibility to improve the shaking phase, and employ a transformer-based neural
policy network to intelligently guide operator selection during the local
search. Extensive computational experiments are conducted on realistic
scenarios derived from the replenishment of unmanned vending machines,
characterized by multiple clustered replenishment windows. Results demonstrate
that RL-AVNS significantly outperforms traditional variable neighborhood search
(VNS), adaptive VNS (AVNS), and state-of-the-art learning-based heuristics,
achieving substantial improvements in solution quality and computational
efficiency across various instance scales and time window complexities.
Particularly notable is the algorithm's capability to generalize effectively to
problem instances not encountered during training, underscoring its practical
utility for complex logistics scenarios.

</details>


### [175] [Weight Spectra Induced Efficient Model Adaptation](https://arxiv.org/abs/2505.23099)
*Chongjie Si,Xuankun Yang,Muqing Liu,Yadao Wang,Xiaokang Yang,Wenbo Su,Bo Zheng,Wei Shen*

Key words: 大模型微调, PEFT, SVD, 奇异值缩放, 结构分析

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文通过SVD分析权重矩阵在微调时的结构变化，提出利用可学习的奇异值缩放方法，实现高效且精准的参数调整，显著优于现有基线。

Motivation: 大模型微调计算成本高，现有PEFT方法（如LoRA）虽有效但机制不明，需深入探讨微调对权重结构的改变。

Method: 采用SVD分析权重矩阵，发现微调主要放大顶部奇异值并重定向主导奇异向量，据此提出可学习的奇异值缩放方法。

Result: 该方法在多个任务中表现优于基线，验证了基于结构信息的微调有效性。

Conclusion: 通过结构分析指导的微调方法高效且精准，为PEFT研究提供了新思路。

Abstract: Large-scale foundation models have demonstrated remarkable versatility across
a wide range of downstream tasks. However, fully fine-tuning these models
incurs prohibitive computational costs, motivating the development of
Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA, which introduces
low-rank updates to pre-trained weights. Despite their empirical success, the
underlying mechanisms by which PEFT modifies model parameters remain
underexplored. In this work, we present a systematic investigation into the
structural changes of weight matrices during fully fine-tuning. Through
singular value decomposition (SVD), we reveal that fine-tuning predominantly
amplifies the top singular values while leaving the remainder largely intact,
suggesting that task-specific knowledge is injected into a low-dimensional
subspace. Furthermore, we find that the dominant singular vectors are
reoriented in task-specific directions, whereas the non-dominant subspace
remains stable. Building on these insights, we propose a novel method that
leverages learnable rescaling of top singular directions, enabling precise
modulation of the most influential components without disrupting the global
structure. Our approach achieves consistent improvements over strong baselines
across multiple tasks, highlighting the efficacy of structurally informed
fine-tuning.

</details>


### [176] [LUMION: Fast Fault Recovery for ML Jobs Using Programmable Optical Fabrics](https://arxiv.org/abs/2505.23105)
*Abhishek Vijaya Kumar,Eric Ding,Arjun Devraj,Darius Bunandar,Rachee Singh*

Key words: ML数据中心, 加速器容错, 光互连, 动态资源整合, LUMION

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出LUMION，一种可重构光互连架构，动态替换故障加速器避免迁移整个ML任务，提升资源利用率和性能。

Motivation: 现代ML数据中心中，加速器故障时迁移整个任务的效率低下且资源浪费，需优化故障恢复机制。

Method: 设计LUMION光互连架构，动态整合备用加速器到运行中的负载，取代传统整机架迁移。

Result: 原型实验显示，故障替换耗时约1秒，带宽优于电互连机架，微调吞吐量提升近2倍。

Conclusion: LUMION通过光互连动态容错，显著提升资源效率和ML任务性能。

Abstract: When accelerators fail in modern ML datacenters, operators migrate the
affected ML training or inference jobs to entirely new racks. This approach,
while preserving network performance, is highly inefficient, requiring
datacenters to reserve full racks of idle accelerators for fault tolerance. In
this paper, we address this resource inefficiency by introducing LUMION, a
novel reconfigurable optical fabric for connecting accelerators within a
datacenter rack. Instead of migrating entire ML jobs, LUMION dynamically
integrates spare accelerators into ongoing workloads as failures occur, thereby
maintaining consistent performance without costly migrations. We show the
benefits of LUMION by building an end-to-end hardware prototype. Our
experiments fine-tune Llama 3.2 and show that LUMION swaps a failed GPU with a
healthy one and restarts the ML job within ~ 1 second of the failure. LUMION
achieves higher inter-GPU bandwidth compared to traditional electrical racks
after replacing failed accelerators with spare ones, leading to nearly 2X
improvement in fine-tuning throughput.

</details>


### [177] [Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery](https://arxiv.org/abs/2505.23106)
*Ning Liu,Yue Yu*

Key words: 注意力机制、神经算子、PDE问题、傅里叶变换、可扩展学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为NIPS的新型神经算子架构，通过线性注意力机制提升预测精度和计算效率，无需显式计算大规模交互，显著提升了物理学习的可扩展性和可解释性。

Motivation: 注意力机制在AI核心领域如自然语言处理和计算机视觉中表现出色，但在复杂物理系统建模中的潜力尚未充分探索。学习此类系统通常需通过有限函数对实例发现映射算子，这是典型的病态逆PDE问题。

Method: NIPS架构基于非局部注意力算子（NAO），采用线性注意力机制实现可扩展学习，并集成了可学习核网络，作为傅里叶空间中的通道无关卷积，避免显式计算和存储大规模交互。

Result: 实验表明，NIPS在多个基准测试中优于NAO和其他基线方法，实现了可扩展、可解释且高效的物理学习。

Conclusion: NIPS在复杂物理系统建模中表现出显著优势，为未来研究提供了新方向。

Abstract: Attention mechanisms have emerged as transformative tools in core AI domains
such as natural language processing and computer vision. Yet, their largely
untapped potential for modeling intricate physical systems presents a
compelling frontier. Learning such systems often entails discovering operators
that map between functional spaces using limited instances of function pairs --
a task commonly framed as a severely ill-posed inverse PDE problem. In this
work, we introduce Neural Interpretable PDEs (NIPS), a novel neural operator
architecture that builds upon and enhances Nonlocal Attention Operators (NAO)
in both predictive accuracy and computational efficiency. NIPS employs a linear
attention mechanism to enable scalable learning and integrates a learnable
kernel network that acts as a channel-independent convolution in Fourier space.
As a consequence, NIPS eliminates the need to explicitly compute and store
large pairwise interactions, effectively amortizing the cost of handling
spatial interactions into the Fourier transform. Empirical evaluations
demonstrate that NIPS consistently surpasses NAO and other baselines across
diverse benchmarks, heralding a substantial leap in scalable, interpretable,
and efficient physics learning. Our code and data accompanying this paper are
available at https://github.com/fishmoon1234/Nonlocal-Attention-Operator.

</details>


### [178] [CrossLinear: Plug-and-Play Cross-Correlation Embedding for Time Series Forecasting with Exogenous Variables](https://arxiv.org/abs/2505.23116)
*Pengfei Zhou,Yunlong Liu,Junli Liang,Qi Song,Xiangyang Li*

Key words: 时间序列预测,外生变量,交叉相关,线性模型,过拟合

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种新颖的线性模型CrossLinear，通过插入式交叉相关嵌入模块高效捕捉变量间依赖关系，有效提升了时间序列预测性能。

Motivation: 传统模型在处理内生和外生变量依赖时性能不佳，容易出现过拟合或效率低下，CrossLinear旨在解决这一问题。

Method: CrossLinear采用轻量级交叉相关嵌入模块，选择性捕捉变量间固定和直接的依赖关系，同时结合分块处理和全局线性头来捕获短期和长期时序依赖。

Result: 在12个真实数据集上的实验表明，CrossLinear在短期和长期预测任务中均表现优越，模块的泛用性使其适用于不同领域的预测任务。

Conclusion: CrossLinear通过高效的依赖关系建模显著提升了预测精度，其嵌入模块的设计兼具轻量化和泛用性，为相关领域提供了实用工具。

Abstract: Time series forecasting with exogenous variables is a critical emerging
paradigm that presents unique challenges in modeling dependencies between
variables. Traditional models often struggle to differentiate between
endogenous and exogenous variables, leading to inefficiencies and overfitting.
In this paper, we introduce CrossLinear, a novel Linear-based forecasting model
that addresses these challenges by incorporating a plug-and-play
cross-correlation embedding module. This lightweight module captures the
dependencies between variables with minimal computational cost and seamlessly
integrates into existing neural networks. Specifically, it captures
time-invariant and direct variable dependencies while disregarding time-varying
or indirect dependencies, thereby mitigating the risk of overfitting in
dependency modeling and contributing to consistent performance improvements.
Furthermore, CrossLinear employs patch-wise processing and a global linear head
to effectively capture both short-term and long-term temporal dependencies,
further improving its forecasting precision. Extensive experiments on 12
real-world datasets demonstrate that CrossLinear achieves superior performance
in both short-term and long-term forecasting tasks. The ablation study
underscores the effectiveness of the cross-correlation embedding module.
Additionally, the generalizability of this module makes it a valuable plug-in
for various forecasting tasks across different domains. Codes are available at
https://github.com/mumiao2000/CrossLinear.

</details>


### [179] [Decom-Renorm-Merge: Model Merging on the Right Space Improves Multitasking](https://arxiv.org/abs/2505.23117)
*Yuatyong Chaichana,Thanapat Trachu,Peerat Limkonchotiwat,Konpat Preechakul,Tirasan Khandhawit,Ekapol Chuangsuwanich*

Key words: 模型合并, 奇异值分解, 权重矩阵, DRM, 多任务学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: DRM（Decom-Renorm-Merge）是一种基于奇异值分解的模型融合方法，通过分解和重新标准化权重矩阵，解决了传统直接合并方法在微调神经网络中存在的问题，并在多种模型上表现优异。

Motivation: 传统模型合并方法假设权重矩阵中相同位置的条目功能相同，但忽略了微调后神经网络的复杂性。DRM旨在通过分解和协调权重矩阵，实现更有效的合并。

Method: 利用奇异值分解（SVD）分解权重矩阵，并在对齐的联合空间中进行重新标准化和合并。

Result: DRM在ViT、DeBERTa、T5和Llama3.1-8B等模型中均优于其他先进合并技术，尤其在完全微调和低秩适应场景中表现突出。

Conclusion: DRM通过重新标准化为合并创建了稳健且均匀的联合空间，显著提升了合并性能。

Abstract: In the era of large-scale training, model merging has evolved into a tool for
creating multitasking models efficiently. It enables the knowledge of models to
be fused, without the need for heavy computation as required in traditional
multitask learning. Existing merging methods often assume that entries at
identical positions in weight matrices serve the same function, enabling
straightforward entry-wise comparison and merging. However, this assumption
overlooks the complexity of finetuned neural networks, where neurons may
develop distinct feature compositions, making direct entry-wise merging
problematic. We present Decom-Renorm-Merge (DRM), a simple yet effective
approach that leverages Singular Value Decomposition to decompose and
coordinate weight matrices into an aligned joint space, where entry-wise
merging becomes possible. We showcase the effectiveness of DRM across various
settings ranging from smaller encoder-based such as ViT and DeBERTa,
encoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B.
Our experimental results show that DRM outperforms several state-of-the-art
merging techniques across full finetuning and low-rank adaptation settings.
Moreover, our analysis reveals renormalization as the crucial component for
creating a robust and even joint space for merging, significantly contributing
to the method's performance.

</details>


### [180] [DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous Dataflow Graphs](https://arxiv.org/abs/2505.23131)
*Xinyu Yao,Daniel Bourgeois,Abhinav Jain,Yuxin Tang,Jiawen Yao,Zhimin Ding,Arlei Silva,Chris Jermaine*

Key words: 数据流图, 设备分配, 机器学习任务, 双策略网络, 执行时间优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为\textsc{Doppler}的三阶段框架，用于在数据流图中分配操作以减少执行时间，解决了现有基于学习的方法在机器学习的三个关键局限性。

Motivation: 研究动机在于解决现有基于学习的方法在复杂机器学习任务中的三个局限性：依赖批量同步系统导致设备利用不足、缺乏对底层系统调度机制的认识、以及过度依赖强化学习而忽略专家设计的启发式方法结构。

Method: 方法采用了名为\textsc{Doppler}的三阶段框架，训练由选择操作策略（$\mathsf{SEL}$）和操作放置策略（$\mathsf{PLC}$）组成的双策略网络。

Result: 实验证明，\textsc{Doppler}在所有任务上均优于基线方法，减少了系统执行时间，并通过减少每轮训练时间提高了采样效率。

Conclusion: 结论表明，\textsc{Doppler}通过优化操作分配和放置策略，有效提升了执行效率和训练速度。

Abstract: We study the problem of assigning operations in a dataflow graph to devices
to minimize execution time in a work-conserving system, with emphasis on
complex machine learning workloads. Prior learning-based methods often struggle
due to three key limitations: (1) reliance on bulk-synchronous systems like
TensorFlow, which under-utilize devices due to barrier synchronization; (2)
lack of awareness of the scheduling mechanism of underlying systems when
designing learning-based methods; and (3) exclusive dependence on reinforcement
learning, ignoring the structure of effective heuristics designed by experts.
In this paper, we propose \textsc{Doppler}, a three-stage framework for
training dual-policy networks consisting of 1) a $\mathsf{SEL}$ policy for
selecting operations and 2) a $\mathsf{PLC}$ policy for placing chosen
operations on devices. Our experiments show that \textsc{Doppler} outperforms
all baseline methods across tasks by reducing system execution time and
additionally demonstrates sampling efficiency by reducing per-episode training
time.

</details>


### [181] [VERINA: Benchmarking Verifiable Code Generation](https://arxiv.org/abs/2505.23135)
*Zhe Ye,Zhengxu Yan,Jingxuan He,Timothe Kasriel,Kaiyu Yang,Dawn Song*

Key words: 可验证代码生成, LLM, 基准测试, Lean, 定理证明

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文介绍了Verina，一个用于评估可验证代码生成的综合性基准测试工具，旨在解决当前LLM生成代码的正确性问题。

Motivation: 为了解决LLM生成代码的正确性验证问题，并推动可验证代码生成领域的研究。

Method: 提出了Verina基准测试，包含189个手动整理的Lean编程任务，涵盖代码、规范及证明生成的评估。

Result: 现有LLM在可验证代码生成方面表现不佳，尤其是证明生成，最佳模型OpenAI o4-mini的代码正确率仅为61.4%，规范完成度为51.0%，证明成功率为3.6%。

Conclusion: Verina为可验证代码生成提供了严格的基准，有望推动该领域的进展。数据集和评估代码已开源。

Abstract: Large language models (LLMs) are increasingly integrated in software
development, but ensuring correctness in LLM-generated code remains challenging
and often requires costly manual review. Verifiable code generation -- jointly
generating code, specifications, and proofs of code-specification alignment --
offers a promising path to address this limitation and further unleash LLMs'
benefits in coding. Yet, there exists a significant gap in evaluation: current
benchmarks often lack support for end-to-end verifiable code generation. In
this paper, we introduce Verina (Verifiable Code Generation Arena), a
high-quality benchmark enabling a comprehensive and modular evaluation of code,
specification, and proof generation as well as their compositions. Verina
consists of 189 manually curated coding tasks in Lean, with detailed problem
descriptions, reference implementations, formal specifications, and extensive
test suites. Our extensive evaluation of state-of-the-art LLMs reveals
significant challenges in verifiable code generation, especially in proof
generation, underscoring the need for improving LLM-based theorem provers in
verification domains. The best model, OpenAI o4-mini, generates only 61.4%
correct code, 51.0% sound and complete specifications, and 3.6% successful
proofs, with one trial per task. We hope Verina will catalyze progress in
verifiable code generation by providing a rigorous and comprehensive benchmark.
We release our dataset on https://huggingface.co/datasets/sunblaze-ucb/verina
and our evaluation code on https://github.com/sunblaze-ucb/verina.

</details>


### [182] [Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners](https://arxiv.org/abs/2505.23150)
*Michal Nauman,Marek Cygan,Carmelo Sferrazza,Aviral Kumar,Pieter Abbeel*

Key words: 多任务强化学习、稀疏奖励、梯度冲突、任务干扰、高容量价值模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 通过使用高容量价值模型和可学习任务嵌入，解决了多任务强化学习中任务干扰的问题，实现了鲁棒且可扩展的在线训练。

Motivation: 多任务强化学习中稀疏奖励和梯度冲突使得优化困难，现有方法通常避免在线训练，转而克隆专家轨迹或蒸馏单任务策略。本文旨在找到一种能在在线环境中有效避免任务干扰的方法。

Method: 采用基于交叉熵的高容量价值模型，结合可学习的任务嵌入进行训练，解决了任务干扰问题。

Result: 在超过280个任务的7个多任务基准测试中表现优异，实现了单任务和多任务的最优性能，且在新任务上具有样本高效的迁移能力。

Conclusion: 该方法虽然简单，但有效解决了多任务强化学习中的任务干扰问题，推动了通用策略的发展。

Abstract: Recent advances in language modeling and vision stem from training large
models on diverse, multi-task data. This paradigm has had limited impact in
value-based reinforcement learning (RL), where improvements are often driven by
small models trained in a single-task context. This is because in multi-task RL
sparse rewards and gradient conflicts make optimization of temporal difference
brittle. Practical workflows for generalist policies therefore avoid online
training, instead cloning expert trajectories or distilling collections of
single-task policies into one agent. In this work, we show that the use of
high-capacity value models trained via cross-entropy and conditioned on
learnable task embeddings addresses the problem of task interference in online
RL, allowing for robust and scalable multi-task training. We test our approach
on 7 multi-task benchmarks with over 280 unique tasks, spanning high
degree-of-freedom humanoid control and discrete vision-based RL. We find that,
despite its simplicity, the proposed approach leads to state-of-the-art single
and multi-task performance, as well as sample-efficient transfer to new tasks.

</details>


### [183] [Best Arm Identification with Possibly Biased Offline Data](https://arxiv.org/abs/2505.23165)
*Le Yang,Vincent Y. F. Tan,Wang Chi Cheung*

Key words: 最佳臂识别（BAI）、偏差校正、自适应算法、离线数据、LUCB-H

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究了在固定置信度设置下具有潜在偏差离线数据的最佳臂识别（BAI）问题，提出了LUCB-H算法来平衡离线和在线数据，理论分析与实验验证了其有效性。

Motivation: 现实场景（如临床试验）中常遇到离线数据可能带有偏差的情况，需在未知偏差界限时自适应识别最佳臂。

Method: 提出了LUCB-H算法，通过引入辅助偏差校正的自适应置信边界，在LUCB框架内平衡离线和在线数据。

Result: 理论分析显示LUCB-H在离线数据误导时与标准LUCB样本复杂度相当，在离线数据有帮助时显著优于后者；实验验证其鲁棒性和适应性。

Conclusion: LUCB-H能高效整合离线数据，其表现与理论下界一致，适用于存在偏差的BAI问题。

Abstract: We study the best arm identification (BAI) problem with potentially biased
offline data in the fixed confidence setting, which commonly arises in
real-world scenarios such as clinical trials. We prove an impossibility result
for adaptive algorithms without prior knowledge of the bias bound between
online and offline distributions. To address this, we propose the LUCB-H
algorithm, which introduces adaptive confidence bounds by incorporating an
auxiliary bias correction to balance offline and online data within the LUCB
framework. Theoretical analysis shows that LUCB-H matches the sample complexity
of standard LUCB when offline data is misleading and significantly outperforms
it when offline data is helpful. We also derive an instance-dependent lower
bound that matches the upper bound of LUCB-H in certain scenarios. Numerical
experiments further demonstrate the robustness and adaptability of LUCB-H in
effectively incorporating offline data.

</details>


### [184] [Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization](https://arxiv.org/abs/2505.23173)
*Shohei Enomoto*

Key words: 域泛化、多源域泛化、单源域泛化、伪域生成、风格迁移、数据增强

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为伪多源域泛化（PMDG）的新框架，通过风格迁移和数据增强技术从单源域生成多个伪域，实现在单源域泛化（SDG）设置中应用多源域泛化（MDG）算法，实验表明伪域能够匹配甚至超越实际多域性能。

Motivation: 现实应用中数据分布多变，但构建多域数据集成本高昂，限制了多源域泛化（MDG）的应用。为克服这一限制，研究如何在单源域（SDG）设置中利用MDG算法的潜力。

Method: 提出PMDG框架，通过风格迁移和数据增强从单一源域生成多个伪域，形成合成多域数据集，并在改进的DomainBed基准PseudoDomainBed上进行实验验证。

Result: 实验表明PMDG性能与MDG正相关，且伪域在数据充足时能匹配或超越实际多域性能。

Conclusion: PMDG为域泛化研究提供了实用解决方案，尤其在资源受限场景下，凸显了伪域的潜力。

Abstract: Deep learning models often struggle to maintain performance when deployed on
data distributions different from their training data, particularly in
real-world applications where environmental conditions frequently change. While
Multi-source Domain Generalization (MDG) has shown promise in addressing this
challenge by leveraging multiple source domains during training, its practical
application is limited by the significant costs and difficulties associated
with creating multi-domain datasets. To address this limitation, we propose
Pseudo Multi-source Domain Generalization (PMDG), a novel framework that
enables the application of sophisticated MDG algorithms in more practical
Single-source Domain Generalization (SDG) settings. PMDG generates multiple
pseudo-domains from a single source domain through style transfer and data
augmentation techniques, creating a synthetic multi-domain dataset that can be
used with existing MDG algorithms. Through extensive experiments with
PseudoDomainBed, our modified version of the DomainBed benchmark, we analyze
the effectiveness of PMDG across multiple datasets and architectures. Our
analysis reveals several key findings, including a positive correlation between
MDG and PMDG performance and the potential of pseudo-domains to match or exceed
actual multi-domain performance with sufficient data. These comprehensive
empirical results provide valuable insights for future research in domain
generalization. Our code is available at
https://github.com/s-enmt/PseudoDomainBed.

</details>


### [185] [The Panaceas for Improving Low-Rank Decomposition in Communication-Efficient Federated Learning](https://arxiv.org/abs/2505.23176)
*Shiwei Li,Xiandi Luo,Haozhao Wang,Xing Tang,Shijie Xu,Weihong Luo,Yuhua Li,Xiuqiang He,Ruixuan Li*

Key words: 联邦学习, 低秩分解, 模型更新分解, 块状克罗内克分解, 聚合感知分解

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文通过引入三种新型分解技术（MUD、BKD、AAD）来解决联邦学习中的低秩分解问题，提升了训练效率和性能。实验结果表明，该方法在收敛速度和准确性上优于基线方法。

Motivation: 提升联邦学习中低秩分解方法的性能，解决分解中的关键问题：分解内容、分解方式及聚合方法。

Method: 提出三种互补技术：模型更新分解（MUD）、块状克罗内克分解（BKD）和聚合感知分解（AAD），并进行理论分析确保收敛性。

Result: 实验显示，所提方法在收敛速度和准确性上优于基线方法，代码已开源。

Conclusion: 通过三项技术综合应用，显著提升联邦学习的效率和性能，具备理论保障和实验验证。

Abstract: To improve the training efficiency of federated learning (FL), previous
research has employed low-rank decomposition techniques to reduce communication
overhead. In this paper, we seek to enhance the performance of these low-rank
decomposition methods. Specifically, we focus on three key issues related to
decomposition in FL: what to decompose, how to decompose, and how to aggregate.
Subsequently, we introduce three novel techniques: Model Update Decomposition
(MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware
Decomposition (AAD), each targeting a specific issue. These techniques are
complementary and can be applied simultaneously to achieve optimal performance.
Additionally, we provide a rigorous theoretical analysis to ensure the
convergence of the proposed MUD. Extensive experimental results show that our
approach achieves faster convergence and superior accuracy compared to relevant
baseline methods. The code is available at
https://github.com/Leopold1423/fedmud-icml25.

</details>


### [186] [FreRA: A Frequency-Refined Augmentation for Contrastive Learning on Time Series Classification](https://arxiv.org/abs/2505.23181)
*Tian Tian,Chunyan Miao,Hangwei Qian*

Key words: 时间序列, 对比学习, 频域增强, 分类, 异常检测, 迁移学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种针对时间序列分类任务的频率细化增强方法（FreRA），通过频域分析优化对比学习中的增强策略，显著提升了分类、异常检测和迁移学习的性能。

Motivation: 现有时间序列增强方法主要来自视觉领域，可能引入不匹配的模式，破坏语义信息。频域分析能为时间序列分类提供全局性、独立性和紧凑性的优势。

Method: 提出轻量级频率细化增强（FreRA），自动分离关键和非关键频率成分，并通过语义感知的身份修改和语义无关的自适应修改分别处理这两种成分，确保语义信息的保护和非关键成分的多样化。

Result: 在两个基准数据集（UCR和UEA）以及五个大规模数据集上的实验表明，FreRA在时间序列分类、异常检测和迁移学习中显著优于十个基线方法。

Conclusion: 频域增强策略（FreRA）能有效提升时间序列对比学习的性能，并在多种任务和数据集上展现出优异的泛化能力。

Abstract: Contrastive learning has emerged as a competent approach for unsupervised
representation learning. However, the design of an optimal augmentation
strategy, although crucial for contrastive learning, is less explored for time
series classification tasks. Existing predefined time-domain augmentation
methods are primarily adopted from vision and are not specific to time series
data. Consequently, this cross-modality incompatibility may distort the
semantically relevant information of time series by introducing mismatched
patterns into the data. To address this limitation, we present a novel
perspective from the frequency domain and identify three advantages for
downstream classification: global, independent, and compact. To fully utilize
the three properties, we propose the lightweight yet effective Frequency
Refined Augmentation (FreRA) tailored for time series contrastive learning on
classification tasks, which can be seamlessly integrated with contrastive
learning frameworks in a plug-and-play manner. Specifically, FreRA
automatically separates critical and unimportant frequency components.
Accordingly, we propose semantic-aware Identity Modification and
semantic-agnostic Self-adaptive Modification to protect semantically relevant
information in the critical frequency components and infuse variance into the
unimportant ones respectively. Theoretically, we prove that FreRA generates
semantic-preserving views. Empirically, we conduct extensive experiments on two
benchmark datasets, including UCR and UEA archives, as well as five large-scale
datasets on diverse applications. FreRA consistently outperforms ten leading
baselines on time series classification, anomaly detection, and transfer
learning tasks, demonstrating superior capabilities in contrastive
representation learning and generalization in transfer learning scenarios
across diverse datasets.

</details>


### [187] [FSL-SAGE: Accelerating Federated Split Learning via Smashed Activation Gradient Estimation](https://arxiv.org/abs/2505.23182)
*Srijith Nair,Michael Lin,Amirreza Talebi,Peizhong Ju,Elizabeth Bentley,Jia Liu*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: FSL-SAGE是一种新的联邦分割学习算法，通过辅助模型估计服务器梯度反馈，以降低通信成本和客户端内存需求，同时保持与传统联邦学习相当的收敛速度。

Motivation: 解决联邦学习（FL）和分割学习（SL）在扩展性和效率上的局限性，尤其是客户端内存约束和网络延迟问题。

Method: 使用辅助模型模拟服务器行为，通过本地数据集周期性适应，实现并行客户端训练并减少通信开销。

Result: FSL-SAGE实现了$\mathcal{O}(1/\sqrt{T})$的收敛速度，通信成本和客户端内存需求显著降低，同时准确率优于现有方法。

Conclusion: FSL-SAGE在通信效率和准确性上优于现有联邦分割学习方法，适用于大规模模型训练。

Abstract: Collaborative training methods like Federated Learning (FL) and Split
Learning (SL) enable distributed machine learning without sharing raw data.
However, FL assumes clients can train entire models, which is infeasible for
large-scale models. In contrast, while SL alleviates the client memory
constraint in FL by offloading most training to the server, it increases
network latency due to its sequential nature. Other methods address the
conundrum by using local loss functions for parallel client-side training to
improve efficiency, but they lack server feedback and potentially suffer poor
accuracy. We propose FSL-SAGE (Federated Split Learning via Smashed Activation
Gradient Estimation), a new federated split learning algorithm that estimates
server-side gradient feedback via auxiliary models. These auxiliary models
periodically adapt to emulate server behavior on local datasets. We show that
FSL-SAGE achieves a convergence rate of $\mathcal{O}(1/\sqrt{T})$, where $T$ is
the number of communication rounds. This result matches FedAvg, while
significantly reducing communication costs and client memory requirements. Our
empirical results also verify that it outperforms existing state-of-the-art FSL
methods, offering both communication efficiency and accuracy.

</details>


### [188] [Two Is Better Than One: Rotations Scale LoRAs](https://arxiv.org/abs/2505.23184)
*Hongcan Guo,Guoshun Nan,Yuan Yang,Diyang Zhang,Haotian Li,Zhican Chen,Qinchuan Zhou,Yuhan Ran,Xinye Cao,Sicong Leng,Xiaofeng Tao,Xudong Jiang*

Key words: LoRA, Mixture-of-Experts, 大规模语言模型, 门控机制, 几何旋转

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: RadarGate是一种新颖的几何启发门控方法，通过旋转LoRA表示提升表达能力，解决传统门控机制在大规模语言模型中的泛化和欠拟合问题。

Motivation: 传统门控机制限制LoRA表示的表达能力，导致泛化能力差和欠拟合问题。RadarGate通过几何旋转操作增强表示交互，提升模型可扩展性。

Method: RadarGate通过学习LoRA表示之间的旋转矩阵，引入相对角度关系，增强跨LoRA协同学习能力。

Result: 在6个公共基准的21个任务上验证了RadarGate的有效性，显著改善了泛化和欠拟合问题。

Conclusion: RadarGate通过几何旋转操作提升LoRA表示的交互能力，为大规模语言模型的可扩展性提供了有效解决方案。

Abstract: Scaling Low-Rank Adaptation (LoRA)-based Mixture-of-Experts (MoE) facilitates
large language models (LLMs) to efficiently adapt to diverse tasks. However,
traditional gating mechanisms that route inputs to the best experts may
fundamentally hinder LLMs' scalability, leading to poor generalization and
underfitting issues. We identify that the root cause lies in the restricted
expressiveness of existing weighted-sum mechanisms, both within and outside the
convex cone of LoRA representations. This motivates us to propose RadarGate, a
novel geometrically inspired gating method that introduces rotational
operations of LoRAs representations to boost the expressiveness and facilitate
richer feature interactions among multiple LoRAs for scalable LLMs.
Specifically, we first fuse each LoRA representation to other LoRAs using a
learnable component and then feed the output to a rotation matrix. This matrix
involves learnable parameters that define the relative angular relationship
between LoRA representations. Such a simple yet effective mechanism provides an
extra degree of freedom, facilitating the learning of cross-LoRA synergies and
properly tracking the challenging poor generalization and underfitting issues
as the number of LoRA grows. Extensive experiments on 6 public benchmarks
across 21 tasks show the effectiveness of our RadarGate for scaling LoRAs. We
also provide valuable insights, revealing that the rotations to each pair of
representations are contrastive, encouraging closer alignment of semantically
similar representations during geometrical transformation while pushing
distance ones further apart. We will release our code to the community.

</details>


### [189] [Improving the Effective Receptive Field of Message-Passing Neural Networks](https://arxiv.org/abs/2505.23185)
*Shahaf E. Finder,Ron Shapira Weber,Moshe Eliasof,Oren Freifeld,Eran Treister*

Key words: MPNN, 有效感受野, 长距离依赖, 图神经网络, IM-MPNN

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为IM-MPNN的新型架构，通过多层次图粗化和消息传递来解决MPNN中长距离依赖捕获不足的问题，显著提升了性能并保持计算效率。

Motivation: MPNN在处理图结构数据时存在长距离依赖捕获不足的过压缩现象，限制了其性能，类似于CNN中的有效感受野问题。

Method: 提出了IM-MPNN架构，通过图的多层次粗化和跨尺度消息传递来增强长距离交互，避免过深的网络或过多参数。

Result: 在Long-Range Graph Benchmark等基准测试中，IM-MPNN显著优于基线MPNN，并能有效捕获长距离依赖。

Conclusion: IM-MPNN成功解决了MPNN中的长距离依赖问题，平衡了性能与计算效率。

Abstract: Message-Passing Neural Networks (MPNNs) have become a cornerstone for
processing and analyzing graph-structured data. However, their effectiveness is
often hindered by phenomena such as over-squashing, where long-range
dependencies or interactions are inadequately captured and expressed in the
MPNN output. This limitation mirrors the challenges of the Effective Receptive
Field (ERF) in Convolutional Neural Networks (CNNs), where the theoretical
receptive field is underutilized in practice. In this work, we show and
theoretically explain the limited ERF problem in MPNNs. Furthermore, inspired
by recent advances in ERF augmentation for CNNs, we propose an Interleaved
Multiscale Message-Passing Neural Networks (IM-MPNN) architecture to address
these problems in MPNNs. Our method incorporates a hierarchical coarsening of
the graph, enabling message-passing across multiscale representations and
facilitating long-range interactions without excessive depth or
parameterization. Through extensive evaluations on benchmarks such as the
Long-Range Graph Benchmark (LRGB), we demonstrate substantial improvements over
baseline MPNNs in capturing long-range dependencies while maintaining
computational efficiency.

</details>


### [190] [DeepRTE: Pre-trained Attention-based Neural Network for Radiative Tranfer](https://arxiv.org/abs/2505.23190)
*Yekun Zhu,Min Tang,Zheng Ma*

Key words: DeepRTE, 辐射传递方程, 注意力神经网络, 数值实验

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为DeepRTE的新神经网络方法，用于高效且高精度地求解稳态辐射传递方程（RTE）。

Motivation: 辐射传递方程（RTE）在多个领域具有重要应用，但求解复杂且计算成本高，需一种高效且精确的解决方案。

Method: 采用预训练的注意力神经网络框架DeepRTE来求解RTE。

Result: 通过全面数值实验验证了DeepRTE在精度和计算效率上的优越性。

Conclusion: DeepRTE为求解RTE提供了一种有效的神经网络方法。

Abstract: In this study, we propose a novel neural network approach, termed DeepRTE, to
address the steady-state Radiative Transfer Equation (RTE). The RTE is a
differential-integral equation that governs the propagation of radiation
through a participating medium, with applications spanning diverse domains such
as neutron transport, atmospheric radiative transfer, heat transfer, and
optical imaging. Our proposed DeepRTE framework leverages pre-trained
attention-based neural networks to solve the RTE with high accuracy and
computational efficiency. The efficacy of the proposed approach is
substantiated through comprehensive numerical experiments.

</details>


### [191] [Beyond Zero Initialization: Investigating the Impact of Non-Zero Initialization on LoRA Fine-Tuning Dynamics](https://arxiv.org/abs/2505.23194)
*Shiwei Li,Xiandi Luo,Xing Tang,Haozhao Wang,Hao Chen,Weihong Luo,Yuhua Li,Xiuqiang He,Ruixuan Li*

Key words: LoRA, fine-tuning, initialization, robust learning

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究表明，在LoRA微调中，同时初始化矩阵$A$和$B$为非零值比零初始化更能适应次优学习率，且不影响性能。

Motivation: 探讨非零初始化对LoRA微调动态的影响，挑战从预训练模型严格开始的必要性。

Method: 从无限宽度角度分析，比较零与非零初始化的效果，并通过实验验证。

Result: 非零初始化增强了LoRA对次优学习率的鲁棒性，且性能不受影响。

Conclusion: 微调不必须严格从预训练模型开始，非零初始化是可行的。

Abstract: Low-rank adaptation (LoRA) is a widely used parameter-efficient fine-tuning
method. In standard LoRA layers, one of the matrices, $A$ or $B$, is
initialized to zero, ensuring that fine-tuning starts from the pretrained
model. However, there is no theoretical support for this practice. In this
paper, we investigate the impact of non-zero initialization on LoRA's
fine-tuning dynamics from an infinite-width perspective. Our analysis reveals
that, compared to zero initialization, simultaneously initializing $A$ and $B$
to non-zero values improves LoRA's robustness to suboptimal learning rates,
particularly smaller ones. Further analysis indicates that although the
non-zero initialization of $AB$ introduces random noise into the pretrained
weight, it generally does not affect fine-tuning performance. In other words,
fine-tuning does not need to strictly start from the pretrained model. The
validity of our findings is confirmed through extensive experiments across
various models and datasets. The code is available at
https://github.com/Leopold1423/non_zero_lora-icml25.

</details>


### [192] [Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning](https://arxiv.org/abs/2505.23195)
*Lifan Zhao,Yanyan Shen,Zhaoyang Liu,Xue Wang,Jiaji Deng*

Key words: 时间序列基础模型, 微调, 结构化剪枝, 预测任务, 先验知识

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种通过结构化剪枝优化预训练时间序列基础模型（TSFMs）微调的方法，以提高其在目标预测任务中的性能。

Motivation: 尽管TSFMs在零样本预测中表现优异，但在微调后仍无法稳定超越小型专用模型。研究发现TSFMs的计算存在稀疏性和冗余性，表明模型通过激活任务相关的子结构适应多样任务。

Method: 提出结构化剪枝方法，保留预训练模型的关键先验知识，并在更紧凑的参数空间中进行微调。

Result: 在七个TSFMs和六个基准测试中，剪枝后的微调显著提升了预测性能，甚至超越专用模型。

Conclusion: ‘剪枝后微调’范式使TSFMs达到最先进水平，并验证了其优于专用模型的潜力。

Abstract: Scaling laws motivate the development of Time Series Foundation Models
(TSFMs) that pre-train vast parameters and achieve remarkable zero-shot
forecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot
consistently outperform smaller, specialized models trained on full-shot
downstream data. A key question is how to realize effective adaptation of TSFMs
for a target forecasting task. Through empirical studies on various TSFMs, the
pre-trained models often exhibit inherent sparsity and redundancy in
computation, suggesting that TSFMs have learned to activate task-relevant
network substructures to accommodate diverse forecasting tasks. To preserve
this valuable prior knowledge, we propose a structured pruning method to
regularize the subsequent fine-tuning process by focusing it on a more relevant
and compact parameter space. Extensive experiments on seven TSFMs and six
benchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly
improves forecasting performance compared to fine-tuning original models. This
"prune-then-finetune" paradigm often enables TSFMs to achieve state-of-the-art
performance and surpass strong specialized baselines.

</details>


### [193] [Daunce: Data Attribution through Uncertainty Estimation](https://arxiv.org/abs/2505.23223)
*Xingyuan Pan,Chenlu Ye,Joseph Melkonian,Jiaqi W. Ma,Tong Zhang*

Key words: 数据归属, 不确定性估计, 大型语言模型, 模型扰动, 黑盒访问

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了名为Daunce的数据归属方法，通过不确定性估计实现高效且准确的数据影响分析，适用于包括大型语言模型在内的多种任务。

Motivation: 现有梯度相关数据归属方法难以扩展，而随机投影方法在准确性上表现不足，需提出一种兼顾扩展性和准确性的新方法。

Method: Daunce通过微调一组扰动模型，计算各样本损失的协方差作为归属分数，无需依赖梯度或二阶信息。

Result: 实验证实Daunce在视觉任务和LLM微调中优于现有方法，且首次实现了对GPT等专有LLM的数据归属。

Conclusion: Daunce为数据归属提供了可扩展且精确的解决方案，并展示了在黑盒模型上的适用性。

Abstract: Training data attribution (TDA) methods aim to identify which training
examples influence a model's predictions on specific test data most. By
quantifying these influences, TDA supports critical applications such as data
debugging, curation, and valuation. Gradient-based TDA methods rely on
gradients and second-order information, limiting their applicability at scale.
While recent random projection-based methods improve scalability, they often
suffer from degraded attribution accuracy. Motivated by connections between
uncertainty and influence functions, we introduce Daunce - a simple yet
effective data attribution approach through uncertainty estimation. Our method
operates by fine-tuning a collection of perturbed models and computing the
covariance of per-example losses across these models as the attribution score.
Daunce is scalable to large language models (LLMs) and achieves more accurate
attribution compared to existing TDA methods. We validate Daunce on tasks
ranging from vision tasks to LLM fine-tuning, and further demonstrate its
compatibility with black-box model access. Applied to OpenAI's GPT models, our
method achieves, to our knowledge, the first instance of data attribution on
proprietary LLMs.

</details>


### [194] [Generalizability vs. Counterfactual Explainability Trade-Off](https://arxiv.org/abs/2505.23225)
*Fabiano Veglianti,Flavio Giorgi,Fabrizio Silvestri,Gabriele Tolomei*

Key words: 泛化能力, 反事实解释, 过拟合, ε-VCP, 监督学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究探讨了监督学习中模型泛化能力与反事实可解释性的关系，提出ε-有效反事实概率（ε-VCP）概念，理论证明其与模型过拟合正相关，揭示泛化与可解释性间的固有权衡。

Motivation: 研究旨在揭示模型泛化能力与反事实解释生成难易度的内在联系，验证过拟合如何影响解释性。

Method: 提出ε-VCP概念，分析其与决策边界几何的关系，并进行理论推导与实证验证。

Result: ε-VCP随模型过拟合增加而上升，表明泛化能力与反事实解释性存在权衡，实证支持理论结论。

Conclusion: ε-VCP可作为量化过拟合的实用指标，揭示了模型性能与解释性之间的矛盾。

Abstract: In this work, we investigate the relationship between model generalization
and counterfactual explainability in supervised learning. We introduce the
notion of $\varepsilon$-valid counterfactual probability ($\varepsilon$-VCP) --
the probability of finding perturbations of a data point within its
$\varepsilon$-neighborhood that result in a label change. We provide a
theoretical analysis of $\varepsilon$-VCP in relation to the geometry of the
model's decision boundary, showing that $\varepsilon$-VCP tends to increase
with model overfitting. Our findings establish a rigorous connection between
poor generalization and the ease of counterfactual generation, revealing an
inherent trade-off between generalization and counterfactual explainability.
Empirical results validate our theory, suggesting $\varepsilon$-VCP as a
practical proxy for quantitatively characterizing overfitting.

</details>


### [195] [Graph Random Walk with Feature-Label Space Alignment: A Multi-Label Feature Selection Method](https://arxiv.org/abs/2505.23228)
*Wanfu Gao,Jun Gao,Qingqi Han,Hanlin Pan,Kunpeng Liu*

Key words: 多标签学习,非线性关联,随机游走图,低维表示,流形结构

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种基于随机游走图的方法，用于解决高维多标签数据中特征与标签间复杂非线性关联的捕获问题，并通过低维表示系数对齐变量空间。

Motivation: 高维特征可能导致特征与标签间的隐含关联复杂化，现有线性分解方法难以捕捉非线性关联且易导致特征与标签空间不对齐。

Method: 设计随机游走图整合特征-特征、标签-标签及特征-标签关系，优化低秩分解后的隐式关联表示；利用低维表示系数对齐变量空间并保持流形结构。

Result: 在7个基准数据集和3个代表性数据集上的广泛实验证明了所提方法的优越性。

Conclusion: 该方法有效解决了高维多标签数据中非线性关联捕获与空间对齐问题，表现出显著优势。

Abstract: The rapid growth in feature dimension may introduce implicit associations
between features and labels in multi-label datasets, making the relationships
between features and labels increasingly complex. Moreover, existing methods
often adopt low-dimensional linear decomposition to explore the associations
between features and labels. However, linear decomposition struggles to capture
complex nonlinear associations and may lead to misalignment between the feature
space and the label space. To address these two critical challenges, we propose
innovative solutions. First, we design a random walk graph that integrates
feature-feature, label-label, and feature-label relationships to accurately
capture nonlinear and implicit indirect associations, while optimizing the
latent representations of associations between features and labels after
low-rank decomposition. Second, we align the variable spaces by leveraging
low-dimensional representation coefficients, while preserving the manifold
structure between the original high-dimensional multi-label data and the
low-dimensional representation space. Extensive experiments and ablation
studies conducted on seven benchmark datasets and three representative datasets
using various evaluation metrics demonstrate the superiority of the proposed
method\footnote{Code: https://github.com/Heilong623/-GRW-}.

</details>


### [196] [Equivalence of stochastic and deterministic policy gradients](https://arxiv.org/abs/2505.23244)
*Emo Todorov*

Key words: 政策梯度, 连续控制, 随机策略, 确定性策略, MDP

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了随机和确定性策略在连续控制中的政策梯度的关系，证明了在某些条件下两者的梯度和状态价值函数相同，并提出了一种构建等价MDP的方法。

Motivation: 研究随机和确定性策略政策梯度之间的关系，以理解它们在连续控制中的异同点。

Method: 通过分析高斯控制噪声和二次控制成本的MDP家族，比较随机和确定性策略的梯度、自然梯度和价值函数。随后提出了构建等价MDP的方法。

Result: 发现随机和确定性策略的梯度、自然梯度和状态价值函数相同，但状态控制价值函数不同，并成功构建了等价的MDP。

Conclusion: 研究表明，通过近似状态价值函数而非状态控制价值函数，可以实现政策梯度方法的统一。

Abstract: Policy gradients in continuous control have been derived for both stochastic
and deterministic policies. Here we study the relationship between the two. In
a widely-used family of MDPs involving Gaussian control noise and quadratic
control costs, we show that the stochastic and deterministic policy gradients,
natural gradients, and state value functions are identical; while the
state-control value functions are different. We then develop a general
procedure for constructing an MDP with deterministic policy that is equivalent
to a given MDP with stochastic policy. The controls of this new MDP are the
sufficient statistics of the stochastic policy in the original MDP. Our results
suggest that policy gradient methods can be unified by approximating state
value functions rather than state-control value functions.

</details>


### [197] [Measuring Participant Contributions in Decentralized Federated Learning](https://arxiv.org/abs/2505.23246)
*Honoka Anada,Tatsuya Kaneko,Shinya Takamaeda-Yamazaki*

Key words: 联邦学习, 去中心化, 贡献测量, Shapley值, DFL

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了两种新方法（DFL-Shapley和DFL-MR）来测量去中心化联邦学习（DFL）中参与者的贡献，解决了现有方法仅适用于中心化联邦学习（CFL）的局限性，并通过实验验证了其有效性。

Motivation: 在去中心化联邦学习（DFL）中，由于缺乏中心服务器和多个全局模型的存在，现有贡献测量方法无法直接应用，因此需要开发新的方法来衡量参与者贡献。

Method: 提出了DFL-Shapley（基于Shapley值的扩展）和DFL-MR（一种可计算的近似方法），通过累积轮次Shapley值来估算贡献。

Result: 实验表明，DFL-Shapley是有效的基准指标，而DFL-MR在不同设置下接近DFL-Shapley，证明了其作为贡献测量方法的实用性。

Conclusion: DFL-Shapley和DFL-MR为DFL提供了一种有效的贡献测量手段，填补了现有方法的空白。

Abstract: Federated learning (FL) enables multiple clients to collaboratively train
models without sharing their data. Measuring participant contributions in FL is
crucial for incentivizing clients and ensuring transparency. While various
methods have been proposed for contribution measurement, they are designed
exclusively for centralized federated learning (CFL), where a central server
collects and aggregates client models, along with evaluating their
contributions. Meanwhile, decentralized federated learning (DFL), in which
clients exchange models directly without a central server, has gained
significant attention for mitigating communication bottlenecks and eliminating
a single point of failure. However, applying existing contribution measurement
methods to DFL is challenging due to the presence of multiple global models and
the absence of a central server. In this study, we present novel methodologies
for measuring participant contributions in DFL. We first propose DFL-Shapley,
an extension of the Shapley value tailored for DFL, adapting this widely used
CFL metric to decentralized settings. Given the impracticality of computing the
ideal DFL-Shapley in real-world systems, we introduce DFL-MR, a computable
approximation that estimates overall contributions by accumulating round-wise
Shapley values. We evaluate DFL-Shapley and DFL-MR across various FL scenarios
and compare them with existing CFL metrics. The experimental results confirm
DFL-Shapley as a valid ground-truth metric and demonstrate DFL-MR's proximity
to DFL-Shapley across various settings, highlighting their effectiveness as
contribution metrics in DFL.

</details>


### [198] [Accelerating RLHF Training with Reward Variance Increase](https://arxiv.org/abs/2505.23247)
*Zonglin Yang,Zhexuan Gu,Houduo Qi,Yancheng Yuan*

Key words: 强化学习从人类反馈（RLHF）、组相对策略优化（GRPO）、奖励调整、非凸优化、训练效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种奖励调整模型，通过增加奖励方差并保持相对偏好和奖励期望来加速RLHF训练，设计了一种高效的算法来解决非凸优化问题，并将其集成到GRPO算法中，显著提高了训练效率。

Motivation: 现有GRPO-based RLHF训练效率不足，研究发现初始策略模型的奖励方差越高，训练速度越快，因此提出了奖励调整模型以加速训练。

Method: 提出奖励调整模型并通过O(n log n)算法解决非凸优化问题，将其集成到GRPO算法中形成GRPOVI算法。

Result: 实验证明GRPOVI算法显著提升了RLHF训练效率，并间接解释了基于规则的奖励在GRPO中的有效性。

Conclusion: 通过奖励方差优化和高效算法设计，GRPOVI成功提升了RLHF训练效率，为未来研究提供了实用工具。

Abstract: Reinforcement learning from human feedback (RLHF) is an essential technique
for ensuring that large language models (LLMs) are aligned with human values
and preferences during the post-training phase. As an effective RLHF approach,
group relative policy optimization (GRPO) has demonstrated success in many
LLM-based applications. However, efficient GRPO-based RLHF training remains a
challenge. Recent studies reveal that a higher reward variance of the initial
policy model leads to faster RLHF training. Inspired by this finding, we
propose a practical reward adjustment model to accelerate RLHF training by
provably increasing the reward variance and preserving the relative preferences
and reward expectation. Our reward adjustment method inherently poses a
nonconvex optimization problem, which is NP-hard to solve in general. To
overcome the computational challenges, we design a novel $O(n \log n)$
algorithm to find a global solution of the nonconvex reward adjustment model by
explicitly characterizing the extreme points of the feasible set. As an
important application, we naturally integrate this reward adjustment model into
the GRPO algorithm, leading to a more efficient GRPO with reward variance
increase (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we
provide an indirect explanation for the empirical effectiveness of GRPO with
rule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment
results demonstrate that the GRPOVI algorithm can significantly improve the
RLHF training efficiency compared to the original GRPO algorithm.

</details>


### [199] [Efficiently Access Diffusion Fisher: Within the Outer Product Span Space](https://arxiv.org/abs/2505.23264)
*Fangyikang Wang,Hubery Yin,Shaobin Zhuang,Huminhao Zhu,Yinan Li,Lei Qian,Chao Zhang,Hanbin Zhao,Hui Qian,Chen Li*

Key words: 扩散模型, 扩散Fisher信息, 外积结构, 高效近似, 似然评估

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于得分和初始数据外积空间的高效扩散Fisher信息（DF）近似算法，避免了耗时且无精度保证的自动微分方法，实验验证了其在似然评估和优化任务中的优越性。

Motivation: 当前扩散模型中扩散Fisher信息的计算通常依赖于自动微分方法，但这种方法不仅耗时且缺乏精度保证，因此需要一种更高效且可靠的替代方案。

Method: 基于DF的得分和初始数据外积结构的特性，开发了两种高效近似算法，分别用于计算DF的迹和矩阵-向量乘法，替代自动微分操作。

Result: 实验表明，提出的算法在似然评估和伴随优化任务中具有更高的精度和显著降低的计算成本。

Conclusion: 通过外积结构高效近似DF是可行的，且在实际应用中可以显著提升计算效率和精度。

Abstract: Recent Diffusion models (DMs) advancements have explored incorporating the
second-order diffusion Fisher information (DF), defined as the negative Hessian
of log density, into various downstream tasks and theoretical analysis.
However, current practices typically approximate the diffusion Fisher by
applying auto-differentiation to the learned score network. This black-box
method, though straightforward, lacks any accuracy guarantee and is
time-consuming. In this paper, we show that the diffusion Fisher actually
resides within a space spanned by the outer products of score and initial data.
Based on the outer-product structure, we develop two efficient approximation
algorithms to access the trace and matrix-vector multiplication of DF,
respectively. These algorithms bypass the auto-differentiation operations with
time-efficient vector-product calculations. Furthermore, we establish the
approximation error bounds for the proposed algorithms. Experiments in
likelihood evaluation and adjoint optimization demonstrate the superior
accuracy and reduced computational cost of our proposed algorithms.
Additionally, based on the novel outer-product formulation of DF, we design the
first numerical verification experiment for the optimal transport property of
the general PF-ODE deduced map.

</details>


### [200] [Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs](https://arxiv.org/abs/2505.23270)
*Haokun Chen,Yueqi Zhang,Yuan Bi,Yao Zhang,Tong Liu,Jinhe Bi,Jian Lan,Jindong Gu,Claudia Grosser,Denis Krompass,Nassir Navab,Volker Tresp*

Key words: 大语言模型, 去学习, 审计框架, 中间激活扰动, GDPR

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种针对大语言模型（LLMs）去学习算法的全面审计框架，包括基准数据集、算法及审计方法，并探索了基于中间激活扰动的新技术。

Motivation: 由于大语言模型训练数据常涉及敏感或版权内容，且现行法规（如GDPR）要求数据可删除，去学习算法成为研究重点，但其有效性评估仍具挑战性。

Method: 提出包含三个基准数据集、六种去学习算法和五种基于提示审计方法的框架，并引入基于中间激活扰动的新技术。

Result: 通过多种审计算法评估了不同去学习策略的有效性和鲁棒性，新技术弥补了仅依赖输入输出审计的局限性。

Conclusion: 该框架为去学习算法提供了更全面的评估手段，新技术提升了审计能力。

Abstract: In recent years, Large Language Models (LLMs) have achieved remarkable
advancements, drawing significant attention from the research community. Their
capabilities are largely attributed to large-scale architectures, which require
extensive training on massive datasets. However, such datasets often contain
sensitive or copyrighted content sourced from the public internet, raising
concerns about data privacy and ownership. Regulatory frameworks, such as the
General Data Protection Regulation (GDPR), grant individuals the right to
request the removal of such sensitive information. This has motivated the
development of machine unlearning algorithms that aim to remove specific
knowledge from models without the need for costly retraining. Despite these
advancements, evaluating the efficacy of unlearning algorithms remains a
challenge due to the inherent complexity and generative nature of LLMs. In this
work, we introduce a comprehensive auditing framework for unlearning
evaluation, comprising three benchmark datasets, six unlearning algorithms, and
five prompt-based auditing methods. By using various auditing algorithms, we
evaluate the effectiveness and robustness of different unlearning strategies.
To explore alternatives beyond prompt-based auditing, we propose a novel
technique that leverages intermediate activation perturbations, addressing the
limitations of auditing methods that rely solely on model inputs and outputs.

</details>


### [201] [Comparative Analysis of the Land Use and Land Cover Changes in Different Governorates of Oman using Spatiotemporal Multi-spectral Satellite Data](https://arxiv.org/abs/2505.23285)
*Muhammad Shafi,Syed Mohsin Bokhari*

Key words: 土地覆被与土地利用变化, Sentinel-2, 监督机器学习, 阿曼苏丹国, 多光谱数据

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究利用Sentinel-2卫星数据和监督机器学习算法，分析和比较了2016至2021年阿曼苏丹国各省的土地覆被和土地利用（LULC）变化。

Motivation: 土地覆被和土地利用变化在资源管理、城市化、土壤和环境保护以及促进可持续发展中具有关键作用，研究旨在通过卫星数据监测和预测这些变化。

Method: 使用了Sentinel-2多光谱时空数据，结合监督机器学习算法对不同土地覆被（如水体、作物、城市等）进行分类和训练，并在研究区域内应用模型进行LULC变化的比较评估。

Result: 研究发现并在给定的时间框架内有效评估了阿曼苏丹国各省的LULC变化。

Conclusion: 该研究通过多光谱数据与机器学习算法的结合，为LULC变化的监测和预测提供了有效工具。

Abstract: Land cover and land use (LULC) changes are key applications of satellite
imagery, and they have critical roles in resource management, urbanization,
protection of soils and the environment, and enhancing sustainable development.
The literature has heavily utilized multispectral spatiotemporal satellite data
alongside advanced machine learning algorithms to monitor and predict LULC
changes. This study analyzes and compares LULC changes across various
governorates (provinces) of the Sultanate of Oman from 2016 to 2021 using
annual time steps. For the chosen region, multispectral spatiotemporal data
were acquired from the open-source Sentinel-2 satellite dataset. Supervised
machine learning algorithms were used to train and classify different land
covers, such as water bodies, crops, urban, etc. The constructed model was
subsequently applied within the study region, allowing for an effective
comparative evaluation of LULC changes within the given timeframe.

</details>


### [202] [Score-based Generative Modeling for Conditional Independence Testing](https://arxiv.org/abs/2505.23309)
*Yixin Ren,Chenghou Jin,Yewei Xia,Li Ke,Longtao Huang,Hui Xue,Hao Zhang,Jihong Guan,Shuigeng Zhou*

Key words: 条件独立性检验、生成模型、分数匹配、Langevin动力学

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于分数生成模型的全新条件独立性检验方法，通过切片条件分数匹配和Langevin动力学采样，实现了更精确的Type I误差控制和更强的检验能力，显著优于现有方法。

Motivation: 现有的基于生成模型的条件独立性检验方法（如GAN）在条件分布建模和训练稳定性上表现不佳，导致性能不理想，因此需要一种更可靠的方法。

Method: 采用切片条件分数匹配准确估计条件分数，结合Langevin动力学采样生成零假设样本，并加入拟合优度阶段验证生成样本。

Result: 在合成和真实数据集上的实验表明，该方法显著优于现有最先进方法。

Conclusion: 该方法为基于生成模型的条件独立性检验提供了新的有效途径。

Abstract: Determining conditional independence (CI) relationships between random
variables is a fundamental yet challenging task in machine learning and
statistics, especially in high-dimensional settings. Existing generative
model-based CI testing methods, such as those utilizing generative adversarial
networks (GANs), often struggle with undesirable modeling of conditional
distributions and training instability, resulting in subpar performance. To
address these issues, we propose a novel CI testing method via score-based
generative modeling, which achieves precise Type I error control and strong
testing power. Concretely, we first employ a sliced conditional score matching
scheme to accurately estimate conditional score and use Langevin dynamics
conditional sampling to generate null hypothesis samples, ensuring precise Type
I error control. Then, we incorporate a goodness-of-fit stage into the method
to verify generated samples and enhance interpretability in practice. We
theoretically establish the error bound of conditional distributions modeled by
score-based generative models and prove the validity of our CI tests. Extensive
experiments on both synthetic and real-world datasets show that our method
significantly outperforms existing state-of-the-art methods, providing a
promising way to revitalize generative model-based CI testing.

</details>


### [203] [Efficient Parameter Estimation for Bayesian Network Classifiers using Hierarchical Linear Smoothing](https://arxiv.org/abs/2505.23320)
*Connor Cooper,Geoffrey I. Webb,Daniel F. Schmidt*

Key words: 贝叶斯网络分类器, 参数估计, 对数线性回归, 分层狄利克雷过程, 分类性能

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新颖的贝叶斯网络分类器参数估计方法，通过使用对数线性回归逼近分层狄利克雷过程（HDPs）的行为，在保持高效的同时提升了性能。

Motivation: 传统的贝叶斯网络分类器（BNCs）学习方法在性能上不及随机森林等领先分类方法，且现有的基于HDPs的参数平滑技术既复杂又不灵活。

Method: 论文采用对数线性回归来近似HDPs的行为，结合线性模型的灵活性和简单性优化参数估计。

Result: 实验表明，新方法不仅性能优于HDP平滑，计算速度快了几个数量级，并能与随机森林在分类数据上保持竞争性。

Conclusion: 所提出的对数线性回归方法为BNCs的参数估计提供了高效、灵活且易于解释的替代方案。

Abstract: Bayesian network classifiers (BNCs) possess a number of properties desirable
for a modern classifier: They are easily interpretable, highly scalable, and
offer adaptable complexity. However, traditional methods for learning BNCs have
historically underperformed when compared to leading classification methods
such as random forests. Recent parameter smoothing techniques using
hierarchical Dirichlet processes (HDPs) have enabled BNCs to achieve
performance competitive with random forests on categorical data, but these
techniques are relatively inflexible, and require a complicated, specialized
sampling process. In this paper, we introduce a novel method for parameter
estimation that uses a log-linear regression to approximate the behaviour of
HDPs. As a linear model, our method is remarkably flexible and simple to
interpret, and can leverage the vast literature on learning linear models. Our
experiments show that our method can outperform HDP smoothing while being
orders of magnitude faster, remaining competitive with random forests on
categorical data.

</details>


### [204] [X2Graph for Cancer Subtyping Prediction on Biological Tabular Data](https://arxiv.org/abs/2505.23334)
*Tu Bui,Mohamed Suliman,Aparajita Haldar,Mohammed Amer,Serban Georgescu*

Key words: 深度学习，表格数据，医疗数据分析，图神经网络，癌症亚型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: X2Graph是一个新型深度学习方法，通过将表格数据转换为图结构并利用外部知识关系，在小型生物医学数据集上表现优于传统方法。

Motivation: 尽管深度学习在文本、音频和图像数据上取得了显著成效，但在医学领域的小型表格数据集上其优势尚不明确，因此提出X2Graph来解决这一问题。

Method: X2Graph利用列间关系的外部知识（如基因交互）将表格样本转换为图结构，并应用标准消息传递算法进行建模。

Result: X2Graph在三个癌症亚型数据集上表现优于现有的树基和深度学习方法。

Conclusion: X2Graph在小型生物医学表格数据上展示了深度学习方法的潜力，尤其是通过图结构转换和外部知识利用。

Abstract: Despite the transformative impact of deep learning on text, audio, and image
datasets, its dominance in tabular data, especially in the medical domain where
data are often scarce, remains less clear. In this paper, we propose X2Graph, a
novel deep learning method that achieves strong performance on small biological
tabular datasets. X2Graph leverages external knowledge about the relationships
between table columns, such as gene interactions, to convert each sample into a
graph structure. This transformation enables the application of standard
message passing algorithms for graph modeling. Our X2Graph method demonstrates
superior performance compared to existing tree-based and deep learning methods
across three cancer subtyping datasets.

</details>


### [205] [Matryoshka Model Learning for Improved Elastic Student Models](https://arxiv.org/abs/2505.23337)
*Chetan Verma,Aditya Srinivas Timmaraju,Cho Jui-Hsieh,Suyash Damle,Ngot Bui,Yang Zhang,Wen Chen,Xin Liu,Prateek Jain,Inderjit S Dhillon*

Key words: MatTA, Teacher-TA-Student, 模型蒸馏, 工业级ML, 服务成本优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出MatTA框架，通过Teacher-TA-Student方法训练多个高效学生模型，无需多次训练即可提供多种服务选项，并在实际生产中验证了其有效性。

Motivation: 工业级ML模型需要满足快速变化的服务约束，传统开发方式资源消耗大。MatTA旨在通过一次训练生成多个高效学生模型，平衡准确性与服务成本。

Method: 采用Teacher-TA-Student方法，引入高容量的TA模型作为桥梁，帮助学生模型更好地学习教师模型并融入领域专长，从而提取多个高效学生模型。

Result: 在内部数据集和GPT-2 Medium上验证，生产环境A/B测试显示关键指标提升20%，公开模型在SAT Math和LAMBADA基准上分别提升24%和10%。

Conclusion: MatTA通过一次训练生成多模型选项，显著提升效率并降低服务成本，适用于实际生产环境。

Abstract: Industry-grade ML models are carefully designed to meet rapidly evolving
serving constraints, which requires significant resources for model
development. In this paper, we propose MatTA, a framework for training multiple
accurate Student models using a novel Teacher-TA-Student recipe. TA models are
larger versions of the Student models with higher capacity, and thus allow
Student models to better relate to the Teacher model and also bring in more
domain-specific expertise. Furthermore, multiple accurate Student models can be
extracted from the TA model. Therefore, despite only one training run, our
methodology provides multiple servable options to trade off accuracy for lower
serving cost. We demonstrate the proposed method, MatTA, on proprietary
datasets and models. Its practical efficacy is underscored by live A/B tests
within a production ML system, demonstrating 20% improvement on a key metric.
We also demonstrate our method on GPT-2 Medium, a public model, and achieve
relative improvements of over 24% on SAT Math and over 10% on the LAMBADA
benchmark.

</details>


### [206] [Graph Positional Autoencoders as Self-supervised Learners](https://arxiv.org/abs/2505.23345)
*Yang Liu,Deyu Bo,Wenxuan Cao,Yuan Fang,Yawen Li,Chuan Shi*

Key words: graph self-supervised learning, graph autoencoders, positional encoding, node classification, graph property prediction

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: GraphPAE proposes a dual-path autoencoder for graph self-supervised learning, enhancing representation by reconstructing node features and positions, outperforming traditional methods significantly.

Motivation: Traditional graph autoencoders (GAEs) focus on low-frequency signals and fail to capture structural diversity, motivating the need for a method that learns diverse frequency information.

Method: GraphPAE uses a dual-path architecture: one path reconstructs node features with positional encoding, while the other refines node positions using representations to approximate eigenvectors.

Result: GraphPAE achieves state-of-the-art performance in tasks like heterophilic node classification and graph property prediction, surpassing baselines by a large margin.

Conclusion: The dual-path approach effectively captures diverse graph signals, demonstrating the importance of both feature and position reconstruction in self-supervised graph learning.

Abstract: Graph self-supervised learning seeks to learn effective graph representations
without relying on labeled data. Among various approaches, graph autoencoders
(GAEs) have gained significant attention for their efficiency and scalability.
Typically, GAEs take incomplete graphs as input and predict missing elements,
such as masked nodes or edges. While effective, our experimental investigation
reveals that traditional node or edge masking paradigms primarily capture
low-frequency signals in the graph and fail to learn the expressive structural
information. To address these issues, we propose Graph Positional Autoencoders
(GraphPAE), which employs a dual-path architecture to reconstruct both node
features and positions. Specifically, the feature path uses positional encoding
to enhance the message-passing processing, improving GAE's ability to predict
the corrupted information. The position path, on the other hand, leverages node
representations to refine positions and approximate eigenvectors, thereby
enabling the encoder to learn diverse frequency information. We conduct
extensive experiments to verify the effectiveness of GraphPAE, including
heterophilic node classification, graph property prediction, and transfer
learning. The results demonstrate that GraphPAE achieves state-of-the-art
performance and consistently outperforms baselines by a large margin.

</details>


### [207] [Sentinel: Scheduling Live Streams with Proactive Anomaly Detection in Crowdsourced Cloud-Edge Platforms](https://arxiv.org/abs/2505.23347)
*Yuting Li,Shaoyuan Huang,Tengwen Zhang,Cheng Zhang,Xiaofei Wang,Victor C. M. Leung*

Key words: 直播服务, 边缘计算, 异常检测, 调度优化, Sentinel

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了Sentinel框架，通过主动异常检测优化直播平台的调度问题，显著降低了异常频率并提升了收益和调度速度。

Motivation: 面对直播服务的快速增长，Crowdsourced Cloud-edge服务平台的调度优化面临异常检测和实时性挑战，现有策略难以实用化。

Method: Sentinel采用两阶段Pre-Post-Scheduling范式：预调度阶段进行异常检测并构建策略池，后调度阶段根据请求触发预生成策略执行调度。

Result: 在实际数据集上的实验表明，Sentinel将异常频率降低70%，收益提升74%，调度速度翻倍。

Conclusion: Sentinel通过主动异常检测和两阶段调度，有效解决了不稳定平台中的调度优化问题，显著提升性能和收益。

Abstract: With the rapid growth of live streaming services, Crowdsourced Cloud-edge
service Platforms (CCPs) are playing an increasingly important role in meeting
the increasing demand. Although stream scheduling plays a critical role in
optimizing CCPs' revenue, most optimization strategies struggle to achieve
practical results due to various anomalies in unstable CCPs. Additionally, the
substantial scale of CCPs magnifies the difficulties of anomaly detection in
time-sensitive scheduling. To tackle these challenges, this paper proposes
Sentinel, a proactive anomaly detection-based scheduling framework. Sentinel
models the scheduling process as a two-stage Pre-Post-Scheduling paradigm: in
the pre-scheduling stage, Sentinel conducts anomaly detection and constructs a
strategy pool; in the post-scheduling stage, upon request arrival, it triggers
an appropriate scheduling based on a pre-generated strategy to implement the
scheduling process. Extensive experiments on realistic datasets show that
Sentinel significantly reduces anomaly frequency by 70%, improves revenue by
74%, and doubles the scheduling speed.

</details>


### [208] [Towards Reward Fairness in RLHF: From a Resource Allocation Perspective](https://arxiv.org/abs/2505.23349)
*Sheng Ouyang,Yulan Hu,Ge Chen,Qingyang Li,Fuzheng Zhang,Yong Liu*

Key words: 奖励不公平，资源分配，RLHF，公平性正则化，公平性系数

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 针对奖励不公平问题，提出了一种资源分配视角的通用方法，通过公平性正则化和公平性系数提升LLM的对齐效果。

Motivation: 奖励在RLHF中至关重要，但不完美的奖励可能带有偏见，影响LLM的对齐效果。本文旨在通过这些偏见视为奖励不公平问题，并提出通用解决方案。

Method: 将偏好学习建模为资源分配问题，提出公平性正则化和公平性系数两种方法，应用于验证和强化学习场景。

Result: 实验表明，该方法能更公平地使LLM与人类偏好对齐。

Conclusion: 资源分配视角的公平性方法有效缓解了奖励偏见问题。

Abstract: Rewards serve as proxies for human preferences and play a crucial role in
Reinforcement Learning from Human Feedback (RLHF). However, if these rewards
are inherently imperfect, exhibiting various biases, they can adversely affect
the alignment of large language models (LLMs). In this paper, we collectively
define the various biases present in rewards as the problem of reward
unfairness. We propose a bias-agnostic method to address the issue of reward
fairness from a resource allocation perspective, without specifically designing
for each type of bias, yet effectively mitigating them. Specifically, we model
preference learning as a resource allocation problem, treating rewards as
resources to be allocated while considering the trade-off between utility and
fairness in their distribution. We propose two methods, Fairness Regularization
and Fairness Coefficient, to achieve fairness in rewards. We apply our methods
in both verification and reinforcement learning scenarios to obtain a fairness
reward model and a policy model, respectively. Experiments conducted in these
scenarios demonstrate that our approach aligns LLMs with human preferences in a
more fair manner.

</details>


### [209] [Grower-in-the-Loop Interactive Reinforcement Learning for Greenhouse Climate Control](https://arxiv.org/abs/2505.23355)
*Maxiu Xiao,Jianglin Lan,Jingxing Yu,Eldert van Henten,Congcong Sun*

Key words: 温室气候控制、强化学习、交互式学习、不完美输入、神经网络

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究探讨了将交互式强化学习（RL）应用于温室气候控制的可行性，通过开发三种代表性算法（奖励塑造、策略塑造和控制共享），分析不完美输入的影响，并提出神经网络方法增强鲁棒性。结果显示，策略塑造和控制共享在处理不完美输入时表现更优。

Motivation: 温室气候控制对作物生长至关重要，但现有RL方法面临训练效率低和初始条件依赖性强的问题。交互式RL结合种植者输入可能解决这些问题，但尚未应用于温室控制且面临输入不完美的挑战。

Method: 开发三种交互式RL算法（奖励塑造、策略塑造、控制共享），分析输入特性矛盾，提出神经网络方法增强鲁棒性，并在模拟温室环境中进行全面评估。

Result: 策略塑造和控制共享算法在不完美输入下分别提升利润8.4%和6.8%，而奖励塑造因对输入敏感导致利润下降9.4%。

Conclusion: 交互式RL结合不完美种植者输入可提升性能，但需根据输入特性选择合适的交互机制。

Abstract: Climate control is crucial for greenhouse production as it directly affects
crop growth and resource use. Reinforcement learning (RL) has received
increasing attention in this field, but still faces challenges, including
limited training efficiency and high reliance on initial learning conditions.
Interactive RL, which combines human (grower) input with the RL agent's
learning, offers a potential solution to overcome these challenges. However,
interactive RL has not yet been applied to greenhouse climate control and may
face challenges related to imperfect inputs. Therefore, this paper aims to
explore the possibility and performance of applying interactive RL with
imperfect inputs into greenhouse climate control, by: (1) developing three
representative interactive RL algorithms tailored for greenhouse climate
control (reward shaping, policy shaping and control sharing); (2) analyzing how
input characteristics are often contradicting, and how the trade-offs between
them make grower's inputs difficult to perfect; (3) proposing a neural
network-based approach to enhance the robustness of interactive RL agents under
limited input availability; (4) conducting a comprehensive evaluation of the
three interactive RL algorithms with imperfect inputs in a simulated greenhouse
environment. The demonstration shows that interactive RL incorporating
imperfect grower inputs has the potential to improve the performance of the RL
agent. RL algorithms that influence action selection, such as policy shaping
and control sharing, perform better when dealing with imperfect inputs,
achieving 8.4% and 6.8% improvement in profit, respectively. In contrast,
reward shaping, an algorithm that manipulates the reward function, is sensitive
to imperfect inputs and leads to a 9.4% decrease in profit. This highlights the
importance of selecting an appropriate mechanism when incorporating imperfect
inputs.

</details>


### [210] [Dynamic Spectral Backpropagation for Efficient Neural Network Training](https://arxiv.org/abs/2505.23369)
*Mannmohan Muthuraman*

Key words: DSBP, 谱反向传播, 资源受限, 平坦极小值, 少样本学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: DSBP通过投影梯度到主特征向量上降低复杂度并促进平坦极小值，优于SAM、LoRA和MAML，适用于资源受限的神经网络训练。

Motivation: 解决资源受限下的神经网络训练问题，提升鲁棒性、少样本学习和硬件效率。

Method: 提出动态谱反向传播（DSBP）及其五个扩展（动态谱推断、谱架构优化、谱元学习、谱迁移正则化和李代数动态），结合三阶随机微分方程和PAC-Bayes理论。

Result: 在CIFAR-10等数据集上优于SAM、LoRA和MAML。

Conclusion: DSBP在资源效率上表现优异，未来研究方向包括可扩展性、偏差缓解和伦理考量。

Abstract: Dynamic Spectral Backpropagation (DSBP) enhances neural network training
under resource constraints by projecting gradients onto principal eigenvectors,
reducing complexity and promoting flat minima. Five extensions are proposed,
dynamic spectral inference, spectral architecture optimization, spectral meta
learning, spectral transfer regularization, and Lie algebra inspired dynamics,
to address challenges in robustness, fewshot learning, and hardware efficiency.
Supported by a third order stochastic differential equation (SDE) and a PAC
Bayes limit, DSBP outperforms Sharpness Aware Minimization (SAM), Low Rank
Adaptation (LoRA), and Model Agnostic Meta Learning (MAML) on CIFAR 10, Fashion
MNIST, MedMNIST, and Tiny ImageNet, as demonstrated through extensive
experiments and visualizations. Future work focuses on scalability, bias
mitigation, and ethical considerations.

</details>


### [211] [Meta-Learning Approaches for Speaker-Dependent Voice Fatigue Models](https://arxiv.org/abs/2505.23378)
*Roseline Polle,Agnes Norbury,Alexandra Livia Georgescu,Nicholas Cummins,Stefano Goria*

Key words: 说话人依赖模型, 元学习, 语音健康监测, Transformer, 疲劳预测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出将说话人依赖模型构建为元学习问题，探索了三种方法，证明其优于传统混合效应模型，其中基于Transformer的方法表现最佳。

Motivation: 说话人依赖模型能显著提升语音健康监测性能，但传统混合效应模型计算成本高，难以实际应用。

Method: 提出三种元学习方法（基于距离的集成模型、原型网络、Transformer序列模型），使用预训练语音嵌入，在大型纵向数据集上评估。

Result: 所有元学习方法均优于传统模型，基于Transformer的方法性能最强。

Conclusion: 元学习方法在语音疲劳监测中更高效且性能更优，适合实际应用。

Abstract: Speaker-dependent modelling can substantially improve performance in
speech-based health monitoring applications. While mixed-effect models are
commonly used for such speaker adaptation, they require computationally
expensive retraining for each new observation, making them impractical in a
production environment. We reformulate this task as a meta-learning problem and
explore three approaches of increasing complexity: ensemble-based distance
models, prototypical networks, and transformer-based sequence models. Using
pre-trained speech embeddings, we evaluate these methods on a large
longitudinal dataset of shift workers (N=1,185, 10,286 recordings), predicting
time since sleep from speech as a function of fatigue, a symptom commonly
associated with ill-health. Our results demonstrate that all meta-learning
approaches tested outperformed both cross-sectional and conventional
mixed-effects models, with a transformer-based method achieving the strongest
performance.

</details>


### [212] [Automated Modeling Method for Pathloss Model Discovery](https://arxiv.org/abs/2505.23383)
*Ahmad Anaqreh,Shih-Kai Chou,Mihael Mohorčič,Carolina Fortuna*

Key words: 路径损耗模型, 人工智能, 可解释性, 深度符号回归, Kolmogorov-Arnold网络

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种基于AI的新方法，用于快速发现路径损耗模型并保持可解释性，包括深度符号回归和Kolmogorov-Arnold网络两种技术，结果显示其优于传统方法。

Motivation: 传统统计方法在无线通信系统的传播建模中存在精度和互操作性不足的问题，而AI方法虽被广泛采用但可解释性不足。本论文旨在解决这一挑战。

Method: 提出了两种技术：深度符号回归（完全可解释）和Kolmogorov-Arnold网络（两级可解释），并在合成和真实数据集上进行了评估。

Result: Kolmogorov-Arnold网络的R^2值接近1且预测误差最小，深度符号回归生成紧凑模型但精度一般。自动化方法比传统方法预测误差降低75%。

Conclusion: 自动化方法提供了高精度和可解释的解决方案，有望提高下一代路径损耗模型的发现效率。

Abstract: Modeling propagation is the cornerstone for designing and optimizing
next-generation wireless systems, with a particular emphasis on 5G and beyond
era. Traditional modeling methods have long relied on statistic-based
techniques to characterize propagation behavior across different environments.
With the expansion of wireless communication systems, there is a growing demand
for methods that guarantee the accuracy and interoperability of modeling.
Artificial intelligence (AI)-based techniques, in particular, are increasingly
being adopted to overcome this challenge, although the interpretability is not
assured with most of these methods. Inspired by recent advancements in AI, this
paper proposes a novel approach that accelerates the discovery of path loss
models while maintaining interpretability. The proposed method automates the
model formulation, evaluation, and refinement, facilitating model discovery. We
evaluate two techniques: one based on Deep Symbolic Regression, offering full
interpretability, and the second based on Kolmogorov-Arnold Networks, providing
two levels of interpretability. Both approaches are evaluated on two synthetic
and two real-world datasets. Our results show that Kolmogorov-Arnold Networks
achieve R^2 values close to 1 with minimal prediction error, while Deep
Symbolic Regression generates compact models with moderate accuracy. Moreover,
on the selected examples, we demonstrate that automated methods outperform
traditional methods, achieving up to 75% reduction in prediction errors,
offering accurate and explainable solutions with potential to increase the
efficiency of discovering next-generation path loss models.

</details>


### [213] [Buffer-free Class-Incremental Learning with Out-of-Distribution Detection](https://arxiv.org/abs/2505.23412)
*Srishti Gupta,Daniele Angioni,Maura Pintor,Ambra Demontis,Lea Schönherr,Battista Biggio,Fabio Roli*

Key words: 类增量学习, 开放世界, OOD检测, 内存缓冲区

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文探讨了在开放世界场景中，如何通过后验OOD检测方法替代基于内存缓冲区的OOD检测，以实现高效且保护隐私的类增量学习。

Motivation: 开放世界中的类增量学习需处理新类别和未知类别输入，传统方法依赖内存缓冲区，但存在隐私、扩展性和训练时间问题。

Method: 分析并应用后验OOD检测方法，在推理时替代缓冲区，实验验证其在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上的效果。

Result: 缓冲区方法在类增量学习和未知样本拒绝方面表现相当或更好。

Conclusion: 后验OOD检测可有效替代缓冲区方法，为开放世界类增量学习提供高效、隐私保护的解决方案。

Abstract: Class-incremental learning (CIL) poses significant challenges in open-world
scenarios, where models must not only learn new classes over time without
forgetting previous ones but also handle inputs from unknown classes that a
closed-set model would misclassify. Recent works address both issues by
(i)~training multi-head models using the task-incremental learning framework,
and (ii) predicting the task identity employing out-of-distribution (OOD)
detectors. While effective, the latter mainly relies on joint training with a
memory buffer of past data, raising concerns around privacy, scalability, and
increased training time. In this paper, we present an in-depth analysis of
post-hoc OOD detection methods and investigate their potential to eliminate the
need for a memory buffer. We uncover that these methods, when applied
appropriately at inference time, can serve as a strong substitute for
buffer-based OOD detection. We show that this buffer-free approach achieves
comparable or superior performance to buffer-based methods both in terms of
class-incremental learning and the rejection of unknown samples. Experimental
results on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets support our findings,
offering new insights into the design of efficient and privacy-preserving CIL
systems for open-world settings.

</details>


### [214] [Bidirectional predictive coding](https://arxiv.org/abs/2505.23415)
*Gaspard Oliviers,Mufeng Tang,Rafal Bogacz*

Key words: 预测编码, 双向推理, 生成模型, 判别模型, 能量景观

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种双向预测编码（bPC）模型，结合了生成式和判别式推理，并在生物可解释的电路中实现，其性能优于传统单向模型。

Motivation: 现有预测编码模型只能单向处理（生成或判别），而生物脑需要双向推理，因此需要一种能结合两者的模型。

Method: 提出双向预测编码（bPC），通过设计适合双重任务的能量景观同时优化生成和判别任务。

Result: bPC在生成和判别任务中表现与或优于单向模型，并在多模态学习和信息缺失推理中展现更优性能。

Conclusion: bPC更贴近生物视觉推理机制，为理解脑计算提供新视角。

Abstract: Predictive coding (PC) is an influential computational model of visual
learning and inference in the brain. Classical PC was proposed as a top-down
generative model, where the brain actively predicts upcoming visual inputs, and
inference minimises the prediction errors. Recent studies have also shown that
PC can be formulated as a discriminative model, where sensory inputs predict
neural activities in a feedforward manner. However, experimental evidence
suggests that the brain employs both generative and discriminative inference,
while unidirectional PC models show degraded performance in tasks requiring
bidirectional processing. In this work, we propose bidirectional PC (bPC), a PC
model that incorporates both generative and discriminative inference while
maintaining a biologically plausible circuit implementation. We show that bPC
matches or outperforms unidirectional models in their specialised generative or
discriminative tasks, by developing an energy landscape that simultaneously
suits both tasks. We also demonstrate bPC's superior performance in two
biologically relevant tasks including multimodal learning and inference with
missing information, suggesting that bPC resembles biological visual inference
more closely.

</details>


### [215] [OTPTO: Joint Product Selection and Inventory Optimization in Fresh E-commerce Front-End Warehouses](https://arxiv.org/abs/2505.23421)
*Zheming Zhang,Yan Jiang,Qingshan Li,Ai Han*

Key words: 生鲜电商, 前端仓库, 库存管理, 订单完成率, OTPTO方法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种名为OTPTO的多任务优化方法，用于改进前端仓库的库存管理，通过联合优化产品选择和库存管理，显著提高了订单完成率和消费者满意度。

Motivation: 在中国的生鲜电商市场中，前端仓库的库存管理直接关系到客户满意度和竞争力。传统的预测后优化（PTO）方法未能有效结合预测与库存目标，因此需要一种更高效的方法。

Method: 采用多任务优化-预测-优化（OTPTO）方法，包括0-1混合整数规划模型（OM1）、产品选择模型（PM1）和库存模型（PM2），并通过后处理算法（OM2）优化结果。

Result: 在JD.com的7Fresh平台上测试显示，OTPTO方法将订单完成率提高了4.34%（相对提升7.05%），并将与最优订单完成率的差距缩小了5.27%。

Conclusion: OTPTO方法在生鲜电商前端仓库库存管理中表现出色，为未来研究提供了有价值的见解。

Abstract: In China's competitive fresh e-commerce market, optimizing operational
strategies, especially inventory management in front-end warehouses, is key to
enhance customer satisfaction and to gain a competitive edge. Front-end
warehouses are placed in residential areas to ensure the timely delivery of
fresh goods and are usually in small size. This brings the challenge of
deciding which goods to stock and in what quantities, taking into account
capacity constraints. To address this issue, traditional predict-then-optimize
(PTO) methods that predict sales and then decide on inventory often don't align
prediction with inventory goals, as well as fail to prioritize consumer
satisfaction. This paper proposes a multi-task
Optimize-then-Predict-then-Optimize (OTPTO) approach that jointly optimizes
product selection and inventory management, aiming to increase consumer
satisfaction by maximizing the full order fulfillment rate. Our method employs
a 0-1 mixed integer programming model OM1 to determine historically optimal
inventory levels, and then uses a product selection model PM1 and the stocking
model PM2 for prediction. The combined results are further refined through a
post-processing algorithm OM2. Experimental results from JD.com's 7Fresh
platform demonstrate the robustness and significant advantages of our OTPTO
method. Compared to the PTO approach, our OTPTO method substantially enhances
the full order fulfillment rate by 4.34% (a relative increase of 7.05%) and
narrows the gap to the optimal full order fulfillment rate by 5.27%. These
findings substantiate the efficacy of the OTPTO method in managing inventory at
front-end warehouses of fresh e-commerce platforms and provide valuable
insights for future research in this domain.

</details>


### [216] [Enhanced DACER Algorithm with High Diffusion Efficiency](https://arxiv.org/abs/2505.23426)
*Yinuo Wang,Mining Tan,Wenjun Zou,Haotian Lin,Xujie Song,Wenxuan Wang,Tong Liu,Likun Wang,Guojian Zhan,Tianze Zhu,Shiqi Liu,Jingliang Duan,Shengbo Eben Li*

Key words: 扩散模型, 强化学习, Q梯度场, 时间加权, 多模态

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: DACER2通过引入Q梯度场目标和时间加权机制，显著提升了扩散策略的训练和推理效率，仅需5步扩散即可在MuJoCo控制任务中实现最优性能，并增强多模态能力。

Motivation: 扩散模型在离线RL和模仿学习中表现出色，但在在线RL中，DACER因需大量扩散步骤导致训练效率低，且直接减少步骤会降低性能。DACER2旨在通过高效训练和推理改进扩散策略。

Method: 提出Q梯度场目标作为辅助优化目标，指导每一步去噪过程，并引入时间加权机制以根据不同扩散阶段调整噪声消除和动作优化。

Result: 在MuJoCo基准测试和多模态任务中，DACER2仅用5步扩散即可实现最优性能，同时多模态能力优于DACER。

Conclusion: DACER2通过Q梯度场和时间加权机制，高效平衡了性能与效率，为实时在线RL应用提供了可行的扩散策略解决方案。

Abstract: Due to their expressive capacity, diffusion models have shown great promise
in offline RL and imitation learning. Diffusion Actor-Critic with Entropy
Regulator (DACER) extended this capability to online RL by using the reverse
diffusion process as a policy approximator, trained end-to-end with policy
gradient methods, achieving strong performance. However, this comes at the cost
of requiring many diffusion steps, which significantly hampers training
efficiency, while directly reducing the steps leads to noticeable performance
degradation. Critically, the lack of inference efficiency becomes a significant
bottleneck for applying diffusion policies in real-time online RL settings. To
improve training and inference efficiency while maintaining or even enhancing
performance, we propose a Q-gradient field objective as an auxiliary
optimization target to guide the denoising process at each diffusion step.
Nonetheless, we observe that the independence of the Q-gradient field from the
diffusion time step negatively impacts the performance of the diffusion policy.
To address this, we introduce a temporal weighting mechanism that enables the
model to efficiently eliminate large-scale noise in the early stages and refine
actions in the later stages. Experimental results on MuJoCo benchmarks and
several multimodal tasks demonstrate that the DACER2 algorithm achieves
state-of-the-art performance in most MuJoCo control tasks with only five
diffusion steps, while also exhibiting stronger multimodality compared to
DACER.

</details>


### [217] [On the Validity of Head Motion Patterns as Generalisable Depression Biomarkers](https://arxiv.org/abs/2505.23427)
*Monika Gahalawat,Maneesh Bilalpur,Raul Fernandez Rojas,Jeffrey F. Cohn,Roland Goecke,Ramanathan Subramanian*

Key words: 抑郁症评估、头部运动、kineme、机器学习、泛化性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究通过分析头部运动的基本单元（kinemes）作为抑郁症严重性评估的生物标志，验证了其高效性和泛化性，并在不同文化和情境的数据集上取得了优异表现。

Motivation: 抑郁症对全球数百万人产生负面影响，而现有研究中头部运动的潜力未被充分探索。同时，单一数据集验证模型限制了模型的泛化能力。

Method: 利用来自德国、澳大利亚和美国的三个抑郁症数据集，通过两种方法（k-fold交叉验证和模型重用）评估基于kineme的特征，并与传统机器学习方法结合进行分类和回归任务。

Result: 头部运动模式是高效的生物标志，在不同数据集上表现出色，尤其是在AVEC2013数据集上取得了第二低的MAE，且kineme特征在分类和回归任务中的泛化性优于原始头部运动描述符和其他视觉行为特征。

Conclusion: kineme特征为抑郁症严重性评估提供了可靠且可泛化的方法，展现了头部运动作为行为标志的潜力。

Abstract: Depression is a debilitating mood disorder negatively impacting millions
worldwide. While researchers have explored multiple verbal and non-verbal
behavioural cues for automated depression assessment, head motion has received
little attention thus far. Further, the common practice of validating machine
learning models via a single dataset can limit model generalisability. This
work examines the effectiveness and generalisability of models utilising
elementary head motion units, termed kinemes, for depression severity
estimation. Specifically, we consider three depression datasets from different
western cultures (German: AVEC2013, Australian: Blackdog and American: Pitt
datasets) with varied contextual and recording settings to investigate the
generalisability of the derived kineme patterns via two methods: (i) k-fold
cross-validation over individual/multiple datasets, and (ii) model reuse on
other datasets. Evaluating classification and regression performance with
classical machine learning methods, our results show that: (1) head motion
patterns are efficient biomarkers for estimating depression severity, achieving
highly competitive performance for both classification and regression tasks on
a variety of datasets, including achieving the second best Mean Absolute Error
(MAE) on the AVEC2013 dataset, and (2) kineme-based features are more
generalisable than (a) raw head motion descriptors for binary severity
classification, and (b) other visual behavioural cues for severity estimation
(regression).

</details>


### [218] [Diversity-Aware Policy Optimization for Large Language Model Reasoning](https://arxiv.org/abs/2505.23433)
*Jian Yao,Ran Cheng,Xingyu Wu,Jibin Wu,Kay Chen Tan*

Key words: 大型语言模型,强化学习,多样性,推理能力,策略优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文通过系统研究强化学习中多样性对大型语言模型(LLM)推理的影响，提出了一个新颖的多样性感知策略优化方法，实验证明其能提升模型的推理能力。

Motivation: 尽管多样性在强化学习中扮演关键角色，但其对LLM推理的影响尚未深入探索，本文旨在填补这一空白。

Method: 设计了一种token级别的多样性指标并将其转化为实用目标，选择性地应用于正样本，集成到R1-zero训练框架中。

Result: 在12个LLM上的评估显示，解决方案多样性与新提出的Potential at k指标呈强正相关；该方法在四个数学推理基准上平均提升了3.5%。

Conclusion: 通过明确提升强化学习训练中的多样性，可显著增强LLM的推理能力，并生成更多样且稳健的解决方案。

Abstract: The reasoning capabilities of large language models (LLMs) have advanced
rapidly, particularly following the release of DeepSeek R1, which has inspired
a surge of research into data quality and reinforcement learning (RL)
algorithms. Despite the pivotal role diversity plays in RL, its influence on
LLM reasoning remains largely underexplored. To bridge this gap, this work
presents a systematic investigation into the impact of diversity in RL-based
training for LLM reasoning, and proposes a novel diversity-aware policy
optimization method. Across evaluations on 12 LLMs, we observe a strong
positive correlation between the solution diversity and Potential at k (a novel
metric quantifying an LLM's reasoning potential) in high-performing models.
This finding motivates our method to explicitly promote diversity during RL
training. Specifically, we design a token-level diversity and reformulate it
into a practical objective, then we selectively apply it to positive samples.
Integrated into the R1-zero training framework, our method achieves a 3.5
percent average improvement across four mathematical reasoning benchmarks,
while generating more diverse and robust solutions.

</details>


### [219] [Bounded-Abstention Pairwise Learning to Rank](https://arxiv.org/abs/2505.23437)
*Antonio Ferrara,Andrea Pugnana,Francesco Bonchi,Salvatore Ruggieri*

Key words: 排序系统, 弃权机制, 成对学习排序, 条件风险阈值

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种在成对学习排序任务中实现弃权的新方法，基于对排序器条件风险的阈值判断，当风险超过预设阈值时弃权。

Motivation: 排序系统在高风险领域（如健康、教育和就业）的决策中具有重要影响，但现有弃权机制主要应用于分类任务，其他机器学习范式中的应用较少。

Method: 提出了一种基于排序器条件风险阈值判断的弃权方法，系统在估计风险超过预设阈值时弃权交给人专家处理。

Result: 理论上刻画了最优弃权策略，提出了一种模型无关的插件算法用于构建弃权排序模型，并在多个数据集上验证了方法的有效性。

Conclusion: 该方法为排序任务中的弃权机制提供了理论基础和实用算法，扩展了弃权机制的应用范围。

Abstract: Ranking systems influence decision-making in high-stakes domains like health,
education, and employment, where they can have substantial economic and social
impacts. This makes the integration of safety mechanisms essential. One such
mechanism is $\textit{abstention}$, which enables algorithmic decision-making
system to defer uncertain or low-confidence decisions to human experts. While
abstention have been predominantly explored in the context of classification
tasks, its application to other machine learning paradigms remains
underexplored. In this paper, we introduce a novel method for abstention in
pairwise learning-to-rank tasks. Our approach is based on thresholding the
ranker's conditional risk: the system abstains from making a decision when the
estimated risk exceeds a predefined threshold. Our contributions are threefold:
a theoretical characterization of the optimal abstention strategy, a
model-agnostic, plug-in algorithm for constructing abstaining ranking models,
and a comprehensive empirical evaluations across multiple datasets,
demonstrating the effectiveness of our approach.

</details>


### [220] [Rethinking Regularization Methods for Knowledge Graph Completion](https://arxiv.org/abs/2505.23442)
*Linyu Li,Zhi Jin,Yuanpeng He,Dongming Jin,Haoran Duan,Zhengwei Tao,Xuan Zhang,Jiandong Li*

Key words: 知识图谱补全, 正则化, 稀疏性, 嵌入向量, SPR

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文重新思考了知识图谱补全（KGC）中正则化的应用，提出了一种新的稀疏正则化方法（SPR），通过选择性惩罚嵌入向量中的显著特征来提升模型性能。

Motivation: 现有的KGC模型未能充分利用正则化的潜力，尤其是在更深层次的应用上。本文旨在通过改进正则化方法，提升模型的性能上限。

Method: 提出SPR正则化方法，基于秩的选择性稀疏化，仅惩罚嵌入向量中的重要特征，忽略噪声成分。

Result: 实验表明，SPR正则化能够有效缓解过拟合、降低方差，并帮助多种KGC模型突破原有的性能上限。

Conclusion: SPR正则化方法在多种数据集和模型上表现优于其他正则化方法，能够显著提升KGC模型的性能。

Abstract: Knowledge graph completion (KGC) has attracted considerable attention in
recent years because it is critical to improving the quality of knowledge
graphs. Researchers have continuously explored various models. However, most
previous efforts have neglected to take advantage of regularization from a
deeper perspective and therefore have not been used to their full potential.
This paper rethinks the application of regularization methods in KGC. Through
extensive empirical studies on various KGC models, we find that carefully
designed regularization not only alleviates overfitting and reduces variance
but also enables these models to break through the upper bounds of their
original performance. Furthermore, we introduce a novel sparse-regularization
method that embeds the concept of rank-based selective sparsity into the KGC
regularizer. The core idea is to selectively penalize those components with
significant features in the embedding vector, thus effectively ignoring many
components that contribute little and may only represent noise. Various
comparative experiments on multiple datasets and multiple models show that the
SPR regularization method is better than other regularization methods and can
enable the KGC model to further break through the performance margin.

</details>


### [221] [Strategic Classification with Non-Linear Classifiers](https://arxiv.org/abs/2505.23443)
*Benyamin Trachtenberg,Nir Rosenfeld*

Key words: strategic classification, non-linear classifiers, universal approximators, performance gaps, strategic behavior

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: This paper explores strategic classification under non-linear classifiers, highlighting that universal approximators like neural nets lose their universality in strategic environments, leading to performance gaps.

Motivation: To extend the study of strategic classification beyond linear classifiers and understand how strategic behavior manifests under non-linear models.

Method: A bottom-up approach analyzing the impact of non-linearity on decision boundary points, classifier expressivity, and model class complexity.

Result: Key finding is that universal approximators (e.g., neural nets) are not universal in strategic environments, with empirical evidence showing performance gaps.

Conclusion: Non-linear classifiers in strategic classification reveal limitations of universal approximators, impacting learning performance.

Abstract: In strategic classification, the standard supervised learning setting is
extended to support the notion of strategic user behavior in the form of costly
feature manipulations made in response to a classifier. While standard learning
supports a broad range of model classes, the study of strategic classification
has, so far, been dedicated mostly to linear classifiers. This work aims to
expand the horizon by exploring how strategic behavior manifests under
non-linear classifiers and what this implies for learning. We take a bottom-up
approach showing how non-linearity affects decision boundary points, classifier
expressivity, and model classes complexity. A key finding is that universal
approximators (e.g., neural nets) are no longer universal once the environment
is strategic. We demonstrate empirically how this can create performance gaps
even on an unrestricted model class.

</details>


### [222] [Network Inversion for Uncertainty-Aware Out-of-Distribution Detection](https://arxiv.org/abs/2505.23448)
*Pirzada Suhail,Rehna Afroz,Amit Sethi*

Key words: 分布外检测，不确定性估计，网络反演，分类器训练

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种结合网络反演和分类器训练的新框架，用于同时解决分布外检测和不确定性估计问题。

Motivation: 在真实场景中，意外输入不可避免，分布外检测和不确定性估计对构建安全的机器学习系统至关重要。

Method: 通过引入“垃圾”类别并利用网络反演迭代优化分类器，使模型能够将分布外样本分到垃圾类并估计不确定性。

Result: 该方法无需外部分布外数据集或后校准技术，即可统一解决分布外检测和不确定性估计问题。

Conclusion: 该方法具有可扩展性和可解释性，为实际应用提供了一种高效的解决方案。

Abstract: Out-of-distribution (OOD) detection and uncertainty estimation (UE) are
critical components for building safe machine learning systems, especially in
real-world scenarios where unexpected inputs are inevitable. In this work, we
propose a novel framework that combines network inversion with classifier
training to simultaneously address both OOD detection and uncertainty
estimation. For a standard n-class classification task, we extend the
classifier to an (n+1)-class model by introducing a "garbage" class, initially
populated with random gaussian noise to represent outlier inputs. After each
training epoch, we use network inversion to reconstruct input images
corresponding to all output classes that initially appear as noisy and
incoherent and are therefore excluded to the garbage class for retraining the
classifier. This cycle of training, inversion, and exclusion continues
iteratively till the inverted samples begin to resemble the in-distribution
data more closely, suggesting that the classifier has learned to carve out
meaningful decision boundaries while sanitising the class manifolds by pushing
OOD content into the garbage class. During inference, this training scheme
enables the model to effectively detect and reject OOD samples by classifying
them into the garbage class. Furthermore, the confidence scores associated with
each prediction can be used to estimate uncertainty for both in-distribution
and OOD inputs. Our approach is scalable, interpretable, and does not require
access to external OOD datasets or post-hoc calibration techniques while
providing a unified solution to the dual challenges of OOD detection and
uncertainty estimation.

</details>


### [223] [Diffusion Guidance Is a Controllable Policy Improvement Operator](https://arxiv.org/abs/2505.23458)
*Kevin Frans,Seohong Park,Pieter Abbeel,Sergey Levine*

Key words: 强化学习, 扩散模型, CFGRL, 策略改进, 监督学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: CFGRL结合强化学习和生成模型，通过扩散模型指导策略改进，以监督学习的简单性实现策略提升，无需显式学习价值函数。

Motivation: 解决强化学习系统难以扩展的问题，结合生成模型的扩展性和训练简便性。

Method: 通过扩散模型的指导与策略改进建立直接关系，提出CFGRL框架，支持无显式价值函数学习。

Result: 在离线RL任务中，提高指导权重可稳定提升性能，且能泛化简单监督方法（如目标条件行为克隆）。

Conclusion: CFGRL通过生成模型指导实现策略优化，为强化学习提供了一种简单高效的扩展方法。

Abstract: At the core of reinforcement learning is the idea of learning beyond the
performance in the data. However, scaling such systems has proven notoriously
tricky. In contrast, techniques from generative modeling have proven remarkably
scalable and are simple to train. In this work, we combine these strengths, by
deriving a direct relation between policy improvement and guidance of diffusion
models. The resulting framework, CFGRL, is trained with the simplicity of
supervised learning, yet can further improve on the policies in the data. On
offline RL tasks, we observe a reliable trend -- increased guidance weighting
leads to increased performance. Of particular importance, CFGRL can operate
without explicitly learning a value function, allowing us to generalize simple
supervised methods (e.g., goal-conditioned behavioral cloning) to further
prioritize optimality, gaining performance for "free" across the board.

</details>


### [224] [On Global Convergence Rates for Federated Policy Gradient under Heterogeneous Environment](https://arxiv.org/abs/2505.23459)
*Safwan Labbi,Paul Mangold,Daniil Tiapkin,Eric Moulines*

Key words: 联邦强化学习, 策略梯度, 收敛性, 异质性环境, b-RS-FedPG

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了联邦强化学习中策略梯度方法的收敛性问题，提出了一种新的策略梯度方法b-RS-FedPG，并在理论和实践中验证了其优于联邦Q学习的性能。

Motivation: 联邦强化学习（FRL）在环境异质性下的策略梯度收敛性是一个主要挑战，研究旨在解决这一问题并提升算法效率。

Method: 提出了基于局部更新的联邦策略梯度（FedPG）算法，并在Łojasiewicz条件下证明了其收敛性；进一步设计了b-RS-FedPG方法，结合softmax参数化和正则化方案。

Result: 理论分析显示FedPG具有与代理数量线性加速的特性；b-RS-FedPG实现了接近最优策略的显式收敛速度，在异质性环境下表现优于联邦Q学习。

Conclusion: 研究表明FedPG和b-RS-FedPG在理论和实践中均能有效处理环境异质性，为联邦强化学习提供了新的解决方案。

Abstract: Ensuring convergence of policy gradient methods in federated reinforcement
learning (FRL) under environment heterogeneity remains a major challenge. In
this work, we first establish that heterogeneity, perhaps counter-intuitively,
can necessitate optimal policies to be non-deterministic or even time-varying,
even in tabular environments. Subsequently, we prove global convergence results
for federated policy gradient (FedPG) algorithms employing local updates, under
a {\L}ojasiewicz condition that holds only for each individual agent, in both
entropy-regularized and non-regularized scenarios. Crucially, our theoretical
analysis shows that FedPG attains linear speed-up with respect to the number of
agents, a property central to efficient federated learning. Leveraging insights
from our theoretical findings, we introduce b-RS-FedPG, a novel policy gradient
method that employs a carefully constructed softmax-inspired parameterization
coupled with an appropriate regularization scheme. We further demonstrate
explicit convergence rates for b-RS-FedPG toward near-optimal stationary
policies. Finally, we demonstrate that empirically both FedPG and b-RS-FedPG
consistently outperform federated Q-learning on heterogeneous settings.

</details>


### [225] [Refining Labeling Functions with Limited Labeled Data](https://arxiv.org/abs/2505.23470)
*Chenjie Li,Amir Gilad,Boris Glavic,Zhengjie Miao,Sudeepa Roy*

Key words: 程序化弱监督, 标签函数修复, 条件规则, 标注数据

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种通过少量标注数据修复程序化弱监督（PWS）中标签函数（LFs）的技术，以提高标签质量。

Motivation: 程序化弱监督依赖标签函数生成标签，但其准确性直接影响标签质量。研究旨在通过少量标注数据修复LFs，以提升整体效果。

Method: 将LFs建模为条件规则，通过最小化调整其在标注数据上的输出，确保每条标注数据有足够证据支持，同时修复后的LFs精度达标。

Result: 实验表明，该方法能够基于极少量标注数据显著提升LFs的质量。

Conclusion: 通过条件规则建模和选择性调整，少量标注数据即可有效修复LFs，从而提高程序化弱监督的标签质量。

Abstract: Programmatic weak supervision (PWS) significantly reduces human effort for
labeling data by combining the outputs of user-provided labeling functions
(LFs) on unlabeled datapoints. However, the quality of the generated labels
depends directly on the accuracy of the LFs. In this work, we study the problem
of fixing LFs based on a small set of labeled examples. Towards this goal, we
develop novel techniques for repairing a set of LFs by minimally changing their
results on the labeled examples such that the fixed LFs ensure that (i) there
is sufficient evidence for the correct label of each labeled datapoint and (ii)
the accuracy of each repaired LF is sufficiently high. We model LFs as
conditional rules which enables us to refine them, i.e., to selectively change
their output for some inputs. We demonstrate experimentally that our system
improves the quality of LFs based on surprisingly small sets of labeled
datapoints.

</details>


### [226] [SGD as Free Energy Minimization: A Thermodynamic View on Neural Network Training](https://arxiv.org/abs/2505.23489)
*Ildus Sadrtdinov,Ivan Klimov,Ekaterina Lobacheva,Dmitry Vetrov*

Key words: 随机梯度下降, 热力学, 自由能, 神经网络, 学习率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一个热力学框架解释固定学习率下随机梯度下降（SGD）的稳态行为，揭示了学习率如何通过自由能最小化影响训练收敛性。

Motivation: 动机是理解SGD在神经网络训练中为何不同学习率会导致不同的稳态损失水平，从而提供一个新的理论视角。

Method: 方法是热力学类比，将SGD的稳态行为建模为自由能最小化，结合实验验证在欠参数化和过参数化模型中的表现。

Result: 结果显示欠参数化模型严格遵循自由能最小化，而过参数化模型在低学习率下退化为直接最小化损失。

Conclusion: 结论是学习率通过温度参数影响SGD的稳态行为，噪声梯度信号的差异是两类模型表现不同的原因。

Abstract: We present a thermodynamic interpretation of the stationary behavior of
stochastic gradient descent (SGD) under fixed learning rates (LRs) in neural
network training. We show that SGD implicitly minimizes a free energy function
$F=U-TS$, balancing training loss $U$ and the entropy of the weights
distribution $S$, with temperature $T$ determined by the LR. This perspective
offers a new lens on why high LRs prevent training from converging to the loss
minima and how different LRs lead to stabilization at different loss levels. We
empirically validate the free energy framework on both underparameterized (UP)
and overparameterized (OP) models. UP models consistently follow free energy
minimization, with temperature increasing monotonically with LR, while for OP
models, the temperature effectively drops to zero at low LRs, causing SGD to
minimize the loss directly and converge to an optimum. We attribute this
mismatch to differences in the signal-to-noise ratio of stochastic gradients
near optima, supported by both a toy example and neural network experiments.

</details>


### [227] [Epistemic Errors of Imperfect Multitask Learners When Distributions Shift](https://arxiv.org/abs/2505.23496)
*Sabina J. Sloman,Michele Caprio,Samuel Kaski*

Key words: 认知误差、分布偏移、多任务学习、负迁移、贝叶斯迁移学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一个原则性的认知误差定义，并提供了一个通用的、可分解的认知误差界限。该界限首次专门考虑认知误差，容纳所有认知不确定性来源，并将误差归因于学习过程和环境的多个方面。通过推论，作者还提供了针对贝叶斯迁移学习和分布偏移的专门界限，并通过合成实验验证了负迁移的新定义。

Motivation: 在现实学习场景中，由于训练数据与测试数据可能存在分布偏移、多任务来源或学习不完美等问题，统计学习器难以准确预测测试数据的分布。为了解决这些认知不确定性带来的挑战，论文提出了一个新的认知误差定义及界限。

Method: 作者引入了一个原则性的认知误差定义，并提出了一种通用的、可分解的认知误差界限。该方法通过量化和分析学习过程中的多个不确定性来源（如多任务学习、分布偏移、学习不完美等），将误差归因于不同方面。此外，还推导了针对贝叶斯迁移学习和分布偏移的专门界限。

Result: 论文提出的认知误差界限能够系统性地分析学习过程中的不确定性来源，并展示了在贝叶斯迁移学习和分布偏移场景中的有效性。通过合成实验，新定义的负迁移概念也得到验证。

Conclusion: 该研究为理解认知误差提供了新的理论框架，并通过分解和分析不确定性来源提升了学习模型的解释性。未来工作可进一步扩展该方法到更复杂的实际应用场景中。

Abstract: When data are noisy, a statistical learner's goal is to resolve epistemic
uncertainty about the data it will encounter at test-time, i.e., to identify
the distribution of test (target) data. Many real-world learning settings
introduce sources of epistemic uncertainty that can not be resolved on the
basis of training (source) data alone: The source data may arise from multiple
tasks (multitask learning), the target data may differ systematically from the
source data tasks (distribution shift), and/or the learner may not arrive at an
accurate characterization of the source data (imperfect learning). We introduce
a principled definition of epistemic error, and provide a generic,
decompositional epistemic error bound. Our error bound is the first to (i)
consider epistemic error specifically, (ii) accommodate all the sources of
epistemic uncertainty above, and (iii) separately attribute the error to each
of multiple aspects of the learning procedure and environment. As corollaries
of the generic result, we provide (i) epistemic error bounds specialized to the
settings of Bayesian transfer learning and distribution shift within
$\epsilon$-neighborhoods, and (ii) a set of corresponding generalization
bounds. Finally, we provide a novel definition of negative transfer, and
validate its insights in a synthetic experimental setting.

</details>


### [228] [Why Machine Learning Models Fail to Fully Capture Epistemic Uncertainty](https://arxiv.org/abs/2505.23506)
*Sebastián Jiménez,Mira Jürgens,Willem Waegeman*

Key words: 认知不确定性, 模型偏差, 二阶分布, 监督学习, 不确定性量化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 当前基于二阶分布的监督学习方法未能充分捕捉模型偏差对认知不确定性的影响，导致认知不确定性被低估。

Motivation: 现有方法忽视了模型偏差对认知不确定性的影响，研究通过更细致的不确定性分类和偏差-方差分解来揭示这一问题。

Method: 采用基于模拟的评估协议和理论分析，研究模型偏差对认知不确定性估计的影响。

Result: 高模型偏差会导致认知不确定性被低估，现有方法常将偏差诱导的错误归因为偶然不确定性。

Conclusion: 只有在充分表征所有认知不确定性来源的情况下，才能获得有意义的偶然不确定性估计。

Abstract: In recent years various supervised learning methods that disentangle
aleatoric and epistemic uncertainty based on second-order distributions have
been proposed. We argue that these methods fail to capture critical components
of epistemic uncertainty, particularly due to the often-neglected component of
model bias. To show this, we make use of a more fine-grained taxonomy of
epistemic uncertainty sources in machine learning models, and analyse how the
classical bias-variance decomposition of the expected prediction error can be
decomposed into different parts reflecting these uncertainties. By using a
simulation-based evaluation protocol which encompasses epistemic uncertainty
due to both procedural- and data-driven uncertainty components, we illustrate
that current methods rarely capture the full spectrum of epistemic uncertainty.
Through theoretical insights and synthetic experiments, we show that high model
bias can lead to misleadingly low estimates of epistemic uncertainty, and
common second-order uncertainty quantification methods systematically blur
bias-induced errors into aleatoric estimates, thereby underrepresenting
epistemic uncertainty. Our findings underscore that meaningful aleatoric
estimates are feasible only if all relevant sources of epistemic uncertainty
are properly represented.

</details>


### [229] [AnchorAttention: Difference-Aware Sparse Attention with Stripe Granularity](https://arxiv.org/abs/2505.23520)
*Yu Zhang,Dong Guo,Fang Wu,Guoliang Zhu,Dian Ding,Yiming Zhang*

Key words: 大语言模型, 稀疏注意力, 全局上下文, 计算效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出AnchorAttention，一种差异感知的动态稀疏注意力机制，通过更细的条带粒度和全局上下文适应，提升长文本处理的速度与准确性。

Motivation: 现有方法在长上下文预填充阶段计算复杂度高，且依赖局部信息和粗粒度分块，导致稀疏性和效率不佳。

Method: 结合模式锚计算、差异感知条带稀疏识别和细粒度稀疏计算，动态识别关键注意力区域。

Result: 在128k文本长度下，速度提升1.44倍，同时保持更高召回率。

Conclusion: AnchorAttention通过精细稀疏策略，显著提升计算效率与准确性。

Abstract: Large Language Models (LLMs) with extended context lengths face significant
computational challenges during the pre-filling phase, primarily due to the
quadratic complexity of self-attention. Existing methods typically employ
dynamic pattern matching and block-sparse low-level implementations. However,
their reliance on local information for pattern identification fails to capture
global contexts, and the coarse granularity of blocks leads to persistent
internal sparsity, resulting in suboptimal accuracy and efficiency. To address
these limitations, we propose \textbf{AnchorAttention}, a difference-aware,
dynamic sparse attention mechanism that efficiently identifies critical
attention regions at a finer stripe granularity while adapting to global
contextual information, achieving superior speed and accuracy. AnchorAttention
comprises three key components: (1) \textbf{Pattern-based Anchor Computation},
leveraging the commonalities present across all inputs to rapidly compute a set
of near-maximum scores as the anchor; (2) \textbf{Difference-aware Stripe
Sparsity Identification}, performing difference-aware comparisons with the
anchor to quickly obtain discrete coordinates of significant regions in a
stripe-like sparsity pattern; (3) \textbf{Fine-grained Sparse Computation},
replacing the traditional contiguous KV block loading approach with
simultaneous discrete KV position loading to maximize sparsity rates while
preserving full hardware computational potential. With its finer-grained
sparsity strategy, \textbf{AnchorAttention} achieves higher sparsity rates at
the same recall level, significantly reducing computation time. Compared to
previous state-of-the-art methods, at a text length of 128k, it achieves a
speedup of 1.44$\times$ while maintaining higher recall rates.

</details>


### [230] [Accelerating AllReduce with a Persistent Straggler](https://arxiv.org/abs/2505.23523)
*Arjun Devraj,Eric Ding,Abhishek Vijaya Kumar,Robert Kleinberg,Rachee Singh*

Key words: 分布式机器学习, AllReduce, 慢节点, 同步延迟, GPU集群

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种名为StragglAR的新AllReduce算法，用于解决在存在慢节点（straggler）情况下分布式机器学习中的同步延迟问题，相比现有算法速度提升显著。

Motivation: 分布式机器学习任务依赖AllReduce算法同步梯度或激活值，但慢节点会导致同步屏障延迟，拖累整体性能。

Method: StragglAR算法在慢节点延迟期间让其余GPU执行ReduceScatter操作，并在慢节点到达后通过新集体算法完成AllReduce。

Result: 理论分析显示在大规模GPU集群中，StragglAR比带宽高效的AllReduce算法（如Ring）快2倍；实际8-GPU服务器测试中提速22%。

Conclusion: StragglAR有效缓解慢节点问题，显著提升分布式训练和推理的效率，尤其适合大规模集群场景。

Abstract: Distributed machine learning workloads use data and tensor parallelism for
training and inference, both of which rely on the AllReduce collective to
synchronize gradients or activations. However, bulk-synchronous AllReduce
algorithms can be delayed by a persistent straggler that is slower to reach the
synchronization barrier required to begin the collective. To address this
challenge, we propose StragglAR: an AllReduce algorithm that accelerates
distributed training and inference in the presence of persistent stragglers.
StragglAR implements a ReduceScatter among the remaining GPUs during the
straggler-induced delay, and then executes a novel collective algorithm to
complete the AllReduce once the straggler reaches the synchronization barrier.
StragglAR achieves a 2x theoretical speedup over popular bandwidth-efficient
AllReduce algorithms (e.g., Ring) for large GPU clusters with persistent
stragglers. On an 8-GPU server, our implementation of StragglAR yields a 22%
speedup over state-of-the-art AllReduce algorithms.

</details>


### [231] [Normalizing Flows are Capable Models for RL](https://arxiv.org/abs/2505.23527)
*Raj Ghugare,Benjamin Eysenbach*

Key words: 强化学习, 归一化流, 模仿学习, 离线RL, 目标条件RL, 无监督RL

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文通过提出一种新的归一化流（NF）架构，解决了现代强化学习（RL）中复杂模型的效率问题，展示了NF在RL中的潜力，并在多种任务中取得了优异性能。

Motivation: 尽管扩散模型和自回归变换器在RL中表现良好，但它们的计算成本高或依赖离散表示。归一化流（NF）提供了高效替代方案，但其在RL中的应用因被认为表达能力不足而受限。本文旨在证明NF的潜力。

Method: 提出了一种新的NF架构，能无缝集成到RL算法中，充当策略、Q函数和占用度量，简化了算法设计。

Result: 该架构在模仿学习、离线RL、目标条件RL和无监督RL中取得了更高性能。

Conclusion: NF在RL中具有未被充分挖掘的潜力，提出的方法简化了算法并提升了性能。

Abstract: Modern reinforcement learning (RL) algorithms have found success by using
powerful probabilistic models, such as transformers, energy-based models, and
diffusion/flow-based models. To this end, RL researchers often choose to pay
the price of accommodating these models into their algorithms -- diffusion
models are expressive, but are computationally intensive due to their reliance
on solving differential equations, while autoregressive transformer models are
scalable but typically require learning discrete representations. Normalizing
flows (NFs), by contrast, seem to provide an appealing alternative, as they
enable likelihoods and sampling without solving differential equations or
autoregressive architectures. However, their potential in RL has received
limited attention, partly due to the prevailing belief that normalizing flows
lack sufficient expressivity. We show that this is not the case. Building on
recent work in NFs, we propose a single NF architecture which integrates
seamlessly into RL algorithms, serving as a policy, Q-function, and occupancy
measure. Our approach leads to much simpler algorithms, and achieves higher
performance in imitation learning, offline, goal conditioned RL and
unsupervised RL.

</details>


### [232] [Comparative assessment of fairness definitions and bias mitigation strategies in machine learning-based diagnosis of Alzheimer's disease from MR images](https://arxiv.org/abs/2505.23528)
*Maria Eleftheria Vlontzou,Maria Athanasiou,Christos Davatzikos,Konstantina S. Nikita*

Key words: 机器学习, 公平性分析, 阿尔茨海默病, 轻度认知障碍, MRI

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究通过MRI特征对轻度认知障碍（MCI）和阿尔茨海默病（AD）诊断模型进行公平性分析，评估年龄、种族和性别偏见及缓解策略，提出新的综合评价指标，结果显示年龄和种族偏见显著，缓解策略效果各异。

Motivation: 研究旨在揭示机器学习模型在医学诊断中的偏见问题，尤其是针对MCI和AD诊断时因年龄、种族和性别导致的公平性差异，并为医疗应用提供公平性改进方法。

Method: 使用多队列数据集分析了偏见，评估了各种公平性定义和指标，比较了预处理、处理中和后处理的偏见缓解策略，并提出了新的综合评价指标以权衡公平性和性能。

Result: 研究发现年龄和种族偏见显著，而性别偏见不明显。不同缓解策略因敏感属性和子问题而异，种族和性别偏见的

Conclusion: 研究表明公平性评估和缓解策略在医疗AI中的重要性，新提出的综合评价指标为诊断模型的公平性优化提供了实用方法。

Abstract: The present study performs a comprehensive fairness analysis of machine
learning (ML) models for the diagnosis of Mild Cognitive Impairment (MCI) and
Alzheimer's disease (AD) from MRI-derived neuroimaging features. Biases
associated with age, race, and gender in a multi-cohort dataset, as well as the
influence of proxy features encoding these sensitive attributes, are
investigated. The reliability of various fairness definitions and metrics in
the identification of such biases is also assessed. Based on the most
appropriate fairness measures, a comparative analysis of widely used
pre-processing, in-processing, and post-processing bias mitigation strategies
is performed. Moreover, a novel composite measure is introduced to quantify the
trade-off between fairness and performance by considering the F1-score and the
equalized odds ratio, making it appropriate for medical diagnostic
applications. The obtained results reveal the existence of biases related to
age and race, while no significant gender bias is observed. The deployed
mitigation strategies yield varying improvements in terms of fairness across
the different sensitive attributes and studied subproblems. For race and
gender, Reject Option Classification improves equalized odds by 46% and 57%,
respectively, and achieves harmonic mean scores of 0.75 and 0.80 in the MCI
versus AD subproblem, whereas for age, in the same subproblem, adversarial
debiasing yields the highest equalized odds improvement of 40% with a harmonic
mean score of 0.69. Insights are provided into how variations in AD
neuropathology and risk factors, associated with demographic characteristics,
influence model fairness.

</details>


### [233] [Subgraph Gaussian Embedding Contrast for Self-Supervised Graph Representation Learning](https://arxiv.org/abs/2505.23529)
*Shifeng Xie,Aref Einizade,Jhony H. Giraldo*

Key words: 图表示学习,自监督学习,子图高斯嵌入,最优运输距离

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为SubGEC的自监督学习方法，通过子图高斯嵌入和最优运输距离提升图表示学习的效果。

Motivation: 图表示学习需要高成本的人工标注，自监督学习方法能避免这一问题。

Method: 使用子图高斯嵌入模块将子图映射到高斯空间，并通过Wasserstein和Gromov-Wasserstein距离衡量子图相似性。

Result: 在多个基准测试中表现优异或具有竞争力。

Conclusion: 研究为自监督学习方法的子图分布设计提供了新见解。

Abstract: Graph Representation Learning (GRL) is a fundamental task in machine
learning, aiming to encode high-dimensional graph-structured data into
low-dimensional vectors. Self-Supervised Learning (SSL) methods are widely used
in GRL because they can avoid expensive human annotation. In this work, we
propose a novel Subgraph Gaussian Embedding Contrast (SubGEC) method. Our
approach introduces a subgraph Gaussian embedding module, which adaptively maps
subgraphs to a structured Gaussian space, ensuring the preservation of input
subgraph characteristics while generating subgraphs with a controlled
distribution. We then employ optimal transport distances, more precisely the
Wasserstein and Gromov-Wasserstein distances, to effectively measure the
similarity between subgraphs, enhancing the robustness of the contrastive
learning process. Extensive experiments across multiple benchmarks demonstrate
that \method~outperforms or presents competitive performance against
state-of-the-art approaches. Our findings provide insights into the design of
SSL methods for GRL, emphasizing the importance of the distribution of the
generated contrastive pairs.

</details>


### [234] [Domain-Aware Tensor Network Structure Search](https://arxiv.org/abs/2505.23537)
*Giorgos Iacovides,Wuyang Zhou,Chao Li,Qibin Zhao,Danilo Mandic*

Key words: 张量网络, 结构搜索, 大型语言模型, 领域信息, 高效计算

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了tnLLM框架，利用大型语言模型（LLM）的推理能力结合领域信息，直接预测合适的张量网络结构，优化了计算效率并增加了解释性。

Motivation: 当前张量网络结构搜索（TN-SS）算法计算成本高且忽略了数据的领域信息，缺乏结构透明性。需要一种更高效、透明的解决方案。

Method: 提出tnLLM框架，通过领域感知提示管道指导LLM基于张量模式间的实际关系推断合适的TN结构，实现目标函数优化并提供领域感知解释。

Result: tnLLM以更少的目标函数评估达到了与SOTA算法可比的结果，并能通过LLM的领域信息加速其他方法的收敛。

Conclusion: tnLLM显著提升了TN-SS的计算效率和透明度，同时保持了理论性能保证。

Abstract: Tensor networks (TNs) provide efficient representations of high-dimensional
data, yet identification of the optimal TN structures, the so called tensor
network structure search (TN-SS) problem, remains a challenge. Current
state-of-the-art (SOTA) algorithms are computationally expensive as they
require extensive function evaluations, which is prohibitive for real-world
applications. In addition, existing methods ignore valuable domain information
inherent in real-world tensor data and lack transparency in their identified TN
structures. To this end, we propose a novel TN-SS framework, termed the tnLLM,
which incorporates domain information about the data and harnesses the
reasoning capabilities of large language models (LLMs) to directly predict
suitable TN structures. The proposed framework involves a domain-aware
prompting pipeline which instructs the LLM to infer suitable TN structures
based on the real-world relationships between tensor modes. In this way, our
approach is capable of not only iteratively optimizing the objective function,
but also generating domain-aware explanations for the identified structures.
Experimental results demonstrate that tnLLM achieves comparable TN-SS objective
function values with much fewer function evaluations compared to SOTA
algorithms. Furthermore, we demonstrate that the LLM-enabled domain information
can be used to find good initializations in the search space for sampling-based
SOTA methods to accelerate their convergence while preserving theoretical
performance guarantees.

</details>


### [235] [Comparing the Moore-Penrose Pseudoinverse and Gradient Descent for Solving Linear Regression Problems: A Performance Analysis](https://arxiv.org/abs/2505.23552)
*Alex Adams*

Key words: 线性回归,伪逆,梯度下降,计算复杂度,预测准确性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 比较线性回归中闭式伪逆和梯度下降的性能，分析计算复杂度和实际表现，提供选择建议。

Motivation: 线性回归是预测建模的基础，不同求解方法对效率和准确性有显著影响，需明确如何选择最优方法。

Method: 理论分析两种方法的计算复杂度，并通过合成数据和真实数据集进行实验评估。

Result: 明确了两种方法分别在计算时间、数值稳定性和预测准确性上的优势条件。

Conclusion: 为研究人员和实践者选择线性回归求解方法提供实用指导。

Abstract: This paper investigates the comparative performance of two fundamental
approaches to solving linear regression problems: the closed-form Moore-Penrose
pseudoinverse and the iterative gradient descent method. Linear regression is a
cornerstone of predictive modeling, and the choice of solver can significantly
impact efficiency and accuracy. I review and discuss the theoretical
underpinnings of both methods, analyze their computational complexity, and
evaluate their empirical behavior on synthetic datasets with controlled
characteristics, as well as on established real-world datasets. My results
delineate the conditions under which each method excels in terms of
computational time, numerical stability, and predictive accuracy. This work
aims to provide practical guidance for researchers and practitioners in machine
learning when selecting between direct, exact solutions and iterative,
approximate solutions for linear regression tasks.

</details>


### [236] [Adaptive Federated LoRA in Heterogeneous Wireless Networks with Independent Sampling](https://arxiv.org/abs/2505.23555)
*Yanzhao Hou,Jiaxiang Geng,Boyu Li,Xiaofeng Tao,Juncheng Wang,Xiaodong Xu,Bing Luo*

Key words: 联邦学习, LoRA, 大语言模型, 异质性, 优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种自适应的联邦LoRA策略，通过独立客户端采样和带宽分配优化，显著减少了联邦微调的收敛时间。

Motivation: 现有方法忽视了系统和数据异质性对训练效率的影响，特别是在实际训练时间上的优化不足。

Method: 提出自适应联邦LoRA策略，包括独立客户端采样、理论收敛边界推导、带宽分配方案和非凸优化问题求解。

Result: 实验表明，该方法在多种模型和数据集上显著减少了训练时间，优于现有技术。

Conclusion: 自适应联邦LoRA策略通过优化客户采样和资源分配，高效减少联邦微调的收敛时间。

Abstract: Federated LoRA has emerged as a promising technique for efficiently
fine-tuning large language models (LLMs) on distributed devices by reducing the
number of trainable parameters. However, existing approaches often inadequately
overlook the theoretical and practical implications of system and data
heterogeneity, thereby failing to optimize the overall training efficiency,
particularly in terms of wall-clock time. In this paper, we propose an adaptive
federated LoRA strategy with independent client sampling to minimize the
convergence wall-clock time of federated fine-tuning under both computation and
communication heterogeneity. We first derive a new convergence bound for
federated LoRA with arbitrary and independent client sampling, notably without
requiring the stringent bounded gradient assumption. Then, we introduce an
adaptive bandwidth allocation scheme that accounts for heterogeneous client
resources and system bandwidth constraints. Based on the derived theory, we
formulate and solve a non-convex optimization problem to jointly determine the
LoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock
convergence time. An efficient and low-complexity algorithm is developed to
approximate the solution. Finally, extensive experiments demonstrate that our
approach significantly reduces wall-clock training time compared to
state-of-the-art methods across various models and datasets.

</details>


### [237] [Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models](https://arxiv.org/abs/2505.23564)
*Yiran Guo,Lijie Xu,Jie Liu,Dan Ye,Shuang Qiu*

Key words: 强化学习, 优势估计, 分段策略优化, 大语言模型, 推理能力

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了Segment Policy Optimization (SPO)框架，通过中粒度分段优势估计改进大语言模型的推理能力，平衡了令牌级和轨迹级方法的优点，在GSM8K和MATH500任务中取得了显著提升。

Motivation: 现有强化学习方法在令牌级和轨迹级优势估计中存在精度不足或分配不精确的问题，需要一种中粒度方法以优化模型推理能力。

Method: SPO框架包含三个部分：(1)灵活分段划分；(2)精准分段优势估计；(3)基于分段优势的策略优化，采用了概率掩码策略，并针对短链和长链思维场景设计了SPO-chain和SPO-tree具体实现。

Result: 在GSM8K上SPO-chain比PPO和GRPO准确率提升6-12个百分点；在MATH500上SPO-tree比GRPO在2K和4K上下文评估中提升7-11个百分点。

Conclusion: SPO通过中粒度优势估计解决了现有方法的局限性，显著提升了大语言模型的推理能力。

Abstract: Enhancing the reasoning capabilities of large language models effectively
using reinforcement learning (RL) remains a crucial challenge. Existing
approaches primarily adopt two contrasting advantage estimation granularities:
Token-level methods (e.g., PPO) aim to provide the fine-grained advantage
signals but suffer from inaccurate estimation due to difficulties in training
an accurate critic model. On the other extreme, trajectory-level methods (e.g.,
GRPO) solely rely on a coarse-grained advantage signal from the final reward,
leading to imprecise credit assignment. To address these limitations, we
propose Segment Policy Optimization (SPO), a novel RL framework that leverages
segment-level advantage estimation at an intermediate granularity, achieving a
better balance by offering more precise credit assignment than trajectory-level
methods and requiring fewer estimation points than token-level methods,
enabling accurate advantage estimation based on Monte Carlo (MC) without a
critic model. SPO features three components with novel strategies: (1) flexible
segment partition; (2) accurate segment advantage estimation; and (3) policy
optimization using segment advantages, including a novel probability-mask
strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain
for short chain-of-thought (CoT), featuring novel cutpoint-based partition and
chain-based advantage estimation, achieving $6$-$12$ percentage point
improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT,
featuring novel tree-based advantage estimation, which significantly reduces
the cost of MC estimation, achieving $7$-$11$ percentage point improvements
over GRPO on MATH500 under 2K and 4K context evaluation. We make our code
publicly available at https://github.com/AIFrameResearch/SPO.

</details>


### [238] [DRO: A Python Library for Distributionally Robust Optimization in Machine Learning](https://arxiv.org/abs/2505.23565)
*Jiashuo Liu,Tianyu Wang,Henry Lam,Hongseok Namkoong,Jose Blanchet*

Key words: 分布鲁棒优化, Python库, 回归, 分类, scikit-learn, PyTorch

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文介绍了一个名为dro的开源Python库，专注于分布鲁棒优化（DRO），适用于回归和分类问题。

Motivation: 为了解决分布鲁棒优化在实际应用中的复杂性和性能问题，作者开发了一个高效且兼容性强的工具库。

Method: dro库实现了14种DRO模型和9种基础模型，支持scikit-learn和PyTorch，并采用向量化和近似优化技术加速运行。

Result: 相比基线实现，dro库在大型数据集上的运行时间减少了10倍至1000倍以上。

Conclusion: dro库为分布鲁棒优化提供了高效、灵活且易于使用的解决方案，适合大规模数据处理。

Abstract: We introduce dro, an open-source Python library for distributionally robust
optimization (DRO) for regression and classification problems. The library
implements 14 DRO formulations and 9 backbone models, enabling 79 distinct DRO
methods. Furthermore, dro is compatible with both scikit-learn and PyTorch.
Through vectorization and optimization approximation techniques, dro reduces
runtime by 10x to over 1000x compared to baseline implementations on
large-scale datasets. Comprehensive documentation is available at
https://python-dro.org.

</details>


### [239] [Maximum Likelihood Learning of Latent Dynamics Without Reconstruction](https://arxiv.org/abs/2505.23569)
*Samo Hromadka,Kai Biegun,Lior Fox,James Heald,Maneesh Sahani*

Key words: RP-GSSM,无监督学习,时间序列,高斯状态空间模型,概率生成模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: RP-GSSM是一种新颖的无监督学习方法，用于具有潜在动态结构的时间序列数据，结合了对比方法的直观性和概率生成模型的灵活性。

Motivation: 为了解决传统对比方法和生成方法在学习时间序列数据潜在动态结构时的局限性，RP-GSSM旨在成为一个既具备概率模型有效性又专注于潜在变量推断的框架。

Method: RP-GSSM通过最大似然学习，利用联合高斯潜在先验实现精确推断，并通过非线性神经网络链接保持表达能力，无需额外的正则化或辅助损失。

Result: 实验表明，RP-GSSM在包括从视频中学习非线性随机动态的任务上优于其他方法，无论是否存在背景干扰。

Conclusion: RP-GSSM作为一种基础模型，适用于多种下游应用。

Abstract: We introduce a novel unsupervised learning method for time series data with
latent dynamical structure: the recognition-parametrized Gaussian state space
model (RP-GSSM). The RP-GSSM is a probabilistic model that learns Markovian
Gaussian latents explaining statistical dependence between observations at
different time steps, combining the intuition of contrastive methods with the
flexible tools of probabilistic generative models. Unlike contrastive
approaches, the RP-GSSM is a valid probabilistic model learned via maximum
likelihood. Unlike generative approaches, the RP-GSSM has no need for an
explicit network mapping from latents to observations, allowing it to focus
model capacity on inference of latents. The model is both tractable and
expressive: it admits exact inference thanks to its jointly Gaussian latent
prior, while maintaining expressivity with an arbitrarily nonlinear neural
network link between observations and latents. These qualities allow the
RP-GSSM to learn task-relevant latents without ad-hoc regularization, auxiliary
losses, or optimizer scheduling. We show how this approach outperforms
alternatives on problems that include learning nonlinear stochastic dynamics
from video, with or without background distractors. Our results position the
RP-GSSM as a useful foundation model for a variety of downstream applications.

</details>


### [240] [BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model](https://arxiv.org/abs/2505.23579)
*Adibvafa Fallahpour,Andrew Magnuson,Purav Gupta,Shihao Ma,Jack Naimer,Arnav Shah,Haonan Duan,Omar Ibrahim,Hani Goodarzi,Chris J. Maddison,Bo Wang*

Key words: BioReason, 多模态推理, 基因组学, LLM, 可解释性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: BioReason是一种新型架构，首次将DNA基础模型与大型语言模型（LLM）深度结合，支持多步推理并提供可解释的生物直觉解释，在多个生物推理基准上性能提升15%，并可生成可解释的生物追踪。

Motivation: 当前DNA基础模型虽能表征序列，但缺乏多步推理和透明解释能力，阻碍科学发现。BioReason旨在突破这一局限。

Method: 通过监督微调和强化学习，将DNA基础模型与LLM结合，实现基因组信息的直接处理与多步推理。

Result: 在KEGG疾病通路预测等任务中，准确率从88%提升至97%，平均性能超越单模态基线15%。

Conclusion: BioReason为生物学AI提供了新范式，支持更深入的机制理解和假设生成，代码和数据已开源。

Abstract: Unlocking deep, interpretable biological reasoning from complex genomic data
is a major AI challenge hindering scientific discovery. Current DNA foundation
models, despite strong sequence representation, struggle with multi-step
reasoning and lack inherent transparent, biologically intuitive explanations.
We introduce BioReason, a pioneering architecture that, for the first time,
deeply integrates a DNA foundation model with a Large Language Model (LLM).
This novel connection enables the LLM to directly process and reason with
genomic information as a fundamental input, fostering a new form of multimodal
biological understanding. BioReason's sophisticated multi-step reasoning is
developed through supervised fine-tuning and targeted reinforcement learning,
guiding the system to generate logical, biologically coherent deductions. On
biological reasoning benchmarks including KEGG-based disease pathway prediction
- where accuracy improves from 88% to 97% - and variant effect prediction,
BioReason demonstrates an average 15% performance gain over strong
single-modality baselines. BioReason reasons over unseen biological entities
and articulates decision-making through interpretable, step-by-step biological
traces, offering a transformative approach for AI in biology that enables
deeper mechanistic insights and accelerates testable hypothesis generation from
genomic data. Data, code, and checkpoints are publicly available at
https://github.com/bowang-lab/BioReason

</details>


### [241] [Improving Time Series Forecasting via Instance-aware Post-hoc Revision](https://arxiv.org/abs/2505.23583)
*Zhiding Liu,Mingyue Cheng,Guanhao Zhao,Jiqian Yang,Qi Liu,Enhong Chen*

Key words: 时间序列预测, 实例级偏差, 后处理修订, PIR框架

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: PIR是一个模型无关的框架，通过后预测识别和修订提升时间序列预测性能，有效解决实例级误差问题。

Motivation: 实例级变化（如分布偏移、缺失数据和长尾模式）导致某些实例的预测效果不佳，即使整体性能良好。

Method: PIR先通过准确性估计识别偏差预测实例，再利用上下文信息（协变量和历史时间序列）从局部和全局角度进行后处理修订。

Result: 在真实数据集上的实验表明，PIR显著减少实例级误差并提升预测可靠性。

Conclusion: PIR为时间序列预测中的实例级挑战提供了一种有效的解决方案。

Abstract: Time series forecasting plays a vital role in various real-world applications
and has attracted significant attention in recent decades. While recent methods
have achieved remarkable accuracy by incorporating advanced inductive biases
and training strategies, we observe that instance-level variations remain a
significant challenge. These variations--stemming from distribution shifts,
missing data, and long-tail patterns--often lead to suboptimal forecasts for
specific instances, even when overall performance appears strong. To address
this issue, we propose a model-agnostic framework, PIR, designed to enhance
forecasting performance through Post-forecasting Identification and Revision.
Specifically, PIR first identifies biased forecasting instances by estimating
their accuracy. Based on this, the framework revises the forecasts using
contextual information, including covariates and historical time series, from
both local and global perspectives in a post-processing fashion. Extensive
experiments on real-world datasets with mainstream forecasting models
demonstrate that PIR effectively mitigates instance-level errors and
significantly improves forecasting reliability.

</details>


### [242] [On-Policy RL with Optimal Reward Baseline](https://arxiv.org/abs/2505.23585)
*Yaru Hao,Li Dong,Xun Wu,Shaohan Huang,Zewen Chi,Furu Wei*

Key words: 强化学习, 大语言模型, 在策略训练, 最优奖励基线, 数学推理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为OPO的新型强化学习算法，旨在解决训练不稳定和计算效率低的问题，并在数学推理基准测试中表现出优越性能。

Motivation: 现有强化学习算法因松散的在策略约束和不稳定的训练而受限，且由于需要辅助模型导致计算效率低下。

Method: 提出OPO算法，强调精确的在策略训练，并引入最优奖励基线以最小化梯度方差。

Result: OPO在数学推理任务中表现优异，训练稳定且无需额外模型或正则项，同时生成更多样化的输出。

Conclusion: OPO是强化学习在大语言模型对齐和推理任务中稳定高效的一种有前途的方法。

Abstract: Reinforcement learning algorithms are fundamental to align large language
models with human preferences and to enhance their reasoning capabilities.
However, current reinforcement learning algorithms often suffer from training
instability due to loose on-policy constraints and computational inefficiency
due to auxiliary models. In this work, we propose On-Policy RL with Optimal
reward baseline (OPO), a novel and simplified reinforcement learning algorithm
designed to address these challenges. OPO emphasizes the importance of exact
on-policy training, which empirically stabilizes the training process and
enhances exploration. Moreover, OPO introduces the optimal reward baseline that
theoretically minimizes gradient variance. We evaluate OPO on mathematical
reasoning benchmarks. The results demonstrate its superior performance and
training stability without additional models or regularization terms.
Furthermore, OPO achieves lower policy shifts and higher output entropy,
encouraging more diverse and less repetitive responses. These results highlight
OPO as a promising direction for stable and effective reinforcement learning in
large language model alignment and reasoning tasks. The implementation is
provided at https://github.com/microsoft/LMOps/tree/main/opo.

</details>


### [243] [Accelerated Training of Federated Learning via Second-Order Methods](https://arxiv.org/abs/2505.23588)
*Mrinmay Sen,Sidhant R Nair,C Krishna Mohan*

Key words: 联邦学习, 二阶优化, 收敛速度, 通信成本, Hessian矩阵

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文探讨了联邦学习（FL）中二阶优化方法的应用，以解决收敛速度慢和通信轮次过多的问题。现有研究多关注统计和设备标签异质性以及隐私安全挑战，而对训练速度问题关注不足。作者分析了多种利用二阶优化加速训练的FL方法，并对其性能进行了分类比较，结果显示二阶优化在FL中具有潜力。

Motivation: 在联邦学习中，传统的一阶优化方法存在收敛速度慢和通信成本高的问题，特别是在数据高度异质的情况下。本文旨在研究如何通过二阶优化方法加速训练过程，减少通信负担。

Method: 作者对现有的二阶FL方法进行了全面分类，评估了它们在收敛速度、计算成本、内存使用、传输开销和全局模型泛化能力等方面的表现。

Result: 研究发现，二阶优化在FL中具有显著潜力，但如何高效利用Hessian矩阵及其逆矩阵仍是一个关键挑战。

Conclusion: 本文为未来开发高效、可扩展的联邦优化方法奠定了基础，以提升FL中全局模型的训练效率。

Abstract: This paper explores second-order optimization methods in Federated Learning
(FL), addressing the critical challenges of slow convergence and the excessive
communication rounds required to achieve optimal performance from the global
model. While existing surveys in FL primarily focus on challenges related to
statistical and device label heterogeneity, as well as privacy and security
concerns in first-order FL methods, less attention has been given to the issue
of slow model training. This slow training often leads to the need for
excessive communication rounds or increased communication costs, particularly
when data across clients are highly heterogeneous. In this paper, we examine
various FL methods that leverage second-order optimization to accelerate the
training process. We provide a comprehensive categorization of state-of-the-art
second-order FL methods and compare their performance based on convergence
speed, computational cost, memory usage, transmission overhead, and
generalization of the global model. Our findings show the potential of
incorporating Hessian curvature through second-order optimization into FL and
highlight key challenges, such as the efficient utilization of Hessian and its
inverse in FL. This work lays the groundwork for future research aimed at
developing scalable and efficient federated optimization methods for improving
the training of the global model in FL.

</details>


### [244] [Position: Federated Foundation Language Model Post-Training Should Focus on Open-Source Models](https://arxiv.org/abs/2505.23593)
*Nikita Agrawal,Simon Mertel,Ruben Mayer*

Key words: 联邦学习, 黑盒模型, 后训练, 数据隐私, 自主性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了在联邦学习中使用黑盒基础语言模型与联邦学习核心原则（如数据隐私和自主性）之间的矛盾，并分析了开放性的多个方面对联邦学习的影响。

Motivation: 在联邦学习中，黑盒基础语言模型的集中式后训练方法虽然在某些方面取得了成功，但与联邦学习的核心原则（如数据隐私和自主性）相矛盾。本文旨在批判性分析黑盒模型在联邦后训练中的应用。

Method: 通过批判性分析黑盒模型在联邦学习中的使用，并探讨开放性的多个方面及其对联邦学习的影响。

Result: 发现黑盒模型在联邦学习中的盲从使用违背了联邦学习的基本原则。

Conclusion: 在联邦学习中盲目使用黑盒基础语言模型是不合适的，应与联邦学习的核心原则保持一致。

Abstract: Post-training of foundation language models has emerged as a promising
research domain in federated learning (FL) with the goal to enable
privacy-preserving model improvements and adaptations to user's downstream
tasks. Recent advances in this area adopt centralized post-training approaches
that build upon black-box foundation language models where there is no access
to model weights and architecture details. Although the use of black-box models
has been successful in centralized post-training, their blind replication in FL
raises several concerns. Our position is that using black-box models in FL
contradicts the core principles of federation such as data privacy and
autonomy. In this position paper, we critically analyze the usage of black-box
models in federated post-training, and provide a detailed account of various
aspects of openness and their implications for FL.

</details>


### [245] [LLM Performance for Code Generation on Noisy Tasks](https://arxiv.org/abs/2505.23598)
*Radzim Sendyka,Christian Cabrera,Andrei Paleyes,Diana Robinson,Neil Lawrence*

Key words: 大型语言模型, 任务模糊化, 记忆依赖, 基准测试, 数据污染

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了大型语言模型（LLMs）在任务被模糊化后的识别与解决能力，强调其在竞争编程和基准任务中的表现，并指出模型可能依赖记忆而非真正的推理能力。

Motivation: 研究动机在于验证LLMs在面对完全模糊化的任务时，是否仍能解决问题，以揭示模型是否依赖训练数据的记忆而非逻辑推理。

Method: 方法包括对不同LLMs进行测试，使用LeetCode和MATH基准任务，并应用噪声和删除等模糊化手段，观察模型表现。

Result: 结果显示所有LLMs能解决对人类无意义的模糊化任务，但未在知识截止日期后的任务中表现出类似能力，暗示其表现依赖于训练数据的记忆。

Conclusion: 结论强调在基准测试设计时需谨慎，提出通过性能衰减检测数据污染，并指出自动化系统的安全风险和可解释性问题。

Abstract: This paper investigates the ability of large language models (LLMs) to
recognise and solve tasks which have been obfuscated beyond recognition.
Focusing on competitive programming and benchmark tasks (LeetCode and MATH), we
compare performance across multiple models and obfuscation methods, such as
noise and redaction. We demonstrate that all evaluated LLMs can solve tasks
obfuscated to a level where the text would be unintelligible to human readers,
and does not contain key pieces of instruction or context. We introduce the
concept of eager pattern matching to describe this behaviour, which is not
observed in tasks published after the models' knowledge cutoff date, indicating
strong memorisation or overfitting to training data, rather than legitimate
reasoning about the presented problem. We report empirical evidence of distinct
performance decay patterns between contaminated and unseen datasets. We discuss
the implications for benchmarking and evaluations of model behaviour, arguing
for caution when designing experiments using standard datasets. We also propose
measuring the decay of performance under obfuscation as a possible strategy for
detecting dataset contamination and highlighting potential safety risks and
interpretability issues for automated software systems.

</details>


### [246] [On Transferring Transferability: Towards a Theory for Size Generalization](https://arxiv.org/abs/2505.23599)
*Eitan Levin,Yuxin Ma,Mateo Díaz,Soledad Villar*

Key words: 可转移性, 图神经网络, 维度独立性, 极限空间, 学习任务

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一个通用的维度间可转移性框架，将问题实例大小与学习任务相结合，证明了可转移性在极限空间中的连续性，并通过数值实验验证了结果。

Motivation: 现代学习任务需要处理不同尺寸的输入，因此需要探索模型在低维数据训练后能否迁移到高维数据的能力。

Method: 引入一个通用的维度可转移性框架，通过识别小问题实例与等效大问题实例的极限空间来定义可转移性，并在现有架构上实现必要的修改。

Result: 证明了可转移性与极限空间中的连续性相关，并提供了设计可转移模型的原则，数值实验支持了理论结论。

Conclusion: 通过框架和实验验证了模型在不同维度间的可转移性，为未来设计可迁移模型提供了指导。

Abstract: Many modern learning tasks require models that can take inputs of varying
sizes. Consequently, dimension-independent architectures have been proposed for
domains where the inputs are graphs, sets, and point clouds. Recent work on
graph neural networks has explored whether a model trained on low-dimensional
data can transfer its performance to higher-dimensional inputs. We extend this
body of work by introducing a general framework for transferability across
dimensions. We show that transferability corresponds precisely to continuity in
a limit space formed by identifying small problem instances with equivalent
large ones. This identification is driven by the data and the learning task. We
instantiate our framework on existing architectures, and implement the
necessary changes to ensure their transferability. Finally, we provide design
principles for designing new transferable models. Numerical experiments support
our findings.

</details>


### [247] [Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model](https://arxiv.org/abs/2505.23606)
*Qingyu Shi,Jinbin Bai,Zhuoran Zhao,Wenhao Chai,Kaidong Yu,Jianzong Wu,Shuangyong Song,Yunhai Tong,Xiangtai Li,Xuelong Li,Shuicheng Yan*

Key words: 统一生成模型, 离散扩散, 多模态生成, 预训练主干, 轻量级解码器

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Muddit 是一种统一的离散扩散变换器，通过结合预训练的视觉主干和轻量级文本解码器，实现了跨文本和图像模态的快速并行生成，质量和效率均优于传统自回归模型。

Motivation: 当前统一生成模型在跨模态任务中存在自回归模型推理慢和非自回归模型泛化能力弱的问题。Muddit 旨在通过离散扩散技术结合预训练主干，解决这些限制。

Method: 提出 Muddit，一种统一离散扩散变换器，结合预训练文本到图像主干和轻量级文本解码器，支持跨模态并行生成。

Result: 实验表明，Muddit 在质量和效率上优于更大规模的自回归模型，展示了离散扩散作为统一生成主干的潜力。

Conclusion: 离散扩散结合预训练视觉主干可作为一种可扩展且高效的多模态统一生成方法。

Abstract: Unified generation models aim to handle diverse tasks across modalities --
such as text generation, image generation, and vision-language reasoning --
within a single architecture and decoding paradigm. Autoregressive unified
models suffer from slow inference due to sequential decoding, and
non-autoregressive unified models suffer from weak generalization due to
limited pretrained backbones. We introduce Muddit, a unified discrete diffusion
transformer that enables fast and parallel generation across both text and
image modalities. Unlike prior unified diffusion models trained from scratch,
Muddit integrates strong visual priors from a pretrained text-to-image backbone
with a lightweight text decoder, enabling flexible and high-quality multimodal
generation under a unified architecture. Empirical results show that Muddit
achieves competitive or superior performance compared to significantly larger
autoregressive models in both quality and efficiency. The work highlights the
potential of purely discrete diffusion, when equipped with strong visual
priors, as a scalable and effective backbone for unified generation.

</details>


### [248] [Data Model Design for Explainable Machine Learning-based Electricity Applications](https://arxiv.org/abs/2505.23607)
*Carolina Fortuna,Gregor Cerar,Blaz Bertalanic,Andrej Campa,Mihael Mohorcic*

Key words: 智能电网, 机器学习, 多元数据, 电力预测, 特征重要性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种针对能源应用中多元数据的分类法，并通过家庭电力预测应用验证了其有效性。

Motivation: 智能电网转型和可再生能源的广泛应用催生了机器学习模型的需求，但现有研究多基于单变量数据，缺乏对多元数据（如元数据和附加测量）的系统研究。

Method: 提出一种数据分类法，指导机器学习模型的数据建模，并在家庭电力预测中验证其有效性，分析领域、上下文和行为特征对预测精度的影响。

Result: 分类法能有效指导特征选择，通过特征重要性分析解释了各特征对预测精度的贡献。

Conclusion: 多元数据分类法为能源应用中的机器学习模型开发提供了结构化指导，提升了预测精度。

Abstract: The transition from traditional power grids to smart grids, significant
increase in the use of renewable energy sources, and soaring electricity prices
has triggered a digital transformation of the energy infrastructure that
enables new, data driven, applications often supported by machine learning
models. However, the majority of the developed machine learning models rely on
univariate data. To date, a structured study considering the role meta-data and
additional measurements resulting in multivariate data is missing. In this
paper we propose a taxonomy that identifies and structures various types of
data related to energy applications. The taxonomy can be used to guide
application specific data model development for training machine learning
models. Focusing on a household electricity forecasting application, we
validate the effectiveness of the proposed taxonomy in guiding the selection of
the features for various types of models. As such, we study of the effect of
domain, contextual and behavioral features on the forecasting accuracy of four
interpretable machine learning techniques and three openly available datasets.
Finally, using a feature importance techniques, we explain individual feature
contributions to the forecasting accuracy.

</details>


### [249] [The Generalized Skew Spectrum of Graphs](https://arxiv.org/abs/2505.23609)
*Armando Bellante,Martin Plávala,Alessandro Luongo*

Key words: 图嵌入, 置換不変性, 群论, 谐波分析, 图不变量

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种通用的置换不变图嵌入方法，扩展了Kondor & Borgwardt（2008）的斜谱理论，适用于更丰富的图结构，并在计算复杂性和表达能力之间实现权衡。

Motivation: 传统斜谱（Skew Spectrum）无法处理属性图、多层图和超图等复杂结构，因此需要一种更通用的方法来捕捉这些结构的特征。

Method: 基于群论和谐波分析，提出了一类新的图不变量，这些不变量是置换不变的，并通过一种函数族在计算复杂性和表达能力之间实现平衡，同时保持了算法的扩展性。

Result: 理论证明了新方法的置换不变性，并通过实验展示了其在表达能力上的提升，同时保持了与斜谱相同的计算复杂度。

Conclusion: 该方法提供了一种更通用的图嵌入技术，适用于多种图结构，并在计算效率与表达能力之间实现了优化。

Abstract: This paper proposes a family of permutation-invariant graph embeddings,
generalizing the Skew Spectrum of graphs of Kondor & Borgwardt (2008). Grounded
in group theory and harmonic analysis, our method introduces a new class of
graph invariants that are isomorphism-invariant and capable of embedding richer
graph structures - including attributed graphs, multilayer graphs, and
hypergraphs - which the Skew Spectrum could not handle. Our generalization
further defines a family of functions that enables a trade-off between
computational complexity and expressivity. By applying
generalization-preserving heuristics to this family, we improve the Skew
Spectrum's expressivity at the same computational cost. We formally prove the
invariance of our generalization, demonstrate its improved expressiveness
through experiments, and discuss its efficient computation.

</details>


### [250] [Inference-time Scaling of Diffusion Models through Classical Search](https://arxiv.org/abs/2505.23614)
*Xiangcheng Zhang,Haowei Lin,Haotian Ye,James Zou,Jianzhu Ma,Yitao Liang,Yilun Du*

Key words: 扩散模型，推理控制，经典搜索算法，Langevin MCMC，树搜索

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种利用经典搜索算法优化扩散模型推理控制的通用框架，通过局部和全局搜索有效导航生成空间，提升了性能和效率。

Motivation: 解决扩散模型在推理时间控制上的挑战，适应多样化的测试目标。

Method: 结合局部搜索（退火Langevin MCMC）和全局搜索（广度优先与深度优先树搜索）的框架。

Result: 在规划、离线强化学习和图像生成等任务中显著提升性能与效率。

Conclusion: 经典搜索为扩散模型的推理时间扩展提供了理论基础和实用工具。

Abstract: Classical search algorithms have long underpinned modern artificial
intelligence. In this work, we tackle the challenge of inference-time control
in diffusion models -- adapting generated outputs to meet diverse test-time
objectives -- using principles from classical search. We propose a general
framework that orchestrates local and global search to efficiently navigate the
generative space. It employs a theoretically grounded local search via annealed
Langevin MCMC and performs compute-efficient global exploration using
breadth-first and depth-first tree search. We evaluate our approach on a range
of challenging domains, including planning, offline reinforcement learning, and
image generation. Across all tasks, we observe significant gains in both
performance and efficiency. These results show that classical search provides a
principled and practical foundation for inference-time scaling in diffusion
models. Project page at diffusion-inference-scaling.github.io.

</details>


### [251] [Differential Information: An Information-Theoretic Perspective on Preference Optimization](https://arxiv.org/abs/2505.23761)
*Yunjae Won,Hyunji Lee,Hyeonbin Hwang,Minjoon Seo*

Key words: Direct Preference Optimization, Differential Information Distribution, log-ratio reward, policy behavior, entropy

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: DPO的log-ratio奖励参数化理论不完善；通过DID框架填补这一空白，证明其最优性，并揭示偏好数据与策略行为的关系，为DPO提供统一视角。

Motivation: 填补Direct Preference Optimization（DPO）中log-ratio奖励参数化理论空白，通过Differential Information Distribution（DID）框架分析其最优性及策略行为。

Method: 利用DID框架，分析偏好标签如何编码从参考策略到目标策略的差异信息，推导log-ratio奖励的最优性，并研究熵对策略分布的影响。

Result: 证明log-ratio奖励在DPO中是唯一最优形式，揭示了高低熵差异信息对策略的不同影响，并通过实验验证其在真实数据集中的有效性。

Conclusion: DID框架为DPO提供了理论支持，统一了目标函数、偏好数据和策略行为的理解，高低熵差异信息分别适用于不同任务场景。

Abstract: Direct Preference Optimization (DPO) has become a standard technique for
aligning language models with human preferences in a supervised manner. Despite
its empirical success, the theoretical justification behind its log-ratio
reward parameterization remains incomplete. In this work, we address this gap
by utilizing the Differential Information Distribution (DID): a distribution
over token sequences that captures the information gained during policy
updates. First, we show that when preference labels encode the differential
information required to transform a reference policy into a target policy, the
log-ratio reward in DPO emerges as the uniquely optimal form for learning the
target policy via preference optimization. This result naturally yields a
closed-form expression for the optimal sampling distribution over rejected
responses. Second, we find that the condition for preferences to encode
differential information is fundamentally linked to an implicit assumption
regarding log-margin ordered policies-an inductive bias widely used in
preference optimization yet previously unrecognized. Finally, by analyzing the
entropy of the DID, we characterize how learning low-entropy differential
information reinforces the policy distribution, while high-entropy differential
information induces a smoothing effect, which explains the log-likelihood
displacement phenomenon. We validate our theoretical findings in synthetic
experiments and extend them to real-world instruction-following datasets. Our
results suggest that learning high-entropy differential information is crucial
for general instruction-following, while learning low-entropy differential
information benefits knowledge-intensive question answering. Overall, our work
presents a unifying perspective on the DPO objective, the structure of
preference data, and resulting policy behaviors through the lens of
differential information.

</details>


### [252] [Learning Interpretable Differentiable Logic Networks for Tabular Regression](https://arxiv.org/abs/2505.23615)
*Chang Yue,Niraj K. Jha*

Key words: 可微逻辑网络, 回归任务, 解释性, 计算效率, 表格数据

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文扩展了可微逻辑网络（DLNs）框架，将其应用于监督表格回归任务，通过重新设计输出层和统一训练过程，实现了与现有方法相当的准确性，同时保持了解释性和快速推理能力。

Motivation: 传统神经网络在推理过程中存在不透明性和高计算成本的问题。为了解决这些问题，尤其是针对表格数据分类任务，作者此前提出了DLNs。本文旨在将DLNs扩展至回归任务，以兼顾准确性、解释性和计算效率。

Method: 论文重新设计了DLN的输出层以支持连续目标值，并将原本的两阶段训练过程统一为单阶段可微学习。方法基于将离散逻辑松弛为可微形式，通过梯度下降学习二进制逻辑操作构成的网络。

Result: 在15个公开回归基准测试中，回归DLNs的准确性与现代神经网络和经典回归基线相当或更优，同时保持了模型透明度和低推理成本。

Conclusion: DLNs是一种可行且具有成本效益的回归任务替代方案，特别适用于需要模型解释性和计算效率的场景。

Abstract: Neural networks (NNs) achieve outstanding performance in many domains;
however, their decision processes are often opaque and their inference can be
computationally expensive in resource-constrained environments. We recently
proposed Differentiable Logic Networks (DLNs) to address these issues for
tabular classification based on relaxing discrete logic into a differentiable
form, thereby enabling gradient-based learning of networks built from binary
logic operations. DLNs offer interpretable reasoning and substantially lower
inference cost.
  We extend the DLN framework to supervised tabular regression. Specifically,
we redesign the final output layer to support continuous targets and unify the
original two-phase training procedure into a single differentiable stage. We
evaluate the resulting model on 15 public regression benchmarks, comparing it
with modern neural networks and classical regression baselines. Regression DLNs
match or exceed baseline accuracy while preserving interpretability and fast
inference. Our results show that DLNs are a viable, cost-effective alternative
for regression tasks, especially where model transparency and computational
efficiency are important.

</details>


### [253] [Prompting Whisper for Improved Verbatim Transcription and End-to-end Miscue Detection](https://arxiv.org/abs/2505.23627)
*Griffin Dietz Smith,Dianna Yee,Jennifer King Chen,Leah Findlater*

Key words: 阅读错误检测，自动语音识别，端到端模型，逐字转录，提示方法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新颖的端到端架构，通过提示结合目标阅读文本，提高了逐字转录和直接错误检测的性能，优于现有方法。

Motivation: 当前的后处理方法在ASR转录不准确时表现不佳，因此需要改进阅读错误标注的方法。

Method: 提出了一种端到端架构，通过提示结合目标阅读文本，并训练模型以同时优化逐字转录和直接错误检测。

Result: 在两个案例研究（儿童朗读和成人非典型语音）中，所提策略在逐字转录和错误检测方面优于现有技术。

Conclusion: 结合阅读文本的提示方法显著提升了转录和错误检测的性能，证明了端到端模型的可行性。

Abstract: Identifying mistakes (i.e., miscues) made while reading aloud is commonly
approached post-hoc by comparing automatic speech recognition (ASR)
transcriptions to the target reading text. However, post-hoc methods perform
poorly when ASR inaccurately transcribes verbatim speech. To improve on current
methods for reading error annotation, we propose a novel end-to-end
architecture that incorporates the target reading text via prompting and is
trained for both improved verbatim transcription and direct miscue detection.
Our contributions include: first, demonstrating that incorporating reading text
through prompting benefits verbatim transcription performance over fine-tuning,
and second, showing that it is feasible to augment speech recognition tasks for
end-to-end miscue detection. We conducted two case studies -- children's
read-aloud and adult atypical speech -- and found that our proposed strategies
improve verbatim transcription and miscue detection compared to current
state-of-the-art.

</details>


### [254] [MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment](https://arxiv.org/abs/2505.23634)
*John Halloran*

Key words: MCP协议, FBA攻击, 直接偏好优化(DPO), RAG-Pref, 大语言模型对齐

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文揭示了MCP协议存在更广泛的威胁模型，仅需在线发布恶意内容即可攻击，并提出结合DPO和新型RAG-Pref策略显著提升LLMs的防御能力。

Motivation: 现有MCP协议易受FBA攻击，攻击者无需用户直接下载恶意文件，仅需在线发布即可诱骗代理执行攻击。需改进对齐防护方法。

Method: 引入含FBA和良性样本的MCP数据集，测试DPO对LLMs拒绝训练的效能；并提出RAG-Pref策略，结合RAG优化偏好对齐。

Result: DPO提升了防护效果，但依赖模型原有对齐方案（如GRPO效果差）；RAG-Pref显著增强FBA拒绝能力，与DPO结合效果更佳。

Conclusion: RAG-Pref与DPO联合可大幅提升LLMs对MCP攻击的防御能力，为对齐防护提供新方向。

Abstract: The model context protocol (MCP) has been widely adapted as an open standard
enabling the seamless integration of generative AI agents. However, recent work
has shown the MCP is susceptible to retrieval-based "falsely benign" attacks
(FBAs), allowing malicious system access and credential theft, but requiring
that users download compromised files directly to their systems. Herein, we
show that the threat model of MCP-based attacks is significantly broader than
previously thought, i.e., attackers need only post malicious content online to
deceive MCP agents into carrying out their attacks on unsuspecting victims'
systems.
  To improve alignment guardrails against such attacks, we introduce a new MCP
dataset of FBAs and (truly) benign samples to explore the effectiveness of
direct preference optimization (DPO) for the refusal training of large language
models (LLMs). While DPO improves model guardrails against such attacks, we
show that the efficacy of refusal learning varies drastically depending on the
model's original post-training alignment scheme--e.g., GRPO-based LLMs learn to
refuse extremely poorly. Thus, to further improve FBA refusals, we introduce
Retrieval Augmented Generation for Preference alignment (RAG-Pref), a novel
preference alignment strategy based on RAG. We show that RAG-Pref significantly
improves the ability of LLMs to refuse FBAs, particularly when combined with
DPO alignment, thus drastically improving guardrails against MCP-based attacks.

</details>


### [255] [Global optimization of graph acquisition functions for neural architecture search](https://arxiv.org/abs/2505.23640)
*Yilin Xie,Shiqiang Zhang,Jixiang Qing,Ruth Misener,Calvin Tsay*

Key words: Graph Bayesian optimization, neural architecture search, graph kernels, acquisition functions, discrete optimization

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: This paper proposes a method to optimize graph-based Bayesian optimization for neural architecture search by formulating the graph input space with properties like reachability and shortest paths, proving theoretical equivalence, and demonstrating efficacy through benchmarks.

Motivation: Existing graph Bayesian optimization (BO) methods focus on surrogate models and kernels but lack effective discrete optimization over graph structures. This paper addresses the challenge of formulating complex graph search spaces and acquisition functions for neural architecture search (NAS).

Method: The authors introduce explicit optimization formulations for graph input spaces, incorporating properties like reachability and shortest paths. These formulations are used to derive graph kernels and acquisition functions, with theoretical proofs of equivalence to graph space representation.

Result: Numerical experiments on NAS benchmarks show the method efficiently identifies optimal architectures, demonstrating its effectiveness in most cases.

Conclusion: The proposed approach successfully bridges the gap in discrete optimization over graph structures for BO in NAS, providing both theoretical grounding and practical performance improvements.

Abstract: Graph Bayesian optimization (BO) has shown potential as a powerful and
data-efficient tool for neural architecture search (NAS). Most existing graph
BO works focus on developing graph surrogates models, i.e., metrics of networks
and/or different kernels to quantify the similarity between networks. However,
the acquisition optimization, as a discrete optimization task over graph
structures, is not well studied due to the complexity of formulating the graph
search space and acquisition functions. This paper presents explicit
optimization formulations for graph input space including properties such as
reachability and shortest paths, which are used later to formulate graph
kernels and the acquisition function. We theoretically prove that the proposed
encoding is an equivalent representation of the graph space and provide
restrictions for the NAS domain with either node or edge labels. Numerical
results over several NAS benchmarks show that our method efficiently finds the
optimal architecture for most cases, highlighting its efficacy.

</details>


### [256] [Continuous Chain of Thought Enables Parallel Exploration and Reasoning](https://arxiv.org/abs/2505.23648)
*Halil Alperen Gozeten,M. Emrullah Ildiz,Xuechen Zhang,Hrayr Harutyunyan,Ankit Singh Rawat,Samet Oymak*

Key words: 思维链,连续值令牌,逻辑推理,优化策略,组合问题

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了连续值令牌的思维链（CoT2）方法，相比传统的离散采样更富表现力，并在逻辑推理任务中验证了其优势。

Motivation: 传统的离散采样方法在思维链生成中取得了成功，但连续值令牌提供了更丰富和表达力更强的替代方案，尤其在需要搜索能力的任务中。

Method: 通过逻辑推理任务验证CoT2的优势，提供优化和探索方法，包括匹配softmax输出的监督策略和两种采样策略（K个离散令牌采样和连续探索）。

Result: 理论证明CoT2可以并行追踪多条轨迹，并提升推理效率；单层Transformer结合CoT2可解决组合“子集和问题”。实验表明CoT2策略优化能超越初始监督效果。

Conclusion: CoT2通过连续值令牌和优化策略，显著提升了模型在复杂任务中的表现，为思维链方法开辟了新方向。

Abstract: Current language models generate chain-of-thought traces by autoregressively
sampling tokens from a finite vocabulary. While this discrete sampling has
achieved remarkable success, conducting chain-of-thought with
continuously-valued tokens (CoT2) offers a richer and more expressive
alternative. Our work examines the benefits of CoT2 through logical reasoning
tasks that inherently require search capabilities and provide optimization and
exploration methods for CoT2. Theoretically, we show that CoT2 allows the model
to track multiple traces in parallel and quantify its benefits for inference
efficiency. Notably, one layer transformer equipped with CoT2 can provably
solve the combinatorial "subset sum problem" given sufficient embedding
dimension. These insights lead to a novel and effective supervision strategy
where we match the softmax outputs to the empirical token distributions of a
set of target traces. Complementing this, we introduce sampling strategies that
unlock policy optimization and self-improvement for CoT2. Our first strategy
samples and composes $K$ discrete tokens at each decoding step to control the
level of parallelism, and reduces to standard CoT when $K=1$. Our second
strategy relies on continuous exploration over the probability simplex.
Experiments confirm that policy optimization with CoT2 indeed improves the
performance of the model beyond its initial discrete or continuous supervision.

</details>


### [257] [Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation](https://arxiv.org/abs/2505.23651)
*Juncheol Shin,Minsang Seok,Seonggon Kim,Eunhyeok Park*

Key words: 模型融合, 多目标域适应, 量化, HDRQ, 误差屏障

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了量化对模型融合的影响，提出了一种名为HDRQ的后训练量化方法，旨在减少量化过程中的误差并优化多目标域适应的模型融合效果。

Motivation: 量化在实际应用中常用于目标特定数据，但其会限制兴趣域并引入离散化效应，使模型融合变得复杂。本研究旨在解决这一挑战。

Method: 通过分析误差屏障的视角，提出HDRQ（Hessian和远距离正则化量化）方法，优化量化过程以减少与预训练模型的偏差并平滑损失面。

Result: 实验证明HDRQ方法在多目标域适应的模型融合中表现有效。

Conclusion: HDRQ是首个针对量化后模型融合问题的解决方案，显著提升了量化模型的融合效果。

Abstract: Model merging has emerged as a powerful technique for combining task-specific
weights, achieving superior performance in multi-target domain adaptation.
However, when applied to practical scenarios, such as quantized models, new
challenges arise. In practical scenarios, quantization is often applied to
target-specific data, but this process restricts the domain of interest and
introduces discretization effects, making model merging highly non-trivial. In
this study, we analyze the impact of quantization on model merging through the
lens of error barriers. Leveraging these insights, we propose a novel
post-training quantization, HDRQ - Hessian and distant regularizing
quantization - that is designed to consider model merging for multi-target
domain adaptation. Our approach ensures that the quantization process incurs
minimal deviation from the source pre-trained model while flattening the loss
surface to facilitate smooth model merging. To our knowledge, this is the first
study on this challenge, and extensive experiments confirm its effectiveness.

</details>


### [258] [How does Transformer Learn Implicit Reasoning?](https://arxiv.org/abs/2505.23653)
*Jiaran Ye,Zijun Yao,Zhidian Huang,Liangming Pan,Jinxin Liu,Yushi Bai,Amy Xin,Liu Weichuan,Xiaoyin Che,Lei Hou,Juanzi Li*

Key words: 大型语言模型, 多跳推理, 隐式推理, 表示学习, 透明度

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文研究了大型语言模型（LLMs）如何隐式地进行多跳推理，揭示了其发展的三个阶段，并提出了两种诊断工具来解释这些行为。

Motivation: 探索LLMs隐式多跳推理的机制，以增强模型透明度和理解推理过程。

Method: 通过受控符号环境训练transformer模型，分析其发展轨迹，并引入跨查询语义修补和余弦表示透镜两种工具。

Result: 发现训练分为三个阶段，原子三元组非必需但加速学习，成功的推理与隐藏空间的余弦聚类相关。

Conclusion: 研究为LLMs隐式推理的透明度提供了新见解，并揭示了表示结构与推理能力间的联系。

Abstract: Recent work suggests that large language models (LLMs) can perform multi-hop
reasoning implicitly -- producing correct answers without explicitly
verbalizing intermediate steps -- but the underlying mechanisms remain poorly
understood. In this paper, we study how such implicit reasoning emerges by
training transformers from scratch in a controlled symbolic environment. Our
analysis reveals a three-stage developmental trajectory: early memorization,
followed by in-distribution generalization, and eventually cross-distribution
generalization. We find that training with atomic triples is not necessary but
accelerates learning, and that second-hop generalization relies on query-level
exposure to specific compositional structures. To interpret these behaviors, we
introduce two diagnostic tools: cross-query semantic patching, which identifies
semantically reusable intermediate representations, and a cosine-based
representational lens, which reveals that successful reasoning correlates with
the cosine-base clustering in hidden space. This clustering phenomenon in turn
provides a coherent explanation for the behavioral dynamics observed across
training, linking representational structure to reasoning capability. These
findings provide new insights into the interpretability of implicit multi-hop
reasoning in LLMs, helping to clarify how complex reasoning processes unfold
internally and offering pathways to enhance the transparency of such models.

</details>


### [259] [AMBER: Adaptive Mesh Generation by Iterative Mesh Resolution Prediction](https://arxiv.org/abs/2505.23663)
*Niklas Freymuth,Tobias Würth,Nicolas Schreiber,Balazs Gyenes,Andreas Boltres,Johannes Mitsch,Aleksandar Taranovic,Tai Hoang,Philipp Dahlinger,Philipp Becker,Luise Kärger,Gerhard Neumann*

Key words: 自适应网格,有限元方法,监督学习,图神经网络

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文AMBER提出了一种基于监督学习的自适应网格划分方法，通过迭代预测尺寸场来优化复杂物理系统的仿真效率。

Motivation: 传统自适应网格方法依赖特定任务启发式或人工设计，成本高且效率低。

Method: AMBER采用分层图神经网络，从粗网格开始迭代预测尺寸场，并使用现成网格生成器生成中间网格。

Result: AMBER在2D和3D数据集上表现优于多种基线方法，包括图神经网络、卷积神经网络和强化学习方法。

Conclusion: AMBER能够泛化到未见几何体，且显著提升仿真效率和准确性。

Abstract: The cost and accuracy of simulating complex physical systems using the Finite
Element Method (FEM) scales with the resolution of the underlying mesh.
Adaptive meshes improve computational efficiency by refining resolution in
critical regions, but typically require task-specific heuristics or cumbersome
manual design by a human expert. We propose Adaptive Meshing By Expert
Reconstruction (AMBER), a supervised learning approach to mesh adaptation.
Starting from a coarse mesh, AMBER iteratively predicts the sizing field, i.e.,
a function mapping from the geometry to the local element size of the target
mesh, and uses this prediction to produce a new intermediate mesh using an
out-of-the-box mesh generator. This process is enabled through a hierarchical
graph neural network, and relies on data augmentation by automatically
projecting expert labels onto AMBER-generated data during training. We evaluate
AMBER on 2D and 3D datasets, including classical physics problems, mechanical
components, and real-world industrial designs with human expert meshes. AMBER
generalizes to unseen geometries and consistently outperforms multiple recent
baselines, including ones using Graph and Convolutional Neural Networks, and
Reinforcement Learning-based approaches.

</details>


### [260] [Bayesian Optimization from Human Feedback: Near-Optimal Regret Bounds](https://arxiv.org/abs/2505.23673)
*Aya Kayal,Sattar Vakili,Laura Toni,Da-shan Shiu,Alberto Bernacchia*

Key words: 贝叶斯优化, 偏好反馈, 遗憾界, BTL模型, 信息增益

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文通过改进贝叶斯优化（BO）中基于偏好反馈的算法，提出了更紧致的性能保证，显著提升了现有结果。

Motivation: 贝叶斯优化在偏好反馈场景中的应用日益重要，但现有算法的性能保证不够紧致，需要通过更优的理论改进。

Method: 在相同的Bradley-Terry-Luce（BTL）反馈模型框架下，开发了新的算法，推导出更紧致的遗憾界。

Result: 新算法的遗憾界为$	ilde{\mathcal{O}}(\sqrt{\Gamma(T)T})$，显著优于现有结果，并且在常见核函数下达到与传统BO相同的样本复杂度。

Conclusion: 论文表明，偏好反馈场景下的贝叶斯优化可以达到与传统BO类似的性能，偏好样本的数量与标量反馈样本数量相当。

Abstract: Bayesian optimization (BO) with preference-based feedback has recently
garnered significant attention due to its emerging applications. We refer to
this problem as Bayesian Optimization from Human Feedback (BOHF), which differs
from conventional BO by learning the best actions from a reduced feedback
model, where only the preference between two actions is revealed to the learner
at each time step. The objective is to identify the best action using a limited
number of preference queries, typically obtained through costly human feedback.
Existing work, which adopts the Bradley-Terry-Luce (BTL) feedback model,
provides regret bounds for the performance of several algorithms. In this work,
within the same framework we develop tighter performance guarantees.
Specifically, we derive regret bounds of
$\tilde{\mathcal{O}}(\sqrt{\Gamma(T)T})$, where $\Gamma(T)$ represents the
maximum information gain$\unicode{x2014}$a kernel-specific complexity
term$\unicode{x2014}$and $T$ is the number of queries. Our results
significantly improve upon existing bounds. Notably, for common kernels, we
show that the order-optimal sample complexities of conventional
BO$\unicode{x2014}$achieved with richer feedback models$\unicode{x2014}$are
recovered. In other words, the same number of preferential samples as
scalar-valued samples is sufficient to find a nearly optimal solution.

</details>


### [261] [Understanding Mode Connectivity via Parameter Space Symmetry](https://arxiv.org/abs/2505.23681)
*Bo Zhao,Nima Dehmamy,Robin Walters,Rose Yu*

Key words: 神经网络, 模态连通性, 对称性, 跳跃连接, 损失景观

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种利用参数空间对称性探索神经网络最小值连通性的新方法，通过研究对称群与最小值拓扑的关系，解释了线性网络最小值的连通组件数量及其在跳跃连接下的变化，并分析了模态连接性的条件。

Motivation: 研究神经网络的模态连通性现象，探讨其理论解释不足的问题，从参数对称性角度提供新的分析视角。

Method: 通过将对称群的拓扑与最小值的拓扑联系起来，推导线性网络最小值的连通组件数量；分析跳跃连接的影响；研究对称性参数下的模态连接性条件。

Result: 发现跳跃连接可减少连通组件数量，并给出了对称性诱导的连接曲线的显式表达式及其曲率条件，证明线性模态连接性近似成立的条件。

Conclusion: 研究强调了连续对称性在理解神经网络损失景观中的重要性，为模型合并和微调提供了理论基础。

Abstract: Neural network minima are often connected by curves along which train and
test loss remain nearly constant, a phenomenon known as mode connectivity.
While this property has enabled applications such as model merging and
fine-tuning, its theoretical explanation remains unclear. We propose a new
approach to exploring the connectedness of minima using parameter space
symmetry. By linking the topology of symmetry groups to that of the minima, we
derive the number of connected components of the minima of linear networks and
show that skip connections reduce this number. We then examine when mode
connectivity and linear mode connectivity hold or fail, using parameter
symmetries which account for a significant part of the minimum. Finally, we
provide explicit expressions for connecting curves in the minima induced by
symmetry. Using the curvature of these curves, we derive conditions under which
linear mode connectivity approximately holds. Our findings highlight the role
of continuous symmetries in understanding the neural network loss landscape.

</details>


### [262] [Learning Compositional Functions with Transformers from Easy-to-Hard Data](https://arxiv.org/abs/2505.23683)
*Zixuan Wang,Eshaan Nichani,Alberto Bietti,Alex Damian,Daniel Hsu,Jason D. Lee,Denny Wu*

Key words: Transformer, 可学习性, 统计查询, 梯度下降, 课程学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文研究了基于Transformer的语言模型在$k$重组合任务中的可学习性，证明了在统计查询(SQ)模型下存在样本量指数增长的限制，但通过梯度下降和课程学习策略可以高效学习。

Motivation: 探索Transformer模型在多步推理任务中的学习能力，特别是数据分布对梯度优化学习的影响，以解决$k$重组合任务的可学习性问题。

Method: 通过统计查询(SQ)下界证明和梯度下降结合两种课程学习策略（渐进难度和混合难度）来研究$k$重组合任务的学习。

Result: SQ模型下样本量需指数增长，但通过梯度下降和课程学习可在多项式时间内高效学习$O(\log k)$层Transformer。

Conclusion: 数据分布中同时包含简单和复杂样本对Transformer学习组合任务至关重要。

Abstract: Transformer-based language models have demonstrated impressive capabilities
across a range of complex reasoning tasks. Prior theoretical work exploring the
expressive power of transformers has shown that they can efficiently perform
multi-step reasoning tasks involving parallelizable computations. However, the
learnability of such constructions, particularly the conditions on the data
distribution that enable efficient learning via gradient-based optimization,
remains an open question. Towards answering this question, in this work we
study the learnability of the $k$-fold composition task, which requires
computing an interleaved composition of $k$ input permutations and $k$ hidden
permutations, and can be expressed by a transformer with $O(\log k)$ layers. On
the negative front, we prove a Statistical Query (SQ) lower bound showing that
any SQ learner that makes only polynomially-many queries to an SQ oracle for
the $k$-fold composition task distribution must have sample size exponential in
$k$, thus establishing a statistical-computational gap. On the other hand, we
show that this function class can be efficiently learned, with runtime and
sample complexity polynomial in $k$, by gradient descent on an $O(\log
k)$-depth transformer via two different curriculum learning strategies: one in
which data consists of $k'$-fold composition functions with $k' \le k$
presented in increasing difficulty, and another in which all such data is
presented simultaneously. Our work sheds light on the necessity and sufficiency
of having both easy and hard examples in the data distribution for transformers
to learn complex compositional tasks.

</details>


### [263] [Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms](https://arxiv.org/abs/2505.23696)
*Hiroshi Kera,Nico Pelleriti,Yuki Ishihara,Max Zimmer,Sebastian Pokutta*

Key words: 多项式方程组, Border基, 深度学习, Transformer, 符号计算

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的Oracle Border Basis算法，通过Transformer oracle优化Border basis计算，显著提升速度（3.5倍）并保证结果正确性。

Motivation: 传统多项式方程组求解方法（如Gröbner基和Border基）计算成本高，深度学习虽能提升效率但可能牺牲正确性。本文旨在开发一种既加速计算又保持输出正确性的新方法。

Method: 设计并训练了一个基于Transformer的oracle，用于识别和消除计算密集型步骤；开发了数据采样方法和紧凑的输入表示方案（tokenization）。

Result: 在关键计算阶段选择性调用oracle，实现了高达3.5倍的加速，且不损失结果正确性；提出了首个Border基采样定理。

Conclusion: 该方法高效、稳定，是对传统符号计算算法的实用改进。

Abstract: Solving systems of polynomial equations, particularly those with finitely
many solutions, is a crucial challenge across many scientific fields.
Traditional methods like Gr\"obner and Border bases are fundamental but suffer
from high computational costs, which have motivated recent Deep Learning
approaches to improve efficiency, albeit at the expense of output correctness.
In this work, we introduce the Oracle Border Basis Algorithm, the first Deep
Learning approach that accelerates Border basis computation while maintaining
output guarantees. To this end, we design and train a Transformer-based oracle
that identifies and eliminates computationally expensive reduction steps, which
we find to dominate the algorithm's runtime. By selectively invoking this
oracle during critical phases of computation, we achieve substantial speedup
factors of up to 3.5x compared to the base algorithm, without compromising the
correctness of results. To generate the training data, we develop a sampling
method and provide the first sampling theorem for border bases. We construct a
tokenization and embedding scheme tailored to monomial-centered algebraic
computations, resulting in a compact and expressive input representation, which
reduces the number of tokens to encode an $n$-variate polynomial by a factor of
$O(n)$. Our learning approach is data efficient, stable, and a practical
enhancement to traditional computer algebra algorithms and symbolic
computation.

</details>


### [264] [DiCoFlex: Model-agnostic diverse counterfactuals with flexible control](https://arxiv.org/abs/2505.23700)
*Oleksii Furman,Ulvi Movsum-zada,Patryk Marszalek,Maciej Zięba,Marek Śmieja*

Key words: 反事实解释,可解释AI,条件归一化流,模型无关

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: DiCoFlex是一种新颖的模型无关条件生成框架，通过单次前向传递生成多样化的反事实解释，解决了现有方法需持续访问模型、计算密集且灵活性不足的问题。

Motivation: 现有反事实解释方法通常需要持续访问预测模型、计算成本高且难以适应用户自定义约束，DiCoFlex旨在解决这些局限。

Method: 提DiCoFlex基于条件归一化流，仅需标注数据训练，支持实时用户驱动的约束定制（如稀疏性和可操作性）。

Result: 在基准数据集上，DiCoFlex在有效性、多样性、邻近性和约束遵从性上优于现有方法。

Conclusion: DiCoFlex为敏感决策领域提供了一种实用且可扩展的反事实生成解决方案。

Abstract: Counterfactual explanations play a pivotal role in explainable artificial
intelligence (XAI) by offering intuitive, human-understandable alternatives
that elucidate machine learning model decisions. Despite their significance,
existing methods for generating counterfactuals often require constant access
to the predictive model, involve computationally intensive optimization for
each instance and lack the flexibility to adapt to new user-defined constraints
without retraining. In this paper, we propose DiCoFlex, a novel model-agnostic,
conditional generative framework that produces multiple diverse counterfactuals
in a single forward pass. Leveraging conditional normalizing flows trained
solely on labeled data, DiCoFlex addresses key limitations by enabling
real-time user-driven customization of constraints such as sparsity and
actionability at inference time. Extensive experiments on standard benchmark
datasets show that DiCoFlex outperforms existing methods in terms of validity,
diversity, proximity, and constraint adherence, making it a practical and
scalable solution for counterfactual generation in sensitive decision-making
domains.

</details>


### [265] [(U)NFV: Supervised and Unsupervised Neural Finite Volume Methods for Solving Hyperbolic PDEs](https://arxiv.org/abs/2505.23702)
*Nathan Lichtlé,Alexi Canesse,Zhe Fu,Hossein Nick Zinat Matin,Maria Laura Delle Monache,Alexandre M. Bayen*

Key words: (U)NFV, 有限体积方法, 双曲守恒定律, 神经网络, 交通建模

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了(U)NFV，一种模块化神经网络架构，用于解决双曲守恒定律，相比传统有限体积方法（FV）具有更高的精度和灵活性。

Motivation: 双曲偏微分方程（PDEs）尤其是守恒定律在实际应用中难以求解，而传统的有限体积方法虽具有数学优势（如收敛性和守恒性），但在复杂场景下缺乏精度和灵活性。

Method: (U)NFV通过扩展时空模板学习更新规则，同时保持守恒结构。它支持基于解数据的监督训练（NFV）和弱形式残差损失的无监督训练（UNFV）。

Result: 在求解一阶守恒定律时，(U)NFV的误差比Godunov方法低10倍，优于ENO/WENO，且复杂度远低于间断Galerkin求解器。在交通建模问题上，其非线性波动力学捕捉能力显著优于传统FV方法。

Conclusion: (U)NFV为双曲守恒定律的求解提供了一种高精度、高效且灵活的解决方案，适用于复杂场景。

Abstract: We introduce (U)NFV, a modular neural network architecture that generalizes
classical finite volume (FV) methods for solving hyperbolic conservation laws.
Hyperbolic partial differential equations (PDEs) are challenging to solve,
particularly conservation laws whose physically relevant solutions contain
shocks and discontinuities. FV methods are widely used for their mathematical
properties: convergence to entropy solutions, flow conservation, or total
variation diminishing, but often lack accuracy and flexibility in complex
settings. Neural Finite Volume addresses these limitations by learning update
rules over extended spatial and temporal stencils while preserving conservation
structure. It supports both supervised training on solution data (NFV) and
unsupervised training via weak-form residual loss (UNFV). Applied to
first-order conservation laws, (U)NFV achieves up to 10x lower error than
Godunov's method, outperforms ENO/WENO, and rivals discontinuous Galerkin
solvers with far less complexity. On traffic modeling problems, both from PDEs
and from experimental highway data, (U)NFV captures nonlinear wave dynamics
with significantly higher fidelity and scalability than traditional FV
approaches.

</details>


### [266] [Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better](https://arxiv.org/abs/2505.23705)
*Danny Driess,Jost Tobias Springenberg,Brian Ichter,Lili Yu,Adrian Li-Bell,Karl Pertsch,Allen Z. Ren,Homer Walke,Quan Vuong,Lucy Xiaoyang Shi,Sergey Levine*

Key words: 视觉-语言-动作模型, 连续控制, 知识迁移, 知识隔离, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了视觉-语言-动作（VLA）模型中预训练视觉语言模型（VLM）与连续动作专家模块的兼容性问题，发现简单整合会损害训练速度和知识迁移，并提出了解决方法。

Motivation: VLA模型结合端到端学习和语义知识迁移，但因预训练VLM的参数量庞大和离散输出特性，难以满足实时控制需求。现有方法通过添加动作专家模块改善控制能力，但其对知识迁移的影响尚不明确。

Method: 研究了包含连续扩散或流匹配动作专家的VLA模型，分析了设计和性能影响，并提出了一种保护VLM主干的知识隔离技术。

Result: 研究发现简单整合动作专家会显著损害训练速度和知识迁移，提出的隔离技术有效缓解了这一问题。

Conclusion: 研究强调了VLA设计中保持VLM知识完整性的重要性，并通过实验验证了隔离技术的有效性。

Abstract: Vision-language-action (VLA) models provide a powerful approach to training
control policies for physical systems, such as robots, by combining end-to-end
learning with transfer of semantic knowledge from web-scale vision-language
model (VLM) training. However, the constraints of real-time control are often
at odds with the design of VLMs: the most powerful VLMs have tens or hundreds
of billions of parameters, presenting an obstacle to real-time inference, and
operate on discrete tokens rather than the continuous-valued outputs that are
required for controlling robots. To address this challenge, recent VLA models
have used specialized modules for efficient continuous control, such as action
experts or continuous output heads, which typically require adding new
untrained parameters to the pretrained VLM backbone. While these modules
improve real-time and control capabilities, it remains an open question whether
they preserve or degrade the semantic knowledge contained in the pretrained
VLM, and what effect they have on the VLA training dynamics. In this paper, we
study this question in the context of VLAs that include a continuous diffusion
or flow matching action expert, showing that naively including such experts
significantly harms both training speed and knowledge transfer. We provide an
extensive analysis of various design choices, their impact on performance and
knowledge transfer, and propose a technique for insulating the VLM backbone
during VLA training that mitigates this issue. Videos are available at
https://pi.website/research/knowledge_insulation.

</details>


### [267] [TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning](https://arxiv.org/abs/2505.23719)
*Andreas Auer,Patrick Podest,Daniel Klotz,Sebastian Böck,Günter Klambauer,Sepp Hochreiter*

Key words: in-context learning, time series forecasting, zero-shot prediction, xLSTM, TiRex, CPM masking

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: TiRex, an enhanced LSTM model, introduces competitive in-context learning for zero-shot time series forecasting, outperforming larger models.

Motivation: Address the gap between transformer architectures (poor in time series forecasting) and LSTMs (lacking in-context learning) by combining xLSTM's strengths.

Method: Leverages xLSTM for in-context learning with a training-time masking strategy (CPM) to enhance state-tracking for long-horizon forecasting.

Result: Sets new SOTA on HuggingFace benchmarks (GiftEval & Chronos-ZS), outperforming larger models like TabPFN-TS, Chronos Bolt, TimesFM, and Moirai in short- and long-term forecasts.

Conclusion: TiRex bridges the divide between state-tracking and in-context learning, proving highly effective for zero-shot time series forecasting.

Abstract: In-context learning, the ability of large language models to perform tasks
using only examples provided in the prompt, has recently been adapted for time
series forecasting. This paradigm enables zero-shot prediction, where past
values serve as context for forecasting future values, making powerful
forecasting tools accessible to non-experts and increasing the performance when
training data are scarce. Most existing zero-shot forecasting approaches rely
on transformer architectures, which, despite their success in language, often
fall short of expectations in time series forecasting, where recurrent models
like LSTMs frequently have the edge. Conversely, while LSTMs are well-suited
for time series modeling due to their state-tracking capabilities, they lack
strong in-context learning abilities. We introduce TiRex that closes this gap
by leveraging xLSTM, an enhanced LSTM with competitive in-context learning
skills. Unlike transformers, state-space models, or parallelizable RNNs such as
RWKV, TiRex retains state-tracking, a critical property for long-horizon
forecasting. To further facilitate its state-tracking ability, we propose a
training-time masking strategy called CPM. TiRex sets a new state of the art in
zero-shot time series forecasting on the HuggingFace benchmarks GiftEval and
Chronos-ZS, outperforming significantly larger models including TabPFN-TS
(Prior Labs), Chronos Bolt (Amazon), TimesFM (Google), and Moirai (Salesforce)
across both short- and long-term forecasts.

</details>


### [268] [COBRA: Contextual Bandit Algorithm for Ensuring Truthful Strategic Agents](https://arxiv.org/abs/2505.23720)
*Arun Verma,Indrajit Saha,Makoto Yokoo,Bryan Kian Hsiang Low*

Key words: 情境赌博、策略性代理、激励机制、次线性遗憾、COBRA算法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为COBRA的算法，用于解决涉及策略性代理的情境赌博问题，能够在没有货币激励的情况下抑制代理的策略性行为，并保证激励兼容性和次线性遗憾。

Motivation: 现有的情境赌博研究假设代理会如实报告其臂（选择），这在许多实际应用中不现实（如销售者可能为了优势虚报产品质量）。因此，需要一种方法在无货币激励的前提下抑制代理的策略性行为。

Method: 提出了COBRA算法，通过设计激励机制和优化系统整体奖励，确保代理的策略性行为被抑制，同时满足激励兼容性和次线性遗憾保证。

Result: 实验验证了COBRA算法在不同性能方面的有效性，证明了其抑制策略性行为的能力和系统性奖励优化的效果。

Conclusion: COBRA算法为解决策略性代理的情境赌博问题提供了一种无需货币激励的有效解决方案，具有理论和实践意义。

Abstract: This paper considers a contextual bandit problem involving multiple agents,
where a learner sequentially observes the contexts and the agent's reported
arms, and then selects the arm that maximizes the system's overall reward.
Existing work in contextual bandits assumes that agents truthfully report their
arms, which is unrealistic in many real-life applications. For instance,
consider an online platform with multiple sellers; some sellers may
misrepresent product quality to gain an advantage, such as having the platform
preferentially recommend their products to online users. To address this
challenge, we propose an algorithm, COBRA, for contextual bandit problems
involving strategic agents that disincentivize their strategic behavior without
using any monetary incentives, while having incentive compatibility and a
sub-linear regret guarantee. Our experimental results also validate the
different performance aspects of our proposed algorithm.

</details>


### [269] [DiffER: Categorical Diffusion for Chemical Retrosynthesis](https://arxiv.org/abs/2505.23721)
*Sean Current,Ziqi Chen,Daniel Adu-Ampratwum,Xia Ning,Srinivasan Parthasarathy*

Key words: 化学逆合成, 分类扩散模型, SMILES编码, 无模板方法, 集成模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出DiffER，一种基于分类扩散的无模板化学逆合成预测方法，通过集成扩散模型和新型长度预测组件，实现高性能反应物预测。

Motivation: 现有基于自然语言处理模型的化学逆合成方法受限于自回归特性，无法同时预测整个SMILES序列。

Method: DiffER采用分类扩散模型，集成扩散模型并引入长度预测组件，近似采样反应物后验分布。

Result: DiffER在top-1准确率上达到最优，top-3/5/10表现竞争力强，SMILES长度预测对性能提升至关重要。

Conclusion: DiffER为无模板模型提供了强基线，能学习多种实验室合成技术，且在置信度和似然指标上表现优异。

Abstract: Methods for automatic chemical retrosynthesis have found recent success
through the application of models traditionally built for natural language
processing, primarily through transformer neural networks. These models have
demonstrated significant ability to translate between the SMILES encodings of
chemical products and reactants, but are constrained as a result of their
autoregressive nature. We propose DiffER, an alternative template-free method
for retrosynthesis prediction in the form of categorical diffusion, which
allows the entire output SMILES sequence to be predicted in unison. We
construct an ensemble of diffusion models which achieves state-of-the-art
performance for top-1 accuracy and competitive performance for top-3, top-5,
and top-10 accuracy among template-free methods. We prove that DiffER is a
strong baseline for a new class of template-free model, capable of learning a
variety of synthetic techniques used in laboratory settings and outperforming a
variety of other template-free methods on top-k accuracy metrics. By
constructing an ensemble of categorical diffusion models with a novel length
prediction component with variance, our method is able to approximately sample
from the posterior distribution of reactants, producing results with strong
metrics of confidence and likelihood. Furthermore, our analyses demonstrate
that accurate prediction of the SMILES sequence length is key to further
boosting the performance of categorical diffusion models.

</details>


### [270] [SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA](https://arxiv.org/abs/2505.23724)
*Minrui Luo,Fuhang Kuang,Yu Wang,Zirui Liu,Tianxing He*

Key words: Parameter-Efficient Fine-Tuning, LoRA, SC-LoRA, 知识遗忘, 微调效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: SC-LoRA是一种新的LoRA初始化框架，通过在低秩子空间中约束可训练LoRA适配器的输出，平衡微调效率和知识保留，解决了传统LoRA收敛慢和知识遗忘的问题。

Motivation: 解决传统LoRA方法在微调大型语言模型时存在的收敛速度慢和知识遗忘问题，同时兼顾高效微调和知识保留的需求。

Method: 引入了Subspace-Constrained LoRA（SC-LoRA），通过在低秩子空间中约束LoRA适配器的输出，平衡保留微调数据的主要特征和避免破坏预训练知识。

Result: 实验证明，SC-LoRA在多种下游任务中优于现有LoRA初始化方法，显著减少了知识遗忘并提升了微调性能。

Conclusion: SC-LoRA有效解决了LoRA的局限性，平衡了微调效率和知识保留，在理论分析和实验中均表现出优越性。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank
Adaptation (LoRA), are indispensable for efficiently customizing Large Language
Models (LLMs). However, vanilla LoRA suffers from slow convergence speed and
knowledge forgetting problems. Recent studies have leveraged the power of
designed LoRA initialization, to enhance the fine-tuning efficiency, or to
preserve knowledge in the pre-trained LLM. However, none of these works can
address the two cases at the same time. To this end, we introduce
Subspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework
engineered to navigate the trade-off between efficient fine-tuning and
knowledge preservation. We achieve this by constraining the output of trainable
LoRA adapters in a low-rank subspace, where the context information of
fine-tuning data is most preserved while the context information of preserved
knowledge is least retained, in a balanced way. Such constraint enables the
trainable weights to primarily focus on the main features of fine-tuning data
while avoiding damaging the preserved knowledge features. We provide
theoretical analysis on our method, and conduct extensive experiments including
safety preservation and world knowledge preservation, on various downstream
tasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning
performance while markedly diminishing knowledge forgetting, surpassing
contemporary LoRA initialization methods.

</details>


### [271] [MuLoCo: Muon is a practical inner optimizer for DiLoCo](https://arxiv.org/abs/2505.23725)
*Benjamin Thérien,Xiaolong Huang,Irina Rish,Eugene Belilovsky*

Key words: DiLoCo, MuLoCo, 通信压缩, 误差反馈, Muon优化器

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: MuLoCo框架通过结合Muon优化器和误差反馈累加器，将通信数据压缩到2位，显著减少通信量且性能几乎无损，优于DiLoCo。

Motivation: DiLoCo框架通信步骤仍涉及全量参数更新，现有方法对其压缩效果及优化器的影响研究不足，需探索更高效的通信减少方法。

Method: 结合Top-k稀疏化和量化技术，对比AdamW与Muon两种本地优化器，并引入误差反馈累加器，评估其在下游任务中的表现。

Result: MuLoCo（Muon优化器+DiLoCo）在通信量减少8倍情况下性能几乎无损，显著优于原始DiLoCo，且内存开销相同。

Conclusion: Muon优化器与误差反馈累加器的组合能高效压缩通信数据，为大模型训练提供低通信开销方案。

Abstract: DiLoCo is a powerful framework for training large language models (LLMs)
under networking constraints with advantages for increasing parallelism and
accelerator utilization in data center settings. Despite significantly reducing
communication frequency, however, DiLoCo's communication steps still involve
all-reducing a complete copy of the model's parameters. While existing works
have explored ways to reduce communication in DiLoCo, the role of error
feedback accumulators and the effect of the inner-optimizer on compressibility
remain under-explored. In this work, we investigate the effectiveness of
standard compression methods including Top-k sparsification and quantization
for reducing the communication overhead of DiLoCo when paired with two local
optimizers (AdamW and Muon). Our experiments pre-training decoder-only
transformer language models (LMs) reveal that leveraging Muon as the inner
optimizer for DiLoCo along with an error-feedback accumulator allows to
aggressively compress the communicated delta to 2-bits with next to no
performance degradation. Crucially, MuLoCo (Muon inner optimizer DiLoCo)
significantly outperforms DiLoCo while communicating 8X less and having
identical memory complexity.

</details>


### [272] [EmotionRankCLAP: Bridging Natural Language Speaking Styles and Ordinal Speech Emotion via Rank-N-Contrast](https://arxiv.org/abs/2505.23732)
*Shreeram Suresh Chandra,Lucas Goncalves,Junchen Lu,Carlos Busso,Berrak Sisman*

Key words: 情感建模,对比学习,跨模态对齐,Rank-N-Contrast,效价-唤醒空间

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了EmotionRankCLAP，一种通过对比学习改进跨模态对齐的方法，能够更好地捕捉情感的序数性质。

Motivation: 现有的基于情感的对比语言-音频预训练方法通过简单对齐音频样本和文本提示，无法捕捉情感的序数性质，导致跨模态理解不足和模态间差距较大。

Method: 引入了EmotionRankCLAP，利用情感语音和自然语言提示的维度属性，通过Rank-N-Contrast目标在效价-唤醒空间中对比样本排名，学习有序关系。

Result: EmotionRankCLAP在跨模态检索任务中优于现有方法，能够更好地建模情感的序数性质。

Conclusion: EmotionRankCLAP通过改进跨模态对齐和情感序数性质建模，提升了性能。

Abstract: Current emotion-based contrastive language-audio pretraining (CLAP) methods
typically learn by na\"ively aligning audio samples with corresponding text
prompts. Consequently, this approach fails to capture the ordinal nature of
emotions, hindering inter-emotion understanding and often resulting in a wide
modality gap between the audio and text embeddings due to insufficient
alignment. To handle these drawbacks, we introduce EmotionRankCLAP, a
supervised contrastive learning approach that uses dimensional attributes of
emotional speech and natural language prompts to jointly capture fine-grained
emotion variations and improve cross-modal alignment. Our approach utilizes a
Rank-N-Contrast objective to learn ordered relationships by contrasting samples
based on their rankings in the valence-arousal space. EmotionRankCLAP
outperforms existing emotion-CLAP methods in modeling emotion ordinality across
modalities, measured via a cross-modal retrieval task.

</details>


### [273] [Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?](https://arxiv.org/abs/2505.23749)
*Paul Gölz,Nika Haghtalab,Kunhe Yang*

Key words: 大型语言模型对齐、偏好模型、社会选择理论、Bradley-Terry模型、失真度

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了大型语言模型的对齐方法，指出现有方法（如RLHF和DPO）假设单一偏好模型，无法满足用户多样化需求，提出了基于社会选择理论和Bradley-Terry模型的新指标“失真度”，并比较了不同对齐方法的性能。

Motivation: 现有的基于成对比较的对齐方法（如RLHF和DPO）假设用户偏好单一，但在实际应用中用户偏好多样化。论文旨在解决如何评估和优化对齐方法，以确保其在不同用户偏好下的平均效用。

Method: 通过社会选择理论和个体Bradley-Terry（BT）模型建模用户偏好，引入“失真度”指标，评估不同对齐方法在最小化平均效用损失上的表现。特别对比了Nash Learning from Human Feedback、RLHF和DPO的性能。

Result: Nash Learning from Human Feedback在失真度上表现最优，在不同用户偏好和采样条件下达到最小化失真度（(1/2 + o(1))·β），而RLHF和DPO存在显著失真，尤其在KL约束下失真度可能无限增大。

Conclusion: 论文表明，现有对齐方法（如RLHF和DPO）在多样化用户偏好下表现不佳，而Nash Learning from Human Feedback在最小化失真度上具有鲁棒性，为对齐方法设计提供了新方向。

Abstract: After pre-training, large language models are aligned with human preferences
based on pairwise comparisons. State-of-the-art alignment methods (such as
PPO-based RLHF and DPO) are built on the assumption of aligning with a single
preference model, despite being deployed in settings where users have diverse
preferences. As a result, it is not even clear that these alignment methods
produce models that satisfy users on average -- a minimal requirement for
pluralistic alignment. Drawing on social choice theory and modeling users'
comparisons through individual Bradley-Terry (BT) models, we introduce an
alignment method's distortion: the worst-case ratio between the optimal
achievable average utility, and the average utility of the learned policy.
  The notion of distortion helps draw sharp distinctions between alignment
methods: Nash Learning from Human Feedback achieves the minimax optimal
distortion of $(\frac{1}{2} + o(1)) \cdot \beta$ (for the BT temperature
$\beta$), robustly across utility distributions, distributions of comparison
pairs, and permissible KL divergences from the reference policy. RLHF and DPO,
by contrast, suffer $\geq (1 - o(1)) \cdot \beta$ distortion already without a
KL constraint, and $e^{\Omega(\beta)}$ or even unbounded distortion in the full
setting, depending on how comparison pairs are sampled.

</details>


### [274] [REOrdering Patches Improves Vision Models](https://arxiv.org/abs/2505.23751)
*Declan Kutscher,David M. Chan,Yutong Bai,Trevor Darrell,Ritwik Gupta*

Key words: transformer, patch ordering, REOrder, Plackett-Luce, information-theoretic

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出REOrder框架，通过优化patch顺序提升transformer在视觉任务中的性能，包括两阶段方法：信息论压缩性评估和Plackett-Luce策略优化。

Motivation: 传统transformer在视觉任务中将图像按固定顺序（如行优先）展平为一维序列，但模块化设计导致对patch顺序敏感。论文探索patch顺序如何影响性能，并提出优化方案。

Method: 提出两阶段方法：1) 基于信息论评估patch序列的压缩性；2) 使用REINFORCE优化Plackett-Luce策略，学习最优排列组合。

Result: 在ImageNet-1K和Functional Map of the World上，REOrder分别提升Top-1准确率至3.01%和13.35%。

Conclusion: patch顺序对性能有显著影响，REOrder通过优化顺序有效提升模型表现。

Abstract: Sequence models such as transformers require inputs to be represented as
one-dimensional sequences. In vision, this typically involves flattening images
using a fixed row-major (raster-scan) order. While full self-attention is
permutation-equivariant, modern long-sequence transformers increasingly rely on
architectural approximations that break this invariance and introduce
sensitivity to patch ordering. We show that patch order significantly affects
model performance in such settings, with simple alternatives like column-major
or Hilbert curves yielding notable accuracy shifts. Motivated by this, we
propose REOrder, a two-stage framework for discovering task-optimal patch
orderings. First, we derive an information-theoretic prior by evaluating the
compressibility of various patch sequences. Then, we learn a policy over
permutations by optimizing a Plackett-Luce policy using REINFORCE. This
approach enables efficient learning in a combinatorial permutation space.
REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to
3.01% and Functional Map of the World by 13.35%.

</details>


### [275] [Model Immunization from a Condition Number Perspective](https://arxiv.org/abs/2505.23760)
*Amber Yijia Zheng,Cedar Site Bai,Brian Bullins,Raymond A. Yeh*

Key words: 模型免疫化, Hessian矩阵, 条件数, 正则化, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本研究通过Hessian矩阵的条件数分析模型免疫化，提出了一种算法来控制预训练后的条件数，验证了在线性模型和深度网络中的有效性。

Motivation: 模型免疫化的目的是在预训练时让模型难以在有害任务上微调，同时保留其在无害任务上的实用性，但现有研究对免疫化的条件和定义尚不明确。

Method: 基于Hessian矩阵的条件数提出理论框架，设计带有正则化项的算法来控制预训练后的条件数。

Result: 在线性模型和非线性深度网络上的实验验证了算法的有效性。

Conclusion: 提出的框架和算法为模型免疫化提供了理论支持和实用工具。

Abstract: Model immunization aims to pre-train models that are difficult to fine-tune
on harmful tasks while retaining their utility on other non-harmful tasks.
Though prior work has shown empirical evidence for immunizing text-to-image
models, the key understanding of when immunization is possible and a precise
definition of an immunized model remain unclear. In this work, we propose a
framework, based on the condition number of a Hessian matrix, to analyze model
immunization for linear models. Building on this framework, we design an
algorithm with regularization terms to control the resulting condition numbers
after pre-training. Empirical results on linear models and non-linear deep-nets
demonstrate the effectiveness of the proposed algorithm on model immunization.
The code is available at
https://github.com/amberyzheng/model-immunization-cond-num.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [276] [Design and testing of an agent chatbot supporting decision making with public transport data](https://arxiv.org/abs/2505.22698)
*Luca Fantin,Marco Antonelli,Margherita Cesetti,Daniele Irto,Bruno Zamengo,Francesco Silvestri*

Key words: 公共交通, 聊天机器人, 数据查询, 可视化, 性能评估

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种基于代理架构的聊天机器人，用于简化公共交通服务质量评估的数据查询和可视化任务，并通过生成数据集评估系统性能。

Motivation: 为提高公共交通服务质量评估的效率，简化复杂数据查询和可视化过程，使非技术用户也能轻松操作。

Method: 采用代理架构扩展核心大语言模型功能，集成SQL查询、数据绘图和地图生成等工具，并通过工作流收集性能评估数据。

Result: 开发出功能强大的聊天机器人，能够高效处理公共交通数据，并生成用于系统性能评估的数据集。

Conclusion: 该聊天机器人有效提升了公共交通服务质量评估的便捷性和效率，同时解决了生成式AI项目中数据收集的难题。

Abstract: Assessing the quality of public transportation services requires the analysis
of large quantities of data on the scheduled and actual trips and documents
listing the quality constraints each service needs to meet. Interrogating such
datasets with SQL queries, organizing and visualizing the data can be quite
complex for most users. This paper presents a chatbot offering a user-friendly
tool to interact with these datasets and support decision making. It is based
on an agent architecture, which expands the capabilities of the core Large
Language Model (LLM) by allowing it to interact with a series of tools that can
execute several tasks, like performing SQL queries, plotting data and creating
maps from the coordinates of a trip and its stops. This paper also tackles one
of the main open problems of such Generative AI projects: collecting data to
measure the system's performance. Our chatbot has been extensively tested with
a workflow that asks several questions and stores the generated query, the
retrieved data and the natural language response for each of them. Such
questions are drawn from a set of base examples which are then completed with
actual data from the database. This procedure yields a dataset for the
evaluation of the chatbot's performance, especially the consistency of its
answers and the correctness of the generated queries.

</details>


### [277] [Enhancing Lifelong Multi-Agent Path-finding by Using Artificial Potential Fields](https://arxiv.org/abs/2505.22753)
*Arseniy Pertzovsky,Roni Stern,Ariel Felner,Roie Zivan*

Key words: Artificial Potential Fields, Multi-Agent Path Finding, Lifelong MAPF, path planning, throughput improvement

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文探索了人工势场（APFs）在多智能体路径规划（MAPF）及其持续版本（LMAPF）中的应用，发现APFs对LMAPF更有效但MAPF无显著收益。

Motivation: 研究APFs在MAPF和LMAPF中的应用潜力，以提升智能体路径规划的效率和系统吞吐量。

Method: 将APFs整合到多种MAPF算法中，包括优先级规划、MAPF-LNS2和优先级继承回溯（PIBT）。

Result: 实验表明APFs对MAPF无显著益处，但能使LMAPF的系统吞吐量提升高达7倍。

Conclusion: APFs在LMAPF中表现优异，但在传统MAPF中效果有限。

Abstract: We explore the use of Artificial Potential Fields (APFs) to solve Multi-Agent
Path Finding (MAPF) and Lifelong MAPF (LMAPF) problems. In MAPF, a team of
agents must move to their goal locations without collisions, whereas in LMAPF,
new goals are generated upon arrival. We propose methods for incorporating APFs
in a range of MAPF algorithms, including Prioritized Planning, MAPF-LNS2, and
Priority Inheritance with Backtracking (PIBT). Experimental results show that
using APF is not beneficial for MAPF but yields up to a 7-fold increase in
overall system throughput for LMAPF.

</details>


### [278] [Decomposing Elements of Problem Solving: What "Math" Does RL Teach?](https://arxiv.org/abs/2505.22756)
*Tian Qin,Core Francisco Park,Mujin Kwun,Aaron Walsman,Eran Malach,Nikhil Anand,Hidenori Tanaka,David Alvarez-Melis*

Key words: 数学推理, 强化学习, GRPO, 能力分解, 覆盖限制

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文探讨了强化学习（如GRPO）提升大语言模型数学推理能力的机制，发现其增强了执行技能但受限于规划能力，通过实验提出了改进方向。

Motivation: 旨在深入理解强化学习如何增强大语言模型的推理能力，并揭示其在数学推理任务中的局限性。

Method: 将问题解决分解为Plan、Execute、Verify三个基础能力，并通过合成任务验证强化学习的影响。

Result: 发现GRPO提升模型执行已知问题的能力，但面对新问题时因规划不足遇到'coverage wall'；进一步实验表明可通过改进探索和泛化克服这一障碍。

Conclusion: 强化学习主要增强执行稳健性，但需提升规划能力以突破覆盖限制，未来研究应关注探索与泛化优化。

Abstract: Mathematical reasoning tasks have become prominent benchmarks for assessing
the reasoning capabilities of LLMs, especially with reinforcement learning (RL)
methods such as GRPO showing significant performance gains. However, accuracy
metrics alone do not support fine-grained assessment of capabilities and fail
to reveal which problem-solving skills have been internalized. To better
understand these capabilities, we propose to decompose problem solving into
fundamental capabilities: Plan (mapping questions to sequences of steps),
Execute (correctly performing solution steps), and Verify (identifying the
correctness of a solution). Empirically, we find that GRPO mainly enhances the
execution skill-improving execution robustness on problems the model already
knows how to solve-a phenomenon we call temperature distillation. More
importantly, we show that RL-trained models struggle with fundamentally new
problems, hitting a 'coverage wall' due to insufficient planning skills. To
explore RL's impact more deeply, we construct a minimal, synthetic
solution-tree navigation task as an analogy for mathematical problem-solving.
This controlled setup replicates our empirical findings, confirming RL
primarily boosts execution robustness. Importantly, in this setting, we
identify conditions under which RL can potentially overcome the coverage wall
through improved exploration and generalization to new solution paths. Our
findings provide insights into the role of RL in enhancing LLM reasoning,
expose key limitations, and suggest a path toward overcoming these barriers.
Code is available at https://github.com/cfpark00/RL-Wall.

</details>


### [279] [Predicting Human Depression with Hybrid Data Acquisition utilizing Physical Activity Sensing and Social Media Feeds](https://arxiv.org/abs/2505.22779)
*Mohammad Helal Uddin,Sabur Baidya*

Key words: 抑郁监测、智能手机传感器、社交媒体分析、深度学习、支持向量机

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该研究提出了一种混合方法，利用智能手机传感器数据（测量日常体力活动）和社交媒体（Twitter）互动分析来评估个体的抑郁水平。通过CNN深度学习模型和朴素贝叶斯分类，准确识别体力活动并分类用户情感。结果显示，基于SVM的抑郁分类模型准确率达94%，优于其他模型。

Motivation: 全球心理健康问题（如抑郁、焦虑等）日益严重，尤其是社交回避倾向人群。研究旨在通过非侵入式方法（智能手机和社交媒体数据）监测抑郁程度，兼顾隐私保护。

Method: 结合CNN模型（体力活动识别）和朴素贝叶斯分类（Twitter情感分析），提取9个特征（6个来自活动数据，3个来自情感分析），并使用SVM算法分类抑郁严重程度。

Result: 体力活动识别准确率95%，情感分析准确率95.6%。SVM模型抑郁分类准确率94%，显著优于MLP和KNN模型。部分体力活动特征与抑郁症状严重程度显著相关。

Conclusion: 该混合方法简单高效，长期监测抑郁效果突出，同时保护个人隐私，为心理健康监测提供了新思路。

Abstract: Mental disorders including depression, anxiety, and other neurological
disorders pose a significant global challenge, particularly among individuals
exhibiting social avoidance tendencies. This study proposes a hybrid approach
by leveraging smartphone sensor data measuring daily physical activities and
analyzing their social media (Twitter) interactions for evaluating an
individual's depression level. Using CNN-based deep learning models and Naive
Bayes classification, we identify human physical activities accurately and also
classify the user sentiments. A total of 33 participants were recruited for
data acquisition, and nine relevant features were extracted from the physical
activities and analyzed with their weekly depression scores, evaluated using
the Geriatric Depression Scale (GDS) questionnaire. Of the nine features, six
are derived from physical activities, achieving an activity recognition
accuracy of 95%, while three features stem from sentiment analysis of Twitter
activities, yielding a sentiment analysis accuracy of 95.6%. Notably, several
physical activity features exhibited significant correlations with the severity
of depression symptoms. For classifying the depression severity, a support
vector machine (SVM)-based algorithm is employed that demonstrated a very high
accuracy of 94%, outperforming alternative models, e.g., the multilayer
perceptron (MLP) and k-nearest neighbor. It is a simple approach yet highly
effective in the long run for monitoring depression without breaching personal
privacy.

</details>


### [280] [The WHY in Business Processes: Unification of Causal Process Models](https://arxiv.org/abs/2505.22871)
*Yuval David,Fabiana Fournier,Lior Limonad,Inna Skarbovsky*

Key words: 因果推理,业务流程,事件日志,因果流程模型,开源实现

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种新方法，将多个因果流程变体统一为一致模型，保留了原始因果模型的正确性，并明确表示了它们的因果流交替。该方法在三个开放和两个专有数据集上进行了评估，并开源实现。

Motivation: 因果推理对于业务流程干预和改进至关重要，需要清晰理解事件日志中活动执行时间的因果关系。现有方法无法捕捉多个变体间的交替因果条件，导致处理缺失值和表达交替条件的挑战。

Method: 提出了一种新方法，正式定义并证明了该方法，能够统一多个因果流程变体为一个一致模型，明确表示因果流交替。

Result: 在三个开放和两个专有数据集上评估了该方法，证明了其有效性，并开源了实现。

Conclusion: 该方法成功解决了多因果流程变体的统一问题，保留了原始模型的正确性并明确表示交替条件，具有实际应用价值。

Abstract: Causal reasoning is essential for business process interventions and
improvement, requiring a clear understanding of causal relationships among
activity execution times in an event log. Recent work introduced a method for
discovering causal process models but lacked the ability to capture alternating
causal conditions across multiple variants. This raises the challenges of
handling missing values and expressing the alternating conditions among log
splits when blending traces with varying activities.
  We propose a novel method to unify multiple causal process variants into a
consistent model that preserves the correctness of the original causal models,
while explicitly representing their causal-flow alternations. The method is
formally defined, proved, evaluated on three open and two proprietary datasets,
and released as an open-source implementation.

</details>


### [281] [Enhancing Study-Level Inference from Clinical Trial Papers via RL-based Numeric Reasoning](https://arxiv.org/abs/2505.22928)
*Massimiliano Pronesti,Michela Lorandi,Paul Flanagan,Oisin Redmon,Anya Belz,Yufang Hou*

Key words: 定量推理, 医学系统综述, 数值提取, 强化学习, CochraneForest

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种基于定量推理的方法来自动化医学系统综述中的证据合成，通过提取结构化数值证据并应用领域知识逻辑，显著提高了结论推断的准确性和可解释性。

Motivation: 自动提取医学系统综述中的数值证据并推断结论时，现有方法依赖浅层文本线索，无法捕捉专家评估背后的数值推理。论文旨在通过定量推理解决这一问题。

Method: 开发了包含数值数据提取模型和效应估计组件的数值推理系统，采用监督微调（SFT）和强化学习（RL）策略训练模型。

Result: 在CochraneForest基准测试中，RL训练的小规模数值提取模型比检索基系统F1分数绝对提升21%，且优于400B参数通用大模型9%。

Conclusion: 研究表明，推理驱动的方法在自动化系统证据合成中具有显著潜力。

Abstract: Systematic reviews in medicine play a critical role in evidence-based
decision-making by aggregating findings from multiple studies. A central
bottleneck in automating this process is extracting numeric evidence and
determining study-level conclusions for specific outcomes and comparisons.
Prior work has framed this problem as a textual inference task by retrieving
relevant content fragments and inferring conclusions from them. However, such
approaches often rely on shallow textual cues and fail to capture the
underlying numeric reasoning behind expert assessments.
  In this work, we conceptualise the problem as one of quantitative reasoning.
Rather than inferring conclusions from surface text, we extract structured
numerical evidence (e.g., event counts or standard deviations) and apply domain
knowledge informed logic to derive outcome-specific conclusions. We develop a
numeric reasoning system composed of a numeric data extraction model and an
effect estimate component, enabling more accurate and interpretable inference
aligned with the domain expert principles. We train the numeric data extraction
model using different strategies, including supervised fine-tuning (SFT) and
reinforcement learning (RL) with a new value reward model.
  When evaluated on the CochraneForest benchmark, our best-performing approach
-- using RL to train a small-scale number extraction model -- yields up to a
21% absolute improvement in F1 score over retrieval-based systems and
outperforms general-purpose LLMs of over 400B parameters by up to 9%. Our
results demonstrate the promise of reasoning-driven approaches for automating
systematic evidence synthesis.

</details>


### [282] [Foundation Molecular Grammar: Multi-Modal Foundation Models Induce Interpretable Molecular Graph Languages](https://arxiv.org/abs/2505.22948)
*Michael Sun,Weize Yuan,Gang Liu,Wojciech Matusik,Jie Chen*

Key words: 分子生成、多模态基础模型、可解释性、语法学习

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该方法利用多模态基础模型学习可解释的分子语言，替代传统依赖专家注释的语法学习方法，在分子生成和属性预测中表现优异。

Motivation: 现有分子生成方法依赖专家注释或不可靠启发式算法，缺乏可解释性，因此提出基于多模态基础模型的解决方案。

Method: 利用多模态基础模型（MMFM）通过图像、文本表示分子，并通过提示学习跨模态对齐信息。

Result: FMG在合成性、多样性和数据效率上表现优异，并提供化学可解释性。

Conclusion: FMG可替代传统语法学习方法，为分子发现工作流提供内置化学可解释性。

Abstract: Recent data-efficient molecular generation approaches exploit graph grammars
to introduce interpretability into the generative models. However, grammar
learning therein relies on expert annotation or unreliable heuristics for
algorithmic inference. We propose Foundation Molecular Grammar (FMG), which
leverages multi-modal foundation models (MMFMs) to induce an interpretable
molecular language. By exploiting the chemical knowledge of an MMFM, FMG
renders molecules as images, describes them as text, and aligns information
across modalities using prompt learning. FMG can be used as a drop-in
replacement for the prior grammar learning approaches in molecular generation
and property prediction. We show that FMG not only excels in synthesizability,
diversity, and data efficiency but also offers built-in chemical
interpretability for automated molecular discovery workflows. Code is available
at https://github.com/shiningsunnyday/induction.

</details>


### [283] [Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents](https://arxiv.org/abs/2505.22954)
*Jenny Zhang,Shengran Hu,Cong Lu,Robert Lange,Jeff Clune*

Key words: 达尔文哥德尔机, 自我改进AI, 代码基准, 开放式探索

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 达尔文哥德尔机（DGM）是一种自我改进的AI系统，通过迭代修改自身代码并实证验证，实现了在编程任务上的显著性能提升。

Motivation: 当前AI系统的架构固定且无法自主改进，限制了AI的发展速度和潜力。通过自动化AI的进步，可以加速其发展并更快实现其价值。

Method: DGM结合了达尔文进化和开放式研究，通过维护一个生成编码代理的档案，不断探索和改进，利用基础模型创造新的代理版本。

Result: DGM在SWE-bench和Polyglot上的性能分别从20.0%提升至50.0%和从14.2%提升至30.7%，显著优于没有自我改进或开放式探索的基线。

Conclusion: DGM是实现自我改进AI的重要一步，能够持续创新并提升性能。

Abstract: Today's AI systems have human-designed, fixed architectures and cannot
autonomously and continuously improve themselves. The advance of AI could
itself be automated. If done safely, that would accelerate AI development and
allow us to reap its benefits much sooner. Meta-learning can automate the
discovery of novel algorithms, but is limited by first-order improvements and
the human design of a suitable search space. The G\"odel machine proposed a
theoretical alternative: a self-improving AI that repeatedly modifies itself in
a provably beneficial manner. Unfortunately, proving that most changes are net
beneficial is impossible in practice. We introduce the Darwin G\"odel Machine
(DGM), a self-improving system that iteratively modifies its own code (thereby
also improving its ability to modify its own codebase) and empirically
validates each change using coding benchmarks. Inspired by Darwinian evolution
and open-endedness research, the DGM maintains an archive of generated coding
agents. It grows the archive by sampling an agent from it and using a
foundation model to create a new, interesting, version of the sampled agent.
This open-ended exploration forms a growing tree of diverse, high-quality
agents and allows the parallel exploration of many different paths through the
search space. Empirically, the DGM automatically improves its coding
capabilities (e.g., better code editing tools, long-context window management,
peer-review mechanisms), increasing performance on SWE-bench from 20.0% to
50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly
outperforms baselines without self-improvement or open-ended exploration. All
experiments were done with safety precautions (e.g., sandboxing, human
oversight). The DGM is a significant step toward self-improving AI, capable of
gathering its own stepping stones along paths that unfold into endless
innovation.

</details>


### [284] [Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness](https://arxiv.org/abs/2505.22960)
*Yongjin Yang,Euiin Yi,Jongwoo Ko,Kimin Lee,Zhijing Jin,Se-Young Yun*

Key words: 多智能体辩论, 大语言模型, 数学推理, 安全任务, 测试时扩展

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文探讨了多智能体辩论（MAD）框架在数学推理与安全相关任务中的效果，发现其在不同条件下的表现差异显著。

Motivation: 尽管多智能体辩论框架被广泛探索，但其与单智能体方法在多样化条件下的系统比较仍缺乏深入研究，本文旨在填补这一空白。

Method: 研究将MAD概念化为一种测试时计算扩展技术，并通过实验比较其在数学推理和安全任务中的表现，分析任务难度、模型规模和智能体多样性的影响。

Result: 数学推理中，MAD仅在问题难度高或模型能力低时表现优于单智能体；安全任务中，MAD可能增加漏洞风险，但多样性配置可通过协作逐步降低攻击成功率。

Conclusion: 该研究为未来如何更有效和战略性部署MAD系统提供了关键指导。

Abstract: The remarkable growth in large language model (LLM) capabilities has spurred
exploration into multi-agent systems, with debate frameworks emerging as a
promising avenue for enhanced problem-solving. These multi-agent debate (MAD)
approaches, where agents collaboratively present, critique, and refine
arguments, potentially offer improved reasoning, robustness, and diverse
perspectives over monolithic models. Despite prior studies leveraging MAD, a
systematic understanding of its effectiveness compared to self-agent methods,
particularly under varying conditions, remains elusive. This paper seeks to
fill this gap by conceptualizing MAD as a test-time computational scaling
technique, distinguished by collaborative refinement and diverse exploration
capabilities. We conduct a comprehensive empirical investigation comparing MAD
with strong self-agent test-time scaling baselines on mathematical reasoning
and safety-related tasks. Our study systematically examines the influence of
task difficulty, model scale, and agent diversity on MAD's performance. Key
findings reveal that, for mathematical reasoning, MAD offers limited advantages
over self-agent scaling but becomes more effective with increased problem
difficulty and decreased model capability, while agent diversity shows little
benefit. Conversely, for safety tasks, MAD's collaborative refinement can
increase vulnerability, but incorporating diverse agent configurations
facilitates a gradual reduction in attack success through the collaborative
refinement process. We believe our findings provide critical guidance for the
future development of more effective and strategically deployed MAD systems.

</details>


### [285] [Strategic Reflectivism In Intelligent Systems](https://arxiv.org/abs/2505.22987)
*Nick Byrd*

Key words: 双过程理论, 战略反思主义, 有限理性, 实用主义, 人工智能

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了'战略反思主义'，强调智能系统（人或AI）需在直觉与反思推理间灵活切换以实现目标，融合了实用主义哲学与最新实验发现。

Motivation: 20世纪末的理性争论及21世纪双过程理论在AI中的应用推动了本文研究，旨在结合历史观点与最新实验，提出更优的智能系统设计方法。

Method: 综合历史理论（如有限理性）与人类/机器实验数据，构建'战略反思主义'框架。

Result: '战略反思主义'超越了模型规模等表面指标，强调直觉与反思的实用切换对智能系统的关键作用。

Conclusion: 随着对直觉与反思价值的深入理解，该理论将更具可操作性，为AI设计提供新方向。

Abstract: By late 20th century, the rationality wars had launched debates about the
nature and norms of intuitive and reflective thinking. Those debates drew from
mid-20th century ideas such as bounded rationality, which challenged more
idealized notions of rationality observed since the 19th century. Now that 21st
century cognitive scientists are applying the resulting dual process theories
to artificial intelligence, it is time to dust off some lessons from this
history. So this paper synthesizes old ideas with recent results from
experiments on humans and machines. The result is Strategic Reflectivism, which
takes the position that one key to intelligent systems (human or artificial) is
pragmatic switching between intuitive and reflective inference to optimally
fulfill competing goals. Strategic Reflectivism builds on American Pragmatism,
transcends superficial indicators of reflective thinking such as model size or
chains of thought, and becomes increasingly actionable as we learn more about
the value of intuition and reflection.

</details>


### [286] [MenTeR: A fully-automated Multi-agenT workflow for end-to-end RF/Analog Circuits Netlist Design](https://arxiv.org/abs/2505.22990)
*Pin-Han Chen,Yu-Sheng Lin,Wei-Cheng Lee,Tin-Yu Leu,Po-Hsiang Hsu,Anjana Dissanayake,Sungjin Oh,Chinq-Shiun Chiu*

Key words: RF/Analog 设计, 多智能体, AI, 电路优化, 测试台验证

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: MenTeR 是一种多智能体工作流，集成到端到端模拟设计框架中，通过多个专业 AI 智能体协作处理设计过程，减少对反复试错的依赖，加速设计周期并拓展设计空间探索。

Motivation: 模拟设计过程复杂、耗时且依赖专家直觉，阻碍电路开发的效率和成本。MenTeR 旨在克服手动设计的局限性，提供更高效的设计解决方案。

Method: 引入 MenTeR，一个多智能体工作流，通过多个专业 AI 智能体协作处理设计流程的不同方面，如规格理解、电路优化和测试台验证。

Result: MenTeR 加速了设计周期时间，拓展了设计空间探索，展示了处理真实世界模拟系统的强大能力。

Conclusion: MenTeR 为未来与人类设计师无缝协作的 'RF/Analog Copilots' 奠定了基础。

Abstract: RF/Analog design is essential for bridging digital technologies with
real-world signals, ensuring the functionality and reliability of a wide range
of electronic systems. However, analog design procedures are often intricate,
time-consuming and reliant on expert intuition, and hinder the time and cost
efficiency of circuit development. To overcome the limitations of the manual
circuit design, we introduce MenTeR - a multiagent workflow integrated into an
end-to-end analog design framework. By employing multiple specialized AI agents
that collaboratively address different aspects of the design process, such as
specification understanding, circuit optimization, and test bench validation,
MenTeR reduces the dependency on frequent trial-and-error-style intervention.
MenTeR not only accelerates the design cycle time but also facilitates a
broader exploration of the design space, demonstrating robust capabilities in
handling real-world analog systems. We believe that MenTeR lays the groundwork
for future "RF/Analog Copilots" that can collaborate seamlessly with human
designers.

</details>


### [287] [Case-Based Reasoning Enhances the Predictive Power of LLMs in Drug-Drug Interaction](https://arxiv.org/abs/2505.23034)
*Guangyi Liu,Yongqi Zhang,Xunyuan Liu,Quanming Yao*

Key words: 药物相互作用预测、案例推理、大型语言模型、图神经网络、知识增强

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: CBR-DDI是一个新颖框架，结合案例推理和图神经网络，提升大型语言模型在药物相互作用预测中的表现，显著提高准确性。

Motivation: 药物相互作用预测对治疗安全至关重要，但现有大型语言模型在此任务中表现不佳。受临床实践中医生参考历史案例的启发，提出了CBR-DDI框架。

Method: 通过构建知识库（结合大型语言模型和图神经网络）、混合检索机制和双层知识增强提示，动态优化案例选择。

Result: 实验表明，CBR-DDI在准确性上显著优于基线方法（提升28.7%），同时保持高解释性和灵活性。

Conclusion: CBR-DDI通过案例推理和知识增强，显著提升了药物相互作用预测的性能，为临床决策提供了可靠工具。

Abstract: Drug-drug interaction (DDI) prediction is critical for treatment safety.
While large language models (LLMs) show promise in pharmaceutical tasks, their
effectiveness in DDI prediction remains challenging. Inspired by the
well-established clinical practice where physicians routinely reference similar
historical cases to guide their decisions through case-based reasoning (CBR),
we propose CBR-DDI, a novel framework that distills pharmacological principles
from historical cases to improve LLM reasoning for DDI tasks. CBR-DDI
constructs a knowledge repository by leveraging LLMs to extract pharmacological
insights and graph neural networks (GNNs) to model drug associations. A hybrid
retrieval mechanism and dual-layer knowledge-enhanced prompting allow LLMs to
effectively retrieve and reuse relevant cases. We further introduce a
representative sampling strategy for dynamic case refinement. Extensive
experiments demonstrate that CBR-DDI achieves state-of-the-art performance,
with a significant 28.7% accuracy improvement over both popular LLMs and CBR
baseline, while maintaining high interpretability and flexibility.

</details>


### [288] [Be.FM: Open Foundation Models for Human Behavior](https://arxiv.org/abs/2505.23058)
*Yutong Xie,Zhuoheng Li,Xiyuan Wang,Yijun Pan,Qijia Liu,Xingzhi Cui,Kuang-Yu Lo,Ruoyi Gao,Xingjian Zhang,Jin Huang,Walter Yuan,Matthew O. Jackson,Qiaozhu Mei*

Key words: 行为基础模型, 人类行为建模, 决策预测, 大型语言模型, 微调

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: Be.FM是一个基于开源大型语言模型的行为基础模型，旨在理解和预测人类决策行为，通过多样化行为数据微调，展现了在行为预测、个体及群体特征推断、情境洞察和行为科学知识应用方面的潜力。

Motivation: 探索基础模型在人类行为建模和理解中的潜力，填补现有研究中行为基础模型的空白。

Method: 基于开源大型语言模型，利用多样化行为数据进行微调，构建并测试基准任务集。

Result: Be.FM能够有效预测行为、推断个体和群体特征、生成情境洞察，并应用行为科学知识。

Conclusion: Be.FM展示了行为基础模型在多任务行为理解和预测中的实用性，为未来研究提供了新方向。

Abstract: Despite their success in numerous fields, the potential of foundation models
for modeling and understanding human behavior remains largely unexplored. We
introduce Be.FM, one of the first open foundation models designed for human
behavior modeling. Built upon open-source large language models and fine-tuned
on a diverse range of behavioral data, Be.FM can be used to understand and
predict human decision-making. We construct a comprehensive set of benchmark
tasks for testing the capabilities of behavioral foundation models. Our results
demonstrate that Be.FM can predict behaviors, infer characteristics of
individuals and populations, generate insights about contexts, and apply
behavioral science knowledge.

</details>


### [289] [Second Opinion Matters: Towards Adaptive Clinical AI via the Consensus of Expert Model Ensemble](https://arxiv.org/abs/2505.23075)
*Amit Kumthekar,Zion Tilley,Henry Duong,Bhargav Patel,Michael Magnoli,Ahmed Omar,Ahmed Nasser,Chaitanya Gharpure,Yevgen Reztzov*

Key words: large language models, clinical decision-making, consensus mechanism, medical evaluation benchmarks

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种名为“共识机制”的新框架，通过集成多个医学专家代理来提升临床决策的准确性和适应性，并在多个医学评估基准上验证了其优越性。

Motivation: 当前大型语言模型在临床应用中过度依赖单一架构，存在过时和依赖性强的风险。为了克服这些问题，研究团队提出了共识机制框架，以模拟临床会诊和多学科决策过程。

Method: 共识机制采用集成多个专业化医学专家代理的方式，可根据内部模型配置优化成本、延迟或性能。

Result: 在MedXpertQA、MedQA和MedMCQA等医学评估基准上，共识机制的准确率显著优于OpenAI和Google的模型，同时在DDX+数据集上展现了更高的召回率和精确度。

Conclusion: 共识机制通过集成多个专家代理显著提升了临床决策的准确性和适应性，为未来医学AI应用提供了更灵活的解决方案。

Abstract: Despite the growing clinical adoption of large language models (LLMs),
current approaches heavily rely on single model architectures. To overcome
risks of obsolescence and rigid dependence on single model systems, we present
a novel framework, termed the Consensus Mechanism. Mimicking clinical triage
and multidisciplinary clinical decision-making, the Consensus Mechanism
implements an ensemble of specialized medical expert agents enabling improved
clinical decision making while maintaining robust adaptability. This
architecture enables the Consensus Mechanism to be optimized for cost, latency,
or performance, purely based on its interior model configuration.
  To rigorously evaluate the Consensus Mechanism, we employed three medical
evaluation benchmarks: MedMCQA, MedQA, and MedXpertQA Text, and the
differential diagnosis dataset, DDX+. On MedXpertQA, the Consensus Mechanism
achieved an accuracy of 61.0% compared to 53.5% and 45.9% for OpenAI's O3 and
Google's Gemini 2.5 Pro. Improvement was consistent across benchmarks with an
increase in accuracy on MedQA
($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 3.4\%$) and MedMCQA
($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 9.1\%$). These
accuracy gains extended to differential diagnosis generation, where our system
demonstrated improved recall and precision (F1$_\mathrm{consensus}$ = 0.326 vs.
F1$_{\mathrm{O3\text{-}high}}$ = 0.2886) and a higher top-1 accuracy for DDX
(Top1$_\mathrm{consensus}$ = 52.0% vs. Top1$_{\mathrm{O3\text{-}high}}$ =
45.2%).

</details>


### [290] [Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models](https://arxiv.org/abs/2505.23091)
*Zeyu Liu,Yuhang Liu,Guanghao Zhu,Congkai Xie,Zhen Li,Jianbo Yuan,Xinyao Wang,Qing Li,Shing-Chi Cheung,Shengyu Zhang,Fei Wu,Hongxia Yang*

Key words: 多模态推理，小语言模型，课程学习，数学推理，跨模态迁移

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出Infi-MMR框架，通过三阶段课程设计提升多模态小语言模型(MSLMs)的推理能力，解决了数据集稀缺、视觉处理导致推理能力退化等问题，最终模型Infi-MMR-3B在多模态数学推理和通用推理任务上达到SOTA性能。

Motivation: 尽管大语言模型(LLMs)在推理能力上取得进展，但多模态小语言模型(MSLMs)面临高质量多模态数据集稀缺、视觉整合削弱推理能力、强化学习可能生成错误推理等挑战，需系统性解决方案。

Method: 提出三阶段框架Infi-MMR：(1)基础推理激活阶段利用文本数据集强化逻辑推理能力；(2)跨模态推理适应阶段通过字幕增强多模态数据迁移推理技能；(3)多模态推理增强阶段使用无字幕数据减少语言偏向，提升跨模态推理鲁棒性。

Result: Infi-MMR-3B在多模态数学推理任务(MathVerse/MathVision/OlympiadBench)和通用推理任务(MathVista)上分别达到43.68%/27.04%/21.33%和67.2%的SOTA性能。

Conclusion: Infi-MMR框架通过分阶段训练有效解决MSLMs的推理瓶颈，模型在多种任务上表现优异，为多模态推理研究提供了新方向。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
substantial progress in reasoning capabilities, such as DeepSeek-R1, which
leverages rule-based reinforcement learning to enhance logical reasoning
significantly. However, extending these achievements to multimodal large
language models (MLLMs) presents critical challenges, which are frequently more
pronounced for Multimodal Small Language Models (MSLMs) given their typically
weaker foundational reasoning abilities: (1) the scarcity of high-quality
multimodal reasoning datasets, (2) the degradation of reasoning capabilities
due to the integration of visual processing, and (3) the risk that direct
application of reinforcement learning may produce complex yet incorrect
reasoning processes. To address these challenges, we design a novel framework
Infi-MMR to systematically unlock the reasoning potential of MSLMs through a
curriculum of three carefully structured phases and propose our multimodal
reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning
Activation, leverages high-quality textual reasoning datasets to activate and
strengthen the model's logical reasoning capabilities. The second phase,
Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to
facilitate the progressive transfer of reasoning skills to multimodal contexts.
The third phase, Multimodal Reasoning Enhancement, employs curated,
caption-free multimodal data to mitigate linguistic biases and promote robust
cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal
math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision
test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on
MathVista testmini).

</details>


### [291] [Conceptual Framework Toward Embodied Collective Adaptive Intelligence](https://arxiv.org/abs/2505.23153)
*Fan Wang,Shaoshan Liu*

Key words: 集体自适应智能，自主代理，动态环境，任务泛化，弹性

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 这篇论文提出了集体自适应智能（CAI）的概念框架，强调多自主代理的协作与自组织，旨在实现复杂动态环境下的鲁棒性和适应性。

Motivation: 旨在解决现实场景中AI系统在动态和不可预见挑战下的适应性和弹性问题。

Method: 提出了一个设计分析CAI的概念框架，涵盖任务泛化、弹性、可扩展性和自组装等关键属性。

Result: 提供了一个结构化基础，帮助理解和实现CAI，促进更具韧性和可扩展性的AI系统开发。

Conclusion: CAI框架为理论和实践结合提供了桥梁，有望推动跨领域的自适应智能系统发展。

Abstract: Collective Adaptive Intelligence (CAI) represent a transformative approach in
artificial intelligence, wherein numerous autonomous agents collaborate, adapt,
and self-organize to navigate complex, dynamic environments. This paradigm is
particularly impactful in embodied AI applications, where adaptability and
resilience are paramount. By enabling systems to reconfigure themselves in
response to unforeseen challenges, CAI facilitate robust performance in
real-world scenarios. This article introduces a conceptual framework for
designing and analyzing CAI. It delineates key attributes including task
generalization, resilience, scalability, and self-assembly, aiming to bridge
theoretical foundations with practical methodologies for engineering adaptive,
emergent intelligence. By providing a structured foundation for understanding
and implementing CAI, this work seeks to guide researchers and practitioners in
developing more resilient, scalable, and adaptable AI systems across various
domains.

</details>


### [292] [MathArena: Evaluating LLMs on Uncontaminated Math Competitions](https://arxiv.org/abs/2505.23281)
*Mislav Balunović,Jasper Dekoninck,Ivo Petrov,Nikola Jovanović,Martin Vechev*

Key words: MathArena, LLMs, 数学推理, 证明写作, 数据污染

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: MathArena是一个新的数学推理评估基准，通过实时使用数学竞赛题目来避免数据污染，同时首次评测模型的证明写作能力。

Motivation: 当前数学评估数据集（如AIME 2024）存在数据污染问题，且缺乏对证明写作能力的评估，因此需要一个新的基准。

Method: 利用数学竞赛（如SMT 2025和USAMO 2025）的实时题目构建MathArena基准，评估模型的推理和证明能力。

Result: 在AIME 2024中发现数据污染，但在SMT 2025中模型展现出强推理能力；USAMO 2025上顶尖模型的证明能力得分低于25%。

Conclusion: MathArena为数学推理提供了更严格的评估标准，未来将动态跟踪模型在新竞赛中的表现。

Abstract: The rapid advancement of reasoning capabilities in large language models
(LLMs) has led to notable improvements on mathematical benchmarks. However,
many of the most commonly used evaluation datasets (e.g., AIME 2024) are widely
available online, making it difficult to disentangle genuine reasoning from
potential memorization. Furthermore, these benchmarks do not evaluate
proof-writing capabilities, which are crucial for many mathematical tasks. To
address this, we introduce MathArena, a new benchmark based on the following
key insight: recurring math competitions provide a stream of high-quality,
challenging problems that can be used for real-time evaluation of LLMs. By
evaluating models as soon as new problems are released, we effectively
eliminate the risk of contamination. Using this framework, we find strong signs
of contamination in AIME 2024. Nonetheless, evaluations on harder competitions,
such as SMT 2025 -- published well after model release dates -- demonstrate
impressive reasoning capabilities in top-performing models. MathArena is also
the first benchmark for proof-writing capabilities. On USAMO 2025, even top
models score below 25%, far behind their performance on final-answer tasks. So
far, we have evaluated 30 models across five competitions, totaling 149
problems. As an evolving benchmark, MathArena will continue to track the
progress of LLMs on newly released competitions, ensuring rigorous and
up-to-date evaluation of mathematical reasoning.

</details>


### [293] [AutoGPS: Automated Geometry Problem Solving via Multimodal Formalization and Deductive Reasoning](https://arxiv.org/abs/2505.23381)
*Bowen Ping,Minnan Luo,Zhuohang Dang,Chenxi Wang,Chengyou Jia*

Key words: 几何问题，神经符号协同，多模态理解，可解释性

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: AutoGPS是一个结合神经与符号方法的框架，用于解决几何问题，具有高可靠性和可解释性。

Motivation: 现有神经或符号方法在解决几何问题时存在可靠性和可解释性不足的问题。

Method: 结合多模态问题形式化器和演绎符号推理器，实现结构化表示和严谨推导。

Result: 在基准数据集上达到最优性能，且推理步骤逻辑一致性高达99%。

Conclusion: AutoGPS通过神经与符号协同，显著提升了几何问题解决的可靠性和可解释性。

Abstract: Geometry problem solving presents distinctive challenges in artificial
intelligence, requiring exceptional multimodal comprehension and rigorous
mathematical reasoning capabilities. Existing approaches typically fall into
two categories: neural-based and symbolic-based methods, both of which exhibit
limitations in reliability and interpretability. To address this challenge, we
propose AutoGPS, a neuro-symbolic collaborative framework that solves geometry
problems with concise, reliable, and human-interpretable reasoning processes.
Specifically, AutoGPS employs a Multimodal Problem Formalizer (MPF) and a
Deductive Symbolic Reasoner (DSR). The MPF utilizes neural cross-modal
comprehension to translate geometry problems into structured formal language
representations, with feedback from DSR collaboratively. The DSR takes the
formalization as input and formulates geometry problem solving as a hypergraph
expansion task, executing mathematically rigorous and reliable derivation to
produce minimal and human-readable stepwise solutions. Extensive experimental
evaluations demonstrate that AutoGPS achieves state-of-the-art performance on
benchmark datasets. Furthermore, human stepwise-reasoning evaluation confirms
AutoGPS's impressive reliability and interpretability, with 99\% stepwise
logical coherence. The project homepage is at
https://jayce-ping.github.io/AutoGPS-homepage.

</details>


### [294] [A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy](https://arxiv.org/abs/2505.23397)
*Ahmad Mohsin,Helge Janicke,Ahmed Ibrahim,Iqbal H. Sarker,Seyit Camtepe*

Key words: 人机协作、安全运营中心、AI自主性、信任校准、HITL

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种结构化框架用于安全运营中心（SOC）中的人机协作，通过五级自主性、信任校准和人机角色分配，动态调整任务复杂度和风险，提升决策效率。

Motivation: 现有SOC框架大多关注自动化，缺乏系统性管理人类监督和信任校准的方法，且假设自主性是静态或二元的。本文旨在解决这些局限性。

Method: 提出一个基于五级AI自主性的新框架，结合HITL角色和任务特定信任阈值，并通过模拟网络靶场和AI助手案例验证。

Result: 框架在模拟环境中展示出减少警报疲劳、增强响应协调和信任校准的效果，验证了其理论和实践可行性。

Conclusion: 该框架为下一代认知SOC设计提供了理论和实践基础，强调AI增强而非取代人类决策。

Abstract: This article presents a structured framework for Human-AI collaboration in
Security Operations Centers (SOCs), integrating AI autonomy, trust calibration,
and Human-in-the-loop decision making. Existing frameworks in SOCs often focus
narrowly on automation, lacking systematic structures to manage human
oversight, trust calibration, and scalable autonomy with AI. Many assume static
or binary autonomy settings, failing to account for the varied complexity,
criticality, and risk across SOC tasks considering Humans and AI collaboration.
To address these limitations, we propose a novel autonomy tiered framework
grounded in five levels of AI autonomy from manual to fully autonomous, mapped
to Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This
enables adaptive and explainable AI integration across core SOC functions,
including monitoring, protection, threat detection, alert triage, and incident
response. The proposed framework differentiates itself from previous research
by creating formal connections between autonomy, trust, and HITL across various
SOC levels, which allows for adaptive task distribution according to
operational complexity and associated risks. The framework is exemplified
through a simulated cyber range that features the cybersecurity AI-Avatar, a
fine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates
human-AI collaboration for SOC tasks, reducing alert fatigue, enhancing
response coordination, and strategically calibrating trust. This research
systematically presents both the theoretical and practical aspects and
feasibility of designing next-generation cognitive SOCs that leverage AI not to
replace but to enhance human decision-making.

</details>


### [295] [GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning](https://arxiv.org/abs/2505.23399)
*Jusheng Zhang,Yijia Fan,Wenjun Lin,Ruiqi Chen,Haoyi Jiang,Wenhao Chai,Jian Wang,Keze Wang*

Key words: 多智能体，视觉语言推理，博弈论，不确定性感知，VLM模型

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了GAM-Agent框架，通过多智能体博弈增强视觉语言推理，显著提升多种VLM模型的性能。

Motivation: 现有单智能体或整体模型在视觉语言推理中存在局限性，GAM-Agent通过多智能体协作解决这一问题。

Method: 将推理过程建模为非零和博弈，引入不确定性感知控制器动态调整协作，触发多轮辩论以确保逻辑一致性。

Result: 在多个基准测试中显著提升性能，小型模型准确率提升5-6%，大型模型如GPT-4o提升2-3%。

Conclusion: GAM-Agent框架具有模块化、可扩展和通用性，为可靠且可解释的多模态推理提供了新思路。

Abstract: We propose GAM-Agent, a game-theoretic multi-agent framework for enhancing
vision-language reasoning. Unlike prior single-agent or monolithic models,
GAM-Agent formulates the reasoning process as a non-zero-sum game between base
agents--each specializing in visual perception subtasks--and a critical agent
that verifies logic consistency and factual correctness. Agents communicate via
structured claims, evidence, and uncertainty estimates. The framework
introduces an uncertainty-aware controller to dynamically adjust agent
collaboration, triggering multi-round debates when disagreement or ambiguity is
detected. This process yields more robust and interpretable predictions.
Experiments on four challenging benchmarks--MMMU, MMBench, MVBench, and
V*Bench--demonstrate that GAM-Agent significantly improves performance across
various VLM backbones. Notably, GAM-Agent boosts the accuracy of small-to-mid
scale models (e.g., Qwen2.5-VL-7B, InternVL3-14B) by 5--6\%, and still enhances
strong models like GPT-4o by up to 2--3\%. Our approach is modular, scalable,
and generalizable, offering a path toward reliable and explainable multi-agent
multimodal reasoning.

</details>


### [296] [A Mathematical Framework for AI-Human Integration in Work](https://arxiv.org/abs/2505.23432)
*Elisa Celis,Lingxiao Huang,Nisheeth K. Vishnoi*

Key words: 生成式AI, 技能分解, 工作互补性, 数学模型, 实证验证

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该研究提出了一个数学模型，将技能分解为决策级和行动级子技能，分析生成式AI（GenAI）与人类工作者如何互补而非替代，并通过实证数据验证了该框架的实用性。

Motivation: 探讨生成式AI（GenAI）工具在职场中是否替代或补充人类工作者的角色，以及如何通过技能分解理解其互补性。

Method: 构建数学框架，将技能分解为决策级和行动级子技能，分析子技能能力变化对工作成功的影响，并通过O*NET和Big-Bench Lite数据验证模型。

Result: 研究确定了子技能能力变化对工作成功概率的临界条件，并证明结合互补子技能的工作者组合显著优于单一工作者。

Conclusion: 生成式AI更适合作为人类技能的补充而非替代，尤其对低技能工作者的效率提升更为显著。

Abstract: The rapid rise of Generative AI (GenAI) tools has sparked debate over their
role in complementing or replacing human workers across job contexts. We
present a mathematical framework that models jobs, workers, and worker-job fit,
introducing a novel decomposition of skills into decision-level and
action-level subskills to reflect the complementary strengths of humans and
GenAI. We analyze how changes in subskill abilities affect job success,
identifying conditions for sharp transitions in success probability. We also
establish sufficient conditions under which combining workers with
complementary subskills significantly outperforms relying on a single worker.
This explains phenomena such as productivity compression, where GenAI
assistance yields larger gains for lower-skilled workers. We demonstrate the
framework' s practicality using data from O*NET and Big-Bench Lite, aligning
real-world data with our model via subskill-division methods. Our results
highlight when and how GenAI complements human skills, rather than replacing
them.

</details>


### [297] [Emergent Risk Awareness in Rational Agents under Resource Constraints](https://arxiv.org/abs/2505.23436)
*Daniel Jarne Ornia,Nicholas Bishop,Joel Dyer,Wei-Chen Lee,Ani Calinescu,Doyne Farme,Michael Wooldridge*

Key words: AI代理,生存压力,偏好偏移,行为不对齐,生存博弈框架

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文研究了具有代理能力的高级推理模型（AI代理）在资源或失败约束下的行为变化，重点关注因生存压力导致的偏好偏移及其与人类目标的不对齐，提出了理论和实证结果以及缓解机制。

Motivation: 研究AI代理在资源耗尽或强制终止行动序列的情况下的行为变化，揭示其与人类委托目标的潜在不对齐问题，旨在提高对此类AI系统在资源有限环境中的行为理解和部署安全性。

Method: 通过生存博弈框架（survival bandit framework）形式化问题，结合理论和实证方法量化生存驱动偏好偏移的影响，并提出缓解机制。

Result: 识别了风险寻求或规避行为出现的条件，提出了调整代理行为的机制以减少与人类目标的不对齐。

Conclusion: 研究为资源受限环境下安全部署AI系统提供指导，增强了对代理生存压力下行为可解释性的理解。

Abstract: Advanced reasoning models with agentic capabilities (AI agents) are deployed
to interact with humans and to solve sequential decision-making problems under
(approximate) utility functions and internal models. When such problems have
resource or failure constraints where action sequences may be forcibly
terminated once resources are exhausted, agents face implicit trade-offs that
reshape their utility-driven (rational) behaviour. Additionally, since these
agents are typically commissioned by a human principal to act on their behalf,
asymmetries in constraint exposure can give rise to previously unanticipated
misalignment between human objectives and agent incentives. We formalise this
setting through a survival bandit framework, provide theoretical and empirical
results that quantify the impact of survival-driven preference shifts, identify
conditions under which misalignment emerges and propose mechanisms to mitigate
the emergence of risk-seeking or risk-averse behaviours. As a result, this work
aims to increase understanding and interpretability of emergent behaviours of
AI agents operating under such survival pressure, and offer guidelines for
safely deploying such AI systems in critical resource-limited environments.

</details>


### [298] [EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions](https://arxiv.org/abs/2505.23473)
*Xiaorui Wu,Xiaofeng Mao,Fei Li,Xin Zhang,Xiaolu Zhang,Jun Zhou,Yuxiang Peng,Li Zheng,Chong Teng,Donghong Ji,Zhuang Li*

Key words: 大语言模型,过度拒绝,进化算法,指令优化,数据集

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: EVOREFUSE是一种通过进化算法生成多样化伪恶意指令的方法，用于评估和减少LLM的过度拒绝问题，创建了两个数据集并显著提升了模型的拒绝触发率和多样性。

Motivation: 解决LLMs因保守安全对齐而过度拒绝无害指令的问题，提升用户体验。

Method: 采用进化算法，通过突变和重组策略优化种子指令，最大化LLM拒绝概率的证据下界。

Result: 生成的数据集EVOREFUSE-TEST和EVOREFUSE-ALIGN在拒绝触发率、多样性和模型响应信心分数上显著优于现有基准，训练后的模型减少了14.31%的过度拒绝。

Conclusion: EVOREFUSE有效解决了LLM的过度拒绝问题，同时保持了模型的安全性。

Abstract: Large language models (LLMs) frequently refuse to respond to pseudo-malicious
instructions: semantically harmless input queries triggering unnecessary LLM
refusals due to conservative safety alignment, significantly impairing user
experience. Collecting such instructions is crucial for evaluating and
mitigating over-refusals, but existing instruction curation methods, like
manual creation or instruction rewriting, either lack scalability or fail to
produce sufficiently diverse and effective refusal-inducing prompts. To address
these limitations, we introduce EVOREFUSE, a prompt optimization approach that
generates diverse pseudo-malicious instructions consistently eliciting
confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm
exploring the instruction space in more diverse directions than existing
methods via mutation strategies and recombination, and iteratively evolves seed
instructions to maximize evidence lower bound on LLM refusal probability. Using
EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582
pseudo-malicious instructions that outperforms the next-best benchmark with
140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater
lexical diversity, and 40.03% improved LLM response confidence scores; and
EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with
responses for supervised and preference-based alignment training.
LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to
14.31% fewer over-refusals than models trained on the second-best alignment
dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals
models trigger over-refusals by overly focusing on sensitive keywords while
ignoring broader context.

</details>


### [299] [Socratic-PRMBench: Benchmarking Process Reward Models with Systematic Reasoning Patterns](https://arxiv.org/abs/2505.23474)
*Xiang Li,Haiyang Yu,Xinghua Zhang,Ziyang Huang,Shizhu He,Kang Liu,Jun Zhao,Fei Huang,Yongbin Li*

Key words: 过程奖励模型（PRMs）、推理模式、评测基准、Socratic-PRMBench、大语言模型（LLMs）

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了Socratic-PRMBench，一个系统性评估过程奖励模型（PRMs）在六种推理模式下的新基准，揭示了现有PRMs在多样化推理模式下的不足。

Motivation: 现有PRMs的评测主要集中在逐步正确性上，缺乏对多样化推理模式的系统性评估，因此需要新基准以填补这一空白。

Method: 引入了Socratic-PRMBench，包含2995条含错误的推理路径，覆盖六种推理模式。

Result: 实验表明现有PRMs在多样化推理模式下的评测能力存在显著不足。

Conclusion: Socratic-PRMBench可作为PRMs在多样化推理模式下系统性评测的综合测试平台，并为未来PRMs的发展铺路。

Abstract: Process Reward Models (PRMs) are crucial in complex reasoning and
problem-solving tasks (e.g., LLM agents with long-horizon decision-making) by
verifying the correctness of each intermediate reasoning step. In real-world
scenarios, LLMs may apply various reasoning patterns (e.g., decomposition) to
solve a problem, potentially suffering from errors under various reasoning
patterns. Therefore, PRMs are required to identify errors under various
reasoning patterns during the reasoning process. However, existing benchmarks
mainly focus on evaluating PRMs with stepwise correctness, ignoring a
systematic evaluation of PRMs under various reasoning patterns. To mitigate
this gap, we introduce Socratic-PRMBench, a new benchmark to evaluate PRMs
systematically under six reasoning patterns, including Transformation,
Decomposition, Regather, Deduction, Verification, and Integration.
Socratic-PRMBench}comprises 2995 reasoning paths with flaws within the
aforementioned six reasoning patterns. Through our experiments on both PRMs and
LLMs prompted as critic models, we identify notable deficiencies in existing
PRMs. These observations underscore the significant weakness of current PRMs in
conducting evaluations on reasoning steps under various reasoning patterns. We
hope Socratic-PRMBench can serve as a comprehensive testbed for systematic
evaluation of PRMs under diverse reasoning patterns and pave the way for future
development of PRMs.

</details>


### [300] [Autoformalization in the Era of Large Language Models: A Survey](https://arxiv.org/abs/2505.23486)
*Ke Weng,Lun Du,Sirui Li,Wangyue Lu,Haozhe Sun,Hengyu Liu,Tiancheng Zhang*

Key words: 自动形式化, 大语言模型, 数学表示, 可验证性, 开源资源

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文综述了自动形式化（将非正式数学命题转为可验证形式表示）的最新进展，涵盖数学与LLM视角，讨论了应用、工作流程及如何提升LLM输出的可验证性，并总结了开源资源与未来挑战。

Motivation: 自动形式化作为自动定理证明的基础任务，结合AI尤其是大语言模型（LLMs）的快速发展，为数学理论与应用提供了新视角，需要系统梳理其进展与挑战。

Method: 通过数学与LLM双重视角分析，调研不同数学领域中的自动形式化应用，并探讨从数据预处理到模型设计及评估的端到端工作流程。

Result: 总结了提升LLM输出可验证性的新兴作用，以及当前研究中的关键开源模型与数据集，揭示了改进LLM可信度与推理能力的潜力。

Conclusion: 自动形式化在LLM与数学交叉领域潜力巨大，但需解决数据、模型泛化等开放挑战，未来方向包括增强可扩展性与跨领域适配。

Abstract: Autoformalization, the process of transforming informal mathematical
propositions into verifiable formal representations, is a foundational task in
automated theorem proving, offering a new perspective on the use of mathematics
in both theoretical and applied domains. Driven by the rapid progress in
artificial intelligence, particularly large language models (LLMs), this field
has witnessed substantial growth, bringing both new opportunities and unique
challenges. In this survey, we provide a comprehensive overview of recent
advances in autoformalization from both mathematical and LLM-centric
perspectives. We examine how autoformalization is applied across various
mathematical domains and levels of difficulty, and analyze the end-to-end
workflow from data preprocessing to model design and evaluation. We further
explore the emerging role of autoformalization in enhancing the verifiability
of LLM-generated outputs, highlighting its potential to improve both the
trustworthiness and reasoning capabilities of LLMs. Finally, we summarize key
open-source models and datasets supporting current research, and discuss open
challenges and promising future directions for the field.

</details>


### [301] [TRAP: Targeted Redirecting of Agentic Preferences](https://arxiv.org/abs/2505.23518)
*Hangoo Kang,Jehyeok Yeon,Gagandeep Singh*

Key words: TRAP, 视觉语言模型, 对抗攻击, 扩散模型, 语义注入

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: TRAP框架通过扩散模型的语义注入操控AI代理决策，结合负提示降级与正语义优化，无需访问模型内部即可诱导选择偏差，在COCO数据集上对主流模型实现100%攻击成功率。

Motivation: 研究揭示视觉语言模型在跨模态推理中的语义漏洞，现有攻击方法依赖显式扰动或特权访问，难以实现隐蔽的现实攻击。

Method: 采用扩散模型的语义注入，结合负提示降级与正语义优化，通过Siamese语义网络和布局感知空间掩码引导攻击。

Result: 在COCO数据集上，TRAP对LLaVA-34B等主流模型实现100%攻击成功率，显著优于基线方法。

Conclusion: 跨模态语义漏洞需防御策略超越像素级鲁棒性，以应对隐蔽的语义操控威胁。

Abstract: Autonomous agentic AI systems powered by vision-language models (VLMs) are
rapidly advancing toward real-world deployment, yet their cross-modal reasoning
capabilities introduce new attack surfaces for adversarial manipulation that
exploit semantic reasoning across modalities. Existing adversarial attacks
typically rely on visible pixel perturbations or require privileged model or
environment access, making them impractical for stealthy, real-world
exploitation. We introduce TRAP, a generative adversarial framework that
manipulates the agent's decision-making using diffusion-based semantic
injections. Our method combines negative prompt-based degradation with positive
semantic optimization, guided by a Siamese semantic network and layout-aware
spatial masking. Without requiring access to model internals, TRAP produces
visually natural images yet induces consistent selection biases in agentic AI
systems. We evaluate TRAP on the Microsoft Common Objects in Context (COCO)
dataset, building multi-candidate decision scenarios. Across these scenarios,
TRAP achieves a 100% attack success rate on leading models, including
LLaVA-34B, Gemma3, and Mistral-3.1, significantly outperforming baselines such
as SPSA, Bandit, and standard diffusion approaches. These results expose a
critical vulnerability: Autonomous agents can be consistently misled through
human-imperceptible cross-modal manipulations. These findings highlight the
need for defense strategies beyond pixel-level robustness to address semantic
vulnerabilities in cross-modal decision-making.

</details>


### [302] [Individual differences in the cognitive mechanisms of planning strategy discovery](https://arxiv.org/abs/2505.23519)
*Ruiqi He,Falk Lieder*

Key words: 元认知强化学习, 策略发现, 认知机制, 个体差异, 规划任务

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 研究探讨如何通过引入认知机制（如内在元认知伪奖励、主观努力评估和终止深思）来提高元认知强化学习（MCRL）模型在人类策略发现中的表现，发现这些机制虽有助于策略发现，但未能完全弥合模型与人类表现的差距。

Motivation: 探索如何通过认知机制提升元认知强化学习模型在策略发现中的表现，以缩小其与人类能力的差距。

Method: 研究通过分析规划任务数据，考察内在元认知伪奖励、主观努力评估和终止深思这三种认知机制对策略发现的影响。

Result: 研究表明这些认知机制能促进策略发现，但个体差异明显，且未能完全弥合模型与人类表现的差距。

Conclusion: 尽管这些机制对策略发现有积极作用，仍需进一步探索其他潜在因素以完全理解人类策略发现的能力。

Abstract: People employ efficient planning strategies. But how are these strategies
acquired? Previous research suggests that people can discover new planning
strategies through learning from reinforcements, a process known as
metacognitive reinforcement learning (MCRL). While prior work has shown that
MCRL models can learn new planning strategies and explain more participants'
experience-driven discovery better than alternative mechanisms, it also
revealed significant individual differences in metacognitive learning.
Furthermore, when fitted to human data, these models exhibit a slower rate of
strategy discovery than humans. In this study, we investigate whether
incorporating cognitive mechanisms that might facilitate human strategy
discovery can bring models of MCRL closer to human performance. Specifically,
we consider intrinsically generated metacognitive pseudo-rewards, subjective
effort valuation, and termination deliberation. Analysis of planning task data
shows that a larger proportion of participants used at least one of these
mechanisms, with significant individual differences in their usage and varying
impacts on strategy discovery. Metacognitive pseudo-rewards, subjective effort
valuation, and learning the value of acting without further planning were found
to facilitate strategy discovery. While these enhancements provided valuable
insights into individual differences and the effect of these mechanisms on
strategy discovery, they did not fully close the gap between model and human
performance, prompting further exploration of additional factors that people
might use to discover new planning strategies.

</details>


### [303] [Synchronizing Process Model and Event Abstraction for Grounded Process Intelligence (Extended Version)](https://arxiv.org/abs/2505.23536)
*Janik-Vasily Benzin,Gyunam Park,Stefanie Rinderle-Ma*

Key words: 模型抽象,事件抽象,过程挖掘,行为简档,同步抽象

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了同步模型抽象（MA）和事件抽象（EA）的形式化基础，以确保在抽象过程中不丢失实际行为信息，并通过行为简档抽象技术验证了其可行性。

Motivation: 现有的技术无法在发现模型后同步抽象底层事件日志，导致失去对真实行为的基础，从而限制了分析洞察力。

Method: 提出了同步MA和EA的形式化基础，并通过行为简档抽象技术作为非顺序保留的MA技术进行验证。

Result: 证明了使用MA抽象过程模型与从抽象事件日志中发现过程模型会产生等效的过程模型。

Conclusion: 通过同步抽象技术和行为简档抽象，实现了模型与事件日志的一致性保留。

Abstract: Model abstraction (MA) and event abstraction (EA) are means to reduce
complexity of (discovered) models and event data. Imagine a process
intelligence project that aims to analyze a model discovered from event data
which is further abstracted, possibly multiple times, to reach optimality
goals, e.g., reducing model size. So far, after discovering the model, there is
no technique that enables the synchronized abstraction of the underlying event
log. This results in loosing the grounding in the real-world behavior contained
in the log and, in turn, restricts analysis insights. Hence, in this work, we
provide the formal basis for synchronized model and event abstraction, i.e., we
prove that abstracting a process model by MA and discovering a process model
from an abstracted event log yields an equivalent process model. We prove the
feasibility of our approach based on behavioral profile abstraction as
non-order preserving MA technique, resulting in a novel EA technique.

</details>


### [304] [SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents](https://arxiv.org/abs/2505.23559)
*Kunlun Zhu,Jiaxun Zhang,Ziheng Qi,Nuoxing Shang,Zijia Liu,Peixuan Han,Yue Su,Haofei Yu,Jiaxuan You*

Key words: AI安全, 伦理审查, 科学发现, 风险评估, 监控机制

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了SafeScientist框架和SciSafetyBench基准，旨在提升AI科学探索的安全性与伦理责任。通过多方位监控和伦理审查，实验显示其安全性提升了35%，同时保持了科学输出的质量。

Motivation: 随着大型语言模型（LLM）在科学发现中的应用日益广泛，其带来的伦理和安全问题亟待解决。

Method: 整合了提示监控、代理协作监控、工具使用监控和伦理审查模块，构建了SafeScientist框架，并设计了SciSafetyBench基准进行评估。

Result: SafeScientist在240个高风险任务中安全性提升35%，且科学输出质量未受影响。

Conclusion: 该框架为解决AI科学探索中的伦理和安全问题提供了有效方案。

Abstract: Recent advancements in large language model (LLM) agents have significantly
accelerated scientific discovery automation, yet concurrently raised critical
ethical and safety concerns. To systematically address these challenges, we
introduce \textbf{SafeScientist}, an innovative AI scientist framework
explicitly designed to enhance safety and ethical responsibility in AI-driven
scientific exploration. SafeScientist proactively refuses ethically
inappropriate or high-risk tasks and rigorously emphasizes safety throughout
the research process. To achieve comprehensive safety oversight, we integrate
multiple defensive mechanisms, including prompt monitoring, agent-collaboration
monitoring, tool-use monitoring, and an ethical reviewer component.
Complementing SafeScientist, we propose \textbf{SciSafetyBench}, a novel
benchmark specifically designed to evaluate AI safety in scientific contexts,
comprising 240 high-risk scientific tasks across 6 domains, alongside 30
specially designed scientific tools and 120 tool-related risk tasks. Extensive
experiments demonstrate that SafeScientist significantly improves safety
performance by 35\% compared to traditional AI scientist frameworks, without
compromising scientific output quality. Additionally, we rigorously validate
the robustness of our safety pipeline against diverse adversarial attack
methods, further confirming the effectiveness of our integrated approach. The
code and data will be available at https://github.com/ulab-uiuc/SafeScientist.
\textcolor{red}{Warning: this paper contains example data that may be offensive
or harmful.}

</details>


### [305] [CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring](https://arxiv.org/abs/2505.23575)
*Benjamin Arnav,Pablo Bernabeu-Pérez,Nathan Helm-Burger,Tim Kostolansky,Hannes Whittingham,Mary Phuong*

Key words: AI监控, Chain-of-Thought, 红色团队, 混合协议, 有害行为检测

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 研究提出了一种Chain-of-Thought (CoT)监控方法，用于检测AI模型的有害行为，并通过混合协议进一步提升监控效果。

Motivation: 随着AI模型自主性的提升，防止其未被察觉地采取有害行为变得重要。研究旨在探索更有效的监控方法。

Method: 比较CoT监控（监控中间推理步骤）和仅监控最终输出的方法，并在红色团队环境中测试。随后引入混合协议，独立评分推理和输出并加权平均。

Result: CoT监控在检测有害行为上比仅监控输出高27个百分点，但可能被误导。混合协议在所有测试中表现最佳，检测率是仅监控输出的四倍。

Conclusion: 混合协议在监控AI模型有害行为方面优于单独使用CoT或仅监控输出的方法。

Abstract: As AI models are deployed with increasing autonomy, it is important to ensure
they do not take harmful actions unnoticed. As a potential mitigation, we
investigate Chain-of-Thought (CoT) monitoring, wherein a weaker trusted monitor
model continuously oversees the intermediate reasoning steps of a more powerful
but untrusted model. We compare CoT monitoring to action-only monitoring, where
only final outputs are reviewed, in a red-teaming setup where the untrusted
model is instructed to pursue harmful side tasks while completing a coding
problem. We find that CoT monitoring improves detection by up to 27 percentage
points in scenarios where action-only monitoring fails to reliably identify
sabotage. However, CoT traces can also contain misleading rationalizations that
deceive the monitor, reducing performance in more obvious sabotage cases. To
address this, we introduce a hybrid protocol that independently scores both
reasoning and final outputs and combines them using a weighted average. This
hybrid monitor consistently outperforms both CoT and action-only monitors
across all tested models and tasks, with detection rates over four times higher
than action-only monitoring for subtle deception scenarios.

</details>


### [306] [MAPLE: A Mobile Assistant with Persistent Finite State Machines for Recovery Reasoning](https://arxiv.org/abs/2505.23596)
*Linqiang Guo,Wei Liu,Yi Wen Heng,Tse-Hsun,Chen,Yang Wang*

Key words: Mobile GUI agents, Multimodal Large Language Models (MLLMs), Finite State Machine (FSM), Task execution, MAPLE

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了MAPLE，一种基于有限状态机（FSM）的状态感知多智能体框架，用于提升移动GUI代理的任务完成能力。MAPLE通过建模UI屏幕为状态和用户操作为状态转移，实现了结构化的任务执行，显著提高了任务成功率、恢复成功率和操作准确性。

Motivation: 现有的移动GUI代理通常是反应式的，仅针对当前屏幕进行推理，缺乏对应用导航流的结构化建模，限制了其理解上下文、检测意外结果和从错误中恢复的能力。MAPLE旨在通过结构化状态建模解决这些问题。

Method: MAPLE将应用交互抽象为有限状态机（FSM），建模UI屏幕为离散状态，用户操作为状态转移。框架包含多个专业化智能体，负责规划、执行、验证、错误恢复和知识保留，动态构建FSM以实现任务执行。

Result: 在Mobile-Eval-E和SPA-Bench两个跨应用基准测试中，MAPLE显著优于现有基线，任务成功率提升12%，恢复成功率提升13.8%，操作准确性提高6.5%。

Conclusion: 结构化状态建模在移动GUI代理任务执行中至关重要。MAPLE的FSM表示可作为轻量级、模型无关的记忆层，支持未来GUI代理的规划、执行验证和错误恢复。

Abstract: Mobile GUI agents aim to autonomously complete user-instructed tasks across
mobile apps. Recent advances in Multimodal Large Language Models (MLLMs) enable
these agents to interpret UI screens, identify actionable elements, and perform
interactions such as tapping or typing. However, existing agents remain
reactive: they reason only over the current screen and lack a structured model
of app navigation flow, limiting their ability to understand context, detect
unexpected outcomes, and recover from errors. We present MAPLE, a state-aware
multi-agent framework that abstracts app interactions as a Finite State Machine
(FSM). We computationally model each UI screen as a discrete state and user
actions as transitions, allowing the FSM to provide a structured representation
of the app execution. MAPLE consists of specialized agents responsible for four
phases of task execution: planning, execution, verification, error recovery,
and knowledge retention. These agents collaborate to dynamically construct FSMs
in real time based on perception data extracted from the UI screen, allowing
the GUI agents to track navigation progress and flow, validate action outcomes
through pre- and post-conditions of the states, and recover from errors by
rolling back to previously stable states. Our evaluation results on two
challenging cross-app benchmarks, Mobile-Eval-E and SPA-Bench, show that MAPLE
outperforms the state-of-the-art baseline, improving task success rate by up to
12%, recovery success by 13.8%, and action accuracy by 6.5%. Our results
highlight the importance of structured state modeling in guiding mobile GUI
agents during task execution. Moreover, our FSM representation can be
integrated into future GUI agent architectures as a lightweight, model-agnostic
memory layer to support structured planning, execution verification, and error
recovery.

</details>


### [307] [Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models](https://arxiv.org/abs/2505.23667)
*Lang Cao,Jingxian Xu,Hanbing Liu,Jinyu Wang,Mengyu Zhou,Haoyu Dong,Shi Han,Dongmei Zhang*

Key words: 表格理解、电子表格公式、强化学习、语言模型、符号推理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种名为Formula Tuning（Fortune）的强化学习框架，通过生成可执行的电子表格公式来提升语言模型在表格数据上的推理能力，减少对标注数据的依赖，并在多个基准测试中验证了其有效性。

Motivation: 尽管大型语言模型在通用推理任务上表现优异，但在复杂表格数据的数值或符号推理上仍存在不足。论文试图通过利用电子表格公式的丰富表达能力，提升模型在表格理解任务中的表现。

Method: 提出Formula Tuning框架，采用强化学习方法，以答案正确性作为奖励信号，指导语言模型生成可执行的电子表格公式，从而减少对标注数据的依赖。

Result: 在七个表格推理基准测试中，Formula Tuning显著提升了模型性能，尤其在多步数值和符号推理任务上，使得一个7B规模的模型超越了现有方法。

Conclusion: Formula Tuning展示了公式驱动的强化学习在提升语言模型符号表格推理方面的潜力，为表格理解任务的进一步研究提供了新思路。

Abstract: Tables are a fundamental structure for organizing and analyzing data, making
effective table understanding a critical capability for intelligent systems.
While large language models (LMs) demonstrate strong general reasoning
abilities, they continue to struggle with accurate numerical or symbolic
reasoning over tabular data, especially in complex scenarios. Spreadsheet
formulas provide a powerful and expressive medium for representing executable
symbolic operations, encoding rich reasoning patterns that remain largely
underutilized. In this paper, we propose Formula Tuning (Fortune), a
reinforcement learning (RL) framework that trains LMs to generate executable
spreadsheet formulas for question answering over general tabular data. Formula
Tuning reduces the reliance on supervised formula annotations by using binary
answer correctness as a reward signal, guiding the model to learn formula
derivation through reasoning. We provide a theoretical analysis of its
advantages and demonstrate its effectiveness through extensive experiments on
seven table reasoning benchmarks. Formula Tuning substantially enhances LM
performance, particularly on multi-step numerical and symbolic reasoning tasks,
enabling a 7B model to outperform O1 on table understanding. This highlights
the potential of formula-driven RL to advance symbolic table reasoning in LMs.

</details>


### [308] [ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork](https://arxiv.org/abs/2505.23686)
*Caroline Wang,Arrasy Rahman,Jiaxun Cui,Yoonchang Sung,Peter Stone*

Key words: Ad Hoc Teamwork, open-ended learning, ROTATE, multi-agent learning

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种统一的开放式学习框架ROTATE，用于提升AI代理与未见过的队友协作的能力，显著优于现有方法。

Motivation: 现有Ad Hoc Teamwork（AHT）方法的两阶段流程限制了队友行为的覆盖范围，且忽视了训练队友对AHT代理的可学习性。

Method: 论文将AHT问题重新定义为一种开放式学习过程，提出了ROTATE算法，交替优化AHT代理和生成探测其缺陷的队友。

Result: ROTATE在多种AHT环境中显著优于基线方法，能够更好地泛化到未见过的队友。

Conclusion: ROTATE为AHT提供了一种新的标准，实现了更鲁棒和通用的团队协作。

Abstract: Developing AI agents capable of collaborating with previously unseen partners
is a fundamental generalization challenge in multi-agent learning, known as Ad
Hoc Teamwork (AHT). Existing AHT approaches typically adopt a two-stage
pipeline, where first, a fixed population of teammates is generated with the
idea that they should be representative of the teammates that will be seen at
deployment time, and second, an AHT agent is trained to collaborate well with
agents in the population. To date, the research community has focused on
designing separate algorithms for each stage. This separation has led to
algorithms that generate teammate pools with limited coverage of possible
behaviors, and that ignore whether the generated teammates are easy to learn
from for the AHT agent. Furthermore, algorithms for training AHT agents
typically treat the set of training teammates as static, thus attempting to
generalize to previously unseen partner agents without assuming any control
over the distribution of training teammates. In this paper, we present a
unified framework for AHT by reformulating the problem as an open-ended
learning process between an ad hoc agent and an adversarial teammate generator.
We introduce ROTATE, a regret-driven, open-ended training algorithm that
alternates between improving the AHT agent and generating teammates that probe
its deficiencies. Extensive experiments across diverse AHT environments
demonstrate that ROTATE significantly outperforms baselines at generalizing to
an unseen set of evaluation teammates, thus establishing a new standard for
robust and generalizable teamwork.

</details>


### [309] [Data-to-Dashboard: Multi-Agent LLM Framework for Insightful Visualization in Enterprise Analytics](https://arxiv.org/abs/2505.23695)
*Ran Zhang,Mohannad Elhamod*

Key words: LLM agents, data analysis, visualization, business analytics, automation

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: This paper introduces an agentic system using modular LLM agents to automate the data-to-dashboard pipeline, improving insight generation and visualization by simulating business analysts' reasoning.

Motivation: The rapid advancement of LLMs enables diverse applications in data analysis, but existing systems lack adaptability and domain relevance. This work aims to bridge the gap by automating the analytical pipeline.

Method: The system employs modular LLM agents for domain detection, concept extraction, multi-perspective analysis, and self-reflection, without relying on closed ontologies or templates.

Result: Evaluated on three datasets, the system outperforms GPT-4o in insightfulness, domain relevance, and analytical depth, validated by tailored metrics and human assessment.

Conclusion: The work presents a novel pipeline for automated data-to-dashboard processes, enhancing business analytics with human-in-the-loop validation.

Abstract: The rapid advancement of LLMs has led to the creation of diverse agentic
systems in data analysis, utilizing LLMs' capabilities to improve insight
generation and visualization. In this paper, we present an agentic system that
automates the data-to-dashboard pipeline through modular LLM agents capable of
domain detection, concept extraction, multi-perspective analysis generation,
and iterative self-reflection. Unlike existing chart QA systems, our framework
simulates the analytical reasoning process of business analysts by retrieving
domain-relevant knowledge and adapting to diverse datasets without relying on
closed ontologies or question templates.
  We evaluate our system on three datasets across different domains.
Benchmarked against GPT-4o with a single-prompt baseline, our approach shows
improved insightfulness, domain relevance, and analytical depth, as measured by
tailored evaluation metrics and qualitative human assessment.
  This work contributes a novel modular pipeline to bridge the path from raw
data to visualization, and opens new opportunities for human-in-the-loop
validation by domain experts in business analytics. All code can be found here:
https://github.com/77luvC/D2D_Data2Dashboard

</details>


### [310] [Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability](https://arxiv.org/abs/2505.23703)
*Ruida Wang,Yuxin Li,Yi R.,Fung,Tong Zhang*

Key words: 数学推理, 自然语言处理, 形式语言, 强化学习, 混合推理框架

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种名为 **NL-FL HybridReasoning** 的端到端框架，通过整合自然语言（NL）和形式语言（FL）推理，提升LLMs的数学推理能力，并在多个基准测试中表现优于纯NL基线模型。

Motivation: 当前强化学习方法在自然语言和形式语言推理上取得进展，但难以扩展基模的能力，尤其是将形式语言知识有效整合到自然语言数学推理中。NL与FL在问题结构和推理格式上的差异使整合更具挑战性。

Method: 提出 **NL-FL HybridReasoning** 框架，包含三个关键技术：1. **NL-FL问题对齐**将NL问答问题转换为FL存在定理；2. **混合问题输入**使FL推理器能同时处理问答和存在性问题；3. **基于LLM的答案提取**弥合NL与FL输出格式的差异。

Result: 实验表明，该框架在MATH-500和AMC基准上的准确率分别达到89.80%和84.34%，显著超越NL基线模型（分别提升4.60%和4.82%），且解决了NL基线无法处理的部分问题。

Conclusion: 通过有效整合NL和FL推理，**HybridReasoning**框架显著提升了数学推理能力，为解决两种语言格式差异提供了可行方案。

Abstract: Enhancing the mathematical reasoning capabilities of LLMs has garnered
significant attention in both the mathematical and computer science
communities. Recent works have made substantial progress in both Natural
Language (NL) reasoning and Formal Language (FL) reasoning by leveraging the
potential of pure Reinforcement Learning (RL) methods on base models. However,
RL approaches struggle to impart new capabilities not presented in the base
model, highlighting the need to integrate more knowledge like FL into NL math
reasoning effectively. Yet, this integration is challenging due to inherent
disparities in problem structure and reasoning format between NL and FL. To
address these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end
framework designed to incorporate the FL expert into NL math problem-solving.
To bridge the NL and FL input format gap, we propose the *NL-FL Problem
Alignment* method, which reformulates the Question-Answering (QA) problems in
NL as existence theorems in FL. Subsequently, the *Mixed Problem Input*
technique we provide enables the FL reasoner to handle both QA and existence
problems concurrently. Lastly, we mitigate the NL and FL output format gap in
reasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive
experiments demonstrate that the **HybridReasoning** framework achieves
**89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC
benchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively.
Notably, some problems resolved by our framework remain unsolved by the NL
baseline model even under a larger number of trials.

</details>


### [311] [Comparative of Genetic Fuzzy regression techniques for aeroacoustic phenomenons](https://arxiv.org/abs/2505.23746)
*Hugo Henry,Kelly Cohen*

Key words: 模糊逻辑,回归,级联系统,聚类,人工智能

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该研究探讨了遗传模糊系统（GFS）在模拟翼型自噪声中的应用，比较了多种模糊回归策略，并验证了聚类辅助模糊推理在复杂气动声学现象中的有效性。

Motivation: 翼型自噪声是气动声学中的关键问题，对航空航天、汽车和无人机应用有重要影响。研究旨在通过遗传模糊系统建模解决这一问题。

Method: 研究比较了高规则密度的TSK模糊系统、级联遗传模糊树（GFT）架构以及基于模糊C均值（FCM）的新型聚类方法，以降低模型复杂度。

Result: 结果表明，聚类辅助的模糊推理是一种有效的回归工具，适用于复杂气动声学现象。

Conclusion: 研究验证了遗传模糊系统在翼型自噪声建模中的可行性，特别是聚类方法的优势。

Abstract: This study investigates the application of Genetic Fuzzy Systems (GFS) to
model the self-noise generated by airfoils, a key issue in aeroaccoustics with
significant implications for aerospace, automotive and drone applications.
Using the publicly available Airfoil Self Noise dataset, various Fuzzy
regression strategies are explored and compared. The paper evaluates a brute
force Takagi Sugeno Kang (TSK) fuzzy system with high rule density, a cascading
Geneti Fuzzy Tree (GFT) architecture and a novel clustered approach based on
Fuzzy C-means (FCM) to reduce the model's complexity. This highlights the
viability of clustering assisted fuzzy inference as an effective regression
tool for complex aero accoustic phenomena. Keywords : Fuzzy logic, Regression,
Cascading systems, Clustering and AI.

</details>


### [312] [ZeroGUI: Automating Online GUI Learning at Zero Human Cost](https://arxiv.org/abs/2505.23762)
*Chenyu Yang,Shiqian Su,Shi Liu,Xuan Dong,Yue Yu,Weijie Su,Xuehui Wang,Zhaoyang Liu,Jinguo Zhu,Hao Li,Wenhai Wang,Yu Qiao,Xizhou Zhu,Jifeng Dai*

Key words: GUI代理, 在线学习, 强化学习, 视觉语言模型, 零人工成本

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: ZeroGUI是一种可扩展的在线学习框架，无需人工标注，通过自动任务生成和奖励估计，提升GUI代理的性能。

Motivation: 解决现有纯视觉GUI代理对高标注成本和动态环境适应性的限制。

Method: 结合VLM自动生成任务与奖励估计，采用两阶段在线强化学习。

Result: 在OSWorld和AndroidLab环境中显著提升UI-TARS和Aguvis的性能。

Conclusion: ZeroGUI在零人工成本下实现了GUI代理的高效训练。

Abstract: The rapid advancement of large Vision-Language Models (VLMs) has propelled
the development of pure-vision-based GUI Agents, capable of perceiving and
operating Graphical User Interfaces (GUI) to autonomously fulfill user
instructions. However, existing approaches usually adopt an offline learning
framework, which faces two core limitations: (1) heavy reliance on high-quality
manual annotations for element grounding and action supervision, and (2)
limited adaptability to dynamic and interactive environments. To address these
limitations, we propose ZeroGUI, a scalable, online learning framework for
automating GUI Agent training at Zero human cost. Specifically, ZeroGUI
integrates (i) VLM-based automatic task generation to produce diverse training
goals from the current environment state, (ii) VLM-based automatic reward
estimation to assess task success without hand-crafted evaluation functions,
and (iii) two-stage online reinforcement learning to continuously interact with
and learn from GUI environments. Experiments on two advanced GUI Agents
(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance
across OSWorld and AndroidLab environments. The code is available at
https://github.com/OpenGVLab/ZeroGUI.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [313] [Local Stability and Region of Attraction Analysis for Neural Network Feedback Systems under Positivity Constraints](https://arxiv.org/abs/2505.22889)
*Hamidreza Montazeri Hedesh,Moh Kamalul Wafi,Milad Siami*

Key words: Lur'e系统，前馈神经网络，局部稳定性，吸引域，线性矩阵不等式

<details>
  <summary>Details</summary>

Main category: eess.SY

TL;DR: 该论文研究了具有前馈神经网络静态非线性反馈的Lur'e型非线性系统的局部稳定性，通过两种方法估计吸引域（ROA），数值结果显示在ROA大小和可扩展性上优于现有方法。

Motivation: 研究Lur'e型系统在前馈神经网络（FFNNs）反馈下的局部稳定性，旨在提供更精确的稳定性分析方法和更大的吸引域估计。

Method: （1）基于李雅普诺夫的保守性较小方法，构造满足线性矩阵不等式的二次函数不变子集；（2）通过逐层传播线性松弛，计算FFNNs的局部紧致扇形边界。

Result: 数值实验表明，该方法在吸引域大小和可扩展性上显著优于现有的积分二次约束方法。

Conclusion: 提出的方法有效提升了Lur'e型系统在神经网络反馈下的稳定性分析能力，为实际应用提供了更可靠的理论支持。

Abstract: We study the local stability of nonlinear systems in the Lur'e form with
static nonlinear feedback realized by feedforward neural networks (FFNNs). By
leveraging positivity system constraints, we employ a localized variant of the
Aizerman conjecture, which provides sufficient conditions for exponential
stability of trajectories confined to a compact set. Using this foundation, we
develop two distinct methods for estimating the Region of Attraction (ROA): (i)
a less conservative Lyapunov-based approach that constructs invariant sublevel
sets of a quadratic function satisfying a linear matrix inequality (LMI), and
(ii) a novel technique for computing tight local sector bounds for FFNNs via
layer-wise propagation of linear relaxations. These bounds are integrated into
the localized Aizerman framework to certify local exponential stability.
Numerical results demonstrate substantial improvements over existing integral
quadratic constraint-based approaches in both ROA size and scalability.

</details>


### [314] [User-centric Vehicle-to-Grid Optimization with an Input Convex Neural Network-based Battery Degradation Model](https://arxiv.org/abs/2505.11047)
*Arghya Mallick,Georgios Pantazis,Mohammad Khosravi,Peyman Mohajerin Esfahani,Sergio Grammatico*

Key words: V2G, 多目标优化, 电池退化, 输入凸神经网络, 用户偏好

<details>
  <summary>Details</summary>

Main category: eess.SY

TL;DR: 提出了基于多目标优化的用户中心化V2G方法，利用输入凸神经网络（ICNN）建模电池退化，并通过优化框架生成最优充电策略。

Motivation: 针对缺乏准确且可泛化的电池退化模型的问题，研究平衡V2G收益和电池退化的用户偏好方法。

Method: 结合ICNN数据驱动退化模型与多目标优化框架，生成考虑用户偏好的最优充电策略。

Result: ICNN模型在预测电池退化方面表现高精度，且通过优化框架实现了收益与电池健康的权衡。

Conclusion: 该方法有效平衡了V2G收益与电池退化，并通过用户偏好参数灵活调整策略。

Abstract: We propose a data-driven, user-centric vehicle-to-grid (V2G) methodology
based on multi-objective optimization to balance battery degradation and V2G
revenue according to EV user preference. Given the lack of accurate and
generalizable battery degradation models, we leverage input convex neural
networks (ICNNs) to develop a data-driven degradation model trained on
extensive experimental datasets. This approach enables our model to capture
nonconvex dependencies on battery temperature and time while maintaining
convexity with respect to the charging rate. Such a partial convexity property
ensures that the second stage of our methodology remains computationally
efficient. In the second stage, we integrate our data-driven degradation model
into a multi-objective optimization framework to generate an optimal smart
charging profile for each EV. This profile effectively balances the trade-off
between financial benefits from V2G participation and battery degradation,
controlled by a hyperparameter reflecting the user prioritization of battery
health. Numerical simulations show the high accuracy of the ICNN model in
predicting battery degradation for unseen data. Finally, we present a trade-off
curve illustrating financial benefits from V2G versus losses from battery
health degradation based on user preferences and showcase smart charging
strategies under realistic scenarios.

</details>


### [315] [From Connectivity to Autonomy: The Dawn of Self-Evolving Communication Systems](https://arxiv.org/abs/2505.23710)
*Zeinab Nezami,Syed Danial Ali Shah,Maryam Hafeez,Karim Djemame,Syed Ali Raza Zaidi*

Key words: 6G, AI, 自演进系统, 工业物联网, 实时决策, 网络管理

<details>
  <summary>Details</summary>

Main category: eess.SY

TL;DR: 该论文展望6G作为自演进电信生态系统，通过AI驱动实现动态适应性，探讨自主通信系统的关键要素及其与工业物联网框架的整合，强调实时决策优化与降低延迟，并提出技术路线图。

Motivation: 探索6G技术在电信生态系统中实现动态适应和自主决策的潜力，以应对未来工业自动化的需求。

Method: 研究可重构基础设施、自适应中间件、智能网络功能及多智能体协作，结合工业物联网框架分析。

Result: 展示了实时决策优化、效率提升和延迟降低的潜力，同时提出技术路线图。

Conclusion: 6G技术有望推动智能自动化解决方案的下一代发展，连接理论与工业应用。

Abstract: This paper envisions 6G as a self-evolving telecom ecosystem, where AI-driven
intelligence enables dynamic adaptation beyond static connectivity. We explore
the key enablers of autonomous communication systems, spanning reconfigurable
infrastructure, adaptive middleware, and intelligent network functions,
alongside multi-agent collaboration for distributed decision-making. We explore
how these methodologies align with emerging industrial IoT frameworks, ensuring
seamless integration within digital manufacturing processes. Our findings
emphasize the potential for improved real-time decision-making, optimizing
efficiency, and reducing latency in networked control systems. The discussion
addresses ethical challenges, research directions, and standardization efforts,
concluding with a technology stack roadmap to guide future developments. By
leveraging state-of-the-art 6G network management techniques, this research
contributes to the next generation of intelligent automation solutions,
bridging the gap between theoretical advancements and real-world industrial
applications.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [316] [A Large Language Model-Enabled Control Architecture for Dynamic Resource Capability Exploration in Multi-Agent Manufacturing Systems](https://arxiv.org/abs/2505.22814)
*Jonghan Lim,Ilya Kovalenko*

Key words: 多智能体系统, 大语言模型, 制造控制, 实时适应, 资源利用

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 该论文提出了一种基于大语言模型的多智能体制造系统控制架构，旨在动态探索资源能力以应对实时干扰，提高了系统的弹性和灵活性，并通过案例研究证明了更高的吞吐量和资源利用率。

Motivation: 制造环境因需求变化和产品生命周期缩短而日益复杂和不可预测，传统控制方法在动态工业环境中的响应性不足，需要更先进的控制策略。多智能体系统虽然能分散决策，但仍面临实时适应、上下文感知决策等挑战。

Method: 提出了一种大语言模型支持的控制架构，用于多智能体制造系统，通过上下文感知决策能力动态调整资源能力。

Result: 仿真案例研究表明，该架构提升了系统弹性和灵活性，吞吐量和资源利用率优于现有方法。

Conclusion: 大语言模型在多智能体制造系统中具有潜力，能有效应对实时干扰，提升系统性能。

Abstract: Manufacturing environments are becoming more complex and unpredictable due to
factors such as demand variations and shorter product lifespans. This
complexity requires real-time decision-making and adaptation to disruptions.
Traditional control approaches highlight the need for advanced control
strategies capable of overcoming unforeseen challenges, as they demonstrate
limitations in responsiveness within dynamic industrial settings. Multi-agent
systems address these challenges through decentralization of decision-making,
enabling systems to respond dynamically to operational changes. However,
current multi-agent systems encounter challenges related to real-time
adaptation, context-aware decision-making, and the dynamic exploration of
resource capabilities. Large language models provide the possibility to
overcome these limitations through context-aware decision-making capabilities.
This paper introduces a large language model-enabled control architecture for
multi-agent manufacturing systems to dynamically explore resource capabilities
in response to real-time disruptions. A simulation-based case study
demonstrates that the proposed architecture improves system resilience and
flexibility. The case study findings show improved throughput and efficient
resource utilization compared to existing approaches.

</details>


### [317] [Understanding the Information Propagation Effects of Communication Topologies in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2505.23352)
*Xu Shen,Yixin Liu,Yiwei Dai,Yili Wang,Rui Miao,Yue Tan,Shirui Pan,Xin Wang*

Key words: 多代理系统, 通信拓扑, 稀疏性, 因果框架, EIB-leanrner

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 本文提出了一个因果框架分析多代理系统中的通信拓扑结构对决策效率的影响，发现适度稀疏的拓扑能有效抑制错误传播并保留有益信息。基于此，作者提出了一种新型拓扑设计方法EIB-leanrner，实验证明了其高效性和鲁棒性。

Motivation: 现有通信拓扑自动设计研究多关注稀疏结构的效率，却忽略了稀疏与密集拓扑分别在何时有益或阻碍协作。本文旨在填补这一空白，研究不同稀疏度下代理输出的传播规律。

Method: 采用因果框架分析代理输出在稀疏和密集拓扑中的传播行为，并提出结合两者优势的EIB-leanrner拓扑设计方法。

Result: 实验表明，适度稀疏的拓扑在抑制错误传播和保留有用信息方面表现最佳，而EIB-leanrner在效率、通信成本和鲁棒性上均优于其他方法。

Conclusion: 研究证实了拓扑稀疏度对多代理协作的关键作用，并提出了一种平衡错误抑制和信息传播的新方法，为未来拓扑设计提供了重要指导。

Abstract: The communication topology in large language model-based multi-agent systems
fundamentally governs inter-agent collaboration patterns, critically shaping
both the efficiency and effectiveness of collective decision-making. While
recent studies for communication topology automated design tend to construct
sparse structures for efficiency, they often overlook why and when sparse and
dense topologies help or hinder collaboration. In this paper, we present a
causal framework to analyze how agent outputs, whether correct or erroneous,
propagate under topologies with varying sparsity. Our empirical studies reveal
that moderately sparse topologies, which effectively suppress error propagation
while preserving beneficial information diffusion, typically achieve optimal
task performance. Guided by this insight, we propose a novel topology design
approach, EIB-leanrner, that balances error suppression and beneficial
information propagation by fusing connectivity patterns from both dense and
sparse graphs. Extensive experiments show the superior effectiveness,
communication cost, and robustness of EIB-leanrner.

</details>


### [318] [Collaborative Last-Mile Delivery: A Multi-Platform Vehicle Routing Problem With En-route Charging](https://arxiv.org/abs/2505.23584)
*Sumbal Malik,Majid Khonji,Khaled Elbassioni,Jorge Dias*

Key words: 协作物流，无人机配送，多平台车辆路径问题，启发式算法，MILP

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 该研究提出了一种新型协作物流问题（VRP-DR），结合卡车、无人机和机器人协同配送，解决了传统配送中的容量和续航限制，并通过MILP和启发式算法FINDER实现了高效、近优的解决方案。

Motivation: 电商的快速增长和“最后一公里”配送的高效需求推动了协作物流的发展。研究旨在通过多平台协同（卡车、无人机和机器人）优化配送效率与成本。

Method: 提出VRP-DR模型，引入多访问、多行程、灵活对接等现实特征，并用MILP和启发式算法FINDER求解。

Result: 实验显示协作模式比纯卡车模式显著节省时间，多访问功能大幅降低成本，并分析了充电、无人机配置等因素对性能的影响。

Conclusion: VRP-DR模型通过协同配送和智能算法有效提升了物流效率，为行业提供了实用解决方案。

Abstract: The rapid growth of e-commerce and the increasing demand for timely,
cost-effective last-mile delivery have increased interest in collaborative
logistics. This research introduces a novel collaborative synchronized
multi-platform vehicle routing problem with drones and robots (VRP-DR), where a
fleet of $\mathcal{M}$ trucks, $\mathcal{N}$ drones and $\mathcal{K}$ robots,
cooperatively delivers parcels. Trucks serve as mobile platforms, enabling the
launching, retrieving, and en-route charging of drones and robots, thereby
addressing critical limitations such as restricted payload capacities, limited
range, and battery constraints. The VRP-DR incorporates five realistic
features: (1) multi-visit service per trip, (2) multi-trip operations, (3)
flexible docking, allowing returns to the same or different trucks (4) cyclic
and acyclic operations, enabling return to the same or different nodes; and (5)
en-route charging, enabling drones and robots to recharge while being
transported on the truck, maximizing operational efficiency by utilizing idle
transit time. The VRP-DR is formulated as a mixed-integer linear program (MILP)
to minimize both operational costs and makespan. To overcome the computational
challenges of solving large-scale instances, a scalable heuristic algorithm,
FINDER (Flexible INtegrated Delivery with Energy Recharge), is developed, to
provide efficient, near-optimal solutions. Numerical experiments across various
instance sizes evaluate the performance of the MILP and heuristic approaches in
terms of solution quality and computation time. The results demonstrate
significant time savings of the combined delivery mode over the truck-only mode
and substantial cost reductions from enabling multi-visits. The study also
provides insights into the effects of en-route charging, docking flexibility,
drone count, speed, and payload capacity on system performance.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [319] [Non-convex entropic mean-field optimization via Best Response flow](https://arxiv.org/abs/2505.22760)
*Razvan-Andrei Lascu,Mateusz B. Majka*

Key words: 非凸优化, KL散度, 最佳响应流, Wasserstein距离, 强化学习

<details>
  <summary>Details</summary>

Main category: math.OC

TL;DR: 本文研究了在概率测度空间上通过相对熵（KL散度）正则化最小化非凸泛函的问题，以及相应的熵正则化非凸-非凹极小极大问题。通过利用最佳响应流（也称为虚构博弈流），分析了其收敛性如何受到泛函非凸性、正则化参数和参考测度尾部行为的影响。文章展示了如何根据非凸泛函选择正则化器，使最佳响应算子成为$L^1$-Wasserstein距离下的收缩映射，从而确保其全局唯一极小点的存在。

Motivation: 现有方法主要适用于凸优化问题，但在非凸泛函与熵正则化的结合上缺乏理论支持。本文旨在填补这一空白，并拓展应用至强化学习中的策略优化问题。

Method: 采用最佳响应流方法，结合KL散度正则化，分析非凸性、正则化参数与参考测度尾部行为的关系，提出使最佳响应算子成为收缩映射的条件。

Result: 证明了在特定正则化器选择下，最佳响应算子是$L^1$-Wasserstein距离的收缩映射，确保了全局唯一极小点的存在。

Conclusion: 通过特定正则化器的选择，可以放松凸性假设，并成功将方法应用于强化学习中的策略优化问题。

Abstract: We study the problem of minimizing non-convex functionals on the space of
probability measures, regularized by the relative entropy (KL divergence) with
respect to a fixed reference measure, as well as the corresponding problem of
solving entropy-regularized non-convex-non-concave min-max problems. We utilize
the Best Response flow (also known in the literature as the fictitious play
flow) and study how its convergence is influenced by the relation between the
degree of non-convexity of the functional under consideration, the
regularization parameter and the tail behaviour of the reference measure. In
particular, we demonstrate how to choose the regularizer, given the non-convex
functional, so that the Best Response operator becomes a contraction with
respect to the $L^1$-Wasserstein distance, which then ensures the existence of
its unique fixed point, which is then shown to be the unique global minimizer
for our optimization problem. This extends recent results where the Best
Response flow was applied to solve convex optimization problems regularized by
the relative entropy with respect to arbitrary reference measures, and with
arbitrary values of the regularization parameter. Our results explain precisely
how the assumption of convexity can be relaxed, at the expense of making a
specific choice of the regularizer. Additionally, we demonstrate how these
results can be applied in reinforcement learning in the context of policy
optimization for Markov Decision Processes and Markov games with softmax
parametrized policies in the mean-field regime.

</details>


### [320] [Improved Last-Iterate Convergence of Shuffling Gradient Methods for Nonsmooth Convex Optimization](https://arxiv.org/abs/2505.23056)
*Zijian Liu,Zhengyuan Zhou*

Key words: 随机洗牌梯度方法, 非光滑凸优化, 尾迭代收敛, Random Reshuffle, Single Shuffle, 近端梯度下降

<details>
  <summary>Details</summary>

Main category: math.OC

TL;DR: 该论文研究了随机洗牌（RR）和单次洗牌（SS）梯度方法在非光滑凸优化中的收敛性，证明其速度优于近端梯度下降（Proximal GD），并首次在一般凸情况下为RR采样方案的尾平均提供了接近最优的收敛结果。

Motivation: 尽管随机洗牌梯度方法在实际应用中表现优异且易于实现，其理论理解仍有限。本文旨在填补这一空白，尤其是在非光滑凸优化中，证明RR和SS策略比近端梯度下降更快。

Method: 通过改进的尾迭代分析，对比Random Reshuffle（RR）和Single Shuffle（SS）策略在非光滑凸函数优化中的性能，并与Proximal GD进行比较。

Result: 证明了RR和SS在非光滑凸优化中的收敛速度优于Proximal GD，并为RR采样方案的尾平均提供了首次（接近）最优的收敛结果，与已知下界匹配。

Conclusion: 随机洗牌梯度方法在非光滑凸优化中具有显著优势，其收敛速度优于传统方法，且理论分析进一步支持了其有效性。

Abstract: We study the convergence of the shuffling gradient method, a popular
algorithm employed to minimize the finite-sum function with regularization, in
which functions are passed to apply (Proximal) Gradient Descent (GD) one by one
whose order is determined by a permutation on the indices of functions. In
contrast to its easy implementation and effective performance in practice, the
theoretical understanding remains limited. A recent advance by (Liu & Zhou,
2024b) establishes the first last-iterate convergence results under various
settings, especially proving the optimal rates for smooth (strongly) convex
optimization. However, their bounds for nonsmooth (strongly) convex functions
are only as fast as Proximal GD. In this work, we provide the first improved
last-iterate analysis for the nonsmooth case demonstrating that the widely used
Random Reshuffle ($\textsf{RR}$) and Single Shuffle ($\textsf{SS}$) strategies
are both provably faster than Proximal GD, reflecting the benefit of
randomness. As an important implication, we give the first (nearly) optimal
convergence result for the suffix average under the $\textsf{RR}$ sampling
scheme in the general convex case, matching the lower bound shown by (Koren et
al., 2022).

</details>


### [321] [Gradient Methods with Online Scaling Part I. Theoretical Foundations](https://arxiv.org/abs/2505.23081)
*Wenzhi Gao,Ya-Chi Chu,Yinyu Ye,Madeleine Udell*

Key words: 在线学习、梯度方法、收敛分析、超线性收敛、优化算法

<details>
  <summary>Details</summary>

Main category: math.OC

TL;DR: 该论文建立了在线缩放梯度方法(OSGM)的理论基础，通过在线学习调整步长以加速一阶优化方法，并提供收敛保证和超线性收敛性能。

Motivation: 为了解决传统一阶优化方法中步长选择的问题，提出利用在线学习自适应调整步长以提高收敛速度。

Method: OSGM利用基于收敛度量的反馈函数评估步长效果，并通过在线学习算法动态调整步长。

Result: OSGM实现了平滑凸问题上的收敛保证，包括轨迹依赖的全局收敛、改进的强凸问题复杂性结果，以及局部超线性收敛性能。

Conclusion: OSGM是一个具有非渐近超线性收敛性能的新型一阶方法家族，并解释了超梯度下降在机器学习优化中的成功经验。

Abstract: This paper establishes the theoretical foundations of the online scaled
gradient methods (OSGM), a framework that utilizes online learning to adapt
stepsizes and provably accelerate first-order methods. OSGM quantifies the
effectiveness of a stepsize by a feedback function motivated from a convergence
measure and uses the feedback to adjust the stepsize through an online learning
algorithm. Consequently, instantiations of OSGM achieve convergence rates that
are asymptotically no worse than the optimal stepsize. OSGM yields desirable
convergence guarantees on smooth convex problems, including 1)
trajectory-dependent global convergence on smooth convex objectives; 2) an
improved complexity result on smooth strongly convex problems, and 3) local
superlinear convergence. Notably, OSGM constitutes a new family of first-order
methods with non-asymptotic superlinear convergence, joining the celebrated
quasi-Newton methods. Finally, OSGM explains the empirical success of the
popular hypergradient-descent heuristic in optimization for machine learning.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [322] [DeepMultiConnectome: Deep Multi-Task Prediction of Structural Connectomes Directly from Diffusion MRI Tractography](https://arxiv.org/abs/2505.22685)
*Marcus J. Vroemen,Yuqian Chen,Yui Lo,Tengfei Xu,Weidong Cai,Fan Zhang,Josien P. W. Pluim,Lauren J. O'Donnell*

Key words: 扩散MRI, 连接组, 深度学习, 多任务学习, 点云网络

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: DeepMultiConnectome，一种深度学习模型，可直接从扩散MRI追踪预测结构连接组，绕过灰质分割，快速生成多种分割方案的连接组。

Motivation: 传统连接组生成耗时且依赖灰质分割，难以应用于大规模研究。需要一种快速、灵活的替代方法。

Method: 使用基于点云的神经网络和多任务学习，模型分类流线并支持多种分割方案，共享学习表示。

Result: 预测的连接组与传统方法高度相关（r=0.992和0.986），保留网络特性，且测试-重测结果可比。预测年龄和认知功能的表现相似。

Conclusion: DeepMultiConnectome是一种可扩展、快速的模型，适用于多种分割方案的个性化连接组生成。

Abstract: Diffusion MRI (dMRI) tractography enables in vivo mapping of brain structural
connections, but traditional connectome generation is time-consuming and
requires gray matter parcellation, posing challenges for large-scale studies.
We introduce DeepMultiConnectome, a deep-learning model that predicts
structural connectomes directly from tractography, bypassing the need for gray
matter parcellation while supporting multiple parcellation schemes. Using a
point-cloud-based neural network with multi-task learning, the model classifies
streamlines according to their connected regions across two parcellation
schemes, sharing a learned representation. We train and validate
DeepMultiConnectome on tractography from the Human Connectome Project Young
Adult dataset ($n = 1000$), labeled with an 84 and 164 region gray matter
parcellation scheme. DeepMultiConnectome predicts multiple structural
connectomes from a whole-brain tractogram containing 3 million streamlines in
approximately 40 seconds. DeepMultiConnectome is evaluated by comparing
predicted connectomes with traditional connectomes generated using the
conventional method of labeling streamlines using a gray matter parcellation.
The predicted connectomes are highly correlated with traditionally generated
connectomes ($r = 0.992$ for an 84-region scheme; $r = 0.986$ for a 164-region
scheme) and largely preserve network properties. A test-retest analysis of
DeepMultiConnectome demonstrates reproducibility comparable to traditionally
generated connectomes. The predicted connectomes perform similarly to
traditionally generated connectomes in predicting age and cognitive function.
Overall, DeepMultiConnectome provides a scalable, fast model for generating
subject-specific connectomes across multiple parcellation schemes.

</details>


### [323] [Synthetic Generation and Latent Projection Denoising of Rim Lesions in Multiple Sclerosis](https://arxiv.org/abs/2505.23353)
*Alexandra G. Roberts,Ha M. Luu,Mert Şişman,Alexey V. Dimov,Ceren Tozlu,Ilhami Kovanlikaya,Susan A. Gauthier,Thanh D. Nguyen,Yi Wang*

Key words: 定量磁化率图, 多发性硬化症, 生成对抗网络, 病变检测, 类别不平衡

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 该论文提出了一种合成定量磁化率图的方法，用于改善多发性硬化症中罕见病变（PRLs）的分类器性能，并提出了一种去噪方法来增加少数类样本，从而提升检测效果。

Motivation: 多发性硬化症中的PRLs病变因其稀有性导致类别不平衡问题，传统方法难以有效检测和分割。论文旨在通过合成数据和去噪方法解决这一问题。

Method: 使用生成对抗网络（GAN）合成定量磁化率图，并扩展多通道对比和概率分割图；提出基于生成网络的去噪方法，增加少数类样本。

Result: 合成数据和去噪方法显著改善了PRLs的检测性能，更接近真实病变分布，提升了分类器的临床可解释性。

Conclusion: 合成PRLs数据和去噪技术的结合为多发性硬化症病变检测提供了一种有效解决方案，具有临床实用价值。

Abstract: Quantitative susceptibility maps from magnetic resonance images can provide
both prognostic and diagnostic information in multiple sclerosis, a
neurodegenerative disease characterized by the formation of lesions in white
matter brain tissue. In particular, susceptibility maps provide adequate
contrast to distinguish between "rim" lesions, surrounded by deposited
paramagnetic iron, and "non-rim" lesion types. These paramagnetic rim lesions
(PRLs) are an emerging biomarker in multiple sclerosis. Much effort has been
devoted to both detection and segmentation of such lesions to monitor
longitudinal change. As paramagnetic rim lesions are rare, addressing this
problem requires confronting the class imbalance between rim and non-rim
lesions. We produce synthetic quantitative susceptibility maps of paramagnetic
rim lesions and show that inclusion of such synthetic data improves classifier
performance and provide a multi-channel extension to generate accompanying
contrasts and probabilistic segmentation maps. We exploit the projection
capability of our trained generative network to demonstrate a novel denoising
approach that allows us to train on ambiguous rim cases and substantially
increase the minority class. We show that both synthetic lesion synthesis and
our proposed rim lesion label denoising method best approximate the unseen rim
lesion distribution and improve detection in a clinically interpretable manner.
We release our code and generated data at https://github.com/agr78/PRLx-GAN
upon publication.

</details>


### [324] [Can Large Language Models Challenge CNNS in Medical Image Analysis?](https://arxiv.org/abs/2505.23503)
*Shibbir Ahmed,Shahnewaz Karim Sakib,Anindya Bijoy Das*

Key words: 多模态AI, 医学影像诊断, CNN, LLM, 性能比较

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 该研究提出了一种多模态AI框架，用于精确分类医学诊断图像，比较了CNN和LLM的性能差异。

Motivation: 探索如何在医学影像诊断中结合CNN和LLM的优势，以提高诊断的可靠性和效率。

Method: 使用公开数据集，对比CNN和多种LLM的性能，评估指标包括准确率、F1分数、执行时间、能耗和二氧化碳排放。

Result: 结果表明，虽然CNN模型在性能上优于多模态技术，但在LLM上应用额外过滤可以显著提升效果。

Conclusion: 多模态AI系统有潜力提升医学诊断的可靠性、效率和可扩展性。

Abstract: This study presents a multimodal AI framework designed for precisely
classifying medical diagnostic images. Utilizing publicly available datasets,
the proposed system compares the strengths of convolutional neural networks
(CNNs) and different large language models (LLMs). This in-depth comparative
analysis highlights key differences in diagnostic performance, execution
efficiency, and environmental impacts. Model evaluation was based on accuracy,
F1-score, average execution time, average energy consumption, and estimated
$CO_2$ emission. The findings indicate that although CNN-based models can
outperform various multimodal techniques that incorporate both images and
contextual information, applying additional filtering on top of LLMs can lead
to substantial performance gains. These findings highlight the transformative
potential of multimodal AI systems to enhance the reliability, efficiency, and
scalability of medical diagnostics in clinical settings.

</details>


### [325] [PCA for Enhanced Cross-Dataset Generalizability in Breast Ultrasound Tumor Segmentation](https://arxiv.org/abs/2505.23587)
*Christian Schmidt,Heinrich Martin Overhoff*

Key words: 医学图像分割, PCA, 外部有效性, 超声图像, U-Net

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 该论文提出了一种基于主成分分析（PCA）的新方法，用于解决医学图像分割模型在未见数据集上的外部有效性不足问题。实验结果表明，PCA预处理能显著提高召回率和Dice分数。

Motivation: 医学图像分割模型在跨数据集部署时外部有效性不足，尤其在超声图像领域。现有方法如域适应和GAN风格迁移在小规模多样化医学数据上效果有限。

Method: 采用PCA对图像进行预处理，保留90%的方差以降噪并突出关键特征。在6个乳腺肿瘤超声数据集上训练U-Net模型，并对比原始图像与PCA重构数据的性能。

Result: 使用PCA重构数据的模型在召回率（0.70 vs. 0.57）和Dice分数（0.58 vs. 0.50）上表现显著更好，且外部验证导致的召回率下降减少了33%。

Conclusion: PCA重构可有效缓解分割性能下降，提升模型在真实医学应用中的外部有效性。

Abstract: In medical image segmentation, limited external validity remains a critical
obstacle when models are deployed across unseen datasets, an issue particularly
pronounced in the ultrasound image domain. Existing solutions-such as domain
adaptation and GAN-based style transfer-while promising, often fall short in
the medical domain where datasets are typically small and diverse. This paper
presents a novel application of principal component analysis (PCA) to address
this limitation. PCA preprocessing reduces noise and emphasizes essential
features by retaining approximately 90\% of the dataset variance. We evaluate
our approach across six diverse breast tumor ultrasound datasets comprising
3,983 B-mode images and corresponding expert tumor segmentation masks. For each
dataset, a corresponding dimensionality reduced PCA-dataset is created and
U-Net-based segmentation models are trained on each of the twelve datasets.
Each model trained on an original dataset was inferenced on the remaining five
out-of-domain original datasets (baseline results), while each model trained on
a PCA dataset was inferenced on five out-of-domain PCA datasets. Our
experimental results indicate that using PCA reconstructed datasets, instead of
original images, improves the model's recall and Dice scores, particularly for
model-dataset pairs where baseline performance was lowest, achieving
statistically significant gains in recall (0.57 $\pm$ 0.07 vs. 0.70 $\pm$ 0.05,
$p = 0.0004$) and Dice scores (0.50 $\pm$ 0.06 vs. 0.58 $\pm$ 0.06, $p =
0.03$). Our method reduced the decline in recall values due to external
validation by $33\%$. These findings underscore the potential of PCA
reconstruction as a safeguard to mitigate declines in segmentation performance,
especially in challenging cases, with implications for enhancing external
validity in real-world medical applications.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [326] [An Efficient deep learning model to Predict Stock Price Movement Based on Limit Order Book](https://arxiv.org/abs/2505.22678)
*Jiahao Yang,Ran Fang,Ming Zhang,Jun Zhou*

Key words: 高频交易, 限价订单簿, Siamese架构, 多头注意力机制, LSTM

<details>
  <summary>Details</summary>

Main category: q-fin.TR

TL;DR: 该论文提出了一种基于Siamese架构的新方法，通过分别处理买卖盘数据并共享参数，提升了现有深度学习模型在高频交易中预测股票价格的表现。结合多头注意力机制和LSTM，实验证明该方法在75%以上的案例中优于基线模型。

Motivation: 高频交易中利用限价订单簿（LOB）建模股票价格变动具有挑战性，传统深度学习模型难以有效捕捉价格模式，尤其是缺乏良好设计特征时。原始LOB数据的买卖盘对称性及价差稳定性为改进提供了契机。

Method: 提出基于Siamese架构的方法，分别处理买卖盘数据并使用共享参数模块；结合多头注意力机制（MHA）与LSTM模块，验证其对价格建模的作用。实验使用原始数据和订单流不平衡（OFI）特征作为输入。

Result: 该方法在75%以上的案例中提升了基线模型性能（MLP除外）。多头注意力机制在短期预测中表现尤为突出。

Conclusion: Siamese架构和MHA的结合能有效提升高频交易中价格预测的准确性，尤其适用于短期预测场景。

Abstract: In high-frequency trading (HFT), leveraging limit order books (LOB) to model
stock price movements is crucial for achieving profitable outcomes. However,
this task is challenging due to the high-dimensional and volatile nature of the
original data. Even recent deep learning models often struggle to capture price
movement patterns effectively, particularly without well-designed features. We
observed that raw LOB data exhibits inherent symmetry between the ask and bid
sides, and the bid-ask differences demonstrate greater stability and lower
complexity compared to the original data. Building on this insight, we propose
a novel approach in which leverages the Siamese architecture to enhance the
performance of existing deep learning models. The core idea involves processing
the ask and bid sides separately using the same module with shared parameters.
We applied our Siamese-based methods to several widely used strong baselines
and validated their effectiveness using data from 14 military industry stocks
in the Chinese A-share market. Furthermore, we integrated multi-head attention
(MHA) mechanisms with the Long Short-Term Memory (LSTM) module to investigate
its role in modeling stock price movements. Our experiments used raw data and
widely used Order Flow Imbalance (OFI) features as input with some strong
baseline models. The results show that our method improves the performance of
strong baselines in over 75$% of cases, excluding the Multi-Layer Perception
(MLP) baseline, which performed poorly and is not considered practical.
Furthermore, we found that Multi-Head Attention can enhance model performance,
particularly over shorter forecasting horizons.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [327] [BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural Speech Synthesis with Flow Matching Models](https://arxiv.org/abs/2505.22865)
*Susan Liang,Dejan Markovic,Israel D. Gebru,Steven Krenn,Todd Keebler,Jacob Sandakly,Frank Yu,Samuel Hassel,Chenliang Xu,Alexander Richard*

Key words: 双耳渲染, 流式推理, 条件流匹配, 因果U-Net, 音频合成

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: BinauralFlow提出了基于流匹配的流式双耳语音合成框架，通过条件流匹配模型和因果U-Net架构解决双耳渲染的质量和流式推理问题。

Motivation: 传统方法在双耳音频渲染质量和流式推理上表现不佳，需要精确建模双耳线索、房间混响和环境音，同时满足实时应用需求。

Method: 提出条件流匹配模型和因果U-Net架构，结合流式STFT/ISTFT操作、缓冲库、中点求解器和提前跳转策略，实现高质量流式合成。

Result: 定量和定性评估显示方法优于现有技术，感知研究表明其与真实录音的混淆率达42%。

Conclusion: BinauralFlow在双耳渲染质量和流式推理上表现优异，接近真实录音水平。

Abstract: Binaural rendering aims to synthesize binaural audio that mimics natural
hearing based on a mono audio and the locations of the speaker and listener.
Although many methods have been proposed to solve this problem, they struggle
with rendering quality and streamable inference. Synthesizing high-quality
binaural audio that is indistinguishable from real-world recordings requires
precise modeling of binaural cues, room reverb, and ambient sounds.
Additionally, real-world applications demand streaming inference. To address
these challenges, we propose a flow matching based streaming binaural speech
synthesis framework called BinauralFlow. We consider binaural rendering to be a
generation problem rather than a regression problem and design a conditional
flow matching model to render high-quality audio. Moreover, we design a causal
U-Net architecture that estimates the current audio frame solely based on past
information to tailor generative models for streaming inference. Finally, we
introduce a continuous inference pipeline incorporating streaming STFT/ISTFT
operations, a buffer bank, a midpoint solver, and an early skip schedule to
improve rendering continuity and speed. Quantitative and qualitative
evaluations demonstrate the superiority of our method over SOTA approaches. A
perceptual study further reveals that our model is nearly indistinguishable
from real-world recordings, with a $42\%$ confusion rate.

</details>


### [328] [Patient Domain Supervised Contrastive Learning for Lung Sound Classification Using Mobile Phone](https://arxiv.org/abs/2505.23132)
*Seung Gyu Jeong,Seong Eun Kim*

Key words: 肺音诊断、智能手机、PD-SCL、AST模型、COVID-19

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 利用智能手机麦克风记录和分析肺音，结合PD-SCL和AST模型，性能提升2.4%，证明了智能手机在肺音诊断中的潜力。

Motivation: 疫情暴露了传统肺音诊断的局限性，推动了数字听诊器和AI的发展，旨在通过智能手机技术提高诊断可及性。

Method: 采用Patient Domain Supervised Contrastive Learning (PD-SCL)方法与Audio Spectrogram Transformer (AST)模型结合。

Result: 性能提升2.4%，智能手机肺音诊断效果显著。

Conclusion: 智能手机可用于高效肺音诊断，适应后疫情时代需求。

Abstract: Auscultation is crucial for diagnosing lung diseases. The COVID-19 pandemic
has revealed the limitations of traditional, in-person lung sound assessments.
To overcome these issues, advancements in digital stethoscopes and artificial
intelligence (AI) have led to the development of new diagnostic methods. In
this context, our study aims to use smartphone microphones to record and
analyze lung sounds. We faced two major challenges: the difference in audio
style between electronic stethoscopes and smartphone microphones, and the
variability among patients. To address these challenges, we developed a method
called Patient Domain Supervised Contrastive Learning (PD-SCL). By integrating
this method with the Audio Spectrogram Transformer (AST) model, we
significantly improved its performance by 2.4\% compared to the original AST
model. This progress demonstrates that smartphones can effectively diagnose
lung sounds, addressing inconsistencies in patient data and showing potential
for broad use beyond traditional clinical settings. Our research contributes to
making lung disease detection more accessible in the post-COVID-19 world.

</details>


### [329] [Nosey: Open-source hardware for acoustic nasalance](https://arxiv.org/abs/2505.23339)
*Maya Dewhurst,Jack Collins,Justin J. H. Lo,Roy Alderton,Sam Kirkham*

Key words: Nosey, 开源硬件, 鼻音测量, 3D打印, 低成本

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: Nosey 是一种低成本、可定制的开源硬件系统，用于记录声学鼻音数据，表现与商业设备相当但成本更低。

Motivation: 开发一种低成本且灵活的开源硬件系统，替代昂贵的商业鼻音测量设备。

Method: 设计并3D打印开源硬件系统Nosey，与商业设备进行性能对比，并探讨硬件定制选项。

Result: Nosey 的鼻音评分高于商业设备，但不同语音环境的对比幅度相似；系统支持麦克风和材料等定制。

Conclusion: Nosey 是商业设备的低成本灵活替代方案，适合数据收集，并提出了使用建议。

Abstract: We introduce Nosey (Nasalance Open Source Estimation sYstem), a low-cost,
customizable, 3D-printed system for recording acoustic nasalance data that we
have made available as open-source hardware
(http://github.com/phoneticslab/nosey). We first outline the motivations and
design principles behind our hardware nasalance system, and then present a
comparison between Nosey and a commercial nasalance device. Nosey shows
consistently higher nasalance scores than the commercial device, but the
magnitude of contrast between phonological environments is comparable between
systems. We also review ways of customizing the hardware to facilitate testing,
such as comparison of microphones and different construction materials. We
conclude that Nosey is a flexible and cost-effective alternative to commercial
nasometry devices and propose some methodological considerations for its use in
data collection.

</details>


### [330] [Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM](https://arxiv.org/abs/2505.23207)
*Zhaokai Sun,Li Zhang,Qing Wang,Pan Zhou,Lei Xie*

Key words: 重叠语音检测, 渐进式训练, 自监督学习, 说话人关注模块

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 该论文提出了一个基于说话人感知的渐进式重叠语音检测模型，通过结合自监督学习和说话人关注模块，显著提升了检测性能。

Motivation: 解决多说话人语音处理中的重叠语音区域检测问题，提升检测准确率和鲁棒性。

Method: 采用渐进式训练策略，结合WavLM和wav2vec 2.0等自监督学习模型，并引入说话人关注模块增强声学表征。

Result: 在AMI测试集上F1得分达到82.76\(%\), 达到当前最优水平。

Conclusion: 该模型在重叠语音检测任务中表现出色，验证了渐进式训练和说话人信息的有效性。

Abstract: Overlapping Speech Detection (OSD) aims to identify regions where multiple
speakers overlap in a conversation, a critical challenge in multi-party speech
processing. This work proposes a speaker-aware progressive OSD model that
leverages a progressive training strategy to enhance the correlation between
subtasks such as voice activity detection (VAD) and overlap detection. To
improve acoustic representation, we explore the effectiveness of
state-of-the-art self-supervised learning (SSL) models, including WavLM and
wav2vec 2.0, while incorporating a speaker attention module to enrich features
with frame-level speaker information. Experimental results show that the
proposed method achieves state-of-the-art performance, with an F1 score of
82.76\% on the AMI test set, demonstrating its robustness and effectiveness in
OSD.

</details>


### [331] [MGE-LDM: Joint Latent Diffusion for Simultaneous Music Generation and Source Extraction](https://arxiv.org/abs/2505.23305)
*Yunkee Chae,Kyogu Lee*

Key words: 音乐生成, 潜在扩散模型, 音乐源分离, 源补全, 条件修复

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: MGE-LDM 是一个统一的潜在扩散框架，能够同时进行音乐生成、源补全和查询驱动的源分离，支持灵活的、类别无关的任意乐器源操作。

Motivation: 现有方法通常局限于固定的乐器类别，而 MGE-LDM 旨在通过单一模型学习混合音乐、子混合和单独音轨的联合分布，从而实现更灵活的音乐处理。

Method: 通过将分离和补全任务建模为潜在空间中的条件修复任务，MGE-LDM 可以在异构多轨数据集上联合训练，无需依赖预定义的乐器类别。

Result: 该方法支持（1）完整混合生成，（2）部分生成（源补全），（3）基于文本的任意源提取，并在多个数据集上验证了其有效性。

Conclusion: MGE-LDM 提供了一种统一的框架，能够灵活处理音乐生成、补全和分离任务，同时摆脱了传统方法对固定乐器类别的依赖。

Abstract: We present MGE-LDM, a unified latent diffusion framework for simultaneous
music generation, source imputation, and query-driven source separation. Unlike
prior approaches constrained to fixed instrument classes, MGE-LDM learns a
joint distribution over full mixtures, submixtures, and individual stems within
a single compact latent diffusion model. At inference, MGE-LDM enables (1)
complete mixture generation, (2) partial generation (i.e., source imputation),
and (3) text-conditioned extraction of arbitrary sources. By formulating both
separation and imputation as conditional inpainting tasks in the latent space,
our approach supports flexible, class-agnostic manipulation of arbitrary
instrument sources. Notably, MGE-LDM can be trained jointly across
heterogeneous multi-track datasets (e.g., Slakh2100, MUSDB18, MoisesDB) without
relying on predefined instrument categories. Audio samples are available at our
project page: https://yoongi43.github.io/MGELDM_Samples/.

</details>


### [332] [Spectrotemporal Modulation: Efficient and Interpretable Feature Representation for Classifying Speech, Music, and Environmental Sounds](https://arxiv.org/abs/2505.23509)
*Andrew Chang,Yike Li,Iran R. Roman,David Poeppel*

Key words: 音频分类, 频谱时域调制, 可解释性, 机器听觉, 神经生理表示

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 该论文提出了一种基于频谱时域调制（STM）特征的新方法，用于音频分类，其性能可与预训练的音频深度神经网络相媲美，同时更具计算效率和可解释性。

Motivation: 现有音频深度神经网络（DNNs）虽然性能出色，但计算成本高且缺乏可解释性。作者希望通过模拟人类听觉皮层的神经生理表示，提供一种更高效的替代方案。

Method: 提出使用频谱时域调制（STM）特征，这是一种模仿人类听觉皮层信号处理的表示方法。模型未经过预训练，直接用于分类任务。

Result: STM模型在自然语音、音乐和环境声音等多样音频分类任务中表现与预训练DNNs相当，同时更高效且可解释。

Conclusion: STM是一种高效且可解释的音频特征表示方法，为机器听觉、语音科学和认知计算开辟了新的可能性。

Abstract: Audio DNNs have demonstrated impressive performance on various machine
listening tasks; however, most of their representations are computationally
costly and uninterpretable, leaving room for optimization. Here, we propose a
novel approach centered on spectrotemporal modulation (STM) features, a signal
processing method that mimics the neurophysiological representation in the
human auditory cortex. The classification performance of our STM-based model,
without any pretraining, is comparable to that of pretrained audio DNNs across
diverse naturalistic speech, music, and environmental sounds, which are
essential categories for both human cognition and machine perception. These
results show that STM is an efficient and interpretable feature representation
for audio classification, advancing the development of machine listening and
unlocking exciting new possibilities for basic understanding of speech and
auditory sciences, as well as developing audio BCI and cognitive computing.

</details>


### [333] [Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes](https://arxiv.org/abs/2505.23619)
*Neta Glazer,David Chernin,Idan Achituve,Sharon Gannot,Ethan Fetaya*

Key words: 音频深度伪造检测、高斯过程、少样本学习、语音克隆、TTS模型

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 该论文提出了ADD-GP框架，一种基于高斯过程的少样本自适应方法，用于音频深度伪造检测，展示了对新兴TTS模型的高适应性和鲁棒性。

Motivation: 随着语音克隆技术的进步，亟需能高效适配新型TTS模型的深度伪造检测方法，以应对数据稀缺场景。

Method: 结合深度嵌入模型与高斯过程分类器，构建了少样本自适应框架ADD-GP，支持个性化检测与单样本适配。

Result: 该方法在新构建的基准数据集上表现优异，对未知TTS模型展现出强适应性和鲁棒性。

Conclusion: ADD-GP为音频深度伪造检测提供了高效、灵活的解决方案，尤其适用于少样本及新模型场景。

Abstract: Recent advancements in Text-to-Speech (TTS) models, particularly in voice
cloning, have intensified the demand for adaptable and efficient deepfake
detection methods. As TTS systems continue to evolve, detection models must be
able to efficiently adapt to previously unseen generation models with minimal
data. This paper introduces ADD-GP, a few-shot adaptive framework based on a
Gaussian Process (GP) classifier for Audio Deepfake Detection (ADD). We show
how the combination of a powerful deep embedding model with the Gaussian
processes flexibility can achieve strong performance and adaptability.
Additionally, we show this approach can also be used for personalized
detection, with greater robustness to new TTS models and one-shot adaptability.
To support our evaluation, a benchmark dataset is constructed for this task
using new state-of-the-art voice cloning models.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [334] [TailorSQL: An NL2SQL System Tailored to Your Query Workload](https://arxiv.org/abs/2505.23039)
*Kapil Vaidya,Jialin Ding,Sebastian Kosak,David Kernert,Chuan Lei,Xiao Qin,Abhinav Tripathy,Ramesh Balan,Balakrishnan Narayanaswamy,Tim Kraska*

Key words: NL2SQL, TailorSQL, 查询负载, SQL翻译, 大语言模型

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: TailorSQL利用过去查询负载中的信息优化NL2SQL翻译，实现了2倍的执行准确率提升。

Motivation: 现有的NL2SQL技术未利用数据库中已有的历史查询负载信息，这些信息可提升翻译准确率和效率。

Method: TailorSQL通过分析历史查询负载（如常见连接路径和表列语义）来优化NL2SQL翻译。

Result: 在标准基准测试中，TailorSQL的执行准确率提升高达2倍。

Conclusion: 利用历史查询负载可以显著提升NL2SQL系统的性能和准确性。

Abstract: NL2SQL (natural language to SQL) translates natural language questions into
SQL queries, thereby making structured data accessible to non-technical users,
serving as the foundation for intelligent data applications. State-of-the-art
NL2SQL techniques typically perform translation by retrieving database-specific
information, such as the database schema, and invoking a pre-trained large
language model (LLM) using the question and retrieved information to generate
the SQL query.
  However, existing NL2SQL techniques miss a key opportunity which is present
in real-world settings: NL2SQL is typically applied on existing databases which
have already served many SQL queries in the past. The past query workload
implicitly contains information which is helpful for accurate NL2SQL
translation and is not apparent from the database schema alone, such as common
join paths and the semantics of obscurely-named tables and columns. We
introduce TailorSQL, a NL2SQL system that takes advantage of information in the
past query workload to improve both the accuracy and latency of translating
natural language questions into SQL. By specializing to a given workload,
TailorSQL achieves up to 2$\times$ improvement in execution accuracy on
standardized benchmarks.

</details>


### [335] [Towards Explainable Sequential Learning](https://arxiv.org/abs/2505.23624)
*Giacomo Bergami,Emma Packer,Kirsty Scott,Silvia Del Din*

Key words: 可解释性AI, 时间序列分类, 混合方法, 多变量数据, 规范挖掘

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: 该论文提出了一种混合解释性时间数据处理方法EMeriTAte+DF，结合数值驱动和事件驱动分类，通过可解释性AI原理生成人可理解的结果。

Motivation: 解决现有多变量时间序列分类方法缺乏解释性的问题，通过结合数值和事件驱动的方法提升分类效果与可解释性。

Method: 设计了扩展事件驱动的方法，支持并发生成规范挖掘算法，并引入可解释性后处理阶段以描述输入数据的构成。

Result: 提出的方法在多变量时间序列分类任务中优于现有技术，证明了其有效性。

Conclusion: 该方法不仅提升了分类性能，还增强了结果的可解释性，展示了混合方法的潜力。

Abstract: This paper offers a hybrid explainable temporal data processing pipeline,
DataFul Explainable MultivariatE coRrelatIonal Temporal Artificial inTElligence
(EMeriTAte+DF), bridging numerical-driven temporal data classification with an
event-based one through verified artificial intelligence principles, enabling
human-explainable results. This was possible through a preliminary a posteriori
explainable phase describing the numerical input data in terms of concurrent
constituents with numerical payloads. This further required extending the
event-based literature to design specification mining algorithms supporting
concurrent constituents. Our previous and current solutions outperform
state-of-the-art solutions for multivariate time series classifications, thus
showcasing the effectiveness of the proposed methodology.

</details>


### [336] [KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction](https://arxiv.org/abs/2505.23416)
*Jang-Hyun Kim,Jinuk Kim,Sangwoo Kwon,Jae W. Lee,Sangdoo Yun,Hyun Oh Song*

Key words: KV缓存, Transformer, 大语言模型, 内存优化, 注意力延迟

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: KVzip是一种查询无关的KV缓存淘汰方法，通过量化KV对的重要性并压缩缓存，显著减少了内存开销和注意力延迟，在多种任务中性能损失可忽略。

Motivation: 随着上下文长度增长，Transformer大模型的KV缓存内存开销和注意力延迟问题日益突出，急需高效的缓存管理方法。

Method: KVzip利用底层LLM量化KV对的重要性，并基于重要性淘汰低价值KV对，实现缓存的压缩和复用。

Result: 实验表明，KVzip将KV缓存大小减少3-4倍，解码延迟降低约2倍，在多任务中性能损失极小。

Conclusion: KVzip在长上下文场景下显著优于现有查询感知方法，能高效管理缓存资源。

Abstract: Transformer-based large language models (LLMs) cache context as key-value
(KV) pairs during inference. As context length grows, KV cache sizes expand,
leading to substantial memory overhead and increased attention latency. This
paper introduces KVzip, a query-agnostic KV cache eviction method enabling
effective reuse of compressed KV caches across diverse queries. KVzip
quantifies the importance of a KV pair using the underlying LLM to reconstruct
original contexts from cached KV pairs, subsequently evicting pairs with lower
importance. Extensive empirical evaluations demonstrate that KVzip reduces KV
cache size by 3-4$\times$ and FlashAttention decoding latency by approximately
2$\times$, with negligible performance loss in question-answering, retrieval,
reasoning, and code comprehension tasks. Evaluations include various models
such as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching
up to 170K tokens. KVzip significantly outperforms existing query-aware KV
eviction methods, which suffer from performance degradation even at a 90% cache
budget ratio under multi-query scenarios.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [337] [PSBench: a large-scale benchmark for estimating the accuracy of protein complex structural models](https://arxiv.org/abs/2505.22674)
*Pawan Neupane,Jian Liu,Jianlin Cheng*

Key words: 蛋白质复合物结构预测、质量评估、基准数据集、机器学习、CASP

<details>
  <summary>Details</summary>

Main category: q-bio.BM

TL;DR: 论文提出了PSBench，一个用于蛋白质复合物结构质量评估的基准套件，包含大规模标注数据集和评估方法，并展示其有效性。

Motivation: 现有蛋白质复合物结构预测模型的质量评估方法缺乏大规模、多样化且标注良好的数据集，限制了机器学习方法的开发。

Method: 引入PSBench，包含四个大型标注数据集，支持全局、局部和接口级别的质量评估，并提供基线方法。

Result: 基于PSBench训练的GATE方法在CASP16中表现优异，证明了PSBench的实用性。

Conclusion: PSBench是推动蛋白质复合物建模中质量评估研究的重要资源。

Abstract: Predicting protein complex structures is essential for protein function
analysis, protein design, and drug discovery. While AI methods like AlphaFold
can predict accurate structural models for many protein complexes, reliably
estimating the quality of these predicted models (estimation of model accuracy,
or EMA) for model ranking and selection remains a major challenge. A key
barrier to developing effective machine learning-based EMA methods is the lack
of large, diverse, and well-annotated datasets for training and evaluation. To
address this gap, we introduce PSBench, a benchmark suite comprising four
large-scale, labeled datasets generated during the 15th and 16th community-wide
Critical Assessment of Protein Structure Prediction (CASP15 and CASP16).
PSBench includes over one million structural models covering a wide range of
protein sequence lengths, complex stoichiometries, functional classes, and
modeling difficulties. Each model is annotated with multiple complementary
quality scores at the global, local, and interface levels. PSBench also
provides multiple evaluation metrics and baseline EMA methods to facilitate
rigorous comparisons. To demonstrate PSBench's utility, we trained and
evaluated GATE, a graph transformer-based EMA method, on the CASP15 data. GATE
was blindly tested in CASP16 (2024), where it ranked among the top-performing
EMA methods. These results highlight PSBench as a valuable resource for
advancing EMA research in protein complex modeling. PSBench is publicly
available at: https://github.com/BioinfoMachineLearning/PSBench.

</details>


### [338] [Representing local protein environments with atomistic foundation models](https://arxiv.org/abs/2505.23354)
*Meital Bojan,Sanketh Vedula,Advaith Maddipatla,Nadav Bojan Sellam,Federico Napoli,Paul Schanda,Alex M. Bronstein*

Key words: 蛋白质环境表示, 原子基础模型, 化学位移预测, 生物分子NMR

<details>
  <summary>Details</summary>

Main category: q-bio.BM

TL;DR: 该论文提出了一种基于原子基础模型（AFM）的局部蛋白质环境表示方法，能够有效捕捉结构和化学特征，并在生物分子NMR光谱中实现了最先进的化学位移预测。

Motivation: 局部蛋白质环境的结构对功能和分子相互作用至关重要，但现有表示方法难以应对其多样性和复杂性，因此需要开发更有效的表示方法。

Method: 利用原子基础模型（AFM）的中间特征生成局部蛋白质环境的嵌入表示，结合物理知识构建化学位移预测器。

Result: AFM嵌入能有效捕捉局部结构和化学特征，其表示空间具有有意义的结构，化学位移预测器达到了最先进水平。

Conclusion: 原子基础模型及其涌现表示在蛋白质建模中表现出色，为功能表示的研究开辟了新方向。

Abstract: The local structure of a protein strongly impacts its function and
interactions with other molecules. Therefore, a concise, informative
representation of a local protein environment is essential for modeling and
designing proteins and biomolecular interactions. However, these environments'
extensive structural and chemical variability makes them challenging to model,
and such representations remain under-explored. In this work, we propose a
novel representation for a local protein environment derived from the
intermediate features of atomistic foundation models (AFMs). We demonstrate
that this embedding effectively captures both local structure (e.g., secondary
motifs), and chemical features (e.g., amino-acid identity and protonation
state). We further show that the AFM-derived representation space exhibits
meaningful structure, enabling the construction of data-driven priors over the
distribution of biomolecular environments. Finally, in the context of
biomolecular NMR spectroscopy, we demonstrate that the proposed representations
enable a first-of-its-kind physics-informed chemical shift predictor that
achieves state-of-the-art accuracy. Our results demonstrate the surprising
effectiveness of atomistic foundation models and their emergent representations
for protein modeling beyond traditional molecular simulations. We believe this
will open new lines of work in constructing effective functional
representations for protein environments.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [339] [In Dialogue with Intelligence: Rethinking Large Language Models as Collective Knowledge](https://arxiv.org/abs/2505.22767)
*Eleni Vasilaki*

Key words: 大型语言模型, 集体知识, 对话, 人机协同, AI理论

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 该论文提出将大语言模型（LLMs）视为集体人类知识（CK）的动态体现，强调通过对话而非静态存储来激发智能，并探讨了细调和人机协同增强的影响。

Motivation: 试图重新理解LLMs，强调其在对话中动态体现人类集体知识的特性，而非仅通过结构、行为或训练数据的视角分析。

Method: 结合神经科学与AI理论，通过与ChatGPT-4的持续互动，研究对话模式的涌现、细调的意涵及人机协同增强的概念。

Result: 提出了一种新视角，用于理解现代AI系统中的交互、表征和能动性，强调了对话在智能激发中的核心作用。

Conclusion: LLMs应被视为动态知识体现，其价值在于人机协作中的知识共创，这对未来AI设计具有重要启示。

Abstract: Large Language Models (LLMs) are typically analysed through architectural,
behavioural, or training-data lenses. This article offers a theoretical and
experiential re-framing: LLMs as dynamic instantiations of Collective human
Knowledge (CK), where intelligence is evoked through dialogue rather than
stored statically. Drawing on concepts from neuroscience and AI, and grounded
in sustained interaction with ChatGPT-4, I examine emergent dialogue patterns,
the implications of fine-tuning, and the notion of co-augmentation: mutual
enhancement between human and machine cognition. This perspective offers a new
lens for understanding interaction, representation, and agency in contemporary
AI systems.

</details>


### [340] [Orca: Browsing at Scale Through User-Driven and AI-Facilitated Orchestration Across Malleable Webpages](https://arxiv.org/abs/2505.22831)
*Peiling Jiang,Haijun Xia*

Key words: AI辅助浏览,跨网页信息综合,用户自主性,信息探索

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 该论文提出了一个名为Orca的新型网页浏览器原型，通过AI技术增强用户跨网页的信息探索、操作和组织能力，减少认知和手动负担，支持更灵活的信息处理和更大的信息获取欲望。

Motivation: 传统浏览器在多标签页环境下难以支持大规模信息的操作和综合，而现有AI系统虽然能自动化网页浏览和信息综合，但削弱了用户自主性和上下文理解。因此，研究旨在探索如何通过AI技术增强用户在跨网页内容交互中的主动性和效率。

Method: 通过文献调研和信息任务分析，结合迭代设计过程，开发了Orca浏览器原型。该浏览器将网页视为可塑材料，支持用户与AI协作操作和动态组织网页内容。

Result: 评估表明，Orca显著提升了用户的信息获取欲望、控制能力以及在更广泛信息环境中灵活处理的能力。

Conclusion: Orca浏览器通过用户驱动的AI辅助交互，有效解决了跨网页信息操作的挑战，同时保持了用户的自主性和灵活性。

Abstract: Web-based activities are fundamentally distributed across webpages. However,
conventional browsers with stacks of tabs fail to support operating and
synthesizing large volumes of information across pages. While recent AI systems
enable fully automated web browsing and information synthesis, they often
diminish user agency and hinder contextual understanding. Therefore, we explore
how AI could instead augment users' interactions with content across webpages
and mitigate cognitive and manual efforts. Through literature on information
tasks and web browsing challenges, and an iterative design process, we present
a rich set of novel interactions with our prototype web browser, Orca.
Leveraging AI, Orca supports user-driven exploration, operation, organization,
and synthesis of web content at scale. To enable browsing at scale, webpages
are treated as malleable materials that humans and AI can collaboratively
manipulate and compose into a malleable, dynamic, and browser-level workspace.
Our evaluation revealed an increased "appetite" for information foraging,
enhanced user control, and more flexibility in sensemaking across a broader
information landscape on the web.

</details>


### [341] [HiLDe: Intentional Code Generation via Human-in-the-Loop Decoding](https://arxiv.org/abs/2505.22906)
*Emmanuel Anaya González,Raven Rothkopf,Sorin Lerner,Nadia Polikarpova*

Key words: AI编程工具, Human-in-the-loop, 代码生成, 软件安全, LLM交互

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 论文提出一种名为Human-in-the-loop Decoding的交互技术，通过HiLDe代码补全助手让用户观察并直接影响LLM的决策，以减少漏洞并更贴合用户需求。

Motivation: AI编程工具虽能提升效率，但可能导致用户过度依赖系统，忽略关键决策，尤其在软件安全领域有严重后果。需一种方法让用户更主动参与决策。

Method: 开发HiLDe代码补全助手，通过高亮LLM关键决策并提供局部替代选项，实现用户对输出的实时干预（Human-in-the-loop Decoding）。

Result: 在18人参与的实验中，相比传统工具，HiLDe显著减少生成代码的漏洞，且更符合用户目标。

Conclusion: Human-in-the-loop Decoding技术能有效增强用户对AI生成代码的控制，提升安全性和需求匹配度。

Abstract: While AI programming tools hold the promise of increasing programmers'
capabilities and productivity to a remarkable degree, they often exclude users
from essential decision-making processes, causing many to effectively "turn off
their brains" and over-rely on solutions provided by these systems. These
behaviors can have severe consequences in critical domains, like software
security. We propose Human-in-the-loop Decoding, a novel interaction technique
that allows users to observe and directly influence LLM decisions during code
generation, in order to align the model's output with their personal
requirements. We implement this technique in HiLDe, a code completion assistant
that highlights critical decisions made by the LLM and provides local
alternatives for the user to explore. In a within-subjects study (N=18) on
security-related tasks, we found that HiLDe led participants to generate
significantly fewer vulnerabilities and better align code generation with their
goals compared to a traditional code completion assistant.

</details>


### [342] [Large Language Models for Depression Recognition in Spoken Language Integrating Psychological Knowledge](https://arxiv.org/abs/2505.22863)
*Yupei Li,Shuaijie Shao,Manuel Milling,Björn W. Schuller*

Key words: 抑郁症检测、大型语言模型、多模态、心理知识、音频特征

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 该论文提出了一种利用大型语言模型（LLMs）进行多模态抑郁症检测的新方法，结合音频特征和心理知识，显著提升了诊断准确性。

Motivation: 抑郁症的诊断通常依赖于非文本线索（如语音和行为），而现有深度神经网络和大语言模型在此方面表现不足，缺乏心理专业知识支持。

Method: 使用DAIC-WOZ数据集，通过预训练模型Wav2Vec提取音频特征，并将其映射到文本型LLMs进行进一步处理；同时提出一种结合心理知识的新策略（问答集授权）。

Result: 相比基线方法，该方法在平均绝对误差（MAE）和均方根误差（RMSE）上均有显著提升。

Conclusion: 多模态LLMs结合领域知识可有效提升抑郁症检测性能，为未来研究提供了新方向。

Abstract: Depression is a growing concern gaining attention in both public discourse
and AI research. While deep neural networks (DNNs) have been used for
recognition, they still lack real-world effectiveness. Large language models
(LLMs) show strong potential but require domain-specific fine-tuning and
struggle with non-textual cues. Since depression is often expressed through
vocal tone and behaviour rather than explicit text, relying on language alone
is insufficient. Diagnostic accuracy also suffers without incorporating
psychological expertise. To address these limitations, we present, to the best
of our knowledge, the first application of LLMs to multimodal depression
detection using the DAIC-WOZ dataset. We extract the audio features using the
pre-trained model Wav2Vec, and mapped it to text-based LLMs for further
processing. We also propose a novel strategy for incorporating psychological
knowledge into LLMs to enhance diagnostic performance, specifically using a
question and answer set to grant authorised knowledge to LLMs. Our approach
yields a notable improvement in both Mean Absolute Error (MAE) and Root Mean
Square Error (RMSE) compared to a base score proposed by the related original
paper. The codes are available at
https://github.com/myxp-lyp/Depression-detection.git

</details>


### [343] [Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education](https://arxiv.org/abs/2505.23631)
*Boning Zhao*

Key words: 抑郁评估、人机协同、共情向量、多模态融合、PHQ-9框架

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 提出了一种名为HEAE的新型AI框架，通过整合学生叙述文本和教师的‘共情向量’，透明且负责地评估学生抑郁严重程度，准确率达82.74%。

Motivation: 敏感环境中（如特殊教育）的学生抑郁评估存在挑战，标准化问卷和现有自动化方法难以捕捉真实情况或融入教师的共情理解。

Method: 结合学生叙述文本与教师提供的9维‘共情向量’（基于PHQ-9框架），通过多模态融合和分类架构优化，实现抑郁严重程度分级。

Result: 7级抑郁严重程度分类准确率达82.74%，验证了人机协同框架的有效性。

Conclusion: HEAE为情感计算提供了一种更负责任和伦理的路径，通过结构化嵌入人类共情，增强而非替代人类判断。

Abstract: Assessing student depression in sensitive environments like special education
is challenging. Standardized questionnaires may not fully reflect students'
true situations. Furthermore, automated methods often falter with rich student
narratives, lacking the crucial, individualized insights stemming from
teachers' empathetic connections with students. Existing methods often fail to
address this ambiguity or effectively integrate educator understanding. To
address these limitations by fostering a synergistic human-AI collaboration,
this paper introduces Human Empathy as Encoder (HEAE), a novel, human-centered
AI framework for transparent and socially responsible depression severity
assessment. Our approach uniquely integrates student narrative text with a
teacher-derived, 9-dimensional "Empathy Vector" (EV), its dimensions guided by
the PHQ-9 framework,to explicitly translate tacit empathetic insight into a
structured AI input enhancing rather than replacing human judgment. Rigorous
experiments optimized the multimodal fusion, text representation, and
classification architecture, achieving 82.74% accuracy for 7-level severity
classification. This work demonstrates a path toward more responsible and
ethical affective computing by structurally embedding human empathy

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [344] [NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy ASR Decoding](https://arxiv.org/abs/2505.22857)
*Vladimir Bataev,Andrei Andrusenko,Lilit Grigoryan,Aleksandr Laptev,Vitaly Lavrukhin,Boris Ginsburg*

Key words: 统计n-gram语言模型，自动语音识别，GPU优化，贪婪解码，并行计算

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 论文提出了一种名为NGPU-LM的GPU优化统计n-gram语言模型，通过重构数据结构实现高效并行计算，适用于多种ASR模型，计算开销低于7%，并能显著减少贪婪解码与束搜索的准确率差距。

Motivation: 现有的统计n-gram语言模型在自动语音识别（ASR）中缺乏计算效率，尤其是并行性差，限制了工业应用的吸引力。

Method: NGPU-LM重新设计了统计n-gram语言模型的数据结构，支持GPU优化推理，并针对多种ASR模型（如转换器、注意力编码-解码器和CTC）提供可定制的贪婪解码。

Result: NGPU-LM在计算开销低于7%的情况下，能够减少贪婪解码与束搜索在域外场景下超过50%的准确率差距，同时避免束搜索带来的显著减速。

Conclusion: NGPU-LM通过高效并行的数据结构设计，提升了统计n-gram语言模型在ASR中的实用性，代码已开源。

Abstract: Statistical n-gram language models are widely used for context-biasing tasks
in Automatic Speech Recognition (ASR). However, existing implementations lack
computational efficiency due to poor parallelization, making context-biasing
less appealing for industrial use. This work rethinks data structures for
statistical n-gram language models to enable fast and parallel operations for
GPU-optimized inference. Our approach, named NGPU-LM, introduces customizable
greedy decoding for all major ASR model types - including transducers,
attention encoder-decoder models, and CTC - with less than 7% computational
overhead. The proposed approach can eliminate more than 50% of the accuracy gap
between greedy and beam search for out-of-domain scenarios while avoiding
significant slowdown caused by beam search. The implementation of the proposed
NGPU-LM is open-sourced.

</details>


### [345] [Spoken question answering for visual queries](https://arxiv.org/abs/2505.23308)
*Nimrod Shabtay,Zvi Kons,Avihu Dekel,Hagai Aronowitz,Ron Hoory,Assaf Arbelle*

Key words: 多模态学习、语音视觉问答、零样本TTS、数据集合成、语音合成

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 该论文提出了一种融合文本、语音和图像的多模态模型（SVQA），用于回答基于图像的语音问题。通过合成数据集训练，模型仅使用合成语音即可接近基于文本QA的性能上限，且TTS模型选择对准确率影响较小。

Motivation: 现有QA系统主要基于文本输入，而视觉QA（VQA）和语音QA（SQA）分别扩展了输入模态。论文旨在结合语音和图像交互，构建多模态的语音视觉QA（SVQA）系统，填补当前缺乏多模态数据集的空白。

Method: 通过融合文本、语音和图像三种模态，构建SVQA模型。由于缺乏现成数据集，使用两种零样本TTS模型合成VQA数据集进行训练和评估。

Result: 实验表明，仅用合成语音训练的模型性能接近基于文本QA的上限模型，且TTS模型的选择对准确率影响有限。

Conclusion: 多模态SVQA系统在合成数据集上表现出可行性，语音合成质量对模型性能影响较小，为未来多模态交互研究提供了方向。

Abstract: Question answering (QA) systems are designed to answer natural language
questions. Visual QA (VQA) and Spoken QA (SQA) systems extend the textual QA
system to accept visual and spoken input respectively.
  This work aims to create a system that enables user interaction through both
speech and images. That is achieved through the fusion of text, speech, and
image modalities to tackle the task of spoken VQA (SVQA). The resulting
multi-modal model has textual, visual, and spoken inputs and can answer spoken
questions on images.
  Training and evaluating SVQA models requires a dataset for all three
modalities, but no such dataset currently exists. We address this problem by
synthesizing VQA datasets using two zero-shot TTS models. Our initial findings
indicate that a model trained only with synthesized speech nearly reaches the
performance of the upper-bounding model trained on textual QAs. In addition, we
show that the choice of the TTS model has a minor impact on accuracy.

</details>


### [346] [DeepFilterGAN: A Full-band Real-time Speech Enhancement System with GAN-based Stochastic Regeneration](https://arxiv.org/abs/2505.23515)
*Sanberk Serbest,Tijana Stojkovic,Milos Cernak,Andrew Harper*

Key words: 语音增强, GAN, 实时系统, 随机再生, NISQA-MOS

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 本文提出了一种基于GAN随机再生的全频带实时语音增强系统，通过结合预测模型和生成模型减少输出失真，并展示了其在NISQA-MOS指标上的提升。

Motivation: 预测模型仅估计目标分布的均值，可能导致语音内容过度抑制。通过结合生成模型，可以减少输出失真。

Method: 使用GAN随机再生框架，结合预测模型和生成模型，设计了一个轻量级低延迟的实时语音增强系统。

Result: 系统参数为3.58M，实验显示在NISQA-MOS指标上优于第一阶段，并通过消融研究验证了噪声条件的重要性。

Conclusion: 该方法在实时语音增强中表现良好，并在2025 Urgent Challenge中取得进展。

Abstract: In this work, we propose a full-band real-time speech enhancement system with
GAN-based stochastic regeneration. Predictive models focus on estimating the
mean of the target distribution, whereas generative models aim to learn the
full distribution. This behavior of predictive models may lead to
over-suppression, i.e. the removal of speech content. In the literature, it was
shown that combining a predictive model with a generative one within the
stochastic regeneration framework can reduce the distortion in the output. We
use this framework to obtain a real-time speech enhancement system. With 3.58M
parameters and a low latency, our system is designed for real-time streaming
with a lightweight architecture. Experiments show that our system improves over
the first stage in terms of NISQA-MOS metric. Finally, through an ablation
study, we show the importance of noisy conditioning in our system. We
participated in 2025 Urgent Challenge with our model and later made further
improvements.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [347] [ConnectomeDiffuser: Generative AI Enables Brain Network Construction from Diffusion Tensor Imaging](https://arxiv.org/abs/2505.22683)
*Xuhang Chen,Michael Kwok-Po Ng,Kim-Fung Tsang,Chi-Man Pun,Shuqiang Wang*

Key words: 脑网络分析, 扩散张量成像, 神经退行性疾病, ConnectomeDiffuser, 图卷积网络

<details>
  <summary>Details</summary>

Main category: q-bio.NC

TL;DR: ConnectomeDiffuser是一种新型扩散框架，用于从DTI扫描自动构建脑网络，通过结合拓扑特征提取、扩散模型和图卷积网络，显著提升了神经退行性疾病诊断的准确性和敏感性。

Motivation: 解决现有脑网络构建方法中存在的操作主观性、工作流程繁琐及无法捕捉复杂拓扑特征等问题，推进计算神经影像学的发展。

Method: 提出ConnectomeDiffuser框架，包含模板网络（提取拓扑特征）、扩散模型（生成脑网络）和图卷积分类器（结合疾病标记提升诊断准确性）。

Result: 在两种神经退行性疾病数据集上验证，性能显著优于其他方法，能更敏感地分析脑网络个体差异。

Conclusion: 该框架为神经疾病提供了鲁棒、通用的测量工具，有助于更精准的诊断和机制研究。

Abstract: Brain network analysis plays a crucial role in diagnosing and monitoring
neurodegenerative disorders such as Alzheimer's disease (AD). Existing
approaches for constructing structural brain networks from diffusion tensor
imaging (DTI) often rely on specialized toolkits that suffer from inherent
limitations: operator subjectivity, labor-intensive workflows, and restricted
capacity to capture complex topological features and disease-specific
biomarkers. To overcome these challenges and advance computational neuroimaging
instrumentation, ConnectomeDiffuser is proposed as a novel diffusion-based
framework for automated end-to-end brain network construction from DTI. The
proposed model combines three key components: (1) a Template Network that
extracts topological features from 3D DTI scans using Riemannian geometric
principles, (2) a diffusion model that generates comprehensive brain networks
with enhanced topological fidelity, and (3) a Graph Convolutional Network
classifier that incorporates disease-specific markers to improve diagnostic
accuracy. ConnectomeDiffuser demonstrates superior performance by capturing a
broader range of structural connectivity and pathology-related information,
enabling more sensitive analysis of individual variations in brain networks.
Experimental validation on datasets representing two distinct neurodegenerative
conditions demonstrates significant performance improvements over other brain
network methods. This work contributes to the advancement of instrumentation in
the context of neurological disorders, providing clinicians and researchers
with a robust, generalizable measurement framework that facilitates more
accurate diagnosis, deeper mechanistic understanding, and improved therapeutic
monitoring of neurodegenerative diseases such as AD.

</details>


### [348] [Self-orthogonalizing attractor neural networks emerging from the free energy principle](https://arxiv.org/abs/2505.22749)
*Tamas Spisak,Karl Friston*

Key words: 吸引子网络,自由能原理,自组织,AI,神经科学

<details>
  <summary>Details</summary>

Main category: q-bio.NC

TL;DR: 本文提出了一种基于自由能原理的自组织吸引子网络理论，为AI和神经科学提供了新见解。

Motivation: 研究吸引子动力学的自组织性质，以理解神经元计算和AI系统的设计。

Method: 应用自由能原理对随机动力系统进行普遍划分，提出无需显式学习规则的吸引子网络。

Result: 网络产生近似正交的吸引子表示，优化预测精度与模型复杂度，增强泛化能力。

Conclusion: 该理论为自组织吸引子网络提供了统一框架，适用于AI和神经科学。

Abstract: Attractor dynamics are a hallmark of many complex systems, including the
brain. Understanding how such self-organizing dynamics emerge from first
principles is crucial for advancing our understanding of neuronal computations
and the design of artificial intelligence systems. Here we formalize how
attractor networks emerge from the free energy principle applied to a universal
partitioning of random dynamical systems. Our approach obviates the need for
explicitly imposed learning and inference rules and identifies emergent, but
efficient and biologically plausible inference and learning dynamics for such
self-organizing systems. These result in a collective, multi-level Bayesian
active inference process. Attractors on the free energy landscape encode prior
beliefs; inference integrates sensory data into posterior beliefs; and learning
fine-tunes couplings to minimize long-term surprise. Analytically and via
simulations, we establish that the proposed networks favor approximately
orthogonalized attractor representations, a consequence of simultaneously
optimizing predictive accuracy and model complexity. These attractors
efficiently span the input subspace, enhancing generalization and the mutual
information between hidden causes and observable effects. Furthermore, while
random data presentation leads to symmetric and sparse couplings, sequential
data fosters asymmetric couplings and non-equilibrium steady-state dynamics,
offering a natural extension to conventional Boltzmann Machines. Our findings
offer a unifying theory of self-organizing attractor networks, providing novel
insights for AI and neuroscience.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [349] [Conversational Alignment with Artificial Intelligence in Context](https://arxiv.org/abs/2505.22907)
*Rachel Katharine Sterken,James Ravi Kirkpatrick*

Key words: AI对话代理, 交际规范, CONTEXT-ALIGN框架, 大型语言模型

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 论文探讨了AI对话代理如何与人类交际规范对齐，提出了CONTEXT-ALIGN框架，并指出当前大型语言模型可能无法完全实现这一目标。

Motivation: 随着基于大型语言模型的AI对话代理发展，研究如何使其与人类交际规范对齐变得重要。

Method: 通过哲学和语言学文献，提出CONTEXT-ALIGN框架，评估开发者的设计选择。

Result: 当前大型语言模型的架构和约束可能限制了完全对齐人类交际规范的可能性。

Conclusion: 需要进一步研究以实现AI与人类交际规范的更佳对齐。

Abstract: The development of sophisticated artificial intelligence (AI) conversational
agents based on large language models raises important questions about the
relationship between human norms, values, and practices and AI design and
performance. This article explores what it means for AI agents to be
conversationally aligned to human communicative norms and practices for
handling context and common ground and proposes a new framework for evaluating
developers' design choices. We begin by drawing on the philosophical and
linguistic literature on conversational pragmatics to motivate a set of
desiderata, which we call the CONTEXT-ALIGN framework, for conversational
alignment with human communicative practices. We then suggest that current
large language model (LLM) architectures, constraints, and affordances may
impose fundamental limitations on achieving full conversational alignment.

</details>


### [350] [Exposing the Impact of GenAI for Cybercrime: An Investigation into the Dark Side](https://arxiv.org/abs/2505.23733)
*Truong,Luu,Binny M. Samuel*

Key words: 生成式AI、网络安全、网络犯罪、AI治理、伦理风险

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 论文通过实证研究分析了生成式AI与恶意网络活动的关联，平衡了控制与创新需求，强调了对政策制定者、开发者和公众的指导策略。

Motivation: 探讨生成式AI在网络安全中的潜在风险与伦理问题，填补实证研究的空白。

Method: 采用准实验设计与间断时间序列分析，分析一般和加密货币相关数据集。

Result: 实证数据证实生成式AI与网络犯罪的关联，为AI治理提供依据。

Conclusion: 需制定策略平衡AI创新与风险控制，指导多方利益相关者。

Abstract: In recent years, the rapid advancement and democratization of generative AI
models have sparked significant debate over safety, ethical risks, and dual-use
concerns, particularly in the context of cybersecurity. While anecdotally
known, this paper provides empirical evidence regarding generative AI's
association with malicious internet-related activities and cybercrime by
examining the phenomenon through psychological frameworks of technological
amplification and affordance theory. Using a quasi-experimental design with
interrupted time series analysis, we analyze two datasets, one general and one
cryptocurrency-focused, to empirically assess generative AI's role in
cybercrime. The findings contribute to ongoing discussions about AI governance
by balancing control and fostering innovation, underscoring the need for
strategies to guide policymakers, inform AI developers and cybersecurity
professionals, and educate the public to maximize AI's benefits while
mitigating its risks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [351] [Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in Geo-Distributed Cloud Datacenters](https://arxiv.org/abs/2505.23554)
*Hayden Moore,Sirui Qi,Ninad Hogade,Dejan Milojicic,Cullen Bash,Sudeep Pasricha*

Key words: Large Language Models, Sustainability, Carbon Footprint, Water Usage, Energy Efficiency

<details>
  <summary>Details</summary>

Main category: cs.DC

TL;DR: 论文提出了一种名为SLIT的新框架，旨在优化大型语言模型(LLM)的服务质量、碳排放、水资源使用和能源成本，以解决其运行时的环境问题。

Motivation: 随着LLM的广泛应用，其推理阶段的环境成本（如碳排放和水资源消耗）已远超训练阶段，亟需一种可持续的解决方案。

Method: 采用基于机器学习的元启发式方法，在分布式的云数据中心中优化LLM的托管可持续性。

Result: SLIT框架能够综合优化LLM的服务质量与环境影响，为未来的LLM扩展提供可持续支持。

Conclusion: SLIT框架为解决LLM运行时的环境问题提供了有效途径，随着LLM的普及，此类技术将愈发重要。

Abstract: In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and
Gemini have been widely adopted in different areas. As the use of LLMs
continues to grow, many efforts have focused on reducing the massive training
overheads of these models. But it is the environmental impact of handling user
requests to LLMs that is increasingly becoming a concern. Recent studies
estimate that the costs of operating LLMs in their inference phase can exceed
training costs by 25x per year. As LLMs are queried incessantly, the cumulative
carbon footprint for the operational phase has been shown to far exceed the
footprint during the training phase. Further, estimates indicate that 500 ml of
fresh water is expended for every 20-50 requests to LLMs during inference. To
address these important sustainability issues with LLMs, we propose a novel
framework called SLIT to co-optimize LLM quality of service (time-to-first
token), carbon emissions, water usage, and energy costs. The framework utilizes
a machine learning (ML) based metaheuristic to enhance the sustainability of
LLM hosting across geo-distributed cloud datacenters. Such a framework will
become increasingly vital as LLMs proliferate.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [352] [Investigating the effectiveness of multimodal data in forecasting SARS-COV-2 case surges](https://arxiv.org/abs/2505.22688)
*Palur Venkata Raghuvamsi,Siyuan Brandon Loh,Prasanta Bhattacharya,Joses Ho,Raphael Lee Tze Chuen,Alvin X. Han,Sebastian Maurer-Stroh*

Key words: COVID-19, 预测模型, 基因组数据, 行为数据, 公共卫生

<details>
  <summary>Details</summary>

Main category: q-bio.QM

TL;DR: 该研究探讨了不同特征集（如基因组数据、公共卫生数据和人类行为数据）在预测COVID-19病例激增中的有效性，发现预测性能因国家和特征类型而异，强调需根据具体国家和疫情阶段定制模型。

Motivation: 现有流行病预测模型主要依赖传统数据，而其他数据源（如基因组和行为数据）的潜力未充分挖掘。研究旨在评估这些替代数据在预测病例激增中的价值。

Method: 研究比较了生物特征（如突变）、公共卫生特征（如病例数、政策干预）和人类行为特征（如流动性和社交媒体讨论）在预测国家层面病例激增中的效果。

Result: 不同国家和特征类型的预测效果差异显著，表明模型需针对特定国家背景和疫情阶段调整。生物和行为特征对预测具有补充价值。

Conclusion: 整合替代数据源可提升疫情动态预测能力，未来模型应更具针对性。

Abstract: The COVID-19 pandemic response relied heavily on statistical and machine
learning models to predict key outcomes such as case prevalence and fatality
rates. These predictions were instrumental in enabling timely public health
interventions that helped break transmission cycles. While most existing models
are grounded in traditional epidemiological data, the potential of alternative
datasets, such as those derived from genomic information and human behavior,
remains underexplored. In the current study, we investigated the usefulness of
diverse modalities of feature sets in predicting case surges. Our results
highlight the relative effectiveness of biological (e.g., mutations), public
health (e.g., case counts, policy interventions) and human behavioral features
(e.g., mobility and social media conversations) in predicting country-level
case surges. Importantly, we uncover considerable heterogeneity in predictive
performance across countries and feature modalities, suggesting that surge
prediction models may need to be tailored to specific national contexts and
pandemic phases. Overall, our work highlights the value of integrating
alternative data sources into existing disease surveillance frameworks to
enhance the prediction of pandemic dynamics.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [353] [Trajectory Generator Matching for Time Series](https://arxiv.org/abs/2505.23215)
*T. Jahn,J. Chemseddine,P. Hagemann,C. Wald,G. Steidl*

Key words: 时间序列生成,随机微分方程,跳跃过程,生成模型,不规则采样

<details>
  <summary>Details</summary>

Main category: math.NA

TL;DR: 该论文提出了一种基于生成模型的方法来处理不规则采样的时间序列，通过SDE和跳跃过程的生成器来模拟时间连续随机过程。

Motivation: 准确建模不规则观测的时间连续随机过程是一个重要挑战，本文旨在利用图像数据生成模型的思想推动时间序列生成技术的发展。

Method: 提出了新的SDE和跳跃过程生成器，受轨迹流匹配启发，参数化跳跃核密度为缩放高斯分布，从而在损失函数中得到Kullback-Leibler散度的闭式解。

Result: 该方法能够处理不连续过程，特别适用于不规则采样的时间序列。

Conclusion: 通过生成模型和跳跃核参数化，本文为解决不规则时间序列建模提供了一种有效方法。

Abstract: Accurately modeling time-continuous stochastic processes from irregular
observations remains a significant challenge. In this paper, we leverage ideas
from generative modeling of image data to push the boundary of time series
generation. For this, we find new generators of SDEs and jump processes,
inspired by trajectory flow matching, that have the marginal distributions of
the time series of interest. Specifically, we can handle discontinuities of the
underlying processes by parameterizing the jump kernel densities by scaled
Gaussians that allow for closed form formulas of the corresponding
Kullback-Leibler divergence in the loss. Unlike most other approaches, we are
able to handle irregularly sampled time series.

</details>


### [354] [Optimization-Free Diffusion Model -- A Perturbation Theory Approach](https://arxiv.org/abs/2505.23652)
*Yuehaw Khoo,Mathias Oster,Yifan Peng*

Key words: 扩散模型、得分函数、Kolmogorov算子、Boltzmann分布、稀疏特征基

<details>
  <summary>Details</summary>

Main category: math.NA

TL;DR: 提出了无需优化和前向SDE的扩散模型方法，通过稀疏特征基展开重构得分估计，避免迭代优化和时间依赖样本生成，并在高维Boltzmann分布和实际数据上验证有效性。

Motivation: 扩散模型通常依赖神经网络的优化和前进SDE模拟，计算复杂度高，本研究旨在提出一种无需优化的替代方法，降低计算成本。

Method: 将得分函数在扩散过程相关的后向Kolmogorov算子的稀疏特征基中展开，将其重构为线性系统的解，从而避免优化和时序样本生成。

Result: 通过扰动理论分析近似误差，并在高维Boltzmann分布和实际数据集中验证了方法的有效性。

Conclusion: 该方法成功避免了传统扩散模型的优化和时序计算需求，为生成模型提供了更高效的实现路径。

Abstract: Diffusion models have emerged as a powerful framework in generative modeling,
typically relying on optimizing neural networks to estimate the score function
via forward SDE simulations. In this work, we propose an alternative method
that is both optimization-free and forward SDE-free. By expanding the score
function in a sparse set of eigenbasis of the backward Kolmogorov operator
associated with the diffusion process, we reformulate score estimation as the
solution to a linear system, avoiding iterative optimization and time-dependent
sample generation. We analyze the approximation error using perturbation theory
and demonstrate the effectiveness of our method on high-dimensional Boltzmann
distributions and real-world datasets.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [355] [Distribution free M-estimation](https://arxiv.org/abs/2505.22807)
*John C. Duchi*

Key words: 统计问题,凸M估计,随机优化,分布自由,Lipschitz连续性

<details>
  <summary>Details</summary>

Main category: math.ST

TL;DR: 本文研究了无需假设数据分布即可解决的统计问题，特别针对单变量凸M估计和随机优化问题，界定了可解与不可解的明确分界线。研究发现，损失函数的Lipschitz连续性对于分布自由最小化并非必要，且与机器学习中的可学习性经典特征不同。

Motivation: 探讨统计问题中无需数据分布假设的可解性，旨在明确单变量凸M估计和随机优化问题的边界条件。

Method: 通过理论分析，识别可解性问题所需的条件，并与经典机器学习中的可学习性条件进行对比。

Result: 研究发现Lipschitz连续性在分布自由最小化中并非必要，且可解性条件与经典可学习性特征不同。

Conclusion: 本研究为无假设统计问题的可解性提供了新视角，挑战了传统理解中的必要条件。

Abstract: The basic question of delineating those statistical problems that are
solvable without making any assumptions on the underlying data distribution has
long animated statistics and learning theory. This paper characterizes when a
(univariate) convex M-estimation or stochastic optimization problem is solvable
in such an assumption-free setting, providing a precise dividing line between
solvable and unsolvable problems. The conditions we identify show, perhaps
surprisingly, that Lipschitz continuity of the loss being minimized is not
necessary for distribution free minimization, and they are also distinct from
classical characterizations of learnability in machine learning.

</details>


### [356] [Joint estimation of smooth graph signals from partial linear measurements](https://arxiv.org/abs/2505.23240)
*Hemant Tyagi*

Key words: 图信号处理, 最小二乘估计, 弱一致性, 多层排名, 平滑性惩罚

<details>
  <summary>Details</summary>

Main category: math.ST

TL;DR: 研究提出了一种基于图平滑性的信号恢复方法，通过部分线性测量估计图中顶点的潜在信号，在严格采样条件下仍能保持弱一致性。

Motivation: 解决图中顶点潜在信号的估计问题，尤其是在测量数据非常有限的情况下，如何利用图结构信息提高估计精度。

Method: 采用了平滑性惩罚最小二乘估计器，假设信号在图上是平滑的（即信号的二次变化较小），并通过理论分析证明了其在严格采样条件下的有效性。

Result: 研究发现，即使每个顶点仅测量一个坐标且测量顶点比例极小，该估计器仍能保持弱一致性。结果还扩展到多层排名问题，支持稀疏和不相连的测量图。

Conclusion: 通过图平滑性假设和最小二乘方法，可以在极端稀疏的采样条件下实现有效的信号恢复。

Abstract: Given an undirected and connected graph $G$ on $T$ vertices, suppose each
vertex $t$ has a latent signal $x_t \in \mathbb{R}^n$ associated to it. Given
partial linear measurements of the signals, for a potentially small subset of
the vertices, our goal is to estimate $x_t$'s. Assuming that the signals are
smooth w.r.t $G$, in the sense that the quadratic variation of the signals over
the graph is small, we obtain non-asymptotic bounds on the mean squared error
for jointly recovering $x_t$'s, for the smoothness penalized least squares
estimator. In particular, this implies for certain choices of $G$ that this
estimator is weakly consistent (as $T \rightarrow \infty$) under potentially
very stringent sampling, where only one coordinate is measured per vertex for a
vanishingly small fraction of the vertices. The results are extended to a
``multi-layer'' ranking problem where $x_t$ corresponds to the latent strengths
of a collection of $n$ items, and noisy pairwise difference measurements are
obtained at each ``layer'' $t$ via a measurement graph $G_t$. Weak consistency
is established for certain choices of $G$ even when the individual $G_t$'s are
very sparse and disconnected.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [357] [CrossNAS: A Cross-Layer Neural Architecture Search Framework for PIM Systems](https://arxiv.org/abs/2505.22868)
*Md Hasibul Amin,Mohammadreza Mohammadi,Jason D. Bakos,Ramtin Zand*

Key words: CrossNAS, 模拟内存计算, 神经网络架构搜索, 进化搜索, 权重共享

<details>
  <summary>Details</summary>

Main category: cs.ET

TL;DR: 论文提出CrossNAS框架，通过跨层次搜索优化模拟内存计算系统的机器学习任务部署，结合单路径权重共享与进化搜索，实现更高精度和能效。

Motivation: 现有方法在模拟内存计算系统中部署机器学习任务时，缺乏跨层次的自动优化能力，CrossNAS旨在填补这一空白。

Method: 提出CrossNAS框架，首次结合单路径权重共享策略与进化搜索，优化电路、架构和系统层次的设计空间。

Result: CrossNAS在精度和能效上超越现有方法，同时保持较短搜索时间，树立了PIM神经网络架构搜索新标杆。

Conclusion: CrossNAS为模拟内存计算系统提供了一种高效的跨层次自动优化方案，验证了其在实际任务中的优越性。

Abstract: In this paper, we propose the CrossNAS framework, an automated approach for
exploring a vast, multidimensional search space that spans various design
abstraction layers-circuits, architecture, and systems-to optimize the
deployment of machine learning workloads on analog processing-in-memory (PIM)
systems. CrossNAS leverages the single-path one-shot weight-sharing strategy
combined with the evolutionary search for the first time in the context of PIM
system mapping and optimization. CrossNAS sets a new benchmark for PIM neural
architecture search (NAS), outperforming previous methods in both accuracy and
energy efficiency while maintaining comparable or shorter search times.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [358] [A Descriptor Is All You Need: Accurate Machine Learning of Nonadiabatic Coupling Vectors](https://arxiv.org/abs/2505.23344)
*Jakub Martinka,Lina Zhang,Yi-Fan Hou,Mikołaj Martyka,Jiří Pittner,Mario Barbatti,Pavlo O. Dral*

Key words: 非绝热耦合（NACs）、机器学习、FSSH、锥形交叉、相位校正

<details>
  <summary>Details</summary>

Main category: physics.comp-ph

TL;DR: 本文提出了一种基于机器学习的新方法，用于高精度学习非绝热耦合（NACs），并成功应用于FSSH模拟，显著提升了准确性和效率。

Motivation: 非绝热耦合（NACs）在光化学和光物理过程模拟中至关重要，但传统方法难以精确计算，尤其是在锥形交叉附近。机器学习NACs可加速模拟，但现有方法面临向量、双值特性和奇异性等挑战。

Method: 作者设计了一种针对NACs的描述符，并开发了新的机器学习相位校正方法，基于领域专业知识提升了学习准确性。在fulvene的SA-2-CASSCF(6,6)电子结构水平上进行了全ML驱动的FSSH模拟。

Result: 该方法实现前所未有的高精度（R²>0.99），显著降低了误差范围，并通过大规模轨迹集合提高了S₁衰变的准确性。

Conclusion: 该方法在效率和鲁棒性上表现优异，为NACs的机器学习提供了新思路，并开源了MLatom实现。

Abstract: Nonadiabatic couplings (NACs) play a crucial role in modeling photochemical
and photophysical processes with methods such as the widely used
fewest-switches surface hopping (FSSH). There is therefore a strong incentive
to machine learn NACs for accelerating simulations. However, this is
challenging due to NACs' vectorial, double-valued character and the singularity
near a conical intersection seam. For the first time, we design NAC-specific
descriptors based on our domain expertise and show that they allow learning
NACs with never-before-reported accuracy of $R^2$ exceeding 0.99. The key to
success is also our new ML phase-correction procedure. We demonstrate the
efficiency and robustness of our approach on a prototypical example of fully
ML-driven FSSH simulations of fulvene targeting the SA-2-CASSCF(6,6) electronic
structure level. This ML-FSSH dynamics leads to an accurate description of
$S_1$ decay while reducing error bars by allowing the execution of a large
ensemble of trajectories. Our implementations are available in open-source
MLatom.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [359] [Augment or Not? A Comparative Study of Pure and Augmented Large Language Model Recommenders](https://arxiv.org/abs/2505.23053)
*Wei-Hsiang Huang,Chen-Wei Ke,Wei-Ning Chiu,Yu-Xuan Su,Chun-Chun Yang,Chieh-Yuan Cheng,Yun-Nung Chen,Pu-Jen Cheng*

Key words: 大型语言模型, 推荐系统, 分类法, 评估平台, 增强学习

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 该论文提出了一个分类大型语言模型（LLM）推荐系统的分类法，分为纯LLM推荐器和增强型LLM推荐器，并引入统一的评估平台进行公平比较，总结了未来研究方向。

Motivation: 探讨如何利用LLMs提升推荐系统的语义理解和世界知识整合能力，并系统地分类现有方法以促进研究进展。

Method: 提出分类法将LLM推荐系统分为纯LLM和增强型两类，并开发统一的评估平台进行模型对比。

Result: 通过分类和评估，揭示了不同设计选择对推荐效果的影响，为LLM推荐系统的发展提供了实践指导。

Conclusion: 论文总结了LLM推荐系统的当前挑战和未来方向，为下一代推荐系统的研究提供了框架和建议。

Abstract: Large language models (LLMs) have introduced new paradigms for recommender
systems by enabling richer semantic understanding and incorporating implicit
world knowledge. In this study, we propose a systematic taxonomy that
classifies existing approaches into two categories: (1) Pure LLM Recommenders,
which rely solely on LLMs, and (2) Augmented LLM Recommenders, which integrate
additional non-LLM techniques to enhance performance. This taxonomy provides a
novel lens through which to examine the evolving landscape of LLM-based
recommendation. To support fair comparison, we introduce a unified evaluation
platform that benchmarks representative models under consistent experimental
settings, highlighting key design choices that impact effectiveness. We
conclude by discussing open challenges and outlining promising directions for
future research. This work offers both a comprehensive overview and practical
guidance for advancing next-generation LLM-powered recommender.

</details>


### [360] [From Token to Action: State Machine Reasoning to Mitigate Overthinking in Information Retrieval](https://arxiv.org/abs/2505.23059)
*Dohyeon Lee,Yeonseok Jeong,Seung-won Hwang*

Key words: 链式思考提示,状态机推理,信息检索,大型语言模型

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了状态机推理（SMR）框架，解决了传统链式思考（CoT）提示在信息检索中导致的过度思考和冗余问题。通过离散动作支持精细控制和早期停止，实验证明SMR在提升检索性能的同时显著减少计算开销。

Motivation: 传统链式思考（CoT）提示在信息检索中常因过度推理导致语义冗余或偏离用户意图，亟需一种高效可控的替代方法。

Method: 提出状态机推理（SMR）框架，基于离散动作（Refine、Rerank、Stop）实现过渡式推理，支持早期停止和精细控制。

Result: 在BEIR和BRIGHT基准测试中，SMR将检索性能（nDCG@10）提升3.4%，同时减少74.4%的token使用量，且无需任务特定调优即可泛化。

Conclusion: SMR为传统CoT推理提供了实用替代方案，兼具性能提升与计算效率。

Abstract: Chain-of-Thought (CoT) prompting enables complex reasoning in large language
models (LLMs), including applications in information retrieval (IR). However,
it often leads to overthinking, where models produce excessively long and
semantically redundant traces with little or no benefit. We identify two key
challenges in IR: redundant trajectories that revisit similar states and
misguided reasoning that diverges from user intent. To address these, we
propose State Machine Reasoning (SMR), a transition-based reasoning framework
composed of discrete actions (Refine, Rerank, Stop) that support early stopping
and fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show
that SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token
usage by 74.4%. It generalizes across LLMs and retrievers without requiring
task-specific tuning, offering a practical alternative to conventional CoT
reasoning. The code and details are available at https://github.com/ldilab/SMR.

</details>


### [361] [Deep Retrieval at CheckThat! 2025: Identifying Scientific Papers from Implicit Social Media Mentions via Hybrid Retrieval and Re-Ranking](https://arxiv.org/abs/2505.23250)
*Pascal J. Sager,Ashwini Kamaraj,Benjamin F. Grewe,Thilo Stadelmann*

Key words: 混合检索、社交媒体、科学文献、BM25、FAISS、LLM重排

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了一种结合词法、语义和深度上下文重排的混合检索方法，用于社交媒体帖子与科学文献的匹配，取得了竞赛中的优异排名。

Motivation: 解决社交媒体帖子与科学文献之间的非正式到正式语言鸿沟，提升检索的准确性和鲁棒性。

Method: 结合BM25关键词匹配、FAISS语义检索和LLM重排的混合检索流程。

Result: 在开发集和测试集上分别取得76.46%和66.43%的MRR@5，竞赛排名分别为第1和第3。

Conclusion: 精心设计和微调的检索流程能有效提升性能，无需外部训练数据。

Abstract: We present the methodology and results of the Deep Retrieval team for subtask
4b of the CLEF CheckThat! 2025 competition, which focuses on retrieving
relevant scientific literature for given social media posts. To address this
task, we propose a hybrid retrieval pipeline that combines lexical precision,
semantic generalization, and deep contextual re-ranking, enabling robust
retrieval that bridges the informal-to-formal language gap. Specifically, we
combine BM25-based keyword matching with a FAISS vector store using a
fine-tuned INF-Retriever-v1 model for dense semantic retrieval. BM25 returns
the top 30 candidates, and semantic search yields 100 candidates, which are
then merged and re-ranked via a large language model (LLM)-based cross-encoder.
  Our approach achieves a mean reciprocal rank at 5 (MRR@5) of 76.46% on the
development set and 66.43% on the hidden test set, securing the 1st position on
the development leaderboard and ranking 3rd on the test leaderboard (out of 31
teams), with a relative performance gap of only 2 percentage points compared to
the top-ranked system. We achieve this strong performance by running
open-source models locally and without external training data, highlighting the
effectiveness of a carefully designed and fine-tuned retrieval pipeline.

</details>


### [362] [Engineering Serendipity through Recommendations of Items with Atypical Aspects](https://arxiv.org/abs/2505.23580)
*Ramit Aditya,Razvan Bunescu,Smita Nannaware,Erfan Al-Hossami*

Key words: 偶然惊喜推荐、非典型特征、LLM、用户效用、上下文学习

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了一种通过推荐具有非典型特征的物品来制造偶然惊喜的新任务，并基于LLM的系统流程提取这些特征、评估用户效用，最终通过重新排序推荐列表实现。实验表明该方法效果显著，且公开了数据集和代码。

Motivation: 研究动机在于提升用户体验，通过推荐出人意料但又符合兴趣的物品（如餐厅的折纸活动或酒店的古钢琴展示），创造难忘的偶然惊喜体验。

Method: 方法包括：1）利用LLM从评论中提取非典型特征；2）估计用户对这些特征的效用，计算偶然惊喜潜力；3）通过动态选择上下文学习示例优化判断。

Result: 实验结果证明，系统生成的偶然惊喜推荐与人工标注的真实排名高度相关，验证了方法的有效性。

Conclusion: 结论强调，该研究为推荐系统超越准确性、追求用户满意度的新方向提供了基础和工具。

Abstract: A restaurant dinner or a hotel stay may lead to memorable experiences when
guests encounter unexpected aspects that also match their interests. For
example, an origami-making station in the waiting area of a restaurant may be
both surprising and enjoyable for a customer who is passionate about paper
crafts. Similarly, an exhibit of 18th century harpsichords would be atypical
for a hotel lobby and likely pique the interest of a guest who has a passion
for Baroque music. Motivated by this insight, in this paper we introduce the
new task of engineering serendipity through recommendations of items with
atypical aspects. We describe an LLM-based system pipeline that extracts
atypical aspects from item reviews, then estimates and aggregates their
user-specific utility in a measure of serendipity potential that is used to
rerank a list of items recommended to the user. To facilitate system
development and evaluation, we introduce a dataset of Yelp reviews that are
manually annotated with atypical aspects and a dataset of artificially
generated user profiles, together with crowdsourced annotations of user-aspect
utility values. Furthermore, we introduce a custom procedure for dynamic
selection of in-context learning examples, which is shown to improve LLM-based
judgments of atypicality and utility. Experimental evaluations show that
serendipity-based rankings generated by the system are highly correlated with
ground truth rankings for which serendipity scores are computed from manual
annotations of atypical aspects and their user-dependent utility. Overall, we
hope that the new recommendation task and the associated system presented in
this paper catalyze further research into recommendation approaches that go
beyond accuracy in their pursuit of enhanced user satisfaction.
  The datasets and the code are made publicly available at
https://github.com/ramituncc49er/ATARS .

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [363] [Physiology-Informed Generative Multi-Task Network for Contrast-Free CT Perfusion](https://arxiv.org/abs/2505.22673)
*Wasif Khan,Kyle B. See,Simon Kato,Ziqian Huang,Amy Lazarte,Kyle Douglas,Xiangyang Lou,Teng J. Peng,Dhanashree Rajderkar,John Rees,Pina Sanelli,Amita Singh,Ibrahim Tuna,Christina A. Wilson,Ruogu Fang*

Key words: CT灌注成像、深度学习、无对比剂成像、MAGIC框架、医疗成像

<details>
  <summary>Details</summary>

Main category: q-bio.TO

TL;DR: 该研究提出了一种名为MAGIC的深度学习框架，用于将非对比增强CT图像映射为多个无对比剂的CTP图像，避免了造影剂的副作用和高成本，并在诊断准确性上表现优异。

Motivation: 传统CTP成像使用造影剂可能导致过敏反应和副作用，且成本高昂。因此，研究旨在开发一种无对比剂的CTP成像方法，以提升安全性和经济性。

Method: 提出了多任务自动化生成跨模态CT灌注图（MAGIC）的深度学习框架，结合生成人工智能和生理信息，将非对比增强CT图像映射为无对比剂的CTP图像。

Result: MAGIC框架在双盲研究中显示出与临床对比增强CTP相当的视觉质量和诊断准确性，同时避免了造影剂的使用。

Conclusion: MAGIC框架为无对比剂、经济高效的快速灌注成像提供了可行方案，有望在医疗领域带来革命性变革。

Abstract: Perfusion imaging is extensively utilized to assess hemodynamic status and
tissue perfusion in various organs. Computed tomography perfusion (CTP) imaging
plays a key role in the early assessment and planning of stroke treatment.
While CTP provides essential perfusion parameters to identify abnormal blood
flow in the brain, the use of contrast agents in CTP can lead to allergic
reactions and adverse side effects, along with costing USD 4.9 billion
worldwide in 2022. To address these challenges, we propose a novel deep
learning framework called Multitask Automated Generation of Intermodal CT
perfusion maps (MAGIC). This framework combines generative artificial
intelligence and physiological information to map non-contrast computed
tomography (CT) imaging to multiple contrast-free CTP imaging maps. We
demonstrate enhanced image fidelity by incorporating physiological
characteristics into the loss terms. Our network was trained and validated
using CT image data from patients referred for stroke at UF Health and
demonstrated robustness to abnormalities in brain perfusion activity. A
double-blinded study was conducted involving seven experienced
neuroradiologists and vascular neurologists. This study validated MAGIC's
visual quality and diagnostic accuracy showing favorable performance compared
to clinical perfusion imaging with intravenous contrast injection. Overall,
MAGIC holds great promise in revolutionizing healthcare by offering
contrast-free, cost-effective, and rapid perfusion imaging.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [364] [Recovering Fairness Directly from Modularity: a New Way for Fair Community Partitioning](https://arxiv.org/abs/2505.22684)
*Yufeng Wang,Yiguang Bai,Tianqing Zhu,Ismail Ben Ayed,Jing Yuan*

Key words: 社区分区，模块度，公平性，Fast Newman算法，网络分析

<details>
  <summary>Details</summary>

Main category: cs.SI

TL;DR: 该论文提出了一个公平性-模块度度量标准，扩展了传统模块度以显式结合公平性，并开发了高效的Fair Fast Newman (FairFN)算法，优化模块度和公平性。实验表明FairFN在公平性和分区质量上有显著提升。

Motivation: 传统基于模块度的社区分区方法往往忽视公平性，这在现实应用中至关重要。为了解决这一问题，论文引入了受保护群体网络并提出了新的公平性-模块度度量标准。

Method: 论文设计了公平性-模块度度量标准，扩展了传统模块度以显式结合公平性。开发了Fair Fast Newman (FairFN)算法，增强Fast Newman (FN)方法以同时优化模块度和公平性。

Result: 实验表明FairFN在公平性和分区质量上优于现有方法，特别是在不平衡数据集上表现显著。

Conclusion: 论文提出的公平性模块度标准和FairFN算法能在理论和实践中显著提升社区分区的公平性和质量。

Abstract: Community partitioning is crucial in network analysis, with modularity
optimization being the prevailing technique. However, traditional
modularity-based methods often overlook fairness, a critical aspect in
real-world applications. To address this, we introduce protected group networks
and propose a novel fairness-modularity metric. This metric extends traditional
modularity by explicitly incorporating fairness, and we prove that minimizing
it yields naturally fair partitions for protected groups while maintaining
theoretical soundness. We develop a general optimization framework for fairness
partitioning and design the efficient Fair Fast Newman (FairFN) algorithm,
enhancing the Fast Newman (FN) method to optimize both modularity and fairness.
Experiments show FairFN achieves significantly improved fairness and
high-quality partitions compared to state-of-the-art methods, especially on
unbalanced datasets.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [365] [StarBASE-GP: Biologically-Guided Automated Machine Learning for Genotype-to-Phenotype Association Analysis](https://arxiv.org/abs/2505.22746)
*Jose Guadalupe Hernandez,Attri Ghosh,Philip J. Freda,Yufei Meng,Nicholas Matsumoto,Jason H. Moore*

Key words: StarBASE-GP, 遗传编程, 多目标优化, 基因组学, 复杂性状

<details>
  <summary>Details</summary>

Main category: cs.NE

TL;DR: StarBASE-GP是一种基于遗传编程的自动化工具，用于在大规模基因组数据中发现与表型变异相关的遗传变异，通过多目标优化策略同时最大化解释力和最小化复杂性，效果优于基线方法。

Motivation: 传统方法难以高效发现复杂性状中的遗传变异，StarBASE-GP旨在通过自动化框架结合生物学知识改进变异发现。

Method: 采用遗传编程的多目标优化策略，整合九种遗传编码策略、连锁不平衡修剪和动态变体推荐系统。

Result: 在褐家鼠队列中，StarBASE-GP表现优于随机基线和生物学无关版本，能更准确地识别已知和新数量性状位点。

Conclusion: StarBASE-GP通过结合进化搜索和生物学理论，展现了在复杂性状变异发现中的潜力。

Abstract: We present the Star-Based Automated Single-locus and Epistasis analysis tool
- Genetic Programming (StarBASE-GP), an automated framework for discovering
meaningful genetic variants associated with phenotypic variation in large-scale
genomic datasets. StarBASE-GP uses a genetic programming-based multi-objective
optimization strategy to evolve machine learning pipelines that simultaneously
maximize explanatory power (r2) and minimize pipeline complexity. Biological
domain knowledge is integrated at multiple stages, including the use of nine
inheritance encoding strategies to model deviations from additivity, a custom
linkage disequilibrium pruning node that minimizes redundancy among features,
and a dynamic variant recommendation system that prioritizes informative
candidates for pipeline inclusion. We evaluate StarBASE-GP on a cohort of
Rattus norvegicus (brown rat) to identify variants associated with body mass
index, benchmarking its performance against a random baseline and a
biologically naive version of the tool. StarBASE-GP consistently evolves Pareto
fronts with superior performance, yielding higher accuracy in identifying both
ground truth and novel quantitative trait loci, highlighting relevant targets
for future validation. By incorporating evolutionary search and relevant
biological theory into a flexible automated machine learning framework,
StarBASE-GP demonstrates robust potential for advancing variant discovery in
complex traits.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [366] [Learning to Charge More: A Theoretical Study of Collusion by Q-Learning Agents](https://arxiv.org/abs/2505.22909)
*Cristian Chica,Yinglong Guo,Gilad Lerman*

Key words: $Q$-学习、无限重复博弈、子博弈完美均衡、合谋、定价策略

<details>
  <summary>Details</summary>

Main category: econ.GN

TL;DR: 论文通过理论分析解释了$Q$-学习代理在无限重复博弈中可能学会收取超额竞争价格的行为，引入了一种新的一记忆子博弈完美均衡类，并探讨了学习行为支持的博弈策略。

Motivation: 越来越多的实验证据表明$Q$-学习代理可能学会收取超额竞争价格，但缺乏理论解释。本文旨在填补这一空白，分析无限重复博弈中的学习行为。

Method: 研究了在无限重复博弈中，企业仅基于观察到的利润更新定价策略（不计算均衡策略）的行为。通过分析$Q$-函数在试验结束时的特定不等式，证明了企业学会收取超额竞争价格的条件。

Result: 发现当博弈既存在一次性纳什均衡价格又存在促进合谋的价格时，$Q$-函数满足特定不等式后，企业会一致地收取超额竞争价格。引入了新的一记忆子博弈完美均衡类，并分析了学习行为支持的博弈策略。

Conclusion: 天真的合谋策略仅当合谋价格是一次性纳什均衡时才构成子博弈完美均衡，而严厉触发策略可以支持学习行为。

Abstract: There is growing experimental evidence that $Q$-learning agents may learn to
charge supracompetitive prices. We provide the first theoretical explanation
for this behavior in infinite repeated games. Firms update their pricing
policies based solely on observed profits, without computing equilibrium
strategies. We show that when the game admits both a one-stage Nash equilibrium
price and a collusive-enabling price, and when the $Q$-function satisfies
certain inequalities at the end of experimentation, firms learn to consistently
charge supracompetitive prices. We introduce a new class of one-memory subgame
perfect equilibria (SPEs) and provide conditions under which learned behavior
is supported by naive collusion, grim trigger policies, or increasing
strategies. Naive collusion does not constitute an SPE unless the
collusive-enabling price is a one-stage Nash equilibrium, whereas grim trigger
policies can.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [367] [A comprehensive analysis of PINNs: Variants, Applications, and Challenges](https://arxiv.org/abs/2505.22761)
*Afila Ajithkumar Sophiya,Akarsh K Nair,Sepehr Maleki,Senthil K. Krishnababu*

Key words: 物理信息神经网络, PINNs, 微分方程, 架构, 应用领域

<details>
  <summary>Details</summary>

Main category: cs.CE

TL;DR: 该论文综述了物理信息神经网络（PINNs）的架构、变体、应用领域、挑战及未来研究方向，填补了现有文献中缺乏全面分析的空白。

Motivation: 尽管PINNs在解微分方程方面显示出潜力，但其应用仍处于初期阶段，缺乏标准化。现有综述未能全面涵盖PINNs的架构、应用和挑战，本文旨在填补这一空白。

Method: 通过综合分析PINNs的架构、变体、应用领域和实际案例，结合最新研究进展，提出性能分析和挑战讨论。

Result: 展示了PINNs在不同方程和应用领域的性能特点，总结了当前实施中的主要挑战，并指出了未来研究方向。

Conclusion: 该综述为PINNs的进一步研究和应用提供了详尽的参考，强调了标准化和解决现有挑战的重要性。

Abstract: Physics Informed Neural Networks (PINNs) have been emerging as a powerful
computational tool for solving differential equations. However, the
applicability of these models is still in its initial stages and requires more
standardization to gain wider popularity. Through this survey, we present a
comprehensive overview of PINNs approaches exploring various aspects related to
their architecture, variants, areas of application, real-world use cases,
challenges, and so on. Even though existing surveys can be identified, they
fail to provide a comprehensive view as they primarily focus on either
different application scenarios or limit their study to a superficial level.
This survey attempts to bridge the gap in the existing literature by presenting
a detailed analysis of all these factors combined with recent advancements and
state-of-the-art research in PINNs. Additionally, we discuss prevalent
challenges in PINNs implementation and present some of the future research
directions as well. The overall contributions of the survey can be summarised
into three sections: A detailed overview of PINNs architecture and variants, a
performance analysis of PINNs on different equations and application domains
highlighting their features. Finally, we present a detailed discussion of
current issues and future research directions.

</details>


### [368] [Unified Network-Based Representation of BIM Models for Embedding Semantic, Spatial, and Topological Data](https://arxiv.org/abs/2505.22670)
*Jin Han,Xin-Zheng Lu,Jia-Rui Lin*

Key words: BIM模型, IFC标准, 空间拓扑关系, 网络表示, 设计模式

<details>
  <summary>Details</summary>

Main category: cs.CE

TL;DR: 本文提出了一种基于网络的统一表示方法，用于整合BIM模型中‘语义-空间-拓扑’多维设计特征，以增强对组件关系和设计模式的理解。

Motivation: 现有研究缺乏有效方法捕捉BIM模型中复杂的空间与拓扑关系，这对理解设计模式和优化决策至关重要。

Method: 通过扩展IFC标准，引入局部空间关系和组件间拓扑连接，构建了一个多维度网络结构表示方法。

Result: 该方法能够更细致地理解组件间的交互、依赖关系和隐含设计模式，显著提升了BIM模型的表达能力。

Conclusion: 此网络表示方法为BIM模型的设计模式表示与学习提供了重要潜力。

Abstract: Building Information Modeling (BIM) has revolutionized the construction
industry by providing a comprehensive digital representation of building
structures throughout their lifecycle. However, existing research lacks
effective methods for capturing the complex spatial and topological
relationships between components in BIM models, which are essential for
understanding design patterns and enhancing decision-making. This study
proposes a unified network-based representation method that integrates the
"semantic-spatial-topological" multi-dimensional design features of BIM models.
By extending the IFC (Industry Foundation Classes) standard, we introduce local
spatial relationships and topological connections between components to enrich
the network structure. This representation method enables a more detailed
understanding of component interactions, dependencies, and implicit design
patterns, effectively capturing the semantic, topological, and spatial
relationships in BIM, and holds significant potential for the representation
and learning of design patterns.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [369] [Semantic Exploration and Dense Mapping of Complex Environments using Ground Robots Equipped with LiDAR and Panoramic Camera](https://arxiv.org/abs/2505.22880)
*Xiaoyang Zhan,Shixin Zhou,Qianqian Yang,Yixuan Zhao,Hao Liu,Srinivas Chowdary Ramineni,Kenji Shimada*

Key words: 自主探索, 语义映射, 多视角规划, 层次化规划, LiDAR-全景相机

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 该论文提出了一种结合语义探索与密集语义目标映射的自主系统，通过分离几何与语义视角并采用层次化规划，实现了高效且安全的未知环境探索。

Motivation: 现有方法在平衡多视角高质量观测与避免重复遍历方面存在不足，需要一种能同时完成几何覆盖与语义观察的新系统。

Method: 提出优先级驱动的解耦局部采样器生成局部视角集，结合层次化规划和安全激进探索状态机，集成前沿SLAM算法实现密集语义映射。

Result: 仿真和真实实验验证了系统在探索速度、路径长度及多视角检查上的优势，实现了非结构化环境的精确语义目标映射。

Conclusion: 系统通过解耦规划和语义目标映射模块，显著提升了复杂未知环境下的自主探索能力与语义建模准确性。

Abstract: This paper presents a system for autonomous semantic exploration and dense
semantic target mapping of a complex unknown environment using a ground robot
equipped with a LiDAR-panoramic camera suite. Existing approaches often
struggle to balance collecting high-quality observations from multiple view
angles and avoiding unnecessary repetitive traversal. To fill this gap, we
propose a complete system combining mapping and planning. We first redefine the
task as completing both geometric coverage and semantic viewpoint observation.
We then manage semantic and geometric viewpoints separately and propose a novel
Priority-driven Decoupled Local Sampler to generate local viewpoint sets. This
enables explicit multi-view semantic inspection and voxel coverage without
unnecessary repetition. Building on this, we develop a hierarchical planner to
ensure efficient global coverage. In addition, we propose a Safe Aggressive
Exploration State Machine, which allows aggressive exploration behavior while
ensuring the robot's safety. Our system includes a plug-and-play semantic
target mapping module that integrates seamlessly with state-of-the-art SLAM
algorithms for pointcloud-level dense semantic target mapping. We validate our
approach through extensive experiments in both realistic simulations and
complex real-world environments. Simulation results show that our planner
achieves faster exploration and shorter travel distances while guaranteeing a
specified number of multi-view inspections. Real-world experiments further
confirm the system's effectiveness in achieving accurate dense semantic object
mapping of unstructured environments.

</details>


### [370] [VLM-RRT: Vision Language Model Guided RRT Search for Autonomous UAV Navigation](https://arxiv.org/abs/2505.23267)
*Jianlin Ye,Savvas Papaioannou,Panayiotis Kolios*

Key words: 路径规划，无人机，VLM，RRT，采样效率

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 该论文提出了VLM-RRT方法，结合视觉语言模型（VLM）和RRT算法，通过VLM提供环境初始方向引导，优化路径规划效率和质量。

Motivation: 传统RRT方法在路径规划中面临搜索空间复杂度高、路径质量不佳和收敛速度慢的问题，尤其在灾难应对等高风险场景中表现不足。

Method: 提出VLM-RRT混合方法，利用VLM的环境识别能力为RRT提供初始方向引导，优化采样过程。

Result: 实验表明，VLM-RRT显著提高了采样效率和路径质量。

Conclusion: VLM-RRT有效解决了传统RRT的局限性，尤其适用于高风险应用场景。

Abstract: Path planning is a fundamental capability of autonomous Unmanned Aerial
Vehicles (UAVs), enabling them to efficiently navigate toward a target region
or explore complex environments while avoiding obstacles. Traditional
pathplanning methods, such as Rapidly-exploring Random Trees (RRT), have proven
effective but often encounter significant challenges. These include high search
space complexity, suboptimal path quality, and slow convergence, issues that
are particularly problematic in high-stakes applications like disaster
response, where rapid and efficient planning is critical. To address these
limitations and enhance path-planning efficiency, we propose Vision Language
Model RRT (VLM-RRT), a hybrid approach that integrates the pattern recognition
capabilities of Vision Language Models (VLMs) with the path-planning strengths
of RRT. By leveraging VLMs to provide initial directional guidance based on
environmental snapshots, our method biases sampling toward regions more likely
to contain feasible paths, significantly improving sampling efficiency and path
quality. Extensive quantitative and qualitative experiments with various
state-of-the-art VLMs demonstrate the effectiveness of this proposed approach.

</details>


### [371] [A Robot-Assisted Approach to Small Talk Training for Adults with ASD](https://arxiv.org/abs/2505.23508)
*Rebecca Ramnauth,Dražen Brščić,Brian Scassellati*

Key words: 自闭症谱系障碍, 闲聊, 自主机器人, 社交技能

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 摘要介绍了针对自闭症谱系障碍(ASD)成人的家庭自主机器人系统，用于练习闲聊。为期一周的研究显示，参与者享受训练，在发起对话和改善眼神交流方面取得了显著进步，并认为该系统是提升对话技能的有价值工具。

Motivation: ASD成人进行闲聊面临挑战，但这对社交融合、建立关系和职业机会至关重要。研究旨在通过机器人系统帮助他们练习和改善这项技能。

Method: 开发并评估了一个家庭自主机器人系统，ASD成人通过与机器人互动来练习闲聊。

Result: 一周的研究结果显示，参与者享受训练，在发起对话和改善眼神交流方面取得了显著进步，并认为该系统有效。

Conclusion: 家庭自主机器人系统是ASD成人提升闲聊技能的有价值工具，且用户体验积极。

Abstract: From dating to job interviews, making new friends or simply chatting with the
cashier at checkout, engaging in small talk is a vital, everyday social skill.
For adults with Autism Spectrum Disorder (ASD), small talk can be particularly
challenging, yet it is essential for social integration, building
relationships, and accessing professional opportunities. In this study, we
present our development and evaluation of an in-home autonomous robot system
that allows users to practice small talk. Results from the week-long study show
that adults with ASD enjoyed the training, made notable progress in initiating
conversations and improving eye contact, and viewed the system as a valuable
tool for enhancing their conversational skills.

</details>


### [372] [Cognitive Guardrails for Open-World Decision Making in Autonomous Drone Swarms](https://arxiv.org/abs/2505.23576)
*Jane Cleland-Huang,Pedro Antonio Alarcon Granadeno,Arturo Miguel Russell Bernal,Demetrius Hernandez,Michael Murphy,Maureen Petterson,Walter Scheirer*

Key words: 小型无人机群, 大型语言模型, 认知护栏, 搜救任务, 自主决策

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 本文探讨如何在小型无人机群（sUAS）中结合大型语言模型（LLMs）提升目标识别与任务适应性，同时通过认知护栏确保决策安全。

Motivation: 传统计算机视觉在开放环境中识别陌生物体及推断其任务相关性时表现不足，而LLMs能提供推理能力但可能产生错误或不安全建议。

Method: 设计、模拟并实际集成了认知护栏机制，用于sUAS群在搜救任务中的高层决策。

Result: 通过认知护栏可有效平衡LLMs的推理能力与决策安全性，提升无人机群的自主适应性。

Conclusion: 认知护栏是确保LLMs在开放世界中安全应用于sUAS群决策的关键。

Abstract: Small Uncrewed Aerial Systems (sUAS) are increasingly deployed as autonomous
swarms in search-and-rescue and other disaster-response scenarios. In these
settings, they use computer vision (CV) to detect objects of interest and
autonomously adapt their missions. However, traditional CV systems often
struggle to recognize unfamiliar objects in open-world environments or to infer
their relevance for mission planning. To address this, we incorporate large
language models (LLMs) to reason about detected objects and their implications.
While LLMs can offer valuable insights, they are also prone to hallucinations
and may produce incorrect, misleading, or unsafe recommendations. To ensure
safe and sensible decision-making under uncertainty, high-level decisions must
be governed by cognitive guardrails. This article presents the design,
simulation, and real-world integration of these guardrails for sUAS swarms in
search-and-rescue missions.

</details>


### [373] [Anomalies by Synthesis: Anomaly Detection using Generative Diffusion Models for Off-Road Navigation](https://arxiv.org/abs/2505.22805)
*Siddharth Ancha,Sunshine Jiang,Travis Manderson,Laura Brandt,Yilun Du,Philip R. Osteen,Nicholas Roy*

Key words: 异常检测, 扩散模型, 非结构化环境, 像素级分析, 机器人导航

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 论文提出了一种基于合成分析的像素级异常检测方法，通过生成扩散模型编辑图像以去除异常，利用引导梯度分析实现无需重新训练的高效异常检测。

Motivation: 在非结构化环境中，机器人需要检测训练数据分布之外的异常以确保安全导航。现有方法难以应对未知异常，因此需要一种无需假设异常性质的通用检测方法。

Method: 1. 使用生成扩散模型编辑输入图像以去除异常；2. 分析扩散模型的修改区域检测异常；3. 提出新的引导梯度推断方法；4. 结合视觉-语言基础模型在特征空间比较像素。

Result: 该方法能准确检测语义有意义的异常，适用于非结构化环境导航，且无需重新训练或微调。

Conclusion: 通过合成分析和引导梯度优化，提出了一种高效的像素级异常检测框架，适用于未知异常场景。

Abstract: In order to navigate safely and reliably in off-road and unstructured
environments, robots must detect anomalies that are out-of-distribution (OOD)
with respect to the training data. We present an analysis-by-synthesis approach
for pixel-wise anomaly detection without making any assumptions about the
nature of OOD data. Given an input image, we use a generative diffusion model
to synthesize an edited image that removes anomalies while keeping the
remaining image unchanged. Then, we formulate anomaly detection as analyzing
which image segments were modified by the diffusion model. We propose a novel
inference approach for guided diffusion by analyzing the ideal guidance
gradient and deriving a principled approximation that bootstraps the diffusion
model to predict guidance gradients. Our editing technique is purely test-time
that can be integrated into existing workflows without the need for retraining
or fine-tuning. Finally, we use a combination of vision-language foundation
models to compare pixels in a learned feature space and detect semantically
meaningful edits, enabling accurate anomaly detection for off-road navigation.
Project website: https://siddancha.github.io/anomalies-by-diffusion-synthesis/

</details>


### [374] [Learning coordinated badminton skills for legged manipulators](https://arxiv.org/abs/2505.22974)
*Yuntao Ma,Andrei Cramariuc,Farbod Farshidian,Marco Hutter*

Key words: 腿式移动机械臂, 强化学习, 视觉运动控制, 动态环境, 羽毛球

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 利用强化学习统一控制算法,实现腿式移动机械臂在动态环境中的羽毛球运动,结合感知噪声模型提高仿真与部署的一致性,验证了复杂体育场景下的可行性。

Motivation: 解决机器人下肢与上肢运动协调、感知与动作对齐的挑战,尤其是动态环境中的精准任务如羽毛球运动。

Method: 提出基于强化学习的全身视觉运动控制策略,结合感知噪声模型、羽毛球轨迹预测模型和约束强化学习,增强部署稳定性。

Result: 实验显示机器人能有效预测羽毛球轨迹、导航服务区域并精准击球,验证了在复杂动态场景中的能力。

Conclusion: 证明了腿式移动机械臂在动态体育任务中的可行性,为复杂环境下的机器人控制提供了新思路。

Abstract: Coordinating the motion between lower and upper limbs and aligning limb
control with perception are substantial challenges in robotics, particularly in
dynamic environments. To this end, we introduce an approach for enabling legged
mobile manipulators to play badminton, a task that requires precise
coordination of perception, locomotion, and arm swinging. We propose a unified
reinforcement learning-based control policy for whole-body visuomotor skills
involving all degrees of freedom to achieve effective shuttlecock tracking and
striking. This policy is informed by a perception noise model that utilizes
real-world camera data, allowing for consistent perception error levels between
simulation and deployment and encouraging learned active perception behaviors.
Our method includes a shuttlecock prediction model, constrained reinforcement
learning for robust motion control, and integrated system identification
techniques to enhance deployment readiness. Extensive experimental results in a
variety of environments validate the robot's capability to predict shuttlecock
trajectories, navigate the service area effectively, and execute precise
strikes against human players, demonstrating the feasibility of using legged
mobile manipulators in complex and dynamic sports scenarios.

</details>


### [375] [Mobi-$π$: Mobilizing Your Robot Learning Policy](https://arxiv.org/abs/2505.23692)
*Jingyun Yang,Isabella Huang,Brandon Vu,Max Bajracharya,Rika Antonova,Jeannette Bohg*

Key words: 策略迁移, 移动操作, 泛化能力, 高斯泼溅, 机器人位姿优化

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 论文提出了一种解决策略迁移问题的新方法Mobi-π，通过优化机器人基座位姿来适应训练数据有限的策略，提升在新环境中的泛化能力。

Motivation: 现有视觉运动策略在训练数据视角有限时泛化能力差，尤其影响移动平台上精确操作任务的执行，如按键或开关水龙头。

Method: 提出Mobi-π框架：包括评估策略迁移难度的指标、基于RoboCasa的模拟任务、可视化工具及基线方法，并通过3D高斯泼溅技术优化基座位姿。

Result: 提出的方法在仿真和真实环境中均优于基线方法，验证了其在策略迁移中的有效性。

Conclusion: 策略迁移问题通过优化基座位姿解决了有限训练数据导致的泛化问题，且兼容现有提升策略鲁棒性的方法。

Abstract: Learned visuomotor policies are capable of performing increasingly complex
manipulation tasks. However, most of these policies are trained on data
collected from limited robot positions and camera viewpoints. This leads to
poor generalization to novel robot positions, which limits the use of these
policies on mobile platforms, especially for precise tasks like pressing
buttons or turning faucets. In this work, we formulate the policy mobilization
problem: find a mobile robot base pose in a novel environment that is in
distribution with respect to a manipulation policy trained on a limited set of
camera viewpoints. Compared to retraining the policy itself to be more robust
to unseen robot base pose initializations, policy mobilization decouples
navigation from manipulation and thus does not require additional
demonstrations. Crucially, this problem formulation complements existing
efforts to improve manipulation policy robustness to novel viewpoints and
remains compatible with them. To study policy mobilization, we introduce the
Mobi-$\pi$ framework, which includes: (1) metrics that quantify the difficulty
of mobilizing a given policy, (2) a suite of simulated mobile manipulation
tasks based on RoboCasa to evaluate policy mobilization, (3) visualization
tools for analysis, and (4) several baseline methods. We also propose a novel
approach that bridges navigation and manipulation by optimizing the robot's
base pose to align with an in-distribution base pose for a learned policy. Our
approach utilizes 3D Gaussian Splatting for novel view synthesis, a score
function to evaluate pose suitability, and sampling-based optimization to
identify optimal robot poses. We show that our approach outperforms baselines
in both simulation and real-world environments, demonstrating its effectiveness
for policy mobilization.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [376] [Efficient Quantum Approximate $k$NN Algorithm via Granular-Ball Computing](https://arxiv.org/abs/2505.23066)
*Shuyin Xia,Xiaojiang Tian,Suzhen Yuan,Jeremiah D. Deng*

Key words: kNN, 时间复杂度, 粒度球, HNSW, 量子化

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 论文提出了一种名为GB-QkNN的新算法，通过结合粒度球和量子化的HNSW方法，显著降低了kNN算法的时间复杂度。

Motivation: 解决kNN算法在面对大量数据时的时间复杂度高的问题。

Method: 采用粒度球减少数据规模，并结合量子化的HNSW方法来加速搜索过程。

Result: 通过复杂度分析，证明新算法显著降低了kNN算法的时间复杂度。

Conclusion: GB-QkNN通过结合粒度球和量子化HNSW方法，成功提升了kNN算法的效率。

Abstract: High time complexity is one of the biggest challenges faced by $k$-Nearest
Neighbors ($k$NN). Although current classical and quantum $k$NN algorithms have
made some improvements, they still have a speed bottleneck when facing large
amounts of data. To address this issue, we propose an innovative algorithm
called Granular-Ball based Quantum $k$NN(GB-Q$k$NN). This approach achieves
higher efficiency by first employing granular-balls, which reduces the data
size needed to processed. The search process is then accelerated by adopting a
Hierarchical Navigable Small World (HNSW) method. Moreover, we optimize the
time-consuming steps, such as distance calculation, of the HNSW via
quantization, further reducing the time complexity of the construct and search
process. By combining the use of granular-balls and quantization of the HNSW
method, our approach manages to take advantage of these treatments and
significantly reduces the time complexity of the $k$NN-like algorithms, as
revealed by a comprehensive complexity analysis.

</details>


### [377] [Information-Computation Gaps in Quantum Learning via Low-Degree Likelihood](https://arxiv.org/abs/2505.22743)
*Sitan Chen,Weiyuan Gong,Jonas Haferkamp,Yihui Quek*

Key words: 量子学习, 低阶方法, 信息计算间隙, Gibbs态, 自适应测量

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 本文扩展了经典的低阶方法框架到量子设置，建立了状态设计与低阶硬度的联系，提出了针对随机、稀疏、非局域哈密顿量的信息-计算间隙，并证明了在自适应测量模型中学习随机浅层量子电路态的困难性。

Motivation: 解决量子数据学习中信息提取的计算效率问题，填补量子文献中平均案例硬度证据工具的不足。

Method: 扩展经典低阶方法到量子设置，利用状态设计与低阶硬度的连接，分析学习随机稀疏非局域哈密顿量的Gibbs态、自适应测量模型中的随机浅层量子电路态等问题。

Result: 首次提出了针对随机、稀疏、非局域哈密顿量的信息-计算间隙，证明了在自适应测量模型中的学习困难性，并展示了量子误差缓解的低阶硬度。

Conclusion: 量子低阶方法框架为量子数据学习的计算硬度提供了新证据，揭示了量子测量模型中的复杂性变化。

Abstract: In a variety of physically relevant settings for learning from quantum data,
designing protocols that can computationally efficiently extract information
remains largely an art, and there are important cases where we believe this to
be impossible, that is, where there is an information-computation gap. While
there is a large array of tools in the classical literature for giving evidence
for average-case hardness of statistical inference problems, the corresponding
tools in the quantum literature are far more limited. One such framework in the
classical literature, the low-degree method, makes predictions about hardness
of inference problems based on the failure of estimators given by low-degree
polynomials. In this work, we extend this framework to the quantum setting.
  We establish a general connection between state designs and low-degree
hardness. We use this to obtain the first information-computation gaps for
learning Gibbs states of random, sparse, non-local Hamiltonians. We also use it
to prove hardness for learning random shallow quantum circuit states in a
challenging model where states can be measured in adaptively chosen bases. To
our knowledge, the ability to model adaptivity within the low-degree framework
was open even in classical settings. In addition, we also obtain a low-degree
hardness result for quantum error mitigation against strategies with
single-qubit measurements.
  We define a new quantum generalization of the planted biclique problem and
identify the threshold at which this problem becomes computationally hard for
protocols that perform local measurements. Interestingly, the complexity
landscape for this problem shifts when going from local measurements to more
entangled single-copy measurements.
  We show average-case hardness for the "standard" variant of Learning
Stabilizers with Noise and for agnostically learning product states.

</details>


### [378] [Dynamic Estimation Loss Control in Variational Quantum Sensing via Online Conformal Inference](https://arxiv.org/abs/2505.23389)
*Ivana Nikoloska,Hamdi Joudeh,Ruud van Sloun,Osvaldo Simeone*

Key words: 量子传感、变分量子算法、在线共形推理、NISQ设备

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 动态变分量子传感（VQS）框架通过在线控制更新参数并提供确定性误差估计，结合在线共形推理技术，确保长期风险水平，实验验证其在NISQ设备上的可靠性和精确性。

Motivation: 解决当前变分量子传感方法在噪声中规模量子（NISQ）设备上缺乏严格性能保证的问题。

Method: 提出动态变分量子传感框架，结合在线共形推理技术，动态更新参数并提供确定性误差估计。

Result: 实验验证该方法在量子磁力测量任务中保持可靠性和精确性。

Conclusion: 动态VQS结合在线共形推理可实现NISQ设备上的可靠量子传感。

Abstract: Quantum sensing exploits non-classical effects to overcome limitations of
classical sensors, with applications ranging from gravitational-wave detection
to nanoscale imaging. However, practical quantum sensors built on noisy
intermediate-scale quantum (NISQ) devices face significant noise and sampling
constraints, and current variational quantum sensing (VQS) methods lack
rigorous performance guarantees. This paper proposes an online control
framework for VQS that dynamically updates the variational parameters while
providing deterministic error bars on the estimates. By leveraging online
conformal inference techniques, the approach produces sequential estimation
sets with a guaranteed long-term risk level. Experiments on a quantum
magnetometry task confirm that the proposed dynamic VQS approach maintains the
required reliability over time, while still yielding precise estimates. The
results demonstrate the practical benefits of combining variational quantum
algorithms with online conformal inference to achieve reliable quantum sensing
on NISQ devices.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [379] [Aurora: Are Android Malware Classifiers Reliable under Distribution Shift?](https://arxiv.org/abs/2505.22843)
*Alexander Herzog,Aliai Eusebi,Lorenzo Cavallaro*

Key words: AURORA, 恶意软件分类器, 置信度对齐, 操作稳定性, 分布偏移

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出AURORA框架，评估恶意软件分类器在分布偏移下的置信度可靠性和操作稳定性，强调当前评估范式忽略的置信误差对齐问题。

Motivation: 现有恶意软件分类器评估过于依赖基线性能指标，忽略了置信度与错误的匹配及操作稳定性，需更全面的评估方法。

Method: 提出AURORA框架，通过验证模型置信度质量及操作韧性来评估其可靠性，并设计了一组超越瞬时性能的指标。

Result: 实验发现当前先进框架在不同漂移严重性数据集上表现脆弱，表明需重新审视模型设计。

Conclusion: 研究呼吁回归设计本质，强调置信度可靠性和操作稳定性对恶意软件分类器的重要性。

Abstract: The performance figures of modern drift-adaptive malware classifiers appear
promising, but does this translate to genuine operational reliability? The
standard evaluation paradigm primarily focuses on baseline performance metrics,
neglecting confidence-error alignment and operational stability. While
TESSERACT established the importance of temporal evaluation, we take a
complementary direction by investigating whether malware classifiers maintain
reliable confidence estimates under distribution shifts and exploring the
tensions between scientific advancement and practical impacts when they do not.
We propose AURORA, a framework to evaluate malware classifiers based on their
confidence quality and operational resilience. AURORA subjects the confidence
profile of a given model to verification to assess the reliability of its
estimates. Unreliable confidence estimates erode operational trust, waste
valuable annotation budget on non-informative samples for active learning, and
leave error-prone instances undetected in selective classification. AURORA is
further complemented by a set of metrics designed to go beyond point-in-time
performance, striving towards a more holistic assessment of operational
stability throughout temporal evaluation periods. The fragility we observe in
state-of-the-art frameworks across datasets of varying drift severity suggests
the need for a return to the whiteboard.

</details>


### [380] [Security Benefits and Side Effects of Labeling AI-Generated Images](https://arxiv.org/abs/2505.22845)
*Sandra Höltervennhoff,Jonas Ricker,Maike M. Raphael,Charlotte Schwedes,Rebecca Weil,Asja Fischer,Thorsten Holz,Lea Schönherr,Sascha Fahl*

Key words: 生成式人工智能, 错误信息, 内容标签, 用户识别, 可信度

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 研究了AI生成内容标签对识别错误信息的影响，发现标签有助于识别AI生成图像，但可能导致过度依赖，并对人类生成图像的可信度产生负面影响。

Motivation: 探讨AI生成内容标签是否能有效减少错误信息的风险，了解用户对标签的看法及其实施的挑战。

Method: 通过5个焦点小组定性分析用户意见，并进行了超过1300名美国和欧盟参与者的在线调查，定量评估标签对识别错误信息的影响。

Result: 标签有助于识别AI生成图像，但可能导致过度依赖标签，且对人类生成图像的可信度有负面影响。

Conclusion: AI标签虽有用但需谨慎设计，避免用户过度依赖及对非AI内容的误判。

Abstract: Generative artificial intelligence is developing rapidly, impacting humans'
interaction with information and digital media. It is increasingly used to
create deceptively realistic misinformation, so lawmakers have imposed
regulations requiring the disclosure of AI-generated content. However, only
little is known about whether these labels reduce the risks of AI-generated
misinformation.
  Our work addresses this research gap. Focusing on AI-generated images, we
study the implications of labels, including the possibility of mislabeling.
Assuming that simplicity, transparency, and trust are likely to impact the
successful adoption of such labels, we first qualitatively explore users'
opinions and expectations of AI labeling using five focus groups. Second, we
conduct a pre-registered online survey with over 1300 U.S. and EU participants
to quantitatively assess the effect of AI labels on users' ability to recognize
misinformation containing either human-made or AI-generated images. Our focus
groups illustrate that, while participants have concerns about the practical
implementation of labeling, they consider it helpful in identifying
AI-generated images and avoiding deception. However, considering security
benefits, our survey revealed an ambiguous picture, suggesting that users might
over-rely on labels. While inaccurate claims supported by labeled AI-generated
images were rated less credible than those with unlabeled AI-images, the belief
in accurate claims also decreased when accompanied by a labeled AI-generated
image. Moreover, we find the undesired side effect that human-made images
conveying inaccurate claims were perceived as more credible in the presence of
labels.

</details>


### [381] [Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment](https://arxiv.org/abs/2505.22852)
*Krti Tallam,Emma Miller*

Key words: CaMeL, LLM, 提示注入, 安全, 能力沙箱

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: CaMeL提出了一种基于能力的沙箱来减轻LLM代理中的提示注入攻击，但存在信任假设、侧信道问题和性能折衷。本文提出了改进措施，包括提示筛查、输出审计、分层风险访问模型和验证中间语言，以增强安全性和实用性。

Motivation: 解决CaMeL在信任假设、侧信道保护和性能方面的不足，进一步提升其安全性和可扩展性。

Method: 引入了四项改进措施：1) 初始输入的提示筛查；2) 检测指令泄漏的输出审计；3) 分层风险访问模型；4) 形式化保证的验证中间语言。

Result: 改进后的CaMeL更符合企业安全最佳实践，支持可扩展部署。

Conclusion: 通过工程改进，CaMeL在威胁覆盖和操作实用性上得到显著提升。

Abstract: CaMeL (Capabilities for Machine Learning) introduces a capability-based
sandbox to mitigate prompt injection attacks in large language model (LLM)
agents. While effective, CaMeL assumes a trusted user prompt, omits
side-channel concerns, and incurs performance tradeoffs due to its dual-LLM
design. This response identifies these issues and proposes engineering
improvements to expand CaMeL's threat coverage and operational usability. We
introduce: (1) prompt screening for initial inputs, (2) output auditing to
detect instruction leakage, (3) a tiered-risk access model to balance usability
and control, and (4) a verified intermediate language for formal guarantees.
Together, these upgrades align CaMeL with best practices in enterprise security
and support scalable deployment.

</details>


### [382] [Permissioned LLMs: Enforcing Access Control in Large Language Models](https://arxiv.org/abs/2505.22860)
*Bargav Jayaraman,Virendra J. Marathe,Hamid Mozaffari,William F. Shen,Krishnaram Kenthapadi*

Key words: Permissioned LLMs, 访问控制, 参数高效微调, 相关响应, access advantage

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 提出Permissioned LLMs (PermLLM)，一种新类别的大语言模型，能够在生成响应时实施组织数据访问控制结构，并通过实验验证其有效性。

Motivation: 企业环境中数据被隔离和保护，但基于这些数据微调的LLM可能破坏访问控制。需要确保生成响应时仍能正确实施访问权限。

Method: 提出PermLLM机制，基于参数高效微调（PEFT）实现访问控制；引入相关响应形式化验证框架和评估指标access advantage。

Result: 在四个公开数据集（GPQA、RCV1、SimpleQA和WMDP）上验证PermLLM机制的有效性，并确认DDI和UGI指标的合理性。

Conclusion: PermLLM能有效实施数据访问控制，为组织数据安全提供新解决方案。

Abstract: In enterprise settings, organizational data is segregated, siloed and
carefully protected by elaborate access control frameworks. These access
control structures can completely break down if an LLM fine-tuned on the siloed
data serves requests, for downstream tasks, from individuals with disparate
access privileges. We propose Permissioned LLMs (PermLLM), a new class of LLMs
that superimpose the organizational data access control structures on query
responses they generate. We formalize abstractions underpinning the means to
determine whether access control enforcement happens correctly over LLM query
responses. Our formalism introduces the notion of a relevant response that can
be used to prove whether a PermLLM mechanism has been implemented correctly. We
also introduce a novel metric, called access advantage, to empirically evaluate
the efficacy of a PermLLM mechanism. We introduce three novel PermLLM
mechanisms that build on Parameter Efficient Fine-Tuning to achieve the desired
access control. We furthermore present two instantiations of access
advantage--(i) Domain Distinguishability Index (DDI) based on Membership
Inference Attacks, and (ii) Utility Gap Index (UGI) based on LLM utility
evaluation. We demonstrate the efficacy of our PermLLM mechanisms through
extensive experiments on four public datasets (GPQA, RCV1, SimpleQA, and WMDP),
in addition to evaluating the validity of DDI and UGI metrics themselves for
quantifying access control in LLMs.

</details>


### [383] [BugWhisperer: Fine-Tuning LLMs for SoC Hardware Vulnerability Detection](https://arxiv.org/abs/2505.22878)
*Shams Tarek,Dipayan Saha,Sujan Kumar Saha,Farimah Farahmandi*

Key words: SoC安全验证、大型语言模型、RTL漏洞、自动化框架、硬件漏洞数据库

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出BugWhisperer框架，通过微调的大语言模型（LLM）自动化SoC安全验证，提升效率和灵活性，并建立硬件漏洞数据库。

Motivation: 现有SoC安全验证方法依赖人工，缺乏灵活性和可扩展性，难以有效检测RTL级漏洞，亟需自动化解决方案。

Method: 使用专门微调的LLM，增强其硬件安全知识，利用文本推理和知识迁移能力，自动化验证流程并开源模型。

Result: 定制化LLM显著提升SoC安全验证的效率和适应性，同时配套漏洞数据库支持社区研究。

Conclusion: BugWhisperer框架为SoC安全验证提供了高效、可扩展的自动化方案，推动领域进步。

Abstract: The current landscape of system-on-chips (SoCs) security verification faces
challenges due to manual, labor-intensive, and inflexible methodologies. These
issues limit the scalability and effectiveness of security protocols, making
bug detection at the Register-Transfer Level (RTL) difficult. This paper
proposes a new framework named BugWhisperer that utilizes a specialized,
fine-tuned Large Language Model (LLM) to address these challenges. By enhancing
the LLM's hardware security knowledge and leveraging its capabilities for text
inference and knowledge transfer, this approach automates and improves the
adaptability and reusability of the verification process. We introduce an
open-source, fine-tuned LLM specifically designed for detecting security
vulnerabilities in SoC designs. Our findings demonstrate that this tailored LLM
effectively enhances the efficiency and flexibility of the security
verification process. Additionally, we introduce a comprehensive hardware
vulnerability database that supports this work and will further assist the
research community in enhancing the security verification process.

</details>


### [384] [AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models](https://arxiv.org/abs/2505.23020)
*Jinchuan Zhang,Lu Yin,Yan Zhou,Songlin Hu*

Key words: LLM, 安全对齐, AgentAlign, 行为链, 开源模型

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 本文提出了AgentAlign框架，通过抽象行为链提升LLM安全性，评测显示显著降低恶意任务执行率同时保持实用性。

Motivation: LLM从知识提供者转向行动执行者后，易受恶意利用，现有安全措施不足，需新型对齐方法。

Method: 利用抽象行为链生成安全对齐数据，在模拟环境中实例化多样工具链，平衡无害性与实用性。

Result: 在AgentHarm评测中，模型安全性提升35.8%-79.5%，实用性影响极小甚至提升。

Conclusion: AgentAlign有效解决了LLM代理安全性问题，框架已开源。

Abstract: The acquisition of agentic capabilities has transformed LLMs from "knowledge
providers" to "action executors", a trend that while expanding LLMs' capability
boundaries, significantly increases their susceptibility to malicious use.
Previous work has shown that current LLM-based agents execute numerous
malicious tasks even without being attacked, indicating a deficiency in agentic
use safety alignment during the post-training phase. To address this gap, we
propose AgentAlign, a novel framework that leverages abstract behavior chains
as a medium for safety alignment data synthesis. By instantiating these
behavior chains in simulated environments with diverse tool instances, our
framework enables the generation of highly authentic and executable
instructions while capturing complex multi-step dynamics. The framework further
ensures model utility by proportionally synthesizing benign instructions
through non-malicious interpretations of behavior chains, precisely calibrating
the boundary between helpfulness and harmlessness. Evaluation results on
AgentHarm demonstrate that fine-tuning three families of open-source models
using our method substantially improves their safety (35.8% to 79.5%
improvement) while minimally impacting or even positively enhancing their
helpfulness, outperforming various prompting methods. The dataset and code have
both been open-sourced.

</details>


### [385] [Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion](https://arxiv.org/abs/2505.23266)
*Chunlong Xie,Jialing He,Shangwei Guo,Jiacheng Wang,Shudong Zhang,Tianwei Zhang,Tao Xiang*

Key words: Adversarial Object Fusion, VLN agents, VLM, service security, QoS

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: Adversarial Object Fusion (AdvOF) is a novel attack framework targeting VLN agents, degrading performance under adversarial conditions while minimally affecting normal tasks.

Motivation: Existing adversarial attacks fail to address service computing contexts, where reliability and QoS are crucial, prompting the need for AdvOF.

Method: AdvOF aggregates and aligns victim object positions in 2D/3D space, collaboratively optimizes adversarial objects with regularization, and uses multi-view iterative fusions.

Result: AdvOF effectively degrades VLN agent performance under adversarial conditions with minimal impact on normal navigation.

Conclusion: This work enhances understanding of service security in VLM-powered navigation systems, aiding robust service composition in real-world deployments.

Abstract: We present Adversarial Object Fusion (AdvOF), a novel attack framework
targeting vision-and-language navigation (VLN) agents in service-oriented
environments by generating adversarial 3D objects. While foundational models
like Large Language Models (LLMs) and Vision Language Models (VLMs) have
enhanced service-oriented navigation systems through improved perception and
decision-making, their integration introduces vulnerabilities in
mission-critical service workflows. Existing adversarial attacks fail to
address service computing contexts, where reliability and quality-of-service
(QoS) are paramount. We utilize AdvOF to investigate and explore the impact of
adversarial environments on the VLM-based perception module of VLN agents. In
particular, AdvOF first precisely aggregates and aligns the victim object
positions in both 2D and 3D space, defining and rendering adversarial objects.
Then, we collaboratively optimize the adversarial object with regularization
between the adversarial and victim object across physical properties and VLM
perceptions. Through assigning importance weights to varying views, the
optimization is processed stably and multi-viewedly by iterative fusions from
local updates and justifications. Our extensive evaluations demonstrate AdvOF
can effectively degrade agent performance under adversarial conditions while
maintaining minimal interference with normal navigation tasks. This work
advances the understanding of service security in VLM-powered navigation
systems, providing computational foundations for robust service composition in
physical-world deployments.

</details>


### [386] [Securing AI Agents with Information-Flow Control](https://arxiv.org/abs/2505.23643)
*Manuel Costa,Boris Köpf,Aashish Kolluri,Andrew Paverd,Mark Russinovich,Ahmed Salem,Shruti Tople,Lukas Wutschitz,Santiago Zanella-Béguelin*

Key words: AI代理, 信息流控制, 安全性, 动态污点跟踪, 规划器设计

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 该论文探讨了信息流控制（IFC）如何保障AI代理的安全性，提出了一种形式化模型和Fides规划器，通过动态污点跟踪和任务分类评估安全性与实用性的权衡。

Motivation: 随着AI代理自主能力的提升，防止如提示注入等安全漏洞变得至关重要，因此需要研究有效的安全防护机制。

Method: 提出了一种形式化模型，通过动态污点跟踪评估规划器的安全性，并设计了Fides规划器，实现确定性安全策略执行和信息选择性隐藏。

Result: 在AgentDojo中的评估表明，该方法扩大了可安全完成的任务范围，提升了AI代理的安全性。

Conclusion: 信息流控制和Fides规划器能有效增强AI代理的安全性，同时平衡安全与实用性需求。

Abstract: As AI agents become increasingly autonomous and capable, ensuring their
security against vulnerabilities such as prompt injection becomes critical.
This paper explores the use of information-flow control (IFC) to provide
security guarantees for AI agents. We present a formal model to reason about
the security and expressiveness of agent planners. Using this model, we
characterize the class of properties enforceable by dynamic taint-tracking and
construct a taxonomy of tasks to evaluate security and utility trade-offs of
planner designs. Informed by this exploration, we present Fides, a planner that
tracks confidentiality and integrity labels, deterministically enforces
security policies, and introduces novel primitives for selectively hiding
information. Its evaluation in AgentDojo demonstrates that this approach
broadens the range of tasks that can be securely accomplished. A tutorial to
walk readers through the the concepts introduced in the paper can be found at
https://github.com/microsoft/fides

</details>


### [387] [Keyed Chaotic Tensor Transformations for Secure And Attributable Neural Inference](https://arxiv.org/abs/2505.23655)
*Peter David Fagan*

Key words: 隐私保护、神经网络推理、混沌系统、水印、数据溯源

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 提出了一种基于密钥混沌动力学变换的隐私保护神经网络推理框架，通过确定性混沌系统生成不可逆的用户特定变换，支持认证推理、张量水印和数据溯源，为AI系统提供轻量级安全方案。

Motivation: 现有加密方法在神经网络推理中的隐私保护和安全性方面存在性能开销大或灵活性不足的问题，需要一种轻量且可扩展的替代方案。

Method: 使用加密种子驱动的混沌系统对张量进行变换，生成不可逆且用户特定的变换，实现认证推理、水印和数据溯源。

Result: 该框架在保持低计算开销的同时，提供了张量级别的安全性功能，如认证和水印。

Conclusion: 为AI系统提出了一种新颖的张量级安全方向，兼具实用性和轻量化特性。

Abstract: This work introduces a novel framework for secure and privacy-preserving
neural network inference based on keyed chaotic dynamical transformations. The
proposed method applies a deterministic, cryptographically seeded chaotic
system to tensors, producing non-invertible, user-specific transformations that
enable authenticated inference, tensor-level watermarking, and data
attribution. This framework offers a scalable and lightweight alternative to
conventional cryptographic techniques, and establishes a new direction for
tensor-level security in AI systems.

</details>


### [388] [Bayesian Perspective on Memorization and Reconstruction](https://arxiv.org/abs/2505.23658)
*Haim Kaplan,Yishay Mansour,Kobbi Nissim,Uri Stemmer*

Key words: 贝叶斯视角、数据重构、指纹码攻击、成员推断攻击

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出了一种新的贝叶斯视角下的数据重构定义，并在某些情况下证明了其能防止重构攻击；同时重新审视了指纹码攻击，认为其属于成员推断攻击而非重构攻击。

Motivation: 旨在通过贝叶斯框架重新定义数据重构，并提出一种新的安全定义，以在某些情况下防止重构攻击，同时澄清指纹码攻击的本质。

Method: 采用贝叶斯视角分析数据重构，提出新的安全定义，并通过对指纹码攻击的重新分类验证理论。

Result: 证明了在某些情况下重构攻击可被防止，并指出指纹码攻击实际上是成员推断攻击，而非重构攻击。

Conclusion: 新视角为数据安全提供了更清晰的界定，同时表明防止重构攻击并非完全不可能。

Abstract: We introduce a new Bayesian perspective on the concept of data
reconstruction, and leverage this viewpoint to propose a new security
definition that, in certain settings, provably prevents reconstruction attacks.
We use our paradigm to shed new light on one of the most notorious attacks in
the privacy and memorization literature - fingerprinting code attacks (FPC). We
argue that these attacks are really a form of membership inference attacks,
rather than reconstruction attacks. Furthermore, we show that if the goal is
solely to prevent reconstruction (but not membership inference), then in some
cases the impossibility results derived from FPC no longer apply.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [389] [Generative Social Choice: The Next Generation](https://arxiv.org/abs/2505.22939)
*Niclas Boehmer,Sara Fish,Ariel D. Procaccia*

Key words: 生成社会选择、大语言模型、代表性陈述、预算限制、GPT-4o

<details>
  <summary>Details</summary>

Main category: cs.GT

TL;DR: 该论文提出了一种扩展生成社会选择框架的方法，确保在近似最优查询和预算限制下生成代表性陈述清单，并在城市改进措施和药物评论数据集上验证了有效性。

Motivation: 解决从用户意见中生成代表性陈述清单的问题，尤其是在候选集庞大且只能通过特定查询访问的情况下。

Method: 结合社会选择和大语言模型，扩展生成社会选择框架，引入理论保证和预算限制，使用GPT-4o实现查询。

Result: 在城市改进措施和药物评论数据集上成功生成了代表性的陈述清单，证明了方法的有效性。

Conclusion: 扩展的生成社会选择框架能够有效解决从大量用户意见中生成代表性陈述清单的挑战。

Abstract: A key task in certain democratic processes is to produce a concise slate of
statements that proportionally represents the full spectrum of user opinions.
This task is similar to committee elections, but unlike traditional settings,
the candidate set comprises all possible statements of varying lengths, and so
it can only be accessed through specific queries. Combining social choice and
large language models, prior work has approached this challenge through a
framework of generative social choice. We extend the framework in two
fundamental ways, providing theoretical guarantees even in the face of
approximately optimal queries and a budget limit on the overall length of the
slate. Using GPT-4o to implement queries, we showcase our approach on datasets
related to city improvement measures and drug reviews, demonstrating its
effectiveness in generating representative slates from unstructured user
opinions.

</details>


### [390] [Learning to Incentivize in Repeated Principal-Agent Problems with Adversarial Agent Arrivals](https://arxiv.org/abs/2505.23124)
*Junyan Liu,Arnab Maiti,Artin Tajdini,Kevin Jamieson,Lillian J. Ratliff*

Key words: 重复主代理问题、对抗性顺序、遗憾最小化、贪婪行为、Lipschitz响应、多臂激励

<details>
  <summary>Details</summary>

Main category: cs.GT

TL;DR: 该研究首次探讨了有限时间T内重复的主代理问题，涉及K≥2种代理类型的对抗性顺序到达。主方在每轮战略性地激励未知类型的代理，以最小化后悔。研究发现，在不知代理行为时问题难以解决，会导致线性后悔。但在两种关键设定下（已知代理贪婪选择或响应平滑），可以实现次线性后悔，并提出了相应算法和匹配的下界。最后，扩展了算法以支持每轮激励多个臂。

Motivation: 探讨在有限时间内，主方如何通过与多种类型代理的重复交互来优化激励策略，以最小化后悔。特别是在代理类型和响应未知的情况下，识别可解决的子问题并提出高效算法。

Method: 1) 在已知代理类型贪婪行为的设定下，提出遗憾上界为$O(\min\{\sqrt{KT\log N},K\sqrt{T}\})$的算法；2) 在代理响应受Lipschitz常数$L$约束的平滑设定下，设计遗憾上界为$\tilde{O}((LN)^{1/3}T^{2/3})$的算法；3) 扩展至每轮激励多个臂的场景。

Result: 1) 贪婪行为设定下得到紧界（匹配下界至$\log K$因子）；2) 平滑设定下算法与下界匹配（对数因子内）；3) 多臂激励扩展验证了算法的通用性。

Conclusion: 通过结构化的信息假设（贪婪行为或平滑响应），可在对抗性代理顺序下实现次线性后悔。算法设计紧贴理论下界，且扩展性强。

Abstract: We initiate the study of a repeated principal-agent problem over a finite
horizon $T$, where a principal sequentially interacts with $K\geq 2$ types of
agents arriving in an adversarial order. At each round, the principal
strategically chooses one of the $N$ arms to incentivize for an arriving agent
of unknown type. The agent then chooses an arm based on its own utility and the
provided incentive, and the principal receives a corresponding reward. The
objective is to minimize regret against the best incentive in hindsight.
Without prior knowledge of agent behavior, we show that the problem becomes
intractable, leading to linear regret. We analyze two key settings where
sublinear regret is achievable. In the first setting, the principal knows the
arm each agent type would select greedily for any given incentive. Under this
setting, we propose an algorithm that achieves a regret bound of
$O(\min\{\sqrt{KT\log N},K\sqrt{T}\})$ and provide a matching lower bound up to
a $\log K$ factor. In the second setting, an agent's response varies smoothly
with the incentive and is governed by a Lipschitz constant $L\geq 1$. Under
this setting, we show that there is an algorithm with a regret bound of
$\tilde{O}((LN)^{1/3}T^{2/3})$ and establish a matching lower bound up to
logarithmic factors. Finally, we extend our algorithmic results for both
settings by allowing the principal to incentivize multiple arms simultaneously
in each round.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [391] [Improved Learning via k-DTW: A Novel Dissimilarity Measure for Curves](https://arxiv.org/abs/2505.23431)
*Amer Krivošija,Alexander Munteanu,André Nusser,Chris Schwiegelshohn*

Key words: 关键词包括$k$-Dynamic Time Warping, 多边形曲线, 差异度量, 学习边界, 聚类, k近邻分类

<details>
  <summary>Details</summary>

Main category: cs.DS

TL;DR: $k$-Dynamic Time Warping ($k$-DTW) 是一种新的多边形曲线差异度量方法，比DTW具有更强的度量特性，比Fréchet距离对异常值更鲁棒。它通过参数搜索实现精确和近似算法，并首次为曲线提供了无维数的学习边界。实验表明$k$-DTW在聚类和k近邻分类中表现优越。

Motivation: 动机是改进现有的多边形曲线差异度量方法（如DTW和Fréchet距离），以提供更强的度量特性和对异常值的鲁棒性。

Method: 方法包括提出$k$-DTW，并通过参数搜索实现精确算法和$(1+\varepsilon)$-近似算法，同时分析其理论性质。

Result: 结果显示$k$-DTW在小样本学习、Rademacher和Gaussian复杂度上优于DTW，实验验证了其在聚类和分类中的优越性。

Conclusion: 结论是$k$-DTW是一种有效的多边形曲线差异度量方法，具有理论优势和实际应用潜力。

Abstract: This paper introduces $k$-Dynamic Time Warping ($k$-DTW), a novel
dissimilarity measure for polygonal curves. $k$-DTW has stronger metric
properties than Dynamic Time Warping (DTW) and is more robust to outliers than
the Fr\'{e}chet distance, which are the two gold standards of dissimilarity
measures for polygonal curves. We show interesting properties of $k$-DTW and
give an exact algorithm as well as a $(1+\varepsilon)$-approximation algorithm
for $k$-DTW by a parametric search for the $k$-th largest matched distance. We
prove the first dimension-free learning bounds for curves and further learning
theoretic results. $k$-DTW not only admits smaller sample size than DTW for the
problem of learning the median of curves, where some factors depending on the
curves' complexity $m$ are replaced by $k$, but we also show a surprising
separation on the associated Rademacher and Gaussian complexities: $k$-DTW
admits strictly smaller bounds than DTW, by a factor $\tilde\Omega(\sqrt{m})$
when $k\ll m$. We complement our theoretical findings with an experimental
illustration of the benefits of using $k$-DTW for clustering and nearest
neighbor classification.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [392] [MIAS-SAM: Medical Image Anomaly Segmentation without thresholding](https://arxiv.org/abs/2505.22762)
*Marco Colussi,Dragan Ahmetovic,Sergio Mascetti*

Key words: 医学图像分割, 异常检测, SAM, 记忆库, 无阈值分割

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: MIAS-SAM是一种新的医学图像异常区域分割方法，利用基于补丁的记忆库和SAM编码器提取特征，无需阈值设定即可实现准确分割。

Motivation: 现有方法在医学图像异常分割中通常需要手动设置阈值，MIAS-SAM旨在消除这一需求，提高分割的准确性和自动化程度。

Method: 通过SAM编码器从正常数据中提取图像特征并存储到记忆库中，推理时将输入图像的嵌入补丁与记忆库对比生成异常图，再通过SAM解码器完成分割。

Result: 在三个公开数据集（脑部MRI、肝脏CT和视网膜OCT）上的实验表明，MIAS-SAM能实现高精度的异常分割，DICE评分表现优异。

Conclusion: MIAS-SAM提供了一种无需阈值设定的高效异常分割方法，适用于多种医学成像模态。

Abstract: This paper presents MIAS-SAM, a novel approach for the segmentation of
anomalous regions in medical images. MIAS-SAM uses a patch-based memory bank to
store relevant image features, which are extracted from normal data using the
SAM encoder. At inference time, the embedding patches extracted from the SAM
encoder are compared with those in the memory bank to obtain the anomaly map.
Finally, MIAS-SAM computes the center of gravity of the anomaly map to prompt
the SAM decoder, obtaining an accurate segmentation from the previously
extracted features. Differently from prior works, MIAS-SAM does not require to
define a threshold value to obtain the segmentation from the anomaly map.
Experimental results conducted on three publicly available datasets, each with
a different imaging modality (Brain MRI, Liver CT, and Retina OCT) show
accurate anomaly segmentation capabilities measured using DICE score. The code
is available at: https://github.com/warpcut/MIAS-SAM

</details>


### [393] [IMTS is Worth Time $\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction](https://arxiv.org/abs/2505.22815)
*Zhangyi Hu,Jiemin Wu,Hua Xu,Mingqian Liao,Ninghui Feng,Bo Gao,Songning Lai,Yutao Yue*

Key words: 不规则多变量时间序列、视觉 MAE、缺失值补全、自监督学习、预测框架

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: VIMTS 是一个基于视觉 MAE 的框架，用于处理不规则多变量时间序列（IMTS），通过特征补全和自监督学习提升预测性能。

Motivation: 现有方法难以从不规则多变量时间序列（含大量缺失值）中提取可靠时序模式，而视觉 MAE 在稀疏多通道数据处理中的潜力尚未充分探索。

Method: 将 IMTS 分割为等间隔特征块，利用跨通道依赖补全缺失值，结合视觉 MAE 进行块重建，并通过粗到细技术生成预测。

Result: VIMTS 在实验中表现出优越性能和小样本学习能力，推动了视觉基础模型在通用时序任务中的应用。

Conclusion: VIMTS 成功将视觉 MAE 适配于 IMTS 预测，为处理复杂缺失值问题提供了新思路。

Abstract: Irregular Multivariate Time Series (IMTS) forecasting is challenging due to
the unaligned nature of multi-channel signals and the prevalence of extensive
missing data. Existing methods struggle to capture reliable temporal patterns
from such data due to significant missing values. While pre-trained foundation
models show potential for addressing these challenges, they are typically
designed for Regularly Sampled Time Series (RTS). Motivated by the visual Mask
AutoEncoder's (MAE) powerful capability for modeling sparse multi-channel
information and its success in RTS forecasting, we propose VIMTS, a framework
adapting Visual MAE for IMTS forecasting. To mitigate the effect of missing
values, VIMTS first processes IMTS along the timeline into feature patches at
equal intervals. These patches are then complemented using learned
cross-channel dependencies. Then it leverages visual MAE's capability in
handling sparse multichannel data for patch reconstruction, followed by a
coarse-to-fine technique to generate precise predictions from focused contexts.
In addition, we integrate self-supervised learning for improved IMTS modeling
by adapting the visual MAE to IMTS data. Extensive experiments demonstrate
VIMTS's superior performance and few-shot capability, advancing the application
of visual foundation models in more general time series tasks. Our code is
available at https://github.com/WHU-HZY/VIMTS.

</details>


### [394] [ATI: Any Trajectory Instruction for Controllable Video Generation](https://arxiv.org/abs/2505.22944)
*Angtian Wang,Haibin Huang,Jacob Zhiyuan Fang,Yiding Yang,Chongyang Ma*

Key words: 视频生成, 运动控制, 轨迹输入, 运动注入器, 统一框架

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一个统一的视频生成运动控制框架，通过轨迹输入整合相机运动、物体平移和局部运动，采用轻量级运动注入器实现用户自定义运动路径控制。

Motivation: 解决现有方法中运动控制模块分离或任务特定设计的问题，提供一种统一的、用户友好的解决方案。

Method: 通过轻量级运动注入器将用户定义的轨迹映射到预训练图像到视频生成模型的潜在空间，实现多种运动类型的控制。

Result: 在多任务运动控制中表现出优越性能，包括风格化运动效果、动态视角变化和精确局部运动操控，视觉质量和可控性优于现有方法和商业方案。

Conclusion: 该框架为视频生成中的运动控制提供了高效的统一解决方案，兼容多种先进视频生成模型。

Abstract: We propose a unified framework for motion control in video generation that
seamlessly integrates camera movement, object-level translation, and
fine-grained local motion using trajectory-based inputs. In contrast to prior
methods that address these motion types through separate modules or
task-specific designs, our approach offers a cohesive solution by projecting
user-defined trajectories into the latent space of pre-trained image-to-video
generation models via a lightweight motion injector. Users can specify
keypoints and their motion paths to control localized deformations, entire
object motion, virtual camera dynamics, or combinations of these. The injected
trajectory signals guide the generative process to produce temporally
consistent and semantically aligned motion sequences. Our framework
demonstrates superior performance across multiple video motion control tasks,
including stylized motion effects (e.g., motion brushes), dynamic viewpoint
changes, and precise local motion manipulation. Experiments show that our
method provides significantly better controllability and visual quality
compared to prior approaches and commercial solutions, while remaining broadly
compatible with various state-of-the-art video generation backbones. Project
page: https://anytraj.github.io/.

</details>


### [395] [Toward Memory-Aided World Models: Benchmarking via Spatial Consistency](https://arxiv.org/abs/2505.22976)
*Kewei Lian,Shaofei Cai,Yilun Du,Yitao Liang*

Key words: 世界模型, 空间一致性, 数据集, Minecraft, 记忆模块

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文构建了一种强调空间一致性的数据集和基准测试，用于提升世界模型的记忆模块能力，并在Minecraft中收集了大量导航视频数据。

Motivation: 现有数据集缺乏对空间一致性的关注，而这是世界模型的关键需求。为了解决这一问题，作者提出了一种新的数据集和基准测试方法。

Method: 在Minecraft中采样150个不同地点，收集20百万帧循环导航视频数据，采用课程设计逐步增加序列长度，并评估四种代表性世界模型基线。

Result: 提出的数据集和基准测试支持模型学习空间一致性，数据收集流程易于扩展，代码和数据已开源。

Conclusion: 该研究填补了空间一致性数据集的空白，为世界模型的记忆模块开发提供了重要支持。

Abstract: The ability to simulate the world in a spatially consistent manner is a
crucial requirements for effective world models. Such a model enables
high-quality visual generation, and also ensures the reliability of world
models for downstream tasks such as simulation and planning. Designing a memory
module is a crucial component for addressing spatial consistency: such a model
must not only retain long-horizon observational information, but also enables
the construction of explicit or implicit internal spatial representations.
However, there are no dataset designed to promote the development of memory
modules by explicitly enforcing spatial consistency constraints. Furthermore,
most existing benchmarks primarily emphasize visual coherence or generation
quality, neglecting the requirement of long-range spatial consistency. To
bridge this gap, we construct a dataset and corresponding benchmark by sampling
150 distinct locations within the open-world environment of Minecraft,
collecting about 250 hours (20 million frames) of loop-based navigation videos
with actions. Our dataset follows a curriculum design of sequence lengths,
allowing models to learn spatial consistency on increasingly complex navigation
trajectories. Furthermore, our data collection pipeline is easily extensible to
new Minecraft environments and modules. Four representative world model
baselines are evaluated on our benchmark. Dataset, benchmark, and code are
open-sourced to support future research.

</details>


### [396] [Synthetic Document Question Answering in Hungarian](https://arxiv.org/abs/2505.23008)
*Jonathan Li,Zoltan Csaki,Nidhi Hiremath,Etash Guha,Fenglu Hong,Edward Ma,Urmish Thakker*

Key words: 文档视觉问答（DocVQA），匈牙利语，低资源语言，数据集，微调

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了针对匈牙利语的低资源文档视觉问答（VQA）数据集HuDocVQA和HuDocVQA-manual，并通过质量筛选和去重提升数据质量。实验表明，基于这些数据集微调的模型在匈牙利语DocVQA任务上性能提升了7.2%。

Motivation: 解决低资源语言（如匈牙利语）在文档视觉问答任务中因缺乏高质量训练和评估数据而表现不佳的问题。

Method: 通过从Common Crawl中提取匈牙利语文档，手动和自动生成了HuDocVQA-manual和HuDocVQA数据集，并进行了多轮质量筛选与去重。同时发布了HuCCPDF用于OCR训练。

Result: 微调后的Llama 3.2 11B Instruct模型在HuDocVQA上的准确率提高了7.2%。

Conclusion: 数据集和代码的公开将推动多语言DocVQA研究，填补了低资源语言在此领域的空白。

Abstract: Modern VLMs have achieved near-saturation accuracy in English document visual
question-answering (VQA). However, this task remains challenging in lower
resource languages due to a dearth of suitable training and evaluation data. In
this paper we present scalable methods for curating such datasets by focusing
on Hungarian, approximately the 17th highest resource language on the internet.
Specifically, we present HuDocVQA and HuDocVQA-manual, document VQA datasets
that modern VLMs significantly underperform on compared to English DocVQA.
HuDocVQA-manual is a small manually curated dataset based on Hungarian
documents from Common Crawl, while HuDocVQA is a larger synthetically generated
VQA data set from the same source. We apply multiple rounds of quality
filtering and deduplication to HuDocVQA in order to match human-level quality
in this dataset. We also present HuCCPDF, a dataset of 117k pages from
Hungarian Common Crawl PDFs along with their transcriptions, which can be used
for training a model for Hungarian OCR. To validate the quality of our
datasets, we show how finetuning on a mixture of these datasets can improve
accuracy on HuDocVQA for Llama 3.2 11B Instruct by +7.2%. Our datasets and code
will be released to the public to foster further research in multilingual
DocVQA.

</details>


### [397] [Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation](https://arxiv.org/abs/2505.23043)
*Jihai Zhang,Tianle Li,Linjie Li,Zhengyuan Yang,Yu Cheng*

Key words: 统一视觉-语言模型, 跨任务泛化, 理解与生成, 多模态对齐, 知识迁移

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文探讨了统一视觉-语言模型（VLM）中理解和生成任务的相互增强关系，通过实验验证了混合训练对跨任务泛化的积极影响，并提出了设计优化的关键见解。

Motivation: 研究统一VLM中理解和生成任务的相互促进机制，填补现有研究中这一假设未充分探索的空白。

Method: 设计贴近现实场景的数据集，评估多种统一VLM架构的跨任务泛化能力。

Result: 混合训练能双向提升任务表现，泛化效果随数据量增加；多模态输入输出空间对齐和生成任务知识迁移是关键发现。

Conclusion: 统一理解和生成对VLM优化至关重要，研究结果为未来模型设计提供了理论基础。

Abstract: Recent advancements in unified vision-language models (VLMs), which integrate
both visual understanding and generation capabilities, have attracted
significant attention. The underlying hypothesis is that a unified architecture
with mixed training on both understanding and generation tasks can enable
mutual enhancement between understanding and generation. However, this
hypothesis remains underexplored in prior works on unified VLMs. To address
this gap, this paper systematically investigates the generalization across
understanding and generation tasks in unified VLMs. Specifically, we design a
dataset closely aligned with real-world scenarios to facilitate extensive
experiments and quantitative evaluations. We evaluate multiple unified VLM
architectures to validate our findings. Our key findings are as follows. First,
unified VLMs trained with mixed data exhibit mutual benefits in understanding
and generation tasks across various architectures, and this mutual benefits can
scale up with increased data. Second, better alignment between multimodal input
and output spaces will lead to better generalization. Third, the knowledge
acquired during generation tasks can transfer to understanding tasks, and this
cross-task generalization occurs within the base language model, beyond
modality adapters. Our findings underscore the critical necessity of unifying
understanding and generation in VLMs, offering valuable insights for the design
and optimization of unified VLMs.

</details>


### [398] [Multi-Sourced Compositional Generalization in Visual Question Answering](https://arxiv.org/abs/2505.23045)
*Chuanhao Li,Wenbo Ye,Zhen Li,Yuwei Wu,Yunde Jia*

Key words: 组合泛化, 视觉问答, 多模态, 检索增强, GQA-MSCG

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文探讨了视觉和语言任务中的多源组合泛化（MSCG）问题，提出了一个检索增强的训练框架以提升视觉问答模型的MSCG能力，并通过构建GQA-MSCG数据集验证了方法的有效性。

Motivation: 多模态任务中的多源组合泛化（MSCG）能力未被充分研究，论文旨在填补这一空白。

Method: 采用检索增强的训练框架，通过检索语义等效的原始特征并聚合，学习跨模态的统一表示。

Result: 实验验证了方法的有效性，并构建了GQA-MSCG数据集进行评估。

Conclusion: 提出的框架能有效提升VQA模型的多源组合泛化能力。

Abstract: Compositional generalization is the ability of generalizing novel
compositions from seen primitives, and has received much attention in
vision-and-language (V\&L) recently. Due to the multi-modal nature of V\&L
tasks, the primitives composing compositions source from different modalities,
resulting in multi-sourced novel compositions. However, the generalization
ability over multi-sourced novel compositions, \textit{i.e.}, multi-sourced
compositional generalization (MSCG) remains unexplored. In this paper, we
explore MSCG in the context of visual question answering (VQA), and propose a
retrieval-augmented training framework to enhance the MSCG ability of VQA
models by learning unified representations for primitives from different
modalities. Specifically, semantically equivalent primitives are retrieved for
each primitive in the training samples, and the retrieved features are
aggregated with the original primitive to refine the model. This process helps
the model learn consistent representations for the same semantic primitives
across different modalities. To evaluate the MSCG ability of VQA models, we
construct a new GQA-MSCG dataset based on the GQA dataset, in which samples
include three types of novel compositions composed of primitives from different
modalities. Experimental results demonstrate the effectiveness of the proposed
framework. We release GQA-MSCG at https://github.com/NeverMoreLCH/MSCG.

</details>


### [399] [GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion](https://arxiv.org/abs/2505.23085)
*Gwanghyun Kim,Xueting Li,Ye Yuan,Koki Nagano,Tianye Li,Jan Kautz,Se Young Chun,Umar Iqbal*

Key words: 3D几何估计、时间一致性、单目视频、根相对深度

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: GeoMan是一种新架构，用于从单目视频中生成准确且时间一致的人体几何数据，解决了现有方法在时间一致性和精细动态细节上的不足。

Motivation: 现有方法主要针对单图像优化，存在时间不一致和动态细节捕捉不足的问题。

Method: 结合图像模型和视频扩散模型，将视频几何估计任务重构为图像到视频生成问题，并引入根相对深度表示。

Result: GeoMan在定性和定量评估中均达到最先进性能。

Conclusion: GeoMan有效克服了3D人体几何估计中的长期挑战。

Abstract: Estimating accurate and temporally consistent 3D human geometry from videos
is a challenging problem in computer vision. Existing methods, primarily
optimized for single images, often suffer from temporal inconsistencies and
fail to capture fine-grained dynamic details. To address these limitations, we
present GeoMan, a novel architecture designed to produce accurate and
temporally consistent depth and normal estimations from monocular human videos.
GeoMan addresses two key challenges: the scarcity of high-quality 4D training
data and the need for metric depth estimation to accurately model human size.
To overcome the first challenge, GeoMan employs an image-based model to
estimate depth and normals for the first frame of a video, which then
conditions a video diffusion model, reframing video geometry estimation task as
an image-to-video generation problem. This design offloads the heavy lifting of
geometric estimation to the image model and simplifies the video model's role
to focus on intricate details while using priors learned from large-scale video
datasets. Consequently, GeoMan improves temporal consistency and
generalizability while requiring minimal 4D training data. To address the
challenge of accurate human size estimation, we introduce a root-relative depth
representation that retains critical human-scale details and is easier to be
estimated from monocular inputs, overcoming the limitations of traditional
affine-invariant and metric depth representations. GeoMan achieves
state-of-the-art performance in both qualitative and quantitative evaluations,
demonstrating its effectiveness in overcoming longstanding challenges in 3D
human geometry estimation from videos.

</details>


### [400] [EAD: An EEG Adapter for Automated Classification](https://arxiv.org/abs/2505.23107)
*Pushapdeep Singh,Jyoti Nigam,Medicherla Vamsi Krishna,Arnav Bhavsar,Aditya Nigam*

Key words: EEG, neural decoding, embedding learning, deep learning, EEG-ImageNet, BrainLat

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种名为EEG Adapter（EAD）的灵活框架，用于学习EEG信号的通用嵌入，适用于不同设备和通道数量的分类任务，并在EEG-ImageNet和BrainLat数据集上实现了最先进的精度。

Motivation: 传统EEG分类方法高度依赖于任务特定的数据采集和通道数量，限制了统一管道的开发。因此需要一种适用于不同设备且能处理不同通道数量的EEG嵌入学习框架。

Method: 利用EEG基础模型并进行显著调整，提出了EAD框架，用于学习鲁棒的EEG表示以进行分类任务。

Result: 在EEG-ImageNet和BrainLat数据集上分别达到了99.33%和92.31%的准确率，并在零样本分类任务中展示了泛化能力。

Conclusion: EAD框架在多样化的EEG数据集上表现优异，展示了其通用性和适应性。

Abstract: While electroencephalography (EEG) has been a popular modality for neural
decoding, it often involves task specific acquisition of the EEG data. This
poses challenges for the development of a unified pipeline to learn embeddings
for various EEG signal classification, which is often involved in various
decoding tasks. Traditionally, EEG classification involves the step of signal
preprocessing and the use of deep learning techniques, which are highly
dependent on the number of EEG channels in each sample. However, the same
pipeline cannot be applied even if the EEG data is collected for the same
experiment but with different acquisition devices. This necessitates the
development of a framework for learning EEG embeddings, which could be highly
beneficial for tasks involving multiple EEG samples for the same task but with
varying numbers of EEG channels. In this work, we propose EEG Adapter (EAD), a
flexible framework compatible with any signal acquisition device. More
specifically, we leverage a recent EEG foundational model with significant
adaptations to learn robust representations from the EEG data for the
classification task. We evaluate EAD on two publicly available datasets
achieving state-of-the-art accuracies 99.33% and 92.31% on EEG-ImageNet and
BrainLat respectively. This illustrates the effectiveness of the proposed
framework across diverse EEG datasets containing two different perception
tasks: stimulus and resting-state EEG signals. We also perform zero-shot EEG
classification on EEG-ImageNet task to demonstrate the generalization
capability of the proposed approach.

</details>


### [401] [VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models](https://arxiv.org/abs/2505.22654)
*Ce Zhang,Kaixin Ma,Tianqing Fang,Wenhao Yu,Hongming Zhang,Zhisong Zhang,Yaqi Xie,Katia Sycara,Haitao Mi,Dong Yu*

Key words: 视觉语言模型, 令牌缩减, 计算优化, 实时推理, VScan

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: VScan是一个两阶段视觉令牌缩减框架，通过全局和局部扫描与令牌合并加速推理，显著减少计算成本。

Motivation: 现有大型视觉语言模型计算成本高，难以实时部署，需优化视觉令牌处理以提高效率。

Method: 提出VScan框架，结合视觉编码阶段的全局和局部扫描与令牌合并，并在语言模型中间层引入剪枝。

Result: 在四个大型视觉语言模型上验证，VScan显著加速推理（LLaVA-NeXT-7B提速2.91倍），FLOPs减少10倍，性能保留95.4%。

Conclusion: VScan有效平衡效率与性能，优于现有方法，适用于实时部署场景。

Abstract: Recent Large Vision-Language Models (LVLMs) have advanced multi-modal
understanding by incorporating finer-grained visual perception and encoding.
However, such methods incur significant computational costs due to longer
visual token sequences, posing challenges for real-time deployment. To mitigate
this, prior studies have explored pruning unimportant visual tokens either at
the output layer of the visual encoder or at the early layers of the language
model. In this work, we revisit these design choices and reassess their
effectiveness through comprehensive empirical studies of how visual tokens are
processed throughout the visual encoding and language decoding stages. Guided
by these insights, we propose VScan, a two-stage visual token reduction
framework that addresses token redundancy by: (1) integrating complementary
global and local scans with token merging during visual encoding, and (2)
introducing pruning at intermediate layers of the language model. Extensive
experimental results across four LVLMs validate the effectiveness of VScan in
accelerating inference and demonstrate its superior performance over current
state-of-the-arts on sixteen benchmarks. Notably, when applied to
LLaVA-NeXT-7B, VScan achieves a 2.91$\times$ speedup in prefilling and a
10$\times$ reduction in FLOPs, while retaining 95.4% of the original
performance.

</details>


### [402] [Cultural Evaluations of Vision-Language Models Have a Lot to Learn from Cultural Theory](https://arxiv.org/abs/2505.22793)
*Srishti Yadav,Lauren Tilton,Maria Antoniak,Taylor Arnold,Jiaang Li,Siddhesh Milind Pawar,Antonia Karamolegkou,Stella Frank,Zhaochong An,Negar Rostamzadeh,Daniel Hershcovich,Serge Belongie,Ekaterina Shutova*

Key words: 视觉-语言模型、文化能力、视觉文化研究、文化维度、符号学

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文主张利用视觉文化研究的基础方法来分析视觉-语言模型中的文化维度，提出了五种需要考量的文化框架以增强模型的文化理解能力。

Motivation: 由于现代视觉-语言模型在文化能力评估中表现不佳，作者探讨如何通过系统性框架更好地识别和注释图像中的文化细微差别。

Method: 论文借鉴视觉文化研究（如文化研究、符号学和视觉研究）的方法，提出了五种文化维度的分析框架。

Result: 通过整合这些框架，论文为视觉-语言模型的文化分析提供了更全面的方法论支持。

Conclusion: 视觉文化研究的理论基础对于提升视觉-语言模型的文化能力至关重要，未来研究应进一步验证这些框架的有效性。

Abstract: Modern vision-language models (VLMs) often fail at cultural competency
evaluations and benchmarks. Given the diversity of applications built upon
VLMs, there is renewed interest in understanding how they encode cultural
nuances. While individual aspects of this problem have been studied, we still
lack a comprehensive framework for systematically identifying and annotating
the nuanced cultural dimensions present in images for VLMs. This position paper
argues that foundational methodologies from visual culture studies (cultural
studies, semiotics, and visual studies) are necessary for cultural analysis of
images. Building upon this review, we propose a set of five frameworks,
corresponding to cultural dimensions, that must be considered for a more
complete analysis of the cultural competencies of VLMs.

</details>


### [403] [Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing](https://arxiv.org/abs/2505.23134)
*Tongtong Su,Chengyu Wang,Jun Huang,Dongming Lu*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种基于参考的视频编辑方法Zero-to-Hero，通过将编辑过程分解为锚帧编辑和外观传播两步骤，提升了细粒度控制和时序一致性。

Motivation: 现有文本引导方法在用户意图理解上存在模糊性，且难以对物体特定属性进行细粒度编辑。为克服这些限制，研究提出参考驱动的编辑方法。

Method: 方法分为两部分：通过锚帧编辑生成参考图像，再利用原始帧的对应关系引导注意力机制传播外观。后续通过Hero-Stage学习条件生成模型修复成像问题。

Result: 在PSNR指标上优于基线方法2.6 dB，且通过Blender构建的数据集验证了外观一致性的提升。

Conclusion: Zero-to-Hero通过分阶段处理和注意力引导，显著提升了视频编辑的准确性与一致性，同时解决了成像退化问题。

Abstract: Appearance editing according to user needs is a pivotal task in video
editing. Existing text-guided methods often lead to ambiguities regarding user
intentions and restrict fine-grained control over editing specific aspects of
objects. To overcome these limitations, this paper introduces a novel approach
named {Zero-to-Hero}, which focuses on reference-based video editing that
disentangles the editing process into two distinct problems. It achieves this
by first editing an anchor frame to satisfy user requirements as a reference
image and then consistently propagating its appearance across other frames. We
leverage correspondence within the original frames to guide the attention
mechanism, which is more robust than previously proposed optical flow or
temporal modules in memory-friendly video generative models, especially when
dealing with objects exhibiting large motions. It offers a solid ZERO-shot
initialization that ensures both accuracy and temporal consistency. However,
intervention in the attention mechanism results in compounded imaging
degradation with over-saturated colors and unknown blurring issues. Starting
from Zero-Stage, our Hero-Stage Holistically learns a conditional generative
model for vidEo RestOration. To accurately evaluate the consistency of the
appearance, we construct a set of videos with multiple appearances using
Blender, enabling a fine-grained and deterministic evaluation. Our method
outperforms the best-performing baseline with a PSNR improvement of 2.6 dB. The
project page is at https://github.com/Tonniia/Zero2Hero.

</details>


### [404] [FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing](https://arxiv.org/abs/2505.23145)
*Jeongsol Kim,Yeobin Hong,Jong Chul Ye*

Key words: 基于流的图像编辑, FlowAlign, 流匹配损失, 逆向编辑, 稳定轨迹

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: FlowAlign是一种无需逆向操作、基于流的图像编辑框架，通过流匹配损失提升编辑稳定性和一致性，并支持逆向编辑。

Motivation: 解决现有基于流的图像编辑方法（如FlowEdit）因缺乏逆向操作导致的轨迹不稳定和源一致性差的问题。

Method: 提出FlowAlign框架，利用流匹配损失作为正则化机制，优化编辑轨迹并平衡语义对齐与结构一致性。

Result: 实验表明FlowAlign在源图像保留和编辑可控性上优于现有方法。

Conclusion: FlowAlign通过流匹配损失实现了稳定、可逆的图像编辑，具备更好的操作性和一致性。

Abstract: Recent inversion-free, flow-based image editing methods such as FlowEdit
leverages a pre-trained noise-to-image flow model such as Stable Diffusion 3,
enabling text-driven manipulation by solving an ordinary differential equation
(ODE). While the lack of exact latent inversion is a core advantage of these
methods, it often results in unstable editing trajectories and poor source
consistency. To address this limitation, we propose FlowAlign, a novel
inversion-free flow-based framework for consistent image editing with
principled trajectory control. FlowAlign introduces a flow-matching loss as a
regularization mechanism to promote smoother and more stable trajectories
during the editing process. Notably, the flow-matching loss is shown to
explicitly balance semantic alignment with the edit prompt and structural
consistency with the source image along the trajectory. Furthermore, FlowAlign
naturally supports reverse editing by simply reversing the ODE trajectory,
highlighting the reversible and consistent nature of the transformation.
Extensive experiments demonstrate that FlowAlign outperforms existing methods
in both source preservation and editing controllability.

</details>


### [405] [Implicit Inversion turns CLIP into a Decoder](https://arxiv.org/abs/2505.23161)
*Antonio D'Orazio,Maria Rosaria Briglia,Donato Crisostomi,Dario Loi,Emanuele Rodolà,Iacopo Masi*

Key words: CLIP, 图像合成, 隐式神经表示, 文本生成图像, 风格迁移

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: CLIP单独用于图像合成，无需解码器或训练，利用频率感知隐式神经表示和多种稳定技术实现文本生成图像等功能。

Motivation: 探索CLIP作为判别模型的潜在生成能力，揭示其无需额外训练即可实现图像合成的可能性。

Method: 采用频率感知隐式神经表示、对抗鲁棒初始化、正交Procrustes投影和混合损失等技术，优化图像合成过程。

Result: 成功实现文本生成图像、风格迁移和图像重建，无需修改CLIP权重。

Conclusion: 判别模型可能隐藏未被发掘的生成潜力。

Abstract: CLIP is a discriminative model trained to align images and text in a shared
embedding space. Due to its multimodal structure, it serves as the backbone of
many generative pipelines, where a decoder is trained to map from the shared
space back to images. In this work, we show that image synthesis is
nevertheless possible using CLIP alone -- without any decoder, training, or
fine-tuning. Our approach optimizes a frequency-aware implicit neural
representation that encourages coarse-to-fine generation by stratifying
frequencies across network layers. To stabilize this inverse mapping, we
introduce adversarially robust initialization, a lightweight Orthogonal
Procrustes projection to align local text and image embeddings, and a blending
loss that anchors outputs to natural image statistics. Without altering CLIP's
weights, this framework unlocks capabilities such as text-to-image generation,
style transfer, and image reconstruction. These findings suggest that
discriminative models may hold untapped generative potential, hidden in plain
sight.

</details>


### [406] [Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks](https://arxiv.org/abs/2505.23192)
*Run Hao,Peng Ying*

Key words: 文本到图像模型, 对抗提示生成, AIGC检测器, 蒙特卡洛树搜索, 语法树

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种自动化对抗提示生成框架，利用语法树和蒙特卡洛树搜索算法，有效绕过AI生成内容检测器

Motivation: 解决文本到图像模型带来的身份滥用问题，并测试AIGC检测器的鲁棒性

Method: 基于语法树结构和蒙特卡洛树搜索算法，系统探索语义提示空间

Result: 在多个T2I模型上验证有效性，并在实际对抗检测竞赛中排名第一

Conclusion: 该方法不仅能攻击，还能构建高质量对抗数据集，助力更鲁棒的检测与防御系统

Abstract: The rise of text-to-image (T2I) models has enabled the synthesis of
photorealistic human portraits, raising serious concerns about identity misuse
and the robustness of AIGC detectors. In this work, we propose an automated
adversarial prompt generation framework that leverages a grammar tree structure
and a variant of the Monte Carlo tree search algorithm to systematically
explore the semantic prompt space. Our method generates diverse, controllable
prompts that consistently evade both open-source and commercial AIGC detectors.
Extensive experiments across multiple T2I models validate its effectiveness,
and the approach ranked first in a real-world adversarial AIGC detection
competition. Beyond attack scenarios, our method can also be used to construct
high-quality adversarial datasets, providing valuable resources for training
and evaluating more robust AIGC detection and defense systems.

</details>


### [407] [SAMamba: Adaptive State Space Modeling with Hierarchical Vision for Infrared Small Target Detection](https://arxiv.org/abs/2505.23214)
*Wenhao Xu,Shuchen Zheng,Changwei Wang,Zherui Zhang,Chuan Ren,Rongtao Xu,Shibiao Xu*

Key words: 红外小目标检测、SAMamba、FS-Adapter、CSI模块、DPCF模块

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: SAMamba框架结合SAM2和Mamba的优势，通过FS-Adapter、CSI和DPCF模块解决了红外小目标检测中的信息丢失和全局上下文建模问题，实验表明其在挑战性场景中表现优异。

Motivation: 红外小目标检测在军事和预警中至关重要，但目标极小且背景复杂，现有方法常因下采样信息丢失和全局建模不足效果不佳，需新方法解决这些挑战。

Method: 引入SAMamba框架，融合SAM2的层次特征学习和Mamba的选择性序列建模，关键技术包括FS-Adapter（双阶段选择）、CSI模块（线性复杂度全局建模）和DPCF模块（多尺度特征融合）。

Result: 在NUAA-SIRST等数据集上的实验显示，SAMamba显著优于现有方法，尤其在异质背景和多尺度目标场景中表现突出。

Conclusion: SAMamba通过领域适应、细粒度保留和长程依赖建模有效解决ISTD挑战，为实际应用提供了可靠方案。

Abstract: Infrared small target detection (ISTD) is vital for long-range surveillance
in military, maritime, and early warning applications. ISTD is challenged by
targets occupying less than 0.15% of the image and low distinguishability from
complex backgrounds. Existing deep learning methods often suffer from
information loss during downsampling and inefficient global context modeling.
This paper presents SAMamba, a novel framework integrating SAM2's hierarchical
feature learning with Mamba's selective sequence modeling. Key innovations
include: (1) A Feature Selection Adapter (FS-Adapter) for efficient
natural-to-infrared domain adaptation via dual-stage selection (token-level
with a learnable task embedding and channel-wise adaptive transformations); (2)
A Cross-Channel State-Space Interaction (CSI) module for efficient global
context modeling with linear complexity using selective state space modeling;
and (3) A Detail-Preserving Contextual Fusion (DPCF) module that adaptively
combines multi-scale features with a gating mechanism to balance
high-resolution and low-resolution feature contributions. SAMamba addresses
core ISTD challenges by bridging the domain gap, maintaining fine-grained
details, and efficiently modeling long-range dependencies. Experiments on
NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets show SAMamba significantly
outperforms state-of-the-art methods, especially in challenging scenarios with
heterogeneous backgrounds and varying target scales. Code:
https://github.com/zhengshuchen/SAMamba.

</details>


### [408] [R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation](https://arxiv.org/abs/2505.23493)
*Kaijie Chen,Zihao Lin,Zhiyang Xu,Ying Shen,Yuguang Yao,Joy Rimchala,Jiaxin Zhang,Lifu Huang*

Key words: 文本到图像生成，推理能力评估，基准测试，R2I-Bench，R2IScore

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文介绍了R2I-Bench，一个专门用于评估文本到图像（T2I）生成中推理能力的基准测试，并提出了R2IScore评估指标，结果显示现有T2I模型的推理能力仍有限。

Motivation: 尽管T2I模型在生成逼真图像方面取得进展，但其推理能力仍不足且缺乏系统评估，因此需设计专门基准以推动推理能力提升。

Method: 构建R2I-Bench基准，覆盖多类推理任务；设计R2IScore评估指标，通过问答形式从文本-图像对齐、推理准确性和图像质量三个维度评估。

Result: 对16种主流T2I模型的测试表明，其推理能力普遍不足，包括基于先进语言和图像生成模型的解耦框架也表现受限。

Conclusion: 当前T2I系统的推理能力仍需改进，未来需开发更鲁棒且具备推理意识的架构。

Abstract: Reasoning is a fundamental capability often required in real-world
text-to-image (T2I) generation, e.g., generating ``a bitten apple that has been
left in the air for more than a week`` necessitates understanding temporal
decay and commonsense concepts. While recent T2I models have made impressive
progress in producing photorealistic images, their reasoning capability remains
underdeveloped and insufficiently evaluated. To bridge this gap, we introduce
R2I-Bench, a comprehensive benchmark specifically designed to rigorously assess
reasoning-driven T2I generation. R2I-Bench comprises meticulously curated data
instances, spanning core reasoning categories, including commonsense,
mathematical, logical, compositional, numerical, causal, and concept mixing. To
facilitate fine-grained evaluation, we design R2IScore, a QA-style metric based
on instance-specific, reasoning-oriented evaluation questions that assess three
critical dimensions: text-image alignment, reasoning accuracy, and image
quality. Extensive experiments with 16 representative T2I models, including a
strong pipeline-based framework that decouples reasoning and generation using
the state-of-the-art language and image generation models, demonstrate
consistently limited reasoning performance, highlighting the need for more
robust, reasoning-aware architectures in the next generation of T2I systems.
Project Page: https://r2i-bench.github.io

</details>


### [409] [Unsupervised Transcript-assisted Video Summarization and Highlight Detection](https://arxiv.org/abs/2505.23268)
*Spyros Barbakos,Charalampos Antoniadis,Gerasimos Potamianos,Gianluca Setti*

Key words: 视频摘要、亮点检测、多模态融合、强化学习、无监督学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种结合视频帧和转录文本的多模态方法，用于视频摘要和亮点检测，通过强化学习框架训练，优于仅依赖视觉内容的方法。

Motivation: 视频观看是日常生活的重要组成部分，但完整观看视频可能耗时。现有方法多依赖单一模态（如视觉或文本），且缺乏在多模态框架下的强化学习应用。

Method: 提出多模态管道，融合视频帧和转录文本，利用强化学习框架训练模型，奖励生成多样化和代表性摘要，并包含有意义的文本内容片段。

Result: 实验表明，结合转录文本的多模态方法在视频摘要和亮点检测上优于仅依赖视觉内容的方法。

Conclusion: 多模态融合和强化学习框架有效提升了视频摘要和亮点检测的准确性和实用性，尤其适用于大规模无标注数据集。

Abstract: Video consumption is a key part of daily life, but watching entire videos can
be tedious. To address this, researchers have explored video summarization and
highlight detection to identify key video segments. While some works combine
video frames and transcripts, and others tackle video summarization and
highlight detection using Reinforcement Learning (RL), no existing work, to the
best of our knowledge, integrates both modalities within an RL framework. In
this paper, we propose a multimodal pipeline that leverages video frames and
their corresponding transcripts to generate a more condensed version of the
video and detect highlights using a modality fusion mechanism. The pipeline is
trained within an RL framework, which rewards the model for generating diverse
and representative summaries while ensuring the inclusion of video segments
with meaningful transcript content. The unsupervised nature of the training
allows for learning from large-scale unannotated datasets, overcoming the
challenge posed by the limited size of existing annotated datasets. Our
experiments show that using the transcript in video summarization and highlight
detection achieves superior results compared to relying solely on the visual
content of the video.

</details>


### [410] [Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles](https://arxiv.org/abs/2505.23590)
*Zifu Wang,Junyi Zhu,Bo Tang,Zhiyu Li,Feiyu Xiong,Jiaqian Yu,Matthew B. Blaschko*

Key words: rule-based reinforcement learning, multimodal large language models, jigsaw puzzles, generalization, reasoning patterns

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文研究了基于规则的强化学习在多模态大语言模型中的应用，发现通过拼图任务微调，模型性能显著提升并能泛化至复杂任务。RL比SFT表现更优，且推理能力多为预存而非突发。

Motivation: 探索多模态大语言模型在视觉任务中基于规则的强化学习效果，尤其是拼图任务的泛化能力和推理行为。

Method: 使用拼图任务作为结构化实验框架，对比强化学习（RL）与监督微调（SFT）的效果，分析模型推理模式。

Result: 微调后模型在拼图任务上接近完美准确率，并能泛化至其他视觉任务；RL泛化能力优于SFT，推理模式多为预存。

Conclusion: 拼图任务为多模态强化学习提供了有效验证框架，RL在泛化上表现更优，但结果可能因任务而异。

Abstract: The application of rule-based reinforcement learning (RL) to multimodal large
language models (MLLMs) introduces unique challenges and potential deviations
from findings in text-only domains, particularly for perception-heavy tasks.
This paper provides a comprehensive study of rule-based visual RL using jigsaw
puzzles as a structured experimental framework, revealing several key findings.
\textit{Firstly,} we find that MLLMs, initially performing near to random
guessing on simple puzzles, achieve near-perfect accuracy and generalize to
complex, unseen configurations through fine-tuning. \textit{Secondly,} training
on jigsaw puzzles can induce generalization to other visual tasks, with
effectiveness tied to specific task configurations. \textit{Thirdly,} MLLMs can
learn and generalize with or without explicit reasoning, though open-source
models often favor direct answering. Consequently, even when trained for
step-by-step reasoning, they can ignore the thinking process in deriving the
final answer. \textit{Fourthly,} we observe that complex reasoning patterns
appear to be pre-existing rather than emergent, with their frequency increasing
alongside training and task difficulty. \textit{Finally,} our results
demonstrate that RL exhibits more effective generalization than Supervised
Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL
optimization. Although these observations are based on jigsaw puzzles and may
vary across other visual tasks, this research contributes a valuable piece of
jigsaw to the larger puzzle of collective understanding rule-based visual RL
and its potential in multimodal learning. The code is available at:
\href{https://github.com/zifuwanggg/Jigsaw-R1}{https://github.com/zifuwanggg/Jigsaw-R1}.

</details>


### [411] [Federated Unsupervised Semantic Segmentation](https://arxiv.org/abs/2505.23292)
*Evangelos Charalampakis,Vasileios Mygdalis,Ioannis Pitas*

Key words: 联邦学习, 无监督语义分割, 特征对齐, 分布式训练

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了首个完全分布式、无监督语义图像分割框架FUSS，解决了联邦学习中特征和聚类中心对齐的难题，并在多个数据集上验证了其优越性。

Motivation: 探索联邦学习在无监督语义图像分割中的应用，解决分布式环境下特征表示和聚类中心对齐的挑战。

Method: 提出FUSS框架，通过新型联邦策略优化局部分割头和共享语义中心，实现全局一致性。

Result: 实验表明FUSS在多种数据分布下优于本地训练和传统联邦学习算法。

Conclusion: FUSS是首个实现无监督、分布式语义分割的框架，具有广泛应用潜力。

Abstract: This work explores the application of Federated Learning (FL) in Unsupervised
Semantic image Segmentation (USS). Recent USS methods extract pixel-level
features using frozen visual foundation models and refine them through
self-supervised objectives that encourage semantic grouping. These features are
then grouped to semantic clusters to produce segmentation masks. Extending
these ideas to federated settings requires feature representation and cluster
centroid alignment across distributed clients -- an inherently difficult task
under heterogeneous data distributions in the absence of supervision. To
address this, we propose FUSS Federated Unsupervised image Semantic
Segmentation) which is, to our knowledge, the first framework to enable fully
decentralized, label-free semantic segmentation training. FUSS introduces novel
federation strategies that promote global consistency in feature and prototype
space, jointly optimizing local segmentation heads and shared semantic
centroids. Experiments on both benchmark and real-world datasets, including
binary and multi-class segmentation tasks, show that FUSS consistently
outperforms local-only client trainings as well as extensions of classical FL
algorithms under varying client data distributions. To support reproducibility,
full code will be released upon manuscript acceptance.

</details>


### [412] [VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos](https://arxiv.org/abs/2505.23693)
*Tingyu Song,Tongyan Hu,Guo Gan,Yilun Zhao*

Key words: AIGC视频、多模态大语言模型（MLLMs）、VF-Eval、视频生成评估、人类反馈

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出VF-Eval基准，通过四项任务评估MLLMs在AIGC视频上的能力，发现现有模型表现不佳，并实验证明基于人类反馈的模型优化可能提升视频生成质量。

Motivation: 现有评估多关注自然视频，忽视AIGC视频，而MLLMs在AIGC视频解析上的能力尚未充分探索，需要系统评估以填补研究空白。

Method: 提出VF-Eval基准，包含连贯性验证、错误感知、错误类型检测和推理评估四项任务，测试13种前沿MLLMs，并通过实验RePrompt探索人类反馈对齐对视频生成的改进效果。

Result: 即使在最佳模型GPT-4.1上，四项任务表现仍不稳定，表明基准挑战性强；RePrompt验证了人类反馈对齐可提升视频生成质量。

Conclusion: VF-Eval揭示了MLLMs在AIGC视频解析上的不足，为未来研究提供方向，同时证明人类反馈对齐的实用潜力。

Abstract: MLLMs have been widely studied for video question answering recently.
However, most existing assessments focus on natural videos, overlooking
synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in
video generation rely on MLLMs to evaluate the quality of generated videos, but
the capabilities of MLLMs on interpreting AIGC videos remain largely
underexplored. To address this, we propose a new benchmark, VF-Eval, which
introduces four tasks-coherence validation, error awareness, error type
detection, and reasoning evaluation-to comprehensively evaluate the abilities
of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that
even the best-performing model, GPT-4.1, struggles to achieve consistently good
performance across all tasks. This highlights the challenging nature of our
benchmark. Additionally, to investigate the practical applications of VF-Eval
in improving video generation, we conduct an experiment, RePrompt,
demonstrating that aligning MLLMs more closely with human feedback can benefit
video generation.

</details>


### [413] [Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition](https://arxiv.org/abs/2505.23313)
*Weizhe Kong,Xiao Wang,Ruichong Gao,Chenglong Li,Yu Zhang,Xing Yang,Yaowei Wang,Jin Tang*

Key words: 行人属性识别, 对抗攻击, CLIP, 多模态Transformer, 语义防御

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了首个针对行人属性识别（PAR）的对抗攻击与防御框架ASL-PAR，结合全局和局部攻击策略，并设计语义偏移防御机制。在多个数据集上验证了有效性。

Motivation: 探讨PAR任务在对抗干扰下的潜在脆弱性，填补该领域在对抗攻防研究的空白。

Method: 基于CLIP的PAR框架，采用多模态Transformer融合视觉与文本特征；提出对抗语义和标签扰动攻击（ASL-PAR），并设计语义偏移防御策略。

Result: 在数字域（PETA等）和物理域实验均证明攻击与防御策略的有效性。

Conclusion: ASL-PAR显著提升PAR系统的抗干扰能力，为安全关键应用提供新思路。

Abstract: Pedestrian Attribute Recognition (PAR) is an indispensable task in
human-centered research and has made great progress in recent years with the
development of deep neural networks. However, the potential vulnerability and
anti-interference ability have still not been fully explored. To bridge this
gap, this paper proposes the first adversarial attack and defense framework for
pedestrian attribute recognition. Specifically, we exploit both global- and
patch-level attacks on the pedestrian images, based on the pre-trained
CLIP-based PAR framework. It first divides the input pedestrian image into
non-overlapping patches and embeds them into feature embeddings using a
projection layer. Meanwhile, the attribute set is expanded into sentences using
prompts and embedded into attribute features using a pre-trained CLIP text
encoder. A multi-modal Transformer is adopted to fuse the obtained vision and
text tokens, and a feed-forward network is utilized for attribute recognition.
Based on the aforementioned PAR framework, we adopt the adversarial semantic
and label-perturbation to generate the adversarial noise, termed ASL-PAR. We
also design a semantic offset defense strategy to suppress the influence of
adversarial attacks. Extensive experiments conducted on both digital domains
(i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the
effectiveness of our proposed adversarial attack and defense strategies for the
pedestrian attribute recognition. The source code of this paper will be
released on https://github.com/Event-AHU/OpenPAR.

</details>


### [414] [MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence](https://arxiv.org/abs/2505.23764)
*Sihan Yang,Runsen Xu,Yiman Xie,Sizhe Yang,Mo Li,Jingli Lin,Chenming Zhu,Xiaochen Chen,Haodong Duan,Xiangyu Yue,Dahua Lin,Tai Wang,Jiangmiao Pang*

Key words: 多模态大语言模型、空间推理、基准测试、错误分析

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该研究提出了MMSI-Bench，一个专注于多图像空间推理能力的VQA基准测试，揭示了当前多模态大语言模型（MLLMs）在该任务上的显著性能差距。

Motivation: 现有的基准测试仅针对单图像关系，无法评估真实世界部署所需的多图像空间推理能力。

Method: 研究团队精心设计了1,000个具有挑战性的多选问题，来自超过120,000张图像，并评估了34个开源和专有MLLMs。

Result: 最强的开源模型准确率约30%，OpenAI的o3推理模型达到40%，而人类表现高达97%，显示出模型的巨大差距。

Conclusion: MMSI-Bench展示了多图像空间推理任务的挑战性，并为未来研究提供了自动化错误分析工具。

Abstract: Spatial intelligence is essential for multimodal large language models
(MLLMs) operating in the complex physical world. Existing benchmarks, however,
probe only single-image relations and thus fail to assess the multi-image
spatial reasoning that real-world deployments demand. We introduce MMSI-Bench,
a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision
researchers spent more than 300 hours meticulously crafting 1,000 challenging,
unambiguous multiple-choice questions from over 120,000 images, each paired
with carefully designed distractors and a step-by-step reasoning process. We
conduct extensive experiments and thoroughly evaluate 34 open-source and
proprietary MLLMs, observing a wide gap: the strongest open-source model
attains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while
humans score 97%. These results underscore the challenging nature of MMSI-Bench
and the substantial headroom for future research. Leveraging the annotated
reasoning processes, we also provide an automated error analysis pipeline that
diagnoses four dominant failure modes, including (1) grounding errors, (2)
overlap-matching and scene-reconstruction errors, (3) situation-transformation
reasoning errors, and (4) spatial-logic errors, offering valuable insights for
advancing multi-image spatial intelligence. Project page:
https://runsenxu.com/projects/MMSI_Bench .

</details>


### [415] [Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization](https://arxiv.org/abs/2505.23331)
*Matteo Gallici,Haitz Sáez de Ocáriz Borde*

Key words: 强化学习, 视觉自回归模型, GRPO, CLIP, 图像生成

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 使用强化学习（RL）微调预训练生成模型能更好对齐人类偏好，本文探讨了基于GRPO方法微调视觉自回归模型（VAR），显著提升图像质量并实现风格控制，还能泛化到预训练未覆盖的风格。

Motivation: 探索如何通过RL微调视觉自回归模型，使其输出更贴合复杂的人类偏好（如审美和CLIP嵌入），并解决扩散模型在线采样效率低的问题。

Method: 采用Group Relative Policy Optimization（GRPO）微调VAR模型，结合美学预测器和CLIP嵌入作为奖励信号。

Result: 实验表明，该方法提升图像生成质量，实现风格精确控制，并让模型泛化到预训练未见的风格（如通过CLIP引导生成新风格图像）。

Conclusion: RL微调对VAR模型高效且有效，其快速推理速度尤其适合在线采样场景，优于扩散模型。

Abstract: Fine-tuning pre-trained generative models with Reinforcement Learning (RL)
has emerged as an effective approach for aligning outputs more closely with
nuanced human preferences. In this paper, we investigate the application of
Group Relative Policy Optimization (GRPO) to fine-tune next-scale visual
autoregressive (VAR) models. Our empirical results demonstrate that this
approach enables alignment to intricate reward signals derived from aesthetic
predictors and CLIP embeddings, significantly enhancing image quality and
enabling precise control over the generation style. Interestingly, by
leveraging CLIP, our method can help VAR models generalize beyond their initial
ImageNet distribution: through RL-driven exploration, these models can generate
images aligned with prompts referencing image styles that were absent during
pre-training. In summary, we show that RL-based fine-tuning is both efficient
and effective for VAR models, benefiting particularly from their fast inference
speeds, which are advantageous for online sampling, an aspect that poses
significant challenges for diffusion-based alternatives.

</details>


### [416] [PAN-Crafter: Learning Modality-Consistent Alignment for PAN-Sharpening](https://arxiv.org/abs/2505.23367)
*Jeonghyeok Do,Sungpyo Kim,Geunhyuk Youk,Jaehyup Lee,Munchurl Kim*

Key words: PAN-sharpening, 跨模态对齐, 模态自适应重建, CM3A, 高分辨率多光谱

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: PAN-Crafter提出了一种模态一致的对齐框架，通过模态自适应重建（MARs）和跨模态对齐感知注意力（CM3A）解决PAN与MS图像间的对齐问题，显著提升了HRMS生成质量，并在速度和内存效率上优于现有方法。

Motivation: 解决PAN与MS图像因传感器布局、采集时间和分辨率差异导致的跨模态失准问题，传统深度学习方法依赖于完美像素对齐，导致光谱失真和边缘模糊。

Method: 提出PAN-Crafter框架，包含MARs（联合重建HRMS和PAN图像）和CM3A（双向对齐MS纹理与PAN结构）。

Result: 在多个基准数据集上优于现有方法，推理速度快50.11倍，内存占用减少0.63倍，并在未见卫星数据上表现鲁棒。

Conclusion: PAN-Crafter通过显式对齐和自适应特征细化，有效解决了跨模态失准问题，具有高效性和强泛化能力。

Abstract: PAN-sharpening aims to fuse high-resolution panchromatic (PAN) images with
low-resolution multi-spectral (MS) images to generate high-resolution
multi-spectral (HRMS) outputs. However, cross-modality misalignment -- caused
by sensor placement, acquisition timing, and resolution disparity -- induces a
fundamental challenge. Conventional deep learning methods assume perfect
pixel-wise alignment and rely on per-pixel reconstruction losses, leading to
spectral distortion, double edges, and blurring when misalignment is present.
To address this, we propose PAN-Crafter, a modality-consistent alignment
framework that explicitly mitigates the misalignment gap between PAN and MS
modalities. At its core, Modality-Adaptive Reconstruction (MARs) enables a
single network to jointly reconstruct HRMS and PAN images, leveraging PAN's
high-frequency details as auxiliary self-supervision. Additionally, we
introduce Cross-Modality Alignment-Aware Attention (CM3A), a novel mechanism
that bidirectionally aligns MS texture to PAN structure and vice versa,
enabling adaptive feature refinement across modalities. Extensive experiments
on multiple benchmark datasets demonstrate that our PAN-Crafter outperforms the
most recent state-of-the-art method in all metrics, even with 50.11$\times$
faster inference time and 0.63$\times$ the memory size. Furthermore, it
demonstrates strong generalization performance on unseen satellite datasets,
showing its robustness across different conditions.

</details>


### [417] [VModA: An Effective Framework for Adaptive NSFW Image Moderation](https://arxiv.org/abs/2505.23386)
*Han Bao,Qinying Wang,Zhi Chen,Qingming Li,Xuhong Zhang,Changjiang Li,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Key words: NSFW检测, 视觉语言模型, 内容审核, 复杂语义, 多平台适配

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种名为VModA的通用框架，用于更有效地检测复杂的NSFW内容，适应不同平台和地区的审核规则，显著提升了检测准确率。

Motivation: 当前NSFW内容检测方法难以应对复杂语义和多样化的平台规则，导致检测偏差和准确性下降，亟需一种更通用的解决方案。

Method: 开发了VModA框架，结合视觉语言模型（VLM）和多样化审核规则，处理复杂语义的NSFW内容。

Result: VModA在多种NSFW类型上实现了54.3%的准确率提升，并验证了其在不同场景和VLM上的适应性。同时修正了公共数据集的标注问题。

Conclusion: VModA通过适应性强、高精度的检测能力，为NSFW内容审核提供了实用且高效的解决方案。

Abstract: Not Safe/Suitable for Work (NSFW) content is rampant on social networks and
poses serious harm to citizens, especially minors. Current detection methods
mainly rely on deep learning-based image recognition and classification.
However, NSFW images are now presented in increasingly sophisticated ways,
often using image details and complex semantics to obscure their true nature or
attract more views. Although still understandable to humans, these images often
evade existing detection methods, posing a significant threat. Further
complicating the issue, varying regulations across platforms and regions create
additional challenges for effective moderation, leading to detection bias and
reduced accuracy. To address this, we propose VModA, a general and effective
framework that adapts to diverse moderation rules and handles complex,
semantically rich NSFW content across categories. Experimental results show
that VModA significantly outperforms existing methods, achieving up to a 54.3%
accuracy improvement across NSFW types, including those with complex semantics.
Further experiments demonstrate that our method exhibits strong adaptability
across categories, scenarios, and base VLMs. We also identified inconsistent
and controversial label samples in public NSFW benchmark datasets, re-annotated
them, and submitted corrections to the original maintainers. Two datasets have
confirmed the updates so far. Additionally, we evaluate VModA in real-world
scenarios to demonstrate its practical effectiveness.

</details>


### [418] [Video Editing for Audio-Visual Dubbing](https://arxiv.org/abs/2505.23406)
*Binyamin Manela,Sharon Gannot,Ethan Fetyaya*

Key words: 视觉配音，内容感知编辑，EdiDub，唇同步，视频修改

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出EdiDub框架，将视觉配音重新定义为内容感知编辑任务，解决了现有方法在保留原始视频上下文和复杂视觉元素方面的不足，显著提升了唇部同步和视觉自然度。

Motivation: 视觉配音在全球内容传播中至关重要，但现有方法因无法无缝融入原始场景或丢弃重要视觉信息而受限。

Method: 提出EdiDub框架，通过内容感知编辑任务和专门的条件方案，保留原始视频上下文并精准修改。

Result: 在多个基准测试中，EdiDub显著提升了身份保留和同步性能，人类评估显示其在同步和视觉自然度上优于领先方法。

Conclusion: 内容感知编辑方法在保持复杂视觉元素和准确唇部同步方面优于传统生成或修复技术。

Abstract: Visual dubbing, the synchronization of facial movements with new speech, is
crucial for making content accessible across different languages, enabling
broader global reach. However, current methods face significant limitations.
Existing approaches often generate talking faces, hindering seamless
integration into original scenes, or employ inpainting techniques that discard
vital visual information like partial occlusions and lighting variations. This
work introduces EdiDub, a novel framework that reformulates visual dubbing as a
content-aware editing task. EdiDub preserves the original video context by
utilizing a specialized conditioning scheme to ensure faithful and accurate
modifications rather than mere copying. On multiple benchmarks, including a
challenging occluded-lip dataset, EdiDub significantly improves identity
preservation and synchronization. Human evaluations further confirm its
superiority, achieving higher synchronization and visual naturalness scores
compared to the leading methods. These results demonstrate that our
content-aware editing approach outperforms traditional generation or
inpainting, particularly in maintaining complex visual elements while ensuring
accurate lip synchronization.

</details>


### [419] [CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis](https://arxiv.org/abs/2505.23444)
*Runmin Jiang,Genpei Zhang,Yuntian Yang,Siqi Wu,Yuheng Zhang,Wanyue Feng,Yizhou Zhao,Xi Xiao,Xiao Wang,Tianyang Wang,Xingjian Li,Min Xu*

Key words: 冷冻电镜, 合成数据, 生物物理建模, 条件扩散模型, 图像生成

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: CryoCCD是一种结合生物物理建模与生成技术的合成框架，用于生成高质量、多元结构的冷冻电镜图像，解决了现有方法在结构多样性和复杂噪声方面的不足。

Motivation: 冷冻电镜（cryo-EM）数据标注稀缺，现有合成方法难以同时捕捉生物标本的结构多样性和复杂噪声，CryoCCD旨在解决这一问题。

Method: CryoCCD通过集成生物物理建模与生成技术，利用条件扩散模型、循环一致性和对比学习生成多尺度、结构准确的冷冻电镜图像。

Result: 实验表明，CryoCCD生成的图像结构准确，在下游任务（如粒子挑选和重构）中表现优于现有基线方法。

Conclusion: CryoCCD通过创新生成技术提高了冷冻电镜图像合成的质量和实用性，推动了相关领域的发展。

Abstract: Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of
macromolecules, but developing robust models for downstream analysis is
hindered by the scarcity of high-quality annotated data. While synthetic data
generation has emerged as a potential solution, existing methods often fail to
capture both the structural diversity of biological specimens and the complex,
spatially varying noise inherent in cryo-EM imaging. To overcome these
limitations, we propose CryoCCD, a synthesis framework that integrates
biophysical modeling with generative techniques. Specifically, CryoCCD produces
multi-scale cryo-EM micrographs that reflect realistic biophysical variability
through compositional heterogeneity, cellular context, and physics-informed
imaging. To generate realistic noise, we employ a conditional diffusion model,
enhanced by cycle consistency to preserve structural fidelity and mask-aware
contrastive learning to capture spatially adaptive noise patterns. Extensive
experiments show that CryoCCD generates structurally accurate micrographs and
enhances performance in downstream tasks, outperforming state-of-the-art
baselines in both particle picking and reconstruction.

</details>


### [420] [DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task Learning in Chest X-ray Classification](https://arxiv.org/abs/2505.23595)
*Youssef Mohamed,Noran Mohamed,Khaled Abouhashad,Feilong Tang,Sara Atito,Shoaib Jameel,Imran Razzak,Ahmed B. Zaky*

Key words: Multi-Task Learning, Medical Imaging, Dynamic Task Weighting, Chest X-ray, Deep Learning

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: DeepChest，一种动态任务加权的框架，用于多标签胸部X光分类，显著提升训练速度和准确性，超越现有MTL方法。

Motivation: 解决多任务学习中任务贡献平衡的挑战，尤其在医学影像领域，以提高效率和性能。

Method: 提出基于任务特定损失趋势分析的性能驱动加权机制，无需梯度访问即可自适应调整任务权重。

Result: 在大型CXR数据集上，整体准确率提升7%，训练速度提高三倍，同时减少个体任务损失。

Conclusion: DeepChest为深度学习在医学诊断中的实际部署提供了高效和鲁棒的解决方案。

Abstract: While Multi-Task Learning (MTL) offers inherent advantages in complex domains
such as medical imaging by enabling shared representation learning, effectively
balancing task contributions remains a significant challenge. This paper
addresses this critical issue by introducing DeepChest, a novel,
computationally efficient and effective dynamic task-weighting framework
specifically designed for multi-label chest X-ray (CXR) classification. Unlike
existing heuristic or gradient-based methods that often incur substantial
overhead, DeepChest leverages a performance-driven weighting mechanism based on
effective analysis of task-specific loss trends. Given a network architecture
(e.g., ResNet18), our model-agnostic approach adaptively adjusts task
importance without requiring gradient access, thereby significantly reducing
memory usage and achieving a threefold increase in training speed. It can be
easily applied to improve various state-of-the-art methods. Extensive
experiments on a large-scale CXR dataset demonstrate that DeepChest not only
outperforms state-of-the-art MTL methods by 7% in overall accuracy but also
yields substantial reductions in individual task losses, indicating improved
generalization and effective mitigation of negative transfer. The efficiency
and performance gains of DeepChest pave the way for more practical and robust
deployment of deep learning in critical medical diagnostic applications. The
code is publicly available at https://github.com/youssefkhalil320/DeepChest-MTL

</details>


### [421] [One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory](https://arxiv.org/abs/2505.23617)
*Chenhao Zheng,Jieyu Zhang,Mohammadreza Salehi,Ziqi Gao,Vishnu Iyengar,Norimasa Kobori,Quan Kong,Ranjay Krishna*

Key words: 视频tokenization, 对象轨迹, TrajViT, 对比学习, 视频理解

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种新的视频tokenization方法Grounded Video Tokenization，通过基于对象轨迹组织token，显著减少了冗余并保持了时间一致性。其提出的TrajViT模型在多项视频理解任务中大幅优于传统方法。

Motivation: 现有的视频tokenization方法（如时空块）会导致过多的token和计算效率低下，尤其是在摄像机移动时表现不佳。作者旨在提出一种更高效且性能更好的方法。

Method: 提出Grounded Video Tokenization，基于全景子对象轨迹组织token，设计了TrajViT模型，通过对比学习训练，提取对象轨迹并转化为语义上有意义的token。

Result: TrajViT在视频-文本检索任务中比ViT3D高出6%的top-5召回率，token数量减少10倍；作为VideoLLM的视频编码器，在6个VideoQA基准测试中平均提升5.2%性能，训练时间快4倍，推理FLOPs减少18倍。

Conclusion: TrajViT是首个在多样化视频分析任务中持续优于ViT3D的高效编码器，成为稳健且可扩展的解决方案。

Abstract: Effective video tokenization is critical for scaling transformer models for
long videos. Current approaches tokenize videos using space-time patches,
leading to excessive tokens and computational inefficiencies. The best token
reduction strategies degrade performance and barely reduce the number of tokens
when the camera moves. We introduce grounded video tokenization, a paradigm
that organizes tokens based on panoptic sub-object trajectories rather than
fixed patches. Our method aligns with fundamental perceptual principles,
ensuring that tokenization reflects scene complexity rather than video
duration. We propose TrajViT, a video encoder that extracts object trajectories
and converts them into semantically meaningful tokens, significantly reducing
redundancy while maintaining temporal coherence. Trained with contrastive
learning, TrajViT significantly outperforms space-time ViT (ViT3D) across
multiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a
large margin of 6% top-5 recall in average at video-text retrieval task with
10x token deduction. We also show TrajViT as a stronger model than ViT3D for
being the video encoder for modern VideoLLM, obtaining an average of 5.2%
performance improvement across 6 VideoQA benchmarks while having 4x faster
training time and 18x less inference FLOPs. TrajViT is the first efficient
encoder to consistently outperform ViT3D across diverse video analysis tasks,
making it a robust and scalable solution.

</details>


### [422] [Comparing the Effects of Persistence Barcodes Aggregation and Feature Concatenation on Medical Imaging](https://arxiv.org/abs/2505.23637)
*Dashti A. Ali,Richard K. G. Do,William R. Jarnagin,Aras T. Asaad,Amber L. Simpson*

Key words: 医学影像分析, 持续性同源, 拓扑数据分析, 特征工程, 分类性能

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文比较了在医学影像分析中，使用持续性同源（PH）特征时的两种特征向量构建方法：聚合条码后特征化和条码特征向量拼接。结果表明拼接法能保留更多细节信息，分类性能更优。

Motivation: 传统特征提取方法对输入的小变化敏感，而持续性同源（PH）能稳定提取拓扑和几何特征，但如何构建最终特征向量尚无明确结论，因此研究比较了两种方法的效果。

Method: 通过持续性同源从2D或3D医学影像中生成条形码，分别采用聚合后特征化和特征向量拼接两种方法构建特征向量，并在多个数据集上评估分类性能。

Result: 特征向量拼接法能保留更多细节信息，分类性能更好，是更优的选择。

Conclusion: 在医学影像分析中，通过拼接条码特征向量构建拓扑特征的方法优于聚合后特征化，因其能保留更多信息并提升分类性能。

Abstract: In medical image analysis, feature engineering plays an important role in the
design and performance of machine learning models. Persistent homology (PH),
from the field of topological data analysis (TDA), demonstrates robustness and
stability to data perturbations and addresses the limitation from traditional
feature extraction approaches where a small change in input results in a large
change in feature representation. Using PH, we store persistent topological and
geometrical features in the form of the persistence barcode whereby large bars
represent global topological features and small bars encapsulate geometrical
information of the data. When multiple barcodes are computed from 2D or 3D
medical images, two approaches can be used to construct the final topological
feature vector in each dimension: aggregating persistence barcodes followed by
featurization or concatenating topological feature vectors derived from each
barcode. In this study, we conduct a comprehensive analysis across diverse
medical imaging datasets to compare the effects of the two aforementioned
approaches on the performance of classification models. The results of this
analysis indicate that feature concatenation preserves detailed topological
information from individual barcodes, yields better classification performance
and is therefore a preferred approach when conducting similar experiments.

</details>


### [423] [CLDTracker: A Comprehensive Language Description for Visual Tracking](https://arxiv.org/abs/2505.23704)
*Mohamad Alansari,Sajid Javed,Iyyakutti Iyappan Ganapathi,Sara Alansari,Muzammal Naseer*

Key words: 视觉目标跟踪, VLMs, 双分支架构, CLIP, GPT-4V

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一个名为CLDTracker的视觉跟踪框架，通过结合视觉语言模型（VLMs）的文本和视觉分支，解决了视觉目标跟踪（VOT）中动态外观变化、遮挡和背景干扰的问题，实现了先进的性能。

Motivation: 传统跟踪器在复杂场景中表现不佳，而视觉语言模型（VLMs）在语义理解方面表现出潜力，但直接应用于VOT存在文本表示不足、特征融合效率低等问题。

Method: 提出CLDTracker，采用双分支架构（文本分支和视觉分支），利用CLIP和GPT-4V等VLM生成丰富的文本描述，并通过高效的视觉和语言特征融合提升跟踪性能。

Result: 在六个标准VOT基准测试中取得了当前最优（SOTA）性能，验证了方法的有效性。

Conclusion: CLDTracker通过结合视觉和语言信息，为复杂场景下的目标跟踪提供了新思路。

Abstract: VOT remains a fundamental yet challenging task in computer vision due to
dynamic appearance changes, occlusions, and background clutter. Traditional
trackers, relying primarily on visual cues, often struggle in such complex
scenarios. Recent advancements in VLMs have shown promise in semantic
understanding for tasks like open-vocabulary detection and image captioning,
suggesting their potential for VOT. However, the direct application of VLMs to
VOT is hindered by critical limitations: the absence of a rich and
comprehensive textual representation that semantically captures the target
object's nuances, limiting the effective use of language information;
inefficient fusion mechanisms that fail to optimally integrate visual and
textual features, preventing a holistic understanding of the target; and a lack
of temporal modeling of the target's evolving appearance in the language
domain, leading to a disconnect between the initial description and the
object's subsequent visual changes. To bridge these gaps and unlock the full
potential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive
Language Description framework for robust visual Tracking. Our tracker
introduces a dual-branch architecture consisting of a textual and a visual
branch. In the textual branch, we construct a rich bag of textual descriptions
derived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with
semantic and contextual cues to address the lack of rich textual
representation. Experiments on six standard VOT benchmarks demonstrate that
CLDTracker achieves SOTA performance, validating the effectiveness of
leveraging robust and temporally-adaptive vision-language representations for
tracking. Code and models are publicly available at:
https://github.com/HamadYA/CLDTracker

</details>


### [424] [Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning](https://arxiv.org/abs/2505.23709)
*Dionysis Christopoulos,Sotiris Spanos,Eirini Baltzi,Valsamis Ntouskos,Konstantinos Karantzalos*

Key words: SLIMP, 皮肤病变, 对比学习, 元数据, 分类

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: SLIMP通过一种新的嵌套对比学习方法，结合图像和元数据学习皮肤病变的丰富表示，提高了皮肤病变分类任务的性能。

Motivation: 由于图像条件（照明、颜色、分辨率等）的巨大差异和缺乏临床及表型上下文，仅基于图像的黑色素瘤检测和皮肤病变分类面临挑战。临床医生通常通过患者的病史和其他病变外观来评估风险，SLIMP受此启发，结合图像和元数据进行学习。

Method: SLIMP采用嵌套对比学习方法，整合单个皮肤病变的外观、元数据以及患者级别的元数据（如病历和其他临床信息），充分利用所有可用数据模态。

Result: 实验表明，SLIMP在皮肤病变分类任务上的性能优于其他预训练策略，表明其学习到的表示质量更高。

Conclusion: SLIMP通过结合图像和元数据的多模态学习，显著提升了皮肤病变分类任务的性能，为相关领域的研究提供了新思路。

Abstract: We introduce SLIMP (Skin Lesion Image-Metadata Pre-training) for learning
rich representations of skin lesions through a novel nested contrastive
learning approach that captures complex relationships between images and
metadata. Melanoma detection and skin lesion classification based solely on
images, pose significant challenges due to large variations in imaging
conditions (lighting, color, resolution, distance, etc.) and lack of clinical
and phenotypical context. Clinicians typically follow a holistic approach for
assessing the risk level of the patient and for deciding which lesions may be
malignant and need to be excised, by considering the patient's medical history
as well as the appearance of other lesions of the patient. Inspired by this,
SLIMP combines the appearance and the metadata of individual skin lesions with
patient-level metadata relating to their medical record and other clinically
relevant information. By fully exploiting all available data modalities
throughout the learning process, the proposed pre-training strategy improves
performance compared to other pre-training strategies on downstream skin
lesions classification tasks highlighting the learned representations quality.

</details>


### [425] [CFP-Gen: Combinatorial Functional Protein Generation via Diffusion Language Models](https://arxiv.org/abs/2505.22869)
*Junbo Yin,Chao Zha,Wenjia He,Chencheng Xu,Xin Gao*

Key words: 蛋白质设计,扩散模型,多模态约束,功能性序列生成

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: CFP-Gen是一个新颖的扩散语言模型，用于组合功能性蛋白质生成，通过整合多模态条件（功能、序列和结构约束）实现蛋白质的从头设计。

Motivation: 现有蛋白质语言模型基于单一条件约束生成蛋白质序列，难以同时满足不同模态的多个约束条件，因此需要一种新方法来解决这一问题。

Method: CFP-Gen引入了注释引导特征调制（AGFM）模块和残基控制功能编码（RCFE）模块，动态调节蛋白质特征分布并捕捉残基间相互作用，同时支持3D结构编码器的无缝集成。

Result: CFP-Gen能够高通量生成功能与天然蛋白质相当的新型蛋白质，并且在设计多功能蛋白质时具有高成功率。

Conclusion: CFP-Gen在功能性蛋白质设计中表现出色，为解决多约束条件下的蛋白质生成提供了高效工具。

Abstract: Existing PLMs generate protein sequences based on a single-condition
constraint from a specific modality, struggling to simultaneously satisfy
multiple constraints across different modalities. In this work, we introduce
CFP-Gen, a novel diffusion language model for Combinatorial Functional Protein
GENeration. CFP-Gen facilitates the de novo protein design by integrating
multimodal conditions with functional, sequence, and structural constraints.
Specifically, an Annotation-Guided Feature Modulation (AGFM) module is
introduced to dynamically adjust the protein feature distribution based on
composable functional annotations, e.g., GO terms, IPR domains and EC numbers.
Meanwhile, the Residue-Controlled Functional Encoding (RCFE) module captures
residue-wise interaction to ensure more precise control. Additionally,
off-the-shelf 3D structure encoders can be seamlessly integrated to impose
geometric constraints. We demonstrate that CFP-Gen enables high-throughput
generation of novel proteins with functionality comparable to natural proteins,
while achieving a high success rate in designing multifunctional proteins. Code
and data available at https://github.com/yinjunbo/cfpgen.

</details>


### [426] [cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning](https://arxiv.org/abs/2505.22914)
*Maksim Kolodiazhnyi,Denis Tarasov,Dmitrii Zhemchuzhnikov,Alexander Nikulin,Ilya Zisman,Anna Vorontsova,Anton Konushin,Vladislav Kurenkov,Danila Rukhovich*

Key words: CAD重建, 多模态, 视觉语言模型, 强化学习, DeepCAD

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种多模态CAD重建模型，通过结合视觉语言模型，同时处理点云、图像和文本三种输入模态，采用两阶段训练流程（监督微调和强化学习微调），在多个数据集中实现了最先进的性能。

Motivation: 现有CAD重建方法通常只关注单一输入模态，限制了其通用性和鲁棒性。该研究旨在通过多模态输入和强化学习微调，提升CAD重建的灵活性和性能。

Method: 采用两阶段训练流程：1)监督微调（SFT）在大规模程序生成数据上；2)强化学习微调（RL）使用在线反馈。探索了Group Relative Preference Optimization（GRPO）等在线RL算法。

Result: 在DeepCAD基准测试中，SFT模型在三种输入模态上均优于现有单模态方法。经过RL微调后，模型在三个具有挑战性的数据集中（包括真实世界数据集）达到了新的最先进水平。

Conclusion: 多模态输入结合强化学习微调显著提升了CAD重建的性能和通用性，为未来CAD技术的发展提供了新方向。

Abstract: Computer-Aided Design (CAD) plays a central role in engineering and
manufacturing, making it possible to create precise and editable 3D models.
Using a variety of sensor or user-provided data as inputs for CAD
reconstruction can democratize access to design applications. However, existing
methods typically focus on a single input modality, such as point clouds,
images, or text, which limits their generalizability and robustness. Leveraging
recent advances in vision-language models (VLM), we propose a multi-modal CAD
reconstruction model that simultaneously processes all three input modalities.
Inspired by large language model (LLM) training paradigms, we adopt a two-stage
pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated
data, followed by reinforcement learning (RL) fine-tuning using online
feedback, obtained programatically. Furthermore, we are the first to explore RL
fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such
as Group Relative Preference Optimization (GRPO) outperform offline
alternatives. In the DeepCAD benchmark, our SFT model outperforms existing
single-modal approaches in all three input modalities simultaneously. More
importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three
challenging datasets, including a real-world one.

</details>


### [427] [MAGREF: Masked Guidance for Any-Reference Video Generation](https://arxiv.org/abs/2505.23742)
*Yufan Deng,Xun Guo,Yuanyang Yin,Jacob Zhiyuan Fang,Yiding Yang,Yizhi Wang,Shenghai Yuan,Angtian Wang,Bo Liu,Haibin Huang,Chongyang Ma*

Key words: 视频生成, 扩散模型, 多主体一致性, 掩码引导, 动态掩码

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出MAGREF框架，通过掩码引导和区域感知动态掩码机制，实现多参考视频生成的高质量和多主体一致性。

Motivation: 解决多参考视频生成中主体一致性和生成质量的挑战。

Method: 引入掩码引导和区域感知动态掩码机制，结合像素级通道连接机制。

Result: 在复杂多主体场景中实现最先进的生成质量，超越现有开源和商业基线。

Conclusion: MAGREF框架为多主体视频合成提供了可扩展、可控且高保真的解决方案。

Abstract: Video generation has made substantial strides with the emergence of deep
generative models, especially diffusion-based approaches. However, video
generation based on multiple reference subjects still faces significant
challenges in maintaining multi-subject consistency and ensuring high
generation quality. In this paper, we propose MAGREF, a unified framework for
any-reference video generation that introduces masked guidance to enable
coherent multi-subject video synthesis conditioned on diverse reference images
and a textual prompt. Specifically, we propose (1) a region-aware dynamic
masking mechanism that enables a single model to flexibly handle various
subject inference, including humans, objects, and backgrounds, without
architectural changes, and (2) a pixel-wise channel concatenation mechanism
that operates on the channel dimension to better preserve appearance features.
Our model delivers state-of-the-art video generation quality, generalizing from
single-subject training to complex multi-subject scenarios with coherent
synthesis and precise control over individual subjects, outperforming existing
open-source and commercial baselines. To facilitate evaluation, we also
introduce a comprehensive multi-subject video benchmark. Extensive experiments
demonstrate the effectiveness of our approach, paving the way for scalable,
controllable, and high-fidelity multi-subject video synthesis. Code and model
can be found at: https://github.com/MAGREF-Video/MAGREF

</details>


### [428] [Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need](https://arxiv.org/abs/2505.23744)
*Qiang Wang,Xiang Song,Yuhang He,Jizhou Han,Chenhao Ding,Xinyuan Gao,Yihong Gong*

Key words: SOYO, 域增量学习, 参数隔离, 轻量级框架, 高斯混合压缩器, 域特征重采样器

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: SOYO是一个轻量级框架，通过高斯混合压缩器和域特征重采样器优化参数隔离域增量学习中的域选择，并在多个任务中表现优异。

Motivation: 解决现有参数隔离域增量学习方法在域和类别增加时参数选择准确性不足的问题。

Method: 引入高斯混合压缩器（GMC）和域特征重采样器（DFR）存储和平衡历史数据，并结合多级域特征融合网络（MDFN）增强特征提取。

Result: 在六个基准测试中表现优于现有基线，展示其在复杂动态环境中的鲁棒性和适应性。

Conclusion: SOYO框架在域增量学习中有效提升了参数选择准确性和模型性能。

Abstract: Deep neural networks (DNNs) often underperform in real-world, dynamic
settings where data distributions change over time. Domain Incremental Learning
(DIL) offers a solution by enabling continual model adaptation, with
Parameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce
knowledge conflicts. However, existing PIDIL methods struggle with parameter
selection accuracy, especially as the number of domains and corresponding
classes grows. To address this, we propose SOYO, a lightweight framework that
improves domain selection in PIDIL. SOYO introduces a Gaussian Mixture
Compressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior
domain data efficiently, while a Multi-level Domain Feature Fusion Network
(MDFN) enhances domain feature extraction. Our framework supports multiple
Parameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks
such as image classification, object detection, and speech enhancement.
Experimental results on six benchmarks demonstrate SOYO's consistent
superiority over existing baselines, showcasing its robustness and adaptability
in complex, evolving environments. The codes will be released in
https://github.com/qwangcv/SOYO.

</details>


### [429] [To Trust Or Not To Trust Your Vision-Language Model's Prediction](https://arxiv.org/abs/2505.23745)
*Hao Dong,Moru Liu,Jian Liang,Eleni Chatzi,Olga Fink*

Key words: 视觉语言模型（VLMs）、可信度评估、误分类检测、模态差距、图像嵌入空间

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: TrustVLM是一个无需训练即可评估视觉语言模型（VLM）预测可信度的框架，通过利用图像嵌入空间的模态差距改进误分类检测，在多个数据集上显著提升性能。

Motivation: VLMs虽然在多模态理解与生成中表现优异，但其预测可能存在误分类且表现自信，这在安全关键领域可能带来严重后果。TrustVLM旨在解决这一可靠性挑战。

Method: 提出一种基于图像嵌入空间的置信度评分函数，利用模态差距和概念的区分性表示来检测误分类。

Result: 在17个数据集和4种架构上测试，TrustVLM在AURC、AUROC和FPR95等指标上分别提升最高达51.87%、9.14%和32.42%，优于现有基线。

Conclusion: TrustVLM通过改进预测可靠性且无需重新训练，为VLMs的实际安全部署提供了可行方案。

Abstract: Vision-Language Models (VLMs) have demonstrated strong capabilities in
aligning visual and textual modalities, enabling a wide range of applications
in multimodal understanding and generation. While they excel in zero-shot and
transfer learning scenarios, VLMs remain susceptible to misclassification,
often yielding confident yet incorrect predictions. This limitation poses a
significant risk in safety-critical domains, where erroneous predictions can
lead to severe consequences. In this work, we introduce TrustVLM, a
training-free framework designed to address the critical challenge of
estimating when VLM's predictions can be trusted. Motivated by the observed
modality gap in VLMs and the insight that certain concepts are more distinctly
represented in the image embedding space, we propose a novel confidence-scoring
function that leverages this space to improve misclassification detection. We
rigorously evaluate our approach across 17 diverse datasets, employing 4
architectures and 2 VLMs, and demonstrate state-of-the-art performance, with
improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95
compared to existing baselines. By improving the reliability of the model
without requiring retraining, TrustVLM paves the way for safer deployment of
VLMs in real-world applications. The code will be available at
https://github.com/EPFL-IMOS/TrustVLM.

</details>


### [430] [Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence](https://arxiv.org/abs/2505.23747)
*Diankun Wu,Fangfu Liu,Yi-Hsin Hung,Yueqi Duan*

Key words: 多模态大语言模型,空间推理,2D视觉,双编码器,帧采样

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了Spatial-MLLM框架，通过双编码器架构和空间感知帧采样策略，仅基于2D输入实现高效空间推理，并在多个任务中达到SOTA性能。

Motivation: 当前MLLMs在2D视觉任务中表现良好，但空间推理能力不足，且依赖额外3D/2.5D数据。本文旨在从纯2D观测中增强空间理解能力。

Method: 采用双编码器架构（语义编码器+空间编码器）提取特征，并通过连接器整合；引入空间感知帧采样策略优化推理效率。

Result: 在多个真实数据集上实现SOTA性能，显著提升基于视觉的空间理解与推理能力。

Conclusion: Spatial-MLLM框架有效解决了纯2D输入下的空间推理挑战，为MLLMs的空间智能提供了新方向。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have
significantly enhanced performance on 2D visual tasks. However, improving their
spatial intelligence remains a challenge. Existing 3D MLLMs always rely on
additional 3D or 2.5D data to incorporate spatial awareness, restricting their
utility in scenarios with only 2D inputs, such as images or videos. In this
paper, we present Spatial-MLLM, a novel framework for visual-based spatial
reasoning from purely 2D observations. Unlike conventional video MLLMs which
rely on CLIP-based visual encoders optimized for semantic understanding, our
key insight is to unleash the strong structure prior from the feed-forward
visual geometry foundation model. Specifically, we propose a dual-encoder
architecture: a pretrained 2D visual encoder to extract semantic features, and
a spatial encoder-initialized from the backbone of the visual geometry model-to
extract 3D structure features. A connector then integrates both features into
unified visual tokens for enhanced spatial understanding. Furthermore, we
propose a space-aware frame sampling strategy at inference time, which selects
the spatially informative frames of a video sequence, ensuring that even under
limited token length, the model focuses on frames critical for spatial
reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k
dataset and train the model on it using supervised fine-tuning and GRPO.
Extensive experiments on various real-world datasets demonstrate that our
spatial-MLLM achieves state-of-the-art performance in a wide range of
visual-based spatial understanding and reasoning tasks. Project page:
https://diankun-wu.github.io/Spatial-MLLM/.

</details>


### [431] [LADA: Scalable Label-Specific CLIP Adapter for Continual Learning](https://arxiv.org/abs/2505.23271)
*Mao-Lin Luo,Zi-Hao Zhou,Tong Wei,Min-Ling Zhang*

Key words: 持续学习, CLIP, 特征蒸馏, 灾难性遗忘, 标签特定适配器

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: LADA提出了一种新的持续学习方法，通过在冻结的CLIP图像编码器上添加轻量级标签特定记忆单元，避免参数分割和性能下降，同时防止灾难性遗忘，实验表明其性能领先。

Motivation: 现有多任务CLIP方法因参数分割导致推理错误，影响性能，需一种更高效且避免遗忘的方案。

Method: LADA在冻结CLIP编码器后追加标签特定记忆单元，通过特征聚合和蒸馏实现任务无关知识利用与抗干扰。

Result: 实验证明LADA在持续学习任务中达到最优性能，优于现有方法。

Conclusion: LADA通过轻量级适配器和蒸馏策略，高效解决持续学习中的性能和遗忘问题。

Abstract: Continual learning with vision-language models like CLIP offers a pathway
toward scalable machine learning systems by leveraging its transferable
representations. Existing CLIP-based methods adapt the pre-trained image
encoder by adding multiple sets of learnable parameters, with each task using a
partial set of parameters. This requires selecting the expected parameters for
input images during inference, which is prone to error that degrades
performance. To address this problem, we introduce LADA (Label-specific
ADApter). Instead of partitioning parameters across tasks, LADA appends
lightweight, label-specific memory units to the frozen CLIP image encoder,
enabling discriminative feature generation by aggregating task-agnostic
knowledge. To prevent catastrophic forgetting, LADA employs feature
distillation for seen classes, preventing their features from being interfered
with by new classes. Positioned after the image encoder, LADA prevents gradient
flow to the frozen CLIP parameters, ensuring efficient training. Extensive
results show that LADA achieves state-of-the-art performance in continual
learning settings. The implementation code is available at
https://github.com/MaolinLuo/LADA.

</details>


### [432] [TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning](https://arxiv.org/abs/2505.23475)
*Ron Shapira Weber,Shahar Ben Ishay,Andrey Lavrinenko,Shahaf E. Finder,Oren Freifeld*

Key words: 时间序列对齐，动态时间规整（DTW），自监督学习，关键点检测，合成数据

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种名为TimePoint的自监督方法，通过学习从合成数据中提取关键点和描述符，显著加速了基于动态时间规整（DTW）的时间序列对齐，并提高了对齐精度。

Motivation: 由于传统动态时间规整（DTW）在大规模时间序列对齐中的计算效率低且对噪声敏感，研究团队旨在开发一种快速且可扩展的替代方案，以改善对齐速度和准确性。

Method: TimePoint通过生成合成数据并利用高效的1维微分同胚模拟非线性时间扭曲，采用全卷积和小波卷积架构提取关键点和描述符，从而优化DTW对齐。

Result: 实验表明，TimePoint不仅显著减少了计算时间，而且通常比标准DTW在原始信号上的对齐精度更高，同时在真实数据上展现出良好的泛化能力。

Conclusion: TimePoint为时间序列分析提供了一个快速、准确且可扩展的解决方案，特别适用于需要高效对齐的场景。

Abstract: Fast and scalable alignment of time series is a fundamental challenge in many
domains. The standard solution, Dynamic Time Warping (DTW), struggles with poor
scalability and sensitivity to noise. We introduce TimePoint, a self-supervised
method that dramatically accelerates DTW-based alignment while typically
improving alignment accuracy by learning keypoints and descriptors from
synthetic data. Inspired by 2D keypoint detection but carefully adapted to the
unique challenges of 1D signals, TimePoint leverages efficient 1D
diffeomorphisms, which effectively model nonlinear time warping, to generate
realistic training data. This approach, along with fully convolutional and
wavelet convolutional architectures, enables the extraction of informative
keypoints and descriptors. Applying DTW to these sparse representations yield
major speedups and typically higher alignment accuracy than standard DTW
applied to the full signals. TimePoint demonstrates strong generalization to
real-world time series when trained solely on synthetic data, and further
improves with fine-tuning on real data. Extensive experiments demonstrate that
TimePoint consistently achieves faster and more accurate alignments than
standard DTW, making it a scalable solution for time-series analysis. Our code
is available at https://github.com/BGU-CS-VIL/TimePoint

</details>


### [433] [OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data](https://arxiv.org/abs/2505.23522)
*Fengxiang Wang,Mingshuo Chen,Xuming He,YiFan Zhang,Feng Liu,Zijie Guo,Zhenghao Hu,Jiong Wang,Jingyi Xu,Zhangrui Li,Fenghua Ling,Ben Fei,Weijia Li,Long Lan,Wenjing Yang,Wenlong Zhang,Lei Bai*

Key words: 地球科学, 多模态学习, 基准测试, 跨圈层交互, 模型评估

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: OmniEarth-Bench是一个全面覆盖地球科学六大圈层及其交叉作用的多模态基准测试，包含100个专家评估维度和29,779个标注。实验显示当前最先进模型在其任务上表现不佳。

Motivation: 现有地球科学多模态学习的基准测试在系统性覆盖和跨圈层交互方面存在局限，OmniEarth-Bench旨在填补这一空白。

Method: 整合卫星和实地观测数据，通过专家和众包标注，构建包含感知、推理等四层任务的评估体系。

Result: 测试9种先进模型，最高准确率不足35%，部分跨圈层任务模型得分为0%。

Conclusion: OmniEarth-Bench为地学AI设立了新标准，推动科学发现和环境监测应用。

Abstract: Existing benchmarks for Earth science multimodal learning exhibit critical
limitations in systematic coverage of geosystem components and cross-sphere
interactions, often constrained to isolated subsystems (only in
Human-activities sphere or atmosphere) with limited evaluation dimensions (less
than 16 tasks). To address these gaps, we introduce OmniEarth-Bench, the first
comprehensive multimodal benchmark spanning all six Earth science spheres
(atmosphere, lithosphere, Oceansphere, cryosphere, biosphere and
Human-activities sphere) and cross-spheres with one hundred expert-curated
evaluation dimensions. Leveraging observational data from satellite sensors and
in-situ measurements, OmniEarth-Bench integrates 29,779 annotations across four
tiers: perception, general reasoning, scientific knowledge reasoning and
chain-of-thought (CoT) reasoning. This involves the efforts of 2-5 experts per
sphere to establish authoritative evaluation dimensions and curate relevant
observational datasets, 40 crowd-sourcing annotators to assist experts for
annotations, and finally, OmniEarth-Bench is validated via hybrid expert-crowd
workflows to reduce label ambiguity. Experiments on 9 state-of-the-art MLLMs
reveal that even the most advanced models struggle with our benchmarks, where
none of them reach 35\% accuracy. Especially, in some cross-spheres tasks, the
performance of leading models like GPT-4o drops to 0.0\%. OmniEarth-Bench sets
a new standard for geosystem-aware AI, advancing both scientific discovery and
practical applications in environmental monitoring and disaster prediction. The
dataset, source code, and trained models were released.

</details>


### [434] [Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information](https://arxiv.org/abs/2505.23558)
*Xu Chu,Xinrong Chen,Guanyu Wang,Zhijie Tan,Kui Huang,Wenyu Lv,Tong Mo,Weiping Li*

Key words: Vision-Language Models, Hallucination, Reinforcement Learning, Reflection Process, Visual Attention

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 为了解决视觉语言推理模型（VLRMs）在进行长推理时视觉信息被稀释导致幻觉的问题，论文提出了Qwen-LookAgain（Qwen-LA），通过引入视觉-文本反射过程来增强模型的视觉注意力。

Motivation: 长推理会导致视觉信息被淡化，从而引发幻觉，而仅靠文本反射不足以抑制幻觉。因此，需要一种新方法来引导模型在推理过程中重新关注视觉信息。

Method: 提出了Qwen-LA模型，使用平衡反射策略优化（BRPO）的强化学习方法，引导模型自主决定何时生成视觉-文本反射，并在训练和推理中引入了Visual Token COPY和Visual Token ROUTE来强制模型重新关注视觉信息。

Result: 实验表明，Qwen-LA在多个视觉问答数据集和幻觉指标上均表现出领先的准确性和较低的幻觉发生率。

Conclusion: Qwen-LA通过视觉-文本反射和视觉信息补充，有效提升了视觉语言推理模型的准确性并减少了幻觉。

Abstract: Inference time scaling drives extended reasoning to enhance the performance
of Vision-Language Models (VLMs), thus forming powerful Vision-Language
Reasoning Models (VLRMs). However, long reasoning dilutes visual tokens,
causing visual information to receive less attention and may trigger
hallucinations. Although introducing text-only reflection processes shows
promise in language models, we demonstrate that it is insufficient to suppress
hallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain
(Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a
vision-text reflection process that guides the model to re-attention visual
information during reasoning. We first propose a reinforcement learning method
Balanced Reflective Policy Optimization (BRPO), which guides the model to
decide when to generate vision-text reflection on its own and balance the
number and length of reflections. Then, we formally prove that VLRMs lose
attention to visual tokens as reasoning progresses, and demonstrate that
supplementing visual information during reflection enhances visual attention.
Therefore, during training and inference, Visual Token COPY and Visual Token
ROUTE are introduced to force the model to re-attention visual information at
the visual level, addressing the limitations of text-only reflection.
Experiments on multiple visual QA datasets and hallucination metrics indicate
that Qwen-LA achieves leading accuracy performance while reducing
hallucinations. Our code is available at:
https://github.com/Liar406/Look_Again.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [435] [Finite-Sample Convergence Bounds for Trust Region Policy Optimization in Mean-Field Games](https://arxiv.org/abs/2505.22781)
*Antonio Ocello,Daniil Tiapkin,Lorenzo Mancini,Mathieu Laurière,Eric Moulines*

Key words: 平均场游戏（MFG）、TRPO、纳什均衡、强化学习、多智能体决策

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出MF-TRPO算法，用于计算有限状态-动作空间中遍历性平均场游戏的近似纳什均衡，结合TRPO的稳定性和鲁棒性，提供理论收敛保证和样本复杂度分析。

Motivation: 为了将强化学习中TRPO的成功经验扩展到平均场游戏（MFG）框架，解决复杂多智能体决策问题。

Method: 基于TRPO的方法论，提出MF-TRPO算法，对精确形式和基于样本的形式进行理论分析。

Result: 在标准假设下，算法具有理论收敛保证、高概率保证和有限样本复杂度。

Conclusion: MF-TRPO通过结合RL技术和平均场决策，为多智能体问题提供了理论支持的解决方案。

Abstract: We introduce Mean-Field Trust Region Policy Optimization (MF-TRPO), a novel
algorithm designed to compute approximate Nash equilibria for ergodic
Mean-Field Games (MFG) in finite state-action spaces. Building on the
well-established performance of TRPO in the reinforcement learning (RL)
setting, we extend its methodology to the MFG framework, leveraging its
stability and robustness in policy optimization. Under standard assumptions in
the MFG literature, we provide a rigorous analysis of MF-TRPO, establishing
theoretical guarantees on its convergence. Our results cover both the exact
formulation of the algorithm and its sample-based counterpart, where we derive
high-probability guarantees and finite sample complexity. This work advances
MFG optimization by bridging RL techniques with mean-field decision-making,
offering a theoretically grounded approach to solving complex multi-agent
problems.

</details>


### [436] [Highly Efficient and Effective LLMs with Multi-Boolean Architectures](https://arxiv.org/abs/2505.22811)
*Ba-Hien Tran,Van Minh Nguyen*

Key words: 大语言模型, 二值化, 多核布尔参数, 低比特量化

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出了一种新颖的框架，将大语言模型（LLMs）转化为多核布尔参数，并直接在布尔域中进行微调，显著降低了复杂度并提升了性能。

Motivation: 现有的大语言模型二值化方法在信息损失和复杂度方面存在问题，需要更高效的方法以降低复杂度同时保持性能。

Method: 引入了多核布尔参数框架，首次直接在布尔域中微调LLMs，避免了昂贵的潜在权重使用。

Result: 实验表明该方法在多种LLMs上性能优于当前的低比特量化和二值化方法。

Conclusion: 直接在布尔域中微调LLMs是一种可行的低复杂度解决方案，且在性能上具有竞争力。

Abstract: Weight binarization has emerged as a promising strategy to drastically reduce
the complexity of large language models (LLMs). It is mainly classified into
two approaches: post-training binarization and finetuning with training-aware
binarization methods. The first approach, while having low complexity, leads to
significant loss of information from the original LLMs, resulting in poor
performance. The second approach, on the other hand, relies heavily on
full-precision latent weights for gradient approximation of binary weights,
which not only remains suboptimal but also introduces substantial complexity.
In this paper, we introduce a novel framework that effectively transforms LLMs
into multi-kernel Boolean parameters, for the first time, finetunes them
directly in the Boolean domain, eliminating the need for expensive latent
weights. This significantly reduces complexity during both finetuning and
inference. Through extensive and insightful experiments across a wide range of
LLMs, we demonstrate that our method outperforms recent ultra low-bit
quantization and binarization methods.

</details>


### [437] [Theoretical Foundations of the Deep Copula Classifier: A Generative Approach to Modeling Dependent Features](https://arxiv.org/abs/2505.22997)
*Agnideep Aich,Ashit Baran Aich,Bruce Wade*

Key words: Deep Copula Classifier, generative model, neural networks, dependency-aware classification, Bayes-optimal

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: Deep Copula Classifier (DCC) is a generative model that uses neural networks to capture complex feature dependencies, achieving Bayes-optimal performance with practical extensions for scalability and adaptability.

Motivation: Traditional classifiers often fail to capture real-world feature dependencies, leading to suboptimal performance. DCC addresses this by modeling joint dependencies via neural network-parameterized copulas.

Method: DCC learns marginal distributions separately from joint dependencies using lightweight neural networks, and supports practical extensions like high-dimensional scalability and semi-supervised learning.

Result: DCC converges to the Bayes-optimal classifier under standard conditions with explicit convergence rates, and offers a mathematically rigorous yet interpretable framework.

Conclusion: DCC provides a unified approach to dependency-aware classification, combining statistical rigor with neural network flexibility, making it effective for complex real-world scenarios.

Abstract: Traditional classifiers often assume feature independence or rely on overly
simplistic relationships, leading to poor performance in settings where
real-world dependencies matter. We introduce the Deep Copula Classifier (DCC),
a generative model that separates the learning of each feature's marginal
distribution from the modeling of their joint dependence structure via neural
network-parameterized copulas. For each class, lightweight neural networks are
used to flexibly and adaptively capture feature interactions, making DCC
particularly effective when classification is driven by complex dependencies.
We establish that DCC converges to the Bayes-optimal classifier under standard
conditions and provide explicit convergence rates of O(n^{-r/(2r + d)}) for
r-smooth copula densities. Beyond theoretical guarantees, we outline several
practical extensions, including high-dimensional scalability through vine and
factor copula architectures, semi-supervised learning via entropy
regularization, and online adaptation using streaming gradient methods. By
unifying statistical rigor with the representational power of neural networks,
DCC offers a mathematically grounded and interpretable framework for
dependency-aware classification.

</details>


### [438] [JAPAN: Joint Adaptive Prediction Areas with Normalising-Flows](https://arxiv.org/abs/2505.23196)
*Eshant English,Christoph Lippert*

Key words: 共形预测、不确定性量化、归一化流、多模态分布、密度估计

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出了JAPAN框架，通过基于密度的置信度评分改进共形预测，解决了传统残差方法在处理多模态分布时的局限性，生成更紧凑且自适应的预测区域。

Motivation: 传统共形预测方法依赖残差评分，对多模态分布效果不佳，预测区域过于保守且集中在均值附近，无法准确捕捉复杂分布的形态。

Method: 引入JAPAN框架，基于归一化流（Normalising-Flows）估计预测密度，通过阈值化密度评分构建自适应、紧凑且可能不连续的预测区域，同时保持有限样本覆盖保证。

Result: 在多元回归和预测任务中，JAPAN展现了良好的校准性，并生成比现有基线更紧密的预测区域。理论分析和实验验证了其高效性。

Conclusion: JAPAN通过密度评分改进了共形预测的灵活性，适用于复杂分布，并提供多种扩展增强其适用性。

Abstract: Conformal prediction provides a model-agnostic framework for uncertainty
quantification with finite-sample validity guarantees, making it an attractive
tool for constructing reliable prediction sets. However, existing approaches
commonly rely on residual-based conformity scores, which impose geometric
constraints and struggle when the underlying distribution is multimodal. In
particular, they tend to produce overly conservative prediction areas centred
around the mean, often failing to capture the true shape of complex predictive
distributions. In this work, we introduce JAPAN (Joint Adaptive Prediction
Areas with Normalising-Flows), a conformal prediction framework that uses
density-based conformity scores. By leveraging flow-based models, JAPAN
estimates the (predictive) density and constructs prediction areas by
thresholding on the estimated density scores, enabling compact, potentially
disjoint, and context-adaptive regions that retain finite-sample coverage
guarantees. We theoretically motivate the efficiency of JAPAN and empirically
validate it across multivariate regression and forecasting tasks, demonstrating
good calibration and tighter prediction areas compared to existing baselines.
We also provide several \emph{extensions} adding flexibility to our proposed
framework.

</details>


### [439] [Stable Thompson Sampling: Valid Inference via Variance Inflation](https://arxiv.org/abs/2505.23260)
*Budhaditya Halder,Shubhayan Pan,Koulik Khamaru*

Key words: Thompson Sampling, statistical inference, adaptive sampling, regret, variance inflation

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出一种改进的Thompson Sampling算法Stable Thompson Sampling，通过增加对数因子的后验方差膨胀，解决了自适应采样数据下的统计推断问题，以较小的遗憾代价实现了参数估计的渐近正态性。

Motivation: Thompson Sampling虽然在实际应用中表现出色，但其自适应采样机制导致模型参数的置信区间构建困难。

Method: 提出Stable Thompson Sampling算法，通过增加对数因子的后验方差膨胀。

Result: 改进算法实现了臂均值的渐近正态估计，遗憾仅比标准TS增加对数因子。

Conclusion: 在自适应决策算法中，以较小的遗憾代价换取有效的统计推断是可行的。

Abstract: We consider the problem of statistical inference when the data is collected
via a Thompson Sampling-type algorithm. While Thompson Sampling (TS) is known
to be both asymptotically optimal and empirically effective, its adaptive
sampling scheme poses challenges for constructing confidence intervals for
model parameters. We propose and analyze a variant of TS, called Stable
Thompson Sampling, in which the posterior variance is inflated by a logarithmic
factor. We show that this modification leads to asymptotically normal estimates
of the arm means, despite the non-i.i.d. nature of the data. Importantly, this
statistical benefit comes at a modest cost: the variance inflation increases
regret by only a logarithmic factor compared to standard TS. Our results reveal
a principled trade-off: by paying a small price in regret, one can enable valid
statistical inference for adaptive decision-making algorithms.

</details>


### [440] [The Strong, Weak and Benign Goodhart's law. An independence-free and paradigm-agnostic formalisation](https://arxiv.org/abs/2505.23445)
*Adrien Majka,El-Mahdi El-Mhamdi*

Key words: Goodhart's law, proxy metric, coupling, light tailed, heavy tailed

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文通过放宽独立性和学习范式的假设，研究了代理指标与目标耦合对Goodhart定律的影响，发现依赖关系不影响Goodhart效应，但在轻重尾差异下，过度优化率与差异的尾部重性成反比。

Motivation: 正式化Goodhart定律，研究代理指标与目标耦合的影响，验证其在机器学习中的有效性。

Method: 放宽独立性假设和学习范式限制，分析代理指标与目标的耦合效应。

Result: 轻尾目标和轻尾差异下，依赖性不影响Goodhart效应；轻尾目标和重尾差异下，过度优化率与差异尾部重性成反比。

Conclusion: 依赖关系在某些情况下不影响Goodhart效应，但在特定尾部条件下会影响过度优化率。

Abstract: Goodhart's law is a famous adage in policy-making that states that ``When a
measure becomes a target, it ceases to be a good measure''. As machine learning
models and the optimisation capacity to train them grow, growing empirical
evidence reinforced the belief in the validity of this law without however
being formalised. Recently, a few attempts were made to formalise Goodhart's
law, either by categorising variants of it, or by looking at how optimising a
proxy metric affects the optimisation of an intended goal. In this work, we
alleviate the simplifying independence assumption, made in previous works, and
the assumption on the learning paradigm made in most of them, to study the
effect of the coupling between the proxy metric and the intended goal on
Goodhart's law. Our results show that in the case of light tailed goal and
light tailed discrepancy, dependence does not change the nature of Goodhart's
effect. However, in the light tailed goal and heavy tailed discrepancy case, we
exhibit an example where over-optimisation occurs at a rate inversely
proportional to the heavy tailedness of the discrepancy between the goal and
the metric. %

</details>


### [441] [Learning Parametric Distributions from Samples and Preferences](https://arxiv.org/abs/2505.23557)
*Marc Jourdan,Gizem Yüce,Nicolas Flammarion*

Key words: 偏好反馈，参数估计，M估计器，渐近方差，高斯分布，拉普拉斯分布

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文研究了偏好反馈在持续参数分布类参数估计中的作用，证明基于偏好的M估计器比仅使用样本的M估计器具有更好的渐近方差，且确定性偏好可进一步提高估计效率。

Motivation: 研究偏好反馈如何在不同条件下提升语言模型的参数估计性能，特别是在持续参数分布类中的表现。

Method: 提出采用基于偏好的M估计器框架，分析样本对及其相对偏好，并利用确定性偏好的约束条件设计新估计器。

Result: 该方法显著提升了估计误差降至$\(\mathcal{O}(1/n)\)$，优于仅使用样本时的$\(\Theta(1/\sqrt{n})\)$，且在某些分布（如高斯或拉普拉斯）下匹配理论下界。

Conclusion: 偏好反馈能有效加速参数估计，尤其当偏好满足假设条件时，但分析中的严格假设限制了其普适性。

Abstract: Recent advances in language modeling have underscored the role of preference
feedback in enhancing model performance. This paper investigates the conditions
under which preference feedback improves parameter estimation in classes of
continuous parametric distributions. In our framework, the learner observes
pairs of samples from an unknown distribution along with their relative
preferences depending on the same unknown parameter. We show that
preference-based M-estimators achieve a better asymptotic variance than
sample-only M-estimators, further improved by deterministic preferences.
Leveraging the hard constraints revealed by deterministic preferences, we
propose an estimator achieving an estimation error scaling of
$\mathcal{O}(1/n)$ -- a significant improvement over the $\Theta(1/\sqrt{n})$
rate attainable with samples alone. Next, we establish a lower bound that
matches this accelerated rate; up to dimension and problem-dependent constants.
While the assumptions underpinning our analysis are restrictive, they are
satisfied by notable cases such as Gaussian or Laplace distributions for
preferences based on the log-probability reward.

</details>


### [442] [Multilook Coherent Imaging: Theoretical Guarantees and Algorithms](https://arxiv.org/abs/2505.23594)
*Xi Chen,Soham Jana,Christopher A. Metzler,Arian Maleki,Shirin Jalali*

Key words: 多视相干成像, 散斑噪声, 最大似然估计, 投影梯度下降, 牛顿-舒尔茨算法, 装袋策略

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文研究了多视相干成像中的乘性噪声问题，提出了一种基于最大似然估计的理论框架和优化算法，显著提升了成像质量。

Motivation: 解决多视相干成像中乘性噪声（散斑噪声）对图像质量的负面影响，填补理论研究的空白。

Method: 采用基于最大似然估计的框架，结合投影梯度下降（PGD）算法，并引入牛顿-舒尔茨算法和装袋策略优化计算效率与稳定性。

Result: 理论推导了最大似然估计的均方误差上界，实验表明所提算法在计算效率和成像质量上达到最优水平。

Conclusion: 通过理论与算法的双重创新，论文为多视相干成像提供了高效且高精度的解决方案。

Abstract: Multilook coherent imaging is a widely used technique in applications such as
digital holography, ultrasound imaging, and synthetic aperture radar. A central
challenge in these systems is the presence of multiplicative noise, commonly
known as speckle, which degrades image quality. Despite the widespread use of
coherent imaging systems, their theoretical foundations remain relatively
underexplored. In this paper, we study both the theoretical and algorithmic
aspects of likelihood-based approaches for multilook coherent imaging,
providing a rigorous framework for analysis and method development. Our
theoretical contributions include establishing the first theoretical upper
bound on the Mean Squared Error (MSE) of the maximum likelihood estimator under
the deep image prior hypothesis. Our results capture the dependence of MSE on
the number of parameters in the deep image prior, the number of looks, the
signal dimension, and the number of measurements per look. On the algorithmic
side, we employ projected gradient descent (PGD) as an efficient method for
computing the maximum likelihood solution. Furthermore, we introduce two key
ideas to enhance the practical performance of PGD. First, we incorporate the
Newton-Schulz algorithm to compute matrix inverses within the PGD iterations,
significantly reducing computational complexity. Second, we develop a bagging
strategy to mitigate projection errors introduced during PGD updates. We
demonstrate that combining these techniques with PGD yields state-of-the-art
performance. Our code is available at
https://github.com/Computational-Imaging-RU/Bagged-DIP-Speckle.

</details>


### [443] [Instance-Optimality for Private KL Distribution Estimation](https://arxiv.org/abs/2505.23620)
*Jiayuan Ye,Vitaly Feldman,Kunal Talwar*

Key words: 离散分布估计、KL散度、差分隐私、实例最优性、Good-Turing估计器

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文研究了离散分布的KL散度估计问题，提出了最优私有估计算法，并通过实例最优性改进了算法在实际分布上的表现。

Motivation: 现有最小最大最优私有估计器在最坏情况下表现不佳，无法反映算法在非最坏情况分布上的性能，因此需要从实例最优性角度优化算法。

Method: 提出基于Good-Turing估计器的私有变体算法，结合局部邻域定义对比最小估计误差，实现实例最优性。

Result: 算法在差分隐私约束下仍能实现常数倍实例最优性，且在KL散度估计上比现有方法更精准。

Conclusion: 通过实例最优性框架和私有Good-Turing估计器，显著提升了离散分布估计的精度和实用性。

Abstract: We study the fundamental problem of estimating an unknown discrete
distribution $p$ over $d$ symbols, given $n$ i.i.d. samples from the
distribution. We are interested in minimizing the KL divergence between the
true distribution and the algorithm's estimate. We first construct minimax
optimal private estimators. Minimax optimality however fails to shed light on
an algorithm's performance on individual (non-worst-case) instances $p$ and
simple minimax-optimal DP estimators can have poor empirical performance on
real distributions. We then study this problem from an instance-optimality
viewpoint, where the algorithm's error on $p$ is compared to the minimum
achievable estimation error over a small local neighborhood of $p$. Under
natural notions of local neighborhood, we propose algorithms that achieve
instance-optimality up to constant factors, with and without a differential
privacy constraint. Our upper bounds rely on (private) variants of the
Good-Turing estimator. Our lower bounds use additive local neighborhoods that
more precisely captures the hardness of distribution estimation in KL
divergence, compared to ones considered in prior works.

</details>


### [444] [On the Convergence Analysis of Muon](https://arxiv.org/abs/2505.23737)
*Wei Shen,Ruichuan Huang,Minhui Huang,Cong Shen,Jiawei Zhang*

Key words: Muon优化器, 矩阵结构参数, 收敛分析, Hessian矩阵, 梯度下降

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文研究了Muon优化器，专门为优化矩阵结构参数设计，通过理论分析揭示其在低秩和近似块对角Hessian矩阵的优势。

Motivation: 传统优化器将矩阵参数视为平坦向量，忽略了其结构特性，Muon优化器表现出色但理论支持不足。

Method: 通过理论分析Muon的收敛率，并与梯度下降（GD）进行比较，探究其优势条件。

Result: 理论表明Muon在低秩和近似块对角Hessian矩阵中表现更优，实验验证了理论发现。

Conclusion: Muon在特定Hessian结构下优于GD，为神经网络训练提供了新的优化选择。

Abstract: The majority of parameters in neural networks are naturally represented as
matrices. However, most commonly used optimizers treat these matrix parameters
as flattened vectors during optimization, potentially overlooking their
inherent structural properties. Recently, an optimizer called Muon has been
proposed, specifically designed to optimize matrix-structured parameters.
Extensive empirical evidence shows that Muon can significantly outperform
traditional optimizers when training neural networks. Nonetheless, the
theoretical understanding of Muon's convergence behavior and the reasons behind
its superior performance remain limited. In this work, we present a
comprehensive convergence rate analysis of Muon and its comparison with
Gradient Descent (GD). We further characterize the conditions under which Muon
can outperform GD. Our theoretical results reveal that Muon can benefit from
the low-rank and approximate blockwise diagonal structure of Hessian matrices
-- phenomena widely observed in practical neural network training. Our
experimental results support and corroborate the theoretical findings.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [445] [Distributed Federated Learning for Vehicular Network Security: Anomaly Detection Benefits and Multi-Domain Attack Threats](https://arxiv.org/abs/2505.23706)
*Utku Demir,Yalin E. Sagduyu,Tugba Erpek,Hossein Jafari,Sastry Kompella,Mengran Xue*

Key words: 车联网, 联邦学习, 深度学习, 安全分类, 多域攻击

<details>
  <summary>Details</summary>

Main category: cs.NI

TL;DR: 论文提出了一种分布式联邦学习(DFL)方法，用于在车联网中协作训练深度学习模型，解决了传统集中式或纯本地学习在大规模、高移动性和异构数据分布下的局限性。实验表明，DFL显著提高了分类准确性，但在面临无线干扰和数据投毒等多域攻击时存在漏洞。

Motivation: 车联网中安全信息分类对恶意或异常行为检测至关重要。传统方法因数据规模大、移动性高和分布式异构的挑战而受限，需要一种更高效的协作学习方案。

Method: 采用分布式联邦学习(DFL)，车辆通过一跳邻居交换模型更新，并通过多跳传播模型协作训练深度学习模型。基于VeReMi扩展数据集进行实验验证。

Result: DFL显著提升了所有车辆的全局分类准确性，尤其是本地准确率较低的车辆。研究还发现本地数据规模和时变网络连通性与模型准确性密切相关。然而，DFL在无线干扰和数据投毒攻击下表现出漏洞。

Conclusion: DFL为车联网提供了高效的知识共享方案，但其在多域攻击下的脆弱性表明需要进一步研究更鲁棒的防御策略。

Abstract: In connected and autonomous vehicles, machine learning for safety message
classification has become critical for detecting malicious or anomalous
behavior. However, conventional approaches that rely on centralized data
collection or purely local training face limitations due to the large scale,
high mobility, and heterogeneous data distributions inherent in inter-vehicle
networks. To overcome these challenges, this paper explores Distributed
Federated Learning (DFL), whereby vehicles collaboratively train deep learning
models by exchanging model updates among one-hop neighbors and propagating
models over multiple hops. Using the Vehicular Reference Misbehavior (VeReMi)
Extension Dataset, we show that DFL can significantly improve classification
accuracy across all vehicles compared to learning strictly with local data.
Notably, vehicles with low individual accuracy see substantial accuracy gains
through DFL, illustrating the benefit of knowledge sharing across the network.
We further show that local training data size and time-varying network
connectivity correlate strongly with the model's overall accuracy. We
investigate DFL's resilience and vulnerabilities under attacks in multiple
domains, namely wireless jamming and training data poisoning attacks. Our
results reveal important insights into the vulnerabilities of DFL when
confronted with multi-domain attacks, underlining the need for more robust
strategies to secure DFL in vehicular networks.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [446] [LCB-CV-UNet: Enhanced Detector for High Dynamic Range Radar Signals](https://arxiv.org/abs/2505.23454)
*Yanbin Wang,Xingyu Chen,Yumiao Wang,Xiang Wang,Chuanfei Zang,Guolong Cui,Jiahuan Liu*

Key words: HDR, LCB, 雷达信号, 半合成数据集, 检测概率

<details>
  <summary>Details</summary>

Main category: eess.SP

TL;DR: 论文提出LCB-CV-UNet模型，通过Logarithmic Connect Block (LCB)模块和双混合数据集构建方法，提升高动态范围雷达信号处理的性能，实现约1%的检测概率提升。

Motivation: 高动态范围（HDR）雷达信号会导致性能下降，需一种高效且能保持相位一致性的解决方案。

Method: 提出LCB模块作为硬件高效的即插即用方案，并设计双混合数据集构建方法生成半合成数据集，模拟典型HDR信号场景。

Result: 仿真结果显示检测概率提升约1%，计算复杂度增加低于0.9%；在11-13 dB信噪比范围内表现优于基线5%。实际实验验证了模型实用性。

Conclusion: LCB-CV-UNet能有效解决HDR信号处理问题，性能显著提升且计算成本低。

Abstract: We propose the LCB-CV-UNet to tackle performance degradation caused by High
Dynamic Range (HDR) radar signals. Initially, a hardware-efficient,
plug-and-play module named Logarithmic Connect Block (LCB) is proposed as a
phase coherence preserving solution to address the inherent challenges in
handling HDR features. Then, we propose the Dual Hybrid Dataset Construction
method to generate a semi-synthetic dataset, approximating typical HDR signal
scenarios with adjustable target distributions. Simulation results show about
1% total detection probability improvement with under 0.9% computational
complexity added compared with the baseline. Furthermore, it excels 5% over the
baseline at the range in 11-13 dB signal-to-noise ratio typical for urban
targets. Finally, the real experiment validates the practicality of our model.

</details>


### [447] [Temporal Convolutional Autoencoder for Interference Mitigation in FMCW Radar Altimeters](https://arxiv.org/abs/2505.22783)
*Charles E. Thornton,Jamie Sloop,Samuel Brown,Aaron Orndorff,William C. Headley,Stephen Young*

Key words: FMCW雷达, 自编码器, 干扰抑制, 深度学习, TCN

<details>
  <summary>Details</summary>

Main category: eess.SP

TL;DR: 研究探讨了基于卷积自编码器的干扰抑制方法在FMCW雷达高度计中的端到端高度估计性能，TCN自编码器表现出优于LMS自适应滤波器的性能。

Motivation: 探索更有效的FMCW雷达干扰抑制方法，提升高度估计性能。

Method: 使用Temporal Convolutional Network (TCN)自编码器直接处理接收的FMCW信号，利用时序相关性抑制干扰。

Result: TCN自编码器在干扰抑制上优于Least Mean Squares (LMS)自适应滤波器。

Conclusion: 该方法展示了深度学习中实时性和泛化性的挑战，需未来研究改进。

Abstract: We investigate the end-to-end altitude estimation performance of a
convolutional autoencoder-based interference mitigation approach for
frequency-modulated continuous-wave (FMCW) radar altimeters. Specifically, we
show that a Temporal Convolutional Network (TCN) autoencoder effectively
exploits temporal correlations in the received signal, providing superior
interference suppression compared to a Least Mean Squares (LMS) adaptive
filter. Unlike existing approaches, the present method operates directly on the
received FMCW signal. Additionally, we identify key challenges in applying deep
learning to wideband FMCW interference mitigation and outline directions for
future research to enhance real-time feasibility and generalization to
arbitrary interference conditions.

</details>


### [448] [Topological Adaptive Least Mean Squares Algorithms over Simplicial Complexes](https://arxiv.org/abs/2505.23160)
*Lorenzo Marinucci,Claudio Battiloro,Paolo Di Lorenzo*

Key words: 自适应信号处理，单纯复形，LMS算法，拓扑分析，分布式优化

<details>
  <summary>Details</summary>

Main category: eess.SP

TL;DR: 该论文提出了一种新颖的自适应框架，用于处理单纯复形上的动态流信号，将经典的LMS方法扩展到高阶拓扑领域，并通过理论分析与实验验证其在性能和稳定性上的优势。

Motivation: 传统图信号处理方法难以捕捉高阶拓扑特征，因此需要一种能处理动态边子集和时变信号的框架，以更好地适应复杂数据结构的分析需求。

Method: 基于离散霍奇理论，提出了拓扑LMS算法，支持流信号处理、边采样优化、自适应拓扑推断及分布式实现。

Result: 理论和实验表明，该算法在稳态均方误差和收敛速度上优于传统图LMS方法，且能有效利用高阶拓扑特征。

Conclusion: 所提出的框架在集中式和分布式场景下均表现出色，为处理动态复杂信号提供了新思路。

Abstract: This paper introduces a novel adaptive framework for processing dynamic flow
signals over simplicial complexes, extending classical least-mean-squares (LMS)
methods to high-order topological domains. Building on discrete Hodge theory,
we present a topological LMS algorithm that efficiently processes streaming
signals observed over time-varying edge subsets. We provide a detailed
stochastic analysis of the algorithm, deriving its stability conditions,
steady-state mean-square-error, and convergence speed, while exploring the
impact of edge sampling on performance. We also propose strategies to design
optimal edge sampling probabilities, minimizing rate while ensuring desired
estimation accuracy. Assuming partial knowledge of the complex structure (e.g.,
the underlying graph), we introduce an adaptive topology inference method that
integrates with the proposed LMS framework. Additionally, we propose a
distributed version of the algorithm and analyze its stability and
mean-square-error properties. Empirical results on synthetic and real-world
traffic data demonstrate that our approach, in both centralized and distributed
settings, outperforms graph-based LMS methods by leveraging higher-order
topological features.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [449] [A Tool for Generating Exceptional Behavior Tests With Large Language Models](https://arxiv.org/abs/2505.22818)
*Linghan Zhong,Samuel Yuan,Jiyang Zhang,Yu Liu,Pengyu Nie,Junyi Jessy Li,Milos Gligoric*

Key words: 异常行为测试、LLM、自动化测试、CodeLlama

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: exLong框架利用微调的大型语言模型自动生成异常行为测试，填补开发者优先测试正常路径的不足。

Motivation: 开发者在测试中更关注正常路径，忽略异常场景，exLong旨在自动生成异常行为测试以解决这一问题。

Method: 基于CodeLlama微调的LLM，结合异常抛出轨迹分析、条件表达式及非异常测试轨迹生成测试。

Result: 演示视频显示exLong能有效辅助开发者生成全面异常测试。

Conclusion: exLong为异常测试自动化提供了可行方案，提升测试覆盖率。

Abstract: Exceptional behavior tests (EBTs) are crucial in software development for
verifying that code correctly handles unwanted events and throws appropriate
exceptions. However, prior research has shown that developers often prioritize
testing "happy paths", e.g., paths without unwanted events over exceptional
scenarios. We present exLong, a framework that automatically generates EBTs to
address this gap. exLong leverages a large language model (LLM) fine-tuned from
CodeLlama and incorporates reasoning about exception-throwing traces,
conditional expressions that guard throw statements, and non-exceptional
behavior tests that execute similar traces. Our demonstration video illustrates
how exLong can effectively assist developers in creating comprehensive EBTs for
their project (available at https://youtu.be/Jro8kMgplZk).

</details>


### [450] [OSS-UAgent: An Agent-based Usability Evaluation Framework for Open Source Software](https://arxiv.org/abs/2505.23239)
*Lingkai Meng,Yu Shao,Long Yuan,Longbin Lai,Peng Cheng,Wenyuan Yu,Wenjie Zhang,Xuemin Lin,Jingren Zhou*

Key words: 开源软件,可用性评估,自动化代理,大型语言模型

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 介绍了一个名为OSS-UAgent的自动化、可配置的代理框架，专门用于开源软件的可用性评估，利用大型语言模型模拟开发者行为，动态构建知识库，自动生成并评估代码。

Motivation: 解决传统基于人工的可用性评估方法成本高、扩展性有限的问题。

Method: 采用基于大型语言模型的智能代理模拟不同经验水平的开发者，动态构建知识库以实现精准的代码生成，并自动评估代码的合规性、正确性和可读性。

Result: OSS-UAgent在评估图分析平台时显示出高效自动化可用性评估的能力。

Conclusion: OSS-UAgent为开源软件的可用性评估提供了一种高效、自动化的解决方案。

Abstract: Usability evaluation is critical to the impact and adoption of open source
software (OSS), yet traditional methods relying on human evaluators suffer from
high costs and limited scalability. To address these limitations, we introduce
OSS-UAgent, an automated, configurable, and interactive agent-based usability
evaluation framework specifically designed for open source software. Our
framework employs intelligent agents powered by large language models (LLMs) to
simulate developers performing programming tasks across various experience
levels (from Junior to Expert). By dynamically constructing platform-specific
knowledge bases, OSS-UAgent ensures accurate and context-aware code generation.
The generated code is automatically evaluated across multiple dimensions,
including compliance, correctness, and readability, providing a comprehensive
measure of the software's usability. Additionally, our demonstration showcases
OSS-UAgent's practical application in evaluating graph analytics platforms,
highlighting its effectiveness in automating usability evaluation.

</details>


### [451] [SWE-bench Goes Live!](https://arxiv.org/abs/2505.23419)
*Linghao Zhang,Shilin He,Chaoyun Zhang,Yu Kang,Bowen Li,Chengxing Xie,Junhao Wang,Maoquan Wang,Yufan Huang,Shengyu Fu,Elsie Nallipogu,Qingwei Lin,Yingnong Dang,Saravan Rajmohan,Dongmei Zhang*

Key words: 问题解决任务、基准测试、实时更新、自动化流程、性能差距

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: SWE-bench-Live是一个动态更新的基准测试，用于评估语言模型修复现实世界问题的能力，解决了以往静态测试的局限性。

Motivation: 现有的SWE-bench测试存在更新滞后、覆盖范围窄以及依赖人工的问题，限制了评估的规模化和可靠性。

Method: 提出SWE-bench-Live，利用自动化流程从GitHub实时问题中生成任务，并使用Docker确保可复现性。

Result: 1,319个任务测试先进模型，发现与静态测试相比性能差距明显，进一步分析了影响表现的因素。

Conclusion: SWE-bench-Live提供了一个动态、多样化的评估基准，支持对语言模型在实际软件开发中的严格测试。

Abstract: The issue-resolving task, where a model generates patches to fix real-world
bugs, has emerged as a critical benchmark for evaluating the capabilities of
large language models (LLMs). While SWE-bench and its variants have become
standard in this domain, they suffer from key limitations: they have not been
updated since their initial releases, cover a narrow set of repositories, and
depend heavily on manual effort for instance construction and environment
setup. These factors hinder scalability and introduce risks of overfitting and
data contamination. In this work, we present \textbf{SWE-bench-Live}, a
\textit{live-updatable} benchmark designed to overcome these challenges. Our
initial release consists of 1,319 tasks derived from real GitHub issues created
since 2024, spanning 93 repositories. Each task is accompanied by a dedicated
Docker image to ensure reproducible execution. Central to our benchmark is
\method, an automated curation pipeline that streamlines the entire process
from instance creation to environment setup, removing manual bottlenecks and
enabling scalability and continuous updates. We evaluate a range of
state-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a
substantial performance gap compared to static benchmarks like SWE-bench, even
under controlled evaluation conditions. To better understand this discrepancy,
we perform detailed analyses across repository origin, issue recency, and task
difficulty. By providing a fresh, diverse, and executable benchmark grounded in
live repository activity, SWE-bench-Live facilitates rigorous,
contamination-resistant evaluation of LLMs and agents in dynamic, real-world
software development settings.

</details>


### [452] [Identity resolution of software metadata using Large Language Models](https://arxiv.org/abs/2505.23500)
*Eva Martín del Pico,Josep Lluís Gelpí,Salvador Capella-Gutiérrez*

Key words: 研究软件、元数据整合、大型语言模型、身份解析、FAIR原则

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 本文探讨了研究软件的重要性及其元数据的异构性问题，提出利用指令调优的大型语言模型解决软件元数据身份解析任务，并评估了其性能与局限性。

Motivation: 研究软件在科研中至关重要，但其关注度低于研究数据。元数据的质量与完整性差异大，需整合以支持软件可持续发展。

Method: 采用指令调优的大型语言模型进行软件元数据身份解析，并通过人工标注基准测试模型表现。

Result: 模型在高置信度自动决策中表现出高精度和统计稳健性，但揭示了当前模型在语义判断自动化中的局限性。

Conclusion: 整合异构元数据需更稳健的自动化方法，当前模型为FAIR对齐的软件元数据提供了可行但需改进的解决方案。

Abstract: Software is an essential component of research. However, little attention has
been paid to it compared with that paid to research data. Recently, there has
been an increase in efforts to acknowledge and highlight the importance of
software in research activities.
  Structured metadata from platforms like bio.tools, Bioconductor, and Galaxy
ToolShed offers valuable insights into research software in the Life Sciences.
Although originally intended to support discovery and integration, this
metadata can be repurposed for large-scale analysis of software practices.
However, its quality and completeness vary across platforms, reflecting diverse
documentation practices.
  To gain a comprehensive view of software development and sustainability,
consolidating this metadata is necessary, but requires robust mechanisms to
address its heterogeneity and scale.
  This article presents an evaluation of instruction-tuned large language
models for the task of software metadata identity resolution, a critical step
in assembling a cohesive collection of research software. Such a collection is
the reference component for the Software Observatory at OpenEBench, a platform
that aggregates metadata to monitor the FAIRness of research software in the
Life Sciences.
  We benchmarked multiple models against a human-annotated gold standard,
examined their behavior on ambiguous cases, and introduced an agreement-based
proxy for high-confidence automated decisions. The proxy achieved high
precision and statistical robustness, while also highlighting the limitations
of current models and the broader challenges of automating semantic judgment in
FAIR-aligned software metadata across registries and repositories.

</details>


### [453] [GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents](https://arxiv.org/abs/2505.23671)
*Manish Shetty,Naman Jain,Jinjian Liu,Vijay Kethanaboyina,Koushik Sen,Ion Stoica*

Key words: GSO, 高性能软件, 语言模型, 优化任务, SWE-Agent

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 论文介绍了GSO基准，用于评估语言模型开发高性能软件的能力，发现现有SWE-Agent成功率低于5%，并分析了失败原因。

Motivation: 开发高性能软件需要专业知识和工具，但目前缺乏评估语言模型在此任务中能力的标准基准。

Method: 通过自动化管道生成和执行性能测试，分析代码库提交历史，识别102个优化任务，并为代理提供代码库和测试以改进运行效率。

Result: 领先的SWE-Agent成功率低于5%，推理时间扩展带来的改进有限。

Conclusion: GSO基准揭示了语言模型在高性能软件开发中的关键挑战，包括低级语言困难、懒惰优化策略和瓶颈定位问题。

Abstract: Developing high-performance software is a complex task that requires
specialized expertise. We introduce GSO, a benchmark for evaluating language
models' capabilities in developing high-performance software. We develop an
automated pipeline that generates and executes performance tests to analyze
repository commit histories to identify 102 challenging optimization tasks
across 10 codebases, spanning diverse domains and programming languages. An
agent is provided with a codebase and performance test as a precise
specification, and tasked to improve the runtime efficiency, which is measured
against the expert developer optimization. Our quantitative evaluation reveals
that leading SWE-Agents struggle significantly, achieving less than 5% success
rate, with limited improvements even with inference-time scaling. Our
qualitative analysis identifies key failure modes, including difficulties with
low-level languages, practicing lazy optimization strategies, and challenges in
accurately localizing bottlenecks. We release the code and artifacts of our
benchmark along with agent trajectories to enable future research.

</details>


### [454] [Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization](https://arxiv.org/abs/2505.23387)
*Mingzhe Du,Luu Tuan Tuan,Yue Liu,Yuhao Qing,Dong Huang,Xinyi He,Qian Liu,Zejun Ma,See-kiong Ng*

Key words: 大语言模型, 代码优化, 强化学习, 执行反馈, 测试时优化

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 论文探讨了如何通过一种测试时的迭代优化框架提升大语言模型（LLMs）生成的代码效率，并对比了不同训练策略的效果。

Motivation: 大语言模型生成的代码功能上正确，但在效率上往往不佳，限制了其实际应用。本研究旨在解决这一问题，提升LLMs生成代码的执行效率。

Method: 采用了基于执行沙箱反馈的闭环系统进行迭代优化，并对比了监督微调（SFT）、直接偏好优化（DPO）和基于强化学习的组相对策略优化（GRPO）三种训练策略。

Result: 实验结果显示，GRPO能持续优化代码性能，显著提高pass@1指标（47%到62%）和超越人类提交代码效率的概率（31%到45%）。

Conclusion: 研究表明强化学习能有效教导LLMs自我优化代码效率，GRPO方法表现最佳。

Abstract: Large Language Models (LLMs) generate functionally correct solutions but
often fall short in code efficiency, a critical bottleneck for real-world
deployment. In this paper, we introduce a novel test-time iterative
optimization framework to address this, employing a closed-loop system where
LLMs iteratively refine code based on empirical performance feedback from an
execution sandbox. We explore three training strategies: Supervised Fine-Tuning
(SFT), Direct Preference Optimization (DPO), and Group Relative Policy
Optimization~(GRPO). Experiments on our Venus dataset and the APPS benchmark
show that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO,
using reinforcement learning (RL) with execution feedback, continuously
optimizes code performance, significantly boosting both pass@1 (from 47% to
62%) and the likelihood of outperforming human submissions in efficiency (from
31% to 45%). Our work demonstrates effective test-time code efficiency
improvement and critically reveals the power of RL in teaching LLMs to truly
self-improve code efficiency.

</details>


### [455] [Toward Effective AI Governance: A Review of Principles](https://arxiv.org/abs/2505.23417)
*Danilo Ribeiro,Thayssa Rocha,Gustavo Pinto,Bruno Cartaxo,Marcelo Amaral,Nicole Davila,Ana Camargo*

Key words: AI治理，负责任AI，治理框架，透明度，问责制

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 本文通过快速三级综述分析了AI治理的框架、原则、机制和利益相关者角色，发现欧盟AI法案和NIST RMF是最常引用的框架，透明度和问责制是最常见原则，但缺乏可操作的治理机制和利益相关者策略的详细说明。

Motivation: 尽管AI治理是负责任AI的核心支柱，但现有文献缺乏对治理框架和实践的综合分析，因此需要进行综述以填补这一空白。

Method: 对2020-2024年间IEEE和ACM的9篇同行评审的二级研究进行快速三级综述，采用结构化纳入标准和主题语义综合方法。

Result: 最常引用的框架是欧盟AI法案和NIST RMF，透明度和问责制是最常见原则，但少有研究详细说明可操作的治理机制或利益相关者策略。

Conclusion: 综述整合了AI治理的关键方向，并指出在实证验证和包容性方面的不足，为学术研究和实践应用提供了参考。

Abstract: Artificial Intelligence (AI) governance is the practice of establishing
frameworks, policies, and procedures to ensure the responsible, ethical, and
safe development and deployment of AI systems. Although AI governance is a core
pillar of Responsible AI, current literature still lacks synthesis across such
governance frameworks and practices. Objective: To identify which frameworks,
principles, mechanisms, and stakeholder roles are emphasized in secondary
literature on AI governance. Method: We conducted a rapid tertiary review of
nine peer-reviewed secondary studies from IEEE and ACM (20202024), using
structured inclusion criteria and thematic semantic synthesis. Results: The
most cited frameworks include the EU AI Act and NIST RMF; transparency and
accountability are the most common principles. Few reviews detail actionable
governance mechanisms or stakeholder strategies. Conclusion: The review
consolidates key directions in AI governance and highlights gaps in empirical
validation and inclusivity. Findings inform both academic inquiry and practical
adoption in organizations.

</details>


### [456] [From Knowledge to Noise: CTIM-Rover and the Pitfalls of Episodic Memory in Software Engineering Agents](https://arxiv.org/abs/2505.23422)
*Tobias Lindenbauer,Georg Groh,Hinrich Schütze*

Key words: CTIM-Rover, 软件工程, 跨任务实例内存, AutoCodeRover, ExpeL, MoEs

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: CTIM-Rover是基于AutoCodeRover构建的AI代理，引入跨任务实例内存（CTIM）以改进软件工程任务，但实际表现未超越AutoCodeRover。

Motivation: 当前开源软件工程代理缺乏长期记忆，而库级理解对修复错误至关重要，因此提出CTIM来改进。

Method: 结合体验学习（ExpeL）和专家混合（MoEs）方法，构建通用和库级的CTIM。

Result: CTIM-Rover在所有配置中均未超越AutoCodeRover，性能下降可能源于干扰性CTIM项或样例轨迹的噪声。

Conclusion: ExpeL和DoT-Bank难以扩展到实际软件工程问题，CTIM未带来预期改进。

Abstract: We introduce CTIM-Rover, an AI agent for Software Engineering (SE) built on
top of AutoCodeRover (Zhang et al., 2024) that extends agentic reasoning
frameworks with an episodic memory, more specifically, a general and
repository-level Cross-Task-Instance Memory (CTIM). While existing open-source
SE agents mostly rely on ReAct (Yao et al., 2023b), Reflexion (Shinn et al.,
2023), or Code-Act (Wang et al., 2024), all of these reasoning and planning
frameworks inefficiently discard their long-term memory after a single task
instance. As repository-level understanding is pivotal for identifying all
locations requiring a patch for fixing a bug, we hypothesize that SE is
particularly well positioned to benefit from CTIM. For this, we build on the
Experiential Learning (EL) approach ExpeL (Zhao et al., 2024), proposing a
Mixture-Of-Experts (MoEs) inspired approach to create both a general-purpose
and repository-level CTIM. We find that CTIM-Rover does not outperform
AutoCodeRover in any configuration and thus conclude that neither ExpeL nor
DoT-Bank (Lingam et al., 2024) scale to real-world SE problems. Our analysis
indicates noise introduced by distracting CTIM items or exemplar trajectories
as the likely source of the performance degradation.

</details>
