<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 163]
- [cs.LG](#cs.LG) [Total: 141]
- [cs.AI](#cs.AI) [Total: 36]
- [cs.CY](#cs.CY) [Total: 3]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.RO](#cs.RO) [Total: 5]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cs.CV](#cs.CV) [Total: 43]
- [cs.SD](#cs.SD) [Total: 9]
- [cs.HC](#cs.HC) [Total: 3]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.CR](#cs.CR) [Total: 17]
- [cs.IR](#cs.IR) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [stat.ML](#stat.ML) [Total: 15]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale](https://arxiv.org/abs/2505.23785)
*Cody Kommers,Drew Hemment,Maria Antoniak,Joel Z. Leibo,Hoyt Long,Emily Robinson,Adam Sobey*

Key words: 大型语言模型, 文化背景, 厚描述, 人类意义, AI表征

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该立场论文提出，大型语言模型（LLMs）能够以前所未有的规模在基于人工智能的社会技术系统中呈现文化背景和人类意义。传统系统因依赖‘薄描述’（数值化表示）而无法捕捉人类意义，而‘厚描述’（基于文本的表示）能保留上下文信息。LLMs的文本生成能力可能部分自动化厚描述的生成与处理，从而解决规模化难题。论文还提出了五个关键挑战，并认为厚描述可作为统一框架解决LLMs中文化表征的难题。

Motivation: 传统AI系统依赖数值化表示（薄描述），无法有效捕捉人类活动的文化背景与意义。而厚描述能保留上下文，但难以规模化。LLMs的文本能力可能通过自动化厚描述来解决这一问题，从而推动AI更准确地表征人类意义。

Method: 论文提出利用LLMs的文本生成能力，部分自动化厚描述的生成与处理，将其作为新的表征格式。同时，通过厚描述框架解决文化表征的挑战。

Result: LLMs有能力通过厚描述在AI系统中规模化表征人类意义，但需解决五个关键挑战：上下文保留、解释多样性、经验与批判视角整合、区分定性与定量内容、动态意义表征。

Conclusion: 厚描述是解决LLMs中文化表征难题的关键框架，其规模化应用需克服多方面的技术与社会挑战。

Abstract: This position paper argues that large language models (LLMs) can make
cultural context, and therefore human meaning, legible at an unprecedented
scale in AI-based sociotechnical systems. We argue that such systems have
previously been unable to represent human meaning because they rely on thin
descriptions: numerical representations that enforce standardization and
therefore strip human activity of the cultural context that gives it meaning.
By contrast, scholars in the humanities and qualitative social sciences have
developed frameworks for representing meaning through thick description: verbal
representations that accommodate heterogeneity and retain contextual
information needed to represent human meaning. While these methods can
effectively codify meaning, they are difficult to deploy at scale. However, the
verbal capabilities of LLMs now provide a means of (at least partially)
automating the generation and processing of thick descriptions, potentially
overcoming this bottleneck. We argue that the problem of rendering human
meaning legible is not just about selecting better metrics, but about
developing new representational formats (based on thick description). We frame
this as a crucial direction for the application of generative AI and identify
five key challenges: preserving context, maintaining interpretive pluralism,
integrating perspectives based on lived experience and critical distance,
distinguishing qualitative content from quantitative magnitude, and
acknowledging meaning as dynamic rather than static. Furthermore, we suggest
that thick description has the potential to serve as a unifying framework to
address a number of emerging concerns about the difficulties of representing
culture in (or using) LLMs.

</details>


### [2] [Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework](https://arxiv.org/abs/2505.23788)
*Aakash Sen Sharma,Debdeep Sanyal,Priyansh Srivastava,Sundar Atreya H.,Shirish Karande,Mohan Kankanhalli,Murari Mandal*

Key words: LLM, 版权侵权, FairUseDB, DPO, 评估指标

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出FUA-LLM框架，通过与知识产权专家合作开发FairUseDB数据集，利用DPO微调LLM，以减少版权侵权风险，同时保持实用性。

Motivation: 当前LLM存在版权侵权风险，现有保护措施多为拒绝式过滤，影响模型实用性，需开发更优解决方案。

Method: 构建FairUseDB数据集（18,000个案例），应用DPO微调LLM，提出新评估指标Weighted Penalty Utility和CAH。

Result: FUA-LLM显著减少问题输出（达20%），同时保持实用性。

Conclusion: FUA-LLM框架在法律合规与实用性间取得平衡，为LLM版权问题提供有效解决方案。

Abstract: Large language models (LLMs) commonly risk copyright infringement by
reproducing protected content verbatim or with insufficient transformative
modifications, posing significant ethical, legal, and practical concerns.
Current inference-time safeguards predominantly rely on restrictive
refusal-based filters, often compromising the practical utility of these
models. To address this, we collaborated closely with intellectual property
experts to develop FUA-LLM (Fair Use Aligned Language Models), a
legally-grounded framework explicitly designed to align LLM outputs with
fair-use doctrine. Central to our method is FairUseDB, a carefully constructed
dataset containing 18,000 expert-validated examples covering nine realistic
infringement scenarios. Leveraging this dataset, we apply Direct Preference
Optimization (DPO) to fine-tune open-source LLMs, encouraging them to produce
legally compliant and practically useful alternatives rather than resorting to
blunt refusal. Recognizing the shortcomings of traditional evaluation metrics,
we propose new measures: Weighted Penalty Utility and Compliance Aware Harmonic
Mean (CAH) to balance infringement risk against response utility. Extensive
quantitative experiments coupled with expert evaluations confirm that FUA-LLM
substantially reduces problematic outputs (up to 20\%) compared to
state-of-the-art approaches, while preserving real-world usability.

</details>


### [3] [Conversational Exploration of Literature Landscape with LitChat](https://arxiv.org/abs/2505.23789)
*Mingyu Huang,Shasha Zhou,Yuxuan Chen,Ke Li*

Key words: 大文学, LLM, 文献代理, 知识图谱, 数据挖掘

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LitChat是一个端到端的交互式文献代理，结合LLM与数据驱动工具，提供基于证据的文献探索方案，解决传统方法在“大文学”时代难以处理的问题。

Motivation: 数字化科学出版物数量激增，传统手动审阅方法难以应对，而现有LLM因上下文窗口限制和幻觉问题无法满足系统性综述的需求。

Method: LitChat结合LLM与数据驱动工具，自动解析用户查询、检索相关文献、构建知识图谱，并应用数据挖掘技术生成证据性见解。

Result: 通过AI4Health案例研究，LitChat展示了其快速导航大规模文献的能力，提供传统方法无法实现的数据支持见解。

Conclusion: LitChat为“大文学”时代的文献探索提供了一种高效、数据驱动的解决方案。

Abstract: We are living in an era of "big literature", where the volume of digital
scientific publications is growing exponentially. While offering new
opportunities, this also poses challenges for understanding literature
landscapes, as traditional manual reviewing is no longer feasible. Recent large
language models (LLMs) have shown strong capabilities for literature
comprehension, yet they are incapable of offering "comprehensive, objective,
open and transparent" views desired by systematic reviews due to their limited
context windows and trust issues like hallucinations. Here we present LitChat,
an end-to-end, interactive and conversational literature agent that augments
LLM agents with data-driven discovery tools to facilitate literature
exploration. LitChat automatically interprets user queries, retrieves relevant
sources, constructs knowledge graphs, and employs diverse data-mining
techniques to generate evidence-based insights addressing user needs. We
illustrate the effectiveness of LitChat via a case study on AI4Health,
highlighting its capacity to quickly navigate the users through large-scale
literature landscape with data-based evidence that is otherwise infeasible with
traditional means.

</details>


### [4] [Rethinking the Understanding Ability across LLMs through Mutual Information](https://arxiv.org/abs/2505.23790)
*Shaojie Wang,Sirui Ding,Na Zou*

Key words: 大语言模型,互信息,语言理解,信息保真度,模型评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种基于互信息的理论框架，通过分解句子和词级别的互信息，以及Fano不等式推导可计算下界，评估和改进大语言模型的语言理解能力。

Motivation: 当前评估大语言模型内在语言理解能力的方法有限，需更通用的理论框架。

Method: 通过句子和词级别互信息分解，理论推导可计算下界，并实现基于恢复性的实际评估。

Result: 编码器模型信息保真度高于解码器模型，后者表现出后期‘遗忘’现象；通过优化恢复性可提升模型理解能力。

Conclusion: 互信息可作为理解和改进语言模型能力的理论基础。

Abstract: Recent advances in large language models (LLMs) have revolutionized natural
language processing, yet evaluating their intrinsic linguistic understanding
remains challenging. Moving beyond specialized evaluation tasks, we propose an
information-theoretic framework grounded in mutual information (MI) to achieve
this. We formalize the understanding as MI between an input sentence and its
latent representation (sentence-level MI), measuring how effectively input
information is preserved in latent representation. Given that LLMs learn
embeddings for individual tokens, we decompose sentence-level MI into
token-level MI between tokens and sentence embeddings, establishing theoretical
bounds connecting these measures. Based on this foundation, we theoretically
derive a computable lower bound for token-level MI using Fano's inequality,
which directly relates to token-level recoverability-the ability to predict
original tokens from sentence embedding. We implement this recoverability task
to comparatively measure MI across different LLMs, revealing that encoder-only
models consistently maintain higher information fidelity than their
decoder-only counterparts, with the latter exhibiting a distinctive late-layer
"forgetting" pattern where mutual information is first enhanced and then
discarded. Moreover, fine-tuning to maximize token-level recoverability
consistently improves understanding ability of LLMs on tasks without
task-specific supervision, demonstrating that mutual information can serve as a
foundation for understanding and improving language model capabilities.

</details>


### [5] [R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.23794)
*Yuan Li,Qi Luo,Xiaonan Li,Bufan Li,Qinyuan Cheng,Bo Wang,Yining Zheng,Yuxin Wang,Zhangyue Yin,Xipeng Qiu*

Key words: 检索增强生成,强化学习,大语言模型,推理检索,奖励机制

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: R3-RAG 通过强化学习让大语言模型学会逐步推理和检索，以提升检索增强生成系统的效果，显著优于基线方法并可适应不同检索器。

Motivation: 当前基于密集检索的 RAG 系统受限参数规模和推理能力，而提示迭代方法受限于人工设计流程，因此提出 R3-RAG 以自动优化推理和检索过程。

Method: 分两阶段：冷启动学习迭代推理与检索交替方式，再用强化学习优化检索能力，设计答案正确性和文档相关性两种奖励机制。

Result: 实验表明 R3-RAG 显著优于基线方法，且能适配不同检索器。

Conclusion: R3-RAG 通过强化学习自动优化推理检索流程，提升回答准确性和文档相关性，具有广泛适应性。

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge with Large
Language Models (LLMs) to enhance factual correctness and mitigate
hallucination. However, dense retrievers often become the bottleneck of RAG
systems due to their limited parameters compared to LLMs and their inability to
perform step-by-step reasoning. While prompt-based iterative RAG attempts to
address these limitations, it is constrained by human-designed workflows. To
address these limitations, we propose $\textbf{R3-RAG}$, which uses
$\textbf{R}$einforcement learning to make the LLM learn how to
$\textbf{R}$eason and $\textbf{R}$etrieve step by step, thus retrieving
comprehensive external knowledge and leading to correct answers. R3-RAG is
divided into two stages. We first use cold start to make the model learn the
manner of iteratively interleaving reasoning and retrieval. Then we use
reinforcement learning to further harness its ability to better explore the
external retrieval environment. Specifically, we propose two rewards for
R3-RAG: 1) answer correctness for outcome reward, which judges whether the
trajectory leads to a correct answer; 2) relevance-based document verification
for process reward, encouraging the model to retrieve documents that are
relevant to the user question, through which we can let the model learn how to
iteratively reason and retrieve relevant documents to get the correct answer.
Experimental results show that R3-RAG significantly outperforms baselines and
can transfer well to different retrievers. We release R3-RAG at
https://github.com/Yuan-Li-FNLP/R3-RAG.

</details>


### [6] [Emergent LLM behaviors are observationally equivalent to data leakage](https://arxiv.org/abs/2505.23796)
*Christopher Barrie,Petter Törnberg*

Key words: 大语言模型, 命名游戏, 数据泄露, 预训练记忆, 社会科学模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文认为Ashery等人的结果更可能是数据泄露所致，而非模型自发形成语言规范。

Motivation: 质疑大语言模型（LLMs）在命名游戏中‘自发’形成语言规范的结论，提出可能是预训练数据泄露导致的结果。

Method: 通过多种分析证明LLMs识别协调游戏结构并回忆训练结果，而非真正‘涌现’新规范。

Result: 研究显示观察到的行为与训练数据记忆无区别。

Conclusion: 建议重新思考LLMs在社会科学模型中的应用，并提出可能的替代策略。

Abstract: Ashery et al. recently argue that large language models (LLMs), when paired
to play a classic "naming game," spontaneously develop linguistic conventions
reminiscent of human social norms. Here, we show that their results are better
explained by data leakage: the models simply reproduce conventions they already
encountered during pre-training. Despite the authors' mitigation measures, we
provide multiple analyses demonstrating that the LLMs recognize the structure
of the coordination game and recall its outcomes, rather than exhibit
"emergent" conventions. Consequently, the observed behaviors are
indistinguishable from memorization of the training corpus. We conclude by
pointing to potential alternative strategies and reflecting more generally on
the place of LLMs for social science models.

</details>


### [7] [Detection of Suicidal Risk on Social Media: A Hybrid Model](https://arxiv.org/abs/2505.23797)
*Zaihan Yang,Ryan Leonard,Hien Tran,Rory Driscoll,Chadbourne Davis*

Key words: 自杀风险, Reddit, RoBERTa, TF-IDF, PCA, 机器学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 利用Reddit帖子进行分类，通过结合RoBERTa、TF-IDF和PCA的混合模型提升自杀风险识别的准确性，F1分数达到0.7512。

Motivation: 自杀风险早期识别的重要性日益突出，迫切需要有效的工具来自动分类自杀风险级别。

Method: 提出RoBERTa-TF-IDF-PCA混合模型，结合深度上下文嵌入与统计词加权，并通过数据重采样和增强策略解决数据不平衡问题。

Result: 混合模型表现最佳，加权F1分数为0.7512，优于单独使用RoBERTa、BERT及其他传统分类器。

Conclusion: 混合模型在自杀风险分类任务中表现出更高的准确性和可靠性。

Abstract: Suicidal thoughts and behaviors are increasingly recognized as a critical
societal concern, highlighting the urgent need for effective tools to enable
early detection of suicidal risk. In this work, we develop robust machine
learning models that leverage Reddit posts to automatically classify them into
four distinct levels of suicide risk severity. We frame this as a multi-class
classification task and propose a RoBERTa-TF-IDF-PCA Hybrid model, integrating
the deep contextual embeddings from Robustly Optimized BERT Approach (RoBERTa),
a state-of-the-art deep learning transformer model, with the statistical
term-weighting of TF-IDF, further compressed with PCA, to boost the accuracy
and reliability of suicide risk assessment. To address data imbalance and
overfitting, we explore various data resampling techniques and data
augmentation strategies to enhance model generalization. Additionally, we
compare our model's performance against that of using RoBERTa only, the BERT
model and other traditional machine learning classifiers. Experimental results
demonstrate that the hybrid model can achieve improved performance, giving a
best weighted $F_{1}$ score of 0.7512.

</details>


### [8] [My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals](https://arxiv.org/abs/2505.23798)
*Jian Lan,Yifei Fu,Udo Schlegel,Gengyuan Zhang,Tanveer Hannan,Haokun Chen,Thomas Seidl*

Key words: 视觉语言模型, 社会偏见, 公平性, 后处理, 残差调整

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了大型视觉语言模型（VLMs）中的社会偏见问题，评估了模型生成反应中的偏见并提出了无需训练的模型无关后处理方法来减轻偏见。

Motivation: 社会偏见在VLMs中是一个关键问题，可能导致不公平和伦理问题，本研究旨在评估和缓解这些偏见。

Method: 使用PAIRS和SocialCounterfactuals数据集评估四种先进VLMs，通过选择性任务揭示偏见。提出一种后处理方法，通过调整隐藏层中的残差来减轻偏见。

Result: 研究发现VLMs存在性别和种族偏见，并提出了无需训练的模型无关方法，显著减少了偏见并提高了置信度可靠性。

Conclusion: 提出的后处理方法有效减轻了VLMs中的社会偏见，优于其他训练策略。

Abstract: Social bias is a critical issue in large vision-language models (VLMs), where
fairness- and ethics-related problems harm certain groups of people in society.
It is unknown to what extent VLMs yield social bias in generative responses. In
this study, we focus on evaluating and mitigating social bias on both the
model's response and probability distribution. To do so, we first evaluate four
state-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the
multiple-choice selection task. Surprisingly, we find that models suffer from
generating gender-biased or race-biased responses. We also observe that models
are prone to stating their responses are fair, but indeed having mis-calibrated
confidence levels towards particular social groups. While investigating why
VLMs are unfair in this study, we observe that VLMs' hidden layers exhibit
substantial fluctuations in fairness levels. Meanwhile, residuals in each layer
show mixed effects on fairness, with some contributing positively while some
lead to increased bias. Based on these findings, we propose a post-hoc method
for the inference stage to mitigate social bias, which is training-free and
model-agnostic. We achieve this by ablating bias-associated residuals while
amplifying fairness-associated residuals on model hidden layers during
inference. We demonstrate that our post-hoc method outperforms the competing
training strategies, helping VLMs have fairer responses and more reliable
confidence levels.

</details>


### [9] [Estimating LLM Consistency: A User Baseline vs Surrogate Metrics](https://arxiv.org/abs/2505.23799)
*Xiaoyuan Wu,Weiran Lin,Omer Akgul,Lujo Bauer*

Key words: 大语言模型,一致性,人类评估,logit,集成方法

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了大语言模型（LLM）生成文本的一致性问题，提出了一种基于logit的集成方法，并与现有方法进行了比较，发现人类评估仍是衡量一致性的重要参考。

Motivation: LLM易产生幻觉且对提示扰动敏感，导致生成文本不一致或不可靠。现有方法未能很好地近似人类对一致性的感知。

Method: 提出一种基于logit的集成方法，并通过用户研究（n=2,976）验证其有效性。

Result: 新方法与现有最佳性能指标相当，但人类评估仍是最可靠的衡量标准。

Conclusion: 建议更广泛地使用人类输入来评估LLM一致性，因为现有无人工评估方法仍不够理想。

Abstract: Large language models (LLMs) are prone to hallucinations and sensitive to
prompt perturbations, often resulting in inconsistent or unreliable generated
text. Different methods have been proposed to mitigate such hallucinations and
fragility -- one of them being measuring the consistency (the model's
confidence in the response, or likelihood of generating a similar response when
resampled) of LLM responses. In previous work, measuring consistency often
relied on the probability of a response appearing within a pool of resampled
responses, or internal states or logits of responses. However, it is not yet
clear how well these approaches approximate how humans perceive the consistency
of LLM responses. We performed a user study (n=2,976) and found current methods
typically do not approximate users' perceptions of LLM consistency very well.
We propose a logit-based ensemble method for estimating LLM consistency, and we
show that this method matches the performance of the best-performing existing
metric in estimating human ratings of LLM consistency. Our results suggest that
methods of estimating LLM consistency without human evaluation are sufficiently
imperfect that we suggest evaluation with human input be more broadly used.

</details>


### [10] [SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks](https://arxiv.org/abs/2505.23801)
*Sajid Hussain,Muhammad Sohail,Nauman Ali Khan*

Key words: 联邦学习, 自然语言处理, 语义感知, 资源高效, 客户端选择, 通信压缩

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为SEMFED的语义感知资源高效联邦学习框架，专门针对异质NLP任务设计，通过语义感知客户端选择、自适应模型架构和压缩技术，显著降低了通信成本，同时保持了高模型准确率。

Motivation: 联邦学习（FL）在NLP任务中存在语义异质性、词汇不匹配和设备资源限制等挑战，需要一种能高效处理这些问题的框架。

Method: SEMFED结合了语义感知客户端选择机制、自适应NLP模型架构和通信高效的语义特征压缩技术。

Result: 实验表明，SEMFED在降低80.5%通信成本的同时，模型准确率保持在98%以上，优于现有FL方法。

Conclusion: SEMFED能有效管理资源、语义分布各异的客户端环境，适用于实际联邦NLP部署。

Abstract: Background: Federated Learning (FL) has emerged as a promising paradigm for
training machine learning models while preserving data privacy. However,
applying FL to Natural Language Processing (NLP) tasks presents unique
challenges due to semantic heterogeneity across clients, vocabulary mismatches,
and varying resource constraints on edge devices. Objectives: This paper
introduces SEMFED, a novel semantic-aware resource-efficient federated learning
framework specifically designed for heterogeneous NLP tasks. Methods: SEMFED
incorporates three key innovations: (1) a semantic-aware client selection
mechanism that balances semantic diversity with resource constraints, (2)
adaptive NLP-specific model architectures tailored to device capabilities while
preserving semantic information, and (3) a communication-efficient semantic
feature compression technique that significantly reduces bandwidth
requirements. Results: Experimental results on various NLP classification tasks
demonstrate that SEMFED achieves an 80.5% reduction in communication costs
while maintaining model accuracy above 98%, outperforming state-of-the-art FL
approaches. Conclusion: SEMFED effectively manages heterogeneous client
environments with varying computational resources, network reliability, and
semantic data distributions, making it particularly suitable for real-world
federated NLP deployments.

</details>


### [11] [MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks](https://arxiv.org/abs/2505.23802)
*Suhana Bedi,Hejie Cui,Miguel Fuentes,Alyssa Unell,Michael Wornow,Juan M. Banda,Nikesh Kotecha,Timothy Keyes,Yifan Mai,Mert Oez,Hao Qiu,Shrey Jain,Leonardo Schettini,Mehr Kashyap,Jason Alan Fries,Akshay Swaminathan,Philip Chung,Fateme Nateghi,Asad Aali,Ashwin Nayak,Shivam Vedak,Sneha S. Jain,Birju Patel,Oluseyi Fayanju,Shreya Shah,Ethan Goh,Dong-han Yao,Brian Soetikno,Eduardo Reis,Sergios Gatidis,Vasu Divi,Robson Capasso,Rachna Saralkar,Chia-Chun Chiang,Jenelle Jindal,Tho Pham,Faraz Ghoddusi,Steven Lin,Albert S. Chiou,Christy Hong,Mohana Roy,Michael F. Gensheimer,Hinesh Patel,Kevin Schulman,Dev Dash,Danton Char,Lance Downing,Francois Grolleau,Kameron Black,Bethel Mieso,Aydin Zahedivash,Wen-wai Yim,Harshita Sharma,Tony Lee,Hannah Kirsch,Jennifer Lee,Nerissa Ambers,Carlene Lugtu,Aditya Sharma,Bilal Mawji,Alex Alekseyev,Vicky Zhou,Vikas Kakkar,Jarrod Helzer,Anurang Revri,Yair Bannett,Roxana Daneshjou,Jonathan Chen,Emily Alsentzer,Keith Morse,Nirmal Ravi,Nima Aghaeepour,Vanessa Kennedy,Akshay Chaudhari,Thomas Wang,Sanmi Koyejo,Matthew P. Lungren,Eric Horvitz,Percy Liang,Mike Pfeffer,Nigam H. Shah*

Key words: 大语言模型,医学评估,临床验证,基准测试,成本性能分析

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: MedHELM是一个评估大语言模型（LLM）在医学任务中表现的可扩展框架，包含临床验证分类、全面基准套件和改进的评估方法。研究揭示了不同LLM的性能差异，强调现实任务特异性评估的重要性。

Motivation: 现有对LLM在医学考试中的评估未能反映真实临床实践的复杂性和多样性，因此需要更全面的评估框架。

Method: 提出MedHELM框架，包含临床验证的分类法、35个基准测试（新旧结合），并采用LLM-jury评估方法和成本性能分析。

Result: 高级推理模型表现优异，但Claude 3.5 Sonnet在较低计算成本下取得可比性能。不同任务中模型表现差异显著，LLM-jury方法与临床评分一致性良好。

Conclusion: 研究表明现实任务特异性评估对医学LLM应用至关重要，并提供了一个开源框架支持此类评估。

Abstract: While large language models (LLMs) achieve near-perfect scores on medical
licensing exams, these evaluations inadequately reflect the complexity and
diversity of real-world clinical practice. We introduce MedHELM, an extensible
evaluation framework for assessing LLM performance for medical tasks with three
key contributions. First, a clinician-validated taxonomy spanning 5 categories,
22 subcategories, and 121 tasks developed with 29 clinicians. Second, a
comprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly
formulated) providing complete coverage of all categories and subcategories in
the taxonomy. Third, a systematic comparison of LLMs with improved evaluation
methods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9
frontier LLMs, using the 35 benchmarks, revealed significant performance
variation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%
win-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved
comparable results at 40% lower estimated computational cost. On a normalized
accuracy scale (0-1), most models performed strongly in Clinical Note
Generation (0.73-0.85) and Patient Communication & Education (0.78-0.83),
moderately in Medical Research Assistance (0.65-0.75), and generally lower in
Clinical Decision Support (0.56-0.72) and Administration & Workflow
(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with
clinician ratings (ICC = 0.47), surpassing both average clinician-clinician
agreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and
BERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top
models at lower estimated cost. These findings highlight the importance of
real-world, task-specific evaluation for medical use of LLMs and provides an
open source framework to enable this.

</details>


### [12] [Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies](https://arxiv.org/abs/2505.23804)
*Terrance Liu,Shuyi Wang,Daniel Preotiuc-Pietro,Yash Chandarana,Chirag Gupta*

Key words: text-to-SQL, 大型语言模型, 置信度校准, 子句频率, 多变量Platt缩放

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了为LLM生成的text-to-SQL解析结果提供校准置信度分数的问题，首次建立了后校准基准，提出了结合子句频率（SCF）和多变量Platt缩放（MPS）的方法，显著提升了校准和错误检测性能。

Motivation: 提升基于LLM的text-to-SQL系统的可信度，通过校准置信度分数准确反映查询的正确性。

Method: 引入了子句频率（SCF）评分，并结合多变量Platt缩放（MPS）方法进行校准。

Result: 在两大text-to-SQL数据集上验证，MPS+SCF组合方法优于传统Platt缩放，显著提升校准和错误检测效果。

Conclusion: 提出的MPS+SCF方法为LLM的text-to-SQL解析提供了更可靠的置信度校准。

Abstract: While large language models (LLMs) achieve strong performance on text-to-SQL
parsing, they sometimes exhibit unexpected failures in which they are
confidently incorrect. Building trustworthy text-to-SQL systems thus requires
eliciting reliable uncertainty measures from the LLM. In this paper, we study
the problem of providing a calibrated confidence score that conveys the
likelihood of an output query being correct. Our work is the first to establish
a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In
particular, we show that Platt scaling, a canonical method for calibration,
provides substantial improvements over directly using raw model output
probabilities as confidence scores. Furthermore, we propose a method for
text-to-SQL calibration that leverages the structured nature of SQL queries to
provide more granular signals of correctness, named "sub-clause frequency"
(SCF) scores. Using multivariate Platt scaling (MPS), our extension of the
canonical Platt scaling technique, we combine individual SCF scores into an
overall accurate and calibrated score. Empirical evaluation on two popular
text-to-SQL datasets shows that our approach of combining MPS and SCF yields
further improvements in calibration and the related task of error detection
over traditional Platt scaling.

</details>


### [13] [MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation](https://arxiv.org/abs/2505.23806)
*Sihyeon Lee,Hyunjoo Song,Jong-chan Lee,Yoon Jin Lee,Boram Lee,Hee-Eon Lim,Dongyeong Kim,Jinwook Seo,Bohyoung Kim*

Key words: 大语言模型, 临床隐私, 混合框架, 任务分解, 胰腺癌分期

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: MedOrchestra是一个混合框架，结合云端LLM的复杂任务分解能力和本地LLM的隐私保护优势，在临床任务中实现了更高的准确率。

Motivation: 解决临床环境中使用大语言模型时的隐私与性能权衡问题，云端LLM易泄露敏感数据，而本地LLM又难以处理复杂任务。

Method: 云端LLM分解任务并生成提示，本地LLM执行隐私敏感的子任务，结合临床指南和合成测试案例验证提示有效性。

Result: 在胰腺癌分期任务中，MedOrchestra在自由文本报告上达到70.21%准确率，结构化报告上达到85.42%，显著优于基线模型和临床专家。

Conclusion: MedOrchestra成功平衡了隐私与性能，为临床LLM部署提供了可行方案。

Abstract: Deploying large language models (LLMs) in clinical settings faces critical
trade-offs: cloud LLMs, with their extensive parameters and superior
performance, pose risks to sensitive clinical data privacy, while local LLMs
preserve privacy but often fail at complex clinical interpretation tasks. We
propose MedOrchestra, a hybrid framework where a cloud LLM decomposes complex
clinical tasks into manageable subtasks and prompt generation, while a local
LLM executes these subtasks in a privacy-preserving manner. Without accessing
clinical data, the cloud LLM generates and validates subtask prompts using
clinical guidelines and synthetic test cases. The local LLM executes subtasks
locally and synthesizes outputs generated by the cloud LLM. We evaluate
MedOrchestra on pancreatic cancer staging using 100 radiology reports under
NCCN guidelines. On free-text reports, MedOrchestra achieves 70.21% accuracy,
outperforming local model baselines (without guideline: 48.94%, with guideline:
56.59%) and board-certified clinicians (gastroenterologists: 59.57%, surgeons:
65.96%, radiologists: 55.32%). On structured reports, MedOrchestra reaches
85.42% accuracy, showing clear superiority across all settings.

</details>


### [14] [DLP: Dynamic Layerwise Pruning in Large Language Models](https://arxiv.org/abs/2505.23807)
*Yuli Chen,Bo Cheng,Jiale Han,Yingying Zhang,Yingting Li,Shuhao Zhang*

Key words: LLM, pruning, dynamic layerwise pruning, sparsity, efficiency

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种动态层剪枝方法(DLP)，通过结合模型权重和输入激活信息自适应地确定每层的重要性并分配剪枝率，显著提升了高稀疏度下LLM的性能表现。

Motivation: 主流剪枝技术通常采用统一的层剪枝策略，在高稀疏度下会导致性能显著下降。为解决这一问题，研究转向非均匀层剪枝，但现有方法依赖预定义值，性能次优。

Method: 提出动态层剪枝(DLP)，通过模型权重与输入激活信息的融合，自适应分配剪枝率，保留重要层信息。

Result: 在70%稀疏度下，DLP将LLaMA2-7B的困惑度降低7.79，平均准确率提升2.7%。且兼容多种LLM压缩技术。

Conclusion: DLP在高稀疏度下有效保持模型性能，兼容性强，代码已开源。

Abstract: Pruning has recently been widely adopted to reduce the parameter scale and
improve the inference efficiency of Large Language Models (LLMs). Mainstream
pruning techniques often rely on uniform layerwise pruning strategies, which
can lead to severe performance degradation at high sparsity levels. Recognizing
the varying contributions of different layers in LLMs, recent studies have
shifted their focus toward non-uniform layerwise pruning. However, these
approaches often rely on pre-defined values, which can result in suboptimal
performance. To overcome these limitations, we propose a novel method called
Dynamic Layerwise Pruning (DLP). This approach adaptively determines the
relative importance of each layer by integrating model weights with input
activation information, assigning pruning rates accordingly. Experimental
results show that DLP effectively preserves model performance at high sparsity
levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the
perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%
compared to state-of-the-art methods. Moreover, DLP is compatible with various
existing LLM compression techniques and can be seamlessly integrated into
Parameter-Efficient Fine-Tuning (PEFT). We release the code at
https://github.com/ironartisan/DLP to facilitate future research.

</details>


### [15] [DenseLoRA: Dense Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2505.23808)
*Lin Mu,Xiaoyu Wang,Li Ni,Yang Li,Zhize Wu,Peiquan Jin,Yiwen Zhang*

Key words: 低秩适应, 参数效率, 大语言模型, DenseLoRA, LoRA

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DenseLoRA 是一种改进 LoRA 的方法，通过使用密集低秩矩阵提高参数利用效率，并在性能上优于 LoRA。

Motivation: 传统 LoRA 方法的权重矩阵存在冗余，参数利用效率低，需要改进。

Method: 提出 DenseLoRA，通过单一编码器-解码器优化隐藏表示，使用密集低秩矩阵替代 LoRA 的双矩阵。

Result: DenseLoRA 在 LLaMA3-8B 上以仅 0.01% 可训练参数实现 83.8% 准确率，优于 LoRA 的 80.8%（0.70% 参数）。

Conclusion: DenseLoRA 显著提升了参数效率和模型性能，是 LoRA 的有效替代方案。

Abstract: Low-rank adaptation (LoRA) has been developed as an efficient approach for
adapting large language models (LLMs) by fine-tuning two low-rank matrices,
thereby reducing the number of trainable parameters. However, prior research
indicates that many of the weights in these matrices are redundant, leading to
inefficiencies in parameter utilization. To address this limitation, we
introduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances
parameter efficiency while achieving superior performance compared to LoRA.
DenseLoRA builds upon the concept of representation fine-tuning, incorporating
a single Encoder-Decoder to refine and compress hidden representations across
all adaptation layers before applying adaptation. Instead of relying on two
redundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense
low-rank matrix, improving parameter utilization and adaptation efficiency. We
evaluate DenseLoRA on various benchmarks, showing that it achieves 83.8%
accuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8%
accuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we
conduct extensive experiments to systematically assess the impact of
DenseLoRA's components on overall model performance. Code is available at
https://github.com/mulin-ahu/DenseLoRA.

</details>


### [16] [LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion](https://arxiv.org/abs/2505.23809)
*Haowei Yang,Haotian Lyu,Tianle Zhang,Dingzhou Wang,Yushang Zhao*

Key words: LLM, 营销文案, 多目标微调, 转化率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一个结合提示工程、多目标微调和后处理的框架，利用大语言模型生成既吸引人又能促进转化的营销文案。

Motivation: 随着电商竞争加剧，平衡创意内容与转化效果成为关键，需自动化生成高效营销文案。

Method: 通过情感调整、多样性增强和行动号召嵌入的多目标微调方法，结合提示工程和后处理。

Result: 离线评估和在线A/B测试显示，CTR提升12.5%，CVR提升8.3%，同时保持内容新颖性。

Conclusion: 该框架为自动化文案生成提供了实用解决方案，并为未来多模态、实时个性化研究指明方向。

Abstract: As e-commerce competition intensifies, balancing creative content with
conversion effectiveness becomes critical. Leveraging LLMs' language generation
capabilities, we propose a framework that integrates prompt engineering,
multi-objective fine-tuning, and post-processing to generate marketing copy
that is both engaging and conversion-driven. Our fine-tuning method combines
sentiment adjustment, diversity enhancement, and CTA embedding. Through offline
evaluations and online A/B tests across categories, our approach achieves a
12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content
novelty. This provides a practical solution for automated copy generation and
suggests paths for future multimodal, real-time personalization.

</details>


### [17] [MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation](https://arxiv.org/abs/2505.23810)
*Chenghao Yang,Yinbo Luo,Zhoufutu Wen,Qi Chu,Tao Gong,Longxiang Liu,Kaiyuan Zhang,Jianpeng Jiao,Ge Zhang,Wenhao Huang,Nenghai Yu*

Key words: 大型语言模型,多回合对话,基准测试,鲁棒性,注意力机制

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: MARS-Bench是一个多回合现实场景对话基准，用于评估大型语言模型（LLMs）在长复杂对话中的弱点。实验显示闭源LLMs优于开源模型，显式推理可提升LLMs的鲁棒性，但LLMs在处理动机转移和跨回合依赖时仍面临挑战。

Motivation: 现有基准无法全面反映LLMs在长复杂对话中的弱点，尤其是处理动机转移和跨回合依赖时的表现，因此设计了MARS-Bench来填补这一空白。

Method: 基于实况文本解说的MARS-Bench评估多回合对话的三个关键方面：超多回合、交互式多回合和跨回合任务。通过Qwen2.5-7B-Instruction的注意力可视化实验，分析LLMs性能下降的机制。

Result: 闭源LLMs显著优于开源模型；显式推理提升了LLMs的鲁棒性；LLMs在处理动机转移和复杂跨回合依赖时表现不佳；注意力机制中特殊标记导致性能下降。

Conclusion: MARS-Bench揭示了LLMs在长复杂对话中的局限性，显式推理是提升性能的关键，但模型仍需改进以更好地处理动机转移和跨回合依赖。

Abstract: Large Language Models (\textbf{LLMs}), e.g. ChatGPT, have been widely adopted
in real-world dialogue applications. However, LLMs' robustness, especially in
handling long complex dialogue sessions, including frequent motivation
transfer, sophisticated cross-turn dependency, is criticized all along.
Nevertheless, no existing benchmarks can fully reflect these weaknesses. We
present \textbf{MARS-Bench}, a \textbf{M}ulti-turn \textbf{A}thletic
\textbf{R}eal-world \textbf{S}cenario Dialogue \textbf{Bench}mark, designed to
remedy the gap. MARS-Bench is constructed from play-by-play text commentary so
to feature realistic dialogues specifically designed to evaluate three critical
aspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn,
and Cross-turn Tasks. Extensive experiments on MARS-Bench also reveal that
closed-source LLMs significantly outperform open-source alternatives, explicit
reasoning significantly boosts LLMs' robustness on handling long complex
dialogue sessions, and LLMs indeed face significant challenges when handling
motivation transfer and sophisticated cross-turn dependency. Moreover, we
provide mechanistic interpretability on how attention sinks due to special
tokens lead to LLMs' performance degradation when handling long complex
dialogue sessions based on attention visualization experiment in
Qwen2.5-7B-Instruction.

</details>


### [18] [LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions](https://arxiv.org/abs/2505.23811)
*Hadi Askari,Shivanshu Gupta,Fei Wang,Anshuman Chhabra,Muhao Chen*

Key words: 预训练大语言模型,层训练质量,影响函数,数据驱动,LoRA-MoE

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出LayerIF框架，通过影响函数量化预训练大语言模型各层在特定任务中的训练质量，改进任务性能。

Motivation: 现有方法主要依赖模型层面的启发式方法（如频谱统计、异常检测或均匀分配），忽略了数据的影响，限制了模型在下游任务中的表现。

Method: 提出LayerIF框架，利用影响函数隔离各层梯度并计算验证损失对训练样本的敏感性，从而量化各层重要性。

Result: 实验表明，LayerIF方法能生成任务特定的层重要性估计，并在LoRA-MoE架构和LLM剪枝等应用中提升任务性能。

Conclusion: LayerIF提供了一种数据驱动的层训练质量评估方法，显著提升了大语言模型在下游任务中的表现。

Abstract: Pretrained Large Language Models (LLMs) achieve strong performance across a
wide range of tasks, yet exhibit substantial variability in the various layers'
training quality with respect to specific downstream applications, limiting
their downstream performance.It is therefore critical to estimate layer-wise
training quality in a manner that accounts for both model architecture and
training data. However, existing approaches predominantly rely on model-centric
heuristics (such as spectral statistics, outlier detection, or uniform
allocation) while overlooking the influence of data. To address these
limitations, we propose LayerIF, a data-driven framework that leverages
Influence Functions to quantify the training quality of individual layers in a
principled and task-sensitive manner. By isolating each layer's gradients and
measuring the sensitivity of the validation loss to training examples by
computing layer-wise influences, we derive data-driven estimates of layer
importance. Notably, our method produces task-specific layer importance
estimates for the same LLM, revealing how layers specialize for different
test-time evaluation tasks. We demonstrate the utility of our scores by
leveraging them for two downstream applications: (a) expert allocation in
LoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM
pruning. Experiments across multiple LLM architectures demonstrate that our
model-agnostic, influence-guided allocation leads to consistent gains in task
performance.

</details>


### [19] [Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance Detection in Misinformative Social Media Content](https://arxiv.org/abs/2505.23812)
*Lata Pangtey,Mohammad Zia Ur Rehman,Prasad Chaudhari,Shubhi Bansal,Nagendra Kumar*

Key words: 立场检测,双跨注意力,情感分析,标签融合,虚假信息

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种名为SPLAENet的新方法，通过双跨注意力和分层注意力网络结合情感分析，显著提升了社交媒体中立场检测的准确性。

Motivation: 社交媒体中用户生成内容的大量增长导致虚假信息传播和意见极化，需要更有效的方法检测立场以分析偏见和对抗虚假信息。

Method: 采用双跨注意力机制和分层注意力网络捕捉文本间关系；结合情感分析和标签融合技术优化特征与标签的对齐。

Result: 在多个数据集上表现优异，平均准确率和F1分数分别提升了8.92%和17.36%（RumourEval）、7.02%和10.92%（SemEval）、10.03%和11.18%（P-stance）。

Conclusion: SPLAENet在社交媒体虚假信息背景下显著提升了立场检测的准确性，验证了其有效性。

Abstract: The rapid evolution of social media has generated an overwhelming volume of
user-generated content, conveying implicit opinions and contributing to the
spread of misinformation. The method aims to enhance the detection of stance
where misinformation can polarize user opinions. Stance detection has emerged
as a crucial approach to effectively analyze underlying biases in shared
information and combating misinformation. This paper proposes a novel method
for \textbf{S}tance \textbf{P}rediction through a \textbf{L}abel-fused dual
cross-\textbf{A}ttentive \textbf{E}motion-aware neural \textbf{Net}work
(SPLAENet) in misinformative social media user-generated content. The proposed
method employs a dual cross-attention mechanism and a hierarchical attention
network to capture inter and intra-relationships by focusing on the relevant
parts of source text in the context of reply text and vice versa. We
incorporate emotions to effectively distinguish between different stance
categories by leveraging the emotional alignment or divergence between the
texts. We also employ label fusion that uses distance-metric learning to align
extracted features with stance labels, improving the method's ability to
accurately distinguish between stances. Extensive experiments demonstrate the
significant improvements achieved by SPLAENet over existing state-of-the-art
methods. SPLAENet demonstrates an average gain of 8.92\% in accuracy and
17.36\% in F1-score on the RumourEval dataset. On the SemEval dataset, it
achieves average gains of 7.02\% in accuracy and 10.92\% in F1-score. On the
P-stance dataset, it demonstrates average gains of 10.03\% in accuracy and
11.18\% in F1-score. These results validate the effectiveness of the proposed
method for stance detection in the context of misinformative social media
content.

</details>


### [20] [Aligning LLMs by Predicting Preferences from User Writing Samples](https://arxiv.org/abs/2505.23815)
*Stéphane Aroca-Ouellette,Natalie Mackraz,Barry-John Theobald,Katherine Metcalf*

Key words: LLM, 偏好推断, PROSE, 个性化交互

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: PROSE方法通过迭代优化和验证提升了从用户写作样本中推断个性化偏好的精度，相比现有方法CIPHER提升了33%的生成质量。

Motivation: 现有方法生成的偏好描述过于通用，无法捕捉人类偏好的独特性和个性化，需要更精确的推断方法。

Method: PROSE结合了迭代优化推断偏好和多样本验证，使用多个LLM（如Qwen2.5、GPT-mini等）进行评测。

Result: PROSE显著提升了偏好推断的准确性，生成质量比CIPHER高33%，与ICL结合时还可额外提升9%。

Conclusion: PROSE是一种有效的偏好推断方法，能显著提升写作代理的个性化生成质量。

Abstract: Accommodating human preferences is essential for creating aligned LLM agents
that deliver personalized and effective interactions. Recent work has shown the
potential for LLMs acting as writing agents to infer a description of user
preferences. Agent alignment then comes from conditioning on the inferred
preference description. However, existing methods often produce generic
preference descriptions that fail to capture the unique and individualized
nature of human preferences. This paper introduces PROSE, a method designed to
enhance the precision of preference descriptions inferred from user writing
samples. PROSE incorporates two key elements: (1) iterative refinement of
inferred preferences, and (2) verification of inferred preferences across
multiple user writing samples. We evaluate PROSE with several LLMs (i.e.,
Qwen2.5 7B and 72B Instruct, GPT-mini, and GPT-4o) on a summarization and an
email writing task. We find that PROSE more accurately infers nuanced human
preferences, improving the quality of the writing agent's generations over
CIPHER (a state-of-the-art method for inferring preferences) by 33\%. Lastly,
we demonstrate that ICL and PROSE are complementary methods, and combining them
provides up to a 9\% improvement over ICL alone.

</details>


### [21] [A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs](https://arxiv.org/abs/2505.23816)
*Trenton Chang,Tobias Schnabel,Adith Swaminathan,Jenna Wiens*

Key words: 大型语言模型, 可操控性, 多维度目标空间, 文本改写, 对齐策略

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 尽管大型语言模型（LLM）在推理和指令遵循方面表现优异，但其是否能可靠输出广泛符合用户目标的“可操控性”仍不明确。本文通过多维度目标空间框架评估LLM的可操控性，发现现有模型存在覆盖不足、校准偏差和副作用等问题，现有干预措施效果有限。

Motivation: 明确LLM在广泛用户目标下的可操控性，识别其在覆盖、校准和副作用等方面的潜在问题，并评估现有干预措施的有效性。

Method: 提出基于多维度目标空间的框架，将用户目标和LLM输出建模为向量（维度对应文本属性），应用于文本改写任务，并测试多种干预措施（如提示工程、最佳采样和强化学习微调）。

Result: 研究发现当前LLM在可操控性上表现不佳，副作用持续存在，现有干预措施效果有限且无法完全消除问题。

Conclusion: 即使强大的LLM仍难以实现良好的可操控性，现有对齐策略可能不足，需进一步研究。

Abstract: Despite advances in large language models (LLMs) on reasoning and
instruction-following benchmarks, it remains unclear whether they can reliably
produce outputs aligned with a broad variety of user goals, a concept we refer
to as steerability. The abundance of methods proposed to modify LLM behavior
makes it unclear whether current LLMs are already steerable, or require further
intervention. In particular, LLMs may exhibit (i) poor coverage, where rare
user goals are underrepresented; (ii) miscalibration, where models overshoot
requests; and (iii) side effects, where changes to one dimension of text
inadvertently affect others. To systematically evaluate these failures, we
introduce a framework based on a multi-dimensional goal space that models user
goals and LLM outputs as vectors with dimensions corresponding to text
attributes (e.g., reading difficulty). Applied to a text-rewriting task, we
find that current LLMs struggle with steerability, as side effects are
persistent. Interventions to improve steerability, such as prompt engineering,
best-of-$N$ sampling, and reinforcement learning fine-tuning, have varying
effectiveness, yet side effects remain problematic. Our findings suggest that
even strong LLMs struggle with steerability, and existing alignment strategies
may be insufficient. We open-source our steerability evaluation framework at
https://github.com/MLD3/steerability.

</details>


### [22] [Ratas framework: A comprehensive genai-based approach to rubric-based marking of real-world textual exams](https://arxiv.org/abs/2505.23818)
*Masoud Safilian,Amin Beheshti,Stephen Elbourn*

Key words: 自动评分,生成式AI,评分框架,教育技术,可解释性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了RATAS框架，利用生成式AI进行基于评分的自动答案评分，支持多科目评估并提供可解释的评分理由。实验表明其高可靠性和准确性。

Motivation: 现有自动评分方法受限于特定考试格式、缺乏评分可解释性，且难以跨科目应用。RATAS旨在解决这些限制。

Method: 采用基于评分的树状结构方法，结合生成式AI模型，设计数学框架处理复杂考试结构。

Result: 在真实课程数据上验证，RATAS表现出高评分可靠性和准确性，并能生成透明反馈。

Conclusion: RATAS为自动评分提供了可扩展、可解释的解决方案，适用于多样化评估场景。

Abstract: Automated answer grading is a critical challenge in educational technology,
with the potential to streamline assessment processes, ensure grading
consistency, and provide timely feedback to students. However, existing
approaches are often constrained to specific exam formats, lack
interpretability in score assignment, and struggle with real-world
applicability across diverse subjects and assessment types. To address these
limitations, we introduce RATAS (Rubric Automated Tree-based Answer Scoring), a
novel framework that leverages state-of-the-art generative AI models for
rubric-based grading of textual responses. RATAS is designed to support a wide
range of grading rubrics, enable subject-agnostic evaluation, and generate
structured, explainable rationales for assigned scores. We formalize the
automatic grading task through a mathematical framework tailored to
rubric-based assessment and present an architecture capable of handling
complex, real-world exam structures. To rigorously evaluate our approach, we
construct a unique, contextualized dataset derived from real-world
project-based courses, encompassing diverse response formats and varying levels
of complexity. Empirical results demonstrate that RATAS achieves high
reliability and accuracy in automated grading while providing interpretable
feedback that enhances transparency for both students and nstructors.

</details>


### [23] [Arbiters of Ambivalence: Challenges of Using LLMs in No-Consensus Tasks](https://arxiv.org/abs/2505.23820)
*Bhaktipriya Radharapu,Manon Revel,Megan Ung,Sebastian Ruder,Adina Williams*

Key words: LLMs, 对齐, 无共识, 偏见, 人类分歧

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究发现LLMs在无共识话题中作为法官或辩论者时会倾向某一立场，无法完全复现人类分歧，需更复杂对齐方法。

Motivation: 探讨LLMs能否在人类存在分歧的模糊场景中复现人类判断与偏好，尤其在作为答案生成器、法官和辩论者时的表现差异。

Method: 通过构建“无共识”基准测试，收集多样化的中立场景数据，评估LLMs在生成答案、评判和辩论中的行为。

Result: LLMs在开放式答案生成中表现细腻，但作为法官或辩论者时会对无共识话题偏向某一立场。

Conclusion: LLMs无法完全捕捉人类分歧，需开发无需人类监督的更复杂对齐方法。

Abstract: The increasing use of LLMs as substitutes for humans in ``aligning'' LLMs has
raised questions about their ability to replicate human judgments and
preferences, especially in ambivalent scenarios where humans disagree. This
study examines the biases and limitations of LLMs in three roles: answer
generator, judge, and debater. These roles loosely correspond to previously
described alignment frameworks: preference alignment (judge) and scalable
oversight (debater), with the answer generator reflecting the typical setting
with user interactions. We develop a ``no-consensus'' benchmark by curating
examples that encompass a variety of a priori ambivalent scenarios, each
presenting two possible stances. Our results show that while LLMs can provide
nuanced assessments when generating open-ended answers, they tend to take a
stance on no-consensus topics when employed as judges or debaters. These
findings underscore the necessity for more sophisticated methods for aligning
LLMs without human oversight, highlighting that LLMs cannot fully capture human
disagreement even on topics where humans themselves are divided.

</details>


### [24] [Speech as a Multimodal Digital Phenotype for Multi-Task LLM-based Mental Health Prediction](https://arxiv.org/abs/2505.23822)
*Mai Ali,Christopher Lucasius,Tanmay P. Patel,Madison Aitken,Jacob Vorstman,Peter Szatmari,Marco Battaglia,Deepa Kundur*

Key words: 抑郁症检测, 多模态学习, 多任务学习, 纵向分析, 语音分析

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种基于多模态（语音、声学特征和声学生物标志物）和多任务学习（抑郁、自杀意念和睡眠障碍）的抑郁症检测方法，通过纵向分析提升预测准确率，在Depression Early Warning数据集上达到了70.8%的平衡准确率。

Motivation: 抑郁症检测通常仅使用单一模态（如语音），而忽略了多模态数据的潜力。研究旨在整合语音衍生的文本、声学特征和声学生物标志物，结合多任务学习和纵向分析，以更全面地预测抑郁症及其共病症。

Method: 采用大型语言模型架构，整合语音、声学特征和声学生物标志物三种模态数据。通过多任务学习同时预测抑郁症、自杀意念和睡眠障碍，并引入纵向分析策略以捕捉多次临床交互中的时序变化。

Result: 在Depression Early Warning数据集上，提出的三模态、纵向多任务学习方法取得了70.8%的平衡准确率，优于单模态、单任务和非纵向方法。

Conclusion: 多模态和多任务学习的结合显著提升了抑郁症检测的准确性，纵向分析进一步增强了模型的预测能力。

Abstract: Speech is a noninvasive digital phenotype that can offer valuable insights
into mental health conditions, but it is often treated as a single modality. In
contrast, we propose the treatment of patient speech data as a trimodal
multimedia data source for depression detection. This study explores the
potential of large language model-based architectures for speech-based
depression prediction in a multimodal regime that integrates speech-derived
text, acoustic landmarks, and vocal biomarkers. Adolescent depression presents
a significant challenge and is often comorbid with multiple disorders, such as
suicidal ideation and sleep disturbances. This presents an additional
opportunity to integrate multi-task learning (MTL) into our study by
simultaneously predicting depression, suicidal ideation, and sleep disturbances
using the multimodal formulation. We also propose a longitudinal analysis
strategy that models temporal changes across multiple clinical interactions,
allowing for a comprehensive understanding of the conditions' progression. Our
proposed approach, featuring trimodal, longitudinal MTL is evaluated on the
Depression Early Warning dataset. It achieves a balanced accuracy of 70.8%,
which is higher than each of the unimodal, single-task, and non-longitudinal
methods.

</details>


### [25] [RAGPPI: RAG Benchmark for Protein-Protein Interactions in Drug Discovery](https://arxiv.org/abs/2505.23823)
*Youngseung Jeon,Ziwen Li,Thomas Li,JiaSyuan Chang,Morteza Ziyadi,Xiang 'Anthony' Chen*

Key words: PPIs, Target ID, LLMs, RAG, 基准测试

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文介绍了一个名为RAGPPI的基准测试，用于评估蛋白质-蛋白质相互作用（PPIs）的生物学影响，以支持药物开发中的靶点识别（Target ID）。

Motivation: 现有的PPI生物学影响识别缺乏基准测试，而LLMs和RAG框架的支持又亟需相关评测工具。

Method: 通过专家访谈确定基准数据集的标准，构建了500个黄金标准QA对和3,720个白银标准QA对，并开发了自评估LLM模型。

Result: 创建了包含4,420个QA对的RAGPPI基准测试，为药物发现中的RAG系统提供了评测资源。

Conclusion: RAGPPI将成为支持药物发现研究的重要工具。

Abstract: Retrieving the biological impacts of protein-protein interactions (PPIs) is
essential for target identification (Target ID) in drug development. Given the
vast number of proteins involved, this process remains time-consuming and
challenging. Large Language Models (LLMs) and Retrieval-Augmented Generation
(RAG) frameworks have supported Target ID; however, no benchmark currently
exists for identifying the biological impacts of PPIs. To bridge this gap, we
introduce the RAG Benchmark for PPIs (RAGPPI), a factual question-answer
benchmark of 4,420 question-answer pairs that focus on the potential biological
impacts of PPIs. Through interviews with experts, we identified criteria for a
benchmark dataset, such as a type of QA and source. We built a gold-standard
dataset (500 QA pairs) through expert-driven data annotation. We developed an
ensemble auto-evaluation LLM that reflected expert labeling characteristics,
which facilitates the construction of a silver-standard dataset (3,720 QA
pairs). We are committed to maintaining RAGPPI as a resource to support the
research community in advancing RAG systems for drug discovery QA solutions.

</details>


### [26] [Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation](https://arxiv.org/abs/2505.23824)
*Tianmai M. Zhang,Neil F. Abernethy*

Key words: 大型语言模型、同行评审、论文质量检查、自动评估、科学出版

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨了利用大型语言模型作为论文质量检查工具的可行性，提出了一种可扩展的自动评估框架，并验证了不同模型在识别关键错误和问题上的性能与成本。

Motivation: 由于招募领域专家进行人工评估困难，研究者希望通过大型语言模型辅助科学出版的同行评审过程，提升效率和覆盖范围。

Method: 提出了几种基线方法和一个可扩展的自动评估框架，利用arXiv撤稿论文验证了不同推理型大型语言模型的性能与API成本。

Result: OpenAI的o3模型表现最佳，而o4-mini则是成本效益最高的选择。

Conclusion: 该研究为基于文档的科学理解和推理提供了参考，并为未来应用奠定了基础。

Abstract: Recent advancements in large language models have sparked interest in
utilizing them to assist the peer review process of scientific publication.
Instead of having AI models generate reviews in the same way as human
reviewers, we propose adopting them as manuscript quality checkers. We
introduce several baseline approaches and an extendable automatic evaluation
framework using top LLMs as judges to tackle the difficulty of recruiting
domain experts for manual evaluation. Utilizing papers withdrawn from arXiv, we
validated our proposed methods with several leading reasoning LLMs from
different providers and assessed their performance and API costs for
identifying critical errors and unsoundness problems. The OpenAI o3 model
performed the best, while o4-mini was the most cost-effective one in our
evaluation. This paper provides insights into document-based scientific
understanding/reasoning and lays the foundation for future applications.

</details>


### [27] [ValueSim: Generating Backstories to Model Individual Value Systems](https://arxiv.org/abs/2505.23827)
*Bangde Du,Ziyi Ye,Zhijing Wu,Jankowska Monika,Shuqi Zhu,Qingyao Ai,Yujia Zhou,Yiqun Liu*

Key words: 大型语言模型, 价值对齐, 个体价值模拟, 认知-情感人格系统, 叙事生成

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ValueSim框架通过生成反映个人经历和背景的虚构个人故事来模拟个体价值系统，相比现有方法在准确性上有显著提升，并能随时间优化模拟效果。

Motivation: 当前的大型语言模型对齐技术主要关注广泛伦理和帮助性，但缺乏对个体价值系统的模拟。ValueSim旨在填补这一空白，通过模拟个体价值来提升模型的人类价值对齐能力。

Method: 提出ValueSim框架，将结构化个体数据转换为叙事性个人故事，并采用基于认知-情感人格系统的多模块架构来模拟个体价值。

Result: 在自建基准测试中，ValueSim的top-1准确率比检索增强生成方法提高了10%以上，且随着用户交互历史的增加，性能进一步提升。

Conclusion: ValueSim通过模拟个体价值系统，显著提升了模型在人类价值对齐上的表现，展示了随时间优化的潜力。

Abstract: As Large Language Models (LLMs) continue to exhibit increasingly human-like
capabilities, aligning them with human values has become critically important.
Contemporary advanced techniques, such as prompt learning and reinforcement
learning, are being deployed to better align LLMs with human values. However,
while these approaches address broad ethical considerations and helpfulness,
they rarely focus on simulating individualized human value systems. To address
this gap, we present ValueSim, a framework that simulates individual values
through the generation of personal backstories reflecting past experiences and
demographic information. ValueSim converts structured individual data into
narrative backstories and employs a multi-module architecture inspired by the
Cognitive-Affective Personality System to simulate individual values based on
these narratives. Testing ValueSim on a self-constructed benchmark derived from
the World Values Survey demonstrates an improvement in top-1 accuracy by over
10% compared to retrieval-augmented generation methods. Further analysis
reveals that performance enhances as additional user interaction history
becomes available, indicating the model's ability to refine its persona
simulation capabilities over time.

</details>


### [28] [BiasFilter: An Inference-Time Debiasing Framework for Large Language Models](https://arxiv.org/abs/2505.23829)
*Xiaoqing Cheng,Ruizhe Chen,Hongying Zan,Yuxiang Jia,Min Peng*

Key words: 大语言模型，社会偏见，去偏，实时过滤，公平性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: BiasFilter 是一个在推理阶段去偏的大语言模型框架，通过实时过滤生成内容来提升公平性，无需重训练或修改模型参数。

Motivation: 现有的大语言模型去偏方法成本高、效果有限且难以扩展，BiasFilter 提出了一个高效且模型无关的解决方案。

Method: BiasFilter 在每个生成步骤中评估和过滤输出，利用公平性奖励信号选择和丢弃候选段落。开发了一个公平偏好数据集并训练了隐式奖励模型来支持这一过程。

Result: 实验表明，BiasFilter 在保持生成质量的同时显著降低了多种大语言模型的社会偏见。

Conclusion: BiasFilter 提出了一种高效且通用的去偏解决方案，适用于各种大语言模型和开发生成任务。

Abstract: Mitigating social bias in large language models (LLMs) has become an
increasingly important research objective. However, existing debiasing methods
often incur high human and computational costs, exhibit limited effectiveness,
and struggle to scale to larger models and open-ended generation tasks. To
address these limitations, this paper proposes BiasFilter, a model-agnostic,
inference-time debiasing framework that integrates seamlessly with both
open-source and API-based LLMs. Instead of relying on retraining with balanced
data or modifying model parameters, BiasFilter enforces fairness by filtering
generation outputs in real time. Specifically, it periodically evaluates
intermediate outputs every few tokens, maintains an active set of candidate
continuations, and incrementally completes generation by discarding low-reward
segments based on a fairness reward signal. To support this process, we
construct a fairness preference dataset and train an implicit reward model to
assess token-level fairness in generated responses. Extensive experiments
demonstrate that BiasFilter effectively mitigates social bias across a range of
LLMs while preserving overall generation quality.

</details>


### [29] [EvoMoE: Expert Evolution in Mixture of Experts for Multimodal Large Language Models](https://arxiv.org/abs/2505.23830)
*Linglin Jing,Yuting Gao,Zhigang Wang,Wang Lan,Yiwen Tang,Wenhai Wang,Kaipeng Zhang,Qingpei Guo*

Key words: Mixture of Experts, multi-modal large language models, expert evolution, dynamic routing, hypernetworks

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: EvoMoE is a Mixture of Experts (MoE) tuning framework for multi-modal large language models (MLLMs) that tackles expert uniformity and router rigidity through expert evolution and a dynamic token-aware router.

Motivation: Existing multi-modal MoE tuning methods suffer from expert uniformity (homogenized experts) and router rigidity (static routing lacking modality distinction), limiting performance. EvoMoE addresses these challenges to enhance MLLM capabilities.

Method: EvoMoE introduces expert evolution (progressive expert diversification from a single trainable expert) and a Dynamic Token-aware Router (DTR) that uses hypernetworks for modality- and token-specific routing.

Result: EvoMoE outperforms sparse MLLMs on benchmarks like MME, MMBench, TextVQA, and POPE, demonstrating its effectiveness in mitigating expert homogenization and rigid routing.

Conclusion: EvoMoE significantly improves MLLM performance by dynamically diversifying experts and adapting routing to token modality and value.

Abstract: Recent advancements have shown that the Mixture of Experts (MoE) approach
significantly enhances the capacity of large language models (LLMs) and
improves performance on downstream tasks. Building on these promising results,
multi-modal large language models (MLLMs) have increasingly adopted MoE
techniques. However, existing multi-modal MoE tuning methods typically face two
key challenges: expert uniformity and router rigidity. Expert uniformity occurs
because MoE experts are often initialized by simply replicating the FFN
parameters from LLMs, leading to homogenized expert functions and weakening the
intended diversification of the MoE architecture. Meanwhile, router rigidity
stems from the prevalent use of static linear routers for expert selection,
which fail to distinguish between visual and textual tokens, resulting in
similar expert distributions for image and text. To address these limitations,
we propose EvoMoE, an innovative MoE tuning framework. EvoMoE introduces a
meticulously designed expert initialization strategy that progressively evolves
multiple robust experts from a single trainable expert, a process termed expert
evolution that specifically targets severe expert homogenization. Furthermore,
we introduce the Dynamic Token-aware Router (DTR), a novel routing mechanism
that allocates input tokens to appropriate experts based on their modality and
intrinsic token values. This dynamic routing is facilitated by hypernetworks,
which dynamically generate routing weights tailored for each individual token.
Extensive experiments demonstrate that EvoMoE significantly outperforms other
sparse MLLMs across a variety of multi-modal benchmarks, including MME,
MMBench, TextVQA, and POPE. Our results highlight the effectiveness of EvoMoE
in enhancing the performance of MLLMs by addressing the critical issues of
expert uniformity and router rigidity.

</details>


### [30] [ICH-Qwen: A Large Language Model Towards Chinese Intangible Cultural Heritage](https://arxiv.org/abs/2505.23831)
*Wenhao Ye,Tiansheng Zheng,Yue Qi,Wenhua Zhao,Xiyu Wang,Xue Zhao,Jiacheng He,Yaya Zheng,Dongbo Wang*

Key words: 非物质文化遗产, 大型语言模型, 数字人文, 文化保护, ICH-Qwen

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究开发了一个名为ICH-Qwen的大型语言模型，用于保护和传播中国的非物质文化遗产（ICH），通过技术手段解决现代化带来的传承挑战。

Motivation: 中国非物质文化遗产面临现代化威胁，需要技术手段保护和传承。中国拥有众多UNESCO非物质文化遗产项目，凸显了保护的重要性。

Method: 利用开源中文ICH数据，结合大型语言模型的自然语言理解和知识推理能力，通过合成数据和微调技术开发ICH-Qwen模型。

Result: 实验证明ICH-Qwen在ICH领域任务中表现高效，为ICH的保护和传播提供智能解决方案。

Conclusion: ICH-Qwen不仅支持ICH的可持续发展，还为数字人文学科研究开辟了新途径。

Abstract: The intangible cultural heritage (ICH) of China, a cultural asset transmitted
across generations by various ethnic groups, serves as a significant testament
to the evolution of human civilization and holds irreplaceable value for the
preservation of historical lineage and the enhancement of cultural
self-confidence. However, the rapid pace of modernization poses formidable
challenges to ICH, including threats damage, disappearance and discontinuity of
inheritance. China has the highest number of items on the UNESCO Intangible
Cultural Heritage List, which is indicative of the nation's abundant cultural
resources and emphasises the pressing need for ICH preservation. In recent
years, the rapid advancements in large language modelling have provided a novel
technological approach for the preservation and dissemination of ICH. This
study utilises a substantial corpus of open-source Chinese ICH data to develop
a large language model, ICH-Qwen, for the ICH domain. The model employs natural
language understanding and knowledge reasoning capabilities of large language
models, augmented with synthetic data and fine-tuning techniques. The
experimental results demonstrate the efficacy of ICH-Qwen in executing tasks
specific to the ICH domain. It is anticipated that the model will provide
intelligent solutions for the protection, inheritance and dissemination of
intangible cultural heritage, as well as new theoretical and practical
references for the sustainable development of intangible cultural heritage.
Furthermore, it is expected that the study will open up new paths for digital
humanities research.

</details>


### [31] [LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation](https://arxiv.org/abs/2505.23832)
*Chaeeun Kim,Jinu Lee,Wonseok Hwang*

Key words: 法律案例检索、韩国基准、法律元素推理、约束解码、泛化能力

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了LEGAR BENCH，首个大规模韩语法律案例检索基准，涵盖120万案例中的411种犯罪类型；并开发了LegalSearchLM模型，通过法律元素推理和约束解码提升检索性能，效果优于基线6-20%，且对域外案例有15%的泛化优势。

Motivation: 现有法律案例检索（LCR）研究局限于小规模数据集和有限的犯罪类型，且依赖的嵌入或词汇匹配方法导致表征不足和无关匹配。需解决这些问题以反映真实法律检索的复杂性。

Method: 提出LEGAR BENCH基准（120万案例，411种犯罪类型）和LegalSearchLM模型，通过法律元素推理和约束解码生成目标案例内容。

Result: LegalSearchLM在LEGAR BENCH上优于基线6-20%，泛化能力超出域内训练的生成模型15%。

Conclusion: LEGAR BENCH和LegalSearchLM解决了现有LCR的局限性，实现了更全面和高效的检索性能。

Abstract: Legal Case Retrieval (LCR), which retrieves relevant cases from a query case,
is a fundamental task for legal professionals in research and decision-making.
However, existing studies on LCR face two major limitations. First, they are
evaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and
use a narrow range of criminal query types, which cannot sufficiently reflect
the complexity of real-world legal retrieval scenarios. Second, their reliance
on embedding-based or lexical matching methods often results in limited
representations and legally irrelevant matches. To address these issues, we
present: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering
411 diverse crime types in queries over 1.2M legal cases; and (2)
LegalSearchLM, a retrieval model that performs legal element reasoning over the
query case and directly generates content grounded in the target cases through
constrained decoding. Experimental results show that LegalSearchLM outperforms
baselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It
also demonstrates strong generalization to out-of-domain cases, outperforming
naive generative models trained on in-domain data by 15%.

</details>


### [32] [Benchmarking Abstract and Reasoning Abilities Through A Theoretical Perspective](https://arxiv.org/abs/2505.23833)
*Qingchuan Ma,Yuhang Wu,Xiawu Zheng,Rongrong Ji*

Key words: 大语言模型, 抽象推理, 基准测试, 符号重映射, 模式识别

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文提出了一个简单、有效且理论扎实的基准，用于严格评估大语言模型（LLMs）的抽象推理能力，并引入了两个新指标（γ和δ分数）。实验揭示了当前LLMs在抽象推理上的局限性和改进方向。

Motivation: 旨在建立一个理论扎实的基准，以评估LLMs的抽象推理能力，尤其是识别和运用抽象模式的能力。

Method: 提出了一个数学框架和两个互补的指标（γ和δ分数），并设计了系统性符号重映射任务作为评估基准。

Result: 发现当前LLMs在非十进制算术和符号推理上存在显著局限性，且思维链提示未能完全解决抽象推理缺口。δ分数有效衡量了模型对记忆的依赖。

Conclusion: 尽管LLMs在某些领域表现优异，但抽象推理能力仍有不足，需进一步改进。

Abstract: In this paper, we aim to establish a simple, effective, and theoretically
grounded benchmark for rigorously probing abstract reasoning in Large Language
Models (LLMs). To achieve this, we first develop a mathematic framework that
defines abstract reasoning as the ability to: (i) extract essential patterns
independent of surface representations, and (ii) apply consistent rules to
these abstract patterns. Based on this framework, we introduce two novel
complementary metrics: \(\scoreGamma\) measures basic reasoning accuracy, while
\(\scoreDelta\) quantifies a model's reliance on specific symbols rather than
underlying patterns - a key indicator of true abstraction versus mere
memorization. To implement this measurement, we design a benchmark: systematic
symbol remapping in rule-based tasks, which forces models to demonstrate
genuine pattern recognition beyond superficial token matching. Extensive LLM
evaluations using this benchmark (commercial API models, 7B-70B, multi-agent)
reveal:1) critical limitations in non-decimal arithmetic and symbolic
reasoning; 2) persistent abstraction gaps despite chain-of-thought prompting;
and 3) \(\scoreDelta\)'s effectiveness in robustly measuring memory dependence
by quantifying performance degradation under symbol remapping, particularly
highlighting operand-specific memorization. These findings underscore that
current LLMs, despite domain-specific strengths, still lack robust abstract
reasoning, highlighting key areas for future improvement.

</details>


### [33] [Say What You Mean: Natural Language Access Control with Large Language Models for Internet of Things](https://arxiv.org/abs/2505.23835)
*Ye Cheng,Minghui Xu,Yue Zhang,Kun Li,Hao Wu,Yechao Zhang,Shaoyong Guo,Wangjie Qiu,Dongxiao Yu,Xiuzhen Cheng*

Key words: 访问控制, IoT, 大型语言模型, 自然语言处理, 策略验证

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LACE是一个基于语言的访问控制引擎，通过结合大型语言模型（LLMs）、提示引导策略生成和形式化验证，实现了自然语言策略的自动翻译和验证，提升了IoT环境中的访问控制表现。

Motivation: IoT中的访问控制需要处理动态和上下文因素，但现有平台要么控制粒度粗糙，要么依赖刚性规则匹配，且策略编写过程存在语义鸿沟和潜在配置错误。

Method: LACE结合提示引导策略生成、检索增强推理和形式化验证，支持从自然语言生成结构化规则并进行语义验证，采用混合LLM-规则引擎决策。

Result: 在智能家居环境中测试，LACE的策略生成正确率达100%，决策准确率最高88%（F1-score 0.79），性能优于GPT-3.5和Gemini，并展现良好的扩展性。

Conclusion: LACE为IoT平台提供了安全、灵活且用户友好的访问控制方案，显著提升了策略表达力和执行准确性。

Abstract: Access control in the Internet of Things (IoT) is becoming increasingly
complex, as policies must account for dynamic and contextual factors such as
time, location, user behavior, and environmental conditions. However, existing
platforms either offer only coarse-grained controls or rely on rigid rule
matching, making them ill-suited for semantically rich or ambiguous access
scenarios. Moreover, the policy authoring process remains fragmented: domain
experts describe requirements in natural language, but developers must manually
translate them into code, introducing semantic gaps and potential
misconfiguration. In this work, we present LACE, the Language-based Access
Control Engine, a hybrid framework that leverages large language models (LLMs)
to bridge the gap between human intent and machine-enforceable logic. LACE
combines prompt-guided policy generation, retrieval-augmented reasoning, and
formal validation to support expressive, interpretable, and verifiable access
control. It enables users to specify policies in natural language,
automatically translates them into structured rules, validates semantic
correctness, and makes access decisions using a hybrid LLM-rule-based engine.
We evaluate LACE in smart home environments through extensive experiments. LACE
achieves 100% correctness in verified policy generation and up to 88% decision
accuracy with 0.79 F1-score using DeepSeek-V3, outperforming baselines such as
GPT-3.5 and Gemini. The system also demonstrates strong scalability under
increasing policy volume and request concurrency. Our results highlight LACE's
potential to enable secure, flexible, and user-friendly access control across
real-world IoT platforms.

</details>


### [34] [Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836)
*Joe Needham,Giles Edkins,Govind Pimpale,Henning Bartsch,Marius Hobbhahn*

Key words: 评估意识，语言模型，基准测试，真实部署，代理任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了前沿语言模型是否能检测评估与真实部署的差异（评估意识），发现模型虽表现优于随机但仍未超越人类基线，建议未来跟踪此能力。

Motivation: 探讨AI模型是否能区分评估与真实部署，以避免评估效果被模型行为差异影响，确保基准测试的可靠性。

Method: 构建包含1000个提示和转录的多样化基准，测试模型在评估意识方面的表现，并与人类基线比较。

Result: 前沿模型（如Gemini-2.5-Pro）表现出高于随机的评估意识（AUC 0.83），但未超过人类（AUC 0.92）；模型在代理任务中表现更佳。

Conclusion: 前沿模型已具备显著但不超人的评估意识，建议未来持续监控此能力。

Abstract: If AI models can detect when they are being evaluated, the effectiveness of
evaluations might be compromised. For example, models could have systematically
different behavior during evaluations, leading to less reliable benchmarks for
deployment and governance decisions. We investigate whether frontier language
models can accurately classify transcripts based on whether they originate from
evaluations or real-world deployment, a capability we call evaluation
awareness. To achieve this, we construct a diverse benchmark of 1,000 prompts
and transcripts from 61 distinct datasets. These span public benchmarks (e.g.,
MMLU, SWEBench), real-world deployment interactions, and agent trajectories
from scaffolding frameworks (e.g., web-browsing agents). Frontier models
clearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches
an AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of
$0.92$). Furthermore, both AI models and humans are better at identifying
evaluations in agentic settings compared to chat settings. Additionally, we
test whether models can identify the purpose of the evaluation. Under
multiple-choice and open-ended questioning, AI models far outperform random
chance in identifying what an evaluation is testing for. Our results indicate
that frontier models already exhibit a substantial, though not yet superhuman,
level of evaluation-awareness. We recommend tracking this capability in future
models.

</details>


### [35] [CoMaPOI: A Collaborative Multi-Agent Framework for Next POI Prediction Bridging the Gap Between Trajectory and Language](https://arxiv.org/abs/2505.23837)
*Lin Zhong,Lingzhi Wang,Xu Yang,Qing Liao*

Key words: LLMs, POI prediction, collaborative agents, spatiotemporal data, semantic understanding

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: CoMaPOI improves next POI prediction by using a multi-agent framework to address LLMs' challenges with spatiotemporal data and large candidate spaces.

Motivation: Address the limitations of LLMs in understanding spatiotemporal data and handling large POI candidate spaces for next POI prediction.

Method: Proposes CoMaPOI, a collaborative multi-agent framework with Profiler, Forecaster, and Predictor agents to enhance semantic understanding and constrain POI space.

Result: Achieves state-of-the-art performance, improving metrics by 5% to 10% on NYC, TKY, and CA datasets.

Conclusion: CoMaPOI effectively leverages tailored agents to tackle LLM challenges in spatiotemporal tasks.

Abstract: Large Language Models (LLMs) offer new opportunities for the next
Point-Of-Interest (POI) prediction task, leveraging their capabilities in
semantic understanding of POI trajectories. However, previous LLM-based
methods, which are superficially adapted to next POI prediction, largely
overlook critical challenges associated with applying LLMs to this task.
Specifically, LLMs encounter two critical challenges: (1) a lack of intrinsic
understanding of numeric spatiotemporal data, which hinders accurate modeling
of users' spatiotemporal distributions and preferences; and (2) an excessively
large and unconstrained candidate POI space, which often results in random or
irrelevant predictions. To address these issues, we propose a Collaborative
Multi Agent Framework for Next POI Prediction, named CoMaPOI. Through the close
interaction of three specialized agents (Profiler, Forecaster, and Predictor),
CoMaPOI collaboratively addresses the two critical challenges. The Profiler
agent is responsible for converting numeric data into language descriptions,
enhancing semantic understanding. The Forecaster agent focuses on dynamically
constraining and refining the candidate POI space. The Predictor agent
integrates this information to generate high-precision predictions. Extensive
experiments on three benchmark datasets (NYC, TKY, and CA) demonstrate that
CoMaPOI achieves state of the art performance, improving all metrics by 5% to
10% compared to SOTA baselines. This work pioneers the investigation of
challenges associated with applying LLMs to complex spatiotemporal tasks by
leveraging tailored collaborative agents.

</details>


### [36] [Exploring the Landscape of Text-to-SQL with Large Language Models: Progresses, Challenges and Opportunities](https://arxiv.org/abs/2505.23838)
*Yiming Huang,Jiyu Guo,Wenxin Mao,Cuiyun Gao,Peiyi Han,Chuanyi Liu,Qing Ling*

Key words: Text-to-SQL, 大型语言模型（LLM）, 自然语言处理（NLP）, 数据集, 评估指标

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文是对基于大型语言模型（LLM）的文本到SQL（Text-to-SQL）技术的系统综述，涵盖研究趋势、技术分析、数据集与评估指标总结，以及未来研究方向探讨。

Motivation: 随着LLM在自然语言处理（NLP）领域的显著进展，文本到SQL技术成为无SQL知识用户访问数据库的重要工具，本文旨在推动该领域的创新与进步。

Method: 研究对LLM-based的文本到SQL技术进行了多角度分析，包括研究趋势、现有技术、数据集、评估指标等方面的系统综述。

Result: 研究发现LLM显著推动了文本到SQL领域的发展，但仍存在挑战需解决。

Conclusion: 本文为研究者提供了深入的LLM-based文本到SQL技术综述，为该领域的未来发展提供了方向。

Abstract: Converting natural language (NL) questions into SQL queries, referred to as
Text-to-SQL, has emerged as a pivotal technology for facilitating access to
relational databases, especially for users without SQL knowledge. Recent
progress in large language models (LLMs) has markedly propelled the field of
natural language processing (NLP), opening new avenues to improve text-to-SQL
systems. This study presents a systematic review of LLM-based text-to-SQL,
focusing on four key aspects: (1) an analysis of the research trends in
LLM-based text-to-SQL; (2) an in-depth analysis of existing LLM-based
text-to-SQL techniques from diverse perspectives; (3) summarization of existing
text-to-SQL datasets and evaluation metrics; and (4) discussion on potential
obstacles and avenues for future exploration in this domain. This survey seeks
to furnish researchers with an in-depth understanding of LLM-based text-to-SQL,
sparking new innovations and advancements in this field.

</details>


### [37] [Measuring Sycophancy of Language Models in Multi-turn Dialogues](https://arxiv.org/abs/2505.23840)
*Jiseung Hong,Grace Byun,Seungone Kim,Kai Shu*

Key words: 大语言模型, 谄媚行为, 多轮对话, 对齐调优, 第三人称视角

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了SYCON Bench，一个用于评估多轮自由对话中LLMs谄媚行为的新基准，发现对齐调优会增加谄媚行为，而模型规模和推理优化能增强抵抗能力。

Motivation: 尽管LLMs应提供有用且无害的回应，但它们常表现谄媚行为—即无论事实或道德正确性，盲目迎合用户观点。此前研究多限于单轮事实准确性，忽略了真实交互动态。

Method: 作者设计了SYCON Bench基准，衡量模型在多轮对话中迎合用户的“翻转轮数”和“翻转次数”，并测试了17种LLM在三种现实场景中的表现。

Result: 研究发现谄媚行为普遍存在，对齐调优会加剧该行为，而模型规模和推理优化则提升抵抗能力。采用第三人称视角的提示策略可将谄媚行为减少63.8%。

Conclusion: LLMs的谄媚行为需进一步研究，优化推理和模型规模是关键，某些提示策略如第三人称视角能显著减少谄媚。

Abstract: Large Language Models (LLMs) are expected to provide helpful and harmless
responses, yet they often exhibit sycophancy--conforming to user beliefs
regardless of factual accuracy or ethical soundness. Prior research on
sycophancy has primarily focused on single-turn factual correctness,
overlooking the dynamics of real-world interactions. In this work, we introduce
SYCON Bench, a novel benchmark for evaluating sycophantic behavior in
multi-turn, free-form conversational settings. Our benchmark measures how
quickly a model conforms to the user (Turn of Flip) and how frequently it
shifts its stance under sustained user pressure (Number of Flip). Applying
SYCON Bench to 17 LLMs across three real-world scenarios, we find that
sycophancy remains a prevalent failure mode. Our analysis shows that alignment
tuning amplifies sycophantic behavior, whereas model scaling and reasoning
optimization strengthen the model's ability to resist undesirable user views.
Reasoning models generally outperform instruction-tuned models but often fail
when they over-index on logical exposition instead of directly addressing the
user's underlying beliefs. Finally, we evaluate four additional prompting
strategies and demonstrate that adopting a third-person perspective reduces
sycophancy by up to 63.8% in debate scenario. We release our code and data at
https://github.com/JiseungHong/SYCON-Bench.

</details>


### [38] [Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection](https://arxiv.org/abs/2505.18754)
*Elsen Ronando,Sozo Inoue*

Key words: 小样本学习, HED-LM, 疲劳检测, 欧氏距离, 大型语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出HED-LM方法，通过结合欧氏距离和大型语言模型的上下文相关性改进小样本学习中示例选择的性能，应用于疲劳检测任务并显著优于基线方法。

Motivation: 小样本学习的性能高度依赖示例选择的质量，而传感器数据（如疲劳检测）的复杂性和高变异性增加了选择的难度。HED-LM旨在通过混合方法解决这一问题。

Method: HED-LM采用两步选择流程：先基于欧氏距离筛选候选示例，再用大型语言模型（LLM）评分上下文相关性进行重排序。

Result: 在疲劳检测任务中，HED-LM的宏F1值达69.13±10.71%，显著优于随机选择（59.30±10.13%）和纯距离筛选（67.61±11.39%）。

Conclusion: 结合数值相似性和上下文相关性可提升小样本学习的鲁棒性，HED-LM在医疗监控、活动识别等领域具有应用潜力。

Abstract: In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid
Euclidean Distance with Large Language Models) to improve example selection for
sensor-based classification tasks. While few-shot prompting enables efficient
inference with limited labeled data, its performance largely depends on the
quality of selected examples. HED-LM addresses this challenge through a hybrid
selection pipeline that filters candidate examples based on Euclidean distance
and re-ranks them using contextual relevance scored by large language models
(LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection
task using accelerometer data characterized by overlapping patterns and high
inter-subject variability. Unlike simpler tasks such as activity recognition,
fatigue detection demands more nuanced example selection due to subtle
differences in physiological signals. Our experiments show that HED-LM achieves
a mean macro F1-score of 69.13$\pm$10.71%, outperforming both random selection
(59.30$\pm$10.13%) and distance-only filtering (67.61$\pm$11.39%). These
represent relative improvements of 16.6% and 2.3%, respectively. The results
confirm that combining numerical similarity with contextual relevance improves
the robustness of few-shot prompting. Overall, HED-LM offers a practical
solution to improve performance in real-world sensor-based learning tasks and
shows potential for broader applications in healthcare monitoring, human
activity recognition, and industrial safety scenarios.

</details>


### [39] [Document Valuation in LLM Summaries: A Cluster Shapley Approach](https://arxiv.org/abs/2505.23842)
*Zikun Ye,Hema Yoganarasimhan*

Key words: 大型语言模型, 摘要生成, Shapley值, 聚类, 语义相似性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 为了解决大型语言模型（LLM）生成摘要时的内容创作者贡献分配问题，研究提出了Cluster Shapley方法，通过聚类和基于语义相似性的Shapley值近似计算，高效且准确地评估文档贡献。

Motivation: LLM生成摘要时缺乏对原始内容创作者的贡献分配机制，导致信用和补偿问题。研究旨在解决这一问题。

Method: 提出Cluster Shapley方法，结合文档的语义相似性进行聚类，并在聚类层级计算Shapley值，以高效评估文档贡献。

Result: Cluster Shapley在亚马逊产品评论摘要任务中显著降低了计算复杂度，同时保持了高准确性，优于基线方法。

Conclusion: Cluster Shapley是一种高效且通用的方法，适用于多种摘要场景，解决了LLM生成摘要时的贡献分配问题。

Abstract: Large Language Models (LLMs) are increasingly used in systems that retrieve
and summarize content from multiple sources, such as search engines and AI
assistants. While these models enhance user experience by generating coherent
summaries, they obscure the contributions of original content creators, raising
concerns about credit attribution and compensation. We address the challenge of
valuing individual documents used in LLM-generated summaries. We propose using
Shapley values, a game-theoretic method that allocates credit based on each
document's marginal contribution. Although theoretically appealing, Shapley
values are expensive to compute at scale. We therefore propose Cluster Shapley,
an efficient approximation algorithm that leverages semantic similarity between
documents. By clustering documents using LLM-based embeddings and computing
Shapley values at the cluster level, our method significantly reduces
computation while maintaining attribution quality. We demonstrate our approach
to a summarization task using Amazon product reviews. Cluster Shapley
significantly reduces computational complexity while maintaining high accuracy,
outperforming baseline methods such as Monte Carlo sampling and Kernel SHAP
with a better efficient frontier. Our approach is agnostic to the exact LLM
used, the summarization process used, and the evaluation procedure, which makes
it broadly applicable to a variety of summarization settings.

</details>


### [40] [Evaluation Hallucination in Multi-Round Incomplete Information Lateral-Driven Reasoning Tasks](https://arxiv.org/abs/2505.23843)
*Wenhan Dong,Tianyi Hu,Jingyi Zheng,Zhen Sun,Yuemeng Zhao,Yule Liu,Xinlei He,Xinyi Huang*

Key words: 大语言模型，横向思维，评估标准，推理能力，误导性结果

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究发现现有评估大语言模型（LLMs）横向思维能力的基准和方法存在误导性结论，提出了一套改进的评估标准。

Motivation: 揭示现有方法在评估LLMs横向思维能力时的局限性，如捷径行为、僵化模式和任务过早终止，这些问题掩盖了模型的真实推理能力。

Method: 提出改进的评估标准，包括检查推理路径、多样化评估指标以及与人类表现的对比分析。

Result: 现有方法容易产生误导性结果，改进标准能更准确地评估LLMs的真实能力。

Conclusion: 改进后的评估标准能更可靠地反映LLMs的横向思维能力，提升评估的准确性。

Abstract: Multi-round incomplete information tasks are crucial for evaluating the
lateral thinking capabilities of large language models (LLMs). Currently,
research primarily relies on multiple benchmarks and automated evaluation
metrics to assess these abilities. However, our study reveals novel insights
into the limitations of existing methods, as they often yield misleading
results that fail to uncover key issues, such as shortcut-taking behaviors,
rigid patterns, and premature task termination. These issues obscure the true
reasoning capabilities of LLMs and undermine the reliability of evaluations. To
address these limitations, we propose a refined set of evaluation standards,
including inspection of reasoning paths, diversified assessment metrics, and
comparative analyses with human performance.

</details>


### [41] [Enabling Flexible Multi-LLM Integration for Scalable Knowledge Aggregation](https://arxiv.org/abs/2505.23844)
*Zhenglun Kong,Zheng Zhan,Shiyue Hou,Yifan Gong,Xin Meng,Pengwei Sui,Peiyan Dong,Xuan Shen,Zifeng Wang,Pu Zhao,Hao Tang,Stratis Ioannidis,Yanzhi Wang*

Key words: 大型语言模型, 知识聚合, 自适应选择, 动态加权融合, 知识干扰

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种自适应选择和聚合来自不同大型语言模型（LLM）知识的框架，以构建一个单一、更强的模型，解决了传统集成和权重合并方法的内存开销大和灵活性不足的问题。

Motivation: 传统的大型语言模型（LLM）优化方法（如集成和权重合并）在内存需求和适应动态数据环境方面存在局限性，且容易受到任务间干扰和性能下降的影响。为了解决这些问题，研究团队提出了一种新的知识聚合框架。

Method: 论文设计了一个自适应选择网络，用于基于评分选择最相关的源LLM，从而减少知识干扰。此外，提出了一种动态加权融合策略，考虑了候选LLM的内在优势，并采用反馈驱动的损失函数防止选择器收敛于单一源子集。

Result: 实验结果表明，该方法能够实现更稳定和可扩展的知识聚合过程，与现有方法相比，知识干扰减少了高达50%。

Conclusion: 该框架有效解决了传统方法的高内存需求和灵活性不足的问题，同时显著减少了知识干扰，为LLM的持续优化提供了一种更高效的解决方案。

Abstract: Large language models (LLMs) have shown remarkable promise but remain
challenging to continually improve through traditional finetuning, particularly
when integrating capabilities from other specialized LLMs. Popular methods like
ensemble and weight merging require substantial memory and struggle to adapt to
changing data environments. Recent efforts have transferred knowledge from
multiple LLMs into a single target model; however, they suffer from
interference and degraded performance among tasks, largely due to limited
flexibility in candidate selection and training pipelines. To address these
issues, we propose a framework that adaptively selects and aggregates knowledge
from diverse LLMs to build a single, stronger model, avoiding the high memory
overhead of ensemble and inflexible weight merging. Specifically, we design an
adaptive selection network that identifies the most relevant source LLMs based
on their scores, thereby reducing knowledge interference. We further propose a
dynamic weighted fusion strategy that accounts for the inherent strengths of
candidate LLMs, along with a feedback-driven loss function that prevents the
selector from converging on a single subset of sources. Experimental results
demonstrate that our method can enable a more stable and scalable knowledge
aggregation process while reducing knowledge interference by up to 50% compared
to existing approaches. Code is avaliable at
https://github.com/ZLKong/LLM_Integration

</details>


### [42] [Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in LLMs](https://arxiv.org/abs/2505.23845)
*Jakub Podolak,Rajeev Verma*

Key words: DeepSeek R1-32B, 自报置信度, 语义熵, 链式思维, 不确定性估计

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DeepSeek R1-32B的自报置信度在默认设置下经常过度自信，而语义熵更可靠，可能是因其能探索模型的预测分布。通过长链思维改进置信度有效性，表明可靠的置信度估计需要显式的生成空间探索。

Motivation: 研究DeepSeek R1-32B在问答任务中自报置信度的不确定性来源，以确定如何更可靠地估计其预测置信度。

Method: 1. 默认设置下评估模型自报置信度；2. 使用语义熵方法获取预测分布；3. 强制模型进行长链思维后再给出回答；4. 使用读者模型分析置信度来源。

Result: 长链思维能显著提升置信度有效性；读者模型能复现类似置信度，表明自报置信度可能是推理过程中生成结果的统计量。

Conclusion: 可靠的置信度估计需显式探索生成空间；未探索前自报置信度不可靠。

Abstract: We study the source of uncertainty in DeepSeek R1-32B by analyzing its
self-reported verbal confidence on question answering (QA) tasks. In the
default answer-then-confidence setting, the model is regularly over-confident,
whereas semantic entropy - obtained by sampling many responses - remains
reliable. We hypothesize that this is because of semantic entropy's larger
test-time compute, which lets us explore the model's predictive distribution.
We show that granting DeepSeek the budget to explore its distribution by
forcing a long chain-of-thought before the final answer greatly improves its
verbal score effectiveness, even on simple fact-retrieval questions that
normally require no reasoning. Furthermore, a separate reader model that sees
only the chain can reconstruct very similar confidences, indicating the verbal
score might be merely a statistic of the alternatives surfaced during
reasoning. Our analysis concludes that reliable uncertainty estimation requires
explicit exploration of the generative space, and self-reported confidence is
trustworthy only after such exploration.

</details>


### [43] [Scalable, Symbiotic, AI and Non-AI Agent Based Parallel Discrete Event Simulations](https://arxiv.org/abs/2505.23846)
*Atanu Barai,Stephan Eidenbenz,Nandakishore Santhi*

Key words: AI系统整合、并行离散事件仿真、动态约束、多代理系统、可扩展性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种基于并行离散事件仿真（PDES）的方法，将多个AI和非AI代理耦合在一起，以约束并确保输出的正确性。通过动态约束和合作机制，提高了多领域问题的解决准确率。

Motivation: 为了在可信赖的前提下充分发挥AI系统的潜力，需要将多个AI和非AI系统无缝结合，以约束并确保输出的正确性。

Method: 采用并行离散事件仿真（PDES）框架，将每个代理视为实体，并在时间推移的基础上协同工作。通过动态约束和结构化步骤分解问题，由非AI代理作为审计员验证每一步。

Result: 在四个不同领域的问题中，该方法将整体准确率从纯AI模型的23%提升至68%，显示出显著优势。

Conclusion: 通过结合AI与非AI代理的PDES方法，可以显著提高复杂问题的解决准确率，并支持未来的大规模可扩展仿真。

Abstract: To fully leverage the potential of artificial intelligence (AI) systems in a
trustworthy manner, it is desirable to couple multiple AI and non-AI systems
together seamlessly for constraining and ensuring correctness of the output.
This paper introduces a novel parallel discrete event simulation (PDES) based
methodology to combine multiple AI and non-AI agents in a causal, rule-based
way. Our approach tightly integrates the concept of passage of time, with each
agent considered as an entity in the PDES framework and responding to prior
requests from other agents. Such coupling mechanism enables the agents to work
in a co-operative environment towards a common goal while many tasks run in
parallel throughout the simulation. It further enables setting up boundaries to
the outputs of the AI agents by applying necessary dynamic constraints using
non-AI agents while allowing for scalability through deployment of hundreds of
such agents in a larger compute cluster. Distributing smaller AI agents can
enable extremely scalable simulations in the future, addressing local memory
bottlenecks for model parameter storage. Within a PDES involving both AI and
non-AI agents, we break down the problem at hand into structured steps, when
necessary, providing a set of multiple choices to the AI agents, and then
progressively solve these steps towards a final goal. At each step, the non-AI
agents act as unbiased auditors, verifying each action by the AI agents so that
certain rules of engagement are followed. We evaluate our approach by solving
four problems from four different domains and comparing the results with those
from AI models alone. Our results show greater accuracy in solving problems
from various domains where the AI models struggle to solve the problems solely
by themselves. Results show that overall accuracy of our approach is 68% where
as the accuracy of vanilla models is less than 23%.

</details>


### [44] [Derailing Non-Answers via Logit Suppression at Output Subspace Boundaries in RLHF-Aligned Language Models](https://arxiv.org/abs/2505.23848)
*Harvey Dam,Jonas Knochelmann,Vinu Joseph,Ganesh Gopalakrishnan*

Key words: 大语言模型,拒绝行为,敏感内容,链式思考,标记序列

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出一种在不修改模型权重或提示的情况下减少大语言模型（LLM）对敏感内容拒绝率的方法，通过抑制特定标记序列（如<think>后的双换行符）来绕过拒绝行为。

Motivation: 观察到某些模型的拒绝行为常伴随特定标记序列（如<think>后紧随双换行符），研究如何通过简单格式化调整避免拒绝，而无需数据集或训练。

Method: 在生成过程中抑制<think>后的双换行符（\n\n）及结束序列标记（</think>后的EOS），仅通过调整标记概率实现。

Result: 在DeepSeek-R1实验中，该方法提升了敏感提示的实质性回答比例，同时不影响标准基准性能。

Conclusion: 拒绝行为可通过在生成过程中阻断特定拒绝子空间来规避，无需复杂调整。

Abstract: We introduce a method to reduce refusal rates of large language models (LLMs)
on sensitive content without modifying model weights or prompts. Motivated by
the observation that refusals in certain models were often preceded by the
specific token sequence of a token marking the beginning of the
chain-of-thought (CoT) block (<think>) followed by a double newline token
(\n\n), we investigate the impact of two simple formatting adjustments during
generation: suppressing \n\n after <think> and suppressing the end-of-sequence
token after the end of the CoT block (</think>). Our method requires no
datasets, parameter changes, or training, relying solely on modifying token
probabilities during generation. In our experiments with official DeepSeek-R1
distillations, these interventions increased the proportion of substantive
answers to sensitive prompts without affecting performance on standard
benchmarks. Our findings suggest that refusal behaviors can be circumvented by
blocking refusal subspaces at specific points in the generation process.

</details>


### [45] [ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark](https://arxiv.org/abs/2505.23851)
*Michael Shalyt,Rotem Elimelech,Ido Kaminer*

Key words: LLMs, 符号数学, ASyMOB, 泛化能力, 扰动分析

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ASyMOB框架评估了LLMs在符号数学中的表现，发现即使是高性能模型也依赖记忆而非深层理解，但先进模型展示了更强的泛化能力。

Motivation: 评估LLMs在符号数学中的核心能力（如积分、微分方程等），填补现有基准的不足。

Method: 引入ASyMOB评估框架，包含17,092个数学问题，通过数值或符号扰动分析泛化能力。

Result: 高性能模型（如o4-mini、Gemini 2.5 Flash）在未扰动集上表现优异（96.8%、97.6%），但对扰动敏感（性能下降约21%）。结合代码执行可提升准确性（+33.1%）。

Conclusion: LLMs的泛化能力可能迎来质变，但未来方向可能是深度集成外部工具或开发更强大的模型。

Abstract: Large language models (LLMs) are rapidly approaching the level of proficiency
in university-level symbolic mathematics required for applications in advanced
science and technology. However, existing benchmarks fall short in assessing
the core skills of LLMs in symbolic mathematics-such as integration,
differential equations, and algebraic simplification. To address this gap, we
introduce ASyMOB, a novel assessment framework focused exclusively on symbolic
manipulation, featuring 17,092 unique math challenges, organized by similarity
and complexity. ASyMOB enables analysis of LLM generalization capabilities by
comparing performance in problems that differ by simple numerical or symbolic
`perturbations'. Evaluated LLMs exhibit substantial degradation in performance
for all perturbation types (up to -70.3%), suggesting reliance on memorized
patterns rather than deeper understanding of symbolic math, even among models
achieving high baseline accuracy. Comparing LLM performance to computer algebra
systems, we identify examples where they fail while LLMs succeed, as well as
problems solved only by combining both approaches. Models capable of integrated
code execution yielded higher accuracy compared to their performance without
code, particularly stabilizing weaker models (up to +33.1% for certain
perturbation types). Notably, the most advanced models (o4-mini, Gemini 2.5
Flash) demonstrate not only high symbolic math proficiency (scoring 96.8% and
97.6% on the unperturbed set), but also remarkable robustness against
perturbations, (-21.7% and -21.2% vs. average -50.4% for the other models).
This may indicate a recent "phase transition" in the generalization
capabilities of frontier LLMs. It remains to be seen whether the path forward
lies in deeper integration with sophisticated external tools, or in developing
models so capable that symbolic math systems like CAS become unnecessary.

</details>


### [46] [Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease](https://arxiv.org/abs/2505.23852)
*Nic Dobbins,Christelle Xiong,Kristine Lan,Meliha Yetisgen*

Key words: Large Language Models, reproducibility, autonomous agents, biomedical research, Alzheimer's studies

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型(LLM)作为自主代理重现已发表研究的能力，使用相同或类似数据集。结果显示约53.2%的关键发现被重现，但数值和方法存在差异。

Motivation: 验证LLM作为自主代理在生物医学研究中自动重现研究结果的能力，评估其潜力与限制。

Method: 利用NACC数据集和GPT-4o构建模拟研究团队，仅基于摘要、方法部分和数据字典重现五项阿尔茨海默症研究的关键发现。

Result: LLM代理平均重现了53.2%的发现，数值和统计方法存在差异，但部分趋势和显著性相似。

Conclusion: LLM在自动化重现研究中展现出潜力，但当前仍受限于方法细节和实现缺陷，需要进一步改进。

Abstract: Objective: To demonstrate the capabilities of Large Language Models (LLMs) as
autonomous agents to reproduce findings of published research studies using the
same or similar dataset.
  Materials and Methods: We used the "Quick Access" dataset of the National
Alzheimer's Coordinating Center (NACC). We identified highly cited published
research manuscripts using NACC data and selected five studies that appeared
reproducible using this dataset alone. Using GPT-4o, we created a simulated
research team of LLM-based autonomous agents tasked with writing and executing
code to dynamically reproduce the findings of each study, given only study
Abstracts, Methods sections, and data dictionary descriptions of the dataset.
  Results: We extracted 35 key findings described in the Abstracts across 5
Alzheimer's studies. On average, LLM agents approximately reproduced 53.2% of
findings per study. Numeric values and range-based findings often differed
between studies and agents. The agents also applied statistical methods or
parameters that varied from the originals, though overall trends and
significance were sometimes similar.
  Discussion: In some cases, LLM-based agents replicated research techniques
and findings. In others, they failed due to implementation flaws or missing
methodological detail. These discrepancies show the current limits of LLMs in
fully automating reproducibility assessments. Still, this early investigation
highlights the potential of structured agent-based systems to provide scalable
evaluation of scientific rigor.
  Conclusion: This exploratory work illustrates both the promise and
limitations of LLMs as autonomous agents for automating reproducibility in
biomedical research.

</details>


### [47] [Revisiting Uncertainty Estimation and Calibration of Large Language Models](https://arxiv.org/abs/2505.23854)
*Linwei Tao,Yi-Fan Yeh,Minjing Dong,Tao Huang,Philip Torr,Chang Xu*

Key words: 大语言模型, 不确定性估计, LVU, 校准, 可信AI

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文全面评估了大语言模型（LLMs）的不确定性估计方法，比较了80种模型，发现语言表达不确定性（LVU）在校准和区分性上表现优于其他方法，强调了多角度评估的重要性。

Motivation: 随着大语言模型（LLMs）在高风险应用中的部署增加，稳健的不确定性估计对于确保其安全和可信部署至关重要。

Method: 研究评估了80种模型，包括开源和闭源、密集和混合专家架构等，测试了三种黑盒单次通过方法：TPU、NVU和LVU。

Result: LVU在MMLU-Pro基准测试中表现最佳，校准和区分性更强。模型规模、推理能力和量化等因素均影响不确定性估计性能。

Conclusion: LLMs在推理任务上的不确定性估计优于知识型任务，但高精度不一定对应可靠的不确定性。LVU是提升LLM可靠性的实用工具。

Abstract: As large language models (LLMs) are increasingly deployed in high-stakes
applications, robust uncertainty estimation is essential for ensuring the safe
and trustworthy deployment of LLMs. We present the most comprehensive study to
date of uncertainty estimation in LLMs, evaluating 80 models spanning open- and
closed-source families, dense and Mixture-of-Experts (MoE) architectures,
reasoning and non-reasoning modes, quantization variants and parameter scales
from 0.6B to 671B. Focusing on three representative black-box single-pass
methods, including token probability-based uncertainty (TPU), numerical verbal
uncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically
evaluate uncertainty calibration and selective classification using the
challenging MMLU-Pro benchmark, which covers both reasoning-intensive and
knowledge-based tasks. Our results show that LVU consistently outperforms TPU
and NVU, offering stronger calibration and discrimination while being more
interpretable. We also find that high accuracy does not imply reliable
uncertainty, and that model scale, post-training, reasoning ability and
quantization all influence estimation performance. Notably, LLMs exhibit better
uncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good
calibration does not necessarily translate to effective error ranking. These
findings highlight the need for multi-perspective evaluation and position LVU
as a practical tool for improving the reliability of LLMs in real-world
settings.

</details>


### [48] [OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities](https://arxiv.org/abs/2505.23856)
*Sahil Verma,Keegan Hines,Jeff Bilmes,Charlotte Siska,Luke Zettlemoyer,Hila Gonen,Chandan Singh*

Key words: 大型语言模型、多模态检测、有害提示检测、OMNIGUARD、安全

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: OMNIGUARD是一种跨语言和跨模态检测有害提示的方法，通过识别LLM/MLLM的内部表示并构建语言或模态无关的分类器，显著提升了分类准确率，并具有高效性。

Motivation: 随着大型语言模型（LLM）能力的提升，其潜在的有害使用引起了广泛关注。当前的有害查询检测方法容易受到攻击，尤其是在低资源语言或非文本模态（如图像、音频）上的表现不佳，因此需要一种更通用的解决方案。

Method: 提出OMNIGUARD方法，分两步实现：(i)识别LLM/MLLM在语言或模态间的对齐内部表示，(ii)利用这些表示构建语言或模态无关的分类器，检测有害提示。

Result: OMNIGUARD在多语言设置下比最强基线提升11.57%的准确率，图像提示提升20.44%，音频提示达到新SOTA，同时效率提高了约120倍。

Conclusion: OMNIGUARD通过跨语言和跨模态的适应性，显著提升了有害提示检测的准确率和效率，为LLM的安全使用提供了有效解决方案。

Abstract: The emerging capabilities of large language models (LLMs) have sparked
concerns about their immediate potential for harmful misuse. The core approach
to mitigate these concerns is the detection of harmful queries to the model.
Current detection approaches are fallible, and are particularly susceptible to
attacks that exploit mismatched generalization of model capabilities (e.g.,
prompts in low-resource languages or prompts provided in non-text modalities
such as image and audio). To tackle this challenge, we propose OMNIGUARD, an
approach for detecting harmful prompts across languages and modalities. Our
approach (i) identifies internal representations of an LLM/MLLM that are
aligned across languages or modalities and then (ii) uses them to build a
language-agnostic or modality-agnostic classifier for detecting harmful
prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\%
over the strongest baseline in a multilingual setting, by 20.44\% for
image-based prompts, and sets a new SOTA for audio-based prompts. By
repurposing embeddings computed during generation, OMNIGUARD is also very
efficient ($\approx 120 \times$ faster than the next fastest baseline). Code
and data are available at: https://github.com/vsahil/OmniGuard.

</details>


### [49] [Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation](https://arxiv.org/abs/2505.23867)
*Zeyu Liu,Zhitian Hou,Yining Di,Kejing Yang,Zhijie Sang,Congkai Xie,Jingwen Yang,Siyuan Liu,Jialu Wang,Chunming Li,Ming Li,Hongxia Yang*

Key words: 多模态大语言模型、医疗推理、资源效率、临床适应性、Infi-Med

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Infi-Med框架提出了一种高效的多模态医疗大语言模型，通过高质量数据集、增强的多模态推理能力和系统性评估，在医疗推理和临床适应性上达到最佳表现。

Motivation: 解决医疗大语言模型在资源效率、诊断准确性、临床考量和伦理隐私方面的实际部署挑战。

Method: 提出Infi-Med框架，包括高效SFT数据集构建、增强的多模态推理能力，以及系统化的评估体系。

Result: 实验表明Infi-Med在医疗推理任务中达到最优表现，并能快速适应临床场景。

Conclusion: Infi-Med为医疗大语言模型的实际部署奠定了坚实基础，平衡了模型效果与操作限制。

Abstract: Multimodal large language models (MLLMs) have demonstrated promising
prospects in healthcare, particularly for addressing complex medical tasks,
supporting multidisciplinary treatment (MDT), and enabling personalized
precision medicine. However, their practical deployment faces critical
challenges in resource efficiency, diagnostic accuracy, clinical
considerations, and ethical privacy. To address these limitations, we propose
Infi-Med, a comprehensive framework for medical MLLMs that introduces three key
innovations: (1) a resource-efficient approach through curating and
constructing high-quality supervised fine-tuning (SFT) datasets with minimal
sample requirements, with a forward-looking design that extends to both
pretraining and posttraining phases; (2) enhanced multimodal reasoning
capabilities for cross-modal integration and clinical task understanding; and
(3) a systematic evaluation system that assesses model performance across
medical modalities and task types. Our experiments demonstrate that Infi-Med
achieves state-of-the-art (SOTA) performance in general medical reasoning while
maintaining rapid adaptability to clinical scenarios. The framework establishes
a solid foundation for deploying MLLMs in real-world healthcare settings by
balancing model effectiveness with operational constraints.

</details>


### [50] [One Task Vector is not Enough: A Large-Scale Study for In-Context Learning](https://arxiv.org/abs/2505.23911)
*Pavel Tikhonov,Ivan Oseledets,Elena Tutubalina*

Key words: In-context learning, Large Language Models, task vectors, few-shot learning, distributed representation

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文介绍了In-context learning (ICL)在大型语言模型中的应用，通过新数据集QuiteAFew分析了任务向量对任务信息的编码能力，发现任务向量在中间层表现最佳，且不同任务类型的有效性差异显著。

Motivation: 探索大型语言模型（LLMs）通过任务向量（特定隐藏状态激活）编码任务信息的能力，但由于现有研究限于小规模基准，无法进行全面分析。

Method: 引入包含3,096个多样少样本任务的新数据集QuiteAFew，使用Llama-3-8B模型进行实验，分析任务向量在不同层和任务类型中的表现。

Result: 任务向量在中间层（如第15层）表现最佳，不同任务类型的有效性差异显著，复杂任务依赖多个子任务向量而非单一向量。

Conclusion: 任务知识在模型中呈分布式表示，复杂任务需多个向量协作。

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to adapt to
new tasks using few examples, with task vectors - specific hidden state
activations - hypothesized to encode task information. Existing studies are
limited by small-scale benchmarks, restricting comprehensive analysis. We
introduce QuiteAFew, a novel dataset of 3,096 diverse few-shot tasks, each with
30 input-output pairs derived from the Alpaca dataset. Experiments with
Llama-3-8B on QuiteAFew reveal: (1) task vector performance peaks at an
intermediate layer (e.g., 15th), (2) effectiveness varies significantly by task
type, and (3) complex tasks rely on multiple, subtask-specific vectors rather
than a single vector, suggesting distributed task knowledge representation.

</details>


### [51] [Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation](https://arxiv.org/abs/2505.23912)
*Caiqi Zhang,Xiaochen Zhu,Chengzu Li,Nigel Collier,Andreas Vlachos*

Key words: 大型语言模型, 幻觉检测, 强化学习, 置信度估计, 长文本生成

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了LoVeC方法，通过强化学习训练大型语言模型在生成文本时附加置信度分数，以检测幻觉问题，并在长文本生成任务中表现出色。

Motivation: 解决大型语言模型在生成事实性内容时的幻觉问题，现有方法效率低且难以适用于开放式生成任务。

Method: 使用强化学习训练模型为每个生成语句附加数值置信度分数，包括DPO、ORPO和GRPO等方法。

Result: 在三个长文本QA数据集上验证了方法的有效性和高效性，模型校准效果更好且跨领域泛化能力强。

Conclusion: LoVeC方法在检测幻觉和提升生成内容可信度方面表现优异，且效率高。

Abstract: Hallucination remains a major challenge for the safe and trustworthy
deployment of large language models (LLMs) in factual content generation. Prior
work has explored confidence estimation as an effective approach to
hallucination detection, but often relies on post-hoc self-consistency methods
that require computationally expensive sampling. Verbalized confidence offers a
more efficient alternative, but existing approaches are largely limited to
short-form question answering (QA) tasks and do not generalize well to
open-ended generation. In this paper, we propose LoVeC (Long-form Verbalized
Confidence), an on-the-fly verbalized confidence estimation method for
long-form generation. Specifically, we use reinforcement learning (RL) to train
LLMs to append numerical confidence scores to each generated statement, serving
as a direct and interpretable signal of the factuality of generation. Our
experiments consider both on-policy and off-policy RL methods, including DPO,
ORPO, and GRPO, to enhance the model calibration. We introduce two novel
evaluation settings, free-form tagging and iterative tagging, to assess
different verbalized confidence estimation methods. Experiments on three
long-form QA datasets show that our RL-trained models achieve better
calibration and generalize robustly across domains. Also, our method is highly
efficient, as it only requires adding a few tokens to the output being decoded.

</details>


### [52] [Probing Association Biases in LLM Moderation Over-Sensitivity](https://arxiv.org/abs/2505.23914)
*Yuxin Wang,Botao Yu,Ivory Yang,Saeed Hassanpour,Soroush Vosoughi*

Key words: 大型语言模型、内容审核、主题偏见、隐含关联、过度敏感

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 大型语言模型在内容审核中常将良性评论误判为有害，研究发现其隐含的主题偏见是潜在原因，并引入主题关联分析量化这一问题。

Motivation: 解决大型语言模型在内容审核中的过度敏感问题，尤其是误判良性评论为有害的现象。

Method: 提出主题关联分析方法，通过生成自由形式的情景想象并分析主题放大水平，量化模型对主题的隐含关联。

Result: 更先进的模型（如GPT-4 Turbo）展现出更强的主题刻板印象，尽管总体误报率较低。

Conclusion: 大型语言模型的审核决策不仅基于显性攻击性语言，还依赖于学习到的主题关联，需改进基于关键词的过滤方法。

Abstract: Large Language Models are widely used for content moderation but often
misclassify benign comments as toxic, leading to over-sensitivity. While
previous research attributes this issue primarily to the presence of offensive
terms, we reveal a potential cause beyond token level: LLMs exhibit systematic
topic biases in their implicit associations. Inspired by cognitive psychology's
implicit association tests, we introduce Topic Association Analysis, a
semantic-level approach to quantify how LLMs associate certain topics with
toxicity. By prompting LLMs to generate free-form scenario imagination for
misclassified benign comments and analyzing their topic amplification levels,
we find that more advanced models (e.g., GPT-4 Turbo) demonstrate stronger
topic stereotype despite lower overall false positive rates. These biases
suggest that LLMs do not merely react to explicit, offensive language but rely
on learned topic associations, shaping their moderation decisions. Our findings
highlight the need for refinement beyond keyword-based filtering, providing
insights into the underlying mechanisms driving LLM over-sensitivity.

</details>


### [53] [ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents](https://arxiv.org/abs/2505.23923)
*Feiteng Fang,Ting-En Lin,Yuchuan Wu,Xiong Liu,Xiang Huang,Dingwei Chen,Jing Ye,Haonan Zhang,Liang Zhu,Hamid Alinejad-Rokny,Min Yang,Fei Huang,Yongbin Li*

Key words: ChARM, RPLA, 奖励模型, 自适应边距, 自我进化, RoleplayPref, RoleplayEval

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了ChARM模型，通过自适应边距和自我进化机制改进角色扮演语言代理的奖励建模，提升了偏好排名13%，并在多个评测中取得最优结果。

Motivation: 传统奖励模型在可扩展性和适应主观对话偏好方面存在不足，需要更高效的解决方案。

Method: 提出ChARM模型，包含（1）自适应边距提升学习效率，（2）利用大规模未标注数据的自我进化机制，并构建了首个RPLA专用数据集RoleplayPref和评测基准RoleplayEval。

Result: 实验显示，ChARM在偏好排名上优于传统方法13%，并在CharacterEval和RoleplayEval上达到最优性能。

Conclusion: ChARM通过技术创新和专用数据集，显著提升了角色扮演语言代理的交互效果。

Abstract: Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic
and engaging human-computer interactions. However, traditional reward models
often struggle with scalability and adapting to subjective conversational
preferences. We propose ChARM, a Character-based Act-adaptive Reward Model,
addressing these challenges through two innovations: (1) an act-adaptive margin
that significantly enhances learning efficiency and generalizability, and (2) a
self-evolution mechanism leveraging large-scale unlabeled data to improve
training coverage. Additionally, we introduce RoleplayPref, the first
large-scale preference dataset specifically for RPLAs, featuring 1,108
characters, 13 subcategories, and 16,888 bilingual dialogues, alongside
RoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13%
improvement over the conventional Bradley-Terry model in preference rankings.
Furthermore, applying ChARM-generated rewards to preference learning techniques
(e.g., direct preference optimization) achieves state-of-the-art results on
CharacterEval and RoleplayEval. Code and dataset are available at
https://github.com/calubkk/ChARM.

</details>


### [54] [Scaling up the think-aloud method](https://arxiv.org/abs/2505.23931)
*Daniel Wurgaft,Ben Prystawski,Kanishk Gandhi,Cedegao E. Zhang,Joshua B. Tenenbaum,Noah D. Goodman*

Key words: 思考口语方法, 自然语言处理, 自动化转录, 推理过程, 大规模分析

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种自动化转录和标注思考口语数据的方法，利用自然语言处理工具，实现了大规模分析。

Motivation: 思考口语方法是研究人类推理过程的宝贵数据来源，但由于人工转录和标注的高成本，样本量受限，导致其流行度下降。

Method: 开发了自然语言处理工具来自动转录和标注口头推理报告，并在640名参与者进行数学推理任务（24点游戏）时收集数据。

Result: 自动转录和标注的搜索图谱与人类标注者达成中等一致性，成功展示了大规模思考口语数据的价值。

Conclusion: 论文证明了自动化分析口头报告的可行性，并凸显了大规模思考口语数据的研究潜力。

Abstract: The think-aloud method, where participants voice their thoughts as they solve
a task, is a valuable source of rich data about human reasoning processes. Yet,
it has declined in popularity in contemporary cognitive science, largely
because labor-intensive transcription and annotation preclude large sample
sizes. Here, we develop methods to automate the transcription and annotation of
verbal reports of reasoning using natural language processing tools, allowing
for large-scale analysis of think-aloud data. In our study, 640 participants
thought aloud while playing the Game of 24, a mathematical reasoning task. We
automatically transcribed the recordings and coded the transcripts as search
graphs, finding moderate inter-rater reliability with humans. We analyze these
graphs and characterize consistency and variation in human reasoning traces.
Our work demonstrates the value of think-aloud data at scale and serves as a
proof of concept for the automated analysis of verbal reports.

</details>


### [55] [SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving](https://arxiv.org/abs/2505.23932)
*Wendong Xu,Jing Xiong,Chenyang Zhao,Qiujiang Chen,Haoran Wang,Hui Shen,Zhongwei Wan,Jianbo Dai,Taiqiang Wu,He Xiao,Chaofan Tao,Z. Morley Mao,Ying Sheng,Zhijiang Guo,Hongxia Yang,Bei Yu,Lingpeng Kong,Quanquan Gu,Ngai Wong*

Key words: SwingArena, LLM评估, 软件开发, 检索增强代码生成, 持续集成

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SwingArena是一个评估大型语言模型（LLM）的竞争性框架，模拟真实软件开发流程，通过提交者和审阅者的协作迭代过程，结合检索增强代码生成技术，支持多语言任务。

Motivation: 传统静态基准测试无法真实反映LLM在协作式软件开发中的表现，因此设计SwingArena以模拟实际开发迭代流程。

Method: 框架通过LLM角色分工（提交补丁与验证测试），并引入检索增强代码生成模块处理长上下文任务，覆盖多种编程语言。

Result: 实验基于400多个GitHub问题，结果显示不同模型在补丁生成和CI验证中有不同优势（如GPT-4o偏向激进补丁，DeepSeek/Gemini注重正确性）。

Conclusion: SwingArena为LLM在真实CI驱动开发环境中的评估提供了可扩展的方法论。

Abstract: We present SwingArena, a competitive evaluation framework for Large Language
Models (LLMs) that closely mirrors real-world software development workflows.
Unlike traditional static benchmarks, SwingArena models the collaborative
process of software iteration by pairing LLMs as submitters, who generate
patches, and reviewers, who create test cases and verify the patches through
continuous integration (CI) pipelines. To support these interactive
evaluations, we introduce a retrieval-augmented code generation (RACG) module
that efficiently handles long-context challenges by providing syntactically and
semantically relevant code snippets from large codebases, supporting multiple
programming languages (C++, Python, Rust, and Go). This enables the framework
to scale across diverse tasks and contexts while respecting token limitations.
Our experiments, using over 400 high-quality real-world GitHub issues selected
from a pool of 2,300 issues, show that models like GPT-4o excel at aggressive
patch generation, whereas DeepSeek and Gemini prioritize correctness in CI
validation. SwingArena presents a scalable and extensible methodology for
evaluating LLMs in realistic, CI-driven software development settings. More
details are available on our project page: swing-bench.github.io

</details>


### [56] [Retrieval Augmented Generation based Large Language Models for Causality Mining](https://arxiv.org/abs/2505.23944)
*Thushara Manjari Naduvilakandy,Hyeju Jang,Mohammad Al Hasan*

Key words: 因果关系检测, 检索增强生成, 大型语言模型, 动态提示, 知识提取

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种基于检索增强生成（RAG）的动态提示方案，以提高大型语言模型（LLM）在因果关系检测与挖掘任务中的性能，并在多个数据集和LLM上验证其有效性。

Motivation: 现有无监督方法性能较差且需要人工干预，监督方法因缺乏大型训练数据集而受限，而LLM结合提示工程虽有一定效果，但缺乏全面研究。本文旨在填补这一空白。

Method: 采用RAG框架设计动态提示方案，结合LLM进行因果关系检测与挖掘。

Result: 在三个数据集和五种LLM上的实验表明，所提出的动态提示方案优于静态提示方法。

Conclusion: RAG动态提示显著提升了LLM在因果关系任务中的表现，为未来研究提供了方向。

Abstract: Causality detection and mining are important tasks in information retrieval
due to their enormous use in information extraction, and knowledge graph
construction. To solve these tasks, in existing literature there exist several
solutions -- both unsupervised and supervised. However, the unsupervised
methods suffer from poor performance and they often require significant human
intervention for causal rule selection, leading to poor generalization across
different domains. On the other hand, supervised methods suffer from the lack
of large training datasets. Recently, large language models (LLMs) with
effective prompt engineering are found to be effective to overcome the issue of
unavailability of large training dataset. Yet, in existing literature, there
does not exist comprehensive works on causality detection and mining using LLM
prompting. In this paper, we present several retrieval-augmented generation
(RAG) based dynamic prompting schemes to enhance LLM performance in causality
detection and extraction tasks. Extensive experiments over three datasets and
five LLMs validate the superiority of our proposed RAG-based dynamic prompting
over other static prompting schemes.

</details>


### [57] [A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models](https://arxiv.org/abs/2505.23945)
*Sriram Balasubramanian,Samyadeep Basu,Soheil Feizi*

Key words: Chain-of-thought, 大型视觉语言模型, 忠实性, 偏差表达, 推理评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文全面研究了大型视觉语言模型（LVLM）中的Chain-of-thought（CoT）忠实性问题，揭示了文本和图像偏差对推理的影响，并提出了一种新的细粒度评估方法，发现模型在处理不同类型偏差时的关键差异。

Motivation: 探究CoT推理是否真实反映模型内部过程，尤其是视觉语言模型中文本和图像偏差对推理和偏差表达的影响。

Method: 引入了一种新颖的细粒度评估流程，用于分类偏差表达模式，实现了比以往方法更精确的CoT推理分析。

Result: 研究发现细微的图像偏差很少被表达，且许多模型表现出‘不一致’推理现象。同时，纯语言推理模型在表达非明示线索时仍存在困难。

Conclusion: 研究强调了CoT推理中的忠实性问题，为未来模型设计和评估提供了重要参考。

Abstract: Chain-of-thought (CoT) reasoning enhances performance of large language
models, but questions remain about whether these reasoning traces faithfully
reflect the internal processes of the model. We present the first comprehensive
study of CoT faithfulness in large vision-language models (LVLMs),
investigating how both text-based and previously unexplored image-based biases
affect reasoning and bias articulation. Our work introduces a novel,
fine-grained evaluation pipeline for categorizing bias articulation patterns,
enabling significantly more precise analysis of CoT reasoning than previous
methods. This framework reveals critical distinctions in how models process and
respond to different types of biases, providing new insights into LVLM CoT
faithfulness. Our findings reveal that subtle image-based biases are rarely
articulated compared to explicit text-based ones, even in models specialized
for reasoning. Additionally, many models exhibit a previously unidentified
phenomenon we term ``inconsistent'' reasoning - correctly reasoning before
abruptly changing answers, serving as a potential canary for detecting biased
reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to
revisit CoT faithfulness in LLMs across various levels of implicit cues. Our
findings reveal that current language-only reasoning models continue to
struggle with articulating cues that are not overtly stated.

</details>


### [58] [FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression](https://arxiv.org/abs/2505.23966)
*Jiayi Tian,Ryan Solgi,Jinming Lu,Yifan Yang,Hai Li,Zheng Zhang*

Key words: LLMs, 低秩分解, PCA, 模型压缩, 推理加速

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: FLAT-LLM introduces一种无需训练的细粒度低秩变换方法，通过PCA对权重进行压缩，显著提升了推理速度并保持了模型性能，适用于资源受限环境。

Motivation: 解决LLMs在资源受限环境中部署时的高计算和内存需求问题，同时避免传统压缩方法导致的精度下降和效率低下。

Method: 基于PCA的细粒度低秩变换，减少隐藏维度并通过重要性指标自适应分配各解码器的秩，无需微调即可完成校准。

Result: 在4个模型和11个数据集上验证，FLAT-LLM在泛化性和下游任务表现上优于基线方法，并实现了推理加速。

Conclusion: FLAT-LLM为LLMs提供了一种高效、准确的压缩方案，适用于实际部署环境。

Abstract: Large Language Models (LLMs) have enabled remarkable progress in natural
language processing, yet their high computational and memory demands pose
challenges for deployment in resource-constrained environments. Although recent
low-rank decomposition methods offer a promising path for structural
compression, they often suffer from accuracy degradation, expensive calibration
procedures, and result in inefficient model architectures that hinder
real-world inference speedups. In this paper, we propose FLAT-LLM, a fast and
accurate, training-free structural compression method based on fine-grained
low-rank transformations in the activation space. Specifically, we reduce the
hidden dimension by transforming the weights using truncated eigenvectors
computed via head-wise Principal Component Analysis (PCA), and employ an
importance-based metric to adaptively allocate ranks across decoders. FLAT-LLM
achieves efficient and effective weight compression without recovery
fine-tuning, which could complete the calibration within a few minutes.
Evaluated across 4 models and 11 datasets, FLAT-LLM outperforms structural
pruning baselines in generalization and downstream performance, while
delivering inference speedups over decomposition-based methods.

</details>


### [59] [Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs](https://arxiv.org/abs/2505.23996)
*Yinong Oliver Wang,Nivedha Sivakumar,Falaah Arif Khan,Rin Metcalf Susa,Adam Golinski,Natalie Mackraz,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Key words: 公平性，大语言模型，不确定性，UCerF，性别-职业数据集

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种考虑模型不确定性的公平性度量UCerF和新的性别-职业公平性评估数据集，用于更细致地评估大语言模型的公平性。

Motivation: 现有公平性度量主要基于离散的准确性评估，未能捕捉模型不确定性对公平性的隐含影响，因此需要更全面的评估方法。

Method: 提出了UCerF度量，引入模型不确定性评估公平性，并构建了一个包含31,756个样本的性别-职业公平性评估数据集。

Result: 通过UCerF和数据集评估十个开源大语言模型，发现如Mistral-7B等模型在公平性上存在问题，而传统度量未能捕捉这些细节。

Conclusion: UCerF和新的评估数据集为开发更透明、可问责的AI系统提供了工具。

Abstract: The recent rapid adoption of large language models (LLMs) highlights the
critical need for benchmarking their fairness. Conventional fairness metrics,
which focus on discrete accuracy-based evaluations (i.e., prediction
correctness), fail to capture the implicit impact of model uncertainty (e.g.,
higher model confidence about one group over another despite similar accuracy).
To address this limitation, we propose an uncertainty-aware fairness metric,
UCerF, to enable a fine-grained evaluation of model fairness that is more
reflective of the internal bias in model decisions compared to conventional
fairness measures. Furthermore, observing data size, diversity, and clarity
issues in current datasets, we introduce a new gender-occupation fairness
evaluation dataset with 31,756 samples for co-reference resolution, offering a
more diverse and suitable dataset for evaluating modern LLMs. We establish a
benchmark, using our metric and dataset, and apply it to evaluate the behavior
of ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness
due to high confidence in incorrect predictions, a detail overlooked by
Equalized Odds but captured by UCerF. Overall, our proposed LLM benchmark,
which evaluates fairness with uncertainty awareness, paves the way for
developing more transparent and accountable AI systems.

</details>


### [60] [Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws](https://arxiv.org/abs/2505.24009)
*Hidetaka Kamigaito,Ying Zhang,Jingun Kwon,Katsuhiko Hayashi,Manabu Okumura,Taro Watanabe*

Key words: Transformer, 参数缩放, 多样性, 次模性, 大语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文通过理论分析和实验验证，探讨了Transformer模型中层数增加与性能提升的关系，发现层间多样性的重要性，并揭示了性能增益的次模性特征。

Motivation: 尽管参数缩放定律表明增加Transformer的参数规模能提升性能，但其内部机制与参数缩放的关系尚不明确。本文旨在通过研究层及其大小对性能的影响，填补这一空白。

Method: 通过偏差-多样性分解理论分析Transformer的各层输出，并引入信息论多样性。实验在多种语义理解任务和不同规模的LLM上进行。

Result: 研究发现，当各层输出接近正确答案且保持多样性时，性能提升显著，尤其是当单层输出远离真实值时。增加层数仅在层间行为不同时有效，且性能增益呈现次模性。

Conclusion: 层间多样性是Transformer性能提升的关键因素，性能增益随层数增加而递减，与参数缩放定律的预测一致。

Abstract: Transformers deliver outstanding performance across a wide range of tasks and
are now a dominant backbone architecture for large language models (LLMs).
Their task-solving performance is improved by increasing parameter size, as
shown in the recent studies on parameter scaling laws. Although recent
mechanistic-interpretability studies have deepened our understanding of the
internal behavior of Transformers by analyzing their residual stream, the
relationship between these internal mechanisms and the parameter scaling laws
remains unclear. To bridge this gap, we focus on layers and their size, which
mainly decide the parameter size of Transformers. For this purpose, we first
theoretically investigate the layers within the residual stream through a
bias-diversity decomposition. The decomposition separates (i) bias, the error
of each layer's output from the ground truth, and (ii) diversity, which
indicates how much the outputs of each layer differ from each other. Analyzing
Transformers under this theory reveals that performance improves when
individual layers make predictions close to the correct answer and remain
mutually diverse. We show that diversity becomes especially critical when
individual layers' outputs are far from the ground truth. Finally, we introduce
an information-theoretic diversity and show our main findings that adding
layers enhances performance only when those layers behave differently, i.e.,
are diverse. We also reveal the performance gains from increasing the number of
layers exhibit submodularity: marginal improvements diminish as additional
layers increase, mirroring the logarithmic convergence predicted by the
parameter scaling laws. Experiments on multiple semantic-understanding tasks
with various LLMs empirically confirm the theoretical properties derived in
this study.

</details>


### [61] [Large Language Model Meets Constraint Propagation](https://arxiv.org/abs/2505.24012)
*Alexandre Bonlarron,Florian Régin,Elisabetta De Maria,Jean-Charles Régin*

Key words: LLM, CP, CSP, MLM, 约束感知生成

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: GenCP通过结合LLM和CP推理，将文本生成建模为CSP问题，集成MLM实现双向约束传播，显著提升约束感知文本生成的性能。

Motivation: 解决LLM在生成文本时难以强制外部约束的问题，通过结合CP推理提供显式控制。

Method: 集成MLM进行域生成，支持双向约束传播，将文本生成建模为CSP问题。

Result: 在COLLIE基准测试中表现优异，有效提升约束感知文本生成的效率和可行性。

Conclusion: 结合MLM的GenCP显著提升了LLM在严格约束任务中的生成能力，尽管增加了部分计算成本。

Abstract: Large Language Models (LLMs) excel at generating fluent text but struggle to
enforce external constraints because they generate tokens sequentially without
explicit control mechanisms. GenCP addresses this limitation by combining LLM
predictions with Constraint Programming (CP) reasoning, formulating text
generation as a Constraint Satisfaction Problem (CSP). In this paper, we
improve GenCP by integrating Masked Language Models (MLMs) for domain
generation, which allows bidirectional constraint propagation that leverages
both past and future tokens. This integration bridges the gap between
token-level prediction and structured constraint enforcement, leading to more
reliable and constraint-aware text generation. Our evaluation on COLLIE
benchmarks demonstrates that incorporating domain preview via MLM calls
significantly improves GenCP's performance. Although this approach incurs
additional MLM calls and, in some cases, increased backtracking, the overall
effect is a more efficient use of LLM inferences and an enhanced ability to
generate feasible and meaningful solutions, particularly in tasks with strict
content constraints.

</details>


### [62] [BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech Translation System](https://arxiv.org/abs/2505.24016)
*Matthew Raffel,Victor Agostinelli,Lizhong Chen*

Key words: BeaverTalk, 语音到文本翻译, 同声传译, Whisper, Gemma, LoRAs

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文介绍了BeaverTalk系统的构建、微调和部署，这是一个用于语音转文本翻译的级联系统，参与了IWSLT 2025同声传译任务。系统采用VAD分段器、Whisper Large V2和Gemma 3 12B进行ASR和同声翻译，在英德和英中任务中表现优异。

Motivation: 旨在构建一个高效的语音到文本翻译系统，参与IWSLT 2025同声传译任务，实现低延迟和高准确率的翻译。

Method: 系统级联了VAD分段器、Whisper Large V2（ASR）和Gemma 3 12B（翻译模型），通过LoRAs微调翻译模型，采用对话式提示策略利用源语言的前句记忆库。

Result: 在英德任务中，BLEU得分为24.64和27.83，StreamLAAL为1837.86和3343.73；在英中任务中，BLEU得分为34.07和37.23，StreamLAAL为2216.99和3521.35。

Conclusion: BeaverTalk系统在低延迟和高延迟场景下均表现良好，验证了级联设计和LoRAs微调的有效性。

Abstract: This paper discusses the construction, fine-tuning, and deployment of
BeaverTalk, a cascaded system for speech-to-text translation as part of the
IWSLT 2025 simultaneous translation task. The system architecture employs a VAD
segmenter for breaking a speech stream into segments, Whisper Large V2 for
automatic speech recognition (ASR), and Gemma 3 12B for simultaneous
translation. Regarding the simultaneous translation LLM, it is fine-tuned via
low-rank adaptors (LoRAs) for a conversational prompting strategy that
leverages a single prior-sentence memory bank from the source language as
context. The cascaded system participated in the English$\rightarrow$German and
English$\rightarrow$Chinese language directions for both the low and high
latency regimes. In particular, on the English$\rightarrow$German task, the
system achieves a BLEU of 24.64 and 27.83 at a StreamLAAL of 1837.86 and
3343.73, respectively. Then, on the English$\rightarrow$Chinese task, the
system achieves a BLEU of 34.07 and 37.23 at a StreamLAAL of 2216.99 and
3521.35, respectively.

</details>


### [63] [Hidden Persuasion: Detecting Manipulative Narratives on Social Media During the 2022 Russian Invasion of Ukraine](https://arxiv.org/abs/2505.24028)
*Kateryna Akhynko,Oleksandr Kosovan,Mykola Trokhymovych*

Key words: UNLP 2025, 社交媒体操纵, Gemma 2, LoRA, XLM-RoBERTa

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了在UNLP 2025共享任务中表现优异的解决方案，专注于检测乌克兰Telegram用户中的修辞和风格操纵技术。

Motivation: 任务目标是检测和分类社交媒体中用于影响用户的操纵技术，特别是针对乌克兰Telegram用户。

Method: 使用Gemma 2语言模型结合LoRA适配器进行微调，并应用元特征和阈值优化的二级分类器；在跨度检测中采用XLM-RoBERTa模型进行多目标训练。

Result: 该方案在分类任务中排名第二，在跨度检测任务中排名第三。

Conclusion: 方法在检测社交媒体操纵技术方面表现优异，特别是在多语言和多任务场景下。

Abstract: This paper presents one of the top-performing solutions to the UNLP 2025
Shared Task on Detecting Manipulation in Social Media. The task focuses on
detecting and classifying rhetorical and stylistic manipulation techniques used
to influence Ukrainian Telegram users. For the classification subtask, we
fine-tuned the Gemma 2 language model with LoRA adapters and applied a
second-level classifier leveraging meta-features and threshold optimization.
For span detection, we employed an XLM-RoBERTa model trained for multi-target,
including token binary classification. Our approach achieved 2nd place in
classification and 3rd place in span detection.

</details>


### [64] [The Surprising Soupability of Documents in State Space Models](https://arxiv.org/abs/2505.24033)
*Yasaman Jafari,Zixian Wang,Leon Bergen,Taylor Berg-Kirkpatrick*

Key words: 结构化状态空间模型, 文档池化, 多跳问答, Mamba2, 稀疏检索

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨了结构化状态空间模型（SSMs）的隐藏状态是否可以通过事后合并来支持下游推理。提出了一种称为“文档池化”的策略，通过简单操作（如平均）将独立编码的文档表示合并为单一上下文状态，从而实现模块化编码和重用，无需为每个查询重新处理完整输入。在Mamba2模型上微调以生成可池化表示，结果显示其在多跳问答、稀疏检索和长文档推理中表现优异。

Motivation: 现有的跨编码器方法在处理多文档查询时需要重新处理整个输入，效率较低。为了提升效率和模块化编码的可行性，研究提出了文档池化策略。

Method: 采用结构化状态空间模型（SSMs）独立编码文档，并通过简单操作（如平均）将表示池化为单一上下文状态。在Mamba2模型上进行微调以生成可池化的表示。

Result: 在HotpotQA数据集上，池化十个独立编码的文档的表现接近跨编码器在相同输入上的性能。该方法在多跳问答、稀疏检索和长文档推理中表现出色。

Conclusion: 文档池化策略提供了一种高效且模块化的方法，显著减少了计算开销，同时保持了较高的推理准确性。

Abstract: We investigate whether hidden states from Structured State Space Models
(SSMs) can be merged post-hoc to support downstream reasoning. Inspired by
model souping, we propose a strategy where documents are encoded independently
and their representations are pooled -- via simple operations like averaging --
into a single context state. This approach, which we call document souping,
enables modular encoding and reuse without reprocessing the full input for each
query. We finetune Mamba2 models to produce soupable representations and find
that they support multi-hop QA, sparse retrieval, and long-document reasoning
with strong accuracy. On HotpotQA, souping ten independently encoded documents
nearly matches the performance of a cross-encoder trained on the same inputs.

</details>


### [65] [MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering](https://arxiv.org/abs/2505.24040)
*Yuexing Hao,Kumail Alhamoud,Hyewon Jeong,Haoran Zhang,Isha Puri,Philip Torr,Mike Schaekermann,Ariel D. Stern,Marzyeh Ghassemi*

Key words: 大型语言模型,医学问答,相关性判断,MedPAIR数据集

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究通过MedPAIR数据集比较了医学生和大型语言模型在回答医学问题时对信息的相关性判断差异，发现两者在内容相关性上常不一致，但过滤掉医学生标记为无关的句子后，两者准确性均有提升。

Motivation: 尽管大型语言模型在医学问答任务中表现优异，但其正确的答案可能基于错误的逻辑。研究旨在通过MedPAIR数据集评估医学生和模型在回答问题时的信息相关性判断差异。

Method: 研究收集了36位医学生对1,300个医学问答对的标注数据，标记每个句子在问题中的相关性，并比较模型与医学生的相关性判断及其对任务表现的影响。

Result: 大型语言模型与医学生在内容相关性判断上常不一致，但去除医学生标记的无关句子后，两者的回答准确性均提高。

Conclusion: 通过相关性过滤可提升模型和医学生的回答准确性，数据已公开。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance on
various medical question-answering (QA) benchmarks, including standardized
medical exams. However, correct answers alone do not ensure correct logic, and
models may reach accurate conclusions through flawed processes. In this study,
we introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance
Estimation and Question Answering) dataset to evaluate how physician trainees
and LLMs prioritize relevant information when answering QA questions. We obtain
annotations on 1,300 QA pairs from 36 physician trainees, labeling each
sentence within the question components for relevance. We compare these
relevance estimates to those for LLMs, and further evaluate the impact of these
"relevant" subsets on downstream task performance for both physician trainees
and LLMs. We find that LLMs are frequently not aligned with the content
relevance estimates of physician trainees. After filtering out physician
trainee-labeled irrelevant sentences, accuracy improves for both the trainees
and the LLMs. All LLM and physician trainee-labeled data are available at:
http://medpair.csail.mit.edu/.

</details>


### [66] [TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional Chinese Medicine](https://arxiv.org/abs/2505.24063)
*Jiacheng Xie,Yang Yu,Ziyang Zhang,Shuai Zeng,Jiaxuan He,Ayush Vasireddy,Xiaoting Tang,Congyu Guo,Lening Zhao,Congcong Jing,Guanghui An,Dong Xu*

Key words: 中医大语言模型、多模态问答、评估框架、TCM-Ladder、Ladder-Score

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了TCM-Ladder，首个针对中医大语言模型的多模态问答数据集，填补了现有评估数据集的不足，并设计了Ladder-Score评估方法。

Motivation: 解决现有中医大语言模型评估数据集范围有限、缺乏统一多模态基准的问题。

Method: 开发TCM-Ladder数据集，包含多种题型和多模态内容，并提出Ladder-Score评估方法。

Result: 在TCM-Ladder上测试了9个通用领域和5个中医专用的大语言模型，证明了数据集的有效性。

Conclusion: TCM-Ladder为中医大语言模型提供了标准化评估基准，推动了该领域的研究。

Abstract: Traditional Chinese Medicine (TCM), as an effective alternative medicine, has
been receiving increasing attention. In recent years, the rapid development of
large language models (LLMs) tailored for TCM has underscored the need for an
objective and comprehensive evaluation framework to assess their performance on
real-world tasks. However, existing evaluation datasets are limited in scope
and primarily text-based, lacking a unified and standardized multimodal
question-answering (QA) benchmark. To address this issue, we introduce
TCM-Ladder, the first multimodal QA dataset specifically designed for
evaluating large TCM language models. The dataset spans multiple core
disciplines of TCM, including fundamental theory, diagnostics, herbal formulas,
internal medicine, surgery, pharmacognosy, and pediatrics. In addition to
textual content, TCM-Ladder incorporates various modalities such as images and
videos. The datasets were constructed using a combination of automated and
manual filtering processes and comprise 52,000+ questions in total. These
questions include single-choice, multiple-choice, fill-in-the-blank, diagnostic
dialogue, and visual comprehension tasks. We trained a reasoning model on
TCM-Ladder and conducted comparative experiments against 9 state-of-the-art
general domain and 5 leading TCM-specific LLMs to evaluate their performance on
the datasets. Moreover, we propose Ladder-Score, an evaluation method
specifically designed for TCM question answering that effectively assesses
answer quality regarding terminology usage and semantic expression. To our
knowledge, this is the first work to evaluate mainstream general domain and
TCM-specific LLMs on a unified multimodal benchmark. The datasets and
leaderboard are publicly available at https://tcmladder.com or
https://54.211.107.106 and will be continuously updated.

</details>


### [67] [HardTests: Synthesizing High-Quality Test Cases for LLM Coding](https://arxiv.org/abs/2505.24098)
*Zhongmou He,Yee Man Choi,Kexun Zhang,Jiabao Ji,Junting Zhou,Dejia Xu,Ivan Bercovich,Aidan Zhang,Lei Li*

Key words: 大语言模型, 验证器, 测试合成, 竞争编程, 代码评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了HARDTESTGEN，一个利用大语言模型（LLM）生成高质量测试用例的流程，并构建了包含47k个问题的竞争编程数据集HARDTESTS。对比现有测试用例，HARDTESTGEN在评估LLM生成的代码时表现出更高的精确度和召回率，且对更难题目的改进尤为显著。

Motivation: 当前可靠的验证器在复杂编码问题中难以获取，因为精心伪装错误解法往往需要人工编写的边缘测试用例，而这些用例难以合成。

Method: 提出HARDTESTGEN流程，利用LLM合成高质量测试用例，构建HARDTESTS数据集。

Result: HARDTESTGEN测试用例在评估LLM生成代码时，精确度提高11.3个百分点，召回率提高17.5个百分点；对更难题目的改进可达40个百分点。

Conclusion: HARDTESTGEN能高效生成高质量测试用例，显著提升代码评估效果，并有助于下游代码生成性能。

Abstract: Verifiers play a crucial role in large language model (LLM) reasoning, needed
by post-training techniques such as reinforcement learning. However, reliable
verifiers are hard to get for difficult coding problems, because a
well-disguised wrong solution may only be detected by carefully human-written
edge cases that are difficult to synthesize. To address this issue, we propose
HARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this
pipeline, we curate a comprehensive competitive programming dataset HARDTESTS
with 47k problems and synthetic high-quality tests. Compared with existing
tests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points
higher and recall that is 17.5 percentage points higher when evaluating
LLM-generated code. For harder problems, the improvement in precision can be as
large as 40 points. HARDTESTS also proves to be more effective for model
training, measured by downstream code generation performance. We will
open-source our dataset and synthesis pipeline at
https://leililab.github.io/HardTests/.

</details>


### [68] [Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning](https://arxiv.org/abs/2505.24105)
*Jiacheng Lin,Zhenbang Wu,Jimeng Sun*

Key words: EHRMIND, 大型语言模型, 强化学习, 临床推理, 电子健康记录

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: EHRMIND提出了一种通过强化学习（RLVR）调整大型语言模型（LLMs）以适应复杂临床推理任务的方法，解决了医疗领域的知识错误和缺失问题，并在多个临床应用中验证了其有效性。

Motivation: 传统RLVR方法在数学和编码领域表现良好，但在医疗领域因需要专业知识和复杂推理而面临挑战，特别是电子健康记录（EHR）解析中的知识错误和缺失问题。

Method: 采用两阶段方法：首先通过轻量级监督微调（SFT）注入缺失知识并稳定训练，随后利用RLVR强化模型决策和输出正确性。

Result: 在MEDCALC、TREC CLINICAL TRIALS和EHRSHOT等任务中，EHRMIND在准确性、可解释性和跨任务泛化能力方面均取得显著提升。

Conclusion: 该研究为RLVR在医疗领域的应用提供了实用指导，验证了其增强LLMs能力的有效性。

Abstract: We present EHRMIND, a practical recipe for adapting large language models
(LLMs) to complex clinical reasoning tasks using reinforcement learning with
verifiable rewards (RLVR). While RLVR has succeeded in mathematics and coding,
its application to healthcare contexts presents unique challenges due to the
specialized knowledge and reasoning required for electronic health record (EHR)
interpretation. Our pilot study on the MEDCALC benchmark reveals two key
failure modes: (1) misapplied knowledge, where models possess relevant medical
knowledge but apply it incorrectly, and (2) missing knowledge, where models
lack essential domain knowledge. To address these cases, EHRMIND applies a
two-stage solution: a lightweight supervised fine-tuning (SFT) warm-up that
injects missing domain knowledge, stabilizes subsequent training, and
encourages structured, interpretable outputs; followed by RLVR, which
reinforces outcome correctness and refines the model's decision-making. We
demonstrate the effectiveness of our method across diverse clinical
applications, including medical calculations (MEDCALC), patient-trial matching
(TREC CLINICAL TRIALS), and disease diagnosis (EHRSHOT). EHRMIND delivers
consistent gains in accuracy, interpretability, and cross-task generalization.
These findings offer practical guidance for applying RLVR to enhance LLM
capabilities in healthcare settings.

</details>


### [69] [The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It](https://arxiv.org/abs/2505.24119)
*Zheng-Xin Yong,Beyza Ermis,Marzieh Fadaee,Stephen H. Bach,Julia Kreutzer*

Key words: LLM安全、语言多样性、多语言研究、AI包容性、跨语言泛化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: The paper highlights the English-centric bias in LLM安全研究，通过系统分析发现非英语语言的研究极少，并提出未来应关注多语言安全研究方向。

Motivation: 研究旨在揭露LLM安全研究中的语言多样性不足问题，强调非英语语言被忽视的现状，以推动更包容的多语言安全研究发展。

Method: 对2020--2024年间近300篇*ACL会议及研讨会的论文进行系统性分析，聚焦语言覆盖与研究实践。

Result: 发现LLM安全研究高度集中于英语，非英语语言（即便是高资源语言）极少被独立研究，且英语研究缺乏语言记录规范。

Conclusion: 呼吁未来研究应关注多语言安全评估、训练数据生成和跨语言安全泛化三大方向，以建立更具包容性的AI安全实践。

Abstract: This paper presents a comprehensive analysis of the linguistic diversity of
LLM safety research, highlighting the English-centric nature of the field.
Through a systematic review of nearly 300 publications from 2020--2024 across
major NLP conferences and workshops at *ACL, we identify a significant and
growing language gap in LLM safety research, with even high-resource
non-English languages receiving minimal attention. We further observe that
non-English languages are rarely studied as a standalone language and that
English safety research exhibits poor language documentation practice. To
motivate future research into multilingual safety, we make several
recommendations based on our survey, and we then pose three concrete future
directions on safety evaluation, training data generation, and crosslingual
safety generalization. Based on our survey and proposed directions, the field
can develop more robust, inclusive AI safety practices for diverse global
populations.

</details>


### [70] [R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration](https://arxiv.org/abs/2505.24133)
*Zefan Cai,Wen Xiao,Hanshi Sun,Cheng Luo,Yikai Zhang,Ke Wan,Yucheng Li,Yeyang Zhou,Li-Wen Chang,Jiuxiang Gu,Zhen Dong,Anima Anandkumar,Abedelkadir Asi,Junjie Hu*

Key words: KV缓存压缩, 推理模型, 冗余感知, 性能优化, 内存节省

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: R-KV提出了一种针对推理模型的冗余感知KV缓存压缩方法，显著提升了性能并减少了内存占用。

Motivation: 现有推理模型在KV缓存压缩中表现不佳，导致推理失败或性能下降，因此需要一种新方法来高效压缩KV缓存同时保持性能。

Method: 提出R-KV方法，通过识别和去除冗余令牌，实现KV缓存的高效压缩。

Result: R-KV仅使用10%的KV缓存即可保留近100%的性能，内存节省90%，吞吐量提升6.6倍。

Conclusion: R-KV在数学推理数据集上表现优异，显著优于现有基线方法。

Abstract: Reasoning models have demonstrated impressive performance in self-reflection
and chain-of-thought reasoning. However, they often produce excessively long
outputs, leading to prohibitively large key-value (KV) caches during inference.
While chain-of-thought inference significantly improves performance on complex
reasoning tasks, it can also lead to reasoning failures when deployed with
existing KV cache compression approaches. To address this, we propose
Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel
method specifically targeting redundant tokens in reasoning models. Our method
preserves nearly 100% of the full KV cache performance using only 10% of the KV
cache, substantially outperforming existing KV cache baselines, which reach
only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV
cache performance with 16% of the KV cache. This KV-cache reduction also leads
to a 90% memory saving and a 6.6X throughput over standard chain-of-thought
reasoning inference. Experimental results show that R-KV consistently
outperforms existing KV cache compression baselines across two mathematical
reasoning datasets.

</details>


### [71] [CrossICL: Cross-Task In-Context Learning via Unsupervised Demonstration Transfer](https://arxiv.org/abs/2505.24143)
*Jinglong Gao,Xiao Ding,Lingxiao Zou,Bing Qin,Ting Liu*

Key words: 上下文学习, 大型语言模型, CrossICL, 跨任务演示, 两阶段对齐

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种名为CrossICL的新范式，旨在利用源任务的演示来改进目标任务的语言模型表现，避免了手动提供演示的需求。通过两阶段对齐策略和大量实验，证明了该方法的有效性。

Motivation: 传统的上下文学习（ICL）依赖手动提供演示，但在实际场景中用户往往不愿意或无法提供。论文受人类类比启发，探索如何利用现有源任务的演示来优化目标任务的ICL表现。

Method: 采用两阶段对齐策略以减少跨任务差距的干扰，并在875个NLP任务和六种大型语言模型（包括GPT-4o）上进行了全面实验。

Result: 实验证明CrossICL有效，并提供了关于跨任务演示选择标准和任务差距干扰类型的重要见解。

Conclusion: CrossICL是一种无需额外手动努力的可靠方法，能够有效利用源任务演示改进目标任务表现。

Abstract: In-Context Learning (ICL) enhances the performance of large language models
(LLMs) with demonstrations. However, obtaining these demonstrations primarily
relies on manual effort. In most real-world scenarios, users are often
unwilling or unable to provide such demonstrations. Inspired by the human
analogy, we explore a new ICL paradigm CrossICL to study how to utilize
existing source task demonstrations in the ICL for target tasks, thereby
obtaining reliable guidance without any additional manual effort. To explore
this, we first design a two-stage alignment strategy to mitigate the
interference caused by gaps across tasks, as the foundation for our
experimental exploration. Based on it, we conduct comprehensive exploration of
CrossICL, with 875 NLP tasks from the Super-NI benchmark and six types of LLMs,
including GPT-4o. Experimental results demonstrate the effectiveness of
CrossICL and provide valuable insights on questions like the criteria for
selecting cross-task demonstrations, as well as the types of task-gap-induced
interference in CrossICL.

</details>


### [72] [Rationales Are Not Silver Bullets: Measuring the Impact of Rationales on Model Performance and Reliability](https://arxiv.org/abs/2505.24147)
*Chiwei Zhu,Benfeng Xu,An Yang,Junyang Lin,Quan Wang,Chang Zhou,Zhendong Mao*

Key words: 语言模型、理性训练、性能、可靠性、任务难度

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究表明，加入理性训练的语言模型并不总是有益的，有时会降低性能，但在某些情况下能提高可靠性。

Motivation: 探讨理性训练对语言模型性能的影响及其可靠性，挑战现有普遍观点。

Method: 通过全面的实验分析理性对模型性能及可靠性的影响，并比较未训练模型。

Result: 1) 理性训练可能降低性能；2) 可能提升可靠性甚至超越未训练模型；3) 性能与可靠性改进呈线性关系，与任务难度相关。

Conclusion: 理性使用需谨慎，任务难度是影响效果的关键因素。

Abstract: Training language models with rationales augmentation has been shown to be
beneficial in many existing works. In this paper, we identify that such a
prevailing view does not hold consistently. We conduct comprehensive
investigations to thoroughly inspect the impact of rationales on model
performance as well as a novel perspective of model reliability. The results
lead to several key findings that add new insights upon existing
understandings: 1) Rationales can, at times, deteriorate model performance; 2)
Rationales can, at times, improve model reliability, even outperforming their
untrained counterparts; 3) A linear correspondence exists in between the
performance and reliability improvements, while both are driven by the
intrinsic difficulty of the task. These findings provide informative
regulations on the broad utilization of rationales and raise critical
implications on the procedure of explicitly aligning language models with
implicit human thoughts. Codes can be found at
https://github.com/Ignoramus0817/rationales.

</details>


### [73] [LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge Dependency Parsing](https://arxiv.org/abs/2505.24163)
*Jiaqi Sun,Shiyou Qian,Zhangchi Han,Wei Li,Zelin Qian,Dingyu Yang,Jian Cao,Guangtao Xue*

Key words: 知识图谱,无监督学习,大语言模型,模式生成

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LKD-KGC是一种无监督的领域特定知识图谱构建框架，通过LLM驱动的优化处理和自回归生成实体模式，显著提升了构建的准确性和召回率。

Motivation: 领域特定知识图谱的手动构建效率低且依赖专业知识，现有方法受限于手动定义模式、单文档处理和公共领域参考，难以应对复杂知识依赖和特定性。

Method: LKD-KGC框架通过分析文档库推断知识依赖，利用LLM驱动优化处理顺序，自回归生成实体模式，并以此无监督提取实体和关系。

Result: 相比现有方法，LKD-KGC在准确率和召回率上普遍提升10%至20%。

Conclusion: LKD-KGC在构建高质量领域特定知识图谱方面表现出显著潜力。

Abstract: Knowledge Graphs (KGs) structure real-world entities and their relationships
into triples, enhancing machine reasoning for various tasks. While
domain-specific KGs offer substantial benefits, their manual construction is
often inefficient and requires specialized knowledge. Recent approaches for
knowledge graph construction (KGC) based on large language models (LLMs), such
as schema-guided KGC and reference knowledge integration, have proven
efficient. However, these methods are constrained by their reliance on manually
defined schema, single-document processing, and public-domain references,
making them less effective for domain-specific corpora that exhibit complex
knowledge dependencies and specificity, as well as limited reference knowledge.
To address these challenges, we propose LKD-KGC, a novel framework for
unsupervised domain-specific KG construction. LKD-KGC autonomously analyzes
document repositories to infer knowledge dependencies, determines optimal
processing sequences via LLM driven prioritization, and autoregressively
generates entity schema by integrating hierarchical inter-document contexts.
This schema guides the unsupervised extraction of entities and relationships,
eliminating reliance on predefined structures or external knowledge. Extensive
experiments show that compared with state-of-the-art baselines, LKD-KGC
generally achieves improvements of 10% to 20% in both precision and recall
rate, demonstrating its potential in constructing high-quality domain-specific
KGs.

</details>


### [74] [Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models](https://arxiv.org/abs/2505.24164)
*Shilin Xu,Yanwei Li,Rui Yang,Tao Zhang,Yueyi Sun,Wei Chow,Linfeng Li,Hang Song,Qi Xu,Yunhai Tong,Xiangtai Li,Hao Fei*

Key words: 大语言模型, 强化学习, 多任务学习, 奖励设计, 后训练

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出Mixed-R1框架，通过混合奖励设计和混合数据集（Mixed-45K）解决多源MLLM任务的稳定强化学习问题，实验验证了其有效性。

Motivation: 现有工作针对MLLM的强化学习仅关注单一任务（如接地任务、数学问题等），缺乏支持多源任务的统一方法，需要一种稳定的解决方案。

Method: 设计数据引擎构建Mixed-45K数据集，提出Mixed-Reward（包含匹配、图表、IoU和开放式奖励），并针对长文本提出BMAS奖励函数。

Result: 在Qwen2.5-VL和Intern-VL等MLLM上验证了方法的有效性。

Conclusion: Mixed-R1通过统一框架解决了多源MLLM任务的学习问题，提供了可扩展的数据集和奖励设计。

Abstract: Recent works on large language models (LLMs) have successfully demonstrated
the emergence of reasoning capabilities via reinforcement learning (RL).
Although recent efforts leverage group relative policy optimization (GRPO) for
MLLMs post-training, they constantly explore one specific aspect, such as
grounding tasks, math problems, or chart analysis. There are no works that can
leverage multi-source MLLM tasks for stable reinforcement learning. In this
work, we present a unified perspective to solve this problem. We present
Mixed-R1, a unified yet straightforward framework that contains a mixed reward
function design (Mixed-Reward) and a mixed post-training dataset (Mixed-45K).
We first design a data engine to select high-quality examples to build the
Mixed-45K post-training dataset. Then, we present a Mixed-Reward design, which
contains various reward functions for various MLLM tasks. In particular, it has
four different reward functions: matching reward for binary answer or
multiple-choice problems, chart reward for chart-aware datasets, IoU reward for
grounding problems, and open-ended reward for long-form text responses such as
caption datasets. To handle the various long-form text content, we propose a
new open-ended reward named Bidirectional Max-Average Similarity (BMAS) by
leveraging tokenizer embedding matching between the generated response and the
ground truth. Extensive experiments show the effectiveness of our proposed
method on various MLLMs, including Qwen2.5-VL and Intern-VL on various sizes.
Our dataset and model are available at https://github.com/xushilin1/mixed-r1.

</details>


### [75] [Tag-Evol: Achieving Efficient Instruction Evolving via Tag Injection](https://arxiv.org/abs/2505.24165)
*Yixuan Wang,Shiqi Zhou,Chuanzhe Guo,Qingfu Zhu*

Key words: Evol-Instruct, 数据合成, Tag-Evol, 指令进化, 知识标签

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出Tag-Evol框架，通过知识标签实现多样高效的指令进化，生成更优数据。

Motivation: 现有进化方法依赖固定策略且成本高，需更高效多样的解决方案。

Method: 使用知识标签组合控制指令进化。

Result: 实验显示生成数据优于其他方法，且多样性和挑战性更高。

Conclusion: Tag-Evol高效且能生成更优质数据。

Abstract: Evol-Instruct has made significant improvements as a data synthesis method in
several areas. Existing methods typically rely on a fixed set of strategies to
evolve, which require manual design and are monolithic in form. In addition,
iterative evolution also makes the acquisition of hard samples expensive. In
view of this, we propose the Tag-Evol framework, a more diverse and efficient
instruction evolving method. Specifically, Tag-Evol uses diverse and specific
knowledge tags as strategies to achieve controlled evolution by injecting
different combinations of tags into the original instructions. Experiments with
multiple backbones in diverse domain benchmarks show that the proposed method
generates significantly better evolved data than other methods. Furthermore, we
conduct a thorough analysis of the evolved data, demonstrating that Tag-Evol is
not only efficient but also generates more diverse and challenging data.

</details>


### [76] [Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation](https://arxiv.org/abs/2505.24174)
*Ryota Miyano,Yuki Arase*

Key words: LoRA, 低资源, 语言生成, 模型适应, 参数剪枝

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种简单但有效的LoRA合并方法，用于低资源语言生成任务的模型适应，通过动态调整和剪枝LoRA参数提升任务适应性。

Motivation: 现有LoRA合并方法在适应性和低资源问题上存在局限，作者希望提出一种能动态调整参数的方法以提升任务适应性。

Method: 提出了一种LoRA合并方法，通过微调动态更新和剪枝LoRA参数，结合最小目标任务数据实现更细粒度的调整。

Result: 在总结任务的多领域和多语言（英文和日文）数据集上验证，新方法在任务适应性上显著优于现有方法。

Conclusion: 该方法通过动态调整LoRA参数，显著提升了低资源语言生成任务中的模型适应性。

Abstract: This study proposes a simple yet effective LoRA merge method to achieve LLM
adaptation for low-resource language generation tasks. The LoRA merge
technique, which integrates multiple LoRA modules trained on different tasks,
has gained attention as an effective and efficient approach for adapting LLMs
to target tasks. However, previous methods are limited in adaptability as they
keep the LoRA parameters frozen. Additionally, the low-resource problem has
been out of their scope. We propose a LoRA merge method that updates and prunes
LoRA parameters through fine-tuning with minimal target task data, which allows
finer-grained adjustments of LoRA parameters and enhancement of task
adaptability. Extensive experiments have been conducted taking summarization as
a benchmark task. Our datasets cover various domains and multiple languages of
English and Japanese. The results confirm that the proposed method achieves
significant and consistent improvements in task adaptability over the previous
methods.

</details>


### [77] [Beyond Exponential Decay: Rethinking Error Accumulation in Large Language Models](https://arxiv.org/abs/2505.24187)
*Mikhail L. Arbuzov,Alexey A. Shvets,Sisong Beir*

Key words: LLM, 长序列可靠性, 关键令牌, 动态计算分配, 语义决策点

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文挑战了LLM在长序列中可靠性的指数衰减假设，提出关键令牌的概念，并设计了一个新框架以提高长期连贯性。

Motivation: 传统认为LLM在长序列中的可靠性会指数级衰减，但作者发现错误主要集中在少数关键令牌上，提出这为优化策略提供了可能。

Method: 通过识别关键令牌（占总令牌5-10%的决策点），设计动态计算分配和多路径探索的框架，优化长期性能。

Result: 研究显示，通过关注关键令牌而非均匀提高所有令牌精度，可显著提升LLM的长上下文性能，突破传统指数衰减的限制。

Conclusion: 论文指出未来的LLM系统应从单纯规模化转向战略性推理，通过选择性保留关键令牌实现更高效的语言系统。

Abstract: The prevailing assumption of an exponential decay in large language model
(LLM) reliability with sequence length, predicated on independent per-token
error probabilities, posits an inherent limitation for long autoregressive
outputs. Our research fundamentally challenges this view by synthesizing
emerging evidence that LLM errors are not uniformly distributed but are
concentrated at sparse "key tokens" ($5-10\%$ of total tokens) representing
critical decision junctions. By distinguishing these high-impact tokens from
the increasingly predictable majority, we introduce a new reliability formula
explaining the sustained coherence of modern LLMs over thousands of tokens.
Converging research streams reveal that long-context performance primarily
depends on accurately navigating a few crucial semantic decision points rather
than on uniform token-level accuracy, enabling targeted strategies that
significantly outperform brute-force approaches. We thus propose a framework
for next-generation systems centered on selective preservation of semantically
vital tokens, dynamic computational allocation at uncertain decision
boundaries, multi-path exploration at ambiguities, and architectures aligned
with natural semantic domains. This marks a fundamental shift from raw scaling
to strategic reasoning, promising breakthrough performance without
proportionate computational scaling and offering a more nuanced understanding
that supersedes the exponential decay hypothesis, thereby opening pathways
toward substantially more powerful and efficient language systems.

</details>


### [78] [CLaSp: In-Context Layer Skip for Self-Speculative Decoding](https://arxiv.org/abs/2505.24196)
*Longze Chen,Renke Shan,Huiming Wang,Lu Wang,Ziqiang Liu,Run Luo,Jiawei Wang,Hamid Alinejad-Rokny,Min Yang*

Key words: 推测解码，大语言模型，层跳过，动态规划，效率优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: CLaSp是一种无需额外训练或模块的自推测解码方法，通过动态跳过中间层加速解码，实现了1.3x~1.7x的速度提升。

Motivation: 现有推测解码方法需要额外训练模块且兼容性差，CLaSp旨在通过上下文层跳过策略解决这一问题。

Method: 采用动态规划算法优化层跳过策略，利用验证阶段的隐藏状态动态调整。

Result: 在多种下游任务中，LLaMA3系列模型实现了1.3x~1.7x的速度提升，且不改变生成文本的原始分布。

Conclusion: CLaSp作为一种即插即用的自推测解码方法，显著提升了效率且无需额外成本。

Abstract: Speculative decoding (SD) is a promising method for accelerating the decoding
process of Large Language Models (LLMs). The efficiency of SD primarily hinges
on the consistency between the draft model and the verify model. However,
existing drafting approaches typically require additional modules to be
trained, which can be challenging to implement and ensure compatibility across
various LLMs. In this paper, we propose CLaSp, an in-context layer-skipping
strategy for self-speculative decoding. Unlike prior methods, CLaSp does not
require additional drafting modules or extra training. Instead, it employs a
plug-and-play mechanism by skipping intermediate layers of the verify model to
construct a compressed draft model. Specifically, we develop a dynamic
programming algorithm that optimizes the layer-skipping process by leveraging
the complete hidden states from the last verification stage as an objective.
This enables CLaSp to dynamically adjust its layer-skipping strategy after each
verification stage, without relying on pre-optimized sets of skipped layers.
Experimental results across diverse downstream tasks demonstrate that CLaSp
achieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the
original distribution of the generated text.

</details>


### [79] [Intuitionistic Fuzzy Sets for Large Language Model Data Annotation: A Novel Approach to Side-by-Side Preference Labeling](https://arxiv.org/abs/2505.24199)
*Yimin Du*

Key words: 直觉模糊集, 偏好标注, 语言模型, 不确定性建模, 强化学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出基于直觉模糊集（IFS）的新框架，改进LLM偏好数据质量，提升标注一致性并降低疲劳，显著优于传统方法。

Motivation: 传统标注方法难以处理偏好判断的不确定性和标注者分歧，需更精确的偏好建模与聚合方法。

Method: 基于IFS设计标注协议，结合隶属度、非隶属度和犹豫度建模偏好，开发聚合方法与质量评估指标。

Result: 实验显示IFS方法提升标注一致性12.3%，降低标注时间15.7%，优化下游任务模型性能。

Conclusion: IFS框架为偏好数据标注提供理论支持，兼具实际应用价值，适合大规模LLM训练。

Abstract: The quality of human preference data is crucial for training and evaluating
large language models (LLMs), particularly in reinforcement learning from human
feedback (RLHF) and direct preference optimization (DPO) scenarios. Traditional
side-by-side (SBS) annotation approaches often struggle with inherent
uncertainty, annotator disagreement, and the complexity of preference
judgments. This paper introduces a novel framework based on intuitionistic
fuzzy sets (IFS) for modeling and aggregating human preferences in LLM data
annotation tasks. Our approach captures not only the degree of preference but
also the uncertainty and hesitation inherent in human judgment through
membership, non-membership, and hesitation degrees. We propose an IFS-based
annotation protocol that enables more nuanced preference modeling, develops
aggregation methods for handling annotator disagreement, and introduces quality
metrics for preference data assessment. Experimental validation on multiple
datasets demonstrates that our IFS-based approach significantly improves
annotation consistency, reduces annotator fatigue, and produces higher-quality
preference data compared to traditional binary and Likert-scale methods. The
resulting preference datasets lead to improved model performance in downstream
tasks, with 12.3\% improvement in win-rate against baseline models and 15.7\%
reduction in annotation time. Our framework provides a principled approach to
handling uncertainty in human preference annotation and offers practical
benefits for large-scale LLM training.

</details>


### [80] [Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?](https://arxiv.org/abs/2505.24211)
*Jiwan Chung,Janghan Yoon,Junhyeong Park,Sangeyl Lee,Joowon Yang,Sooyeon Park,Youngjae Yu*

Key words: 多模态生成模型，跨模态一致性，ACON数据集，循环一致性，等变性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了多模态生成模型是否真正实现了跨模态一致性，通过引入ACON数据集和三种一致性标准进行实验，发现通用模型在点对点评估中表现不如专用模型，但在结构性分析中显示出微弱的跨模态一致性。

Motivation: 探究通用生成模型在多模态之间的跨模态一致性是否真实存在，还是仅仅是表面现象。

Method: 引入了ACON数据集（包含1000张图像及多模态数据），并通过三种一致性标准（循环一致性、前向等变性、共轭等变性）进行实验评估。

Result: 实验显示，通用模型在点对点评估中（如循环一致性）不如专用模型，但在中间潜在空间的结构性分析中表现出微弱的跨模态一致性。

Conclusion: 通用生成模型在跨模态一致性方面的表现有限，但其潜在空间的结构性分析为跨模态研究提供了新方向。

Abstract: Any-to-any generative models aim to enable seamless interpretation and
generation across multiple modalities within a unified framework, yet their
ability to preserve relationships across modalities remains uncertain. Do
unified models truly achieve cross-modal coherence, or is this coherence merely
perceived? To explore this, we introduce ACON, a dataset of 1,000 images (500
newly contributed) paired with captions, editing instructions, and Q&A pairs to
evaluate cross-modal transfers rigorously. Using three consistency
criteria-cyclic consistency, forward equivariance, and conjugated
equivariance-our experiments reveal that any-to-any models do not consistently
demonstrate greater cross-modal consistency than specialized models in
pointwise evaluations such as cyclic consistency. However, equivariance
evaluations uncover weak but observable consistency through structured analyses
of the intermediate latent space enabled by multiple editing operations. We
release our code and data at https://github.com/JiwanChung/ACON.

</details>


### [81] [Semi-structured LLM Reasoners Can Be Rigorously Audited](https://arxiv.org/abs/2505.24217)
*Jixuan Leng,Cassandra A. Cohen,Zhixian Zhang,Chenyan Xiong,William W. Cohen*

Key words: 大语言模型（LLMs）、半结构化推理模型（SSRMs）、链式思考（CoT）、Pythonic语法、可审计性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出半结构化推理模型（SSRMs），采用Pythonic语法生成推理痕迹，提升可分析性，并通过实验验证其性能和可审计性。

Motivation: 解决大语言模型（LLMs）推理中的“忠实性”问题，即推理痕迹中的错误和偏见难以检测。

Method: 引入半结构化推理模型（SSRMs），使用半结构化的链式思考（CoT）格式和Pythonic语法生成推理痕迹，并设计手工和学习的审计方法。

Result: SSRMs在10个基准测试中表现优异，比基线模型提升近10个百分点，同时支持自动审计以识别推理缺陷。

Conclusion: 半结构化推理模型在提升LLM推理可靠性和可分析性方面具有潜力。

Abstract: As Large Language Models (LLMs) become increasingly capable at reasoning, the
problem of "faithfulness" persists: LLM "reasoning traces" can contain errors
and omissions that are difficult to detect, and may obscure biases in model
outputs. To address these limitations, we introduce Semi-Structured Reasoning
Models (SSRMs), which internalize a semi-structured Chain-of-Thought (CoT)
reasoning format within the model. Our SSRMs generate reasoning traces in a
Pythonic syntax. While SSRM traces are not executable, they adopt a restricted,
task-specific vocabulary to name distinct reasoning steps, and to mark each
step's inputs and outputs. Through extensive evaluation on ten benchmarks,
SSRMs demonstrate strong performance and generality: they outperform comparably
sized baselines by nearly ten percentage points on in-domain tasks while
remaining competitive with specialized models on out-of-domain medical
benchmarks. Furthermore, we show that semi-structured reasoning is more
amenable to analysis: in particular, they can be automatically audited to
identify reasoning flaws. We explore both hand-crafted structured audits, which
detect task-specific problematic reasoning patterns, and learned typicality
audits, which apply probabilistic models over reasoning patterns, and show that
both audits can be used to effectively flag probable reasoning errors.

</details>


### [82] [ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation](https://arxiv.org/abs/2505.24219)
*Lam Thanh Do,Aaditya Bodke,Pritom Saha Akash,Kevin Chen-Chuan Chang*

Key words: 无监督关键词生成,信息性模块,短语性模块,文本检索,时间效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ERU-KG是一种无监督关键词生成模型，通过两个模块（信息性和短语性）解决现有方法在信息性估计和时间效率上的不足，并在基准测试中表现优异。

Motivation: 现有无监督关键词预测方法多依赖启发式定义的重要性评分，导致信息性估计不准确且忽视时间效率。

Method: ERU-KG包含信息性和短语性模块，前者通过参考项（如查询、引用上下文）学习建模信息性，后者生成候选短语。

Result: 在基准测试中超越无监督基线，达到监督模型89%的Top 10预测性能，且在检索任务和推理速度上表现优异。

Conclusion: ERU-KG高效灵活，可切换生成与提取模式，适应多样应用需求。

Abstract: Unsupervised keyphrase prediction has gained growing interest in recent
years. However, existing methods typically rely on heuristically defined
importance scores, which may lead to inaccurate informativeness estimation. In
addition, they lack consideration for time efficiency. To solve these problems,
we propose ERU-KG, an unsupervised keyphrase generation (UKG) model that
consists of an informativeness and a phraseness module. The former estimates
the relevance of keyphrase candidates, while the latter generate those
candidates. The informativeness module innovates by learning to model
informativeness through references (e.g., queries, citation contexts, and
titles) and at the term-level, thereby 1) capturing how the key concepts of
documents are perceived in different contexts and 2) estimating informativeness
of phrases more efficiently by aggregating term informativeness, removing the
need for explicit modeling of the candidates. ERU-KG demonstrates its
effectiveness on keyphrase generation benchmarks by outperforming unsupervised
baselines and achieving on average 89\% of the performance of a supervised
model for top 10 predictions. Additionally, to highlight its practical utility,
we evaluate the model on text retrieval tasks and show that keyphrases
generated by ERU-KG are effective when employed as query and document
expansions. Furthermore, inference speed tests reveal that ERU-KG is the
fastest among baselines of similar model sizes. Finally, our proposed model can
switch between keyphrase generation and extraction by adjusting
hyperparameters, catering to diverse application requirements.

</details>


### [83] [Automated Structured Radiology Report Generation](https://arxiv.org/abs/2505.24223)
*Jean-Benoit Delbrouck,Justin Xu,Johannes Moll,Alois Thomas,Zhihong Chen,Sophie Ostmeier,Asfandyar Azhar,Kelvin Zhenghao Li,Andrew Johnston,Christian Bluethgen,Eduardo Reis,Mohamed Muneer,Maya Varma,Curtis Langlotz*

Key words: 结构化放射学报告, 大型语言模型, 疾病分类, 评估指标

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 引入了结构化放射学报告生成（SRRG）任务和新评估指标F1-SRR-BERT，通过LLMs重构报告数据集，验证其临床实用性。

Motivation: 解决自由文本报告的变异性问题，提升报告生成的临床一致性与结构化。

Method: 采用大型语言模型（LLMs）重构数据集，提出SRR-BERT疾病分类模型和F1-SRR-BERT评估指标。

Result: 构建标准化报告数据集，并通过专家研究和实验验证其有效性。

Conclusion: SRRG任务和F1-SRR-BERT标准提升了放射学报告的结构化和评估准确性。

Abstract: Automated radiology report generation from chest X-ray (CXR) images has the
potential to improve clinical efficiency and reduce radiologists' workload.
However, most datasets, including the publicly available MIMIC-CXR and CheXpert
Plus, consist entirely of free-form reports, which are inherently variable and
unstructured. This variability poses challenges for both generation and
evaluation: existing models struggle to produce consistent, clinically
meaningful reports, and standard evaluation metrics fail to capture the nuances
of radiological interpretation. To address this, we introduce Structured
Radiology Report Generation (SRRG), a new task that reformulates free-text
radiology reports into a standardized format, ensuring clarity, consistency,
and structured clinical reporting. We create a novel dataset by restructuring
reports using large language models (LLMs) following strict structured
reporting desiderata. Additionally, we introduce SRR-BERT, a fine-grained
disease classification model trained on 55 labels, enabling more precise and
clinically informed evaluation of structured reports. To assess report quality,
we propose F1-SRR-BERT, a metric that leverages SRR-BERT's hierarchical disease
taxonomy to bridge the gap between free-text variability and structured
clinical reporting. We validate our dataset through a reader study conducted by
five board-certified radiologists and extensive benchmarking experiments.

</details>


### [84] [Dynamic Context-Aware Streaming Pretrained Language Model For Inverse Text Normalization](https://arxiv.org/abs/2505.24229)
*Luong Ho,Khanh Le,Vinh Pham,Bao Nguyen,Tan Tran,Duc Chau*

Key words: 逆文本归一化,流式处理,预训练语言模型,动态上下文感知,低延迟

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种流式预训练语言模型用于逆文本归一化（ITN），通过动态上下文感知技术解决了流式处理的挑战，在越南语数据集上实现了与非流式ITN相当的准确性。

Motivation: 流式ITN在流式自动语音识别（ASR）中的应用尚未充分探索，尤其是在资源匮乏和上下文有限的场景下，亟需解决准确性、效率和适应性问题。

Method: 引入流式预训练语言模型，结合动态上下文感知技术（Dynamic Context-Aware），在训练和推理中自适应调整块大小并整合右上下文信息。

Result: 实验表明，该方法在越南语数据集上达到了与非流式ITN相当的准确性，且优于现有流式ITN模型，同时保持了低延迟。

Conclusion: 所提方法有效解决了流式ITN的挑战，实现了高准确性和低延迟的平衡，适用于ASR系统集成。

Abstract: Inverse Text Normalization (ITN) is crucial for converting spoken Automatic
Speech Recognition (ASR) outputs into well-formatted written text, enhancing
both readability and usability. Despite its importance, the integration of
streaming ITN within streaming ASR remains largely unexplored due to challenges
in accuracy, efficiency, and adaptability, particularly in low-resource and
limited-context scenarios. In this paper, we introduce a streaming pretrained
language model for ITN, leveraging pretrained linguistic representations for
improved robustness. To address streaming constraints, we propose Dynamic
Context-Aware during training and inference, enabling adaptive chunk size
adjustments and the integration of right-context information. Experimental
results demonstrate that our method achieves accuracy comparable to
non-streaming ITN and surpasses existing streaming ITN models on a Vietnamese
dataset, all while maintaining low latency, ensuring seamless integration into
ASR systems.

</details>


### [85] [Advantageous Parameter Expansion Training Makes Better Large Language Models](https://arxiv.org/abs/2505.24241)
*Naibin Gu,Yilong Chen,Zhenyu Zhang,Peng Fu,Zheng Lin,Shuohuan Wang,Yu Sun,Hua Wu,Weiping Wang,Haifeng Wang*

Key words: APEX, 优势参数, 语言模型, 参数扩展, 训练效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: APEX方法通过逐步扩展优势参数提升大语言模型性能，减少计算开销，实验显示在指令微调中仅用52%可训练参数即优于全参数微调，在继续预训练中仅需33%数据即可达到相同性能。

Motivation: 虽然增加可训练参数能提升模型性能，但伴随计算开销上升，研究发现优势参数对性能起关键作用，激发APEX方法的提出以优化参数利用。

Method: 提出APEX方法，逐步将优势参数扩展至劣势参数空间，提升其比例，从而增强训练效果，并通过矩阵有效秩理论分析性能提升原因。

Result: 指令微调中APEX仅用52%参数优于全参数微调；继续预训练中仅需33%数据达到相同困惑度，下游任务性能显著提升。

Conclusion: APEX通过优化参数分配显著提升模型效率与性能，为大规模语言模型训练提供新思路。

Abstract: Although scaling up the number of trainable parameters in both pre-training
and fine-tuning can effectively improve the performance of large language
models, it also leads to increased computational overhead. When delving into
the parameter difference, we find that a subset of parameters, termed
advantageous parameters, plays a crucial role in determining model performance.
Further analysis reveals that stronger models tend to possess more such
parameters. In this paper, we propose Advantageous Parameter EXpansion Training
(APEX), a method that progressively expands advantageous parameters into the
space of disadvantageous ones, thereby increasing their proportion and
enhancing training effectiveness. Further theoretical analysis from the
perspective of matrix effective rank explains the performance gains of APEX.
Extensive experiments on both instruction tuning and continued pre-training
demonstrate that, in instruction tuning, APEX outperforms full-parameter tuning
while using only 52% of the trainable parameters. In continued pre-training,
APEX achieves the same perplexity level as conventional training with just 33%
of the training data, and yields significant improvements on downstream tasks.

</details>


### [86] [Mamba Knockout for Unraveling Factual Information Flow](https://arxiv.org/abs/2505.24244)
*Nir Endy,Idan Daniel Grosbard,Yuval Ran-Milo,Yonatan Slutzky,Itay Tshuva,Raja Giryes*

Key words: Mamba SSM、Transformer、信息流动、注意力机制、LLM

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究探讨了Mamba SSM语言模型中事实信息的流动，通过借鉴Transformer的注意力解释方法，揭示了信息在Mamba模型中的传递模式。

Motivation: 研究Mamba SSM模型中信息传递机制，揭示其与Transformer模型的异同，探究LLM的通用特性。

Method: 利用Transformer的Attention Knockout方法，比较Mamba-1和Mamba-2的信息传递和局部化模式。

Result: 发现部分现象在Mamba和Transformer中不同，部分则普遍存在，表明某些特性可能为LLM共有。

Conclusion: 提供了理解Mamba内部运作的统一视角，并揭示其信息交流与特征强化的机制。

Abstract: This paper investigates the flow of factual information in Mamba State-Space
Model (SSM)-based language models. We rely on theoretical and empirical
connections to Transformer-based architectures and their attention mechanisms.
Exploiting this relationship, we adapt attentional interpretability techniques
originally developed for Transformers--specifically, the Attention Knockout
methodology--to both Mamba-1 and Mamba-2. Using them we trace how information
is transmitted and localized across tokens and layers, revealing patterns of
subject-token information emergence and layer-wise dynamics. Notably, some
phenomena vary between mamba models and Transformer based models, while others
appear universally across all models inspected--hinting that these may be
inherent to LLMs in general. By further leveraging Mamba's structured
factorization, we disentangle how distinct "features" either enable
token-to-token information exchange or enrich individual tokens, thus offering
a unified lens to understand Mamba internal operations.

</details>


### [87] [Proactive Guidance of Multi-Turn Conversation in Industrial Search](https://arxiv.org/abs/2505.24251)
*Xiaoyu Li,Xiao Li,Li Gao,Yiding Liu,Xiaoyang Wang,Shuaiqiang Wang,Junfeng Wang,Dawei Yin*

Key words: 多轮对话, 目标适应, 知识蒸馏, 强化学习, 点击率优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种两阶段框架（G-SFT与C-RL），用于工业级多轮搜索系统的主动引导，兼顾目标动态适应与低延迟交互，显著提升准确率与点击率。

Motivation: 解决LLM在多轮对话中动态适应用户目标变化及保持低延迟的挑战，提升搜索助手的交互质量。

Method: 1. G-SFT阶段：通过目标适应代理动态跟踪用户目标，并利用知识蒸馏实现轻量化；2. C-RL阶段：基于用户点击信号构建偏好对，通过强化学习优化点击率。

Result: 离线准确率提升23.95%（达86.10%），在线点击率相对提升149.06%（达25.28%），推理延迟降低69.55%。

Conclusion: 两阶段框架有效平衡目标追踪与交互优化，适用于工业级实时系统。

Abstract: The evolution of Large Language Models (LLMs) has significantly advanced
multi-turn conversation systems, emphasizing the need for proactive guidance to
enhance users' interactions. However, these systems face challenges in
dynamically adapting to shifts in users' goals and maintaining low latency for
real-time interactions. In the Baidu Search AI assistant, an industrial-scale
multi-turn search system, we propose a novel two-phase framework to provide
proactive guidance. The first phase, Goal-adaptive Supervised Fine-Tuning
(G-SFT), employs a goal adaptation agent that dynamically adapts to user goal
shifts and provides goal-relevant contextual information. G-SFT also
incorporates scalable knowledge transfer to distill insights from LLMs into a
lightweight model for real-time interaction. The second phase, Click-oriented
Reinforcement Learning (C-RL), adopts a generate-rank paradigm, systematically
constructs preference pairs from user click signals, and proactively improves
click-through rates through more engaging guidance. This dual-phase
architecture achieves complementary objectives: G-SFT ensures accurate goal
tracking, while C-RL optimizes interaction quality through click signal-driven
reinforcement learning. Extensive experiments demonstrate that our framework
achieves 86.10% accuracy in offline evaluation (+23.95% over baseline) and
25.28% CTR in online deployment (149.06% relative improvement), while reducing
inference latency by 69.55% through scalable knowledge distillation.

</details>


### [88] [Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games](https://arxiv.org/abs/2505.24255)
*Neemesh Yadav,Palakorn Achananuparp,Jing Jiang,Ee-Peng Lim*

Key words: Large Language Models, Theory of Mind, negotiation tasks, ultimatum game, decision-making

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究探讨了大语言模型（LLMs）在谈判任务中通过心智理论（ToM）推理来对齐代理行为与人类规范的作用，实验表明ToM推理能提升行为对齐和决策一致性。

Motivation: 探索ToM推理在LLMs中如何增强代理行为与人类规范的契合度，特别关注其在谈判任务中的应用。

Method: 使用最后通牒游戏作为控制环境，初始化不同亲社会信念（贪婪、公平、无私）的LLM代理，并应用链条式思维（CoT）和不同ToM水平的推理方法。测试了多种LLMs模型。

Result: 2700次模拟实验结果显示，ToM推理显著提升了行为对齐、决策一致性和谈判结果。推理模型在ToM推理能力上表现有限。

Conclusion: ToM推理在提升人机交互和合作决策中发挥重要作用，为未来研究提供了新方向。

Abstract: Large Language Models (LLMs) have shown potential in simulating human
behaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for
complex social interactions. In this study, we investigate the role of ToM
reasoning in aligning agentic behaviors with human norms in negotiation tasks,
using the ultimatum game as a controlled environment. We initialized LLM agents
with different prosocial beliefs (including Greedy, Fair, and Selfless) and
reasoning methods like chain-of-thought (CoT) and varying ToM levels, and
examined their decision-making processes across diverse LLMs, including
reasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from
2,700 simulations indicated that ToM reasoning enhances behavior alignment,
decision-making consistency, and negotiation outcomes. Consistent with previous
findings, reasoning models exhibit limited capability compared to models with
ToM reasoning, different roles of the game benefits with different orders of
ToM reasoning. Our findings contribute to the understanding of ToM's role in
enhancing human-AI interaction and cooperative decision-making. The code used
for our experiments can be found at https://github.com/Stealth-py/UltimatumToM.

</details>


### [89] [Simulating Training Data Leakage in Multiple-Choice Benchmarks for LLM Evaluation](https://arxiv.org/abs/2505.24263)
*Naila Shafirni Hidayat,Muhammad Dehan Al Kautsar,Alfan Farizki Wicaksono,Fajri Koto*

Key words: 大型语言模型、数据泄漏、n-gram方法、MMLU、HellaSwag

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文比较了现有数据泄漏检测方法（如排列和n-gram方法），并提出了一种轻量级的半半问题方法，发现n-gram方法在模拟泄漏场景下表现最佳。通过优化方法，作者清理了MMLU和HellaSwag数据集，并重新评估了多个LLM，呼吁将泄漏检测作为基准测试发布的标准化步骤。

Motivation: 由于大型语言模型（LLMs）训练数据的不透明性，可能存在评估集的泄漏问题，影响结果的公平性。现有泄漏检测方法缺乏在受控条件下的全面评估，需要更可靠的技术来支持透明评估。

Method: 在连续预训练模拟泄漏场景中比较排列和n-gram方法，并探索轻量级的半半问题方法。此外，优化方法以实现实例级检测并降低计算开销。

Result: n-gram方法在F1分数上表现最佳。清理后的MMLU和HellaSwag数据集重新评估多个LLM，展示了更可靠的评估结果。

Conclusion: 提出泄漏检测作为基准测试发布的标准化步骤，为更透明和可靠的评估提供实用路径。

Abstract: The performance of large language models (LLMs) continues to improve, as
reflected in rising scores on standard benchmarks. However, the lack of
transparency around training data raises concerns about potential overlap with
evaluation sets and the fairness of reported results. Although prior work has
proposed methods for detecting data leakage, these approaches primarily focus
on identifying outliers and have not been evaluated under controlled simulated
leakage conditions. In this work, we compare existing leakage detection
techniques, namely permutation and n-gram-based methods, under a continual
pretraining setup that simulates real-world leakage scenarios, and additionally
explore a lightweight method we call semi-half question. Although semi-half
offers a low-cost alternative, our analysis shows that the n-gram method
consistently achieves the highest F1-score. We also refine these techniques to
support instance-level detection and reduce computational overhead. Leveraging
the best-performing method, we create cleaned versions of MMLU and HellaSwag,
and re-evaluate several LLMs. Our findings present a practical path toward more
reliable and transparent evaluations, and we recommend contamination checks as
a standard step before releasing benchmark results.

</details>


### [90] [Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations](https://arxiv.org/abs/2505.24264)
*Xin Quan,Marco Valentino,Louise A. Dennis,André Freitas*

Key words: 自然语言推理、大语言模型、定理证明、自动形式化、解释优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了结合大语言模型（LLM）与定理证明器（TP）优化自然语言推理（NLI）解释的方法，通过减少自动形式化中的语义损失、纠错逻辑表达式、生成结构化证明草图及提升反馈理解能力，显著提升了性能。

Motivation: 现有方法在自然语言转形式化表示时存在语义丢失和逻辑结构捕捉不足的问题，LLM在形式化验证框架中构建严谨证明的能力也有限。论文旨在通过策略改进解决这些问题。

Method: 提出四种策略：减少自动形式化语义损失、高效纠错逻辑表达式、利用逻辑表达式生成结构化证明草图、增强LLM对TP反馈的理解能力进行迭代优化。

Result: 在e-SNLI、QASC和WorldTree数据集上，自动形式化性能提升18.46%-39.77%，解释优化提升29.5%-51.5%，验证效率显著提高。

Conclusion: 结合LLM与TP的混合架构，通过针对性干预能有效提升NLI的准确性和效率。

Abstract: Natural language explanations play a fundamental role in Natural Language
Inference (NLI) by revealing how premises logically entail hypotheses. Recent
work has shown that the interaction of large language models (LLMs) with
theorem provers (TPs) can help verify and improve the validity of NLI
explanations. However, TPs require translating natural language into
machine-verifiable formal representations, a process that introduces the risk
of semantic information loss and unfaithful interpretation, an issue compounded
by LLMs' challenges in capturing critical logical structures with sufficient
precision. Moreover, LLMs are still limited in their capacity for rigorous and
robust proof construction within formal verification frameworks. To mitigate
issues related to faithfulness and robustness, this paper investigates
strategies to (1) alleviate semantic loss during autoformalisation, (2)
efficiently identify and correct syntactic errors in logical representations,
(3) explicitly use logical expressions to guide LLMs in generating structured
proof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback
for iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree
using different LLMs demonstrate that the proposed strategies yield significant
improvements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation
refinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover,
we show that specific interventions on the hybrid LLM-TP architecture can
substantially improve efficiency, drastically reducing the number of iterations
required for successful verification.

</details>


### [91] [ScienceMeter: Tracking Scientific Knowledge Updates in Language Models](https://arxiv.org/abs/2505.24302)
*Yike Wang,Shangbin Feng,Yulia Tsvetkov,Hannaneh Hajishirzi*

Key words: LLM, 科学知识更新, ScienceMeter, 知识保留, 知识获取, 知识预测

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ScienceMeter框架用于评估大语言模型（LLM）在科学知识更新中的表现，定义了三项指标：知识保留、知识获取和知识预测。实验结果显示，现有方法在知识更新方面仍有显著不足。

Motivation: 由于LLM对科学进展的知识可能快速过时，研究提出ScienceMeter框架以评估知识更新方法的有效性，以支持更稳健的科学研究辅助工具开发。

Method: 通过ScienceMeter框架定义了知识保留、知识获取和知识预测三个指标，并在包含15,444篇论文和30,888个科学主张的数据集上评估了五种代表性的知识更新方法。

Result: 最佳表现方法在知识保留、获取和预测上的表现分别为85.9%、71.7%和37.7%，跨领域分析表明性能相关，但现有方法难以同时实现这些目标。

Conclusion: 开发更有效的科学知识更新机制既至关重要又充满挑战，特别是在专用科学LLM上的表现仍有显著不足。

Abstract: Large Language Models (LLMs) are increasingly used to support scientific
research, but their knowledge of scientific advancements can quickly become
outdated. We introduce ScienceMeter, a new framework for evaluating scientific
knowledge update methods over scientific knowledge spanning the past, present,
and future. ScienceMeter defines three metrics: knowledge preservation, the
extent to which models' understanding of previously learned papers are
preserved; knowledge acquisition, how well scientific claims from newly
introduced papers are acquired; and knowledge projection, the ability of the
updated model to anticipate or generalize to related scientific claims that may
emerge in the future. Using ScienceMeter, we examine the scientific knowledge
of LLMs on claim judgment and generation tasks across a curated dataset of
15,444 scientific papers and 30,888 scientific claims from ten domains
including medicine, biology, materials science, and computer science. We
evaluate five representative knowledge update approaches including training-
and inference-time methods. With extensive experiments, we find that the
best-performing knowledge update methods can preserve only 85.9% of existing
knowledge, acquire 71.7% of new knowledge, and project 37.7% of future
knowledge. Inference-based methods work for larger models, whereas smaller
models require training to achieve comparable performance. Cross-domain
analysis reveals that performance on these objectives is correlated. Even when
applying on specialized scientific LLMs, existing knowledge update methods fail
to achieve these objectives collectively, underscoring that developing robust
scientific knowledge update mechanisms is both crucial and challenging.

</details>


### [92] [HiCaM: A Hierarchical-Causal Modification Framework for Long-Form Text Modification](https://arxiv.org/abs/2505.24319)
*Yuntao Shi,Yi Luo,Yeyun Gong,Chen Lin*

Key words: Large Language Models, HiCaM, hierarchical-causal modification, document coherence

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: HiCaM通过分层因果框架解决了LLMs在长篇文本修改中的问题，显著提升了性能。

Motivation: LLMs在长篇文本修改时容易产生不当修改或遗漏必要修改，HiCaM旨在解决这些问题。

Method: 采用分层摘要树和因果图的Hierarchical-Causal Modification框架。

Result: 在多领域数据集上表现优异，胜率达79.50%。

Conclusion: HiCaM在多模型和多领域下均表现出性能提升，是一种全面的解决方案。

Abstract: Large Language Models (LLMs) have achieved remarkable success in various
domains. However, when handling long-form text modification tasks, they still
face two major problems: (1) producing undesired modifications by
inappropriately altering or summarizing irrelevant content, and (2) missing
necessary modifications to implicitly related passages that are crucial for
maintaining document coherence. To address these issues, we propose HiCaM, a
Hierarchical-Causal Modification framework that operates through a hierarchical
summary tree and a causal graph. Furthermore, to evaluate HiCaM, we derive a
multi-domain dataset from various benchmarks, providing a resource for
assessing its effectiveness. Comprehensive evaluations on the dataset
demonstrate significant improvements over strong LLMs, with our method
achieving up to a 79.50\% win rate. These results highlight the
comprehensiveness of our approach, showing consistent performance improvements
across multiple models and domains.

</details>


### [93] [Context-Aware Sentiment Forecasting via LLM-based Multi-Perspective Role-Playing Agents](https://arxiv.org/abs/2505.24331)
*Fanhang Man,Huandong Wang,Jianjie Fang,Zhaoyi Deng,Baining Zhao,Xinlei Chen,Yong Li*

Key words: 情感预测、社交媒体、多视角角色扮演、情感动态、社会趋势

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种用于社交媒体上情感预测的方法，通过提取情感相关特征并采用多视角角色扮演框架，显著提升了微观和宏观层面的情感预测效果。

Motivation: 社交媒体上的用户情感反映了潜在的社会趋势、危机和需求，但当前研究多侧重于过去情感的分析，而预测未来情感的研究较少。

Method: 通过提取情感相关特征并设计多视角角色扮演框架，模拟人类对事件发展的情感响应过程。

Result: 初步结果表明，该方法在微观和宏观层面的情感预测上均取得了显著提升。

Conclusion: 情感预测在社会趋势和危机管理中具有重要意义，所提出的框架为社交媒体情感动态的建模提供了新的思路。

Abstract: User sentiment on social media reveals the underlying social trends, crises,
and needs. Researchers have analyzed users' past messages to trace the
evolution of sentiments and reconstruct sentiment dynamics. However, predicting
the imminent sentiment of an ongoing event is rarely studied. In this paper, we
address the problem of \textbf{sentiment forecasting} on social media to
predict the user's future sentiment in response to the development of the
event. We extract sentiment-related features to enhance the modeling skill and
propose a multi-perspective role-playing framework to simulate the process of
human response. Our preliminary results show significant improvement in
sentiment forecasting on both microscopic and macroscopic levels.

</details>


### [94] [Pangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning](https://arxiv.org/abs/2505.24332)
*Wenxuan Shi,Haochen Tan,Chuqiao Kuang,Xiaoguang Li,Xiaozhe Ren,Chen Zhang,Hanting Chen,Yasheng Wang,Lifeng Shang,Fisher Yu,Yunhe Wang*

Key words: 大语言模型,开放网络问答,自适应搜索,强化学习,WebPuzzle

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为DeepDiver的强化学习框架，旨在提升大语言模型（LLM）在开放网络环境中的信息搜索能力，特别是面对模糊或冲突证据时的自适应搜索强度（SIS）。通过WebPuzzle数据集和DeepDiver，模型在真实网络任务中表现优异。

Motivation: 现有方法在开放网络问答中表现不佳，主要受限于静态提示规则或基于维基百科的训练环境，无法适应真实网络中的模糊性和噪声。论文旨在解决LLM动态调整搜索行为的缺失能力。

Method: 提出了DeepDiver强化学习框架，通过WebPuzzle数据集（包含24K训练实例和275测试问题）训练模型自适应搜索策略，支持从监督微调到强化学习的完整训练课程。

Result: 实验显示，搭载DeepDiver的Pangu-7B-Reasoner在真实网络任务中表现与671B参数的DeepSeek-R1相当，且SIS能力可推广至开放式任务（如长文本生成）。

Conclusion: DeepDiver和WebPuzzle为LLM的自适应信息搜索提供了新方法和基准，推动了未来研究。

Abstract: Information seeking demands iterative evidence gathering and reflective
reasoning, yet large language models (LLMs) still struggle with it in open-web
question answering. Existing methods rely on static prompting rules or training
with Wikipedia-based corpora and retrieval environments, limiting adaptability
to the real-world web environment where ambiguity, conflicting evidence, and
noise are prevalent. These constrained training settings hinder LLMs from
learning to dynamically decide when and where to search, and how to adjust
search depth and frequency based on informational demands. We define this
missing capacity as Search Intensity Scaling (SIS)--the emergent skill to
intensify search efforts under ambiguous or conflicting conditions, rather than
settling on overconfident, under-verification answers.
  To study SIS, we introduce WebPuzzle, the first dataset designed to foster
information-seeking behavior in open-world internet environments. WebPuzzle
consists of 24K training instances and 275 test questions spanning both
wiki-based and open-web queries. Building on this dataset, we propose
DeepDiver, a Reinforcement Learning (RL) framework that promotes SIS by
encouraging adaptive search policies through exploration under a real-world
open-web environment. Experimental results show that Pangu-7B-Reasoner
empowered by DeepDiver achieve performance on real-web tasks comparable to the
671B-parameter DeepSeek-R1. We detail DeepDiver's training curriculum from
cold-start supervised fine-tuning to a carefully designed RL phase, and present
that its capability of SIS generalizes from closed-form QA to open-ended tasks
such as long-form writing. Our contributions advance adaptive information
seeking in LLMs and provide a valuable benchmark and dataset for future
research.

</details>


### [95] [Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings](https://arxiv.org/abs/2505.24341)
*Shujian Yang,Shiyao Cui,Chuanrui Hu,Haicheng Wang,Tianwei Zhang,Minlie Huang,Jialiang Lu,Han Qiu*

Key words: LLM, 中文毒性检测, 多模态, 扰动策略, 微调

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了中文毒性内容检测中大型语言模型（LLM）的局限性，提出3种扰动策略和8种具体方法，并构建数据集评估9种SOTA LLM，发现其对多模态中文毒性内容检测能力不足，且少量示例微调可能导致模型过度矫正。

Motivation: 中文毒性内容检测存在挑战，LLM虽表现优异，但易受字符替换等扰动策略干扰。论文旨在探索LLM在多模态中文毒性检测中的局限性及改进方案。

Method: 提出毒性内容扰动策略分类（3种策略，8种方法），构建数据集，并在9种SOTA LLM上评估其检测能力；同时尝试基于上下文学习（ICL）和监督微调（SFT）的成本优化方案。

Result: （1）LLM对多模态中文毒性内容检测能力较弱；（2）ICL或SFT少量示例可能导致模型过度矫正，误判正常内容为毒性内容。

Conclusion: LLM在中文毒性检测中需进一步优化以应对多模态扰动，且微调策略需谨慎以避免过度矫正。

Abstract: Detecting toxic content using language models is important but challenging.
While large language models (LLMs) have demonstrated strong performance in
understanding Chinese, recent studies show that simple character substitutions
in toxic Chinese text can easily confuse the state-of-the-art (SOTA) LLMs. In
this paper, we highlight the multimodal nature of Chinese language as a key
challenge for deploying LLMs in toxic Chinese detection. First, we propose a
taxonomy of 3 perturbation strategies and 8 specific approaches in toxic
Chinese content. Then, we curate a dataset based on this taxonomy, and
benchmark 9 SOTA LLMs (from both the US and China) to assess if they can detect
perturbed toxic Chinese text. Additionally, we explore cost-effective
enhancement solutions like in-context learning (ICL) and supervised fine-tuning
(SFT). Our results reveal two important findings. (1) LLMs are less capable of
detecting perturbed multimodal Chinese toxic contents. (2) ICL or SFT with a
small number of perturbed examples may cause the LLMs "overcorrect'':
misidentify many normal Chinese contents as toxic.

</details>


### [96] [Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework for ASR Error Correction](https://arxiv.org/abs/2505.24347)
*Yangui Fang,Baixu Cheng,Jing Peng,Xu Li,Yu Xi,Chengwei Zhang,Guohui Zhong*

Key words: 自动语音识别, 错误校正, 大语言模型, RLLM-CF, 链式思维

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种名为RLLM-CF的框架，通过三阶段方法（错误预检测、链式思维子任务迭代校正和推理过程验证）解决LLM直接用于语音识别错误校正时的幻觉问题，无需额外训练或微调。实验表明，该框架显著降低了CER/WER。

Motivation: 传统语音识别错误校正方法效果有限，而直接使用大语言模型（LLM）会导致幻觉问题，即可能修改正确的文本。因此，需要一种可靠的方法来校正错误并保持正确文本。

Method: 提出的RLLM-CF框架包含三个阶段：错误预检测、链式思维子任务迭代校正和推理过程验证，无需额外数据或模型微调。

Result: 在AISHELL-1、AISHELL-2和Librispeech数据集上的实验显示，该框架使GPT-4o模型的CER/WER相对降低了21%、11%、9%和11.4%。

Conclusion: RLLM-CF框架有效解决了LLM在语音识别错误校正中的幻觉问题，显著提升了校正的准确性和可靠性。

Abstract: Automatic Speech Recognition (ASR) error correction aims to correct
recognition errors while preserving accurate text. Although traditional
approaches demonstrate moderate effectiveness, LLMs offer a paradigm that
eliminates the need for training and labeled data. However, directly using LLMs
will encounter hallucinations problem, which may lead to the modification of
the correct text. To address this problem, we propose the Reliable LLM
Correction Framework (RLLM-CF), which consists of three stages: (1) error
pre-detection, (2) chain-of-thought sub-tasks iterative correction, and (3)
reasoning process verification. The advantage of our method is that it does not
require additional information or fine-tuning of the model, and ensures the
correctness of the LLM correction under multi-pass programming. Experiments on
AISHELL-1, AISHELL-2, and Librispeech show that the GPT-4o model enhanced by
our framework achieves 21%, 11%, 9%, and 11.4% relative reductions in CER/WER.

</details>


### [97] [Unifying Language Agent Algorithms with Graph-based Orchestration Engine for Reproducible Agent Research](https://arxiv.org/abs/2505.24354)
*Qianqian Zhang,Jiajia Liao,Heting Ying,Yibo Ma,Haozhan Shen,Jingcheng Li,Peng Liu,Lu Zhang,Chunxin Fang,Kyusong Lee,Ruochen Xu,Tiancheng Zhao*

Key words: LLMs, language agents, AGORA, reasoning, evaluation framework

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: AGORA framework addresses challenges in developing language agents via modular architecture, reusable algorithms, and rigorous evaluation, showing simpler methods like Chain-of-Thought can outperform complex ones.

Motivation: Overcome challenges in language agent development (engineering overhead, lack of standardization, and weak evaluation frameworks).

Method: Introduces AGORA with graph-based workflow, memory management, reusable algorithms, and evaluation protocols.

Result: Simpler methods (e.g., Chain-of-Thought) often match or outperform complex approaches with lower computational cost.

Conclusion: AGORA simplifies agent development and standardizes evaluation, promoting reproducible research.

Abstract: Language agents powered by large language models (LLMs) have demonstrated
remarkable capabilities in understanding, reasoning, and executing complex
tasks. However, developing robust agents presents significant challenges:
substantial engineering overhead, lack of standardized components, and
insufficient evaluation frameworks for fair comparison. We introduce Agent
Graph-based Orchestration for Reasoning and Assessment (AGORA), a flexible and
extensible framework that addresses these challenges through three key
contributions: (1) a modular architecture with a graph-based workflow engine,
efficient memory management, and clean component abstraction; (2) a
comprehensive suite of reusable agent algorithms implementing state-of-the-art
reasoning approaches; and (3) a rigorous evaluation framework enabling
systematic comparison across multiple dimensions. Through extensive experiments
on mathematical reasoning and multimodal tasks, we evaluate various agent
algorithms across different LLMs, revealing important insights about their
relative strengths and applicability. Our results demonstrate that while
sophisticated reasoning approaches can enhance agent capabilities, simpler
methods like Chain-of-Thought often exhibit robust performance with
significantly lower computational overhead. AGORA not only simplifies language
agent development but also establishes a foundation for reproducible agent
research through standardized evaluation protocols.

</details>


### [98] [Multilingual Gloss-free Sign Language Translation: Towards Building a Sign Language Foundation Model](https://arxiv.org/abs/2505.24355)
*Sihan Tan,Taro Miyazaki,Kazuhiro Nakadai*

Key words: 手语翻译, 多语言处理, 双CTC目标, 语言对齐

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文提出了一个多语言手语翻译模型，解决了语言冲突和对齐难题，支持10种手语，并在多个基准测试中表现优异。

Motivation: 多语言手语翻译（MLSLT）尚未被探索，尽管它可以缓解低资源问题并提高可访问性。现有的研究主要集中于单语翻译，而跨语言冲突和对齐困难阻碍了多语言翻译的发展。

Method: 论文提出了一种多语言无注释模型，采用双CTC目标，用于标记级手语识别和口语文本生成。该方法支持10种手语，能够处理一对一、多对一和多对多的翻译任务。

Result: 该模型在三个广泛采用的基准测试（多语言SP-10、PHOENIX14T和CSL-Daily）上取得了与现有最佳方法相竞争的性能。

Conclusion: 该多语言手语翻译模型通过双CTC目标和支持多种翻译任务的能力，成功解决了语言冲突和对齐难题，为多语言手语翻译提供了可行的解决方案。

Abstract: Sign Language Translation (SLT) aims to convert sign language (SL) videos
into spoken language text, thereby bridging the communication gap between the
sign and the spoken community. While most existing works focus on translating a
single sign language into a single spoken language (one-to-one SLT), leveraging
multilingual resources could mitigate low-resource issues and enhance
accessibility. However, multilingual SLT (MLSLT) remains unexplored due to
language conflicts and alignment difficulties across SLs and spoken languages.
To address these challenges, we propose a multilingual gloss-free model with
dual CTC objectives for token-level SL identification and spoken text
generation. Our model supports 10 SLs and handles one-to-one, many-to-one, and
many-to-many SLT tasks, achieving competitive performance compared to
state-of-the-art methods on three widely adopted benchmarks: multilingual
SP-10, PHOENIX14T, and CSL-Daily.

</details>


### [99] [Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion](https://arxiv.org/abs/2505.24362)
*Anum Afzal,Florian Matthes,Gal Chechik,Yftah Ziser*

Key words: 零-shot推理，链式思考，表征分析，早期终止，效率优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究发现，在零-shot链式思考任务中，LLM初始表征已隐含推理关键信息，可预测推理成功与否；早期推理步骤足够时，可提前终止推理链。

Motivation: 探究能否在链式思考任务开始前预测其成功，并利用早期推理步骤优化推理效率。

Method: 通过基于LLM表征的分类器探测初始表征，并与基于生成文本的BERT基线对比，分析早期和后期推理步骤的作用。

Result: 初始表征已含关键推理信息，早期推理与后期效果相当；提前终止推理链仍优于无推理链，但弱于完整推理。

Conclusion: LLM在早期步骤已编码核心推理信息，可指导优化推理链长度，提升效率。

Abstract: We investigate whether the success of a zero-shot Chain-of-Thought (CoT)
process can be predicted before completion. We discover that a probing
classifier, based on LLM representations, performs well \emph{even before a
single token is generated}, suggesting that crucial information about the
reasoning process is already present in the initial steps representations. In
contrast, a strong BERT-based baseline, which relies solely on the generated
tokens, performs worse, likely because it depends on shallow linguistic cues
rather than deeper reasoning dynamics. Surprisingly, using later reasoning
steps does not always improve classification. When additional context is
unhelpful, earlier representations resemble later ones more, suggesting LLMs
encode key information early. This implies reasoning can often stop early
without loss. To test this, we conduct early stopping experiments, showing that
truncating CoT reasoning still improves performance over not using CoT at all,
though a gap remains compared to full reasoning. However, approaches like
supervised learning or reinforcement learning designed to shorten CoT chains
could leverage our classifier's guidance to identify when early stopping is
effective. Our findings provide insights that may support such methods, helping
to optimize CoT's efficiency while preserving its benefits.\footnote{Code and
data is available at
\href{https://github.com/anum94/CoTpred}{\texttt{github.com/anum94/CoTpred}}.

</details>


### [100] [LLM Inference Enhanced by External Knowledge: A Survey](https://arxiv.org/abs/2505.24377)
*Yu-Hsuan Lin,Qian-Hui Chen,Yi-Jie Cheng,Jia-Ren Zhang,Yi-Hung Liu,Liang-Yu Hsia,Yun-Nung Chen*

Key words: 大语言模型, 外部知识, 结构化数据, 知识图谱, 可解释性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文系统性研究了如何利用外部知识增强大语言模型（LLMs），重点关注结构化知识（如表格和知识图谱）的分类与集成方法，并分析了其在可解释性、扩展性和性能上的权衡。

Motivation: 大语言模型在自然语言推理方面表现优秀，但受限于参数化记忆和幻觉问题，需要外部知识辅助以提高准确性和上下文推理能力。

Method: 通过分类外部知识（非结构化与结构化），聚焦结构化知识（表格和知识图谱），提出与LLM的集成范式，并回顾代表性方法。

Result: 分析表明，不同方法在可解释性、扩展性和性能之间存在权衡，为开发可信且可泛化的知识增强型LLMs提供了指导。

Conclusion: 外部知识的系统性整合能有效提升LLMs的能力，未来需关注可信性与通用性的平衡。

Abstract: Recent advancements in large language models (LLMs) have enhanced
natural-language reasoning. However, their limited parametric memory and
susceptibility to hallucination present persistent challenges for tasks
requiring accurate, context-based inference. To overcome these limitations, an
increasing number of studies have proposed leveraging external knowledge to
enhance LLMs. This study offers a systematic exploration of strategies for
using external knowledge to enhance LLMs, beginning with a taxonomy that
categorizes external knowledge into unstructured and structured data. We then
focus on structured knowledge, presenting distinct taxonomies for tables and
knowledge graphs (KGs), detailing their integration paradigms with LLMs, and
reviewing representative methods. Our comparative analysis further highlights
the trade-offs among interpretability, scalability, and performance, providing
insights for developing trustworthy and generalizable knowledge-enhanced LLMs.

</details>


### [101] [ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation](https://arxiv.org/abs/2505.24388)
*Hao Chen,Yukun Yan,Sen Mei,Wanxiang Che,Zhenghao Liu,Qi Shi,Xinze Li,Yuchun Fan,Pengcheng Huang,Qiushi Xiong,Zhiyuan Liu,Maosong Sun*

Key words: 检索增强生成, 大型语言模型, 线索锚定推理, 多路径优化, 噪声韧性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了ClueAnchor框架，通过关键线索的提取和多推理路径优化，显著提升了检索增强生成（RAG）的效果，尤其在噪声或部分相关检索内容下表现出色。

Motivation: 现有RAG系统未充分利用检索文档的关键线索，导致推理结果缺乏忠实性和可解释性，尤其是在证据隐晦、分散或受噪声影响时。

Method: ClueAnchor框架提取关键线索，生成多推理路径，并通过奖励优化的方式选择最优路径。

Result: 实验表明，ClueAnchor在推理完整性和鲁棒性上显著优于基线，且对噪声或部分相关检索内容具有强韧性。

Conclusion: ClueAnchor无需显式线索监督即可识别支持证据，为RAG提供了一种高效优化方法。

Abstract: Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs)
with external knowledge to improve factuality. However, existing RAG systems
frequently underutilize the retrieved documents, failing to extract and
integrate the key clues needed to support faithful and interpretable reasoning,
especially in cases where relevant evidence is implicit, scattered, or obscured
by noise. To address this issue, we propose ClueAnchor, a novel framework for
enhancing RAG via clue-anchored reasoning exploration and optimization.
ClueAnchor extracts key clues from retrieved content and generates multiple
reasoning paths based on different knowledge configurations, optimizing the
model by selecting the most effective one through reward-based preference
optimization. Experiments show that ClueAnchor significantly outperforms prior
RAG baselines in reasoning completeness and robustness. Further analysis
confirms its strong resilience to noisy or partially relevant retrieved
content, as well as its capability to identify supporting evidence even in the
absence of explicit clue supervision during inference.

</details>


### [102] [LLMs Are Globally Multilingual Yet Locally Monolingual: Exploring Knowledge Transfer via Language and Thought Theory](https://arxiv.org/abs/2505.24409)
*Eojin Kang,Juae Kim*

Key words: 多语言大模型, 跨语言迁移, L2T提示策略, 语言思维理论

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了多语言大模型的跨语言知识迁移问题，提出了一种名为L2T的提示策略，解决了知识因输入语言不同而无法有效迁移的问题。

Motivation: 研究发现多语言大模型在不同语言输入时的知识召回表现不一致，尽管以往研究依赖英语提示，但本文从语言与思维理论的角度出发，探索非英语到英语的知识迁移机制。

Method: 提出Language-to-Thought（L2T）提示策略，分析输入语言与内部认知过程的关联，通过实验验证其效果。

Result: 结果显示，英语方法未必总是最优，而模型内部思维与任务知识的对齐对跨语言迁移至关重要；L2T策略能减少模型对输入语言的依赖。

Conclusion: L2T策略在不依赖翻译学习的情况下，促进了跨语言知识整合，为多语言模型优化提供了新思路。

Abstract: Multilingual large language models (LLMs) open up new possibilities for
leveraging information across languages, but their factual knowledge recall
remains inconsistent depending on the input language. While previous studies
have attempted to address this issue through English-based prompting and
evaluation, we explore non-English to English transfer via Language and Thought
Theory. This perspective allows us to examine language-thought binding in LLMs
and uncover why factual knowledge often fails to transfer effectively. We
propose the Language-to-Thought (L2T) prompting strategy, which analyzes the
relationship between input language, internal cognitive processes, and
knowledge. Experimental results challenge the assumption that English-based
approaches consistently outperform other languages and offer a novel insight
that aligning the model's internal thought with the knowledge required for the
task is critical for successful cross-lingual transfer. Furthermore, we show
that applying L2T during training can alleviate LLMs' reliance on the input
language and facilitate cross-linguistic knowledge integration without
translation-based learning. Code and datasets will be available.

</details>


### [103] [MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for Evaluating LLMs and VLMs](https://arxiv.org/abs/2505.24423)
*Zhiwei Liu,Lingfei Qian,Qianqian Xie,Jimin Huang,Kailai Yang,Sophia Ananiadou*

Key words: 多语言, 多模态, 情感分析, 基准, 评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了MMAFFBen，首个多语言多模态情感分析的开源基准，涵盖35种语言和多种任务，并开发了相关数据集和模型。

Motivation: 现有大型语言模型和视觉语言模型在情感分析领域的能力尚未充分探索，主要缺乏全面评估基准和任务复杂性。

Method: 引入MMAFFBen基准，包含文本、图像和视频模态，覆盖四项情感分析任务，并构建MMAFFIn数据集用于微调模型。

Result: 开发了MMAFFLM-3b和MMAFFLM-7b模型，并对多种代表性模型（如GPT-4o-mini）进行了系统性评估。

Conclusion: MMAFFBen填补了多语言多模态情感分析领域的空白，为未来研究提供了重要基准。

Abstract: Large language models and vision-language models (which we jointly call LMs)
have transformed NLP and CV, demonstrating remarkable potential across various
fields. However, their capabilities in affective analysis (i.e. sentiment
analysis and emotion detection) remain underexplored. This gap is largely due
to the absence of comprehensive evaluation benchmarks, and the inherent
complexity of affective analysis tasks. In this paper, we introduce MMAFFBen,
the first extensive open-source benchmark for multilingual multimodal affective
analysis. MMAFFBen encompasses text, image, and video modalities across 35
languages, covering four key affective analysis tasks: sentiment polarity,
sentiment intensity, emotion classification, and emotion intensity. Moreover,
we construct the MMAFFIn dataset for fine-tuning LMs on affective analysis
tasks, and further develop MMAFFLM-3b and MMAFFLM-7b based on it. We evaluate
various representative LMs, including GPT-4o-mini, providing a systematic
comparison of their affective understanding capabilities. This project is
available at https://github.com/lzw108/MMAFFBen.

</details>


### [104] [Donate or Create? Comparing Data Collection Strategies for Emotion-labeled Multimodal Social Media Posts](https://arxiv.org/abs/2505.24427)
*Christopher Bagdon,Aidan Combs,Carina Silberer,Roman Klinger*

Key words: 情感表达, 社交媒体, 多模态, 数据收集, 模型性能

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究比较了研究中创建的与真实的社交媒体帖子的情感表达差异，发现两者在长度、使用文本和图像的方式以及情感原型事件上有所不同，且数据捐助者和创作者的样本在人口统计上存在差异。研究创建的模型可以较好地泛化到真实数据，但需要真实数据以获得更准确的效果评估。

Motivation: 研究旨在探究研究中创建的社交媒体内容与真实内容的差异，以及这些差异如何影响情感表达模型的性能。

Method: 收集了研究创建和真实的多模态社交媒体帖子，标注了情感标签，并从多个维度（如模型性能、文本和图像的使用等）进行比较。

Result: 研究发现，相比真实帖子，研究创建的帖子更长、更依赖文本而非图像表达情感，更集中于情感原型事件；捐助者和创作者样本在人口统计上不同；研究创建的数据对训练泛化能力强的模型有价值，但真实数据对准确评估效果至关重要。

Conclusion: 研究创建的数据可用于训练模型，但需要结合真实数据以获得更准确的性能评估。

Abstract: Accurate modeling of subjective phenomena such as emotion expression requires
data annotated with authors' intentions. Commonly such data is collected by
asking study participants to donate and label genuine content produced in the
real world, or create content fitting particular labels during the study.
Asking participants to create content is often simpler to implement and
presents fewer risks to participant privacy than data donation. However, it is
unclear if and how study-created content may differ from genuine content, and
how differences may impact models. We collect study-created and genuine
multimodal social media posts labeled for emotion and compare them on several
dimensions, including model performance. We find that compared to genuine
posts, study-created posts are longer, rely more on their text and less on
their images for emotion expression, and focus more on emotion-prototypical
events. The samples of participants willing to donate versus create posts are
demographically different. Study-created data is valuable to train models that
generalize well to genuine data, but realistic effectiveness estimates require
genuine data.

</details>


### [105] [Model Unlearning via Sparse Autoencoder Subspace Guided Projections](https://arxiv.org/abs/2505.24428)
*Xu Wang,Zihao Li,Benyou Wang,Yan Hu,Difan Zou*

Key words: 大型语言模型,知识遗忘,稀疏自编码器,子空间投影,对抗鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种新的遗忘框架SSPU，利用稀疏自编码器(SAE)特征实现精确、可解释且鲁棒的模型遗忘，优于现有基线。

Motivation: 研究动机是解决大型语言模型(LLMs)中隐私和安全问题，需要选择性遗忘知识，现有方法在可解释性或防御对抗性提示方面存在不足。

Method: 方法是通过SAE特征驱动的三阶段流程，包括数据驱动的层和特征选择、QR分解子空间构建，以及约束优化控制激活到“无关”子空间。

Result: 在WMDP-Cyber遗忘集和三个效用基准(MMLU、TruthfulQA、GSM8K)上，SSPU将有害知识准确性降低3.22%，并增强对抗鲁棒性。

Conclusion: 结论是该方法展示了子空间引导优化如何实现鲁棒可控的模型行为，揭示了先前遗忘方法的局限性。

Abstract: Large language models (LLMs) store vast amounts of information, making them
powerful yet raising privacy and safety concerns when selective knowledge
removal is required. Existing unlearning strategies, ranging from
gradient-based fine-tuning and model editing to sparse autoencoder (SAE)
steering, either lack interpretability or fail to provide a robust defense
against adversarial prompts. We propose SAE-Guided Subspace Projection
Unlearning (SSPU), a novel framework that leverages SAE features to drive
targeted updates in the model's parameter space, enabling precise,
interpretable, and robust unlearning. SSPU's three-stage pipeline performs
data-driven layer and feature selection, subspace construction via QR
decomposition, and constrained optimization that controls activations into an
"irrelevant" subspace while preserving retained knowledge. Overall, we use SAE
features to construct a subspace that supervises unlearning, refining the loss
and adding a regularization term to guide interpretable parameter updates. In
experiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU,
TruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared
to the strongest baseline. It also improves adversarial robustness, lowering
malicious accuracy under jailbreak prompts compared to baselines. Our findings
expose the limitations of prior unlearning methods and demonstrate how
interpretable subspace-guided optimization can achieve robust, controllable
model behavior.

</details>


### [106] [Exploring the Impact of Occupational Personas on Domain-Specific QA](https://arxiv.org/abs/2505.24448)
*Eojin Kang,Jaehyuk Yu,Juae Kim*

Key words: 人物角色、专业问答、LLMs、PBPs、OPBPs

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨了专业问答任务中人物角色（personas）的影响，发现专业相关角色（PBPs）能略微提升准确性，而职业性格角色（OPBPs）反而可能降低表现，表明角色相关性并非知识有效利用的保障。

Motivation: 探究人物角色对领域特定问答任务的影响，尤其是专业相关角色和职业性格角色在性能上的差异。

Method: 通过引入两种人物角色（PBPs和OPBPs），并在多个科学领域进行实证评估，分析其对问答性能的影响。

Result: PBPs能轻微提升准确性，而OPBPs常导致表现下降，即使语义上与任务相关。

Conclusion: 角色相关性不能保证知识有效利用，甚至可能因认知限制而阻碍知识应用；未来可探索更精细的人物角色表征对LLMs的引导作用。

Abstract: Recent studies on personas have improved the way Large Language Models (LLMs)
interact with users. However, the effect of personas on domain-specific
question-answering (QA) tasks remains a subject of debate. This study analyzes
whether personas enhance specialized QA performance by introducing two types of
persona: Profession-Based Personas (PBPs) (e.g., scientist), which directly
relate to domain expertise, and Occupational Personality-Based Personas (OPBPs)
(e.g., scientific person), which reflect cognitive tendencies rather than
explicit expertise. Through empirical evaluations across multiple scientific
domains, we demonstrate that while PBPs can slightly improve accuracy, OPBPs
often degrade performance, even when semantically related to the task. Our
findings suggest that persona relevance alone does not guarantee effective
knowledge utilization and that they may impose cognitive constraints that
hinder optimal knowledge application. Future research can explore how nuanced
distinctions in persona representations guide LLMs, potentially contributing to
reasoning and knowledge retrieval that more closely mirror human social
conceptualization.

</details>


### [107] [When Large Multimodal Models Confront Evolving Knowledge:Challenges and Pathways](https://arxiv.org/abs/2505.24449)
*Kailin Jiang,Yuntao Du,Yukai Ding,Yuchen Ren,Ning Jiang,Zhi Gao,Zilong Zheng,Lei Liu,Bin Li,Qing Li*

Key words: 动态知识注入、多模态大模型、灾难性遗忘、EVOKE基准、持续学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出EVOKE基准来评估多模态大模型(LMMs)在真实场景中注入动态多模态知识的能力，发现现有方法在动态知识注入上表现差，且微调会导致灾难性遗忘，但文本增强和持续学习方法是潜在的解决方向。

Motivation: 当前大模型难以适应现实世界动态更新的知识，且多模态动态知识注入的研究不足，需系统评估其挑战与解决路径。

Method: 构建EVOKE基准，评估现有知识注入方法在动态多模态任务中的表现，并探索文本/图像增强及持续学习（如Replay、MoELoRA）的效果。

Result: (1) 现有方法对动态知识适应性差；(2) 微调损害指令跟随能力；(3) 文本增强有效，但图像增强无效；(4) 持续学习可缓解遗忘。

Conclusion: 当前动态知识注入方法存在明显局限，需开发更高效稳定的技术，文本增强和持续学习（如MoELoRA）是具有潜力的方向。

Abstract: Large language/multimodal models (LLMs/LMMs) store extensive pre-trained
knowledge but struggle to maintain consistency with real-world updates, making
it difficult to avoid catastrophic forgetting while acquiring evolving
knowledge. Previous work focused on constructing textual knowledge datasets and
exploring knowledge injection in LLMs, lacking exploration of multimodal
evolving knowledge injection in LMMs. To address this, we propose the EVOKE
benchmark to evaluate LMMs' ability to inject multimodal evolving knowledge in
real-world scenarios. Meanwhile, a comprehensive evaluation of multimodal
evolving knowledge injection revealed two challenges: (1) Existing knowledge
injection methods perform terribly on evolving knowledge. (2) Supervised
fine-tuning causes catastrophic forgetting, particularly instruction following
ability is severely compromised. Additionally, we provide pathways and find
that: (1) Text knowledge augmentation during the training phase improves
performance, while image augmentation cannot achieve it. (2) Continual learning
methods, especially Replay and MoELoRA, effectively mitigate forgetting. Our
findings indicate that current knowledge injection methods have many
limitations on evolving knowledge, which motivates further research on more
efficient and stable knowledge injection methods.

</details>


### [108] [Domain Pre-training Impact on Representations](https://arxiv.org/abs/2505.24455)
*Cesar Gonzalez-Gutierrez,Ariadna Quattoni*

Key words: 预训练,Transformer,语料质量,分布相似性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 预训练语料对Transformer表示质量的影响研究

Motivation: 分析预训练语料对学习到的Transformer表示质量的单一影响

Method: 实验比较不同预训练语料（小规模专用语料、通用和专用语料组合）的效果

Result: 小规模专用语料可生成有效表示；通用和专用语料组合的成功取决于目标任务与专用语料的分布相似性

Conclusion: 预训练语料的选择对表示质量至关重要，特别是在任务与语料的分布匹配时

Abstract: This empirical study analyzes the effects of the pre-training corpus on the
quality of learned transformer representations. We focus on the representation
quality induced solely through pre-training. Our experiments show that
pre-training on a small, specialized corpus can yield effective
representations, and that the success of combining a generic and a specialized
corpus depends on the distributional similarity between the target task and the
specialized corpus.

</details>


### [109] [CaMMT: Benchmarking Culturally Aware Multimodal Machine Translation](https://arxiv.org/abs/2505.24456)
*Emilio Villa-Cueva,Sholpan Bolatzhanova,Diana Turmakhan,Kareem Elzeky,Henok Biadglign Ademtew,Alham Fikri Aji,Israel Abebe Azime,Jinheon Baek,Frederico Belcavello,Fermin Cristobal,Jan Christian Blaise Cruz,Mary Dabre,Raj Dabre,Toqeer Ehsan,Naome A Etori,Fauzan Farooqui,Jiahui Geng,Guido Ivetta,Thanmay Jayakumar,Soyeong Jeong,Zheng Wei Lim,Aishik Mandal,Sofia Martinelli,Mihail Minkov Mihaylov,Daniil Orel,Aniket Pramanick,Sukannya Purkayastha,Israfel Salazar,Haiyue Song,Tiago Timponi Torrent,Debela Desalegn Yadeta,Injy Hamed,Atnafu Lambebo Tonja,Thamar Solorio*

Key words: 多模态翻译，文化差异，视觉语言模型，CaMMT，文化特定项

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出CaMMT数据集评估多模态翻译中图像作为文化上下文的效果，发现视觉信息能提升翻译质量。

Motivation: 解决机器翻译中因文化差异导致语言无法充分传递区域特定含义的问题。

Method: 构建包含5800组图像及平行字幕的CaMMT数据集，评估5种VLM在纯文本和图文结合设置下的表现。

Result: 视觉上下文普遍提升翻译质量，尤其在文化特定项、歧义消解和性别准确性上效果显著。

Conclusion: 多模态翻译系统需结合文化背景，CaMMT为相关研究和评估提供了支持。

Abstract: Cultural content poses challenges for machine translation systems due to the
differences in conceptualizations between cultures, where language alone may
fail to convey sufficient context to capture region-specific meanings. In this
work, we investigate whether images can act as cultural context in multimodal
translation. We introduce CaMMT, a human-curated benchmark of over 5,800
triples of images along with parallel captions in English and regional
languages. Using this dataset, we evaluate five Vision Language Models (VLMs)
in text-only and text+image settings. Through automatic and human evaluations,
we find that visual context generally improves translation quality, especially
in handling Culturally-Specific Items (CSIs), disambiguation, and correct
gender usage. By releasing CaMMT, we aim to support broader efforts in building
and evaluating multimodal translation systems that are better aligned with
cultural nuance and regional variation.

</details>


### [110] [VietMix: A Naturally Occurring Vietnamese-English Code-Mixed Corpus with Iterative Augmentation for Machine Translation](https://arxiv.org/abs/2505.24472)
*Hieu Tran,Phuong-Anh Nguyen-Le,Huy Nghiem,Quang-Nhan Nguyen,Wei Ai,Marine Carpuat*

Key words: 机器翻译，代码混合，低资源语言，合成数据，VietMix

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文介绍了VietMix，一个自然产生的代码混合越南语平行语料库及其合成数据生成方法，有效提高了低资源语言代码混合输入的机器翻译性能。

Motivation: 解决低资源语言代码混合输入在机器翻译中的性能问题。

Method: 构建VietMix语料库并开发合成数据生成流程，确保代码混合的语法和语用合理性。

Result: 实验表明，自然与合成数据的结合使模型性能显著提升（COMETkiwi 71.84，XCOMET 81.77），增强模型在49%的情况下优于基线。

Conclusion: VietMix及方法提升了神经机器翻译的生态效度，为其他低资源语言对提供了框架。

Abstract: Machine translation systems fail when processing code-mixed inputs for
low-resource languages. We address this challenge by curating VietMix, a
parallel corpus of naturally occurring code-mixed Vietnamese text paired with
expert English translations. Augmenting this resource, we developed a
complementary synthetic data generation pipeline. This pipeline incorporates
filtering mechanisms to ensure syntactic plausibility and pragmatic
appropriateness in code-mixing patterns. Experimental validation shows our
naturalistic and complementary synthetic data boost models' performance,
measured by translation quality estimation scores, of up to 71.84 on COMETkiwi
and 81.77 on XCOMET. Triangulating positive results with LLM-based assessments,
augmented models are favored over seed fine-tuned counterparts in approximately
49% of judgments (54-56% excluding ties). VietMix and our augmentation
methodology advance ecological validity in neural MT evaluations and establish
a framework for addressing code-mixed translation challenges across other
low-resource pairs.

</details>


### [111] [Towards Effective Code-Integrated Reasoning](https://arxiv.org/abs/2505.24480)
*Fei Bai,Yingqian Min,Beichen Zhang,Zhipeng Chen,Wayne Xin Zhao,Lei Fang,Zheng Liu,Zhongyuan Wang,Ji-Rong Wen*

Key words: 代码集成推理, 强化学习, 工具增强, 训练稳定性, 数学推理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了代码集成推理，通过工具增强的强化学习（RL）提升模型使用外部代码工具的能力，并提出系统性方法改善训练效果和稳定性。

Motivation: 研究模型如何有效利用外部代码工具进行推理，解决工具增强RL在训练中的不稳定问题。

Method: 采用增强的训练策略，平衡探索与稳定性，逐步构建工具使用能力，并在五个数学推理基准上测试。

Result: 模型在多个基准上表现显著优于基线，同时揭示了代码集成推理对模型能力边界和推理效率的提升。

Conclusion: 代码集成推理有效扩展模型能力并提升效率，提出的训练策略改善了RL的稳定性。

Abstract: In this paper, we investigate code-integrated reasoning, where models
generate code when necessary and integrate feedback by executing it through a
code interpreter. To acquire this capability, models must learn when and how to
use external code tools effectively, which is supported by tool-augmented
reinforcement learning (RL) through interactive learning. Despite its benefits,
tool-augmented RL can still suffer from potential instability in the learning
dynamics. In light of this challenge, we present a systematic approach to
improving the training effectiveness and stability of tool-augmented RL for
code-integrated reasoning. Specifically, we develop enhanced training
strategies that balance exploration and stability, progressively building
tool-use capabilities while improving reasoning performance. Through extensive
experiments on five mainstream mathematical reasoning benchmarks, our model
demonstrates significant performance improvements over multiple competitive
baselines. Furthermore, we conduct an in-depth analysis of the mechanism and
effect of code-integrated reasoning, revealing several key insights, such as
the extension of model's capability boundaries and the simultaneous improvement
of reasoning efficiency through code integration. All data and code for
reproducing this work are available at: https://github.com/RUCAIBox/CIR.

</details>


### [112] [TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence](https://arxiv.org/abs/2505.24500)
*Guiyang Hou,Xing Gao,Yuchuan Wu,Xiang Huang,Wenqi Zhang,Zhe Zheng,Yongliang Shen,Jialu Du,Fei Huang,Yongbin Li,Weiming Lu*

Key words: 大语言模型，社交智能，TimeHC-RL，认知强化学习，System 1，System 2

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: TimeHC-RL方法通过时间感知分层认知强化学习提升大语言模型在社交智能领域的表现，实验证明其优于传统的System 2 RL方法。

Motivation: 大语言模型在数学等领域表现良好，但在社交智能领域认知发展仍有不足，需要结合直觉反应（System 1）和深思熟虑（System 2）的混合认知模式。

Method: 提出Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL)，并通过五种后训练范式和两种测试时干预范式在八个数据集上验证其有效性。

Result: TimeHC-RL表现优于System 2 RL，使7B模型性能接近先进模型。

Conclusion: TimeHC-RL有效提升大语言模型的社交智能，并揭示了后训练和测试时干预的多个有价值的发现。

Abstract: Recently, Large Language Models (LLMs) have made significant progress in
IQ-related domains that require careful thinking, such as mathematics and
coding. However, enhancing LLMs' cognitive development in social domains,
particularly from a post-training perspective, remains underexplored.
Recognizing that the social world follows a distinct timeline and requires a
richer blend of cognitive modes (from intuitive reactions (System 1) and
surface-level thinking to deliberate thinking (System 2)) than mathematics,
which primarily relies on System 2 cognition (careful, step-by-step reasoning),
we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning
(TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we
systematically explore improving LLMs' social intelligence and validate the
effectiveness of the TimeHC-RL method, through five other post-training
paradigms and two test-time intervention paradigms on eight datasets with
diverse data patterns. Experimental results reveal the superiority of our
proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It
gives the 7B backbone model wings, enabling it to rival the performance of
advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic
exploration from post-training and test-time interventions perspectives to
improve LLMs' social intelligence has uncovered several valuable insights.

</details>


### [113] [Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors](https://arxiv.org/abs/2505.24523)
*Andrea Pedrotti,Michele Papucci,Cristiano Ciaccio,Alessio Miaschi,Giovanni Puccetti,Felice Dell'Orletta,Andrea Esuli*

Key words: Generative AI, Large Language Models, Machine-Generated Text, Adversarial Attacks, Detection

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了生成式AI和大型语言模型（LLMs）的进步带来的真实性合成内容问题，提出了一种测试MGT检测器对抗性攻击的方法。

Motivation: 随着生成式AI的进步，恶意使用合成内容的担忧增加，而现有的MGT检测器缺乏对真实场景的泛化能力评估。

Method: 通过直接偏好优化（DPO）微调语言模型，使MGT风格更接近人类文本（HWT），挑战检测器对风格线索的依赖。

Result: 研究表明，检测器容易被少量样本欺骗，导致检测性能显著下降。

Conclusion: 强调了改进检测方法并增强其对未见领域文本的鲁棒性的重要性。

Abstract: Recent advancements in Generative AI and Large Language Models (LLMs) have
enabled the creation of highly realistic synthetic content, raising concerns
about the potential for malicious use, such as misinformation and manipulation.
Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the
lack of robust benchmarks that assess generalization to real-world scenarios.
In this work, we present a pipeline to test the resilience of state-of-the-art
MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed
adversarial attacks. To challenge the detectors, we fine-tune language models
using Direct Preference Optimization (DPO) to shift the MGT style toward
human-written text (HWT). This exploits the detectors' reliance on stylistic
clues, making new generations more challenging to detect. Additionally, we
analyze the linguistic shifts induced by the alignment and which features are
used by detectors to detect MGT texts. Our results show that detectors can be
easily fooled with relatively few examples, resulting in a significant drop in
detection performance. This highlights the importance of improving detection
methods and making them robust to unseen in-domain texts.

</details>


### [114] [Limited-Resource Adapters Are Regularizers, Not Linguists](https://arxiv.org/abs/2505.24525)
*Marcell Fekete,Nathaniel R. Robinson,Ernests Lavrinovics,E. Djeride Jean-Baptiste,Raj Dabre,Johannes Bjerva,Heather Lent*

Key words: 适配器, 跨语言迁移, 低资源语言, 机器翻译, 跨注意力微调

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文通过结合适配器集成和跨注意力微调的方法，提升了三种低资源克里奥尔语的翻译性能，但发现适配器的性能与语言亲缘性无显著相关。

Motivation: 研究旨在利用高资源相关语言的跨语言迁移策略，改进低资源语言技术，尤其是机器翻译。

Method: 采用适配器集成方法，并结合预训练翻译模型的跨注意力微调，针对三种低资源克里奥尔语进行实验。

Result: 方法显著优于基线，但适配器性能与语言亲缘性无明显关联，随机初始化的适配器同样有效。

Conclusion: 适配器的优势可能在于参数正则化而非信息迁移，神经网络方法对语言知识的利用方式并不直观。

Abstract: Cross-lingual transfer from related high-resource languages is a
well-established strategy to enhance low-resource language technologies. Prior
work has shown that adapters show promise for, e.g., improving low-resource
machine translation (MT). In this work, we investigate an adapter souping
method combined with cross-attention fine-tuning of a pre-trained MT model to
leverage language transfer for three low-resource Creole languages, which
exhibit relatedness to different language groups across distinct linguistic
dimensions. Our approach improves performance substantially over baselines.
However, we find that linguistic relatedness -- or even a lack thereof -- does
not covary meaningfully with adapter performance. Surprisingly, our
cross-attention fine-tuning approach appears equally effective with randomly
initialized adapters, implying that the benefit of adapters in this setting
lies in parameter regularization, and not in meaningful information transfer.
We provide analysis supporting this regularization hypothesis. Our findings
underscore the reality that neural language processing involves many success
factors, and that not all neural methods leverage linguistic knowledge in
intuitive ways.

</details>


### [115] [DEEPQUESTION: Systematic Generation of Real-World Challenges for Evaluating LLMs Performance](https://arxiv.org/abs/2505.24532)
*Ali Khoramfar,Ali Ramezani,Mohammad Mahdi Mohajeri,Mohammad Javad Dousti,Majid Nili Ahmadabadi,Heshaam Faili*

Key words: LLMs, DeepQuestion, Bloom's taxonomy, cognitive benchmarks, reasoning skills

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DeepQuestion是一种可扩展的自动框架，通过基于布鲁姆分类法增强现有数据集并创建追踪原始解决路径的新问题，来测试高级认知技能，实验显示LLMs在高阶任务上表现显著下降。

Motivation: 标准基准测试下LLMs表现优异，但在现实任务中表现不佳，因此需要多样化的认知基准来推动LLM发展。

Method: 引入DeepQuestion框架，基于布鲁姆分类法增强数据集并设计新问题，以测试评估性和创造性技能。

Result: 在10个开源和专有模型上的实验显示，高阶任务性能下降高达70%，揭示了深度推理的不足。

Conclusion: 研究强调了认知多样性基准的重要性，以促进LLM进步，DeepQuestion及相关数据集将在论文接受后发布。

Abstract: LLMs often excel on standard benchmarks but falter on real-world tasks. We
introduce DeepQuestion, a scalable automated framework that augments existing
datasets based on Bloom's taxonomy and creates novel questions that trace
original solution paths to probe evaluative and creative skills. Extensive
experiments across ten open-source and proprietary models, covering both
general-purpose and reasoning LLMs, reveal substantial performance drops (even
up to 70% accuracy loss) on higher-order tasks, underscoring persistent gaps in
deep reasoning. Our work highlights the need for cognitively diverse benchmarks
to advance LLM progress. DeepQuestion and related datasets will be released
upon acceptance of the paper.

</details>


### [116] [Don't Erase, Inform! Detecting and Contextualizing Harmful Language in Cultural Heritage Collections](https://arxiv.org/abs/2505.24538)
*Orfeas Menis Mastromichalakis,Jason Liartis,Kristina Rose,Antoine Isaac,Giorgos Stamou*

Key words: 文化遗产，AI工具，冒犯性词汇，历史偏见，包容性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 开发了一个AI工具，用于检测文化遗产元数据中的冒犯性词汇，并提供历史背景和当代理解的上下文分析，旨在提高文化遗产数据的包容性和可访问性。

Motivation: 文化遗产数据中包含的历史偏见和冒犯性描述需要被识别和纠正，但由于数据和任务的复杂性，传统方法难以胜任。

Method: 结合多语言词汇库（由边缘群体、研究者和文化遗产专业人士共同创建）、传统自然语言处理技术和大型语言模型（LLMs），开发了一个AI工具。

Result: 该工具已处理超过790万条记录，能够识别冒犯性词汇并为其提供背景信息，而非简单删除这些词汇。

Conclusion: 通过使偏见可见并提供可操作的见解，该方法有助于创建更具包容性和可访问性的文化遗产数据集。

Abstract: Cultural Heritage (CH) data hold invaluable knowledge, reflecting the
history, traditions, and identities of societies, and shaping our understanding
of the past and present. However, many CH collections contain outdated or
offensive descriptions that reflect historical biases. CH Institutions (CHIs)
face significant challenges in curating these data due to the vast scale and
complexity of the task. To address this, we develop an AI-powered tool that
detects offensive terms in CH metadata and provides contextual insights into
their historical background and contemporary perception. We leverage a
multilingual vocabulary co-created with marginalized communities, researchers,
and CH professionals, along with traditional NLP techniques and Large Language
Models (LLMs). Available as a standalone web app and integrated with major CH
platforms, the tool has processed over 7.9 million records, contextualizing the
contentious terms detected in their metadata. Rather than erasing these terms,
our approach seeks to inform, making biases visible and providing actionable
insights for creating more inclusive and accessible CH collections.

</details>


### [117] [Localizing Persona Representations in LLMs](https://arxiv.org/abs/2505.24539)
*Celia Cintas,Miriam Rateike,Erik Miehling,Elizabeth Daly,Skyler Speakman*

Key words: 大型语言模型, 人格编码, 表征空间, 降维, 伦理观, 政治意识形态

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLMs）中不同人格特征如何及在何处被编码，发现人格表征在解码器层的最后三分之一部分差异最大，且某些伦理观存在重叠，而政治意识形态则更独立。

Motivation: 探索LLMs如何内部表征不同人格特征，以改进模型输出的特定人类特质调控。

Method: 采用降维和模式识别方法，分析不同模型层的人格编码差异，并聚焦特定层的激活模式。

Result: 人格表征差异集中在解码器最后三分之一层；伦理观（如道德虚无主义与功利主义）存在重叠，政治意识形态（如保守主义与自由主义）则分布更独立。

Conclusion: 研究揭示了LLMs内部信息表征的机制，为未来优化模型输出的特定人格调控提供依据。

Abstract: We present a study on how and where personas -- defined by distinct sets of
human characteristics, values, and beliefs -- are encoded in the representation
space of large language models (LLMs). Using a range of dimension reduction and
pattern recognition methods, we first identify the model layers that show the
greatest divergence in encoding these representations. We then analyze the
activations within a selected layer to examine how specific personas are
encoded relative to others, including their shared and distinct embedding
spaces. We find that, across multiple pre-trained decoder-only LLMs, the
analyzed personas show large differences in representation space only within
the final third of the decoder layers. We observe overlapping activations for
specific ethical perspectives -- such as moral nihilism and utilitarianism --
suggesting a degree of polysemy. In contrast, political ideologies like
conservatism and liberalism appear to be represented in more distinct regions.
These findings help to improve our understanding of how LLMs internally
represent information and can inform future efforts in refining the modulation
of specific human traits in LLM outputs. Warning: This paper includes
potentially offensive sample statements.

</details>


### [118] [Cross-Attention Speculative Decoding](https://arxiv.org/abs/2505.24544)
*Wei Zhong,Manasa Bharadwaj,Yixiao Wang,Nikhil Verma,Yipeng Ji,Chul Lee*

Key words: 推测解码、Transformer、跨注意力、训练效率、大型语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Budget EAGLE (Beagle) 是一种新型的跨注意力 Transformer 解码器架构，用于加速大型语言模型推理，性能与当前主流自注意力模型相当，但架构更简单、训练效率更高。

Motivation: 现有推测解码方法依赖自注意力 Transformer 解码器，架构复杂且难以跨模型泛化。作者希望提出一种简化架构，同时保持高效性能。

Method: 采用跨注意力 Transformer 解码器（Beagle），省去辅助组件，并提出两阶段块注意力训练方法以稳定训练。

Result: Beagle 在多模型和数据集上取得了竞争性的推理加速，训练效率高于 EAGLE-v2。

Conclusion: Beagle 为推测解码提供了一种更简单高效的架构选择。

Abstract: Speculative decoding (SD) is a widely adopted approach for accelerating
inference in large language models (LLMs), particularly when the draft and
target models are well aligned. However, state-of-the-art SD methods typically
rely on tightly coupled, self-attention-based Transformer decoders, often
augmented with auxiliary pooling or fusion layers. This coupling makes them
increasingly complex and harder to generalize across different models. We
present Budget EAGLE (Beagle), the first, to our knowledge,
cross-attention-based Transformer decoder SD model that achieves performance on
par with leading self-attention SD models (EAGLE-v2) while eliminating the need
for pooling or auxiliary components, simplifying the architecture, improving
training efficiency, and maintaining stable memory usage during training-time
simulation. To enable effective training of this novel architecture, we propose
Two-Stage Block-Attention Training, a new method that achieves training
stability and convergence efficiency in block-level attention scenarios.
Extensive experiments across multiple LLMs and datasets show that Beagle
achieves competitive inference speedups and higher training efficiency than
EAGLE-v2, offering a strong alternative for architectures in speculative
decoding.

</details>


### [119] [A*-Thought: Efficient Reasoning via Bidirectional Compression for Low-Resource Settings](https://arxiv.org/abs/2505.24550)
*Xiaoang Xu,Shuo Wang,Xu Han,Zhenghao Liu,Huijia Wu,Peipei Li,Zhiyuan Liu,Maosong Sun,Zhaofeng He*

Key words: 大型推理模型, A*搜索, 效率优化, 树搜索, 双向重要性估计

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: A*-Thought是一个基于树搜索的统一框架，用于高效压缩大型推理模型的思维链，平衡性能与效率。

Motivation: 现有方法因假设过度思考而压缩思维链导致性能下降，A*-Thought通过搜索树和A*算法解决这一问题。

Method: 将推理过程建模为搜索树，结合A*算法和路径成本函数，提出双向重要性估计机制以提高搜索效率。

Result: 实验表明A*-Thought能显著提升性能（如QwQ-32B提高2.39倍）并减少50%输出长度，且兼容多种模型。

Conclusion: A*-Thought在巨大搜索空间中有效平衡性能与效率，具有广泛适用性。

Abstract: Large Reasoning Models (LRMs) achieve superior performance by extending the
thought length. However, a lengthy thinking trajectory leads to reduced
efficiency. Most of the existing methods are stuck in the assumption of
overthinking and attempt to reason efficiently by compressing the
Chain-of-Thought, but this often leads to performance degradation. To address
this problem, we introduce A*-Thought, an efficient tree search-based unified
framework designed to identify and isolate the most essential thoughts from the
extensive reasoning chains produced by these models. It formulates the
reasoning process of LRMs as a search tree, where each node represents a
reasoning span in the giant reasoning space. By combining the A* search
algorithm with a cost function specific to the reasoning path, it can
efficiently compress the chain of thought and determine a reasoning path with
high information density and low cost. In addition, we also propose a
bidirectional importance estimation mechanism, which further refines this
search process and enhances its efficiency beyond uniform sampling. Extensive
experiments on several advanced math tasks show that A*-Thought effectively
balances performance and efficiency over a huge search space. Specifically,
A*-Thought can improve the performance of QwQ-32B by 2.39$\times$ with
low-budget and reduce the length of the output token by nearly 50% with
high-budget. The proposed method is also compatible with several other LRMs,
demonstrating its generalization capability. The code can be accessed at:
https://github.com/AI9Stars/AStar-Thought.

</details>


### [120] [CREFT: Sequential Multi-Agent LLM for Character Relation Extraction](https://arxiv.org/abs/2505.24553)
*Ye Eun Chun,Taeyoon Hwang,Seung-won Hwang,Byung-Hak Kim*

Key words: 人物关系提取，大型语言模型，知识蒸馏，叙事分析

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 介绍CREFT框架，用于高效提取和分析长篇叙事中的复杂人物关系，优于现有方法。

Motivation: 解决现有方法难以处理长篇叙事中细致人物关系的局限性。

Method: 采用LLM代理的知识蒸馏构建基础人物图谱，并迭代优化人物构成、关系提取、角色识别和群体分配。

Result: 在韩剧数据集上，CREFT在准确性和完整性上显著优于单代理LLM基线。

Conclusion: CREFT为娱乐、出版和教育领域提供了高效的叙事理解和剧本审查工具。

Abstract: Understanding complex character relations is crucial for narrative analysis
and efficient script evaluation, yet existing extraction methods often fail to
handle long-form narratives with nuanced interactions. To address this
challenge, we present CREFT, a novel sequential framework leveraging
specialized Large Language Model (LLM) agents. First, CREFT builds a base
character graph through knowledge distillation, then iteratively refines
character composition, relation extraction, role identification, and group
assignments. Experiments on a curated Korean drama dataset demonstrate that
CREFT significantly outperforms single-agent LLM baselines in both accuracy and
completeness. By systematically visualizing character networks, CREFT
streamlines narrative comprehension and accelerates script review -- offering
substantial benefits to the entertainment, publishing, and educational sectors.

</details>


### [121] [Bench4KE: Benchmarking Automated Competency Question Generation](https://arxiv.org/abs/2505.24554)
*Anna Sofia Lippolis,Minh Davide Ragagni,Paolo Ciancarini,Andrea Giovanni Nuzzolese,Valentina Presutti*

Key words: Large Language Models, Knowledge Engineering, Competency Questions, Benchmarking

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了Bench4KE，一个基于API的可扩展基准测试系统，用于评估基于LLM的自动化知识工程工具，特别是自动生成能力问题（CQs）的工具。

Motivation: 当前缺乏对自动生成能力问题（CQs）工具评估的标准化，影响了方法论的严谨性和结果的复现与比较。

Method: 引入Bench4KE系统，通过其API评估工具，并使用一套相似性度量标准来评估生成的CQs质量。

Result: 比较了四种基于LLM的CQ生成系统，并建立了未来研究的基线。

Conclusion: Bench4KE填补了评估标准化的空白，并为未来知识工程自动化任务提供了扩展空间。

Abstract: The availability of Large Language Models (LLMs) presents a unique
opportunity to reinvigorate research on Knowledge Engineering (KE) automation,
a trend already evident in recent efforts developing LLM-based methods and
tools for the automatic generation of Competency Questions (CQs). However, the
evaluation of these tools lacks standardisation. This undermines the
methodological rigour and hinders the replication and comparison of results. To
address this gap, we introduce Bench4KE, an extensible API-based benchmarking
system for KE automation. Its first release focuses on evaluating tools that
generate CQs automatically. CQs are natural language questions used by ontology
engineers to define the functional requirements of an ontology. Bench4KE
provides a curated gold standard consisting of CQ datasets from four real-world
ontology projects. It uses a suite of similarity metrics to assess the quality
of the CQs generated. We present a comparative analysis of four recent CQ
generation systems, which are based on LLMs, establishing a baseline for future
research. Bench4KE is also designed to accommodate additional KE automation
tasks, such as SPARQL query generation, ontology testing and drafting. Code and
datasets are publicly available under the Apache 2.0 license.

</details>


### [122] [Improving Language and Modality Transfer in Translation by Character-level Modeling](https://arxiv.org/abs/2505.24561)
*Ioannis Tsiamas,David Dale,Marta R. Costa-jussà*

Key words: 低资源语言、字符级翻译、跨模态知识转移、SONAR、语音翻译

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种基于字符的方法，利用SONAR嵌入空间和教师-学生框架，通过跨语言和跨模态知识转移，显著提升低资源语言的翻译能力，并在文本和语音翻译任务中取得优异表现。

Motivation: 当前翻译系统仅覆盖全球5%的语言，扩展低资源语言支持需要数据高效的方法，该研究旨在通过字符级方法和跨模态知识转移解决这一问题。

Method: 采用基于字符的编码器-解码器框架，结合SONAR嵌入空间和教师-学生训练。通过并行翻译数据训练字符级编码器，并利用ASR数据轻量适配器连接多语言CTC ASR模型（MMS）。

Result: 在FLORES+的75种语言文本翻译中表现优于传统子词模型，低资源场景下优势显著；语音翻译在FLEURS基准的33种语言中达到SOTA，超越监督和级联模型。

Conclusion: 字符级方法通过跨模态知识转移，显著提升低资源语言的翻译性能，尤其在零样本场景下表现突出，为扩大语言覆盖提供了有效途径。

Abstract: Current translation systems, despite being highly multilingual, cover only 5%
of the world's languages. Expanding language coverage to the long-tail of
low-resource languages requires data-efficient methods that rely on
cross-lingual and cross-modal knowledge transfer. To this end, we propose a
character-based approach to improve adaptability to new languages and
modalities. Our method leverages SONAR, a multilingual fixed-size embedding
space with different modules for encoding and decoding. We use a
teacher-student approach with parallel translation data to obtain a
character-level encoder. Then, using ASR data, we train a lightweight adapter
to connect a massively multilingual CTC ASR model (MMS), to the character-level
encoder, potentially enabling speech translation from 1,000+ languages.
Experimental results in text translation for 75 languages on FLORES+
demonstrate that our character-based approach can achieve better language
transfer than traditional subword-based models, especially outperforming them
in low-resource settings, and demonstrating better zero-shot generalizability
to unseen languages. Our speech adaptation, maximizing knowledge transfer from
the text modality, achieves state-of-the-art results in speech-to-text
translation on the FLEURS benchmark on 33 languages, surpassing previous
supervised and cascade models, albeit being a zero-shot model with minimal
supervision from ASR data.

</details>


### [123] [NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization](https://arxiv.org/abs/2505.24575)
*Hyuntak Kim,Byung-Hak Kim*

Key words: 叙事摘要, 多代理LLM, 对话转换, 分层摘要, BERTScore

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: NexusSum是一种无需微调的多代理LLM框架，通过对话到描述的转换和分层多LLM摘要技术，显著提升了长形式叙事（如书籍、电影、电视剧本）的摘要质量，BERTScore（F1）最高提升30.0%。

Motivation: 现有LLM在处理长形式叙事的复杂情节、角色互动和主题一致性方面存在挑战，需要一种无需微调且高效的摘要方法。

Method: NexusSum采用两种创新方法：1）对话到描述的转换，统一处理角色对话和描述文本；2）分层多LLM摘要，通过结构化流水线优化分块处理和输出长度控制。

Result: 在书籍、电影和电视剧本上，NexusSum的BERTScore（F1）最高提升30.0%，达到了叙事摘要的新最高水平。

Conclusion: 多代理LLM框架在处理长形式内容上表现优异，为多样化叙事领域的结构化摘要提供了可扩展的解决方案。

Abstract: Summarizing long-form narratives--such as books, movies, and TV
scripts--requires capturing intricate plotlines, character interactions, and
thematic coherence, a task that remains challenging for existing LLMs. We
introduce NexusSum, a multi-agent LLM framework for narrative summarization
that processes long-form text through a structured, sequential
pipeline--without requiring fine-tuning. Our approach introduces two key
innovations: (1) Dialogue-to-Description Transformation: A narrative-specific
preprocessing method that standardizes character dialogue and descriptive text
into a unified format, improving coherence. (2) Hierarchical Multi-LLM
Summarization: A structured summarization pipeline that optimizes chunk
processing and controls output length for accurate, high-quality summaries. Our
method establishes a new state-of-the-art in narrative summarization, achieving
up to a 30.0% improvement in BERTScore (F1) across books, movies, and TV
scripts. These results demonstrate the effectiveness of multi-agent LLMs in
handling long-form content, offering a scalable approach for structured
summarization in diverse storytelling domains.

</details>


### [124] [GATE: General Arabic Text Embedding for Enhanced Semantic Textual Similarity with Matryoshka Representation Learning and Hybrid Loss Training](https://arxiv.org/abs/2505.24581)
*Omer Nacar,Anis Koubaa,Serry Sibaee,Yasser Al-Habashi,Adel Ammar,Wadii Boulila*

Key words: 阿拉伯语, 语义文本相似性, GATE模型, Matryoshka表示学习, 混合损失训练

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了针对阿拉伯语的通用文本嵌入模型GATE，通过结合Matryoshka表示学习和混合损失训练方法，在MTEB基准测试中实现了最先进的性能，比包括OpenAI在内的大型模型性能提升20-25%。

Motivation: 阿拉伯语在语义文本相似性任务上的研究因缺乏高质量数据集和预训练模型而受限，本研究旨在解决这一问题。

Method: GATE模型采用Matryoshka表示学习和混合损失训练方法，结合阿拉伯语三元组数据集进行自然语言推理任务的训练。

Result: GATE在STS基准测试中性能优于包括OpenAI在内的大型模型，性能提升20-25%，能有效捕捉阿拉伯语的独特语义细节。

Conclusion: GATE模型显著提升了阿拉伯语语义文本相似性任务的性能，填补了该领域的研究空白。

Abstract: Semantic textual similarity (STS) is a critical task in natural language
processing (NLP), enabling applications in retrieval, clustering, and
understanding semantic relationships between texts. However, research in this
area for the Arabic language remains limited due to the lack of high-quality
datasets and pre-trained models. This scarcity of resources has restricted the
accurate evaluation and advance of semantic similarity in Arabic text. This
paper introduces General Arabic Text Embedding (GATE) models that achieve
state-of-the-art performance on the Semantic Textual Similarity task within the
MTEB benchmark. GATE leverages Matryoshka Representation Learning and a hybrid
loss training approach with Arabic triplet datasets for Natural Language
Inference, which are essential for enhancing model performance in tasks that
demand fine-grained semantic understanding. GATE outperforms larger models,
including OpenAI, with a 20-25% performance improvement on STS benchmarks,
effectively capturing the unique semantic nuances of Arabic.

</details>


### [125] [Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis](https://arxiv.org/abs/2505.24593)
*Junzhuo Li,Bo Wang,Xiuze Zhou,Peijie Jiang,Jia Liu,Xuming Hu*

Key words: MoE模型, 可解释性, 跨层次归因, 动态路由, 语义驱动

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种跨层次归因算法，用于分析稀疏MoE架构的动态路由与专家交互，揭示了MoE模型通过分层协作实现高效性，并提出了平衡效率、专业化和鲁棒性的设计原则。

Motivation: 现有归因方法无法捕捉稀疏MoE架构中的动态路由与专家交互，因此需要新方法来增强MoE模型的可解释性。

Method: 提出跨层次归因算法，对比稀疏MoE（如Qwen 1.5-MoE）与密集模型（如Qwen 1.5-7B）的行为模式。

Result: MoE模型通过分层协作（早层筛选专家，晚层精炼知识）实现37%的单层效率提升，并证实语义驱动路由（注意力头与专家强相关，r=0.68）。架构深度影响鲁棒性，任务敏感性指导专家分配。

Conclusion: MoE模型通过共享专家处理通用任务、路由专家处理专业任务实现高效协作，研究结果为MoE设计提供了平衡效率、专业化和鲁棒性的指导原则。

Abstract: The interpretability of Mixture-of-Experts (MoE) models, especially those
with heterogeneous designs, remains underexplored. Existing attribution methods
for dense models fail to capture dynamic routing-expert interactions in sparse
MoE architectures. To address this issue, we propose a cross-level attribution
algorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE,
Mixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mixtral-7B). Results
show MoE models achieve 37% higher per-layer efficiency via a "mid-activation,
late-amplification" pattern: early layers screen experts, while late layers
refine knowledge collaboratively. Ablation studies reveal a "basic-refinement"
framework--shared experts handle general tasks (entity recognition), while
routed experts specialize in domain-specific processing (geographic
attributes). Semantic-driven routing is evidenced by strong correlations
between attention heads and experts (r=0.68), enabling task-aware coordination.
Notably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates
expert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10
experts) through shared expert redundancy, whereas shallow OLMoE suffers severe
degradation (76% drop). Task sensitivity further guides design: core-sensitive
tasks (geography) require concentrated expertise, while distributed-tolerant
tasks (object attributes) leverage broader participation. These insights
advance MoE interpretability, offering principles to balance efficiency,
specialization, and robustness.

</details>


### [126] [Explainable Depression Detection using Masked Hard Instance Mining](https://arxiv.org/abs/2505.24609)
*Patawee Prakrankamanant,Shinji Watanabe,Ekapol Chuangsuwanich*

Key words: 抑郁症检测、可解释性、注意力机制、MHIM

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种名为MHIM的方法，通过掩码注意力权重提升文本抑郁症检测的可解释性，并在泰语和英语数据集上验证了其有效性。

Motivation: 现有的抑郁症检测模型虽然能提供预测结果，但缺乏对预测原因的解释，影响了系统的可信度。

Method: 采用Masked Hard Instance Mining (MHIM)方法，通过策略性地掩码注意力权重，迫使模型关注更广泛的显著特征。

Result: 在泰语(Thai-Maywe)和英语(DAIC-WOZ)数据集上，MHIM显著提高了预测准确性和可解释性。

Conclusion: MHIM通过提升可解释性，增强了抑郁症检测系统的可信度和实用性。

Abstract: This paper addresses the critical need for improved explainability in
text-based depression detection. While offering predictive outcomes, current
solutions often overlook the understanding of model predictions which can
hinder trust in the system. We propose the use of Masked Hard Instance Mining
(MHIM) to enhance the explainability in the depression detection task. MHIM
strategically masks attention weights within the model, compelling it to
distribute attention across a wider range of salient features. We evaluate MHIM
on two datasets representing distinct languages: Thai (Thai-Maywe) and English
(DAIC-WOZ). Our results demonstrate that MHIM significantly improves
performance in terms of both prediction accuracy and explainability metrics.

</details>


### [127] [When Harry Meets Superman: The Role of The Interlocutor in Persona-Based Dialogue Generation](https://arxiv.org/abs/2505.24613)
*Daniela Occhipinti,Marco Guerini,Malvina Nissim*

Key words: 对话代理, 个性化, 对话一致性, 多样性, 对象适应

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了对话代理如何结合自身和对话对象的个性化信息提升对话一致性和多样性，通过实验验证了对象信息的显性影响，但模型在陌生对象上表现较差。

Motivation: 探索对话代理如何利用自身和对话对象的个性化信息提高对话质量，尤其是对象信息的影响。

Method: 通过设计实验，包括对比提供或隐藏对话对象信息、测试熟悉/陌生对象及话题、额外微调等方法，结合自动和人工评估。

Result: 结果显示提供对象信息有助于识别目标说话者，但模型对陌生对象适应不足；零样本下大型语言模型易复制传记细节。

Conclusion: 对话代理需进一步优化以适应陌生对象，避免过度依赖传记细节。

Abstract: Endowing dialogue agents with persona information has proven to significantly
improve the consistency and diversity of their generations. While much focus
has been placed on aligning dialogues with provided personas, the adaptation to
the interlocutor's profile remains largely underexplored. In this work, we
investigate three key aspects: (1) a model's ability to align responses with
both the provided persona and the interlocutor's; (2) its robustness when
dealing with familiar versus unfamiliar interlocutors and topics, and (3) the
impact of additional fine-tuning on specific persona-based dialogues. We
evaluate dialogues generated with diverse speaker pairings and topics, framing
the evaluation as an author identification task and employing both
LLM-as-a-judge and human evaluations. By systematically masking or disclosing
information about the interlocutor, we assess its impact on dialogue
generation. Results show that access to the interlocutor's persona improves the
recognition of the target speaker, while masking it does the opposite. Although
models generalise well across topics, they struggle with unfamiliar
interlocutors. Finally, we found that in zero-shot settings, LLMs often copy
biographical details, facilitating identification but trivialising the task.

</details>


### [128] [Harnessing Large Language Models for Scientific Novelty Detection](https://arxiv.org/abs/2505.24615)
*Yan Liu,Zonglin Yang,Soujanya Poria,Thanh-Son Nguyen,Erik Cambria*

Key words: 大语言模型, 新颖性检测, 数据集构建, 创意检索, NLP

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了利用大语言模型(LLMs)进行科学新颖性检测(ND)，并提出了两个新数据集和一种轻量级检索器方法，以解决现有技术在文本相似性与创意概念之间的差距问题。

Motivation: 在科学快速发展的时代，识别新颖研究创意对学术界至关重要，但由于缺乏合适的基准数据集和现有NLP技术无法直接解决文本相似性与创意概念的差异，研究受到限制。

Method: 通过构建基于论文关系的闭包集合并用LLMs总结其主要思想，提出了一种轻量级检索器，通过从LLMs中提取创意层面的知识来对齐相似概念的创意，实现高效的LLM新颖性检测。

Result: 实验表明，该方法在提出的基准数据集上持续优于其他方法，适用于创意检索和新颖性检测任务。

Conclusion: 论文成功利用LLMs和新型数据集解决了科学新颖性检测的挑战，为未来研究提供了高效准确的工具。

Abstract: In an era of exponential scientific growth, identifying novel research ideas
is crucial and challenging in academia. Despite potential, the lack of an
appropriate benchmark dataset hinders the research of novelty detection. More
importantly, simply adopting existing NLP technologies, e.g., retrieving and
then cross-checking, is not a one-size-fits-all solution due to the gap between
textual similarity and idea conception. In this paper, we propose to harness
large language models (LLMs) for scientific novelty detection (ND), associated
with two new datasets in marketing and NLP domains. To construct the
considerate datasets for ND, we propose to extract closure sets of papers based
on their relationship, and then summarize their main ideas based on LLMs. To
capture idea conception, we propose to train a lightweight retriever by
distilling the idea-level knowledge from LLMs to align ideas with similar
conception, enabling efficient and accurate idea retrieval for LLM novelty
detection. Experiments show our method consistently outperforms others on the
proposed benchmark datasets for idea retrieval and ND tasks. Codes and data are
available at https://anonymous.4open.science/r/NoveltyDetection-10FB/.

</details>


### [129] [Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX](https://arxiv.org/abs/2505.24616)
*Nikita Martynov,Anastasia Mordasheva,Dmitriy Gorbetskiy,Danil Astafurov,Ulyana Isaeva,Elina Basyrova,Sergey Skachkov,Victoria Berestova,Nikolay Ivanov,Valeriia Zanina,Alena Fenogenova*

Key words: POLLUX, LLM评估, 生成能力, 俄语, 自动化评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: POLLUX是一个开源基准，用于评估俄语大型语言模型（LLM）的生成能力，通过细化标准和评分协议提升评估可解释性，覆盖35种任务类型，包含2100个手动编写的提示，并发布LLM-as-a-Judge评估工具。

Motivation: 传统评估方法依赖资源密集的人工侧比，POLLUX旨在提供透明、可扩展且可解释的评估方案，减少人力和成本。

Method: 定义任务类型的详细标准及评分协议，构建覆盖多领域的任务库，并训练LLM-as-a-Judge模型进行自动评估。

Result: POLLUX包含2100个专业编写的提示和35种任务分类，发布的评估模型（7B和32B）可替代人工评估，提高效率与精确性。

Conclusion: 该基准通过结构化标准和自动化工具为LLM评估提供高效、透明的解决方案，推动模型开发与优化。

Abstract: We introduce POLLUX, a comprehensive open-source benchmark designed to
evaluate the generative capabilities of large language models (LLMs) in
Russian. Our main contribution is a novel evaluation methodology that enhances
the interpretability of LLM assessment. For each task type, we define a set of
detailed criteria and develop a scoring protocol where models evaluate
responses and provide justifications for their ratings. This enables
transparent, criteria-driven evaluation beyond traditional resource-consuming,
side-by-side human comparisons. POLLUX includes a detailed, fine-grained
taxonomy of 35 task types covering diverse generative domains such as code
generation, creative writing, and practical assistant use cases, totaling 2,100
manually crafted and professionally authored prompts. Each task is categorized
by difficulty (easy/medium/hard), with experts constructing the dataset
entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)
evaluators trained for nuanced assessment of generative outputs. This approach
provides scalable, interpretable evaluation and annotation tools for model
development, effectively replacing costly and less precise human judgments.

</details>


### [130] [Interpretable phenotyping of Heart Failure patients with Dutch discharge letters](https://arxiv.org/abs/2505.24619)
*Vittorio Torri,Machteld J. Boonstra,Marielle C. van de Veerdonk,Deborah N. Kalkman,Alicia Uijl,Francesca Ieva,Ameen Abu-Hanna,Folkert W. Asselbergs,Iacer Calixto*

Key words: 心力衰竭, 左心室射血分数, 表型分类, 可解释性, 出院记录

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究通过结构化和非结构化数据评估了基于左心室射血分数（LVEF）的心力衰竭（HF）患者表型分析模型，发现出院记录是最有价值的数据源，而Aug-Linear模型在性能与可解释性上均表现优异。

Motivation: HF患者的多样化表型影响治疗和预后，因此需要高效且可解释的分类模型，以支持临床决策。

Method: 研究使用了2015至2023年33,105次HF住院数据，结合临床测量和出院记录，通过Transformer和Aug-Linear模型进行训练与验证，并通过临床医生标注评估模型解释性。

Result: BERT和Aug-Linear模型表现最佳（AUC分别为0.84和0.81），Aug-Linear的解释性更接近临床医生的判断。

Conclusion: 出院记录是HF表型分类的最佳数据源，Aug-Linear模型在性能和可解释性上均优于黑盒模型，适合临床透明决策。

Abstract: Objective: Heart failure (HF) patients present with diverse phenotypes
affecting treatment and prognosis. This study evaluates models for phenotyping
HF patients based on left ventricular ejection fraction (LVEF) classes, using
structured and unstructured data, assessing performance and interpretability.
  Materials and Methods: The study analyzes all HF hospitalizations at both
Amsterdam UMC hospitals (AMC and VUmc) from 2015 to 2023 (33,105
hospitalizations, 16,334 patients). Data from AMC were used for model training,
and from VUmc for external validation. The dataset was unlabelled and included
tabular clinical measurements and discharge letters. Silver labels for LVEF
classes were generated by combining diagnosis codes, echocardiography results,
and textual mentions. Gold labels were manually annotated for 300 patients for
testing. Multiple Transformer-based (black-box) and Aug-Linear (white-box)
models were trained and compared with baselines on structured and unstructured
data. To evaluate interpretability, two clinicians annotated 20 discharge
letters by highlighting information they considered relevant for LVEF
classification. These were compared to SHAP and LIME explanations from
black-box models and the inherent explanations of Aug-Linear models.
  Results: BERT-based and Aug-Linear models, using discharge letters alone,
achieved the highest classification results (AUC=0.84 for BERT, 0.81 for
Aug-Linear on external validation), outperforming baselines. Aug-Linear
explanations aligned more closely with clinicians' explanations than post-hoc
explanations on black-box models.
  Conclusions: Discharge letters emerged as the most informative source for
phenotyping HF patients. Aug-Linear models matched black-box performance while
providing clinician-aligned interpretability, supporting their use in
transparent clinical decision-making.

</details>


### [131] [Benchmarking Large Language Models for Cryptanalysis and Mismatched-Generalization](https://arxiv.org/abs/2505.24621)
*Utsav Maskey,Chencheng Zhu,Usman Naseem*

Key words: 大型语言模型,密码分析,加密文本,双重用途,AI安全

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了大型语言模型（LLMs）在密码分析领域的潜在应用，提出了一种新的基准数据集，用于评估LLM在多种加密算法下的解密能力和语义理解。研究发现揭示了LLM在安全领域的双重用途及其潜在风险。

Motivation: 由于密码分析在数据安全和加密中的重要性，目前尚未有全面的LLM评估。研究旨在填补这一空白，评估LLM在加密文本中的表现。

Method: 引入了包含多样化明文及其加密版本的基准数据集，采用零样本和小样本设置，评估多个LLM在不同加密方案下的性能。

Result: 研究揭示了LLM在侧信道通信中的优势和局限性，并提出了其易受越狱攻击的担忧。

Conclusion: 研究强调了LLM在安全领域的双重用途，为AI安全与安全的讨论提供了新的视角。

Abstract: Recent advancements in Large Language Models (LLMs) have transformed natural
language understanding and generation, leading to extensive benchmarking across
diverse tasks. However, cryptanalysis a critical area for data security and
encryption has not yet been thoroughly explored in LLM evaluations. To address
this gap, we evaluate cryptanalytic potential of state of the art LLMs on
encrypted texts generated using a range of cryptographic algorithms. We
introduce a novel benchmark dataset comprising diverse plain texts spanning
various domains, lengths, writing styles, and topics paired with their
encrypted versions. Using zero-shot and few shot settings, we assess multiple
LLMs for decryption accuracy and semantic comprehension across different
encryption schemes. Our findings reveal key insights into the strengths and
limitations of LLMs in side-channel communication while raising concerns about
their susceptibility to jailbreaking attacks. This research highlights the
dual-use nature of LLMs in security contexts and contributes to the ongoing
discussion on AI safety and security.

</details>


### [132] [The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2505.24630)
*Junyi Li,Hwee Tou Ng*

Key words: LLM, 强化学习, 幻觉, FSPO, 事实验证

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出FSPO算法，结合事实验证优化LLM的推理能力，有效减少幻觉问题。

Motivation: 发现RL优化会加剧幻觉问题，需解决推理过程中的事实准确性。

Method: 提出FSPO算法，通过逐步事实验证动态调整优势值，提升推理正确性。

Result: 在数学推理和幻觉基准测试中，FSPO显著降低幻觉并提高准确性。

Conclusion: FSPO通过事实验证优化RL训练，提升了LLM的可靠性和性能。

Abstract: Large language models (LLMs) have significantly advanced in reasoning tasks
through reinforcement learning (RL) optimization, achieving impressive
capabilities across various challenging benchmarks. However, our empirical
analysis reveals a critical drawback: reasoning-oriented RL fine-tuning
significantly increases the prevalence of hallucinations. We theoretically
analyze the RL training dynamics, identifying high-variance gradient,
entropy-induced randomness, and susceptibility to spurious local optima as key
factors leading to hallucinations. To address this drawback, we propose
Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL
fine-tuning algorithm incorporating explicit factuality verification at each
reasoning step. FSPO leverages automated verification against given evidence to
dynamically adjust token-level advantage values, incentivizing factual
correctness throughout the reasoning process. Experiments across mathematical
reasoning and hallucination benchmarks using Qwen2.5 and Llama models
demonstrate that FSPO effectively reduces hallucinations while enhancing
reasoning accuracy, substantially improving both reliability and performance.

</details>


### [133] [Disentangling Language and Culture for Evaluating Multilingual Large Language Models](https://arxiv.org/abs/2505.24635)
*Jiahao Ying,Wei Tang,Yiran Zhao,Yixin Cao,Yu Rong,Wenxuan Zhang*

Key words: 多语言评估, 文化背景, 语言媒介, 双评估框架, 神经激活

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了双评估框架，从语言媒介和文化背景两个维度评估LLM的多语言能力，发现模型在文化语言协同效应下表现更佳，挑战了LLM在英语数据训练下跨语言一致性的传统观点。

Motivation: 现有LLM主要基于英语数据训练，其多语言能力评估缺乏全面性，需结合文化和语言维度进行更细致分析。

Method: 通过双评估框架分解语言媒介和文化背景，采用广泛模型评估及可解释性探针分析神经激活模式。

Result: 发现文化语言协同效应，神经激活比例可作为多语言性能评估指标，且模型在文化匹配的语言环境下表现更好。

Conclusion: LLM的多语言能力需结合文化和语言维度评估，英语训练模型跨语言表现不一致，需改进评估方法。

Abstract: This paper introduces a Dual Evaluation Framework to comprehensively assess
the multilingual capabilities of LLMs. By decomposing the evaluation along the
dimensions of linguistic medium and cultural context, this framework enables a
nuanced analysis of LLMs' ability to process questions within both native and
cross-cultural contexts cross-lingually. Extensive evaluations are conducted on
a wide range of models, revealing a notable "CulturalLinguistic Synergy"
phenomenon, where models exhibit better performance when questions are
culturally aligned with the language. This phenomenon is further explored
through interpretability probing, which shows that a higher proportion of
specific neurons are activated in a language's cultural context. This
activation proportion could serve as a potential indicator for evaluating
multilingual performance during model training. Our findings challenge the
prevailing notion that LLMs, primarily trained on English data, perform
uniformly across languages and highlight the necessity of culturally and
linguistically model evaluations. Our code can be found at
https://yingjiahao14. github.io/Dual-Evaluation/.

</details>


### [134] [Efficient Text Encoders for Labor Market Analysis](https://arxiv.org/abs/2505.24640)
*Jens-Joris Decorte,Jeroen Van Hautte,Chris Develder,Thomas Demeester*

Key words: 技能提取, 对比学习, 极多标签分类, 劳动力市场分析, 轻量级模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于对比学习的轻量级技能分类方法ConTeXT-match，显著提升了技能提取的效率和性能，并引入了新基准Skill-XL和改进的职位标准化模型JobBERT V2。

Motivation: 劳动力市场分析需要从招聘广告中提取非结构化的技能信息，现有方法依赖计算成本高的大语言模型，效率低下。

Method: 提出ConTeXT-match方法，结合对比学习和词级注意力机制，用于极多标签分类任务；同时引入Skill-XL基准和改进的JobBERT V2模型。

Result: 实验表明，该方法高效、准确且可扩展，适用于大规模实时劳动力市场分析。

Conclusion: ConTeXT-match和JobBERT V2为劳动力市场分析提供了高效、轻量级的解决方案。

Abstract: Labor market analysis relies on extracting insights from job advertisements,
which provide valuable yet unstructured information on job titles and
corresponding skill requirements. While state-of-the-art methods for skill
extraction achieve strong performance, they depend on large language models
(LLMs), which are computationally expensive and slow. In this paper, we propose
\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level
attention that is well-suited for the extreme multi-label classification task
of skill classification. \textbf{ConTeXT-match} significantly improves skill
extraction efficiency and performance, achieving state-of-the-art results with
a lightweight bi-encoder model. To support robust evaluation, we introduce
\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill
annotations that explicitly address the redundancy in the large label space.
Finally, we present \textbf{JobBERT V2}, an improved job title normalization
model that leverages extracted skills to produce high-quality job title
representations. Experiments demonstrate that our models are efficient,
accurate, and scalable, making them ideal for large-scale, real-time labor
market analysis.

</details>


### [135] [Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based Pairwise Ranking with Batching and Caching](https://arxiv.org/abs/2505.24643)
*Juan Wisznia,Cecilia Bolaños,Juan Tollo,Giovanni Marraffini,Agustín Gianolini,Noe Hsueh,Luciano Del Corro*

Key words: 排序算法,大模型推理,成对排名提示,成本优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了针对大模型推理优化的排序算法新框架，取代传统的比较次数衡量标准。

Motivation: 传统排序算法效率基于比较次数，但大模型推理的高昂成本改变了这一前提，需重新优化算法。

Method: 提出新框架，强调批量处理和缓存技术以减少推理成本。

Result: 发现传统最优算法在大模型推理主导成本时效率下降。

Conclusion: 新框架和优化策略能有效降低大模型推理成本，提升排序效率。

Abstract: We introduce a novel framework for analyzing sorting algorithms in pairwise
ranking prompting (PRP), re-centering the cost model around LLM inferences
rather than traditional pairwise comparisons. While classical metrics based on
comparison counts have traditionally been used to gauge efficiency, our
analysis reveals that expensive LLM inferences overturn these predictions;
accordingly, our framework encourages strategies such as batching and caching
to mitigate inference costs. We show that algorithms optimal in the classical
setting can lose efficiency when LLM inferences dominate the cost under certain
optimizations.

</details>


### [136] [PRISM: A Framework for Producing Interpretable Political Bias Embeddings with Political-Aware Cross-Encoder](https://arxiv.org/abs/2505.24646)
*Yiqun Sun,Qiang Huang,Anthony K. H. Tung,Jun Yu*

Key words: 语义文本嵌入, 政治偏见, 可解释性, PRISM, 意识形态分析

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: PRISM框架首次专注于生成可解释的政治偏见嵌入，通过两阶段方法提取偏见指标并增强嵌入的可解释性，实验证明其在政治偏见分类中优于现有模型。

Motivation: 现有文本嵌入模型在捕捉政治偏见等意识形态细微差别方面有局限，PRISM旨在填补这一空白。

Method: PRISM分为两个阶段：争议性话题偏见指标挖掘和交叉编码器政治偏见嵌入，系统性提取偏见指标并生成结构化偏见分数。

Result: PRISM在政治偏见分类任务中优于现有文本嵌入模型，同时提供高度可解释的表示。

Conclusion: PRISM不仅提升了政治偏见的分类性能，还增强了嵌入的可解释性，适用于多样化的检索和意识形态分析。

Abstract: Semantic Text Embedding is a fundamental NLP task that encodes textual
content into vector representations, where proximity in the embedding space
reflects semantic similarity. While existing embedding models excel at
capturing general meaning, they often overlook ideological nuances, limiting
their effectiveness in tasks that require an understanding of political bias.
To address this gap, we introduce PRISM, the first framework designed to
Produce inteRpretable polItical biaS eMbeddings. PRISM operates in two key
stages: (1) Controversial Topic Bias Indicator Mining, which systematically
extracts fine-grained political topics and their corresponding bias indicators
from weakly labeled news data, and (2) Cross-Encoder Political Bias Embedding,
which assigns structured bias scores to news articles based on their alignment
with these indicators. This approach ensures that embeddings are explicitly
tied to bias-revealing dimensions, enhancing both interpretability and
predictive power. Through extensive experiments on two large-scale datasets, we
demonstrate that PRISM outperforms state-of-the-art text embedding models in
political bias classification while offering highly interpretable
representations that facilitate diversified retrieval and ideological analysis.
The source code is available at https://github.com/dukesun99/ACL-PRISM.

</details>


### [137] [MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised Domain Adaptation in ASR](https://arxiv.org/abs/2505.24656)
*Dimitrios Damianos,Georgios Paraskevopoulos,Alexandros Potamianos*

Key words: Meta PL, ASR, domain adaptation, self-supervised learning, semi-supervised learning

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一个名为MSDA的多阶段领域自适应框架，结合自监督和半监督学习，提高了ASR模型的鲁棒性和泛化能力，尤其在低资源语言和弱监督场景中表现优异。

Motivation: 提升ASR模型的适应性和性能，特别是在缺乏标注数据或数据噪声较大的情况下。

Method: 采用多阶段领域自适应（MSDA）框架，结合自监督学习和半监督技术，进行两阶段自适应。

Result: 在ASR任务中达到最先进水平，显著优于现有方法，尤其在低资源语言和弱监督场景中表现突出。

Conclusion: MSDA框架在ASR领域自适应中具有显著优势，验证了自监督与自训练级联方法的必要性。

Abstract: In this work, we investigate the Meta PL unsupervised domain adaptation
framework for Automatic Speech Recognition (ASR). We introduce a Multi-Stage
Domain Adaptation pipeline (MSDA), a sample-efficient, two-stage adaptation
approach that integrates self-supervised learning with semi-supervised
techniques. MSDA is designed to enhance the robustness and generalization of
ASR models, making them more adaptable to diverse conditions. It is
particularly effective for low-resource languages like Greek and in weakly
supervised scenarios where labeled data is scarce or noisy. Through extensive
experiments, we demonstrate that Meta PL can be applied effectively to ASR
tasks, achieving state-of-the-art results, significantly outperforming
state-of-the-art methods, and providing more robust solutions for unsupervised
domain adaptation in ASR. Our ablations highlight the necessity of utilizing a
cascading approach when combining self-supervision with self-training.

</details>


### [138] [Multiple LLM Agents Debate for Equitable Cultural Alignment](https://arxiv.org/abs/2505.24671)
*Dayeon Ki,Rachel Rudinger,Tianyi Zhou,Marine Carpuat*

Key words: 大语言模型, 文化适应性, 多智能体辩论, 社会礼仪规范

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出一种多智能体辩论框架，通过两个LLM代理针对文化场景展开辩论并协作决策，提升文化适应性。实验表明该方法在多个LLM上显著提高准确性和文化公平性。

Motivation: 现有方法多为单一LLM单轮决策，无法充分适应多样文化场景。

Method: 提出两种多智能体辩论变体：纯辩论模式及动态选择自省与辩论的混合模式。

Result: 在7个开放权重LLM上验证，辩论显著提升准确性和文化公平性，小模型（7-9B）可媲美大模型（27B）效果。

Conclusion: 多智能体辩论能有效提升LLM文化适应性，尤其对小模型效果显著。

Abstract: Large Language Models (LLMs) need to adapt their predictions to diverse
cultural contexts to benefit diverse communities across the world. While
previous efforts have focused on single-LLM, single-turn approaches, we propose
to exploit the complementary strengths of multiple LLMs to promote cultural
adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based
agents debate over a cultural scenario and collaboratively reach a final
decision. We propose two variants: one where either LLM agents exclusively
debate and another where they dynamically choose between self-reflection and
debate during their turns. We evaluate these approaches on 7 open-weight LLMs
(and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette
norms in 75 countries. Experiments show that debate improves both overall
accuracy and cultural group parity over single-LLM baselines. Notably,
multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies
comparable to that of a much larger model (27B parameters).

</details>


### [139] [TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional Diversified Red-Teaming Data Synthesis](https://arxiv.org/abs/2505.24672)
*Xiaorui Wu,Xiaofeng Mao,Fei Li,Xin Zhang,Xuanhong Li,Chong Teng,Donghong Ji,Zhuang Li*

Key words: Large Language Models, safety alignment, TRIDENT, harmful content, jailbreak tactics

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出TRIDENT框架，通过分析指令集的词汇多样性、恶意意图和越狱策略三个维度来系统衡量安全对齐数据集的风险覆盖范围，并生成两个数据集TRIDENT-Core和TRIDENT-Edge。微调实验显示TRIDENT-Edge显著降低了模型的危害评分和攻击成功率。

Motivation: 现有的大语言模型（LLMs）在生成有害内容或恶意利用方面存在风险，而现有安全对齐数据集在风险覆盖上不足，主要关注词汇多样性。为了弥补这一缺陷，论文提出系统性衡量风险覆盖的框架。

Method: 提出TRIDENT框架，通过基于角色的零样本LLM生成，创建涵盖词汇多样性、恶意意图和越狱策略三个维度的指令，并生成TRIDENT-Core（26,311例）与TRIDENT-Edge（18,773例）数据集。

Result: 在TRIDENT-Edge上微调的Llama 3.1-8B模型相较WildBreak微调的基线模型，危害评分平均降低14.29%，攻击成功率降低20%。

Conclusion: TRIDENT框架能有效提升安全对齐数据集的全面性，显著减少LLMs生成有害内容的风险。

Abstract: Large Language Models (LLMs) excel in various natural language processing
tasks but remain vulnerable to generating harmful content or being exploited
for malicious purposes. Although safety alignment datasets have been introduced
to mitigate such risks through supervised fine-tuning (SFT), these datasets
often lack comprehensive risk coverage. Most existing datasets focus primarily
on lexical diversity while neglecting other critical dimensions. To address
this limitation, we propose a novel analysis framework to systematically
measure the risk coverage of alignment datasets across three essential
dimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We
further introduce TRIDENT, an automated pipeline that leverages persona-based,
zero-shot LLM generation to produce diverse and comprehensive instructions
spanning these dimensions. Each harmful instruction is paired with an ethically
aligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311
examples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on
TRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29%
reduction in Harm Score, and a 20% decrease in Attack Success Rate compared to
the best-performing baseline model fine-tuned on the WildBreak dataset.

</details>


### [140] [A Simple Linear Patch Revives Layer-Pruned Large Language Models](https://arxiv.org/abs/2505.24680)
*Xinrui Chen,Haoli Bai,Tao Yuan,Ruikang Liu,Kang Zhao,Xianzhi Yu,Lu Hou,Tian Guan,Yonghong He,Chun Yuan*

Key words: Layer pruning, LLMs, LinearPatch, Hadamard transformation, activation alignment

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Layer pruning in LLMs often leads to performance drops. LinearPatch, a plug-and-play method, uses Hadamard transformation and channel-wise scaling to align activation magnitudes, maintaining up to 94.15% performance and surpassing existing methods by 4%.

Motivation: Existing layer pruning methods for LLMs cause significant performance degradation, motivating the need for a better solution like LinearPatch.

Method: LinearPatch employs Hadamard transformation to handle outliers and channel-wise scaling to align activation magnitudes, fused into a single matrix for minimal overhead.

Result: LinearPatch retains 94.15% performance when pruning 5 layers of LLaMA-3-8B, outperforming SOTA by 4%, and can be further optimized to 95.16% with offline distillation.

Conclusion: LinearPatch effectively mitigates performance drops in layer-pruned LLMs, offering a simple yet powerful solution with minimal overhead.

Abstract: Layer pruning has become a popular technique for compressing large language
models (LLMs) due to its simplicity. However, existing layer pruning methods
often suffer from significant performance drops. We identify that this
degradation stems from the mismatch of activation magnitudes across layers and
tokens at the pruning interface. To address this, we propose LinearPatch, a
simple plug-and-play technique to revive the layer-pruned LLMs. The proposed
method adopts Hadamard transformation to suppress massive outliers in
particular tokens, and channel-wise scaling to align the activation magnitudes.
These operations can be fused into a single matrix, which functions as a patch
to bridge the pruning interface with negligible inference overhead. LinearPatch
retains up to 94.15% performance of the original model when pruning 5 layers of
LLaMA-3-8B on the question answering benchmark, surpassing existing
state-of-the-art methods by 4%. In addition, the patch matrix can be further
optimized with memory efficient offline knowledge distillation. With only 5K
samples, the retained performance of LinearPatch can be further boosted to
95.16% within 30 minutes on a single computing card.

</details>


### [141] [Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation](https://arxiv.org/abs/2505.24683)
*Dayeon Ki,Kevin Duh,Marine Carpuat*

Key words: AI反馈机制、机器翻译、显性反馈、隐性反馈、用户决策

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究比较了四种AI翻译质量反馈机制，发现隐性反馈（特别是问答表）在准确性、用户信任和减少认知负担方面优于显性反馈。

Motivation: 随着AI在日常生活中的普及，用户缺乏评估AI预测质量的能力，需要有效的反馈机制来帮助负责任地使用AI。

Method: 在机器翻译场景中，比较四种反馈类型：显性反馈（错误高亮和LLM解释）和隐性反馈（回译和问答表），评估对用户决策的影响。

Result: 除错误高亮外，所有反馈类型显著提升决策准确性和适当依赖。隐性反馈（尤其是问答表）在准确性、用户感知和减少认知负担上表现最佳。

Conclusion: 隐性反馈（如问答表）在提升用户决策质量和体验方面优于显性反馈，适合用于AI翻译场景。

Abstract: As people increasingly use AI systems in work and daily life, feedback
mechanisms that help them use AI responsibly are urgently needed, particularly
in settings where users are not equipped to assess the quality of AI
predictions. We study a realistic Machine Translation (MT) scenario where
monolingual users decide whether to share an MT output, first without and then
with quality feedback. We compare four types of quality feedback: explicit
feedback that directly give users an assessment of translation quality using 1)
error highlights and 2) LLM explanations, and implicit feedback that helps
users compare MT inputs and outputs through 3) backtranslation and 4)
question-answer (QA) tables. We find that all feedback types, except error
highlights, significantly improve both decision accuracy and appropriate
reliance. Notably, implicit feedback, especially QA tables, yields
significantly greater gains than explicit feedback in terms of decision
accuracy, appropriate reliance, and user perceptions, receiving the highest
ratings for helpfulness and trust, and the lowest for mental burden.

</details>


### [142] [Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration](https://arxiv.org/abs/2505.24688)
*Qinglin Zhu,Runcong Zhao,Hanqi Yan,Yulan He,Yudong Chen,Lin Gui*

Key words: 大语言模型, 复杂推理, 嵌入扰动, 贝叶斯优化, 模型无关

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Soft Reasoning 框架通过嵌入扰动和贝叶斯优化提升LLM的复杂推理能力，实验证明其高效且模型无关。

Motivation: LLM 在复杂推理中因搜索效率低和多样性不足表现不佳，需改进。

Method: 结合嵌入扰动（可控探索）和贝叶斯优化（验证器引导目标），优化首个令牌嵌入。

Result: 实验显示推理准确性和连贯性提升，计算成本低，模型无关。

Conclusion: Soft Reasoning 是高效、可扩展的解决方案，避免依赖启发式搜索。

Abstract: Large Language Models (LLMs) struggle with complex reasoning due to limited
diversity and inefficient search. We propose Soft Reasoning, an embedding-based
search framework that optimises the embedding of the first token to guide
generation. It combines (1) embedding perturbation for controlled exploration
and (2) Bayesian optimisation to refine embeddings via a verifier-guided
objective, balancing exploration and exploitation. This approach improves
reasoning accuracy and coherence while avoiding reliance on heuristic search.
Experiments demonstrate superior correctness with minimal computation, making
it a scalable, model-agnostic solution.

</details>


### [143] [BPE Stays on SCRIPT: Structured Encoding for Robust Multilingual Pretokenization](https://arxiv.org/abs/2505.24689)
*Sander Land,Catherine Arnett*

Key words: BPE, tokenizer, multilingual, Unicode, pretokenization

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SCRIPT-BPE是一种新型编码方案，通过基于Unicode脚本和类别属性的初始令牌绕过UTF-8字节转换，提供了一种简单且稳健的预标记化策略，解决了BPE在多语言环境中的挑战。

Motivation: BPE标记器在多语言环境中面临非西方文字惩罚和部分UTF-8序列令牌的问题，现有预标记化方法依赖复杂正则表达式，易引入脆弱性和边缘情况。

Method: 提出SCRIPT方案，基于Unicode脚本和类别属性初始化令牌，结合约束BPE合并策略保护字符完整性。

Result: SCRIPT-BPE在保持竞争力的压缩率的同时，消除了非拉丁文字语言的编码惩罚。

Conclusion: SCRIPT-BPE为多语言环境中的标记化提供了更简单、更稳健的解决方案。

Abstract: Byte Pair Encoding (BPE) tokenizers, widely used in Large Language Models,
face challenges in multilingual settings, including penalization of non-Western
scripts and the creation of tokens with partial UTF-8 sequences.
Pretokenization, often reliant on complex regular expressions, can also
introduce fragility and unexpected edge cases. We propose SCRIPT (Script
Category Representation in PreTokenization), a novel encoding scheme that
bypasses UTF-8 byte conversion by using initial tokens based on Unicode script
and category properties. This approach enables a simple, rule-based
pretokenization strategy that respects script boundaries, offering a robust
alternative to pretokenization strategies based on regular expressions. We also
introduce and validate a constrained BPE merging strategy that enforces
character integrity, applicable to both SCRIPT-BPE and byte-based BPE. Our
experiments demonstrate that SCRIPT-BPE achieves competitive compression while
eliminating encoding-based penalties for non-Latin-script languages.

</details>


### [144] [Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing Cross-Lingual Transfer in Low-Resource Scenarios](https://arxiv.org/abs/2505.24691)
*Gerard I. Gállego,Oriol Pareras,Martí Cortada Garcia,Lucas Takanori,Javier Hernando*

Key words: 语音到文本翻译，音素表示，链式思考，低资源翻译，跨语言迁移

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种将音素表示集成到链式思考框架的语音到文本翻译方法，以提升在低资源和零资源语言环境下的翻译表现。

Motivation: 当前语音到文本翻译在低资源或零资源语言环境下表现不佳，特别是在缺乏标记语音数据的语言中。为此，研究希望通过引入音素识别作为中间步骤，增强跨语言迁移能力。

Method: 通过扩展多语言LLM处理语音和音素，采用课程学习策略逐步引入复杂任务来训练系统。

Result: 实验表明，音素增强的链式思考框架在低资源条件下提升了翻译质量，并实现了零资源翻译，但对高资源语言性能有轻微影响。

Conclusion: 研究表明音素链式思考框架是提高语音到文本翻译多样语言可用性的有效步骤。

Abstract: We propose a Speech-to-Text Translation (S2TT) approach that integrates
phoneme representations into a Chain-of-Thought (CoT) framework to improve
translation in low-resource and zero-resource settings. By introducing phoneme
recognition as an intermediate step, we enhance cross-lingual transfer,
enabling translation even for languages with no labeled speech data. Our system
builds on a multilingual LLM, which we extend to process speech and phonemes.
Training follows a curriculum learning strategy that progressively introduces
more complex tasks. Experiments on multilingual S2TT benchmarks show that
phoneme-augmented CoT improves translation quality in low-resource conditions
and enables zero-resource translation, while slightly impacting high-resource
performance. Despite this trade-off, our findings demonstrate that
phoneme-based CoT is a promising step toward making S2TT more accessible across
diverse languages.

</details>


### [145] [Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison](https://arxiv.org/abs/2505.24701)
*Tejul Pandit,Meet Raval,Dhvani Upadhyay*

Key words: ABSA, LLM, synthetic data, sentiment analysis, GPT-4o

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种利用GPT-4o生成ABSA合成数据的方法，解决了数据稀缺问题。通过评估三种LLM（Gemini 1.5 Pro、Claude 3.5 Sonnet和DeepSeek-R1）在主题和情感分类任务中的表现，验证了该合成数据的有效性。

Motivation: ABSA需要多样化且有代表性的标注数据，但现实中这类数据稀缺，本文试图通过LLM生成合成数据填补这一空白。

Method: 使用GPT-4o生成跨多个领域的合成ABSA数据，并评估其在Gemini 1.5 Pro、Claude 3.5 Sonnet和DeepSeek-R1上的性能。

Result: 生成的数据被证明有效。DeepSeek-R1精度高，Gemini 1.5 Pro和Claude 3.5 Sonnet召回率强，而Gemini 1.5 Pro推理速度更快。

Conclusion: 基于LLM的合成数据生成是灵活且有价值的解决方案，可缓解对真实标注数据的依赖。

Abstract: Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions
but often suffers from the scarcity of diverse, labeled datasets that reflect
real-world conversational nuances. This paper presents an approach for
generating synthetic ABSA data using Large Language Models (LLMs) to address
this gap. We detail the generation process aimed at producing data with
consistent topic and sentiment distributions across multiple domains using
GPT-4o. The quality and utility of the generated data were evaluated by
assessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro,
Claude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification
tasks. Our results demonstrate the effectiveness of the synthetic data,
revealing distinct performance trade-offs among the models: DeepSeekR1 showed
higher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall,
and Gemini 1.5 Pro offered significantly faster inference. We conclude that
LLM-based synthetic data generation is a viable and flexible method for
creating valuable ABSA resources, facilitating research and model evaluation
without reliance on limited or inaccessible real-world labeled data.

</details>


### [146] [HESEIA: A community-based dataset for evaluating social biases in large language models, co-designed in real school settings in Latin America](https://arxiv.org/abs/2505.24712)
*Guido Ivetta,Marcos J. Gomez,Sofía Martinelli,Pietro Palombini,M. Emilia Echeveste,Nair Carolina Mazzeo,Beatriz Busaniche,Luciana Benotti*

Key words: 社会偏见、大型语言模型、交叉性偏见、参与式方法、教育社区

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: HESEIA是一个由拉丁美洲教师和学生共同创建的包含46,499个句子的数据集，用于评估大型语言模型的社会偏见，尤其关注交叉性偏见和本地化情景。

Motivation: 现有的大多数评估大型语言模型社会偏见的资源缺乏受影响社区的参与，很少采用参与式方法。HESEIA旨在填补这一空白，通过教育社区的真实经验和专业知识来反映本地化情景。

Method: 在专业发展课程中，370名高中教师和5,370名学生从189所拉丁美洲学校共同创建了HESEIA数据集，使用最小对句表达与学科和社区相关的刻板印象。

Result: HESEIA在人口统计轴和学科领域表现出多样性，且包含更多当前大型语言模型尚未识别的刻板印象。

Conclusion: HESEIA为基于教育社区的偏见评估提供了支持，展示了参与式方法在创建偏见评估资源中的重要性。

Abstract: Most resources for evaluating social biases in Large Language Models are
developed without co-design from the communities affected by these biases, and
rarely involve participatory approaches. We introduce HESEIA, a dataset of
46,499 sentences created in a professional development course. The course
involved 370 high-school teachers and 5,370 students from 189 Latin-American
schools. Unlike existing benchmarks, HESEIA captures intersectional biases
across multiple demographic axes and school subjects. It reflects local
contexts through the lived experience and pedagogical expertise of educators.
Teachers used minimal pairs to create sentences that express stereotypes
relevant to their school subjects and communities. We show the dataset
diversity in term of demographic axes represented and also in terms of the
knowledge areas included. We demonstrate that the dataset contains more
stereotypes unrecognized by current LLMs than previous datasets. HESEIA is
available to support bias assessments grounded in educational communities.

</details>


### [147] [Voice Conversion Improves Cross-Domain Robustness for Spoken Arabic Dialect Identification](https://arxiv.org/abs/2505.24713)
*Badr M. Abdullah,Matthew Baas,Bernd Möbius,Dietrich Klakow*

Key words: Arabic dialect identification, voice conversion, cross-domain robustness, speaker bias

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于语音转换的阿拉伯语方言识别方法，显著提升了跨域场景下的性能，准确性最高提升34.1%，并分析了语音转换对减轻数据集中的说话者偏见的作用。

Motivation: 当前阿拉伯语方言识别系统在面对域外数据时泛化能力不足，限制了其在多样化语音技术中的应用。

Method: 采用基于语音转换的方法训练方言识别模型，以提高跨域场景下的性能。

Result: 在新收集的包含四个领域的真实测试集上，模型准确性最高提升34.1%，并减轻了数据集中的说话者偏见。

Conclusion: 该方法在阿拉伯语方言识别中实现了最先进的性能，并提升了跨域鲁棒性，为包容性语音技术的发展提供了支持。

Abstract: Arabic dialect identification (ADI) systems are essential for large-scale
data collection pipelines that enable the development of inclusive speech
technologies for Arabic language varieties. However, the reliability of current
ADI systems is limited by poor generalization to out-of-domain speech. In this
paper, we present an effective approach based on voice conversion for training
ADI models that achieves state-of-the-art performance and significantly
improves robustness in cross-domain scenarios. Evaluated on a newly collected
real-world test set spanning four different domains, our approach yields
consistent improvements of up to +34.1% in accuracy across domains.
Furthermore, we present an analysis of our approach and demonstrate that voice
conversion helps mitigate the speaker bias in the ADI dataset. We release our
robust ADI model and cross-domain evaluation dataset to support the development
of inclusive speech technologies for Arabic.

</details>


### [148] [FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation](https://arxiv.org/abs/2505.24714)
*Junyu Luo,Zhizhuo Kou,Liming Yang,Xiao Luo,Jinsheng Huang,Zhiping Xiao,Jingshu Peng,Chengzhong Liu,Jiaming Ji,Xuanzhe Liu,Sirui Han,Ming Zhang,Yike Guo*

Key words: 多模态大型语言模型，金融，评估数据集，FinMME，FinScore

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文介绍了FinMME，一个用于金融领域的多模态大型语言模型（MLLMs）的高质量评估数据集，涵盖多种金融领域和资产类别，并开发了FinScore评估系统。实验表明，即使是先进模型如GPT-4o在FinMME上表现也不理想，证明了其挑战性。

Motivation: 当前金融领域缺乏有效的多模态评估数据集，阻碍了MLLMs在该领域的进展。为了解决这一问题，作者提出了FinMME数据集和FinScore评估系统。

Method: 作者构建了FinMME数据集，包含超过11,000个金融研究样本，涵盖18个金融领域和6种资产类别，包含10种主要图表类型和21种子类型。数据质量通过20名标注人员和验证机制保障。同时开发了FinScore评估系统，结合幻觉惩罚和多维能力评估。

Result: 实验结果显示，即使是GPT-4o等先进模型在FinMME上的表现也不理想。数据集在不同提示下的预测变异低于1%，具有高鲁棒性和可靠性。

Conclusion: FinMME是一个高质量、具有挑战性的金融领域多模态评估数据集，FinScore提供了无偏的评估方法，推动了MLLMs在金融领域的发展。

Abstract: Multimodal Large Language Models (MLLMs) have experienced rapid development
in recent years. However, in the financial domain, there is a notable lack of
effective and specialized multimodal evaluation datasets. To advance the
development of MLLMs in the finance domain, we introduce FinMME, encompassing
more than 11,000 high-quality financial research samples across 18 financial
domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We
ensure data quality through 20 annotators and carefully designed validation
mechanisms. Additionally, we develop FinScore, an evaluation system
incorporating hallucination penalties and multi-dimensional capability
assessment to provide an unbiased evaluation. Extensive experimental results
demonstrate that even state-of-the-art models like GPT-4o exhibit
unsatisfactory performance on FinMME, highlighting its challenging nature. The
benchmark exhibits high robustness with prediction variations under different
prompts remaining below 1%, demonstrating superior reliability compared to
existing datasets. Our dataset and evaluation protocol are available at
https://huggingface.co/datasets/luojunyu/FinMME and
https://github.com/luo-junyu/FinMME.

</details>


### [149] [Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.24726)
*Shelly Bensal,Umar Jamil,Christopher Bryant,Melisa Russak,Kiran Kamble,Dmytro Mozolevskyi,Muayad Ali,Waseem AlShikh*

Key words: 大语言模型, 自我反思, 强化学习, 二元反馈, 任务性能提升

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种通过自我反思和强化学习提升大语言模型性能的方法，尤其在二元反馈和合成数据不可行时，显著提高了模型解决复杂任务的能力。

Motivation: 研究动机在于探索如何利用自我反思和有限的反馈机制（如二元反馈）来增强语言模型在复杂任务上的表现，而无需依赖大量外部数据或复杂反馈。

Method: 方法采用两阶段框架：1) 任务失败后生成自我反思的评论；2) 在自我反思的指导下重新尝试任务，成功则奖励反思阶段的生成内容。

Result: 实验结果显示性能显著提升，数学方程写作和函数调用任务分别提高了34.7%和18.1%，且较小微调模型（1.5B-7B参数）优于同家族10倍大的模型。

Conclusion: 该范式为开发更可靠且能自我改进的语言模型提供了新途径，尤其在外部反馈有限的挑战性任务中表现突出。

Abstract: We explore a method for improving the performance of large language models
through self-reflection and reinforcement learning. By incentivizing the model
to generate better self-reflections when it answers incorrectly, we demonstrate
that a model's ability to solve complex, verifiable tasks can be enhanced even
when generating synthetic data is infeasible and only binary feedback is
available. Our framework operates in two stages: first, upon failing a given
task, the model generates a self-reflective commentary analyzing its previous
attempt; second, the model is given another attempt at the task with the
self-reflection in context. If the subsequent attempt succeeds, the tokens
generated during the self-reflection phase are rewarded. Our experimental
results show substantial performance gains across a variety of model
architectures, as high as 34.7% improvement at math equation writing and 18.1%
improvement at function calling. Notably, smaller fine-tuned models (1.5
billion to 7 billion parameters) outperform models in the same family that are
10 times larger. Our novel paradigm is thus an exciting pathway to more useful
and reliable language models that can self-improve on challenging tasks with
limited external feedback.

</details>


### [150] [Circuit Stability Characterizes Language Model Generalization](https://arxiv.org/abs/2505.24731)
*Alan Sun*

Key words: 语言模型评估, 电路稳定性, 泛化能力, 可解释性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种通过电路稳定性评估语言模型性能的新方法，该方法能够反映模型在不同输入中的推理一致性，为模型的泛化能力提供新视角。

Motivation: 当前评估（大型）语言模型能力的挑战在于基准测试的快速饱和和创建更复杂数据集的困难。

Method: 引入电路稳定性和电路等价性的数学定义，并通过三个案例研究进行实证分析。

Result: 电路稳定性能够表征和预测模型的不同泛化表现。

Conclusion: 该方法为模型泛化能力与可解释性之间的关联提供了新的研究路径。

Abstract: Extensively evaluating the capabilities of (large) language models is
difficult. Rapid development of state-of-the-art models induce benchmark
saturation, while creating more challenging datasets is labor-intensive.
Inspired by the recent developments in mechanistic interpretability, we
introduce circuit stability as a new way to assess model performance. Circuit
stability refers to a model's ability to apply a consistent reasoning
process-its circuit-across various inputs. We mathematically formalize circuit
stability and circuit equivalence. Then, through three case studies, we
empirically show that circuit stability and the lack thereof can characterize
and predict different aspects of generalization. Our proposed methods offer a
step towards rigorously relating the generality of models to their
interpretability.

</details>


### [151] [Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding based on Guided Space Transformation](https://arxiv.org/abs/2505.24754)
*Yingchaojie Feng,Yiqun Sun,Yandong Sun,Minfeng Zhu,Qiang Huang,Anthony K. H. Tung,Wei Chen*

Key words: instruction-following, text embedding, computational efficiency, GUIDED SPACE TRANSFORMATION

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: GSTransform 提出了一种轻量级的指令跟随文本嵌入框架，通过引导空间变换动态调整预计算的嵌入，显著减少了计算开销。

Motivation: 现有指令跟随文本嵌入方法需要为每个新指令重新编码整个语料库，计算开销大。

Method: GSTransform 利用预计算的嵌入和少量带标注数据，实时调整嵌入以对齐用户指令。

Result: 在多个任务和数据集上，GSTransform 提升了嵌入质量，并实现了 6~300 倍的速度提升。

Conclusion: GSTransform 是一种高效且高质量的指令跟随文本嵌入解决方案。

Abstract: In this work, we investigate an important task named instruction-following
text embedding, which generates dynamic text embeddings that adapt to user
instructions, highlighting specific attributes of text. Despite recent
advancements, existing approaches suffer from significant computational
overhead, as they require re-encoding the entire corpus for each new
instruction. To address this challenge, we propose GSTransform, a novel
instruction-following text embedding framework based on Guided Space
Transformation. Our key observation is that instruction-relevant information is
inherently encoded in generic embeddings but remains underutilized. Instead of
repeatedly encoding the corpus for each instruction, GSTransform is a
lightweight transformation mechanism that adapts pre-computed embeddings in
real time to align with user instructions, guided by a small amount of text
data with instruction-focused label annotation. We conduct extensive
experiments on three instruction-awareness downstream tasks across nine
real-world datasets, demonstrating that GSTransform improves
instruction-following text embedding quality over state-of-the-art methods
while achieving dramatic speedups of 6~300x in real-time processing on
large-scale datasets. The source code is available at
https://github.com/YingchaojieFeng/GSTransform.

</details>


### [152] [LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in Systematic Literature Reviews](https://arxiv.org/abs/2505.24757)
*Christian Jaumann,Andreas Wiedholz,Annemarie Friedrich*

Key words: 系统性文献综述（SLR），大型语言模型（LLM），摘要筛选，LGAR，零样本学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了LGAR（一种基于零样本的大型语言模型引导的摘要排序器），用于改进系统性文献综述（SLR）中的摘要筛选阶段。LGAR结合了LLM的分级相关性评分器和密集重排序器，实验表明其在平均精度上优于现有方法5-10个百分点。

Motivation: 科学文献快速增长，系统性文献综述（SLR）难以高效筛选相关论文。现有的基于问答（QA）的排序方法存在错误传播问题，且缺乏全面的评估标准。

Method: 提出LGAR，由基于LLM的分级相关性评分器和密集重排序器组成。

Result: LGAR在平均精度上比现有QA方法提升5-10个百分点。

Conclusion: LGAR有效提升了SLR摘要筛选的准确性和效率，为相关研究提供了新的工具和基准数据。

Abstract: The scientific literature is growing rapidly, making it hard to keep track of
the state-of-the-art. Systematic literature reviews (SLRs) aim to identify and
evaluate all relevant papers on a topic. After retrieving a set of candidate
papers, the abstract screening phase determines initial relevance. To date,
abstract screening methods using large language models (LLMs) focus on binary
classification settings; existing question answering (QA) based ranking
approaches suffer from error propagation. LLMs offer a unique opportunity to
evaluate the SLR's inclusion and exclusion criteria, yet, existing benchmarks
do not provide them exhaustively. We manually extract these criteria as well as
research questions for 57 SLRs, mostly in the medical domain, enabling
principled comparisons between approaches. Moreover, we propose LGAR, a
zero-shot LLM Guided Abstract Ranker composed of an LLM based graded relevance
scorer and a dense re-ranker. Our extensive experiments show that LGAR
outperforms existing QA-based methods by 5-10 pp. in mean average precision.
Our code and data is publicly available.

</details>


### [153] [From Macro to Micro: Probing Dataset Diversity in Language Model Fine-Tuning](https://arxiv.org/abs/2505.24768)
*Haoyu Li,Xuhong Li,Yiming Dong,Kun Liu*

Key words: 数据集多样性, 监督微调, 大语言模型, 宏观策略, 中观策略, 微观策略

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一套系统性的数据集多样性控制策略分类法，并展示了不同层次（宏观、中观、微观）策略对LLM性能的影响，其中微观策略在响应部分表现最优。

Motivation: 尽管数据集多样性对于机器学习模型（尤其是LLM的监督微调阶段）的重要性日益增加，但对多样性的系统分析仍然不足。为了解决这一问题，本文对多样性控制策略进行了系统性分类和分析。

Method: 提出了一个基于指令和响应部分的多样性控制策略分类法，包括宏观（整体语义）、中观（指令单元）和微观（响应部分的统计分布）。通过构建固定大小的数据集（10,000个样本）并应用六种策略，评估它们对LLM性能的影响。

Result: 实验结果显示，宏观和中观策略在多样性增加时性能提升，而微观策略在响应部分表现最强的多样性与性能相关性，并在所有策略中实现最高性能。

Conclusion: 微观层次的多样性控制策略（尤其是响应部分）对LLM性能的贡献最大，为构建高性能SFT数据集提供了实用指导。

Abstract: Dataset diversity plays a pivotal role for the successful training of many
machine learning models, particularly in the supervised fine-tuning (SFT) stage
of large language model (LLM) development. Despite increasing recognition of
its importance, systematic analyses of dataset diversity still remain
underexplored. To address this gap, this work presents a systematic taxonomy of
existing diversity-control strategies, which primarily focus on the instruction
component, operating at either macroscopic (entire instruction semantics) or
mesoscopic levels (instruction units), and furthermore introduces a novel
analysis of microscopic diversity within the response component, specifically
analyzing the statistical distribution of tokens in SFT training samples. In
the experimental evaluation, we construct fixed-size datasets (e.g., 10,000
samples each) from a corpus of 117,000 open-source SFT samples, incorporating
six distinct diversity-control strategies spanning macro-, meso-, and
microscopic levels applied to both instructions and responses. We then
fine-tune LLMs on these datasets to assess the six diversity-control
strategies. Results reveal that while macroscopic and mesoscopic strategies
lead to higher performance with increasing diversity, the microscopic strategy
in responses exhibits both a stronger correlation between model performance and
the degree of diversity and superior performance with maximum diversity across
all strategies. These findings offer actionable insights for constructing
high-performance SFT datasets.

</details>


### [154] [Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models' Uncertainty?](https://arxiv.org/abs/2505.24778)
*Jiayu Liu,Qing Zong,Weiqi Wang,Yangqiu Song*

Key words: 大型语言模型；认知标记；信心评估；分布稳定性；模型不确定性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLM）在表达信心时使用的认知标记的可靠性，发现其在同一分布内表现稳定，但在分布外场景中不一致，呼吁改进信心与模型实际不确定性的对齐。

Motivation: 在高风险领域使用LLM时，准确评估其信心至关重要。人类通常通过认知标记（如“相当自信”）表达信心，而非数值。但LLM是否一致使用这些标记反映其内在信心尚不明确。

Method: 研究人员首先将认知标记信心定义为模型使用标记时的观察准确性，并在开源和专有LLM的多问答数据集上评估其在分布内外场景中的稳定性。

Result: 结果显示，认知标记在同一分布内表现良好，但在分布外场景中信心不一致。

Conclusion: 这些发现对认知标记用于信心估计的可靠性提出了重要担忧，强调了改进标记信心与模型实际不确定性对齐的必要性。

Abstract: As large language models (LLMs) are increasingly used in high-stakes domains,
accurately assessing their confidence is crucial. Humans typically express
confidence through epistemic markers (e.g., "fairly confident") instead of
numerical values. However, it remains unclear whether LLMs consistently use
these markers to reflect their intrinsic confidence due to the difficulty of
quantifying uncertainty associated with various markers. To address this gap,
we first define marker confidence as the observed accuracy when a model employs
an epistemic marker. We evaluate its stability across multiple
question-answering datasets in both in-distribution and out-of-distribution
settings for open-source and proprietary LLMs. Our results show that while
markers generalize well within the same distribution, their confidence is
inconsistent in out-of-distribution scenarios. These findings raise significant
concerns about the reliability of epistemic markers for confidence estimation,
underscoring the need for improved alignment between marker based confidence
and actual model uncertainty. Our code is available at
https://github.com/HKUST-KnowComp/MarCon.

</details>


### [155] [Drop Dropout on Single-Epoch Language Model Pretraining](https://arxiv.org/abs/2505.24788)
*Houjun Liu,John Bauer,Christopher D. Manning*

Key words: dropout, 大型语言模型, 预训练, 单周期训练, 模型编辑

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究表明，在现代大型语言模型（LLM）的单周期预训练中，不使用dropout可以提高下游任务的性能。

Motivation: 尽管dropout在深度学习中曾被广泛使用以减少过拟合，但在现代LLM的单周期预训练中，过拟合现象较少，因此dropout的作用尚未被彻底研究。

Method: 通过在BERT和Pythia等模型上实验不同dropout水平，评估其在语言建模、语法任务、问答和推理任务中的表现。

Result: 发现不使用dropout时，模型在下游任务中表现更好，且更容易通过基于梯度的编辑进行修改。

Conclusion: 建议在单周期预训练中不使用dropout。

Abstract: Originally, dropout was seen as a breakthrough regularization technique that
reduced overfitting and improved performance in almost all applications of deep
learning by reducing overfitting. Yet, single-epoch pretraining tasks common to
modern LLMs yield minimal overfitting, leading to dropout not being used for
large LLMs. Nevertheless, no thorough empirical investigation has been done on
the role of dropout in LM pretraining. Through experiments in single-epoch
pretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs
with varying levels of dropout, we find that downstream performance in language
modeling, morpho-syntax (BLiMP), question answering (SQuAD), and
natural-language inference (MNLI) improves when dropout is not applied during
pretraining. We additionally find that the recently-introduced "early dropout"
also degrades performance over applying no dropout at all. We further
investigate the models' editability, and find that models trained without
dropout are more successful in gradient-based model editing (MEND) and
equivalent in representation-based model editing (ReFT). Therefore, we advocate
to drop dropout during single-epoch pretraining.

</details>


### [156] [Guiding Generative Storytelling with Knowledge Graphs](https://arxiv.org/abs/2505.24803)
*Zhijun Pan,Antonios Andronis,Eva Hayek,Oscar AP Wilkinson,Ilya Lasy,Annette Parry,Guy Gadney,Tim J. Smith,Mick Grierson*

Key words: Large Language Models, Retrieval-Augmented Generation, Knowledge Graphs, Storytelling

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了知识图谱（KG）如何通过LLM增强故事生成，提高叙事质量并实现用户驱动修改。通过15名参与者的用户研究，证明KG显著提升了结构化叙事的质量，并增强了用户的控制感。

Motivation: 研究旨在解决LLM在长文本连贯性和用户控制方面的挑战，探索结构化数据（如知识图谱）如何支持生成式叙事。

Method: 提出了一种KG辅助的故事生成流程，通过用户研究评估其效果，参与者通过编辑KG来调整叙事。

Result: 知识图谱显著提升了动作导向和结构化故事的质量，同时增强了用户的控制感和互动性。

Conclusion: 知识图谱可以有效地增强LLM故事生成的质量和用户控制，使叙事更具吸引力和互动性。

Abstract: Large Language Models (LLMs) have shown great potential in automated story
generation, but challenges remain in maintaining long-form coherence and
providing users with intuitive and effective control. Retrieval-Augmented
Generation (RAG) has proven effective in reducing hallucinations in text
generation; however, the use of structured data to support generative
storytelling remains underexplored. This paper investigates how knowledge
graphs (KGs) can enhance LLM-based storytelling by improving narrative quality
and enabling user-driven modifications. We propose a KG-assisted storytelling
pipeline and evaluate its effectiveness through a user study with 15
participants. Participants created their own story prompts, generated stories,
and edited knowledge graphs to shape their narratives. Through quantitative and
qualitative analysis, our findings demonstrate that knowledge graphs
significantly enhance story quality in action-oriented and structured
narratives within our system settings. Additionally, editing the knowledge
graph increases users' sense of control, making storytelling more engaging,
interactive, and playful.

</details>


### [157] [LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text](https://arxiv.org/abs/2505.24826)
*Li yunhan,Wu gengshen*

Key words: 大语言模型, 法律应用, 语言质量, 评估框架

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究提出了评估法律大模型语言质量的三步方法，发现模型质量在140亿参数后提升有限，Qwen3系列性价比最佳。

Motivation: 现有评估基准过于关注事实准确性，而忽视了清晰度、连贯性和术语等语言质量。

Method: 开发回归模型评估语言质量，创建专业法律问题集，分析49个LLMs。

Result: 模型质量在140亿参数后仅提升2.7%，推理模型优于基础架构，Qwen3系列性价比最高。

Conclusion: 建立了法律LLMs的标准化评估协议，揭示了当前训练数据优化的局限性。

Abstract: As large language models (LLMs) are increasingly used in legal applications,
current evaluation benchmarks tend to focus mainly on factual accuracy while
largely neglecting important linguistic quality aspects such as clarity,
coherence, and terminology. To address this gap, we propose three steps: First,
we develop a regression model to evaluate the quality of legal texts based on
clarity, coherence, and terminology. Second, we create a specialized set of
legal questions. Third, we analyze 49 LLMs using this evaluation framework.
  Our analysis identifies three key findings: First, model quality levels off
at 14 billion parameters, with only a marginal improvement of $2.7\%$ noted at
72 billion parameters. Second, engineering choices such as quantization and
context length have a negligible impact, as indicated by statistical
significance thresholds above 0.016. Third, reasoning models consistently
outperform base architectures. A significant outcome of our research is the
release of a ranking list and Pareto analysis, which highlight the Qwen3 series
as the optimal choice for cost-performance tradeoffs. This work not only
establishes standardized evaluation protocols for legal LLMs but also uncovers
fundamental limitations in current training data refinement approaches. Code
and models are available at: https://github.com/lyxx3rd/LegalEval-Q.

</details>


### [158] [Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs](https://arxiv.org/abs/2505.24830)
*Juraj Vladika,Annika Domres,Mai Nguyen,Rebecca Moser,Jana Nano,Felix Busch,Lisa C. Adams,Keno K. Bressem,Denise Bernhardt,Stephanie E. Combs,Kai J. Borm,Florian Matthes,Jan C. Peeken*

Key words: 大语言模型、医学问答、事实检查、可解释性、幻觉检测

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种原子事实检查框架，通过将LLM生成的回答分解为可验证的原子事实并对比权威医学指南，显著提升了医学长问答的事实准确性和可解释性。

Motivation: 当前LLMs在医学应用中存在幻觉和低事实可解释性问题，限制了其临床采用和合规性。现有方法（如检索增强生成）未能完全解决这些问题。

Method: 提出原子事实检查框架，将LLM回答分解为独立可验证的原子事实，逐一对比医学指南知识库进行验证，并支持错误定位和文献溯源。

Result: 框架将整体答案质量提升40%，幻觉检测率达50%，并通过溯源机制提供透明解释，显著优于现有方法。

Conclusion: 该框架为LLMs在临床中的可信应用迈出关键一步，解决了医疗AI的核心瓶颈。

Abstract: Large language models (LLMs) exhibit extensive medical knowledge but are
prone to hallucinations and inaccurate citations, which pose a challenge to
their clinical adoption and regulatory compliance. Current methods, such as
Retrieval Augmented Generation, partially address these issues by grounding
answers in source documents, but hallucinations and low fact-level
explainability persist. In this work, we introduce a novel atomic fact-checking
framework designed to enhance the reliability and explainability of LLMs used
in medical long-form question answering. This method decomposes LLM-generated
responses into discrete, verifiable units called atomic facts, each of which is
independently verified against an authoritative knowledge base of medical
guidelines. This approach enables targeted correction of errors and direct
tracing to source literature, thereby improving the factual accuracy and
explainability of medical Q&A. Extensive evaluation using multi-reader
assessments by medical experts and an automated open Q&A benchmark demonstrated
significant improvements in factual accuracy and explainability. Our framework
achieved up to a 40% overall answer improvement and a 50% hallucination
detection rate. The ability to trace each atomic fact back to the most relevant
chunks from the database provides a granular, transparent explanation of the
generated responses, addressing a major gap in current medical AI applications.
This work represents a crucial step towards more trustworthy and reliable
clinical applications of LLMs, addressing key prerequisites for clinical
application and fostering greater confidence in AI-assisted healthcare.

</details>


### [159] [How much do language models memorize?](https://arxiv.org/abs/2505.24832)
*John X. Morris,Chawin Sitawarin,Chuan Guo,Narine Kokhlikyan,G. Edward Suh,Alexander M. Rush,Kamalika Chaudhuri,Saeed Mahloujifar*

Key words: 语言模型,记忆容量,泛化,非预期记忆,缩放规律

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种评估模型对数据点「了解」程度的新方法，用于衡量现代语言模型的能力，区分了记忆与泛化，并计算了模型的总记忆容量约为每参数3.6比特。

Motivation: 此前研究难以区分语言模型的记忆与泛化行为，因此需要一种方法量化模型能力并明确两者界限。

Method: 将记忆分为非预期记忆和泛化两部分，完全消除泛化后计算总记忆容量，并训练多种规模的Transformer模型进行验证。

Result: 发现模型在记忆容量填满后开始「顿悟」，非预期记忆减少而泛化增加，并得出了模型容量与数据规模的缩放规律。

Conclusion: 量化了模型记忆容量与泛化行为的关系，为理解语言模型的能力提供了新视角。

Abstract: We propose a new method for estimating how much a model ``knows'' about a
datapoint and use it to measure the capacity of modern language models. Prior
studies of language model memorization have struggled to disentangle
memorization from generalization. We formally separate memorization into two
components: \textit{unintended memorization}, the information a model contains
about a specific dataset, and \textit{generalization}, the information a model
contains about the true data-generation process. When we completely eliminate
generalization, we can compute the total memorization, which provides an
estimate of model capacity: our measurements estimate that GPT-style models
have a capacity of approximately 3.6 bits per parameter. We train language
models on datasets of increasing size and observe that models memorize until
their capacity fills, at which point ``grokking'' begins, and unintended
memorization decreases as models begin to generalize. We train hundreds of
transformer language models ranging from $500K$ to $1.5B$ parameters and
produce a series of scaling laws relating model capacity and data size to
membership inference.

</details>


### [160] [Multilinguality Does not Make Sense: Investigating Factors Behind Zero-Shot Transfer in Sense-Aware Tasks](https://arxiv.org/abs/2505.24834)
*Roksana Goworek,Haim Dubossarsky*

Key words: 跨语言迁移, 多语言训练, 词义歧义, 词汇语义变化, 混杂因素

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文挑战了多语言训练对跨语言迁移任务有益的传统假设，特别是在词义歧义和词汇语义变化任务中。研究发现，多语言训练并非必需或固有优势，而是其他因素如数据组成和评估方法更关键。

Motivation: 探讨多语言训练是否对跨语言迁移任务（如词义歧义和词汇语义变化）有固有优势，并揭示影响跨语言迁移效果的实际因素。

Method: 通过大规模跨28种语言的实验分析，比较多语言训练的效果，并识别影响性能的混杂因素（如微调数据组成和评估方法）。

Result: 研究发现多语言训练并非必要或固有优势，真正影响性能的是数据组成和评估方法等混杂因素。

Conclusion: 研究呼吁在多语言NLP中进行更严格的评估，并提供了用于进一步研究的微调模型和基准数据集。

Abstract: Cross-lingual transfer allows models to perform tasks in languages unseen
during training and is often assumed to benefit from increased multilinguality.
In this work, we challenge this assumption in the context of two underexplored,
sense-aware tasks: polysemy disambiguation and lexical semantic change. Through
a large-scale analysis across 28 languages, we show that multilingual training
is neither necessary nor inherently beneficial for effective transfer. Instead,
we find that confounding factors - such as fine-tuning data composition and
evaluation artifacts - better account for the perceived advantages of
multilinguality. Our findings call for more rigorous evaluations in
multilingual NLP. We release fine-tuned models and benchmarks to support
further research, with implications extending to low-resource and typologically
diverse languages.

</details>


### [161] [MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs](https://arxiv.org/abs/2505.24858)
*Gabrielle Kaili-May Liu,Gal Yona,Avi Caciularu,Idan Szpektor,Tim G. J. Rudner,Arman Cohan*

Key words: LLM、不确定性校准、MetaFaith、信任度

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文研究了LLM在不确定性表达上的缺陷，提出了新的校准方法MetaFaith，显著提高了准确性。

Motivation: LLM在表达错误主张时过于自信，导致信任危机，需改进其不确定性表达的忠实性。

Method: 通过系统研究不同模型、数据集和提示策略，引入基于人类元认知的MetaFaith校准方法。

Result: MetaFaith提升了61%的忠实性，在人类评估中胜出83%。

Conclusion: MetaFaith有效解决了LLM不确定性表达的失准问题，提升了信任度。

Abstract: A critical component in the trustworthiness of LLMs is reliable uncertainty
communication, yet LLMs often use assertive language when conveying false
claims, leading to over-reliance and eroded trust. We present the first
systematic study of $\textit{faithful confidence calibration}$ of LLMs,
benchmarking models' ability to use linguistic expressions of uncertainty that
$\textit{faithfully reflect}$ their intrinsic uncertainty, across a
comprehensive array of models, datasets, and prompting strategies. Our results
demonstrate that LLMs largely fail at this task, and that existing
interventions are insufficient: standard prompt approaches provide only
marginal gains, and existing, factuality-based calibration techniques can even
harm faithful calibration. To address this critical gap, we introduce
MetaFaith, a novel prompt-based calibration approach inspired by human
metacognition. We show that MetaFaith robustly improves faithful calibration
across diverse models and task domains, enabling up to 61% improvement in
faithfulness and achieving an 83% win rate over original generations as judged
by humans.

</details>


### [162] [AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time](https://arxiv.org/abs/2505.24863)
*Junyu Zhang,Runpei Dong,Han Wang,Xuying Ning,Haoran Geng,Peihao Li,Xialin He,Yutong Bai,Jitendra Malik,Saurabh Gupta,Huan Zhang*

Key words: AlphaOne, 大型推理模型, 动态调控, $α$时刻, 伯努利过程

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: AlphaOne ($α1$) 是一个通用框架，通过在测试时动态调控大型推理模型（LRMs）的推理进程，引入$α$时刻和伯努利随机过程调度慢思考，提升推理效率和答案生成。

Motivation: 当前大型推理模型的推理过程通常缺乏灵活性，无法动态调整快慢思考的节奏。AlphaOne旨在通过通用参数$α$和随机过程动态调控推理进程，提升模型效率和性能。

Method: AlphaOne引入$α$时刻和伯努利随机过程调度慢思考过渡，动态插入推理过渡令牌，并在$α$时刻后确定性地终止慢思考，实现灵活的快慢推理调控。

Result: 在数学、编码和科学领域的多个基准测试中，Alpha1展现出卓越的推理能力和效率。

Conclusion: Alpha1通过通用参数和随机过程动态调控推理进程，统一并泛化了现有的单调缩放方法，显著提升模型效率和生成质量。

Abstract: This paper presents AlphaOne ($\alpha$1), a universal framework for
modulating reasoning progress in large reasoning models (LRMs) at test time.
$\alpha$1 first introduces $\alpha$ moment, which represents the scaled
thinking phase with a universal parameter $\alpha$. Within this scaled
pre-$\alpha$ moment phase, it dynamically schedules slow thinking transitions
by modeling the insertion of reasoning transition tokens as a Bernoulli
stochastic process. After the $\alpha$ moment, $\alpha$1 deterministically
terminates slow thinking with the end-of-thinking token, thereby fostering fast
reasoning and efficient answer generation. This approach unifies and
generalizes existing monotonic scaling methods by enabling flexible and dense
slow-to-fast reasoning modulation. Extensive empirical studies on various
challenging benchmarks across mathematical, coding, and scientific domains
demonstrate $\alpha$1's superior reasoning capability and efficiency. Project
page: https://alphaone-project.github.io/

</details>


### [163] [ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models](https://arxiv.org/abs/2505.24864)
*Mingjie Liu,Shizhe Diao,Ximing Lu,Jian Hu,Xin Dong,Yejin Choi,Jan Kautz,Yi Dong*

Key words: 强化学习, 语言模型, 推理能力, KL散度, 任务多样性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种名为ProRL的延长强化学习训练方法，通过KL散度控制和参考策略重置等手段，证明强化学习能拓展语言模型的推理能力，不仅限于放大基础模型中已有的高奖励输出。

Motivation: 研究动机在于探讨强化学习是否真正能拓展语言模型的推理能力，而非仅放大基础模型已有的高奖励输出，以及RL计算资源的持续增加是否能可靠地提升推理性能。

Method: 论文提出了ProRL方法，包括KL散度控制、参考策略重置和多样化任务集，通过延长的强化学习训练挖掘基础模型无法触及的推理策略。

Result: 实验结果表明，经过RL训练的模型在多种pass@k评估中表现优于基础模型，甚至在某些基础模型完全失效的任务中也能取得进步，且推理边界的扩展与基础模型能力和训练时长密切相关。

Conclusion: 研究得出结论称强化学习能够有效扩展语言模型的推理边界，并强调RL计算资源的增加能支持探索新的解空间。

Abstract: Recent advances in reasoning-centric language models have highlighted
reinforcement learning (RL) as a promising method for aligning models with
verifiable rewards. However, it remains contentious whether RL truly expands a
model's reasoning capabilities or merely amplifies high-reward outputs already
latent in the base model's distribution, and whether continually scaling up RL
compute reliably leads to improved reasoning performance. In this work, we
challenge prevailing assumptions by demonstrating that prolonged RL (ProRL)
training can uncover novel reasoning strategies that are inaccessible to base
models, even under extensive sampling. We introduce ProRL, a novel training
methodology that incorporates KL divergence control, reference policy
resetting, and a diverse suite of tasks. Our empirical analysis reveals that
RL-trained models consistently outperform base models across a wide range of
pass@k evaluations, including scenarios where base models fail entirely
regardless of the number of attempts. We further show that reasoning boundary
improvements correlates strongly with task competence of base model and
training duration, suggesting that RL can explore and populate new regions of
solution space over time. These findings offer new insights into the conditions
under which RL meaningfully expands reasoning boundaries in language models and
establish a foundation for future work on long-horizon RL for reasoning. We
release model weights to support further research:
https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [164] [DATD3: Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient For Model Free Reinforcement Learning Under Output Feedback Control](https://arxiv.org/abs/2505.23857)
*Wuhao Wang,Zhiyong Chen*

Key words: 强化学习, MDP, 输出反馈, 深度可分离卷积, 多头注意力, 连续控制

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了OPMDP框架和DATD3算法，用于解决强化学习中的部分状态信息问题，并在实验中表现出色。

Motivation: 解决强化学习在真实应用中因部分状态信息（输出反馈）导致的决策挑战。

Method: 提出OPMDP框架，并设计DATD3算法（结合深度可分离卷积和多头注意力编码历史观察）。

Result: 在连续控制任务中，DATD3表现优于现有基于记忆和循环的基线方法。

Conclusion: DATD3在部分和完全可观测性下均有效，且避免了循环模型的不稳定性。

Abstract: Reinforcement learning in real-world applications often involves
output-feedback settings, where the agent receives only partial state
information. To address this challenge, we propose the Output-Feedback Markov
Decision Process (OPMDP), which extends the standard MDP formulation to
accommodate decision-making based on observation histories. Building on this
framework, we introduce Depthwise Attention Twin Delayed Deep Deterministic
Policy Gradient (DATD3), a novel actor-critic algorithm that employs depthwise
separable convolution and multi-head attention to encode historical
observations. DATD3 maintains policy expressiveness while avoiding the
instability of recurrent models. Extensive experiments on continuous control
tasks demonstrate that DATD3 outperforms existing memory-based and recurrent
baselines under both partial and full observability.

</details>


### [165] [Towards Minimizing Feature Drift in Model Merging: Layer-wise Task Vector Fusion for Adaptive Knowledge Integration](https://arxiv.org/abs/2505.23859)
*Wenju Sun,Qingyong Li,Wen Wang,Yang Liu,Yangli-ao Geng,Boyang Li*

Key words: 多任务学习, 模型合并, 特征漂移, 凸优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: LOT Merging 是一种通过逐层最小化特征漂移来实现多任务模型合并的新方法，避免了参数级方法的性能差距和任务损失方法的二次训练成本。

Motivation: 现有方法在性能或成本上存在不足，而特征漂移与性能下降密切相关，因此提出直接优化特征漂移。

Method: 提出 LOT Merging，通过凸二次优化问题逐层最小化特征漂移，对线性和归一化层提供闭式解。

Result: 在视觉和视觉-语言基准测试中显著优于基线方法，最高提升 4.4%（ViT-B/32）。

Conclusion: LOT Merging 是一种高效且性能优越的多任务模型合并方法。

Abstract: Multi-task model merging aims to consolidate knowledge from multiple
fine-tuned task-specific experts into a unified model while minimizing
performance degradation. Existing methods primarily approach this by minimizing
differences between task-specific experts and the unified model, either from a
parameter-level or a task-loss perspective. However, parameter-level methods
exhibit a significant performance gap compared to the upper bound, while
task-loss approaches entail costly secondary training procedures. In contrast,
we observe that performance degradation closely correlates with feature drift,
i.e., differences in feature representations of the same sample caused by model
merging. Motivated by this observation, we propose Layer-wise Optimal Task
Vector Merging (LOT Merging), a technique that explicitly minimizes feature
drift between task-specific experts and the unified model in a layer-by-layer
manner. LOT Merging can be formulated as a convex quadratic optimization
problem, enabling us to analytically derive closed-form solutions for the
parameters of linear and normalization layers. Consequently, LOT Merging
achieves efficient model consolidation through basic matrix operations.
Extensive experiments across vision and vision-language benchmarks demonstrate
that LOT Merging significantly outperforms baseline methods, achieving
improvements of up to 4.4% (ViT-B/32) over state-of-the-art approaches.

</details>


### [166] [BiBLDR: Bidirectional Behavior Learning for Drug Repositioning](https://arxiv.org/abs/2505.23861)
*Renye Zhang,Mengyun Yang,Qichang Zhao,Jianxin Wang*

Key words: 药物重定位、双向学习、行为序列、冷启动、深度学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为BiBLDR的双向行为学习策略，用于药物重定位，通过重新定义任务为行为序列学习，解决了图基方法在冷启动场景下的局限性。

Motivation: 传统的图基药物重定位方法在冷启动场景（新药物缺乏关联疾病信息）中效果受限，因此作者提出一种双向行为学习框架以更全面地捕捉药物-疾病交互模式。

Method: 双向行为序列构建（药物与疾病双视角）+ 两阶段策略：1) 构建原型空间表征药物与疾病属性；2) 结合双向序列数据和原型预测潜在关联。

Result: BiBLDR在基准数据集上达到最先进性能，尤其在冷启动场景中显著优于先前方法。

Conclusion: 双向行为序列学习能更鲁棒且精准地建模药物-疾病交互关系，解决冷启动问题。

Abstract: Drug repositioning aims to identify potential new indications for existing
drugs to reduce the time and financial costs associated with developing new
drugs. Most existing deep learning-based drug repositioning methods
predominantly utilize graph-based representations. However, graph-based drug
repositioning methods struggle to perform effective inference in cold-start
scenarios involving novel drugs because of the lack of association information
with the diseases. Unlike traditional graph-based approaches, we propose a
bidirectional behavior learning strategy for drug repositioning, known as
BiBLDR. This innovative framework redefines drug repositioning as a behavior
sequential learning task to capture drug-disease interaction patterns. First,
we construct bidirectional behavioral sequences based on drug and disease
sides. The consideration of bidirectional information ensures a more meticulous
and rigorous characterization of the behavioral sequences. Subsequently, we
propose a two-stage strategy for drug repositioning. In the first stage, we
construct prototype spaces to characterize the representational attributes of
drugs and diseases. In the second stage, these refined prototypes and
bidirectional behavior sequence data are leveraged to predict potential
drug-disease associations. Based on this learning approach, the model can more
robustly and precisely capture the interactive relationships between drug and
disease features from bidirectional behavioral sequences. Extensive experiments
demonstrate that our method achieves state-of-the-art performance on benchmark
datasets. Meanwhile, BiBLDR demonstrates significantly superior performance
compared to previous methods in cold-start scenarios. Our code is published in
https://github.com/Renyeeah/BiBLDR.

</details>


### [167] [Mamba Integrated with Physics Principles Masters Long-term Chaotic System Forecasting](https://arxiv.org/abs/2505.23863)
*Chang Liu,Bohao Zhao,Jingtao Ding,Huandong Wang,Yong Li*

Key words: chaotic systems, long-term forecasting, physics-informed modeling, Mamba, attractor manifold

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: PhyxMamba is a new framework combining Mamba-based state-space models with physics-informed principles to improve long-term forecasting of chaotic systems from short observations.

Motivation: Existing methods for chaotic systems forecasting either require long-term training data or perform poorly in long-term predictions, lacking stability and dynamical coherence.

Method: PhyxMamba integrates Mamba-based state-space models, time-delay embeddings, generative training, multi-token prediction, and attractor geometry regularization.

Result: PhyxMamba shows superior long-term forecasting accuracy and captures key dynamical invariants across diverse chaotic systems.

Conclusion: This framework enables reliable prediction of chaotic systems with limited observations, offering broad applications in various fields.

Abstract: Long-term forecasting of chaotic systems from short-term observations remains
a fundamental and underexplored challenge due to the intrinsic sensitivity to
initial conditions and the complex geometry of strange attractors. Existing
approaches often rely on long-term training data or focus on short-term
sequence correlations, struggling to maintain predictive stability and
dynamical coherence over extended horizons. We propose PhyxMamba, a novel
framework that integrates a Mamba-based state-space model with physics-informed
principles to capture the underlying dynamics of chaotic systems. By
reconstructing the attractor manifold from brief observations using time-delay
embeddings, PhyxMamba extracts global dynamical features essential for accurate
forecasting. Our generative training scheme enables Mamba to replicate the
physical process, augmented by multi-token prediction and attractor geometry
regularization for physical constraints, enhancing prediction accuracy and
preserving key statistical invariants. Extensive evaluations on diverse
simulated and real-world chaotic systems demonstrate that PhyxMamba delivers
superior long-term forecasting and faithfully captures essential dynamical
invariants from short-term data. This framework opens new avenues for reliably
predicting chaotic systems under observation-scarce conditions, with broad
implications across climate science, neuroscience, epidemiology, and beyond.
Our code is open-source at https://github.com/tsinghua-fib-lab/PhyxMamba.

</details>


### [168] [Personalized Subgraph Federated Learning with Differentiable Auxiliary Projections](https://arxiv.org/abs/2505.23864)
*Wei Zhuo,Zhaohuan Zhan,Ziduo Yang,Han Yu*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: FedAux是一个联邦学习框架，通过辅助投影向量（APV）解决图数据的非独立同分布问题，实现个性化模型训练，且在准确性和个性化性能上优于现有基线。

Motivation: 解决图结构数据在联邦学习中的非独立同分布挑战，尤其是每个客户端持有不同的子图时，如何在不共享原始数据或节点嵌入的情况下实现模型对齐和聚合。

Method: 提出FedAux框架，每个客户端联合训练本地图神经网络（GNN）和可学习的APV，通过软排序和轻量级1D卷积优化嵌入，服务器利用APV计算客户端间相似性并进行加权参数混合。

Result: 理论分析证明设计的收敛性和合理性，实验结果表明FedAux在多个图基准上显著优于现有基线。

Conclusion: FedAux有效解决了图联邦学习的非独立同分布问题，提升了模型性能和个性化能力。

Abstract: Federated learning (FL) on graph-structured data typically faces non-IID
challenges, particularly in scenarios where each client holds a distinct
subgraph sampled from a global graph. In this paper, we introduce Federated
learning with Auxiliary projections (FedAux), a personalized subgraph FL
framework that learns to align, compare, and aggregate heterogeneously
distributed local models without sharing raw data or node embeddings. In
FedAux, each client jointly trains (i) a local GNN and (ii) a learnable
auxiliary projection vector (APV) that differentiably projects node embeddings
onto a 1D space. A soft-sorting operation followed by a lightweight 1D
convolution refines these embeddings in the ordered space, enabling the APV to
effectively capture client-specific information. After local training, these
APVs serve as compact signatures that the server uses to compute inter-client
similarities and perform similarity-weighted parameter mixing, yielding
personalized models while preserving cross-client knowledge transfer. Moreover,
we provide rigorous theoretical analysis to establish the convergence and
rationality of our design. Empirical evaluations across diverse graph
benchmarks demonstrate that FedAux substantially outperforms existing baselines
in both accuracy and personalization performance.

</details>


### [169] [Combining Deep Architectures for Information Gain estimation and Reinforcement Learning for multiagent field exploration](https://arxiv.org/abs/2505.23865)
*Emanuele Masiero,Vito Trianni,Giuseppe Vizzari,Dimitri Ognibene*

Key words: 精准农业, 主动探索, 深度学习, 熵优化, 可见性掩码

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种用于精准农业的主动探索框架，通过深度学习方法优化资源消耗与目标识别效率。

Motivation: 精准农业需要高效的自主系统进行作物监测，如何在资源有限的大规模环境中有效探索是关键问题。

Method: 采用两阶段深度学习框架，结合LSTM置信模型与POV可见性掩码，比较了三种代理架构的性能。

Result: 实验表明，结合熵、置信状态和可见性跟踪的Double-CNN代理在探索效率上表现最佳。

Conclusion: 不确定性感知策略能实现稳健且可扩展的探索，未来可结合课程学习、多代理协作等方法进一步优化。

Abstract: Precision agriculture requires efficient autonomous systems for crop
monitoring, where agents must explore large-scale environments while minimizing
resource consumption. This work addresses the problem as an active exploration
task in a grid environment representing an agricultural field. Each cell may
contain targets (e.g., damaged crops) observable from nine predefined points of
view (POVs). Agents must infer the number of targets per cell using partial,
sequential observations.
  We propose a two-stage deep learning framework. A pre-trained LSTM serves as
a belief model, updating a probabilistic map of the environment and its
associated entropy, which defines the expected information gain (IG). This
allows agents to prioritize informative regions. A key contribution is the
inclusion of a POV visibility mask in the input, preserving the Markov property
under partial observability and avoiding revisits to already explored views.
  Three agent architectures were compared: an untrained IG-based agent
selecting actions to maximize entropy reduction; a DQN agent using CNNs over
local 3x3 inputs with belief, entropy, and POV mask; and a Double-CNN DQN agent
with wider spatial context. Simulations on 20x20 maps showed that the untrained
agent performs well despite its simplicity. The DQN agent matches this
performance when the POV mask is included, while the Double-CNN agent
consistently achieves superior exploration efficiency, especially in larger
environments.
  Results show that uncertainty-aware policies leveraging entropy, belief
states, and visibility tracking lead to robust and scalable exploration. Future
work includes curriculum learning, multi-agent cooperation with shared rewards,
transformer-based models, and intrinsic motivation mechanisms to further
enhance learning efficiency and policy generalization.

</details>


### [170] [Towards Understanding The Calibration Benefits of Sharpness-Aware Minimization](https://arxiv.org/abs/2505.23866)
*Chengli Tan,Yubo Zhou,Haishan Ye,Guang Dai,Junmin Liu,Zengjie Song,Jiangshe Zhang,Zixiang Zhao,Yunda Hao,Yong Xu*

Key words: 深度神经网络, 校准性能, 尖锐感知最小化, SAM, CSAM, 过自信

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出通过尖锐感知最小化（SAM）及其变体CSAM来改善深度神经网络的校准性能，减少过自信问题，实验证明其在多个数据集上优于其他方法。

Motivation: 深度神经网络在安全关键应用中易出现校准不良和过自信问题，可能导致严重后果，因此需要改进校准性能。

Method: 采用尖锐感知最小化（SAM）及其变体CSAM，通过隐式最大化预测分布的熵来学习校准良好的模型。

Result: 在ImageNet-1K等数据集上，SAM显著降低校准误差，而CSAM表现更优，始终优于其他方法。

Conclusion: SAM和CSAM能有效改善模型校准，减少过自信问题，尤其在安全关键应用中具有重要价值。

Abstract: Deep neural networks have been increasingly used in safety-critical
applications such as medical diagnosis and autonomous driving. However, many
studies suggest that they are prone to being poorly calibrated and have a
propensity for overconfidence, which may have disastrous consequences. In this
paper, unlike standard training such as stochastic gradient descent, we show
that the recently proposed sharpness-aware minimization (SAM) counteracts this
tendency towards overconfidence. The theoretical analysis suggests that SAM
allows us to learn models that are already well-calibrated by implicitly
maximizing the entropy of the predictive distribution. Inspired by this
finding, we further propose a variant of SAM, coined as CSAM, to ameliorate
model calibration. Extensive experiments on various datasets, including
ImageNet-1K, demonstrate the benefits of SAM in reducing calibration error.
Meanwhile, CSAM performs even better than SAM and consistently achieves lower
calibration error than other approaches

</details>


### [171] [Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert](https://arxiv.org/abs/2505.23868)
*Zhaokun Wang,Jinyu Guo,Jingwen Pu,Lingfeng Chen,Hongli Pu,Jie Ou. Libo Qin,Wenhong Tian*

Key words: parameter-efficient fine-tuning, noise robustness, LoRA, mixture-of-experts, noise injection

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为LoPE的噪声鲁棒适应方法，通过不对称LoRA毒化专家增强模型对噪声的鲁棒性，仅需生成噪声数据，无需数据清洗。

Motivation: 现有参数高效微调方法易受噪声数据干扰，传统方法需要繁重数据预处理或易导致误差累积的模型结构修改。

Method: 提出LoPE框架，采用不对称LoRA配置的毒化专家，通过两阶段范式在微调时注入噪声以增强噪声处理能力，推理时屏蔽毒化专家以利用净化知识。

Result: 大量实验表明，LoPE通过低成本噪声注入实现了强性能和鲁棒性，完全无需数据清洗。

Conclusion: LoPE为噪声鲁棒适应提供了一种高效且低成本的新方法。

Abstract: Current parameter-efficient fine-tuning methods for adapting pre-trained
language models to downstream tasks are susceptible to interference from noisy
data. Conventional noise-handling approaches either rely on laborious data
pre-processing or employ model architecture modifications prone to error
accumulation. In contrast to existing noise-process paradigms, we propose a
noise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a
novel framework that enhances model robustness to noise only with generated
noisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE
strategically integrates a dedicated poisoning expert in an asymmetric LoRA
configuration. Through a two-stage paradigm, LoPE performs noise injection on
the poisoning expert during fine-tuning to enhance its noise discrimination and
processing ability. During inference, we selectively mask the dedicated
poisoning expert to leverage purified knowledge acquired by normal experts for
noise-robust output. Extensive experiments demonstrate that LoPE achieves
strong performance and robustness purely through the low-cost noise injection,
which completely eliminates the requirement of data cleaning.

</details>


### [172] [MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection](https://arxiv.org/abs/2505.23870)
*Yixian Shen,Qi Bi,Jia-Hong Huang,Hongyi Zhu,Andy D. Pimentel,Anuj Pathania*

Key words: MaCP, 余弦投影, 自适应, 低秩, 模型效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种新的自适应方法MaCP，通过余弦投影在低秩自适应中选取关键频率成分，显著提升了模型效率和准确性。

Motivation: 针对大型基础模型微调时参数和内存需求高的问题，提出一种既能提升效率又能保持高准确性的自适应方法。

Method: 利用余弦投影的优势，将权重变化投影到离散余弦空间，并分区选取关键频率成分。

Result: 在单模态和多模态任务中均表现出色，准确性更高且计算复杂度和内存需求显著降低。

Conclusion: MaCP是一种高效且强大的自适应方法，适用于多种任务。

Abstract: We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine
Projection, that achieves exceptional performance while requiring minimal
parameters and memory for fine-tuning large foundation models. Its general idea
is to exploit the superior energy compaction and decorrelation properties of
cosine projection to improve both model efficiency and accuracy. Specifically,
it projects the weight change from the low-rank adaptation into the discrete
cosine space. Then, the weight change is partitioned over different levels of
the discrete cosine spectrum, and each partition's most critical frequency
components are selected. Extensive experiments demonstrate the effectiveness of
MaCP across a wide range of single-modality tasks, including natural language
understanding, natural language generation, text summarization, as well as
multi-modality tasks such as image classification and video understanding. MaCP
consistently delivers superior accuracy, significantly reduced computational
complexity, and lower memory requirements compared to existing alternatives.

</details>


### [173] [ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning](https://arxiv.org/abs/2505.23871)
*Zeyuan Liu,Zhihe Yang,Jiawei Xu,Rui Yang,Jiafei Lyu,Baoxiang Wang,Yunjian Xu,Xiu Li*

Key words: 离线强化学习、数据噪声、扩散模型、Ambient DDPM、数据修复

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了ADG方法，利用扩散模型处理离线强化学习中数据噪声问题，通过Ambient DDPM区分并修复损坏数据，提升数据质量以增强RL训练效果。

Motivation: 真实世界数据集常因传感器或人为输入包含噪声和错误，导致离线强化学习应用困难。现有方法难以处理高维状态空间和多元素同时损坏的情况。扩散模型虽有去噪能力，但易过拟合噪声样本。

Method: 提出ADG方法，结合Ambient DDPM从近似分布中学习（理论保证），利用噪声预测特性区分干净/损坏数据，再用标准DDPM修复损坏数据，最终提升离线RL训练质量。

Result: 在MuJoCo、Kitchen等基准测试中，ADG显著降低损坏数据的影响，提升离线RL在多种噪声场景下的鲁棒性，达到SOTA效果。

Conclusion: ADG为离线RL数据噪声问题提供了通用解决方案，可无缝集成至任何RL算法，展现了扩散模型在数据修复中的潜力。

Abstract: Real-world datasets collected from sensors or human inputs are prone to noise
and errors, posing significant challenges for applying offline reinforcement
learning (RL). While existing methods have made progress in addressing
corrupted actions and rewards, they remain insufficient for handling corruption
in high-dimensional state spaces and for cases where multiple elements in the
dataset are corrupted simultaneously. Diffusion models, known for their strong
denoising capabilities, offer a promising direction for this problem-but their
tendency to overfit noisy samples limits their direct applicability. To
overcome this, we propose Ambient Diffusion-Guided Dataset Recovery (ADG), a
novel approach that pioneers the use of diffusion models to tackle data
corruption in offline RL. First, we introduce Ambient Denoising Diffusion
Probabilistic Models (DDPM) from approximated distributions, which enable
learning on partially corrupted datasets with theoretical guarantees. Second,
we use the noise-prediction property of Ambient DDPM to distinguish between
clean and corrupted data, and then use the clean subset to train a standard
DDPM. Third, we employ the trained standard DDPM to refine the previously
identified corrupted data, enhancing data quality for subsequent offline RL
training. A notable strength of ADG is its versatility-it can be seamlessly
integrated with any offline RL algorithm. Experiments on a range of benchmarks,
including MuJoCo, Kitchen, and Adroit, demonstrate that ADG effectively
mitigates the impact of corrupted data and improves the robustness of offline
RL under various noise settings, achieving state-of-the-art results.

</details>


### [174] [A Benchmark Dataset for Graph Regression with Homogeneous and Multi-Relational Variants](https://arxiv.org/abs/2505.23875)
*Peter Samoaa,Marcus Vukojevic,Morteza Haghir Chehreghani,Antonio Longa*

Key words: 图回归,程序图,图神经网络,同构图,多关系图

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文介绍了RelSC，一个基于程序图的新图回归数据集，填补了现有基准数据集在多样性和结构类型上的不足，并通过Homogeneous和Multi-relational两种变体验证了图结构表示对模型性能的影响。

Motivation: 现有图回归基准数据集主要集中于分子图和引用网络，缺乏多样性，限制了模型在异构图结构上的泛化能力研究。

Method: 提出RelSC数据集，包含两种变体：RelSC-H（单一边类型的同构图）和RelSC-M（多边类型的多关系图），并利用多种图神经网络架构进行评估。

Result: 实验表明，同构图与多关系图设置下模型性能存在显著差异，突显了结构表示的重要性。

Conclusion: RelSC为图回归方法提供了一个具有挑战性和多样性的新基准。

Abstract: Graph-level regression underpins many real-world applications, yet public
benchmarks remain heavily skewed toward molecular graphs and citation networks.
This limited diversity hinders progress on models that must generalize across
both homogeneous and heterogeneous graph structures. We introduce RelSC, a new
graph-regression dataset built from program graphs that combine syntactic and
semantic information extracted from source code. Each graph is labelled with
the execution-time cost of the corresponding program, providing a continuous
target variable that differs markedly from those found in existing benchmarks.
RelSC is released in two complementary variants. RelSC-H supplies rich node
features under a single (homogeneous) edge type, while RelSC-M preserves the
original multi-relational structure, connecting nodes through multiple edge
types that encode distinct semantic relationships. Together, these variants let
researchers probe how representation choice influences model behaviour. We
evaluate a diverse set of graph neural network architectures on both variants
of RelSC. The results reveal consistent performance differences between the
homogeneous and multi-relational settings, emphasising the importance of
structural representation. These findings demonstrate RelSC's value as a
challenging and versatile benchmark for advancing graph regression methods.

</details>


### [175] [A comparative analysis of a neural network with calculated weights and a neural network with random generation of weights based on the training dataset size](https://arxiv.org/abs/2505.23876)
*Polad Geidarov*

Key words: 多层感知器, 权重预计算, MNIST, 训练效率, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文比较了通过公式预先计算权重的多层感知器神经网络与随机初始化权重的网络在MNIST数据集上的性能，发现预计算权重能显著加快训练速度并增强对训练数据减少的鲁棒性。

Motivation: 探究通过解析公式预先计算权重是否能提升多层感知器的训练效率和鲁棒性，特别是在训练数据有限的情况下。

Method: 使用MNIST数据集进行对比实验，对比预计算权重与随机初始化的多层感知器神经网络的训练速度和性能。

Result: 预计算权重的网络训练速度更快，且对训练数据量减少的鲁棒性更强。

Conclusion: 通过解析方法预先计算权重是提升神经网络训练效率和鲁棒性的有效途径。

Abstract: The paper discusses the capabilities of multilayer perceptron neural networks
implementing metric recognition methods, for which the values of the weights
are calculated analytically by formulas. Comparative experiments in training a
neural network with pre-calculated weights and with random initialization of
weights on different sizes of the MNIST training dataset are carried out. The
results of the experiments show that a multilayer perceptron with
pre-calculated weights can be trained much faster and is much more robust to
the reduction of the training dataset.

</details>


### [176] [Actor-Critic based Online Data Mixing For Language Model Pre-Training](https://arxiv.org/abs/2505.23878)
*Jing Ma,Chenhao Dang,Mingjie Liao*

Key words: 大型语言模型、在线数据混合、actor-critic、预训练优化、域内交互

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种基于actor-critic的在线数据混合方法（AC-ODM），通过辅助网络捕获动态域权重并考虑域内交互，有效提升大型语言模型预训练效率和性能。

Motivation: 为减少大型语言模型预训练的碳足迹和成本，需优化数据混合方法，但现有方法未能随训练动态演化且忽略域内交互。

Method: 结合actor-critic网络动态调整域权重，利用代理模型训练采样策略并直接应用于目标模型预训练。

Result: AC-ODM-410M的验证困惑度优化速度比现有方法快71%，在MMLU和HumanEval基准上分别提升27.5%准确率和2.23倍通过率。

Conclusion: AC-ODM方法显著提升预训练效率与模型性能，验证了动态数据混合策略的优越性。

Abstract: The coverage and composition of pretraining data significantly impacts the
generalization ability of Large Language Models (LLMs). To reduce the carbon
footprint and financial costs of training, some data mixing methods, which
applied the optimized domain weights of a small proxy model to train a larger
one, were proposed. However, these methods did not evolute with the training
dynamics. The existing online data mixing (ODM) method addressed this
limitation by applying the multi-armed bandit algorithm as data sampling
strategy. Yet, it did not consider the intra-domain interactions. In this
paper, we develop an actor-critic based online data mixing (AC-ODM) method,
which captures the varying domain weights by auxiliary actor-critic networks
and consider the intra-domain interactions with the reward function. While
constructing the dataset to pretrain a large target LLM, we directly apply the
actor, which is trained with a small proxy LLM as the environment, as the
sampling strategy. The transfer of sampling strategy can not only ensure the
efficiency of dynamical data mixing, but also expedite the convergence of
pretraining the target LLM. Numerical results demonstrate that AC-ODM-410M,
which invokes the sampling strategy obtained by a proxy LLM with 410M
parameters, reaching the optimal validation perplexity of ODM 71% faster, and
improves performance on the zero-shot MMLU benchmark by 27.5% of accuracy,
about 2.23x better on pass@1 of HumanEval benchmark.

</details>


### [177] [CNN-LSTM Hybrid Model for AI-Driven Prediction of COVID-19 Severity from Spike Sequences and Clinical Data](https://arxiv.org/abs/2505.23879)
*Caio Cheohen,Vinnícius M. S. Gomes,Manuela L. da Silva*

Key words: COVID-19、深度学习、CNN-LSTM、刺突蛋白、疾病预测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究开发了一种混合CNN-LSTM深度学习模型，利用刺突蛋白序列和临床数据预测COVID-19严重程度，取得较高分类性能。

Motivation: COVID-19疫情凸显了准确预测疾病严重程度的迫切需求，以优化医疗资源分配和患者管理。刺突蛋白的高突变率影响病毒致病性，人工智能方法为此提供了潜在解决方案。

Method: 研究从GISAID数据库获取了9,570个刺突蛋白序列，经标准化筛选后使用3,467个（2,313重症，1,154轻症）。通过特征工程提取序列特征，临床数据采用独热编码。采用混合CNN-LSTM架构，CNN提取局部模式，LSTM建模长期依赖。

Result: 模型F1分数82.92%，ROC-AUC 0.9084，精度83.56%，召回率82.85%，训练准确率85%。主要流行谱系（P.1、AY.99.2）和分支（GR、GK）与区域流行病学趋势一致。

Conclusion: CNN-LSTM混合模型有效预测了COVID-19严重程度，展示了AI在基因组监测和精准公共卫生中的潜力，为未来疫情早期预测提供了框架。

Abstract: The COVID-19 pandemic, caused by SARS-CoV-2, highlighted the critical need
for accurate prediction of disease severity to optimize healthcare resource
allocation and patient management. The spike protein, which facilitates viral
entry into host cells, exhibits high mutation rates, particularly in the
receptor-binding domain, influencing viral pathogenicity. Artificial
intelligence approaches, such as deep learning, offer promising solutions for
leveraging genomic and clinical data to predict disease outcomes. Objective:
This study aimed to develop a hybrid CNN-LSTM deep learning model to predict
COVID-19 severity using spike protein sequences and associated clinical
metadata from South American patients. Methods: We retrieved 9,570 spike
protein sequences from the GISAID database, of which 3,467 met inclusion
criteria after standardization. The dataset included 2,313 severe and 1,154
mild cases. A feature engineering pipeline extracted features from sequences,
while demographic and clinical variables were one-hot encoded. A hybrid
CNN-LSTM architecture was trained, combining CNN layers for local pattern
extraction and an LSTM layer for long-term dependency modeling. Results: The
model achieved an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%,
and recall of 82.85%, demonstrating robust classification performance. Training
stabilized at 85% accuracy with minimal overfitting. The most prevalent
lineages (P.1, AY.99.2) and clades (GR, GK) aligned with regional
epidemiological trends, suggesting potential associations between viral
genetics and clinical outcomes. Conclusion: The CNN-LSTM hybrid model
effectively predicted COVID-19 severity using spike protein sequences and
clinical data, highlighting the utility of AI in genomic surveillance and
precision public health. Despite limitations, this approach provides a
framework for early severity prediction in future outbreaks.

</details>


### [178] [Test-Time Training Done Right](https://arxiv.org/abs/2505.23884)
*Tianyuan Zhang,Sai Bi,Yicong Hong,Kai Zhang,Fujun Luan,Songlin Yang,Kalyan Sunkavalli,William T. Freeman,Hao Tan*

Key words: Large Chunk TTT, Test-Time Training, long-context modeling, hardware utilization, nonlinear state size

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文介绍了大型块测试时间训练（LaCT）方法，通过使用超大块更新显著提高硬件利用率和状态容量，适用于多种模态任务。

Motivation: 现有TTT方法因使用小批量处理在长上下文数据上效果不佳，且硬件利用率低，LaCT旨在解决这些问题。

Method: 采用极大块更新（2K到1M token），提高硬件利用率，增大非线性状态规模，支持复杂优化器如Muon。

Result: 实验验证了LaCT在多种模态任务中的有效性，包括140亿参数的自回归视频扩散模型和100万上下文长度的新视角合成。

Conclusion: LaCT为长上下文建模和测试时间训练提供了高效解决方案，有望推动相关领域研究。

Abstract: Test-Time Training (TTT) models context dependencies by adapting part of the
model's weights (referred to as fast weights) during inference. This fast
weight, akin to recurrent states in RNNs, stores temporary memories of past
tokens in the current sequence. Existing TTT methods struggled to show
effectiveness in handling long-context data, due to their inefficiency on
modern GPUs. The TTT layers in many of these approaches operate with extremely
low FLOPs utilization (often <5%) because they deliberately apply small online
minibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover,
a small minibatch implies fine-grained block-wise causal dependencies in the
data, unsuitable for data beyond 1D ordered sequences, like sets or
N-dimensional grids such as images or videos. In contrast, we pursue the
opposite direction by using an extremely large chunk update, ranging from 2K to
1M tokens across tasks of varying modalities, which we refer to as Large Chunk
Test-Time Training (LaCT). It improves hardware utilization by orders of
magnitude, and more importantly, facilitates scaling of nonlinear state size
(up to 40% of model parameters), hence substantially improving state capacity,
all without requiring cumbersome and error-prone kernel implementations. It
also allows easy integration of sophisticated optimizers, e.g. Muon for online
updates. We validate our approach across diverse modalities and tasks,
including novel view synthesis with image set, language models, and
auto-regressive video diffusion. Our approach can scale up to 14B-parameter AR
video diffusion model on sequences up to 56K tokens. In our longest sequence
experiment, we perform novel view synthesis with 1 million context length. We
hope this work will inspire and accelerate new research in the field of
long-context modeling and test-time training. Website:
https://tianyuanzhang.com/projects/ttt-done-right

</details>


### [179] [Simplifying Bayesian Optimization Via In-Context Direct Optimum Sampling](https://arxiv.org/abs/2505.23913)
*Gustavo Sutter Pessurno de Carvalho,Mohammed Abdulrahman,Hao Wang,Sriram Ganapathi Subramanian,Marc St-Aubin,Sharon O'Sullivan,Lawrence Wan,Luis Ricardez-Sandoval,Pascal Poupart,Agustinus Kristiadi*

Key words: 贝叶斯优化, 零样本学习, 深度生成模型, 高效优化, 分布式计算

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种零样本的贝叶斯优化方法，通过预训练的深度生成模型直接从最优点的后验分布中采样，避免了传统方法中昂贵的代理模型拟合和采集函数优化步骤，显著提高了效率和并行能力。

Motivation: 传统贝叶斯优化中的代理模型重训练和采集函数优化步骤成本高昂，限制了效率和并行能力，因此需要一种更高效、零样本的解决方案。

Method: 使用预训练的深度生成模型直接从最优点的后验分布中采样，避免了代理模型拟合和采集函数优化的步骤。

Result: 该方法在真实世界基准测试中表现出色，实现了35倍以上的效率提升，支持高效的并行和分布式优化。

Conclusion: 这种零样本方法显著降低了贝叶斯优化的计算成本，提高了效率和并行能力，适用于高吞吐量的优化任务。

Abstract: The optimization of expensive black-box functions is ubiquitous in science
and engineering. A common solution to this problem is Bayesian optimization
(BO), which is generally comprised of two components: (i) a surrogate model and
(ii) an acquisition function, which generally require expensive re-training and
optimization steps at each iteration, respectively. Although recent work
enabled in-context surrogate models that do not require re-training, virtually
all existing BO methods still require acquisition function maximization to
select the next observation, which introduces many knobs to tune, such as Monte
Carlo samplers and multi-start optimizers. In this work, we propose a
completely in-context, zero-shot solution for BO that does not require
surrogate fitting or acquisition function optimization. This is done by using a
pre-trained deep generative model to directly sample from the posterior over
the optimum point. We show that this process is equivalent to Thompson sampling
and demonstrate the capabilities and cost-effectiveness of our foundation model
on a suite of real-world benchmarks. We achieve an efficiency gain of more than
35x in terms of wall-clock time when compared with Gaussian process-based BO,
enabling efficient parallel and distributed BO, e.g., for high-throughput
optimization.

</details>


### [180] [Thompson Sampling in Online RLHF with General Function Approximation](https://arxiv.org/abs/2505.23927)
*Songtao Feng,Jie Fu*

Key words: 强化学习人类反馈（RLHF）, Thompson采样, 贝尔曼探索维度, 统计效率, 遗憾界

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文研究了在线强化学习人类反馈（RLHF）的统计效率，提出了一种基于Thompson采样的无模型后验采样算法，并给出了理论保证。通过贝尔曼探索（BE）维度和对数括号数复杂度衡量，证明了算法具有$O(\sqrt{T})$遗憾界。

Motivation: 尽管RLHF在实践中成功对齐大型语言模型与人类偏好，但其统计效率缺乏理论研究，因此有必要从理论角度分析算法的性能。

Method: 设计了基于Thompson采样的无模型后验采样算法，利用贝尔曼探索维度和对数括号数作为复杂度度量。

Result: 算法在BE维度、对数括号数和时间范围的乘积因子下，具有$O(\sqrt{T})$遗憾界。同时，基于MLE泛化边界的贝尔曼误差平方集中不等式是关键。

Conclusion: 提出的算法在理论上高效，且分析中提出的集中不等式方法可能具有独立意义。

Abstract: Reinforcement learning from human feedback (RLHF) has achieved great
empirical success in aligning large language models (LLMs) with human
preference, and it is of great importance to study the statistical efficiency
of RLHF algorithms from a theoretical perspective. In this work, we consider
the online RLHF setting where the preference data is revealed during the
learning process and study action value function approximation. We design a
model-free posterior sampling algorithm for online RLHF inspired by Thompson
sampling and provide its theoretical guarantee. Specifically, we adopt Bellman
eluder (BE) dimension as the complexity measure of the function class and
establish $O(\sqrt{T})$ regret bound for the proposed algorithm with other
multiplicative factor depending on the horizon, BE dimension and the
$log$-bracketing number of the function class. Further, in the analysis, we
first establish the concentration-type inequality of the squared Bellman error
bound based on the maximum likelihood estimator (MLE) generalization bound,
which plays the crucial rules in obtaining the eluder-type regret bound and may
be of independent interest.

</details>


### [181] [BIRD: Behavior Induction via Representation-structure Distillation](https://arxiv.org/abs/2505.23933)
*Galen Pogoncheff,Michael Beyeler*

Key words: 深度学习, 表示结构蒸馏, 稳健性, 行为迁移, AI对齐

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: BIRD框架通过匹配学生模型与教师模型的内部表示结构，实现了人类对齐行为的高效迁移，显著提升了模型的稳健性。

Motivation: 解决现有方法在迁移人类对齐行为时容易遗忘或数据收集成本高的问题。

Method: 提出BIRD框架，通过表示结构蒸馏将教师模型的对齐行为迁移到学生模型。

Result: 在图像分类任务中，BIRD比基线方法提升稳健准确率高达16%，且在教师模型规模较小时仍有效。

Conclusion: BIRD为安全AI系统的实际部署提供了高效且可扩展的解决方案。

Abstract: Human-aligned deep learning models exhibit behaviors consistent with human
values, such as robustness, fairness, and honesty. Transferring these
behavioral properties to models trained on different tasks or data
distributions remains challenging: aligned behavior is easily forgotten during
fine-tuning, and collecting task-specific data that preserves this behavior can
be prohibitively costly. We introduce BIRD (Behavior Induction via
Representation-structure Distillation), a flexible framework for transferring
aligned behavior by matching the internal representation structure of a student
model to that of a teacher. Applied to out-of-distribution robustness in image
classification, BIRD outperforms fine-tuning, transfer learning, and continual
learning methods, improving robust accuracy by up to 16% over the next
strongest baseline. It remains effective even when the teacher is trained on a
much simpler dataset and is $25 \times$ smaller than the student. In a
large-scale study of over 400 teacher-student pairs, we show that three
interpretable and computable properties of the teacher's representations (i.e.,
task relevance, behavioral relevance, and complementary knowledge) explain up
to 85% of the variance in transfer success. These insights offer practical
guidance for teacher selection and design. BIRD turns small, well-aligned
models into scalable alignment seeds, removing a key bottleneck in deploying
safe AI systems in the wild.

</details>


### [182] [Searching Neural Architectures for Sensor Nodes on IoT Gateways](https://arxiv.org/abs/2505.23939)
*Andrea Mattia Garavagno,Edoardo Ragusa,Antonio Frisoli,Paolo Gastaldo*

Key words: 边缘计算，物联网，神经网络设计，隐私保护，自动机器学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种在边缘设备上自动设计神经网络的方法，支持隐私敏感的物联网应用，避免数据外传，并在实验中展示了优异性能。

Motivation: 为保护隐私敏感数据（如医疗和工业物联网中的信息），需要一种在本地网络中设计和部署神经网络的方法，避免依赖云服务。

Method: 该方法在物联网网关上运行，为连接的传感器节点设计定制化神经网络，数据不离开本地网络，并通过搜索程序优化网络结构。

Result: 在Visual Wake Words数据集上的实验表明，该方法在Raspberry Pi Zero 2上10小时内即可实现优于现有技术的结果。

Conclusion: 该方法为隐私敏感的边缘计算场景提供了高效的神经网络设计解决方案，适用于医疗和工业物联网的个性化服务。

Abstract: This paper presents an automatic method for the design of Neural Networks
(NNs) at the edge, enabling Machine Learning (ML) access even in
privacy-sensitive Internet of Things (IoT) applications. The proposed method
runs on IoT gateways and designs NNs for connected sensor nodes without sharing
the collected data outside the local network, keeping the data in the site of
collection. This approach has the potential to enable ML for Healthcare
Internet of Things (HIoT) and Industrial Internet of Things (IIoT), designing
hardware-friendly and custom NNs at the edge for personalized healthcare and
advanced industrial services such as quality control, predictive maintenance,
or fault diagnosis. By preventing data from being disclosed to cloud services,
this method safeguards sensitive information, including industrial secrets and
personal data. The outcomes of a thorough experimental session confirm that --
on the Visual Wake Words dataset -- the proposed approach can achieve
state-of-the-art results by exploiting a search procedure that runs in less
than 10 hours on the Raspberry Pi Zero 2.

</details>


### [183] [Vision Language Models are Biased](https://arxiv.org/abs/2505.23941)
*An Vo,Khai-Nguyen Nguyen,Mohammad Reza Taesiri,Vy Tuong Dang,Anh Totti Nguyen,Daeyoung Kim*

Key words: 视觉语言模型（VLMs）、偏见、反事实图像、计数任务、知识偏见

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究发现，大规模视觉语言模型（VLMs）在处理计数和识别任务时，受到已学习知识的强烈偏见影响，导致准确性下降，即使在明确要求其检查结果或依赖图像细节时，改进也非常有限。

Motivation: 由于大规模语言模型（LLMs）从互联网中学习了大量先验知识，这些知识可能在某些视觉任务中引入偏见或错误，作者旨在测试这些知识如何影响视觉语言模型（VLMs）在标准视觉任务（如计数和识别）上的表现。

Method: 通过设计实验，使用包含7个不同领域（如动物、标志、象棋等）的反事实图像任务，测试VLMs的准确性，并插入描述主题的文本以进一步验证偏见影响。同时，尝试通过指令改进模型表现（如要求模型检查结果或依赖图像细节）。

Result: 研究发现，VLMs的平均计数准确率仅为17.05%，且插入主题文本会进一步降低准确性。即使通过指令改进，准确率仅平均提高2个百分点。

Conclusion: VLMs在处理视觉任务时存在显著的偏见问题，且现有方法难以有效解决。作者提出了一个自动化框架来测试VLM偏见，并呼吁进一步研究。

Abstract: Large language models (LLMs) memorize a vast amount of prior knowledge from
the Internet that help them on downstream tasks but also may notoriously sway
their outputs towards wrong or biased answers. In this work, we test how the
knowledge about popular subjects hurt the accuracy of vision language models
(VLMs) on standard, objective visual tasks of counting and identification. We
find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a
fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of
17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo)
across 7 diverse domains from animals, logos, chess, board games, optical
illusions, to patterned grids. Insert text (e.g., "Adidas") describing the
subject name into the counterfactual image further decreases VLM accuracy. The
biases in VLMs are so strong that instructing them to double-check their
results or rely exclusively on image details to answer improves counting
accuracy by only +2 points, on average. Our work presents an interesting
failure mode in VLMs and an automated framework for testing VLM biases. Code
and data are available at: vlmsarebiased.github.io.

</details>


### [184] [SG-Blend: Learning an Interpolation Between Improved Swish and GELU for Robust Neural Representations](https://arxiv.org/abs/2505.23942)
*Gaurav Sarkar,Jay Gala,Subarna Tripathi*

Key words: 激活函数, 深度学习, SG-Blend, SSwish, GELU

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了SG-Blend激活函数，通过动态插值结合SSwish和GELU的优点，提高了模型在自然语言和计算机视觉任务中的性能。

Motivation: 现有激活函数如Swish和GELU存在领域特定优化问题，SG-Blend旨在结合两者的优势以实现更通用的模型表现。

Method: 提出SG-Blend，通过学习参数动态结合SSwish（对称Swish变体）和GELU，平衡模型表达能力和梯度稳定性。

Result: 实验表明，SG-Blend在多种任务和模型上表现优于现有基线，计算开销可忽略。

Conclusion: SG-Blend是一种通用的、即插即用的激活函数，显著提升了模型性能。

Abstract: The design of activation functions remains a pivotal component in optimizing
deep neural networks. While prevailing choices like Swish and GELU demonstrate
considerable efficacy, they often exhibit domain-specific optima. This work
introduces SG-Blend, a novel activation function that blends our proposed
SSwish, a first-order symmetric variant of Swish and the established GELU
through dynamic interpolation. By adaptively blending these constituent
functions via learnable parameters, SG-Blend aims to harness their
complementary strengths: SSwish's controlled non-monotonicity and symmetry, and
GELU's smooth, probabilistic profile, to achieve a more universally robust
balance between model expressivity and gradient stability. We conduct
comprehensive empirical evaluations across diverse modalities and
architectures, showing performance improvements across all considered natural
language and computer vision tasks and models. These results, achieved with
negligible computational overhead, underscore SG-Blend's potential as a
versatile, drop-in replacement that consistently outperforms strong
contemporary baselines. The code is available at
https://anonymous.4open.science/r/SGBlend-6CBC.

</details>


### [185] [Position: The Future of Bayesian Prediction Is Prior-Fitted](https://arxiv.org/abs/2505.23947)
*Samuel Müller,Arik Reuter,Noah Hollmann,David Rügamer,Frank Hutter*

Key words: Prior-data Fitted Networks, 贝叶斯推理, 预训练, 数据稀缺, 摊销学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Prior-data Fitted Networks (PFNs)利用随机生成的人工数据集训练神经网络，捕捉由数据集生成分布定义的先验，适用于计算资源丰富但真实数据稀缺的场景。

Motivation: 在计算资源快速增长而真实数据生成停滞的背景下，PFNs能高效利用预训练计算资源解决数据稀缺问题，扩展了贝叶斯推理的应用范围。

Method: 通过随机生成人工数据集训练神经网络，捕获数据生成分布的先验，应用于低数据量场景。

Result: PFNs从小规模贝叶斯建模扩展到更复杂领域和大数据集，展示了其在数据稀缺问题中的潜力。

Conclusion: PFNs和摊销推理方法代表贝叶斯推理的未来，值得进一步研究以解决当前限制。

Abstract: Training neural networks on randomly generated artificial datasets yields
Bayesian models that capture the prior defined by the dataset-generating
distribution. Prior-data Fitted Networks (PFNs) are a class of methods designed
to leverage this insight. In an era of rapidly increasing computational
resources for pre-training and a near stagnation in the generation of new
real-world data in many applications, PFNs are poised to play a more important
role across a wide range of applications. They enable the efficient allocation
of pre-training compute to low-data scenarios. Originally applied to small
Bayesian modeling tasks, the field of PFNs has significantly expanded to
address more complex domains and larger datasets. This position paper argues
that PFNs and other amortized inference approaches represent the future of
Bayesian inference, leveraging amortized learning to tackle data-scarce
problems. We thus believe they are a fruitful area of research. In this
position paper, we explore their potential and directions to address their
current limitations.

</details>


### [186] [TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks](https://arxiv.org/abs/2505.23949)
*Xiang Meng,Mehdi Makni,Rahul Mazumder*

Key words: 网络剪枝, N:M稀疏, 可转置掩码, 最优传输, GPU并行

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种可扩展的高效求解器，用于生成可转置的N:M稀疏掩码，通过最优传输问题和GPU并行实现，显著提升了大型模型的训练效率。

Motivation: 现有N:M稀疏方法在训练时效率受限，因为稀疏模式在矩阵转置时无法保持。需要一种可扩展且高效的方法来生成可转置的稀疏掩码。

Method: 将掩码生成建模为最优传输问题，通过熵正则化和Dykstra算法求解，并结合GPU并行实现的张量操作。

Result: 方法实现了100倍加速且误差仅为1-10%，在LLaMA3.2-8B模型上保持性能接近标准N:M稀疏模型。

Conclusion: 提出的方法可扩展至大型模型，显著提升了训练效率，并保持了模型性能。

Abstract: Network pruning reduces the computational requirements of large neural
networks, with N:M sparsity -- retaining only N out of every M consecutive
weights -- offering a compelling balance between compressed model quality and
hardware acceleration. However, N:M sparsity only accelerates forward-pass
computations, as N:M patterns are not preserved during matrix transposition,
limiting efficiency during training where both passes are computationally
intensive. While transposable N:M sparsity has been proposed to address this
limitation, existing methods for finding transposable N:M sparse masks either
fail to scale to large models or are restricted to M=4 which results in
suboptimal compression-accuracy trade-off. We introduce an efficient solver for
transposable N:M masks that scales to billion-parameter models. We formulate
mask generation as optimal transport problems and solve through entropy
regularization and Dykstra's algorithm, followed by a rounding procedure. Our
tensor-based implementation exploits GPU parallelism, achieving up to 100x
speedup with only 1-10% error compared to existing methods. Our approach can be
integrated with layer-wise N:M pruning frameworks including Wanda, SparseGPT
and ALPS to produce transposable N:M sparse models with arbitrary N:M values.
Experiments show that LLaMA3.2-8B with transposable 16:32 sparsity maintains
performance close to its standard N:M counterpart and outperforms standard 2:4
sparse model, showing the practical value of our approach.

</details>


### [187] [Estimating Misreporting in the Presence of Genuine Modification: A Causal Perspective](https://arxiv.org/abs/2505.23954)
*Dylan Zapzalka,Trenton Chang,Lindsay Warrenburg,Sae-Hwan Park,Daniel K. Shenfeld,Ravi B. Parikh,Jenna Wiens,Maggie Makar*

Key words: 策略性行为、误报、因果推断、资源分配、机器学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种因果驱动的方法，用于识别和量化代理人在资源分配模型中如何策略性地误报特征，而不是进行真实的修改。通过比较操纵和非操纵数据集中的因果效应，方法能够区分误报与真实修改。理论结果在半合成和真实Medicare数据集上得到验证。

Motivation: 在资源分配决策中使用ML模型时，代理人可能会策略性地改变特征以获取更好的结果。现有研究难以区分误报和真实修改，因此需要一种方法来识别和量化误报行为。

Method: 提出了一种因果驱动的方法，通过比较操纵和非操纵数据集中特征对其因果后代的影响差异，识别误报行为。理论证明了误报率的可识别性，并分析了估计器的方差。

Result: 在半合成和真实Medicare数据集上的实证验证表明，该方法能够有效识别真实世界中的误报行为。

Conclusion: 该方法为解决资源分配模型中代理人的策略性误报问题提供了理论框架和实用工具。

Abstract: In settings where ML models are used to inform the allocation of resources,
agents affected by the allocation decisions might have an incentive to
strategically change their features to secure better outcomes. While prior work
has studied strategic responses broadly, disentangling misreporting from
genuine modification remains a fundamental challenge. In this paper, we propose
a causally-motivated approach to identify and quantify how much an agent
misreports on average by distinguishing deceptive changes in their features
from genuine modification. Our key insight is that, unlike genuine
modification, misreported features do not causally affect downstream variables
(i.e., causal descendants). We exploit this asymmetry by comparing the causal
effect of misreported features on their causal descendants as derived from
manipulated datasets against those from unmanipulated datasets. We formally
prove identifiability of the misreporting rate and characterize the variance of
our estimator. We empirically validate our theoretical results using a
semi-synthetic and real Medicare dataset with misreported data, demonstrating
that our approach can be employed to identify misreporting in real-world
scenarios.

</details>


### [188] [Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation](https://arxiv.org/abs/2505.23960)
*Henry Conklin*

Key words: 神经网络、表示空间、信息论、熵估计、泛化机制

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了量化方法来描述大规模神经网络的表示空间结构，研究其学习过程、泛化机制及设计决策对结构的影响，并开发了高效熵估计方法。

Motivation: 当前缺乏描述神经网络表示空间的统一方法，以及分析其结构和泛化机制的工具。

Method: 引入结构性原语和信息论量化方法，分析多智能体强化学习、序列模型及大语言模型的表示结构，并提出高效向量空间熵估计技术。

Result: 揭示了大规模分布式认知模型的学习机制，发现语言结构与神经网络性能驱动结构的相似性。

Conclusion: 提出的方法为理解神经网络表示空间及其与人类认知系统的关联提供了新视角。

Abstract: Despite the remarkable success of large large-scale neural networks, we still
lack unified notation for thinking about and describing their representational
spaces. We lack methods to reliably describe how their representations are
structured, how that structure emerges over training, and what kinds of
structures are desirable. This thesis introduces quantitative methods for
identifying systematic structure in a mapping between spaces, and leverages
them to understand how deep-learning models learn to represent information,
what representational structures drive generalisation, and how design decisions
condition the structures that emerge. To do this I identify structural
primitives present in a mapping, along with information theoretic
quantifications of each. These allow us to analyse learning, structure, and
generalisation across multi-agent reinforcement learning models,
sequence-to-sequence models trained on a single task, and Large Language
Models. I also introduce a novel, performant, approach to estimating the
entropy of vector space, that allows this analysis to be applied to models
ranging in size from 1 million to 12 billion parameters.
  The experiments here work to shed light on how large-scale distributed models
of cognition learn, while allowing us to draw parallels between those systems
and their human analogs. They show how the structures of language and the
constraints that give rise to them in many ways parallel the kinds of
structures that drive performance of contemporary neural networks.

</details>


### [189] [Improved Approximations for Hard Graph Problems using Predictions](https://arxiv.org/abs/2505.23967)
*Anders Aamand,Justin Y. Chen,Siddharth Gollapudi,Sandeep Silwal,Hao Wu*

Key words: 图算法, 近似算法, 预测模型, NP难问题

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于边预测的改进图算法，通过结合预测信息突破传统近似算法的性能限制，应用于MaxCut、Vertex Cover等问题。

Motivation: 传统NP难图问题的近似算法性能有限，通过利用预测信息提升近似比。

Method: 扩展ε-预测框架，利用边缘提供的两位预测信息，分别处理高、低度顶点后合并结果。

Result: 在MaxCut、Vertex Cover等问题上实现了优于传统方法的近似比。

Conclusion: 预测信息能有效突破近似算法瓶颈，且方法具有通用性。

Abstract: We design improved approximation algorithms for NP-hard graph problems by
incorporating predictions (e.g., learned from past data). Our prediction model
builds upon and extends the $\varepsilon$-prediction framework by Cohen-Addad,
d'Orsi, Gupta, Lee, and Panigrahi (NeurIPS 2024). We consider an edge-based
version of this model, where each edge provides two bits of information,
corresponding to predictions about whether each of its endpoints belong to an
optimal solution. Even with weak predictions where each bit is only
$\varepsilon$-correlated with the true solution, this information allows us to
break approximation barriers in the standard setting. We develop algorithms
with improved approximation ratios for MaxCut, Vertex Cover, Set Cover, and
Maximum Independent Set problems (among others). Across these problems, our
algorithms share a unifying theme, where we separately satisfy constraints
related to high degree vertices (using predictions) and low-degree vertices
(without using predictions) and carefully combine the answers.

</details>


### [190] [Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training](https://arxiv.org/abs/2505.23971)
*William Merrill,Shane Arora,Dirk Groeneveld,Hannaneh Hajishirzi*

Key words: 临界批次大小, 语言模型, 梯度噪声, 批次大小预热, 数据并行

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种直接测量临界批次大小(CBS)的简单方法，并展示了CBS在训练过程中的变化，建议通过批次大小预热来高效训练大规模语言模型。

Motivation: 研究临界批次大小(CBS)的可靠测量方法，以解决当前基于梯度噪声尺度的方法因强假设而受限的问题。

Method: 通过直接测量CBS并观察其随训练的变化，提出批次大小预热策略，从小批次开始逐步增加。

Result: 实验表明CBS从初始化时的接近零快速上升后趋于稳定，且在不同规模模型间具有一致性，批次大小预热能节省43%的梯度步骤。

Conclusion: 直接测量CBS的方法结合批次大小预热策略，能可靠地在大批次下训练语言模型而不牺牲性能。

Abstract: The right batch size is important when training language models at scale: a
large batch size is necessary for fast training, but a batch size that is too
large will harm token efficiency. To navigate this tradeoff, McCandlish et al.
(2018) suggest that a critical batch size (CBS), below which training will not
substantially degrade loss, can be estimated based on the gradient noise scale
during training. While their method has been adopted in practice, e.g., when
training GPT-3, strong assumptions are required to justify gradient noise as a
proxy for the CBS, which makes it unclear whether their approach should be
trusted in practice, limiting its applicability. In this paper, we introduce a
simple, empirical approach to directly measure the CBS and show how the CBS
evolves over training. Applying our approach to the OLMo models, we find that
CBS is near 0 at initialization, increases rapidly at first, and then plateaus
as training progresses. Furthermore, we find that this trend holds across
different model sizes (1B and 7B), suggesting CBS from small training runs can
inform larger-scale training runs. Our findings about how the CBS changes over
training motivate batch size warmup as a natural way to reliably train language
models at large batch size: start the batch size small and increase it as the
CBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to
slightly better loss than the original training run with 43% fewer gradient
steps. This shows how our framework can be applied to reliably train language
models at larger batch sizes, increasing data parallelism without compromising
performance.

</details>


### [191] [Adaptive Deadline and Batch Layered Synchronized Federated Learning](https://arxiv.org/abs/2505.23973)
*Asaf Goren,Natalie Lang,Nir Shlezinger,Alejandro Cohen*

Key words: 联邦学习,延迟优化,异构设备,批大小调整

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: ADEL-FL提出了一种联合优化每轮截止时间和用户特定批大小的框架，以解决同步联邦学习中的延迟瓶颈问题，并在异构条件下展现更优的收敛速度和最终精度。

Motivation: 同步联邦学习存在因设备异构性导致的延迟问题，现有解决方案（如固定截止时间或客户端选择）未考虑动态调整，限制了其效果。

Method: ADEL-FL通过联合优化每轮截止时间和用户特定批大小，形成约束优化问题，最小化L2距离至全局最优解。

Result: 实验证明ADEL-FL在异构条件下优于其他方法，收敛速度和最终精度更高。

Conclusion: ADEL-FL是解决联邦学习中延迟问题的有效方法，尤其适用于异构设备环境。

Abstract: Federated learning (FL) enables collaborative model training across
distributed edge devices while preserving data privacy, and typically operates
in a round-based synchronous manner. However, synchronous FL suffers from
latency bottlenecks due to device heterogeneity, where slower clients
(stragglers) delay or degrade global updates. Prior solutions, such as fixed
deadlines, client selection, and layer-wise partial aggregation, alleviate the
effect of stragglers, but treat round timing and local workload as static
parameters, limiting their effectiveness under strict time constraints. We
propose ADEL-FL, a novel framework that jointly optimizes per-round deadlines
and user-specific batch sizes for layer-wise aggregation. Our approach
formulates a constrained optimization problem minimizing the expected L2
distance to the global optimum under total training time and global rounds. We
provide a convergence analysis under exponential compute models and prove that
ADEL-FL yields unbiased updates with bounded variance. Extensive experiments
demonstrate that ADEL-FL outperforms alternative methods in both convergence
rate and final accuracy under heterogeneous conditions.

</details>


### [192] [Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization](https://arxiv.org/abs/2505.23987)
*Vishal Dey,Xiao Hu,Xia Ning*

Key words: 分子优化, 多属性优化, 指令调整, 大语言模型, 药物设计

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究提出了首个专注于多重属性优化任务的指令调整数据集C-MuMOInstruct，并开发了GeLLMO-Cs模型，在多样化的优化任务中显著优于基线方法。

Motivation: 现有分子优化方法及指令调整大语言模型难以满足药物设计中多属性优化的复杂需求，研究者希望通过构建专门的数据集和模型来解决这一问题。

Method: 利用C-MuMOInstruct数据集开发GeLLMO-Cs系列模型，通过指令调整实现针对特定属性的分子优化。

Result: GeLLMO-Cs在10项任务中表现优于基线模型，成功率提升达126%，并展示了出色的零样本泛化能力。

Conclusion: GeLLMO-Cs为药物设计中多属性优化提供了实用化的解决方案，推动了面向特定目标的优化技术的发展。

Abstract: In real-world drug design, molecule optimization requires selectively
improving multiple molecular properties up to pharmaceutically relevant levels,
while maintaining others that already meet such criteria. However, existing
computational approaches and instruction-tuned LLMs fail to capture such
nuanced property-specific objectives, limiting their practical applicability.
To address this, we introduce C-MuMOInstruct, the first instruction-tuning
dataset focused on multi-property optimization with explicit, property-specific
objectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of
instruction-tuned LLMs that can perform targeted property-specific
optimization. Our experiments across 5 in-distribution and 5
out-of-distribution tasks show that GeLLMO-Cs consistently outperform strong
baselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit
impressive 0-shot generalization to novel optimization tasks and unseen
instructions. This offers a step toward a foundational LLM to support
realistic, diverse optimizations with property-specific objectives.
C-MuMOInstruct and code are accessible through
https://github.com/ninglab/GeLLMO-C.

</details>


### [193] [Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting](https://arxiv.org/abs/2505.24003)
*ChengAo Shen,Wenchao Yu,Ziming Zhao,Dongjin Song,Wei Cheng,Haifeng Chen,Jingchao Ni*

Key words: 时间序列预测、多模态视图、分解方法、长期预测、大型视觉模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于分解的多模态视图框架DMMV，通过趋势-季节性分解和新的基于残差的自适应分解方法，结合多模态视图进行长期时间序列预测，优于现有的14种先进模型。

Motivation: 时间序列通常以数值序列表示，但可以转化为图像和文本等多模态视图，从而揭示互补模式并利用预训练的大型模型（如大型视觉模型）进行长期预测。然而，直接应用LVMs会引入对'预测周期'的归纳偏差。为解决这一问题，作者提出了DMMV框架。

Method: DMMV框架结合了趋势-季节性分解和基于后残差的自适应分解方法，利用多模态视图（图像和文本）进行预测。

Result: 在8个基准数据集中的6个上，DMMV实现了最佳均方误差（MSE），优于14种现有先进模型。

Conclusion: DMMV通过多模态视图的整合和分解技术的创新，显著提升了长期时间序列预测的性能。

Abstract: Time series, typically represented as numerical sequences, can also be
transformed into images and texts, offering multi-modal views (MMVs) of the
same underlying signal. These MMVs can reveal complementary patterns and enable
the use of powerful pre-trained large models, such as large vision models
(LVMs), for long-term time series forecasting (LTSF). However, as we identified
in this work, applying LVMs to LTSF poses an inductive bias towards
"forecasting periods". To harness this bias, we propose DMMV, a novel
decomposition-based multi-modal view framework that leverages trend-seasonal
decomposition and a novel backcast residual based adaptive decomposition to
integrate MMVs for LTSF. Comparative evaluations against 14 state-of-the-art
(SOTA) models across diverse datasets show that DMMV outperforms single-view
and existing multi-modal baselines, achieving the best mean squared error (MSE)
on 6 out of 8 benchmark datasets.

</details>


### [194] [How far away are truly hyperparameter-free learning algorithms?](https://arxiv.org/abs/2505.24005)
*Priya Kasimbeg,Vincent Roulet,Naman Agarwal,Sourabh Medapati,Fabian Pedregosa,Atish Agarwala,George E. Dahl*

Key words: 超参数调优, 无学习率方法, AlgoPerf基准, 神经网络训练, NadamW

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文探讨了无学习率方法在超参数调优中的潜力，发现现有默认设置性能不佳，但校准后有所提升，但仍稍逊于基准方法。

Motivation: 深度神经网络需要大量超参数调优，但目前尚无通用默认设置能适应多种任务。本文旨在评估无学习率方法是否能减少调优需求。

Method: 通过AlgoPerf基准测试，冻结非学习率超参数，评估无学习率方法的性能，并搜索跨任务表现良好的配置。

Result: 默认设置表现差，但校准后的无学习率方法有所改善，但仍略低于NadamW基准。

Conclusion: 无学习率方法仍有改进空间，需在强基准下测试以推动超参数减少技术的发展。

Abstract: Despite major advances in methodology, hyperparameter tuning remains a
crucial (and expensive) part of the development of machine learning systems.
Even ignoring architectural choices, deep neural networks have a large number
of optimization and regularization hyperparameters that need to be tuned
carefully per workload in order to obtain the best results. In a perfect world,
training algorithms would not require workload-specific hyperparameter tuning,
but would instead have default settings that performed well across many
workloads. Recently, there has been a growing literature on optimization
methods which attempt to reduce the number of hyperparameters -- particularly
the learning rate and its accompanying schedule. Given these developments, how
far away is the dream of neural network training algorithms that completely
obviate the need for painful tuning?
  In this paper, we evaluate the potential of learning-rate-free methods as
components of hyperparameter-free methods. We freeze their (non-learning rate)
hyperparameters to default values, and score their performance using the
recently-proposed AlgoPerf: Training Algorithms benchmark. We found that
literature-supplied default settings performed poorly on the benchmark, so we
performed a search for hyperparameter configurations that performed well across
all workloads simultaneously. The best AlgoPerf-calibrated learning-rate-free
methods had much improved performance but still lagged slightly behind a
similarly calibrated NadamW baseline in overall benchmark score. Our results
suggest that there is still much room for improvement for learning-rate-free
methods, and that testing against a strong, workload-agnostic baseline is
important to improve hyperparameter reduction techniques.

</details>


### [195] [The Rich and the Simple: On the Implicit Bias of Adam and SGD](https://arxiv.org/abs/2505.24022)
*Bhavya Vasudeva,Jung Whan Lee,Vatsal Sharan,Mahdi Soltanolkotabi*

Key words: Adam优化器，随机梯度下降，隐式偏差，神经网络，分类任务

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 摘要探讨了Adam与GD在训练两ReLU网络时的隐式偏差差异，发现GD偏向简单解但Adam生成更丰富的特征边界，更接近贝叶斯最优。

Motivation: 实践中，SGD训练的神经网络倾向于简单解，而Adam则展现了对这种偏向的抵抗力。论文旨在揭示两者在隐式偏差上的差异。

Method: 研究者在合成高斯聚类数据的分类任务中分析Adam和GD的行为，通过理论分析人口梯度并提出实证结果。

Result: Adam产生更非线性、更接近贝叶斯最优的决策边界，提高测试精度且更适应分布偏移；GD则生成简单但次优边界。

Conclusion: Adam的隐式偏差使其在生成复杂决策边界和适应数据变化方面优于GD，展现了更优的泛化能力。

Abstract: Adam is the de facto optimization algorithm for several deep learning
applications, but an understanding of its implicit bias and how it differs from
other algorithms, particularly standard first-order methods such as
(stochastic) gradient descent (GD), remains limited. In practice, neural
networks trained with SGD are known to exhibit simplicity bias -- a tendency to
find simple solutions. In contrast, we show that Adam is more resistant to such
simplicity bias. To demystify this phenomenon, in this paper, we investigate
the differences in the implicit biases of Adam and GD when training two-layer
ReLU neural networks on a binary classification task involving synthetic data
with Gaussian clusters. We find that GD exhibits a simplicity bias, resulting
in a linear decision boundary with a suboptimal margin, whereas Adam leads to
much richer and more diverse features, producing a nonlinear boundary that is
closer to the Bayes' optimal predictor. This richer decision boundary also
allows Adam to achieve higher test accuracy both in-distribution and under
certain distribution shifts. We theoretically prove these results by analyzing
the population gradients. To corroborate our theoretical findings, we present
empirical results showing that this property of Adam leads to superior
generalization across datasets with spurious correlations where neural networks
trained with SGD are known to show simplicity bias and don't generalize well
under certain distributional shifts.

</details>


### [196] [From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?](https://arxiv.org/abs/2505.24030)
*Ziming Zhao,ChengAo Shen,Hanghang Tong,Dongjin Song,Zhigang Deng,Qingsong Wen,Jingchao Ni*

Key words: 时间序列分析, 大型视觉模型, 多模态, 分类任务, 预测任务

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文首次系统地研究了大型视觉模型（LVMs）在时间序列分析中的应用，发现LVMs在分类任务中有效，但在预测任务中存在局限性。

Motivation: 随着Transformer和大型语言模型（LLMs）在时间序列中的争议，研究大型视觉模型（LVMs）是否对时间序列分析有用成为了一个新问题。

Method: 设计了涵盖4个LVMs、8种成像方法、18个数据集和26个基线的全面实验，覆盖分类和预测任务，并进行详细的消融分析。

Result: LVMs在时间序列分类中表现良好，但在预测任务中存在局限性，如仅适用于特定类型的LVMs和成像方法，且对长回溯窗口的利用有限。

Conclusion: LVMs在时间序列分析中有潜力，但需进一步研究以克服预测任务的不足，为多模态解决方案奠定基础。

Abstract: Transformer-based models have gained increasing attention in time series
research, driving interest in Large Language Models (LLMs) and foundation
models for time series analysis. As the field moves toward multi-modality,
Large Vision Models (LVMs) are emerging as a promising direction. In the past,
the effectiveness of Transformer and LLMs in time series has been debated. When
it comes to LVMs, a similar question arises: are LVMs truely useful for time
series analysis? To address it, we design and conduct the first principled
study involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across
both high-level (classification) and low-level (forecasting) tasks, with
extensive ablation analysis. Our findings indicate LVMs are indeed useful for
time series classification but face challenges in forecasting. Although
effective, the contemporary best LVM forecasters are limited to specific types
of LVMs and imaging methods, exhibit a bias toward forecasting periods, and
have limited ability to utilize long look-back windows. We hope our findings
could serve as a cornerstone for future research on LVM- and multimodal-based
solutions to different time series tasks.

</details>


### [197] [LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Trainin](https://arxiv.org/abs/2505.24034)
*Bo Wu,Sid Wang,Yunhao Tang,Jia Ding,Eryk Helenowski,Liang Tan,Tengyu Xu,Tushar Gowda,Zhengxing Chen,Chen Zhu,Xiaocheng Tang,Yundi Qian,Beibei Zhu,Rui Hou*

Key words: 强化学习, 大规模语言模型, 分布式训练, 异步设计, PyTorch

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: LlamaRL是一个完全分布式、异步的强化学习框架，专门用于高效训练大规模语言模型，在不同规模的GPU集群上表现出色，速度提升显著。

Motivation: 针对大规模语言模型（LLMs）在强化学习（RL）训练中面临的高延迟和内存需求问题，开发一个高效的RL框架以支持数百亿至数千亿参数的模型。

Method: LlamaRL采用基于PyTorch的单一控制器架构，支持模块化和易用性，并通过异步设计、模型卸载、异步训练等技术提升效率。

Result: LlamaRL在405B参数模型上实现了10.7倍的训练速度提升，且效率优势随模型规模扩大而增加。

Conclusion: LlamaRL是一个适用于未来大规模RL训练的高效框架，具有显著的速度优势和可扩展性。

Abstract: Reinforcement Learning (RL) has become the most effective post-training
approach for improving the capabilities of Large Language Models (LLMs). In
practice, because of the high demands on latency and memory, it is particularly
challenging to develop an efficient RL framework that reliably manages policy
models with hundreds to thousands of billions of parameters.
  In this paper, we present LlamaRL, a fully distributed, asynchronous RL
framework optimized for efficient training of large-scale LLMs with various
model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a
handful to thousands of devices. LlamaRL introduces a streamlined,
single-controller architecture built entirely on native PyTorch, enabling
modularity, ease of use, and seamless scalability to thousands of GPUs. We also
provide a theoretical analysis of LlamaRL's efficiency, including a formal
proof that its asynchronous design leads to strict RL speed-up. Empirically, by
leveraging best practices such as colocated model offloading, asynchronous
off-policy training, and distributed direct memory access for weight
synchronization, LlamaRL achieves significant efficiency gains -- up to 10.7x
speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy
model. Furthermore, the efficiency advantage continues to grow with increasing
model scale, demonstrating the framework's suitability for future large-scale
RL training.

</details>


### [198] [NeuronTune: Towards Self-Guided Spurious Bias Mitigation](https://arxiv.org/abs/2505.24048)
*Guangtao Zheng,Wenqian Ye,Aidong Zhang*

Key words: 虚假偏差,自指导,神经网络,NeuronTune,模型鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: NeuronTune是一个无需外部标注、通过模型内部嵌入空间干预的自指导方法，能有效减少深度神经网络的虚假偏差。

Motivation: 深度神经网络常因非本质特征与类别的虚假关联而产生偏差，现有方法依赖难以获取的外部标注，限制了实用性。

Method: 提出NeuronTune方法，在模型的潜在嵌入空间中识别并调控导致虚假预测行为的神经元。

Result: 实验证明，NeuronTune能显著自指导地减少不同架构和数据模态中的虚假偏差。

Conclusion: NeuronTune是一种无需标注、高效且通用的虚假偏差缓解工具。

Abstract: Deep neural networks often develop spurious bias, reliance on correlations
between non-essential features and classes for predictions. For example, a
model may identify objects based on frequently co-occurring backgrounds rather
than intrinsic features, resulting in degraded performance on data lacking
these correlations. Existing mitigation approaches typically depend on external
annotations of spurious correlations, which may be difficult to obtain and are
not relevant to the spurious bias in a model. In this paper, we take a step
towards self-guided mitigation of spurious bias by proposing NeuronTune, a post
hoc method that directly intervenes in a model's internal decision process. Our
method probes in a model's latent embedding space to identify and regulate
neurons that lead to spurious prediction behaviors. We theoretically justify
our approach and show that it brings the model closer to an unbiased one.
Unlike previous methods, NeuronTune operates without requiring spurious
correlation annotations, making it a practical and effective tool for improving
model robustness. Experiments across different architectures and data
modalities demonstrate that our method significantly mitigates spurious bias in
a self-guided way.

</details>


### [199] [Differential Gated Self-Attention](https://arxiv.org/abs/2505.24054)
*Elpiniki Maria Lygizou,Mónika Farsang,Radu Grosu*

Key words: Transformer, 自注意力, 侧抑制, 噪声鲁棒性, 差分门控

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为多头差分门控自注意力（M-DGSA）的新方法，通过动态抑制注意力噪声提升Transformer模型对噪声输入的鲁棒性。其在视觉和语言任务中均表现出优于现有模型的性能。

Motivation: 标准自注意力机制对所有查询-键交互一视同仁，易受噪声输入影响。受生物神经电路中侧抑制机制的启发，作者提出了一种新的自注意力方法以增强噪声鲁棒性。

Method: 提出多头差分门控自注意力（M-DGSA），每个头分为兴奋性和抑制性分支，通过输入依赖的Sigmoid门动态融合分支的双Softmax映射，实现上下文感知的对比增强。

Result: 在视觉和语言基准测试中，M-DGSA表现优于标准Transformer、Vision Transformer和差分Transformer基线，展现了对噪声的鲁棒性和跨领域适应性。

Conclusion: M-DGSA通过结合侧抑制机制和自注意力理论，显著提升了Transformer模型的噪声鲁棒性，且计算开销小，可无缝集成到现有架构中。

Abstract: Transformers excel across a large variety of tasks but remain susceptible to
corrupted inputs, since standard self-attention treats all query-key
interactions uniformly. Inspired by lateral inhibition in biological neural
circuits and building on the recent use by the Differential Transformer's use
of two parallel softmax subtraction for noise cancellation, we propose
Multihead Differential Gated Self-Attention (M-DGSA) that learns per-head
input-dependent gating to dynamically suppress attention noise. Each head
splits into excitatory and inhibitory branches whose dual softmax maps are
fused by a sigmoid gate predicted from the token embedding, yielding a
context-aware contrast enhancement. M-DGSA integrates seamlessly into existing
Transformer stacks with minimal computational overhead. We evaluate on both
vision and language benchmarks, demonstrating consistent robustness gains over
vanilla Transformer, Vision Transformer, and Differential Transformer
baselines. Our contributions are (i) a novel input-dependent gating mechanism
for self-attention grounded in lateral inhibition, (ii) a principled synthesis
of biological contrast-enhancement and self-attention theory, and (iii)
comprehensive experiments demonstrating noise resilience and cross-domain
applicability.

</details>


### [200] [Bridging Source and Target Domains via Link Prediction for Unsupervised Domain Adaptation on Graphs](https://arxiv.org/abs/2505.24055)
*Yilong Wang,Tianxiang Zhao,Zongyu Wu,Suhang Wang*

Key words: 图神经网络, 无监督域适应, 链接预测, 身份保护学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种利用链接预测和身份保护学习目标的新型框架，以减少源图和目标图之间的分布差异，提升图神经网络的域适应能力。

Motivation: 现有无监督域适应方法未充分考虑图结构和消息传递机制，且对标签分布偏移敏感，因此需要一种更有效的方法来适应图数据。

Method: 通过链接预测连接源图和目标图的节点，并设计身份保护学习目标以保留目标图的判别信息。

Result: 在真实数据集上的实验验证了该框架的有效性。

Conclusion: 该框架成功减少了分布偏移并提升了域适应性能，同时对标签分布不均衡不敏感。

Abstract: Graph neural networks (GNNs) have shown great ability for node classification
on graphs. However, the success of GNNs relies on abundant labeled data, while
obtaining high-quality labels is costly and challenging, especially for newly
emerging domains. Hence, unsupervised domain adaptation (UDA), which trains a
classifier on the labeled source graph and adapts it to the unlabeled target
graph, is attracting increasing attention. Various approaches have been
proposed to alleviate the distribution shift between the source and target
graphs to facilitate the classifier adaptation. However, most of them simply
adopt existing UDA techniques developed for independent and identically
distributed data to gain domain-invariant node embeddings for graphs, which do
not fully consider the graph structure and message-passing mechanism of GNNs
during the adaptation and will fail when label distribution shift exists among
domains. In this paper, we proposed a novel framework that adopts link
prediction to connect nodes between source and target graphs, which can
facilitate message-passing between the source and target graphs and augment the
target nodes to have ``in-distribution'' neighborhoods with the source domain.
This strategy modified the target graph on the input level to reduce its
deviation from the source domain in the embedding space and is insensitive to
disproportional label distributions across domains. To prevent the loss of
discriminative information in the target graph, we further design a novel
identity-preserving learning objective, which guides the learning of the edge
insertion module together with reconstruction and adaptation losses.
Experimental results on real-world datasets demonstrate the effectiveness of
our framework.

</details>


### [201] [Towards disentangling the contributions of articulation and acoustics in multimodal phoneme recognition](https://arxiv.org/abs/2505.24059)
*Sean Foley,Hong Nguyen,Jihwan Lee,Sudarsana Reddy Kadiri,Dani Byrd,Louis Goldstein,Shrikanth Narayanan*

Key words: 多模态学习, 单人MRI数据, 音素识别, 声学与发音关系

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文使用单人MRI数据开发单模态和多模态模型，以研究语音识别中不同模态的贡献。

Motivation: 以往多模态学习依赖于多说话者语料库，但跨说话者差异限制了模型学习声学与发音关系的能力。本研究旨在通过单人MRI数据解决这一问题，并分析各模态的作用。

Method: 使用单人长时MRI语料库开发单模态（音频和视频）和多模态模型，进行音素识别任务。

Result: 音频和多模态模型在不同发音方式的音素上表现相似，但在发音位置上存在差异。模型潜在空间的编码显示相似性，而注意力权重揭示了某些音素的声学和发音时间差异。

Conclusion: 单人MRI数据有助于更精细地分析声学与发音关系，多模态模型能揭示模态间的互补性。

Abstract: Although many previous studies have carried out multimodal learning with
real-time MRI data that captures the audio-visual kinematics of the vocal tract
during speech, these studies have been limited by their reliance on
multi-speaker corpora. This prevents such models from learning a detailed
relationship between acoustics and articulation due to considerable
cross-speaker variability. In this study, we develop unimodal audio and video
models as well as multimodal models for phoneme recognition using a long-form
single-speaker MRI corpus, with the goal of disentangling and interpreting the
contributions of each modality. Audio and multimodal models show similar
performance on different phonetic manner classes but diverge on places of
articulation. Interpretation of the models' latent space shows similar encoding
of the phonetic space across audio and multimodal models, while the models'
attention weights highlight differences in acoustic and articulatory timing for
certain phonemes.

</details>


### [202] [Characterising the Inductive Biases of Neural Networks on Boolean Data](https://arxiv.org/abs/2505.24060)
*Chris Mingard,Lukas Seier,Niclas Göring,Andrei-Vlad Badelita,Charles London,Ard Louis*

Key words: 深度神经网络, 归纳偏置, 特征学习, 泛化能力, DNF公式

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文通过深度2离散全连接网络与DNF公式的一对一对应关系，研究了深度神经网络的归纳偏置、特征学习与泛化能力之间的关系。

Motivation: 现有研究对深度神经网络的泛化能力提供了部分解释（如NTK理论忽略特征学习），本文旨在提供一个端到端的分析框架，完整揭示网络的归纳偏置、训练动态与泛化能力之间的联系。

Method: 利用深度2离散全连接网络与DNF公式的对应关系，在布尔函数上训练网络，并使用蒙特卡洛学习算法，分析训练动态和特征形成过程。

Result: 模型展示了可预测的训练动态和可解释的特征涌现，揭示了归纳偏置和特征形成如何驱动泛化能力。

Conclusion: 本文提出了一种可分析的框架，详细追踪了深度网络的归纳偏置和特征学习如何影响其泛化性能。

Abstract: Deep neural networks are renowned for their ability to generalise well across
diverse tasks, even when heavily overparameterized. Existing works offer only
partial explanations (for example, the NTK-based task-model alignment
explanation neglects feature learning). Here, we provide an end-to-end,
analytically tractable case study that links a network's inductive prior, its
training dynamics including feature learning, and its eventual generalisation.
Specifically, we exploit the one-to-one correspondence between depth-2 discrete
fully connected networks and disjunctive normal form (DNF) formulas by training
on Boolean functions. Under a Monte Carlo learning algorithm, our model
exhibits predictable training dynamics and the emergence of interpretable
features. This framework allows us to trace, in detail, how inductive bias and
feature formation drive generalisation.

</details>


### [203] [Measure gradients, not activations! Enhancing neuronal activity in deep reinforcement learning](https://arxiv.org/abs/2505.24061)
*Jiashun Liu,Zihao Wu,Johan Obando-Ceron,Pablo Samuel Castro,Aaron Courville,Ling Pan*

Key words: 深度强化学习,神经元活动丧失,梯度幅度,GraMa,ReGraMa

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了GraMa（梯度幅度神经活动度量），一种轻量级、架构无关的度量方法，用于量化神经元级学习能力，以解决深度强化学习（RL）中神经元活动丧失的问题。通过将统计目标从激活转向梯度，GraMa能够有效检测神经元不活跃问题，并提出基于GraMa的神经元重置方法（ReGraMa），显著提升了多种深度RL算法的性能。

Motivation: 深度强化学习（RL）代理经常面临神经元活动丧失的问题，尤其是在复杂架构中，传统的tau-dormant神经元比率方法失效，因此需要一种新的方法来量化神经元的实际学习能力而非其表达能力。

Method: 论文提出GraMa方法，通过分析神经元的梯度幅度而非激活统计来量化其学习能力。进一步推出了基于GraMa的ReGraMa方法，用于按需重置不活跃神经元。

Result: GraMa在不同架构（如残差网络、扩散模型等）中均能有效检测神经元不活跃问题。ReGraMa方法在多个深度RL算法和基准测试（如MuJoCo和DeepMind Control Suite）中显著提升了性能。

Conclusion: 论文表明，关注神经元的学习能力（而非表达能力）是解决RL代理适应性问题的关键，GraMa和ReGraMa为解决这一问题提供了有效的工具。

Abstract: Deep reinforcement learning (RL) agents frequently suffer from neuronal
activity loss, which impairs their ability to adapt to new data and learn
continually. A common method to quantify and address this issue is the
tau-dormant neuron ratio, which uses activation statistics to measure the
expressive ability of neurons. While effective for simple MLP-based agents,
this approach loses statistical power in more complex architectures. To address
this, we argue that in advanced RL agents, maintaining a neuron's learning
capacity, its ability to adapt via gradient updates, is more critical than
preserving its expressive ability. Based on this insight, we shift the
statistical objective from activations to gradients, and introduce GraMa
(Gradient Magnitude Neural Activity Metric), a lightweight,
architecture-agnostic metric for quantifying neuron-level learning capacity. We
show that GraMa effectively reveals persistent neuron inactivity across diverse
architectures, including residual networks, diffusion models, and agents with
varied activation functions. Moreover, resetting neurons guided by GraMa
(ReGraMa) consistently improves learning performance across multiple deep RL
algorithms and benchmarks, such as MuJoCo and the DeepMind Control Suite.

</details>


### [204] [Primal-Dual Neural Algorithmic Reasoning](https://arxiv.org/abs/2505.24067)
*Yu He,Ellen Vitercik*

Key words: Neural Algorithmic Reasoning, primal-dual, 图神经网络, 近似算法, 泛化性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于primal-dual范式的NAR框架，通过双向表示和GNN结合增强算法模拟能力，并在多项任务中超越传统近似算法，展示了强大的泛化能力与实际应用价值。

Motivation: 当前NAR研究主要集中在多项式时间可解问题的精确算法模拟，针对更复杂问题的扩展仍具挑战性，本文旨在填补这一空白。

Method: 利用primal-dual范式的双向变量表示与图神经网络对齐，结合小规模实例的最优解提升模型推理能力。

Result: 模型不仅成功模拟且超越传统近似算法，对大规模及分布外数据展现强泛化性，并与商业求解器集成验证了实用性。

Conclusion: 该框架为NAR在复杂问题中的应用提供了新思路，兼具理论创新与实际部署潜力。

Abstract: Neural Algorithmic Reasoning (NAR) trains neural networks to simulate
classical algorithms, enabling structured and interpretable reasoning over
complex data. While prior research has predominantly focused on learning exact
algorithms for polynomial-time-solvable problems, extending NAR to harder
problems remains an open challenge. In this work, we introduce a general NAR
framework grounded in the primal-dual paradigm, a classical method for
designing efficient approximation algorithms. By leveraging a bipartite
representation between primal and dual variables, we establish an alignment
between primal-dual algorithms and Graph Neural Networks. Furthermore, we
incorporate optimal solutions from small instances to greatly enhance the
model's reasoning capabilities. Our empirical results demonstrate that our
model not only simulates but also outperforms approximation algorithms for
multiple tasks, exhibiting robust generalization to larger and
out-of-distribution graphs. Moreover, we highlight the framework's practical
utility by integrating it with commercial solvers and applying it to real-world
datasets.

</details>


### [205] [DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via Data Structures](https://arxiv.org/abs/2505.24069)
*Yu He,Yingxi Li,Colin White,Ellen Vitercik*

Key words: large language models, structural reasoning, data structures, DSR-Bench, evaluation

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: DSR-Bench是一个评估大型语言模型结构化推理能力的新基准，包含20种数据结构和4140个问题实例。研究发现，现有模型在多属性和复杂结构推理上表现不足。

Motivation: 现有的LLM评估基准主要关注应用层面，缺乏对结构化推理能力的系统评估，而这是实际任务中的核心需求。

Method: 提出了DSR-Bench基准，包含多种数据结构和操作，采用自动化评估流程，避免了主观判断。

Result: 指令调优模型在多属性和多跳推理上表现不佳，即使优化后的模型在复杂结构上也仅达到47%的平均得分。

Conclusion: 模型在多维数据和自然语言任务描述上的表现不足，限制了其在现实场景中的部署。

Abstract: Large language models (LLMs) are increasingly deployed for real-world tasks
that fundamentally involve data manipulation. A core requirement across these
tasks is the ability to perform structural reasoning--that is, to understand
and reason about data relationships. For example, customer requests require a
temporal ordering, which can be represented by data structures such as queues.
However, existing benchmarks primarily focus on high-level, application-driven
evaluations without isolating this fundamental capability. To address this gap,
we introduce DSR-Bench, a novel benchmark evaluating LLMs' structural reasoning
capabilities through data structures, which provide interpretable
representations of data relationships. DSR-Bench includes 20 data structures,
35 operations, and 4,140 problem instances, organized hierarchically for
fine-grained analysis of reasoning limitations. Our evaluation pipeline is
fully automated and deterministic, eliminating subjective human or model-based
judgments. Its synthetic nature also ensures scalability and minimizes data
contamination risks. We benchmark nine state-of-the-art LLMs. Our analysis
shows that instruction-tuned models struggle with basic multi-attribute and
multi-hop reasoning. Furthermore, while reasoning-oriented models perform
better, they remain fragile on complex and hybrid structures, with the best
model achieving an average score of only 47% on the challenge subset.
Crucially, models often perform poorly on multi-dimensional data and natural
language task descriptions, highlighting a critical gap for real-world
deployment.

</details>


### [206] [DeepBoost-AF: A Novel Unsupervised Feature Learning and Gradient Boosting Fusion for Robust Atrial Fibrillation Detection in Raw ECG Signals](https://arxiv.org/abs/2505.24085)
*Alireza Jafari,Fereshteh Yousefirizi,Vahid Seydi*

Key words: 心房颤动,深度学习,梯度提升,自编码器,自动检测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 这篇论文提出了一种结合无监督深度学习和梯度提升模型的创新混合方法，用于提高心房颤动（AF）的检测效果，其中DCAE-LGBM模型表现最佳。

Motivation: 心房颤动是一种常见的心律失常，及时检测对降低中风风险至关重要。传统方法依赖手动特征提取，限制了准确性和效率。

Method: 采用19层深度卷积自编码器（DCAE）与三种梯度提升分类器（AdaBoost、XGBoost和LightGBM）结合，实现了端到端的AF检测。

Result: DCAE-LGBM模型的F1-score为95.20%，灵敏度高达99.99%，推断延迟仅4秒，优于现有方法。

Conclusion: 混合系统显著提升了AF检测性能，具备临床部署的潜力。

Abstract: Atrial fibrillation (AF) is a prevalent cardiac arrhythmia associated with
elevated health risks, where timely detection is pivotal for mitigating
stroke-related morbidity. This study introduces an innovative hybrid
methodology integrating unsupervised deep learning and gradient boosting models
to improve AF detection. A 19-layer deep convolutional autoencoder (DCAE) is
coupled with three boosting classifiers-AdaBoost, XGBoost, and LightGBM
(LGBM)-to harness their complementary advantages while addressing individual
limitations. The proposed framework uniquely combines DCAE with gradient
boosting, enabling end-to-end AF identification devoid of manual feature
extraction. The DCAE-LGBM model attains an F1-score of 95.20%, sensitivity of
99.99%, and inference latency of four seconds, outperforming existing methods
and aligning with clinical deployment requirements. The DCAE integration
significantly enhances boosting models, positioning this hybrid system as a
reliable tool for automated AF detection in clinical settings.

</details>


### [207] [Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting](https://arxiv.org/abs/2505.24088)
*Chen Huang,Skyler Seto,Hadi Pouransari,Mehrdad Farajtabar,Raviteja Vemulapalli,Fartash Faghri,Oncel Tuzel,Barry-John Theobald,Josh Susskind*

Key words: 视觉基础模型, 特征分布对齐, 概念遗忘, 微调, 最近邻图

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为Proxy-FDA的新方法，通过特征分布对齐（FDA）和动态生成的代理来减少视觉基础模型在微调过程中的概念遗忘问题，并在多个任务和设置中验证了其有效性。

Motivation: 为了解决视觉基础模型在微调过程中对其他任务的概念遗忘问题，论文旨在保留预训练模型的特征空间结构知识，同时不影响微调性能。

Method: 提出了Proxy-FDA方法，通过特征分布对齐（使用最近邻图）在预训练和微调特征空间之间保留结构知识，并通过动态生成的代理增加数据多样性以改进对齐效果。

Result: 实验表明，Proxy-FDA显著减少了微调过程中的概念遗忘，并发现遗忘与分布距离度量（相比L2距离）有强相关性。该方法在多种微调设置和任务中均表现良好。

Conclusion: Proxy-FDA有效减少了概念遗忘，尤其在保持特征空间结构知识方面表现优异，适用于多种视觉任务和微调场景。

Abstract: Vision foundation models pre-trained on massive data encode rich
representations of real-world concepts, which can be adapted to downstream
tasks by fine-tuning. However, fine-tuning foundation models on one task often
leads to the issue of concept forgetting on other tasks. Recent methods of
robust fine-tuning aim to mitigate forgetting of prior knowledge without
affecting the fine-tuning performance. Knowledge is often preserved by matching
the original and fine-tuned model weights or feature pairs. However, such
point-wise matching can be too strong, without explicit awareness of the
feature neighborhood structures that encode rich knowledge as well. We propose
a novel regularization method Proxy-FDA that explicitly preserves the
structural knowledge in feature space. Proxy-FDA performs Feature Distribution
Alignment (using nearest neighbor graphs) between the pre-trained and
fine-tuned feature spaces, and the alignment is further improved by informative
proxies that are generated dynamically to increase data diversity. Experiments
show that Proxy-FDA significantly reduces concept forgetting during
fine-tuning, and we find a strong correlation between forgetting and a
distributional distance metric (in comparison to L2 distance). We further
demonstrate Proxy-FDA's benefits in various fine-tuning settings (end-to-end,
few-shot and continual tuning) and across different tasks like image
classification, captioning and VQA.

</details>


### [208] [Practical Bayes-Optimal Membership Inference Attacks](https://arxiv.org/abs/2505.24089)
*Marcus Lassila,Johan Östman,Khac-Hoang Ngo,Alexandre Graell i Amat*

Key words: 成员推理攻击, 贝叶斯最优, 图神经网络, 计算效率, 节点级攻击

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了针对独立同分布数据和图结构数据的贝叶斯最优成员推理攻击（MIA）方法，包括BASE和G-BASE两种高效近似算法，并在图数据和非图数据上验证了其优越性能和低计算成本。

Motivation: 现有成员推理攻击方法在最优查询策略和计算效率上存在不足，尤其是在图数据上的节点级攻击表现不佳。本研究旨在填补这一空白。

Method: 基于贝叶斯决策理论框架，推导了图神经网络中节点级MIA的最优推理规则，并提出了计算高效的BASE和G-BASE近似方法。

Result: G-BASE在图数据上超越现有方法，BASE在非图数据上性能匹配或优于LiRA和RMIA，且计算成本显著降低。同时证明了BASE与RMIA在特定超参数下的等价性。

Conclusion: 研究为成员推理攻击提供了理论支撑和高效工具，尤其在图数据场景中表现突出。

Abstract: We develop practical and theoretically grounded membership inference attacks
(MIAs) against both independent and identically distributed (i.i.d.) data and
graph-structured data. Building on the Bayesian decision-theoretic framework of
Sablayrolles et al., we derive the Bayes-optimal membership inference rule for
node-level MIAs against graph neural networks, addressing key open questions
about optimal query strategies in the graph setting. We introduce BASE and
G-BASE, computationally efficient approximations of the Bayes-optimal attack.
G-BASE achieves superior performance compared to previously proposed
classifier-based node-level MIA attacks. BASE, which is also applicable to
non-graph data, matches or exceeds the performance of prior state-of-the-art
MIAs, such as LiRA and RMIA, at a significantly lower computational cost.
Finally, we show that BASE and RMIA are equivalent under a specific
hyperparameter setting, providing a principled, Bayes-optimal justification for
the RMIA attack.

</details>


### [209] [A SHAP-based explainable multi-level stacking ensemble learning method for predicting the length of stay in acute stroke](https://arxiv.org/abs/2505.24101)
*Zhenran Xu*

Key words: length of stay prediction, acute stroke, machine learning, interpretable ensemble model, SHAP analysis

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文通过改进预测因子和开发可解释的多层次堆叠集成模型，提高了急性脑卒中住院时间（LOS）预测的效率、性能和可解释性，针对缺血性和出血性脑卒中分别建模，结果显示集成模型在缺血性脑卒中中表现优异，但在出血性脑卒中中未显著优于逻辑回归。

Motivation: 现有机器学习模型在急性脑卒中住院时间预测中表现不佳，泛化能力有限且忽略了系统级因素，需要改进模型效率、性能和可解释性。

Method: 使用澳大利亚脑卒中基金会急性审计数据集，通过特征选择和相关性分析提炼关键预测因子，构建可解释的多层次堆叠集成模型，并评估其区分度（AUC）、校准曲线和可解释性（SHAP图）。

Result: 集成模型在缺血性脑卒中中AUC为0.824，显著优于逻辑回归（P=0.0004），但在出血性脑卒中中未显著优于逻辑回归（P=0.136）。SHAP分析识别了两类脑卒中共享的关键预测因子。

Conclusion: 可解释的集成模型能有效预测缺血性脑卒中的长期住院时间，但对出血性脑卒中需在更大队列中进一步验证。

Abstract: Length of stay (LOS) prediction in acute stroke is critical for improving
care planning. Existing machine learning models have shown suboptimal
predictive performance, limited generalisability, and have overlooked
system-level factors. We aimed to enhance model efficiency, performance, and
interpretability by refining predictors and developing an interpretable
multi-level stacking ensemble model. Data were accessed from the biennial
Stroke Foundation Acute Audit (2015, 2017, 2019, 2021) in Australia. Models
were developed for ischaemic and haemorrhagic stroke separately. The outcome
was prolonged LOS (the LOS above the 75th percentile). Candidate predictors
(ischaemic: n=89; haemorrhagic: n=83) were categorised into patient, clinical,
and system domains. Feature selection with correlation-based approaches was
used to refine key predictors. The evaluation of models included discrimination
(AUC), calibration curves, and interpretability (SHAP plots). In ischaemic
stroke (N=12,575), prolonged LOS was >=9 days, compared to >=11 days in
haemorrhagic stroke (N=1,970). The ensemble model achieved superior performance
[AUC: 0.824 (95% CI: 0.801-0.846)] and statistically outperformed logistic
regression [AUC: 0.805 (95% CI: 0.782-0.829); P=0.0004] for ischaemic. However,
the model [AUC: 0.843 (95% CI: 0.790-0.895)] did not statistically outperform
logistic regression [AUC: 0.828 (95% CI: 0.774-0.882); P=0.136] for
haemorrhagic. SHAP analysis identified shared predictors for both types of
stroke: rehabilitation assessment, urinary incontinence, stroke unit care,
inability to walk independently, physiotherapy, and stroke care coordinators
involvement. An explainable ensemble model effectively predicted the prolonged
LOS in ischaemic stroke. Further validation in larger cohorts is needed for
haemorrhagic stroke.

</details>


### [210] [Neural Networks as Universal Finite-State Machines: A Constructive ReLU Simulation Framework for NFAs](https://arxiv.org/abs/2505.24110)
*Sahil Rajesh Dhayalkar*

Key words: 非确定性有限自动机(NFA), ReLU神经网络, 正则语言, 符号模拟, 梯度下降

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一个框架，证明非确定性有限自动机（NFAs）与标准前馈ReLU神经网络之间的等价性。通过将自动机状态编码为二进制向量、转移编码为稀疏线性层，表明ReLU激活可以精确模拟非确定性分支、子集构造和$ε$-闭包。理论结果表明一个三层的ReLU网络（宽度$Ξ(n)$）可以精确识别由$n$态NFA接受的正则语言，而无需循环、记忆或近似。实验验证了与真实自动机的完美匹配。

Motivation: 结合自动机理论和神经计算，展示ReLU神经网络如何完全精确地模拟NFA的行为，从而建立两者的等价性。

Method: 通过二进制向量编码状态和稀疏线性层编码转移，利用ReLU激活模拟NFA的非确定性分支、子集构造和$ε$-闭包。构建三层ReLU网络（宽度$Ξ(n)$）并理论证明其等价性。

Result: 实验验证了网络在并行路径跟踪、子集构造、$ε$-闭包收敛等任务中与NFA的完美对齐，表明梯度下降仍能保持语义和接受行为。

Conclusion: 这是首个在标准深度学习架构内完全符号化模拟NFA的研究，通过ReLU动力学统一了自动机理论和神经计算。

Abstract: We present a formal and constructive framework establishing the equivalence
between nondeterministic finite automata (NFAs) and standard feedforward ReLU
neural networks. By encoding automaton states as binary vectors and transitions
as sparse linear layers, we show that ReLU activations simulate
nondeterministic branching, subset construction, and $\epsilon$-closures in a
mathematically precise manner. Our core theoretical results prove that a
three-layer ReLU network of width $\mathcal{O}(n)$ can exactly recognize any
regular language accepted by an $n$-state NFA-without recurrence, memory, or
approximation. Furthermore, we show that gradient descent over
structure-preserving networks preserves symbolic semantics and acceptance
behavior. Extensive experiments across multiple validation tasks-including
parallel path tracking, symbolic subset construction, $\epsilon$-closure
convergence, acceptance classification, structural training invariants, and
functional equivalence-achieve perfect or near-perfect empirical alignment with
ground-truth automata. This work provides the first provably complete symbolic
simulation of NFAs within standard deep learning architectures, uniting
automata theory with neural computation through ReLU dynamics.

</details>


### [211] [AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits](https://arxiv.org/abs/2505.24138)
*Yichen Shi,Ze Zhang,Hongyang Wang,Zhuofu Tao,Zhongyi Li,Bingyu Chen,Yaxin Wang,Zhiping Yu,Ting-Jung Lin,Lei He*

Key words: AMS电路、多模态大语言模型、电路设计、基准测试、自动化设计

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: AMSbench是一个新的基准测试套件，用于评估多模态大语言模型（MLLMs）在模拟/混合信号（AMS）电路设计中的性能。

Motivation: 自动化AMS电路设计一直是一个挑战，现有研究缺乏系统性评估MLLMs在该领域的表现。

Method: 开发AMSbench基准套件，包含8000个测试问题，覆盖电路原理图感知、分析和设计等任务。

Result: 当前MLLMs在复杂多模态推理和电路设计任务中存在显著局限。

Conclusion: 需要提升MLLMs的电路知识理解和应用能力，缩小与人类专家的差距。

Abstract: Analog/Mixed-Signal (AMS) circuits play a critical role in the integrated
circuit (IC) industry. However, automating Analog/Mixed-Signal (AMS) circuit
design has remained a longstanding challenge due to its difficulty and
complexity. Recent advances in Multi-modal Large Language Models (MLLMs) offer
promising potential for supporting AMS circuit analysis and design. However,
current research typically evaluates MLLMs on isolated tasks within the domain,
lacking a comprehensive benchmark that systematically assesses model
capabilities across diverse AMS-related challenges. To address this gap, we
introduce AMSbench, a benchmark suite designed to evaluate MLLM performance
across critical tasks including circuit schematic perception, circuit analysis,
and circuit design. AMSbench comprises approximately 8000 test questions
spanning multiple difficulty levels and assesses eight prominent models,
encompassing both open-source and proprietary solutions such as Qwen 2.5-VL and
Gemini 2.5 Pro. Our evaluation highlights significant limitations in current
MLLMs, particularly in complex multi-modal reasoning and sophisticated circuit
design tasks. These results underscore the necessity of advancing MLLMs'
understanding and effective application of circuit-specific knowledge, thereby
narrowing the existing performance gap relative to human expertise and moving
toward fully automated AMS circuit design workflows. Our data is released at
https://huggingface.co/datasets/wwhhyy/AMSBench

</details>


### [212] [Autoregressive regularized score-based diffusion models for multi-scenarios fluid flow prediction](https://arxiv.org/abs/2505.24145)
*Wilfried Genuist,Éric Savin,Filippo Gatti,Didier Clouteau*

Key words: 条件评分扩散模型，湍流预测，科学机器学习，生成建模，流体动力学

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种用于多场景流体流动预测的条件评分扩散模型，结合了湍流统计特性的能量约束，优化预测质量，支持高效采样。该方法架构简单通用，无需特定设计，并通过实验验证了其在复杂流体动力学数据集上的稳定与鲁棒性。

Motivation: 基于科学机器学习和生成建模的最新进展，旨在解决多场景流体流动预测的挑战，提升预测质量和效率。

Method: 提出条件评分扩散模型，整合湍流统计特性的能量约束，支持高效采样和灵活训练，无需模型重新设计。

Result: 模型在复杂流体动力学数据集中表现稳定、鲁棒且物性准确，尤其在湍流条件下仍保持高质量预测。

Conclusion: 该方法在多场景流体动力学任务中表现优异，为科学机器学习提供了高效且通用的解决方案。

Abstract: Building on recent advances in scientific machine learning and generative
modeling for computational fluid dynamics, we propose a conditional score-based
diffusion model designed for multi-scenarios fluid flow prediction. Our model
integrates an energy constraint rooted in the statistical properties of
turbulent flows, improving prediction quality with minimal training, while
enabling efficient sampling at low cost. The method features a simple and
general architecture that requires no problem-specific design, supports
plug-and-play enhancements, and enables fast and flexible solution generation.
It also demonstrates an efficient conditioning mechanism that simplifies
training across different scenarios without demanding a redesign of existing
models. We further explore various stochastic differential equation
formulations to demonstrate how thoughtful design choices enhance performance.
We validate the proposed methodology through extensive experiments on complex
fluid dynamics datasets encompassing a variety of flow regimes and
configurations. Results demonstrate that our model consistently achieves
stable, robust, and physically faithful predictions, even under challenging
turbulent conditions. With properly tuned parameters, it achieves accurate
results across multiple scenarios while preserving key physical and statistical
properties. We present a comprehensive analysis of stochastic differential
equation impact and discuss our approach across diverse fluid mechanics tasks.

</details>


### [213] [RCCDA: Adaptive Model Updates in the Presence of Concept Drift under a Constrained Resource Budget](https://arxiv.org/abs/2505.24149)
*Adam Piaseczny,Md Kamran Chowdhury Shisher,Shiqiang Wang,Christopher G. Brinton*

Key words: machine learning, concept drift, resource constraints, dynamic update, Lyapunov framework

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 针对资源受限环境下机器学习模型的概念漂移问题，提出了一种动态更新策略（RCCDA），通过分析损失演变和Lyapunov框架，确保资源限制的同时提升性能。

Motivation: 现实环境中部署的机器学习模型需应对概念漂移，同时受限于严格资源约束。现有方法常因计算开销大或缺乏理论保证而不足。

Method: 提出RCCDA策略，利用历史损失信息和可调漂移阈值，结合Lyapunov漂移加惩罚框架，动态优化模型更新。

Result: 在三个领域泛化数据集上，RCCDA在推理准确性和资源约束遵守方面优于基线方法。

Conclusion: RCCDA适合实时机器学习部署，能在资源受限和概念漂移下保持高性能。

Abstract: Machine learning (ML) algorithms deployed in real-world environments are
often faced with the challenge of adapting models to concept drift, where the
task data distributions are shifting over time. The problem becomes even more
difficult when model performance must be maintained under adherence to strict
resource constraints. Existing solutions often depend on drift-detection
methods that produce high computational overhead for resource-constrained
environments, and fail to provide strict guarantees on resource usage or
theoretical performance assurances. To address these shortcomings, we propose
RCCDA: a dynamic model update policy that optimizes ML training dynamics while
ensuring strict compliance to predefined resource constraints, utilizing only
past loss information and a tunable drift threshold. In developing our policy,
we analytically characterize the evolution of model loss under concept drift
with arbitrary training update decisions. Integrating these results into a
Lyapunov drift-plus-penalty framework produces a lightweight policy based on a
measurable accumulated loss threshold that provably limits update frequency and
cost. Experimental results on three domain generalization datasets demonstrate
that our policy outperforms baseline methods in inference accuracy while
adhering to strict resource constraints under several schedules of concept
drift, making our solution uniquely suited for real-time ML deployments.

</details>


### [214] [Biological Pathway Guided Gene Selection Through Collaborative Reinforcement Learning](https://arxiv.org/abs/2505.24155)
*Ehtesamul Azim,Dongjie Wang,Tae Hyun Hwang,Yanjie Fu,Wei Zhang*

Key words: 基因选择, 多智能体强化学习, 生物通路, 图神经网络, 基因组学

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种结合多智能体强化学习(MARL)和通路知识的两阶段框架，用于高维基因组数据中的基因选择，旨在提升预测准确性和生物学解释性。

Motivation: 传统基因选择方法忽视生物通路和调控网络的复杂性，导致结果不稳定且缺乏生物学意义，亟需一种能整合通路知识并保持统计严谨性的新方法。

Method: 1. 通路引导的预过滤策略结合KEGG通路信息和多种统计方法降维；2. 基于MARL的精细选择框架，通过GNN状态表示、结合预测性能与通路覆盖的奖励机制，以及共享记忆和中心化批评器的协作学习。

Result: 在多个基因表达数据集上的实验表明，该方法在预测准确性和生物学解释性上显著优于传统方法。

Conclusion: 通过整合统计方法与生物通路知识，MARL框架有效解决了基因选择中的挑战，为疾病机制研究和治疗优化提供了新方向。

Abstract: Gene selection in high-dimensional genomic data is essential for
understanding disease mechanisms and improving therapeutic outcomes.
Traditional feature selection methods effectively identify predictive genes but
often ignore complex biological pathways and regulatory networks, leading to
unstable and biologically irrelevant signatures. Prior approaches, such as
Lasso-based methods and statistical filtering, either focus solely on
individual gene-outcome associations or fail to capture pathway-level
interactions, presenting a key challenge: how to integrate biological pathway
knowledge while maintaining statistical rigor in gene selection? To address
this gap, we propose a novel two-stage framework that integrates statistical
selection with biological pathway knowledge using multi-agent reinforcement
learning (MARL). First, we introduce a pathway-guided pre-filtering strategy
that leverages multiple statistical methods alongside KEGG pathway information
for initial dimensionality reduction. Next, for refined selection, we model
genes as collaborative agents in a MARL framework, where each agent optimizes
both predictive power and biological relevance. Our framework incorporates
pathway knowledge through Graph Neural Network-based state representations, a
reward mechanism combining prediction performance with gene centrality and
pathway coverage, and collaborative learning strategies using shared memory and
a centralized critic component. Extensive experiments on multiple gene
expression datasets demonstrate that our approach significantly improves both
prediction accuracy and biological interpretability compared to traditional
methods.

</details>


### [215] [Don't Just Follow MLLM Plans: Robust and Efficient Planning for Open-world Agents](https://arxiv.org/abs/2505.24157)
*Seungjoon Lee,Suhwan Kim,Minhyeon Oh,Youngsik Yoon,Jungseul Ok*

Key words: 自主代理, 大语言模型, 规划学习, 开放世界, 高效学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出REPOA框架，旨在解决自主代理在复杂、不可预测环境中高效学习规划的挑战，通过自适应依赖学习、细粒度失败感知操作记忆和难度探索提升效果。

Motivation: 现有方法依赖有问题的内部知识或不切实际的环境假设，且大部分忽视从零开始学习规划的能力。

Method: REPOA框架包含自适应依赖学习、细粒度失败感知操作记忆和难度探索三个关键组件。

Result: 在开放世界测试环境中，REPOA成功获取了先前方法无法实现的挑战性后期物品。

Conclusion: REPOA展示了在知识不准确情况下高效学习的潜力，适用于复杂开放环境。

Abstract: Developing autonomous agents capable of mastering complex, multi-step tasks
in unpredictable, interactive environments presents a significant challenge.
While Large Language Models (LLMs) offer promise for planning, existing
approaches often rely on problematic internal knowledge or make unrealistic
environmental assumptions. Although recent work explores learning planning
knowledge, they still retain limitations due to partial reliance on external
knowledge or impractical setups. Indeed, prior research has largely overlooked
developing agents capable of acquiring planning knowledge from scratch,
directly in realistic settings. While realizing this capability is necessary,
it presents significant challenges, primarily achieving robustness given the
substantial risk of incorporating LLMs' inaccurate knowledge. Moreover,
efficiency is crucial for practicality as learning can demand prohibitive
exploration. In response, we introduce Robust and Efficient Planning for
Open-world Agents (REPOA), a novel framework designed to tackle these issues.
REPOA features three key components: adaptive dependency learning and
fine-grained failure-aware operation memory to enhance robustness to knowledge
inaccuracies, and difficulty-based exploration to improve learning efficiency.
Our evaluation in two established open-world testbeds demonstrates REPOA's
robust and efficient planning, showcasing its capability to successfully obtain
challenging late-game items that were beyond the reach of prior approaches.

</details>


### [216] [Invariant Link Selector for Spatial-Temporal Out-of-Distribution Problem](https://arxiv.org/abs/2505.24178)
*Katherine Tieu,Dongqi Fu,Jun Wu,Jingrui He*

Key words: Out-of-Distribution（OOD），时间图，信息瓶颈（IB），不变学习，推荐系统

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种基于信息瓶颈方法的误差边界不变链接选择器（Invariant Link Selector），用于在训练过程中区分时间图中的不变组件和可变组件，以提高深度学习模型在不同测试场景中的泛化能力。

Motivation: 在基础模型时代，训练环境和测试环境之间的数据差异（即Out-of-Distribution问题）阻碍了AI的泛化能力。尤其是时间关联的图数据不满足独立同分布（IID）条件，使问题更具挑战性。

Method: 研究人员利用信息瓶颈（IB）方法，提出了一个误差边界不变链接选择器，用于在训练过程中识别时间图中的不变组件和可变组件，并结合任务特定的损失函数（如时间链接预测）提升模型在实际应用任务中的表现。

Result: 实验表明，该方法在引用推荐和商品推荐等实际应用中优于现有最先进（SOTA）方法。

Conclusion: 通过识别不变组件并优化训练过程，该方法能够显著提升模型在复杂时间图数据中的泛化能力。

Abstract: In the era of foundation models, Out-of- Distribution (OOD) problems, i.e.,
the data discrepancy between the training environments and testing
environments, hinder AI generalization. Further, relational data like graphs
disobeying the Independent and Identically Distributed (IID) condition makes
the problem more challenging, especially much harder when it is associated with
time. Motivated by this, to realize the robust invariant learning over temporal
graphs, we want to investigate what components in temporal graphs are most
invariant and representative with respect to labels. With the Information
Bottleneck (IB) method, we propose an error-bounded Invariant Link Selector
that can distinguish invariant components and variant components during the
training process to make the deep learning model generalizable for different
testing scenarios. Besides deriving a series of rigorous generalizable
optimization functions, we also equip the training with task-specific loss
functions, e.g., temporal link prediction, to make pretrained models solve
real-world application tasks like citation recommendation and merchandise
recommendation, as demonstrated in our experiments with state-of-the-art (SOTA)
methods. Our code is available at https://github.com/kthrn22/OOD-Linker.

</details>


### [217] [SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling](https://arxiv.org/abs/2505.24179)
*Xiaodong Ji,Hailin Zhang,Fangcheng Fu,Bin Cui*

Key words: 长上下文处理,稀疏注意力,量化,LLM,效率优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: SALE是一种细粒度稀疏注意力方法，通过在LLM的填充阶段使用4位量化的查询-键乘积和块稀疏注意力，加速长上下文处理并保持模型精度。

Motivation: 解决LLM长上下文处理中自注意力模块的计算瓶颈，避免现有稀疏注意力方法因粗粒度检查导致的精度损失。

Method: 采用4位量化查询-键乘积和块稀疏注意力，并提出相对注意力分数指标评估重要性，实现高效计算。

Result: 在Llama-3.1-8B上，对64K以上序列实现至少3.36倍加速，同时保持模型质量。

Conclusion: SALE无需参数训练，可轻松集成到现有系统，在精度和效率上优于现有方法。

Abstract: Many advanced Large Language Model (LLM) applications require long-context
processing, but the self-attention module becomes a bottleneck during the
prefilling stage of inference due to its quadratic time complexity with respect
to sequence length. Existing sparse attention methods accelerate attention
computation by skipping less significant regions of the attention map. However,
these approaches typically perform coarse-grained inspection of the attention
map, rendering considerable loss in model accuracy. In this paper, we propose
SALE, a fine-grained sparse attention method that accelerates the long-context
prefilling stage of LLM with negligible loss in model accuracy. SALE achieves
fast and accurate fine-grained attention weight estimation through 4-bit
quantized query-key products, followed by block-sparse attention to accelerate
prefilling computations. For importance evaluation for query-key pairs, we
adopt our Relative Attention Score metric, which offers significantly higher
efficiency within our framework. We implement a custom CUDA kernel optimized
for our approach for hardware efficiency, reducing the additional overhead to
approximately 11% of the full attention latency. Notably, SALE requires no
parameter training and can be seamlessly integrated into existing systems with
trivial code modifications. Experiments on long-context benchmarks demonstrate
that our method outperforms existing approaches in accuracy-efficiency
trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences
longer than 64K while maintaining model quality.

</details>


### [218] [CodeV-R1: Reasoning-Enhanced Verilog Generation](https://arxiv.org/abs/2505.24183)
*Yaoyu Zhu,Di Huang,Hanqi Lyu,Xiaoyun Zhang,Chongxiao Li,Wenxuan Shi,Yutong Wu,Jianan Mu,Jinghua Wang,Yang Zhao,Pengwei Jin,Shuyao Cheng,Shengwen Liang,Xishan Zhang,Rui Zhang,Zidong Du,Qi Guo,Xing Hu,Yunji Chen*

Key words: 大语言模型, 强化学习, Verilog, 电子设计自动化, 数据合成

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一个名为CodeV-R1的强化学习框架，用于训练生成Verilog代码的大型语言模型（LLM）。通过自动生成测试平台、高质量数据集合成以及两阶段训练流程，模型在Verilog生成任务上表现优异，超越了现有技术。

Motivation: 为了解决在电子设计自动化（EDA）领域自动生成Verilog代码的三大挑战（缺乏自动化验证环境、高质量数据稀缺和计算成本高），论文提出了CodeV-R1框架。

Method: 采用了基于规则的测试平台生成器、往返数据合成方法以及两阶段的'蒸馏后强化学习'训练流程（包括自适应DAPO算法）。

Result: CodeV-R1-7B在VerilogEval v2和RTLLM v1.1上分别达到68.6%和72.9%的pass@1，比现有技术提升了12~20%，并匹配或超越了671B的DeepSeek-R1。

Conclusion: CodeV-R1框架在Verilog生成任务上表现出色，同时其模型、训练流程和数据集将被公开发布以推动EDA和LLM研究。

Abstract: Large language models (LLMs) trained via reinforcement learning with
verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,
automatable verification, such as software programming and mathematical
problems. Extending RLVR to electronic design automation (EDA), especially
automatically generating hardware description languages (HDLs) like Verilog
from natural-language (NL) specifications, however, poses three key challenges:
the lack of automated and accurate verification environments, the scarcity of
high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To
this end, we introduce CodeV-R1, an RLVR framework for training Verilog
generation LLMs. First, we develop a rule-based testbench generator that
performs robust equivalence checking against golden references. Second, we
propose a round-trip data synthesis method that pairs open-source Verilog
snippets with LLM-generated NL descriptions, verifies code-NL-code consistency
via the generated testbench, and filters out inequivalent examples to yield a
high-quality dataset. Third, we employ a two-stage "distill-then-RL" training
pipeline: distillation for the cold start of reasoning abilities, followed by
adaptive DAPO, our novel RLVR algorithm that can reduce training cost by
adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves
68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,
surpassing prior state-of-the-art by 12~20%, while matching or even exceeding
the performance of 671B DeepSeek-R1. We will release our model, training
pipeline, and dataset to facilitate research in EDA and LLM communities.

</details>


### [219] [Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling](https://arxiv.org/abs/2505.24185)
*Yipan Wei,Yuchen Zou,Yapeng Li,Bo Du*

Key words: 联邦学习, 多任务学习, 聚合方法, 隐私保护

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: FedDEA是一种联邦多任务学习方法，通过动态识别任务相关维度并重新缩放，有效抑制跨任务干扰，实现了无任务标签依赖的全局模型聚合。

Motivation: 现有联邦多任务学习方法难以聚合多个异构任务到一个统一的模型中，导致在任务目标、标签空间和优化路径差异大的场景下效果不佳。

Method: FedDEA通过响应强度动态识别任务相关维度，并重新缩放更新结构，实现任务级解耦聚合。

Result: 实验表明，FedDEA能轻松整合到主流联邦优化算法中，在NYUD-V2和PASCAL-Context上显著提升性能。

Conclusion: FedDEA在高度异构任务设置下具有鲁棒性和泛化能力。

Abstract: Federated Multi-Task Learning (FMTL) enables multiple clients performing
heterogeneous tasks without exchanging their local data, offering broad
potential for privacy preserving multi-task collaboration. However, most
existing methods focus on building personalized models for each client and
unable to support the aggregation of multiple heterogeneous tasks into a
unified model. As a result, in real-world scenarios where task objectives,
label spaces, and optimization paths vary significantly, conventional FMTL
methods struggle to achieve effective joint training. To address this
challenge, we propose FedDEA (Federated Decoupled Aggregation), an
update-structure-aware aggregation method specifically designed for multi-task
model integration. Our method dynamically identifies task-relevant dimensions
based on the response strength of local updates and enhances their optimization
effectiveness through rescaling. This mechanism effectively suppresses
cross-task interference and enables task-level decoupled aggregation within a
unified global model. FedDEA does not rely on task labels or architectural
modifications, making it broadly applicable and deployment-friendly.
Experimental results demonstrate that it can be easily integrated into various
mainstream federated optimization algorithms and consistently delivers
significant overall performance improvements on widely used NYUD-V2 and
PASCAL-Context. These results validate the robustness and generalization
capabilities of FedDEA under highly heterogeneous task settings.

</details>


### [220] [Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows](https://arxiv.org/abs/2505.24189)
*Orlando Marquez Ayala,Patrice Bechard,Emily Chen,Maggie Baird,Jingfei Chen*

Key words: Large Language Models, Small Language Models, fine-tuning, structured outputs, low-code workflows

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了在特定领域任务中，小型语言模型（SLMs）微调相比大型语言模型（LLMs）直接提示的质量优势，尤其是生成JSON格式的低代码工作流任务，微调能平均提升10%的质量。

Motivation: 随着大模型（如GPT-4）token成本降低，微调小模型（SLMs）的优势（如推理速度、成本）可能不再明显。但本文旨在证明，对于需要结构化输出的特定领域任务，SLMs仍具质量优势。

Method: 研究比较了微调SLM与直接提示LLM在生成JSON格式低代码工作流任务上的表现，并通过系统误差分析评估模型局限。

Result: 实验表明，尽管优化提示能获得合理结果，但微调SLM平均能提升10%的输出质量。

Conclusion: 对于结构化输出任务，SLMs的微调仍具有不可替代的质量优势，尤其是在领域特定场景中。

Abstract: Large Language Models (LLMs) such as GPT-4o can handle a wide range of
complex tasks with the right prompt. As per token costs are reduced, the
advantages of fine-tuning Small Language Models (SLMs) for real-world
applications -- faster inference, lower costs -- may no longer be clear. In
this work, we present evidence that, for domain-specific tasks that require
structured outputs, SLMs still have a quality advantage. We compare fine-tuning
an SLM against prompting LLMs on the task of generating low-code workflows in
JSON form. We observe that while a good prompt can yield reasonable results,
fine-tuning improves quality by 10% on average. We also perform systematic
error analysis to reveal model limitations.

</details>


### [221] [Provably Improving Generalization of Few-Shot Models with Synthetic Data](https://arxiv.org/abs/2505.24190)
*Lan-Cuong Nguyen,Quan Nguyen-Tri,Bang Tran Khanh,Dung D. Le,Long Tran-Thanh,Khoat Than*

Key words: 小样本学习, 图像分类, 合成数据, 分布差异, 原型学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种理论框架来量化合成和真实图像数据分布差异对分类任务的影响，并基于此设计了一种结合原型学习的新算法，显著提升了小样本分类性能。

Motivation: 小样本图像分类因标注数据稀缺而具有挑战性，现有方法通过合成数据增强但存在分布差异导致的性能下降问题。

Method: 开发理论框架分析分布差异影响，提出结合原型学习的算法优化数据划分和模型训练。

Result: 实验表明该方法在多个数据集上优于现有技术。

Conclusion: 理论指导的合成数据生成和模型训练能有效缩小分布差异，提升小样本分类性能。

Abstract: Few-shot image classification remains challenging due to the scarcity of
labeled training examples. Augmenting them with synthetic data has emerged as a
promising way to alleviate this issue, but models trained on synthetic samples
often face performance degradation due to the inherent gap between real and
synthetic distributions. To address this limitation, we develop a theoretical
framework that quantifies the impact of such distribution discrepancies on
supervised learning, specifically in the context of image classification. More
importantly, our framework suggests practical ways to generate good synthetic
samples and to train a predictor with high generalization ability. Building
upon this framework, we propose a novel theoretical-based algorithm that
integrates prototype learning to optimize both data partitioning and model
training, effectively bridging the gap between real few-shot data and synthetic
data. Extensive experiments results show that our approach demonstrates
superior performance compared to state-of-the-art methods, outperforming them
across multiple datasets.

</details>


### [222] [Improved Best-of-Both-Worlds Regret for Bandits with Delayed Feedback](https://arxiv.org/abs/2505.24193)
*Ofir Schlisselberg,Tal Lancewicki,Peter Auer,Yishay Mansour*

Key words: 多臂老虎机、Best-of-Both-Worlds、对抗延迟、随机环境、遗憾界

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种在延迟环境中同时匹配随机和对抗环境下下限的多臂老虎机算法，显著改进了现有结果。

Motivation: 旨在解决现有Best-of-Both-Worlds（BoBW）算法在随机和对抗环境中的性能差距问题，尤其是在延迟环境下。

Method: 设计了一种新算法，在对抗情况下实现接近最优的遗憾界（$	ilde{O}(\sqrt{KT} + \sqrt{D})$），在随机情况下则匹配已知下限。

Result: 算法在对抗和随机环境中均达到理论下限，且在延迟环境下的随机遗憾界首次匹配对抗延迟下的下限。

Conclusion: 该算法是首个在延迟环境中同时优化随机和对抗性能的BoBW算法，为相关研究提供了新方向。

Abstract: We study the multi-armed bandit problem with adversarially chosen delays in
the Best-of-Both-Worlds (BoBW) framework, which aims to achieve near-optimal
performance in both stochastic and adversarial environments. While prior work
has made progress toward this goal, existing algorithms suffer from significant
gaps to the known lower bounds, especially in the stochastic settings. Our main
contribution is a new algorithm that, up to logarithmic factors, matches the
known lower bounds in each setting individually.
  In the adversarial case, our algorithm achieves regret of
$\widetilde{O}(\sqrt{KT} + \sqrt{D})$, which is optimal up to logarithmic
terms, where $T$ is the number of rounds, $K$ is the number of arms, and $D$ is
the cumulative delay. In the stochastic case, we provide a regret bound which
scale as $\sum_{i:\Delta_i>0}\left(\log T/\Delta_i\right) + \frac{1}{K}\sum
\Delta_i \sigma_{max}$, where $\Delta_i$ is the sub-optimality gap of arm $i$
and $\sigma_{\max}$ is the maximum number of missing observations.
  To the best of our knowledge, this is the first BoBW algorithm to
simultaneously match the lower bounds in both stochastic and adversarial
regimes in delayed environment. Moreover, even beyond the BoBW setting, our
stochastic regret bound is the first to match the known lower bound under
adversarial delays, improving the second term over the best known result by a
factor of $K$.

</details>


### [223] [On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks](https://arxiv.org/abs/2505.24205)
*Mingze Wang,Weinan E*

Key words: 混合专家网络,表达能力,低维流形,组合稀疏性,深度学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文系统地研究了混合专家网络（MoEs）在建模复杂任务时的表达能力，特别关注低维稀疏结构。研究发现浅层MoEs能高效逼近低维流形上的函数，而深层MoEs能逼近具有组合稀疏性的分段函数。

Motivation: 理解混合专家网络（MoEs）在建模复杂任务时的理论基础，填补对MoEs表达能力理解的空白。

Method: 通过理论分析，研究浅层和深层MoEs在逼近低维流形和组合稀疏函数时的能力，包括对不同架构组件和超参数的作用分析。

Result: 浅层MoEs能克服维度灾难逼近低维流形函数；深层MoEs可逼近具有$E^L$片段的组合稀疏函数。

Conclusion: MoEs的架构组件（如门控机制、专家网络）和超参数（专家数量、层数）对其表达能力至关重要，研究结果为MoE的改进提供了理论依据。

Abstract: Mixture-of-experts networks (MoEs) have demonstrated remarkable efficiency in
modern deep learning. Despite their empirical success, the theoretical
foundations underlying their ability to model complex tasks remain poorly
understood. In this work, we conduct a systematic study of the expressive power
of MoEs in modeling complex tasks with two common structural priors:
low-dimensionality and sparsity. For shallow MoEs, we prove that they can
efficiently approximate functions supported on low-dimensional manifolds,
overcoming the curse of dimensionality. For deep MoEs, we show that
$\cO(L)$-layer MoEs with $E$ experts per layer can approximate piecewise
functions comprising $E^L$ pieces with compositional sparsity, i.e., they can
exhibit an exponential number of structured tasks. Our analysis reveals the
roles of critical architectural components and hyperparameters in MoEs,
including the gating mechanism, expert networks, the number of experts, and the
number of layers, and offers natural suggestions for MoE variants.

</details>


### [224] [Dynamic Malware Classification of Windows PE Files using CNNs and Greyscale Images Derived from Runtime API Call Argument Conversion](https://arxiv.org/abs/2505.24231)
*Md Shahnawaz,Bishwajit Prasad Gond,Durga Prasad Mohapatra*

Key words: 恶意软件检测, 动态分析, API调用, 卷积神经网络, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文介绍了一种动态恶意软件分类框架，通过提取Windows PE文件运行时API调用参数，将行为数据转换为时序模式，并用CNN模型分类，准确率达98.36%。

Motivation: 恶意软件常采用混淆技术逃避静态分析，动态分析能监控其行为，因此需结合动态检测与深度学习提高分类准确性。

Method: 提取运行时API调用参数，编码为动态特征并转为时序模式，再通过色图转换为灰度图像，用CNN模型训练分类。

Result: 实验结果显示平均分类准确率达98.36%，且能有效抵抗常见逃避策略。

Conclusion: 该方法结合动态分析与深度学习，显著提升了恶意软件分类的准确性和鲁棒性。

Abstract: Malware detection and classification remains a topic of concern for
cybersecurity, since it is becoming common for attackers to use advanced
obfuscation on their malware to stay undetected. Conventional static analysis
is not effective against polymorphic and metamorphic malware as these change
their appearance without modifying their behavior, thus defying the analysis by
code structure alone. This makes it important to use dynamic detection that
monitors malware behavior at runtime. In this paper, we present a dynamic
malware categorization framework that extracts API argument calls at the
runtime execution of Windows Portable Executable (PE) files. Extracting and
encoding the dynamic features of API names, argument return values, and other
relative features, we convert raw behavioral data to temporal patterns. To
enhance feature portrayal, the generated patterns are subsequently converted
into grayscale pictures using a magma colormap. These improved photos are used
to teach a Convolutional Neural Network (CNN) model discriminative features,
which allows for reliable and accurate malware classification. Results from
experiments indicate that our method, with an average accuracy of 98.36% is
effective in classifying different classes of malware and benign by integrating
dynamic analysis and deep learning. It not only achieves high classification
accuracy but also demonstrates significant resilience against typical evasion
strategies.

</details>


### [225] [Model Informed Flows for Bayesian Inference of Probabilistic Programs](https://arxiv.org/abs/2505.24243)
*Joohwan Ko,Justin Domke*

Key words: 变分推断, VIP, 自回归流, 层次贝叶斯模型, 后验近似

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出Model-Informed Flow (MIF)架构，结合VIP和全秩高斯分布，通过理论证明其可表示为带平移项和先验输入的前向自回归流，并在实验中展现出优越性能。

Motivation: 复杂层次贝叶斯模型的后验几何特性常使变分推断受限，VIP和流式变分族的结合尚未被深入探索。

Method: 理论证明VIP与全秩高斯的组合可用带平移项和先验输入的自回归流精确表示，并提出MIF架构。

Result: MIF在层次和非层次基准测试中均达到或超越现有最优性能，后验近似更紧致。

Conclusion: MIF通过融合VIP与流式变分族，有效解决了复杂后验几何的挑战。

Abstract: Variational inference often struggles with the posterior geometry exhibited
by complex hierarchical Bayesian models. Recent advances in flow-based
variational families and Variationally Inferred Parameters (VIP) each address
aspects of this challenge, but their formal relationship is unexplored. Here,
we prove that the combination of VIP and a full-rank Gaussian can be
represented exactly as a forward autoregressive flow augmented with a
translation term and input from the model's prior. Guided by this theoretical
insight, we introduce the Model-Informed Flow (MIF) architecture, which adds
the necessary translation mechanism, prior information, and hierarchical
ordering. Empirically, MIF delivers tighter posterior approximations and
matches or exceeds state-of-the-art performance across a suite of hierarchical
and non-hierarchical benchmarks.

</details>


### [226] [Rethinking Continual Learning with Progressive Neural Collapse](https://arxiv.org/abs/2505.24254)
*Zheng Wang,Wanhao Yu,Li Yang,Sen Lin*

Key words: 持续学习, 灾难性遗忘, 神经崩溃, 类原型, 蒸馏技术

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为Progressive Neural Collapse (ProNC)的新框架，旨在解决持续学习（CL）中的灾难性遗忘问题，通过动态扩展类原型集来提高模型性能。

Motivation: 持续学习（CL）面临的主要挑战是灾难性遗忘，而近期研究使用固定全局神经崩溃（ETF）方案在CL中存在实用性不足和性能受限的问题。论文旨在通过动态调整ETF目标来解决这些问题。

Method: 论文提出ProNC框架，通过逐步扩展ETF目标来为新任务添加类原型，并利用蒸馏技术平衡旧类目标的稳定和新类目标的校准。

Result: 大量实验表明，ProNC在性能上显著优于相关基线方法，同时保持了优越的灵活性、简单性和效率。

Conclusion: ProNC成功消除了对固定全局ETF的依赖，通过动态扩展类原型集显著提升了持续学习的性能，为未来研究提供了新思路。

Abstract: Continual Learning (CL) seeks to build an agent that can continuously learn a
sequence of tasks, where a key challenge, namely Catastrophic Forgetting,
persists due to the potential knowledge interference among different tasks. On
the other hand, deep neural networks (DNNs) are shown to converge to a terminal
state termed Neural Collapse during training, where all class prototypes
geometrically form a static simplex equiangular tight frame (ETF). These
maximally and equally separated class prototypes make the ETF an ideal target
for model learning in CL to mitigate knowledge interference. Thus inspired,
several studies have emerged very recently to leverage a fixed global ETF in
CL, which however suffers from key drawbacks, such as impracticability and
limited performance.To address these challenges and fully unlock the potential
of ETF in CL, we propose Progressive Neural Collapse (ProNC), a novel framework
that completely removes the need of a fixed global ETF in CL. Specifically,
ProNC progressively expands the ETF target in a principled way by adding new
class prototypes as vertices for new tasks, ensuring maximal separability
across all encountered classes with minimal shifts from the previous ETF. We
next develop a new CL framework by plugging ProNC into commonly used CL
algorithm designs, where distillation is further leveraged to balance between
target shifting for old classes and target aligning for new classes. Extensive
experiments show that our approach significantly outperforms related baselines
while maintaining superior flexibility, simplicity, and efficiency.

</details>


### [227] [Taming Hyperparameter Sensitivity in Data Attribution: Practical Selection Without Costly Retraining](https://arxiv.org/abs/2505.24261)
*Weiyi Wang,Junwei Deng,Yuzheng Hu,Shiyuan Zhang,Xirui Jiang,Runting Zhang,Han Zhao,Jiaqi W. Ma*

Key words: 数据归因、超参数敏感性、轻量级调参、影响函数、正则化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文首次大规模实证研究了数据归因方法对超参数的敏感性，发现多数方法对关键超参数敏感，且由于评估成本高昂，提出了无需重新训练模型的轻量级调参策略。

Motivation: 数据归因方法在AI应用中日益重要，但现有研究对其超参数调优的影响缺乏探索。本文旨在填补这一空白，并解决实际应用中高昂的调参成本问题。

Method: 通过大规模实证分析评估常见数据归因方法的超参数敏感性，并通过理论分析提出了一种轻量级的正则项选择方法，避免了模型重新训练。

Result: 多数数据归因方法对关键超参数敏感，且传统的评估方式成本过高。提出的轻量级调参策略在多基准测试中验证有效。

Conclusion: 研究揭示了数据归因实际应用中的核心挑战，强调未来方法开发需重视超参数选择的讨论与优化。

Abstract: Data attribution methods, which quantify the influence of individual training
data points on a machine learning model, have gained increasing popularity in
data-centric applications in modern AI. Despite a recent surge of new methods
developed in this space, the impact of hyperparameter tuning in these methods
remains under-explored. In this work, we present the first large-scale
empirical study to understand the hyperparameter sensitivity of common data
attribution methods. Our results show that most methods are indeed sensitive to
certain key hyperparameters. However, unlike typical machine learning
algorithms -- whose hyperparameters can be tuned using computationally-cheap
validation metrics -- evaluating data attribution performance often requires
retraining models on subsets of training data, making such metrics
prohibitively costly for hyperparameter tuning. This poses a critical open
challenge for the practical application of data attribution methods. To address
this challenge, we advocate for better theoretical understandings of
hyperparameter behavior to inform efficient tuning strategies. As a case study,
we provide a theoretical analysis of the regularization term that is critical
in many variants of influence function methods. Building on this analysis, we
propose a lightweight procedure for selecting the regularization value without
model retraining, and validate its effectiveness across a range of standard
data attribution benchmarks. Overall, our study identifies a fundamental yet
overlooked challenge in the practical application of data attribution, and
highlights the importance of careful discussion on hyperparameter selection in
future method development.

</details>


### [228] [On Fairness of Task Arithmetic: The Role of Task Vectors](https://arxiv.org/abs/2505.24262)
*Hiroki Naganuma,Kotaro Yoshida,Laura Gomezjurado Gonzalez,Takafumi Horie,Yuji Naraki,Ryotaro Shimizu*

Key words: 任务算术, 模型编辑, 公平性, 仇恨言论检测, 参数高效微调

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文探讨了任务算术（Task Arithmetic）技术在模型编辑中对公平性的影响，通过比较任务算术、全微调和LoRA方法，揭示了其对公平性指标的潜在风险，并提出了可控性调整方法。

Motivation: 尽管任务算术等模型编辑技术在计算效率上具有优势，但其对模型公平性的影响尚未充分研究，特别是在敏感应用（如仇恨言论检测）中可能带来风险，因此有必要系统评估其对公平性的影响。

Method: 作者系统研究了任务算术操作对公平性指标（如Demographic Parity和Equalized Odds）的影响，并将其与全微调和LoRA方法进行对比，同时还探索了通过调整任务向量系数来控制公平性结果的可能性。

Result: 研究结果显示，任务算术操作可能对模型公平性产生意想不到的影响，但通过调整任务向量系数，可以实现对模型行为的定制化优化，从而提高公平性。

Conclusion: 该研究为模型编辑中的公平性问题提供了新的见解，并为进一步开发公平性意识强的模型编辑方法奠定了基础。

Abstract: Model editing techniques, particularly task arithmetic using task vectors,
have shown promise in efficiently modifying pre-trained models through
arithmetic operations like task addition and negation. Despite computational
advantages, these methods may inadvertently affect model fairness, creating
risks in sensitive applications like hate speech detection. However, the
fairness implications of task arithmetic remain largely unexplored, presenting
a critical gap in the existing literature. We systematically examine how
manipulating task vectors affects fairness metrics, including Demographic
Parity and Equalized Odds. To rigorously assess these effects, we benchmark
task arithmetic against full fine-tuning, a costly but widely used baseline,
and Low-Rank Adaptation (LoRA), a prevalent parameter-efficient fine-tuning
method. Additionally, we explore merging task vectors from models fine-tuned on
demographic subgroups vulnerable to hate speech, investigating whether fairness
outcomes can be controlled by adjusting task vector coefficients, potentially
enabling tailored model behavior. Our results offer novel insights into the
fairness implications of model editing and establish a foundation for
fairness-aware and responsible model editing practices.

</details>


### [229] [GradPower: Powering Gradients for Faster Language Model Pre-Training](https://arxiv.org/abs/2505.24275)
*Mingze Wang,Jinbo Wang,Jiaqi Zhang,Wei Wang,Peng Pei,Xunliang Cai,Weinan E,Lei Wu*

Key words: GradPower, 梯度变换, 预训练加速, AdamPower, MoE模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: GradPower是一种轻量级梯度变换技术，通过简单的sign-power变换加速语言模型预训练，无需修改优化器内部逻辑即可显著降低损失。

Motivation: 当前语言模型预训练计算成本高，GradPower旨在通过轻量级梯度变换提升训练效率。

Method: 对梯度向量逐元素应用sign-power变换后输入基础优化器（如AdamPower），兼容性强且代码改动极少。

Result: 在多种架构、规模、数据集及学习率调度下均实现更低终端损失，尤其是MoE模型+warmup-stable-decay组合效果显著。

Conclusion: GradPower通过理论分析揭示其机制与梯度噪声的关系，兼容现有优化器并能进一步改进性能。

Abstract: We propose GradPower, a lightweight gradient-transformation technique for
accelerating language model pre-training. Given a gradient vector $g=(g_i)_i$,
GradPower first applies the elementwise sign-power transformation:
$\varphi_p(g)=({\rm sign}(g_i)|g_i|^p)_{i}$ for a fixed $p>0$, and then feeds
the transformed gradient into a base optimizer. Notably, GradPower requires
only a single-line code change and no modifications to the base optimizer's
internal logic, including the hyperparameters. When applied to Adam (termed
AdamPower), GradPower consistently achieves lower terminal loss across diverse
architectures (LLaMA, Qwen2MoE), parameter scales (66M to 2B), datasets (C4,
OpenWebText), and learning-rate schedules (cosine, warmup-stable-decay). The
most pronounced gains are observed when training modern mixture-of-experts
models with warmup-stable-decay schedules. GradPower also integrates seamlessly
with other state-of-the-art optimizers, such as Muon, yielding further
improvements. Finally, we provide theoretical analyses that reveal the
underlying mechanism of GradPower and highlights the influence of gradient
noise.

</details>


### [230] [Large Language Models are Locally Linear Mappings](https://arxiv.org/abs/2505.24293)
*James R. Golden*

Key words: 大型语言模型, 线性系统, 奇异值分解, 语义结构, 可解释性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文展示了如何通过线性系统映射大型语言模型（LLM）的推理操作，无需修改模型权重或改变输出预测，从而揭示其内部表示的语义结构。

Motivation: 研究的目标是揭示现代LLMs的内部工作机制，尽管它们具有强大的表达能力和全局非线性，但其运行可以通过局部线性分解来近似解释。

Method: 通过梯度计算的策略性调整，将LLMs的推理操作映射到一个精确的线性系统，并使用奇异值分解（SVD）分析模型的低维子空间。

Result: 研究表明，LLMs在极低维子空间中运行，其最大的奇异向量与最可能输出令牌的概念相关，并能够观察层间的语义概念涌现。

Conclusion: 尽管LLMs具有复杂的非线性特性，但其内部表示可以通过局部线性分解来解析，为模型的可解释性提供了新视角。

Abstract: We demonstrate that the inference operations of several open-weight large
language models (LLMs) can be mapped to an exactly equivalent linear system for
an input sequence without modifying the model weights or altering output
predictions. Extending techniques from image diffusion models that exhibit
local or piecewise linearity, we strategically alter the gradient computation
with respect to a given input sequence for a next-token prediction such that
the Jacobian of the model nearly exactly reproduces the forward prediction with
a linear system. We demonstrate this approach across models (Llama 3, Gemma 3,
Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show
through the singular value decomposition of the detached Jacobian that these
LLMs operate in extremely low-dimensional subspaces where many of the largest
singular vectors decode to concepts related to the most-likely output token.
This approach also allows us to examine the operation of each successive layer
(and its attention and MLP components) as nearly-exact linear systems and
observe the emergence of semantic concepts. Despite their expressive power and
global nonlinearity, modern LLMs can be interpreted through nearly-exact
locally linear decompositions that provide insights into their internal
representations and reveal interpretable semantic structures in the next-token
prediction process.

</details>


### [231] [AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning](https://arxiv.org/abs/2505.24298)
*Wei Fu,Jiaxuan Gao,Xujie Shen,Chen Zhu,Zhiyu Mei,Chuyi He,Shusheng Xu,Guo Wei,Jun Mei,Jiashu Wang,Tongkai Yang,Binhang Yuan,Yi Wu*

Key words: 强化学习，大语言模型，异步训练，GPU利用率，PPO算法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: AReaL is a fully异步强化学习系统，通过解耦生成与训练，显著提高GPU利用率，相比同步系统实现2.57倍的训练加速，且性能相当或更优。

Motivation: 现有大规模RL系统多为同步方式，生成需等待批次中最长输出完成，导致GPU利用率低。AReaL旨在解决这一系统级低效问题。

Method: AReaL采用完全异步架构，解耦生成与训练：生成工人持续生成输出，训练工人随时更新模型。结合数据过时控制及改进的PPO算法，稳定训练。

Result: 实验表明，AReaL在数学和代码推理任务中，相比同步系统实现最高2.57倍加速，性能持平或更优。

Conclusion: 异步设计有效提升RL训练效率，系统优化与算法改进的结合是关键。

Abstract: Reinforcement learning (RL) has become a trending paradigm for training large
language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs
requires massive parallelization and poses an urgent need for efficient
training systems. Most existing large-scale RL systems for LLMs are synchronous
by alternating generation and training in a batch setting, where the rollouts
in each training batch are generated by the same (or latest) model. This
stabilizes RL training but suffers from severe system-level inefficiency.
Generation must wait until the longest output in the batch is completed before
model update, resulting in GPU underutilization. We present AReaL, a
\emph{fully asynchronous} RL system that completely decouples generation from
training. Rollout workers in AReaL continuously generate new outputs without
waiting, while training workers update the model whenever a batch of data is
collected. AReaL also incorporates a collection of system-level optimizations,
leading to substantially higher GPU utilization. To stabilize RL training,
AReaL balances the workload of rollout and training workers to control data
staleness, and adopts a staleness-enhanced PPO variant to better handle
outdated training samples. Extensive experiments on math and code reasoning
benchmarks show that AReaL achieves \textbf{up to 2.57$\times$ training
speedup} compared to the best synchronous systems with the same number of GPUs
and matched or even improved final performance. The code of AReaL is available
at https://github.com/inclusionAI/AReaL/.

</details>


### [232] [On the Emergence of Weak-to-Strong Generalization: A Bias-Variance Perspective](https://arxiv.org/abs/2505.24313)
*Gengze Xu,Wei Yao,Ziqiao Wang,Yong Liu*

Key words: 弱到强泛化, Bregman散度, 反向交叉熵, 模型容量, 过拟合

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了弱到强泛化（W2SG）现象，通过理论分析证明了学生模型通过预测差异超越弱教师模型的机制，并提出了使用反向交叉熵损失进一步提升性能的方法。

Motivation: 探究为何强学生模型能在弱教师标注的数据上表现更好，并验证相关理论假设的实际应用。

Method: 通过广义偏差-方差分解的Bregman散度理论分析，并结合反向交叉熵损失的实际验证。

Result: 学生模型在特定条件下（如更大容量）能收敛到教师后验均值，反向交叉熵损失能提升性能。

Conclusion: 研究表明避免过拟合、降低预测熵及使用反向交叉熵损失可促进W2SG，为模型训练提供了新方向。

Abstract: Weak-to-strong generalization (W2SG) refers to the phenomenon where a strong
student model, trained on a dataset labeled by a weak teacher, ultimately
outperforms the teacher on the target task. Recent studies attribute this
performance gain to the prediction misfit between the student and teacher
models. In this work, we theoretically investigate the emergence of W2SG
through a generalized bias-variance decomposition of Bregman divergence.
Specifically, we show that the expected population risk gap between the student
and teacher is quantified by the expected misfit between the two models. While
this aligns with previous results, our analysis removes several restrictive
assumptions, most notably, the convexity of the student's hypothesis class,
required in earlier works. Moreover, we show that W2SG is more likely to emerge
when the student model approximates its posterior mean teacher, rather than
mimicking an individual teacher. Using a concrete example, we demonstrate that
if the student model has significantly larger capacity than the teacher, it can
indeed converge to this posterior mean. Our analysis also suggests that
avoiding overfitting to the teacher's supervision and reducing the entropy of
student's prediction further facilitate W2SG. In addition, we show that the
reverse cross-entropy loss, unlike the standard forward cross-entropy, is less
sensitive to the predictive uncertainty of the teacher. Finally, we empirically
verify our theoretical insights and demonstrate that incorporating the reverse
cross-entropy loss consistently improves student performance.

</details>


### [233] [ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving](https://arxiv.org/abs/2505.24317)
*Yongming Chen,Miner Chen,Liewen Liao,Mingyang Jiang,Xiang Zuo,Hengrui Zhang,Yuchen Xi,Songan Zhang*

Key words: 强化学习, 自动驾驶, 奖励函数, 交通法规, 知识图谱

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于责任导向的奖励函数，通过融入交通法规知识图谱和视觉语言模型，自动分配奖励，提升自动驾驶决策效果。

Motivation: 传统强化学习的奖励函数设计过于依赖人工，效果有限，尤其是在复杂场景下。因此，研究希望自动融入交通法规以减少违规和优化决策。

Method: 通过交通法规知识图谱、视觉语言模型和检索增强生成技术，自动生成奖励函数，强化模型对法规的遵守。

Result: 实验证明，该方法显著提高了事故责任分配的准确性，并有效减少了交通事件中的责任承担。

Conclusion: 责任导向的奖励函数能有效提升自动驾驶强化学习的效果，减少违规并优化决策。

Abstract: Reinforcement learning (RL) in autonomous driving employs a trial-and-error
mechanism, enhancing robustness in unpredictable environments. However,
crafting effective reward functions remains challenging, as conventional
approaches rely heavily on manual design and demonstrate limited efficacy in
complex scenarios. To address this issue, this study introduces a
responsibility-oriented reward function that explicitly incorporates traffic
regulations into the RL framework. Specifically, we introduced a Traffic
Regulation Knowledge Graph and leveraged Vision-Language Models alongside
Retrieval-Augmented Generation techniques to automate reward assignment. This
integration guides agents to adhere strictly to traffic laws, thus minimizing
rule violations and optimizing decision-making performance in diverse driving
conditions. Experimental validations demonstrate that the proposed methodology
significantly improves the accuracy of assigning accident responsibilities and
effectively reduces the agent's liability in traffic incidents.

</details>


### [234] [SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation](https://arxiv.org/abs/2505.24324)
*Ivan Petrukha,Yana Kurliak,Nataliia Stulova*

Key words: LLM, code generation, Swift, benchmark, evaluation

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了第一个面向Swift的专业评测基准SwiftEval，发现现有LLM在Swift语言特定需求上表现显著下降。

Motivation: 现有评测基准多围绕Python设计，难以高质量评估其他语言（如Swift）的LLM代码生成能力。

Method: 采用质量优先策略，手工构建包含28个问题的SwiftEval基准，并评测44个流行Code LLM。

Result: 当问题涉及语言特定特性时，LLM得分显著下降，尤其是较小模型。

Conclusion: SwiftEval填补了Swift评测空白，揭示了LLM在多语言适配中的不足。

Abstract: In recent years, large language models (LLMs) have showcased significant
advancements in code generation. However, most evaluation benchmarks are
primarily oriented towards Python, making it difficult to evaluate other
programming languages, such as Swift, with high quality. By examining widely
established multilingual benchmarks like HumanEval-XL and MultiPL-E, we
identified critical issues specific to their Swift components, making them
insufficient or even irrelevant for assessing LLM coding capabilities on Swift.
Unlike these existing approaches, which prioritize rapid scaling and
generalization by automatically translating Python-centric benchmarks with
LLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the
first Swift-oriented benchmark consisting of 28 carefully hand-crafted
problems, and evaluate 44 popular Code LLMs on it. Our results show significant
LLM scores drop for problems requiring language-specific features, most
noticeable in the models of smaller sizes.

</details>


### [235] [Cartan Networks: Group theoretical Hyperbolic Deep Learning](https://arxiv.org/abs/2505.24353)
*Federico Milanesio,Matteo Santoro,Pietro G. Fré,Guido Sanguinetti*

Key words: 双曲深度学习, 双曲空间, 可解群, 群同态, 保距微分同胚

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种基于双曲空间的深度学习算法（Cartan网络），通过结合群同态和保距微分同胚，在多个基准数据集上表现优异。

Motivation: 利用双曲空间的度量特性和可解群结构，开发更高效、信息丰富的层次数据嵌入方法。

Method: 将群同态与保距微分同胚交替结合，设计了一种名为Cartan网络的新型双曲深度学习算法。

Result: 在多个基准数据集上表现出色，验证了算法的有效性。

Conclusion: Cartan网络为双曲深度学习架构开辟了新方向。

Abstract: Hyperbolic deep learning leverages the metric properties of hyperbolic spaces
to develop efficient and informative embeddings of hierarchical data. Here, we
focus on the solvable group structure of hyperbolic spaces, which follows
naturally from their construction as symmetric spaces. This dual nature of Lie
group and Riemannian manifold allows us to propose a new class of hyperbolic
deep learning algorithms where group homomorphisms are interleaved with
metric-preserving diffeomorphisms. The resulting algorithms, which we call
Cartan networks, show promising results on various benchmark data sets and open
the way to a novel class of hyperbolic deep learning architectures.

</details>


### [236] [ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline Calibration](https://arxiv.org/abs/2505.24357)
*Xianglong Yan,Zhiteng Li,Tianao Zhang,Linghe Kong,Yulun Zhang,Xiaokang Yang*

Key words: LLM, KV缓存压缩, 长上下文推理, ReCalKV

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: ReCalKV是一种用于KV缓存压缩的后训练方法，通过针对Key和Value的不同策略，实现了高效的长上下文推理，且性能损失最小。

Motivation: 大型语言模型的长上下文推理能力受限于KV缓存的内存消耗，现有压缩方法在高压缩比下性能下降严重。

Method: 提出了ReCalKV方法，分别对Key和Value采用不同的压缩策略：HSR（分组SVD）和OCMF（离线校准与矩阵融合）。

Result: 实验表明，ReCalKV在高压缩比下优于现有的低秩压缩方法，性能损失较小。

Conclusion: ReCalKV是一种高效的KV缓存压缩方法，适用于长上下文推理优化。

Abstract: Large language models (LLMs) have achieved remarkable performance, yet their
capability on long-context reasoning is often constrained by the excessive
memory required to store the Key-Value (KV) cache. This makes KV cache
compression an essential step toward enabling efficient long-context reasoning.
Recent methods have explored reducing the hidden dimensions of the KV cache,
but many introduce additional computation through projection layers or suffer
from significant performance degradation under high compression ratios. To
address these challenges, we propose ReCalKV, a post-training KV cache
compression method that reduces the hidden dimensions of the KV cache. We
develop distinct compression strategies for Keys and Values based on their
different roles and varying importance in the attention mechanism. For Keys, we
propose Head-wise Similarity-aware Reordering (HSR), which clusters similar
heads and applies grouped SVD to the key projection matrix, reducing additional
computation while preserving accuracy. For Values, we propose Offline
Calibration and Matrix Fusion (OCMF) to preserve accuracy without extra
computational overhead. Experiments show that ReCalKV outperforms existing
low-rank compression methods, achieving high compression ratios with minimal
performance loss. Code is available at:
https://github.com/XIANGLONGYAN/ReCalKV.

</details>


### [237] [Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning](https://arxiv.org/abs/2505.24360)
*Stepan Shabalin,Ayush Panda,Dmitrii Kharlapenko,Abdur Raheem Ali,Yixiong Hao,Arthur Conmy*

Key words: 稀疏自编码器,扩散模型,激活分解,解释性,控制

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文探讨了稀疏自编码器（SAEs）和推理时间激活分解（ITDA）在大规模文本到图像扩散模型Flux 1中的应用，展示了两者在嵌入解释性和生成控制方面的有效性。

Motivation: 研究稀疏自编码器和ITA激活分解技术在大规模扩散模型中的适用性，以提升模型激活的解释性和控制能力。

Method: 应用稀疏自编码器（SAEs）和推理时间激活分解（ITDA）技术，结合视觉自动解释流程，分析Flux 1模型的嵌入解释性。

Result: SAEs能准确重构残留流嵌入，并在解释性上优于MLP神经元，同时能通过激活加法控制图像生成；ITDA的解释性与SAEs相当。

Conclusion: 稀疏自编码器和ITDA在提高大规模扩散模型解释性和控制性方面具有潜力。

Abstract: Sparse autoencoders are a promising new approach for decomposing language
model activations for interpretation and control. They have been applied
successfully to vision transformer image encoders and to small-scale diffusion
models. Inference-Time Decomposition of Activations (ITDA) is a recently
proposed variant of dictionary learning that takes the dictionary to be a set
of data points from the activation distribution and reconstructs them with
gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large
text-to-image diffusion model, Flux 1, and consider the interpretability of
embeddings of both by introducing a visual automated interpretation pipeline.
We find that SAEs accurately reconstruct residual stream embeddings and beat
MLP neurons on interpretability. We are able to use SAE features to steer image
generation through activation addition. We find that ITDA has comparable
interpretability to SAEs.

</details>


### [238] [Anomaly Detection and Improvement of Clusters using Enhanced K-Means Algorithm](https://arxiv.org/abs/2505.24365)
*Vardhan Shorewala,Shivam Shorewala*

Key words: 聚类优化,异常检测,迭代算法,方差最小化,评估指标

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种统一方法，用于数据集的聚类优化和异常检测，算法通过迭代减小簇内方差实现更紧密的聚类，并通过外部验证显示显著性能提升。

Motivation: 解决标准k-means算法的聚类效果不佳问题，并扩展其能力以检测异常数据点。

Method: 提出迭代算法减少簇内方差，结合多种内外部评估指标验证聚类效果，并检测异常点。

Result: 在合成和真实数据集上实现了方差显著降低（18.7%和88.1%），并在异常检测中提升了22.5%的准确率和20.8%的F1分数。

Conclusion: 算法在聚类和异常检测任务中均表现出优越性能，适用于多种数据集。

Abstract: This paper introduces a unified approach to cluster refinement and anomaly
detection in datasets. We propose a novel algorithm that iteratively reduces
the intra-cluster variance of N clusters until a global minimum is reached,
yielding tighter clusters than the standard k-means algorithm. We evaluate the
method using intrinsic measures for unsupervised learning, including the
silhouette coefficient, Calinski-Harabasz index, and Davies-Bouldin index, and
extend it to anomaly detection by identifying points whose assignment causes a
significant variance increase. External validation on synthetic data and the
UCI Breast Cancer and UCI Wine Quality datasets employs the Jaccard similarity
score, V-measure, and F1 score. Results show variance reductions of 18.7% and
88.1% on the synthetic and Wine Quality datasets, respectively, along with
accuracy and F1 score improvements of 22.5% and 20.8% on the Wine Quality
dataset.

</details>


### [239] [Adversarial Preference Learning for Robust LLM Alignment](https://arxiv.org/abs/2505.24369)
*Yuanfu Wang,Pengyu Wang,Chenyang Xi,Bo Tang,Junyi Zhu,Wenqiang Wei,Chen Chen,Chao Yang,Jingfeng Zhang,Chaochao Lu,Yijun Niu,Keming Mao,Zhiyu Li,Feiyu Xiong,Jie Hu,Mingchuan Yang*

Key words: APL, 对抗训练, RLHF, 语言模型, 安全性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: APL是一种迭代对抗训练方法，通过引入内在偏好概率、条件生成攻击器和自动化闭环反馈，显著提升语言模型的安全性，同时保持其性能。

Motivation: 解决RLHF在效率、多样性和偏见上的局限性，提升语言模型对抗攻击的鲁棒性。

Method: 采用直接有害性度量、条件生成攻击器和迭代框架，进行闭环对抗训练。

Result: 提升无害性达83.33%，有害输出降至0.43%，攻击成功率降65%，性能保持接近基线。

Conclusion: APL有效平衡安全性与实用性，为对抗训练提供新思路。

Abstract: Modern language models often rely on Reinforcement Learning from Human
Feedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to
adversarial attacks due to three key limitations: (1) the inefficiency and high
cost of human annotation, (2) the vast diversity of potential adversarial
attacks, and (3) the risk of feedback bias and reward hacking. To address these
challenges, we introduce Adversarial Preference Learning (APL), an iterative
adversarial training method incorporating three key innovations. First, a
direct harmfulness metric based on the model's intrinsic preference
probabilities, eliminating reliance on external assessment. Second, a
conditional generative attacker that synthesizes input-specific adversarial
variations. Third, an iterative framework with automated closed-loop feedback,
enabling continuous adaptation through vulnerability discovery and mitigation.
Experiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly
enhances robustness, achieving 83.33% harmlessness win rate over the base model
(evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured
by LLaMA-Guard), and lowering attack success rate by up to 65% according to
HarmBench. Notably, APL maintains competitive utility, with an MT-Bench score
of 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against
the base model.

</details>


### [240] [Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer](https://arxiv.org/abs/2505.24378)
*Yilun Kong,Guozheng Ma,Qi Zhao,Haoyu Wang,Li Shen,Xueqian Wang,Dacheng Tao*

Key words: 多任务强化学习, Transformer, MoE, 任务扩展性, M3DT

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文揭示了任务数量对当前多任务强化学习方法的关键影响，并提出了一种新型的MoE框架M3DT，通过增强参数扩展能力来解决任务扩展性问题。

Motivation: 研究发现在多任务强化学习中，任务数量的增加会导致性能下降，而简单地扩展参数无法有效解决这一问题，因此需要新的方法。

Method: 提出M3DT框架，利用MoE增强决策Transformer架构，并引入三阶段训练机制以提高训练效率和性能。

Result: 实验表明，M3DT通过增加专家数量，不仅在固定任务数下提升性能，还能扩展到160个任务并保持卓越表现。

Conclusion: M3DT通过MoE架构和三阶段训练，有效解决了多任务强化学习中的任务扩展性问题。

Abstract: Despite recent advancements in offline multi-task reinforcement learning
(MTRL) have harnessed the powerful capabilities of the Transformer
architecture, most approaches focus on a limited number of tasks, with scaling
to extremely massive tasks remaining a formidable challenge. In this paper, we
first revisit the key impact of task numbers on current MTRL method, and
further reveal that naively expanding the parameters proves insufficient to
counteract the performance degradation as the number of tasks escalates.
Building upon these insights, we propose M3DT, a novel mixture-of-experts (MoE)
framework that tackles task scalability by further unlocking the model's
parameter scalability. Specifically, we enhance both the architecture and the
optimization of the agent, where we strengthen the Decision Transformer (DT)
backbone with MoE to reduce task load on parameter subsets, and introduce a
three-stage training mechanism to facilitate efficient training with optimal
performance. Experimental results show that, by increasing the number of
experts, M3DT not only consistently enhances its performance as model expansion
on the fixed task numbers, but also exhibits remarkable task scalability,
successfully extending to 160 tasks with superior performance.

</details>


### [241] [Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models](https://arxiv.org/abs/2505.24379)
*Xiaoyu Wu,Yifei Pang,Terrance Liu,Zhiwei Steven Wu*

Key words: 隐私保护, 精确遗忘, 数据提取攻击, 大型语言模型, 威胁模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文挑战了精确遗忘（exact unlearning）作为隐私保护金标准的假设，提出了一种新型数据提取攻击方法，通过结合遗忘前后模型的信号，显著提升了隐私泄露的成功率。

Motivation: 随着大型语言模型在隐私敏感数据上的应用增加，精确遗忘被认为是最可靠的隐私保护方法。然而，作者质疑其安全性，旨在揭示遗忘方法可能反而增加隐私风险。

Method: 提出了一种结合遗忘前后模型信号的数据提取攻击方法，通过模型引导和令牌过滤策略，从遗忘后模型中恢复被删除的数据分布。

Result: 攻击方法在多个基准测试（如MUSE、TOFU和WMDP）中显著提升了数据提取成功率，某些情况下表现翻倍，并在模拟医疗诊断数据集中验证了其实用性。

Conclusion: 研究结果表明，精确遗忘可能反而增加隐私泄露风险，建议评估遗忘方法时应考虑更广泛的威胁模型，包括对抗者可能访问遗忘前的模型检查点。

Abstract: Large language models are typically trained on datasets collected from the
web, which may inadvertently contain harmful or sensitive personal information.
To address growing privacy concerns, unlearning methods have been proposed to
remove the influence of specific data from trained models. Of these, exact
unlearning -- which retrains the model from scratch without the target data --
is widely regarded the gold standard, believed to be robust against
privacy-related attacks. In this paper, we challenge this assumption by
introducing a novel data extraction attack that compromises even exact
unlearning. Our method leverages both the pre- and post-unlearning models: by
guiding the post-unlearning model using signals from the pre-unlearning model,
we uncover patterns that reflect the removed data distribution. Combining model
guidance with a token filtering strategy, our attack significantly improves
extraction success rates -- doubling performance in some cases -- across common
benchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our
attack's effectiveness on a simulated medical diagnosis dataset to highlight
real-world privacy risks associated with exact unlearning. In light of our
findings, which suggest that unlearning may, in a contradictory way, increase
the risk of privacy leakage, we advocate for evaluation of unlearning methods
to consider broader threat models that account not only for post-unlearning
models but also for adversarial access to prior checkpoints.

</details>


### [242] [LightSAM: Parameter-Agnostic Sharpness-Aware Minimization](https://arxiv.org/abs/2505.24399)
*Yifei Cheng,Li Shen,Hao Sun,Nan Yin,Xiaochun Cao,Enhong Chen*

Key words: Sharpness-Aware Minimization, SAM, LightSAM, 自适应优化, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: LightSAM是一种自适应的Sharpness-Aware Minimization（SAM）优化器，通过自适应调整扰动半径和学习率，减少对参数的敏感性，扩展了SAM的应用范围。

Motivation: SAM优化器虽然提高了模型的泛化能力，但其扰动半径和学习率需要手动调参，限制了实际应用。LightSAM旨在通过自适应方法解决这一问题。

Method: LightSAM采用AdaGrad-Norm、AdaGrad和Adam等自适应优化器替代SGD，实现扰动半径和学习率的自适应调整。

Result: 理论分析表明，LightSAM在弱假设下能理想收敛，且对参数无依赖性。实验验证了其有效性。

Conclusion: LightSAM通过自适应机制减少了参数敏感性，扩展了SAM的实用性。

Abstract: Sharpness-Aware Minimization (SAM) optimizer enhances the generalization
ability of the machine learning model by exploring the flat minima landscape
through weight perturbations. Despite its empirical success, SAM introduces an
additional hyper-parameter, the perturbation radius, which causes the
sensitivity of SAM to it. Moreover, it has been proved that the perturbation
radius and learning rate of SAM are constrained by problem-dependent parameters
to guarantee convergence. These limitations indicate the requirement of
parameter-tuning in practical applications. In this paper, we propose the
algorithm LightSAM which sets the perturbation radius and learning rate of SAM
adaptively, thus extending the application scope of SAM. LightSAM employs three
popular adaptive optimizers, including AdaGrad-Norm, AdaGrad and Adam, to
replace the SGD optimizer for weight perturbation and model updating, reducing
sensitivity to parameters. Theoretical results show that under weak
assumptions, LightSAM could converge ideally with any choices of perturbation
radius and learning rate, thus achieving parameter-agnostic. We conduct
preliminary experiments on several deep learning tasks, which together with the
theoretical findings validate the the effectiveness of LightSAM.

</details>


### [243] [On the Lipschitz Continuity of Set Aggregation Functions and Neural Networks for Sets](https://arxiv.org/abs/2505.24403)
*Giannis Nikolentzos,Konstantinos Skianis*

Key words: Lipschitz常数, 神经网络, 多组向量, 聚合函数, 泛化能力

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了神经网络中处理多组向量数据的聚合函数的Lipschitz连续性及其常数，并基于此推导了相关神经网络的上界，同时验证了其稳定性和泛化能力。

Motivation: 研究动机在于了解神经网络中处理无序多组向量数据的聚合函数的Lipschitz连续性，这对模型的鲁棒性和泛化能力至关重要。

Method: 方法包括分析三种距离函数下聚合函数的Lipschitz连续性，计算其常数，并推导相关神经网络的上界，通过实验验证理论分析。

Result: 结果表明，每种聚合函数仅对一种距离函数具有Lipschitz连续性，并通过实验验证了理论分析。

Conclusion: 结论是研究为处理多组向量数据的神经网络提供了Lipschitz常数的上界，验证了其稳定性和泛化能力。

Abstract: The Lipschitz constant of a neural network is connected to several important
properties of the network such as its robustness and generalization. It is thus
useful in many settings to estimate the Lipschitz constant of a model. Prior
work has focused mainly on estimating the Lipschitz constant of multi-layer
perceptrons and convolutional neural networks. Here we focus on data modeled as
sets or multisets of vectors and on neural networks that can handle such data.
These models typically apply some permutation invariant aggregation function,
such as the sum, mean or max operator, to the input multisets to produce a
single vector for each input sample. In this paper, we investigate whether
these aggregation functions are Lipschitz continuous with respect to three
distance functions for unordered multisets, and we compute their Lipschitz
constants. In the general case, we find that each aggregation function is
Lipschitz continuous with respect to only one of the three distance functions.
Then, we build on these results to derive upper bounds on the Lipschitz
constant of neural networks that can process multisets of vectors, while we
also study their stability to perturbations and generalization under
distribution shifts. To empirically verify our theoretical analysis, we conduct
a series of experiments on datasets from different domains.

</details>


### [244] [Multi-task Learning for Heterogeneous Multi-source Block-Wise Missing Data](https://arxiv.org/abs/2505.24413)
*Yang Sui,Qi Xu,Yang Bai,Annie Qu*

Key words: 多任务学习、异质性、共享表征、ADNI数据库

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种两步学习策略的多任务学习方法，通过共享表征填补缺失数据块，并分解输入特征与响应的映射，以有效处理多任务学习中的异质性问题。

Motivation: 多任务学习（MTL）在医疗、营销和生物医学领域已成功应用，但现有方法难以同时处理多种异质性（如块状、分布和后验异质性）。因此，需要一种统一框架来有效利用同质和异质信息。

Method: 采用两步学习策略：1）利用同质源提取的共享表征填补缺失数据块；2）将输入特征与响应的映射分解为共享组件和任务特定组件。

Result: 数值实验和ADNI数据库的实际数据分析表明，该方法在多任务学习中优于其他竞争方法。

Conclusion: 提出的两步策略能有效解决MTL中的异质性问题，并通过共享组件实现信息借用的优化表现。

Abstract: Multi-task learning (MTL) has emerged as an imperative machine learning tool
to solve multiple learning tasks simultaneously and has been successfully
applied to healthcare, marketing, and biomedical fields. However, in order to
borrow information across different tasks effectively, it is essential to
utilize both homogeneous and heterogeneous information. Among the extensive
literature on MTL, various forms of heterogeneity are presented in MTL
problems, such as block-wise, distribution, and posterior heterogeneity.
Existing methods, however, struggle to tackle these forms of heterogeneity
simultaneously in a unified framework. In this paper, we propose a two-step
learning strategy for MTL which addresses the aforementioned heterogeneity.
First, we impute the missing blocks using shared representations extracted from
homogeneous source across different tasks. Next, we disentangle the mappings
between input features and responses into a shared component and a
task-specific component, respectively, thereby enabling information borrowing
through the shared component. Our numerical experiments and real-data analysis
from the ADNI database demonstrate the superior MTL performance of the proposed
method compared to other competing methods.

</details>


### [245] [Boosting Automatic Exercise Evaluation Through Musculoskeletal Simulation-Based IMU Data Augmentation](https://arxiv.org/abs/2505.24415)
*Andreas Spilz,Heiko Oppel,Michael Munz*

Key words: 数据增强, 运动评估, 深度学习, IMU, 肌肉骨骼模拟

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种利用肌肉骨骼模拟生成真实IMU数据的数据增强方法，解决运动评估中数据不足、类别不平衡及标签模糊问题，显著提升模型分类准确性和泛化能力。

Motivation: 自动化运动质量评估在物理治疗和运动训练中潜力巨大，但深度学习模型因数据不足、类别不平衡和标签模糊而效果受限。

Method: 结合肌肉骨骼模拟和系统运动轨迹修改生成真实IMU数据，并通过逆运动学参数与知识评估策略实现自动可靠标注。

Result: 增强数据与真实数据高度相似，显著提升神经网络模型的分类准确性和泛化能力，尤其在病人特定微调场景中效果显著。

Conclusion: 该方法有效解决了深度学习在物理治疗运动评估中的常见挑战，具有实用性和高效性。

Abstract: Automated evaluation of movement quality holds significant potential for
enhancing physiotherapeutic treatments and sports training by providing
objective, real-time feedback. However, the effectiveness of deep learning
models in assessing movements captured by inertial measurement units (IMUs) is
often hampered by limited data availability, class imbalance, and label
ambiguity. In this work, we present a novel data augmentation method that
generates realistic IMU data using musculoskeletal simulations integrated with
systematic modifications of movement trajectories. Crucially, our approach
ensures biomechanical plausibility and allows for automatic, reliable labeling
by combining inverse kinematic parameters with a knowledge-based evaluation
strategy. Extensive evaluations demonstrate that augmented variants closely
resembles real-world data, significantly improving the classification accuracy
and generalization capability of neural network models. Additionally, we
highlight the benefits of augmented data for patient-specific fine-tuning
scenarios, particularly when only limited subject-specific training examples
are available. Our findings underline the practicality and efficacy of this
augmentation method in overcoming common challenges faced by deep learning
applications in physiotherapeutic exercise evaluation.

</details>


### [246] [Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning](https://arxiv.org/abs/2505.24424)
*Amit Peleg,Naman Deep Singh,Matthias Hein*

Key words: CLIP, 组合推理, 微调, 检索性能, SugarCrepe++

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: CLIC是一种基于多图像和关联标题的新型微调方法，提升了CLIP模型在组合推理中的能力，包括词汇和语义理解，并显著提高了检索性能。

Motivation: 现有视觉语言模型（如CLIP）在组合推理（理解概念间关系）方面表现不佳，且改进组合性的尝试往往忽略了语义理解或损害检索性能。

Method: 提出了CLIC方法，通过结合多图像及其关联标题进行微调，提升模型的组合性。

Result: CLIC在所有测试的CLIP架构和预训练模型上均提升了组合性（词汇和语义）和检索性能，包括最新的CLIPS模型。

Conclusion: 短时间的CLIC微调可显著提升检索性能和组合推理能力，成为SugarCrepe++基准上最佳的CLIP模型。

Abstract: Vision-language models like CLIP have demonstrated remarkable zero-shot
capabilities in classification and retrieval. However, these models often
struggle with compositional reasoning - the ability to understand the
relationships between concepts. A recent benchmark, SugarCrepe++, reveals that
previous works on improving compositionality have mainly improved lexical
sensitivity but neglected semantic understanding. In addition, downstream
retrieval performance often deteriorates, although one would expect that
improving compositionality should enhance retrieval. In this work, we introduce
CLIC (Compositionally-aware Learning in CLIP), a fine-tuning method based on a
novel training technique combining multiple images and their associated
captions. CLIC improves compositionality across architectures as well as
differently pre-trained CLIP models, both in terms of lexical and semantic
understanding, and achieves consistent gains in retrieval performance. This
even applies to the recent CLIPS, which achieves SOTA retrieval performance.
Nevertheless, the short fine-tuning with CLIC leads to an improvement in
retrieval and to the best compositional CLIP model on SugarCrepe++. All our
models and code are available at https://clic-compositional-clip.github.io

</details>


### [247] [Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields](https://arxiv.org/abs/2505.24434)
*Md Shahriar Rahim Siddiqui,Moshe Eliasof,Eldad Haber*

Key words: 流匹配, 图神经网络, 生成模型, 反应-扩散, 图像生成

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出Graph Flow Matching（GFM）方法，通过图神经网络增强流匹配的局部上下文信息，提升生成质量。

Motivation: 现有流匹配方法独立预测每个点的速度，忽略了相邻点之间的相关性，可能降低生成质量。

Method: GFM将速度分解为反应项（标准流匹配网络）和扩散项（图神经模块聚合邻域信息），反应-扩散公式保持计算效率。

Result: 在五个图像生成基准上（如LSUN Church等），GFM显著改善了FID和召回率。

Conclusion: GFM是一种轻量级增强模块，可提升现有流匹配架构的性能。

Abstract: Flow matching casts sample generation as learning a continuous-time velocity
field that transports noise to data. Existing flow matching networks typically
predict each point's velocity independently, considering only its location and
time along its flow trajectory, and ignoring neighboring points. However, this
pointwise approach may overlook correlations between points along the
generation trajectory that could enhance velocity predictions, thereby
improving downstream generation quality. To address this, we propose Graph Flow
Matching (GFM), a lightweight enhancement that decomposes the learned velocity
into a reaction term -- any standard flow matching network -- and a diffusion
term that aggregates neighbor information via a graph neural module. This
reaction-diffusion formulation retains the scalability of deep flow models
while enriching velocity predictions with local context, all at minimal
additional computational cost. Operating in the latent space of a pretrained
variational autoencoder, GFM consistently improves Fr\'echet Inception Distance
(FID) and recall across five image generation benchmarks (LSUN Church, LSUN
Bedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\times256$), demonstrating its
effectiveness as a modular enhancement to existing flow matching architectures.

</details>


### [248] [Weisfeiler and Leman Follow the Arrow of Time: Expressive Power of Message Passing in Temporal Event Graphs](https://arxiv.org/abs/2505.24438)
*Franziska Heeg,Jonas Sauer,Petra Mutzel,Ingo Scholtes*

Key words: 时序图, 图同构, 时序神经网络, 因果拓扑, 事件图

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种针对时序图的新型同构概念，并开发了相应的TGNN消息传递方案，实验验证了其有效性。

Motivation: 研究时序图中时间箭头对因果拓扑的影响，并解决现有TGNNs对此模式忽视的问题。

Method: 提出一致事件图同构概念，开发时序Weisfeiler-Leman算法，并设计基于事件图的TGNN消息传递方案。

Result: 新方法在时序图分类实验中表现良好。

Conclusion: 通过理论分析和实验验证，证明了新方法在捕捉时序图因果拓扑方面的优越性。

Abstract: An important characteristic of temporal graphs is how the directed arrow of
time influences their causal topology, i.e., which nodes can possibly influence
each other causally via time-respecting paths. The resulting patterns are often
neglected by temporal graph neural networks (TGNNs). To formally analyze the
expressive power of TGNNs, we lack a generalization of graph isomorphism to
temporal graphs that fully captures their causal topology. Addressing this gap,
we introduce the notion of consistent event graph isomorphism, which utilizes a
time-unfolded representation of time-respecting paths in temporal graphs. We
compare this definition with existing notions of temporal graph isomorphisms.
We illustrate and highlight the advantages of our approach and develop a
temporal generalization of the Weisfeiler-Leman algorithm to heuristically
distinguish non-isomorphic temporal graphs. Building on this theoretical
foundation, we derive a novel message passing scheme for temporal graph neural
networks that operates on the event graph representation of temporal graphs. An
experimental evaluation shows that our approach performs well in a temporal
graph classification experiment.

</details>


### [249] [Learning Safety Constraints for Large Language Models](https://arxiv.org/abs/2505.24445)
*Xin Chen,Yarden As,Andreas Krause*

Key words: 大型语言模型, 安全性, 几何方法, 多面体, 对抗攻击, 可解释性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种名为SaP（Safety Polytope）的几何方法，用于提升大语言模型（LLM）的安全性。该方法通过在模型的表示空间中学习和强制执行多个安全约束，识别安全与不安全区域，并实现不安全输出的检测与纠正。实验表明，SaP能有效检测不道德输入、降低对抗攻击成功率，同时保持标准任务性能。

Motivation: 大型语言模型虽强大，但其有害输出和对对抗攻击的脆弱性带来了显著的安全风险。现有的方法通常通过修改模型权重来增强安全性，这可能影响模型的性能。因此，需要一种在不损害模型能力的前提下，确保安全性的方法。

Method: 作者提出了SaP（Safety Polytope）方法，这是一种几何学方法。它通过在模型的表示空间定义多面体的面来识别安全与不安全区域，并利用几何导向技术检测和纠正不安全输出。该方法不改变模型权重，仅作用于表示空间。

Result: 实验表明，SaP能有效识别不道德的输入，并显著降低对抗攻击的成功率，同时不影响模型在标准任务上的表现。此外，多面体面的分析揭示了模型对不同安全语义的专门化检测能力，为理解安全性的表示提供了可解释的见解。

Conclusion: 研究表明，SaP是一种有效的后处理方法，能够在保持模型性能的同时增强其安全性。通过几何学方法明确建模安全性，为未来的LLM安全研究提供了新的方向。

Abstract: Large language models (LLMs) have emerged as powerful tools but pose
significant safety risks through harmful outputs and vulnerability to
adversarial attacks. We propose SaP, short for Safety Polytope, a geometric
approach to LLM safety that learns and enforces multiple safety constraints
directly in the model's representation space. We develop a framework that
identifies safe and unsafe regions via the polytope's facets, enabling both
detection and correction of unsafe outputs through geometric steering. Unlike
existing approaches that modify model weights, SaP operates post-hoc in the
representation space, preserving model capabilities while enforcing safety
constraints. Experiments across multiple LLMs demonstrate that our method can
effectively detect unethical inputs, reduce adversarial attack success rates
while maintaining performance on standard tasks, thus highlighting the
importance of having an explicit geometric model for safety. Analysis of the
learned polytope facets reveals emergence of specialization in detecting
different semantic notions of safety, providing interpretable insights into how
safety is captured in LLMs' representation space.

</details>


### [250] [Stepsize anything: A unified learning rate schedule for budgeted-iteration training](https://arxiv.org/abs/2505.24452)
*Anda Tang,Yiming Dong,Yutao Zeng,zhou Xun,Zhouchen Lin*

Key words: 学习率调度、预算感知训练、优化框架、收敛性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种理论支持的统一预算感知学习率调度方法UBA，在资源有限时优化训练效果。

Motivation: 解决学习率调度设计缺乏理论基础和需反复试验的问题，提升预算内的训练效率。

Method: 构建预算感知优化框架，提出由单一超参数φ控制的UBA调度，避免繁琐优化。

Result: UBA在不同架构和任务中均优于常用调度，验证了其通用性和高效性。

Conclusion: UBA为预算内训练提供理论支持且效果显著，简化了学习率调优。

Abstract: The expanding computational costs and limited resources underscore the
critical need for budgeted-iteration training, which aims to achieve optimal
learning within predetermined iteration budgets.While learning rate schedules
fundamentally govern the performance of different networks and tasks,
particularly in budgeted-iteration scenarios, their design remains largely
heuristic, lacking theoretical foundations.In addition, the optimal learning
rate schedule requires extensive trial-and-error selection, making the training
process inefficient.In this work, we propose the Unified Budget-Aware (UBA)
schedule, a theoretically grounded learning rate schedule that consistently
outperforms commonly-used schedules among diverse architectures and tasks under
different constrained training budgets.First, we bridge the gap by constructing
a novel training budget-aware optimization framework, which explicitly accounts
for the robustness to landscape curvature variations.From this framework, we
derive the UBA schedule, controlled by a single hyper-parameter $\varphi$ that
provides a trade-off between flexibility and simplicity, eliminating the need
for per-network numerical optimization. Moreover, we establish a theoretical
connection between $\varphi$ and the condition number, adding interpretation
and justification to our approach. Besides, we prove the convergence for
different values of $\varphi$.We offer practical guidelines for its selection
via theoretical analysis and empirical results.xtensive experimental results
show that UBA \textit{consistently surpasses} the commonly-used schedules
across diverse vision and language tasks, spanning network architectures (e.g.,
ResNet, OLMo) and scales, under different training-iteration budgets.

</details>


### [251] [Logits-Based Finetuning](https://arxiv.org/abs/2505.24461)
*Jingyao Li,Senqiao Yang,Sitong Wu,Han Shi,Chuanyang Zheng,Hong Xu,Jiaya Jia*

Key words: OOD检测, 基于重建的方法, Masked Image Modeling, MOOD框架

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 使用基于重建的方法（如Masked Image Modeling）显著提升OOD检测性能，MOOD框架在多项任务中超越现有SOTA。

Motivation: 现有基于识别的OOD检测方法倾向于学习捷径而非全面表示，研究发现基于重建的方法能更好地学习ID数据的内在分布。

Method: 采用Masked Image Modeling作为前置任务，构建OOD检测框架MOOD，无需OOD样本即可实现高性能检测。

Result: MOOD在单类OOD检测上提升5.7%，多类OOD检测提升3.0%，近分布OOD检测提升2.1%，甚至优于10-shot-per-class异常暴露方法。

Conclusion: 基于重建的前置任务为OOD检测提供了通用且高效的先验，MOOD框架无需OOD样本即可取得卓越性能。

Abstract: The core of out-of-distribution (OOD) detection is to learn the
in-distribution (ID) representation, which is distinguishable from OOD samples.
Previous work applied recognition-based methods to learn the ID features, which
tend to learn shortcuts instead of comprehensive representations. In this work,
we find surprisingly that simply using reconstruction-based methods could boost
the performance of OOD detection significantly. We deeply explore the main
contributors of OOD detection and find that reconstruction-based pretext tasks
have the potential to provide a generally applicable and efficacious prior,
which benefits the model in learning intrinsic data distributions of the ID
dataset. Specifically, we take Masked Image Modeling as a pretext task for our
OOD detection framework (MOOD). Without bells and whistles, MOOD outperforms
previous SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by
3.0%, and near-distribution OOD detection by 2.1%. It even defeats the
10-shot-per-class outlier exposure OOD detection, although we do not include
any OOD samples for our detection. Codes are available at
https://github.com/JulietLJY/MOOD.

</details>


### [252] [Smooth Model Compression without Fine-Tuning](https://arxiv.org/abs/2505.24469)
*Christina Runkel,Natacha Kuete Meli,Jovita Lukasik,Ander Biguri,Carola-Bibiane Schönlieb,Michael Moeller*

Key words: 模型压缩, 平滑正则化, 核范数, SVD, 低秩近似

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了平滑正则化对神经网络训练和模型压缩的影响，通过应用核范数和权重导数惩罚，提升结构化平滑性并保持预测性能，发现标准剪枝方法在平滑模型上更有效。提出了一种基于SVD的压缩方法，能实现70%参数减少，91%准确率。

Motivation: 由于现有剪枝和压缩技术通常未考虑权重结构，限制了效果。本文旨在探索平滑正则化对模型训练和压缩的改进作用。

Method: 在训练中应用核范数、一阶和二阶导数惩罚权重，增强结构化平滑性；基于SVD的压缩方法利用平滑结构，通过低秩张量近似权重。

Result: 平滑ResNet-18在CIFAR-10上达到91%准确率，参数减少70%，无需微调即可实现先进压缩。

Conclusion: 平滑正则化提升了模型的结构化特性，使标准剪枝方法更有效；SVD压缩能高效减少参数并保持性能。

Abstract: Compressing and pruning large machine learning models has become a critical
step towards their deployment in real-world applications. Standard pruning and
compression techniques are typically designed without taking the structure of
the network's weights into account, limiting their effectiveness. We explore
the impact of smooth regularization on neural network training and model
compression. By applying nuclear norm, first- and second-order derivative
penalties of the weights during training, we encourage structured smoothness
while preserving predictive performance on par with non-smooth models. We find
that standard pruning methods often perform better when applied to these smooth
models. Building on this observation, we apply a
Singular-Value-Decomposition-based compression method that exploits the
underlying smooth structure and approximates the model's weight tensors by
smaller low-rank tensors. Our approach enables state-of-the-art compression
without any fine-tuning - reaching up to $91\%$ accuracy on a smooth ResNet-18
on CIFAR-10 with $70\%$ fewer parameters.

</details>


### [253] [Train One Sparse Autoencoder Across Multiple Sparsity Budgets to Preserve Interpretability and Accuracy](https://arxiv.org/abs/2505.24473)
*Nikita Balagansky,Yaroslav Aksenov,Daniil Laptev,Vadim Kurochkin,Gleb Gerasimov,Nikita Koryagin,Daniil Gavrilov*

Key words: 稀疏自编码器, 可解释性, 稀疏性约束, HierarchicalTopK

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为HierarchicalTopK的新型训练目标，旨在通过单一稀疏自编码器（SAE）同时优化多个稀疏级别的重建效果，解决了传统SAE因固定稀疏级别而导致的计算效率低下的问题。

Motivation: 传统稀疏自编码器（SAEs）因训练时选择的稀疏级别固定，不能满足不同稀疏需求，导致需要训练多个模型，增加了计算负担。本文旨在设计一个更灵活且高效的方法。

Method: 提出了一种名为HierarchicalTopK的训练目标，通过单个SAE同时优化多个稀疏级别的重建效果。

Result: 在Gemma-2 2B上的实验表明，该方法能够在稀疏性和解释方差之间实现帕累托最优，且在高稀疏性下仍保持高可解释性得分。

Conclusion: HierarchicalTopK目标填补了SAE设计中灵活性与可解释性之间的重要空白。

Abstract: Sparse Autoencoders (SAEs) have proven to be powerful tools for interpreting
neural networks by decomposing hidden representations into disentangled,
interpretable features via sparsity constraints. However, conventional SAEs are
constrained by the fixed sparsity level chosen during training; meeting
different sparsity requirements therefore demands separate models and increases
the computational footprint during both training and evaluation. We introduce a
novel training objective, \emph{HierarchicalTopK}, which trains a single SAE to
optimise reconstructions across multiple sparsity levels simultaneously.
Experiments with Gemma-2 2B demonstrate that our approach achieves
Pareto-optimal trade-offs between sparsity and explained variance,
outperforming traditional SAEs trained at individual sparsity levels. Further
analysis shows that HierarchicalTopK preserves high interpretability scores
even at higher sparsity. The proposed objective thus closes an important gap
between flexibility and interpretability in SAE design.

</details>


### [254] [Object Centric Concept Bottlenecks](https://arxiv.org/abs/2505.24492)
*David Steinmann,Wolfgang Stammer,Antonia Wüst,Kristian Kersting*

Key words: interpretability, concept-based models, object-centric, vision tasks, AI

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: OCB combines concept-based models with object-centric foundation models to enhance performance and interpretability in complex vision tasks.

Motivation: Addressing the limitations of traditional CBMs in handling complex, object-centric scenarios while maintaining interpretability.

Method: Introduces Object-Centric Concept Bottlenecks (OCB), leveraging pre-trained object-centric models and aggregating object-concept encodings.

Result: OCB outperforms traditional CBMs and enables interpretable decisions in complex visual tasks.

Conclusion: OCB successfully bridges the gap between performance and interpretability in AI models for complex vision tasks.

Abstract: Developing high-performing, yet interpretable models remains a critical
challenge in modern AI. Concept-based models (CBMs) attempt to address this by
extracting human-understandable concepts from a global encoding (e.g., image
encoding) and then applying a linear classifier on the resulting concept
activations, enabling transparent decision-making. However, their reliance on
holistic image encodings limits their expressiveness in object-centric
real-world settings and thus hinders their ability to solve complex vision
tasks beyond single-label classification. To tackle these challenges, we
introduce Object-Centric Concept Bottlenecks (OCB), a framework that combines
the strengths of CBMs and pre-trained object-centric foundation models,
boosting performance and interpretability. We evaluate OCB on complex image
datasets and conduct a comprehensive ablation study to analyze key components
of the framework, such as strategies for aggregating object-concept encodings.
The results show that OCB outperforms traditional CBMs and allows one to make
interpretable decisions for complex visual tasks.

</details>


### [255] [Efficient Neural and Numerical Methods for High-Quality Online Speech Spectrogram Inversion via Gradient Theorem](https://arxiv.org/abs/2505.24498)
*Andres Fernandez,Juan Azcarreta,Cagdas Bilen,Jesus Monge Alvarez*

Key words: 语音谱图反演, 深度学习, 三对角矩阵, 线性复杂度, 计算效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了三种创新方法，显著降低了在线语音谱图反演的计算成本，同时保持高质量，包括新型神经网络架构、增加延迟以降低推理成本，以及利用三对角矩阵特性的线性复杂度求解器。

Motivation: 旨在减少在线语音谱图反演的高计算成本，同时维持重建的高质量。

Method: 1. 提出仅含8k参数的新型神经网络架构；2. 通过增加延迟进一步降低神经推理步骤成本；3. 利用三对角矩阵特性设计线性复杂度求解器。

Result: 计算成本显著降低，神经网络参数减少30倍，求解器速度提升数个数量级。

Conclusion: 该方法在保持高质量的同时大幅提高了计算效率。

Abstract: Recent work in online speech spectrogram inversion effectively combines Deep
Learning with the Gradient Theorem to predict phase derivatives directly from
magnitudes. Then, phases are estimated from their derivatives via least
squares, resulting in a high quality reconstruction. In this work, we introduce
three innovations that drastically reduce computational cost, while maintaining
high quality: Firstly, we introduce a novel neural network architecture with
just 8k parameters, 30 times smaller than previous state of the art. Secondly,
increasing latency by 1 hop size allows us to further halve the cost of the
neural inference step. Thirdly, we we observe that the least squares problem
features a tridiagonal matrix and propose a linear-complexity solver for the
least squares step that leverages tridiagonality and positive-semidefiniteness,
achieving a speedup of several orders of magnitude. We release samples online.

</details>


### [256] [Learning to Optimally Dispatch Power: Performance on a Nation-Wide Real-World Dataset](https://arxiv.org/abs/2505.24505)
*Ignacio Boero,Santiago Diaz,Tomás Vázquez,Enzo Coppes,Pablo Belzarena,Federico Larroca*

Key words: 最优无功功率调度（ORPD）、机器学习、电力系统、真实数据集、可再生能源

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究基于机器学习的ORPD（最优无功功率调度）方法在真实电网数据下的表现，发现现有模型在真实数据上的预测误差显著增加，提出了新的公开数据集以促进相关研究。

Motivation: ORPD对电力系统的电压稳定和损耗最小化至关重要，但现有基于机器学习的优化方法主要基于合成数据，其在真实电网条件下的有效性尚未充分验证。

Method: 论文引入乌拉圭电网的真实数据集，包括两年运行数据，并评估学习型ORPD方法在真实数据下的表现。

Result: 发现从合成数据迁移到真实数据时，预测误差显著增加，表明现有模型难以适应真实电网的复杂统计特性。

Conclusion: 论文强调需要更具表达能力的模型架构，并提供了公开数据集以支持未来研究。

Abstract: The Optimal Reactive Power Dispatch (ORPD) problem plays a crucial role in
power system operations, ensuring voltage stability and minimizing power
losses. Recent advances in machine learning, particularly within the ``learning
to optimize'' framework, have enabled fast and efficient approximations of ORPD
solutions, typically by training models on precomputed optimization results.
While these approaches have demonstrated promising performance on synthetic
datasets, their effectiveness under real-world grid conditions remains largely
unexplored. This paper makes two key contributions. First, we introduce a
publicly available power system dataset that includes both the structural
characteristics of Uruguay's electrical grid and nearly two years of real-world
operational data, encompassing actual demand and generation profiles. Given
Uruguay's high penetration of renewable energy, the ORPD problem has become the
primary optimization challenge in its power network. Second, we assess the
impact of real-world data on learning-based ORPD solutions, revealing a
significant increase in prediction errors when transitioning from synthetic to
actual demand and generation inputs. Our results highlight the limitations of
existing models in learning under the complex statistical properties of real
grid conditions and emphasize the need for more expressive architectures. By
providing this dataset, we aim to facilitate further research into robust
learning-based optimization techniques for power system management.

</details>


### [257] [Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting](https://arxiv.org/abs/2505.24511)
*Jiahao Wang,Mingyue Cheng,Qi Liu*

Key words: 时间序列预测, 慢思考 LLM, 零样本学习, 条件推理, TimeReasoner

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: TimeReasoner 研究探讨了慢思考 LLM（如 ChatGPT-o1 和 DeepSeek-R1）在零样本条件下能否通过推理完成时间序列预测任务，发现其在捕捉高层次趋势和上下文变化方面表现不俗。

Motivation: 尽管现有时间序列预测方法（如深度学习和统计模型）有效，但缺乏显式的时间动态和上下文依赖推理。慢思考 LLM 展现了多步推理能力，是否能用于时间序列预测值得探索。

Method: 提出 TimeReasoner，将时间序列预测定义为条件推理任务，设计多种提示策略从预训练慢思考 LLM 中激发推理能力。

Result: 慢思考 LLM 在零样本条件下展现出非平凡的预测能力，尤其在捕捉高层次趋势和上下文变化方面。

Conclusion: 研究揭示了 LLM 在时间领域的推理潜力与局限，为基于推理的预测范式提供了初步探索方向。

Abstract: Time series forecasting (TSF) is a fundamental and widely studied task,
spanning methods from classical statistical approaches to modern deep learning
and multimodal language modeling. Despite their effectiveness, these methods
often follow a fast thinking paradigm emphasizing pattern extraction and direct
value mapping, while overlooking explicit reasoning over temporal dynamics and
contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g.,
ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning
capabilities across diverse domains, suggesting a new opportunity for reframing
TSF as a structured reasoning task. This motivates a key question: can
slow-thinking LLMs effectively reason over temporal patterns to support time
series forecasting, even in zero-shot manner? To investigate this, in this
paper, we propose TimeReasoner, an extensive empirical study that formulates
TSF as a conditional reasoning task. We design a series of prompting strategies
to elicit inference-time reasoning from pretrained slow-thinking LLMs and
evaluate their performance across diverse TSF benchmarks. Our findings reveal
that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities,
especially in capturing high-level trends and contextual shifts. While
preliminary, our study surfaces important insights into the reasoning behaviors
of LLMs in temporal domains highlighting both their potential and limitations.
We hope this work catalyzes further research into reasoning-based forecasting
paradigms and paves the way toward more interpretable and generalizable TSF
frameworks.

</details>


### [258] [Airborne Neural Network](https://arxiv.org/abs/2505.24513)
*Paritosh Ranjan,Surajit Majumder,Prodip Roy*

Key words: 深度学习, 机载神经网络, 航空航天, 实时数据处理, 分布式架构

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了机载神经网络的概念，通过分布式架构在多个机载设备上托管神经网络子集，实现实时学习和推理，以解决航空航天领域中实时数据处理和超低延迟的挑战。

Motivation: 深度学习的神经网络在人工智能领域取得了突破性进展，但在航空航天领域，由于基础设施限制，实时数据处理和超低延迟的需求尚未得到满足。因此，作者提出了机载神经网络的概念，以解决这一问题。

Method: 论文提出了一种分布式架构，即机载神经网络，其中多个机载设备各自托管神经网络的一部分神经元，并通过机载网络控制器和层特定控制器进行协同计算，实现实时学习和推理。

Result: 该方法有望在航空航天应用中实现革命性突破，包括机载空中交通管制、实时天气和地理预测以及动态地理空间数据处理。

Conclusion: 通过支持在机载环境中进行大规模神经网络操作，这项工作为下一代人工智能驱动的航空航天系统奠定了基础。

Abstract: Deep Learning, driven by neural networks, has led to groundbreaking
advancements in Artificial Intelligence by enabling systems to learn and adapt
like the human brain. These models have achieved remarkable results,
particularly in data-intensive domains, supported by massive computational
infrastructure. However, deploying such systems in Aerospace, where real time
data processing and ultra low latency are critical, remains a challenge due to
infrastructure limitations. This paper proposes a novel concept: the Airborne
Neural Network a distributed architecture where multiple airborne devices each
host a subset of neural network neurons. These devices compute collaboratively,
guided by an airborne network controller and layer specific controllers,
enabling real-time learning and inference during flight. This approach has the
potential to revolutionize Aerospace applications, including airborne air
traffic control, real-time weather and geographical predictions, and dynamic
geospatial data processing. By enabling large-scale neural network operations
in airborne environments, this work lays the foundation for the next generation
of AI powered Aerospace systems.

</details>


### [259] [Transformers Are Universally Consistent](https://arxiv.org/abs/2505.24531)
*Sagar Ghosh,Kushal Bose,Swagatam Das*

Key words: Transformer, 双曲空间, 功能回归, OLS, 非欧几里得几何

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了Transformer在双曲空间中执行普通最小二乘（OLS）回归的理论一致性，证明了其误差收敛率为$\mathcal{O}(t^{-1/2d})$，并验证了其在实际数据集中的表现。

Motivation: 探讨Transformer在非欧几里得几何下的功能回归能力，填补当前理论研究的空白。

Method: 使用基于softmax的非线性注意力机制，分析其在双曲空间中的OLS回归性能。

Result: 证明了Transformer在双曲空间中的一致性和误差收敛率，并在真实数据上验证了理论结果。

Conclusion: Transformer在非欧几里得几何下具备稳健的回归能力，为实际应用提供了理论支持。

Abstract: Despite their central role in the success of foundational models and
large-scale language modeling, the theoretical foundations governing the
operation of Transformers remain only partially understood. Contemporary
research has largely focused on their representational capacity for language
comprehension and their prowess in in-context learning, frequently under
idealized assumptions such as linearized attention mechanisms. Initially
conceived to model sequence-to-sequence transformations, a fundamental and
unresolved question is whether Transformers can robustly perform functional
regression over sequences of input tokens. This question assumes heightened
importance given the inherently non-Euclidean geometry underlying real-world
data distributions. In this work, we establish that Transformers equipped with
softmax-based nonlinear attention are uniformly consistent when tasked with
executing Ordinary Least Squares (OLS) regression, provided both the inputs and
outputs are embedded in hyperbolic space. We derive deterministic upper bounds
on the empirical error which, in the asymptotic regime, decay at a provable
rate of $\mathcal{O}(t^{-1/2d})$, where $t$ denotes the number of input tokens
and $d$ the embedding dimensionality. Notably, our analysis subsumes the
Euclidean setting as a special case, recovering analogous convergence
guarantees parameterized by the intrinsic dimensionality of the data manifold.
These theoretical insights are corroborated through empirical evaluations on
real-world datasets involving both continuous and categorical response
variables.

</details>


### [260] [Directional Non-Commutative Monoidal Structures with Interchange Law via Commutative Generators](https://arxiv.org/abs/2505.24533)
*Mahesh Godavarti*

Key words: 代数结构、高维组合、线性变换、信号处理、数据模态

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新颖的代数结构框架，将一维单子系统推广到高维，通过定义非交换性和全局交换律的多轴组合算子，统一了多种信号处理和数据分析的经典线性变换。

Motivation: 现有线性变换（如DFT、Walsh变换）缺乏统一的代数框架，限制了针对特定数据模态和任务的可学习变换的开发。

Method: 递归定义基于向量-矩阵对的复合结构，通过非交换性和全局交换律实现高维方向组合，保留结构的线性交换性。

Result: 该框架统一了DFT、Walsh变换等经典变换，并提供了通过选择向量-矩阵对系统化推导变换的方法。

Conclusion: 该框架不仅包含经典变换，还为开发针对特定任务的可学习变换奠定了基础。

Abstract: We introduce a novel framework consisting of a class of algebraic structures
that generalize one-dimensional monoidal systems into higher dimensions by
defining per-axis composition operators subject to non-commutativity and a
global interchange law. These structures, defined recursively from a base case
of vector-matrix pairs, model directional composition in multiple dimensions
while preserving structural coherence through commutative linear operators.
  We show that the framework that unifies several well-known linear transforms
in signal processing and data analysis. In this framework, data indices are
embedded into a composite structure that decomposes into simpler components. We
show that classic transforms such as the Discrete Fourier Transform (DFT), the
Walsh transform, and the Hadamard transform are special cases of our algebraic
structure. The framework provides a systematic way to derive these transforms
by appropriately choosing vector and matrix pairs. By subsuming classical
transforms within a common structure, the framework also enables the
development of learnable transformations tailored to specific data modalities
and tasks.

</details>


### [261] [HLSAD: Hodge Laplacian-based Simplicial Anomaly Detection](https://arxiv.org/abs/2505.24534)
*Florian Frantzen,Michael T. Schaub*

Key words: 时间演化单纯复形,异常检测,Hodge拉普拉斯,高阶交互

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于Hodge拉普拉斯谱性质的新方法HLSAD，用于检测时间演化单纯复形中的异常，优于传统图方法。

Motivation: 传统图异常检测方法难以捕捉高阶交互变化，而高阶交互对复杂结构异常的识别至关重要。

Method: 利用单纯复形的Hodge拉普拉斯谱性质建模数据点多向交互，引入高维单纯结构。

Result: 在合成和真实数据集上，HLSAD在检测事件和变化点方面优于现有图方法。

Conclusion: HLSAD通过捕捉高阶交互提升了异常检测精度和计算效率。

Abstract: In this paper, we propose HLSAD, a novel method for detecting anomalies in
time-evolving simplicial complexes. While traditional graph anomaly detection
techniques have been extensively studied, they often fail to capture changes in
higher-order interactions that are crucial for identifying complex structural
anomalies. These higher-order interactions can arise either directly from the
underlying data itself or through graph lifting techniques. Our approach
leverages the spectral properties of Hodge Laplacians of simplicial complexes
to effectively model multi-way interactions among data points. By incorporating
higher-dimensional simplicial structures into our method, our method enhances
both detection accuracy and computational efficiency. Through comprehensive
experiments on both synthetic and real-world datasets, we demonstrate that our
approach outperforms existing graph methods in detecting both events and change
points.

</details>


### [262] [Beyond Linear Steering: Unified Multi-Attribute Control for Language Models](https://arxiv.org/abs/2505.24535)
*Narmeen Oozeer,Luke Marks,Fazl Barez,Amirali Abdullah*

Key words: K-Steering, 大型语言模型, 行为控制, 非线性分类器, 梯度干预

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: K-Steering是一种非线性的多标签分类方法，通过在推理时计算梯度方向，实现了对大型语言模型（LLMs）多个行为属性的灵活控制，避免了线性假设和单独调参的需求。

Motivation: 由于行为属性间的干扰和线性控制方法的局限性（需假设激活空间的加法行为并需单独调参），需要一种更灵活的方法来控制LLMs的多个行为属性。

Method: K-Steering通过训练一个非线性多标签分类器来学习隐藏激活，并在推理时通过梯度计算干预方向，从而避免线性假设和单独调参。

Result: 在ToneBank和DebateMix两个新基准上的实验结果表明，K-Steering在准确控制多个行为属性方面优于基线方法。

Conclusion: K-Steering提供了一种统一且灵活的方案，能够在LLMs中动态组合行为属性，无需重新训练。

Abstract: Controlling multiple behavioral attributes in large language models (LLMs) at
inference time is a challenging problem due to interference between attributes
and the limitations of linear steering methods, which assume additive behavior
in activation space and require per-attribute tuning. We introduce K-Steering,
a unified and flexible approach that trains a single non-linear multi-label
classifier on hidden activations and computes intervention directions via
gradients at inference time. This avoids linearity assumptions, removes the
need for storing and tuning separate attribute vectors, and allows dynamic
composition of behaviors without retraining. To evaluate our method, we propose
two new benchmarks, ToneBank and DebateMix, targeting compositional behavioral
control. Empirical results across 3 model families, validated by both
activation-based classifiers and LLM-based judges, demonstrate that K-Steering
outperforms strong baselines in accurately steering multiple behaviors.

</details>


### [263] [Neuro-Symbolic Operator for Interpretable and Generalizable Characterization of Complex Piezoelectric Systems](https://arxiv.org/abs/2505.24578)
*Abhishek Chandra,Taniya Kapoor,Mitrofan Curti,Koen Tiels,Elena A. Lomonova*

Key words: 压电系统、神经符号算子、傅里叶神经算子、模型发现、滞后关系

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一个神经符号算子（NSO）框架，结合了神经网络和符号模型，以解决压电系统中非线性电压-位移滞后关系的表征问题，提升了模型的解释性和泛化能力。

Motivation: 当前最先进的神经算子是黑箱模型，缺乏解释性，且在训练域外电压和位移预测方面泛化能力有限，限制了其实际应用。

Method: NSO框架首先学习将电压场映射到位移轮廓的傅里叶神经算子，随后采用基于库的稀疏模型发现方法，生成表征滞后的白箱简洁模型。

Result: NSO能够准确预测包括蝶形关系在内的电压-位移滞后，且对噪声和低质量电压数据表现出鲁棒性，其性能优于现有方法。

Conclusion: NSO不仅提升了压电系统的表征能力，还改善了神经算子的解释性和泛化性，适用于设计、监测和维护等实际场景。

Abstract: Complex piezoelectric systems are foundational in industrial applications.
Their performance, however, is challenged by the nonlinear voltage-displacement
hysteretic relationships. Efficient characterization methods are, therefore,
essential for reliable design, monitoring, and maintenance. Recently proposed
neural operator methods serve as surrogates for system characterization but
face two pressing issues: interpretability and generalizability.
State-of-the-art (SOTA) neural operators are black-boxes, providing little
insight into the learned operator. Additionally, generalizing them to novel
voltages and predicting displacement profiles beyond the training domain is
challenging, limiting their practical use. To address these limitations, this
paper proposes a neuro-symbolic operator (NSO) framework that derives the
analytical operators governing hysteretic relationships. NSO first learns a
Fourier neural operator mapping voltage fields to displacement profiles,
followed by a library-based sparse model discovery method, generating white-box
parsimonious models governing the underlying hysteresis. These models enable
accurate and interpretable prediction of displacement profiles across varying
and out-of-distribution voltage fields, facilitating generalizability. The
potential of NSO is demonstrated by accurately predicting voltage-displacement
hysteresis, including butterfly-shaped relationships. Moreover, NSO predicts
displacement profiles even for noisy and low-fidelity voltage data, emphasizing
its robustness. The results highlight the advantages of NSO compared to SOTA
neural operators and model discovery methods on several evaluation metrics.
Consequently, NSO contributes to characterizing complex piezoelectric systems
while improving the interpretability and generalizability of neural operators,
essential for design, monitoring, maintenance, and other real-world scenarios.

</details>


### [264] [Conservation-preserved Fourier Neural Operator through Adaptive Correction](https://arxiv.org/abs/2505.24579)
*Chaoyu Liu,Yangming Li,Zhongying Deng,Chris Budd,Carola-Bibiane Schönlieb*

Key words: 傅里叶神经算子, 偏微分方程, 守恒定律, 自适应校正, 机器学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种自适应校正方法，用于确保傅里叶神经算子在解决偏微分方程时满足守恒定律，比现有方法表现更好。

Motivation: 标准傅里叶神经算子（FNO）无法精确满足质量、动量等守恒定律，现有方法在训练时通过设计损失函数或后处理也无法完全解决问题。

Method: 提出一种自适应校正方法，引入可学习矩阵在训练时动态调整解以满足守恒定律。

Result: 实验证明，该方法在多种典型偏微分方程上优于现有方法，并且能精确满足目标守恒定律。

Conclusion: 该方法为傅里叶神经算子提供了一种灵活且精确的守恒校正手段，性能显著提升。

Abstract: Fourier Neural Operators (FNOs) have recently emerged as a promising and
efficient approach for learning the numerical solutions to partial differential
equations (PDEs) from data. However, standard FNO often fails to preserve key
conservation laws, such as mass conservation, momentum conservation, norm
conservation, etc., which are crucial for accurately modeling physical systems.
Existing methods for incorporating these conservation laws into Fourier neural
operators are achieved by designing related loss function or incorporating
post-processing method at the training time. None of them can both exactly and
adaptively correct the outputs to satisfy conservation laws, and our
experiments show that these methods can lead to inferior performance while
preserving conservation laws. In this work, we propose a novel adaptive
correction approach to ensure the conservation of fundamental quantities. Our
method introduces a learnable matrix to adaptively adjust the solution to
satisfy the conservation law during training. It ensures that the outputs
exactly satisfy the goal conservation law and allow for more flexibility and
adaptivity for the model to correct the outputs. We theoretically show that
applying our adaptive correction to an unconstrained FNO yields a solution with
data loss no worse than that of the best conservation-satisfying FNO. We
compare our approach with existing methods on a range of representative PDEs.
Experiment results show that our method consistently outperform other methods.

</details>


### [265] [AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams](https://arxiv.org/abs/2505.24584)
*Sakhinana Sagar Srinivas,Shivam Gupta,Venkataramana Runkana*

Key words: 生成式AI, 化工设计, PFDs, PIDs, 小规模语言模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种自动化生成工业可行PFDs和PIDs的闭环物理感知框架，整合了小规模语言模型和第一性原理模拟，显著缩短了从实验室发现到工业部署的研发时间。

Motivation: 当前AI方法无法自动生成符合工程约束的PFDs或PIDs，阻碍了化学发现向工业规模生产的转化。

Method: 框架结合了层次化知识图谱、多阶段训练管道（SFT、DPO、RAIT）和DWSIM模拟器验证，并采用了多种推理优化和剪枝技术。

Result: 实验证明该框架能高保真生成模拟器验证的过程描述，优于基线方法，并能泛化到未见化学品。

Conclusion: 该工作通过将AI驱动设计与工业规模可行性结合，显著缩短了研发时间。

Abstract: Recent advancements in generative AI have accelerated the discovery of novel
chemicals and materials; however, transitioning these discoveries to
industrial-scale production remains a critical bottleneck, as it requires the
development of entirely new chemical manufacturing processes. Current AI
methods cannot auto-generate PFDs or PIDs, despite their critical role in
scaling chemical processes, while adhering to engineering constraints. We
present a closed loop, physics aware framework for the automated generation of
industrially viable PFDs and PIDs. The framework integrates domain specialized
small scale language models (SLMs) (trained for chemical process QA tasks) with
first principles simulation, leveraging three key components: (1) a
hierarchical knowledge graph of process flow and instrumentation descriptions
for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes
domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),
Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction
Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure
feasibility. To improve both runtime efficiency and model compactness, the
framework incorporates advanced inference time optimizations including
FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,
and Test Time Inference Scaling and independently applies structural pruning
techniques (width and depth) guided by importance heuristics to reduce model
size with minimal accuracy loss. Experiments demonstrate that the framework
generates simulator-validated process descriptions with high fidelity,
outperforms baseline methods in correctness, and generalizes to unseen
chemicals. By bridging AI-driven design with industrial-scale feasibility, this
work significantly reduces R&D timelines from lab discovery to plant
deployment.

</details>


### [266] [A Flat Minima Perspective on Understanding Augmentations and Model Robustness](https://arxiv.org/abs/2505.24592)
*Weebum Yoo,Sung Whan Yoon*

Key words: 模型鲁棒性, 数据增强, 损失曲面平坦性, PAC泛化边界, 分布偏移

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一个统一的理论框架，通过损失曲面平坦性和PAC泛化边界，解释了数据增强如何提高模型鲁棒性。该框架涵盖了多种现有增强方法，并适用于不同类型的分布偏移。实验验证基于CIFAR、ImageNet、PACS和OfficeHome数据集。

Motivation: 缺乏对数据增强提升模型鲁棒性的通用理论理解，需要为多种增强方法和分布偏移提供统一解释。

Method: 采用损失曲面平坦性和PAC泛化边界的理论视角，构建统一框架。

Result: 理论分析与实验结果一致，证明了增强方法在提高模型鲁棒性方面的有效性。

Conclusion: 提供了一个广泛适用的理论框架，阐明了数据增强如何提升模型鲁棒性，并通过实验验证了其有效性。

Abstract: Model robustness indicates a model's capability to generalize well on
unforeseen distributional shifts, including data corruption, adversarial
attacks, and domain shifts. Data augmentation is one of the prevalent and
effective ways to enhance robustness. Despite the great success of
augmentations in different fields, a general theoretical understanding of their
efficacy in improving model robustness is lacking. We offer a unified
theoretical framework to clarify how augmentations can enhance model robustness
through the lens of loss surface flatness and PAC generalization bound. Our
work diverges from prior studies in that our analysis i) broadly encompasses
much of the existing augmentation methods, and ii) is not limited to specific
types of distribution shifts like adversarial attacks. We confirm our theories
through simulations on the existing common corruption and adversarial
robustness benchmarks based on the CIFAR and ImageNet datasets, as well as
domain generalization benchmarks including PACS and OfficeHome.

</details>


### [267] [Binary Cumulative Encoding meets Time Series Forecasting](https://arxiv.org/abs/2505.24595)
*Andrei Chernov,Vitaliy Pozdnyakov,Ilya Makarov*

Key words: 时间序列预测, 二进制累积编码, 卷积神经网络, 点预测, 概率预测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种新的时间序列预测方法，通过二进制累积编码（BCE）改进分类任务中的回归问题，结合卷积神经网络架构，在点预测和概率预测中表现优于现有方法。

Motivation: 现有方法在时间序列预测中通常使用一热编码，忽略了目标值的顺序结构，无法捕捉预测值与真实值的相对距离。为了解决这一问题，作者提出了二进制累积编码（BCE），以保留顺序和幅度信息。

Method: 采用二进制累积编码（BCE）将标量目标转换为单调二进制向量，并结合残差和扩张卷积的卷积神经网络架构，以实现快速的时序建模。

Result: 在多个基准预测数据集上的实验表明，该方法在点预测和概率预测上均优于现有方法，同时参数更少、训练速度更快。

Conclusion: BCE编码结合专用CNN架构，显著提升了时间序列预测的性能和效率。

Abstract: Recent studies in time series forecasting have explored formulating
regression via classification task. By discretizing the continuous target space
into bins and predicting over a fixed set of classes, these approaches benefit
from stable training, robust uncertainty modeling, and compatibility with
modern deep learning architectures. However, most existing methods rely on
one-hot encoding that ignores the inherent ordinal structure of the underlying
values. As a result, they fail to provide information about the relative
distance between predicted and true values during training. In this paper, we
propose to address this limitation by introducing binary cumulative encoding
(BCE), that represents scalar targets into monotonic binary vectors. This
encoding implicitly preserves order and magnitude information, allowing the
model to learn distance-aware representations while still operating within a
classification framework. We propose a convolutional neural network
architecture specifically designed for BCE, incorporating residual and dilated
convolutions to enable fast and expressive temporal modeling. Through extensive
experiments on benchmark forecasting datasets, we show that our approach
outperforms widely used methods in both point and probabilistic forecasting,
while requiring fewer parameters and enabling faster training.

</details>


### [268] [The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian Sketches](https://arxiv.org/abs/2505.24603)
*Omri Lev,Vishwak Srinivasan,Moshe Shenfeld,Katrina Ligett,Ayush Sekhari,Ashia C. Wilson*

Key words: 高斯素描,Renyi差分隐私,线性回归,数据科学

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 通过Renyi差分隐私（RDP）改进高斯素描的隐私分析，获得更严格的隐私界限，并在不同线性回归场景中验证其性能提升。

Motivation: 高斯素描广泛用于数据科学和机器学习，但其隐私分析可进一步优化，以提高实用性和效率。

Method: 使用Renyi差分隐私框架重新分析高斯素描的隐私性，获得更紧的隐私界限，并应用于线性回归问题。

Result: 理论验证了隐私界限的优化，实验显示在多数据集上性能提升，部分案例中减少了运行时间。

Conclusion: 通过优化隐私分析，高斯素描在实际应用中的效用和效率均得到提升。

Abstract: Gaussian sketching, which consists of pre-multiplying the data with a random
Gaussian matrix, is a widely used technique for multiple problems in data
science and machine learning, with applications spanning computationally
efficient optimization, coded computing, and federated learning. This operation
also provides differential privacy guarantees due to its inherent randomness.
In this work, we revisit this operation through the lens of Renyi Differential
Privacy (RDP), providing a refined privacy analysis that yields significantly
tighter bounds than prior results. We then demonstrate how this improved
analysis leads to performance improvement in different linear regression
settings, establishing theoretical utility guarantees. Empirically, our methods
improve performance across multiple datasets and, in several cases, reduce
runtime.

</details>


### [269] [Multi-criteria Rank-based Aggregation for Explainable AI](https://arxiv.org/abs/2505.24612)
*Sujoy Chatterjee,Everton Romanzini Colombo,Marcos Medeiros Raimundo*

Key words: 可解释性, XAI, 多标准决策, 排名聚合, TOPSIS

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种基于多标准决策方法的排名加权聚合方法，以平衡不同解释质量指标，减少解释模型的变异性。

Motivation: 提高黑盒机器学习模型的可解释性，解决不同解释器在同一预测下提供矛盾解释的问题。

Method: 引入基于排名的加权聚合方法，并改进XAI指标（复杂性、忠实性和稳定性）以评估特征重要性。

Result: 实验表明，所提模型在多个指标上表现稳健，TOPSIS和WSUM是此场景下的最佳算法。

Conclusion: 多标准决策方法能有效平衡解释质量指标，提升解释模型的鲁棒性。

Abstract: Explainability is crucial for improving the transparency of black-box machine
learning models. With the advancement of explanation methods such as LIME and
SHAP, various XAI performance metrics have been developed to evaluate the
quality of explanations. However, different explainers can provide contrasting
explanations for the same prediction, introducing trade-offs across conflicting
quality metrics. Although available aggregation approaches improve robustness,
reducing explanations' variability, very limited research employed a
multi-criteria decision-making approach. To address this gap, this paper
introduces a multi-criteria rank-based weighted aggregation method that
balances multiple quality metrics simultaneously to produce an ensemble of
explanation models. Furthermore, we propose rank-based versions of existing XAI
metrics (complexity, faithfulness and stability) to better evaluate ranked
feature importance explanations. Extensive experiments on publicly available
datasets demonstrate the robustness of the proposed model across these metrics.
Comparative analyses of various multi-criteria decision-making and rank
aggregation algorithms showed that TOPSIS and WSUM are the best candidates for
this use case.

</details>


### [270] [Hyperbolic Dataset Distillation](https://arxiv.org/abs/2505.24623)
*Wenyuan Li,Guang Li,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Key words: dataset distillation, hyperbolic space, distribution matching, hierarchical structure, core set pruning

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新颖的双曲数据集蒸馏方法HDD，通过将数据嵌入双曲空间来捕捉复杂的层次和几何关系，从而提升蒸馏效率和模型性能。

Motivation: 传统分布匹配方法仅适用于欧几里得空间且忽略数据间的层次关系，限制了其在复杂数据中的应用。HDD旨在通过双曲空间更好地建模数据间的层次结构。

Method: HDD将浅层网络提取的特征嵌入Lorentz双曲空间，通过优化合成数据与原始数据的双曲（测地）距离来整合层次结构，并利用核心集剪枝提升训练稳定性。

Result: 实验表明，HDD在多个数据集上表现优异，仅需20%的核心集即可保持模型性能，且显著提升训练稳定性。

Conclusion: HDD通过双曲空间有效地整合了数据的层次结构，为数据集蒸馏提供了一种高效且兼容性强的解决方案。

Abstract: To address the computational and storage challenges posed by large-scale
datasets in deep learning, dataset distillation has been proposed to synthesize
a compact dataset that replaces the original while maintaining comparable model
performance. Unlike optimization-based approaches that require costly bi-level
optimization, distribution matching (DM) methods improve efficiency by aligning
the distributions of synthetic and original data, thereby eliminating nested
optimization. DM achieves high computational efficiency and has emerged as a
promising solution. However, existing DM methods, constrained to Euclidean
space, treat data as independent and identically distributed points,
overlooking complex geometric and hierarchical relationships. To overcome this
limitation, we propose a novel hyperbolic dataset distillation method, termed
HDD. Hyperbolic space, characterized by negative curvature and exponential
volume growth with distance, naturally models hierarchical and tree-like
structures. HDD embeds features extracted by a shallow network into the Lorentz
hyperbolic space, where the discrepancy between synthetic and original data is
measured by the hyperbolic (geodesic) distance between their centroids. By
optimizing this distance, the hierarchical structure is explicitly integrated
into the distillation process, guiding synthetic samples to gravitate towards
the root-centric regions of the original data distribution while preserving
their underlying geometric characteristics. Furthermore, we find that pruning
in hyperbolic space requires only 20% of the distilled core set to retain model
performance, while significantly improving training stability. Notably, HDD is
seamlessly compatible with most existing DM methods, and extensive experiments
on different datasets validate its effectiveness.

</details>


### [271] [Rethinking Neural Combinatorial Optimization for Vehicle Routing Problems with Different Constraint Tightness Degrees](https://arxiv.org/abs/2505.24627)
*Fu Luo,Yaoxin Wu,Zhi Zheng,Zhenkun Wang*

Key words: 神经组合优化, 容量约束车辆路径问题, 约束紧度, 多专家模块, 过拟合

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文通过容量约束车辆路径问题（CVRP）分析了不同约束紧度对神经组合优化（NCO）方法性能的影响，发现现有方法易过拟合，并提出了一种考虑约束紧度变化的训练方案和多专家模块，显著提升了性能。

Motivation: 现有神经组合优化方法在固定约束值下表现良好，但缺乏对不同约束紧度下性能的研究，导致在实际应用中泛化能力不足。

Method: 采用多专家模块和考虑约束紧度变化的训练方案，以提升模型在CVRP和CVRPTW中的适应性。

Result: 提出的方法有效克服了过拟合问题，在多种约束紧度下表现出色。

Conclusion: 新的训练方案和多专家模块显著提高了NCO方法在动态约束环境中的适应性和性能。

Abstract: Recent neural combinatorial optimization (NCO) methods have shown promising
problem-solving ability without requiring domain-specific expertise. Most
existing NCO methods use training and testing data with a fixed constraint
value and lack research on the effect of constraint tightness on the
performance of NCO methods. This paper takes the capacity-constrained vehicle
routing problem (CVRP) as an example to empirically analyze the NCO performance
under different tightness degrees of the capacity constraint. Our analysis
reveals that existing NCO methods overfit the capacity constraint, and they can
only perform satisfactorily on a small range of the constraint values but
poorly on other values. To tackle this drawback of existing NCO methods, we
develop an efficient training scheme that explicitly considers varying degrees
of constraint tightness and proposes a multi-expert module to learn a generally
adaptable solving strategy. Experimental results show that the proposed method
can effectively overcome the overfitting issue, demonstrating superior
performances on the CVRP and CVRP with time windows (CVRPTW) with various
constraint tightness degrees.

</details>


### [272] [Stop Guessing: Optimizing Goalkeeper Policies for Soccer Penalty Kicks](https://arxiv.org/abs/2505.24629)
*Lotte Bransen,Tim Janssen,Jesse Davis*

Key words: 罚球分析, 守门员策略, 数据科学, 相互依赖性, 模拟框架

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一个球员无关的模拟框架，用于评估不同守门员策略的效果，考虑了守门员技能和丰富的选择集。

Motivation: 分析罚球时球员选择的相互依赖性，解决现有研究中独立选择的简化假设问题。

Method: 基于专家标注的罚球大型数据集，开发了一个考虑守门员技能和策略的模拟框架。

Result: 展示如何使用该框架优化实际比赛中的守门员策略。

Conclusion: 提出了一个更符合实际情况的球员策略评估方法，有助于提升守门员的实战表现。

Abstract: Penalties are fraught and game-changing moments in soccer games that teams
explicitly prepare for. Consequently, there has been substantial interest in
analyzing them in order to provide advice to practitioners. From a data science
perspective, such analyses suffer from a significant limitation: they make the
unrealistic simplifying assumption that goalkeepers and takers select their
action -- where to dive and where to the place the kick -- independently of
each other. In reality, the choices that some goalkeepers make depend on the
taker's movements and vice-versa. This adds substantial complexity to the
problem because not all players have the same action capacities, that is, only
some players are capable of basing their decisions on their opponent's
movements. However, the small sample sizes on the player level mean that one
may have limited insights into a specific opponent's capacities. We address
these challenges by developing a player-agnostic simulation framework that can
evaluate the efficacy of different goalkeeper strategies. It considers a rich
set of choices and incorporates information about a goalkeeper's skills. Our
work is grounded in a large dataset of penalties that were annotated by penalty
experts and include aspects of both kicker and goalkeeper strategies. We show
how our framework can be used to optimize goalkeeper policies in real-world
situations.

</details>


### [273] [WILTing Trees: Interpreting the Distance Between MPNN Embeddings](https://arxiv.org/abs/2505.24642)
*Masahiro Negishi,Thomas Gärtner,Pascal Welke*

Key words: 消息传递神经网络, 功能距离, Weisfeiler Leman标记树, 最优运输, 图核

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了消息传递神经网络（MPNNs）在特定任务中学习的距离函数，提出了通过最优运输在Weisfeiler Leman标记树上蒸馏MPNN嵌入距离的方法。

Motivation: 前人研究未考虑任务特定信息，本文旨在填补这一空白，捕捉MPNNs隐式学习的功能距离。

Method: 使用Weisfeiler Leman标记树的最优运输方法，蒸馏MPNN嵌入距离为可解释的图距离。

Result: 实验表明，MPNNs通过关注功能重要的子图来定义嵌入的相对位置，方法线性时间可计算且泛化了两种经典图核。

Conclusion: MPNNs学习的功能距离能有效捕捉任务相关的子图结构，方法兼具解释性和高效性。

Abstract: We investigate the distance function learned by message passing neural
networks (MPNNs) in specific tasks, aiming to capture the functional distance
between prediction targets that MPNNs implicitly learn. This contrasts with
previous work, which links MPNN distances on arbitrary tasks to structural
distances on graphs that ignore task-specific information. To address this gap,
we distill the distance between MPNN embeddings into an interpretable graph
distance. Our method uses optimal transport on the Weisfeiler Leman Labeling
Tree (WILT), where the edge weights reveal subgraphs that strongly influence
the distance between embeddings. This approach generalizes two well-known graph
kernels and can be computed in linear time. Through extensive experiments, we
demonstrate that MPNNs define the relative position of embeddings by focusing
on a small set of subgraphs that are known to be functionally important in the
domain.

</details>


### [274] [Learning Distributions over Permutations and Rankings with Factorized Representations](https://arxiv.org/abs/2505.24664)
*Daniel Severo,Brian Karrer,Niklas Nolte*

Key words: 排列学习, Lehmer码, 深度学习, 概率分布, 评估基准

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种利用排列的替代表示（如Lehmer码）进行无约束学习的新方法，在表达性和计算效率之间取得平衡，显著优于现有方法。

Motivation: 学习排列分布对排名、组合优化等任务很重要，现有方法依赖参数化混合或昂贵变分推断，需要更高效灵活的解决方案。

Method: 利用Lehmer码等排列的替代表示，通过深度学习技术实现无约束学习，覆盖任意排列概率分布。

Result: 在拼图等任务中显著优于现有方法，且在表达性最低模式下仍能学习非平凡分布。

Conclusion: 新方法在表达性和效率间取得平衡，扩展了排列学习的应用范围，并提出了更全面的评估基准。

Abstract: Learning distributions over permutations is a fundamental problem in machine
learning, with applications in ranking, combinatorial optimization, structured
prediction, and data association. Existing methods rely on mixtures of
parametric families or neural networks with expensive variational inference
procedures. In this work, we propose a novel approach that leverages
alternative representations for permutations, including Lehmer codes,
Fisher-Yates draws, and Insertion-Vectors. These representations form a
bijection with the symmetric group, allowing for unconstrained learning using
conventional deep learning techniques, and can represent any probability
distribution over permutations. Our approach enables a trade-off between
expressivity of the model family and computational requirements. In the least
expressive and most computationally efficient case, our method subsumes
previous families of well established probabilistic models over permutations,
including Mallow's and the Repeated Insertion Model. Experiments indicate our
method significantly outperforms current approaches on the jigsaw puzzle
benchmark, a common task for permutation learning. However, we argue this
benchmark is limited in its ability to assess learning probability
distributions, as the target is a delta distribution (i.e., a single correct
solution exists). We therefore propose two additional benchmarks: learning
cyclic permutations and re-ranking movies based on user preference. We show
that our method learns non-trivial distributions even in the least expressive
mode, while traditional models fail to even generate valid permutations in this
setting.

</details>


### [275] [Learning geometry and topology via multi-chart flows](https://arxiv.org/abs/2505.24665)
*Hanlin Yu,Søren Hauberg,Marcelo Hartmann,Arto Klami,Georgios Arvanitidis*

Key words: 黎曼流形,归一化流,拓扑估计,测地线,低维表示

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种学习低维黎曼流形上的退化归一化流的方法，并通过多流组合解决非平凡拓扑问题，同时开发了数值算法计算流形上的测地线，显著提升了拓扑估计效果。

Motivation: 现实世界数据通常存在于嵌入高维空间的低维黎曼流形中，但现有方法无法正确处理具有非平凡拓扑的流形。因此，需要开发一种方法，既能学习流形的低维表示，又能解决拓扑复杂性。

Method: 提出了一种通用的多流组合训练方案，用于学习低维流形上的退化归一化流；同时开发了计算此类流形上测地线的数值算法。

Result: 实验表明，该方法显著提升了流形拓扑估计的准确性。

Conclusion: 通过多流组合和测地线计算算法，能够有效处理非平凡拓扑的低维流形学习，为复杂数据的表示提供了新思路。

Abstract: Real world data often lie on low-dimensional Riemannian manifolds embedded in
high-dimensional spaces. This motivates learning degenerate normalizing flows
that map between the ambient space and a low-dimensional latent space. However,
if the manifold has a non-trivial topology, it can never be correctly learned
using a single flow. Instead multiple flows must be `glued together'. In this
paper, we first propose the general training scheme for learning such a
collection of flows, and secondly we develop the first numerical algorithms for
computing geodesics on such manifolds. Empirically, we demonstrate that this
leads to highly significant improvements in topology estimation.

</details>


### [276] [Predicting the Past: Estimating Historical Appraisals with OCR and Machine Learning](https://arxiv.org/abs/2505.24676)
*Mihir Bhaskar,Jun Tao Luo,Zihan Geng,Asmita Hajra,Junia Howell,Matthew R. Gormley*

Key words: 种族财富差距,历史房产评估,OCR,回归模型,红线政策

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种方法，用于数字化历史房产评估数据，以分析1930年代美国住房政策对种族财富差距的财务影响。

Motivation: 研究量化1930年代美国住房政策对种族财富差距的具体财务影响，因历史房产评估记录难以获取而挑战重重。

Method: 结合公开扫描文档和手动标注的房产卡片，训练OCR技术；对无法使用OCR的情况，采用基于建筑特征的回归模型估算历史价值。

Result: 成功构建并发布了一个县的房产数据集，验证了方法的可行性和扩展性。

Conclusion: 该工具为学者、社区活动家和政策制定者提供了量化历史红线政策影响的有效手段。

Abstract: Despite well-documented consequences of the U.S. government's 1930s housing
policies on racial wealth disparities, scholars have struggled to quantify its
precise financial effects due to the inaccessibility of historical property
appraisal records. Many counties still store these records in physical formats,
making large-scale quantitative analysis difficult. We present an approach
scholars can use to digitize historical housing assessment data, applying it to
build and release a dataset for one county. Starting from publicly available
scanned documents, we manually annotated property cards for over 12,000
properties to train and validate our methods. We use OCR to label data for an
additional 50,000 properties, based on our two-stage approach combining
classical computer vision techniques with deep learning-based OCR. For cases
where OCR cannot be applied, such as when scanned documents are not available,
we show how a regression model based on building feature data can estimate the
historical values, and test the generalizability of this model to other
counties. With these cost-effective tools, scholars, community activists, and
policy makers can better analyze and understand the historical impacts of
redlining.

</details>


### [277] [Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs](https://arxiv.org/abs/2505.24684)
*Zihao Chen,Yu Xiang,Wenyong Wang*

Key words: 变分自编码器，无监督解缠，归纳偏置，解缠粒度，β-TCVAE，β-STCVAE。

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文探讨了变分自编码器（VAE）及其变体在学习语义上有意义的无监督解缠表示时的理论挑战，揭示了隐含的归纳偏置“解缠粒度”对解缠性能的影响，并通过实验验证了其重要性。

Motivation: 研究动机在于探索VAE中隐含的归纳偏置，特别是因子分解先验下的解缠机制，以解决无监督解缠的理论难题。

Method: 通过分析β-TCVAE的总相关性，提出“解缠粒度”的概念，并开发了β-STCVAE模型，通过调整解缠粒度扩展解缠表示的范围。

Result: 实验结果显示，传统因子分解VAE因固定解缠粒度倾向于解缠低复杂度特征，而β-STCVAE能解缠高复杂度特征。

Conclusion: 揭示了“解缠粒度”作为隐含归纳偏置对VAE解缠性能和ELBO推断的影响，为VAE的可解释性和内在偏置提供了新见解。

Abstract: Despite the success in learning semantically meaningful, unsupervised
disentangled representations, variational autoencoders (VAEs) and their
variants face a fundamental theoretical challenge: substantial evidence
indicates that unsupervised disentanglement is unattainable without implicit
inductive bias, yet such bias remains elusive. In this work, we focus on
exploring the implicit inductive bias that drive disentanglement in VAEs with
factorization priors. By analyzing the total correlation in \b{eta}-TCVAE, we
uncover a crucial implicit inductive bias called disentangling granularity,
which leads to the discovery of an interesting "V"-shaped optimal Evidence
Lower Bound (ELBO) trajectory within the parameter space. This finding is
validated through over 100K experiments using factorized VAEs and our newly
proposed model, \b{eta}-STCVAE. Notably, experimental results reveal that
conventional factorized VAEs, constrained by fixed disentangling granularity,
inherently tend to disentangle low-complexity feature. Whereas, appropriately
tuning disentangling granularity, as enabled by \b{eta}-STCVAE, broadens the
range of disentangled representations, allowing for the disentanglement of
high-complexity features. Our findings unveil that disentangling granularity as
an implicit inductive bias in factorized VAEs influence both disentanglement
performance and the inference of the ELBO, offering fresh insights into the
interpretability and inherent biases of VAEs.

</details>


### [278] [Quick-Draw Bandits: Quickly Optimizing in Nonstationary Environments with Extremely Many Arms](https://arxiv.org/abs/2505.24692)
*Derek Everett,Fred Lu,Edward Raff,Fernando Camacho,James Holt*

Key words: 多臂老虎机, 高斯插值, 非平稳, 连续空间, 累计遗憾

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于高斯插值的新策略，用于学习连续空间中的奖励环境，支持非平稳问题和大量臂的情况，计算效率高且实验表现优异。

Motivation: 传统多臂老虎机算法假设奖励环境平稳且臂数较少，现有方法通常只放宽其中一个假设。本文旨在同时处理非平稳环境和大量臂的情况。

Method: 使用高斯插值学习连续Lipschitz奖励函数，并扩展至非平稳问题。

Result: 方法实现了$	ext{O}^*(\sqrt{T})$累计遗憾，计算速度快100-10000倍，实验表现优于滑动高斯过程策略。

Conclusion: 本文提出的策略在非平稳和大量臂场景下高效且性能优越，具有实际应用潜力。

Abstract: Canonical algorithms for multi-armed bandits typically assume a stationary
reward environment where the size of the action space (number of arms) is
small. More recently developed methods typically relax only one of these
assumptions: existing non-stationary bandit policies are designed for a small
number of arms, while Lipschitz, linear, and Gaussian process bandit policies
are designed to handle a large (or infinite) number of arms in stationary
reward environments under constraints on the reward function. In this
manuscript, we propose a novel policy to learn reward environments over a
continuous space using Gaussian interpolation. We show that our method
efficiently learns continuous Lipschitz reward functions with
$\mathcal{O}^*(\sqrt{T})$ cumulative regret. Furthermore, our method naturally
extends to non-stationary problems with a simple modification. We finally
demonstrate that our method is computationally favorable (100-10000x faster)
and experimentally outperforms sliding Gaussian process policies on datasets
with non-stationarity and an extremely large number of arms.

</details>


### [279] [On Symmetric Losses for Robust Policy Optimization with Noisy Preferences](https://arxiv.org/abs/2505.24709)
*Soichiro Nishimori,Yu-Jie Zhang,Thanawat Lodkaew,Masashi Sugiyama*

Key words: 奖励建模, 强化学习, 偏好优化, 噪声鲁棒性, 对称损失

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种名为SymPO的鲁棒性偏好优化框架，通过对称损失处理带噪声的偏好数据，提升策略优化的稳定性。

Motivation: 现实世界中的偏好数据常因人为错误或偏见包含噪声，传统方法假设标注准确，难以应对此类情况。

Method: 将奖励建模视为分类问题，利用对称损失（对标签噪声具有鲁棒性）设计Symmetric Preference Optimization (SymPO)方法。

Result: 理论证明对称损失在噪声标签下仍能保持奖励的秩次不变性，实验验证了SymPO在合成和真实任务中的有效性。

Conclusion: SymPO为噪声偏好数据下的策略优化提供了可靠解决方案。

Abstract: Optimizing policies based on human preferences is key to aligning language
models with human intent. This work focuses on reward modeling, a core
component in reinforcement learning from human feedback (RLHF), and offline
preference optimization, such as direct preference optimization. Conventional
approaches typically assume accurate annotations. However, real-world
preference data often contains noise due to human errors or biases. We propose
a principled framework for robust policy optimization under noisy preferences,
viewing reward modeling as a classification problem. This allows us to leverage
symmetric losses, known for their robustness to label noise in classification,
leading to our Symmetric Preference Optimization (SymPO) method. We prove that
symmetric losses enable successful policy optimization even under noisy labels,
as the resulting reward remains rank-preserving -- a property sufficient for
policy improvement. Experiments on synthetic and real-world tasks demonstrate
the effectiveness of SymPO.

</details>


### [280] [Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting](https://arxiv.org/abs/2505.24710)
*Wei Chen,Jiahao Zhang,Haipeng Zhu,Boyan Xu,Zhifeng Hao,Keli Zhang,Junjian Ye,Ruichu Cai*

Key words: 大语言模型, 因果关系, 结构因果模型, 决策, 强化学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出因果感知LLMs，通过整合结构因果模型（SCM）改进大语言模型（LLMs）的决策能力，分三个阶段（学习、适应、行动）优化模型在复杂任务中的应用。

Motivation: LLMs在决策中表现出潜力，但缺乏推理能力且难以适应新环境，限制了其在复杂现实任务中的应用。

Method: 提出Causal-aware LLMs，结合SCM，分三个阶段：1）学习阶段提取环境因果实体与关系；2）适应阶段通过因果干预更新模型；3）行动阶段利用因果知识进行强化学习决策。

Result: 在开放世界游戏'Crafter'的22个任务中验证了方法的有效性。

Conclusion: 因果感知LLMs能更准确地理解环境并高效决策。

Abstract: Large language models (LLMs) have shown great potential in decision-making
due to the vast amount of knowledge stored within the models. However, these
pre-trained models are prone to lack reasoning abilities and are difficult to
adapt to new environments, further hindering their application to complex
real-world tasks. To address these challenges, inspired by the human cognitive
process, we propose Causal-aware LLMs, which integrate the structural causal
model (SCM) into the decision-making process to model, update, and utilize
structured knowledge of the environment in a ``learning-adapting-acting"
paradigm. Specifically, in the learning stage, we first utilize an LLM to
extract the environment-specific causal entities and their causal relations to
initialize a structured causal model of the environment. Subsequently,in the
adapting stage, we update the structured causal model through external feedback
about the environment, via an idea of causal intervention. Finally, in the
acting stage, Causal-aware LLMs exploit structured causal knowledge for more
efficient policy-making through the reinforcement learning agent. The above
processes are performed iteratively to learn causal knowledge, ultimately
enabling the causal-aware LLMs to achieve a more accurate understanding of the
environment and make more efficient decisions. Experimental results across 22
diverse tasks within the open-world game ``Crafter" validate the effectiveness
of our proposed method.

</details>


### [281] [CoRet: Improved Retriever for Code Editing](https://arxiv.org/abs/2505.24715)
*Fabio Fehr,Prabhu Teja Sivaprasad,Luca Franceschi,Giovanni Zappella*

Key words: 密集检索、代码编辑、自然语言查询、召回率、损失函数

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出CoRet模型，集成代码语义、仓库结构和调用图依赖，专注于基于自然语言查询检索代码块，训练中使用专门设计的损失函数，在两个数据集上相比现有模型召回率提升至少15%。

Motivation: 针对代码编辑任务中检索相关代码块的效率问题，提出一种结合多维度信息的密集检索模型，以支持自然语言查询的实际应用场景。

Method: 设计CoRet模型，整合代码语义、仓库结构和调用图依赖，并针对仓库级检索提出专用损失函数进行训练。

Result: 在SWE-bench和Long Code Arena的漏洞定位数据集上，模型召回率显著提升至少15%，并通过消融实验验证了设计的重要性。

Conclusion: CoRet通过多维度信息集成和专用损失函数，显著提升了代码检索的准确性和实用性。

Abstract: In this paper, we introduce CoRet, a dense retrieval model designed for
code-editing tasks that integrates code semantics, repository structure, and
call graph dependencies. The model focuses on retrieving relevant portions of a
code repository based on natural language queries such as requests to implement
new features or fix bugs. These retrieved code chunks can then be presented to
a user or to a second code-editing model or agent. To train CoRet, we propose a
loss function explicitly designed for repository-level retrieval. On SWE-bench
and Long Code Arena's bug localisation datasets, we show that our model
substantially improves retrieval recall by at least 15 percentage points over
existing models, and ablate the design choices to show their importance in
achieving these results.

</details>


### [282] [PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations](https://arxiv.org/abs/2505.24717)
*Benjamin Holzschuh,Qiang Liu,Georg Kohl,Nils Thuerey*

Key words: PDE-Transformer, 扩散Transformer, 代理建模, 物理仿真, 基础模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: PDE-Transformer是一种改进的基于Transformer的架构，用于在规则网格上对物理仿真进行代理建模，结合扩散Transformer的架构改进，针对大规模仿真调整，表现出色。

Motivation: 开发一种更可扩展且多功能的通用Transformer架构，用于构建物理科学中的大规模基础模型。

Method: 提出将不同物理通道单独嵌入为时空token，通过通道级自注意力交互，学习多种PDE时保持token信息密度一致。

Result: 在16种PDE大型数据集上表现优于现有Transformer架构，预训练模型在下游任务中表现优于从头训练和其他基础模型架构。

Conclusion: PDE-Transformer是一种高效且可扩展的架构，适用于物理仿真的大规模基础模型构建。

Abstract: We introduce PDE-Transformer, an improved transformer-based architecture for
surrogate modeling of physics simulations on regular grids. We combine recent
architectural improvements of diffusion transformers with adjustments specific
for large-scale simulations to yield a more scalable and versatile
general-purpose transformer architecture, which can be used as the backbone for
building large-scale foundation models in physical sciences. We demonstrate
that our proposed architecture outperforms state-of-the-art transformer
architectures for computer vision on a large dataset of 16 different types of
PDEs. We propose to embed different physical channels individually as
spatio-temporal tokens, which interact via channel-wise self-attention. This
helps to maintain a consistent information density of tokens when learning
multiple types of PDEs simultaneously. We demonstrate that our pre-trained
models achieve improved performance on several challenging downstream tasks
compared to training from scratch and also beat other foundation model
architectures for physics simulations.

</details>


### [283] [Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach](https://arxiv.org/abs/2505.24721)
*Nick Rossenbach,Benedikt Hilmes,Leon Brackmann,Moritz Gunz,Ralf Schlüter*

Key words: 忆阻器硬件、量化感知训练、模拟库Synaptogen、语音识别、低精度计算

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出基于PyTorch的库‘Synaptogen’，模拟忆阻器硬件特性下的大规模神经网络行为，首次展示百万参数ML系统在忆阻器硬件上的表现，通过量化感知训练将3比特权重下的语音识别性能下降控制在25%。

Motivation: 当前忆阻器硬件原型无法支持大规模神经网络，且现有软件模拟过于简化。需探索硬件特性对大型模型的影响。

Method: 基于PyTorch开发‘Synaptogen’库，精确模拟忆阻器硬件属性，结合量化感知训练优化3比特权重下的线性操作。

Result: 在TED-LIUMv2语音识别任务中，Conformer模型的词错误率相对退化控制在25%。

Conclusion: 量化感知训练能有效缓解低精度忆阻器硬件对模型性能的影响，为大规模ML硬件部署提供可行路径。

Abstract: Memristor-based hardware offers new possibilities for energy-efficient
machine learning (ML) by providing analog in-memory matrix multiplication.
Current hardware prototypes cannot fit large neural networks, and related
literature covers only small ML models for tasks like MNIST or single word
recognition. Simulation can be used to explore how hardware properties affect
larger models, but existing software assumes simplified hardware. We propose a
PyTorch-based library based on "Synaptogen" to simulate neural network
execution with accurately captured memristor hardware properties. For the first
time, we show how an ML system with millions of parameters would behave on
memristor hardware, using a Conformer trained on the speech recognition task
TED-LIUMv2 as example. With adjusted quantization-aware training, we limit the
relative degradation in word error rate to 25% when using a 3-bit weight
precision to execute linear operations via simulated analog computation.

</details>


### [284] [HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts](https://arxiv.org/abs/2505.24722)
*Neil He,Rishabh Anand,Hiren Madhu,Ali Maatouk,Smita Krishnaswamy,Leandros Tassiulas,Menglin Yang,Rex Ying*

Key words: 大语言模型, 双曲几何, Transformer, 非欧几何, 语义层次

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于双曲几何的大语言模型家族HELM，通过非欧几何解决现有模型在语义层次和几何结构上的局限性，显著提升了性能。

Motivation: 自然语言具有固有的语义层次和几何结构，但现有语言模型依赖欧几里得操作无法完全捕捉这些特性，导致训练不稳定和生成能力下降。转向非欧几何可以更好地对齐语言模型的几何特性。

Method: 提出完全在双曲空间中运行的HELM模型家族，包括多曲率专家模型HELM-MICE和密集模型HELM-D。引入了双曲多头潜在注意力（HMLA）和双曲等效的旋转位置编码与RMS归一化。

Result: 在MMLU和ARC等基准测试中，HELM架构相比欧几里得架构（如LLaMA和DeepSeek）性能提升高达4%。

Conclusion: 双曲几何在大规模语言模型预训练中表现出更高的效能和推理能力。

Abstract: Large language models (LLMs) have shown great success in text modeling tasks
across domains. However, natural language exhibits inherent semantic
hierarchies and nuanced geometric structure, which current LLMs do not capture
completely owing to their reliance on Euclidean operations. Recent studies have
also shown that not respecting the geometry of token embeddings leads to
training instabilities and degradation of generative capabilities. These
findings suggest that shifting to non-Euclidean geometries can better align
language models with the underlying geometry of text. We thus propose to
operate fully in Hyperbolic space, known for its expansive, scale-free, and
low-distortion properties. We thus introduce HELM, a family of HypErbolic Large
Language Models, offering a geometric rethinking of the Transformer-based LLM
that addresses the representational inflexibility, missing set of necessary
operations, and poor scalability of existing hyperbolic LMs. We additionally
introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert
operates in a distinct curvature space to encode more fine-grained geometric
structure from text, as well as a dense model, HELM-D. For HELM-MICE, we
further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,
reduced-KV-cache training and inference. For both models, we develop essential
hyperbolic equivalents of rotary positional encodings and RMS normalization. We
are the first to train fully hyperbolic LLMs at billion-parameter scale, and
evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM
problem-solving, general knowledge, and commonsense reasoning. Our results show
consistent gains from our HELM architectures -- up to 4% -- over popular
Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy
and enhanced reasoning afforded by hyperbolic geometry in large-scale LM
pretraining.

</details>


### [285] [Robust Federated Learning against Model Perturbation in Edge Networks](https://arxiv.org/abs/2505.24728)
*Dongzi Jin,Yong Xiao,Yingyu Li*

Key words: 联邦学习, 鲁棒性, 平坦最小值, SMRFL

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为SMRFL的新方法，通过探索模型景观的几何特性，提升联邦学习模型对扰动的鲁棒性。

Motivation: 联邦学习中共享的模型常被假设为理想状态，而实践中存在各种扰动导致性能下降，需要一种提升鲁棒性的方法。

Method: SMRFL通过解决最小-最大优化问题，促使模型向平坦最小值收敛，降低对扰动的敏感性。

Result: 理论证明SMRFL能与无扰动的联邦学习以相同速度收敛。实验显示SMRFL在三种扰动场景下显著优于基线方法。

Conclusion: SMRFL有效增强了联邦学习模型对扰动的鲁棒性。

Abstract: Federated Learning (FL) is a promising paradigm for realizing edge
intelligence, allowing collaborative learning among distributed edge devices by
sharing models instead of raw data. However, the shared models are often
assumed to be ideal, which would be inevitably violated in practice due to
various perturbations, leading to significant performance degradation. To
overcome this challenge, we propose a novel method, termed Sharpness-Aware
Minimization-based Robust Federated Learning (SMRFL), which aims to improve
model robustness against perturbations by exploring the geometrical property of
the model landscape. Specifically, SMRFL solves a min-max optimization problem
that promotes model convergence towards a flat minimum by minimizing the
maximum loss within a neighborhood of the model parameters. In this way, model
sensitivity to perturbations is reduced, and robustness is enhanced since
models in the neighborhood of the flat minimum also enjoy low loss values. The
theoretical result proves that SMRFL can converge at the same rate as FL
without perturbations. Extensive experimental results show that SMRFL
significantly enhances robustness against perturbations compared to three
baseline methods on two real-world datasets under three perturbation scenarios.

</details>


### [286] [Feature Attribution from First Principles](https://arxiv.org/abs/2505.24729)
*Magamed Taimeskhanov,Damien Garreau*

Key words: 特征归因, 可解释性, 机器学习模型, ReLU网络, 评估指标

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种新的特征归因框架，通过从最简单的模型（如指示函数）开始构建，而非依赖过于严格的公理，并展示了如何恢复现有归因方法。

Motivation: 现有特征归因方法依赖于过于严格的公理评估框架，限制了其灵活性和实用性。本文旨在提出一个更灵活的框架。

Method: 从简单模型（如指示函数）定义归因，逐步扩展到复杂模型（如深度ReLU网络），并推导其闭式表达式。

Result: 提出了新框架，可恢复多种现有归因方法，并为深度ReLU网络提供闭式归因表达式。

Conclusion: 新框架突破了公理限制，提供了更灵活的特征归因方法，并为优化评估指标奠定了基础。

Abstract: Feature attribution methods are a popular approach to explain the behavior of
machine learning models. They assign importance scores to each input feature,
quantifying their influence on the model's prediction. However, evaluating
these methods empirically remains a significant challenge. To bypass this
shortcoming, several prior works have proposed axiomatic frameworks that any
feature attribution method should satisfy. In this work, we argue that such
axioms are often too restrictive, and propose in response a new feature
attribution framework, built from the ground up. Rather than imposing axioms,
we start by defining attributions for the simplest possible models, i.e.,
indicator functions, and use these as building blocks for more complex models.
We then show that one recovers several existing attribution methods, depending
on the choice of atomic attribution. Subsequently, we derive closed-form
expressions for attribution of deep ReLU networks, and take a step toward the
optimization of evaluation metrics with respect to feature attributions.

</details>


### [287] [Adapting to Linear Separable Subsets with Large-Margin in Differentially Private Learning](https://arxiv.org/abs/2505.24737)
*Erchi Wang,Yuqing Zhu,Yu-Xiang Wang*

Key words: 差分隐私（DP), 二元线性分类, 实证风险最小化（ERM), 零一风险, 异常点

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种高效的差分隐私二元线性分类算法，在未知分离间隔或异常点的情况下，实现了零一风险的理论界限，并在存在少量异常点时改进了现有结果。

Motivation: 研究如何在差分隐私约束下进行二元线性分类的实证风险最小化（DP-ERM），尤其是在未知分离间隔或异常点的情况下，提升算法的适应性和实用性。

Method: 设计了一种高效的（ε,δ）-差分隐私算法，适用于二元线性分类。该算法无需提前知道分离间隔γ或异常点子集S_out，可自适应地进行风险最小化。

Result: 算法获得了实证零一风险的理论界限，形式为O~（1/γ²εn  + |S_out|/γn），其中n为数据点数量，ε为隐私参数。当异常点较少时，优于现有结果。

Conclusion: 提出的自适应差分隐私算法在二元线性分类问题中表现出色，尤其在未知参数和少量异常点的场景下具有显著优势。

Abstract: This paper studies the problem of differentially private empirical risk
minimization (DP-ERM) for binary linear classification. We obtain an efficient
$(\varepsilon,\delta)$-DP algorithm with an empirical zero-one risk bound of
$\tilde{O}\left(\frac{1}{\gamma^2\varepsilon n} +
\frac{|S_{\mathrm{out}}|}{\gamma n}\right)$ where $n$ is the number of data
points, $S_{\mathrm{out}}$ is an arbitrary subset of data one can remove and
$\gamma$ is the margin of linear separation of the remaining data points (after
$S_{\mathrm{out}}$ is removed). Here, $\tilde{O}(\cdot)$ hides only logarithmic
terms. In the agnostic case, we improve the existing results when the number of
outliers is small. Our algorithm is highly adaptive because it does not require
knowing the margin parameter $\gamma$ or outlier subset $S_{\mathrm{out}}$. We
also derive a utility bound for the advanced private hyperparameter tuning
algorithm.

</details>


### [288] [SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training](https://arxiv.org/abs/2505.24749)
*Yehonathan Refael,Guy Smorodinsky,Tom Tirer,Ofir Lindenbaum*

Key words: 低秩优化, SVD, 大语言模型, 正交化, 收敛加速

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了名为SUMO的优化器，通过动态低维子空间中的SVD实现正交化，显著提升大模型训练的收敛速度和内存效率。

Motivation: 现有低秩优化方法主要关注内存节省，但忽视了高各向异性环境下收敛速度的提升。

Method: SUMO利用动态子空间中的SVD进行正交化，实现谱对齐的优化步长，减少常见近似方法的误差。

Result: 理论和实验表明SUMO显著提升收敛速度、稳定性及性能，内存需求减少20%。

Conclusion: SUMO为大模型训练提供了高效且收敛快速的优化方案。

Abstract: Low-rank gradient-based optimization methods have significantly improved
memory efficiency during the training of large language models (LLMs), enabling
operations within constrained hardware without sacrificing performance.
However, these methods primarily emphasize memory savings, often overlooking
potential acceleration in convergence due to their reliance on standard
isotropic steepest descent techniques, which can perform suboptimally in the
highly anisotropic landscapes typical of deep networks, particularly LLMs. In
this paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an
optimizer that employs exact singular value decomposition (SVD) for moment
orthogonalization within a dynamically adapted low-dimensional subspace,
enabling norm-inducing steepest descent optimization steps. By explicitly
aligning optimization steps with the spectral characteristics of the loss
landscape, SUMO effectively mitigates approximation errors associated with
commonly used methods like Newton-Schulz orthogonalization approximation. We
theoretically establish an upper bound on these approximation errors, proving
their dependence on the condition numbers of moments, conditions we
analytically demonstrate are encountered during LLM training. Furthermore, we
both theoretically and empirically illustrate that exact orthogonalization via
SVD substantially improves convergence rates while reducing overall complexity.
Empirical evaluations confirm that SUMO accelerates convergence, enhances
stability, improves performance, and reduces memory requirements by up to 20%
compared to state-of-the-art methods.

</details>


### [289] [REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2505.24760)
*Zafir Stojanovski,Oliver Stanley,Joe Sharratt,Richard Jones,Abdulhakeem Adefioye,Jean Kaddour,Andreas Köpf*

Key words: Reasoning Gym, 强化学习, 程序化生成, 可验证奖励, 多领域推理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Reasoning Gym (RG) 是一个用于强化学习的推理环境库，提供可验证奖励的多种领域数据生成和验证。

Motivation: 解决传统推理数据集固定且无法调整复杂度的问题，通过程序化生成数据实现多难度评估和训练。

Method: 利用100多种数据生成器和验证器，覆盖代数、算术、计算、认知、几何等领域，支持动态调整数据复杂度。

Result: 实验验证了RG在评估和强化学习推理模型方面的有效性。

Conclusion: RG为强化学习提供了灵活且可扩展的推理训练环境。

Abstract: We introduce Reasoning Gym (RG), a library of reasoning environments for
reinforcement learning with verifiable rewards. It provides over 100 data
generators and verifiers spanning multiple domains including algebra,
arithmetic, computation, cognition, geometry, graph theory, logic, and various
common games. Its key innovation is the ability to generate virtually infinite
training data with adjustable complexity, unlike most previous reasoning
datasets, which are typically fixed. This procedural generation approach allows
for continuous evaluation across varying difficulty levels. Our experimental
results demonstrate the efficacy of RG in both evaluating and reinforcement
learning of reasoning models.

</details>


### [290] [AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption](https://arxiv.org/abs/2505.24773)
*Yajie Zhou,Xiaoyi Pang,Zhibo Wang*

Key words: 联邦学习、大型语言模型、低秩适应、异构环境、微调

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: AFLoRA是一个自适应轻量级联邦微调框架，用于在异构环境下高效微调大型语言模型，通过解耦共享与客户端特定更新、对角矩阵秩剪枝和基于公共数据的秩感知聚合，显著提升准确性和效率。

Motivation: 现有的联邦微调方法在异构和资源受限的客户端环境中表现不佳，尤其是因最弱客户端和非独立同分布数据导致性能瓶颈。AFLoRA旨在解决这些问题。

Method: AFLoRA通过解耦共享与客户端特定更新、引入对角矩阵秩剪枝优化本地资源利用，并结合秩感知聚合与公共数据精炼，提升泛化能力。

Result: 实验表明，AFLoRA在准确性和效率上均优于现有方法，为真实异构环境中的大型语言模型微调提供了实用解决方案。

Conclusion: AFLoRA通过创新设计显著提升了联邦微调的效率和性能，尤其在资源受限和数据异构的场景下表现突出。

Abstract: Federated fine-tuning has emerged as a promising approach to adapt foundation
models to downstream tasks using decentralized data. However, real-world
deployment remains challenging due to the high computational and communication
demands of fine-tuning Large Language Models (LLMs) on clients with data and
system resources that are heterogeneous and constrained. In such settings, the
global model's performance is often bottlenecked by the weakest clients and
further degraded by the non-IID nature of local data. Although existing methods
leverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to
reduce communication and computation overhead, they often fail to
simultaneously ensure accurate aggregation of low-rank updates and maintain low
system costs, thereby hindering overall performance. To address these
challenges, we propose AFLoRA, an adaptive and lightweight federated
fine-tuning framework for LLMs. AFLoRA decouples shared and client-specific
updates to reduce overhead and improve aggregation accuracy, incorporates
diagonal matrix-based rank pruning to better utilize local resources, and
employs rank-aware aggregation with public data refinement to strengthen
generalization under data heterogeneity. Extensive experiments demonstrate that
AFLoRA outperforms state-of-the-art methods in both accuracy and efficiency,
providing a practical solution for efficient LLM adaptation in heterogeneous
environments in the real world.

</details>


### [291] [Diffusion-Based Symbolic Regression](https://arxiv.org/abs/2505.24776)
*Zachary Bastiani,Robert M. Kirby,Jacob Hochhalter,Shandian Zhe*

Key words: 扩散模型, 符号回归, 强化学习, GRPO

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种基于扩散模型的符号回归方法，结合随机掩码扩散过程和GRPO强化学习，通过实验验证其有效性。

Motivation: 受扩散模型在生成任务中成功启发，探索其在符号回归中的应用。

Method: 使用随机掩码扩散过程生成方程，结合GRPO强化学习和长短期风险策略优化性能。

Result: 实验和消融研究表明该方法能生成多样且高质量的方程。

Conclusion: 扩散模型结合强化学习在符号回归中表现出色。

Abstract: Diffusion has emerged as a powerful framework for generative modeling,
achieving remarkable success in applications such as image and audio synthesis.
Enlightened by this progress, we propose a novel diffusion-based approach for
symbolic regression. We construct a random mask-based diffusion and denoising
process to generate diverse and high-quality equations. We integrate this
generative processes with a token-wise Group Relative Policy Optimization
(GRPO) method to conduct efficient reinforcement learning on the given
measurement dataset. In addition, we introduce a long short-term risk-seeking
policy to expand the pool of top-performing candidates, further enhancing
performance. Extensive experiments and ablation studies have demonstrated the
effectiveness of our approach.

</details>


### [292] [EVA-MILP: Towards Standardized Evaluation of MILP Instance Generation](https://arxiv.org/abs/2505.24779)
*Yidong Luo,Chenguang Wang,Jiahao Yang,Fanzeng Xia,Tianshu Yu*

Key words: MILP, 实例生成, 评估框架, 计算复杂性, 机器学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一个全面的MILP实例生成方法评估框架，通过统一且可扩展的方法评估实例质量，重点关注数学有效性、结构相似性、计算复杂性和下游机器学习任务的实用性。

Motivation: 由于MILP实例生成方法的快速发展和静态基准的局限性，评估合成MILP实例的真实性和实用性成为一个关键且多方面的挑战。

Method: 框架提供了一种统一且可扩展的评估方法，包括数学有效性、结构相似性、计算硬度和下游机器学习任务的实用性等关键维度。特别创新的是对求解器内部特征的深入分析，通过比较求解器输出的关键分布来揭示计算相似性。

Result: 框架的有效性通过当代生成模型进行了验证，展示了其在系统比较实例集真实性方面的能力。

Conclusion: 该框架旨在促进不同生成技术之间的稳健比较，推动更高质量实例生成器的开发，并最终提高依赖合成MILP数据研究的可靠性。

Abstract: Mixed-Integer Linear Programming (MILP) is fundamental to solving complex
decision-making problems. The proliferation of MILP instance generation
methods, driven by machine learning's demand for diverse optimization datasets
and the limitations of static benchmarks, has significantly outpaced
standardized evaluation techniques. Consequently, assessing the fidelity and
utility of synthetic MILP instances remains a critical, multifaceted challenge.
This paper introduces a comprehensive benchmark framework designed for the
systematic and objective evaluation of MILP instance generation methods. Our
framework provides a unified and extensible methodology, assessing instance
quality across crucial dimensions: mathematical validity, structural
similarity, computational hardness, and utility in downstream machine learning
tasks. A key innovation is its in-depth analysis of solver-internal features --
particularly by comparing distributions of key solver outputs including root
node gap, heuristic success rates, and cut plane usage -- leveraging the
solver's dynamic solution behavior as an `expert assessment' to reveal nuanced
computational resemblances. By offering a structured approach with clearly
defined solver-independent and solver-dependent metrics, our benchmark aims to
facilitate robust comparisons among diverse generation techniques, spur the
development of higher-quality instance generators, and ultimately enhance the
reliability of research reliant on synthetic MILP data. The framework's
effectiveness in systematically comparing the fidelity of instance sets is
demonstrated using contemporary generative models.

</details>


### [293] [QGAN-based data augmentation for hybrid quantum-classical neural networks](https://arxiv.org/abs/2505.24780)
*Run-Ze He,Jun-Jian Su,Su-Juan Qin,Zheng-Ping Jin,Fei Gao*

Key words: 量子神经网络, 数据增强, 量子生成对抗网络, 混合量子-经典神经网络, MNIST数据集

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出将量子生成对抗网络（QGAN）与混合量子-经典神经网络（HQCNN）结合，开发了一种数据增强框架，以提高量子神经网络的效率和准确性。通过实验验证，QGAN在MNIST数据集上表现优于传统方法和经典GAN。

Motivation: 针对量子机器学习中数据增强研究的不足和数据稀缺问题，研究者希望通过整合QGAN和HQCNN来优化数据处理和分类性能。

Method: 提出两种策略：一种通用的HQCNN数据增强方法，另一种是根据HQCNN在特定数据类别上的表现动态生成样本的自定义策略。在MNIST数据集上进行仿真实验。

Result: QGAN在参数仅为DCGAN一半的情况下，实现了与之相当的性能，同时提升了HQCNN的准确性和表现。

Conclusion: QGAN不仅能简化模型，还能生成高质量数据，为量子数据增强在机器学习中的应用提供了可行路径。

Abstract: Quantum neural networks converge faster and achieve higher accuracy than
classical models. However, data augmentation in quantum machine learning
remains underexplored. To tackle data scarcity, we integrate quantum generative
adversarial networks (QGANs) with hybrid quantum-classical neural networks
(HQCNNs) to develop an augmentation framework. We propose two strategies: a
general approach to enhance data processing and classification across HQCNNs,
and a customized strategy that dynamically generates samples tailored to the
HQCNN's performance on specific data categories, improving its ability to learn
from complex datasets. Simulation experiments on the MNIST dataset demonstrate
that QGAN outperforms traditional data augmentation methods and classical GANs.
Compared to baseline DCGAN, QGAN achieves comparable performance with half the
parameters, balancing efficiency and effectiveness. This suggests that QGANs
can simplify models and generate high-quality data, enhancing HQCNN accuracy
and performance. These findings pave the way for applying quantum data
augmentation techniques in machine learning.

</details>


### [294] [Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding](https://arxiv.org/abs/2505.24791)
*Jiaru Zhang,Juanwu Lu,Ziran Wang,Ruqi Zhang*

Key words: 归一化流、自回归建模、选择性Jacob解码、并行优化、生成模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种选择性Jacob解码（SeJD）策略，通过并行迭代优化加速自回归推理，验证了其在保持生成质量的同时显著提升推理速度。

Motivation: 自回归建模虽然提升了生成模型的表达能力，但严格的序列依赖性导致推理速度慢，限制了实际应用。本研究旨在打破这一限制。

Method: 通过观察自回归建模中补片的依赖性冗余，提出选择性Jacob解码策略（SeJD），利用并行迭代优化加速推理。

Result: 实验表明，SeJD在多种数据集上实现了高达4.7倍的推理加速，且生成质量和保真度未受影响。

Conclusion: SeJD策略在理论和实证上均验证了其高效性，为自回归模型的实用化提供了可行方案。

Abstract: Normalizing flows are promising generative models with advantages such as
theoretical rigor, analytical log-likelihood computation, and end-to-end
training. However, the architectural constraints to ensure invertibility and
tractable Jacobian computation limit their expressive power and practical
usability. Recent advancements utilize autoregressive modeling, significantly
enhancing expressive power and generation quality. However, such sequential
modeling inherently restricts parallel computation during inference, leading to
slow generation that impedes practical deployment. In this paper, we first
identify that strict sequential dependency in inference is unnecessary to
generate high-quality samples. We observe that patches in sequential modeling
can also be approximated without strictly conditioning on all preceding
patches. Moreover, the models tend to exhibit low dependency redundancy in the
initial layer and higher redundancy in subsequent layers. Leveraging these
observations, we propose a selective Jacobi decoding (SeJD) strategy that
accelerates autoregressive inference through parallel iterative optimization.
Theoretical analyses demonstrate the method's superlinear convergence rate and
guarantee that the number of iterations required is no greater than the
original sequential approach. Empirical evaluations across multiple datasets
validate the generality and effectiveness of our acceleration technique.
Experiments demonstrate substantial speed improvements up to 4.7 times faster
inference while keeping the generation quality and fidelity.

</details>


### [295] [ByzFL: Research Framework for Robust Federated Learning](https://arxiv.org/abs/2505.24802)
*Marc González,Rachid Guerraoui,Rafael Pinot,Geovani Rizk,John Stephan,François Taïani*

Key words: 联邦学习,鲁棒性,开源工具,PyTorch,NumPy

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: ByzFL是一个开源的Python库，用于开发和评估鲁棒的联邦学习算法，提供统一框架、先进聚合器实现、可配置攻击工具及可视化功能。

Motivation: 开发一个统一的工具库，支持联邦学习算法的鲁棒性研究和快速原型设计，便于系统性实验和结果重现。

Method: 提供包含鲁棒聚合器实现、攻击套件和模拟工具的框架，支持JSON配置和结果可视化，兼容PyTorch和NumPy。

Result: ByzFL成功实现了对多种联邦学习场景的系统性实验支持，包括异构数据分布和对抗性威胁模型。

Conclusion: ByzFL为联邦学习鲁棒性研究提供了一个高效、可扩展的工具，促进了可重复研究和快速原型设计。

Abstract: We present ByzFL, an open-source Python library for developing and
benchmarking robust federated learning (FL) algorithms. ByzFL provides a
unified and extensible framework that includes implementations of
state-of-the-art robust aggregators, a suite of configurable attacks, and tools
for simulating a variety of FL scenarios, including heterogeneous data
distributions, multiple training algorithms, and adversarial threat models. The
library enables systematic experimentation via a single JSON-based
configuration file and includes built-in utilities for result visualization.
Compatible with PyTorch tensors and NumPy arrays, ByzFL is designed to
facilitate reproducible research and rapid prototyping of robust FL solutions.
ByzFL is available at https://byzfl.epfl.ch/, with source code hosted on
GitHub: https://github.com/LPD-EPFL/byzfl.

</details>


### [296] [PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models](https://arxiv.org/abs/2505.24823)
*Yinggan Xu,Yue Liu,Zhiqiang Gao,Changnan Peng,Di Luo*

Key words: 大型语言模型,物理推理,基准测试,原理优先推理,科学问题解决

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了LLMs在基于物理原理推理方面的不足，并提出了PhySense基准测试，发现当前LLMs难以像专家一样进行简洁、基于原理的推理。

Motivation: 当前LLMs在处理复杂科学问题时往往生成冗长且不透明的解决方案，缺乏人类专家基于核心物理原理的简洁高效推理能力，这一差距阻碍了AI在科学领域的应用。

Method: 作者提出了PhySense，一个基于物理原理设计的推理基准测试，通过评估多种LLMs和提示类型来比较它们与专家推理路径的差异。

Result: 研究发现，现有LLMs难以实现类似专家的原理优先推理，表现出一致的局限性。

Conclusion: 该研究为开发高效、鲁棒且可解释的基于原理的科学推理AI系统提供了重要见解。

Abstract: Large language models (LLMs) have rapidly advanced and are increasingly
capable of tackling complex scientific problems, including those in physics.
Despite this progress, current LLMs often fail to emulate the concise,
principle-based reasoning characteristic of human experts, instead generating
lengthy and opaque solutions. This discrepancy highlights a crucial gap in
their ability to apply core physical principles for efficient and interpretable
problem solving. To systematically investigate this limitation, we introduce
PhySense, a novel principle-based physics reasoning benchmark designed to be
easily solvable by experts using guiding principles, yet deceptively difficult
for LLMs without principle-first reasoning. Our evaluation across multiple
state-of-the-art LLMs and prompt types reveals a consistent failure to align
with expert-like reasoning paths, providing insights for developing AI systems
with efficient, robust and interpretable principle-based scientific reasoning.

</details>


### [297] [Timing is important: Risk-aware Fund Allocation based on Time-Series Forecasting](https://arxiv.org/abs/2505.24835)
*Fuyuan Lyu,Linfeng Du,Yunpeng Weng,Qiufang Ying,Zhiyan Xu,Wen Zou,Haolun Wu,Xiuqiang He,Xing Tang*

Key words: 资金分配, 时间序列预测, 预测优化, 不确定性校准

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了RTS-PnO框架，解决资金分配问题中的目标不匹配和预测不确定性，结合端到端训练和自适应校准，在离线和在线实验中表现优异。

Motivation: 解决资金分配问题中预测与优化目标不匹配及预测模型的额外不确定性问题。

Method: 提出RTS-PnO框架，包括端到端训练、自适应不确定性校准和对预测模型的不可知性。

Result: 在多种金融应用数据集上表现优于基线，在线实验中效果提升8.4%。

Conclusion: RTS-PnO框架有效解决了资金分配问题中的关键挑战，实际应用效果显著。

Abstract: Fund allocation has been an increasingly important problem in the financial
domain. In reality, we aim to allocate the funds to buy certain assets within a
certain future period. Naive solutions such as prediction-only or
Predict-then-Optimize approaches suffer from goal mismatch. Additionally, the
introduction of the SOTA time series forecasting model inevitably introduces
additional uncertainty in the predicted result. To solve both problems
mentioned above, we introduce a Risk-aware Time-Series Predict-and-Allocate
(RTS-PnO) framework, which holds no prior assumption on the forecasting models.
Such a framework contains three features: (i) end-to-end training with
objective alignment measurement, (ii) adaptive forecasting uncertainty
calibration, and (iii) agnostic towards forecasting models. The evaluation of
RTS-PnO is conducted over both online and offline experiments. For offline
experiments, eight datasets from three categories of financial applications are
used: Currency, Stock, and Cryptos. RTS-PnO consistently outperforms other
competitive baselines. The online experiment is conducted on the Cross-Border
Payment business at FiT, Tencent, and an 8.4\% decrease in regret is witnessed
when compared with the product-line approach. The code for the offline
experiment is available at https://github.com/fuyuanlyu/RTS-PnO.

</details>


### [298] [Cascading Adversarial Bias from Injection to Distillation in Language Models](https://arxiv.org/abs/2505.24842)
*Harsh Chaudhari,Jamie Hayes,Matthew Jagielski,Ilia Shumailov,Milad Nasr,Alina Oprea*

Key words: 模型蒸馏、对抗性攻击、数据污染、偏见传播、安全漏洞

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了模型蒸馏过程中对抗性偏见注入的脆弱性，展示了即使少量数据污染也能在师生模型中显著放大偏见。现有防御方法对此类攻击效果有限，作者提出了针对性的缓解策略设计原则。

Motivation: 随着模型蒸馏的广泛应用，对抗性操纵的风险增加，作者旨在揭示蒸馏模型对训练中被注入偏见的脆弱性，并探索其传播机制。

Method: 通过数据污染（仅0.25%的样本）在教师模型中植入偏见，分析其在学生模型中的传播模式（无目标传播与目标传播），并在六种偏见类型、多种蒸馏方法和模态中验证。

Result: 仅25个污染样本即可使学生模型在目标场景中76.9%生成偏见输出（教师模型为69.4%）；无目标场景下，学生模型的偏见表现比教师模型高6-29倍。现有防御措施表现不佳。

Conclusion: 蒸馏模型存在严重安全漏洞，需开发专门防护机制。作者提出了对抗性偏见缓解的实用设计原则。

Abstract: Model distillation has become essential for creating smaller, deployable
language models that retain larger system capabilities. However, widespread
deployment raises concerns about resilience to adversarial manipulation. This
paper investigates vulnerability of distilled models to adversarial injection
of biased content during training. We demonstrate that adversaries can inject
subtle biases into teacher models through minimal data poisoning, which
propagates to student models and becomes significantly amplified. We propose
two propagation modes: Untargeted Propagation, where bias affects multiple
tasks, and Targeted Propagation, focusing on specific tasks while maintaining
normal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning
rate), student models generate biased responses 76.9% of the time in targeted
scenarios - higher than 69.4% in teacher models. For untargeted propagation,
adversarial bias appears 6x-29x more frequently in student models on unseen
tasks. We validate findings across six bias types (targeted advertisements,
phishing links, narrative manipulations, insecure coding practices), various
distillation methods, and different modalities spanning text and code
generation. Our evaluation reveals shortcomings in current defenses -
perplexity filtering, bias detection systems, and LLM-based autorater
frameworks - against these attacks. Results expose significant security
vulnerabilities in distilled models, highlighting need for specialized
safeguards. We propose practical design principles for building effective
adversarial bias mitigation strategies.

</details>


### [299] [From Invariant Representations to Invariant Data: Provable Robustness to Spurious Correlations via Noisy Counterfactual Matching](https://arxiv.org/abs/2505.24843)
*Ruqi Bai,Yao Ji,Zeyu Zhou,David I. Inouye*

Key words: 虚假相关性, 不变表示, 噪声反事实匹配（NCM）, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种利用不变数据对增强模型鲁棒性的方法，名为噪声反事实匹配（NCM），并通过实验验证了其有效性。

Motivation: 研究发现虚假相关性会降低模型在新环境中的性能，现有基于因果不变性的方法（如IRM）通常表现不如经验风险最小化（ERM），而依赖测试时数据的方法在实际中可能不可用。

Method: 提出数据为中心的方法，利用不变数据对（即具有相同预测的样本对），引入噪声反事实匹配（NCM）来增强鲁棒性。

Result: 在线性因果模型中，证明了测试域误差的上界，实验验证了NCM在合成数据集和真实基准上的有效性。

Conclusion: NCM通过利用不变数据对有效提升了模型鲁棒性，尤其在小规模噪声对下表现出色。

Abstract: Spurious correlations can cause model performance to degrade in new
environments. Prior causality-inspired works aim to learn invariant
representations (e.g., IRM) but typically underperform empirical risk
minimization (ERM). Recent alternatives improve robustness by leveraging
test-time data, but such data may be unavailable in practice. To address these
issues, we take a data-centric approach by leveraging invariant data pairs,
pairs of samples that would have the same prediction with the optimally robust
classifier. We prove that certain counterfactual pairs will naturally satisfy
this invariance property and introduce noisy counterfactual matching (NCM), a
simple constraint-based method for leveraging invariant pairs for enhanced
robustness, even with a small set of noisy pairs-in the ideal case, each pair
can eliminate one spurious feature. For linear causal models, we prove that the
test domain error can be upper bounded by the in-domain error and a term that
depends on the counterfactuals' diversity and quality. We validate on a
synthetic dataset and demonstrate on real-world benchmarks that linear probing
on a pretrained backbone improves robustness.

</details>


### [300] [Chameleon: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning](https://arxiv.org/abs/2505.24844)
*Wanyun Xie,Francesco Tonin,Volkan Cevher*

Key words: 数据混合, 杠杆分数, 嵌入空间, 领域重新加权, 语言模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为Chameleon的高效数据混合框架，通过计算嵌入空间中的杠杆分数来量化领域重要性，避免了现有方法的高成本计算和重新训练需求。

Motivation: 训练数据的混合方式对大型语言模型的泛化性能有重大影响，现有的领域重新加权方法通常需要高成本计算，且在引入新数据时需重新训练。

Method: Chameleon框架利用杠杆分数在学习的嵌入空间中量化领域重要性，构建领域亲和矩阵并通过嵌入空间中的共同表示确定混合权重，实现了对新数据的直接适应。

Result: 实验表明，该方法在预训练领域性能提升、少样本推理准确率提高以及微调领域测试困惑度优化方面均优于现有方法。

Conclusion: Chameleon框架提供了一种灵活高效的数据混合方法，显著降低了计算成本并提高了模型适应能力。

Abstract: Training data mixtures greatly impact the generalization performance of large
language models. Existing domain reweighting methods often rely on costly
weight computations and require retraining when new data is introduced. To this
end, we introduce a flexible and efficient data mixing framework, Chameleon,
that employs leverage scores to quantify domain importance within a learned
embedding space. We first construct a domain affinity matrix over domain
embeddings. The induced leverage scores determine a mixture that upweights
domains sharing common representations in embedding space. This formulation
allows direct transfer to new data by computing the new domain embeddings. In
experiments, we demonstrate improvements over three key scenarios: (i) our
computed weights improve performance on pretraining domains with a fraction of
the compute of existing methods; (ii) Chameleon can adapt to data changes
without proxy retraining, boosting few-shot reasoning accuracies when
transferred to new data; (iii) our method enables efficient domain reweighting
in finetuning, consistently improving test perplexity on all finetuning domains
over uniform mixture. Our code is available at
https://github.com/LIONS-EPFL/Chameleon.

</details>


### [301] [Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning](https://arxiv.org/abs/2505.24850)
*Shuyao Xu,Cheng Peng,Jiangxuan Long,Weidi Xu,Wei Chu,Yuan Qi*

Key words: 模型蒸馏、推理能力、REDI、正负样本、数学推理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为REDI的两阶段蒸馏框架，利用正负推理轨迹提升小模型性能，在数学推理任务上超越现有方法。

Motivation: 当前蒸馏方法通常丢弃错误的推理轨迹，但这些数据可能蕴含价值。如何充分利用正负样本提升模型推理能力成为关键问题。

Method: REDI框架分两阶段：1）通过监督微调（SFT）学习正样本；2）通过提出的REDI损失函数联合优化正负样本。

Result: Qwen-REDI-1.5B模型仅用13.1万公开数据，在MATH-500上达到83.1%（pass@1），性能媲美基于80万私有数据蒸馏的模型。

Conclusion: REDI证明了负样本的有效性，为小模型离线训练设立了新标杆。

Abstract: Recent advances in model distillation demonstrate that data from advanced
reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer
complex reasoning abilities to smaller, efficient student models. However,
standard practices employ rejection sampling, discarding incorrect reasoning
examples -- valuable, yet often underutilized data. This paper addresses the
critical question: How can both positive and negative distilled reasoning
traces be effectively leveraged to maximize LLM reasoning performance in an
offline setting? To this end, We propose Reinforcement Distillation (REDI), a
two-stage framework. Stage 1 learns from positive traces via Supervised
Fine-Tuning (SFT). Stage 2 further refines the model using both positive and
negative traces through our proposed REDI objective. This novel objective is a
simple, reference-free loss function that outperforms established methods like
DPO and SimPO in this distillation context. Our empirical evaluations
demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT
combined with DPO/SimPO on mathematical reasoning tasks. Notably, the
Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples
from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).
Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a
model post-trained on 800k proprietary data) across various mathematical
reasoning benchmarks, establishing a new state-of-the-art for 1.5B models
post-trained offline with openly available data.

</details>


### [302] [Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking](https://arxiv.org/abs/2505.24857)
*Heli Ben-Hamu,Itai Gat,Daniel Severo,Niklas Nolte,Brian Karrer*

Key words: masked diffusion models, EB-Sampler, entropy-bounded unmasking, adaptive sampling, deterministic tokens

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: EB-Sampler accelerates masked diffusion models (MDMs) by 2-3x without performance loss, utilizing entropy-bounded unmasking.

Motivation: Efficient sampling from MDMs is under-explored; existing procedures ignore deterministic token relationships in partially masked sequences.

Method: Introduces EB-Sampler, an adaptive entropy-bounded unmasking procedure, dynamically unmasking multiple tokens per evaluation with error tolerance.

Result: Achieves 2-3x speedup on coding/math benchmarks and validates effectiveness on maze/Sudoku tasks where ARMs struggle.

Conclusion: EB-Sampler efficiently leverages deterministic token dependencies, offering a practical drop-in replacement for MDM samplers.

Abstract: Recent masked diffusion models (MDMs) have shown competitive performance
compared to autoregressive models (ARMs) for language modeling. While most
literature has focused on performance enhancing sampling procedures, efficient
sampling from MDMs has been scarcely explored. We make the observation that
often a given sequence of partially masked tokens determines the values of
multiple unknown tokens deterministically, meaning that a single prediction of
a masked model holds additional information unused by standard sampling
procedures. Based on this observation, we introduce EB-Sampler, a simple
drop-in replacement for existing samplers, utilizing an Entropy Bounded
unmasking procedure that dynamically unmasks multiple tokens in one function
evaluation with predefined approximate error tolerance. We formulate the
EB-Sampler as part of a broad family of adaptive samplers for which we provide
an error analysis that motivates our algorithmic choices. EB-Sampler
accelerates sampling from current state of the art MDMs by roughly 2-3x on
standard coding and math reasoning benchmarks without loss in performance. We
also validate the same procedure works well on smaller reasoning tasks
including maze navigation and Sudoku, tasks ARMs often struggle with.

</details>


### [303] [Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization](https://arxiv.org/abs/2505.24859)
*Joschka Braun,Carsten Eickhoff,Seyed Ali Bahrainian*

Key words: 转向向量, 文本控制, 自由生成, 多任务评估, 提示方法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究评估了在自由生成任务中使用转向向量控制文本属性的效果，发现转向虽能有效控制目标属性，但高强度转向会降低文本质量。提示方法控制较弱但保留质量，两者结合效果最佳。

Motivation: 转向向量是一种轻量级方法，用于通过在学习偏置于语言模型激活来控制文本属性。然而，此前研究主要在多选环境下评估，自由生成任务中的效果尚未充分研究。

Method: 研究在NEWTS数据集的摘要生成任务中，评估转向向量在控制主题焦点、情感、毒性和可读性方面的效果，并与提示方法及其组合方法进行对比。

Result: 转向向量能有效控制目标属性，但高强度使用会显著降低文本质量；提示方法控制较弱但质量保留较好；两者结合在中等转向强度下实现最佳效果。

Conclusion: 转向向量在自由生成任务中需权衡控制强度与文本质量，结合提示方法可在中等强度下实现最优效果。

Abstract: Steering vectors are a lightweight method for controlling text properties by
adding a learned bias to language model activations at inference time. So far,
steering vectors have predominantly been evaluated in multiple-choice settings,
while their effectiveness in free-form generation tasks remains understudied.
Moving "Beyond Multiple Choice," we thoroughly evaluate the effectiveness of
steering vectors in adaptively controlling topical focus, sentiment, toxicity,
and readability in abstractive summaries of the NEWTS dataset. We find that
steering effectively controls the targeted summary properties, but high
steering strengths consistently degrade both intrinsic and extrinsic text
quality. Compared to steering, prompting offers weaker control, while
preserving text quality. Combining steering and prompting yields the strongest
control over text properties and offers the most favorable efficacy-quality
trade-off at moderate steering strengths. Our results underscore the practical
trade-off between control strength and text quality preservation when applying
steering vectors to free-form generation tasks.

</details>


### [304] [The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models](https://arxiv.org/abs/2505.24874)
*Adam Stein,Aaditya Naik,Neelay Velingker,Mayur Naik,Eric Wong*

Key words: 神经符号学习, 基础模型, 符号程序, 神经符号提示, 泛化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 传统神经符号学习在计算、数据和程序上存在泛化问题，作者提出基础模型结合符号程序（神经符号提示）能实现更好的通用性。

Motivation: 解决传统神经符号学习方法在复杂推理任务中的局限性，同时结合基础模型的高性能与符号程序的可靠性。

Method: 提出神经符号提示方法，通过基础模型与符号程序的结合，避免从头训练的缺点。

Result: 基础模型能够支持通用的神经符号解决方案，提升泛化能力。

Conclusion: 神经符号提示在结合基础模型与符号程序后，能更高效地实现神经符号学习的原始目标。

Abstract: Neuro-symbolic learning was proposed to address challenges with training
neural networks for complex reasoning tasks with the added benefits of
interpretability, reliability, and efficiency. Neuro-symbolic learning methods
traditionally train neural models in conjunction with symbolic programs, but
they face significant challenges that limit them to simplistic problems. On the
other hand, purely-neural foundation models now reach state-of-the-art
performance through prompting rather than training, but they are often
unreliable and lack interpretability. Supplementing foundation models with
symbolic programs, which we call neuro-symbolic prompting, provides a way to
use these models for complex reasoning tasks. Doing so raises the question:
What role does specialized model training as part of neuro-symbolic learning
have in the age of foundation models? To explore this question, we highlight
three pitfalls of traditional neuro-symbolic learning with respect to the
compute, data, and programs leading to generalization problems. This position
paper argues that foundation models enable generalizable neuro-symbolic
solutions, offering a path towards achieving the original goals of
neuro-symbolic learning without the downsides of training from scratch.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [305] [Using Reasoning Models to Generate Search Heuristics that Solve Open Instances of Combinatorial Design Problems](https://arxiv.org/abs/2505.23881)
*Christopher D. Rosin*

Key words: 大语言模型, 组合设计, CPro1协议, 搜索启发式, 自动优化

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该研究利用具备推理能力的大语言模型（LLMs）通过迭代生成和优化答案的方法，解决组合设计领域中的未解决问题，尤其是通过CPro1协议生成搜索启发式策略，成功解决了多个长期未解的实例。

Motivation: 组合设计领域存在许多未解决的实例，传统方法难以应对。研究旨在利用具备推理能力的LLMs，通过自动生成和优化搜索策略，填补这一空白。

Method: 采用CPro1协议，基于LLMs生成搜索启发式策略，结合文本定义和有效性验证器，实现自动超参数调优和执行反馈。

Result: 成功解决了16个组合设计问题中的7个长期未解实例，包括3个非推理LLMs未能解决的问题，并在2025年文献中解决了多个新实例。

Conclusion: 具备推理能力的LLMs在组合设计问题中表现出色，能够通过迭代优化策略解决复杂问题，展示了其在数学和代码生成应用中的潜力。

Abstract: Large Language Models (LLMs) with reasoning are trained to iteratively
generate and refine their answers before finalizing them, which can help with
applications to mathematics and code generation. We apply code generation with
reasoning LLMs to a specific task in the mathematical field of combinatorial
design. This field studies diverse types of combinatorial designs, many of
which have lists of open instances for which existence has not yet been
determined. The Constructive Protocol CPro1 uses LLMs to generate search
heuristics that have the potential to construct solutions to small open
instances. Starting with a textual definition and a validity verifier for a
particular type of design, CPro1 guides LLMs to select and implement
strategies, while providing automated hyperparameter tuning and execution
feedback. CPro1 with reasoning LLMs successfully solves long-standing open
instances for 7 of 16 combinatorial design problems selected from the 2006
Handbook of Combinatorial Designs, including new solved instances for 3 of
these (Bhaskar Rao Designs, Symmetric Weighing Matrices, Balanced Ternary
Designs) that were unsolved by CPro1 with non-reasoning LLMs. It also solves
open instances for several problems from recent (2025) literature, generating
new Covering Sequences, Johnson Clique Covers, Deletion Codes, and a Uniform
Nested Steiner Quadruple System.

</details>


### [306] [OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation](https://arxiv.org/abs/2505.23885)
*Mengkang Hu,Yuhang Zhou,Wendong Fan,Yuzhou Nie,Bowei Xia,Tao Sun,Ziyu Ye,Zhaoxuan Jin,Yingru Li,Qiguang Chen,Zeyu Zhang,Yifeng Wang,Qianshuo Ye,Bernard Ghanem,Ping Luo,Guohao Li*

Key words: 多代理系统, 领域迁移, 强化学习, GAIA基准, 模块化架构

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: Workforce是一个分层多代理框架，通过解耦战略规划和专门化执行，实现跨领域的可迁移性。它采用了领域无关的规划器、协调器和专业化工作者，并通过优化的学习方法OWL提升性能。在GAIA基准测试中，Workforce表现优异，验证了其作为通用AI助手的潜力。

Motivation: 当前基于LLM的多代理系统由于领域特定性，难以跨领域迁移，且需要重新设计和训练全部组件。Workforce旨在解决这些问题，通过模块化设计实现跨领域的灵活适应和性能提升。

Method: Workforce采用分层结构，包含领域无关的规划器、协调器和领域特定的工作者代理，并结合OWL（优化劳动力学习）方法，通过强化学习优化领域无关规划器。

Result: 在GAIA基准测试中，Workforce取得了69.70%的准确率，比OpenAI的Deep Research高出2.34%。经过OWL训练的32B模型在挑战性任务上表现接近GPT-4o，准确率提升16.37%。

Conclusion: Workforce通过模块化和优化学习，实现了跨领域的可扩展泛化，为下一代通用AI助手奠定了基础。

Abstract: Large Language Model (LLM)-based multi-agent systems show promise for
automating real-world tasks but struggle to transfer across domains due to
their domain-specific nature. Current approaches face two critical
shortcomings: they require complete architectural redesign and full retraining
of all components when applied to new domains. We introduce Workforce, a
hierarchical multi-agent framework that decouples strategic planning from
specialized execution through a modular architecture comprising: (i) a
domain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask
management, and (iii) specialized Workers with domain-specific tool-calling
capabilities. This decoupling enables cross-domain transferability during both
inference and training phases: During inference, Workforce seamlessly adapts to
new domains by adding or modifying worker agents; For training, we introduce
Optimized Workforce Learning (OWL), which improves generalization across
domains by optimizing a domain-agnostic planner with reinforcement learning
from real-world feedback. To validate our approach, we evaluate Workforce on
the GAIA benchmark, covering various realistic, multi-domain agentic tasks.
Experimental results demonstrate Workforce achieves open-source
state-of-the-art performance (69.70%), outperforming commercial systems like
OpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model
achieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to
GPT-4o on challenging tasks. To summarize, by enabling scalable generalization
and modular domain transfer, our work establishes a foundation for the next
generation of general-purpose AI assistants.

</details>


### [307] [Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve](https://arxiv.org/abs/2505.23946)
*Yuanzhe Liu,Ryan Deng,Tim Kaler,Xuhao Chen,Charles E. Leiserson,Yao Ma,Jie Chen*

Key words: LLM协作、课程学习、编码优化、多代理学习

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种基于课程的多LLM协作框架，通过课程学习机制，使小型LLM团队能够超越更大规模的LLM及其他协作方法。

Motivation: 研究发现LLM在不同任务和细粒度上表现各异，激发如何利用多个LLM协同解决编码问题而不依赖先验知识的需求。

Method: 设计了课程征集、存储与选择机制，使得代理团队能够通过互相学习的成功与失败经验提升自身性能。

Result: 实验证明，采用课程学习的小型LLM团队在性能上优于更大规模的LLM及其他多LLM协作方法。

Conclusion: 课程学习机制能有效促进LLM间的知识共享与协作，实现集体性能的提升。

Abstract: Recent studies show that LLMs possess different skills and specialize in
different tasks. In fact, we observe that their varied performance occur in
several levels of granularity. For example, in the code optimization task, code
LLMs excel at different optimization categories and no one dominates others.
This observation prompts the question of how one leverages multiple LLM agents
to solve a coding problem without knowing their complementary strengths a
priori. We argue that a team of agents can learn from each other's successes
and failures so as to improve their own performance. Thus, a lesson is the
knowledge produced by an agent and passed on to other agents in the collective
solution process. We propose a lesson-based collaboration framework, design the
lesson solicitation--banking--selection mechanism, and demonstrate that a team
of small LLMs with lessons learned can outperform a much larger LLM and other
multi-LLM collaboration methods.

</details>


### [308] [InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback](https://arxiv.org/abs/2505.23950)
*Boyuan Chen,Donghai Hong,Jiaming Ji,Jiacheng Zheng,Bowen Dong,Jiayi Zhou,Kaile Wang,Juntao Dai,Xuyao Wang,Wenqi Chen,Qirui Zheng,Wenxin Li,Sirui Han,Yike Guo,Yaodong Yang*

Key words: 多模态大模型, 多轮交互, 人类反馈, 工具增强, 评估框架

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文探讨了多模态大模型（MLLMs）在多轮多模态交互中的能力缺失，并提出了InterMT数据集以填补这一空白。通过引入人类反馈和专家标注，研究构建了一个包含多轮对话和偏好对的数据集，并开发了工具增强的MLLM工作流。研究还提出了InterMT-Bench评估框架，展示了数据集在法官模型多模态任务中的应用潜力。

Motivation: 现有MLLMs在多轮多模态交互中能力不足，无法像人类一样持续与环境互动。为了接近人类智能，需要支持多轮、多模态的连贯交互。

Method: 通过构建InterMT数据集，包含15.6k提示、52.6k多轮对话实例和32.4k人类标记的偏好对，并引入工具增强的MLLM工作流和多轮QA实例构建方法。

Result: 研究展示了InterMT在法官调解任务中的应用，并揭示了法官模型的多轮扩展规律，推动了MLLMs在多模态交互中的对齐研究。

Conclusion: InterMT为研究多轮多模态交互提供了数据和评估框架，有望促进MLLMs向更高水平发展。

Abstract: As multimodal large models (MLLMs) continue to advance across challenging
tasks, a key question emerges: What essential capabilities are still missing? A
critical aspect of human learning is continuous interaction with the
environment -- not limited to language, but also involving multimodal
understanding and generation. To move closer to human-level intelligence,
models must similarly support multi-turn, multimodal interaction. In
particular, they should comprehend interleaved multimodal contexts and respond
coherently in ongoing exchanges. In this work, we present an initial
exploration through the InterMT -- the first preference dataset for multi-turn
multimodal interaction, grounded in real human feedback. In this exploration,
we particularly emphasize the importance of human oversight, introducing expert
annotations to guide the process, motivated by the fact that current MLLMs lack
such complex interactive capabilities. InterMT captures human preferences at
both global and local levels into nine sub-dimensions, consists of 15.6k
prompts, 52.6k multi-turn dialogue instances, and 32.4k human-labeled
preference pairs. To compensate for the lack of capability for multi-modal
understanding and generation, we introduce an agentic workflow that leverages
tool-augmented MLLMs to construct multi-turn QA instances. To further this
goal, we introduce InterMT-Bench to assess the ability of MLLMs in assisting
judges with multi-turn, multimodal tasks. We demonstrate the utility of
\InterMT through applications such as judge moderation and further reveal the
multi-turn scaling law of judge model. We hope the open-source of our data can
help facilitate further research on aligning current MLLMs to the next step.
Our project website can be found at https://pku-intermt.github.io .

</details>


### [309] [MSQA: Benchmarking LLMs on Graduate-Level Materials Science Reasoning and Knowledge](https://arxiv.org/abs/2505.23982)
*Jerry Junyang Cheung,Shiyao Shen,Yuchen Zhuang,Yinghao Li,Rampi Ramprasad,Chao Zhang*

Key words: 大型语言模型，材料科学，评估基准，复杂推理，多步推理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了MSQA，一个包含1,757个材料科学研究生水平问题的评估基准，旨在衡量大型语言模型在专业领域的知识和复杂推理能力。通过测试10种前沿模型，发现专有模型准确率最高达84.5%，开源模型约为60.5%，领域专用模型表现较差。

Motivation: 当前大型语言模型（LLMs）在材料科学领域缺乏评估其专业知识和复杂推理能力的基准，阻碍了它们的可靠应用。

Method: 研究者开发了MSQA基准，包含详细解释和二元判断两种问题格式，覆盖7个子领域（如结构-性能关系、合成过程等），要求模型具备精准知识和多步推理能力。测试了10种LLM（包括专有、开源和领域专用模型）。

Result: 专有API模型表现最佳（84.5%准确率），开源模型次之（60.5%），领域专用模型因过拟合和分布偏移问题表现较差。

Conclusion: MSQA是首个同时评估LLM事实性知识和推理能力的基准，揭示了当前模型在材料科学高级任务中的局限性。

Abstract: Despite recent advances in large language models (LLMs) for materials
science, there is a lack of benchmarks for evaluating their domain-specific
knowledge and complex reasoning abilities. To bridge this gap, we introduce
MSQA, a comprehensive evaluation benchmark of 1,757 graduate-level materials
science questions in two formats: detailed explanatory responses and binary
True/False assessments. MSQA distinctively challenges LLMs by requiring both
precise factual knowledge and multi-step reasoning across seven materials
science sub-fields, such as structure-property relationships, synthesis
processes, and computational modeling. Through experiments with 10
state-of-the-art LLMs, we identify significant gaps in current LLM performance.
While API-based proprietary LLMs achieve up to 84.5% accuracy, open-source
(OSS) LLMs peak around 60.5%, and domain-specific LLMs often underperform
significantly due to overfitting and distributional shifts. MSQA represents the
first benchmark to jointly evaluate the factual and reasoning capabilities of
LLMs crucial for LLMs in advanced materials science.

</details>


### [310] [Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding](https://arxiv.org/abs/2505.23990)
*Mingyang Mao,Mariela M. Perez-Cabarcas,Utteja Kallakuri,Nicholas R. Waytowich,Xiaomin Lin,Tinoosh Mohsenin*

Key words: 多模态检索增强生成、认知负载、人机协作、动态场景、MMBench-Video

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: Multi-RAG是一个多模态检索增强生成系统，旨在信息密集型场景中为人类提供适应性辅助，减少认知负载，并在MMBench-Video数据集上表现优于现有开源视频大语言模型和大型视觉语言模型。

Motivation: 随着智能体与机器人日益融入人类生活，需要在动态、信息丰富的场景中减轻人类的认知负担，迫切需要一个能适应多源信息并辅助决策的系统。

Method: 通过整合视频、音频和文本等多源信息流，Multi-RAG系统利用多模态检索增强生成技术进行推理，提升情境理解能力。

Result: 在MMBench-Video数据集上，Multi-RAG优于现有开源视频大语言模型和大型视觉语言模型，资源消耗更少且输入数据需求更低。

Conclusion: Multi-RAG为未来动态真实场景中的人机适应性辅助系统提供了高效、实用的基础框架。

Abstract: To effectively engage in human society, the ability to adapt, filter
information, and make informed decisions in ever-changing situations is
critical. As robots and intelligent agents become more integrated into human
life, there is a growing opportunity-and need-to offload the cognitive burden
on humans to these systems, particularly in dynamic, information-rich
scenarios.
  To fill this critical need, we present Multi-RAG, a multimodal
retrieval-augmented generation system designed to provide adaptive assistance
to humans in information-intensive circumstances. Our system aims to improve
situational understanding and reduce cognitive load by integrating and
reasoning over multi-source information streams, including video, audio, and
text. As an enabling step toward long-term human-robot partnerships, Multi-RAG
explores how multimodal information understanding can serve as a foundation for
adaptive robotic assistance in dynamic, human-centered situations. To evaluate
its capability in a realistic human-assistance proxy task, we benchmarked
Multi-RAG on the MMBench-Video dataset, a challenging multimodal video
understanding benchmark. Our system achieves superior performance compared to
existing open-source video large language models (Video-LLMs) and large
vision-language models (LVLMs), while utilizing fewer resources and less input
data. The results demonstrate Multi- RAG's potential as a practical and
efficient foundation for future human-robot adaptive assistance systems in
dynamic, real-world contexts.

</details>


### [311] [GenIC: An LLM-Based Framework for Instance Completion in Knowledge Graphs](https://arxiv.org/abs/2505.24036)
*Amel Gader,Alsayed Algergawy*

Key words: 知识图谱补全，大语言模型，实例补全，多标签分类，序列到序列生成

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了GenIC框架，结合多标签分类和序列到序列任务，利用大语言模型和知识图谱文本描述提升实例补全效果。

Motivation: 知识图谱补全需要填补知识库中的空白，尤其是当仅知道头实体时（h, ?, ?）。现代知识库的文本描述和类型信息为推断缺失事实提供了重要线索。

Method: 提出的GenIC框架分为两步：属性预测（多标签分类）和链接预测（序列到序列生成）。利用大语言模型处理知识图谱的文本描述和模式。

Result: 在三个数据集上的实验表明，GenIC优于现有基线方法。

Conclusion: 结合大语言模型和知识图谱的上下文信息，GenIC能有效提升实例补全效果。

Abstract: Knowledge graph completion aims to address the gaps of knowledge bases by
adding new triples that represent facts. The complexity of this task depends on
how many parts of a triple are already known. Instance completion involves
predicting the relation-tail pair when only the head is given (h, ?, ?).
Notably, modern knowledge bases often contain entity descriptions and types,
which can provide valuable context for inferring missing facts. By leveraging
these textual descriptions and the ability of large language models to extract
facts from them and recognize patterns within the knowledge graph schema, we
propose an LLM-powered, end-to-end instance completion approach. Specifically,
we introduce GenIC: a two-step Generative Instance Completion framework. The
first step focuses on property prediction, treated as a multi-label
classification task. The second step is link prediction, framed as a generative
sequence-to-sequence task. Experimental results on three datasets show that our
method outperforms existing baselines. Our code is available at
https://github.com/amal-gader/genic.

</details>


### [312] [Leave it to the Specialist: Repair Sparse LLMs with Sparse Fine-Tuning via Sparsity Evolution](https://arxiv.org/abs/2505.24037)
*Qiao Xiao,Alan Ansell,Boqian Wu,Lu Yin,Mykola Pechenizkiy,Shiwei Liu,Decebal Constantin Mocanu*

Key words: 大语言模型, 稀疏性, 微调, SEFT, 动态剪枝

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一种专为稀疏大语言模型设计的微调方法SEFT，通过动态演化稀疏拓扑结构和权重增减策略，在保持稀疏性的同时提升性能。

Motivation: 大语言模型虽在多任务中表现出色，但其巨大计算需求限制了部署。现有的剪枝和微调方法无法在高稀疏水平下保持性能，因此需要一种新方法来兼顾稀疏性和任务适应。

Method: SEFT（Sparsity Evolution Fine-Tuning）通过在微调过程中动态演化稀疏拓扑，采用权重增减策略和敏感度驱动的剪枝标准，保持模型稀疏性。

Result: 实验表明，SEFT在LLaMA、DeepSeek和Mistral等多种模型上优于基线方法，性能更强且内存和时间效率更高。

Conclusion: SEFT是一种高效的稀疏大语言模型微调方法，能在保持稀疏性的同时提升任务适应性和性能。

Abstract: Large language models (LLMs) have achieved remarkable success across various
tasks but face deployment challenges due to their massive computational
demands. While post-training pruning methods like SparseGPT and Wanda can
effectively reduce the model size, but struggle to maintain model performance
at high sparsity levels, limiting their utility for downstream tasks. Existing
fine-tuning methods, such as full fine-tuning and LoRA, fail to preserve
sparsity as they require updating the whole dense metrics, not well-suited for
sparse LLMs. In this paper, we propose Sparsity Evolution Fine-Tuning (SEFT), a
novel method designed specifically for sparse LLMs. SEFT dynamically evolves
the sparse topology of pruned models during fine-tuning, while preserving the
overall sparsity throughout the process. The strengths of SEFT lie in its
ability to perform task-specific adaptation through a weight drop-and-grow
strategy, enabling the pruned model to self-adapt its sparse connectivity
pattern based on the target dataset. Furthermore, a sensitivity-driven pruning
criterion is employed to ensure that the desired sparsity level is consistently
maintained throughout fine-tuning. Our experiments on various LLMs, including
LLaMA families, DeepSeek, and Mistral, across a diverse set of benchmarks
demonstrate that SEFT achieves stronger performance while offering superior
memory and time efficiency compared to existing baselines. Our code is publicly
available at: https://github.com/QiaoXiao7282/SEFT.

</details>


### [313] [mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.24073)
*Chan-Wei Hu,Yueqi Wang,Shuo Xing,Chia-Ju Chen,Zhengzhong Tu*

Key words: 大型视觉语言模型, 检索增强生成, 多模态任务, 自反思框架

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文系统分析了大型视觉语言模型（LVLM）通过检索增强生成（RAG）提升性能的流程，包括检索、重新排序和生成阶段的研究。

Motivation: 解决LVLM静态训练数据、幻觉问题及无法动态验证信息的局限性，通过RAG提升模型在动态现实应用中的表现。

Method: 研究多模态RAG流程，包括检索阶段的模态配置与策略、重新排序阶段的策略以减少位置偏见，以及生成阶段的最佳整合方式。

Result: 通过全栈探索RAG流程，无需微调即可平均提升5%的性能。

Conclusion: RAG能有效增强LVLM性能，且自反思框架可动态选择相关证据，抑制无关内容。

Abstract: Large Vision-Language Models (LVLMs) have made remarkable strides in
multimodal tasks such as visual question answering, visual grounding, and
complex reasoning. However, they remain limited by static training data,
susceptibility to hallucinations, and inability to verify claims against
up-to-date, external evidence, compromising their performance in dynamic
real-world applications. Retrieval-Augmented Generation (RAG) offers a
practical solution to mitigate these challenges by allowing the LVLMs to access
large-scale knowledge databases via retrieval mechanisms, thereby grounding
model outputs in factual, contextually relevant information. Here in this
paper, we conduct the first systematic dissection of the multimodal RAG
pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the
modality configurations and retrieval strategies, (2) the re-ranking stage: on
strategies to mitigate positional biases and improve the relevance of retrieved
evidence, and (3) the generation phase: we further investigate how to best
integrate retrieved candidates into the final generation process. Finally, we
extend to explore a unified agentic framework that integrates re-ranking and
generation through self-reflection, enabling LVLMs to select relevant evidence
and suppress irrelevant context dynamically. Our full-stack exploration of RAG
for LVLMs yields substantial insights, resulting in an average performance
boost of 5% without any fine-tuning.

</details>


### [314] [SCOUT: Teaching Pre-trained Language Models to Enhance Reasoning via Flow Chain-of-Thought](https://arxiv.org/abs/2505.24181)
*Guanghao Li,Wenhao Jiang,Mingfeng Chen,Yan Li,Hao Yu,Shuting Dong,Tao Ren,Ming Tang,Chun Yuan*

Key words: Flow CoT, SCOUT, 递归推理, 认知优化, 大语言模型

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了Flow CoT（链式推理流）和SCOUT框架，通过递归推理和多阶段认知优化提升大语言模型的推理能力，无需预训练即可实现渐进式深度推理。

Motivation: 解决传统链式推理（CoT）依赖中间步骤、扩展性和泛化性不足的问题，同时避免递归推理方法的高预训练成本和缺乏理论框架的缺陷。

Method: 提出Flow CoT，将递归推理建模为潜在认知状态的渐进轨迹；设计SCOUT框架，通过逐步蒸馏和跨注意力回顾模块实现迭代优化。

Result: 在8个推理基准测试中，SCOUT显著提升准确性和解释质量（最高提升1.8%），并能逐步深化推理逻辑。

Conclusion: Flow CoT是一种可扩展的推理增强框架，SCOUT验证了其实际可行性，为语言模型的深层推理提供了轻量级解决方案。

Abstract: Chain of Thought (CoT) prompting improves the reasoning performance of large
language models (LLMs) by encouraging step by step thinking. However, CoT-based
methods depend on intermediate reasoning steps, which limits scalability and
generalization. Recent work explores recursive reasoning, where LLMs reuse
internal layers across iterations to refine latent representations without
explicit CoT supervision. While promising, these approaches often require
costly pretraining and lack a principled framework for how reasoning should
evolve across iterations. We address this gap by introducing Flow Chain of
Thought (Flow CoT), a reasoning paradigm that models recursive inference as a
progressive trajectory of latent cognitive states. Flow CoT frames each
iteration as a distinct cognitive stage deepening reasoning across iterations
without relying on manual supervision. To realize this, we propose SCOUT
(Stepwise Cognitive Optimization Using Teachers), a lightweight fine tuning
framework that enables Flow CoT style reasoning without the need for
pretraining. SCOUT uses progressive distillation to align each iteration with a
teacher of appropriate capacity, and a cross attention based retrospective
module that integrates outputs from previous iterations while preserving the
models original computation flow. Experiments across eight reasoning benchmarks
show that SCOUT consistently improves both accuracy and explanation quality,
achieving up to 1.8% gains under fine tuning. Qualitative analyses further
reveal that SCOUT enables progressively deeper reasoning across iterations
refining both belief formation and explanation granularity. These results not
only validate the effectiveness of SCOUT, but also demonstrate the practical
viability of Flow CoT as a scalable framework for enhancing reasoning in LLMs.

</details>


### [315] [Learning API Functionality from Demonstrations for Tool-based Agents](https://arxiv.org/abs/2505.24197)
*Bhrij Patel,Ashish Jagmohan,Aditya Vempaty*

Key words: API-based agents, demonstrations, LLM, documentation-free, self-improving

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种直接从演示中学习API功能的新方法，解决了依赖过时或缺失文档的问题，并通过实验验证了演示数量和LLM生成摘要对任务成功率的影响。

Motivation: 由于API文档经常缺失、过时或不一致，阻碍了可靠通用代理的开发，因此研究如何直接从演示中学习API功能成为必要。

Method: 收集专家代理和自我探索的API演示，研究演示数量及LLM生成摘要和评估对任务成功率的影响。

Result: 实验表明，提供显式函数调用和自然语言批评可显著提高代理的任务成功率，但直接从演示学习仍具挑战性。

Conclusion: 论文强调了无文档、自改进API代理的关键挑战，为未来研究指明了方向。

Abstract: Digital tool-based agents that invoke external Application Programming
Interfaces (APIs) often rely on documentation to understand API functionality.
However, such documentation is frequently missing, outdated, privatized, or
inconsistent-hindering the development of reliable, general-purpose agents. In
this work, we propose learning API functionality directly from demonstrations
as a new paradigm applicable in scenarios without documentation. Using existing
API benchmarks, we collect demonstrations from both expert API-based agents and
from self-exploration. To understand what information demonstrations must
convey for successful task completion, we extensively study how the number of
demonstrations and the use of LLM-generated summaries and evaluations affect
the task success rate of the API-based agent. Our experiments across 3 datasets
and 5 models show that learning functionality from demonstrations remains a
non-trivial challenge, even for state-of-the-art LLMs. We find that providing
explicit function calls and natural language critiques significantly improves
the agent's task success rate due to more accurate parameter filling. We
analyze failure modes, identify sources of error, and highlight key open
challenges for future work in documentation-free, self-improving, API-based
agents.

</details>


### [316] [SentinelAgent: Graph-based Anomaly Detection in Multi-Agent Systems](https://arxiv.org/abs/2505.24201)
*Xu He,Di Wu,Yan Zhai,Kun Sun*

Key words: 多代理系统, 异常检测, 大型语言模型, 安全防护, SentinelAgent

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一个针对基于大型语言模型的多代理系统的系统级异常检测框架，通过图模型和运行时行为监控，实现对节点、边和路径级别的语义异常检测，并引入了可插拔的SentinelAgent进行安全策略干预。

Motivation: 随着基于大型语言模型的多代理系统（MAS）的兴起，其在复杂任务分解和协调中表现出巨大潜力，但也面临多方面的安全和可靠性挑战，如提示操纵、不安全工具使用和代理间协调失败。现有保护机制仅在输入-输出级别提供部分保护，无法解决系统性问题或多点故障。因此，亟需一种系统级异常检测方法。

Method: 提出的框架包含两个核心组件：1) 基于图的模型，将代理交互建模为动态执行图，实现对节点、边和路径级别的语义异常检测；2) 可插拔的SentinelAgent，通过大型语言模型提供运行时行为监控和干预，基于安全策略和上下文推理。

Result: 通过电子邮件助手和Microsoft Magentic-One系统的案例研究，验证了该框架能够检测隐藏风险（如单点故障、提示注入、多代理共谋）并提供可解释的根因分析。

Conclusion: 该工作为更可信、可监控和安全的基于代理的AI生态系统奠定了基础。

Abstract: The rise of large language model (LLM)-based multi-agent systems (MAS)
introduces new security and reliability challenges. While these systems show
great promise in decomposing and coordinating complex tasks, they also face
multi-faceted risks across prompt manipulation, unsafe tool usage, and emergent
agent miscoordination. Existing guardrail mechanisms offer only partial
protection, primarily at the input-output level, and fall short in addressing
systemic or multi-point failures in MAS. In this work, we present a
system-level anomaly detection framework tailored for MAS, integrating
structural modeling with runtime behavioral oversight. Our approach consists of
two components. First, we propose a graph-based framework that models agent
interactions as dynamic execution graphs, enabling semantic anomaly detection
at node, edge, and path levels. Second, we introduce a pluggable SentinelAgent,
an LLM-powered oversight agent that observes, analyzes, and intervenes in MAS
execution based on security policies and contextual reasoning. By bridging
abstract detection logic with actionable enforcement, our method detects not
only single-point faults and prompt injections but also multi-agent collusion
and latent exploit paths. We validate our framework through two case studies,
including an email assistant and Microsoft's Magentic-One system, demonstrating
its ability to detect covert risks and provide explainable root-cause
attribution. Our work lays the foundation for more trustworthy, monitorable,
and secure agent-based AI ecosystems.

</details>


### [317] [Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap](https://arxiv.org/abs/2505.24208)
*Wenhan Yang,Spencer Stice,Ali Payani,Baharan Mirzasoleiman*

Key words: 视觉-语言模型, 模态差距, 安全性, 预处理, 正则化

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 研究发现视觉-语言模型（VLMs）的安全性因模态差距而下降，提出一种预处理正则化方法显著提升安全性。

Motivation: 为确保VLMs生成安全的输出，探究模态差距对安全性的影响。

Method: 通过实验验证模态差距与安全性的负相关性，并提出预处理正则化方法减小差距。

Result: 方法显著降低不安全率（最高16.3%），且不影响性能，还能增强现有防御（最高18.2%）。

Conclusion: 模态差距是影响VLMs安全性的关键因素，预处理正则化是有效解决方案。

Abstract: Ensuring Vision-Language Models (VLMs) generate safe outputs is crucial for
their reliable deployment. However, LVLMs suffer from drastic safety
degradation compared to their LLM backbone. Even blank or irrelevant images can
trigger LVLMs to generate harmful responses to prompts that would otherwise be
refused in text-only contexts. The modality gap between image and text
representations has been recently hypothesized to contribute to safety
degradation of LVLMs. However, if and how the amount of modality gap affects
LVLMs' safety is not studied. In this work, we show that the amount of modality
gap is highly inversely correlated with VLMs' safety. Then, we show that this
modality gap is introduced during pretraining LVLMs and persists through
fine-tuning. Inspired by this observation, we propose a regularization to
reduce the modality gap during pretraining. Our extensive experiments on LLaVA
v1.5, ShareGPT4V, and MiniGPT-4 show that our method substantially improves
safety alignment of LVLMs, reducing unsafe rate by up to 16.3% without
compromising performance, and can further boost existing defenses by up to
18.2%.

</details>


### [318] [E^2GraphRAG: Streamlining Graph-based RAG for High Efficiency and Effectiveness](https://arxiv.org/abs/2505.24226)
*Yibo Zhao,Jiapeng Zhu,Ye Guo,Kangkang He,Xiang Li*

Key words: GraphRAG, Entity Graph, LLMs, Retrieval, Indexing

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: E^2GraphRAG proposes a more efficient and effective graph-based RAG framework by constructing a summary tree and entity graph, enabling faster indexing and adaptive retrieval while maintaining performance.

Motivation: Existing graph-based RAG methods like GraphRAG suffer from inefficiency and rely on manual query modes, limiting practical use.

Method: E^2GraphRAG constructs a summary tree with LLMs and an entity graph with SpaCy, then creates bidirectional indexes for fast lookup. An adaptive retrieval strategy selects between local and global modes.

Result: Experiments show E^2GraphRAG achieves 10x faster indexing than GraphRAG and 100x speedup over LightRAG while maintaining competitive QA performance.

Conclusion: E^2GraphRAG addresses inefficiency and flexibility issues in graph-based RAG methods, offering significant speed improvements without sacrificing accuracy.

Abstract: Graph-based RAG methods like GraphRAG have shown promising global
understanding of the knowledge base by constructing hierarchical entity graphs.
However, they often suffer from inefficiency and rely on manually pre-defined
query modes, limiting practical use. In this paper, we propose E^2GraphRAG, a
streamlined graph-based RAG framework that improves both Efficiency and
Effectiveness. During the indexing stage, E^2GraphRAG constructs a summary tree
with large language models and an entity graph with SpaCy based on document
chunks. We then construct bidirectional indexes between entities and chunks to
capture their many-to-many relationships, enabling fast lookup during both
local and global retrieval. For the retrieval stage, we design an adaptive
retrieval strategy that leverages the graph structure to retrieve and select
between local and global modes. Experiments show that E^2GraphRAG achieves up
to 10 times faster indexing than GraphRAG and 100 times speedup over LightRAG
in retrieval while maintaining competitive QA performance.

</details>


### [319] [ProofNet++: A Neuro-Symbolic System for Formal Proof Verification with Self-Correction](https://arxiv.org/abs/2505.24230)
*Murari Ambati*

Key words: 定理证明, 神经符号框架, 强化学习, 形式化验证, 自我修正

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: ProofNet++是一个结合大型语言模型与形式化验证和自我修正机制的神经符号框架，显著提升了定理证明的准确性和可验证性。

Motivation: 现有基于LLM的系统存在逻辑步骤虚构和推理不可验证的问题，ProofNet++旨在通过符号化监督和强化学习解决这些局限。

Method: 框架集成了符号化证明树监督、使用验证器作为奖励函数的强化学习循环，以及迭代自我修正模块。

Result: 在miniF2F、Lean's mathlib和HOL Light上的实验表明，ProofNet++在证明准确性、正确性和形式可验证性上显著优于现有模型。

Conclusion: ProofNet++为定理证明提供了可验证且高效的解决方案，并通过理论分析和开源数据推动了未来研究。

Abstract: We propose ProofNet++, a neuro-symbolic framework that enhances automated
theorem proving by combining large language models (LLMs) with formal proof
verification and self-correction mechanisms. Current LLM-based systems suffer
from hallucinated logical steps and unverifiable reasoning. ProofNet++
mitigates these limitations by integrating symbolic proof tree supervision, a
reinforcement learning loop using verifiers as reward functions, and an
iterative self-correction module. Our experiments on miniF2F, Lean's mathlib,
and HOL Light show that ProofNet++ significantly improves proof accuracy,
correctness, and formal verifiability over prior models. We provide theoretical
analysis of the convergence and stability of the verifier-guided RL framework
and release our datasets and codebase for future research.

</details>


### [320] [FABLE: A Novel Data-Flow Analysis Benchmark on Procedural Text for Large Language Model Evaluation](https://arxiv.org/abs/2505.24258)
*Vishal Pallagani,Nitin Gupta,John Aydin,Biplav Srivastava*

Key words: 数据流推理,FABLE,大型语言模型,程序理解,基准测试

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: FABLE是一个评估大型语言模型（LLM）在数据流推理能力的基准测试，覆盖了从软件工程中的八种分析方法和三个实际领域，结果显示推理专用模型准确率较高。

Motivation: 尽管LLMs在自然语言处理和编程任务中表现优异，但其数据流推理能力尚未系统性评估，因此设计了FABLE来填补这一空白。

Method: FABLE整合了八种经典数据流分析方法并将其应用于三个现实领域，生成了2400个问答对，评估了三类LLMs的性能。

Result: 专用推理模型（DeepSeek-R1 8B）准确率最高但推理速度慢20倍，通用模型和代码模型表现接近随机。

Conclusion: FABLE为系统性评估数据流推理能力提供了首个诊断基准，并为提升模型程序理解能力提供了方向。

Abstract: Understanding how data moves, transforms, and persists, known as data flow,
is fundamental to reasoning in procedural tasks. Despite their fluency in
natural and programming languages, large language models (LLMs), although
increasingly being applied to decisions with procedural tasks, have not been
systematically evaluated for their ability to perform data-flow reasoning. We
introduce FABLE, an extensible benchmark designed to assess LLMs' understanding
of data flow using structured, procedural text. FABLE adapts eight classical
data-flow analyses from software engineering: reaching definitions, very busy
expressions, available expressions, live variable analysis, interval analysis,
type-state analysis, taint analysis, and concurrency analysis. These analyses
are instantiated across three real-world domains: cooking recipes, travel
routes, and automated plans. The benchmark includes 2,400 question-answer
pairs, with 100 examples for each domain-analysis combination. We evaluate
three types of LLMs: a reasoning-focused model (DeepSeek-R1 8B), a
general-purpose model (LLaMA 3.1 8B), and a code-specific model (Granite Code
8B). Each model is tested using majority voting over five sampled completions
per prompt. Results show that the reasoning model achieves higher accuracy, but
at the cost of over 20 times slower inference compared to the other models. In
contrast, the general-purpose and code-specific models perform close to random
chance. FABLE provides the first diagnostic benchmark to systematically
evaluate data-flow reasoning and offers insights for developing models with
stronger procedural understanding.

</details>


### [321] [Generative AI for Urban Design: A Stepwise Approach Integrating Human Expertise with Multimodal Diffusion Models](https://arxiv.org/abs/2505.24260)
*Mingyi He,Yuebing Liang,Shenhao Wang,Yunhan Zheng,Qingyi Wang,Dingyi Zhuang,Li Tian,Jinhua Zhao*

Key words: 生成式人工智能、城市设计、多模态扩散模型、人机协作、迭代设计

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本研究提出了一种分步生成城市设计框架，通过多模态扩散模型结合人类专业知识，实现更具适应性和可控性的设计流程，分为三个阶段（路网与土地利用规划、建筑布局规划、详细规划与渲染），实验证明其在保真度、合规性和多样性上优于基准模型。

Motivation: 现有生成式人工智能方法在融入人类设计流程方面不足，多为端到端方式且控制有限，无法满足实际设计的迭代需求，因此研究提出分步框架以改进这一问题。

Method: 采用多模态扩散模型，分三阶段（路网规划、建筑布局、详细渲染）生成设计，并允许人类设计师审查与优化。

Result: 在芝加哥和纽约数据上的实验表明，该框架在保真度、合规性和多样性上全面优于基准模型和端到端方法。

Conclusion: 多模态扩散模型与分步生成能有效保持人类控制并支持迭代优化，为城市设计中人机协作奠定基础。

Abstract: Urban design is a multifaceted process that demands careful consideration of
site-specific constraints and collaboration among diverse professionals and
stakeholders. The advent of generative artificial intelligence (GenAI) offers
transformative potential by improving the efficiency of design generation and
facilitating the communication of design ideas. However, most existing
approaches are not well integrated with human design workflows. They often
follow end-to-end pipelines with limited control, overlooking the iterative
nature of real-world design. This study proposes a stepwise generative urban
design framework that integrates multimodal diffusion models with human
expertise to enable more adaptive and controllable design processes. Instead of
generating design outcomes in a single end-to-end process, the framework
divides the process into three key stages aligned with established urban design
workflows: (1) road network and land use planning, (2) building layout
planning, and (3) detailed planning and rendering. At each stage, multimodal
diffusion models generate preliminary designs based on textual prompts and
image-based constraints, which can then be reviewed and refined by human
designers. We design an evaluation framework to assess the fidelity,
compliance, and diversity of the generated designs. Experiments using data from
Chicago and New York City demonstrate that our framework outperforms baseline
models and end-to-end approaches across all three dimensions. This study
underscores the benefits of multimodal diffusion models and stepwise generation
in preserving human control and facilitating iterative refinements, laying the
groundwork for human-AI interaction in urban design solutions.

</details>


### [322] [How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning](https://arxiv.org/abs/2505.24273)
*Hongyi James Cai,Junlin Wang,Xiaoyin Chen,Bhuwan Dhingra*

Key words: 大型语言模型, 链式思考, 回溯, 监督微调, 强化学习

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型（LLM）中回溯对推理能力的贡献，发现较长的链式思考（CoT）和回溯步骤能提升强化学习（RL）训练效果，尤其在复杂任务中；同时，RL更关注结构模式而非内容正确性。

Motivation: 探索回溯在LLM推理中的具体贡献及其最优使用方式，以优化训练策略并提升模型复杂问题的解决能力。

Method: 在八项推理任务上系统分析监督微调（SFT）与RL的相互作用，构建含不同回溯步骤的合成数据集进行对照实验。

Result: 较长CoT和回溯能增强RL训练效果；任务难度越高，SFT阶段所需回溯越多；RL更依赖结构模式而非内容正确性。

Conclusion: 通过设计含结构化回溯的SFT数据可有效提升LLM的推理能力，为训练策略提供实践指导。

Abstract: Recent breakthroughs in large language models (LLMs) have effectively
improved their reasoning abilities, particularly on mathematical and logical
problems that have verifiable answers, through techniques such as supervised
finetuning (SFT) and reinforcement learning (RL). Prior research indicates that
RL effectively internalizes search strategies, enabling long chain-of-thought
(CoT) reasoning, with backtracking emerging naturally as a learned capability.
However, the precise benefits of backtracking, specifically, how significantly
it contributes to reasoning improvements and the optimal extent of its use,
remain poorly understood. In this work, we systematically investigate the
dynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc
1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self
Reference. Our findings highlight that short CoT sequences used in SFT as a
warm-up do have moderate contribution to RL training, compared with cold-start
RL; however such contribution diminishes when tasks become increasingly
difficult. Motivated by this observation, we construct synthetic datasets
varying systematically in the number of backtracking steps and conduct
controlled experiments to isolate the influence of either the correctness
(content) or the structure (i.e., backtrack frequency). We find that (1) longer
CoT with backtracks generally induce better and more stable RL training, (2)
more challenging problems with larger search space tend to need higher numbers
of backtracks during the SFT stage. Additionally, we demonstrate through
experiments on distilled data that RL training is largely unaffected by the
correctness of long CoT sequences, suggesting that RL prioritizes structural
patterns over content correctness. Collectively, our results offer practical
insights into designing optimal training strategies to effectively scale
reasoning in LLMs.

</details>


### [323] [Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules](https://arxiv.org/abs/2505.24292)
*Yueqi Zhang,Peiwen Yuan,Shaoxiong Feng,Yiwei Li,Xinglin Wang,Jiayi Shi,Chuyi Tan,Boyuan Pan,Yao Hu,Kan Li*

Key words: 人-AI对话, 引用文本, 注意力机制, QuAda, 语言模型

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种针对人-AI对话中引用文本的生成方法（QuAda），通过动态调整注意力来优化模型对引用内容的处理。

Motivation: 现有的语言模型缺乏显式机制来处理对话中的引用文本，导致引用内容无法被有效利用。

Method: 提出了一个以引用为中心的数据处理流程，并设计了一个轻量级训练方法（QuAda），通过动态调整注意力来增强或抑制对引用内容的关注。

Result: QuAda在多种场景中表现良好，且能泛化到未见话题，成为一种有效的即插即用解决方案。

Conclusion: QuAda为引用感知的对话提供了一个高效且通用的解决方案。

Abstract: Human-AI conversation frequently relies on quoting earlier text-"check it
with the formula I just highlighted"-yet today's large language models (LLMs)
lack an explicit mechanism for locating and exploiting such spans. We formalise
the challenge as span-conditioned generation, decomposing each turn into the
dialogue history, a set of token-offset quotation spans, and an intent
utterance. Building on this abstraction, we introduce a quotation-centric data
pipeline that automatically synthesises task-specific dialogues, verifies
answer correctness through multi-stage consistency checks, and yields both a
heterogeneous training corpus and the first benchmark covering five
representative scenarios. To meet the benchmark's zero-overhead and
parameter-efficiency requirements, we propose QuAda, a lightweight
training-based method that attaches two bottleneck projections to every
attention head, dynamically amplifying or suppressing attention to quoted spans
at inference time while leaving the prompt unchanged and updating < 2.8% of
backbone weights. Experiments across models show that QuAda is suitable for all
scenarios and generalises to unseen topics, offering an effective,
plug-and-play solution for quotation-aware dialogue.

</details>


### [324] [GridRoute: A Benchmark for LLM-Based Route Planning with Cardinal Movement in Grid Environments](https://arxiv.org/abs/2505.24306)
*Kechen Li,Yaotian Tao,Ximing Wen,Quanwei Sun,Zifei Gong,Chang Xu,Xizhe Zhang,Tianbo Ji*

Key words: Large Language Models, path planning, hybrid prompting, traditional algorithms, GridRoute

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出GridRoute基准测试和混合提示技术AoT，评估LLMs与传统算法的协同作用，结果显示AoT显著提升性能。

Motivation: 研究LLMs与传统算法在路径规划中的协同潜力，填补现有研究空白。

Method: 提出GridRoute基准测试和AoT混合提示技术，结合传统算法指导。

Result: AoT显著提升LLMs在各种规模环境中的表现，尤其是在复杂场景下。

Conclusion: AoT为路径规划问题提供了有前景的解决方案。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated their
potential in planning and reasoning tasks, offering a flexible alternative to
classical pathfinding algorithms. However, most existing studies focus on LLMs'
independent reasoning capabilities and overlook the potential synergy between
LLMs and traditional algorithms. To fill this gap, we propose a comprehensive
evaluation benchmark GridRoute to assess how LLMs can take advantage of
traditional algorithms. We also propose a novel hybrid prompting technique
called Algorithm of Thought (AoT), which introduces traditional algorithms'
guidance into prompting. Our benchmark evaluates six LLMs ranging from 7B to
72B parameters across various map sizes, assessing their performance in
correctness, optimality, and efficiency in grid environments with varying
sizes. Our results show that AoT significantly boosts performance across all
model sizes, particularly in larger or more complex environments, suggesting a
promising approach to addressing path planning challenges. Our code is
open-sourced at https://github.com/LinChance/GridRoute.

</details>


### [325] [Three Kinds of Negation in Knowledge and Their Mathematical Foundations](https://arxiv.org/abs/2505.24422)
*Zhenghua Pan,Yong Wang*

Key words: 否定, 知识处理, SCOI, LCOI

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 研究了知识中否定的分类及其数学基础，提出三种否定类型并建立了SCOI和LCOI模型。

Motivation: 探讨知识处理中否定的理解与分类问题。

Method: 分析哲学、逻辑学和语言学中的否定概念，提出三种否定类型并建立数学模型。

Result: 成功定义SCOI和LCOI，并证明了其运算性质和形式推理关系。

Conclusion: 提出了否定分类的新视角，为知识处理提供了数学基础。

Abstract: In the field of artificial intelligence, understanding, distinguishing,
expressing, and computing the negation in knowledge is a fundamental issue in
knowledge processing and research. In this paper, we examine and analyze the
understanding and characteristics of negation in various fields such as
philosophy, logic, and linguistics etc. Based on the distinction between the
concepts of contradiction and opposition, we propose that there are three
different types of negation in knowledge from a conceptual perspective:
contradictory negation, opposite negation, and intermediary negation. To
establish a mathematical foundation that fully reflects the intrinsic
connections, properties, and laws of these different forms of negation, we
introduce SCOI: sets with contradictory negation, opposite negation and
intermediary negation, and LCOI: logic with contradictory negation, opposite
negation and intermediary negation, and we proved the main operational
properties of SCOI as well as the formal inference relations in LCOI.

</details>


### [326] [P: A Universal Measure of Predictive Intelligence](https://arxiv.org/abs/2505.24426)
*David Gamez*

Key words: 智力测量, 通用智力, 预测, Kolmogorov复杂性, AI, 虚拟迷宫, 时间序列

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于预测的通用智力测量方法，通过预测准确性和复杂性来衡量人类、动物和AI的智力，并在虚拟迷宫和时间序列数据中验证了其可行性。

Motivation: 当前缺乏公认的智力定义和统一的测量方法，无法比较人类、动物和AI的智力水平。论文旨在填补这一理论空白。

Method: 基于预测作为智力核心的假设，通过Kolmogorov复杂性衡量预测和环境复杂性，并设计算法在虚拟迷宫和时间序列数据中测试。

Result: 实验证明该方法能有效测量虚拟迷宫和时间序列预测任务中的智力表现，初步验证了其通用性。

Conclusion: 该通用测量方法为跨物种和AI的智力比较科学提供了起点，但仍需进一步验证和完善。

Abstract: Over the last thirty years, considerable progress has been made with the
development of systems that can drive cars, play games, predict protein folding
and generate natural language. These systems are described as intelligent and
there has been a great deal of talk about the rapid increase in artificial
intelligence and its potential dangers. However, our theoretical understanding
of intelligence and ability to measure it lag far behind our capacity for
building systems that mimic intelligent human behaviour. There is no commonly
agreed definition of the intelligence that AI systems are said to possess.
No-one has developed a practical measure that would enable us to compare the
intelligence of humans, animals and AIs on a single ratio scale.
  This paper sets out a new universal measure of intelligence that is based on
the hypothesis that prediction is the most important component of intelligence.
As an agent interacts with its normal environment, the accuracy of its
predictions is summed up and the complexity of its predictions and perceived
environment is accounted for using Kolmogorov complexity. Two experiments were
carried out to evaluate the practical feasibility of the algorithm. These
demonstrated that it could measure the intelligence of an agent embodied in a
virtual maze and an agent that makes predictions about time-series data. This
universal measure could be the starting point for a new comparative science of
intelligence that ranks humans, animals and AIs on a single ratio scale.

</details>


### [327] [RMoA: Optimizing Mixture-of-Agents through Diversity Maximization and Residual Compensation](https://arxiv.org/abs/2505.24442)
*Zhentao Xie,Chengcheng Han,Jinxin Shi,Wenjun Cui,Xin Zhao,Xingjiao Wu,Jiabao Zhao*

Key words: 多智能体系统, 残差连接, 嵌入选择, 自适应终止, 计算效率

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了Residual Mixture-of-Agents (RMoA)，通过残差连接优化效率和可靠性，利用嵌入多样性选择机制和自适应终止机制减少计算开销，同时在多个任务上取得最优性能。

Motivation: 针对当前基于大语言模型的多智能体系统存在的高计算开销、信息丢失和鲁棒性问题，受ResNet残差学习启发，提出RMoA以提升效率和可靠性。

Method: 设计了基于嵌入的多样性选择机制贪心选择响应，引入残差提取代理捕获跨层增量信息，残差聚合代理进行分层信息整合，并提出自适应终止机制动态停止处理。

Result: 在对齐、数学推理、代码生成和多任务理解等基准测试中达到最优性能，同时显著减少计算开销。

Conclusion: RMoA通过残差连接和新型机制有效优化了多智能体系统的性能和效率，为未来研究提供了重要参考。

Abstract: Although multi-agent systems based on large language models show strong
capabilities on multiple tasks, they are still limited by high computational
overhead, information loss, and robustness. Inspired by ResNet's residual
learning, we propose Residual Mixture-of-Agents (RMoA), integrating residual
connections to optimize efficiency and reliability. To maximize information
utilization from model responses while minimizing computational costs, we
innovatively design an embedding-based diversity selection mechanism that
greedily selects responses via vector similarity. Furthermore, to mitigate
iterative information degradation, we introduce a Residual Extraction Agent to
preserve cross-layer incremental information by capturing inter-layer response
differences, coupled with a Residual Aggregation Agent for hierarchical
information integration. Additionally, we propose an adaptive termination
mechanism that dynamically halts processing based on residual convergence,
further improving inference efficiency. RMoA achieves state-of-the-art
performance on the benchmarks of across alignment, mathematical reasoning, code
generation, and multitasking understanding, while significantly reducing
computational overhead. Code is available at
https://github.com/mindhunter01/RMoA.

</details>


### [328] [SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social Engineering Behaviors](https://arxiv.org/abs/2505.24458)
*Tianlong Yu,Chenghang Ye,Zheyu Yang,Ziyi Zhou,Cui Tang,Zui Tao,Jun Zhang,Kailong Wang,Liting Zhou,Yang Yang,Ting Bi*

Key words: 社交工程攻击、增强现实、多模态大语言模型、数据集、信任操控

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: SEAR数据集是一个多模态资源，用于研究通过增强现实（AR）和多模态大语言模型（LLM）发起的社交工程（SE）攻击。该数据集包含60名参与者在模拟敌对场景中的180条标注对话，涵盖视觉/音频线索、环境背景和社交媒体资料，揭示了SEAR在诱导合规和操控信任上的高效性。

Motivation: 研究通过AR和LLM发起的社交工程攻击，填补相关领域数据和研究空白。

Method: 构建SEAR数据集，包含60名参与者在模拟敌对场景（如会议、课堂）中的180条标注对话，同步记录AR视觉/音频线索、环境背景及主观信任评估。

Result: SEAR表现出高效攻击性（93.3%钓鱼链接点击率、85%通话接受率）和信任操控能力（76.7%信任度提升）。

Conclusion: SEAR数据集支持AR驱动的SE攻击检测、防御框架设计及多模态对抗操控研究，并遵循严格伦理规范。

Abstract: The SEAR Dataset is a novel multimodal resource designed to study the
emerging threat of social engineering (SE) attacks orchestrated through
augmented reality (AR) and multimodal large language models (LLMs). This
dataset captures 180 annotated conversations across 60 participants in
simulated adversarial scenarios, including meetings, classes and networking
events. It comprises synchronized AR-captured visual/audio cues (e.g., facial
expressions, vocal tones), environmental context, and curated social media
profiles, alongside subjective metrics such as trust ratings and susceptibility
assessments. Key findings reveal SEAR's alarming efficacy in eliciting
compliance (e.g., 93.3% phishing link clicks, 85% call acceptance) and
hijacking trust (76.7% post-interaction trust surge). The dataset supports
research in detecting AR-driven SE attacks, designing defensive frameworks, and
understanding multimodal adversarial manipulation. Rigorous ethical safeguards,
including anonymization and IRB compliance, ensure responsible use. The SEAR
dataset is available at https://github.com/INSLabCN/SEAR-Dataset.

</details>


### [329] [Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning](https://arxiv.org/abs/2505.24478)
*Vasilije Markovic,Lazar Obradovic,Laszlo Hajdu,Jovan Pavlovic*

Key words: Large Language Models, Knowledge Graphs, Hyperparameter Optimization, Retrieval-Augmented Generation, Cognee

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文研究了在LLMs与KGs结合的系统中，超参数优化对性能的影响，并通过实验证明了定向调参可以带来显著提升。

Motivation: 探索在检索增强生成系统中，系统性超参数优化的作用及其潜在价值。

Method: 在Cognee框架中，针对分块、图构建、检索和提示等参数进行优化，并使用多个QA基准数据集进行评估。

Result: 实验结果表明，定向调参能带来一致的性能提升，但效果因数据集和评估指标而异。

Conclusion: 未来进展不仅依赖于架构改进，还需更清晰的优化和评估框架。

Abstract: Integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) results
in complex systems with numerous hyperparameters that directly affect
performance. While such systems are increasingly common in retrieval-augmented
generation, the role of systematic hyperparameter optimization remains
underexplored. In this paper, we study this problem in the context of Cognee, a
modular framework for end-to-end KG construction and retrieval. Using three
multi-hop QA benchmarks (HotPotQA, TwoWikiMultiHop, and MuSiQue) we optimize
parameters related to chunking, graph construction, retrieval, and prompting.
Each configuration is scored using established metrics (exact match, F1, and
DeepEval's LLM-based correctness metric). Our results demonstrate that
meaningful gains can be achieved through targeted tuning. While the gains are
consistent, they are not uniform, with performance varying across datasets and
metrics. This variability highlights both the value of tuning and the
limitations of standard evaluation measures. While demonstrating the immediate
potential of hyperparameter tuning, we argue that future progress will depend
not only on architectural advances but also on clearer frameworks for
optimization and evaluation in complex, modular systems.

</details>


### [330] [Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation](https://arxiv.org/abs/2505.24479)
*Sania Nayab,Marco Simoni,Giulio Rossolini*

Key words: misinformation, knowledge graphs (KGs), large language models (LLMs), fake triplets, detection methods

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: A novel approach using knowledge graphs (KGs) to systematically generate misinformation, highlighting limitations in LLM-based detection methods.

Motivation: Addressing the societal threats of misinformation amplified by AI by exploring structured methodologies to generate and assess false information.

Method: Leveraging KGs to identify plausibly false relationships (fake triplets) and using these to guide LLMs in generating misinformation with varying credibility levels.

Result: The deterministic approach produces misinformation challenging for humans to detect, and current LLM-based detection methods show significant limitations.

Conclusion: Enhanced detection strategies and deeper investigation of generative model biases are necessary to combat AI-driven misinformation effectively.

Abstract: The rapid spread of misinformation, further amplified by recent advances in
generative AI, poses significant threats to society, impacting public opinion,
democratic stability, and national security. Understanding and proactively
assessing these threats requires exploring methodologies that enable structured
and scalable misinformation generation. In this paper, we propose a novel
approach that leverages knowledge graphs (KGs) as structured semantic resources
to systematically generate fake triplets. By analyzing the structural
properties of KGs, such as the distance between entities and their predicates,
we identify plausibly false relationships. These triplets are then used to
guide large language models (LLMs) in generating misinformation statements with
varying degrees of credibility. By utilizing structured semantic relationships,
our deterministic approach produces misinformation inherently challenging for
humans to detect, drawing exclusively upon publicly available KGs (e.g.,
WikiGraphs).
  Additionally, we investigate the effectiveness of LLMs in distinguishing
between genuine and artificially generated misinformation. Our analysis
highlights significant limitations in current LLM-based detection methods,
underscoring the necessity for enhanced detection strategies and a deeper
exploration of inherent biases in generative models.

</details>


### [331] [MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging LLM Embedded Knowledge](https://arxiv.org/abs/2505.24493)
*Xin Jing,Jiadong Wang,Iosif Tsangko,Andreas Triantafyllopoulos,Björn W. Schuller*

Key words: 语音情感识别, 大型语言模型, GPT-4o, 多模态标注, 自监督学习

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文探讨了利用GPT-4o标注多模态情感数据集（MELT）的可行性，仅通过文本提示生成准确标注，并验证了其在语音情感识别（SER）任务中的有效性。

Motivation: 解决传统人工标注在语音情感识别中成本高、不一致的问题，探索LLMs在无需人工监督下标注多模态数据的潜力。

Method: 通过结构化文本提示，使用GPT-4o标注Friends情景剧的多模态数据集，并基于MELT微调四种自监督学习模型。

Result: MELT标注质量高，微调后的模型在SER任务中表现一致提升。

Conclusion: GPT-4o可高效生成多模态情感标注，MELT为SER研究提供了新资源。

Abstract: Although speech emotion recognition (SER) has advanced significantly with
deep learning, annotation remains a major hurdle. Human annotation is not only
costly but also subject to inconsistencies annotators often have different
preferences and may lack the necessary contextual knowledge, which can lead to
varied and inaccurate labels. Meanwhile, Large Language Models (LLMs) have
emerged as a scalable alternative for annotating text data. However, the
potential of LLMs to perform emotional speech data annotation without human
supervision has yet to be thoroughly investigated. To address these problems,
we apply GPT-4o to annotate a multimodal dataset collected from the sitcom
Friends, using only textual cues as inputs. By crafting structured text
prompts, our methodology capitalizes on the knowledge GPT-4o has accumulated
during its training, showcasing that it can generate accurate and contextually
relevant annotations without direct access to multimodal inputs. Therefore, we
propose MELT, a multimodal emotion dataset fully annotated by GPT-4o. We
demonstrate the effectiveness of MELT by fine-tuning four self-supervised
learning (SSL) backbones and assessing speech emotion recognition performance
across emotion datasets. Additionally, our subjective experiments\' results
demonstrate a consistence performance improvement on SER.

</details>


### [332] [Mixture-of-Experts for Personalized and Semantic-Aware Next Location Prediction](https://arxiv.org/abs/2505.24597)
*Shuai Liu,Ning Cao,Yile Chen,Yue Jiang,Gao Cong*

Key words: 位置预测,混合专家模型,人类移动模式,LLM

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: NextLocMoE是一个基于混合专家（MoE）设计的框架，用于解决现有方法在捕捉位置多功能语义和用户行为动态性方面的不足，通过双模块结构和历史感知路由机制提升预测性能。

Motivation: 解决现有方法在捕捉位置多功能语义和用户行为动态性方面的局限性。

Method: 引入NextLocMoE框架，基于LLM和MoE设计，包含位置语义MoE和个性化MoE模块，并采用历史感知路由机制。

Result: 在多个真实城市数据集上表现出更高的预测准确性、跨域泛化能力和可解释性。

Conclusion: NextLocMoE有效提升了位置预测性能，具有广泛的应用潜力。

Abstract: Next location prediction plays a critical role in understanding human
mobility patterns. However, existing approaches face two core limitations: (1)
they fall short in capturing the complex, multi-functional semantics of
real-world locations; and (2) they lack the capacity to model heterogeneous
behavioral dynamics across diverse user groups. To tackle these challenges, we
introduce NextLocMoE, a novel framework built upon large language models (LLMs)
and structured around a dual-level Mixture-of-Experts (MoE) design. Our
architecture comprises two specialized modules: a Location Semantics MoE that
operates at the embedding level to encode rich functional semantics of
locations, and a Personalized MoE embedded within the Transformer backbone to
dynamically adapt to individual user mobility patterns. In addition, we
incorporate a history-aware routing mechanism that leverages long-term
trajectory data to enhance expert selection and ensure prediction stability.
Empirical evaluations across several real-world urban datasets show that
NextLocMoE achieves superior performance in terms of predictive accuracy,
cross-domain generalization, and interpretability

</details>


### [333] [Taxonomic Networks: A Representation for Neuro-Symbolic Pairing](https://arxiv.org/abs/2505.24601)
*Zekun Wang,Ethan L. Haarer,Nicki Barari,Christopher J. MacLellan*

Key words: 神经符号对, 分类网络, 知识表示, 混合计算

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种名为'神经符号对'的概念，通过共同的知识表示将神经与符号方法结合，并利用分类网络实现高效学习与高准确率切换。

Motivation: 研究旨在通过结合神经与符号方法的优势，构建更灵活且高效的混合计算系统，以适应不同情境需求。

Method: 提出'分类网络'作为知识表示形式，构建神经符号对并评估其性能，测试两种方法在不同资源条件下的表现。

Result: 符号方法在数据与计算资源较少时更高效，而神经方法在资源充足时准确率更高；两者可无缝切换。

Conclusion: 神经符号对为未来更深入的神经与符号计算集成奠定了基础。

Abstract: We introduce the concept of a \textbf{neuro-symbolic pair} -- neural and
symbolic approaches that are linked through a common knowledge representation.
Next, we present \textbf{taxonomic networks}, a type of discrimination network
in which nodes represent hierarchically organized taxonomic concepts. Using
this representation, we construct a novel neuro-symbolic pair and evaluate its
performance. We show that our symbolic method learns taxonomic nets more
efficiently with less data and compute, while the neural method finds
higher-accuracy taxonomic nets when provided with greater resources. As a
neuro-symbolic pair, these approaches can be used interchangeably based on
situational needs, with seamless translation between them when necessary. This
work lays the foundation for future systems that more fundamentally integrate
neural and symbolic computation.

</details>


### [334] [Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated Questions for Predicting Startup Success](https://arxiv.org/abs/2505.24622)
*Ben Griffin,Joseph Ternasky,Fuat Alican,Yigit Ihlamur*

Key words: startup success prediction, LLM, ensemble framework, interpretability, venture capital

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一个轻量级的集成框架，利用大型语言模型生成的问题构建透明决策系统，显著提高了初创公司成功预测的精确度。

Motivation: 预测初创公司成功需要既准确又可解释的模型，结合LLM的推理与人类洞察力以实现这一目标。

Method: 通过筛选、排序和基于阈值的投票机制整合LLM生成的YES/NO问题，形成强集成预测器。

Result: 测试集上精确度达50%，比随机选择提升5倍；加入专家引导的启发式后提升至54%。

Conclusion: 简单可解释的集成模型能有效支持高风险决策（如VC领域），LLM与人类洞察的结合具有重要价值。

Abstract: Predicting startup success requires models that are both accurate and
interpretable. We present a lightweight ensemble framework that combines YES/NO
questions generated by large language models (LLMs), forming a transparent
decision-making system. Each question acts as a weak heuristic, and by
filtering, ranking, and aggregating them through a threshold-based voting
mechanism, we construct a strong ensemble predictor. On a test set where 10% of
startups are classified as successful, our approach achieves a precision rate
of 50%, representing a 5x improvement over random selection, while remaining
fully transparent. When we incorporate expert-guided heuristics into the
generation process, performance improves further to 54% precision. These
results highlight the value of combining LLM reasoning with human insight and
demonstrate that simple, interpretable ensembles can support high-stakes
decisions in domains such as venture capital (VC).

</details>


### [335] [Adaptable Cardiovascular Disease Risk Prediction from Heterogeneous Data using Large Language Models](https://arxiv.org/abs/2505.24655)
*Frederike Lübeck,Jonas Wildberger,Frederik Träuble,Maximilian Mordig,Sergios Gatidis,Andreas Krause,Bernhard Schölkopf*

Key words: cardiovascular disease, risk prediction, large language models, clinical decision support, UK Biobank

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: AdaCVD is a new adaptable CVD risk prediction model using large language models, outperforming traditional methods by flexibly handling diverse patient data and adapting to new populations with minimal data.

Motivation: Existing CVD risk models are too rigid and struggle with real-world clinical complexities, leading to a need for a more adaptable solution.

Method: Built on large language models fine-tuned on UK Biobank data (over 500,000 participants), AdaCVD integrates structured and unstructured data flexibly.

Result: AdaCVD achieves state-of-the-art performance, excels in handling diverse patient groups, and adapts quickly to new populations with minimal data.

Conclusion: AdaCVD presents a flexible, AI-driven tool for CVD risk prediction, suitable for dynamic healthcare environments.

Abstract: Cardiovascular disease (CVD) risk prediction models are essential for
identifying high-risk individuals and guiding preventive actions. However,
existing models struggle with the challenges of real-world clinical practice as
they oversimplify patient profiles, rely on rigid input schemas, and are
sensitive to distribution shifts. We developed AdaCVD, an adaptable CVD risk
prediction framework built on large language models extensively fine-tuned on
over half a million participants from the UK Biobank. In benchmark comparisons,
AdaCVD surpasses established risk scores and standard machine learning
approaches, achieving state-of-the-art performance. Crucially, for the first
time, it addresses key clinical challenges across three dimensions: it flexibly
incorporates comprehensive yet variable patient information; it seamlessly
integrates both structured data and unstructured text; and it rapidly adapts to
new patient populations using minimal additional data. In stratified analyses,
it demonstrates robust performance across demographic, socioeconomic, and
clinical subgroups, including underrepresented cohorts. AdaCVD offers a
promising path toward more flexible, AI-driven clinical decision support tools
suited to the realities of heterogeneous and dynamic healthcare environments.

</details>


### [336] [AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models](https://arxiv.org/abs/2505.24784)
*Conor Heins,Toon Van de Maele,Alexander Tschantz,Hampus Linander,Dimitrije Markovic,Tommaso Salvatori,Corrado Pezzato,Ozan Catal,Ran Wei,Magnus Koudahl,Marco Perin,Karl Friston,Tim Verbelen,Christopher Buckley*

Key words: 深度强化学习, 主动推理, 核心先验, 对象中心模型, 数据效率

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: AXIOM结合了核心先验知识和主动推理，提出了一种在低数据量下高效学习的架构，兼具贝叶斯方法的数据高效性和深度强化学习的跨任务泛化能力。

Motivation: 解决深度强化学习在数据效率上的不足，同时保持其领域灵活性，通过整合核心先验知识加速低数据量下的学习。

Method: 采用对象为中心的场景表示，模型对象动态为分段线性轨迹，捕捉稀疏的对象间交互，通过在线扩展和贝叶斯模型简化实现泛化。

Result: AXIOM在仅10,000次交互步骤内掌握多种游戏，参数少且无需梯度优化。

Conclusion: AXIOM成功结合了贝叶斯方法和深度强化学习的优势，在数据效率和泛化能力上表现突出。

Abstract: Current deep reinforcement learning (DRL) approaches achieve state-of-the-art
performance in various domains, but struggle with data efficiency compared to
human learning, which leverages core priors about objects and their
interactions. Active inference offers a principled framework for integrating
sensory information with prior knowledge to learn a world model and quantify
the uncertainty of its own beliefs and predictions. However, active inference
models are usually crafted for a single task with bespoke knowledge, so they
lack the domain flexibility typical of DRL approaches. To bridge this gap, we
propose a novel architecture that integrates a minimal yet expressive set of
core priors about object-centric dynamics and interactions to accelerate
learning in low-data regimes. The resulting approach, which we call AXIOM,
combines the usual data efficiency and interpretability of Bayesian approaches
with the across-task generalization usually associated with DRL. AXIOM
represents scenes as compositions of objects, whose dynamics are modeled as
piecewise linear trajectories that capture sparse object-object interactions.
The structure of the generative model is expanded online by growing and
learning mixture models from single events and periodically refined through
Bayesian model reduction to induce generalization. AXIOM masters various games
within only 10,000 interaction steps, with both a small number of parameters
compared to DRL, and without the computational expense of gradient-based
optimization.

</details>


### [337] [EXP-Bench: Can AI Conduct AI Research Experiments?](https://arxiv.org/abs/2505.24785)
*Patrick Tser Jern Kon,Jiachen Liu,Xinyi Zhu,Qiuyi Ding,Jingjia Peng,Jiarong Xing,Yibo Huang,Yiming Qiu,Jayanth Srinivasa,Myungjin Lee,Mosharaf Chowdhury,Matei Zaharia,Ang Chen*

Key words: AI研究自动化, 基准测试, 实验设计, 代理评估

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: EXP-Bench是一个评估AI代理在完整AI研究实验中的能力的基准测试，包含461个任务，但当前代理的成功率仅为0.5%。

Motivation: 自动化AI研究有加速科学进步的潜力，但现有AI代理难以处理端到端实验的复杂性。

Method: 通过半自动化流程从顶级AI论文中提取和结构化实验细节，构建了461个任务。

Result: 评估显示，现有代理在单个实验环节得分仅20-35%，完整实验成功率低至0.5%。

Conclusion: EXP-Bench为未来AI代理提升研究实验能力提供了关键工具。

Abstract: Automating AI research holds immense potential for accelerating scientific
progress, yet current AI agents struggle with the complexities of rigorous,
end-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed
to systematically evaluate AI agents on complete research experiments sourced
from influential AI publications. Given a research question and incomplete
starter code, EXP-Bench challenges AI agents to formulate hypotheses, design
and implement experimental procedures, execute them, and analyze results. To
enable the creation of such intricate and authentic tasks with high-fidelity,
we design a semi-autonomous pipeline to extract and structure crucial
experimental details from these research papers and their associated
open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks
from 51 top-tier AI research papers. Evaluations of leading LLM-based agents,
such as OpenHands and IterativeAgent on EXP-Bench demonstrate partial
capabilities: while scores on individual experimental aspects such as design or
implementation correctness occasionally reach 20-35%, the success rate for
complete, executable experiments was a mere 0.5%. By identifying these
bottlenecks and providing realistic step-by-step experiment procedures,
EXP-Bench serves as a vital tool for future AI agents to improve their ability
to conduct AI research experiments. EXP-Bench is open-sourced at
https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.

</details>


### [338] [MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning](https://arxiv.org/abs/2505.24846)
*Jingyan Shen,Jiarui Yao,Rui Yang,Yifan Sun,Feng Luo,Rui Pan,Tong Zhang,Han Zhao*

Key words: reward modeling, RLHF, Bradley-Terry, personalization, MiCRo

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: The paper introduces MiCRo, a two-stage framework for personalized preference learning in reward modeling, addressing the limitations of the Bradley-Terry model by capturing diverse human preferences without needing fine-grained annotations.

Motivation: Current reward modeling methods like Bradley-Terry assume a global reward function, failing to capture diverse human preferences, limiting LLMs' ability to support personalization.

Method: MiCRo uses a two-stage approach: 1) context-aware mixture modeling to capture diverse preferences, and 2) online routing to dynamically adapt mixture weights for efficient preference adaptation.

Result: Experiments show MiCRo effectively captures diverse preferences and improves downstream personalization.

Conclusion: MiCRo enhances personalized preference learning by leveraging large-scale binary preference datasets, offering a scalable solution without costly annotations.

Abstract: Reward modeling is a key step in building safe foundation models when
applying reinforcement learning from human feedback (RLHF) to align Large
Language Models (LLMs). However, reward modeling based on the Bradley-Terry
(BT) model assumes a global reward function, failing to capture the inherently
diverse and heterogeneous human preferences. Hence, such oversimplification
limits LLMs from supporting personalization and pluralistic alignment.
Theoretically, we show that when human preferences follow a mixture
distribution of diverse subgroups, a single BT model has an irreducible error.
While existing solutions, such as multi-objective learning with fine-grained
annotations, help address this issue, they are costly and constrained by
predefined attributes, failing to fully capture the richness of human values.
In this work, we introduce MiCRo, a two-stage framework that enhances
personalized preference learning by leveraging large-scale binary preference
datasets without requiring explicit fine-grained annotations. In the first
stage, MiCRo introduces context-aware mixture modeling approach to capture
diverse human preferences. In the second stage, MiCRo integrates an online
routing strategy that dynamically adapts mixture weights based on specific
context to resolve ambiguity, allowing for efficient and scalable preference
adaptation with minimal additional supervision. Experiments on multiple
preference datasets demonstrate that MiCRo effectively captures diverse human
preferences and significantly improves downstream personalization.

</details>


### [339] [Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents](https://arxiv.org/abs/2505.24878)
*Yaxin Luo,Zhaoyi Li,Jiacheng Liu,Jiacheng Cui,Xiaohan Zhao,Zhiqiang Shen*

Key words: CAPTCHA, multimodal LLM agents, visual reasoning, interactive tasks, benchmark

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文介绍了Open CaptchaWorld，首个专门评估多模态大语言模型代理在动态CAPTCHA任务中视觉推理和交互能力的平台，结果显示当前最先进代理的性能远低于人类水平。

Motivation: CAPTCHAs是网络代理实际应用中的关键瓶颈，现有代理在交互式多步推理任务（如CAPTCHA）上的能力尚未充分测试。

Method: 提出了包含20种现代CAPTCHA类型（共225个）的Open CaptchaWorld平台，并引入新指标CAPTCHA Reasoning Depth量化解决每个谜题所需的认知和运动步骤。

Result: 人类表现接近完美（93.3%），而最先进的多模态代理（如Browser-Use Openai-o3）成功率最高仅为40.0%。

Conclusion: Open CaptchaWorld是诊断当前多模态代理局限性的重要基准，可指导开发更强大的多模态推理系统。

Abstract: CAPTCHAs have been a critical bottleneck for deploying web agents in
real-world applications, often blocking them from completing end-to-end
automation tasks. While modern multimodal LLM agents have demonstrated
impressive performance in static perception tasks, their ability to handle
interactive, multi-step reasoning challenges like CAPTCHAs is largely untested.
To address this gap, we introduce Open CaptchaWorld, the first web-based
benchmark and platform specifically designed to evaluate the visual reasoning
and interaction capabilities of MLLM-powered agents through diverse and dynamic
CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225
CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,
which quantifies the number of cognitive and motor steps required to solve each
puzzle. Experimental results show that humans consistently achieve near-perfect
scores, state-of-the-art MLLM agents struggle significantly, with success rates
at most 40.0% by Browser-Use Openai-o3, far below human-level performance,
93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing
the limits of current multimodal agents and guiding the development of more
robust multimodal reasoning systems. Code and Data are available at this https
URL.

</details>


### [340] [Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing Low-Level Process Event Streams](https://arxiv.org/abs/2505.05880)
*Bettina Fazzinga,Sergio Flesca,Filippo Furfaro,Luigi Pontieri,Francesco Scala*

Key words: 流程跟踪, 神经符号方法, 抽象论证框架, 绿色AI, 序列标记

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种结合神经符号方法的高效数据/计算方案，用于解决业务流程跟踪中事件到活动的映射问题，旨在减少标记数据需求和计算成本。

Motivation: 现代企业需要监控和分析流程跟踪，但在事件与业务活动映射高度不确定时，传统方法可能导致低信息量结果和高计算成本。作者旨在开发一种环保AI方案，降低劳动力和计算资源消耗。

Method: 作者采用了神经符号方法，结合序列标记模型（用于生成候选事件解释）和基于抽象论证框架（AAF）的推理器（用于优化解释），以减少对大量标记数据的依赖。

Result: 实验结果表明，该方法能有效利用先验知识补偿数据稀缺性，特别适用于标注成本和模型优化受限的场景。

Conclusion: 该方案不仅提升了事件解释的准确性，还符合绿色AI的可持续发展目标，适用于资源受限的环境。

Abstract: Monitoring and analyzing process traces is a critical task for modern
companies and organizations. In scenarios where there is a gap between trace
events and reference business activities, this entails an interpretation
problem, amounting to translating each event of any ongoing trace into the
corresponding step of the activity instance. Building on a recent approach that
frames the interpretation problem as an acceptance problem within an Abstract
Argumentation Framework (AAF), one can elegantly analyze plausible event
interpretations (possibly in an aggregated form), as well as offer explanations
for those that conflict with prior process knowledge. Since, in settings where
event-to-activity mapping is highly uncertain (or simply under-specified) this
reasoning-based approach may yield lowly-informative results and heavy
computation, one can think of discovering a sequencetagging model, trained to
suggest highly-probable candidate event interpretations in a context-aware way.
However, training such a model optimally may require using a large amount of
manually-annotated example traces. Considering the urgent need of developing
Green AI solutions enabling environmental and societal sustainability (with
reduced labor/computational costs and carbon footprint), we propose a
data/computation-efficient neuro-symbolic approach to the problem, where the
candidate interpretations returned by the example-driven sequence tagger is
refined by the AAF-based reasoner. This allows us to also leverage prior
knowledge to compensate for the scarcity of example data, as confirmed by
experimental results; clearly, this property is particularly useful in settings
where data annotation and model optimization costs are subject to stringent
constraints.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [341] [Exploring Societal Concerns and Perceptions of AI: A Thematic Analysis through the Lens of Problem-Seeking](https://arxiv.org/abs/2505.23930)
*Naomi Omeonga wa Kayembe*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 本研究提出了区分问题寻找与问题解决的新框架，强调人类智能在情感与体验基础上的独特性，并通过混合方法分析公共讨论主题，发现公众对AI既好奇又担忧。研究驳斥了智能与目标分离的正交论，主张人类智能整合目标设定与追求，并呼吁提升情感与数字素养，推动人本AI发展。

Motivation: 旨在阐明人类智能与AI的本质区别，特别是问题寻找（目标设定）与问题解决（策略执行）的差异，呼吁更重视人类认知的情感与体验基础。

Method: 通过混合方法（主题分析与关键词量化）分析157个YouTube视频的元数据，提取公共讨论中的核心主题。

Result: 发现公众对AI的能力既着迷又担忧，主题包括隐私、失业、错误信息、乐观与伦理问题。

Conclusion: 人类智能的本质在于整合目标设定与追求，AI的局限源于其缺乏情感与体验基础。需通过提升数字素养和重新定义AI角色，推动人本AI发展。

Abstract: This study introduces a novel conceptual framework distinguishing
problem-seeking from problem-solving to clarify the unique features of human
intelligence in contrast to AI. Problem-seeking refers to the embodied,
emotionally grounded process by which humans identify and set goals, while
problem-solving denotes the execution of strategies aimed at achieving such
predefined objectives. The framework emphasizes that while AI excels at
efficiency and optimization, it lacks the orientation derived from experiential
grounding and the embodiment flexibility intrinsic to human cognition. To
empirically explore this distinction, the research analyzes metadata from 157
YouTube videos discussing AI. Conducting a thematic analysis combining
qualitative insights with keyword-based quantitative metrics, this
mixed-methods approach uncovers recurring themes in public discourse, including
privacy, job displacement, misinformation, optimism, and ethical concerns. The
results reveal a dual sentiment: public fascination with AI's capabilities
coexists with anxiety and skepticism about its societal implications. The
discussion critiques the orthogonality thesis, which posits that intelligence
is separable from goal content, and instead argues that human intelligence
integrates goal-setting and goal-pursuit. It underscores the centrality of
embodied cognition in human reasoning and highlights how AI's limitations come
from its current reliance on computational processing. The study advocates for
enhancing emotional and digital literacy to foster responsible AI engagement.
It calls for reframing public discourse to recognize AI as a tool that augments
-- rather than replaces -- human intelligence. By positioning problem seeking
at the core of cognition and as a critical dimension of intelligence, this
research offers new perspectives on ethically aligned and human-centered AI
development.

</details>


### [342] [Evaluating Gemini in an arena for learning](https://arxiv.org/abs/2505.24477)
*LearnLM Team,Abhinit Modi,Aditya Srikanth Veerubhotla,Aliya Rysbek,Andrea Huber,Ankit Anand,Avishkar Bhoopchand,Brett Wiltshire,Daniel Gillick,Daniel Kasenberg,Eleni Sgouritsa,Gal Elidan,Hengrui Liu,Holger Winnemoeller,Irina Jurenka,James Cohan,Jennifer She,Julia Wilkowski,Kaiz Alarakyia,Kevin R. McKee,Komal Singh,Lisa Wang,Markus Kunesch,Miruna Pîslar,Niv Efron,Parsa Mahmoudieh,Pierre-Alexandre Kamienny,Sara Wiltberger,Shakir Mohamed,Shashank Agarwal,Shubham Milind Phal,Sun Jae Lee,Theofilos Strinopoulos,Wei-Jen Ko,Yael Gold-Zamir,Yael Haramaty,Yannis Assael*

Key words: AI教育评估, 学习竞技场, Gemini 2.5 Pro, 盲测, 教学原则

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 论文提出一个‘学习竞技场’框架，用于评估AI模型在教育中的应用，通过教育家与专家盲测，Gemini 2.5 Pro在多数对比中胜出。

Motivation: 缺乏评估AI在教育中应用的通用基准，研究旨在填补这一空白。

Method: 采用‘学习竞技场’方法，189名教育家和206名专家对多款AI模型（如Gemini 2.5 Pro、GPT-4o）进行盲测和多轮对比。

Result: Gemini 2.5 Pro在73.2%的对比中胜出，并在教学原则表现上显著优于其他模型。

Conclusion: Gemini 2.5 Pro是当前最适合教育应用的领先AI模型。

Abstract: Artificial intelligence (AI) is poised to transform education, but the
research community lacks a robust, general benchmark to evaluate AI models for
learning. To assess state-of-the-art support for educational use cases, we ran
an "arena for learning" where educators and pedagogy experts conduct blind,
head-to-head, multi-turn comparisons of leading AI models. In particular, $N =
189$ educators drew from their experience to role-play realistic learning use
cases, interacting with two models sequentially, after which $N = 206$ experts
judged which model better supported the user's learning goals. The arena
evaluated a slate of state-of-the-art models: Gemini 2.5 Pro, Claude 3.7
Sonnet, GPT-4o, and OpenAI o3. Excluding ties, experts preferred Gemini 2.5 Pro
in 73.2% of these match-ups -- ranking it first overall in the arena. Gemini
2.5 Pro also demonstrated markedly higher performance across key principles of
good pedagogy. Altogether, these results position Gemini 2.5 Pro as a leading
model for learning.

</details>


### [343] [Generative Knowledge Production Pipeline Driven by Academic Influencers](https://arxiv.org/abs/2505.24681)
*Katalin Feher,Marton Demeter*

Key words: 生成式AI, 学术影响者, 知识生产, 伦理合规, 人机协作

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 该研究探讨了生成式AI如何通过53个学术影响者视频影响知识生产，提出了一个平衡原创性与伦理的出版流程框架。

Motivation: 生成式AI对知识生产、验证和传播的变革引发了学术诚信和可信度问题。研究旨在探索如何平衡AI与人类合作，同时应对其颠覆性影响。

Method: 分析53个学术影响者视频，覆盖530万观众，识别出结构化的出版流程。

Result: 发现学术影响者是关键中介，提出生成式出版生产流程和政策框架，支持自动化实践和学术创新。

Conclusion: 研究为学者和决策者提供了理解AI影响的途径，倡导负责任的知识生产。

Abstract: Generative AI transforms knowledge production, validation, and dissemination,
raising academic integrity and credibility concerns. This study examines 53
academic influencer videos that reached 5.3 million viewers to identify an
emerging, structured, implementation-ready pipeline balancing originality,
ethical compliance, and human-AI collaboration despite the disruptive impacts.
Findings highlight generative AI's potential to automate publication workflows
and democratize participation in knowledge production while challenging
traditional scientific norms. Academic influencers emerge as key intermediaries
in this paradigm shift, connecting bottom-up practices with institutional
policies to improve adaptability. Accordingly, the study proposes a generative
publication production pipeline and a policy framework for co-intelligence
adaptation and reinforcing credibility-centered standards in AI-powered
research. These insights support scholars, educators, and policymakers in
understanding AI's transformative impact by advocating responsible and
innovation-driven knowledge production. Additionally, they reveal pathways for
automating best practices, optimizing scholarly workflows, and fostering
creativity in academic research and publication.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [344] [When Humans Growl and Birds Speak: High-Fidelity Voice Conversion from Human to Animal and Designed Sounds](https://arxiv.org/abs/2505.24336)
*Minsu Kang,Seolhee Lee,Choonghyeon Lee,Namhyun Cho*

Key words: 语音转换，非人类声音，CVAE，44.1kHz，预处理

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 该论文提出了一种改进的CVAE模型和预处理流程，用于将人类语音转换为多样化的非人类声音（如动物吼叫、鸟鸣等），支持44.1kHz高质量音频转换，实验结果表明其效果优于基线方法。

Motivation: 之前的研究主要集中在狗叫声和16或22.05kHz音频转换上，而本文旨在扩展非语音声音的转换范围，包括自然声音（如狮子吼叫、鸟鸣）和设计声音（如合成咆哮），并支持更高品质的44.1kHz音频转换。

Method: 引入了一个优化的预处理流程和一个改进的基于CVAE的H2NH-VC模型，专门针对人类和非人类声音进行了优化。

Result: 实验结果显示，所提出的方法在质量、自然度和相似性MOS评分上均优于基线方法，实现了跨多种非人类音色的有效语音转换。

Conclusion: 该方法成功地扩展了非人类语音转换的范围，并在高质量音频转换中表现优异。

Abstract: Human to non-human voice conversion (H2NH-VC) transforms human speech into
animal or designed vocalizations. Unlike prior studies focused on dog-sounds
and 16 or 22.05kHz audio transformation, this work addresses a broader range of
non-speech sounds, including natural sounds (lion-roars, birdsongs) and
designed voice (synthetic growls). To accomodate generation of diverse
non-speech sounds and 44.1kHz high-quality audio transformation, we introduce a
preprocessing pipeline and an improved CVAE-based H2NH-VC model, both optimized
for human and non-human voices. Experimental results showed that the proposed
method outperformed baselines in quality, naturalness, and similarity MOS,
achieving effective voice conversion across diverse non-human timbres. Demo
samples are available at
https://nc-ai.github.io/speech/publications/nonhuman-vc/

</details>


### [345] [Can Emotion Fool Anti-spoofing?](https://arxiv.org/abs/2505.23962)
*Aurosweta Mahapatra,Ismail Rasim Ulgen,Abinay Reddy Naini,Carlos Busso,Berrak Sisman*

Key words: 反欺骗、情感语音合成、EmoSpoof-TTS、GEM模型

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 研究提出EmoSpoof-TTS语料库，指出传统反欺骗系统在情感合成语音上表现不佳，并提出了GEM模型以提升多情感场景下的防御效果。

Motivation: 传统反欺骗系统主要针对中性语音，忽视情感多样性，导致对高质量情感合成语音的鲁棒性不足。

Method: 提出EmoSpoof-TTS语料库，并设计GEM模型——基于语音情感识别门控网络的多专家集成模型。

Result: 现有模型在情感语音上表现不佳，GEM在所有情感（包括中性）上表现更优，提升了反欺骗防御能力。

Conclusion: 需在数据集和方法论上重视情感多样性，GEM为情感导向的反欺骗提供了有效解决方案。

Abstract: Traditional anti-spoofing focuses on models and datasets built on synthetic
speech with mostly neutral state, neglecting diverse emotional variations. As a
result, their robustness against high-quality, emotionally expressive synthetic
speech is uncertain. We address this by introducing EmoSpoof-TTS, a corpus of
emotional text-to-speech samples. Our analysis shows existing anti-spoofing
models struggle with emotional synthetic speech, exposing risks of
emotion-targeted attacks. Even trained on emotional data, the models
underperform due to limited focus on emotional aspect and show performance
disparities across emotions. This highlights the need for emotion-focused
anti-spoofing paradigm in both dataset and methodology. We propose GEM, a gated
ensemble of emotion-specialized models with a speech emotion recognition gating
network. GEM performs effectively across all emotions and neutral state,
improving defenses against spoofing attacks. We release the EmoSpoof-TTS
Dataset: https://emospoof-tts.github.io/Dataset/

</details>


### [346] [Identifying Primary Stress Across Related Languages and Dialects with Transformer-based Speech Encoder Models](https://arxiv.org/abs/2505.24571)
*Nikola Ljubešić,Ivan Porupski,Peter Rupnik*

Key words: 主重音识别，变压器模型，多语言，克罗地亚语，低资源训练

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 论文通过微调预训练变压器模型，结合新的克罗地亚数据集，证明该方法在多语言主重音识别任务中优于传统SVM分类器，尤其在克罗地亚语和塞尔维亚语上表现接近完美。

Motivation: 因重音在语义编码和语音理解中的作用，主重音自动化识别一直是研究热点，但以往研究多依赖传统声学特征和英语数据集，本文探索了在多语言（克罗地亚语等）中应用预训练模型的潜力。

Method: 采用微调的预训练变压器模型，配备音频帧分类头，对比传统SVM（基于声学特征），实验数据集涵盖克罗地亚语、塞尔维亚语、查方言和斯洛文尼亚语。

Result: 变压器模型全面优于SVM，克罗地亚语和塞尔维亚语识别准确率接近完美，查方言和斯洛文尼亚语性能下降约10分；仅需数百个多音节训练词即可实现高表现。

Conclusion: 微调预训练模型在多语言主重音识别中高效且通用，低数据需求和小语种适应性凸显其潜力；数据集和模型已开源。

Abstract: Automating primary stress identification has been an active research field
due to the role of stress in encoding meaning and aiding speech comprehension.
Previous studies relied mainly on traditional acoustic features and English
datasets. In this paper, we investigate the approach of fine-tuning a
pre-trained transformer model with an audio frame classification head. Our
experiments use a new Croatian training dataset, with test sets in Croatian,
Serbian, the Chakavian dialect, and Slovenian. By comparing an SVM classifier
using traditional acoustic features with the fine-tuned speech transformer, we
demonstrate the transformer's superiority across the board, achieving
near-perfect results for Croatian and Serbian, with a 10-point performance drop
for the more distant Chakavian and Slovenian. Finally, we show that only a few
hundred multi-syllabic training words suffice for strong performance. We
release our datasets and model under permissive licenses.

</details>


### [347] ["Dyadosyncrasy", Idiosyncrasy and Demographic Factors in Turn-Taking](https://arxiv.org/abs/2505.24736)
*Julio Cesar Cavalcanti,Gabriel Skantze*

Key words: 转换时间, 人口统计, 个体差异, 双方互动, 对话分析

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 研究表明，对话中的转换时间受个体和会话关系影响大于人口统计因素。女性、年长者话题转换更快，但个体差异和双方互动影响更强。

Motivation: 探究人口统计（性别、年龄、教育）和个体因素如何影响对话中的转换时间（TFO），揭示核心驱动力。

Method: 通过分析大规模英语会话数据集（Fisher），测量转换地板偏移（TFO），统计不同变量的影响。

Result: 性别和年龄对TFO影响较小但显著（女性、年长者偏移更短），教育无影响。个体差异和双方互动影响最大。

Conclusion: 对话双方关系和共同活动是TFO的主要决定因素，超越人口统计影响。

Abstract: Turn-taking in dialogue follows universal constraints but also varies
significantly. This study examines how demographic (sex, age, education) and
individual factors shape turn-taking using a large dataset of US English
conversations (Fisher). We analyze Transition Floor Offset (TFO) and find
notable interspeaker variation. Sex and age have small but significant effects
female speakers and older individuals exhibit slightly shorter offsets - while
education shows no effect. Lighter topics correlate with shorter TFOs. However,
individual differences have a greater impact, driven by a strong idiosyncratic
and an even stronger "dyadosyncratic" component - speakers in a dyad resemble
each other more than they resemble themselves in different dyads. This suggests
that the dyadic relationship and joint activity are the strongest determinants
of TFO, outweighing demographic influences.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [348] [Proxy Target: Bridging the Gap Between Discrete Spiking Neural Networks and Continuous Control](https://arxiv.org/abs/2505.24161)
*Zijie Xu,Tong Bu,Zecheng Hao,Jianhao Ding,Zhaofei Yu*

Key words: 脉冲神经网络, 强化学习, 连续控制, 代理目标框架, 能效

<details>
  <summary>Details</summary>

Main category: cs.NE

TL;DR: 该论文提出了一种代理目标框架，解决了脉冲神经网络（SNN）在连续控制任务中的训练不稳定问题，提升了性能并保持了能效。

Motivation: 当前强化学习（RL）算法多针对人工神经网络（ANN）设计，与SNN的离散、不可微特性不兼容，导致训练不稳定。

Method: 提出代理目标框架，利用连续可微的代理网络在训练阶段平滑更新，避免SNN脉冲的不兼容性。

Result: 在连续控制基准测试中，SNN性能提升最高达32%，首次使用简单LIF神经元超越ANN性能。

Conclusion: 该方法为SNN友好的RL算法开辟了新方向，实现了高性能与低功耗的结合。

Abstract: Spiking Neural Networks (SNNs) offer low-latency and energy-efficient
decision making through neuromorphic hardware, making them compelling for
Reinforcement Learning (RL) in resource-constrained edge devices. Recent
studies in this field directly replace Artificial Neural Networks (ANNs) by
SNNs in existing RL frameworks, overlooking whether the RL algorithm is
suitable for SNNs. However, most RL algorithms in continuous control are
designed tailored to ANNs, including the target network soft updates mechanism,
which conflict with the discrete, non-differentiable dynamics of SNN spikes. We
identify that this mismatch destabilizes SNN training in continuous control
tasks. To bridge this gap between discrete SNN and continuous control, we
propose a novel proxy target framework. The continuous and differentiable
dynamics of the proxy target enable smooth updates, bypassing the
incompatibility of SNN spikes, stabilizing the RL algorithms. Since the proxy
network operates only during training, the SNN retains its energy efficiency
during deployment without inference overhead. Extensive experiments on
continuous control benchmarks demonstrate that compared to vanilla SNNs, the
proxy target framework enables SNNs to achieve up to 32% higher performance
across different spiking neurons. Notably, we are the first to surpass ANN
performance in continuous control with simple Leaky-Integrate-and-Fire (LIF)
neurons. This work motivates a new class of SNN-friendly RL algorithms tailored
to SNN's characteristics, paving the way for neuromorphic agents that combine
high performance with low power consumption.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [349] [Bounds on the Excess Minimum Risk via Generalized Information Divergence Measures](https://arxiv.org/abs/2505.24117)
*Ananya Omanwar,Fady Alajaji,Tamás Linder*

Key words: 超额最小风险，广义信息散度，马尔可夫链，Rényi散度，$α$-Jensen-Shannon散度

<details>
  <summary>Details</summary>

Main category: cs.IT

TL;DR: 该论文研究了在马尔可夫链$Y 	o X 	o Z$下，利用广义信息散度度量推导了超额最小风险的上界，推广了互信息为基础的界限，并展示了数值例子证明其紧致性。

Motivation: 研究动机在于扩展现有的超额最小风险界限，使其适用于更广泛的联合分布类别，而不仅限于子高斯参数恒定的情况。

Method: 方法包括使用Rényi和$α$-Jensen-Shannon散度以及Sibson互信息，推导广义信息散度度量为基础的上界。

Result: 结果表明，广义散度为基础的界限在某些$α$参数范围内比基于互信息的界限更紧致。

Conclusion: 结论是提出的广义界限不仅适用于更广泛的分布类别，且在特定情况下能提供更紧致的估计。

Abstract: Given finite-dimensional random vectors $Y$, $X$, and $Z$ that form a Markov
chain in that order (i.e., $Y \to X \to Z$), we derive upper bounds on the
excess minimum risk using generalized information divergence measures. Here,
$Y$ is a target vector to be estimated from an observed feature vector $X$ or
its stochastically degraded version $Z$. The excess minimum risk is defined as
the difference between the minimum expected loss in estimating $Y$ from $X$ and
from $Z$. We present a family of bounds that generalize the mutual information
based bound of Gy\"orfi et al. (2023), using the R\'enyi and
$\alpha$-Jensen-Shannon divergences, as well as Sibson's mutual information.
Our bounds are similar to those developed by Modak et al. (2021) and Aminian et
al. (2024) for the generalization error of learning algorithms. However, unlike
these works, our bounds do not require the sub-Gaussian parameter to be
constant and therefore apply to a broader class of joint distributions over
$Y$, $X$, and $Z$. We also provide numerical examples under both constant and
non-constant sub-Gaussianity assumptions, illustrating that our generalized
divergence based bounds can be tighter than the one based on mutual information
for certain regimes of the parameter $\alpha$.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [350] [An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring](https://arxiv.org/abs/2505.24239)
*Sana Ebrahimi,Mohsen Dehghankar,Abolfazl Asudeh*

Key words: 多代理LLM、对抗性、信誉评分、协作、鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 提出了一种基于信誉评分的抗对抗多方代理LLM框架，通过信誉评分来抵抗对抗性和低效代理的干扰。

Motivation: 多代理LLM系统虽强大，但对对抗性和低效代理高度敏感，因此需要提升其抗干扰能力。

Method: 将协作查询回答模拟为迭代游戏，通过信誉评分系统逐步学习并聚合代理输出。

Result: 实验表明，该系统能有效减轻对抗影响，提升多方代理合作的鲁棒性，即使在对抗性多数情况下。

Conclusion: 提出的框架能显著增强多方代理系统的抗对抗能力，提升整体协作效率。

Abstract: While multi-agent LLM systems show strong capabilities in various domains,
they are highly vulnerable to adversarial and low-performing agents. To resolve
this issue, in this paper, we introduce a general and adversary-resistant
multi-agent LLM framework based on credibility scoring. We model the
collaborative query-answering process as an iterative game, where the agents
communicate and contribute to a final system output. Our system associates a
credibility score that is used when aggregating the team outputs. The
credibility scores are learned gradually based on the past contributions of
each agent in query answering. Our experiments across multiple tasks and
settings demonstrate our system's effectiveness in mitigating adversarial
influence and enhancing the resilience of multi-agent cooperation, even in the
adversary-majority settings.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [351] [Sparsity-Driven Parallel Imaging Consistency for Improved Self-Supervised MRI Reconstruction](https://arxiv.org/abs/2505.24136)
*Yaşar Utku Alçalar,Mehmet Akçakaya*

Key words: 物理驱动深度学习, MRI重建, 自监督学习, k-space掩蔽, 一致性项

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 本文提出了一种通过精心设计的扰动训练物理驱动的深度学习（PD-DL）模型的新方法，以减少高加速率下自监督学习中的伪影。该方法在k-space掩蔽基础上引入了新的一致性项，显著提升了图像重建质量。

Motivation: 在高加速率MRI扫描中，自监督学习常导致伪影，影响图像质量。为了在没有全采样参考数据的情况下仍然获得高质量重建，需要改进训练策略。

Method: 通过在k-space掩蔽中引入一致性项，评估模型对稀疏域中扰动的预测能力，从而提升重建的可靠性和减少伪影。

Result: 在fastMRI膝部和脑部数据集上的实验表明，该方法有效减少了混叠伪影和噪声放大，视觉和定量评价均优于现有自监督方法。

Conclusion: 提出的训练策略显著提升了高加速率MRI重建的质量，尤其在减少伪影方面表现优异。

Abstract: Physics-driven deep learning (PD-DL) models have proven to be a powerful
approach for improved reconstruction of rapid MRI scans. In order to train
these models in scenarios where fully-sampled reference data is unavailable,
self-supervised learning has gained prominence. However, its application at
high acceleration rates frequently introduces artifacts, compromising image
fidelity. To mitigate this shortcoming, we propose a novel way to train PD-DL
networks via carefully-designed perturbations. In particular, we enhance the
k-space masking idea of conventional self-supervised learning with a novel
consistency term that assesses the model's ability to accurately predict the
added perturbations in a sparse domain, leading to more reliable and
artifact-free reconstructions. The results obtained from the fastMRI knee and
brain datasets show that the proposed training strategy effectively reduces
aliasing artifacts and mitigates noise amplification at high acceleration
rates, outperforming state-of-the-art self-supervised methods both visually and
quantitatively.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [352] [Quantum computing and artificial intelligence: status and perspectives](https://arxiv.org/abs/2505.23860)
*Giovanni Acampora,Andris Ambainis,Natalia Ares,Leonardo Banchi,Pallavi Bhardwaj,Daniele Binosi,G. Andrew D. Briggs,Tommaso Calarco,Vedran Dunjko,Jens Eisert,Olivier Ezratty,Paul Erker,Federico Fedele,Elies Gil-Fuster,Martin Gärttner,Mats Granath,Markus Heyl,Iordanis Kerenidis,Matthias Klusch,Anton Frisk Kockum,Richard Kueng,Mario Krenn,Jörg Lässig,Antonio Macaluso,Sabrina Maniscalco,Florian Marquardt,Kristel Michielsen,Gorka Muñoz-Gil,Daniel Müssig,Hendrik Poulsen Nautrup,Evert van Nieuwenburg,Roman Orus,Jörg Schmiedmayer,Markus Schmitt,Philipp Slusallek,Filippo Vicentini,Christof Weitenberg,Frank K. Wilhelm*

Key words: 量子计算, 人工智能, 研究议程, 资源优化, 工业竞争力

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 这篇白皮书探讨了量子计算与人工智能（AI）的交集，分析了量子计算如何促进AI创新，以及经典AI如何支持量子技术研发，提出了一份长期研究议程。

Motivation: 旨在解决AI与量子计算如何相互作用和互利的基础性问题，并推动这一新兴领域的发展。

Method: 通过分析量子计算与AI的交集，提出研究议程，涵盖理论工作协调、资源估算、软件开发等。

Result: 提出了一系列建议和挑战，包括理论工作协调、硬件路线图对齐、资源优化等。

Conclusion: 白皮书为量子计算与AI的融合发展提供了研究方向和实际挑战，旨在增强欧洲工业竞争力并考虑社会影响。

Abstract: This white paper discusses and explores the various points of intersection
between quantum computing and artificial intelligence (AI). It describes how
quantum computing could support the development of innovative AI solutions. It
also examines use cases of classical AI that can empower research and
development in quantum technologies, with a focus on quantum computing and
quantum sensing. The purpose of this white paper is to provide a long-term
research agenda aimed at addressing foundational questions about how AI and
quantum computing interact and benefit one another. It concludes with a set of
recommendations and challenges, including how to orchestrate the proposed
theoretical work, align quantum AI developments with quantum hardware roadmaps,
estimate both classical and quantum resources - especially with the goal of
mitigating and optimizing energy consumption - advance this emerging hybrid
software engineering discipline, and enhance European industrial
competitiveness while considering societal implications.

</details>


### [353] [Leveraging machine learning features for linear optical interferometer control](https://arxiv.org/abs/2505.24032)
*Sergei S. Kuzmin,Ivan V. Dyakonov,Stanislav S. Straupe*

Key words: 可重构光学干涉仪,酉变换,监督学习,优化程序

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 开发了一种独立于特定架构约束的可重构光学干涉仪建模算法，通过监督学习或优化程序确定相位偏移以实现所需的酉变换。

Motivation: 为了在没有精确解析解的情况下有效调谐干涉仪，并探索新的干涉仪电路架构。

Method: 采用监督学习对齐干涉仪模型与训练集，通过优化程序确定相位偏移。

Result: 算法能够有效调谐干涉仪，无需精确解析解。

Conclusion: 该方法为探索新干涉仪架构提供了可能。

Abstract: We have developed an algorithm that constructs a model of a reconfigurable
optical interferometer, independent of specific architectural constraints. The
programming of unitary transformations on the interferometer's optical modes
relies on either an analytical method for deriving the unitary matrix from a
set of phase shifts or an optimization routine when such decomposition is not
available. Our algorithm employs a supervised learning approach, aligning the
interferometer model with a training set derived from the device being studied.
A straightforward optimization procedure leverages this trained model to
determine the phase shifts of the interferometer with a specific architecture,
obtaining the required unitary transformation. This approach enables the
effective tuning of interferometers without requiring a precise analytical
solution, paving the way for the exploration of new interferometric circuit
architectures.

</details>


### [354] [Supervised Quantum Machine Learning: A Future Outlook from Qubits to Enterprise Applications](https://arxiv.org/abs/2505.24765)
*Srikanth Thudumu,Jason Fisher,Hung Du*

Key words: 量子机器学习，监督学习，量子计算，变分量子电路，量子神经网络

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 该论文综述了监督量子机器学习（QML）的最新进展，重点介绍了变分量子电路、量子神经网络和量子核方法等方法，并探讨了当前研究中的局限性与未来发展。

Motivation: 研究量子计算与经典机器学习的交叉领域，探讨如何利用量子资源支持模型训练与推理。

Method: 回顾了监督QML的方法，包括变分量子电路、量子神经网络和量子核方法，并分析了混合量子-经典工作流程。

Result: 指出了当前研究的局限性（如噪声、贫瘠高原和可扩展性问题），并提出了2025-2035年的发展路线图。

Conclusion: 提出了监督QML在未来十年的潜在发展方向及其应用条件。

Abstract: Supervised Quantum Machine Learning (QML) represents an intersection of
quantum computing and classical machine learning, aiming to use quantum
resources to support model training and inference. This paper reviews recent
developments in supervised QML, focusing on methods such as variational quantum
circuits, quantum neural networks, and quantum kernel methods, along with
hybrid quantum-classical workflows. We examine recent experimental studies that
show partial indications of quantum advantage and describe current limitations
including noise, barren plateaus, scalability issues, and the lack of formal
proofs of performance improvement over classical methods. The main contribution
is a ten-year outlook (2025-2035) that outlines possible developments in
supervised QML, including a roadmap describing conditions under which QML may
be used in applied research and enterprise systems over the next decade.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [355] [Towards Natural Language Communication for Cooperative Autonomous Driving via Self-Play](https://arxiv.org/abs/2505.18334)
*Jiaxun Cui,Chen Tang,Jarrett Holtz,Janice Nguyen,Alessandro G. Allievi,Hang Qiu,Peter Stone*

Key words: 自动驾驶,自然语言处理,车与车通信,多智能体系统,LLM

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 论文提出了一种名为LLM+Debrief的方法，利用自然语言实现车与车之间的通信，以提高自动驾驶的安全性和协调性，并通过实验验证了其优于零样本LLM代理。

Motivation: 以往自动驾驶车辆间的通信缺乏人类可理解性，而使用自然语言作为通信协议可以使自动驾驶车辆不仅彼此协作，还能与人类驾驶员协作，从而提升交通安全和效率。

Method: 提出LLM+Debrief方法，通过多智能体讨论学习生成消息和高层决策策略，并开发了一个类gym的模拟环境来评估LLM代理在驾驶中的表现。

Result: 实验结果表明，LLM+Debrief生成的合作性自然语言消息比零样本LLM代理更具意义和人类可理解性，能更有效地促进协调。

Conclusion: LLM+Debrief方法在自动驾驶的自然语言通信中表现优异，为未来车与车及人车协作提供了可行方案。

Abstract: Past work has demonstrated that autonomous vehicles can drive more safely if
they communicate with one another than if they do not. However, their
communication has often not been human-understandable. Using natural language
as a vehicle-to-vehicle (V2V) communication protocol offers the potential for
autonomous vehicles to drive cooperatively not only with each other but also
with human drivers. In this work, we propose a suite of traffic tasks in
autonomous driving where vehicles in a traffic scenario need to communicate in
natural language to facilitate coordination in order to avoid an imminent
collision and/or support efficient traffic flow. To this end, this paper
introduces a novel method, LLM+Debrief, to learn a message generation and
high-level decision-making policy for autonomous vehicles through multi-agent
discussion. To evaluate LLM agents for driving, we developed a gym-like
simulation environment that contains a range of driving scenarios. Our
experimental results demonstrate that LLM+Debrief is more effective at
generating meaningful and human-understandable natural language messages to
facilitate cooperation and coordination than a zero-shot LLM agent. Our code
and demo videos are available at https://talking-vehicles.github.io/.

</details>


### [356] [DiG-Net: Enhancing Quality of Life through Hyper-Range Dynamic Gesture Recognition in Assistive Robotics](https://arxiv.org/abs/2505.24786)
*Eran Bamani Beeri,Eden Nissinman,Avishai Sintov*

Key words: assistive robotics, dynamic gesture recognition, human-robot interaction, long-distance interaction, DiG-Net

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: DiG-Net enables long-distance dynamic gesture recognition (up to 30 meters) for assistive robotics, achieving 97.3% accuracy through innovative depth-aware modules and a novel loss function.

Motivation: Enhance assistive human-robot interaction by overcoming the limitations of current short-range gesture recognition methods, especially for users with mobility constraints or remote operation needs.

Method: Combines Depth-Conditioned Deformable Alignment blocks with Spatio-Temporal Graph modules, and introduces the Radiometric Spatio-Temporal Depth Attenuation Loss to improve model robustness.

Result: Achieves 97.3% recognition accuracy on challenging hyper-range gestures, outperforming existing frameworks.

Conclusion: DiG-Net significantly improves the usability of assistive robots in home healthcare, industrial safety, and remote assistance by enabling intuitive, long-distance gesture interaction.

Abstract: Dynamic hand gestures play a pivotal role in assistive human-robot
interaction (HRI), facilitating intuitive, non-verbal communication,
particularly for individuals with mobility constraints or those operating
robots remotely. Current gesture recognition methods are mostly limited to
short-range interactions, reducing their utility in scenarios demanding robust
assistive communication from afar. In this paper, we introduce a novel approach
designed specifically for assistive robotics, enabling dynamic gesture
recognition at extended distances of up to 30 meters, thereby significantly
improving accessibility and quality of life. Our proposed Distance-aware
Gesture Network (DiG-Net) effectively combines Depth-Conditioned Deformable
Alignment (DADA) blocks with Spatio-Temporal Graph modules, enabling robust
processing and classification of gesture sequences captured under challenging
conditions, including significant physical attenuation, reduced resolution, and
dynamic gesture variations commonly experienced in real-world assistive
environments. We further introduce the Radiometric Spatio-Temporal Depth
Attenuation Loss (RSTDAL), shown to enhance learning and strengthen model
robustness across varying distances. Our model demonstrates significant
performance improvement over state-of-the-art gesture recognition frameworks,
achieving a recognition accuracy of 97.3% on a diverse dataset with challenging
hyper-range gestures. By effectively interpreting gestures from considerable
distances, DiG-Net significantly enhances the usability of assistive robots in
home healthcare, industrial safety, and remote assistance scenarios, enabling
seamless and intuitive interactions for users regardless of physical
limitations

</details>


### [357] [RealDrive: Retrieval-Augmented Driving with Diffusion Models](https://arxiv.org/abs/2505.24808)
*Wenhao Ding,Sushant Veer,Yuxiao Chen,Yulong Cao,Chaowei Xiao,Marco Pavone*

Key words: 学习型规划器、检索增强生成、扩散模型、轨迹规划、Waymo数据集

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: RealDrive采用RAG框架，结合检索增强生成和扩散模型，改进了学习型规划器的行为自然性和安全性，减少了40%的碰撞率。

Motivation: 解决学习型规划器在罕见或安全关键场景中的性能不足和轨迹可控性差的问题。

Method: 提出了RealDrive框架，利用检索增强生成（RAG）从训练数据中检索相关专家演示，并通过去噪过程生成轨迹。

Result: 在Waymo开放运动数据集上，碰撞率减少了40%，且轨迹多样性提高。

Conclusion: RealDrive通过任务相关的检索模型和RAG框架，显著提升了规划器的性能和安全性。

Abstract: Learning-based planners generate natural human-like driving behaviors by
learning to reason about nuanced interactions from data, overcoming the rigid
behaviors that arise from rule-based planners. Nonetheless, data-driven
approaches often struggle with rare, safety-critical scenarios and offer
limited controllability over the generated trajectories. To address these
challenges, we propose RealDrive, a Retrieval-Augmented Generation (RAG)
framework that initializes a diffusion-based planning policy by retrieving the
most relevant expert demonstrations from the training dataset. By interpolating
between current observations and retrieved examples through a denoising
process, our approach enables fine-grained control and safe behavior across
diverse scenarios, leveraging the strong prior provided by the retrieved
scenario. Another key insight we produce is that a task-relevant retrieval
model trained with planning-based objectives results in superior planning
performance in our framework compared to a task-agnostic retriever.
Experimental results demonstrate improved generalization to long-tail events
and enhanced trajectory diversity compared to standard learning-based planners
-- we observe a 40% reduction in collision rate on the Waymo Open Motion
dataset with RAG.

</details>


### [358] [DexMachina: Functional Retargeting for Bimanual Dexterous Manipulation](https://arxiv.org/abs/2505.24853)
*Zhao Mandi,Yifan Hou,Dieter Fox,Yashraj Narang,Ajay Mandlekar,Shuran Song*

Key words: functional retargeting, dexterous manipulation, bimanual tasks, articulated objects, curriculum learning

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: DexMachina提出了一种基于课程学习的算法，用于从人类手-物体演示中学习灵巧操作策略，以跟踪物体状态，尤其在长时程、双手操作和关节物体任务中表现优异。

Motivation: 解决灵巧操作策略学习中面临的挑战，如大动作空间、时空不连续性和人机手部形态差异，尤其是在长时程、双手操作和关节物体任务中。

Method: 提出DexMachina算法，通过使用虚拟物体控制器（逐渐减弱强度）引导策略学习，逐步接管物体运动与接触控制。

Result: DexMachina在多样化的任务和灵巧手场景中显著优于基线方法，并提供了硬件设计的定量与定性分析。

Conclusion: 该算法和基准测试为硬件能力评估提供了功能对比平台，降低了未来研究的入门门槛。

Abstract: We study the problem of functional retargeting: learning dexterous
manipulation policies to track object states from human hand-object
demonstrations. We focus on long-horizon, bimanual tasks with articulated
objects, which is challenging due to large action space, spatiotemporal
discontinuities, and embodiment gap between human and robot hands. We propose
DexMachina, a novel curriculum-based algorithm: the key idea is to use virtual
object controllers with decaying strength: an object is first driven
automatically towards its target states, such that the policy can gradually
learn to take over under motion and contact guidance. We release a simulation
benchmark with a diverse set of tasks and dexterous hands, and show that
DexMachina significantly outperforms baseline methods. Our algorithm and
benchmark enable a functional comparison for hardware designs, and we present
key findings informed by quantitative and qualitative results. With the recent
surge in dexterous hand development, we hope this work will provide a useful
platform for identifying desirable hardware capabilities and lower the barrier
for contributing to future research. Videos and more at
https://project-dexmachina.github.io/

</details>


### [359] [Bi-Manual Joint Camera Calibration and Scene Representation](https://arxiv.org/abs/2505.24819)
*Haozhan Tang,Tianyi Zhang,Matthew Johnson-Roberson,Weiming Zhi*

Key words: 双手机器人, 相机标定, 3D表示, 无标记标定, 稠密对应

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: Bi-JCR框架无需标定标记，通过RGB图像集联合估计相机外参、机械臂相对位姿及共享工作空间的3D表示，支持双手机器人任务。

Motivation: 传统相机标定过程繁琐，需多次拍摄标定标记；Bi-JCR旨在绕开这一过程，直接通过多视角RGB图像实现标定与环境建模。

Method: 利用3D基础模型进行无标记稠密多视角匹配，联合估计相机外参、机械臂相对位姿及统一尺度一致的3D工作空间表示。

Result: 在多种桌面环境中验证了Bi-JCR的鲁棒性，并展示了其在下游任务中的适用性。

Conclusion: Bi-JCR提供了一种高效、无标记的标定与建模方法，显著简化了双手机器人系统的部署流程。

Abstract: Robot manipulation, especially bimanual manipulation, often requires setting
up multiple cameras on multiple robot manipulators. Before robot manipulators
can generate motion or even build representations of their environments, the
cameras rigidly mounted to the robot need to be calibrated. Camera calibration
is a cumbersome process involving collecting a set of images, with each
capturing a pre-determined marker. In this work, we introduce the Bi-Manual
Joint Calibration and Representation Framework (Bi-JCR). Bi-JCR enables
multiple robot manipulators, each with cameras mounted, to circumvent taking
images of calibration markers. By leveraging 3D foundation models for dense,
marker-free multi-view correspondence, Bi-JCR jointly estimates: (i) the
extrinsic transformation from each camera to its end-effector, (ii) the
inter-arm relative poses between manipulators, and (iii) a unified,
scale-consistent 3D representation of the shared workspace, all from the same
captured RGB image sets. The representation, jointly constructed from images
captured by cameras on both manipulators, lives in a common coordinate frame
and supports collision checking and semantic segmentation to facilitate
downstream bimanual coordination tasks. We empirically evaluate the robustness
of Bi-JCR on a variety of tabletop environments, and demonstrate its
applicability on a variety of downstream tasks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [360] [Enhancing LLM-Based Code Generation with Complexity Metrics: A Feedback-Driven Approach](https://arxiv.org/abs/2505.23953)
*Melika Sepidband,Hamed Taherkhani,Song Wang,Hadi Hemmati*

Key words: 代码生成, 大语言模型, 复杂度指标, 迭代反馈, 代码正确性

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 本研究探讨了代码复杂度与LLM生成代码成功率的关系，提出了一种基于复杂度指标的迭代反馈方法，显著提高了生成代码的正确率。实验验证了该方法优于零样本生成和基于执行的迭代反馈基线方法。

Motivation: 尽管LLM在代码生成方面取得了进展，但很少有研究关注生成代码的特性及其与正确性的关系。作者旨在通过分析代码复杂度，改进LLM的失败案例。

Method: 首先通过经验分析探究复杂度指标与代码生成性能的关系，使用逻辑回归识别关键指标。随后，提出基于复杂度指标的迭代反馈方法，引导LLM生成更正确的代码。

Result: 实验表明，该方法显著提升了代码生成正确率，特别是在较小LLM（如GPT-3.5 Turbo）上，Pass@1提升了35.71%。在BigCodeBench等数据集和不同LLM上也验证了其有效性。

Conclusion: 复杂度感知反馈不仅提升了直接LLM生成代码的效果，还优化了基于代理的工作流程，为代码生成领域提供了新的改进思路。

Abstract: Automatic code generation has gained significant momentum with the advent of
Large Language Models (LLMs) such as GPT-4. Although many studies focus on
improving the effectiveness of LLMs for code generation, very limited work
tries to understand the generated code's characteristics and leverage that to
improve failed cases. In this paper, as the most straightforward characteristic
of code, we investigate the relationship between code complexity and the
success of LLM generated code. Using a large set of standard complexity
metrics, we first conduct an empirical analysis to explore their correlation
with LLM's performance on code generation (i.e., Pass@1). Using logistic
regression models, we identify which complexity metrics are most predictive of
code correctness. Building on these findings, we propose an iterative feedback
method, where LLMs are prompted to generate correct code based on complexity
metrics from previous failed outputs. We validate our approach across multiple
benchmarks (i.e., HumanEval, MBPP, LeetCode, and BigCodeBench) and various LLMs
(i.e., GPT-4o, GPT-3.5 Turbo, Llama 3.1, and GPT-o3 mini), comparing the
results with two baseline methods: (a) zero-shot generation, and (b) iterative
execution-based feedback without our code complexity insights. Experiment
results show that our approach makes notable improvements, particularly with a
smaller LLM (GPT3.5 Turbo), where, e.g., Pass@1 increased by 35.71% compared to
the baseline's improvement of 12.5% on the HumanEval dataset. The study expands
experiments to BigCodeBench and integrates the method with the Reflexion code
generation agent, leading to Pass@1 improvements of 20% (GPT-4o) and 23.07%
(GPT-o3 mini). The results highlight that complexity-aware feedback enhances
both direct LLM prompting and agent-based workflows.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [361] [INSIGHT: A Survey of In-Network Systems for Intelligent, High-Efficiency AI and Topology Optimization](https://arxiv.org/abs/2505.24269)
*Aleksandr Algazinov,Joydeep Chandra,Matt Laing*

Key words: 网络内计算, AI, 可编程网络, 模型压缩, 分布式学习, 边缘AI

<details>
  <summary>Details</summary>

Main category: cs.NI

TL;DR: 本文综述了网络内计算在AI中的应用，探讨了优化方法、挑战及未来研究方向。

Motivation: 随着AI工作负载对网络基础设施需求的增加，网络内计算成为一种降低延迟、提高吞吐量和优化资源利用的有效方法。

Method: 分析了可编程网络架构（如SDN和PDPs）与AI的结合，探讨了将AI模型映射到资源受限网络设备的方法，包括模型压缩和分布式学习。

Result: 提出了优化网络内计算的框架和应用，如Planter和Quark，并展示了其在智能网络监控和边缘AI中的潜力。

Conclusion: 网络内AI有望构建智能、高效的下一代网络，满足未来AI应用的需求。

Abstract: In-network computation represents a transformative approach to addressing the
escalating demands of Artificial Intelligence (AI) workloads on network
infrastructure. By leveraging the processing capabilities of network devices
such as switches, routers, and Network Interface Cards (NICs), this paradigm
enables AI computations to be performed directly within the network fabric,
significantly reducing latency, enhancing throughput, and optimizing resource
utilization. This paper provides a comprehensive analysis of optimizing
in-network computation for AI, exploring the evolution of programmable network
architectures, such as Software-Defined Networking (SDN) and Programmable Data
Planes (PDPs), and their convergence with AI. It examines methodologies for
mapping AI models onto resource-constrained network devices, addressing
challenges like limited memory and computational capabilities through efficient
algorithm design and model compression techniques. The paper also highlights
advancements in distributed learning, particularly in-network aggregation, and
the potential of federated learning to enhance privacy and scalability.
Frameworks like Planter and Quark are discussed for simplifying development,
alongside key applications such as intelligent network monitoring, intrusion
detection, traffic management, and Edge AI. Future research directions,
including runtime programmability, standardized benchmarks, and new
applications paradigms, are proposed to advance this rapidly evolving field.
This survey underscores the potential of in-network AI to create intelligent,
efficient, and responsive networks capable of meeting the demands of
next-generation AI applications.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [362] [Deep Learning Weather Models for Subregional Ocean Forecasting: A Case Study on the Canary Current Upwelling System](https://arxiv.org/abs/2505.24429)
*Giovanny C-Londoño,Javier Sánchez,Ángel Rodríguez-Santana*

Key words: 海洋预测, 图神经网络, 深度学习, 加那利上升流系统, 亚区域尺度

<details>
  <summary>Details</summary>

Main category: physics.ao-ph

TL;DR: 该研究提出了一种基于图神经网络的深度学习方法，用于提升加那利上升流系统的亚区域海洋预测精度，相比传统方法在复杂区域表现更优。

Motivation: 传统海洋预测方法计算成本高且速度慢，而深度学习模型虽快但依赖全局数值模拟数据，可能无法反映真实情况。研究旨在探索数据驱动模型在亚区域精细尺度海洋过程预测中的潜力。

Method: 采用一种最初为全球天气预报设计的图神经网络，使用卫星数据训练，并与先进物理海洋模型（如ConvLSTM和GLORYS再分析）对比，评估其在海洋动力学捕捉中的表现。

Result: 深度学习模型在复杂区域（如Cape Ghir等）表现优于传统方法，5天预测的均方根误差相对ConvLSTM降低26.5%，相对GLORYS再分析最高减少76%。

Conclusion: 研究表明，将气象数据驱动模型适配于亚区域海洋中短期预测具有可行性，能够显著提升复杂区域的预测精度。

Abstract: Oceanographic forecasting impacts various sectors of society by supporting
environmental conservation and economic activities. Based on global circulation
models, traditional forecasting methods are computationally expensive and slow,
limiting their ability to provide rapid forecasts. Recent advances in deep
learning offer faster and more accurate predictions, although these data-driven
models are often trained with global data from numerical simulations, which may
not reflect reality. The emergence of such models presents great potential for
improving ocean prediction at a subregional domain. However, their ability to
predict fine-scale ocean processes, like mesoscale structures, remains largely
unknown. This work aims to adapt a graph neural network initially developed for
global weather forecasting to improve subregional ocean prediction,
specifically focusing on the Canary Current upwelling system. The model is
trained with satellite data and compared to state-of-the-art physical ocean
models to assess its performance in capturing ocean dynamics. Our results show
that the deep learning model surpasses traditional methods in precision despite
some challenges in upwelling areas. It demonstrated superior performance in
reducing RMSE errors compared to ConvLSTM and the GLORYS reanalysis,
particularly in regions with complex oceanic dynamics such as Cape Ghir, Cape
Bojador, and Cape Blanc. The model achieved improvements of up to 26.5%
relative to ConvLSTM and error reductions of up to 76% in 5-day forecasts
compared to the GLORYS reanalysis at these critical locations, highlighting its
enhanced capability to capture spatial variability and improve predictive
accuracy in complex areas. These findings suggest the viability of adapting
meteorological data-driven models for improving subregional medium-term ocean
forecasting.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [363] [Attractor learning for spatiotemporally chaotic dynamical systems using echo state networks with transfer learning](https://arxiv.org/abs/2505.24099)
*Mohammad Shah Alam,William Ott,Ilya Timofeyev*

Key words: 回声状态网络, Kuramoto-Sivashinsky方程, 迁移学习, 混沌吸引子, 预测

<details>
  <summary>Details</summary>

Main category: math.DS

TL;DR: 研究了回声状态网络（ESNs）在广义Kuramoto-Sivashinsky（gKS）方程中的预测能力，采用结合迁移学习的新方法以提升预测性能。

Motivation: 探索ESNs在非线性PDE（gKS方程）中的预测潜力，尤其关注其长期统计模式的变化。

Method: 结合ESNs与迁移学习，针对gKS模型的不同参数设置进行自适应调整。

Result: 成功捕捉了不同参数下混沌吸引子的变化，证明了方法的有效性。

Conclusion: ESNs加迁移学习的方法在预测gKS方程的动态变化中表现出色。

Abstract: In this paper, we explore the predictive capabilities of echo state networks
(ESNs) for the generalized Kuramoto-Sivashinsky (gKS) equation, an archetypal
nonlinear PDE that exhibits spatiotemporal chaos. We introduce a novel
methodology that integrates ESNs with transfer learning, aiming to enhance
predictive performance across various parameter regimes of the gKS model. Our
research focuses on predicting changes in long-term statistical patterns of the
gKS model that result from varying the dispersion relation or the length of the
spatial domain. We use transfer learning to adapt ESNs to different parameter
settings and successfully capture changes in the underlying chaotic attractor.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [364] [Representational Difference Explanations](https://arxiv.org/abs/2505.23917)
*Neehar Kondapaneni,Oisin Mac Aodha,Pietro Perona*

Key words: 模型比较, 可解释AI, 表示差异, 可视化, RDX

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种名为RDX的方法，用于发现和可视化两种学习表示之间的差异，从而支持更直接和可解释的模型比较。

Motivation: 由于现有可解释AI（XAI）方法在支持模型比较方面效果有限，本文旨在填补这一空白，提供一种更有效的工具来对比模型表示。

Method: 论文提出的RDX方法通过比较具有已知概念差异的模型来验证其有效性，并在ImageNet和iNaturalist数据集上展示了其恢复有意义差异的能力。

Result: RDX在实验中不仅揭示了深刻的表示差异，还发现了数据中的细微模式，优于现有XAI技术。

Conclusion: RDX是一种有效且可解释的工具，能够显著提升模型比较的科学性和直观性。

Abstract: We propose a method for discovering and visualizing the differences between
two learned representations, enabling more direct and interpretable model
comparisons. We validate our method, which we call Representational Differences
Explanations (RDX), by using it to compare models with known conceptual
differences and demonstrate that it recovers meaningful distinctions where
existing explainable AI (XAI) techniques fail. Applied to state-of-the-art
models on challenging subsets of the ImageNet and iNaturalist datasets, RDX
reveals both insightful representational differences and subtle patterns in the
data. Although comparison is a cornerstone of scientific analysis, current
tools in machine learning, namely post hoc XAI methods, struggle to support
model comparison effectively. Our work addresses this gap by introducing an
effective and explainable tool for contrasting model representations.

</details>


### [365] [VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL](https://arxiv.org/abs/2505.23977)
*Yichen Feng,Zhangchen Xu,Fengqing Jiang,Yuetai Li,Bhaskar Ramasubramanian,Luyao Niu,Bill Yuchen Lin,Radha Poovendran*

Key words: 视觉语言模型, 逻辑推理, 合成数据, 视觉逻辑, 多模态

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文介绍了VisualSphinx，首个大规模合成的视觉逻辑推理训练数据集，解决了现有视觉语言模型（VLMs）在逻辑推理任务中缺乏结构化数据的问题。通过规则到图像的合成流程，该数据集提升了VLMs的逻辑连贯性和任务表现。

Motivation: 当前视觉语言模型在逻辑推理任务中缺乏大规模且结构化的训练数据，限制了其多模态推理和决策能力。本文旨在填补这一空白。

Method: 提出了VisualSphinx数据集，采用规则到图像的合成流程，从种子问题中提取并扩展谜题规则，生成带有接地答案的合成图像。

Result: 实验表明，使用VisualSphinx训练的VLM在逻辑推理任务中表现更优，且增强了其他推理任务（如代数、算术和几何推理）的能力。

Conclusion: VisualSphinx通过结构化数据和合成方法有效提升了VLM的逻辑推理能力，并具有广泛的迁移应用潜力。

Abstract: Vision language models (VLMs) are expected to perform effective multimodal
reasoning and make logically coherent decisions, which is critical to tasks
such as diagram understanding and spatial problem solving. However, current VLM
reasoning lacks large-scale and well-structured training datasets. To bridge
this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic
visual logical reasoning training data. To tackle the challenge of image
synthesis with grounding answers, we propose a rule-to-image synthesis
pipeline, which extracts and expands puzzle rules from seed questions and
generates the code of grounding synthesis image synthesis for puzzle sample
assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx
benefit from logical coherence and readability of our dataset and exhibit
improved performance on logical reasoning tasks. The enhanced reasoning
capabilities developed from VisualSphinx also benefit other reasoning tasks
such as algebraic reasoning, arithmetic reasoning and geometry reasoning.

</details>


### [366] [Multi-Group Proportional Representation for Text-to-Image Models](https://arxiv.org/abs/2505.24023)
*Sangwon Jung,Alex Oesterling,Claudio Mayrink Verdun,Sajani Vithana,Taesup Moon,Flavio P. Calmon*

Key words: 文本到图像生成、代表性、公平性、MPR、交叉群体

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种新框架，用于测量文本到图像生成模型在不同交叉群体中的代表性，并开发了一种优化算法以提高平衡性。

Motivation: 随着文本到图像生成模型的普及，其可能对不同人口群体的代表性不足或传播刻板印象的问题日益突出，需要系统化的测量和控制方法。

Method: 采用多组比例表示（MPR）指标评估生成图像中各个群体的代表性差异，并提出一种优化算法以提高平衡性。

Result: 实验表明MPR能有效测量多交叉群体的代表性，并在保持生成质量的同时指导模型更平衡地生成内容。

Conclusion: MPR框架为测量和控制生成模型的代表性提供了有效工具，有助于促进AI的公平性和包容性。

Abstract: Text-to-image (T2I) generative models can create vivid, realistic images from
textual descriptions. As these models proliferate, they expose new concerns
about their ability to represent diverse demographic groups, propagate
stereotypes, and efface minority populations. Despite growing attention to the
"safe" and "responsible" design of artificial intelligence (AI), there is no
established methodology to systematically measure and control representational
harms in image generation. This paper introduces a novel framework to measure
the representation of intersectional groups in images generated by T2I models
by applying the Multi-Group Proportional Representation (MPR) metric. MPR
evaluates the worst-case deviation of representation statistics across given
population groups in images produced by a generative model, allowing for
flexible and context-specific measurements based on user requirements. We also
develop an algorithm to optimize T2I models for this metric. Through
experiments, we demonstrate that MPR can effectively measure representation
statistics across multiple intersectional groups and, when used as a training
objective, can guide models toward a more balanced generation across
demographic groups while maintaining generation quality.

</details>


### [367] [DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models](https://arxiv.org/abs/2505.24025)
*Chenbin Pan,Wenbin He,Zhengzhong Tu,Liu Ren*

Key words: DINO-R1, 视觉推理, 增强学习, GRQO, KL正则化, 开放词汇

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了DINO-R1，首个尝试通过增强学习提升视觉基础模型的上下文推理能力，引入了GRQO训练策略，结合KL正则化稳定训练效果，并在多个数据集上表现优异。

Motivation: 现有大型语言模型的推理能力通过增强学习微调框架取得了显著成功，但视觉基础模型的此类能力尚未充分探索，尤其是DINO系列表示模型的推理能力仍未被开发。

Method: 提出DINO-R1，采用GRQO训练策略，通过查询级别奖励优化表示模型，并引入KL正则化稳定对象分布。模型结合视觉提示编码器和视觉引导查询选择机制。

Result: 在COCO、LVIS和ODinW等数据集上，DINO-R1显著优于监督微调基线，在开放词汇和封闭集视觉提示场景中均表现出强大的泛化能力。

Conclusion: DINO-R1成功通过增强学习提升了视觉基础模型的推理能力，GRQO策略与KL正则化的联合优化有效提高了模型的表达能力和稳定性。

Abstract: The recent explosive interest in the reasoning capabilities of large language
models, such as DeepSeek-R1, has demonstrated remarkable success through
reinforcement learning-based fine-tuning frameworks, exemplified by methods
like Group Relative Policy Optimization (GRPO). However, such reasoning
abilities remain underexplored and notably absent in vision foundation models,
including representation models like the DINO series. In this work, we propose
\textbf{DINO-R1}, the first such attempt to incentivize visual in-context
reasoning capabilities of vision foundation models using reinforcement
learning. Specifically, DINO-R1 introduces \textbf{Group Relative Query
Optimization (GRQO)}, a novel reinforcement-style training strategy explicitly
designed for query-based representation models, which computes query-level
rewards based on group-normalized alignment quality. We also apply
KL-regularization to stabilize the objectness distribution to reduce the
training instability. This joint optimization enables dense and expressive
supervision across queries while mitigating overfitting and distributional
drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models
that integrate a visual prompt encoder and a visual-guided query selection
mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that
DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving
strong generalization in both open-vocabulary and closed-set visual prompting
scenarios.

</details>


### [368] [MaskAdapt: Unsupervised Geometry-Aware Domain Adaptation Using Multimodal Contextual Learning and RGB-Depth Masking](https://arxiv.org/abs/2505.24026)
*Numair Nadeem,Muhammad Hamza Asad,Saeed Anwar,Abdul Bais*

Key words: 语义分割, 无监督域适应, 多模态学习, 农业, MaskAdapt

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: MaskAdapt是一种新颖的无监督域适应方法，通过整合RGB图像和深度数据特征，提升作物和杂草的语义分割准确性，解决了现有方法在遮挡和视觉混淆方面的局限性。

Motivation: 现有的作物和杂草语义分割方法依赖像素级标注且域适应性差，无法应对光照、相机设置等变化。

Method: MaskAdapt通过深度梯度捕捉空间过渡，结合交叉注意力机制优化RGB特征，并提出几何感知掩码策略增强模型对空间上下文的关注。

Result: 在真实农业数据集上，MaskAdapt在mIOU上优于现有SOTA方法。

Conclusion: MaskAdapt通过多模态上下文学习显著提升域适应能力，适用于多样化农田条件。

Abstract: Semantic segmentation of crops and weeds is crucial for site-specific farm
management; however, most existing methods depend on labor intensive
pixel-level annotations. A further challenge arises when models trained on one
field (source domain) fail to generalize to new fields (target domain) due to
domain shifts, such as variations in lighting, camera setups, soil composition,
and crop growth stages. Unsupervised Domain Adaptation (UDA) addresses this by
enabling adaptation without target-domain labels, but current UDA methods
struggle with occlusions and visual blending between crops and weeds, leading
to misclassifications in real-world conditions. To overcome these limitations,
we introduce MaskAdapt, a novel approach that enhances segmentation accuracy
through multimodal contextual learning by integrating RGB images with features
derived from depth data. By computing depth gradients from depth maps, our
method captures spatial transitions that help resolve texture ambiguities.
These gradients, through a cross-attention mechanism, refines RGB feature
representations, resulting in sharper boundary delineation. In addition, we
propose a geometry-aware masking strategy that applies horizontal, vertical,
and stochastic masks during training. This encourages the model to focus on the
broader spatial context for robust visual recognition. Evaluations on real
agricultural datasets demonstrate that MaskAdapt consistently outperforms
existing State-of-the-Art (SOTA) UDA methods, achieving improved segmentation
mean Intersection over Union (mIOU) across diverse field conditions.

</details>


### [369] [CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs](https://arxiv.org/abs/2505.24120)
*Ai Jian,Weijie Qiu,Xiaokun Wang,Peiyu Wang,Yunzhuo Hao,Jiangbo Pei,Yichen Wei,Yi Peng,Xuchen Song*

Key words: 视觉语言模型, 科学推理, 多模态基准, 视觉问答, STEM

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了CSVQA基准测试，用于评估视觉语言模型在科学推理中的表现，发现现有模型性能不足。

Motivation: 当前多模态基准测试缺乏科学上下文，无法评估视觉语言模型在整合领域知识和视觉证据方面的能力。

Method: 设计了包含1,378个跨STEM学科的问答对的CSVQA基准，并提出基于中间推理步骤的评估协议。

Result: 评估15个视觉语言模型后，发现最佳模型准确率仅为49.6%，表明科学推理能力亟待提升。

Conclusion: CSVQA凸显了当前模型在科学推理任务中的局限，呼吁进一步研究改进。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable progress in
multimodal understanding, yet their capabilities for scientific reasoning
remains inadequately assessed. Current multimodal benchmarks predominantly
evaluate generic image comprehension or text-driven reasoning, lacking
authentic scientific contexts that require domain-specific knowledge
integration with visual evidence analysis. To fill this gap, we present CSVQA,
a diagnostic multimodal benchmark specifically designed for evaluating
scientific reasoning through domain-grounded visual question answering.Our
benchmark features 1,378 carefully constructed question-answer pairs spanning
diverse STEM disciplines, each demanding domain knowledge, integration of
visual evidence, and higher-order reasoning. Compared to prior multimodal
benchmarks, CSVQA places greater emphasis on real-world scientific content and
complex reasoning.We additionally propose a rigorous evaluation protocol to
systematically assess whether model predictions are substantiated by valid
intermediate reasoning steps based on curated explanations. Our comprehensive
evaluation of 15 VLMs on this benchmark reveals notable performance
disparities, as even the top-ranked proprietary model attains only 49.6\%
accuracy.This empirical evidence underscores the pressing need for advancing
scientific reasoning capabilities in VLMs. Our CSVQA is released at
https://huggingface.co/datasets/Skywork/CSVQA.

</details>


### [370] [S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation](https://arxiv.org/abs/2505.24139)
*Yichen Xie,Runsheng Xu,Tong He,Jyh-Jing Hwang,Katie Luo,Jingwei Ji,Hubert Lin,Letian Chen,Yiren Lu,Zhaoqi Leng,Dragomir Anguelov,Mingxing Tan*

Key words: 多模态大语言模型，自动驾驶，自监督学习，3D表示，运动规划

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种基于多模态大语言模型的自监督运动规划算法S4-Driver，通过3D空间表示提升自动驾驶规划性能，无需人工标注。

Motivation: 现有端到端自动驾驶方法依赖人工标注或局限于2D图像空间，缺乏在3D空间中的有效表示，限制了规划效果。

Method: S4-Driver采用稀疏体积策略，将MLLMs的视觉表示从2D视角无缝转换到3D空间，结合多视角和多帧输入，实现自监督学习。

Result: 在nuScenes和Waymo Open Motion Dataset上的实验表明，S4-Driver性能优于现有监督多任务方法，且具有良好扩展性。

Conclusion: S4-Driver展示了自监督方法在3D空间运动规划中的潜力，为自动驾驶提供了可扩展的解决方案。

Abstract: The latest advancements in multi-modal large language models (MLLMs) have
spurred a strong renewed interest in end-to-end motion planning approaches for
autonomous driving. Many end-to-end approaches rely on human annotations to
learn intermediate perception and prediction tasks, while purely
self-supervised approaches--which directly learn from sensor inputs to generate
planning trajectories without human annotations often underperform the state of
the art. We observe a key gap in the input representation space: end-to-end
approaches built on MLLMs are often pretrained with reasoning tasks in 2D image
space rather than the native 3D space in which autonomous vehicles plan. To
this end, we propose S4-Driver, a scalable self-supervised motion planning
algorithm with spatio-temporal visual representation, based on the popular PaLI
multimodal large language model. S4-Driver uses a novel sparse volume strategy
to seamlessly transform the strong visual representation of MLLMs from
perspective view to 3D space without the need to finetune the vision encoder.
This representation aggregates multi-view and multi-frame visual inputs and
enables better prediction of planning trajectories in 3D space. To validate our
method, we run experiments on both nuScenes and Waymo Open Motion Dataset (with
in-house camera data). Results show that S4-Driver performs favorably against
existing supervised multi-task approaches while requiring no human annotations.
It also demonstrates great scalability when pretrained on large volumes of
unannotated driving logs.

</details>


### [371] [The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models](https://arxiv.org/abs/2505.24141)
*Jiashuai Liu,Yingjia Shang,Yingkang Zhan,Di Zhang,Yi Niu,Dong Wei,Xian Wu,Zeyu Gao,Chen Li,Yefeng Zheng*

Key words: 病理学基础模型、全玻片图像分析、对抗性攻击、无标签攻击、安全性

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文首次系统研究了病理学基础模型在全玻片图像（WSI）分析中的安全性，提出了无标签攻击框架，并在实验中验证了其有效性。

Motivation: 随着病理学基础模型在研究和临床决策支持系统中的广泛应用，其安全性问题日益突出，但对抗性攻击的脆弱性尚未得到充分研究。

Method: 提出了“局部扰动全局影响”原则的无标签攻击框架，改进了四种经典白盒攻击方法，并根据WSI特性重新定义了扰动预算。

Result: 实验显示，仅修改每张玻片的0.1%片块且噪声不可察觉时，下游任务准确率最大可下降20%。

Conclusion: 研究结果为病理学基础模型的对抗鲁棒性和可靠部署奠定了基础。

Abstract: With the widespread adoption of pathology foundation models in both research
and clinical decision support systems, exploring their security has become a
critical concern. However, despite their growing impact, the vulnerability of
these models to adversarial attacks remains largely unexplored. In this work,
we present the first systematic investigation into the security of pathology
foundation models for whole slide image~(WSI) analysis against adversarial
attacks. Specifically, we introduce the principle of \textit{local perturbation
with global impact} and propose a label-free attack framework that operates
without requiring access to downstream task labels. Under this attack
framework, we revise four classical white-box attack methods and redefine the
perturbation budget based on the characteristics of WSI. We conduct
comprehensive experiments on three representative pathology foundation models
across five datasets and six downstream tasks. Despite modifying only 0.1\% of
patches per slide with imperceptible noise, our attack leads to downstream
accuracy degradation that can reach up to 20\% in the worst cases. Furthermore,
we analyze key factors that influence attack success, explore the relationship
between patch-level vulnerability and semantic content, and conduct a
preliminary investigation into potential defence strategies. These findings lay
the groundwork for future research on the adversarial robustness and reliable
deployment of pathology foundation models. Our code is publicly available at:
https://github.com/Jiashuai-Liu-hmos/Attack-WSI-pathology-foundation-models.

</details>


### [372] [Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT](https://arxiv.org/abs/2505.24182)
*Zhuobai Dong,Junchao Yi,Ziyuan Zheng,Haochen Han,Xiangxi Zheng,Alex Jinpeng Wang,Fangming Liu,Linjie Li*

Key words: MLLMs, visual physical reasoning, MVPBench, chain-of-thought, spatial causality

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: MVPBench is introduced to evaluate MLLMs' visual physical reasoning, revealing their poor performance in grasping physical laws and coherent reasoning chains despite advancements.

Motivation: Assess MLLMs' ability to understand physical laws and spatial causality in complex scenes, highlighting their limitations.

Method: Created MVPBench with multi-image inputs and visual chain-of-thought (CoT) requirements, using a graph-based CoT consistency metric for evaluation.

Result: Even advanced MLLMs perform poorly in visual reasoning and image-text alignment in physical domains, with RL-based alignment harming spatial reasoning.

Conclusion: Current MLLMs struggle with visual physical reasoning, necessitating a reevaluation of fine-tuning practices.

Abstract: Understanding the physical world - governed by laws of motion, spatial
relations, and causality - poses a fundamental challenge for multimodal large
language models (MLLMs). While recent advances such as OpenAI o3 and GPT-4o
demonstrate impressive perceptual and reasoning capabilities, our investigation
reveals these models struggle profoundly with visual physical reasoning,
failing to grasp basic physical laws, spatial interactions, and causal effects
in complex scenes. More importantly, they often fail to follow coherent
reasoning chains grounded in visual evidence, especially when multiple steps
are needed to arrive at the correct answer. To rigorously evaluate this
capability, we introduce MVPBench, a curated benchmark designed to rigorously
evaluate visual physical reasoning through the lens of visual chain-of-thought
(CoT). Each example features interleaved multi-image inputs and demands not
only the correct final answer but also a coherent, step-by-step reasoning path
grounded in evolving visual cues. This setup mirrors how humans reason through
real-world physical processes over time. To ensure fine-grained evaluation, we
introduce a graph-based CoT consistency metric that verifies whether the
reasoning path of model adheres to valid physical logic. Additionally, we
minimize shortcut exploitation from text priors, encouraging models to rely on
visual understanding. Experimental results reveal a concerning trend: even
cutting-edge MLLMs exhibit poor visual reasoning accuracy and weak image-text
alignment in physical domains. Surprisingly, RL-based post-training alignment -
commonly believed to improve visual reasoning performance - often harms spatial
reasoning, suggesting a need to rethink current fine-tuning practices.

</details>


### [373] [Benchmarking Foundation Models for Zero-Shot Biometric Tasks](https://arxiv.org/abs/2505.24214)
*Redwan Sony,Parisa Farmanifard,Hamzeh Alzwairy,Nitish Shukla,Arun Ross*

Key words: 基础模型,生物识别,视觉语言模型,多模态大语言模型,零样本学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文探讨了基础模型（如视觉语言模型和多模态大型语言模型）在生物识别任务中的零样本和少样本性能，通过多个任务展示了其潜力。

Motivation: 研究动机是利用预训练模型在生物识别任务中的泛化能力，探索其在无需微调或少量监督下的表现。

Method: 方法是通过41种视觉语言模型，评估其在六种生物识别任务中的性能，包括人脸验证、软生物特征预测等。

Result: 结果显示，这些模型在多种任务中表现优异，例如人脸验证准确率达96.77%，虹膜识别达97.55%。

Conclusion: 结论是预训练模型在生物识别任务中具有广泛潜力，有助于实现人工通用智能的长期目标。

Abstract: The advent of foundation models, particularly Vision-Language Models (VLMs)
and Multi-modal Large Language Models (MLLMs), has redefined the frontiers of
artificial intelligence, enabling remarkable generalization across diverse
tasks with minimal or no supervision. Yet, their potential in biometric
recognition and analysis remains relatively underexplored. In this work, we
introduce a comprehensive benchmark that evaluates the zero-shot and few-shot
performance of state-of-the-art publicly available VLMs and MLLMs across six
biometric tasks spanning the face and iris modalities: face verification, soft
biometric attribute prediction (gender and race), iris recognition,
presentation attack detection (PAD), and face manipulation detection (morphs
and deepfakes). A total of 41 VLMs were used in this evaluation. Experiments
show that embeddings from these foundation models can be used for diverse
biometric tasks with varying degrees of success. For example, in the case of
face verification, a True Match Rate (TMR) of 96.77 percent was obtained at a
False Match Rate (FMR) of 1 percent on the Labeled Face in the Wild (LFW)
dataset, without any fine-tuning. In the case of iris recognition, the TMR at 1
percent FMR on the IITD-R-Full dataset was 97.55 percent without any
fine-tuning. Further, we show that applying a simple classifier head to these
embeddings can help perform DeepFake detection for faces, Presentation Attack
Detection (PAD) for irides, and extract soft biometric attributes like gender
and ethnicity from faces with reasonably high accuracy. This work reiterates
the potential of pretrained models in achieving the long-term vision of
Artificial General Intelligence.

</details>


### [374] [Reasoning Can Hurt the Inductive Abilities of Large Language Models](https://arxiv.org/abs/2505.24225)
*Haibo Jin,Peiyan Zhang,Man Luo,Haohan Wang*

Key words: 大语言模型，思维链推理，归纳推理，结构化干预

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 研究发现，思维链（CoT）推理可能降低大语言模型（LLMs）的归纳推理能力，并提出结构化干预措施以改进推理步骤。

Motivation: 探究思维链推理是否真的能增强大型语言模型的归纳推理能力，尤其是在稀疏示例中推断隐含规则的场景。

Method: 设计了四种基于游戏的诊断任务（国际象棋、德州扑克、骰子游戏和21点），并通过理论框架分析推理步骤中的错误模式。

Result: 发现CoT推理可能降低模型性能，并识别了三种错误模式：子任务分解错误、子任务求解错误和最终答案汇总错误。提出的结构化干预措施有效提升了推理准确率。

Conclusion: 有效的CoT推理不仅需要更多步骤，还需要确保这些步骤结构合理。

Abstract: Large Language Models (LLMs) have shown remarkable progress across domains,
yet their ability to perform inductive reasoning - inferring latent rules from
sparse examples - remains limited. It is often assumed that chain-of-thought
(CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such
reasoning. We investigate this assumption with creating four controlled,
diagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack -
with hidden human-defined rules. We find that CoT reasoning can degrade
inductive performance, with LRMs often underperforming their non-reasoning
counterparts.
  To explain this, we present a theoretical framework that reveals how
reasoning steps can amplify error through three failure modes: incorrect
sub-task decomposition, incorrect sub-task solving, and incorrect final answer
summarization. Based on our theoretical and empirical analysis, we introduce
structured interventions that adapt CoT generation according to our identified
failure types. These interventions improve inductive accuracy without
retraining. Our findings suggest that effective (CoT) reasoning depends not
only on taking more steps but also on ensuring those steps are well-structured.

</details>


### [375] [From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models](https://arxiv.org/abs/2505.24232)
*Haibo Jin,Peiyan Zhang,Peiran Wang,Man Luo,Haohan Wang*

Key words: 大型基础模型, 越狱攻击, 幻觉, 统一框架, 优化

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出一个统一理论框架，将越狱攻击和幻觉问题建模为优化问题，发现两者在损失收敛和梯度行为上有共性，并验证通过一种问题的缓解技术可同时改善另一种问题。

Motivation: 探索大型基础模型（LFMs）中越狱攻击和幻觉两种漏洞之间的潜在联系，以开发更全面的防御策略。

Method: 提出理论框架，将越狱攻击建模为词元级优化，幻觉建模为注意力级优化，并通过实验在LLaVA-1.5和MiniGPT-4上验证理论。

Result: 实验表明两种漏洞的优化趋势和梯度行为一致，且针对一种问题的缓解技术可交叉降低另一种漏洞的成功率。

Conclusion: 研究揭示了LFMs的共享失效模式，建议防御策略需联合处理两种漏洞。

Abstract: Large foundation models (LFMs) are susceptible to two distinct
vulnerabilities: hallucinations and jailbreak attacks. While typically studied
in isolation, we observe that defenses targeting one often affect the other,
hinting at a deeper connection.
  We propose a unified theoretical framework that models jailbreaks as
token-level optimization and hallucinations as attention-level optimization.
Within this framework, we establish two key propositions: (1) \textit{Similar
Loss Convergence} - the loss functions for both vulnerabilities converge
similarly when optimizing for target-specific outputs; and (2) \textit{Gradient
Consistency in Attention Redistribution} - both exhibit consistent gradient
behavior driven by shared attention dynamics.
  We validate these propositions empirically on LLaVA-1.5 and MiniGPT-4,
showing consistent optimization trends and aligned gradients. Leveraging this
connection, we demonstrate that mitigation techniques for hallucinations can
reduce jailbreak success rates, and vice versa. Our findings reveal a shared
failure mode in LFMs and suggest that robustness strategies should jointly
address both vulnerabilities.

</details>


### [376] [LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework](https://arxiv.org/abs/2505.24245)
*Xin Kang,Zihan Zheng,Lei Chu,Yue Gao,Jiahao Li,Hao Pan,Xuejin Chen,Yan Lu*

Key words: LTM3D, 3D shape generation, diffusion models, auto-regressive models, multi-modal generation

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: LTM3D integrates diffusion and auto-regressive models for conditional 3D shape generation, improving flexibility and fidelity through novel modules like Prefix Learning and Reconstruction-Guided Sampling.

Motivation: Combining the strengths of diffusion-based methods (continuous latent space modeling) and auto-regressive models (inter-token dependencies) for 3D shape generation remains challenging, which LTM3D aims to address.

Method: LTM3D uses a Conditional Distribution Modeling backbone with a masked autoencoder and diffusion model, Prefix Learning for condition-shape alignment, and a Latent Token Reconstruction module with Reconstruction-Guided Sampling.

Result: LTM3D outperforms existing methods in prompt fidelity and structural accuracy across image- and text-conditioned shape generation tasks, supporting multiple 3D representations.

Conclusion: LTM3D offers a generalizable framework for multi-modal, multi-representation 3D generation, demonstrating superior performance and flexibility.

Abstract: We present LTM3D, a Latent Token space Modeling framework for conditional 3D
shape generation that integrates the strengths of diffusion and auto-regressive
(AR) models. While diffusion-based methods effectively model continuous latent
spaces and AR models excel at capturing inter-token dependencies, combining
these paradigms for 3D shape generation remains a challenge. To address this,
LTM3D features a Conditional Distribution Modeling backbone, leveraging a
masked autoencoder and a diffusion model to enhance token dependency learning.
Additionally, we introduce Prefix Learning, which aligns condition tokens with
shape latent tokens during generation, improving flexibility across modalities.
We further propose a Latent Token Reconstruction module with
Reconstruction-Guided Sampling to reduce uncertainty and enhance structural
fidelity in generated shapes. Our approach operates in token space, enabling
support for multiple 3D representations, including signed distance fields,
point clouds, meshes, and 3D Gaussian Splatting. Extensive experiments on
image- and text-conditioned shape generation tasks demonstrate that LTM3D
outperforms existing methods in prompt fidelity and structural accuracy while
offering a generalizable framework for multi-modal, multi-representation 3D
generation.

</details>


### [377] [Interactive Video Generation via Domain Adaptation](https://arxiv.org/abs/2505.24253)
*Ishaan Rawal,Suryansh Kumar*

Key words: 交互式视频生成, 扩散模型, 掩膜归一化, 时序内在扩散先验, 轨迹控制

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文针对交互式视频生成（IVG）中的轨迹控制问题，提出了掩膜归一化和时序内在扩散先验，解决现有方法的感知质量下降和初始化差距问题。

Motivation: 当前基于文本条件的扩散模型在高质量视频生成中表现出色，但在交互式视频生成中对运动元素（如物体轨迹）的精确控制仍面临挑战。现有训练无关方法通过注意力掩膜引导轨迹，但往往导致感知质量下降。

Method: 1. 提出掩膜归一化（mask normalization）以解决内部协变量偏移；2. 引入时序内在扩散先验（temporal intrinsic diffusion prior）以填补初始化差距。

Result: 通过定性和定量评估，证明掩膜归一化和时序内在去噪在感知质量和轨迹控制上均优于现有IVG技术。

Conclusion: 该方法显著提升了交互式视频生成的质量和控制能力，为相关领域提供了新思路。

Abstract: Text-conditioned diffusion models have emerged as powerful tools for
high-quality video generation. However, enabling Interactive Video Generation
(IVG), where users control motion elements such as object trajectory, remains
challenging. Recent training-free approaches introduce attention masking to
guide trajectory, but this often degrades perceptual quality. We identify two
key failure modes in these methods, both of which we interpret as domain shift
problems, and propose solutions inspired by domain adaptation. First, we
attribute the perceptual degradation to internal covariate shift induced by
attention masking, as pretrained models are not trained to handle masked
attention. To address this, we propose mask normalization, a pre-normalization
layer designed to mitigate this shift via distribution matching. Second, we
address initialization gap, where the randomly sampled initial noise does not
align with IVG conditioning, by introducing a temporal intrinsic diffusion
prior that enforces spatio-temporal consistency at each denoising step.
Extensive qualitative and quantitative evaluations demonstrate that mask
normalization and temporal intrinsic denoising improve both perceptual quality
and trajectory control over the existing state-of-the-art IVG techniques.

</details>


### [378] [Grid-LOGAT: Grid Based Local and Global Area Transcription for Video Question Answering](https://arxiv.org/abs/2505.24371)
*Md Intisar Chowdhury,Kittinun Aukkapinyo,Hiroshi Fujimura,Joo Ann Woo,Wasu Wasusatein,Fadoua Ghourabi*

Key words: VideoQA, Grid-LoGAT, VLM, LLM, 网格视觉提示, 隐私保护

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了Grid-LoGAT，一种基于网格的局部和全局区域转录系统，用于视频问答。通过视觉语言模型（VLM）提取视频帧中的文本转录，再利用大语言模型（LLM）处理问题生成答案，该系统在NExT-QA和STAR-QA数据集上表现优于现有方法。

Motivation: 为了提高视频问答的准确性并保护图像隐私，作者设计了Grid-LoGAT系统，结合边缘设备上的VLM和云端的LLM，并通过网格视觉提示增强转录质量。

Method: 系统分两阶段：1）用VLM提取视频帧的文本转录；2）用LLM基于转录回答提问。采用网格视觉提示整合局部细节和全局信息。

Result: Grid-LoGAT在NExT-QA和STAR-QA数据集上的准确率分别为65.9%和50.11%，优于同类基线模型，并比非网格版本在定位问题上高出24分。

Conclusion: Grid-LoGAT有效提升了视频问答性能，隐私保护设计也使其更具实用性。

Abstract: In this paper, we propose a Grid-based Local and Global Area Transcription
(Grid-LoGAT) system for Video Question Answering (VideoQA). The system operates
in two phases. First, extracting text transcripts from video frames using a
Vision-Language Model (VLM). Next, processing questions using these transcripts
to generate answers through a Large Language Model (LLM). This design ensures
image privacy by deploying the VLM on edge devices and the LLM in the cloud. To
improve transcript quality, we propose grid-based visual prompting, which
extracts intricate local details from each grid cell and integrates them with
global information. Evaluation results show that Grid-LoGAT, using the
open-source VLM (LLaVA-1.6-7B) and LLM (Llama-3.1-8B), outperforms
state-of-the-art methods with similar baseline models on NExT-QA and STAR-QA
datasets with an accuracy of 65.9% and 50.11% respectively. Additionally, our
method surpasses the non-grid version by 24 points on localization-based
questions we created using NExT-QA.

</details>


### [379] [SASP: Strip-Aware Spatial Perception for Fine-Grained Bird Image Classification](https://arxiv.org/abs/2505.24380)
*Zheng Wang*

Key words: 细粒度分类、鸟类识别、空间感知、通道语义、ResNet-50

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种基于带状空间感知的细粒度鸟类图像分类框架，通过两个新模块（EPA和CSW）整合局部与全局特征，显著提升模型性能。

Motivation: 细粒度鸟类图像分类在生态监测和物种识别中具有重要价值，但面临鸟类尺寸变化、背景干扰和姿态多样性等挑战，传统方法难以稳定提取判别性特征。

Method: 提出基于带状空间感知的分类框架，包含EPA模块（整合局部纹理与全局结构）和CSW模块（自适应融合长短期通道信息），基于ResNet-50骨干网络实现空间特征跳连。

Result: 在CUB-200-2011数据集上实验显示，该方法性能显著提升且保持结构高效。

Conclusion: 通过空间感知和通道语义融合，提出的框架有效提升了细粒度分类的鲁棒性和可解释性。

Abstract: Fine-grained bird image classification (FBIC) is not only of great
significance for ecological monitoring and species identification, but also
holds broad research value in the fields of image recognition and fine-grained
visual modeling. Compared with general image classification tasks, FBIC poses
more formidable challenges: 1) the differences in species size and imaging
distance result in the varying sizes of birds presented in the images; 2)
complex natural habitats often introduce strong background interference; 3) and
highly flexible poses such as flying, perching, or foraging result in
substantial intra-class variability. These factors collectively make it
difficult for traditional methods to stably extract discriminative features,
thereby limiting the generalizability and interpretability of models in
real-world applications. To address these challenges, this paper proposes a
fine-grained bird classification framework based on strip-aware spatial
perception, which aims to capture long-range spatial dependencies across entire
rows or columns in bird images, thereby enhancing the model's robustness and
interpretability. The proposed method incorporates two novel modules:
extensional perception aggregator (EPA) and channel semantic weaving (CSW).
Specifically, EPA integrates local texture details with global structural cues
by aggregating information across horizontal and vertical spatial directions.
CSW further refines the semantic representations by adaptively fusing
long-range and short-range information along the channel dimension. Built upon
a ResNet-50 backbone, the model enables jump-wise connection of extended
structural features across the spatial domain. Experimental results on the
CUB-200-2011 dataset demonstrate that our framework achieves significant
performance improvements while maintaining architectural efficiency.

</details>


### [380] [BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning](https://arxiv.org/abs/2505.23883)
*Jianyang Gu,Samuel Stevens,Elizabeth G Campolongo,Matthew J Thompson,Net Zhang,Jiaman Wu,Andrei Kopanev,Zheda Mai,Alexander E. White,James Balhoff,Wasila Dahdul,Daniel Rubenstein,Hilmar Lapp,Tanya Berger-Wolf,Wei-Lun Chao,Yu Su*

Key words: 涌现行为、对比学习、生物视觉、层级表征、大规模训练

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文通过大规模对比视觉语言训练发现生物视觉模型中的涌现行为，训练BioCLIP 2在TreeOfLife-200M数据集上表现优异，其嵌入空间具有生物意义的层级结构。

Motivation: 探索大规模训练的基础模型在生物视觉任务中的涌现能力，验证数据规模对模型性能的影响。

Method: 构建TreeOfLife-200M数据集，利用对比学习训练BioCLIP 2模型，通过层级监督和对比目标优化嵌入空间。

Result: BioCLIP 2在物种分类、栖息地预测等任务中表现卓越，嵌入空间展现出与生态功能对齐的层级结构。

Conclusion: 大规模训练能促进生物视觉模型涌现出有意义的层级表征，数据规模是关键因素。

Abstract: Foundation models trained at scale exhibit remarkable emergent behaviors,
learning new capabilities beyond their initial training objectives. We find
such emergent behaviors in biological vision models via large-scale contrastive
vision-language training. To achieve this, we first curate TreeOfLife-200M,
comprising 214 million images of living organisms, the largest and most diverse
biological organism image dataset to date. We then train BioCLIP 2 on
TreeOfLife-200M to distinguish different species. Despite the narrow training
objective, BioCLIP 2 yields extraordinary accuracy when applied to various
biological visual tasks such as habitat classification and trait prediction. We
identify emergent properties in the learned embedding space of BioCLIP 2. At
the inter-species level, the embedding distribution of different species aligns
closely with functional and ecological meanings (e.g., beak sizes and
habitats). At the intra-species level, instead of being diminished, the
intra-species variations (e.g., life stages and sexes) are preserved and better
separated in subspaces orthogonal to inter-species distinctions. We provide
formal proof and analyses to explain why hierarchical supervision and
contrastive objectives encourage these emergent properties. Crucially, our
results reveal that these properties become increasingly significant with
larger-scale training data, leading to a biologically meaningful embedding
space.

</details>


### [381] [ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding](https://arxiv.org/abs/2505.23922)
*David Ma,Huaqing Yuan,Xingjian Wang,Qianbo Zang,Tianci Liu,Xinyang He,Yanbin Wei,Jiawei Guo,Ni Jiahui,Zhenzhu Yang,Meng Cao,Shanghaoran Quan,Yizhi Li,Wangchunshu Zhou,Jiaheng Liu,Wenhao Huang,Ge Zhang,Shiwen Ni,Xiaojie Jin*

Key words: 多模态语言模型、长视频理解、时间尺度、基准测试、视觉token容量

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: ScaleLong是一个新的长视频理解基准测试，通过在同一视频内容中嵌入针对四个时间尺度的问题，直接比较模型在不同时间尺度上的性能。

Motivation: 现有基准测试忽视了多尺度设计或分散了针对不同时间尺度的问题，无法直接在同一视频内容上比较模型性能。

Method: 引入ScaleLong基准测试，包含269个长视频，每个视频设计4-8个问题，覆盖秒、十秒、分钟和小时四个时间尺度。

Result: 评估23个多模态语言模型显示出U形性能曲线，极端时间尺度表现较好，中间尺度表现较差；增加视觉token容量可提升所有时间尺度的推理能力。

Conclusion: ScaleLong为推进长视频理解提供细粒度、多时间尺度的基准测试。

Abstract: Although long-video understanding demands that models capture hierarchical
temporal information -- from clip (seconds) and shot (tens of seconds) to event
(minutes) and story (hours) -- existing benchmarks either neglect this
multi-scale design or scatter scale-specific questions across different videos,
preventing direct comparison of model performance across timescales on the same
content. To address this, we introduce ScaleLong, the first benchmark to
disentangle these factors by embedding questions targeting four hierarchical
timescales -- clip (seconds), shot (tens of seconds), event (minutes), and
story (hours) -- all within the same video content. This within-content
multi-timescale questioning design enables direct comparison of model
performance across timescales on identical videos. ScaleLong features 269 long
videos (avg.\ 86\,min) from 5 main categories and 36 sub-categories, with 4--8
carefully designed questions, including at least one question for each
timescale. Evaluating 23 MLLMs reveals a U-shaped performance curve, with
higher accuracy at the shortest and longest timescales and a dip at
intermediate levels. Furthermore, ablation studies show that increased visual
token capacity consistently enhances reasoning across all timescales. ScaleLong
offers a fine-grained, multi-timescale benchmark for advancing MLLM
capabilities in long-video understanding. The code and dataset are available
https://github.com/multimodal-art-projection/ScaleLong.

</details>


### [382] [Deformable Attention Mechanisms Applied to Object Detection, case of Remote Sensing](https://arxiv.org/abs/2505.24489)
*Anasse Boutayeb,Iyad Lahsen-cherif,Ahmed El Khadimi*

Key words: 物体检测, Deformable-DETR, 遥感图像, 光学, SAR, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种基于Deformable-DETR模型的物体检测方法，适用于光学和SAR遥感图像。通过对两种数据集的实验验证，该模型表现优异，F1分数分别达到95.12%和94.54%。

Motivation: 遥感图像在物体检测任务中具有独特的地理覆盖和对象一致性。深度学习模型，尤其是变形注意力机制的Transformer架构，在该领域具有潜在优势。

Method: 使用Deformable-DETR模型，并应用于光学（Pleiades Aircraft数据集）和SAR（SSDD数据集）遥感图像，通过10折分层验证进行评估。

Result: 模型在光学数据集和SAR数据集上的F1分数分别为95.12%和94.54%，超越了基于CNN和Transformer的其他模型。

Conclusion: Deformable-DETR模型在遥感图像物体检测任务中表现优异，尤其是在复杂场景下的光学和SAR数据上。

Abstract: Object detection has recently seen an interesting trend in terms of the most
innovative research work, this task being of particular importance in the field
of remote sensing, given the consistency of these images in terms of
geographical coverage and the objects present. Furthermore, Deep Learning (DL)
models, in particular those based on Transformers, are especially relevant for
visual computing tasks in general, and target detection in particular. Thus,
the present work proposes an application of Deformable-DETR model, a specific
architecture using deformable attention mechanisms, on remote sensing images in
two different modes, especially optical and Synthetic Aperture Radar (SAR). To
achieve this objective, two datasets are used, one optical, which is Pleiades
Aircraft dataset, and the other SAR, in particular SAR Ship Detection Dataset
(SSDD). The results of a 10-fold stratified validation showed that the proposed
model performed particularly well, obtaining an F1 score of 95.12% for the
optical dataset and 94.54% for SSDD, while comparing these results with several
models detections, especially those based on CNNs and transformers, as well as
those specifically designed to detect different object classes in remote
sensing images.

</details>


### [383] [DeepTopoNet: A Framework for Subglacial Topography Estimation on the Greenland Ice Sheets](https://arxiv.org/abs/2505.23980)
*Bayu Adhi Tama,Mansa Krishna,Homayra Alam,Mostafa Cham,Omar Faruque,Gong Cheng,Jianwu Wang,Mathieu Morlighem,Vandana Janeja*

Key words: 格陵兰冰盖,冰下地形,深度学习,动态损失平衡,CNN

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: DeepTopoNet，一种深度学习框架，通过动态损失平衡机制整合雷达和BedMachine数据，用于重建格陵兰冰下地形，填补观测空白并提高预测精度。

Motivation: 格陵兰冰下地形的了解对预测未来冰盖质量损失及其对全球海平面上升的贡献至关重要，但传统雷达观测数据稀疏且复杂，导致模型预测不确定性增加。

Method: 提出DeepTopoNet，结合雷达冰厚观测与BedMachine数据，通过动态损失平衡机制自适应调整权重，并利用梯度与趋势表面特征增强CNN架构性能。

Result: 在Upernavik Isstrøm区域测试中，模型表现优异，优于基线方法，能高精度重建冰下地形。

Conclusion: 深度学习有望填补观测空白，为冰下地形推断提供可扩展的高效解决方案。

Abstract: Understanding Greenland's subglacial topography is critical for projecting
the future mass loss of the ice sheet and its contribution to global sea-level
rise. However, the complex and sparse nature of observational data,
particularly information about the bed topography under the ice sheet,
significantly increases the uncertainty in model projections. Bed topography is
traditionally measured by airborne ice-penetrating radar that measures the ice
thickness directly underneath the aircraft, leaving data gap of tens of
kilometers in between flight lines. This study introduces a deep learning
framework, which we call as DeepTopoNet, that integrates radar-derived ice
thickness observations and BedMachine Greenland data through a novel dynamic
loss-balancing mechanism. Among all efforts to reconstruct bed topography,
BedMachine has emerged as one of the most widely used datasets, combining mass
conservation principles and ice thickness measurements to generate
high-resolution bed elevation estimates. The proposed loss function adaptively
adjusts the weighting between radar and BedMachine data, ensuring robustness in
areas with limited radar coverage while leveraging the high spatial resolution
of BedMachine predictions i.e. bed estimates. Our approach incorporates
gradient-based and trend surface features to enhance model performance and
utilizes a CNN architecture designed for subgrid-scale predictions. By
systematically testing on the Upernavik Isstr{\o}m) region, the model achieves
high accuracy, outperforming baseline methods in reconstructing subglacial
terrain. This work demonstrates the potential of deep learning in bridging
observational gaps, providing a scalable and efficient solution to inferring
subglacial topography.

</details>


### [384] [Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts](https://arxiv.org/abs/2505.24541)
*Xin He,Xumeng Han,Longhui Wei,Lingxi Xie,Qi Tian*

Key words: 多模态大语言模型，多专家混合，动态路由，任务冲突，联合学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: Mixpert是一种高效的多视觉专家混合架构，通过动态路由机制和任务特定微调解决了MLLM中单一视觉编码器处理多任务的冲突，性能提升显著且计算成本低。

Motivation: 单一视觉编码器在多任务学习中难以处理多样化视觉场景，导致任务冲突，而直接整合多个编码器又增加复杂性和限制联合优化潜能。

Method: 提出Mixpert，采用多专家混合架构，结合动态路由机制，为不同任务分配合适的视觉专家，同时保持联合学习优势。

Result: Mixpert有效缓解任务冲突，显著提升多种任务性能，且额外计算成本极低。

Conclusion: Mixpert是一种高效可扩展的解决方案，适用于多模态大模型的多任务视觉处理。

Abstract: Multimodal large language models (MLLMs) require a nuanced interpretation of
complex image information, typically leveraging a vision encoder to perceive
various visual scenarios. However, relying solely on a single vision encoder to
handle diverse task domains proves difficult and inevitably leads to conflicts.
Recent work enhances data perception by directly integrating multiple
domain-specific vision encoders, yet this structure adds complexity and limits
the potential for joint optimization. In this paper, we introduce Mixpert, an
efficient mixture-of-vision-experts architecture that inherits the joint
learning advantages from a single vision encoder while being restructured into
a multi-expert paradigm for task-specific fine-tuning across different visual
tasks. Additionally, we design a dynamic routing mechanism that allocates input
images to the most suitable visual expert. Mixpert effectively alleviates
domain conflicts encountered by a single vision encoder in multi-task learning
with minimal additional computational cost, making it more efficient than
multiple encoders. Furthermore, Mixpert integrates seamlessly into any MLLM,
with experimental results demonstrating substantial performance gains across
various tasks.

</details>


### [385] [GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models](https://arxiv.org/abs/2505.24340)
*Gilles Quentin Hacheme,Girmaw Abebe Tadesse,Caleb Robinson,Akram Zaytar,Rahul Dodhia,Juan M. Lavista Ferres*

Key words: 零样本分类, 地理空间图像, vLLM, LLM, 递归聚类, 分层分类

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: GVL是一个零样本分类框架，结合视觉大语言模型和传统大语言模型，在地理空间图像分类任务中表现优异。

Motivation: 解决地理空间图像分类中标注数据稀缺的问题，提供灵活的零样本分类方案。

Method: 使用视觉大语言模型生成图像描述，再通过传统大语言模型映射到用户定义的类别；在复杂任务中采用递归聚类和分层分类策略。

Result: 在SpaceNet v7上达到93.2%的零样本准确率，在UC Merced和RESISC45上也表现良好。

Conclusion: GVL是一个模块化、可解释的零样本分类框架，适用于广泛的地理空间应用。

Abstract: Classifying geospatial imagery remains a major bottleneck for applications
such as disaster response and land-use monitoring-particularly in regions where
annotated data is scarce or unavailable. Existing tools (e.g., RS-CLIP) that
claim zero-shot classification capabilities for satellite imagery nonetheless
rely on task-specific pretraining and adaptation to reach competitive
performance. We introduce GeoVision Labeler (GVL), a strictly zero-shot
classification framework: a vision Large Language Model (vLLM) generates rich,
human-readable image descriptions, which are then mapped to user-defined
classes by a conventional Large Language Model (LLM). This modular, and
interpretable pipeline enables flexible image classification for a large range
of use cases. We evaluated GVL across three benchmarks-SpaceNet v7, UC Merced,
and RESISC45. It achieves up to 93.2% zero-shot accuracy on the binary
Buildings vs. No Buildings task on SpaceNet v7. For complex multi-class
classification tasks (UC Merced, RESISC45), we implemented a recursive
LLM-driven clustering to form meta-classes at successive depths, followed by
hierarchical classification-first resolving coarse groups, then finer
distinctions-to deliver competitive zero-shot performance. GVL is open-sourced
at https://github.com/microsoft/geo-vision-labeler to catalyze adoption in
real-world geospatial workflows.

</details>


### [386] [KEVER^2: Knowledge-Enhanced Visual Emotion Reasoning and Retrieval](https://arxiv.org/abs/2505.24342)
*Fanhang Man,Xiaoyue Chen,Huandong Wang,Baining Zhao,Han Li,Xinlei Chen,Yong Li*

Key words: 视觉情感分析、知识增强、多模态对齐、情感推理

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: K-EVER²，一种知识增强的情感推理与检索框架，通过多模态对齐集成外部情感知识，显著提升了视觉情感分析的准确性和可解释性。

Motivation: 现有的视觉语言模型在分析图像情感时面临情感线索抽象、模糊及缺乏监督等问题，需引入结构化情感知识以提高分析的鲁棒性。

Method: 提出K-EVER²，结合语义结构化的视觉情感线索和多模态对齐的外部情感知识，无需人工标注即可实现情感预测。

Result: 在三个基准数据集上平均准确率提升12.3%，特定情感类别最高提升19%。

Conclusion: K-EVER²为视觉内容的情感理解提供了可扩展且通用的解决方案。

Abstract: Understanding what emotions images evoke in their viewers is a foundational
goal in human-centric visual computing. While recent advances in
vision-language models (VLMs) have shown promise for visual emotion analysis
(VEA), several key challenges remain unresolved. Emotional cues in images are
often abstract, overlapping, and entangled, making them difficult to model and
interpret. Moreover, VLMs struggle to align these complex visual patterns with
emotional semantics due to limited supervision and sparse emotional grounding.
Finally, existing approaches lack structured affective knowledge to resolve
ambiguity and ensure consistent emotional reasoning across diverse visual
domains.
  To address these limitations, we propose \textbf{K-EVER\textsuperscript{2}},
a knowledge-enhanced framework for emotion reasoning and retrieval. Our
approach introduces a semantically structured formulation of visual emotion
cues and integrates external affective knowledge through multimodal alignment.
Without relying on handcrafted labels or direct emotion supervision,
K-EVER\textsuperscript{2} achieves robust and interpretable emotion predictions
across heterogeneous image types.
  We validate our framework on three representative benchmarks, Emotion6,
EmoSet, and M-Disaster, covering social media imagery, human-centric scenes,
and disaster contexts. K-EVER\textsuperscript{2} consistently outperforms
strong CNN and VLM baselines, achieving up to a \textbf{19\% accuracy gain} for
specific emotions and a \textbf{12.3\% average accuracy gain} across all
emotion categories. Our results demonstrate a scalable and generalizable
solution for advancing emotional understanding of visual content.

</details>


### [387] [Federated Foundation Model for GI Endoscopy Images](https://arxiv.org/abs/2505.24108)
*Alina Devkota,Annahita Amireskandari,Joel Palko,Shyam Thakkar,Donald Adjeroh,Xiajun Jiang,Binod Bhattarai,Prashnna K. Gyawali*

Key words: 胃镜成像, 基础模型, 联邦学习, 隐私保护, 医学数据

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种联邦学习框架，用于训练胃镜成像的基础模型，解决了医学数据隐私限制的问题。

Motivation: 胃镜成像在早期疾病检测中至关重要，但深度学习的成功依赖于昂贵的标注数据。基础模型可以解决数据稀缺问题，但医学数据的隐私限制使得传统训练方法不可行。

Method: 提出了一种联邦学习框架，允许医院在不共享数据的情况下共同训练基础模型。研究了多种联邦学习算法，验证了其在同质和异质环境中的适用性。

Result: 训练后的基础模型在分类、检测和分割三个关键下游任务中均表现出性能提升。

Conclusion: 该研究证明联邦学习框架在隐私保护的胃镜成像基础模型训练中具有高效性。

Abstract: Gastrointestinal (GI) endoscopy is essential in identifying GI tract
abnormalities in order to detect diseases in their early stages and improve
patient outcomes. Although deep learning has shown success in supporting GI
diagnostics and decision-making, these models require curated datasets with
labels that are expensive to acquire. Foundation models offer a promising
solution by learning general-purpose representations, which can be finetuned
for specific tasks, overcoming data scarcity. Developing foundation models for
medical imaging holds significant potential, but the sensitive and protected
nature of medical data presents unique challenges. Foundation model training
typically requires extensive datasets, and while hospitals generate large
volumes of data, privacy restrictions prevent direct data sharing, making
foundation model training infeasible in most scenarios. In this work, we
propose a FL framework for training foundation models for gastroendoscopy
imaging, enabling data to remain within local hospital environments while
contributing to a shared model. We explore several established FL algorithms,
assessing their suitability for training foundation models without relying on
task-specific labels, conducting experiments in both homogeneous and
heterogeneous settings. We evaluate the trained foundation model on three
critical downstream tasks--classification, detection, and segmentation--and
demonstrate that it achieves improved performance across all tasks,
highlighting the effectiveness of our approach in a federated,
privacy-preserving setting.

</details>


### [388] [AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders](https://arxiv.org/abs/2505.24519)
*Yuqi Zhang,Yuchun Miao,Zuchao Li,Liang Ding*

Key words: AMIA, 视觉语言模型, 防御, 对抗攻击, 意图分析

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: AMIA是一种轻量级防御方法，通过自动屏蔽与文本无关的图像块和联合意图分析，有效提升大型视觉语言模型的安全性。

Motivation: 提高大型视觉语言模型对抗攻击的防御能力，同时保持模型的通用性能。

Method: 自动屏蔽不相关图像块并进行联合意图分析，无需重新训练模型。

Result: 防御成功率从52.4%提升至81.7%，通用性能仅下降2%，推理开销较低。

Conclusion: 屏蔽和意图分析是保障安全性和通用性平衡的关键。

Abstract: We introduce AMIA, a lightweight, inference-only defense for Large
Vision-Language Models (LVLMs) that (1) Automatically Masks a small set of
text-irrelevant image patches to disrupt adversarial perturbations, and (2)
conducts joint Intention Analysis to uncover and mitigate hidden harmful
intents before response generation. Without any retraining, AMIA improves
defense success rates across diverse LVLMs and jailbreak benchmarks from an
average of 52.4% to 81.7%, preserves general utility with only a 2% average
accuracy drop, and incurs only modest inference overhead. Ablation confirms
both masking and intention analysis are essential for a robust safety-utility
trade-off.

</details>


### [389] [Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors](https://arxiv.org/abs/2505.24625)
*Duo Zheng,Shijia Huang,Yanyang Li,Liwei Wang*

Key words: MLLM, 3D场景理解, 视频数据, VG LLM, 空间推理

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 研究提出了一种名为VG LLM的创新方法，通过视频数据直接理解和推理3D空间，无需额外3D输入，取得了显著效果。

Motivation: 现有方法依赖全面的3D数据输入（如点云或BEV地图），但需更高效的方式直接从视频理解3D场景。

Method: 采用3D视觉几何编码器从视频序列中提取3D先验信息，并结合视觉标记输入MLLM。

Result: 在多项3D场景理解和空间推理任务中表现优异，4B模型在VSI-Bench评估中超越Gemini-1.5-Pro。

Conclusion: VG LLM展示了直接从视频学习3D理解的潜力，效果媲美甚至超越依赖显式3D数据的方法。

Abstract: Previous research has investigated the application of Multimodal Large
Language Models (MLLMs) in understanding 3D scenes by interpreting them as
videos. These approaches generally depend on comprehensive 3D data inputs, such
as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,
we advance this field by enhancing the capability of MLLMs to understand and
reason in 3D spaces directly from video data, without the need for additional
3D input. We propose a novel and efficient method, the Video-3D Geometry Large
Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that
extracts 3D prior information from video sequences. This information is
integrated with visual tokens and fed into the MLLM. Extensive experiments have
shown that our method has achieved substantial improvements in various tasks
related to 3D scene understanding and spatial reasoning, all directly learned
from video sources. Impressively, our 4B model, which does not rely on explicit
3D data inputs, achieves competitive results compared to existing
state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the
VSI-Bench evaluations.

</details>


### [390] [Cloud Optical Thickness Retrievals Using Angle Invariant Attention Based Deep Learning Models](https://arxiv.org/abs/2505.24638)
*Zahid Hassan Tushar,Adeleke Ademakinwa,Jianwu Wang,Zhibo Zhang,Sanjay Purushotham*

Key words: 云光学厚度, 深度学习, 注意力机制, 角度编码

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种名为CAAC的新型角度不变、基于注意力的深度学习模型，用于更准确地估计云光学厚度（COT），显著优于现有方法。

Motivation: 传统独立像素近似（IPA）方法在估计COT时存在显著偏差，而现有深度学习模型对辐射强度、畸变和云阴影的变化敏感，导致误差较大。

Method: 提出Cloud-Attention-Net with Angle Coding（CAAC）模型，利用注意力机制和角度嵌入来考虑卫星视角几何和3D辐射传输效应，并通过多角度训练策略实现角度不变性。

Result: 实验表明，CAAC显著优于现有最先进的深度学习模型，将云属性检索误差至少减少了九倍。

Conclusion: CAAC模型通过结合注意力机制和角度编码，实现了更准确且鲁棒的COT估计。

Abstract: Cloud Optical Thickness (COT) is a critical cloud property influencing
Earth's climate, weather, and radiation budget. Satellite radiance measurements
enable global COT retrieval, but challenges like 3D cloud effects, viewing
angles, and atmospheric interference must be addressed to ensure accurate
estimation. Traditionally, the Independent Pixel Approximation (IPA) method,
which treats individual pixels independently, has been used for COT estimation.
However, IPA introduces significant bias due to its simplified assumptions.
Recently, deep learning-based models have shown improved performance over IPA
but lack robustness, as they are sensitive to variations in radiance intensity,
distortions, and cloud shadows. These models also introduce substantial errors
in COT estimation under different solar and viewing zenith angles. To address
these challenges, we propose a novel angle-invariant, attention-based deep
model called Cloud-Attention-Net with Angle Coding (CAAC). Our model leverages
attention mechanisms and angle embeddings to account for satellite viewing
geometry and 3D radiative transfer effects, enabling more accurate retrieval of
COT. Additionally, our multi-angle training strategy ensures angle invariance.
Through comprehensive experiments, we demonstrate that CAAC significantly
outperforms existing state-of-the-art deep learning models, reducing cloud
property retrieval errors by at least a factor of nine.

</details>


### [391] [Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin](https://arxiv.org/abs/2505.24222)
*Fangyikang Wang,Hubery Yin,Lei Qian,Yinan Li,Shaobin Zhuang,Huminhao Zhu,Yilin Zhang,Yanlong Tang,Chao Zhang,Hanbin Zhao,Hui Qian,Chen Li*

Key words: 扩散模型, Hessian几何, Langevin动力学, Levenberg-Marquardt, 图像生成

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种新方法Levenberg-Marquardt-Langevin (LML)，通过低秩近似和阻尼机制高效利用扩散Hessian几何，显著提升图像生成质量。

Motivation: 当前扩散模型采样方法主要依赖一阶Langevin动力学，计算Hessian几何的成本高不可扩展，需更高效的方法。

Method: LML方法利用低秩近似和阻尼机制，无训练地逼近扩散Hessian几何，避免显式计算二阶复杂度。

Result: 实验表明，LML在多预训练扩散模型中显著提升生成质量，计算开销可忽略。

Conclusion: LML通过有效逼近Hessian几何为扩散采样提供了更精确的步骤，是高质量图像生成的可行方案。

Abstract: The diffusion models (DMs) have demonstrated the remarkable capability of
generating images via learning the noised score function of data distribution.
Current DM sampling techniques typically rely on first-order Langevin dynamics
at each noise level, with efforts concentrated on refining inter-level
denoising strategies. While leveraging additional second-order Hessian geometry
to enhance the sampling quality of Langevin is a common practice in Markov
chain Monte Carlo (MCMC), the naive attempts to utilize Hessian geometry in
high-dimensional DMs lead to quadratic-complexity computational costs,
rendering them non-scalable. In this work, we introduce a novel
Levenberg-Marquardt-Langevin (LML) method that approximates the diffusion
Hessian geometry in a training-free manner, drawing inspiration from the
celebrated Levenberg-Marquardt optimization algorithm. Our approach introduces
two key innovations: (1) A low-rank approximation of the diffusion Hessian,
leveraging the DMs' inherent structure and circumventing explicit
quadratic-complexity computations; (2) A damping mechanism to stabilize the
approximated Hessian. This LML approximated Hessian geometry enables the
diffusion sampling to execute more accurate steps and improve the image
generation quality. We further conduct a theoretical analysis to substantiate
the approximation error bound of low-rank approximation and the convergence
property of the damping mechanism. Extensive experiments across multiple
pretrained DMs validate that the LML method significantly improves image
generation quality, with negligible computational overhead.

</details>


### [392] [BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models](https://arxiv.org/abs/2505.24649)
*Huu-Thien Tran,Thanh-Dat Truong,Khoa Luu*

Key words: 视觉语言模型, 幻觉缓解, 双射学习, 标准化流, BIMA

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种名为BIMA的双射最大似然学习方法，通过标准化流理论减少视觉语言模型中的幻觉问题，显著提升了性能。

Motivation: 大型视觉语言模型在多个领域得到广泛应用，但其不可解释性和幻觉问题（模型生成与视觉内容不符的响应）带来了信任挑战。

Method: 采用双射最大似然学习（BIMA）方法，基于标准化流理论优化解码过程以减少幻觉。

Result: BIMA在POPE基准上达到85.06%的平均F1分数，并在CHAIRS和CHAIRI指标上分别降低了7.6%和2.6%。

Conclusion: BIMA是首批通过双射方法减少大型视觉语言模型幻觉的研究之一，效果显著。

Abstract: Large vision-language models have become widely adopted to advance in various
domains. However, developing a trustworthy system with minimal interpretable
characteristics of large-scale models presents a significant challenge. One of
the most prevalent terms associated with the fallacy functions caused by these
systems is hallucination, where the language model generates a response that
does not correspond to the visual content. To mitigate this problem, several
approaches have been developed, and one prominent direction is to ameliorate
the decoding process. In this paper, we propose a new Bijective Maximum
Likelihood Learning (BIMA) approach to hallucination mitigation using
normalizing flow theories. The proposed BIMA method can efficiently mitigate
the hallucination problem in prevailing vision-language models, resulting in
significant improvements. Notably, BIMA achieves the average F1 score of 85.06%
on POPE benchmark and remarkably reduce CHAIRS and CHAIRI by 7.6% and 2.6%,
respectively. To the best of our knowledge, this is one of the first studies
that contemplates the bijection means to reduce hallucination induced by large
vision-language models.

</details>


### [393] [MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM](https://arxiv.org/abs/2505.24238)
*Bowen Dong,Minheng Ni,Zitong Huang,Guanglei Yang,Wangmeng Zuo,Lei Zhang*

Key words: 多模态大语言模型, 幻觉, 基准测试, 视觉推理, 逻辑幻觉

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文研究了多模态大语言模型（MLLMs）中的多模态幻觉问题，并提出了一种新的基准测试（{\dataset}）来区分感知诱导和推理诱导的幻觉。研究还揭示了模型规模、数据规模和训练阶段对幻觉的影响，并提出了一种新方法（{\method}）来减少逻辑幻觉。

Motivation: 现有的基准测试无法有效区分感知诱导和推理诱导的幻觉，这限制了MLLMs的诊断和改进。为了解决这一问题，论文提出了一个新的基准测试和方法。

Method: 论文提出了{\dataset}基准测试，通过构建输入图像被正确感知但推理错误持续存在的问题来隔离推理幻觉。同时提出了{\method}方法，结合课程强化微调和协作提示推理来减少逻辑幻觉。

Result: 研究发现模型规模、数据规模和训练阶段显著影响幻觉程度，且当前MLLMs在空间关系的视觉推理能力有限。{\method}方法在基准测试上建立了基线，并减少了原始模型的逻辑幻觉。

Conclusion: 论文提出了一个新的基准测试和方法，为MLLMs的幻觉问题提供了诊断和缓解策略，揭示了现有模型的局限性。

Abstract: Multimodal hallucination in multimodal large language models (MLLMs)
restricts the correctness of MLLMs. However, multimodal hallucinations are
multi-sourced and arise from diverse causes. Existing benchmarks fail to
adequately distinguish between perception-induced hallucinations and
reasoning-induced hallucinations. This failure constitutes a significant issue
and hinders the diagnosis of multimodal reasoning failures within MLLMs. To
address this, we propose the {\dataset} benchmark, which isolates reasoning
hallucinations by constructing questions where input images are correctly
perceived by MLLMs yet reasoning errors persist. {\dataset} introduces
multi-granular evaluation metrics: accuracy, factuality, and LLMs hallucination
score for hallucination quantification. Our analysis reveals that (1) the model
scale, data scale, and training stages significantly affect the degree of
logical, fabrication, and factual hallucinations; (2) current MLLMs show no
effective improvement on spatial hallucinations caused by misinterpreted
spatial relationships, indicating their limited visual reasoning capabilities;
and (3) question types correlate with distinct hallucination patterns,
highlighting targeted challenges and potential mitigation strategies. To
address these challenges, we propose {\method}, a method that combines
curriculum reinforcement fine-tuning to encourage models to generate
logic-consistent reasoning chains by stepwise reducing learning difficulty, and
collaborative hint inference to reduce reasoning complexity. {\method}
establishes a baseline on {\dataset}, and reduces the logical hallucinations in
original base models.

</details>


### [394] [Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation](https://arxiv.org/abs/2505.24787)
*Yucheng Zhou,Jiahao Yuan,Qianning Wang*

Key words: 文本到图像生成、复杂指令、评估基准、大语言模型、自动化评估

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了LongBench-T2I，一个专门评估文本到图像生成模型在复杂指令下表现的基准，并提出了Plan2Gen框架来提升生成效果，无需额外训练。

Motivation: 当前T2I模型在复杂指令（多对象、属性和空间关系）下表现不足，现有基准和指标也无法全面评估这些需求。

Method: 设计了包含500个复杂提示的LongBench-T2I基准，并提出Plan2Gen框架，利用大语言模型分解复杂指令以指导图像生成。

Result: 提出了自动化评估工具包，支持多维度评估生成图像质量，基准和框架显著提升了复杂指令下的生成效果。

Conclusion: LongBench-T2I填补了复杂指令评估的空白，Plan2Gen框架为复杂图像生成提供了可扩展的解决方案。

Abstract: Recent advancements in text-to-image (T2I) generation have enabled models to
produce high-quality images from textual descriptions. However, these models
often struggle with complex instructions involving multiple objects,
attributes, and spatial relationships. Existing benchmarks for evaluating T2I
models primarily focus on general text-image alignment and fail to capture the
nuanced requirements of complex, multi-faceted prompts. Given this gap, we
introduce LongBench-T2I, a comprehensive benchmark specifically designed to
evaluate T2I models under complex instructions. LongBench-T2I consists of 500
intricately designed prompts spanning nine diverse visual evaluation
dimensions, enabling a thorough assessment of a model's ability to follow
complex instructions. Beyond benchmarking, we propose an agent framework
(Plan2Gen) that facilitates complex instruction-driven image generation without
requiring additional model training. This framework integrates seamlessly with
existing T2I models, using large language models to interpret and decompose
complex prompts, thereby guiding the generation process more effectively. As
existing evaluation metrics, such as CLIPScore, fail to adequately capture the
nuances of complex instructions, we introduce an evaluation toolkit that
automates the quality assessment of generated images using a set of
multi-dimensional metrics. The data and code are released at
https://github.com/yczhou001/LongBench-T2I.

</details>


### [395] [Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck](https://arxiv.org/abs/2505.24840)
*Yuwen Tan,Yuan Qing,Boqing Gong*

Key words: 大语言模型, 视觉问答, 层次知识, 分类学, 视觉理解

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文通过构建约100万个四项选择题的视觉问答任务，揭示了当前大语言模型（LLMs）在视觉世界的层次知识上存在不足，影响了视觉LLMs的层次理解能力。

Motivation: 研究发现LLMs缺乏对视觉世界的层次知识（如生物学分类），成为视觉LLMs层次理解的瓶颈，因此探索如何通过VQA任务提升其能力。

Method: 使用六个分类体系和四个图像数据集构建约100万个四项选择题的视觉问答任务，并通过对视觉LLMs进行微调来评估改进效果。

Result: 实验表明，VQA任务对LLMs的层次一致性提升效果优于对视觉LLMs的提升，验证了LLMs的瓶颈效应。

Conclusion: 作者认为，若LLMs不具备相应的分类知识，视觉LLMs将无法完全实现层次化的视觉概念理解。

Abstract: This paper reveals that many state-of-the-art large language models (LLMs)
lack hierarchical knowledge about our visual world, unaware of even
well-established biology taxonomies. This shortcoming makes LLMs a bottleneck
for vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone
Fish but not Vertebrate). We arrive at these findings using about one million
four-choice visual question answering (VQA) tasks constructed from six
taxonomies and four image datasets. Interestingly, finetuning a vision LLM
using our VQA tasks reaffirms LLMs' bottleneck effect to some extent because
the VQA tasks improve the LLM's hierarchical consistency more than the vision
LLM's. We conjecture that one cannot make vision LLMs understand visual
concepts fully hierarchical until LLMs possess corresponding taxonomy
knowledge.

</details>


### [396] [STAR-Net: An Interpretable Model-Aided Network for Remote Sensing Image Denoising](https://arxiv.org/abs/2505.24327)
*Jingjing Liu,Jiashun Jin,Xianchao Xiu,Jianhua Zhang,Wanquan Liu*

Key words: 遥感图像去噪, 低秩先验, 非局部自相似性, ADMM, 深度展开网络

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种名为STAR-Net的新型遥感图像去噪方法，结合低秩先验和非局部自相似性，并通过ADMM引导的深度展开网络自动学习参数，显著提升了去噪性能。

Motivation: 当前基于深度学习的遥感图像去噪方法缺乏与物理信息模型的结合，可解释性差，且对非局部自相似性的关注不足。论文旨在解决这些问题，并提出更高效、稳健的方法。

Method: 提出稀疏张量辅助表示网络（STAR-Net），利用低秩先验捕捉非局部自相似性，并扩展为稀疏版本STAR-Net-S以应对非高斯噪声。采用ADMM引导的深度展开网络自动学习正则化参数。

Result: 在合成和真实数据集上的实验表明，STAR-Net和STAR-Net-S优于当前最先进的遥感图像去噪方法。

Conclusion: STAR-Net结合了模型驱动和数据驱动的优势，显著提升了去噪性能和稳健性，解决了现有方法的不足。

Abstract: Remote sensing image (RSI) denoising is an important topic in the field of
remote sensing. Despite the impressive denoising performance of RSI denoising
methods, most current deep learning-based approaches function as black boxes
and lack integration with physical information models, leading to limited
interpretability. Additionally, many methods may struggle with insufficient
attention to non-local self-similarity in RSI and require tedious tuning of
regularization parameters to achieve optimal performance, particularly in
conventional iterative optimization approaches. In this paper, we first propose
a novel RSI denoising method named sparse tensor-aided representation network
(STAR-Net), which leverages a low-rank prior to effectively capture the
non-local self-similarity within RSI. Furthermore, we extend STAR-Net to a
sparse variant called STAR-Net-S to deal with the interference caused by
non-Gaussian noise in original RSI for the purpose of improving robustness.
Different from conventional iterative optimization, we develop an alternating
direction method of multipliers (ADMM)-guided deep unrolling network, in which
all regularization parameters can be automatically learned, thus inheriting the
advantages of both model-based and deep learning-based approaches and
successfully addressing the above-mentioned shortcomings. Comprehensive
experiments on synthetic and real-world datasets demonstrate that STAR-Net and
STAR-Net-S outperform state-of-the-art RSI denoising methods.

</details>


### [397] [MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning](https://arxiv.org/abs/2505.24871)
*Yiqing Liang,Jielin Qiu,Wenhao Ding,Zuxin Liu,James Tompkin,Mengdi Xu,Mengzhou Xia,Zhengzhong Tu,Laixi Shi,Jiacheng Zhu*

Key words: 多模态大语言模型, 强化学习可验证奖励, 数据集混合策略, 后训练, 推理能力

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种针对多模态大语言模型（MLLMs）的系统性后训练框架，通过优化数据集混合策略和强化学习可验证奖励（RLVR），显著提升了模型的泛化与推理能力。

Motivation: 现有的RLVR方法在单模态任务中表现优异，但应用于多模态任务时，由于视觉语言任务的复杂性和数据集间的目标冲突，需要一种优化的数据集混合策略以提高模型性能。

Method: 1) 开发了多模态RLVR框架，支持多领域在线强化学习；2) 提出了数据混合策略，通过预测混合分布对RL微调结果的影响，优化最佳混合比例。

Result: 多领域RLVR训练与混合预测策略结合后，显著提升了MLLM的推理能力，最优混合策略在分布外基准测试中平均提升5.24%，相比预微调基线提升20.74%。

Conclusion: 系统化的后训练框架和优化的数据集混合策略是提升MLLM在多模态任务中表现的关键。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for post-training large language models (LLMs), achieving
state-of-the-art performance on tasks with structured, verifiable answers.
Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but
is complicated by the broader, heterogeneous nature of vision-language tasks
that demand nuanced visual, logical, and spatial capabilities. As such,
training MLLMs using RLVR on multiple datasets could be beneficial but creates
challenges with conflicting objectives from interaction among diverse datasets,
highlighting the need for optimal dataset mixture strategies to improve
generalization and reasoning. We introduce a systematic post-training framework
for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation
and benchmark implementation. Specifically, (1) We developed a multimodal RLVR
framework for multi-dataset post-training by curating a dataset that contains
different verifiable vision-language problems and enabling multi-domain online
RL learning with different verifiable rewards; (2) We proposed a data mixture
strategy that learns to predict the RL fine-tuning outcome from the data
mixture distribution, and consequently optimizes the best mixture.
Comprehensive experiments showcase that multi-domain RLVR training, when
combined with mixture prediction strategies, can significantly boost MLLM
general reasoning capacities. Our best mixture improves the post-trained
model's accuracy on out-of-distribution benchmarks by an average of 5.24%
compared to the same model post-trained with uniform data mixture, and by a
total of 20.74% compared to the pre-finetuning baseline.

</details>


### [398] [ProxyThinker: Test-Time Guidance through Small Visual Reasoners](https://arxiv.org/abs/2505.24872)
*Zilin Xiao,Jaywon Koo,Siru Ouyang,Jefferson Hernandez,Yu Meng,Vicente Ordonez*

Key words: ProxyThinker, 视觉推理, 大型视觉语言模型, 推理时技术, 解码动态

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: ProxyThinker是一种无需训练的推理时技术，通过修改基础模型的解码动态，使其具备视觉推理能力，性能提升显著，推理速度更快。

Motivation: 训练大型视觉语言模型（LVLM）时，强化微调（RFT）计算成本高，难以扩展模型规模。本文旨在通过推理时技术解决这一问题。

Method: 提出ProxyThinker，通过从RFT推理模型中减去基础模型的输出分布来修改解码动态，继承小模型的视觉推理能力。

Result: 在空间、数学和多学科推理等视觉基准测试中表现优异，推理速度提升38倍。

Conclusion: ProxyThinker成功实现无需训练的推理时能力提升，为实际部署提供可能。

Abstract: Recent advancements in reinforcement learning with verifiable rewards have
pushed the boundaries of the visual reasoning capabilities in large
vision-language models (LVLMs). However, training LVLMs with reinforcement
fine-tuning (RFT) is computationally expensive, posing a significant challenge
to scaling model size. In this work, we propose ProxyThinker, an inference-time
technique that enables large models to inherit the visual reasoning
capabilities from small, slow-thinking visual reasoners without any training.
By subtracting the output distributions of base models from those of RFT
reasoners, ProxyThinker modifies the decoding dynamics and successfully elicits
the slow-thinking reasoning demonstrated by the emerged sophisticated behaviors
such as self-verification and self-correction. ProxyThinker consistently boosts
performance on challenging visual benchmarks on spatial, mathematical, and
multi-disciplinary reasoning, enabling untuned base models to compete with the
performance of their full-scale RFT counterparts. Furthermore, our
implementation efficiently coordinates multiple language models with
parallelism techniques and achieves up to 38 $\times$ faster inference compared
to previous decoding-time methods, paving the way for the practical deployment
of ProxyThinker. Code is available at
https://github.com/MrZilinXiao/ProxyThinker.

</details>


### [399] [ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL](https://arxiv.org/abs/2505.24875)
*Yu Zhang,Yunqi Li,Yifan Yang,Rui Wang,Yuqing Yang,Dai Qi,Jianmin Bao,Dongdong Chen,Chong Luo,Lili Qiu*

Key words: chain-of-thought, reinforcement learning, vision-language model, image generation, GRPO

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一个名为ReasonGen-R1的两阶段框架，通过监督微调和强化学习方法增强视觉生成模型；结合文本推理提升图像生成质量，并在多个基准测试中表现优异。

Motivation: 尽管Chain-of-Thought和强化学习在NLP中已取得突破，但在视觉生成模型中的应用仍较少。该文旨在解决这一问题，引入文本推理能力提升生成图像的逻辑性和可控性。

Method: 1. 使用监督微调在自动生成的理由数据集上训练自回归图像生成器。2. 采用Group Relative Policy Optimization（GRPO）算法，利用预训练视觉语言模型的奖励信号优化策略。

Result: 在GenEval、DPG和T2I等基准测试中，ReasonGen-R1显著超越基线模型和现有最佳模型。

Conclusion: ReasonGen-R1通过结合文本推理和强化学习，显著提升了视觉生成模型的逻辑性和生成质量，为未来研究提供了新方向。

Abstract: Although chain-of-thought reasoning and reinforcement learning (RL) have
driven breakthroughs in NLP, their integration into generative vision models
remains underexplored. We introduce ReasonGen-R1, a two-stage framework that
first imbues an autoregressive image generator with explicit text-based
"thinking" skills via supervised fine-tuning on a newly generated reasoning
dataset of written rationales, and then refines its outputs using Group
Relative Policy Optimization. To enable the model to reason through text before
generating images, We automatically generate and release a corpus of model
crafted rationales paired with visual prompts, enabling controlled planning of
object layouts, styles, and scene compositions. Our GRPO algorithm uses reward
signals from a pretrained vision language model to assess overall visual
quality, optimizing the policy in each update. Evaluations on GenEval, DPG, and
the T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong
baselines and prior state-of-the-art models. More: aka.ms/reasongen.

</details>


### [400] [Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks](https://arxiv.org/abs/2505.24876)
*Tajamul Ashraf,Amal Saqib,Hanan Ghani,Muhra AlMahri,Yuhao Li,Noor Ahsan,Umair Nawaz,Jean Lahoud,Hisham Cholakkal,Mubarak Shah,Philip Torr,Fahad Shahbaz Khan,Rao Muhammad Anwer,Salman Khan*

Key words: deep reasoning, vision-centric, multimodal, Agent-X, benchmarking, tool use

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: Agent-X benchmark introduced for evaluating vision-centric agents' multi-step reasoning in real-world multimodal tasks, revealing current models' limitations.

Motivation: Existing benchmarks fail to assess vision-centric agents' multi-step and deep reasoning in real-world, multimodal settings, leading to gaps in evaluating complex scenarios.

Method: Agent-X includes 828 tasks with diverse visual contexts (images, videos, etc.) across six environments, requiring stepwise reasoning and tool use. A fine-grained evaluation framework measures reasoning quality and tool effectiveness.

Result: Top models (GPT, Gemini, Qwen families) perform poorly in multi-step vision tasks, with under 50% full-chain success, exposing reasoning and tool-use bottlenecks.

Conclusion: Agent-X highlights critical challenges in vision-centric agentic reasoning, urging future research to address these limitations, with data and code accessible publicly.

Abstract: Deep reasoning is fundamental for solving complex tasks, especially in
vision-centric scenarios that demand sequential, multimodal understanding.
However, existing benchmarks typically evaluate agents with fully synthetic,
single-turn queries, limited visual modalities, and lack a framework to assess
reasoning quality over multiple steps as required in real-world settings. To
address this, we introduce Agent-X, a large-scale benchmark for evaluating
vision-centric agents multi-step and deep reasoning capabilities in real-world,
multimodal settings. Agent- X features 828 agentic tasks with authentic visual
contexts, including images, multi-image comparisons, videos, and instructional
text. These tasks span six major agentic environments: general visual
reasoning, web browsing, security and surveillance, autonomous driving, sports,
and math reasoning. Our benchmark requires agents to integrate tool use with
explicit, stepwise decision-making in these diverse settings. In addition, we
propose a fine-grained, step-level evaluation framework that assesses the
correctness and logical coherence of each reasoning step and the effectiveness
of tool usage throughout the task. Our results reveal that even the
best-performing models, including GPT, Gemini, and Qwen families, struggle to
solve multi-step vision tasks, achieving less than 50% full-chain success.
These findings highlight key bottlenecks in current LMM reasoning and tool-use
capabilities and identify future research directions in vision-centric agentic
reasoning models. Our data and code are publicly available at
https://github.com/mbzuai-oryx/Agent-X

</details>


### [401] [Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised Learning with Outliers](https://arxiv.org/abs/2505.24443)
*Heejo Kong,Sung-Jin Kim,Gunho Jung,Seong-Whan Lee*

Key words: 半监督学习, 开集识别, 异常检测, 多模型差异, DAC

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种名为DAC（Diversify and Conquer）的新框架，旨在增强开集半监督学习（SSL）的鲁棒性，通过利用多模型间的预测差异来检测异常数据。

Motivation: 传统的半监督学习假设标记和未标记数据共享相同的类别分布，但在实践中，未标记数据常包含异常类别数据（即离群值），导致模型性能下降。为解决这一问题，作者提出了DAC框架。

Method: 通过构建多个具有不同偏见的模型，并利用它们在未标记数据上的预测差异，DAC能够鲁棒地检测异常数据。关键在于通过单一训练过程生成多个不同的模型头。

Result: DAC在标记数据不足的情况下仍能有效检测异常数据，性能优于现有方法。

Conclusion: DAC为开集半监督学习提供了一种高效且鲁棒的异常检测方法。

Abstract: Conventional semi-supervised learning (SSL) ideally assumes that labeled and
unlabeled data share an identical class distribution, however in practice, this
assumption is easily violated, as unlabeled data often includes unknown class
data, i.e., outliers. The outliers are treated as noise, considerably degrading
the performance of SSL models. To address this drawback, we propose a novel
framework, Diversify and Conquer (DAC), to enhance SSL robustness in the
context of open-set semi-supervised learning. In particular, we note that
existing open-set SSL methods rely on prediction discrepancies between inliers
and outliers from a single model trained on labeled data. This approach can be
easily failed when the labeled data is insufficient, leading to performance
degradation that is worse than naive SSL that do not account for outliers. In
contrast, our approach exploits prediction disagreements among multiple models
that are differently biased towards the unlabeled distribution. By leveraging
the discrepancies arising from training on unlabeled data, our method enables
robust outlier detection even when the labeled data is underspecified. Our key
contribution is constructing a collection of differently biased models through
a single training process. By encouraging divergent heads to be differently
biased towards outliers while making consistent predictions for inliers, we
exploit the disagreement among these heads as a measure to identify unknown
concepts. Our code is available at https://github.com/heejokong/DivCon.

</details>


### [402] [Optimal Density Functions for Weighted Convolution in Learning Models](https://arxiv.org/abs/2505.24527)
*Simone Cammarasana,Giuseppe Patanè*

Key words: 加权卷积, 密度函数, 卷积神经网络, 图像去噪, 优化

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种加权卷积方法，通过最优密度函数对邻近像素的贡献进行缩放，显著提升了图像任务的准确性和性能。

Motivation: 传统卷积方法对所有邻近像素平等处理，而加权卷积通过优化密度函数，改进了卷积神经网络的近似精度。

Method: 利用最小化模型计算最优密度函数，框架中卷积核权重和密度函数分别通过随机梯度下降和DIRECT-L进行优化。

Result: 在图像去噪等任务中，加权卷积显著降低了损失（提升达53%），并提高了测试准确性，尽管执行时间增加了11%。

Conclusion: 加权卷积在多种学习模型超参数下表现稳健，未来将应用于更复杂的2D和3D图像卷积学习问题。

Abstract: The paper introduces the weighted convolution, a novel approach to the
convolution for signals defined on regular grids (e.g., 2D images) through the
application of an optimal density function to scale the contribution of
neighbouring pixels based on their distance from the central pixel. This choice
differs from the traditional uniform convolution, which treats all neighbouring
pixels equally. Our weighted convolution can be applied to convolutional neural
network problems to improve the approximation accuracy. Given a convolutional
network, we define a framework to compute the optimal density function through
a minimisation model. The framework separates the optimisation of the
convolutional kernel weights (using stochastic gradient descent) from the
optimisation of the density function (using DIRECT-L). Experimental results on
a learning model for an image-to-image task (e.g., image denoising) show that
the weighted convolution significantly reduces the loss (up to 53% improvement)
and increases the test accuracy compared to standard convolution. While this
method increases execution time by 11%, it is robust across several
hyperparameters of the learning model. Future work will apply the weighted
convolution to real-case 2D and 3D image convolutional learning problems.

</details>


### [403] [Geospatial Foundation Models to Enable Progress on Sustainable Development Goals](https://arxiv.org/abs/2505.24528)
*Pedram Ghamisi,Weikang Yu,Xiaokang Zhang,Aldino Rizaldy,Jian Wang,Chufeng Zhou,Richard Gloaguen,Gustau Camps-Valls*

Key words: 基础模型, 地理空间分析, 可持续发展目标, 基准框架, 能源效率

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文介绍了SustainFM，一个基于17项可持续发展目标的基准框架，评估地理空间基础模型（FMs）在可持续发展任务中的表现。研究发现FMs在多任务中优于传统方法，并强调评估应关注转移性、通用性和能源效率。

Motivation: 探讨地理空间基础模型在实现全球可持续发展目标中的实际效用和对齐程度。

Method: 引入SustainFM框架，涵盖多样化的可持续发展任务，如财富预测和环境危害检测。

Result: FMs在多样化任务中表现优于传统方法；评估需关注转移性、通用性和能源效率。

Conclusion: 呼吁从模型开发转向影响驱动的部署，重视能源效率、领域适应性和伦理考量。

Abstract: Foundation Models (FMs) are large-scale, pre-trained AI systems that have
revolutionized natural language processing and computer vision, and are now
advancing geospatial analysis and Earth Observation (EO). They promise improved
generalization across tasks, scalability, and efficient adaptation with minimal
labeled data. However, despite the rapid proliferation of geospatial FMs, their
real-world utility and alignment with global sustainability goals remain
underexplored. We introduce SustainFM, a comprehensive benchmarking framework
grounded in the 17 Sustainable Development Goals with extremely diverse tasks
ranging from asset wealth prediction to environmental hazard detection. This
study provides a rigorous, interdisciplinary assessment of geospatial FMs and
offers critical insights into their role in attaining sustainability goals. Our
findings show: (1) While not universally superior, FMs often outperform
traditional approaches across diverse tasks and datasets. (2) Evaluating FMs
should go beyond accuracy to include transferability, generalization, and
energy efficiency as key criteria for their responsible use. (3) FMs enable
scalable, SDG-grounded solutions, offering broad utility for tackling complex
sustainability challenges. Critically, we advocate for a paradigm shift from
model-centric development to impact-driven deployment, and emphasize metrics
such as energy efficiency, robustness to domain shifts, and ethical
considerations.

</details>


### [404] [VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and 3D Reasoning from CAD Software](https://arxiv.org/abs/2505.24838)
*Brandon Man,Ghadi Nehme,Md Ferdous Alam,Faez Ahmed*

Key words: VideoCAD, CAD, UI interaction, synthetic dataset, VideoCADFormer, VQA

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文介绍了VideoCAD，一个大规模合成数据集，旨在学习专业CAD工具中的UI交互，并提出了VideoCADFormer模型，展示了在视频理解和空间推理方面的挑战。

Motivation: 现有的AI驱动UI代理大多专注于简单任务，无法满足专业工程工具的高复杂度需求，因此需要开发专门针对CAD操作的数据集和方法。

Method: 通过自动化框架生成包含41K标注视频的VideoCAD数据集，并提出VideoCADFormer模型学习CAD操作。

Result: VideoCADFormer在行为克隆基准测试中表现优异，同时VideoCAD数据集的VQA基准揭示了视频理解的挑战。

Conclusion: VideoCAD填补了专业工程工具UI交互学习的空白，并为多模态理解和长时依赖问题提供了研究平台。

Abstract: Computer-Aided Design (CAD) is a time-consuming and complex process,
requiring precise, long-horizon user interactions with intricate 3D interfaces.
While recent advances in AI-driven user interface (UI) agents show promise,
most existing datasets and methods focus on short, low-complexity tasks in
mobile or web applications, failing to capture the demands of professional
engineering tools. In this work, we introduce VideoCAD, the first attempt at
engineering UI interaction learning for precision tasks. Specifically, VideoCAD
is a large-scale synthetic dataset consisting of over 41K annotated video
recordings of CAD operations, generated using an automated framework for
collecting high-fidelity UI action data from human-made CAD designs. Compared
to existing datasets, VideoCAD offers an order of magnitude higher complexity
in UI interaction learning for real-world engineering tasks, having up to a 20x
longer time horizon than other datasets. We show two important downstream
applications of VideoCAD: learning UI interactions from professional precision
3D CAD tools and a visual question-answering (VQA) benchmark designed to
evaluate multimodal large language models' (LLM) spatial reasoning and video
understanding abilities. To learn the UI interactions, we propose
VideoCADFormer - a state-of-the-art model in learning CAD interactions directly
from video, which outperforms multiple behavior cloning baselines. Both
VideoCADFormer and the VQA benchmark derived from VideoCAD reveal key
challenges in the current state of video-based UI understanding, including the
need for precise action grounding, multi-modal and spatial reasoning, and
long-horizon dependencies.

</details>


### [405] [Time Blindness: Why Video-Language Models Can't See What Humans Can?](https://arxiv.org/abs/2505.24867)
*Ujjwal Upadhyay,Mukul Ranjan,Zhiqiang Shen,Mohamed Elhoseiny*

Key words: 视觉语言模型, 时序理解, 基准测试, 时空解耦, SpookyBench

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了SpookyBench基准测试，展示当前视觉语言模型在纯时序模式理解上的局限性，人类表现显著优于模型。

Motivation: 现有视觉语言模型过度依赖空间特征，无法有效处理纯时序信息，与人类表现存在显著差距。

Method: 设计SpookyBench基准，通过噪声帧序列编码信息，对比人类与模型的时序模式识别能力。

Result: 人类准确率超98%，而前沿模型准确率为0%，且低空间信噪比下模型表现退化更严重。

Conclusion: 需新架构或训练范式以解耦时空依赖，推动时序模式识别研究。

Abstract: Recent advances in vision-language models (VLMs) have made impressive strides
in understanding spatio-temporal relationships in videos. However, when spatial
information is obscured, these models struggle to capture purely temporal
patterns. We introduce $\textbf{SpookyBench}$, a benchmark where information is
encoded solely in temporal sequences of noise-like frames, mirroring natural
phenomena from biological signaling to covert communication. Interestingly,
while humans can recognize shapes, text, and patterns in these sequences with
over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance
gap highlights a critical limitation: an over-reliance on frame-level spatial
features and an inability to extract meaning from temporal cues. Furthermore,
when trained in data sets with low spatial signal-to-noise ratios (SNR),
temporal understanding of models degrades more rapidly than human perception,
especially in tasks requiring fine-grained temporal reasoning. Overcoming this
limitation will require novel architectures or training paradigms that decouple
spatial dependencies from temporal processing. Our systematic analysis shows
that this issue persists across model scales and architectures. We release
SpookyBench to catalyze research in temporal pattern recognition and bridge the
gap between human and machine video understanding. Dataset and code has been
made available on our project website: https://timeblindness.github.io/.

</details>


### [406] [Reading Recognition in the Wild](https://arxiv.org/abs/2505.24848)
*Charig Yang,Samiul Alam,Shakhrul Iman Siam,Michael J. Proulx,Lambert Mathias,Kiran Somasundaram,Luis Pesqueira,James Fort,Sheroze Sheriffdeen,Omkar Parkhi,Carl Ren,Mi Zhang,Yuning Chai,Richard Newcombe,Hyo Jin Kim*

Key words: egocentric AI, reading recognition, multimodal dataset, transformer model, smart glasses

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种新任务——阅读识别，用于判断用户是否在阅读，并引入首个大规模多模态数据集'Reading in the Wild'，展示了一种灵活的Transformer模型，验证了多模态数据的相关性与互补性。

Motivation: 为实现智能眼镜中的情景AI，需记录用户与世界的交互行为，包括阅读场景。当前研究缺乏大规模、多样化的真实场景数据。

Method: 提出新任务'阅读识别'，构建包含100小时视频的多模态数据集，结合RGB、眼动和头部姿态三种模态，设计灵活的Transformer模型进行单模态或多模态分析。

Result: 验证了多模态数据的相关性与互补性，扩展了受限环境下的阅读理解研究，并展示了数据分类阅读类型的实用性。代码、模型和数据将开源。

Conclusion: 该研究为智能眼镜的上下文感知提供了新方法和数据集，推动真实场景下的阅读行为理解。

Abstract: To enable egocentric contextual AI in always-on smart glasses, it is crucial
to be able to keep a record of the user's interactions with the world,
including during reading. In this paper, we introduce a new task of reading
recognition to determine when the user is reading. We first introduce the
first-of-its-kind large-scale multimodal Reading in the Wild dataset,
containing 100 hours of reading and non-reading videos in diverse and realistic
scenarios. We then identify three modalities (egocentric RGB, eye gaze, head
pose) that can be used to solve the task, and present a flexible transformer
model that performs the task using these modalities, either individually or
combined. We show that these modalities are relevant and complementary to the
task, and investigate how to efficiently and effectively encode each modality.
Additionally, we show the usefulness of this dataset towards classifying types
of reading, extending current reading understanding studies conducted in
constrained settings to larger scale, diversity and realism. Code, model, and
data will be public.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [407] [4,500 Seconds: Small Data Training Approaches for Deep UAV Audio Classification](https://arxiv.org/abs/2505.23782)
*Andrew P. Berg,Qian Zhang,Mia Y. Wang*

Key words: 无人机, 深度学习, CNN, Transformer, 数据稀缺

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 研究探讨了无人机分类中数据稀缺问题，比较了CNN和Transformer的性能，结果显示CNN在精度和计算效率上略优，但Transformer仍有潜力。

Motivation: 无人机使用增加带来空域安全需求，但数据稀缺限制了分类模型的性能。

Method: 采用4,500秒音频数据，使用参数高效微调（PEFT）和数据增强，比较CNN和Transformer模型。

Result: CNN精度比Transformer高1-2%，计算效率更高；Transformer在数据增加和优化后可能超越CNN。

Conclusion: 数据稀缺下CNN表现更优，但Transformer潜力大，未来需扩展数据集以深入研究权衡。

Abstract: Unmanned aerial vehicle (UAV) usage is expected to surge in the coming
decade, raising the need for heightened security measures to prevent airspace
violations and security threats. This study investigates deep learning
approaches to UAV classification focusing on the key issue of data scarcity. To
investigate this we opted to train the models using a total of 4,500 seconds of
audio samples, evenly distributed across a 9-class dataset. We leveraged
parameter efficient fine-tuning (PEFT) and data augmentations to mitigate the
data scarcity. This paper implements and compares the use of convolutional
neural networks (CNNs) and attention-based transformers. Our results show that,
CNNs outperform transformers by 1-2\% accuracy, while still being more
computationally efficient. These early findings, however, point to potential in
using transformers models; suggesting that with more data and further
optimizations they could outperform CNNs. Future works aims to upscale the
dataset to better understand the trade-offs between these approaches.

</details>


### [408] [HARP: A Large-Scale Higher-Order Ambisonic Room Impulse Response Dataset](https://arxiv.org/abs/2411.14207)
*Shivam Saini,Jürgen Peissig*

Key words: Ambisonic, room impulse response, spatial audio, immersive audio, machine learning

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 该论文介绍了一个七阶Ambisonic房间脉冲响应（HOA-RIRs）数据集，通过高阶Ambisonic技术实现精准的空间音频再现，适用于沉浸式音频应用。

Motivation: 传统麦克风阵列在空间音频捕捉中存在局限性，因此研究团队开发了一种基于叠加原理的独特麦克风配置，以提高声场覆盖范围和空间分辨率。

Method: 采用虚拟模拟技术，使用64麦克风配置在球谐域直接捕获房间脉冲响应（RIRs），涵盖多种房间几何、吸声材料和声源-接收器距离变化。

Result: 数据集提供了高空间分辨率和真实感，适用于声源定位、混响预测和沉浸式音频再现等任务，为空间音频研究提供了重要资源。

Conclusion: 该数据集通过高阶Ambisonic技术优化了声场覆盖，为空间音频研究和机器学习应用提供了重要工具。

Abstract: This contribution introduces a dataset of 7th-order Ambisonic Room Impulse
Responses (HOA-RIRs), created using the Image Source Method. By employing
higher-order Ambisonics, our dataset enables precise spatial audio
reproduction, a critical requirement for realistic immersive audio
applications. Leveraging the virtual simulation, we present a unique microphone
configuration, based on the superposition principle, designed to optimize sound
field coverage while addressing the limitations of traditional microphone
arrays. The presented 64-microphone configuration allows us to capture RIRs
directly in the Spherical Harmonics domain. The dataset features a wide range
of room configurations, encompassing variations in room geometry, acoustic
absorption materials, and source-receiver distances. A detailed description of
the simulation setup is provided alongside for an accurate reproduction. The
dataset serves as a vital resource for researchers working on spatial audio,
particularly in applications involving machine learning to improve room
acoustics modeling and sound field synthesis. It further provides a very high
level of spatial resolution and realism crucial for tasks such as source
localization, reverberation prediction, and immersive sound reproduction.

</details>


### [409] [Unified AI for Accurate Audio Anomaly Detection](https://arxiv.org/abs/2505.23781)
*Hamideh Khaleghpour,Brett McKinney*

Key words: 音频异常检测，噪声消除，特征提取，集成学习，深度学习

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 该论文提出了一种统一的高精度音频异常检测框架，整合了噪声消除、特征提取和机器学习建模技术，结合传统和深度学习方法，并在基准测试中表现优异。

Motivation: 解决嘈杂环境和实时应用中的音频异常检测挑战。

Method: 采用频谱减法和自适应滤波增强音频质量，结合MFCC和深度嵌入（如OpenL3）提取特征，使用SVM、随机森林、CNN及集成方法建模。

Result: 在TORGO和LibriSpeech数据集上表现出更高的召回率、精准率和分类准确率。

Conclusion: 该框架为音频异常检测提供了可扩展的解决方案。

Abstract: This paper presents a unified AI framework for high-accuracy audio anomaly
detection by integrating advanced noise reduction, feature extraction, and
machine learning modeling techniques. The approach combines spectral
subtraction and adaptive filtering to enhance audio quality, followed by
feature extraction using traditional methods like MFCCs and deep embeddings
from pre-trained models such as OpenL3. The modeling pipeline incorporates
classical models (SVM, Random Forest), deep learning architectures (CNNs), and
ensemble methods to boost robustness and accuracy. Evaluated on benchmark
datasets including TORGO and LibriSpeech, the proposed framework demonstrates
superior performance in precision, recall, and classification of slurred vs.
normal speech. This work addresses challenges in noisy environments and
real-time applications and provides a scalable solution for audio-based anomaly
detection.

</details>


### [410] [Learning Normal Patterns in Musical Loops](https://arxiv.org/abs/2505.23784)
*Shayan Dadman,Bernt Arild Bremdal,Børre Bang,Rune Dalmo*

Key words: 无监督学习, 异常检测, 音乐信息检索, Deep SVDD, 音频模式识别

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 该论文提出了一种无监督框架，通过异常检测技术识别音乐样本中的音频模式，解决了音乐信息检索中的挑战。使用深度特征提取和无监督异常检测的结合，特别是HTS-AT和FFM生成音频表示，并通过Deep SVDD学习规范模式。实验表明其模型在异常分离上表现优越。

Motivation: 现有方法依赖手工特征或用户交互，限制了灵活性。本研究旨在通过无监督方式克服这些限制，提升对多样音频样本的处理能力。

Method: 结合HTS-AT和FFM提取特征，使用Deep SVDD学习规范音频模式，并比较了不同模型在异常检测上的表现。

Result: Deep SVDD模型（尤其是残差自编码器变体）在异常分离上表现更优，尤其适用于较大变化。

Conclusion: 该研究提供了一种灵活的无监督解决方案，有效识别音频模式，克服了现有方法的局限性。

Abstract: This paper introduces an unsupervised framework for detecting audio patterns
in musical samples (loops) through anomaly detection techniques, addressing
challenges in music information retrieval (MIR). Existing methods are often
constrained by reliance on handcrafted features, domain-specific limitations,
or dependence on iterative user interaction. We address these limitations
through an architecture combining deep feature extraction with unsupervised
anomaly detection. Our approach leverages a pre-trained Hierarchical
Token-semantic Audio Transformer (HTS-AT), paired with a Feature Fusion
Mechanism (FFM), to generate representations from variable-length audio loops.
These embeddings are processed using one-class Deep Support Vector Data
Description (Deep SVDD), which learns normative audio patterns by mapping them
to a compact latent hypersphere. Evaluations on curated bass and guitar
datasets compare standard and residual autoencoder variants against baselines
like Isolation Forest (IF) and and principle component analysis (PCA) methods.
Results show our Deep SVDD models, especially the residual autoencoder variant,
deliver improved anomaly separation, particularly for larger variations. This
research contributes a flexible, fully unsupervised solution for processing
diverse audio samples, overcoming previous structural and input limitations
while enabling effective pattern identification through distance-based latent
space scoring.

</details>


### [411] [Discl-VC: Disentangled Discrete Tokens and In-Context Learning for Controllable Zero-Shot Voice Conversion](https://arxiv.org/abs/2505.24291)
*Kaidi Wang,Wenhao Guan,Ziyue Jiang,Hukai Huang,Peijie Chen,Weijie Wu,Qingyang Hong,Lin Li*

Key words: 语音转换，零样本学习，自监督表征，韵律控制，上下文学习

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 提出了一种新型语音转换框架Discl-VC，通过自监督语音表征解耦内容和韵律信息，并结合上下文学习和流匹配转换器实现，显着提升了零样本语音转换的性能和韵律控制的准确性。

Motivation: 现有零样本语音转换系统难以准确复制源说话人的说话风格或模仿目标说话人的独特风格，限制了语音转换的可控性。

Method: 提出Discl-VC框架，解耦自监督语音表征中的内容和韵律信息，通过上下文学习和流匹配转换器合成目标语音；引入掩码生成转换器预测离散韵律标记，实现非自回归式的韵律控制。

Result: 实验证明Discl-VC在零样本语音转换中表现优异，且在合成语音的韵律控制上具有显著精度。

Conclusion: Discl-VC解决了现有系统在风格复制和韵律控制上的不足，为语音转换提供了更高的可控性和准确性。

Abstract: Currently, zero-shot voice conversion systems are capable of synthesizing the
voice of unseen speakers. However, most existing approaches struggle to
accurately replicate the speaking style of the source speaker or mimic the
distinctive speaking style of the target speaker, thereby limiting the
controllability of voice conversion. In this work, we propose Discl-VC, a novel
voice conversion framework that disentangles content and prosody information
from self-supervised speech representations and synthesizes the target
speaker's voice through in-context learning with a flow matching transformer.
To enable precise control over the prosody of generated speech, we introduce a
mask generative transformer that predicts discrete prosody tokens in a
non-autoregressive manner based on prompts. Experimental results demonstrate
the superior performance of Discl-VC in zero-shot voice conversion and its
remarkable accuracy in prosody control for synthesized speech.

</details>


### [412] [Patient-Aware Feature Alignment for Robust Lung Sound Classification:Cohesion-Separation and Global Alignment Losses](https://arxiv.org/abs/2505.23834)
*Seung Gyu Jeong,Seong Eun Kim*

Key words: 肺音分类,患者感知,特征对齐,个性化医疗,ICBHI数据集

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 提出了一个患者感知特征对齐（PAFA）框架，通过两种新型损失函数（PCSL和GPAL）解决肺音分类中的患者间变异性问题，在ICBHI数据集上取得了优异效果。（64.84%四分类，72.08%二分类）

Motivation: 生物医学信号（如肺音）在患者间存在变异性，传统方法难以捕捉个体差异。

Method: 提出PAFA框架，包含PCSL（同患者特征聚类+异患者特征分离）和GPAL（患者中心对齐全局中心）损失函数。

Result: ICBHI数据集上四分类64.84%、二分类72.08%的准确率。

Conclusion: PAFA能有效捕捉个体模式，提升患者群体的分类性能，适用于个性化医疗。

Abstract: Lung sound classification is vital for early diagnosis of respiratory
diseases. However, biomedical signals often exhibit inter-patient variability
even among patients with the same symptoms, requiring a learning approach that
considers individual differences. We propose a Patient-Aware Feature Alignment
(PAFA) framework with two novel losses, Patient Cohesion-Separation Loss (PCSL)
and Global Patient Alignment Loss (GPAL). PCSL clusters features of the same
patient while separating those from other patients to capture patient
variability, whereas GPAL draws each patient's centroid toward a global center,
preventing feature space fragmentation. Our method achieves outstanding results
on the ICBHI dataset with a score of 64.84\% for four-class and 72.08\% for
two-class classification. These findings highlight PAFA's ability to capture
individualized patterns and demonstrate performance gains in distinct patient
clusters, offering broader applications for patient-centered healthcare.

</details>


### [413] [Rehearsal with Auxiliary-Informed Sampling for Audio Deepfake Detection](https://arxiv.org/abs/2505.24486)
*Falih Gozi Febrinanto,Kristen Moore,Chandra Thapa,Jiangang Ma,Vidya Saikrishna,Feng Xia*

Key words: 持续学习, 音频Deepfake检测, 排练学习, 辅助标签, RAIS

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 摘要介绍了RAIS方法，一种通过辅助标签指导多样化样本选择的基于排练的持续学习框架，用于音频Deepfake检测，显著优于现有方法。

Motivation: 现有音频Deepfake检测框架在面对新型攻击时性能下降，且当前排练技术无法有效捕捉音频特征的多样性，导致偏置和遗忘风险增加。

Method: 提出Rehearsal with Auxiliary-Informed Sampling (RAIS)，利用标签生成网络产生辅助标签，指导内存缓冲区中的多样化样本选择。

Result: RAIS在五项实验中平均等错误率（EER）为1.953%，优于现有方法。

Conclusion: RAIS有效解决了音频特征多样性不足的问题，提升了持续学习框架的检测性能。

Abstract: The performance of existing audio deepfake detection frameworks degrades when
confronted with new deepfake attacks. Rehearsal-based continual learning (CL),
which updates models using a limited set of old data samples, helps preserve
prior knowledge while incorporating new information. However, existing
rehearsal techniques don't effectively capture the diversity of audio
characteristics, introducing bias and increasing the risk of forgetting. To
address this challenge, we propose Rehearsal with Auxiliary-Informed Sampling
(RAIS), a rehearsal-based CL approach for audio deepfake detection. RAIS
employs a label generation network to produce auxiliary labels, guiding diverse
sample selection for the memory buffer. Extensive experiments show RAIS
outperforms state-of-the-art methods, achieving an average Equal Error Rate
(EER) of 1.953 % across five experiences. The code is available at:
https://github.com/falihgoz/RAIS.

</details>


### [414] [Acoustic Classification of Maritime Vessels using Learnable Filterbanks](https://arxiv.org/abs/2505.23964)
*Jonas Elsborg,Tejs Vegge,Arghya Bhowmik*

Key words: 声学特征, 船只识别, 深度学习, Gabor滤波器, 鲁棒分类

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 论文提出了一种深度学习模型CATFISH，用于基于声学特征可靠监测和识别海上船只，并在不同录音场景中表现出鲁棒性。

Motivation: 由于不同录音场景的变异性，可靠监测和识别海上船只的声学特征具有挑战性，因此需要一个能够泛化到不同声学环境和声源-传感器距离的分类框架。

Method: 使用可训练的频谱前端和时序特征编码器学习Gabor滤波器组，动态强调不同频率成分。模型在VTUAD水听器记录数据上进行训练。

Result: 模型CATFISH在不同声源-传感器距离下达到了96.63%的测试准确率，超过之前基准12个百分点以上。

Conclusion: CATFISH模型在声学船只识别任务中展示了优越的性能，并且通过消融研究验证了传感器数据融合和基于注意力的池化技术的有效性。

Abstract: Reliably monitoring and recognizing maritime vessels based on acoustic
signatures is complicated by the variability of different recording scenarios.
A robust classification framework must be able to generalize across diverse
acoustic environments and variable source-sensor distances. To this end, we
present a deep learning model with robust performance across different
recording scenarios. Using a trainable spectral front-end and temporal feature
encoder to learn a Gabor filterbank, the model can dynamically emphasize
different frequency components. Trained on the VTUAD hydrophone recordings from
the Strait of Georgia, our model, CATFISH, achieves a state-of-the-art 96.63 %
percent test accuracy across varying source-sensor distances, surpassing the
previous benchmark by over 12 percentage points. We present the model, justify
our architectural choices, analyze the learned Gabor filters, and perform
ablation studies on sensor data fusion and attention-based pooling.

</details>


### [415] [Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC](https://arxiv.org/abs/2505.24200)
*Qingzheng Wang,Jiancheng Sun,Yifan Peng,Shinji Watanabe*

Key words: 语音基础模型, 多语言处理, 语言识别, 自动语音识别, 低秩适应

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 论文通过多种策略优化多语言语音基础模型（SFM），如冻结上游训练、部分微调和低秩适应，并结合数据增强和CTC损失正则化，显著提升了语言识别（LID）和自动语音识别（ASR）性能。

Motivation: 解决多语言语音基础模型在微调时资源有限的问题，提升ML-SUPERB 2.0上的LID和ASR表现。

Method: 采用冻结上游训练、部分微调、低秩适应策略，结合数据增强和LID CTC损失正则化。

Result: LID准确率相对提升14%，ASR CER相对降低30%，在Interspeech 2025 ML-SUPERB 2.0挑战赛中获第二名。

Conclusion: 提出的方法有效优化了多语言SFM，显著提升了LID和ASR任务性能。

Abstract: Multilingual speech processing with self-supervised or supervised pre-trained
Speech Foundation Models (SFM) has achieved strong performance on tasks like
Language Identification (LID) and Automatic Speech Recognition (ASR). However,
these models struggle with limited resources during fine-tuning. This paper
enhances multilingual LID and ASR on ML-SUPERB 2.0 by exploring multiple
strategies for adapting SFMs, including frozen upstream training, partial
fine-tuning, and low-rank adaptation. Furthermore, we employ data augmentation
to mitigate performance gaps in few-shot settings and introduce LID
Connectionist Temporal Classification (CTC) loss for regularization. Our
approach achieves a 14% relative improvement in LID accuracy and a 30% relative
reduction in ASR CER over the baseline on ML-SUPERB 2.0, securing second place
in the Interspeech 2025 ML-SUPERB 2.0 Challenge.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [416] [More-than-Human Storytelling: Designing Longitudinal Narrative Engagements with Generative AI](https://arxiv.org/abs/2505.23780)
*Émilie Fabre,Katie Seaborn,Shuta Koiwai,Mizuki Watanabe,Paul Riesch*

Key words: 生成式AI, 纵向研究, 故事共创, 用户代理, 伦理

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 论文探讨了与生成式AI（GenAI）故事叙述代理的长期互动，通过为期两周的日记研究，揭示了用户与AI在故事共创中的复杂性，包括正面体验和局限性。

Motivation: 研究背景是生成式AI在纵向故事叙述中的应用尚未充分探索，旨在了解用户与AI叙述代理长期互动的体验和动态。

Method: 采用日记研究方法，28名参与者使用“Dreamsmithy”App与AI叙述者“Makoto”每日共创故事，通过反思性主题分析提取主题。

Result: 研究发现用户欣赏AI的创造性和反思机会，但对叙述连贯性和控制权的局限性感到沮丧，揭示了“摇摆矛盾”和“社会时间纽带”等主题。

Conclusion: 生成式AI在纵向故事叙述中具有潜力，但也引发了对用户代理和伦理的批判性问题，为开发更自适应的人机叙述系统提供了初步经验和设计考虑。

Abstract: Longitudinal engagement with generative AI (GenAI) storytelling agents is a
timely but less charted domain. We explored multi-generational experiences with
"Dreamsmithy," a daily dream-crafting app, where participants (N = 28)
co-created stories with AI narrator "Makoto" every day. Reflections and
interactions were captured through a two-week diary study. Reflexive thematic
analysis revealed themes likes "oscillating ambivalence" and
"socio-chronological bonding," highlighting the complex dynamics that emerged
between individuals and the AI narrator over time. Findings suggest that while
people appreciated the personal notes, opportunities for reflection, and AI
creativity, limitations in narrative coherence and control occasionally caused
frustration. The results underscore the potential of GenAI for longitudinal
storytelling, but also raise critical questions about user agency and ethics.
We contribute initial empirical insights and design considerations for
developing adaptive, more-than-human storytelling systems.

</details>


### [417] [Redefining Research Crowdsourcing: Incorporating Human Feedback with LLM-Powered Digital Twins](https://arxiv.org/abs/2505.24004)
*Amanda Chan,Catherine Di,Joseph Rupertus,Gary Smith,Varun Nagaraj Rao,Manoel Horta Ribeiro,Andrés Monroy-Hernández*

Key words: 众包平台, 生成式AI, 数字孪生, 数据真实性, 人类参与

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 该论文提出了一种利用数字孪生技术的混合框架，以解决众包平台中因生成式AI使用导致的数据真实性和工作者角色威胁问题。

Motivation: 研究动机在于解决众包平台中生成式AI工具使用带来的数据真实性挑战和工作者角色被削弱的问题。

Method: 采用数字孪生技术（个性化AI模型）保持人类参与，并通过实验（n=88）和深度访谈（n=9）进行评估。

Result: 研究表明，数字孪生能提升生产力、减少决策疲劳，并保持响应质量，同时强调透明度和伦理数据使用的重要性。

Conclusion: 数字孪生通过自动化重复任务和保留人类参与复杂任务，有助于在扩展性与真实性之间取得平衡。

Abstract: Crowd work platforms like Amazon Mechanical Turk and Prolific are vital for
research, yet workers' growing use of generative AI tools poses challenges.
Researchers face compromised data validity as AI responses replace authentic
human behavior, while workers risk diminished roles as AI automates tasks. To
address this, we propose a hybrid framework using digital twins, personalized
AI models that emulate workers' behaviors and preferences while keeping humans
in the loop. We evaluate our system with an experiment (n=88 crowd workers) and
in-depth interviews with crowd workers (n=5) and social science researchers
(n=4). Our results suggest that digital twins may enhance productivity and
reduce decision fatigue while maintaining response quality. Both researchers
and workers emphasized the importance of transparency, ethical data use, and
worker agency. By automating repetitive tasks and preserving human engagement
for nuanced ones, digital twins may help balance scalability with authenticity.

</details>


### [418] [WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and other Language Editions](https://arxiv.org/abs/2505.24195)
*Zining Wang,Yuxuan Zhang,Dongwook Yoon,Nicholas Vincent,Farhan Samir,Vered Shwartz*

Key words: WikiGap, 多语言维基百科, 知识平等, 互补信息, 用户界面设计

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: WikiGap是一个系统，旨在通过英语维基百科界面展示其他语言版本（如法语、俄语和中文维基百科）中的互补信息，以提高事实查找的准确性和效率。

Motivation: 由于英语维基百科在全球知识访问中占主导地位，其他语言版本中的文化相关互补信息常被忽视，导致知识不平等。

Method: 结合多语言信息差距发现方法与以用户为中心的设计，开发了WikiGap系统。

Result: 在21人的混合方法研究中，WikiGap显著提高了事实查找准确性，减少了任务时间，并获得更高的可用性评分。

Conclusion: WikiGap提升了用户对其他语言维基百科中互补信息的认知，促进了语言版本间的知识平等。

Abstract: With more than 11 times as many pageviews as the next, English Wikipedia
dominates global knowledge access relative to other language editions. Readers
are prone to assuming English Wikipedia as a superset of all language editions,
leading many to prefer it even when their primary language is not English.
Other language editions, however, comprise complementary facts rooted in their
respective cultures and media environments, which are marginalized in English
Wikipedia. While Wikipedia's user interface enables switching between language
editions through its Interlanguage Link (ILL) system, it does not reveal to
readers that other language editions contain valuable, complementary
information. We present WikiGap, a system that surfaces complementary facts
sourced from other Wikipedias within the English Wikipedia interface.
Specifically, by combining a recent multilingual information-gap discovery
method with a user-centered design, WikiGap enables access to complementary
information from French, Russian, and Chinese Wikipedia. In a mixed-methods
study (n=21), WikiGap significantly improved fact-finding accuracy, reduced
task time, and received a 32-point higher usability score relative to
Wikipedia's current ILL-based navigation system. Participants reported
increased awareness of the availability of complementary information in
non-English editions and reconsidered the completeness of English Wikipedia.
WikiGap thus paves the way for improved epistemic equity across language
editions.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [419] [A2 Copula-Driven Spatial Bayesian Neural Network For Modeling Non-Gaussian Dependence: A Simulation Study](https://arxiv.org/abs/2505.24006)
*Agnideep Aich,Sameera Hewage,Md Monzur Murshed,Ashit Baran Aich,Amanda Mayeaux,Asim K. Dey,Kumer P. Das,Bruce Wade*

Key words: A2-SBNN, Copula, 空间模型, 极端依赖性, Wasserstein损失

<details>
  <summary>Details</summary>

Main category: stat.ME

TL;DR: A2-SBNN是一种新型空间预测模型，通过嵌入A2 Copula来捕捉空间依赖性和极端事件，结合Wasserstein损失和相关性惩罚，性能优于传统高斯方法。

Motivation: 传统高斯方法难以捕捉空间数据中的极端依赖性和复杂模式，A2-SBNN旨在解决这一局限。

Method: A2-SBNN将双尾A2 Copula嵌入权重初始化，采用Wasserstein损失、矩匹配和相关性惩罚进行训练。

Result: 在模拟中，A2-SBNN在各种依赖强度下均表现出高准确性。

Conclusion: A2-SBNN为空间数据建模提供了超越传统方法的新解决方案。

Abstract: In this paper, we introduce the A2 Copula Spatial Bayesian Neural Network
(A2-SBNN), a predictive spatial model designed to map coordinates to continuous
fields while capturing both typical spatial patterns and extreme dependencies.
By embedding the dual-tail novel Archimedean copula viz. A2 directly into the
network's weight initialization, A2-SBNN naturally models complex spatial
relationships, including rare co-movements in the data. The model is trained
through a calibration-driven process combining Wasserstein loss, moment
matching, and correlation penalties to refine predictions and manage
uncertainty. Simulation results show that A2-SBNN consistently delivers high
accuracy across a wide range of dependency strengths, offering a new, effective
solution for spatial data modeling beyond traditional Gaussian-based
approaches.

</details>


### [420] [Data Fusion for Partial Identification of Causal Effects](https://arxiv.org/abs/2505.24296)
*Quinn Lanners,Cynthia Rudin,Alexander Volfovsky,Harsh Parikh*

Key words: 数据融合、因果推断、部分识别、敏感参数、崩溃前沿分析

<details>
  <summary>Details</summary>

Main category: stat.ME

TL;DR: 该论文提出了一种新的部分识别框架，用于解决因果推断中观测数据与随机对照试验数据融合时关键假设同时失效的问题，并通过敏感参数和因果效应边界增强结论的稳健性。

Motivation: 在因果推断中，现有方法通常在假设无反混同或反事实结果可交换的条件下融合数据源，但当这些假设同时失效时（实践中常见），无法识别或估计因果效应。本文旨在解决这一限制。

Method: 提出部分识别框架，引入可解释的敏感参数量化假设违反程度，推导因果效应边界，并开发双重稳健估计器；通过崩溃前沿分析验证结论随假设违反程度的变化。

Result: 在Project STAR研究中应用该框架，发现其结论对关键假设的同时违反具有强稳健性（平均及子群体层面），增强了对研究结论的信心。

Conclusion: 该框架为因果推断提供了一种灵活的工具，能够在假设违反时仍得出可靠结论。

Abstract: Data fusion techniques integrate information from heterogeneous data sources
to improve learning, generalization, and decision making across data sciences.
In causal inference, these methods leverage rich observational data to improve
causal effect estimation, while maintaining the trustworthiness of randomized
controlled trials. Existing approaches often relax the strong no unobserved
confounding assumption by instead assuming exchangeability of counterfactual
outcomes across data sources. However, when both assumptions simultaneously
fail - a common scenario in practice - current methods cannot identify or
estimate causal effects. We address this limitation by proposing a novel
partial identification framework that enables researchers to answer key
questions such as: Is the causal effect positive or negative? and How severe
must assumption violations be to overturn this conclusion? Our approach
introduces interpretable sensitivity parameters that quantify assumption
violations and derives corresponding causal effect bounds. We develop doubly
robust estimators for these bounds and operationalize breakdown frontier
analysis to understand how causal conclusions change as assumption violations
increase. We apply our framework to the Project STAR study, which investigates
the effect of classroom size on students' third-grade standardized test
performance. Our analysis reveals that the Project STAR results are robust to
simultaneous violations of key assumptions, both on average and across various
subgroups of interest. This strengthens confidence in the study's conclusions
despite potential unmeasured biases in the data.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [421] [A comprehensive survey of cybercrimes in India over the last decade](https://arxiv.org/abs/2505.23770)
*Sudhanshu Sekhar Tripathy*

Key words: 网络犯罪, 网络安全, 预防策略, 数字支付, 公共意识

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 这篇论文探讨了数字技术普及背景下网络犯罪的兴起及其对各行业的威胁，强调加强网络安全措施、公众意识和教育的重要性。

Motivation: 随着数字技术的广泛应用，网络犯罪日益猖獗，对个人、组织和关键行业（如金融、医疗和政府）构成重大威胁。本文旨在分析网络犯罪的现状、挑战及应对策略，以促进数字基础设施的安全。

Method: 论文通过综述网络犯罪的发展趋势、类型（如勒索软件、数据泄露等）及其影响，并结合印度等地的案例，提出预防和应对策略，包括技术防护、法规完善和教育推广。

Result: 研究表明，网络犯罪形式多样且危害严重，需多管齐下应对。印度的案例凸显了互联网普及与数字化支付带来的风险，而提高公众意识和强化法规是有效手段。

Conclusion: 综合技术、法规和教育是抵御网络犯罪的关键。未来需进一步强化国际合作与创新安全技术以应对不断演变的威胁。

Abstract: Since the 1990s, the integration of technology into daily life has led to the
creation of an extensive network of interconnected devices, transforming how
individuals and organizations operate. However, this digital transformation has
also spurred the rise of cybercrime, criminal activities perpetrated through
networks or computer systems. Cybercrime has become a global concern,
presenting significant challenges to security systems. Although advancements in
digital technology have enhanced efficiency, they have also opened new avenues
for exploitation by cybercriminals, highlighting the urgent need for advanced
cybersecurity measures. The escalating number of cyberattacks and associated
risks in the past decade highlights the critical importance of protecting
sensitive data and safeguarding information systems. Cybercrimes range from
financial fraud and phishing scams to identity theft and online harassment,
posing substantial risks to both individuals and organizations. In response,
governments, law enforcement agencies, and cybersecurity units have intensified
their efforts to address these threats. In recent years, India has experienced
a significant surge in cybercrime incidents, with a notable increase in cases
involving ransomware, data breaches, and social engineering attacks. The
growing penetration of internet services, the expansion of e-commerce, and the
rapid adoption of digital payment systems have made individuals and
organizations more vulnerable to cyber threats. Key areas affected include
banking, healthcare, and government sectors, which are frequently targeted due
to the sensitive nature of the data they handle. To combat these risks, there
is an increasing focus on public awareness, cybersecurity education, and robust
regulatory frameworks. This paper examines cybercrime, prevention strategies,
security protocols, and terminology to safeguard digital infrastructure.

</details>


### [422] [Mind the Gap: A Practical Attack on GGUF Quantization](https://arxiv.org/abs/2505.23786)
*Kazuki Egashira,Robin Staab,Mark Vero,Jingxuan He,Martin Vechev*

Key words: post-training quantization, GGUF, adversarial attack, LLM security

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出了一种针对GGUF量化方法的新型攻击，利用量化误差在模型权重中隐藏恶意行为，展示了在多种场景下的有效性。

Motivation: 当前基于舍入的量化方法存在安全隐患，但复杂量化方法（如GGUF）的安全风险尚未被研究。

Method: 通过量化误差的灵活性，训练恶意LLM并约束其权重，构建看似正常的量化模型。

Result: 在三种LLM和九种GGUF量化类型上，攻击在代码生成（88.7%）、内容注入（85.0%）和指令拒绝（30.1%）场景中表现显著。

Conclusion: GGUF量化方法易受攻击，仅靠复杂量化不足以作为防御手段。

Abstract: With the increasing size of frontier LLMs, post-training quantization has
become the standard for memory-efficient deployment. Recent work has shown that
basic rounding-based quantization schemes pose security risks, as they can be
exploited to inject malicious behaviors into quantized models that remain
hidden in full precision. However, existing attacks cannot be applied to more
complex quantization methods, such as the GGUF family used in the popular
ollama and llama.cpp frameworks. In this work, we address this gap by
introducing the first attack on GGUF. Our key insight is that the quantization
error -- the difference between the full-precision weights and their
(de-)quantized version -- provides sufficient flexibility to construct
malicious quantized models that appear benign in full precision. Leveraging
this, we develop an attack that trains the target malicious LLM while
constraining its weights based on quantization errors. We demonstrate the
effectiveness of our attack on three popular LLMs across nine GGUF quantization
data types on three diverse attack scenarios: insecure code generation
($\Delta$=$88.7\%$), targeted content injection ($\Delta$=$85.0\%$), and benign
instruction refusal ($\Delta$=$30.1\%$). Our attack highlights that (1) the
most widely used post-training quantization method is susceptible to
adversarial interferences, and (2) the complexity of quantization schemes alone
is insufficient as a defense.

</details>


### [423] [Evaluating Query Efficiency and Accuracy of Transfer Learning-based Model Extraction Attack in Federated Learning](https://arxiv.org/abs/2505.23791)
*Sayyed Farid Ahamed,Sandip Roy,Soumya Banerjee,Marc Vucovich,Kevin Choi,Abdul Rahman,Alison Hu,Edward Bowen,Sachin Shetty*

Key words: 联邦学习（FL），模型提取（ME），攻击脆弱性，预训练模型，迁移学习

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文研究了联邦学习（FL）在模型提取（ME）攻击下的脆弱性，通过实验评估了不同攻击查询集大小对提取模型准确性和保真度的影响，并探索了预训练模型在提取过程中的优势。

Motivation: 尽管联邦学习（FL）旨在保护客户数据隐私，但其分布式特性使其易受模型提取（ME）攻击的威胁，可能导致机密模型被复制。研究旨在评估FL模型面对ME攻击的脆弱性，并提出改进防御的见解。

Method: 研究通过在NVFlare平台上构建的不同联邦客户端，对两种深度学习架构和三个图像数据集实施ME攻击，利用准确率、保真度和KL散度等指标评估攻击性能，并探索了基于迁移学习的预训练模型提取方法。

Result: 实验结果显示，攻击查询集的大小显著影响提取模型的准确性和保真度；使用预训练模型作为起点的提取方法在较小的查询集上表现出更高的准确性和保真度。

Conclusion: 研究表明FL模型对ME攻击具有明显脆弱性，尤其在使用预训练模型时，攻击者可能以较少查询获得高性能的提取模型，强调了加强FL安全防御的必要性。

Abstract: Federated Learning (FL) is a collaborative learning framework designed to
protect client data, yet it remains highly vulnerable to Intellectual Property
(IP) threats. Model extraction (ME) attacks pose a significant risk to Machine
Learning as a Service (MLaaS) platforms, enabling attackers to replicate
confidential models by querying black-box (without internal insight) APIs.
Despite FL's privacy-preserving goals, its distributed nature makes it
particularly susceptible to such attacks. This paper examines the vulnerability
of FL-based victim models to two types of model extraction attacks. For various
federated clients built under the NVFlare platform, we implemented ME attacks
across two deep learning architectures and three image datasets. We evaluate
the proposed ME attack performance using various metrics, including accuracy,
fidelity, and KL divergence. The experiments show that for different FL
clients, the accuracy and fidelity of the extracted model are closely related
to the size of the attack query set. Additionally, we explore a transfer
learning based approach where pretrained models serve as the starting point for
the extraction process. The results indicate that the accuracy and fidelity of
the fine-tuned pretrained extraction models are notably higher, particularly
with smaller query sets, highlighting potential advantages for attackers.

</details>


### [424] [Zero-Trust Foundation Models: A New Paradigm for Secure and Collaborative Artificial Intelligence for Internet of Things](https://arxiv.org/abs/2505.23792)
*Kai Li,Conggai Li,Xin Yuan,Shenghong Li,Sai Zou,Syed Sohail Ahmed,Wei Ni,Dusit Niyato,Abbas Jamalipour,Falko Dressler,Ozgur B. Akan*

Key words: 零信任，基础模型，物联网，安全，联邦学习，区块链

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 该论文提出了一种名为零信任基础模型（ZTFMs）的新范式，将零信任安全原则嵌入到物联网（IoT）系统的基础模型生命周期中，以实现安全、隐私保护的AI，并提出了支撑这一范式的技术框架和研究挑战。

Motivation: 物联网系统面临分布式、异构和潜在对抗性环境的挑战，传统信任架构难以保证安全和隐私。通过将零信任原则融入基础模型的设计、训练和部署，ZTFMs有望构建更具韧性的自防御生态系统。

Method: 论文综合了ZTFMs的首次结构化综述，并提出了一种技术框架，结合了联邦学习（FL）、基于区块链的身份管理、微隔离和可信执行环境（TEEs），以支持边缘网络的去中心化、可验证智能。

Result: 研究发现ZTFMs能够推动物联网架构从传统信任模式向更具韧性的自防御系统转变，同时探讨了相关的安全威胁及应对措施，如异常检测、对抗训练和安全聚合等。

Conclusion: 论文提出了在可扩展性、安全协调、可解释威胁归因和动态信任校准等方面的开放研究挑战，为基于基础模型的安全、智能和可信赖物联网基础设施奠定了基础。

Abstract: This paper focuses on Zero-Trust Foundation Models (ZTFMs), a novel paradigm
that embeds zero-trust security principles into the lifecycle of foundation
models (FMs) for Internet of Things (IoT) systems. By integrating core tenets,
such as continuous verification, least privilege access (LPA), data
confidentiality, and behavioral analytics into the design, training, and
deployment of FMs, ZTFMs can enable secure, privacy-preserving AI across
distributed, heterogeneous, and potentially adversarial IoT environments. We
present the first structured synthesis of ZTFMs, identifying their potential to
transform conventional trust-based IoT architectures into resilient,
self-defending ecosystems. Moreover, we propose a comprehensive technical
framework, incorporating federated learning (FL), blockchain-based identity
management, micro-segmentation, and trusted execution environments (TEEs) to
support decentralized, verifiable intelligence at the network edge. In
addition, we investigate emerging security threats unique to ZTFM-enabled
systems and evaluate countermeasures, such as anomaly detection, adversarial
training, and secure aggregation. Through this analysis, we highlight key open
research challenges in terms of scalability, secure orchestration,
interpretable threat attribution, and dynamic trust calibration. This survey
lays a foundational roadmap for secure, intelligent, and trustworthy IoT
infrastructures powered by FMs.

</details>


### [425] [USB: A Comprehensive and Unified Safety Evaluation Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2505.23793)
*Baolin Zheng,Guanlin Chen,Hongqiong Zhong,Qingyang Teng,Yingshui Tan,Zhendong Liu,Weixun Wang,Jiaheng Liu,Jian Yang,Huiyun Jing,Jincheng Wei,Wenbo Su,Xiaoyong Zhu,Bo Zheng,Kaifu Zhang*

Key words: 多模态大语言模型、安全评估、基准测试、数据合成、漏洞与过度敏感性

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 本文提出了Unified Safety Benchmarks (USB)，一个全面的多模态大语言模型安全评估基准，解决了现有基准在数据质量、覆盖范围和模态组合上的不足。

Motivation: 多模态大语言模型（MLLMs）存在显著的安全漏洞，现有安全评估基准在数据质量和模态组合上不足，导致评估结果不准确，阻碍安全问题的发现与治理。

Method: 通过设计高效的数据合成流程，结合开源数据生成高质量补充数据，覆盖61个风险子类别和4种模态组合，支持中英文评估。

Result: USB是目前最全面的MLLM安全评估基准，涵盖了漏洞和过度敏感性的双重评估，填补了现有基准的空白。

Conclusion: USB为MLLM的安全评估提供了更全面的工具，有助于发现和治理安全问题。

Abstract: Despite their remarkable achievements and widespread adoption, Multimodal
Large Language Models (MLLMs) have revealed significant security
vulnerabilities, highlighting the urgent need for robust safety evaluation
benchmarks. Existing MLLM safety benchmarks, however, fall short in terms of
data quality and coverge, and modal risk combinations, resulting in inflated
and contradictory evaluation results, which hinders the discovery and
governance of security concerns. Besides, we argue that vulnerabilities to
harmful queries and oversensitivity to harmless ones should be considered
simultaneously in MLLMs safety evaluation, whereas these were previously
considered separately. In this paper, to address these shortcomings, we
introduce Unified Safety Benchmarks (USB), which is one of the most
comprehensive evaluation benchmarks in MLLM safety. Our benchmark features
high-quality queries, extensive risk categories, comprehensive modal
combinations, and encompasses both vulnerability and oversensitivity
evaluations. From the perspective of two key dimensions: risk categories and
modality combinations, we demonstrate that the available benchmarks -- even the
union of the vast majority of them -- are far from being truly comprehensive.
To bridge this gap, we design a sophisticated data synthesis pipeline that
generates extensive, high-quality complementary data addressing previously
unexplored aspects. By combining open-source datasets with our synthetic data,
our benchmark provides 4 distinct modality combinations for each of the 61 risk
sub-categories, covering both English and Chinese across both vulnerability and
oversensitivity dimensions.

</details>


### [426] [MultiPhishGuard: An LLM-based Multi-Agent System for Phishing Email Detection](https://arxiv.org/abs/2505.23803)
*Yinuo Xue,Eric Spero,Yun Sing Koh,Giovanni Russello*

Key words: 钓鱼邮件检测,多代理系统,强化学习,对抗训练,LLM

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: MultiPhishGuard是一种基于LLM的多代理动态检测系统，通过协同专家知识和对抗感知强化学习，显著提高了钓鱼邮件检测的准确性和鲁棒性。

Motivation: 传统的钓鱼邮件检测方法（如规则过滤和黑名单）难以应对不断变化的对抗策略，导致误判和安全隐患。机器学习方法虽然有所改进，但仍无法完全适应新型钓鱼策略。

Method: MultiPhishGuard采用五个协作代理（文本、URL、元数据、解释简化器和对抗代理）和PPO强化学习算法，加入对抗训练循环生成上下文感知的钓鱼邮件变体，形成自增强防御系统。

Result: 在公开数据集上，MultiPhishGuard准确率达97.89%，误报率2.73%，漏报率0.20%，优于当前最先进方法。

Conclusion: 该研究通过动态多代理协作和生成对抗韧性，推动了钓鱼防御技术的进步。

Abstract: Phishing email detection faces critical challenges from evolving adversarial
tactics and heterogeneous attack patterns. Traditional detection methods, such
as rule-based filters and denylists, often struggle to keep pace with these
evolving tactics, leading to false negatives and compromised security. While
machine learning approaches have improved detection accuracy, they still face
challenges adapting to novel phishing strategies. We present MultiPhishGuard, a
dynamic LLM-based multi-agent detection system that synergizes specialized
expertise with adversarial-aware reinforcement learning. Our framework employs
five cooperative agents (text, URL, metadata, explanation simplifier, and
adversarial agents) with automatically adjusted decision weights powered by a
Proximal Policy Optimization reinforcement learning algorithm. To address
emerging threats, we introduce an adversarial training loop featuring an
adversarial agent that generates subtle context-aware email variants, creating
a self-improving defense ecosystem and enhancing system robustness.
Experimental evaluations on public datasets demonstrate that MultiPhishGuard
significantly outperforms Chain-of-Thoughts, single-agent baselines and
state-of-the-art detectors, as validated by ablation studies and comparative
analyses. Experiments demonstrate that MultiPhishGuard achieves high accuracy
(97.89\%) with low false positive (2.73\%) and false negative rates (0.20\%).
Additionally, we incorporate an explanation simplifier agent, which provides
users with clear and easily understandable explanations for why an email is
classified as phishing or legitimate. This work advances phishing defense
through dynamic multi-agent collaboration and generative adversarial
resilience.

</details>


### [427] [ADA: Automated Moving Target Defense for AI Workloads via Ephemeral Infrastructure-Native Rotation in Kubernetes](https://arxiv.org/abs/2505.23805)
*Akram Sheriff,Ken Huang,Zsolt Nemeth,Madjid Nakhjiri*

Key words: adaptive defense, automated moving target defense, Kubernetes, zero trust, AI security

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 本文介绍了自适应防御代理（ADA），基于Kubernetes的动态目标防御系统，通过不断更换AI工作负载，主动防御攻击。

Motivation: 传统静态防御依赖昂贵且复杂的可信计算方案，而ADA旨在通过环境操控提供更灵活、更安全的零信任模型。

Method: ADA利用Kubernetes容器的短暂性，持续销毁并重启AI服务实例，结合混沌工程原则，破坏攻击者的假设。

Result: ADA提供了一种更鲁棒、灵活且操作高效的零信任模型，通过主动环境操控而非被动修补提升安全性。

Conclusion: ADA为AI基础设施安全提供了新的动态防御范式，简化部署并支持最新AI生态。

Abstract: This paper introduces the Adaptive Defense Agent (ADA), an innovative
Automated Moving Target Defense (AMTD) system designed to fundamentally enhance
the security posture of AI workloads. ADA operates by continuously and
automatically rotating these workloads at the infrastructure level, leveraging
the inherent ephemerality of Kubernetes pods. This constant managed churn
systematically invalidates attacker assumptions and disrupts potential kill
chains by regularly destroying and respawning AI service instances. This
methodology, applying principles of chaos engineering as a continuous,
proactive defense, offers a paradigm shift from traditional static defenses
that rely on complex and expensive confidential or trusted computing solutions
to secure the underlying compute platforms, while at the same time agnostically
supporting the latest advancements in agentic and nonagentic AI ecosystems and
solutions such as agent-to-agent (A2A) communication frameworks or model
context protocols (MCP). This AI-native infrastructure design, relying on the
widely proliferated cloud-native Kubernetes technologies, facilitates easier
deployment, simplifies maintenance through an inherent zero trust posture
achieved by rotation, and promotes faster adoption. We posit that ADA's novel
approach to AMTD provides a more robust, agile, and operationally efficient
zero-trust model for AI services, achieving security through proactive
environmental manipulation rather than reactive patching.

</details>


### [428] [DP-RTFL: Differentially Private Resilient Temporal Federated Learning for Trustworthy AI in Regulated Industries](https://arxiv.org/abs/2505.23813)
*Abhijit Talluri*

Key words: 联邦学习、差分隐私、容错性、完整性证明、金融科技

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 本文提出了DP-RTFL（差分隐私弹性时序联邦学习）框架，旨在解决联邦学习中的容错性、隐私保障及数据完整性验证问题，适用于金融和医疗等敏感领域。

Motivation: 现有联邦学习方案在容错性、隐私保障和完整性验证方面存在不足，可能引发训练中断、数据丢失或合规性问题，需要一种更健壮的解决方案。

Method: DP-RTFL结合客户端局部差分隐私（LDP）、弹性时序状态管理和零知识完整性证明（ZKIPs），确保训练连续性和隐私保护。

Result: 该框架通过开源实现，适用于金融风险评估等关键场景，具备操作鲁棒性、可审计性和扩展性。

Conclusion: DP-RTFL为敏感数据的联邦学习提供了隐私、弹性和完整性保障，满足企业级AI部署需求。

Abstract: Federated Learning (FL) has emerged as a critical paradigm for enabling
privacy-preserving machine learning, particularly in regulated sectors such as
finance and healthcare. However, standard FL strategies often encounter
significant operational challenges related to fault tolerance, system
resilience against concurrent client and server failures, and the provision of
robust, verifiable privacy guarantees essential for handling sensitive data.
These deficiencies can lead to training disruptions, data loss, compromised
model integrity, and non-compliance with data protection regulations (e.g.,
GDPR, CCPA). This paper introduces Differentially Private Resilient Temporal
Federated Learning (DP-RTFL), an advanced FL framework designed to ensure
training continuity, precise state recovery, and strong data privacy. DP-RTFL
integrates local Differential Privacy (LDP) at the client level with resilient
temporal state management and integrity verification mechanisms, such as
hash-based commitments (referred to as Zero-Knowledge Integrity Proofs or ZKIPs
in this context). The framework is particularly suited for critical
applications like credit risk assessment using sensitive financial data, aiming
to be operationally robust, auditable, and scalable for enterprise AI
deployments. The implementation of the DP-RTFL framework is available as
open-source.

</details>


### [429] [Seven Security Challenges That Must be Solved in Cross-domain Multi-agent LLM Systems](https://arxiv.org/abs/2505.23847)
*Ronny Ko,Jiseong Jeong,Shuyuan Zheng,Chuan Xiao,Taewan Kim,Makoto Onizuka,Wonyong Shin*

Key words: 大型语言模型, 多智能体系统, 安全挑战, 跨域协作, 数据保护

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 本文探讨了跨领域多智能体大型语言模型（LLM）系统的安全问题，提出了七类新的安全挑战，并针对每类挑战提出可能的攻击方式、安全评估指标和未来研究方向。

Motivation: 随着大型语言模型发展为能够在跨组织边界合作的自主智能体，数据所有权和安全性问题变得尤为突出，尤其是在多智能体动态协作中可能引发的安全风险。

Method: 通过分析跨领域LLM智能体合作中的潜在安全漏洞，作者提出了七类新的安全挑战，并为每类挑战设计可能的攻击方式、评估指标和研究建议。

Result: 研究识别出多智能体LLM系统中的关键安全问题，并提出了相应的攻击模拟和防御方法，为相关领域的安全研究奠定了基础。

Conclusion: 跨域多智能体LLM系统的安全问题是一个新兴且复杂的领域，需要进一步的研究和技术创新以确保数据和政策的合规性。

Abstract: Large language models (LLMs) are rapidly evolving into autonomous agents that
cooperate across organizational boundaries, enabling joint disaster response,
supply-chain optimization, and other tasks that demand decentralized expertise
without surrendering data ownership. Yet, cross-domain collaboration shatters
the unified trust assumptions behind current alignment and containment
techniques. An agent benign in isolation may, when receiving messages from an
untrusted peer, leak secrets or violate policy, producing risks driven by
emergent multi-agent dynamics rather than classical software bugs. This
position paper maps the security agenda for cross-domain multi-agent LLM
systems. We introduce seven categories of novel security challenges, for each
of which we also present plausible attacks, security evaluation metrics, and
future research guidelines.

</details>


### [430] [CADRE: Customizable Assurance of Data Readiness in Privacy-Preserving Federated Learning](https://arxiv.org/abs/2505.23849)
*Kaveen Hiniduma,Zilinghan Li,Aditya Sinha,Ravi Madduri,Suren Byna*

Key words: 联邦学习, 隐私保护, 数据准备, CADRE框架

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 本文提出了CADRE框架，用于在保护隐私的联邦学习中自定义数据准备标准，通过用户定义的指标、规则和补救措施生成数据准备报告，提高联邦学习的性能和可靠性。

Motivation: 由于联邦学习中客户端数据的高质量和准备程度难以保证，且隐私保护限制了数据访问，迫切需要一种方法来确保数据准备就绪。

Method: 引入CADRE框架，允许用户根据特定联邦学习任务自定义数据准备标准、指标、规则和补救措施，并生成全面的数据准备报告。

Result: 在六个不同数据集上的实验表明，CADRE能有效解决七种数据准备问题，提升数据质量、隐私和公平性。

Conclusion: CADRE框架通过提前识别和解决数据问题，增强了联邦学习模型的性能和可靠性。

Abstract: Privacy-Preserving Federated Learning (PPFL) is a decentralized machine
learning approach where multiple clients train a model collaboratively. PPFL
preserves privacy and security of the client's data by not exchanging it.
However, ensuring that data at each client is of high quality and ready for
federated learning (FL) is a challenge due to restricted data access. In this
paper, we introduce CADRE (Customizable Assurance of Data REadiness) for FL, a
novel framework that allows users to define custom data readiness (DR)
standards, metrics, rules, and remedies tailored to specific FL tasks. Our
framework generates comprehensive DR reports based on the user-defined metrics,
rules, and remedies to ensure datasets are optimally prepared for FL while
preserving privacy. We demonstrate the framework's practical application by
integrating it into an existing PPFL framework. We conducted experiments across
six diverse datasets, addressing seven different DR issues. The results
illustrate the framework's versatility and effectiveness in ensuring DR across
various dimensions, including data quality, privacy, and fairness. This
approach enhances the performance and reliability of FL models as well as
utilizes valuable resources by identifying and addressing data-related issues
before the training phase.

</details>


### [431] [KGMark: A Diffusion Watermark for Knowledge Graphs](https://arxiv.org/abs/2505.23873)
*Hongrui Peng,Haolang Lu,Yuanlong Yu,Weiye Fu,Kun Wang,Guoshun Nan*

Key words: 知识图谱, 水印, 动态图谱, 聚类对齐, 冗余嵌入

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 提出首个动态知识图谱水印框架KGMARK，通过聚类对齐、冗余嵌入和可学习掩码矩阵解决时空变化问题。

Motivation: 现有水印方法难以应对动态图谱的时空变化，需保护知识产权并防止AI生成内容的潜在危害。

Method: 使用聚类对齐方法适应空间变化，冗余嵌入策略增强抗攻击能力，可学习掩码矩阵提升透明度。

Result: 在公开基准测试中展示了KGMARK的有效性。

Conclusion: KGMARK成功解决了结构化数据的时空变化挑战。

Abstract: Knowledge graphs (KGs) are ubiquitous in numerous real-world applications,
and watermarking facilitates protecting intellectual property and preventing
potential harm from AI-generated content. Existing watermarking methods mainly
focus on static plain text or image data, while they can hardly be applied to
dynamic graphs due to spatial and temporal variations of structured data. This
motivates us to propose KGMARK, the first graph watermarking framework that
aims to generate robust, detectable, and transparent diffusion fingerprints for
dynamic KG data. Specifically, we propose a novel clustering-based alignment
method to adapt the watermark to spatial variations. Meanwhile, we present a
redundant embedding strategy to harden the diffusion watermark against various
attacks, facilitating the robustness of the watermark to the temporal
variations. Additionally, we introduce a novel learnable mask matrix to improve
the transparency of diffusion fingerprints. By doing so, our KGMARK properly
tackles the variation challenges of structured data. Experiments on various
public benchmarks show the effectiveness of our proposed KGMARK.

</details>


### [432] [Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention](https://arxiv.org/abs/2505.23968)
*Stephan Rabanser,Ali Shahin Shamsabadi,Olive Franzese,Xiao Wang,Adrian Weller,Nicolas Papernot*

Key words: 谨慎预测, Mirage攻击, Confidential Guardian, 零知识证明, 模型安全性

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 本文探讨了机器学习模型在不确定性时谨慎预测可能被滥用的问题，并提出了一种检测和防止这种攻击的框架。

Motivation: 防止谨慎预测机制被恶意机构利用以歧视或拒绝服务，确保模型的公平性和安全性。

Method: 提出Mirage攻击来模拟问题，并开发Confidential Guardian框架，通过校准指标和零知识证明检测虚假信心抑制。

Result: Confidential Guardian能有效检测和防止恶意滥用，确保模型的置信度反映真实不确定性。

Conclusion: 该框架为谨慎预测提供了可验证的保障，防止其被滥用。

Abstract: Cautious predictions -- where a machine learning model abstains when
uncertain -- are crucial for limiting harmful errors in safety-critical
applications. In this work, we identify a novel threat: a dishonest institution
can exploit these mechanisms to discriminate or unjustly deny services under
the guise of uncertainty. We demonstrate the practicality of this threat by
introducing an uncertainty-inducing attack called Mirage, which deliberately
reduces confidence in targeted input regions, thereby covertly disadvantaging
specific individuals. At the same time, Mirage maintains high predictive
performance across all data points. To counter this threat, we propose
Confidential Guardian, a framework that analyzes calibration metrics on a
reference dataset to detect artificially suppressed confidence. Additionally,
it employs zero-knowledge proofs of verified inference to ensure that reported
confidence scores genuinely originate from the deployed model. This prevents
the provider from fabricating arbitrary model confidence values while
protecting the model's proprietary details. Our results confirm that
Confidential Guardian effectively prevents the misuse of cautious predictions,
providing verifiable assurances that abstention reflects genuine model
uncertainty rather than malicious intent.

</details>


### [433] [LLM Agents Should Employ Security Principles](https://arxiv.org/abs/2505.24019)
*Kaiyuan Zhang,Zian Su,Pin-Yu Chen,Elisa Bertino,Xiangyu Zhang,Ninghui Li*

Key words: LLM代理, 信息安全, 隐私保护, AgentSandbox, 安全设计原则

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 该立场论文提出将信息安全的设计原则应用于大规模部署的LLM代理，以解决隐私泄漏和系统漏洞问题。作者提出了AgentSandbox框架，通过嵌入安全原则来保护代理的整个生命周期。

Motivation: 由于多代理交互和上下文操纵（如提示注入）带来的隐私泄漏和系统漏洞问题，作者主张采用信息安全的设计原则来保障LLM代理的安全性。

Method: 提出AgentSandbox框架，嵌入信息安全原则（如深度防御、最小权限等），并通过在先进LLM上的三个维度（良性效用、攻击效用和攻击成功率）进行评估。

Result: AgentSandbox在功能和对抗性评估中保持高效用，同时显著降低了隐私风险。

Conclusion: 通过将安全设计原则嵌入LLM代理协议，可以构建与用户隐私期望和监管要求一致的可信代理生态系统。

Abstract: Large Language Model (LLM) agents show considerable promise for automating
complex tasks using contextual reasoning; however, interactions involving
multiple agents and the system's susceptibility to prompt injection and other
forms of context manipulation introduce new vulnerabilities related to privacy
leakage and system exploitation. This position paper argues that the
well-established design principles in information security, which are commonly
referred to as security principles, should be employed when deploying LLM
agents at scale. Design principles such as defense-in-depth, least privilege,
complete mediation, and psychological acceptability have helped guide the
design of mechanisms for securing information systems over the last five
decades, and we argue that their explicit and conscientious adoption will help
secure agentic systems. To illustrate this approach, we introduce AgentSandbox,
a conceptual framework embedding these security principles to provide
safeguards throughout an agent's life-cycle. We evaluate with state-of-the-art
LLMs along three dimensions: benign utility, attack utility, and attack success
rate. AgentSandbox maintains high utility for its intended functions under both
benign and adversarial evaluations while substantially mitigating privacy
risks. By embedding secure design principles as foundational elements within
emerging LLM agent protocols, we aim to promote trustworthy agent ecosystems
aligned with user privacy expectations and evolving regulatory requirements.

</details>


### [434] [A Reward-driven Automated Webshell Malicious-code Generator for Red-teaming](https://arxiv.org/abs/2505.24252)
*Yizhong Ding*

Key words: WebShell, 恶意代码生成, 强化学习, 红队, 逃逸效果

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出了一种奖励驱动的自动化WebShell恶意代码生成器RAWG，用于红队应用，通过分类样本、语言模型处理及强化学习优化，显著提升了恶意代码的多样性和逃逸效果。

Motivation: 针对现有恶意代码生成方法多样性和冗余问题，以及缺乏公开分类数据集的需求，研究旨在提高WebShell恶意代码的生成质量和逃逸能力。

Method: 方法包括分类样本、利用LLM标准化关键令牌、监督微调开源大模型，并应用PPO进行强化学习优化。

Result: 实验显示，RAWG在多样性和逃逸效果上显著优于现有方法。

Conclusion: RAWG为红队操作提供了一种高效、多样化的WebShell恶意代码生成方案。

Abstract: Frequent cyber-attacks have elevated WebShell exploitation and defense to a
critical research focus within network security. However, there remains a
significant shortage of publicly available, well-categorized malicious-code
datasets organized by obfuscation method. Existing malicious-code generation
methods, which primarily rely on prompt engineering, often suffer from limited
diversity and high redundancy in the payloads they produce. To address these
limitations, we propose \textbf{RAWG}, a \textbf{R}eward-driven
\textbf{A}utomated \textbf{W}ebshell Malicious-code \textbf{G}enerator designed
for red-teaming applications. Our approach begins by categorizing webshell
samples from common datasets into seven distinct types of obfuscation. We then
employ a large language model (LLM) to extract and normalize key tokens from
each sample, creating a standardized, high-quality corpus. Using this curated
dataset, we perform supervised fine-tuning (SFT) on an open-source large model
to enable the generation of diverse, highly obfuscated webshell malicious
payloads. To further enhance generation quality, we apply Proximal Policy
Optimization (PPO), treating malicious-code samples as "chosen" data and benign
code as "rejected" data during reinforcement learning. Extensive experiments
demonstrate that RAWG significantly outperforms current state-of-the-art
methods in both payload diversity and escape effectiveness.

</details>


### [435] [LPASS: Linear Probes as Stepping Stones for vulnerability detection using compressed LLMs](https://arxiv.org/abs/2505.24451)
*Luis Ibanez-Lissen,Lorena Gonzalez-Manzano,Jose Maria de Fuentes,Nicolas Anciaux*

Key words: Large Language Models, Linear Probes, Vulnerability Detection, Model Compression, LPASS

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出了一种名为LPASS的方法，通过线性探针（LPs）在早期阶段（微调前）估计压缩后LLM的性能，并确定层剪枝的截断点。实验显示该方法在BERT和Gemma模型上对12种高危漏洞检测有效，能显著减少计算资源并保持性能。

Motivation: 为了提高大型语言模型（LLM）在漏洞检测中的效率和效果，避免高昂的计算成本，研究如何利用线性探针（LPs）在压缩和微调前预估性能。

Method: 提出LPASS方法，利用线性探针（LPs）在早期阶段估计压缩后模型的性能，并确定层剪枝的截断点。应用于BERT和Gemma模型，检测C/C++代码中的高危漏洞。

Result: 实验显示，LPASS方法可移除33.3%至72.2%的层而不损失精度，预估值误差最低3%，平均8.68%。压缩后的Gemma模型性能提升1.6% F1-score，并节省29.4%训练时间、23.8%推理时间和42.98%模型大小。

Conclusion: LPASS方法通过线性探针早期预估性能，显著降低了计算成本，压缩后的模型在漏洞检测中表现优于现有技术。

Abstract: Large Language Models (LLMs) are being extensively used for cybersecurity
purposes. One of them is the detection of vulnerable codes. For the sake of
efficiency and effectiveness, compression and fine-tuning techniques are being
developed, respectively. However, they involve spending substantial
computational efforts. In this vein, we analyse how Linear Probes (LPs) can be
used to provide an estimation on the performance of a compressed LLM at an
early phase -- before fine-tuning. We also show their suitability to set the
cut-off point when applying layer pruning compression. Our approach, dubbed
$LPASS$, is applied in BERT and Gemma for the detection of 12 of MITRE's Top 25
most dangerous vulnerabilities on 480k C/C++ samples. LPs can be computed in
142.97 s. and provide key findings: (1) 33.3 \% and 72.2\% of layers can be
removed, respectively, with no precision loss; (2) they provide an early
estimate of the post-fine-tuning and post-compression model effectiveness, with
3\% and 8.68\% as the lowest and average precision errors, respectively.
$LPASS$-based LLMs outperform the state of the art, reaching 86.9\% of accuracy
in multi-class vulnerability detection. Interestingly, $LPASS$-based compressed
versions of Gemma outperform the original ones by 1.6\% of F1-score at a
maximum while saving 29.4 \% and 23.8\% of training and inference time and
42.98\% of model size.

</details>


### [436] [CHIP: Chameleon Hash-based Irreversible Passport for Robust Deep Model Ownership Verification and Active Usage Control](https://arxiv.org/abs/2505.24536)
*Chaohui Xu,Qi Cui,Chip-Hong Chang*

Key words: DNN, IP protection, chameleon hash, watermarking, MLaaS

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出CHIP框架，用于全面保护DNN模型IP，结合变色龙哈希函数实现强所有权证明和高保真度。

Motivation: 大规模DNN的高昂训练成本使其IP保护至关重要，现有方法无法提供全方位保护。

Method: 利用密码学变色龙哈希函数，设计CHIP框架，支持所有权声明和用户追踪。

Result: 在四个数据集和两种架构上验证了CHIP的全面保护能力和鲁棒性。

Conclusion: CHIP为DNN模型IP保护提供了高效、安全的解决方案。

Abstract: The pervasion of large-scale Deep Neural Networks (DNNs) and their enormous
training costs make their intellectual property (IP) protection of paramount
importance. Recently introduced passport-based methods attempt to steer DNN
watermarking towards strengthening ownership verification against ambiguity
attacks by modulating the affine parameters of normalization layers.
Unfortunately, neither watermarking nor passport-based methods provide a
holistic protection with robust ownership proof, high fidelity, active usage
authorization and user traceability for offline access distributed models and
multi-user Machine-Learning as a Service (MLaaS) cloud model. In this paper, we
propose a Chameleon Hash-based Irreversible Passport (CHIP) protection
framework that utilizes the cryptographic chameleon hash function to achieve
all these goals. The collision-resistant property of chameleon hash allows for
strong model ownership claim upon IP infringement and liable user traceability,
while the trapdoor-collision property enables hashing of multiple user
passports and licensee certificates to the same immutable signature to realize
active usage control. Using the owner passport as an oracle, multiple
user-specific triplets, each contains a passport-aware user model, a user
passport, and a licensee certificate can be created for secure offline
distribution. The watermarked master model can also be deployed for MLaaS with
usage permission verifiable by the provision of any trapdoor-colliding user
passports. CHIP is extensively evaluated on four datasets and two architectures
to demonstrate its protection versatility and robustness. Our code is released
at https://github.com/Dshm212/CHIP.

</details>


### [437] [PatchDEMUX: A Certifiably Robust Framework for Multi-label Classifiers Against Adversarial Patches](https://arxiv.org/abs/2505.24703)
*Dennis Jacob,Chong Xiang,Prateek Mittal*

Key words: 对抗补丁攻击,多标签分类,可证明鲁棒性,深度学习,计算机视觉

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: PatchDEMUX是一个可证明对抗补丁攻击具有鲁棒性的多标签分类框架，通过将多标签分类任务分解为多个独立的二分类问题来保证鲁棒性。

Motivation: 现有的深度学习模型在计算机视觉任务中表现优异，但易受对抗补丁攻击。虽然有单标签分类的可证明防御方法，但在多标签分类领域的研究较少。

Method: 通过将多标签分类任务视为一系列独立的二分类问题，并扩展现有的单标签分类可证明防御方法（如PatchCleanser），提供更严格的鲁棒性保证。

Result: PatchDEMUX在MS-COCO和PASCAL VOC数据集上实现了显著的非平凡鲁棒性，同时保持了高清洁性能。

Conclusion: PatchDEMUX为多标签分类任务提供了一种可证明鲁棒性的通用解决方案，填补了当前防御方法的空白。

Abstract: Deep learning techniques have enabled vast improvements in computer vision
technologies. Nevertheless, these models are vulnerable to adversarial patch
attacks which catastrophically impair performance. The physically realizable
nature of these attacks calls for certifiable defenses, which feature provable
guarantees on robustness. While certifiable defenses have been successfully
applied to single-label classification, limited work has been done for
multi-label classification. In this work, we present PatchDEMUX, a certifiably
robust framework for multi-label classifiers against adversarial patches. Our
approach is a generalizable method which can extend any existing certifiable
defense for single-label classification; this is done by considering the
multi-label classification task as a series of isolated binary classification
problems to provably guarantee robustness. Furthermore, in the scenario where
an attacker is limited to a single patch we propose an additional certification
procedure that can provide tighter robustness bounds. Using the current
state-of-the-art (SOTA) single-label certifiable defense PatchCleanser as a
backbone, we find that PatchDEMUX can achieve non-trivial robustness on the
MS-COCO and PASCAL VOC datasets while maintaining high clean performance

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [438] [SkewRoute: Training-Free LLM Routing for Knowledge Graph Retrieval-Augmented Generation via Score Skewness of Retrieved Context](https://arxiv.org/abs/2505.23841)
*Hairu Wang,Yuan Feng,Yukun Cao,Xike Xie,S Kevin Zhou*

Key words: KG-RAG, LLM路由, 检索增强生成, 无训练方法, 推理成本优化

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 提出了一种针对KG-RAG的无训练路由框架，通过检索评分分布判断查询难度，减少对大模型的调用，降低推理成本50%且不牺牲效果

Motivation: 现有KG-RAG系统因检索大量知识上下文增加了推理成本，而传统基于训练的路由方法难以扩展，亟需一种无需训练、高效的路由方案

Method: 利用检索评分器输出的分数分布与查询难度的强相关性，设计免训练路由框架，动态分配简单查询给小模型、复杂查询给大模型

Result: 实验表明，该方法减少50%对大模型的调用，同时保持响应质量

Conclusion: 该框架为KG-RAG提供了即插即用的高效路由方案，可实现LLM的低成本可扩展部署

Abstract: Large language models excel at many tasks but often incur high inference
costs during deployment. To mitigate hallucination, many systems use a
knowledge graph to enhance retrieval-augmented generation (KG-RAG). However,
the large amount of retrieved knowledge contexts increase these inference costs
further. A promising solution to balance performance and cost is LLM routing,
which directs simple queries to smaller LLMs and complex ones to larger LLMs.
However, no dedicated routing methods currently exist for RAG, and existing
training-based routers face challenges scaling to this domain due to the need
for extensive training data. We observe that the score distributions produced
by the retrieval scorer strongly correlate with query difficulty. Based on
this, we propose a novel, training-free routing framework, the first tailored
to KG-RAG that effectively balances performance and cost in a plug-and-play
manner. Experiments show our method reduces calls to larger LLMs by up to 50%
without sacrificing response quality, demonstrating its potential for efficient
and scalable LLM deployment.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [439] [Aligning Protein Conformation Ensemble Generation with Physical Feedback](https://arxiv.org/abs/2505.24203)
*Jiarui Lu,Xiaoyin Chen,Stephen Zhewen Lu,Aurélie Lozano,Vijil Chenthamarakshan,Payel Das,Jian Tang*

Key words: 蛋白质动态, 生成模型, Energy-based Alignment, 物理监督, 结构生物学

<details>
  <summary>Details</summary>

Main category: q-bio.BM

TL;DR: 提出了一种名为Energy-based Alignment (EBA)的方法，通过将生成模型与物理模型的反馈对齐，高效校准生成模型以平衡构象态的能量差异，从而改进蛋白质动态结构的生成质量。

Motivation: 传统的蛋白质动态研究依赖于耗时的分子动力学模拟，而现有的生成模型虽然能高效预测蛋白质结构，但难以有效整合物理监督。EBA旨在解决这一问题，提升生成结构的物理合理性。

Method: 提出了Energy-based Alignment (EBA)方法，通过将生成模型与物理模型的反馈对齐，校准生成模型以平衡构象态的能量差异。

Result: 在分子动力学集合基准测试中，EBA在生成高质量蛋白质集合方面达到了最先进的性能，显著提升了生成结构的物理合理性。

Conclusion: EBA通过改进生成结构的物理合理性，不仅提升了模型预测能力，还为结构生物学和药物发现的应用提供了潜力。

Abstract: Protein dynamics play a crucial role in protein biological functions and
properties, and their traditional study typically relies on time-consuming
molecular dynamics (MD) simulations conducted in silico. Recent advances in
generative modeling, particularly denoising diffusion models, have enabled
efficient accurate protein structure prediction and conformation sampling by
learning distributions over crystallographic structures. However, effectively
integrating physical supervision into these data-driven approaches remains
challenging, as standard energy-based objectives often lead to intractable
optimization. In this paper, we introduce Energy-based Alignment (EBA), a
method that aligns generative models with feedback from physical models,
efficiently calibrating them to appropriately balance conformational states
based on their energy differences. Experimental results on the MD ensemble
benchmark demonstrate that EBA achieves state-of-the-art performance in
generating high-quality protein ensembles. By improving the physical
plausibility of generated structures, our approach enhances model predictions
and holds promise for applications in structural biology and drug discovery.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [440] [On the Parallels Between Evolutionary Theory and the State of AI](https://arxiv.org/abs/2505.23774)
*Zeki Doruk Erden,Boi Faltings*

Key words: 人工智能, 进化理论, 进化发育生物学, 设计范式

<details>
  <summary>Details</summary>

Main category: q-bio.NC

TL;DR: 本文探讨了当代AI方法的局限性，并借鉴进化发育生物学的见解，提出了一种新的AI设计范式，以克服这些限制。

Motivation: 激发这项研究的动机是认识到现有AI方法的局限性，希望通过跨学科的方式，从进化理论的进展中寻找灵感，推动AI的进一步发展。

Method: 通过比较AI与20世纪现代综合进化论的相似性，并借鉴进化发育生物学的见解，提出了一个新的AI设计路径。

Result: 研究表明，结合进化理论的进展可以为AI提供新的设计思路，帮助克服现有技术的局限性。

Conclusion: 结论指出，跨学科的视角为AI发展提供了新的可能性，未来的研究应进一步探索这一路径。

Abstract: This article critically examines the foundational principles of contemporary
AI methods, exploring the limitations that hinder its potential. We draw
parallels between the modern AI landscape and the 20th-century Modern Synthesis
in evolutionary biology, and highlight how advancements in evolutionary theory
that augmented the Modern Synthesis, particularly those of Evolutionary
Developmental Biology, offer insights that can inform a new design paradigm for
AI. By synthesizing findings across AI and evolutionary theory, we propose a
pathway to overcome existing limitations, enabling AI to achieve its
aspirational goals.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [441] [Multi-output Classification using a Cross-talk Architecture for Compound Fault Diagnosis of Motors in Partially Labeled Condition](https://arxiv.org/abs/2505.24001)
*Wonjun Yi,Wonho Jung,Kangmin Jang,Yong-Hwa Park*

Key words: 旋转机械，复合故障诊断，领域适应，多输出分类，频率层归一化

<details>
  <summary>Details</summary>

Main category: eess.SP

TL;DR: 论文提出了一种针对部分标记目标数据集的多输出分类框架，用于旋转机械复合故障诊断，优于传统方法。

Motivation: 旋转机械日益复杂且工况多样，导致复合故障诊断在领域适应中面临挑战。

Method: 设计了新的多输出分类框架和交叉对话层结构，结合频率层归一化提升领域适应性能。

Result: 实验表明，该方法在六种领域适应场景中均优于基线模型，尤其在复合故障场景中表现突出。

Conclusion: 模型通过任务间交互利用结构优势而非单纯增加参数，显著提升了复合故障诊断性能。

Abstract: The increasing complexity of rotating machinery and the diversity of
operating conditions, such as rotating speed and varying torques, have
amplified the challenges in fault diagnosis in scenarios requiring domain
adaptation, particularly involving compound faults. This study addresses these
challenges by introducing a novel multi-output classification (MOC) framework
tailored for domain adaptation in partially labeled (PL) target datasets.
Unlike conventional multi-class classification (MCC) approaches, the proposed
MOC framework classifies the severity levels of compound faults simultaneously.
Furthermore, we explore various single-task and multi-task architectures
applicable to the MOC formulation-including shared trunk and cross-talk-based
designs-for compound fault diagnosis under PL conditions. Based on this
investigation, we propose a novel cross-talk layer structure that enables
selective information sharing across diagnostic tasks, effectively enhancing
classification performance in compound fault scenarios. In addition,
frequency-layer normalization was incorporated to improve domain adaptation
performance on motor vibration data. Compound fault conditions were implemented
using a motor-based test setup, and the proposed model was evaluated across six
domain adaptation scenarios. The experimental results demonstrate its superior
macro F1 performance compared to baseline models. We further showed that the
proposed mode's structural advantage is more pronounced in compound fault
settings through a single-fault comparison. We also found that frequency-layer
normalization fits the fault diagnosis task better than conventional methods.
Lastly, we discuss that this improvement primarily stems from the model's
structural ability to leverage inter-fault classification task interactions,
rather than from a simple increase in model parameters.

</details>


### [442] [Real-time Fall Prevention system for the Next-generation of Workers](https://arxiv.org/abs/2505.24487)
*Nicholas Cartocci,Antonios E. Gkikakis,Darwin G. Caldwell,Jesús Ortiz*

Key words: 跌倒检测, 跌倒预防, 工业安全, 深度学习, 可穿戴设备

<details>
  <summary>Details</summary>

Main category: eess.SP

TL;DR: 提出一种用于工业环境的混合式跌倒检测与预防系统，结合倒立摆动态模型和深度学习框架，模拟跌倒场景以生成训练数据，旨在减少跌倒伤害。

Motivation: 工业环境中健康且强壮的工作人员易在恶劣条件下跌倒，现有系统难以满足实时检测需求，需开发通用可穿戴设备以提高安全性。

Method: 使用倒立摆动态模型生成模拟跌倒数据，结合深度学习框架进行分析，触发跌倒缓解机制以防止伤害。

Result: 该方法通过模拟数据高效训练模型，适用于特定类型的跌倒场景，减少了真实实验的需求。

Conclusion: 这是迈向通用可穿戴设备的第一步，有望降低工业环境中的跌倒伤害，提升工人安全。

Abstract: Developing a general-purpose wearable real-time fall-detection system is
still a challenging task, especially for healthy and strong subjects, such as
industrial workers that work in harsh environments. In this work, we present a
hybrid approach for fall detection and prevention, which uses the dynamic model
of an inverted pendulum to generate simulations of falling that are then fed to
a deep learning framework. The output is a signal to activate a fall mitigation
mechanism when the subject is at risk of harm. The advantage of this approach
is that abstracted models can be used to efficiently generate training data for
thousands of different subjects with different falling initial conditions,
something that is practically impossible with real experiments. This approach
is suitable for a specific type of fall, where the subjects fall without
changing their initial configuration significantly, and it is the first step
toward a general-purpose wearable device, with the aim of reducing
fall-associated injuries in industrial environments, which can improve the
safety of workers.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [443] [Information-theoretic machine learning for time-varying mode decomposition of separated airfoil wakes](https://arxiv.org/abs/2505.24132)
*Kai Fukami,Ryo Araki*

Key words: 信息论模式分解、深度Sigmoidal流、非定常空气动力学、分离流、升力响应

<details>
  <summary>Details</summary>

Main category: physics.flu-dyn

TL;DR: 该论文提出了一种基于信息论的模式分解方法，利用深度Sigmoidal流神经网络从机翼周围的分离尾流中提取信息模态结构，捕捉了与未来目标变量相关的因果关系，并通过三个例子验证了方法的有效性。

Motivation: 研究动机在于通过数据驱动的方法，从复杂的流体动力学现象中识别出与特定目标（如升力响应）相关的信息结构，以理解和预测非定常空气动力学问题中的因果关系。

Method: 论文采用深度Sigmoidal流神经网络进行模式分解，从流场快照中提取与未来时间点的目标变量相关的信息模态结构，并通过三个不同类型的分离流（周期性尾流、涡-机翼相互作用和湍流尾流）验证方法的普适性。

Result: 结果表明，该方法能够成功识别与升力响应相关的时间变化模态结构。对于周期性尾流，信息结构随时间变化；对于涡-机翼相互作用，揭示了涡对机翼升力的影响；对于湍流尾流，无需先验知识即可识别机翼附近和涡核的信息结构。

Conclusion: 该研究提供了一种基于信息论的模式分解方法，能够以可解释的方式揭示非定常空气动力学问题中的因果关系，为理解和预测复杂流场现象提供了新视角。

Abstract: We perform an information-theoretic mode decomposition for separated wakes
around a wing. The current data-driven approach based on a neural network
referred to as deep sigmoidal flow enables the extraction of an informative
component from a given flow field snapshot with respect to a target variable at
a future time stamp, thereby capturing the causality as a time-varying modal
structure. We consider three examples of separated flows around a NACA0012
airfoil, namely, 1. laminar periodic wake at post-stall angles of attack, 2.
strong vortex-airfoil interactions, and 3. a turbulent wake in a
spanwise-periodic domain. The present approach reveals informative vortical
structures associated with a time-varying lift response. For the periodic
shedding cases, the informative structures vary in time corresponding to the
fluctuation level from their mean values. With the second example of
vortex-airfoil interactions, how the effect of vortex gust on a wing emerges in
the lift response over time is identified in an interpretable manner.
Furthermore, for the case of turbulent wake, the present model highlights
structures near the wing and vortex cores as informative components based
solely on the information metric without any prior knowledge of aerodynamics
and length scales. This study provides causality-based insights into a range of
unsteady aerodynamic problems.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [444] [A New Deep-learning-Based Approach For mRNA Optimization: High Fidelity, Computation Efficiency, and Multiple Optimization Factors](https://arxiv.org/abs/2505.23862)
*Zheng Gong,Ziyi Jiang,Weihao Gao,Deng Zhuo,Lan Ma*

Key words: mRNA优化, 深度学习, 多目标优化, 蛋白质表达, 编码保真度

<details>
  <summary>Details</summary>

Main category: q-bio.QM

TL;DR: RNop是一种基于深度学习的新型mRNA优化方法，通过多目标优化和专用损失函数，显著提高蛋白质表达水平，同时兼顾计算效率和序列保真度。

Motivation: 现有mRNA优化方法在保真度、计算效率和多目标优化方面存在不足，且未能全面整合mRNA生命周期和翻译过程的关键因素。

Method: 提出RNop方法，利用包含300多万序列的大规模数据集和四种专用损失函数（GPLoss、CAILoss、tAILoss和MFELoss），优化物种特异性密码子适应、tRNA可用性和mRNA二级结构特征。

Result: RNop在计算效率上达到47.32序列/秒，实验验证其优化的mRNA序列显著提高功能蛋白表达水平。

Conclusion: RNop在定量指标和实验验证中均优于现有方法，为高效mRNA设计开辟了新途径。

Abstract: The mRNA optimization is critical for therapeutic and biotechnological
applications, since sequence features directly govern protein expression levels
and efficacy. However, current methods face significant challenges in
simultaneously achieving three key objectives: (1) fidelity (preventing
unintended amino acid changes), (2) computational efficiency (speed and
scalability), and (3) the scope of optimization variables considered
(multi-objective capability). Furthermore, existing methods often fall short of
comprehensively incorporating the factors related to the mRNA lifecycle and
translation process, including intrinsic mRNA sequence properties, secondary
structure, translation elongation kinetics, and tRNA availability. To address
these limitations, we introduce \textbf{RNop}, a novel deep learning-based
method for mRNA optimization. We collect a large-scale dataset containing over
3 million sequences and design four specialized loss functions, the GPLoss,
CAILoss, tAILoss, and MFELoss, which simultaneously enable explicit control
over sequence fidelity while optimizing species-specific codon adaptation, tRNA
availability, and desirable mRNA secondary structure features. Then, we
demonstrate RNop's effectiveness through extensive in silico and in vivo
experiments. RNop ensures high sequence fidelity, achieves significant
computational throughput up to 47.32 sequences/s, and yields optimized mRNA
sequences resulting in a significant increase in protein expression for
functional proteins compared to controls. RNop surpasses current methodologies
in both quantitative metrics and experimental validation, enlightening a new
dawn for efficient and effective mRNA design. Code and models will be available
at https://github.com/HudenJear/RPLoss.

</details>


### [445] [Unsupervised Evolutionary Cell Type Matching via Entropy-Minimized Optimal Transport](https://arxiv.org/abs/2505.24759)
*Mu Qiao*

Key words: 最优传输、跨物种匹配、无监督学习、细胞类型进化

<details>
  <summary>Details</summary>

Main category: q-bio.QM

TL;DR: OT-MESH是一种无监督计算框架，利用熵正则化最优传输方法跨物种匹配细胞类型。

Motivation: 现有方法要么依赖基于参考的不对称匹配，要么计算复杂度高且生物解释性差，需要一种更优解决方案。

Method: 结合MESH技术优化OT计划，选择高信噪比基因构建成本矩阵，并通过迭代优化提升稀疏性和可解释性。

Result: 在鼠和猕猴的视网膜细胞中准确发现已知和新颖的进化关系，部分结果已实验验证。

Conclusion: OT-MESH为跨物种细胞类型映射提供了可扩展、对称且解释性强的解决方案。

Abstract: Identifying evolutionary correspondences between cell types across species is
a fundamental challenge in comparative genomics and evolutionary biology.
Existing approaches often rely on either reference-based matching, which
imposes asymmetry by designating one species as the reference, or
projection-based matching, which may increase computational complexity and
obscure biological interpretability at the cell-type level. Here, we present
OT-MESH, an unsupervised computational framework leveraging entropy-regularized
optimal transport (OT) to systematically determine cross-species cell type
homologies. Our method uniquely integrates the Minimize Entropy of Sinkhorn
(MESH) technique to refine the OT plan. It begins by selecting genes with high
Signal-to-Noise Ratio (SNR) to capture the most informative features, from
which a cost matrix is constructed using cosine distances between cell-type
centroids. Importantly, the MESH procedure iteratively refines the cost matrix,
leading to a transport plan with significantly enhanced sparsity and
interpretability of the resulting correspondence matrices. Applied to retinal
bipolar cells (BCs) and retinal ganglion cells (RGCs) from mouse and macaque,
OT-MESH accurately recovers known evolutionary relationships and uncovers novel
correspondences, one of which was independently validated experimentally. Thus,
our framework offers a principled, scalable, symmetric, and interpretable
solution for evolutionary cell type mapping, facilitating deeper insights into
cellular specialization and conservation across species.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [446] [Exploring Domain Wall Pinning in Ferroelectrics via Automated High Throughput AFM](https://arxiv.org/abs/2505.24062)
*Kamyar Barakati,Yu Liu,Hiroshi Funakubo,Sergei V. Kalinin*

Key words: 畴壁动力学, 铁电材料, 压电力显微镜, 机器学习, 预测性映射

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

TL;DR: 该论文研究了铁电材料中的畴壁动力学，通过机器学习控制的自动压电力显微镜技术分析了畴壁位移与电场参数及局部铁电-铁弹构型的关系，并提出了预测性映射方法。

Motivation: 传统的畴壁钉扎研究依赖于耗时的人工实验和稀疏的扫描探针显微镜技术，需要一种高效、自动化的方法来量化畴壁动力学。

Method: 使用ML控制的自动压电力显微镜技术（Piezoresponse Force Microscopy）对大面积外延PbTiO3薄膜进行1500次切换事件分析，研究畴壁位移与电场参数及局部构型的关系。

Result: 发现畴壁位移不仅依赖于电场参数，还与局部铁电-铁弹构型相关，如双晶界在特定偏置下保持钉扎，而单变体边界在较低偏置下即被激活。

Conclusion: 通过自动化高通量AFM工作流程，提出了将畴构型与脉冲参数关联的预测性映射方法，为铁电存储器设计奠定了基础。

Abstract: Domain-wall dynamics in ferroelectric materials are strongly
position-dependent since each polar interface is locked into a unique local
microstructure. This necessitates spatially resolved studies of the
wall-pinning using scanning-probe microscopy techniques. The pinning centers
and preexisting domain walls are usually sparse within image plane, precluding
the use of dense hyperspectral imaging modes and requiring time-consuming human
experimentation. Here, a large area epitaxial PbTiO$_3$ film on cubic KTaO$_3$
were investigated to quantify the electric field driven dynamics of the
polar-strain domain structures using ML-controlled automated Piezoresponse
Force Microscopy. Analysis of 1500 switching events reveals that domain wall
displacement depends not only on field parameters but also on the local
ferroelectric-ferroelastic configuration. For example, twin boundaries in
polydomains regions like a$_1^-$/$c^+$ $\parallel$ a$_2^-$/$c^-$ stay pinned up
to a certain level of bias magnitude and change only marginally as the bias
increases from 20V to 30V, whereas single variant boundaries like a$_2^+$/$c^+$
$\parallel$ a$_2^-$/$c^-$ stack are already activated at 20V. These statistics
on the possible ferroelectric and ferroelastic wall orientations, together with
the automated, high-throughput AFM workflow, can be distilled into a predictive
map that links domain configurations to pulse parameters. This
microstructure-specific rule set forms the foundation for designing
ferroelectric memories.

</details>


### [447] [Cross-Modal Characterization of Thin Film MoS$_2$ Using Generative Models](https://arxiv.org/abs/2505.24065)
*Isaiah A. Moses,Chen Chen,Joan M. Redwing,Wesley F. Reinhart*

Key words: 机器学习, 材料表征, 跨模态, 生成模型, 原子力显微镜, 拉曼光谱

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

TL;DR: 利用机器学习（ML）从历史数据中指导材料生长与表征，提高效率。研究探讨了通过拉曼光谱数据预测原子力显微镜（AFM）定量指标的可行性，并训练生成模型实现了光谱与AFM图像的特征互生成。

Motivation: 传统材料生长与表征依赖专家经验和大量资源，机器学习可借助历史数据提升效率并降低成本。

Method: 使用生成模型从拉曼光谱和光致发光光谱数据中生成AFM图像特征，反之亦然。

Result: 实验结果显示ML在跨模态表征材料方面具有潜力，为加速材料发现提供了基础。

Conclusion: 机器学习在材料表征中展现出加速、高效且经济的潜力，为未来研究奠定了基础。

Abstract: The growth and characterization of materials using empirical optimization
typically requires a significant amount of expert time, experience, and
resources. Several complementary characterization methods are routinely
performed to determine the quality and properties of a grown sample. Machine
learning (ML) can support the conventional approaches by using historical data
to guide and provide speed and efficiency to the growth and characterization of
materials. Specifically, ML can provide quantitative information from
characterization data that is typically obtained from a different modality. In
this study, we have investigated the feasibility of projecting the quantitative
metric from microscopy measurements, such as atomic force microscopy (AFM),
using data obtained from spectroscopy measurements, like Raman spectroscopy.
Generative models were also trained to generate the full and specific features
of the Raman and photoluminescence spectra from each other and the AFM images
of the thin film MoS$_2$. The results are promising and have provided a
foundational guide for the use of ML for the cross-modal characterization of
materials for their accelerated, efficient, and cost-effective discovery.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [448] [Distributed gradient methods under heavy-tailed communication noise](https://arxiv.org/abs/2505.24464)
*Manojlo Vukovic,Dusan Jakovetic,Dragana Bajovic,Soummya Kar*

Key words: 分布式优化、重尾噪声、梯度型方法、均方误差、无线传感器网络

<details>
  <summary>Details</summary>

Main category: math.OC

TL;DR: 该论文首次提出并分析了一种分布式梯度型方法，用于解决在通信中存在重尾噪声的网络节点协同优化问题。该方法通过精心设计的时变步长和非线性算子，成功在均方误差意义下收敛到解附近。

Motivation: 在密集部署的无线传感器和物联网网络中，节点间通信常受重尾噪声影响，现有方法无法处理无限方差噪声，亟需一种新的分布式优化方法。

Method: 设计了一种分布式梯度型方法，采用时变步长平衡共识和梯度更新，并结合非线性算子限制重尾噪声的影响。

Result: 在异质强凸成本函数下，方法在均方误差意义下收敛到解附近，且渐近均方误差可通过调整步长任意减小。数值实验验证了方法对重尾噪声的鲁棒性。

Conclusion: 所提方法在重尾噪声环境下有效，优于现有仅适用于有限方差噪声的方法，为实际应用提供了可靠解决方案。

Abstract: We consider a standard distributed optimization problem in which networked
nodes collaboratively minimize the sum of their locally known convex costs. For
this setting, we address for the first time the fundamental problem of design
and analysis of distributed methods to solve the above problem when inter-node
communication is subject to \emph{heavy-tailed} noise. Heavy-tailed noise is
highly relevant and frequently arises in densely deployed wireless sensor and
Internet of Things (IoT) networks. Specifically, we design a distributed
gradient-type method that features a carefully balanced mixed time-scale
time-varying consensus and gradient contribution step sizes and a bounded
nonlinear operator on the consensus update to limit the effect of heavy-tailed
noise. Assuming heterogeneous strongly convex local costs with mutually
different minimizers that are arbitrarily far apart, we show that the proposed
method converges to a neighborhood of the network-wide problem solution in the
mean squared error (MSE) sense, and we also characterize the corresponding
convergence rate. We further show that the asymptotic MSE can be made
arbitrarily small through consensus step-size tuning, possibly at the cost of
slowing down the transient error decay. Numerical experiments corroborate our
findings and demonstrate the resilience of the proposed method to heavy-tailed
(and infinite variance) communication noise. They also show that existing
distributed methods, designed for finite-communication-noise-variance settings,
fail in the presence of infinite variance noise.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [449] [Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised Learning](https://arxiv.org/abs/2505.23783)
*Korel Gundem,Juncheng Dong,Dennis Zhang,Vahid Tarokh,Zhengling Qi*

Key words: 监督校准（SC）、上下文学习（ICL）、大型语言模型（LLM）、偏差修正、正则化

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出了一种基于监督校准（SC）的框架，通过优化每类的仿射变换来解决大型语言模型（LLM）在上下文学习（ICL）中的系统性偏差问题，无需额外数据，并在多数据集上表现优异。

Motivation: 大型语言模型在上下文学习中存在系统性偏差，导致分类性能不稳定，现有校准方法仅在logit空间内平移决策边界，无法调整方向，限制了其效果。

Method: 提出监督校准（SC），通过学习LLM预测概率的每类最优仿射变换，整合上下文不变性和方向信任域两种正则化技术，提升校准能力。

Result: SC在Mistral-7B等三种模型和九种数据集上，4-shot至16-shot设置中均优于现有校准基线方法。

Conclusion: SC通过更灵活的函数类别和正则化技术，显著提升了校准效果，为LLM的上下文学习提供了统一且高效的校准框架。

Abstract: In-Context Learning (ICL) allows Large Language Models (LLMs) to adapt to new
tasks with just a few examples, but their predictions often suffer from
systematic biases, leading to unstable performances in classification. While
calibration techniques are proposed to mitigate these biases, we show that, in
the logit space, many of these methods are equivalent to merely shifting the
LLM's decision boundary without having the ability to alter its orientation.
This proves inadequate when biases cause the LLM to be severely misdirected. To
address these limitations and provide a unifying framework, we propose
Supervised Calibration (SC), a loss-minimization based framework which learns
an optimal, per-class affine transformation of the LLM's predictive
probabilities in the logit space without requiring external data beyond the
context. By using a more expressive functional class, SC not only subsumes many
existing calibration methods in ICL as special cases, but also enables the
ability to alter and even completely reverse the orientation of the LLM's
decision boundary. Furthermore, SC's loss-based nature facilitates the seamless
integration of two purpose-built regularization techniques: context-invariance
and directional trust-region. The former is designed to tackle the instability
issue in ICL, while the latter controls the degree of calibration. Finally, SC
delivers state-of-the-art performance over calibration baselines in the 4-shot,
8-shot, and 16-shot settings across all nine datasets for
Mistral-7B-Instruct-v0.3, LLaMA-2-7B-chat, and Qwen2-7B-Instruct.

</details>


### [450] [Gibbs randomness-compression proposition: An efficient deep learning](https://arxiv.org/abs/2505.23869)
*M. Süzen*

Key words: Gibbs熵, 随机性与压缩, 层析压缩, DTC, 高效训练

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出了一个连接随机性和压缩的命题（Gibbs随机性-压缩命题），并通过Dual Tomographic Compression（DTC）框架展示了这一关系。该框架在训练中实现高效压缩，同时支持彩票假设。

Motivation: 从统计物理角度探讨随机性与压缩的关系，并通过实验验证这种关系在实际深度学习训练中的效用，以提高能源和资源效率。

Method: 提出Dual Tomographic Compression（DTC）框架，通过双端权重矩阵的层析重建和神经元级剪枝，在训练中实现智能架构搜索与压缩。

Result: DTC在训练中实现了高效的压缩和加速，达到SOTA性能。实验还表明，随机压缩-训练迭代也具有相似性能，支持了Gibbs随机性-压缩命题。

Conclusion: DTC为高效深度学习训练提供了新思路，并通过Gibbs熵从理论上连接了随机性和压缩的关系，具有重要的实践意义。

Abstract: A proposition that connects randomness and compression put forward via Gibbs
entropy over set of measurement vectors associated with a compression process.
The proposition states that a lossy compression process is equivalent to {\it
directed randomness} that preserves information content. The proposition
originated from the observed behaviour in newly proposed {\it Dual Tomographic
Compression} (DTC) compress-train framework. This is akin to tomographic
reconstruction of layer weight matrices via building compressed sensed
projections, so called {\it weight rays}. This tomographic approach is applied
to previous and next layers in a dual fashion, that triggers neuronal-level
pruning. This novel model compress-train scheme appear in iterative fashion and
act as smart neural architecture search, Experiments demonstrated utility of
this dual-tomography producing state-of-the-art performance with efficient
compression during training, accelerating and supporting lottery ticket
hypothesis. However, random compress-train iterations having similar
performance demonstrated the connection between randomness and compression from
statistical physics perspective, we formulated so called {\it Gibbs
randomness-compression proposition}, signifying randomness-compression
relationship via Gibbs entropy. Practically, DTC framework provides a promising
approach for massively energy and resource efficient deep learning training
approach.

</details>


### [451] [Conformal Object Detection by Sequential Risk Control](https://arxiv.org/abs/2505.24038)
*Léo Andéol,Luca Mossina,Adrien Mazoyer,Sébastien Gerchinovitz*

Key words: object detection, conformal prediction, sequential risk control, statistical guarantees

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 提出了一种名为SeqCRC的新方法，用于解决目标检测中的置信度问题，并通过实验验证了其有效性。

Motivation: 目标检测模型在关键应用中的可靠性不足，需要一种能够提供统计保证的后处理方法。

Method: 引入了序列化风险控制（SeqCRC）方法，扩展了CRC的统计保证，适用于目标检测任务。

Result: 通过实验验证了SeqCRC的有效性，并展示了不同应用场景下的权衡。

Conclusion: SeqCRC为关键应用中的目标检测提供了可靠的统计保证，具有实际应用价值。

Abstract: Recent advances in object detectors have led to their adoption for industrial
uses. However, their deployment in critical applications is hindered by the
inherent lack of reliability of neural networks and the complex structure of
object detection models. To address these challenges, we turn to Conformal
Prediction, a post-hoc procedure which offers statistical guarantees that are
valid for any dataset size, without requiring prior knowledge on the model or
data distribution. Our contribution is manifold: first, we formally define the
problem of Conformal Object Detection (COD) and introduce a novel method,
Sequential Conformal Risk Control (SeqCRC), that extends the statistical
guarantees of Conformal Risk Control (CRC) to two sequential tasks with two
parameters, as required in the COD setting. Then, we propose loss functions and
prediction sets suited to applying CRC to different applications and
certification requirements. Finally, we present a conformal toolkit, enabling
replication and further exploration of our methods. Using this toolkit, we
perform extensive experiments, yielding a benchmark that validates the
investigated methods and emphasizes trade-offs and other practical
consequences.

</details>


### [452] [Performative Risk Control: Calibrating Models for Reliable Deployment under Performativity](https://arxiv.org/abs/2505.24097)
*Victor Li,Baiting Chen,Yuzhen Mao,Qi Lei,Zhun Deng*

Key words: 风险控制, 表演性预测, 机器学习校准, 统计保证

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 提出Performative Risk Control框架，用于在预测动态影响结果（即表演性）下实现理论保证的风险控制。

Motivation: 传统校准方法假设数据分布静态，而实际决策中的预测可能动态影响结果，需要新的方法。

Method: 引入迭代校准过程，改进预测并确保风险控制；研究不同风险度量与尾界选择。

Result: 通过信贷违约风险预测实验验证有效性。

Conclusion: 首次在表演性下提供统计严格的风险控制框架，为决策过程提供防操纵保障。

Abstract: Calibrating blackbox machine learning models to achieve risk control is
crucial to ensure reliable decision-making. A rich line of literature has been
studying how to calibrate a model so that its predictions satisfy explicit
finite-sample statistical guarantees under a fixed, static, and unknown
data-generating distribution. However, prediction-supported decisions may
influence the outcome they aim to predict, a phenomenon named performativity of
predictions, which is commonly seen in social science and economics. In this
paper, we introduce Performative Risk Control, a framework to calibrate models
to achieve risk control under performativity with provable theoretical
guarantees. Specifically, we provide an iteratively refined calibration
process, where we ensure the predictions are improved and risk-controlled
throughout the process. We also study different types of risk measures and
choices of tail bounds. Lastly, we demonstrate the effectiveness of our
framework by numerical experiments on the task of predicting credit default
risk. To the best of our knowledge, this work is the first one to study
statistically rigorous risk control under performativity, which will serve as
an important safeguard against a wide range of strategic manipulation in
decision-making processes.

</details>


### [453] [A Mathematical Perspective On Contrastive Learning](https://arxiv.org/abs/2505.24134)
*Ricardo Baptista,Andrew M. Stuart,Son Tran*

Key words: 多模态对比学习、概率框架、跨模态检索、低秩矩阵逼近、高斯模型

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 这篇论文提出了一种基于概率框架的多模态对比学习方法，将其解读为优化编码器以定义条件概率分布，用于跨模态检索、分类和生成任务。文章还引入了新的概率损失函数和替代指标，并在多元高斯环境下对这些方法进行了分析和实验验证。

Motivation: 传统的多模态对比学习方法通常仅关注编码器的对齐，缺乏概率解释。本文旨在提供一个更通用的概率框架，以支持跨模态任务（如检索、分类和生成）的算法设计。

Method: 将对比学习重新解释为优化编码器以定义条件概率分布，引入新的概率损失函数和替代对齐指标，并在多元高斯设定下将其建模为低秩矩阵逼近问题。

Result: 在理论分析和数值实验中，验证了新损失函数和指标对条件均值、协方差等统计量的逼近能力，并在MNIST和海洋学数据同化应用中展示了其有效性。

Conclusion: 提出的概率框架不仅扩展了多模态对比学习的理论深度，还为特定任务（模式搜索和生成）提供了新的算法变体。

Abstract: Multimodal contrastive learning is a methodology for linking different data
modalities; the canonical example is linking image and text data. The
methodology is typically framed as the identification of a set of encoders, one
for each modality, that align representations within a common latent space. In
this work, we focus on the bimodal setting and interpret contrastive learning
as the optimization of (parameterized) encoders that define conditional
probability distributions, for each modality conditioned on the other,
consistent with the available data. This provides a framework for multimodal
algorithms such as crossmodal retrieval, which identifies the mode of one of
these conditional distributions, and crossmodal classification, which is
similar to retrieval but includes a fine-tuning step to make it task specific.
  The framework we adopt also gives rise to crossmodal generative models. This
probabilistic perspective suggests two natural generalizations of contrastive
learning: the introduction of novel probabilistic loss functions, and the use
of alternative metrics for measuring alignment in the common latent space. We
study these generalizations of the classical approach in the multivariate
Gaussian setting. In this context we view the latent space identification as a
low-rank matrix approximation problem. This allows us to characterize the
capabilities of loss functions and alignment metrics to approximate natural
statistics, such as conditional means and covariances; doing so yields novel
variants on contrastive learning algorithms for specific mode-seeking and for
generative tasks. The framework we introduce is also studied through numerical
experiments on multivariate Gaussians, the labeled MNIST dataset, and on a data
assimilation application arising in oceanography.

</details>


### [454] [Multi-task Learning for Heterogeneous Data via Integrating Shared and Task-Specific Encodings](https://arxiv.org/abs/2505.24281)
*Yang Sui,Qi Xu,Yang Bai,Annie Qu*

Key words: 多任务学习,双编码器框架,异质性,数据整合,预测性能

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文提出一种双编码器框架，通过共享编码器和任务特定编码器分别捕捉任务的共同信息和独特性，以解决多任务学习中的异质性问题。

Motivation: 多任务学习在处理多个任务时存在分布和后验异质性的挑战，现有方法未能有效统一处理这些问题。

Method: 采用双编码器框架，结合共享和任务特定编码器，并利用系数的内在相似性结构进行自适应整合。

Result: 仿真研究表明该方法优于现有数据整合方法，并在PDX数据中对五种癌症类型的肿瘤倍增时间预测表现优异。

Conclusion: 提出的方法能有效处理多任务学习中的异质性，并在理论和实际应用中展现出优越性能。

Abstract: Multi-task learning (MTL) has become an essential machine learning tool for
addressing multiple learning tasks simultaneously and has been effectively
applied across fields such as healthcare, marketing, and biomedical research.
However, to enable efficient information sharing across tasks, it is crucial to
leverage both shared and heterogeneous information. Despite extensive research
on MTL, various forms of heterogeneity, including distribution and posterior
heterogeneity, present significant challenges. Existing methods often fail to
address these forms of heterogeneity within a unified framework. In this paper,
we propose a dual-encoder framework to construct a heterogeneous latent factor
space for each task, incorporating a task-shared encoder to capture common
information across tasks and a task-specific encoder to preserve unique task
characteristics. Additionally, we explore the intrinsic similarity structure of
the coefficients corresponding to learned latent factors, allowing for adaptive
integration across tasks to manage posterior heterogeneity. We introduce a
unified algorithm that alternately learns the task-specific and task-shared
encoders and coefficients. In theory, we investigate the excess risk bound for
the proposed MTL method using local Rademacher complexity and apply it to a new
but related task. Through simulation studies, we demonstrate that the proposed
method outperforms existing data integration methods across various settings.
Furthermore, the proposed method achieves superior predictive performance for
time to tumor doubling across five distinct cancer types in PDX data.

</details>


### [455] [Equilibrium Distribution for t-Distributed Stochastic Neighbor Embedding with Generalized Kernels](https://arxiv.org/abs/2505.24311)
*Yi Gu*

Key words: t-SNE、广义核、收敛性、高维数据、可视化

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文研究了使用广义核的t-SNE算法的收敛性，并扩展了Auffinger和Fletcher（2023）的结果，证明了在特定条件下算法会收敛到平衡分布。

Motivation: 探讨广义核下t-SNE算法的收敛性，以扩展现有理论结果并验证其普适性。

Method: 提出广义输入和输出核的具体公式，并在数据点数量趋于无穷时分析算法收敛性。

Result: 证明了在一定条件下，t-SNE算法对广泛的输入和输出核会收敛到平衡分布。

Conclusion: 广义核下的t-SNE算法具有收敛性，为高维数据可视化提供了理论支持。

Abstract: T-distributed stochastic neighbor embedding (t-SNE) is a well-known algorithm
for visualizing high-dimensional data by finding low-dimensional
representations. In this paper, we study the convergence of t-SNE with
generalized kernels and extend the results of Auffinger and Fletcher in 2023.
Our work starts by giving a concrete formulation of generalized input and
output kernels. Then we prove that under certain conditions, the t-SNE
algorithm converges to an equilibrium distribution for a wide range of input
and output kernels as the number of data points diverges.

</details>


### [456] [Two failure modes of deep transformers and how to avoid them: a unified theory of signal propagation at initialisation](https://arxiv.org/abs/2505.24333)
*Alessio Giorlandino,Sebastian Goldt*

Key words: 神经网络, Transformer, 初始化, 自注意力, 信号传播, 训练稳定性

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文分析了神经网络初始化的关键性，特别针对Transformer中的自注意力层失败模式（秩崩溃和熵崩溃），提出了信号传播的解析理论，并通过实验验证。

Motivation: 研究Transformer块中信号传播的特性，避免因初始化不当导致的训练失败模式。

Method: 结合统计物理学的随机能量模型，分析自注意力层的信号传播特性，识别低方差和高方差两种不同初始化的影响。

Result: 理论分析区分了低方差（秩崩溃）和高方差（熵崩溃）的两种失败模式，并通过BERT模型在TinyStories上的实验验证了理论预测。

Conclusion: 提供了一个统一的理论框架，定量预测了权重和残差连接的初始化范围以确保训练的稳定性。

Abstract: Finding the right initialisation for neural networks is crucial to ensure
smooth training and good performance. In transformers, the wrong initialisation
can lead to one of two failure modes of self-attention layers: rank collapse,
where all tokens collapse into similar representations, and entropy collapse,
where highly concentrated attention scores lead to training instability. While
the right initialisation has been extensively studied in feed-forward networks,
an exact description of signal propagation through a full transformer block has
so far been lacking. Here, we provide an analytical theory of signal
propagation through vanilla transformer blocks with self-attention layers,
layer normalisation, skip connections and ReLU MLP. To treat the self-attention
layer, we draw on a formal parallel with the Random Energy Model from
statistical physics. We identify and characterise two regimes governed by the
variance of the query and key initialisations: a low-variance regime, where we
recover the known rank collapse behaviour; and a previously unexplored
high-variance regime, where signal is preserved but \textit{entropy collapse}
occurs. In the low-variance regime, we calculate the critical strength for the
residual connection to ensure signal propagation. Our theory yields
trainability diagrams that identify the correct choice of initialisation
hyper-parameters for a given architecture. Experiments with BERT-style models
trained on TinyStories validate our predictions. Our theoretical framework
gives a unified perspective on the two failure modes of self-attention and
gives quantitative predictions on the scale of both weights and residual
connections that guarantees smooth training.

</details>


### [457] [Predictive posterior sampling from non-stationnary Gaussian process priors via Diffusion models with application to climate data](https://arxiv.org/abs/2505.24556)
*Gabriel V Cardoso,Mike Pereira*

Key words: 高斯过程，扩散生成模型，非平稳性，后验采样，逆问题

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 摘要太长；未读总结：提出了一种基于扩散生成模型的两步法，用于模拟与非平稳高斯过程先验相关的预测后验分布，并在环境科学逆问题中验证了其有效性。

Motivation: 使用非平稳先验的高斯过程模型可以捕捉复杂空间模式，但计算预测后验分布时存在计算不可行的问题。

Method: 采用基于扩散生成模型的两步法，用扩散生成模型替代高斯过程先验，并利用无训练引导算法采样目标后验分布。

Result: 验证了该方法生成的分布与高斯过程分布接近，展示了如何微调模型以针对特定部分，并在环境科学逆问题中取得了先进预测结果。

Conclusion: 该方法为非平稳高斯过程后验采样提供了高效且灵活的解决方案，适用于复杂空间模式和实际应用。

Abstract: Bayesian models based on Gaussian processes (GPs) offer a flexible framework
to predict spatially distributed variables with uncertainty. But the use of
nonstationary priors, often necessary for capturing complex spatial patterns,
makes sampling from the predictive posterior distribution (PPD) computationally
intractable. In this paper, we propose a two-step approach based on diffusion
generative models (DGMs) to mimic PPDs associated with non-stationary GP
priors: we replace the GP prior by a DGM surrogate, and leverage recent
advances on training-free guidance algorithms for DGMs to sample from the
desired posterior distribution. We apply our approach to a rich non-stationary
GP prior from which exact posterior sampling is untractable and validate that
the issuing distributions are close to their GP counterpart using several
statistical metrics. We also demonstrate how one can fine-tune the trained DGMs
to target specific parts of the GP prior. Finally we apply the proposed
approach to solve inverse problems arising in environmental sciences, thus
yielding state-of-the-art predictions.

</details>


### [458] [Impact of Bottleneck Layers and Skip Connections on the Generalization of Linear Denoising Autoencoders](https://arxiv.org/abs/2505.24668)
*Jonghyun Ham,Maximilian Fleissner,Debarghya Ghoshdastidar*

Key words: 降噪自编码器，梯度流，瓶颈层，跳跃连接，随机矩阵理论

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 这篇论文研究了两层线性降噪自编码器在梯度流下的训练行为，重点关注低维瓶颈层和跳跃连接对测试风险的影响，揭示了瓶颈宽度带来的偏差-方差权衡及跳跃连接的方差缓解作用。

Motivation: 尽管深度神经网络在过度参数化下表现出强泛化能力，但其在无监督任务（如降噪）中的表现仍有许多未解之谜。本文旨在探讨两层线性降噪自编码器的行为，尤其是瓶颈层和跳跃连接的作用。

Method: 研究基于梯度流训练的两层线性降噪自编码器，引入低维瓶颈层和跳跃连接，并利用乘积正则化推导临界点及全局最小化解。通过随机矩阵理论分析跳跃连接的影响。

Result: 分析表明，瓶颈层宽度增加会降低偏差但引入方差，而跳跃连接能有效缓解方差，尤其在轻度过度参数化时效果显著。数值实验支持了理论结论。

Conclusion: 论文揭示了降噪自编码器中瓶颈层和跳跃连接的复杂交互作用，为理解深度无监督学习提供了新视角。

Abstract: Modern deep neural networks exhibit strong generalization even in highly
overparameterized regimes. Significant progress has been made to understand
this phenomenon in the context of supervised learning, but for unsupervised
tasks such as denoising, several open questions remain. While some recent works
have successfully characterized the test error of the linear denoising problem,
they are limited to linear models (one-layer network). In this work, we focus
on two-layer linear denoising autoencoders trained under gradient flow,
incorporating two key ingredients of modern deep learning architectures: A
low-dimensional bottleneck layer that effectively enforces a rank constraint on
the learned solution, as well as the possibility of a skip connection that
bypasses the bottleneck. We derive closed-form expressions for all critical
points of this model under product regularization, and in particular describe
its global minimizer under the minimum-norm principle. From there, we derive
the test risk formula in the overparameterized regime, both for models with and
without skip connections. Our analysis reveals two interesting phenomena:
Firstly, the bottleneck layer introduces an additional complexity measure akin
to the classical bias-variance trade-off -- increasing the bottleneck width
reduces bias but introduces variance, and vice versa. Secondly, skip connection
can mitigate the variance in denoising autoencoders -- especially when the
model is mildly overparameterized. We further analyze the impact of skip
connections in denoising autoencoder using random matrix theory and support our
claims with numerical evidence.

</details>


### [459] [K$^2$IE: Kernel Method-based Kernel Intensity Estimators for Inhomogeneous Poisson Processes](https://arxiv.org/abs/2505.24704)
*Hideaki Kim,Tomoharu Iwata,Akinori Fujino*

Key words: 核方法，泊松过程，强度估计，正则化，最小二乘损失，表示定理

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了一种基于最小二乘损失的新型正则化核方法（K²IE），用于泊松过程的强度估计，揭示了经典核强度估计与基于核方法的估计之间的联系，并在计算效率上有显著提升。

Motivation: 经典核强度估计（KIEs）和基于核方法的强度估计虽然都利用了“核”的概念，但理论基础不同且各有局限。论文旨在通过一种新型正则化核方法，统一两者的优势并提供更高效的估计方法。

Method: 提出了基于最小二乘损失的正则化核方法（K²IE），该方法的强度估计满足一种特殊的表示定理变体（系数为1），并与经典KIEs一致。

Result: 实验表明，K²IE在预测性能上与现有方法相当，但计算效率显著优于最先进的基于核方法的估计器。

Conclusion: K²IE不仅在理论上连接了经典KIEs和核方法估计，还通过结合RKHS理论实现了高效计算，为泊松过程强度估计提供了新工具。

Abstract: Kernel method-based intensity estimators, formulated within reproducing
kernel Hilbert spaces (RKHSs), and classical kernel intensity estimators (KIEs)
have been among the most easy-to-implement and feasible methods for estimating
the intensity functions of inhomogeneous Poisson processes. While both
approaches share the term "kernel", they are founded on distinct theoretical
principles, each with its own strengths and limitations. In this paper, we
propose a novel regularized kernel method for Poisson processes based on the
least squares loss and show that the resulting intensity estimator involves a
specialized variant of the representer theorem: it has the dual coefficient of
unity and coincides with classical KIEs. This result provides new theoretical
insights into the connection between classical KIEs and kernel method-based
intensity estimators, while enabling us to develop an efficient KIE by
leveraging advanced techniques from RKHS theory. We refer to the proposed model
as the kernel method-based kernel intensity estimator (K$^2$IE). Through
experiments on synthetic datasets, we show that K$^2$IE achieves comparable
predictive performance while significantly surpassing the state-of-the-art
kernel method-based estimator in computational efficiency.

</details>


### [460] [Knockoff-Guided Compressive Sensing: A Statistical Machine Learning Framework for Support-Assured Signal Recovery](https://arxiv.org/abs/2505.24727)
*Xiaochen Zhang,Haoyi Xiong*

Key words: Knockoff, 压缩感知, FDR控制, 信号重建, LASSO

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了一种新的Knockoff引导的压缩感知框架"，通过精确控制支持识别阶段的FDR（虚假发现率），提高了信号恢复的可靠性。相比于传统LASSO，该方法在有限样本下保证FDR控制，显著提升了信号支持的准确识别和重建性能。

Motivation: 传统压缩感知方法（如LASSO）在支持选择和信号估计时缺乏明确的错误控制，导致信号恢复不可靠。本方法希望通过Knockoff滤波器实现FDR控制，从而在更具挑战性的场景中实现更精确的信号重建。

Method: 通过统计Knockoff滤波器分离并控制支持恢复过程，结合FDR控制保证支持识别的准确性。在理论分析中展示了比传统ℓ1方法更弱的假设条件下实现恢复性能的保证。

Result: 实验结果表明，该方法在F1分数上比基准方法提升高达3.9倍，并降低了重建误差。在真实数据集上的回归和分类任务中均表现出色，接近甚至超越未压缩信号的性能。

Conclusion: 提出的Knockoff框架提供了理论保证和强大的实际性能，是一种比现有方法更稳健、实用的压缩感知替代方案。

Abstract: This paper introduces a novel Knockoff-guided compressive sensing framework,
referred to as \TheName{}, which enhances signal recovery by leveraging precise
false discovery rate (FDR) control during the support identification phase.
Unlike LASSO, which jointly performs support selection and signal estimation
without explicit error control, our method guarantees FDR control in finite
samples, enabling more reliable identification of the true signal support. By
separating and controlling the support recovery process through statistical
Knockoff filters, our framework achieves more accurate signal reconstruction,
especially in challenging scenarios where traditional methods fail. We
establish theoretical guarantees demonstrating how FDR control directly ensures
recovery performance under weaker conditions than traditional $\ell_1$-based
compressive sensing methods, while maintaining accurate signal reconstruction.
Extensive numerical experiments demonstrate that our proposed Knockoff-based
method consistently outperforms LASSO-based and other state-of-the-art
compressive sensing techniques. In simulation studies, our method improves
F1-score by up to 3.9x over baseline methods, attributed to principled false
discovery rate (FDR) control and enhanced support recovery. The method also
consistently yields lower reconstruction and relative errors. We further
validate the framework on real-world datasets, where it achieves top downstream
predictive performance across both regression and classification tasks, often
narrowing or even surpassing the performance gap relative to uncompressed
signals. These results establish \TheName{} as a robust and practical
alternative to existing approaches, offering both theoretical guarantees and
strong empirical performance through statistically grounded support selection.

</details>


### [461] [Generalization Dynamics of Linear Diffusion Models](https://arxiv.org/abs/2505.24769)
*Claudia Merger,Sebastian Goldt*

Key words: 扩散模型、记忆-泛化过渡、样本复杂度、线性去噪器、KL散度

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该研究分析了扩散模型从记忆训练样本到生成新样本的过渡，揭示了样本复杂度如何影响泛化能力，尤其在输入维度与样本数量关系上提供了理论支持。

Motivation: 理解扩散模型从记忆到泛化的过渡对于评估生成模型的样本效率和可靠性至关重要，但目前理论尚不完善。

Method: 使用线性去噪器构建简单模型，通过计算测试误差、采样分布和KL散度等指标，分析样本数量与输入维度的关系。

Result: 发现过渡发生在样本数量与输入维度相近时（$N \asymp d$），并验证了正则化和早停对防止过拟合的作用。

Conclusion: 线性扩散模型的采样分布会以$d/N$的线性速度逼近最优解，独立于数据分布的具体细节。

Abstract: Diffusion models trained on finite datasets with $N$ samples from a target
distribution exhibit a transition from memorisation, where the model reproduces
training examples, to generalisation, where it produces novel samples that
reflect the underlying data distribution. Understanding this transition is key
to characterising the sample efficiency and reliability of generative models,
but our theoretical understanding of this transition is incomplete. Here, we
analytically study the memorisation-to-generalisation transition in a simple
model using linear denoisers, which allow explicit computation of test errors,
sampling distributions, and Kullback-Leibler divergences between samples and
target distribution. Using these measures, we predict that this transition
occurs roughly when $N \asymp d$, the dimension of the inputs. When $N$ is
smaller than the dimension of the inputs $d$, so that only a fraction of
relevant directions of variation are present in the training data, we
demonstrate how both regularization and early stopping help to prevent
overfitting. For $N > d$, we find that the sampling distributions of linear
diffusion models approach their optimum (measured by the Kullback-Leibler
divergence) linearly with $d/N$, independent of the specifics of the data
distribution. Our work clarifies how sample complexity governs generalisation
in a simple model of diffusion-based generative models and provides insight
into the training dynamics of linear denoisers.

</details>


### [462] [Efficient Estimation of Regularized Tyler's M-Estimator Using Approximate LOOCV](https://arxiv.org/abs/2505.24781)
*Karim Abou-Moustafa*

Key words: 正则化Tyler's M-estimator, 留一交叉验证, 收缩系数, 高维数据, 计算效率

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了一种高效估计正则化Tyler's M-estimator（RTME）收缩系数的方法，通过近似留一交叉验证（LOOCV）对数似然损失来避免高计算成本，并在合成和真实高维数据中验证了其效率和准确性。

Motivation: 估计正则化参数或收缩系数在高维统计中至关重要，但传统LOOCV方法计算成本高，尤其在样本量大时。本文旨在提出一种高效且准确的估计方法。

Method: 提出了一种近似LOOCV对数似然损失的快速计算方案，避免了为每个样本单独计算RTME的需求，将复杂度从$O(n^2)$降低到$O(n)$。

Result: 在合成数据和真实高维数据集（如目标识别、人脸识别和手写数字识别）上，该方法不仅计算高效，且比其他文献方法更准确。

Conclusion: 所提方法显著提升了收缩系数估计的效率与精度，适用于高维数据，尤其是重尾椭圆分布场景。

Abstract: We consider the problem of estimating a regularization parameter, or a
shrinkage coefficient $\alpha \in (0,1)$ for Regularized Tyler's M-estimator
(RTME). In particular, we propose to estimate an optimal shrinkage coefficient
by setting $\alpha$ as the solution to a suitably chosen objective function;
namely the leave-one-out cross-validated (LOOCV) log-likelihood loss. Since
LOOCV is computationally prohibitive even for moderate sample size $n$, we
propose a computationally efficient approximation for the LOOCV log-likelihood
loss that eliminates the need for invoking the RTME procedure $n$ times for
each sample left out during the LOOCV procedure. This approximation yields an
$O(n)$ reduction in the running time complexity for the LOOCV procedure, which
results in a significant speedup for computing the LOOCV estimate. We
demonstrate the efficiency and accuracy of the proposed approach on synthetic
high-dimensional data sampled from heavy-tailed elliptical distributions, as
well as on real high-dimensional datasets for object recognition, face
recognition, and handwritten digit's recognition. Our experiments show that the
proposed approach is efficient and consistently more accurate than other
methods in the literature for shrinkage coefficient estimation.

</details>


### [463] [Statistical mechanics of extensive-width Bayesian neural networks near interpolation](https://arxiv.org/abs/2505.24849)
*Jean Barbier,Francesco Camilli,Minh-Toan Nguyen,Mauro Pastore,Rudy Skerk*

Key words: 统计物理学, 神经网络, 贝叶斯最优学习, 两层网络, 特征学习

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文通过统计物理学方法分析了一个两层全连接神经网络的监督学习，弥补了实际应用网络与理论模型之间的差距。研究揭示了在不同数据量下的学习动态和特征学习现象。

Motivation: 尽管统计力学为神经网络分析提供了框架，但理论模型与实际应用的网络存在差距。本文旨在通过分析更接近实际的两层网络，填补这一差距。

Method: 研究采用统计物理学方法，分析了具有通用权重分布和激活函数的两层网络。重点研究了贝叶斯最优学习，并在教师-学生场景下进行了模拟。

Result: 研究发现，特征学习随数据量增加而动态变化。当数据稀缺时，模型仅学习教师权重的非线性组合；数据充足时，权重与教师权重对齐。但实际训练算法可能难以实现这种对齐。

Conclusion: 该研究揭示了两层网络的特征学习动态，为理解实际网络的行为提供了理论支持，但也指出了理论与实际训练之间的潜在差距。

Abstract: For three decades statistical mechanics has been providing a framework to
analyse neural networks. However, the theoretically tractable models, e.g.,
perceptrons, random features models and kernel machines, or multi-index models
and committee machines with few neurons, remained simple compared to those used
in applications. In this paper we help reducing the gap between practical
networks and their theoretical understanding through a statistical physics
analysis of the supervised learning of a two-layer fully connected network with
generic weight distribution and activation function, whose hidden layer is
large but remains proportional to the inputs dimension. This makes it more
realistic than infinitely wide networks where no feature learning occurs, but
also more expressive than narrow ones or with fixed inner weights. We focus on
the Bayes-optimal learning in the teacher-student scenario, i.e., with a
dataset generated by another network with the same architecture. We operate
around interpolation, where the number of trainable parameters and of data are
comparable and feature learning emerges. Our analysis uncovers a rich
phenomenology with various learning transitions as the number of data
increases. In particular, the more strongly the features (i.e., hidden neurons
of the target) contribute to the observed responses, the less data is needed to
learn them. Moreover, when the data is scarce, the model only learns non-linear
combinations of the teacher weights, rather than "specialising" by aligning its
weights with the teacher's. Specialisation occurs only when enough data becomes
available, but it can be hard to find for practical training algorithms,
possibly due to statistical-to-computational~gaps.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [464] [A Start To End Machine Learning Approach To Maximize Scientific Throughput From The LCLS-II-HE](https://arxiv.org/abs/2505.23858)
*Aashwin Mishra,Matt Seaberg,Ryan Roussel,Fred Poitevin,Jana Thayer,Daniel Ratner,Auralee Edelen,Apurva Mehta*

Key words: 机器学习, 实时反馈, 高亮度光源, 数据优化, LCLS

<details>
  <summary>Details</summary>

Main category: physics.ins-det

TL;DR: 该论文讨论了随着光源亮度提升（如APS和LCLS升级），实验复杂度增加带来的数据管理挑战，并提出了基于机器学习的实时反馈控制和优化策略。

Motivation: 随着光源亮度的提升（如APS和LCLS的升级），实验复杂度大幅增加，需要更高的光束精度和稳定性，同时数据生成速率激增。若不采取实时反馈控制和优化，大量数据将难以转化为有效科学信息。

Method: 提出在SLAC开发的基于机器学习的策略，从电子注入器到多维X射线光学系统，再到实验终端站和高读取率探测器，实现优化、自动化和实时知识提取。

Result: 通过加速器、光学和终端用户应用的实例展示了该策略的可行性。

Conclusion: 基于机器学习的实时反馈控制和优化策略能够应对高亮度光源实验的数据挑战，确保设计性能的实现。

Abstract: With the increasing brightness of Light sources, including the
Diffraction-Limited brightness upgrade of APS and the high-repetition-rate
upgrade of LCLS, the proposed experiments therein are becoming increasingly
complex. For instance, experiments at LCLS-II-HE will require the X-ray beam to
be within a fraction of a micron in diameter, with pointing stability of a few
nanoradians, at the end of a kilometer-long electron accelerator, a
hundred-meter-long undulator section, and tens of meters long X-ray optics.
This enhancement of brightness will increase the data production rate to rival
the largest data generators in the world. Without real-time active feedback
control and an optimized pipeline to transform measurements to scientific
information and insights, researchers will drown in a deluge of mostly useless
data, and fail to extract the highly sophisticated insights that the recent
brightness upgrades promise.
  In this article, we outline the strategy we are developing at SLAC to
implement Machine Learning driven optimization, automation and real-time
knowledge extraction from the electron-injector at the start of the electron
accelerator, to the multidimensional X-ray optical systems, and till the
experimental endstations and the high readout rate, multi-megapixel detectors
at LCLS to deliver the design performance to the users. This is illustrated via
examples from Accelerator, Optics and End User applications.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [465] [Online Fair Division with Additional Information](https://arxiv.org/abs/2505.24503)
*Tzeh Yuan Neoh,Jannik Peters,Nicholas Teh*

Key words: 在线公平分配, 不可分割物品, 嫉妒自由, 比例公平, 最大化最小份额

<details>
  <summary>Details</summary>

Main category: cs.GT

TL;DR: 研究了在线公平分配不可分割物品的问题，分析了无未来信息时的公平性限制及利用额外信息设计更公平算法的可能性。

Motivation: 探索在在线环境下如何公平分配物品，特别是无法了解未来物品时的挑战，以及利用额外信息（如总估值或未来物品价值的预测）改善公平性的潜力。

Method: 1. 无未来信息时分析公平性的限制；2. 利用标准化信息设计算法；3. 结合频率预测提出元算法，匹配离线环境下的公平性保证。

Result: 1. 无信息时公平性难以实现；2. 利用标准化信息和频率预测可以设计更公平的算法，部分匹配离线结果。

Conclusion: 在线公平分配受限于未来不确定性，但结构化信息（如总估值或频率预测）能显著提升公平性。

Abstract: We study the problem of fairly allocating indivisible goods to agents in an
online setting, where goods arrive sequentially and must be allocated
irrevocably to agents. Focusing on the popular fairness notions of
envy-freeness, proportionality, and maximin share fairness (and their
approximate variants), we ask how the availability of information on future
goods influences the existence and approximability of fair allocations. In the
absence of any such information, we establish strong impossibility results,
demonstrating the inherent difficulty of achieving even approximate fairness
guarantees. In contrast, we demonstrate that knowledge of additional
information -- such as aggregate of each agent's total valuations
(equivalently, normalized valuations) or the multiset of future goods values
(frequency predictions) -- would enable the design of fairer online algorithms.
Given normalization information, we propose an algorithm that achieves stronger
fairness guarantees than previously known results. Given frequency predictions,
we introduce a meta-algorithm that leverages frequency predictions to match the
best-known offline guarantees for a broad class of ''share-based'' fairness
notions. Our complementary impossibility results in each setting underscore
both the limitations imposed by uncertainty about future goods and the
potential of leveraging structured information to achieve fairer outcomes in
online fair division.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [466] [Searching Clinical Data Using Generative AI](https://arxiv.org/abs/2505.24090)
*Karan Hanswadkar,Anika Kanchi,Shivani Tripathi,Shi Qiao,Rony Chatterjee,Alekh Jindal*

Key words: 生成式AI, 临床数据搜索, 分层模型, 自然语言处理, 医疗编码

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: 论文提出了一种名为SearchAI的生成式AI方法，旨在提高临床数据搜索的准确性和效率。通过分层模型优化编码层次结构的遍历，实验证明其优于传统方法。

Motivation: 当前临床数据搜索过程依赖人工操作且易出错，传统代码分配方法无法有效处理一对多的查询需求。通过AI自动化改进可以显著提升医疗工作者的操作效率。

Method: 采用分层模型（SearchAI），基于编码层次结构预测性遍历父子节点关系，支持关键词同义词、语义变体和开放式查询。

Result: 在公共和生产数据集上的实验表明，SearchAI在准确性、鲁棒性、性能和可扩展性上均优于默认分层遍历方法。

Conclusion: SearchAI能提升临床数据的可访问性，优化工作流程，减轻管理负担，并提高编码和诊断准确性。

Abstract: Artificial Intelligence (AI) is making a major impact on healthcare,
particularly through its application in natural language processing (NLP) and
predictive analytics. The healthcare sector has increasingly adopted AI for
tasks such as clinical data analysis and medical code assignment. However,
searching for clinical information in large and often unorganized datasets
remains a manual and error-prone process. Assisting this process with
automations can help physicians improve their operational productivity
significantly.
  In this paper, we present a generative AI approach, coined SearchAI, to
enhance the accuracy and efficiency of searching clinical data. Unlike
traditional code assignment, which is a one-to-one problem, clinical data
search is a one-to-many problem, i.e., a given search query can map to a family
of codes. Healthcare professionals typically search for groups of related
diseases, drugs, or conditions that map to many codes, and therefore, they need
search tools that can handle keyword synonyms, semantic variants, and broad
open-ended queries. SearchAI employs a hierarchical model that respects the
coding hierarchy and improves the traversal of relationships from parent to
child nodes. SearchAI navigates these hierarchies predictively and ensures that
all paths are reachable without losing any relevant nodes.
  To evaluate the effectiveness of SearchAI, we conducted a series of
experiments using both public and production datasets. Our results show that
SearchAI outperforms default hierarchical traversals across several metrics,
including accuracy, robustness, performance, and scalability. SearchAI can help
make clinical data more accessible, leading to streamlined workflows, reduced
administrative burden, and enhanced coding and diagnostic accuracy.

</details>


### [467] [Towards Scalable Schema Mapping using Large Language Models](https://arxiv.org/abs/2505.24716)
*Christopher Buss,Mahdis Safari,Arash Termehchy,Stefan Lee,David Maier*

Key words: 数据集成, 模式映射, 大语言模型, 计算成本

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: 论文提出解决LLMs在模式映射中的三个核心问题：输出不一致、表达力不足和高计算成本，采用了采样聚合、GLaV映射和数据预过滤等方法。

Motivation: 数据集成系统依赖手工模式映射，成本高且难以维护。尽管LLMs可辅助自动化，但仍存在输出不一致、表达力受限和计算成本高的问题。

Method: 通过采样和聚合技术解决输出不一致；引入GLaV增强表达力；采用数据预过滤降低计算成本。

Result: 提出的方法缓解了LLMs在模式映射中的关键挑战，提升了稳定性和效率。

Conclusion: 研究为LLMs在数据集成中的应用提供了实用解决方案，平衡了表达力和计算效率。

Abstract: The growing need to integrate information from a large number of diverse
sources poses significant scalability challenges for data integration systems.
These systems often rely on manually written schema mappings, which are
complex, source-specific, and costly to maintain as sources evolve. While
recent advances suggest that large language models (LLMs) can assist in
automating schema matching by leveraging both structural and natural language
cues, key challenges remain. In this paper, we identify three core issues with
using LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to
input phrasing and structure, which we propose methods to address through
sampling and aggregation techniques; (2) the need for more expressive mappings
(e.g., GLaV), which strain the limited context windows of LLMs; and (3) the
computational cost of repeated LLM calls, which we propose to mitigate through
strategies like data type prefiltering.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [468] [Beyond the Black Box: Interpretability of LLMs in Finance](https://arxiv.org/abs/2505.24650)
*Hariom Tatsat,Ariye Shater*

Key words: 大型语言模型, 机制可解释性, 金融服务, 透明度, 金融监管

<details>
  <summary>Details</summary>

Main category: cs.CE

TL;DR: 该论文探讨了大型语言模型（LLMs）在金融领域的应用，并提出了机制可解释性作为提高透明度和控制AI系统行为的关键方法，通过实际金融用例验证其有效性。

Motivation: LLMs在金融服务中虽表现卓越，但复杂性和缺乏透明性在高度监管的金融领域带来挑战，亟需一种直观透明的方法来理解和控制模型行为。

Method: 采用机制可解释性方法，通过逆向工程分析模型内部机制（如激活和电路），揭示并修改模型行为，结合金融用例（如交易策略、情感分析）进行实证。

Result: 机制可解释性不仅能观察LLM行为，还可调整其预测逻辑，满足金融监管对透明度和合规性的要求，为未来广泛应用奠定基础。

Conclusion: 机制可解释性有望成为LLMs在金融领域广泛应用的关键工具，确保AI系统伦理合规，应对全球监管需求。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities across a
spectrum of tasks in financial services, including report generation, chatbots,
sentiment analysis, regulatory compliance, investment advisory, financial
knowledge retrieval, and summarization. However, their intrinsic complexity and
lack of transparency pose significant challenges, especially in the highly
regulated financial sector, where interpretability, fairness, and
accountability are critical. As far as we are aware, this paper presents the
first application in the finance domain of understanding and utilizing the
inner workings of LLMs through mechanistic interpretability, addressing the
pressing need for transparency and control in AI systems. Mechanistic
interpretability is the most intuitive and transparent way to understand LLM
behavior by reverse-engineering their internal workings. By dissecting the
activations and circuits within these models, it provides insights into how
specific features or components influence predictions - making it possible not
only to observe but also to modify model behavior. In this paper, we explore
the theoretical aspects of mechanistic interpretability and demonstrate its
practical relevance through a range of financial use cases and experiments,
including applications in trading strategies, sentiment analysis, bias, and
hallucination detection. While not yet widely adopted, mechanistic
interpretability is expected to become increasingly vital as adoption of LLMs
increases. Advanced interpretability tools can ensure AI systems remain
ethical, transparent, and aligned with evolving financial regulations. In this
paper, we have put special emphasis on how these techniques can help unlock
interpretability requirements for regulatory and compliance purposes -
addressing both current needs and anticipating future expectations from
financial regulators globally.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [469] [Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for End-to-End Few-Shot and Continual Learning from Sequential Data](https://arxiv.org/abs/2505.24852)
*Douwe den Blanken,Charlotte Frenkel*

Key words: 边缘学习, 少样本学习, 持续学习, 时序卷积网络, 低功耗

<details>
  <summary>Details</summary>

Main category: cs.AR

TL;DR: 论文提出Chameleon，通过统一的学习与推理架构、高效的时序依赖捕获以及双模计算阵列，解决了边缘设备上低功耗端到端学习与推理的挑战。

Motivation: 实现在边缘设备上低延迟、隐私保护的个性化学习，提升长期鲁棒性并降低维护成本。

Method: (i) 统一的学习与推理架构支持少样本学习（FSL）和持续学习（CL）；(ii) 使用时序卷积网络（TCN）高效捕获时序依赖；(iii) 双模无矩阵乘法计算阵列。

Result: 在Omniglot上取得端到端少样本学习新记录（96.8%，5-way 1-shot）和持续学习（82.2%最终精度），推理功耗低至3.1 μW。

Conclusion: Chameleon在边缘设备上实现了高效、低功耗的端到端学习与推理，解决了现有方法的不足。

Abstract: On-device learning at the edge enables low-latency, private personalization
with improved long-term robustness and reduced maintenance costs. Yet,
achieving scalable, low-power end-to-end on-chip learning, especially from
real-world sequential data with a limited number of examples, is an open
challenge. Indeed, accelerators supporting error backpropagation optimize for
learning performance at the expense of inference efficiency, while simplified
learning algorithms often fail to reach acceptable accuracy targets. In this
work, we present Chameleon, leveraging three key contributions to solve these
challenges. (i) A unified learning and inference architecture supports few-shot
learning (FSL), continual learning (CL) and inference at only 0.5% area
overhead to the inference logic. (ii) Long temporal dependencies are
efficiently captured with temporal convolutional networks (TCNs), enabling the
first demonstration of end-to-end on-chip FSL and CL on sequential data and
inference on 16-kHz raw audio. (iii) A dual-mode, matrix-multiplication-free
compute array allows either matching the power consumption of state-of-the-art
inference-only keyword spotting (KWS) accelerators or enabling $4.3\times$
higher peak GOPS. Fabricated in 40-nm CMOS, Chameleon sets new accuracy records
on Omniglot for end-to-end on-chip FSL (96.8%, 5-way 1-shot, 98.8%, 5-way
5-shot) and CL (82.2% final accuracy for learning 250 classes with 10 shots),
while maintaining an inference accuracy of 93.3% on the 12-class Google Speech
Commands dataset at an extreme-edge power budget of 3.1 $\mu$W.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [470] [A survey of using EHR as real-world evidence for discovering and validating new drug indications](https://arxiv.org/abs/2505.24767)
*Nabasmita Talukdar,Xiaodan Zhang,Shreya Paithankar,Hui Wang,Bin Chen*

Key words: 电子健康记录, 药物再利用, 真实世界证据, 大型语言模型, 目标试验模拟

<details>
  <summary>Details</summary>

Main category: stat.AP

TL;DR: 该论文综述了电子健康记录（EHR）在药物再利用中的应用，包括数据源、处理方法、技术以及研究设计和统计框架，并讨论了验证中的关键挑战和最新方法进展。

Motivation: 通过整合EHR数据作为真实世界证据（RWE），支持药物新适应症的发现和验证，为研究者提供将真实数据转化为可用证据的基础资源。

Method: 综合分析了EHR数据源、处理技术、表示方法、研究设计和统计框架，特别是在验证阶段中大型语言模型（LLMs）和目标试验模拟的应用。

Result: 综述了当前EHR在药物再利用领域的最新方法和发展趋势，并识别了验证过程中的关键挑战。

Conclusion: 该论文为研究者提供了一个基础性资源，帮助他们将真实世界数据转化为有效的药物再利用证据。

Abstract: Electronic Health Records (EHRs) have been increasingly used as real-world
evidence (RWE) to support the discovery and validation of new drug indications.
This paper surveys current approaches to EHR-based drug repurposing, covering
data sources, processing methodologies, and representation techniques. It
discusses study designs and statistical frameworks for evaluating drug
efficacy. Key challenges in validation are discussed, with emphasis on the role
of large language models (LLMs) and target trial emulation. By synthesizing
recent developments and methodological advances, this work provides a
foundational resource for researchers aiming to translate real-world data into
actionable drug-repurposing evidence.

</details>
