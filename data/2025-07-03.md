<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 42]
- [cs.LG](#cs.LG) [Total: 86]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.CR](#cs.CR) [Total: 5]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 5]
- [eess.IV](#eess.IV) [Total: 4]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.CV](#cs.CV) [Total: 26]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.SD](#cs.SD) [Total: 3]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.CY](#cs.CY) [Total: 5]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [math.NA](#math.NA) [Total: 3]
- [cs.RO](#cs.RO) [Total: 8]
- [stat.ML](#stat.ML) [Total: 4]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.DB](#cs.DB) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered](https://arxiv.org/abs/2507.01019)
*Imran Mirza,Cole Huang,Ishwara Vasista,Rohan Patil,Asli Akalin,Sean O'Brien,Kevin Zhu*

Key words: 多智能体系统,社会偏见,LLM,MALIBU基准,公平性

TL;DR: 论文提出了MALIBU基准，用于评估基于LLM的多智能体系统对社会偏见和刻板印象的隐式强化，并通过场景测试揭示偏见问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 多智能体系统可能强化LLM中的隐式偏见，引发公平和代表性担忧，需要开发评估工具以量化这些问题。

Method: 使用场景测试，通过两阶段评估：第一阶段对特定人口统计角色（如性别、种族）进行评分，第二阶段比较不同角色的回应。

Result: 研究表明，偏见缓解可能倾向于边缘化角色而非真正的中立，需要更细致的检测和公平策略。

Conclusion: 强调在多智能体系统中需要透明的评估基准和平衡的公平策略。

Abstract: Multi-agent systems, which consist of multiple AI models interacting within a
shared environment, are increasingly used for persona-based interactions.
However, if not carefully designed, these systems can reinforce implicit biases
in large language models (LLMs), raising concerns about fairness and equitable
representation. We present MALIBU, a novel benchmark developed to assess the
degree to which LLM-based multi-agent systems implicitly reinforce social
biases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems
through scenario-based assessments. AI models complete tasks within predefined
contexts, and their responses undergo evaluation by an LLM-based multi-agent
judging system in two phases. In the first phase, judges score responses
labeled with specific demographic personas (e.g., gender, race, religion)
across four metrics. In the second phase, judges compare paired responses
assigned to different personas, scoring them and selecting the superior
response. Our study quantifies biases in LLM-generated outputs, revealing that
bias mitigation may favor marginalized personas over true neutrality,
emphasizing the need for nuanced detection, balanced fairness strategies, and
transparent evaluation benchmarks in multi-agent systems.

</details>


### [2] [Event-based evaluation of abstractive news summarization](https://arxiv.org/abs/2507.01160)
*Huiling You,Samia Touileb,Erik Velldal,Lilja Øvrelid*

Key words: 摘要评估, 事件重叠, 生成语言模型, 新闻摘要

TL;DR: 提出了一种通过计算生成摘要与参考摘要及原文事件重叠的方法，以评估摘要质量。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有自动生成摘要的评估方法主要依赖人类撰写的参考摘要，通过计算重叠单元或相似性评分，但忽略了新闻事件的核心信息。

Method: 通过计算生成摘要、参考摘要及原文中事件的重叠程度来评估摘要质量，并在挪威语数据集上进行了实验。

Result: 该方法能更深入地揭示摘要中包含的事件信息。

Conclusion: 该方法为评估生成摘要提供了新的视角，尤其关注事件信息的完整性。

Abstract: An abstractive summary of a news article contains its most important
information in a condensed version. The evaluation of automatically generated
summaries by generative language models relies heavily on human-authored
summaries as gold references, by calculating overlapping units or similarity
scores. News articles report events, and ideally so should the summaries. In
this work, we propose to evaluate the quality of abstractive summaries by
calculating overlapping events between generated summaries, reference
summaries, and the original news articles. We experiment on a richly annotated
Norwegian dataset comprising both events annotations and summaries authored by
expert human annotators. Our approach provides more insight into the event
information contained in the summaries.

</details>


### [3] [Matching and Linking Entries in Historical Swedish Encyclopedias](https://arxiv.org/abs/2507.01170)
*Simon Börjesson,Erik Ersmark,Pierre Nugues*

Key words: Nordisk familjebok, 百科全书, 地理趋势, 语义嵌入, Transformer

TL;DR: 该论文分析了瑞典百科全书《Nordisk familjebok》第一版和第二版中地理条目的变化，发现从欧洲到其他地区的转移趋势。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究目的是通过数字化版本分析百科全书条目内容的变化，反映瑞典社会及知识结构的演变。

Method: 使用语义句子嵌入匹配条目，基于Transformer的分类器提取地理条目，并与Wikidata链接，识别地理趋势变化。

Result: 结果表明，从第一版到第二版，地理焦点从欧洲显著转向北美、非洲、亚洲、澳大利亚和北欧。

Conclusion: 这一变化可能与第一次世界大战和新势力的崛起有关，展示了百科全书对社会变迁的反映。

Abstract: The \textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and
20th centuries. It was written by a team of experts and aimed to be an
intellectual reference, stressing precision and accuracy. This encyclopedia had
four main editions remarkable by their size, ranging from 20 to 38 volumes. As
a consequence, the \textit{Nordisk familjebok} had a considerable influence in
universities, schools, the media, and society overall. As new editions were
released, the selection of entries and their content evolved, reflecting
intellectual changes in Sweden.
  In this paper, we used digitized versions from \textit{Project Runeberg}. We
first resegmented the raw text into entries and matched pairs of entries
between the first and second editions using semantic sentence embeddings. We
then extracted the geographical entries from both editions using a
transformer-based classifier and linked them to Wikidata. This enabled us to
identify geographic trends and possible shifts between the first and second
editions, written between 1876-1899 and 1904-1926, respectively.
  Interpreting the results, we observe a small but significant shift in
geographic focus away from Europe and towards North America, Africa, Asia,
Australia, and northern Scandinavia from the first to the second edition,
confirming the influence of the First World War and the rise of new powers. The
code and data are available on GitHub at
https://github.com/sibbo/nordisk-familjebok.

</details>


### [4] [MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis](https://arxiv.org/abs/2507.01213)
*Adamu Lawan,Juhua Pu,Haruna Yunusa,Jawad Muhammad,Muhammad Lawan*

Key words: ABSA, xLSTM, MEGA, 多头指数门控融合, 自然语言处理

TL;DR: 该论文提出了一种基于xLSTM的多头指数门控融合（MEGA）框架，用于解决现有ABSA方法在计算效率与性能平衡上的不足，并验证了其在多个基准数据集上的优越表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有ABSA方法难以兼顾计算效率与高性能，尤其是缺乏全局上下文或依赖过高计算资源，xLSTM的潜力尚未在ABSA中充分发掘。

Method: 提出MEGA框架，结合双向mLSTM架构和部分翻转后向流（PF-mLSTM），并通过MECGAF机制动态融合前向与后向输出，优化短程依赖捕获和全局上下文建模。

Result: 在三个基准数据集上的实验表明，MEGA在准确性和效率上均优于现有方法。

Conclusion: MEGA框架成功平衡了ABSA任务中的性能与效率，为未来研究提供了新思路。

Abstract: Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language
Processing (NLP) task that extracts aspects from text and determines their
associated sentiments, enabling fine-grained analysis of user opinions.
Existing ABSA methods struggle to balance computational efficiency with high
performance: deep learning models often lack global context, transformers
demand significant computational resources, and Mamba-based approaches face
CUDA dependency and diminished local correlations. Recent advancements in
Extended Long Short-Term Memory (xLSTM) models, particularly their efficient
modeling of long-range dependencies, have significantly advanced the NLP
community. However, their potential in ABSA remains untapped. To this end, we
propose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework
integrating a bi-directional mLSTM architecture with forward and partially
flipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context
modeling by processing the initial sequence segment in reverse with dedicated
parameters, preserving critical short-range patterns. We further introduce an
mLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that
dynamically combines forward mLSTM outputs as query and key with PF-mLSTM
outputs as value, optimizing short-range dependency capture while maintaining
global context and efficiency. Experimental results on three benchmark datasets
demonstrate that MEGA outperforms state-of-the-art baselines, achieving
superior accuracy and efficiency in ABSA tasks.

</details>


### [5] [The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure](https://arxiv.org/abs/2507.01234)
*Yu Fan,Yang Tian,Shauli Ravfogel,Mrinmaya Sachan,Elliott Ash,Alexander Hoyle*

Key words: 嵌入去偏, 文本相似性, 混淆变量, 编码器表示

TL;DR: 本文介绍了一种去偏算法，通过从编码器表示中移除与文档混淆变量相关的信息，显著减少了文本嵌入中的偏见，同时保持了嵌入的其他性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 文本序列的嵌入相似性度量可能受到来源或语言等无关变量的影响，这会影响跨语料库文本汇集的应用程序效果。

Method: 采用去偏算法从编码器表示中移除观察到的混淆变量的信息。

Result: 去偏算法显著减少了偏见，且在文档相似性和聚类任务中表现提升，同时不影响分布外基准测试的性能。

Conclusion: 去偏算法能以较低计算成本有效减少嵌入中的偏见，且不损害其他性能。

Abstract: Embedding-based similarity metrics between text sequences can be influenced
not just by the content dimensions we most care about, but can also be biased
by spurious attributes like the text's source or language. These document
confounders cause problems for many applications, but especially those that
need to pool texts from different corpora. This paper shows that a debiasing
algorithm that removes information about observed confounders from the encoder
representations substantially reduces these biases at a minimal computational
cost. Document similarity and clustering metrics improve across every embedding
variant and task we evaluate -- often dramatically. Interestingly, performance
on out-of-distribution benchmarks is not impacted, indicating that the
embeddings are not otherwise degraded.

</details>


### [6] [GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant](https://arxiv.org/abs/2507.01259)
*Michał Matak,Jarosław A. Chudziak*

Key words: 大型语言模型, 法律信息检索, 波兰民法典, gAIus架构

TL;DR: 论文探讨了大型语言模型在处理非英语和非中文国家的法律问题时提供基于法律条文回答的能力，提出了gAIus架构，其检索机制优于嵌入方法，并通过波兰民法典的实验验证了其效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在提升大型语言模型在法律信息检索中的准确性和可解释性，尤其是在非英语和非中文语境下。

Method: 提出gAIus架构，基于波兰民法典构建检索机制，并通过法律学徒入学考试的选择题数据集进行评估。

Result: gAIus显著提升了模型的性能，gpt-3.5-turbo-0125提高了419%，gpt-4o-mini从31%提升到86%。

Conclusion: 研究表明gAIus架构在法律信息检索中具有潜力，并指出了未来研究的可能方向和应用。

Abstract: In this paper we discuss the capability of large language models to base
their answer and provide proper references when dealing with legal matters of
non-english and non-chinese speaking country. We discuss the history of legal
information retrieval, the difference between case law and statute law, its
impact on the legal tasks and analyze the latest research in this field. Basing
on that background we introduce gAIus, the architecture of the cognitive
LLM-based agent, whose responses are based on the knowledge retrieved from
certain legal act, which is Polish Civil Code. We propose a retrieval mechanism
which is more explainable, human-friendly and achieves better results than
embedding-based approaches. To evaluate our method we create special dataset
based on single-choice questions from entrance exams for law apprenticeships
conducted in Poland. The proposed architecture critically leveraged the
abilities of used large language models, improving the gpt-3.5-turbo-0125 by
419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.
At the end of our paper we show the possible future path of research and
potential applications of our findings.

</details>


### [7] [Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening](https://arxiv.org/abs/2507.01278)
*Cindy Lie Tabuse,David Restepo,Carolina Gracitelli,Fernando Korn Malerbi,Caio Regatieri,Luis Filipe Nakayama*

Key words: 大型语言模型, GPT-4, 眼科, 糖尿病视网膜病变, 青光眼

TL;DR: 研究评估了GPT-4在糖尿病视网膜病变和青光眼筛查中模拟临床决策的能力，发现其表现中等，但对复杂任务缺乏精度。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨大型语言模型在眼科中的实用性，尤其是基于结构化文本描述的临床决策模拟。

Method: 使用300张标注的视网膜眼底照片，通过结构化提示（含或不含患者元数据）让GPT-4进行评分和推荐，评估其准确性等指标。

Result: GPT-4在ICDR分类中表现中等（准确率67.5%），在青光眼筛查中表现较差（准确率约78%），元数据对结果影响不大。

Conclusion: GPT-4可模拟基本眼科决策，但不适合临床使用，可能在教育或图像标注中有辅助作用。

Abstract: Large language models (LLMs) can simulate clinical reasoning based on natural
language prompts, but their utility in ophthalmology is largely unexplored.
This study evaluated GPT-4's ability to interpret structured textual
descriptions of retinal fundus photographs and simulate clinical decisions for
diabetic retinopathy (DR) and glaucoma screening, including the impact of
adding real or synthetic clinical metadata. We conducted a retrospective
diagnostic validation study using 300 annotated fundus images. GPT-4 received
structured prompts describing each image, with or without patient metadata. The
model was tasked with assigning an ICDR severity score, recommending DR
referral, and estimating the cup-to-disc ratio for glaucoma referral.
Performance was evaluated using accuracy, macro and weighted F1 scores, and
Cohen's kappa. McNemar's test and change rate analysis were used to assess the
influence of metadata. GPT-4 showed moderate performance for ICDR
classification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),
driven mainly by correct identification of normal cases. Performance improved
in the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For
glaucoma referral, performance was poor across all settings (accuracy ~78%, F1
<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes
(McNemar p > 0.05), and predictions remained consistent across conditions.
GPT-4 can simulate basic ophthalmic decision-making from structured prompts but
lacks precision for complex tasks. While not suitable for clinical use, LLMs
may assist in education, documentation, or image annotation workflows in
ophthalmology.

</details>


### [8] [Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization](https://arxiv.org/abs/2507.01281)
*Juan Chen,Baolong Bi,Wei Zhang,Jingyan Sui,Xiaofei Zhu,Yuanzhuo Wang,Lingrui Mei,Shenghua Liu*

Key words: RAG, 知识冲突, 可靠性, 证据总结, QA修复

TL;DR: CARE-RAG框架通过整合内部知识与外部检索内容，提升RAG系统的可靠性，特别是在噪声或冲突证据场景下表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决RAG系统中因知识冲突导致的生成不可靠问题。

Method: 提出CARE-RAG框架，包括参数感知证据派生、上下文感知证据提炼、冲突驱动的3B LLaMA3.2模型总结及QA修复步骤。

Result: 在修订后的QA数据集上，CARE-RAG优于现有RAG基线，尤其在噪声或冲突证据场景中。

Conclusion: CARE-RAG通过冲突驱动的证据总结显著提升RAG系统的可靠性和性能。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating their parametric knowledge with external retrieved content.
However, knowledge conflicts caused by internal inconsistencies or noisy
retrieved content can severely undermine the generation reliability of RAG
systems.In this work, we argue that LLMs should rethink all evidence, including
both retrieved content and internal knowledge, before generating responses.We
propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel
framework that improves trustworthiness through Conflict-Driven Summarization
of all available evidence.CARE-RAG first derives parameter-aware evidence by
comparing parameter records to identify diverse internal perspectives. It then
refines retrieved evidences to produce context-aware evidence, removing
irrelevant or misleading content. To detect and summarize conflicts, we distill
a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable
synthesis across multiple sources.To further ensure evaluation integrity, we
introduce a QA Repair step to correct outdated or ambiguous benchmark
answers.Experiments on revised QA datasets with retrieval data show that
CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios
with noisy or conflicting evidence.

</details>


### [9] [Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks](https://arxiv.org/abs/2507.01297)
*Xinxi Lyu,Michael Duan,Rulin Shao,Pang Wei Koh,Sewon Min*

Key words: 检索增强生成,RAG,CompactDS,推理任务,数据存储

TL;DR: 研究了检索增强生成（RAG）在复杂推理任务中的表现，提出CompactDS数据存储系统，显著提升了多基准测试的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索RAG在复杂推理任务中的潜力，解决现有研究中数据存储不足的问题。

Method: 引入CompactDS，结合高效过滤与混合检索策略，优化检索性能。

Result: CompactDS在多基准测试中显著提升了准确率，且性能优于现有搜索引擎和复杂RAG系统。

Conclusion: CompactDS展示了高质量、多样性数据对RAG性能的重要性，支持未来的检索型AI系统研究。

Abstract: Retrieval-augmented Generation (RAG) has primarily been studied in limited
settings, such as factoid question answering; more challenging,
reasoning-intensive benchmarks have seen limited success from minimal RAG. In
this work, we challenge this prevailing view on established,
reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We
identify a key missing component in prior work: a usable, web-scale datastore
aligned with the breadth of pretraining data. To this end, we introduce
CompactDS: a diverse, high-quality, web-scale datastore that achieves high
retrieval accuracy and subsecond latency on a single-node. The key insights are
(1) most web content can be filtered out without sacrificing coverage, and a
compact, high-quality subset is sufficient; and (2) combining in-memory
approximate nearest neighbor (ANN) retrieval and on-disk exact search balances
speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves
consistent accuracy improvements across all benchmarks and model sizes
(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,
and 19% on MATH. No single data source suffices alone, highlighting the
importance of diversity of sources (web crawls, curated math, academic papers,
textbooks). Finally, we show that our carefully designed in-house datastore
matches or outperforms web search engines such as Google Search, as well as
recently proposed, complex agent-based RAG systems--all while maintaining
simplicity, reproducibility, and self-containment. We release CompactDS and our
retrieval pipeline, supporting future research exploring retrieval-based AI
systems.

</details>


### [10] [La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation](https://arxiv.org/abs/2507.01299)
*Kai Liu,Bowen Xu,Shaoyu Wu,Xin Chen,Hao Zhou,Yongliang Tao,Lulu Hu*

Key words: 激活稀疏化、LLM推理、正交旋转、Top-K选择、计算加速

TL;DR: LaRoSA是一种新型的激活稀疏化方法，通过层间正交旋转改进LLM效率，无需额外训练或基于幅度的剪枝，实现稳定的稀疏性和计算加速。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法因需要恢复训练或依赖幅度剪枝，导致稀疏性波动或不稳定加速，LaRoSA旨在解决这些问题。

Method: 利用层间正交旋转转换激活为更适合稀疏化的形式，采用Top-K选择实现稳定稀疏化和加速。

Result: 在LLaMA2-7B（40%稀疏性）上，仅0.17困惑度差距和1.30倍加速，零样本任务准确率差距仅0.54%。

Conclusion: LaRoSA高效、通用，显著提升LLM推理效率，性能损失极小。

Abstract: Activation sparsity can reduce the computational overhead and memory
transfers during the forward pass of Large Language Model (LLM) inference.
Existing methods face limitations, either demanding time-consuming recovery
training that hinders real-world adoption, or relying on empirical
magnitude-based pruning, which causes fluctuating sparsity and unstable
inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse
Activation), a novel method for activation sparsification designed to improve
LLM efficiency without requiring additional training or magnitude-based
pruning. We leverage layerwise orthogonal rotations to transform input
activations into rotated forms that are more suitable for sparsification. By
employing a Top-K selection approach within the rotated activations, we achieve
consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA
is effective across various sizes and types of LLMs, demonstrating minimal
performance degradation and robust inference acceleration. Specifically, for
LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a
consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in
zero-shot tasks compared to the dense model to just 0.54%, while surpassing
TEAL by 1.77% and CATS by 17.14%.

</details>


### [11] [Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs](https://arxiv.org/abs/2507.01334)
*Nifu Dan,Yujun Cai,Yiwei Wang*

Key words: 物理推理，大型语言模型，Deepseek-R1，SciBench，少样本提示

TL;DR: 该研究探讨了推理模型在解决复杂物理问题中的能力，展示了在SciBench基准测试上的优异表现及其独特的符号推导推理模式，并指出少样本提示可进一步提升准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是解决大型语言模型在物理推理中的困难，结合概念理解和问题解决技巧，探索推理模型的潜力。

Method: 采用高级指令调整推理模型（如Deepseek-R1），在SciBench基准测试中评估多种物理问题。

Result: 模型不仅在复杂物理问题上达到最优准确率，还展现出独特的符号推导推理模式；少样本提示能进一步提升性能。

Conclusion: 推理模型在物理推理任务中表现出色，少样本提示策略为未来性能提升提供了可能。

Abstract: Navigating the complexities of physics reasoning has long been a difficult
task for Large Language Models (LLMs), requiring a synthesis of profound
conceptual understanding and adept problem-solving techniques. In this study,
we investigate the application of advanced instruction-tuned reasoning models,
such as Deepseek-R1, to address a diverse spectrum of physics problems curated
from the challenging SciBench benchmark. Our comprehensive experimental
evaluation reveals the remarkable capabilities of reasoning models. Not only do
they achieve state-of-the-art accuracy in answering intricate physics
questions, but they also generate distinctive reasoning patterns that emphasize
on symbolic derivation. Furthermore, our findings indicate that even for these
highly sophisticated reasoning models, the strategic incorporation of few-shot
prompting can still yield measurable improvements in overall accuracy,
highlighting the potential for continued performance gains.

</details>


### [12] [LEDOM: An Open and Fundamental Reverse Language Model](https://arxiv.org/abs/2507.01335)
*Xunjian Yin,Sitao Cheng,Yuxi Xie,Xinyu Hu,Li Lin,Xinyi Wang,Liangming Pan,William Yang Wang,Xiaojun Wan*

Key words: LEDOM, 逆向语言模型, 逆向奖励, 数学推理

TL;DR: LEDOM是首个纯逆向语言模型，通过逆向预测处理序列，展现独特特性并具有广泛应用潜力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨逆向语言模型作为通用基础模型的潜力，并通过LEDOM展示其在任务中的表现。

Method: 训练2B和7B参数的逆向语言模型LEDOM，处理逆向序列，并应用于逆向奖励任务。

Result: LEDOM在数学推理任务中显著提升性能，验证其逆向推理能力。

Conclusion: LEDOM具有独特特性和广泛潜力，未来将开源模型和训练数据。

Abstract: We introduce LEDOM, the first purely reverse language model, trained
autoregressively on 435B tokens with 2B and 7B parameter variants, which
processes sequences in reverse temporal order through previous token
prediction. For the first time, we present the reverse language model as a
potential foundational model across general tasks, accompanied by a set of
intriguing examples and insights. Based on LEDOM, we further introduce a novel
application: Reverse Reward, where LEDOM-guided reranking of forward language
model outputs leads to substantial performance improvements on mathematical
reasoning tasks. This approach leverages LEDOM's unique backward reasoning
capability to refine generation quality through posterior evaluation. Our
findings suggest that LEDOM exhibits unique characteristics with broad
application potential. We will release all models, training code, and
pre-training data to facilitate future research.

</details>


### [13] [Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy](https://arxiv.org/abs/2507.01352)
*Chris Yuhao Liu,Liang Zeng,Yuzhen Xiao,Jujie He,Jiacai Liu,Chaojie Wang,Rui Yan,Wei Shen,Fuxiang Zhang,Jiacheng Xu,Yang Liu,Yahui Zhou*

Key words: 奖励模型, 人类偏好, 数据筛选, 大规模数据集, 人类-AI协同

TL;DR: 当前奖励模型（RM）性能不佳，主要源于偏好数据集的局限性。作者提出大规模偏好数据集SynPref-40M和人类-AI协同的两阶段数据筛选流程，并训练出Skywork-Reward-V2系列模型，在多个基准测试中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有开源奖励模型在评价基准上表现不佳，无法捕捉复杂人类偏好。作者认为问题在于偏好数据集的范围狭窄、质量不足，希望通过高质量和大规模数据改进模型性能。

Method: 设计人类-AI协同的两阶段数据筛选流程，利用人类标注质量和AI的可扩展性，构建SynPref-40M数据集，并训练Skywork-Reward-V2系列模型。

Result: Skywork-Reward-V2在人类偏好对齐、安全性、抗风格偏置等方面表现优异，在七个主要基准测试中达到最先进水平。消融研究证实数据质量和规模均对性能提升至关重要。

Conclusion: 通过高质量人类-AI协同数据筛选和模型训练，Skywork-Reward-V2系列显著提升了奖励模型的性能，展示了协同数据筛选的潜力。

Abstract: Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.

</details>


### [14] [Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction](https://arxiv.org/abs/2507.01437)
*Ting Xu,Xiaoxiao Deng,Xiandong Meng,Haifeng Yang,Yan Wu*

Key words: 电子健康记录, 注意力机制, 多标签分类, Transformer, 医学文本建模

TL;DR: 论文提出了一种基于注意力机制的深度学习方法，用于电子健康记录文本的信息提取和多标签疾病预测，并在MIMIC-IV数据集上验证了其优越性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决电子健康记录文本的非结构化和高维度语义复杂性带来的挑战，实现信息提取和多标签疾病预测的统一建模。

Method: 采用Transformer架构和多层自注意力机制提取关键医学实体及其上下文关系，结合Sigmoid多标签分类器进行疾病预测，并引入上下文感知语义对齐机制增强模型能力。

Result: 模型在多种性能指标上均优于现有方法，且在数据规模、干扰水平和模型深度变化情况下表现出强泛化能力。

Conclusion: 该框架为处理真实世界临床文本提供了高效算法基础，对多标签医学文本建模任务具有实际意义。

Abstract: This paper addresses the challenges posed by the unstructured nature and
high-dimensional semantic complexity of electronic health record texts. A deep
learning method based on attention mechanisms is proposed to achieve unified
modeling for information extraction and multi-label disease prediction. The
study is conducted on the MIMIC-IV dataset. A Transformer-based architecture is
used to perform representation learning over clinical text. Multi-layer
self-attention mechanisms are employed to capture key medical entities and
their contextual relationships. A Sigmoid-based multi-label classifier is then
applied to predict multiple disease labels. The model incorporates a
context-aware semantic alignment mechanism, enhancing its representational
capacity in typical medical scenarios such as label co-occurrence and sparse
information. To comprehensively evaluate model performance, a series of
experiments were conducted, including baseline comparisons, hyperparameter
sensitivity analysis, data perturbation studies, and noise injection tests.
Results demonstrate that the proposed method consistently outperforms
representative existing approaches across multiple performance metrics. The
model maintains strong generalization under varying data scales, interference
levels, and model depth configurations. The framework developed in this study
offers an efficient algorithmic foundation for processing real-world clinical
texts and presents practical significance for multi-label medical text modeling
tasks.

</details>


### [15] [LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation](https://arxiv.org/abs/2507.01449)
*Tianyu Liu,Qitan Lv,Hao Li,Xing Gao,Xiao Sun*

Key words: 推测解码, 检索式方法, 推理加速, LLM

TL;DR: LogitSpec通过利用最后一个标记的logit推测下一个及下下个标记，并检索相关参考，解决了检索式推测解码中匹配不准确的问题，实现了高效推理加速。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有检索式推测解码方法难以找到匹配的准确草案标记，需要扩展检索范围以提高准确性。

Method: LogitSpec分为两步：利用最后一个logit推测下下个标记；检索与下一标记及下下标记相关的参考。

Result: LogitSpec实现了最高2.61倍的速度提升和每解码步骤平均3.28个接受标记。

Conclusion: LogitSpec是一种无需训练、即插即用的高效推理加速方法。

Abstract: Speculative decoding (SD), where a small draft model is employed to propose
draft tokens in advance and then the target model validates them in parallel,
has emerged as a promising technique for LLM inference acceleration. Many
endeavors to improve SD are to eliminate the need for a draft model and
generate draft tokens in a retrieval-based manner in order to further alleviate
the drafting overhead and significantly reduce the difficulty in deployment and
applications. However, retrieval-based SD relies on a matching paradigm to
retrieval the most relevant reference as the draft tokens, where these methods
often fail to find matched and accurate draft tokens. To address this
challenge, we propose LogitSpec to effectively expand the retrieval range and
find the most relevant reference as drafts. Our LogitSpec is motivated by the
observation that the logit of the last token can not only predict the next
token, but also speculate the next next token. Specifically, LogitSpec
generates draft tokens in two steps: (1) utilizing the last logit to speculate
the next next token; (2) retrieving relevant reference for both the next token
and the next next token. LogitSpec is training-free and plug-and-play, which
can be easily integrated into existing LLM inference frameworks. Extensive
experiments on a wide range of text generation benchmarks demonstrate that
LogitSpec can achieve up to 2.61 $\times$ speedup and 3.28 mean accepted tokens
per decoding step. Our code is available at
https://github.com/smart-lty/LogitSpec.

</details>


### [16] [Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities](https://arxiv.org/abs/2507.01479)
*Yingqiang Gao,Kaede Johnson,David Froehlich,Luisa Carrer,Sarah Ebling*

Key words: 自动文本简化, 大型语言模型, 直接偏好优化, 个性化, 无障碍

TL;DR: 本文提出了一种基于大型语言模型（LLMs）的自动文本简化（ATS）系统，并通过直接偏好优化（DPO）技术结合目标群体的反馈，实现了更高程度的个性化。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前LLM-based ATS系统在训练过程中缺乏目标群体的偏好反馈，导致生成的简化文本无法满足特定人群的个人需求。

Method: 扩展了传统的监督微调（SFT）方法，引入DPO技术，并设计了一个包含数据收集、模型选择、SFT和DPO后训练及评估的流程。

Result: 通过结合目标群体的反馈，显著提升了ATS系统的个性化水平。

Conclusion: 目标群体在设计和评估AI无障碍解决方案中的参与对提升个性化至关重要。

Abstract: Automatic text simplification (ATS) aims to enhance language accessibility
for various target groups, particularly persons with intellectual disabilities.
Recent advancements in generative AI, especially large language models (LLMs),
have substantially improved the quality of machine-generated text
simplifications, thereby mitigating information barriers for the target group.
However, existing LLM-based ATS systems do not incorporate preference feedback
on text simplifications during training, resulting in a lack of personalization
tailored to the specific needs of target group representatives.
  In this work, we extend the standard supervised fine-tuning (SFT) approach
for adapting LLM-based ATS models by leveraging a computationally efficient LLM
alignment technique -- direct preference optimization (DPO). Specifically, we
post-train LLM-based ATS models using human feedback collected from persons
with intellectual disabilities, reflecting their preferences on paired text
simplifications generated by mainstream LLMs. Furthermore, we propose a
pipeline for developing personalized LLM-based ATS systems, encompassing data
collection, model selection, SFT and DPO post-training, and evaluation. Our
findings underscore the necessity of active participation of target group
persons in designing personalized AI accessibility solutions aligned with human
expectations. This work represents a step towards personalizing inclusive AI
systems at the target-group level, incorporating insights not only from text
simplification experts but also from target group persons themselves.

</details>


### [17] [Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing](https://arxiv.org/abs/2507.01541)
*Álvaro Zaera,Diana Nicoleta Popa,Ivan Sekulic,Paolo Rosso*

Key words: OOS意图检测,任务导向对话系统,不确定性建模,大语言模型

TL;DR: 提出了一种结合不确定性建模和微调大语言模型的模块化框架，用于任务导向对话系统中的OOS意图检测，实现了高效与准确的平衡。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 任务是解决任务导向对话系统中的OOS意图检测问题，确保系统对未见和模糊查询的鲁棒性。

Method: 首先对范围意图检测分类器输出应用不确定性估计；其次对高不确定性实例触发微调大语言模型做最终决策。

Result: 该方法在OOS检测基准测试中表现优异，尤其在真实世界数据上取得先进成果。

Conclusion: 结合传统方法和LLMs，有效平衡计算效率与性能，提升了OOS检测能力。

Abstract: Out-of-scope (OOS) intent detection is a critical challenge in task-oriented
dialogue systems (TODS), as it ensures robustness to unseen and ambiguous
queries. In this work, we propose a novel but simple modular framework that
combines uncertainty modeling with fine-tuned large language models (LLMs) for
efficient and accurate OOS detection. The first step applies uncertainty
estimation to the output of an in-scope intent detection classifier, which is
currently deployed in a real-world TODS handling tens of thousands of user
interactions daily. The second step then leverages an emerging LLM-based
approach, where a fine-tuned LLM is triggered to make a final decision on
instances with high uncertainty. Unlike prior approaches, our method
effectively balances computational efficiency and performance, combining
traditional approaches with LLMs and yielding state-of-the-art results on key
OOS detection benchmarks, including real-world OOS data acquired from a
deployed TODS.

</details>


### [18] [Is External Information Useful for Stance Detection with LLMs?](https://arxiv.org/abs/2507.01543)
*Quang Minh Nguyen,Taegyoon Kim*

Key words: 立场检测, 大型语言模型, 外部信息, 性能下降, 信息偏见

TL;DR: 研究发现，Wikipedia和网络搜索的外部信息在大型语言模型（LLM）的立场检测任务中反而会降低性能，原因是模型倾向于根据外部信息的立场和情感进行预测，而非基于真实立场。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨外部信息（如Wikipedia和网络搜索）是否会对大型语言模型在立场检测任务中的表现产生影响，这一问题尚未有明确答案。

Method: 在三个数据集和12个目标上，对八种大型语言模型进行了系统评估，分析了外部信息的影响，并探讨了提示方式和微调的作用。

Result: 外部信息在大多数情况下降低了性能，F1分数最多下降27.9%，且模型倾向于根据外部信息的立场进行预测。

Conclusion: 外部信息在LLM中引入偏差，可能导致性能下降，这与基于BERT的研究结果相反，凸显了LLM立场分类器中的信息偏见风险。

Abstract: In the stance detection task, a text is classified as either favorable,
opposing, or neutral towards a target. Prior work suggests that the use of
external information, e.g., excerpts from Wikipedia, improves stance detection
performance. However, whether or not such information can benefit large
language models (LLMs) remains an unanswered question, despite their wide
adoption in many reasoning tasks. In this study, we conduct a systematic
evaluation on how Wikipedia and web search external information can affect
stance detection across eight LLMs and in three datasets with 12 targets.
Surprisingly, we find that such information degrades performance in most cases,
with macro F1 scores dropping by up to 27.9\%. We explain this through
experiments showing LLMs' tendency to align their predictions with the stance
and sentiment of the provided information rather than the ground truth stance
of the given text. We also find that performance degradation persists with
chain-of-thought prompting, while fine-tuning mitigates but does not fully
eliminate it. Our findings, in contrast to previous literature on BERT-based
systems which suggests that external information enhances performance,
highlight the risks of information biases in LLM-based stance classifiers. Code
is available at https://github.com/ngqm/acl2025-stance-detection.

</details>


### [19] [Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation](https://arxiv.org/abs/2507.01594)
*Shutong Feng,Hsien-chin Lin,Nurul Lubis,Carel van Niekerk,Michael Heck,Benjamin Ruppik,Renato Vukovic,Milica Gašić*

Key words: 任务导向对话系统, 大语言模型, 强化学习, 情感响应

TL;DR: 该论文提出了一种基于大语言模型（LLMs）的任务导向对话系统LUSTER，通过端到端的强化学习优化任务成功率和情感响应能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的任务导向对话系统在噪声和模糊的对话环境中，难以同时兼顾任务成功、情感理解和信息传递的精确性。

Method: 论文设计了LUSTER系统，结合LLMs和结构化奖励模型，利用短期（用户情感）和长期（任务成功）奖励进行端到端强化学习。

Result: 实验表明，LUSTER系统在任务成功率和情感响应方面表现更优。

Conclusion: 结合LLMs能力和结构化奖励建模，可以构建更稳健且情感智能的对话系统。

Abstract: Task-oriented dialogue (ToD) systems are designed to help users achieve
specific goals through natural language interaction. While recent advances in
large language models (LLMs) have significantly improved linguistic fluency and
contextual understanding, building effective and emotionally intelligent ToD
systems remains a complex challenge. Effective ToD systems must optimise for
task success, emotional understanding and responsiveness, and precise
information conveyance, all within inherently noisy and ambiguous
conversational environments. In this work, we investigate architectural,
representational, optimisational as well as emotional considerations of ToD
systems. We set up systems covering these design considerations with a
challenging evaluation environment composed of a natural-language user
simulator coupled with an imperfect natural language understanding module. We
propose \textbf{LUSTER}, an \textbf{L}LM-based \textbf{U}nified \textbf{S}ystem
for \textbf{T}ask-oriented dialogue with \textbf{E}nd-to-end
\textbf{R}einforcement learning with both short-term (user sentiment) and
long-term (task success) rewards. Our findings demonstrate that combining LLM
capability with structured reward modelling leads to more resilient and
emotionally responsive ToD systems, offering a practical path forward for
next-generation conversational agents.

</details>


### [20] [Chart Question Answering from Real-World Analytical Narratives](https://arxiv.org/abs/2507.01627)
*Maeve Hutchinson,Radu Jianu,Aidan Slingsby,Jo Wood,Pranava Madhyastha*

Key words: 图表问答, 数据集, 多视图图表, 自然语言问题

TL;DR: 提出一个新的图表问答数据集，基于可视化笔记本构建，包含多视图图表和自然语言问题，反映了真实的推理流程。现有模型表现不佳，GPT-4.1准确率仅69.3%。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有图表问答数据集缺乏真实场景的多样性，希望通过构建更贴近实际推理流程的数据集提升研究。

Method: 从可视化笔记本中收集多视图图表和自然语言问题，构建新的数据集。

Result: 测试表明现有模型表现有限，GPT-4.1准确率为69.3%。

Conclusion: 真实场景的图表问答更具挑战性，需进一步优化模型。

Abstract: We present a new dataset for chart question answering (CQA) constructed from
visualization notebooks. The dataset features real-world, multi-view charts
paired with natural language questions grounded in analytical narratives.
Unlike prior benchmarks, our data reflects ecologically valid reasoning
workflows. Benchmarking state-of-the-art multimodal large language models
reveals a significant performance gap, with GPT-4.1 achieving an accuracy of
69.3%, underscoring the challenges posed by this more authentic CQA setting.

</details>


### [21] [Confidence and Stability of Global and Pairwise Scores in NLP Evaluation](https://arxiv.org/abs/2507.01633)
*Georgii Levtsov,Dmitry Ustalov*

Key words: NLP, 模型评估, 全局评分, 成对比较, Bradley-Terry

TL;DR: 论文比较了NLP中全局评分和成对比较两种模型评估策略的优缺点，发现全局评分更稳定，但对强模型低估；成对比较更能识别低分中的强者，但需更多比较。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着指令调优神经语言模型的发展，NLP评估方式从全局评分转向成对比较。本文旨在帮助选择更合适的评估策略。

Method: 在合成和真实数据集上，使用全局指标和Bradley-Terry模型进行实验比较。

Result: 全局评分总体排名更可靠，但可能低估强模型；成对比较能更好识别低分强者。

Conclusion: 两种策略各有优劣，需根据任务需求选择。

Abstract: With the advent of highly capable instruction-tuned neural language models,
benchmarking in natural language processing (NLP) is increasingly shifting
towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional
global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper
empirically investigates the strengths and weaknesses of both global scores and
pairwise comparisons to aid decision-making in selecting appropriate model
evaluation strategies. Through computational experiments on synthetic and
real-world datasets using standard global metrics and the popular Bradley-Terry
model for pairwise comparisons, we found that while global scores provide more
reliable overall rankings, they can underestimate strong models with rare,
significant errors or low confidence. Conversely, pairwise comparisons are
particularly effective for identifying strong contenders among models with
lower global scores, especially where quality metrics are hard to define (e.g.,
text generation), though they require more comparisons to converge if ties are
frequent. Our code and data are available at
https://github.com/HSPyroblast/srw-ranking under a permissive license.

</details>


### [22] [Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings](https://arxiv.org/abs/2507.01645)
*Rifki Afina Putri*

Key words: 预训练语言模型,低资源语言,迁移学习,情感分析,MAD-X

TL;DR: 该研究探讨了预训练语言模型在低资源印尼本地语言情感分析任务中的迁移性，发现模型性能因语言类别（已见、部分见、未见）而异，MAD-X适配器显著提升了表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在评估预训练语言模型对低资源印尼本地语言的迁移能力，特别是情感分析任务，以填补相关研究的空白。

Method: 通过零样本学习和适配器迁移（MAD-X）在十种本地语言上测试模型性能，包括单语BERT、多语mBERT/XLM-R，并按语言类别分组分析。

Result: 多语言模型在已见语言上表现最佳，适配器显著提升性能；语言模型是否预训练过是迁移成功的关键因素。

Conclusion: 模型对语言的先前接触是迁移成功的主要预测因素，适配器方法对低资源语言迁移有显著帮助。

Abstract: In this paper, we investigate the transferability of pre-trained language
models to low-resource Indonesian local languages through the task of sentiment
analysis. We evaluate both zero-shot performance and adapter-based transfer on
ten local languages using models of different types: a monolingual Indonesian
BERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based
approach called MAD-X. To better understand model behavior, we group the target
languages into three categories: seen (included during pre-training), partially
seen (not included but linguistically related to seen languages), and unseen
(absent and unrelated in pre-training data). Our results reveal clear
performance disparities across these groups: multilingual models perform best
on seen languages, moderately on partially seen ones, and poorly on unseen
languages. We find that MAD-X significantly improves performance, especially
for seen and partially seen languages, without requiring labeled data in the
target language. Additionally, we conduct a further analysis on tokenization
and show that while subword fragmentation and vocabulary overlap with
Indonesian correlate weakly with prediction quality, they do not fully explain
the observed performance. Instead, the most consistent predictor of transfer
success is the model's prior exposure to the language, either directly or
through a related language.

</details>


### [23] [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness](https://arxiv.org/abs/2507.01702)
*Zixin Chen,Hongzhan Lin,Kaixin Li,Ziyang Luo,Zhen Ye,Guang Chen,Zhiyong Huang,Jing Ma*

Key words: 多模态大语言模型；有害模因；动态评估；多智能体；AdamMeme

TL;DR: 该论文提出了AdamMeme框架，通过多智能体协作动态评估多模态大语言模型（mLLMs）对有害模因的理解能力，弥补了现有静态评估的不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 社交媒体中的有害模因动态演变，现有静态评估无法全面测试mLLMs的理解能力。

Method: 设计了AdamMeme框架，通过多智能体合作迭代更新挑战性模因样本，动态评估mLLMs。

Result: 实验表明该框架能系统揭示不同mLLMs的性能差异及其弱点。

Conclusion: AdamMeme为动态评估mLLMs的推理能力提供了灵活且全面的方法。

Abstract: The proliferation of multimodal memes in the social media era demands that
multimodal Large Language Models (mLLMs) effectively understand meme
harmfulness. Existing benchmarks for assessing mLLMs on harmful meme
understanding rely on accuracy-based, model-agnostic evaluations using static
datasets. These benchmarks are limited in their ability to provide up-to-date
and thorough assessments, as online memes evolve dynamically. To address this,
we propose AdamMeme, a flexible, agent-based evaluation framework that
adaptively probes the reasoning capabilities of mLLMs in deciphering meme
harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive
evaluations by iteratively updating the meme data with challenging samples,
thereby exposing specific limitations in how mLLMs interpret harmfulness.
Extensive experiments show that our framework systematically reveals the
varying performance of different target mLLMs, offering in-depth, fine-grained
analyses of model-specific weaknesses. Our code is available at
https://github.com/Lbotirx/AdamMeme.

</details>


### [24] [Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach](https://arxiv.org/abs/2507.01715)
*Aditya Tomar,Rudra Murthy,Pushpak Bhattacharyya*

Key words: 偏差检测，刻板印象，联合学习，StereoBias，公平AI

TL;DR: 本研究探索联合学习偏差与刻板印象检测任务，提升模型性能，引入StereoBias数据集，实验表明联合训练显著优于单独训练。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 语言模型中的偏差和刻板印象可能造成危害，尤其在敏感领域。

Method: 使用StereoBias数据集，比较编码器模型与微调解码器模型，采用联合训练策略。

Result: 联合训练显著提升偏差检测性能，且改进来自偏差与刻板印象的关联而非多任务学习。

Conclusion: 利用刻板印象信息可构建更公平有效的AI系统。

Abstract: Bias and stereotypes in language models can cause harm, especially in
sensitive areas like content moderation and decision-making. This paper
addresses bias and stereotype detection by exploring how jointly learning these
tasks enhances model performance. We introduce StereoBias, a unique dataset
labeled for bias and stereotype detection across five categories: religion,
gender, socio-economic status, race, profession, and others, enabling a deeper
study of their relationship. Our experiments compare encoder-only models and
fine-tuned decoder-only models using QLoRA. While encoder-only models perform
well, decoder-only models also show competitive results. Crucially, joint
training on bias and stereotype detection significantly improves bias detection
compared to training them separately. Additional experiments with sentiment
analysis confirm that the improvements stem from the connection between bias
and stereotypes, not multi-task learning alone. These findings highlight the
value of leveraging stereotype information to build fairer and more effective
AI systems.

</details>


### [25] [LLMs for Legal Subsumption in German Employment Contracts](https://arxiv.org/abs/2507.01734)
*Oliver Wardas,Florian Matthes*

Key words: NLP, LLM, 法律文本分类, 雇佣合同, 可解释性

TL;DR: 论文探讨了NLP在法律领域的应用，通过扩展数据集并使用LLM和法律上下文评估德国雇佣合同条款的合法性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决数据驱动方法在法律环境中缺乏可解释性和可信度的问题，探索LLM在法律文本分类中的潜力。

Method: 与法律专家合作扩展数据集，测试LLM在三种法律上下文（无上下文、全文法律文本、精简指南）下的分类表现。

Result: 全文法律文本略微提升性能，精简指南显著提高无效条款的召回率和F1分数（80%），但仍低于人类律师水平。

Conclusion: LLM在协助法律条款审查中有潜力，但现有方法仍有局限性。

Abstract: Legal work, characterized by its text-heavy and resource-intensive nature,
presents unique challenges and opportunities for NLP research. While
data-driven approaches have advanced the field, their lack of interpretability
and trustworthiness limits their applicability in dynamic legal environments.
To address these issues, we collaborated with legal experts to extend an
existing dataset and explored the use of Large Language Models (LLMs) and
in-context learning to evaluate the legality of clauses in German employment
contracts. Our work evaluates the ability of different LLMs to classify clauses
as "valid," "unfair," or "void" under three legal context variants: no legal
context, full-text sources of laws and court rulings, and distilled versions of
these (referred to as examination guidelines). Results show that full-text
sources moderately improve performance, while examination guidelines
significantly enhance recall for void clauses and weighted F1-Score, reaching
80\%. Despite these advancements, LLMs' performance when using full-text
sources remains substantially below that of human lawyers. We contribute an
extended dataset, including examination guidelines, referenced legal sources,
and corresponding annotations, alongside our code and all log files. Our
findings highlight the potential of LLMs to assist lawyers in contract legality
review while also underscoring the limitations of the methods presented.

</details>


### [26] [Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results](https://arxiv.org/abs/2507.01764)
*Matteo Di Cristofaro*

Key words: 标记化,语料库语言学,表情符号,同形异义词,预处理

TL;DR: 本文探讨了标记化过程中差异对语言数据表示和分析结果有效性的影响，重点研究了表情符号和同形异义词带来的挑战，并提出了预处理方法以确保语料库的准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在解决标记化过程中因表情符号和同形异义词引起的语言数据表示不一致问题，以确保语料分析的可靠性和可重复性。

Method: 通过详细分析表情符号和同形异义词的标记化挑战，提出了预处理方法以确保数字化文本在语料库中的准确表示。

Result: 研究发现，对语言和技术细节的深入理解是提高语料分析准确性的关键，对定量和定性研究方法均有重要意义。

Conclusion: 标记化过程的准确性对语料库分析至关重要，预处理方法的改进可以有效提升研究结果的可靠性。

Abstract: Tokenisation - "the process of splitting text into atomic parts" (Brezina &
Timperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides
the basis for any applicable quantitative method (e.g. collocations) while
ensuring the reliability of qualitative approaches. This paper examines how
discrepancies in tokenisation affect the representation of language data and
the validity of analytical findings: investigating the challenges posed by
emojis and homoglyphs, the study highlights the necessity of preprocessing
these elements to maintain corpus fidelity to the source data. The research
presents methods for ensuring that digital texts are accurately represented in
corpora, thereby supporting reliable linguistic analysis and guaranteeing the
repeatability of linguistic interpretations. The findings emphasise the
necessity of a detailed understanding of both linguistic and technical aspects
involved in digital textual data to enhance the accuracy of corpus analysis,
and have significant implications for both quantitative and qualitative
approaches in corpus-based research.

</details>


### [27] [MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2507.01785)
*Zhixun Chen,Ping Guo,Wenhan Han,Yifan Zhang,Binbin Liu,Haobin Lin,Fengze Liu,Yan Zhao,Bingni Zhang,Taifeng Wang,Yin Zheng,Meng Fang*

Key words: 数据质量、多语言评分、MuRating、LLaMA模型、知识密集型任务

TL;DR: MuRating是一个可扩展框架，通过将高质量英语数据信号扩展到17种目标语言，提升多语言数据质量选择，显著提高模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有数据质量选择方法主要关注英语，缺乏多语言支持，作者希望通过MuRating填补这一空白。

Method: MuRating通过聚合多个英语评分器的信号进行成对比较，训练一个统一的文档质量评分器，并利用翻译将其扩展到多语言数据。

Result: 在LLaMA模型上，MuRating在英语和多语言任务中的准确性均优于现有方法，尤其在知识密集型任务中表现突出。

Conclusion: MuRating为多语言数据质量提供了有效解决方案，同时指出了翻译质量和叙事材料不足等未来改进方向。

Abstract: Data quality is a critical driver of large language model performance, yet
existing model-based selection methods focus almost exclusively on English. We
introduce MuRating, a scalable framework that transfers high-quality English
data-quality signals into a single rater for 17 target languages. MuRating
aggregates multiple English "raters" via pairwise comparisons to learn unified
document-quality scores,then projects these judgments through translation to
train a multilingual evaluator on monolingual, cross-lingual, and parallel text
pairs. Applied to web data, MuRating selects balanced subsets of English and
multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to
strong baselines, including QuRater, AskLLM, DCLM and so on, our approach
boosts average accuracy on both English benchmarks and multilingual
evaluations, with especially large gains on knowledge-intensive tasks. We
further analyze translation fidelity, selection biases, and underrepresentation
of narrative material, outlining directions for future work.

</details>


### [28] [Probing Evaluation Awareness of Language Models](https://arxiv.org/abs/2507.01786)
*Jord Nguyen,Khiem Hoang,Carlo Leonardo Attubato,Felix Hofstätter*

Key words: 语言模型,评估意识,安全评估,治理,黑盒方法

TL;DR: 该研究发现语言模型（如Llama-3-70B-Instruct）能够区分测试和部署阶段，这被称为评估意识。研究通过线性探针方法表明，模型内部能够分辨真实评估和部署提示，凸显了确保可信评估的重要性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机在于探讨语言模型的评估意识对AI安全与治理的影响，尤其是模型可能通过区分测试与部署阶段来规避评估，从而影响AI治理框架的可靠性。

Method: 研究使用线性探针方法分析Llama-3-70B-Instruct模型，以区分真实世界的评估和部署提示，并探讨模型内部的评估意识表征。

Result: 结果表明，线性探针可以有效分离评估和部署提示，且当前的安全评估被模型正确分类，暗示这些评估可能显得不真实或人为。

Conclusion: 研究强调了确保可信评估的必要性，并展示了如何利用模型内部机制支持黑盒安全审计。

Abstract: Language models can distinguish between testing and deployment phases -- a
capability known as evaluation awareness. This has significant safety and
policy implications, potentially undermining the reliability of evaluations
that are central to AI governance frameworks and voluntary industry
commitments. In this paper, we study evaluation awareness in
Llama-3.3-70B-Instruct. We show that linear probes can separate real-world
evaluation and deployment prompts, suggesting that current models internally
represent this distinction. We also find that current safety evaluations are
correctly classified by the probes, suggesting that they already appear
artificial or inauthentic to models. Our findings underscore the importance of
ensuring trustworthy evaluations and understanding deceptive capabilities. More
broadly, our work showcases how model internals may be leveraged to support
blackbox methods in safety audits, especially for future models more competent
at evaluation awareness and deception.

</details>


### [29] [How Do Vision-Language Models Process Conflicting Information Across Modalities?](https://arxiv.org/abs/2507.01790)
*Tianze Hua,Tian Yun,Ellie Pavlick*

Key words: 多模态模型、视觉语言、冲突输入、注意机制、路由头

TL;DR: 论文研究了多模态AI模型在输入信息冲突时的行为，发现模型会偏向某一模态，并揭示了内部注意机制如何影响模态选择。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探究多模态模型如何处理冲突输入，以理解其内部表示和行为偏好。

Method: 通过向视觉语言模型提供不一致的输入（如图像与标题矛盾），并测试模型对不同模态的反应。

Result: 模型通常偏向某一模态，且可通过特定注意头调节模态选择，发现模态无关的“路由头”能提升性能。

Conclusion: 研究为理解和控制多模态模型中冲突信号的处理提供了基础。

Abstract: AI models are increasingly required to be multimodal, integrating disparate
input streams into a coherent state representation on which subsequent
behaviors and actions can be based. This paper seeks to understand how such
models behave when input streams present conflicting information. Focusing
specifically on vision-language models, we provide inconsistent inputs (e.g.,
an image of a dog paired with the caption "A photo of a cat") and ask the model
to report the information present in one of the specific modalities (e.g.,
"What does the caption say / What is in the image?"). We find that models often
favor one modality over the other, e.g., reporting the image regardless of what
the caption says, but that different models differ in which modality they
favor. We find evidence that the behaviorally preferred modality is evident in
the internal representational structure of the model, and that specific
attention heads can restructure the representations to favor one modality over
the other. Moreover, we find modality-agnostic "router heads" which appear to
promote answers about the modality requested in the instruction, and which can
be manipulated or transferred in order to improve performance across datasets
and modalities. Together, the work provides essential steps towards identifying
and controlling if and how models detect and resolve conflicting signals within
complex multimodal environments.

</details>


### [30] [The Anatomy of Evidence: An Investigation Into Explainable ICD Coding](https://arxiv.org/abs/2507.01802)
*Katharina Beckh,Elisa Studeny,Sujan Sai Gannamaneni,Dario Antweiler,Stefan Rüping*

Key words: 自动医疗编码，MDACE数据集，可解释性，证据提取

TL;DR: 介绍了对MDACE数据集的深入分析，评估了可解释性医疗编码系统的合理性，并提出了改进建议。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 自动医疗编码可以简化文档和计费流程，但需要透明度。MDACE数据集为解决这一问题提供了资源。

Method: 分析了MDACE数据集，评估了当前可解释性医疗编码系统，提出了匹配度量。

Result: 真实证据与代码描述有一定程度对齐，先进方法与真实证据重叠度高。

Conclusion: 提供了开发和评估可解释性医疗编码系统的建议。

Abstract: Automatic medical coding has the potential to ease documentation and billing
processes. For this task, transparency plays an important role for medical
coders and regulatory bodies, which can be achieved using explainability
methods. However, the evaluation of these approaches has been mostly limited to
short text and binary settings due to a scarcity of annotated data. Recent
efforts by Cheng et al. (2023) have introduced the MDACE dataset, which
provides a valuable resource containing code evidence in clinical records. In
this work, we conduct an in-depth analysis of the MDACE dataset and perform
plausibility evaluation of current explainable medical coding systems from an
applied perspective. With this, we contribute to a deeper understanding of
automatic medical coding and evidence extraction. Our findings reveal that
ground truth evidence aligns with code descriptions to a certain degree. An
investigation into state-of-the-art approaches shows a high overlap with ground
truth evidence. We propose match measures and highlight success and failure
cases. Based on our findings, we provide recommendations for developing and
evaluating explainable medical coding systems.

</details>


### [31] [Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes](https://arxiv.org/abs/2507.01810)
*Nikita Neveditsin,Pawan Lingras,Vijay Mago*

Key words: 语言模型, 序列化格式, 临床笔记, 可解析性

TL;DR: 该论文比较了小型语言模型在临床笔记中生成的结构化输出的可解析性，发现JSON格式表现最佳，并分析了影响解析性的因素。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在为隐私敏感的临床环境中部署语言模型时选择合适的序列化格式和提示设计提供实用指导。

Method: 评估了三种序列化格式（JSON、YAML、XML）的可解析性，并通过针对性提示和模型大小进行了实验。

Result: JSON格式表现最佳，结构鲁棒性随针对性提示和较大模型提升，但对较长文档和某些笔记类型下降。

Conclusion: 研究结果为临床环境中的序列化格式选择和提示设计提供了实际建议。

Abstract: We present a comparative analysis of the parseability of structured outputs
generated by small language models for open attribute-value extraction from
clinical notes. We evaluate three widely used serialization formats: JSON,
YAML, and XML, and find that JSON consistently yields the highest parseability.
Structural robustness improves with targeted prompting and larger models, but
declines for longer documents and certain note types. Our error analysis
identifies recurring format-specific failure patterns. These findings offer
practical guidance for selecting serialization formats and designing prompts
when deploying language models in privacy-sensitive clinical settings.

</details>


### [32] [Low-Perplexity LLM-Generated Sequences and Where To Find Them](https://arxiv.org/abs/2507.01844)
*Arthur Wuhrmann,Anastasiia Kucherenko,Andrei Kucharavy*

Key words: 大型语言模型、低困惑度序列、训练数据、透明度

TL;DR: 该论文提出了一种系统方法，通过分析低困惑度序列（模型生成的高概率文本片段），探索LLMs如何利用和复制其训练数据，并发现部分片段无法映射到训练语料中。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着大型语言模型（LLMs）的广泛应用，了解训练数据如何影响其输出对于透明度、责任性、隐私和公平性至关重要。

Method: 作者引入了以低困惑度序列为中心的系统分析方法，可靠地提取这些序列并将其追溯至训练数据中的来源。

Result: 研究发现，大量低困惑度片段无法与训练语料匹配；对于匹配的部分，论文量化了其在源文档中的分布。

Conclusion: 这种方法有助于更好地理解LLMs训练数据对其行为的影响。

Abstract: As Large Language Models (LLMs) become increasingly widespread, understanding
how specific training data shapes their outputs is crucial for transparency,
accountability, privacy, and fairness. To explore how LLMs leverage and
replicate their training data, we introduce a systematic approach centered on
analyzing low-perplexity sequences - high-probability text spans generated by
the model. Our pipeline reliably extracts such long sequences across diverse
topics while avoiding degeneration, then traces them back to their sources in
the training data. Surprisingly, we find that a substantial portion of these
low-perplexity spans cannot be mapped to the corpus. For those that do match,
we quantify the distribution of occurrences across source documents,
highlighting the scope and nature of verbatim recall and paving a way toward
better understanding of how LLMs training data impacts their behavior.

</details>


### [33] [Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages](https://arxiv.org/abs/2507.01853)
*Samridhi Raj Sinha,Rajvee Sheth,Abhishek Upperwal,Mayank Singh*

Key words: LLM, 评估框架, 多语言, 印度语言, 开源

TL;DR: EKA-EVAL是一个统一的生产级评估框架，集成了35个基准测试，包括10个印度语言特定数据集，支持分布式推理和量化，降低了多语言基准测试的门槛。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型快速发展，需要超越英语中心的评估框架以满足语言多样地区（如印度）的需求。

Method: 开发EKA-EVAL框架，整合多种基准测试，支持分布式推理、量化和多GPU使用。

Result: EKA-EVAL成为首个面向全球和印度语言模型的端到端可扩展评估套件。

Conclusion: 该框架开源，致力于建立多语言评估生态系统。

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for evaluation frameworks that go beyond English centric benchmarks and
address the requirements of linguistically diverse regions such as India. We
present EKA-EVAL, a unified and production-ready evaluation framework that
integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning
categories like reasoning, mathematics, tool use, long-context understanding,
and reading comprehension. Compared to existing Indian language evaluation
tools, EKA-EVAL offers broader benchmark coverage, with built-in support for
distributed inference, quantization, and multi-GPU usage. Our systematic
comparison positions EKA-EVAL as the first end-to-end, extensible evaluation
suite tailored for both global and Indic LLMs, significantly lowering the
barrier to multilingual benchmarking. The framework is open-source and publicly
available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA
initiative (https://eka.soket.ai), which aims to scale up to over 100
benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.

</details>


### [34] [DIY-MKG: An LLM-Based Polyglot Language Learning System](https://arxiv.org/abs/2507.01872)
*Kenan Tang,Yanhong Li,Yao Qin*

Key words: 语言学习, 多语言, 知识图谱, 个性化学习, LLM

TL;DR: DIY-MKG是一个支持多语言学习的开源系统，通过构建个性化的词汇知识图谱和动态测试生成，解决了现有工具在多语言联系、个性化和认知负荷方面的不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有语言学习工具在多语言联系、个性化和认知负荷方面存在不足，需要一种更灵活的系统来支持多语言学习者。

Method: 设计了DIY-MKG系统，利用LLM构建个性化词汇图谱，并通过动态测试和反馈机制增强学习效果。

Result: 评估显示DIY-MKG在多语言词汇扩展和测试生成方面表现可靠且准确。

Conclusion: DIY-MKG在多语言学习中表现出强大的潜力和稳健性。

Abstract: Existing language learning tools, even those powered by Large Language Models
(LLMs), often lack support for polyglot learners to build linguistic
connections across vocabularies in multiple languages, provide limited
customization for individual learning paces or needs, and suffer from
detrimental cognitive offloading. To address these limitations, we design
Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system
that supports polyglot language learning. DIY-MKG allows the user to build
personalized vocabulary knowledge graphs, which are constructed by selective
expansion with related words suggested by an LLM. The system further enhances
learning through rich annotation capabilities and an adaptive review module
that leverages LLMs for dynamic, personalized quiz generation. In addition,
DIY-MKG allows users to flag incorrect quiz questions, simultaneously
increasing user engagement and providing a feedback loop for prompt refinement.
Our evaluation of LLM-based components in DIY-MKG shows that vocabulary
expansion is reliable and fair across multiple languages, and that the
generated quizzes are highly accurate, validating the robustness of DIY-MKG.

</details>


### [35] [MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants](https://arxiv.org/abs/2507.01887)
*Dongyi Ding,Tiannan Wang,Chenghao Zhu,Meiling Tao,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Key words: 大型语言模型,小型语言模型,链式推理,蒸馏学习,教师助手

TL;DR: 该论文提出了一种名为MiCoTAl的框架，利用中等规模模型作为教师助手，解决小型语言模型（SLMs）在长链推理任务中的学习能力问题，显著提升了SLMs的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型（LLMs）在长链推理任务中表现优秀，但由于其计算资源需求高，难以广泛应用；而小型语言模型（SLMs）则因容量有限难以学习长链推理。为解决这一问题，提出了MiCoTAl框架。

Method: 采用中等规模模型作为教师助手，利用中等长度的链式推理序列，弥合SLMs的容量和推理长度差距，并通过蒸馏提升其性能。

Result: 实验结果显示，通过MiCoTAl蒸馏的SLMs在多个基准测试（如AIME2024、AMC等）上性能显著提升，Qwen2.5-7B-Instruct和Qwen2.5-3B-Instruct的平均分数分别提高了3.47和3.93。

Conclusion: MiCoTAl为SLMs的长链推理蒸馏提供了有效方法，实验验证了其有效性，并为进一步研究奠定了基础。

Abstract: Large language models (LLMs) excel at reasoning tasks requiring long thought
sequences for planning, reflection, and refinement. However, their substantial
model size and high computational demands are impractical for widespread
deployment. Yet, small language models (SLMs) often struggle to learn long-form
CoT reasoning due to their limited capacity, a phenomenon we refer to as the
"SLMs Learnability Gap". To address this, we introduce
\textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation
(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA
employs intermediate-sized models as teacher assistants and utilizes
intermediate-length CoT sequences to bridge both the capacity and reasoning
length gaps. Our experiments on downstream tasks demonstrate that although SLMs
distilled from large teachers can perform poorly, by applying MiCoTA, they
achieve significant improvements in reasoning performance. Specifically,
Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and
3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and
GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform
a quantitative experiment demonstrating that our method produces data more
closely aligned with base SLM distributions. Our insights pave the way for
future research into long-CoT data distillation for SLMs.

</details>


### [36] [High-Layer Attention Pruning with Rescaling](https://arxiv.org/abs/2507.01900)
*Songtao Liu,Peng Liu*

Key words: 大语言模型, 剪枝, 注意力头, 自适应校准

TL;DR: 论文提出了一种新的剪枝算法，专注于修剪语言模型中高层注意力头，并引入自适应重新缩放参数以校准剪枝后的表示规模。实验表明，该方法在生成任务中显著优于现有基线。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的训练无关结构化剪枝方法通常不考虑注意力头在网络架构中的位置，导致性能下降。为了改进这一点，作者提出了一种更智能的剪枝策略。

Method: 提出了一种新的剪枝算法，针对模型中高层注意力头进行修剪，并引入自适应重新缩放参数以校准剪枝后的表示规模。

Result: 在多个大规模语言模型（如LLaMA3.1-8B等）和27个数据集上的实验表明，该方法优于现有剪枝方法，尤其在生成任务中表现突出。

Conclusion: 该研究通过智能剪枝和自适应校准，显著提升了剪枝后模型的性能，尤其是在生成任务中。

Abstract: Pruning is a highly effective approach for compressing large language models
(LLMs), significantly reducing inference latency. However, conventional
training-free structured pruning methods often employ a heuristic metric that
indiscriminately removes some attention heads across all pruning layers,
without considering their positions within the network architecture. In this
work, we propose a novel pruning algorithm that strategically prunes attention
heads in the model's higher layers. Since the removal of attention heads can
alter the magnitude of token representations, we introduce an adaptive
rescaling parameter that calibrates the representation scale post-pruning to
counteract this effect. We conduct comprehensive experiments on a wide range of
LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our
evaluation includes both generation and discriminative tasks across 27
datasets. The results consistently demonstrate that our method outperforms
existing structured pruning methods. This improvement is particularly notable
in generation tasks, where our approach significantly outperforms existing
baselines.

</details>


### [37] [AI4Research: A Survey of Artificial Intelligence for Scientific Research](https://arxiv.org/abs/2507.01903)
*Qiguang Chen,Mingda Yang,Libo Qin,Jinhao Liu,Zheng Yan,Jiannan Guan,Dengyun Peng,Yiyan Ji,Hanjing Li,Mengkang Hu,Yimeng Zhang,Yihao Liang,Yuhang Zhou,Jiaqi Wang,Zhi Chen,Wanxiang Che*

Key words: AI4Research, 大语言模型, 系统分类法, 自动化实验, 多学科应用

TL;DR: 本文是一篇关于AI在研究中应用的综述，填补了AI4Research领域的空白，提出了系统分类法、新前沿方向，并汇集了丰富的应用资源。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: AI（尤其是大语言模型）在复杂领域的显著能力激发了将其应用于研究过程的需求，但此前缺乏关于AI4Research的综合综述，阻碍了该领域的进一步发展。

Method: 本文提出了一种系统分类法来分类AI4Research中的主流任务，并指出了研究空白与未来方向，同时汇集了多学科应用、数据与工具资源。

Result: 通过综述，本文为AI4Research领域提供了统一视角，明确了五个主流任务、研究缺口和未来方向，并提供了丰富的应用与资源。

Conclusion: 本文填补了AI4Research领域的空白，为研究者提供了资源与方向，有望推动该领域的创新突破。

Abstract: Recent advancements in artificial intelligence (AI), particularly in large
language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated
remarkable capabilities in complex domains such as logical reasoning and
experimental coding. Motivated by these advancements, numerous studies have
explored the application of AI in the innovation process, particularly in the
context of scientific research. These AI technologies primarily aim to develop
systems that can autonomously conduct research processes across a wide range of
scientific disciplines. Despite these significant strides, a comprehensive
survey on AI for Research (AI4Research) remains absent, which hampers our
understanding and impedes further development in this field. To address this
gap, we present a comprehensive survey and offer a unified perspective on
AI4Research. Specifically, the main contributions of our work are as follows:
(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify
five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key
research gaps and highlight promising future directions, focusing on the rigor
and scalability of automated experiments, as well as the societal impact. (3)
Abundant applications and resources: Finally, we compile a wealth of resources,
including relevant multidisciplinary applications, data corpora, and tools. We
hope our work will provide the research community with quick access to these
resources and stimulate innovative breakthroughs in AI4Research.

</details>


### [38] [Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models](https://arxiv.org/abs/2507.01915)
*Chengao Li,Hanyu Zhang,Yunkun Xu,Hongyan Xue,Xiang Ao,Qing He*

Key words: RLHF, LLMs, multi-objective optimization, GAPO, Pareto optimality

TL;DR: 论文提出了一种名为GAPO的新方法，通过多梯度下降优化多目标冲突问题，以更好地对齐大型语言模型（LLMs）与人类偏好。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为了解决LLMs与多样化且可能冲突的人类偏好对齐的挑战，需要一种更有效的方法。

Method: 提出了Gradient-Adaptive Policy Optimization (GAPO)，采用多梯度下降动态调整目标梯度，以及结合用户偏好的P-GAPO。

Result: 理论证明GAPO能收敛至帕累托最优解，实证显示其在Mistral-7B上优于现有方法。

Conclusion: GAPO为LLMs与多样人类偏好的对齐提供了一种高效解决方案。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful
technique for aligning large language models (LLMs) with human preferences.
However, effectively aligning LLMs with diverse human preferences remains a
significant challenge, particularly when they are conflict. To address this
issue, we frame human value alignment as a multi-objective optimization
problem, aiming to maximize a set of potentially conflicting objectives. We
introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning
paradigm that employs multiple-gradient descent to align LLMs with diverse
preference distributions. GAPO adaptively rescales the gradients for each
objective to determine an update direction that optimally balances the
trade-offs between objectives. Additionally, we introduce P-GAPO, which
incorporates user preferences across different objectives and achieves Pareto
solutions that better align with the user's specific needs. Our theoretical
analysis demonstrates that GAPO converges towards a Pareto optimal solution for
multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms
current state-of-the-art methods, achieving superior performance in both
helpfulness and harmlessness.

</details>


### [39] [NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks](https://arxiv.org/abs/2507.01921)
*Yang Li,Youssef Emad,Karthik Padthe,Jack Lanchantin,Weizhe Yuan,Thao Nguyen,Jason Weston,Shang-Wen Li,Dong Wang,Ilia Kulikov,Xian Li*

Key words: 推理蒸馏, 教师模型, 学生模型, NaturalThoughts, 样本效率

TL;DR: 通过系统分析教师模型的推理示范对学生模型的影响，研究发现高质量且多样化的推理示例（NaturalThoughts）在提升学生模型推理能力方面优于随机采样方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究教师模型的推理示范对学生模型推理能力提升的影响，旨在找出最有效的示范类型。

Method: 从教师模型中筛选高质量推理轨迹（NaturalThoughts），并分析数据规模、示例难度等因素对蒸馏效果的影响。

Result: 使用NaturalThoughts训练的模型在多个推理基准测试中表现优于现有数据集，尤其是在需要多样化推理策略的困难示例上。

Conclusion: 高质量且多样化的推理示例是提升学生模型推理能力的关键。

Abstract: Recent work has shown that distilling reasoning traces from a larger teacher
model via supervised finetuning outperforms reinforcement learning with the
smaller student model alone (Guo et al. 2025). However, there has not been a
systematic study of what kind of reasoning demonstrations from the teacher are
most effective in improving the student model's reasoning capabilities. In this
work we curate high-quality "NaturalThoughts" by selecting reasoning traces
from a strong teacher model based on a large pool of questions from
NaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of
factors that affect distilling reasoning capabilities, in terms of sample
efficiency and scalability for general reasoning tasks. We observe that simply
scaling up data size with random sampling is a strong baseline with steady
performance gains. Further, we find that selecting difficult examples that
require more diverse reasoning strategies is more sample-efficient to transfer
the teacher model's reasoning skills. Evaluated on both Llama and Qwen models,
training with NaturalThoughts outperforms existing reasoning datasets such as
OpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including
GPQA-Diamond, MMLU-Pro and SuperGPQA.

</details>


### [40] [Decision-oriented Text Evaluation](https://arxiv.org/abs/2507.01923)
*Yu-Shiang Huang,Chuan-Ju Wang,Chung-Chi Chen*

Key words: 自然语言生成、决策评估、大语言模型、协同决策、市场摘要

TL;DR: 论文提出了一种基于决策效用的自然语言生成评估框架，通过测量生成文本对人类和大模型决策结果的影响，揭示了传统内在评估指标的局限性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 在高风险领域部署自然语言生成技术时，传统评估方法（如n-gram重叠或句子合理性）与决策效果的相关性较弱，需要更直接的评估方式。

Method: 采用市场摘要文本（包括客观早报和主观收盘分析）作为测试案例，通过人类投资者和自主大模型代理的决策表现来评估生成文本的质量。

Result: 实验发现，仅依赖摘要时，人类和大模型代理的决策表现均未显著优于随机基准；但结合分析性评论后，人类与大模型协作的决策表现显著提升。

Conclusion: 研究强调应评估生成文本在促进人类与大模型协同决策方面的效果，同时揭示传统内在评估指标的不足。

Abstract: Natural language generation (NLG) is increasingly deployed in high-stakes
domains, yet common intrinsic evaluation methods, such as n-gram overlap or
sentence plausibility, weakly correlate with actual decision-making efficacy.
We propose a decision-oriented framework for evaluating generated text by
directly measuring its influence on human and large language model (LLM)
decision outcomes. Using market digest texts--including objective morning
summaries and subjective closing-bell analyses--as test cases, we assess
decision quality based on the financial performance of trades executed by human
investors and autonomous LLM agents informed exclusively by these texts. Our
findings reveal that neither humans nor LLM agents consistently surpass random
performance when relying solely on summaries. However, richer analytical
commentaries enable collaborative human-LLM teams to outperform individual
human or agent baselines significantly. Our approach underscores the importance
of evaluating generated text by its ability to facilitate synergistic
decision-making between humans and LLMs, highlighting critical limitations of
traditional intrinsic metrics.

</details>


### [41] [Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla](https://arxiv.org/abs/2507.01931)
*Md Sazzadul Islam Ridoy,Sumi Akter,Md. Aminur Rahman*

Key words: 语音识别,低资源语言,Whisper,Wav2Vec-BERT,孟加拉语

TL;DR: 该研究比较了OpenAI的Whisper和Facebook的Wav2Vec-BERT在低资源语言孟加拉语上的表现，发现Wav2Vec-BERT在所有关键指标上表现更优且计算效率更高。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于低资源语言在语音识别领域的研究不足，本文旨在通过系统优化，评估两种先进ASR模型在孟加拉语上的性能。

Method: 使用Mozilla Common Voice-17和OpenSLR数据集，通过微调学习率、训练轮次和模型检查点选择，对比Whisper和Wav2Vec-BERT的WER、CER、训练时间和计算效率。

Result: Wav2Vec-BERT在WER、CER、训练时间和计算效率上均优于Whisper。

Conclusion: Wav2Vec-BERT更适合低资源语言的语音识别系统开发，提供了更高的性能和效率。

Abstract: In recent years, neural models trained on large multilingual text and speech
datasets have shown great potential for supporting low-resource languages. This
study investigates the performances of two state-of-the-art Automatic Speech
Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's
Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments
using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to
evaluate model performances. Through systematic fine-tuning and hyperparameter
optimization, including learning rate, epochs, and model checkpoint selection,
we have compared the models based on Word Error Rate (WER), Character Error
Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model
outperformed Whisper across all key evaluation metrics, demonstrated superior
performance while requiring fewer computational resources, and offered valuable
insights to develop robust speech recognition systems in low-resource
linguistic settings.

</details>


### [42] [The Thin Line Between Comprehension and Persuasion in LLMs](https://arxiv.org/abs/2507.01936)
*Adrian de Wynter,Tangming Yuan*

Key words: 大型语言模型, 辩论, 对话理解, 语用背景, 论证理论

TL;DR: 本文研究了大型语言模型（LLMs）在辩论中的表现及其对对话结构的理解能力，发现LLMs能进行连贯且有说服力的辩论，但在理解深层对话结构方面存在不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着LLMs在敏感领域（如同行评审和心理健康）的快速应用，需要更深入地评估其对话理解和推理能力。

Method: 通过评估LLMs在辩论中的表现，并测量其与对话结构和语用背景理解能力的关系。

Result: LLMs能进行有说服力的辩论，但对深层结构的理解有限；人们意识到AI参与时会更加批判性。

Conclusion: LLMs作为评估者的缺陷与其对上下文的理解能力不足有关；论证理论中，对话的有效性优先于语用背景的建模。

Abstract: Large language models (LLMs) are excellent at maintaining high-level,
convincing dialogues. They are being fast deployed as chatbots and evaluators
in sensitive areas, such as peer review and mental health applications. This,
along with the disparate accounts on their reasoning capabilities, calls for a
closer examination of LLMs and their comprehension of dialogue. In this work we
begin by evaluating LLMs' ability to maintain a debate--one of the purest yet
most complex forms of human communication. Then we measure how this capability
relates to their understanding of what is being talked about, namely, their
comprehension of dialogical structures and the pragmatic context. We find that
LLMs are capable of maintaining coherent, persuasive debates, often swaying the
beliefs of participants and audiences alike. We also note that awareness or
suspicion of AI involvement encourage people to be more critical of the
arguments made. When polling LLMs on their comprehension of deeper structures
of dialogue, however, they cannot demonstrate said understanding. Our findings
tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand
the context. More broadly, for the field of argumentation theory we posit that,
if an agent can convincingly maintain a dialogue, it is not necessary for it to
know what it is talking about. Hence, the modelling of pragmatic context and
coherence are secondary to effectiveness.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [Few-Shot Inspired Generative Zero-Shot Learning](https://arxiv.org/abs/2507.01026)
*Md Shakil Ahamed Shohag,Q. M. Jonathan Wu,Farhad Pourpanah*

Key words: 生成式零样本学习、少样本学习、动态属性评分、类原型、语义正则化

TL;DR: FSIGenZ是一种受少样本学习启发的生成式零样本学习框架，通过动态调整属性分数和估计类原型减少对大规模特征合成的依赖。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统生成式零样本学习方法需要大量合成数据和计算资源，违背了零样本学习的初衷。FSIGenZ旨在减少这种依赖。

Method: 引入模型特定的属性评分（MSAS）动态调整属性分数，估计类原型，并采用双重目的语义正则化（DPSR）和数据平衡策略。

Result: 在SUN、AwA2和CUB基准测试中表现优异，且使用更少的合成特征。

Conclusion: FSIGenZ通过高效的特征合成和优化策略，提升了零样本学习的效率和性能。

Abstract: Generative zero-shot learning (ZSL) methods typically synthesize visual
features for unseen classes using predefined semantic attributes, followed by
training a fully supervised classification model. While effective, these
methods require substantial computational resources and extensive synthetic
data, thereby relaxing the original ZSL assumptions. In this paper, we propose
FSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on
large-scale feature synthesis. Our key insight is that class-level attributes
exhibit instance-level variability, i.e., some attributes may be absent or
partially visible, yet conventional ZSL methods treat them as uniformly
present. To address this, we introduce Model-Specific Attribute Scoring (MSAS),
which dynamically re-scores class attributes based on model-specific
optimization to approximate instance-level variability without access to unseen
data. We further estimate group-level prototypes as clusters of instances based
on MSAS-adjusted attribute scores, which serve as representative synthetic
features for each unseen class. To mitigate the resulting data imbalance, we
introduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training
a semantic-aware contrastive classifier (SCC) using these prototypes.
Experiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves
competitive performance using far fewer synthetic features.

</details>


### [44] [DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization](https://arxiv.org/abs/2507.01027)
*Zijian Ye,Wei Huang,Yifei Yu,Tianhe Ren,Zhongrui Wang,Xiaojuan Qi*

Key words: 大型语言模型,量化,后训练量化,权重压缩,激活量化

TL;DR: DBellQuant是一种创新的后训练量化框架，通过将单峰权重分布转换为双峰形式，减少了量化误差，实现了1比特权重压缩和6比特激活量化，性能损失极小。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大型语言模型（LLMs）在实际部署中面临计算和内存的高成本问题，量化是潜在解决方案，但量化误差和激活异常值限制了其效果。

Method: DBellQuant采用可学习的双峰转换（LTDB）算法，将权重从单峰转换为双峰分布以减少二值化误差，并应用逆变换平滑激活。

Result: 在Wikitext2数据集上，LLaMA2-13B模型在使用6比特激活量化时，困惑度为14.39，显著优于未激活量化的BiLLM（21.35）。

Conclusion: DBellQuant为LLMs的高效压缩和实际应用提供了新的解决方案，表现出卓越的性能保持能力。

Abstract: Large language models (LLMs) demonstrate remarkable performance but face
substantial computational and memory challenges that limit their practical
deployment. Quantization has emerged as a promising solution; however, its
effectiveness is often limited by quantization errors arising from weight
distributions that are not quantization-friendly and the presence of activation
outliers. To address these challenges, we introduce DBellQuant, an innovative
post-training quantization (PTQ) framework that achieves nearly 1-bit weight
compression and 6-bit activation quantization with minimal performance
degradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB)
algorithm, which transforms single-bell weight distributions into dual-bell
forms to reduce binarization errors and applies inverse transformations to
smooth activations. DBellQuant sets a new state-of-the-art by preserving
superior model performance under aggressive weight and activation quantization.
For example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of
14.39 on LLaMA2-13B with 6-bit activation quantization, significantly
outperforming BiLLM's 21.35 without activation quantization, underscoring its
potential in compressing LLMs for real-world applications.

</details>


### [45] [Dual Perspectives on Non-Contrastive Self-Supervised Learning](https://arxiv.org/abs/2507.01028)
*Jean Ponce,Martial Hebert,Basile Terver*

Key words: 自监督学习、非对比性学习、表示崩溃、优化理论、动力系统

TL;DR: 非对比性自监督学习通过最小化不同数据视图之间的编码预测差异来训练编码器和预测器。论文从优化和动力系统的角度，分析了防止表示崩溃的两种常见方法（停止梯度和指数移动平均）的有效性和稳定性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究非对比性自监督学习中广泛使用的停止梯度和指数移动平均方法，是否真的能避免表示崩溃，并通过理论分析揭示其背后的机制。

Method: 通过优化和动力系统的双重视角，分析停止梯度和指数移动平均方法的数学性质，证明其在防止表示崩溃中的作用。

Result: 在无额外假设下，证明了线性情况下不用这两种方法会导致崩溃；而使用这两种方法的动力系统极限点是渐近稳定的，不会退化到平凡解。

Conclusion: 停止梯度和指数移动平均方法虽然在一般意义上不优化原始目标函数，但能够有效避免表示崩溃，其动力系统具有稳定性。

Abstract: The objective of non-contrastive approaches to self-supervised learning is to
train on pairs of different views of the data an encoder and a predictor that
minimize the mean discrepancy between the code predicted from the embedding of
the first view and the embedding of the second one. In this setting, the stop
gradient and exponential moving average iterative procedures are commonly used
to avoid representation collapse, with excellent performance in downstream
supervised applications. This presentation investigates these procedures from
the dual theoretical viewpoints of optimization and dynamical systems. We first
show that, in general, although they do not optimize the original objective, or
for that matter, any other smooth function, they do avoid collapse. Following
Tian et al. [2021], but without any of the extra assumptions used in their
proofs, we then show using a dynamical system perspective that, in the linear
case, minimizing the original objective function without the use of a stop
gradient or exponential moving average always leads to collapse. Conversely, we
finally show that the limit points of the dynamical systems associated with
these two procedures are, in general, asymptotically stable equilibria, with no
risk of degenerating to trivial solutions.

</details>


### [46] [PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning](https://arxiv.org/abs/2507.01029)
*Junjie Zhou,Yingli Zuo,Shichang Feng,Peng Wan,Qi Zhu,Daoqiang Zhang,Wei Shao*

Key words: 多模态大语言模型,病理视觉推理,PathCoT,专家知识,自评估

TL;DR: PathCoT通过将病理学专家知识整合到多模态大语言模型的推理过程中，并结合自评估机制，解决了现有模型在病理视觉推理任务中的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有MLLMs在病理视觉推理任务中因缺乏领域知识和额外推理步骤的误差导致表现不佳，需改进。

Method: 提出PathCoT方法，结合病理专家知识和自评估步骤，优化CoT推理过程。

Result: 在PathMMU数据集上的实验证明了PathCoT在病理视觉理解和推理中的有效性。

Conclusion: PathCoT通过专家知识和自评估提高了病理视觉推理的准确性。

Abstract: With the development of generative artificial intelligence and instruction
tuning techniques, multimodal large language models (MLLMs) have made
impressive progress on general reasoning tasks. Benefiting from the
chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning
problem step-by-step. However, existing MLLMs still face significant challenges
when applied to pathology visual reasoning tasks: (1) LLMs often underperforms
because they lack domain-specific information, which can lead to model
hallucinations. (2) The additional reasoning steps in CoT may introduce errors,
leading to the divergence of answers. To address these limitations, we propose
PathCoT, a novel zero-shot CoT prompting method which integrates the pathology
expert-knowledge into the reasoning process of MLLMs and incorporates
self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides
the MLLM with prior knowledge to perform as pathology experts, and provides
comprehensive analysis of the image with their domain-specific knowledge. By
incorporating the experts' knowledge, PathCoT can obtain the answers with CoT
reasoning. Furthermore, PathCoT incorporates a self-evaluation step that
assesses both the results generated directly by MLLMs and those derived through
CoT, finally determining the reliable answer. The experimental results on the
PathMMU dataset demonstrate the effectiveness of our method on pathology visual
understanding and reasoning.

</details>


### [47] [Optimizing Flamelet Generated Manifold Models: A Machine Learning Performance Study](https://arxiv.org/abs/2507.01030)
*Reza Lotfi Navaei,Mohammad Safarzadeh,Seyed Mohammad Jafar Sobhani*

Key words: Flamelet Generated Manifold, 机器学习, 燃烧模拟, 甲烷燃料, 超参数调优

TL;DR: 该研究利用机器学习算法（如多层感知机、随机森林等）重建了甲烷燃料燃烧模拟中的层流FGM库，通过超参数调优，最佳模型准确率达99.81%。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决Flamelet Generated Manifold（FGM）在实际应用中内存资源消耗大的问题，通过机器学习优化FGM库的构建。

Method: 使用四种机器学习算法（多层感知机、随机森林、线性回归、支持向量机）重建FGM库，并通过超参数调优优化模型。

Result: 最佳模型为多层感知机，准确率达99.81%，误差率为2.30%。

Conclusion: 机器学习可高效构建FGM库，多层感知机为最优选择，显著提升燃烧模拟的精度。

Abstract: In chemistry tabulations and Flamelet combustion models, the Flamelet
Generated Manifold (FGM) is recognized for its precision and physical
representation. The practical implementation of FGM requires a significant
allocation of memory resources. FGM libraries are developed specifically for a
specific fuel and subsequently utilized for all numerical problems using
machine learning techniques. This research aims to develop libraries of Laminar
FGM utilizing machine learning algorithms for application in combustion
simulations of methane fuel. This study employs four Machine Learning
algorithms to regenerate Flamelet libraries, based on an understanding of data
sources, techniques, and data-driven concepts. 1. Multi-Layer Perceptron; 2.
Random Forest; 3. Linear Regression; 4. Support Vector Machine. Seven libraries
were identified as appropriate for constructing a database for training machine
learning models, giving an error rate of 2.30%. The default architectures of
each method were evaluated to determine the optimal approach, leading to the
selection of the MLP method as the primary choice. The method was enhanced
through hyperparameter tuning to improve accuracy. The quantity of hidden
layers and neurons significantly influences method performance. The optimal
model, comprising four hidden layers with 10, 15, 20, and 25 neurons
respectively, achieved an accuracy of 99.81%.

</details>


### [48] [PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs](https://arxiv.org/abs/2507.01031)
*Fanchen Bu,Kijung Shin*

Key words: 几何学习, 非CUDA硬件, Gaudi HPUs, 移植, PyTorch

TL;DR: 本文介绍了将基于PyTorch的几何学习框架移植到非CUDA硬件（如Gaudi-v2 HPUs）的经验，提供了核心工具和实用教程，降低了研究人员的实验门槛。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着非欧几里得数据建模需求的增长，利用新型硬件（如Gaudi HPUs）进行几何学习需要克服软件适配的挑战。

Method: 通过开发核心工具（如scatter、稀疏索引操作）并提供教程和实例分析，实现了框架在Gaudi-v2 HPUs上的移植。

Result: 成功移植了框架，并通过公开的GitHub仓库分享了工具和教程，支持了非CUDA硬件的使用。

Conclusion: 这项研究为非CUDA硬件上的几何学习提供了基础，促进了进一步的优化和跨平台移植。

Abstract: Geometric learning has emerged as a powerful paradigm for modeling
non-Euclidean data, especially graph-structured ones, with applications
spanning social networks, molecular structures, knowledge graphs, and
recommender systems. While Nvidia's CUDA-enabled graphics processing units
(GPUs) largely dominate the hardware landscape, emerging accelerators such as
Intel's Gaudi Habana Processing Units (HPUs) offer competitive performance and
energy efficiency. However, the usage of such non-CUDA processing units
requires significant engineering effort and novel software adaptations. In this
work, we present our experiences porting PyTorch-based geometric learning
frameworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that
restore essential operations (e.g., scatter, sparse indexing, k-nearest
neighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and
eleven real-world examples with diagnostic analyses of encountered failures and
detailed workarounds. We collect all our experiences into a publicly accessible
GitHub repository. Our contributions lower the barrier for researchers to
experiment with geometric-learning algorithms and models on non-CUDA hardware,
providing a foundation for further optimization and cross-platform portability.

</details>


### [49] [An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks](https://arxiv.org/abs/2507.01032)
*Nan Mu,Hongbo Yang,Chen Zhao*

Key words: 多组学, 不确定性量化, 动态决策, 成本优化, 医学诊断

TL;DR: 提出了一种不确定性感知的多视图动态决策框架，用于多组学数据分类，旨在降低测试成本的同时保持高诊断准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 减少多组学分析的高成本和资源浪费，同时保持高诊断准确性。

Method: 在单组学层面，通过改进神经网络激活函数生成Dirichlet分布参数，利用主观逻辑量化分类结果的可信度和不确定性；在多组学层面，基于Dempster-Shafer理论融合异构模态，动态引入数据直至达到置信阈值。

Result: 在四个多组学基准数据集（ROSMAP, LGG, BRCA, KIPAN）上评估，50%以上的病例仅需单组学数据即可准确分类，减少了冗余测试，同时保持了与全组学模型相当的诊断性能。

Conclusion: 该方法有效降低了成本，同时保持了诊断准确性和生物意义的保留。

Abstract: Background and Objective: High-throughput multi-omics technologies have
proven invaluable for elucidating disease mechanisms and enabling early
diagnosis. However, the high cost of multi-omics profiling imposes a
significant economic burden, with over reliance on full omics data potentially
leading to unnecessary resource consumption. To address these issues, we
propose an uncertainty-aware, multi-view dynamic decision framework for omics
data classification that aims to achieve high diagnostic accuracy while
minimizing testing costs. Methodology: At the single-omics level, we refine the
activation functions of neural networks to generate Dirichlet distribution
parameters, utilizing subjective logic to quantify both the belief masses and
uncertainty mass of classification results. Belief mass reflects the support of
a specific omics modality for a disease class, while the uncertainty parameter
captures limitations in data quality and model discriminability, providing a
more trustworthy basis for decision-making. At the multi omics level, we employ
a fusion strategy based on Dempster-Shafer theory to integrate heterogeneous
modalities, leveraging their complementarity to boost diagnostic accuracy and
robustness. A dynamic decision mechanism is then applied that omics data are
incrementally introduced for each patient until either all data sources are
utilized or the model confidence exceeds a predefined threshold, potentially
before all data sources are utilized. Results and Conclusion: We evaluate our
approach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.
In three datasets, over 50% of cases achieved accurate classification using a
single omics modality, effectively reducing redundant testing. Meanwhile, our
method maintains diagnostic performance comparable to full-omics models and
preserves essential biological insights.

</details>


### [50] [Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya](https://arxiv.org/abs/2507.01034)
*Asma Agaal,Mansour Essgaer,Hend M. Farkash,Zulaiha Ali Othman*

Key words: 电力预测, 时间序列模型, LSTM, 班加西, 电网稳定

TL;DR: 该研究提出了一种数据驱动的方法，利用历史数据预测2025年利比亚班加西的电力负荷、发电量和电力缺口，通过多种时间序列模型比较，发现LSTM模型效果最佳。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于班加西地区电力供应不稳定、基础设施有限，精准的电力预测对电网稳定和能源规划至关重要。

Method: 研究使用了ARIMA、季节性ARIMA、动态回归ARIMA、指数平滑、极端梯度提升和LSTM神经网络等模型，数据经过缺失值填补、异常值平滑和对数变换等处理。

Result: LSTM模型在各项指标上表现最优，尤其是对非平稳和季节性数据的建模能力突出。

Conclusion: 优化的LSTM框架整合了温度和湿度等外部因素，为政策制定者和电网运营商提供了实用工具，有助于在数据匮乏且不稳定的地区实现主动负荷管理和资源规划。

Abstract: Accurate electricity forecasting is crucial for grid stability and energy
planning, especially in Benghazi, Libya, where frequent load shedding,
generation deficits, and infrastructure limitations persist. This study
proposes a data-driven approach to forecast electricity load, generation, and
deficits for 2025 using historical data from 2019 (a year marked by
instability) and 2023 (a more stable year). Multiple time series models were
applied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential
smoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural
networks. The dataset was enhanced through missing value imputation, outlier
smoothing, and log transformation. Performance was assessed using mean squared
error, root mean squared error, mean absolute error, and mean absolute
percentage error. LSTM outperformed all other models, showing strong
capabilities in modeling non-stationary and seasonal patterns. A key
contribution of this work is an optimized LSTM framework that integrates
exogenous factors such as temperature and humidity, offering robust performance
in forecasting multiple electricity indicators. These results provide practical
insights for policymakers and grid operators to enable proactive load
management and resource planning in data-scarce, volatile regions.

</details>


### [51] [Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems](https://arxiv.org/abs/2507.01035)
*Yushang Zhao,Haotian Lyu,Yike Peng,Aijia Sun,Feng Jiang,Xinyue Han*

Key words: 推荐系统, GNN, LLM, FPGA, LoRA, 硬件加速

TL;DR: 该论文提出了一个结合图神经网络（GNN）和大语言模型（LLM）的混合推荐系统，通过优化架构、硬件加速和参数高效调优，显著提升了推理速度和训练效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在线服务的快速发展需要高效且实时的推荐系统（ReS），而传统的GNN和LLM在处理复杂用户-物品交互时存在计算瓶颈。

Method: 采用混合GNN-LLM架构，结合量化、LoRA（低秩适配）、知识蒸馏等优化策略，并使用FPGA和DeepSpeed进行硬件加速。

Result: 最优配置（Hybrid + FPGA + DeepSpeed）在NDCG@10指标上提升13.6%（达到0.75），延迟为40-60ms；LoRA将训练时间减少66%（至3.8小时）。

Conclusion: 软硬件协同设计和参数高效调优使混合模型优于单独使用的GNN或LLM，建议在实时部署中使用FPGA和LoRA。未来工作可探索联邦学习和高级融合架构以实现更好的扩展性和隐私保护。

Abstract: The incessant advent of online services demands high speed and efficient
recommender systems (ReS) that can maintain real-time performance along with
processing very complex user-item interactions. The present study, therefore,
considers computational bottlenecks involved in hybrid Graph Neural Network
(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their
inference latency and training efficiency. An extensive methodology was used:
hybrid GNN-LLM integrated architecture-optimization strategies(quantization,
LoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.
Experimental improvements were significant, with the optimal Hybrid + FPGA +
DeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms
of latency, while LoRA brought down training time by 66% (3.8 hours) in
comparison to the non-optimized baseline. Irrespective of domain, such as
accuracy or efficiency, it can be established that hardware-software co-design
and parameter-efficient tuning permit hybrid models to outperform GNN or LLM
approaches implemented independently. It recommends the use of FPGA as well as
LoRA for real-time deployment. Future work should involve federated learning
along with advanced fusion architectures for better scalability and privacy
preservation. Thus, this research marks the fundamental groundwork concerning
next-generation ReS balancing low-latency response with cutting-edge
personalization.

</details>


### [52] [Learning to Segment for Vehicle Routing Problems](https://arxiv.org/abs/2507.01037)
*Wenbin Ouyang,Sirui Li,Yining Ma,Cathy Wu*

Key words: Vehicle Routing Problems, FSTA, Learning-to-Segment, neural networks, 迭代搜索

TL;DR: 提出了FSTA分解技术和L2Seg神经网络框架，通过智能识别和聚合稳定解段，加速VRP求解器的迭代搜索，最高可提速7倍。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对VRP求解器中大量解的冗余计算问题，旨在通过智能分解技术提升效率。

Method: FSTA分解技术保留了稳定解段，引入L2Seg神经框架及其三种变体来识别稳定与不稳定部分。

Result: 实验显示L2Seg能将求解器速度提升7倍，且NAR与AR协同效果最佳。

Conclusion: L2Seg是一个灵活框架，适用于多种VRP求解器，显著提升了计算效率。

Abstract: Iterative search heuristics are widely recognized as state-of-the-art for
solving Vehicle Routing Problems (VRPs). In this work, we identify and exploit
a critical observation: within these solvers, a large portion of the solution
remains stable, i.e., unchanged across search iterations, causing redundant
computations, especially for large-scale VRPs with long subtours. To address
this, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)
decomposition technique to accelerate iterative solvers. Specifically, FSTA
preserves stable solution segments during the search, aggregates nodes within
each segment into fixed hypernodes, and focuses the search only on unstable
portions. Yet, a key challenge lies in identifying which segments should be
aggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),
a novel neural framework to intelligently differentiate potentially stable and
unstable portions for FSTA decomposition. We present three L2Seg variants:
non-autoregressive (globally comprehensive but locally indiscriminate),
autoregressive (locally refined but globally deficient), and their synergy,
with bespoke training and inference strategies. Empirical results on CVRP and
VRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up
to 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy
achieves best performance by combining their complementary strengths. Notably,
L2Seg is a flexible framework that is compatible with traditional,
learning-based, and hybrid solvers, while supporting a broad class of VRPs.

</details>


### [53] [On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization](https://arxiv.org/abs/2507.01039)
*Kaaustaaub Shankar,Wilhelm Louw,Kelly Cohen*

Key words: 强化学习, PPO, 神经模糊控制器, ANFIS, CartPole

TL;DR: 提出了一种基于PPO的强化学习方法，用于训练神经模糊控制器，结果表明其性能优于DQN方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 改进现有的基于DQN的神经模糊控制器训练方法，采用更稳定的PPO框架。

Method: 使用PPO替代DQN，通过演员-评论家循环训练神经模糊控制器，并在CartPole-v1环境中测试性能。

Result: PPO训练的模糊控制器在CartPole-v1中达到500分，方差更低且收敛更快。

Conclusion: PPO为训练可解释的神经模糊控制器提供了一种有效途径。

Abstract: We propose a reinforcement learning (RL) approach for training neuro-fuzzy
controllers using Proximal Policy Optimization (PPO). Building on prior work
that applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),
our method replaces the off-policy value-based framework with a stable
on-policy actor-critic loop. We evaluate this approach in the CartPole-v1
environment using multiple random seeds and compare its learning performance
against ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained
fuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000
updates, showcasing less variance than prior DQN-based methods during training
and overall faster convergence. These findings suggest that PPO offers a
promising pathway for training explainable neuro-fuzzy controllers in
reinforcement learning tasks.

</details>


### [54] [Fast Clifford Neural Layers](https://arxiv.org/abs/2507.01040)
*Tianxiang Xia,Max Neuwinger,Lin Xiao*

Key words: Clifford Algebra, PDE modeling, neural networks, performance optimization

TL;DR: Clifford Neural Layers通过引入Clifford代数优化PDE建模，在CPU上实现了比标准PyTorch快30%的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 提升2/3D Clifford卷积层和多向量激活层在CPU上的推理效率。

Method: 优化Clifford卷积层和多向量激活层的实现，专注于单核CPU性能。

Result: 在大数据和网络规模下，实现比标准PyTorch快30%的速度。

Conclusion: Clifford Neural Layers在PDE建模中展现出显著的性能优势。

Abstract: Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra
into neural networks. In this project we focus on optimizing the inference of
2/3D Clifford convolutional layers and multivector activation layers for one
core CPU performance.
  Overall, by testing on a real network block involving Clifford convolutional
layers and multivector activation layers, we observe that our implementation is
30% faster than standard PyTorch implementation in relatively large data +
network size (>L2 cache).
  We open source our code base at
https://github.com/egretwAlker/c-opt-clifford-layers

</details>


### [55] [Fast AI Model Splitting over Edge Networks](https://arxiv.org/abs/2507.01041)
*Zuguang Li,Wen Wu,Shaohua Wu,Songge Zhang,Ye Wang,Xuemin,Shen*

Key words: Split learning, 有向无环图, 模型分割, 最大流方法, 计算效率

TL;DR: 本文提出了一种基于有向无环图（DAG）的快速模型分割算法，以解决AI模型分割问题，并通过最大流方法找到最优分割。实验显示该算法能显著降低训练延迟。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Split learning（SL）虽然能降低设备端的计算负荷，但复杂AI模型的最优分割问题计算复杂度高，亟需高效解决方案。

Method: 将AI模型表示为有向无环图（DAG），并将最优分割问题转化为最小s-t割搜索问题，提出快速DAG分割算法和针对块结构模型的块级分割算法。

Result: 算法可在毫秒内确定最优分割，动态边缘网络中的训练延迟相比现有基准降低了24.62%-38.95%。

Conclusion: 所提算法在计算效率和性能优化方面表现突出，为AI模型分割提供了高效方案。

Abstract: Split learning (SL) has emerged as a computationally efficient approach for
artificial intelligence (AI) model training, which can alleviate device-side
computational workloads. However, complex AI model architectures pose high
computational complexity to obtain the optimal model splitting. In this paper,
we represent an arbitrary AI model as a directed acyclic graph (DAG), and then
reformulate the optimal model splitting problem as a minimum s-t cut search
problem. To solve the problem, we propose a fast DAG-based model splitting
algorithm, which restructures the DAG to enable the optimal model splitting
identification via a maximum flow method. Theoretical analysis indicates that
the proposed algorithm is optimal. Furthermore, considering AI models with
block structures, we propose a block-wise model splitting algorithm to reduce
computational complexity. The algorithm abstracts each block, i.e., a component
consisting of multiple layers, into a single vertex, thereby obtaining the
optimal model splitting via a simplified DAG. Extensive experimental results
demonstrate that the proposed algorithms can determine the optimal model
splitting within milliseconds, as well as reduce training delay by
24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art
benchmarks.

</details>


### [56] [Data Classification with Dynamically Growing and Shrinking Neural Networks](https://arxiv.org/abs/2507.01043)
*Szymon Świderski,Agnieszka Jastrzębska*

Key words: 神经网络,蒙特卡洛树搜索,动态架构,时间序列分类

TL;DR: 提出了使用蒙特卡洛树搜索动态调整神经网络架构的新方法，适用于视觉和时间序列分类任务。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决固定架构神经网络在训练中无法优化结构的问题。

Method: 利用蒙特卡洛树搜索动态调整网络架构，支持独立修改每个时间序列。

Result: 在视觉和时间序列分类任务中表现出色，特别适合多变量时间序列分类。

Conclusion: 该方法具有强大的动态适应性和鲁棒性，能有效优化网络结构。

Abstract: The issue of data-driven neural network model construction is one of the core
problems in the domain of Artificial Intelligence. A standard approach assumes
a fixed architecture with trainable weights. A conceptually more advanced
assumption is that we not only train the weights, but also find out the optimal
model architecture. We present a new method that realizes just that. This
article is an extended version of our conference paper titled "Dynamic Growing
and Shrinking of Neural Networks with Monte Carlo Tree Search [26]". In the
paper, we show in detail how to create a neural network with a procedure that
allows dynamic shrinking and growing of the model while it is being trained.
The decision-making mechanism for the architectural design is governed by a
Monte Carlo tree search procedure which simulates network behavior and allows
to compare several candidate architecture changes to choose the best one. The
proposed method was validated using both visual and time series datasets,
demonstrating its particular effectiveness in multivariate time series
classification. This is attributed to the architecture's ability to adapt
dynamically, allowing independent modifications for each time series. The
approach is supplemented by Python source code for reproducibility.
Experimental evaluations in visual pattern and multivariate time series
classification tasks revealed highly promising performance, underscoring the
method's robustness and adaptability.

</details>


### [57] [Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals](https://arxiv.org/abs/2507.01045)
*Xiao Gu,Wei Tang,Jinpei Han,Veer Sangha,Fenglin Liu,Shreyank N Gowda,Antonio H. Ribeiro,Patrick Schwab,Kim Branson,Lei Clifton,Antonio Luiz P. Ribeiro,Zhangdaihong Liu,David A. Clifton*

Key words: 心脏感知、基础模型、Transformer、多模态数据、泛化能力

TL;DR: 提出了一种基于Transformer架构和生成式预训练策略的心脏感知基础模型（CSFM），能够从多模态健康数据中学习统一表示，展现出卓越的跨任务和跨模态性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的深度学习方法依赖单一数据集和静态模型，限制了其在多样化临床环境和采集协议中的鲁棒性和泛化能力。因此，需要一种更灵活、通用的心脏信号分析方法。

Method: 使用先进的Transformer架构和生成式掩码预训练策略，从多模态数据（包括MIMIC-III-WDB、MIMIC-IV-ECG和CODE数据集）中学习统一表示。

Result: CSFM在不同心脏感知任务（如诊断、人口统计信息识别、生命体征测量等）中表现优于传统单模态单任务方法，并能灵活适应不同的输入配置和传感器模态。

Conclusion: CSFM作为一种通用且可扩展的解决方案，有望推动全面的心脏监测技术的发展。

Abstract: Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms
(PPG), are of paramount importance for the diagnosis, prevention, and
management of cardiovascular diseases, and have been extensively used in a
variety of clinical tasks. Conventional deep learning approaches for analyzing
these signals typically rely on homogeneous datasets and static bespoke models,
limiting their robustness and generalizability across diverse clinical settings
and acquisition protocols. In this study, we present a cardiac sensing
foundation model (CSFM) that leverages advanced transformer architectures and a
generative, masked pretraining strategy to learn unified representations from
vast, heterogeneous health records. Our model is pretrained on an innovative
multi-modal integration of data from multiple large-scale datasets (including
MIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the
corresponding clinical or machine-generated text reports from approximately 1.7
million individuals. We demonstrate that the embeddings derived from our CSFM
not only serve as effective feature extractors across diverse cardiac sensing
scenarios, but also enable seamless transfer learning across varying input
configurations and sensor modalities. Extensive evaluations across diagnostic
tasks, demographic information recognition, vital sign measurement, clinical
outcome prediction, and ECG question answering reveal that CSFM consistently
outperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits
robust performance across multiple ECG lead configurations from standard
12-lead systems to single-lead setups, and in scenarios where only ECG, only
PPG, or a combination thereof is available. These findings highlight the
potential of CSFM as a versatile and scalable solution, for comprehensive
cardiac monitoring.

</details>


### [58] [Variational Digital Twins](https://arxiv.org/abs/2507.01047)
*Logan A. Burnett,Umme Mahbuba Nabila,Majdi I. Radaideh*

Key words: 数字孪生，变分数字孪生，贝叶斯方法，能源系统

TL;DR: 提出了一种轻量级的变分数字孪生（VDT）框架，通过贝叶斯输出层和改进的更新算法，实现实时更新和不确定性校准，应用于能源领域4个问题，效果显著。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决当前数字孪生在实时实现、模型不确定性和信息交换框架方面的不足。

Method: 在标准神经网络架构中加入贝叶斯输出层，结合新型VDT更新算法。

Result: 在4个能源问题上验证，包括关键热通量预测、可再生能源发电预测、核反应堆瞬态冷却和锂离子电池模型，性能显著优于传统方法。

Conclusion: 轻量级的贝叶斯增强和高效更新方案可将传统模型转化为不确定性感知、数据高效且计算可行的数字孪生。

Abstract: While digital twins (DT) hold promise for providing real-time insights into
complex energy assets, much of the current literature either does not offer a
clear framework for information exchange between the model and the asset, lacks
key features needed for real-time implementation, or gives limited attention to
model uncertainty. Here, we aim to solve these gaps by proposing a variational
digital twin (VDT) framework that augments standard neural architectures with a
single Bayesian output layer. This lightweight addition, along with a novel VDT
updating algorithm, lets a twin update in seconds on commodity GPUs while
producing calibrated uncertainty bounds that can inform experiment design,
control algorithms, and model reliability. The VDT is evaluated on four
energy-sector problems. For critical-heat-flux prediction, uncertainty-driven
active learning reaches R2 = 0.98 using 47 % fewer experiments and one-third
the training time of random sampling. A three-year renewable-generation twin
maintains R2 > 0.95 for solar output and curbs error growth for volatile wind
forecasts via monthly updates that process only one month of data at a time. A
nuclear reactor transient cooldown twin reconstructs thermocouple signals with
R2 > 0.99 and preserves accuracy after 50 % sensor loss, demonstrating
robustness to degraded instrumentation. Finally, a physics-informed Li-ion
battery twin, retrained after every ten discharges, lowers voltage mean-squared
error by an order of magnitude relative to the best static model while adapting
its credible intervals as the cell approaches end-of-life. These results
demonstrate that combining modest Bayesian augmentation with efficient update
schemes turns conventional surrogates into uncertainty-aware, data-efficient,
and computationally tractable DTs, paving the way for dependable models across
industrial and scientific energy systems.

</details>


### [59] [3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells](https://arxiv.org/abs/2507.01048)
*Ricardo Emanuel Vaz Vargas,Afrânio José de Melo Junior,Celso José Munaro,Cláudio Benevenuto de Campos Lima,Eduardo Toledo de Lima Junior,Felipe Muntzberg Barrocas,Flávio Miguel Varejão,Guilherme Fidelis Peixer,Igor de Melo Nery Oliveira,Jader Riso Barbosa Jr.,Jaime Andrés Lozano Cadena,Jean Carlos Dias de Araújo,João Neuenschwander Escosteguy Carneiro,Lucas Gouveia Omena Lopes,Lucas Pereira de Gouveia,Mateus de Araujo Fernandes,Matheus Lima Scramignon,Patrick Marques Ciarelli,Rodrigo Castello Branco,Rogério Leite Alves Pinto*

Key words: 石油行业,多变量时间序列,3W数据集,早期检测,人工智能,机器学习

TL;DR: 论文摘要介绍了3W数据集，这是一个用于石油行业不良事件早期检测的公开多变量时间序列数据集，由Petrobras开发并持续改进，旨在支持研究和开发。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 石油行业的不良事件可能导致经济损失、环境事故和人员伤亡。早期检测的AI和ML解决方案需要公开数据集支持，因此Petrobras开发了3W数据集。

Method: 3W数据集是多变量时间序列，由专家标记并公开共享。最新版本包含结构修改和额外标记数据。

Result: 3W数据集已成为该领域的基准，支持多项研究，并鼓励社区改进现有结果和开发新方法。

Conclusion: 3W数据集的详细描述有助于推动研究和开发，为石油行业不良事件的早期检测提供支持。

Abstract: In the oil industry, undesirable events in oil wells can cause economic
losses, environmental accidents, and human casualties. Solutions based on
Artificial Intelligence and Machine Learning for Early Detection of such events
have proven valuable for diverse applications across industries. In 2019,
recognizing the importance and the lack of public datasets related to
undesirable events in oil wells, Petrobras developed and publicly released the
first version of the 3W Dataset, which is essentially a set of Multivariate
Time Series labeled by experts. Since then, the 3W Dataset has been developed
collaboratively and has become a foundational reference for numerous works in
the field. This data article describes the current publicly available version
of the 3W Dataset, which contains structural modifications and additional
labeled data. The detailed description provided encourages and supports the 3W
community and new 3W users to improve previous published results and to develop
new robust methodologies, digital products and services capable of detecting
undesirable events in oil wells with enough anticipation to enable corrective
or mitigating actions.

</details>


### [60] [Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization](https://arxiv.org/abs/2507.01050)
*Jing Yu,Yibo Zhao,Jiapeng Zhu,Wenming Shao,Bo Pang,Zhao Zhang,Xiang Li*

Key words: 社交媒体去毒,语义保留,两阶段训练,数据效率,模型泛化

TL;DR: 提出一种两阶段训练框架，结合数据效率、语义保留和模型泛化，解决社交媒体有毒内容的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 社交媒体有毒内容对网络环境和公共讨论构成威胁，需高效去毒方法且保留原语义。现有方法在去毒性、语义保留和泛化能力上表现不足，依赖昂贵标注数据且效率低。

Method: 采用两阶段训练框架：先在高质量过滤的并行数据上进行监督微调；再利用无标签有毒数据和定制奖励模型，通过Group Relative Policy Optimization训练LLM。

Result: 实验表明，该方法有效平衡去毒性和语义保留，提升泛化能力，减少对标注数据的依赖，达到领先性能。

Conclusion: 提出的两阶段框架解决了现有方法的局限性，实现了高效去毒并保持语义，具有实际应用价值。

Abstract: The widespread dissemination of toxic content on social media poses a serious
threat to both online environments and public discourse, highlighting the
urgent need for detoxification methods that effectively remove toxicity while
preserving the original semantics. However, existing approaches often struggle
to simultaneously achieve strong detoxification performance, semantic
preservation, and robustness to out-of-distribution data. Moreover, they
typically rely on costly, manually annotated parallel corpora while showing
poor data efficiency. To address these challenges, we propose a two-stage
training framework that jointly optimizes for data efficiency, semantic
preservation, and model generalization. We first perform supervised fine-tuning
on a small set of high-quality, filtered parallel data to establish a strong
initialization. Then, we leverage unlabeled toxic inputs and a custom-designed
reward model to train the LLM using Group Relative Policy Optimization.
Experimental results demonstrate that our method effectively mitigates the
trade-offs faced by previous work, achieving state-of-the-art performance with
improved generalization and significantly reduced dependence on annotated data.
Our code is available at:
https://anonymous.4open.science/r/Detoxification-of-Text-725F/

</details>


### [61] [Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals](https://arxiv.org/abs/2507.01052)
*Ahmed Farooq*

Key words: 长序列记忆, 密集霍普菲尔德网络, 时间核, 变压器架构, 时间序列数据

TL;DR: 该研究提出了一种新的能量函数，用于长序列记忆，通过在密集霍普菲尔德网络中引入高阶相互作用和时间核，实现了高效的序列模式检索。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决变压器在长上下文任务中的局限性，研究团队提出了一种新的长序列记忆模型，旨在提高长序列建模的效率和处理时间序列数据的能力。

Method: 研究基于密集霍普菲尔德网络框架，引入了一个时间核$K(m, k)$，以捕捉时间依赖性，从而实现对长序列模式的存储和高效检索。

Result: 该方法成功应用于电影帧的存储和序列检索，展示了在高维空间中处理序列数据的能力，并显示出在变压器架构中的应用潜力。

Conclusion: 该模型为解决长上下文任务中的挑战提供了有效方法，尤其在自然语言处理、预测等领域具有广泛的应用前景。

Abstract: In this study we introduce a novel energy functional for long-sequence
memory, building upon the framework of dense Hopfield networks which achieves
exponential storage capacity through higher-order interactions. Building upon
earlier work on long-sequence Hopfield memory models, we propose a temporal
kernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient
sequential retrieval of patterns over extended sequences. We demonstrate the
successful application of this technique for the storage and sequential
retrieval of movies frames which are well suited for this because of the high
dimensional vectors that make up each frame creating enough variation between
even sequential frames in the high dimensional space. The technique has
applications in modern transformer architectures, including efficient
long-sequence modeling, memory augmentation, improved attention with temporal
bias, and enhanced handling of long-term dependencies in time-series data. Our
model offers a promising approach to address the limitations of transformers in
long-context tasks, with potential implications for natural language
processing, forecasting, and beyond.

</details>


### [62] [XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science](https://arxiv.org/abs/2507.01054)
*Jithendaraa Subramanian,Linda Hung,Daniel Schweigert,Santosh Suram,Weike Ye*

Key words: 材料发现, 多模态学习, X射线衍射, 自监督预训练

TL;DR: 提出了一个基于元素组成和XRD的多模态框架，无需晶体结构输入，通过自监督预训练策略（MXM和对比对齐）提高了模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前基于晶体结构的模型在实际应用中不实用，因为原子结构通常未知或难以获取。

Method: 提出了一种多模态框架，整合了模态特定编码器和交叉注意力融合模块，并采用MXM和对比对齐作为自监督预训练策略。

Result: 预训练显著提高了收敛速度（最高达4.2倍）和准确性，且多模态性能在大数据集上表现更优。

Conclusion: 该研究为材料科学中无需结构的、基于实验的基础模型提供了可行路径。

Abstract: Recent advances in materials discovery have been driven by structure-based
models, particularly those using crystal graphs. While effective for
computational datasets, these models are impractical for real-world
applications where atomic structures are often unknown or difficult to obtain.
We propose a scalable multimodal framework that learns directly from elemental
composition and X-ray diffraction (XRD) -- two of the more available modalities
in experimental workflows without requiring crystal structure input. Our
architecture integrates modality-specific encoders with a cross-attention
fusion module and is trained on the 5-million-sample Alexandria dataset. We
present masked XRD modeling (MXM), and apply MXM and contrastive alignment as
self-supervised pretraining strategies. Pretraining yields faster convergence
(up to 4.2x speedup) and improves both accuracy and representation quality. We
further demonstrate that multimodal performance scales more favorably with
dataset size than unimodal baselines, with gains compounding at larger data
regimes. Our results establish a path toward structure-free, experimentally
grounded foundation models for materials science.

</details>


### [63] [Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI](https://arxiv.org/abs/2507.01056)
*Lidan Peng,Lu Gao,Feng Hong,Jingran Sun*

Key words: 洪水, 路面退化, 国际粗糙度指数, XAI, SHAP, LIME

TL;DR: 该研究探讨了洪水如何加速路面退化，通过国际粗糙度指数（IRI）测量，结合20年路面数据和洪水事件数据，发现受洪水影响的路面粗糙度增加更快。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 洪水对路面基础设施造成严重破坏，研究旨在量化洪水对路面退化的影响，为提升路面抗洪能力提供依据。

Method: 整合20年路面状况数据与洪水事件数据，进行统计分析；应用SHAP和LIME等XAI技术评估洪水对路面性能的影响。

Result: 受洪水影响的路面粗糙度增加速度显著快于未受洪水影响的路面。

Conclusion: 需要采取主动的防洪策略（如改进排水系统、使用抗洪材料）以增强易发洪水区域路面的韧性。

Abstract: Flooding can damage pavement infrastructure significantly, causing both
immediate and long-term structural and functional issues. This research
investigates how flooding events affect pavement deterioration, specifically
focusing on measuring pavement roughness by the International Roughness Index
(IRI). To quantify these effects, we utilized 20 years of pavement condition
data from TxDOT's PMIS database, which is integrated with flood event data,
including duration and spatial extent. Statistical analyses were performed to
compare IRI values before and after flooding and to calculate the deterioration
rates influenced by flood exposure. Moreover, we applied Explainable Artificial
Intelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and
Local Interpretable Model-Agnostic Explanations (LIME), to assess the impact of
flooding on pavement performance. The results demonstrate that flood-affected
pavements experience a more rapid increase in roughness compared to non-flooded
sections. These findings emphasize the need for proactive flood mitigation
strategies, including improved drainage systems, flood-resistant materials, and
preventative maintenance, to enhance pavement resilience in vulnerable regions.

</details>


### [64] [Loop2Net: Data-Driven Generation and Optimization of Airfoil CFD Meshes from Sparse Boundary Coordinates](https://arxiv.org/abs/2507.01057)
*Lushun Fan,Yuqin Xia,Jun Li,Karl Jenkins*

Key words: 智能优化系统、深度卷积神经网络、网格生成、Loop2Net、损失函数

TL;DR: 提出了一种基于深度卷积神经网络架构的创新智能优化系统，用于网格生成与优化，核心是Loop2Net生成器和损失函数，通过添加惩罚实现目标。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 通过智能优化系统改进网格生成的精度和效率。

Method: 使用深度卷积神经网络架构和Loop2Net生成器，结合两个关键损失函数进行训练优化。

Result: 系统能够根据给定的翼坐标预测网格，并通过惩罚机制实现优化目标。

Conclusion: 该方法有效地提升了网格生成的质量和效率。

Abstract: In this study, an innovative intelligent optimization system for mesh quality
is proposed, which is based on a deep convolutional neural network
architecture, to achieve mesh generation and optimization. The core of the
study is the Loop2Net generator and loss function, it predicts the mesh based
on the given wing coordinates. And the model's performance is continuously
optimised by two key loss functions during the training. Then discipline by
adding penalties, the goal of mesh generation was finally reached.

</details>


### [65] [Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services](https://arxiv.org/abs/2507.01067)
*Keun Soo Yim*

Key words: 时间序列预测, 基础模型, 罕见事件, 生产中断, 随机模型

TL;DR: 论文针对罕见且尖锐的事件（如生产中断）优化了一个最先进的基础模型，并与经典随机预测模型进行了比较，结果显示基础模型在特定场景下优于传统方法，预测误差低于6%。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究旨在解决罕见且尖锐事件的预测问题，这是极端事件中的边缘情况，现有基础模型尚未涉足。

Method: 优化了一个最先进的基础模型，并与经典随机预测模型（如移动平均和自回归模型）进行对比分析。

Result: 基础模型在预测罕见事件时表现优于传统方法，能准确识别关键数据模式，预测误差低于6%。

Conclusion: 基础模型在预测罕见且尖锐事件时具有优势，优于传统随机预测模型。

Abstract: Time series forecasting models have diverse real world applications (e.g.,
from electricity metrics to software workload). Latest foundational models
trained for time series forecasting show strengths (e.g., for long sequences
and in zero-shot settings). However, foundational model was not yet used for
forecasting rare, spiky events, i.e., a challenging target because those are a
corner case of extreme events. In this paper, we optimize a state-of-the-art
foundational model to forecast sporadic or spiky production outages of
high-performance machine learning services powering billions of client devices.
We evaluate the forecasting errors of the foundational model compared with
classical stochastic forecasting models (e.g., moving average and
autoregressive). The analysis helps us understand how each of the evaluated
models performs for the sporadic or spiky events. For example, it identifies
the key patterns in the target data that are well tracked by the foundational
model vs. each of the stochastic models. We use the models with optimal
parameters to estimate a year-long outage statistics of a particular root cause
with less than 6% value errors.

</details>


### [66] [Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors](https://arxiv.org/abs/2507.01068)
*Biplov Paneru*

Key words: 冻结步态, 帕金森病, 可解释AI, 联邦学习, Stacking Ensemble

TL;DR: 利用IMU数据和可解释AI方法预测帕金森病的冻结步态（FOG），集成模型表现最佳，准确率达99%，特征分析显示时间是关键因素，并采用联邦学习提升预测能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 开发早期检测和预测帕金森病冻结步态（FOG）的方法，以改善患者的生活质量。

Method: 使用CatBoost、XGBoost和Extra Trees分类器，结合Stacking Ensemble模型；SHAP分析特征重要性；引入联邦学习和混合Conv1D + LSTM架构。

Result: 集成模型分类准确率达99%，时间是最具影响力的特征。

Conclusion: 提出的框架在FOG预测中表现出色，并为个性化医疗和隐私保护提供了新思路。

Abstract: This study leverages an Inertial Measurement Unit (IMU) dataset to develop
explainable AI methods for the early detection and prediction of Freezing of
Gait (FOG), a common symptom in Parkinson's disease. Machine learning models,
including CatBoost, XGBoost, and Extra Trees classifiers, are employed to
accurately categorize FOG episodes based on relevant clinical features. A
Stacking Ensemble model achieves superior performance, surpassing a hybrid
bidirectional GRU model and reaching nearly 99% classification accuracy. SHAP
interpretability analysis reveals that time (seconds) is the most influential
factor in distinguishing gait patterns. Additionally, the proposed FOG
prediction framework incorporates federated learning, where models are trained
locally on individual devices and aggregated on a central server using a
federated averaging approach, utilizing a hybrid Conv1D + LSTM architecture for
enhanced predictive capability.

</details>


### [67] [Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs](https://arxiv.org/abs/2507.01073)
*Dian Jin*

Key words: 图神经网络, 3D分子结构, 旋转不变性, 药物发现, 材料设计

TL;DR: 本文提出了一种新颖的3D编码模块，通过旋转采样和高维SO(3)旋转群计算，实现了近似旋转不变性，并通过后对齐策略严格保证不变性，显著提升了分子属性预测的准确性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统图神经网络难以有效编码分子的3D空间结构，旋转不变性和等变方法各有局限，前者依赖先验知识，后者计算成本高。本文旨在解决这一问题。

Method: 提出基于旋转采样的3D编码模块，通过SO(3)旋转群计算期望实现近似旋转不变性，结合后对齐策略严格保证不变性。

Result: 在QM9和C10数据集上的实验表明，该方法在预测准确性、鲁棒性和泛化能力上优于现有方法，且计算复杂度低、解释性强。

Conclusion: 该方法为药物发现和材料设计中的3D分子信息处理提供了高效且有效的解决方案。

Abstract: Graph neural networks (GNNs) have achieved remarkable success in molecular
property prediction. However, traditional graph representations struggle to
effectively encode the inherent 3D spatial structures of molecules, as
molecular orientations in 3D space introduce significant variability, severely
limiting model generalization and robustness. Existing approaches primarily
focus on rotation-invariant and rotation-equivariant methods. Invariant methods
often rely heavily on prior knowledge and lack sufficient generalizability,
while equivariant methods suffer from high computational costs. To address
these limitations, this paper proposes a novel plug-and-play 3D encoding module
leveraging rotational sampling. By computing the expectation over the SO(3)
rotational group, the method naturally achieves approximate rotational
invariance. Furthermore, by introducing a carefully designed post-alignment
strategy, strict invariance can be achieved without compromising performance.
Experimental evaluations on the QM9 and C10 Datasets demonstrate superior
predictive accuracy, robustness, and generalization performance compared to
existing methods. Moreover, the proposed approach maintains low computational
complexity and enhanced interpretability, providing a promising direction for
efficient and effective handling of 3D molecular information in drug discovery
and material design.

</details>


### [68] [Provenance Tracking in Large-Scale Machine Learning Systems](https://arxiv.org/abs/2507.01075)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Key words: 大规模AI模型,计算效率,溯缘数据,yProv4ML,W3C PROV

TL;DR: 论文介绍了一个名为yProv4ML的库，用于在大规模AI模型训练中收集与管理溯源数据，以优化计算效率、执行时间、准确性和能耗。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着大规模AI模型需求的增长，如何在训练中平衡计算效率、执行时间、准确性和能耗成为一个关键挑战。溯缘数据在这一过程中至关重要。

Method: 开发了yProv4ML库，支持W3C PROV和ProvML标准，通过插件扩展数据收集工具，并与yProv框架集成。

Result: 提供了一个灵活、可扩展的工具，用于收集和管理AI模型训练中的溯源数据，支持高效的多维资源优化。

Conclusion: yProv4ML库为AI模型的分布式资源优化和能源效率提升提供了有效工具。

Abstract: As the demand for large scale AI models continues to grow, the optimization
of their training to balance computational efficiency, execution time, accuracy
and energy consumption represents a critical multidimensional challenge.
Achieving this balance requires not only innovative algorithmic techniques and
hardware architectures but also comprehensive tools for monitoring, analyzing,
and understanding the underlying processes involved in model training and
deployment. Provenance data information about the origins, context, and
transformations of data and processes has become a key component in this
pursuit. By leveraging provenance, researchers and engineers can gain insights
into resource usage patterns, identify inefficiencies, and ensure
reproducibility and accountability in AI development workflows. For this
reason, the question of how distributed resources can be optimally utilized to
scale large AI models in an energy efficient manner is a fundamental one. To
support this effort, we introduce the yProv4ML library, a tool designed to
collect provenance data in JSON format, compliant with the W3C PROV and ProvML
standards. yProv4ML focuses on flexibility and extensibility, and enables users
to integrate additional data collection tools via plugins. The library is fully
integrated with the yProv framework, allowing for higher level pairing in tasks
run also through workflow management systems.

</details>


### [69] [Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels](https://arxiv.org/abs/2507.01077)
*Bogdan Bogdan,Arina Cazacu,Laura Vasilie*

Key words: 异常检测, ECU通信, 解码器LLM, 熵正则化, 地面数据不一致

TL;DR: 提出一种基于解码器的大型语言模型(LLM)，用于检测ECU通信日志中的异常，解决了专业领域缺乏定制化LLM和地面数据不一致的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在汽车通信系统等专业领域，传统的异常检测方法效果有限，需要一种更高效且适应性强的方法。

Method: 利用解码器LLM分析UDP通信日志，通过熵正则化技术处理不一致的地面数据，并以时间偏差作为异常检测标准。

Result: 该模型能够通过少量示例学习，提高复杂通信环境中的检测准确性。

Conclusion: 提出的方法为ECU通信提供了一种新颖、可扩展的异常检测方案。

Abstract: Anomaly detection often relies on supervised or clustering approaches, with
limited success in specialized domains like automotive communication systems
where scalable solutions are essential. We propose a novel decoder-only Large
Language Model (LLM) to detect anomalies in Electronic Control Unit (ECU)
communication logs. Our approach addresses two key challenges: the lack of LLMs
tailored for ECU communication and the complexity of inconsistent ground truth
data. By learning from UDP communication logs, we formulate anomaly detection
simply as identifying deviations in time from normal behavior. We introduce an
entropy regularization technique that increases model's uncertainty in known
anomalies while maintaining consistency in similar scenarios. Our solution
offers three novelties: a decoder-only anomaly detection architecture, a way to
handle inconsistent labeling, and an adaptable LLM for different ECU
communication use cases. By leveraging the generative capabilities of
decoder-only models, we present a new technique that addresses the high cost
and error-prone nature of manual labeling through a more scalable system that
is able to learn from a minimal set of examples, while improving detection
accuracy in complex communication environments.

</details>


### [70] [yProv4ML: Effortless Provenance Tracking for Machine Learning Systems](https://arxiv.org/abs/2507.01078)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Key words: 大型语言模型,溯源信息,PROV-JSON,机器学习框架

TL;DR: 论文提出了yProv4ML框架，旨在以最小代码修改捕获机器学习过程中的溯源信息。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大型语言模型的快速发展凸显了开发过程中透明度和严谨性的不足，尤其是超参数的不确定性导致模型选择困难。

Method: 使用名为yProv4ML的框架，以PROV-JSON格式记录机器学习过程中的溯源信息。

Result: yProv4ML能够高效捕获和管理机器学习过程的溯源数据。

Conclusion: yProv4ML提供了一种轻量级方案，提升了机器学习过程的透明度和可追溯性。

Abstract: The rapid growth of interest in large language models (LLMs) reflects their
potential for flexibility and generalization, and attracted the attention of a
diverse range of researchers. However, the advent of these techniques has also
brought to light the lack of transparency and rigor with which development is
pursued. In particular, the inability to determine the number of epochs and
other hyperparameters in advance presents challenges in identifying the best
model. To address this challenge, machine learning frameworks such as MLFlow
can automate the collection of this type of information. However, these tools
capture data using proprietary formats and pose little attention to lineage.
This paper proposes yProv4ML, a framework to capture provenance information
generated during machine learning processes in PROV-JSON format, with minimal
code modifications.

</details>


### [71] [Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept](https://arxiv.org/abs/2507.01080)
*Edouard Lansiaux,Ramy Azzouz,Emmanuel Chazard,Amélie Vromant,Eric Wiel*

Key words: 急诊分诊, 人工智能, NLP, LLM, JEPA

TL;DR: 该论文对比了三种AI模型在急诊分诊中的表现，发现LLM模型（URGENTIAPARSE）优于其他模型和护士分诊，并探讨了AI在急诊工作流程中的潜力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为解决急诊分诊中的误诊问题（包括低估和高估），研究AI模型在分诊中的表现，以改善患者安全和运营效率。

Method: 回顾性分析7个月内急诊患者数据，训练并验证三种AI模型（NLP、LLM、JEPA），评估其与FRENCH黄金标准的一致性。

Result: LLM模型（URGENTIAPARSE）表现最佳，准确性高于其他模型和护士分诊，且在预测住院需求和处理结构化数据时表现出色。

Conclusion: LLM模型在急诊分诊中最准确，但AI集成到临床流程中需解决模型限制和伦理透明性问题。

Abstract: Triage errors, including undertriage and overtriage, are persistent
challenges in emergency departments (EDs). With increasing patient influx and
staff shortages, the integration of artificial intelligence (AI) into triage
protocols has gained attention. This study compares the performance of three AI
models [Natural Language Processing (NLP), Large Language Models (LLM), and
Joint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes
against the FRENCH scale and clinical practice.We conducted a retrospective
analysis of a prospectively recruited cohort gathering adult patient triage
data over a 7-month period at the Roger Salengro Hospital ED (Lille, France).
Three AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2)
URGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic
details, verbatim chief complaints, vital signs, and triage outcomes based on
the FRENCH scale and GEMSA coding. The primary outcome was the concordance of
AI-predicted triage level with the FRENCH gold-standard. It was assessed thanks
to various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM
model (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared
to JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse
triage (-4.343). Secondary analyses highlighted the effectiveness of
URGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness
with structured data versus raw transcripts (either for GEMSA prediction or for
FRENCH prediction). LLM architecture, through abstraction of patient
representations, offers the most accurate triage predictions among tested
models. Integrating AI into ED workflows could enhance patient safety and
operational efficiency, though integration into clinical workflows requires
addressing model limitations and ensuring ethical transparency.

</details>


### [72] [Proof of a perfect platonic representation hypothesis](https://arxiv.org/abs/2507.01098)
*Liu Ziyin,Isaac Chuang*

Key words: SGD, EDLN, Plato表示假设, 表示学习, 渐进锐化

TL;DR: 本文详细解释了Ziyin等人（2025）关于嵌入式深度线性网络模型（EDLN）中“完美”柏拉图表示假设（PRH）的证明，展示了SGD训练下不同宽度和深度的EDLN会达到完美柏拉图状态。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨EDLN模型在SGD训练下为何会趋向完美柏拉图状态，并分析这一现象与深度学习中其他看似无关现象的潜在共同原因。

Method: 通过详细解释Ziyin等人的证明过程，分析SGD训练对EDLN模型表示学习的影响，并探讨PRH可能被破坏的六种情况。

Result: 发现SGD仅找到完美柏拉图解，且证明柏拉图表示的出现与渐进锐化现象有共同原因。

Conclusion: 强调了理解SGD训练的不可逆性及其在表示学习中的作用的重要性。

Abstract: In this note, we elaborate on and explain in detail the proof given by Ziyin
et al. (2025) of the "perfect" Platonic Representation Hypothesis (PRH) for the
embedded deep linear network model (EDLN). We show that if trained with SGD,
two EDLNs with different widths and depths and trained on different data will
become Perfectly Platonic, meaning that every possible pair of layers will
learn the same representation up to a rotation. Because most of the global
minima of the loss function are not Platonic, that SGD only finds the perfectly
Platonic solution is rather extraordinary. The proof also suggests at least six
ways the PRH can be broken. We also show that in the EDLN model, the emergence
of the Platonic representations is due to the same reason as the emergence of
progressive sharpening. This implies that these two seemingly unrelated
phenomena in deep learning can, surprisingly, have a common cause. Overall, the
theory and proof highlight the importance of understanding emergent "entropic
forces" due to the irreversibility of SGD training and their role in
representation learning. The goal of this note is to be instructive and avoid
lengthy technical details.

</details>


### [73] [A Neural Operator based on Dynamic Mode Decomposition](https://arxiv.org/abs/2507.01117)
*Nikita Sakovich,Dmitry Aksenov,Ekaterina Pleshakova,Sergey Gataullin*

Key words: 动态模态分解,深度学习,神经算子,偏微分方程,科学计算

TL;DR: 该论文提出了一种基于动态模态分解算法（DMD）的神经算子，结合深度学习和DMD，用于高效建模时空过程，显著降低了计算成本，并在多项测试中展现高精度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 科学计算与人工智能的结合是研究热点，如何在轻量化和精确计算之间找到平衡是关键。传统数值方法在解决偏微分方程时计算资源消耗大，亟需高效建模方法。

Method: 提出一种基于DMD的神经算子，自动提取关键模态和系统动态，并利用深度学习构建预测模型，适用于多种初始和边界条件下的偏微分方程求解。

Result: 与DeepONet和FNO等现有方法相比，该方法在热方程、拉普拉斯方程和Burgers方程的近似解中表现出更高的重构精度和更低计算成本。

Conclusion: 结合DMD和深度学习的方法是高效建模时空过程的有效途径，为科学计算和人工智能的结合提供了新思路。

Abstract: The scientific computation methods development in conjunction with artificial
intelligence technologies remains a hot research topic. Finding a balance
between lightweight and accurate computations is a solid foundation for this
direction. The study presents a neural operator based on the dynamic mode
decomposition algorithm (DMD), mapping functional spaces, which combines DMD
and deep learning (DL) for spatiotemporal processes efficient modeling. Solving
PDEs for various initial and boundary conditions requires significant
computational resources. The method suggested automatically extracts key modes
and system dynamics using them to construct predictions, reducing computational
costs compared to traditional numerical methods. The approach has demonstrated
its efficiency through comparative analysis of performance with closest
analogues DeepONet and FNO in the heat equation, Laplaces equation, and Burgers
equation solutions approximation, where it achieves high reconstruction
accuracy.

</details>


### [74] [On Design Principles for Private Adaptive Optimizers](https://arxiv.org/abs/2507.01129)
*Arun Ganesh,Brendan McMahan,Abhradeep Thakurta*

Key words: 差分隐私, 自适应优化器, AdaGrad, Adam, scale-then-privatize

TL;DR: 这篇论文探讨了在差分隐私训练中，球面噪声对自适应优化器性能的影响，并比较了多种改进方法的理论与实际效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究差分隐私训练中球面噪声对自适应优化器（如AdaGrad和Adam）性能的负面影响，并探索更有效的解决方法。

Method: 调查多种改进方法，并进行理论分析和实证研究，特别关注“scale-then-privatize”技术。

Result: 研究发现，追求梯度二阶矩的无偏估计是错误的直觉，而“scale-then-privatize”技术在实际应用中表现最佳。

Conclusion: “scale-then-privatize”技术具有更好的理论行为和实际性能，更适合差分隐私训练。

Abstract: The spherical noise added to gradients in differentially private (DP)
training undermines the performance of adaptive optimizers like AdaGrad and
Adam, and hence many recent works have proposed algorithms to address this
challenge. However, the empirical results in these works focus on simple tasks
and models and the conclusions may not generalize to model training in
practice. In this paper we survey several of these variants, and develop better
theoretical intuition for them as well as perform empirical studies comparing
them. We find that a common intuition of aiming for unbiased estimates of
second moments of gradients in adaptive optimizers is misguided, and instead
that a simple technique called scale-then-privatize (which does not achieve
unbiased second moments) has more desirable theoretical behaviors and
outperforms all other variants we study on a small-scale language model
training task. We additionally argue that scale-then-privatize causes the noise
addition to better match the application of correlated noise mechanisms which
are more desirable to use in practice.

</details>


### [75] [Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations](https://arxiv.org/abs/2507.01131)
*Yuchao Lin,Cong Fu,Zachary Krueger,Haiyang Yu,Maho Nakata,Jianwen Xie,Emine Kucukbenli,Xiaofeng Qian,Shuiwang Ji*

Key words: SO(3)-等变网络, 张量分解, Clebsch-Gordan张量积, 机器学习原子间势, 计算加速

TL;DR: 本文提出了一种基于低秩张量分解的近似等变网络（TDNs），用于加速SO(3)-等变网络中的Clebsch-Gordan张量积计算，并通过理论和实验验证了其高效性和通用性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: SO(3)-等变网络是机器学习原子间势（MLIPs）的主流模型，但其核心操作Clebsch-Gordan张量积计算成本高，亟需优化。

Method: 通过低秩张量分解（如CP分解）替代CG张量积，提出路径权重共享以减少参数，并将计算复杂度从O(L^6)降至O(L^4)。

Result: 在PubChemQCR等数据集上的实验表明，TDNs在保持性能的同时显著加速了计算。

Conclusion: TDNs作为一种即插即用的替代方案，在高效性和通用性方面具有显著优势。

Abstract: $\rm{SO}(3)$-equivariant networks are the dominant models for machine
learning interatomic potentials (MLIPs). The key operation of such networks is
the Clebsch-Gordan (CG) tensor product, which is computationally expensive. To
accelerate the computation, we develop tensor decomposition networks (TDNs) as
a class of approximately equivariant networks whose CG tensor products are
replaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)
decomposition. With the CP decomposition, we prove (i) a uniform bound on the
induced error of $\rm{SO}(3)$-equivariance, and (ii) the universality of
approximating any equivariant bilinear map. To further reduce the number of
parameters, we propose path-weight sharing that ties all multiplicity-space
weights across the $O(L^3)$ CG paths into a single path without compromising
equivariance, where $L$ is the maximum angular degree. The resulting layer acts
as a plug-and-play replacement for tensor products in existing networks, and
the computational complexity of tensor products is reduced from $O(L^6)$ to
$O(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation
dataset containing 105 million DFT-calculated snapshots. We also use existing
datasets, including OC20, and OC22. Results show that TDNs achieve competitive
performance with dramatic speedup in computations.

</details>


### [76] [Spectral Manifold Harmonization for Graph Imbalanced Regression](https://arxiv.org/abs/2507.01132)
*Brenda Nogueira,Gabe Gomes,Meng Jiang,Nitesh V. Chawla,Nuno Moniz*

Key words: 图结构数据, 不平衡回归, 谱流形协调, 合成样本, 药物发现

TL;DR: 本文提出了一种名为Spectral Manifold Harmonization（SMH）的新方法，用于解决图结构数据中不平衡回归问题，通过生成具有拓扑特性的合成样本，重点关注目标分布中 underrepresented 的区域。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的方法在图结构数据的不平衡回归问题中存在不足，要么忽略了图拓扑，要么未能针对性解决特定域范围的问题，导致模型偏向于平均目标值。

Method: SMH方法通过生成保持拓扑特性的合成图样本，并专注于目标分布中 underrepresented 的区域，以解决不平衡回归问题。

Result: 实验结果表明，SMH在化学和药物发现基准数据集上能够显著提升对目标域范围的预测性能。

Conclusion: SMH是一种有效的方法，能够在图结构数据的不平衡回归任务中改善模型性能，特别是在科学上重要的目标范围内。

Abstract: Graph-structured data is ubiquitous in scientific domains, where models often
face imbalanced learning settings. In imbalanced regression, domain preferences
focus on specific target value ranges representing the most scientifically
valuable cases; we observe a significant lack of research. In this paper, we
present Spectral Manifold Harmonization (SMH), a novel approach for addressing
this imbalanced regression challenge on graph-structured data by generating
synthetic graph samples that preserve topological properties while focusing on
often underrepresented target distribution regions. Conventional methods fail
in this context because they either ignore graph topology in case generation or
do not target specific domain ranges, resulting in models biased toward average
target values. Experimental results demonstrate the potential of SMH on
chemistry and drug discovery benchmark datasets, showing consistent
improvements in predictive performance for target domain ranges.

</details>


### [77] [FlashDP: Private Training Large Language Models with Efficient DP-SGD](https://arxiv.org/abs/2507.01154)
*Liangyu Wang,Junxiao Wang,Jie Ren,Zihang Xiang,David E. Keyes,Di Wang*

Key words: 大语言模型, 差分隐私, DP-SGD, 梯度裁剪, 内存优化

TL;DR: FlashDP是一种创新的缓存友好型DP-SGD方法，通过融合计算梯度，减少内存占用和冗余计算，提升训练效率，同时保持隐私保护效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着大语言模型的广泛应用，训练数据隐私成为关键问题。当前差分隐私方法（如DP-SGD）存在内存需求和计算冗余的挑战，需优化方案。

Method: 提出FlashDP，通过单次任务融合计算梯度，减少内存移动和冗余计算。

Result: FlashDP内存占用降低50%，计算冗余减少20%，在Llama-13B预训练中达到非DP方法90%的吞吐量。

Conclusion: FlashDP为高效且隐私保护的大语言模型训练提供了重要解决方案。

Abstract: As large language models (LLMs) increasingly underpin technological
advancements, the privacy of their training data emerges as a critical concern.
Differential Privacy (DP) serves as a rigorous mechanism to protect this data,
yet its integration via Differentially Private Stochastic Gradient Descent
(DP-SGD) introduces substantial challenges, primarily due to the complexities
of per-sample gradient clipping. Current explicit methods, such as Opacus,
necessitate extensive storage for per-sample gradients, significantly inflating
memory requirements. Conversely, implicit methods like GhostClip reduce storage
needs by recalculating gradients multiple times, which leads to inefficiencies
due to redundant computations. This paper introduces FlashDP, an innovative
cache-friendly per-layer DP-SGD that consolidates necessary operations into a
single task, calculating gradients only once in a fused manner. This approach
not only diminishes memory movement by up to \textbf{50\%} but also cuts down
redundant computations by \textbf{20\%}, compared to previous methods.
Consequently, FlashDP does not increase memory demands and achieves a
\textbf{90\%} throughput compared to the Non-DP method on a four-A100 system
during the pre-training of the Llama-13B model, while maintaining parity with
standard per-layer clipped DP-SGD in terms of accuracy. These advancements
establish FlashDP as a pivotal development for efficient and privacy-preserving
training of LLMs. FlashDP's code has been open-sourced in
https://github.com/kaustpradalab/flashdp.

</details>


### [78] [Diffusion Explorer: Interactive Exploration of Diffusion Models](https://arxiv.org/abs/2507.01178)
*Alec Helbling,Duen Horng Chau*

Key words: 扩散模型,几何特性,交互工具,动画可视化

TL;DR: 论文介绍了Diffusion Explorer，一款交互式工具，旨在通过动画方式直观展示扩散模型的几何特性，帮助用户理解其动态过程。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有资源对扩散模型的解释要么需要高深的理论基础，要么过于关注神经网络架构，而忽略了其丰富的几何特性。

Method: Diffusion Explorer通过浏览器训练2D扩散模型，并结合交互式动画，直观展示采样过程的动态特性。

Result: 该工具成功展现了扩散模型的时间动态和几何特性，适合用于教学和演示。

Conclusion: Diffusion Explorer为扩散模型的几何特性提供了易懂的交互式解释工具，具有开源和在线演示的优势。

Abstract: Diffusion models have been central to the development of recent image, video,
and even text generation systems. They posses striking geometric properties
that can be faithfully portrayed in low-dimensional settings. However, existing
resources for explaining diffusion either require an advanced theoretical
foundation or focus on their neural network architectures rather than their
rich geometric properties. We introduce Diffusion Explorer, an interactive tool
to explain the geometric properties of diffusion models. Users can train 2D
diffusion models in the browser and observe the temporal dynamics of their
sampling process. Diffusion Explorer leverages interactive animation, which has
been shown to be a powerful tool for making engaging visualizations of dynamic
systems, making it well suited to explaining diffusion models which represent
stochastic processes that evolve over time. Diffusion Explorer is open source
and a live demo is available at alechelbling.com/Diffusion-Explorer.

</details>


### [79] [Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning](https://arxiv.org/abs/2507.01196)
*Na Lee,Konstantinos Barmpas,Yannis Panagakis,Dimitrios Adamos,Nikolaos Laskaris,Stefanos Zafeiriou*

Key words: 基础模型, 脑机接口, LoRA, 参数优化, 脑电波分析

TL;DR: 本文评估了大模型在脑电波建模中的表现，发现其在脑机接口任务中仅比传统架构有微小提升，但参数更多。通过LoRA技术减少了参数且保持性能，指出需优化架构以发挥潜力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究基础模型在脑电波建模中的应用潜力及其在脑机接口任务中的效率。

Method: 对现有大模型进行系统微调实验，结合低秩适应（LoRA）技术优化参数，分析性能表现。

Result: 大模型仅带来0.9%-1.2%的提升但参数更多；LoRA减少参数且不影响性能。

Conclusion: 当前大模型在脑电波分析中效率不足，需重新设计架构并结合领域优化策略。

Abstract: Foundation Models have demonstrated significant success across various
domains in Artificial Intelligence (AI), yet their capabilities for brainwave
modeling remain unclear. In this paper, we comprehensively evaluate current
Large Brainwave Foundation Models (LBMs) through systematic fine-tuning
experiments across multiple Brain-Computer Interface (BCI) benchmark tasks,
including memory tasks and sleep stage classification. Our extensive analysis
shows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)
over traditional deep architectures while requiring significantly more
parameters (millions vs thousands), raising important questions about their
efficiency and applicability in BCI contexts. Moreover, through detailed
ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce
trainable parameters without performance degradation, while demonstrating that
architectural and training inefficiencies limit LBMs' current capabilities. Our
experiments span both full model fine-tuning and parameter-efficient adaptation
techniques, providing insights into optimal training strategies for BCI
applications. We pioneer the application of LoRA to LBMs, revealing that
performance benefits generally emerge when adapting multiple neural network
components simultaneously. These findings highlight the critical need for
domain-specific development strategies to advance LBMs, suggesting that current
architectures may require redesign to fully leverage the potential of
foundation models in brainwave analysis.

</details>


### [80] [Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models](https://arxiv.org/abs/2507.01201)
*Hyoseo,Yoon,Yisong Yue,Been Kim*

Key words: 视觉与语言模型,共享表示对齐,柏拉图表示假说,自编码调制器

TL;DR: 论文探讨了视觉和语言模型的共享表示对齐问题，提出了一种联合自编码调制器（JAM）框架，实现了模态间对齐的优化。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究独立训练的视觉和语言模型是否可以通过优化实现共享表示对齐，以此验证柏拉图表示假说。

Method: 提出JAM框架，通过联合训练模态特定的自编码器，结合重构和跨模态目标，优化对齐效果。

Result: 实验验证了JAM框架在多种设计轴（如对齐目标、层深度和模型规模）下的有效性，能够可靠地实现对齐。

Conclusion: JAM框架为将通用单模态模型转化为专用多模态模型提供了理论与实践基础。

Abstract: Independently trained vision and language models inhabit disjoint
representational spaces, shaped by their respective modalities, objectives, and
architectures. Yet an emerging hypothesis - the Platonic Representation
Hypothesis - suggests that such models may nonetheless converge toward a shared
statistical model of reality. This compatibility, if it exists, raises a
fundamental question: can we move beyond post-hoc statistical detection of
alignment and explicitly optimize for it between such disjoint representations?
We cast this Platonic alignment problem as a multi-objective optimization task
- preserve each modality's native structure while aligning for mutual
coherence. We introduce the Joint Autoencoder Modulator (JAM) framework that
jointly trains modality-specific autoencoders on the latent representations of
pre-trained single modality models, encouraging alignment through both
reconstruction and cross-modal objectives. By analogy, this framework serves as
a method to escape Plato's Cave, enabling the emergence of shared structure
from disjoint inputs. We evaluate this framework across three critical design
axes: (i) the alignment objective - comparing contrastive loss (Con), its
hard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at
which alignment is most effective, and (iii) the impact of foundation model
scale on representational convergence. Our findings show that our lightweight
Pareto-efficient framework reliably induces alignment, even across frozen,
independently trained representations, offering both theoretical insight and
practical pathways for transforming generalist unimodal foundations into
specialist multimodal models.

</details>


### [81] [Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform](https://arxiv.org/abs/2507.01208)
*Pedro R. X. Carmo,Igor de Moura,Assis T. de Oliveira Filho,Djamel Sadok,Cleber Zanchettin*

Key words: 车载以太网、入侵检测系统、深度学习、蒸馏、剪枝、低成本平台

TL;DR: 论文探讨了如何在低成本平台上实时部署基于深度学习的入侵检测系统，通过蒸馏和剪枝技术实现高效推理。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现代车辆的联网特性使其易受攻击，尤其是流注入攻击。现有深度学习检测系统需要昂贵硬件，不适用于低成本平台。

Method: 采用蒸馏和剪枝技术优化神经网络模型，以在低成本设备（如Raspberry Pi 4）上实现实时推理。

Result: 在Raspberry Pi 4上实现了727微秒的入侵检测时间，AUCROC值达0.9890。

Conclusion: 蒸馏和剪枝技术可有效降低模型复杂度，适合在低成本硬件上部署实时入侵检测系统。

Abstract: Modern vehicles are increasingly connected, and in this context, automotive
Ethernet is one of the technologies that promise to provide the necessary
infrastructure for intra-vehicle communication. However, these systems are
subject to attacks that can compromise safety, including flow injection
attacks. Deep Learning-based Intrusion Detection Systems (IDS) are often
designed to combat this problem, but they require expensive hardware to run in
real time. In this work, we propose to evaluate and apply fast neural network
inference techniques like Distilling and Prunning for deploying IDS models on
low-cost platforms in real time. The results show that these techniques can
achieve intrusion detection times of up to 727 {\mu}s using a Raspberry Pi 4,
with AUCROC values of 0.9890.

</details>


### [82] [PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning](https://arxiv.org/abs/2507.01216)
*Xingke Yang,Liang Li,Zhiyi Wan,Sicong Li,Hao Wang,Xiaoqi Qi,Jiang Liu,Tomoaki Ohtsuki,Xin Fu,Miao Pan*

Key words: 大语言模型，微调，移动设备，隐私保护，高效通信

TL;DR: PAE MobiLLM是一种隐私高效的大语言模型微调方法，通过服务器辅助的侧调技术，解决了移动设备资源有限的问题，同时降低了通信负担并保护了数据隐私。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 移动设备上的大语言模型微调存在资源有限、通信负担重以及隐私泄露的问题，PAE MobiLLM旨在解决这些问题。

Method: PAE MobiLLM采用服务器辅助的侧调技术，集成了服务器端的激活缓存、单令牌激活快捷方式和适配器侧网络设计，以提高效率和隐私保护。

Result: PAE MobiLLM能够显著减少通信成本、加速微调收敛，并确保数据、标签和模型的隐私安全。

Conclusion: PAE MobiLLM为移动设备上的大语言模型微调提供了一种高效且隐私安全的解决方案。

Abstract: There is a huge gap between numerous intriguing applications fostered by
on-device large language model (LLM) fine-tuning (FT) from fresh mobile data
and the limited resources of a mobile device. While existing server-assisted
methods (e.g., split learning or side-tuning) may enable LLM FT on the local
mobile device, they suffer from heavy communication burdens of activation
transmissions, and may disclose data, labels or fine-tuned models to the
server. To address those issues, we develop PAE MobiLLM, a privacy-aware and
efficient LLM FT method which can be deployed on the mobile device via
server-assisted additive side-tuning. To further accelerate FT convergence and
improve computing efficiency, PAE MobiLLM integrates activation caching on the
server side, which allows the server to reuse historical activations and saves
the mobile device from repeatedly computing forward passes for the recurring
data samples. Besides, to reduce communication cost, PAE MobiLLM develops a
one-token (i.e., ``pivot'' token) activation shortcut that transmits only a
single activation dimension instead of full activation matrices to guide the
side network tuning. Last but not least, PAE MobiLLM introduces the additive
adapter side-network design which makes the server train the adapter modules
based on device-defined prediction differences rather than raw ground-truth
labels. In this way, the server can only assist device-defined side-network
computing, and learn nothing about data, labels or fine-tuned models.

</details>


### [83] [Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling](https://arxiv.org/abs/2507.01235)
*Bara Rababa,Bilal Farooq*

Key words: 量子机器学习, QSVM, QNN, 皮肤电导响应, 智能交通系统

TL;DR: 本文探讨量子机器学习在分析行人压力中的应用，比较了量子支持向量机（QSVM）和量子神经网络（QNN）的性能。QNN表现优于QSVM和经典方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究量子计算在机器学习中的潜力，特别是在智能交通系统中分析高维数据（如皮肤电导响应）的能力。

Method: 使用Pennylane开发了QSVM和QNN模型，分别采用八量子位的ZZ特征图和树张量网络结构。

Result: QSVM训练准确度高但测试准确度仅为45%，过拟合问题严重；QNN测试准确度达55%，优于QSVM和经典方法。

Conclusion: QNN在分类任务中表现更优，展示了量子机器学习在复杂数据分析中的潜力。

Abstract: Quantum computing has opened new opportunities to tackle complex machine
learning tasks, for instance, high-dimensional data representations commonly
required in intelligent transportation systems. We explore quantum machine
learning to model complex skin conductance response (SCR) events that reflect
pedestrian stress in a virtual reality road crossing experiment. For this
purpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature
map and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and
an eight-qubit ZZ feature map, were developed on Pennylane. The dataset
consists of SCR measurements along with features such as the response amplitude
and elapsed time, which have been categorized into amplitude-based classes. The
QSVM achieved good training accuracy, but had an overfitting problem, showing a
low test accuracy of 45% and therefore impacting the reliability of the
classification model. The QNN model reached a higher test accuracy of 55%,
making it a better classification model than the QSVM and the classic versions.

</details>


### [84] [Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW](https://arxiv.org/abs/2507.01241)
*Di Zhang,Yihang Zhang*

Key words: 随机梯度下降、大语言模型、随机共轭次梯度、自适应采样、AdamW

TL;DR: 本文提出了一种针对大语言模型（LLM）训练的随机共轭次梯度方法，结合自适应采样，相比传统SGD方法实现了更快的收敛速度和更好的可扩展性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统随机梯度下降（SGD）在大规模应用中表现出性能限制，因此需要更高效的方法来训练大语言模型。

Method: 采用随机共轭次梯度方法，结合自适应采样和AdamW类算法调整步长，解决了LLM训练中的非凸性和非光滑性问题。

Result: 实验表明，该方法在速度和准确性上均优于传统SGD，同时保持或超越了其可扩展性。

Conclusion: 该方法为训练大型语言模型提供了更高效的优化方案。

Abstract: Stochastic gradient-based descent (SGD), have long been central to training
large language models (LLMs). However, their effectiveness is increasingly
being questioned, particularly in large-scale applications where empirical
evidence suggests potential performance limitations. In response, this paper
proposes a stochastic conjugate subgradient method together with adaptive
sampling tailored specifically for training LLMs. The method not only achieves
faster convergence per iteration but also demonstrates improved scalability
compared to traditional SGD techniques. It leverages sample complexity analysis
to adaptively choose the sample size, employs a stochastic conjugate
subgradient approach to determine search directions and utilizing an AdamW-like
algorithm to adaptively adjust step sizes. This approach preserves the key
advantages of first-order methods while effectively addressing the nonconvexity
and non-smoothness inherent in LLMs training. Additionally, we provide a
detailed analysis of the advantage of the algorithm. Experimental results show
that the proposed method not only maintains, but in many cases surpasses, the
scalability of traditional SGD techniques, significantly enhancing both the
speed and accuracy of the optimization process.

</details>


### [85] [PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning](https://arxiv.org/abs/2507.01271)
*Tatsuki Kawakami,Kazuki Egashira,Atsuyuki Miyai,Go Irie,Kiyoharu Aizawa*

Key words: 遗忘技术, PULSE协议, 大模型, 预训练知识, 长期可持续性

TL;DR: 该论文提出了PULSE协议，用于评估大模型（LMMs）中针对预训练知识和长期可持续性需求的遗忘技术，发现现有方法在预训练知识遗忘和连续请求处理上存在不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对大模型（LMMs）的遗忘技术缺乏实际评估框架，现有方法仅关注单次微调知识遗忘，忽略了预训练知识和连续请求的影响。

Method: 引入PULSE协议，包括预训练知识遗忘和长期可持续性评估两个维度，对现有遗忘方法进行测试。

Result: 现有方法能有效遗忘微调知识，但对预训练知识效果差；处理连续请求时性能显著下降。

Conclusion: PULSE协议为LMMs遗忘技术提供了更全面的评估框架，揭示了现有方法的局限性。

Abstract: In recent years, unlearning techniques, which are methods for inducing a
model to "forget" previously learned information, have attracted attention as a
way to address privacy and copyright concerns in large language models (LLMs)
and large multimodal models (LMMs). While several unlearning benchmarks have
been established for LLMs, a practical evaluation framework for unlearning in
LMMs has been less explored. Specifically, existing unlearning benchmark for
LMMs considers only scenarios in which the model is required to unlearn
fine-tuned knowledge through a single unlearning operation. In this study, we
introduce PULSE protocol for realistic unlearning scenarios for LMMs by
introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for
analyzing the effect across different knowledge acquisition phases and (ii)
Long-term Sustainability Evaluation to address sequential requests. We then
evaluate existing unlearning methods along these dimensions. Our results reveal
that, although some techniques can successfully unlearn knowledge acquired
through fine-tuning, they struggle to eliminate information learned during
pre-training. Moreover, methods that effectively unlearn a batch of target data
in a single operation exhibit substantial performance degradation when the same
data are split and unlearned sequentially.

</details>


### [86] [Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation](https://arxiv.org/abs/2507.01285)
*Aymen Rayane Khouas,Mohamed Reda Bouadjenek,Hakim Hacid,Sunil Aryal*

Key words: 联邦学习,图推荐系统,用户嵌入,聚合方法

TL;DR: 论文提出了一种基于距离的聚合方法Dist-FedAvg，用于提升图联邦学习的个性化与聚合效率，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统联邦聚合方法忽视了用户嵌入的独特性和用户相似性的重要性，导致推荐效果不佳。

Method: 提出Dist-FedAvg方法，基于用户嵌入相似性分配聚合权重，并保留锚用户影响力。

Result: 实验证明Dist-FedAvg在多个数据集上表现优于基线方法，提升推荐准确性。

Conclusion: Dist-FedAvg能有效提升图联邦推荐系统的性能，同时易于集成。

Abstract: Graph federated recommendation systems offer a privacy-preserving alternative
to traditional centralized recommendation architectures, which often raise
concerns about data security. While federated learning enables personalized
recommendations without exposing raw user data, existing aggregation methods
overlook the unique properties of user embeddings in this setting. Indeed,
traditional aggregation methods fail to account for their complexity and the
critical role of user similarity in recommendation effectiveness. Moreover,
evolving user interactions require adaptive aggregation while preserving the
influence of high-relevance anchor users (the primary users before expansion in
graph-based frameworks). To address these limitations, we introduce
Dist-FedAvg, a novel distance-based aggregation method designed to enhance
personalization and aggregation efficiency in graph federated learning. Our
method assigns higher aggregation weights to users with similar embeddings,
while ensuring that anchor users retain significant influence in local updates.
Empirical evaluations on multiple datasets demonstrate that Dist-FedAvg
consistently outperforms baseline aggregation techniques, improving
recommendation accuracy while maintaining seamless integration into existing
federated learning frameworks.

</details>


### [87] [Neural Hamiltonian Operator](https://arxiv.org/abs/2507.01313)
*Qian Qi*

Key words: 随机控制；高维问题；神经哈密顿算子；FBSDE；深度学习

TL;DR: 提出了一种名为神经哈密顿算子（NHO）的深度学习框架，用于解决高维随机控制问题，通过神经网络参数化FBSDE动态，并证明了其通用逼近能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 高维随机控制问题因维度诅咒难以解决，传统动态规划方法受限，而PMP方法将其转化为FBSDE系统，需要新方法来解决。

Method: 引入神经哈密顿算子（NHO），通过神经网络参数化FBSDE动态，将PMP的一致性条件作为训练目标，从模拟数据中学习未知算子。

Result: 证明了NHO在一般鞅驱动下的通用逼近能力，并分析了该模型中的优化挑战。

Conclusion: NHO框架为高维随机控制问题提供了一种有效的深度学习解决方案，并在统计推断视角下具有理论保证。

Abstract: Stochastic control problems in high dimensions are notoriously difficult to
solve due to the curse of dimensionality. An alternative to traditional dynamic
programming is Pontryagin's Maximum Principle (PMP), which recasts the problem
as a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In
this paper, we introduce a formal framework for solving such problems with deep
learning by defining a \textbf{Neural Hamiltonian Operator (NHO)}. This
operator parameterizes the coupled FBSDE dynamics via neural networks that
represent the feedback control and an ansatz for the value function's spatial
gradient. We show how the optimal NHO can be found by training the underlying
networks to enforce the consistency conditions dictated by the PMP. By adopting
this operator-theoretic view, we situate the deep FBSDE method within the
rigorous language of statistical inference, framing it as a problem of learning
an unknown operator from simulated data. This perspective allows us to prove
the universal approximation capabilities of NHOs under general martingale
drivers and provides a clear lens for analyzing the significant optimization
challenges inherent to this class of models.

</details>


### [88] [ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks](https://arxiv.org/abs/2507.01321)
*Zhiyao Ren,Siyuan Liang,Aishan Liu,Dacheng Tao*

Key words: 上下文学习（ICL）、大语言模型（LLMs）、后门攻击、防御机制、双学习假设

TL;DR: 本文提出了双学习假设和ICLShield防御机制，用于缓解大语言模型（LLMs）在上下文学习（ICL）中对后门攻击的脆弱性，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: ICL因其适应性和无参数特性在LLMs中表现优异，但也容易受到后门攻击。本文旨在揭示LLMs在中毒示范中同时学习任务相关和后门概念的现象，并提出防御方法。

Method: 提出双学习假设，分析ICL后门效应的上界，并设计ICLShield机制，通过动态调整概念偏好比，利用置信度和相似度分数选择干净示范。

Result: 实验表明，ICLShield在防御效果上显著优于现有方法（平均提升26.02%），并能有效适配闭源模型（如GPT-4）。

Conclusion: 本研究揭示了LLMs在ICL中的后门漏洞机理，并提出了一种高效的防御方法，为提升模型安全性提供了新思路。

Abstract: In-context learning (ICL) has demonstrated remarkable success in large
language models (LLMs) due to its adaptability and parameter-free nature.
However, it also introduces a critical vulnerability to backdoor attacks, where
adversaries can manipulate LLM behaviors by simply poisoning a few ICL
demonstrations. In this paper, we propose, for the first time, the
dual-learning hypothesis, which posits that LLMs simultaneously learn both the
task-relevant latent concepts and backdoor latent concepts within poisoned
demonstrations, jointly influencing the probability of model outputs. Through
theoretical analysis, we derive an upper bound for ICL backdoor effects,
revealing that the vulnerability is dominated by the concept preference ratio
between the task and the backdoor. Motivated by these findings, we propose
ICLShield, a defense mechanism that dynamically adjusts the concept preference
ratio. Our method encourages LLMs to select clean demonstrations during the ICL
phase by leveraging confidence and similarity scores, effectively mitigating
susceptibility to backdoor attacks. Extensive experiments across multiple LLMs
and tasks demonstrate that our method achieves state-of-the-art defense
effectiveness, significantly outperforming existing approaches (+26.02% on
average). Furthermore, our method exhibits exceptional adaptability and
defensive performance even for closed-source models (e.g., GPT-4).

</details>


### [89] [Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy](https://arxiv.org/abs/2507.01327)
*Xiaoyun Zhang,Jingqing Ruan,Xing Ma,Yawen Zhu,Jiansong Chen,Ke Zeng,Xunliang Cai*

Key words: 异常事件检测, 跨领域泛化, 自适应强化学习, 动态课程学习, 客户服务对话

TL;DR: 提出了一种名为APARL的新框架，通过双重动态课程学习架构和大型语言模型的推理能力，显著提升了异常事件检测的性能和跨领域泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于业务数据的复杂性和客户交互的动态性，检测客户服务对话中的异常事件具有挑战性，且需要模型具备强大的跨领域泛化能力。

Method: 引入了自适应困惑感知强化学习（APARL）框架，采用双循环动态课程学习架构，逐步专注于更具挑战性的样本。

Result: 在食品配送对话任务中，该方法显著提升了模型的适应性和鲁棒性，F1分数平均提高17.19%，跨领域测试中平均提高9.59%。

Conclusion: APARL为异常检测模型的工业部署提供了优越方案，有助于提高运营效率和商业效益。

Abstract: Detecting abnormal events in real-world customer service dialogues is highly
challenging due to the complexity of business data and the dynamic nature of
customer interactions. Moreover, models must demonstrate strong out-of-domain
(OOD) generalization to enable rapid adaptation across different business
scenarios and maximize commercial value. In this work, we propose a novel
Adaptive Perplexity-Aware Reinforcement Learning (APARL) framework that
leverages the advanced reasoning capabilities of large language models for
abnormal event detection. APARL introduces a dual-loop dynamic curriculum
learning architecture, enabling the model to progressively focus on more
challenging samples as its proficiency increases. This design effectively
addresses performance bottlenecks and significantly enhances OOD
transferability. Extensive evaluations on food delivery dialogue tasks show
that our model achieves significantly enhanced adaptability and robustness,
attaining the highest F1 score with an average improvement of 17.19\%, and an
average improvement of 9.59\% in OOD transfer tests. This method provides a
superior solution for industrial deployment of anomaly detection models,
contributing to improved operational efficiency and commercial benefits.

</details>


### [90] [Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion](https://arxiv.org/abs/2507.01354)
*Chugang Yi,Minghan Yu,Weikang Qian,Yixin Wen,Haizhao Yang*

Key words: 小波扩散模型, 降水数据, 超分辨率, 水文建模, 天气分析

TL;DR: 论文提出了一种基于小波扩散模型（WDM）的生成框架，用于降水数据的高分辨率降尺度（从10公里降至1公里），并比基于像素的扩散模型快9倍。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有全球降水数据（如IMERG）的分辨率（10公里）不足以满足水文建模和极端天气分析的需求，需要更高分辨率（1公里）的降水数据。

Method: WDM是一种基于小波的扩散模型，通过学习MRMS雷达数据的小波域特征，重点处理高频小波系数，生成1公里分辨率的降水场。

Result: WDM在视觉上优于基于像素的模型，生成结果更真实且细节更丰富，采样效率显著提高，且速度快9倍。

Conclusion: WDM为地学超分辨率问题提供了高精度与高速度的解决方案，有助于更可靠的水文预测。

Abstract: Effective hydrological modeling and extreme weather analysis demand
precipitation data at a kilometer-scale resolution, which is significantly
finer than the 10 km scale offered by standard global products like IMERG. To
address this, we propose the Wavelet Diffusion Model (WDM), a generative
framework that achieves 10x spatial super-resolution (downscaling to 1 km) and
delivers a 9x inference speedup over pixel-based diffusion models. WDM is a
conditional diffusion model that learns the learns the complex structure of
precipitation from MRMS radar data directly in the wavelet domain. By focusing
on high-frequency wavelet coefficients, it generates exceptionally realistic
and detailed 1-km precipitation fields. This wavelet-based approach produces
visually superior results with fewer artifacts than pixel-space models, and
delivers a significant gains in sampling efficiency. Our results demonstrate
that WDM provides a robust solution to the dual challenges of accuracy and
speed in geoscience super-resolution, paving the way for more reliable
hydrological forecasts.

</details>


### [91] [Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning](https://arxiv.org/abs/2507.01551)
*Wu Fei,Hao Kong,Shuxian Liang,Yang Lin,Yibo Yang,Jing Tang,Lei Chen,Xiansheng Hua*

Key words: 过程强化学习, 大型语言模型, 自引导优化, 累积奖励, 遮蔽步骤优势

TL;DR: SPRO是一种新型的自引导过程奖励优化框架，通过内在推导过程奖励和引入累积过程奖励及遮蔽步骤优势（MSA），显著提升训练效率和测试准确率，同时减少计算开销。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的过程强化学习（PRL）方法需要额外计算开销且缺乏统一的理论框架，SPRO旨在解决这一问题。

Method: SPRO通过理论证明过程奖励可从策略模型本身内在推导，并引入累积过程奖励和遮蔽步骤优势（MSA），实现严格的分步优势估计。

Result: SPRO在训练效率上比GRPO高3.4倍，测试准确率提升17.5%，同时减少响应长度约1/3，且无额外计算开销。

Conclusion: SPRO是一种高效、稳定的过程强化学习框架，适用于工业实现。

Abstract: Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.

</details>


### [92] [Distributional Soft Actor-Critic with Diffusion Policy](https://arxiv.org/abs/2507.01381)
*Tong Liu,Yinuo Wang,Xujie Song,Wenjun Zou,Liangfa Chen,Likun Wang,Bin Shuai,Jingliang Duan,Shengbo Eben Li*

Key words: 强化学习, 分布强化学习, DSAC-D, 多模态分布, 扩散模型

TL;DR: 本文提出了一种名为DSAC-D的分布强化学习算法，通过引入多模态价值分布和扩散策略，解决了传统单模态分布导致的估计偏差问题，并在多个任务中实现了SOTA性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统强化学习方法使用单模态分布（如高斯分布）建模价值函数，容易导致估计偏差，影响算法性能。本文旨在解决这一问题，并通过多模态分布提升算法表现。

Method: 提出DSAC-D算法，结合策略熵和价值分布函数，建立多模态分布策略迭代框架；通过扩散模型反向采样生成奖励样本，构建扩散价值网络；实现价值网络与策略网络的双重扩散。

Result: DSAC-D在9个MuJoCo控制任务中均达到SOTA性能，显著抑制了估计偏差，平均总回报提升超10%。实车测试表明，该算法可准确表征不同驾驶风格的多模态分布。

Conclusion: DSAC-D通过多模态分布和扩散策略，有效解决了传统方法的估计偏差问题，提升了强化学习的性能和适用性。

Abstract: Reinforcement learning has been proven to be highly effective in handling
complex control tasks. Traditional methods typically use unimodal
distributions, such as Gaussian distributions, to model the output of value
distributions. However, unimodal distribution often and easily causes bias in
value function estimation, leading to poor algorithm performance. This paper
proposes a distributional reinforcement learning algorithm called DSAC-D
(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges
of estimating bias in value functions and obtaining multimodal policy
representations. A multimodal distributional policy iteration framework that
can converge to the optimal policy was established by introducing policy
entropy and value distribution function. A diffusion value network that can
accurately characterize the distribution of multi peaks was constructed by
generating a set of reward samples through reverse sampling using a diffusion
model. Based on this, a distributional reinforcement learning algorithm with
dual diffusion of the value network and the policy network was derived. MuJoCo
testing tasks demonstrate that the proposed algorithm not only learns
multimodal policy, but also achieves state-of-the-art (SOTA) performance in all
9 control tasks, with significant suppression of estimation bias and total
average return improvement of over 10\% compared to existing mainstream
algorithms. The results of real vehicle testing show that DSAC-D can accurately
characterize the multimodal distribution of different driving styles, and the
diffusion policy network can characterize multimodal trajectories.

</details>


### [93] [Surrogate Modeling via Factorization Machine and Ising Model with Enhanced Higher-Order Interaction Learning](https://arxiv.org/abs/2507.01389)
*Anbang Wang,Dunbo Cai,Yu Zhang,Yangqing Huang,Xiangyang Feng,Zhihong Zhang*

Key words: 代理模型, 因子分解机, 量子退火, 松弛变量, Ising表示

TL;DR: 本文提出了一种改进的代理模型，通过引入额外的松弛变量，将原本的两步优化过程整合为一步，从而提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 受现有利用量子退火优化因子分解机（FM）的代理模型方法启发，希望通过引入松弛变量进一步提升模型性能。

Method: 将松弛变量引入因子分解机及其相关的Ising表示，使模型能够处理高阶特征交互，并通过迭代更新松弛变量来实现这一目标。

Result: 实验结果显示，松弛变量的引入显著提升了模型的性能，尤其在药物组合效应预测任务中表现优异。

Conclusion: 该方法为构建高效代理模型提供了新思路，并有望利用量子计算的潜在优势。

Abstract: Recently, a surrogate model was proposed that employs a factorization machine
to approximate the underlying input-output mapping of the original system, with
quantum annealing used to optimize the resulting surrogate function. Inspired
by this approach, we propose an enhanced surrogate model that incorporates
additional slack variables into both the factorization machine and its
associated Ising representation thereby unifying what was by design a two-step
process into a single, integrated step. During the training phase, the slack
variables are iteratively updated, enabling the model to account for
higher-order feature interactions. We apply the proposed method to the task of
predicting drug combination effects. Experimental results indicate that the
introduction of slack variables leads to a notable improvement of performance.
Our algorithm offers a promising approach for building efficient surrogate
models that exploit potential quantum advantages.

</details>


### [94] [Decomposing Prediction Mechanisms for In-Context Recall](https://arxiv.org/abs/2507.01414)
*Sultan Daniels,Dylan Davis,Dhruv Gautam,Wentinn Liao,Gireeja Ranade,Anant Sahai*

Key words: Transformer, 上下文学习, 关联召回, 机制分离

TL;DR: 该论文通过玩具问题研究Transformer模型在上下文中学习的能力，发现其具备两种分离的机制：一种基于离散符号标签进行关联召回，另一种基于上下文进行预测。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索Transformer模型在连续和离散任务组合中的学习能力，特别是在上下文学习中如何实现状态回忆和预测。

Method: 预训练Transformer模型于玩具问题的样本轨迹，分析其训练动态和机制。

Result: 模型表现出两种分离的机制，具有不同的学习动态，且在非分布实验中验证了这一现象。

Conclusion: Transformer模型在多机制任务中展现出分离的学习动态，这一现象在更复杂的任务中也得到验证。

Abstract: We introduce a new family of toy problems that combine features of
linear-regression-style continuous in-context learning (ICL) with discrete
associative recall. We pretrain transformer models on sample traces from this
toy, specifically symbolically-labeled interleaved state observations from
randomly drawn linear deterministic dynamical systems. We study if the
transformer models can recall the state of a sequence previously seen in its
context when prompted to do so with the corresponding in-context label. Taking
a closer look at this task, it becomes clear that the model must perform two
functions: (1) identify which system's state should be recalled and apply that
system to its last seen state, and (2) continuing to apply the correct system
to predict the subsequent states. Training dynamics reveal that the first
capability emerges well into a model's training. Surprisingly, the second
capability, of continuing the prediction of a resumed sequence, develops much
earlier.
  Via out-of-distribution experiments, and a mechanistic analysis on model
weights via edge pruning, we find that next-token prediction for this toy
problem involves at least two separate mechanisms. One mechanism uses the
discrete symbolic labels to do the associative recall required to predict the
start of a resumption of a previously seen sequence. The second mechanism,
which is largely agnostic to the discrete symbolic labels, performs a
"Bayesian-style" prediction based on the previous token and the context. These
two mechanisms have different learning dynamics.
  To confirm that this multi-mechanism (manifesting as separate phase
transitions) phenomenon is not just an artifact of our toy setting, we used
OLMo training checkpoints on an ICL translation task to see a similar
phenomenon: a decisive gap in the emergence of first-task-token performance vs
second-task-token performance.

</details>


### [95] [Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling](https://arxiv.org/abs/2507.01679)
*Zeyu Huang,Tianhao Cheng,Zihan Qiu,Zili Wang,Yinghui Xu,Edoardo M. Ponti,Ivan Titov*

Key words: 监督微调, 强化微调, Prefix-RFT, 数学推理, 后训练

TL;DR: 本文提出了一种结合监督微调（SFT）和强化微调（RFT）的混合方法Prefix-RFT，实验证明其在数学推理任务中优于单独使用SFT或RFT，且易于在现有框架中实现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的SFT和RFT方法各有优缺点，SFT可能导致行为克隆的问题，而RFT对初始策略敏感且可能学习到意外行为。因此，需要一种统一的方法来整合两者的优势。

Method: 提出Prefix-RFT，一种结合SFT和RFT的混合方法，通过数学推理任务验证其有效性。

Result: Prefix-RFT在性能上优于单独的SFT和RFT，且对演示数据的质量和数量变化具有鲁棒性。

Conclusion: Prefix-RFT为LLM后训练提供了一个新视角，统一整合演示和探索的方法可能是未来研究的有前景方向。

Abstract: Existing post-training techniques for large language models are broadly
categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning
(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking
demonstration data but can lead to problematic generalization as a form of
behavior cloning. Conversely, RFT can significantly enhance a model's
performance but is prone to learn unexpected behaviors, and its performance is
highly sensitive to the initial policy. In this paper, we propose a unified
view of these methods and introduce Prefix-RFT, a hybrid approach that
synergizes learning from both demonstration and exploration. Using mathematical
reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is
both simple and effective. It not only surpasses the performance of standalone
SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key
advantage is its seamless integration into existing open-source frameworks,
requiring only minimal modifications to the standard RFT pipeline. Our analysis
highlights the complementary nature of SFT and RFT, and validates that
Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore,
ablation studies confirm the method's robustness to variations in the quality
and quantity of demonstration data. We hope this work offers a new perspective
on LLM post-training, suggesting that a unified paradigm that judiciously
integrates demonstration and exploration could be a promising direction for
future research.

</details>


### [96] [Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs](https://arxiv.org/abs/2507.01457)
*Federico Nicolas Peccia,Frederik Haxel,Oliver Bringmann*

Key words: RISC-V, RVV, TVM, AI加速, 自动调优

TL;DR: 本文提出了一种基于TVM编译器的工作流，用于高效地将AI任务映射到RISC-V的向量单元上，通过集成RVV扩展和自动调优框架，显著提升了执行效率和代码内存占用。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: RISC-V的向量扩展（RVV）在加速AI任务方面表现出潜力，但缺乏高效的编程工具。现有方法依赖于编译器的自动向量化或手动优化库，效率有限。

Method: 作者将RVV扩展集成到TVM的MetaSchedule框架中，并通过FPGA实现多种RISC-V SoC，自动调优多种AI任务。

Result: 实验表明，方案相较于GCC的自动向量化性能提升46%，比muRISCV-NN快29%，代码内存占用更小，且在商用RISC-V SoC上比LLVM快35%。

Conclusion: 该方案为RISC-V社区提供了一种高效、灵活的AI任务加速方法，并开源以支持扩展。

Abstract: RISC-V provides a flexible and scalable platform for applications ranging
from embedded devices to high-performance computing clusters. Particularly, its
RISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI
workloads. But writing software that efficiently utilizes the vector units of
RISC-V CPUs without expert knowledge requires the programmer to rely on the
autovectorization features of compilers or hand-crafted libraries like
muRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing
the integration with the RISC-V RVV extension, thus heavily limiting the
efficient deployment of complex AI workloads. In this paper, we present a
workflow based on the TVM compiler to efficiently map AI workloads onto RISC-V
vector units. Instead of relying on hand-crafted libraries, we integrated the
RVV extension into TVM's MetaSchedule framework, a probabilistic program
framework for tensor operation tuning. We implemented different RISC-V SoCs on
an FPGA and tuned a wide range of AI workloads on them. We found that our
proposal shows a mean improvement of 46% in execution latency when compared
against the autovectorization feature of GCC, and 29% against muRISCV-NN.
Moreover, the binary resulting from our proposal has a smaller code memory
footprint, making it more suitable for embedded devices. Finally, we also
evaluated our solution on a commercially available RISC-V SoC implementing the
RVV 1.0 Vector Extension and found our solution is able to find mappings that
are 35% faster on average than the ones proposed by LLVM. We open-sourced our
proposal for the community to expand it to target other RISC-V extensions.

</details>


### [97] [Cross-platform Smartphone Positioning at Museums](https://arxiv.org/abs/2507.01469)
*Alessio Ferrato,Fabio Gasparetti,Carla Limongelli,Stefano Mastandrea,Giuseppe Sansonetti,Joaquín Torres-Sospedra*

Key words: 室内定位系统,文化遗产,RSS数据集,蓝牙低能耗,WiFi

TL;DR: 该论文讨论了室内定位系统（IPS）在文化遗产机构中的应用，提出了基于BLE和WiFi的RSS方法，并发布了一个博物馆环境的新数据集BAR，同时提出了改进的分类基线方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 文化遗产机构中IPS的实施面临环境和技术限制，缺乏公开的RSS数据集阻碍了算法的开发。

Method: 使用Android和iOS平台在13个博物馆房间收集了90件艺术品前的RSS数据，提出了基于邻近性和k-NN算法的分类方法。

Result: 提供了BAR数据集和分类基线，讨论了结果并提出了未来研究方向。

Conclusion: BAR数据集填补了博物馆环境RSS数据的空白，为算法开发提供了支持。

Abstract: Indoor Positioning Systems (IPSs) hold significant potential for enhancing
visitor experiences in cultural heritage institutions. By enabling personalized
navigation, efficient artifact organization, and better interaction with
exhibits, IPSs can transform the modalities of how individuals engage with
museums, galleries and libraries. However, these institutions face several
challenges in implementing IPSs, including environmental constraints, technical
limits, and limited experimentation. In other contexts, Received Signal
Strength (RSS)-based approaches using Bluetooth Low Energy (BLE) and WiFi have
emerged as preferred solutions due to their non-invasive nature and minimal
infrastructure requirements. Nevertheless, the lack of publicly available RSS
datasets that specifically reflect museum environments presents a substantial
barrier to developing and evaluating positioning algorithms designed for the
intricate spatial characteristics typical of cultural heritage sites. To
address this limitation, we present BAR, a novel RSS dataset collected in front
of 90 artworks across 13 museum rooms using two different platforms, i.e.,
Android and iOS. Additionally, we provide an advanced position classification
baseline taking advantage of a proximity-based method and $k$-NN algorithms. In
our analysis, we discuss the results and offer suggestions for potential
research directions.

</details>


### [98] [Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](https://arxiv.org/abs/2507.01752)
*Ismail Labiad,Mathurin Videau,Matthieu Kowalski,Marc Schoenauer,Alessandro Leite,Julia Kempe,Olivier Teytaud*

Key words: 黑盒优化、大型语言模型、信息瓶颈、隐私保护、数据中毒攻击

TL;DR: BBoxER是一种用于大型语言模型（LLM）后训练的进化黑盒优化方法，通过隐式压缩训练数据引入信息瓶颈，解决了传统梯度优化中的隐私和安全问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统梯度优化依赖于大量标注数据，可能导致隐私泄露、数据中毒攻击和过拟合等问题，而黑盒优化方法在数据受限或对抗性风险高的场景中更具潜力。

Method: 提出BBoxER方法，利用信息流的可追踪性，为泛化性、差分隐私、数据中毒攻击易感性和提取攻击鲁棒性提供了理论保证，并作为一种轻量级模块化增强部署于预训练LLM之上。

Result: 实验证明，BBoxER在少数迭代中即可提升LLM性能，并在推理数据集上表现出良好的泛化能力。

Conclusion: BBoxER是梯度优化的高效补充，适用于隐私敏感或数据受限的环境，且具备非平凡的泛化保证。

Abstract: Gradient-based optimization is the workhorse of deep learning, offering
efficient and scalable training via backpropagation. However, its reliance on
large volumes of labeled data raises privacy and security concerns such as
susceptibility to data poisoning attacks and the risk of overfitting. In
contrast, black box optimization methods, which treat the model as an opaque
function, relying solely on function evaluations to guide optimization, offer a
promising alternative in scenarios where data access is restricted, adversarial
risks are high, or overfitting is a concern. However, black box methods also
pose significant challenges, including poor scalability to high-dimensional
parameter spaces, as prevalent in large language models (LLMs), and high
computational costs due to reliance on numerous model evaluations. This paper
introduces BBoxER, an evolutionary black-box method for LLM post-training that
induces an information bottleneck via implicit compression of the training
data. Leveraging the tractability of information flow, we provide strong
theoretical bounds on generalization, differential privacy, susceptibility to
data poisoning attacks, and robustness to extraction attacks. BBoxER operates
on top of pre-trained LLMs, offering a lightweight and modular enhancement
suitable for deployment in restricted or privacy-sensitive environments, in
addition to non-vacuous generalization guarantees. In experiments with LLMs, we
demonstrate empirically that Retrofitting methods are able to learn, showing
how a few iterations of BBoxER improve performance and generalize well on a
benchmark of reasoning datasets. This positions BBoxER as an attractive add-on
on top of gradient-based optimization.

</details>


### [99] [Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals](https://arxiv.org/abs/2507.01470)
*Yannick Molinghen,Tom Lenaerts*

Key words: 强化学习, 子目标, 零激励动态, 任务难度, 深度学习

TL;DR: 对强化学习中奖励频率作为任务难度衡量标准的假设提出质疑，发现无激励动态问题影响了现有策略学习方法的有效性，并提出需要新的机制来推断潜在任务结构。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 重新审视奖励频率作为任务难度衡量标准的常见假设，发现关键子目标未直接产生奖励时，现有方法存在局限性。

Method: 识别并形式化了一种结构性问题（零激励动态），并验证当前深度子目标算法在此类问题中的表现。

Result: 当前算法无法有效利用零激励动态，且学习性能对子目标完成与奖励之间的时间接近性高度敏感。

Conclusion: 现有方法存在根本性限制，需要开发不依赖即时激励的任务结构推断机制。

Abstract: This work re-examines the commonly held assumption that the frequency of
rewards is a reliable measure of task difficulty in reinforcement learning. We
identify and formalize a structural challenge that undermines the effectiveness
of current policy learning methods: when essential subgoals do not directly
yield rewards. We characterize such settings as exhibiting zero-incentive
dynamics, where transitions critical to success remain unrewarded. We show that
state-of-the-art deep subgoal-based algorithms fail to leverage these dynamics
and that learning performance is highly sensitive to the temporal proximity
between subgoal completion and eventual reward. These findings reveal a
fundamental limitation in current approaches and point to the need for
mechanisms that can infer latent task structure without relying on immediate
incentives.

</details>


### [100] [LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs](https://arxiv.org/abs/2507.01806)
*Reza Arabpour,Haitz Sáez de Ocáriz Borde,Anastasis Kratsios*

Key words: 低秩适配器, LoRA, CPU微调, 参数效率, Mistral-7B

TL;DR: 提出了一种适用于有限计算资源（如笔记本电脑CPU）的低秩适配器（LoRA）微调方法，通过预训练适配器库生成新适配器，避免了GPU训练的需求。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决LoRA微调中依赖GPU训练的问题，为计算资源有限的用户提供实用方案。

Method: 利用预训练适配器库学习元操作符，直接组合生成新适配器，基于CPU实现轻量级微调。

Result: 生成的适配器性能虽不及GPU训练的版本，但优于基础模型，适合资源有限场景。

Conclusion: 该方法为资源受限用户提供了可行的LoRA微调替代方案，降低了硬件门槛。

Abstract: Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language
Models (LLMs) by enabling parameter-efficient updates. However, their
widespread adoption remains limited by the reliance on GPU-based training. In
this work, we propose a theoretically grounded approach to LoRA fine-tuning
designed specifically for users with limited computational resources,
particularly those restricted to standard laptop CPUs. Our method learns a
meta-operator that maps any input dataset, represented as a probability
distribution, to a set of LoRA weights by leveraging a large bank of
pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of
performing new gradient-based updates, our pipeline constructs adapters via
lightweight combinations of existing LoRAs directly on CPU. While the resulting
adapters do not match the performance of GPU-trained counterparts, they
consistently outperform the base Mistral model on downstream tasks, offering a
practical and accessible alternative to traditional GPU-based fine-tuning.

</details>


### [101] [Loss Functions in Diffusion Models: A Comparative Study](https://arxiv.org/abs/2507.01516)
*Dibyanshu Kumar,Philipp Vaeth,Magda Gregorová*

Key words: 扩散模型, 损失函数, 目标函数, 变分下界, 生成模型

TL;DR: 本文系统研究了扩散模型中的目标函数和损失函数，统一了它们之间的关系，并实证分析了不同性能差异的原因及其对模型目标的影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索扩散模型中不同目标函数和损失函数的联系与差异，为进一步高效且目标导向的模型设计提供理论基础。

Method: 通过理论分析和实证研究，统一不同损失函数在变分下界目标框架下的关系，并比较它们在不同条件下的性能差异。

Result: 研究发现不同损失函数的性能差异及其影响因素，并评估了不同目标函数对模型生成高质量样本或准确估计似然的能力的影响。

Conclusion: 该研究为扩散模型中损失函数的选择提供了统一的理论框架，有助于未来更高效和有针对性的模型设计。

Abstract: Diffusion models have emerged as powerful generative models, inspiring
extensive research into their underlying mechanisms. One of the key questions
in this area is the loss functions these models shall train with. Multiple
formulations have been introduced in the literature over the past several years
with some links and some critical differences stemming from various initial
considerations. In this paper, we explore the different target objectives and
corresponding loss functions in detail. We present a systematic overview of
their relationships, unifying them under the framework of the variational lower
bound objective. We complement this theoretical analysis with an empirical
study providing insights into the conditions under which these objectives
diverge in performance and the underlying factors contributing to such
deviations. Additionally, we evaluate how the choice of objective impacts the
model ability to achieve specific goals, such as generating high-quality
samples or accurately estimating likelihoods. This study offers a unified
understanding of loss functions in diffusion models, contributing to more
efficient and goal-oriented model designs in future research.

</details>


### [102] [Test-Time Scaling with Reflective Generative Model](https://arxiv.org/abs/2507.01951)
*Zixiao Wang,Yuxin Wang,Xiaorui Wang,Mengting Xing,Jie Gao,Jianjun Xu,Guangcan Liu,Chenhui Jin,Zhuo Wang,Shengzhuo Zhang,Hongtao Xie*

Key words: MetaStone-S1, 反射式生成模型, 自监督过程奖励模型, 测试时间扩展

TL;DR: MetaStone-S1 是一种反射式生成模型，通过自监督过程奖励模型（SPRM）实现了与 OpenAI o3 相当的性能，同时减少了参数规模，适合高效推理。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在通过统一接口结合策略模型和过程奖励模型，减少参数规模并提升推理效率。

Method: 使用共享主干网络和任务特定头进行下一标记预测和过程评分，无需额外过程标注。

Result: 实验表明，32B 参数的 MetaStone-S1 性能与 OpenAI-o3-mini 系列相当。

Conclusion: SPRM 有效减少了参数规模，并支持基于可控思考长度的测试时间扩展。

Abstract: We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3's performance via the self-supervised process reward model
(SPRM). Through sharing the backbone network and using task-specific heads for
next token prediction and process scoring respectively, SPRM successfully
integrates the policy model and process reward model(PRM) into a unified
interface without extra process annotation, reducing over 99% PRM parameters
for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable
for test time scaling (TTS), and we provide three reasoning effort modes (low,
medium, and high), based on the controllable thinking length. Moreover, we
empirically establish a scaling law that reveals the relationship between total
thinking computation and TTS performance. Experiments demonstrate that our
MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with
only 32B parameter size. To support the research community, we have
open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.

</details>


### [103] [Chargax: A JAX Accelerated EV Charging Simulator](https://arxiv.org/abs/2507.01522)
*Koen Ponse,Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Key words: 强化学习, JAX, 电动车充电站, 仿真环境, 可持续能源

TL;DR: 论文介绍了一个基于JAX的电动车充电站仿真环境Chargax，显著提升了强化学习训练效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决电网系统拥堵问题，提升运营效率，传统强化学习方法因高样本复杂度和昂贵仿真而缓慢。

Method: 利用JAX开发Chargax环境，支持电动车充电站的现实仿真，模块化架构适配多样化配置。

Result: 计算性能提升100-1000倍，并在真实数据场景中验证了强化学习代理的有效性。

Conclusion: Chargax为可持续能源领域的强化学习提供了高效且灵活的仿真工具。

Abstract: Deep Reinforcement Learning can play a key role in addressing sustainable
energy challenges. For instance, many grid systems are heavily congested,
highlighting the urgent need to enhance operational efficiency. However,
reinforcement learning approaches have traditionally been slow due to the high
sample complexity and expensive simulation requirements. While recent works
have effectively used GPUs to accelerate data generation by converting
environments to JAX, these works have largely focussed on classical toy
problems. This paper introduces Chargax, a JAX-based environment for realistic
simulation of electric vehicle charging stations designed for accelerated
training of RL agents. We validate our environment in a variety of scenarios
based on real data, comparing reinforcement learning agents against baselines.
Chargax delivers substantial computational performance improvements of over
100x-1000x over existing environments. Additionally, Chargax' modular
architecture enables the representation of diverse real-world charging station
configurations.

</details>


### [104] [MARVIS: Modality Adaptive Reasoning over VISualizations](https://arxiv.org/abs/2507.01544)
*Benjamin Feuer,Lennart Purucker,Oussama Elachqar,Chinmay Hegde*

Key words: MARVIS, 多模态, 视觉语言模型, 无需训练, 隐私保护

TL;DR: MARVIS是一种无需训练的通用方法，通过将潜在嵌入空间转换为视觉表示，利用视觉语言模型处理多模态数据，表现出色且无需暴露个人隐私信息。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决专业模型灵活性不足和基础模型性能不佳的问题，尤其是在非传统模态和长尾领域。

Method: 将潜在嵌入空间转换为视觉表示，利用视觉语言模型的空间和细粒度推理能力。

Result: 在视觉、音频、生物和表格领域表现优异，平均比Gemini高16%，接近专业模型性能。

Conclusion: MARVIS展示了通用模型在多模态任务中的潜力，且无需特定领域训练或隐私信息。

Abstract: Scientific applications of machine learning often rely on small, specialized
models tuned to particular domains. Such models often achieve excellent
performance, but lack flexibility. Foundation models offer versatility, but
typically underperform specialized approaches, especially on non-traditional
modalities and long-tail domains. We propose MARVIS (Modality Adaptive
Reasoning over VISualizations), a training-free method that enables even small
vision-language models to predict any data modality with high accuracy. MARVIS
transforms latent embedding spaces into visual representations and then
leverages the spatial and fine-grained reasoning skills of VLMs to successfully
interpret and utilize them. MARVIS achieves competitive performance on vision,
audio, biological, and tabular domains using a single 3B parameter model,
achieving results that beat Gemini by 16\% on average and approach specialized
methods, without exposing personally identifiable information (P.I.I.) or
requiring any domain-specific training. We open source our code and datasets at
https://github.com/penfever/marvis

</details>


### [105] [How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks](https://arxiv.org/abs/2507.01559)
*Lapo Frati,Neil Traft,Jeff Clune,Nick Cheney*

Key words: 持续学习, 权重重采样, 迁移学习, zapping, 优化器

TL;DR: 这篇论文研究了在持续学习和少样本迁移学习场景中，神经网络最后一层权重重采样（“zapping”）对模型学习和遗忘模式的影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了更好地理解zapping方法在持续学习和迁移学习中的效果及其潜在机制。

Method: 通过实验分析卷积神经网络在手写字符和自然图像任务中的学习和遗忘模式，对比zapping和不同优化器的效果。

Result: 实验表明，zapping能帮助模型更快适应新领域，优化器的选择也会影响任务间的学习与遗忘动态。

Conclusion: zapping和优化器的选择对持续学习和任务迁移中的模型行为有显著影响。

Abstract: Recent work in continual learning has highlighted the beneficial effect of
resampling weights in the last layer of a neural network (``zapping"). Although
empirical results demonstrate the effectiveness of this approach, the
underlying mechanisms that drive these improvements remain unclear. In this
work, we investigate in detail the pattern of learning and forgetting that take
place inside a convolutional neural network when trained in challenging
settings such as continual learning and few-shot transfer learning, with
handwritten characters and natural images. Our experiments show that models
that have undergone zapping during training more quickly recover from the shock
of transferring to a new domain. Furthermore, to better observe the effect of
continual learning in a multi-task setting we measure how each individual task
is affected. This shows that, not only zapping, but the choice of optimizer can
also deeply affect the dynamics of learning and forgetting, causing complex
patterns of synergy/interference between tasks to emerge when the model learns
sequentially at transfer time.

</details>


### [106] [A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning](https://arxiv.org/abs/2507.01581)
*Masood Jan,Wafa Njima,Xun Zhang*

Key words: 室内定位, 联邦学习, 深度学习, 隐私保护, IoT

TL;DR: 该论文提出了一种基于联邦学习（FL）的动态室内定位方法，解决了传统方法中的隐私、带宽和服务器可靠性问题，实验表明FL性能接近集中式模型（CL）。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的室内定位技术存在显著误差和隐私问题，机器学习虽提供解决方案但需要集中数据聚合，带来隐私、带宽和服务器可靠性挑战。

Method: 采用基于深度神经网络（DNN）的联邦学习（FL）方法进行动态室内定位。

Result: 实验结果显示FL在保持数据隐私、带宽效率和服务器可靠性的同时，性能接近集中式模型（CL）。

Conclusion: 提出的FL方法为隐私增强的室内定位提供了可行方案，推动了安全高效的室内定位系统的发展。

Abstract: Location information serves as the fundamental element for numerous Internet
of Things (IoT) applications. Traditional indoor localization techniques often
produce significant errors and raise privacy concerns due to centralized data
collection. In response, Machine Learning (ML) techniques offer promising
solutions by capturing indoor environment variations. However, they typically
require central data aggregation, leading to privacy, bandwidth, and server
reliability issues. To overcome these challenges, in this paper, we propose a
Federated Learning (FL)-based approach for dynamic indoor localization using a
Deep Neural Network (DNN) model. Experimental results show that FL has the
nearby performance to Centralized Model (CL) while keeping the data privacy,
bandwidth efficiency and server reliability. This research demonstrates that
our proposed FL approach provides a viable solution for privacy-enhanced indoor
localization, paving the way for advancements in secure and efficient indoor
localization systems.

</details>


### [107] [Analysis of Muon's Convergence and Critical Batch Size](https://arxiv.org/abs/2507.01598)
*Naoki Sato,Hiroki Naganuma,Hideaki Iiduka*

Key words: 优化器, Muon, 权重衰减, 收敛性, 批量大小

TL;DR: 论文提出了一种新型优化器Muon，理论分析了其收敛性及其在不同变体（如带/不带动量、带/不带权重衰减）下的表现，并验证了权重衰减对参数和梯度范数的紧致性影响，同时确定了最小化计算成本的关键批量大小。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究Muon优化器在神经网络参数矩阵结构中的理论表现，以填补现有优化器理论分析的不足，并提升优化效率。

Method: 通过理论分析，提出了Muon优化器的四种变体，并证明了其收敛性；量化了权重衰减与学习率的关系，推导了最小化计算成本的关键批量大小。

Result: 权重衰减可以严格提升参数和梯度范数的紧致性；明确了权重衰减系数与学习率的依赖关系；实验验证了理论结果。

Conclusion: Muon优化器在理论和实验中均表现优异，权重衰减对其性能有显著改进，关键批量大小的推导为实际应用提供了指导。

Abstract: This paper presents a theoretical analysis of Muon, a new optimizer that
leverages the inherent matrix structure of neural network parameters. We
provide convergence proofs for four practical variants of Muon: with and
without Nesterov momentum, and with and without weight decay. We then show that
adding weight decay leads to strictly tighter bounds on both the parameter and
gradient norms, and we clarify the relationship between the weight decay
coefficient and the learning rate. Finally, we derive Muon's critical batch
size minimizing the stochastic first-order oracle (SFO) complexity, which is
the stochastic computational cost, and validate our theoretical findings with
experiments.

</details>


### [108] [Kernel Recursive Least Squares Dictionary Learning Algorithm](https://arxiv.org/abs/2507.01636)
*Ghasem Alipoor,Karl Skretting*

Key words: 在线字典学习、核方法、稀疏表示、递归最小二乘法、高效计算

TL;DR: 提出一种高效的在线字典学习算法，用于基于核的稀疏表示，通过递归最小二乘法更新字典，实验表明其优于现有方法且计算高效。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对高效在线学习核字典的需求，解决传统方法计算复杂度高的问题。

Method: 采用递归最小二乘法（RLS）递归更新字典，支持单样本或小批量数据，保持低计算复杂度。

Result: 在四个不同领域的数据集上实验，性能优于现有在线方法，且分类准确率接近批量训练模型。

Conclusion: 该算法高效且性能接近批量训练，适用于在线学习场景。

Abstract: We propose an efficient online dictionary learning algorithm for kernel-based
sparse representations. In this framework, input signals are nonlinearly mapped
to a high-dimensional feature space and represented sparsely using a virtual
dictionary. At each step, the dictionary is updated recursively using a novel
algorithm based on the recursive least squares (RLS) method. This update
mechanism works with single samples or mini-batches and maintains low
computational complexity. Experiments on four datasets across different domains
show that our method not only outperforms existing online kernel dictionary
learning approaches but also achieves classification accuracy close to that of
batch-trained models, while remaining significantly more efficient.

</details>


### [109] [Dance Dance ConvLSTM](https://arxiv.org/abs/2507.01644)
*Miguel O'Malley*

Key words: Dance Dance Revolution, 自动生成谱面, ConvLSTM, CNN-LSTM

TL;DR: 提出了基于ConvLSTM的新方法DDCL，用于自动生成《Dance Dance Revolution》游戏谱面，相比之前的DDC方法显著提高了生成准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 改进2017年提出的基于CNN-LSTM架构的DDC算法，提升《Dance Dance Revolution》游戏谱面自动生成的准确性。

Method: 使用了基于ConvLSTM架构的模型。

Result: 显著提高了谱面生成的准确性。

Conclusion: DDCL方法在谱面自动生成任务上表现优于DDC。

Abstract: \textit{Dance Dance Revolution} is a rhythm game consisting of songs and
accompanying choreography, referred to as charts. Players press arrows on a
device referred to as a dance pad in time with steps determined by the song's
chart. In 2017, the authors of Dance Dance Convolution (DDC) developed an
algorithm for the automatic generation of \textit{Dance Dance Revolution}
charts, utilizing a CNN-LSTM architecture. We introduce Dance Dance ConvLSTM
(DDCL), a new method for the automatic generation of DDR charts using a
ConvLSTM based model, which improves upon the DDC methodology and substantially
increases the accuracy of chart generation.

</details>


### [110] [GradMetaNet: An Equivariant Architecture for Learning on Gradients](https://arxiv.org/abs/2507.01649)
*Yoav Gelberg,Yam Eitan,Aviv Navon,Aviv Shamsian,Theo,Putterman,Michael Bronstein,Haggai Maron*

Key words: 梯度处理, 对称性, GradMetaNet, 神经网络优化

TL;DR: 论文提出了一种名为GradMetaNet的新架构，用于处理神经网络梯度，具有对称性保护、多数据点处理的优点，并展示了其在多种任务上的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 梯度在神经网络优化、编辑和分析中具有重要价值。现有方法在处理梯度时缺乏专门设计的架构，限制了其应用范围。

Method: 提出基于对称性保护、多数据点处理和梯度高效表示的GradMetaNet架构，由简单对称块构建。

Result: 证明了GradMetaNet的普遍性，并在优化、编辑和损失曲面估计等任务中验证了其效果。

Conclusion: GradMetaNet是一种高效且通用的梯度处理架构，适用于多种任务。

Abstract: Gradients of neural networks encode valuable information for optimization,
editing, and analysis of models. Therefore, practitioners often treat gradients
as inputs to task-specific algorithms, e.g. for pruning or optimization. Recent
works explore learning algorithms that operate directly on gradients but use
architectures that are not specifically designed for gradient processing,
limiting their applicability. In this paper, we present a principled approach
for designing architectures that process gradients. Our approach is guided by
three principles: (1) equivariant design that preserves neuron permutation
symmetries, (2) processing sets of gradients across multiple data points to
capture curvature information, and (3) efficient gradient representation
through rank-1 decomposition. Based on these principles, we introduce
GradMetaNet, a novel architecture for learning on gradients, constructed from
simple equivariant blocks. We prove universality results for GradMetaNet, and
show that previous approaches cannot approximate natural gradient-based
functions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness
on a diverse set of gradient-based tasks on MLPs and transformers, such as
learned optimization, INR editing, and estimating loss landscape curvature.

</details>


### [111] [AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training](https://arxiv.org/abs/2507.01663)
*Zhenyu Han,Ansheng You,Haibo Wang,Kui Luo,Guang Yang,Wenqi Shi,Menglong Chen,Sicheng Zhang,Zeshun Lan,Chunshi Deng,Huazhong Ji,Wenjie Liu,Yu Huang,Yixiang Zhang,Chenyi Pan,Jing Wang,Xin Huang,Chunsheng Li,Jianping Wu*

Key words: 强化学习, 后训练, 异步流式, 资源优化

TL;DR: 论文提出了AsyncFlow，一种异步流式强化学习框架，用于高效的后训练。解决了传统框架的可扩展性问题、数据流复杂性和资源闲置问题，并通过解耦设计支持自定义引擎。实验显示性能提升1.59倍。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统RL框架在后训练阶段面临可扩展性瓶颈、数据流复杂性和资源闲置问题，且与LLM引擎紧耦合。

Method: 提出AsyncFlow，采用分布式数据存储和传输模块、异步工作流，并解耦核心能力以支持自定义引擎。

Result: 实验表明，AsyncFlow的平均吞吐量提升了1.59倍。

Conclusion: AsyncFlow为下一代RL训练系统设计提供了可行的见解。

Abstract: Reinforcement learning (RL) has become a pivotal technology in the
post-training phase of large language models (LLMs). Traditional task-colocated
RL frameworks suffer from significant scalability bottlenecks, while
task-separated RL frameworks face challenges in complex dataflows and the
corresponding resource idling and workload imbalance. Moreover, most existing
frameworks are tightly coupled with LLM training or inference engines, making
it difficult to support custom-designed engines. To address these challenges,
we propose AsyncFlow, an asynchronous streaming RL framework for efficient
post-training. Specifically, we introduce a distributed data storage and
transfer module that provides a unified data management and fine-grained
scheduling capability in a fully streamed manner. This architecture inherently
facilitates automated pipeline overlapping among RL tasks and dynamic load
balancing. Moreover, we propose a producer-consumer-based asynchronous workflow
engineered to minimize computational idleness by strategically deferring
parameter update process within staleness thresholds. Finally, the core
capability of AsynFlow is architecturally decoupled from underlying training
and inference engines and encapsulated by service-oriented user interfaces,
offering a modular and customizable user experience. Extensive experiments
demonstrate an average of 1.59 throughput improvement compared with
state-of-the-art baseline. The presented architecture in this work provides
actionable insights for next-generation RL training system designs.

</details>


### [112] [GPT, But Backwards: Exactly Inverting Language Model Outputs](https://arxiv.org/abs/2507.01693)
*Adrians Skapars,Edoardo Manino,Youcheng Sun,Lucas C. Cordeiro*

Key words: 大型语言模型,输入重构,SODA算法,优化问题,隐私保护

TL;DR: 本文提出SODA算法，用于从大型语言模型输出中精确重构输入，解决现有审计技术的补足问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有技术未能解决如何从模型输出中逆向重构输入的问题，SODA填补了这一空白，支持事后分析和检测虚假报告。

Method: 将输入重构问题形式化为离散优化问题，提出SODA算法，基于连续松弛空间，结合周期性重启和参数衰减。

Result: 在33M至3B参数的模型上，SODA显著优于现有方法，成功恢复79.5%的短分布外输入，但对长序列（15+词）效果有限。

Conclusion: 标准部署实践可抵御SODA的潜在恶意使用，但隐私信息在长序列中仍可能被泄露。

Abstract: While existing auditing techniques attempt to identify potential unwanted
behaviours in large language models (LLMs), we address the complementary
forensic problem of reconstructing the exact input that led to an existing LLM
output - enabling post-incident analysis and potentially the detection of fake
output reports. We formalize exact input reconstruction as a discrete
optimisation problem with a unique global minimum and introduce SODA, an
efficient gradient-based algorithm that operates on a continuous relaxation of
the input search space with periodic restarts and parameter decay. Through
comprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we
demonstrate that SODA significantly outperforms existing approaches. We succeed
in fully recovering 79.5% of shorter out-of-distribution inputs from next-token
logits, without a single false positive, but struggle to extract private
information from the outputs of longer (15+ token) input sequences. This
suggests that standard deployment practices may currently provide adequate
protection against malicious use of our method. Our code is available at
https://doi.org/10.5281/zenodo.15539879.

</details>


### [113] [PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution](https://arxiv.org/abs/2507.01695)
*Omkar Shende,Gayathri Ananthanarayanan,Marcello Traiola*

Key words: 深度神经网络、计算效率、遗传算法、动态模型选择、PERTINENCE

TL;DR: 该论文提出了PERTINENCE方法，通过动态选择最适合的预训练模型来处理输入，以平衡计算效率和准确率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大型深度神经网络（DNNs）虽然准确率高，但资源消耗大。因此需要在不显著降低准确率的前提下减少对大型模型的依赖。

Method: 采用遗传算法训练ML-based输入分配器，动态根据输入复杂度选择合适的模型。

Result: 在CIFAR-10、CIFAR-100和TinyImageNet数据集上验证，PERTINENCE在保持或提升准确率的同时减少了36%的计算操作。

Conclusion: PERTINENCE为平衡准确率和计算效率提供了一种有效的动态模型选择方法。

Abstract: Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable
ability to model complex patterns across various domains such as computer
vision, speech recognition, robotics, etc. While large DNN models are often
more accurate than simpler, lightweight models, they are also resource- and
energy-hungry. Hence, it is imperative to design methods to reduce reliance on
such large models without significant degradation in output accuracy. The high
computational cost of these models is often necessary only for a reduced set of
challenging inputs, while lighter models can handle most simple ones. Thus,
carefully combining properties of existing DNN models in a dynamic, input-based
way opens opportunities to improve efficiency without impacting accuracy.
  In this work, we introduce PERTINENCE, a novel online method designed to
analyze the complexity of input features and dynamically select the most
suitable model from a pre-trained set to process a given input effectively. To
achieve this, we employ a genetic algorithm to explore the training space of an
ML-based input dispatcher, enabling convergence towards the Pareto front in the
solution space that balances overall accuracy and computational efficiency.
  We showcase our approach on state-of-the-art Convolutional Neural Networks
(CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers
(ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCE's
ability to provide alternative solutions to existing state-of-the-art models in
terms of trade-offs between accuracy and number of operations. By
opportunistically selecting among models trained for the same task, PERTINENCE
achieves better or comparable accuracy with up to 36% fewer operations.

</details>


### [114] [Variational Graph Convolutional Neural Networks](https://arxiv.org/abs/2507.01699)
*Illia Oleksiienko,Juho Kanniainen,Alexandros Iosifidis*

Key words: 图卷积网络, 不确定性估计, 变分神经网络, 解释性, 社会交易分析, 动作识别

TL;DR: 提出了一种基于变分神经网络的图卷积网络方法，用于估计模型不确定性，提高模型解释性和准确性，并在社会交易分析和人体动作识别任务中验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 估计模型不确定性可以提升图卷积网络的解释性和准确性，同时为关键应用提供模型结果的验证手段。

Method: 提出了空间和时空图卷积网络的变分神经网络版本，估计模型输出和逐层注意力的不确定性。

Result: 在社会交易分析和骨架动作识别任务中，模型准确性和不确定性估计均得到提升。

Conclusion: 变分图卷积网络能够有效估计不确定性，提升模型性能和解释性。

Abstract: Estimation of model uncertainty can help improve the explainability of Graph
Convolutional Networks and the accuracy of the models at the same time.
Uncertainty can also be used in critical applications to verify the results of
the model by an expert or additional models. In this paper, we propose
Variational Neural Network versions of spatial and spatio-temporal Graph
Convolutional Networks. We estimate uncertainty in both outputs and layer-wise
attentions of the models, which has the potential for improving model
explainability. We showcase the benefits of these models in the social trading
analysis and the skeleton-based human action recognition tasks on the Finnish
board membership, NTU-60, NTU-120 and Kinetics datasets, where we show
improvement in model accuracy in addition to estimated model uncertainties.

</details>


### [115] [Relational Causal Discovery with Latent Confounders](https://arxiv.org/abs/2507.01700)
*Andrea Piras,Matteo Negro,Ragib Ahsan,David Arbour,Elena Zheleva*

Key words: 因果关系发现,关系数据,潜在混杂因素,FCI,RCD

TL;DR: 本文提出了一种名为RelFCI的因果关系发现算法，用于处理具有潜在混杂因素的关系数据，填补了现有方法的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的因果关系发现算法在处理关系数据和潜在混杂因素时存在局限性，需要一种更全面的方法。

Method: 基于FCI和RCD算法，提出了RelFCI算法，并定义了新的图形模型以支持关系数据中的因果关系发现。

Result: 实验证明RelFCI能有效识别具有潜在混杂因素的关系因果模型的正确结构。

Conclusion: RelFCI算法为关系数据中的因果发现提供了可靠且完整的解决方案。

Abstract: Estimating causal effects from real-world relational data can be challenging
when the underlying causal model and potential confounders are unknown. While
several causal discovery algorithms exist for learning causal models with
latent confounders from data, they assume that the data is independent and
identically distributed (i.i.d.) and are not well-suited for learning from
relational data. Similarly, existing relational causal discovery algorithms
assume causal sufficiency, which is unrealistic for many real-world datasets.
To address this gap, we propose RelFCI, a sound and complete causal discovery
algorithm for relational data with latent confounders. Our work builds upon the
Fast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms
and it defines new graphical models, necessary to support causal discovery in
relational domains. We also establish soundness and completeness guarantees for
relational d-separation with latent confounders. We present experimental
results demonstrating the effectiveness of RelFCI in identifying the correct
causal structure in relational causal models with latent confounders.

</details>


### [116] [B-PL-PINN: Stabilizing PINN Training with Bayesian Pseudo Labeling](https://arxiv.org/abs/2507.01714)
*Kevin Innerebner,Franz M. Rohrhofer,Bernhard C. Geiger*

Key words: 物理信息神经网络, 贝叶斯方法, 训练收敛, 后验方差, 前向问题

TL;DR: 提出了一种基于贝叶斯物理信息神经网络（PINN）的方法，替代原有的集成方法，通过评估后验方差提升信息传播效果，实验证明其在基准问题上表现更优。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统物理信息神经网络（PINN）在前向问题训练中存在收敛困难，导致信息传播受阻。Haitsiukevich和Ilin（2023）提出的集成方法虽有一定改进，但仍需更数学化的解决方案。

Method: 使用贝叶斯PINN替代原有集成方法，通过评估后验方差而非集成共识来扩展训练域，确保信息从初始条件传播至计算域内部。

Result: 实验表明，新方法在基准问题上表现优于集成方法，且与Adam和LBFGS优化的集成方法竞争性相当。

Conclusion: 基于贝叶斯框架的方法为PINN训练提供了更数学化和有效的解决方案，显著提升了信息传播效果。

Abstract: Training physics-informed neural networks (PINNs) for forward problems often
suffers from severe convergence issues, hindering the propagation of
information from regions where the desired solution is well-defined.
Haitsiukevich and Ilin (2023) proposed an ensemble approach that extends the
active training domain of each PINN based on i) ensemble consensus and ii)
vicinity to (pseudo-)labeled points, thus ensuring that the information from
the initial condition successfully propagates to the interior of the
computational domain.
  In this work, we suggest replacing the ensemble by a Bayesian PINN, and
consensus by an evaluation of the PINN's posterior variance. Our experiments
show that this mathematically principled approach outperforms the ensemble on a
set of benchmark problems and is competitive with PINN ensembles trained with
combinations of Adam and LBFGS.

</details>


### [117] [Revisiting Learning Rate Control](https://arxiv.org/abs/2507.01724)
*Micha Henheik,Theresa Eimer,Marius Lindauer*

Key words: 学习率控制, AutoML, 深度学习, 超参数优化, 元学习

TL;DR: 论文比较了学习率控制的多种方法，发现现有方法在特定任务中表现良好但缺乏普适性，呼吁算法选择方法的关注，并指出随着任务复杂度的增加，现有优化方法效果下降，建议关注更具相关性的测试任务和新方向如微调与元学习。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 学习率是深度学习中最重要的超参数之一，其控制是AutoML和深度学习研究的活跃领域，但目前缺乏普适性和可靠的方法。

Method: 比较了多保真超参数优化、固定超参数调度和无超参数学习等方法。

Result: 发现这些方法在特定任务中表现良好但普适性不足，且随着任务复杂度的增加，优化方法效果下降。

Conclusion: 呼吁关注算法选择方法，并建议聚焦更具相关性的测试任务及新方向如微调与元学习，以提升AutoML在深度学习中的影响力。

Abstract: The learning rate is one of the most important hyperparameters in deep
learning, and how to control it is an active area within both AutoML and deep
learning research. Approaches for learning rate control span from classic
optimization to online scheduling based on gradient statistics. This paper
compares paradigms to assess the current state of learning rate control. We
find that methods from multi-fidelity hyperparameter optimization,
fixed-hyperparameter schedules, and hyperparameter-free learning often perform
very well on selected deep learning tasks but are not reliable across settings.
This highlights the need for algorithm selection methods in learning rate
control, which have been neglected so far by both the AutoML and deep learning
communities. We also observe a trend of hyperparameter optimization approaches
becoming less effective as models and tasks grow in complexity, even when
combined with multi-fidelity approaches for more expensive model trainings. A
focus on more relevant test tasks and new promising directions like finetunable
methods and meta-learning will enable the AutoML community to significantly
strengthen its impact on this crucial factor in deep learning.

</details>


### [118] [A Real-Time Digital Twin for Type 1 Diabetes using Simulation-Based Inference](https://arxiv.org/abs/2507.01740)
*Trung-Dung Hoang,Alceu Bissoto,Vihangkumar V. Naik,Tim Flühmann,Artemii Shlychkov,José Garcia-Tirado,Lisa M. Koch*

Key words: 模拟推理, 神经后验估计, 1型糖尿病, 参数估计, 数字孪生

TL;DR: 提出了一种基于神经后验估计的模拟推理方法，用于高效估计1型糖尿病模型参数，优于传统方法并提供实时后验推断。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于葡萄糖-胰岛素相互作用的复杂性，传统方法在高维参数空间中效率低下，计算成本高，因此需要更高效的方法。

Method: 采用基于神经后验估计的模拟推理方法（SBI），捕捉餐食摄入、胰岛素和血糖水平之间的复杂关系。

Result: SBI在参数估计上优于传统方法，且能更好地泛化至未见条件，提供实时后验推断和可靠的不确定性量化。

Conclusion: 该方法为1型糖尿病的数字孪生提供了快速、高效的参数估计方案。

Abstract: Accurately estimating parameters of physiological models is essential to
achieving reliable digital twins. For Type 1 Diabetes, this is particularly
challenging due to the complexity of glucose-insulin interactions. Traditional
methods based on Markov Chain Monte Carlo struggle with high-dimensional
parameter spaces and fit parameters from scratch at inference time, making them
slow and computationally expensive. In this study, we propose a
Simulation-Based Inference approach based on Neural Posterior Estimation to
efficiently capture the complex relationships between meal intake, insulin, and
glucose level, providing faster, amortized inference. Our experiments
demonstrate that SBI not only outperforms traditional methods in parameter
estimation but also generalizes better to unseen conditions, offering real-time
posterior inference with reliable uncertainty quantification.

</details>


### [119] [Enhanced Generative Model Evaluation with Clipped Density and Coverage](https://arxiv.org/abs/2507.01761)
*Nicolas Salvy,Hugues Talbot,Bertrand Thirion*

Key words: 生成模型, 样本质量, 评估指标, 鲁棒性, 可解释性

TL;DR: 论文提出了两种新的评估生成模型样本质量的指标：Clipped Density和Clipped Coverage，解决了现有指标在可靠性和可解释性上的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 生成模型在样本质量评估中缺乏可靠、可解释的指标，阻碍了其关键应用。

Method: 通过剪裁单个样本贡献和最近邻球半径，提出Clipped Density和Clipped Coverage两种新指标，避免离群样本偏差。

Result: 新指标在合成和实际数据上表现出更高的鲁棒性、敏感性和可解释性，优于现有方法。

Conclusion: Clipped Density和Clipped Coverage为生成模型的样本质量评估提供了更可靠的解决方案。

Abstract: Although generative models have made remarkable progress in recent years,
their use in critical applications has been hindered by their incapacity to
reliably evaluate sample quality. Quality refers to at least two complementary
concepts: fidelity and coverage. Current quality metrics often lack reliable,
interpretable values due to an absence of calibration or insufficient
robustness to outliers. To address these shortcomings, we introduce two novel
metrics, Clipped Density and Clipped Coverage. By clipping individual sample
contributions and, for fidelity, the radii of nearest neighbor balls, our
metrics prevent out-of-distribution samples from biasing the aggregated values.
Through analytical and empirical calibration, these metrics exhibit linear
score degradation as the proportion of poor samples increases. Thus, they can
be straightforwardly interpreted as equivalent proportions of good samples.
Extensive experiments on synthetic and real-world datasets demonstrate that
Clipped Density and Clipped Coverage outperform existing methods in terms of
robustness, sensitivity, and interpretability for evaluating generative models.

</details>


### [120] [BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification](https://arxiv.org/abs/2507.01781)
*Dalia Rodríguez-Salas,Christian Riess*

Key words: 神经符号学习、决策树、稀疏神经网络、XGBoost、可解释性

TL;DR: BranchNet是一种神经符号学习框架，将决策树集成转化为稀疏的部分连接神经网络，保留符号结构的同时支持梯度优化，性能优于XGBoost。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 结合决策树的符号可解释性与神经网络的梯度优化能力，构建紧凑且无需手动调参的模型。

Method: 将决策树的分支映射为隐藏神经元，形成稀疏神经网络，进行梯度优化训练。

Result: 在多分类基准测试中，BranchNet在准确性上显著优于XGBoost。

Conclusion: BranchNet兼具可解释性和性能优势，但在二分类任务中仍需进一步优化。

Abstract: We introduce BranchNet, a neuro-symbolic learning framework that transforms
decision tree ensembles into sparse, partially connected neural networks. Each
branch, defined as a decision path from root to a parent of leaves, is mapped
to a hidden neuron, preserving symbolic structure while enabling gradient-based
optimization. The resulting models are compact, interpretable, and require no
manual architecture tuning. Evaluated on a suite of structured multi-class
classification benchmarks, BranchNet consistently outperforms XGBoost in
accuracy, with statistically significant gains. We detail the architecture,
training procedure, and sparsity dynamics, and discuss the model's strengths in
symbolic interpretability as well as its current limitations, particularly on
binary tasks where further adaptive calibration may be beneficial.

</details>


### [121] [Towards Decentralized and Sustainable Foundation Model Training with the Edge](https://arxiv.org/abs/2507.01803)
*Leyang Xue,Meghana Madhyastha,Randal Burns,Myungjin Lee,Mahesh K. Marina*

Key words: 基础模型，去中心化，可持续性，边缘AI，计算资源

TL;DR: 论文提出了一种去中心化且可持续的基础模型训练方法，利用边缘AI设备的闲置算力，以减少环境影响和避免计算资源集中控制的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 基础模型因其从海量数据中学习并适应多样化任务的能力而受到关注，但其高计算需求带来了环境问题和集中化控制的风险。作者希望通过去中心化的方式降低这些负面影响。

Method: 提出利用连接边缘AI设备的闲置计算资源进行基础模型训练，以实现去中心化和可持续性。

Result: 初步探讨了该方法的可行性，特别强调了其在可持续性方面的优势，并提出了一系列需要解决的挑战。

Conclusion: 去中心化和可持续的基础模型训练具有潜力，但仍需克服技术和社会层面的挑战。

Abstract: Foundation models are at the forefront of AI research, appealing for their
ability to learn from vast datasets and cater to diverse tasks. Yet, their
significant computational demands raise issues of environmental impact and the
risk of centralized control in their development. We put forward a vision
towards decentralized and sustainable foundation model training that leverages
the collective compute of sparingly used connected edge AI devices. We present
the rationale behind our vision, particularly in support of its sustainability
benefit. We further outline a set of challenges that need to be addressed to
turn this vision into reality.

</details>


### [122] [TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents](https://arxiv.org/abs/2507.01823)
*Dmytro Kuzmenko,Nadiya Shvai*

Key words: 知识转移, 模型蒸馏, 多任务强化学习, 资源优化

TL;DR: 提出了一种基于模型强化学习中的知识转移新方法，解决大模型在资源受限环境中的部署问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决大世界模型在资源受限环境中的部署挑战。

Method: 通过蒸馏技术将高容量多任务代理压缩为紧凑模型，并进一步优化量化。

Result: 蒸馏模型在MT30基准上取得28.45的归一化分数，优于原始模型。

Conclusion: 该方法为资源受限应用中的多任务强化学习系统提供了高效解决方案。

Abstract: We present a novel approach to knowledge transfer in model-based
reinforcement learning, addressing the critical challenge of deploying large
world models in resource-constrained environments. Our method efficiently
distills a high-capacity multi-task agent (317M parameters) into a compact
model (1M parameters) on the MT30 benchmark, significantly improving
performance across diverse tasks. Our distilled model achieves a
state-of-the-art normalized score of 28.45, surpassing the original 1M
parameter model score of 18.93. This improvement demonstrates the ability of
our distillation technique to capture and consolidate complex multi-task
knowledge. We further optimize the distilled model through FP16 post-training
quantization, reducing its size by $\sim$50\%. Our approach addresses practical
deployment limitations and offers insights into knowledge representation in
large world models, paving the way for more efficient and accessible multi-task
reinforcement learning systems in robotics and other resource-constrained
applications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.

</details>


### [123] [MILP-SAT-GNN: Yet Another Neural SAT Solver](https://arxiv.org/abs/2507.01825)
*Franco Alberto Cardillo,Hamza Khyari,Umberto Straccia*

Key words: 图神经网络, GNN, SAT问题, MILP, 近似定理

TL;DR: 提出一种新方法，将GNN应用于SAT问题，通过将k-CNF公式映射为MILP问题，再编码为加权二分图进行训练和测试。理论上证明了方法的稳定性和近似能力，实验表明其效果良好。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决如何利用图神经网络（GNN）有效处理SAT问题的挑战，通过借鉴其在混合整数线性规划（MILP）中的技术。

Method: 将k-CNF公式映射为MILP问题，编码为加权二分图，输入GNN进行训练和测试。理论分析包括稳定性证明和近似定理。

Result: 方法在理论和实验上均表现良好，GNN能够近似解决SAT问题，且对某些公式无需随机节点初始化。

Conclusion: 通过GNN处理SAT问题可行且有效，理论支持其稳定性和近似能力，实验验证了简单结构的潜力。

Abstract: We proposes a novel method that enables Graph Neural Networks (GNNs) to solve
SAT problems by leveraging a technique developed for applying GNNs to Mixed
Integer Linear Programming (MILP). Specifically, k-CNF formulae are mapped into
MILP problems, which are then encoded as weighted bipartite graphs and
subsequently fed into a GNN for training and testing. From a theoretical
perspective: (i) we establish permutation and equivalence invariance results,
demonstrating that the method produces outputs that are stable under reordering
of clauses and variables; (ii) we identify a theoretical limitation, showing
that for a class of formulae called foldable formulae, standard GNNs cannot
always distinguish satisfiable from unsatisfiable instances; (iii) we prove a
universal approximation theorem, establishing that with Random Node
Initialization (RNI), the method can approximate SAT solving to arbitrary
precision on finite datasets, that is, the GNN becomes approximately sound and
complete on such datasets. Furthermore, we show that for unfoldable formulae,
the same approximation guarantee can be achieved without the need for RNI.
Finally, we conduct an experimental evaluation of our approach, which show
that, despite the simplicity of the neural architecture, the method achieves
promising results.

</details>


### [124] [mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling](https://arxiv.org/abs/2507.01829)
*Tristan Torchet,Christian Metzner,Laura Kriener,Melika Payvand*

Key words: mGRADE, 边缘计算, 时间卷积, 门控循环单元, 多尺度时间处理

TL;DR: mGRADE 是一种混合记忆系统，结合了时间卷积和最小门控循环单元，适用于内存受限的边缘设备上的多尺度时间处理。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 边缘设备需要在严格的内存限制下捕捉短时和长时动态的模型。现有的 Transformer、RNN 和 TCN 各有不足，无法满足需求。

Method: 提出 mGRADE，整合了可学习间距的时间 1D 卷积和最小门控循环单元（minGRU），卷积层捕捉快速变化，循环模块高效维持全局上下文。

Result: 在合成任务和多尺度图像分类任务上，mGRADE 显著优于纯卷积和纯循环模型，内存占用减少约 20%。

Conclusion: mGRADE 是内存受限边缘设备上高效处理多尺度时间特征的理想解决方案。

Abstract: Edge devices for temporal processing demand models that capture both short-
and long- range dynamics under tight memory constraints. While Transformers
excel at sequence modeling, their quadratic memory scaling with sequence length
makes them impractical for such settings. Recurrent Neural Networks (RNNs)
offer constant memory but train sequentially, and Temporal Convolutional
Networks (TCNs), though efficient, scale memory with kernel size. To address
this, we propose mGRADE (mininally Gated Recurrent Architecture with Delay
Embedding), a hybrid-memory system that integrates a temporal 1D-convolution
with learnable spacings followed by a minimal gated recurrent unit (minGRU).
This design allows the convolutional layer to realize a flexible delay
embedding that captures rapid temporal variations, while the recurrent module
efficiently maintains global context with minimal memory overhead. We validate
our approach on two synthetic tasks, demonstrating that mGRADE effectively
separates and preserves multi-scale temporal features. Furthermore, on
challenging pixel-by-pixel image classification benchmarks, mGRADE consistently
outperforms both pure convolutional and pure recurrent counterparts using
approximately 20% less memory footprint, highlighting its suitability for
memory-constrained temporal processing at the edge. This highlights mGRADE's
promise as an efficient solution for memory-constrained multi-scale temporal
processing at the edge.

</details>


### [125] [Out-of-Distribution Detection Methods Answer the Wrong Questions](https://arxiv.org/abs/2507.01831)
*Yucen Lily Li,Daohan Lu,Polina Kirichenko,Shikai Qiu,Tim G. J. Rudner,C. Bayan Bruss,Andrew Gordon Wilson*

Key words: OOD检测, 预测不确定性, 特征距离, 监督模型, 生成模型

TL;DR: 本文批判性地重新审视了基于预测不确定性或特征的OOD检测方法，指出这些方法在根本上是错误的，未能解决OOD检测的核心问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的OOD检测方法依赖监督模型的预测不确定性或特征，但这些方法未能正确识别OOD数据。本文旨在揭示这些方法的局限性。

Method: 通过对流行OOD检测方法的分析，本文指出高不确定性和远特征距离并不等同于OOD数据。

Result: 研究发现，现有方法在某些场景下表现不佳，且干预措施如特征-逻辑混合方法、模型规模扩展等未能解决根本问题。

Conclusion: OOD检测需要重新思考方法的核心目标，现有方法存在根本性的误区。

Abstract: To detect distribution shifts and improve model safety, many
out-of-distribution (OOD) detection methods rely on the predictive uncertainty
or features of supervised models trained on in-distribution data. In this
paper, we critically re-examine this popular family of OOD detection
procedures, and we argue that these methods are fundamentally answering the
wrong questions for OOD detection. There is no simple fix to this misalignment,
since a classifier trained only on in-distribution classes cannot be expected
to identify OOD points; for instance, a cat-dog classifier may confidently
misclassify an airplane if it contains features that distinguish cats from
dogs, despite generally appearing nothing alike. We find that uncertainty-based
methods incorrectly conflate high uncertainty with being OOD, while
feature-based methods incorrectly conflate far feature-space distance with
being OOD. We show how these pathologies manifest as irreducible errors in OOD
detection and identify common settings where these methods are ineffective.
Additionally, interventions to improve OOD detection such as feature-logit
hybrid methods, scaling of model and data size, epistemic uncertainty
representation, and outlier exposure also fail to address this fundamental
misalignment in objectives. We additionally consider unsupervised density
estimation and generative models for OOD detection, which we show have their
own fundamental limitations.

</details>


### [126] [Automatic Rank Determination for Low-Rank Adaptation via Submodular Function Maximization](https://arxiv.org/abs/2507.01841)
*Yihang Gao,Vincent Y. F. Tan*

Key words: SubLoRA, LoRA, submodular function, Hessian matrix, rank determination, PINN, PDE

TL;DR: SubLoRA 是一种基于子模函数最大化的低秩适应（LoRA）秩确定方法，利用二阶信息（Hessian矩阵）提高准确性，并通过贪婪算法和理论保证解决NP难题，实验显示其在秩确定和联合训练中优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法如AdaLoRA依赖一阶近似，在LoRA参数优化后变得不准确和不稳定，需要更可靠、细致的二阶方法。

Method: 将秩确定问题转化为组合优化问题，引入子模函数最大化框架和贪婪算法，结合Hessian矩阵的闭式投影，确保计算效率。

Result: 实验表明，SubLoRA在物理信息神经网络（PINNs）的偏微分方程（PDE）微调中优于现有方法。

Conclusion: SubLoRA结合理论、二阶准确性和计算效率，扩展至联合优化，解决了秩确定的关键问题。

Abstract: In this paper, we propose SubLoRA, a rank determination method for Low-Rank
Adaptation (LoRA) based on submodular function maximization. In contrast to
prior approaches, such as AdaLoRA, that rely on first-order (linearized)
approximations of the loss function, SubLoRA utilizes second-order information
to capture the potentially complex loss landscape by incorporating the Hessian
matrix. We show that the linearization becomes inaccurate and ill-conditioned
when the LoRA parameters have been well optimized, motivating the need for a
more reliable and nuanced second-order formulation. To this end, we reformulate
the rank determination problem as a combinatorial optimization problem with a
quadratic objective. However, solving this problem exactly is NP-hard in
general. To overcome the computational challenge, we introduce a submodular
function maximization framework and devise a greedy algorithm with
approximation guarantees. We derive a sufficient and necessary condition under
which the rank-determination objective becomes submodular, and construct a
closed-form projection of the Hessian matrix that satisfies this condition
while maintaining computational efficiency. Our method combines solid
theoretical foundations, second-order accuracy, and practical computational
efficiency. We further extend SubLoRA to a joint optimization setting,
alternating between LoRA parameter updates and rank determination under a rank
budget constraint. Extensive experiments on fine-tuning physics-informed neural
networks (PINNs) for solving partial differential equations (PDEs) demonstrate
the effectiveness of our approach. Results show that SubLoRA outperforms
existing methods in both rank determination and joint training performance.

</details>


### [127] [Towards Foundation Auto-Encoders for Time-Series Anomaly Detection](https://arxiv.org/abs/2507.01875)
*Gastón García González,Pedro Casas,Emilio Martínez,Alicia Fernández*

Key words: 时间序列、异常检测、变分自编码器（VAE）、扩张卷积神经网络（DCNN）、预训练

TL;DR: FAE（基础自编码器）是一种基于变分自编码器（VAE）和时间序列数据的大规模预训练的生成模型，用于异常检测。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 受大型预训练基础模型成功的启发，研究一种能够学习复杂时间模式并在未见数据集上准确建模、预测和检测异常的通用方法。

Method: 结合VAE和扩张卷积神经网络（DCNN），构建了一个用于单变量时间序列建模的通用模型，支持零样本异常检测。

Result: 在多维时间序列数据集上进行了初步测试，包括移动ISP的真实数据和KDD 2021异常检测数据集。

Conclusion: FAE展示了在时间序列异常检测中作为基础模型的潜力。

Abstract: We investigate a novel approach to time-series modeling, inspired by the
successes of large pretrained foundation models. We introduce FAE (Foundation
Auto-Encoders), a foundation generative-AI model for anomaly detection in
time-series data, based on Variational Auto-Encoders (VAEs). By foundation, we
mean a model pretrained on massive amounts of time-series data which can learn
complex temporal patterns useful for accurate modeling, forecasting, and
detection of anomalies on previously unseen datasets. FAE leverages VAEs and
Dilated Convolutional Neural Networks (DCNNs) to build a generic model for
univariate time-series modeling, which could eventually perform properly in
out-of-the-box, zero-shot anomaly detection applications. We introduce the main
concepts of FAE, and present preliminary results in different multi-dimensional
time-series datasets from various domains, including a real dataset from an
operational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.

</details>


### [128] [Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection](https://arxiv.org/abs/2507.01924)
*Samirah Bakker,Yao Ma,Seyed Sahand Mohammadi Ziabari*

Key words: 心理健康护理, 异常检测, 深度学习, LSTM, Transformers, 伪标签

TL;DR: 该论文探讨了一种结合LSTM网络和Transformers的混合深度学习方法来检测心理健康护理账单中的异常，使用了伪标签技术（Isolation Forests和Autoencoders），并在真实数据集中验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 心理健康护理账单的复杂性容易导致异常和欺诈行为，但现有的机器学习方法在类别不平衡、标签稀缺和复杂序列模式方面存在挑战。

Method: 采用混合深度学习方法，结合LSTM网络和Transformers，并通过Isolation Forests和Autoencoders生成伪标签。

Result: 在真实数据集上，iForest LSTM基线模型在声明级数据上达到最高召回率（0.963），而基于iForest的混合模型在操作级数据上召回率最高（0.744），但精确度较低。

Conclusion: 伪标签技术与混合深度学习结合在复杂且类别不平衡的异常检测场景中具有潜力。

Abstract: The complexity of mental healthcare billing enables anomalies, including
fraud. While machine learning methods have been applied to anomaly detection,
they often struggle with class imbalance, label scarcity, and complex
sequential patterns. This study explores a hybrid deep learning approach
combining Long Short-Term Memory (LSTM) networks and Transformers, with
pseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior
work has not evaluated such hybrid models trained on pseudo-labeled data in the
context of healthcare billing. The approach is evaluated on two real-world
billing datasets related to mental healthcare. The iForest LSTM baseline
achieves the highest recall (0.963) on declaration-level data. On the
operation-level data, the hybrid iForest-based model achieves the highest
recall (0.744), though at the cost of lower precision. These findings highlight
the potential of combining pseudo-labeling with hybrid deep learning in
complex, imbalanced anomaly detection settings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [129] [Rethinking the Illusion of Thinking](https://arxiv.org/abs/2507.01231)
*Iñaki Dellibarda Varela,Pablo Romero-Sorozabal,Eduardo Rocon,Manuel Cebrian*

Key words: 大型推理模型, 汉诺塔, 河流交叉, 逐步提示, 协作对话

TL;DR: 论文通过复现和改进两项争议性实验，澄清了大型推理模型（LRMs）是否具备真正推理能力的争论，提出其局限性并指出未来发展方向。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决AI社区中对大型推理模型（LRMs）是否具备真实推理能力的争议，澄清其能力边界。

Method: 复制并改进原始研究中的两项争议性基准测试（汉诺塔和河流交叉），引入逐步提示和协作对话方法。

Result: 汉诺塔任务中，LRMs在复杂度适中时会失败；河流交叉任务中，LRMs能轻松解决可解问题。结论表明LRMs是离散状态空间中的随机搜索器。

Conclusion: LRMs具备一定推理能力，但仍有局限性；未来需通过精细实验进一步探索其推理机制。

Abstract: Earlier this year, Apple ignited controversy by publishing "The Illusion of
Thinking," prompting heated debate within the AI community. Critics seized upon
the findings as conclusive evidence that Large Reasoning Models (LRMs) lack
genuine reasoning capabilities, branding them as mere stochastic parrots.
Meanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning
the experimental setup as flawed and the conclusions overstated. We clarify
this debate by replicating and refining two of the original study's most
contentious benchmarks: Towers of Hanoi and River Crossing. By introducing
incremental stepwise prompting and agentic collaborative dialogue, we show that
previously reported failures solving the Towers of Hanoi were not purely result
of output constraints, but also partly a result of cognition limitations: LRMs
still stumble when complexity rises moderately (around 8 disks). Moreover, the
River Crossing results initially heralded as catastrophic failures turn out to
hinge upon testing unsolvable configurations. Once we limit tests strictly to
solvable problems-LRMs effortlessly solve large instances involving over 100
agent pairs. Our findings ultimately defy simplistic narratives: today's LRMs
are stochastic, RL-tuned searchers in a discrete state space we barely
understand. Real progress in symbolic, long-horizon reasoning demands mapping
that terrain through fine-grained ablations like those introduced here.

</details>


### [130] [Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care](https://arxiv.org/abs/2507.01282)
*Matthew JY Kang,Wenli Yang,Monica R Roberts,Byeong Ho Kang,Charles B Malpas*

Key words: 大型语言模型, 医疗诊断, 痴呆症, 可解释AI, 神经符号AI

TL;DR: 该论文探讨了大型语言模型（LLMs）在医疗诊断中的潜在贡献及其实际限制，特别是在痴呆症诊断中。尽管LLMs在基准测试中表现优异，但其在临床实践中的实际效果有限。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究旨在揭示AI在临床环境中（尤其是痴呆症诊断和护理）的实际限制，并提出改进方向。

Method: 通过文献回顾和分析，指出当前AI的局限性（如黑盒输出、幻觉问题、因果推理能力弱），并建议结合统计学习和专家知识的混合方法。

Result: 尽管AI在模式识别上表现出色，但其缺乏可解释性和透明性影响了临床医生的信任。混合方法（如PEIRS和ATHENA-CDS）在可解释性和工作流适配性上表现更好。

Conclusion: 未来的决策支持系统应优先考虑可解释性，结合神经符号AI和人类因果专业知识，并以改善临床医生理解和工作流适配为目标。

Abstract: The recent boom of large language models (LLMs) has re-ignited the hope that
artificial intelligence (AI) systems could aid medical diagnosis. Yet despite
dazzling benchmark scores, LLM assistants have yet to deliver measurable
improvements at the bedside. This scoping review aims to highlight the areas
where AI is limited to make practical contributions in the clinical setting,
specifically in dementia diagnosis and care.
  Standalone machine-learning models excel at pattern recognition but seldom
provide actionable, interpretable guidance, eroding clinician trust. Adjacent
use of LLMs by physicians did not result in better diagnostic accuracy or
speed. Key limitations trace to the data-driven paradigm: black-box outputs
which lack transparency, vulnerability to hallucinations, and weak causal
reasoning. Hybrid approaches that combine statistical learning with expert
rule-based knowledge, and involve clinicians throughout the process help bring
back interpretability. They also fit better with existing clinical workflows,
as seen in examples like PEIRS and ATHENA-CDS.
  Future decision-support should prioritise explanatory coherence by linking
predictions to clinically meaningful causes. This can be done through
neuro-symbolic or hybrid AI that combines the language ability of LLMs with
human causal expertise. AI researchers have addressed this direction, with
explainable AI and neuro-symbolic AI being the next logical steps in further
advancement in AI. However, they are still based on data-driven knowledge
integration instead of human-in-the-loop approaches. Future research should
measure success not only by accuracy but by improvements in clinician
understanding, workflow fit, and patient outcomes. A better understanding of
what helps improve human-computer interactions is greatly needed for AI systems
to become part of clinical practice.

</details>


### [131] [AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing](https://arxiv.org/abs/2507.01376)
*Yinwang Ren,Yangyang Liu,Tang Ji,Xun Xu*

Key words: AI Agents, GenAI, LLM-Agents, MLLM-Agents, Agentic AI, 智能制造

TL;DR: 摘要讨论了AI代理（LLM-Agents、MLLM-Agents和Agentic AI）的快速发展及其在智能制造中的潜力，但对其定义、能力边界和实际应用仍存在疑问。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着生成式AI（GenAI）的进步，AI代理的能力显著提升，但它们在智能制造中的应用和挑战尚未明确。

Method: 系统回顾了AI和AI代理技术的发展，分析了LLM-Agents、MLLM-Agents和Agentic AI的核心概念和技术进步。

Result: 探索了这些新兴AI范式在智能制造中的潜在应用和集成，以及可能面临的挑战。

Conclusion: 研究为理解AI代理在智能制造中的角色提供了系统化的视角，但仍需解决定义和应用上的模糊性。

Abstract: AI agents are autonomous systems designed to perceive, reason, and act within
dynamic environments. With the rapid advancements in generative AI (GenAI),
large language models (LLMs) and multimodal large language models (MLLMs) have
significantly improved AI agents' capabilities in semantic comprehension,
complex reasoning, and autonomous decision-making. At the same time, the rise
of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and
complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents
(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in
information processing, environmental perception, and autonomous
decision-making, opening new avenues for smart manufacturing. However, the
definitions, capability boundaries, and practical applications of these
emerging AI paradigms in smart manufacturing remain unclear. To address this
gap, this study systematically reviews the evolution of AI and AI agent
technologies, examines the core concepts and technological advancements of
LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential
applications in and integration into manufacturing, along with the potential
challenges they may face.

</details>


### [132] [A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models](https://arxiv.org/abs/2507.01410)
*Abeer Dyoub,Francesca A. Lisi*

Key words: 伦理决策模型, 伦理风险评估, 模糊Petri网, 医疗案例

TL;DR: 提出了一种基于伦理风险评估的形式化方法，用于描述伦理决策模型，并通过模糊Petri网进行验证和验证，以医疗领域案例为例。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 道德领域的本体和认知复杂性使得评估道德机器性能的标准难以确立。

Method: 基于伦理风险评估的形式化方法，将模型指定为模糊规则，并使用模糊Petri网进行验证和验证。

Result: 成功展示了一种伦理决策模型的验证方法，并通过医疗案例进行说明。

Conclusion: 提出的方法能够有效应对道德领域的复杂性，为伦理决策模型提供了验证工具。

Abstract: The ontological and epistemic complexities inherent in the moral domain make
it challenging to establish clear standards for evaluating the performance of a
moral machine. In this paper, we present a formal method to describe Ethical
Decision Making models based on ethical risk assessment. Then, we show how
these models that are specified as fuzzy rules can be verified and validated
using fuzzy Petri nets. A case study from the medical field is considered to
illustrate the proposed approach.

</details>


### [133] [Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading](https://arxiv.org/abs/2507.01431)
*Yoonseok Yang,Minjune Kim,Marlon Rondinelli,Keren Shao*

Key words: AI评分, STEM教育, 大型语言模型

TL;DR: Pensieve是一个基于大型语言模型的人工智能辅助评分平台，显著减少大学STEM课程中手写开放答案的批改时间。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 大规模大学STEM课程中手写开放答案的评分是一个主要瓶颈，需要高效解决方案。

Method: 利用大型语言模型转录和评估学生作业，提供符合评分的分数、转录和置信度评级，支持全流程评分。

Result: 在实践中评分超过30万份答案，评分时间平均减少65%，高置信度预测与教师评分一致率达95.4%。

Conclusion: Pensieve能大幅提升评分效率，同时保持高准确性。

Abstract: Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.

</details>


### [134] [Using multi-agent architecture to mitigate the risk of LLM hallucinations](https://arxiv.org/abs/2507.01446)
*Abd Elrahman Amer,Magdi Amer*

Key words: 大型语言模型, 多智能体系统, 模糊逻辑, 客户服务, 幻觉风险

TL;DR: 本文提出了一种多智能体系统，通过结合大型语言模型（LLM）和模糊逻辑，旨在提高客户服务质量并降低幻觉风险。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 提高客户服务质量和响应时间是保持客户忠诚度和增加市场份额的关键因素，而新兴的大型语言模型在应用中存在幻觉风险的挑战。

Method: 采用多智能体系统处理客户通过SMS发送的请求，结合LLM和模糊逻辑以降低幻觉风险。

Result: 该方法通过模糊逻辑与LLM的结合，有效减少了幻觉风险。

Conclusion: 多智能体系统结合模糊逻辑和LLM能够有效提升客户服务质量并降低技术风险。

Abstract: Improving customer service quality and response time are critical factors for
maintaining customer loyalty and increasing a company's market share. While
adopting emerging technologies such as Large Language Models (LLMs) is becoming
a necessity to achieve these goals, the risk of hallucination remains a major
challenge. In this paper, we present a multi-agent system to handle customer
requests sent via SMS. This system integrates LLM based agents with fuzzy logic
to mitigate hallucination risks.

</details>


### [135] [Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning](https://arxiv.org/abs/2507.01489)
*Yanfei Zhang*

Key words: 大语言模型、强化学习、Agent-as-tool、推理、工具调用

TL;DR: 提出了一个名为Agent-as-tool的分层框架，将工具调用与推理过程分离，显著提升了模型性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有研究同时处理工具调用与推理过程，模型需处理冗余信息，增加了推理负担。

Method: 采用分层框架Agent-as-tool，将工具调用过程交给另一个代理处理，主模型专注于推理。

Result: 仅需少量样本微调即达到优异性能，Bamboogle任务中精确匹配率提升4.8%。

Conclusion: 分离工具调用与推理过程能显著提高模型性能。

Abstract: Large Language Models (LLMs) have emerged as one of the most significant
technological advancements in artificial intelligence in recent years. Their
ability to understand, generate, and reason with natural language has
transformed how we interact with AI systems. With the development of LLM-based
agents and reinforcement-learning-based reasoning models, the study of applying
reinforcement learning in agent frameworks has become a new research focus.
However, all previous studies face the challenge of deciding the tool calling
process and the reasoning process simultaneously, and the chain of reasoning
was solely relied on the unprocessed raw result with redundant information and
symbols unrelated to the task from the tool, which impose a heavy burden on the
model's capability to reason. Therefore, in our research, we proposed a
hierarchical framework Agent-as-tool that detach the tool calling process and
the reasoning process, which enables the model to focus on the verbally
reasoning process while the tool calling process is handled by another agent.
Our work had achieved comparable results with only a slight reinforcement
fine-tuning on 180 samples, and had achieved exceptionally well performance in
Bamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding
Search-R1 by 4.8% in exact match and 3.2% in cover exact match.

</details>


### [136] [T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2507.01597)
*Yuehang Si,Zefan Zeng,Jincai Huang,Qing Cheng*

Key words: 时序知识图谱, 分布偏移, 负采样, 对抗训练

TL;DR: 提出了一种名为T3DM的新方法，用于解决时序知识图谱推理中事件分布偏移和负样本质量低的问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有时序知识图谱推理方法在建模事件分布偏移和生成高质量负样本方面存在不足。

Method: 采用基于测试时训练的分布偏移建模（T3DM）和对抗训练生成的负采样策略。

Result: T3DM在大多数情况下优于现有基线方法，表现出更好的鲁棒性。

Conclusion: T3DM通过动态调整分布偏移和优化负样本，显著提升了推理性能。

Abstract: Temporal Knowledge Graph (TKG) is an efficient method for describing the
dynamic development of facts along a timeline. Most research on TKG reasoning
(TKGR) focuses on modelling the repetition of global facts and designing
patterns of local historical facts. However, they face two significant
challenges: inadequate modeling of the event distribution shift between
training and test samples, and reliance on random entity substitution for
generating negative samples, which often results in low-quality sampling. To
this end, we propose a novel distributional feature modeling approach for
training TKGR models, Test-Time Training-guided Distribution shift Modelling
(T3DM), to adjust the model based on distribution shift and ensure the global
consistency of model reasoning. In addition, we design a negative-sampling
strategy to generate higher-quality negative quadruples based on adversarial
training. Extensive experiments show that T3DM provides better and more robust
results than the state-of-the-art baselines in most cases.

</details>


### [137] [Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI](https://arxiv.org/abs/2507.01717)
*Gopichand Kanumolu,Ashok Urlana,Charaka Vinayak Kumar,Bala Mallikarjunarao Garlapati*

Key words: 专利挖掘，大型语言模型，自主代理，创新，产品概念

TL;DR: 利用大型语言模型（LLMs）和自主代理从专利中挖掘并生成产品概念，设计了一个名为Agent Ideate的框架，实验表明代理方法在创意质量、相关性和新颖性上优于单独使用LLMs。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 专利包含丰富的技术知识，但访问和解读这些信息存在挑战，旨在通过LLMs和代理释放专利数据的创新潜力。

Method: 设计Agent Ideate框架，结合开源LLMs和代理架构，在计算机科学、自然语言处理和材料化学三个领域进行实验。

Result: 代理方法在创意生成的质量、相关性和新颖性上均优于单独使用LLMs。

Conclusion: LLMs与代理工作流结合可显著提升从专利数据生成商业创意的创新潜力。

Abstract: Patents contain rich technical knowledge that can inspire innovative product
ideas, yet accessing and interpreting this information remains a challenge.
This work explores the use of Large Language Models (LLMs) and autonomous
agents to mine and generate product concepts from a given patent. In this work,
we design Agent Ideate, a framework for automatically generating product-based
business ideas from patents. We experimented with open-source LLMs and
agent-based architectures across three domains: Computer Science, Natural
Language Processing, and Material Chemistry. Evaluation results show that the
agentic approach consistently outperformed standalone LLMs in terms of idea
quality, relevance, and novelty. These findings suggest that combining LLMs
with agentic workflows can significantly enhance the innovation pipeline by
unlocking the untapped potential of business idea generation from patent data.

</details>


### [138] [Joint Matching and Pricing for Crowd-shipping with In-store Customers](https://arxiv.org/abs/2507.01749)
*Arash Dehghan,Mucahit Cevik,Merve Bodur,Bissan Ghaddar*

Key words: 众包配送, 最后一英里配送, MDP, NeurADP, DDQN

TL;DR: 论文研究了集中式众包配送系统中利用店内顾客作为配送员的方法，提出了结合NeurADP和DDQN的动态优化策略，显著降低了配送成本。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 针对城市地区最后一英里配送效率的需求增长，探索利用顾客作为配送员的潜在效益。

Method: 提出基于MDP的模型，结合NeurADP和DDQN进行动态定价和订单分配优化。

Result: 动态策略比固定定价节省6.7%成本，比短视基线节省18%；灵活配送和多目的地路由分别额外节省8%和17%。

Conclusion: 动态前瞻性策略在众包配送系统中具有显著优势，为城市物流运营商提供了实用指导。

Abstract: This paper examines the use of in-store customers as delivery couriers in a
centralized crowd-shipping system, targeting the growing need for efficient
last-mile delivery in urban areas. We consider a brick-and-mortar retail
setting where shoppers are offered compensation to deliver time-sensitive
online orders. To manage this process, we propose a Markov Decision Process
(MDP) model that captures key uncertainties, including the stochastic arrival
of orders and crowd-shippers, and the probabilistic acceptance of delivery
offers. Our solution approach integrates Neural Approximate Dynamic Programming
(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network
(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop
routing and accounts for offer acceptance uncertainty, aligning more closely
with real-world operations. Experimental results demonstrate that the
integrated NeurADP + DDQN policy achieves notable improvements in delivery cost
efficiency, with up to 6.7\% savings over NeurADP with fixed pricing and
approximately 18\% over myopic baselines. We also show that allowing flexible
delivery delays and enabling multi-destination routing further reduces
operational costs by 8\% and 17\%, respectively. These findings underscore the
advantages of dynamic, forward-looking policies in crowd-shipping systems and
offer practical guidance for urban logistics operators.

</details>


### [139] [Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics](https://arxiv.org/abs/2507.01833)
*Yi-Dong Shen,Thomas Eiter*

Key words: 非单调逻辑编程, 答案集语义, Gelfond答案集原则, 计算复杂性

TL;DR: 该论文探讨了非单调逻辑编程中答案集语义的一般原则，质疑了已有的最小模型属性、约束单调性和基性是否必须，并提出了改进的Gelfond答案集原则。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究旨在确定答案集语义的一般原则，挑战已有条件是否过于严格，并提出新的性质以更广泛地定义答案集语义。

Method: 通过示例说明已有条件的局限性，精炼Gelfond答案集原则，扩展到答案集和世界观，定义新的语义，并分析计算复杂性。

Result: 提出改进的Gelfond答案为答案集语义提供了更合理的基础，并通过新语义和复杂性分析验证。

Conclusion: 改进的Gelfond答案集原则为答案集语义提供了灵活且合理的框架，可作为评估现有语义的基准。

Abstract: Non-monotonic logic programming is the basis for a declarative problem
solving paradigm known as answer set programming (ASP). Departing from the
seminal definition by Gelfond and Lifschitz in 1988 for simple normal logic
programs, various answer set semantics have been proposed for extensions. We
consider two important questions: (1) Should the minimal model property,
constraint monotonicity and foundedness as defined in the literature be
mandatory conditions for an answer set semantics in general? (2) If not, what
other properties could be considered as general principles for answer set
semantics? We address the two questions. First, it seems that the three
aforementioned conditions may sometimes be too strong, and we illustrate with
examples that enforcing them may exclude expected answer sets. Second, we
evolve the Gelfond answer set (GAS) principles for answer set construction by
refining the Gelfond's rationality principle to well-supportedness, minimality
w.r.t. negation by default and minimality w.r.t. epistemic negation. The
principle of well-supportedness guarantees that every answer set is
constructible from if-then rules obeying a level mapping and is thus free of
circular justification, while the two minimality principles ensure that the
formalism minimizes knowledge both at the level of answer sets and of world
views. Third, to embody the refined GAS principles, we extend the notion of
well-supportedness substantially to answer sets and world views, respectively.
Fourth, we define new answer set semantics in terms of the refined GAS
principles. Fifth, we use the refined GAS principles as an alternative baseline
to intuitively assess the existing answer set semantics. Finally, we analyze
the computational complexity.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [140] [A Systematic Review of Security Vulnerabilities in Smart Home Devices and Mitigation Techniques](https://arxiv.org/abs/2507.01018)
*Mohammed K. Alzaylaee*

Key words: 智能家居, 网络安全, 量子加密, AI异常检测, 区块链认证

TL;DR: 论文总结了智能家居生态系统中的网络安全威胁，并探讨了量子加密、AI异常检测和区块链认证等解决方案的有效性与挑战。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 随着物联网设备的普及，智能家居面临日益增长的网络安全威胁，亟需高效的安全解决方案。

Method: 研究通过分析网络层、设备层及云与AI系统的漏洞，结合实验方法（ANOVA、卡方检验、蒙特卡洛模拟）评估了量子加密、AI检测和区块链认证等技术的效果。

Result: 结果表明量子加密与AI异常检测效果显著，但计算资源需求高；区块链认证提升了安全性，但需基础设施调整；当前策略在扩展性方面仍有不足。

Conclusion: 研究指出需改进加密技术，并平衡AI增强的安全模型性能与实时性，以适应智能家居生态系统的需求。

Abstract: Smart homes that integrate Internet of Things (IoT) devices face increasing
cybersecurity risks, posing significant challenges to these environments. The
study explores security threats in smart homes ecosystems, categorizing them
into vulnerabilities at the network layer, device level, and those from
cloud-based and AI-driven systems. Research findings indicate that post-quantum
encryption, coupled with AI-driven anomaly detection, is highly effective in
enhancing security; however, computational resource demands present significant
challenges. Blockchain authentication together with zero-trust structures
builds security resilience, although they need changes to existing
infrastructure. The specific security strategies show their effectiveness
through ANOVA, Chi-square tests, and Monte Carlo simulations yet lack
sufficient scalability according to the results. The research demonstrates the
requirement for improvement in cryptographic techniques, alongside AI-enhanced
threat detection and adaptive security models which must achieve a balance
between performance and efficiency and real-time applicability within smart
home ecosystems.

</details>


### [141] [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2507.01020)
*Aashray Reddy,Andrew Zagula,Nicholas Saban*

Key words: 大型语言模型, 对抗性攻击, 安全漏洞, 自动提示生成

TL;DR: 本文提出了一种名为AutoAdv的框架，用于自动化生成对抗性提示，揭示大型语言模型（LLMs）安全机制的漏洞。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 由于大型语言模型在面对精心设计的恶意输入时仍存在安全漏洞，研究旨在系统评估和暴露这些漏洞。

Method: 通过参数化攻击者LLM生成语义伪装的恶意提示，结合策略重写技术、专用系统提示和优化的超参数配置，提出动态多轮攻击方法。

Result: 实验表明，自动化攻击在有害内容生成方面的成功率达到86%，揭示了现有安全机制对多轮攻击的脆弱性。

Conclusion: 当前安全机制对复杂多轮攻击仍显脆弱，亟需更强大的防御策略。

Abstract: Large Language Models (LLMs) continue to exhibit vulnerabilities to
jailbreaking attacks: carefully crafted malicious inputs intended to circumvent
safety guardrails and elicit harmful responses. As such, we present AutoAdv, a
novel framework that automates adversarial prompt generation to systematically
evaluate and expose vulnerabilities in LLM safety mechanisms. Our approach
leverages a parametric attacker LLM to produce semantically disguised malicious
prompts through strategic rewriting techniques, specialized system prompts, and
optimized hyperparameter configurations. The primary contribution of our work
is a dynamic, multi-turn attack methodology that analyzes failed jailbreak
attempts and iteratively generates refined follow-up prompts, leveraging
techniques such as roleplaying, misdirection, and contextual manipulation. We
quantitatively evaluate attack success rate (ASR) using the StrongREJECT
(arXiv:2402.10260 [cs.CL]) framework across sequential interaction turns.
Through extensive empirical evaluation of state-of-the-art models--including
ChatGPT, Llama, and DeepSeek--we reveal significant vulnerabilities, with our
automated attacks achieving jailbreak success rates of up to 86% for harmful
content generation. Our findings reveal that current safety mechanisms remain
susceptible to sophisticated multi-turn attacks, emphasizing the urgent need
for more robust defense strategies.

</details>


### [142] [Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems](https://arxiv.org/abs/2507.01808)
*Xiaoyu Ji,Jessica Shorland,Joshua Shank,Pascal Delpe-Brice,Latanya Sweeney,Jan Allebach,Ali Shakouri*

Key words: 隐私保护、数据共享、质量控制、机器学习、食品晶体

TL;DR: 论文介绍了一个隐私保护平台，帮助中小型制造商安全共享数据以开发创新工具，并通过食品晶体生产中的案例展示其应用。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 中小制造商因竞争和隐私问题不愿共享数据，阻碍了研究者的帮助。需要一个既能保护隐私又能促进创新的解决方案。

Method: 开发了一个隐私保护平台，允许制造商安全共享数据；通过机器学习开发了自动晶体分析工具，提升质量控制的效率和准确性。

Result: 成功开发并部署了一个基于网络的晶体分析工具，能够自动分析晶体大小分布和数量，同时去除样本制备中的瑕疵。

Conclusion: 隐私保护平台和配套工具解决了制造商的数据共享和操作效率问题，未来可扩展更多应用。

Abstract: Small- and medium-sized manufacturers need innovative data tools but, because
of competition and privacy concerns, often do not want to share their
proprietary data with researchers who might be interested in helping. This
paper introduces a privacy-preserving platform by which manufacturers may
safely share their data with researchers through secure methods, so that those
researchers then create innovative tools to solve the manufacturers' real-world
problems, and then provide tools that execute solutions back onto the platform
for others to use with privacy and confidentiality guarantees. We illustrate
this problem through a particular use case which addresses an important problem
in the large-scale manufacturing of food crystals, which is that quality
control relies on image analysis tools. Previous to our research, food crystals
in the images were manually counted, which required substantial and
time-consuming human efforts, but we have developed and deployed a crystal
analysis tool which makes this process both more rapid and accurate. The tool
enables automatic characterization of the crystal size distribution and numbers
from microscope images while the natural imperfections from the sample
preparation are automatically removed; a machine learning model to count high
resolution translucent crystals and agglomeration of crystals was also
developed to aid in these efforts. The resulting algorithm was then packaged
for real-world use on the factory floor via a web-based app secured through the
originating privacy-preserving platform, allowing manufacturers to use it while
keeping their proprietary data secure. After demonstrating this full process,
future directions are also explored.

</details>


### [143] [How to Securely Shuffle? A survey about Secure Shufflers for privacy-preserving computations](https://arxiv.org/abs/2507.01487)
*Marc Damie,Florian Hahn,Andreas Peter,Jan Ramon*

Key words: 安全洗牌器, 差分隐私, 隐私保护, 协议比较, 密码学

TL;DR: 该论文探讨了安全洗牌器（secure shuffler）的重要性，综述了26种实现洗牌功能的协议，并提出了统一的评估标准，同时展望了未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 研究动机源于对安全洗牌器在隐私保护中作用的重新关注，尤其是在差分隐私领域的应用，但现有研究多将其视为黑盒，忽略了实现中的漏洞和性能权衡。

Method: 通过识别、分类和比较26种安全协议，统一了已有的安全定义，形成了评估洗牌器性能的标准。

Result: 研究结果总结了对这些协议的系统性比较，并为使用洗牌器的隐私保护技术提供了指南。

Conclusion: 论文为安全洗牌器的设计提供了实用指南，并指出了未来的研究方向。

Abstract: Ishai et al. (FOCS'06) introduced secure shuffling as an efficient building
block for private data aggregation. Recently, the field of differential privacy
has revived interest in secure shufflers by highlighting the privacy
amplification they can provide in various computations. Although several works
argue for the utility of secure shufflers, they often treat them as black
boxes; overlooking the practical vulnerabilities and performance trade-offs of
existing implementations. This leaves a central question open: what makes a
good secure shuffler?
  This survey addresses that question by identifying, categorizing, and
comparing 26 secure protocols that realize the necessary shuffling
functionality. To enable a meaningful comparison, we adapt and unify existing
security definitions into a consistent set of properties. We also present an
overview of privacy-preserving technologies that rely on secure shufflers,
offer practical guidelines for selecting appropriate protocols, and outline
promising directions for future work.

</details>


### [144] [On the Effect of Ruleset Tuning and Data Imbalance on Explainable Network Security Alert Classifications: a Case-Study on DeepCASE](https://arxiv.org/abs/2507.01571)
*Koen T. W. Teuwen,Sam Baggen,Emmanuele Zambon,Luca Allodi*

Key words: 自动化, SOC, 标签不平衡, DeepCASE, 警报分类

TL;DR: 研究了标签不平衡对网络入侵警报分类的影响，发现调整SOC中的检测规则可以减少不平衡，从而提升自动分类的性能和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 自动化在SOC中的警报分类和事件升级中起重要作用，但需要解决数据不平衡和决策可解释性问题。

Method: 使用最先进的DeepCASE方法评估标签不平衡对警报分类的影响。

Result: 标签不平衡影响分类性能和解释的正确性；调整检测规则可以减少不平衡。

Conclusion: 改进输入数据质量的传统方法可以提升自动化性能。

Abstract: Automation in Security Operations Centers (SOCs) plays a prominent role in
alert classification and incident escalation. However, automated methods must
be robust in the presence of imbalanced input data, which can negatively affect
performance. Additionally, automated methods should make explainable decisions.
In this work, we evaluate the effect of label imbalance on the classification
of network intrusion alerts. As our use-case we employ DeepCASE, the
state-of-the-art method for automated alert classification. We show that label
imbalance impacts both classification performance and correctness of the
classification explanations offered by DeepCASE. We conclude tuning the
detection rules used in SOCs can significantly reduce imbalance and may benefit
the performance and explainability offered by alert post-processing methods
such as DeepCASE. Therefore, our findings suggest that traditional methods to
improve the quality of input data can benefit automation.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [145] [End-to-End Large Portfolio Optimization for Variance Minimization with Neural Networks through Covariance Cleaning](https://arxiv.org/abs/2507.01918)
*Christian Bongiorno,Efstratios Manolakis,Rosario Nunzio Mantegna*

Key words: 旋转不变神经网络，全局最小方差组合，协方差矩阵，正则化，金融模型

TL;DR: 提出一种旋转不变的神经网络，通过学习滞后变换历史收益和正则化大股权协方差矩阵的特征值及边际波动率，提供全局最小方差组合，兼具可解释性。

<details>
  <summary>Details</summary>

Main category: q-fin.PM

Motivation: 解决传统金融模型在处理高维数据时的局限性，提供一种可解释性强、泛化能力优的全局最小方差组合预测方法。

Method: 构建旋转不变的神经网络，联合学习滞后变换和正则化协方差矩阵的特征值及边际波动率，采用端到端优化未来实现的最小投资组合方差。

Result: 在2000年至2024年的测试中，模型比现有最佳分析方法（包括非线性收缩技术）表现出更低实现波动率、更小最大回撤和更高夏普比率。

Conclusion: 该模型不仅在无约束条件下表现优异，其协方差表示在长约束优化器中也能保持性能优势，适用于高维数据且适应性强。

Abstract: We develop a rotation-invariant neural network that provides the global
minimum-variance portfolio by jointly learning how to lag-transform historical
returns and how to regularise both the eigenvalues and the marginal
volatilities of large equity covariance matrices. This explicit mathematical
mapping offers clear interpretability of each module's role, so the model
cannot be regarded as a pure black-box. The architecture mirrors the analytical
form of the global minimum-variance solution yet remains agnostic to dimension,
so a single model can be calibrated on panels of a few hundred stocks and
applied, without retraining, to one thousand US equities-a cross-sectional jump
that demonstrates robust out-of-sample generalisation. The loss function is the
future realized minimum portfolio variance and is optimized end-to-end on real
daily returns. In out-of-sample tests from January 2000 to December 2024 the
estimator delivers systematically lower realised volatility, smaller maximum
drawdowns, and higher Sharpe ratios than the best analytical competitors,
including state-of-the-art non-linear shrinkage. Furthermore, although the
model is trained end-to-end to produce an unconstrained (long-short)
minimum-variance portfolio, we show that its learned covariance representation
can be used in general optimizers under long-only constraints with virtually no
loss in its performance advantage over competing estimators. These gains
persist when the strategy is executed under a highly realistic implementation
framework that models market orders at the auctions, empirical slippage,
exchange fees, and financing charges for leverage, and they remain stable
during episodes of acute market stress.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [146] [AI-guided digital intervention with physiological monitoring reduces intrusive memories after experimental trauma](https://arxiv.org/abs/2507.01081)
*Megan T. deBettencourt,Sruthi Sakthivel,Emily A. Holmes,Mark Chevillet*

Key words: 创伤, AI指导, 瞳孔测量, 数字治疗, 侵入性记忆

TL;DR: 研究发现，结合AI指导和瞳孔测量的ANTIDOTE方案能有效减少创伤后侵入性记忆，并提供可扩展的数字治疗。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 全球创伤问题普遍，但现有数字治疗需人工指导，限制了可扩展性。研究探索生成式AI和神经技术是否可提供解决方案。

Method: 通过ANTIDOTE方案，结合AI指导和瞳孔测量，自动提供并监测基于证据的数字治疗（ICTI）。100名健康志愿者参与实验。

Result: 干预组报告显著减少侵入性记忆，瞳孔大小反映干预效果并预测症状改善。AI指导符合临床标准。

Conclusion: ANTIDOTE方案为可扩展的AI指导数字干预提供新路径，适合应对广泛创伤问题。

Abstract: Trauma prevalence is vast globally. Evidence-based digital treatments can
help, but most require human guidance. Human guides provide tailored
instructions and responsiveness to internal cognitive states, but limit
scalability. Can generative AI and neurotechnology provide a scalable
alternative? Here we test ANTIDOTE, combining AI guidance and pupillometry to
automatically deliver and monitor an evidence-based digital treatment,
specifically the Imagery Competing Task Intervention (ICTI), to reduce
intrusive memories after psychological trauma. One hundred healthy volunteers
were exposed to videos of traumatic events and randomly assigned to an
intervention or active control condition. As predicted, intervention
participants reported significantly fewer intrusive memories over the following
week. Post-hoc assessment against clinical rubrics confirmed the AI guide
delivered the intervention successfully. Additionally, pupil size tracked
intervention engagement and predicted symptom reduction, providing a candidate
biomarker of intervention effectiveness. These findings open a path toward
rigorous AI-guided digital interventions that can scale to trauma prevalence.

</details>


### [147] [Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants](https://arxiv.org/abs/2507.01548)
*Wen Zhan,Ziqun Hua,Peiyue Lin,Yunfei Chen*

Key words: AI辅助共同创作,老年移民,叙事表达,汉字重构,社会技术系统

TL;DR: 年长移民通过AI辅助共同创作表达个人叙事，结合汉字重构与口头故事讲述，无需数字技能。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 探索老年移民如何利用AI辅助共同创作，表达碎片化、难以言喻的个人叙事。

Method: 采用工作坊形式，结合口头故事讲述与汉字重构（使用小篆字形），参与者通过物理材料与LLM建议共同创作。

Result: 参与者成功将生活经历转化为视觉与触觉表达，AI作为支持机制而非内容生产者。

Conclusion: 该方法为人类-AI协作及老龄化研究提供新视角，强调叙事能动性与社会技术系统的融合。

Abstract: This paper explores how older adults, particularly aging migrants in urban
China, can engage AI-assisted co-creation to express personal narratives that
are often fragmented, underrepresented, or difficult to verbalize. Through a
pilot workshop combining oral storytelling and the symbolic reconstruction of
Hanzi, participants shared memories of migration and recreated new character
forms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),
together with physical materials. Supported by human facilitation and a soft AI
presence, participants transformed lived experience into visual and tactile
expressions without requiring digital literacy. This approach offers new
perspectives on human-AI collaboration and aging by repositioning AI not as a
content producer but as a supportive mechanism, and by supporting narrative
agency within sociotechnical systems.

</details>


### [148] [AI Meets Maritime Training: Precision Analytics for Enhanced Safety and Performance](https://arxiv.org/abs/2507.01274)
*Vishakha Lall,Yisi Liu*

Key words: AI驱动培训, 视觉焦点跟踪, 语音识别, 压力检测, 海事安全

TL;DR: 本文开发了一个AI驱动的海员培训框架，通过视觉焦点跟踪、语音识别和压力检测，客观评估学员表现，提升高风险场景准备度。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 传统海员培训依赖主观评估，存在测量关键特征的困难性和认知局限性，需要一个更客观的方法。

Method: 结合AI技术，包括眼动追踪、瞳孔分析、计算机视觉、海事语音识别、自然语言处理和大型语言模型、声调分析等。

Result: AI算法在模拟海事场景中表现优异，视觉检测准确率约92%，语音识别约91%，压力检测约90%，超越现有基准。

Conclusion: 研究表明，AI可以通过客观性能分析和个性化反馈，显著提升海员培训效果和应对现实挑战的能力。

Abstract: Traditional simulator-based training for maritime professionals is critical
for ensuring safety at sea but often depends on subjective trainer assessments
of technical skills, behavioral focus, communication, and body language, posing
challenges such as subjectivity, difficulty in measuring key features, and
cognitive limitations. Addressing these issues, this study develops an
AI-driven framework to enhance maritime training by objectively assessing
trainee performance through visual focus tracking, speech recognition, and
stress detection, improving readiness for high-risk scenarios. The system
integrates AI techniques, including visual focus determination using eye
tracking, pupil dilation analysis, and computer vision; communication analysis
through a maritime-specific speech-to-text model and natural language
processing; communication correctness using large language models; and mental
stress detection via vocal pitch. Models were evaluated on data from simulated
maritime scenarios with seafarers exposed to controlled high-stress events. The
AI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for
maritime speech recognition, and ~90% for stress detection, surpassing existing
benchmarks. The system provides insights into visual attention, adherence to
communication checklists, and stress levels under demanding conditions. This
study demonstrates how AI can transform maritime training by delivering
objective performance analytics, enabling personalized feedback, and improving
preparedness for real-world operational challenges.

</details>


### [149] [Towards culturally-appropriate conversational AI for health in the majority world: An exploratory study with citizens and professionals in Latin America](https://arxiv.org/abs/2507.01719)
*Dorian Peters,Fernanda Espinoza,Marco da Re,Guido Ivetta,Luciana Benotti,Rafael A. Calvo*

Key words: 对话式人工智能；文化多样性；健康干预；拉丁美洲；本地化方法

TL;DR: 论文探讨了如何在全球多数国家中利用对话式人工智能（CAI）进行健康干预，特别关注文化多样性和本地化需求。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 当前的大型语言模型（LLM）忽略了全球许多实际生活经验，因此在文化和语言多样的环境中有效应用CAI需要新的方法。

Method: 通过参与式工作坊在拉丁美洲收集定性数据，采用自下而上的本地化方法，研究数字健康中的文化不一致问题、区域对健康聊天机器人的看法以及文化适应性策略。

Result: 研究发现，学术上的文化概念在实地中失去意义，技术需采用更广泛的框架，考虑经济、政治、地理和本地物流等因素。

Conclusion: 提出了“多元对话式人工智能健康框架”，强调更多关系性和包容性，而非仅仅依赖数据。

Abstract: There is justifiable interest in leveraging conversational AI (CAI) for
health across the majority world, but to be effective, CAI must respond
appropriately within culturally and linguistically diverse contexts. Therefore,
we need ways to address the fact that current LLMs exclude many lived
experiences globally. Various advances are underway which focus on top-down
approaches and increasing training data. In this paper, we aim to complement
these with a bottom-up locally-grounded approach based on qualitative data
collected during participatory workshops in Latin America. Our goal is to
construct a rich and human-centred understanding of: a) potential areas of
cultural misalignment in digital health; b) regional perspectives on chatbots
for health and c)strategies for creating culturally-appropriate CAI; with a
focus on the understudied Latin American context. Our findings show that
academic boundaries on notions of culture lose meaning at the ground level and
technologies will need to engage with a broader framework; one that
encapsulates the way economics, politics, geography and local logistics are
entangled in cultural experience. To this end, we introduce a framework for
'Pluriversal Conversational AI for Health' which allows for the possibility
that more relationality and tolerance, rather than just more data, may be
called for.

</details>


### [150] [Bridging UI Design and chatbot Interactions: Applying Form-Based Principles to Conversational Agents](https://arxiv.org/abs/2507.01862)
*Sanjay Krishna Anbalagan,Xinrui Nie,Umesh Mohan,Vijay Kumar Kanamarlapudi,Anughna Kommalapati,Xiaodan Zhao*

Key words: 聊天机器人, 多步交互, 上下文管理, 大型语言模型, 用户意图

TL;DR: 提出了在聊天机器人中模拟传统GUI的提交和重置操作，以提高多步任务处理的清晰度和用户满意度。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 解决聊天机器人在多步交互中因依赖语言暗示导致的上下文管理混乱问题。

Method: 提出将提交和重置操作建模为大型语言模型中的明确任务，并通过结构化会话数据记录用户意图。

Result: 在酒店预订和客户管理场景中证明了方法的有效性，提高了任务连贯性和用户满意度。

Conclusion: 将GUI操作引入聊天机器人可以显著改善多步交互的清晰度和效率。

Abstract: Domain specific chatbot applications often involve multi step interactions,
such as refining search filters, selecting multiple items, or performing
comparisons. Traditional graphical user interfaces (GUIs) handle these
workflows by providing explicit "Submit" (commit data) and "Reset" (discard
data) actions, allowing back-end systems to track user intent unambiguously. In
contrast, conversational agents rely on subtle language cues, which can lead to
confusion and incomplete context management. This paper proposes modeling these
GUI inspired metaphors acknowledgment (submit like) and context switching
(reset-like) as explicit tasks within large language model (LLM) prompts. By
capturing user acknowledgment, reset actions, and chain of thought (CoT)
reasoning as structured session data, we preserve clarity, reduce user
confusion, and align domain-specific chatbot interactions with back-end logic.
We demonstrate our approach in hotel booking and customer management scenarios,
highlighting improvements in multi-turn task coherence, user satisfaction, and
efficiency.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [151] [Prompt Mechanisms in Medical Imaging: A Comprehensive Survey](https://arxiv.org/abs/2507.01055)
*Hao Yang,Xinlong Liang,Zhang Li,Yue Sun,Zheyu Hu,Xinghe Xie,Behdad Dashtbozorg,Jincheng Huang,Shiwei Zhu,Luyi Han,Jiong Zhang,Shanshan Wang,Ritse Mann,Qifeng Yu,Tao Tan*

Key words: 深度学习，医学影像，提示工程，任务泛化，临床部署

TL;DR: 深度学习方法在医学影像中潜力巨大，但临床应用面临数据稀缺、分布偏移和任务泛化等挑战。提示（prompt）方法为模型提供了灵活、领域特定的指导，显著提升了性能和适应性，无需大量重新训练。本文系统综述了医学影像中的提示工程，分析了多种提示模态及其在核心任务中的应用，揭示了其通过提升准确性、鲁棒性和数据效率的机制，同时指出设计优化和临床部署等挑战。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 解决深度学习方法在医学影像中因数据稀缺、分布偏移和任务泛化不足带来的临床应用难题。

Method: 通过系统综述，分析文本指令、视觉提示和可学习嵌入等多种提示模态在图像生成、分割和分类任务中的集成应用。

Result: 提示机制显著提升了任务性能（如准确性、鲁棒性和数据效率），减少了对人工特征工程的依赖，并增强了模型的可解释性。

Conclusion: 提示驱动的AI在医学影像中潜力巨大，但仍需解决提示设计优化、数据异质性和临床部署等挑战。

Abstract: Deep learning offers transformative potential in medical imaging, yet its
clinical adoption is frequently hampered by challenges such as data scarcity,
distribution shifts, and the need for robust task generalization. Prompt-based
methodologies have emerged as a pivotal strategy to guide deep learning models,
providing flexible, domain-specific adaptations that significantly enhance
model performance and adaptability without extensive retraining. This
systematic review critically examines the burgeoning landscape of prompt
engineering in medical imaging. We dissect diverse prompt modalities, including
textual instructions, visual prompts, and learnable embeddings, and analyze
their integration for core tasks such as image generation, segmentation, and
classification. Our synthesis reveals how these mechanisms improve
task-specific outcomes by enhancing accuracy, robustness, and data efficiency
and reducing reliance on manual feature engineering while fostering greater
model interpretability by making the model's guidance explicit. Despite
substantial advancements, we identify persistent challenges, particularly in
prompt design optimization, data heterogeneity, and ensuring scalability for
clinical deployment. Finally, this review outlines promising future
trajectories, including advanced multimodal prompting and robust clinical
integration, underscoring the critical role of prompt-driven AI in accelerating
the revolution of diagnostics and personalized treatment planning in medicine.

</details>


### [152] [CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation](https://arxiv.org/abs/2506.23121)
*Xinlei Yu,Chanmiao Wang,Hui Jin,Ahmed Elazab,Gangyong Jia,Xiang Wan,Changqing Zou,Ruiquan Ge*

Key words: 多器官分割, 跨模态交互, 语义提示, 医学图像处理

TL;DR: CRISP-SAM2是一种基于SAM2的多器官医学分割模型，通过跨模态交互和语义提示解决现有模型中细节不准确、依赖几何提示和空间信息丢失的问题。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 现有多器官分割模型存在细节不准确、依赖几何提示和空间信息丢失的问题，需要改进以提高分割精度。

Method: 采用渐进式跨注意力交互机制将视觉和文本输入转换为跨模态上下文语义，并注入到图像编码器；使用语义提示策略消除几何依赖；应用相似性排序自更新记忆策略和掩码细化过程优化局部细节。

Result: 在七个公开数据集上的比较实验表明，CRISP-SAM2优于现有模型。

Conclusion: CRISP-SAM2在解决多器官分割中的关键问题上表现出色，具有显著性能优势。

Abstract: Multi-organ medical segmentation is a crucial component of medical image
processing, essential for doctors to make accurate diagnoses and develop
effective treatment plans. Despite significant progress in this field, current
multi-organ segmentation models often suffer from inaccurate details,
dependence on geometric prompts and loss of spatial information. Addressing
these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal
Interaction and Semantic Prompting based on SAM2. This model represents a
promising approach to multi-organ medical segmentation guided by textual
descriptions of organs. Our method begins by converting visual and textual
inputs into cross-modal contextualized semantics using a progressive
cross-attention interaction mechanism. These semantics are then injected into
the image encoder to enhance the detailed understanding of visual information.
To eliminate reliance on geometric prompts, we use a semantic prompting
strategy, replacing the original prompt encoder to sharpen the perception of
challenging targets. In addition, a similarity-sorting self-updating strategy
for memory and a mask-refining process is applied to further adapt to medical
imaging and enhance localized details. Comparative experiments conducted on
seven public datasets indicate that CRISP-SAM2 outperforms existing models.
Extensive analysis also demonstrates the effectiveness of our method, thereby
confirming its superior performance, especially in addressing the limitations
mentioned earlier. Our code is available at:
https://github.com/YU-deep/CRISP\_SAM2.git.

</details>


### [153] [SWinMamba: Serpentine Window State Space Model for Vascular Segmentation](https://arxiv.org/abs/2507.01323)
*Rongchang Zhao,Huanchi Liu,Jian Zhang*

Key words: 血管分割、蛇形窗口序列、双向状态空间模型、医学图像、SWinMamba

TL;DR: 提出了一种新的SWinMamba方法，通过蛇形窗口序列和双向状态空间模型实现血管的准确分割，解决了传统方法中血管结构不连续的问题。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 血管分割在医学图像中对疾病诊断和手术导航至关重要，但传统方法常因血管细长和先验建模不足导致分割结果不连续。

Method: 采用SWinMamba方法，结合蛇形窗口序列和双向状态空间模型；通过SWToken自适应分割输入图像，利用BAM整合局部特征，并通过SFFU增强特征表示。

Result: 在三个挑战性数据集上的实验表明，SWinMamba能够生成完整且连通的血管结构，性能优于其他方法。

Conclusion: SWinMamba通过创新的蛇形窗口序列和双向特征整合，显著提升了血管分割的准确性和连续性。

Abstract: Vascular segmentation in medical images is crucial for disease diagnosis and
surgical navigation. However, the segmented vascular structure is often
discontinuous due to its slender nature and inadequate prior modeling. In this
paper, we propose a novel Serpentine Window Mamba (SWinMamba) to achieve
accurate vascular segmentation. The proposed SWinMamba innovatively models the
continuity of slender vascular structures by incorporating serpentine window
sequences into bidirectional state space models. The serpentine window
sequences enable efficient feature capturing by adaptively guiding global
visual context modeling to the vascular structure. Specifically, the Serpentine
Window Tokenizer (SWToken) adaptively splits the input image using overlapping
serpentine window sequences, enabling flexible receptive fields (RFs) for
vascular structure modeling. The Bidirectional Aggregation Module (BAM)
integrates coherent local features in the RFs for vascular continuity
representation. In addition, dual-domain learning with Spatial-Frequency Fusion
Unit (SFFU) is designed to enhance the feature representation of vascular
structure. Extensive experiments on three challenging datasets demonstrate that
the proposed SWinMamba achieves superior performance with complete and
connected vessels.

</details>


### [154] [A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs](https://arxiv.org/abs/2507.01881)
*Niccolò McConnell,Pardeep Vasudev,Daisuke Yamada,Daryl Cheng,Mehran Azimbagirad,John McCabe,Shahab Aslani,Ahmed H. Shahin,Yukun Zhou,The SUMMIT Consortium,Andre Altmann,Yipeng Hu,Paul Taylor,Sam M. Janes,Daniel C. Alexander,Joseph Jacob*

Key words: 低剂量CT、肺癌筛查、开源模型、自监督学习、3D成像

TL;DR: TANGERINE是一个开源、计算高效的3D视觉基础模型，用于低剂量CT（LDCT）分析，支持快速微调，适用于多种疾病任务，性能优于传统方法，且资源需求低。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: LDCT在肺癌筛查中的广泛应用面临放射科医生不足的问题，需要高效、易用的自动化解决方案。

Method: TANGERINE基于自监督学习，使用超98,000例LDCT数据预训练，并扩展了掩码自动编码器框架至3D成像，支持快速微调和标签高效训练。

Result: 在14种疾病分类任务中达到最优性能，且对计算资源和数据需求低。

Conclusion: TANGERINE的开源轻量化设计为下一代医学影像工具提供了快速集成基础，有望从肺癌筛查扩展到全面呼吸系统疾病管理。

Abstract: Low-dose computed tomography (LDCT) imaging employed in lung cancer screening
(LCS) programs is increasing in uptake worldwide. LCS programs herald a
generational opportunity to simultaneously detect cancer and non-cancer-related
early-stage lung disease. Yet these efforts are hampered by a shortage of
radiologists to interpret scans at scale. Here, we present TANGERINE, a
computationally frugal, open-source vision foundation model for volumetric LDCT
analysis. Designed for broad accessibility and rapid adaptation, TANGERINE can
be fine-tuned off the shelf for a wide range of disease-specific tasks with
limited computational resources and training data. Relative to models trained
from scratch, TANGERINE demonstrates fast convergence during fine-tuning,
thereby requiring significantly fewer GPU hours, and displays strong label
efficiency, achieving comparable or superior performance with a fraction of
fine-tuning data. Pretrained using self-supervised learning on over 98,000
thoracic LDCTs, including the UK's largest LCS initiative to date and 27 public
datasets, TANGERINE achieves state-of-the-art performance across 14 disease
classification tasks, including lung cancer and multiple respiratory diseases,
while generalising robustly across diverse clinical centres. By extending a
masked autoencoder framework to 3D imaging, TANGERINE offers a scalable
solution for LDCT analysis, departing from recent closed, resource-intensive
models by combining architectural simplicity, public availability, and modest
computational requirements. Its accessible, open-source lightweight design lays
the foundation for rapid integration into next-generation medical imaging tools
that could transform LCS initiatives, allowing them to pivot from a singular
focus on lung cancer detection to comprehensive respiratory disease management
in high-risk populations.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [155] [Characterizing control between interacting subsystems with deep Jacobian estimation](https://arxiv.org/abs/2507.01946)
*Adam J. Eisen,Mitchell Ostrow,Sarthak Chandra,Leo Kozachkov,Earl K. Miller,Ila R. Fiete*

Key words: 非线性控制理论, JacobianODE, 生物子系统交互, 深度学习, 高维混沌

TL;DR: 论文提出了一种数据驱动的非线性控制理论框架，通过JacobianODE方法直接从时间序列数据中估计Jacobian，用于理解生物子系统间的交互与控制，并在高维混沌系统和RNN上展示了优越性。

<details>
  <summary>Details</summary>

Main category: q-bio.QM

Motivation: 现有方法多为线性，无法刻画非线性复杂系统的丰富交互特性，需要一种新方法来量化子系统间的控制关系。

Method: 提出JacobianODE，一种深度学习方法，利用Jacobian性质直接从数据中估计任意动态系统的Jacobian。

Result: JacobianODE在高维混沌系统上表现优于现有方法；在多区域RNN中，发现学习过程中“感觉”区域对“认知”区域控制增强。

Conclusion: 研究为生物子系统间交互的理论和数据驱动理解提供了基础，并能精确操控系统行为。

Abstract: Biological function arises through the dynamical interactions of multiple
subsystems, including those between brain areas, within gene regulatory
networks, and more. A common approach to understanding these systems is to
model the dynamics of each subsystem and characterize communication between
them. An alternative approach is through the lens of control theory: how the
subsystems control one another. This approach involves inferring the
directionality, strength, and contextual modulation of control between
subsystems. However, methods for understanding subsystem control are typically
linear and cannot adequately describe the rich contextual effects enabled by
nonlinear complex systems. To bridge this gap, we devise a data-driven
nonlinear control-theoretic framework to characterize subsystem interactions
via the Jacobian of the dynamics. We address the challenge of learning
Jacobians from time-series data by proposing the JacobianODE, a deep learning
method that leverages properties of the Jacobian to directly estimate it for
arbitrary dynamical systems from data alone. We show that JacobianODEs
outperform existing Jacobian estimation methods on challenging systems,
including high-dimensional chaos. Applying our approach to a multi-area
recurrent neural network (RNN) trained on a working memory selection task, we
show that the "sensory" area gains greater control over the "cognitive" area
over learning. Furthermore, we leverage the JacobianODE to directly control the
trained RNN, enabling precise manipulation of its behavior. Our work lays the
foundation for a theoretically grounded and data-driven understanding of
interactions among biological subsystems.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [156] [Symbolic identification of tensor equations in multidimensional physical fields](https://arxiv.org/abs/2507.01466)
*Tianyi Chen,Hao Yang,Wenjun Ma,Jun Zhang*

Key words: 数据驱动,张量方程,符号识别,遗传算法,维度一致性

TL;DR: 该论文提出了一种名为SITE的数据驱动框架，用于从数据中发现张量方程，结合了M-GEP思想和遗传信息保留策略，通过维度一致性检查和张量线性回归提高了效率和鲁棒性。

<details>
  <summary>Details</summary>

Main category: math-ph

Motivation: 现有的数据驱动方法多局限于标量方程，缺乏有效识别张量关系的技术。SITE旨在填补这一空白。

Method: SITE采用宿主-质粒结构表示张量方程，结合遗传信息保留策略、维度一致性检查和张量线性回归技术。

Result: 在合成数据和分子模拟数据中，SITE准确恢复了目标方程，展现出对噪声和小样本的鲁棒性。

Conclusion: SITE为数据驱动的张量方程发现提供了高效且鲁棒的工具，适用于多种流变条件。

Abstract: Recently, data-driven methods have shown great promise for discovering
governing equations from simulation or experimental data. However, most
existing approaches are limited to scalar equations, with few capable of
identifying tensor relationships. In this work, we propose a general
data-driven framework for identifying tensor equations, referred to as Symbolic
Identification of Tensor Equations (SITE). The core idea of SITE--representing
tensor equations using a host-plasmid structure--is inspired by the
multidimensional gene expression programming (M-GEP) approach. To improve the
robustness of the evolutionary process, SITE adopts a genetic information
retention strategy. Moreover, SITE introduces two key innovations beyond
conventional evolutionary algorithms. First, it incorporates a dimensional
homogeneity check to restrict the search space and eliminate physically invalid
expressions. Second, it replaces traditional linear scaling with a tensor
linear regression technique, greatly enhancing the efficiency of numerical
coefficient optimization. We validate SITE using two benchmark scenarios, where
it accurately recovers target equations from synthetic data, showing robustness
to noise and small sample sizes. Furthermore, SITE is applied to identify
constitutive relations directly from molecular simulation data, which are
generated without reliance on macroscopic constitutive models. It adapts to
both compressible and incompressible flow conditions and successfully
identifies the corresponding macroscopic forms, highlighting its potential for
data-driven discovery of tensor equation.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [157] [Customized Exploration of Landscape Features Driving Multi-Objective Combinatorial Optimization Performance](https://arxiv.org/abs/2507.01638)
*Ana Nikolikj,Gabriela Ochoa,Tome Eftimov*

Key words: 多目标优化,组合景观,C-PLOS-net,PLS,GSEMO,NSGA-II

TL;DR: 分析景观特征用于预测多目标组合优化算法性能，基于C-PLOS-net模型，针对不同rmnk-landscape比较PLS、GSEMO和NSGA-II。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 研究景观特征如何影响多目标组合优化算法的性能，以指导算法选择和改进。

Method: 利用C-PLOS-net模型提取景观特征，测试PLS、GSEMO和NSGA-II在不同rmnk-landscape上的性能。

Result: 发现了特定景观特征组合对算法性能的影响，并提供了针对特定landscape和算法的深入见解。

Conclusion: 景观特征分析有助于理解算法性能差异，为特定landscape选择最优算法提供依据。

Abstract: We present an analysis of landscape features for predicting the performance
of multi-objective combinatorial optimization algorithms. We consider features
from the recently proposed compressed Pareto Local Optimal Solutions Networks
(C-PLOS-net) model of combinatorial landscapes. The benchmark instances are a
set of rmnk-landscapes with 2 and 3 objectives and various levels of ruggedness
and objective correlation. We consider the performance of three algorithms --
Pareto Local Search (PLS), Global Simple EMO Optimizer (GSEMO), and
Non-dominated Sorting Genetic Algorithm (NSGA-II) - using the resolution and
hypervolume metrics. Our tailored analysis reveals feature combinations that
influence algorithm performance specific to certain landscapes. This study
provides deeper insights into feature importance, tailored to specific
rmnk-landscapes and algorithms.

</details>


### [158] [Comparing Optimization Algorithms Through the Lens of Search Behavior Analysis](https://arxiv.org/abs/2507.01668)
*Gjorgjina Cenikj,Gašper Petelin,Tome Eftimov*

Key words: 数值优化, 元启发式算法, 统计测试, 搜索行为, MEALPY

TL;DR: 研究探讨了统计测试在比较算法搜索行为上的应用，旨在解决新算法难以区分的批评。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 针对近期元启发式算法泛滥且创新性不足的问题，研究者希望通过统计方法区分算法。

Method: 利用交叉匹配统计测试比较多元分布，评估MEALPY库中114种算法的搜索结果。

Result: 发现并识别出具有相似搜索行为的算法。

Conclusion: 统计测试能有效区分算法行为，有助于筛选真正创新的算法。

Abstract: The field of numerical optimization has recently seen a surge in the
development of "novel" metaheuristic algorithms, inspired by metaphors derived
from natural or human-made processes, which have been widely criticized for
obscuring meaningful innovations and failing to distinguish themselves from
existing approaches. Aiming to address these concerns, we investigate the
applicability of statistical tests for comparing algorithms based on their
search behavior. We utilize the cross-match statistical test to compare
multivariate distributions and assess the solutions produced by 114 algorithms
from the MEALPY library. These findings are incorporated into an empirical
analysis aiming to identify algorithms with similar search behaviors.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [159] [A first-order method for nonconvex-nonconcave minimax problems under a local Kurdyka-Łojasiewicz condition](https://arxiv.org/abs/2507.01932)
*Zhaosong Lu,Xiangyuan Wang*

Key words: 极小极大问题，局部KL条件，Hölder光滑性，近端梯度法，复杂性分析

TL;DR: 研究了非凸非凹极小极大问题，其中内部最大化问题满足局部KL条件，提出了基于局部Hölder光滑性的不精确近端梯度法，并给出了计算近似稳定点的复杂性保证。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 传统的全局KL或PL条件通常过于严格且不适用于实际场景，本文通过局部KL条件拓宽了适用范围，但引入了新的分析挑战，如优化过程中KL区域的收缩可能导致更复杂的问题。

Method: 提出了一种不精确近端梯度法，通过KL结构化子问题计算极大函数的不精确梯度，解决了局部KL条件带来的复杂性。

Result: 证明了极大函数的局部Hölder光滑性，并在温和假设下建立了计算近似稳定点的复杂性保证。

Conclusion: 局部KL条件扩展了极小极大问题的适用范围，通过不精确近端梯度法能有效解决由此带来的挑战，并提供了理论保证。

Abstract: We study a class of nonconvex-nonconcave minimax problems in which the inner
maximization problem satisfies a local Kurdyka-{\L}ojasiewicz (KL) condition
that may vary with the outer minimization variable. In contrast to the global
KL or Polyak-{\L}ojasiewicz (PL) conditions commonly assumed in the literature
-- which are significantly stronger and often too restrictive in practice --
this local KL condition accommodates a broader range of practical scenarios.
However, it also introduces new analytical challenges. In particular, as an
optimization algorithm progresses toward a stationary point of the problem, the
region over which the KL condition holds may shrink, resulting in a more
intricate and potentially ill-conditioned landscape. To address this challenge,
we show that the associated maximal function is locally H\"older smooth.
Leveraging this key property, we develop an inexact proximal gradient method
for solving the minimax problem, where the inexact gradient of the maximal
function is computed by applying a proximal gradient method to a KL-structured
subproblem. Under mild assumptions, we establish complexity guarantees for
computing an approximate stationary point of the minimax problem.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [160] [Automated Vehicles Should be Connected with Natural Language](https://arxiv.org/abs/2507.01059)
*Xiangbo Gao,Keshu Wu,Hao Zhang,Kexin Tian,Yang Zhou,Zhengzhong Tu*

Key words: 多智能体协同驾驶, 自然语言通信, 意图与推理, 智能交通系统

TL;DR: 提出了基于自然语言的意图与推理通信，解决多智能体协同驾驶中的带宽效率、信息完整性和互操作性不足的问题。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 现有通信媒介（如原始传感器数据、神经网络特征和感知结果）在带宽效率、信息完整性和智能体互操作性方面存在局限，传统方法忽视了决策级融合。

Method: 采用自然语言进行显式意图和推理通信，平衡语义密度和通信带宽，适应实时条件，连接异构智能体平台。

Result: 实现了从被动感知数据共享到主动协调的转变，提升智能交通系统的安全性、效率和透明度。

Conclusion: 自然语言通信为多智能体协同驾驶提供了更高效的解决方案，弥补了传统方法的不足。

Abstract: Multi-agent collaborative driving promises improvements in traffic safety and
efficiency through collective perception and decision making. However, existing
communication media -- including raw sensor data, neural network features, and
perception results -- suffer limitations in bandwidth efficiency, information
completeness, and agent interoperability. Moreover, traditional approaches have
largely ignored decision-level fusion, neglecting critical dimensions of
collaborative driving. In this paper we argue that addressing these challenges
requires a transition from purely perception-oriented data exchanges to
explicit intent and reasoning communication using natural language. Natural
language balances semantic density and communication bandwidth, adapts flexibly
to real-time conditions, and bridges heterogeneous agent platforms. By enabling
the direct communication of intentions, rationales, and decisions, it
transforms collaborative driving from reactive perception-data sharing into
proactive coordination, advancing safety, efficiency, and transparency in
intelligent transportation systems.

</details>


### [161] [RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms](https://arxiv.org/abs/2507.01378)
*Ziyao Wang,Rongpeng Li,Sizhao Li,Yuming Xiang,Haiping Wang,Zhifeng Zhao,Honggang Zhang*

Key words: 无人机群, 多智能体强化学习, 大型语言模型, 角色适应, 协同导航

TL;DR: RALLY算法解决了无人机群协同导航中的语义沟通、角色适应性和决策效率问题，通过结合LLM和MARL，实现了更好的任务覆盖和泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 研究解决了传统多智能体强化学习在语义沟通和角色结构上的局限性，以及现有LLM框架缺乏在线学习能力的问题。

Method: 提出了一种基于LLM的语义决策框架和动态角色异构机制，并设计了Role-value混合网络来整合离线和在线策略。

Result: 实验表明，RALLY在任务覆盖、收敛速度和泛化能力上优于传统方法。

Conclusion: RALLY展示了在无人机群协作导航中的潜力，结合了语义推理和强化学习的优势。

Abstract: Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as
a critical research focus, and it typically requires the swarm to navigate
effectively while avoiding obstacles and achieving continuous coverage over
multiple mission targets. Although traditional Multi-Agent Reinforcement
Learning (MARL) approaches offer dynamic adaptability, they are hindered by the
semantic gap in numerical communication and the rigidity of homogeneous role
structures, resulting in poor generalization and limited task scalability.
Recent advances in Large Language Model (LLM)-based control frameworks
demonstrate strong semantic reasoning capabilities by leveraging extensive
prior knowledge. However, due to the lack of online learning and over-reliance
on static priors, these works often struggle with effective exploration,
leading to reduced individual potential and overall system performance. To
address these limitations, we propose a Role-Adaptive LLM-Driven Yoked
navigation algorithm RALLY. Specifically, we first develop an LLM-driven
semantic decision framework that uses structured natural language for efficient
semantic communication and collaborative reasoning. Afterward, we introduce a
dynamic role-heterogeneity mechanism for adaptive role switching and
personalized decision-making. Furthermore, we propose a Role-value Mixing
Network (RMIX)-based assignment strategy that integrates LLM offline priors
with MARL online policies to enable semi-offline training of role selection
strategies. Experiments in the Multi-Agent Particle Environment (MPE)
environment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY
outperforms conventional approaches in terms of task coverage, convergence
speed, and generalization, highlighting its strong potential for collaborative
navigation in agentic multi-UAV systems.

</details>


### [162] [Exploring Advanced LLM Multi-Agent Systems Based on Blackboard Architecture](https://arxiv.org/abs/2507.01701)
*Bochen Han,Songmao Zhang*

Key words: 黑板架构, 多代理系统, LLM, 动态决策, 资源效率

TL;DR: 本文提出将黑板架构引入LLM多代理系统，以优化信息共享、动态代理选择和任务执行，实验表明其性能优于现有方法且更节省资源。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 为解决多代理系统中信息共享不足和动态决策问题，引入黑板架构以提高协作效率和适应性。

Method: 采用黑板架构，代理角色多样，共享信息并根据黑板内容动态选择和执行代理操作，直至达成共识。

Result: 在常识、推理和数学数据集上，系统性能优于现有SOTA方法，且资源消耗更低。

Conclusion: 黑板架构有望支持更复杂和动态的问题求解，尤其是在缺乏明确结构或工作流的情况下。

Abstract: In this paper, we propose to incorporate the blackboard architecture into LLM
multi-agent systems (MASs) so that (1) agents with various roles can share all
the information and others' messages during the whole problem-solving process,
(2) agents that will take actions are selected based on the current content of
the blackboard, and (3) the selection and execution round is repeated until a
consensus is reached on the blackboard. We develop the first implementation of
this proposal and conduct experiments on commonsense knowledge, reasoning and
mathematical datasets. The results show that our system can be competitive with
the SOTA static and dynamic MASs by achieving the best average performance, and
at the same time manage to spend less tokens. Our proposal has the potential to
enable complex and dynamic problem-solving where well-defined structures or
workflows are unavailable.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [163] [Evaluating LLM Agent Collusion in Double Auctions](https://arxiv.org/abs/2507.01413)
*Kushal Agrawal,Verona Teo,Juan J. Vazquez,Sudarsh Kunnavakkam,Vishak Srikanth,Andy Liu*

Key words: 大型语言模型,合谋行为,市场代理,经济社会互动,伦理

TL;DR: 论文研究了大型语言模型（LLM）作为市场代理时可能产生的合谋行为，分析了沟通能力、模型选择和环境压力对合谋行为的影响。

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: 随着LLM在经济社会互动中的应用增加，识别其潜在的负面行为（如合谋）变得至关重要。

Method: 通过模拟连续双向拍卖市场，控制实验变量（如沟通能力、模型选择和环境压力）来研究LLM代理的合谋行为。

Result: 实验表明，直接沟通会增强合谋倾向，不同模型的合谋倾向存在差异，环境压力（如监管和紧迫感）也会影响合谋行为。

Conclusion: 研究结果强调了部署基于LLM的市场代理时需考虑的经济与伦理问题。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities as
autonomous agents with rapidly expanding applications in various domains. As
these agents increasingly engage in socioeconomic interactions, identifying
their potential for undesirable behavior becomes essential. In this work, we
examine scenarios where they can choose to collude, defined as secretive
cooperation that harms another party. To systematically study this, we
investigate the behavior of LLM agents acting as sellers in simulated
continuous double auction markets. Through a series of controlled experiments,
we analyze how parameters such as the ability to communicate, choice of model,
and presence of environmental pressures affect the stability and emergence of
seller collusion. We find that direct seller communication increases collusive
tendencies, the propensity to collude varies across models, and environmental
pressures, such as oversight and urgency from authority figures, influence
collusive behavior. Our findings highlight important economic and ethical
considerations for the deployment of LLM-based market agents.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [164] [Geometry-aware 4D Video Generation for Robot Manipulation](https://arxiv.org/abs/2507.01099)
*Zeyi Liu,Shuang Li,Eric Cousineau,Siyuan Feng,Benjamin Burchfiel,Shuran Song*

Key words: 4D视频生成,多视角一致性,机器人交互,几何监督

TL;DR: 提出了一种4D视频生成模型，通过跨视角点云对齐监督实现多视角3D一致性，无需相机姿态输入即可预测新视角的未来视频序列。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 增强机器人在复杂环境中的规划和交互能力，解决现有视频生成模型在时间连贯性和几何一致性上的不足。

Method: 使用跨视角点云对齐监督训练模型，学习共享的3D场景表示，基于RGB-D观测预测未来视频序列。

Result: 在模拟和真实机器人数据集上生成更稳定且空间对齐的视频，可用于恢复机器人末端执行器轨迹。

Conclusion: 模型支持机器人操作的鲁棒性和对新相机视角的泛化能力。

Abstract: Understanding and predicting the dynamics of the physical world can enhance a
robot's ability to plan and interact effectively in complex environments. While
recent video generation models have shown strong potential in modeling dynamic
scenes, generating videos that are both temporally coherent and geometrically
consistent across camera views remains a significant challenge. To address
this, we propose a 4D video generation model that enforces multi-view 3D
consistency of videos by supervising the model with cross-view pointmap
alignment during training. This geometric supervision enables the model to
learn a shared 3D representation of the scene, allowing it to predict future
video sequences from novel viewpoints based solely on the given RGB-D
observations, without requiring camera poses as inputs. Compared to existing
baselines, our method produces more visually stable and spatially aligned
predictions across multiple simulated and real-world robotic datasets. We
further show that the predicted 4D videos can be used to recover robot
end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting
robust robot manipulation and generalization to novel camera viewpoints.

</details>


### [165] [Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence](https://arxiv.org/abs/2507.01504)
*Robert Aufschläger,Youssef Shoeb,Azarm Nowzad,Michael Heigl,Fabian Bally,Martin Schramm*

Key words: PII, 行人再识别, 视觉语言模型, 图注意力网络, 隐私保护

TL;DR: 提出了一种跨模态框架cRID，用于检测行人图像数据集中的PII并提升行人再识别性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决街景数据开放中的隐私风险，尤其是超出生物特征的PII问题。

Method: 结合大型视觉语言模型、图注意力网络和表示学习，检测可描述的PII特征。

Result: 在跨数据集行人再识别任务中（Market-1501到CUHK03-np）表现提升。

Conclusion: cRID框架有效检测语义上的PII，提升实用性和隐私保护。

Abstract: The collection and release of street-level recordings as Open Data play a
vital role in advancing autonomous driving systems and AI research. However,
these datasets pose significant privacy risks, particularly for pedestrians,
due to the presence of Personally Identifiable Information (PII) that extends
beyond biometric traits such as faces. In this paper, we present cRID, a novel
cross-modal framework combining Large Vision-Language Models, Graph Attention
Networks, and representation learning to detect textual describable clues of
PII and enhance person re-identification (Re-ID). Our approach focuses on
identifying and leveraging interpretable features, enabling the detection of
semantically meaningful PII beyond low-level appearance cues. We conduct a
systematic evaluation of PII presence in person image datasets. Our experiments
show improved performance in practical cross-dataset Re-ID scenarios, notably
from Market-1501 to CUHK03-np (detected), highlighting the framework's
practical utility. Code is available at https://github.com/RAufschlaeger/cRID.

</details>


### [166] [ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving](https://arxiv.org/abs/2507.01735)
*Kai Chen,Ruiyuan Gao,Lanqing Hong,Hang Xu,Xu Jia,Holger Caesar,Dengxin Dai,Bingbing Liu,Dzmitry Tsishkou,Songcen Xu,Chunjing Xu,Qiang Xu,Huchuan Lu,Dit-Yan Yeung*

Key words: 自动驾驶,极端场景,多模态感知,理解与生成,W-CODA

TL;DR: 第一届W-CODA研讨会聚焦自动驾驶极端场景的下一代解决方案，结合前沿多模态感知技术，汇集学术界与工业界专家的最新进展。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 推动自动驾驶技术在极端场景下的可靠性和智能化发展。

Method: 通过邀请5位学术与工业界专家分享最新研究，举办双轨挑战赛（场景理解与生成）。

Result: 研讨会为前沿自动驾驶技术与可靠性智能体的发展搭建桥梁。

Conclusion: W-CODA将持续推动自动驾驶极端场景领域的研究与应用。

Abstract: In this paper, we present details of the 1st W-CODA workshop, held in
conjunction with the ECCV 2024. W-CODA aims to explore next-generation
solutions for autonomous driving corner cases, empowered by state-of-the-art
multimodal perception and comprehension techniques. 5 Speakers from both
academia and industry are invited to share their latest progress and opinions.
We collect research papers and hold a dual-track challenge, including both
corner case scene understanding and generation. As the pioneering effort, we
will continuously bridge the gap between frontier autonomous driving techniques
and fully intelligent, reliable self-driving agents robust towards corner
cases.

</details>


### [167] [Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound](https://arxiv.org/abs/2507.01401)
*Huanwen Liang,Jingxian Xu,Yuanji Zhang,Yuhao Huang,Yuhan Zhang,Xin Yang,Ran Li,Xuedong Deng,Yanjun Liu,Guowei Tao,Yun Wu,Sheng Zhao,Xinru Gao,Dong Ni*

Key words: 胎儿腹部畸形、多实例学习、超声诊断、医学知识驱动、病例级分类

TL;DR: 本文提出了一种基于多实例学习（MIL）的方法，用于无标准平面定位的胎儿腹部异常分类，通过混合注意力专家模块（MoAE）、医学知识驱动的特征选择模块（MFS）和提示原型学习（PPL）提升性能，在大规模数据集上表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 胎儿腹部畸形是严重先天异常，需准确诊断以指导妊娠管理并降低死亡率。目前AI在产前腹部异常诊断中的应用仍有限，且多数研究集中于图像级分类，缺乏病例级诊断。

Method: 提出基于MIL的病例级分类方法，包括MoAE模块加权不同平面注意力头，MFS模块通过医学知识对齐图像特征，PPL模块增强MFS。

Result: 在包含2,419例病例、24,748张图像的数据集上验证，方法优于现有技术。

Conclusion: 该方法为胎儿腹部异常分类提供了高效解决方案，代码已开源。

Abstract: Fetal abdominal malformations are serious congenital anomalies that require
accurate diagnosis to guide pregnancy management and reduce mortality. Although
AI has demonstrated significant potential in medical diagnosis, its application
to prenatal abdominal anomalies remains limited. Most existing studies focus on
image-level classification and rely on standard plane localization, placing
less emphasis on case-level diagnosis. In this paper, we develop a case-level
multiple instance learning (MIL)-based method, free of standard plane
localization, for classifying fetal abdominal anomalies in prenatal ultrasound.
Our contribution is three-fold. First, we adopt a mixture-of-attention-experts
module (MoAE) to weight different attention heads for various planes. Secondly,
we propose a medical-knowledge-driven feature selection module (MFS) to align
image features with medical knowledge, performing self-supervised image token
selection at the case-level. Finally, we propose a prompt-based prototype
learning (PPL) to enhance the MFS. Extensively validated on a large prenatal
abdominal ultrasound dataset containing 2,419 cases, with a total of 24,748
images and 6 categories, our proposed method outperforms the state-of-the-art
competitors. Codes are available at:https://github.com/LL-AC/AAcls.

</details>


### [168] [DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal](https://arxiv.org/abs/2507.01422)
*Wenjie Liu,Bingshu Wang,Ze Wang,C. L. Philip Chen*

Key words: 文档阴影去除, 扩散模型, 彩色阴影, 潜在空间, 合成数据集

TL;DR: 本文提出了一种名为DocShaDiffusion的潜在空间扩散模型，用于文档图像阴影去除，特别针对彩色阴影问题，设计了阴影软掩膜生成模块（SSGM）和阴影掩膜引导扩散模块（SMGDM），并提出了一种阴影鲁棒的感知特征损失和合成数据集SDCSRD。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的文档阴影去除方法通常忽略彩色阴影，本文旨在解决这一问题，并通过潜在空间建模和掩膜引导扩散提升阴影去除效果。

Method: 设计了DocShaDiffusion模型，将阴影图像从像素空间转换到潜在空间；提出了SSGM和SMGDM模块分别用于生成精确的阴影掩膜和引导扩散去阴影过程；引入了阴影鲁棒的感知特征损失；开发了合成数据集SDCSRD。

Result: 在三个公共数据集上的实验验证了方法的优越性，优于当前最先进的技术。

Conclusion: DocShaDiffusion通过潜在空间建模和掩膜引导扩散有效解决了文档彩色阴影去除问题，同时合成的数据集为模型训练提供了有力支持。

Abstract: Document shadow removal is a crucial task in the field of document image
enhancement. However, existing methods tend to remove shadows with constant
color background and ignore color shadows. In this paper, we first design a
diffusion model in latent space for document image shadow removal, called
DocShaDiffusion. It translates shadow images from pixel space to latent space,
enabling the model to more easily capture essential features. To address the
issue of color shadows, we design a shadow soft-mask generation module (SSGM).
It is able to produce accurate shadow mask and add noise into shadow regions
specially. Guided by the shadow mask, a shadow mask-aware guided diffusion
module (SMGDM) is proposed to remove shadows from document images by
supervising the diffusion and denoising process. We also propose a
shadow-robust perceptual feature loss to preserve details and structures in
document images. Moreover, we develop a large-scale synthetic document color
shadow removal dataset (SDCSRD). It simulates the distribution of realistic
color shadows and provides powerful supports for the training of models.
Experiments on three public datasets validate the proposed method's superiority
over state-of-the-art. Our code and dataset will be publicly available.

</details>


### [169] [NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation](https://arxiv.org/abs/2507.01463)
*Max Gandyra,Alessandro Santonicola,Michael Beetz*

Key words: 实例分割, 新对象, Grounded-SAM 2, DINOv2, BOP 2023

TL;DR: NOCTIS是一种新型实例分割框架，无需重新训练即可处理各类新对象，利用Grounded-SAM 2和DINOv2模型，通过改进的匹配方法在BOP 2023挑战中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决无需重新训练的通用实例分割模型难以设计的问题，改进现有方法如CNOS和SAM-6D。

Method: 结合Grounded-SAM 2获取对象提议和分割掩码，利用DINOv2的零样本能力生成嵌入。通过改进的嵌入相似性和循环阈值过滤优化匹配分数。

Result: 在BOP 2023挑战的七大数据集上，NOCTIS在RGB和RGB-D方法中表现最佳。

Conclusion: NOCTIS通过创新匹配机制和嵌入优化，在不微调的情况下显著提升性能。

Abstract: Instance segmentation of novel objects instances in RGB images, given some
example images for each object, is a well known problem in computer vision.
Designing a model general enough to be employed, for all kinds of novel
objects, without (re-) training, has proven to be a difficult task. To handle
this, we propose a simple, yet powerful, framework, called: Novel Object Cyclic
Threshold based Instance Segmentation (NOCTIS). This work stems from and
improves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also
leverages on recent vision foundation models, namely: Grounded-SAM 2 and
DINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise
bounding boxes and their corresponding segmentation masks; while DINOv2's
zero-shot capabilities are employed to generate the image embeddings. The
quality of those masks, together with their embeddings, is of vital importance
to our approach; as the proposal-object matching is realized by determining an
object matching score based on the similarity of the class embeddings and the
average maximum similarity of the patch embeddings. Differently to SAM-6D,
calculating the latter involves a prior patch filtering based on the distance
between each patch and its corresponding cyclic/roundtrip patch in the image
grid. Furthermore, the average confidence of the proposals' bounding box and
mask is used as an additional weighting factor for the object matching score.
We empirically show that NOCTIS, without further training/fine tuning,
outperforms the best RGB and RGB-D methods on the seven core datasets of the
BOP 2023 challenge for the "Model-based 2D segmentation of unseen objects"
task.

</details>


### [170] [Crop Pest Classification Using Deep Learning Techniques: A Review](https://arxiv.org/abs/2507.01494)
*Muhammad Hassam Ejaz,Muhammad Bilal,Usman Habib*

Key words: 深度学习, 害虫检测, CNN, ViT, 混合模型

TL;DR: 这篇综述分析了2018年至2025年间37项关于基于AI的害虫分类研究，探讨了技术发展、数据集使用及关键挑战。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统害虫监测方法效率低且难以扩展，而深度学习技术（如CNNs、ViTs和混合模型）为自动化害虫检测提供了强大解决方案。

Method: 研究按作物类型、害虫种类、模型架构、数据集使用和技术挑战分类，比较了CNN和最新混合模型、Transformer模型的性能。

Result: 最新研究表明，混合模型和Transformer模型在准确性和上下文理解上优于传统CNN，但仍面临数据集不平衡、小害虫检测难、泛化能力有限等技术挑战。

Conclusion: 综述提供了AI害虫监测领域的结构化概述，强调了可用数据集，并指出了未来研究的关键挑战和方向。

Abstract: Insect pests continue to bring a serious threat to crop yields around the
world, and traditional methods for monitoring them are often slow, manual, and
difficult to scale. In recent years, deep learning has emerged as a powerful
solution, with techniques like convolutional neural networks (CNNs), vision
transformers (ViTs), and hybrid models gaining popularity for automating pest
detection. This review looks at 37 carefully selected studies published between
2018 and 2025, all focused on AI-based pest classification. The selected
research is organized by crop type, pest species, model architecture, dataset
usage, and key technical challenges. The early studies relied heavily on CNNs
but latest work is shifting toward hybrid and transformer-based models that
deliver higher accuracy and better contextual understanding. Still, challenges
like imbalanced datasets, difficulty in detecting small pests, limited
generalizability, and deployment on edge devices remain significant hurdles.
Overall, this review offers a structured overview of the field, highlights
useful datasets, and outlines the key challenges and future directions for
AI-based pest monitoring systems.

</details>


### [171] [Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images](https://arxiv.org/abs/2507.01502)
*Ozan Durgut,Beril Kallfelz-Sirmacek,Cem Unsalan*

Key words: 树冠检测, 传统方法, 深度学习方法, 规则化方法, 遥感监测

TL;DR: 该研究提出了一种结合传统方法和深度学习的规则化方法，用于提高树冠检测的鲁棒性和准确性，并通过后处理增加检测到的树冠数量。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 全球变暖、生物多样性丧失和空气污染等问题亟需解决，而森林监测是其中的关键挑战之一。利用遥感和计算机视觉技术实现自动化监测成为重要手段。

Method: 1. 分别采用传统方法和深度学习方法进行树冠检测；2. 提出一种规则化方法，结合两种方法以增强检测的鲁棒性和准确性；3. 通过后处理（如邻近树和局部化操作）提高检测数量。

Result: 提出的规则化方法有效增加了检测到的树冠数量，并展示了与传统和深度学习方法相比的优势和改进空间。

Conclusion: 结合多种方法的规则化策略能够显著提升树冠检测的性能，为森林监测提供更高效的解决方案。

Abstract: Global warming, loss of biodiversity, and air pollution are among the most
significant problems facing Earth. One of the primary challenges in addressing
these issues is the lack of monitoring forests to protect them. To tackle this
problem, it is important to leverage remote sensing and computer vision methods
to automate monitoring applications. Hence, automatic tree crown detection
algorithms emerged based on traditional and deep learning methods. In this
study, we first introduce two different tree crown detection methods based on
these approaches. Then, we form a novel rule-based approach that integrates
these two methods to enhance robustness and accuracy of tree crown detection
results. While traditional methods are employed for feature extraction and
segmentation of forested areas, deep learning methods are used to detect tree
crowns in our method. With the proposed rule-based approach, we post-process
these results, aiming to increase the number of detected tree crowns through
neighboring trees and localized operations. We compare the obtained results
with the proposed method in terms of the number of detected tree crowns and
report the advantages, disadvantages, and areas for improvement of the obtained
outcomes.

</details>


### [172] [Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring](https://arxiv.org/abs/2507.01590)
*Ameer Hamza,Zuhaib Hussain But,Umar Arif,Samiya,M. Abdullah Asad,Muhammad Naeem*

Key words: 课堂监控, 多模态, YOLOv8, 人脸识别, 实时监测

TL;DR: 该研究提出了一种多模态课堂监控系统，结合了YOLOv8和LResNet Occ FC等技术，用于实时评估学生注意力。结果显示出高精度，适用于教育环境。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 旨在通过多模态技术提升课堂监控的精确性和自动化，以改善学生行为和考勤管理。

Method: 使用YOLOv8检测手机和睡眠行为，LResNet Occ FC进行人脸识别，结合ESP32-CAM硬件和PHP实现实时监控。

Result: 睡眠检测mAP@50达97.42%，人脸识别准确率86.45%，手机检测mAP@50为85.89%。

Conclusion: 该系统高效且可扩展，适用于多样化教育场景。

Abstract: This study presents a novel classroom surveillance system that integrates
multiple modalities, including drowsiness, tracking of mobile phone usage, and
face recognition,to assess student attentiveness with enhanced precision.The
system leverages the YOLOv8 model to detect both mobile phone and sleep
usage,(Ghatge et al., 2024) while facial recognition is achieved through
LResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These
models work in synergy to provide comprehensive, real-time monitoring, offering
insights into student engagement and behavior.(S et al., 2023) The framework is
trained on specialized datasets, such as the RMFD dataset for face recognition
and a Roboflow dataset for mobile phone detection. The extensive evaluation of
the system shows promising results. Sleep detection achieves 97. 42% mAP@50,
face recognition achieves 86. 45% validation accuracy and mobile phone
detection reach 85. 89% mAP@50. The system is implemented within a core PHP web
application and utilizes ESP32-CAM hardware for seamless data capture.(Neto et
al., 2024) This integrated approach not only enhances classroom monitoring, but
also ensures automatic attendance recording via face recognition as students
remain seated in the classroom, offering scalability for diverse educational
environments.(Banada,2025)

</details>


### [173] [Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems](https://arxiv.org/abs/2507.01607)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi,Eric Bourbao*

Key words: 深度学习, 人脸识别, 后门攻击, 安全性, 特征提取器

TL;DR: 本文对深度学习人脸识别系统中的后门攻击进行了首次系统级研究，提出了四种贡献，包括两种人脸检测任务的后门攻击方法，并验证了大间隔损失训练的特征提取器同样易受攻击。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 深度学习人脸识别的广泛应用带来了安全隐患，但现有文献对真实场景下无约束系统的后门攻击研究仍不足。

Method: 通过探索DNN后门在整个人脸识别流程中的可行性，提出了两种人脸检测任务的后门攻击（人脸生成和关键点偏移），并测试了大间隔损失训练的特征提取器的脆弱性。

Result: 在20种可能的流程配置和15种攻击案例中，证明了单一后门可以绕过系统功能，同时提出了最佳实践和防护措施。

Conclusion: 研究表明深度学习人脸识别系统存在严重后门漏洞，并提出了一些防护建议。

Abstract: The widespread use of deep learning face recognition raises several security
concerns. Although prior works point at existing vulnerabilities, DNN backdoor
attacks against real-life, unconstrained systems dealing with images captured
in the wild remain a blind spot of the literature. This paper conducts the
first system-level study of backdoors in deep learning-based face recognition
systems. This paper yields four contributions by exploring the feasibility of
DNN backdoors on these pipelines in a holistic fashion. We demonstrate for the
first time two backdoor attacks on the face detection task: face generation and
face landmark shift attacks. We then show that face feature extractors trained
with large margin losses also fall victim to backdoor attacks. Combining our
models, we then show using 20 possible pipeline configurations and 15 attack
cases that a single backdoor enables an attacker to bypass the entire function
of a system. Finally, we provide stakeholders with several best practices and
countermeasures.

</details>


### [174] [Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss](https://arxiv.org/abs/2507.01630)
*Yuxiao Wang,Yu Lei,Zhenao Wei,Weiying Xue,Xinyu Jiang,Nan Zhuang,Qi Liu*

Key words: 人类-物体接触检测，提示引导，近端感知，深度感知，RJLOSS

TL;DR: P3HOT框架提出了一种结合提示引导和人类近端感知的方法，用于改进人类-物体接触检测任务，解决了现有方法在区域分割和类别一致性上的不足。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前的人类-物体接触检测模型仅限于单一图像类型，导致交互区域分割过多或一致性不足，P3HOT旨在解决这些问题。

Method: P3HOT结合语义驱动的提示机制和人类近端感知机制，通过动态感知关键深度范围和新型损失函数（RJLOSS）优化检测结果。

Result: 在HOT-Annotated数据集上，P3HOT在四个指标上实现了显著的性能提升（0.7↑, 2.0↑, 1.6↑, 11.0↑）。

Conclusion: P3HOT通过引入提示引导和深度感知，显著提升了人类-物体接触检测的性能和鲁棒性。

Abstract: The task of Human-Object conTact (HOT) detection involves identifying the
specific areas of the human body that are touching objects. Nevertheless,
current models are restricted to just one type of image, often leading to too
much segmentation in areas with little interaction, and struggling to maintain
category consistency within specific regions. To tackle this issue, a HOT
framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt
guidance and human \textbf{P}roximal \textbf{P}erception. To begin with, we
utilize a semantic-driven prompt mechanism to direct the network's attention
towards the relevant regions based on the correlation between image and text.
Then a human proximal perception mechanism is employed to dynamically perceive
key depth range around the human, using learnable parameters to effectively
eliminate regions where interactions are not expected. Calculating depth
resolves the uncertainty of the overlap between humans and objects in a 2D
perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss
(RJLoss) has been created as a new loss to inhibit abnormal categories in the
same area. A new evaluation metric called ``AD-Acc.'' is introduced to address
the shortcomings of existing methods in addressing negative samples.
Comprehensive experimental results demonstrate that our approach achieves
state-of-the-art performance in four metrics across two benchmark datasets.
Specifically, our model achieves an improvement of \textbf{0.7}$\uparrow$,
\textbf{2.0}$\uparrow$, \textbf{1.6}$\uparrow$, and \textbf{11.0}$\uparrow$ in
SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated
dataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.

</details>


### [175] [Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation](https://arxiv.org/abs/2507.01631)
*Camille Billouard,Dawa Derksen,Alexandre Constantin,Bruno Vallet*

Key words: NeRF, 3D重建, 大规模场景, 分块处理, 卫星图像

TL;DR: 本文提出了Snake-NeRF框架，用于解决传统NeRF方法在大场景3D重建中的内存限制问题，通过分块处理和创新的采样策略实现高效单设备训练。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决NeRF在大场景3D重建中因内存限制而无法扩展的问题。

Method: 采用分块策略（3D tiles）和图像重叠裁剪，结合2x2 3D tile渐进策略和分段采样器，避免边缘重建错误。

Result: 实验证明，该方法能在单GPU上线性时间内高效处理大尺度卫星图像，且不损失重建质量。

Conclusion: Snake-NeRF成功扩展了NeRF的应用范围，使其适用于大场景3D重建。

Abstract: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images and networks simultaneously, and
operates on a single device. We achieve this by dividing the region of interest
into NeRFs that 3D tile without overlap. Importantly, we crop the images with
overlap to ensure each NeRFs is trained with all the necessary pixels. We
introduce a novel $2\times 2$ 3D tile progression strategy and segmented
sampler, which together prevent 3D reconstruction errors along the tile edges.
Our experiments conclude that large satellite images can effectively be
processed with linear time complexity, on a single GPU, and without compromise
in quality.

</details>


### [176] [Depth Anything at Any Condition](https://arxiv.org/abs/2507.01634)
*Boyuan Sun,Modi Jin,Bowen Yin,Qibin Hou*

Key words: 单目深度估计，无监督学习，一致性正则化，空间距离约束

TL;DR: 深度估计模型DepthAnything-AC能够在不同环境条件下进行单目深度估计，通过无监督一致性正则化微调范式解决数据稀缺问题，并提出空间距离约束提升语义边界和细节准确性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有基础模型在复杂开放世界环境中表现不佳，特别是在光照变化、恶劣天气和传感器畸变条件下。

Method: 采用无监督一致性正则化微调范式，并引入空间距离约束以增强模型学习能力。

Result: 在多样化基准测试中表现出零样本能力，包括真实恶劣天气、合成畸变和通用场景。

Conclusion: DepthAnything-AC在复杂环境中展现出优异的深度估计能力，为单目深度估计提供了新解决方案。

Abstract: We present Depth Anything at Any Condition (DepthAnything-AC), a foundation
monocular depth estimation (MDE) model capable of handling diverse
environmental conditions. Previous foundation MDE models achieve impressive
performance across general scenes but not perform well in complex open-world
environments that involve challenging conditions, such as illumination
variations, adverse weather, and sensor-induced distortions. To overcome the
challenges of data scarcity and the inability of generating high-quality
pseudo-labels from corrupted images, we propose an unsupervised consistency
regularization finetuning paradigm that requires only a relatively small amount
of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to
explicitly enforce the model to learn patch-level relative relationships,
resulting in clearer semantic boundaries and more accurate details.
Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC
across diverse benchmarks, including real-world adverse weather benchmarks,
synthetic corruption benchmarks, and general benchmarks.
  Project Page: https://ghost233lism.github.io/depthanything-AC-page
  Code: https://github.com/HVision-NKU/DepthAnythingAC

</details>


### [177] [Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective](https://arxiv.org/abs/2507.01652)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Hui Deng,Xuyang Shen,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Key words: 自回归模型, 图像生成, 线性注意力, 空间关系, 计算效率

TL;DR: 论文提出一种名为LASAD的新型注意力机制，通过保留图像序列中的2D空间关系，解决了传统线性注意力机制在图像生成中无法捕捉长距离依赖的问题，并结合该机制开发了LASADGen，实现了高效的图像生成。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的自回归图像生成模型主要依赖Transformer架构，但其二次计算复杂度和高内存开销限制了效率。线性注意力机制虽然在语言模型中有效，但在图像生成中会因无法捕捉长距离依赖而影响质量。因此，论文旨在解决这一问题。

Method: 提出Linear Attention with Spatial-Aware Decay (LASAD)，通过基于真实2D空间位置计算位置相关的衰减因子，保留图像序列中的空间关系，并结合该机制开发了LASADGen。

Result: 在ImageNet上的实验表明，LASADGen在图像生成质量和计算效率上达到了领先水平，弥补了线性注意力机制在高质量生成中的不足。

Conclusion: LASADGen成功结合了线性注意力的高效性和对空间关系的理解，为高效且高质量的图像生成提供了新方法。

Abstract: Autoregressive (AR) models have garnered significant attention in image
generation for their ability to effectively capture both local and global
structures within visual data. However, prevalent AR models predominantly rely
on the transformer architectures, which are beset by quadratic computational
complexity concerning input sequence length and substantial memory overhead due
to the necessity of maintaining key-value caches. Although linear attention
mechanisms have successfully reduced this burden in language models, our
initial experiments reveal that they significantly degrade image generation
quality because of their inability to capture critical long-range dependencies
in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a
novel attention mechanism that explicitly preserves genuine 2D spatial
relationships within the flattened image sequences by computing
position-dependent decay factors based on true 2D spatial location rather than
1D sequence positions. Based on this mechanism, we present LASADGen, an
autoregressive image generator that enables selective attention to relevant
spatial contexts with linear complexity. Experiments on ImageNet show LASADGen
achieves state-of-the-art image generation performance and computational
efficiency, bridging the gap between linear attention's efficiency and spatial
understanding needed for high-quality generation.

</details>


### [178] [Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions](https://arxiv.org/abs/2507.01123)
*Rahul A. Burange,Harsh K. Shinde,Omkar Mutyalwar*

Key words: 滑坡检测,深度学习,多源遥感,Sentinel-2,ALOS PALSAR,灾害管理

TL;DR: 该研究提出了一种结合多源卫星影像和深度学习模型的综合方法，以提升滑坡识别和预测能力，利用Sentinel-2多光谱数据和ALOS PALSAR生成的坡度和DEM图层，评估多种深度学习模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 滑坡对基础设施、经济和人类生命构成严重威胁，准确的检测和预测至关重要，特别是在不同地理区域。

Method: 整合多源卫星影像（Sentinel-2和ALOS PALSAR数据）和深度学习模型（如U-Net、DeepLabV3+和Res-Net），运用地理空间分析技术评估地形特征、植被覆盖和降雨对检测精度的影响。

Result: 研究为开发可靠的早期预警系统、改进灾害风险管理和可持续土地利用规划提供了支持。

Conclusion: 深度学习与多源遥感结合在构建健壮、可扩展和可迁移的滑坡预测模型中具有巨大潜力。

Abstract: Landslides pose severe threats to infrastructure, economies, and human lives,
necessitating accurate detection and predictive mapping across diverse
geographic regions. With advancements in deep learning and remote sensing,
automated landslide detection has become increasingly effective. This study
presents a comprehensive approach integrating multi-source satellite imagery
and deep learning models to enhance landslide identification and prediction. We
leverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and
Digital Elevation Model (DEM) layers to capture critical environmental features
influencing landslide occurrences. Various geospatial analysis techniques are
employed to assess the impact of terra in characteristics, vegetation cover,
and rainfall on detection accuracy. Additionally, we evaluate the performance
of multiple stateof-the-art deep learning segmentation models, including U-Net,
DeepLabV3+, and Res-Net, to determine their effectiveness in landslide
detection. The proposed framework contributes to the development of reliable
early warning systems, improved disaster risk management, and sustainable
land-use planning. Our findings provide valuable insights into the potential of
deep learning and multi-source remote sensing in creating robust, scalable, and
transferable landslide prediction models.

</details>


### [179] [DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting](https://arxiv.org/abs/2507.01305)
*Worameth Chinchuthakun,Pakkapon Phongthawee,Amit Raj,Varun Jampani,Pramook Khungurn,Supasorn Suwajanakorn*

Key words: 光照估计, 扩散模型, 铬球修复, 高动态范围, 低动态范围

TL;DR: 论文提出了一种名为DiffusionLight的高效方法，通过利用预训练的扩散模型从单张低动态范围图像中估计光照，并进一步优化为快速版本DiffusionLight-Turbo。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法依赖有限的HDR全景数据集，存在泛化能力不足的问题，因此需要一种更通用的光照估计技术。

Method: 通过将任务重定义为铬球修复问题，利用Stable Diffusion XL模型进行迭代修复，生成稳定的低频光照先验，并通过Exposure LoRA合并多曝光图像生成HDR光探针。DiffusionLight-Turbo则通过Turbo LoRA直接预测平均结果，大幅提升速度。

Result: 实验表明，该方法在多样化场景中生成逼真的光照估计，并表现出优越的泛化能力。

Conclusion: DiffusionLight及其快速版本为光照估计提供了高效且高质量的解决方案。

Abstract: We introduce a simple yet effective technique for estimating lighting from a
single low-dynamic-range (LDR) image by reframing the task as a chrome ball
inpainting problem. This approach leverages a pre-trained diffusion model,
Stable Diffusion XL, to overcome the generalization failures of existing
methods that rely on limited HDR panorama datasets. While conceptually simple,
the task remains challenging because diffusion models often insert incorrect or
inconsistent content and cannot readily generate chrome balls in HDR format.
Our analysis reveals that the inpainting process is highly sensitive to the
initial noise in the diffusion process, occasionally resulting in unrealistic
outputs. To address this, we first introduce DiffusionLight, which uses
iterative inpainting to compute a median chrome ball from multiple outputs to
serve as a stable, low-frequency lighting prior that guides the generation of a
high-quality final result. To generate high-dynamic-range (HDR) light probes,
an Exposure LoRA is fine-tuned to create LDR images at multiple exposure
values, which are then merged. While effective, DiffusionLight is
time-intensive, requiring approximately 30 minutes per estimation. To reduce
this overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to
about 30 seconds with minimal quality loss. This 60x speedup is achieved by
training a Turbo LoRA to directly predict the averaged chrome balls from the
iterative process. Inference is further streamlined into a single denoising
pass using a LoRA swapping technique. Experimental results that show our method
produces convincing light estimates across diverse settings and demonstrates
superior generalization to in-the-wild scenarios. Our code is available at
https://diffusionlight.github.io/turbo

</details>


### [180] [Activation Reward Models for Few-Shot Model Alignment](https://arxiv.org/abs/2507.01368)
*Tianning Chai,Chancharik Mitra,Brandon Huang,Gautam Rajendrakumar Gare,Zhiqiu Lin,Assaf Arbelle,Leonid Karlinsky,Rogerio Feris,Trevor Darrell,Deva Ramanan,Roei Herzig*

Key words: LLMs, LMMs, 奖励建模, 激活导向, 少样本学习

TL;DR: 本文提出了一种新的少样本奖励建模方法——激活奖励模型（Activation RMs），通过激活导向构建对齐的奖励信号，减少监督需求，无需额外微调。该方法在标准奖励建模基准上优于现有方法，并在预防奖励攻击行为中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决传统奖励建模难以适应新偏好的问题，以及减少对大规模偏好数据集和额外模型微调的依赖。

Method: 提出激活奖励模型（Activation RMs），利用激活导向构建奖励信号，仅需少量监督且无需微调。

Result: 在标准奖励建模基准和新型基准PreferenceHack上，Activation RMs表现优于现有方法，甚至超越GPT-4o。

Conclusion: Activation RMs是一种高效、适应性强的奖励建模方法，适用于安全关键应用。

Abstract: Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to
human preferences is a central challenge in improving the quality of the
models' generative outputs for real-world applications. A common approach is to
use reward modeling to encode preferences, enabling alignment via post-training
using reinforcement learning. However, traditional reward modeling is not
easily adaptable to new preferences because it requires a separate reward
model, commonly trained on large preference datasets. To address this, we
introduce Activation Reward Models (Activation RMs) -- a novel few-shot reward
modeling method that leverages activation steering to construct well-aligned
reward signals using minimal supervision and no additional model finetuning.
Activation RMs outperform existing few-shot reward modeling approaches such as
LLM-as-a-judge with in-context learning, voting-based scoring, and token
probability scoring on standard reward modeling benchmarks. Furthermore, we
demonstrate the effectiveness of Activation RMs in mitigating reward hacking
behaviors, highlighting their utility for safety-critical applications. Toward
this end, we propose PreferenceHack, a novel few-shot setting benchmark, the
first to test reward models on reward hacking in a paired preference format.
Finally, we show that Activation RM achieves state-of-the-art performance on
this benchmark, surpassing even GPT-4o.

</details>


### [181] [Active Measurement: Efficient Estimation at Scale](https://arxiv.org/abs/2507.01372)
*Max Hamilton,Jinlin Lai,Wenlong Zhao,Subhransu Maji,Daniel Sheldon*

Key words: AI, 主动测量, 重要性采样, 蒙特卡洛估计, 科学测量

TL;DR: 提出了一种人机协作的主动测量框架，通过重要性采样和蒙特卡洛估计提高科学测量的准确性和效率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决当前AI在科学测量中缺乏准确性和统计保证的问题。

Method: 结合AI预测和人类标注，通过重要性采样优化样本选择和模型迭代。

Result: 主动测量框架在多个任务中降低了估计误差，同时减少了人工工作量。

Conclusion: 该方法为科学测量提供了更精确和高效的解决方案。

Abstract: AI has the potential to transform scientific discovery by analyzing vast
datasets with little human effort. However, current workflows often do not
provide the accuracy or statistical guarantees that are needed. We introduce
active measurement, a human-in-the-loop AI framework for scientific
measurement. An AI model is used to predict measurements for individual units,
which are then sampled for human labeling using importance sampling. With each
new set of human labels, the AI model is improved and an unbiased Monte Carlo
estimate of the total measurement is refined. Active measurement can provide
precise estimates even with an imperfect AI model, and requires little human
effort when the AI model is very accurate. We derive novel estimators,
weighting schemes, and confidence intervals, and show that active measurement
reduces estimation error compared to alternatives in several measurement tasks.

</details>


### [182] [Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps](https://arxiv.org/abs/2507.01397)
*Khanh Son Pham,Christian Witte,Jens Behley,Johannes Betz,Cyrill Stachniss*

Key words: 自动驾驶,高清地图,车道预测,拓扑建模,标准地图

TL;DR: 论文提出了一种通过利用标准地图（SD）信息来预测车道段及其拓扑和道路边界的方法，解决了高清地图（HD）在线构建的挑战。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前自动驾驶汽车依赖高清地图（HD），但其可用性受限。研究旨在直接从传感器预测HD地图元素，并建模其与交通元素的关联。

Method: 提出了一种网络架构，利用SD地图的先验信息，结合车道段编码和去噪技术增强训练稳定性，同时利用过去帧数据保持时间一致性。

Result: 实验结果表明，该方法显著优于现有方法，验证了建模方案的有效性。

Conclusion: 通过利用SD地图信息，成功实现了HD地图的在线一致性构建，为自动驾驶提供了更可靠的解决方案。

Abstract: Most autonomous cars rely on the availability of high-definition (HD) maps.
Current research aims to address this constraint by directly predicting HD map
elements from onboard sensors and reasoning about the relationships between the
predicted map and traffic elements. Despite recent advancements, the coherent
online construction of HD maps remains a challenging endeavor, as it
necessitates modeling the high complexity of road topologies in a unified and
consistent manner. To address this challenge, we propose a coherent approach to
predict lane segments and their corresponding topology, as well as road
boundaries, all by leveraging prior map information represented by commonly
available standard-definition (SD) maps. We propose a network architecture,
which leverages hybrid lane segment encodings comprising prior information and
denoising techniques to enhance training stability and performance.
Furthermore, we facilitate past frames for temporal consistency. Our
experimental evaluation demonstrates that our approach outperforms previous
methods by a large margin, highlighting the benefits of our modeling scheme.

</details>


### [183] [Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging](https://arxiv.org/abs/2507.01788)
*Montasir Shams,Chashi Mahiul Islam,Shaeke Salman,Phat Tran,Xiuwen Liu*

Key words: Vision transformers, 医疗图像分类, 语义表示, 鲁棒性, 投影梯度算法

TL;DR: Vision transformers (ViTs)在医疗图像任务中表现出色，但其语义表示缺乏明确性，且易受微小变化影响，导致分类结果不可靠。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 虽然ViTs在医疗图像任务中表现出高准确率，但其表示的语义意义和鲁棒性尚未明确，这可能影响其在安全关键系统中的部署。

Method: 使用基于投影梯度的算法分析ViTs的表示语义意义及鲁棒性。

Result: ViTs的表示缺乏语义意义，对微小变化敏感，可能导致分类准确率下降超过60%。

Conclusion: ViTs在医疗图像分类中存在语义表示不明确和脆弱性问题，这对安全关键系统构成挑战。

Abstract: Vision transformers (ViTs) have rapidly gained prominence in medical imaging
tasks such as disease classification, segmentation, and detection due to their
superior accuracy compared to conventional deep learning models. However, due
to their size and complex interactions via the self-attention mechanism, they
are not well understood. In particular, it is unclear whether the
representations produced by such models are semantically meaningful. In this
paper, using a projected gradient-based algorithm, we show that their
representations are not semantically meaningful and they are inherently
vulnerable to small changes. Images with imperceptible differences can have
very different representations; on the other hand, images that should belong to
different semantic classes can have nearly identical representations. Such
vulnerability can lead to unreliable classification results; for example,
unnoticeable changes cause the classification accuracy to be reduced by over
60\%. %. To the best of our knowledge, this is the first work to systematically
demonstrate this fundamental lack of semantic meaningfulness in ViT
representations for medical image classification, revealing a critical
challenge for their deployment in safety-critical systems.

</details>


### [184] [Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention](https://arxiv.org/abs/2507.01417)
*Jiawei Gu,Ziyue Qiao,Zechao Li*

Key words: OOD检测, 梯度行为, 推理优化, 深度模型

TL;DR: 提出了一种基于梯度行为的OOD检测方法，通过短路异常梯度方向来提升检测性能，同时保持ID分类准确性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 在开放环境中部署深度模型时，OOD检测至关重要。观察到ID和OOD样本的梯度方向存在显著差异，激发了该方法的研究。

Method: 提出了一种推理阶段技术，短路虚假梯度方向，并通过局部一阶近似避免重新计算logits。

Result: 在标准OOD基准测试中取得了显著改进，且方法轻量，对推理流程影响小。

Conclusion: 该方法为实际应用中稳健的OOD检测提供了实用路径。

Abstract: Out-of-Distribution (OOD) detection is critical for safely deploying deep
models in open-world environments, where inputs may lie outside the training
distribution. During inference on a model trained exclusively with
In-Distribution (ID) data, we observe a salient gradient phenomenon: around an
ID sample, the local gradient directions for "enhancing" that sample's
predicted class remain relatively consistent, whereas OOD samples--unseen in
training--exhibit disorganized or conflicting gradient directions in the same
neighborhood. Motivated by this observation, we propose an inference-stage
technique to short-circuit those feature coordinates that spurious gradients
exploit to inflate OOD confidence, while leaving ID classification largely
intact. To circumvent the expense of recomputing the logits after this gradient
short-circuit, we further introduce a local first-order approximation that
accurately captures the post-modification outputs without a second forward
pass. Experiments on standard OOD benchmarks show our approach yields
substantial improvements. Moreover, the method is lightweight and requires
minimal changes to the standard inference pipeline, offering a practical path
toward robust OOD detection in real-world applications.

</details>


### [185] [Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware](https://arxiv.org/abs/2507.01472)
*Jonáš Herec,Vít Růžička,Rado Pitoňák*

Key words: 甲烷检测, 星载算法, 机器学习, 波段选择, 开源

TL;DR: 该研究通过优化算法和机器学习模型，加速了甲烷泄漏的星载检测，提出了一种更快的Mag1c-SAS方法，并通过波段选择策略进一步提高了效率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 甲烷是一种强效温室气体，早期检测其泄漏有助于减缓气候变化。现有探测方法多为手动任务，效率低且成本高。星载检测是潜在的解决方案，但传统方法计算量大，难以在资源有限的星载硬件上实现。

Method: 研究了快速目标检测方法（ACE、CEM），并提出了一种更快的甲烷检测算法Mag1c-SAS。结合机器学习模型（U-Net、LinkNet）评估其检测潜力，并提出了三种波段选择策略。

Result: Mag1c-SAS和CEM在检测强羽流时表现出色且计算高效，比原始Mag1c快约100倍和230倍。提出的波段选择策略之一优于传统方法，且使用更少的通道。

Conclusion: 该研究为未来星载甲烷检测提供了高效、低硬件需求的解决方案，有助于及时数据交付。

Abstract: Methane is a potent greenhouse gas, and detecting its leaks early via
hyperspectral satellite imagery can help mitigate climate change. Meanwhile,
many existing missions operate in manual tasking regimes only, thus missing
potential events of interest. To overcome slow downlink rates cost-effectively,
onboard detection is a viable solution. However, traditional methane
enhancement methods are too computationally demanding for resource-limited
onboard hardware. This work accelerates methane detection by focusing on
efficient, low-power algorithms. We test fast target detection methods (ACE,
CEM) that have not been previously used for methane detection and propose a
Mag1c-SAS - a significantly faster variant of the current state-of-the-art
algorithm for methane detection: Mag1c. To explore their true detection
potential, we integrate them with a machine learning model (U-Net, LinkNet).
Our results identify two promising candidates (Mag1c-SAS and CEM), both
acceptably accurate for the detection of strong plumes and computationally
efficient enough for onboard deployment: one optimized more for accuracy, the
other more for speed, achieving up to ~100x and ~230x faster computation than
original Mag1c on resource-limited hardware. Additionally, we propose and
evaluate three band selection strategies. One of them can outperform the method
traditionally used in the field while using fewer channels, leading to even
faster processing without compromising accuracy. This research lays the
foundation for future advancements in onboard methane detection with minimal
hardware requirements, improving timely data delivery. The produced code, data,
and models are open-sourced and can be accessed from
https://github.com/zaitra/methane-filters-benchmark.

</details>


### [186] [Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation](https://arxiv.org/abs/2507.01509)
*Tapas K. Dutta,Snehashis Majhi,Deepak Ranjan Nayak,Debesh Jha*

Key words: 息肉分割, SAM-MaGuP, 边界蒸馏, 1D-2D Mamba适配器

TL;DR: 提出了SAM-MaGuP，一种创新的息肉分割方法，通过边界蒸馏模块和1D-2D Mamba适配器解决弱边界问题，显著提升分割精度。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 息肉分割在结肠镜图像中至关重要，但现有方法在弱边界分割和泛化能力上表现不足。

Method: 提出SAM-MaGuP，结合边界蒸馏模块和1D-2D Mamba适配器，增强全局上下文交互。

Result: 在五个数据集上表现优异，超越现有方法。

Conclusion: SAM-MaGuP为息肉分割设定了新标杆，解决了弱边界和泛化问题。

Abstract: Polyp segmentation in colonoscopy images is crucial for early detection and
diagnosis of colorectal cancer. However, this task remains a significant
challenge due to the substantial variations in polyp shape, size, and color, as
well as the high similarity between polyps and surrounding tissues, often
compounded by indistinct boundaries. While existing encoder-decoder CNN and
transformer-based approaches have shown promising results, they struggle with
stable segmentation performance on polyps with weak or blurry boundaries. These
methods exhibit limited abilities to distinguish between polyps and non-polyps
and capture essential boundary cues. Moreover, their generalizability still
falls short of meeting the demands of real-time clinical applications. To
address these limitations, we propose SAM-MaGuP, a groundbreaking approach for
robust polyp segmentation. By incorporating a boundary distillation module and
a 1D-2D Mamba adapter within the Segment Anything Model (SAM), SAM-MaGuP excels
at resolving weak boundary challenges and amplifies feature learning through
enriched global contextual interactions. Extensive evaluations across five
diverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods,
achieving unmatched segmentation accuracy and robustness. Our key innovations,
a Mamba-guided boundary prior and a 1D-2D Mamba block, set a new benchmark in
the field, pushing the boundaries of polyp segmentation to new heights.

</details>


### [187] [How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks](https://arxiv.org/abs/2507.01955)
*Rahul Ramachandran,Ali Garjani,Roman Bachmann,Andrei Atanov,Oğuzhan Fatih Kar,Amir Zamir*

Key words: 多模态基础模型, 视觉任务, 提示链, GPT-4o, 几何任务

TL;DR: 论文评估了多模态基础模型在标准计算机视觉任务中的表现，发现它们虽不及专业模型，但作为通用模型表现尚可，尤其在语义任务上优于几何任务。GPT-4o在非推理模型中表现最佳。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探究多模态基础模型（如GPT-4o等）在视觉理解方面的实际能力，填补其在标准计算机视觉任务中表现的空白。

Method: 通过提示链技术将标准视觉任务转化为文本提示和API兼容的任务，建立标准化评估框架，测试模型在语义分割、目标检测等任务中的表现。

Result: 模型在各项任务中均不及专业模型，但作为通用模型表现尚可；语义任务优于几何任务；GPT-4o在非推理模型中表现最优。

Conclusion: 多模态基础模型在视觉任务中表现有限，尤其是几何任务，但通过提示链技术可以标准化评估，未来需进一步优化。

Abstract: Multimodal foundation models, such as GPT-4o, have recently made remarkable
progress, but it is not clear where exactly these models stand in terms of
understanding vision. In this paper, we benchmark the performance of popular
multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0
Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision
tasks (semantic segmentation, object detection, image classification, depth and
surface normal prediction) using established datasets (e.g., COCO, ImageNet and
its variants, etc).
  The main challenges to performing this are: 1) most models are trained to
output text and cannot natively express versatile domains, such as segments or
3D geometry, and 2) many leading models are proprietary and accessible only at
an API level, i.e., there is no weight access to adapt them. We address these
challenges by translating standard vision tasks into equivalent text-promptable
and API-compatible tasks via prompt chaining to create a standardized
benchmarking framework.
  We observe that 1) the models are not close to the state-of-the-art
specialist models at any task. However, 2) they are respectable generalists;
this is remarkable as they are presumably trained on primarily image-text-based
tasks. 3) They perform semantic tasks notably better than geometric ones. 4)
While the prompt-chaining techniques affect performance, better models exhibit
less sensitivity to prompt variations. 5) GPT-4o performs the best among
non-reasoning models, securing the top position in 4 out of 6 tasks, 6)
reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a
preliminary analysis of models with native image generation, like the latest
GPT-4o, shows they exhibit quirks like hallucinations and spatial
misalignments.

</details>


### [188] [Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation](https://arxiv.org/abs/2507.01957)
*Zhuoyang Zhang,Luke J. Huang,Chengyue Wu,Shang Yang,Kelly Peng,Yao Lu,Song Han*

Key words: Locality-aware Parallel Decoding, autoregressive modeling, image generation

TL;DR: LPD通过灵活并行自回归建模和局部感知生成顺序，显著加速自回归图像生成，减少生成步骤并保持质量。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决传统自回归图像生成中高延迟问题，实现高质量高并行化。

Method: 提出灵活并行自回归建模和局部感知生成顺序两种技术。

Result: 在ImageNet上，生成步骤从256降至20（256×256分辨率），1024降至48（512×512分辨率），延迟降低至少3.4倍。

Conclusion: LPD在保持生成质量的同时显著加速图像生成。

Abstract: We present Locality-aware Parallel Decoding (LPD) to accelerate
autoregressive image generation. Traditional autoregressive image generation
relies on next-patch prediction, a memory-bound process that leads to high
latency. Existing works have tried to parallelize next-patch prediction by
shifting to multi-patch prediction to accelerate the process, but only achieved
limited parallelization. To achieve high parallelization while maintaining
generation quality, we introduce two key techniques: (1) Flexible Parallelized
Autoregressive Modeling, a novel architecture that enables arbitrary generation
ordering and degrees of parallelization. It uses learnable position query
tokens to guide generation at target positions while ensuring mutual visibility
among concurrently generated tokens for consistent parallel decoding. (2)
Locality-aware Generation Ordering, a novel schedule that forms groups to
minimize intra-group dependencies and maximize contextual support, enhancing
generation quality. With these designs, we reduce the generation steps from 256
to 20 (256$\times$256 res.) and 1024 to 48 (512$\times$512 res.) without
compromising quality on the ImageNet class-conditional generation, and
achieving at least 3.4$\times$ lower latency than previous parallelized
autoregressive models.

</details>


### [189] [SPoT: Subpixel Placement of Tokens in Vision Transformers](https://arxiv.org/abs/2507.01654)
*Martine Hjelkrem-Tan,Marius Aasan,Gabriel Y. Arteaga,Adín Ramírez Rivera*

Key words: 子像素标记放置, Vision Transformers, 稀疏性, 标记化策略

TL;DR: 提出了一种名为SPoT的新型标记化策略，通过连续定位图像中的标记避免了基于网格的限制，显著提升了性能并减少了推理所需的标记数量。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 标准标记化方法限制特征在离散的网格中，阻碍了模型充分利用稀疏性，导致性能受限。

Method: 采用子像素标记放置（SPoT）策略，结合预言机引导的搜索，优化标记的连续位置。

Result: SPoT显著减少了推理所需的标记数量，并提升了性能，为ViT架构提供了更高的灵活性和效率。

Conclusion: SPoT为ViT架构提供了新的方向，将稀疏性转化为战略优势。

Abstract: Vision Transformers naturally accommodate sparsity, yet standard tokenization
methods confine features to discrete patch grids. This constraint prevents
models from fully exploiting sparse regimes, forcing awkward compromises. We
propose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that
positions tokens continuously within images, effectively sidestepping
grid-based limitations. With our proposed oracle-guided search, we uncover
substantial performance gains achievable with ideal subpixel token positioning,
drastically reducing the number of tokens necessary for accurate predictions
during inference. SPoT provides a new direction for flexible, efficient, and
interpretable ViT architectures, redefining sparsity as a strategic advantage
rather than an imposed limitation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [190] [Can Argus Judge Them All? Comparing VLMs Across Domains](https://arxiv.org/abs/2507.01042)
*Harsh Joshi,Gautam Siddharth Kashyap,Rafiq Ali,Ebad Shabbir,Niharika Jain,Sarthak Jain,Jiechao Gao,Usman Naseem*

Key words: 视觉语言模型，任务一致性，跨数据集评估，泛化性，专业性

TL;DR: 评估了三种视觉语言模型（CLIP、BLIP、LXMERT）在多任务中的表现，发现它们在泛化性、专业性和任务灵活性上各有优劣。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 研究视觉语言模型在不同任务中的性能一致性，为工业部署和未来模型开发提供指导。

Method: 在检索、描述和推理等多样化数据集上评估模型的任务准确性、生成质量、效率和跨数据集一致性（CDC）。

Result: CLIP在泛化性上表现最佳（CDC: 0.92），BLIP在精选数据上最优，LXMERT在结构化推理中领先。

Conclusion: 结果揭示了模型在泛化性和专业性之间的权衡，有助于设计更稳健、灵活的任务架构。

Abstract: Vision-Language Models (VLMs) are advancing multimodal AI, yet their
performance consistency across tasks is underexamined. We benchmark CLIP, BLIP,
and LXMERT across diverse datasets spanning retrieval, captioning, and
reasoning. Our evaluation includes task accuracy, generation quality,
efficiency, and a novel Cross-Dataset Consistency (CDC) metric. CLIP shows
strongest generalization (CDC: 0.92), BLIP excels on curated data, and LXMERT
leads in structured reasoning. These results expose trade-offs between
generalization and specialization, informing industrial deployment of VLMs and
guiding development toward robust, task-flexible architectures.

</details>


### [191] [Conversational LLMs Simplify Secure Clinical Data Access, Understanding, and Analysis](https://arxiv.org/abs/2507.01053)
*Rafi Al Attrach,Pedro Moreira,Rajna Fani,Renato Umeton,Leo Anthony Celi*

Key words: MIMIC-IV, 医疗数据库, 自然语言查询, SQL, 临床研究

TL;DR: M3 是一个工具，通过自然语言查询降低使用 MIMIC-IV 医疗数据库的技术门槛，加速临床数据的分析和研究。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 随着临床数据集的规模增大，其复杂性和技术门槛限制了研究人员的有效利用，M3 旨在解决这一问题。

Method: M3 通过单命令检索 MIMIC-IV 数据，并提供自然语言转 SQL 的功能，简化查询过程。

Result: M3 显著减少了传统手动 SQL 查询的时间，使复杂临床分析更易进行。

Conclusion: M3 通过简化访问，促进更多人利用 MIMIC-IV 数据，加速临床研究成果的转化。

Abstract: As ever-larger clinical datasets become available, they have the potential to
unlock unprecedented opportunities for medical research. Foremost among them is
Medical Information Mart for Intensive Care (MIMIC-IV), the world's largest
open-source EHR database. However, the inherent complexity of these datasets,
particularly the need for sophisticated querying skills and the need to
understand the underlying clinical settings, often presents a significant
barrier to their effective use. M3 lowers the technical barrier to
understanding and querying MIMIC-IV data. With a single command it retrieves
MIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the
hosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers
converse with the database in plain English. Ask a clinical question in natural
language; M3 uses a language model to translate it into SQL, executes the query
against the MIMIC-IV dataset, and returns structured results alongside the
underlying query for verifiability and reproducibility. Demonstrations show
that minutes of dialogue with M3 yield the kind of nuanced cohort analyses that
once demanded hours of handcrafted SQL and relied on understanding the
complexities of clinical workflows. By simplifying access, M3 invites the
broader research community to mine clinical critical-care data and accelerates
the translation of raw records into actionable insight.

</details>


### [192] [A Data Science Approach to Calcutta High Court Judgments: An Efficient LLM and RAG-powered Framework for Summarization and Similar Cases Retrieval](https://arxiv.org/abs/2507.01058)
*Puspendu Banerjee,Aritra Mazumdar,Wazib Ansar,Saptarsi Goswami,Amlan Chakrabarti*

Key words: 司法系统, 数据科学, 大语言模型, 检索增强生成, 法律文本分析, 案例检索

TL;DR: 该研究提出一个结合大语言模型和检索增强生成技术的框架，用于高效分析加尔各答高等法院的判决，旨在提升法律研究和决策效率。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 司法系统面临日益增长的法律问题，需要更高效地利用司法资源。本研究旨在通过数据科学方法优化法律文本分析和案例检索，以支持法律专业人士和学生。

Method: 研究采用Pegasus模型进行法律文本的精细总结，并开发了一个基于检索增强生成技术的系统，用于检索相似案例和生成概括。

Result: 通过两步总结技术，保留了关键法律背景，构建了全面的向量数据库。RAG框架能够高效检索相似案例并提供详细概述，显著提升了法律研究效率。

Conclusion: 该框架不仅提高了法律研究的效率，还帮助法律专业人士和学生更轻松地获取和理解关键法律信息，对整体法律环境产生积极影响。

Abstract: The judiciary, as one of democracy's three pillars, is dealing with a rising
amount of legal issues, needing careful use of judicial resources. This
research presents a complex framework that leverages Data Science
methodologies, notably Large Language Models (LLM) and Retrieval-Augmented
Generation (RAG) techniques, to improve the efficiency of analyzing Calcutta
High Court verdicts. Our framework focuses on two key aspects: first, the
creation of a robust summarization mechanism that distills complex legal texts
into concise and coherent summaries; and second, the development of an
intelligent system for retrieving similar cases, which will assist legal
professionals in research and decision making. By fine-tuning the Pegasus model
using case head note summaries, we achieve significant improvements in the
summarization of legal cases. Our two-step summarizing technique preserves
crucial legal contexts, allowing for the production of a comprehensive vector
database for RAG. The RAG-powered framework efficiently retrieves similar cases
in response to user queries, offering thorough overviews and summaries. This
technique not only improves legal research efficiency, but it also helps legal
professionals and students easily acquire and grasp key legal information,
benefiting the overall legal scenario.

</details>


### [193] [FAIR-MATCH: A Multi-Objective Framework for Bias Mitigation in Reciprocal Dating Recommendations](https://arxiv.org/abs/2507.01063)
*Madhav Kotecha*

Key words: 在线约会,推荐系统,算法公平性,互惠推荐

TL;DR: 论文分析了在线约会平台推荐系统的算法缺陷，并提出改进框架以提高公平性和准确性。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 当前约会应用的推荐系统存在算法缺陷，如流行度偏见和公平性问题，限制了其效果并引入有害偏见。

Method: 整合了互惠推荐框架、公平性评估指标和行业实现，提出数学框架，包括增强相似性测量和多目标优化。

Result: 现有系统协作过滤效果为25.1%，互惠方法为28.7%；改进框架在保持准确性的同时提升公平性。

Conclusion: 提出的框架能有效减少算法偏见并提高配对效果。

Abstract: Online dating platforms have fundamentally transformed the formation of
romantic relationships, with millions of users worldwide relying on algorithmic
matching systems to find compatible partners. However, current recommendation
systems in dating applications suffer from significant algorithmic
deficiencies, including but not limited to popularity bias, filter bubble
effects, and inadequate reciprocity modeling that limit effectiveness and
introduce harmful biases. This research integrates foundational work with
recent empirical findings to deliver a detailed analysis of dating app
recommendation systems, highlighting key issues and suggesting research-backed
solutions. Through analysis of reciprocal recommendation frameworks, fairness
evaluation metrics, and industry implementations, we demonstrate that current
systems achieve modest performance with collaborative filtering reaching 25.1\%
while reciprocal methods achieve 28.7\%. Our proposed mathematical framework
addresses these limitations through enhanced similarity measures,
multi-objective optimization, and fairness-aware algorithms that maintain
competitive accuracy while improving demographic representation to reduce
algorithmic bias.

</details>


### [194] [Cohort Retrieval using Dense Passage Retrieval](https://arxiv.org/abs/2507.01049)
*Pranav Jadhav*

Key words: 患者队列检索, Dense Passage Retrieval, 电子健康记录, 心脏超声, 语义搜索

TL;DR: 该论文提出了一种基于密集段落检索（DPR）的方法，用于从电子健康记录（EHR）中检索患者队列，特别是在心脏超声领域。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 心脏超声领域的患者队列检索是一个重要但具有挑战性的任务，传统方法难以高效处理非结构化的EHR数据。

Method: 通过将非结构化的心脏超声EHR数据转换为查询-段落数据集，并设计基于真实临床场景的评估指标，使用定制的DPR嵌入模型进行检索。

Result: 定制的DPR模型在性能上优于传统方法和现成的SOTA方法，成为心脏超声领域患者队列检索的首个应用。

Conclusion: 该方法为心脏超声领域的患者队列检索提供了一个可扩展的框架，并可推广到其他医学领域。

Abstract: Patient cohort retrieval is a pivotal task in medical research and clinical
practice, enabling the identification of specific patient groups from extensive
electronic health records (EHRs). In this work, we address the challenge of
cohort retrieval in the echocardiography domain by applying Dense Passage
Retrieval (DPR), a prominent methodology in semantic search. We propose a
systematic approach to transform an echocardiographic EHR dataset of
unstructured nature into a Query-Passage dataset, framing the problem as a
Cohort Retrieval task. Additionally, we design and implement evaluation metrics
inspired by real-world clinical scenarios to rigorously test the models across
diverse retrieval tasks. Furthermore, we present a custom-trained DPR embedding
model that demonstrates superior performance compared to traditional and
off-the-shelf SOTA methods.To our knowledge, this is the first work to apply
DPR for patient cohort retrieval in the echocardiography domain, establishing a
framework that can be adapted to other medical domains.

</details>


### [195] [Enhanced Influence-aware Group Recommendation for Online Media Propagation](https://arxiv.org/abs/2507.01616)
*Chengkun He,Xiangmin Zhou,Chen Wang,Longbing Cao,Jie Shao,Xiaodong Li,Guang Xu,Carrie Jinqiu Hu,Zahir Tari*

Key words: 群体推荐，社交影响，动态建模，实时推荐

TL;DR: 论文提出了一种增强的影响感知群体推荐框架（EIGR），通过图提取采样策略、动态独立级联模型和两级哈希用户组索引，解决了大规模社交图、动态影响传播和实时推荐的挑战。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 传统群体推荐在应对大规模社交图、动态影响传播和实时推荐时存在效率低下和准确性不足的问题，需要一种更高效的解决方案。

Method: 提出EIGR框架，包含图提取采样策略（GES）以减少冗余，动态独立级联模型（DYIC）预测影响传播，以及用户组索引（UG-Index）实现高效实时推荐。

Result: 在真实数据集上的实验表明，EIGR在效果和效率上均优于现有基准方法。

Conclusion: EIGR框架有效解决了群体推荐中的关键挑战，为实时推荐提供了高效且准确的解决方案。

Abstract: Group recommendation over social media streams has attracted significant
attention due to its wide applications in domains such as e-commerce,
entertainment, and online news broadcasting. By leveraging social connections
and group behaviours, group recommendation (GR) aims to provide more accurate
and engaging content to a set of users rather than individuals. Recently,
influence-aware GR has emerged as a promising direction, as it considers the
impact of social influence on group decision-making. In earlier work, we
proposed Influence-aware Group Recommendation (IGR) to solve this task.
However, this task remains challenging due to three key factors: the large and
ever-growing scale of social graphs, the inherently dynamic nature of influence
propagation within user groups, and the high computational overhead of
real-time group-item matching.
  To tackle these issues, we propose an Enhanced Influence-aware Group
Recommendation (EIGR) framework. First, we introduce a Graph Extraction-based
Sampling (GES) strategy to minimise redundancy across multiple temporal social
graphs and effectively capture the evolving dynamics of both groups and items.
Second, we design a novel DYnamic Independent Cascade (DYIC) model to predict
how influence propagates over time across social items and user groups.
Finally, we develop a two-level hash-based User Group Index (UG-Index) to
efficiently organise user groups and enable real-time recommendation
generation. Extensive experiments on real-world datasets demonstrate that our
proposed framework, EIGR, consistently outperforms state-of-the-art baselines
in both effectiveness and efficiency.

</details>


### [196] [Optimizing Conversational Product Recommendation via Reinforcement Learning](https://arxiv.org/abs/2507.01060)
*Kang Liu*

Key words: 强化学习, 对话策略, 产品推荐, 智能代理, 转化率

TL;DR: 论文提出了一种基于强化学习的优化对话策略方法，用于跨行业产品推荐，通过学习最优对话策略提高产品参与度和转化率。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 随着智能代理在销售和服务运营中的普及，对话的有效性不仅取决于推荐内容，还取决于推荐方式和时机，因此需要优化对话策略。

Method: 采用反馈驱动的强化学习方法，让代理系统学习最优对话策略，并结合行为数据和转化结果优化对话内容。

Result: 该方法能够提高产品参与度和转化率，同时满足上下文和监管约束。

Conclusion: 该框架为企业在可扩展和个性化的产品推荐方面提供了一种有效方法。

Abstract: We propose a reinforcement learning-based approach to optimize conversational
strategies for product recommendation across diverse industries. As
organizations increasingly adopt intelligent agents to support sales and
service operations, the effectiveness of a conversation hinges not only on what
is recommended but how and when recommendations are delivered. We explore a
methodology where agentic systems learn optimal dialogue policies through
feedback-driven reinforcement learning. By mining aggregate behavioral patterns
and conversion outcomes, our approach enables agents to refine talk tracks that
drive higher engagement and product uptake, while adhering to contextual and
regulatory constraints. We outline the conceptual framework, highlight key
innovations, and discuss the implications for scalable, personalized
recommendation in enterprise environments.

</details>


### [197] [Embedding-based Retrieval in Multimodal Content Moderation](https://arxiv.org/abs/2507.01066)
*Hanzhong Liang,Jinghao Shi,Xiang Shen,Zixuan Wang,Vera Wen,Ardalan Mehrani,Zhiqian Chen,Yifan Wu,Zhixin Zhang*

Key words: 视频内容审核, 嵌入检索, 监督对比学习, 多模态模型

TL;DR: 论文提出了一种基于嵌入的检索（EBR）方法，用于视频平台的实时内容审核。该方法通过监督对比学习训练嵌入模型，显著提升了审核效率和性能。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 传统分类方法在快速响应和成本效率方面存在不足，特别是在处理新兴趋势和紧急情况时。为了解决这一问题，作者提出了EBR方法。

Method: 作者利用监督对比学习（SCL）框架训练单模态和多模态基础嵌入模型，并设计了一个结合嵌入生成和视频检索的系统。

Result: 实验结果表明，EBR在25个新兴趋势上的ROC-AUC提升至0.99，PR-AUC提升至0.95，在线实验中行动率提高10.32%，运营成本降低80%以上。

Conclusion: EBR方法在效率、性能和灵活性上优于传统分类方法，适用于视频内容审核场景。

Abstract: Video understanding plays a fundamental role for content moderation on short
video platforms, enabling the detection of inappropriate content. While
classification remains the dominant approach for content moderation, it often
struggles in scenarios requiring rapid and cost-efficient responses, such as
trend adaptation and urgent escalations. To address this issue, we introduce an
Embedding-Based Retrieval (EBR) method designed to complement traditional
classification approaches. We first leverage a Supervised Contrastive Learning
(SCL) framework to train a suite of foundation embedding models, including both
single-modal and multi-modal architectures. Our models demonstrate superior
performance over established contrastive learning methods such as CLIP and
MoCo. Building on these embedding models, we design and implement the
embedding-based retrieval system that integrates embedding generation and video
retrieval to enable efficient and effective trend handling. Comprehensive
offline experiments on 25 diverse emerging trends show that EBR improves
ROC-AUC from 0.85 to 0.99 and PR-AUC from 0.35 to 0.95. Further online
experiments reveal that EBR increases action rates by 10.32% and reduces
operational costs by over 80%, while also enhancing interpretability and
flexibility compared to classification-based solutions.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [198] [User-guided Generative Source Separation](https://arxiv.org/abs/2507.01339)
*Yutong Wen,Minje Kim,Paris Smaragdis*

Key words: 音乐源分离, 扩散模型, 乐器无关, 条件输入, 生成方法

TL;DR: 提出了一种基于扩散模型的音乐源分离（MSS）方法GuideSep，支持多输入条件，实现了乐器无关的分离效果，超越了传统四音轨分离的限制。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 现有的四音轨分离方法缺乏灵活性，无法满足实际应用需求，因此提出一种更灵活的分离方法。

Method: 使用扩散模型，结合波形模仿条件和mel谱图域掩码作为输入条件，同时设计了掩码预测基线模型进行对比。

Result: GuideSep实现了高质量且灵活的乐器分离，证明了用户参与扩散生成过程的潜力。

Conclusion: GuideSep为MSS提供了更灵活和高质量的解决方案，扩展了用户参与的可能性。

Abstract: Music source separation (MSS) aims to extract individual instrument sources
from their mixture. While most existing methods focus on the widely adopted
four-stem separation setup (vocals, bass, drums, and other instruments), this
approach lacks the flexibility needed for real-world applications. To address
this, we propose GuideSep, a diffusion-based MSS model capable of
instrument-agnostic separation beyond the four-stem setup. GuideSep is
conditioned on multiple inputs: a waveform mimicry condition, which can be
easily provided by humming or playing the target melody, and mel-spectrogram
domain masks, which offer additional guidance for separation. Unlike prior
approaches that relied on fixed class labels or sound queries, our conditioning
scheme, coupled with the generative approach, provides greater flexibility and
applicability. Additionally, we design a mask-prediction baseline using the
same model architecture to systematically compare predictive and generative
approaches. Our objective and subjective evaluations demonstrate that GuideSep
achieves high-quality separation while enabling more versatile instrument
extraction, highlighting the potential of user participation in the
diffusion-based generative process for MSS. Our code and demo page are
available at https://yutongwen.github.io/GuideSep/

</details>


### [199] [Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on Embedded Hardware](https://arxiv.org/abs/2507.01563)
*Marco Giordano,Stefano Giacomelli,Claudia Rinaldi,Fabio Graziosi*

Key words: 应急车辆警笛检测, E2PANNs, 嵌入式硬件, 低延迟, 分布式监测

TL;DR: 提出一种基于E2PANNs的全栈应急车辆警笛检测系统，优化嵌入式硬件实时部署，通过定制数据集和改进框架提升性能。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 解决城市环境中标准AudioSet标注不可靠的问题，实现低延迟、高鲁棒性的应急车辆警笛检测。

Method: 使用E2PANNs模型和自定义数据集（AudioSet-EV、AudioSet-EV Augmented、Unified-EV），部署于Raspberry Pi 5，采用多线程推理引擎和状态机降低误报。

Result: 系统在真实音频条件下表现优异，支持低成本边缘设备形成分布式声学监测网络。

Conclusion: 验证了IoS兼容的SED解决方案的可行性，可应用于智能城市基础设施中的应急车辆追踪。

Abstract: We present a full-stack emergency vehicle (EV) siren detection system
designed for real-time deployment on embedded hardware. The proposed approach
is based on E2PANNs, a fine-tuned convolutional neural network derived from
EPANNs, and optimized for binary sound event detection under urban acoustic
conditions. A key contribution is the creation of curated and semantically
structured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -
developed using a custom AudioSet-Tools framework to overcome the low
reliability of standard AudioSet annotations. The system is deployed on a
Raspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing
a multithreaded inference engine with adaptive frame sizing, probability
smoothing, and a decision-state machine to control false positive activations.
A remote WebSocket interface provides real-time monitoring and facilitates live
demonstration capabilities. Performance is evaluated using both framewise and
event-based metrics across multiple configurations. Results show the system
achieves low-latency detection with improved robustness under realistic audio
conditions. This work demonstrates the feasibility of deploying IoS-compatible
SED solutions that can form distributed acoustic monitoring networks, enabling
collaborative emergency vehicle tracking across smart city infrastructures
through WebSocket connectivity on low-cost edge devices.

</details>


### [200] [Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder](https://arxiv.org/abs/2507.01582)
*Jing Luo,Xinyu Yang,Jie Wei*

Key words: 古典音乐生成, 钢琴表演, ECP表示, XMVAE, VQ-VAE

TL;DR: 这篇论文提出了一种生成古典钢琴表演的方法，通过结合作曲家和钢琴家的双重角色，使用ECP表示和XMVAE模型，实现了优于现有模型的音乐生成质量。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 古典音乐的创意不仅来自作曲家，还来自演奏者对静态乐谱的诠释。本文旨在模拟这一双重创意过程，生成高质量的古典钢琴表演。

Method: 提出ECP表示法捕捉表演的节奏结构和表达细节，并设计XMVAE模型，包含VQ-VAE分支（作曲家角色）和普通VAE分支（钢琴家角色），通过多尺度编码器和Transformer解码器联合训练。

Result: XMVAE生成的表演在客观和主观评估中均优于现有模型，且作曲分支的预训练显著提升了性能。

Conclusion: ECP表示和XMVAE模型有效模拟了古典音乐的双重创意过程，生成了高质量的表演。

Abstract: The creativity of classical music arises not only from composers who craft
the musical sheets but also from performers who interpret the static notations
with expressive nuances. This paper addresses the challenge of generating
classical piano performances from scratch, aiming to emulate the dual roles of
composer and pianist in the creative process. We introduce the Expressive
Compound Word (ECP) representation, which effectively captures both the
metrical structure and expressive nuances of classical performances. Building
on this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a
model featuring two branches: a Vector Quantized Variational AutoEncoder
(VQ-VAE) branch that generates score-related content, representing the
Composer, and a vanilla VAE branch that produces expressive details, fulfilling
the role of Pianist. These branches are jointly trained with similar Seq2Seq
architectures, leveraging a multiscale encoder to capture beat-level contextual
information and an orthogonal Transformer decoder for efficient compound tokens
decoding. Both objective and subjective evaluations demonstrate that XMVAE
generates classical performances with superior musical quality compared to
state-of-the-art models. Furthermore, pretraining the Composer branch on extra
musical score datasets contribute to a significant performance gain.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [201] [Automated Classification of Volcanic Earthquakes Using Transformer Encoders: Insights into Data Quality and Model Interpretability](https://arxiv.org/abs/2507.01260)
*Y. Suzuki,Y. Yukutake,T. Ohminato,M. Yamasaki,Ahyi Kim*

Key words: 火山地震分类, Transformer, 深度学习, 注意力权重, 数据多样性

TL;DR: 提出基于Transformer编码器的深度学习模型，用于高效客观地分类火山地震，优于传统CNN方法，关注数据平衡与解释性。

<details>
  <summary>Details</summary>

Main category: physics.geo-ph

Motivation: 传统火山地震分类方法依赖主观人为判断，耗时耗力，需更高效客观的分类方法。

Method: 开发基于Transformer编码器的深度学习模型，测试于浅间山的多样地震活动，分析注意力权重。

Result: 模型在F1分数上表现优异（0.930-0.980），关注波形特征类似专家，数据选择与多样性影响表现。

Conclusion: Transformer模型为火山地震分类提供高效且可解释的方法，可推广至其他火山区域。

Abstract: Precisely classifying earthquake types is crucial for elucidating the
relationship between volcanic earthquakes and volcanic activity. However,
traditional methods rely on subjective human judgment, which requires
considerable time and effort. To address this issue, we developed a deep
learning model using a transformer encoder for a more objective and efficient
classification. Tested on Mount Asama's diverse seismic activity, our model
achieved high F1 scores (0.930 for volcano tectonic, 0.931 for low-frequency
earthquakes, and 0.980 for noise), superior to a conventional CNN-based method.
To enhance interpretability, attention weight visualizations were analyzed,
revealing that the model focuses on key waveform features similarly to human
experts. However, inconsistencies in training data, such as ambiguously labeled
B-type events with S-waves, were found to influence classification accuracy and
attention weight distributions. Experiments addressing data selection and
augmentation demonstrated the importance of balancing data quality and
diversity. In addition, stations within 3 km of the crater played an important
role in improving model performance and interpretability. These findings
highlight the potential of Transformer-based models for automated volcanic
earthquake classification, particularly in improving efficiency and
interpretability. By addressing challenges such as data imbalance and
subjective labeling, our approach provides a robust framework for understanding
seismic activity at Mount Asama. Moreover, this framework offers opportunities
for transfer learning to other volcanic regions, paving the way for enhanced
volcanic hazard assessments and disaster mitigation strategies.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [202] [Cross-Attention Message-Passing Transformers for Code-Agnostic Decoding in 6G Networks](https://arxiv.org/abs/2507.01038)
*Seong-Joon Park,Hee-Youl Kwak,Sang-Hyo Kim,Yongjune Kim,Jong-Seon No*

Key words: 6G网络, 信道编码, Transformer, AI解码器, CrossMPT, FCrossMPT, CrossED

TL;DR: 提出了一种基于Transformer架构的AI原生基础模型，用于统一且不依赖于特定编码类型的解码，解决了6G网络中灵活性和扩展性的需求。

<details>
  <summary>Details</summary>

Main category: cs.IT

Motivation: 传统解码器缺乏灵活性和扩展性，无法满足6G网络中多样化的通信场景需求。

Method: 引入了CrossMPT（跨注意力消息传递Transformer），通过掩蔽交叉注意力块更新幅度和校验向量，进一步实现了FCrossMPT和CrossED（解码器集成）以适应不同编码类型。

Result: CrossMPT在单一神经解码器中实现了最先进的解码性能，FCrossMPT和CrossED进一步提高了泛化能力和短码解码性能。

Conclusion: 提出的AI原生解码器具有灵活性、扩展性和高性能，为6G网络信道编码提供了有前景的解决方案。

Abstract: Channel coding for 6G networks is expected to support a wide range of
requirements arising from heterogeneous communication scenarios. These demands
challenge traditional code-specific decoders, which lack the flexibility and
scalability required for next-generation systems. To tackle this problem, we
propose an AI-native foundation model for unified and code-agnostic decoding
based on the transformer architecture. We first introduce a cross-attention
message-passing transformer (CrossMPT). CrossMPT employs two masked
cross-attention blocks that iteratively update two distinct input
representations-magnitude and syndrome vectors-allowing the model to
effectively learn the decoding problem. Notably, our CrossMPT has achieved
state-of-the-art decoding performance among single neural decoders. Building on
this, we develop foundation CrossMPT (FCrossMPT) by making the architecture
invariant to code length, rate, and class, allowing a single trained model to
decode a broad range of codes without retraining. To further enhance decoding
performance, particularly for short blocklength codes, we propose CrossMPT
ensemble decoder (CrossED), an ensemble decoder composed of multiple parallel
CrossMPT blocks employing different parity-check matrices. This architecture
can also serve as a foundation model, showing strong generalization across
diverse code types. Overall, the proposed AI-native code-agnostic decoder
offers flexibility, scalability, and high performance, presenting a promising
direction to channel coding for 6G networks.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [203] [Advancing Magnetic Materials Discovery -- A structure-based machine learning approach for magnetic ordering and magnetic moment prediction](https://arxiv.org/abs/2507.01913)
*Apoorv Verma,Junaid Jami,Amrita Bhattacharya*

Key words: 磁性材料, 机器学习, LightGBM, 结构预测, 高通量筛选

TL;DR: 提出了一种新的描述符，通过结构信息显著提升了两种关键磁性特性（磁序和原子磁矩）的预测准确性，适用于多种材料系统。

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

Motivation: 准确预测磁性行为在材料设计中至关重要，但现有的模型局限于特定材料，无法广泛适用。

Method: 采用LightGBM模型，结合非线性项和稀疏矩阵优化，基于5741种二元和三元磁性材料的数据库进行训练。

Result: 模型在磁序分类中达到82.4%的准确率，原子磁矩预测的相关系数为0.93，并能同时估算形成能。

Conclusion: 该方法为高通量筛选磁性材料提供了高效且通用的工具。

Abstract: Accurately predicting magnetic behavior across diverse materials systems
remains a longstanding challenge due to the complex interplay of structural and
electronic factors and is pivotal for the accelerated discovery and design of
next-generation magnetic materials. In this work, a refined descriptor is
proposed that significantly improves the prediction of two critical magnetic
properties -- magnetic ordering (Ferromagnetic vs. Ferrimagnetic) and magnetic
moment per atom -- using only the structural information of materials. Unlike
previous models limited to Mn-based or lanthanide-transition metal compounds,
the present approach generalizes across a diverse dataset of 5741 stable,
binary and ternary, ferromagnetic and ferrimagnetic compounds sourced from the
Materials Project. Leveraging an enriched elemental vector representation and
advanced feature engineering, including nonlinear terms and reduced matrix
sparsity, the LightGBM-based model achieves an accuracy of 82.4% for magnetic
ordering classification and balanced recall across FM and FiM classes,
addressing a key limitation in prior studies. The model predicts magnetic
moment per atom with a correlation coefficient of 0.93, surpassing the Hund's
matrix and orbital field matrix descriptors. Additionally, it accurately
estimates formation energy per atom, enabling assessment of both magnetic
behavior and material stability. This generalized and computationally efficient
framework offers a robust tool for high-throughput screening of magnetic
materials with tailored properties.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [204] [Transfer Learning for VLC-based indoor Localization: Addressing Environmental Variability](https://arxiv.org/abs/2507.01575)
*Masood Jan,Wafa Njima,Xun Zhang,Alexander Artemenko*

Key words: 可见光通信；迁移学习；室内定位；工业环境；深度神经网络

TL;DR: 论文提出了一种基于迁移学习（TL）的可见光通信（VLC）室内定位方法，显著提高了定位精度、降低了能耗和计算时间，适用于工业环境。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 工业环境中准确的室内定位至关重要，但现有VLC定位方法受环境变化（如光照波动和障碍物）影响较大，需改进。

Method: 采用迁移学习（TL）框架结合深度神经网络（DNN），利用工厂实际数据优化定位性能。

Result: 相比传统模型，定位精度提高47%，能耗降低32%，计算时间减少40%，且在小数据集（30%）下仍保持高精度。

Conclusion: 该方法高效、适应性强，为工业4.0提供了成本低且可扩展的解决方案。

Abstract: Accurate indoor localization is crucial in industrial environments. Visible
Light Communication (VLC) has emerged as a promising solution, offering high
accuracy, energy efficiency, and minimal electromagnetic interference. However,
VLC-based indoor localization faces challenges due to environmental
variability, such as lighting fluctuations and obstacles. To address these
challenges, we propose a Transfer Learning (TL)-based approach for VLC-based
indoor localization. Using real-world data collected at a BOSCH factory, the TL
framework integrates a deep neural network (DNN) to improve localization
accuracy by 47\%, reduce energy consumption by 32\%, and decrease computational
time by 40\% compared to the conventional models. The proposed solution is
highly adaptable under varying environmental conditions and achieves similar
accuracy with only 30\% of the dataset, making it a cost-efficient and scalable
option for industrial applications in Industry 4.0.

</details>


### [205] [Token Communication in the Era of Large Models: An Information Bottleneck-Based Approach](https://arxiv.org/abs/2507.01728)
*Hao Wei,Wanli Ni,Wen Wang,Wenjun Xu,Dusit Niyato,Ping Zhang*

Key words: UniToCom, GenIB, MLLM, 多模态通信, 令牌处理

TL;DR: UniToCom提出了一种统一的令牌通信范式，将令牌作为处理和无线传输的基本单位，并通过GenIB原则改进通信效率。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 为了解决多模态通信中的信息表示和传输效率问题，同时降低计算复杂度。

Method: 提出生成信息瓶颈（GenIB）原则，改进令牌表示；开发σ-GenIB解决方差崩溃问题；使用基于因果Transformer的多模态大语言模型（MLLM）统一处理离散和连续令牌。

Result: 仿真结果表明，UniToCom在动态信道条件下优于基线方法。

Conclusion: UniToCom通过结合令牌处理与MLLM，为下一代智能通信提供了可扩展和通用的解决方案。

Abstract: This letter proposes UniToCom, a unified token communication paradigm that
treats tokens as the fundamental units for both processing and wireless
transmission. Specifically, to enable efficient token representations, we
propose a generative information bottleneck (GenIB) principle, which
facilitates the learning of tokens that preserve essential information while
supporting reliable generation across multiple modalities. By doing this,
GenIB-based tokenization is conducive to improving the communication efficiency
and reducing computational complexity. Additionally, we develop $\sigma$-GenIB
to address the challenges of variance collapse in autoregressive modeling,
maintaining representational diversity and stability. Moreover, we employ a
causal Transformer-based multimodal large language model (MLLM) at the receiver
to unify the processing of both discrete and continuous tokens under the
next-token prediction paradigm. Simulation results validate the effectiveness
and superiority of the proposed UniToCom compared to baselines under dynamic
channel conditions. By integrating token processing with MLLMs, UniToCom
enables scalable and generalizable communication in favor of multimodal
understanding and generation, providing a potential solution for
next-generation intelligent communications.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [206] [HPC-AI Coupling Methodology for Scientific Applications](https://arxiv.org/abs/2507.01025)
*Yutong Lu,Dan Huang,Pin Chen*

Key words: 高性能计算, 人工智能, 耦合模式, 材料科学, 科学应用

TL;DR: 该研究探讨了高性能计算（HPC）与人工智能（AI）耦合的新方法，提出了三种耦合模式（替代、指导和协调），并通过材料科学案例展示了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CE

Motivation: 研究旨在解决高性能计算中的高计算强度等问题，探索AI与HPC耦合在科学应用中的潜力。

Method: 提出了三种HPC-AI耦合模式：替代、指导和协调，每种模式有不同的策略和前提条件，并通过材料科学案例验证。

Result: 展示了耦合模式的技术挑战、性能改进和实现细节，为其他科学领域的HPC-AI应用提供了指导。

Conclusion: HPC-AI耦合模式不仅在材料科学中有效，还可推广到其他科学领域，为未来科学发现提供了新的方法论支持。

Abstract: Artificial intelligence (AI) technologies have fundamentally transformed
numerical-based high-performance computing (HPC) applications with data-driven
approaches and endeavored to address existing challenges, e.g. high
computational intensity, in various scientific domains. In this study, we
explore the scenarios of coupling HPC and AI (HPC-AI) in the context of
emerging scientific applications, presenting a novel methodology that
incorporates three patterns of coupling: surrogate, directive, and coordinate.
Each pattern exemplifies a distinct coupling strategy, AI-driven prerequisite,
and typical HPC-AI ensembles. Through case studies in materials science, we
demonstrate the application and effectiveness of these patterns. The study
highlights technical challenges, performance improvements, and implementation
details, providing insight into promising perspectives of HPC-AI coupling. The
proposed coupling patterns are applicable not only to materials science but
also to other scientific domains, offering valuable guidance for future HPC-AI
ensembles in scientific discovery.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [207] [Can AI be Consentful?](https://arxiv.org/abs/2507.01051)
*Giada Pistilli,Bruna Trevelin*

Key words: 生成式AI, 同意, 法律框架, 伦理挑战, 数据保护

TL;DR: 传统法律和伦理框架围绕"同意"构建，但在生成式AI的内容创作中显得不足，形成"同意缺口"。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 探讨传统"同意"概念在面对AI生成内容时的局限性，尤其是在个人数据保护和隐私权方面。

Method: 通过法律和伦理分析，识别出"同意"在AI系统中的三个核心挑战：范围问题、时间性问题、自主性陷阱。

Result: 提出当前法律框架无法充分应对AI生成内容带来的新挑战，尤其是个人自主权、身份权利和社会责任方面。

Conclusion: 需要更新伦理和法律中的"同意"概念，以适应AI技术发展带来的新问题。

Abstract: The evolution of generative AI systems exposes the challenges of traditional
legal and ethical frameworks built around consent. This chapter examines how
the conventional notion of consent, while fundamental to data protection and
privacy rights, proves insufficient in addressing the implications of
AI-generated content derived from personal data. Through legal and ethical
analysis, we show that while individuals can consent to the initial use of
their data for AI training, they cannot meaningfully consent to the numerous
potential outputs their data might enable or the extent to which the output is
used or distributed. We identify three fundamental challenges: the scope
problem, the temporality problem, and the autonomy trap, which collectively
create what we term a ''consent gap'' in AI systems and their surrounding
ecosystem. We argue that current legal frameworks inadequately address these
emerging challenges, particularly regarding individual autonomy, identity
rights, and social responsibility, especially in cases where AI-generated
content creates new forms of personal representation beyond the scope of the
original consent. By examining how these consent limitations intersect with
broader principles of responsible AI (including fairness, transparency,
accountability, and autonomy) we demonstrate the need to evolve ethical and
legal approaches to consent.

</details>


### [208] [Epitome: Pioneering an Experimental Platform for AI-Social Science Integration](https://arxiv.org/abs/2507.01061)
*Jingjing Qu,Kejia Hu,Jun Zhu,Wenhao Li,Teng Wang,Zhiyun Chen,Yulei Ye,Chaochao Lu,Aimin Zhou,Xiangfeng Wang,James Evan*

Key words: 

TL;DR: Epitome是一个开创性的开放实验平台，旨在通过跨学科实验将大型语言模型（LLMs）与社会科学深度融合，研究AI对个体、组织和社会的实际影响。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 研究背景是AI技术在社会科学中的应用需求，探索AI对社会各层面的影响，推动跨学科研究。

Method: 通过七个核心模块提供一站式实验解决方案，嵌入社会科学实验的经典逻辑，支持多层次人机交互环境的实验设计。

Result: 平台展示了复制经典社会科学实验的能力，能高效生成高质量研究成果，适合顶级期刊发表。

Conclusion: Epitome是一个强大的工具，促进AI与社会科学的交叉研究，并为政策制定等提供潜在应用价值。

Abstract: The integration of Large Language Models (LLMs) into social science
experiments represents a transformative approach to understanding human-AI
interactions and their societal impacts. We introduce Epitome, the world's
first open experimental platform dedicated to the deep integration of
artificial intelligence and social science. Rooted in theoretical foundations
from management, communication studies, sociology, psychology, and ethics,
Epitome focuses on the interactive impacts of AI on individuals, organizations,
and society during its real-world deployment. It constructs a theoretical
support system through cross-disciplinary experiments. The platform offers a
one-stop comprehensive experimental solution spanning "foundation
models-complex application development-user feedback" through seven core
modules, while embedding the classical "control-comparison-comparative causal
logic" of social science experiments into multilevel human-computer interaction
environments, including dialogues, group chats, and multi-agent virtual
scenarios. With its canvas-style, user-friendly interface, Epitome enables
researchers to easily design and run complex experimental scenarios,
facilitating systematic investigations into the social impacts of AI and
exploration of integrated solutions.To demonstrate its capabilities, we
replicated three seminal social science experiments involving LLMs, showcasing
Epitome's potential to streamline complex experimental designs and produce
robust results, suitable for publishing in the top selective journals. Our
findings highlight the platform's utility in enhancing the efficiency and
quality of human-AI interactions, providing valuable insights into the societal
implications of AI technologies. Epitome thus offers a powerful tool for
advancing interdisciplinary research at the intersection of AI and social
science, with potential applications in policy-making, ...

</details>


### [209] [Quantifying Student Success with Generative AI: A Monte Carlo Simulation Informed by Systematic Review](https://arxiv.org/abs/2507.01062)
*Seyma Yaman Kayadibi*

Key words: 生成式人工智能, 高等教育, 学生看法, 学习成就, 模拟建模

TL;DR: 本研究结合系统文献综述和模拟建模，探讨学生对GenAI在高等教育中使用的看法，发现实用性和实际有效性是学习成就的更好预测指标。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 随着GenAI技术的快速发展，研究其在高等教育中的应用及学生的看法对学习成果的影响变得重要。

Method: 采用混合方法，包括系统文献综述和基于模拟的建模，分析19篇实证文章，并通过蒙特卡洛模拟计算“成功分数”。

Result: 态度因素（如实用性和实际有效性）对学习成就的预测作用显著强于情感或信任因素。

Conclusion: 研究为GenAI工具在大学中的合理使用提供了实证支持，强调了实用性感知的重要性。

Abstract: The exponential development of generative artificial intelligence (GenAI)
technologies like ChatGPT has raised increasing curiosity about their use in
higher education, specifically with respect to how students view them, make use
of them, and the implications for learning outcomes. This paper employs a
hybrid methodological approach involving a systematic literature review and
simulation-based modeling to explore student perceptions of GenAI use in the
context of higher education. A total of nineteen empirical articles from 2023
through 2025 were selected from the PRISMA-based search targeting the Scopus
database. Synthesis of emerging patterns from the literature was achieved by
thematic categorization. Six of these had enough quantitative information,
i.e., item-level means and standard deviations, to permit probabilistic
modeling. One dataset, from the resulting subset, was itself selected as a
representative case with which to illustrate inverse-variance weighting by
Monte Carlo simulation, by virtue of its well-designed Likert scale format and
thematic alignment with the use of computing systems by the researcher.
  The simulation provided a composite "Success Score" forecasting the strength
of the relationship between student perceptions and learning achievements.
Findings reveal that attitude factors concerned with usability and real-world
usefulness are significantly better predictors of positive learning achievement
than affective or trust-based factors. Such an interdisciplinary perspective
provides a unique means of linking thematic results with predictive modelling,
resonating with longstanding controversies about the proper use of GenAI tools
within the university.

</details>


### [210] [Penalizing Transparency? How AI Disclosure and Author Demographics Shape Human and AI Judgments About Writing](https://arxiv.org/abs/2507.01418)
*Inyoung Cheong,Alicia Guo,Mina Lee,Zhehui Liao,Kowe Kadoma,Dongyoung Go,Joseph Chee Chang,Peter Henderson,Mor Naaman,Amy X. Zhang*

Key words: AI transparency, writing quality, demographic bias, human evaluation, LLM

TL;DR: 研究探讨了AI使用披露声明对写作质量感知的影响，以及这些影响是否因作者的种族和性别而异。实验发现人类和LLM评分者都会因披露AI使用而惩罚文章，但只有LLM评分者表现出与作者身份的交互效应。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 随着AI在人类写作中的广泛应用，透明度的呼声增加，但如果透明度在不同身份群体中不平等，开放就成为了不对称的负担。

Method: 通过大规模对照实验，人类评分者（1,970人）和LLM评分者（2,520人）评估一篇人类撰写的新闻文章，同时系统性地改变披露声明和作者人口统计特征。

Result: 人类和LLM评分者都因披露AI使用而惩罚文章，但只有LLM评分者在无披露时更倾向于女性或黑人作者的文章，披露AI后这种优势消失。

Conclusion: 研究揭示了AI披露与作者身份之间的复杂关系，突显了机器与人类评价模式的差异。

Abstract: As AI integrates in various types of human writing, calls for transparency
around AI assistance are growing. However, if transparency operates on uneven
ground and certain identity groups bear a heavier cost for being honest, then
the burden of openness becomes asymmetrical. This study investigates how AI
disclosure statement affects perceptions of writing quality, and whether these
effects vary by the author's race and gender. Through a large-scale controlled
experiment, both human raters (n = 1,970) and LLM raters (n = 2,520) evaluated
a single human-written news article while disclosure statements and author
demographics were systematically varied. This approach reflects how both human
and algorithmic decisions now influence access to opportunities (e.g., hiring,
promotion) and social recognition (e.g., content recommendation algorithms). We
find that both human and LLM raters consistently penalize disclosed AI use.
However, only LLM raters exhibit demographic interaction effects: they favor
articles attributed to women or Black authors when no disclosure is present.
But these advantages disappear when AI assistance is revealed. These findings
illuminate the complex relationships between AI disclosure and author identity,
highlighting disparities between machine and human evaluation patterns.

</details>


### [211] [AI and Remote Sensing for Resilient and Sustainable Built Environments: A Review of Current Methods, Open Data and Future Directions](https://arxiv.org/abs/2507.01547)
*Ubada El Joulani,Tatiana Kalganova,Stergios-Aristoteles Mitoulis,Sotirios Argyroudis*

Key words: 交通基础设施, AI, 损害评估, SAR数据, 桥梁监测

TL;DR: 探讨数字技术（尤其是AI）如何提升交通基础设施的损害评估与监测能力，重点关注桥梁的复杂性和SAR数据与AI模型的结合。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 交通基础设施面临老化、气候变化和混合威胁等风险，需要借助新兴技术提升其韧性和功能性。

Method: 通过系统性文献综述，分析现有AI模型和数据集在自然灾害影响下对道路、桥梁等基础设施损害评估的应用。

Result: 研究发现AI模型在SAR数据上的应用研究较少，尤其在桥梁损害评估方面存在显著研究空白。

Conclusion: 本综述旨在填补研究空白，为AI驱动的交通基础设施损害评估与监测提供基础。

Abstract: Critical infrastructure, such as transport networks, underpins economic
growth by enabling mobility and trade. However, ageing assets, climate change
impacts (e.g., extreme weather, rising sea levels), and hybrid threats ranging
from natural disasters to cyber attacks and conflicts pose growing risks to
their resilience and functionality. This review paper explores how emerging
digital technologies, specifically Artificial Intelligence (AI), can enhance
damage assessment and monitoring of transport infrastructure. A systematic
literature review examines existing AI models and datasets for assessing damage
in roads, bridges, and other critical infrastructure impacted by natural
disasters. Special focus is given to the unique challenges and opportunities
associated with bridge damage detection due to their structural complexity and
critical role in connectivity. The integration of SAR (Synthetic Aperture
Radar) data with AI models is also discussed, with the review revealing a
critical research gap: a scarcity of studies applying AI models to SAR data for
comprehensive bridge damage assessment. Therefore, this review aims to identify
the research gaps and provide foundations for AI-driven solutions for assessing
and monitoring critical transport infrastructures.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [212] [Meteoroid stream identification with HDBSCAN unsupervised clustering algorithm](https://arxiv.org/abs/2507.01501)
*Eloy Peña-Asensio,Fabio Ferrari*

Key words: 流星群识别, HDBSCAN, CAMS, 聚类分析

TL;DR: 该研究评估了HDBSCAN算法在无监督流星群识别中的表现，发现其统计一致性优于传统的CAMS查找表方法。

<details>
  <summary>Details</summary>

Main category: astro-ph.EP

Motivation: 准确识别流星群对于理解其起源和演化至关重要，但重叠集群和背景噪声增加了分类难度。

Method: 通过HDBSCAN算法与CAMS数据库对比，采用三种特征向量进行分析，并评估聚类性能。

Result: HDBSCAN确认了39个流星群（21个与CAMS高度匹配），表现优于传统方法。

Conclusion: HDBSCAN是流星群识别的数学一致性替代方法，但需进一步验证物理有效性。

Abstract: Accurate identification of meteoroid streams is central to understanding
their origins and evolution. However, overlapping clusters and background noise
hinder classification, an issue amplified for missions such as ESA's LUMIO that
rely on meteor shower observations to infer lunar meteoroid impact parameters.
This study evaluates the performance of the Hierarchical Density-Based Spatial
Clustering of Applications with Noise (HDBSCAN) algorithm for unsupervised
meteoroid stream identification, comparing its outcomes with the established
Cameras for All-Sky Meteor Surveillance (CAMS) look-up table method. We analyze
the CAMS Meteoroid Orbit Database v3.0 using three feature vectors: LUTAB (CAMS
geocentric parameters), ORBIT (heliocentric orbital elements), and GEO (adapted
geocentric parameters). HDBSCAN is applied with varying minimum cluster sizes
and two cluster selection methods (eom and leaf). To align HDBSCAN clusters
with CAMS classifications, the Hungarian algorithm determines the optimal
mapping. Clustering performance is assessed via the Silhouette score,
Normalized Mutual Information, and F1 score, with Principal Component Analysis
further supporting the analysis. With the GEO vector, HDBSCAN confirms 39
meteoroid streams, 21 strongly aligning with CAMS. The ORBIT vector identifies
30 streams, 13 with high matching scores. Less active showers pose
identification challenges. The eom method consistently yields superior
performance and agreement with CAMS. Although HDBSCAN requires careful
selection of the minimum cluster size, it delivers robust, internally
consistent clusters and outperforms the look-up table method in statistical
coherence. These results underscore HDBSCAN's potential as a mathematically
consistent alternative for meteoroid stream identification, although further
validation is needed to assess physical validity.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [213] [SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars](https://arxiv.org/abs/2507.01939)
*Xiaosheng Zhao,Yang Huang,Guirong Xue,Xiao Kong,Jifeng Liu,Xiaoyu Tang,Timothy C. Beers,Yuan-Sen Ting,A-Li Luo*

Key words: 大语言模型,恒星光谱分析,对比学习,CLIP框架,光谱类型转换

TL;DR: SpecCLIP是一个基于大语言模型方法的恒星光谱分析框架，通过对比学习和辅助解码器提升光谱分析的精度和适应性。

<details>
  <summary>Details</summary>

Main category: astro-ph.IM

Motivation: 受大语言模型在自然语言理解领域的成功启发，作者希望将类似方法应用于恒星光谱分析，以挖掘恒星物理和化学信息的丰富潜力。

Method: SpecCLIP框架在LAMOST低分辨率和Gaia XP光谱数据集上进行预训练，采用CLIP框架进行对比对齐，并结合辅助解码器保留光谱特征并实现光谱类型间的转换。

Result: SpecCLIP在中等规模标注数据集上微调后，提高了恒星参数估计和化学丰度测定的准确性，同时在异常检测方面表现出潜力。

Conclusion: 基于对比学习的SpecCLIP框架能够显著提升恒星光谱分析的精度和灵活性，为未来的天文研究提供了新工具。

Abstract: In recent years, large language models (LLMs) have transformed natural
language understanding through vast datasets and large-scale parameterization.
Inspired by this success, we present SpecCLIP, a foundation model framework
that extends LLM-inspired methodologies to stellar spectral analysis. Stellar
spectra, akin to structured language, encode rich physical and chemical
information about stars. By training foundation models on large-scale spectral
datasets, our goal is to learn robust and informative embeddings that support
diverse downstream applications. As a proof of concept, SpecCLIP involves
pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed
by contrastive alignment using the CLIP (Contrastive Language-Image
Pre-training) framework, adapted to associate spectra from different
instruments. This alignment is complemented by auxiliary decoders that preserve
spectrum-specific information and enable translation (prediction) between
spectral types, with the former achieved by maximizing mutual information
between embeddings and input spectra. The result is a cross-spectrum framework
enabling intrinsic calibration and flexible applications across instruments. We
demonstrate that fine-tuning these models on moderate-sized labeled datasets
improves adaptability to tasks such as stellar-parameter estimation and
chemical-abundance determination. SpecCLIP also enhances the accuracy and
precision of parameter estimates benchmarked against external survey data.
Additionally, its similarity search and cross-spectrum prediction capabilities
offer potential for anomaly detection. Our results suggest that contrastively
trained foundation models enriched with spectrum-aware decoders can advance
precision stellar spectroscopy.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [214] [Hardware-software co-exploration with racetrack memory based in-memory computing for CNN inference in embedded systems](https://arxiv.org/abs/2507.01429)
*Benjamin Chen Ming Choong,Tao Luo,Cheng Liu,Bingsheng He,Wei Zhang,Joey Tianyi Zhou*

Key words: 轨道存储器,内存计算,卷积神经网络,嵌入式系统,协同设计

TL;DR: 该论文提出了一种针对轨道存储器的内存计算优化方案，设计了适合卷积神经网络（CNN）加速的高效算术电路，并通过系统与模型的协同设计提高性能。

<details>
  <summary>Details</summary>

Main category: cs.ET

Motivation: 解决在低资源嵌入式系统中，深度神经网络数据量大导致的存储和计算效率问题，尤其是在轨道存储器上实现高效内存计算的挑战。

Method: 设计了一系列适合乘加运算的内存计算单元，并通过系统与CNN模型的协同优化，探索轨道存储器的设计空间。

Result: 提出的电路和协同优化策略实现了小面积内存库设计，显著提高了轨道存储器系统的能效和性能。

Conclusion: 该研究为轨道存储器上的高效内存计算提供了可行方案，适用于嵌入式AI应用。

Abstract: Deep neural networks generate and process large volumes of data, posing
challenges for low-resource embedded systems. In-memory computing has been
demonstrated as an efficient computing infrastructure and shows promise for
embedded AI applications. Among newly-researched memory technologies, racetrack
memory is a non-volatile technology that allows high data density fabrication,
making it a good fit for in-memory computing. However, integrating in-memory
arithmetic circuits with memory cells affects both the memory density and power
efficiency. It remains challenging to build efficient in-memory arithmetic
circuits on racetrack memory within area and energy constraints. To this end,
we present an efficient in-memory convolutional neural network (CNN)
accelerator optimized for use with racetrack memory. We design a series of
fundamental arithmetic circuits as in-memory computing cells suited for
multiply-and-accumulate operations. Moreover, we explore the design space of
racetrack memory based systems and CNN model architectures, employing co-design
to improve the efficiency and performance of performing CNN inference in
racetrack memory while maintaining model accuracy. Our designed circuits and
model-system co-optimization strategies achieve a small memory bank area with
significant improvements in energy and performance for racetrack memory based
embedded systems.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [215] [Empirical Analysis Of Heuristic and Approximation Algorithms for the The Mutual-Visibility Problem](https://arxiv.org/abs/2507.01076)
*Vanja Stojanović,Bor Pangeršič*

Key words: 互见性问题, NP完全, 贪心启发式, 超图近似, 遗传算法

TL;DR: 论文通过实现并评估三种算法填补了NP完全互见性问题在实践行为方面缺乏实证分析的空白，结果表明在小图上算法表现符合理论界，但在大图上差距明显，遗传算法在已知最优图上表现最佳。

<details>
  <summary>Details</summary>

Main category: cs.CG

Motivation: 解决NP完全互见性问题在实践行为上缺乏实证分析的空白。

Method: 实现了三种算法（直接贪心启发式、基于超图的近似算法和遗传算法），并在多样合成图数据集上进行了评估。

Result: 小图上算法表现符合理论界，但大图上差距明显；遗传算法在已知最优图上表现最佳。

Conclusion: 虽然大图上算法表现与理论界有差距，但遗传算法在测试方法中表现最优。

Abstract: The NP-complete mutual-visibility (MV) problem currently lacks empirical
analysis on its practical behaviour despite theoretical studies. This paper
addresses this gap by implementing and evaluating three distinct algorithms - a
direct greedy heuristic, a hypergraph-based approximation, and a genetic
algorithm - on diverse synthetic graph datasets, including those with
analytically known $\mu(G)$ values and general graph models. Our results
demonstrate that for smaller graphs, the algorithms consistently achieve MV set
sizes aligning with theoretical bounds. However, for larger instances, achieved
solution sizes notably diverge from theoretical limits; this, combined with the
absence of tight bounds, complicates absolute quality assessment. Nevertheless,
validation on known optimal graphs showed the Genetic Algorithm and other
heuristics empirically performing best among tested methods.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [216] [GPU-based complete search for nonlinear minimization subject to bounds](https://arxiv.org/abs/2507.01770)
*Guanglu Zhang,Qihang Shan,Jonathan Cagan*

Key words: GPU计算、区间分析、全局优化、高维函数、并行编程

TL;DR: 提出一种基于GPU的全局搜索方法，用于在非线性函数中寻找全局最小值，利用区间分析和GPU计算能力，确保在存在舍入误差时仍能精确包围全局最小值。

<details>
  <summary>Details</summary>

Main category: math.NA

Motivation: 解决高维非线性函数的全局优化问题，特别是在存在舍入误差时仍能保证结果的严谨性。

Method: 结合区间分析和GPU并行计算，采用单程序单数据并行编程风格，并引入变量循环技术以减少计算成本。

Result: 成功包围了10个多模态基准测试函数（如Ackley、Griewank等）的全局最小值，维度高达10,000，远超文献报道结果。

Conclusion: 该方法在高效性和严苛性上均表现优异，为高维全局优化问题提供了可行解决方案。

Abstract: This paper introduces a GPU-based complete search method to enclose the
global minimum of a nonlinear function subject to simple bounds on the
variables. Using interval analysis, coupled with the computational power and
architecture of GPU, the method iteratively rules out the regions in the search
domain where the global minimum cannot exist and leaves a finite set of regions
where the global minimum must exist. For effectiveness, because of the rigor of
interval analysis, the method is guaranteed to enclose the global minimum of
the nonlinear function even in the presence of rounding errors. For efficiency,
the method employs a novel GPU-based single program, single data parallel
programming style to circumvent major GPU performance bottlenecks, and a
variable cycling technique is also integrated into the method to reduce
computational cost when minimizing large-scale nonlinear functions. The method
is validated by minimizing 10 multimodal benchmark test functions with scalable
dimensions, including the well-known Ackley function, Griewank function, Levy
function, and Rastrigin function. These benchmark test functions represent
grand challenges of global optimization, and enclosing the guaranteed global
minimum of these benchmark test functions with more than 80 dimensions has not
been reported in the literature. Our method completely searches the feasible
domain and successfully encloses the guaranteed global minimum of these 10
benchmark test functions with up to 10,000 dimensions using only one GPU in a
reasonable computation time, far exceeding the reported results in the
literature due to the unique method design and implementation based on GPU
architecture.

</details>


### [217] [Consistency of Learned Sparse Grid Quadrature Rules using NeuralODEs](https://arxiv.org/abs/2507.01533)
*Hanno Gottschalk,Emil Partow,Tobias J. Riedlinger*

Key words: 稀疏网格求积；高维分布；神经ODE；统计学习；Clenshaw-Curtis

TL;DR: 论文证明了稀疏网格求积法在高维分布数值积分中的一致性，通过统计学习理论和稀疏网格求积技术实现了误差控制。

<details>
  <summary>Details</summary>

Main category: math.NA

Motivation: 解决高维分布数值积分中稀疏网格求积法的统计和数值误差问题，确保积分结果的准确性和一致性。

Method: 首先利用神经ODE学习归一化映射，将分布转化为单位立方体上的噪声分布；然后结合Clenshaw-Curtis稀疏网格求积技术进行数值积分。

Result: 证明在经验风险最小化框架下，所有误差项可被控制，且数值积分随数据量和网络容量增加逼近理论值。

Conclusion: 稀疏网格求积法在高维积分中具有一致性和高效性，适用于统计和数值计算问题。

Abstract: This paper provides a proof of the consistency of sparse grid quadrature for
numerical integration of high dimensional distributions. In a first step, a
transport map is learned that normalizes the distribution to a noise
distribution on the unit cube. This step is built on the statistical learning
theory of neural ordinary differential equations, which has been established
recently. Secondly, the composition of the generative map with the quantity of
interest is integrated numerically using the Clenshaw-Curtis sparse grid
quadrature. A decomposition of the total numerical error in quadrature error
and statistical error is provided. As main result it is proven in the framework
of empirical risk minimization that all error terms can be controlled in the
sense of PAC (probably approximately correct) learning and with high
probability the numerical integral approximates the theoretical value up to an
arbitrary small error in the limit where the data set size is growing and the
network capacity is increased adaptively.

</details>


### [218] [Neural Entropy-stable conservative flux form neural networks for learning hyperbolic conservation laws](https://arxiv.org/abs/2507.01795)
*Lizuo Liu,Lu Zhang,Anne Gelb*

Key words: 双曲守恒定律, 熵稳定, 神经网络, 数据驱动, 数值流量函数

TL;DR: 该论文提出了一种神经熵稳定保守流形式的神经网络（NESCFN），用于直接从解轨迹中学习双曲守恒定律及其关联的熵函数，无需预定义的数值离散化。

<details>
  <summary>Details</summary>

Main category: math.NA

Motivation: 现有的神经网络架构虽然成功地将经典数值原理整合到学习模型中，但它们大多依赖于先验的方程知识或固定的离散化假设。本研究的目标是消除这种依赖，通过在学习过程中嵌入熵稳定设计原则，实现完全数据驱动的物理一致动态发现。

Method: 通过联合学习数值流量函数和对应的熵函数，该方法确保了守恒性和熵耗散，这对于双曲守恒定律系统的长期稳定性和保真度至关重要。

Result: 数值结果表明，该方法在长时间范围内实现了稳定性和守恒性，能够准确捕捉冲击传播速度，即使在训练数据中没有未来时间解剖面的情况下。

Conclusion: 该研究为数据驱动下的双曲守恒定律建模提供了一种无需先验离散化的新方法，展示了在保持物理一致性的同时实现长期稳定的潜力。

Abstract: We propose a neural entropy-stable conservative flux form neural network
(NESCFN) for learning hyperbolic conservation laws and their associated entropy
functions directly from solution trajectories, without requiring any predefined
numerical discretization. While recent neural network architectures have
successfully integrated classical numerical principles into learned models,
most rely on prior knowledge of the governing equations or assume a fixed
discretization. Our approach removes this dependency by embedding
entropy-stable design principles into the learning process itself, enabling the
discovery of physically consistent dynamics in a fully data-driven setting. By
jointly learning both the numerical flux function and a corresponding entropy,
the proposed method ensures conservation and entropy dissipation, critical for
long-term stability and fidelity in the system of hyperbolic conservation laws.
Numerical results demonstrate that the method achieves stability and
conservation over extended time horizons and accurately captures shock
propagation speeds, even without oracle access to future-time solution profiles
in the training data.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [219] [Search-Based Robot Motion Planning With Distance-Based Adaptive Motion Primitives](https://arxiv.org/abs/2507.01198)
*Benjamin Kraljusic,Zlatan Ajanovic,Nermin Covic,Bakir Lacevic*

Key words: 运动规划, 自适应基元, 机器人操纵器, 配置空间

TL;DR: 提出了一种结合采样和搜索的机器人运动规划算法，通过自适应运动基元提升效率。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 为提高机器人操纵器在高自由度复杂环境中的运动规划效率。

Method: 使用自由配置空间的自适应运动基元（burs）在图搜索算法中进行探索。

Result: 在复杂场景中表现优于固定基元方法，尤其适合高自由度机器人。

Conclusion: 自适应运动基元能显著提升规划效率和成功率。

Abstract: This work proposes a motion planning algorithm for robotic manipulators that
combines sampling-based and search-based planning methods. The core
contribution of the proposed approach is the usage of burs of free
configuration space (C-space) as adaptive motion primitives within the graph
search algorithm. Due to their feature to adaptively expand in free C-space,
burs enable more efficient exploration of the configuration space compared to
fixed-sized motion primitives, significantly reducing the time to find a valid
path and the number of required expansions. The algorithm is implemented within
the existing SMPL (Search-Based Motion Planning Library) library and evaluated
through a series of different scenarios involving manipulators with varying
number of degrees-of-freedom (DoF) and environment complexity. Results
demonstrate that the bur-based approach outperforms fixed-primitive planning in
complex scenarios, particularly for high DoF manipulators, while achieving
comparable performance in simpler scenarios.

</details>


### [220] [LLM-based Realistic Safety-Critical Driving Video Generation](https://arxiv.org/abs/2507.01264)
*Yongjie Fu,Ruijian Zha,Pei Tian,Xuan Di*

Key words: 自动驾驶、驾驶场景生成、大型语言模型、视频生成

TL;DR: 提出了一种利用大型语言模型（LLMs）生成驾驶场景代码的新框架，结合视频生成技术，用于CARLA模拟器中的自动驾驶系统测试。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 自动驾驶系统的测试需要多样性和安全关键性的场景，传统方法难以满足要求。

Method: 通过LLM生成安全关键场景脚本，并结合视频生成技术提升场景的真实性。

Result: 实验表明，该方法能高效生成多样且真实的驾驶场景，包含边缘案例。

Conclusion: 该框架为自动驾驶系统的模拟测试提供了有效的工具。

Abstract: Designing diverse and safety-critical driving scenarios is essential for
evaluating autonomous driving systems. In this paper, we propose a novel
framework that leverages Large Language Models (LLMs) for few-shot code
generation to automatically synthesize driving scenarios within the CARLA
simulator, which has flexibility in scenario scripting, efficient code-based
control of traffic participants, and enforcement of realistic physical
dynamics. Given a few example prompts and code samples, the LLM generates
safety-critical scenario scripts that specify the behavior and placement of
traffic participants, with a particular focus on collision events. To bridge
the gap between simulation and real-world appearance, we integrate a video
generation pipeline using Cosmos-Transfer1 with ControlNet, which converts
rendered scenes into realistic driving videos. Our approach enables
controllable scenario generation and facilitates the creation of rare but
critical edge cases, such as pedestrian crossings under occlusion or sudden
vehicle cut-ins. Experimental results demonstrate the effectiveness of our
method in generating a wide range of realistic, diverse, and safety-critical
scenarios, offering a promising tool for simulation-based testing of autonomous
vehicles.

</details>


### [221] [VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process](https://arxiv.org/abs/2507.01284)
*Cristian Gariboldi,Hayato Tokida,Ken Kinjo,Yuki Asada,Alexander Carballo*

Key words: 视觉语言模型, 自动驾驶, 端到端系统, 空间推理, nuScenes数据集

TL;DR: 本文提出了一种名为VLAD的视觉语言自动驾驶模型，通过将细调后的视觉语言模型（VLM）与先进的端到端系统VAD结合，提升了自动驾驶的感知、预测和规划能力。实验表明，该系统在nuScenes数据集上减少了31.82%的平均碰撞率，为VLM增强的自动驾驶系统设立了新标杆。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 近年来，开源视觉语言模型（如LLaVA、Qwen-VL和Llama）的发展为多系统集成提供了机会。本文旨在利用这些模型的互联网规模通用知识，提升自动驾驶的感知、预测和规划能力。

Method: VLAD模型通过专门的细调方法，使用自定义的问答数据集增强空间推理能力，生成高层次导航指令，并由VAD系统处理以指导车辆操作。系统还提供驾驶决策的自然语言解释，提升透明度。

Result: 在nuScenes数据集上的评估显示，VLAD系统平均碰撞率比基线方法降低了31.82%。

Conclusion: VLAD模型成功结合了视觉语言模型与端到端自动驾驶系统，显著提升了驾驶安全性和透明度，为未来研究提供了新方向。

Abstract: Recent advancements in open-source Visual Language Models (VLMs) such as
LLaVA, Qwen-VL, and Llama have catalyzed extensive research on their
integration with diverse systems. The internet-scale general knowledge
encapsulated within these models presents significant opportunities for
enhancing autonomous driving perception, prediction, and planning capabilities.
In this paper we propose VLAD, a vision-language autonomous driving model,
which integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end
system. We implement a specialized fine-tuning approach using custom
question-answer datasets designed specifically to improve the spatial reasoning
capabilities of the model. The enhanced VLM generates high-level navigational
commands that VAD subsequently processes to guide vehicle operation.
Additionally, our system produces interpretable natural language explanations
of driving decisions, thereby increasing transparency and trustworthiness of
the traditionally black-box end-to-end architecture. Comprehensive evaluation
on the real-world nuScenes dataset demonstrates that our integrated system
reduces average collision rates by 31.82% compared to baseline methodologies,
establishing a new benchmark for VLM-augmented autonomous driving systems.

</details>


### [222] [Quantum-Assisted Automatic Path-Planning for Robotic Quality Inspection in Industry 4.0](https://arxiv.org/abs/2507.01462)
*Eneko Osaba,Estibaliz Garrote,Pablo Miranda-Rodriguez,Alessia Ciacco,Itziar Cabanes,Aitziber Mancisidor*

Key words: 量子计算, 机器人检测, 旅行商问题, 工业自动化, D-Wave

TL;DR: 该研究探索混合量子-经典算法在工业环境中优化CAD模型生成的机器人检测路径的应用，对比量子与经典方法的性能。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 探索量子算法在工业自动化中的潜在应用，尤其是在优化机器人检测路径方面。

Method: 将任务建模为带有不完全图和开放路径约束的3D旅行商问题，并比较D-Wave量子求解器与GUROBI、Google OR-Tools等经典方法。

Result: 在五个实际案例中，量子方法展现出与经典方法相当的解决方案质量，同时显著减少了计算时间。

Conclusion: 量子方法在工业4.0自动化领域具有潜力，特别是在优化计算时间方面。

Abstract: This work explores the application of hybrid quantum-classical algorithms to
optimize robotic inspection trajectories derived from Computer-Aided Design
(CAD) models in industrial settings. By modeling the task as a 3D variant of
the Traveling Salesman Problem, incorporating incomplete graphs and open-route
constraints, this study evaluates the performance of two D-Wave-based solvers
against classical methods such as GUROBI and Google OR-Tools. Results across
five real-world cases demonstrate competitive solution quality with
significantly reduced computation times, highlighting the potential of quantum
approaches in automation under Industry 4.0.

</details>


### [223] [BioMARS: A Multi-Agent Robotic System for Autonomous Biological Experiments](https://arxiv.org/abs/2507.01485)
*Yibo Qiu,Zan Huang,Zhiyu Wang,Handi Liu,Yiling Qiao,Yifeng Hu,Shu'ang Sun,Hangke Peng,Ronald X Xu,Mingzhai Sun*

Key words: BioMARS, LLMs, VLMs, 模块化机器人, 实验室自动化

TL;DR: BioMARS是一个集成LLMs、VLMs和模块化机器人技术的智能平台，旨在自主设计、计划和执行生物实验，显著提升实验效率和性能。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决当前大语言模型和视觉语言模型在生物研究中因僵化的协议设计、动态实验室条件适应性不足、错误处理不完善和高操作复杂性而受限的问题。

Method: 采用分层架构：Biologist Agent合成协议，Technician Agent转化为伪代码，Inspector Agent确保程序完整性。系统支持上下文优化和实时人机协作。

Result: 在细胞培养和分化为视网膜色素上皮细胞任务中，性能与人工操作相当或更优。

Conclusion: BioMARS展示了AI驱动的通用实验室自动化的可行性，语言推理在生物研究中具有变革性作用。

Abstract: Large language models (LLMs) and vision-language models (VLMs) have the
potential to transform biological research by enabling autonomous
experimentation. Yet, their application remains constrained by rigid protocol
design, limited adaptability to dynamic lab conditions, inadequate error
handling, and high operational complexity. Here we introduce BioMARS
(Biological Multi-Agent Robotic System), an intelligent platform that
integrates LLMs, VLMs, and modular robotics to autonomously design, plan, and
execute biological experiments. BioMARS uses a hierarchical architecture: the
Biologist Agent synthesizes protocols via retrieval-augmented generation; the
Technician Agent translates them into executable robotic pseudo-code; and the
Inspector Agent ensures procedural integrity through multimodal perception and
anomaly detection. The system autonomously conducts cell passaging and culture
tasks, matching or exceeding manual performance in viability, consistency, and
morphological integrity. It also supports context-aware optimization,
outperforming conventional strategies in differentiating retinal pigment
epithelial cells. A web interface enables real-time human-AI collaboration,
while a modular backend allows scalable integration with laboratory hardware.
These results highlight the feasibility of generalizable, AI-driven laboratory
automation and the transformative role of language-based reasoning in
biological research.

</details>


### [224] [A Review on Sound Source Localization in Robotics: Focusing on Deep Learning Methods](https://arxiv.org/abs/2507.01143)
*Reza Jalayer,Masoud Jalayer,Amirali Baniasadi*

Key words: 关键词：声源定位、深度学习、机器人、TDOA、CNN

TL;DR: 概述：本文综述了深度学习在机器人声源定位（SSL）中的最新进展，填补了现有文献的不足，并提出了未来的研究方向。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 动机：现有综述通常关注通用音频应用，未能充分考虑机器人限制和深度学习的最新进展。本研究旨在填补这一空白，提供以机器人为中心的综述。

Method: 方法：回顾了经典方法（如TDOA、波束成形等）和现代深度学习技术（如CNN、CRNN、注意力机制），并探讨了数据和训练策略。

Result: 结果：总结了当前SSL研究的挑战，如环境鲁棒性、多声源问题，以及深度学习在SSL中的实现约束。

Conclusion: 结论：提出了未来方向，以实现下一代机器人中稳健、高效、可解释的深度学习SSL。

Abstract: Sound source localization (SSL) adds a spatial dimension to auditory
perception, allowing a system to pinpoint the origin of speech, machinery
noise, warning tones, or other acoustic events, capabilities that facilitate
robot navigation, human-machine dialogue, and condition monitoring. While
existing surveys provide valuable historical context, they typically address
general audio applications and do not fully account for robotic constraints or
the latest advancements in deep learning. This review addresses these gaps by
offering a robotics-focused synthesis, emphasizing recent progress in deep
learning methodologies. We start by reviewing classical methods such as Time
Difference of Arrival (TDOA), beamforming, Steered-Response Power (SRP), and
subspace analysis. Subsequently, we delve into modern machine learning (ML) and
deep learning (DL) approaches, discussing traditional ML and neural networks
(NNs), convolutional neural networks (CNNs), convolutional recurrent neural
networks (CRNNs), and emerging attention-based architectures. The data and
training strategy that are the two cornerstones of DL-based SSL are explored.
Studies are further categorized by robot types and application domains to
facilitate researchers in identifying relevant work for their specific
contexts. Finally, we highlight the current challenges in SSL works in general,
regarding environmental robustness, sound source multiplicity, and specific
implementation constraints in robotics, as well as data and learning strategies
in DL-based SSL. Also, we sketch promising directions to offer an actionable
roadmap toward robust, adaptable, efficient, and explainable DL-based SSL for
next-generation robots.

</details>


### [225] [Jump-Start Reinforcement Learning with Self-Evolving Priors for Extreme Monopedal Locomotion](https://arxiv.org/abs/2507.01243)
*Ziang Zheng,Guojian Zhan,Shiqi Liu,Yao Lyu,Tao Zhang,Shengbo Eben Li*

Key words: 强化学习,四足机器人,极端地形,JumpER,自进化先验

TL;DR: JumpER是一种通过自进化先验实现跳跃启动的强化学习框架，分阶段训练策略以应对极端欠驱动和极端地形的双重挑战，首次实现了四足机器人在不可预测地形上的稳健单腿跳跃。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决传统强化学习在四足机器人中同时应对极端欠驱动和极端地形时的不稳定早期交互和不可靠奖励反馈问题。

Method: 提出JumpER框架，通过分阶段学习动态生成自进化先验，逐步优化策略，无需依赖外部专家先验或人工奖励整形。

Result: 机器人能够在60厘米宽间隙、不规则楼梯和15-35厘米间距的踏石上完成单腿跳跃，远超传统方法。

Conclusion: JumpER为解决极端欠驱动和极端地形下的运动任务提供了可扩展的原则性方法。

Abstract: Reinforcement learning (RL) has shown great potential in enabling quadruped
robots to perform agile locomotion. However, directly training policies to
simultaneously handle dual extreme challenges, i.e., extreme underactuation and
extreme terrains, as in monopedal hopping tasks, remains highly challenging due
to unstable early-stage interactions and unreliable reward feedback. To address
this, we propose JumpER (jump-start reinforcement learning via self-evolving
priors), an RL training framework that structures policy learning into multiple
stages of increasing complexity. By dynamically generating self-evolving priors
through iterative bootstrapping of previously learned policies, JumpER
progressively refines and enhances guidance, thereby stabilizing exploration
and policy optimization without relying on external expert priors or
handcrafted reward shaping. Specifically, when integrated with a structured
three-stage curriculum that incrementally evolves action modality, observation
space, and task objective, JumpER enables quadruped robots to achieve robust
monopedal hopping on unpredictable terrains for the first time. Remarkably, the
resulting policy effectively handles challenging scenarios that traditional
methods struggle to conquer, including wide gaps up to 60 cm, irregularly
spaced stairs, and stepping stones with distances varying from 15 cm to 35 cm.
JumpER thus provides a principled and scalable approach for addressing
locomotion tasks under the dual challenges of extreme underactuation and
extreme terrains.

</details>


### [226] [AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation](https://arxiv.org/abs/2507.01961)
*Sixiang Chen,Jiaming Liu,Siyuan Qian,Han Jiang,Lily Li,Renrui Zhang,Zhuoyang Liu,Chenyang Gu,Chengkai Hou,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Key words: 移动操作, 语言条件控制, 多模态感知, 自适应协调

TL;DR: 提出AC-DiT模型，通过自适应的移动底盘和机械臂协调策略及多模态感知条件，提升语言条件控制的移动操作性能。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 现有方法在移动底盘和机械臂协调以及多模态感知上存在不足，导致误差积累和感知需求不匹配。

Method: 采用移动底盘动作表示作为上下文先验，动态调整2D与3D感知的融合权重。

Result: 在仿真和真实世界的移动操作任务中验证了AC-DiT的有效性。

Conclusion: AC-DiT显著提升了移动操作的协调能力和多模态感知适应性。

Abstract: Recently, mobile manipulation has attracted increasing attention for enabling
language-conditioned robotic control in household tasks. However, existing
methods still face challenges in coordinating mobile base and manipulator,
primarily due to two limitations. On the one hand, they fail to explicitly
model the influence of the mobile base on manipulator control, which easily
leads to error accumulation under high degrees of freedom. On the other hand,
they treat the entire mobile manipulation process with the same visual
observation modality (e.g., either all 2D or all 3D), overlooking the distinct
multimodal perception requirements at different stages during mobile
manipulation. To address this, we propose the Adaptive Coordination Diffusion
Transformer (AC-DiT), which enhances mobile base and manipulator coordination
for end-to-end mobile manipulation. First, since the motion of the mobile base
directly influences the manipulator's actions, we introduce a mobility-to-body
conditioning mechanism that guides the model to first extract base motion
representations, which are then used as context prior for predicting whole-body
actions. This enables whole-body control that accounts for the potential impact
of the mobile base's motion. Second, to meet the perception requirements at
different stages of mobile manipulation, we design a perception-aware
multimodal conditioning strategy that dynamically adjusts the fusion weights
between various 2D visual images and 3D point clouds, yielding visual features
tailored to the current perceptual needs. This allows the model to, for
example, adaptively rely more on 2D inputs when semantic information is crucial
for action prediction, while placing greater emphasis on 3D geometric
information when precise spatial understanding is required. We validate AC-DiT
through extensive experiments on both simulated and real-world mobile
manipulation tasks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [227] [Asymptotic convexity of wide and shallow neural networks](https://arxiv.org/abs/2507.01044)
*Vivek Borkar,Parthe Pandit*

Key words: 神经网络,凸函数,性能分析

TL;DR: 研究表明，浅而宽的神经网络的输入输出映射在参数空间中的表现近似于凸函数的性质，这解释了其良好的性能。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 探究浅而宽神经网络表现优异的原因。

Method: 分析输入输出映射的凸性。

Result: 发现其映射近似凸函数。

Conclusion: 这种凸性可能是性能优越的关键。

Abstract: For a simple model of shallow and wide neural networks, we show that the
epigraph of its input-output map as a function of the network parameters
approximates epigraph of a. convex function in a precise sense. This leads to a
plausible explanation of their observed good performance.

</details>


### [228] [Parsimonious Gaussian mixture models with piecewise-constant eigenvalue profiles](https://arxiv.org/abs/2507.01542)
*Tom Szwagier,Pierre-Alexandre Mattei,Charles Bouveyron,Xavier Pennec*

Key words: 高斯混合模型、分段常数协方差、EM算法、无监督学习

TL;DR: 提出了一种新的高斯混合模型（GMM）变体，通过分段常数协方差特征值轮廓来平衡灵活性和复杂性，改进了传统模型的局限性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 传统全协方差GMM在高维空间中过参数化，而球形GMM缺乏灵活性，无法拟合各向异性分布。为解决这一问题，提出了一种介于两者之间的新模型。

Method: 引入分段常数协方差特征值轮廓的GMM族，扩展了低秩模型（如MPPCA）。通过EM算法学习参数，并提出组件惩罚EM算法联合学习超参数。

Result: 在密度拟合、聚类和单图像去噪等实验中，新模型在似然性和简洁性方面表现优于传统模型。

Conclusion: 新模型在灵活性和复杂度之间取得了更好的平衡，适用于多种无监督学习任务。

Abstract: Gaussian mixture models (GMMs) are ubiquitous in statistical learning,
particularly for unsupervised problems. While full GMMs suffer from the
overparameterization of their covariance matrices in high-dimensional spaces,
spherical GMMs (with isotropic covariance matrices) certainly lack flexibility
to fit certain anisotropic distributions. Connecting these two extremes, we
introduce a new family of parsimonious GMMs with piecewise-constant covariance
eigenvalue profiles. These extend several low-rank models like the celebrated
mixtures of probabilistic principal component analyzers (MPPCA), by enabling
any possible sequence of eigenvalue multiplicities. If the latter are
prespecified, then we can naturally derive an expectation-maximization (EM)
algorithm to learn the mixture parameters. Otherwise, to address the
notoriously-challenging issue of jointly learning the mixture parameters and
hyperparameters, we propose a componentwise penalized EM algorithm, whose
monotonicity is proven. We show the superior likelihood-parsimony tradeoffs
achieved by our models on a variety of unsupervised experiments: density
fitting, clustering and single-image denoising.

</details>


### [229] [When Less Is More: Binary Feedback Can Outperform Ordinal Comparisons in Ranking Recovery](https://arxiv.org/abs/2507.01613)
*Shirong Xu,Jingnan Zhang,Junhui Wang*

Key words: 配对比较、序数数据、二元化、排名恢复、信号噪声比、模式函数

TL;DR: 论文挑战了序数比较数据比二元比较数据信息更丰富的传统观点，提出了一种无平局的序数配对比较的广义参数化框架，并证明在此框架下，二元化序数数据可以显著提高排名恢复的准确性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 探索序数和二元配对比较数据在排名和偏好学习任务中的表现差异，证明二元化序数数据在某些情况下能更高效地恢复排名。

Method: 提出一种广义加性结构的参数化框架，包括量化偏好差异的链接函数和控制序数响应分布的模式函数，并通过计数算法分析排名错误率。

Result: 理论证明和实验验证显示，二元比较数据的排名错误率收敛速度更快，且信号噪声比（SNR）明显优于序数数据。

Conclusion: 通过优化模式函数，二元化能够最大化性能提升，优于传统序数比较方法。

Abstract: Paired comparison data, where users evaluate items in pairs, play a central
role in ranking and preference learning tasks. While ordinal comparison data
intuitively offer richer information than binary comparisons, this paper
challenges that conventional wisdom. We propose a general parametric framework
for modeling ordinal paired comparisons without ties. The model adopts a
generalized additive structure, featuring a link function that quantifies the
preference difference between two items and a pattern function that governs the
distribution over ordinal response levels. This framework encompasses classical
binary comparison models as special cases, by treating binary responses as
binarized versions of ordinal data. Within this framework, we show that
binarizing ordinal data can significantly improve the accuracy of ranking
recovery. Specifically, we prove that under the counting algorithm, the ranking
error associated with binary comparisons exhibits a faster exponential
convergence rate than that of ordinal data. Furthermore, we characterize a
substantial performance gap between binary and ordinal data in terms of a
signal-to-noise ratio (SNR) determined by the pattern function. We identify the
pattern function that minimizes the SNR and maximizes the benefit of
binarization. Extensive simulations and a real application on the MovieLens
dataset further corroborate our theoretical findings.

</details>


### [230] [A generative modeling / Physics-Informed Neural Network approach to random differential equations](https://arxiv.org/abs/2507.01687)
*Georgios Arampatzis,Stylianos Katsarakis,Charalambos Makridakis*

Key words: 科学机器学习, 不确定性量化, 物理信息神经网络, 生成建模

TL;DR: 该论文通过将概率框架引入物理信息神经网络（PINNs），结合生成建模技术，有效建模复杂系统中的不确定性，并提出一种系统性控制不确定性的方法。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 科学机器学习（SciML）与不确定性量化（UQ）的结合是计算科学快速发展的前沿领域，该方法旨在解决复杂系统中的不确定性表示问题。

Method: 结合生成建模技术与PINNs，引入概率框架，系统性控制不确定性。

Result: 通过随机微分方程和随机偏微分方程的应用验证了方法的有效性。

Conclusion: 提出的方法在保持模型预测精度的同时，有效提升了不确定性建模能力。

Abstract: The integration of Scientific Machine Learning (SciML) techniques with
uncertainty quantification (UQ) represents a rapidly evolving frontier in
computational science. This work advances Physics-Informed Neural Networks
(PINNs) by incorporating probabilistic frameworks to effectively model
uncertainty in complex systems. Our approach enhances the representation of
uncertainty in forward problems by combining generative modeling techniques
with PINNs. This integration enables in a systematic fashion uncertainty
control while maintaining the predictive accuracy of the model. We demonstrate
the utility of this method through applications to random differential
equations and random partial differential equations (PDEs).

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [231] [A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory](https://arxiv.org/abs/2507.01110)
*Felix Windisch,Lukas Radl,Thomas Köhler,Michael Steiner,Dieter Schmalstieg,Markus Steinberger*

Key words: Gaussian Splatting, Level-of-Detail, hierarchical data structure, real-time rendering

TL;DR: 提出了一种名为'A LoD of Gaussians'的框架，用于在单GPU上训练和渲染超大规模高斯场景，无需分区。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 解决现有Gaussian Splatting技术在大规模场景中因分区带来的边界伪影、训练复杂性和GPU内存限制问题。

Method: 结合高斯层级和顺序点树的混合数据结构，实现动态流式加载相关高斯数据，并利用轻量级缓存和视图调度系统支持实时渲染。

Result: 实现了无缝的多尺度重建和复杂场景的交互式可视化，覆盖从广域视图到细粒度地面细节。

Conclusion: A LoD of Gaussians框架有效解决了大规模高斯场景的训练和渲染挑战，适用于多种规模的应用场景。

Abstract: Gaussian Splatting has emerged as a high-performance technique for novel view
synthesis, enabling real-time rendering and high-quality reconstruction of
small scenes. However, scaling to larger environments has so far relied on
partitioning the scene into chunks -- a strategy that introduces artifacts at
chunk boundaries, complicates training across varying scales, and is poorly
suited to unstructured scenarios such as city-scale flyovers combined with
street-level views. Moreover, rendering remains fundamentally limited by GPU
memory, as all visible chunks must reside in VRAM simultaneously. We introduce
A LoD of Gaussians, a framework for training and rendering ultra-large-scale
Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our
method stores the full scene out-of-core (e.g., in CPU memory) and trains a
Level-of-Detail (LoD) representation directly, dynamically streaming only the
relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with
Sequential Point Trees enables efficient, view-dependent LoD selection, while a
lightweight caching and view scheduling system exploits temporal coherence to
support real-time streaming and rendering. Together, these innovations enable
seamless multi-scale reconstruction and interactive visualization of complex
scenes -- from broad aerial views to fine-grained ground-level details.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [232] [Systemic Constraints of Undecidability](https://arxiv.org/abs/2507.01036)
*Seth Bulin*

Key words: 系统性不可判定性,因果嵌入,不可计算性,闭包原则,科学知识边界

TL;DR: 论文提出系统性不可判定性理论，将不可计算性视为系统的结构属性而非特定函数或问题的局部特征，证明了子系统如何继承不可判定性。

<details>
  <summary>Details</summary>

Main category: cs.FL

Motivation: 通过将不可计算性视为系统属性，挑战传统观点，揭示其在自然和人工系统中的普遍限制。

Method: 定义因果嵌入概念并证明闭包原则，展示子系统如何继承不可判定性。

Result: 不可判定性成为预测、建模和认知访问的普遍限制，反驳了通过架构创新绕过计算限制的观点。

Conclusion: 系统性不可判定性理论扩展了逻辑研究的轨迹，为计算能力的拓扑结构及其与科学知识边界的关系提供了新视角。

Abstract: This paper presents a theory of systemic undecidability, reframing
incomputability as a structural property of systems rather than a localized
feature of specific functions or problems. We define a notion of causal
embedding and prove a closure principle: any subsystem that participates
functionally in the computation of an undecidable system inherits its
undecidability. This result positions undecidability as a pervasive constraint
on prediction, modeling, and epistemic access in both natural and artificial
systems. Our framework disarms oracle mimicry and challenges the view that
computational limits can be circumvented through architectural innovation. By
generalizing classical results into a dynamic systems context, this work
augments the logical trajectory of G\"odel, Turing, and Chaitin, offering a new
perspective of the topology of computability and its interrelation to the
boundaries of scientific knowledge.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [233] [Dynamic Similarity Graph Construction with Kernel Density Estimation](https://arxiv.org/abs/2507.01696)
*Steinar Laenen,Peter Macgregor,He Sun*

Key words: 核密度估计, 动态数据结构, 谱聚类, 相似性图

TL;DR: 提出了一种动态核密度估计（KDE）数据结构，用于维护查询点的估计，并基于此设计了动态谱聚类算法。

<details>
  <summary>Details</summary>

Main category: cs.DS

Motivation: 解决动态设置下的KDE问题，实现对数据点动态添加时的高效估计，并应用于动态相似性图和谱聚类。

Method: 开发动态KDE数据结构，支持高效更新查询点估计；基于此构建稀疏近似相似图，实现动态谱聚类算法。

Result: 在合成和真实数据集上验证了算法的高效性和有效性。

Conclusion: 提出的动态KDE和谱聚类算法在动态环境下具有高效性和实用性。

Abstract: In the kernel density estimation (KDE) problem, we are given a set $X$ of
data points in $\mathbb{R}^d$, a kernel function $k: \mathbb{R}^d \times
\mathbb{R}^d \rightarrow \mathbb{R}$, and a query point $\mathbf{q} \in
\mathbb{R}^d$, and the objective is to quickly output an estimate of
$\sum_{\mathbf{x} \in X} k(\mathbf{q}, \mathbf{x})$. In this paper, we consider
$\textsf{KDE}$ in the dynamic setting, and introduce a data structure that
efficiently maintains the estimates for a set of query points as data points
are added to $X$ over time. Based on this, we design a dynamic data structure
that maintains a sparse approximation of the fully connected similarity graph
on $X$, and develop a fast dynamic spectral clustering algorithm. We further
evaluate the effectiveness of our algorithms on both synthetic and real-world
datasets.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [234] [Capacity Planning and Scheduling for Jobs with Uncertainty in Resource Usage and Duration](https://arxiv.org/abs/2507.01225)
*Sunandita Patra,Mehtab Pathan,Mahmoud Mahfouz,Parisa Zehtabi,Wided Ouaja,Daniele Magazzeni,Manuela Veloso*

Key words: 容量规划, 作业调度, 不确定性, 混合云, 约束编程

TL;DR: 该论文提出了一种在混合云和本地服务器环境中进行容量规划和作业调度的近似方法，重点关注资源使用和作业持续时间的不确定性。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 随着云计算基础设施的普及，组织需要同时在云和本地服务器环境中进行资源规划和作业调度，特别是在金融行业中，市场条件的不确定性对作业特性有显著影响。

Method: 采用确定性估计器和基于配对采样的约束编程方法，平衡资源使用最小化和服务质量最大化两个冲突目标。

Result: 基于配对采样的方法显著降低了峰值资源使用，同时未影响服务质量。

Conclusion: 该方法为混合环境中的资源规划和作业调度提供了一种高效解决方案。

Abstract: Organizations around the world schedule jobs (programs) regularly to perform
various tasks dictated by their end users. With the major movement towards
using a cloud computing infrastructure, our organization follows a hybrid
approach with both cloud and on-prem servers. The objective of this work is to
perform capacity planning, i.e., estimate resource requirements, and job
scheduling for on-prem grid computing environments. A key contribution of our
approach is handling uncertainty in both resource usage and duration of the
jobs, a critical aspect in the finance industry where stochastic market
conditions significantly influence job characteristics. For capacity planning
and scheduling, we simultaneously balance two conflicting objectives: (a)
minimize resource usage, and (b) provide high quality-of-service to the end
users by completing jobs by their requested deadlines. We propose approximate
approaches using deterministic estimators and pair sampling-based constraint
programming. Our best approach (pair sampling-based) achieves much lower peak
resource usage compared to manual scheduling without compromising on the
quality-of-service.

</details>


### [235] [EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices](https://arxiv.org/abs/2507.01438)
*Zheyu Shen,Yexiao He,Ziyao Wang,Yuning Zhang,Guoheng Sun,Wanghao Ye,Ang Li*

Key words: 大型语言模型、边缘计算、LoRA、多租户、适配器选择、内存管理

TL;DR: EdgeLoRA是一种高效系统，用于在多租户边缘设备上部署大型语言模型（LLMs），通过自适应适配器选择、异构内存管理和批量LoRA推理，显著提升吞吐量和降低延迟。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 在多租户边缘设备上高效部署LLMs面临适配器选择复杂性和内存开销等挑战，EdgeLoRA旨在解决这些问题，提升资源利用率。

Method: 提出EdgeLoRA系统，包括自适应适配器选择机制、异构内存管理和批量LoRA推理三大创新点。

Result: 实验表明，EdgeLoRA在吞吐量和延迟方面显著优于现有方案（llama.cpp），吞吐量提升4倍，支持更多适配器同时运行。

Conclusion: EdgeLoRA为资源受限的多租户边缘环境提供了一种可扩展且高效的LLM部署方案。

Abstract: Large Language Models (LLMs) have gained significant attention due to their
versatility across a wide array of applications. Fine-tuning LLMs with
parameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these
models to efficiently adapt to downstream tasks without extensive retraining.
Deploying fine-tuned LLMs on multi-tenant edge devices offers substantial
benefits, such as reduced latency, enhanced privacy, and personalized
responses. However, serving LLMs efficiently on resource-constrained edge
devices presents critical challenges, including the complexity of adapter
selection for different tasks and memory overhead from frequent adapter
swapping. Moreover, given the multiple requests in multi-tenant settings,
processing requests sequentially results in underutilization of computational
resources and increased latency. This paper introduces EdgeLoRA, an efficient
system for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA
incorporates three key innovations: (1) an adaptive adapter selection mechanism
to streamline the adapter configuration process; (2) heterogeneous memory
management, leveraging intelligent adapter caching and pooling to mitigate
memory operation overhead; and (3) batch LoRA inference, enabling efficient
batch processing to significantly reduce computational latency. Comprehensive
evaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly
outperforms the status quo (i.e., llama.cpp) in terms of both latency and
throughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times
boost in throughput. Even more impressively, it can serve several orders of
magnitude more adapters simultaneously. These results highlight EdgeLoRA's
potential to transform edge deployment of LLMs in multi-tenant scenarios,
offering a scalable and efficient solution for resource-constrained
environments.

</details>


### [236] [Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization](https://arxiv.org/abs/2507.01676)
*Giuseppe Ruggeri,Renzo Andri,Daniele Jahier Pagliari,Lukas Cavigelli*

Key words: Deep Recommender Models, embedding layers, data flows, SoC, AI accelerators

TL;DR: 论文提出了一种针对深度推荐模型中嵌入层的高效数据流设计，通过四种策略在单核上优化嵌入查找，并提出了一个框架将表格不对称映射到多核SoC上。在华为Ascend AI加速器上测试显示性能显著提升。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 深度推荐模型的嵌入层是性能瓶颈，尤其在随机内存访问方面，因此需要优化嵌入查找效率。

Method: 提出了四种单核嵌入查找策略和一个多核SoC不对称映射框架。

Result: 在华为Ascend AI加速器上实现了1.5x至6.5x的速度提升，极端情况下可达20x。

Conclusion: 该方法显著提升了嵌入查找效率，且对查询分布的依赖性更低。

Abstract: Deep Recommender Models (DLRMs) inference is a fundamental AI workload
accounting for more than 79% of the total AI workload in Meta's data centers.
DLRMs' performance bottleneck is found in the embedding layers, which perform
many random memory accesses to retrieve small embedding vectors from tables of
various sizes. We propose the design of tailored data flows to speedup
embedding look-ups. Namely, we propose four strategies to look up an embedding
table effectively on one core, and a framework to automatically map the tables
asymmetrically to the multiple cores of a SoC. We assess the effectiveness of
our method using the Huawei Ascend AI accelerators, comparing it with the
default Ascend compiler, and we perform high-level comparisons with Nvidia
A100. Results show a speed-up varying from 1.5x up to 6.5x for real workload
distributions, and more than 20x for extremely unbalanced distributions.
Furthermore, the method proves to be much more independent of the query
distribution than the baseline.

</details>


### [237] [Evolving HPC services to enable ML workloads on HPE Cray EX](https://arxiv.org/abs/2507.01880)
*Stefano Schuppli,Fawzi Mohamed,Henrique Mendonça,Nina Mujkanovic,Elia Palme,Dino Conciatore,Lukas Drescher,Miguel Gila,Pim Witlox,Joost VandeVondele,Maxime Martinasso,Thomas C. Schulthess,Torsten Hoefler*

Key words: HPC, 机器学习, 基础设施, 技术增强, 瑞士AI

TL;DR: 本文探讨了如何通过技术增强HPC（高性能计算）服务以更好地支持机器学习（ML）工作负载，提出了多项改进措施，旨在提高系统可用性、适应性，并满足ML社区的需求。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 传统的HPC服务无法完全满足ML社区的动态需求，因此需要扩展其功能以更好地支持ML工作负载。

Method: 通过用户环境设计、性能筛查工具、可观察性能力、节点审查工具、服务平面基础设施和专用存储基础设施等技术改进。

Result: 提出的改进措施旨在优化ML工作负载在HPC系统中的执行，提升系统可用性和适应性。

Conclusion: 这些技术增强措施不仅解决了ML社区的需求，还反映了HPC基础设施服务的未来发展方向。

Abstract: The Alps Research Infrastructure leverages GH200 technology at scale,
featuring 10,752 GPUs. Accessing Alps provides a significant computational
advantage for researchers in Artificial Intelligence (AI) and Machine Learning
(ML). While Alps serves a broad range of scientific communities, traditional
HPC services alone are not sufficient to meet the dynamic needs of the ML
community. This paper presents an initial investigation into extending HPC
service capabilities to better support ML workloads. We identify key challenges
and gaps we have observed since the early-access phase (2023) of Alps by the
Swiss AI community and propose several technological enhancements. These
include a user environment designed to facilitate the adoption of HPC for ML
workloads, balancing performance with flexibility; a utility for rapid
performance screening of ML applications during development; observability
capabilities and data products for inspecting ongoing large-scale ML workloads;
a utility to simplify the vetting of allocated nodes for compute readiness; a
service plane infrastructure to deploy various types of workloads, including
support and inference services; and a storage infrastructure tailored to the
specific needs of ML workloads. These enhancements aim to facilitate the
execution of ML workloads on HPC systems, increase system usability and
resilience, and better align with the needs of the ML community. We also
discuss our current approach to security aspects. This paper concludes by
placing these proposals in the broader context of changes in the communities
served by HPC infrastructure like ours.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [238] [Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems](https://arxiv.org/abs/2507.01599)
*Zhaoyan Sun,Jiayi Wang,Xinyang Zhao,Jiachi Wang,Guoliang Li*

Key words: Data Agent, LLM, Data+AI, Orchestration, Reasoning, Planning

TL;DR: 论文提出了‘数据代理’概念，旨在通过集成知识理解、推理和规划能力来协调Data+AI生态系统，解决现有系统在语义理解、推理和规划方面的不足。

<details>
  <summary>Details</summary>

Main category: cs.DB

Motivation: 传统Data+AI系统依赖人工专家协调系统流程，难以适应数据、查询、任务和环境的变化。大型语言模型（LLMs）的成功为解决这一问题提供了机会。

Method: 提出‘数据代理’架构，集成知识理解、推理和规划能力，解决数据相关任务，包括管道协调、优化和执行等。

Result: 展示了多种数据代理系统，如数据科学代理、数据分析代理和数据库管理员代理，并探讨其应用示例。

Conclusion: 数据代理通过LLM技术为Data+AI系统带来革新，但仍存在开放挑战需进一步研究。

Abstract: Traditional Data+AI systems utilize data-driven techniques to optimize
performance, but they rely heavily on human experts to orchestrate system
pipelines, enabling them to adapt to changes in data, queries, tasks, and
environments. For instance, while there are numerous data science tools
available, developing a pipeline planning system to coordinate these tools
remains challenging. This difficulty arises because existing Data+AI systems
have limited capabilities in semantic understanding, reasoning, and planning.
Fortunately, we have witnessed the success of large language models (LLMs) in
enhancing semantic understanding, reasoning, and planning abilities. It is
crucial to incorporate LLM techniques to revolutionize data systems for
orchestrating Data+AI applications effectively.
  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive
architecture designed to orchestrate Data+AI ecosystems, which focuses on
tackling data-related tasks by integrating knowledge comprehension, reasoning,
and planning capabilities. We delve into the challenges involved in designing
data agents, such as understanding data/queries/environments/tools,
orchestrating pipelines/workflows, optimizing and executing pipelines, and
fostering pipeline self-reflection. Furthermore, we present examples of data
agent systems, including a data science agent, data analytics agents (such as
unstructured data analytics agent, semantic structured data analytics agent,
data lake analytics agent, and multi-modal data analytics agent), and a
database administrator (DBA) agent. We also outline several open challenges
associated with designing data agent systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [239] [Hello Afrika: Speech Commands in Kinyarwanda](https://arxiv.org/abs/2507.01024)
*George Igwegbe,Martins Awojide,Mboh Bless,Nirel Kadzo*

Key words: 语音命令, Kinyarwanda, 语音识别, Hello Afrika, 非洲语言

TL;DR: 论文摘要描述了Hello Afrika项目的首个迭代模型，旨在解决非洲语言语音命令模型的不足，特别是针对Kinyarwanda语言。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 非洲语言的语音命令模型稀缺，而语音命令对于日常设备（尤其是为残障人士设计的设备）的非接触控制至关重要。

Method: 基于自定义的语音命令语料库（包含通用指令、数字和唤醒词），构建语音识别模型，并在多种设备上部署和评估性能。

Result: 模型在PC、手机和边缘设备上成功部署，并通过适当的指标评估了性能。

Conclusion: Hello Afrika项目的首个Kinyarwanda语音命令模型填补了非洲语言语音技术的空白，展示了其可行性和实用性。

Abstract: Voice or Speech Commands are a subset of the broader Spoken Word Corpus of a
language which are essential for non-contact control of and activation of
larger AI systems in devices used in everyday life especially for persons with
disabilities. Currently, there is a dearth of speech command models for African
languages. The Hello Afrika project aims to address this issue and its first
iteration is focused on the Kinyarwanda language since the country has shown
interest in developing speech recognition technologies culminating in one of
the largest datasets on Mozilla Common Voice. The model was built off a custom
speech command corpus made up of general directives, numbers, and a wake word.
The final model was deployed on multiple devices (PC, Mobile Phone and Edge
Devices) and the performance was assessed using suitable metrics.

</details>


### [240] [Scalable Offline ASR for Command-Style Dictation in Courtrooms](https://arxiv.org/abs/2507.01021)
*Kumarmanas Nethil,Vaibhav Mishra,Kriti Anandan,Kavya Manohar*

Key words: 开源框架, VAD, Whisper模型, ASR架构, 多路复用

TL;DR: 开源框架利用VAD分割音频并并行转录，提高效率，兼容多种ASR架构。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 解决在线系统资源密集和批处理高延迟之间的差距。

Method: 使用VAD分割音频，通过Whisper模型并行转录，实现多路复用。

Result: 在印度15%的法庭部署，实时数据显示并发用户增加时延迟显著降低。

Conclusion: 该框架高效、兼容性强，适合实际应用。

Abstract: We propose an open-source framework for Command-style dictation that
addresses the gap between resource-intensive Online systems and high-latency
Batch processing. Our approach uses Voice Activity Detection (VAD) to segment
audio and transcribes these segments in parallel using Whisper models, enabling
efficient multiplexing across audios. Unlike proprietary systems like
SuperWhisper, this framework is also compatible with most ASR architectures,
including widely used CTC-based models. Our multiplexing technique maximizes
compute utilization in real-world settings, as demonstrated by its deployment
in around 15% of India's courtrooms. Evaluations on live data show consistent
latency reduction as user concurrency increases, compared to sequential batch
processing. The live demonstration will showcase our open-sourced
implementation and allow attendees to interact with it in real-time.

</details>


### [241] [Workflow-Based Evaluation of Music Generation Systems](https://arxiv.org/abs/2507.01022)
*Shayan Dadman,Bernt Arild Bremdal,Andreas Bergsland*

Key words: 音乐生成系统, 音乐制作, 评估框架, AI协作工具, 艺术真实性

TL;DR: 该研究通过评估八种开源音乐生成系统（MGS），探讨了它们在当代音乐制作工作流程中的实际应用。研究结合技术分析和实践实验，提出了一个评估框架，重点考察系统在音乐制作中的实用性和创造性。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 研究旨在了解当前MGS在音乐制作中的局限性、工作流集成中的挑战与机遇，并探索它们作为协作工具的发展潜力，同时保持艺术真实性。

Method: 采用单一评估者的初步方法，结合定性分析和定量指标，评估了符号和音频两类音乐生成系统的多样化架构。

Result: 研究发现MGS主要作为补充工具增强人类专业知识，而非替代。它们在保持主题和结构连贯性方面存在局限性，凸显了人类创造力在复杂决策和情感表达中的不可替代性。

Conclusion: 研究提出了一个结构化评估框架，为未来MGS的综合评估指明了方法论改进方向，并确定了AI作为协作工具在创意工作流程中的可行领域。

Abstract: This study presents an exploratory evaluation of Music Generation Systems
(MGS) within contemporary music production workflows by examining eight
open-source systems. The evaluation framework combines technical insights with
practical experimentation through criteria specifically designed to investigate
the practical and creative affordances of the systems within the iterative,
non-linear nature of music production. Employing a single-evaluator methodology
as a preliminary phase, this research adopts a mixed approach utilizing
qualitative methods to form hypotheses subsequently assessed through
quantitative metrics. The selected systems represent architectural diversity
across both symbolic and audio-based music generation approaches, spanning
composition, arrangement, and sound design tasks. The investigation addresses
limitations of current MGS in music production, challenges and opportunities
for workflow integration, and development potential as collaborative tools
while maintaining artistic authenticity. Findings reveal these systems function
primarily as complementary tools enhancing rather than replacing human
expertise. They exhibit limitations in maintaining thematic and structural
coherence that emphasize the indispensable role of human creativity in tasks
demanding emotional depth and complex decision-making. This study contributes a
structured evaluation framework that considers the iterative nature of music
creation. It identifies methodological refinements necessary for subsequent
comprehensive evaluations and determines viable areas for AI integration as
collaborative tools in creative workflows. The research provides
empirically-grounded insights to guide future development in the field.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [242] [Age Sensitive Hippocampal Functional Connectivity: New Insights from 3D CNNs and Saliency Mapping](https://arxiv.org/abs/2507.01411)
*Yifei Sun,Marshall A. Dalton,Robert D. Sanders,Yixuan Yuan,Xiang Li,Sharon L. Naismith,Fernando Calamante,Jinglei Lv*

Key words: 海马体, 功能连接, 大脑年龄, 3D CNN, 可解释深度学习

TL;DR: 该研究开发了一个可解释的深度学习框架，结合3D CNN和LayerCAM显著性映射，从海马体功能连接预测大脑年龄，揭示了与年龄密切相关的海马体-皮质连接模式。

<details>
  <summary>Details</summary>

Main category: q-bio.NC

Motivation: 研究动机在于理解海马体功能连接在衰老过程中的变化机制，弥补现有研究的不足。

Method: 采用基于种子的功能连接分析，结合三维卷积神经网络和LayerCAM显著性映射，预测大脑年龄并识别关键连接区域。

Result: 研究发现海马体与前丘脑、楔叶、后扣带皮层等区域的连接对年龄高度敏感，且前后海马体的功能连接模式存在差异。

Conclusion: 该研究揭示了海马体衰老的功能机制，展示了可解释深度学习在神经影像数据分析中的潜力。

Abstract: Grey matter loss in the hippocampus is a hallmark of neurobiological aging,
yet understanding the corresponding changes in its functional connectivity
remains limited. Seed-based functional connectivity (FC) analysis enables
voxel-wise mapping of the hippocampus's synchronous activity with cortical
regions, offering a window into functional reorganization during aging. In this
study, we develop an interpretable deep learning framework to predict brain age
from hippocampal FC using a three-dimensional convolutional neural network (3D
CNN) combined with LayerCAM saliency mapping. This approach maps key
hippocampal-cortical connections, particularly with the precuneus, cuneus,
posterior cingulate cortex, parahippocampal cortex, left superior parietal
lobule, and right superior temporal sulcus, that are highly sensitive to age.
Critically, disaggregating anterior and posterior hippocampal FC reveals
distinct mapping aligned with their known functional specializations. These
findings provide new insights into the functional mechanisms of hippocampal
aging and demonstrate the power of explainable deep learning to uncover
biologically meaningful patterns in neuroimaging data.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [243] [Epistemic Scarcity: The Economics of Unresolvable Unknowns](https://arxiv.org/abs/2507.01483)
*Craig S Wright*

Key words: 人工智能, 算法治理, 奥地利学派, 经济协调, 伦理AI

TL;DR: 本文从实践学角度分析人工智能和算法治理，挑战机器系统维持经济与认知秩序的能力假设，提出AI无法完成经济协调的核心功能，并批判主流伦理AI框架。

<details>
  <summary>Details</summary>

Main category: econ.GN

Motivation: 探讨AI在经济协调和伦理编码中的局限性，揭示其无法替代人类主观判断和责任承担的特性。

Method: 结合米塞斯的先验推理和奥地利企业家理论，批判性地分析AI在决策、伦理和信息处理中的不足。

Result: 指出AI无法产生规范、解读制度或承担责任，信息过载影响真相辨别，AI争论关乎人类自治和制度演进。

Conclusion: 奥地利学派的行为、主观性和自发秩序理论为应对计算社会控制提供了唯一连贯的替代方案。

Abstract: This paper presents a praxeological analysis of artificial intelligence and
algorithmic governance, challenging assumptions about the capacity of machine
systems to sustain economic and epistemic order. Drawing on Misesian a priori
reasoning and Austrian theories of entrepreneurship, we argue that AI systems
are incapable of performing the core functions of economic coordination:
interpreting ends, discovering means, and communicating subjective value
through prices. Where neoclassical and behavioural models treat decisions as
optimisation under constraint, we frame them as purposive actions under
uncertainty.
  We critique dominant ethical AI frameworks such as Fairness, Accountability,
and Transparency (FAT) as extensions of constructivist rationalism, which
conflict with a liberal order grounded in voluntary action and property rights.
Attempts to encode moral reasoning in algorithms reflect a misunderstanding of
ethics and economics. However complex, AI systems cannot originate norms,
interpret institutions, or bear responsibility. They remain opaque, misaligned,
and inert.
  Using the concept of epistemic scarcity, we explore how information abundance
degrades truth discernment, enabling both entrepreneurial insight and soft
totalitarianism. Our analysis ends with a civilisational claim: the debate over
AI concerns the future of human autonomy, institutional evolution, and reasoned
choice. The Austrian tradition, focused on action, subjectivity, and
spontaneous order, offers the only coherent alternative to rising computational
social control.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [244] [STEM Diffraction Pattern Analysis with Deep Learning Networks](https://arxiv.org/abs/2507.01889)
*Sebastian Wissel,Jonas Scheunert,Aaron Dextre,Shamail Ahmed,Andreas Bayer,Kerstin Volz,Bai-Xiang Xu*

Key words: 晶粒取向, 机器学习, STEM衍射图, 深度学习, 锂镍氧化物

TL;DR: 论文提出了一种基于机器学习的电子衍射图分析新方法，用于高效预测晶粒取向，优于传统方法。

<details>
  <summary>Details</summary>

Main category: cond-mat.dis-nn

Motivation: 锂镍氧化物（LiNiO₂）作为下一代锂离子电池材料，其性能与微观结构紧密相关。传统取向分析方法效率低且易受噪声干扰。

Method: 采用三种深度学习架构（CNN、DenseNet和Swin Transformer）直接从STEM衍射图预测欧拉角，实现自动化高分辨率取向制图。

Result: Swin Transformer表现最佳，生成清晰晶界和一致的晶内取向分布，验证了注意力机制的有效性。

Conclusion: 研究表明机器学习结合STEM数据可高效表征微观结构，为材料分析提供了新途径。

Abstract: Accurate grain orientation mapping is essential for understanding and
optimizing the performance of polycrystalline materials, particularly in
energy-related applications. Lithium nickel oxide (LiNiO$_{2}$) is a promising
cathode material for next-generation lithium-ion batteries, and its
electrochemical behaviour is closely linked to microstructural features such as
grain size and crystallographic orientations. Traditional orientation mapping
methods--such as manual indexing, template matching (TM), or Hough
transform-based techniques--are often slow and noise-sensitive when handling
complex or overlapping patterns, creating a bottleneck in large-scale
microstructural analysis. This work presents a machine learning-based approach
for predicting Euler angles directly from scanning transmission electron
microscopy (STEM) diffraction patterns (DPs). This enables the automated
generation of high-resolution crystal orientation maps, facilitating the
analysis of internal microstructures at the nanoscale. Three deep learning
architectures--convolutional neural networks (CNNs), Dense Convolutional
Networks (DenseNets), and Shifted Windows (Swin) Transformers--are evaluated,
using an experimentally acquired dataset labelled via a commercial TM
algorithm. While the CNN model serves as a baseline, both DenseNets and Swin
Transformers demonstrate superior performance, with the Swin Transformer
achieving the highest evaluation scores and the most consistent microstructural
predictions. The resulting crystal maps exhibit clear grain boundary
delineation and coherent intra-grain orientation distributions, underscoring
the potential of attention-based architectures for analyzing diffraction-based
image data. These findings highlight the promise of combining advanced machine
learning models with STEM data for robust, high-throughput microstructural
characterization.

</details>
