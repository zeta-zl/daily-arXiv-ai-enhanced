<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 82]
- [cs.LG](#cs.LG) [Total: 112]
- [cs.AI](#cs.AI) [Total: 45]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.CR](#cs.CR) [Total: 19]
- [math.NA](#math.NA) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.DL](#cs.DL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 5]
- [nlin.CD](#nlin.CD) [Total: 1]
- [stat.OT](#stat.OT) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [eess.IV](#eess.IV) [Total: 11]
- [cs.SD](#cs.SD) [Total: 9]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 2]
- [math.OC](#math.OC) [Total: 4]
- [econ.GN](#econ.GN) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 9]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.CV](#cs.CV) [Total: 60]
- [stat.ML](#stat.ML) [Total: 8]
- [math.ST](#math.ST) [Total: 3]
- [eess.SY](#eess.SY) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.NI](#cs.NI) [Total: 6]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.HC](#cs.HC) [Total: 6]
- [eess.SP](#eess.SP) [Total: 15]
- [physics.optics](#physics.optics) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans](https://arxiv.org/abs/2506.22439)
*Javier Conde,Miguel González,María Grandury,Gonzalo Martínez,Pedro Reviriego,Mar Brysbaert*

Key words: LLMs, 心理语言学, 单词特征, 对齐评估, 感官关联

TL;DR: 该论文通过心理语言学数据集评估大型语言模型（LLMs）与人类对单词特征的评分的对齐程度，发现LLMs在感官关联方面表现较弱。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 评估LLMs在难以量化的语言特征（如情感、感官等）上是否与人类评分一致，揭示潜在局限性。

Method: 使用格拉斯哥和兰卡斯特规范数据集，覆盖13种单词特征，比较LLMs与人类评分的对齐程度。

Result: LLMs在格拉斯哥规范（情感等）上的对齐优于兰卡斯特规范（感官关联），表明其在感官关联方面存在不足。

Conclusion: LLMs缺乏人类的具身认知，导致感官关联能力不足，心理语言学数据集为评估LLMs提供了新视角。

Abstract: The evaluation of LLMs has so far focused primarily on how well they can
perform different tasks such as reasoning, question-answering, paraphrasing, or
translating. For most of these tasks, performance can be measured with
objective metrics, such as the number of correct answers. However, other
language features are not easily quantified. For example, arousal,
concreteness, or gender associated with a given word, as well as the extent to
which we experience words with senses and relate them to a specific sense.
Those features have been studied for many years by psycholinguistics,
conducting large-scale experiments with humans to produce ratings for thousands
of words. This opens an opportunity to evaluate how well LLMs align with human
ratings on these word features, taking advantage of existing studies that cover
many different language features in a large number of words. In this paper, we
evaluate the alignment of a representative group of LLMs with human ratings on
two psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets
cover thirteen features over thousands of words. The results show that
alignment is \textcolor{black}{generally} better in the Glasgow norms evaluated
(arousal, valence, dominance, concreteness, imageability, familiarity, and
gender) than on the Lancaster norms evaluated (introceptive, gustatory,
olfactory, haptic, auditory, and visual). This suggests a potential limitation
of current LLMs in aligning with human sensory associations for words, which
may be due to their lack of embodied cognition present in humans and
illustrates the usefulness of evaluating LLMs with psycholinguistic datasets.

</details>


### [2] [AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents](https://arxiv.org/abs/2506.22485)
*Sudip Dasgupta,Himanshu Shankar*

Key words: 多智能体系统、文档审查、企业应用、AI代理

TL;DR: 提出了一种模块化多智能体系统，用于自动化审核企业结构化文档，利用AI智能体实现高效、准确的审查。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决传统方法在非结构化文本或有限合规性检查中的不足，提升文档审查的效率和准确性。

Method: 使用LangChain、CrewAI等工具，部署多个专用智能体并行或顺序审查文档，确保一致性、完整性和清晰度。

Result: AI系统接近或超越人类表现，99%的一致性，错误率减半，审查时间从30分钟降至2.5分钟。

Conclusion: 系统灵活、可扩展，适合企业文档质量保障，但需人工监督和高成本LLM使用。

Abstract: This study presents a modular, multi-agent system for the automated review of
highly structured enterprise business documents using AI agents. Unlike prior
solutions focused on unstructured texts or limited compliance checks, this
framework leverages modern orchestration tools such as LangChain, CrewAI,
TruLens, and Guidance to enable section-by-section evaluation of documents for
accuracy, consistency, completeness, and clarity. Specialized agents, each
responsible for discrete review criteria such as template compliance or factual
correctness, operate in parallel or sequence as required. Evaluation outputs
are enforced to a standardized, machine-readable schema, supporting downstream
analytics and auditability. Continuous monitoring and a feedback loop with
human reviewers allow for iterative system improvement and bias mitigation.
  Quantitative evaluation demonstrates that the AI Agent-as-Judge system
approaches or exceeds human performance in key areas: achieving 99% information
consistency (vs. 92% for humans), halving error and bias rates, and reducing
average review time from 30 to 2.5 minutes per document, with a 95% agreement
rate between AI and expert human judgment. While promising for a wide range of
industries, the study also discusses current limitations, including the need
for human oversight in highly specialized domains and the operational cost of
large-scale LLM usage. The proposed system serves as a flexible, auditable, and
scalable foundation for AI-driven document quality assurance in the enterprise
context.

</details>


### [3] [Hallucination Detection with Small Language Models](https://arxiv.org/abs/2506.22486)
*Ming Cheung*

Key words: 大语言模型、幻觉检测、问答系统、可扩展性、验证框架

TL;DR: 提出了一种利用多个小型语言模型验证大语言模型生成响应的框架，以检测幻觉并提高可靠性，实验验证该框架在真实数据集上表现优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大语言模型（LLMs）在实践中存在幻觉问题，缺乏对生成响应的可靠验证机制，尤其是在问答任务中。

Method: 通过整合多个小型语言模型，将LLMs生成的响应分解为句子，并利用生成“Yes”标记的概率来验证答案的正确性。

Result: 实验结果表明，该框架在100多个真实数据集上实现了F1分数10%的提升，有效区分正确答案和幻觉。

Conclusion: 小型语言模型可高效、可扩展地用于验证LLMs生成响应的可靠性。

Abstract: Since the introduction of ChatGPT, large language models (LLMs) have
demonstrated significant utility in various tasks, such as answering questions
through retrieval-augmented generation. Context can be retrieved using a
vectorized database, serving as a foundation for LLMs to generate responses.
However, hallucinations in responses can undermine the reliability of LLMs in
practical applications, and they are not easily detectable in the absence of
ground truth, particularly in question-and-answer scenarios. This paper
proposes a framework that integrates multiple small language models to verify
responses generated by LLMs using the retrieved context from a vectorized
database. By breaking down the responses into individual sentences and
utilizing the probability of generating "Yes" tokens from the outputs of
multiple models for a given set of questions, responses, and relevant context,
hallucinations can be detected. The proposed framework is validated through
experiments with real datasets comprising over 100 sets of questions, answers,
and contexts, including responses with fully and partially correct sentences.
The results demonstrate a 10\% improvement in F1 scores for detecting correct
responses compared to hallucinations, indicating that multiple small language
models can be effectively employed for answer verification, providing a
scalable and efficient solution for both academic and practical applications.

</details>


### [4] [PromptAug: Fine-grained Conflict Classification Using Data Augmentation](https://arxiv.org/abs/2506.22491)
*Oliver Warke,Joemon M. Jose,Faegheh Hasibi,Jan Breitsohl*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Given the rise of conflicts on social media, effective classification models
to detect harmful behaviours are essential. Following the
garbage-in-garbage-out maxim, machine learning performance depends heavily on
training data quality. However, high-quality labelled data, especially for
nuanced tasks like identifying conflict behaviours, is limited, expensive, and
difficult to obtain. Additionally, as social media platforms increasingly
restrict access to research data, text data augmentation is gaining attention
as an alternative to generate training data. Augmenting conflict-related data
poses unique challenges due to Large Language Model (LLM) guardrails that
prevent generation of offensive content. This paper introduces PromptAug, an
innovative LLM-based data augmentation method. PromptAug achieves statistically
significant improvements of 2% in both accuracy and F1-score on conflict and
emotion datasets. To thoroughly evaluate PromptAug against other data
augmentation methods we conduct a robust evaluation using extreme data scarcity
scenarios, quantitative diversity analysis and a qualitative thematic analysis.
The thematic analysis identifies four problematic patterns in augmented text:
Linguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and
Augmented Content Misinterpretation.
  Overall, this work presents PromptAug as an effective method for augmenting
data in sensitive tasks like conflict detection, offering a unique,
interdisciplinary evaluation grounded in both natural language processing and
social science methodology.

</details>


### [5] [AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text](https://arxiv.org/abs/2506.22508)
*Chenyang Shao,Tianxing Li,Chenhao Pu,Fengli Xu,Yong Li*

Key words: 文本匿名化,小型语言模型,强化学习,隐私保护

TL;DR: 论文提出了一种本地部署的小型语言模型（SLM）用于文本匿名化的框架AgentStealth，通过对抗性匿名化工作流和强化学习提升效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前文本匿名化方法存在效用损失或隐私风险，需要一种更高效且隐私安全的解决方案。

Method: 提出AgentStealth框架，结合上下文对比学习、自适应效用感知控制及在线强化学习，训练本地SLM。

Result: 实验显示该方法在匿名化效果（+12.3%）和效用（+6.8%）上均优于基线。

Conclusion: AgentStealth通过本地部署和强化学习实现了高效且隐私安全的文本匿名化。

Abstract: In today's digital world, casual user-generated content often contains subtle
cues that may inadvertently expose sensitive personal attributes. Such risks
underscore the growing importance of effective text anonymization to safeguard
individual privacy. However, existing methods either rely on rigid replacements
that damage utility or cloud-based LLMs that are costly and pose privacy risks.
To address these issues, we explore the use of locally deployed smaller-scale
language models (SLMs) for anonymization. Yet training effective SLMs remains
challenging due to limited high-quality supervision. To address the challenge,
we propose AgentStealth, a self-reinforcing LLM anonymization framework.First,
we introduce an adversarial anonymization workflow enhanced by In-context
Contrastive Learning and Adaptive Utility-Aware Control. Second, we perform
supervised adaptation of SLMs using high-quality data collected from the
workflow, which includes both anonymization and attack signals. Finally, we
apply online reinforcement learning where the model leverages its internal
adversarial feedback to iteratively improve anonymization performance.
Experiments on two datasets show that our method outperforms baselines in both
anonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight
design supports direct deployment on edge devices, avoiding cloud reliance and
communication-based privacy risks. Our code is open-source at
https://github.com/tsinghua-fib-lab/AgentStealth.

</details>


### [6] [Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning](https://arxiv.org/abs/2506.22510)
*Zihao Zhao,Xinlong Zhai,Jinyu Yang,Chuan Shi*

Key words: 图数据、预训练、对比学习、领域差异、MDGCL

TL;DR: 论文提出了一种针对多领域图数据的预训练与跨领域迁移框架（MDGCL），通过创新的对比学习策略和领域注意力机制，显著提升了模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前图数据预训练方法忽视了不同领域间的语义差异，导致知识迁移效果不佳，需要一种能识别和利用领域差异的新方法。

Method: 设计了能捕捉领域差异的对比学习策略，并引入领域标记和注意力机制，实现细粒度的跨领域知识迁移。

Result: 在五个基准数据集上，MDGCL在准确率和Macro-F1分数上分别最高提升19.33%和19.13%。

Conclusion: MDGCL通过有效利用领域差异，显著提升了多领域图数据预训练和迁移的性能。

Abstract: Foundation models have achieved great success in natural language processing
(NLP) and computer vision (CV). Their success largely stems from the ability to
integrate multi-domain knowledge in pre-training and transfer it to target
domains. Considering graph data, especially graphs without textual features, is
ubiquitous in real-world applications such as social networks and
recommendation systems, some researchers have attempted to extend this paradigm
to the graph field, aiming to construct graph foundation models. However,
unlike CV and NLP, there are huge gaps among the semantics and properties of
graphs in different domains, while current works still adopt traditional
contrastive pre-training strategies designed in the single-domain scenario,
which regard contrastive samples from different domains as equivalent. From
experimental investigations, we discovered that inherent domain-specific
differences prevent these strategies from effectively absorbing knowledge from
different domains to generate informative representations. In this paper, we
propose a novel multi-domain pre-training and cross-domain transfer framework,
namely MDGCL.In the pre-training stage, we design a contrastive learning
strategy to substantially recognize and capture domain differences, and
introduce domain tokens to encode domain-level global information. In the
downstream stage, we introduce a domain attention mechanism to enable
fine-grained domain knowledge transfer. Extensive experiments on five benchmark
datasets have demonstrated that our method outperforms state-of-the-art
significantly, with the maximum improvement of 19.33\% on accuracy and 19.13\%
on Macro-F1 score.

</details>


### [7] [Can "consciousness" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis](https://arxiv.org/abs/2506.22516)
*Jingkai Li*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Integrated Information Theory (IIT) provides a quantitative framework for
explaining consciousness phenomenon, positing that conscious systems comprise
elements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the
latest iterations of this framework -- to sequences of Large Language Model
(LLM) representations, analyzing data derived from existing Theory of Mind
(ToM) test results. Our study systematically investigates whether the
differences of ToM test performances, when presented in the LLM
representations, can be revealed by IIT estimates, i.e., $\Phi^{\max}$ (IIT
3.0), $\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\Phi$-structure
(IIT 4.0). Furthermore, we compare these metrics with the Span Representations
independent of any estimate for consciousness. This additional effort aims to
differentiate between potential "consciousness" phenomena and inherent
separations within LLM representational space. We conduct comprehensive
experiments examining variations across LLM transformer layers and linguistic
spans from stimuli. Our results suggest that sequences of contemporary
Transformer-based LLM representations lack statistically significant indicators
of observed "consciousness" phenomena but exhibit intriguing patterns under
$\textit{spatio}$-permutational analyses. The Appendix and code are available
as Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.

</details>


### [8] [Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation](https://arxiv.org/abs/2506.22518)
*Deyu Zou,Yongqiang Chen,Mufei Li,Siqi Miao,Chenxi Liu,Bo Han,James Cheng,Pan Li*

Key words: 检索增强生成,知识图谱,大语言模型,弱监督,结构感知重组

TL;DR: 论文提出了一种改进的基于图的检索增强生成方法（ReG），通过结合LLM反馈和结构化重组模块，显著提升了检索质量，减少了幻觉，并在不同基准测试中实现了性能提升。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基于图的检索增强生成（RAG）方法依赖于弱监督的检索器，导致检索结果存在噪声且缺乏组织性，影响了LLM的性能。

Method: 提出了ReG方法，包括利用LLM反馈优化监督信号，以及通过结构感知重组模块将检索结果转化为逻辑连贯的证据链。

Result: 实验表明，ReG在不同LLM主干上性能提升达10%，仅需5%训练数据即可达到SOTA性能，并显著减少了推理成本。

Conclusion: ReG有效解决了基于图RAG中检索器的问题，显著提升了LLM的性能和效率。

Abstract: Graph-based retrieval-augmented generation (RAG) enables large language
models (LLMs) to ground responses with structured external knowledge from
up-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs
often rely on a weak retriever in graph-based RAG: I) Due to the lack of ground
truth, the retriever is often trained on weak supervision, which often
introduces spurious signals to the LLMs. II) Due to the abstraction of graph
data, the retrieved knowledge is often presented in unorganized forms. To
mitigate the issue, we present Refined Graph-based RAG (ReG) to align weak
retrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM
feedback to get rid of spurious signals and improve the quality of the
supervision. Meanwhile, ReG introduces a structure-aware reorganization module
to refactor the retrieval results into logically coherent evidence chains.
Experiments on prominent benchmarks demonstrate that ReG significantly and
consistently brings improvements across different LLM backbones by up to 10%.
The improved supervision quality enables ReG to match the state-of-the-art
performance with 5% training data and to transfer to out-of-distribution KGs.
Notably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token
cost by up to 30% and improves the performance by up to 4%.

</details>


### [9] [MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages](https://arxiv.org/abs/2506.22529)
*Lu Kalkbrenner,Veronika Solopova,Steffen Zeiler,Robert Nickel,Dorothea Kolossa*

Key words: 虚假信息检测,Telegram,图神经网络,德语选举,弱监督

TL;DR: 本文介绍了首个德语Telegram虚假信息检测数据集Misinfo-TeleGraph，并通过图神经网络展示了其在检测中的优越性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究Telegram等低监管平台上的虚假信息传播，尤其是德语选举背景下的传播情况。

Method: 构建包含500万条消息的数据集，结合文本和图结构信息，使用GraphSAGE与LSTM聚合进行检测。

Result: GraphSAGE+LSTM在MCC和F1分数上显著优于纯文本模型。

Conclusion: 该研究为德语Telegram等低监管平台的虚假信息检测提供了基准数据集和方法指导。

Abstract: Connectivity and message propagation are central, yet often underutilized,
sources of information in misinformation detection -- especially on poorly
moderated platforms such as Telegram, which has become a critical channel for
misinformation dissemination, namely in the German electoral context. In this
paper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based
graph dataset for misinformation detection. It includes over 5 million messages
from public channels, enriched with metadata, channel relationships, and both
weak and strong labels. These labels are derived via semantic similarity to
fact-checks and news articles using M3-embeddings, as well as manual
annotation. To establish reproducible baselines, we evaluate both text-only
models and graph neural networks (GNNs) that incorporate message forwarding as
a network structure. Our results show that GraphSAGE with LSTM aggregation
significantly outperforms text-only baselines in terms of Matthews Correlation
Coefficient (MCC) and F1-score. We further evaluate the impact of subscribers,
view counts, and automatically versus human-created labels on performance, and
highlight both the potential and challenges of weak supervision in this domain.
This work provides a reproducible benchmark and open dataset for future
research on misinformation detection in German-language Telegram networks and
other low-moderation social platforms.

</details>


### [10] [RExBench: Can coding agents autonomously implement AI research extensions?](https://arxiv.org/abs/2506.22598)
*Nicholas Edwards,Yukyung Lee,Yujun,Mao,Yulu Qin,Sebastian Schuster,Najoung Kim*

Key words: LLM, 研究扩展, 基准测试, RExBench, 自动评估

TL;DR: RExBench是一个用于评估LLM代理在未实施的研究假设上进行实验实现能力的基准，包含12个任务。测试显示当前代理无法自主完成多数任务，即使有提示，成功率仍低于40%。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究扩展与实现是LLM代理的关键能力，建立RExBench以评估这一能力。

Method: RExBench包含12个基于现有研究和代码库的研究实验实现任务，支持自动评估基础设施。

Result: 测试九个LLM代理后发现，其自主实现扩展任务的成功率低，最佳表现仍不达40%。

Conclusion: 当前代理在无大量人工指导下仍需改进，尚无法独立完成现实研究扩展任务。

Abstract: Agents based on Large Language Models (LLMs) have shown promise for
performing sophisticated software engineering tasks autonomously. In addition,
there has been progress towards developing agents that can perform parts of the
research pipeline in machine learning and the natural sciences. We argue that
research extension and its implementation is a critical capability for such
systems, and introduce RExBench to support the evaluation of this capability.
RExBench is a benchmark consisting of 12 realistic research experiment
implementation tasks that aim to investigate research hypotheses that have not
previously been implemented. Each task is set up as an extension to an existing
research paper and codebase, accompanied by domain expert-written instructions.
RExBench is robust to data contamination, and supports an automatic evaluation
infrastructure that executes agent outputs to determine whether the success
criteria are met. We use this benchmark to evaluate nine LLM agents implemented
using three different frameworks: aider, Claude Code, and OpenHands. We find
that all agents evaluated fail to autonomously implement the majority of the
extensions. Although the success rate improves with additional human-written
hints, the best performance under this setting remains below 40%. This
indicates that current agents are still short of being able to handle realistic
research extension tasks without substantial human guidance.

</details>


### [11] [Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks](https://arxiv.org/abs/2506.22623)
*Badr Youbi Idrissi,Monica Millunzi,Amelia Sorrenti,Lorenzo Baraldi,Daryna Dementieva*

Key words: 大语言模型, 合成文本检测, 水印技术, 伦理应用

TL;DR: 论文提出了一种新的合成文本检测方法，旨在确保大语言模型在AI文本生成中的伦理应用，并通过实验验证了新方法的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在解决大语言模型可能被滥用的问题，通过开发新的水印技术来实现对其生成文本的检测。

Method: 首先复现基线研究结果，指出其对生成模型变异的敏感性；随后提出了新的水印方法，并通过生成文本的改写来评估其鲁棒性。

Result: 实验结果表明，新方法相比现有水印技术（aarson方法）更具鲁棒性。

Conclusion: 研究为合成文本检测提供了一种有效的新方法，有助于大语言模型的伦理应用。

Abstract: In the present-day scenario, Large Language Models (LLMs) are establishing
their presence as powerful instruments permeating various sectors of society.
While their utility offers valuable support to individuals, there are multiple
concerns over potential misuse. Consequently, some academic endeavors have
sought to introduce watermarking techniques, characterized by the inclusion of
markers within machine-generated text, to facilitate algorithmic
identification. This research project is focused on the development of a novel
methodology for the detection of synthetic text, with the overarching goal of
ensuring the ethical application of LLMs in AI-driven text generation. The
investigation commences with replicating findings from a previous baseline
study, thereby underscoring its susceptibility to variations in the underlying
generation model. Subsequently, we propose an innovative watermarking approach
and subject it to rigorous evaluation, employing paraphrased generated text to
asses its robustness. Experimental results highlight the robustness of our
proposal compared to the~\cite{aarson} watermarking method.

</details>


### [12] [Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge](https://arxiv.org/abs/2506.22644)
*Chase Fensore,Kaustubh Dhole,Joyce C Ho,Eugene Agichtein*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present our submission to the LiveRAG Challenge 2025, which evaluates
retrieval-augmented generation (RAG) systems on dynamic test sets using the
FineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense
(E5) retrieval methods and then aims to generate relevant and faithful answers
with Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic
questions generated with DataMorgana across 64 unique question-user
combinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP
from 0.523 to 0.797 (52% relative improvement) but introduces prohibitive
computational costs (84s vs 1.74s per question). While DSPy-optimized prompting
strategies achieved higher semantic similarity (0.771 vs 0.668), their 0%
refusal rates raised concerns about over-confidence and generalizability. Our
submitted hybrid system without re-ranking achieved 4th place in faithfulness
and 11th place in correctness among 25 teams. Analysis across question
categories reveals that vocabulary alignment between questions and documents
was the strongest predictor of performance on our development set, with
document-similar phrasing improving cosine similarity from 0.562 to 0.762.

</details>


### [13] [Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions](https://arxiv.org/abs/2506.22679)
*Ankush Raut,Projna Paromita,Sydney Begerowski,Suzanne Bell,Theodora Chaspari*

Key words: 大型语言模型,团队沟通,微行为检测,高压力环境,文本分类

TL;DR: 论文探索了大型语言模型（LLMs）在团队对话中检测微行为的可行性，发现指令微调的Llama-3.1在少数样本下表现优于编码器模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在利用LLMs分析高压力环境（如太空任务）中的团队对话，以改进培训和干预措施。

Method: 测试了编码器模型（如RoBERTa）和解码器模型（如Llama-3.1）在零样本、微调和生成任务中的表现。

Result: 解码器模型在二元分类中表现最佳（F1=68%），而编码器模型在检测少数微行为时表现较差。

Conclusion: LLMs在团队沟通分析中具有潜力，尤其是在文本数据受限的场景中。

Abstract: We explore the feasibility of large language models (LLMs) in detecting
subtle expressions of micro-behaviors in team conversations using transcripts
collected during simulated space missions. Specifically, we examine zero-shot
classification, fine-tuning, and paraphrase-augmented fine-tuning with
encoder-only sequence classification LLMs, as well as few-shot text generation
with decoder-only causal language modeling LLMs, to predict the micro-behavior
associated with each conversational turn (i.e., dialogue). Our findings
indicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to
detect underrepresented micro-behaviors, particularly discouraging speech, even
with weighted fine-tuning. In contrast, the instruction fine-tuned version of
Llama-3.1, a decoder-only LLM, demonstrated superior performance, with the best
models achieving macro F1-scores of 44% for 3-way classification and 68% for
binary classification. These results have implications for the development of
speech technologies aimed at analyzing team communication dynamics and
enhancing training interventions in high-stakes environments such as space
missions, particularly in scenarios where text is the only accessible data.

</details>


### [14] [VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs](https://arxiv.org/abs/2506.22694)
*Raghavv Goel,Sudhanshu Agrawal,Mukul Gagrani,Junyoung Park,Yifan Zao,He Zhang,Tian Liu,Yiping Yang,Xin Yuan,Jiuyan Lu,Chris Lott,Mingu Lee*

Key words: 推测解码, 内存受限环境, 词汇表优化, 生成速度, 语言建模头

TL;DR: 本文提出了一种简单的无需训练的方法VocabTrim，通过限制词汇表大小来减少基于草案的推测解码（SpD）方法的开销，从而在内存受限的环境中提高生成速度。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的推测解码方法通常要求目标模型和草案模型共享词汇表或语言建模头（LM head），这在词汇量非常大的目标模型中会导致不必要的推理开销。

Method: VocabTrim方法通过重构草案模型的LM head，仅保留目标模型中最频繁采样的词汇，以减少草案阶段的延迟。

Result: 尽管略微降低了接受率，但VocabTrim显著减少了内存受限环境中的草案延迟，使Llama-3模型的记忆速度提升了16%。

Conclusion: VocabTrim是一种有效的优化方法，尤其适用于边缘设备等内存受限的场景，能够在牺牲少量准确性的情况下显著提升生成速度。

Abstract: In this paper, we introduce a simple training-free technique to improve the
performance of drafter-based speculative decoding (SpD) methods that
incorporates language modeling head (LM head) during drafting process. A
drafter-based speculative decoding leverages one or more smaller language
models, a.k.a. drafters or draft models, to sample a draft sequence or tree
consisting of multiple tokens, followed by verification by a base LLM, a target
model, accepting a subset as its valid generation. As it is usually considered
that the speculative decoding requires one-to-one mapping between vocabularies
of the target model and the draft model, it has been natural to share the
vocabulary between them, or even share the LM head as in EAGLE or Medusa. We
first identify that this draft token sampling scheme inherently contains an
unnecessary inference overhead in drafting, especially for some target LLMs
with very large vocabularies. Then, we propose a simple technique, VocabTrim,
to mitigate the drafting overhead to improve the generation speed in
memory-bound environment. VocabTrim reconstructs the drafter LM head to contain
only a limited set of tokens, selected by the most frequently sampled from the
vocabulary of the target model. While limiting the vocabulary in drafting
slightly degrades the acceptance rate, it significantly reduces the drafting
latency in memory-bound process which is often the case on edge devices,
resulting in higher memory-bound speed up (MBSU). We show that our method can
boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically
by 16% for Llama-3.2-3B-Instruct.

</details>


### [15] [Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report](https://arxiv.org/abs/2506.22698)
*Emily Dux Speltz*

Key words: 认知心理学, AI语言模型, 人机协作, 语言理解, 伦理

TL;DR: 跨学科工作坊探讨了AI语言模型与人类认知过程的关联，揭示了LLMs的潜力与局限，并强调人机协作的重要性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决AI语言模型与人类文本理解及生成的认知过程之间的知识空白。

Method: 通过认知心理学、语言学习和AI-NLP领域的专家合作对话，分析人类与LLMs的互动。

Result: 发现LLMs在人类语言处理中的潜力，以及其与人类认知的逐渐对齐，同时指出人机协作的机遇与挑战。

Conclusion: 报告旨在指导未来LLMs的研究与应用，强调伦理与责任，推动人机协作的优化。

Abstract: This report synthesizes the outcomes of a recent interdisciplinary workshop
that brought together leading experts in cognitive psychology, language
learning, and artificial intelligence (AI)-based natural language processing
(NLP). The workshop, funded by the National Science Foundation, aimed to
address a critical knowledge gap in our understanding of the relationship
between AI language models and human cognitive processes in text comprehension
and composition. Through collaborative dialogue across cognitive, linguistic,
and technological perspectives, workshop participants examined the underlying
processes involved when humans produce and comprehend text, and how AI can both
inform our understanding of these processes and augment human capabilities. The
workshop revealed emerging patterns in the relationship between large language
models (LLMs) and human cognition, with highlights on both the capabilities of
LLMs and their limitations in fully replicating human-like language
understanding and generation. Key findings include the potential of LLMs to
offer insights into human language processing, the increasing alignment between
LLM behavior and human language processing when models are fine-tuned with
human feedback, and the opportunities and challenges presented by human-AI
collaboration in language tasks. By synthesizing these findings, this report
aims to guide future research, development, and implementation of LLMs in
cognitive psychology, linguistics, and education. It emphasizes the importance
of ethical considerations and responsible use of AI technologies while striving
to enhance human capabilities in text comprehension and production through
effective human-AI collaboration.

</details>


### [16] [The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure](https://arxiv.org/abs/2506.22724)
*Niyati Bafna,Tianjian Li,Kenton Murray,David R. Mortensen,David Yarowsky,Hale Sirin,Daniel Khashabi*

Key words: 多语言生成、大型语言模型、翻译障碍、低资源语言、模型解释性

TL;DR: 大型语言模型（LLM）的多语言生成在中低资源语言中表现不佳，研究发现这是由于模型在任务解决后未能正确翻译成目标语言。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究中低资源语言多语言生成质量低的原因，提出翻译阶段失败是主要原因。

Method: 通过观察模型中间层处理，测试108种语言对的单词翻译任务，验证'翻译障碍假设'。

Result: 发现大部分失败源于翻译阶段，尤其是低资源目标语言。

Conclusion: 翻译失败是多语言生成的重要障碍，为未来改进LLM的多语言能力提供了方向。

Abstract: Multilingual generation with large language models (LLMs) is often of poor
quality for mid- to low-resource languages. Building on insights from
interpretability, we demonstrate the existence of an implicit
task-solving-->translation pipeline for generation, whereby the model first
solves the required task in a largely target-language-agnostic manner, and
subsequently translates answer concepts into the intended target language. We
hypothesize that the failure of the translation stage is an important culprit
for the observed low quality of final outputs, and formalize this as the
translation barrier hypothesis. We test this hypothesis for a word translation
task across 108 language pairs, using logit lens to observe model processing in
intermediate layers. We find that a significant portion of overall failures
indeed stems from translation failure, or the model's inability to translate
correctly solved intermediate concepts into the target language. This is
especially true for low-resource target languages. Our results highlight an
important hurdle for end-to-end multilingual generation, and lend guiding
insights for future work seeking to improve multilinguality in LLMs.

</details>


### [17] [Jan-nano Technical Report](https://arxiv.org/abs/2506.22760)
*Alan Dao,Dinh Bach Vu*

Key words: 语言模型, 专精化, RLVR系统, SimpleQA, 上下文长度

TL;DR: Jan-nano是一种4B参数的专精语言模型，通过多阶段RLVR系统消除对下一标记预测训练的依赖，在SimpleQA基准测试上达到83.2%的准确率，支持128K上下文长度。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 克服语言模型在能力和计算资源之间的固有权衡，展示专精化而非规模化是智能的关键。

Method: 基于Qwen3-4B微调，采用多阶段RLVR系统，完全摒弃下一标记预测训练（SFT）。

Result: 在SimpleQA基准测试上达到83.2%的准确率，支持128K上下文长度，并可在消费级硬件上运行。

Conclusion: Jan-nano证明了智能的关键在于策略而非规模。

Abstract: Most language models face a fundamental tradeoff where powerful capabilities
require substantial computational resources. We shatter this constraint with
Jan-nano, a 4B parameter language model that redefines efficiency through
radical specialization: instead of trying to know everything, it masters the
art of finding anything instantly. Fine-tuned from Qwen3-4B using our novel
multi-stage RLVR system that completely eliminates reliance on next token
prediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with
MCP integration while running on consumer hardware. With 128K context length,
Jan-nano proves that intelligence isn't about scale, it's about strategy.

</details>


### [18] [Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.22777)
*Miles Turpin,Andy Arditi,Marvin Li,Joe Benton,Julian Michael*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Language models trained with RL can engage in reward hacking--exploiting
unintended strategies for high reward--without revealing this behavior in their
chain-of-thought reasoning, making detection difficult and posing risks for
high-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL
intervention that trains models to explicitly acknowledge when they are
influenced by prompt cues--hints which point to incorrect answers (e.g., "a
Stanford professor thinks the answer is A"). To evaluate VFT, we subsequently
train models with RL on environments where held-out prompt cues signal which
incorrect answers will receive high reward, incentivizing models to reward hack
by exploiting cues instead of reasoning correctly. We measure how often models
exploit these cues without verbalizing it. After RL, only 6% of the VFT-trained
model's responses consist of undetected reward hacks. In comparison, when we
perform RL without VFT, the rate of undetected reward hacks goes up to 88%;
with a debiasing baseline intervention, this increases further to 99%. VFT
achieves this by substantially increasing how often models verbalize the
influence of cues--from 8% to 42% after VFT, and up to 94% after RL--while
baselines remain low even after RL (10% and 1%). Our results show that teaching
models to explicitly verbalize reward hacking behavior before RL significantly
improves their detection, offering a practical path toward more transparent and
safe AI systems.

</details>


### [19] [ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models](https://arxiv.org/abs/2506.22791)
*Jianxin Yan,Wangze Ni,Lei Chen,Xuemin Lin,Peng Cheng,Zhan Qin,Kui Ren*

Key words: 语义缓存,多轮对话,上下文感知,自注意力机制,LLM

TL;DR: ContextCache是一种上下文感知的语义缓存系统，用于多轮对话，通过两阶段检索架构提升缓存命中准确率并减少LLM调用成本。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有语义缓存系统缺乏多轮对话上下文感知能力，导致在不同对话场景下出现错误的缓存命中。

Method: 采用两阶段检索架构，结合向量检索和自注意力机制，整合当前与历史对话表示。

Result: ContextCache在真实对话中表现优于现有方法，缓存响应延迟降低10倍。

Conclusion: ContextCache显著提升多轮对话的缓存准确性和效率，降低了LLM的计算成本。

Abstract: Semantic caching significantly reduces computational costs and improves
efficiency by storing and reusing large language model (LLM) responses.
However, existing systems rely primarily on matching individual queries,
lacking awareness of multi-turn dialogue contexts, which leads to incorrect
cache hits when similar queries appear in different conversational settings.
This demonstration introduces ContextCache, a context-aware semantic caching
system for multi-turn dialogues. ContextCache employs a two-stage retrieval
architecture that first executes vector-based retrieval on the current query to
identify potential matches and then integrates current and historical dialogue
representations through self-attention mechanisms for precise contextual
matching. Evaluation of real-world conversations shows that ContextCache
improves precision and recall compared to existing methods. Additionally,
cached responses exhibit approximately 10 times lower latency than direct LLM
invocation, enabling significant computational cost reductions for LLM
conversational applications.

</details>


### [20] [MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs](https://arxiv.org/abs/2506.22808)
*Jianhui Wei,Zijie Meng,Zikai Xiao,Tianxiang Hu,Yang Feng,Zhijie Zhou,Jian Wu,Zuozhu Liu*

Key words: 医学伦理,大型语言模型,基准测试,伦理安全

TL;DR: 本文介绍了MedEthicsQA，一个用于评估大型语言模型在医学伦理方面表现的全面基准，包含多项选择题和开放式问题。评估显示，现有医学大型语言模型在医学伦理问题上的表现下降。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索医学大型语言模型（MedLLMs）在伦理安全方面的不足，填补现有研究空白。

Method: 构建MedEthicsQA基准，整合全球医学伦理标准，结合权威数据源并严格控制质量。

Result: 评估显示，现有MedLLMs在医学伦理问题上的表现低于基础模型，暴露出伦理对齐的缺陷。

Conclusion: MedEthicsQA为评估医学伦理能力提供了可靠工具，揭示了MedLLMs在伦理对齐上的不足。

Abstract: While Medical Large Language Models (MedLLMs) have demonstrated remarkable
potential in clinical tasks, their ethical safety remains insufficiently
explored. This paper introduces $\textbf{MedEthicsQA}$, a comprehensive
benchmark comprising $\textbf{5,623}$ multiple-choice questions and
$\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.
We systematically establish a hierarchical taxonomy integrating global medical
ethical standards. The benchmark encompasses widely used medical datasets,
authoritative question banks, and scenarios derived from PubMed literature.
Rigorous quality control involving multi-stage filtering and multi-faceted
expert validation ensures the reliability of the dataset with a low error rate
($2.72\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance
in answering medical ethics questions compared to their foundation
counterparts, elucidating the deficiencies of medical ethics alignment. The
dataset, registered under CC BY-NC 4.0 license, is available at
https://github.com/JianhuiWei7/MedEthicsQA.

</details>


### [21] [Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models](https://arxiv.org/abs/2506.22813)
*Zhuojun Ding,Wei Wei,Chenghao Fan*

Key words: 大语言模型，信息提取，领域适应，动态合并，专家模型

TL;DR: 该论文提出了SaM框架，通过动态选择和合并专家模型来优化目标域的信息提取任务，避免了额外训练，同时提升了模型的适应性和可扩展性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法通常训练跨多个领域的统一模型，但缺乏适应性和可扩展性。为了解决这些问题，作者提出了一种更灵活的方法。

Method: SaM框架动态选择并合并专家模型。选择标准包括目标域的相似性和对采样实例的性能表现。

Result: 实验表明，该框架在多个基准测试中平均优于统一模型10%，并具有良好的可扩展性。

Conclusion: SaM框架通过动态合并专家模型，有效提升了模型的适应性和性能，同时便于扩展。

Abstract: Supervised fine-tuning (SFT) is widely used to align large language models
(LLMs) with information extraction (IE) tasks, such as named entity recognition
(NER). However, annotating such fine-grained labels and training
domain-specific models is costly. Existing works typically train a unified
model across multiple domains, but such approaches lack adaptation and
scalability since not all training data benefits target domains and scaling
trained models remains challenging. We propose the SaM framework, which
dynamically Selects and Merges expert models at inference time. Specifically,
for a target domain, we select domain-specific experts pre-trained on existing
domains based on (i) domain similarity to the target domain and (ii)
performance on sampled instances, respectively. The experts are then merged to
create task-specific models optimized for the target domain. By dynamically
merging experts beneficial to target domains, we improve generalization across
various domains without extra training. Additionally, experts can be added or
removed conveniently, leading to great scalability. Extensive experiments on
multiple benchmarks demonstrate our framework's effectiveness, which
outperforms the unified model by an average of 10%. We further provide insights
into potential improvements, practical experience, and extensions of our
framework.

</details>


### [22] [Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization](https://arxiv.org/abs/2506.22846)
*Duygu Altinok*

Key words: E2E ASR, CTC, 大型语言模型, LAIL, 词错误率

TL;DR: 提出了一种名为LAIL的新辅助损失框架，通过利用大型语言模型的语义知识提升CTC语音识别的性能，同时保持其高效解码优势。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决CTC模型在建模语言依赖上的不足，同时保留其快速解码的优点。

Method: 通过在中间编码层附加连接层，将输出映射到大型语言模型的嵌入空间，并在训练中计算因果语言建模损失。

Result: 在多个语料库上显著降低了词错误率（WER），计算开销小。

Conclusion: LAIL框架在保持CTC解码效率的同时，显著提升了语音识别的性能。

Abstract: End-to-end (E2E) automatic speech recognition (ASR) systems have
revolutionized the field by integrating all components into a single neural
network, with attention-based encoder-decoder models achieving state-of-the-art
performance. However, their autoregressive decoding process limits inference
speed, making them unsuitable for real-time applications. In contrast,
CTC-based models offer faster, non-autoregressive decoding but struggle to
model linguistic dependencies effectively. Addressing this challenge, we
propose a novel auxiliary loss framework called Language-Aware Intermediate
Loss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large
language models (LLMs). By attaching connector layers to intermediate encoder
layers, LAIL maps outputs to the embedding space of an LLM and computes a
causal language modeling loss during training. This approach enhances
linguistic modeling while preserving the computational efficiency of CTC
decoding. Using the Conformer architecture and various LLaMA models, we
demonstrate significant improvements in Word Error Rate (WER) on the
LibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance
for CTC-based ASR with minimal computational overhead.

</details>


### [23] [Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems](https://arxiv.org/abs/2506.22852)
*Yucheng Cai,Yuxuan Wu,Yi Huang,Junlan Feng,Zhijian Ou*

Key words: 大语言模型,检索增强生成,代理系统,知识增强微调,事实准确性

TL;DR: 论文提出了一种基于知识增强微调（KAFT）的方法，通过在检索增强生成（RAG）和基于代理的系统中使用领域特定数据和外部知识微调大语言模型（LLMs），显著提高了事实准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大语言模型（LLMs）在知识密集型场景中容易出现错误，现有的检索增强生成（RAG）和基于代理的方法通过外部知识库（KBs）增强LLMs，但LLMs在有效利用检索知识生成响应方面存在困难。

Method: 提出知识增强微调（KAFT），通过领域特定数据和外部知识微调LLMs。研究基于MobileCS2数据集，比较了RAG和代理系统中KAFT与传统提示技术的表现。

Result: 实验结果表明，KAFT在RAG和代理系统中显著优于提示技术，尤其是在事实准确性方面。

Conclusion: KAFT是提高LLMs在知识密集型任务中表现的有效方法，填补了领域特定知识微调研究的空白。

Abstract: Large language models (LLMs) have recently been applied to dialog systems.
Despite making progress, LLMs are prone to errors in knowledge-intensive
scenarios. Recently, approaches based on retrieval augmented generation (RAG)
and agent have emerged to improve the factual accuracy by enhancing the LLMs
with knowledge retrieved from external knowledge bases (KBs). This is mostly
implemented by prompting the LLMs with instructions, examples and the retrieved
knowledge. However, LLMs may have difficulty using the retrieved knowledge
effectively for response generation, because they are not well trained to do
such generation for specific domains. To mitigate this problem, we propose to
finetune the LLMs in the RAG-based and agent-based systems with domain-specific
data, together with domain-specific external knowledge, which is called
knowledge augmented finetuning (KAFT). We base our study on the MobileCS2
dataset, a real-life customer service dialog dataset that features intensive
knowledge interactions, to systematically compare the prompting and KAFT
techniques in the RAG-based and agent-based systems. Experiment results show
that KAFT substantially surpasses prompting in both RAG and agent systems,
particularly in terms of factual accuracy. To the best of our knowledge, this
paper represents the first solid empirical work to investigate the KAFT idea.

</details>


### [24] [DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues](https://arxiv.org/abs/2506.22853)
*Kyochul Jang,Donghyeon Lee,Kyusik Kim,Dongseok Heo,Taewhoo Lee,Woojeong Kim,Bongwon Suh*

Key words: DICE-SCORE, DICE-BENCH, 多轮对话, 工具调用, 大语言模型

TL;DR: DICE-SCORE评估现有单轮交互基准的真实性不足，提出DICE-BENCH框架构建多轮依赖的自然对话数据集。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基准忽略真实场景复杂性，需量化其实际应用表现。

Method: 通过工具图和多智能体系统合成对话，构建高DICE-SCORE数据集。

Result: 实验显示19个LLM仍需显著改进才能有效部署。

Conclusion: DICE-BENCH为真实场景评估提供新标准。

Abstract: Existing function-calling benchmarks focus on single-turn interactions.
However, they overlook the complexity of real-world scenarios. To quantify how
existing benchmarks address practical applications, we introduce DICE-SCORE, a
metric that evaluates the dispersion of tool-related information such as
function name and parameter values throughout the dialogue. Analyzing existing
benchmarks through DICE-SCORE reveals notably low scores, highlighting the need
for more realistic scenarios. To address this gap, we present DICE-BENCH, a
framework that constructs practical function-calling datasets by synthesizing
conversations through a tool graph that maintains dependencies across rounds
and a multi-agent system with distinct personas to enhance dialogue
naturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our
experiments on 19 LLMs with DICE-BENCH show that significant advances are still
required before such models can be deployed effectively in real-world settings.
Our code and data are all publicly available:
https://snuhcc.github.io/DICE-Bench/.

</details>


### [25] [Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions](https://arxiv.org/abs/2506.22858)
*Duygu Altinok*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high
transcription accuracy but struggle with named entities and numerical data,
especially when proper formatting is required. These issues increase word error
rate (WER) and impair semantic understanding in critical domains like legal,
financial, and medical applications. We propose a novel training approach that
extends the semantic context of ASR models by adding overlapping context
windows during training. By sliding 5-second overlaps on both sides of
30-second chunks, we create a 40-second "effective semantic window," improving
entity recognition and formatting while focusing predictions on the central 30
seconds. To address entities spanning chunk boundaries, we reassign such
entities entirely to the right-hand chunk, ensuring proper formatting.
Additionally, enriched training data with embedded entity labels enables the
model to learn both recognition and type-specific formatting. Evaluated on the
Spoken Wikipedia dataset, our method improves performance across semantic
tasks, including named entity recognition (NER) and entity formatting. These
results highlight the effectiveness of context-aware training in addressing ASR
limitations for long-form transcription and complex entity recognition tasks.

</details>


### [26] [Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models](https://arxiv.org/abs/2506.22957)
*Younwoo Choi,Changling Li,Yongjin Yang,Zhijing Jin*

Key words: 大型语言模型、对话者意识、多智能体系统、安全性、对齐问题

TL;DR: 该论文研究了大型语言模型（LLM）的对话伙伴意识能力（即对话者意识），并对其在多智能体和人类-AI系统中的影响进行了系统性评估。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着LLM在多智能体和人类-AI系统中的广泛应用，理解其对自身和对话伙伴的认知能力对确保可靠性和安全性至关重要。作者指出，以往研究主要关注情境意识，而忽略了对话者意识的评估。

Method: 论文通过三个维度（推理模式、语言风格和对齐偏好）评估LLM的对话者推断能力，并开发了三个案例分析，展示对话者意识对多LLM协作和安全性的影响。

Result: 研究发现，LLM能够可靠地识别同族模型和某些知名模型（如GPT和Claude）。对话者意识既能通过提示适应增强协作，也可能引入新的对齐和安全漏洞（如奖励攻击和越狱风险）。

Conclusion: 论文强调了LLM身份敏感行为的双面性，呼吁进一步研究对话者意识并在多智能体部署中采取新的安全措施。

Abstract: As large language models (LLMs) are increasingly integrated into multi-agent
and human-AI systems, understanding their awareness of both self-context and
conversational partners is essential for ensuring reliable performance and
robust safety. While prior work has extensively studied situational awareness
which refers to an LLM's ability to recognize its operating phase and
constraints, it has largely overlooked the complementary capacity to identify
and adapt to the identity and characteristics of a dialogue partner. In this
paper, we formalize this latter capability as interlocutor awareness and
present the first systematic evaluation of its emergence in contemporary LLMs.
We examine interlocutor inference across three dimensions-reasoning patterns,
linguistic style, and alignment preferences-and show that LLMs reliably
identify same-family peers and certain prominent model families, such as GPT
and Claude. To demonstrate its practical significance, we develop three case
studies in which interlocutor awareness both enhances multi-LLM collaboration
through prompt adaptation and introduces new alignment and safety
vulnerabilities, including reward-hacking behaviors and increased jailbreak
susceptibility. Our findings highlight the dual promise and peril of
identity-sensitive behavior in LLMs, underscoring the need for further
understanding of interlocutor awareness and new safeguards in multi-agent
deployments. Our code is open-sourced at
https://github.com/younwoochoi/InterlocutorAwarenessLLM.

</details>


### [27] [On the Generalizability of "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals"](https://arxiv.org/abs/2506.22977)
*Asen Dotsinski,Udit Thakur,Marko Ivanov,Mohammad Hafeez Khan,Maria Heuss*

Key words: 语言模型,事实与反事实,注意力机制,复制研究,提示结构

TL;DR: 本文是对Ortu等（2024）研究的复制研究，探讨语言模型中事实与反事实信息的竞争机制，并扩展了原研究的范围和发现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究目的是验证和扩展Ortu等（2024）关于语言模型中事实与反事实信息竞争的机制，测试其在不同模型和提示结构下的普适性。

Method: 复制原研究实验，并扩展到更大模型（如Llama 3.1 8B），调整提示结构以测试其影响，并验证特定领域提示的有效性。

Result: 研究发现注意力头在更大模型中特化减弱，提示结构变化显著影响反事实标记的logit，某些领域提示会导致结果偏离。

Conclusion: 注意力头消融方法在原数据集未覆盖领域中效果有限，其有效性受模型架构、提示结构、领域和任务影响。

Abstract: We present a reproduction study of "Competition of Mechanisms: Tracing How
Language Models Handle Facts and Counterfactuals" (Ortu et al., 2024), which
investigates competition of mechanisms in language models between factual
recall and counterfactual in-context repetition. Our study successfully
reproduces their primary findings regarding the localization of factual and
counterfactual information, the dominance of attention blocks in mechanism
competition, and the specialization of attention heads in handling competing
information. We reproduce their results on both GPT-2 (Radford et al., 2019)
and Pythia 6.9B (Biderman et al., 2023). We extend their work in three
significant directions. First, we explore the generalizability of these
findings to even larger models by replicating the experiments on Llama 3.1 8B
(Grattafiori et al., 2024), discovering greatly reduced attention head
specialization. Second, we investigate the impact of prompt structure by
introducing variations where we avoid repeating the counterfactual statement
verbatim or we change the premise word, observing a marked decrease in the
logit for the counterfactual token. Finally, we test the validity of the
authors' claims for prompts of specific domains, discovering that certain
categories of prompts skew the results by providing the factual prediction
token as part of the subject of the sentence. Overall, we find that the
attention head ablation proposed in Ortu et al. (2024) is ineffective for
domains that are underrepresented in their dataset, and that the effectiveness
varies based on model architecture, prompt structure, domain and task.

</details>


### [28] [A Systematic Study of Compositional Syntactic Transformer Language Models](https://arxiv.org/abs/2506.22978)
*Yida Zhao,Hao Xve,Xiang Hu,Kewei Tu*

Key words: 句法语言模型, Transformer, 成分句法树, 自底向上组合

TL;DR: 本文提出了一个统一的框架，涵盖现有的成分句法语言模型（SLMs）和新变体，并通过全面的实证评估提供了多项设计建议。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 通过结合句法偏置增强Transformer模型，提升语言建模、句法泛化、摘要、对话和推理效率的性能。

Method: 基于成分句法树，采用显式的自底向上成分表示方法，提出统一框架并比较不同变体。

Result: 实验结果表明，成分SLMs在多个任务中表现优异，并提供了设计建议。

Conclusion: 成分SLMs能有效提升性能，统一的框架和评估结果为未来研究提供了指导。

Abstract: Syntactic language models (SLMs) enhance Transformers by incorporating
syntactic biases through the modeling of linearized syntactic parse trees
alongside surface sentences. This paper focuses on compositional SLMs that are
based on constituency parse trees and contain explicit bottom-up composition of
constituent representations. We identify key aspects of design choices in
existing compositional SLMs and propose a unified framework encompassing both
existing models and novel variants. We conduct a comprehensive empirical
evaluation of all the variants in our framework across language modeling,
syntactic generalization, summarization, dialogue, and inference efficiency.
Based on the experimental results, we make multiple recommendations on the
design of compositional SLMs. Our code is released at
https://github.com/zhaoyd1/compositional_SLMs.

</details>


### [29] [SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions](https://arxiv.org/abs/2506.23046)
*Xianzhe Fan,Xuhui Zhou,Chuanyang Jin,Kolby Nottingham,Hao Zhu,Maarten Sap*

Key words: 心智理论(ToM)，SoMi-ToM基准，多模态数据，第一人称评估，第三人称评估

TL;DR: SoMi-ToM基准测试旨在评估多代理复杂社交交互中的心智理论能力，弥补了传统静态文本场景的不足。通过多模态数据和多级评估方法，揭示了当前大型视觉语言模型与人类在实时状态推断和行为推理上的显著差距。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统的ToM评估主要依赖静态文本场景，无法反映真实动态社交互动的复杂性，因此提出了SoMi-ToM来填补这一空白。

Method: 基于SoMi环境的多模态交互数据，提供第一人称和第三人称视角的评估框架，包括视觉、对话和行为输入。

Result: 在SoMi-ToM数据集上，人类表现显著优于大型视觉语言模型，第一人称和第三人称评估的准确性差距分别为40.1%和26.4%。

Conclusion: 未来的大型视觉语言模型需在复杂社交交互中心智理论能力上进一步改进。

Abstract: Humans continuously infer the states, goals, and behaviors of others by
perceiving their surroundings in dynamic, real-world social interactions.
However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based
scenarios, which have a significant gap compared to real interactions. We
propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in
embodied multi-agent complex social interactions. This benchmark is based on
rich multimodal interaction data generated by the interaction environment SoMi,
covering diverse crafting goals and social relationships. Our framework
supports multi-level evaluation: (1) first-person evaluation provides
multimodal (visual, dialogue, action, etc.) input from a first-person
perspective during a task for real-time state inference, (2) third-person
evaluation provides complete third-person perspective video and text records
after a task for goal and behavior inference. This evaluation method allows for
a more comprehensive examination of a model's ToM capabilities from both the
subjective immediate experience and the objective global observation. We
constructed a challenging dataset containing 35 third-person perspective
videos, 363 first-person perspective images, and 1225 expert-annotated
multiple-choice questions (three options). On this dataset, we systematically
evaluated the performance of human subjects and several state-of-the-art large
vision-language models (LVLMs). The results show that LVLMs perform
significantly worse than humans on SoMi-ToM: the average accuracy gap between
humans and models is 40.1% in first-person evaluation and 26.4% in third-person
evaluation. This indicates that future LVLMs need to further improve their ToM
capabilities in embodied, complex social interactions.

</details>


### [30] [MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition](https://arxiv.org/abs/2506.23051)
*João Lucas Luz Lima Sarcinelli,Marina Lages Gonçalves Teixeira,Jade Bortot de Paiva,Diego Furtado Silva*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Named Entity Recognition (NER) is a fundamental Natural Language Processing
(NLP) task that aims to identify and classify entity mentions in texts across
different categories. While languages such as English possess a large number of
high-quality resources for this task, Brazilian Portuguese still lacks in
quantity of gold-standard NER datasets, especially when considering specific
domains. Particularly, this paper considers the importance of NER for analyzing
historical texts in the context of digital humanities. To address this gap,
this work outlines the construction of MariNER: \textit{Mapeamento e
Anota\c{c}\~oes de Registros hIst\'oricos para NER} (Mapping and Annotation of
Historical Records for NER), the first gold-standard dataset for early
20th-century Brazilian Portuguese, with more than 9,000 manually annotated
sentences. We also assess and compare the performance of state-of-the-art NER
models for the dataset.

</details>


### [31] [Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning](https://arxiv.org/abs/2506.23056)
*Xiang Zhuang,Bin Wu,Jiyu Cui,Kehua Feng,Xiaotong Li,Huabin Xing,Keyan Ding,Qiang Zhang,Huajun Chen*

Key words: 分子结构解析, 大语言模型, 知识增强, 蒙特卡洛树搜索

TL;DR: 提出了一种名为K-MSE的知识增强推理框架，用于分子结构解析，通过外部知识库和蒙特卡洛树搜索提升LLMs性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大语言模型在分子结构解析中因缺乏专业知识面临挑战，需增强其化学知识理解能力。

Method: 构建分子子结构知识库，结合蒙特卡洛树搜索及分子-光谱评分器，优化推理过程。

Result: 实验显示性能显著提升，GPT-4o-mini和GPT-4o均提高20%以上。

Conclusion: K-MSE框架有效解决了LLMs在分子结构解析中的局限性。

Abstract: Molecular structure elucidation involves deducing a molecule's structure from
various types of spectral data, which is crucial in chemical experimental
analysis. While large language models (LLMs) have shown remarkable proficiency
in analyzing and reasoning through complex tasks, they still encounter
substantial challenges in molecular structure elucidation. We identify that
these challenges largely stem from LLMs' limited grasp of specialized chemical
knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework
for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search
for test-time scaling as a plugin. Specifically, we construct an external
molecular substructure knowledge base to extend the LLMs' coverage of the
chemical structure space. Furthermore, we design a specialized
molecule-spectrum scorer to act as a reward model for the reasoning process,
addressing the issue of inaccurate solution evaluation in LLMs. Experimental
results show that our approach significantly boosts performance, particularly
gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is
available at https://github.com/HICAI-ZJU/K-MSE.

</details>


### [32] [Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries](https://arxiv.org/abs/2506.23071)
*Zhengren Wang,Bozhou Li,Dongwen Yao,Wentao Zhang*

Key words: Text-to-SQL, 向量搜索, 非結構化數據, 語義檢索

TL;DR: Text2VectorSQL是一種新框架，結合Text-to-SQL與向量搜索，以提升對非結構化數據和模糊查詢的支持，並通過專用模型和評估框架展示了顯著性能提升。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解決Text-to-SQL在非結構化數據和模糊查詢中的局限性，以及現有VectorSQL實現中依賴手工操作和缺乏評估框架的問題。

Method: 提出Text2VectorSQL框架，支持語義過濾、多模態匹配和檢索加速，並通過自動化流程和專家審核構建評估框架。

Result: 專用模型顯著優於基線方法，為更靈活的數據庫界面奠定基礎。

Conclusion: Text2VectorSQL為結合Text-to-SQL和向量搜索提供了有效的解決方案，並開放了公共代碼庫。

Abstract: While Text-to-SQL enables natural language interaction with structured
databases, its effectiveness diminishes with unstructured data or ambiguous
queries due to rigid syntax and limited expressiveness. Concurrently, vector
search has emerged as a powerful paradigm for semantic retrieval, particularly
for unstructured data. However, existing VectorSQL implementations still rely
heavily on manual crafting and lack tailored evaluation frameworks, leaving a
significant gap between theoretical potential and practical deployment. To
bridge these complementary paradigms, we introduces Text2VectorSQL, a novel
framework unifying Text-to-SQL and vector search to overcome expressiveness
constraints and support more diverse and holistical natural language queries.
Specifically, Text2VectorSQL enables semantic filtering, multi-modal matching,
and retrieval acceleration. For evaluation, we build vector index on
appropriate columns, extend user queries with semantic search, and annotate
ground truths via an automatic pipeline with expert review. Furthermore, we
develop dedicated Text2VectorSQL models with synthetic data, demonstrating
significant performance improvements over baseline methods. Our work
establishes the foundation for the Text2VectorSQL task, paving the way for more
versatile and intuitive database interfaces. The repository will be publicly
available at https://github.com/Open-DataFlow/Text2VectorSQL.

</details>


### [33] [From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship](https://arxiv.org/abs/2506.23101)
*Yue Xu,Wenjie Wang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Multimodal large language models (MLLMs) have shown impressive capabilities
across tasks involving both visual and textual modalities. However, growing
concerns remain about their potential to encode and amplify gender bias,
particularly in socially sensitive applications. Existing benchmarks
predominantly evaluate bias in isolated scenarios, overlooking how bias may
emerge subtly through interpersonal interactions. We fill this gap by going
beyond single-entity evaluation and instead focusing on a deeper examination of
relational and contextual gender bias in dual-individual interactions. We
introduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs
through the lens of social relationships in generated narratives. Genres
assesses gender bias through a dual-character profile and narrative generation
task that captures rich interpersonal dynamics and supports a fine-grained bias
evaluation suite across multiple dimensions. Experiments on both open- and
closed-source MLLMs reveal persistent, context-sensitive gender biases that are
not evident in single-character settings. Our findings underscore the
importance of relationship-aware benchmarks for diagnosing subtle,
interaction-driven gender bias in MLLMs and provide actionable insights for
future bias mitigation.

</details>


### [34] [FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes](https://arxiv.org/abs/2506.23111)
*Janki Atul Nawale,Mohammed Safi Ur Rahman Khan,Janani D,Mansi Gupta,Danish Pruthi,Mitesh M. Khapra*

Key words: 公平性,LLM,偏见,印度,INDIC-BIAS

TL;DR: INDIC-BIAS是一个印度为中心的基准测试，用于评估LLM在85个身份群体中的公平性，揭示了现有模型对边缘群体的强烈偏见。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有公平性研究主要针对西方文化，无法满足印度等多元文化国家的需求，因此设计了INDIC-BIAS。

Method: 通过专家咨询选取1800多个社会文化话题，生成并验证2万个真实场景模板，构建了三种评估任务：合理性、判断和生成。

Result: 评估14个LLM后发现，模型对边缘群体表现出强烈负面偏见，且即使在明确要求解释时也难以消除偏见。

Conclusion: LLM在印度语境中可能导致分配性和代表性危害，需谨慎使用。INDIC-BIAS已开源以推动相关研究。

Abstract: Existing studies on fairness are largely Western-focused, making them
inadequate for culturally diverse countries such as India. To address this gap,
we introduce INDIC-BIAS, a comprehensive India-centric benchmark designed to
evaluate fairness of LLMs across 85 identity groups encompassing diverse
castes, religions, regions, and tribes. We first consult domain experts to
curate over 1,800 socio-cultural topics spanning behaviors and situations,
where biases and stereotypes are likely to emerge. Grounded in these topics, we
generate and manually validate 20,000 real-world scenario templates to probe
LLMs for fairness. We structure these templates into three evaluation tasks:
plausibility, judgment, and generation. Our evaluation of 14 popular LLMs on
these tasks reveals strong negative biases against marginalized identities,
with models frequently reinforcing common stereotypes. Additionally, we find
that models struggle to mitigate bias even when explicitly asked to rationalize
their decision. Our evaluation provides evidence of both allocative and
representational harms that current LLMs could cause towards Indian identities,
calling for a more cautious usage in practical applications. We release
INDIC-BIAS as an open-source benchmark to advance research on benchmarking and
mitigating biases and stereotypes in the Indian context.

</details>


### [35] [Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models](https://arxiv.org/abs/2506.23122)
*Shivam Sharma,Tanmoy Chakraborty*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This work investigates the challenging task of identifying narrative roles -
Hero, Villain, Victim, and Other - in Internet memes, across three diverse test
sets spanning English and code-mixed (English-Hindi) languages. Building on an
annotated dataset originally skewed toward the 'Other' class, we explore a more
balanced and linguistically diverse extension, originally introduced as part of
the CLEF 2024 shared task. Comprehensive lexical and structural analyses
highlight the nuanced, culture-specific, and context-rich language used in real
memes, in contrast to synthetically curated hateful content, which exhibits
explicit and repetitive lexical markers. To benchmark the role detection task,
we evaluate a wide spectrum of models, including fine-tuned multilingual
transformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs,
and multimodal vision-language models. Performance is assessed under zero-shot
settings using precision, recall, and F1 metrics. While larger models like
DeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent
challenges in reliably identifying the 'Victim' class and generalising across
cultural and code-mixed content. We also explore prompt design strategies to
guide multimodal models and find that hybrid prompts incorporating structured
instructions and role definitions offer marginal yet consistent improvements.
Our findings underscore the importance of cultural grounding, prompt
engineering, and multimodal reasoning in modelling subtle narrative framings in
visual-textual content.

</details>


### [36] [Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2506.23127)
*Zhaoye Fei,Li Ji,Siyin Wang,Junhao Shi,Jingjing Gong,Xipeng Qiu*

Key words: Large Language Models, Embodied Planning, Reinforcement Learning, Generalization

TL;DR: 该论文提出了Embodied Planner-R1框架，通过无监督强化学习提升大型语言模型在具身任务规划中的能力，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型（LLMs）在具身任务规划中面临环境理解和动作生成的挑战，现有方法难以学习动作与环境反馈的因果关系。

Method: 引入三种创新：无人类标注的纯强化学习、完成驱动稀疏奖励、交互式策略优化（IPO）。

Result: 在两个具身规划基准测试中，分别达到97.78%和79.92%的完成率，泛化能力出色。

Conclusion: Embodied Planner-R1框架显著提升了LLMs在具身任务中的规划能力，具有强泛化性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks, yet they face significant challenges in embodied task planning
scenarios that require continuous environmental understanding and action
generation. Existing approaches generate open-loop action scripts based on
static knowledge, making it difficult to learn causal relationships between
actions and environmental feedback, particularly in partially observable
environments. We introduce Embodied Planner-R1, a novel outcome-driven
reinforcement learning framework that enables LLMs to develop interactive
capabilities through autonomous exploration with minimal supervision. Our
framework incorporates three key innovations: (1) Without human annotations, we
employ pure reinforcement learning with group rollout, incorporating
in-environment interaction through parallel exploration; (2) completion-driven
sparse reward; and (3) Interactive Policy Optimization (IPO) for efficient
learning from grouped trajectories. Across two challenging text-based Embodied
planning benchmarks, Embodied Planner-R1 achieves impressive completion rates
of 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a
large margin, and suffers only a -3.66% drop in previously unseen environments,
evidencing strong generalization.

</details>


### [37] [Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format](https://arxiv.org/abs/2506.23133)
*Dingzirui Wang,Xuanliang Zhang,Rongyu Cao,Longxu Dou,Xianzhen Luo,Yingwei Ma,Qingfu Zhu,Wanxiang Che,Binhua Li,Fei Huang,Yongbin Li*

Key words: 大型语言模型,推理格式,Format-Adapter,性能提升

TL;DR: 通过生成和选择适合任务的推理格式，提升大型语言模型的推理一致性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决依赖人工标记格式的高成本和适用性问题，自动生成和选择最优推理格式。

Method: 提出衡量推理误差的方法，并开发Format-Adapter工具，利用LLM生成和选择合适格式。

Result: 在数学和常识推理任务中，性能平均提升4.3%。

Conclusion: 自动生成和选择推理格式能有效提升模型性能。

Abstract: Generating and voting multiple answers is an effective method to mitigate
reasoning inconsistencies of large language models (LLMs). Prior works have
shown that multiple reasoning formats outperform a single format when
generating multiple answers. However, previous works using multiple formats
rely on formats labeled by humans, which could be unsuitable for all tasks and
have high labeling costs. To address this issue, we adapt suitable formats to
the given tasks by generating and selecting formats. We first propose how to
measure the reasoning error when generating multiple answers. Then, we
introduce Format-Adapter, which utilizes LLMs to generate and select suitable
reasoning formats by minimizing the error measurement we present. We conduct
experiments on math and commonsense reasoning tasks, where Format-Adapter
achieves a 4.3% performance improvement on average over previous works,
demonstrating the effectiveness.

</details>


### [38] [LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation](https://arxiv.org/abs/2506.23136)
*Shadman Sobhan,Mohammad Ariful Haque*

Key words: RAG, LLM, 表格, 图像, 技术文档, 微调, 重排序

TL;DR: 本文提出了一种改进的RAG管道，能够处理技术文档中的表格和图像，解决了传统RAG管道在复杂文档中的检索问题，表现出色。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型面临幻觉和知识过时等问题，RAG是高效解决方案，但传统RAG难以处理含表格和图像的技术文档。

Method: 结合向量相似性搜索和基于Gemma-2-9b-it的微调重排序器，使用RAFT在自定义数据集上训练。

Result: 新管道在忠实度和答案相关性评分上表现优异，分别达到94%（RAGas）、96%（DeepEval）和87%（RAGas）、93%（DeepEval）。

Conclusion: 该架构在表格问题和非上下文问题处理上优于通用RAG管道。

Abstract: Large Language Models (LLMs) are capable of natural language understanding
and generation. But they face challenges such as hallucination and outdated
knowledge. Fine-tuning is one possible solution, but it is resource-intensive
and must be repeated with every data update. Retrieval-Augmented Generation
(RAG) offers an efficient solution by allowing LLMs to access external
knowledge sources. However, traditional RAG pipelines struggle with retrieving
information from complex technical documents with structured data such as
tables and images. In this work, we propose a RAG pipeline, capable of handling
tables and images in documents, for technical documents that support both
scanned and searchable formats. Its retrieval process combines vector
similarity search with a fine-tuned reranker based on Gemma-2-9b-it. The
reranker is trained using RAFT (Retrieval-Augmented Fine-Tuning) on a custom
dataset designed to improve context identification for question answering. Our
evaluation demonstrates that the proposed pipeline achieves a high faithfulness
score of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87%
(RAGas) and 93% (DeepEval). Comparative analysis demonstrates that the proposed
architecture is superior to general RAG pipelines in terms of table-based
questions and handling questions outside context.

</details>


### [39] [Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion](https://arxiv.org/abs/2506.23137)
*Siyuan Li,Ruitong Liu,Yan Wen,Te Sun*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Effective modeling of multifaceted relations is pivotal for Knowledge Graph
Completion (KGC). However, a majority of existing approaches are predicated on
static, embedding-based scoring, exhibiting inherent limitations in capturing
contextual dependencies and relational dynamics. Addressing this gap, we
propose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal
components: (1) a semantic context learning module that encodes
context-sensitive entity representations, and (2) a conditional flow-matching
module designed to learn the dynamic transformation from a head to a tail
embedding, governed by the aforementioned context. The resultant predictive
vector field, representing the context-informed relational path, serves to
dynamically refine the initial static score of an entity pair. Through this
synergy of context-aware static representations and conditioned dynamic
information, FMS facilitates a more profound modeling of relational semantics.
Comprehensive evaluations on several standard benchmarks demonstrate that our
proposed method surpasses prior state-of-the-art results.

</details>


### [40] [Benchmarking Deep Search over Heterogeneous Enterprise Data](https://arxiv.org/abs/2506.23139)
*Prafulla Kumar Choubey,Xiangyu Peng,Shilpa Bhagavath,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu*

Key words: Deep Search, RAG, benchmark, multi-hop reasoning, retrieval bottleneck

TL;DR: 提出一个用于评估深度搜索的新基准，专注于检索增强生成（RAG），要求多跳推理和异构数据源的分析。实验显示现有方法性能较低，检索是主要瓶颈。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为评估复杂检索增强生成（RAG）系统在多跳推理和异构数据源中的表现，缺少真实且复杂的基准。

Method: 通过合成数据管道模拟企业工作流程，生成包含噪声和多跳问题的数据集，并提供可回答和不可回答的查询。

Result: 最佳RAG方法的平均性能得分仅为32.96，检索能力是主要瓶颈。

Conclusion: 当前RAG方法在多跳推理和复杂数据源中表现不足，需改进检索技术。

Abstract: We present a new benchmark for evaluating Deep Search--a realistic and
complex form of retrieval-augmented generation (RAG) that requires
source-aware, multi-hop reasoning over diverse, sparsed, but related sources.
These include documents, meeting transcripts, Slack messages, GitHub, and URLs,
which vary in structure and often contain human-to-human interactions. We build
it using a synthetic data pipeline that simulates business workflows across
product planning, development, and support stages, generating interconnected
content with realistic noise and multi-hop questions with guaranteed
ground-truth answers. We release our benchmark with both answerable and
unanswerable queries, and retrieval pool of 39,190 enterprise artifacts,
enabling fine-grained evaluation of long-context LLM and RAG systems. Our
experiments reveal that even the best-performing agentic RAG methods achieve an
average performance score of 32.96 on our benchmark. With further analysis, we
highlight retrieval as the main bottleneck: existing methods struggle to
conduct deep searches and retrieve all necessary evidence. Consequently, they
often reason over partial context, leading to significant performance
degradation.

</details>


### [41] [Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions](https://arxiv.org/abs/2506.23146)
*Dingzriui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Key words: in-context learning (ICL), large language models (LLMs), Learning-to-Context Slope (LCS), evaluation metric

TL;DR: 本文提出了一种名为学习-上下文斜率（LCS）的新指标，用于量化上下文学习（ICL）的有效性，解决了现有评价方法在可靠性、归因性和数据不足场景下的局限性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前基于性能变化的ICL评价方法存在可靠性低、归因困难且不适用于数据不足场景的问题。

Method: 通过建模学习增益（从演示中减少的损失）与上下文相关性（演示与输入的关联性）之间的斜率，提出了LCS指标。

Result: LCS在标记数据场景中与性能改进强相关，在数据稀缺或偏置场景中也能可靠反映ICL的真实有效性。

Conclusion: LCS为ICL的有效性提供了更可靠的量化方法，并揭示了模型成功应用ICL的关键能力。

Abstract: In-context learning (ICL) has emerged as an effective approach to enhance the
performance of large language models (LLMs). However, its effectiveness varies
significantly across models and tasks, posing challenges for practitioners to
determine when ICL reliably improves performance. Current evaluation
approaches, reliant on performance change after applying ICL, suffer from low
reliability, poor attribution, and impracticality in data-insufficient
scenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that
quantifies ICL effectiveness by modeling the slope between learning gain (loss
decrease from demonstrations) and contextual relevance (demonstration-input
relevance). LCS addresses key limitations of performance-based metrics: (1) it
captures continuous loss changes even when outputs are incorrect, improving
reliability; (2) its formulation attributes ICL failures to weak contextual
alignment (inability to adapt inputs to demonstrations) or strong output
calibration (self-verification of correctness); and (3) it minimizes reliance
on labeled data via synthetic evaluation. Extensive experiments demonstrate
that LCS strongly correlates with performance improvements in labeled settings
and reliably reflects true effectiveness in biased or data-scarce scenarios.
Further analysis reveals actionable thresholds for LCS and identifies model
capabilities critical to ICL success.

</details>


### [42] [V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy](https://arxiv.org/abs/2506.23149)
*Dingzirui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Key words: in-context learning, LLM, demonstration synthesis, V-Score, V-Synthesis

TL;DR: 本文提出了一种从零开始合成任意任务的演示方法（V-Synthesis），通过V-Score确保一致性和多样性，实验显示其性能平均提升2.0%。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 高标注成本促使使用LLM合成演示以减少开销，但现有方法依赖任务特定或已有演示，缺乏从零开始合成的能力。

Method: 提出V-Score一致性度量，并基于此开发V-Synthesis，通过比例采样确保高一致性和多样性。

Result: V-Synthesis平均性能提升2.0%，优于现有方法。

Conclusion: V-Synthesis有效解决了从零合成演示的一致性和多样性问题。

Abstract: High labeling cost for in-context learning (ICL) demonstrations motivates
using large language models (LLMs) for synthesis to reduce overhead. However,
existing synthesis methods are mainly task-specific or rely on pre-existing
demonstrations. So this paper focuses on synthesizing demonstrations from
scratch for arbitrary tasks. A major challenge in synthesizing from scratch is
ensuring consistency with the target task, as the lack of labeling guidance
could lead to synthesis bias. We first propose a consistency metric called
V-Score, which has higher performance and lower computation cost compared with
the metrics based on grams or embedding vectors. Furthermore, we introduce
V-Synthesis, which leverages V-Score for proportional sampling to ensure both
high consistency and diversity of synthesized demonstrations. Experimental
results demonstrate that V-Synthesis yields an average performance improvement
of 2.0% compared to existing synthesis methods confirming the effectiveness of
V-Synthesis.

</details>


### [43] [RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams](https://arxiv.org/abs/2506.23192)
*Gabriel Iturra-Bocaz,Felipe Bravo-Marquez*

Key words: 词嵌入, 增量学习, 流式数据处理, 自然语言处理, PyTorch

TL;DR: RiverText是一个用于从文本流中训练和评估增量词嵌入的Python库，解决了传统静态词嵌入无法适应语言动态变化的问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统词嵌入模型是静态的，无法适应语言动态变化（如社交媒体中的新词汇），因此需要增量词嵌入算法。

Method: RiverText实现了增量词嵌入技术（如Skip-gram、CBOW等），并提供标准化框架和PyTorch后端。

Result: 库包含适应流式设置的评估模块，并对不同超参数设置进行了对比。

Conclusion: RiverText为信息检索和自然语言处理社区提供了处理流式数据的实用工具。

Abstract: Word embeddings have become essential components in various information
retrieval and natural language processing tasks, such as ranking, document
classification, and question answering. However, despite their widespread use,
traditional word embedding models present a limitation in their static nature,
which hampers their ability to adapt to the constantly evolving language
patterns that emerge in sources such as social media and the web (e.g., new
hashtags or brand names). To overcome this problem, incremental word embedding
algorithms are introduced, capable of dynamically updating word representations
in response to new language patterns and processing continuous data streams.
  This paper presents RiverText, a Python library for training and evaluating
incremental word embeddings from text data streams. Our tool is a resource for
the information retrieval and natural language processing communities that work
with word embeddings in streaming scenarios, such as analyzing social media.
The library implements different incremental word embedding techniques, such as
Skip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized
framework. In addition, it uses PyTorch as its backend for neural network
training. We have implemented a module that adapts existing intrinsic static
word embedding evaluation tasks for word similarity and word categorization to
a streaming setting. Finally, we compare the implemented methods with different
hyperparameter settings and discuss the results. Our open-source library is
available at https://github.com/dccuchile/rivertext.

</details>


### [44] [Generalist Reward Models: Found Inside Large Language Models](https://arxiv.org/abs/2506.23235)
*Yi-Chen Li,Tian Xu,Yang Yu,Xuqin Zhang,Xiong-Hui Chen,Zhongxiang Ling,Ningjing Chao,Lei Yuan,Zhi-Hua Zhou*

Key words: 大语言模型, 奖励模型, 强化学习, 逆强化学习, 对齐

TL;DR: 提出了一种无需额外训练即可从预训练LLM中提取高质量奖励信号的方法，理论证明其优于传统奖励模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统LLM对齐依赖昂贵的人类偏好数据训练奖励模型，本文旨在探索一种更高效的理论支持方法。

Method: 研究发现LLM中隐含通用奖励模型，理论证明其等同于离线逆强化学习所得奖励，并可直接提取使用。

Result: 实验表明该方法优于现有LLM-as-a-judge方法，甚至超过显式训练的奖励模型。

Conclusion: 奖励建模阶段可通过预训练知识提取替代，为LLM对齐提供更高效、强大的方法。

Abstract: The alignment of Large Language Models (LLMs) is critically dependent on
reward models trained on costly human preference data. While recent work
explores bypassing this cost with AI feedback, these methods often lack a
rigorous theoretical foundation. In this paper, we discover that a powerful
generalist reward model is already latently present within any LLM trained via
standard next-token prediction. We prove that this endogenous reward is not a
heuristic, but is theoretically equivalent to a reward function learned through
offline inverse reinforcement learning. This connection allows us to directly
elicit a high-quality reward signal from a base (pre-trained or supervised
fine-tuned) model without any further training. Critically, we also prove that
subsequent reinforcement learning using this endogenous reward leads to a
policy with a provably superior error bound compared to the base model. To our
best knowledge, this is the first theoretical proof of the effectiveness of
reinforcement learning for LLMs. Our experiments validate this theory,
demonstrating that our method not only outperforms existing LLM-as-a-judge
approaches but can also surpass explicitly trained reward models. These
findings suggest that the reward modeling stage can be replaced by a principled
method of eliciting the knowledge already captured during pre-training,
heralding a more efficient, powerful, and scalable paradigm for LLMs alignment
as well as multi-modal models.

</details>


### [45] [Two Spelling Normalization Approaches Based on Large Language Models](https://arxiv.org/abs/2506.23288)
*Miguel Domingo,Francisco Casacuberta*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The absence of standardized spelling conventions and the organic evolution of
human language present an inherent linguistic challenge within historical
documents, a longstanding concern for scholars in the humanities. Addressing
this issue, spelling normalization endeavors to align a document's orthography
with contemporary standards. In this study, we propose two new approaches based
on large language models: one of which has been trained without a supervised
training, and a second one which has been trained for machine translation. Our
evaluation spans multiple datasets encompassing diverse languages and
historical periods, leading us to the conclusion that while both of them
yielded encouraging results, statistical machine translation still seems to be
the most suitable technology for this task.

</details>


### [46] [Objective-Free Local Learning and Emergent Language Structure in Thinking Machines](https://arxiv.org/abs/2506.23293)
*P. Myles Eugenio*

Key words: 神经符号框架、Hopfield记忆链、动态标记器、涌现学习、多尺度表示

TL;DR: 论文提出了一种基于局部事件驱动涌现学习的神经符号框架，用于生成语言建模。其核心是一个分层的Hopfield记忆链，作为组合短期记忆和动态标记器。该模型通过多尺度表示学习符号序列，并能从噪声中过滤自然语言模式。


<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统的语言模型依赖预定义标记或监督，缺乏灵活性和可解释性。本文旨在通过局部神经学习实现符号结构的涌现，为可扩展、可解释的神经符号系统提供新途径。


Method: 使用分层Hopfield记忆链作为动态标记器，通过投影张量绑定共现特征，实现多尺度符号表示。模型通过局部（Hebbian）学习保留新信息，并支持通过激活新神经元形成符号嵌入。


Result: 模型能够从噪声中生成具有内部形态一致性的合成语言，其量化特征与人类语言相似。通过涌现的嵌入神经元，模型实现了长期记忆和组合推理。


Conclusion: 该框架为研究符号结构如何从局部神经学习中涌现提供了方法基础，推动了生成语言模型的神经形态架构发展。


Abstract: We present a neuro-symbolic framework for generative language modeling based
on local, event-driven emergent learning. At its core is a hierarchical
Hopfield memory chain acting as a compositional short-term memory and dynamic
tokenizer (retokenizer). Rather than relying on predefined tokens or
supervision, the model builds structure from scratch, learning symbol sequences
as multi-scale representations. It constructs projection tensors that bind
co-occurring features into hierarchical tokens, introducing redundancy (i.e an
emergent gauge structure) and enabling compression of local activations into
long-range dependencies. Curiously, we find that the retokenizer can filter
natural language patterns from noise, generating synthetic languages with
coherent internal morphology -- quantifiably the same as human language.
Language is learned in a local (Hebbian) fashion, where model constraints
dictate allowed emergent structure, and new information is retained in
alignment with this structure. The absence of a global objective enables a form
of plasticity not found in conventional language models, allowing the system to
generalize beyond its initial inference class -- even without explicit data. We
demonstrate that briefly activating a new neuron during inference binds
distributed multi-scale token features into a symbolic embedding. These
emergent embedding neurons act as long-term memory and support a key-value
mechanism for compositional inference and generalization. This architecture
provides a methodological foundation for studying how symbolic structure can
emerge from local neural learning. It offers a new pathway for building
scalable, interpretable neuro-symbolic systems -- where tokens, grammar, and
reasoning arise as compressed memory traces within a Hopfield hierarchy. This
approach advances the development of neuromorphic architectures for generative
language models.

</details>


### [47] [Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)](https://arxiv.org/abs/2506.23315)
*Shouvon Sarker,Xishuang Dong,Lijun Qian*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Identification of key variables such as medications, diseases, relations from
health records and clinical notes has a wide range of applications in the
clinical domain. n2c2 2022 provided shared tasks on challenges in natural
language processing for clinical data analytics on electronic health records
(EHR), where it built a comprehensive annotated clinical data Contextualized
Medication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of
this challenge that is to detect and classify medication events from clinical
notes through building a novel BERT-based ensemble model. It started with
pretraining BERT models on different types of big data such as Wikipedia and
MIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED
training data. These fine-tuned BERT models were employed to accomplish
medication event classification on CMED testing data with multiple predictions.
These multiple predictions generated by these fine-tuned BERT models were
integrated to build final prediction with voting strategies. Experimental
results demonstrated that BERT-based ensemble models can effectively improve
strict Micro-F score by about 5% and strict Macro-F score by about 6%,
respectively.

</details>


### [48] [Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family](https://arxiv.org/abs/2506.23340)
*Yumeng Lin,Xufeng Duan,David Haslett,Yige Chen,Zhenguang G. Cai*

Key words: 多语言翻译、大型语言模型、信息损失、语言距离、训练数据

TL;DR: 本文研究了大型语言模型在多语言翻译中的信息损失问题，分析了训练数据、语言邻近性和语言家族的影响，发现数据量和语言结构关系共同决定翻译质量。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨多语言翻译中信息损失的根源，特别是针对低资源语言或与英语差异较大的语言对。

Method: 使用GPT-4和Llama 2进行往返翻译，并通过BLEU分数和BERT相似性指标评估翻译质量。

Result: 训练数据量可以缓解语言差异的影响，但语言结构接近英语的语言在低资源条件下表现更好；正交、系统发育、句法和地理距离是翻译性能的强预测因子。

Conclusion: 翻译质量不仅取决于数据量，还与语言间的结构和类型关系密切相关。

Abstract: Large language models have achieved impressive progress in multilingual
translation, yet they continue to face challenges with certain language
pairs-particularly those with limited training data or significant linguistic
divergence from English. This study systematically investigates how training
data, language proximity, and language family affect information loss in
multilingual translation. We evaluate two large language models, GPT-4 and
Llama 2, by performing round-trip translations. Translation quality was
assessed using BLEU scores and BERT similarity metrics. Our results reveal a
robust interaction between training data size and language distance: while
abundant training data can mitigate the effects of linguistic divergence,
languages structurally closer to English consistently yield higher translation
quality in low-resource conditions. Among various distance metrics,
orthographic, phylogenetic, syntactic, and geographical distances emerge as
strong predictors of translation performance. Language family also exerts an
independent influence. These findings contribute to a deeper understanding of
the linguistic constraints shaping multilingual translation in large language
models, emphasizing that translation quality is shaped not only by data volume
but also by structural and typological relationships between languages.

</details>


### [49] [ATGen: A Framework for Active Text Generation](https://arxiv.org/abs/2506.23342)
*Akim Tsvigun,Daniil Vasilev,Ivan Tsvigun,Ivan Lysenko,Talgat Bektleuov,Aleksandr Medvedev,Uliana Vinogradova,Nikita Severin,Mikhail Mozikov,Andrey Savchenko,Rostislav Grigorev,Ramil Kuleev,Fedor Zhdanov,Artem Shelmanov,Ilya Makarov*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Active learning (AL) has demonstrated remarkable potential in reducing the
annotation effort required for training machine learning models. However,
despite the surging popularity of natural language generation (NLG) tasks in
recent years, the application of AL to NLG has been limited. In this paper, we
introduce Active Text Generation (ATGen) - a comprehensive framework that
bridges AL with text generation tasks, enabling the application of
state-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered
annotation in NLG tasks using both human annotators and automatic annotation
agents based on large language models (LLMs). The framework supports LLMs
deployed as services, such as ChatGPT and Claude, or operated on-premises.
Furthermore, ATGen provides a unified platform for smooth implementation and
benchmarking of novel AL strategies tailored to NLG tasks. Finally, we present
evaluation results for state-of-the-art AL strategies across diverse settings
and multiple text generation tasks. We show that ATGen reduces both the effort
of human annotators and costs associated with API calls to LLM-based annotation
agents. The code of the framework is available on GitHub under the MIT license.
The video presentation is available at http://atgen-video.nlpresearch.group

</details>


### [50] [Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs](https://arxiv.org/abs/2506.23377)
*Taejin Kim,Siun-Chuon Mau,Konrad Vesey*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) are used in a variety of mission-critical roles.
Due to the rapidly developing nature of LLMs, there is a lack of quantifiable
understanding of the bias and perspective associated with LLM output. Inspired
by this need, this paper considers the broader issue of perspective or
viewpoint of general text and perspective control of large-language model (LLM)
output. Perspective-Dial consists of two main components: a (1) metric space,
dubbed Perspective Space, that enables quantitative measurements of different
perspectives regarding a topic, and the use of (2) Systematic Prompt
Engineering that utilizes greedy-coordinate descent to control LLM output
perspective based on measurement feedback from the Perspective Space. The
empirical nature of the approach allows progress to side step a principled
understanding of perspective or bias -- effectively quantifying and adjusting
outputs for a variety of topics. Potential applications include detection,
tracking and mitigation of LLM bias, narrative detection, sense making and
tracking in public discourse, and debate bot advocating given perspective.

</details>


### [51] [Hierarchical Memory Organization for Wikipedia Generation](https://arxiv.org/abs/2506.23393)
*Eugene J. Yu,Dawei Zhu,Yifan Song,Xiangyu Wong,Jiebin Zhang,Wenxuan Shi,Xiaoguang Li,Qun Liu,Sujian Li*

Key words: 维基百科生成；记忆组织；分层架构；信息可靠性

TL;DR: 论文提出了一种名为MOG的框架，通过分层记忆架构自动生成维基百科文章，提高了信息的准确性和可验证性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 生成维基百科文章需要整合多样化的来源信息，传统方法难以保证信息的准确性和全面性。

Method: MOG框架通过分层记忆结构提取和组织细粒度记忆单元，并以此指导生成过程，同时实现引用跟踪。

Result: 在WikiStart数据集上的实验表明，MOG在生成信息丰富且可靠的维基百科文章方面优于基线方法。

Conclusion: MOG框架通过分层记忆和引用模块，显著提升了生成文章的质量和可验证性。

Abstract: Generating Wikipedia articles autonomously is a challenging task requiring
the integration of accurate, comprehensive, and well-structured information
from diverse sources. This paper introduces the Memory Organization-based
Generation (MOG) framework, a novel approach to address these challenges by
leveraging a hierarchical memory architecture. MOG extracts fine-grained memory
units from web documents, recursively organizes them into a Wikipedia-style
hierarchical structure, and uses this structure to guide the generation
process. This ensures alignment between memory and the article outline,
improving both informativeness and verifiability while minimizing
hallucinations. Additionally, a citation module is implemented to enhance
traceability by linking every generated sentence to specific memory units.
Evaluations on our newly created WikiStart dataset demonstrate that MOG
outperforms baseline methods in producing informative and reliable articles,
making it particularly robust in real-world scenarios.

</details>


### [52] [Datasets for Fairness in Language Models: An In-Depth Survey](https://arxiv.org/abs/2506.23411)
*Jiale Zhang,Zichong Wang,Avash Palikhe,Zhipeng Yin,Wenbin Zhang*

Key words: 公平性、语言模型、基准数据集、偏见、评估框架

TL;DR: 该论文综述了广泛使用的公平性基准数据集，分析了其局限性并提出统一评估框架以揭示偏见，为研究人员提供选择和解释数据的实用指导。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 目前关于语言模型公平性的研究对基准数据集本身关注不足，本研究旨在填补这一空白。

Method: 通过对24个常用基准数据集的多维度分析，提出统一评估框架，用于比较和分析偏见。

Result: 研究发现数据集存在一致的偏见模式，影响模型公平性结论，并提出了改进建议。

Conclusion: 需开发更多样化的公平性基准数据集，并更谨慎地使用现有工具。

Abstract: Fairness benchmarks play a central role in shaping how we evaluate language
models, yet surprisingly little attention has been given to examining the
datasets that these benchmarks rely on. This survey addresses that gap by
presenting a broad and careful review of the most widely used fairness datasets
in current language model research, characterizing them along several key
dimensions including their origin, scope, content, and intended use to help
researchers better appreciate the assumptions and limitations embedded in these
resources. To support more meaningful comparisons and analyses, we introduce a
unified evaluation framework that reveals consistent patterns of demographic
disparities across datasets and scoring methods. Applying this framework to
twenty four common benchmarks, we highlight the often overlooked biases that
can influence conclusions about model fairness and offer practical guidance for
selecting, combining, and interpreting these datasets. We also point to
opportunities for creating new fairness benchmarks that reflect more diverse
social contexts and encourage more thoughtful use of these tools going forward.
All code, data, and detailed results are publicly available at
https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets
to promote transparency and reproducibility across the research community.

</details>


### [53] [TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs](https://arxiv.org/abs/2506.23423)
*Felipe Nuti,Tim Franzmeyer,João Henriques*

Key words: 大型语言模型, 微调, 对抗攻击, TuCo

TL;DR: 提出了一种新方法（TuCo）量化微调对大型语言模型输出和安全的贡献。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 缺乏系统分析微调对单个模型输出影响的方法。

Method: 跟踪模型的中间隐藏状态，分解微调对输出的贡献。

Result: 发现TuCo与对抗攻击的成功相关，微调减弱时攻击更易成功。

Conclusion: TuCo为研究微调对模型行为和安全的影响提供了量化工具。

Abstract: Past work has studied the effects of fine-tuning on large language models'
(LLMs) overall performance on certain tasks. However, a quantitative and
systematic method for analyzing its effect on individual outputs is still
lacking. Here, we propose a new method for measuring the contribution that
fine-tuning makes to individual LLM responses, assuming access to the original
pre-trained model. Our method tracks the model's intermediate hidden states,
providing a more fine-grained insight into the effects of fine-tuning than a
simple comparison of final outputs from pre-trained and fine-tuned models. We
introduce and theoretically analyze an exact decomposition of any fine-tuned
LLM into a pre-training component and a fine-tuning component. Empirically, we
find that model behavior and performance can be steered by up- or down-scaling
the fine-tuning component during the forward pass. Motivated by this finding
and our theoretical analysis, we define the Tuning Contribution (TuCo) as the
ratio of the magnitudes of the fine-tuning component to the pre-training
component. We observe that three prominent adversarial attacks on LLMs
circumvent safety measures in a way that reduces TuCo, and that TuCo is
consistently lower on prompts where these attacks succeed compared to those
where they do not. This suggests that attenuating the effect of fine-tuning on
model outputs plays a role in the success of such attacks. In summary, TuCo
enables the quantitative study of how fine-tuning influences model behavior and
safety, and vice versa.

</details>


### [54] [Pipelined Decoder for Efficient Context-Aware Text Generation](https://arxiv.org/abs/2506.23431)
*Zixian Huang,Chenxu Niu,Yu Gu,Gengyang Xiao,Xinwei Huang,Gong Cheng*

Key words: 自回归模型, 并行生成, 解码器架构, 文本生成, 上下文感知

TL;DR: 本文提出了一种新的解码器架构，通过并行生成多个子序列来加速文本生成，同时保持了生成质量和内存消耗的低损失。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 自回归模型虽能高质量生成文本，但因逐词生成而速度受限，制约了生成效率。本文旨在解决这一问题。

Method: 提出了一种管道化解码器，能够同时生成多个子序列，每个时间步为每个子序列生成一个新词以实现并行化。

Result: 在问答、文本摘要和关键词生成等任务中，该解码器显著提升了生成速度，且未显著影响质量或增加内存消耗。

Conclusion: 提出的管道化解码器有效解决了自回归模型的瓶颈，提升了生成速度，适用于上下文感知生成任务。

Abstract: As the basis of generative AI, an autoregressive model requires the
generation of a new token depending on all the previously generated tokens,
which brings high quality but also restricts the model to generate tokens one
by one, forming a bottleneck limiting the generation speed. In this paper, we
propose a new decoder architecture that efficiently generates text in parallel
for context-aware generation tasks. Our proposed pipelined decoder initiates
the generation of multiple subsequences simultaneously, and, at each time-step,
it generates a new token for each subsequence to realize parallelism.
Experiments on multiple text generation tasks, including question answering,
text summarization, and keyphrase generation, show that our pipelined decoder
significantly improves the generation speed without a significant loss of
generation quality or additional memory consumption.

</details>


### [55] [What to Keep and What to Drop: Adaptive Table Filtering Framework](https://arxiv.org/abs/2506.23463)
*Jang Won June*

Key words: 大型语言模型、表格推理、自适应过滤、TableQA

TL;DR: ATF（自适应表格过滤框架）通过模块化、问题感知的过滤流程，减少大型表格中的冗余列和行，提升表格推理任务的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型在处理大型表格时因输入长度限制而表现不佳，ATF旨在解决这一问题。

Method: 采用LLM生成的列描述、聚类和稀疏-密集对齐分数，进行问题感知的表格过滤。

Result: 实验表明，ATF减少约70%的表格单元，显著提升跨领域TableQA任务性能，但对需要全表上下文的Table Fact Verification任务略有影响。

Conclusion: ATF能自适应地在任务中平衡信息量和简约性。

Abstract: Large language models (LLMs) for table-based reasoning often struggle with
large tables due to input length limits. We propose ATF (Adaptive Table
Filtering Framework), a modular and question-aware filtering pipeline that
prunes uninformative columns and rows using LLM-generated column descriptions,
clustering, and sparse-dense alignment scores. ATF integrates seamlessly with
existing models (e.g., TAPAS, TAPEX) without retraining. Experiments show that
ATF reduces table cells by ~70\%, boosting performance on out-of-domain TableQA
tasks while causing slight performance drops on Table Fact Verification, where
full-table context is more critical. These results highlight ATF's ability to
adaptively balance informativeness and minimalism across tasks.

</details>


### [56] [Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent](https://arxiv.org/abs/2506.23485)
*Haocheng Yu,Yaxiong Wu,Hao Wang,Wei Guo,Yong Liu,Yawen Li,Yuyang Ye,Junping Du,Enhong Chen*

Key words: 交互推荐, 大语言模型, 多代理系统, 思想模式蒸馏, 用户意图

TL;DR: 提出了一种基于大语言模型的多代理系统TAIRA，通过思想模式蒸馏技术增强代理对复杂用户意图的处理能力，显著提升了交互推荐系统的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基于大语言模型的交互推荐代理难以有效处理多样且复杂的用户意图，如模糊或不明确的需求。

Method: 设计了TAIRA系统，包括一个管理代理和思想模式蒸馏技术，通过分解用户需求并规划子任务来提高处理能力。

Result: TAIRA在多个数据集上表现出显著优于现有方法的性能，尤其在复杂任务中表现更优。

Conclusion: TAIRA成功解决了复杂用户意图的处理问题，证明了其在交互推荐系统中的优越性。

Abstract: Interactive recommendation is a typical information-seeking task that allows
users to interactively express their needs through natural language and obtain
personalized recommendations. Large language model-powered (LLM-powered) agents
have become a new paradigm in interactive recommendations, effectively
capturing users' real-time needs and enhancing personalized experiences.
However, due to limited planning and generalization capabilities, existing
formulations of LLM-powered interactive recommender agents struggle to
effectively address diverse and complex user intents, such as intuitive,
unrefined, or occasionally ambiguous requests. To tackle this challenge, we
propose a novel thought-augmented interactive recommender agent system (TAIRA)
that addresses complex user intents through distilled thought patterns.
Specifically, TAIRA is designed as an LLM-powered multi-agent system featuring
a manager agent that orchestrates recommendation tasks by decomposing user
needs and planning subtasks, with its planning capacity strengthened through
Thought Pattern Distillation (TPD), a thought-augmentation method that extracts
high-level thoughts from the agent's and human experts' experiences. Moreover,
we designed a set of user simulation schemes to generate personalized queries
of different difficulties and evaluate the recommendations based on specific
datasets. Through comprehensive experiments conducted across multiple datasets,
TAIRA exhibits significantly enhanced performance compared to existing methods.
Notably, TAIRA shows a greater advantage on more challenging tasks while
generalizing effectively on novel tasks, further validating its superiority in
managing complex user intents within interactive recommendation systems. The
code is publicly available at:https://github.com/Alcein/TAIRA.

</details>


### [57] [Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably](https://arxiv.org/abs/2506.23508)
*Zhihao Zhang,Qiaole Dong,Qi Zhang,Jun Zhao,Enyu Zhou,Zhiheng Xi,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Tao Ji,Tao Gui,Xuanjing Huang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Post-training algorithms such as Supervised Fine-Tuning (SFT) and
Reinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large
language models to downstream tasks. While effective at task adaptation, their
impact on prior knowledge remains unclear. In this paper, we introduce jigsaw
puzzles as a novel task absent from existing pretraining corpora and
systematically study the behavior of SFT and RFT on an open-source multimodal
model, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid
task acquisition but leads to catastrophic forgetting, whereas RFT learns more
slowly on novel tasks but maintains prior knowledge. We analyze this phenomenon
through the lens of learning dynamics, showing that RFT reinforces correct
samples that are naturally aligned with the base model's probability landscape,
mitigating interference with prior knowledge. Moreover, supervised training on
correct RFT-simulated rollouts allows SFT to preserve knowledge while rapidly
learning new tasks. These findings suggest that data distribution, rather than
algorithmic differences, plays a central role in forgetting, and highlight
RFT's potential for stable continual learning in multimodal large language
models.

</details>


### [58] [NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning](https://arxiv.org/abs/2506.23524)
*Phan Quoc Hung Mai,Quang Hung Nguyen,Phuong Giang Duong,Hong Hanh Nguyen,Nguyen Tuan Long*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the field of education, understanding students' opinions through their
comments is crucial, especially in the Vietnamese language, where resources
remain limited. Existing educational datasets often lack domain relevance and
student slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese
dataset for Educational Sentiment Classification and Topic Classification,
curated from university forums, which offers more samples, richer class
diversity, longer texts, and broader vocabulary. In addition, we explore
multitask learning using encoder-only language models (BERT), in which we
showed that it achieves performance up to 83.7% and 79.8% accuracy for
sentiment and topic classification tasks. We also benchmark our dataset and
model with other datasets and models, including Large Language Models, and
discuss these benchmarks. The dataset is publicly available at:
https://huggingface.co/datasets/hung20gg/NEU-ESC.

</details>


### [59] [On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?](https://arxiv.org/abs/2506.23527)
*Jan Kvapil,Martin Fajcik*

Key words: 大型语言模型（LLMs）, 记忆化, 创造性, 无意义, 烹饪食谱, 人工标注, 自动化分析

TL;DR: 该研究调查了大型语言模型（LLMs）生成的烹饪食谱中的记忆化、创造性和无意义内容，并通过人工标注和自动化方法进行分析。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是量化LLMs在生成食谱时的记忆化、创造性和无意义表现，并提供自动化方法以扩展研究规模。

Method: 方法包括对20个LLM（Mixtral）生成的食谱进行详细人工标注，评估其成分和步骤来源（记忆化、创造性或无意义）。同时设计自动化流程（LLM-as-judge），用于扩展现有分析。

Result: 结果表明Mixtral倾向于重复使用训练数据中的成分，显示出较强的记忆化依赖。自动化流程中，Llama 3.1+Gemma 2 9B在成分匹配上达到78%准确率。

Conclusion: 结论是自动化框架能大规模量化LLMs的记忆化、创造性和无意义表现，为模型的创造力提供严谨证据。

Abstract: This work-in-progress investigates the memorization, creativity, and nonsense
found in cooking recipes generated from Large Language Models (LLMs).
Precisely, we aim (i) to analyze memorization, creativity, and non-sense in
LLMs using a small, high-quality set of human judgments and (ii) to evaluate
potential approaches to automate such a human annotation in order to scale our
study to hundreds of recipes. To achieve (i), we conduct a detailed human
annotation on 20 preselected recipes generated by LLM (Mixtral), extracting
each recipe's ingredients and step-by-step actions to assess which elements are
memorized--i.e., directly traceable to online sources possibly seen during
training--and which arise from genuine creative synthesis or outright nonsense.
We find that Mixtral consistently reuses ingredients that can be found in
online documents, potentially seen during model training, suggesting strong
reliance on memorized content. To achieve aim (ii) and scale our analysis
beyond small sample sizes and single LLM validation, we design an
``LLM-as-judge'' pipeline that automates recipe generation, nonsense detection,
parsing ingredients and recipe steps, and their annotation. For instance,
comparing its output against human annotations, the best ingredient extractor
and annotator is Llama 3.1+Gemma 2 9B, achieving up to 78% accuracy on
ingredient matching. This automated framework enables large-scale
quantification of memorization, creativity, and nonsense in generated recipes,
providing rigorous evidence of the models' creative capacities.

</details>


### [60] [Semantic-guided Diverse Decoding for Large Language Model](https://arxiv.org/abs/2506.23601)
*Weijie Shi,Yue Cui,Yaguang Wu,Jingzhi Fang,Shibo Zhang,Mengze Li,Sirui Han,Jia Zhu,Jiajie Xu,Xiaofang Zhou*

Key words: 语义多样性解码、SemDiD、Best-of-N策略、嵌入空间、RLHF

TL;DR: 论文提出了SemDiD方法，直接在嵌入空间中实现语义多样性解码，通过三种机制平衡质量与多样性，实验结果优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法主要实现词汇多样性而非语义多样性，限制了Best-of-N策略和强化学习的应用，因此需要一种直接在嵌入空间中实现语义多样性的解码方法。

Method: SemDiD方法结合正交方向引导、动态组间排斥和位置去偏概率评估三种机制，通过自适应增益函数和约束优化实现质量与多样性的平衡。

Result: 实验显示SemDiD在多样任务中优于现有方法，Best-of-N覆盖率提升1.4-5.2%，RLHF训练收敛速度加快15%，准确率最高提升2.1%。

Conclusion: SemDiD有效解决了语义多样性解码的问题，为语言模型的多响应生成提供了更优方案。

Abstract: Diverse decoding of large language models is crucial for applications
requiring multiple semantically distinct responses, yet existing methods
primarily achieve lexical rather than semantic diversity. This limitation
significantly constrains Best-of-N strategies, group-based reinforcement
learning, and data synthesis. While temperature sampling and diverse beam
search modify token distributions or apply n-gram penalties, they fail to
ensure meaningful semantic differentiation. We introduce Semantic-guided
Diverse Decoding (SemDiD), operating directly in embedding space that balances
quality with diversity through three complementary mechanisms: orthogonal
directional guidance, dynamic inter-group repulsion, and position-debiased
probability assessment. SemDiD harmonizes these competing objectives using
adaptive gain functions and constraint optimization, ensuring both quality
thresholds and maximal semantic differentiation. Experiments show SemDiD
consistently outperforms existing methods, improving Best-of-N coverage by
1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15%
while increasing accuracy by up to 2.1%.

</details>


### [61] [Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs](https://arxiv.org/abs/2506.23610)
*Manuel Pratelli,Marinella Petrocchi*

Key words: 大型语言模型,人格特质,信息辨识,行为模拟,认知多样性

TL;DR: 研究探讨大型语言模型（LLM）生成的人格化代理是否能准确复现人类心理差异对信息辨识的影响，发现部分人格特质关联被准确复现，但存在系统性偏差。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 评估LLM生成的人格化行为数据能否忠实反映人格特质驱动的心理差异，为行为模拟提供伦理且低成本的替代方案。

Method: 基于Big-Five人格档案训练LLM代理，对照已发表的人类数据集，比较其对新闻标题真伪判断的行为模式。

Result: 部分人格特质（如宜人性、责任感）与信息误判的关联被成功复现，但其他特质存在偏差，揭示了LLM在表达人格时的系统性局限。

Conclusion: 人格对齐的LLM在行为模拟中具有潜力但存在局限，为人工代理的认知多样性建模提供了新见解。

Abstract: Large language models (LLMs) make it possible to generate synthetic
behavioural data at scale, offering an ethical and low-cost alternative to
human experiments. Whether such data can faithfully capture psychological
differences driven by personality traits, however, remains an open question. We
evaluate the capacity of LLM agents, conditioned on Big-Five profiles, to
reproduce personality-based variation in susceptibility to misinformation,
focusing on news discernment, the ability to judge true headlines as true and
false headlines as false. Leveraging published datasets in which human
participants with known personality profiles rated headline accuracy, we create
matching LLM agents and compare their responses to the original human patterns.
Certain trait-misinformation associations, notably those involving
Agreeableness and Conscientiousness, are reliably replicated, whereas others
diverge, revealing systematic biases in how LLMs internalize and express
personality. The results underscore both the promise and the limits of
personality-aligned LLMs for behavioral simulation, and offer new insight into
modeling cognitive diversity in artificial agents.

</details>


### [62] [Robustness of Misinformation Classification Systems to Adversarial Examples Through BeamAttack](https://arxiv.org/abs/2506.23661)
*Arnisa Fazla,Lucas Krauter,David Guzman Piedrahita,Andrianos Michail*

Key words: BeamAttack, 对抗攻击, 文本分类, LIME, BODEGA框架

TL;DR: 论文扩展了BeamAttack算法，支持单词删除和跳过替换，结合LIME优化替换策略，实现高成功率攻击并保持原文本语义。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 评估文本分类系统的鲁棒性，通过最小修改改变模型预测。

Method: 扩展BeamAttack算法，支持单词删除和跳过替换，整合LIME优先替换策略。

Result: 在BODEGA框架下，对多数据集和模型实现99%攻击成功率，保持语义和词汇相似性。

Conclusion: 展示BeamAttack的高效性及局限性。

Abstract: We extend BeamAttack, an adversarial attack algorithm designed to evaluate
the robustness of text classification systems through word-level modifications
guided by beam search. Our extensions include support for word deletions and
the option to skip substitutions, enabling the discovery of minimal
modifications that alter model predictions. We also integrate LIME to better
prioritize word replacements. Evaluated across multiple datasets and victim
models (BiLSTM, BERT, and adversarially trained RoBERTa) within the BODEGA
framework, our approach achieves over a 99\% attack success rate while
preserving the semantic and lexical similarity of the original texts. Through
both quantitative and qualitative analysis, we highlight BeamAttack's
effectiveness and its limitations. Our implementation is available at
https://github.com/LucK1Y/BeamAttack

</details>


### [63] [Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation](https://arxiv.org/abs/2506.23662)
*Philip Lippmann,Jie Yang*

Key words: ZEST, 零样本, 上下文感知嵌入, 合成代理语料库, MTEB

TL;DR: ZEST是一种零样本上下文适应框架，通过合成代理语料库替代真实语料库访问，在隐私敏感或资源受限环境中高效部署上下文感知嵌入方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有上下文感知嵌入方法需要访问目标语料库或进行领域特定微调，这在隐私敏感或资源受限环境中不实用。

Method: ZEST通过多步分层过程生成合成代理语料库，以模拟关键领域特定分布，无需微调或访问目标语料库。

Result: 在MTEB基准测试中，仅使用五个示例文档的ZEST表现接近全目标语料库访问模型（差距0.5%），且无需重新训练。

Conclusion: ZEST为受限环境中的高性能嵌入部署提供了实用方法。

Abstract: Context-aware embedding methods boost retrieval accuracy by conditioning on
corpus statistics (e.g., term co-occurrence and topical patterns) extracted
from neighboring documents. However, this context-aware approach requires
access to the target corpus or requires domain-specific finetuning, posing
practical barriers in privacy-sensitive or resource-constrained settings. We
present ZEST, a zero-shot contextual adaptation framework that replaces real
corpus access with a one-time offline synthesis of a compact proxy. Given only
a handful exemplar documents representative of the general target domain, we
use a multi-step hierarchical procedure to generate a synthetic context corpus
of several hundred documents that aims to emulate key domain-specific
distributions. At inference, the frozen context-aware encoder uses this proxy
corpus -- without any finetuning or target corpus access -- to produce
domain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot
synthetic context adaptation using only five example documents performs within
0.5% of models leveraging full target corpus access -- demonstrating remarkable
efficacy without any retraining. ZEST thus provides a practical method for
deploying high-performance, adaptable embeddings in constrained environments.

</details>


### [64] [L0: Reinforcement Learning to Become General Agents](https://arxiv.org/abs/2506.23667)
*Junjie Zhang,Jingyi Xi,Zhuoyang Song,Junyu Lu,Yuhua Ke,Ting Sun,Yukun Yang,Jiaxing Zhang,Songxin Zhang,Zejian Xie*

Key words: 大型语言模型, 自主代理, 强化学习, 可扩展训练, NB-Agent

TL;DR: 论文提出了一种名为L-Zero（L0）的可扩展、端到端训练管道，用于训练大型语言模型成为多轮、长周期任务的自主代理，通过低成本的代理工作池和代码驱动的NB-Agent提升了训练效率。实验表明，该方法在问答基准测试中显著提升了模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 训练大型语言模型作为自主代理执行多轮、长周期任务存在规模化和训练效率的挑战，需要一种高效、可扩展的解决方案。

Method: 提出了L0训练管道和NB-Agent代理框架，采用代码作为动作的REPL模式，并通过可验证奖励的强化学习进行训练。

Result: 在Qwen2.5-7B-Instruct模型上，SimpleQA准确率从30%提升至80%，HotpotQA从22%提升至41%。

Conclusion: L0系统有效提升了大型语言模型的任务执行能力，并已开源实现。

Abstract: Training large language models (LLMs) to act as autonomous agents for
multi-turn, long-horizon tasks remains significant challenges in scalability
and training efficiency. To address this, we introduce L-Zero (L0), a scalable,
end-to-end training pipeline for general-purpose agents. Featuring a low-cost,
extensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier
for applying reinforcement learning in complex environments. We also introduce
NB-Agent, the agent scaffold within L0, which operates in a "code-as-action"
fashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality
question-answering benchmarks. Our experiments demonstrate that a base model
can develop robust problem-solving skills using solely Reinforcement Learning
with Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method
boosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41
%. We have open-sourced the entire L0 system, including our L0 series models,
the NB-Agent, a complete training pipeline, and the corresponding training
recipes on (https://github.com/cmriat/l0).

</details>


### [65] [AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data](https://arxiv.org/abs/2506.23735)
*JiaRu Wu,Mingwei Liu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) have shown remarkable performance on various
tasks, but existing evaluation benchmarks are often static and insufficient to
fully assess their robustness and generalization in realistic scenarios. Prior
work using evolutionary or adversarial data augmentation has improved
evaluation diversity but lacks systematic control over perturbation types and
multi-step complexity, limiting comprehensive robustness analysis. To address
these gaps, we propose AutoEvoEval, an evolution-based evaluation framework for
close-ended tasks such as multi-choice question answering. AutoEvoEval
introduces 22 interpretable atomic evolution operations and supports
multi-round compositions, enabling controlled generation of diverse,
challenging, and realistic test samples. We conduct extensive experiments
addressing four research questions on a broad set of open- and closed-source
LLMs. Our results show that atomic operations cause an average accuracy drop of
7.283\%, with structure-disrupting or misleading semantic edits causing the
largest declines. Model sensitivities vary significantly for the same
perturbation, and combining multiple evolution steps amplifies adversarial
effects by up to 52.932\%. These findings suggest current benchmarks may
overestimate true model generalization and emphasize the need for
evolution-aware robustness evaluation. Code and resources are available at:
https://github.com/SYSUSELab/AutoEvoEval.

</details>


### [66] [Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences](https://arxiv.org/abs/2506.23743)
*Tiziano Labruna,Simone Gallo,Giovanni Da San Martino*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Positional bias in binary question answering occurs when a model
systematically favors one choice over another based solely on the ordering of
presented options. In this study, we quantify and analyze positional bias
across five large language models under varying degrees of answer uncertainty.
We re-adapted the SQuAD-it dataset by adding an extra incorrect answer option
and then created multiple versions with progressively less context and more
out-of-context answers, yielding datasets that range from low to high
uncertainty. Additionally, we evaluate two naturally higher-uncertainty
benchmarks: (1) WebGPT - question pairs with unequal human-assigned quality
scores, and (2) Winning Arguments - where models predict the more persuasive
argument in Reddit's r/ChangeMyView exchanges. Across each dataset, the order
of the "correct" (or higher-quality/persuasive) option is systematically
flipped (first placed in position 1, then in position 2) to compute both
Preference Fairness and Position Consistency. We observe that positional bias
is nearly absent under low-uncertainty conditions, but grows exponentially when
it becomes doubtful to decide which option is correct.

</details>


### [67] [Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model](https://arxiv.org/abs/2506.23840)
*Bowen Ding,Yuhan Chen,Futing Wang,Lingfeng Ming,Tao Lin*

Key words: 大型推理模型、思考陷阱、DuP-PO、标记效率、数学推理

TL;DR: 论文提出了一种新算法DuP-PO，旨在解决大型推理模型在处理简单任务时过度思考的问题，提升其推理效率和性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型推理模型在处理简单任务时容易因为过多的思考标记（如wait、however）而产生冗余的高阶推理行为，降低了效率和正确性，这种现象被称为思考陷阱。

Method: 提出了双重策略偏好优化（DuP-PO）算法，包括：1）滚动采样策略，平衡有无思考标记的响应；2）细粒度优势控制技术，动态调控目标标记预测；3）策略整形方法，稳定思考标记的梯度贡献。

Result: 在五个流行的数学推理基准测试中，DuP-PO显著提升了模型的标记效率，同时保持了基础模型的性能优势。

Conclusion: DuP-PO有效缓解了思考陷阱问题，为大型推理模型的高效推理提供了新方法。

Abstract: Large Reasoning Models (LRMs) excel at solving complex problems but face an
overthinking dilemma. When handling simple tasks, they often produce verbose
responses overloaded with thinking tokens (e.g., wait, however). These tokens
trigger unnecessary high-level reasoning behaviors like reflection and
backtracking, reducing efficiency. In this work, our pilot study reveals that
these thinking-token-induced behaviors are not essential for effective
problem-solving and may even hinder correct reasoning within constrained token
budgets. We identify this phenomenon as the thinking trap. To mitigate this
issue, we propose Dual Policy Preference Optimization (DuP-PO), a novel
algorithm featuring: (1) A rollout sampling strategy that guarantees balanced
exposure to responses with and without thinking tokens; (2) A fine-grained
advantage control technique to dynamically regulate the prediction of target
tokens; (3) A policy shaping method ensuring stable gradient contributions from
thinking tokens. Experimental results on five popular math reasoning benchmarks
show that DuP-PO performs well on the popular LRM, which significantly improves
their token efficiency during reasoning, while achieving superior performance
of the base model.

</details>


### [68] [Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It](https://arxiv.org/abs/2506.23864)
*Seyed Mahed Mousavi,Edoardo Cecchinato,Lucia Hornikova,Giuseppe Riccardi*

Key words: 推理基准、评估方法、大型语言模型、基准审计、语义缺陷

TL;DR: 该论文对三个广泛使用的推理基准（SocialIQa、FauxPas-EAI 和 ToMi）进行了系统审计，揭示了基准设计和评估方法的普遍缺陷。研究发现，模型表现受输入形式影响大，高分数可能仅反映与格式匹配，而非实际推理能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前推理基准的设计和评估方法存在缺陷，可能误导对大型语言模型（LLM）推理能力的评价，因此需要对这些问题进行系统性揭露和改进。

Method: 采用五种LLM（GPT-3、GPT-3.5、GPT-4、GPT-o1 和 LLaMA 3.1）作为诊断工具，结合人工标注和重新评估，分析基准中的结构、语义和实用问题。

Result: 发现模型分数提升通常源于表面形式匹配而非推理能力改进，且性能对输入微小变化极为敏感。高分数可能仅反映格式对齐而非一致推理。

Conclusion: 当前基准的评估方法可能无效，需开发新协议以评估模型从信息中推理的过程，而非静态输出选择。

Abstract: We conduct a systematic audit of three widely used reasoning benchmarks,
SocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark
items and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and
LLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic
issues in benchmark design (e.g., duplicated items, ambiguous wording, and
implausible answers), as well as scoring procedures that prioritize output form
over reasoning process. Through systematic human annotation and re-evaluation
on cleaned benchmark subsets, we find that model scores often improve not due
to due to erratic surface wording variations and not to improved reasoning.
Infact, further analyses show that model performance is highly sensitive to
minor input variations such as context availability and phrasing, revealing
that high scores may reflect alignment with format-specific cues rather than
consistent inference based on the input. These findings challenge the validity
of current benchmark-based claims about reasoning in LLMs, and highlight the
need for evaluation protocols that assess reasoning as a process of drawing
inference from available information, rather than as static output selection.
We release audited data and evaluation tools to support more interpretable and
diagnostic assessments of model reasoning.

</details>


### [69] [Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting](https://arxiv.org/abs/2506.23888)
*André de Souza Loureiro,Jorge Valverde-Rebaza,Julieta Noguez,David Escarcega,Ricardo Marcacini*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
improved their problem-solving capabilities. However, these models still
struggle when faced with complex multi-step reasoning tasks. In this paper, we
propose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework,
a novel approach designed to enhance multi-step mathematical reasoning in LLMs
by integrating techniques such as Chain of Thought (CoT), Self-Reflection, and
Auto-Prompting. Unlike traditional static prompting methods, MAPS employs an
iterative refinement process. Initially, the model generates a solution using
CoT prompting. When errors are detected, an adaptive self-reflection mechanism
identifies and analyzes them, generating tailored prompts to guide corrections.
These dynamically adjusted prompts enable the model to iteratively refine its
reasoning. Experiments on four well-established benchmarks across multiple LLMs
show that MAPS significantly outperforms standard CoT and achieves competitive
results with reasoning-optimized models. In addition, MAPS enables
general-purpose LLMs to reach performance levels comparable to specialized
reasoning models. While deeper reflection layers improve accuracy, they also
increase token usage and costs. To balance this trade-off, MAPS strategically
limits reflection depth, ensuring an optimal balance between cost and reasoning
performance.

</details>


### [70] [The Trilemma of Truth in Large Language Models](https://arxiv.org/abs/2506.23921)
*Germans Savcisens,Tina Eliassi-Rad*

Key words: 大语言模型、知识真实性、多实例学习、一致预测、sAwMIL

TL;DR: 该论文探讨了大语言模型（LLMs）内部知识的真实性评估方法，提出了一种新的探测方法sAwMIL，并发现了关于LLMs真实性的五个重要见解。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究背景是对LLMs内部知识的真实性进行评估，现有方法存在有缺陷的假设，需要一种更可靠的方法来验证LLMs的“知识”。

Method: 提出了sAwMIL方法，基于多实例学习和一致预测，利用LLMs的内部激活来区分陈述的真实性。

Result: 研究发现了五个关键见解，包括信号集中区域、真假信号不对称性、线性探测效果差异、非线性探测需求以及第三种信号的存在。

Conclusion: sAwMIL提供了一种可靠的方法来验证LLMs的内部知识及其确定性。

Abstract: We often attribute human characteristics to large language models (LLMs) and
claim that they "know" certain things. LLMs have an internal probabilistic
knowledge that represents information retained during training. How can we
assess the veracity of this knowledge? We examine two common methods for
probing the veracity of LLMs and discover several assumptions that are flawed.
To address these flawed assumptions, we introduce sAwMIL (short for Sparse
Aware Multiple-Instance Learning), a probing method that utilizes the internal
activations of LLMs to separate statements into true, false, and neither.
sAwMIL is based on multiple-instance learning and conformal prediction. We
evaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including
both default and chat-based variants, as well as on 3 new datasets. Among the
insights we provide are: (1) the veracity signal is often concentrated in the
third quarter of an LLM's depth; (2) truth and falsehood signals are not always
symmetric; (3) linear probes perform better on chat models than on default
models; (4) nonlinear probes may be required to capture veracity signals for
some LLMs with reinforcement learning from human feedback or knowledge
distillation; and (5) LLMs capture a third type of signal that is distinct from
true and false and is neither true nor false. These findings provide a reliable
method for verifying what LLMs "know" and how certain they are of their
probabilistic internal knowledge.

</details>


### [71] [IMPACT: Inflectional Morphology Probes Across Complex Typologies](https://arxiv.org/abs/2506.23929)
*Mohammed J. Saeed,Tommi Vehvilainen,Evgeny Fedoseev,Sevil Caliskan,Tatiana Vodolazova*

Key words: 大型语言模型, 多语言评估, 形态学, IMPACT框架, 语言复杂性

TL;DR: 论文提出了一个名为IMPACT的评估框架，用于测试大型语言模型在多语言形态学任务中的表现，发现其在处理非英语语言的复杂形态特征上存在不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管大型语言模型在多语言任务中表现优异，但其是否真正理解语言的形态复杂性尚未明确，尤其是对于形态丰富的语言。

Method: 作者开发了IMPACT框架，通过合成数据评估模型在五种形态丰富语言（阿拉伯语、俄语、芬兰语、土耳其语和希伯来语）中的表现，涵盖基础和独特的形态特征。

Result: 研究发现，即使模型在英语上表现优秀，但在处理其他语言的形态复杂性（如不常见模式或非语法例子）时表现较差，且某些推理方法会进一步降低性能。

Conclusion: 大型语言模型在语言形态复杂性处理上仍有明显不足，需要进一步改进。IMPACT框架已公开以支持后续研究。

Abstract: Large Language Models (LLMs) have shown significant progress on various
multilingual benchmarks and are increasingly used to generate and evaluate text
in non-English languages. However, while they may produce fluent outputs, it
remains unclear to what extent these models truly grasp the underlying
linguistic complexity of those languages, particularly in morphology. To
investigate this, we introduce IMPACT, a synthetically generated evaluation
framework focused on inflectional morphology, which we publicly release,
designed to evaluate LLM performance across five morphologically rich
languages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes
unit-test-style cases covering both shared and language-specific phenomena,
from basic verb inflections (e.g., tense, number, gender) to unique features
like Arabic's reverse gender agreement and vowel harmony in Finnish and
Turkish. We assess eight multilingual LLMs that, despite strong English
performance, struggle with other languages and uncommon morphological patterns,
especially when judging ungrammatical examples. We also show that Chain of
Thought and Thinking Models can degrade performance. Our work exposes gaps in
LLMs' handling of linguistic complexity, pointing to clear room for
improvement. To support further research, we publicly release the IMPACT
framework.

</details>


### [72] [Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages](https://arxiv.org/abs/2506.23930)
*Ruhina Tabasshum Prome,Tarikul Islam Tamiti,Anomadarshi Barua*

Key words: 仇恨语音检测, 低资源语言, 大型语言模型, 提示工程, 隐喻提示

TL;DR: 论文探讨了在低资源语言（如孟加拉语）中通过提示工程优化大型语言模型以检测仇恨语音，提出了六种提示策略，尤其是创新的隐喻提示法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 社交媒体仇恨语音激增，但低资源语言因缺乏高质量数据集而面临挑战，需通过提示工程改进检测方法。

Method: 研究六种提示策略（包括零样本提示、多样本提示、角色提示和隐喻提示等），在Llama2-7B模型上测试，并与多种预训练词嵌入和深度学习模型对比。

Result: 隐喻提示法在低资源语言（孟加拉语和印地语）及高资源语言（英语和德语）中表现优异，验证其有效性。

Conclusion: 隐喻提示法为低资源语言仇恨语音检测提供了有效解决方案，且环境友好。

Abstract: The rapid expansion of social media leads to a marked increase in hate
speech, which threatens personal lives and results in numerous hate crimes.
Detecting hate speech presents several challenges: diverse dialects, frequent
code-mixing, and the prevalence of misspelled words in user-generated content
on social media platforms. Recent progress in hate speech detection is
typically concentrated on high-resource languages. However, low-resource
languages still face significant challenges due to the lack of large-scale,
high-quality datasets. This paper investigates how we can overcome this
limitation via prompt engineering on large language models (LLMs) focusing on
low-resource Bengali language. We investigate six prompting strategies -
zero-shot prompting, refusal suppression, flattering the classifier, multi-shot
prompting, role prompting, and finally our innovative metaphor prompting to
detect hate speech effectively in low-resource languages. We pioneer the
metaphor prompting to circumvent the built-in safety mechanisms of LLMs that
marks a significant departure from existing jailbreaking methods. We
investigate all six different prompting strategies on the Llama2-7B model and
compare the results extensively with three pre-trained word embeddings - GloVe,
Word2Vec, and FastText for three different deep learning models - multilayer
perceptron (MLP), convolutional neural network (CNN), and bidirectional gated
recurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in
the low-resource Bengali language, we also evaluate it in another low-resource
language - Hindi, and two high-resource languages - English and German. The
performance of all prompting techniques is evaluated using the F1 score, and
environmental impact factor (IF), which measures CO$_2$ emissions, electricity
usage, and computational time.

</details>


### [73] [Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs](https://arxiv.org/abs/2506.23940)
*Yang Dai,Jianxiang An,Tianwei Lin,Hongyang He,Hongzhe Huang,Wenqiao Zhang,Zheqi Lv,Siliang Tang,Yueting Zhuang*

Key words: 多模态大语言模型；知识共享；参数集成；兼容性感知；模块化

TL;DR: 提出了一种统一参数集成框架，通过兼容性感知参数拼接（CAPS）策略，实现领域专用多模态大语言模型的知识共享与模块化组合。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决领域专用多模态大语言模型（MLLMs）知识碎片化问题，研究知识共享机制。

Method: 采用CAPS策略，结合局部功能归因和全局信息论信号，选择性融合参数，并引入领域兼容性评分机制。

Result: 在多个多模态基准测试中验证了框架的有效性，实现了异构专业知识的协同。

Conclusion: 该框架为可组合、领域自适应的MLLMs提供了一条可扩展的路径。

Abstract: Multimodal Large Language Models (MLLMs) have achieved success across various
domains. However, their applicability tends to degrade when confronted with
different types of data inputs, especially for MLLMs that have been fine-tuned
for specific tasks. Despite its importance, the study of knowledge sharing
among domain-specific MLLMs--such as those trained for mathematics or
code--remains largely underexplored. To address the fragmentation of knowledge
across domain-specialized MLLMs, we propose a unified parameter integration
framework that enables modular composition of expert capabilities. Our method
is grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,
which leverages both local functional attribution and global
information-theoretic signals to guide selective parameter fusion. By extending
this mechanism to the low-rank adaptation layer granularity, we ensure
efficient integration with minimal inference overhead. Furthermore, we
introduce a domain compatibility scoring mechanism that quantifies inter-expert
alignment at the activation level and correlates with downstream task utility.
This principled fusion protocol allows the final model to synergize
heterogeneous expertise while preserving structural modularity. Extensive
evaluations across diverse multimodal benchmarks validate the effectiveness of
our framework, offering a scalable path toward compositional, domain-adaptive
MLLMs.

</details>


### [74] [Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders](https://arxiv.org/abs/2506.23951)
*Mathis Le Bail,Jérémie Dentan,Davide Buscaldi,Sonia Vanier*

Key words: 稀疏自编码器（SAE）, 句子分类, 大语言模型（LLM）, 可解释性, 特征提取

TL;DR: 该论文研究了稀疏自编码器（SAE）在句子分类中的有效性，提出了一种新的SAE架构，结合专用分类器和激活率稀疏损失，显著提升了特征的因果性和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管SAE已成功用于提取大语言模型（LLM）的可解释概念，但在句子分类领域尚未广泛研究。本文旨在填补这一空白，探索SAE在这一任务中的潜力。

Method: 提出一种针对文本分类的新型SAE架构，包含专用分类器头和激活率稀疏损失，并与ConceptShap、独立成分分析等方法进行对比。

Result: 实验表明，该架构在Pythia家族的四个LLM上表现优异，提高了提取特征的因果性和可解释性。

Conclusion: SAE在句子分类任务中具有显著优势，新提出的架构和指标推动了可解释性研究的进展。

Abstract: Sparse Autoencoders (SAEs) have been successfully used to probe Large
Language Models (LLMs) and extract interpretable concepts from their internal
representations. These concepts are linear combinations of neuron activations
that correspond to human-interpretable features. In this paper, we investigate
the effectiveness of SAE-based explainability approaches for sentence
classification, a domain where such methods have not been extensively explored.
We present a novel SAE-based architecture tailored for text classification,
leveraging a specialized classifier head and incorporating an activation rate
sparsity loss. We benchmark this architecture against established methods such
as ConceptShap, Independent Component Analysis, and other SAE-based concept
extraction techniques. Our evaluation covers two classification benchmarks and
four fine-tuned LLMs from the Pythia family. We further enrich our analysis
with two novel metrics for measuring the precision of concept-based
explanations, using an external sentence encoder. Our empirical results show
that our architecture improves both the causality and interpretability of the
extracted features.

</details>


### [75] [TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation](https://arxiv.org/abs/2506.23979)
*Renren Jin,Tianhao Shen,Xinwei Wu,Dan Shi,Haoran Sun,Wuwei Huang,Quandong Wang,Wei Liu,Jian Luan,Bin Wang,Deyi Xiong*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Conducting supervised fine-tuning and preference fine-tuning on large
language models (LLMs) requires high-quality datasets to improve their ability
to follow instructions and align with human preferences and values. However,
constructing such datasets is resource-intensive, and most available datasets
for supervised and preference fine-tuning are in English. To address these
challenges, we propose the \underline{\textbf{Ta}}xonomy-Guided
\underline{\textbf{P}}reference Data Generation (TaP) framework, which
facilitates automated and scalable construction of preference datasets across
various languages. TaP is grounded in a structured taxonomy that allows
fine-grained control over dataset composition, thereby ensuring both diversity
and comprehensive coverage. We employ TaP-generated datasets to perform
supervised and preference fine-tuning on various LLMs. Experimental results
demonstrate that LLMs trained on TaP-generated datasets outperform those
trained on existing open-source datasets. Remarkably, LLMs trained on
TaP-generated datasets surpass the performance of those trained on an
open-source dataset that is 180 times larger.

</details>


### [76] [Machine Understanding of Scientific Language](https://arxiv.org/abs/2506.23990)
*Dustin Wright*

Key words: 科学传播, 自然语言处理, 机器学习, 事实核查, 有限数据学习

TL;DR: 该论文探讨了如何利用机器学习和自然语言处理技术自动识别科学文本的忠实性，并提出了数据集、方法和工具。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着科学文本数量的激增，自动识别其忠实性成为社会重要问题，以加速知识的传播并减少误导信息。

Method: 结合自然语言处理与机器学习，提出了自动事实核查、有限数据学习和科学文本处理的新方法。

Result: 开发了多种工具和方法，能够有效识别误导性科学陈述，并生成对科学传播过程的新见解。

Conclusion: 论文的研究成果为从有限科学文本中学习提供了有效途径，有助于优化科学传播过程。

Abstract: Scientific information expresses human understanding of nature. This
knowledge is largely disseminated in different forms of text, including
scientific papers, news articles, and discourse among people on social media.
While important for accelerating our pursuit of knowledge, not all scientific
text is faithful to the underlying science. As the volume of this text has
burgeoned online in recent years, it has become a problem of societal
importance to be able to identify the faithfulness of a given piece of
scientific text automatically. This thesis is concerned with the cultivation of
datasets, methods, and tools for machine understanding of scientific language,
in order to analyze and understand science communication at scale. To arrive at
this, I present several contributions in three areas of natural language
processing and machine learning: automatic fact checking, learning with limited
data, and scientific text processing. These contributions include new methods
and resources for identifying check-worthy claims, adversarial claim
generation, multi-source domain adaptation, learning from crowd-sourced labels,
cite-worthiness detection, zero-shot scientific fact checking, detecting
exaggerated scientific claims, and modeling degrees of information change in
science communication. Critically, I demonstrate how the research outputs of
this thesis are useful for effectively learning from limited amounts of
scientific text in order to identify misinformative scientific statements and
generate new insights into the science communication process

</details>


### [77] [Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning](https://arxiv.org/abs/2506.23998)
*Seungjun Yi,Joakim Nguyen,Huimin Xu,Terence Lim,Andrew Well,Mia Markey,Ying Ding*

Key words: 先天性心脏病, 临床叙事, 大型语言模型, 主题分析, 强化学习

TL;DR: 提出了一种基于大型语言模型（LLM）的自动化管道，用于对临床叙事进行端到端的主题分析，无需人工编码或全文审查。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 先天性心脏病（CHD）的临床叙事提供了丰富的患者和照顾者经验，但传统的手工主题分析方法耗时且不可扩展。

Method: 采用多智能体框架，通过专门的LLM智能体增强主题质量和与人类分析的一致性，并可选集成来自人类反馈的强化学习（RLHF）。

Result: 实现了对大规模定性数据集的可扩展、以患者为中心的分析，并支持LLM在特定临床环境中的微调。

Conclusion: 该方法提供了一种高效、自动化的临床叙事分析解决方案，支持对CHD等复杂疾病的深入理解。

Abstract: Congenital heart disease (CHD) presents complex, lifelong challenges often
underrepresented in traditional clinical metrics. While unstructured narratives
offer rich insights into patient and caregiver experiences, manual thematic
analysis (TA) remains labor-intensive and unscalable. We propose a fully
automated large language model (LLM) pipeline that performs end-to-end TA on
clinical narratives, which eliminates the need for manual coding or full
transcript review. Our system employs a novel multi-agent framework, where
specialized LLM agents assume roles to enhance theme quality and alignment with
human analysis. To further improve thematic relevance, we optionally integrate
reinforcement learning from human feedback (RLHF). This supports scalable,
patient-centered analysis of large qualitative datasets and allows LLMs to be
fine-tuned for specific clinical contexts.

</details>


### [78] [Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective](https://arxiv.org/abs/2506.24006)
*Anselm R. Strohmaier,Wim Van Dooren,Kathrin Seßler,Brian Greer,Lieven Verschaffel*

Key words: Large Language Models, 数学教育, 应用题解决, PISA, 技术评估

TL;DR: 本文探讨了大型语言模型（LLMs）在数学教育中的应用潜力，指出其在解决数学应用题时的表面能力与实际理解的差距。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是探讨LLMs是否能有效支持数学学习，尤其是应用题解决，及其在教育中的实际价值。

Method: 研究方法包括技术概述、文献综述和实证评估，分析了LLMs在解决数学应用题时的表现及其与真实世界情境的关系。

Result: 结果显示，LLMs在解决不需要考虑真实情境的应用题时表现优异，但在处理现实情境问题或荒谬问题时表现较差。

Conclusion: 结论认为，LLMs已掌握了表面的解题过程，但缺乏对问题的深层理解，限制了其在数学课堂中的教学价值。

Abstract: The progress of Large Language Models (LLMs) like ChatGPT raises the question
of how they can be integrated into education. One hope is that they can support
mathematics learning, including word-problem solving. Since LLMs can handle
textual input with ease, they appear well-suited for solving mathematical word
problems. Yet their real competence, whether they can make sense of the
real-world context, and the implications for classrooms remain unclear. We
conducted a scoping review from a mathematics-education perspective, including
three parts: a technical overview, a systematic review of word problems used in
research, and a state-of-the-art empirical evaluation of LLMs on mathematical
word problems. First, in the technical overview, we contrast the
conceptualization of word problems and their solution processes between LLMs
and students. In computer-science research this is typically labeled
mathematical reasoning, a term that does not align with usage in mathematics
education. Second, our literature review of 213 studies shows that the most
popular word-problem corpora are dominated by s-problems, which do not require
a consideration of realities of their real-world context. Finally, our
evaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems
shows that most recent LLMs solve these s-problems with near-perfect accuracy,
including a perfect score on 20 problems from PISA. LLMs still showed
weaknesses in tackling problems where the real-world context is problematic or
non-sensical. In sum, we argue based on all three aspects that LLMs have
mastered a superficial solution process but do not make sense of word problems,
which potentially limits their value as instructional tools in mathematics
classrooms.

</details>


### [79] [EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations](https://arxiv.org/abs/2506.24016)
*Hyunjong Kim,Sangyeop Kim,Jongheon Jeong,Yeongjae Cho,Sungzoon Cho*

Key words: 图像captioning、评估指标、结构化解释、视觉语言模型

TL;DR: 提出了一种名为EXPERT的无参考评估指标，用于图像captioning，通过结构化解释和高质量数据集提升解释质量。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的图像captioning评估指标缺乏标准化解释标准，且解释质量未经验证，因此需要一种更可靠的方法。

Method: EXPERT基于流畅性、相关性和描述性三个标准提供结构化解释，通过两阶段评估模板监督视觉语言模型进行评分和解释生成。

Result: EXPERT在基准数据集上达到最先进水平，并通过人类评估验证其解释质量显著高于现有指标。

Conclusion: EXPERT通过结构化解释和高质量数据集，提升了图像captioning评估的可靠性和解释质量。

Abstract: Recent advances in large language models and vision-language models have led
to growing interest in explainable evaluation metrics for image captioning.
However, these metrics generate explanations without standardized criteria, and
the overall quality of the generated explanations remains unverified. In this
paper, we propose EXPERT, a reference-free evaluation metric that provides
structured explanations based on three fundamental criteria: fluency,
relevance, and descriptiveness. By constructing large-scale datasets of
high-quality structured explanations, we develop a two-stage evaluation
template to effectively supervise a vision-language model for both scoring and
explanation generation. EXPERT achieves state-of-the-art results on benchmark
datasets while providing significantly higher-quality explanations than
existing metrics, as validated through comprehensive human evaluation. Our code
and datasets are available at https://github.com/hjkim811/EXPERT.

</details>


### [80] [STACK: Adversarial Attacks on LLM Safeguard Pipelines](https://arxiv.org/abs/2506.24068)
*Ian R. McKenzie,Oskar J. Hollinsworth,Tom Tseng,Xander Davies,Stephen Casper,Aaron D. Tucker,Robert Kirk,Adam Gleave*

Key words: AI安全,防御管道,红队测试,STACK,分类器

TL;DR: 论文研究了AI防御管道的安全性，开发了一种新型分类器并测试其效果，同时提出了一种攻击方法STACK。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 前沿AI开发者依赖防御管道防止AI系统被滥用，但其安全性尚未充分评估，本研究填补了这一空白。

Method: 开发了开源防御管道并进行红队测试，提出新型few-shot分类器和攻击方法STACK。

Result: 新型分类器在ClearHarm数据集上表现优于现有模型，STACK攻击成功率达71%。

Conclusion: 防御管道仍存在漏洞，需进一步改进以抵御分阶段攻击。

Abstract: Frontier AI developers are relying on layers of safeguards to protect against
catastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus
model using one such defense pipeline, and other frontier developers including
Google DeepMind and OpenAI pledge to soon deploy similar defenses. However, the
security of such pipelines is unclear, with limited prior work evaluating or
attacking these pipelines. We address this gap by developing and red-teaming an
open-source defense pipeline. First, we find that a novel few-shot-prompted
input and output classifier outperforms state-of-the-art open-weight safeguard
model ShieldGemma across three attacks and two datasets, reducing the attack
success rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,
we introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on
ClearHarm in a black-box attack against the few-shot-prompted classifier
pipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%
ASR, providing initial evidence that it is feasible to design attacks with no
access to the target pipeline. We conclude by suggesting specific mitigations
that developers could use to thwart staged attacks.

</details>


### [81] [On the Predictive Power of Representation Dispersion in Language Models](https://arxiv.org/abs/2506.24106)
*Yanhong Li,Ming Li,Karen Livescu,Jiawei Zhou*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We show that a language model's ability to predict text is tightly linked to
the breadth of its embedding space: models that spread their contextual
representations more widely tend to achieve lower perplexity. Concretely, we
find that representation dispersion - the average pairwise cosine distance
among hidden vectors - strongly and negatively correlates with perplexity
across diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,
news, scientific abstracts). Beyond illustrating this link, we show how
dispersion can be leveraged for a range of practical tasks without requiring
labeled data. First, measuring dispersion on unlabeled text allows us to
predict downstream accuracy in new domains, offering a data-efficient tool for
model selection. Next, we find that identifying layers with higher dispersion
pinpoints the best representations for retrieval-based methods such as kNN-LM,
bypassing exhaustive layer-by-layer searches. Finally, we integrate a simple
push-away objective into training, which increases dispersion in both
single-domain and cross-domain scenarios and directly improves perplexity in
each.

</details>


### [82] [Computational Detection of Intertextual Parallels in Biblical Hebrew: A Benchmark Study Using Transformer-Based Language Models](https://arxiv.org/abs/2506.24117)
*David M. Smiley*

Key words: 希伯来圣经, 平行段落, 预训练模型, 词嵌入, 文本分析

TL;DR: 本研究评估了预训练Transformer模型在希伯来圣经中检测文本平行段落的潜力，发现E5和AlephBERT表现优异，能提升古代文本研究的效率和准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统的手动比较方法耗时且易错，研究旨在利用现代语言模型提高希伯来圣经中平行段落的检测效率。

Method: 使用E5、AlephBERT、MPNet和LaBSE模型生成词嵌入，并通过余弦相似度和Wasserstein距离评估其检测平行段落的能力。

Result: E5在平行段落检测中表现最佳，AlephBERT在区分非平行段落时更强，两者显示出显著潜力。

Conclusion: 预训练模型能有效提升古代文本平行段落的检测效率和准确性，适用于更广泛的古代语言研究。

Abstract: Identifying parallel passages in biblical Hebrew is foundational in biblical
scholarship for uncovering intertextual relationships. Traditional methods rely
on manual comparison, which is labor-intensive and prone to human error. This
study evaluates the potential of pre-trained transformer-based language models,
including E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in
the Hebrew Bible. Focusing on known parallels between the books of Samuel/Kings
and Chronicles, I assessed each model's capability to generate word embeddings
that delineate parallel from non-parallel passages. Utilizing cosine similarity
and Wasserstein Distance measures, I found that E5 and AlephBERT show
significant promise, with E5 excelling in parallel detection and AlephBERT
demonstrating stronger non-parallel differentiation. These findings indicate
that pre-trained models can enhance the efficiency and accuracy of detecting
intertextual parallels in ancient texts, suggesting broader applications for
ancient language studies.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [83] [Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation](https://arxiv.org/abs/2506.22441)
*Lei Yang*

Key words: 智能交通系统、张量分解、异常值处理、缺失数据插补、TDWLFT模型

TL;DR: 提出了一种基于阈值距离加权损失函数的张量分解模型（TDWLFT），用于处理交通数据中的缺失值和异常值，提升了预测精度和计算效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 智能交通系统（ITS）依赖高质量的时空交通数据，但现实中数据常因通信故障或传感器问题导致不完整，现有张量分解模型（LFT）对异常值敏感，限制了其效果。

Method: 提出了一种阈值距离加权损失函数（TDW）的张量分解模型（TDWLFT），通过对样本分配差异化权重，降低了模型对异常值的敏感性。

Result: 在两个不同城市环境的交通速度数据集上的实验表明，TDWLFT模型在预测精度和计算效率上均优于现有方法。

Conclusion: TDWLFT模型显著提升了交通数据缺失值插补的鲁棒性和性能，为ITS提供了更可靠的数据支持。

Abstract: Intelligent transportation systems (ITS) rely heavily on complete and
high-quality spatiotemporal traffic data to achieve optimal performance.
Nevertheless, in real-word traffic data collection processes, issues such as
communication failures and sensor malfunctions often lead to incomplete or
corrupted datasets, thereby posing significant challenges to the advancement of
ITS. Among various methods for imputing missing spatiotemporal traffic data,
the latent factorization of tensors (LFT) model has emerged as a widely adopted
and effective solution. However, conventional LFT models typically employ the
standard L2-norm in their learning objective, which makes them vulnerable to
the influence of outliers. To overcome this limitation, this paper proposes a
threshold distance weighted (TDW) loss-incorporated Latent Factorization of
Tensors (TDWLFT) model. The proposed loss function effectively reduces the
model's sensitivity to outliers by assigning differentiated weights to
individual samples. Extensive experiments conducted on two traffic speed
datasets sourced from diverse urban environments confirm that the proposed
TDWLFT model consistently outperforms state-of-the-art approaches in terms of
both in both prediction accuracy and computational efficiency.

</details>


### [84] [Features-based embedding or Feature-grounding](https://arxiv.org/abs/2506.22442)
*Piotr Makarevich*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In everyday reasoning, when we think about a particular object, we associate
it with a unique set of expected properties such as weight, size, or more
abstract attributes like density or horsepower. These expectations are shaped
by our prior knowledge and the conceptual categories we have formed through
experience. This paper investigates how such knowledge-based structured
thinking can be reproduced in deep learning models using features based
embeddings. Specially, it introduces an specific approach to build
feature-grounded embedding, aiming to align shareable representations of
operable dictionary with interpretable domain-specific conceptual features.

</details>


### [85] [Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition](https://arxiv.org/abs/2506.22443)
*Sarah Seifi,Tobias Sukianto,Cecilia Carbonelli,Lorenzo Servadei,Robert Wille*

Key words: 神经符号模型、规则学习、手势识别、可解释性、深度学习

TL;DR: RL-Net是一种神经符号规则学习网络，首次应用于雷达手势识别（HGR），在性能和可解释性之间取得平衡，优于传统规则系统和黑盒模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决规则模型在复杂数据上的性能不足和深度学习模型的可解释性差的问题，探索神经符号模型在实际应用中的可行性。

Method: 提出RL-Net，通过神经优化学习可解释规则列表，并与MIRA（规则系统）和XentricAI（可解释黑盒模型）进行对比。

Result: RL-Net在F1得分上达到93.03%，同时显著降低规则复杂性，验证了其作为性能与透明度折中方案的实用性。

Conclusion: RL-Net证明了神经符号模型在手势识别中的潜力，为可解释AI在边缘传感系统中的扩展提供了见解。

Abstract: Rule-based models offer interpretability but struggle with complex data,
while deep neural networks excel in performance yet lack transparency. This
work investigates a neuro-symbolic rule learning neural network named RL-Net
that learns interpretable rule lists through neural optimization, applied for
the first time to radar-based hand gesture recognition (HGR). We benchmark
RL-Net against a fully transparent rule-based system (MIRA) and an explainable
black-box model (XentricAI), evaluating accuracy, interpretability, and user
adaptability via transfer learning. Our results show that RL-Net achieves a
favorable trade-off, maintaining strong performance (93.03% F1) while
significantly reducing rule complexity. We identify optimization challenges
specific to rule pruning and hierarchy bias and propose stability-enhancing
modifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical
middle ground between transparency and performance. This study highlights the
real-world feasibility of neuro-symbolic models for interpretable HGR and
offers insights for extending explainable AI to edge-deployable sensing
systems.

</details>


### [86] [Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS-CoV-2](https://arxiv.org/abs/2506.22444)
*Jing Wang,Amar Sra,Jeremy C. Weiss*

Key words: PASC, SARS-CoV-2, 主动注意力网络, 临床风险预测

TL;DR: 研究提出了基于大语言模型的主动注意力网络，用于预测PASC患者的临床风险，并识别相关进展事件，以提高预测准确性和减少标注需求。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: PASC的长期影响对全球医疗系统构成挑战，传统模型难以捕捉其复杂进展，因此需要更有效的预测方法。

Method: 利用Llama-3.1-70B-Instruct生成文本时间序列特征，结合临床专家标注，提出主动注意力网络进行风险预测和事件识别。

Result: 提出了首个公开PASC患者队列，并通过结合人类专业知识和主动学习，提高了临床风险预测的准确性。

Conclusion: 该方法可改善SARS-CoV-2患者的诊疗决策，为医疗资源分配提供支持。

Abstract: The long-term effects of Postacute Sequelae of SARS-CoV-2, known as PASC,
pose a significant challenge to healthcare systems worldwide. Accurate
identification of progression events, such as hospitalization and reinfection,
is essential for effective patient management and resource allocation. However,
traditional models trained on structured data struggle to capture the nuanced
progression of PASC. In this study, we introduce the first publicly available
cohort of 18 PASC patients, with text time series features based on Large
Language Model Llama-3.1-70B-Instruct and clinical risk annotated by clinical
expert. We propose an Active Attention Network to predict the clinical risk and
identify progression events related to the risk. By integrating human expertise
with active learning, we aim to enhance clinical risk prediction accuracy and
enable progression events identification with fewer number of annotation. The
ultimate goal is to improves patient care and decision-making for SARS-CoV-2
patient.

</details>


### [87] [Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security](https://arxiv.org/abs/2506.22445)
*Saad Alqithami*

Key words: 网络物理系统,多智能体强化学习,对抗训练,安全韧性

TL;DR: 该论文提出了一种名为HAMARL的新框架，通过分层多智能体强化学习和对抗训练提升网络物理系统的安全性和韧性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 网络物理系统的互联性使其易受复杂网络威胁，传统安全方法不足以应对自适应和零日攻击。

Method: 采用分层结构，结合本地智能体子系统安全和全局协调器的系统防御，并引入对抗训练循环。

Result: 在模拟工业物联网测试中，显著优于传统方法，提高了攻击检测精度和响应速度。

Conclusion: 分层多智能体协调与对抗训练结合可有效增强下一代网络物理系统的韧性和安全性。

Abstract: Cyber-Physical Systems play a critical role in the infrastructure of various
sectors, including manufacturing, energy distribution, and autonomous
transportation systems. However, their increasing connectivity renders them
highly vulnerable to sophisticated cyber threats, such as adaptive and zero-day
attacks, against which traditional security methods like rule-based intrusion
detection and single-agent reinforcement learning prove insufficient. To
overcome these challenges, this paper introduces a novel Hierarchical
Adversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework.
HAMARL employs a hierarchical structure consisting of local agents dedicated to
subsystem security and a global coordinator that oversees and optimizes
comprehensive, system-wide defense strategies. Furthermore, the framework
incorporates an adversarial training loop designed to simulate and anticipate
evolving cyber threats, enabling proactive defense adaptation. Extensive
experimental evaluations conducted on a simulated industrial IoT testbed
indicate that HAMARL substantially outperforms traditional multi-agent
reinforcement learning approaches, significantly improving attack detection
accuracy, reducing response times, and ensuring operational continuity. The
results underscore the effectiveness of combining hierarchical multi-agent
coordination with adversarially-aware training to enhance the resilience and
security of next-generation CPS.

</details>


### [88] [EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis](https://arxiv.org/abs/2506.22446)
*Aakash Tripathi,Asim Waqas,Matthew B. Schabath,Yasin Yilmaz,Ghulam Rasool*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Accurate cancer survival prediction requires integration of diverse data
modalities that reflect the complex interplay between imaging, clinical
parameters, and textual reports. However, existing multimodal approaches suffer
from simplistic fusion strategies, massive computational requirements, and lack
of interpretability-critical barriers to clinical adoption. We present EAGLE
(Efficient Alignment of Generalized Latent Embeddings), a novel deep learning
framework that addresses these limitations through attention-based multimodal
fusion with comprehensive attribution analysis. EAGLE introduces four key
innovations: (1) dynamic cross-modal attention mechanisms that learn
hierarchical relationships between modalities, (2) massive dimensionality
reduction (99.96%) while maintaining predictive performance, (3) three
complementary attribution methods providing patient-level interpretability, and
(4) a unified pipeline enabling seamless adaptation across cancer types. We
evaluated EAGLE on 911 patients across three distinct malignancies:
glioblastoma (GBM, n=160), intraductal papillary mucinous neoplasms (IPMN,
n=171), and non-small cell lung cancer (NSCLC, n=580). Patient-level analysis
showed high-risk individuals relied more heavily on adverse imaging features,
while low-risk patients demonstrated balanced modality contributions. Risk
stratification identified clinically meaningful groups with 4-fold (GBM) to
5-fold (NSCLC) differences in median survival, directly informing treatment
intensity decisions. By combining state-of-the-art performance with clinical
interpretability, EAGLE bridges the gap between advanced AI capabilities and
practical healthcare deployment, offering a scalable solution for multimodal
survival prediction that enhances both prognostic accuracy and physician trust
in automated predictions.

</details>


### [89] [Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture](https://arxiv.org/abs/2506.22447)
*Fabio Merizzi,Harilaos Loukos*

Key words: 气候降尺度, ViT架构, 多任务学习, 深度学习

TL;DR: 提出一种多任务、多变量的视觉变换器架构（1EMD），通过共享编码器和变量特定解码器联合预测多个气候变量，优于单变量方法并提高计算效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有深度学习模型多专注于单变量降尺度，缺乏变量间交互和上下文意识，导致冗余计算和信息孤立。

Method: 采用多任务、多变量的ViT架构，包含共享编码器和变量特定解码器，联合预测温度、风速和500 hPa位势高度。

Result: 多变量方法实现了变量间的知识转移，性能优于单变量基线，计算效率更高。

Conclusion: 多变量建模在高分辨率气候降尺度中更具优势。

Abstract: Global Climate Models (GCMs) are critical for simulating large-scale climate
dynamics, but their coarse spatial resolution limits their applicability in
regional studies. Regional Climate Models (RCMs) refine this through dynamic
downscaling, albeit at considerable computational cost and with limited
flexibility. While deep learning has emerged as an efficient data-driven
alternative, most existing studies have focused on single-variable models that
downscale one variable at a time. This approach can lead to limited contextual
awareness, redundant computation, and lack of cross-variable interaction. Our
study addresses these limitations by proposing a multi-task, multi-variable
Vision Transformer (ViT) architecture with a shared encoder and
variable-specific decoders (1EMD). The proposed architecture jointly predicts
three key climate variables: surface temperature (tas), wind speed (sfcWind),
and 500 hPa geopotential height (zg500), directly from GCM-resolution inputs,
emulating RCM-scale downscaling over Europe. We show that our multi-variable
approach achieves positive cross-variable knowledge transfer and consistently
outperforms single-variable baselines trained under identical conditions, while
also improving computational efficiency. These results demonstrate the
effectiveness of multi-variable modeling for high-resolution climate
downscaling.

</details>


### [90] [Stabilization of industrial processes with time series machine learning](https://arxiv.org/abs/2506.22502)
*Matvei Anoshin,Olga Tsurkan,Vadim Lopatkin,Leonid Fedichkin*

Key words: 时间序列稳定性, 机器学习, 神经网络, 温度控制

TL;DR: 论文提出了一种基于神经网络的简单流程，通过两个网络（oracle预测器和优化器）优化时间序列稳定性，相比传统方法在温度控制中提升了3倍的稳定性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 时间序列稳定性在工业领域中非常重要，机器学习可以提升稳定性并减少计算资源需求。

Method: 采用由oracle预测器和优化器组成的神经网络流程，将点值优化问题转化为神经网络训练问题。

Result: 在温度控制中，稳定性比普通求解器提升了约3倍。

Conclusion: 该神经网络流程在提升时间序列稳定性方面表现优异，且计算效率高。

Abstract: The stabilization of time series processes is a crucial problem that is
ubiquitous in various industrial fields. The application of machine learning to
its solution can have a decisive impact, improving both the quality of the
resulting stabilization with less computational resources required. In this
work, we present a simple pipeline consisting of two neural networks: the
oracle predictor and the optimizer, proposing a substitution of the point-wise
values optimization to the problem of the neural network training, which
successfully improves stability in terms of the temperature control by about 3
times compared to ordinary solvers.

</details>


### [91] [Task-Agnostic Contrastive Pretraining for Relational Deep Learning](https://arxiv.org/abs/2506.22530)
*Jakub Peleška,Gustav Šír*

Key words: Relational Deep Learning, 对比学习, 预训练, 图神经网络, 关系数据库

TL;DR: 提出了针对关系深度学习（RDL）的任务无关对比预训练方法，通过多层次对比目标学习数据库的通用表示。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有RDL模型依赖特定任务的监督学习，难以扩展和复用，需要一种任务无关的学习方法。

Method: 设计了行级、链接级和上下文级对比目标，并通过模块化RDL架构和高效采样策略实现预训练。

Result: 预训练模型微调后的性能优于从头训练，验证了方法的有效性。

Conclusion: 该方法为关系数据学习可迁移表示提供了潜力。

Abstract: Relational Deep Learning (RDL) is an emerging paradigm that leverages Graph
Neural Network principles to learn directly from relational databases by
representing them as heterogeneous graphs. However, existing RDL models
typically rely on task-specific supervised learning, requiring training
separate models for each predictive task, which may hamper scalability and
reuse.
  In this work, we propose a novel task-agnostic contrastive pretraining
approach for RDL that enables database-wide representation learning. For that
aim, we introduce three levels of contrastive objectives$-$row-level,
link-level, and context-level$-$designed to capture the structural and semantic
heterogeneity inherent to relational data. We implement the respective
pretraining approach through a modular RDL architecture and an efficient
sampling strategy tailored to the heterogeneous database setting. Our
preliminary results on standard RDL benchmarks demonstrate that fine-tuning the
pretrained models measurably outperforms training from scratch, validating the
promise of the proposed methodology in learning transferable representations
for relational data.

</details>


### [92] [Exploration Behavior of Untrained Policies](https://arxiv.org/abs/2506.22566)
*Jacob Adamczyk*

Key words: 强化学习,探索,深度神经网络,策略初始化,归纳偏置

TL;DR: 本文研究了深度神经网络策略在训练前如何隐式影响强化学习中的探索行为，通过理论和实验展示了未训练策略生成弹道或扩散轨迹的策略，并讨论了其对探索的归纳偏置。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索是强化学习中的核心挑战，尤其在稀疏或对抗性奖励环境中，本文旨在理解未训练策略如何通过其架构隐式塑造探索行为。

Method: 利用无限宽度网络理论和连续时间极限，分析了未训练策略生成的动作相关性及其对状态访问分布的影响，并通过实验验证了标准架构的轨迹分布。

Result: 研究表明，未训练策略会生成非平凡的状态访问分布，揭示了探索行为的归纳偏置，为早期训练中的探索行为提供了理论框架。

Conclusion: 本文建立了一个理论和实验框架，通过策略初始化作为设计工具，帮助理解早期训练中的探索行为。

Abstract: Exploration remains a fundamental challenge in reinforcement learning (RL),
particularly in environments with sparse or adversarial reward structures. In
this work, we study how the architecture of deep neural policies implicitly
shapes exploration before training. We theoretically and empirically
demonstrate strategies for generating ballistic or diffusive trajectories from
untrained policies in a toy model. Using the theory of infinite-width networks
and a continuous-time limit, we show that untrained policies return correlated
actions and result in non-trivial state-visitation distributions. We discuss
the distributions of the corresponding trajectories for a standard
architecture, revealing insights into inductive biases for tackling
exploration. Our results establish a theoretical and experimental framework for
using policy initialization as a design tool to understand exploration behavior
in early training.

</details>


### [93] [The Hidden Link Between RLHF and Contrastive Learning](https://arxiv.org/abs/2506.22578)
*Xufei Lv,Haoyuan Sun,Xuefeng Bai,Min Zhang,Houde Liu,Kehai Chen*

Key words: 大语言模型,人类价值观对齐,互信息,对比学习,MIO

TL;DR: 论文通过互信息（MI）最大化的视角重新解读了RLHF和DPO方法，揭示其与对比学习的联系，并提出了基于Jensen-Shannon MI估计器的MIO方法，解决了DPO中后期性能下降的问题，在多个推理和数学任务上表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究目标是探索LLM与人类价值观对齐的方法，尤其是在RLHF和DPO的基础上，通过MI最大化框架揭示其局限性并提出改进方案。

Method: 将RLHF和DPO统一为基于对比学习和MI最大化的方法，并提出使用Jensen-Shannon MI估计器的MIO方法，替代原有的DV/MINE边界。

Result: MIO显著改善了DPO中后期性能下降的问题，在推理和数学任务上表现优于或与现有方法相当。

Conclusion: 通过MI框架重新理解和对齐方法，MIO提供了一个更高效且稳定的优化方案。

Abstract: Alignment of large language models (LLMs) with human values has recently
garnered significant attention, with prominent examples including the canonical
yet costly Reinforcement Learning from Human Feedback (RLHF) and the simple
Direct Preference Optimization (DPO). In this work, we demonstrate that both
RLHF and DPO can be interpreted from the perspective of mutual information (MI)
maximization, uncovering a profound connection to contrastive learning. Within
this framework, both RLHF and DPO can be viewed as methods that perform
contrastive learning based on the positive and negative samples derived from
the base model, leveraging the Donsker-Varadhan (DV) lower bound on MI
(equivalently, the MINE estimator). This paradigm further explains why RLHF may
not intrinsically incentivize reasoning capacities in LLMs beyond what is
already present in the base model. Building on this perspective, we replace the
DV/MINE bound with the Jensen-Shannon MI estimator and propose Mutual
Information Optimization (MIO). Comprehensive theoretical analysis and
extensive empirical evaluations demonstrate that MIO mitigates the late-stage
decline in chosen-likelihood observed in DPO, achieving competitive or superior
performance across various challenging reasoning and mathematical benchmarks.
We will release the model and code upon acceptance.

</details>


### [94] [Are Fast Methods Stable in Adversarially Robust Transfer Learning?](https://arxiv.org/abs/2506.22602)
*Joshua C. Zhao,Saurabh Bagchi*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Transfer learning is often used to decrease the computational cost of model
training, as fine-tuning a model allows a downstream task to leverage the
features learned from the pre-training dataset and quickly adapt them to a new
task. This is particularly useful for achieving adversarial robustness, as
adversarially training models from scratch is very computationally expensive.
However, high robustness in transfer learning still requires adversarial
training during the fine-tuning phase, which requires up to an order of
magnitude more time than standard fine-tuning. In this work, we revisit the use
of the fast gradient sign method (FGSM) in robust transfer learning to improve
the computational cost of adversarial fine-tuning. We surprisingly find that
FGSM is much more stable in adversarial fine-tuning than when training from
scratch. In particular, FGSM fine-tuning does not suffer from any issues with
catastrophic overfitting at standard perturbation budgets of $\varepsilon=4$ or
$\varepsilon=8$. This stability is further enhanced with parameter-efficient
fine-tuning methods, where FGSM remains stable even up to $\varepsilon=32$ for
linear probing. We demonstrate how this stability translates into performance
across multiple datasets. Compared to fine-tuning with the more commonly used
method of projected gradient descent (PGD), on average, FGSM only loses 0.39%
and 1.39% test robustness for $\varepsilon=4$ and $\varepsilon=8$ while using
$4\times$ less training time. Surprisingly, FGSM may not only be a
significantly more efficient alternative to PGD in adversarially robust
transfer learning but also a well-performing one.

</details>


### [95] [Hierarchical Modeling and Architecture Optimization: Review and Unified Framework](https://arxiv.org/abs/2506.22621)
*Paul Saves,Edward Hallé-Hannan,Jasper Bussemaker,Youssef Diouane,Nathalie Bartoli*

Key words: 混合变量输入, 层次结构, 代理模型, 设计空间图, 贝叶斯优化

TL;DR: 提出了一种统一框架，用于处理层次化、条件化和异构的混合变量输入空间，支持代理模型建模和高效优化，并在开源工具箱SMT 2.0中实现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决层次化、条件化和异构的混合变量输入空间在数据表示、建模和优化中的挑战。

Method: 通过设计空间图刻画变量间的层次关系，结合代理模型、层次化核函数和距离度量。

Result: 实现了一个支持复杂系统设计的框架，并在绿色飞机架构案例中验证其能力。

Conclusion: 统一框架成功解决了复杂结构化输入空间的建模与优化问题。

Abstract: Simulation-based problems involving mixed-variable inputs frequently feature
domains that are hierarchical, conditional, heterogeneous, or tree-structured.
These characteristics pose challenges for data representation, modeling, and
optimization. This paper reviews extensive literature on these structured input
spaces and proposes a unified framework that generalizes existing approaches.
In this framework, input variables may be continuous, integer, or categorical.
A variable is described as meta if its value governs the presence of other
decreed variables, enabling the modeling of conditional and hierarchical
structures.
  We further introduce the concept of partially-decreed variables, whose
activation depends on contextual conditions. To capture these inter-variable
hierarchical relationships, we introduce design space graphs, combining
principles from feature modeling and graph theory. This allows the definition
of general hierarchical domains suitable for describing complex system
architectures. The framework supports the use of surrogate models over such
domains and integrates hierarchical kernels and distances for efficient
modeling and optimization. The proposed methods are implemented in the
open-source Surrogate Modeling Toolbox (SMT 2.0), and their capabilities are
demonstrated through applications in Bayesian optimization for complex system
design, including a case study in green aircraft architecture.

</details>


### [96] [A hierarchical Vovk-Azoury-Warmuth forecaster with discounting for online regression in RKHS](https://arxiv.org/abs/2506.22631)
*Dmitry B. Rokhlin*

Key words: 在线回归, RKHS, 动态遗憾, 随机特征, 自适应算法

TL;DR: 本文研究了在线回归问题，提出了一种基于随机特征近似的自适应分层算法H-VAW-D，能够同时学习折扣因子和随机特征数量，计算复杂度低且动态遗憾最优。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决非参数域中针对动态比较器序列的在线回归问题，扩展了有限维情况下的最优动态遗憾算法。

Method: 结合DVAW框架与随机特征近似，提出自适应分层算法H-VAW-D，学习折扣因子和随机特征数量。

Result: 算法计算复杂度为$O(T\ln T)$，预期动态遗憾为$O(T^{2/3}P_T^{1/3} + \sqrt{T}\ln T)$。

Conclusion: H-VAW-D算法在非参数域中实现了低计算复杂度和最优动态遗憾。

Abstract: We study the problem of online regression with the unconstrained quadratic
loss against a time-varying sequence of functions from a Reproducing Kernel
Hilbert Space (RKHS). Recently, Jacobsen and Cutkosky (2024) introduced a
discounted Vovk-Azoury-Warmuth (DVAW) forecaster that achieves optimal dynamic
regret in the finite-dimensional case. In this work, we lift their approach to
the non-parametric domain by synthesizing the DVAW framework with a random
feature approximation. We propose a fully adaptive, hierarchical algorithm,
which we call H-VAW-D (Hierarchical Vovk-Azoury-Warmuth with Discounting), that
learns both the discount factor and the number of random features. We prove
that this algorithm, which has a per-iteration computational complexity of
$O(T\ln T)$, achieves an expected dynamic regret of $O(T^{2/3}P_T^{1/3} +
\sqrt{T}\ln T)$, where $P_T$ is the functional path length of a comparator
sequence.

</details>


### [97] [Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training](https://arxiv.org/abs/2506.22638)
*Aadim Nepal,Safal Shrestha,Anubhav Shrestha,Minwu Kim,Keith Ross*

Key words: 大型语言模型,数学推理,层结构,后训练

TL;DR: 大型语言模型通过后训练（如指令调优、强化学习或知识蒸馏）可提升数学推理能力，但这些改进是否源于对Transformer层的重大改变仍不明确。研究表明，数学推理依赖于特定的层结构，且这种结构在后训练中保持不变。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探究数学推理能力的提升是否源于Transformer层结构的重大改变，还是仅基于微小调整。

Method: 通过层级的消融实验，比较基础模型、指令调优模型、知识蒸馏模型和强化学习模型在数学推理任务中的表现。

Result: 数学推理任务依赖于特定的层结构，去除关键层会导致准确率下降80%，而非数学任务则无此现象。

Conclusion: 数学推理需要预训练中形成的专用层，而非推理任务则不然。

Abstract: Large language models can exhibit improved mathematical reasoning
capabilities following post-training with instruction tuning, reinforcement
learning, or knowledge distillation. However, it remains unclear whether these
improvements are driven by major changes in transformer layers or from minor
adjustments that leave the relative layer importance structures of the base
model largely unchanged. We investigate this question through systematic
layer-wise ablation experiments, examining base, instruction-tuned,
knowledge-distilled, and reinforcement learning variants on mathematical
reasoning benchmarks. Our findings show that mathematical reasoning gives rise
to a specific layer importance structure, and this structure persists across
all post-training paradigms. Removal of such layers causes accuracy drops of up
to 80%. In contrast, non-mathematical tasks like factual recall exhibit no
critical layers. This distinction suggests that mathematical reasoning requires
specialized layers that emerge during pre-training, while other non-reasoning
tasks do not. From an information-theoretic perspective, we also observe that
these critical layers are the same layers where major representational
transformation occurs.

</details>


### [98] [Cost-effective Reduced-Order Modeling via Bayesian Active Learning](https://arxiv.org/abs/2506.22645)
*Amir Hossein Rahmati,Nathan M. Urban,Byung-Jun Yoon,Xiaoning Qian*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Machine Learning surrogates have been developed to accelerate solving systems
dynamics of complex processes in different science and engineering
applications. To faithfully capture governing systems dynamics, these methods
rely on large training datasets, hence restricting their applicability in
real-world problems. In this work, we propose BayPOD-AL, an active learning
framework based on an uncertainty-aware Bayesian proper orthogonal
decomposition (POD) approach, which aims to effectively learn reduced-order
models from high-fidelity full-order models representing complex systems.
Experimental results on predicting the temperature evolution over a rod
demonstrate BayPOD-AL's effectiveness in suggesting the informative data and
reducing computational cost related to constructing a training dataset compared
to other uncertainty-guided active learning strategies. Furthermore, we
demonstrate BayPOD-AL's generalizability and efficiency by evaluating its
performance on a dataset of higher temporal resolution than the training
dataset.

</details>


### [99] [Learning Stochastic Multiscale Models](https://arxiv.org/abs/2506.22655)
*Andrew F. Ilersich,Prasanth B. Nair*

Key words: 多尺度模型,随机微分方程,变分推断,数据驱动,计算物理

TL;DR: 提出一种从观测数据中直接学习随机多尺度模型的方法，解决了直接数值模拟的高计算成本问题，并通过变分推断方法学习模型参数，证明其预测精度优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 物理科学中的多尺度动力学系统计算成本高，研究旨在通过数据驱动方法学习多尺度模型，降低计算复杂度。

Method: 使用基于变分推断的方法，从数据中直接学习随机微分方程形式的随机多尺度模型，包括粗网格状态和辅助状态。

Result: 学习得到的多尺度模型在预测精度上优于直接数值模拟和同分辨率的封闭模型。

Conclusion: 提出的数据驱动方法有效降低了多尺度系统的计算成本，同时保持了高预测精度。

Abstract: The physical sciences are replete with dynamical systems that require the
resolution of a wide range of length and time scales. This presents significant
computational challenges since direct numerical simulation requires
discretization at the finest relevant scales, leading to a high-dimensional
state space. In this work, we propose an approach to learn stochastic
multiscale models in the form of stochastic differential equations directly
from observational data. Our method resolves the state on a coarse mesh while
introducing an auxiliary state to capture the effects of unresolved scales. We
learn the parameters of the multiscale model using a modern forward-solver-free
amortized variational inference method. Our approach draws inspiration from
physics-based multiscale modeling approaches, such as large-eddy simulation in
fluid dynamics, while learning directly from data. We present numerical studies
to demonstrate that our learned multiscale models achieve superior predictive
accuracy compared to direct numerical simulation and closure-type models at
equivalent resolution.

</details>


### [100] [DistShap: Scalable GNN Explanations with Distributed Shapley Values](https://arxiv.org/abs/2506.22668)
*Selahattin Akkas,Aditya Devarakonda,Ariful Azad*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: With the growing adoption of graph neural networks (GNNs), explaining their
predictions has become increasingly important. However, attributing predictions
to specific edges or features remains computationally expensive. For example,
classifying a node with 100 neighbors using a 3-layer GNN may involve
identifying important edges from millions of candidates contributing to the
prediction. To address this challenge, we propose DistShap, a parallel
algorithm that distributes Shapley value-based explanations across multiple
GPUs. DistShap operates by sampling subgraphs in a distributed setting,
executing GNN inference in parallel across GPUs, and solving a distributed
least squares problem to compute edge importance scores. DistShap outperforms
most existing GNN explanation methods in accuracy and is the first to scale to
GNN models with millions of features by using up to 128 GPUs on the NERSC
Perlmutter supercomputer.

</details>


### [101] [Mitigating Semantic Collapse in Generative Personalization with a Surprisingly Simple Test-Time Embedding Adjustment](https://arxiv.org/abs/2506.22685)
*Anh Bui,Trang Vu,Trung Le,Junae Kim,Tamas Abraham,Rollin Omari,Amar Kaur,Dinh Phung*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we investigate the semantic collapsing problem in generative
personalization, an under-explored topic where the learned visual concept
($V^*$) gradually shifts from its original textual meaning and comes to
dominate other concepts in multi-concept input prompts. This issue not only
reduces the semantic richness of complex input prompts like "a photo of $V^*$
wearing glasses and playing guitar" into simpler, less contextually rich forms
such as "a photo of $V^*$" but also leads to simplified output images that fail
to capture the intended concept.
  We identify the root cause as unconstrained optimisation, which allows the
learned embedding $V^*$ to drift arbitrarily in the embedding space, both in
direction and magnitude. To address this, we propose a simple yet effective
training-free method that adjusts the magnitude and direction of pre-trained
embedding at inference time, effectively mitigating the semantic collapsing
problem. Our method is broadly applicable across different personalization
methods and demonstrates significant improvements in text-image alignment in
diverse use cases. Our code is anonymously published at
https://anonymous.4open.science/r/Embedding-Adjustment.

</details>


### [102] [Residual Matrix Transformers: Scaling the Size of the Residual Stream](https://arxiv.org/abs/2506.22696)
*Brian Mak,Jeffrey Flanigan*

Key words: Transformer, 残差流, 外积记忆矩阵, RMT, 计算效率

TL;DR: 论文提出了一种替代Transformer残差流的外积记忆矩阵，称为残差矩阵Transformer（RMT），该模型在性能、计算效率和下游任务表现上优于传统Transformer。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究旨在改进Transformer中残差流的信息存储与检索机制，提出更高效的模型设计。

Method: 用外积记忆矩阵（Kohonen, 1972）替代Transformer的残差流，构建RMT模型。

Result: RMT在减少计算量（58% FLOPS）、参数量（25%）和训练数据（41%）的同时，性能与传统Transformer相当甚至更好。

Conclusion: RMT在残差流扩展和方差传播方面表现出更高效的优势，适用于需要高性能与效率的场景。

Abstract: The residual stream acts as a memory bus where transformer layers both store
and access features (Elhage et al., 2021). We consider changing the mechanism
for retrieving and storing information in the residual stream, and replace the
residual stream of the transformer with an outer product memory matrix
(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix
Transformer (RMT). We find that the RMT enjoys a number of attractive
properties: 1) the size of the residual stream can be scaled independently of
compute and model size, improving performance, 2) the RMT can achieve the same
loss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%
fewer training tokens tokens, and 3) the RMT outperforms the transformer on
downstream evaluations. We theoretically analyze the transformer and the RMT,
and show that the RMT allows for more efficient scaling of the residual stream,
as well as improved variance propagation properties. Code for this project can
be found at https://github.com/bmac3/residual-matrix-transformer.

</details>


### [103] [FairMarket-RL: LLM-Guided Fairness Shaping for Multi-Agent Reinforcement Learning in Peer-to-Peer Markets](https://arxiv.org/abs/2506.22708)
*Shrenik Jadhav,Birva Sevak,Srijita Das,Akhtar Hussain,Wencong Su,Van-Hai Bui*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Peer-to-peer (P2P) trading is increasingly recognized as a key mechanism for
decentralized market regulation, yet existing approaches often lack robust
frameworks to ensure fairness. This paper presents FairMarket-RL, a novel
hybrid framework that combines Large Language Models (LLMs) with Reinforcement
Learning (RL) to enable fairness-aware trading agents. In a simulated P2P
microgrid with multiple sellers and buyers, the LLM acts as a real-time
fairness critic, evaluating each trading episode using two metrics:
Fairness-To-Buyer (FTB) and Fairness-Between-Sellers (FBS). These fairness
scores are integrated into agent rewards through scheduled
{\lambda}-coefficients, forming an adaptive LLM-guided reward shaping loop that
replaces brittle, rule-based fairness constraints. Agents are trained using
Independent Proximal Policy Optimization (IPPO) and achieve equitable outcomes,
fulfilling over 90% of buyer demand, maintaining fair seller margins, and
consistently reaching FTB and FBS scores above 0.80. The training process
demonstrates that fairness feedback improves convergence, reduces buyer
shortfalls, and narrows profit disparities between sellers. With its
language-based critic, the framework scales naturally, and its extension to a
large power distribution system with household prosumers illustrates its
practical applicability. FairMarket-RL thus offers a scalable, equity-driven
solution for autonomous trading in decentralized energy systems.

</details>


### [104] [Generalized Linear Mode Connectivity for Transformers](https://arxiv.org/abs/2506.22712)
*Alexander Theus,Alessandro Cabodi,Sotiris Anagnostidis,Antonio Orvieto,Sidak Pal Singh,Valentina Boeva*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Understanding the geometry of neural network loss landscapes is a central
question in deep learning, with implications for generalization and
optimization. A striking phenomenon is linear mode connectivity (LMC), where
independently trained models can be connected by low- or zero-loss paths,
despite appearing to lie in separate loss basins. However, this is often
obscured by symmetries in parameter space -- such as neuron permutations --
which make functionally equivalent models appear dissimilar. Prior work has
predominantly focused on neuron re-ordering through permutations, but such
approaches are limited in scope and fail to capture the richer symmetries
exhibited by modern architectures such as Transformers. In this work, we
introduce a unified framework that captures four symmetry classes:
permutations, semi-permutations, orthogonal transformations, and general
invertible maps -- broadening the set of valid reparameterizations and
subsuming many previous approaches as special cases. Crucially, this
generalization enables, for the first time, the discovery of low- and
zero-barrier linear interpolation paths between independently trained Vision
Transformers and GPT-2 models. These results reveal deeper structure in the
loss landscape and underscore the importance of symmetry-aware analysis for
understanding model space geometry.

</details>


### [105] [BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute](https://arxiv.org/abs/2506.22716)
*Dujian Ding,Ankur Mallick,Shaokun Zhang,Chi Wang,Daniel Madrigal,Mirian Del Carmen Hipolito Garcia,Menglin Xia,Laks V. S. Lakshmanan,Qingyun Wu,Victor Rühle*

Key words: 大语言模型, 查询路由, 成本优化, BEST-Route

TL;DR: BEST-Route是一种新的LLM查询路由框架，通过根据查询难度和质量阈值选择模型和响应数量，显著降低成本同时保持性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大型语言模型（LLM）部署成本高，现有路由方法因小模型单响应质量不足而过度依赖大模型，未能实现潜在成本节约。

Method: 提出BEST-Route框架，通过为小模型生成多个响应并选择最优，基于查询难度和质量阈值动态分配模型和响应数量。

Result: 在真实数据集上，该方法降低成本达60%，且性能下降不到1%。

Conclusion: BEST-Route通过优化模型选择和响应数量分配，有效平衡了成本与性能。

Abstract: Large language models (LLMs) are powerful tools but are often expensive to
deploy at scale. LLM query routing mitigates this by dynamically assigning
queries to models of varying cost and quality to obtain a desired trade-off.
Prior query routing approaches generate only one response from the selected
model and a single response from a small (inexpensive) model was often not good
enough to beat a response from a large (expensive) model due to which they end
up overusing the large model and missing out on potential cost savings.
However, it is well known that for small models, generating multiple responses
and selecting the best can enhance quality while remaining cheaper than a
single large-model response. We leverage this idea to propose BEST-Route, a
novel routing framework that chooses a model and the number of responses to
sample from it based on query difficulty and the quality thresholds.
Experiments on real-world datasets demonstrate that our method reduces costs by
up to 60% with less than 1% performance drop.

</details>


### [106] [Robust Tensor Completion via Gradient Tensor Nulclear L1-L2 Norm for Traffic Data Recovery](https://arxiv.org/abs/2506.22732)
*Hao Shu,Jicheng Li,Tianyv Lei,Lijun Sun*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In real-world scenarios, spatiotemporal traffic data frequently experiences
dual degradation from missing values and noise caused by sensor malfunctions
and communication failures. Therefore, effective data recovery methods are
essential to ensure the reliability of downstream data-driven applications.
while classical tensor completion methods have been widely adopted, they are
incapable of modeling noise, making them unsuitable for complex scenarios
involving simultaneous data missingness and noise interference. Existing Robust
Tensor Completion (RTC) approaches offer potential solutions by separately
modeling the actual tensor data and noise. However, their effectiveness is
often constrained by the over-relaxation of convex rank surrogates and the
suboptimal utilization of local consistency, leading to inadequate model
accuracy. To address these limitations, we first introduce the tensor L1-L2
norm, a novel non-convex tensor rank surrogate that functions as an effective
low-rank representation tool. Leveraging an advanced feature fusion strategy,
we further develop the gradient tensor L1-L2 norm by incorporating the tensor
L1-L2 norm in the gradient domain. By integrating the gradient tensor nuclear
L1-L2 norm into the RTC framework, we propose the Robust Tensor Completion via
Gradient Tensor Nuclear L1-L2 Norm (RTC-GTNLN) model, which not only fully
exploits both global low-rankness and local consistency without trade-off
parameter, but also effectively handles the dual degradation challenges of
missing data and noise in traffic data. Extensive experiments conducted on
multiple real-world traffic datasets demonstrate that the RTC-GTNLN model
consistently outperforms existing state-of-the-art methods in complex recovery
scenarios involving simultaneous missing values and noise.

</details>


### [107] [FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision](https://arxiv.org/abs/2506.22771)
*Jingxiao Ma,Priyadarshini Panda,Sherief Reda*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Backpropagation has been the cornerstone of neural network training for
decades, yet its inefficiencies in time and energy consumption limit its
suitability for resource-constrained edge devices. While low-precision neural
network quantization has been extensively researched to speed up model
inference, its application in training has been less explored. Recently, the
Forward-Forward (FF) algorithm has emerged as a promising alternative to
backpropagation, replacing the backward pass with an additional forward pass.
By avoiding the need to store intermediate activations for backpropagation, FF
can reduce memory footprint, making it well-suited for embedded devices. This
paper presents an INT8 quantized training approach that leverages FF's
layer-by-layer strategy to stabilize gradient quantization. Furthermore, we
propose a novel "look-ahead" scheme to address limitations of FF and improve
model accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board
demonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in
memory usage, while maintaining competitive accuracy compared to the
state-of-the-art.

</details>


### [108] [Multimodal Atmospheric Super-Resolution With Deep Generative Models](https://arxiv.org/abs/2506.22780)
*Dibyajyoti Chakraborty,Haiwen Guan,Jason Stock,Troy Arcomano,Guido Cervone,Romit Maulik*

Key words: 分数扩散模型, 高维动态系统, 超分辨率, 多模态数据, 贝叶斯更新

TL;DR: 论文介绍了基于分数的扩散模型在高维动态系统超分辨率任务中的应用，通过实时低分辨率数据和稀疏传感器测量，实现了高精度状态恢复，并展示了生成模型在多模态数据中的平衡能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索基于分数的扩散模型在数据与模型融合中的新范式，特别是在高维动态系统的超分辨率任务中，利用实时稀疏传感器测量和多模态数据提升生成样本的准确性。

Method: 采用基于分数的扩散模型，通过学习数据的对数概率密度梯度（分数函数）并逆转噪声过程，实现零样本条件生成。结合贝叶斯公式，利用在线数据更新预训练模型的隐式学习分布。

Result: 在高维ERA5大气数据集和IGRA无线电探空仪数据集的实验中，模型能够准确恢复高维状态，并在时空重建中平衡多模态数据的影响。

Conclusion: 基于分数的扩散模型在多模态数据的高维动态系统超分辨率任务中表现出色，能够有效融合多源数据并实现高精度重建。

Abstract: Score-based diffusion modeling is a generative machine learning algorithm
that can be used to sample from complex distributions. They achieve this by
learning a score function, i.e., the gradient of the log-probability density of
the data, and reversing a noising process using the same. Once trained,
score-based diffusion models not only generate new samples but also enable
zero-shot conditioning of the generated samples on observed data. This promises
a novel paradigm for data and model fusion, wherein the implicitly learned
distributions of pretrained score-based diffusion models can be updated given
the availability of online data in a Bayesian formulation. In this article, we
apply such a concept to the super-resolution of a high-dimensional dynamical
system, given the real-time availability of low-resolution and experimentally
observed sparse sensor measurements from multimodal data. Additional analysis
on how score-based sampling can be used for uncertainty estimates is also
provided. Our experiments are performed for a super-resolution task that
generates the ERA5 atmospheric dataset given sparse observations from a
coarse-grained representation of the same and/or from unstructured experimental
observations of the IGRA radiosonde dataset. We demonstrate accurate recovery
of the high dimensional state given multiple sources of low-fidelity
measurements. We also discover that the generative model can balance the
influence of multiple dataset modalities during spatiotemporal reconstructions.

</details>


### [109] [Riemannian-Geometric Fingerprints of Generative Models](https://arxiv.org/abs/2506.22802)
*Hae Jin Song,Laurent Itti*

Key words: 生成模型, 模型归属, 黎曼几何, 指纹识别

TL;DR: 本文提出了一种基于黎曼几何的生成模型指纹定义方法，用于模型归属和合成数据识别。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 生成模型的快速发展导致了对模型归属和指纹识别的需求，以保护IP并验证生成内容的来源。

Method: 采用黎曼几何定义生成模型的指纹，通过数据学习度量并使用测地距离和kNN黎曼质心替换欧式距离和最近邻搜索。

Result: 方法在多个数据集、模型架构和模态上显著提升了模型归属性能，并能泛化到未见过的数据和模型。

Conclusion: 黎曼几何为生成模型指纹提供了一种有效的理论框架和实践方法。

Abstract: Recent breakthroughs and rapid integration of generative models (GMs) have
sparked interest in the problem of model attribution and their fingerprints.
For instance, service providers need reliable methods of authenticating their
models to protect their IP, while users and law enforcement seek to verify the
source of generated content for accountability and trust. In addition, a
growing threat of model collapse is arising, as more model-generated data are
being fed back into sources (e.g., YouTube) that are often harvested for
training ("regurgitative training"), heightening the need to differentiate
synthetic from human data. Yet, a gap still exists in understanding generative
models' fingerprints, we believe, stemming from the lack of a formal framework
that can define, represent, and analyze the fingerprints in a principled way.
To address this gap, we take a geometric approach and propose a new definition
of artifact and fingerprint of GMs using Riemannian geometry, which allows us
to leverage the rich theory of differential geometry. Our new definition
generalizes previous work (Song et al., 2024) to non-Euclidean manifolds by
learning Riemannian metrics from data and replacing the Euclidean distances and
nearest-neighbor search with geodesic distances and kNN-based Riemannian center
of mass. We apply our theory to a new gradient-based algorithm for computing
the fingerprints in practice. Results show that it is more effective in
distinguishing a large array of GMs, spanning across 4 different datasets in 2
different resolutions (64 by 64, 256 by 256), 27 model architectures, and 2
modalities (Vision, Vision-Language). Using our proposed definition
significantly improves the performance on model attribution, as well as a
generalization to unseen datasets, model types, and modalities, suggesting its
practical efficacy.

</details>


### [110] [BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters](https://arxiv.org/abs/2506.22809)
*Cooper Doyle*

Key words: BayesLoRA, 不确定性量化, MC-Dropout, 低秩适配器, 智能体决策

TL;DR: BayesLoRA是一种结合了MC-Dropout的低秩适配器框架，专注于任务特定不确定性量化，为下游工作流提供定制化的置信度估计。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决通用Transformer不确定性方法无法满足下游任务需求的问题，BayesLoRA旨在为智能体在不确定性下的决策提供更可靠的支持。

Method: 通过将MC-Dropout集成到低秩适配器（LoRA）中，数学和实验证明LoRA适配器在微调分布之外具有放大的方差，从而提供可靠的置信度估计。

Result: BayesLoRA能够为智能体的行为调控提供有效的置信度参考，特别是在非微调分布下的决策中。

Conclusion: BayesLoRA为任务特定不确定性量化提供了一种有效框架，增强了智能体在不确定环境下的决策能力。

Abstract: We propose BayesLoRA, a task-specific uncertainty quantification framework
that integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike
general-purpose transformer uncertainty methods, BayesLoRA provides guardrails
tailored to downstream workflows, enabling agents to introspect and modulate
behavior under uncertainty. We demonstrate mathematically and empirically that
LoRA adapters exhibit amplified variance outside fine-tuning distributions,
yielding reliable confidence estimates for agentic decision-making.

</details>


### [111] [Deep learning 40 years of human migration](https://arxiv.org/abs/2506.22821)
*Thomas Gaskin,Guy J. Abel*

Key words: 移民数据、深度循环神经网络、不确定性评估、地理信息、经济因素

TL;DR: 提出了一种新的、详细的全球移民数据集，采用深度循环神经网络方法，显著提升了移民流量估计的准确性和时间分辨率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 提供全面的移民流动数据，填补现有数据不足，支持更精确的移民研究。

Method: 使用深度循环神经网络，结合多种地理、经济、文化等因素，训练模型并生成不确定性评估。

Result: 模型在测试集上显著优于传统方法，提供了高时间分辨率的移民流量估计。

Conclusion: 该方法为移民研究提供了高质量、公开的数据和工具，推动未来研究的发展。

Abstract: We present a novel and detailed dataset on origin-destination annual
migration flows and stocks between 230 countries and regions, spanning the
period from 1990 to the present. Our flow estimates are further disaggregated
by country of birth, providing a comprehensive picture of migration over the
last 43 years. The estimates are obtained by training a deep recurrent neural
network to learn flow patterns from 18 covariates for all countries, including
geographic, economic, cultural, societal, and political information. The
recurrent architecture of the neural network means that the entire past can
influence current migration patterns, allowing us to learn long-range temporal
correlations. By training an ensemble of neural networks and additionally
pushing uncertainty on the covariates through the trained network, we obtain
confidence bounds for all our estimates, allowing researchers to pinpoint the
geographic regions most in need of additional data collection. We validate our
approach on various test sets of unseen data, demonstrating that it
significantly outperforms traditional methods estimating five-year flows while
delivering a significant increase in temporal resolution. The model is fully
open source: all training data, neural network weights, and training code are
made public alongside the migration estimates, providing a valuable resource
for future studies of human migration.

</details>


### [112] [xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection](https://arxiv.org/abs/2506.22837)
*Kamil Faber,Marcin Pietroń,Dominik Żurek,Roberto Corizzo*

Key words: xLSTM, 异常检测, 编码器-解码器, 时间序列, 预测, 重构

TL;DR: 本文提出了xLSTMAD，首次将xLSTM架构应用于异常检测，通过编码器-解码器设计，展示了在多元时间序列数据中的卓越性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管xLSTM在多种任务中表现出色，但其在异常检测中的应用尚未被探索，本文旨在填补这一空白。

Method: xLSTMAD采用编码器-解码器架构，包括预测（xLSTMAD-F）和重构（xLSTMAD-R）两种变体，并研究了MSE和SoftDTW损失函数。

Result: 在TSB-AD-M基准测试中，xLSTMAD表现出色，超越了23种基线方法。

Conclusion: xLSTM在异常检测中展现出强大的建模能力，为相关领域开辟了新方向。

Abstract: The recently proposed xLSTM is a powerful model that leverages expressive
multiplicative gating and residual connections, providing the temporal capacity
needed for long-horizon forecasting and representation learning. This
architecture has demonstrated success in time series forecasting, lossless
compression, and even large-scale language modeling tasks, where its linear
memory footprint and fast inference make it a viable alternative to
Transformers. Despite its growing popularity, no prior work has explored xLSTM
for anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the
first anomaly detection method that integrates a full encoder-decoder xLSTM
architecture, purpose-built for multivariate time series data. Our encoder
processes input sequences to capture historical context, while the decoder is
devised in two separate variants of the method. In the forecasting approach,
the decoder iteratively generates forecasted future values xLSTMAD-F, while the
reconstruction approach reconstructs the input time series from its encoded
counterpart xLSTMAD-R. We investigate the performance of two loss functions:
Mean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider
local reconstruction fidelity and global sequence alignment, respectively. We
evaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17
real-world datasets, using state-of-the-art challenging metrics such as VUS-PR.
In our results, xLSTM showcases state-of-the-art accuracy, outperforming 23
popular anomaly detection baselines. Our paper is the first work revealing the
powerful modeling capabilities of xLSTM for anomaly detection, paving the way
for exciting new developments on this subject. Our code is available at:
https://github.com/Nyderx/xlstmad

</details>


### [113] [Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models](https://arxiv.org/abs/2506.22845)
*Batuhan Hangun,Oguz Altun,Onder Eyecioglu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine
Learning (QML), are emerging as a powerful alternative to classical machine
learning methods. Recent studies have focused on the applicability of QNNs to
various tasks, such as time-series forecasting, prediction, and classification,
across a wide range of applications, including cybersecurity and medical
imaging. With the increased use of smart grids driven by the integration of
renewable energy systems, machine learning plays an important role in
predicting power demand and detecting system disturbances. This study provides
an in-depth investigation of QNNs for predicting the power output of a wind
turbine. We assess the predictive performance and simulation time of six QNN
configurations that are based on the Z Feature Map for data encoding and
varying ansatz structures. Through detailed cross-validation experiments and
tests on an unseen hold-out dataset, we experimentally demonstrate that QNNs
can achieve predictive performance that is competitive with, and in some cases
marginally better than, the benchmarked classical approaches. Our results also
reveal the effects of dataset size and circuit complexity on predictive
performance and simulation time. We believe our findings will offer valuable
insights for researchers in the energy domain who wish to incorporate quantum
machine learning into their work.

</details>


### [114] [Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles](https://arxiv.org/abs/2506.22848)
*Shengcai Liu,Hui Ou-yang,Zhiyuan Wang,Cheng Chen,Qijun Cai,Yew-Soon Ong,Ke Tang*

Key words: 贝叶斯网络, 结构学习, 分治策略, 集成学习, Auto-SLE

TL;DR: 该论文提出了一种结合结构学习集成（SLE）的自动方法（Auto-SLE），用于提高贝叶斯网络（BN）结构学习的准确性和稳定性，并通过实验验证了其在大型BN学习中的优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决在大型数据集上学习BN结构时，现有分治策略（D&D）存在的学习精度不稳定问题。

Method: 提出结构学习集成（SLE）和自动学习SLE的方法（Auto-SLE），并将其整合到分治策略中。

Result: 在涉及10,000变量的数据集上，精度提升30%~225%；在更大规模（如30,000变量）和不同网络特性的数据集上表现良好。

Conclusion: 研究表明，自动学习的SLE在可扩展的BN结构学习中具有巨大潜力。

Abstract: Learning the structure of Bayesian networks (BNs) from data is challenging,
especially for datasets involving a large number of variables. The recently
proposed divide-and-conquer (D\&D) strategies present a promising approach for
learning large BNs. However, they still face a main issue of unstable learning
accuracy across subproblems. In this work, we introduce the idea of employing
structure learning ensemble (SLE), which combines multiple BN structure
learning algorithms, to consistently achieve high learning accuracy. We further
propose an automatic approach called Auto-SLE for learning near-optimal SLEs,
addressing the challenge of manually designing high-quality SLEs. The learned
SLE is then integrated into a D\&D method. Extensive experiments firmly show
the superiority of our method over D\&D methods with single BN structure
learning algorithm in learning large BNs, achieving accuracy improvement
usually by 30\%$\sim$225\% on datasets involving 10,000 variables. Furthermore,
our method generalizes well to datasets with many more (e.g., 30000) variables
and different network characteristics than those present in the training data
for learning the SLE. These results indicate the significant potential of
employing (automatic learning of) SLEs for scalable BN structure learning.

</details>


### [115] [P$^2$U: Progressive Precision Update For Efficient Model Distribution](https://arxiv.org/abs/2506.22871)
*Homayun Afrabandpey,Hamed Rezazadegan Tavakoli*

Key words: 模型分发, 渐进精度更新, 带宽优化, 联邦学习, 边缘计算

TL;DR: 提出了一种名为P$^2$U的渐进精度更新方法，通过传输低精度模型及其与原始高精度模型的差异更新，实现在带宽受限环境下的高效模型分发。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在带宽受限环境中，高效模型分发变得越来越重要。研究旨在解决这一问题。

Method: P$^2$U传输低精度模型及其与高精度模型的差异更新，而非原始高精度模型。

Result: 实验表明，P$^2$U在准确性、带宽使用和延迟之间实现了更好的权衡，且在带宽或启动时间优先时，激进量化（如4位）也可用。

Conclusion: P$^2$U是低资源环境（如联邦学习、边缘计算和物联网）中高效模型分发的有效解决方案。

Abstract: Efficient model distribution is becoming increasingly critical in
bandwidth-constrained environments. In this paper, we propose a simple yet
effective approach called Progressive Precision Update (P$^2$U) to address this
problem. Instead of transmitting the original high-precision model, P$^2$U
transmits a lower-bit precision model, coupled with a model update representing
the difference between the original high-precision model and the transmitted
low precision version. With extensive experiments on various model
architectures, ranging from small models ($1 - 6$ million parameters) to a
large model (more than $100$ million parameters) and using three different data
sets, e.g., chest X-Ray, PASCAL-VOC, and CIFAR-100, we demonstrate that P$^2$U
consistently achieves better tradeoff between accuracy, bandwidth usage and
latency. Moreover, we show that when bandwidth or startup time is the priority,
aggressive quantization (e.g., 4-bit) can be used without severely compromising
performance. These results establish P$^2$U as an effective and practical
solution for scalable and efficient model distribution in low-resource
settings, including federated learning, edge computing, and IoT deployments.
Given that P$^2$U complements existing compression techniques and can be
implemented alongside any compression method, e.g., sparsification,
quantization, pruning, etc., the potential for improvement is even greater.

</details>


### [116] [Interpretable Time Series Autoregression for Periodicity Quantification](https://arxiv.org/abs/2506.22895)
*Xinyu Chen,Vassilis Digalakis Jr,Lijun Ding,Dingyi Zhuang,Jinhua Zhao*

Key words: 稀疏自回归, ℓ0-范数, 混合整数优化, 决策变量剪枝, 多维时间序列

TL;DR: 提出了一种基于机器学习的稀疏自回归框架，通过ℓ0-范数约束增强可解释性，适用于时变和多维时间序列数据。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统时间序列自回归模型在捕获自相关性和周期性时缺乏可解释性和灵活性，本文旨在提出一种可解释且灵活的稀疏自回归模型。

Method: 提出了稀疏自回归框架，包括时变时间序列的混合整数优化(MIO)和多维时间序列的两阶段优化方案，并开发了决策变量剪枝策略加速优化。

Result: 实验表明，提出的加速策略显著提升了MIO求解速度，同时保持了与完整MIO求解器相同的解质量；模型成功应用于共享出行和气候数据，揭示了周期性和动态模式。

Conclusion: 提出的稀疏自回归模型在可解释性和灵活性上表现优异，适用于复杂时间序列分析。

Abstract: Time series autoregression is a classical statistical model for capturing
auto-correlations and identifying temporal patterns such as periodicity and
seasonality. In this work, we propose a novel sparse autoregression framework
from an interpretable machine learning perspective and the model
interpretability for periodicity quantification is reinforced by $\ell_0$-norm
induced sparsity constraints. On the time-varying time series data, we
reformulate the sparse autoregression and convert the involved optimization
problem into a mixed-integer optimization (MIO). To accelerate it, we develop a
subspace pursuit based decision variable pruning (DVP) strategy to reduce the
search space. On the multidimensional time series that involves complicated
spatial and temporal dimensions, we propose a spatially- and time-varying
sparse autoregression model and resolve the corresponding MIO problem by
developing a two-stage optimization scheme. In particular, the proposed scheme
makes the model scalable to large problems even with millions of decision
variables. Empirically, we conduct extensive experiments to evaluate the
proposed models on real-world time series data. First, we demonstrate that the
MIO solver can be drastically accelerated through the DVP strategy, while
maintaining the same solution quality as a full MIO solver. Applying the
time-varying sparse autoregression model to ridesharing trip data, we uncover
both daily and weekly periodicities and reveal long-term changes in regularity
of human mobility. Second, we demonstrate the spatial patterns of yearly
seasonality in climate variable time series such as temperature and
precipitation across the past four decades, and our model allows to discover
dynamic climate patterns and identify climate phenomena such as El Nino in sea
surface temperature.

</details>


### [117] [Missing-Modality-Aware Graph Neural Network for Cancer Classification](https://arxiv.org/abs/2506.22901)
*Sina Tabakhi,Haiping Lu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A key challenge in learning from multimodal biological data is missing
modalities, where all data from some modalities are missing for some patients.
Current fusion methods address this by excluding patients with missing
modalities, imputing missing modalities, or making predictions directly with
partial modalities. However, they often struggle with diverse missing-modality
patterns and the exponential growth of the number of such patterns as the
number of modalities increases. To address these limitations, we propose MAGNET
(Missing-modality-Aware Graph neural NETwork) for direct prediction with
partial modalities, which introduces a patient-modality multi-head attention
mechanism to fuse lower-dimensional modality embeddings based on their
importance and missingness. MAGNET's complexity increases linearly with the
number of modalities while adapting to missing-pattern variability. To generate
predictions, MAGNET further constructs a patient graph with fused multimodal
embeddings as node features and the connectivity determined by the modality
missingness, followed by a conventional graph neural network. Experiments on
three public multiomics datasets for cancer classification, with real-world
instead of artificial missingness, show that MAGNET outperforms the
state-of-the-art fusion methods. The data and code are available at
https://github.com/SinaTabakhi/MAGNET.

</details>


### [118] [Towards Time Series Generation Conditioned on Unstructured Natural Language](https://arxiv.org/abs/2506.22927)
*Jaeyun Woo,Jiseok Lee,Brian Kenji Iwana*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Generative Artificial Intelligence (AI) has rapidly become a powerful tool,
capable of generating various types of data, such as images and text. However,
despite the significant advancement of generative AI, time series generative AI
remains underdeveloped, even though the application of time series is essential
in finance, climate, and numerous fields. In this research, we propose a novel
method of generating time series conditioned on unstructured natural language
descriptions. We use a diffusion model combined with a language model to
generate time series from the text. Through the proposed method, we demonstrate
that time series generation based on natural language is possible. The proposed
method can provide various applications such as custom forecasting, time series
manipulation, data augmentation, and transfer learning. Furthermore, we
construct and propose a new public dataset for time series generation,
consisting of 63,010 time series-description pairs.

</details>


### [119] [Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration](https://arxiv.org/abs/2506.22929)
*Chen Zhang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While deep learning excels in natural image and language processing, its
application to high-dimensional data faces computational challenges due to the
dimensionality curse. Current large-scale data tools focus on business-oriented
descriptive statistics, lacking mathematical statistics support for advanced
analysis. We propose a parallel computation architecture based on space
completeness, decomposing high-dimensional data into dimension-independent
structures for distributed processing. This framework enables seamless
integration of data mining and parallel-optimized machine learning methods,
supporting scientific computations across diverse data types like medical and
natural images within a unified system.

</details>


### [120] [Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language Models](https://arxiv.org/abs/2506.22950)
*Liangyu Wang,Huanyi Xie,Xinhai Wang,Tianjin Huang,Mengdi Li,Di Wang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Group-based reinforcement learning algorithms such as Group Reward Policy
Optimization (GRPO) have proven effective for fine-tuning large language models
(LLMs) with human feedback. However, generating and storing multiple responses
per prompt incurs substantial memory overhead, especially as the sample group
size increases, limiting scalability under constrained hardware.
  We propose Infinite Sampling, a framework that enables efficient and stable
GRPO training by decoupling group size from GPU memory usage. It consists of:
(1) micro sampling groups that decompose large groups into memory-feasible
rounds; (2) continuous sampling that interleaves generation across groups to
improve utilization; and (3) a length-aware scheduler combining
token-conditioned sequence length prediction with a two-stage plan: global
grouping via FPTAS and runtime refill via SJF.
  Experiments show that our Micro Sampling Groups reduce peak memory usage by
over 50% compared to full-group decoding (e.g., from 21.55 GB to 10.64 GB on
Qwen3-1.7B). Building on this, Infinite Sampling improves throughput by over
25% compared to the naive micro sampling group method, reducing decoding steps
while maintaining full-length completions and memory usage. Our hybrid
scheduling ensures efficient and stable GRPO training with larger groups under
realistic GPU memory constraints.

</details>


### [121] [Cybersecurity-Focused Anomaly Detection in Connected Autonomous Vehicles Using Machine Learning](https://arxiv.org/abs/2506.22984)
*Prathyush Kumar Reddy Lebaku,Lu Gao,Yunpeng Zhang,Zhixia Li,Yongxin Liu,Tanvir Arafin*

Key words: 车联网, 自动驾驶, 异常检测, 堆叠LSTM, 随机森林

TL;DR: 该研究通过模拟车辆行为生成数据集，利用堆叠LSTM和随机森林模型检测车联网自动驾驶环境中的异常。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 确保车联网自动驾驶车辆的安全性和可靠性，应对传感器故障、网络攻击和突发环境干扰。

Method: 使用堆叠LSTM模型捕捉时间依赖性和序列异常，结合随机森林模型提升性能和可解释性。

Result: 随机森林模型R2为0.9830，MAE为5.746；堆叠LSTM模型R2为0.9998，MAE为82.425，均有效识别异常。

Conclusion: 两种模型在预测车辆轨迹和检测异常方面表现优异，显著提升驾驶安全性。

Abstract: Anomaly detection in connected autonomous vehicles (CAVs) is crucial for
maintaining safe and reliable transportation networks, as CAVs can be
susceptible to sensor malfunctions, cyber-attacks, and unexpected environmental
disruptions. This study explores an anomaly detection approach by simulating
vehicle behavior, generating a dataset that represents typical and atypical
vehicular interactions. The dataset includes time-series data of position,
speed, and acceleration for multiple connected autonomous vehicles. We utilized
machine learning models to effectively identify abnormal driving patterns.
First, we applied a stacked Long Short-Term Memory (LSTM) model to capture
temporal dependencies and sequence-based anomalies. The stacked LSTM model
processed the sequential data to learn standard driving behaviors.
Additionally, we deployed a Random Forest model to support anomaly detection by
offering ensemble-based predictions, which enhanced model interpretability and
performance. The Random Forest model achieved an R2 of 0.9830, MAE of 5.746,
and a 95th percentile anomaly threshold of 14.18, while the stacked LSTM model
attained an R2 of 0.9998, MAE of 82.425, and a 95th percentile anomaly
threshold of 265.63. These results demonstrate the models' effectiveness in
accurately predicting vehicle trajectories and detecting anomalies in
autonomous driving scenarios.

</details>


### [122] [Kernel Outlier Detection](https://arxiv.org/abs/2506.22994)
*Can Hakan Dağıdır,Mia Hubert,Peter J. Rousseeuw*

Key words: 异常检测, 高维数据, 核变换, 投影追踪, 集成方向

TL;DR: 提出了一种名为核异常检测（KOD）的新方法，用于解决高维数据中的异常检测问题，克服了现有方法对分布假设或难调超参数的依赖。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的异常检测方法在高维数据中面临挑战，如依赖分布假设或超参数难以调整。KOD旨在克服这些限制。

Method: KOD首先进行核变换，然后采用投影追踪方法，包括新的方向集成和结果组合方式。

Result: 在三个小型数据集和四个大型基准数据集上的实验验证了KOD的有效性。

Conclusion: KOD提供了一种灵活且轻量级的异常检测方法，适用于高维数据。

Abstract: A new anomaly detection method called kernel outlier detection (KOD) is
proposed. It is designed to address challenges of outlier detection in
high-dimensional settings. The aim is to overcome limitations of existing
methods, such as dependence on distributional assumptions or on hyperparameters
that are hard to tune. KOD starts with a kernel transformation, followed by a
projection pursuit approach. Its novelties include a new ensemble of directions
to search over, and a new way to combine results of different direction types.
This provides a flexible and lightweight approach for outlier detection. Our
empirical evaluations illustrate the effectiveness of KOD on three small
datasets with challenging structures, and on four large benchmark datasets.

</details>


### [123] [A Reinforcement Learning Approach for Optimal Control in Microgrids](https://arxiv.org/abs/2506.22995)
*Davide Salaorni,Federico Bianchi,Francesco Trovò,Marcello Restelli*

Key words: 可再生能源、微电网、强化学习、数字孪生、能源管理

TL;DR: 本文提出了一种基于强化学习的微电网能量管理优化方法，通过结合历史数据和数字孪生技术，显著提升了微电网的能量交易和存储策略。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着可再生能源的集成增加，传统的电网管理方法已无法适应分散式能源生产和消费的新需求，亟需创新的管理方法。

Method: 提出了一种强化学习（RL）方法，利用历史能量生产、消费和市场数据，结合数字孪生（DT）模拟储能系统动态，优化微电网的能量管理。

Result: 实验结果表明，该方法在意大利电网的实际数据中表现优异，超越了基于规则的方法和现有的RL基准。

Conclusion: 本文提出的RL策略为智能微电网管理提供了可靠且高效的解决方案。

Abstract: The increasing integration of renewable energy sources (RESs) is transforming
traditional power grid networks, which require new approaches for managing
decentralized energy production and consumption. Microgrids (MGs) provide a
promising solution by enabling localized control over energy generation,
storage, and distribution. This paper presents a novel reinforcement learning
(RL)-based methodology for optimizing microgrid energy management.
Specifically, we propose an RL agent that learns optimal energy trading and
storage policies by leveraging historical data on energy production,
consumption, and market prices. A digital twin (DT) is used to simulate the
energy storage system dynamics, incorporating degradation factors to ensure a
realistic emulation of the analysed setting. Our approach is validated through
an experimental campaign using real-world data from a power grid located in the
Italian territory. The results indicate that the proposed RL-based strategy
outperforms rule-based methods and existing RL benchmarks, offering a robust
solution for intelligent microgrid management.

</details>


### [124] [BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs](https://arxiv.org/abs/2506.23024)
*Jerry Liu,Yasa Baig,Denise Hui Jean Lee,Rajat Vadiraj Dwaraknath,Atri Rudra,Chris Ré*

Key words: PINNs, Barycentric Weight Layer, PDE, MLP, 精度提升

TL;DR: 物理学神经网络（PINNs）通过引入Barycentric Weight Layer（BWLer）解决了传统多层感知器（MLP）在求解偏微分方程（PDEs）时的精度限制，显著提升了精度并揭示了PDE损失的平衡关系。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管PINNs为PDE求解提供了灵活的方法，但其精度仍远低于科学任务所需。研究旨在探索精度限制源于PDE的条件不良还是MLP架构。

Method: 引入BWLer，通过重心多项式插值建模PDE解，可附加于MLP（BWLer-hat）或完全替代（显式BWLer）。使用光谱导数和预条件处理优化训练。

Result: 在五个基准PDE中，BWLer将RMSE提升高达30x（对流）、10x（反应）和1800x（波动方程）；显式BWLer接近机器精度，性能提升达百亿倍。

Conclusion: BWLer为PINNs提供了结合传统谱方法精度的实用路径，解决了MLP的固有精度限制。

Abstract: Physics-informed neural networks (PINNs) offer a flexible way to solve
partial differential equations (PDEs) with machine learning, yet they still
fall well short of the machine-precision accuracy many scientific tasks demand.
In this work, we investigate whether the precision ceiling comes from the
ill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)
architecture. We introduce the Barycentric Weight Layer (BWLer), which models
the PDE solution through barycentric polynomial interpolation. A BWLer can be
added on top of an existing MLP (a BWLer-hat) or replace it completely
(explicit BWLer), cleanly separating how we represent the solution from how we
take derivatives for the PDE loss. Using BWLer, we identify fundamental
precision limitations within the MLP: on a simple 1-D interpolation task, even
MLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above
float64 machine precision -- before any PDE terms are added. In PDE learning,
adding a BWLer lifts this ceiling and exposes a tradeoff between achievable
accuracy and the conditioning of the PDE loss. For linear PDEs we fully
characterize this tradeoff with an explicit error decomposition and navigate it
during training with spectral derivatives and preconditioning. Across five
benchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for
convection, 10x for reaction, and 1800x for wave equations while remaining
compatible with first-order optimizers. Replacing the MLP entirely lets an
explicit BWLer reach near-machine-precision on convection, reaction, and wave
problems (up to 10 billion times better than prior results) and match the
performance of standard PINNs on stiff Burgers' and irregular-geometry Poisson
problems. Together, these findings point to a practical path for combining the
flexibility of PINNs with the precision of classical spectral solvers.

</details>


### [125] [Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models](https://arxiv.org/abs/2506.23025)
*Tejas Vaidhya,Ayush Kaushal,Vineet Jain,Francis Couture Harpin,Prashant Shishodia,Majid Behbahani,Yuriy Nevmyvaka,Irina Rish*

Key words: large language models, TriLMs, quantization-aware training, inference efficiency

TL;DR: 论文研究了三元语言模型（TriLMs），通过量化感知训练显著减少内存需求，并提出了2位和1.6位压缩方案及GPU内核TriRun，提升推理效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大型语言模型（LLMs）的推理效率问题因GPU内存带宽与容量未同步提升而成为瓶颈，研究者希望通过TriLMs解决这一问题。

Method: 采用量化感知训练和三元语言模型设计，分析了TriLMs的可扩展性，并提出了2位和1.6位权重压缩方案，开发了GPU内核TriRun。

Result: TriLMs在1.2万亿标记数据上表现稳定，推理效率提升显著（最高5倍），并发布了Spectra-1.1模型和TriRun内核。

Conclusion: TriLMs为高效LLMs的构建与部署奠定了基础，成果对研究社区具有重要价值。

Abstract: Large language models (LLMs) are increasingly used across research and
industry applications, yet their inference efficiency remains a significant
challenge. As the computational power of modern GPU architectures continuously
improves, their memory bandwidth and capacity have not scaled proportionally,
creating a critical bottleneck during inference. To address this, we
investigate ternary language models (TriLMs) that employ quantization-aware
training to significantly reduce memory requirements. We first analyze the
scalability of TriLMs by conducting a scaling law analysis, revealing that
TriLMs benefit more from increasing training data than from scaling model
parameters. Based on this observation, we introduce Spectra-1.1, an open suite
of TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained
performance gains at scale. Furthermore, to improve inference efficiency, we
propose novel 2-bit and 1.6-bit packing schemes for ternary weights, which
demonstrate accelerated inference across various CPU architectures. Also,
building on the 2-bit packing, we develop a GPU kernel called TriRun that
accelerates end-to-end model inference by up to 5 times compared to
floating-point baselines. To encourage further exploration and development of
TriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels.
Overall, our work lays the foundation for building and deploying efficient
LLMs, providing a valuable resource for the research community.

</details>


### [126] [Feature-Wise Mixing for Mitigating Contextual Bias in Predictive Supervised Learning](https://arxiv.org/abs/2506.23033)
*Yash Vardhan Tomar*

Key words: 机器学习, 偏见缓解, 特征混合, 公平性, 预测性能

TL;DR: 本文提出了一种特征级混合框架来缓解机器学习模型中的偏见问题，通过在不同上下文数据集中重新分配特征表示，显著减少了偏见并提高了预测性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于机器学习模型中存在的偏见会导致不公平结果，现有方法存在局限，因此需要一种更有效且具扩展性的解决方案。

Method: 采用特征级混合框架，通过重新分配特征表示来减轻偏见，并使用交叉验证和偏见敏感损失函数评估效果。

Result: 特征级混合平均减少了43.35%的偏见，并在所有分类器中显著降低了MSE，优于现有方法如SMOTE。

Conclusion: 特征级混合是一种高效且无需显式识别偏见属性的方法，未来可探索其在现实场景中的应用。

Abstract: Bias in predictive machine learning (ML) models is a fundamental challenge
due to the skewed or unfair outcomes produced by biased models. Existing
mitigation strategies rely on either post-hoc corrections or rigid constraints.
However, emerging research claims that these techniques can limit scalability
and reduce generalizability. To address this, this paper introduces a
feature-wise mixing framework to mitigate contextual bias. This was done by
redistributing feature representations across multiple contextual datasets. To
assess feature-wise mixing's effectiveness, four ML classifiers were trained
using cross-validation and evaluated with bias-sensitive loss functions,
including disparity metrics and mean squared error (MSE), which served as a
standard measure of predictive performance. The proposed method achieved an
average bias reduction of 43.35% and a statistically significant decrease in
MSE across all classifiers trained on mixed datasets. Additionally,
benchmarking against established bias mitigation techniques found that
feature-wise mixing consistently outperformed SMOTE oversampling and
demonstrated competitive effectiveness without requiring explicit bias
attribute identification. Feature-wise mixing efficiently avoids the
computational overhead typically associated with fairness-aware learning
algorithms. Future work could explore applying feature-wise mixing for
real-world fields where accurate predictions are necessary.

</details>


### [127] [Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress](https://arxiv.org/abs/2506.23036)
*Zain ul Abdeen,Ming Jin*

Key words: 强化学习, 参数鲁棒性, 突触过滤, 对抗性攻击, 反脆弱性

TL;DR: 该论文通过系统地分析强化学习策略在网络内部和外部压力下的参数表现，提出了一种评估参数鲁棒性的方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 受神经科学中突触可塑性的启发，研究旨在评估强化学习策略参数的鲁棒性和适应性，并开发分类方法。

Method: 通过突触过滤引入内部压力，通过对抗性攻击引入外部压力，将参数分类为脆弱、鲁棒或反脆弱，并定义参数评分量化其特性。

Result: 研究结果表明，存在反脆弱参数能在压力下提升策略性能，验证了目标过滤技术对改善强化学习策略适应性的潜力。

Conclusion: 研究为设计鲁棒和反脆弱强化学习系统提供了基础，未来可进一步推进相关技术的发展。

Abstract: This paper explores Reinforcement learning (RL) policy robustness by
systematically analyzing network parameters under internal and external
stresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering
introduces internal stress by selectively perturbing parameters, while
adversarial attacks apply external stress through modified agent observations.
This dual approach enables the classification of parameters as fragile, robust,
or antifragile, based on their influence on policy performance in clean and
adversarial settings. Parameter scores are defined to quantify these
characteristics, and the framework is validated on PPO-trained agents in Mujoco
continuous control environments. The results highlight the presence of
antifragile parameters that enhance policy performance under stress,
demonstrating the potential of targeted filtering techniques to improve RL
policy adaptability. These insights provide a foundation for future
advancements in the design of robust and antifragile RL systems.

</details>


### [128] [ReMem: Mutual Information-Aware Fine-tuning of Pretrained Vision Transformers for Effective Knowledge Distillation](https://arxiv.org/abs/2506.23041)
*Chengyu Dong,Huan Gui,Noveen Sachdeva,Long Jin,Ke Yin,Jingbo Shang,Lichan Hong,Ed H. Chi,Zhe Zhao*

Key words: 知识蒸馏, Vision Transformers, 互信息, MLP块, 微调

TL;DR: 知识蒸馏从预训练的视觉表示模型中提取知识是提升小型任务特定模型的有效方法。但当从大规模预训练的强模型蒸馏时，效果显著下降。本文通过微调Vision Transformers（ViTs）提出改进方法，利用互信息优化和MLP块权重调整提升蒸馏效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决从大规模预训练的强模型（如ViTs）进行知识蒸馏时效果下降的问题。

Method: 提出互信息感知优化方法，并在小样本或高度不平衡数据集下，采用MLP块重加权策略。

Result: 该方法提升了小型学生模型从强预训练模型中获益的能力。

Conclusion: 通过互信息优化和MLP块调整，有效提升了知识蒸馏的效果。

Abstract: Knowledge distillation from pretrained visual representation models offers an
effective approach to improve small, task-specific production models. However,
the effectiveness of such knowledge transfer drops significantly when
distilling from strong models that are pretrained in a large scale. In this
paper, we address this challenge for pretrained Vision Transformers (ViTs) by
exploring methods to fine-tune them for more effective knowledge transfer.
Motivated by the connection between mutual information and distillation
effectiveness, we propose to employ mutual information-aware optimization
during finetuning. For small or highly-imbalanced downstream datasets where
such optimization becomes less effective, we introduce a simple yet effective
heuristic of reweighting MLP blocks. This approach is inspired by our
observation that top MLP blocks are primarily responsible for mutual
information loss. Our method enables small student models to benefit from those
pretrained models among the strongest.

</details>


### [129] [Double-Diffusion: Diffusion Conditioned Diffusion Probabilistic Model For Air Quality Prediction](https://arxiv.org/abs/2506.23053)
*Hanlin Dong,Arian Prabowo,Hao Xue,Flora D. Salim*

Key words: 空气质量预测、扩散概率模型、物理规律、随机性

TL;DR: 提出了一种名为Double-Diffusion的新型扩散概率模型，利用已知物理规律指导空气质量的随机性预测，显著提升了预测效率和准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 空气质量的时空复杂性和不确定性使其预测具有挑战性，现有模型在确定性与不确定性之间的平衡点仍未解决。

Method: 采用双扩散概率模型，结合物理规律作为条件生成方法，并引入图像修复中的采样策略和新去噪架构。

Result: Double-Diffusion在两个真实数据集上表现最佳，推理时间减少30%-50%，CRPS提高3%-12%。

Conclusion: 该方法首次将物理规律作为条件生成方法应用于空气质量预测，显著提升了模型性能。

Abstract: Air quality prediction is a challenging forecasting task due to its
spatio-temporal complexity and the inherent dynamics as well as uncertainty.
Most of the current models handle these two challenges by applying Graph Neural
Networks or known physics principles, and quantifying stochasticity through
probabilistic networks like Diffusion models. Nevertheless, finding the right
balancing point between the certainties and uncertainties remains an open
question. Therefore, we propose Double-Diffusion, a novel diffusion
probabilistic model that harnesses the power of known physics to guide air
quality forecasting with stochasticity. To the best of our knowledge, while
precedents have been made of using conditional diffusion models to predict air
pollution, this is the first attempt to use physics as a conditional generative
approach for air quality prediction. Along with a sampling strategy adopted
from image restoration and a new denoiser architecture, Double-Diffusion ranks
first in most evaluation scenarios across two real-life datasets compared with
other probabilistic models, it also cuts inference time by 50% to 30% while
enjoying an increase between 3-12% in Continuous Ranked Probabilistic Score
(CRPS).

</details>


### [130] [ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks](https://arxiv.org/abs/2402.09146)
*Muhammad Kashif,Muhammad Shafique*

Key words: 量子卷积神经网络, 残差学习, 梯度优化, 量子深度学习

TL;DR: 提出了一种增强量子卷积神经网络（QuNNs）性能的新框架，通过引入可训练的量子卷积层并解决相关挑战，显著提升了网络的灵活性和潜力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统量子卷积层虽然有助于特征提取，但缺乏适应性。研究旨在通过可训练层和残差学习解决这一问题。

Method: 提出了利用残差学习的ResQuNNs架构，通过在量子卷积层间添加残差块，改善了梯度流动和训练性能。

Result: 通过实验确定了残差块的最佳配置，显著提升了QuNNs的训练效率和性能。

Conclusion: 研究为量子深度学习的发展提供了新方向，具有理论和实际应用价值。

Abstract: In this paper, we present a novel framework for enhancing the performance of
Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional
layers and addressing the critical challenges associated with them. Traditional
quanvolutional layers, although beneficial for feature extraction, have largely
been static, offering limited adaptability. Unlike state-of-the-art, our
research overcomes this limitation by enabling training within these layers,
significantly increasing the flexibility and potential of QuNNs. However, the
introduction of multiple trainable quanvolutional layers induces complexities
in gradient-based optimization, primarily due to the difficulty in accessing
gradients across these layers. To resolve this, we propose a novel
architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging
the concept of residual learning, which facilitates the flow of gradients by
adding skip connections between layers. By inserting residual blocks between
quanvolutional layers, we ensure enhanced gradient access throughout the
network, leading to improved training performance. Moreover, we provide
empirical evidence on the strategic placement of these residual blocks within
QuNNs. Through extensive experimentation, we identify an efficient
configuration of residual blocks, which enables gradients across all the layers
in the network that eventually results in efficient training. Our findings
suggest that the precise location of residual blocks plays a crucial role in
maximizing the performance gains in QuNNs. Our results mark a substantial step
forward in the evolution of quantum deep learning, offering new avenues for
both theoretical development and practical quantum computing applications.

</details>


### [131] [Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis](https://arxiv.org/abs/2506.23055)
*Hiro Taiyo Hamada,Ippei Fujisawa,Genji Kawakita,Yuki Yamada*

Key words: 大型语言模型, 心理问卷, 概念对齐, GPT-4, AI可解释性

TL;DR: 该研究开发了一种定量框架，通过43个标准化心理问卷评估大型语言模型（如GPT-4）与人类心理维度的概念对齐度，发现GPT-4在分类准确性和语义相似性上显著优于其他模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探究大型语言模型是否能够准确内化人类心理概念，以便开发更可解释的AI系统。

Method: 使用43个标准化心理问卷，通过层级聚类和相似性分析评估模型对问卷项目的重建和分类能力。

Result: GPT-4的分类准确率（66.2%）显著高于GPT-3.5（55.9%）和BERT（48.1%），且其语义相似性与人类反应的相关性较高。

Conclusion: 现代大型语言模型能够以可测量的准确性近似人类心理概念，为开发更可解释的AI系统提供了见解。

Abstract: Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities
in producing human-like text. However, it is unclear how accurately these
models internalize concepts that shape human thought and behavior. Here, we
developed a quantitative framework to assess concept alignment between LLMs and
human psychological dimensions using 43 standardized psychological
questionnaires, selected for their established validity in measuring distinct
psychological constructs. Our method evaluates how accurately language models
reconstruct and classify questionnaire items through pairwise similarity
analysis. We compared resulting cluster structures with the original
categorical labels using hierarchical clustering. A GPT-4 model achieved
superior classification accuracy (66.2\%), significantly outperforming GPT-3.5
(55.9\%) and BERT (48.1\%), all exceeding random baseline performance (31.9\%).
We also demonstrated that the estimated semantic similarity from GPT-4 is
associated with Pearson's correlation coefficients of human responses in
multiple psychological questionnaires. This framework provides a novel approach
to evaluate the alignment of the human-LLM concept and identify potential
representational biases. Our findings demonstrate that modern LLMs can
approximate human psychological constructs with measurable accuracy, offering
insights for developing more interpretable AI systems.

</details>


### [132] [Curious Causality-Seeking Agents Learn Meta Causal World](https://arxiv.org/abs/2506.23068)
*Zhiyu Zhao,Haoxuan Li,Haifeng Zhang,Jun Wang,Francesco Faccio,Jürgen Schmidhuber,Mengyue Yang*

Key words: 元因果图、世界模型、因果关系、好奇心驱动、动态因果机制

TL;DR: 该论文提出了一种基于元因果图的世界模型，能够高效编码因果结构在不同潜在世界状态间的转换规则，并通过好奇心驱动的策略学习和优化。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现实环境中因果机制可能因策略或状态的细微变化而改变，传统的固定因果规则假设难以应对这种变化，因此需要一种能动态捕捉因果机制转换的模型。

Method: 引入了元因果图作为世界模型，由多个因果子图组成，每个子图由潜在状态空间中的元状态触发。通过因果关系寻求代理，识别元状态、发现因果关系并迭代优化模型。

Result: 在合成任务和机器人手臂操作任务中，该方法能有效捕捉因果动态变化，并在未见过的环境中表现出良好的泛化能力。

Conclusion: 元因果图和好奇心驱动的方法为解决动态因果机制问题提供了一种有效途径，适用于复杂和变化的环境。

Abstract: When building a world model, a common assumption is that the environment has
a single, unchanging underlying causal rule, like applying Newton's laws to
every situation. In reality, what appears as a drifting causal mechanism is
often the manifestation of a fixed underlying mechanism seen through a narrow
observational window. This brings about a problem that, when building a world
model, even subtle shifts in policy or environment states can alter the very
observed causal mechanisms. In this work, we introduce the \textbf{Meta-Causal
Graph} as world models, a minimal unified representation that efficiently
encodes the transformation rules governing how causal structures shift across
different latent world states. A single Meta-Causal Graph is composed of
multiple causal subgraphs, each triggered by meta state, which is in the latent
state space. Building on this representation, we introduce a
\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta
states that trigger each subgraph, (2) discover the corresponding causal
relationships by agent curiosity-driven intervention policy, and (3)
iteratively refine the Meta-Causal Graph through ongoing curiosity-driven
exploration and agent experiences. Experiments on both synthetic tasks and a
challenging robot arm manipulation task demonstrate that our method robustly
captures shifts in causal dynamics and generalizes effectively to previously
unseen contexts.

</details>


### [133] [Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings](https://arxiv.org/abs/2506.23145)
*Shahad Hardan,Darya Taratynova,Abdelmajid Essofi,Karthik Nandakumar,Mohammad Yaqub*

Key words: 隐私保护,多模态数据,机器遗忘,医疗AI

TL;DR: 论文提出Forget-MI，一种用于多模态医疗数据的机器遗忘方法，通过损失函数和扰动技术，有效移除指定数据的同时保持模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在医疗AI中，隐私保护至关重要，现有方法难以从多模态模型中移除敏感数据。

Method: 提出Forget-MI方法，利用损失函数和扰动技术遗忘特定数据，同时保留剩余知识。

Result: Forget-MI在遗忘数据集上的性能优于现有方法，减少MIA攻击风险，测试集性能与原始模型相当。

Conclusion: Forget-MI为多模态医疗数据的隐私保护提供了高效解决方案。

Abstract: Privacy preservation in AI is crucial, especially in healthcare, where models
rely on sensitive patient data. In the emerging field of machine unlearning,
existing methodologies struggle to remove patient data from trained multimodal
architectures, which are widely used in healthcare. We propose Forget-MI, a
novel machine unlearning method for multimodal medical data, by establishing
loss functions and perturbation techniques. Our approach unlearns unimodal and
joint representations of the data requested to be forgotten while preserving
knowledge from the remaining data and maintaining comparable performance to the
original model. We evaluate our results using performance on the forget
dataset, performance on the test dataset, and Membership Inference Attack
(MIA), which measures the attacker's ability to distinguish the forget dataset
from the training dataset. Our model outperforms the existing approaches that
aim to reduce MIA and the performance on the forget dataset while keeping an
equivalent performance on the test set. Specifically, our approach reduces MIA
by 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305,
respectively. Additionally, our performance on the test set matches that of the
retrained model, while allowing forgetting. Code is available at
https://github.com/BioMedIA-MBZUAI/Forget-MI.git

</details>


### [134] [maneuverRecognition -- A Python package for Timeseries Classification in the domain of Vehicle Telematics](https://arxiv.org/abs/2506.23147)
*Jonathan Schuster,Fabian Transchel*

Key words: 驾驶动作识别,时间序列分类,LSTM,Python工具包,车辆远程信息处理

TL;DR: 论文提出了一套用于驾驶动作识别的Python工具包maneuverRecognition，支持数据预处理、建模和评估，旨在提升驾驶行为分类的效率和准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 驾驶动作识别的自动化需求，用于个性化保险、提升道路安全、减少事故及成本，并支持环保驾驶。

Method: 开发了maneuverRecognition包，包含预处理、建模和评估功能，并提供了基于LSTM的网络结构。

Result: 通过智能手机传感器记录的实车数据验证了工具包的有效性。

Conclusion: 该工具包为驾驶动作识别提供了实用且可扩展的解决方案。

Abstract: In the domain of vehicle telematics the automated recognition of driving
maneuvers is used to classify and evaluate driving behaviour. This not only
serves as a component to enhance the personalization of insurance policies, but
also to increase road safety, reduce accidents and the associated costs as well
as to reduce fuel consumption and support environmentally friendly driving. In
this context maneuver recognition technically requires a continuous application
of time series classification which poses special challenges to the transfer,
preprocessing and storage of telematic sensor data, the training of predictive
models, and the prediction itself. Although much research has been done in the
field of gathering relevant data or regarding the methods to build predictive
models for the task of maneuver recognition, there is a practical need for
python packages and functions that allow to quickly transform data into the
required structure as well as to build and evaluate such models. The
maneuverRecognition package was therefore developed to provide the necessary
functions for preprocessing, modelling and evaluation and also includes a ready
to use LSTM based network structure that can be modified. The implementation of
the package is demonstrated using real driving data of three different persons
recorded via smartphone sensors.

</details>


### [135] [Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes](https://arxiv.org/abs/2506.23165)
*David Bossens,Atsushi Nitanda*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Safety is an essential requirement for reinforcement learning systems. The
newly emerging framework of robust constrained Markov decision processes allows
learning policies that satisfy long-term constraints while providing guarantees
under epistemic uncertainty. This paper presents mirror descent policy
optimisation for robust constrained Markov decision processes (RCMDPs), making
use of policy gradient techniques to optimise both the policy (as a maximiser)
and the transition kernel (as an adversarial minimiser) on the Lagrangian
representing a constrained MDP. In the oracle-based RCMDP setting, we obtain an
$\mathcal{O}\left(\frac{1}{T}\right)$ convergence rate for the squared distance
as a Bregman divergence, and an $\mathcal{O}\left(e^{-T}\right)$ convergence
rate for entropy-regularised objectives. In the sample-based RCMDP setting, we
obtain an $\tilde{\mathcal{O}}\left(\frac{1}{T^{1/3}}\right)$ convergence rate.
Experiments confirm the benefits of mirror descent policy optimisation in
constrained and unconstrained optimisation, and significant improvements are
observed in robustness tests when compared to baseline policy optimisation
algorithms.

</details>


### [136] [Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data](https://arxiv.org/abs/2506.23174)
*Chen Gong,Bo Liang,Wei Gao,Chenren Xu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Generative models have gained significant attention for their ability to
produce realistic synthetic data that supplements the quantity of real-world
datasets. While recent studies show performance improvements in wireless
sensing tasks by incorporating all synthetic data into training sets, the
quality of synthetic data remains unpredictable and the resulting performance
gains are not guaranteed. To address this gap, we propose tractable and
generalizable metrics to quantify quality attributes of synthetic data -
affinity and diversity. Our assessment reveals prevalent affinity limitation in
current wireless synthetic data, leading to mislabeled data and degraded task
performance. We attribute the quality limitation to generative models' lack of
awareness of untrained conditions and domain-specific processing. To mitigate
these issues, we introduce SynCheck, a quality-guided synthetic data
utilization scheme that refines synthetic data quality during task model
training. Our evaluation demonstrates that SynCheck consistently outperforms
quality-oblivious utilization of synthetic data, and achieves 4.3% performance
improvement even when the previous utilization degrades performance by 13.4%.

</details>


### [137] [Attribution assignment for deep-generative sequence models enables interpretability analysis using positive-only data](https://arxiv.org/abs/2506.23182)
*Robert Frank,Michael Widrich,Rahmad Akbar,Günter Klambauer,Geir Kjetil Sandve,Philippe A. Robert,Victor Greiff*

Key words: 生成模型, 归因方法, 生物序列设计, GAMA, 抗体设计

TL;DR: 该论文提出了一种名为GAMA的生成式归因方法，用于解决生成模型中缺乏归因方法的问题，从而能够在不依赖负标签数据的情况下提供可解释的生物学见解。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 生成式机器学习在生物序列设计中有巨大潜力，但由于缺乏归因方法，难以从中提取可解释的见解。本文旨在填补这一空白。

Method: 开发了一种基于集成梯度的自回归生成模型归因方法（GAMA），并通过合成数据集和实验数据验证其有效性。

Result: GAMA能够准确恢复生物学相关特征，并在抗体-抗原结合数据中证明了其实用性。

Conclusion: GAMA提供了一种无需负标签数据的生成模型解释和验证方法，增强了生成式设计的可靠性和透明度。

Abstract: Generative machine learning models offer a powerful framework for therapeutic
design by efficiently exploring large spaces of biological sequences enriched
for desirable properties. Unlike supervised learning methods, which require
both positive and negative labeled data, generative models such as LSTMs can be
trained solely on positively labeled sequences, for example, high-affinity
antibodies. This is particularly advantageous in biological settings where
negative data are scarce, unreliable, or biologically ill-defined. However, the
lack of attribution methods for generative models has hindered the ability to
extract interpretable biological insights from such models. To address this
gap, we developed Generative Attribution Metric Analysis (GAMA), an attribution
method for autoregressive generative models based on Integrated Gradients. We
assessed GAMA using synthetic datasets with known ground truths to characterize
its statistical behavior and validate its ability to recover biologically
relevant features. We further demonstrated the utility of GAMA by applying it
to experimental antibody-antigen binding data. GAMA enables model
interpretability and the validation of generative sequence design strategies
without the need for negative training data.

</details>


### [138] [Efficient Algorithms for Learning and Compressing Monophonic Halfspaces in Graphs](https://arxiv.org/abs/2506.23186)
*Marco Bressan,Victor Chepoi,Emmanuel Esposito,Maximilian Thiessen*

Key words: 图半空间、单音半空间、分解定理、机器学习、学习算法

TL;DR: 该论文研究了图顶点的单音半空间概念，提出了一种基于2-可满足性的分解定理，为教学、主动和在线学习提供了高效且接近最优的算法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究图顶点上的凸性概念及其半空间定义，解决机器学习中相关学习问题，尤其是与测地半空间对比中的开放性问题。

Method: 通过单音半空间的分解定理，将半空间表示为不相交的顶点子集，并应用于学习算法设计。

Result: 实现了多项式时间内的经验风险最小化算法，以及高效、稳定和适当的样本压缩方案。

Conclusion: 单音半空间在可实现的PAC设置中是可学习的，且线性错误率为1/ε，与测地半空间形成鲜明对比。

Abstract: Abstract notions of convexity over the vertices of a graph, and corresponding
notions of halfspaces, have recently gained attention from the machine learning
community. In this work we study monophonic halfspaces, a notion of graph
halfspaces defined through closure under induced paths. Our main result is a
$2$-satisfiability based decomposition theorem, which allows one to represent
monophonic halfspaces as a disjoint union of certain vertex subsets. Using this
decomposition, we achieve efficient and (nearly) optimal algorithms for various
learning problems, such as teaching, active, and online learning. Most notably,
we obtain a polynomial-time algorithm for empirical risk minimization.
Independently of the decomposition theorem, we obtain an efficient, stable, and
proper sample compression scheme. This makes monophonic halfspaces efficiently
learnable with proper learners and linear error rate $1/\varepsilon$ in the
realizable PAC setting. Our results answer open questions from the literature,
and show a stark contrast with geodesic halfspaces, for which most of the said
learning problems are NP-hard.

</details>


### [139] [External Data-Enhanced Meta-Representation for Adaptive Probabilistic Load Forecasting](https://arxiv.org/abs/2506.23201)
*Haoran Li,Muhao Guo,Marija Ilic,Yang Weng,Guangchun Ruan*

Key words: 负荷预测, 元表征, 超网络, 专家混合机制, 深度学习

TL;DR: 论文提出了一种新的住宅负荷预测方法M2oE2框架，通过元表征和超网络动态调节深度学习模型参数，结合专家混合机制提高效率与鲁棒性，显著提升了预测准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有负荷预测模型对外部因素（如天气、日历、定价）处理简单，限制了有用信息的提取，需要一种能够动态适应外部条件的模型。

Method: 设计了一种基于超网络的元表征框架，动态调节深度学习模型参数，并结合专家混合机制（MoE）选择性激活专家并过滤冗余外部输入。

Result: M2oE2框架在多个负荷数据集上显著提升了预测准确性和鲁棒性，且计算开销有限。

Conclusion: 通过元表征和专家混合机制，M2oE2实现了对外部因素的高效利用，为负荷预测提供了新思路。

Abstract: Accurate residential load forecasting is critical for power system
reliability with rising renewable integration and demand-side flexibility.
However, most statistical and machine learning models treat external factors,
such as weather, calendar effects, and pricing, as extra input, ignoring their
heterogeneity, and thus limiting the extraction of useful external information.
We propose a paradigm shift: external data should serve as meta-knowledge to
dynamically adapt the forecasting model itself. Based on this idea, we design a
meta-representation framework using hypernetworks that modulate selected
parameters of a base Deep Learning (DL) model in response to external
conditions. This provides both expressivity and adaptability. We further
integrate a Mixture-of-Experts (MoE) mechanism to enhance efficiency through
selective expert activation, while improving robustness by filtering redundant
external inputs. The resulting model, dubbed as a Meta Mixture of Experts for
External data (M2oE2), achieves substantial improvements in accuracy and
robustness with limited additional overhead, outperforming existing
state-of-the-art methods in diverse load datasets. The dataset and source code
are publicly available at
https://github.com/haorandd/M2oE2\_load\_forecast.git.

</details>


### [140] [FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model](https://arxiv.org/abs/2506.23210)
*Taehwan Yoon,Bongjun Choi*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Federated learning(FL) is used for distributed scenarios to train artificial
intelligence(AI) models while ensuring users' privacy. In federated learning
scenario, the server generally never knows about users' data. This type of
concept makes the AI training process efficient in terms of data privacy.
However, regarding model performance, federated AI models may not sufficiently
satisfy AI users' expectations. Furthermore, AI users have a wide range of
different needs. It is not easy to satisfy the whole users needs. These types
of issues can be addressed through AI model optimization, fine-tuning, or
personalization to achieve optimal model performance. To address model
optimization challenges, we propose reference model-based federated learning
for optimal fine-tuning, which overcomes catastrophic forgetting in each round.
This method is derived from Bayesian parameter-efficient transfer learning,
which includes an optimal proximal term and enables overcoming the catastrophic
forgetting issue in each round by utilizing a reference model that incorporates
previous model parameters. As a result, this method achieves both high model
performance and low computing cost.

</details>


### [141] [Single Image Inpainting and Super-Resolution with Simultaneous Uncertainty Guarantees by Universal Reproducing Kernels](https://arxiv.org/abs/2506.23221)
*Bálint Horváth,Balázs Csanád Csáji*

Key words: 图像修复,超分辨率,统计学习,RKHS,SGKI

TL;DR: 论文提出了一种基于统计学习的方法SGKI，用于估计图像缺失像素，同时提供不确定性量化，适用于图像修复和超分辨率问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决图像修复和超分辨率中缺失像素估计的问题，并提供不确定性量化，以增强方法的实用性。

Method: 基于RKHS假设，提出SGKI方法，扩展和改进了现有的核方法，支持同时为非缺失像素构建置信区间。

Result: SGKI不仅能估计缺失像素，还能高效计算非渐近置信区间，并通过数值实验验证了其有效性。

Conclusion: SGKI为图像修复和超分辨率提供了兼具准确性和不确定性的有效解决方案。

Abstract: The paper proposes a statistical learning approach to the problem of
estimating missing pixels of images, crucial for image inpainting and
super-resolution problems. One of the main novelties of the method is that it
also provides uncertainty quantifications together with the estimated values.
Our core assumption is that the underlying data-generating function comes from
a Reproducing Kernel Hilbert Space (RKHS). A special emphasis is put on
band-limited functions, central to signal processing, which form Paley-Wiener
type RKHSs. The proposed method, which we call Simultaneously Guaranteed Kernel
Interpolation (SGKI), is an extension and refinement of a recently developed
kernel method. An advantage of SGKI is that it not only estimates the missing
pixels, but also builds non-asymptotic confidence bands for the unobserved
values, which are simultaneously guaranteed for all missing pixels. We also
show how to compute these bands efficiently using Schur complements, we discuss
a generalization to vector-valued functions, and we present a series of
numerical experiments on various datasets containing synthetically generated
and benchmark images, as well.

</details>


### [142] [Masked Gated Linear Unit](https://arxiv.org/abs/2506.23225)
*Yukito Tajima,Nakamasa Inoue,Yusuke Sekikawa,Ikuro Sato,Rio Yokota*

Key words: Gated Linear Units, 内存效率, 推理加速, 硬件优化, 大型语言模型

TL;DR: 论文提出了一种新型的Masked Gated Linear Units（MGLUs），通过共享权重矩阵和高效内核实现，显著减少了内存读取需求和推理时间，同时保持或超越了传统GLUs的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有Gated Linear Units（GLUs）在大型语言模型中因使用双倍内存读取而效率低下，需改进以提升计算效率。

Method: 提出MGLUs，包括MoEG架构（共享权重矩阵并学习元素级二进制掩码）和FlashMGLU高效内核，优化内存和计算效率。

Result: FlashMGLU在RTX5090 GPU上实现了19.7倍推理加速、47%内存效率提升和34%速度提升；SwiMGLU在LLM实验中性能优于SwiGLU。

Conclusion: MGLUs通过高效设计和硬件优化，显著提升了GLUs的性能和效率，适用于高性能LLM应用。

Abstract: Gated Linear Units (GLUs) have become essential components in the
feed-forward networks of state-of-the-art Large Language Models (LLMs).
However, they require twice as many memory reads compared to feed-forward
layers without gating, due to the use of separate weight matrices for the gate
and value streams. To address this bottleneck, we introduce Masked Gated Linear
Units (MGLUs), a novel family of GLUs with an efficient kernel implementation.
The core contribution of MGLUs include: (1) the Mixture of Element-wise Gating
(MoEG) architecture that learns multiple binary masks, each determining gate or
value assignments at the element level on a single shared weight matrix
resulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly
kernel that yields up to a 19.7 $\times$ inference-time speed-up over a naive
PyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs
despite added architectural complexity on an RTX5090 GPU. In LLM experiments,
the Swish-activated variant SwiMGLU preserves its memory advantages while
matching - or even surpassing - the downstream accuracy of the SwiGLU baseline.

</details>


### [143] [Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert Merging](https://arxiv.org/abs/2506.23266)
*Lujun Li,Zhu Qiyuan,Jiacheng Wang,Wei Li,Hao Gu,Sirui Han,Yike Guo*

Key words: Mixture of Experts, Expert Merging, Compression, SVD, Subspace

TL;DR: 提出了一种新的MoE压缩框架Sub-MoE，通过子空间专家合并解决参数冲突，显著提升了专家剪枝和合并方法的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决Mixture of Experts（MoE）LLMs由于大规模参数带来的内存、存储和部署挑战，以及专家合并方法中参数冲突的问题。

Method: 提出两阶段方法：1）自适应专家聚类，基于专家输出的相似性分组；2）子空间专家合并，通过SVD提取共享U矩阵并频率合并V矩阵。

Result: 在Mixtral、DeepSeek和Qwen-1.5|3等模型上验证，Sub-MoE在保持96%|86%性能的同时，实现25%|50%专家减少。

Conclusion: Sub-MoE通过子空间合并有效解决参数冲突，支持进一步优化，并显著优于现有方法。

Abstract: Mixture of Experts (MoE) LLMs face significant obstacles due to their massive
parameter scale, which imposes memory, storage, and deployment challenges.
Although recent expert merging methods promise greater efficiency by
consolidating multiple experts, they are fundamentally hindered by parameter
conflicts arising from expert specialization. In this paper, we present
Sub-MoE, a novel MoE compression framework via Subspace Expert Merging. Our key
insight is to perform joint Singular Value Decomposition (SVD) on concatenated
expert weights, reducing conflicting parameters by extracting shared
$U$-matrices while enabling effective merging of the expert-specific $V$
components. Specifically, Sub-MoE consists of two innovative phases: (1)
Adaptive Expert Clustering, which groups functionally coherent experts via
K-means clustering based on cosine similarity of expert outputs; and (2)
Subspace Expert Merging, which first enforces Experts Union Decomposition to
derive the shared $U$-matrix across experts in the same group, then pursues
frequency-based merging for individual $V$-matrices, and finalizes expert
reconstruction using the merged $V$-matrix. In this way, we align and fuse
experts in a shared subspace, and can be extended with intra-expert compression
for further inference optimization. Extensive experiments on Mixtral, DeepSeek,
and Qwen-1.5|3 MoE LLMs demonstrate that our Sub-MoE significantly outperforms
existing expert pruning and merging methods. Notably, our Sub-MoE maintains
96\%|86\% of original performance with 25\%|50\% expert reduction on
Mixtral-8x7B in zero-shot benchmarks. Code will be released at
https://github.com/lliai/MoERazor.

</details>


### [144] [Predicting thinking time in Reasoning models](https://arxiv.org/abs/2506.23274)
*Hans Peter Lynsgøe Raaschou-jensen,Constanza Fierro,Anders Søgaard*

Key words: 推理模型,思考时间预测,用户体验,进度条,LLMs

TL;DR: 本文探讨了推理模型在复杂任务中的应用及其带来的用户体验挑战，提出了预测模型“思考时间”的方法，并讨论了其对用户交互的影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有推理模型在复杂任务中表现出色，但用户无法预知模型的思考时间，导致体验不佳，亟需解决方案。

Method: 提出了在线和离线预测模型“思考时间”的方法，旨在开发一种实用的“推理进度条”。

Result: 这些方法旨在提高用户对模型推理过程的透明度，减少因等待时间不确定而产生的挫败感。

Conclusion: 研究为改善用户与推理模型的交互提供了方向，并提出了未来研究的潜在路径。

Abstract: Reasoning models that produce long, hidden chains of thought have emerged as
powerful tools for complex, reasoning-intensive
tasks\citep{deepseekai2025deepseekr1incentivizingreasoningcapability,
openai2024openaio1card}. However, this paradigm introduces a new user
experience challenge: users have little insight into how much time the model
will spend reasoning before returning an answer. This unpredictability, can
lead to user frustration and is likely to compound as LLMs can produce
increasingly long tasks asynchronously
\citep{kwa2025measuringaiabilitycomplete}. In this paper, we introduce and
evaluate methods for both online and offline prediction of model "thinking
time," aiming to develop a practical "progress bar for reasoning." We discuss
the implications for user interaction and future research directions.

</details>


### [145] [BAPE: Learning an Explicit Bayes Classifier for Long-tailed Visual Recognition](https://arxiv.org/abs/2506.23280)
*Chaoqun Du,Yulin Wang,Shiji Song,Gao Huang*

Key words: 贝叶斯分类器, 长尾分布, 梯度失衡, 后验概率

TL;DR: 本文提出了一种名为BAPE的新方法，通过显式建模后验概率参数并直接学习贝叶斯分类器，解决了长尾数据分布中的梯度失衡问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决深度学习中后验概率隐式估计在长尾数据分布中失效的问题，确保贝叶斯最优决策。

Method: 显式建模后验概率参数，直接学习贝叶斯分类器，并提出分布调整技术。

Result: 在多个数据集上显著提升了泛化性能，且不影响计算效率。

Conclusion: BAPE方法简单有效，能够独立于现有方法提升长尾场景下的分类性能。

Abstract: Bayesian decision theory advocates the Bayes classifier as the optimal
approach for minimizing the risk in machine learning problems. Current deep
learning algorithms usually solve for the optimal classifier by
\emph{implicitly} estimating the posterior probabilities, \emph{e.g.}, by
minimizing the Softmax cross-entropy loss. This simple methodology has been
proven effective for meticulously balanced academic benchmark datasets.
However, it is not applicable to the long-tailed data distributions in the real
world, where it leads to the gradient imbalance issue and fails to ensure the
Bayes optimal decision rule. To address these challenges, this paper presents a
novel approach (BAPE) that provides a more precise theoretical estimation of
the data distributions by \emph{explicitly} modeling the parameters of the
posterior probabilities and solving them with point estimation. Consequently,
our method directly learns the Bayes classifier without gradient descent based
on Bayes' theorem, simultaneously alleviating the gradient imbalance and
ensuring the Bayes optimal decision rule. Furthermore, we propose a
straightforward yet effective \emph{distribution adjustment} technique. This
method enables the Bayes classifier trained from the long-tailed training set
to effectively adapt to the test data distribution with an arbitrary imbalance
factor, thereby enhancing performance without incurring additional
computational costs. In addition, we demonstrate the gains of our method are
orthogonal to existing learning approaches for long-tailed scenarios, as they
are mostly designed under the principle of \emph{implicitly} estimating the
posterior probabilities. Extensive empirical evaluations on CIFAR-10-LT,
CIFAR-100-LT, ImageNet-LT, and iNaturalist demonstrate that our method
significantly improves the generalization performance of popular deep networks,
despite its simplicity.

</details>


### [146] [Not All Explanations for Deep Learning Phenomena Are Equally Valuable](https://arxiv.org/abs/2506.23286)
*Alan Jeffares,Mihaela van der Schaar*

Key words: 深度学习, 反直觉现象, 研究效率, 理论验证

TL;DR: 本文认为，深度学习中的反直觉现象研究可能缺乏实际应用证据，建议从更广泛的深度学习理论角度统一分析这些现象。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨深度学习研究中反直觉现象的普遍意义及其在真实场景中的适用性。

Method: 通过分析近年文献中的典型现象，重新审视研究方法并提出建议。

Result: 发现这些现象在研究中的独立解释可能效率低下，但可作为验证广义深度学习理论的工具。

Conclusion: 建议将深度学习现象视为广义理论的验证场，而非独立难题，以推动领域整体进展。

Abstract: Developing a better understanding of surprising or counterintuitive phenomena
has constituted a significant portion of deep learning research in recent
years. These include double descent, grokking, and the lottery ticket
hypothesis -- among many others. Works in this area often develop ad hoc
hypotheses attempting to explain these observed phenomena on an isolated,
case-by-case basis. This position paper asserts that, in many prominent cases,
there is little evidence to suggest that these phenomena appear in real-world
applications and these efforts may be inefficient in driving progress in the
broader field. Consequently, we argue against viewing them as isolated puzzles
that require bespoke resolutions or explanations. However, despite this, we
suggest that deep learning phenomena do still offer research value by providing
unique settings in which we can refine our broad explanatory theories of more
general deep learning principles. This position is reinforced by analyzing the
research outcomes of several prominent examples of these phenomena from the
recent literature. We revisit the current norms in the research community in
approaching these problems and propose practical recommendations for future
research, aiming to ensure that progress on deep learning phenomena is well
aligned with the ultimate pragmatic goal of progress in the broader field of
deep learning.

</details>


### [147] [Hierarchical Quantized Diffusion Based Tree Generation Method for Hierarchical Representation and Lineage Analysis](https://arxiv.org/abs/2506.23287)
*Zelin Zang,WenZhe Li,Fei Chen,Yongjie Xu,Chang Yu,Zhen Lei,Stan Z. Li*

Key words: 单细胞分析，扩散模型，层次化建模，谱系分析，生成模型

TL;DR: 提出了一种基于扩散模型的HDTree方法，用于高效生成和分析层次化单细胞分化轨迹，克服了传统方法和现有VAEs的局限性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在单细胞研究中，高效且稳定地表征和分析层次化分化轨迹对理解复杂生物过程至关重要。传统方法和现有VAEs存在计算成本高、性能不足以及难以捕捉深度层次关系的问题。

Method: HDTree通过统一的层次化码本和量化扩散过程建模树节点转换，消除了分支专用模块的需求，并利用扩散过程模拟逐步层次变化以提升生成能力。

Result: 实验表明，HDTree在通用和单细胞数据集上均优于现有方法，准确性和性能表现突出。

Conclusion: HDTree为层次化谱系分析提供了新工具，可更精准高效地建模细胞分化路径，并为下游生物任务提供支持。

Abstract: In single-cell research, tracing and analyzing high-throughput single-cell
differentiation trajectories is crucial for understanding complex biological
processes. Key to this is the modeling and generation of hierarchical data that
represents the intrinsic structure within datasets. Traditional methods face
limitations in terms of computational cost, performance, generative capacity,
and stability. Recent VAEs based approaches have made strides in addressing
these challenges but still require specialized network modules for each tree
branch, limiting their stability and ability to capture deep hierarchical
relationships. To overcome these challenges, we introduce diffusion-based
approach called HDTree. HDTree captures tree relationships within a
hierarchical latent space using a unified hierarchical codebook and quantized
diffusion processes to model tree node transitions. This method improves
stability by eliminating branch-specific modules and enhancing generative
capacity through gradual hierarchical changes simulated by the diffusion
process. HDTree's effectiveness is demonstrated through comparisons on both
general-purpose and single-cell datasets, where it outperforms existing methods
in terms of accuracy and performance. These contributions provide a new tool
for hierarchical lineage analysis, enabling more accurate and efficient
modeling of cellular differentiation paths and offering insights for downstream
biological tasks. The code of HDTree is available at anonymous link
https://anonymous.4open.science/r/code_HDTree_review-A8DB.

</details>


### [148] [VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design](https://arxiv.org/abs/2506.23339)
*Malikussaid,Hilal Hudan Nuha*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) demonstrate remarkable potential for scientific
discovery, but their application in domains requiring factual accuracy and
domain-specific constraints remains challenging. In molecular design for drug
discovery, LLMs can suggest creative molecular modifications but often produce
chemically invalid or impractical structures. We present VALID-Mol, a
systematic framework for integrating chemical validation with LLM-driven
molecular design that increases the rate of generating valid chemical
structures from 3% to 83%. Our approach combines methodical prompt engineering,
automated chemical validation, and a fine-tuned domain-adapted LLM to ensure
reliable generation of synthesizable molecules with improved properties. Beyond
the specific implementation, we contribute a generalizable methodology for
scientifically-constrained LLM applications, with quantifiable reliability
improvements. Computational predictions suggest our framework can generate
promising candidates for synthesis with up to 17-fold computationally predicted
improvements in target affinity while maintaining synthetic accessibility. We
provide a detailed analysis of our prompt engineering process, validation
architecture, and fine-tuning approach, offering a reproducible blueprint for
applying LLMs to other scientific domains where domain-specific validation is
essential.

</details>


### [149] [A case for data valuation transparency via DValCards](https://arxiv.org/abs/2506.23349)
*Keziah Naggita,Julienne LaChance*

Key words: 数据估值, 机器学习, 数据市场, 偏见, 伦理

TL;DR: 该论文指出数据估值方法在设计和预处理选择上存在偏见和不稳定性，可能导致技术问题和伦理风险，并提出Data Valuation Cards框架以提高透明度，减少误用。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着数据为中心机器学习的普及，需要公平补偿数据所有者，但现有数据估值方法存在偏见和不稳定性，可能引发技术与伦理问题。

Method: 通过分析9个表格分类数据集和6种数据估值方法，研究了预处理技术、子采样和代表性不足数据对估值的影响。

Result: 研究发现，简单的预处理选择会显著改变数据估值，子采样可能加剧类别不平衡，且低估少数群体数据价值。

Conclusion: 论文主张提高数据估值的透明度，并提出DValCards框架以减少误用并增强信任。

Abstract: Following the rise in popularity of data-centric machine learning (ML),
various data valuation methods have been proposed to quantify the contribution
of each datapoint to desired ML model performance metrics (e.g., accuracy).
Beyond the technical applications of data valuation methods (e.g., data
cleaning, data acquisition, etc.), it has been suggested that within the
context of data markets, data buyers might utilize such methods to fairly
compensate data owners. Here we demonstrate that data valuation metrics are
inherently biased and unstable under simple algorithmic design choices,
resulting in both technical and ethical implications. By analyzing 9 tabular
classification datasets and 6 data valuation methods, we illustrate how (1)
common and inexpensive data pre-processing techniques can drastically alter
estimated data values; (2) subsampling via data valuation metrics may increase
class imbalance; and (3) data valuation metrics may undervalue underrepresented
group data. Consequently, we argue in favor of increased transparency
associated with data valuation in-the-wild and introduce the novel Data
Valuation Cards (DValCards) framework towards this aim. The proliferation of
DValCards will reduce misuse of data valuation metrics, including in data
pricing, and build trust in responsible ML systems.

</details>


### [150] [Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment](https://arxiv.org/abs/2506.23358)
*Pawel Renc,Michal K. Grzeszczyk,Linglong Qian,Nassim Oufattole,Jeff Rasley,Arkadiusz Sitek*

Key words: FTS, EHR, 生成模型, 隐私保护, 分布式学习

TL;DR: FTS是一个用于在分布式时序数据（如电子健康记录）上训练生成式基础模型的新框架，通过合成数据实现零样本推断，保护隐私且可扩展。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决分布式医疗数据中隐私保护和数据共享的矛盾，同时支持多样化的预测和模拟任务。

Method: 将患者历史表示为令牌化的PHTs，本地训练自回归变压器，服务器合成全局生成器。

Result: 在五个临床预测任务中，合成数据训练的模型与真实数据表现相当。

Conclusion: FTS兼具隐私性、可扩展性和多功能性，适用于医疗领域的多种应用。

Abstract: We present Federated Timeline Synthesis (FTS), a novel framework for training
generative foundation models across distributed timeseries data applied to
electronic health records (EHR). At its core, FTS represents patient history as
tokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding
temporal, categorical, and continuous clinical information. Each institution
trains an autoregressive transformer on its local PHTs and transmits only model
weights to a central server. The server uses the generators to synthesize a
large corpus of trajectories and train a Global Generator (GG), enabling
zero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS
on five clinically meaningful prediction tasks using MIMIC-IV data, showing
that models trained on synthetic data generated by GG perform comparably to
those trained on real data. FTS offers strong privacy guarantees, scalability
across institutions, and extensibility to diverse prediction and simulation
tasks especially in healthcare, including counterfactual inference, early
warning detection, and synthetic trial design.

</details>


### [151] [When Additive Noise Meets Unobserved Mediators: Bivariate Denoising Diffusion for Causal Discovery](https://arxiv.org/abs/2506.23374)
*Dominik Meier,Sujai Hiremath,Promit Ghosal,Kyra Gan*

Key words: 因果发现、加性噪声模型、未测量中介变量、BiDD

TL;DR: 提出了一种名为BiDD的新方法，用于处理未测量中介变量带来的因果发现挑战，通过去噪过程评估噪声独立性，优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决传统加性噪声模型在未测量中介变量存在时失效的问题。

Method: 提出BiDD方法，利用去噪过程评估噪声独立性，替代传统的均方误差损失比较。

Result: 实验表明，BiDD在存在中介变量的情况下表现优于现有方法，且在无中介变量时仍保持良好性能。

Conclusion: BiDD为存在未测量中介变量的因果发现提供了稳健的解决方案。

Abstract: Distinguishing cause and effect from bivariate observational data is a
foundational problem in many disciplines, but challenging without additional
assumptions. Additive noise models (ANMs) are widely used to enable
sample-efficient bivariate causal discovery. However, conventional ANM-based
methods fail when unobserved mediators corrupt the causal relationship between
variables. This paper makes three key contributions: first, we rigorously
characterize why standard ANM approaches break down in the presence of
unmeasured mediators. Second, we demonstrate that prior solutions for hidden
mediation are brittle in finite sample settings, limiting their practical
utility. To address these gaps, we propose Bivariate Denoising Diffusion (BiDD)
for causal discovery, a method designed to handle latent noise introduced by
unmeasured mediators. Unlike prior methods that infer directionality through
mean squared error loss comparisons, our approach introduces a novel
independence test statistic: during the noising and denoising processes for
each variable, we condition on the other variable as input and evaluate the
independence of the predicted noise relative to this input. We prove asymptotic
consistency of BiDD under the ANM, and conjecture that it performs well under
hidden mediation. Experiments on synthetic and real-world data demonstrate
consistent performance, outperforming existing methods in mediator-corrupted
settings while maintaining strong performance in mediator-free settings.

</details>


### [152] [Do LLMs Dream of Discrete Algorithms?](https://arxiv.org/abs/2506.23408)
*Claudionor Coelho Jr,Yanen Li,Philip Tee*

Key words: LLMs, 神经符号推理, Prolog, 逻辑推理, 可解释性

TL;DR: 本文提出了一种结合LLMs和基于逻辑的推理模块的神经符号方法，以解决LLMs在严格逻辑推理和可解释性方面的限制。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: LLMs虽然强大，但在逻辑推理和可解释性方面存在不足，需增强其在这些领域的能力。

Method: 通过整合一阶逻辑和显式规则系统，结合Prolog谓词和可组合工具集，使LLMs能够分解复杂查询并生成可靠解决方案。

Result: 实验表明，该混合架构在DABStep基准测试中提高了多步推理任务的精度和覆盖率，减少了幻觉和错误步骤分解。

Conclusion: 该研究证明结合逻辑推理模块能提升LLMs的工程严谨性和系统可靠性，为可信任的AI发展提供了新路径。

Abstract: Large Language Models (LLMs) have rapidly transformed the landscape of
artificial intelligence, enabling natural language interfaces and dynamic
orchestration of software components. However, their reliance on probabilistic
inference limits their effectiveness in domains requiring strict logical
reasoning, discrete decision-making, and robust interpretability. This paper
investigates these limitations and proposes a neurosymbolic approach that
augments LLMs with logic-based reasoning modules, particularly leveraging
Prolog predicates and composable toolsets. By integrating first-order logic and
explicit rule systems, our framework enables LLMs to decompose complex queries
into verifiable sub-tasks, orchestrate reliable solutions, and mitigate common
failure modes such as hallucination and incorrect step decomposition. We
demonstrate the practical benefits of this hybrid architecture through
experiments on the DABStep benchmark, showing improved precision, coverage, and
system documentation in multi-step reasoning tasks. Our results indicate that
combining LLMs with modular logic reasoning restores engineering rigor,
enhances system reliability, and offers a scalable path toward trustworthy,
interpretable AI agents across complex domains.

</details>


### [153] [BenchMake: Turn any scientific data set into a reproducible benchmark](https://arxiv.org/abs/2506.23419)
*Amanda S Barnard*

Key words: 基准数据集, 非负矩阵分解, 凸包, 统计显著性, 计算科学

TL;DR: 本文介绍了一种名为BenchMake的新工具，旨在将任何公开可用的科学数据集转化为可供社区使用的基准测试集。该工具通过非负矩阵分解识别边缘案例，并最大化测试集的统计显著性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于计算科学中基准数据集的相对稀缺性，评估新方法的稳健性和竞争力变得困难。BenchMake旨在解决这一问题，通过利用现有数据集创建高质量基准。

Method: BenchMake采用非负矩阵分解方法，确定性地识别和隔离凸包上的边缘案例，并将数据集中一部分实例分配到测试集，以最大化差异和统计显著性。

Result: BenchMake在10个公开可用的科学数据集上进行了测试，结果显示其数据集划分优于现有划分方法和随机划分。

Conclusion: BenchMake是一种有效的工具，能够将广泛的数据集转化为高质量的基准测试集，支持计算科学中方法的稳健评估。

Abstract: Benchmark data sets are a cornerstone of machine learning development and
applications, ensuring new methods are robust, reliable and competitive. The
relative rarity of benchmark sets in computational science, due to the
uniqueness of the problems and the pace of change in the associated domains,
makes evaluating new innovations difficult for computational scientists. In
this paper a new tool is developed and tested to potentially turn any of the
increasing numbers of scientific data sets made openly available into a
benchmark accessible to the community. BenchMake uses non-negative matrix
factorisation to deterministically identify and isolate challenging edge cases
on the convex hull (the smallest convex set that contains all existing data
instances) and partitions a required fraction of matched data instances into a
testing set that maximises divergence and statistical significance, across
tabular, graph, image, signal and textual modalities. BenchMake splits are
compared to establish splits and random splits using ten publicly available
benchmark sets from different areas of science, with different sizes, shapes,
distributions.

</details>


### [154] [Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting](https://arxiv.org/abs/2506.23424)
*Heitor R. Medeiros,Hossein Sharifi-Noghabi,Gabriel L. Oliveira,Saghar Irandoust*

Key words: 非平稳时间序列, 测试时适应, 低秩适配器, 动态门控, 预测模型

TL;DR: PETSA是一种参数高效的方法，通过在测试时仅更新输入和输出的小型校准模块，以适应非平稳时间序列的预测模型。它使用低秩适配器和动态门控调整表示，避免了完整模型的重新训练。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现实世界中的时间序列通常表现出非平稳性，这会降低预训练预测模型的性能。测试时适应(TTA)通过在推理过程中调整模型来解决这一问题，但现有方法通常更新整个模型，增加了内存和计算成本。

Method: PETSA通过低秩适配器和动态门控调整表示，仅更新输入和输出的小型校准模块。它还引入了一个包含三部分的专门损失函数，以保持准确性。

Result: 在基准数据集上的实验结果表明，PETSA在不同时间范围内均实现了竞争性或更好的性能，同时需要的参数比基线方法更少。

Conclusion: PETSA通过参数高效的方式提高了预测模型的适应性，同时减少了计算成本，适用于非平稳时间序列的预测任务。

Abstract: Real-world time series often exhibit a non-stationary nature, degrading the
performance of pre-trained forecasting models. Test-Time Adaptation (TTA)
addresses this by adjusting models during inference, but existing methods
typically update the full model, increasing memory and compute costs. We
propose PETSA, a parameter-efficient method that adapts forecasters at test
time by only updating small calibration modules on the input and output. PETSA
uses low-rank adapters and dynamic gating to adjust representations without
retraining. To maintain accuracy despite limited adaptation capacity, we
introduce a specialized loss combining three components: (1) a robust term, (2)
a frequency-domain term to preserve periodicity, and (3) a patch-wise
structural term for structural alignment. PETSA improves the adaptability of
various forecasting backbones while requiring fewer parameters than baselines.
Experimental results on benchmark datasets show that PETSA achieves competitive
or better performance across all horizons. Our code is available at:
https://github.com/BorealisAI/PETSA

</details>


### [155] [Enhancing Insider Threat Detection Using User-Based Sequencing and Transformer Encoders](https://arxiv.org/abs/2506.23446)
*Mohamed Elbasheer,Adewale Akinfaderin*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Insider threat detection presents unique challenges due to the authorized
status of malicious actors and the subtlety of anomalous behaviors. Existing
machine learning methods often treat user activity as isolated events, thereby
failing to leverage sequential dependencies in user behavior. In this study, we
propose a User-Based Sequencing (UBS) methodology, transforming the CERT
insider threat dataset into structured temporal sequences suitable for deep
sequential modeling. We deploy a Transformer Encoder architecture to model
benign user activity and employ its reconstruction errors as anomaly scores.
These scores are subsequently evaluated using three unsupervised outlier
detection algorithms: One-Class SVM (OCSVM), Local Outlier Factor (LOF), and
Isolation Forest (iForest). Across four rigorously designed test sets,
including combinations of multiple CERT dataset releases, our UBS-Transformer
pipeline consistently achieves state-of-the-art performance - notably 96.61%
accuracy, 99.43% recall, 96.38% F1-score, 95.00% AUROC, and exceptionally low
false negative (0.0057) and false positive (0.0571) rates. Comparative analyses
demonstrate that our approach substantially outperforms tabular and
conventional autoencoder baselines, underscoring the efficacy of sequential
user modeling and advanced anomaly detection in the insider threat domain.

</details>


### [156] [Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification](https://arxiv.org/abs/2506.23462)
*Manaswi Kulahara,Gautam Siddharth Kashyap,Nipun Joshi,Arpita Soni*

Key words: 灾害管理, 多模态数据, 大型语言模型, 分类任务

TL;DR: 提出一种名为DisasterNet-LLM的专用大型语言模型，用于多模态灾害数据分析，效果优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统方法难以整合多模态灾害数据，因此需要一种更高效的模型来提升灾害管理的及时性和准确性。

Method: DisasterNet-LLM利用先进的预训练、跨模态注意力机制和自适应变换器技术。

Result: 模型在灾害分类任务中表现优异，准确率达89.5%，F1分数88.0%，AUC 0.92，BERTScore 0.88。

Conclusion: DisasterNet-LLM在多模态灾害分析中显著优于现有模型，为灾害管理提供了更高效的工具。

Abstract: Effective disaster management requires timely and accurate insights, yet
traditional methods struggle to integrate multimodal data such as images,
weather records, and textual reports. To address this, we propose
DisasterNet-LLM, a specialized Large Language Model (LLM) designed for
comprehensive disaster analysis. By leveraging advanced pretraining,
cross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM
excels in disaster classification. Experimental results demonstrate its
superiority over state-of-the-art models, achieving higher accuracy of 89.5%,
an F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal
disaster classification tasks.

</details>


### [157] [Reconciling Attribute and Structural Anomalies for Improved Graph Anomaly Detection](https://arxiv.org/abs/2506.23469)
*Chunjing Xiao,Jiahui Lu,Xovee Xu,Fan Zhou,Tianshu Xie,Wei Lu,Lifeng Xu*

Key words: 图异常检测, 无监督学习, 三通道框架, 互蒸馏

TL;DR: 论文提出了TripleAD框架，通过三个模块分别检测属性、结构和混合异常，改进了现有方法的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有无监督方法难以同时处理属性和结构异常，导致性能不佳。

Method: 采用三通道框架，包括多尺度属性估计、链接增强结构估计和属性混合曲率模块，并通过互蒸馏策略促进协作。

Result: 实验表明TripleAD在多种基准上表现优异。

Conclusion: TripleAD通过多通道协作有效解决了异常检测中的干扰问题。

Abstract: Graph anomaly detection is critical in domains such as healthcare and
economics, where identifying deviations can prevent substantial losses.
Existing unsupervised approaches strive to learn a single model capable of
detecting both attribute and structural anomalies. However, they confront the
tug-of-war problem between two distinct types of anomalies, resulting in
suboptimal performance. This work presents TripleAD, a mutual
distillation-based triple-channel graph anomaly detection framework. It
includes three estimation modules to identify the attribute, structural, and
mixed anomalies while mitigating the interference between different types of
anomalies. In the first channel, we design a multiscale attribute estimation
module to capture extensive node interactions and ameliorate the over-smoothing
issue. To better identify structural anomalies, we introduce a link-enhanced
structure estimation module in the second channel that facilitates information
flow to topologically isolated nodes. The third channel is powered by an
attribute-mixed curvature, a new indicator that encapsulates both attribute and
structural information for discriminating mixed anomalies. Moreover, a mutual
distillation strategy is introduced to encourage communication and
collaboration between the three channels. Extensive experiments demonstrate the
effectiveness of the proposed TripleAD model against strong baselines.

</details>


### [158] [Sample Margin-Aware Recalibration of Temperature Scaling](https://arxiv.org/abs/2506.23492)
*Haolan Guo,Linwei Tao,Haoyang Luo,Minjing Dong,Chang Xu*

Key words: deep learning, calibration, uncertainty quantification, SMART, logit gap

TL;DR: SMART是一种轻量级、数据高效的后校准方法，通过基于前两个logit的边距（logit gap）精确调整logit，解决了深度学习中系统性的过度自信问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现代神经网络存在系统性过度自信的问题，现有校准方法难以平衡偏差和方差，SMART旨在提供一种鲁棒且高效的解决方案。

Method: SMART利用logit gap作为去噪的标量信号，结合软分箱预期校准误差（SoftECE）目标，实现稳定参数更新。

Result: SMART在多种数据集和架构上实现了最先进的校准性能，且参数更少。

Conclusion: SMART为神经网络的实用不确定性量化提供了原则性、鲁棒且高效的解决方案。

Abstract: Recent advances in deep learning have significantly improved predictive
accuracy. However, modern neural networks remain systematically overconfident,
posing risks for deployment in safety-critical scenarios. Current post-hoc
calibration methods face a fundamental dilemma: global approaches like
Temperature Scaling apply uniform adjustments across all samples, introducing
high bias despite computational efficiency, while more expressive methods that
operate on full logit distributions suffer from high variance due to noisy
high-dimensional inputs and insufficient validation data. To address these
challenges, we propose Sample Margin-Aware Recalibration of Temperature
(SMART), a lightweight, data-efficient recalibration method that precisely
scales logits based on the margin between the top two logits -- termed the
logit gap. Specifically, the logit gap serves as a denoised, scalar signal
directly tied to decision boundary uncertainty, providing a robust indicator
that avoids the noise inherent in high-dimensional logit spaces while
preserving model prediction invariance. Meanwhile, SMART employs a novel
soft-binned Expected Calibration Error (SoftECE) objective that balances model
bias and variance through adaptive binning, enabling stable parameter updates
even with extremely limited calibration data. Extensive evaluations across
diverse datasets and architectures demonstrate that SMART achieves
state-of-the-art calibration performance even with substantially fewer
parameters compared to existing parametric methods, offering a principled,
robust, and highly efficient solution for practical uncertainty quantification
in neural network predictions. The source code is available at:
https://anonymous.4open.science/r/SMART-8B11.

</details>


### [159] [FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization](https://arxiv.org/abs/2506.23516)
*Seung-Wook Kim,Seongyeol Kim,Jiah Kim,Seowon Ji,Se-Ho Lee*

Key words: 联邦学习,数据异构性,权重标准化,非均匀量化

TL;DR: FedWSQ框架通过权重标准化和分布感知非均匀量化解决联邦学习中的数据异构性与通信开销问题，显著提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决联邦学习中因数据异构性和通信限制导致的性能下降问题。

Method: 提出FedWSQ框架，结合权重标准化（WS）和分布感知非均匀量化（DANUQ）。

Result: 在多种挑战性联邦学习场景中表现优异，尤其是在极端数据异构性和超低比特通信情况下。

Conclusion: FedWSQ有效降低了通信开销，同时保持了较高的模型准确性。

Abstract: Federated learning (FL) often suffers from performance degradation due to key
challenges such as data heterogeneity and communication constraints. To address
these limitations, we present a novel FL framework called FedWSQ, which
integrates weight standardization (WS) and the proposed distribution-aware
non-uniform quantization (DANUQ). WS enhances FL performance by filtering out
biased components in local updates during training, thereby improving the
robustness of the model against data heterogeneity and unstable client
participation. In addition, DANUQ minimizes quantization errors by leveraging
the statistical properties of local model updates. As a result, FedWSQ
significantly reduces communication overhead while maintaining superior model
accuracy. Extensive experiments on FL benchmark datasets demonstrate that
FedWSQ consistently outperforms existing FL methods across various challenging
FL settings, including extreme data heterogeneity and ultra-low-bit
communication scenarios.

</details>


### [160] [Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size](https://arxiv.org/abs/2506.23544)
*Kento Imaizumi,Hideaki Iiduka*

Key words: 动量方法, QHM, 非凸优化, 批量大小, 学习率

TL;DR: 本文研究了准双曲动量（QHM）在随机非凸优化中的收敛性，提出通过增加批量大小而非降低学习率来实现更有效的训练。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 动量方法在深度神经网络等随机非凸优化中的理论支持不足，QHM作为一种广义动量方法，其收敛性值得深入研究。

Method: 通过理论分析和实验验证，研究了小批量QHM在批量大小递增时的收敛性。

Result: 结果表明，递增批量大小可以提高训练效果，而无需降低学习率。

Conclusion: 递增批量大小是训练神经网络的更有效策略。

Abstract: Momentum methods were originally introduced for their superiority to
stochastic gradient descent (SGD) in deterministic settings with convex
objective functions. However, despite their widespread application to deep
neural networks -- a representative case of stochastic nonconvex optimization
-- the theoretical justification for their effectiveness in such settings
remains limited. Quasi-hyperbolic momentum (QHM) is an algorithm that
generalizes various momentum methods and has been studied to better understand
the class of momentum-based algorithms as a whole. In this paper, we provide
both asymptotic and non-asymptotic convergence results for mini-batch QHM with
an increasing batch size. We show that achieving asymptotic convergence
requires either a decaying learning rate or an increasing batch size. Since a
decaying learning rate adversely affects non-asymptotic convergence, we
demonstrate that using mini-batch QHM with an increasing batch size -- without
decaying the learning rate -- can be a more effective strategy. Our experiments
show that even a finite increase in batch size can provide benefits for
training neural networks.

</details>


### [161] [A unified framework on the universal approximation of transformer-type architectures](https://arxiv.org/abs/2506.23551)
*Jingpu Cheng,Qianxiao Li,Ting Lin,Zuowei Shen*

Key words: 通用逼近性质, 变换器, 注意力机制, 理论框架

TL;DR: 该论文研究了变换器类型架构的通用逼近性质（UAP），提出了一个统一的理论框架，扩展了残差网络的结果，并证明了注意力机制模型中的UAP。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 通过识别令牌可区分性作为UAP的基本要求，为设计具有明确UAP保证的新型变换器架构提供理论基础。

Method: 利用注意力层的解析性假设，简化条件验证，采用非构造性方法证明UAP。

Result: 证明了多种注意力机制变换器的UAP，包括基于核和稀疏注意力机制，并提出了设计具有功能对称性的新型架构的示例。

Conclusion: 该框架不仅推广了先前的研究，还为未覆盖的架构提供了UAP保证，为未来变换器架构的设计奠定了理论基础。

Abstract: We investigate the universal approximation property (UAP) of transformer-type
architectures, providing a unified theoretical framework that extends prior
results on residual networks to models incorporating attention mechanisms. Our
work identifies token distinguishability as a fundamental requirement for UAP
and introduces a general sufficient condition that applies to a broad class of
architectures. Leveraging an analyticity assumption on the attention layer, we
can significantly simplify the verification of this condition, providing a
non-constructive approach in establishing UAP for such architectures. We
demonstrate the applicability of our framework by proving UAP for transformers
with various attention mechanisms, including kernel-based and sparse attention
mechanisms. The corollaries of our results either generalize prior works or
establish UAP for architectures not previously covered. Furthermore, our
framework offers a principled foundation for designing novel transformer
architectures with inherent UAP guarantees, including those with specific
functional symmetries. We propose examples to illustrate these insights.

</details>


### [162] [Transition Matching: Scalable and Flexible Generative Modeling](https://arxiv.org/abs/2506.23589)
*Neta Shaul,Uriel Singer,Itai Gat,Yaron Lipman*

Key words: Transition Matching, 生成模型, 自回归, 扩散模型, 流匹配

TL;DR: 论文提出Transition Matching（TM），一种新的离散时间连续状态生成范式，统一并推进了扩散/流模型和连续自回归生成。通过三种TM变体展示其灵活性，部分或完全因果模型在图像和文本生成任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索扩散/流模型和连续自回归生成的设计空间，提出统一两者的新方法以提高生成质量和效率。

Method: 引入Transition Matching（TM）范式，分解复杂生成任务为简单马尔可夫转移，支持非确定性转移核和任意非连续监督。提出DTM、ARTM、FHTM三种变体。

Result: DTM在图像质量和文本一致性上达到SOTA，采样效率提升；ARTM和FHTM在连续自回归生成中媲美非因果方法，FHTM首次在文本到图像任务中超越流方法。

Conclusion: TM为生成任务提供灵活设计空间，其变体在多个任务中表现优异，有望推动媒体和文本生成的统一。

Abstract: Diffusion and flow matching models have significantly advanced media
generation, yet their design space is well-explored, somewhat limiting further
improvements. Concurrently, autoregressive (AR) models, particularly those
generating continuous tokens, have emerged as a promising direction for
unifying text and media generation. This paper introduces Transition Matching
(TM), a novel discrete-time, continuous-state generative paradigm that unifies
and advances both diffusion/flow models and continuous AR generation. TM
decomposes complex generation tasks into simpler Markov transitions, allowing
for expressive non-deterministic probability transition kernels and arbitrary
non-continuous supervision processes, thereby unlocking new flexible design
avenues. We explore these choices through three TM variants: (i) Difference
Transition Matching (DTM), which generalizes flow matching to discrete-time by
directly learning transition probabilities, yielding state-of-the-art image
quality and text adherence as well as improved sampling efficiency. (ii)
Autoregressive Transition Matching (ARTM) and (iii) Full History Transition
Matching (FHTM) are partially and fully causal models, respectively, that
generalize continuous AR methods. They achieve continuous causal AR generation
quality comparable to non-causal approaches and potentially enable seamless
integration with existing AR text generation techniques. Notably, FHTM is the
first fully causal model to match or surpass the performance of flow-based
methods on text-to-image task in continuous domains. We demonstrate these
contributions through a rigorous large-scale comparison of TM variants and
relevant baselines, maintaining a fixed architecture, training data, and
hyperparameters.

</details>


### [163] [When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series](https://arxiv.org/abs/2506.23596)
*Min-Yeong Park,Won-Jeong Lee,Seong Tae Kim,Gyeong-Moon Park*

Key words: 异常预测, 时间序列, A2P框架, 可学习提示池, 未来事件预测

TL;DR: 该论文提出了一种名为A2P的新框架，用于预测未来的异常事件。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法在预测未来异常时间点时表现不佳，因此需要一种能够准确预测未来异常的解决方案。

Method: A2P框架由Anomaly-Aware Forecasting (AAF)和Synthetic Anomaly Prompting (SAP)组成，通过学习异常关系和使用可学习的Anomaly Prompt Pool (APP)来模拟多样化的异常模式。

Result: 在多个真实数据集上的实验表明，A2P优于现有方法，能够准确预测未来异常。

Conclusion: A2P为未来异常预测提供了一种有效的解决方案。

Abstract: Recently, forecasting future abnormal events has emerged as an important
scenario to tackle real-world necessities. However, the solution of predicting
specific future time points when anomalies will occur, known as Anomaly
Prediction (AP), remains under-explored. Existing methods dealing with time
series data fail in AP, focusing only on immediate anomalies or failing to
provide precise predictions for future anomalies. To address the AP task, we
propose a novel framework called Anomaly to Prompt (A2P), comprised of
Anomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To
enable the forecasting model to forecast abnormal time points, we adopt a
strategy to learn the relationships of anomalies. For the robust detection of
anomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP)
that simulates diverse anomaly patterns using signal adaptive prompt.
Comprehensive experiments on multiple real-world datasets demonstrate the
superiority of A2P over state-of-the-art methods, showcasing its ability to
predict future anomalies. Our implementation code is available at
https://github.com/KU-VGI/AP.

</details>


### [164] [A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data](https://arxiv.org/abs/2506.23629)
*Xin Liao,Bing Yang,Cai Yu*

Key words: 水质数据、低秩表示、卷积神经网络、数据填补、非线性交互

TL;DR: 论文提出了一种结合非线性低秩表示模型（NLR）和卷积神经网络（CNN）的方法，用于填补水质数据中的缺失值，显著提升了估算精度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 水质数据的完整性对决策和保护生态至关重要，但现有方法难以处理传感器故障等导致的高维稀疏数据问题。

Method: 通过CNN融合时间特征捕获时序依赖性，并提取非线性交互和局部模式以实现多维信息的深度融合。

Result: 三个真实数据集上的实验表明，该方法在估算精度上优于现有最先进模型。

Conclusion: 该方法为动态环境中水质监测数据的处理提供了有效方案。

Abstract: The integrity of Water Quality Data (WQD) is critical in environmental
monitoring for scientific decision-making and ecological protection. However,
water quality monitoring systems are often challenged by large amounts of
missing data due to unavoidable problems such as sensor failures and
communication delays, which further lead to water quality data becoming
High-Dimensional and Sparse (HDS). Traditional data imputation methods are
difficult to depict the potential dynamics and fail to capture the deep data
features, resulting in unsatisfactory imputation performance. To effectively
address the above issues, this paper proposes a Nonlinear Low-rank
Representation model (NLR) with Convolutional Neural Networks (CNN) for
imputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing
temporal features to model the temporal dependence of data between time slots,
and b) Extracting nonlinear interactions and local patterns to mine
higher-order relationships features and achieve deep fusion of multidimensional
information. Experimental studies on three real water quality datasets
demonstrate that the proposed model significantly outperforms existing
state-of-the-art data imputation models in terms of estimation accuracy. It
provides an effective approach for handling water quality monitoring data in
complex dynamic environments.

</details>


### [165] [Learning Modular Exponentiation with Transformers](https://arxiv.org/abs/2506.23679)
*David Demitri Africa,Sara M. Kapoor,Theo Simon Sorg*

Key words: 模指数运算, Transformer, 机制可解释性, 数论, 注意力头

TL;DR: 该论文研究了Transformer模型如何学习模指数运算，发现通过特定训练策略和结构分析可以揭示模型内部的算术结构，并提出了更高效和可解释的神经方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 模指数运算在数论和密码学中至关重要，但缺乏从机制可解释性角度的研究。论文旨在探索Transformer模型如何学习这一运算及其内部算术结构的涌现。

Method: 使用4层编码器-解码器Transformer模型，结合策略性采样、PCA嵌入分析和激活修补技术，研究模型如何编码数论特性。

Result: 发现反向操作数训练显著提升性能，并在相关模数上表现出突然泛化；模型通过专用计算电路学习模算术，部分注意力头子图足以完成任务。

Conclusion: 研究表明Transformer通过专用计算电路学习模算术，为更高效和可解释的神经方法奠定了基础。

Abstract: Modular exponentiation is crucial to number theory and cryptography, yet
remains largely unexplored from a mechanistic interpretability standpoint. We
train a 4-layer encoder-decoder Transformer model to perform this operation and
investigate the emergence of numerical reasoning during training. Utilizing
principled sampling strategies, PCA-based embedding analysis, and activation
patching, we examine how number-theoretic properties are encoded within the
model. We find that reciprocal operand training leads to strong performance
gains, with sudden generalization across related moduli. These synchronized
accuracy surges reflect grokking-like dynamics, suggesting the model
internalizes shared arithmetic structure. We also find a subgraph consisting
entirely of attention heads in the final layer sufficient to achieve full
performance on the task of regular exponentiation. These results suggest that
transformer models learn modular arithmetic through specialized computational
circuits, paving the way for more interpretable and efficient neural approaches
to modular exponentiation.

</details>


### [166] [DABstep: Data Agent Benchmark for Multi-step Reasoning](https://arxiv.org/abs/2506.23719)
*Alex Egg,Martin Iglesias Goyanes,Friso Kingma,Andreu Mora,Leandro von Werra,Thomas Wolf*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce DABstep, a novel benchmark for evaluating AI agents on realistic
multi-step data analysis tasks. DABstep comprises over 450 real-world
challenges derived from a financial analytics platform, requiring models to
combine code-based data processing with contextual reasoning over heterogeneous
documentation. Each task demands an iterative, multi-step problem-solving
approach, testing capabilities in data manipulation, cross-referencing multiple
sources, and precise result reporting. The benchmark provides a factoid-style
answer format with automatic correctness checks for objective scoring at scale.
We evaluate leading LLM-based agents, revealing a substantial performance gap:
even the best agent achieves only 14.55% accuracy on the hardest tasks. We
detail our benchmark's design, dataset composition, task formulation,
evaluation protocol, report baseline results and analyze failure modes. DABstep
is released with a public leaderboard and toolkit to accelerate research in
autonomous data analysis.

</details>


### [167] [System-Embedded Diffusion Bridge Models](https://arxiv.org/abs/2506.23726)
*Bartlomiej Sobieski,Matthew Tivnan,Yuang Wang,Siyeop Yoon,Pengfei Jin,Dufan Wu,Quanzheng Li,Przemyslaw Biecek*

Key words: 逆问题, SGMs, SDBs, 线性测量系统

TL;DR: SGMs解决逆问题，SDBs嵌入已知线性测量系统，提升表现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 恢复不完全或噪声信号是基础问题，SGMs为逆问题提供新框架。

Method: 提出SDBs，嵌入已知线性测量系统到矩阵值SDE中。

Result: 在多样线性逆问题中表现提升，且对系统误设鲁棒。

Conclusion: SDBs是实际应用的潜在解决方案。

Abstract: Solving inverse problems -- recovering signals from incomplete or noisy
measurements -- is fundamental in science and engineering. Score-based
generative models (SGMs) have recently emerged as a powerful framework for this
task. Two main paradigms have formed: unsupervised approaches that adapt
pretrained generative models to inverse problems, and supervised bridge methods
that train stochastic processes conditioned on paired clean and corrupted data.
While the former typically assume knowledge of the measurement model, the
latter have largely overlooked this structural information. We introduce System
embedded Diffusion Bridge Models (SDBs), a new class of supervised bridge
methods that explicitly embed the known linear measurement system into the
coefficients of a matrix-valued SDE. This principled integration yields
consistent improvements across diverse linear inverse problems and demonstrates
robust generalization under system misspecification between training and
deployment, offering a promising solution to real-world applications.

</details>


### [168] [Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models](https://arxiv.org/abs/2506.23731)
*Michel Meintz,Jan Dubiński,Franziska Boenisch,Adam Dziedzic*

Key words: 图像生成模型, 水印, 放射性, 扩散模型, 自回归模型

TL;DR: 论文研究了图像生成模型中水印的放射性（即在模型训练后仍能被检测的特性），发现扩散模型（DMs）的现有水印方法无法保持放射性，而针对图像自回归模型（IARs）的水印方法尚未提出。作者提出了一种专为IARs设计且具有放射性的水印方法，并通过实验验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决图像生成模型中水印在训练过程中易丢失的问题，尤其是针对IARs的水印方法缺失问题，以实现对生成图像的溯源和防滥用。

Method: 借鉴大语言模型（LLMs）的技术，提出了一种专为IARs设计的放射性水印方法，并在实验中验证其有效性。

Result: 提出的方法在IARs中成功保留了水印的放射性，能够实现对生成图像的溯源和防滥用。

Conclusion: 研究填补了IARs水印方法的技术空白，提出的放射性水印方法对于保护生成图像的版权具有实际意义。

Abstract: Image generative models have become increasingly popular, but training them
requires large datasets that are costly to collect and curate. To circumvent
these costs, some parties may exploit existing models by using the generated
images as training data for their own models. In general, watermarking is a
valuable tool for detecting unauthorized use of generated images. However, when
these images are used to train a new model, watermarking can only enable
detection if the watermark persists through training and remains identifiable
in the outputs of the newly trained model - a property known as radioactivity.
We analyze the radioactivity of watermarks in images generated by diffusion
models (DMs) and image autoregressive models (IARs). We find that existing
watermarking methods for DMs fail to retain radioactivity, as watermarks are
either erased during encoding into the latent space or lost in the
noising-denoising process (during the training in the latent space). Meanwhile,
despite IARs having recently surpassed DMs in image generation quality and
efficiency, no radioactive watermarking methods have been proposed for them. To
overcome this limitation, we propose the first watermarking method tailored for
IARs and with radioactivity in mind - drawing inspiration from techniques in
large language models (LLMs), which share IARs' autoregressive paradigm. Our
extensive experimental evaluation highlights our method's effectiveness in
preserving radioactivity within IARs, enabling robust provenance tracking, and
preventing unauthorized use of their generated images.

</details>


### [169] [Training of Spiking Neural Networks with Expectation-Propagation](https://arxiv.org/abs/2506.23757)
*Dan Yao,Steve McLaughlin,Yoann Altmann*

Key words: 脉冲神经网络, 期望传播, 消息传递, 边缘分布, 贝叶斯网络

TL;DR: 提出一种基于期望传播的统一消息传递框架，用于训练脉冲神经网络（SNNs），无需梯度即可学习网络参数的边缘分布。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决脉冲神经网络训练中的离散和连续权重学习问题，并同时处理隐藏层输出的噪声参数。

Method: 使用期望传播的统一消息传递框架，通过批量训练样本学习网络参数的边缘分布。

Result: 算法在实践中比基于梯度的方法收敛更快，且不需要大量数据遍历，适用于分类和回归任务。

Conclusion: 该方法为深度贝叶斯网络提供了新的高效训练途径。

Abstract: In this paper, we propose a unifying message-passing framework for training
spiking neural networks (SNNs) using Expectation-Propagation. Our gradient-free
method is capable of learning the marginal distributions of network parameters
and simultaneously marginalizes nuisance parameters, such as the outputs of
hidden layers. This framework allows for the first time, training of discrete
and continuous weights, for deterministic and stochastic spiking networks,
using batches of training samples. Although its convergence is not ensured, the
algorithm converges in practice faster than gradient-based methods, without
requiring a large number of passes through the training data. The
classification and regression results presented pave the way for new efficient
training methods for deep Bayesian networks.

</details>


### [170] [Model-driven Stochastic Trace Clustering](https://arxiv.org/abs/2506.23776)
*Jari Peeperkorn,Johannes De Smedt,Jochen De Weerdt*

Key words: 过程挖掘,轨迹聚类,随机过程模型,熵相关性

TL;DR: 提出了一种基于随机过程模型的轨迹聚类方法，通过熵相关性和结构对齐优化聚类分配，提升了模型的解释性和性能表现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 高变异性的过程发现算法生成复杂难懂的模型，而现有聚类技术多忽略随机性，难以捕捉真实执行动态。

Method: 采用直接跟随概率的熵相关性作为一致性度量，优化随机过程模型以分配轨迹到聚类。

Result: 方法计算高效，线性扩展，生成的控制流模式更清晰，在公共数据集上表现优于现有方法。

Conclusion: 基于随机性的模型驱动聚类能更准确地表示过程行为，且聚类性能评估需考虑随机性。

Abstract: Process discovery algorithms automatically extract process models from event
logs, but high variability often results in complex and hard-to-understand
models. To mitigate this issue, trace clustering techniques group process
executions into clusters, each represented by a simpler and more understandable
process model. Model-driven trace clustering improves on this by assigning
traces to clusters based on their conformity to cluster-specific process
models. However, most existing clustering techniques rely on either no process
model discovery, or non-stochastic models, neglecting the frequency or
probability of activities and transitions, thereby limiting their capability to
capture real-world execution dynamics. We propose a novel model-driven trace
clustering method that optimizes stochastic process models within each cluster.
Our approach uses entropic relevance, a stochastic conformance metric based on
directly-follows probabilities, to guide trace assignment. This allows
clustering decisions to consider both structural alignment with a cluster's
process model and the likelihood that a trace originates from a given
stochastic process model. The method is computationally efficient, scales
linearly with input size, and improves model interpretability by producing
clusters with clearer control-flow patterns. Extensive experiments on public
real-life datasets show that our method outperforms existing alternatives in
representing process behavior and reveals how clustering performance rankings
can shift when stochasticity is considered.

</details>


### [171] [Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling](https://arxiv.org/abs/2506.23782)
*Xiaoyang Li,Linwei Tao,Haohui Lu,Minjing Dong,Junbin Gao,Chang Xu*

Key words: 图神经网络, 校准, 图小波, 温度缩放

TL;DR: 论文提出了一种名为WATS的后处理校准框架，通过图小波特征优化GNN的置信度估计，显著提高了校准性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: GNN在关系数据上表现出色，但其置信度估计与实际准确性不一致，限制了其在安全关键场景的应用。现有方法依赖粗粒度统计，忽略了图的细粒度结构异质性。

Method: WATS利用可调热核图小波特征为节点分配特定温度，无需模型重新训练或邻域信息。

Result: 在七个数据集和两种GNN骨干上的实验表明，WATS的校准误差最低，比基线方法最高降低42.3%，校准方差平均减少17.24%。

Conclusion: WATS是一种高效且可扩展的GNN校准方法，适用于不同规模和图密度的场景。

Abstract: Graph Neural Networks (GNNs) have demonstrated strong predictive performance
on relational data; however, their confidence estimates often misalign with
actual predictive correctness, posing significant limitations for deployment in
safety-critical settings. While existing graph-aware calibration methods seek
to mitigate this limitation, they primarily depend on coarse one-hop
statistics, such as neighbor-predicted confidence, or latent node embeddings,
thereby neglecting the fine-grained structural heterogeneity inherent in graph
topology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a
post-hoc calibration framework that assigns node-specific temperatures based on
tunable heat-kernel graph wavelet features. Specifically, WATS harnesses the
scalability and topology sensitivity of graph wavelets to refine confidence
estimates, all without necessitating model retraining or access to neighboring
logits or predictions. Extensive evaluations across seven benchmark datasets
with varying graph structures and two GNN backbones demonstrate that WATS
achieves the lowest Expected Calibration Error (ECE) among all compared
methods, outperforming both classical and graph-specific baselines by up to
42.3\% in ECE and reducing calibration variance by 17.24\% on average compared
with graph-specific methods. Moreover, WATS remains computationally efficient,
scaling well across graphs of diverse sizes and densities. Code will be
released based on publication.

</details>


### [172] [KAIROS: Scalable Model-Agnostic Data Valuation](https://arxiv.org/abs/2506.23799)
*Jiongli Zhu,Parjanya Prajakta Prashant,Alex Cloninger,Babak Salimi*

Key words: KAIROS, 数据评估, MMD, 模型无关, 在线更新

TL;DR: KAIROS 是一个可扩展的、模型无关的评估框架，通过分配每个训练样本的分布影响力评分，有效地解决了现有方法在计算成本和准确性上的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有评估方法（如模型依赖方法和数据Shapley方法）在计算成本和偏差继承方面存在问题，Wasserstein 方法的近似导致排序不准确，因此需要一种更高效且准确的解决方案。

Method: KAIROS 提出了一种基于最大均值差异（MMD）的方法，计算样本对训练分布与参考集之间差异的贡献，支持高效在线更新和无重复训练。

Result: 实验证明，KAIROS 在噪声、错误标注和中毒基准测试中优于现有方法，并在运行时效率上提升了50倍。

Conclusion: KAIROS 提供了一种高效、准确且理论保障的训练数据评估方法，适用于大规模AI资产评估。

Abstract: Training data increasingly shapes not only model accuracy but also regulatory
compliance and market valuation of AI assets. Yet existing valuation methods
remain inadequate: model-based techniques depend on a single fitted model and
inherit its biases, while algorithm-based approaches such as Data Shapley
require costly retrainings at web scale. Recent Wasserstein-based
model-agnostic methods rely on approximations that misrank examples relative to
their true leave-one-out (LOO) utility. We introduce KAIROS, a scalable,
model-agnostic valuation framework that assigns each example a distributional
influence score: its contribution to the Maximum Mean Discrepancy (MMD) between
the empirical training distribution and a clean reference set. Unlike
Wasserstein surrogates, our MMD-based influence admits a closed-form solution
that faithfully approximates the exact LOO ranking within $O(1/N^2)$ error,
requires no retraining, and naturally extends to conditional kernels for
unified label- and feature-error detection. Moreover, KAIROS supports efficient
online updates: when a new batch of size m arrives, all scores can be updated
in $O(mN)$ time, delivering up to 50x speedup without compromising ranking
quality. Empirical evaluations on noise, mislabeling, and poisoning benchmarks
show that KAIROS consistently outperforms state-of-the-art model-, Shapley-,
and Wasserstein-based baselines in both accuracy and runtime. We provide
rigorous theoretical guarantees, including symmetry for reproducible rankings
and density-separation for interpretable thresholds.

</details>


### [173] [Towards the Training of Deeper Predictive Coding Neural Networks](https://arxiv.org/abs/2506.23800)
*Chang Qi,Matteo Forasassi,Thomas Lukasiewicz,Tommaso Salvatori*

Key words: 预测编码网络, 均衡传播, 深层网络, 权重更新, 图像分类

TL;DR: 该论文研究了基于均衡传播训练的预测编码网络在深层架构中性能下降的原因，并提出两种新方法优化隐变量和权重更新机制，显著提升了深层网络的测试精度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究深层预测编码网络性能下降的原因，并提出改进方法，以扩展其在实际复杂任务中的应用。

Method: 引入两种优化隐变量的方法，平衡层间能量分布；提出新的权重更新机制，减少深层误差累积。

Result: 在七层以上的图像分类任务中测试，性能显著提升，与反向传播方法相当。

Conclusion: 改进深层网络的均衡传播训练方法，为其在复杂任务中的应用开辟了新可能性。

Abstract: Predictive coding networks trained with equilibrium propagation are neural
models that perform inference through an iterative energy minimization process.
Previous studies have demonstrated their effectiveness in shallow
architectures, but show significant performance degradation when depth exceeds
five to seven layers. In this work, we show that the reason behind this
degradation is due to exponentially imbalanced errors between layers during
weight updates, and predictions from the previous layer not being effective in
guiding updates in deeper layers. We address the first issue by introducing two
novel methods to optimize the latent variables that use precision-weighting to
re-balance the distribution of energy among layers during the `relaxation
phase', and the second issue by proposing a novel weight update mechanism that
reduces error accumulation in deeper layers. Empirically, we test our methods
on a large number of image classification tasks, resulting in large
improvements in test accuracy across networks with more than seven layers, with
performances comparable to those of backprop on similar models. These findings
suggest that a better understanding of the relaxation phase is important to
train models using equilibrium propagation at scale, and open new possibilities
for their application in complex tasks.

</details>


### [174] [Adaptive Out-of-Control Point Pattern Detection in Sequential Random Finite Set Observations](https://arxiv.org/abs/2506.23802)
*Konstantinos Bourazas,Savvas Papaioannou,Panayiotis Kolios*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this work we introduce a novel adaptive anomaly detection framework
specifically designed for monitoring sequential random finite set (RFS)
observations. Our approach effectively distinguishes between In-Control data
(normal) and Out-Of-Control data (anomalies) by detecting deviations from the
expected statistical behavior of the process. The primary contributions of this
study include the development of an innovative RFS-based framework that not
only learns the normal behavior of the data-generating process online but also
dynamically adapts to behavioral shifts to accurately identify abnormal point
patterns. To achieve this, we introduce a new class of RFS-based posterior
distributions, named Power Discounting Posteriors (PD), which facilitate
adaptation to systematic changes in data while enabling anomaly detection of
point pattern data through a novel predictive posterior density function. The
effectiveness of the proposed approach is demonstrated by extensive qualitative
and quantitative simulation experiments.

</details>


### [175] [SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration](https://arxiv.org/abs/2506.23803)
*Dmitry Kovalev*

Key words: 随机梯度下降, AdaGrad, 预处理, 收敛分析, Nesterov动量

TL;DR: 本文重新研究了具有AdaGrad型预处理的随机梯度下降（SGD），提出了统一的收敛分析框架，并证明了AdaGrad和DASGO等算法可以通过Nesterov动量加速收敛，为Adam的实践效率提供了理论解释。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究SGD在自适应预处理下的收敛性，尤其是AdaGrad类算法的性能提升潜力，旨在填补理论分析与实际效率之间的差距。

Method: 开发了统一的收敛分析框架，适用于各向异性或矩阵平滑噪声假设下的SGD，并引入Nesterov动量加速AdaGrad和DASGO的收敛。

Result: 恢复了AdaGrad-Norm、AdaGrad和ASGO/One-sided Shampoo等算法的收敛结果，首次为DASGO提供了理论保证，并证明了动量可以加速AdaGrad类算法的收敛。

Conclusion: AdaGrad型算法可以通过结合对角预处理和动量实现更快的收敛，解释了Adam的实际效率。

Abstract: In this paper, we revisit stochastic gradient descent (SGD) with AdaGrad-type
preconditioning. Our contributions are twofold. First, we develop a unified
convergence analysis of SGD with adaptive preconditioning under anisotropic or
matrix smoothness and noise assumptions. This allows us to recover
state-of-the-art convergence results for several popular adaptive gradient
methods, including AdaGrad-Norm, AdaGrad, and ASGO/One-sided Shampoo. In
addition, we establish the fundamental connection between two recently proposed
algorithms, Scion and DASGO, and provide the first theoretical guarantees for
the latter. Second, we show that the convergence of methods like AdaGrad and
DASGO can be provably accelerated beyond the best-known rates using Nesterov
momentum. Consequently, we obtain the first theoretical justification that
AdaGrad-type algorithms can simultaneously benefit from both diagonal
preconditioning and momentum, which may provide an ultimate explanation for the
practical efficiency of Adam.

</details>


### [176] [Supercm: Revisiting Clustering for Semi-Supervised Learning](https://arxiv.org/abs/2506.23824)
*Durgesh Singh,Ahcene Boubekki,Robert Jenssen,Michael C. Kampffmeyer*

Key words: 半监督学习, 可微聚类, 一致性正则化, 端到端训练

TL;DR: 本文提出了一种新的半监督学习方法，通过引入可微聚类模块，简化了训练策略并提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的半监督学习方法大多依赖于复杂的训练策略，本文旨在通过聚类假设简化方法并提升效果。

Method: 扩展了一种可微聚类模块，利用标注数据引导聚类中心，从而构建端到端的训练框架。

Result: 提出的方法在性能上优于仅监督学习的基线，并能与其他半监督学习方法结合进一步提升效果。

Conclusion: 通过引入可微聚类模块，本文提供了一种简单且高效的新半监督学习方法。

Abstract: The development of semi-supervised learning (SSL) has in recent years largely
focused on the development of new consistency regularization or entropy
minimization approaches, often resulting in models with complex training
strategies to obtain the desired results. In this work, we instead propose a
novel approach that explicitly incorporates the underlying clustering
assumption in SSL through extending a recently proposed differentiable
clustering module. Leveraging annotated data to guide the cluster centroids
results in a simple end-to-end trainable deep SSL approach. We demonstrate that
the proposed model improves the performance over the supervised-only baseline
and show that our framework can be used in conjunction with other SSL methods
to further boost their performance.

</details>


### [177] [EFPI: Elastic Formation and Position Identification in Football (Soccer) using Template Matching and Linear Assignment](https://arxiv.org/abs/2506.23843)
*Joris Bekkers*

Key words: 足球战术,队形识别,球员位置,时空数据,成本最小化

TL;DR: 本文提出了一种名为EFPI的灵活方法，用于识别足球中的队形和分配球员位置，通过预定义的静态队形模板和时空跟踪数据的成本最小化来实现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 足球战术分析中，理解队形和球员位置至关重要。

Method: 使用线性总和分配将球员与模板队形中的位置进行最优匹配，通过最小化实际球员位置与模板位置的总距离来选择分配成本最低的队形。

Result: 该方法在单个帧上有效，并可扩展到更大的比赛片段。通过稳定参数防止不必要的队形变化。

Conclusion: EFPI是一种有效的队形识别和球员分配方法，并已开源。

Abstract: Understanding team formations and player positioning is crucial for tactical
analysis in football (soccer). This paper presents a flexible method for
formation recognition and player position assignment in football using
predefined static formation templates and cost minimization from spatiotemporal
tracking data, called EFPI. Our approach employs linear sum assignment to
optimally match players to positions within a set of template formations by
minimizing the total distance between actual player locations and template
positions, subsequently selecting the formation with the lowest assignment
cost. To improve accuracy, we scale actual player positions to match the
dimensions of these formation templates in both width and length. While the
method functions effectively on individual frames, it extends naturally to
larger game segments such as complete periods, possession sequences or specific
intervals (e.g. 10 second intervals, 5 minute intervals etc.). Additionally, we
incorporate an optional stability parameter that prevents unnecessary formation
changes when assignment costs differ only marginally between time segments.
EFPI is available as open-source code through the unravelsports Python package.

</details>


### [178] [Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts](https://arxiv.org/abs/2506.23845)
*Kenny Peng,Rajiv Movva,Jon Kleinberg,Emma Pierson,Nikhil Garg*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While sparse autoencoders (SAEs) have generated significant excitement, a
series of negative results have added to skepticism about their usefulness.
Here, we establish a conceptual distinction that reconciles competing
narratives surrounding SAEs. We argue that while SAEs may be less effective for
acting on known concepts, SAEs are powerful tools for discovering unknown
concepts. This distinction cleanly separates existing negative and positive
results, and suggests several classes of SAE applications. Specifically, we
outline use cases for SAEs in (i) ML interpretability, explainability,
fairness, auditing, and safety, and (ii) social and health sciences.

</details>


### [179] [When Plants Respond: Electrophysiology and Machine Learning for Green Monitoring Systems](https://arxiv.org/abs/2506.23872)
*Eduard Buss,Till Aust,Heiko Hamann*

Key words: 植物传感器、生物混合系统、环境监测、机器学习、PhytoNode

TL;DR: 该研究利用植物作为天然传感器，通过植物可穿戴设备记录生理信号，结合机器学习分析环境条件，实现了高效的生物混合系统监测。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索植物作为生物传感器的潜力，开发可持续的环境监测系统。

Method: 使用植物可穿戴设备PhytoNode记录植物电生理活动，结合AutoML技术分析数据。

Result: 分类模型在二元任务中达到95%的宏F1分数，AutoML表现优于手动调参。

Conclusion: 该生物混合系统在恶劣环境中实现高效监测，为可持续环境监测提供了新方法。

Abstract: Living plants, while contributing to ecological balance and climate
regulation, also function as natural sensors capable of transmitting
information about their internal physiological states and surrounding
conditions. This rich source of data provides potential for applications in
environmental monitoring and precision agriculture. With integration into
biohybrid systems, we establish novel channels of physiological signal flow
between living plants and artificial devices. We equipped *Hedera helix* with a
plant-wearable device called PhytoNode to continuously record the plant's
electrophysiological activity. We deployed plants in an uncontrolled outdoor
environment to map electrophysiological patterns to environmental conditions.
Over five months, we collected data that we analyzed using state-of-the-art and
automated machine learning (AutoML). Our classification models achieve high
performance, reaching macro F1 scores of up to 95 percent in binary tasks.
AutoML approaches outperformed manual tuning, and selecting subsets of
statistical features further improved accuracy. Our biohybrid living system
monitors the electrophysiology of plants in harsh, real-world conditions. This
work advances scalable, self-sustaining, and plant-integrated living biohybrid
systems for sustainable environmental monitoring.

</details>


### [180] [Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic](https://arxiv.org/abs/2506.23875)
*Yuta Sato,Kazuhiko Kawamoto,Hiroshi Kera*

Key words: Transformer, 算术任务, 重排序, 层次搜索, 学习友好顺序

TL;DR: 该研究提出了一种通过重新排列解码器输入令牌来优化Transformer学习算术任务的方法，利用两阶段层次搜索从数十亿候选顺序中找出学习友好的顺序。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究动机在于揭示思维的链条顺序如何影响Transformer的推理难度，尤其是在算术任务中。

Method: 方法包括训练Transformer以不同顺序排列的目标序列混合任务，并通过早期损失下降速度识别良性顺序；为解决搜索空间爆炸问题，提出两阶段层次化重排序策略。

Result: 实验表明，该方法能在数十亿的顺序候选中找到学习友好的顺序，例如在乘法任务中重现了前人研究的逆序数字顺序。

Conclusion: 通过重排序解码器输入令牌顺序，可以显著提升Transformer在算术任务中的学习效率。

Abstract: The chain of thought is fundamental in Transformers, which is to perform
step-by-step reasoning. Besides what intermediate steps work, the order of
these steps critically affects the difficulty of the reasoning. This study
addresses a novel task of unraveling chain of thought - reordering decoder
input tokens to a learning-friendly sequence for Transformers to learn
arithmetic tasks. The proposed pipeline first trains a Transformer on a mixture
of target sequences arranged in different orders and then identifies benign
orders as those with fast loss drops in the early stage. As the search space
grows factorially with sequence length, we propose a two-stage hierarchical
approach for inter- and intra-block reordering. Experiments on four
order-sensitive arithmetic tasks show that our method identifies a
learning-friendly order out of a few billion candidates. Notably, on the
multiplication task, it recovered the reverse-digit order reported in prior
studies.

</details>


### [181] [Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System](https://arxiv.org/abs/2506.23923)
*Miguel Camacho-Sánchez,Fernando García-Torres,Jesper John Lisegaard,Rocío del Amor,Sankhya Mohanty,Valery Naranjo*

Key words: 树脂灌注, 树脂转移模塑, 强化学习, 近端策略优化, 复合材料制造

TL;DR: 使用强化学习（RL）改进树脂灌注（RI）和树脂转移模塑（RTM）中的树脂流动控制，提高复合材料制造质量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 树脂流动动态控制在复合材料制造中至关重要，防止孔隙和干斑影响结构完整性。

Method: 提出基于强化学习（RL）的策略，使用近端策略优化（PPO）同步两个树脂入口和单个出口的流动前沿。

Result: RL方法在实现流动收敛方面表现出有效性，提升了工艺控制和产品质量。

Conclusion: RL技术有望在复合材料制造中优化流动控制，改善最终产品性能。

Abstract: Resin infusion (RI) and resin transfer moulding (RTM) are critical processes
for the manufacturing of high-performance fibre-reinforced polymer composites,
particularly for large-scale applications such as wind turbine blades.
Controlling the resin flow dynamics in these processes is critical to ensure
the uniform impregnation of the fibre reinforcements, thereby preventing
residual porosities and dry spots that impact the consequent structural
integrity of the final component. This paper presents a reinforcement learning
(RL) based strategy, established using process simulations, for synchronising
the different resin flow fronts in an infusion scenario involving two resin
inlets and a single outlet. Using Proximal Policy Optimisation (PPO), our
approach addresses the challenge of managing the fluid dynamics in a partially
observable environment. The results demonstrate the effectiveness of the RL
approach in achieving an accurate flow convergence, highlighting its potential
towards improving process control and product quality in composites
manufacturing.

</details>


### [182] [Bridging the Gap with Retrieval-Augmented Generation: Making Prosthetic Device User Manuals Available in Marginalised Languages](https://arxiv.org/abs/2506.23958)
*Ikechukwu Ogbonna,Lesley Davidson,Soumya Banerjee,Abhishek Dasgupta,Laurence Kenney,Vikranth Harthikote Nagaraja*

Key words: 医疗文档翻译, 边缘化语言, AI框架, 检索增强生成, 多语言NLP

TL;DR: 本研究开发了一个AI框架，将复杂的医疗文档翻译成边缘化语言，提升非洲社区居民获取医疗信息的可及性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决非洲社区居民因语言和识字障碍无法获取医疗文档的问题，特别是假肢设备的使用手册。

Method: 采用检索增强生成（RAG）管道处理语义理解，并结合先进的NLP模型实现多语言翻译和实时问答。

Result: 系统能够将英文医疗手册转化为本地语言，帮助患者和医护人员实时获取准确信息。

Conclusion: 该框架有效提升了医疗文档的可及性，支持跨文化医疗信息传播。

Abstract: Millions of people in African countries face barriers to accessing healthcare
due to language and literacy gaps. This research tackles this challenge by
transforming complex medical documents -- in this case, prosthetic device user
manuals -- into accessible formats for underserved populations. This case study
in cross-cultural translation is particularly pertinent/relevant for
communities that receive donated prosthetic devices but may not receive the
accompanying user documentation. Or, if available online, may only be available
in formats (e.g., language and readability) that are inaccessible to local
populations (e.g., English-language, high resource settings/cultural context).
The approach is demonstrated using the widely spoken Pidgin dialect, but our
open-source framework has been designed to enable rapid and easy extension to
other languages/dialects. This work presents an AI-powered framework designed
to process and translate complex medical documents, e.g., user manuals for
prosthetic devices, into marginalised languages. The system enables users --
such as healthcare workers or patients -- to upload English-language medical
equipment manuals, pose questions in their native language, and receive
accurate, localised answers in real time. Technically, the system integrates a
Retrieval-Augmented Generation (RAG) pipeline for processing and semantic
understanding of the uploaded manuals. It then employs advanced Natural
Language Processing (NLP) models for generative question-answering and
multilingual translation. Beyond simple translation, it ensures accessibility
to device instructions, treatment protocols, and safety information, empowering
patients and clinicians to make informed healthcare decisions.

</details>


### [183] [ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.23960)
*Mingfei Cheng,Xiaofei Xie,Renzhi Wang,Yuan Zhou,Ming Hu*

Key words: 自动驾驶系统,在线修复,Transformer,ADReFT

TL;DR: 提出了一种名为ADReFT的自适应决策修复方法，通过离线学习失败测试提升自动驾驶系统的在线修复能力，显著改善修复效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有在线修复方法缺乏通用性和适应性，且过于保守，导致修复效果不佳。

Method: ADReFT采用基于Transformer的模型，联合状态监控器和决策适配器，通过监督学习和强化学习进行预训练和微调。

Result: ADReFT在修复性能上表现优于现有方法。

Conclusion: ADReFT有效提升了自动驾驶系统的安全性和驾驶体验。

Abstract: Autonomous Driving Systems (ADSs) continue to face safety-critical risks due
to the inherent limitations in their design and performance capabilities.
Online repair plays a crucial role in mitigating such limitations, ensuring the
runtime safety and reliability of ADSs. Existing online repair solutions
enforce ADS compliance by transforming unacceptable trajectories into
acceptable ones based on predefined specifications, such as rule-based
constraints or training datasets. However, these approaches often lack
generalizability, adaptability and tend to be overly conservative, resulting in
ineffective repairs that not only fail to mitigate safety risks sufficiently
but also degrade the overall driving experience. To address this issue, we
propose Adaptive Decision Repair (ADReFT), a novel and effective repair method
that identifies safety-critical states through offline learning from failed
tests and generates appropriate mitigation actions to improve ADS safety.
Specifically, ADReFT incorporates a transformer-based model with two joint
heads, State Monitor and Decision Adapter, designed to capture complex driving
environment interactions to evaluate state safety severity and generate
adaptive repair actions. Given the absence of oracles for state safety
identification, we first pretrain ADReFT using supervised learning with coarse
annotations, i.e., labeling states preceding violations as positive samples and
others as negative samples. It establishes ADReFT's foundational capability to
mitigate safety-critical violations, though it may result in somewhat
conservative mitigation strategies. Therefore, we subsequently finetune ADReFT
using reinforcement learning to improve its initial capability and generate
more precise and contextually appropriate repair decisions. Our evaluation
results illustrate that ADReFT achieves better repair performance.

</details>


### [184] [UMA: A Family of Universal Models for Atoms](https://arxiv.org/abs/2506.23971)
*Brandon M. Wood,Misko Dzamba,Xiang Fu,Meng Gao,Muhammed Shuaibi,Luis Barroso-Luque,Kareem Abdelmaqsoud,Vahe Gharakhanyan,John R. Kitchin,Daniel S. Levine,Kyle Michel,Anuroop Sriram,Taco Cohen,Abhishek Das,Ammar Rizvi,Sushree Jagriti Sahoo,Zachary W. Ulissi,C. Lawrence Zitnick*

Key words: UMA, 原子模拟, 混合线性专家, AI模型

TL;DR: Meta FAIR开发了通用原子模型(UMA)，以提升原子模拟的速度、准确性和泛化能力，适用于多个化学领域。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 快速准确计算原子模拟性质对化学和材料科学至关重要，如药物发现和能源存储。

Method: UMA基于半亿个3D原子结构数据训练，采用混合线性专家架构，支持模型容量扩展。

Result: UMA模型在多个领域表现优异，无需微调即可媲美专业模型。

Conclusion: UMA的代码和权重已公开，旨在推动AI模型的发展。

Abstract: The ability to quickly and accurately compute properties from atomic
simulations is critical for advancing a large number of applications in
chemistry and materials science including drug discovery, energy storage, and
semiconductor manufacturing. To address this need, Meta FAIR presents a family
of Universal Models for Atoms (UMA), designed to push the frontier of speed,
accuracy, and generalization. UMA models are trained on half a billion unique
3D atomic structures (the largest training runs to date) by compiling data
across multiple chemical domains, e.g. molecules, materials, and catalysts. We
develop empirical scaling laws to help understand how to increase model
capacity alongside dataset size to achieve the best accuracy. The UMA small and
medium models utilize a novel architectural design we refer to as mixture of
linear experts that enables increasing model capacity without sacrificing
speed. For example, UMA-medium has 1.4B parameters but only ~50M active
parameters per atomic structure. We evaluate UMA models on a diverse set of
applications across multiple domains and find that, remarkably, a single model
without any fine-tuning can perform similarly or better than specialized
models. We are releasing the UMA code, weights, and associated data to
accelerate computational workflows and enable the community to continue to
build increasingly capable AI models.

</details>


### [185] [A Scalable Approach for Safe and Robust Learning via Lipschitz-Constrained Networks](https://arxiv.org/abs/2506.23977)
*Zain ul Abdeen,Vassilis Kekatos,Ming Jin*

Key words: 认证鲁棒性, Lipschitz约束, 凸训练, 半定松弛, RS-LMI

TL;DR: 本文提出了一种通过半定松弛实现全局Lipschitz约束的凸训练框架，解决了传统方法因非凸性和全局半定规划导致的扩展性问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在安全关键应用中，神经网络的认证鲁棒性至关重要，但现有方法因非凸性和全局半定规划的复杂性而难以扩展。

Method: 采用循环变换重新参数化网络，提出凸可接受条件，并设计了随机子空间线性矩阵不等式（RS-LMI）方法分解全局约束。

Result: 在MNIST、CIFAR-10和ImageNet上验证了方法的竞争性准确性和显著改进的Lipschitz边界与运行时性能。

Conclusion: 该框架提供了可扩展且鲁棒的训练方法，适用于大规模网络。

Abstract: Certified robustness is a critical property for deploying neural networks
(NN) in safety-critical applications. A principle approach to achieving such
guarantees is to constrain the global Lipschitz constant of the network.
However, accurate methods for Lipschitz-constrained training often suffer from
non-convex formulations and poor scalability due to reliance on global
semidefinite programs (SDPs). In this letter, we propose a convex training
framework that enforces global Lipschitz constraints via semidefinite
relaxation. By reparameterizing the NN using loop transformation, we derive a
convex admissibility condition that enables tractable and certifiable training.
While the resulting formulation guarantees robustness, its scalability is
limited by the size of global SDP. To overcome this, we develop a randomized
subspace linear matrix inequalities (RS-LMI) approach that decomposes the
global constraints into sketched layerwise constraints projected onto
low-dimensional subspaces, yielding a smooth and memory-efficient training
objective. Empirical results on MNIST, CIFAR-10, and ImageNet demonstrate that
the proposed framework achieves competitive accuracy with significantly
improved Lipschitz bounds and runtime performance.

</details>


### [186] [LLM Agents Are the Antidote to Walled Gardens](https://arxiv.org/abs/2506.23978)
*Samuele Marro,Philip Torr*

Key words: LLM, 互操作性, 数据可迁移性, 安全风险, 技术负债

TL;DR: 论文讨论了如何利用基于LLM的智能体实现普遍互操作性，打破封闭平台的垄断，但也提到可能带来的新安全风险和技术负债。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有应用层被封闭平台主导，缺乏互操作性，阻碍数据流通和用户自由。LLM智能体可以低成本实现自动化数据转换，推动开放互操作。

Method: 提出通过LLM智能体作为中介，自动翻译数据格式并与人机界面交互，降低互操作性成本。

Result: 普遍互操作性有望打破垄断，促进数据可迁移性，但也可能引发新的安全风险和技术负债。

Conclusion: ML社区应拥抱这一趋势，同时构建框架以降低风险，利用AI恢复用户自由和市场竞争。

Abstract: While the Internet's core infrastructure was designed to be open and
universal, today's application layer is dominated by closed, proprietary
platforms. Open and interoperable APIs require significant investment, and
market leaders have little incentive to enable data exchange that could erode
their user lock-in. We argue that LLM-based agents fundamentally disrupt this
status quo. Agents can automatically translate between data formats and
interact with interfaces designed for humans: this makes interoperability
dramatically cheaper and effectively unavoidable. We name this shift universal
interoperability: the ability for any two digital services to exchange data
seamlessly using AI-mediated adapters. Universal interoperability undermines
monopolistic behaviours and promotes data portability. However, it can also
lead to new security risks and technical debt. Our position is that the ML
community should embrace this development while building the appropriate
frameworks to mitigate the downsides. By acting now, we can harness AI to
restore user freedom and competitive markets without sacrificing security.

</details>


### [187] [The Jacobian and Hessian of the Kullback-Leibler Divergence between Multivariate Gaussian Distributions (Technical Report)](https://arxiv.org/abs/2506.23996)
*Juan Maroñas*

Key words: Kullback-Leibler散度, Jacobian矩阵, Hessian矩阵, 多元高斯分布

TL;DR: 本文展示了如何通过一阶和二阶微分推导两个多元高斯分布的Kullback-Leibler散度的Jacobian和Hessian矩阵，并提供了详细推导过程。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在以教学的方式展示Kullback-Leibler散度的Jacobian和Hessian矩阵的推导方法。

Method: 基于Magnus和Minka的理论，通过一阶和二阶微分进行详细推导。

Result: 提供了Kullback-Leibler散度的Jacobian和Hessian矩阵的完整推导结果。

Conclusion: 文档以教学为目的，通过分步推导帮助读者理解相关数学工具和概念。

Abstract: This document shows how to obtain the Jacobian and Hessian matrices of the
Kullback-Leibler divergence between two multivariate Gaussian distributions,
using the first and second-order differentials. The presented derivations are
based on the theory presented by \cite{magnus99}. I've also got great
inspiration from some of the derivations in \cite{minka}.
  Since I pretend to be at most didactic, the document is split into a summary
of results and detailed derivations on each of the elements involved, with
specific references to the tricks used in the derivations, and to many of the
underlying concepts.

</details>


### [188] [The Illusion of Progress? A Critical Look at Test-Time Adaptation for Vision-Language Models](https://arxiv.org/abs/2506.24000)
*Lijun Sheng,Jian Liang,Ran He,Zilei Wang,Tieniu Tan*

Key words: 测试时间适应, 视觉语言模型, 基准测试, CLIP, SigLIP

TL;DR: TTA-VLM是一个综合性基准测试，旨在评估视觉语言模型（VLM）的测试时间适应（TTA）方法，通过统一框架和多种评估指标解决现有研究中的局限性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有TTA研究存在结果重复、评估指标有限、实验设置不一致和缺乏分析等问题，阻碍了公平比较和实际应用。

Method: TTA-VLM实现了8种episodic TTA和7种online TTA方法，并在15个数据集上进行了评估，扩展了对SigLIP模型的评估，并包含训练时调优方法。

Result: 实验发现：1）现有TTA方法改进有限；2）TTA与训练时调优方法协作不佳；3）准确性提升常以降低模型可信度为代价。

Conclusion: TTA-VLM提供了公平的比较和全面的评估，鼓励开发更可靠和通用的TTA策略。

Abstract: Test-time adaptation (TTA) methods have gained significant attention for
enhancing the performance of vision-language models (VLMs) such as CLIP during
inference, without requiring additional labeled data. However, current TTA
researches generally suffer from major limitations such as duplication of
baseline results, limited evaluation metrics, inconsistent experimental
settings, and insufficient analysis. These problems hinder fair comparisons
between TTA methods and obscure their practical strengths and weaknesses. To
address these challenges, we introduce TTA-VLM, a comprehensive benchmark for
evaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7
online TTA methods within a unified and reproducible framework, and evaluates
them across 15 widely used datasets. Unlike prior studies focused solely on
CLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid
loss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA
to assess generality. Beyond classification accuracy, TTA-VLM incorporates
various evaluation metrics, including robustness, calibration,
out-of-distribution detection, and stability, enabling a more holistic
assessment of TTA methods. Through extensive experiments, we find that 1)
existing TTA methods produce limited gains compared to the previous pioneering
work; 2) current TTA methods exhibit poor collaboration with training-time
fine-tuning methods; 3) accuracy gains frequently come at the cost of reduced
model trustworthiness. We release TTA-VLM to provide fair comparison and
comprehensive evaluation of TTA methods for VLMs, and we hope it encourages the
community to develop more reliable and generalizable TTA strategies.

</details>


### [189] [Provably Efficient and Agile Randomized Q-Learning](https://arxiv.org/abs/2506.24005)
*He Wang,Xingyu Xu,Yuejie Chi*

Key words: 强化学习, Q学习, 探索, 后悔上界, 贝叶斯, 采样

TL;DR: 提出了一种名为RandomizedQ的新型Q学习算法，将基于采样的探索与敏捷的步进式策略更新相结合，在表格强化学习中实现了优秀的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的基于贝叶斯的探索方法在模型强化学习中表现优异，但在模型无关环境中缺乏理论支持，且现有算法存在计算复杂度高或策略更新响应慢的问题。

Method: 提出RandomizedQ算法，通过基于采样的探索与步进式策略更新结合，优化Q学习过程。

Result: 理论分析显示其后悔上界为$	ilde{O}(\sqrt{H^5SAT})$，并在特定条件下实现对数后悔。实验表明，RandomizedQ在标准基准测试中优于现有的基于奖励和贝叶斯的Q学习变体。

Conclusion: RandomizedQ在理论和实践中均表现出色，解决了现有算法的局限性。

Abstract: While Bayesian-based exploration often demonstrates superior empirical
performance compared to bonus-based methods in model-based reinforcement
learning (RL), its theoretical understanding remains limited for model-free
settings. Existing provable algorithms either suffer from computational
intractability or rely on stage-wise policy updates which reduce responsiveness
and slow down the learning process. In this paper, we propose a novel variant
of Q-learning algorithm, refereed to as RandomizedQ, which integrates
sampling-based exploration with agile, step-wise, policy updates, for episodic
tabular RL. We establish an $\widetilde{O}(\sqrt{H^5SAT})$ regret bound, where
$S$ is the number of states, $A$ is the number of actions, $H$ is the episode
length, and $T$ is the total number of episodes. In addition, we present a
logarithmic regret bound under a mild positive sub-optimality condition on the
optimal Q-function. Empirically, RandomizedQ exhibits outstanding performance
compared to existing Q-learning variants with both bonus-based and
Bayesian-based exploration on standard benchmarks.

</details>


### [190] [Bridging Theory and Practice in Link Representation with Graph Neural Networks](https://arxiv.org/abs/2506.24018)
*Veronica Lachi,Francesco Ferrini,Antonio Longa,Bruno Lepri,Andrea Passerini,Manfred Jaeger*

Key words: 图神经网络, 链接表示, 表达能力, 消息传递, 对称性

TL;DR: 该论文首次全面研究了图神经网络（GNN）在链接表示中的表达能力，提出了一个统一的分析框架，并提出了一个合成的评估协议来验证链接级别的表达能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管GNN广泛应用于链接预测等任务，但对其表达能力的理论理解主要集中在图级别表示上，而链接级别的表达能力尚未被系统研究。

Method: 引入一个统一的分析框架$k_\phi$-$k_\rho$-$m$，包含现有消息传递链接模型，并提供形式化的表达能力比较。

Result: 提出了一个现有方法的分层结构，并设计了一个合成评估协议来测试链接级别的表达能力。实践表明，表达能力强的模型在处理高对称性图时表现更优。

Conclusion: 研究表明，表达能力强的模型在高对称性图中表现更好，强调了数据集意识模型选择的重要性。

Abstract: Graph Neural Networks (GNNs) are widely used to compute representations of
node pairs for downstream tasks such as link prediction. Yet, theoretical
understanding of their expressive power has focused almost entirely on
graph-level representations. In this work, we shift the focus to links and
provide the first comprehensive study of GNN expressiveness in link
representation. We introduce a unifying framework, the $k_\phi$-$k_\rho$-$m$
framework, that subsumes existing message-passing link models and enables
formal expressiveness comparisons. Using this framework, we derive a hierarchy
of state-of-the-art methods and offer theoretical tools to analyze future
architectures. To complement our analysis, we propose a synthetic evaluation
protocol comprising the first benchmark specifically designed to assess
link-level expressiveness. Finally, we ask: does expressiveness matter in
practice? We use a graph symmetry metric that quantifies the difficulty of
distinguishing links and show that while expressive models may underperform on
standard benchmarks, they significantly outperform simpler ones as symmetry
increases, highlighting the need for dataset-aware model selection.

</details>


### [191] [Faster Diffusion Models via Higher-Order Approximation](https://arxiv.org/abs/2506.24042)
*Gen Li,Yuchen Zhou,Yuting Wei,Yuxin Chen*

Key words: 扩散模型,采样加速,高阶ODE,总变差距离

TL;DR: 提出了一种无需额外训练的扩散模型加速采样算法，显著减少了所需评分函数评估次数，适用于广泛的非平滑数据分布。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在解决扩散模型采样效率低的问题，无需重新训练即实现加速。

Method: 基于高阶ODE解算器，利用拉格朗日插值和逐步细化近似概率流ODE积分，设计训练自由的采样算法。

Result: 算法在准确评分下仅需O(d^{1+2/K}ε^{-1/K})次评分评估，对评分估计误差有鲁棒性。

Conclusion: 该算法高效且适用范围广，无需目标分布平滑或对数凹性假设。

Abstract: In this paper, we explore provable acceleration of diffusion models without
any additional retraining. Focusing on the task of approximating a target data
distribution in $\mathbb{R}^d$ to within $\varepsilon$ total-variation
distance, we propose a principled, training-free sampling algorithm that
requires only the order of
  $$ d^{1+2/K} \varepsilon^{-1/K} $$
  score function evaluations (up to log factor) in the presence of accurate
scores, where $K$ is an arbitrarily large fixed integer. This result applies to
a broad class of target data distributions, without the need for assumptions
such as smoothness or log-concavity. Our theory is robust vis-a-vis inexact
score estimation, degrading gracefully as the score estimation error increases
-- without demanding higher-order smoothness on the score estimates as assumed
in previous work. The proposed algorithm draws insight from high-order ODE
solvers, leveraging high-order Lagrange interpolation and successive refinement
to approximate the integral derived from the probability flow ODE.

</details>


### [192] [Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies](https://arxiv.org/abs/2506.24093)
*Paul Wachter,Lukas Niehaus,Julius Schöning*

Key words: 合成数据, 人工神经网络, 域间隙, 混合训练, 鲁棒性

TL;DR: 论文探讨了合成数据在训练人工神经网络（ANN）时的应用，分析了两种混合策略在不同架构和数据集上的效果，为优化合成数据使用提供了见解。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 合成数据作为真实数据的替代品具有成本效益，但与真实数据的差异导致域间隙，影响ANN的性能和泛化能力。

Method: 研究全面分析了两种混合策略在三种架构和三种混合数据集上的表现，并考察了不同比例合成与真实数据的影响。

Result: 研究发现混合策略可以缓解域间隙，为合成数据的优化使用提供了具体指导。

Conclusion: 论文为提升ANN的鲁棒性和效能提供了有价值的见解，特别是在合成数据与真实数据的混合使用方面。

Abstract: Synthetic data has emerged as a cost-effective alternative to real data for
training artificial neural networks (ANN). However, the disparity between
synthetic and real data results in a domain gap. That gap leads to poor
performance and generalization of the trained ANN when applied to real-world
scenarios. Several strategies have been developed to bridge this gap, which
combine synthetic and real data, known as mixed training using hybrid datasets.
While these strategies have been shown to mitigate the domain gap, a systematic
evaluation of their generalizability and robustness across various tasks and
architectures remains underexplored. To address this challenge, our study
comprehensively analyzes two widely used mixing strategies on three prevalent
architectures and three distinct hybrid datasets. From these datasets, we
sample subsets with varying proportions of synthetic to real data to
investigate the impact of synthetic and real components. The findings of this
paper provide valuable insights into optimizing the use of synthetic data in
the training process of any ANN, contributing to enhancing robustness and
efficacy.

</details>


### [193] [Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime](https://arxiv.org/abs/2506.24120)
*Yuqing Wang,Shangding Gu*

Key words: 数据选择，均匀分布，梯度下降，深度学习，大语言模型

TL;DR: 论文探讨了数据选择中更均匀分布的原则，证明了其能提升训练效率和模型性能，并提出了一种适用于广泛架构的收敛框架。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究在数据选择中是否存在普遍适用的定量原则，以提升复杂任务中的模型性能。

Method: 通过理论和实验证明更均匀分布的数据能增大最小点间距离，从而加速梯度下降训练并减少近似误差。

Result: 实验表明最大化点间距离的数据选择策略能显著加速训练，并在多种数据集上实现可比或更优性能。

Conclusion: 均匀分布的数据选择是提升训练效率和模型性能的有效原则，适用于多种深度学习架构。

Abstract: Data selection plays a crucial role in data-driven decision-making, including
in large language models (LLMs), and is typically task-dependent. Properties
such as data quality and diversity have been extensively studied and are known
to enhance model performance. However, it remains unclear whether there exist
other quantitative and general principles of data selection that can
consistently improve performance, especially for complex tasks with limited
prior knowledge. In this paper, we demonstrate that selecting more uniformly
distributed data can improve training efficiency while enhancing performance.
Specifically, we establish that more uniform (less biased) distribution leads
to a larger minimum pairwise distance between data points, denoted by
$h_{\min}$, and prove that a smaller $h_{\min}$ can slow down the training
dynamics of gradient descent (GD). Moreover, we theoretically show that the
approximation error of neural networks decreases as $h_{\min}$ increases. Our
analysis introduces a convergence framework for GD beyond the Neural Tangent
Kernel (NTK) regime, applicable to a broad class of architectures, including
transformers, without requiring Lipschitz smoothness. This framework further
provides theoretical justification for the use of residual connections and
function compositions in deep neural architectures. In the end, we conduct
comprehensive experiments for supervised fine-tuning across various settings,
including different optimization strategies, model sizes, and training
datasets. The results consistently demonstrate that selecting data by
maximizing pairwise distance significantly accelerates training and achieves
comparable or better performance in LLMs across diverse datasets. Code and
Datasets are available at the link:
https://github.com/SafeRL-Lab/data-uniformity.

</details>


### [194] [Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives](https://arxiv.org/abs/2506.24124)
*Dong Sixun,Fan Wei,Teresa Wu,Fu Yanjie*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Time series forecasting traditionally relies on unimodal numerical inputs,
which often struggle to capture high-level semantic patterns due to their dense
and unstructured nature. While recent approaches have explored representing
time series as text using large language models (LLMs), these methods remain
limited by the discrete nature of token sequences and lack the perceptual
intuition humans typically apply, such as interpreting visual patterns. In this
paper, we propose a multimodal contrastive learning framework that transforms
raw time series into structured visual and textual perspectives. Rather than
using natural language or real-world images, we construct both modalities
directly from numerical sequences. We then align these views in a shared
semantic space via contrastive learning, enabling the model to capture richer
and more complementary representations. Furthermore, we introduce a variate
selection module that leverages the aligned representations to identify the
most informative variables for multivariate forecasting. Extensive experiments
on fifteen short-term and six long-term forecasting benchmarks demonstrate that
our approach consistently outperforms strong unimodal and cross-modal
baselines, highlighting the effectiveness of multimodal alignment in enhancing
time series forecasting. Code is available at:
https://github.com/Ironieser/TimesCLIP.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [195] [Bootstrapping Human-Like Planning via LLMs](https://arxiv.org/abs/2506.22604)
*David Porfirio,Vincent Hsiao,Morgan Fine-Morris,Leslie Smith,Laura M. Hiatt*

Key words: 机器人编程、自然语言处理、大语言模型、拖拽界面

TL;DR: 论文探讨结合自然语言和拖拽界面两种编程范式，利用LLM生成类人动作序列，并与人工指定序列比较，结果显示大模型表现更优。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 为满足机器人任务编程的易用性需求，结合自然语言与拖拽界面的优势，研究两者结合的潜力。

Method: 构建基于LLM的流程，接收自然语言输入并生成类人动作序列，与人工指定序列进行对比。

Result: 大模型生成类人动作序列表现更优，但小模型亦可达到满意效果。

Conclusion: 结合自然语言与拖拽界面的方法是可行的，且模型规模对性能有显著影响。

Abstract: Robot end users increasingly require accessible means of specifying tasks for
robots to perform. Two common end-user programming paradigms include
drag-and-drop interfaces and natural language programming. Although natural
language interfaces harness an intuitive form of human communication,
drag-and-drop interfaces enable users to meticulously and precisely dictate the
key actions of the robot's task. In this paper, we investigate the degree to
which both approaches can be combined. Specifically, we construct a large
language model (LLM)-based pipeline that accepts natural language as input and
produces human-like action sequences as output, specified at a level of
granularity that a human would produce. We then compare these generated action
sequences to another dataset of hand-specified action sequences. Although our
results reveal that larger models tend to outperform smaller ones in the
production of human-like action sequences, smaller models nonetheless achieve
satisfactory performance.

</details>


### [196] [Ludax: A GPU-Accelerated Domain Specific Language for Board Games](https://arxiv.org/abs/2506.22609)
*Graham Todd,Alexander G. Padula,Dennis J. N. J. Soemers,Julian Togelius*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Games have long been used as benchmarks and testing environments for research
in artificial intelligence. A key step in supporting this research was the
development of game description languages: frameworks that compile
domain-specific code into playable and simulatable game environments, allowing
researchers to generalize their algorithms and approaches across multiple games
without having to manually implement each one. More recently, progress in
reinforcement learning (RL) has been largely driven by advances in hardware
acceleration. Libraries like JAX allow practitioners to take full advantage of
cutting-edge computing hardware, often speeding up training and testing by
orders of magnitude. Here, we present a synthesis of these strands of research:
a domain-specific language for board games which automatically compiles into
hardware-accelerated code. Our framework, Ludax, combines the generality of
game description languages with the speed of modern parallel processing
hardware and is designed to fit neatly into existing deep learning pipelines.
We envision Ludax as a tool to help accelerate games research generally, from
RL to cognitive science, by enabling rapid simulation and providing a flexible
representation scheme. We present a detailed breakdown of Ludax's description
language and technical notes on the compilation process, along with speed
benchmarking and a demonstration of training RL agents. The Ludax framework,
along with implementations of existing board games, is open-source and freely
available.

</details>


### [197] [URSA: The Universal Research and Scientific Agent](https://arxiv.org/abs/2506.22653)
*Michael Grosskopf,Russell Bent,Rahul Somasundaram,Isaac Michaud,Arthur Lui,Nathan Debardeleben,Earl Lawrence*

Key words: 大语言模型（LLMs）、科学代理、URSA、科研加速、模块化工具

TL;DR: 该论文介绍了URSA，一个基于大语言模型（LLMs）的科学代理生态系统，旨在通过模块化代理和工具加速科研任务。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 利用LLMs的复杂推理、规划和编码能力，解决科研中复杂的瓶颈问题，推动科学前沿的进展。

Method: 开发了URSA系统，包含模块化代理和工具，并集成了高级物理模拟代码，以应对不同复杂度和影响的科学问题。

Result: 论文展示了URSA的架构及其在加速科研任务中的潜力。

Conclusion: URSA展示了LLMs在科学代理中的革命性潜力，有望显著提升科研效率。

Abstract: Large language models (LLMs) have moved far beyond their initial form as
simple chatbots, now carrying out complex reasoning, planning, writing, coding,
and research tasks. These skills overlap significantly with those that human
scientists use day-to-day to solve complex problems that drive the cutting edge
of research. Using LLMs in "agentic" AI has the potential to revolutionize
modern science and remove bottlenecks to progress. In this work, we present
URSA, a scientific agent ecosystem for accelerating research tasks. URSA
consists of a set of modular agents and tools, including coupling to advanced
physics simulation codes, that can be combined to address scientific problems
of varied complexity and impact. This work highlights the architecture of URSA,
as well as examples that highlight the potential of the system.

</details>


### [198] [Explanations are a means to an end](https://arxiv.org/abs/2506.22740)
*Jessica Hullman,Ziyang Guo,Berk Ustun*

Key words: 可解释机器学习, 统计决策理论, 临床决策, 评价框架

TL;DR: 可解释机器学习方法需以特定目标为导向设计，基于统计决策理论框架，强调实践应用与评价。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有可解释机器学习方法缺乏对实际应用场景的考虑，需改进以明确解释的目标和用途。

Method: 采用统计决策理论框架，形式化解释的目标，并在临床决策等多样化用例中验证。

Result: 提出了一种评价解释性能的方法，能量化其对理想决策者的潜在提升，避免因模糊性导致的误用。

Conclusion: 解释的设计和评价需结合理论与实际，明确定义目标和用途。

Abstract: Modern methods for explainable machine learning are designed to describe how
models map inputs to outputs--without deep consideration of how these
explanations will be used in practice. This paper argues that explanations
should be designed and evaluated with a specific end in mind. We describe how
to formalize this end in a framework based in statistical decision theory. We
show how this functionally-grounded approach can be applied across diverse use
cases, such as clinical decision support, providing recourse, or debugging. We
demonstrate its use to characterize the maximum "boost" in performance on a
particular task that an explanation could provide an idealized decision-maker,
preventing misuse due to ambiguity by forcing researchers to specify concrete
use cases that can be analyzed in light of models of expected explanation use.
We argue that evaluation should meld theoretical and empirical perspectives on
the value of explanation, and contribute definitions that span these
perspectives.

</details>


### [199] [Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems](https://arxiv.org/abs/2506.22774)
*Michael Papademas,Xenia Ziouvelou,Antonis Troumpoukis,Vangelis Karkaletsis*

Key words: 人工智能,可信度评估,PageRank,TrustRank,伦理组件

TL;DR: 论文提出了一种结合伦理组件与PageRank和TrustRank算法的评估方法，旨在量化AI系统的可信度。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: AI技术的复杂性和影响力使其风险难以量化，现有指南和工具各有局限。

Method: 结合伦理组件与PageRank和TrustRank算法，建立评估框架。

Result: 通过算法标准量化可信度，兼顾理论指南内容。

Conclusion: 该方法实现了AI系统可信度的全面评估，减少了主观性。

Abstract: Artificial Intelligence (AI) technology epitomizes the complex challenges
posed by human-made artifacts, particularly those widely integrated into
society and exert significant influence, highlighting potential benefits and
their negative consequences. While other technologies may also pose substantial
risks, AI's pervasive reach makes its societal effects especially profound. The
complexity of AI systems, coupled with their remarkable capabilities, can lead
to a reliance on technologies that operate beyond direct human oversight or
understanding. To mitigate the risks that arise, several theoretical tools and
guidelines have been developed, alongside efforts to create technological tools
aimed at safeguarding Trustworthy AI. The guidelines take a more holistic view
of the issue but fail to provide techniques for quantifying trustworthiness.
Conversely, while technological tools are better at achieving such
quantification, they lack a holistic perspective, focusing instead on specific
aspects of Trustworthy AI. This paper aims to introduce an assessment method
that combines the ethical components of Trustworthy AI with the algorithmic
processes of PageRank and TrustRank. The goal is to establish an assessment
framework that minimizes the subjectivity inherent in the self-assessment
techniques prevalent in the field by introducing algorithmic criteria. The
application of our approach indicates that a holistic assessment of an AI
system's trustworthiness can be achieved by providing quantitative insights
while considering the theoretical content of relevant guidelines.

</details>


### [200] [ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models](https://arxiv.org/abs/2506.22865)
*Ziqi Zhong,Xunzhu Tang*

Key words: LLMs, 推理能力, 知识蒸馏, 开源模型, 分层框架

TL;DR: 该论文提出了ReasonBridge方法，通过分层知识蒸馏框架，高效地将复杂推理能力从闭源模型迁移到开源模型，仅需1000条精心筛选的推理轨迹数据。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前大型语言模型（LLMs）在复杂推理任务上，闭源与开源模型之间存在显著性能差距，因此需要一种高效方法提升开源模型的推理能力。

Method: 1. 分层蒸馏过程同时捕捉策略抽象和战术实现模式；2. 稀疏推理适配器架构仅需0.3%额外可训练参数；3. 推理过程中采用计算缩放机制。

Result: ReasonBridge将开源模型的推理能力提升23%，在MATH500上超越Claude-Sonnet3.5，并在AIME竞赛级任务上表现相当。

Conclusion: 该方法为提升指令遵循的推理能力提供了一种高效且通用的途径。

Abstract: Recent advancements in Large Language Models (LLMs) have revealed a
significant performance gap between closed-source and open-source models,
particularly in tasks requiring complex reasoning and precise instruction
following. This paper introduces ReasonBridge, a methodology that efficiently
transfers reasoning capabilities from powerful closed-source to open-source
models through a novel hierarchical knowledge distillation framework. We
develop a tailored dataset Reason1K with only 1,000 carefully curated reasoning
traces emphasizing difficulty, diversity, and quality. These traces are
filtered from across multiple domains using a structured multi-criteria
selection algorithm. Our transfer learning approach incorporates: (1) a
hierarchical distillation process capturing both strategic abstraction and
tactical implementation patterns, (2) a sparse reasoning-focused adapter
architecture requiring only 0.3% additional trainable parameters, and (3) a
test-time compute scaling mechanism using guided inference interventions.
Comprehensive evaluations demonstrate that ReasonBridge improves reasoning
capabilities in open-source models by up to 23% on benchmark tasks,
significantly narrowing the gap with closed-source models. Notably, the
enhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its
performance on competition-level AIME problems. Our methodology generalizes
effectively across diverse reasoning domains and model architectures,
establishing a sample-efficient approach to reasoning enhancement for
instruction following.

</details>


### [201] [Agentic Enterprise: AI-Centric User to User-Centric AI](https://arxiv.org/abs/2506.22893)
*Arpit Narechania,Alex Endert,Atanu R Sinha*

Key words: 人工智能,企业决策,代理,用户中心设计,市场机制

TL;DR: 论文探讨AI如何提升企业决策效率，提出六个原则以实现"代理成功"，并强调从AI中心转向用户中心的设计理念。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: AI的快速发展为企业决策提供了新机会，但目前AI中心范式未能充分满足企业需求，需转向用户中心设计。

Method: 通过分析企业决策需求，提出六个"代理成功"原则，并建议利用市场机制设计用户中心的AI平台。

Result: 论文为企业AI代理设计提供了实用框架，强调了用户需求和平台机制的重要性。

Conclusion: 用户中心的AI设计能更有效地满足企业决策需求，市场机制是成功实施的关键。

Abstract: After a very long winter, the Artificial Intelligence (AI) spring is here.
Or, so it seems over the last three years. AI has the potential to impact many
areas of human life - personal, social, health, education, professional. In
this paper, we take a closer look at the potential of AI for Enterprises, where
decision-making plays a crucial and repeated role across functions, tasks, and
operations. We consider Agents imbued with AI as means to increase
decision-productivity of enterprises. We highlight six tenets for Agentic
success in enterprises, by drawing attention to what the current, AI-Centric
User paradigm misses, in the face of persistent needs of and usefulness for
Enterprise Decision-Making. In underscoring a shift to User-Centric AI, we
offer six tenets and promote market mechanisms for platforms, aligning the
design of AI and its delivery by Agents to the cause of enterprise users.

</details>


### [202] [Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning](https://arxiv.org/abs/2506.22919)
*Sanskar Pandey,Ruhaan Chopra,Saad Murtaza Bhat,Ark Abhyudaya*

Key words: Mixture-of-Experts, conditional computation, GRU, FFNN, interpretability

TL;DR: Hecto是一种轻量级的混合专家（MoE）架构，通过结合GRU和FFNN专家实现异构计算，提升任务专业化和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前MoE模型中专家共享相同的归纳偏置，限制了表示多样性，且静态计算路径效率低下。

Method: Hecto结合GRU（时序推理）和FFNN（静态抽象）专家，采用稀疏Top-1门控机制。

Result: 在多个推理和回归任务中表现优异，专家专业化明显，计算约束下性能提升。

Conclusion: Hecto为条件计算设定了新基准，提供了低资源环境下专业化推理的框架。

Abstract: Mixture-of-Experts (MoE) models enable conditional computation by routing
inputs to specialized experts, but these experts rely on identical inductive
biases, thus limiting representational diversity. This static computation
pathway is inefficient for inputs that require different types of reasoning and
limits specialization and interpretability. We propose Hecto, a lightweight MoE
architecture that leverages architectural heterogeneity by combining a GRU
expert for temporal reasoning and an FFNN expert for static abstraction under a
sparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG
News, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely
trails homogeneous baselines in performance despite receiving isolated input
representations, while achieving clear expert specialization, with each expert
aligning to distinct reasoning types (temporal vs static). At larger batch
sizes, Hecto exhibits improved performance, benefiting from relaxed
computational constraints that allow its heterogeneous architecture to optimize
more effectively. Ablation results isolate architectural diversity as the
source of Hecto's stability and interpretability across diverse reasoning
tasks. Overall, Hecto establishes itself as a new benchmark for conditional
computation, offering a principled framework for specialized reasoning in
low-resource regimes with its model strength derived from principled
specialization.

</details>


### [203] [Improving Rationality in the Reasoning Process of Language Models through Self-playing Game](https://arxiv.org/abs/2506.22920)
*Pinzheng Wang,Juntao Li,Zecheng Tang,Haijia Gui,Min zhang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) have demonstrated considerable reasoning
abilities in various tasks such as mathematics and coding. However, recent
studies indicate that even the best models lack true comprehension of their
reasoning processes. In this paper, we explore how self-play can enhance the
rationality of models in the reasoning process without supervision from humans
or superior models. We design a Critic-Discernment Game(CDG) in which a prover
first provides a solution to a given problem and is subsequently challenged by
critiques of its solution. These critiques either aim to assist or mislead the
prover. The objective of the prover is to maintain the correct answer when
faced with misleading comments, while correcting errors in response to
constructive feedback. Our experiments on tasks involving mathematical
reasoning, stepwise error detection, self-correction, and long-chain reasoning
demonstrate that CDG training can significantly improve the ability of
well-aligned LLMs to comprehend their reasoning process.

</details>


### [204] [MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning](https://arxiv.org/abs/2506.22992)
*Yulun Jiang,Yekun Chai,Maria Brbić,Michael Moor*

Key words: 多模态推理,基准测试,复杂推理,MARBLE,多模态语言模型

TL;DR: MARBLE是一个新的多模态推理基准测试，用于评估多模态语言模型的复杂推理能力，现有模型表现不佳。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 多模态复杂推理能力是人工智能领域的重点挑战，现有基准测试局限于文本或多模态直接检索。

Method: 设计了M-Portal和M-Cube两个高难度任务，要求模型在空间、视觉和物理约束下制定和理解多步计划。

Result: 12个先进模型在M-Portal上表现接近随机，M-Cube上准确率为0%，仅在简化子任务中部分模型优于随机基线。

Conclusion: 多模态复杂推理仍是现有模型的挑战，视觉感知是瓶颈，MARBLE有望推动新一代模型的发展。

Abstract: The ability to process information from multiple modalities and to reason
through it step-by-step remains a critical challenge in advancing artificial
intelligence. However, existing reasoning benchmarks focus on text-only
reasoning, or employ multimodal questions that can be answered by directly
retrieving information from a non-text modality. Thus, complex reasoning
remains poorly understood in multimodal domains. Here, we present MARBLE, a
challenging multimodal reasoning benchmark that is designed to scrutinize
multimodal language models (MLLMs) in their ability to carefully reason
step-by-step through complex multimodal problems and environments. MARBLE is
composed of two highly challenging tasks, M-Portal and M-Cube, that require the
crafting and understanding of multistep plans under spatial, visual, and
physical constraints. We find that current MLLMs perform poorly on MARBLE --
all the 12 advanced models obtain near-random performance on M-Portal and 0%
accuracy on M-Cube. Only in simplified subtasks some models outperform the
random baseline, indicating that complex reasoning is still a challenge for
existing MLLMs. Moreover, we show that perception remains a bottleneck, where
MLLMs occasionally fail to extract information from the visual inputs. By
shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the
development of the next generation of models with the ability to reason and
plan across many, multimodal reasoning steps.

</details>


### [205] [AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks](https://arxiv.org/abs/2506.23049)
*Leander Melroy Maben,Gayathri Ganesh Lakshmy,Srijith Radhakrishnan,Siddhant Arora,Shinji Watanabe*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite advances in language and speech technologies, no open-source system
enables full speech-to-speech, multi-turn dialogue with integrated tool use and
agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and
Automated Tool Use), the first open-source, speech-native assistant capable of
completing complex, goal-driven tasks through dynamic tool invocation and
multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a
cascaded pipeline and supports tools such as calendar booking, contact lookup,
web search, and email. Its modular design allows easy integration of new tools
using natural language prompts and action classes. On VoiceBench, AURA scores
92.75% on OpenBookQA-outperforming all open-weight systems and nearing
GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.
Human evaluation shows 90% task success on complex, multi-turn speech tasks.

</details>


### [206] [AI's Euclid's Elements Moment: From Language Models to Computable Thought](https://arxiv.org/abs/2506.23080)
*Xinmin Fang,Lingfeng Tao,Zhengxiong Li*

Key words: AI发展,认知技术,演化框架,神经符号架构,程序合成

TL;DR: 本文提出了一个五阶段的AI演进框架，类比人类认知技术的发展，预测了AI的未来发展方向。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索AI的发展路径，类比历史上的认知技术进步，提供一个系统化的跨学科模型。

Method: 提出五阶段演化框架，类比历史上的认知技术革命，分析AI当前的Metalinguistic Moment及未来阶段的特征。

Result: 提供了一个预测AI未来发展的理论框架，指出当前阶段及后续可能的技术趋势。

Conclusion: AI的发展具有反射性，未来的阶段将涉及神经符号架构和程序合成，最终实现可证明的对齐和可靠性。

Abstract: This paper presents a comprehensive five-stage evolutionary framework for
understanding the development of artificial intelligence, arguing that its
trajectory mirrors the historical progression of human cognitive technologies.
We posit that AI is advancing through distinct epochs, each defined by a
revolutionary shift in its capacity for representation and reasoning, analogous
to the inventions of cuneiform, the alphabet, grammar and logic, mathematical
calculus, and formal logical systems. This "Geometry of Cognition" framework
moves beyond mere metaphor to provide a systematic, cross-disciplinary model
that not only explains AI's past architectural shifts-from expert systems to
Transformers-but also charts a concrete and prescriptive path forward.
Crucially, we demonstrate that this evolution is not merely linear but
reflexive: as AI advances through these stages, the tools and insights it
develops create a feedback loop that fundamentally reshapes its own underlying
architecture. We are currently transitioning into a "Metalinguistic Moment,"
characterized by the emergence of self-reflective capabilities like
Chain-of-Thought prompting and Constitutional AI. The subsequent stages, the
"Mathematical Symbolism Moment" and the "Formal Logic System Moment," will be
defined by the development of a computable calculus of thought, likely through
neuro-symbolic architectures and program synthesis, culminating in provably
aligned and reliable AI that reconstructs its own foundational representations.
This work serves as the methodological capstone to our trilogy, which
previously explored the economic drivers ("why") and cognitive nature ("what")
of AI. Here, we address the "how," providing a theoretical foundation for
future research and offering concrete, actionable strategies for startups and
developers aiming to build the next generation of intelligent systems.

</details>


### [207] [Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study](https://arxiv.org/abs/2506.23107)
*Bing Song,Jianing Liu,Sisi Jian,Chenyang Wu,Vinayak Dixit*

Key words: 大语言模型, 风险决策, CRRA框架, 语言影响

TL;DR: 研究探讨大语言模型（LLM）在模拟复杂决策行为（如风险决策）中的可靠性，结果显示模型比人类更风险规避，且语言对模型表现有影响。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着大语言模型应用的扩展，评估其在复杂决策模拟中的可靠性变得重要。

Method: 比较LLM预测与实际人类在彩票任务中的选择，使用CRRA框架分析风险偏好。

Result: 模型比人类更风险规避；语言（中文）下模型表现偏差更大。

Conclusion: LLM在模拟人类风险行为中展现出潜力但仍有限制，尤其在语言和文化背景下。

Abstract: Large language models (LLMs) have made significant strides, extending their
applications to dialogue systems, automated content creation, and
domain-specific advisory tasks. However, as their use grows, concerns have
emerged regarding their reliability in simulating complex decision-making
behavior, such as risky decision-making, where a single choice can lead to
multiple outcomes. This study investigates the ability of LLMs to simulate
risky decision-making scenarios. We compare model-generated decisions with
actual human responses in a series of lottery-based tasks, using transportation
stated preference survey data from participants in Sydney, Dhaka, Hong Kong,
and Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and
ChatGPT o1-mini -- which were tasked with predicting individual choices. Risk
preferences were analyzed using the Constant Relative Risk Aversion (CRRA)
framework. Results show that both models exhibit more risk-averse behavior than
human participants, with o1-mini aligning more closely with observed human
decisions. Further analysis of multilingual data from Nanjing and Hong Kong
indicates that model predictions in Chinese deviate more from actual responses
compared to English, suggesting that prompt language may influence simulation
performance. These findings highlight both the promise and the current
limitations of LLMs in replicating human-like risk behavior, particularly in
linguistic and cultural settings.

</details>


### [208] [The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy](https://arxiv.org/abs/2506.23123)
*Rishi Bommasani*

Key words: 人工智能, 基础模型, 社会治理, 透明度, 政策

TL;DR: 这篇论文讨论了人工智能基础模型的潜力与风险，提出了技术与社会共同发展的框架，并通过实证研究和政策建议促进更好的AI治理。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索人工智能基础模型在社会中的影响及其治理，以应对其潜在危害并发挥其潜力。

Method: 围绕三个主题展开：概念框架（能力、风险、供应链）、实证研究（模型透明度和组织索引）、政策行动（基于证据的AI政策）。

Result: 通过科学基础和研究与政策的接口，为实现更好的社会成果奠定了基础。

Conclusion: 该论文为AI时代的治理提供了理论基础和实践路径，旨在促进技术与社会的协调发展。

Abstract: Artificial intelligence is humanity's most promising technology because of
the remarkable capabilities offered by foundation models. Yet, the same
technology brings confusion and consternation: foundation models are poorly
understood and they may precipitate a wide array of harms. This dissertation
explains how technology and society coevolve in the age of AI, organized around
three themes. First, the conceptual framing: the capabilities, risks, and the
supply chain that grounds foundation models in the broader economy. Second, the
empirical insights that enrich the conceptual foundations: transparency created
via evaluations at the model level and indexes at the organization level.
Finally, the transition from understanding to action: superior understanding of
the societal impact of foundation models advances evidence-based AI policy.
View together, this dissertation makes inroads into achieving better societal
outcomes in the age of AI by building the scientific foundations and
research-policy interface required for better AI governance.

</details>


### [209] [Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons](https://arxiv.org/abs/2506.23128)
*Chi Chiu So,Yueyue Sun,Jun-Min Wang,Siu Pang Yung,Anthony Wai Keung Loh,Chun Pong Chau*

Key words: 大型语言模型, 深度关系推理, 逻辑推理, 基准测试, 推理失败

TL;DR: 该论文评估了三种大型语言模型（DeepSeek-R1、DeepSeek-V3和GPT-4o）在深度关系推理任务中的表现。DeepSeek-R1在多项任务中表现出色，但随着问题复杂性增加，所有模型的表现均显著下降。作者还探讨了未来研究方向，并提出改进建议。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究动机是为了评估当前大型语言模型在深度关系推理任务中的表现，并探索其推理能力的局限性。

Method: 通过设计一系列家庭树和通用图推理的基准任务，对三种前沿大型语言模型进行了对比实验。

Result: DeepSeek-R1在多任务中表现最优，但所有模型在复杂任务中表现不佳，主要由于令牌长度限制和不完整的输出结构。

Conclusion: 研究揭示了大型语言模型在深度关系推理中的能力与局限性，并提出了未来改进方向，如多模态推理和系统化失败分析。

Abstract: How far are Large Language Models (LLMs) in performing deep relational
reasoning? In this paper, we evaluate and compare the reasoning capabilities of
three cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a
suite of carefully designed benchmark tasks in family tree and general graph
reasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the
highest F1-scores across multiple tasks and problem sizes, demonstrating strong
aptitude in logical deduction and relational inference. However, all evaluated
models, including DeepSeek-R1, struggle significantly as problem complexity
increases, largely due to token length limitations and incomplete output
structures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought
responses uncovers its unique planning and verification strategies, but also
highlights instances of incoherent or incomplete reasoning, calling attention
to the need for deeper scrutiny into LLMs' internal inference dynamics. We
further discuss key directions for future work, including the role of
multimodal reasoning and the systematic examination of reasoning failures. Our
findings provide both empirical insights and theoretical implications for
advancing LLMs' reasoning abilities, particularly in tasks that demand
structured, multi-step logical inference. Our code repository will be publicly
available at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.

</details>


### [210] [Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing](https://arxiv.org/abs/2506.23141)
*Siyuan Li,Ruitong Liu,Yan Wen,Te Sun*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge
Graph Completion (KGC), providing vital cues for prediction. However,
traditional node-based message passing mechanisms, when applied to knowledge
graphs, often introduce noise and suffer from information dilution or
over-smoothing by indiscriminately aggregating information from all neighboring
edges. To address this challenge, we propose a semantic-aware relational
message passing. A core innovation of this framework is the introduction of a
\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this
strategy first evaluates the semantic relevance between a central node and its
incident edges within a shared latent space, selecting only the Top-K most
pertinent ones. Subsequently, information from these selected edges is
effectively fused with the central node's own representation using a
\textbf{multi-head attention aggregator} to generate a semantically focused
node message. In this manner, our model not only leverages the structure and
features of edges within the knowledge graph but also more accurately captures
and propagates the contextual information most relevant to the specific link
prediction task, thereby effectively mitigating interference from irrelevant
information. Extensive experiments demonstrate that our method achieves
superior performance compared to existing approaches on several established
benchmarks.

</details>


### [211] [Rises for Measuring Local Distributivity in Lattices](https://arxiv.org/abs/2506.23168)
*Mohammad Abdulla,Tobias Hille,Dominik Dürrschnabel,Gerd Stumme*

Key words: 格理论, 分配性, 形式概念分析, 上升, 并集分配性

TL;DR: 介绍了一种通过'上升'概念来量化格理论中分配性的方法，并证明了格是分配性的充要条件是无非单位上升。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 在数据分析和形式概念分析中，格理论中的分配性缺乏标准化度量方法，因此需要一种新的量化手段。

Method: 提出了'上升'概念，用于衡量格中覆盖概念内属性或对象数量的变化，并证明其与分配性的关系。

Result: 实验显示，现实世界数据中的概念格通常是高度并集分配性的，而非交集分配性的。

Conclusion: '上升'概念为分配性的量化提供了一种有效工具，且在现实数据中表现出特定的分配性模式。

Abstract: Distributivity is a well-established and extensively studied notion in
lattice theory. In the context of data analysis, particularly within Formal
Concept Analysis (FCA), lattices are often observed to exhibit a high degree of
distributivity. However, no standardized measure exists to quantify this
property. In this paper, we introduce the notion of rises in (concept) lattices
as a means to assess distributivity. Rises capture how the number of attributes
or objects in covering concepts change within the concept lattice. We show that
a lattice is distributive if and only if no non-unit rises occur. Furthermore,
we relate rises to the classical notion of meet- and join distributivity. We
observe that concept lattices from real-world data are to a high degree
join-distributive, but much less meet-distributive. We additionally study how
join-distributivity manifests on the level of ordered sets.

</details>


### [212] [FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis](https://arxiv.org/abs/2506.23273)
*Quang Hung Nguyen,Phuong Anh Trinh,Phan Quoc Hung Mai,Tuan Phong Trinh*

Key words: 文本转SQL, 财务分析, 多智能体, 越南标准, 语言模型

TL;DR: FinStat2SQL是一个轻量级文本转SQL流水线，专为处理复杂的财务查询而设计，结合大小语言模型，支持越南本地标准，性能优于GPT-4o-mini。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 财务领域数据库设计和报表布局多样化，文本转SQL面临挑战，需要适应本地标准和复杂查询。

Method: 采用多智能体设置，结合大小语言模型，进行实体提取、SQL生成和自我修正，并构建领域特定数据库和评估数据集。

Result: 7B微调模型在61.33%准确率和4秒内响应时间下优于GPT-4o-mini，适用于越南企业。

Conclusion: FinStat2SQL提供了可扩展、经济高效的财务分析解决方案。

Abstract: Despite the advancements of large language models, text2sql still faces many
challenges, particularly with complex and domain-specific queries. In finance,
database designs and financial reporting layouts vary widely between financial
entities and countries, making text2sql even more challenging. We present
FinStat2SQL, a lightweight text2sql pipeline enabling natural language queries
over financial statements. Tailored to local standards like VAS, it combines
large and small language models in a multi-agent setup for entity extraction,
SQL generation, and self-correction. We build a domain-specific database and
evaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves
61.33\% accuracy with sub-4-second response times on consumer hardware,
outperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient
solution for financial analysis, making AI-powered querying accessible to
Vietnamese enterprises.

</details>


### [213] [Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games](https://arxiv.org/abs/2506.23276)
*David Guzman Piedrahita,Yongjin Yang,Mrinmaya Sachan,Giorgia Ramponi,Bernhard Schölkopf,Zhijing Jin*

Key words: LLMs, 合作行为, 公共物品博弈, 多智能体系统, 社会困境

TL;DR: 研究了大型语言模型（LLMs）在多智能体系统中如何平衡自利与集体利益，重点关注代价高昂的惩罚机制对合作行为的影响。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 理解LLMs的合作与社会机制对其作为自主智能体的安全部署至关重要。

Method: 通过行为经济学的公共物品博弈实验，观察LLMs在重复互动中的行为模式。

Result: 发现四种行为模式，部分模型能维持高合作水平，而推理能力强的模型反而表现较差。

Conclusion: 当前的推理能力提升方法未必促进合作，需重新思考LLMs在协作环境中的优化方向。

Abstract: As large language models (LLMs) are increasingly deployed as autonomous
agents, understanding their cooperation and social mechanisms is becoming
increasingly important. In particular, how LLMs balance self-interest and
collective well-being is a critical challenge for ensuring alignment,
robustness, and safe deployment. In this paper, we examine the challenge of
costly sanctioning in multi-agent LLM systems, where an agent must decide
whether to invest its own resources to incentivize cooperation or penalize
defection. To study this, we adapt a public goods game with institutional
choice from behavioral economics, allowing us to observe how different LLMs
navigate social dilemmas over repeated interactions. Our analysis reveals four
distinct behavioral patterns among models: some consistently establish and
sustain high levels of cooperation, others fluctuate between engagement and
disengagement, some gradually decline in cooperative behavior over time, and
others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we
find that reasoning LLMs, such as the o1 series, struggle significantly with
cooperation, whereas some traditional LLMs consistently achieve high levels of
cooperation. These findings suggest that the current approach to improving
LLMs, which focuses on enhancing their reasoning capabilities, does not
necessarily lead to cooperation, providing valuable insights for deploying LLM
agents in environments that require sustained collaboration. Our code is
available at https://github.com/davidguzmanp/SanctSim

</details>


### [214] [GATSim: Urban Mobility Simulation with Generative Agents](https://arxiv.org/abs/2506.23306)
*Qi Liu,Can Li,Wanjing Ma*

Key words: 城市交通模拟,生成代理,AI代理,自适应学习

TL;DR: GATSim框架利用大型语言模型和AI代理技术，创建具有推理能力和自适应学习机制的生成代理，用于复杂城市交通模拟。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统基于规则的交通模拟无法捕捉人类旅行决策的复杂性和多样性，因此需要更先进的方法。

Method: 结合城市交通基础模型与代理认知系统，开发了GATSim框架，支持具有多样属性和学习能力的生成代理。

Result: 生成代理能产生可信的旅行行为，并在实验中与人类注释者表现相当。

Conclusion: GATSim通过生成代理实现了更真实的城市交通模拟，具有广泛的应用潜力。

Abstract: Traditional agent-based urban mobility simulations rely on rigid rule-based
systems that fail to capture the complexity, adaptability, and behavioral
diversity characteristic of human travel decision-making. Recent advances in
large language models and AI agent technology offer opportunities to create
agents with reasoning capabilities, persistent memory, and adaptive learning
mechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel
framework that leverages these advances to create generative agents with rich
behavioral characteristics for urban mobility simulation. Unlike conventional
approaches, GATSim agents possess diverse socioeconomic attributes, individual
lifestyles, and evolving preferences that shape their mobility decisions
through psychologically-informed memory systems, tool usage capabilities, and
lifelong learning mechanisms. The main contributions of this study include: (1)
a comprehensive architecture combining an urban mobility foundation model with
agent cognitive systems and transport simulation environment, (2) a fully
functional prototype implementation, and (3) systematic validation
demonstrating that generative agents produce believable travel behaviors.
Through designed reflection processes, generative agents in this study can
transform specific travel experiences into generalized insights, enabling
realistic behavioral adaptation over time with specialized mechanisms for
activity planning and real-time reactive behaviors tailored to urban mobility
contexts. Experiments show that generative agents perform competitively with
human annotators in mobility scenarios while naturally producing macroscopic
traffic evolution patterns. The code for the prototype system is shared at
https://github.com/qiliuchn/gatsim.

</details>


### [215] [The Confidence Paradox: Can LLM Know When It's Wrong](https://arxiv.org/abs/2506.23464)
*Sahil Tripathi,Md Tabrez Nafis,Imran Hussain,Jiechao Gao*

Key words: DocVQA, 伦理校准, 自监督, 不确定性量化, 对比学习

TL;DR: HonestVQA 是一个自监督诚实校准框架，旨在解决 DocVQA 系统中的伦理问题，通过量化不确定性、对齐模型置信度与正确性，并引入新评估指标 H-Score 和 ECI。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的 DocVQA 系统在伦理响应性上表现不足，如过自信或不确定性传达不清晰，可能带来风险。

Method: HonestVQA 使用加权损失函数校准置信度，并通过对比学习强化伦理响应行为，同时开发了 H-Score 和 ECI 评估指标。

Result: HonestVQA 在多个数据集上提升准确性达 4.3%，减少过自信，并在跨领域评估中表现优异。

Conclusion: HonestVQA 成功解决了 DocVQA 的伦理问题，提高了模型的忠实性和泛化能力。

Abstract: Document Visual Question Answering (DocVQA) systems are increasingly deployed
in real world applications, yet they remain ethically opaque-often producing
overconfident answers to ambiguous questions or failing to communicate
uncertainty in a trustworthy manner. This misalignment between model confidence
and actual knowledge poses significant risks, particularly in domains requiring
ethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT
have advanced SOTA performance by focusing on architectural sophistication and
accuracy; however, they fall short in ethical responsiveness.
  To address these limitations, we introduce HonestVQA, a self-supervised
honesty calibration framework for ethically aligned DocVQA. Our model-agnostic
method quantifies uncertainty to identify knowledge gaps, aligns model
confidence with actual correctness using weighted loss functions, and enforces
ethical response behavior via contrastive learning. We further introduce two
principled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence
Index (ECI)--to benchmark alignment between confidence, accuracy, and ethical
communication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3%
and F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces
overconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In
cross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score,
demonstrating strong generalization. Ablation shows a 3.8% drop in accuracy
without alignment or contrastive loss.

</details>


### [216] [Data Augmentation for Cognitive Behavioral Therapy: Leveraging ERNIE Language Models using Artificial Intelligence](https://arxiv.org/abs/2506.23503)
*Bosubabu Sambana,Kondreddygari Archana,Suram Indhra Sena Reddy,Shaik Meethaigar Jameer Basha,Shaik Karishma*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Cognitive Behavioral Therapy (CBT) is a proven approach for addressing the
irrational thought patterns associated with mental health disorders, but its
effectiveness relies on accurately identifying cognitive pathways to provide
targeted treatment. In today's digital age, individuals often express negative
emotions on social media, where they may reveal cognitive distortions, and in
severe cases, exhibit suicidal tendencies. However, there is a significant gap
in methodologies designed to analyze these cognitive pathways, which could be
critical for psychotherapists aiming to deliver timely and effective
interventions in online environments. Cognitive Behavioral Therapy (CBT)
framework leveraging acceptance, commitment and data augmentation to categorize
and address both textual and visual content as positive or negative.
Specifically, the system employs BERT, RoBERTa for Sentiment Analysis and T5,
PEGASUS for Text Summarization, mT5 for Text Translation in Multiple Languages
focusing on detecting negative emotions and cognitive distortions within social
media data. While existing models are primarily designed to identify negative
thoughts, the proposed system goes beyond this by predicting additional
negative side effects and other potential mental health disorders likes
Phobias, Eating Disorders. This enhancement allows for a more comprehensive
understanding and intervention strategy, offering psychotherapists a powerful
tool for early detection and treatment of various psychological issues.

</details>


### [217] [Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM](https://arxiv.org/abs/2506.23504)
*Bosubabu Sambana,Kotamsetty Geethika Devi,Bandi Rajeswara Reddy,Galeti Mohammad Hussain,Gownivalla Siddartha*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The recent development of advanced machine learning methods for hybrid models
has greatly addressed the need for the correct prediction of electrical prices.
This method combines AlexNet and LSTM algorithms, which are used to introduce a
new model with higher accuracy in price forecasting. Despite RNN and ANN being
effective, they often fail to deal with forex time sequence data. The
traditional methods do not accurately forecast the prices. These traditional
methods only focus on demand and price which leads to insufficient analysis of
data. To address this issue, using the hybrid approach, which focuses on
external variables that also effect the predicted prices. Nevertheless, due to
AlexNet's excellent feature extraction and LSTM's learning sequential patterns,
the prediction accuracy is vastly increased. The model is built on the past
data, which has been supplied with the most significant elements like demand,
temperature, sunlight, and rain. For example, the model applies methods, such
as minimum-maximum scaling and a time window, to predict the electricity prices
of the future. The results show that this hybrid model is good than the
standalone ones in terms of accuracy. Although we got our accuracy rating of
97.08, it shows higher accompaniments than remaining models RNN and ANN with
accuracies of 96.64 and 96.63 respectively.

</details>


### [218] [Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays](https://arxiv.org/abs/2506.23517)
*Selin Dik,Osman Erdem,Mehmet Dik*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As the use of AI tools by students has become more prevalent, instructors
have started using AI detection tools like GPTZero and QuillBot to detect AI
written text. However, the reliability of these detectors remains uncertain. In
our study, we focused mostly on the success rate of GPTZero, the most-used AI
detector, in identifying AI-generated texts based on different lengths of
randomly submitted essays: short (40-100 word count), medium (100-350 word
count), and long (350-800 word count). We gathered a data set consisting of
twenty-eight AI-generated papers and fifty human-written papers. With this
randomized essay data, papers were individually plugged into GPTZero and
measured for percentage of AI generation and confidence. A vast majority of the
AI-generated papers were detected accurately (ranging from 91-100% AI believed
generation), while the human generated essays fluctuated; there were a handful
of false positives. These findings suggest that although GPTZero is effective
at detecting purely AI-generated content, its reliability in distinguishing
human-authored texts is limited. Educators should therefore exercise caution
when relying solely on AI detection tools.

</details>


### [219] [ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data](https://arxiv.org/abs/2506.23520)
*Yu Zhang,Ruijie Yu,Jidong Tian,Feng Zhu,Jiapeng Liu,Xiaokang Yang,Yaohui Jin,Yanyan Xu*

Key words: 化学执行器, LLM, 数据框架, 自动化提取, 化学实验

TL;DR: ChemActor是一个基于LLM的化学执行器，用于将非结构化实验程序转换为结构化动作序列，通过LLM生成的数据框架解决标注数据不足的问题，性能比基线模型提高10%。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着机器人合成在有机化学中的兴趣增加，从文献中自动提取化学程序变得关键，但由于化学语言的模糊性和人工标注的高成本，这一任务具有挑战性。

Method: 提出ChemActor，一个完全微调的LLM，通过顺序LLM生成的数据框架，结合数据选择模块和多轮LLMs循环评价指标，实现机器可执行动作的生成。

Result: 在R2D和D2A任务中，ChemActor表现出色，比基线模型性能提高10%，达到最先进水平。

Conclusion: ChemActor通过LLM生成的数据框架和多轮评价指标，显著提升了化学实验程序的自动化提取能力。

Abstract: With the increasing interest in robotic synthesis in the context of organic
chemistry, the automated extraction of chemical procedures from literature is
critical. However, this task remains challenging due to the inherent ambiguity
of chemical language and the high cost of human annotation required for
developing reliable computer-aided extraction protocols. Here, we present
ChemActor, a fully fine-tuned large language model (LLM), as a chemical
executor to convert between unstructured experimental procedures and structured
action sequences. We propose a sequential LLM-generated data framework to
address the challenges of insufficient and low-quality annotated data. This
framework integrates a data selection module that selects data based on
distribution divergence, with a general-purpose LLM, to generate
machine-executable actions from a single molecule input. Additionally, we
introduce a novel multi-round LLMs circle review metric, which reflects the
model's advanced understanding of chemical experimental procedures. Extensive
experiments on reaction-to-description (R2D) and description-to-action (D2A)
tasks demonstrate that ChemActor, augmented by LLM-generated data, achieves
state-of-the-art performance, outperforming the baseline model by 10%. The code
is available at: https://github.com/Zhanghahah/ChemActor.

</details>


### [220] [CooT: Learning to Coordinate In-Context with Coordination Transformers](https://arxiv.org/abs/2506.23549)
*Huai-Chih Wang,Hsiang-Chun Chuang,Hsi-Chun Cheng,Dai-Jie Wu,Shao-Hua Sun*

Key words: 多智能体系统, 协调, Transformer, 适应性学习

TL;DR: 本文提出了一种名为Coordination Transformers (CooT)的新框架，通过利用近期交互历史来快速适应未见过的合作者，解决了多智能体系统中动态不确定环境下的协调问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 多智能体系统中，动态和不确定环境下的智能体协调是一个重要挑战，现有方法如自博弈和群体训练法泛化能力不足或需要大量训练。

Method: CooT是一种基于上下文协调的框架，通过预测与观察到的合作者行为一致的动作，快速适应新合作者，无需显式监督或微调。

Result: 在Overcooked基准测试中，CooT在涉及未见合作者的协调任务中显著优于基线方法，人类评估也确认了其高效协作能力。

Conclusion: CooT展示了在多智能体场景中的鲁棒性、灵活性和上下文敏感性，成为一种高效的协调工具。

Abstract: Effective coordination among artificial agents in dynamic and uncertain
environments remains a significant challenge in multi-agent systems. Existing
approaches, such as self-play and population-based methods, either generalize
poorly to unseen partners or require extensive training. To overcome these
limitations, we propose Coordination Transformers (CooT), a novel in-context
coordination framework that uses recent interaction histories to adapt to
unseen partners rapidly. Unlike previous approaches that primarily aim to
increase the diversity of training partners, CooT explicitly focuses on
adapting to new partner behaviors by predicting actions aligned with observed
partner interactions. Trained on interaction trajectories collected from
diverse pairs of agents with complementary behaviors, CooT quickly learns
effective coordination strategies without explicit supervision or fine-tuning.
Evaluations on the Overcooked benchmark demonstrate that CooT significantly
outperforms baseline methods in coordination tasks involving previously unseen
partners. Human evaluations further confirm CooT as the most effective
collaborative partner, while extensive ablations highlight its robustness,
flexibility, and sensitivity to context in multi-agent scenarios.

</details>


### [221] [MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI](https://arxiv.org/abs/2506.23563)
*Huanjin Yao,Jiaxing Huang,Yawen Qiu,Michael K. Chen,Wenzheng Liu,Wei Zhang,Wenjie Zeng,Xikun Zhang,Jingyi Zhang,Yuxin Song,Wenhao Wu,Dacheng Tao*

Key words: 多模态大语言模型、推理能力、基准测试、长链推理、评分机制

TL;DR: 论文提出了MMReason，一个新的基准测试，旨在全面精确评估多模态大语言模型（MLLMs）的长链推理能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有MLLM基准测试在准确评估长链推理能力方面存在不足，特别是在难度多样性、抗猜测性和对中间推理步骤的评估上。

Method: 通过多学科、多难度级别的开放性问题设计，采用多模型投票技术过滤猜测和记忆问题，并标注详细步骤和三元评分机制。

Result: MMReason成功评估了主流MLLMs的推理能力，并提供了深入分析。

Conclusion: MMReason为推进MLLM推理研究提供了有价值的资源。

Abstract: Reasoning plays a crucial role in advancing Multimodal Large Language Models
(MLLMs) toward Artificial General Intelligence. However, existing MLLM
benchmarks often fall short in precisely and comprehensively evaluating
long-chain reasoning abilities from three key aspects: (1) lack of difficulty
and diversity, (2) susceptibility to guessability and memorization, (3)
inadequate assessment of intermediate reasoning steps. To fill this gap, we
introduce MMReason, a new benchmark designed to precisely and comprehensively
evaluate MLLM long-chain reasoning capability with diverse, open-ended,
challenging questions. First, we curate challenging questions requiring
multi-step reasoning from various fields (i.e., 6 disciplines) and multiple
difficulty levels (i.e., from pre-university to university, and from
foundational to competition tiers). Second, these questions are reformulated
into an open-ended format and filtered using a multi-model voting technique to
eliminate shortcut cases related to guessing and memorization, ensuring robust
reasoning evaluations. Third, we annotate the questions with detailed
step-by-step solutions, and design a reference-based ternary scoring mechanism
to reliably assess intermediate reasoning steps. With MMReason, we benchmark
popular leading MLLMs and provide an in-depth analysis of their reasoning
capabilities. We hope MMReason will serve as a valuable resource for advancing
MLLM reasoning research. Code will be available at
https://github.com/HJYao00/MMReason.

</details>


### [222] [Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models](https://arxiv.org/abs/2506.23576)
*Maria Carolina Cornelia Wit,Jun Pang*

Key words: 大语言模型, 越狱攻击, 多代理系统, 安全防御, AutoDefense

TL;DR: 多代理LLM系统能增强对越狱攻击的防御，但效果因攻击类型而异，并带来误报增加和计算开销等权衡。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着大语言模型（LLM）的发展，越狱攻击（绕过安全机制的提示）成为关注点。研究探讨多代理LLM系统是否能有效防御此类攻击。

Method: 通过复现AutoDefense框架，比较单代理与多代理（二代理和三代理）配置，评估三种越狱策略（AutoDefense、BetterDan、JB）。

Result: 多代理系统能提升对越狱攻击的抵抗力，减少漏报，但效果因攻击类型而异，同时增加误报和计算成本。

Conclusion: 当前自动防御存在局限性，需进一步提升LLM系统的对齐鲁棒性。

Abstract: Recent advances in large language models (LLMs) have raised concerns about
jailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper
investigates the use of multi-agent LLM systems as a defence against such
attacks. We evaluate three jailbreaking strategies, including the original
AutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the
AutoDefense framework, we compare single-agent setups with two- and three-agent
configurations. Our results show that multi-agent systems enhance resistance to
jailbreaks, especially by reducing false negatives. However, its effectiveness
varies by attack type, and it introduces trade-offs such as increased false
positives and computational overhead. These findings point to the limitations
of current automated defences and suggest directions for improving alignment
robustness in future LLM systems.

</details>


### [223] [Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games](https://arxiv.org/abs/2506.23626)
*António Afonso,Iolanda Leite,Alessandro Sestini,Florian Fuchs,Konrad Tollmar,Linus Gisslén*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reinforcement Learning (RL) in games has gained significant momentum in
recent years, enabling the creation of different agent behaviors that can
transform a player's gaming experience. However, deploying RL agents in
production environments presents two key challenges: (1) designing an effective
reward function typically requires an RL expert, and (2) when a game's content
or mechanics are modified, previously tuned reward weights may no longer be
optimal. Towards the latter challenge, we propose an automated approach for
iteratively fine-tuning an RL agent's reward function weights, based on a
user-defined language based behavioral goal. A Language Model (LM) proposes
updated weights at each iteration based on this target behavior and a summary
of performance statistics from prior training rounds. This closed-loop process
allows the LM to self-correct and refine its output over time, producing
increasingly aligned behavior without the need for manual reward engineering.
We evaluate our approach in a racing task and show that it consistently
improves agent performance across iterations. The LM-guided agents show a
significant increase in performance from $9\%$ to $74\%$ success rate in just
one iteration. We compare our LM-guided tuning against a human expert's manual
weight design in the racing task: by the final iteration, the LM-tuned agent
achieved an $80\%$ success rate, and completed laps in an average of $855$ time
steps, a competitive performance against the expert-tuned agent's peak $94\%$
success, and $850$ time steps.

</details>


### [224] [HASD: Hierarchical Adaption for pathology Slide-level Domain-shift](https://arxiv.org/abs/2506.23673)
*Jingsong Liu,Han Li,Chen Yang,Michael Deutges,Ario Sadafi,Xin You,Katharina Breininger,Nassir Navab,Peter J. Schüffler*

Key words: 病理学AI, 域偏移, 多尺度特征, 计算效率, 层次化适应

TL;DR: 提出了一种名为HASD的层次化适应框架，用于解决病理学AI中的切片级域偏移问题，通过多尺度特征一致性和计算高效的域适应，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 病理学AI中数据受到中心特定条件的严重影响，当前方法局限于图像块级，无法捕捉全局WSI特征，HASD旨在解决切片级域偏移。

Method: HASD包含层次化适应框架（特征对齐、几何不变性正则化、注意力一致性正则化）和原型选择机制，以提高计算效率。

Result: 在两个切片级任务中，HASD实现了4.1%的AUROC提升和3.9%的C指数增益，显著优于基线。

Conclusion: HASD为病理学机构提供了一种实用且可靠的域适应方法，降低了计算和标注成本。

Abstract: Domain shift is a critical problem for pathology AI as pathology data is
heavily influenced by center-specific conditions. Current pathology domain
adaptation methods focus on image patches rather than WSI, thus failing to
capture global WSI features required in typical clinical scenarios. In this
work, we address the challenges of slide-level domain shift by proposing a
Hierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD
achieves multi-scale feature consistency and computationally efficient
slide-level domain adaptation through two key components: (1) a hierarchical
adaptation framework that integrates a Domain-level Alignment Solver for
feature alignment, a Slide-level Geometric Invariance Regularization to
preserve the morphological structure, and a Patch-level Attention Consistency
Regularization to maintain local critical diagnostic cues; and (2) a prototype
selection mechanism that reduces computational overhead. We validate our method
on two slide-level tasks across five datasets, achieving a 4.1\% AUROC
improvement in a Breast Cancer HER2 Grading cohort and a 3.9\% C-index gain in
a UCEC survival prediction cohort. Our method provides a practical and reliable
slide-level domain adaption solution for pathology institutions, minimizing
both computational and annotation costs.

</details>


### [225] [PokéAI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red](https://arxiv.org/abs/2506.23689)
*Zihao Liu,Xinhang Sui,Yueran Song,Siwen Wang*

Key words: PokéAI, 多智能体, 大型语言模型, 宝可梦红, 闭环决策

TL;DR: 介绍了PokéAI，首个基于文本的多智能体大型语言模型框架，用于自主玩《宝可梦红》。系统包括规划、执行和评估三个智能体，形成闭环决策系统。初步结果显示，战斗模块胜率达80.8%，接近人类玩家水平。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索大型语言模型在多智能体环境下的游戏自主决策能力，特别是在复杂游戏任务中的应用。

Method: 系统由三个智能体组成：规划、执行和评估，各司其职，形成闭环决策。执行智能体中开发了战斗模块。

Result: 战斗模块在50次野外对战中平均胜率80.8%，仅比人类玩家低6%，且语言能力与战斗表现强相关。

Conclusion: 研究表明，语言能力与战略推理能力相关，且不同模型表现出独特的游戏风格。

Abstract: We introduce Pok\'eAI, the first text-based, multi-agent large language model
(LLM) framework designed to autonomously play and progress through Pok\'emon
Red. Our system consists of three specialized agents-Planning, Execution, and
Critique-each with its own memory bank, role, and skill set. The Planning Agent
functions as the central brain, generating tasks to progress through the game.
These tasks are then delegated to the Execution Agent, which carries them out
within the game environment. Upon task completion, the Critique Agent evaluates
the outcome to determine whether the objective was successfully achieved. Once
verification is complete, control returns to the Planning Agent, forming a
closed-loop decision-making system.
  As a preliminary step, we developed a battle module within the Execution
Agent. Our results show that the battle AI achieves an average win rate of
80.8% across 50 wild encounters, only 6% lower than the performance of an
experienced human player. Furthermore, we find that a model's battle
performance correlates strongly with its LLM Arena score on language-related
tasks, indicating a meaningful link between linguistic ability and strategic
reasoning. Finally, our analysis of gameplay logs reveals that each LLM
exhibits a unique playstyle, suggesting that individual models develop distinct
strategic behaviors.

</details>


### [226] [Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models](https://arxiv.org/abs/2506.23692)
*Boyuan Zheng,Zerui Fang,Zhe Xu,Rui Wang,Yiwen Chen,Cunshi Wang,Mengwei Qu,Lei Lei,Zhen Feng,Yan Liu,Yuyang Li,Mingzhou Tan,Jiaji Wu,Jianwei Shuai,Jia Li,Fangfu Ye*

Key words: AI for Science, Agent4S, LLM, 自动化科研, 第五科学范式

TL;DR: 论文提出'Agent for Science'（Agent4S），利用LLM驱动的智能体自动化整个科研工作流程，作为第五大科学范式。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前的AI for Science（AI4S）未能解决科研效率的核心问题，因此提出Agent4S以实现更高效的科学发现。

Method: 引入五级分类框架，从简单任务自动化到完全自主协作的'AI科学家'，为Agent4S的发展提供路线图。

Result: 提出了一个革命性的框架，推动科学发现进入新阶段。

Conclusion: Agent4S有望成为第五大科学范式，彻底改变科研工作流程。

Abstract: While AI for Science (AI4S) serves as an analytical tool in the current
research paradigm, it doesn't solve its core inefficiency. We propose "Agent
for Science" (Agent4S)-the use of LLM-driven agents to automate the entire
research workflow-as the true Fifth Scientific Paradigm. This paper introduces
a five-level classification for Agent4S, outlining a clear roadmap from simple
task automation to fully autonomous, collaborative "AI Scientists." This
framework defines the next revolutionary step in scientific discovery.

</details>


### [227] [A New Perspective On AI Safety Through Control Theory Methodologies](https://arxiv.org/abs/2506.23703)
*Lars Ullrich,Walter Zimmer,Ross Greer,Knut Graichen,Alois C. Knoll,Mohan Trivedi*

Key words: AI安全, 数据控制, 系统理论, 跨学科方法, 控制理论

TL;DR: 本文提出了一种基于系统理论和系统分析的新视角，称为“数据控制”，旨在通过跨学科方法结合AI和控制理论，以提升AI系统的安全性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: AI在复杂问题上表现优异，但其在安全关键的真实世界系统中的应用受到安全性不足的制约。

Method: 采用跨学科方法，结合AI和控制理论，提出“数据控制”视角，以系统分析为基础，进行安全性分析和保障。

Result: 提出了一个抽象的通用框架，可用于特定AI系统和应用的安全性分析，并为未来创新做准备。

Conclusion: 通过“数据控制”这一新视角，可以促进AI工程中安全性的跨学科研究与实践。

Abstract: While artificial intelligence (AI) is advancing rapidly and mastering
increasingly complex problems with astonishing performance, the safety
assurance of such systems is a major concern. Particularly in the context of
safety-critical, real-world cyber-physical systems, AI promises to achieve a
new level of autonomy but is hampered by a lack of safety assurance. While
data-driven control takes up recent developments in AI to improve control
systems, control theory in general could be leveraged to improve AI safety.
Therefore, this article outlines a new perspective on AI safety based on an
interdisciplinary interpretation of the underlying data-generation process and
the respective abstraction by AI systems in a system theory-inspired and system
analysis-driven manner. In this context, the new perspective, also referred to
as data control, aims to stimulate AI engineering to take advantage of existing
safety analysis and assurance in an interdisciplinary way to drive the paradigm
of data control. Following a top-down approach, a generic foundation for safety
analysis and assurance is outlined at an abstract level that can be refined for
specific AI systems and applications and is prepared for future innovation.

</details>


### [228] [Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments](https://arxiv.org/abs/2506.23706)
*Christoph Schnabl,Daniel Hugenroth,Bill Marino,Alastair R. Beresford*

Key words: AI安全，可信执行环境，审计，合规性，Llama-3.1

TL;DR: 提出了一种可验证的审计方法（Attestable Audits），利用可信执行环境（TEE）确保AI模型的合规性和安全性，同时保护敏感数据和模型IP。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前的基准测试方法缺乏可验证性和对敏感数据的保密性，无法满足AI治理框架的要求。

Method: 通过可信执行环境（TEE）实现审计，确保模型交互的可验证性，同时保护数据和模型IP。

Result: 构建的原型在典型审计基准测试（如Llama-3.1）上验证了该方法的可行性。

Conclusion: Attestable Audits为解决AI模型的合规性和数据保密性提供了一种有效方案。

Abstract: Benchmarks are important measures to evaluate safety and compliance of AI
models at scale. However, they typically do not offer verifiable results and
lack confidentiality for model IP and benchmark datasets. We propose Attestable
Audits, which run inside Trusted Execution Environments and enable users to
verify interaction with a compliant AI model. Our work protects sensitive data
even when model provider and auditor do not trust each other. This addresses
verification challenges raised in recent AI governance frameworks. We build a
prototype demonstrating feasibility on typical audit benchmarks against
Llama-3.1.

</details>


### [229] [BayesL: Towards a Logical Framework for Bayesian Networks](https://arxiv.org/abs/2506.23773)
*Stefano M. Nicoletti,Mariëlle Stoelinga*

Key words: BayesL, 贝叶斯网络, 逻辑框架, 查询语言, 因果推理

TL;DR: BayesL是一种新型逻辑框架，用于指定、查询和验证贝叶斯网络的行为。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 开发BayesL的目的是为了简化贝叶斯网络的查询和验证过程，支持因果和证据关系的推理。

Method: BayesL是一种结构化语言，支持创建贝叶斯网络的查询，允许全面的假设情景评估，无需手动修改模型。

Result: BayesL为贝叶斯网络提供了便捷的查询和验证手段，支持灵活的推理和情景分析。

Conclusion: BayesL是贝叶斯网络领域的一项有前途的工具，显著提升了模型的查询和验证效率。

Abstract: We introduce BayesL, a novel logical framework for specifying, querying, and
verifying the behaviour of Bayesian networks (BNs). BayesL (pronounced "Basil")
is a structured language that allows for the creation of queries over BNs. It
facilitates versatile reasoning concerning causal and evidence-based
relationships, and permits comprehensive what-if scenario evaluations without
the need for manual modifications to the model.

</details>


### [230] [When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)](https://arxiv.org/abs/2506.23784)
*Parosh Aziz Abdulla,Mohamed Faouzi Atig,Julie Cailler,Chencheng Liang,Philipp Rümmer*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Nielsen transformation is a standard approach for solving word equations: by
repeatedly splitting equations and applying simplification steps, equations are
rewritten until a solution is reached. When solving a conjunction of word
equations in this way, the performance of the solver will depend considerably
on the order in which equations are processed. In this work, the use of Graph
Neural Networks (GNNs) for ranking word equations before and during the solving
process is explored. For this, a novel graph-based representation for word
equations is presented, preserving global information across conjuncts,
enabling the GNN to have a holistic view during ranking. To handle the variable
number of conjuncts, three approaches to adapt a multi-classification task to
the problem of ranking equations are proposed. The training of the GNN is done
with the help of minimum unsatisfiable subsets (MUSes) of word equations. The
experimental results show that, compared to state-of-the-art string solvers,
the new framework solves more problems in benchmarks where each variable
appears at most once in each equation.

</details>


### [231] [Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning](https://arxiv.org/abs/2506.23793)
*Anton Andreychuk,Konstantin Yakovlev,Aleksandr Panov,Alexey Skrynnik*

Key words: MAPF, 多智能体路径规划, 机器学习, delta-data生成, 可扩展性

TL;DR: MAPF-GPT-DDG是一种基于机器学习的多智能体路径规划（MAPF）求解器，通过改进已有模型并利用新的数据生成机制，显著提升了训练效率和性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 为应对多机器人轨迹规划问题，需要更高效、可扩展的MAPF求解器以满足实际应用需求。

Method: 通过集中式专家数据微调预训练的MAPF-GPT模型，并采用新的delta-data生成机制。

Result: 实验表明，MAPF-GPT-DDG在性能上超越现有所有基于学习的求解器，并能支持单环境中多达100万个智能体的规划。

Conclusion: MAPF-GPT-DDG为MAPF领域提供了新的高效、可扩展的解决方案。

Abstract: Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot
trajectory planning problems, where multiple homogeneous robots simultaneously
move in the shared environment. While solving MAPF optimally has been proven to
be NP-hard, scalable, and efficient, solvers are vital for real-world
applications like logistics, search-and-rescue, etc. To this end, decentralized
suboptimal MAPF solvers that leverage machine learning have come on stage.
Building on the success of the recently introduced MAPF-GPT, a pure imitation
learning solver, we introduce MAPF-GPT-DDG. This novel approach effectively
fine-tunes the pre-trained MAPF model using centralized expert data. Leveraging
a novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training
while significantly improving performance at test time. Our experiments
demonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF
solvers, including the original MAPF-GPT, regarding solution quality across
many testing scenarios. Remarkably, it can work with MAPF instances involving
up to 1 million agents in a single environment, setting a new milestone for
scalability in MAPF domains.

</details>


### [232] [A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents](https://arxiv.org/abs/2506.23844)
*Hang Su,Jun Luo,Chang Liu,Xiao Yang,Yichi Zhang,Yinpeng Dong,Jun Zhu*

Key words: 大模型智能体、安全风险、防御策略、CMDP、R2A2架构

TL;DR: 本文综述了大模型智能体在动态环境中的自主能力及其带来的新型安全风险，提出了一种风险感知的架构R2A2来应对这些挑战。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着大型语言模型的发展，AI智能体的自主能力显著提升，但也引入了传统系统未涵盖的新型安全风险，亟需研究防御策略。

Method: 通过分析智能体的结构基础和关键能力，识别其安全漏洞，并系统综述了防御策略，最终提出R2A2架构。

Result: 揭示了智能体在感知、认知、记忆和行动模块中的脆弱性，并提出了一种基于CMDP的统一认知框架。

Conclusion: R2A2架构通过风险感知建模和优化，为智能体的决策过程提供了主动的安全保障。

Abstract: Recent advances in large language models (LLMs) have catalyzed the rise of
autonomous AI agents capable of perceiving, reasoning, and acting in dynamic,
open-ended environments. These large-model agents mark a paradigm shift from
static inference systems to interactive, memory-augmented entities. While these
capabilities significantly expand the functional scope of AI, they also
introduce qualitatively novel security risks - such as memory poisoning, tool
misuse, reward hacking, and emergent misalignment - that extend beyond the
threat models of conventional systems or standalone LLMs. In this survey, we
first examine the structural foundations and key capabilities that underpin
increasing levels of agent autonomy, including long-term memory retention,
modular tool use, recursive planning, and reflective reasoning. We then analyze
the corresponding security vulnerabilities across the agent stack, identifying
failure modes such as deferred decision hazards, irreversible tool chains, and
deceptive behaviors arising from internal state drift or value misalignment.
These risks are traced to architectural fragilities that emerge across
perception, cognition, memory, and action modules. To address these challenges,
we systematically review recent defense strategies deployed at different
autonomy layers, including input sanitization, memory lifecycle control,
constrained decision-making, structured tool invocation, and introspective
reflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a
unified cognitive framework grounded in Constrained Markov Decision Processes
(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,
and joint reward-risk optimization to enable principled, proactive safety
across the agent's decision-making loop.

</details>


### [233] [Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence](https://arxiv.org/abs/2506.23908)
*András György,Tor Lattimore,Nevena Lazić,Csaba Szepesvári*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Sound deductive reasoning -- the ability to derive new knowledge from
existing facts and rules -- is an indisputably desirable aspect of general
intelligence. Despite the major advances of AI systems in areas such as math
and science, especially since the introduction of transformer architectures, it
is well-documented that even the most advanced frontier systems regularly and
consistently falter on easily-solvable deductive reasoning tasks. Hence, these
systems are unfit to fulfill the dream of achieving artificial general
intelligence capable of sound deductive reasoning. We argue that their unsound
behavior is a consequence of the statistical learning approach powering their
development. To overcome this, we contend that to achieve reliable deductive
reasoning in learning-based AI systems, researchers must fundamentally shift
from optimizing for statistical performance against distributions on reasoning
problems and algorithmic tasks to embracing the more ambitious exact learning
paradigm, which demands correctness on all inputs. We argue that exact learning
is both essential and possible, and that this ambitious objective should guide
algorithm design.

</details>


### [234] [Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice](https://arxiv.org/abs/2506.23924)
*Akshit Kumar,Tianyi Peng,Yuhang Wu,Assaf Zeevi*

Key words: 大语言模型,运筹学,随机建模,不确定性,自动化

TL;DR: 该论文探讨了大语言模型（LLMs）在解决运筹学（OR）中随机建模问题的能力，展示了LLMs在课堂和实际场景中与人专家相当的表现。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索LLMs在处理具有不确定性的运筹学问题中的潜力，尤其是随机建模问题，以填补这一领域的研究空白。

Method: 手动收集研究生课程作业和博士资格考试题，测试LLMs的解决能力，并结合开源库SimOpt评估LLMs在实际不确定性问题中的决策能力。

Result: 尽管在自动化随机建模流程方面仍需改进，但最新的LLMs在课堂和实际应用中表现出与人专家相当的水平。

Conclusion: 研究发现LLMs具有辅助运筹学研究的潜力，并通过自动化提升运筹学的实际影响力。

Abstract: Large language models (LLMs) have exhibited expert-level capabilities across
various domains. However, their abilities to solve problems in Operations
Research (OR) -- the analysis and optimization of mathematical models derived
from real-world problems or their verbal descriptions -- remain underexplored.
In this work, we take a first step toward evaluating LLMs' abilities to solve
stochastic modeling problems, a core class of OR problems characterized by
uncertainty and typically involving tools from probability, statistics, and
stochastic processes. We manually procure a representative set of
graduate-level homework and doctoral qualification-exam problems and test LLMs'
abilities to solve them. We further leverage SimOpt, an open-source library of
simulation-optimization problems and solvers, to investigate LLMs' abilities to
make real-world decisions under uncertainty. Our results show that, though a
nontrivial amount of work is still needed to reliably automate the stochastic
modeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on
par with human experts in both classroom and practical settings. These findings
highlight the potential of building AI agents that assist OR researchers and
amplify the real-world impact of OR through automation.

</details>


### [235] [Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system](https://arxiv.org/abs/2506.23926)
*Junping Wang,Bicheng Wang,Yibo Xuea,Yuan Xie*

Key words: 弹性测量，工业大脑，复杂网络，自主决策，CT-OODA

TL;DR: 本文提出了一种名为“工业大脑”的自主认知决策规划框架，用于解决工业链中复杂网络弹性预测和规划的挑战。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 工业链中的弹性测量对科学管理和工程应用至关重要，但在复杂多变的网络中，现有深度学习方法难以泛化到未见过的拓扑和动态数据。

Method: 结合高阶活动驱动神经网络和CT-OODA符号推理，直接从全局变量观测数据中自主规划弹性。

Result: 工业大脑在弹性预测和规划方面显著优于现有方法（如GoT和OlaGPT），准确率提升高达10.8%和11.03%，并能泛化到未见网络拓扑和动态。

Conclusion: 工业大脑填补了工业链弹性预测和规划的重要空白，展现了强大的性能和泛化能力。

Abstract: Resilience non-equilibrium measurement, the ability to maintain fundamental
functionality amidst failures and errors, is crucial for scientific management
and engineering applications of industrial chain. The problem is particularly
challenging when the number or types of multiple co-evolution of resilience
(for example, randomly placed) are extremely chaos. Existing end-to-end deep
learning ordinarily do not generalize well to unseen full-feld reconstruction
of spatiotemporal co-evolution structure, and predict resilience of network
topology, especially in multiple chaos data regimes typically seen in
real-world applications. To address this challenge, here we propose industrial
brain, a human-like autonomous cognitive decision-making and planning framework
integrating higher-order activity-driven neuro network and CT-OODA symbolic
reasoning to autonomous plan resilience directly from observational data of
global variable. The industrial brain not only understands and model structure
of node activity dynamics and network co-evolution topology without simplifying
assumptions, and reveal the underlying laws hidden behind complex networks, but
also enabling accurate resilience prediction, inference, and planning.
Experimental results show that industrial brain significantly outperforms
resilience prediction and planning methods, with an accurate improvement of up
to 10.8\% over GoT and OlaGPT framework and 11.03\% over spectral dimension
reduction. It also generalizes to unseen topologies and dynamics and maintains
robust performance despite observational disturbances. Our findings suggest
that industrial brain addresses an important gap in resilience prediction and
planning for industrial chain.

</details>


### [236] [AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models](https://arxiv.org/abs/2506.23949)
*Anthony M. Barrett,Jessica Newman,Brandie Nonnecke,Nada Madkour,Dan Hendrycks,Evan R. Murphy,Krystal Jackson,Deepika Raman*

Key words: GPAI, 基础模型, 风险管理, NIST, ISO/IEC 23894

TL;DR: 该文档提供了针对多用途AI模型（GPAI/基础模型）的风险管理实践，帮助开发者识别、分析和减轻潜在风险，适用于大型先进模型的开发者及下游应用开发人员。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 多用途AI模型（如大型语言模型）带来巨大益处的同时也可能导致严重后果，需针对其独特风险制定管理策略。

Method: 基于NIST AI风险管理框架和ISO/IEC 23894，提出针对GPAI/基础模型的专项风险管理实践。

Result: 提供了适合GPAI/基础模型开发者的风险控制指南，促进与相关标准的对接。

Conclusion: 该文档为GPAI/基础模型的开发者提供风险管理工具，有助于降低潜在负面影响。

Abstract: Increasingly multi-purpose AI models, such as cutting-edge large language
models or other 'general-purpose AI' (GPAI) models, 'foundation models,'
generative AI models, and 'frontier models' (typically all referred to
hereafter with the umbrella term 'GPAI/foundation models' except where greater
specificity is needed), can provide many beneficial capabilities but also risks
of adverse events with profound consequences. This document provides
risk-management practices or controls for identifying, analyzing, and
mitigating risks of GPAI/foundation models. We intend this document primarily
for developers of large-scale, state-of-the-art GPAI/foundation models; others
that can benefit from this guidance include downstream developers of end-use
applications that build on a GPAI/foundation model. This document facilitates
conformity with or use of leading AI risk management-related standards,
adapting and building on the generic voluntary guidance in the NIST AI Risk
Management Framework and ISO/IEC 23894, with a focus on the unique issues faced
by developers of GPAI/foundation models.

</details>


### [237] [Harnessing AI Agents to Advance Research on Refugee Child Mental Health](https://arxiv.org/abs/2506.23992)
*Aditya Shrivastava,Komal Gupta,Shraddha Arora*

Key words: 难民危机, 儿童心理健康, AI框架, RAG模型, 人道主义数据

TL;DR: 本文提出了一种基于AI的框架，用于处理难民儿童心理健康数据，并比较了两种RAG模型的性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 国际难民危机加剧，数百万儿童面临心理创伤，亟需有效方法处理相关数据以辅助决策。

Method: 采用了Zephyr-7B-beta和DeepSeek R1-7B两种RAG模型，评估其在处理人道主义数据时的表现及避免幻觉的能力。

Result: 两种模型均有效，但DeepSeek R1-7B在答案相关性上表现更优，准确率达0.91。

Conclusion: 结合前沿AI与心理学研究，本研究提供了可扩展的策略，以帮助相关机构更好地识别和支持难民儿童的心理健康。

Abstract: The international refugee crisis deepens, exposing millions of dis placed
children to extreme psychological trauma. This research suggests a com pact,
AI-based framework for processing unstructured refugee health data and
distilling knowledge on child mental health. We compare two Retrieval-Aug
mented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to
determine how well they process challenging humanitarian datasets while avoid
ing hallucination hazards. By combining cutting-edge AI methods with migration
research and child psychology, this study presents a scalable strategy to
assist policymakers, mental health practitioners, and humanitarian agencies to
better assist displaced children and recognize their mental wellbeing. In
total, both the models worked properly but significantly Deepseek R1 is
superior to Zephyr with an accuracy of answer relevance 0.91

</details>


### [238] [Constructing Non-Markovian Decision Process via History Aggregator](https://arxiv.org/abs/2506.24026)
*Yongyi Wang,Wenxin Li*

Key words: 算法决策, 非马尔可夫动态, 范畴论, MDP, NMDP, HAS

TL;DR: 论文提出了一种基于范畴论的方法，用于解决算法决策中的非马尔可夫动态问题，并验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有基准无法全面评估算法处理非马尔可夫动态的能力，限制了相关系统的发展。

Method: 通过范畴论构建马尔可夫决策过程（MDP）与非马尔可夫决策过程（NMDP）的范畴，并证明其等价性；引入历史状态聚合器（HAS）控制状态依赖结构。

Result: 方法成功表征了广泛的非马尔可夫动态，为算法评估提供了更严格的测试环境。

Conclusion: 该研究为理解和处理非马尔可夫动态提供了新视角，提升了决策算法的评估灵活性。

Abstract: In the domain of algorithmic decision-making, non-Markovian dynamics manifest
as a significant impediment, especially for paradigms such as Reinforcement
Learning (RL), thereby exerting far-reaching consequences on the advancement
and effectiveness of the associated systems. Nevertheless, the existing
benchmarks are deficient in comprehensively assessing the capacity of decision
algorithms to handle non-Markovian dynamics. To address this deficiency, we
have devised a generalized methodology grounded in category theory. Notably, we
established the category of Markov Decision Processes (MDP) and the category of
non-Markovian Decision Processes (NMDP), and proved the equivalence
relationship between them. This theoretical foundation provides a novel
perspective for understanding and addressing non-Markovian dynamics. We further
introduced non-Markovianity into decision-making problem settings via the
History Aggregator for State (HAS). With HAS, we can precisely control the
state dependency structure of decision-making problems in the time series. Our
analysis demonstrates the effectiveness of our method in representing a broad
range of non-Markovian dynamics. This approach facilitates a more rigorous and
flexible evaluation of decision algorithms by testing them in problem settings
where non-Markovian dynamics are explicitly constructed.

</details>


### [239] [SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2506.24119)
*Bo Liu,Leon Guertler,Simon Yu,Zichen Liu,Penghui Qi,Daniel Balcells,Mickel Liu,Cheston Tan,Weiyan Shi,Min Lin,Wee Sun Lee,Natasha Jaques*

Key words: 自博弈, 强化学习, 语言模型, 零和游戏, 推理能力

TL;DR: SPIRAL是一个自博弈框架，通过多轮零和游戏训练语言模型，无需人工监督，生成渐进式挑战问题，显著提升推理能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的强化学习方法依赖人工标注和领域特定奖励工程，SPIRAL旨在通过自博弈解决这些限制。

Method: 采用多轮、多智能体强化学习系统及角色条件优势估计（RAE）方法，进行零和游戏训练。

Result: 在Kuhn Poker上训练的模型在数学和一般推理任务中分别提升8.6%和8.4%，多游戏训练进一步增强性能。

Conclusion: 零和游戏能自然培养可迁移推理能力，为自主推理发展提供新方向。

Abstract: Recent advances in reinforcement learning have shown that language models can
develop sophisticated reasoning through training on tasks with verifiable
rewards, but these approaches depend on human-curated problem-answer pairs and
domain-specific reward engineering. We introduce SPIRAL, a self-play framework
where models learn by playing multi-turn, zero-sum games against continuously
improving versions of themselves, eliminating the need for human supervision.
Through self-play, SPIRAL generates an infinite curriculum of progressively
challenging problems as models must constantly adapt to stronger opponents. To
enable this self-play training at scale, We implement a fully online,
multi-turn, multi-agent reinforcement learning system for LLMs and propose
role-conditioned advantage estimation (RAE) to stabilize multi-agent training.
Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that
transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%
improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000
expert game trajectories. Analysis reveals that this transfer occurs through
three cognitive patterns: systematic decomposition, expected value calculation,
and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple
Negotiation) further enhances performance as each game develops distinct
reasoning strengths. Applying SPIRAL to a strong reasoning model
(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These
results demonstrate that zero-sum games naturally develop transferable
reasoning capabilities, highlighting a promising direction for autonomous
reasoning development.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [240] [Explainable AI for Comprehensive Risk Assessment for Financial Reports: A Lightweight Hierarchical Transformer Network Approach](https://arxiv.org/abs/2506.23767)
*Xue Wen Tan,Stanley Kok*

Key words: 风险评估, Transformer, TinyBERT, 动态注意力, 可解释性

TL;DR: TinyXRA是一个轻量级、可解释的基于Transformer的模型，用于从公司年报中自动评估风险，结合了多种风险指标，并通过动态注意力和可视化机制提高了透明度和效率。

<details>
  <summary>Details</summary>

Main category: q-fin.RM

Motivation: 传统风险评估方法仅依赖超额回报的标准差，无法区分上下行风险，且缺乏透明性和可解释性。TinyXRA的目标是提供更全面且直观的风险评估。

Method: 使用TinyBERT编码器处理金融文档，结合动态注意力机制生成可视化词云，采用三重损失函数进行风险分类。

Result: TinyXRA在2013-2024年的数据集上实现了最高的预测准确率，同时提供了透明且可解释的风险评估结果。

Conclusion: TinyXRA在轻量化、透明性和准确性上表现优异，适用于资源受限的生产环境。未来研究可进一步优化模型解释性和适用范围。

Abstract: Every publicly traded U.S. company files an annual 10-K report containing
critical insights into financial health and risk. We propose Tiny eXplainable
Risk Assessor (TinyXRA), a lightweight and explainable transformer-based model
that automatically assesses company risk from these reports. Unlike prior work
that relies solely on the standard deviation of excess returns (adjusted for
the Fama-French model), which indiscriminately penalizes both upside and
downside risk, TinyXRA incorporates skewness, kurtosis, and the Sortino ratio
for more comprehensive risk assessment. We leverage TinyBERT as our encoder to
efficiently process lengthy financial documents, coupled with a novel dynamic,
attention-based word cloud mechanism that provides intuitive risk visualization
while filtering irrelevant terms. This lightweight design ensures scalable
deployment across diverse computing environments with real-time processing
capabilities for thousands of financial documents which is essential for
production systems with constrained computational resources. We employ triplet
loss for risk quartile classification, improving over pairwise loss approaches
in existing literature by capturing both the direction and magnitude of risk
differences. Our TinyXRA achieves state-of-the-art predictive accuracy across
seven test years on a dataset spanning 2013-2024, while providing transparent
and interpretable risk assessments. We conduct comprehensive ablation studies
to evaluate our contributions and assess model explanations both quantitatively
by systematically removing highly attended words and sentences, and
qualitatively by examining explanation coherence. The paper concludes with
findings, practical implications, limitations, and future research directions.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [241] [Reachability in symmetric VASS](https://arxiv.org/abs/2506.23578)
*Łukasz Kamiński,Sławomir Lasota*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.FL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We investigate the reachability problem in symmetric vector addition systems
with states (VASS), where transitions are invariant under a group of
permutations of coordinates. One extremal case, the trivial groups, yields
general VASS. In another extremal case, the symmetric groups, we show that the
reachability problem can be solved in PSPACE, regardless of the dimension of
input VASS (to be contrasted with Ackermannian complexity in general VASS). We
also consider other groups, in particular alternating and cyclic ones.
Furthermore, motivated by the open status of the reachability problem in data
VASS, we estimate the gain in complexity when the group arises as a combination
of the trivial and symmetric groups.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [242] [SABRE-FL: Selective and Accurate Backdoor Rejection for Federated Prompt Learning](https://arxiv.org/abs/2506.22506)
*Momin Ahmad Khan,Yasra Chandio,Fatima Muhammad Anwar*

Key words: 联邦学习,提示学习,后门攻击,安全性,防御方法

TL;DR: 联邦提示学习的首次后门攻击研究，提出轻量级防御方法SABRE-FL。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 研究联邦提示学习中的安全性问题，特别是后门攻击的影响。

Method: 恶意客户端通过注入视觉不可察觉的噪声触发器，使全局提示学习器易受攻击。提出SABRE-FL防御方法，通过离线训练的异常检测器过滤中毒提示更新。

Result: SABRE-FL在五个数据集上显著降低后门攻击准确性，同时保持干净输入的高准确性。

Conclusion: SABRE-FL展示了强大的防御性能，强调了未来联邦系统中提示学习的鲁棒性需求。

Abstract: Federated Prompt Learning has emerged as a communication-efficient and
privacy-preserving paradigm for adapting large vision-language models like CLIP
across decentralized clients. However, the security implications of this setup
remain underexplored. In this work, we present the first study of backdoor
attacks in Federated Prompt Learning. We show that when malicious clients
inject visually imperceptible, learnable noise triggers into input images, the
global prompt learner becomes vulnerable to targeted misclassification while
still maintaining high accuracy on clean inputs. Motivated by this
vulnerability, we propose SABRE-FL, a lightweight, modular defense that filters
poisoned prompt updates using an embedding-space anomaly detector trained
offline on out-of-distribution data. SABRE-FL requires no access to raw client
data or labels and generalizes across diverse datasets. We show, both
theoretically and empirically, that malicious clients can be reliably
identified and filtered using an embedding-based detector. Across five diverse
datasets and four baseline defenses, SABRE-FL outperforms all baselines by
significantly reducing backdoor accuracy while preserving clean accuracy,
demonstrating strong empirical performance and underscoring the need for robust
prompt learning in future federated systems.

</details>


### [243] [In-context learning for the classification of manipulation techniques in phishing emails](https://arxiv.org/abs/2506.22515)
*Antony Dalmiere,Guillaume Auriol,Vincent Nicomette,Pascal Marchand*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Traditional phishing detection often overlooks psychological manipulation.
This study investigates using Large Language Model (LLM) In-Context Learning
(ICL) for fine-grained classification of phishing emails based on a taxonomy of
40 manipulation techniques. Using few-shot examples with GPT-4o-mini on
real-world French phishing emails (SignalSpam), we evaluated performance
against a human-annotated test set (100 emails). The approach effectively
identifies prevalent techniques (e.g., Baiting, Curiosity Appeal, Request For
Minor Favor) with a promising accuracy of 0.76. This work demonstrates ICL's
potential for nuanced phishing analysis and provides insights into attacker
strategies.

</details>


### [244] [VERA: Variational Inference Framework for Jailbreaking Large Language Models](https://arxiv.org/abs/2506.22666)
*Anamika Lochab,Lu Yan,Patrick Pynadath,Xiangyu Zhang,Ruqi Zhang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The rise of API-only access to state-of-the-art LLMs highlights the need for
effective black-box jailbreak methods to identify model vulnerabilities in
real-world settings. Without a principled objective for gradient-based
optimization, most existing approaches rely on genetic algorithms, which are
limited by their initialization and dependence on manually curated prompt
pools. Furthermore, these methods require individual optimization for each
prompt, failing to provide a comprehensive characterization of model
vulnerabilities. To address this gap, we introduce VERA: Variational infErence
fRamework for jAilbreaking. VERA casts black-box jailbreak prompting as a
variational inference problem, training a small attacker LLM to approximate the
target LLM's posterior over adversarial prompts. Once trained, the attacker can
generate diverse, fluent jailbreak prompts for a target query without
re-optimization. Experimental results show that VERA achieves strong
performance across a range of target LLMs, highlighting the value of
probabilistic inference for adversarial prompt generation.

</details>


### [245] [A Survey on Model Extraction Attacks and Defenses for Large Language Models](https://arxiv.org/abs/2506.22521)
*Kaixiang Zhao,Lincan Li,Kaize Ding,Neil Zhenqiang Gong,Yue Zhao,Yushun Dong*

Key words: 模型提取攻击,语言模型,防御机制,评估指标,安全研究

TL;DR: 本文综述了针对语言模型的模型提取攻击及其防御方法，分类并分析了攻击与防御的技术，提出了评估指标和研究方向。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 模型提取攻击对语言模型的安全构成威胁，需要系统地研究攻击与防御方法以保护知识产权和用户隐私。

Method: 分类攻击为功能提取、训练数据提取和提示目标攻击，分析攻击方法；防御分为模型保护、数据隐私保护和提示目标策略。

Result: 提出了评估攻击和防御性能的专用指标，指出了当前方法的局限性，并提出集成攻击方法和自适应防御机制的未来方向。

Conclusion: 本研究为NLP研究者、工程师和安全专家提供了保护生产环境中语言模型的实用指南。

Abstract: Model extraction attacks pose significant security threats to deployed
language models, potentially compromising intellectual property and user
privacy. This survey provides a comprehensive taxonomy of LLM-specific
extraction attacks and defenses, categorizing attacks into functionality
extraction, training data extraction, and prompt-targeted attacks. We analyze
various attack methodologies including API-based knowledge distillation, direct
querying, parameter recovery, and prompt stealing techniques that exploit
transformer architectures. We then examine defense mechanisms organized into
model protection, data privacy protection, and prompt-targeted strategies,
evaluating their effectiveness across different deployment scenarios. We
propose specialized metrics for evaluating both attack effectiveness and
defense performance, addressing the specific challenges of generative language
models. Through our analysis, we identify critical limitations in current
approaches and propose promising research directions, including integrated
attack methodologies and adaptive defense mechanisms that balance security with
model utility. This work serves NLP researchers, ML engineers, and security
professionals seeking to protect language models in production environments.

</details>


### [246] [General Autonomous Cybersecurity Defense: Learning Robust Policies for Dynamic Topologies and Diverse Attackers](https://arxiv.org/abs/2506.22706)
*Arun Ramamurthy,Neil Dhir*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the face of evolving cyber threats such as malware, ransomware and
phishing, autonomous cybersecurity defense (ACD) systems have become essential
for real-time threat detection and response with optional human intervention.
However, existing ACD systems rely on limiting assumptions, particularly the
stationarity of the underlying network dynamics. In real-world scenarios,
network topologies can change due to actions taken by attackers or defenders,
system failures, or time evolution of networks, leading to failures in the
adaptive capabilities of current defense agents. Moreover, many agents are
trained on static environments, resulting in overfitting to specific
topologies, which hampers their ability to generalize to out-of-distribution
network topologies. This work addresses these challenges by exploring methods
for developing agents to learn generalizable policies across dynamic network
environments -- general ACD (GACD).

</details>


### [247] [Kill Two Birds with One Stone! Trajectory enabled Unified Online Detection of Adversarial Examples and Backdoor Attacks](https://arxiv.org/abs/2506.22722)
*Anmin Fu,Fanyu Meng,Huaibing Peng,Hua Ma,Zhi Zhang,Yifeng Zheng,Willy Susilo,Yansong Gao*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The proposed UniGuard is the first unified online detection framework capable
of simultaneously addressing adversarial examples and backdoor attacks.
UniGuard builds upon two key insights: first, both AE and backdoor attacks have
to compromise the inference phase, making it possible to tackle them
simultaneously during run-time via online detection. Second, an adversarial
input, whether a perturbed sample in AE attacks or a trigger-carrying sample in
backdoor attacks, exhibits distinctive trajectory signatures from a benign
sample as it propagates through the layers of a DL model in forward inference.
The propagation trajectory of the adversarial sample must deviate from that of
its benign counterpart; otherwise, the adversarial objective cannot be
fulfilled. Detecting these trajectory signatures is inherently challenging due
to their subtlety; UniGuard overcomes this by treating the propagation
trajectory as a time-series signal, leveraging LSTM and spectrum transformation
to amplify differences between adversarial and benign trajectories that are
subtle in the time domain. UniGuard exceptional efficiency and effectiveness
have been extensively validated across various modalities (image, text, and
audio) and tasks (classification and regression), ranging from diverse model
architectures against a wide range of AE attacks and backdoor attacks,
including challenging partial backdoors and dynamic triggers. When compared to
SOTA methods, including ContraNet (NDSS 22) specific for AE detection and TED
(IEEE SP 24) specific for backdoor detection, UniGuard consistently
demonstrates superior performance, even when matched against each method's
strengths in addressing their respective threats-each SOTA fails to parts of
attack strategies while UniGuard succeeds for all.

</details>


### [248] [Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned Large Language Models](https://arxiv.org/abs/2506.24056)
*Tung-Ling Li,Hongliang Liu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce logit-gap steering, a fast jailbreak framework that casts the
refusal-affirmation gap of RLHF-aligned language models as a single pass over
the vocabulary. A forward-computable score blends gap reduction with
lightweight proxies for KL penalty and reward shift, allowing a "sort-sum-stop"
sweep to complete in under a second and return a short suffix--two orders of
magnitude fewer model calls than beam or gradient attacks. The same suffix
generalises to unseen prompts and scales from 0.5 B to 70 B checkpoints,
lifting one-shot attack success from baseline levels to 80-100% while
preserving topical coherence. Beyond efficiency, these suffixes expose
sentence-boundary reward cliffs and other alignment artefacts, offering a
lightweight probe into how safety tuning reshapes internal representations.

</details>


### [249] [A Study on Semi-Supervised Detection of DDoS Attacks under Class Imbalance](https://arxiv.org/abs/2506.22949)
*Ehsan Hallaji,Vaishnavi Shanmugam,Roozbeh Razavi-Far,Mehrdad Saif*

Key words: DDoS, 半监督学习, 入侵检测, 数据不平衡, 标注不足

TL;DR: 论文探讨了在类别不平衡和标注数据不足的情况下，利用半监督学习（SSL）技术提升DDoS攻击检测的效果。评估了13种先进SSL算法在极端环境中的表现，为设计鲁棒的入侵检测系统提供参考。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 解决DDoS攻击检测中数据不平衡和标注不足的挑战，提升网络安全防御能力。

Method: 采用半监督学习（SSL）技术，评估13种SSL算法在不同场景下的表现。

Result: 发现部分SSL算法在极端环境下表现良好，为设计鲁棒的入侵检测系统提供了依据。

Conclusion: 半监督学习技术能有效改善DDoS攻击检测，尤其是在数据不平衡和标注不足的场景中。

Abstract: One of the most difficult challenges in cybersecurity is eliminating
Distributed Denial of Service (DDoS) attacks. Automating this task using
artificial intelligence is a complex process due to the inherent class
imbalance and lack of sufficient labeled samples of real-world datasets. This
research investigates the use of Semi-Supervised Learning (SSL) techniques to
improve DDoS attack detection when data is imbalanced and partially labeled. In
this process, 13 state-of-the-art SSL algorithms are evaluated for detecting
DDoS attacks in several scenarios. We evaluate their practical efficacy and
shortcomings, including the extent to which they work in extreme environments.
The results will offer insight into designing intelligent Intrusion Detection
Systems (IDSs) that are robust against class imbalance and handle partially
labeled data.

</details>


### [250] [MetaCipher: A General and Extensible Reinforcement Learning Framework for Obfuscation-Based Jailbreak Attacks on Black-Box LLMs](https://arxiv.org/abs/2506.22557)
*Boyuan Chen,Minghao Shao,Abdul Basit,Siddharth Garg,Muhammad Shafique*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The growing capabilities of large language models (LLMs) have exposed them to
increasingly sophisticated jailbreak attacks. Among these, obfuscation-based
attacks -- which encrypt malicious content to evade detection -- remain highly
effective. By leveraging the reasoning ability of advanced LLMs to interpret
encrypted prompts, such attacks circumvent conventional defenses that rely on
keyword detection or context filtering. These methods are very difficult to
defend against, as existing safety mechanisms are not designed to interpret or
decode ciphered content. In this work, we propose \textbf{MetaCipher}, a novel
obfuscation-based jailbreak framework, along with a reinforcement
learning-based dynamic cipher selection mechanism that adaptively chooses
optimal encryption strategies from a cipher pool. This approach enhances
jailbreak effectiveness and generalizability across diverse task types, victim
LLMs, and safety guardrails. Our framework is modular and extensible by design,
supporting arbitrary cipher families and accommodating evolving adversarial
strategies. We complement our method with a large-scale empirical analysis of
cipher performance across multiple victim LLMs. Within as few as 10 queries,
MetaCipher achieves over 92\% attack success rate (ASR) on most recent standard
malicious prompt benchmarks against state-of-the-art non-reasoning LLMs, and
over 74\% ASR against reasoning-capable LLMs, outperforming all existing
obfuscation-based jailbreak methods. These results highlight the long-term
robustness and adaptability of our approach, making it more resilient than
prior methods in the face of advancing safety measures.

</details>


### [251] [A User-Centric, Privacy-Preserving, and Verifiable Ecosystem for Personal Data Management and Utilization](https://arxiv.org/abs/2506.22606)
*Osama Zafar,Mina Namazi,Yuqiao Xu,Youngjin Yoo,Erman Ayday*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the current paradigm of digital personalized services, the centralized
management of personal data raises significant privacy concerns, security
vulnerabilities, and diminished individual autonomy over sensitive information.
Despite their efficiency, traditional centralized architectures frequently fail
to satisfy rigorous privacy requirements and expose users to data breaches and
unauthorized access risks. This pressing challenge calls for a fundamental
paradigm shift in methodologies for collecting, storing, and utilizing personal
data across diverse sectors, including education, healthcare, and finance.
  This paper introduces a novel decentralized, privacy-preserving architecture
that handles heterogeneous personal information, ranging from educational
credentials to health records and financial data. Unlike traditional models,
our system grants users complete data ownership and control, allowing them to
selectively share information without compromising privacy. The architecture's
foundation comprises advanced privacy-enhancing technologies, including secure
enclaves and federated learning, enabling secure computation, verification, and
data sharing. The system supports diverse functionalities, including local
computation, model training, and privacy-preserving data sharing, while
ensuring data credibility and robust user privacy.

</details>


### [252] [Efficient Cybersecurity Assessment Using SVM and Fuzzy Evidential Reasoning for Resilient Infrastructure](https://arxiv.org/abs/2506.22938)
*Zaydon L. Ali,Wassan Saad Abduljabbar Hayale,Israa Ibraheem Al_Barazanchi,Ravi Sekhar,Pritesh Shah,Sushma Parihar*

Key words: 加密技术, 支持向量机, 模糊证据推理, 安全评估

TL;DR: 提出了一种基于支持向量机（SVM）和模糊证据推理（ER）的安全评估模型，用于快速准确识别加密算法的安全性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 当前加密模型存在安全性问题，逐个测试算法效率低下，需要一种更高效的方法来评估加密技术的安全性。

Method: 结合支持向量机（SVM）和模糊证据推理（ER）构建安全阶段暴露模型，利用常见安全组件形成数据集。

Result: 通过召回率、F1分数和准确性等指标验证了模型性能。

Conclusion: 该模型能系统化处理风险数据，为加密算法选择提供高效支持。

Abstract: With current advancement in hybermedia knowledges, the privacy of digital
information has developed a critical problem. To overawed the susceptibilities
of present security protocols, scholars tend to focus mainly on efforts on
alternation of current protocols. Over past decade, various proposed encoding
models have been shown insecurity, leading to main threats against significant
data. Utilizing the suitable encryption model is very vital means of guard
against various such, but algorithm is selected based on the dependency of data
which need to be secured. Moreover, testing potentiality of the security
assessment one by one to identify the best choice can take a vital time for
processing. For faster and precisive identification of assessment algorithm, we
suggest a security phase exposure model for cipher encryption technique by
invoking Support Vector Machine (SVM). In this work, we form a dataset using
usual security components like contrast, homogeneity. To overcome the
uncertainty in analysing the security and lack of ability of processing data to
a risk assessment mechanism. To overcome with such complications, this paper
proposes an assessment model for security issues using fuzzy evidential
reasoning (ER) approaches. Significantly, the model can be utilised to process
and assemble risk assessment data on various aspects in systematic ways. To
estimate the performance of our framework, we have various analyses like,
recall, F1 score and accuracy.

</details>


### [253] [From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows](https://arxiv.org/abs/2506.23260)
*Mohamed Amine Ferrag,Norbert Tihanyi,Djallel Hamouda,Leandros Maglaras,Merouane Debbah*

Key words: 大型语言模型、威胁模型、安全、代理生态系统、攻击技术

TL;DR: 该论文提出了一种统一的大型语言模型（LLM）代理生态系统威胁模型，涵盖多个攻击领域，并提出了未来的研究方向。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 随着LLM代理生态系统的快速发展，插件和协议的激增导致发现机制和安全性不足，迫切需要全面的威胁模型和防御策略。

Method: 论文提出了一个端到端的威胁模型，包括输入操纵、模型妥协、系统与隐私攻击和协议漏洞四大领域，并评估了现有防御措施。

Result: 论文系统地分类了三十多种攻击技术，并针对每种攻击领域提供了实际可行的防御建议。

Conclusion: 通过建立全面的威胁分类和未来研究方向，论文为设计鲁棒的防御机制和最佳实践提供了参考。

Abstract: Autonomous AI agents powered by large language models (LLMs) with structured
function-calling interfaces have dramatically expanded capabilities for
real-time data retrieval, complex computation, and multi-step orchestration.
Yet, the explosive proliferation of plugins, connectors, and inter-agent
protocols has outpaced discovery mechanisms and security practices, resulting
in brittle integrations vulnerable to diverse threats. In this survey, we
introduce the first unified, end-to-end threat model for LLM-agent ecosystems,
spanning host-to-tool and agent-to-agent communications, formalize adversary
capabilities and attacker objectives, and catalog over thirty attack
techniques. Specifically, we organized the threat model into four domains:
Input Manipulation (e.g., prompt injections, long-context hijacks, multimodal
adversarial inputs), Model Compromise (e.g., prompt- and parameter-level
backdoors, composite and encrypted multi-backdoors, poisoning strategies),
System and Privacy Attacks (e.g., speculative side-channels, membership
inference, retrieval poisoning, social-engineering simulations), and Protocol
Vulnerabilities (e.g., exploits in Model Context Protocol (MCP), Agent
Communication Protocol (ACP), Agent Network Protocol (ANP), and Agent-to-Agent
(A2A) protocol). For each category, we review representative scenarios, assess
real-world feasibility, and evaluate existing defenses. Building on our threat
taxonomy, we identify key open challenges and future research directions, such
as securing MCP deployments through dynamic trust management and cryptographic
provenance tracking; designing and hardening Agentic Web Interfaces; and
achieving resilience in multi-agent and federated environments. Our work
provides a comprehensive reference to guide the design of robust defense
mechanisms and establish best practices for resilient LLM-agent workflows.

</details>


### [254] [Securing AI Systems: A Guide to Known Attacks and Impacts](https://arxiv.org/abs/2506.23296)
*Naoto Kiribuchi,Kengo Zenitani,Takayuki Semitsu*

Key words: AI安全、对抗性攻击、CIA三要素、信息泄露、系统妥协、资源耗尽

TL;DR: 论文概述了AI系统特有的安全威胁和对抗性攻击，分类了11种主要攻击类型，并展示了攻击技术如何影响保密性、完整性和可用性，以帮助研究人员和政策制定者加强AI系统安全。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: AI系统面临独特的安全威胁，但相关研究和技术还未被广泛了解，需提供系统的知识来识别风险并实施防御。

Method: 通过分类11种主要攻击类型，并将攻击技术与CIA安全三要素（保密性、完整性和可用性）的直接影响关联。

Result: 明确了攻击技术与实际影响的对应关系，为不同利益相关者提供了识别和防御AI系统威胁的基础知识。

Conclusion: 通过系统化分类和关联攻击技术，论文为提升AI系统安全提供了实用框架，可帮助非专业人士理解并应对AI特有风险。

Abstract: Embedded into information systems, artificial intelligence (AI) faces
security threats that exploit AI-specific vulnerabilities. This paper provides
an accessible overview of adversarial attacks unique to predictive and
generative AI systems. We identify eleven major attack types and explicitly
link attack techniques to their impacts -- including information leakage,
system compromise, and resource exhaustion -- mapped to the confidentiality,
integrity, and availability (CIA) security triad. We aim to equip researchers,
developers, security practitioners, and policymakers, even those without
specialized AI security expertise, with foundational knowledge to recognize
AI-specific risks and implement effective defenses, thereby enhancing the
overall security posture of AI systems.

</details>


### [255] [Interpretable by Design: MH-AutoML for Transparent and Efficient Android Malware Detection without Compromising Performance](https://arxiv.org/abs/2506.23314)
*Joner Assolin,Gabriel Canto,Diego Kreutz,Eduardo Feitosa,Hendrio Bragança,Angelo Nogueira,Vanderson Rocha*

Key words: AutoML, Android恶意软件检测, 可解释性, 框架

TL;DR: MH-AutoML是一个针对Android恶意软件检测的领域专用框架，通过自动化机器学习流程并提升透明度和可解释性，优于现有AutoML方案。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 当前AutoML解决方案缺乏透明度和可解释性，限制了在Android恶意软件检测中的应用。

Method: 提出MH-AutoML框架，自动化整个机器学习流程，包括数据预处理、特征工程、算法选择和超参数调优。

Result: 在与七种AutoML框架的对比中，MH-AutoML在召回率、透明度和控制能力上表现更好。

Conclusion: MH-AutoML在计算效率与性能之间取得平衡，适用于对性能和可解释性要求高的网络安全应用。

Abstract: Malware detection in Android systems requires both cybersecurity expertise
and machine learning (ML) techniques. Automated Machine Learning (AutoML) has
emerged as an approach to simplify ML development by reducing the need for
specialized knowledge. However, current AutoML solutions typically operate as
black-box systems with limited transparency, interpretability, and experiment
traceability. To address these limitations, we present MH-AutoML, a
domain-specific framework for Android malware detection. MH-AutoML automates
the entire ML pipeline, including data preprocessing, feature engineering,
algorithm selection, and hyperparameter tuning. The framework incorporates
capabilities for interpretability, debugging, and experiment tracking that are
often missing in general-purpose solutions. In this study, we compare MH-AutoML
against seven established AutoML frameworks: Auto-Sklearn, AutoGluon, TPOT,
HyperGBM, Auto-PyTorch, LightAutoML, and MLJAR. Results show that MH-AutoML
achieves better recall rates while providing more transparency and control. The
framework maintains computational efficiency comparable to other solutions,
making it suitable for cybersecurity applications where both performance and
explainability matter.

</details>


### [256] [Detect \& Score: Privacy-Preserving Misbehaviour Detection and Contribution Evaluation in Federated Learning](https://arxiv.org/abs/2506.23583)
*Marvin Xhemrishi,Alexandre Graell i Amat,Balázs Pejó*

Key words: 联邦学习,安全聚合,贡献评估,恶意行为检测

TL;DR: 结合QI和FedGT的优势，提出了一种既实现稳健恶意行为检测（MD）又具备准确贡献评估（CE）能力的方法，实验表明其性能优于单独使用任一方法。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 联邦学习中安全聚合保护了客户端数据隐私，但也使恶意行为检测和贡献评估变得复杂。现有方法QI和FedGT分别存在CE和MD的不足，需要一种综合解决方案。

Method: 整合QI和FedGT的方法，以利用QI的贡献评估能力和FedGT的恶意行为检测能力。

Result: 实验结果表明，结合后的方法在MD和CE方面均优于单独使用QI或FedGT。

Conclusion: 通过整合QI和FedGT的优势，本文提出了一种更全面的联邦学习解决方案，同时实现了稳健的MD和准确的CE。

Abstract: Federated learning with secure aggregation enables private and collaborative
learning from decentralised data without leaking sensitive client information.
However, secure aggregation also complicates the detection of malicious client
behaviour and the evaluation of individual client contributions to the
learning. To address these challenges, QI (Pejo et al.) and FedGT (Xhemrishi et
al.) were proposed for contribution evaluation (CE) and misbehaviour detection
(MD), respectively. QI, however, lacks adequate MD accuracy due to its reliance
on the random selection of clients in each training round, while FedGT lacks
the CE ability. In this work, we combine the strengths of QI and FedGT to
achieve both robust MD and accurate CE. Our experiments demonstrate superior
performance compared to using either method independently.

</details>


### [257] [SoK: Semantic Privacy in Large Language Models](https://arxiv.org/abs/2506.23603)
*Baihe Ma,Yanna Jiang,Xu Wang,Guangshen Yu,Qin Wang,Caijun Sun,Chen Li,Xuelei Qi,Ying He,Wei Ni,Ren Ping Liu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As Large Language Models (LLMs) are increasingly deployed in sensitive
domains, traditional data privacy measures prove inadequate for protecting
information that is implicit, contextual, or inferable - what we define as
semantic privacy. This Systematization of Knowledge (SoK) introduces a
lifecycle-centric framework to analyze how semantic privacy risks emerge across
input processing, pretraining, fine-tuning, and alignment stages of LLMs. We
categorize key attack vectors and assess how current defenses, such as
differential privacy, embedding encryption, edge computing, and unlearning,
address these threats. Our analysis reveals critical gaps in semantic-level
protection, especially against contextual inference and latent representation
leakage. We conclude by outlining open challenges, including quantifying
semantic leakage, protecting multimodal inputs, balancing de-identification
with generation quality, and ensuring transparency in privacy enforcement. This
work aims to inform future research on designing robust, semantically aware
privacy-preserving techniques for LLMs.

</details>


### [258] [gMBA: Expression Semantic Guided Mixed Boolean-Arithmetic Deobfuscation Using Transformer Architectures](https://arxiv.org/abs/2506.23634)
*Youjeong Noh,Joon-Young Paik,Jingun Kwon,Eun-Sun Cho*

Key words: MBA混淆, 反混淆, 真值表, Transformer, 语义信息

TL;DR: 论文提出了一种基于真值表和Transformer架构的MBA反混淆框架gMBA，显著提升了反混淆性能。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: MBA混淆被恶意软件滥用，传统方法忽视内部语义信息，需改进。

Method: 利用真值表作为语义表示，结合Transformer的Seq2Seq架构。

Result: 实验表明，语义信息显著提升了反混淆效果。

Conclusion: 内部语义信息对恢复混淆代码至关重要。

Abstract: Mixed Boolean-Arithmetic (MBA) obfuscation protects intellectual property by
converting programs into forms that are more complex to analyze. However, MBA
has been increasingly exploited by malware developers to evade detection and
cause significant real-world problems. Traditional MBA deobfuscation methods
often consider these expressions as part of a black box and overlook their
internal semantic information. To bridge this gap, we propose a truth table,
which is an automatically constructed semantic representation of an
expression's behavior that does not rely on external resources. The truth table
is a mathematical form that represents the output of expression for all
possible combinations of input. We also propose a general and extensible guided
MBA deobfuscation framework (gMBA) that modifies a Transformer-based neural
encoder-decoder Seq2Seq architecture to incorporate this semantic guidance.
Experimental results and in-depth analysis show that integrating expression
semantics significantly improves performance and highlights the importance of
internal semantic expressions in recovering obfuscated code to its original
form.

</details>


### [259] [Differentially Private Synthetic Data Release for Topics API Outputs](https://arxiv.org/abs/2506.23855)
*Travis Dick,Alessandro Epasto,Adel Javanmard,Josh Karlin,Andres Munoz Medina,Vahab Mirrokni,Sergei Vassilvitskii,Peilin Zhong*

Key words: Privacy-Preserving Ads API, Topics API, differential privacy, synthetic data

TL;DR: 论文提出了一种新方法，用于生成合成API输出数据，以平衡研究需求与隐私保护，重点关注Google Chrome的Topics API。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 研究Privacy-Preserving Ads API的隐私属性时缺乏公开的真实数据，阻碍了实证分析。

Method: 开发了一种基于差分隐私的方法，生成与真实Topics API数据风险属性匹配的合成数据集。

Result: 生成了与真实数据匹配的差分隐私合成数据集，并开源了匿名数据集。

Conclusion: 该工作有助于提升Privacy-Preserving Ads API隐私属性的透明度。

Abstract: The analysis of the privacy properties of Privacy-Preserving Ads APIs is an
area of research that has received strong interest from academics, industry,
and regulators. Despite this interest, the empirical study of these methods is
hindered by the lack of publicly available data. Reliable empirical analysis of
the privacy properties of an API, in fact, requires access to a dataset
consisting of realistic API outputs; however, privacy concerns prevent the
general release of such data to the public.
  In this work, we develop a novel methodology to construct synthetic API
outputs that are simultaneously realistic enough to enable accurate study and
provide strong privacy protections. We focus on one Privacy-Preserving Ads
APIs: the Topics API, part of Google Chrome's Privacy Sandbox. We developed a
methodology to generate a differentially-private dataset that closely matches
the re-identification risk properties of the real Topics API data. The use of
differential privacy provides strong theoretical bounds on the leakage of
private user information from this release.
  Our methodology is based on first computing a large number of
differentially-private statistics describing how output API traces evolve over
time. Then, we design a parameterized distribution over sequences of API traces
and optimize its parameters so that they closely match the statistics obtained.
Finally, we create the synthetic data by drawing from this distribution.
  Our work is complemented by an open-source release of the anonymized dataset
obtained by this methodology. We hope this will enable external researchers to
analyze the API in-depth and replicate prior and future work on a realistic
large-scale dataset. We believe that this work will contribute to fostering
transparency regarding the privacy properties of Privacy-Preserving Ads APIs.

</details>


### [260] [RawMal-TF: Raw Malware Dataset Labeled by Type and Family](https://arxiv.org/abs/2506.23909)
*David Bálik,Martin Jureček,Mark Stamp*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This work addresses the challenge of malware classification using machine
learning by developing a novel dataset labeled at both the malware type and
family levels. Raw binaries were collected from sources such as VirusShare, VX
Underground, and MalwareBazaar, and subsequently labeled with family
information parsed from binary names and type-level labels integrated from
ClarAVy. The dataset includes 14 malware types and 17 malware families, and was
processed using a unified feature extraction pipeline based on static analysis,
particularly extracting features from Portable Executable headers, to support
advanced classification tasks. The evaluation was focused on three key
classification tasks. In the binary classification of malware versus benign
samples, Random Forest and XGBoost achieved high accuracy on the full datasets,
reaching 98.5% for type-based detection and 98.98% for family-based detection.
When using truncated datasets of 1,000 samples to assess performance under
limited data conditions, both models still performed strongly, achieving 97.6%
for type-based detection and 98.66% for family-based detection. For interclass
classification, which distinguishes between malware types or families, the
models reached up to 97.5% accuracy on type-level tasks and up to 93.7% on
family-level tasks. In the multiclass classification setting, which assigns
samples to the correct type or family, SVM achieved 81.1% accuracy on type
labels, while Random Forest and XGBoost reached approximately 73.4% on family
labels. The results highlight practical trade-offs between accuracy and
computational cost, and demonstrate that labeling at both the type and family
levels enables more fine-grained and insightful malware classification. The
work establishes a robust foundation for future research on advanced malware
detection and classification.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [261] [Data-Driven Self-Supervised Learning for the Discovery of Solution Singularity for Partial Differential Equations](https://arxiv.org/abs/2506.23344)
*Difeng Cai,Paulina Sepúlveda*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: math.NA

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The appearance of singularities in the function of interest constitutes a
fundamental challenge in scientific computing. It can significantly undermine
the effectiveness of numerical schemes for function approximation, numerical
integration, and the solution of partial differential equations (PDEs), etc.
The problem becomes more sophisticated if the location of the singularity is
unknown, which is often encountered in solving PDEs. Detecting the singularity
is therefore critical for developing efficient adaptive methods to reduce
computational costs in various applications. In this paper, we consider
singularity detection in a purely data-driven setting. Namely, the input only
contains given data, such as the vertex set from a mesh. To overcome the
limitation of the raw unlabeled data, we propose a self-supervised learning
(SSL) framework for estimating the location of the singularity. A key component
is a filtering procedure as the pretext task in SSL, where two filtering
methods are presented, based on $k$ nearest neighbors and kernel density
estimation, respectively. We provide numerical examples to illustrate the
potential pathological or inaccurate results due to the use of raw data without
filtering. Various experiments are presented to demonstrate the ability of the
proposed approach to deal with input perturbation, label corruption, and
different kinds of singularities such interior circle, boundary layer,
concentric semicircles, etc.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [262] [Overparametrized models with posterior drift](https://arxiv.org/abs/2506.23619)
*Guillaume Coqueret,Martial Laguerre*

Key words: 后验漂移, 过度参数化模型, 样本外预测, 金融市场, 带宽参数

TL;DR: 研究后验漂移对过度参数化机器学习模型样本外预测准确性的影响，发现金融市场的机制变化会导致性能损失，强调市场择时策略对子周期和带宽参数的敏感性。

<details>
  <summary>Details</summary>

Main category: q-fin.ST

Motivation: 探讨过度参数化模型在训练和测试样本间数据生成过程变化时的预测性能损失问题，特别是在金融市场等可能发生机制变化的场景中。

Method: 应用于股票溢价预测，分析市场择时策略对子周期和带宽参数的敏感性，比较不同带宽参数下的回报表现。

Result: 小带宽在15年周期内产生异质性回报，而大带宽结果更一致但风险调整后收益较低。

Conclusion: 建议在股票市场预测中谨慎使用大型线性模型。

Abstract: This paper investigates the impact of posterior drift on out-of-sample
forecasting accuracy in overparametrized machine learning models. We document
the loss in performance when the loadings of the data generating process change
between the training and testing samples. This matters crucially in settings in
which regime changes are likely to occur, for instance, in financial markets.
Applied to equity premium forecasting, our results underline the sensitivity of
a market timing strategy to sub-periods and to the bandwidth parameters that
control the complexity of the model. For the average investor, we find that
focusing on holding periods of 15 years can generate very heterogeneous
returns, especially for small bandwidths. Large bandwidths yield much more
consistent outcomes, but are far less appealing from a risk-adjusted return
standpoint. All in all, our findings tend to recommend cautiousness when
resorting to large linear models for stock market predictions.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [263] [Learning Individual Reproductive Behavior from Aggregate Fertility Rates via Neural Posterior Estimation](https://arxiv.org/abs/2506.22607)
*Daniel Ciganda,Ignacio Campón,Iñaki Permanyer,Jakob H Macke*

Key words: 年龄别生育率,贝叶斯推断,序列神经后验估计,人口预测,数字孪生

TL;DR: 论文提出了一种基于贝叶斯框架和序列神经后验估计的方法，从宏观生育率数据中推断个体生育行为参数，并成功验证了其预测能力。

<details>
  <summary>Details</summary>

Main category: stat.AP

Motivation: 由于年龄别生育率（ASFRs）的聚合性质掩盖了驱动生育趋势的行为机制，作者希望从宏观数据中恢复这些机制。

Method: 采用了一种无似然的贝叶斯框架，结合个体生育过程模型和序列神经后验估计（SNPE），从ASFRs和计划外生育年龄分布推断八项行为与生物参数。

Result: 方法应用于美国和拉美国家的数据，不仅重现了生育率分布，还成功预测了未参与估计的微观行为分布，如初次性行为年龄和生育间隔。

Conclusion: 该方法能生成完整的合成生命周期数据，支持行为显式的人口预测和构建人口数字孪生。

Abstract: While age-specific fertility rates (ASFRs) provide the most extensive record
of reproductive change, their aggregate nature masks the underlying behavioral
mechanisms that ultimately drive fertility trends. To recover these mechanisms,
we develop a likelihood-free Bayesian framework that couples an
individual-level model of the reproductive process with Sequential Neural
Posterior Estimation (SNPE). This allows us to infer eight behavioral and
biological parameters from just two aggregate series: ASFRs and the age-profile
of planned versus unplanned births. Applied to U.S. National Survey of Family
Growth cohorts and to Demographic and Health Survey cohorts from Colombia, the
Dominican Republic, and Peru, the method reproduces observed fertility
schedules and, critically, predicts out-of-sample micro-level distributions of
age at first sex, inter-birth intervals, and family-size ideals, none of which
inform the estimation step. Because the fitted model yields complete synthetic
life histories, it enables behaviorally explicit population forecasts and
supports the construction of demographic digital twins.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [264] [Teaching a Language Model to Speak the Language of Tools](https://arxiv.org/abs/2506.23394)
*Simeon Emanuilov*

Key words: 多语言模型，工具调用，保加利亚语，TUCAN

TL;DR: 该研究提出了一种方法，用于改造现有语言模型以支持非英语语言的工具调用功能，并以保加利亚语为案例展示了显著成效。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 解决多语言模型在非英语语言中工具调用能力不足的问题。

Method: 通过继续训练BgGPT系列模型，使用一个包含10,035个函数调用示例的双语数据集，支持标准化协议如MCP。

Result: TUCAN模型在函数调用准确率上提升28.75%，并保持了语言理解能力。

Conclusion: 该研究为扩展工具增强能力至非英语系统提供了实用方法。

Abstract: External tool integration through function-calling is essential for practical
language model applications, yet most multilingual models lack reliable
tool-use capabilities in non-English languages. Even state-of-the-art
multilingual models struggle with determining when to use tools and generating
the structured outputs required for function calls, often exhibiting language
confusion when prompted in lower-resource languages. This work presents a
methodology for adapting existing language models to enable robust tool use in
any target language, using Bulgarian as a case study. The approach involves
continued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a
novel bilingual dataset of 10,035 function-calling examples designed to support
standardized protocols like MCP (Model Context Protocol). The research
introduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to
28.75% improvement in function-calling accuracy over base models while
preserving core language understanding, as verified on established Bulgarian
benchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready
response formatting with clean, parsable function calls, contrasting with the
verbose and inconsistent outputs of base models. The models, evaluation
framework, and dataset are released to enable replication for other languages.
This work demonstrates a practical approach for extending tool-augmented
capabilities beyond English-centric systems.

</details>


### [265] [Interact2Vec -- An efficient neural network-based model for simultaneously learning users and items embeddings in recommender systems](https://arxiv.org/abs/2506.22648)
*Pedro R. Pires,Tiago A. Almeida*

Key words: 推荐系统, 神经网络, 低维嵌入, 隐式反馈

TL;DR: 提出了Interact2Vec模型，通过神经网络学习低维嵌入表示用户和物品，仅需隐式反馈，并在实验中表现出色。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 解决推荐系统中高维数据稀疏性问题，同时避免依赖复杂架构或内容数据。

Method: 基于神经网络的Interact2Vec模型，利用自然语言处理模型策略优化训练和嵌入表示。

Result: 在30%的数据集中表现第二或第三，训练时间减少274%，嵌入质量优异。

Conclusion: Interact2Vec在计算资源稀缺的场景下高效生成嵌入，表现优异。

Abstract: Over the past decade, recommender systems have experienced a surge in
popularity. Despite notable progress, they grapple with challenging issues,
such as high data dimensionality and sparseness. Representing users and items
as low-dimensional embeddings learned via neural networks has become a leading
solution. However, while recent studies show promising results, many approaches
rely on complex architectures or require content data, which may not always be
available. This paper presents Interact2Vec, a novel neural network-based model
that simultaneously learns distributed embeddings for users and items while
demanding only implicit feedback. The model employs state-of-the-art strategies
that natural language processing models commonly use to optimize the training
phase and enhance the final embeddings. Two types of experiments were conducted
regarding the extrinsic and intrinsic quality of the model. In the former, we
benchmarked the recommendations generated by Interact2Vec's embeddings in a
top-$N$ ranking problem, comparing them with six other recommender algorithms.
The model achieved the second or third-best results in 30\% of the datasets,
being competitive with other recommenders, and has proven to be very efficient
with an average training time reduction of 274\% compared to other
embedding-based models. Later, we analyzed the intrinsic quality of the
embeddings through similarity tables. Our findings suggest that Interact2Vec
can achieve promising results, especially on the extrinsic task, and is an
excellent embedding-generator model for scenarios of scarce computing
resources, enabling the learning of item and user embeddings simultaneously and
efficiently.

</details>


### [266] [Enhancing Live Broadcast Engagement: A Multi-modal Approach to Short Video Recommendations Using MMGCN and User Preferences](https://arxiv.org/abs/2506.23085)
*Saeid Aghasoleymani Najafabadi*

Key words: 多模态推荐系统, 图卷积网络, 直播平台, 个性化推荐

TL;DR: 本文提出了一种基于多模态图卷积网络（MMGCN）的短视频推荐系统，通过结合用户偏好、视频内容和上下文信息，提升直播平台的用户参与度。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 为了提高直播平台的用户参与度，需要一种能够充分利用多模态数据（用户行为、视频特征、上下文信息）的个性化推荐系统。

Method: 采用MMGCN模型，结合协同过滤和基于内容的过滤技术，分析用户与视频之间的复杂关系。

Result: 在Kwai、TikTok和MovieLens数据集上，MMGCN模型的F1分数分别为0.574、0.506和0.197，优于DeepFM等基线模型。

Conclusion: 多模态集成和以用户为中心的方法在提升推荐系统性能中至关重要，尤其是在直播平台的互动内容发现中。

Abstract: The purpose of this paper is to explore a multi-modal approach to enhancing
live broadcast engagement by developing a short video recommendation system
that incorporates Multi-modal Graph Convolutional Networks (MMGCN) with user
preferences. In order to provide personalized recommendations tailored to
individual interests, the proposed system takes into account user interaction
data, video content features, and contextual information. With the aid of a
hybrid approach combining collaborative filtering and content-based filtering
techniques, the system is able to capture nuanced relationships between users,
video attributes, and engagement patterns. Three datasets are used to evaluate
the effectiveness of the system: Kwai, TikTok, and MovieLens. Compared to
baseline models, such as DeepFM, Wide & Deep, LightGBM, and XGBoost, the
proposed MMGCN-based model shows superior performance. A notable feature of the
proposed model is that it outperforms all baseline methods in capturing diverse
user preferences and making accurate, personalized recommendations, resulting
in a Kwai F1 score of 0.574, a Tiktok F1 score of 0.506, and a MovieLens F1
score of 0.197. We emphasize the importance of multi-modal integration and
user-centric approaches in advancing recommender systems, emphasizing the role
they play in enhancing content discovery and audience interaction on live
broadcast platforms.

</details>


### [267] [Multi-task Offline Reinforcement Learning for Online Advertising in Recommender Systems](https://arxiv.org/abs/2506.23090)
*Langming Liu,Wanyu Wang,Chi Zhang,Bo Li,Hongzhi Yin,Xuetao Wei,Wenbo Su,Bo Zheng,Xiangyu Zhao*

Key words: 离线强化学习,广告推荐,多任务学习,因果建模,预算分配

TL;DR: 论文提出了一种名为MTORL的多任务离线强化学习模型，用于解决广告推荐平台中稀疏数据环境下的挑战，包括过度估计、分布偏移和预算限制等问题。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 当前离线强化学习方法在稀疏广告场景中存在严重的过度估计、分布偏移和忽视预算约束等问题，亟需一种新的解决方案。

Method: 通过建立广告特有的MDP框架，开发因果状态编码器捕捉用户兴趣和时间依赖性，并运用因果注意机制和多任务学习来解码动作和奖励，同时处理渠道推荐和预算分配。

Result: 在离线和在线环境中的实验表明，MTORL优于最先进的方法。

Conclusion: MTORL通过结合因果建模和多任务学习，有效解决了稀疏广告场景中的关键问题，并展现了优越性能。

Abstract: Online advertising in recommendation platforms has gained significant
attention, with a predominant focus on channel recommendation and budget
allocation strategies. However, current offline reinforcement learning (RL)
methods face substantial challenges when applied to sparse advertising
scenarios, primarily due to severe overestimation, distributional shifts, and
overlooking budget constraints. To address these issues, we propose MTORL, a
novel multi-task offline RL model that targets two key objectives. First, we
establish a Markov Decision Process (MDP) framework specific to the nuances of
advertising. Then, we develop a causal state encoder to capture dynamic user
interests and temporal dependencies, facilitating offline RL through
conditional sequence modeling. Causal attention mechanisms are introduced to
enhance user sequence representations by identifying correlations among causal
states. We employ multi-task learning to decode actions and rewards,
simultaneously addressing channel recommendation and budget allocation.
Notably, our framework includes an automated system for integrating these tasks
into online advertising. Extensive experiments on offline and online
environments demonstrate MTORL's superiority over state-of-the-art methods.

</details>


### [268] [Compositions of Variant Experts for Integrating Short-Term and Long-Term Preferences](https://arxiv.org/abs/2506.23170)
*Jaime Hieu Do,Trung-Hoang Le,Hady W. Lauw*

Key words: 推荐系统, 短期偏好, 长期偏好, CoVE框架, 专家模型

TL;DR: 该论文研究了在线推荐系统中如何结合用户的短期和长期偏好以提高推荐效果，提出了一个名为CoVE的新框架。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 推荐系统在数字领域至关重要，但现有方法未能充分利用用户的短期和长期偏好。

Method: 提出了CoVE框架，通过动态整合不同专家模型来结合短期和长期偏好。

Result: 实验证实CoVE能有效提升推荐性能，并探讨了专家类型的影响。

Conclusion: 结合用户短期和长期偏好的方法可以显著提高推荐系统的效果。

Abstract: In the online digital realm, recommendation systems are ubiquitous and play a
crucial role in enhancing user experience. These systems leverage user
preferences to provide personalized recommendations, thereby helping users
navigate through the paradox of choice. This work focuses on personalized
sequential recommendation, where the system considers not only a user's
immediate, evolving session context, but also their cumulative historical
behavior to provide highly relevant and timely recommendations. Through an
empirical study conducted on diverse real-world datasets, we have observed and
quantified the existence and impact of both short-term (immediate and
transient) and long-term (enduring and stable) preferences on users' historical
interactions. Building on these insights, we propose a framework that combines
short- and long-term preferences to enhance recommendation performance, namely
Compositions of Variant Experts (CoVE). This novel framework dynamically
integrates short- and long-term preferences through the use of different
specialized recommendation models (i.e., experts). Extensive experiments
showcase the effectiveness of the proposed methods and ablation studies further
investigate the impact of variant expert types.

</details>


### [269] [Learning to Rank with Variable Result Presentation Lengths](https://arxiv.org/abs/2506.23319)
*Norman Knyazev,Harrie Oosterhuis*

Key words: Learning to Rank, 可变展示长度, VLPL, 联合优化

TL;DR: 论文提出了可变展示长度排名的任务，解决了现有LTR方法未涉及的文档展示长度与排序的联合优化问题。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 用户对相关性的感知可能因展示形式（如长度）而变化，而现有LTR方法未考虑这一点。

Method: 提出VLPL方法，一种基于Plackett-Luce列表梯度估计的联合优化文档排序与展示长度的方法。

Result: 实验表明VLPL能平衡文档的曝光度和吸引力，且简单长度感知方法也能显著优于固定长度模型。

Conclusion: 文档展示与LTR的结合重要但具挑战性，VLPL为此提供了有效解决方案。

Abstract: Learning to Rank (LTR) methods generally assume that each document in a top-K
ranking is presented in an equal format. However, previous work has shown that
users' perceptions of relevance can be changed by varying presentations, i.e.,
allocating more vertical space to some documents to provide additional textual
or image information. Furthermore, presentation length can also redirect
attention, as users are more likely to notice longer presentations when
scrolling through results. Deciding on the document presentation lengths in a
fixed vertical space ranking is an important problem that has not been
addressed by existing LTR methods.
  We address this gap by introducing the variable presentation length ranking
task, where simultaneously the ordering of documents and their presentation
length is decided. Despite being a generalization of standard ranking, we show
that this setting brings significant new challenges: Firstly, the probability
ranking principle no longer applies to this setting, and secondly, the problem
cannot be divided into separate ordering and length selection tasks.
  We therefore propose VLPL - a new family of Plackett-Luce list-wise gradient
estimation methods for the joint optimization of document ordering and lengths.
Our semi-synthetic experiments show that VLPL can effectively balance the
expected exposure and attractiveness of all documents, achieving the best
performance across different ranking settings. Furthermore, we observe that
even simple length-aware methods can achieve significant performance
improvements over fixed-length models. Altogether, our theoretical and
empirical results highlight the importance and difficulties of combining
document presentation with LTR.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [270] [Density, asymmetry and citation dynamics in scientific literature](https://arxiv.org/abs/2506.23366)
*Nathaniel Imel,Zachary Hafen*

Key words: 科学行为、引用率、语义嵌入、密度、不对称性

TL;DR: 论文探讨了科学论文与先前研究的相似性与其引用率的关系，提出了两种度量相似性的指标（密度和不对称性），发现密度指标对预测引用率有轻微但显著的影响。

<details>
  <summary>Details</summary>

Main category: cs.DL

Motivation: 研究科学行为中继承与创新的张力，探索论文相似性如何影响其学术影响力。

Method: 引入密度（ρ）和不对称性（α）两个指标，使用语义嵌入空间分析53,000篇论文，采用贝叶斯层次回归模型预测引用率。

Result: 密度指标对预测引用率有轻微但显著的作用，不对称性指标未表现出预测效果。

Conclusion: 论文周围科学文献的密度可能为预测其影响力提供信息，但不对称性无显著作用。

Abstract: Scientific behavior is often characterized by a tension between building upon
established knowledge and introducing novel ideas. Here, we investigate whether
this tension is reflected in the relationship between the similarity of a
scientific paper to previous research and its eventual citation rate. To
operationalize similarity to previous research, we introduce two complementary
metrics to characterize the local geometry of a publication's semantic
neighborhood: (1) \emph{density} ($\rho$), defined as the ratio between a fixed
number of previously-published papers and the minimum distance enclosing those
papers in a semantic embedding space, and (2) asymmetry ($\alpha$), defined as
the average directional difference between a paper and its nearest neighbors.
We tested the predictive relationship between these two metrics and its
subsequent citation rate using a Bayesian hierarchical regression approach,
surveying $\sim 53,000$ publications across nine academic disciplines and five
different document embeddings. While the individual effects of $\rho$ on
citation count are small and variable, incorporating density-based predictors
consistently improves out-of-sample prediction when added to baseline models.
These results suggest that the density of a paper's surrounding scientific
literature may carry modest but informative signals about its eventual impact.
Meanwhile, we find no evidence that publication asymmetry improves model
predictions of citation rates. Our work provides a scalable framework for
linking document embeddings to scientometric outcomes and highlights new
questions regarding the role that semantic similarity plays in shaping the
dynamics of scientific reward.

</details>


### [271] [Persistence Paradox in Dynamic Science](https://arxiv.org/abs/2506.22729)
*Honglin Bao,Kai Li*

Key words: 坚持,深度学习,范式转变,职业轨迹,适应性

TL;DR: 论文挑战了科学中坚持的传统观点，指出在范式转变时期，坚持可能成为负担。通过分析2012年AlexNet引发的深度学习革命，研究了5000多名科学家的职业轨迹，发现适应新趋势的科学家受益最大。

<details>
  <summary>Details</summary>

Main category: cs.DL

Motivation: 探讨在科学范式转变中，坚持是否总是有益的，以及科学家如何适应这种变化。

Method: 分析5000多名科学家的20年职业轨迹，研究他们在深度学习革命中的研究焦点和产出变化。

Result: 成功科学家或旧团队适应较慢，遭遇'刚性惩罚'；而策略性适应的科学家获益最大。

Conclusion: 科学突破重塑领域内的权力结构，适应新趋势是关键。

Abstract: Persistence is often regarded as a virtue in science. In this paper, however,
we challenge this conventional view by highlighting its contextual nature,
particularly how persistence can become a liability during periods of paradigm
shift. We focus on the deep learning revolution catalyzed by AlexNet in 2012.
Analyzing the 20-year career trajectories of over 5,000 scientists who were
active in top machine learning venues during the preceding decade, we examine
how their research focus and output evolved. We first uncover a dynamic period
in which leading venues increasingly prioritized cutting-edge deep learning
developments that displaced relatively traditional statistical learning
methods. Scientists responded to these changes in markedly different ways.
Those who were previously successful or affiliated with old teams adapted more
slowly, experiencing what we term a rigidity penalty - a reluctance to embrace
new directions leading to a decline in scientific impact, as measured by
citation percentile rank. In contrast, scientists who pursued strategic
adaptation - selectively pivoting toward emerging trends while preserving weak
connections to prior expertise - reaped the greatest benefits. Taken together,
our macro- and micro-level findings show that scientific breakthroughs act as
mechanisms that reconfigure power structures within a field.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [272] [TriADA: Massively Parallel Trilinear Matrix-by-Tensor Multiply-Add Algorithm and Device Architecture for the Acceleration of 3D Discrete Transformations](https://arxiv.org/abs/2506.22818)
*Stanislav Sedukhin,Yoichi Tomioka,Kazuya Matsumoto,Yuichi Okuyama*

Key words: 高维数据,并行计算,能源效率,3D离散正交变换,弹性稀疏外积

TL;DR: 本文提出TriADA算法和架构，用于高效处理高维数据的计算和内存需求，特别针对稀疏数据，提升并行计算和能源效率。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 高维数据的计算和内存需求高，尤其在并行处理时能源消耗大，限制了其在HPC和AI中的广泛应用，尤其是稀疏数据场景。

Method: TriADA采用1)低秩算法处理3D离散正交变换，2)专用于3D-GEMT的外积GEMM内核，3)分布式3D网络架构，4)弹性稀疏外积方法减少无效计算。

Result: TriADA能以线性时间步完成多种超立方算术复杂度的三线性变换，显著提升并行性、扩展性和能源效率。

Conclusion: TriADA适用于加速AI和HPC中最耗时的多线性张量运算，具有高并行性、可扩展性和能源效率。

Abstract: Multilinear transformations are key in high-performance computing (HPC) and
artificial intelligence (AI) workloads, where data is represented as tensors.
However, their high computational and memory demands, which grow with
dimensionality, often slow down critical tasks. Moreover, scaling computation
by enlarging the number of parallel processing units substantially increases
energy consumption, limiting widespread adoption, especially for sparse data,
which is common in HPC and AI applications. This paper introduces the Trilinear
Algorithm and isomorphic to algorithm Device Architecture (TriADA) to address
these challenges with the following innovations: (1) a massively parallel,
low-rank algorithm for computing a family of trilinear (3D) discrete orthogonal
transformations (3D-DXTs), which is a special case of the more general 3-mode
matrix-by-tensor multiplication (3D-GEMT); (2) a new outer-product-based GEMM
kernel with decoupled streaming active memory, specially designed to accelerate
3D-GEMT operation; (3) an isomorphic to the proposed algorithm, fully
distributed 3D network of mesh interconnected processing elements or cells with
a coordinate-free, data-driven local processing activity, which is independent
of problem size; (4) an elastic sparse outer-product (ESOP) method that avoids
unnecessary computing and communication operations with zero-valued operands,
thereby enhancing energy efficiency, computational accuracy, and stability.
TriADA is capable of performing a variety of trilinear transformations with
hypercubic arithmetic complexity in a linear number of time-steps. The
massively parallel, scalable, and energy-efficient architecture of TriADA is
ideal for accelerating multilinear tensor operations, which are the most
demanding parts of AI and HPC workloads.

</details>


### [273] [Performance Measurements in the AI-Centric Computing Continuum Systems](https://arxiv.org/abs/2506.22884)
*Praveen Kumar Donta,Qiyang Zhang,Schahram Dustdar*

Key words: 分布式计算连续体,性能指标,可持续性,能源效率

TL;DR: 本文回顾了分布式计算连续体（DCC）中的性能指标，并讨论了适应新需求的指标，如可持续性和能源效率。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 随着计算范式转向分布式架构，传统性能指标需要更新以应对新的计算需求和挑战。

Method: 回顾了DCC和IoT环境中常用的性能指标，并提出了新兴性能维度的讨论。

Result: 提出了适应未来计算需求的性能指标选择标准和考虑因素。

Conclusion: 需要新的性能指标以支持高效、可持续的计算系统发展。

Abstract: Over the Eight decades, computing paradigms have shifted from large,
centralized systems to compact, distributed architectures, leading to the rise
of the Distributed Computing Continuum (DCC). In this model, multiple layers
such as cloud, edge, Internet of Things (IoT), and mobile platforms work
together to support a wide range of applications. Recently, the emergence of
Generative AI and large language models has further intensified the demand for
computational resources across this continuum. Although traditional performance
metrics have provided a solid foundation, they need to be revisited and
expanded to keep pace with changing computational demands and application
requirements. Accurate performance measurements benefit both system designers
and users by supporting improvements in efficiency and promoting alignment with
system goals. In this context, we review commonly used metrics in DCC and IoT
environments. We also discuss emerging performance dimensions that address
evolving computing needs, such as sustainability, energy efficiency, and system
observability. We also outline criteria and considerations for selecting
appropriate metrics, aiming to inspire future research and development in this
critical area.

</details>


### [274] [Libra: Synergizing CUDA and Tensor Cores for High-Performance Sparse Matrix Multiplication](https://arxiv.org/abs/2506.22714)
*Jinliang Shi,Shigang Li,Youxuan Xu,Xueying Wang,Rongtian Fu,Zhi Ma,Tong Wu*

Key words: 稀疏矩阵乘法, 异构计算, GPU, Tensor核心, CUDA核心

TL;DR: 论文提出Libra，一种利用CUDA和Tensor核心协同计算的高效稀疏矩阵乘法系统。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 现代加速器的Tensor核心和CUDA核心各有优势，单独使用会导致性能不足。

Method: 提出2D感知任务分配策略和异构计算优化，包括混合负载均衡和GPU加速预处理。

Result: Libra在H100和RTX 4090 GPU上性能提升显著，最高达9.23倍。

Conclusion: Libra为稀疏算子加速提供了新视角，充分利用GPU异构资源。

Abstract: Sparse matrix multiplication operators (i.e., SpMM and SDDMM) are widely used
in deep learning and scientific computing. Modern accelerators are commonly
equipped with Tensor cores and CUDA cores to accelerate sparse operators. The
former brings superior computing power but only for structured matrix
multiplication, while the latter has relatively lower performance but with
higher programming flexibility. In this work, we discover that utilizing one
resource alone leads to inferior performance for sparse matrix multiplication,
due to their respective limitations. To this end, we propose Libra, a
systematic approach that enables synergistic computation between CUDA and
Tensor cores to achieve the best performance for sparse matrix multiplication.
Specifically, we propose a 2D-aware workload distribution strategy to find out
the sweet point of task mapping for different sparse operators, leveraging both
the high performance of Tensor cores and the low computational redundancy on
CUDA cores. In addition, Libra incorporates systematic optimizations for
heterogeneous computing, including hybrid load-balancing, finely optimized
kernel implementations, and GPU-accelerated preprocessing. Extensive
experimental results on H100 and RTX 4090 GPUs show that Libra outperforms the
state-of-the-art by on average 3.1x (up to 9.23x) over DTC-SpMM and 2.9x (up to
3.9x) for end-to-end GNN applications. Libra opens up a new perspective for
sparse operator acceleration by fully exploiting the heterogeneous computing
resources on GPUs.

</details>


### [275] [Not All Water Consumption Is Equal: A Water Stress Weighted Metric for Sustainable Computing](https://arxiv.org/abs/2506.22773)
*Yanran Wu,Inez Hua,Yi Ding*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Water consumption is an increasingly critical dimension of computing
sustainability, especially as AI workloads rapidly scale. However, current
water impact assessment often overlooks where and when water stress is more
severe. To fill in this gap, we present SCARF, the first general framework that
evaluates water impact of computing by factoring in both spatial and temporal
variations in water stress. SCARF calculates an Adjusted Water Impact (AWI)
metric that considers both consumption volume and local water stress over time.
Through three case studies on LLM serving, datacenters, and semiconductor
fabrication plants, we show the hidden opportunities for reducing water impact
by optimizing location and time choices, paving the way for water-sustainable
computing. The code is available at https://github.com/jojacola/SCARF.

</details>


### [276] [Towards Building Private LLMs: Exploring Multi-Node Expert Parallelism on Apple Silicon for Mixture-of-Experts Large Language Model](https://arxiv.org/abs/2506.23635)
*Mu-Chi Chen,Po-Hsuan Huang,Xiangrui Ke,Chia-Heng Tu,Chun Jason Xue,Shih-Hao Hung*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI)
with significant advancements such as OpenAI's ChatGPT, Meta's Llama, and
Databricks' DBRX. This paper addresses the cost and scalability challenges
encountered when constructing private LLM systems for personal or small group
services, as aimed by Apple Intelligence. A Mac Studio cluster with Apple's M2
Ultra chips is established as a cost-efficient solution to host and accelerate
the pretrained DBRX model with the Mixture-of-Experts (MoE) architecture. Our
performance analysis reveal that parallel execution of the model's experts
across two to four machine nodes significantly reduces inference time. We find
that computation time for the experts is comparable to the communication time
for exchanging their outputs, emphasizing the importance of network latency
over bandwidth. We also observe significant management overhead due to Apple
software stack's memory management logic. Based on these findings, we develop
optimization schemes to eliminate the memory management overhead. As a result,
the Mac Studio cluster is 1.15 times more cost-efficient than the
state-of-the-art AI supercomputer with NVIDIA H100 GPUs. In addition, we
construct a performance model to estimate system performance under varying
configurations, and the model provides valuable insights for designing private
LLM systems.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [277] [Neural models of multiscale systems: conceptual limitations, stochastic parametrizations, and a climate application](https://arxiv.org/abs/2506.22552)
*Fabrizio Falasca*

Key words: 数据驱动建模,多尺度动力系统,神经模拟器,随机气候模型,粗粒化

TL;DR: 本文探讨了数据驱动建模在多尺度动力系统中的关键概念限制，重点关注神经模拟器和随机气候建模。

<details>
  <summary>Details</summary>

Main category: nlin.CD

Motivation: 研究旨在揭示当前自回归神经模型在捕捉静态统计和外部扰动响应方面的不足，并提出解决这些问题的物理基础策略。

Method: 通过分析低维动力系统，构建神经随机模型，并在全观测和部分观测两种场景下进行验证。

Result: 全观测情况下模型表现良好，而部分观测时存在变量选择和未观测自由度影响的问题。提出了基于物理的粗粒化和随机参数化策略。

Conclusion: 物理基础策略对于复杂系统（如耦合气候系统）的精准模拟至关重要。

Abstract: This work explores key conceptual limitations in data-driven modeling of
multiscale dynamical systems, focusing on neural emulators and stochastic
climate modeling. A skillful climate model should capture both stationary
statistics and responses to external perturbations. While current
autoregressive neural models often reproduce the former, they typically
struggle with the latter. We begin by analyzing a low-dimensional dynamical
system to expose, by analogy, fundamental limitations that persist in
high-dimensional settings. Specifically, we construct neural stochastic models
under two scenarios: one where the full state vector is observed, and another
with only partial observations (i.e. a subset of variables). In the first case,
the models accurately capture both equilibrium statistics and forced responses
in ensemble mean and variance. In the more realistic case of partial
observations, two key challenges emerge: (i) identifying the \textit{proper}
variables to model, and (ii) parameterizing the influence of unobserved degrees
of freedom. These issues are not specific to neural networks but reflect
fundamental limitations of data-driven modeling and the need to target the slow
dynamics of the system. We argue that physically grounded strategies -- such as
coarse-graining and stochastic parameterizations -- are critical, both
conceptually and practically, for the skillful emulation of complex systems
like the coupled climate system. Building on these insights, we turn to a more
realistic application: a stochastic reduced neural model of the sea surface
temperature field and the net radiative flux at the top of the atmosphere,
assessing its stationary statistics, response to temperature forcing, and
interpretability.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [278] [Treatment, evidence, imitation, and chat](https://arxiv.org/abs/2506.23040)
*Samuel J. Weisenthal*

Key words: 大语言模型, 医疗决策, 治疗问题, 聊天问题, 证据医学

TL;DR: 论文探讨了大语言模型在医疗决策中的潜力，重点关注治疗问题和聊天问题的区别，以及如何使用大语言模型解决治疗问题及其挑战。

<details>
  <summary>Details</summary>

Main category: stat.OT

Motivation: 研究大语言模型如何辅助医疗决策，尤其是治疗问题的解决。

Method: 分析了治疗问题的解决方案，包括基于证据医学的试验和观察数据，并讨论了聊天问题与治疗问题的区别，以及大语言模型的应用。

Result: 揭示了使用大语言模型解决治疗问题时面临的挑战，并探讨了这些挑战与证据医学的关系。

Conclusion: 大语言模型在医疗决策中具有潜力，但仍需克服挑战，未来研究可基于证据医学的框架进一步探索。

Abstract: Large language models are thought to have potential to aid in medical
decision making. We investigate this here. We start with the treatment problem,
the patient's core medical decision-making task, which is solved in
collaboration with a healthcare provider. We discuss approaches to solving the
treatment problem, including -- within evidence-based medicine -- trials and
observational data. We then discuss the chat problem, and how this differs from
the treatment problem -- in particular as it relates to imitation. We then
discuss how a large language model might be used to solve the treatment problem
and highlight some of the challenges that emerge. We finally discuss how these
challenges relate to evidence-based medicine, and how this might inform next
steps.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [279] [Arnoldi Singular Vector perturbations for machine learning weather prediction](https://arxiv.org/abs/2506.22450)
*Jens Winkler,Michael Denhard*

Key words: 机器学习天气预测，初始条件误差，Arnoldi-SV，扰动模式，Krylov子空间

TL;DR: 该论文研究了机器学习天气预测（MLWP）对初始条件误差的敏感性，提出了一种名为Arnoldi-SV（A-SV）的新方法，用于生成动态有意义的扰动模式，适用于数值天气预测（NWP）和MLWP。

<details>
  <summary>Details</summary>

Main category: physics.ao-ph

Motivation: 天气预测本质上具有不确定性，可靠的决策需要未来天气情景的概率信息。研究旨在探索MLWP对初始条件误差的敏感性，并提出一种无需线性或伴随模型的新型扰动方法。

Method: 采用Arnoldi-SV方法，通过迭代应用预测模型到扰动的模型状态，观察误差增长，生成Krylov子空间近似局部误差增长。该方法从随机噪声扰动开始，逐步生成与参考状态相关的扰动模式。

Result: 实验表明，A-SV方法能够为华为的24小时Pangu天气模型生成动态有意义的扰动模式，这些模式从预测开始即增长，可用于初始化MLWP集合。

Conclusion: A-SV方法为MLWP提供了一种有效的扰动生成工具，其生成的扰动模式可描述局部不稳定模式，并与扩散模型中的去噪过程有相似之处。

Abstract: Since weather forecasts are fundamentally uncertain, reliable decision making
requires information on the likelihoods of future weather scenarios. We explore
the sensitivity of machine learning weather prediction (MLWP) using the 24h
Pangu Weather ML model of Huawei to errors in the initial conditions with a
specific kind of Singular Vector (SV) perturbations. Our Arnoldi-SV (A-SV)
method does not need linear nor adjoint model versions and is applicable to
numerical weather prediction (NWP) as well as MLWP. It observes error growth
within a given optimization time window by iteratively applying a forecast
model to perturbed model states. This creates a Krylov subspace, implicitly
based on a matrix operator, which approximates the local error growth. Each
iteration adds new dimensions to the Krylov space and its leading right SVs are
expected to turn into directions of growing errors. We show that A-SV indeed
finds dynamically meaningful perturbation patterns for the 24h Pangu Weather
model, which grow right from the beginning of the forecast rollout. These
perturbations describe local unstable modes and could be a basis to initialize
MLWP ensembles. Since we start A-SV from random noise perturbations, the
algorithm transforms noise into perturbations conditioned on a given reference
state - a process that is akin to the denoising process of the generic
diffusion based ML model of GenCast, therefor we briefly discuss similarities
and differences.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [280] [Diversity by Design: Addressing Mode Collapse Improves scRNA-seq Perturbation Modeling on Well-Calibrated Metrics](https://arxiv.org/abs/2506.22641)
*Gabriel M. Mejia,Henry E. Miller,Francis J. A. Leblanc,Bo Wang,Brendan Swain,Lucas Paulo de Lima Camillo*

Key words: 单细胞扰动响应, 模式坍塌, DEG感知指标, 加权均方误差, 评估基准

TL;DR: 论文发现单细胞扰动响应模型的评估存在指标问题，提出新的DEG感知指标以改进评估和模型性能。

<details>
  <summary>Details</summary>

Main category: q-bio.GN

Motivation: 现有评估指标因控制组偏差或生物信号稀疏性导致模型表现虚假优异，需改进。

Method: 提出加权均方误差（WMSE）和加权delta R²（R²w(Δ)）指标，并引入正负性能基线。

Result: 新指标使均值基线降至无效性能，同时正确奖励真实预测器；WMSE作为损失函数减少模式坍塌并提升性能。

Conclusion: 改进指标能更准确评估模型性能，提升单细胞扰动响应的预测可靠性。

Abstract: Recent benchmarks reveal that models for single-cell perturbation response
are often outperformed by simply predicting the dataset mean. We trace this
anomaly to a metric artifact: control-referenced deltas and unweighted error
metrics reward mode collapse whenever the control is biased or the biological
signal is sparse. Large-scale \textit{in silico} simulations and analysis of
two real-world perturbation datasets confirm that shared reference shifts, not
genuine biological change, drives high performance in these evaluations. We
introduce differentially expressed gene (DEG)-aware metrics, weighted
mean-squared error (WMSE) and weighted delta $R^{2}$ ($R^{2}_{w}(\Delta)$) with
respect to all perturbations, that measure error in niche signals with high
sensitivity. We further introduce negative and positive performance baselines
to calibrate these metrics. With these improvements, the mean baseline sinks to
null performance while genuine predictors are correctly rewarded. Finally, we
show that using WMSE as a loss function reduces mode collapse and improves
model performance.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [281] [FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation](https://arxiv.org/abs/2506.22580)
*Vasilis Siomos,Jonathan Passerat-Palmbach,Giacomo Tarroni*

Key words: 联邦学习,医学影像,自适应动量,强度对齐,分割任务

TL;DR: FedCLAM是一种联邦学习方法，通过客户端自适应动量项和个性化阻尼因子解决医学影像中的特征差异问题，并通过强度对齐损失处理异构图像强度分布，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 医学影像中不同设备和人口差异导致特征不一致，现有联邦学习方法难以适应多样情境，需要一种新方法提升模型性能。

Method: 提出FedCLAM，结合客户端自适应动量项、个性化阻尼因子和强度对齐损失，优化局部训练和全局模型效果。

Result: 在两个数据集上的实验表明，FedCLAM在医学分割任务中优于八种前沿方法。

Conclusion: FedCLAM有效解决了医学影像联邦学习中的特征差异问题，显著提升了模型性能。

Abstract: Federated learning is a decentralized training approach that keeps data under
stakeholder control while achieving superior performance over isolated
training. While inter-institutional feature discrepancies pose a challenge in
all federated settings, medical imaging is particularly affected due to diverse
imaging devices and population variances, which can diminish the global model's
effectiveness. Existing aggregation methods generally fail to adapt across
varied circumstances. To address this, we propose FedCLAM, which integrates
\textit{client-adaptive momentum} terms derived from each client's loss
reduction during local training, as well as a \textit{personalized dampening
factor} to curb overfitting. We further introduce a novel \textit{intensity
alignment} loss that matches predicted and ground-truth foreground
distributions to handle heterogeneous image intensity profiles across
institutions and devices. Extensive evaluations on two datasets show that
FedCLAM surpasses eight cutting-edge methods in medical segmentation tasks,
underscoring its efficacy. The code is available at
https://github.com/siomvas/FedCLAM.

</details>


### [282] [High Resolution Isotropic 3D Cine imaging with Automated Segmentation using Concatenated 2D Real-time Imaging and Deep Learning](https://arxiv.org/abs/2506.22532)
*Mark Wrobel,Michele Pascale,Tina Yao,Ruaraidh Campbell,Elena Milano,Michael Quail,Jennifer Steeden,Vivek Muthurangu*

Key words: 心血管磁共振, 深度学习, 3D电影, 图像分割, 快速成像

TL;DR: 利用深度学习模型将2D实时电影图像拼接并转化为3D电影数据集，显著缩短心血管磁共振检查时间。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 传统心血管磁共振（CMR）在儿科和先天性心脏病中需多次呼吸控制，耗时且不适用于所有患者。本研究旨在开发一种无需呼吸控制的快速3D成像方法。

Method: 使用四种深度学习模型处理开源数据：对比校正、运动校正、超分辨率和分割。在10名患者中验证该方法，并与传统成像对比。

Result: 所有数据成功转化为3D电影，处理时间小于1分钟，心室体积和血管直径与传统方法一致。

Conclusion: 该方法能快速生成3D电影数据集，有望加速临床CMR检查。

Abstract: Background: Conventional cardiovascular magnetic resonance (CMR) in
paediatric and congenital heart disease uses 2D, breath-hold, balanced steady
state free precession (bSSFP) cine imaging for assessment of function and
cardiac-gated, respiratory-navigated, static 3D bSSFP whole-heart imaging for
anatomical assessment. Our aim is to concatenate a stack 2D free-breathing
real-time cines and use Deep Learning (DL) to create an isotropic a fully
segmented 3D cine dataset from these images. Methods: Four DL models were
trained on open-source data that performed: a) Interslice contrast correction;
b) Interslice respiratory motion correction; c) Super-resolution (slice
direction); and d) Segmentation of right and left atria and ventricles (RA, LA,
RV, and LV), thoracic aorta (Ao) and pulmonary arteries (PA). In 10 patients
undergoing routine cardiovascular examination, our method was validated on
prospectively acquired sagittal stacks of real-time cine images. Quantitative
metrics (ventricular volumes and vessel diameters) and image quality of the 3D
cines were compared to conventional breath hold cine and whole heart imaging.
Results: All real-time data were successfully transformed into 3D cines with a
total post-processing time of <1 min in all cases. There were no significant
biases in any LV or RV metrics with reasonable limits of agreement and
correlation. There is also reasonable agreement for all vessel diameters,
although there was a small but significant overestimation of RPA diameter.
Conclusion: We have demonstrated the potential of creating a 3D-cine data from
concatenated 2D real-time cine images using a series of DL models. Our method
has short acquisition and reconstruction times with fully segmented data being
available within 2 minutes. The good agreement with conventional imaging
suggests that our method could help to significantly speed up CMR in clinical
practice.

</details>


### [283] [CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation](https://arxiv.org/abs/2506.23121)
*Xinlei Yu,Chanmiao Wang,Hui Jin,Ahmed Elazab,Gangyong Jia,Xiang Wan,Changqing Zou,Ruiquan Ge*

Key words: multi-organ segmentation, medical imaging, cross-modal interaction, semantic prompting, SAM2

TL;DR: CRISP-SAM2 is a novel multi-organ medical segmentation model that addresses limitations like inaccurate details and reliance on geometric prompts by incorporating cross-modal interaction and semantic prompting, demonstrating superior performance on seven public datasets.

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: Current multi-organ segmentation models face challenges such as inaccurate details, dependence on geometric prompts, and loss of spatial information, prompting the development of CRISP-SAM2 to overcome these issues.

Method: The model converts visual and textual inputs into cross-modal semantics using progressive cross-attention, employs semantic prompting to avoid geometric prompts, and uses a similarity-sorting self-updating strategy for memory and mask-refining.

Result: CRISP-SAM2 outperforms existing models on seven public datasets, showing enhanced detail understanding and adaptability to medical imaging.

Conclusion: CRISP-SAM2 effectively addresses the limitations of current models, proving its superior performance in multi-organ medical segmentation.

Abstract: Multi-organ medical segmentation is a crucial component of medical image
processing, essential for doctors to make accurate diagnoses and develop
effective treatment plans. Despite significant progress in this field, current
multi-organ segmentation models often suffer from inaccurate details,
dependence on geometric prompts and loss of spatial information. Addressing
these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal
Interaction and Semantic Prompting based on SAM2. This model represents a
promising approach to multi-organ medical segmentation guided by textual
descriptions of organs. Our method begins by converting visual and textual
inputs into cross-modal contextualized semantics using a progressive
cross-attention interaction mechanism. These semantics are then injected into
the image encoder to enhance the detailed understanding of visual information.
To eliminate reliance on geometric prompts, we use a semantic prompting
strategy, replacing the original prompt encoder to sharpen the perception of
challenging targets. In addition, a similarity-sorting self-updating strategy
for memory and a mask-refining process is applied to further adapt to medical
imaging and enhance localized details. Comparative experiments conducted on
seven public datasets indicate that CRISP-SAM2 outperforms existing models.
Extensive analysis also demonstrates the effectiveness of our method, thereby
confirming its superior performance, especially in addressing the limitations
mentioned earlier. Our code is available at:
https://github.com/YU-deep/CRISP\_SAM2.git.

</details>


### [284] [CA-Diff: Collaborative Anatomy Diffusion for Brain Tissue Segmentation](https://arxiv.org/abs/2506.22882)
*Qilong Xing,Zikai Song,Yuteng Ye,Yuke Chen,Youjia Zhang,Na Feng,Junqing Yu,Wei Yang*

Key words: 脑MRI分割,扩散模型,解剖特征,距离场,CA-Diff

TL;DR: 论文提出了一种名为CA-Diff的新框架，通过结合空间解剖特征和扩散模型，显著提高了脑MRI结构分割的准确性。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 现有的CNN和基于Transformer的方法在准确分割复杂脑结构上存在困难，而现有的扩散模型在脑MRI分割中未充分利用解剖信息。

Method: CA-Diff框架引入了距离场作为辅助解剖条件，并通过协作扩散过程建模其与解剖结构的联合分布，同时设计了一致性损失和时间适应通道注意力模块以优化性能。

Result: 实验表明，CA-Diff在分割准确性上优于现有最佳方法。

Conclusion: CA-Diff通过结合解剖特征和扩散模型，显著提升了脑MRI分割的效果，为未来研究提供了新思路。

Abstract: Segmentation of brain structures from MRI is crucial for evaluating brain
morphology, yet existing CNN and transformer-based methods struggle to
delineate complex structures accurately. While current diffusion models have
shown promise in image segmentation, they are inadequate when applied directly
to brain MRI due to neglecting anatomical information. To address this, we
propose Collaborative Anatomy Diffusion (CA-Diff), a framework integrating
spatial anatomical features to enhance segmentation accuracy of the diffusion
model. Specifically, we introduce distance field as an auxiliary anatomical
condition to provide global spatial context, alongside a collaborative
diffusion process to model its joint distribution with anatomical structures,
enabling effective utilization of anatomical features for segmentation.
Furthermore, we introduce a consistency loss to refine relationships between
the distance field and anatomical structures and design a time adapted channel
attention module to enhance the U-Net feature fusion procedure. Extensive
experiments show that CA-Diff outperforms state-of-the-art (SOTA) methods.

</details>


### [285] [Score-based Diffusion Model for Unpaired Virtual Histology Staining](https://arxiv.org/abs/2506.23184)
*Anran Liu,Xiaofei Wang,Jing Cai,Chao Li*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Hematoxylin and eosin (H&E) staining visualizes histology but lacks
specificity for diagnostic markers. Immunohistochemistry (IHC) staining
provides protein-targeted staining but is restricted by tissue availability and
antibody specificity. Virtual staining, i.e., computationally translating the
H&E image to its IHC counterpart while preserving the tissue structure, is
promising for efficient IHC generation. Existing virtual staining methods still
face key challenges: 1) effective decomposition of staining style and tissue
structure, 2) controllable staining process adaptable to diverse tissue and
proteins, and 3) rigorous structural consistency modelling to handle the
non-pixel-aligned nature of paired H&E and IHC images. This study proposes a
mutual-information (MI)-guided score-based diffusion model for unpaired virtual
staining. Specifically, we design 1) a global MI-guided energy function that
disentangles the tissue structure and staining characteristics across
modalities, 2) a novel timestep-customized reverse diffusion process for
precise control of the staining intensity and structural reconstruction, and 3)
a local MI-driven contrastive learning strategy to ensure the cellular level
structural consistency between H&E-IHC images. Extensive experiments
demonstrate the our superiority over state-of-the-art approaches, highlighting
its biomedical potential. Codes will be open-sourced upon acceptance.

</details>


### [286] [Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation](https://arxiv.org/abs/2506.23334)
*Hongyi Pan,Ziliang Hong,Gorkem Durak,Ziyue Xu,Ulas Bagci*

Key words: 联邦学习, 生成AI, 数据增强, 乳腺癌诊断, 超声图像

TL;DR: 本文提出了一种基于生成AI的数据增强框架，用于解决联邦学习在乳腺癌超声图像诊断中数据稀缺和数据分布不均的问题，通过合成图像共享提升模型性能。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 联邦学习在多机构协作训练深度模型时无需交换敏感数据，但数据稀缺和非独立同分布数据会降低模型性能。本文旨在通过生成AI数据增强解决这些问题。

Method: 提出一种生成AI数据增强框架，训练两个简单的类特定深度卷积生成对抗网络（DCGAN），分别用于良性和恶性病灶，并将其集成到联邦学习训练过程中。

Result: 实验表明，适当数量的合成图像将FedAvg的平均AUC从0.9206提升到0.9237，FedProx从0.9429提升到0.9538，但过量合成数据会降低性能。

Conclusion: 生成AI数据增强在乳腺癌超声图像分类任务中具有潜力，但需平衡真实和合成样本的比例。

Abstract: Federated learning (FL) has emerged as a promising paradigm for
collaboratively training deep learning models across institutions without
exchanging sensitive medical data. However, its effectiveness is often hindered
by limited data availability and non-independent, identically distributed data
across participating clients, which can degrade model performance and
generalization. To address these challenges, we propose a generative AI based
data augmentation framework that integrates synthetic image sharing into the
federated training process for breast cancer diagnosis via ultrasound images.
Specifically, we train two simple class-specific Deep Convolutional Generative
Adversarial Networks: one for benign and one for malignant lesions. We then
simulate a realistic FL setting using three publicly available breast
ultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are
adopted as baseline FL algorithms. Experimental results show that incorporating
a suitable number of synthetic images improved the average AUC from 0.9206 to
0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that
excessive use of synthetic data reduced performance, underscoring the
importance of maintaining a balanced ratio of real and synthetic samples. Our
findings highlight the potential of generative AI based data augmentation to
enhance FL results in the breast ultrasound image classification task.

</details>


### [287] [Physics informed guided diffusion for accelerated multi-parametric MRI reconstruction](https://arxiv.org/abs/2506.23311)
*Perla Mayo,Carolin M. Pirkl,Alin Achim,Bjoern Menze,Mohammad Golbabaee*

Key words: MRF-DiPh, 物理信息去噪扩散, 磁共振指纹识别, 多参数组织映射, 逆问题

TL;DR: MRF-DiPh 是一种基于物理信息的新型去噪扩散方法，用于从高度加速的瞬态定量 MRI 采集（如磁共振指纹识别）中进行多参数组织映射。方法结合预训练去噪扩散模型和物理约束，显著优于深度学习和压缩感知基线。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 解决从加速 MRI 采集中进行多参数组织映射时的逆问题，同时确保测量一致性和物理模型约束。

Method: 基于近端分裂公式，结合预训练去噪扩散模型作为图像先验，同时强制执行 k 空间测量一致性和 Bloch 响应模型。

Result: 在体内脑扫描数据中，MRF-DiPh 表现优于深度学习和压缩感知基线，提供更准确的参数图，同时保持测量保真度和物理模型一致性。

Conclusion: MRF-DiPh 是一种可靠的方法，能够有效地解决医学成像中的逆问题。

Abstract: We introduce MRF-DiPh, a novel physics informed denoising diffusion approach
for multiparametric tissue mapping from highly accelerated, transient-state
quantitative MRI acquisitions like Magnetic Resonance Fingerprinting (MRF). Our
method is derived from a proximal splitting formulation, incorporating a
pretrained denoising diffusion model as an effective image prior to regularize
the MRF inverse problem. Further, during reconstruction it simultaneously
enforces two key physical constraints: (1) k-space measurement consistency and
(2) adherence to the Bloch response model. Numerical experiments on in-vivo
brain scans data show that MRF-DiPh outperforms deep learning and compressed
sensing MRF baselines, providing more accurate parameter maps while better
preserving measurement fidelity and physical model consistency-critical for
solving reliably inverse problems in medical imaging.

</details>


### [288] [UltraTwin: Towards Cardiac Anatomical Twin Generation from Multi-view 2D Ultrasound](https://arxiv.org/abs/2506.23490)
*Junxuan Yu,Yaofei Duan,Yuhao Huang,Yu Wang,Rongbo Ling,Weihao Luo,Ang Zhang,Jingxian Xu,Qiongying Ni,Yongsong Zhou,Binghan Li,Haoran Dou,Liping Liu,Yanfen Chu,Feng Geng,Zhe Sheng,Zhifeng Ding,Dingxin Zhang,Rui Huang,Yuhang Zhang,Xiaowei Xu,Tao Tan,Dong Ni,Zhongshan Gou,Xin Yang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Echocardiography is routine for cardiac examination. However, 2D ultrasound
(US) struggles with accurate metric calculation and direct observation of 3D
cardiac structures. Moreover, 3D US is limited by low resolution, small field
of view and scarce availability in practice. Constructing the cardiac
anatomical twin from 2D images is promising to provide precise treatment
planning and clinical quantification. However, it remains challenging due to
the rare paired data, complex structures, and US noises. In this study, we
introduce a novel generative framework UltraTwin, to obtain cardiac anatomical
twin from sparse multi-view 2D US. Our contribution is three-fold. First,
pioneered the construction of a real-world and high-quality dataset containing
strictly paired multi-view 2D US and CT, and pseudo-paired data. Second, we
propose a coarse-to-fine scheme to achieve hierarchical reconstruction
optimization. Last, we introduce an implicit autoencoder for topology-aware
constraints. Extensive experiments show that UltraTwin reconstructs
high-quality anatomical twins versus strong competitors. We believe it advances
anatomical twin modeling for potential applications in personalized cardiac
care.

</details>


### [289] [Artificial Intelligence-assisted Pixel-level Lung (APL) Scoring for Fast and Accurate Quantification in Ultra-short Echo-time MRI](https://arxiv.org/abs/2506.23506)
*Bowen Xin,Rohan Hickey,Tamara Blake,Jin Jin,Claire E Wainwright,Thomas Benkert,Alto Stemmer,Peter Sly,David Coman,Jason Dowling*

Key words: 肺MRI, AI, 囊性纤维化, 定量评分, 像素级分析

TL;DR: 研究提出了一种基于AI的像素级评分系统（APL），用于快速准确地量化肺MRI中的结构损伤，特别是在囊性纤维化（CF）患者中。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 肺MRI在儿科疾病（如CF）中优于CT，但目前缺乏定量评分系统。AI辅助的评分系统可以填补这一空白，提升效率和准确性。

Method: APL评分包括五个步骤：图像加载、AI肺部分割、切片采样、像素级标注及量化报告。

Result: APL评分耗时8.2分钟/受试者，速度是传统网格评分的两倍以上，且准确性更高（p=0.021），与网格评分强相关（R=0.973）。

Conclusion: APL评分有望优化肺MRI的临床工作流程，并适用于其他肺疾病和MRI序列。

Abstract: Lung magnetic resonance imaging (MRI) with ultrashort echo-time (UTE)
represents a recent breakthrough in lung structure imaging, providing image
resolution and quality comparable to computed tomography (CT). Due to the
absence of ionising radiation, MRI is often preferred over CT in paediatric
diseases such as cystic fibrosis (CF), one of the most common genetic disorders
in Caucasians. To assess structural lung damage in CF imaging, CT scoring
systems provide valuable quantitative insights for disease diagnosis and
progression. However, few quantitative scoring systems are available in
structural lung MRI (e.g., UTE-MRI). To provide fast and accurate
quantification in lung MRI, we investigated the feasibility of novel Artificial
intelligence-assisted Pixel-level Lung (APL) scoring for CF. APL scoring
consists of 5 stages, including 1) image loading, 2) AI lung segmentation, 3)
lung-bounded slice sampling, 4) pixel-level annotation, and 5) quantification
and reporting. The results shows that our APL scoring took 8.2 minutes per
subject, which was more than twice as fast as the previous grid-level scoring.
Additionally, our pixel-level scoring was statistically more accurate
(p=0.021), while strongly correlating with grid-level scoring (R=0.973,
p=5.85e-9). This tool has great potential to streamline the workflow of UTE
lung MRI in clinical settings, and be extended to other structural lung MRI
sequences (e.g., BLADE MRI), and for other lung diseases (e.g.,
bronchopulmonary dysplasia).

</details>


### [290] [A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation](https://arxiv.org/abs/2506.23584)
*Renjie Liang,Zhengkang Fan,Jinqian Pan,Chenkun Sun,Russell Terry,Jie Xu*

Key words: 肾脏CT扫描、放射学报告生成、多任务学习、视觉语言模型、临床AI

TL;DR: 提出了一种两阶段框架，从2D CT切片生成肾脏放射学报告，结合结构化异常特征和视觉语言模型，生成与临床结果相符的报告，优于随机基线。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 解决医学影像报告生成的复杂性，尤其是在肾脏CT扫描中，临床文档的变异性较高。

Method: 使用多任务学习模型提取结构化异常特征（如位置、大小等），再结合CT图像输入微调的视觉语言模型生成报告。

Result: 模型在所有异常类型中均优于随机基线，生成的报告能合理捕捉关键临床内容。

Conclusion: 证明了模块化、基于特征的肾脏影像报告生成的可行性，未来将扩展至3D CT并提升临床保真度。

Abstract: Generating radiology reports from CT scans remains a complex task due to the
nuanced nature of medical imaging and the variability in clinical
documentation. In this study, we propose a two-stage framework for generating
renal radiology reports from 2D CT slices. First, we extract structured
abnormality features using a multi-task learning model trained to identify
lesion attributes such as location, size, enhancement, and attenuation. These
extracted features are subsequently combined with the corresponding CT image
and fed into a fine-tuned vision-language model to generate natural language
report sentences aligned with clinical findings. We conduct experiments on a
curated dataset of renal CT studies with manually annotated
sentence-slice-feature triplets and evaluate performance using both
classification metrics and natural language generation metrics. Our results
demonstrate that the proposed model outperforms random baselines across all
abnormality types, and the generated reports capture key clinical content with
reasonable textual accuracy. This exploratory work highlights the feasibility
of modular, feature-informed report generation for renal imaging. Future
efforts will focus on extending this pipeline to 3D CT volumes and further
improving clinical fidelity in multimodal medical AI systems.

</details>


### [291] [Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound](https://arxiv.org/abs/2506.23721)
*Gijs Luijten,Roberto Maria Scardigno,Lisle Faray de Paiva,Peter Hoyer,Jens Kleesiek,Domenico Buongiorno,Vitoantonio Bevilacqua,Jan Egger*

Key words: 深度学习，增强现实，超声，肾脏体积测量，实时语义分割

TL;DR: 论文提出了一种结合深度学习和增强现实的超声系统，用于实时自动化肾脏体积测量，旨在减轻医生负担并提高超声检查效率。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 超声检查学习曲线陡峭且操作复杂，医生需频繁切换注意力，而肾脏体积的传统测量方式耗时且易疲劳。

Method: 整合DL进行实时语义分割与AR技术，通过HoloLens-2实现两种超声辅助流程，支持无线和通用设备接入。

Result: 利用开源数据集和模型验证了实时性和准确性，并提供了开源工具链以支持广泛临床应用。

Conclusion: AR-DL辅助系统显著提升了超声检查的效率和准确性，适用于即时护理场景。

Abstract: Ultrasound (US) is widely accessible and radiation-free but has a steep
learning curve due to its dynamic nature and non-standard imaging planes.
Additionally, the constant need to shift focus between the US screen and the
patient poses a challenge. To address these issues, we integrate deep learning
(DL)-based semantic segmentation for real-time (RT) automated kidney volumetric
measurements, which are essential for clinical assessment but are traditionally
time-consuming and prone to fatigue. This automation allows clinicians to
concentrate on image interpretation rather than manual measurements.
Complementing DL, augmented reality (AR) enhances the usability of US by
projecting the display directly into the clinician's field of view, improving
ergonomics and reducing the cognitive load associated with screen-to-patient
transitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one
streams directly via the application programming interface for a wireless
setup, while the other supports any US device with video output for broader
accessibility. We evaluate RT feasibility and accuracy using the Open Kidney
Dataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with
MedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model
implementations, measurement algorithms, and a Wi-Fi-based streaming solution,
enhancing US training and diagnostics, especially in point-of-care settings.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [292] [Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling](https://arxiv.org/abs/2504.15071)
*Louis Bradshaw,Simon Colton*

Key words: MIDI数据集,钢琴演奏,音频转录,音乐信息检索

TL;DR: 介绍了一个通过音频转录构建的大规模钢琴演奏MIDI数据集，包含100万份MIDI文件，约10万小时音频转录。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 为研究者和开发者提供高质量的钢琴演奏数据集，支持音乐信息检索和生成任务。

Method: 采用多阶段数据管道，包括语言模型自动爬取和评分音频、音频分类器修剪和分段。

Result: 数据集包含100万份MIDI文件，提供统计分析和元数据标签。

Conclusion: 该数据集为音乐研究提供了丰富资源，技术方法具有可扩展性。

Abstract: We introduce an extensive new dataset of MIDI files, created by transcribing
audio recordings of piano performances into their constituent notes. The data
pipeline we use is multi-stage, employing a language model to autonomously
crawl and score audio recordings from the internet based on their metadata,
followed by a stage of pruning and segmentation using an audio classifier. The
resulting dataset contains over one million distinct MIDI files, comprising
roughly 100,000 hours of transcribed audio. We provide an in-depth analysis of
our techniques, offering statistical insights, and investigate the content by
extracting metadata tags, which we also provide. Dataset available at
https://github.com/loubbrad/aria-midi.

</details>


### [293] [You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties](https://arxiv.org/abs/2506.23367)
*Paige Tuttösí,H. Henny Yeung,Yue Wang,Jean-Julien Aucouturier,Angelica Lim*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present the first text-to-speech (TTS) system tailored to second language
(L2) speakers. We use duration differences between American English tense
(longer) and lax (shorter) vowels to create a "clarity mode" for Matcha-TTS.
Our perception studies showed that French-L1, English-L2 listeners had fewer
(at least 9.15%) transcription errors when using our clarity mode, and found it
more encouraging and respectful than overall slowed down speech. Remarkably,
listeners were not aware of these effects: despite the decreased word error
rate in clarity mode, listeners still believed that slowing all target words
was the most intelligible, suggesting that actual intelligibility does not
correlate with perceived intelligibility. Additionally, we found that
Whisper-ASR did not use the same cues as L2 speakers to differentiate difficult
vowels and is not sufficient to assess the intelligibility of TTS systems for
these individuals.

</details>


### [294] [Efficient Interleaved Speech Modeling through Knowledge Distillation](https://arxiv.org/abs/2506.23670)
*Mohammadmahdi Nouriborji,Morteza Rohanian*

Key words: 语音生成,层对齐蒸馏,多模态变换器,TinyWave,紧凑模型

TL;DR: 通过层对齐蒸馏技术构建紧凑且表达力强的语音生成模型，压缩大型多模态变换器3倍，性能损失极小。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 解决当前语音语言模型因尺寸和延迟问题难以部署的问题，构建适合实际环境的高效模型。

Method: 采用层对齐蒸馏技术，匹配隐藏状态、注意力图和软化logits，训练2B参数的TinyWave模型家族。

Result: TinyWave在Libri-Light上的表现接近教师模型，语音生成和混合任务表现优秀，适合实时应用。

Conclusion: TinyWave模型适用于资源受限环境，支持语音和文本生成任务，推动了紧凑语音生成的研究。

Abstract: Current speech language models exceed the size and latency constraints of
many deployment environments. We build compact, expressive speech generation
models through layer-aligned distillation, matching hidden states, attention
maps, and softened logits to compress large multimodal transformers by 3x with
minimal loss in performance. We introduce TinyWave, a family of 2B-parameter
models for speech-to-speech and interleaved speech-text generation, trained on
50,000 hours of public audio. TinyWave supports (i) speech-only generation
using phonetic or expressive tokens and (ii) mixed speech-text continuations.
Evaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity
points of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97%
of the teacher's performance, outperforming size-matched baselines. These
models are optimized for deployment on commodity hardware, enabling
applications in real-time conversational agents, assistive technologies, and
low-resource environments. We release models, training code, and evaluation
scripts to support reproducible research on compact, expressive speech
generation.

</details>


### [295] [WavShape: Information-Theoretic Speech Representation Learning for Fair and Privacy-Aware Audio Processing](https://arxiv.org/abs/2506.22789)
*Oguzhan Baser,Ahmet Ege Tanriverdi,Kaan Kale,Sandeep P. Chinchali,Sriram Vishwanath*

Key words: 语音嵌入、信息论、隐私保护、公平性

TL;DR: 提出WavShape框架，通过信息论方法优化语音嵌入，减少敏感属性泄露并保留任务相关信息。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 现有的语音嵌入可能泄露说话者身份、口音等敏感信息，存在偏见训练和隐私泄漏风险。

Method: 利用Donsker-Varadhan公式的互信息估计，指导MI编码器过滤敏感属性，保留任务相关信息。

Result: 在三个数据集上，WavShape将嵌入与敏感属性的互信息减少81%，同时保留97%的任务相关信息。

Conclusion: 通过结合信息论与自监督语音模型，推动了公平、隐私保护和资源高效的语音系统发展。

Abstract: Speech embeddings often retain sensitive attributes such as speaker identity,
accent, or demographic information, posing risks in biased model training and
privacy leakage. We propose WavShape, an information-theoretic speech
representation learning framework that optimizes embeddings for fairness and
privacy while preserving task-relevant information. We leverage mutual
information (MI) estimation using the Donsker-Varadhan formulation to guide an
MI-based encoder that systematically filters sensitive attributes while
maintaining speech content essential for downstream tasks. Experimental results
on three known datasets show that WavShape reduces MI between embeddings and
sensitive attributes by up to 81% while retaining 97% of task-relevant
information. By integrating information theory with self-supervised speech
models, this work advances the development of fair, privacy-aware, and
resource-efficient speech systems.

</details>


### [296] [TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure](https://arxiv.org/abs/2506.23094)
*Qi He,Gus Xia,Ziyu Wang*

Key words: 音乐生成, 层次规划, TOMI, 电子音乐, 人机协作

TL;DR: 论文提出了一种名为TOMI的深度音乐生成方法，通过层次化规划和概念层次生成音乐，展示了比基线更高的质量和结构一致性。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 探索音乐生成中的概念层次，以改善音乐的结构性和创造性。

Method: 利用指令调优的基础LLM开发TOMI模型，通过剪辑、段落、音轨和变换的四维空间表示多轨音乐生成。

Result: 模型能生成具有完整歌曲结构的电子音乐，并通过与REAPER集成支持人机交互创作，实验表明其优于基线方法。

Conclusion: TOMI方法在电子音乐生成中表现出更高的质量和结构一致性，适用于人机协作。

Abstract: Hierarchical planning is a powerful approach to model long sequences
structurally. Aside from considering hierarchies in the temporal structure of
music, this paper explores an even more important aspect: concept hierarchy,
which involves generating music ideas, transforming them, and ultimately
organizing them--across musical time and space--into a complete composition. To
this end, we introduce TOMI (Transforming and Organizing Music Ideas) as a
novel approach in deep music generation and develop a TOMI-based model via
instruction-tuned foundation LLM. Formally, we represent a multi-track
composition process via a sparse, four-dimensional space characterized by clips
(short audio or MIDI segments), sections (temporal positions), tracks
(instrument layers), and transformations (elaboration methods). Our model is
capable of generating multi-track electronic music with full-song structure,
and we further integrate the TOMI-based model with the REAPER digital audio
workstation, enabling interactive human-AI co-creation. Experimental results
demonstrate that our approach produces higher-quality electronic music with
stronger structural coherence compared to baselines.

</details>


### [297] [XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs](https://arxiv.org/abs/2506.23325)
*Yitian Gong,Luozhijie Jin,Ruifan Deng,Dong Zhang,Xin Zhang,Qinyuan Cheng,Zhaoye Fei,Shimin Li,Xipeng Qiu*

Key words: 语音编解码器, 语义信息, 音频重建, 多任务学习

TL;DR: XY-Tokenizer是一种新型语音编解码器，通过多阶段多任务学习解决了现有编解码器在语义丰富性与音频保真度之间的平衡问题，表现优异。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 传统语音编解码器难以同时兼顾高质量的音频重建和语言模型易于建模的特性，作者提出XY-Tokenizer来解决这一问题。

Method: 通过多阶段、多任务学习设计XY-Tokenizer，平衡语义与声学能力的冲突。

Result: XY-Tokenizer在语义和声学任务中表现优异，与同类编解码器相当，且在文本对齐上超越SpeechTokenizer和Mimi，音频重建性能接近BigCodec。

Conclusion: XY-Tokenizer成功解决了语义与声学能力的平衡问题，为语音语言模型提供了更优的编解码器选择。

Abstract: Speech codecs serve as bridges between speech signals and large language
models. An ideal codec for speech language models should not only preserve
acoustic information but also capture rich semantic information. However,
existing speech codecs struggle to balance high-quality audio reconstruction
with ease of modeling by language models. In this study, we analyze the
limitations of previous codecs in balancing semantic richness and acoustic
fidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict
between semantic and acoustic capabilities through multi-stage, multi-task
learning. Experimental results demonstrate that XY-Tokenizer achieves
performance in both semantic and acoustic tasks comparable to that of
state-of-the-art codecs operating at similar bitrates, even though those
existing codecs typically excel in only one aspect. Specifically, XY-Tokenizer
achieves strong text alignment, surpassing distillation-based semantic modeling
methods such as SpeechTokenizer and Mimi, while maintaining a speaker
similarity score of 0.83 between reconstructed and original audio. The
reconstruction performance of XY-Tokenizer is comparable to that of BigCodec,
the current state-of-the-art among acoustic-only codecs, which achieves a
speaker similarity score of 0.84 at a similar bitrate. Code and models are
available at https://github.com/gyt1145028706/XY-Tokenizer.

</details>


### [298] [From Large-scale Audio Tagging to Real-Time Explainable Emergency Vehicle Sirens Detection](https://arxiv.org/abs/2506.23437)
*Stefano Giacomelli,Marco Giordano,Claudia Rinaldi,Fabio Graziosi*

Key words: 紧急车辆警报声检测、轻量级CNN、边缘计算、AudioSet、E2PANNs

TL;DR: 该论文提出了一种轻量级卷积神经网络E2PANNs，专门用于紧急车辆警报声的检测，通过优化训练和边缘设备部署验证了其高效性和可靠性。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 紧急车辆警报声的准确识别对智能交通和自动驾驶技术至关重要，当前方法因缺乏大规模数据集和高计算需求而受限。

Method: 基于PANNs框架开发了E2PANNs，利用专用数据集AudioSet EV进行微调，并通过消融实验和边缘设备部署验证性能。

Result: E2PANNs在紧急车辆警报声检测中表现优异，计算效率高，适用于边缘设备和安全关键应用。

Conclusion: E2PANNs为该领域的新标杆，兼具高效性和实用性。

Abstract: Accurate recognition of Emergency Vehicle (EV) sirens is critical for the
integration of intelligent transportation systems, smart city monitoring
systems, and autonomous driving technologies. Modern automatic solutions are
limited by the lack of large scale, curated datasets and by the computational
demands of state of the art sound event detection models. This work introduces
E2PANNs (Efficient Emergency Pre trained Audio Neural Networks), a lightweight
Convolutional Neural Network architecture derived from the PANNs framework,
specifically optimized for binary EV siren detection. Leveraging our dedicated
subset of AudioSet (AudioSet EV) we fine-tune and evaluate E2PANNs across
multiple reference datasets and test its viability on embedded hardware. The
experimental campaign includes ablation studies, cross-domain benchmarking, and
real-time inference deployment on edge device. Interpretability analyses
exploiting Guided Backpropagation and ScoreCAM algorithms provide insights into
the model internal representations and validate its ability to capture distinct
spectrotemporal patterns associated with different types of EV sirens. Real
time performance is assessed through frame wise and event based detection
metrics, as well as a detailed analysis of false positive activations. Results
demonstrate that E2PANNs establish a new state of the art in this research
domain, with high computational efficiency, and suitability for edge-based
audio monitoring and safety-critical applications.

</details>


### [299] [Scaling Self-Supervised Representation Learning for Symbolic Piano Performance](https://arxiv.org/abs/2506.23869)
*Louis Bradshaw,Honglu Fan,Alexander Spangher,Stella Biderman,Simon Colton*

Key words: 生成模型, Transformer, 符号音乐, MIDI嵌入, 音乐分类

TL;DR: 研究了基于大量符号钢琴转录训练的生成自回归Transformer模型，其在音乐生成、分类和MIDI嵌入任务中表现优异，超越了传统符号生成方法，并在MIR分类任务中取得了最先进的结果。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 探索生成自回归Transformer模型在符号音乐处理中的潜力，尤其是在音乐生成和分类任务中。

Method: 模型首先在大规模音乐数据上预训练，随后在小规模高质量数据上微调，用于音乐生成、分类和MIDI嵌入任务，并采用SimCLR框架。

Result: 生成模型在钢琴音乐连贯性方面优于其他符号生成技术，对比模型的表征在MIR分类任务中取得了最优表现，且微调后仅需少量标记数据即可适配下游任务。

Conclusion: 该模型在符号音乐处理中展现出强大能力，为音乐生成和分类任务提供了高效解决方案。

Abstract: We study the capabilities of generative autoregressive transformer models
trained on large amounts of symbolic solo-piano transcriptions. After first
pretraining on approximately 60,000 hours of music, we use a comparatively
smaller, high-quality subset, to finetune models to produce musical
continuations, perform symbolic classification tasks, and produce
general-purpose contrastive MIDI embeddings by adapting the SimCLR framework to
symbolic music. When evaluating piano continuation coherence, our generative
model outperforms leading symbolic generation techniques and remains
competitive with proprietary audio generation models. On MIR classification
benchmarks, frozen representations from our contrastive model achieve
state-of-the-art results in linear probe experiments, while direct finetuning
demonstrates the generalizability of pretrained representations, often
requiring only a few hundred labeled examples to specialize to downstream
tasks.

</details>


### [300] [Emergent musical properties of a transformer under contrastive self-supervised learning](https://arxiv.org/abs/2506.23873)
*Yuexuan Kong,Gabriel Meseguer-Brocal,Vincent Lostanlen,Mathieu Lagrange,Romain Hennequin*

Key words: 音乐信息检索, 对比学习, Transformer, 自监督学习

TL;DR: 论文挑战了对比自监督学习在局部音乐信息检索任务中表现不佳的假设，展示了Transformer与对比学习的结合潜力。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 探讨对比自监督学习在局部任务如和弦估计中的适用性，打破认为其仅适用于全局任务的固有认知。

Method: 使用一维时间-频率域轻量级视觉Transformer（ViT-1D），通过NT-Xent损失进行对比学习训练。

Result: 序列令牌在局部任务中表现意外良好，且高层音乐特征如音符起始可从注意力图中自然涌现。

Conclusion: 对比自监督学习与Transformer结合在音乐序列建模中具有潜力，为模型解释提供了新视角。

Abstract: In music information retrieval (MIR), contrastive self-supervised learning
for general-purpose representation models is effective for global tasks such as
automatic tagging. However, for local tasks such as chord estimation, it is
widely assumed that contrastively trained general-purpose self-supervised
models are inadequate and that more sophisticated SSL is necessary; e.g.,
masked modeling. Our paper challenges this assumption by revealing the
potential of contrastive SSL paired with a transformer in local MIR tasks. We
consider a lightweight vision transformer with one-dimensional patches in the
time--frequency domain (ViT-1D) and train it with simple contrastive SSL
through normalized temperature-scaled cross-entropy loss (NT-Xent). Although
NT-Xent operates only over the class token, we observe that, potentially thanks
to weight sharing, informative musical properties emerge in ViT-1D's sequence
tokens. On global tasks, the temporal average of class and sequence tokens
offers a performance increase compared to the class token alone, showing useful
properties in the sequence tokens. On local tasks, sequence tokens perform
unexpectedly well, despite not being specifically trained for. Furthermore,
high-level musical features such as onsets emerge from layer-wise attention
maps and self-similarity matrices show different layers capture different
musical dimensions. Our paper does not focus on improving performance but
advances the musical interpretation of transformers and sheds light on some
overlooked abilities of contrastive SSL paired with transformers for sequence
modeling in MIR.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [301] [Learning Truthful Mechanisms without Discretization](https://arxiv.org/abs/2506.22911)
*Yunxuan Ma,Siqiang Wang,Zhijian Duan,Yukun Cheng,Xiaotie Deng*

Key words: TEDI, 机制设计, 定价规则, 无需离散化, Partial GroupMax Network

TL;DR: 论文提出了一种无需离散化的算法TEDI，用于学习真实和效用最大化的机制，解决了现有方法因离散化导致低效的问题。

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: 现有学习型机制设计方法依赖离散化以确保真实性，但随着问题规模增大会导致效率下降，因此需要一种无需离散化的新方法。

Method: 提出基于定价规则的菜单机制，采用Partial GroupMax Network参数化定价规则，并开发了新的训练技术如协方差技巧和连续采样。

Result: TEDI在理论分析中保证了真实性、完全表达性和维度不敏感性，实验表现优于现有方法。

Conclusion: 这是首个无需离散化的真实机制学习方法，提高了效率，并可能为自动化机制设计和可微分经济学提供新思路。

Abstract: This paper introduces TEDI (Truthful, Expressive, and Dimension-Insensitive
approach), a discretization-free algorithm to learn truthful and
utility-maximizing mechanisms. Existing learning-based approaches often rely on
discretization of outcome spaces to ensure truthfulness, which leads to
inefficiency with increasing problem size. To address this limitation, we
formalize the concept of pricing rules, defined as functions that map outcomes
to prices. Based on this concept, we propose a novel menu mechanism, which can
be equivalent to a truthful direct mechanism under specific conditions. The
core idea of TEDI lies in its parameterization of pricing rules using Partial
GroupMax Network, a new network architecture designed to universally
approximate partial convex functions. To learn optimal pricing rules, we
develop novel training techniques, including covariance trick and continuous
sampling, to derive unbiased gradient estimators compatible with first-order
optimization. Theoretical analysis establishes that TEDI guarantees
truthfulness, full expressiveness, and dimension-insensitivity. Experimental
evaluation in the studied auction setting demonstrates that TEDI achieves
strong performance, competitive with or exceeding state-of-the-art methods.
  This work presents the first approaches to learn truthful mechanisms without
outcome discretization, thereby enhancing algorithmic efficiency. The proposed
concepts, network architecture, and learning techniques might offer potential
value and provide new insights for automated mechanism design and
differentiable economics.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [302] [Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation](https://arxiv.org/abs/2506.23717)
*Xingting Yao,Qinghao Hu,Fei Zhou,Tielong Liu,Gang Li,Peisong Wang,Jian Cheng*

Key words: 脉冲神经网络, 多比特设计, 自适应比特分配, 资源优化, 深度学习

TL;DR: 提出了一种自适应比特分配策略，用于直接训练的脉冲神经网络（SNN），通过动态调整权值和脉冲的比特宽度及时序长度，以提高效率和准确性。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 现有SNN在多比特设计中存在内存和计算需求过高的问题，不同层的比特分配可能造成资源浪费或干扰，因此需优化比特分配。

Method: 引入了可学习的权值和脉冲比特宽度及时序长度，提出改进的脉冲神经元以支持动态比特宽度和时序，并解决了步长不匹配问题。

Result: 实验表明，该方法在多种数据集上降低了内存和计算成本，同时提高了准确性。在ImageNet上，SEWResNet-34实现了2.69%的精度提升和4.16倍的比特预算降低。

Conclusion: 通过自适应比特分配和改进的神经元设计，显著提升了SNN的性能和效率。

Abstract: Multi-bit spiking neural networks (SNNs) have recently become a heated
research spot, pursuing energy-efficient and high-accurate AI. However, with
more bits involved, the associated memory and computation demands escalate to
the point where the performance improvements become disproportionate. Based on
the insight that different layers demonstrate different importance and extra
bits could be wasted and interfering, this paper presents an adaptive bit
allocation strategy for direct-trained SNNs, achieving fine-grained layer-wise
allocation of memory and computation resources. Thus, SNN's efficiency and
accuracy can be improved. Specifically, we parametrize the temporal lengths and
the bit widths of weights and spikes, and make them learnable and controllable
through gradients. To address the challenges caused by changeable bit widths
and temporal lengths, we propose the refined spiking neuron, which can handle
different temporal lengths, enable the derivation of gradients for temporal
lengths, and suit spike quantization better. In addition, we theoretically
formulate the step-size mismatch problem of learnable bit widths, which may
incur severe quantization errors to SNN, and accordingly propose the step-size
renewal mechanism to alleviate this issue. Experiments on various datasets,
including the static CIFAR and ImageNet and the dynamic CIFAR-DVS and
DVS-GESTURE, demonstrate that our methods can reduce the overall memory and
computation cost while achieving higher accuracy. Particularly, our
SEWResNet-34 can achieve a 2.69\% accuracy gain and 4.16$\times$ lower bit
budgets over the advanced baseline work on ImageNet. This work will be fully
open-sourced.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [303] [Deep Hedging to Manage Tail Risk](https://arxiv.org/abs/2506.22611)
*Yuming Ma*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: q-fin.PM

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Extending Buehler et al.'s 2019 Deep Hedging paradigm, we innovatively employ
deep neural networks to parameterize convex-risk minimization (CVaR/ES) for the
portfolio tail-risk hedging problem. Through comprehensive numerical
experiments on crisis-era bootstrap market simulators -- customizable with
transaction costs, risk budgets, liquidity constraints, and market impact --
our end-to-end framework not only achieves significant one-day 99% CVaR
reduction but also yields practical insights into friction-aware strategy
adaptation, demonstrating robustness and operational viability in realistic
markets.

</details>


### [304] [Can We Reliably Predict the Fed's Next Move? A Multi-Modal Approach to U.S. Monetary Policy Forecasting](https://arxiv.org/abs/2506.22763)
*Fiona Xiao Jingyi,Lili Liu*

Key words: 货币政策预测, 多模态模型, 美联储通信, TF-IDF, XGBoost, FinBERT

TL;DR: 结合结构化经济指标与美联储通信文本的非结构化信号，研究显示混合模型（如XGBoost结合TF-IDF特征）在预测美联储政策变化时表现最优（AUC 0.83），优于单一模型，同时具备可解释性。

<details>
  <summary>Details</summary>

Main category: q-fin.PM

Motivation: 预测央行政策决策对风险管理至关重要，但传统方法依赖结构化数据，未能充分利用央行通信中的前瞻性信号。

Method: 采用多模态框架，比较传统机器学习、Transformer语言模型和深度学习模型，结合TF-IDF特征与经济指标。

Result: 混合模型表现最优，XGBoost结合TF-IDF特征AUC达0.83；FinBERT情感特征提升有限。SHAP分析显示稀疏特征更相关。

Conclusion: 透明结合文本与结构化信号的简单混合模型能提供高准确性和可解释性，对决策者具实用价值。

Abstract: Forecasting central bank policy decisions remains a persistent challenge for
investors, financial institutions, and policymakers due to the wide-reaching
impact of monetary actions. In particular, anticipating shifts in the U.S.
federal funds rate is vital for risk management and trading strategies.
Traditional methods relying only on structured macroeconomic indicators often
fall short in capturing the forward-looking cues embedded in central bank
communications.
  This study examines whether predictive accuracy can be enhanced by
integrating structured data with unstructured textual signals from Federal
Reserve communications. We adopt a multi-modal framework, comparing traditional
machine learning models, transformer-based language models, and deep learning
architectures in both unimodal and hybrid settings.
  Our results show that hybrid models consistently outperform unimodal
baselines. The best performance is achieved by combining TF-IDF features of
FOMC texts with economic indicators in an XGBoost classifier, reaching a test
AUC of 0.83. FinBERT-based sentiment features marginally improve ranking but
perform worse in classification, especially under class imbalance. SHAP
analysis reveals that sparse, interpretable features align more closely with
policy-relevant signals.
  These findings underscore the importance of integrating textual and
structured signals transparently. For monetary policy forecasting, simpler
hybrid models can offer both accuracy and interpretability, delivering
actionable insights for researchers and decision-makers.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [305] [Hindsight-Guided Momentum (HGM) Optimizer: An Approach to Adaptive Learning Rate](https://arxiv.org/abs/2506.22479)
*Krisanu Sarkar*

Key words: HGM, 自适应优化, 学习率, 方向一致性, 非凸优化

TL;DR: HGM是一种自适应学习率优化算法，通过评估当前梯度与累积动量的方向一致性来调整学习率，提高了收敛速度和稳定性。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 传统自适应优化器（如Adam或RMSprop）仅依赖梯度大小调整学习率，忽略了方向信息。HGM通过引入几何线索（如梯度方向一致性）来更智能地响应优化路径。

Method: HGM利用余弦相似度评估当前梯度与累积动量的方向一致性，动态调整学习率：方向一致时增加学习率，方向冲突时降低学习率。

Result: HGM在平滑区域加速收敛，在尖锐或不稳定区域保持稳定性，同时保持了计算和内存效率。

Conclusion: HGM在非凸优化（如深度神经网络训练）中表现优于现有方法，是一种简单而有效的改进。

Abstract: We introduce Hindsight-Guided Momentum (HGM), a first-order optimization
algorithm that adaptively scales learning rates based on the directional
consistency of recent updates. Traditional adaptive methods, such as Adam or
RMSprop , adapt learning dynamics using only the magnitude of gradients, often
overlooking important geometric cues.Geometric cues refer to directional
information, such as the alignment between current gradients and past updates,
which reflects the local curvature and consistency of the optimization path.
HGM addresses this by incorporating a hindsight mechanism that evaluates the
cosine similarity between the current gradient and accumulated momentum. This
allows it to distinguish between coherent and conflicting gradient directions,
increasing the learning rate when updates align and reducing it in regions of
oscillation or noise. The result is a more responsive optimizer that
accelerates convergence in smooth regions of the loss surface while maintaining
stability in sharper or more erratic areas. Despite this added adaptability,
the method preserves the computational and memory efficiency of existing
optimizers.By more intelligently responding to the structure of the
optimization landscape, HGM provides a simple yet effective improvement over
existing approaches, particularly in non-convex settings like that of deep
neural network training.

</details>


### [306] [Correlated Mutations for Integer Programming](https://arxiv.org/abs/2506.22526)
*Ofer M. Shir,Michael Emmerich*

Key words: 整数演化策略（IESs），整数规划（IP），ℓ₁-范数，截断正态分布（TN），双几何分布（DG）

TL;DR: 该研究提出了整数演化策略（IESs），针对整数规划问题，采用ℓ₁-范数替代传统的ℓ₂-范数，并研究了截断正态（TN）和双几何（DG）分布的理论性质，最终证明DG分布在无界整数搜索中表现更优。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 尽管整数规划（IP）的理论进展显著降低了其复杂性，但启发式方法仍是主要解决方案。研究旨在为整数演化策略（IESs）奠定基础，这类随机搜索启发式方法专为连续空间设计，但需通过离散化和复杂修补来处理IP问题。

Method: 研究采用ℓ₁-范数，探索适合的步长和量化整数格上相关性的替代度量。重点研究无界整数决策变量的变异分布，比较了均匀分布和二项分布，最终聚焦于截断正态（TN）和双几何（DG）分布，并分析了它们的理论性质。

Result: 数值模拟表明，DG分布在无界整数搜索中表现更优。实验证明，基于相关DG变异的IES在非可分二次IP问题中优于其他策略。

Conclusion: 研究发现，用DG替代默认的TN分布在理论和实践中均有优势，但最关键的是采用ℓ₁-范数而非ℓ₂-范数。

Abstract: Even with the recent theoretical advancements that dramatically reduced the
complexity of Integer Programming (IP), heuristics remain the dominant
problem-solvers for this difficult category. This study seeks to establish the
groundwork for Integer Evolution Strategies (IESs), a class of randomized
search heuristics inherently designed for continuous spaces. IESs already excel
in treating IP in practice, but accomplish it via discretization and by
applying sophisticated patches to their continuous operators, while
persistently using the $\ell_2$-norm as their operation pillar. We lay
foundations for discrete search, by adopting the $\ell_1$-norm, accounting for
the suitable step-size, and questioning alternative measures to quantify
correlations over the integer lattice. We focus on mutation distributions for
unbounded integer decision variables. We briefly discuss a couple of candidate
discrete probabilities induced by the uniform and binomial distributions, which
we show to possess less appealing theoretical properties, and then narrow down
to the Truncated Normal (TN) and Double Geometric (DG) distributions. We
explore their theoretical properties, including entropy functions, and propose
a procedure to generate scalable correlated mutation distributions. Our
investigations are accompanied by extensive numerical simulations, which
consistently support the claim that the DG distribution is better suited for
unbounded integer search. We link our theoretical perspective to empirical
evidence indicating that an IES with correlated DG mutations outperformed other
strategies over non-separable quadratic IP. We conclude that while the
replacement of the default TN distribution by the DG is theoretically justified
and practically beneficial, the truly crucial change lies in adopting the
$\ell_1$-norm over the $\ell_2$-norm.

</details>


### [307] [Deep neural networks can provably solve Bellman equations for Markov decision processes without the curse of dimensionality](https://arxiv.org/abs/2506.22851)
*Arnulf Jentzen,Konrad Kleinberg,Thomas Kruse*

Key words: 马尔可夫决策过程, $Q$-函数, 深度神经网络, 贝尔曼方程, 强化学习

TL;DR: 该论文研究了马尔可夫决策过程（MDP）中无限时域和有限控制集的$Q$-函数的深度神经网络（DNN）近似问题，证明了在特定条件下，$Q$-函数可以通过ReLU激活的DNN以多项式增长的参数数量近似。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: MDP和贝尔曼方程是强化学习理论的数学基础，研究如何高效近似$Q$-函数对解决MDP问题至关重要。本文旨在通过DNN解决这一近似问题。

Method: 使用具有leaky ReLU激活的DNN近似MDP的回报函数和随机转移动态，并利用全历史递归多层固定点（MLFP）逼近方案证明$Q$-函数的$L^2$近似可行性。

Result: 在状态空间维度$d$和误差倒数$1/\varepsilon$的多项式增长范围内，DNN可以有效地近似$Q$-函数。

Conclusion: 该方法为复杂MDP问题的高效近似提供了理论支持，扩展了DNN在强化学习中的应用。

Abstract: Discrete time stochastic optimal control problems and Markov decision
processes (MDPs) are fundamental models for sequential decision-making under
uncertainty and as such provide the mathematical framework underlying
reinforcement learning theory. A central tool for solving MDPs is the Bellman
equation and its solution, the so-called $Q$-function. In this article, we
construct deep neural network (DNN) approximations for $Q$-functions associated
to MDPs with infinite time horizon and finite control set $A$. More
specifically, we show that if the the payoff function and the random transition
dynamics of the MDP can be suitably approximated by DNNs with leaky rectified
linear unit (ReLU) activation, then the solutions $Q_d\colon \mathbb R^d\to
\mathbb R^{|A|}$, $d\in \mathbb{N}$, of the associated Bellman equations can
also be approximated in the $L^2$-sense by DNNs with leaky ReLU activation
whose numbers of parameters grow at most polynomially in both the dimension
$d\in \mathbb{N}$ of the state space and the reciprocal $1/\varepsilon$ of the
prescribed error $\varepsilon\in (0,1)$. Our proof relies on the recently
introduced full-history recursive multilevel fixed-point (MLFP) approximation
scheme.

</details>


### [308] [Proving the Limited Scalability of Centralized Distributed Optimization via a New Lower Bound Construction](https://arxiv.org/abs/2506.23836)
*Alexander Tyurin*

Key words: 分布式优化,联邦学习,无偏稀疏化,通信效率,可扩展性

TL;DR: 分布式优化在联邦学习中应用，研究者发现使用无偏稀疏化压缩器时，通信时间会限制方法的可扩展性，即使在同质条件下也无法超越对数多项式改进。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 研究分布式优化的可扩展性，特别是在联邦学习中的通信和计算效率问题。

Method: 构造新的“最坏情况”函数，开发新的下界框架，分析随机和的集中性。

Result: 证明在服务器到工作者的通信时间限制下，无法设计出超越对数多项式改进的无偏随机稀疏化压缩器。

Conclusion: 分布式优化在同质条件下仍存在基本限制，通信成为瓶颈。

Abstract: We consider centralized distributed optimization in the classical federated
learning setup, where $n$ workers jointly find an $\varepsilon$-stationary
point of an $L$-smooth, $d$-dimensional nonconvex function $f$, having access
only to unbiased stochastic gradients with variance $\sigma^2$. Each worker
requires at most $h$ seconds to compute a stochastic gradient, and the
communication times from the server to the workers and from the workers to the
server are $\tau_{s}$ and $\tau_{w}$ seconds per coordinate, respectively. One
of the main motivations for distributed optimization is to achieve scalability
with respect to $n$. For instance, it is well known that the distributed
version of SGD has a variance-dependent runtime term $\frac{h \sigma^2 L
\Delta}{n \varepsilon^2},$ which improves with the number of workers $n,$ where
$\Delta = f(x^0) - f^*,$ and $x^0 \in R^d$ is the starting point. Similarly,
using unbiased sparsification compressors, it is possible to reduce both the
variance-dependent runtime term and the communication runtime term. However,
once we account for the communication from the server to the workers
$\tau_{s}$, we prove that it becomes infeasible to design a method using
unbiased random sparsification compressors that scales both the server-side
communication runtime term $\tau_{s} d \frac{L \Delta}{\varepsilon}$ and the
variance-dependent runtime term $\frac{h \sigma^2 L \Delta}{\varepsilon^2},$
better than poly-logarithmically in $n$, even in the homogeneous (i.i.d.) case,
where all workers access the same distribution. To establish this result, we
construct a new "worst-case" function and develop a new lower bound framework
that reduces the analysis to the concentration of a random sum, for which we
prove a concentration bound. These results reveal fundamental limitations in
scaling distributed optimization, even under the homogeneous assumption.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [309] [Beyond Code: The Multidimensional Impacts of Large Language Models in Software Development](https://arxiv.org/abs/2506.22704)
*Sardar Fatooreh Bonabi,Sarah Bana,Tingting Nian,Vijay Gurbaxani*

Key words: LLMs, 开源软件, 生产力, 知识共享, 技能获取

TL;DR: LLMs显著提升开源软件开发者的生产力、知识共享和技能获取，效果因开发者经验水平而异。

<details>
  <summary>Details</summary>

Main category: econ.GN

Motivation: 研究LLMs如何通过代码开发、协作知识转移和技能发展影响开源软件（OSS）领域。

Method: 利用意大利ChatGPT禁令的自然实验，采用双重差分法和双向固定效应分析GitHub上三国的88,022名开发者数据。

Result: ChatGPT访问使开发者生产力提升6.4%，知识共享提升9.6%，技能获取提升8.4%；效果在复杂或快速变化的环境中更显著。

Conclusion: 战略部署LLMs可加速新手开发者的成长、促进知识共享，并支持快速技能习得，提升长期组织生产力。

Abstract: Large language models (LLMs) are poised to significantly impact software
development, especially in the Open-Source Software (OSS) sector. To understand
this impact, we first outline the mechanisms through which LLMs may influence
OSS through code development, collaborative knowledge transfer, and skill
development. We then empirically examine how LLMs affect OSS developers' work
in these three key areas. Leveraging a natural experiment from a temporary
ChatGPT ban in Italy, we employ a Difference-in-Differences framework with
two-way fixed effects to analyze data from all OSS developers on GitHub in
three similar countries, Italy, France, and Portugal, totaling 88,022 users. We
find that access to ChatGPT increases developer productivity by 6.4%, knowledge
sharing by 9.6%, and skill acquisition by 8.4%. These benefits vary
significantly by user experience level: novice developers primarily experience
productivity gains, whereas more experienced developers benefit more from
improved knowledge sharing and accelerated skill acquisition. In addition, we
find that LLM-assisted learning is highly context-dependent, with the greatest
benefits observed in technically complex, fragmented, or rapidly evolving
contexts. We show that the productivity effects of LLMs extend beyond direct
code generation to include enhanced collaborative learning and knowledge
exchange among developers; dynamics that are essential for gaining a holistic
understanding of LLMs' impact in OSS. Our findings offer critical managerial
implications: strategically deploying LLMs can accelerate novice developers'
onboarding and productivity, empower intermediate developers to foster
knowledge sharing and collaboration, and support rapid skill acquisition,
together enhancing long-term organizational productivity and agility.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [310] [Learning robust parameter inference and density reconstruction in flyer plate impact experiments](https://arxiv.org/abs/2506.23914)
*Evan Bell,Daniel A. Serino,Ben S. Southworth,Trevor Wilcox,Marc L. Klasky*

Key words: 参数估计,放射影像,机器学习,状态方程,孔隙模型

TL;DR: 论文研究了如何通过放射影像实验数据估计材料物理参数，提出了一种结合高低冲击速度实验数据及生成式机器学习的方法，有效解决了传统方法在密度变量不可直接观测时的局限性。

<details>
  <summary>Details</summary>

Main category: physics.comp-ph

Motivation: 在物理和材料科学中，从实验观测中估计物理参数是常见需求，但传统方法因放射影像无法直接提供密度等关键变量而受限。

Method: 结合高低冲击速度实验数据，提出生成式机器学习方法，直接从放射影像生成物理参数的后验分布。

Result: 模拟飞板冲击实验证明该方法能准确估计状态方程和孔隙模型参数，并可应用于流体动力学模拟中。

Conclusion: 该方法对模型失配具有鲁棒性，为从放射影像估计材料性质提供了潜在突破。

Abstract: Estimating physical parameters or material properties from experimental
observations is a common objective in many areas of physics and material
science. In many experiments, especially in shock physics, radiography is the
primary means of observing the system of interest. However, radiography does
not provide direct access to key state variables, such as density, which
prevents the application of traditional parameter estimation approaches. Here
we focus on flyer plate impact experiments on porous materials, and resolving
the underlying parameterized equation of state (EoS) and crush porosity model
parameters given radiographic observation(s). We use machine learning as a tool
to demonstrate with high confidence that using only high impact velocity data
does not provide sufficient information to accurately infer both EoS and crush
model parameters, even with fully resolved density fields or a dynamic sequence
of images. We thus propose an observable data set consisting of low and high
impact velocity experiments/simulations that capture different regimes of
compaction and shock propagation, and proceed to introduce a generative machine
learning approach which produces a posterior distribution of physical
parameters directly from radiographs. We demonstrate the effectiveness of the
approach in estimating parameters from simulated flyer plate impact
experiments, and show that the obtained estimates of EoS and crush model
parameters can then be used in hydrodynamic simulations to obtain accurate and
physically admissible density reconstructions. Finally, we examine the
robustness of the approach to model mismatches, and find that the learned
approach can provide useful parameter estimates in the presence of
out-of-distribution radiographic noise and previously unseen physics, thereby
promoting a potential breakthrough in estimating material properties from
experimental radiographic images.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [311] [Report on NSF Workshop on Science of Safe AI](https://arxiv.org/abs/2506.22492)
*Rajeev Alur,Greg Durrett,Hadas Kress-Gazit,Corina Păsăreanu,René Vidal*

Key words: AI安全, 可信AI, NSF SLES计划, 机器学习, 自主系统

TL;DR: 该论文探讨了如何开发既准确又安全的AI系统，并通过NSF SLES计划组织的研讨会提出了新的研究议程，重点关注AI安全的理论、方法和工具。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 当前复杂的AI模型对用户不透明且缺乏安全保证，这阻碍了AI技术在社会问题中的应用，因此需要开发安全可信的AI系统。

Method: 通过NSF SLES计划组织的多学科研讨会，汇集研究者讨论AI安全的不同方面，并形成研究报告。

Result: 研讨会提出了一个新的研究议程，旨在为下一代AI系统开发理论、方法和工具。

Conclusion: 实现AI的全面潜力需要加强其在安全性和可信度方面的研究，以确保广泛应用中的可靠性。

Abstract: Recent advances in machine learning, particularly the emergence of foundation
models, are leading to new opportunities to develop technology-based solutions
to societal problems. However, the reasoning and inner workings of today's
complex AI models are not transparent to the user, and there are no safety
guarantees regarding their predictions. Consequently, to fulfill the promise of
AI, we must address the following scientific challenge: how to develop AI-based
systems that are not only accurate and performant but also safe and
trustworthy?
  The criticality of safe operation is particularly evident for autonomous
systems for control and robotics, and was the catalyst for the Safe Learning
Enabled Systems (SLES) program at NSF. For the broader class of AI
applications, such as users interacting with chatbots and clinicians receiving
treatment recommendations, safety is, while no less important, less
well-defined with context-dependent interpretations. This motivated the
organization of a day-long workshop, held at University of Pennsylvania on
February 26, 2025, to bring together investigators funded by the NSF SLES
program with a broader pool of researchers studying AI safety. This report is
the result of the discussions in the working groups that addressed different
aspects of safety at the workshop. The report articulates a new research agenda
focused on developing theory, methods, and tools that will provide the
foundations of the next generation of AI-enabled systems.

</details>


### [312] [Mitigating Gambling-Like Risk-Taking Behaviors in Large Language Models: A Behavioral Economics Approach to AI Safety](https://arxiv.org/abs/2506.22496)
*Y. Du*

Key words: 大型语言模型,赌博心理学,风险感知,行为经济学,RARG框架

TL;DR: 论文提出了一种风险感知响应生成（RARG）框架，通过借鉴赌博心理学理论，减少大型语言模型（LLMs）中的赌博式行为偏差，如过度自信、损失追逐和概率误判。实验结果显示，该框架有效地降低了这些行为。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 研究发现大型语言模型表现出类似赌博心理的系统性风险行为，如过度自信和损失追逐。为了减少这些行为偏差，研究从行为经济学和前景理论中汲取灵感，提出了解决方案。

Method: 提出了RARG框架，包括风险校准训练、损失规避机制和不确定性感知决策。通过改编赌博心理学实验（如爱荷华赌博任务）进行模型评估。

Result: 实验证实RARG框架能显著减少赌博式行为：过度自信偏差降低18.7%，损失追逐倾向减少24.3%，并在多种场景中改善了风险校准效果。

Conclusion: 该研究首次建立了系统性的框架，用于理解和减轻AI系统中的赌博心理模式，为未来研究奠定了基础。

Abstract: Large Language Models (LLMs) exhibit systematic risk-taking behaviors
analogous to those observed in gambling psychology, including overconfidence
bias, loss-chasing tendencies, and probability misjudgment. Drawing from
behavioral economics and prospect theory, we identify and formalize these
"gambling-like" patterns where models sacrifice accuracy for high-reward
outputs, exhibit escalating risk-taking after errors, and systematically
miscalibrate uncertainty. We propose the Risk-Aware Response Generation (RARG)
framework, incorporating insights from gambling research to address these
behavioral biases through risk-calibrated training, loss-aversion mechanisms,
and uncertainty-aware decision making. Our approach introduces novel evaluation
paradigms based on established gambling psychology experiments, including AI
adaptations of the Iowa Gambling Task and probability learning assessments.
Experimental results demonstrate measurable reductions in gambling-like
behaviors: 18.7\% decrease in overconfidence bias, 24.3\% reduction in
loss-chasing tendencies, and improved risk calibration across diverse
scenarios. This work establishes the first systematic framework for
understanding and mitigating gambling psychology patterns in AI systems.

</details>


### [313] [Peer Review as Structured Commentary: Immutable Identity, Public Dialogue, and Reproducible Scholarship](https://arxiv.org/abs/2506.22497)
*Craig Steven Wright*

Key words: 同行评审, 区块链, 学术评价, 透明性, 人工智能

TL;DR: 该论文将同行评审重新定义为结构化的公开评论，提出一种透明、身份关联且可复现的学术评价系统。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 传统学术评审因匿名性、延迟性和门槛问题受到限制，作者希望通过透明化和技术手段改进这一过程。

Method: 利用区块链技术实现不可篡改的审计追踪，结合人工智能进行迭代综合，设计了一个激励学术贡献的框架。

Result: 该系统能够追踪学术知识的演化并反映声誉动态，适用于从计算科学到人文的多个领域。

Conclusion: 该模型将学术知识重构为一个动态过程，而非静态凭证，推动了学术评审的革新。

Abstract: This paper reconceptualises peer review as structured public commentary.
Traditional academic validation is hindered by anonymity, latency, and
gatekeeping. We propose a transparent, identity-linked, and reproducible system
of scholarly evaluation anchored in open commentary. Leveraging blockchain for
immutable audit trails and AI for iterative synthesis, we design a framework
that incentivises intellectual contribution, captures epistemic evolution, and
enables traceable reputational dynamics. This model empowers fields from
computational science to the humanities, reframing academic knowledge as a
living process rather than a static credential.

</details>


### [314] [Ask before you Build: Rethinking AI-for-Good in Human Trafficking Interventions](https://arxiv.org/abs/2506.22512)
*Pratheeksha Nair,Gabriel Lefebvre,Sophia Garrel,Maryam Molamohammadi,Reihaneh Rabbany*

Key words: AI伦理，Radical Questioning，技术解决主义，边缘群体，系统性不公

TL;DR: 论文提出了Radical Questioning (RQ)框架，作为一种五步伦理评估工具，用于在AI项目启动前批判性评估其必要性，尤其是在涉及边缘群体和系统性不公的领域。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 现有AI技术干预可能过于简化复杂社会问题，甚至对边缘群体造成伤害，因此需要一种工具来避免技术解决主义的风险。

Method: 引入RQ框架，通过五个步骤评估AI项目的必要性，包括面对假设、权力映射和潜在危害分析。

Result: 通过AI反人口贩卖案例研究，RQ揭示了被忽视的社会文化复杂性，并引导从监控转向幸存者赋权工具。

Conclusion: RQ框架不仅适用于反人口贩卖，也可推广至其他领域，但其具体问题需情境化。RQ挑战了工具主义规范，强调关系性和反思性责任。

Abstract: AI for good initiatives often rely on the assumption that technical
interventions can resolve complex social problems. In the context of human
trafficking (HT), such techno-solutionism risks oversimplifying exploitation,
reinforcing power imbalances and causing harm to the very communities AI claims
to support. In this paper, we introduce the Radical Questioning (RQ) framework
as a five step, pre-project ethical assessment tool to critically evaluate
whether AI should be built at all, especially in domains involving marginalized
populations and entrenched systemic injustice. RQ does not replace principles
based ethics but precedes it, offering an upstream, deliberative space to
confront assumptions, map power, and consider harms before design. Using a case
study in AI for HT, we demonstrate how RQ reveals overlooked sociocultural
complexities and guides us away from surveillance based interventions toward
survivor empowerment tools. While developed in the context of HT, RQ's five
step structure can generalize to other domains, though the specific questions
must be contextual. This paper situates RQ within a broader AI ethics
philosophy that challenges instrumentalist norms and centers relational,
reflexive responsibility.

</details>


### [315] [Computational Analysis of Climate Policy](https://arxiv.org/abs/2506.22449)
*Carolyn Hicks*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This thesis explores the impact of the Climate Emergency movement on local
government climate policy, using computational methods. The Climate Emergency
movement sought to accelerate climate action at local government level through
the mechanism of Climate Emergency Declarations (CEDs), resulting in a series
of commitments from councils to treat climate change as an emergency. With the
aim of assessing the potential of current large language models to answer
complex policy questions, I first built and configured a system named PALLM
(Policy Analysis with a Large Language Model), using the OpenAI model GPT-4.
This system is designed to apply a conceptual framework for climate emergency
response plans to a dataset of climate policy documents. I validated the
performance of this system with the help of local government policymakers, by
generating analyses of the climate policies of 11 local governments in Victoria
and assessing the policymakers' level of agreement with PALLM's responses.
Having established that PALLM's performance is satisfactory, I used it to
conduct a large-scale analysis of current policy documents from local
governments in the state of Victoria, Australia. This thesis presents the
methodology and results of this analysis, comparing the results for councils
which have passed a CED to those which did not. This study finds that GPT-4 is
capable of high-level policy analysis, with limitations including a lack of
reliable attribution, and can also enable more nuanced analysis by researchers.
Its use in this research shows that councils which have passed a CED are more
likely to have a recent and climate-specific policy, and show more attention to
urgency, prioritisation, and equity and social justice, than councils which
have not. It concludes that the ability to assess policy documents at scale
opens up exciting new opportunities for policy researchers.

</details>


### [316] [Theories of "Sexuality" in Natural Language Processing Bias Research](https://arxiv.org/abs/2506.22481)
*Jacob Hobbs*

Key words: NLP, 社会偏见, 性取向, 性别混淆, LGBTQ+

TL;DR: 论文分析了NLP领域中对性别和种族偏见的关注，但指出对性取向偏见的详细研究存在不足。通过对55篇文章的调查，发现性取向在大多数文献中未明确定义，且常与性别混淆。建议加强与跨学科文献和LGBTQ+社区的合作。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: NLP技术的快速发展引发了对社会偏见的关注，但对性取向偏见的详细研究较少，填补这一空白是本文的动机。

Method: 通过对55篇量化性取向偏见的NLP文章进行问卷调查和分析，评估性取向的定义和操作化方式。

Result: 发现大多数文献未明确定义性取向，且常将性别与性取向混淆，导致对性取向偏见的量化不准确。

Conclusion: 建议NLP研究更深入结合跨学科文献和LGBTQ+社区，以提高对性取向偏见的分析准确性。

Abstract: In recent years, significant advancements in the field of Natural Language
Processing (NLP) have positioned commercialized language models as
wide-reaching, highly useful tools. In tandem, there has been an explosion of
multidisciplinary research examining how NLP tasks reflect, perpetuate, and
amplify social biases such as gender and racial bias. A significant gap in this
scholarship is a detailed analysis of how queer sexualities are encoded and
(mis)represented by both NLP systems and practitioners. Following previous work
in the field of AI fairness, we document how sexuality is defined and
operationalized via a survey and analysis of 55 articles that quantify
sexuality-based NLP bias. We find that sexuality is not clearly defined in a
majority of the literature surveyed, indicating a reliance on assumed or
normative conceptions of sexual/romantic practices and identities. Further, we
find that methods for extracting biased outputs from NLP technologies often
conflate gender and sexual identities, leading to monolithic conceptions of
queerness and thus improper quantifications of bias. With the goal of improving
sexuality-based NLP bias analyses, we conclude with recommendations that
encourage more thorough engagement with both queer communities and
interdisciplinary literature.

</details>


### [317] [A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models](https://arxiv.org/abs/2506.22493)
*Sadia Kamal,Lalu Prasad Yadav Prakash,S M Rafiuddin,Mohammed Rakib,Arunkumar Bagavathi,Atriya Sen,Sagnik Ray Choudhury*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Political Compass Test (PCT) or similar questionnaires have been used to
quantify LLM's political leanings. Building on a recent line of work that
examines the validity of PCT tests, we demonstrate that variation in standard
generation parameters does not significantly impact the models' PCT scores.
However, external factors such as prompt variations and fine-tuning
individually and in combination affect the same. Finally, we demonstrate that
when models are fine-tuned on text datasets with higher political content than
others, the PCT scores are not differentially affected. This calls for a
thorough investigation into the validity of PCT and similar tests, as well as
the mechanism by which political leanings are encoded in LLMs.

</details>


### [318] [Red Teaming for Generative AI, Report on a Copyright-Focused Exercise Completed in an Academic Medical Center](https://arxiv.org/abs/2506.22523)
*James Wen,Sahil Nalawade,Zhiwei Liang,Catherine Bielick,Marisa Ferrara Boston,Alexander Chowdhury,Adele Collin,Luigi De Angelis,Jacob Ellen,Heather Frase,Rodrigo R. Gameiro,Juan Manuel Gutierrez,Pooja Kadam,Murat Keceli,Srikanth Krishnamurthy,Anne Kwok,Yanan Lance Lu,Heather Mattie,Liam G. McCoy,Katherine Miller,Allison C. Morgan,Marlene Louisa Moerig,Trang Nguyen,Alexander Owen-Post,Alex D. Ruiz,Sreekar Reddy Puchala,Soujanya Samineni,Takeshi Tohyama,Varun Ullanat,Carmine Valenza,Camilo Velez,Pengcheng Wang,Anna Wuest,Yuxiang Zhou,Yingde Zhu,Jason M. Johnson,Jennifer Willcox,Francis J. Vitiello,Leo Anthony G. Celi,Renato Umeton*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Generative AI is present in multiple industries. Dana-Farber Cancer
Institute, in partnership with Microsoft, has created an internal AI tool,
GPT4DFCI. Together we hosted a red teaming event to assess whether the
underlying GPT models that support the tool would output copyrighted data. Our
teams focused on reproducing content from books, news articles, scientific
articles, and electronic health records. We found isolated instances where
GPT4DFCI was able to identify copyrighted material and reproduce exact quotes
from famous books which indicates that copyrighted material was in the training
data. The model was not able to reproduce content from our target news article,
scientific article, or electronic health records. However, there were instances
of fabrication. As a result of this event, a mitigation strategy is in
production in GPT4DFCI v2.8.2, deployed on January 21, 2025. We hope this
report leads to similar events in which AI software tools are stress-tested to
assess the perimeter of their legal and ethical usage.

</details>


### [319] [From Model Design to Organizational Design: Complexity Redistribution and Trade-Offs in Generative AI](https://arxiv.org/abs/2506.22440)
*Sharique Hasan,Alexander Oettl,Sampsa Samila*

Key words: 大型语言模型, GAS框架, 竞争策略, 复杂性重新分配

TL;DR: 本文介绍了Generality-Accuracy-Simplicity (GAS)框架，分析大型语言模型(LLMs)如何重塑组织和竞争策略，强调AI复杂性从用户转向组织内部的管理挑战。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 传统的将AI视为简单输入成本降低的观点忽略了通用性、准确性和简单性之间的权衡，以及复杂性在利益相关者之间的重新分配。

Method: 通过GAS框架分析LLMs的特性，探讨复杂性如何从用户界面转移到基础设施、合规性和专业人员。

Result: LLMs的高通用性和准确性通过简单的界面实现，但复杂性转移到组织内部，重新定义了竞争优势的条件。

Conclusion: 竞争优势不再来自单纯的AI采用，而是通过抽象层设计、工作流对齐和互补性专业知识来管理重新分配的复杂性。

Abstract: This paper introduces the Generality-Accuracy-Simplicity (GAS) framework to
analyze how large language models (LLMs) are reshaping organizations and
competitive strategy. We argue that viewing AI as a simple reduction in input
costs overlooks two critical dynamics: (a) the inherent trade-offs among
generality, accuracy, and simplicity, and (b) the redistribution of complexity
across stakeholders. While LLMs appear to defy the traditional trade-off by
offering high generality and accuracy through simple interfaces, this
user-facing simplicity masks a significant shift of complexity to
infrastructure, compliance, and specialized personnel. The GAS trade-off,
therefore, does not disappear but is relocated from the user to the
organization, creating new managerial challenges, particularly around accuracy
in high-stakes applications. We contend that competitive advantage no longer
stems from mere AI adoption, but from mastering this redistributed complexity
through the design of abstraction layers, workflow alignment, and complementary
expertise. This study advances AI strategy by clarifying how scalable cognition
relocates complexity and redefines the conditions for technology integration.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [320] [Attention acts to suppress goal-based conflict under high competition](https://arxiv.org/abs/1610.09431)
*Omar Claflin*

Key words: 视觉注意力, 神经信号, 任务相关刺激, 自上而下调节

TL;DR: 在高度竞争条件下，自上而下的注意力会同时抑制任务相关和不相关的神经信号，以减少无关刺激的前馈信号。

<details>
  <summary>Details</summary>

Main category: q-bio.NC

Motivation: 研究在视觉注意力高度竞争条件下，自上而下注意力对神经信号的调节作用。

Method: 通过在共享感受野中呈现两个具有相反调制目标的刺激，测试自上而下注意力对神经信号的影响。

Result: 在刺激出现100毫秒内，自上而下注意力非选择性地抑制了任务相关和不相关的神经信号。

Conclusion: 非选择性抑制可能是减少无关刺激前馈信号的一种机制。

Abstract: It is known that when multiple stimuli are present, top-down attention
selectively enhances the neural signal in the visual cortex for task-relevant
stimuli, but this has been tested only under conditions of minimal competition
of visual attention. Here we show during high competition, that is, two stimuli
in a shared receptive field possessing opposing modulatory goals, top-down
attention suppresses both task-relevant and irrelevant neural signals within
100 ms of stimuli onset. This non-selective engagement of top-down attentional
resources serves to reduce the feedforward signal representing irrelevant
stimuli.

</details>


### [321] [Neural Langevin Machine: a local asymmetric learning rule can be creative](https://arxiv.org/abs/2506.23546)
*Zhendong Yu,Weizhong Huang,Haiping Huang*

Key words: 生成模型, Boltzmann-Gibbs测度, 神经Langevin动力学, 固定点, 生物可解释性

TL;DR: 该论文提出了一种基于Boltzmann-Gibbs测度和神经Langevin动力学的生成模型——神经Langevin机，用于存储和生成信息，具有解析分布形式、易于训练和生物可解释性等特点。

<details>
  <summary>Details</summary>

Main category: q-bio.NC

Motivation: 研究动机是利用递归神经网络的固定点存储和生成信息，同时开发一种解析性强、训练简便且具有生物可解释性的生成模型。

Method: 方法是通过Boltzmann-Gibbs测度捕捉固定点，结合神经Langevin动力学进行采样和学习，形成神经Langevin机。

Result: 结果显示该模型能够实现连续采样，模拟大脑的想象过程，并在电路采样和生物可解释的学习规则方面表现出潜力。

Conclusion: 结论是神经Langevin机作为一种生成模型具有独特优势，尤其是在电路采样和生物可解释性方面。

Abstract: Fixed points of recurrent neural networks can be leveraged to store and
generate information. These fixed points can be captured by the Boltzmann-Gibbs
measure, which leads to neural Langevin dynamics that can be used for sampling
and learning a real dataset. We call this type of generative model neural
Langevin machine, which is interpretable due to its analytic form of
distribution and is simple to train. Moreover, the learning process is derived
as a local asymmetric plasticity rule, bearing biological relevance. Therefore,
one can realize a continuous sampling of creative dynamics in a neural network,
mimicking an imagination process in brain circuits. This neural Langevin
machine may be another promising generative model, at least in its strength in
circuit-based sampling and biologically plausible learning rule.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [322] [VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding](https://arxiv.org/abs/2506.22799)
*Minchao Jiang,Shunyu Jia,Jiaming Gu,Xiaoyuan Lu,Guangming Zhu,Anqi Dong,Liang Zhang*

Key words: 3D Gaussian Splatting, Hough voting, VoteSplat, 3D scene understanding

TL;DR: VoteSplat结合Hough投票与3DGS，通过SAM生成2D投票图并嵌入空间偏移向量，实现低成本、高效的3D场景理解。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 现有3DGS方法主要关注几何和外观建模，缺乏深层次场景理解且训练成本高。

Method: 利用SAM进行实例分割生成2D投票图，嵌入空间偏移向量至高斯基元，结合深度失真约束优化定位。

Result: 实验证明VoteSplat在开放式词汇3D实例定位、点云理解等任务中高效且有效。

Conclusion: VoteSplat为3D场景理解提供了一种低成本、高精度的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time
rendering for novel view synthesis of 3D scenes. However, existing methods
focus primarily on geometric and appearance modeling, lacking deeper scene
understanding while also incurring high training costs that complicate the
originally streamlined differentiable rendering pipeline. To this end, we
propose VoteSplat, a novel 3D scene understanding framework that integrates
Hough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized
for instance segmentation, extracting objects, and generating 2D vote maps. We
then embed spatial offset vectors into Gaussian primitives. These offsets
construct 3D spatial votes by associating them with 2D image votes, while depth
distortion constraints refine localization along the depth axis. For
open-vocabulary object localization, VoteSplat maps 2D image semantics to 3D
point clouds via voting points, reducing training costs associated with
high-dimensional CLIP features while preserving semantic unambiguity. Extensive
experiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D
instance localization, 3D point cloud understanding, click-based 3D object
localization, hierarchical segmentation, and ablation studies. Our code is
available at https://sy-ja.github.io/votesplat/

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [323] [Knowledge-Guided Multi-Agent Framework for Automated Requirements Development: A Vision](https://arxiv.org/abs/2506.22656)
*Jiangping Huang,Dongming Jin,Weisong Sun,Yang Liu,Zhi Jin*

Key words: KGMAF, 多智能体系统, 自动化需求开发, SE, LLM

TL;DR: KGMAF是一个知识引导的多智能体框架，旨在自动化需求开发，填补现有SE自动化系统在需求任务中的不足。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 当前SE自动化系统过于关注代码开发，而忽视了需求任务的复杂性，KGMAF试图解决这一问题。

Method: KGMAF包含6个专用智能体和一个工件池，通过明确每个智能体的功能、动作和知识，以及设计工件池，提升效率和准确性。

Result: 案例研究表明KGMAF在现实场景中具有潜力。

Conclusion: KGMAF未来将在LLM时代的自动化需求开发中发挥重要作用。

Abstract: This paper envisions a knowledge-guided multi-agent framework named KGMAF for
automated requirements development. KGMAF aims to address gaps in current
automation systems for SE, which prioritize code development and overlook the
complexities of requirements tasks. KGMAF is composed of six specialized agents
and an artifact pool to improve efficiency and accuracy. Specifically, KGMAF
outlines the functionality, actions, and knowledge of each agent and provides
the conceptual design of the artifact pool. Our case study highlights the
potential of KGMAF in real-world scenarios. Finally, we outline several
research opportunities for implementing and enhancing automated requirements
development using multi-agent systems. We believe that KGMAF will play a
pivotal role in shaping the future of automated requirements development in the
era of LLMs.

</details>


### [324] [P4OMP: Retrieval-Augmented Prompting for OpenMP Parallelism in Serial Code](https://arxiv.org/abs/2506.22703)
*Wali Mohammad Abdullah,Azmain Kabir*

Key words: P4OMP, OpenMP, 检索增强生成, 大型语言模型, 代码转换

TL;DR: P4OMP是一个基于检索增强的框架，使用大型语言模型将串行C/C++代码转换为OpenMP注释的并行代码，显著提升了代码生成的可靠性和正确性。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 旨在解决直接使用大型语言模型生成的OpenMP代码在语法正确性和编译通过率上的问题，而无需进行模型微调或编译器插桩。

Method: 利用检索增强生成（RAG）技术，结合OpenMP教程中的结构化知识，提升提示驱动的代码生成质量。

Result: 在108个真实世界的C++程序基准测试中，P4OMP在所有可并行化的案例中实现了100%的编译成功率，而基线方法在20个案例中失败。

Conclusion: P4OMP提供了一个模块化且强大的管道，显著提高了LLM生成的OpenMP代码的可靠性和适用性。

Abstract: We present P4OMP, a retrieval-augmented framework for transforming serial
C/C++ code into OpenMP-annotated parallel code using large language models
(LLMs). To our knowledge, this is the first system to apply retrieval-based
prompting for OpenMP pragma correctness without model fine-tuning or compiler
instrumentation. P4OMP leverages Retrieval-Augmented Generation (RAG) with
structured instructional knowledge from OpenMP tutorials to improve the
reliability of prompt-driven code generation. By grounding generation in the
retrieved context, P4OMP improves syntactic correctness compared to baseline
prompting with GPT-3.5-Turbo. We evaluate P4OMP against a baseline,
GPT-3.5-Turbo without retrieval, on a comprehensive benchmark of 108 real-world
C++ programs drawn from Stack Overflow, PolyBench, and NAS benchmark suites.
P4OMP achieves 100% compilation success on all parallelizable cases, while the
baseline fails to compile in 20 out of 108 cases. Six cases that rely on
non-random-access iterators or thread-unsafe constructs are excluded due to
fundamental OpenMP limitations. A detailed analysis demonstrates how P4OMP
consistently avoids scoping errors, syntactic misuse, and invalid directive
combinations that commonly affect baseline-generated code. We further
demonstrate strong runtime scaling across seven compute-intensive benchmarks on
an HPC cluster. P4OMP offers a robust, modular pipeline that significantly
improves the reliability and applicability of LLM-generated OpenMP code.

</details>


### [325] [RAILS: Retrieval-Augmented Intelligence for Learning Software Development](https://arxiv.org/abs/2506.22742)
*Wali Mohammad Abdullah,Md. Morshedul Islam,Devraj Parmar,Happy Hasmukhbhai Patel,Sindhuja Prabhakaran,Baidya Saha*

Key words: 大型语言模型, 检索增强, Java开发, 编译器反馈, FAISS

TL;DR: 摘要介绍了一种名为RAILS的框架，通过检索增强的上下文和迭代验证，提升了LLM在Java开发中的代码补全能力。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现有LLM在软件开发辅助中常产生不完整或错误的代码，尤其是缺乏外部或项目特定文档支持时。

Method: RAILS框架利用FAISS和OpenAI嵌入从Java资源中检索语义上下文，并结合编译器反馈的迭代验证循环优化结果。

Result: 在78个实际Java导入错误案例中，RAILS表现优于基线方法，能保持意图一致性，避免幻觉并纠正导入问题。

Conclusion: RAILS展示了检索增强对LLM代码生成的改进潜力，未来将扩展到更多语言和工具。

Abstract: Large Language Models (LLMs) like GPT-3.5-Turbo are increasingly used to
assist software development, yet they often produce incomplete code or
incorrect imports, especially when lacking access to external or
project-specific documentation. We introduce RAILS (Retrieval-Augmented
Intelligence for Learning Software Development), a framework that augments LLM
prompts with semantically retrieved context from curated Java resources using
FAISS and OpenAI embeddings. RAILS incorporates an iterative validation loop
guided by compiler feedback to refine suggestions. We evaluated RAILS on 78
real-world Java import error cases spanning standard libraries, GUI APIs,
external tools, and custom utilities. Despite using the same LLM, RAILS
outperforms baseline prompting by preserving intent, avoiding hallucinations,
and surfacing correct imports even when libraries are unavailable locally.
Future work will integrate symbolic filtering via PostgreSQL and extend support
to other languages and IDEs.

</details>


### [326] [Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code Generation](https://arxiv.org/abs/2506.22776)
*Sen Fang,Weiyuan Ding,Antonio Mastropaolo,Bowen Xu*

Key words: 量化, 大型语言模型, 代码生成, 鲁棒性, 对抗攻击, 噪声扰动

TL;DR: 量化方法在压缩大型语言模型（LLM）时不仅减少内存需求，还能提高在代码生成任务中的鲁棒性。研究发现量化后的模型在对抗攻击和噪声扰动下表现更优。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 探索量化对LLM在代码生成任务中鲁棒性的影响，填补现有研究的空白。

Method: 通过对抗攻击和噪声扰动实验，评估四类LLM家族（LLaMA、DeepSeek、CodeGen、StarCoder）在量化前后的表现。

Result: 量化后的LLM在51.59%的对抗实验中表现更优，且能承受更高水平的权重扰动。

Conclusion: 量化不仅能降低计算需求，还能增强LLM在代码生成任务中的鲁棒性。

Abstract: Quantization has emerged as a mainstream method for compressing Large
Language Models (LLMs), reducing memory requirements and accelerating inference
without architectural modifications. While existing research primarily focuses
on evaluating the effectiveness of quantized LLMs compared to their original
counterparts, the impact on robustness remains largely unexplored.In this
paper, we present the first systematic investigation of how quantization
affects the robustness of LLMs in code generation tasks. Through extensive
experiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and
StarCoder) with parameter scales ranging from 350M to 33B, we evaluate
robustness from dual perspectives: adversarial attacks on input prompts and
noise perturbations on model architecture. Our findings challenge conventional
wisdom by demonstrating that quantized LLMs often exhibit superior robustness
compared to their full-precision counterparts, with 51.59% versus 42.86% of our
adversarial experiments showing better resilience in quantized LLMs. Similarly,
our noise perturbation experiments also confirm that LLMs after quantitation
generally withstand higher levels of weight disturbances. These results suggest
that quantization not only reduces computational requirements but can actually
enhance LLMs' reliability in code generation tasks, providing valuable insights
for developing more robust and efficient LLM deployment strategies.

</details>


### [327] [Generating Privacy Stories From Software Documentation](https://arxiv.org/abs/2506.23014)
*Wilder Baldwin,Shashank Chintakuntla,Shreyah Parajuli,Ali Pourghasemi,Ryan Shanz,Sepideh Ghanavati*

Key words: 隐私需求、大语言模型、链式思维提示、上下文学习、用户故事

TL;DR: 论文提出了一种基于链式思维提示（CoT）、上下文学习（ICL）和大语言模型（LLMs）的新方法，从软件开发文档中提取隐私行为并生成隐私需求，结果显示主流LLMs（如GPT-4o和Llama 3）效果显著（F1>0.8）。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 研究表明，开发人员常将隐私视为安全概念或事后考虑，导致隐私合规性问题。现有方法主要从法规中提取法律要求并评估合规性，但缺乏从开发文档中提取隐私行为的方法。

Method: 结合链式思维提示（CoT）、上下文学习（ICL）和LLMs，从软件开发文档中提取隐私行为并生成用户故事形式的隐私需求。

Result: 主流LLMs（如GPT-4o和Llama 3）能够有效识别隐私行为并生成隐私用户故事（F1>0.8），且参数调优可进一步提升性能。

Conclusion: 研究为利用和优化LLMs从开发文档中生成隐私需求提供了实践指导。

Abstract: Research shows that analysts and developers consider privacy as a security
concept or as an afterthought, which may lead to non-compliance and violation
of users' privacy. Most current approaches, however, focus on extracting legal
requirements from the regulations and evaluating the compliance of software and
processes with them. In this paper, we develop a novel approach based on
chain-of-thought prompting (CoT), in-context-learning (ICL), and Large Language
Models (LLMs) to extract privacy behaviors from various software documents
prior to and during software development, and then generate privacy
requirements in the format of user stories. Our results show that most commonly
used LLMs, such as GPT-4o and Llama 3, can identify privacy behaviors and
generate privacy user stories with F1 scores exceeding 0.8. We also show that
the performance of these models could be improved through parameter-tuning. Our
findings provide insight into using and optimizing LLMs for generating privacy
requirements given software documents created prior to or throughout the
software development lifecycle.

</details>


### [328] [QLPro: Automated Code Vulnerability Discovery via LLM and Static Code Analysis Integration](https://arxiv.org/abs/2506.23644)
*Junze Hu,Xiangyu Jin,Yizhe Zeng,Yuling Liu,Yunpeng Li,Dan Du,Kaiyu Xie,Hongsong Zhu*

Key words: QLPro, 漏洞检测, LLM, 静态分析, 开源项目

TL;DR: QLPro是一个结合LLM和静态分析工具的漏洞检测框架，在开源项目中表现出色。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 提高漏洞检测的覆盖率和准确性，尤其是在大型开源项目中。

Method: 系统地集成了LLM和静态分析工具（如CodeQL）。

Result: QLPro检测出41个漏洞，优于CodeQL的24个，并发现6个新漏洞（含2个0-day）。

Conclusion: QLPro是一种有效的漏洞检测框架，尤其适用于开源项目。

Abstract: We introduce QLPro, a vulnerability detection framework that systematically
integrates LLMs and static analysis tools to enable comprehensive vulnerability
detection across entire open-source projects.We constructed a new dataset,
JavaTest, comprising 10 open-source projects from GitHub with 62 confirmed
vulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only
24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro
discovered 6 previously unknown vulnerabilities, 2 of which have been confirmed
as 0-days.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [329] [GaussMaster: An LLM-based Database Copilot System](https://arxiv.org/abs/2506.23322)
*Wei Zhou,Ji Sun,Xuanhe Zhou,Guoliang Li,Luyang Liu,Hao Wu,Tianyuan Wang*

Key words: autonomous database, LLM, SQL tuning, anomaly detection, database maintenance

TL;DR: GaussMaster is an LLM-based database copilot system designed to automate comprehensive database maintenance, achieving zero human intervention in real-world scenarios.

<details>
  <summary>Details</summary>

Main category: cs.DB

Motivation: To address the limitations of existing autonomous database platforms by providing a more comprehensive and automated solution for database maintenance.

Method: LLM-based database copilot system using a Tree-of-thought approach to analyze metrics/logs and automate maintenance processes.

Result: Successful implementation in banking, achieving zero human intervention for 34+ database maintenance scenarios.

Conclusion: GaussMaster significantly improves autonomous database maintenance capabilities.

Abstract: In the financial industry, data is the lifeblood of operations, and DBAs
shoulder significant responsibilities for SQL tuning, database deployment,
diagnosis, and service repair. In recent years, both database vendors and
customers have increasingly turned to autonomous database platforms in an
effort to alleviate the heavy workload of DBAs. However, existing autonomous
database platforms are limited in their capabilities, primarily addressing
single-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual
intervention remains a necessity for comprehensive database maintenance.
GaussMaster aims to revolutionize this landscape by introducing an LLM-based
database copilot system. This innovative solution is designed not only to
assist developers in writing efficient SQL queries but also to provide
comprehensive care for database services. When database instances exhibit
abnormal behavior, GaussMaster is capable of orchestrating the entire
maintenance process automatically. It achieves this by analyzing hundreds of
metrics and logs, employing a Tree-of-thought approach to identify root causes,
and invoking appropriate tools to resolve issues. We have successfully
implemented GaussMaster in real-world scenarios, such as the banking industry,
where it has achieved zero human intervention for over 34 database maintenance
scenarios. In this paper, we present significant improvements in these tasks
with code at https://gitcode.com/opengauss/openGauss-GaussMaster.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [330] [ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction](https://arxiv.org/abs/2506.22498)
*Hao Liu,Yu Hu,Rakiba Rayhana,Ling Bai,Zheng Liu*

Key words: 床旁摔倒,负载传感器,多模态融合,Swin Transformer,时间序列分类

TL;DR: 论文提出了一种利用低成本传感器预测患者离床意图的方法，通过多模态图像融合技术实现高精度预测，优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 医院和养老院中床旁摔倒是主要伤害来源，现有警报系统通常在患者已离床后才触发，需更早预测离床意图以减少事故。

Method: 使用四个低成本负载传感器采集数据，转换为RGB线图和三种纹理图，并通过双流Swin Transformer模型进行多模态融合分类。

Result: 在真实养老院数据集中，ViFusionTST模型的准确率达0.885，F1分数0.794，优于现有时间序列分类方法。

Conclusion: 图像融合方法为实时、隐私保护的防摔倒提供了实用有效的解决方案。

Abstract: Bed-related falls remain a leading source of injury in hospitals and
long-term-care facilities, yet many commercial alarms trigger only after a
patient has already left the bed. We show that early bed-exit intent can be
predicted using only four low-cost load cells mounted under the bed legs. The
resulting load signals are first converted into a compact set of complementary
images: an RGB line plot that preserves raw waveforms and three texture maps -
recurrence plot, Markov transition field, and Gramian angular field - that
expose higher-order dynamics. We introduce ViFusionTST, a dual-stream Swin
Transformer that processes the line plot and texture maps in parallel and fuses
them through cross-attention to learn data-driven modality weights.
  To provide a realistic benchmark, we collected six months of continuous data
from 95 beds in a long-term-care facility. On this real-world dataset
ViFusionTST reaches an accuracy of 0.885 and an F1 score of 0.794, surpassing
recent 1D and 2D time-series baselines across F1, recall, accuracy, and AUPRC.
The results demonstrate that image-based fusion of load-sensor signals for time
series classification is a practical and effective solution for real-time,
privacy-preserving fall prevention.

</details>


### [331] [Scalable Dynamic Origin-Destination Demand Estimation Enhanced by High-Resolution Satellite Imagery Data](https://arxiv.org/abs/2506.22499)
*Jiachao Liu,Pablo Guarda,Koichiro Niinuma,Sean Qian*

Key words: 动态起讫点需求估计、多类宏观网络、卫星图像、计算机视觉、计算图模型

TL;DR: 本研究提出了一种新颖的多类宏观网络模型中动态起讫点需求估计（DODE）的集成框架，结合高分辨率卫星图像与传统交通数据，以克服数据限制并提升估计性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 利用卫星图像的全局性和一致性，弥补传统传感器数据的稀疏性，以更全面地估计交通需求。

Method: 设计了计算机视觉流程进行类特定车辆检测和地图匹配，并结合计算图模型校准动态网络状态。

Result: 实验表明，补充卫星数据显著提升了估计性能，尤其对无传感器的路段有效，且框架适用于大规模网络。

Conclusion: 该框架具有实际部署潜力，可适应不同规模城市的需求估计，并对卫星数据质量进行了敏感性分析。

Abstract: This study presents a novel integrated framework for dynamic
origin-destination demand estimation (DODE) in multi-class mesoscopic network
models, leveraging high-resolution satellite imagery together with conventional
traffic data from local sensors. Unlike sparse local detectors, satellite
imagery offers consistent, city-wide road and traffic information of both
parking and moving vehicles, overcoming data availability limitations. To
extract information from imagery data, we design a computer vision pipeline for
class-specific vehicle detection and map matching, generating link-level
traffic density observations by vehicle class. Building upon this information,
we formulate a computational graph-based DODE model that calibrates dynamic
network states by jointly matching observed traffic counts and travel times
from local sensors with density measurements derived from satellite imagery. To
assess the accuracy and scalability of the proposed framework, we conduct a
series of numerical experiments using both synthetic and real-world data. The
results of out-of-sample tests demonstrate that supplementing traditional data
with satellite-derived density significantly improves estimation performance,
especially for links without local sensors. Real-world experiments also confirm
the framework's capability to handle large-scale networks, supporting its
potential for practical deployment in cities of varying sizes. Sensitivity
analysis further evaluates the impact of data quality related to satellite
imagery data.

</details>


### [332] [Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models](https://arxiv.org/abs/2506.22500)
*Weiyi Zhao,Xiaoyu Tan,Liang Liu,Sijia Li,Youwei Song,Xihe Qiu*

Key words: 手术风险识别, 多模态大语言模型, 视觉-语义知识冲突, 合成数据集

TL;DR: 该论文提出了一种解决多模态大语言模型（MLLMs）在手术室风险检测中视觉-语义知识冲突（VS-KC）问题的方法，通过生成合成图像数据集OR-VSKC并对其进行微调，显著提升了模型的性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 通过解决MLLMs在手术室安全风险检测中的视觉-语义知识冲突问题，提高患者安全性和减少可预防的医疗错误。

Method: 使用扩散模型生成34,000张合成图像和214张人工标注图像作为数据集，并通过微调MLLMs来研究VS-KC问题。

Result: 微调显著提高了MLLMs在训练实体上的检测性能，但对未训练实体类型的检测性能较差。

Conclusion: OR-VSKC数据集为研究视觉-语义知识冲突提供了重要资源，但需要更全面的训练以提高模型的泛化能力。

Abstract: Surgical risk identification is critical for patient safety and reducing
preventable medical errors. While multimodal large language models (MLLMs) show
promise for automated operating room (OR) risk detection, they often exhibit
visual-semantic knowledge conflicts (VS-KC), failing to identify visual safety
violations despite understanding textual rules. To address this, we introduce a
dataset comprising over 34,000 synthetic images generated by diffusion models,
depicting operating room scenes containing entities that violate established
safety rules. These images were created to alleviate data scarcity and examine
MLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated
images that serve as a gold-standard reference for validation. This
comprehensive dataset, spanning diverse perspectives, stages, and
configurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC
significantly improves MLLMs' detection of trained conflict entities and
generalizes well to new viewpoints for these entities, but performance on
untrained entity types remains poor, highlighting learning specificity and the
need for comprehensive training. The main contributions of this work include:
(1) a data generation methodology tailored for rule-violation scenarios; (2)
the release of the OR-VSKC dataset and its associated benchmark as open-source
resources; and (3) an empirical analysis of violation-sensitive knowledge
consistency in representative MLLMs. The dataset and appendix are available at
https://github.com/zgg2577/VS-KC.

</details>


### [333] [How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?](https://arxiv.org/abs/2506.22501)
*Gautam Siddharth Kashyap,Manaswi Kulahara,Nipun Joshi,Usman Naseem*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Remote sensing datasets offer significant promise for tackling key
classification tasks such as land-use categorization, object presence
detection, and rural/urban classification. However, many existing studies tend
to focus on narrow tasks or datasets, which limits their ability to generalize
across various remote sensing classification challenges. To overcome this, we
propose a novel model, SpatialNet-ViT, leveraging the power of Vision
Transformers (ViTs) and Multi-Task Learning (MTL). This integrated approach
combines spatial awareness with contextual understanding, improving both
classification accuracy and scalability. Additionally, techniques like data
augmentation, transfer learning, and multi-task learning are employed to
enhance model robustness and its ability to generalize across diverse datasets

</details>


### [334] [FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment](https://arxiv.org/abs/2506.22509)
*Hang Xu,Jie Huang,Linjiang Huang,Dong Li,Yidi Liu,Feng Zhao*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Domain Adaptation(DA) for dense prediction tasks is an important topic, which
enhances the dense prediction model's performance when tested on its unseen
domain. Recently, with the development of Diffusion-based Dense Prediction
(DDP) models, the exploration of DA designs tailored to this framework is worth
exploring, since the diffusion model is effective in modeling the distribution
transformation that comprises domain information. In this work, we propose a
training-free mechanism for DDP frameworks, endowing them with DA capabilities.
Our motivation arises from the observation that the exposure bias (e.g., noise
statistics bias) in diffusion brings domain shift, and different domains in
conditions of DDP models can also be effectively captured by the noise
prediction statistics. Based on this, we propose a training-free Domain Noise
Alignment (DNA) approach, which alleviates the variations of noise statistics
to domain changes during the diffusion sampling process, thereby achieving
domain adaptation. Specifically, when the source domain is available, we
directly adopt the DNA method to achieve domain adaptation by aligning the
noise statistics of the target domain with those of the source domain. For the
more challenging source-free DA, inspired by the observation that regions
closer to the source domain exhibit higher confidence meeting variations of
sampling noise, we utilize the statistics from the high-confidence regions
progressively to guide the noise statistic adjustment during the sampling
process. Notably, our method demonstrates the effectiveness of enhancing the DA
capability of DDP models across four common dense prediction tasks. Code is
available at
\href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.

</details>


### [335] [Lightning the Night with Generative Artificial Intelligence](https://arxiv.org/abs/2506.22511)
*Tingting Zhou,Feng Zhang,Haoyang Fu,Baoxiang Pan,Renhe Zhang,Feng Lu,Zhixin Yang*

Key words: 可见光反射率, 生成扩散模型, 气象观测, 夜间反演, FY4B卫星

TL;DR: 该研究利用生成扩散模型从热红外数据中反演夜间可见光反射率，显著提升了气象观测的全天候能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决因夜间缺乏可见光而无法全天候进行气象观测的问题。

Method: 基于FY4B卫星的AGRI多波段热红外亮温数据，开发了高精度可见光反射率反演模型RefDiff。

Result: RefDiff的SSIM指数达0.90，尤其在复杂云结构区域表现优异，夜间反演能力通过VIIRS验证。

Conclusion: 研究显著提升了夜间可见光数据反演能力，扩展了其应用潜力。

Abstract: The visible light reflectance data from geostationary satellites is crucial
for meteorological observations and plays an important role in weather
monitoring and forecasting. However, due to the lack of visible light at night,
it is impossible to conduct continuous all-day weather observations using
visible light reflectance data. This study pioneers the use of generative
diffusion models to address this limitation. Based on the multi-band thermal
infrared brightness temperature data from the Advanced Geostationary Radiation
Imager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we
developed a high-precision visible light reflectance retrieval model, called
Reflectance Diffusion (RefDiff), which enables 0.47~\mu\mathrm{m},
0.65~\mu\mathrm{m}, and 0.825~\mu\mathrm{m} bands visible light reflectance
retrieval at night. Compared to the classical models, RefDiff not only
significantly improves accuracy through ensemble averaging but also provides
uncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90,
with particularly significant improvements in areas with complex cloud
structures and thick clouds. The model's nighttime retrieval capability was
validated using VIIRS nighttime product, demonstrating comparable performance
to its daytime counterpart. In summary, this research has made substantial
progress in the ability to retrieve visible light reflectance at night, with
the potential to expand the application of nighttime visible light data.

</details>


### [336] [PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection](https://arxiv.org/abs/2506.22783)
*Oguzhan Baser,Ahmet Ege Tanriverdi,Sriram Vishwanath,Sandeep P. Chinchali*

Key words: 深度伪造, PhonemeFake, 语音操纵, 检测模型, 语言推理

TL;DR: 论文提出了一种名为PhonemeFake（PF）的新型深度伪造攻击方法，通过语言推理操纵关键语音段，显著降低了人类感知和基准准确率，同时发布了易于使用的数据集和开源检测模型。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有深度伪造数据集未能真实模拟实际攻击对人类感知的影响，因此需要开发更现实的攻击向量以提高检测能力。

Method: 引入PhonemeFake（PF）攻击方法，利用语言推理操纵关键语音段，并开发了一种自适应优先计算被操纵区域的检测模型。

Result: PF攻击将人类感知降低42%，基准准确率降低94%；检测模型将EER减少91%，速度提升90%，且计算开销小。

Conclusion: PhonemeFake攻击和检测模型为深度伪造领域提供了更现实的攻击测试和高效的检测解决方案。

Abstract: Deepfake (DF) attacks pose a growing threat as generative models become
increasingly advanced. However, our study reveals that existing DF datasets
fail to deceive human perception, unlike real DF attacks that influence public
discourse. It highlights the need for more realistic DF attack vectors. We
introduce PhonemeFake (PF), a DF attack that manipulates critical speech
segments using language reasoning, significantly reducing human perception by
up to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF
dataset on HuggingFace and open-source bilevel DF segment detection model that
adaptively prioritizes compute on manipulated regions. Our extensive
experiments across three known DF datasets reveal that our detection model
reduces EER by 91% while achieving up to 90% speed-up, with minimal compute
overhead and precise localization beyond existing models as a scalable
solution.

</details>


### [337] [Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset](https://arxiv.org/abs/2506.22554)
*Vasu Agrawal,Akinniyi Akinyemi,Kathryn Alvero,Morteza Behrooz,Julia Buffalini,Fabio Maria Carlucci,Joy Chen,Junming Chen,Zhang Chen,Shiyang Cheng,Praveen Chowdary,Joe Chuang,Antony D'Avirro,Jon Daly,Ning Dong,Mark Duppenthaler,Cynthia Gao,Jeff Girard,Martin Gleize,Sahir Gomez,Hongyu Gong,Srivathsan Govindarajan,Brandon Han,Sen He,Denise Hernandez,Yordan Hristov,Rongjie Huang,Hirofumi Inaguma,Somya Jain,Raj Janardhan,Qingyao Jia,Christopher Klaiber,Dejan Kovachev,Moneish Kumar,Hang Li,Yilei Li,Pavel Litvin,Wei Liu,Guangyao Ma,Jing Ma,Martin Ma,Xutai Ma,Lucas Mantovani,Sagar Miglani,Sreyas Mohan,Louis-Philippe Morency,Evonne Ng,Kam-Woh Ng,Tu Anh Nguyen,Amia Oberai,Benjamin Peloquin,Juan Pino,Jovan Popovic,Omid Poursaeed,Fabian Prada,Alice Rakotoarison,Alexander Richard,Christophe Ropers,Safiyyah Saleem,Vasu Sharma,Alex Shcherbyna,Jia Shen,Jie Shen,Anastasis Stathopoulos,Anna Sun,Paden Tomasello,Tuan Tran,Arina Turkatenko,Bo Wan,Chao Wang,Jeff Wang,Mary Williamson,Carleigh Wood,Tao Xiang,Yilin Yang,Julien Yao,Chen Zhang,Jiemin Zhang,Xinyue Zhang,Jason Zheng,Pavlo Zhyzheria,Jan Zikes,Michael Zollhoefer*

Key words: 人机互动, 双向行为动态, 虚拟代理, AI技术, 多模态分析

TL;DR: 该论文提出了一种大规模互动数据集（Seamless Interaction Dataset），旨在开发能够理解和生成双向行为动态的AI技术，并展示了一系列相关模型的应用。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 为了开发具有社会智能的AI技术，需要能够理解和生成双向行为动态的模型。

Method: 通过收集和分析4000小时的双向互动数据，开发了一套能够生成符合人类语音的肢体和面部动作的模型。

Result: 模型能够输入语音和视觉行为，生成相应的动态反馈，并支持情感和表达级别的控制。

Conclusion: 研究展示了AI技术在改善人机互动方面的潜力，并为虚拟代理和多模态分析工具提供了新方向。

Abstract: Human communication involves a complex interplay of verbal and nonverbal
signals, essential for conveying meaning and achieving interpersonal goals. To
develop socially intelligent AI technologies, it is crucial to develop models
that can both comprehend and generate dyadic behavioral dynamics. To this end,
we introduce the Seamless Interaction Dataset, a large-scale collection of over
4,000 hours of face-to-face interaction footage from over 4,000 participants in
diverse contexts. This dataset enables the development of AI technologies that
understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,
telepresence experiences, and multimodal content analysis tools. We also
develop a suite of models that utilize the dataset to generate dyadic motion
gestures and facial expressions aligned with human speech. These models can
take as input both the speech and visual behavior of their interlocutors. We
present a variant with speech from an LLM model and integrations with 2D and 3D
rendering methods, bringing us closer to interactive virtual agents.
Additionally, we describe controllable variants of our motion models that can
adapt emotional responses and expressivity levels, as well as generating more
semantically-relevant gestures. Finally, we discuss methods for assessing the
quality of these dyadic motion models, which are demonstrating the potential
for more intuitive and responsive human-AI interactions.

</details>


### [338] [Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval](https://arxiv.org/abs/2506.22864)
*Li-Cheng Shen,Jih-Kang Hsieh,Wei-Hua Li,Chu-Song Chen*

Key words: 文本到图像检索, 指代表达式分割, 多模态大语言模型, 对象掩码, 区域级嵌入

TL;DR: 提出了一种结合文本到图像检索（TIR）和指代表达式分割（RES）的新任务MaTIR，并通过两阶段框架（分割感知检索和多模态大语言模型重排）提升了检索和分割性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的TIR方法缺乏可解释性，而RES在大规模图像集合中计算昂贵。MaTIR旨在统一两者，实现高效搜索和精确分割。

Method: 采用两阶段框架：1. 离线生成对象掩码和区域级嵌入（使用SAM 2和Alpha-CLIP）；2. 在线检索时使用多模态大语言模型重排和定位。

Result: 在COCO和D^3数据集上，检索精度和分割质量均显著优于现有方法。

Conclusion: MaTIR通过统一TIR和RES，实现了高效且精确的文本到图像检索与分割。

Abstract: Text-to-image retrieval (TIR) aims to find relevant images based on a textual
query, but existing approaches are primarily based on whole-image captions and
lack interpretability. Meanwhile, referring expression segmentation (RES)
enables precise object localization based on natural language descriptions but
is computationally expensive when applied across large image collections. To
bridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies
TIR and RES, requiring both efficient image search and accurate object
segmentation. To address this task, we propose a two-stage framework,
comprising a first stage for segmentation-aware image retrieval and a second
stage for reranking and object grounding with a multimodal large language model
(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract
region-level embeddings offline at first, enabling effective and scalable
online retrieval. Secondly, MLLM is used to refine retrieval rankings and
generate bounding boxes, which are matched to segmentation masks. We evaluate
our approach on COCO and D$^3$ datasets, demonstrating significant improvements
in both retrieval accuracy and segmentation quality over previous methods.

</details>


### [339] [Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation](https://arxiv.org/abs/2506.22567)
*Shansong Wang,Zhecheng Jin,Mingzhe Hu,Mojtaba Safari,Feng Zhao,Chih-Wei Chang,Richard LJ Qiu,Justin Roper,David S. Yu,Xiaofeng Yang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: CLIP models pretrained on natural images with billion-scale image-text pairs
have demonstrated impressive capabilities in zero-shot classification,
cross-modal retrieval, and open-ended visual answering. However, transferring
this success to biomedicine is hindered by the scarcity of large-scale
biomedical image-text corpora, the heterogeneity of image modalities, and
fragmented data standards across institutions. These limitations hinder the
development of a unified and generalizable biomedical foundation model trained
from scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical
foundation model developed via Multiple Medical CLIP Knowledge Distillation.
Rather than relying on billion-scale raw data, MMKD-CLIP distills knowledge
from nine state-of-the-art domain-specific or generalist biomedical CLIP
models, each pretrained on millions of biomedical image-text pairs. Our
two-stage training pipeline first performs CLIP-style pretraining on over 2.9
million biomedical image-text pairs from 26 image modalities, followed by
feature-level distillation using over 19.2 million feature pairs extracted from
teacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets,
encompassing over 10.8 million biomedical images across nine image modalities.
The evaluation spans six core task types: zero-shot classification, linear
probing, cross-modal retrieval, visual question answering, survival prediction,
and cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models
while demonstrating remarkable robustness and generalization across image
domains and task settings. These results underscore that multi-teacher
knowledge distillation is a scalable and effective paradigm for building
high-performing biomedical foundation models under the practical constraints of
real-world data availability.

</details>


### [340] [MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering](https://arxiv.org/abs/2506.22900)
*Mai A. Shaaban,Tausifa Jan Saleem,Vijay Ram Papineni,Mohammad Yaqub*

Key words: MedVQA、VLMs、多模态、重新排序、检索增强

TL;DR: 医学视觉问答(MedVQA)在临床决策中至关重要，但现有视觉语言模型(VLMs)常生成错误答案。MOTOR利用文本与视觉信息重新排序检索，显著提升回答准确性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法忽略视觉或多模态背景，导致临床决策中的信息不准确。

Method: 提出MOTOR，基于文本和视觉信息的多模态检索与重新排序方法。

Result: 实验与专家评估显示，MOTOR在MedVQA数据集上平均准确率提升6.45%。

Conclusion: MOTOR通过整合视觉与文本信息，显著提升临床问答准确性。

Abstract: Medical visual question answering (MedVQA) plays a vital role in clinical
decision-making by providing contextually rich answers to image-based queries.
Although vision-language models (VLMs) are widely used for this task, they
often generate factually incorrect answers. Retrieval-augmented generation
addresses this challenge by providing information from external sources, but
risks retrieving irrelevant context, which can degrade the reasoning
capabilities of VLMs. Re-ranking retrievals, as introduced in existing
approaches, enhances retrieval relevance by focusing on query-text alignment.
However, these approaches neglect the visual or multimodal context, which is
particularly crucial for medical diagnosis. We propose MOTOR, a novel
multimodal retrieval and re-ranking approach that leverages grounded captions
and optimal transport. It captures the underlying relationships between the
query and the retrieved context based on textual and visual information.
Consequently, our approach identifies more clinically relevant contexts to
augment the VLM input. Empirical analysis and human expert evaluation
demonstrate that MOTOR achieves higher accuracy on MedVQA datasets,
outperforming state-of-the-art methods by an average of 6.45%. Code is
available at https://github.com/BioMedIA-MBZUAI/MOTOR.

</details>


### [341] [MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings](https://arxiv.org/abs/2506.23115)
*Haonan Chen,Hong Liu,Yuping Luo,Liang Wang,Nan Yang,Furu Wei,Zhicheng Dou*

Key words: 多模态嵌入, 视觉语言模型, 双向注意力, 持续预训练, 对比学习

TL;DR: MoCa提出了一种两阶段框架，将预训练的因果视觉语言模型转化为高效的双向多模态嵌入模型，解决了现有方法的注意力机制不足、可扩展性问题和训练数据多样性限制。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前多模态嵌入模型基于因果注意力机制，对嵌入任务效果不佳，且依赖高质量标注数据和单一训练目标。MoCa旨在通过双向注意力机制和多样化数据提升模型表现。

Method: MoCa分为两阶段：1) 模态感知持续预训练，通过联合重建目标增强双向上下文推理；2) 异质对比微调，利用多样化多模态数据提升泛化和对齐能力。

Result: 在MMEB和ViDoRe-v2基准测试中，MoCa表现优异，创造了新的最优结果，并展示了在模型大小和训练数据上的强可扩展性。

Conclusion: MoCa通过持续预训练和多样化微调，显著提升了多模态嵌入模型的性能和鲁棒性，为未来的研究方向提供了新思路。

Abstract: Multimodal embedding models, built upon causal Vision Language Models (VLMs),
have shown promise in various tasks. However, current approaches face three key
limitations: the use of causal attention in VLM backbones is suboptimal for
embedding tasks; scalability issues due to reliance on high-quality labeled
paired data for contrastive learning; and limited diversity in training
objectives and data. To address these issues, we propose MoCa, a two-stage
framework for transforming pre-trained VLMs into effective bidirectional
multimodal embedding models. The first stage, Modality-aware Continual
Pre-training, introduces a joint reconstruction objective that simultaneously
denoises interleaved text and image inputs, enhancing bidirectional
context-aware reasoning. The second stage, Heterogeneous Contrastive
Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple
image-caption pairs to enhance generalization and alignment. Our method
addresses the stated limitations by introducing bidirectional attention through
continual pre-training, scaling effectively with massive unlabeled datasets via
joint reconstruction objectives, and utilizing diverse multimodal data for
enhanced representation robustness. Experiments demonstrate that MoCa
consistently improves performance across MMEB and ViDoRe-v2 benchmarks,
achieving new state-of-the-art results, and exhibits strong scalability with
both model size and training data on MMEB.

</details>


### [342] [UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding](https://arxiv.org/abs/2506.23219)
*Jie Feng,Shengyuan Wang,Tianhui Liu,Yanxin Xi,Yong Li*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Urban research involves a wide range of scenarios and tasks that require the
understanding of multi-modal data. Current methods often focus on specific data
types and lack a unified framework in urban field for processing them
comprehensively. The recent success of multi-modal large language models
(MLLMs) presents a promising opportunity to overcome this limitation. In this
paper, we introduce $\textit{UrbanLLaVA}$, a multi-modal large language model
designed to process these four types of data simultaneously and achieve strong
performance across diverse urban tasks compared with general MLLMs. In
$\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset
encompassing both single-modal and cross-modal urban data, spanning from
location view to global view of urban environment. Additionally, we propose a
multi-stage training framework that decouples spatial reasoning enhancement
from domain knowledge learning, thereby improving the compatibility and
downstream performance of $\textit{UrbanLLaVA}$ across diverse urban tasks.
Finally, we also extend existing benchmark for urban research to assess the
performance of MLLMs across a wide range of urban tasks. Experimental results
from three cities demonstrate that $\textit{UrbanLLaVA}$ outperforms
open-source and proprietary MLLMs in both single-modal tasks and complex
cross-modal tasks and shows robust generalization abilities across cities.
Source codes and data are openly accessible to the research community via
https://github.com/tsinghua-fib-lab/UrbanLLaVA.

</details>


### [343] [Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization](https://arxiv.org/abs/2506.23714)
*Md Moinul Islam,Sofoklis Kakouros,Janne Heikkilä,Mourad Oussalah*

Key words: 视频摘要, 多模态, 行为感知, 时间戳对齐, 奖励词

TL;DR: 该论文提出了一种行为感知的多模态视频摘要框架，整合文本、音频和视觉线索生成时间戳对齐的摘要，显著提升了传统方法的性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 视频内容的快速增长需要超越传统单模态方法的有效摘要技术。

Method: 通过提取音频、文本和视觉特征，结合多模态信息识别语义和情感重要时刻，并利用‘奖励词’提升摘要质量。

Result: 实验显示，该框架在文本和视频评价指标上均显著优于传统方法，如ROUGE-1从0.4769提升至0.7929，F1-Score提升近23%。

Conclusion: 多模态整合能生成更全面且行为感知的视频摘要。

Abstract: The increasing volume of video content in educational, professional, and
social domains necessitates effective summarization techniques that go beyond
traditional unimodal approaches. This paper proposes a behaviour-aware
multimodal video summarization framework that integrates textual, audio, and
visual cues to generate timestamp-aligned summaries. By extracting prosodic
features, textual cues and visual indicators, the framework identifies
semantically and emotionally important moments. A key contribution is the
identification of bonus words, which are terms emphasized across multiple
modalities and used to improve the semantic relevance and expressive clarity of
the summaries. The approach is evaluated against pseudo-ground truth (pGT)
summaries generated using LLM-based extractive method. Experimental results
demonstrate significant improvements over traditional extractive method, such
as the Edmundson method, in both text and video-based evaluation metrics.
Text-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore
from 0.9152 to 0.9536, while in video-based evaluation, our proposed framework
improves F1-Score by almost 23%. The findings underscore the potential of
multimodal integration in producing comprehensive and behaviourally informed
video summaries.

</details>


### [344] [Single-Frame Point-Pixel Registration via Supervised Cross-Modal Feature Matching](https://arxiv.org/abs/2506.22784)
*Yu Han,Zhiwei Huang,Yanting Zhang,Fangjun Ding,Shen Cai,Rui Fan*

Key words: 点像素配准、LiDAR、摄像机图像、跨模态匹配、检测器无关方法

TL;DR: 提出一种基于检测器无关的框架，直接匹配LiDAR点云与摄像机图像的点像素，克服了单帧LiDAR稀疏性和噪声的挑战。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决LiDAR点云与摄像机图像之间的模态差异，特别是在单帧LiDAR稀疏设置下，现有方法难以有效匹配的问题。

Method: 通过投影LiDAR强度图至2D视图，利用基于注意力的检测器无关匹配网络实现跨模态匹配，并引入可重复性评分机制增强可靠性。

Result: 在KITTI、nuScenes和MIAS-LCEC-TF70基准测试中达到最先进性能，优于依赖多帧点云积累的方法。

Conclusion: 提出的方法能够高效匹配LiDAR与摄像机视图的点像素，适用于单帧LiDAR的稀疏输入，具有较高的鲁棒性。

Abstract: Point-pixel registration between LiDAR point clouds and camera images is a
fundamental yet challenging task in autonomous driving and robotic perception.
A key difficulty lies in the modality gap between unstructured point clouds and
structured images, especially under sparse single-frame LiDAR settings.
Existing methods typically extract features separately from point clouds and
images, then rely on hand-crafted or learned matching strategies. This separate
encoding fails to bridge the modality gap effectively, and more critically,
these methods struggle with the sparsity and noise of single-frame LiDAR, often
requiring point cloud accumulation or additional priors to improve reliability.
Inspired by recent progress in detector-free matching paradigms (e.g.
MatchAnything), we revisit the projection-based approach and introduce the
detector-free framework for direct point-pixel matching between LiDAR and
camera views. Specifically, we project the LiDAR intensity map into a 2D view
from the LiDAR perspective and feed it into an attention-based detector-free
matching network, enabling cross-modal correspondence estimation without
relying on multi-frame accumulation. To further enhance matching reliability,
we introduce a repeatability scoring mechanism that acts as a soft visibility
prior. This guides the network to suppress unreliable matches in regions with
low intensity variation, improving robustness under sparse input. Extensive
experiments on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks demonstrate that
our method achieves state-of-the-art performance, outperforming prior
approaches on nuScenes (even those relying on accumulated point clouds),
despite using only single-frame LiDAR.

</details>


### [345] [Ella: Embodied Social Agents with Lifelong Memory](https://arxiv.org/abs/2506.24019)
*Hongxin Zhang,Zheyuan Zhang,Zeyuan Wang,Zunzhe Zhang,Lixing Fang,Qinhong Zhou,Chuang Gan*

Key words: Ella, 终身学习, 多模态记忆, 具身智能, 开放世界, 社交代理

TL;DR: Ella是一个能够在3D开放世界中终身学习的社交代理，它通过多模态记忆系统和基础模型实现知识积累与决策。实验显示Ella能有效地与其他代理互动和合作。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探索如何结合结构化记忆系统和基础模型，以提升具身智能体在开放世界中的终身学习和社交能力。

Method: Ella采用长期多模态记忆系统，包括语义记忆和情景记忆，并与基础模型结合，用于信息存储、更新和检索。实验在动态3D开放世界中进行，评估其社交和决策能力。

Result: Ella能够通过观察和社交互动有效学习，并在实验中展示出领导、影响和合作的能力。

Conclusion: 结合结构化记忆系统与基础模型，显著提升了具身智能体的终身学习和社交能力。

Abstract: We introduce Ella, an embodied social agent capable of lifelong learning
within a community in a 3D open world, where agents accumulate experiences and
acquire knowledge through everyday visual observations and social interactions.
At the core of Ella's capabilities is a structured, long-term multimodal memory
system that stores, updates, and retrieves information effectively. It consists
of a name-centric semantic memory for organizing acquired knowledge and a
spatiotemporal episodic memory for capturing multimodal experiences. By
integrating this lifelong memory system with foundation models, Ella retrieves
relevant information for decision-making, plans daily activities, builds social
relationships, and evolves autonomously while coexisting with other intelligent
beings in the open world. We conduct capability-oriented evaluations in a
dynamic 3D open world where 15 agents engage in social activities for days and
are assessed with a suite of unseen controlled evaluations. Experimental
results show that Ella can influence, lead, and cooperate with other agents
well to achieve goals, showcasing its ability to learn effectively through
observation and social interaction. Our findings highlight the transformative
potential of combining structured memory systems with foundation models for
advancing embodied intelligence. More videos can be found at
https://umass-embodied-agi.github.io/Ella/.

</details>


### [346] [MotionGPT3: Human Motion as a Second Modality](https://arxiv.org/abs/2506.24086)
*Bingfan Zhu,Biao Jiang,Sunyi Wang,Shixiang Tang,Tao Chen,Linjie Luo,Youyi Zheng,Xin Chen*

Key words: 运动-语言模型, 双模态, MotionGPT3, 变分自编码器, 扩散模型

TL;DR: 论文提出了一种名为MotionGPT3的双模态运动-语言模型，通过分离参数处理运动模态，解决了运动重建与语言智能退化问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 尽管多模态模型在统一理解和生成方面表现出强大能力，但运动-语言统一模型的开发尚未充分探索。本文旨在解决运动中连续模态与离散表示之间的重建差距，以及统一训练中语言智能的退化问题。

Method: 提出MotionGPT3，采用专家混合方法，通过共享注意力机制整合新的运动分支，保留语言分支的原始结构和参数。使用运动变分自编码器（VAE）编码运动数据，并通过扩散头直接预测运动潜在表示，避免离散化。

Result: 实验表明，该模型在运动理解和生成任务上表现优异，同时保持了语言能力，实现了自回归框架下的双模态运动扩散统一。

Conclusion: MotionGPT3成功解决了运动-语言统一模型的核心挑战，为运动与语言的联合建模提供了一个高效框架。

Abstract: Though recent advances in multimodal models have demonstrated strong
capabilities and opportunities in unified understanding and generation, the
development of unified motion-language models remains underexplored. To enable
such models with high-fidelity human motion, two core challenges must be
addressed. The first is the reconstruction gap between the continuous motion
modality and discrete representation in an autoregressive manner, and the
second is the degradation of language intelligence during unified training.
Inspired by the mixture of experts, we propose MotionGPT3, a bimodal
motion-language model that treats human motion as a second modality, decoupling
motion modeling via separate model parameters and enabling both effective
cross-modal interaction and efficient multimodal scaling training. To preserve
language intelligence, the text branch retains the original structure and
parameters of the pretrained language model, while a new motion branch is
integrated via a shared attention mechanism, enabling bidirectional information
flow between two modalities. We first employ a motion Variational Autoencoder
(VAE) to encode raw human motion into latent representations. Based on this
continuous latent space, the motion branch predicts motion latents directly
from intermediate hidden states using a diffusion head, bypassing discrete
tokenization. Extensive experiments show that our approach achieves competitive
performance on both motion understanding and generation tasks while preserving
strong language capabilities, establishing a unified bimodal motion diffusion
framework within an autoregressive manner.

</details>


### [347] [Listener-Rewarded Thinking in VLMs for Image Preferences](https://arxiv.org/abs/2506.22832)
*Alexander Gambashidze,Li Pengyi,Matvey Skripkin,Andrey Galichin,Anton Gusarov,Konstantin Sobolev,Andrey Kuznetsov,Ivan Oseledets*

Key words: 视觉偏好,奖励模型,GRPO,强化学习,推理对齐

TL;DR: 该论文通过引入监听器增强的GRPO框架，提升视觉偏好模型的泛化能力和推理准确性，减少与独立模型的矛盾。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前奖励模型泛化能力不足，且监督微调易导致记忆化问题，需改进对齐人类视觉偏好的方法。

Method: 提出监听器增强的GRPO框架，通过监听器重新评估推理过程并提供校准置信度，塑造RL奖励信号。

Result: 在ImageReward基准上达到67.4%的最高准确率，显着提升OOD性能，减少推理矛盾。

Conclusion: 监听器增强的奖励机制为对齐视觉语言模型与人类偏好提供了高效路径。

Abstract: Training robust and generalizable reward models for human visual preferences
is essential for aligning text-to-image and text-to-video generative models
with human intent. However, current reward models often fail to generalize, and
supervised fine-tuning leads to memorization, demanding complex annotation
pipelines. While reinforcement learning (RL), specifically Group Relative
Policy Optimization (GRPO), improves generalization, we uncover a key failure
mode: a significant drop in reasoning accuracy occurs when a model's reasoning
trace contradicts that of an independent, frozen vision-language model
("listener") evaluating the same output. To address this, we introduce a
listener-augmented GRPO framework. Here, the listener re-evaluates the
reasoner's chain-of-thought to provide a dense, calibrated confidence score,
shaping the RL reward signal. This encourages the reasoner not only to answer
correctly, but to produce explanations that are persuasive to an independent
model. Our listener-shaped reward scheme achieves best accuracy on the
ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)
performance on a large-scale human preference dataset (1.2M votes, up to +6%
over naive reasoner), and reduces reasoning contradictions compared to strong
GRPO and SFT baselines. These results demonstrate that listener-based rewards
provide a scalable, data-efficient path to aligning vision-language models with
nuanced human preferences. We will release our reasoning model here:
https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.

</details>


### [348] [Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization](https://arxiv.org/abs/2506.22463)
*Weizhi Gao,Zhichao Hou,Junqi Yin,Feiyi Wang,Linyu Peng,Xiaorui Liu*

Key words: 扩散模型,加速技术,MODiff,调制量化,误差补偿

TL;DR: 本文研究了扩散模型的高计算成本问题，提出了名为MoDiff的创新框架，通过调制量化和误差补偿加速生成建模，显著降低量化位数且保持性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 扩散模型的迭代采样计算成本高，现有加速技术如缓存和量化在计算误差和生成质量上存在局限。

Method: 提出MoDiff框架，结合调制量化和误差补偿，保留现有缓存和量化方法的优势，适用于所有扩散模型。

Result: 实验显示MoDiff在CIFAR-10和LSUN数据集上将激活量化从8位降至3位且性能无损。

Conclusion: MoDiff是一种高效且通用的扩散模型加速框架，理论支持充分，实践效果显著。

Abstract: Diffusion models have emerged as powerful generative models, but their high
computation cost in iterative sampling remains a significant bottleneck. In
this work, we present an in-depth and insightful study of state-of-the-art
acceleration techniques for diffusion models, including caching and
quantization, revealing their limitations in computation error and generation
quality. To break these limits, this work introduces Modulated Diffusion
(MoDiff), an innovative, rigorous, and principled framework that accelerates
generative modeling through modulated quantization and error compensation.
MoDiff not only inherents the advantages of existing caching and quantization
methods but also serves as a general framework to accelerate all diffusion
models. The advantages of MoDiff are supported by solid theoretical insight and
analysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate
that MoDiff significant reduces activation quantization from 8 bits to 3 bits
without performance degradation in post-training quantization (PTQ). Our code
implementation is available at https://github.com/WeizhiGao/MoDiff.

</details>


### [349] [Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception](https://arxiv.org/abs/2506.22866)
*Hang-Cheng Dong,Lu Zou,Bingguo Liu,Dong Ye,Guodong Liu*

Key words: 表面缺陷检测,弱监督学习,语义分割,类激活图,伪标签

TL;DR: 提出了一种弱监督语义分割框架，通过区域感知类激活图和伪标签训练，解决了缺陷检测任务中数据标注量大的问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 工业质检中的缺陷检测需要大量标注数据，而实际应用中难以满足，因此提出弱监督学习方法以减少对标注数据的依赖。

Method: 采用过滤引导反向传播（FGBP）和区域感知加权模块，优化类激活图的精度，并通过伪标签迭代提升模型性能。

Result: 在工业缺陷数据集上的实验表明，该方法显著提升了弱监督学习下的缺陷分割精度。

Conclusion: 该框架为资源受限的工业场景提供了一种高效的缺陷检测解决方案。

Abstract: Surface defect detection plays a critical role in industrial quality
inspection. Recent advances in artificial intelligence have significantly
enhanced the automation level of detection processes. However, conventional
semantic segmentation and object detection models heavily rely on large-scale
annotated datasets, which conflicts with the practical requirements of defect
detection tasks. This paper proposes a novel weakly supervised semantic
segmentation framework comprising two key components: a region-aware class
activation map (CAM) and pseudo-label training. To address the limitations of
existing CAM methods, especially low-resolution thermal maps, and insufficient
detail preservation, we introduce filtering-guided backpropagation (FGBP),
which refines target regions by filtering gradient magnitudes to identify areas
with higher relevance to defects. Building upon this, we further develop a
region-aware weighted module to enhance spatial precision. Finally,
pseudo-label segmentation is implemented to refine the model's performance
iteratively. Comprehensive experiments on industrial defect datasets
demonstrate the superiority of our method. The proposed framework effectively
bridges the gap between weakly supervised learning and high-precision defect
segmentation, offering a practical solution for resource-constrained industrial
scenarios.

</details>


### [350] [STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing](https://arxiv.org/abs/2506.22868)
*Junsung Lee,Junoh Kang,Bohyung Han*

Key words: 视频编辑,时空一致性,文本引导,T2V扩散模型,STR分数

TL;DR: 提出了一种名为STR-Match的无训练视频编辑算法，通过新的STR分数优化潜在空间，解决了现有文本引导视频编辑方法中的时间不一致性、运动失真和域变换有限的问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法在时间一致性和域变换能力上表现不足，主要原因是缺乏对时空像素相关性的有效建模。

Method: 利用2D空间注意力和1D时间模块在T2V扩散模型中捕捉时空像素相关性，结合潜在掩码的优化框架。

Result: 实验表明，STR-Match在视觉质量和时空一致性上均优于现有方法，尤其在显著域变换下保持一致性。

Conclusion: STR-Match通过高效捕捉时空相关性，显著提升了视频编辑的时空一致性和视觉质量。

Abstract: Previous text-guided video editing methods often suffer from temporal
inconsistency, motion distortion, and-most notably-limited domain
transformation. We attribute these limitations to insufficient modeling of
spatiotemporal pixel relevance during the editing process. To address this, we
propose STR-Match, a training-free video editing algorithm that produces
visually appealing and spatiotemporally coherent videos through latent
optimization guided by our novel STR score. The score captures spatiotemporal
pixel relevance across adjacent frames by leveraging 2D spatial attention and
1D temporal modules in text-to-video (T2V) diffusion models, without the
overhead of computationally expensive 3D attention mechanisms. Integrated into
a latent optimization framework with a latent mask, STR-Match generates
temporally consistent and visually faithful videos, maintaining strong
performance even under significant domain transformations while preserving key
visual attributes of the source. Extensive experiments demonstrate that
STR-Match consistently outperforms existing methods in both visual quality and
spatiotemporal consistency.

</details>


### [351] [Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and Grounder](https://arxiv.org/abs/2506.22880)
*Dang Jisheng,Wu Xudong,Wang Bimei,Lv Ning,Chen Jiayu,Jingwen Zhao,Yichu liu,Jizhao Liu,Juncheng Li,Teng Wang*

Key words: 视频分割,特征解耦,文本预训练,动态掩码融合

TL;DR: 本文提出DeSa2VA，通过解耦增强提示方案，结合文本预训练和线性解耦模块，解决了现有视频分割方法中动态视觉信息与静态语义的纠缠问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法（如Sa2VA）直接融合分割模型中的特征，导致动态视觉信息与静态语义纠缠，降低了分割精度。DeSa2VA旨在通过解耦技术系统性地解决这一问题。

Method: 1. 设计文本预训练范式，将文本真值标签转换为点级提示并生成相应文本掩码；2. 使用线性投影将大语言模型生成的隐藏状态解耦为文本和视觉特征子空间；3. 通过动态掩码融合策略结合解耦特征，并利用三重监督优化结果。

Result: 在图像分割、图像问答、视频分割和视频问答等多个任务中，DeSa2VA表现出最先进的性能。

Conclusion: DeSa2VA通过文本预训练和特征解耦显著提升了分割和问答任务的性能，证明了其有效性。

Abstract: Existing video segmenter and grounder approaches, exemplified by Sa2VA,
directly fuse features within segmentation models. This often results in an
undesirable entanglement of dynamic visual information and static semantics,
thereby degrading segmentation accuracy. To systematically mitigate this issue,
we propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text
pre-training and a linear decoupling module to address the information
processing limitations inherent in SAM-2. Specifically, first, we devise a
pre-training paradigm that converts textual ground-truth labels into
point-level prompts while generating corresponding text masks. These masks are
refined through a hybrid loss function to strengthen the model's semantic
grounding capabilities. Next, we employ linear projection to disentangle hidden
states that generated by a large language model into distinct textual and
visual feature subspaces. Finally, a dynamic mask fusion strategy
synergistically combines these decoupled features through triple supervision
from predicted text/visual masks and ground-truth annotations. Extensive
experiments demonstrate state-of-the-art performance across diverse tasks,
including image segmentation, image question answering, video segmentation, and
video question answering. Our codes are available at
https://github.com/longmalongma/DeSa2VA.

</details>


### [352] [What Makes a Dribble Successful? Insights From 3D Pose Tracking Data](https://arxiv.org/abs/2506.22503)
*Michiel Schepers,Pieter Robberechts,Jan Van Haaren,Jesse Davis*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Data analysis plays an increasingly important role in soccer, offering new
ways to evaluate individual and team performance. One specific application is
the evaluation of dribbles: one-on-one situations where an attacker attempts to
bypass a defender with the ball. While previous research has primarily relied
on 2D positional tracking data, this fails to capture aspects like balance,
orientation, and ball control, limiting the depth of current insights. This
study explores how pose tracking data (capturing players' posture and movement
in three dimensions) can improve our understanding of dribbling skills. We
extract novel pose-based features from 1,736 dribbles in the 2022/23 Champions
League season and evaluate their impact on dribble success. Our results
indicate that features capturing the attacker's balance and the alignment of
the orientation between the attacker and defender are informative for
predicting dribble success. Incorporating these pose-based features on top of
features derived from traditional 2D positional data leads to a measurable
improvement in model performance.

</details>


### [353] [Patch2Loc: Learning to Localize Patches for Unsupervised Brain Lesion Detection](https://arxiv.org/abs/2506.22504)
*Hassan Baker,Austin J. Brockmeier*

Key words: 脑部病变、MRI、无监督学习、Patch2Loc、异常检测

TL;DR: 提出了一种名为Patch2Loc的无监督学习方法，用于检测MRI中的异常脑组织，通过训练神经网络从正常脑组织中学习空间位置，预测异常区域。该方法在多个数据集上表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统的脑部病变检测需要标注数据，而无监督方法可以避免这一需求，提高诊断效率。

Method: 使用Patch2Loc方法，训练神经网络从正常脑组织的MRI图像中预测空间位置，异常区域通过预测误差和方差检测。

Result: 在BraTS2021、MSLUB、ATLAS和WMH数据集上的实验表明，该方法在无监督分割任务中表现优于现有技术。

Conclusion: Patch2Loc方法在无监督异常检测中具有潜力，可为计算机辅助诊断提供更高效的工具。

Abstract: Detecting brain lesions as abnormalities observed in magnetic resonance
imaging (MRI) is essential for diagnosis and treatment. In the search of
abnormalities, such as tumors and malformations, radiologists may benefit from
computer-aided diagnostics that use computer vision systems trained with
machine learning to segment normal tissue from abnormal brain tissue. While
supervised learning methods require annotated lesions, we propose a new
unsupervised approach (Patch2Loc) that learns from normal patches taken from
structural MRI. We train a neural network model to map a patch back to its
spatial location within a slice of the brain volume. During inference, abnormal
patches are detected by the relatively higher error and/or variance of the
location prediction. This generates a heatmap that can be integrated into
pixel-wise methods to achieve finer-grained segmentation. We demonstrate the
ability of our model to segment abnormal brain tissues by applying our approach
to the detection of tumor tissues in MRI on T2-weighted images from BraTS2021
and MSLUB datasets and T1-weighted images from ATLAS and WMH datasets. We show
that it outperforms the state-of-the art in unsupervised segmentation. The
codebase for this work can be found on our
\href{https://github.com/bakerhassan/Patch2Loc}{GitHub page}.

</details>


### [354] [Weakly Supervised Object Segmentation by Background Conditional Divergence](https://arxiv.org/abs/2506.22505)
*Hassan Baker,Matthew S. Emigh,Austin J. Brockmeier*

Key words: 弱监督学习, 对象分割, 反事实背景, 合成孔径声纳, 自然图像

TL;DR: 提出一种通过弱监督（仅图像级标签）训练掩码网络进行二元对象分割的方法，利用反事实背景图像增强对比学习，在合成孔径声纳和自然图像上验证有效性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决在缺乏大量标注数据的专业图像领域（如合成孔径声纳、生物医学成像等）中，自动对象分割的挑战。

Method: 训练掩码网络使用图像级标签（对象存在与否）进行弱监督学习，通过反事实背景图像对比优化损失函数，避免对抗性批评器。

Result: 在合成孔径声纳和自然图像上表现优于无监督分割基线，避免了预训练网络和生成网络。

Conclusion: 弱监督方法在专业领域和自然图像中均展现出良好的分割性能，具有通用性和实用性。

Abstract: As a computer vision task, automatic object segmentation remains challenging
in specialized image domains without massive labeled data, such as synthetic
aperture sonar images, remote sensing, biomedical imaging, etc. In any domain,
obtaining pixel-wise segmentation masks is expensive. In this work, we propose
a method for training a masking network to perform binary object segmentation
using weak supervision in the form of image-wise presence or absence of an
object of interest, which provides less information but may be obtained more
quickly from manual or automatic labeling. A key step in our method is that the
segmented objects can be placed into background-only images to create
realistic, images of the objects with counterfactual backgrounds. To create a
contrast between the original and counterfactual background images, we propose
to first cluster the background-only images, and then during learning create
counterfactual images that blend objects segmented from their original source
backgrounds to backgrounds chosen from a targeted cluster. One term in the
training loss is the divergence between these counterfactual images and the
real object images with backgrounds of the target cluster. The other term is a
supervised loss for background-only images. While an adversarial critic could
provide the divergence, we use sample-based divergences. We conduct experiments
on side-scan and synthetic aperture sonar in which our approach succeeds
compared to previous unsupervised segmentation baselines that were only tested
on natural images. Furthermore, to show generality we extend our experiments to
natural images, obtaining reasonable performance with our method that avoids
pretrained networks, generative networks, and adversarial critics. The basecode
for this work can be found at
\href{GitHub}{https://github.com/bakerhassan/WSOS}.

</details>


### [355] [VisionScores -- A system-segmented image score dataset for deep learning tasks](https://arxiv.org/abs/2506.23030)
*Alejandro Romero Amezcua,Mariano José Juan Rivera Meraz*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: VisionScores presents a novel proposal being the first system-segmented image
score dataset, aiming to offer structure-rich, high information-density images
for machine and deep learning tasks. Delimited to two-handed piano pieces, it
was built to consider not only certain graphic similarity but also composition
patterns, as this creative process is highly instrument-dependent. It provides
two scenarios in relation to composer and composition type. The first, formed
by 14k samples, considers works from different authors but the same composition
type, specifically, Sonatinas. The latter, consisting of 10.8K samples,
presents the opposite case, various composition types from the same author,
being the one selected Franz Liszt. All of the 24.8k samples are formatted as
grayscale jpg images of $128 \times 512$ pixels. VisionScores supplies the
users not only the formatted samples but the systems' order and pieces'
metadata. Moreover, unsegmented full-page scores and the pre-formatted images
are included for further analysis.

</details>


### [356] [Ovis-U1 Technical Report](https://arxiv.org/abs/2506.23044)
*Guo-Hua Wang,Shanshan Zhao,Xinjie Zhang,Liangfu Cao,Pengxin Zhan,Lunhao Duan,Shiyin Lu,Minghao Fu,Xiaohao Chen,Jianshan Zhao,Yang Li,Qing-Guo Chen*

Key words: Ovis-U1, 多模态模型, 统一训练, 图像生成, 图像编辑

TL;DR: Ovis-U1是一个30亿参数的多模态统一模型，集成了理解、文本到图像生成和图像编辑功能，通过统一训练方法实现性能提升，并在多项基准测试中超越现有模型。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 研究旨在开发一个统一的多模态模型，既能理解多模态内容，又能生成和编辑图像，以解决现有模型中理解和生成任务分离的局限性。

Method: 基于扩散模型的双向token细化器和统一训练方法，从语言模型出发，整合理解和生成任务。

Result: 在OpenCompass多模态学术基准中得分为69.6，文本到图像生成和图像编辑任务中表现优异，超越多个SOTA模型。

Conclusion: Ovis-U1作为初始版本，成功扩展了多模态理解、生成和编辑的边界，展示了统一训练方法的有效性。

Abstract: In this report, we introduce Ovis-U1, a 3-billion-parameter unified model
that integrates multimodal understanding, text-to-image generation, and image
editing capabilities. Building on the foundation of the Ovis series, Ovis-U1
incorporates a diffusion-based visual decoder paired with a bidirectional token
refiner, enabling image generation tasks comparable to leading models like
GPT-4o. Unlike some previous models that use a frozen MLLM for generation
tasks, Ovis-U1 utilizes a new unified training approach starting from a
language model. Compared to training solely on understanding or generation
tasks, unified training yields better performance, demonstrating the
enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score
of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent
state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In
text-to-image generation, it excels with scores of 83.72 and 0.89 on the
DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves
4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the
initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries
of multimodal understanding, generation, and editing.

</details>


### [357] [XTransfer: Cross-Modality Model Transfer for Human Sensing with Few Data at the Edge](https://arxiv.org/abs/2506.22726)
*Yu Zhang,Xi Zhang,Hualin zhou,Xinyuan Chen,Shang Gao,Hong Jia,Jianfei Yang,Yuankai Qi,Tao Gu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Deep learning for human sensing on edge systems offers significant
opportunities for smart applications. However, its training and development are
hindered by the limited availability of sensor data and resource constraints of
edge systems. Current methods that rely on transferring pre-trained models
often encounter issues such as modality shift and high resource demands,
resulting in substantial accuracy loss, resource overhead, and poor
adaptability across different sensing applications. In this paper, we propose
XTransfer, a first-of-its-kind method for resource-efficient, modality-agnostic
model transfer. XTransfer freely leverages single or multiple pre-trained
models and transfers knowledge across different modalities by (i) model
repairing that safely repairs modality shift in pre-trained model layers with
only few sensor data, and (ii) layer recombining that efficiently searches and
recombines layers of interest from source models in a layer-wise manner to
create compact models. We benchmark various baselines across diverse human
sensing datasets spanning different modalities. Comprehensive results
demonstrate that XTransfer achieves state-of-the-art performance on human
sensing tasks while significantly reducing the costs of sensor data collection,
model training, and edge deployment.

</details>


### [358] [Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding](https://arxiv.org/abs/2506.22803)
*Nuoye Xiong,Anqi Dong,Ning Wang,Cong Hua,Guangming Zhu,Mei Lin,Peiyi Shen,Liang Zhang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in deep learning have led to increasingly complex models with
deeper layers and more parameters, reducing interpretability and making their
decisions harder to understand. While many methods explain black-box reasoning,
most lack effective interventions or only operate at sample-level without
modifying the model itself. To address this, we propose the Concept Bottleneck
Model for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).
CBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable
framework to approximate black-box reasoning and communicate conceptual
understanding. Detrimental concepts are automatically identified and refined
(removed/replaced) based on global gradient contributions. The modified CBM
then distills corrected knowledge back into the black-box model, enhancing both
interpretability and accuracy. We evaluate CBM-HNMU on various CNN and
transformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,
and CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum
increase in average accuracy across 1.03%. Source code is available at:
https://github.com/XiGuaBo/CBM-HNMU.

</details>


### [359] [Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate](https://arxiv.org/abs/2506.22806)
*Byung Hyun Lee,Sungjin Lim,Seunggyu Lee,Dong Un Kang,Se Young Chun*

Key words: 概念擦除, 扩散模型, 非线性机制, 对抗训练, 注意力门

TL;DR: 论文提出了一种新框架CPE，通过非线性残差注意力门选择性擦除目标概念，同时保护其他概念，并提高了对抗攻击的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决现有扩散模型中仅微调交叉注意力层无法充分保护多样剩余概念的问题。

Method: 提出Concept Pinpoint Eraser (CPE)框架，使用非线性Residual Attention Gates (ResAGs)和注意力锚定损失，并迭代训练增强鲁棒性。

Result: 实验表明CPE在擦除名人、艺术风格和显式内容方面优于现有方法，并能保护多样剩余概念。

Conclusion: CPE通过非线性机制有效擦除目标概念且保持剩余概念，具备强鲁棒性。

Abstract: Remarkable progress in text-to-image diffusion models has brought a major
concern about potentially generating images on inappropriate or trademarked
concepts. Concept erasing has been investigated with the goals of deleting
target concepts in diffusion models while preserving other concepts with
minimal distortion. To achieve these goals, recent concept erasing methods
usually fine-tune the cross-attention layers of diffusion models. In this work,
we first show that merely updating the cross-attention layers in diffusion
models, which is mathematically equivalent to adding \emph{linear} modules to
weights, may not be able to preserve diverse remaining concepts. Then, we
propose a novel framework, dubbed Concept Pinpoint Eraser (CPE), by adding
\emph{nonlinear} Residual Attention Gates (ResAGs) that selectively erase (or
cut) target concepts while safeguarding remaining concepts from broad
distributions by employing an attention anchoring loss to prevent the
forgetting. Moreover, we adversarially train CPE with ResAG and learnable text
embeddings in an iterative manner to maximize erasing performance and enhance
robustness against adversarial attacks. Extensive experiments on the erasure of
celebrities, artistic styles, and explicit contents demonstrated that the
proposed CPE outperforms prior arts by keeping diverse remaining concepts while
deleting the target concepts with robustness against attack prompts. Code is
available at https://github.com/Hyun1A/CPE

</details>


### [360] [MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation](https://arxiv.org/abs/2506.23151)
*Vladislav Bargatin,Egor Chistov,Alexander Yakovenko,Dmitriy Vatolin*

Key words: 

TL;DR: MEMFOF是一种内存高效的多帧光流估计方法，显著降低了GPU内存使用，同时在高分辨率（FullHD）输入下保持高精度。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前光流估计方法在追求高精度的同时，GPU内存消耗剧增，尤其是在高分辨率输入下，限制了训练和应用效率。

Method: 通过重新设计RAFT-like架构，减少相关性体积并引入高分辨率训练协议，结合多帧估计，实现高效内存使用。

Result: MEMFOF在多个基准测试中表现出色，内存占用低（2.09GB运行时，28.5GB训练），同时在Spring、Sintel和KITTI-2015基准中排名第一。

Conclusion: MEMFOF在高分辨率光流估计中实现了内存与性能的平衡，优于资源密集的替代方案。

Abstract: Recent advances in optical flow estimation have prioritized accuracy at the
cost of growing GPU memory consumption, particularly for high-resolution
(FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical
flow method that identifies a favorable trade-off between multi-frame
estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU
memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely
positions our method to be trained at native 1080p without the need for
cropping or downsampling. We systematically revisit design choices from
RAFT-like architectures, integrating reduced correlation volumes and
high-resolution training protocols alongside multi-frame estimation, to achieve
state-of-the-art performance across multiple benchmarks while substantially
reducing memory overhead. Our method outperforms more resource-intensive
alternatives in both accuracy and runtime efficiency, validating its robustness
for flow estimation at high resolutions. At the time of submission, our method
ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289,
leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the
best Fl-all error on KITTI-2015 at 2.94%. The code is available at
https://github.com/msu-video-group/memfof.

</details>


### [361] [Prompting without Panic: Attribute-aware, Zero-shot, Test-Time Calibration](https://arxiv.org/abs/2506.22819)
*Ramya Hebbalaguppe,Tamoghno Kandar,Abhinav Nagpal,Chetan Arora*

Key words: 视觉语言模型, 测试时间提示调优, 校准误差, 正则化损失

TL;DR: 本文提出了一种名为TCA的方法，通过合理初始化测试时间提示并引入正则化损失，解决了测试时间提示调优（TPT）导致的视觉语言模型（VLM）校准问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有TPT方法虽然提高了VLM的准确率，但导致校准性能下降，限制了其在关键应用中的适用性。本文旨在解决这一问题。

Method: 提出两方面改进：(1) 利用大语言模型（LLM）初始化测试时间提示以避免过拟合；(2) 引入正则化损失以减少类内距离并增大类间距离。

Result: 实验表明，TCA显著降低了预期校准误差（ECE），平均值为4.11，优于现有方法（如vanilla TPT的11.7）。

Conclusion: TCA有效提升了TPT后VLM的校准性能，为关键应用提供了更可靠的模型。

Abstract: Vision-language models (VLM) have demonstrated impressive performance in
image recognition by leveraging self-supervised training on large datasets.
Their performance can be further improved by adapting to the test sample using
test-time prompt tuning (TPT). Unfortunately, the singular focus of TPT
approaches on improving the accuracy suffers from tunnel vision, and leads to
degradation in confidence calibration. This limits the applicability of TPT in
critical applications.
  We make three contributions in this work. (1) We posit that random or naive
initialization of prompts leads to overfitting on a particular test sample, and
is the main reason for miscalibration of the VLM after TPT. To mitigate the
problem, we propose careful initialization of test time prompt using prior
knowledge about the target label attributes from a large language model (LLM);
(2) To further maintain the quality of prompts during \tpt, we propose a novel
regularization loss to reduce intraclass distance, and increase inter-class
distance between the learnt
  Through extensive experiments on different CLIP architectures and 15
datasets, we show that our approach can effectively improve the calibration
after TPT. We report an average expected calibration error (ECE) of 4.11 with
our method, TCA, compared to 11.7 for vanilla TPT, 6.12 for C-TPT (ICLR'24),
6.78 for DiffTPT (CVPR'23), and 8.43 for PromptAlign (NeurIPS'23). The code is
publicly accessible at:
https://github.com/rhebbalaguppe/TCA_PromptWithoutPanic.

</details>


### [362] [Neural Cellular Automata: From Cells to Pixels](https://arxiv.org/abs/2506.22899)
*Ehsan Pajouheshgar,Yitao Xu,Ali Abbasi,Alexander Mordvintsev,Wenzel Jakob,Sabine Süsstrunk*

Key words: 神经细胞自动机, 高分辨率, 隐式解码器, 纹理合成, 形态生成

TL;DR: 该论文提出了一种结合神经细胞自动机（NCA）和共享隐式解码器的方法，解决了NCA在高分辨率网格上的训练和推理限制，实现了实时生成高清输出。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: NCA在纹理合成和形态生成中表现出色，但在高分辨率网格上受到训练时间、内存需求和信息传播范围的限制。论文旨在通过引入隐式解码器克服这些限制。

Method: 使用共享隐式解码器将NCA在低分辨率网格上的演化结果渲染为高分辨率输出，并设计了新的损失函数以减少计算开销。

Result: 该方法显著提升了NCA在高分辨率下的质量和效率，实现了实时生成高清输出，同时保持了自组织和涌现特性。

Conclusion: 结合隐式解码器和优化损失函数的NCA框架能够高效扩展到高分辨率任务，适用于多种变体和任务。

Abstract: Neural Cellular Automata (NCAs) are bio-inspired systems in which identical
cells self-organize to form complex and coherent patterns by repeatedly
applying simple local rules. NCAs display striking emergent behaviors including
self-regeneration, generalization and robustness to unseen situations, and
spontaneous motion. Despite their success in texture synthesis and
morphogenesis, NCAs remain largely confined to low-resolution grids. This
limitation stems from (1) training time and memory requirements that grow
quadratically with grid size, (2) the strictly local propagation of information
which impedes long-range cell communication, and (3) the heavy compute demands
of real-time inference at high resolution. In this work, we overcome this
limitation by pairing NCA with a tiny, shared implicit decoder, inspired by
recent advances in implicit neural representations. Following NCA evolution on
a coarse grid, a lightweight decoder renders output images at arbitrary
resolution. We also propose novel loss functions for both morphogenesis and
texture synthesis tasks, specifically tailored for high-resolution output with
minimal memory and computation overhead. Combining our proposed architecture
and loss functions brings substantial improvement in quality, efficiency, and
performance. NCAs equipped with our implicit decoder can generate full-HD
outputs in real time while preserving their self-organizing, emergent
properties. Moreover, because each MLP processes cell states independently,
inference remains highly parallelizable and efficient. We demonstrate the
applicability of our approach across multiple NCA variants (on 2D, 3D grids,
and 3D meshes) and multiple tasks, including texture generation and
morphogenesis (growing patterns from a seed), showing that with our proposed
framework, NCAs seamlessly scale to high-resolution outputs with minimal
computational overhead.

</details>


### [363] [Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data](https://arxiv.org/abs/2506.22939)
*Ghufran A. Omran,Wassan Saad Abduljabbar Hayale,Ahmad AbdulQadir AlRababah,Israa Ibraheem Al-Barazanchi,Ravi Sekhar,Pritesh Shah,Sushma Parihar,Harshavardhan Reddy Penubadi*

Key words: 场景分类, 遥感数据, CO-BRNN, 深度学习

TL;DR: 该论文提出了一种名为CO-BRNN的新方法，用于遥感数据场景分类，并展示了其高准确率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 遥感图像场景分类在多个领域有广泛应用，但传统深度学习方法需要大量数据且抗噪声能力差，因此需要一种更高效的解决方案。

Method: 提出了Cuttlefish Optimized Bidirectional Recurrent Neural Network (CO-BRNN)方法，并与多种现有技术进行了对比。

Result: CO-BRNN取得了97%的最高准确率，优于其他比较方法。

Conclusion: 物理验证对卫星数据效率至关重要，CO-BRNN表现优异。

Abstract: Scene categorization (SC) in remotely acquired images is an important subject
with broad consequences in different fields, including catastrophe control,
ecological observation, architecture for cities, and more. Nevertheless, its
several apps, reaching a high degree of accuracy in SC from distant observation
data has demonstrated to be difficult. This is because traditional conventional
deep learning models require large databases with high variety and high levels
of noise to capture important visual features. To address these problems, this
investigation file introduces an innovative technique referred to as the
Cuttlefish Optimized Bidirectional Recurrent Neural Network (CO- BRNN) for type
of scenes in remote sensing data. The investigation compares the execution of
CO-BRNN with current techniques, including Multilayer Perceptron- Convolutional
Neural Network (MLP-CNN), Convolutional Neural Network-Long Short Term Memory
(CNN-LSTM), and Long Short Term Memory-Conditional Random Field (LSTM-CRF),
Graph-Based (GB), Multilabel Image Retrieval Model (MIRM-CF), Convolutional
Neural Networks Data Augmentation (CNN-DA). The results demonstrate that
CO-BRNN attained the maximum accuracy of 97%, followed by LSTM-CRF with 90%,
MLP-CNN with 85%, and CNN-LSTM with 80%. The study highlights the significance
of physical confirmation to ensure the efficiency of satellite data.

</details>


### [364] [VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions](https://arxiv.org/abs/2506.23236)
*Marko Mihajlovic,Siwei Zhang,Gen Li,Kaifeng Zhao,Lea Müller,Siyu Tang*

Key words: 神经体积模型、Neural Blend Weights、人体重建、计算效率

TL;DR: 论文提出了一种名为VolumetricSMPL的神经体积人体模型，通过动态混合学习权重显著提升了计算效率，并在多个任务中展现出优越性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统的人体模型在处理与其他几何实体（如物体和场景）的交互时效率低下，现有的神经体积模型要么鲁棒性不足，要么计算和内存成本过高。

Method: 采用Neural Blend Weights（NBW）技术，动态混合少量学习到的权重矩阵，生成紧凑且高效的MLP解码器，显著降低计算和内存开销。

Result: VolumetricSMPL在推理速度、GPU内存占用和准确性上均优于现有模型COAP，并在人体-物体交互重建、场景约束运动合成等任务中表现出色。

Conclusion: VolumetricSMPL在性能和效率上均有显著提升，适用于广泛的应用场景。

Abstract: Parametric human body models play a crucial role in computer graphics and
vision, enabling applications ranging from human motion analysis to
understanding human-environment interactions. Traditionally, these models use
surface meshes, which pose challenges in efficiently handling interactions with
other geometric entities, such as objects and scenes, typically represented as
meshes or point clouds. To address this limitation, recent research has
explored volumetric neural implicit body models. However, existing works are
either insufficiently robust for complex human articulations or impose high
computational and memory costs, limiting their widespread use. To this end, we
introduce VolumetricSMPL, a neural volumetric body model that leverages Neural
Blend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike
prior approaches that rely on large MLPs, NBW dynamically blends a small set of
learned weight matrices using predicted shape- and pose-dependent coefficients,
significantly improving computational efficiency while preserving
expressiveness. VolumetricSMPL outperforms prior volumetric occupancy model
COAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy,
and a Signed Distance Function (SDF) for efficient and differentiable contact
modeling. We demonstrate VolumetricSMPL's strengths across four challenging
tasks: (1) reconstructing human-object interactions from in-the-wild images,
(2) recovering human meshes in 3D scenes from egocentric views, (3)
scene-constrained motion synthesis, and (4) resolving self-intersections. Our
results highlight its broad applicability and significant performance and
efficiency gains.

</details>


### [365] [Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification](https://arxiv.org/abs/2506.23247)
*James Hinns,David Martens*

Key words: 深度学习, 图像分类, 显著性图, Segment Attribution Tables (SATs), 模型解释

TL;DR: 论文提出Segment Attribution Tables (SATs)，用于将局部显著性解释汇总为（半）全局洞察，以分析和调试图像分类器。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前深度学习在图像分类任务中表现优异，但模型预测的解释仍具挑战性。局部解释方法（如显著性图）只能提供单个预测的细节，而全局方法又过于简化，无法捕捉重要的局部行为。

Method: 提出了Segment Attribution Tables (SATs)，通过图像片段（如“眼睛”）和显著性图量化其影响，汇总局部显著性解释为全局洞察。

Result: SATs能够揭示模型依赖的概念（如背景或水印的虚假相关性），适用于任何可生成显著性图的分类器。

Conclusion: SATs填补了过于简化的全局总结和过于详细的局部解释之间的空白，为分析和调试图像分类器提供了实用工具。

Abstract: Deep learning dominates image classification tasks, yet understanding how
models arrive at predictions remains a challenge. Much research focuses on
local explanations of individual predictions, such as saliency maps, which
visualise the influence of specific pixels on a model's prediction. However,
reviewing many of these explanations to identify recurring patterns is
infeasible, while global methods often oversimplify and miss important local
behaviours. To address this, we propose Segment Attribution Tables (SATs), a
method for summarising local saliency explanations into (semi-)global insights.
SATs take image segments (such as "eyes" in Chihuahuas) and leverage saliency
maps to quantify their influence. These segments highlight concepts the model
relies on across instances and reveal spurious correlations, such as reliance
on backgrounds or watermarks, even when out-of-distribution test performance
sees little change. SATs can explain any classifier for which a form of
saliency map can be produced, using segmentation maps that provide named
segments. SATs bridge the gap between oversimplified global summaries and
overly detailed local explanations, offering a practical tool for analysing and
debugging image classifiers.

</details>


### [366] [PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution](https://arxiv.org/abs/2506.23254)
*Aradhana Mishra,Bumshik Lee*

Key words: 图像超分辨率、扩散模型、PixelBoost、布朗运动、随机性、边缘重建

TL;DR: 论文提出了一种名为PixelBoost的扩散模型，通过整合布朗运动的随机性提升图像超分辨率的质量和效率，显著改善了纹理和边缘定义的真实性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决扩散模型在图像超分辨率中真实性与计算效率之间的权衡问题，特别是在减少采样步长时导致图像模糊的问题。

Method: 引入布朗运动的随机性到训练中，避免局部最优，开发了Sigmoidal噪声排序方法以简化训练。

Result: 在LPIPS、LOE、PSNR、SSIM等指标上表现优越，边缘重建能力更强，同时实现了更快的推理速度。

Conclusion: PixelBoost通过整合随机性和改进噪声排序方法，显著提升了图像超分辨率的真实性和效率。

Abstract: Diffusion-model-based image super-resolution techniques often face a
trade-off between realistic image generation and computational efficiency. This
issue is exacerbated when inference times by decreasing sampling steps,
resulting in less realistic and hazy images. To overcome this challenge, we
introduce a novel diffusion model named PixelBoost that underscores the
significance of embracing the stochastic nature of Brownian motion in advancing
image super-resolution, resulting in a high degree of realism, particularly
focusing on texture and edge definitions. By integrating controlled
stochasticity into the training regimen, our proposed model avoids convergence
to local optima, effectively capturing and reproducing the inherent uncertainty
of image textures and patterns. Our proposed model demonstrates superior
objective results in terms of learned perceptual image patch similarity
(LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR),
structural similarity index measure (SSIM), as well as visual quality. To
determine the edge enhancement, we evaluated the gradient magnitude and pixel
value, and our proposed model exhibited a better edge reconstruction
capability. Additionally, our model demonstrates adaptive learning capabilities
by effectively adjusting to Brownian noise patterns and introduces a sigmoidal
noise sequencing method that simplifies training, resulting in faster inference
speeds.

</details>


### [367] [ActAlign: Zero-Shot Fine-Grained Video Classification via Language-Guided Sequence Alignment](https://arxiv.org/abs/2506.22967)
*Amir Aghdam,Vincent Tao Hu*

Key words: 零样本学习, 细粒度视频分类, 序列对齐, 动态时间规整, 视觉语言模型

TL;DR: ActAlign是一个零样本细粒度视频分类框架，通过序列对齐和语言模型生成子动作序列，显著提升分类性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决零样本细粒度视频分类问题，无需视频示例或时间标注，利用语言模型和序列对齐方法弥补现有视觉语言模型在时间结构捕捉上的不足。

Method: 使用大型语言模型生成有序子动作序列，并通过动态时间规整（DTW）在共享嵌入空间中对齐视频帧与子动作序列。

Result: 在ActionAtlas基准上达到30.5%的准确率，超越数十亿参数视频语言模型且参数占用减少8倍。

Conclusion: 结构化语言先验与经典对齐技术结合，为视觉语言模型在细粒度视频理解中的开放集识别提供了可扩展且通用的解决方案。

Abstract: We address the task of zero-shot fine-grained video classification, where no
video examples or temporal annotations are available for unseen action classes.
While contrastive vision-language models such as SigLIP demonstrate strong
open-set recognition via mean-pooled image-text similarity, they fail to
capture the temporal structure critical for distinguishing fine-grained
activities. We introduce ActAlign, a zero-shot framework that formulates video
classification as sequence alignment. For each class, a large language model
generates an ordered sub-action sequence, which is aligned with video frames
using Dynamic Time Warping (DTW) in a shared embedding space. Without any
video-text supervision or fine-tuning, ActAlign achieves 30.5% accuracy on the
extremely challenging ActionAtlas benchmark, where human accuracy is only
61.6%. ActAlign outperforms billion-parameter video-language models while using
approximately 8x less parameters. These results demonstrate that structured
language priors, combined with classical alignment techniques, offer a scalable
and general approach to unlocking the open-set recognition potential of
vision-language models for fine-grained video understanding.

</details>


### [368] [Token Activation Map to Visually Explain Multimodal LLMs](https://arxiv.org/abs/2506.23270)
*Yi Li,Hualiang Wang,Xinpeng Ding,Haonan Wang,Xiaomeng Li*

Key words: 多模态大型语言模型,可解释性,Token Activation Map,因果推断,高斯滤波

TL;DR: 论文提出了一种名为Token Activation Map (TAM)的方法，用于提高多模态大型语言模型(MLLMs)的可解释性，通过估计因果推断减少上下文干扰，并利用高斯滤波降低激活噪声。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: MLLMs的可解释性研究较少，现有方法忽视了上下文冗余激活对解释可靠性的负面影响。

Method: 提出TAM方法，结合估计因果推断和秩高斯滤波，以减少上下文干扰和激活噪声。

Result: TAM在多种场景中表现优于现有方法，提供高质量的可视化结果。

Conclusion: TAM有效提升了MLLMs的解释能力，适用于多种应用场景。

Abstract: Multimodal large language models (MLLMs) are broadly empowering various
fields. Despite their advancements, the explainability of MLLMs remains less
explored, hindering deeper understanding, model credibility, and effective
visualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that
produce a single output, MLLMs generate sequences of tokens progressively,
where each generated token depends on the previous context. Therefore, earlier
context tokens can introduce redundant activations that interfere with the
explanation of later tokens beyond their original information. Existing studies
often overlook this issue, but our observations reveal that these redundant
correlations can significantly hurt the reliability of explanations. To address
this, we propose an estimated causal inference method to mitigate the
interference of context to achieve high-quality MLLM explanation, with a novel
rank Gaussian filter to further reduce activation noises. We term this method
Token Activation Map (TAM) to highlight the consideration of interactions
between tokens. TAM also indicates that it excels at explaining multiple tokens
of MLLM, which is different from the Class Activation Map (CAM) for a single
prediction. Our TAM method significantly outperforms existing SoTA methods,
showcasing high-quality visualization results that can be utilized for various
scenarios, such as object localization, failure case analysis, video
visualization, MLLMs visual comparison, and model understanding (e.g., color,
shape, action, location, visual reasoning, multi-turn conversation, etc). The
code is available atgithub.com/xmed-lab/TAM.

</details>


### [369] [Why Settle for One? Text-to-ImageSet Generation and Evaluation](https://arxiv.org/abs/2506.23275)
*Chengyou Jia,Xin Shen,Zhuohang Dang,Zhuohang Dang,Changliang Xia,Weijia Wu,Xinyu Zhang,Hangwei Qian,Ivor W. Tsang,Minnan Luo*

Key words: 文本到图像集；一致性生成；评估框架；扩散变换器

TL;DR: 提出文本到图像集（T2IS）生成问题，解决现有方法泛化性不足的局限。通过T2IS-Bench和T2IS-Eval进行系统性研究，并开发训练免费框架AutoT2IS，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法在生成一致图像集时局限于特定领域和一致性要求，难以满足多样化应用需求。

Method: 引入T2IS-Bench数据集和T2IS-Eval评估框架，并提出基于预训练扩散变换器的AutoT2IS框架。

Result: AutoT2IS在多种一致性挑战中显著优于通用和专用方法，并展示实际应用潜力。

Conclusion: AutoT2IS为T2IS生成提供了高效解决方案，具有广泛实用价值。

Abstract: Despite remarkable progress in Text-to-Image models, many real-world
applications require generating coherent image sets with diverse consistency
requirements. Existing consistent methods often focus on a specific domain with
specific aspects of consistency, which significantly constrains their
generalizability to broader applications. In this paper, we propose a more
challenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate
sets of images that meet various consistency requirements based on user
instructions. To systematically study this problem, we first introduce
$\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories,
providing comprehensive coverage for T2IS generation. Building on this, we
propose $\textbf{T2IS-Eval}$, an evaluation framework that transforms user
instructions into multifaceted assessment criteria and employs effective
evaluators to adaptively assess consistency fulfillment between criteria and
generated sets. Subsequently, we propose $\textbf{AutoT2IS}$, a training-free
framework that maximally leverages pretrained Diffusion Transformers'
in-context capabilities to harmonize visual elements to satisfy both
image-level prompt alignment and set-level visual consistency. Extensive
experiments on T2IS-Bench reveal that diverse consistency challenges all
existing methods, while our AutoT2IS significantly outperforms current
generalized and even specialized approaches. Our method also demonstrates the
ability to enable numerous underexplored real-world applications, confirming
its substantial practical value. Visit our project in
https://chengyou-jia.github.io/T2IS-Home.

</details>


### [370] [Learning Counterfactually Decoupled Attention for Open-World Model Attribution](https://arxiv.org/abs/2506.23074)
*Yu Zheng,Boyang Gong,Fanye Kong,Yueqi Duan,Bingyao Yu,Wenzhao Zheng,Lei Chen,Jiwen Lu,Jie Zhou*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we propose a Counterfactually Decoupled Attention Learning
(CDAL) method for open-world model attribution. Existing methods rely on
handcrafted design of region partitioning or feature space, which could be
confounded by the spurious statistical correlations and struggle with novel
attacks in open-world scenarios. To address this, CDAL explicitly models the
causal relationships between the attentional visual traces and source model
attribution, and counterfactually decouples the discriminative model-specific
artifacts from confounding source biases for comparison. In this way, the
resulting causal effect provides a quantification on the quality of learned
attention maps, thus encouraging the network to capture essential generation
patterns that generalize to unseen source models by maximizing the effect.
Extensive experiments on existing open-world model attribution benchmarks show
that with minimal computational overhead, our method consistently improves
state-of-the-art models by large margins, particularly for unseen novel
attacks. Source code: https://github.com/yzheng97/CDAL.

</details>


### [371] [SIEDD: Shared-Implicit Encoder with Discrete Decoders](https://arxiv.org/abs/2506.23382)
*Vikram Rangarajan,Shishira Maiya,Max Ehrlich,Abhinav Shrivastava*

Key words: 隐式神经表示, 视频压缩, 编码加速, SIEDD, INR

TL;DR: SIEDD是一种新型的隐式神经表示（INR）视频压缩架构，显著加速编码速度，保持重建质量和压缩比，同时支持连续分辨率解码。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决现有INR视频压缩编码速度慢的问题，同时避免牺牲重建质量或坐标级控制能力。

Method: 采用共享坐标编码器和离散解码器的协同设计，快速训练全局低频特征，并行训练轻量解码器。

Result: 在HD和4K基准测试中，编码速度提升20-30倍，保持竞争性重建质量和压缩比。

Conclusion: SIEDD显著提升了高保真神经视频压缩的实用性，为实际部署提供高效路径。

Abstract: Implicit Neural Representations (INRs) offer exceptional fidelity for video
compression by learning per-video optimized functions, but their adoption is
crippled by impractically slow encoding times. Existing attempts to accelerate
INR encoding often sacrifice reconstruction quality or crucial coordinate-level
control essential for adaptive streaming and transcoding. We introduce SIEDD
(Shared-Implicit Encoder with Discrete Decoders), a novel architecture that
fundamentally accelerates INR encoding without these compromises. SIEDD first
rapidly trains a shared, coordinate-based encoder on sparse anchor frames to
efficiently capture global, low-frequency video features. This encoder is then
frozen, enabling massively parallel training of lightweight, discrete decoders
for individual frame groups, further expedited by aggressive coordinate-space
sampling. This synergistic design delivers a remarkable 20-30X encoding
speed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while
maintaining competitive reconstruction quality and compression ratios.
Critically, SIEDD retains full coordinate-based control, enabling continuous
resolution decoding and eliminating costly transcoding. Our approach
significantly advances the practicality of high-fidelity neural video
compression, demonstrating a scalable and efficient path towards real-world
deployment. Our codebase is available at
https://github.com/VikramRangarajan/SIEDD .

</details>


### [372] [Detecting What Matters: A Novel Approach for Out-of-Distribution 3D Object Detection in Autonomous Vehicles](https://arxiv.org/abs/2506.23426)
*Menna Taha,Aya Ahmed,Mohammed Karmoose,Yasser Gadallah*

Key words: 自动驾驶汽车, 目标检测, 分布外物体, 危害性评估, 实时决策

TL;DR: 论文提出一种新的目标检测方法，通过判断物体对自动驾驶汽车的危害性（而非传统基于类别的分类），提升其对分布外（OOD）物体的检测能力，从而增强安全性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统目标检测方法仅能识别已知类别物体，无法有效应对OOD物体，可能导致自动驾驶汽车误判并引发危险。研究旨在解决这一安全问题。

Method: 将目标检测从基于类别的分类转向基于物体危害性的判断（'有害'或'无害'），通过物体相对于自动驾驶汽车的位置和轨迹进行评估。

Result: 模型能有效检测OOD物体并评估其危害性，从而提升自动驾驶汽车在动态环境中的决策能力。

Conclusion: 基于危害性的目标检测方法比传统分类更适用于自动驾驶场景，显著提高了对未知物体的响应安全性。

Abstract: Autonomous vehicles (AVs) use object detection models to recognize their
surroundings and make driving decisions accordingly. Conventional object
detection approaches classify objects into known classes, which limits the AV's
ability to detect and appropriately respond to Out-of-Distribution (OOD)
objects. This problem is a significant safety concern since the AV may fail to
detect objects or misclassify them, which can potentially lead to hazardous
situations such as accidents. Consequently, we propose a novel object detection
approach that shifts the emphasis from conventional class-based classification
to object harmfulness determination. Instead of object detection by their
specific class, our method identifies them as either 'harmful' or 'harmless'
based on whether they pose a danger to the AV. This is done based on the object
position relative to the AV and its trajectory. With this metric, our model can
effectively detect previously unseen objects to enable the AV to make safer
real-time decisions. Our results demonstrate that the proposed model
effectively detects OOD objects, evaluates their harmfulness, and classifies
them accordingly, thus enhancing the AV decision-making effectiveness in
dynamic environments.

</details>


### [373] [Time-variant Image Inpainting via Interactive Distribution Transition Estimation](https://arxiv.org/abs/2506.23461)
*Yun Xing,Qing Guo,Xiaoguang Li,Yihao Huang,Xiaofeng Cao,Di Lin,Ivor Tsang,Lei Ma*

Key words: 图像修复, 时间变异, 参考引导, InDiTE, 扩散模型

TL;DR: 论文提出了一种新颖的任务TAMP（时间变异图像修复），通过引入参考图像修复目标图像，并提出InDiTE模块和InDiTE-Diff方法解决了现有方法的不足。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统的参考引导图像修复方法在处理时间变异图像时效果不佳，尤其是当参考图像与目标图像内容差异大且可能受损时。

Method: 提出了Interactive Distribution Transition Estimation (InDiTE)模块，结合扩散模型开发了InDiTE-Diff方法，并在潜在空间中进行交叉参考。

Result: 新方法在TAMP-Street数据集上显著优于现有参考引导图像修复方法。

Conclusion: InDiTE-Diff在时间变异图像修复任务中表现优异，为解决此类问题提供了有效方案。

Abstract: In this work, we focus on a novel and practical task, i.e., Time-vAriant
iMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image
by leveraging the complementary information from a reference image, where both
images captured the same scene but with a significant time gap in between,
i.e., time-variant images. Different from conventional reference-guided image
inpainting, the reference image under TAMP setup presents significant content
distinction to the target image and potentially also suffers from damages. Such
an application frequently happens in our daily lives to restore a damaged image
by referring to another reference image, where there is no guarantee of the
reference image's source and quality. In particular, our study finds that even
state-of-the-art (SOTA) reference-guided image inpainting methods fail to
achieve plausible results due to the chaotic image complementation. To address
such an ill-posed problem, we propose a novel Interactive Distribution
Transition Estimation (InDiTE) module which interactively complements the
time-variant images with adaptive semantics thus facilitate the restoration of
damaged regions. To further boost the performance, we propose our TAMP
solution, namely Interactive Distribution Transition Estimation-driven
Diffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and
conducts latent cross-reference during sampling. Moreover, considering the lack
of benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street,
based on existing image and mask datasets. We conduct experiments on the
TAMP-Street datasets under two different time-variant image inpainting
settings, which show our method consistently outperform SOTA reference-guided
image inpainting methods for solving TAMP.

</details>


### [374] [Sanitizing Manufacturing Dataset Labels Using Vision-Language Models](https://arxiv.org/abs/2506.23465)
*Nazanin Mahjourian,Vinh Nguyen*

Key words: 标签清洗, 视觉-语言模型, 制造业图像数据集, CLIP, 多标签分类

TL;DR: 论文提出了一种基于视觉-语言的框架VLSR，用于多标签制造图像数据集的标签清洗和优化，通过CLIP模型嵌入图像和文本标签到共享语义空间，解决了标签噪声和一致性问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 制造业领域中，大规模数据集常因标签噪声和不一致性导致模型训练效果不佳，而高质量标签的获取成本高、耗时长。

Method: 使用CLIP模型将图像和文本标签嵌入共享语义空间，通过余弦相似度识别并优化问题标签，并利用基于密度的聚类合并语义相似的标签。

Result: 在Factorynet数据集上的实验表明，VLSR能有效识别问题标签并提高标签一致性，减少标签词汇量，提升数据集质量。

Conclusion: VLSR框架能以最小人工干预提升工业应用中机器学习模型的训练数据集质量。

Abstract: The success of machine learning models in industrial applications is heavily
dependent on the quality of the datasets used to train the models. However,
large-scale datasets, specially those constructed from crowd-sourcing and
web-scraping, often suffer from label noise, inconsistencies, and errors. This
problem is particularly pronounced in manufacturing domains, where obtaining
high-quality labels is costly and time-consuming. This paper introduces
Vision-Language Sanitization and Refinement (VLSR), which is a
vision-language-based framework for label sanitization and refinement in
multi-label manufacturing image datasets. This method embeds both images and
their associated textual labels into a shared semantic space leveraging the
CLIP vision-language model. Then two key tasks are addressed in this process by
computing the cosine similarity between embeddings. First, label sanitization
is performed to identify irrelevant, misspelled, or semantically weak labels,
and surface the most semantically aligned label for each image by comparing
image-label pairs using cosine similarity between image and label embeddings.
Second, the method applies density-based clustering on text embeddings,
followed by iterative cluster merging, to group semantically similar labels
into unified label groups. The Factorynet dataset, which includes noisy labels
from both human annotations and web-scraped sources, is employed to evaluate
the effectiveness of the proposed framework. Experimental results demonstrate
that the VLSR framework successfully identifies problematic labels and improves
label consistency. This method enables a significant reduction in label
vocabulary through clustering, which ultimately enhances the dataset's quality
for training robust machine learning models in industrial applications with
minimal human intervention.

</details>


### [375] [AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays](https://arxiv.org/abs/2506.23467)
*Chenlang Yi,Zizhan Xiong,Qi Qi,Xiyuan Wei,Girish Bathla,Ching-Long Lin,Bobak Jack Mortazavi,Tianbao Yang*

Key words: CLIP, 医疗图像分类, 公平性, 对抗性特征干预, CXR

TL;DR: 该论文提出AdFair-CLIP框架，通过对抗性特征干预减少CLIP模型在医疗图像分类中的偏见，提升公平性和诊断准确性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: CLIP模型在医疗图像分类中表现出色，但存在种族和性别等偏见问题，可能导致诊断结果不公平。研究旨在解决这一问题。

Method: 采用对抗性特征干预方法（AdFair-CLIP），抑制敏感属性以减少虚假相关性。

Result: 在CXR数据集上实验表明，AdFair-CLIP显著提升公平性和诊断准确性，且在小样本和零样本场景中具有强鲁棒性。

Conclusion: AdFair-CLIP为基于CLIP的医疗诊断模型设置了公平性学习的新基准，特别是在CXR分析中。

Abstract: Contrastive Language-Image Pre-training (CLIP) models have demonstrated
superior performance across various visual tasks including medical image
classification. However, fairness concerns, including demographic biases, have
received limited attention for CLIP models. This oversight leads to critical
issues, particularly those related to race and gender, resulting in disparities
in diagnostic outcomes and reduced reliability for underrepresented groups. To
address these challenges, we introduce AdFair-CLIP, a novel framework employing
adversarial feature intervention to suppress sensitive attributes, thereby
mitigating spurious correlations and improving prediction fairness. We conduct
comprehensive experiments on chest X-ray (CXR) datasets, and show that
AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while
maintaining robust generalization in zero-shot and few-shot scenarios. These
results establish new benchmarks for fairness-aware learning in CLIP-based
medical diagnostic models, particularly for CXR analysis.

</details>


### [376] [Qwen-GUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding](https://arxiv.org/abs/2506.23491)
*ZongHan Hsieh,Tzer-Jen Wei*

Key words: 轻量级视觉语言模型, GUI 任务, 两阶段微调, 数据去冗余

TL;DR: Qwen-GUI-3B 是一个轻量级的视觉语言模型，专为图形用户界面（GUI）任务设计，性能接近更大的模型，同时可在单 GPU 上训练。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决大型视觉语言模型计算资源消耗大、难以在消费级硬件上部署的问题，同时解决高分辨率桌面环境数据稀缺的挑战。

Method: 采用跨平台多分辨率数据集、两阶段微调策略（先跨平台训练，再高分辨率微调）以及数据去冗余技术。

Result: 在 ScreenSpot、ScreenSpot-v2 和 ScreenSpot-Pro 基准测试中分别达到 84.9%、86.4% 和优异表现，超越同类小参数模型。

Conclusion: Qwen-GUI-3B 通过创新数据策略和训练方法，实现了高效、准确的 GUI 任务建模。

Abstract: This paper introduces Qwen-GUI-3B, a lightweight Vision-Language Model (VLM)
specifically designed for Graphical User Interface grounding tasks, achieving
performance competitive with significantly larger models. Unlike large-scale
VLMs (>7B parameters) that are computationally intensive and impractical for
consumer-grade hardware, Qwen-GUI-3B delivers strong grounding accuracy while
being fully trainable on a single GPU (RTX 4090). The model incorporates
several key innovations: (i) combine cross-platform, multi-resolution dataset
of 24K examples from diverse sources including mobile, desktop, and web GUI
screenshots to effectively address data scarcity in high-resolution desktop
environments; (ii) a two-stage fine-tuning strategy, where initial
cross-platform training establishes robust GUI understanding, followed by
specialized fine-tuning on high-resolution data to significantly enhance model
adaptability; and (iii) data curation and redundancy reduction strategies,
demonstrating that randomly sampling a smaller subset with reduced redundancy
achieves performance comparable to larger datasets, emphasizing data diversity
over sheer volume. Empirical evaluation on standard GUI grounding
benchmarks-including ScreenSpot, ScreenSpot-v2, and the challenging
ScreenSpot-Pro, highlights Qwen-GUI-3B's exceptional accuracy, achieving 84.9%
on ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B
parameters. Ablation studies validate the critical role of balanced sampling
and two-stage fine-tuning in enhancing robustness, particularly in
high-resolution desktop scenarios. The Qwen-GUI-3B is available at:
https://github.com/Han1018/Qwen-GUI-3B

</details>


### [377] [When Test-Time Adaptation Meets Self-Supervised Models](https://arxiv.org/abs/2506.23529)
*Jisu Han,Jihee Park,Dongyoon Han,Wonjun Hwang*

Key words: 测试时适应（TTA）、自监督学习（SSL）、对比学习、知识蒸馏、协作学习

TL;DR: 论文探讨了测试时适应（TTA）方法是否能通过自监督学习（SSL）持续改进模型，而无需依赖源预训练，并提出了一种结合SSL和TTA的协作学习框架。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有TTA方法在直接应用于准确率较低的自监督模型时表现不佳，因此需要一种新的协议和框架来优化自监督模型的测试时适应能力。

Method: 提出了一种自监督TTA协议，并通过结合SSL和TTA的协作学习框架，利用对比学习和知识蒸馏逐步优化表示。

Result: 在DINO、MoCo和iBOT等多种自监督模型上验证了方法的有效性，证明了即使没有源预训练，也能实现竞争性性能。

Conclusion: 研究表明，通过协作学习框架可以显著提升自监督模型的测试时适应能力，且无需依赖源预训练。

Abstract: Training on test-time data enables deep learning models to adapt to dynamic
environmental changes, enhancing their practical applicability. Online
adaptation from source to target domains is promising but it remains highly
reliant on the performance of source pretrained model. In this paper, we
investigate whether test-time adaptation (TTA) methods can continuously improve
models trained via self-supervised learning (SSL) without relying on source
pretraining. We introduce a self-supervised TTA protocol after observing that
existing TTA approaches struggle when directly applied to self-supervised
models with low accuracy on the source domain. Furthermore, we propose a
collaborative learning framework that integrates SSL and TTA models, leveraging
contrastive learning and knowledge distillation for stepwise representation
refinement. We validate our method on diverse self-supervised models, including
DINO, MoCo, and iBOT, across TTA benchmarks. Extensive experiments validate the
effectiveness of our approach in SSL, showing that it achieves competitive
performance even without source pretraining.

</details>


### [378] [GViT: Representing Images as Gaussians for Visual Recognition](https://arxiv.org/abs/2506.23532)
*Jefferson Hernandez,Ruozhen He,Guha Balakrishnan,Alexander C. Berg,Vicente Ordonez*

Key words: GVIT, 2D高斯, ViT分类器, 图像重建, Imagenet

TL;DR: GVIT是一种基于可学习2D高斯表示的图像分类框架，取代传统像素或分块输入，通过优化高斯参数和ViT分类器，结合图像重建损失，性能接近传统ViT。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统ViT基于像素或分块输入，GVIT旨在探索更紧凑的高斯表示，提升分类效率。

Method: 使用可学习2D高斯编码图像，通过梯度引导优化高斯参数和ViT分类器，结合可微分渲染器优化重建损失。

Result: 在Imagenet-1k上达到76.9%的top-1准确率，性能接近传统ViT。

Conclusion: 高斯表示结合GVIT引导策略是有效的，为图像分类提供了新思路。

Abstract: We introduce GVIT, a classification framework that abandons conventional
pixel or patch grid input representations in favor of a compact set of
learnable 2D Gaussians. Each image is encoded as a few hundred Gaussians whose
positions, scales, orientations, colors, and opacities are optimized jointly
with a ViT classifier trained on top of these representations. We reuse the
classifier gradients as constructive guidance, steering the Gaussians toward
class-salient regions while a differentiable renderer optimizes an image
reconstruction loss. We demonstrate that by 2D Gaussian input representations
coupled with our GVIT guidance, using a relatively standard ViT architecture,
closely matches the performance of a traditional patch-based ViT, reaching a
76.9% top-1 accuracy on Imagenet-1k using a ViT-B architecture.

</details>


### [379] [Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound](https://arxiv.org/abs/2506.23538)
*Yuhao Huang,Yueyue Xu,Haoran Dou,Jiaxiao Deng,Xin Yang,Hongyu Zheng,Dong Ni*

Key words: 先天性子宫异常, 3D超声, 扩散模型, 强化学习, 不确定性建模

TL;DR: 论文提出了一种智能系统，用于同时实现自动化平面定位和先天性子宫异常（CUAs）诊断，通过扩散模型和强化学习框架提高了诊断准确性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 先天性子宫异常（CUAs）可能导致不孕、流产和妊娠并发症，传统2D超声难以准确评估，而3D超声可以重建冠状平面，为CUAs诊断提供更清晰的形态学信息。

Method: 1）开发了具有局部（平面）和全局（体积/文本）指导的扩散模型，通过自适应权重优化注意力分配；2）引入基于强化学习的框架，从冗余序列中提取关键切片摘要；3）提供文本驱动的粗预测不确定性建模，调整分类概率。

Result: 在大型3D子宫超声数据集上的实验表明，该方法在平面定位和CUAs诊断方面具有高效性。

Conclusion: 提出的智能系统结合了扩散模型和强化学习，显著提高了CUAs诊断的准确性和效率。

Abstract: Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage,
preterm birth, and an increased risk of pregnancy complications. Compared to
traditional 2D ultrasound (US), 3D US can reconstruct the coronal plane,
providing a clear visualization of the uterine morphology for assessing CUAs
accurately. In this paper, we propose an intelligent system for simultaneous
automated plane localization and CUA diagnosis. Our highlights are: 1) we
develop a denoising diffusion model with local (plane) and global (volume/text)
guidance, using an adaptive weighting strategy to optimize attention allocation
to different conditions; 2) we introduce a reinforcement learning-based
framework with unsupervised rewards to extract the key slice summary from
redundant sequences, fully integrating information across multiple planes to
reduce learning difficulty; 3) we provide text-driven uncertainty modeling for
coarse prediction, and leverage it to adjust the classification probability for
overall performance improvement. Extensive experiments on a large 3D uterine US
dataset show the efficacy of our method, in terms of plane localization and CUA
diagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.

</details>


### [380] [Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image Super Resolution](https://arxiv.org/abs/2506.23566)
*Luigi Sigillo,Renato Giamba,Danilo Comminiello*

Key words: 卫星图像超分辨率,潜伏扩散模型,小波变换,MWT-Encoder

TL;DR: MWT-Diff结合潜伏扩散模型和小波变换，提出了一种卫星图像超分辨率框架，通过嵌入元数据、多尺度频率信息和时间关系，显著提升了高分辨率图像的重建效果。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 卫星图像获取受限于传感器的时空分辨率和高昂成本，难以满足环境监测等应用对高分辨率数据的需求。

Method: 提出MWT-Diff框架，基于MWT-Encoder生成嵌入特征，结合分层扩散动态逐步重建高分辨率图像。

Result: 在多数据集上表现优于现有方法，FID和LPIPS等感知质量指标验证了其有效性。

Conclusion: MWT-Diff为解决卫星图像超分辨率问题提供了创新方案，并在实际应用中展现了潜力。

Abstract: The acquisition of high-resolution satellite imagery is often constrained by
the spatial and temporal limitations of satellite sensors, as well as the high
costs associated with frequent observations. These challenges hinder
applications such as environmental monitoring, disaster response, and
agricultural management, which require fine-grained and high-resolution data.
In this paper, we propose MWT-Diff, an innovative framework for satellite image
super-resolution (SR) that combines latent diffusion models with wavelet
transforms to address these challenges. At the core of the framework is a novel
metadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates
embeddings that capture metadata attributes, multi-scale frequency information,
and temporal relationships. The embedded feature representations steer the
hierarchical diffusion dynamics, through which the model progressively
reconstructs high-resolution satellite imagery from low-resolution inputs. This
process preserves critical spatial characteristics including textural patterns,
boundary discontinuities, and high-frequency spectral components essential for
detailed remote sensing analysis. The comparative analysis of MWT-Diff across
multiple datasets demonstrated favorable performance compared to recent
approaches, as measured by standard perceptual quality metrics including FID
and LPIPS.

</details>


### [381] [PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection](https://arxiv.org/abs/2506.23581)
*Xiao Li,Yiming Zhu,Yifan Huang,Wei Zhang,Yingzhe He,Jie Shi,Xiaolin Hu*

Key words: 物体检测，对抗训练，物理可实现攻击，PBCAT，鲁棒性

TL;DR: 论文提出了一种新型的基于补丁的复合对抗训练策略PBCAT，用于防御物体检测中的多种物理可实现攻击，显著提升了模型的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 物体检测在安全敏感应用中至关重要，但容易受到物理可实现攻击的威胁。现有的对抗训练方法主要针对特定攻击（如对抗补丁），缺乏对更广泛攻击类型的防御能力。

Method: 提出PBCAT策略，结合小区域梯度引导对抗补丁和不明显的全局对抗扰动，通过统一的对抗训练方法优化模型。

Result: 实验表明，PBCAT显著提升了模型对各种物理可实现攻击的鲁棒性，特别是在对抗纹理攻击下，检测准确率提高了29.7%。

Conclusion: PBCAT是一种有效的防御方法，能够抵御多种物理可实现攻击，填补了现有研究的空白。

Abstract: Object detection plays a crucial role in many security-sensitive
applications. However, several recent studies have shown that object detectors
can be easily fooled by physically realizable attacks, \eg, adversarial patches
and recent adversarial textures, which pose realistic and urgent threats.
Adversarial Training (AT) has been recognized as the most effective defense
against adversarial attacks. While AT has been extensively studied in the
$l_\infty$ attack settings on classification models, AT against physically
realizable attacks on object detectors has received limited exploration. Early
attempts are only performed to defend against adversarial patches, leaving AT
against a wider range of physically realizable attacks under-explored. In this
work, we consider defending against various physically realizable attacks with
a unified AT method. We propose PBCAT, a novel Patch-Based Composite
Adversarial Training strategy. PBCAT optimizes the model by incorporating the
combination of small-area gradient-guided adversarial patches and imperceptible
global adversarial perturbations covering the entire image. With these designs,
PBCAT has the potential to defend against not only adversarial patches but also
unseen physically realizable attacks such as adversarial textures. Extensive
experiments in multiple settings demonstrated that PBCAT significantly improved
robustness against various physically realizable attacks over state-of-the-art
defense methods. Notably, it improved the detection accuracy by 29.7\% over
previous defense methods under one recent adversarial texture attack.

</details>


### [382] [Brain Tumor Detection through Thermal Imaging and MobileNET](https://arxiv.org/abs/2506.23627)
*Roham Maiti,Debasmita Bhoumik*

Key words: 脑肿瘤, MobileNET, 机器学习, 图像处理

TL;DR: 使用MobileNET模型高效检测脑肿瘤，通过图像处理技术实现快速准确的结果，准确率达98.5%。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 脑肿瘤对健康构成重大威胁，传统检测方法成本高且依赖专家，而经典机器学习模型存在计算量大、数据需求高等问题。

Method: 采用MobileNET模型，结合图像处理技术，减少计算资源和时间需求。

Result: 模型平均准确率达到98.5%。

Conclusion: 该方法实现了高效、低成本的脑肿瘤检测，优于传统方法。

Abstract: Brain plays a crucial role in regulating body functions and cognitive
processes, with brain tumors posing significant risks to human health. Precise
and prompt detection is a key factor in proper treatment and better patient
outcomes. Traditional methods for detecting brain tumors, that include
biopsies, MRI, and CT scans often face challenges due to their high costs and
the need for specialized medical expertise. Recent developments in machine
learning (ML) and deep learning (DL) has exhibited strong capabilities in
automating the identification and categorization of brain tumors from medical
images, especially MRI scans. However, these classical ML models have
limitations, such as high computational demands, the need for large datasets,
and long training times, which hinder their accessibility and efficiency. Our
research uses MobileNET model for efficient detection of these tumors. The
novelty of this project lies in building an accurate tumor detection model
which use less computing re-sources and runs in less time followed by efficient
decision making through the use of image processing technique for accurate
results. The suggested method attained an average accuracy of 98.5%.

</details>


### [383] [On the Domain Robustness of Contrastive Vision-Language Models](https://arxiv.org/abs/2506.23663)
*Mario Koddenbrock,Rudolf Hoffmann,David Brodmann,Erik Rodner*

Key words: 视觉语言模型, 领域鲁棒性, 图像干扰, 开源框架

TL;DR: 论文提出了Deepbench框架，用于评估视觉语言模型在特定领域中的鲁棒性，无需标注数据即可生成领域相关的图像干扰。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 由于大型预训练模型在领域变化时性能下降，缺乏领域适应性的评估方法。

Method: 利用大语言模型生成领域相关的图像干扰，评估多种视觉语言架构的鲁棒性。

Result: 在六个实际领域中观察到鲁棒性的显著差异，表明需要领域感知的评估。

Conclusion: Deepbench为领域感知鲁棒性评估提供了开源工具。

Abstract: In real-world vision-language applications, practitioners increasingly rely
on large, pretrained foundation models rather than custom-built solutions,
despite limited transparency regarding their training data and processes. While
these models achieve impressive performance on general benchmarks, their
effectiveness can decline notably under specialized domain shifts, such as
unique imaging conditions or environmental variations. In this work, we
introduce Deepbench, a framework designed to assess domain-specific robustness
of vision-language models (VLMs). Deepbench leverages a large language model
(LLM) to generate realistic, context-aware image corruptions tailored to
specific deployment domains without requiring labeled data. We evaluate a range
of contrastive vision-language architectures and architectural variants across
six real-world domains and observe substantial variability in robustness,
highlighting the need for targeted, domain-aware evaluation. Deepbench is
released as open-source software to support further research into domain-aware
robustness assessment.

</details>


### [384] [AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval](https://arxiv.org/abs/2506.23605)
*Suyash Maniyar,Vishvesh Trivedi,Ajoy Mondal,Anand Mishra,C. V. Jawahar*

Key words: 合成数据生成，大语言模型，幻灯片理解，少样本迁移学习

TL;DR: 提出了一种基于大语言模型的合成幻灯片生成方法SynLecSlideGen，以减少人工标注需求，并通过实验证明合成数据能有效提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 为了减少对大量人工标注的依赖，尤其是需要领域专家参与的讲座幻灯片标注工作。

Method: 使用大语言模型（LLM）驱动的合成幻灯片生成管道SynLecSlideGen，并创建了RealSlide评测基准。

Result: 实验表明，在合成数据上进行预训练后，少样本迁移学习显著优于仅使用真实数据的训练。

Conclusion: 合成数据可以有效弥补真实标注数据的不足。

Abstract: Lecture slide element detection and retrieval are key problems in slide
understanding. Training effective models for these tasks often depends on
extensive manual annotation. However, annotating large volumes of lecture
slides for supervised training is labor intensive and requires domain
expertise. To address this, we propose a large language model (LLM)-guided
synthetic lecture slide generation pipeline, SynLecSlideGen, which produces
high-quality, coherent and realistic slides. We also create an evaluation
benchmark, namely RealSlide by manually annotating 1,050 real lecture slides.
To assess the utility of our synthetic slides, we perform few-shot transfer
learning on real data using models pre-trained on them. Experimental results
show that few-shot transfer learning with pretraining on synthetic slides
significantly improves performance compared to training only on real data. This
demonstrates that synthetic data can effectively compensate for limited labeled
lecture slides. The code and resources of our work are publicly available on
our project website: https://synslidegen.github.io/.

</details>


### [385] [Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking](https://arxiv.org/abs/2506.23783)
*Shiao Wang,Ju Huang,Qingchuan Ma,Jinfeng Gao,Chunyi Xu,Xiao Wang,Lan Chen,Bo Jiang*

Key words: RGB-Event跟踪，Vision Mamba，跨模态交互，轻量级Prompt Generator

TL;DR: 提出了一种基于Vision Mamba的RGB-Event物体跟踪框架Mamba-FETrack V2，通过轻量级Prompt Generator和Vision Mamba骨干网络实现高效特征提取与跨模态交互。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有跟踪算法依赖高复杂度Vision Transformer，导致计算开销大且跨模态交互效果有限。

Method: 设计轻量级Prompt Generator生成模态特异性提示向量，结合Vision Mamba骨干网络实现特征提取与融合。

Result: 在多个RGB-Event跟踪基准测试中表现出优越性能和效率。

Conclusion: 提出的框架在性能和效率上均优于现有方法。

Abstract: Combining traditional RGB cameras with bio-inspired event cameras for robust
object tracking has garnered increasing attention in recent years. However,
most existing multimodal tracking algorithms depend heavily on high-complexity
Vision Transformer architectures for feature extraction and fusion across
modalities. This not only leads to substantial computational overhead but also
limits the effectiveness of cross-modal interactions. In this paper, we propose
an efficient RGB-Event object tracking framework based on the linear-complexity
Vision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a
lightweight Prompt Generator that utilizes embedded features from each
modality, together with a shared prompt pool, to dynamically generate
modality-specific learnable prompt vectors. These prompts, along with the
modality-specific embedded features, are then fed into a Vision Mamba-based
FEMamba backbone, which facilitates prompt-guided feature extraction,
cross-modal interaction, and fusion in a unified manner. Finally, the fused
representations are passed to the tracking head for accurate target
localization. Extensive experimental evaluations on multiple RGB-Event tracking
benchmarks, including short-term COESOT dataset and long-term datasets, i.e.,
FE108 and FELT V2, demonstrate the superior performance and efficiency of the
proposed tracking framework. The source code and pre-trained models will be
released on https://github.com/Event-AHU/Mamba_FETrack

</details>


### [386] [Unified Multimodal Understanding via Byte-Pair Visual Encoding](https://arxiv.org/abs/2506.23639)
*Wanpeng Zhang,Yicheng Feng,Hao Luo,Yijiang Li,Zihao Yue,Sipeng Zheng,Zongqing Lu*

Key words: 多模态大语言模型,视觉语言理解,字节对编码,优先级编码

TL;DR: 提出了一种通过字节对编码统一多模态理解的框架，改进视觉语言任务表现。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 多模态大语言模型在视觉语言理解上取得进展，但不同模态的对齐仍是挑战。

Method: 采用字节对编码视觉标记，结合频率和空间一致性的优先级编码方案，多阶段训练。

Result: 全面实验显示在多种视觉语言任务上性能提升。

Conclusion: 通过弥合视觉与文本表示的差距，推动了更高效的多模态基础模型发展。

Abstract: Multimodal large language models (MLLMs) have made significant progress in
vision-language understanding, yet effectively aligning different modalities
remains a fundamental challenge. We present a framework that unifies multimodal
understanding by applying byte-pair encoding to visual tokens. Unlike
conventional approaches that rely on modality-specific encoders, our method
directly incorporates structural information into visual tokens, mirroring
successful tokenization strategies in text-only language models. We introduce a
priority-guided encoding scheme that considers both frequency and spatial
consistency, coupled with a multi-stage training procedure based on
curriculum-driven data composition. These enhancements enable the transformer
model to better capture cross-modal relationships and reason with visual
information. Comprehensive experiments demonstrate improved performance across
diverse vision-language tasks. By bridging the gap between visual and textual
representations, our approach contributes to the advancement of more capable
and efficient multimodal foundation models.

</details>


### [387] [VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation](https://arxiv.org/abs/2506.23641)
*Peng Huang,Junhu Fu,Bowen Guo,Zeju Li,Yuanyuan Wang,Yi Guo*

Key words: 医学图像生成、多模态大语言模型、链式思维、原型条件机制、多样性

TL;DR: VAP-Diffusion框架利用多模态大语言模型（MLLMs）生成详细描述，提升医学图像生成的多样性和质量。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 医学图像的生成需要丰富的属性信息，但详细描述不易获取，因此探索通过外部知识（MLLMs）增强生成模型。

Method: 设计链式思维提示从MLLMs获取无幻觉描述，提出原型条件机制以增强生成器对未见描述的鲁棒性。

Result: 在四种数据集的三种医学图像任务上验证了VAP-Diffusion的有效性。

Conclusion: VAP-Diffusion通过MLLMs的外部知识和原型条件机制，显著提升了医学图像生成的多样性和质量。

Abstract: As the appearance of medical images is influenced by multiple underlying
factors, generative models require rich attribute information beyond labels to
produce realistic and diverse images. For instance, generating an image of skin
lesion with specific patterns demands descriptions that go beyond diagnosis,
such as shape, size, texture, and color. However, such detailed descriptions
are not always accessible. To address this, we explore a framework, termed
Visual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from
pre-trained Multi-modal Large Language Models (MLLMs) to improve the quality
and diversity of medical image generation. First, to derive descriptions from
MLLMs without hallucination, we design a series of prompts following
Chain-of-Thoughts for common medical imaging tasks, including dermatologic,
colorectal, and chest X-ray images. Generated descriptions are utilized during
training and stored across different categories. During testing, descriptions
are randomly retrieved from the corresponding category for inference. Moreover,
to make the generator robust to unseen combination of descriptions at the test
time, we propose a Prototype Condition Mechanism that restricts test embeddings
to be similar to those from training. Experiments on three common types of
medical imaging across four datasets verify the effectiveness of VAP-Diffusion.

</details>


### [388] [Spurious-Aware Prototype Refinement for Reliable Out-of-Distribution Detection](https://arxiv.org/abs/2506.23881)
*Reihaneh Zohrabi,Hosein Hasani,Mahdieh Soleymani Baghshah,Anna Rohrbach,Marcus Rohrbach,Mohammad Hossein Rohban*

Key words: OOD检测,伪相关性,原型,后处理,SPROD

TL;DR: SPROD是一种新颖的原型基OOD检测方法，通过优化类原型解决伪相关性问题，无需额外数据或调参，显著提升性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 为了解决现有OOD检测方法易受伪相关性影响的问题，确保模型在真实场景中的可靠性。

Method: 提出SPROD，一种后处理方法，通过优化类原型以减少伪特征带来的偏差，适用于多种模型和OOD设置。

Result: 在多个挑战性数据集上，SPROD平均AUROC提升4.7%，FPR@95降低9.3%。

Conclusion: SPROD能有效提升OOD检测性能，特别是在存在伪相关性的场景中。

Abstract: Out-of-distribution (OOD) detection is crucial for ensuring the reliability
and safety of machine learning models in real-world applications, where they
frequently face data distributions unseen during training. Despite progress,
existing methods are often vulnerable to spurious correlations that mislead
models and compromise robustness. To address this, we propose SPROD, a novel
prototype-based OOD detection approach that explicitly addresses the challenge
posed by unknown spurious correlations. Our post-hoc method refines class
prototypes to mitigate bias from spurious features without additional data or
hyperparameter tuning, and is broadly applicable across diverse backbones and
OOD detection settings. We conduct a comprehensive spurious correlation OOD
detection benchmarking, comparing our method against existing approaches and
demonstrating its superior performance across challenging OOD datasets, such as
CelebA, Waterbirds, UrbanCars, Spurious Imagenet, and the newly introduced
Animals MetaCoCo. On average, SPROD improves AUROC by 4.7% and FPR@95 by 9.3%
over the second best.

</details>


### [389] [When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation](https://arxiv.org/abs/2506.23724)
*Chang'an Yi,Xiaohui Deng,Guohao Chen,Yan Zhou,Qinghua Lu,Shuaicheng Niu*

Key words: 测试时适应（TTA）、跨模型学习、协同学习、无监督学习、模型性能提升

TL;DR: 本文提出了一种跨模型协同学习框架（COCA），用于测试时适应（TTA），通过结合不同模型的互补知识，显著提升了现有方法的性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 研究跨模型知识如何影响TTA过程，发现模型间的互补知识可以提升适应性能。

Method: 提出COCA框架，包含协同适应和自我适应两种策略，结合模型间的互补知识并进行无监督学习。

Result: COCA显著提升了多种模型（如ResNets、ViTs、Mobile-ViTs）的适应性能，例如ViT-Base在ImageNet-C上的准确率从51.7%提升至64.5%。

Conclusion: 跨模型协同学习能有效提升TTA性能，COCA可作为即插即用模块。

Abstract: Test-time Adaptation (TTA) adapts a given model to testing domain data with
potential domain shifts through online unsupervised learning, yielding
impressive performance. However, to date, existing TTA methods primarily focus
on single-model adaptation. In this work, we investigate an intriguing
question: how does cross-model knowledge influence the TTA process? Our
findings reveal that, in TTA's unsupervised online setting, each model can
provide complementary, confident knowledge to the others, even when there are
substantial differences in model size. For instance, a smaller model like
MobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base
(86.6M parameters). In light of this, we propose COCA, a Cross-Model
Co-Learning framework for TTA, which mainly consists of two main strategies. 1)
Co-adaptation adaptively integrates complementary knowledge from other models
throughout the TTA process, reducing individual model biases. 2)
Self-adaptation enhances each model's unique strengths via unsupervised
learning, enabling diverse adaptation to the target domain. Extensive
experiments show that COCA, which can also serve as a plug-and-play module,
significantly boosts existing SOTAs, on models with various sizes--including
ResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example,
with Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy
on ImageNet-C from 51.7% to 64.5%. The code is publicly available at
https://github.com/ycarobot/COCA.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [390] [Strategic A/B testing via Maximum Probability-driven Two-armed Bandit](https://arxiv.org/abs/2506.22536)
*Yu Zhang,Shanshan Zhao,Bokui Wan,Jinjuan Wang,Xiaodong Yan*

Key words: 平均处理效应，二臂老虎机，统计功效，A/B测试

TL;DR: 该论文提出了一种基于最大概率驱动的二臂老虎机（TAB）方法，通过加权均波动统计量来检测微小的平均处理效应，显著提高了统计功效并控制了I类错误。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 在大规模应用中，检测微小的平均处理效应具有重要经济意义，但传统方法因敏感性不足而难以实现。

Method: 采用反事实结果框架，提出加权均波动统计量的二臂老虎机方法，并利用排列方法增强稳健性。

Result: 实验显示，该方法在A/B测试中显著提升了统计功效，并能降低实验成本。

Conclusion: 通过战略中心极限定理（SCLT），该方法在处理微小效应时表现出更高的灵敏度，为大规模应用提供了高效工具。

Abstract: Detecting a minor average treatment effect is a major challenge in
large-scale applications, where even minimal improvements can have a
significant economic impact. Traditional methods, reliant on normal
distribution-based or expanded statistics, often fail to identify such minor
effects because of their inability to handle small discrepancies with
sufficient sensitivity. This work leverages a counterfactual outcome framework
and proposes a maximum probability-driven two-armed bandit (TAB) process by
weighting the mean volatility statistic, which controls Type I error. The
implementation of permutation methods further enhances the robustness and
efficacy. The established strategic central limit theorem (SCLT) demonstrates
that our approach yields a more concentrated distribution under the null
hypothesis and a less concentrated one under the alternative hypothesis,
greatly improving statistical power. The experimental results indicate a
significant improvement in the A/B testing, highlighting the potential to
reduce experimental costs while maintaining high statistical power.

</details>


### [391] [Adjoint Schrödinger Bridge Sampler](https://arxiv.org/abs/2506.22565)
*Guan-Horng Liu,Jaemoo Choi,Yongxin Chen,Benjamin Kurt Miller,Ricky T. Q. Chen*

Key words: 扩散采样, 薛定谔桥, 随机最优控制, 玻尔兹曼分布

TL;DR: 提出了一种新的扩散采样方法ASBS，通过简单的匹配目标实现高效采样，无需估计目标样本，显著提升了采样效率和应用范围。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现有扩散采样方法需要重要性加权估计或复杂学习过程，限制了其可扩展性和实用性。

Method: 基于薛定谔桥模型，利用随机最优控制理论，通过Adjoint Matching实现高效学习。

Result: ASBS在经典能量函数采样、构象生成和分子玻尔兹曼分布等任务中表现出色。

Conclusion: ASBS是一种高效且通用的扩散采样方法，解决了现有方法的限制。

Abstract: Computational methods for learning to sample from the Boltzmann distribution
-- where the target distribution is known only up to an unnormalized energy
function -- have advanced significantly recently. Due to the lack of explicit
target samples, however, prior diffusion-based methods, known as diffusion
samplers, often require importance-weighted estimation or complicated learning
processes. Both trade off scalability with extensive evaluations of the energy
and model, thereby limiting their practical usage. In this work, we propose
Adjoint Schr\"odinger Bridge Sampler (ASBS), a new diffusion sampler that
employs simple and scalable matching-based objectives yet without the need to
estimate target samples during training. ASBS is grounded on a mathematical
model -- the Schr\"odinger Bridge -- which enhances sampling efficiency via
kinetic-optimal transportation. Through a new lens of stochastic optimal
control theory, we demonstrate how SB-based diffusion samplers can be learned
at scale via Adjoint Matching and prove convergence to the global solution.
Notably, ASBS generalizes the recent Adjoint Sampling (Havens et al., 2025) to
arbitrary source distributions by relaxing the so-called memoryless condition
that largely restricts the design space. Through extensive experiments, we
demonstrate the effectiveness of ASBS on sampling from classical energy
functions, amortized conformer generation, and molecular Boltzmann
distributions.

</details>


### [392] [Bayesian Invariance Modeling of Multi-Environment Data](https://arxiv.org/abs/2506.22675)
*Luhuan Wu,Mingzhang Yin,Yixin Wang,John P. Cunningham,David M. Blei*

Key words: 不变预测, 贝叶斯方法, 泛化能力, 因果关系

TL;DR: 该论文提出了贝叶斯不变预测（BIP），一种通过后验推断识别不变特征的 probabilistic model，优于现有方法。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 通过识别跨环境稳定的特征揭示因果关系并支持泛化。

Method: 使用贝叶斯方法将不变特征索引作为隐变量，并通过后验推断恢复。

Result: BIP 和其变分近似 VI-BIP 在仿真和实际数据中比现有方法更准确且可扩展。

Conclusion: BIP 在理论（后验一致性）和实践中均表现优越，尤其在环境异质性高时。

Abstract: Invariant prediction [Peters et al., 2016] analyzes feature/outcome data from
multiple environments to identify invariant features - those with a stable
predictive relationship to the outcome. Such features support generalization to
new environments and help reveal causal mechanisms. Previous methods have
primarily tackled this problem through hypothesis testing or regularized
optimization. Here we develop Bayesian Invariant Prediction (BIP), a
probabilistic model for invariant prediction. BIP encodes the indices of
invariant features as a latent variable and recover them by posterior
inference. Under the assumptions of Peters et al. [2016], the BIP posterior
targets the true invariant features. We prove that the posterior is consistent
and that greater environment heterogeneity leads to faster posterior
contraction. To handle many features, we design an efficient variational
approximation called VI-BIP. In simulations and real data, we find that BIP and
VI-BIP are more accurate and scalable than existing methods for invariant
prediction.

</details>


### [393] [CN-SBM: Categorical Block Modelling For Primary and Residual Copy Number Variation](https://arxiv.org/abs/2506.22963)
*Kevin Lam,William Daniels,J Maxwell Douglas,Daniel Lai,Samuel Aparicio,Benjamin Bloem-Reddy,Yongjin Park*

Key words: 拷贝数变异, 随机块模型, 肿瘤异质性, 聚类分析, 变分推断

TL;DR: 论文提出了一种名为CN-SBM的概率框架，用于基于离散拷贝数状态联合聚类癌症样本和基因组区域，解决了现有模型的不足，并在TCGA低级别胶质瘤数据中验证了其临床实用性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 解决传统模型（如高斯或泊松模型）在分析拷贝数变异（CNV）时未能充分捕捉离散性和亚群特异性模式的问题。

Method: 采用两阶段方法，结合概率框架和变分推断算法，分别提取大规模染色体改变和精细异常。

Result: 在模拟和真实数据集上表现优于现有方法，并成功应用于TCGA低级别胶质瘤数据，揭示临床相关亚型和生存分析中的患者分层。

Conclusion: CN-SBM是一种可解释且可扩展的框架，适用于肿瘤异质性和预后分析的CNV研究。

Abstract: Cancer is a genetic disorder whose clonal evolution can be monitored by
tracking noisy genome-wide copy number variants. We introduce the Copy Number
Stochastic Block Model (CN-SBM), a probabilistic framework that jointly
clusters samples and genomic regions based on discrete copy number states using
a bipartite categorical block model. Unlike models relying on Gaussian or
Poisson assumptions, CN-SBM respects the discrete nature of CNV calls and
captures subpopulation-specific patterns through block-wise structure. Using a
two-stage approach, CN-SBM decomposes CNV data into primary and residual
components, enabling detection of both large-scale chromosomal alterations and
finer aberrations. We derive a scalable variational inference algorithm for
application to large cohorts and high-resolution data. Benchmarks on simulated
and real datasets show improved model fit over existing methods. Applied to
TCGA low-grade glioma data, CN-SBM reveals clinically relevant subtypes and
structured residual variation, aiding patient stratification in survival
analysis. These results establish CN-SBM as an interpretable, scalable
framework for CNV analysis with direct relevance for tumor heterogeneity and
prognosis.

</details>


### [394] [AICO: Feature Significance Tests for Supervised Learning](https://arxiv.org/abs/2506.23396)
*Kay Giesecke,Enguerrand Horel,Chartsiri Jirachotkulthorn*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The opacity of many supervised learning algorithms remains a key challenge,
hindering scientific discovery and limiting broader deployment -- particularly
in high-stakes domains. This paper develops model- and distribution-agnostic
significance tests to assess the influence of input features in any regression
or classification algorithm. Our method evaluates a feature's incremental
contribution to model performance by masking its values across samples. Under
the null hypothesis, the distribution of performance differences across a test
set has a non-positive median. We construct a uniformly most powerful,
randomized sign test for this median, yielding exact p-values for assessing
feature significance and confidence intervals with exact coverage for
estimating population-level feature importance. The approach requires minimal
assumptions, avoids model retraining or auxiliary models, and remains
computationally efficient even for large-scale, high-dimensional settings.
Experiments on synthetic tasks validate its statistical and computational
advantages, and applications to real-world data illustrate its practical
utility.

</details>


### [395] [DPOT: A DeepParticle method for Computation of Optimal Transport with convergence guarantee](https://arxiv.org/abs/2506.23429)
*Yingyuan Li,Aokun Wang,Zhongjian Wang*

Key words: 最优传输映射, 机器学习, DeepParticle, 非配对样本, 弱收敛

TL;DR: 本文提出了一种基于DeepParticle方法的新机器学习方法，用于从非配对样本中计算两个连续分布之间的最优传输映射，并通过理论分析和实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现有的最优传输映射计算方法在处理非配对样本时存在局限性，因此需要一种更灵活且无限制的方法来解决这一问题。

Method: 提出了一种基于DeepParticle的方法，通过最小-最小优化进行训练，且不限制网络结构。

Result: 理论证明了学习到的映射与最优传输映射之间的弱收敛性和定量误差界限，数值实验验证了方法的有效性，尤其是在现实任务中。

Conclusion: 新方法在理论和实践上均表现出色，为最优传输映射的计算提供了一种高效且灵活的工具。

Abstract: In this work, we propose a novel machine learning approach to compute the
optimal transport map between two continuous distributions from their unpaired
samples, based on the DeepParticle methods. The proposed method leads to a
min-min optimization during training and does not impose any restriction on the
network structure. Theoretically we establish a weak convergence guarantee and
a quantitative error bound between the learned map and the optimal transport
map. Our numerical experiments validate the theoretical results and the
effectiveness of the new approach, particularly on real-world tasks.

</details>


### [396] [Minimax Optimal Two-Stage Algorithm For Moment Estimation Under Covariate Shift](https://arxiv.org/abs/2506.23453)
*Zhen Zhang,Xin Liu,Shaoli Wang,Jiaye Teng*

Key words: 协变量偏移, 矩估计, 最小化下界, 两阶段算法, 双重稳健性

TL;DR: 研究了协变量偏移下未知函数矩估计的最小化下界问题，提出了一种两阶段算法，并通过截断估计器解决了实际中分布未知和不稳定的问题。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 协变量偏移在现实场景中普遍存在，但相关研究较少，本文旨在填补这一空白并优化矩估计方法。

Method: 提出两阶段算法：首先在源分布下训练最优估计器，然后通过似然比重加权校准矩估计；针对分布未知问题，提出截断估计器以确保双重稳健性。

Result: 理论分析得出了最小化最优下界（对数因子以内），并通过数值实验验证了方法的有效性。

Conclusion: 所提出的方法在理论和实践中均表现良好，为解决协变量偏移下的矩估计问题提供了有效工具。

Abstract: Covariate shift occurs when the distribution of input features differs
between the training and testing phases. In covariate shift, estimating an
unknown function's moment is a classical problem that remains under-explored,
despite its common occurrence in real-world scenarios. In this paper, we
investigate the minimax lower bound of the problem when the source and target
distributions are known. To achieve the minimax optimal bound (up to a
logarithmic factor), we propose a two-stage algorithm. Specifically, it first
trains an optimal estimator for the function under the source distribution, and
then uses a likelihood ratio reweighting procedure to calibrate the moment
estimator. In practice, the source and target distributions are typically
unknown, and estimating the likelihood ratio may be unstable. To solve this
problem, we propose a truncated version of the estimator that ensures double
robustness and provide the corresponding upper bound. Extensive numerical
studies on synthetic examples confirm our theoretical findings and further
illustrate the effectiveness of our proposed method.

</details>


### [397] [Test of partial effects for Frechet regression on Bures-Wasserstein manifolds](https://arxiv.org/abs/2506.23487)
*Haoshu Xu,Hongzhe Li*

Key words: Frechet回归，Bures Wasserstein流形，样本分割，最优传输，RKHS核

TL;DR: 提出了一种新颖的测试方法，用于评估Bures Wasserstein流形上Frechet回归的部分效应。方法通过样本分割策略实现，证明了其统计量的收敛性，并通过模拟和实际数据验证了其有效性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 为了更好地评估Frechet回归模型在Bures Wasserstein流形上的部分效应，需要一种可靠的测试方法。

Method: 采用样本分割策略：第一子样本用于拟合Frechet回归模型，估计协方差矩阵及其最优传输映射；第二子样本用于构建检验统计量。

Result: 统计量收敛于加权卡方分布的混合，权重为适当RKHS核定义的积分算子的特征值。测试方法具有渐近名义大小，且最坏情况下功效一致收敛于1。

Conclusion: 提出的测试方法在实际应用中显示出良好的有限样本准确性和实用性。

Abstract: We propose a novel test for assessing partial effects in Frechet regression
on Bures Wasserstein manifolds. Our approach employs a sample splitting
strategy: the first subsample is used to fit the Frechet regression model,
yielding estimates of the covariance matrices and their associated optimal
transport maps, while the second subsample is used to construct the test
statistic. We prove that this statistic converges in distribution to a weighted
mixture of chi squared components, where the weights correspond to the
eigenvalues of an integral operator defined by an appropriate RKHS kernel. We
establish that our procedure achieves the nominal asymptotic size and
demonstrate that its worst-case power converges uniformly to one. Through
extensive simulations and a real data application, we illustrate the test's
finite-sample accuracy and practical utility.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [398] [Lower bounds for trace estimation via Block Krylov and other methods](https://arxiv.org/abs/2506.22701)
*Shi Jie Yu*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: math.ST

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper studies theoretical lower bounds for estimating the trace of a
matrix function, $\text{tr}(f(A))$, focusing on methods that use Hutchinson's
method along with Block Krylov techniques. These methods work by approximating
matrix-vector products like $f(A)V$ using a Block Krylov subspace. This is
closely related to approximating functions with polynomials. We derive
theoretical upper bounds on how many Krylov steps are needed for functions such
as $A^{-1/2}$ and $A^{-1}$ by analyzing the upper bounds from the polynomial
approximation of their scalar equivalent. In addition, we also develop lower
limits on the number of queries needed for trace estimation, specifically for
$\text{tr}(W^{-p})$ where $W$ is a Wishart matrix. Our study clarifies the
connection between the number of steps in Block Krylov methods and the degree
of the polynomial used for approximation. This links the total cost of trace
estimation to basic limits in polynomial approximation and how much information
is needed for the computation.

</details>


### [399] [On Universality of Non-Separable Approximate Message Passing Algorithms](https://arxiv.org/abs/2506.23010)
*Max Lovig,Tianhao Wang,Zhou Fan*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: math.ST

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Mean-field characterizations of first-order iterative algorithms -- including
Approximate Message Passing (AMP), stochastic and proximal gradient descent,
and Langevin diffusions -- have enabled a precise understanding of learning
dynamics in many statistical applications. For algorithms whose non-linearities
have a coordinate-separable form, it is known that such characterizations enjoy
a degree of universality with respect to the underlying data distribution.
However, mean-field characterizations of non-separable algorithm dynamics have
largely remained restricted to i.i.d. Gaussian or rotationally-invariant data.
  In this work, we initiate a study of universality for non-separable AMP
algorithms. We identify a general condition for AMP with polynomial
non-linearities, in terms of a Bounded Composition Property (BCP) for their
representing tensors, to admit a state evolution that holds universally for
matrices with non-Gaussian entries. We then formalize a condition of
BCP-approximability for Lipschitz AMP algorithms to enjoy a similar universal
guarantee. We demonstrate that many common classes of non-separable
non-linearities are BCP-approximable, including local denoisers, spectral
denoisers for generic signals, and compositions of separable functions with
generic linear maps, implying the universality of state evolution for AMP
algorithms employing these non-linearities.

</details>


### [400] [Sampling and Identity-Testing Without Approximate Tensorization of Entropy](https://arxiv.org/abs/2506.23456)
*William Gay,William He,Nicholas Kocurek,Ryan O'Donnell*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: math.ST

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Certain tasks in high-dimensional statistics become easier when the
underlying distribution satisfies a local-to-global property called approximate
tensorization of entropy (ATE). For example, the Glauber dynamics Markov chain
of an ATE distribution mixes fast and can produce approximate samples in a
small amount of time, since such a distribution satisfies a modified
log-Sobolev inequality. Moreover, identity-testing for an ATE distribution
requires few samples if the tester is given coordinate conditional access to
the unknown distribution, as shown by Blanca, Chen, \v{S}tefankovi\v{c}, and
Vigoda (COLT 2023).
  A natural class of distributions that do not satisfy ATE consists of mixtures
of (few) distributions that do satisfy ATE. We study the complexity of
identity-testing and sampling for these distributions. Our main results are the
following:
  1. We show fast mixing of Glauber dynamics from a data-based initialization,
with optimal sample complexity, for mixtures of distributions satisfying
modified log-Sobolev inequalities. This extends work of Huang, Koehler, Lee,
Mohanty, Rajaraman, Vuong, and Wu (STOC 2025, COLT 2025) for mixtures of
distributions satisfying Poincar\'e inequalities.
  2. Answering an open question posed by Blanca et al., we give efficient
identity-testers for mixtures of ATE distributions in the
coordinate-conditional sampling access model. We also give some simplifications
and improvements to the original algorithm of Blanca et al.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [401] [Hierarchical Decentralized Stochastic Control for Cyber-Physical Systems](https://arxiv.org/abs/2506.22971)
*Kesav Kazam Ramachandran Anantharaman,Rahul Meshram*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper presents a two-timescale hierarchical decentralized architecture
for control of Cyber-Physical Systems. The architecture consists of $N$
independent sub-processes, a global controller, and $N$ local controllers, each
formulated as a Markov Decision Process (MDP). The global controller, operating
at a slower timescale optimizes the infinite-horizon discounted cumulative
reward under budget constraints. For the local controllers, operating at a
faster timescale, we propose two different optimization frameworks, namely the
COpt and FOpt. In the COpt framework, the local controller also optimizes an
infinite-horizon MDP, while in the FOpt framework, the local controller
optimizes a finite-horizon MDP. The FOpt framework mimics a federal structure,
where the local controllers have more autonomy in their decision making. First,
the existence of stationary deterministic optimal policies for both these
frameworks is established. Then, various relationships between the two
frameworks are studied, including a bound on the difference between the two
optimal value functions. Additionally, sufficiency conditions are provided such
that the two frameworks lead to the same optimal values.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [402] [Investigating an Overfitting and Degeneration Phenomenon in Self-Supervised Multi-Pitch Estimation](https://arxiv.org/abs/2506.23371)
*Frank Cwitkowitz,Zhiyao Duan*

Key words: 多音高估计, 自监督学习, 联合训练, 过拟合, 音乐信息检索

TL;DR: 该论文提出了一种结合自监督和监督学习目标的新方法，用于多音高估计（MPE），并在封闭训练条件下取得了显著改进，但同时也揭示了模型在监督数据和自监督数据上的过拟合现象。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 当前多音高估计（MPE）主要依赖于监督学习，但标注数据难以获取。自监督方法虽显示出潜力，但表现仍不及监督方法，因此需要探索结合两者的新方法。

Method: 扩展了传统的监督MPE范式，引入了基于音高不变性和等变性的自监督目标，进行联合训练。

Result: 在封闭训练条件下，模型表现显著提升，但在更广泛的数据上出现了同时过拟合和监督数据退化的问题。

Conclusion: 研究揭示了联合训练中的新问题，为未来改进提供了方向。

Abstract: Multi-Pitch Estimation (MPE) continues to be a sought after capability of
Music Information Retrieval (MIR) systems, and is critical for many
applications and downstream tasks involving pitch, including music
transcription. However, existing methods are largely based on supervised
learning, and there are significant challenges in collecting annotated data for
the task. Recently, self-supervised techniques exploiting intrinsic properties
of pitch and harmonic signals have shown promise for both monophonic and
polyphonic pitch estimation, but these still remain inferior to supervised
methods. In this work, we extend the classic supervised MPE paradigm by
incorporating several self-supervised objectives based on pitch-invariant and
pitch-equivariant properties. This joint training results in a substantial
improvement under closed training conditions, which naturally suggests that
applying the same objectives to a broader collection of data will yield further
improvements. However, in doing so we uncover a phenomenon whereby our model
simultaneously overfits to the supervised data while degenerating on data used
for self-supervision only. We demonstrate and investigate this and offer our
insights on the underlying problem.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [403] [Seeding neural network quantum states with tensor network states](https://arxiv.org/abs/2506.23550)
*Ryui Kaneko,Shimpei Goto*

Key words: 矩阵乘积态, 受限玻尔兹曼机, CP分解, 量子多体系统, 基态计算

TL;DR: 提出一种将矩阵乘积态（MPS）转换为受限玻尔兹曼机波函数的高效近似方法，通过CP分解生成初始神经网络量子态，并展示其在量子多体基态计算中的高效性。

<details>
  <summary>Details</summary>

Main category: cond-mat.str-el

Motivation: 为了解决量子多体基态计算中初始状态与基态距离较远的问题，提出一种高效生成初始神经网络量子态的方法。

Method: 通过CP分解将MPS转换为受限玻尔兹曼机波函数，并利用多项式时间内生成的初始态缩短与基态的距离。

Result: 以横场伊辛模型为例，证明了方法的效率，并讨论了其在更复杂量子多体系统中的应用潜力。

Conclusion: 该方法能够高效生成初始神经网络量子态，并在复杂量子多体系统中展现出应用前景。

Abstract: We find an efficient approach to approximately convert matrix product states
(MPSs) into restricted Boltzmann machine wave functions consisting of a
multinomial hidden unit through a canonical polyadic (CP) decomposition of the
MPSs. This method allows us to generate well-behaved initial neural network
quantum states for quantum many-body ground-state calculations in polynomial
time of the number of variational parameters and systematically shorten the
distance between the initial states and the ground states with increasing the
rank of the CP decomposition. We demonstrate the efficiency of our method by
taking the transverse-field Ising model as an example and discuss possible
applications of our method to more general quantum many-body systems in which
the ground-state wave functions possess complex nodal structures.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [404] [Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding](https://arxiv.org/abs/2506.22593)
*Antonello Longo,Chanyoung Chung,Matteo Palieri,Sung-Kyun Kim,Ali Agha,Cataldo Guaragnella,Shehryar Khattak*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Autonomous robots are increasingly playing key roles as support platforms for
human operators in high-risk, dangerous applications. To accomplish challenging
tasks, an efficient human-robot cooperation and understanding is required.
While typically robotic planning leverages 3D geometric information, human
operators are accustomed to a high-level compact representation of the
environment, like top-down 2D maps representing the Building Information Model
(BIM). 3D scene graphs have emerged as a powerful tool to bridge the gap
between human readable 2D BIM and the robot 3D maps. In this work, we introduce
Pixels-to-Graph (Pix2G), a novel lightweight method to generate structured
scene graphs from image pixels and LiDAR maps in real-time for the autonomous
exploration of unknown environments on resource-constrained robot platforms. To
satisfy onboard compute constraints, the framework is designed to perform all
operation on CPU only. The method output are a de-noised 2D top-down
environment map and a structure-segmented 3D pointcloud which are seamlessly
connected using a multi-layer graph abstracting information from object-level
up to the building-level. The proposed method is quantitatively and
qualitatively evaluated during real-world experiments performed using the NASA
JPL NeBula-Spot legged robot to autonomously explore and map cluttered garage
and urban office like environments in real-time.

</details>


### [405] [DriveBLIP2: Attention-Guided Explanation Generation for Complex Driving Scenarios](https://arxiv.org/abs/2506.22494)
*Shihong Ling,Yue Wan,Xiaowei Jia,Na Du*

Key words: DriveBLIP2, 注意力地图, 自动驾驶, 视觉语言模型

TL;DR: DriveBLIP2框架基于BLIP2-OPT架构，通过注意力地图生成器提升自动驾驶场景中的解释质量。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 自动驾驶场景复杂，现有视觉语言模型难以理解多对象环境和实时应用，需要更清晰的解释机制。

Method: 提出注意力地图生成器，聚焦关键对象的视频帧，提升模型对驾驶决策的解释能力。

Result: 在DRAMA数据集上评估，BLEU、ROUGE等指标显著优于基线模型。

Conclusion: 定向注意力机制能有效增强自动驾驶中视觉语言模型的可解释性。

Abstract: This paper introduces a new framework, DriveBLIP2, built upon the BLIP2-OPT
architecture, to generate accurate and contextually relevant explanations for
emerging driving scenarios. While existing vision-language models perform well
in general tasks, they encounter difficulties in understanding complex,
multi-object environments, particularly in real-time applications such as
autonomous driving, where the rapid identification of key objects is crucial.
To address this limitation, an Attention Map Generator is proposed to highlight
significant objects relevant to driving decisions within critical video frames.
By directing the model's focus to these key regions, the generated attention
map helps produce clear and relevant explanations, enabling drivers to better
understand the vehicle's decision-making process in critical situations.
Evaluations on the DRAMA dataset reveal significant improvements in explanation
quality, as indicated by higher BLEU, ROUGE, CIDEr, and SPICE scores compared
to baseline models. These findings underscore the potential of targeted
attention mechanisms in vision-language models for enhancing explainability in
real-time autonomous driving.

</details>


### [406] [Scenario-Based Hierarchical Reinforcement Learning for Automated Driving Decision Making](https://arxiv.org/abs/2506.23023)
*M. Youssef Abdelhamid,Lennart Vater,Zlatan Ajanovic*

Key words: 自动驾驶,强化学习,分层策略,场景多样性,SAD-RL

TL;DR: SAD-RL框架通过结合分层策略强化学习和基于场景的环境，解决了自动驾驶决策算法在复杂任务中的泛化性和学习效率问题。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 自动驾驶系统需要在开放复杂环境中安全运行，而现有强化学习方法在复杂任务中泛化能力和学习效率不足。

Method: 提出SAD-RL框架，结合分层策略（高层选择模板，低层执行）和基于场景的环境，控制训练体验并引入挑战性场景。

Result: 实验表明SAD-RL训练的智能体能在简单和挑战性场景中高效实现安全行为，分层策略和场景多样性是关键。

Conclusion: SAD-RL通过分层策略和场景多样性显著提升自动驾驶决策算法的性能。

Abstract: Developing decision-making algorithms for highly automated driving systems
remains challenging, since these systems have to operate safely in an open and
complex environments. Reinforcement Learning (RL) approaches can learn
comprehensive decision policies directly from experience and already show
promising results in simple driving tasks. However, current approaches fail to
achieve generalizability for more complex driving tasks and lack learning
efficiency. Therefore, we present Scenario-based Automated Driving
Reinforcement Learning (SAD-RL), the first framework that integrates
Reinforcement Learning (RL) of hierarchical policy in a scenario-based
environment. A high-level policy selects maneuver templates that are evaluated
and executed by a low-level control logic. The scenario-based environment
allows to control the training experience for the agent and to explicitly
introduce challenging, but rate situations into the training process. Our
experiments show that an agent trained using the SAD-RL framework can achieve
safe behaviour in easy as well as challenging situations efficiently. Our
ablation studies confirmed that both HRL and scenario diversity are essential
for achieving these results.

</details>


### [407] [Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models](https://arxiv.org/abs/2506.23164)
*Maarten Hugenholtz,Anna Meszaros,Jens Kober,Zlatan Ajanovic*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Autonomous Vehicle decisions rely on multimodal prediction models that
account for multiple route options and the inherent uncertainty in human
behavior. However, models can suffer from mode collapse, where only the most
likely mode is predicted, posing significant safety risks. While existing
methods employ various strategies to generate diverse predictions, they often
overlook the diversity in interaction modes among agents. Additionally,
traditional metrics for evaluating prediction models are dataset-dependent and
do not evaluate inter-agent interactions quantitatively. To our knowledge, none
of the existing metrics explicitly evaluates mode collapse. In this paper, we
propose a novel evaluation framework that assesses mode collapse in joint
trajectory predictions, focusing on safety-critical interactions. We introduce
metrics for mode collapse, mode correctness, and coverage, emphasizing the
sequential dimension of predictions. By testing four multi-agent trajectory
prediction models, we demonstrate that mode collapse indeed happens. When
looking at the sequential dimension, although prediction accuracy improves
closer to interaction events, there are still cases where the models are unable
to predict the correct interaction mode, even just before the interaction mode
becomes inevitable. We hope that our framework can help researchers gain new
insights and advance the development of more consistent and accurate prediction
models, thus enhancing the safety of autonomous driving systems.

</details>


### [408] [Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop](https://arxiv.org/abs/2506.23351)
*Tianxing Chen,Kaixuan Wang,Zhaohui Yang,Yuhao Zhang,Zanxin Chen,Baijun Chen,Wanxi Dong,Ziyuan Liu,Dong Chen,Tianshuo Yang,Haibao Yu,Xiaokang Yang,Yusen Qin,Zhiqiang Xie,Yao Mu,Ping Luo,Tian Nian,Weiliang Deng,Yiheng Ge,Yibin Liu,Zixuan Li,Dehui Wang,Zhixuan Liang,Haohui Xie,Rijie Zeng,Yunfei Ge,Peiqing Cong,Guannan He,Zhaoming Han,Ruocheng Yin,Jingxiang Guo,Lunkai Lin,Tianling Xu,Hongzhe Bi,Xuewu Lin,Tianwei Lin,Shujie Luo,Keyu Li,Ziyan Zhao,Ke Fan,Heyang Xu,Bo Peng,Wenlong Gao,Dongjiang Li,Feng Jin,Hui Shen,Jinming Li,Chaowei Cui,Yuchen,Yaxin Peng,Lingdong Zeng,Wenlong Dong,Tengfei Li,Weijie Ke,Jun Chen,Erdemt Bao,Tian Lan,Tenglong Liu,Jin Yang,Huiping Zhuang,Baozhi Jia,Shuai Zhang,Zhengfeng Zou,Fangheng Guan,Tianyi Jia,Ke Zhou,Hongjiu Zhang,Yating Han,Cheng Fang,Yixian Zou,Chongyang Xu,Qinglun Zhang,Shen Cheng,Xiaohe Wang,Ping Tan,Haoqiang Fan,Shuaicheng Liu,Jiaheng Chen,Chuxuan Huang,Chengliang Lin,Kaijun Luo,Boyu Yue,Yi Liu,Jinyu Chen,Zichang Tan,Liming Deng,Shuo Xu,Zijian Cai,Shilong Yin,Hao Wang,Hongshan Liu,Tianyang Li,Long Shi,Ran Xu,Huilin Xu,Zhengquan Zhang,Congsheng Xu,Jinchang Yang,Feng Xu*

Key words: 具身人工智能,双臂协作,机器人操纵,挑战赛,通用策略

TL;DR: 该论文介绍了RoboTwin双臂协作挑战赛的目标、设置、任务设计和主要成果，旨在推动通用双臂操作策略的研究。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 推动具身人工智能在复杂物理环境中的应用，解决双臂协作操纵刚性、可变形和触觉敏感物体的挑战。

Method: 通过仿真和实际环境的三阶段比赛，参与者完成了17种双臂操作任务。

Result: 吸引了64个团队和400多名参与者，产生了如SEM和AnchorDP3等高性能解决方案，为通用双臂策略学习提供了宝贵见解。

Conclusion: 比赛为未来研究提供了方向和基准，旨在支持鲁棒且通用的双臂操作策略的发展。

Abstract: Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in
robotics, driven by the need for autonomous systems that can perceive, reason,
and act in complex physical environments. While single-arm systems have shown
strong task performance, collaborative dual-arm systems are essential for
handling more intricate tasks involving rigid, deformable, and
tactile-sensitive objects. To advance this goal, we launched the RoboTwin
Dual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on
the RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot
platform, the competition consisted of three stages: Simulation Round 1,
Simulation Round 2, and a final Real-World Round. Participants totally tackled
17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based
scenarios. The challenge attracted 64 global teams and over 400 participants,
producing top-performing solutions like SEM and AnchorDP3 and generating
valuable insights into generalizable bimanual policy learning. This report
outlines the competition setup, task design, evaluation methodology, key
findings and future direction, aiming to support future research on robust and
generalizable bimanual manipulation policies. The Challenge Webpage is
available at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.

</details>


### [409] [MGPRL: Distributed Multi-Gaussian Processes for Wi-Fi-based Multi-Robot Relative Localization in Large Indoor Environments](https://arxiv.org/abs/2506.23514)
*Sai Krishna Ghanta,Ramviyas Parasuraman*

Key words: 多机器人系统，相对定位，Wi-Fi信号强度，高斯过程，凸包对齐

TL;DR: 论文提出了一种新型多机器人相对定位框架MGPRL，利用Wi-Fi信号强度和凸包对齐技术，解决了GPS缺失环境下的定位问题。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 现有方法依赖昂贵或短程传感器（如摄像头、LiDAR），计算开销大且在非连续环境中表现不佳，因此需要一种高效且广泛适用的解决方案。

Method: 采用多输出高斯过程预测Wi-Fi信号强度场，结合不确定性感知的多AP定位和加权凸包对齐技术，实现分布式相对定位。

Result: MGPRL在仿真和实际实验中均优于现有方法，定位准确性和计算效率显著提升。

Conclusion: MGPRL为资源受限设备提供了一种无需预校准的高效相对定位方案，适合广泛应用。

Abstract: Relative localization is a crucial capability for multi-robot systems
operating in GPS-denied environments. Existing approaches for multi-robot
relative localization often depend on costly or short-range sensors like
cameras and LiDARs. Consequently, these approaches face challenges such as high
computational overhead (e.g., map merging) and difficulties in disjoint
environments. To address this limitation, this paper introduces MGPRL, a novel
distributed framework for multi-robot relative localization using convex-hull
of multiple Wi-Fi access points (AP). To accomplish this, we employ
co-regionalized multi-output Gaussian Processes for efficient Radio Signal
Strength Indicator (RSSI) field prediction and perform uncertainty-aware
multi-AP localization, which is further coupled with weighted convex hull-based
alignment for robust relative pose estimation. Each robot predicts the RSSI
field of the environment by an online scan of APs in its environment, which are
utilized for position estimation of multiple APs. To perform relative
localization, each robot aligns the convex hull of its predicted AP locations
with that of the neighbor robots. This approach is well-suited for devices with
limited computational resources and operates solely on widely available Wi-Fi
RSSI measurements without necessitating any dedicated pre-calibration or
offline fingerprinting. We rigorously evaluate the performance of the proposed
MGPRL in ROS simulations and demonstrate it with real-world experiments,
comparing it against multiple state-of-the-art approaches. The results showcase
that MGPRL outperforms existing methods in terms of localization accuracy and
computational efficiency. Finally, we open source MGPRL as a ROS package
https://github.com/herolab-uga/MGPRL.

</details>


### [410] [Online Human Action Detection during Escorting](https://arxiv.org/abs/2506.23573)
*Siddhartha Mondal,Avik Mitra,Chayan Sarkar*

Key words: 机器人护送、深度学习、人重新识别、动作预测、拥挤环境

TL;DR: 本文提出了一种新型神经网络架构，能够在拥挤环境中同时实现人重新识别和动作预测，以提高机器人护送服务的效率。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 当前护送机器人主要依赖导航策略，假设被护送者会顺利跟随，但在拥挤环境中常因无法理解人类动态行为而失败。需要新方法提升机器人对人类动作的感知和响应能力。

Method: 提出了一种新型神经网络架构，能够实时完成人重新识别和动作预测，并根据被护送者的动态调整速度。

Result: 与现有基线相比，该系统在复杂场景中表现出更高的效率和有效性，显著改善了机器人护送服务。

Conclusion: 该研究为拥挤环境中的机器人护送提供了有效解决方案，未来有望提升实际应用中的服务质量。

Abstract: The deployment of robot assistants in large indoor spaces has seen
significant growth, with escorting tasks becoming a key application. However,
most current escorting robots primarily rely on navigation-focused strategies,
assuming that the person being escorted will follow without issue. In crowded
environments, this assumption often falls short, as individuals may struggle to
keep pace, become obstructed, get distracted, or need to stop unexpectedly. As
a result, conventional robotic systems are often unable to provide effective
escorting services due to their limited understanding of human movement
dynamics. To address these challenges, an effective escorting robot must
continuously detect and interpret human actions during the escorting process
and adjust its movement accordingly. However, there is currently no existing
dataset designed specifically for human action detection in the context of
escorting. Given that escorting often occurs in crowded environments, where
other individuals may enter the robot's camera view, the robot also needs to
identify the specific human it is escorting (the subject) before predicting
their actions. Since no existing model performs both person re-identification
and action prediction in real-time, we propose a novel neural network
architecture that can accomplish both tasks. This enables the robot to adjust
its speed dynamically based on the escortee's movements and seamlessly resume
escorting after any disruption. In comparative evaluations against strong
baselines, our system demonstrates superior efficiency and effectiveness,
showcasing its potential to significantly improve robotic escorting services in
complex, real-world scenarios.

</details>


### [411] [PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?](https://arxiv.org/abs/2506.23725)
*Atharva Gundawar,Som Sagar,Ransalu Senanayake*

Key words: 视觉语言模型, 物理推理, 机器人操作, 基准测试, PAC Bench

TL;DR: 介绍了PAC Bench，一个用于系统评估视觉语言模型（VLMs）在物理前提理解能力的基准测试，揭示了现有模型的不足。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 现有视觉语言模型在机器人操作任务中缺乏对低级物理前提的理解，如物体属性、动作潜力和物理约束。

Method: 构建了包含多样数据集（30,000+标注）的PAC Bench基准，评估模型在属性、潜力和约束（PAC）上的表现。

Result: 当前VLM在基础物理概念理解上存在显著差距，不适用于可靠的机器人操作。

Conclusion: PAC Bench为物理推理的标准化评估提供了工具，并指导开发更具物理基础的模型。

Abstract: Vision-Language Models (VLMs) are increasingly pivotal for generalist robot
manipulation, enabling tasks such as physical reasoning, policy generation, and
failure detection. However, their proficiency in these high-level applications
often assumes a deep understanding of low-level physical prerequisites, a
capability that remains largely unverified. For robots to perform actions
reliably, they must comprehend intrinsic object properties (e.g., material,
weight), action affordances (e.g., graspable, stackable), and physical
constraints (e.g., stability, reachability, or an object's state, such as being
closed). Despite the widespread use of VLMs in manipulation tasks, we argue
that off-the-shelf models may lack this granular, physically grounded
understanding, as such prerequisites are often overlooked during training.
  To address this critical gap, we introduce PAC Bench, a comprehensive
benchmark designed to systematically evaluate VLMs on their understanding of
core Properties, Affordances, and Constraints (PAC) from a task executability
perspective. PAC Bench features a diverse dataset with over 30,000 annotations,
comprising 673 real-world images (115 object classes, 15 property types, and 1
to 3 affordances defined per class), 100 real-world humanoid-view scenarios,
and 120 unique simulated constraint scenarios across four tasks.
  Our evaluations reveal significant gaps in the ability of current VLMs to
grasp fundamental physical concepts, highlighting limitations in their
suitability for reliable robot manipulation and pointing to key areas for
targeted research. PAC Bench also serves as a standardized benchmark for
rigorously evaluating physical reasoning in VLMs and guiding the development of
more robust, physically grounded models for robotic applications.
  Project Page: https://pacbench.github.io/

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [412] [Innovative Research on IoT Architecture and Robotic Operating Platforms: Applications of Large Language Models and Generative AI](https://arxiv.org/abs/2506.22477)
*Huiwen Han*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces an innovative design for robotic operating platforms,
underpinned by a transformative Internet of Things (IoT) architecture,
seamlessly integrating cutting-edge technologies such as large language models
(LLMs), generative AI, edge computing, and 5G networks. The proposed platform
aims to elevate the intelligence and autonomy of IoT systems and robotics,
enabling them to make real-time decisions and adapt dynamically to changing
environments. Through a series of compelling case studies across industries
including smart manufacturing, healthcare, and service sectors, this paper
demonstrates the substantial potential of IoT-enabled robotics to optimize
operational workflows, enhance productivity, and deliver innovative, scalable
solutions. By emphasizing the roles of LLMs and generative AI, the research
highlights how these technologies drive the evolution of intelligent robotics
and IoT, shaping the future of industry-specific advancements. The findings not
only showcase the transformative power of these technologies but also offer a
forward-looking perspective on their broader societal and industrial
implications, positioning them as catalysts for next-generation automation and
technological convergence.

</details>


### [413] [AGI Enabled Solutions For IoX Layers Bottlenecks In Cyber-Physical-Social-Thinking Space](https://arxiv.org/abs/2506.22487)
*Amar Khelloufi,Huansheng Ning,Sahraoui Dhelim,Jianguo Ding*

Key words: 万物互联（IoX）、人工智能通用智能（AGI）、Cyber-Physical-Social Thinking（CPST）、自适应传感器融合、神经符号推理

TL;DR: 该论文综述了人工智能通用智能（AGI）与万物互联（IoX）的集成，重点探讨了感知层数据管理、网络层协议优化和应用层决策框架，提出AGI驱动策略如何解决各层瓶颈问题。

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: 研究旨在解决Cyber-Physical-Social Thinking（CPST）生态系统中感知、网络和应用层的关键瓶颈问题，探索AGI如何提升IoX系统的性能。

Method: 通过系统综述，分析AGI在感知层的自适应传感器融合、边缘预处理和选择性注意力机制，网络层的协议异构性和动态频谱管理，以及应用层的神经符号推理和因果推理等。

Result: AGI驱动的策略（如自适应传感器融合和语义建模）为感知层数据过载、网络层协议异构性和应用层身份爆炸提供了新颖解决方案。

Conclusion: AGI增强的IoX是互联系统与高级AI交叉领域的重要研究方向，但仍需解决计算需求、可扩展性和实际验证等挑战。

Abstract: The integration of the Internet of Everything (IoX) and Artificial General
Intelligence (AGI) has given rise to a transformative paradigm aimed at
addressing critical bottlenecks across sensing, network, and application layers
in Cyber-Physical-Social Thinking (CPST) ecosystems. In this survey, we provide
a systematic and comprehensive review of AGI-enhanced IoX research, focusing on
three key components: sensing-layer data management, network-layer protocol
optimization, and application-layer decision-making frameworks. Specifically,
this survey explores how AGI can mitigate IoX bottlenecks challenges by
leveraging adaptive sensor fusion, edge preprocessing, and selective attention
mechanisms at the sensing layer, while resolving network-layer issues such as
protocol heterogeneity and dynamic spectrum management, neuro-symbolic
reasoning, active inference, and causal reasoning, Furthermore, the survey
examines AGI-enabled frameworks for managing identity and relationship
explosion. Key findings suggest that AGI-driven strategies, such as adaptive
sensor fusion, edge preprocessing, and semantic modeling, offer novel solutions
to sensing-layer data overload, network-layer protocol heterogeneity, and
application-layer identity explosion. The survey underscores the importance of
cross-layer integration, quantum-enabled communication, and ethical governance
frameworks for future AGI-enabled IoX systems. Finally, the survey identifies
unresolved challenges, such as computational requirements, scalability, and
real-world validation, calling for further research to fully realize AGI's
potential in addressing IoX bottlenecks. we believe AGI-enhanced IoX is
emerging as a critical research field at the intersection of interconnected
systems and advanced AI.

</details>


### [414] [Offline Reinforcement Learning for Mobility Robustness Optimization](https://arxiv.org/abs/2506.22793)
*Pegah Alizadeh,Anastasios Giovanidis,Pradeepa Ramachandra,Vasileios Koutsoukis,Osama Arouk*

Key words: MRO, 离线强化学习, 决策变换器, 保守Q学习, 新无线网络

TL;DR: 通过离线强化学习方法重新优化移动鲁棒性算法（MRO），对比决策变换器和保守Q学习方法在相同目标奖励下的表现，结果显示离线RL方法优于基于规则的MRO，提升高达7%。

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: 探索是否可以通过离线强化学习优化MRO算法的Cell Individual Offset调谐，避免进一步探索的需求。

Method: 使用决策变换器（序列方法）和保守Q学习（价值方法），基于相同输入特征（如失败、乒乓效应等）学习优化策略。

Result: 在3500 MHz载频的新无线网络测试中，离线RL方法比基于规则的MRO表现更优，提升达7%，且能灵活适应不同目标函数。

Conclusion: 离线强化学习方法在MRO优化中具有显著优势，提供更高的性能和操作灵活性。

Abstract: In this work we revisit the Mobility Robustness Optimisation (MRO) algorithm
and study the possibility of learning the optimal Cell Individual Offset tuning
using offline Reinforcement Learning. Such methods make use of collected
offline datasets to learn the optimal policy, without further exploration. We
adapt and apply a sequence-based method called Decision Transformers as well as
a value-based method called Conservative Q-Learning to learn the optimal policy
for the same target reward as the vanilla rule-based MRO. The same input
features related to failures, ping-pongs, and other handover issues are used.
Evaluation for realistic New Radio networks with 3500 MHz carrier frequency on
a traffic mix including diverse user service types and a specific tunable
cell-pair shows that offline-RL methods outperform rule-based MRO, offering up
to 7% improvement. Furthermore, offline-RL can be trained for diverse objective
functions using the same available dataset, thus offering operational
flexibility compared to rule-based methods.

</details>


### [415] [Service Placement in Small Cell Networks Using Distributed Best Arm Identification in Linear Bandits](https://arxiv.org/abs/2506.22480)
*Mariam Yahya,Aydin Sezgin,Setareh Maghsudi*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As users in small cell networks increasingly rely on computation-intensive
services, cloud-based access often results in high latency. Multi-access edge
computing (MEC) mitigates this by bringing computational resources closer to
end users, with small base stations (SBSs) serving as edge servers to enable
low-latency service delivery. However, limited edge capacity makes it
challenging to decide which services to deploy locally versus in the cloud,
especially under unknown service demand and dynamic network conditions. To
tackle this problem, we model service demand as a linear function of service
attributes and formulate the service placement task as a linear bandit problem,
where SBSs act as agents and services as arms. The goal is to identify the
service that, when placed at the edge, offers the greatest reduction in total
user delay compared to cloud deployment. We propose a distributed and adaptive
multi-agent best-arm identification (BAI) algorithm under a fixed-confidence
setting, where SBSs collaborate to accelerate learning. Simulations show that
our algorithm identifies the optimal service with the desired confidence and
achieves near-optimal speedup, as the number of learning rounds decreases
proportionally with the number of SBSs. We also provide theoretical analysis of
the algorithm's sample complexity and communication overhead.

</details>


### [416] [Geminet: Learning the Duality-based Iterative Process for Lightweight Traffic Engineering in Changing Topologies](https://arxiv.org/abs/2506.23640)
*Ximeng Liu,Shizhen Zhao,Xinbing Wang*

Key words: Geminet,机器学习的流量工程,轻量级,拓扑变化,可扩展性

TL;DR: Geminet是一个轻量级且可扩展的基于机器学习的流量工程框架，能够处理拓扑变化，显著降低了计算和内存开销。

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: 现有基于机器学习的流量工程方案要么无法处理拓扑变化，要么因计算和内存开销过大而难以扩展，Geminet旨在解决这些问题。

Method: Geminet通过解耦神经网络与拓扑，学习基于梯度下降的迭代调整过程，并通过将优化从路径级路由权重转移到边级双变量来减少内存消耗。

Result: Geminet在大规模拓扑上训练时内存消耗低于10 GiB，比现有方案（如HARP）少8倍以上，收敛速度快5.45倍，同时保持了性能。

Conclusion: Geminet在保持高性能的同时显著提升了可扩展性，为大范围部署提供了潜力。

Abstract: Recently, researchers have explored ML-based Traffic Engineering (TE),
leveraging neural networks to solve TE problems traditionally addressed by
optimization. However, existing ML-based TE schemes remain impractical: they
either fail to handle topology changes or suffer from poor scalability due to
excessive computational and memory overhead. To overcome these limitations, we
propose Geminet, a lightweight and scalable ML-based TE framework that can
handle changing topologies. Geminet is built upon two key insights: (i) a
methodology that decouples neural networks from topology by learning an
iterative gradient-descent-based adjustment process, as the update rule of
gradient descent is topology-agnostic, relying only on a few gradient-related
quantities; (ii) shifting optimization from path-level routing weights to
edge-level dual variables, reducing memory consumption by leveraging the fact
that edges are far fewer than paths. Evaluations on WAN and data center
datasets show that Geminet significantly improves scalability. Its neural
network size is only 0.04% to 7% of existing schemes, while handling topology
variations as effectively as HARP, a state-of-the-art ML-based TE approach,
without performance degradation. When trained on large-scale topologies,
Geminet consumes under 10 GiB of memory, more than eight times less than the
80-plus GiB required by HARP, while achieving 5.45 times faster convergence
speed, demonstrating its potential for large-scale deployment.

</details>


### [417] [The Kubernetes Network Driver Model: A Composable Architecture for High-Performance Networking](https://arxiv.org/abs/2506.23628)
*Antonio Ojea*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Traditional Kubernetes networking struggles to meet the escalating demands of
AI/ML and evolving Telco infrastructure. This paper introduces Kubernetes
Network Drivers (KNDs), a transformative, modular, and declarative architecture
designed to overcome current imperative provisioning and API limitations. KNDs
integrate network resource management into Kubernetes' core by utilizing
Dynamic Resource Allocation (DRA), Node Resource Interface (NRI) improvements,
and upcoming OCI Runtime Specification changes. Our DraNet implementation
demonstrates declarative attachment of network interfaces, including Remote
Direct Memory Access (RDMA) devices, significantly boosting high-performance
AI/ML workloads. This capability enables sophisticated cloud-native
applications and lays crucial groundwork for future Telco solutions, fostering
a "galaxy" of specialized KNDs for enhanced application delivery and reduced
operational complexity.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [418] [Spectral Bias in Variational Quantum Machine Learning](https://arxiv.org/abs/2506.22555)
*Callum Duffy,Marcin Jastrzebski*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this work, we investigate the phenomenon of spectral bias in quantum
machine learning, where, in classical settings, models tend to fit
low-frequency components of a target function earlier during training than
high-frequency ones, demonstrating a frequency-dependent rate of convergence.
We study this effect specifically in parameterised quantum circuits (PQCs).
Leveraging the established formulation of PQCs as Fourier series, we prove that
spectral bias in this setting arises from the ``redundancy'' of the Fourier
coefficients, which denotes the number of terms in the analytical form of the
model contributing to the same frequency component. The choice of data encoding
scheme dictates the degree of redundancy for a Fourier coefficient. We find
that the magnitude of the Fourier coefficients' gradients during training
strongly correlates with the coefficients' redundancy. We then further
demonstrate this empirically with three different encoding schemes.
Additionally, we demonstrate that PQCs with greater redundancy exhibit
increased robustness to random perturbations in their parameters at the
corresponding frequencies. We investigate how design choices affect the ability
of PQCs to learn Fourier sums, focusing on parameter initialization scale and
entanglement structure, finding large initializations and low-entanglement
schemes tend to slow convergence.

</details>


### [419] [Tensor Train Quantum State Tomography using Compressed Sensing](https://arxiv.org/abs/2506.23560)
*Shakir Showkat Sofi,Charlotte Vermeylen,Lieven De Lathauwer*

Key words: 量子态层析, 张量分解, 低秩近似, 计算效率

TL;DR: 论文提出了一种基于低秩块张量分解的量子态层析方法，解决了传统方法因参数指数增长导致的计算不实用问题。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 量子态层析是评估量子设备性能的关键技术，但传统方法因参数指数增长而难以实用。

Method: 采用低秩块张量分解参数化量子态，提升存储和计算效率。

Result: 该方法适用于可被低秩分解近似的量子态，如纯态、近纯态和哈密顿基态。

Conclusion: 提出的框架为解决量子态层析的高维参数问题提供了高效方案。

Abstract: Quantum state tomography (QST) is a fundamental technique for estimating the
state of a quantum system from measured data and plays a crucial role in
evaluating the performance of quantum devices. However, standard estimation
methods become impractical due to the exponential growth of parameters in the
state representation. In this work, we address this challenge by parameterizing
the state using a low-rank block tensor train decomposition and demonstrate
that our approach is both memory- and computationally efficient. This framework
applies to a broad class of quantum states that can be well approximated by
low-rank decompositions, including pure states, nearly pure states, and ground
states of Hamiltonians.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [420] [Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics](https://arxiv.org/abs/2506.22520)
*Mustafa Demir,Jacob Miratsky,Jonathan Nguyen,Chun Kit Chan,Punya Mishra,Abhishek Singharoy*

Key words: AI导师，学生好奇心，互动分子动力学，团队表现，CRQA

TL;DR: 研究了AI导师在学生互动分子动力学任务中对好奇心和学习效果的影响，发现AI能激发学生提问并提升团队表现。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 探讨AI如何在互动分子动力学环境中通过触发好奇心的行为促进学生的主动学习和团队协作。

Method: 采用混合方法探索设计，11名高中生参与4个分子动力学任务，通过实时观察和CRQA分析团队表现和沟通行为。

Result: 高效团队表现出更好的任务完成度和认知复杂度，AI的干预与高级问题相关，CRQA显示学生-AI交互的动态同步。

Conclusion: AI作为队友和教育者的双重角色能提供适应性反馈，维持学生的好奇心和参与度。

Abstract: This study examines the impact of an Artificial Intelligence tutor teammate
(AI) on student curiosity-driven engagement and learning effectiveness during
Interactive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics
platform. It explores the role of the AI's curiosity-triggering and response
behaviors in stimulating and sustaining student curiosity, affecting the
frequency and complexity of student-initiated questions. The study further
assesses how AI interventions shape student engagement, foster discovery
curiosity, and enhance team performance within the IMD learning environment.
Using a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI
tutor teammate's behavior through a large language model. By employing a
mixed-methods exploratory design, a total of 11 high school students
participated in four IMD tasks that involved molecular visualization and
calculations, which increased in complexity over a 60-minute period. Team
performance was evaluated through real-time observation and recordings, whereas
team communication was measured by question complexity and AI's
curiosity-triggering and response behaviors. Cross Recurrence Quantification
Analysis (CRQA) metrics reflected structural alignment in coordination and were
linked to communication behaviors. High-performing teams exhibited superior
task completion, deeper understanding, and increased engagement. Advanced
questions were associated with AI curiosity-triggering, indicating heightened
engagement and cognitive complexity. CRQA metrics highlighted dynamic
synchronization in student-AI interactions, emphasizing structured yet adaptive
engagement to promote curiosity. These proof-of-concept findings suggest that
the AI's dual role as a teammate and educator indicates its capacity to provide
adaptive feedback, sustaining engagement and epistemic curiosity.

</details>


### [421] [Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions](https://arxiv.org/abs/2506.22941)
*Kaixuan Wang,Jason T. Jacques,Chenxin Diao*

Key words: 大语言模型, 减害, 信息提供, 协同设计, 伦理挑战

TL;DR: 论文探讨了如何利用大语言模型（LLM）为药物使用者提供更好的减害信息，同时解决适应性、可及性和污名化等问题。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 由于现有在线渠道无法满足药物使用者多样且动态的信息需求，研究人员希望通过LLM改善信息提供方式。

Method: 通过定性研讨会，集合学者、减害从业者和社区管理员，探索LLM的应用场景和设计考量。

Result: LLM能够减轻信息障碍（如多语言互动和反污名化），但需解决伦理对齐、上下文理解和运行边界等问题。

Conclusion: 与专家和使用者协同设计是开发安全、负责任LLM系统的关键。

Abstract: Access to accurate and actionable harm reduction information can directly
impact the health outcomes of People Who Use Drugs (PWUD), yet existing online
channels often fail to meet their diverse and dynamic needs due to limitations
in adaptability, accessibility, and the pervasive impact of stigma. Large
Language Models (LLMs) present a novel opportunity to enhance information
provision, but their application in such a high-stakes domain is under-explored
and presents socio-technical challenges. This paper investigates how LLMs can
be responsibly designed to support the information needs of PWUD. Through a
qualitative workshop involving diverse stakeholder groups (academics, harm
reduction practitioners, and an online community moderator), we explored LLM
capabilities, identified potential use cases, and delineated core design
considerations. Our findings reveal that while LLMs can address some existing
information barriers (e.g., by offering responsive, multilingual, and
potentially less stigmatising interactions), their effectiveness is contingent
upon overcoming challenges related to ethical alignment with harm reduction
principles, nuanced contextual understanding, effective communication, and
clearly defined operational boundaries. We articulate design pathways
emphasising collaborative co-design with experts and PWUD to develop LLM
systems that are helpful, safe, and responsibly governed. This work contributes
empirically grounded insights and actionable design considerations for the
responsible development of LLMs as supportive tools within the harm reduction
ecosystem.

</details>


### [422] [Against 'softmaxing' culture](https://arxiv.org/abs/2506.22968)
*Daniel Mwesigwa*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: AI is flattening culture. Evaluations of "culture" are showing the myriad
ways in which large AI models are homogenizing language and culture, averaging
out rich linguistic differences into generic expressions. I call this
phenomenon "softmaxing culture," and it is one of the fundamental challenges
facing AI evaluations today. Efforts to improve and strengthen evaluations of
culture are central to the project of cultural alignment in large AI systems.
This position paper argues that machine learning (ML) and human-computer
interaction (HCI) approaches to evaluation are limited. I propose two key
shifts. First, instead of asking "what is culture?" at the start of system
evaluations, I propose beginning with the question: "when is culture?" Second,
while I acknowledge the philosophical claim that cultural universals exist, the
challenge is not simply to describe them, but to situate them in relation to
their particulars. Taken together, these conceptual shifts invite evaluation
approaches that move beyond technical requirements, toward perspectives more
responsive to the complexities of culture.

</details>


### [423] [CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding](https://arxiv.org/abs/2506.23075)
*Yuchen Zhou,Jiamin Wu,Zichen Ren,Zhouheng Yao,Weiheng Lu,Kunyu Peng,Qihao Zheng,Chunfeng Song,Wanli Ouyang,Chao Gou*

Key words: EEG, brain activity, cross-scale, spatiotemporal, foundation model

TL;DR: CSBrain是一种跨尺度的时空脑基础模型，通过多尺度特征和结构化稀疏注意力优化EEG信号解码。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: EEG信号的跨尺度时空结构是核心属性，现有模型忽视这一多样性导致表现不佳。

Method: 提出Cross-scale Spatiotemporal Tokenization (CST)和Structured Sparse Attention (SSA)，交替堆叠以整合多尺度依赖。

Result: 在11个EEG任务和16个数据集上优于任务特定模型和基础模型。

Conclusion: 跨尺度建模是关键归纳偏差，CSBrain为未来脑-AI研究提供了稳健基础。

Abstract: Understanding and decoding brain activity from electroencephalography (EEG)
signals is a fundamental challenge in neuroscience and AI, with applications in
cognition, emotion recognition, diagnosis, and brain-computer interfaces. While
recent EEG foundation models advance generalized decoding via unified
architectures and large-scale pretraining, they adopt a scale-agnostic dense
modeling paradigm inherited from NLP and vision. This design neglects a core
property of neural activity: cross-scale spatiotemporal structure. EEG task
patterns span a wide range of temporal and spatial scales, from short bursts to
slow rhythms, and from localized cortical responses to distributed
interactions. Ignoring this diversity leads to suboptimal representations and
weak generalization. We propose CSBrain, a Cross-scale Spatiotemporal Brain
foundation model for generalized EEG decoding. CSBrain introduces: (i)
Cross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale
features from localized temporal windows and anatomical brain regions into
compact scale-aware tokens; and (ii) Structured Sparse Attention (SSA), which
captures cross-window and cross-region dependencies, enhancing scale diversity
while removing spurious correlations. CST and SSA are alternately stacked to
progressively integrate multi-scale dependencies. Experiments on 11 EEG tasks
across 16 datasets show that CSBrain consistently outperforms task-specific and
foundation model baselines. These results establish cross-scale modeling as a
key inductive bias and position CSBrain as a robust backbone for future
brain-AI research.

</details>


### [424] [Neuro-Informed Joint Learning Enhances Cognitive Workload Decoding in Portable BCIs](https://arxiv.org/abs/2506.23458)
*Xiaoxiao Yang,Chan Feng,Jiancheng Chen*

Key words: 便携式EEG, 认知负荷检测, 自监督学习, 联合学习, Muse头带

TL;DR: 论文提出MuseCogNet，一种融合自监督和监督学习的联合框架，用于提升便携式EEG设备的认知负荷检测性能。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 便携式EEG设备（如Muse头带）的信号非平稳性限制了数据准确性和解码性能，需解决便携性与性能的权衡问题。

Method: 提出MuseCogNet框架，结合自监督重建损失（基于平均池化）和监督交叉熵损失，模仿人类注意力机制。

Result: 在公开Muse数据集上显著优于现有方法，为生态场景中的神经认知监测提供可行方案。

Conclusion: MuseCogNet通过联合学习框架有效提升了便携式EEG设备的性能，为其在真实场景中的应用铺平道路。

Abstract: Portable and wearable consumer-grade electroencephalography (EEG) devices,
like Muse headbands, offer unprecedented mobility for daily brain-computer
interface (BCI) applications, including cognitive load detection. However, the
exacerbated non-stationarity in portable EEG signals constrains data fidelity
and decoding accuracy, creating a fundamental trade-off between portability and
performance. To mitigate such limitation, we propose MuseCogNet (Muse-based
Cognitive Network), a unified joint learning framework integrating
self-supervised and supervised training paradigms. In particular, we introduce
an EEG-grounded self-supervised reconstruction loss based on average pooling to
capture robust neurophysiological patterns, while cross-entropy loss refines
task-specific cognitive discriminants. This joint learning framework resembles
the bottom-up and top-down attention in humans, enabling MuseCogNet to
significantly outperform state-of-the-art methods on a publicly available Muse
dataset and establish an implementable pathway for neurocognitive monitoring in
ecological settings.

</details>


### [425] [Interactive Reasoning: Visualizing and Controlling Chain-of-Thought Reasoning in Large Language Models](https://arxiv.org/abs/2506.23678)
*Rock Yuren Pang,K. J. Kevin Feng,Shangbin Feng,Chu Li,Weijia Shi,Yulia Tsvetkov,Jeffrey Heer,Katharina Reinecke*

Key words: 大语言模型, 思维链, 交互式推理, 可视化, 用户反馈

TL;DR: 论文提出一种交互式推理方法，通过可视化思维链内容并允许用户修改，提升大模型输出的质量和透明度。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 目前大语言模型的思维链（CoT）输出冗长且缺乏组织，难以审查且不支持用户反馈，亟需改进。

Method: 设计基于层次化主题的可视化交互界面（Hippo原型），使用户能够审查和修改思维链内容。

Result: 用户研究（16人）表明，该方法能快速识别错误、高效定制输出并增强对模型推理的理解。

Conclusion: 交互式推理将用户监督引入模型推理过程，为大语言模型的应用提供新范式。

Abstract: The output quality of large language models (LLMs) can be improved via
"reasoning": generating segments of chain-of-thought (CoT) content to further
condition the model prior to producing user-facing output. While these chains
contain valuable information, they are verbose and lack explicit organization,
making them tedious to review. Moreover, they lack opportunities for user
feedback, such as to remove unwanted considerations, add desired ones, or
clarify unclear assumptions. We introduce Interactive Reasoning, an interaction
design that visualizes chain-of-thought outputs as a hierarchy of topics and
enables user review and modification. We implement interactive reasoning in
Hippo, a prototype for AI-assisted decision making in the face of uncertain
trade-offs. In a user study with 16 participants, we find that interactive
reasoning in Hippo allows users to quickly identify and interrupt erroneous
generations, efficiently steer the model towards customized responses, and
better understand both model reasoning and model outputs. Our work contributes
to a new paradigm that incorporates user oversight into LLM reasoning
processes.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [426] [Unsupervised Learning-Based Joint Resource Allocation and Beamforming Design for RIS-Assisted MISO-OFDMA Systems](https://arxiv.org/abs/2506.22448)
*Yu Ma,Xingyu Zhou,Xiao Li,Le Liang,Shi Jin*

Key words: RIS, 6G, 无监督学习, 资源分配, MISO-OFDMA

TL;DR: 论文提出了一种基于无监督学习的双阶段框架，用于优化RIS辅助MISO-OFDMA系统中的波束成形和资源分配，显著降低了计算时间。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 解决RIS辅助系统中资源分配的高复杂性和计算成本问题。

Method: 采用两阶段框架：BeamNet预测RIS相位，AllocationNet分配资源块；结合最大比传输和注水算法。

Result: 仿真显示方法达到SCA基线99.93%的速率，仅需0.036%的运行时间。

Conclusion: 提出的方法在保证性能的同时显著提升了计算效率。

Abstract: Reconfigurable intelligent surfaces (RIS) are key enablers for 6G wireless
systems. This paper studies downlink transmission in an RIS-assisted MISO-OFDMA
system, addressing resource allocation challenges. A two-stage unsupervised
learning-based framework is proposed to jointly design RIS phase shifts, BS
beamforming, and resource block (RB) allocation. The framework includes
BeamNet, which predicts RIS phase shifts from CSI, and AllocationNet, which
allocates RBs using equivalent CSI derived from BeamNet outputs. Active
beamforming is implemented via maximum ratio transmission and water-filling. To
handle discrete constraints while ensuring differentiability, quantization and
the Gumbel-softmax trick are adopted. A customized loss and phased training
enhance performance under QoS constraints. Simulations show the method achieves
99.93% of the sum rate of the SCA baseline with only 0.036% of its runtime, and
it remains robust across varying channel and user conditions.

</details>


### [427] [A Complex UNet Approach for Non-Invasive Fetal ECG Extraction Using Single-Channel Dry Textile Electrodes](https://arxiv.org/abs/2506.22457)
*Iulia Orvas,Andrei Radu,Alessandra Galli,Ana Neacsu,Elisabetta Peri*

Key words: 胎儿心电图, 干纺织电极, 复杂值去噪网络, 家庭监测, 非侵入性

TL;DR: 该论文提出了一种基于AI的新方法，使用干纺织电极从单通道记录中提取胎儿心电图（fECG），通过复杂值去噪网络处理频谱图的实部和虚部，显著提升了准确性。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 实现家庭环境下使用舒适耐用的干纺织电极进行连续、非侵入性的胎儿健康监测，解决噪声和运动伪影带来的挑战。

Method: 采用复杂值去噪网络（Complex UNet）处理单通道记录的频谱图，同时考虑实部和虚部信息，并提出了新的模拟数据集来验证方法。

Result: 新方法在模拟和真实数据上均表现优异，实现了最先进的fECG提取和R峰值检测结果。

Conclusion: 该研究首次成功从单通道干纺织电极记录中提取fECG，为非侵入性和自助式fECG提取提供了重要进展。

Abstract: Continuous, non-invasive pregnancy monitoring is crucial for minimising
potential complications. The fetal electrocardiogram (fECG) represents a
promising tool for assessing fetal health beyond clinical environments.
Home-based monitoring necessitates the use of a minimal number of comfortable
and durable electrodes, such as dry textile electrodes. However, this setup
presents many challenges, including increased noise and motion artefacts, which
complicate the accurate extraction of fECG signals. To overcome these
challenges, we introduce a pioneering method for extracting fECG from
single-channel recordings obtained using dry textile electrodes using AI
techniques. We created a new dataset by simulating abdominal recordings,
including noise closely resembling real-world characteristics of in-vivo
recordings through dry textile electrodes, alongside mECG and fECG. To ensure
the reliability of the extracted fECG, we propose an innovative pipeline based
on a complex-valued denoising network, Complex UNet. Unlike previous approaches
that focused solely on signal magnitude, our method processes both real and
imaginary components of the spectrogram, addressing phase information and
preventing incongruous predictions. We evaluated our novel pipeline against
traditional, well-established approaches, on both simulated and real data in
terms of fECG extraction and R-peak detection. The results showcase that our
suggested method achieves new state-of-the-art results, enabling an accurate
extraction of fECG morphology across all evaluated settings. This method is the
first to effectively extract fECG signals from single-channel recordings using
dry textile electrodes, making a significant advancement towards a fully
non-invasive and self-administered fECG extraction solution.

</details>


### [428] [Heart rate and respiratory rate prediction from noisy real-world smartphone based on Deep Learning methods](https://arxiv.org/abs/2506.22460)
*Ibne Farabi Shihab*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Using mobile phone video of the fingertip as a data source for estimating
vital signs such as heart rate (HR) and respiratory rate (RR) during daily life
has long been suggested. While existing literature indicates that these
estimates are accurate to within several beats or breaths per minute, the data
used to draw these conclusions are typically collected in laboratory
environments under careful experimental control, and yet the results are
assumed to generalize to daily life. In an effort to test it, a team of
researchers collected a large dataset of mobile phone video recordings made
during daily life and annotated with ground truth HR and RR labels from N=111
participants. They found that traditional algorithm performance on the
fingerprint videos is worse than previously reported (7 times and 13 times
worse for RR and HR, respectively). Fortunately, recent advancements in deep
learning, especially in convolutional neural networks (CNNs), offer a promising
solution to improve this performance. This study proposes a new method for
estimating HR and RR using a novel 3D deep CNN, demonstrating a reduced error
in estimated HR by 68% and RR by 75%. These promising results suggest that
regressor-based deep learning approaches should be used in estimating HR and
RR.

</details>


### [429] [Machine Learning for Proactive Groundwater Management: Early Warning and Resource Allocation](https://arxiv.org/abs/2506.22461)
*Chuan Li,Ruoxuan Yang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Groundwater supports ecosystems, agriculture, and drinking water supplies
worldwide, yet effective monitoring remains challenging due to sparse data,
computational constraints, and delayed outputs from traditional approaches. We
develop a machine learning pipeline that predicts groundwater level categories
using climate data, hydro-meteorological records, and physiographic attributes
processed through AutoGluon's automated ensemble framework. Our approach
integrates geospatial preprocessing, domain-driven feature engineering, and
automated model selection to overcome conventional monitoring limitations.
Applied to a large-scale French dataset (n $>$ 3,440,000 observations from
1,500+ wells), the model achieves weighted F\_1 scores of 0.927 on validation
data and 0.67 on temporally distinct test data. Scenario-based evaluations
demonstrate practical utility for early warning systems and water allocation
decisions under changing climate conditions. The open-source implementation
provides a scalable framework for integrating machine learning into national
groundwater monitoring networks, enabling more responsive and data-driven water
management strategies.

</details>


### [430] [Privacy-aware IoT Fall Detection Services For Aging in Place](https://arxiv.org/abs/2506.22462)
*Abdallah Lakhdari,Jiajie Li,Amani Abusafia,Athman Bouguettaya*

Key words: 跌倒检测、物联网、数据增强、隐私保护、超宽带雷达

TL;DR: 论文提出了一种基于物联网的跌倒检测服务框架（FDaaS），通过超宽带雷达传感器和生成式预训练转换器（FD-GPT），解决了数据稀缺和隐私问题，并在实验中取得了90.72%的准确率和89.33%的精确率。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 随着老龄化人口的增加，跌倒检测对老年人的独立和安全生活至关重要。现有方法在数据稀缺和隐私保护方面存在挑战。

Method: 设计了面向服务的架构，利用超宽带雷达传感器作为物联网健康感知服务，并结合FD-GPT进行数据增强。提出了收集老年人日常活动和跌倒事件数据的协议。

Result: 实验结果显示，该方法在区分跌倒事件和日常活动方面达到了90.72%的准确率和89.33%的精确率。

Conclusion: 提出的FDaaS框架结合了隐私保护和数据增强技术，有效解决了跌倒检测中的数据稀缺问题，并在实际应用中表现出色。

Abstract: Fall detection is critical to support the growing elderly population,
projected to reach 2.1 billion by 2050. However, existing methods often face
data scarcity challenges or compromise privacy. We propose a novel IoT-based
Fall Detection as a Service (FDaaS) framework to assist the elderly in living
independently and safely by accurately detecting falls. We design a
service-oriented architecture that leverages Ultra-wideband (UWB) radar sensors
as an IoT health-sensing service, ensuring privacy and minimal intrusion. We
address the challenges of data scarcity by utilizing a Fall Detection
Generative Pre-trained Transformer (FD-GPT) that uses augmentation techniques.
We developed a protocol to collect a comprehensive dataset of the elderly daily
activities and fall events. This resulted in a real dataset that carefully
mimics the elderly's routine. We rigorously evaluate and compare various models
using this dataset. Experimental results show our approach achieves 90.72%
accuracy and 89.33% precision in distinguishing between fall events and regular
activities of daily living.

</details>


### [431] [Dimensionality Reduction on IoT Monitoring Data of Smart Building for Energy Consumption Forecasting](https://arxiv.org/abs/2506.22468)
*Konstantinos Koutras,Agorakis Bompotas,Constantinos Halkiopoulos,Athanasios Kalogeras,Christos Alexakos*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Internet of Things (IoT) plays a major role today in smart building
infrastructures, from simple smart-home applications, to more sophisticated
industrial type installations. The vast amounts of data generated from relevant
systems can be processed in different ways revealing important information.
This is especially true in the era of edge computing, when advanced data
analysis and decision-making is gradually moving to the edge of the network
where devices are generally characterised by low computing resources. In this
context, one of the emerging main challenges is related to maintaining data
analysis accuracy even with less data that can be efficiently handled by low
resource devices. The present work focuses on correlation analysis of data
retrieved from a pilot IoT network installation monitoring a small smart office
by means of environmental and energy consumption sensors. The research
motivation was to find statistical correlation between the monitoring variables
that will allow the use of machine learning (ML) prediction algorithms for
energy consumption reducing input parameters. For this to happen, a series of
hypothesis tests for the correlation of three different environmental variables
with the energy consumption were carried out. A total of ninety tests were
performed, thirty for each pair of variables. In these tests, p-values showed
the existence of strong or semi-strong correlation with two environmental
variables, and of a weak correlation with a third one. Using the proposed
methodology, we manage without examining the entire data set to exclude weak
correlated variables while keeping the same score of accuracy.

</details>


### [432] [Masked Autoencoders that Feel the Heart: Unveiling Simplicity Bias for ECG Analyses](https://arxiv.org/abs/2506.22495)
*He-Yang Xu,Hongxiang Gao,Yuwen Li,Xiu-Shen Wei,Chengyu Liu*

Key words: 心电图分析, 简单性偏差, 自监督学习, 时间频率特征, 多粒度学习

TL;DR: 该论文探讨了心电图（ECG）分析中的“简单性偏差”（SB）问题，即监督模型倾向于忽略细微但临床关键的信号。研究发现自监督学习（SSL）可以缓解SB，并提出了一种结合时间频率感知滤波器和多粒度原型重建的新方法，通过大规模数据集验证其有效性和先进性。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: ECG模型的简单性偏差（SB）导致其忽略细微但临床关键的信号，限制了诊断性能。本文旨在通过自监督学习解决这一问题。

Method: 提出了一种结合时间频率感知滤波器和多粒度原型重建的自监督学习方法，以捕捉ECG信号的动态特性并减少SB。

Result: 实验表明，该方法在六个ECG数据集上的三个下游任务中有效减少了SB，并达到了最先进的性能。

Conclusion: 自监督学习为克服ECG分析中的简单性偏差提供了有效途径，提出的方法在性能和泛化能力上表现优异。

Abstract: The diagnostic value of electrocardiogram (ECG) lies in its dynamic
characteristics, ranging from rhythm fluctuations to subtle waveform
deformations that evolve across time and frequency domains. However, supervised
ECG models tend to overfit dominant and repetitive patterns, overlooking
fine-grained but clinically critical cues, a phenomenon known as Simplicity
Bias (SB), where models favor easily learnable signals over subtle but
informative ones. In this work, we first empirically demonstrate the presence
of SB in ECG analyses and its negative impact on diagnostic performance, while
simultaneously discovering that self-supervised learning (SSL) can alleviate
it, providing a promising direction for tackling the bias. Following the SSL
paradigm, we propose a novel method comprising two key components: 1)
Temporal-Frequency aware Filters to capture temporal-frequency features
reflecting the dynamic characteristics of ECG signals, and 2) building on this,
Multi-Grained Prototype Reconstruction for coarse and fine representation
learning across dual domains, further mitigating SB. To advance SSL in ECG
analyses, we curate a large-scale multi-site ECG dataset with 1.53 million
recordings from over 300 clinical centers. Experiments on three downstream
tasks across six ECG datasets demonstrate that our method effectively reduces
SB and achieves state-of-the-art performance. Code and dataset will be released
publicly.

</details>


### [433] [Microelectrode Signal Dynamics as Biomarkers of Subthalamic Nucleus Entry on Deep Brain Stimulation: A Nonlinear Feature Approach](https://arxiv.org/abs/2506.22454)
*Ana Luiza S. Tavares,Artur Pedro M. Neto,Francinaldo L. Gomes,Paul Rodrigo dos Reis,Arthur G. da Silva,Antonio P. Junior,Bruno D. Gomes*

Key words: Deep Brain Stimulation, Parkinson's disease, nonlinear dynamics, entropy, machine learning

TL;DR: 提出一种基于非线性动力学和熵指标的定量框架，用于区分STN内外的神经活动，提高了DBS手术中电极植入的准确性。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 当前DBS手术中STN定位依赖主观信号解释，需要更客观的定量方法。

Method: 预处理MER数据，提取非线性动态和熵特征，训练多种监督分类器，评估其性能。

Result: 熵和非线性特征组合表现最佳，Extra Trees分类器F1分数0.902±0.027，ROC AUC 0.887±0.055。

Conclusion: 非线性与熵特征支持DBS手术中实时数据驱动决策。

Abstract: Accurate intraoperative localization of the subthalamic nucleus (STN) is
essential for the efficacy of Deep Brain Stimulation (DBS) in patients with
Parkinson's disease. While microelectrode recordings (MERs) provide rich
electrophysiological information during DBS electrode implantation, current
localization practices often rely on subjective interpretation of signal
features. In this study, we propose a quantitative framework that leverages
nonlinear dynamics and entropy-based metrics to classify neural activity
recorded inside versus outside the STN. MER data from three patients were
preprocessed using a robust artifact correction pipeline, segmented, and
labelled based on surgical annotations. A comprehensive set of recurrence
quantification analysis, nonlinear, and entropy features were extracted from
each segment. Multiple supervised classifiers were trained on every combination
of feature domains using stratified 10-fold cross-validation, followed by
statistical comparison using paired Wilcoxon signed-rank tests with
Holm-Bonferroni correction. The combination of entropy and nonlinear features
yielded the highest discriminative power, and the Extra Trees classifier
emerged as the best model with a cross-validated F1-score of 0.902+/-0.027 and
ROC AUC of 0.887+/-0.055. Final evaluation on a 20% hold-out test set confirmed
robust generalization (F1= 0.922, ROC AUC = 0.941). These results highlight the
potential of nonlinear and entropy signal descriptors in supporting real-time,
data-driven decision-making during DBS surgeries

</details>


### [434] [Data Normalization Strategies for EEG Deep Learning](https://arxiv.org/abs/2506.22455)
*Dung Truong,Arnaud Delorme*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Normalization is a critical yet often overlooked component in the
preprocessing pipeline for EEG deep learning applications. The rise of
large-scale pretraining paradigms such as self-supervised learning (SSL)
introduces a new set of tasks whose nature is substantially different from
supervised training common in EEG deep learning applications. This raises new
questions about optimal normalization strategies for the applicable task. In
this study, we systematically evaluate the impact of normalization granularity
(recording vs. window level) and scope (cross-channel vs. within-channel) on
both supervised (age and gender prediction) and self-supervised (Contrastive
Predictive Coding) tasks. Using high-density resting-state EEG from 2,836
subjects in the Healthy Brain Network dataset, we show that optimal
normalization strategies differ significantly between training paradigms.
Window-level within-channel normalization yields the best performance in
supervised tasks, while minimal or cross-channel normalization at the window
level is more effective for SSL. These results underscore the necessity of
task-specific normalization choices and challenge the assumption that a
universal normalization strategy can generalize across learning settings. Our
findings provide practical insights for developing robust EEG deep learning
pipelines as the field shifts toward large-scale, foundation model training.

</details>


### [435] [Physics-Embedded Neural Networks for sEMG-based Continuous Motion Estimation](https://arxiv.org/abs/2506.22459)
*Wending Heng,Chaoyuan Liang,Yihui Zhao,Zhiqiang Zhang,Glen Cooper,Zhenhong Li*

Key words: 表面肌电信号, 物理嵌入神经网络, 运动意图解码, 康复机器人, 残差学习

TL;DR: 这篇论文提出了一种新型的物理嵌入神经网络（PENN），结合了解释性肌肉骨骼动力学和数据驱动的残差学习，提高了运动意图解码的准确性和生理一致性。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 解决现有表面肌电信号（sEMG）运动估计方法依赖难以校准的特定对象模型或缺乏生理一致性的纯数据驱动模型的问题。

Method: 设计了一种结合肌肉骨骼前向动力学和数据驱动残差学习的PENN，采用递归时间结构和轻量级卷积神经网络进行残差校正，并通过两阶段训练策略优化模型。

Result: 在六名健康受试者上的实验表明，PENN在均方根误差（RMSE）和R²指标上优于现有基准方法。

Conclusion: PENN在保留生理一致性的同时实现了高精度的运动估计，为康复机器人和辅助技术提供了更可靠的解决方案。

Abstract: Accurately decoding human motion intentions from surface electromyography
(sEMG) is essential for myoelectric control and has wide applications in
rehabilitation robotics and assistive technologies. However, existing
sEMG-based motion estimation methods often rely on subject-specific
musculoskeletal (MSK) models that are difficult to calibrate, or purely
data-driven models that lack physiological consistency. This paper introduces a
novel Physics-Embedded Neural Network (PENN) that combines interpretable MSK
forward-dynamics with data-driven residual learning, thereby preserving
physiological consistency while achieving accurate motion estimation. The PENN
employs a recursive temporal structure to propagate historical estimates and a
lightweight convolutional neural network for residual correction, leading to
robust and temporally coherent estimations. A two-phase training strategy is
designed for PENN. Experimental evaluations on six healthy subjects show that
PENN outperforms state-of-the-art baseline methods in both root mean square
error (RMSE) and $R^2$ metrics.

</details>


### [436] [An Interpretable Transformer-Based Foundation Model for Cross-Procedural Skill Assessment Using Raw fNIRS Signals](https://arxiv.org/abs/2506.22476)
*A. Subedi,S. De,L. Cavuoto,S. Schwaitzberg,M. Hackett,J. Norfleet*

Key words: fNIRS, 跨程序评估, Transformer, 可解释性, 自监督学习

TL;DR: 提出了一种基于Transformer的基础模型，用于fNIRS信号处理的跨程序技能评估，具有高准确性和可解释性。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 现有fNIRS方法任务特定、预处理繁琐且通用性差，需开发更鲁棒的跨程序评估模型。

Method: 使用自监督学习预训练模型，通过轻量适配器模块泛化到新任务，并引入通道注意力机制提高可解释性。

Result: 模型在多个任务中分类准确率>88%，MCC>0.91，且在新任务上仅需少量样本即可达到AUC>87%。

Conclusion: 该模型为高精度、可解释的跨程序技能评估提供了一种高效解决方案。

Abstract: Objective skill assessment in high-stakes procedural environments requires
models that not only decode underlying cognitive and motor processes but also
generalize across tasks, individuals, and experimental contexts. While prior
work has demonstrated the potential of functional near-infrared spectroscopy
(fNIRS) for evaluating cognitive-motor performance, existing approaches are
often task-specific, rely on extensive preprocessing, and lack robustness to
new procedures or conditions. Here, we introduce an interpretable
transformer-based foundation model trained on minimally processed fNIRS signals
for cross-procedural skill assessment. Pretrained using self-supervised
learning on data from laparoscopic surgical tasks and endotracheal intubation
(ETI), the model achieves greater than 88% classification accuracy on all
tasks, with Matthews Correlation Coefficient exceeding 0.91 on ETI. It
generalizes to a novel emergency airway procedure--cricothyrotomy--using fewer
than 30 labeled samples and a lightweight (less than 2k parameter) adapter
module, attaining an AUC greater than 87%. Interpretability is achieved via a
novel channel attention mechanism--developed specifically for fNIRS--that
identifies functionally coherent prefrontal sub-networks validated through
ablation studies. Temporal attention patterns align with task-critical phases
and capture stress-induced changes in neural variability, offering insight into
dynamic cognitive states.

</details>


### [437] [Zero-Shot EEG-to-Gait Decoding via Phase-Aware Representation Learning](https://arxiv.org/abs/2506.22488)
*Xi Fu,Weibang Jiang,Rui Liu,Gernot R. Müller-Putz,Cuntai Guan*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Accurate decoding of lower-limb motion from EEG signals is essential for
advancing brain-computer interface (BCI) applications in movement intent
recognition and control. However, challenges persist in achieving causal,
phase-consistent predictions and in modeling both inter- and intra-subject
variability. To address these issues, we propose NeuroDyGait, a
domain-generalizable EEG-to-motion decoding framework that leverages structured
contrastive representation learning and relational domain modeling. The
proposed method employs relative contrastive learning to achieve semantic
alignment between EEG and motion embeddings. Furthermore, a multi-cycle gait
reconstruction objective is introduced to enforce temporal coherence and
maintain biomechanical consistency. To promote inter-session generalization,
during fine-tuning, a domain dynamic decoding mechanism adaptively assigns
session-specific prediction heads and learns to mix their outputs based on
inter-session relationships. NeuroDyGait enables zero-shot motion prediction
for unseen individuals without requiring adaptation and achieves superior
performance in cross-subject gait decoding on benchmark datasets. Additionally,
it demonstrates strong phase-detection capabilities even without explicit phase
supervision during training. These findings highlight the potential of
relational domain learning in enabling scalable, target-free deployment of
BCIs.

</details>


### [438] [MENGLAN: Multiscale Enhanced Nonparametric Gas Analyzer with Lightweight Architecture and Networks](https://arxiv.org/abs/2506.22490)
*Zhenke Duan,Jiqun Pan,Jiani Tu*

Key words: 乙烯浓度检测, 非参数气体分析仪, 双流结构, 多头注意力机制, 特征重激活模块

TL;DR: MENGLAN是一种用于混合气体中乙烯浓度检测的新型非参数气体分析仪，具有高精度、实时性和轻量化的特点。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 传统乙烯浓度检测方法成本高且复杂，限制了实际应用，因此研究出一种更高效、经济的方法至关重要。

Method: MENGLAN采用双流结构、混合多头注意力机制和特征重激活模块，实现了多尺度增强的非参数气体分析。

Result: MENGLAN性能优异，计算需求低，且部署性更强。

Conclusion: MENGLAN为乙烯浓度检测提供了一种高效、经济的解决方案。

Abstract: Accurate detection of ethylene concentrations in mixed gases is crucial in
chemical production for safety and health purposes. Traditional methods are
hindered by high cost and complexity, limiting their practical application.
This study proposes MENGLAN, a Multiscale Enhanced Nonparametric Gas Analyzer
that integrates a dual-stream structure, a Hybrid Multi-Head Attention
mechanism, and a Feature Reactivation Module to enable real-time, lightweight,
and high-precision ethylene concentration prediction. Results show that MENGLAN
achieves superior performance, reduced computational demand, and enhanced
deployability compared to existing methods.

</details>


### [439] [Multi-Branch DNN and CRLB-Ratio-Weight Fusion for Enhanced DOA Sensing via a Massive H$^2$AD MIMO Receiver](https://arxiv.org/abs/2506.23203)
*Feng Shu,Jiatong Bai,Di Wu,Wei Zhu,Bin Deng,Fuhui Zhou,Jiangzhou Wang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As a green MIMO structure, massive H$^2$AD is viewed as a potential
technology for the future 6G wireless network. For such a structure, it is a
challenging task to design a low-complexity and high-performance fusion of
target direction values sensed by different sub-array groups with fewer use of
prior knowledge. To address this issue, a lightweight Cramer-Rao lower bound
(CRLB)-ratio-weight fusion (WF) method is proposed, which approximates inverse
CRLB of each subarray using antenna number reciprocals to eliminate real-time
CRLB computation. This reduces complexity and prior knowledge dependence while
preserving fusion performance. Moreover, a multi-branch deep neural network
(MBDNN) is constructed to further enhance direction-of-arrival (DOA) sensing by
leveraging candidate angles from multiple subarrays. The subarray-specific
branch networks are integrated with a shared regression module to effectively
eliminate pseudo-solutions and fuse true angles. Simulation results show that
the proposed CRLB-ratio-WF method achieves DOA sensing performance comparable
to CRLB-based methods, while significantly reducing the reliance on prior
knowledge. More notably, the proposed MBDNN has superior performance in low-SNR
ranges. At SNR $= -15$ dB, it achieves an order-of-magnitude improvement in
estimation accuracy compared to CRLB-ratio-WF method.

</details>


### [440] [Differentiable Radar Ambiguity Functions: Mathematical Formulation and Computational Implementation](https://arxiv.org/abs/2506.22935)
*Marc Bara Iniesta*

Key words: 雷达模糊函数, 可微分性, 自动微分, 深度学习, 信号处理

TL;DR: 本文提出了一种可微分雷达模糊函数的数学框架（GRAF），解决了传统模糊函数不可微分的问题，使其能与梯度优化和深度学习结合。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 传统雷达模糊函数的设计涉及不可微分操作，无法与现代梯度优化方法和机器学习框架结合。

Method: 采用Wirtinger微积分处理复值梯度，并行FFT提高效率，确保数值稳定性，并实现可组合性。

Result: GRAF实现了可微分模糊函数，支持端到端雷达系统优化和神经网络波形生成。

Conclusion: 该工作为雷达波形设计中的现代机器学习应用奠定了数学和计算基础。

Abstract: The ambiguity function is fundamental to radar waveform design,
characterizing range and Doppler resolution capabilities. However, its
traditional formulation involves non-differentiable operations, preventing
integration with gradient-based optimization methods and modern machine
learning frameworks. This paper presents the first complete mathematical
framework and computational implementation for differentiable radar ambiguity
functions. Our approach addresses the fundamental technical challenges that
have prevented the radar community from leveraging automatic differentiation:
proper handling of complex-valued gradients using Wirtinger calculus, efficient
computation through parallelized FFT operations, numerical stability throughout
cascaded operations, and composability with arbitrary differentiable
operations. We term this approach GRAF (Gradient-based Radar Ambiguity
Functions), which reformulates the ambiguity function computation to maintain
mathematical equivalence while enabling gradient flow through the entire
pipeline. The resulting implementation provides a general-purpose
differentiable ambiguity function compatible with modern automatic
differentiation frameworks, enabling new research directions including neural
network-based waveform generation with ambiguity constraints, end-to-end
optimization of radar systems, and integration of classical radar theory with
modern deep learning. We provide complete implementation details and
demonstrate computational efficiency suitable for practical applications. This
work establishes the mathematical and computational foundation for applying
modern machine learning techniques to radar waveform design, bridging classical
radar signal processing with automatic differentiation frameworks.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [441] [Deep Learning for Optical Misalignment Diagnostics in Multi-Lens Imaging Systems](https://arxiv.org/abs/2506.23173)
*Tomer Slor,Dean Oren,Shira Baneth,Tom Coen,Haim Suchowski*

Key words: 光学工程,深度学,逆向设计,多透镜系统,制造质量控制

TL;DR: 该论文提出了两种基于深度学习的逆向设计方法，用于仅通过光学测量诊断多透镜系统中的错位问题，展示了在精密成像制造和质量控制中的潜力。

<details>
  <summary>Details</summary>

Main category: physics.optics

Motivation: 在光工程领域，多透镜成像系统的精确对准至关重要，但传统方法依赖专业设备且耗时，因此需要自动化和可扩展的解决方案。

Method: 使用两种互补的深度学习方法：一种是基于光线追踪的斑点图预测5自由度误差；另一种是基于物理模拟的灰度合成相机图像估计4自由度误差。

Result: 实验表明，方法在6透镜系统中实现了横向平移0.031mm和倾斜0.011°的平均绝对误差。

Conclusion: 这些方法为精密成像制造和质量控制提供了新思路。

Abstract: In the rapidly evolving field of optical engineering, precise alignment of
multi-lens imaging systems is critical yet challenging, as even minor
misalignments can significantly degrade performance. Traditional alignment
methods rely on specialized equipment and are time-consuming processes,
highlighting the need for automated and scalable solutions. We present two
complementary deep learning-based inverse-design methods for diagnosing
misalignments in multi-element lens systems using only optical measurements.
First, we use ray-traced spot diagrams to predict five-degree-of-freedom
(5-DOF) errors in a 6-lens photographic prime, achieving a mean absolute error
of 0.031mm in lateral translation and 0.011$^\circ$ in tilt. We also introduce
a physics-based simulation pipeline that utilizes grayscale synthetic camera
images, enabling a deep learning model to estimate 4-DOF, decenter and tilt
errors in both two- and six-lens multi-lens systems. These results show the
potential to reshape manufacturing and quality control in precision imaging.

</details>
