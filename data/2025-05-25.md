<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 130]
- [cs.LG](#cs.LG) [Total: 123]
- [cs.AI](#cs.AI) [Total: 58]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.ET](#cs.ET) [Total: 1]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [eess.AS](#eess.AS) [Total: 8]
- [hep-th](#hep-th) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 11]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cs.NE](#cs.NE) [Total: 7]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 58]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.CC](#cs.CC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [stat.ML](#stat.ML) [Total: 17]
- [cs.AR](#cs.AR) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.RO](#cs.RO) [Total: 9]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.SI](#cs.SI) [Total: 2]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.SE](#cs.SE) [Total: 4]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.CR](#cs.CR) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law](https://arxiv.org/abs/2505.15916)
*Juvenal Domingos Júnior,Augusto Faria,E. Seiti de Oliveira,Erick de Brito,Matheus Teotonio,Andre Assumpção,Diedre Carmo,Roberto Lotufo,Jayr Pereira*

Key words: BR-TaxQA-R, 巴西税法, QA, RAG, 法律问答

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了BR-TaxQA-R数据集，用于支持巴西个人所得税法的问答任务，并展示了基于RAG的问答系统在相关性和法律有效性上的表现。

Motivation: 为了解决在巴西税法领域中高质量问答数据集的缺乏，并探索AI在法律问答中的表现。

Method: 使用RAG流程，结合OpenAI嵌入和GPT-4o-mini生成答案，对比不同文本分割策略，并与商业工具如ChatGPT和Perplexity.ai进行基准测试。

Result: 自定义RAG流程在查询相关性上优于商业系统，而商业模型在事实准确性和流畅性上表现更好。

Conclusion: 在法律高风险的领域，如税法中，AI生成的答案需要结合人类专家的评估以确保法律有效性。

Abstract: This paper presents BR-TaxQA-R, a novel dataset designed to support question
answering with references in the context of Brazilian personal income tax law.
The dataset contains 715 questions from the 2024 official Q\&A document
published by Brazil's Internal Revenue Service, enriched with statutory norms
and administrative rulings from the Conselho Administrativo de Recursos Fiscais
(CARF). We implement a Retrieval-Augmented Generation (RAG) pipeline using
OpenAI embeddings for searching and GPT-4o-mini for answer generation. We
compare different text segmentation strategies and benchmark our system against
commercial tools such as ChatGPT and Perplexity.ai using RAGAS-based metrics.
Results show that our custom RAG pipeline outperforms commercial systems in
Response Relevancy, indicating stronger alignment with user queries, while
commercial models achieve higher scores in Factual Correctness and fluency.
These findings highlight a trade-off between legally grounded generation and
linguistic fluency. Crucially, we argue that human expert evaluation remains
essential to ensure the legal validity of AI-generated answers in high-stakes
domains such as taxation. BR-TaxQA-R is publicly available at
https://huggingface.co/datasets/unicamp-dl/BR-TaxQA-R.

</details>


### [2] [Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization](https://arxiv.org/abs/2505.15918)
*Aliakbar Nafar,Kristen Brent Venable,Zijun Cui,Parisa Kordjamshidi*

Key words: LLM, 贝叶斯网络, 概率知识, 参数化, 专家先验

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了利用大型语言模型（LLMs）生成贝叶斯网络（BN）中事件概率估计的潜力，通过实验验证其有效性。

Motivation: LLMs作为事实知识库的潜力已被证明，但其在生成现实世界事件的概率知识方面的能力尚不明确，因此需探索其在贝叶斯网络参数化中的应用。

Method: 通过查询LLMs获取事件的条件概率，用于参数化贝叶斯网络，并在多个领域的80个公开BN上进行实验比较。

Result: 实验表明，LLM生成的分布相比基线（如随机或均匀分布）更具意义，可作为专家先验减少系统偏差。

Conclusion: 结合LLM的概率知识与少量真实数据，为自动构建贝叶斯网络提供了新策略。

Abstract: Large Language Models (LLMs) have demonstrated potential as factual knowledge
bases; however, their capability to generate probabilistic knowledge about
real-world events remains understudied. This paper investigates using
probabilistic knowledge inherent in LLMs to derive probability estimates for
statements concerning events and their interrelationships captured via a
Bayesian Network (BN). Using LLMs in this context allows for the
parameterization of BNs, enabling probabilistic modeling within specific
domains. Experiments on eighty publicly available Bayesian Networks, from
healthcare to finance, demonstrate that querying LLMs about the conditional
probabilities of events provides meaningful results when compared to baselines,
including random and uniform distributions, as well as approaches based on
next-token generation probabilities. We explore how these LLM-derived
distributions can serve as expert priors to refine distributions extracted from
minimal data, significantly reducing systematic biases. Overall, this work
introduces a promising strategy for automatically constructing Bayesian
Networks by combining probabilistic knowledge extracted from LLMs with small
amounts of real-world data. Additionally, we evaluate several prompting
strategies for eliciting probabilistic knowledge from LLMs and establish the
first comprehensive baseline for assessing LLM performance in extracting
probabilistic knowledge.

</details>


### [3] [Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition](https://arxiv.org/abs/2505.15922)
*Dong Won Lee,Hae Won Park,Cynthia Breazeal,Louis-Philippe Morency*

Key words: 大语言模型, 奖励分解, 对话对齐, 多模态反馈, RL微调

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的奖励分解框架，仅需会话级反馈信号即可对齐对话代理，无需人工奖励构建或细粒度反馈。

Motivation: 旨在利用预训练大语言模型（LLM）的推理能力，从全局会话级反馈中分解出细粒度的局部奖励，从而降低人工干预成本。

Method: 通过冻结的LLM分解奖励，分为纯文本和多模态两种变体，后者结合行为线索（如语调、注视和面部表情），并将分解的奖励蒸馏为轻量级奖励模型用于RL微调。

Result: 在对话质量的人类评估中，两种变体均显著优于现有奖励分解方法，验证了LLM作为奖励分解器的有效性。

Conclusion: LLM是强大的奖励分解工具，可替代人工奖励构建和细粒度反馈，提升对话生成质量。

Abstract: We propose a large language model based reward decomposition framework for
aligning dialogue agents using only a single session-level feedback signal. We
leverage the reasoning capabilities of a frozen, pretrained large language
model (LLM) to infer fine-grained local implicit rewards by decomposing global,
session-level feedback. Our first text-only variant prompts the LLM to perform
reward decomposition using only the dialogue transcript. The second multimodal
variant incorporates additional behavioral cues, such as pitch, gaze, and
facial affect, expressed as natural language descriptions. These inferred
turn-level rewards are distilled into a lightweight reward model, which we
utilize for RL-based fine-tuning for dialogue generation. We evaluate both
text-only and multimodal variants against state-of-the-art reward decomposition
methods and demonstrate notable improvements in human evaluations of
conversation quality, suggesting that LLMs are strong reward decomposers that
obviate the need for manual reward shaping and granular human feedback.

</details>


### [4] [Citation Parsing and Analysis with Language Models](https://arxiv.org/abs/2505.15948)
*Parth Sarin,Juan Pablo Alperin*

Key words: 知识共享, 全球南方, 语言模型, 引用解析, 开源工具

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种基于开源语言模型的工具，用于标记和解析学术论文的引用，以提高全球知识共享网络的透明度，尤其关注全球南方学者被忽视的问题。

Motivation: 当前缺乏有效工具来追踪全球知识共享网络，尤其是全球南方学者的引用情况，这加剧了殖民式知识体系的不平等。

Method: 研究构建了包含纯文本和注释引用的数据集，并评估了多种开源语言模型在引用标记任务中的表现。

Result: 现有的语言模型（如Qwen3-0.6B）即使未经调优也能高精度解析引用字段，且小模型通过少量迭代即可达到高准确率。

Conclusion: 该工具可显著提升引用网络的准确性，改善研究索引和发现，同时支持元科学研究。

Abstract: A key type of resource needed to address global inequalities in knowledge
production and dissemination is a tool that can support journals in
understanding how knowledge circulates. The absence of such a tool has resulted
in comparatively less information about networks of knowledge sharing in the
Global South. In turn, this gap authorizes the exclusion of researchers and
scholars from the South in indexing services, reinforcing colonial arrangements
that de-center and minoritize those scholars. In order to support citation
network tracking on a global scale, we investigate the capacity of open-weight
language models to mark up manuscript citations in an indexable format. We
assembled a dataset of matched plaintext and annotated citations from preprints
and published research papers. Then, we evaluated a number of open-weight
language models on the annotation task. We find that, even out of the box,
today's language models achieve high levels of accuracy on identifying the
constituent components of each citation, outperforming state-of-the-art
methods. Moreover, the smallest model we evaluated, Qwen3-0.6B, can parse all
fields with high accuracy in $2^5$ passes, suggesting that post-training is
likely to be effective in producing small, robust citation parsing models. Such
a tool could greatly improve the fidelity of citation networks and thus
meaningfully improve research indexing and discovery, as well as further
metascientific research.

</details>


### [5] [Training Step-Level Reasoning Verifiers with Formal Verification Tools](https://arxiv.org/abs/2505.15960)
*Ryo Kamoi,Yusen Zhang,Nan Zhang,Sarkar Snigdha Sarathi Das,Rui Zhang*

Key words: Process Reward Models, 形式化验证, 步骤级反馈, LLM, 推理任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: FoVer通过形式化验证工具自动标注步骤级错误标签，训练PRM模型，提升其在多样化推理任务中的泛化能力，显著优于基线模型，并在多个基准测试中表现优异。

Motivation: 解决PRM模型训练中步骤级错误标签依赖昂贵人工标注的问题，并拓展其在非数学推理任务中的应用。

Method: 利用形式化验证工具（如Z3和Isabelle）自动生成错误标签，合成训练数据集，训练LLM-based PRM模型。

Result: FoVer训练的PRM在12个推理基准测试中表现优异，超越基线模型并与人工标注或更强模型训练的PRM竞争。

Conclusion: 自动标注的步骤级错误标签能有效训练PRM模型，且具备跨任务泛化能力。

Abstract: Process Reward Models (PRMs), which provide step-by-step feedback on the
reasoning generated by Large Language Models (LLMs), are receiving increasing
attention. However, two key research gaps remain: collecting accurate
step-level error labels for training typically requires costly human
annotation, and existing PRMs are limited to math reasoning problems. In
response to these gaps, this paper aims to address the challenges of automatic
dataset creation and the generalization of PRMs to diverse reasoning tasks. To
achieve this goal, we propose FoVer, an approach for training PRMs on
step-level error labels automatically annotated by formal verification tools,
such as Z3 for formal logic and Isabelle for theorem proof, which provide
automatic and accurate verification for symbolic tasks. Using this approach, we
synthesize a training dataset with error labels on LLM responses for formal
logic and theorem proof tasks without human annotation. Although this data
synthesis is feasible only for tasks compatible with formal verification, we
observe that LLM-based PRMs trained on our dataset exhibit cross-task
generalization, improving verification across diverse reasoning tasks.
Specifically, PRMs trained with FoVer significantly outperform baseline PRMs
based on the original LLMs and achieve competitive or superior results compared
to state-of-the-art PRMs trained on labels annotated by humans or stronger
models, as measured by step-level verification on ProcessBench and Best-of-K
performance across 12 reasoning benchmarks, including MATH, AIME, ANLI, MMLU,
and BBH. The datasets, models, and code are provided at
https://github.com/psunlpgroup/FoVer.

</details>


### [6] [Pre-training Large Memory Language Models with Internal and External Knowledge](https://arxiv.org/abs/2505.15962)
*Linxi Zhao,Sofian Zalouk,Christian K. Belardi,Justin Lovelace,Jin Peng Zhou,Kilian Q. Weinberger,Yoav Artzi,Jennifer J. Sun*

Key words: 语言模型, 知识库, 可解释性, 预训练, 知识编辑

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了大型记忆语言模型（LMLM），通过内外双存储方式解耦知识分布，提升模型的可编辑性与可验证性，性能接近大模型且具备透明知识管理。

Motivation: 传统神经语言模型的知识分散且不可控，无法高效验证或更新，需构建可显式编辑的知识库与模型。

Method: 预训练时屏蔽外部检索的知识值损失，强制模型学习外部查询而非依赖权重记忆。

Result: LMLM在标准评测中接近大模型性能，同时支持外部知识库的显式编辑和验证。

Conclusion: LMLM通过解耦知识存储实现了语言模型对知识的透明化管控，是知识管理方式的范式革新。

Abstract: Neural language models are black-boxes -- both linguistic patterns and
factual knowledge are distributed across billions of opaque parameters. This
entangled encoding makes it difficult to reliably inspect, verify, or update
specific facts. We propose a new class of language models, Large Memory
Language Models (LMLM) with a pre-training recipe that stores factual knowledge
in both internal weights and an external database. Our approach strategically
masks externally retrieved factual values from the training loss, thereby
teaching the model to perform targeted lookups rather than relying on
memorization in model weights. Our experiments demonstrate that LMLMs achieve
competitive performance compared to significantly larger, knowledge-dense LLMs
on standard benchmarks, while offering the advantages of explicit, editable,
and verifiable knowledge bases. This work represents a fundamental shift in how
language models interact with and manage factual knowledge.

</details>


### [7] [Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku](https://arxiv.org/abs/2505.15993)
*Anirudh Maiya,Razan Alghamdi,Maria Leonor Pacheco,Ashutosh Trivedi,Fabio Somenzi*

Key words: 大型语言模型, 六数独, 人类-AI协作, 解释能力, 决策支持

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究表明，大型语言模型（LLMs）在解决六数独谜题时表现有限，且无法提供反映战略推理或直观问题解决的解释，突显了LLMs在人类-AI协作决策中的挑战。

Motivation: 评估LLMs在解决和解释六数独谜题时的表现，以探索其在人类-AI协作决策中的潜力。

Method: 通过测试五个LLMs解决和解释六数独谜题的能力，分析其表现。

Result: 仅有一个LLM能部分解决问题，所有模型均无法提供战略推理或直观解释。

Conclusion: LLMs在成为有效的人类-AI协作伙伴前，仍需解决重大挑战。

Abstract: The success of Large Language Models (LLMs) in human-AI collaborative
decision-making hinges on their ability to provide trustworthy, gradual, and
tailored explanations. Solving complex puzzles, such as Sudoku, offers a
canonical example of this collaboration, where clear and customized
explanations often hold greater importance than the final solution. In this
study, we evaluate the performance of five LLMs in solving and explaining
\sixsix{} Sudoku puzzles. While one LLM demonstrates limited success in solving
puzzles, none can explain the solution process in a manner that reflects
strategic reasoning or intuitive problem-solving. These findings underscore
significant challenges that must be addressed before LLMs can become effective
partners in human-AI collaborative decision-making.

</details>


### [8] [Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model](https://arxiv.org/abs/2505.16000)
*Mehrdad ghassabi,Pedram Rostami,Hamidreza Baradaran Kashani,Amirhossein Poursina,Zahra Kazemi,Milad Tavakoli*

Key words: 语言模型, 医学领域, 波斯语, 低资源语言, 微调

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究通过爬取波斯语医学杂志和真实医患问答对数据，首次构建了波斯语医学领域数据集，并基于此微调小型语言模型，提升了其在医学问答中的准确性和响应质量。

Motivation: 波斯语等低资源语言的小型语言模型在医学领域表现不佳，且缺乏相关数据集。研究旨在利用公开在线数据提升其医学知识能力。

Method: 爬取波斯语医学杂志和医患问答对构建数据集，并用于微调基线模型。

Result: 微调后模型在医学问答任务中准确性提高，响应质量优于基线。

Conclusion: 开放数据可有效增强小型语言模型的医学能力，为波斯语医疗AI提供资源友好方案。

Abstract: The rapid advancement of language models has demonstrated the potential of
artificial intelligence in the healthcare industry. However, small language
models struggle with specialized domains in low-resource languages like
Persian. While numerous medical-domain websites exist in Persian, no curated
dataset or corpus has been available making ours the first of its kind. This
study explores the enhancement of medical knowledge in a small language model
by leveraging accessible online data, including a crawled corpus from medical
magazines and a dataset of real doctor-patient QA pairs. We fine-tuned a
baseline model using our curated data to improve its medical knowledge.
Benchmark evaluations demonstrate that the fine-tuned model achieves improved
accuracy in medical question answering and provides better responses compared
to its baseline. This work highlights the potential of leveraging open-access
online data to enrich small language models in medical fields, providing a
novel solution for Persian medical AI applications suitable for
resource-constrained environments.

</details>


### [9] [Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions](https://arxiv.org/abs/2505.16002)
*Sasha Boguraev,Christopher Potts,Kyle Mahowald*

Key words: 大语言模型、句法理论、因果可解释性、填充语-空缺依赖、分布式交换干预

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）为语言学家研究句法理论提供了有力证据。通过因果可解释性方法分析LLMs，揭示其习得的抽象机制，特别是在英语填充语-空缺依赖结构中的相似分析结果，并发现了一些可能修改传统语言学理论的因素。

Motivation: 探索LLMs如何通过内部机制帮助推进语言学理论，尤其是针对英语填充语-空缺依赖结构的抽象分析。

Method: 采用分布式交换干预（Distributed Interchange Interventions）实验方法，分析LLMs的抽象机制。

Result: LLMs对填充语-空缺依赖结构展现了一致的抽象分析，并揭示了频率、填充语类型和上下文等可能影响语言学理论的因素。

Conclusion: LLMs的机制性内部分析能够推动语言学理论的进步。

Abstract: Large Language Models (LLMs) have emerged as powerful sources of evidence for
linguists seeking to develop theories of syntax. In this paper, we argue that
causal interpretability methods, applied to LLMs, can greatly enhance the value
of such evidence by helping us characterize the abstract mechanisms that LLMs
learn to use. Our empirical focus is a set of English filler-gap dependency
constructions (e.g., questions, relative clauses). Linguistic theories largely
agree that these constructions share many properties. Using experiments based
in Distributed Interchange Interventions, we show that LLMs converge on similar
abstract analyses of these constructions. These analyses also reveal previously
overlooked factors -- relating to frequency, filler type, and surrounding
context -- that could motivate changes to standard linguistic theory. Overall,
these results suggest that mechanistic, internal analyses of LLMs can push
linguistic theory forward.

</details>


### [10] [SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models](https://arxiv.org/abs/2505.16003)
*Roland Daynauth,Christopher Clarke,Krisztian Flautner,Lingjia Tang,Jason Mars*

Key words: LLM-as-a-Judge, calibration, human preference, entropy maximization, SLMEval

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SLMEval 是一种基于熵最大化的高效校准方法，通过少量人类偏好数据提高语言模型评估器在开放任务中的表现，显著提升了与人类判断的相关性并降低了评估成本。

Motivation: 现有校准技术在开放任务中表现不佳，与人类判断相关性弱甚至负相关，亟需一种能泛化到真实场景的高效校准方法。

Method: 提出 SLMEval 方法，基于熵最大化利用少量人类偏好数据，估计模型质量的潜在分布并重新加权评估分数。

Result: SLMEval 在真实生产用例和公共基准测试中与人类评估强相关（如 Spearman 系数 0.57），且评估成本比 GPT-4 校准器低 5-30 倍。

Conclusion: SLMEval 是一种高效、泛化性强的校准方法，显著提升了开放任务中语言模型评估的准确性和经济性。

Abstract: The LLM-as-a-Judge paradigm offers a scalable, reference-free approach for
evaluating language models. Although several calibration techniques have been
proposed to better align these evaluators with human judgment, prior studies
focus primarily on narrow, well-structured benchmarks. As a result, it remains
unclear whether such calibrations generalize to real-world, open-ended tasks.
  In this work, we show that SOTA calibrated evaluators often fail in these
settings, exhibiting weak or even negative correlation with human judgments. To
address this, we propose SLMEval, a novel and efficient calibration method
based on entropy maximization over a small amount of human preference data. By
estimating a latent distribution over model quality and reweighting evaluator
scores accordingly, SLMEval achieves strong correlation with human evaluations
across two real-world production use cases and the public benchmark. For
example, on one such task, SLMEval achieves a Spearman correlation of 0.57 with
human judgments, while G-Eval yields a negative correlation. In addition,
SLMEval reduces evaluation costs by 5-30x compared to GPT-4-based calibrated
evaluators such as G-eval.

</details>


### [11] [LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization](https://arxiv.org/abs/2505.16008)
*Wenrui Yu,Yiyi Chen,Johannes Bjerva,Sokol Kosta,Qiongxiu Li*

Key words: 跨语言嵌入, 隐私攻击, 少样本学习, 图优化, 语言相似性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LAGO是一种基于语言相似性的图优化方法，用于提升少样本跨语言嵌入反转攻击的效果，显著提高了攻击的可迁移性。

Motivation: 针对多语言NLP系统中的隐私漏洞，现有方法忽视了语言间的关联性，LAGO通过显式建模语言关系来解决这一问题。

Method: LAGO采用图约束分布式优化框架，结合句法和词汇相似性作为边约束，实现跨语言的协作参数学习。

Result: 实验表明，LAGO在极少量数据（每种语言仅10个样本）下显著提升了攻击的可迁移性，Rouge-L分数比基线提高了10-20%。

Conclusion: 语言相似性是影响反转攻击可迁移性的关键因素，未来需更多关注语言感知的隐私保护多语言嵌入。

Abstract: We propose LAGO - Language Similarity-Aware Graph Optimization - a novel
approach for few-shot cross-lingual embedding inversion attacks, addressing
critical privacy vulnerabilities in multilingual NLP systems. Unlike prior work
in embedding inversion attacks that treat languages independently, LAGO
explicitly models linguistic relationships through a graph-based constrained
distributed optimization framework. By integrating syntactic and lexical
similarity as edge constraints, our method enables collaborative parameter
learning across related languages. Theoretically, we show this formulation
generalizes prior approaches, such as ALGEN, which emerges as a special case
when similarity constraints are relaxed. Our framework uniquely combines
Frobenius-norm regularization with linear inequality or total variation
constraints, ensuring robust alignment of cross-lingual embedding spaces even
with extremely limited data (as few as 10 samples per language). Extensive
experiments across multiple languages and embedding models demonstrate that
LAGO substantially improves the transferability of attacks with 10-20% increase
in Rouge-L score over baselines. This work establishes language similarity as a
critical factor in inversion attack transferability, urging renewed focus on
language-aware privacy-preserving multilingual embeddings.

</details>


### [12] [Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains](https://arxiv.org/abs/2505.16014)
*Yash Saxena,Anpur Padia,Mandar S Chaudhary,Kalpa Gunaratna,Srinivasan Parthasarathy,Manas Gaur*

Key words: RAG, explainability, adversarial robustness, rationale-driven selection, METEORA

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: METEORA improves RAG by replacing re-ranking with rationale-driven selection, enhancing accuracy and resilience to adversarial content.

Motivation: Address limitations of traditional RAG pipelines, such as lack of explainability and robustness against adversarial content.

Method: Uses a two-stage approach: rationale generation via preference-tuned LLM and rationale-guided chunk selection with verification.

Result: 33.34% accuracy improvement with 50% fewer chunks; F1 score rises from 0.10 to 0.44 in adversarial settings.

Conclusion: METEORA provides explainable, robust, and efficient retrieval-augmented generation.

Abstract: Traditional Retrieval-Augmented Generation (RAG) pipelines rely on
similarity-based retrieval and re-ranking, which depend on heuristics such as
top-k, and lack explainability, interpretability, and robustness against
adversarial content. To address this gap, we propose a novel method METEORA
that replaces re-ranking in RAG with a rationale-driven selection approach.
METEORA operates in two stages. First, a general-purpose LLM is
preference-tuned to generate rationales conditioned on the input query using
direct preference optimization. These rationales guide the evidence chunk
selection engine, which selects relevant chunks in three stages: pairing
individual rationales with corresponding retrieved chunks for local relevance,
global selection with elbow detection for adaptive cutoff, and context
expansion via neighboring chunks. This process eliminates the need for top-k
heuristics. The rationales are also used for consistency check using a Verifier
LLM to detect and filter poisoned or misleading content for safe generation.
The framework provides explainable and interpretable evidence flow by using
rationales consistently across both selection and verification. Our evaluation
across six datasets spanning legal, financial, and academic research domains
shows that METEORA improves generation accuracy by 33.34% while using
approximately 50% fewer chunks than state-of-the-art re-ranking methods. In
adversarial settings, METEORA significantly improves the F1 score from 0.10 to
0.44 over the state-of-the-art perplexity-based defense baseline, demonstrating
strong resilience to poisoning attacks. Code available at:
https://anonymous.4open.science/r/METEORA-DC46/README.md

</details>


### [13] [NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning](https://arxiv.org/abs/2505.16022)
*Wei Liu,Siya Qi,Xinyu Wang,Chen Qian,Yali Du,Yulan He*

Key words: NOVER, 强化学习, 激励训练, 无验证器

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为NOVER的无验证器强化学习框架，该方法仅需标准监督微调数据，无需外部验证器，适用于多种文本到文本任务，性能优于同规模模型。

Motivation: 现有激励训练方法依赖外部验证器，限制了在数学和编程等领域的应用，且验证器训练成本高。NOVER旨在解决这一问题，提供更通用的强化学习框架。

Method: NOVER采用无验证器强化学习，仅需标准监督微调数据，避免依赖外部验证器。

Result: NOVER在多种任务中表现优异，性能超越同规模模型7.7%。

Conclusion: NOVER是一种灵活且高效的强化学习框架，为大型语言模型优化提供了新可能。

Abstract: Recent advances such as DeepSeek R1-Zero highlight the effectiveness of
incentive training, a reinforcement learning paradigm that computes rewards
solely based on the final answer part of a language model's output, thereby
encouraging the generation of intermediate reasoning steps. However, these
methods fundamentally rely on external verifiers, which limits their
applicability to domains like mathematics and coding where such verifiers are
readily available. Although reward models can serve as verifiers, they require
high-quality annotated data and are costly to train. In this work, we propose
NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning
framework that requires only standard supervised fine-tuning data with no need
for an external verifier. NOVER enables incentive training across a wide range
of text-to-text tasks and outperforms the model of the same size distilled from
large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the
flexibility of NOVER enables new possibilities for optimizing large language
models, such as inverse incentive training.

</details>


### [14] [Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild](https://arxiv.org/abs/2505.16023)
*Sheshera Mysore,Debarati Das,Hancheng Cao,Bahareh Sarrafzadeh*

Key words: 大型语言模型、人机协作、写作任务、行为模式、模型对齐

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 总结出用户与大型语言模型（LLM）协作时的典型行为（PATHs），并发现这些行为与写作意图显著相关。

Motivation: 研究用户在复杂写作任务中如何主动调整LLM输出，以改进模型对齐需求。

Method: 通过分析Bing Copilot和WildChat的实际用户会话数据，识别典型协作行为模式（PATHs）。

Result: 少数PATHs能解释大部分用户交互行为，且特定写作意图与PATHs显著相关。

Conclusion: 用户意图直接影响协作行为，这为LLM的对齐设计提供了依据。

Abstract: As large language models (LLMs) are used in complex writing workflows, users
engage in multi-turn interactions to steer generations to better fit their
needs. Rather than passively accepting output, users actively refine, explore,
and co-construct text. We conduct a large-scale analysis of this collaborative
behavior for users engaged in writing tasks in the wild with two popular AI
assistants, Bing Copilot and WildChat. Our analysis goes beyond simple task
classification or satisfaction estimation common in prior work and instead
characterizes how users interact with LLMs through the course of a session. We
identify prototypical behaviors in how users interact with LLMs in prompts
following their original request. We refer to these as Prototypical Human-AI
Collaboration Behaviors (PATHs) and find that a small group of PATHs explain a
majority of the variation seen in user-LLM interaction. These PATHs span users
revising intents, exploring texts, posing questions, adjusting style or
injecting new content. Next, we find statistically significant correlations
between specific writing intents and PATHs, revealing how users' intents shape
their collaboration behaviors. We conclude by discussing the implications of
our findings on LLM alignment.

</details>


### [15] [OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models](https://arxiv.org/abs/2505.16036)
*Burak Erinç Çetin,Yıldırım Özen,Elif Naz Demiryılmaz,Kaan Engür,Cagri Toraman*

Key words: 大语言模型, 伦理评估, 鲁棒性, 可靠性, 安全性, 公平性, 多语言

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究对29个开源大语言模型进行了全面的伦理评估，覆盖鲁棒性、可靠性、安全性和公平性四个维度，并比较了英语和土耳其语的表现。结果显示，许多模型在安全性和公平性上表现较好，但可靠性仍有改进空间，且参数更多的模型伦理表现更优。

Motivation: 当前对生成式大语言模型的伦理研究通常局限于狭窄的维度和有限的语种覆盖，本研究旨在填补评估广度、语言多样性和模型多样性上的空白。

Method: 采用基于LLM-as-a-Judge的新数据收集方法，评估29个开源大语言模型在鲁棒性、可靠性、安全性和公平性四个方面的表现，并对比英语和土耳其语环境下的行为。

Result: 许多模型在安全性和公平性上表现较好，鲁棒性较好，但可靠性仍是问题；语言不影响伦理评估效果；参数更多的大模型（如Gemma和Qwen）伦理表现更优。

Conclusion: 本研究为更全面的大语言模型伦理评估提供了框架，并证明伦理评估可以跨语言有效实施，同时指出可靠性仍需改进。

Abstract: Generative large language models present significant potential but also raise
critical ethical concerns. Most studies focus on narrow ethical dimensions, and
also limited diversity of languages and models. To address these gaps, we
conduct a broad ethical evaluation of 29 recent open-source large language
models using a novel data collection including four ethical aspects:
Robustness, reliability, safety, and fairness. We analyze model behavior in
both a commonly used language, English, and a low-resource language, Turkish.
Our aim is to provide a comprehensive ethical assessment and guide safer model
development by filling existing gaps in evaluation breadth, language coverage,
and model diversity. Our experimental results, based on LLM-as-a-Judge, reveal
that optimization efforts for many open-source models appear to have
prioritized safety and fairness, and demonstrated good robustness while
reliability remains a concern. We demonstrate that ethical evaluation can be
effectively conducted independently of the language used. In addition, models
with larger parameter counts tend to exhibit better ethical performance, with
Gemma and Qwen models demonstrating the most ethical behavior among those
evaluated.

</details>


### [16] [Internal and External Impacts of Natural Language Processing Papers](https://arxiv.org/abs/2505.16061)
*Yu Zhang*

Key words: NLP, 影响力分析, 语言模型, 伦理, 专利, 媒体, 政策

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究分析了1979至2024年间顶级NLP会议的论文影响力，发现语言模型在学术界和公众中影响最广，而语言基础研究影响较低。内部和外部影响总体一致，但伦理、偏见和公平性在政策文件中受关注多于学术引用。不同领域偏好不同：专利关注实际应用，媒体和政策更关注社会影响。

Motivation: 探究NLP研究在不同领域（学术界、专利、媒体、政策）的影响力差异，揭示研究主题的传播和接受模式。

Method: 通过分析ACL、EMNLP和NAACL会议论文的引用数据及外部来源（专利、媒体、政策文件），量化不同NLP主题的影响力。

Result: 语言模型影响力最广，语言基础研究较低；伦理类主题在政策中受关注但学术引用较少；专利重应用，媒体和政策重社会影响。

Conclusion: NLP研究的内外部影响力存在差异，不同领域对主题的偏好显著，需针对性优化研究传播策略。

Abstract: We investigate the impacts of NLP research published in top-tier conferences
(i.e., ACL, EMNLP, and NAACL) from 1979 to 2024. By analyzing citations from
research articles and external sources such as patents, media, and policy
documents, we examine how different NLP topics are consumed both within the
academic community and by the broader public. Our findings reveal that language
modeling has the widest internal and external influence, while linguistic
foundations have lower impacts. We also observe that internal and external
impacts generally align, but topics like ethics, bias, and fairness show
significant attention in policy documents with much fewer academic citations.
Additionally, external domains exhibit distinct preferences, with patents
focusing on practical NLP applications and media and policy documents engaging
more with the societal implications of NLP models.

</details>


### [17] [Small Language Models in the Real World: Insights from Industrial Text Classification](https://arxiv.org/abs/2505.16078)
*Lujun Li,Lama Sleem,Niccolo' Gentile,Geoffrey Nichil,Radu State*

Key words: 小规模语言模型, 文本分类, 提示工程, 监督微调, 工业场景

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文评估了小规模Transformer模型在文本分类任务中的表现和效率，重点研究了提示工程和监督微调方法，并针对工业场景的实际需求进行了分析。

Motivation: 探讨小规模语言模型是否能有效处理文本分类任务，解决大规模模型在推理效率、资源需求和提示依赖性上的不足。

Method: 采用提示工程和监督微调方法，对多种小规模Transformer模型在工业场景下的文本分类任务进行综合评估。

Result: 小规模模型在文本分类任务中表现出色，尤其是在效率和VRAM利用率方面，适合工业场景的本地部署。

Conclusion: 小规模模型能够有效替代大规模模型处理文本分类任务，尤其是在资源受限的工业场景下具有显著优势。

Abstract: With the emergence of ChatGPT, Transformer models have significantly advanced
text classification and related tasks. Decoder-only models such as Llama
exhibit strong performance and flexibility, yet they suffer from inefficiency
on inference due to token-by-token generation, and their effectiveness in text
classification tasks heavily depends on prompt quality. Moreover, their
substantial GPU resource requirements often limit widespread adoption. Thus,
the question of whether smaller language models are capable of effectively
handling text classification tasks emerges as a topic of significant interest.
However, the selection of appropriate models and methodologies remains largely
underexplored. In this paper, we conduct a comprehensive evaluation of prompt
engineering and supervised fine-tuning methods for transformer-based text
classification. Specifically, we focus on practical industrial scenarios,
including email classification, legal document categorization, and the
classification of extremely long academic texts. We examine the strengths and
limitations of smaller models, with particular attention to both their
performance and their efficiency in Video Random-Access Memory (VRAM)
utilization, thereby providing valuable insights for the local deployment and
application of compact models in industrial settings.

</details>


### [18] [BiasLab: Toward Explainable Political Bias Detection with Dual-Axis Annotations and Rationale Indicators](https://arxiv.org/abs/2505.16081)
*KMA Solaiman*

Key words: BiasLab, 政治偏见, 可解释性, 众包标注, GPT-4o

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 文章介绍了一个名为BiasLab的数据集，包含300篇政治新闻文章，标注了意识形态偏见，并提供了基线模型以供分析偏见感知和解释效果。

Motivation: 为了促进透明、社会意识强的自然语言处理系统的开发，需要可解释的政治偏见模型和评估方法。

Method: 通过众包标注文章对民主党和共和党的情感倾向，结合GPT-4o模拟标注，分析标注一致性和偏见分类任务。

Result: 数据集和基线模型为偏见感知和解释性建模提供了基础，揭示了人类和模型在偏见分类上的不对称性。

Conclusion: BiasLab数据集支持可解释偏见建模和解释效果评估，推动透明NLP的发展。

Abstract: We present BiasLab, a dataset of 300 political news articles annotated for
perceived ideological bias. These articles were selected from a curated
900-document pool covering diverse political events and source biases. Each
article is labeled by crowdworkers along two independent scales, assessing
sentiment toward the Democratic and Republican parties, and enriched with
rationale indicators. The annotation pipeline incorporates targeted worker
qualification and was refined through pilot-phase analysis. We quantify
inter-annotator agreement, analyze misalignment with source-level outlet bias,
and organize the resulting labels into interpretable subsets. Additionally, we
simulate annotation using schema-constrained GPT-4o, enabling direct comparison
to human labels and revealing mirrored asymmetries, especially in
misclassifying subtly right-leaning content. We define two modeling tasks:
perception drift prediction and rationale type classification, and report
baseline performance to illustrate the challenge of explainable bias detection.
BiasLab's rich rationale annotations provide actionable interpretations that
facilitate explainable modeling of political bias, supporting the development
of transparent, socially aware NLP systems. We release the dataset, annotation
schema, and modeling code to encourage research on human-in-the-loop
interpretability and the evaluation of explanation effectiveness in real-world
settings.

</details>


### [19] [Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning](https://arxiv.org/abs/2505.16088)
*Gagan Bhatia,Maxime Peyrard,Wei Zhao*

Key words: BPE分词器, 时间推理, 日期碎片化, 大语言模型, 注意力机制

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了BPE分词器对日期的碎片化处理及其对时间推理的影响，提出了日期碎片化比率度量、DateAugBench测试套件，并发现大语言模型通过抽象机制拼接日期碎片进行推理。

Motivation: 现代BPE分词器常将日期拆分为无意义的片段，影响时间推理的鲁棒性。本研究旨在量化这种影响并探索模型如何应对。

Method: 提出日期碎片化比率度量，发布DateAugBench测试套件，并通过分层探测和注意力分析研究模型行为。

Result: 过度碎片化导致罕见日期准确率下降10%，模型越大越能快速抽象拼接日期碎片。推理路径与人不同（年→月→日）。

Conclusion: 日期碎片化损害模型性能，但大模型能通过抽象机制缓解，揭示了其独特的推理路径。

Abstract: Modern BPE tokenizers often split calendar dates into meaningless fragments,
e.g., 20250312 $\rightarrow$ 202, 503, 12, inflating token counts and obscuring
the inherent structure needed for robust temporal reasoning. In this work, we
(1) introduce a simple yet interpretable metric, termed date fragmentation
ratio, that measures how faithfully a tokenizer preserves multi-digit date
components; (2) release DateAugBench, a suite of 6500 examples spanning three
temporal reasoning tasks: context-based date resolution, format-invariance
puzzles, and date arithmetic across historical, contemporary, and future
regimes; and (3) through layer-wise probing and causal attention-hop analyses,
uncover an emergent date-abstraction mechanism whereby large language models
stitch together the fragments of month, day, and year components for temporal
reasoning. Our experiments show that excessive fragmentation correlates with
accuracy drops of up to 10 points on uncommon dates like historical and
futuristic dates. Further, we find that the larger the model, the faster the
emergent date abstraction that heals date fragments is accomplished. Lastly, we
observe a reasoning path that LLMs follow to assemble date fragments, typically
differing from human interpretation (year $\rightarrow$ month $\rightarrow$
day).

</details>


### [20] [Continually Self-Improving Language Models for Bariatric Surgery Question--Answering](https://arxiv.org/abs/2505.16102)
*Yash Kumar Atri,Thomas H Shin,Thomas Hartvigsen*

Key words: 减重手术；代谢手术；检索增强生成；医疗差异；自适应模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为bRAGgen的自适应检索增强生成模型，用于解决代谢和减重手术(MBS)患者因医疗差异导致的信息获取障碍问题，并通过bRAGq数据集和专家评估验证了其优越性能。

Motivation: 代谢和减重手术(MBS)患者的全病程管理依赖于多学科协作，但医疗差异（如访问障碍）阻碍了患者获取及时可靠的信息，亟需技术解决方案。

Method: 开发了基于RAG的自适应模型bRAGgen，能动态整合实时医学证据；同时构建了bRAGq数据集（1,302个术相关问题），并通过专家验证。

Result: 通过两阶段评估（LLM指标和专家评审），bRAGgen在生成临床准确相关回答方面显著优于现有模型。

Conclusion: bRAGgen为MBS领域提供了高效、精准的信息支持，填补了大规模专业数据集的空白，具有重要的临床应用价值。

Abstract: While bariatric and metabolic surgery (MBS) is considered the gold standard
treatment for severe and morbid obesity, its therapeutic efficacy hinges upon
active and longitudinal engagement with multidisciplinary providers, including
surgeons, dietitians/nutritionists, psychologists, and endocrinologists. This
engagement spans the entire patient journey, from preoperative preparation to
long-term postoperative management. However, this process is often hindered by
numerous healthcare disparities, such as logistical and access barriers, which
impair easy patient access to timely, evidence-based, clinician-endorsed
information. To address these gaps, we introduce bRAGgen, a novel adaptive
retrieval-augmented generation (RAG)-based model that autonomously integrates
real-time medical evidence when response confidence dips below dynamic
thresholds. This self-updating architecture ensures that responses remain
current and accurate, reducing the risk of misinformation. Additionally, we
present bRAGq, a curated dataset of 1,302 bariatric surgery--related questions,
validated by an expert bariatric surgeon. bRAGq constitutes the first
large-scale, domain-specific benchmark for comprehensive MBS care. In a
two-phase evaluation, bRAGgen is benchmarked against state-of-the-art models
using both large language model (LLM)--based metrics and expert surgeon review.
Across all evaluation dimensions, bRAGgen demonstrates substantially superior
performance in generating clinically accurate and relevant responses.

</details>


### [21] [Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models](https://arxiv.org/abs/2505.16104)
*Yue Li,Xin Yi,Dongsheng Shi,Gerard de Melo,Xiaoling Wang,Linlin Wang*

Key words: 大型视觉语言模型,剪枝,安全性能,层次化安全重新对齐,注意力头,神经元

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种名为层次化安全重新对齐（HSR）的轻量级方法，旨在解决大型视觉语言模型（LVLMs）在剪枝后安全性能下降的问题。

Motivation: 随着LVLMs规模的增加，剪枝技术用于压缩模型以适应资源受限环境的需求增多，但剪枝会导致安全性能下降。

Method: HSR通过量化注意力头对安全的贡献，识别关键注意力头，并在这些头中选择性恢复关键神经元，实现从注意力头到神经元层次的安全重新对齐。

Result: 实验证明，HSR在不同模型和剪枝策略中显著提升了安全性能。

Conclusion: 这是首个明确关注剪枝后LVLMs安全恢复的研究，HSR方法有效且轻量。

Abstract: With the increasing size of Large Vision-Language Models (LVLMs), network
pruning techniques aimed at compressing models for deployment in
resource-constrained environments have garnered significant attention. However,
we observe that pruning often leads to a degradation in safety performance. To
address this issue, we present a novel and lightweight approach, termed
Hierarchical Safety Realignment (HSR). HSR operates by first quantifying the
contribution of each attention head to safety, identifying the most critical
ones, and then selectively restoring neurons directly within these attention
heads that play a pivotal role in maintaining safety. This process
hierarchically realigns the safety of pruned LVLMs, progressing from the
attention head level to the neuron level. We validate HSR across various models
and pruning strategies, consistently achieving notable improvements in safety
performance. To our knowledge, this is the first work explicitly focused on
restoring safety in LVLMs post-pruning.

</details>


### [22] [MPL: Multiple Programming Languages with Large Language Models for Information Extraction](https://arxiv.org/abs/2505.16107)
*Bo Li,Gexiang Fang,Wei Ye,Zhenghua Xu,Jinglei Zhang,Hao Cheng,Shikun Zhang*

Key words: 信息抽取, 编程语言, 监督微调, 大语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种名为MPL的新框架，通过结合多种编程语言在监督微调阶段提升信息抽取任务的性能，并引入了虚拟运行的函数提示来更高效地模拟代码式输入。

Motivation: 现有的信息抽取研究主要集中于Python语言，忽略了其他流行编程语言（如C++和Java）在监督微调阶段的潜力。

Method: 提出MPL框架，探索多种编程语言在监督微调阶段的应用，并引入函数提示与虚拟运行技术优化代码式输入模拟。

Result: 在多个数据集上的实验证明了MPL的有效性，并提供了全面的分析。

Conclusion: MPL框架成功利用多种编程语言提升了信息抽取任务的性能，并公开了代码以促进未来研究。

Abstract: Recent research in information extraction (IE) focuses on utilizing
code-style inputs to enhance structured output generation. The intuition behind
this is that the programming languages (PLs) inherently exhibit greater
structural organization than natural languages (NLs). This structural advantage
makes PLs particularly suited for IE tasks. Nevertheless, existing research
primarily focuses on Python for code-style simulation, overlooking the
potential of other widely-used PLs (e.g., C++ and Java) during the supervised
fine-tuning (SFT) phase. In this research, we propose \textbf{M}ultiple
\textbf{P}rogramming \textbf{L}anguages with large language models for
information extraction (abbreviated as \textbf{MPL}), a novel framework that
explores the potential of incorporating different PLs in the SFT phase.
Additionally, we introduce \texttt{function-prompt} with virtual running to
simulate code-style inputs more effectively and efficiently. Experimental
results on a wide range of datasets demonstrate the effectiveness of MPL.
Furthermore, we conduct extensive experiments to provide a comprehensive
analysis. We have released our code for future research.

</details>


### [23] [Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics](https://arxiv.org/abs/2505.16118)
*Haotian Lan,Yao Gao,Yujun Cheng,Wei Yuan,Kun Wang*

Key words: 用户生成内容, LLM框架, 旅游分析, 期望量化, 社交媒体

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种双方法LLM框架，结合无监督与监督学习，量化社交媒体用户生成内容中的旅行期望，发现休闲/社交因素比自然/情感因素更驱动参与。

Motivation: 社交媒体用户生成内容对旅行决策至关重要，但现有分析方法缺乏可扩展性。

Method: 采用无监督期望提取与调查反馈的监督微调相结合的LLM框架。

Result: 研究发现休闲/社交期望比自然/情感因素更能驱动用户参与。

Conclusion: LLM框架可作为量化期望的精准工具，推动旅游分析方法的进步，并为个性化体验和社交旅行推广提供策略。

Abstract: Social media's rise establishes user-generated content (UGC) as pivotal for
travel decisions, yet analytical methods lack scalability. This study
introduces a dual-method LLM framework: unsupervised expectation extraction
from UGC paired with survey-informed supervised fine-tuning. Findings reveal
leisure/social expectations drive engagement more than foundational
natural/emotional factors. By establishing LLMs as precision tools for
expectation quantification, we advance tourism analytics methodology and
propose targeted strategies for experience personalization and social travel
promotion. The framework's adaptability extends to consumer behavior research,
demonstrating computational social science's transformative potential in
marketing optimization.

</details>


### [24] [KoBALT: Korean Benchmark For Advanced Linguistic Tasks](https://arxiv.org/abs/2505.16125)
*Hyopil Shin,Sangah Lee,Dongjun Jang,Wooseok Song,Jaeyoon Kim,Chaeyoung Oh,Hyemi Jo,Youngchae Ahn,Sihyun Oh,Hyohyeong Chang,Sunkyoung Kim,Jinsik Lee*

Key words: KoBALT, 韩语基准测试, 大型语言模型, 语言学评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: KoBALT 是一个针对韩语的全面语言学基准测试，包含700个选择题，涵盖五个语言学领域，旨在评估大型语言模型（LLMs）的真实语言理解能力。

Motivation: 解决现有韩语基准测试缺乏语言学深度和类型学基础的问题，推动对韩语LLMs的更准确评估。

Method: 开发了包含24种语言现象的700个专家策划题目，减少与标准语料库的n-gram重叠，避免数据污染。

Result: 测试20个LLMs，最高模型准确率61%，语义表现最佳（66%），音系学和形态学表现最弱（31%、36%）。

Conclusion: KoBALT填补了韩语语言学评估的空白，提供了衡量LLMs真实语言能力的有效工具。

Abstract: We introduce KoBALT (Korean Benchmark for Advanced Linguistic Tasks), a
comprehensive linguistically-motivated benchmark comprising 700 multiple-choice
questions spanning 24 phenomena across five linguistic domains: syntax,
semantics, pragmatics, phonetics/phonology, and morphology. KoBALT is designed
to advance the evaluation of large language models (LLMs) in Korean, a
morphologically rich language, by addressing the limitations of conventional
benchmarks that often lack linguistic depth and typological grounding. It
introduces a suite of expert-curated, linguistically motivated questions with
minimal n-gram overlap with standard Korean corpora, substantially mitigating
the risk of data contamination and allowing a more robust assessment of true
language understanding. Our evaluation of 20 contemporary LLMs reveals
significant performance disparities, with the highest-performing model
achieving 61\% general accuracy but showing substantial variation across
linguistic domains - from stronger performance in semantics (66\%) to
considerable weaknesses in phonology (31\%) and morphology (36\%). Through
human preference evaluation with 95 annotators, we demonstrate a strong
correlation between KoBALT scores and human judgments, validating our
benchmark's effectiveness as a discriminative measure of Korean language
understanding. KoBALT addresses critical gaps in linguistic evaluation for
typologically diverse languages and provides a robust framework for assessing
genuine linguistic competence in Korean language models.

</details>


### [25] [Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning](https://arxiv.org/abs/2505.16128)
*Yue Zhou,Barbara Di Eugenio*

Key words: 大型语言模型、人口统计偏见、归因偏见、评估偏见、教育和评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究发现，尽管大型语言模型（LLM）已明确对齐以消除人口统计刻板印象，但在数学、编码、常识和写作等任务中仍表现出与解决方案真实性相关的人口统计偏见。这些偏见包括归因偏见和评估偏见，模型倾向于将正确答案归于某些群体，而对相同解决方案的评价因作者人口统计信息而异。

Motivation: 探讨LLMs在真实性评估中是否存在人口统计偏见，尤其在教育和评估场景中的应用可能带来的问题。

Method: 通过五个与人类价值观对齐的LLMs，在数学、编码、常识和写作问题上进行实验，分析归因偏见和评估偏见的表现。

Result: LLMs在数学和编码任务中更少将正确答案归因于非裔美国人群，且在写作评估中较少偏好亚裔作者。此外，模型在可视化代码中会自动为不同人口统计群体分配刻板颜色，表明偏见根植于其推理过程中。

Conclusion: 人口统计偏见不仅限于表层刻板印象或社会情境触发，LLMs在教育与评估环境中的部署需谨慎。

Abstract: Despite LLMs' explicit alignment against demographic stereotypes, they have
been shown to exhibit biases under various social contexts. In this work, we
find that LLMs exhibit concerning biases in how they associate solution
veracity with demographics. Through experiments across five human value-aligned
LLMs on mathematics, coding, commonsense, and writing problems, we reveal two
forms of such veracity biases: Attribution Bias, where models
disproportionately attribute correct solutions to certain demographic groups,
and Evaluation Bias, where models' assessment of identical solutions varies
based on perceived demographic authorship. Our results show pervasive biases:
LLMs consistently attribute fewer correct solutions and more incorrect ones to
African-American groups in math and coding, while Asian authorships are least
preferred in writing evaluation. In additional studies, we show LLMs
automatically assign racially stereotypical colors to demographic groups in
visualization code, suggesting these biases are deeply embedded in models'
reasoning processes. Our findings indicate that demographic bias extends beyond
surface-level stereotypes and social context provocations, raising concerns
about LLMs' deployment in educational and evaluation settings.

</details>


### [26] [LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods](https://arxiv.org/abs/2505.16129)
*Hyang Cui*

Key words: 机器翻译质量评估, LLM, 生成式评估, 语义相似度

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种基于生成的机器翻译质量评估方法，通过生成高质量参考译文并结合语义相似度评分，优于传统直接评分方法。

Motivation: 现有直接评分方法在段级与人工评估的相关性较低，需要更准确的评估方法。

Method: 采用仅解码器的大型语言模型生成参考译文，再通过句子嵌入计算语义相似度评分。

Result: 在8个LLM和8种语言对上验证，优于直接评分和非LLM参考无关指标。

Conclusion: 生成式评估结合语义分析具有优势，支持向混合方法转变。

Abstract: Recent studies have applied large language models (LLMs) to machine
translation quality estimation (MTQE) by prompting models to assign numeric
scores. Nonetheless, these direct scoring methods tend to show low
segment-level correlation with human judgments. In this paper, we propose a
generation-based evaluation paradigm that leverages decoder-only LLMs to
produce high-quality references, followed by semantic similarity scoring using
sentence embeddings. We conduct the most extensive evaluation to date in MTQE,
covering 8 LLMs and 8 language pairs. Empirical results show that our method
outperforms both intra-LLM direct scoring baselines and external non-LLM
reference-free metrics from MTME. These findings demonstrate the strength of
generation-based evaluation and support a shift toward hybrid approaches that
combine fluent generation with accurate semantic assessment.

</details>


### [27] [Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models](https://arxiv.org/abs/2505.16134)
*Menschikov Mikhail,Alexander Kharitonov,Maiia Kotyga,Vadim Porvatov,Anna Zhukovskaya,David Kagramanyan,Egor Shvetsov,Evgeny Burnaev*

Key words: 位置偏见, 大型语言模型, 跨语言研究, 模型不确定性, 句法

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型的位置偏见及其与语言多样性的关系，发现偏见是模型驱动且语言特定的，并提出位置引导会降低准确性。

Motivation: 探讨大型语言模型中的位置偏见如何与不同语言的特性相互作用，以及其对模型行为和准确性的影响。

Method: 在五种类型不同的语言（英语、俄语、德语、印地语、越南语）上进行了跨语言研究，分析位置偏见与模型不确定性、句法和提示的互动。

Result: 发现位置偏见是模型驱动的，语言间存在差异；明确的位置引导降低准确性；与偏见对齐的语境增加熵，但低熵不预示高准确性；在自由词序语言中，模型偏好主导词序。

Conclusion: 位置偏见影响模型表现，提示工程需谨慎，且需考虑语言特性。

Abstract: Large language models exhibit positional bias -- systematic neglect of
information at specific context positions -- yet its interplay with linguistic
diversity remains poorly understood. We present a cross-linguistic study across
five typologically distinct languages (English, Russian, German, Hindi,
Vietnamese), examining how positional bias interacts with model uncertainty,
syntax, and prompting. Key findings: (1) Positional bias is model-driven, with
language-specific variations -- Qwen2.5-7B favors late positions, challenging
assumptions of early-token bias; (2) Explicit positional guidance (e.g.,
correct context is at position X) reduces accuracy across languages,
undermining prompt-engineering practices; (3) Aligning context with positional
bias increases entropy, yet minimal entropy does not predict accuracy. (4) We
further uncover that LLMs differently impose dominant word order in
free-word-order languages like Hindi.

</details>


### [28] [Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.16142)
*Shicheng Xu,Liang Pang,Yunchang Zhu,Jia Gu,Zihao Wei,Jingcheng Deng,Feiyang Pan,Huawei Shen,Xueqi Cheng*

Key words: 知识蒸馏, 强化学习, 大语言模型, 推理能力, GSRM

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于强化学习的知识蒸馏方法RLKD，通过生成结构奖励模型（GSRM）提升小语言模型的推理能力，效果优于传统监督微调方法。

Motivation: 现有的监督微调方法只能捕捉教师模型推理路径的表面特征，无法传递其隐含的多分支推理结构，导致学生模型无法充分学习。

Method: 提出RLKD框架，利用GSRM将推理路径分解为元推理和问题求解的多步骤，并通过强化学习奖励机制对齐师生模型的推理结构。

Result: 实验表明，即使仅使用0.1%的数据，RLKD在强化学习模式下仍优于传统监督微调方法，显著提升学生模型的推理能力。

Conclusion: RLKD通过结构奖励模型和强化学习的结合，有效传递教师模型的推理结构，为学生模型带来更高的推理潜力。

Abstract: Distilling reasoning paths from teacher to student models via supervised
fine-tuning (SFT) provides a shortcut for improving the reasoning ability of
smaller Large Language Models (LLMs). However, the reasoning paths generated by
teacher models often reflect only surface-level traces of their underlying
authentic reasoning. Insights from cognitive neuroscience suggest that
authentic reasoning involves a complex interweaving between meta-reasoning
(which selects appropriate sub-problems from multiple candidates) and solving
(which addresses the sub-problem). This implies authentic reasoning has an
implicit multi-branch structure. Supervised fine-tuning collapses this rich
structure into a flat sequence of token prediction in the teacher's reasoning
path, preventing effective distillation of this structure to students. To
address this limitation, we propose RLKD, a reinforcement learning (RL)-based
distillation framework guided by a novel Generative Structure Reward Model
(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving
steps and computes rewards to measure structural alignment between student and
teacher reasoning. RLKD combines this reward with RL, enabling student LLMs to
internalize the teacher's implicit multi-branch reasoning structure rather than
merely mimicking fixed output paths. Experiments show RLKD surpasses standard
SFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,
unlocking greater student reasoning potential than SFT-based distillation.

</details>


### [29] [EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios](https://arxiv.org/abs/2505.16160)
*Bin Xu,Yu Bai,Huashan Sun,Yiguan Lin,Siming Liu,Xinyue Liang,Yaolin Li,Yang Gao,Heyan Huang*

Key words: 教育基准测试、语言模型、评估指标、合成数据、小型模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文提出了一个专为教育场景设计的多样化基准测试（EduBench），包含9大场景和4000多个教育情境，并提出了12维评估指标，同时训练了一个小型模型，性能接近顶尖大模型。

Motivation: 大语言模型在教育领域的应用尚未充分探索和优化，论文旨在填补这一空白。

Method: 构建包含合成数据的多样化教育基准测试，提出多维评估指标，并通过人类标注优化模型评估。

Result: 成功训练的小型模型在测试集上性能接近顶尖大模型（如Deepseek V3、Qwen Max）。

Conclusion: 该研究为教育导向的语言模型开发和评估提供了实践基础。

Abstract: As large language models continue to advance, their application in
educational contexts remains underexplored and under-optimized. In this paper,
we address this gap by introducing the first diverse benchmark tailored for
educational scenarios, incorporating synthetic data containing 9 major
scenarios and over 4,000 distinct educational contexts. To enable comprehensive
assessment, we propose a set of multi-dimensional evaluation metrics that cover
12 critical aspects relevant to both teachers and students. We further apply
human annotation to ensure the effectiveness of the model-generated evaluation
responses. Additionally, we succeed to train a relatively small-scale model on
our constructed dataset and demonstrate that it can achieve performance
comparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on
the test set. Overall, this work provides a practical foundation for the
development and evaluation of education-oriented language models. Code and data
are released at https://github.com/ybai-nlp/EduBench.

</details>


### [30] [KNN-SSD: Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization](https://arxiv.org/abs/2505.16162)
*Mingbo Song,Heming Xia,Jun Zhang,Chak Tou Leong,Qiancheng Xu,Wenjie Li,Sujian Li*

Key words: Speculative Decoding, LLM, KNN, domain shift, acceleration

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SD加速LLM推断，但层跳过对领域切换敏感，KNN-SSD通过KNN搜索匹配不同领域输入，提升性能。

Motivation: 解决SD中层跳过对领域切换敏感的问题，提升加速性能的通用性。

Method: 引入KNN-SSD算法，利用KNN搜索将不同跳层与不同领域输入匹配。

Result: 在多种模型和任务中，KNN-SSD实现1.3x-1.6x的LLM推断加速。

Conclusion: KNN-SSD有效提升领域泛化能力，显著加速LLM推断。

Abstract: Speculative Decoding (SD) has emerged as a widely used paradigm to accelerate
the inference of large language models (LLMs) without compromising generation
quality. It works by efficiently drafting multiple tokens using a compact model
and then verifying them in parallel using the target LLM. Notably,
Self-Speculative Decoding proposes skipping certain layers to construct the
draft model, which eliminates the need for additional parameters or training.
Despite its strengths, we observe in this work that drafting with layer
skipping exhibits significant sensitivity to domain shifts, leading to a
substantial drop in acceleration performance. To enhance the domain
generalizability of this paradigm, we introduce KNN-SSD, an algorithm that
leverages K-Nearest Neighbor (KNN) search to match different skipped layers
with various domain inputs. We evaluated our algorithm in various models and
multiple tasks, observing that its application leads to 1.3x-1.6x speedup in
LLM inference.

</details>


### [31] [Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task](https://arxiv.org/abs/2505.16164)
*Mengyang Qiu,Zoe Brisebois,Siena Sun*

Key words: 大型语言模型,人类行为变异,音素流畅性任务,模型配置,多样性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究了大型语言模型（LLMs）能否在音素流畅性任务中模拟人类行为变异性，发现某些模型配置能匹配人类平均值，但无法复现人类变异的范围。

Motivation: 探讨LLMs是否可以作为人类认知任务的替代品，尤其是在模拟人类行为变异性方面的能力。

Method: 评估了34种模型配置（包括提示特异性、采样温度和模型类型），并与106名人类参与者的反应进行对比。

Result: 部分模型配置（如Claude 3.7 Sonnet）能匹配人类平均值，但所有模型的输出多样性均不及人类，且结构更僵化。网络分析还揭示了人类与模型在检索结构上的根本差异。

Conclusion: LLMs在模拟人类认知和行为方面存在显著局限性。

Abstract: Large language models (LLMs) are increasingly explored as substitutes for
human participants in cognitive tasks, but their ability to simulate human
behavioral variability remains unclear. This study examines whether LLMs can
approximate individual differences in the phonemic fluency task, where
participants generate words beginning with a target letter. We evaluated 34
model configurations, varying prompt specificity, sampling temperature, and
model type, and compared outputs to responses from 106 human participants.
While some configurations, especially Claude 3.7 Sonnet, matched human averages
and lexical preferences, none reproduced the scope of human variability. LLM
outputs were consistently less diverse and structurally rigid, and LLM
ensembles failed to increase diversity. Network analyses further revealed
fundamental differences in retrieval structure between humans and models. These
results highlight key limitations in using LLMs to simulate human cognition and
behavior.

</details>


### [32] [When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction](https://arxiv.org/abs/2505.16170)
*Yuqing Yang,Robin Jia*

Key words: 大型语言模型,错误承认,内部信念,监督微调,自我验证

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究表明，大型语言模型（LLMs）在知道自己错误时可以承认错误，但频率较低，这种行为与其内部信念密切相关。通过监督微调可以显著提升模型的错误承认能力。

Motivation: 探讨LLMs在何时及为何会选择承认错误（撤回先前的错误回答），以理解其内部信念如何影响行为。

Method: 构建模型特定数据集评估LLMs的错误承认行为，并通过实验验证内部信念与错误承认的因果关系。

Result: LLMs能够但较少承认错误，这种行为与其内部信念直接相关。监督微调能显著提升其错误承认能力。

Conclusion: LLMs的内部信念直接影响其是否承认错误，通过微调可以优化这一行为。

Abstract: Can large language models (LLMs) admit their mistakes when they should know
better? In this work, we define the behavior of acknowledging errors in
previously generated answers as "retraction" and aim to understand when and why
LLMs choose to retract. We first construct model-specific datasets to evaluate
whether a model will retract an incorrect answer that contradicts its own
parametric knowledge. While LLMs are capable of retraction, they do so only
infrequently. We demonstrate that retraction is closely tied to previously
identified indicators of models' internal belief: models fail to retract wrong
answers that they "believe" to be factually correct. Steering experiments
further demonstrate that internal belief causally influences model retraction.
In particular, when the model does not believe its answer, this not only
encourages the model to attempt to verify the answer, but also alters attention
behavior during self-verification. Finally, we demonstrate that simple
supervised fine-tuning significantly improves retraction performance by helping
the model learn more accurate internal beliefs. Code and datasets are available
on https://github.com/ayyyq/llm-retraction.

</details>


### [33] [Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss](https://arxiv.org/abs/2505.16172)
*Abhay Kumara Sri Krishna Nandiraju,Gondy Leroy,David Kauchak,Arif Ahmed*

Key words: 健康信息简化、生成式AI、缺失信息修复、文本评估、GPT-4

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究比较了生成式AI在简化健康信息文本中缺失信息的检测与修复方法，发现添加所有缺失实体比添加排名靠前的实体或随机词更能改善文本再生质量。

Motivation: 简化健康信息有助于公众理解，但现有生成式AI可能在简化过程中遗漏关键信息，影响理解效果。研究旨在改进这一过程。

Method: 收集50份健康文本，用GPT-4简化后，采用五种方法（添加缺失实体、单词、排名前三实体、随机实体作为对照）识别并补全缺失信息，通过余弦相似度和ROUGE评分评估效果。

Result: 添加所有缺失实体的方法在文本再生中表现最佳，优于其他方法；当前工具能识别实体但无法有效排序。

Conclusion: 生成式AI简化文本时需关注缺失实体的补充，现有工具在实体排序方面仍需改进。

Abstract: Understanding health information is essential in achieving and maintaining a
healthy life. We focus on simplifying health information for better
understanding. With the availability of generative AI, the simplification
process has become efficient and of reasonable quality, however, the algorithms
remove information that may be crucial for comprehension. In this study, we
compare generative AI to detect missing information in simplified text,
evaluate its importance, and fix the text with the missing information. We
collected 50 health information texts and simplified them using gpt-4-0613. We
compare five approaches to identify missing elements and regenerate the text by
inserting the missing elements. These five approaches involve adding missing
entities and missing words in various ways: 1) adding all the missing entities,
2) adding all missing words, 3) adding the top-3 entities ranked by gpt-4-0613,
and 4, 5) serving as controls for comparison, adding randomly chosen entities.
We use cosine similarity and ROUGE scores to evaluate the semantic similarity
and content overlap between the original, simplified, and reconstructed
simplified text. We do this for both summaries and full text. Overall, we find
that adding missing entities improves the text. Adding all the missing entities
resulted in better text regeneration, which was better than adding the
top-ranked entities or words, or random words. Current tools can identify these
entities, but are not valuable in ranking them.

</details>


### [34] [Understanding Fact Recall in Language Models: Why Two-Stage Training Encourages Memorization but Mixed Training Teaches Knowledge](https://arxiv.org/abs/2505.16178)
*Ying Zhang,Benjamin Heinzerling,Dongyuan Li,Ryoma Ishigaki,Yuta Hitomi,Kentaro Inui*

Key words: 语言模型、事实检索、训练策略、共享参数、混合训练

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究探讨了不同训练策略如何影响语言模型（LMs）对事实知识的检索能力，发现混合训练能够更好地促进共享参数的形成，从而提高事实检索的泛化能力。

Motivation: 尽管语言模型具有强大的通用能力，但其事实检索能力仍面临挑战。研究者希望理解不同训练策略（如两阶段训练和混合训练）如何影响模型参数的形成，以及这些差异如何影响事实检索性能。

Method: 研究采用交叉任务梯度追踪（cross-task gradient trace）方法，识别受事实存储和事实检索示例共同影响的共享参数。实验在合成事实检索数据集上进行，使用了Llama-3.2B和Pythia-2.8B模型。

Result: 混合训练相比两阶段训练，能够促进更大且更集中的共享参数集合的形成。这些共享参数可能在模型泛化事实知识方面起到关键作用。

Conclusion: 混合训练通过优化共享参数的形成，能更有效地提升语言模型的事实检索泛化能力。这一发现为改进语言模型的训练策略提供了新思路。

Abstract: Fact recall, the ability of language models (LMs) to retrieve specific
factual knowledge, remains a challenging task despite their impressive general
capabilities. Common training strategies often struggle to promote robust
recall behavior with two-stage training, which first trains a model with
fact-storing examples (e.g., factual statements) and then with fact-recalling
examples (question-answer pairs), tending to encourage rote memorization rather
than generalizable fact retrieval. In contrast, mixed training, which jointly
uses both types of examples, has been empirically shown to improve the ability
to recall facts, but the underlying mechanisms are still poorly understood. In
this work, we investigate how these training strategies affect how model
parameters are shaped during training and how these differences relate to their
ability to recall facts. We introduce cross-task gradient trace to identify
shared parameters, those strongly influenced by both fact-storing and
fact-recalling examples. Our analysis on synthetic fact recall datasets with
the Llama-3.2B and Pythia-2.8B models reveals that mixed training encouraging a
larger and more centralized set of shared parameters. These findings suggest
that the emergence of parameters may play a key role in enabling LMs to
generalize factual knowledge across task formulations.

</details>


### [35] [SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models](https://arxiv.org/abs/2505.16188)
*Zirui He,Mingyu Jin,Bo Shen,Ali Payani,Yongfeng Zhang,Mengnan Du*

Key words: 大型语言模型, 行为控制, 稀疏自编码器, 监督引导, 可解释性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种新的监督引导方法，通过在稀疏可解释表示空间中进行操作，有效控制大型语言模型（LLMs）的行为。利用稀疏自编码器（SAEs）获取稀疏潜在表示，识别任务相关维度的子空间，并优化监督引导向量以对齐目标行为。实验证明该方法在多项任务中成功率更高且生成质量下降较小。进一步分析表明，仅需很小的子空间即可实现有效引导。

Motivation: 当前大型语言模型在开放生成场景中行为控制仍具挑战性，传统方法难以在保持生成质量的同时实现可靠控制。因此，研究提出了一种基于稀疏可解释表示空间的新型监督引导方法。

Method: 使用稀疏自编码器（SAEs）提取稀疏潜在表示，训练线性分类器识别任务相关维度的子空间，并在该子空间内优化监督引导向量以对齐目标行为。

Result: 实验结果表明，该方法在情感、真实性和政治极性等任务中，相比现有方法实现了更高的成功率且生成质量下降较小。分析还发现仅需很小的子空间即可有效引导。

Conclusion: 该研究提出的监督引导方法在控制LLMs行为方面表现出高效性和可解释性，为模型行为干预提供了新思路。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but controlling their behavior
reliably remains challenging, especially in open-ended generation settings.
This paper introduces a novel supervised steering approach that operates in
sparse, interpretable representation spaces. We employ sparse autoencoders
(SAEs)to obtain sparse latent representations that aim to disentangle semantic
attributes from model activations. Then we train linear classifiers to identify
a small subspace of task-relevant dimensions in latent representations.
Finally, we learn supervised steering vectors constrained to this subspace,
optimized to align with target behaviors. Experiments across sentiment,
truthfulness, and politics polarity steering tasks with multiple LLMs
demonstrate that our supervised steering vectors achieve higher success rates
with minimal degradation in generation quality compared to existing methods.
Further analysis reveals that a notably small subspace is sufficient for
effective steering, enabling more targeted and interpretable interventions.

</details>


### [36] [The Language of Interoception: Examining Embodiment and Emotion Through a Corpus of Body Part Mentions](https://arxiv.org/abs/2505.16189)
*Sophie Wu,Jan Philip Wahle,Saif M. Mohammad*

Key words: 身体部位提及, 情感分析, 自然语言处理, 身心健康, 语料库研究

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文首次研究了大规模自然语言数据中情绪、具身化和日常语言之间的联系，分析了在线文本中身体部位提及（BPMs）的情感相关性及其对健康结果的影响。

Motivation: 研究动机是探索身体部位在语言中的使用如何反映情绪状态，以及其与健康结果的关系，填补了自然语言处理、情感科学和人类福祉研究的空白。

Method: 方法包括构建在线英文文本（博客和推文）中BPMs的语料库，并通过人工标注和情感关联词典分析BPMs的使用模式和情感关联性。

Result: 结果显示BPMs在个人叙事和推文中普遍存在（5%至10%），其使用模式随时间、地理位置显著变化。包含BPMs的文本情感更强烈，且与较差的健康结果显著相关。

Conclusion: 结论认为研究身体部位相关语言为未来NLP、情感科学和人类福祉的交叉研究开辟了新方向。

Abstract: This paper is the first investigation of the connection between emotion,
embodiment, and everyday language in a large sample of natural language data.
We created corpora of body part mentions (BPMs) in online English text (blog
posts and tweets). This includes a subset featuring human annotations for the
emotions of the person whose body part is mentioned in the text. We show that
BPMs are common in personal narratives and tweets (~5% to 10% of posts include
BPMs) and that their usage patterns vary markedly by time and %geographic
location. Using word-emotion association lexicons and our annotated data, we
show that text containing BPMs tends to be more emotionally charged, even when
the BPM is not explicitly used to describe a physical reaction to the emotion
in the text. Finally, we discover a strong and statistically significant
correlation between body-related language and a variety of poorer health
outcomes. In sum, we argue that investigating the role of body-part related
words in language can open up valuable avenues of future research at the
intersection of NLP, the affective sciences, and the study of human wellbeing.

</details>


### [37] [An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability](https://arxiv.org/abs/2505.16193)
*Daiqing Wu,Dongbao Yang,Sicheng Zhao,Can Ma,Yu Zhou*

Key words: 多模态大语言模型（MLLMs）, 多模态情感分析（MSA）, 零样本范式, 上下文学习（ICL）, 情感预测偏差

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文通过研究上下文学习（ICL）优化多模态大语言模型（MLLMs）在情感分析（MSA）任务中的表现，发现并解决了零样本范式的问题，显著提升了准确率。

Motivation: 多模态大语言模型（MLLMs）在零样本范式下表现不佳，尤其是在多模态情感分析（MSA）任务中。本文旨在验证MLLMs是否具备感知情感的能力，并通过优化上下文学习（ICL）配置来提升性能。

Method: 论文扩展了零样本范式到上下文学习（ICL），通过深入研究演示（demonstrations）的检索、呈现和分布三个关键因素，发现并解决了MLLMs固有的情感预测偏差。

Result: 优化策略在六个MSA数据集上平均准确率提升了15.9%（对比零样本范式）和11.2%（对比随机ICL基线）。

Conclusion: 通过优化上下文学习的配置，MLLMs在多模态情感分析任务中的表现显著提升，验证了其感知情感的潜力。

Abstract: The advancements in Multimodal Large Language Models (MLLMs) have enabled
various multimodal tasks to be addressed under a zero-shot paradigm. This
paradigm sidesteps the cost of model fine-tuning, emerging as a dominant trend
in practical application. Nevertheless, Multimodal Sentiment Analysis (MSA), a
pivotal challenge in the quest for general artificial intelligence, fails to
accommodate this convenience. The zero-shot paradigm exhibits undesirable
performance on MSA, casting doubt on whether MLLMs can perceive sentiments as
competent as supervised models. By extending the zero-shot paradigm to
In-Context Learning (ICL) and conducting an in-depth study on configuring
demonstrations, we validate that MLLMs indeed possess such capability.
Specifically, three key factors that cover demonstrations' retrieval,
presentation, and distribution are comprehensively investigated and optimized.
A sentimental predictive bias inherent in MLLMs is also discovered and later
effectively counteracted. By complementing each other, the devised strategies
for three factors result in average accuracy improvements of 15.9% on six MSA
datasets against the zero-shot paradigm and 11.2% against the random ICL
baseline.

</details>


### [38] [Large Language Models based ASR Error Correction for Child Conversations](https://arxiv.org/abs/2505.16212)
*Anfeng Xu,Tiantian Feng,So Hyun Kim,Somer Bishop,Catherine Lord,Shrikanth Narayanan*

Key words: ASR, LLMs, 儿童语音, 对话语音, CTC, Whisper

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探索了大型语言模型（LLMs）在纠正儿童对话语音ASR错误中的应用，发现LLMs对零样本和CTC模型输出有改进，但对上下文信息整合或Whisper等自回归模型效果有限。

Motivation: 儿童语音的准确识别仍是ASR领域的挑战，LLMs虽在ASR转录中表现出潜力，但尚未充分探索在儿童对话语音中的应用。

Method: 通过实验，使用LLMs纠正儿童对话语音的ASR错误，测试了零样本、微调的CTC模型及Whisper等自回归模型的输出。

Result: LLMs能改进零样本和CTC模型输出的ASR转录，但对整合上下文或Whisper等自回归模型的输出效果不佳。

Conclusion: LLMs在儿童语音ASR中部分有效，但需进一步研究其局限性。

Abstract: Automatic Speech Recognition (ASR) has recently shown remarkable progress,
but accurately transcribing children's speech remains a significant challenge.
Recent developments in Large Language Models (LLMs) have shown promise in
improving ASR transcriptions. However, their applications in child speech
including conversational scenarios are underexplored. In this study, we explore
the use of LLMs in correcting ASR errors for conversational child speech. We
demonstrate the promises and challenges of LLMs through experiments on two
children's conversational speech datasets with both zero-shot and fine-tuned
ASR outputs. We find that while LLMs are helpful in correcting zero-shot ASR
outputs and fine-tuned CTC-based ASR outputs, it remains challenging for LLMs
to improve ASR performance when incorporating contextual information or when
using fine-tuned autoregressive ASR (e.g., Whisper) outputs.

</details>


### [39] [Memorization or Reasoning? Exploring the Idiom Understanding of LLMs](https://arxiv.org/abs/2505.16216)
*Jisu Kim,Youngwoo Shin,Uiji Hwang,Jihun Choi,Richeng Xuan,Taeuk Kim*

Key words: 习语处理，大型语言模型，多语言数据集，上下文推理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型在6种语言中处理习语的能力，提出了MIDAS数据集，发现模型不仅依赖记忆，还结合上下文和推理，尤其是在处理具有组合性的习语时。

Motivation: 习语因其独特的语言特性与其他常见表达不同，现有研究虽然利用大型语言模型处理习语任务，但对模型在多语言环境下处理习语的内部机制了解不足。

Method: 引入多语言的MIDAS数据集，对大型语言模型进行综合评估，分析影响其习语处理能力的关键因素。

Result: 结果表明，模型在习语处理时不仅依赖记忆，还会结合上下文线索和推理，尤其是在处理具有组合性的习语时。

Conclusion: 大型语言模型对习语的理解是内部知识检索和基于推理的推断共同作用的结果。

Abstract: Idioms have long posed a challenge due to their unique linguistic properties,
which set them apart from other common expressions. While recent studies have
leveraged large language models (LLMs) to handle idioms across various tasks,
e.g., idiom-containing sentence generation and idiomatic machine translation,
little is known about the underlying mechanisms of idiom processing in LLMs,
particularly in multilingual settings. To this end, we introduce MIDAS, a new
large-scale dataset of idioms in six languages, each paired with its
corresponding meaning. Leveraging this resource, we conduct a comprehensive
evaluation of LLMs' idiom processing ability, identifying key factors that
influence their performance. Our findings suggest that LLMs rely not only on
memorization, but also adopt a hybrid approach that integrates contextual cues
and reasoning, especially when processing compositional idioms. This implies
that idiom understanding in LLMs emerges from an interplay between internal
knowledge retrieval and reasoning-based inference.

</details>


### [40] [Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation](https://arxiv.org/abs/2505.16222)
*Jiwon Moon,Yerin Hwang,Dongryeol Lee,Taegwan Kang,Yongil Kim,Kyomin Jung*

Key words: 大型语言模型, 代码评估, 语义等效, 偏见, 编程语言

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在代码评估任务中是否能公平、稳健地处理语义相同但表面形式不同的代码，发现所有测试的LLM评估者均存在正负偏见。

Motivation: 随着LLMs作为评估者的广泛应用，其在代码评估任务中的公平性和稳健性尚未明确，尤其是对语义相同但表面形式不同的代码。

Method: 定义了六种潜在的代码评估偏见类型，并在五种编程语言和多种LLM上进行了实证研究。

Result: 所有测试的LLM评估者均表现出了正负偏见，导致评分不准确，即使生成测试用例后仍存在偏见。

Conclusion: 研究揭示了LLM评估者在代码评估中的偏见问题，呼吁开发更稳健的评估方法。

Abstract: With the growing use of large language models(LLMs) as evaluators, their
application has expanded to code evaluation tasks, where they assess the
correctness of generated code without relying on reference implementations.
While this offers scalability and flexibility, it also raises a critical,
unresolved question: Can LLM judges fairly and robustly evaluate semantically
equivalent code with superficial variations? Functionally correct code often
exhibits variations-such as differences in variable names, comments, or
formatting-that should not influence its correctness. Yet, whether LLM judges
can reliably handle these variations remains unclear. We present the first
comprehensive study of this issue, defining six types of potential bias in code
evaluation and revealing their systematic impact on LLM judges. Across five
programming languages and multiple LLMs, we empirically demonstrate that all
tested LLM judges are susceptible to both positive and negative biases,
resulting in inflated or unfairly low scores. Moreover, we observe that LLM
judges remain vulnerable to these biases even when prompted to generate test
cases before scoring, highlighting the need for more robust code evaluation
methods.

</details>


### [41] [Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2505.16227)
*Bohao Wu,Qingyun Wang,Yue Guo*

Key words: 个性化术语检测,低秩适应(LoRA),轻量级微调,开源语言模型,可扩展性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种高效且可扩展的个性化术语检测方法，通过轻量级微调和个性化提示策略，显著提高了性能并减少了标注数据需求。

Motivation: 研究旨在解决个性化术语检测中资源消耗大的问题，提出高效且可扩展的方法，使技术文档对多学科背景读者更易访问。

Method: 采用两种策略：(1)基于开源模型的低秩适应(LoRA)轻量级微调，(2)个性化提示推理。还研究了结合有限标注数据和无监督用户背景信号的混合方法。

Result: 个性化LoRA模型在F1分数上优于GPT-4 21.4%，超过最佳基线8.3%，且仅需10%标注数据即可达到可比性能。

Conclusion: 研究首次系统性探索了基于开源语言模型的低资源个性化术语检测，为可扩展的用户自适应NLP系统提供了实用路径。

Abstract: Personalizing jargon detection and explanation is essential for making
technical documents accessible to readers with diverse disciplinary
backgrounds. However, tailoring models to individual users typically requires
substantial annotation efforts and computational resources due to user-specific
finetuning. To address this, we present a systematic study of personalized
jargon detection, focusing on methods that are both efficient and scalable for
real-world deployment. We explore two personalization strategies: (1)
lightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,
and (2) personalized prompting, which tailors model behavior at inference time
without retaining. To reflect realistic constraints, we also investigate hybrid
approaches that combine limited annotated data with unsupervised user
background signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in
F1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,
our method achieves comparable performance using only 10% of the annotated
training data, demonstrating its practicality for resource-constrained
settings. Our study offers the first work to systematically explore efficient,
low-resource personalization of jargon detection using open-source language
models, offering a practical path toward scalable, user-adaptive NLP system.

</details>


### [42] [MuseRAG: Idea Originality Scoring At Scale](https://arxiv.org/abs/2505.16232)
*Ali Sarosh Bangash,Krish Veera,Ishfat Abrar Islam,Raiyan Abdul Baten*

Key words: 创造力评估、原创性评分、LLMs、检索增强生成、自动化工具

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一个自动化方法MuseRAG，结合LLMs与RAG框架，用于评估创意想法的原创性，其表现与人工评分高度一致。

Motivation: 传统的人工评估创意原创性的方法费时且容易出错，需要一种高效且准确的自动化工具。

Method: 采用了MuseRAG方法，整合了大型语言模型与检索增强生成框架，通过零样提示来判断新想法是否属于现有类别或形成新类别。

Result: 在五个数据集上，MuseRAG与人工评分者在新颖性评分上表现出高度一致性（r = 0.89），同时在想法聚类上也达到了较高的一致性。

Conclusion: MuseRAG能够大规模、高效且准确地评估创意想法的原创性，适用于创造力研究的规模应用。

Abstract: An objective, face-valid way to assess the originality of creative ideas is
to measure how rare each idea is within a population -- an approach long used
in creativity research but difficult to automate at scale. Tabulating response
frequencies via manual bucketing of idea rephrasings is labor-intensive,
error-prone, and brittle under large corpora. We introduce a fully automated,
psychometrically validated pipeline for frequency-based originality scoring.
Our method, MuseRAG, combines large language models (LLMs) with an externally
orchestrated retrieval-augmented generation (RAG) framework. Given a new idea,
the system retrieves semantically similar prior idea buckets and zero-shot
prompts the LLM to judge whether the new idea belongs to an existing bucket or
forms a new one. The resulting buckets enable computation of frequency-based
originality metrics. Across five datasets (N=1143, n_ideas=16294), MuseRAG
matches human annotators in idea clustering structure and resolution (AMI =
0.59) and in participant-level scoring (r = 0.89) -- while exhibiting strong
convergent and external validity. Our work enables intent-sensitive,
human-aligned originality scoring at scale to aid creativity research.

</details>


### [43] [LIFEBench: Evaluating Length Instruction Following in Large Language Models](https://arxiv.org/abs/2505.16234)
*Wei Zhang,Zhenhong Zhou,Junfeng Fang,Rongwu Xu,Kun Wang,Yuanhe Zhang,Rui Wang,Ge Zhang,Xinfeng Li,Li Sun,Lingjuan Lyu,Yang Liu,Sen Su*

Key words: 大语言模型（LLMs），长度指令，LIFEBench，文本生成，评估基准

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LIFEBench评估发现，现有大语言模型（LLMs）在遵循长度指令（如生成指定长度的文本）时表现不佳，尤其是长文本任务中急剧退化，且未能达到厂商宣称的最大输出长度。推理型LLMs反而表现最优。

Motivation: 现有LLMs能解决PhD级推理问题，却在简单的长度指令遵循（如生成10,000字小说）上表现不佳。当前基准测试忽略了对长度约束的评估，因此需要专门工具填补这一空白。

Method: 提出跨任务、多语言的LIFEBench基准测试（10,800个实例，覆盖16至8192词长度），评估26种主流LLMs的指令遵循能力。

Result: 大多数模型对短指令表现良好，但超过一定长度后性能骤降；几乎所有模型的实际输出长度均低于厂商声称上限；推理型LLMs表现超越专为生成长文本设计的模型。

Conclusion: 当前LLMs在长度指令遵循上存在根本性局限，需针对性改进；LIFEBench为未来研究提供了关键评估工具。

Abstract: While large language models (LLMs) can solve PhD-level reasoning problems
over long context inputs, they still struggle with a seemingly simpler task:
following explicit length instructions-e.g., write a 10,000-word novel.
Additionally, models often generate far too short outputs, terminate
prematurely, or even refuse the request. Existing benchmarks focus primarily on
evaluating generations quality, but often overlook whether the generations meet
length constraints. To this end, we introduce Length Instruction Following
Evaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to
follow length instructions across diverse tasks and a wide range of specified
lengths. LIFEBench consists of 10,800 instances across 4 task categories in
both English and Chinese, covering length constraints ranging from 16 to 8192
words. We evaluate 26 widely-used LLMs and find that most models reasonably
follow short-length instructions but deteriorate sharply beyond a certain
threshold. Surprisingly, almost all models fail to reach the vendor-claimed
maximum output lengths in practice, as further confirmed by our evaluations
extending up to 32K words. Even long-context LLMs, despite their extended
input-output windows, counterintuitively fail to improve length-instructions
following. Notably, Reasoning LLMs outperform even specialized long-text
generation models, achieving state-of-the-art length following. Overall,
LIFEBench uncovers fundamental limitations in current LLMs' length instructions
following ability, offering critical insights for future progress.

</details>


### [44] [Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16237)
*Derong Xu,Pengyue Jia,Xiaopeng Li,Yingyi Zhang,Maolin Wang,Qidong Liu,Xiangyu Zhao,Yichao Wang,Huifeng Guo,Ruiming Tang,Enhong Chen,Tong Xu*

Key words: 大语言模型,检索增强生成,图RAG,双对齐

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Align-GRAG提出了一种基于图的双对齐框架，通过检索增强生成（RAG）解决大语言模型（LLM）中的幻觉和过时信息问题，并结合子图检索和优化对齐来实现更准确的生成。

Motivation: 大语言模型（LLM）存在幻觉和过时信息的问题，现有的RAG方法虽能部分解决，但图RAG在检索子图时引入无关节点和表示差距限制了其效果，需要更高效的框架。

Method: Align-GRAG采用推理引导的双对齐框架：先检索子图，再通过KL散度损失和对比损失优化图编码器与LLM推理结果，对齐节点与表示，最后结合图数据和LLM生成答案。

Result: 在GraphQA基准测试的常识推理、场景图理解和知识图谱推理任务中验证了方法的有效性。

Conclusion: Align-GRAG通过双对齐提升了图RAG的性能，解决了无关节点和表示差距问题，生成的结果更准确实用。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities, but
still struggle with issues like hallucinations and outdated information.
Retrieval-augmented generation (RAG) addresses these issues by grounding LLM
outputs in external knowledge with an Information Retrieval (IR) system.
Building on this foundation, graph-based RAG systems go a step further by
retrieving subgraphs, which preserve the relationships between knowledge
entities and provide more comprehensive context. However, graph RAG faces two
challenges: (1) Retrieving relevant information introduces irrelevant nodes
(especially in dense graph databases, where retrieval usually extends to
adjacent nodes), and leads to overly lengthy inputs that hinder efficiency; (2)
The representation gap between graph and language during generation with LLMs
limits the ability to fully leverage graph structures for enhanced
understanding. To address these limitations, we propose Align-GRAG, a novel
reasoning-guided dual alignment framework in post-retrieval phrase. It first
formulates a subgraph by retrieving nodes and edges. Then an Aligner is
proposed to jointly optimizes a graph encoder with LLM-summarized reasoning. It
achieves dual alignment of graph node and representation by leveraging KL
divergence loss and contrastive loss, facilitating efficient pruning of
irrelevant knowledge and establishing a unified semantic space. The Generator
integrates the aligned graph data with LLM to produce coherent and accurate
answers. Experiments on GraphQA benchmark across three tasks (including common
sense reasoning, scene graph understanding, and knowledge graph reasoning)
validate the effectiveness of our method. The code will be available upon
accepted.

</details>


### [45] [Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers](https://arxiv.org/abs/2505.16241)
*Viet-Anh Nguyen,Shiqian Zhao,Gia Dao,Runyi Hu,Yi Xie,Luu Anh Tuan*

Key words: 大型推理模型, 越狱攻击, 自适应加密, 安全漏洞, 动态策略

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种名为SEAL的新颖越狱攻击方法，针对大型推理模型（LRMs），通过自适应加密管道绕过其推理过程和安全机制。

Motivation: 尽管LRMs表现出强大的逻辑能力，但其可能引入更严重的安全漏洞尚未充分研究，现有越狱方法难以平衡有效性与对自适应安全机制的鲁棒性。

Method: SEAL采用堆叠加密方法结合多种密码，并通过随机和自适应动态策略调整密码长度、顺序和组合，以覆盖模型的推理能力和规避安全机制。

Result: 实验表明，SEAL在GPT o4-mini上的攻击成功率达到80.8%，显著优于现有基线方法27.2%。

Conclusion: SEAL展示了LRMs在安全性上的潜在脆弱性，强调了进一步研究模型安全防护的必要性。

Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated superior logical
capabilities compared to traditional Large Language Models (LLMs), gaining
significant attention. Despite their impressive performance, the potential for
stronger reasoning abilities to introduce more severe security vulnerabilities
remains largely underexplored. Existing jailbreak methods often struggle to
balance effectiveness with robustness against adaptive safety mechanisms. In
this work, we propose SEAL, a novel jailbreak attack that targets LRMs through
an adaptive encryption pipeline designed to override their reasoning processes
and evade potential adaptive alignment. Specifically, SEAL introduces a stacked
encryption approach that combines multiple ciphers to overwhelm the models
reasoning capabilities, effectively bypassing built-in safety mechanisms. To
further prevent LRMs from developing countermeasures, we incorporate two
dynamic strategies - random and adaptive - that adjust the cipher length,
order, and combination. Extensive experiments on real-world reasoning models,
including DeepSeek-R1, Claude Sonnet, and OpenAI GPT-o4, validate the
effectiveness of our approach. Notably, SEAL achieves an attack success rate of
80.8% on GPT o4-mini, outperforming state-of-the-art baselines by a significant
margin of 27.2%. Warning: This paper contains examples of inappropriate,
offensive, and harmful content.

</details>


### [46] [Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models](https://arxiv.org/abs/2505.16245)
*Vijeta Deshpande,Debasmita Ghose,John D. Patterson,Roger Beaty,Anna Rumshisky*

Key words: 多样性, 语言模型, 自学习, 长度控制, 偏好优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Diverse-NS 是一个长度控制的自学习框架，旨在提升语言模型的输出多样性，同时避免常见多样性指标和奖励模型对较短输出的偏好。

Motivation: 现有的多样性指标和奖励模型倾向于偏好较短输出，限制了表达的多样性，因此需要一种方法来平衡多样性、质量和长度。

Method: 通过生成和筛选平衡多样性、质量和长度的偏好数据，Diverse-NS 仅需 3,000 对偏好数据即可进行有效训练。

Result: 在 LLaMA-3.1-8B 和 Olmo-2 系列模型上，Diverse-NS 显著提升了词汇和语义多样性，且在四个创意生成任务中保持或略微提升了响应质量。

Conclusion: Diverse-NS 通过明确解决长度偏差问题，有效推动了模型生成更多样化和富有表现力的输出。

Abstract: Diverse language model responses are crucial for creative generation,
open-ended tasks, and self-improvement training. We show that common diversity
metrics, and even reward models used for preference optimization,
systematically bias models toward shorter outputs, limiting expressiveness. To
address this, we introduce Diverse, not Short (Diverse-NS), a length-controlled
self-learning framework that improves response diversity while maintaining
length parity. By generating and filtering preference data that balances
diversity, quality, and length, Diverse-NS enables effective training using
only 3,000 preference pairs. Applied to LLaMA-3.1-8B and the Olmo-2 family,
Diverse-NS substantially enhances lexical and semantic diversity. We show
consistent improvement in diversity with minor reduction or gains in response
quality on four creative generation tasks: Divergent Associations, Persona
Generation, Alternate Uses, and Creative Writing. Surprisingly, experiments
with the Olmo-2 model family (7B, and 13B) show that smaller models like
Olmo-2-7B can serve as effective "diversity teachers" for larger models. By
explicitly addressing length bias, our method efficiently pushes models toward
more diverse and expressive outputs.

</details>


### [47] [Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models](https://arxiv.org/abs/2505.16252)
*Hwiyeong Lee,Uiji Hwang,Hyelim Lim,Taeuk Kim*

Key words: 知识遗忘, 大型语言模型, 局部遗忘, 参数更新

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型中知识遗忘的有效性，发现现有局部遗忘方法的假设存在问题，参数局部性并不能保证有效的知识移除。

Motivation: 大型语言模型可能在训练过程中保留了一些非预期内容，需要探索知识遗忘方法以删除特定知识，同时保留其他无关的一般知识。现有的局部遗忘方法假设参数局部性与有效的知识移除相关，但这一假设的有效性尚不明确。

Method: 论文首先回顾了现有的局部遗忘方法，随后进行了受控实验，旨在严格评估局部参数更新对知识遗忘的因果贡献。

Result: 研究发现，有效遗忘所需的参数修改集合并不是严格固定的，这挑战了局部遗忘方法的核心假设，即参数局部性能够有效指示知识移除。

Conclusion: 局部遗忘方法的假设存在问题，参数局部性与知识移除的有效性之间并无必然联系，需要更深入的研究以找到更可靠的知识遗忘方法。

Abstract: Large language models often retain unintended content, prompting growing
interest in knowledge unlearning. Recent approaches emphasize localized
unlearning, which restricts parameter updates to specific regions in an effort
to remove target knowledge while preserving unrelated general knowledge.
However, their effectiveness remains uncertain due to the lack of robust and
thorough evaluation of the trade-off between the competing goals of unlearning.
In this paper, we begin by revisiting existing localized unlearning approaches.
We then conduct controlled experiments to rigorously evaluate whether local
parameter updates causally contribute to unlearning. Our findings reveal that
the set of parameters that must be modified for effective unlearning is not
strictly determined, challenging the core assumption of localized unlearning
that parameter locality is inherently indicative of effective knowledge
removal.

</details>


### [48] [IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection](https://arxiv.org/abs/2505.16258)
*Aashish Anantha Ramakrishnan,Aadarsh Anantha Ramakrishnan,Dongwon Lee*

Key words: 多模态讽刺检测，连贯关系，上下文学习，零样本学习，认知洞察

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了IRONIC框架，通过利用多模态连贯关系，在零样本多模态讽刺检测任务中取得了最佳性能，强调将语言和认知洞察融入多模态推理策略设计的必要性。

Motivation: 当前链式思维方法未能有效模拟人类识别讽刺的认知过程，导致在多模态输入中解释比喻性语言（如讽刺）效率低下，需要任务特定微调和大量推理步骤。

Method: 提出了IRONIC框架，利用多模态连贯关系（如指代、类比和语用图像-文本联系）进行上下文学习，以识别讽刺。

Result: 实验表明，IRONIC在零样本多模态讽刺检测任务中超越了不同基线模型，达到了最先进的性能。

Conclusion: 研究验证了将语言和认知洞察融入多模态推理策略设计的重要性，IRONIC框架为多模态讽刺检测提供了高效解决方案。

Abstract: Interpreting figurative language such as sarcasm across multi-modal inputs
presents unique challenges, often requiring task-specific fine-tuning and
extensive reasoning steps. However, current Chain-of-Thought approaches do not
efficiently leverage the same cognitive processes that enable humans to
identify sarcasm. We present IRONIC, an in-context learning framework that
leverages Multi-modal Coherence Relations to analyze referential, analogical
and pragmatic image-text linkages. Our experiments show that IRONIC achieves
state-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across
different baselines. This demonstrates the need for incorporating linguistic
and cognitive insights into the design of multi-modal reasoning strategies. Our
code is available at: https://github.com/aashish2000/IRONIC

</details>


### [49] [Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning](https://arxiv.org/abs/2505.16270)
*Jiaru Zou,Yikun Ban,Zihao Li,Yunzhe Qi,Ruizhong Qiu,Ling Yang,Jingrui He*

Key words: 大语言模型,微调,Transformer Copilot,Mistake Log,logits修正

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了Transformer Copilot框架，通过引入Copilot模型和Mistake Log优化Pilot模型的微调，性能提升显著。

Motivation: 传统微调方法仅关注生成损失优化，忽视了模型自身的学习信号，类似于人类通过反思错误提升能力。

Method: 设计Mistake Log记录错误，构建Copilot模型修正Pilot的logits，并采用联合训练和融合推理范式。

Result: 在12个基准测试中性能提升最高达34.5%，计算开销低且具备强扩展性和可迁移性。

Conclusion: Transformer Copilot通过利用学习信号显著提升模型性能，为微调提供了新思路。

Abstract: Large language models are typically adapted to downstream tasks through
supervised fine-tuning on domain-specific data. While standard fine-tuning
focuses on minimizing generation loss to optimize model parameters, we take a
deeper step by retaining and leveraging the model's own learning signals,
analogous to how human learners reflect on past mistakes to improve future
performance. We first introduce the concept of Mistake Log to systematically
track the model's learning behavior and recurring errors throughout
fine-tuning. Treating the original transformer-based model as the Pilot, we
correspondingly design a Copilot model to refine the Pilot's inference
performance via logits rectification. We name the overall Pilot-Copilot
framework the Transformer Copilot, which introduces (i) a novel Copilot model
design, (ii) a joint training paradigm where the Copilot continuously learns
from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference
paradigm where the Copilot rectifies the Pilot's logits for enhanced
generation. We provide both theoretical and empirical analyses on our new
learning framework. Experiments on 12 benchmarks spanning commonsense,
arithmetic, and recommendation tasks demonstrate that Transformer Copilot
consistently improves performance by up to 34.5%, while introducing marginal
computational overhead to Pilot models and exhibiting strong scalability and
transferability.

</details>


### [50] [Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility](https://arxiv.org/abs/2505.16277)
*Sheng-Fu Wang,Laurent Prevot,Jou-an Chi,Ri-Sheng Huang,Shu-Kai Hsieh*

Key words: 大语言模型，自然语言处理，语音缩减，韵律突出，预训练数据

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究提出利用自发语音语料库提取生产变量（如语音缩减和韵律突出），并测试不同预训练数据集（书面、口语和混合类型）训练的模型对这些变量的预测能力。结果表明，经过微调后，模型可以显著优于基线预测这些变量，且口语类型训练数据比书面数据预测更准确。

Motivation: 研究旨在从认知角度更好地理解大语言模型在处理语言时的特性，尤其关注其在预测行为（如眼动追踪）和生理（如大脑反应）变量方面的能力，并将其应用于生产变量（如语音缩减和韵律突出）的预测。

Method: 研究者从自发语音语料库中提取生产变量（语音缩减和韵律突出），并测试不同预训练数据集（书面、口语和混合类型）训练的模型对这些变量的预测能力。

Result: 经过微调后，模型在预测生产变量方面显著优于基线，且口语类型训练数据比书面数据预测更准确。

Conclusion: 结果表明，高质量语音语料库可以作为评估大语言模型的有效基准，口语类型数据在预测生产变量中表现更优。

Abstract: The achievements of Large Language Models in Natural Language Processing,
especially for high-resource languages, call for a better understanding of
their characteristics from a cognitive perspective. Researchers have attempted
to evaluate artificial models by testing their ability to predict behavioral
(e.g., eye-tracking fixations) and physiological (e.g., brain responses)
variables during language processing (e.g., reading/listening). In this paper,
we propose using spontaneous speech corpora to derive production variables
(speech reductions, prosodic prominences) and applying them in a similar
fashion. More precisely, we extract. We then test models trained with a
standard procedure on different pretraining datasets (written, spoken, and
mixed genres) for their ability to predict these two variables. Our results
show that, after some fine-tuning, the models can predict these production
variables well above baselines. We also observe that spoken genre training data
provides more accurate predictions than written genres. These results
contribute to the broader effort of using high-quality speech corpora as
benchmarks for LLMs.

</details>


### [51] [HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation](https://arxiv.org/abs/2505.16281)
*Shijie Zhang,Renhao Li,Songsheng Wang,Philipp Koehn,Min Yang,Derek F. Wong*

Key words: 機器翻譯評估、分層多智能體框架、MQM、LLM、錯誤識別

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了 HiMATE，一種基於分層多智能體框架的機器翻譯評估方法，通過利用 MQM 錯誤分類實現更細粒度的評估，並通過自我反思和智能體討論減少幻覺問題，顯著提升評估效果。

Motivation: 現有的 LLM 翻譯評估方法在錯誤範圍識別和嚴重性評估上仍存在不足，未能充分利用 MQM 分層結構中的細粒度信息。

Method: 提出 HiMATE 框架，基於 MQM 分層錯誤分類，結合多智能體系統，利用自我反思和智能體討論（非對稱信息）減少幻覺。

Result: 在數據集上表現優於基線模型，錯誤範圍檢測和嚴重性評估 F1 分數平均提升 89%。

Conclusion: HiMATE 框架顯著提升翻譯評估的人類一致性，尤其在錯誤細節處理上表現突出。

Abstract: The advancement of Large Language Models (LLMs) enables flexible and
interpretable automatic evaluations. In the field of machine translation
evaluation, utilizing LLMs with translation error annotations based on
Multidimensional Quality Metrics (MQM) yields more human-aligned judgments.
However, current LLM-based evaluation methods still face challenges in
accurately identifying error spans and assessing their severity. In this paper,
we propose HiMATE, a Hierarchical Multi-Agent Framework for Machine Translation
Evaluation. We argue that existing approaches inadequately exploit the
fine-grained structural and semantic information within the MQM hierarchy. To
address this, we develop a hierarchical multi-agent system grounded in the MQM
error typology, enabling granular evaluation of subtype errors. Two key
strategies are incorporated to further mitigate systemic hallucinations within
the framework: the utilization of the model's self-reflection capability and
the facilitation of agent discussion involving asymmetric information.
Empirically, HiMATE outperforms competitive baselines across different datasets
in conducting human-aligned evaluations. Further analyses underscore its
significant advantage in error span detection and severity assessment,
achieving an average F1-score improvement of 89% over the best-performing
baseline. We make our code and data publicly available at
https://anonymous.4open.science/r/HiMATE-Anony.

</details>


### [52] [Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA](https://arxiv.org/abs/2505.16293)
*Rishabh Maheshwary,Masoud Hashemi,Khyati Mahajan,Shiva Krishna Reddy Malay,Sai Rajeswar,Sathwik Tejaswi Madhusudhan,Spandana Gella,Vikas Yadav*

Key words: 迭代RAG、多跳问答、信息压缩、上下文长度、LLM

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了Note Writing方法，通过在每一步生成简洁相关的笔记来减少噪声和保留关键信息，从而提高迭代RAG在多跳问答中的性能。

Motivation: 迭代RAG在处理长上下文和累积不相关信息时面临挑战，限制了模型对检索内容的理解和推理能力。现有方法多限于单轮RAG或缺乏可扩展性。

Method: 提出了Notes Writing，在每轮迭代中从检索文档生成简洁且相关的笔记，减少噪声并提高LLM的有效上下文长度。

Result: 在三种迭代RAG方法、四种数据集上的实验表明，该方法平均性能提升15.6%，且输出标记量增加极少。

Conclusion: Notes Writing是一种无需微调、框架无关的通用方法，显著提升了迭代RAG的效果。

Abstract: Iterative RAG for multi-hop question answering faces challenges with lengthy
contexts and the buildup of irrelevant information. This hinders a model's
capacity to process and reason over retrieved content and limits performance.
While recent methods focus on compressing retrieved information, they are
either restricted to single-round RAG, require finetuning or lack scalability
in iterative RAG. To address these challenges, we propose Notes Writing, a
method that generates concise and relevant notes from retrieved documents at
each step, thereby reducing noise and retaining only essential information.
This indirectly increases the effective context length of Large Language Models
(LLMs), enabling them to reason and plan more effectively while processing
larger volumes of input text. Notes Writing is framework agnostic and can be
integrated with different iterative RAG methods. We demonstrate its
effectiveness with three iterative RAG methods, across two models and four
evaluation datasets. Notes writing yields an average improvement of 15.6
percentage points overall, with minimal increase in output tokens.

</details>


### [53] [ToDi: Token-wise Distillation via Fine-Grained Divergence Control](https://arxiv.org/abs/2505.16297)
*Seongryong Jung,Suwan Yoon,DongGeon Kim,Hwanhee Lee*

Key words: 大语言模型, 知识蒸馏, FKL, RKL, token-wise, ToDi

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ToDi是一种新的知识蒸馏方法，通过自适应结合FKL和RKL逐token优化，提高学生模型的性能和效率。

Motivation: 大语言模型（LLMs）在资源受限部署中因高延迟和能耗不实用，知识蒸馏（KD）通过将知识从大模型转移到小模型来解决这一问题。然而，传统的KD方法忽视了token级预测差异。

Method: 提出Token-wise Distillation（ToDi），通过梯度分析揭示FKL和RKL的互补作用，使用基于sigmoid的权重函数自适应地为每个token结合FKL和RKL。

Result: ToDi在指令跟随基准测试中优于最新蒸馏基线，并通过消融研究和效率分析验证了其有效性和实用性。

Conclusion: ToDi通过动态强调每个token的适当分歧，实现了精确的分布对齐，是一种高效且实用的知识蒸馏方法。

Abstract: Large language models (LLMs) offer impressive performance but are impractical
for resource-constrained deployment due to high latency and energy consumption.
Knowledge distillation (KD) addresses this by transferring knowledge from a
large teacher to a smaller student model. However, conventional KD, notably
approaches like Forward KL (FKL) and Reverse KL (RKL), apply uniform divergence
loss across the entire vocabulary, neglecting token-level prediction
discrepancies. By investigating these representative divergences via gradient
analysis, we reveal that FKL boosts underestimated tokens, while RKL suppresses
overestimated ones, showing their complementary roles. Based on this
observation, we propose Token-wise Distillation (ToDi), a novel method that
adaptively combines FKL and RKL per token using a sigmoid-based weighting
function derived from the teacher-student probability log-ratio. ToDi
dynamically emphasizes the appropriate divergence for each token, enabling
precise distribution alignment. We demonstrate that ToDi consistently
outperforms recent distillation baselines using uniform or less granular
strategies across instruction-following benchmarks. Extensive ablation studies
and efficiency analysis further validate ToDi's effectiveness and practicality.

</details>


### [54] [INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling](https://arxiv.org/abs/2505.16303)
*Haochen Shi,Tianshi Zheng,Weiqi Wang,Baixuan Xu,Chunyang Li,Chunkit Chan,Tao Fan,Yangqiu Song,Qiang Yang*

Key words: Large Language Model, LLM routing, InferenceDynamics, RouteMix, scalability, adaptability

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了InferenceDynamics，一个灵活且可扩展的多维路由框架，旨在优化大型语言模型（LLM）的选择以适应不同领域的用户查询，同时有效管理计算资源。

Motivation: 当前的LLM路由方法在大规模专业LLM池中扩展性和适应性方面存在局限性，无法有效应对模型范围的扩展和能力领域的演变。

Method: 论文提出了InferenceDynamics框架，通过建模模型的能力和知识来实现多维路由，并在RouteMix数据集上验证其效果。

Result: 在MMLU-Pro、GPQA、BigGenBench和LiveBench等现代基准测试中，InferenceDynamics展示了其在识别并利用高性能模型方面的有效性，实现了资源高效利用的优越结果。

Conclusion: InferenceDynamics的广泛采用可以充分发挥LLM生态系统的专业潜力，推动进一步的研究。论文代码将公开以促进研究发展。

Abstract: Large Language Model (LLM) routing is a pivotal technique for navigating a
diverse landscape of LLMs, aiming to select the best-performing LLMs tailored
to the domains of user queries, while managing computational resources.
However, current routing approaches often face limitations in scalability when
dealing with a large pool of specialized LLMs, or in their adaptability to
extending model scope and evolving capability domains. To overcome those
challenges, we propose InferenceDynamics, a flexible and scalable
multi-dimensional routing framework by modeling the capability and knowledge of
models. We operate it on our comprehensive dataset RouteMix, and demonstrate
its effectiveness and generalizability in group-level routing using modern
benchmarks including MMLU-Pro, GPQA, BigGenBench, and LiveBench, showcasing its
ability to identify and leverage top-performing models for given tasks, leading
to superior outcomes with efficient resource utilization. The broader adoption
of Inference Dynamics can empower users to harness the full specialized
potential of the LLM ecosystem, and our code will be made publicly available to
encourage further research.

</details>


### [55] [PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models](https://arxiv.org/abs/2505.16307)
*Chenzhuo Zhao,Ziqian Liu,Xingda Wang,Junting Lu,Chaoyi Ruan*

Key words: 提示优化，大型语言模型，PMPO，交叉熵损失，轻量级评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: PMPO是一种新的统一框架，通过token级交叉熵损失优化提示，避免输出采样或人工评估，显著提升LLM性能。

Motivation: 现有提示优化方法依赖昂贵输出生成或人工标注，限制其扩展性，尤其是小型或非指令调整模型。PMPO旨在提供轻量级替代方案。

Method: 使用token级交叉熵损失作为评估信号，通过掩码识别低质量提示段，并基于正负样例改写和选择优化版本。

Result: 在BBH、GSM8K、AQUA-RAT等任务中表现优于现有方法，AlpacaEval 2.0胜率提升超19分。

Conclusion: PMPO高效、通用，无需采样或人工干预，为提示优化提供了有效解决方案。

Abstract: Prompt optimization offers a practical and broadly applicable alternative to
fine-tuning for improving large language model (LLM) performance. However,
existing methods often rely on costly output generation, self-critiquing
abilities, or human-annotated preferences, which limit their scalability,
especially for smaller or non-instruction-tuned models. We introduce PMPO
(Probabilistic Metric Prompt Optimization), a unified framework that refines
prompts using token-level cross-entropy loss as a direct, lightweight
evaluation signal. PMPO identifies low-quality prompt segments by masking and
measuring their impact on loss, then rewrites and selects improved variants by
minimizing loss over positive and negative examples. Unlike prior methods, it
requires no output sampling or human evaluation during optimization, relying
only on forward passes and log-likelihoods. PMPO supports both supervised and
preference-based tasks through a closely aligned loss-based evaluation
strategy. Experiments show that PMPO consistently outperforms prior methods
across model sizes and tasks: it achieves the highest average accuracy on BBH,
performs strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates
by over 19 points. These results highlight PMPO's effectiveness, efficiency,
and broad applicability.

</details>


### [56] [CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation](https://arxiv.org/abs/2505.16325)
*Yuyang Jiang,Chacha Chen,Shengyuan Wang,Feng Li,Zecong Tang,Benjamin M. Mervak,Lydia Chelala,Christopher M Straus,Reve Chahine,Samuel G. Armato III,Chenhao Tan*

Key words: 放射学报告评估, 临床框架, 属性级比较, CLEAR

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为CLEAR的临床表格框架，用于更全面和可解释地评估放射学报告质量。

Motivation: 现有评估指标在捕捉候选报告与真实放射学报告间的临床差异时，缺乏细粒度和可解释性，导致评估结果不理想。

Method: CLEAR框架通过专家标注的表格和属性级比较，评估报告对医学条件的准确描述，包括首次出现、变化、严重性、描述位置和建议五个属性。

Result: CLEAR在多维度属性级输出上表现优异，能够更全面地评估报告质量，且其自动化指标与临床判断高度一致。

Conclusion: CLEAR框架提供了一个更全面、临床可解释的放射学报告评估方法，显著提升了评估效果。

Abstract: Existing metrics often lack the granularity and interpretability to capture
nuanced clinical differences between candidate and ground-truth radiology
reports, resulting in suboptimal evaluation. We introduce a Clinically-grounded
tabular framework with Expert-curated labels and Attribute-level comparison for
Radiology report evaluation (CLEAR). CLEAR not only examines whether a report
can accurately identify the presence or absence of medical conditions, but also
assesses whether it can precisely describe each positively identified condition
across five key attributes: first occurrence, change, severity, descriptive
location, and recommendation. Compared to prior works, CLEAR's
multi-dimensional, attribute-level outputs enable a more comprehensive and
clinically interpretable evaluation of report quality. Additionally, to measure
the clinical alignment of CLEAR, we collaborate with five board-certified
radiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from
MIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions.
Our experiments show that CLEAR achieves high accuracy in extracting clinical
attributes and provides automated metrics that are strongly aligned with
clinical judgment.

</details>


### [57] [SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers](https://arxiv.org/abs/2505.16330)
*Wenqing Wu,Chengzhi Zhang,Tong Bao,Yi Zhao*

Key words: 新颖性评估,章节组合,语言模型,学术论文,自动化评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了学术论文新颖性评估的最佳章节组合，通过输入不同章节组合到语言模型中预测新颖性分数，发现引言、结果和讨论部分最为关键。

Motivation: 现有方法通常关注单词或实体组合，对新颖性评估的洞察有限。因此，研究如何通过论文不同核心章节的最佳组合来提升自动化新颖性评估至关重要。

Method: 利用自然语言处理技术识别论文的章节结构（如IMRaD），并将不同章节组合输入预训练语言模型（PLMs）和大语言模型（LLMs），以专家评分为标签预测新颖性分数。

Result: 研究发现，引言、结果和讨论部分的组合最适合评估新颖性，而全文输入效果不显著。此外，引言和结果部分对新颖性预测任务最为重要。

Conclusion: 论文展示了章节组合对新颖性评估的重要性，为自动化评估提供了新方向。

Abstract: Novelty is a core component of academic papers, and there are multiple
perspectives on the assessment of novelty. Existing methods often focus on word
or entity combinations, which provide limited insights. The content related to
a paper's novelty is typically distributed across different core sections,
e.g., Introduction, Methodology and Results. Therefore, exploring the optimal
combination of sections for evaluating the novelty of a paper is important for
advancing automated novelty assessment. In this paper, we utilize different
combinations of sections from academic papers as inputs to drive language
models to predict novelty scores. We then analyze the results to determine the
optimal section combinations for novelty score prediction. We first employ
natural language processing techniques to identify the sectional structure of
academic papers, categorizing them into introduction, methods, results, and
discussion (IMRaD). Subsequently, we used different combinations of these
sections (e.g., introduction and methods) as inputs for pretrained language
models (PLMs) and large language models (LLMs), employing novelty scores
provided by human expert reviewers as ground truth labels to obtain prediction
results. The results indicate that using introduction, results and discussion
is most appropriate for assessing the novelty of a paper, while the use of the
entire text does not yield significant results. Furthermore, based on the
results of the PLMs and LLMs, the introduction and results appear to be the
most important section for the task of novelty score prediction. The code and
dataset for this paper can be accessed at
https://github.com/njust-winchy/SC4ANM.

</details>


### [58] [Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance](https://arxiv.org/abs/2505.16348)
*Taeyoon Kwon,Dongwook Choi,Sunghwan Kim,Hyojun Kim,Seungjun Moon,Beong-woo Kwak,Kuan-Hao Huang,Jinyoung Yeo*

Key words: 具身代理, 大型语言模型, 个性化辅助, 记忆利用, MEMENTO

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了MEMENTO框架，用于评估个性化具身代理在记忆利用方面的能力，揭示当前大型语言模型（如GPT-4o）在个性化辅助任务中的局限性。

Motivation: 现有的具身代理主要关注单轮交互和简化指令，未能真正解决个性化辅助中的挑战，尤其是用户对物理世界的独特语义理解。

Method: 设计了包含两阶段记忆评估流程的MEMENTO框架，评估代理在目标对象识别和用户模式推理中的记忆利用能力。

Result: 实验表明，即使先进模型如GPT-4o，在需要引用多重记忆时性能下降30.5%，尤其在用户模式相关任务中表现不佳。

Conclusion: 研究为未来开发更有效的个性化具身代理提供了重要见解，并呼吁改进记忆利用能力。

Abstract: Embodied agents empowered by large language models (LLMs) have shown strong
performance in household object rearrangement tasks. However, these tasks
primarily focus on single-turn interactions with simplified instructions, which
do not truly reflect the challenges of providing meaningful assistance to
users. To provide personalized assistance, embodied agents must understand the
unique semantics that users assign to the physical world (e.g., favorite cup,
breakfast routine) by leveraging prior interaction history to interpret
dynamic, real-world instructions. Yet, the effectiveness of embodied agents in
utilizing memory for personalized assistance remains largely underexplored. To
address this gap, we present MEMENTO, a personalized embodied agent evaluation
framework designed to comprehensively assess memory utilization capabilities to
provide personalized assistance. Our framework consists of a two-stage memory
evaluation process design that enables quantifying the impact of memory
utilization on task performance. This process enables the evaluation of agents'
understanding of personalized knowledge in object rearrangement tasks by
focusing on its role in goal interpretation: (1) the ability to identify target
objects based on personal meaning (object semantics), and (2) the ability to
infer object-location configurations from consistent user patterns, such as
routines (user patterns). Our experiments across various LLMs reveal
significant limitations in memory utilization, with even frontier models like
GPT-4o experiencing a 30.5% performance drop when required to reference
multiple memories, particularly in tasks involving user patterns. These
findings, along with our detailed analyses and case studies, provide valuable
insights for future research in developing more effective personalized embodied
agents. Project website: https://connoriginal.github.io/MEMENTO

</details>


### [59] [Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization](https://arxiv.org/abs/2505.16349)
*Pierre Achkar,Tim Gollub,Martin Potthast*

Key words: 多文档摘要, 检索增强生成, 科学文献, XSum

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: XSum是一个用于科学领域多文档摘要的模块化流程，通过检索增强生成（RAG）技术动态生成问题并编辑合成摘要，在SurveySum数据集上表现优于现有方法。

Motivation: 科学出版物数量激增，研究人员难以有效跟踪和整合知识，因此需要一种高效的多文档摘要方法。

Method: 采用检索增强生成（RAG）技术，包括问题生成模块和编辑模块，动态生成问题并合成符合学术规范的摘要。

Result: 在SurveySum数据集上，XSum在CheckEval、G-Eval和Ref-F1等指标上显著优于现有方法。

Conclusion: XSum为科学摘要提供了一个透明、可适应的框架，具有广泛的应用潜力。

Abstract: The exponential growth of scientific publications has made it increasingly
difficult for researchers to stay updated and synthesize knowledge effectively.
This paper presents XSum, a modular pipeline for multi-document summarization
(MDS) in the scientific domain using Retrieval-Augmented Generation (RAG). The
pipeline includes two core components: a question-generation module and an
editor module. The question-generation module dynamically generates questions
adapted to the input papers, ensuring the retrieval of relevant and accurate
information. The editor module synthesizes the retrieved content into coherent
and well-structured summaries that adhere to academic standards for proper
citation. Evaluated on the SurveySum dataset, XSum demonstrates strong
performance, achieving considerable improvements in metrics such as CheckEval,
G-Eval and Ref-F1 compared to existing approaches. This work provides a
transparent, adaptable framework for scientific summarization with potential
applications in a wide range of domains. Code available at
https://github.com/webis-de/scolia25-xsum

</details>


### [60] [PaTH Attention: Position Encoding via Accumulating Householder Transformations](https://arxiv.org/abs/2505.16381)
*Songlin Yang,Yikang Shen,Kaiyue Wen,Shawn Tan,Mayank Mishra,Liliang Ren,Rameswar Panda,Yoon Kim*

Key words: 位置编码, Householder变换, 大型语言模型, 数据依赖, 并行训练

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了PaTH，一种基于数据依赖的Householder变换的灵活位置编码方案，相比RoPE和其他基线方法在性能和表达力上表现更优。

Motivation: 由于RoPE中键/查询变换仅依赖相对位置而与输入无关，限制了基于RoPE的变换器的表达能力，因此需要一种更灵活的数据依赖位置编码方案。

Method: 提出PaTH，基于数据依赖的Householder变换，开发了高效的并行训练算法，并采用FlashAttention风格的块状算法减少I/O开销。

Result: 在合成基准测试和实际语言建模实验中，PaTH相比RoPE和其他基线方法表现更优。

Conclusion: PaTH作为一种数据依赖的位置编码方案，提升了表达能力和性能，适用于现代大型语言模型。

Abstract: The attention mechanism is a core primitive in modern large language models
(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,
position encoding is essential for modeling structured domains such as
language. Rotary position encoding (RoPE) has emerged as the de facto standard
approach for position encoding and is part of many modern LLMs. However, in
RoPE the key/query transformation between two elements in a sequence is only a
function of their relative position and otherwise independent of the actual
input. This limits the expressivity of RoPE-based transformers.
  This paper describes PaTH, a flexible data-dependent position encoding scheme
based on accumulated products of Householder(like) transformations, where each
transformation is data-dependent, i.e., a function of the input. We derive an
efficient parallel algorithm for training through exploiting a compact
representation of products of Householder matrices, and implement a
FlashAttention-style blockwise algorithm that minimizes I/O cost. Across both
targeted synthetic benchmarks and moderate-scale real-world language modeling
experiments, we find that PaTH demonstrates superior performance compared to
RoPE and other recent baselines.

</details>


### [61] [Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models](https://arxiv.org/abs/2505.16385)
*Kaiyu He,Tong Zhou,Yubo Chen,Delai Qiu,Shengping Liu,Kang Liu,Jun Zhao*

Key words: 大语言模型、跨语言能力、可解释性、语义枢纽、预训练数据集

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种词级跨语言翻译任务，通过追踪大语言模型(LLMs)中间层输出来研究其跨语言能力的学习方式。研究发现LLMs在任务中表现出两种不同行为：共现行为和语义枢纽行为，并进一步提出通过语义枢纽重建预训练数据集来提升模型的跨语言能力。

Motivation: 为了准确量化LLMs的跨语言能力并提升其可解释性，研究团队探索了LLMs如何学习跨语言能力的关键机制。

Method: 提出了词级跨语言翻译任务，并通过追踪LLMs中间层输出来分析其行为。研究识别了共现行为和语义枢纽行为，并基于语义枢纽比例重建了预训练数据集。

Result: 实验证明，基于语义枢纽重建的预训练数据集能有效提升LLMs的跨语言能力。

Conclusion: 研究揭示了LLMs跨语言能力的可解释性机制，并为提升此能力提供了一种方法。

Abstract: Large language models (LLMs) demonstrate remarkable ability in cross-lingual
tasks. Understanding how LLMs acquire this ability is crucial for their
interpretability. To quantify the cross-lingual ability of LLMs accurately, we
propose a Word-Level Cross-Lingual Translation Task. To find how LLMs learn
cross-lingual ability, we trace the outputs of LLMs' intermediate layers in the
word translation task. We identify and distinguish two distinct behaviors in
the forward pass of LLMs: co-occurrence behavior and semantic pivot behavior.
We attribute LLMs' two distinct behaviors to the co-occurrence frequency of
words and find the semantic pivot from the pre-training dataset. Finally, to
apply our findings to improve the cross-lingual ability of LLMs, we reconstruct
a semantic pivot-aware pre-training dataset using documents with a high
proportion of semantic pivots. Our experiments validate the effectiveness of
our approach in enhancing cross-lingual ability. Our research contributes
insights into the interpretability of LLMs and offers a method for improving
LLMs' cross-lingual ability.

</details>


### [62] [Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning](https://arxiv.org/abs/2502.15401)
*Xuetao Ma,Wenbin Jiang,Hua Huang*

Key words: 上下文学习,课程学习,大语言模型,问题解决逻辑

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种基于问题解决逻辑的课程式上下文学习方法，通过分析和排序示例来提升大语言模型的复杂推理能力，实验证明其优于现有方法。

Motivation: 现有上下文学习方法通常依赖简单特征衡量示例相关性，不足以反映示例间的内在联系，因此需要更有效的方法。

Method: 构建问题解决逻辑指令集，微调语言模型分析示例逻辑，按题目步骤数评估难度并排序示例，从易到难作为上下文提示。

Result: 在多个基准测试中，该方法在性能和效率上优于现有上下文学习方法，显著提升了大语言模型的推理能力。

Conclusion: 基于问题解决逻辑的课程式上下文学习方法能有效提升模型的复杂推理能力，未来将公开项目。

Abstract: In-context learning (ICL) can significantly enhance the complex reasoning
capabilities of large language models (LLMs), with the key lying in the
selection and ordering of demonstration examples. Previous methods typically
relied on simple features to measure the relevance between examples. We argue
that these features are not sufficient to reflect the intrinsic connections
between examples. In this study, we propose a curriculum ICL strategy guided by
problem-solving logic. We select demonstration examples by analyzing the
problem-solving logic and order them based on curriculum learning.
Specifically, we constructed a problem-solving logic instruction set based on
the BREAK dataset and fine-tuned a language model to analyze the
problem-solving logic of examples. Subsequently, we selected appropriate
demonstration examples based on problem-solving logic and assessed their
difficulty according to the number of problem-solving steps. In accordance with
the principles of curriculum learning, we ordered the examples from easy to
hard to serve as contextual prompts. Experimental results on multiple
benchmarks indicate that our method outperforms previous ICL approaches in
terms of performance and efficiency, effectively enhancing the complex
reasoning capabilities of LLMs. Our project will be publicly available
subsequently.

</details>


### [63] [Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection](https://arxiv.org/abs/2505.16392)
*Benjamin Vendeville,Liana Ermakova,Pierre De Loor*

Key words: 自动文本简化(ATS),错误检测,分类法,大型语言模型(LLMs),信息失真

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文提出了一种用于自动文本简化(ATS)错误检测和分类的测试集合，包括错误分类法、带标注的平行数据集以及现有模型的错误检测性能分析，旨在提升ATS评估的准确性。

Motivation: 当前ATS评估方法在大型语言模型(LLMs)时代显得不足，尤其是现有指标未能有效反映错误，导致信息失真问题。论文旨在填补这一空白，提供更精细的评估框架。

Method: 提出错误分类法（重点关注信息失真），构建人工标注的平行科学文本简化数据集，并分析现有模型基于该分类法的错误检测性能。

Result: 建立了首个针对ATS错误检测的标注数据集，验证了现有模型在错误分类上的局限性，为未来研究提供了基准工具。

Conclusion: 该研究为ATS领域提供了实用的错误评估工具，推动更可靠的模型开发，最终提升简化文本的质量。

Abstract: The general public often encounters complex texts but does not have the time
or expertise to fully understand them, leading to the spread of misinformation.
Automatic Text Simplification (ATS) helps make information more accessible, but
its evaluation methods have not kept up with advances in text generation,
especially with Large Language Models (LLMs). In particular, recent studies
have shown that current ATS metrics do not correlate with the presence of
errors. Manual inspections have further revealed a variety of errors,
underscoring the need for a more nuanced evaluation framework, which is
currently lacking. This resource paper addresses this gap by introducing a test
collection for detecting and classifying errors in simplified texts. First, we
propose a taxonomy of errors, with a formal focus on information distortion.
Next, we introduce a parallel dataset of automatically simplified scientific
texts. This dataset has been human-annotated with labels based on our proposed
taxonomy. Finally, we analyze the quality of the dataset, and we study the
performance of existing models to detect and classify errors from that
taxonomy. These contributions give researchers the tools to better evaluate
errors in ATS, develop more reliable models, and ultimately improve the quality
of automatically simplified texts.

</details>


### [64] [UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models](https://arxiv.org/abs/2505.14679)
*Xiaojie Gu,Guangxu Chen,Jungang Li,Jia-Chen Gu,Xuming Hu,Kai Zhang*

Key words: 终身学习, 大语言模型, 模型编辑, 高效更新, 线性代数, 归一化策略

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ULTRAEDIT是一种创新的大语言模型编辑方法，通过轻量级线性代数操作实现高效知识更新，速度快且资源消耗低，适用于大规模持续学习场景。

Motivation: 解决现有模型编辑方法在实际大规模持续学习中的效率和扩展性问题。

Method: 采用自包含的轻量级线性代数操作计算参数调整，结合终身归一化策略适应数据分布变化。

Result: 编辑速度比现有最快方法快7倍，显存消耗减少2/3，支持百万次编辑并保持高准确率。

Conclusion: ULTRAEDIT在多种模型编辑场景中表现优异，是大规模持续学习的理想解决方案。

Abstract: Lifelong learning enables large language models (LLMs) to adapt to evolving
information by continually updating their internal knowledge. An ideal system
should support efficient, wide-ranging updates while preserving existing
capabilities and ensuring reliable deployment. Model editing stands out as a
promising solution for this goal, offering a focused and efficient way to
revise a model's internal knowledge. Although recent paradigms have made
notable progress, they often struggle to meet the demands of practical lifelong
adaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally
new editing solution that is training-, subject- and memory-free, making it
particularly well-suited for ultra-scalable, real-world lifelong model editing.
ULTRAEDIT performs editing through a self-contained process that relies solely
on lightweight linear algebra operations to compute parameter shifts, enabling
fast and consistent parameter modifications with minimal overhead. To improve
scalability in lifelong settings, ULTRAEDIT employs a lifelong normalization
strategy that continuously updates feature statistics across turns, allowing it
to adapt to distributional shifts and maintain consistency over time. ULTRAEDIT
achieves editing speeds over 7x faster than the previous state-of-the-art
method-which was also the fastest known approach-while consuming less than 1/3
the VRAM, making it the only method currently capable of editing a 7B LLM on a
24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest
dataset in the field to date, with over 2M editing pairs-and demonstrate that
our method supports up to 1M edits while maintaining high accuracy.
Comprehensive experiments on four datasets and six models show that ULTRAEDIT
consistently achieves superior performance across diverse model editing
scenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.

</details>


### [65] [On the reliability of feature attribution methods for speech classification](https://arxiv.org/abs/2505.16406)
*Gaofei Shen,Hosein Mohebbi,Arianna Bisazza,Afra Alishahi,Grzegorz Chrupała*

Key words: 特征归因，语音处理，预训练模型，词对齐扰动

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了特征归因方法在语音处理中的可靠性，发现标准方法通常不可靠，但词对齐扰动方法在基于词的分类任务中表现良好。

Motivation: 随着预训练模型能力的增强，理解其输出决定因素的重要性增加。特征归因旨在揭示输入元素中对模型输出影响最大的部分。在语音处理中，输入信号的独特特性使得特征归因方法的应用具有挑战性。

Method: 研究了输入类型、聚合和扰动时间跨度等因素如何影响标准特征归因方法的可靠性，以及这些因素如何与每个分类任务的特征交互。

Result: 发现标准特征归因方法在语音领域中通常不可靠，但在基于词的分类任务中词对齐扰动方法例外。

Conclusion: 标准特征归因方法在语音处理中表现不佳，但词对齐扰动方法在特定任务中有效。

Abstract: As the capabilities of large-scale pre-trained models evolve, understanding
the determinants of their outputs becomes more important. Feature attribution
aims to reveal which parts of the input elements contribute the most to model
outputs. In speech processing, the unique characteristics of the input signal
make the application of feature attribution methods challenging. We study how
factors such as input type and aggregation and perturbation timespan impact the
reliability of standard feature attribution methods, and how these factors
interact with characteristics of each classification task. We find that
standard approaches to feature attribution are generally unreliable when
applied to the speech domain, with the exception of word-aligned perturbation
methods when applied to word-based classification tasks.

</details>


### [66] [From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs](https://arxiv.org/abs/2505.16408)
*Muhammad Farid Adilazuarda,Chen Cecilia Liu,Iryna Gurevych,Alham Fikri Aji*

Key words: 大型语言模型, 文化价值观, 世界价值观调查, 文化叙事, 任务特定行为

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了基于世界价值观调查（WVS）数据训练大型语言模型（LLMs）的局限性，发现仅依赖调查数据会导致文化同质化并干扰事实知识。通过引入维基百科和NormAd的文化叙事数据，提升了文化独特性。

Motivation: 大型语言模型在适应不同文化价值观时面临偏见和训练数据有限的挑战，现有方法主要依赖WVS数据，但其是否有效捕捉文化差异尚不明确。

Method: 系统研究基于WVS的训练方法，并用维基百科和NormAd的文化叙事数据增强WVS数据。

Result: 引入文化叙事数据比仅用调查数据更能提升文化独特性，但对下游任务效果不一。

Conclusion: 研究揭示了文化价值观对齐的复杂性，需结合多种数据源以指导任务特定行为。

Abstract: Adapting cultural values in Large Language Models (LLMs) presents significant
challenges, particularly due to biases and limited training data. Prior work
primarily aligns LLMs with different cultural values using World Values Survey
(WVS) data. However, it remains unclear whether this approach effectively
captures cultural nuances or produces distinct cultural representations for
various downstream tasks. In this paper, we systematically investigate
WVS-based training for cultural value adaptation and find that relying solely
on survey data can homogenize cultural norms and interfere with factual
knowledge. To investigate these issues, we augment WVS with encyclopedic and
scenario-based cultural narratives from Wikipedia and NormAd. While these
narratives may have variable effects on downstream tasks, they consistently
improve cultural distinctiveness than survey data alone. Our work highlights
the inherent complexity of aligning cultural values with the goal of guiding
task-specific behavior.

</details>


### [67] [Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning](https://arxiv.org/abs/2505.16410)
*Guanting Dong,Yifei Chen,Xiaoxi Li,Jiajie Jin,Hongjin Qian,Yutao Zhu,Hangyu Mao,Guorui Zhou,Zhicheng Dou,Ji-Rong Wen*

Key words: 大语言模型, 强化学习, 多工具协作推理, Tool-Star

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 介绍了一个名为Tool-Star的基于强化学习的框架，旨在通过自主调用多工具来增强大语言模型的多工具协作推理能力。

Motivation: 当前大语言模型在多工具协作推理方面的能力仍有待提升，Tool-Star旨在解决这一问题。

Method: 提出了一种工具集成推理数据合成流程，包括数据生成、质量归一化和难度分类；并采用两阶段训练框架（冷启动微调和多工具自评RL算法）来增强协作推理。

Result: 在10多个推理基准测试中表现出高效性。

Conclusion: Tool-Star在多工具协作推理中表现出色，为相关领域提供了新思路。

Abstract: Recently, large language models (LLMs) have shown remarkable reasoning
capabilities via large-scale reinforcement learning (RL). However, leveraging
the RL algorithm to empower effective multi-tool collaborative reasoning in
LLMs remains an open challenge. In this paper, we introduce Tool-Star, an
RL-based framework designed to empower LLMs to autonomously invoke multiple
external tools during stepwise reasoning. Tool-Star integrates six types of
tools and incorporates systematic designs in both data synthesis and training.
To address the scarcity of tool-use data, we propose a general tool-integrated
reasoning data synthesis pipeline, which combines tool-integrated prompting
with hint-based sampling to automatically and scalably generate tool-use
trajectories. A subsequent quality normalization and difficulty-aware
classification process filters out low-quality samples and organizes the
dataset from easy to hard. Furthermore, we propose a two-stage training
framework to enhance multi-tool collaborative reasoning by: (1) cold-start
fine-tuning, which guides LLMs to explore reasoning patterns via
tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with
hierarchical reward design, which reinforces reward understanding and promotes
effective tool collaboration. Experimental analyses on over 10 challenging
reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.
The code is available at https://github.com/dongguanting/Tool-Star.

</details>


### [68] [Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16415)
*Ruizhe Li,Chen Chen,Yuchen Hu,Yanjun Gao,Xi Wang,Emine Yilmaz*

Key words: 检索增强生成, 上下文归因, Jensen-Shannon Divergence, 计算效率, 机制分析

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出ARC-JSD方法，通过Jensen-Shannon Divergence实现高效准确的上下文归因，无需微调或替代建模，显著提升计算效率和准确率。

Motivation: 现有RAG模型中，上下文归因方法计算成本高，依赖微调或人工标注，亟需高效且准确的解决方案。

Method: 提出ARC-JSD方法，基于Jensen-Shannon Divergence直接识别关键上下文句子，避免额外训练或替代模型。

Result: 实验显示ARC-JSD在TyDi QA等基准测试中准确率和计算效率均优于替代方法，并揭示了RAG模型内部归因机制。

Conclusion: ARC-JSD为上下文归因提供了高效、可解释的方案，同时深入解析了模型内部工作原理。

Abstract: Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)
combined with external contexts to enhance the accuracy and reliability of
generated responses. However, reliably attributing generated content to
specific context segments, context attribution, remains challenging due to the
computationally intensive nature of current methods, which often require
extensive fine-tuning or human annotation. In this work, we introduce a novel
Jensen-Shannon Divergence driven method to Attribute Response to Context
(ARC-JSD), enabling efficient and accurate identification of essential context
sentences without additional fine-tuning or surrogate modelling. Evaluations on
a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using
instruction-tuned LLMs in different scales demonstrate superior accuracy and
significant computational efficiency improvements compared to the previous
surrogate-based method. Furthermore, our mechanistic analysis reveals specific
attention heads and multilayer perceptron (MLP) layers responsible for context
attribution, providing valuable insights into the internal workings of RAG
models.

</details>


### [69] [Exploring the Relationship Between Diversity and Quality in Ad Text Generation](https://arxiv.org/abs/2505.16418)
*Yoichi Aoki,Soichiro Murakami,Ukyo Honda,Akihiko Kato*

Key words: 广告文本生成，多样性，文本质量，自然语言生成

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨了广告文本生成中多样性与质量的关联，分析了多样化增强方法及其对广告效果的影响。

Motivation: 由于广告文本的特殊性，现有多样性增强方法在广告生成中的作用未充分研究，有必要探索其与广告质量的关系。

Method: 通过考虑多样化方法、超参数、输入输出格式及模型，分析其对广告文本质量的影响。

Result: 研究发现多样化方法在广告生成中对质量有显著影响，具体效果取决于方法选择及参数设定。

Conclusion: 广告文本生成需针对性调整多样化方法以平衡多样性与质量，为实际应用提供指导。

Abstract: In natural language generation for advertising, creating diverse and engaging
ad texts is crucial for capturing a broad audience and avoiding advertising
fatigue. Regardless of the importance of diversity, the impact of the
diversity-enhancing methods in ad text generation -- mainly tested on tasks
such as summarization and machine translation -- has not been thoroughly
explored. Ad text generation significantly differs from these tasks owing to
the text style and requirements. This research explores the relationship
between diversity and ad quality in ad text generation by considering multiple
factors, such as diversity-enhancing methods, their hyperparameters,
input-output formats, and the models.

</details>


### [70] [WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.16421)
*Zhepei Wei,Wenlin Yao,Yao Liu,Weizhi Zhang,Qin Lu,Liang Qiu,Changlong Yu,Puyang Xu,Chao Zhang,Bing Yin,Hyokun Yun,Lihong Li*

Key words: 强化学习, 多轮交互, 网页代理, 长时决策, 行为克隆

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: WebAgent-R1是一种端到端的多轮强化学习框架，通过异步生成多样化轨迹和二元奖励，显著提升了多轮网页交互任务的性能。在WebArena-Lite基准测试中，任务成功率提升明显，优于现有方法和大模型。

Motivation: 当前强化学习主要关注单轮任务，而多轮网页交互任务由于动态性和长时决策复杂性，训练高效代理仍具挑战性。

Method: 提出WebAgent-R1框架，基于在线交互异步生成多样化轨迹，依赖二元奖励（任务成功与否）进行训练。包含两种变体（Zero和CoT），探讨初始化策略和长链推理的作用。

Result: 在WebArena-Lite中，Qwen-2.5-3B和Llama-3.1-8B的任务成功率分别从6.1%提升至33.9%，从8.5%提升至44.8%。

Conclusion: WebAgent-R1框架在多轮网页交互任务中表现优异，其基于思考的提示策略和测试时扩展策略有效，且初始化策略（如行为克隆）和长链推理对性能至关重要。

Abstract: While reinforcement learning (RL) has demonstrated remarkable success in
enhancing large language models (LLMs), it has primarily focused on single-turn
tasks such as solving math problems. Training effective web agents for
multi-turn interactions remains challenging due to the complexity of
long-horizon decision-making across dynamic web interfaces. In this work, we
present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework
for training web agents. It learns directly from online interactions with web
environments by asynchronously generating diverse trajectories, entirely guided
by binary rewards depending on task success. Experiments on the WebArena-Lite
benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task
success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to
44.8%, significantly outperforming existing state-of-the-art methods and strong
proprietary models such as OpenAI o3. In-depth analyses reveal the
effectiveness of the thinking-based prompting strategy and test-time scaling
through increased interactions for web tasks. We further investigate different
RL initialization policies by introducing two variants, namely WebAgent-R1-Zero
and WebAgent-R1-CoT, which highlight the importance of the warm-up training
stage (i.e., behavior cloning) and provide insights on incorporating long
chain-of-thought (CoT) reasoning in web agents.

</details>


### [71] [$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion](https://arxiv.org/abs/2505.16425)
*Jing Bi,Pinxin Liu,Ali Vosoughi,Jiarui Wu,Jinxi He,Chenliang Xu*

Key words: NLP, 程序知识, 视觉指令, 语言学结构, 多模态

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种基于语言的框架，将程序文本转化为视觉指令，通过语言学结构建模和视觉生成技术，显著提升了指令与视觉内容的一致性。

Motivation: 解决NLP中程序知识有效传达的挑战，特别是文本无法充分表达复杂物理动作和空间关系的问题。

Method: 采用语言学结构分解（目标陈述和步骤序列），结合选区解析器文本编码、成对语篇连贯性模型和新的评估协议。

Result: 在三个指令数据集上，方法显著优于现有基线，生成更准确反映语言内容和序列性的视觉内容。

Conclusion: 该研究为程序语言与视觉内容的关联提供了新方法，应用领域包括教育、任务指导和多模态语言理解。

Abstract: The effective communication of procedural knowledge remains a significant
challenge in natural language processing (NLP), as purely textual instructions
often fail to convey complex physical actions and spatial relationships. We
address this limitation by proposing a language-driven framework that
translates procedural text into coherent visual instructions. Our approach
models the linguistic structure of instructional content by decomposing it into
goal statements and sequential steps, then conditioning visual generation on
these linguistic elements. We introduce three key innovations: (1) a
constituency parser-based text encoding mechanism that preserves semantic
completeness even with lengthy instructions, (2) a pairwise discourse coherence
model that maintains consistency across instruction sequences, and (3) a novel
evaluation protocol specifically designed for procedural language-to-image
alignment. Our experiments across three instructional datasets (HTStep,
CaptainCook4D, and WikiAll) demonstrate that our method significantly
outperforms existing baselines in generating visuals that accurately reflect
the linguistic content and sequential nature of instructions. This work
contributes to the growing body of research on grounding procedural language in
visual content, with applications spanning education, task guidance, and
multimodal language understanding.

</details>


### [72] [Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems](https://arxiv.org/abs/2505.16429)
*Song Jin,Juntian Zhang,Yuhan Liu,Xun Zhang,Yufei Zhang,Guojun Yin,Fei Jiang,Wei Lin,Rui Yan*

Key words: 推荐系统,代理模拟,动态交互,LLM,涌现现象

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: RecInter是一个新型的基于代理的推荐系统模拟平台，通过实时动态更新商品属性和引入商家代理，显著提升了模拟的真实性和可信度。

Motivation: 传统A/B测试资源消耗大，离线方法难以捕捉动态用户-平台交互，因此需要一个更真实的模拟平台。

Method: 采用基于代理的模拟，结合多维用户画像、高级代理架构和LLM微调，实现动态交互。

Result: 平台显著提升了模拟可信度，成功复现了品牌忠诚度等涌现现象。

Conclusion: RecInter为推荐系统研究提供了可信的测试平台。

Abstract: Evaluating and iterating upon recommender systems is crucial, yet traditional
A/B testing is resource-intensive, and offline methods struggle with dynamic
user-platform interactions. While agent-based simulation is promising, existing
platforms often lack a mechanism for user actions to dynamically reshape the
environment. To bridge this gap, we introduce RecInter, a novel agent-based
simulation platform for recommender systems featuring a robust interaction
mechanism. In RecInter platform, simulated user actions (e.g., likes, reviews,
purchases) dynamically update item attributes in real-time, and introduced
Merchant Agents can reply, fostering a more realistic and evolving ecosystem.
High-fidelity simulation is ensured through Multidimensional User Profiling
module, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought
(CoT) enriched interaction data. Our platform achieves significantly improved
simulation credibility and successfully replicates emergent phenomena like
Brand Loyalty and the Matthew Effect. Experiments demonstrate that this
interaction mechanism is pivotal for simulating realistic system evolution,
establishing our platform as a credible testbed for recommender systems
research.

</details>


### [73] [University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection](https://arxiv.org/abs/2505.16460)
*Ikhlasul Akmal Hanif,Eryawan Presma Yulianrifat,Jaycent Gunawan Ongris,Eduardus Tjitrahardja,Muhammad Falensi Azmi,Rahmat Bryan Naufal,Alfan Farizki Wicaksono*

Key words: 多标签情感分类；变换器模型；微调；集成模型；多语言

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了针对SemEval 2025 Task 11 Track A的多标签情感分类方法，通过两种主要策略（完全微调变换器模型和仅训练分类器）在28种语言上进行实验。结果显示，基于提示的编码器如mE5和BGE的分类器训练表现优于完全微调XLMR和mBERT，最佳模型为多BGE模型集成搭配CatBoost分类器，平均F1-macro得分为56.58。

Motivation: 解决多语言多标签情感分类的挑战，探索不同策略（完全微调与分类器训练）在不同语言中的表现，并提升任务性能。

Method: 采用两种策略：完全微调变换器模型（如XLMR和mBERT）和仅在基于提示的编码器（如mE5和BGE）上训练分类器，同时评估不同设置（微调策略、模型架构、损失函数等）。最佳模型为多BGE模型集成搭配CatBoost分类器。

Result: 基于提示的编码器（mE5和BGE）的分类器训练表现显著优于完全微调XLMR和mBERT。集成模型在28种语言上的平均F1-macro得分为56.58。

Conclusion: 在基于提示的编码器上训练分类器是多语言多标签情感分类的有效策略，集成模型能进一步提升性能。

Abstract: This paper presents our approach for SemEval 2025 Task 11 Track A, focusing
on multilabel emotion classification across 28 languages. We explore two main
strategies: fully fine-tuning transformer models and classifier-only training,
evaluating different settings such as fine-tuning strategies, model
architectures, loss functions, encoders, and classifiers. Our findings suggest
that training a classifier on top of prompt-based encoders such as mE5 and BGE
yields significantly better results than fully fine-tuning XLMR and mBERT. Our
best-performing model on the final leaderboard is an ensemble combining
multiple BGE models, where CatBoost serves as the classifier, with different
configurations. This ensemble achieves an average F1-macro score of 56.58
across all languages.

</details>


### [74] [Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization](https://arxiv.org/abs/2505.16467)
*Vera Neplenbroek,Arianna Bisazza,Raquel Fernández*

Key words: LLMs, 隐性个性化, 刻板印象, 人口统计推断, 模型偏见

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LLMs能从对话中的细微线索推断用户人口统计信息，可能导致对少数群体的偏见回应。研究发现，即使用户明确声明不同身份，模型仍会基于刻板印象推断，但通过干预内部表征可以缓解这一问题。

Motivation: 探讨LLMs如何基于刻板印象推断用户身份，及其对少数群体响应质量的影响，以解决模型偏见问题。

Method: 使用可控合成对话分析LLMs潜在用户表征，结合模型内部结构和生成答案研究推断行为，并设计线性探针干预。

Result: LLMs确实会基于刻板信号推断人口属性，即使用户明确声明不同身份。干预内部表征可有效缓解偏见。

Conclusion: LLMs在用户身份表征上需更透明可控，干预内部表征是减少偏见的可行方法。

Abstract: Generative Large Language Models (LLMs) infer user's demographic information
from subtle cues in the conversation -- a phenomenon called implicit
personalization. Prior work has shown that such inferences can lead to lower
quality responses for users assumed to be from minority groups, even when no
demographic information is explicitly provided. In this work, we systematically
explore how LLMs respond to stereotypical cues using controlled synthetic
conversations, by analyzing the models' latent user representations through
both model internals and generated answers to targeted user questions. Our
findings reveal that LLMs do infer demographic attributes based on these
stereotypical signals, which for a number of groups even persists when the user
explicitly identifies with a different demographic group. Finally, we show that
this form of stereotype-driven implicit personalization can be effectively
mitigated by intervening on the model's internal representations using a
trained linear probe to steer them toward the explicitly stated identity. Our
results highlight the need for greater transparency and control in how LLMs
represent user identity.

</details>


### [75] [Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning](https://arxiv.org/abs/2505.16483)
*Shuzheng Si,Haozhe Zhao,Cheng Gao,Yuzhuo Bai,Zhitong Wang,Bofei Gao,Kangyang Luo,Wenhao Li,Yufei Huang,Gang Chen,Fanchao Qi,Minjia Zhang,Baobao Chang,Maosong Sun*

Key words: LLM, faithfulness, CANOE, Dual-GRPO, QA

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出CANOE框架，通过合成短形式QA数据和Dual-GRPO强化学习方法，提升LLM在长短文本生成中的忠实度，无需人工标注。实验表明其在11项任务中优于GPT-4o等先进模型。

Motivation: 为提高LLM在上下文中的忠实度，构建可靠的信息检索系统，需解决人工标注成本高和短文本优化过度的问题。

Method: 1. 合成短形式QA数据；2. 提出Dual-GRPO强化学习方法，结合三种规则奖励，优化长短文本生成。

Result: CANOE显著提升LLM的忠实度，在11项任务中超越GPT-4o等先进模型。

Conclusion: CANOE框架高效且无需人工标注，为提升LLM忠实度提供了可行方案。

Abstract: Teaching large language models (LLMs) to be faithful in the provided context
is crucial for building reliable information-seeking systems. Therefore, we
propose a systematic framework, CANOE, to improve the faithfulness of LLMs in
both short-form and long-form generation tasks without human annotations.
Specifically, we first synthesize short-form question-answering (QA) data with
four diverse tasks to construct high-quality and easily verifiable training
data without human annotation. Also, we propose Dual-GRPO, a rule-based
reinforcement learning method that includes three tailored rule-based rewards
derived from synthesized short-form QA data, while simultaneously optimizing
both short-form and long-form response generation. Notably, Dual-GRPO
eliminates the need to manually label preference data to train reward models
and avoids over-optimizing short-form generation when relying only on the
synthesized short-form QA data. Experimental results show that CANOE greatly
improves the faithfulness of LLMs across 11 different downstream tasks, even
outperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.

</details>


### [76] [LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing](https://arxiv.org/abs/2505.16491)
*Dario Di Palma,Alessandro De Bellis,Giovanni Servedio,Vito Walter Anelli,Fedelucio Narducci,Tommaso Di Noia*

Key words: 大型语言模型, Llama, 情感分析, 探针分类器, 内存优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究探究了Llama模型中情感特征的分布及对情感分析的影响，通过探针分类器识别出情感信息主要集中在中间层，检测准确率提高了14%，内存需求降低了57%。

Motivation: 理解大型语言模型（LLMs）如何捕捉情感信息，并优化情感分析任务的效率和性能。

Method: 使用探针分类器分析不同层和规模的情感编码，识别最有效捕捉情感信号的层和池化方法。

Result: 情感信息在中间层最集中，检测准确率提升14%，内存需求降低57%，且解码器模型中末个令牌并非总是最具信息量。

Conclusion: 层特异性探测是超越提示技术的有效方法，可提升模型效用并减少内存需求。

Abstract: Large Language Models (LLMs) have rapidly become central to NLP,
demonstrating their ability to adapt to various tasks through prompting
techniques, including sentiment analysis. However, we still have a limited
understanding of how these models capture sentiment-related information. This
study probes the hidden layers of Llama models to pinpoint where sentiment
features are most represented and to assess how this affects sentiment
analysis.
  Using probe classifiers, we analyze sentiment encoding across layers and
scales, identifying the layers and pooling methods that best capture sentiment
signals. Our results show that sentiment information is most concentrated in
mid-layers for binary polarity tasks, with detection accuracy increasing up to
14% over prompting techniques. Additionally, we find that in decoder-only
models, the last token is not consistently the most informative for sentiment
encoding. Finally, this approach enables sentiment tasks to be performed with
memory requirements reduced by an average of 57%.
  These insights contribute to a broader understanding of sentiment in LLMs,
suggesting layer-specific probing as an effective approach for sentiment tasks
beyond prompting, with potential to enhance model utility and reduce memory
requirements.

</details>


### [77] [Sparse Activation Editing for Reliable Instruction Following in Narratives](https://arxiv.org/abs/2505.16505)
*Runcong Zhao,Chengyu Cao,Qinglin Zhu,Xiucheng Lv,Shun Shao,Lin Gui,Ruifeng Xu,Yulan He*

Key words: 指令遵循,自然语言处理,无监督学习,复杂叙述

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Concise-SAE通过自然语言指令识别和编辑相关神经元，提升语言模型在复杂叙述场景中的指令遵循能力，无需标记数据。FreeInstruct基准包含1,212个多样实例，验证了方法的有效性。

Motivation: 解决现有基准未能捕捉的复杂叙述场景中语言模型指令遵循的困难。

Method: 提出无需训练的框架Concise-SAE，通过自然语言指令识别并编辑指令相关神经元。

Result: 在FreeInstruct基准上表现优异，保持生成质量的同时实现最先进的指令遵循。

Conclusion: Concise-SAE不仅适用于复杂叙述，还能推广到多样化任务中。

Abstract: Complex narrative contexts often challenge language models' ability to follow
instructions, and existing benchmarks fail to capture these difficulties. To
address this, we propose Concise-SAE, a training-free framework that improves
instruction following by identifying and editing instruction-relevant neurons
using only natural language instructions, without requiring labelled data. To
thoroughly evaluate our method, we introduce FreeInstruct, a diverse and
realistic benchmark of 1,212 examples that highlights the challenges of
instruction following in narrative-rich settings. While initially motivated by
complex narratives, Concise-SAE demonstrates state-of-the-art instruction
adherence across varied tasks without compromising generation quality.

</details>


### [78] [AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios](https://arxiv.org/abs/2505.16514)
*Yuting Huang,Meitong Guo,Yiquan Wu,Ang Li,Xiaozhong Liu,Keting Yin,Changlong Sun,Fei Wu,Kun Kuang*

Key words: LegalAI, 上诉案例, 数据集, 司法决策, 任务评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文介绍了AppealCase数据集，包含10,000对真实的一审和二审法律文书，并标注了五个上诉审查核心维度。基于此，提出了五项新任务，评估了20个主流模型，结果显示出上诉场景的复杂性。

Motivation: 当前LegalAI主要关注单例判决分析，忽略了上诉流程的重要性。上诉是纠错和确保公平审判的核心机制，具有重要研究和实践价值。

Method: 构建了AppealCase数据集，标注了五个关键维度，并设计了五项新任务，评估了20个主流模型。

Result: 所有模型在判决逆转预测任务上的F1分数均低于50%，表明上诉场景的复杂性。

Conclusion: AppealCase数据集有望推动LegalAI在上诉案例分析中的研究，提升司法决策一致性。

Abstract: Recent advances in LegalAI have primarily focused on individual case judgment
analysis, often overlooking the critical appellate process within the judicial
system. Appeals serve as a core mechanism for error correction and ensuring
fair trials, making them highly significant both in practice and in research.
To address this gap, we present the AppealCase dataset, consisting of 10,000
pairs of real-world, matched first-instance and second-instance documents
across 91 categories of civil cases. The dataset also includes detailed
annotations along five dimensions central to appellate review: judgment
reversals, reversal reasons, cited legal provisions, claim-level decisions, and
whether there is new information in the second instance. Based on these
annotations, we propose five novel LegalAI tasks and conduct a comprehensive
evaluation across 20 mainstream models. Experimental results reveal that all
current models achieve less than 50% F1 scores on the judgment reversal
prediction task, highlighting the complexity and challenge of the appeal
scenario. We hope that the AppealCase dataset will spur further research in
LegalAI for appellate case analysis and contribute to improving consistency in
judicial decision-making.

</details>


### [79] [CUB: Benchmarking Context Utilisation Techniques for Language Models](https://arxiv.org/abs/2505.16518)
*Lovisa Hagström,Youna Kim,Haeun Yu,Sang-goo Lee,Richard Johansson,Hyunsoo Cho,Isabelle Augenstein*

Key words: 知识密集型任务，上下文利用操作技术（CMTs），检索增强生成（RAG），基准测试

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文开发了CUB基准测试，用于评估和比较多种上下文利用操作技术（CMTs），结果表明现有CMT在真实场景中表现不足，需开发更全面的方法。

Motivation: 解决语言模型在知识密集型任务中忽略相关或受无关上下文干扰的问题，需系统评估上下文利用操作技术（CMTs）的实际效果。

Method: 开发CUB基准测试，涵盖三种关键上下文类型，并在三个数据集上评估七种代表性CMTs应用于九种语言模型的表现。

Result: 多数现有CMTs难以应对真实检索增强场景中的多样上下文类型，且在合成数据集上的性能被高估。

Conclusion: 需开发能处理多样化上下文的CMTs，并进行更全面的测试。

Abstract: Incorporating external knowledge is crucial for knowledge-intensive tasks,
such as question answering and fact checking. However, language models (LMs)
may ignore relevant information that contradicts outdated parametric memory or
be distracted by irrelevant contexts. While many context utilisation
manipulation techniques (CMTs) that encourage or suppress context utilisation
have recently been proposed to alleviate these issues, few have seen systematic
comparison. In this paper, we develop CUB (Context Utilisation Benchmark) to
help practitioners within retrieval-augmented generation (RAG) identify the
best CMT for their needs. CUB allows for rigorous testing on three distinct
context types, observed to capture key challenges in realistic context
utilisation scenarios. With this benchmark, we evaluate seven state-of-the-art
methods, representative of the main categories of CMTs, across three diverse
datasets and tasks, applied to nine LMs. Our results show that most of the
existing CMTs struggle to handle the full set of types of contexts that may be
encountered in real-world retrieval-augmented scenarios. Moreover, we find that
many CMTs display an inflated performance on simple synthesised datasets,
compared to more realistic datasets with naturally occurring samples.
Altogether, our results show the need for holistic tests of CMTs and the
development of CMTs that can handle multiple context types.

</details>


### [80] [Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs](https://arxiv.org/abs/2505.16520)
*Giovanni Servedio,Alessandro De Bellis,Dario Di Palma,Vito Walter Anelli,Tommaso Di Noia*

Key words: 大型语言模型、事实性幻觉、虚假内容、数据集、评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文质疑之前关于大型语言模型（LLM）在生成虚假内容时内部状态隐含真实性信息的研究，提出了更真实的数据集生成方法，并验证了之前研究的局限性。

Motivation: 解决大型语言模型生成虚假内容的问题，并评估其内部状态是否隐含真实性信息。

Method: （1）从表格数据中采样真实与虚假事实句子；（2）从问答数据集中生成依赖于LLM的真实与虚假数据集。

Result: 验证了之前研究的部分结果，但发现LLM生成的数据集仍具有挑战性。

Conclusion: 为未来LLM事实性研究奠定基础，并提供了更有效的评估指南。

Abstract: Factual hallucinations are a major challenge for Large Language Models
(LLMs). They undermine reliability and user trust by generating inaccurate or
fabricated content. Recent studies suggest that when generating false
statements, the internal states of LLMs encode information about truthfulness.
However, these studies often rely on synthetic datasets that lack realism,
which limits generalization when evaluating the factual accuracy of text
generated by the model itself. In this paper, we challenge the findings of
previous work by investigating truthfulness encoding capabilities, leading to
the generation of a more realistic and challenging dataset. Specifically, we
extend previous work by introducing: (1) a strategy for sampling plausible
true-false factoid sentences from tabular data and (2) a procedure for
generating realistic, LLM-dependent true-false datasets from Question Answering
collections. Our analysis of two open-source LLMs reveals that while the
findings from previous studies are partially validated, generalization to
LLM-generated datasets remains challenging. This study lays the groundwork for
future research on factuality in LLMs and offers practical guidelines for more
effective evaluation.

</details>


### [81] [Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing](https://arxiv.org/abs/2505.16522)
*Zhouhao Sun,Zhiyuan Kan,Xiao Ding,Li Du,Yang Zhao,Bing Qin,Ting Liu*

Key words: 大型语言模型, 多偏见, 因果效应估计, 泛化能力, 消除偏见

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文提出了一个多偏见基准和因果效应估计引导的多偏见消除方法（CMBE），以解决当前大型语言模型（LLMs）在推理中仍存在的多偏见问题，并提升模型的泛化能力。

Motivation: 尽管大型语言模型取得了显著进展，但现有研究显示其在推理中仍可能利用偏见，导致泛化能力不佳。现有基准通常仅包含单一偏见，而实际应用中数据可能同时存在多种偏见，因此需要新的方法来解决这一问题。

Method: 作者提出了CMBE方法，首先同时估计多种偏见的因果效应，然后在推理过程中从语义信息和偏见的总体因果效应中消除偏见的因果效应。

Result: 实验结果表明，CMBE能有效同时消除多种偏见，显著提升LLMs的泛化能力。

Conclusion: CMBE为解决LLMs中的多偏见问题提供了有效方法，并为提升模型泛化能力开辟了新方向。

Abstract: Despite significant progress, recent studies have indicated that current
large language models (LLMs) may still utilize bias during inference, leading
to the poor generalizability of LLMs. Some benchmarks are proposed to
investigate the generalizability of LLMs, with each piece of data typically
containing one type of controlled bias. However, a single piece of data may
contain multiple types of biases in practical applications. To bridge this gap,
we propose a multi-bias benchmark where each piece of data contains five types
of biases. The evaluations conducted on this benchmark reveal that the
performance of existing LLMs and debiasing methods is unsatisfying,
highlighting the challenge of eliminating multiple types of biases
simultaneously. To overcome this challenge, we propose a causal effect
estimation-guided multi-bias elimination method (CMBE). This method first
estimates the causal effect of multiple types of biases simultaneously.
Subsequently, we eliminate the causal effect of biases from the total causal
effect exerted by both the semantic information and biases during inference.
Experimental results show that CMBE can effectively eliminate multiple types of
bias simultaneously to enhance the generalizability of LLMs.

</details>


### [82] [EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance](https://arxiv.org/abs/2505.16526)
*Heejae Suh,Yejin Jeon,Deokhyung Kang,Taehee Park,Yejin Min,Gary Geunbae Lee*

Key words: 小型大语言模型，主题一致性，任务导向对话系统，EnSToM

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种名为EnSToM的新方法，通过动态调整引导强度来解决小型大语言模型在任务导向对话系统中的主题一致性不足问题。

Motivation: 小型大语言模型在资源受限环境中表现高效，但其在任务导向对话系统中难以维持主题一致性，尤其在面对离题或恶意输入时。

Method: 提出Entropy-scaled Steering vectors for Topic Maintenance (EnSToM)，通过输入不确定性动态调整引导强度，以有效处理离题干扰并保持主题准确性。

Result: 实验表明，相比于微调方法，EnSToM在较小数据量下显著提升了性能，同时保持了模型效率。

Conclusion: EnSToM为增强基于小型大语言模型的对话系统提供了一种高效且可靠的解决方案。

Abstract: Small large language models (sLLMs) offer the advantage of being lightweight
and efficient, which makes them suitable for resource-constrained environments.
However, sLLMs often struggle to maintain topic consistency in task-oriented
dialogue systems, which is critical for scenarios such as service chatbots.
Specifically, it is important to ensure that the model denies off-topic or
malicious inputs and adheres to its intended functionality so as to prevent
potential misuse and uphold reliability. Towards this, existing activation
engineering approaches have been proposed to manipulate internal activations
during inference. While these methods are effective in certain scenarios, our
preliminary experiments reveal their limitations in ensuring topic adherence.
Therefore, to address this, we propose a novel approach termed Entropy-scaled
Steering vectors for Topic Maintenance (EnSToM). EnSToM dynamically adjusts the
steering intensity based on input uncertainty, which allows the model to handle
off-topic distractors effectively while preserving on-topic accuracy. Our
experiments demonstrate that EnSToM achieves significant performance gain with
a relatively small data size compared to fine-tuning approaches. By improving
topic adherence without compromising efficiency, our approach provides a robust
solution for enhancing sLLM-based dialogue systems.

</details>


### [83] [Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models](https://arxiv.org/abs/2505.16538)
*Ercong Nie,Helmut Schmid,Hinrich Schütze*

Key words: 语言混淆, 大型语言模型, 机制可解释性, 神经元编辑, 多语言建模

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文通过机制可解释性研究分析了大型语言模型中的语言混淆现象，发现了关键神经元编辑可显著减少混淆。

Motivation: 解决英语中心模型在生成非目标语言时的语言混淆问题。

Method: 结合行为基准测试和神经元级分析，使用语言混淆基准（LCB）识别混淆点（CP），并通过TunedLens和神经元归因分析最后一层的转换失败。

Result: 通过编辑少量关键神经元，显著减少了语言混淆，同时保持模型的通用能力和流畅性。

Conclusion: 神经元级干预是提升多语言建模鲁棒性和可解释性的有前景方向。

Abstract: Language confusion -- where large language models (LLMs) generate unintended
languages against the user's need -- remains a critical challenge, especially
for English-centric models. We present the first mechanistic interpretability
(MI) study of language confusion, combining behavioral benchmarking with
neuron-level analysis. Using the Language Confusion Benchmark (LCB), we show
that confusion points (CPs) -- specific positions where language switches occur
-- are central to this phenomenon. Through layer-wise analysis with TunedLens
and targeted neuron attribution, we reveal that transition failures in the
final layers drive confusion. We further demonstrate that editing a small set
of critical neurons, identified via comparative analysis with
multilingual-tuned models, substantially mitigates confusion without harming
general competence or fluency. Our approach matches multilingual alignment in
confusion reduction for most languages and yields cleaner, higher-quality
outputs. These findings provide new insights into the internal dynamics of LLMs
and highlight neuron-level interventions as a promising direction for robust,
interpretable multilingual language modeling.

</details>


### [84] [Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains](https://arxiv.org/abs/2505.16552)
*Wenhui Tan,Jiaze Li,Jianzhong Ju,Zhenbo Luo,Jian Luan,Ruihua Song*

Key words: LLM, CoT, 压缩潜在推理, 强化学习, 数学推理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: CoLaR 是一种新型框架，通过动态压缩推理过程来提升 LLM 的效率，结合了监督微调和强化学习，显著减少了推理链长度，同时保持了高准确性。

Motivation: 传统 CoT 推理在 token 级别效率低下且计算成本高，CoLaR 旨在通过在潜在空间动态压缩推理过程来提升效率。

Method: 1. 监督微调：加入辅助的压缩嵌入预测目标；2. 强化学习：利用潜在头的非确定性探索多样推理路径。

Result: 在数学推理任务上，CoLaR 比基线方法准确率提升 14.1%，推理链长度减少 53.3%，在更复杂任务中性能提升 5.4%，长度减少 82.8%。

Conclusion: CoLaR 显著优化了 LLM 的推理效率，适用于高复杂度任务，同时支持动态调整推理速度。

Abstract: Large Language Models (LLMs) achieve superior performance through
Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are
computationally expensive and inefficient. In this paper, we introduce
Compressed Latent Reasoning (CoLaR), a novel framework that dynamically
compresses reasoning processes in latent space through a two-stage training
approach. First, during supervised fine-tuning, CoLaR extends beyond next-token
prediction by incorporating an auxiliary next compressed embedding prediction
objective. This process merges embeddings of consecutive tokens using a
compression factor randomly sampled from a predefined range, and trains a
specialized latent head to predict distributions of subsequent compressed
embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that
leverages the latent head's non-deterministic nature to explore diverse
reasoning paths and exploit more compact ones. This approach enables CoLaR to:
i) perform reasoning at a dense latent level (i.e., silently), substantially
reducing reasoning chain length, and ii) dynamically adjust reasoning speed at
inference time by simply prompting the desired compression factor. Extensive
experiments across four mathematical reasoning datasets demonstrate that CoLaR
achieves 14.1% higher accuracy than latent-based baseline methods at comparable
compression ratios, and reduces reasoning chain length by 53.3% with only 4.8%
performance degradation compared to explicit CoT method. Moreover, when applied
to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR
demonstrates performance gains of up to 5.4% while dramatically reducing latent
reasoning chain length by 82.8%. The code and models will be released upon
acceptance.

</details>


### [85] [ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts](https://arxiv.org/abs/2505.16566)
*Dongwon Noh,Donghyeok Koh,Junghun Yuk,Gyuwan Kim,Jaeyong Lee,Kyungtae Lim,Cheoneum Park*

Key words: 大型语言模型，学术评估，双语数据集，复杂任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了ScholarBench，一个新的用于评估大型语言模型（LLM）专业知识和学术解决问题能力的基准测试，它针对复杂学术任务设计，包含双语数据集。

Motivation: 当前评估LLM领域专业知识的基准测试规模不足，无法处理复杂的学术任务，因此需要更专业的评估工具。

Method: 通过三步构建过程，设计了ScholarBench，涵盖五种问题类型和八个研究领域，确保高质量评估数据。

Result: ScholarBench包含5031个韩语和5309个英语例子，即使最先进的模型如o3-mini平均得分仅0.543，显示其高难度。

Conclusion: ScholarBench是一个具有挑战性的新基准测试，可以有效评估LLM的专业知识和学术推理能力。

Abstract: Prior benchmarks for evaluating the domain-specific knowledge of large
language models (LLMs) lack the scalability to handle complex academic tasks.
To address this, we introduce \texttt{ScholarBench}, a benchmark centered on
deep expert knowledge and complex academic problem-solving, which evaluates the
academic reasoning ability of LLMs and is constructed through a three-step
process. \texttt{ScholarBench} targets more specialized and logically complex
contexts derived from academic literature, encompassing five distinct problem
types. Unlike prior benchmarks, \texttt{ScholarBench} evaluates the
abstraction, comprehension, and reasoning capabilities of LLMs across eight
distinct research domains. To ensure high-quality evaluation data, we define
category-specific example attributes and design questions that are aligned with
the characteristic research methodologies and discourse structures of each
domain. Additionally, this benchmark operates as an English-Korean bilingual
dataset, facilitating simultaneous evaluation for linguistic capabilities of
LLMs in both languages. The benchmark comprises 5,031 examples in Korean and
5,309 in English, with even state-of-the-art models like o3-mini achieving an
average evaluation score of only 0.543, demonstrating the challenging nature of
this benchmark.

</details>


### [86] [URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training](https://arxiv.org/abs/2505.16570)
*Dongyang Fan,Vinko Sabolčec,Martin Jaggi*

Key words: 大型语言模型, 元数据, 预训练, 上下文感知, 生成控制

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了在大型语言模型预训练中加入不同类型的上下文元数据（如URL、质量评分、主题/格式）的效果，发现只有URL能加速训练，而长提示下URL条件化能提升下游性能。主题和格式元数据虽不加速训练，但可用于生成控制。

Motivation: 探索在预训练阶段加入上下文元数据（如来源、质量、主题）的效果，以弥补传统上下文无关学习的不足，提升训练效率和下游任务表现。

Method: 系统评估不同类型的元数据（URL、质量评分、主题/格式）对预训练效率和下游任务的影响，并在更长提示下测试性能。

Result: 仅URL能加速训练，而质量评分和主题/格式信息无显著帮助；URL条件化在长提示下提升下游性能；主题/格式元数据可用于生成控制。

Conclusion: 上下文感知预训练（尤其是URL）能提升效率和控制性，但需根据元数据类型和推理条件选择性使用。

Abstract: Large Language Models (LLMs) are commonly pretrained on vast corpora of text
without utilizing contextual metadata such as source, quality, or topic,
leading to a context-free learning paradigm. While recent studies suggest that
adding metadata like URL information as context (i.e., auxiliary inputs not
used in the loss calculation) can improve training efficiency and downstream
performance, they offer limited understanding of which types of metadata are
truly effective and under what conditions. In this work, we conduct a
systematic evaluation and find that not all metadata types contribute equally.
Only URL context speeds up training, whereas quality scores and topic/format
domain information offer no clear benefit. Furthermore, the improved downstream
performances of URL conditioning emerge only when longer prompts are used at
inference time. In addition, we demonstrate that context-aware pretraining
enables more controllable generation than context-free pretraining, in a
classifier-free guidance fashion. Although topic and format metadata do not
accelerate training, they are effective for steering outputs, offering
human-interpretable control over generation.

</details>


### [87] [EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions](https://arxiv.org/abs/2505.16576)
*Spencer Hong,Meng Luo,Xinyi Wan*

Key words: 声明验证, 多智能体框架, 事实核查, EMULATE, 人类行为模拟

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种名为EMULATE的新型多智能体框架，用于模拟人类行为进行声明验证，效果优于现有方法。

Motivation: 现有的事实核查系统通过检索证据并利用大语言模型分类，偏离了人类行为方式，需改进以更贴近人类操作。

Method: 通过多智能体框架，每个智能体执行特定子任务（如搜索结果排名、网页内容评估），模拟人类的逐步验证过程。

Result: 在多个基准测试中表现优于现有方法，验证了多智能体框架的有效性。

Conclusion: EMULATE框架通过模拟人类行为显著提升了声明验证的准确性和效率。

Abstract: Determining the veracity of atomic claims is an imperative component of many
recently proposed fact-checking systems. Many approaches tackle this problem by
first retrieving evidence by querying a search engine and then performing
classification by providing the evidence set and atomic claim to a large
language model, but this process deviates from what a human would do in order
to perform the task. Recent work attempted to address this issue by proposing
iterative evidence retrieval, allowing for evidence to be collected several
times and only when necessary. Continuing along this line of research, we
propose a novel claim verification system, called EMULATE, which is designed to
better emulate human actions through the use of a multi-agent framework where
each agent performs a small part of the larger task, such as ranking search
results according to predefined criteria or evaluating webpage content.
Extensive experiments on several benchmarks show clear improvements over prior
work, demonstrating the efficacy of our new multi-agent framework.

</details>


### [88] [O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering](https://arxiv.org/abs/2505.16582)
*Jianbiao Mei,Tao Hu,Daocheng Fu,Licheng Wen,Xuemeng Yang,Rong Wu,Pinlong Cai,Xing Gao,Yu Yang,Chengjun Xie,Botian Shi,Yong Liu,Yu Qiao*

Key words: 大型语言模型、开放性问题、强化学习、搜索代理、动态知识获取

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出O$^2$-Searcher，一种结合强化学习的搜索代理，有效解决开放域中的开放和封闭问题，并通过新基准O$^2$-QA验证其优于现有LLM代理的性能。

Motivation: 大型语言模型（LLMs）的静态参数知识限制了其在需要最新开放域信息任务中的表现，尤其是开放性问题（无标准答案或多答案）当前研究不足。

Method: O$^2$-Searcher利用强化学习设计搜索代理，通过本地模拟搜索环境动态获取知识，分离外部知识与模型推理，并使用统一训练机制和奖励函数适配不同问题类型。

Result: 实验显示，O$^2$-Searcher在开放性问题基准O$^2$-QA上显著优于主流LLM代理（仅用3B模型），同时在封闭问题上媲美更大模型。

Conclusion: 该研究为LLM处理开放性问题提供了有效方案，并通过新基准推进了相关评估。

Abstract: Large Language Models (LLMs), despite their advancements, are fundamentally
limited by their static parametric knowledge, hindering performance on tasks
requiring open-domain up-to-date information. While enabling LLMs to interact
with external knowledge environments is a promising solution, current efforts
primarily address closed-end problems. Open-ended questions, which
characterized by lacking a standard answer or providing non-unique and diverse
answers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a
novel search agent leveraging reinforcement learning to effectively tackle both
open-ended and closed-ended questions in the open domain. O$^2$-Searcher
leverages an efficient, locally simulated search environment for dynamic
knowledge acquisition, effectively decoupling the external world knowledge from
model's sophisticated reasoning processes. It employs a unified training
mechanism with meticulously designed reward functions, enabling the agent to
identify problem types and adapt different answer generation strategies.
Furthermore, to evaluate performance on complex open-ended tasks, we construct
O$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain
open-ended questions with associated web page caches. Extensive experiments
show that O$^2$-Searcher, using only a 3B model, significantly surpasses
leading LLM agents on O$^2$-QA. It also achieves SOTA results on various
closed-ended QA benchmarks against similarly-sized models, while performing on
par with much larger ones.

</details>


### [89] [Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering](https://arxiv.org/abs/2505.16591)
*Bowen Jiang,Runchuan Zhu,Jiang Wu,Zinco Jiang,Yifan He,Junyuan Gao,Jia Yu,Rui Min,Yinfan Wang,Haote Yang,Songyang Zhang,Dahua Lin,Lijun Wu,Conghui He*

Key words: Large Language Models, multilingual evaluation, factual ability, benchmark, self-awareness

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: KoLasSimpleQA是一个评估大型语言模型（LLM）多语言事实能力的首个基准，包含9种语言并设计了双领域问题，结果揭示了不同模型在性能和鲁棒性上的显著差异。

Motivation: 旨在填补LLM多语言能力评估的空白，通过设计覆盖广泛语言和领域的基准，帮助识别模型的能力边界并提供优化指导。

Method: 构建了具有单知识点覆盖、绝对客观性、唯一答案和时间稳定性的问题集，采用LLM-as-judge范式评估模型的事实记忆和自知能力。

Result: 主流LLM在通用领域和语言特定领域的表现存在显著差异，尤其在性能指标、排序、校准和鲁棒性方面。

Conclusion: KoLasSimpleQA为多语言环境下LLM的能力评估和优化提供了重要工具，未来将公开数据集以推动研究。

Abstract: We introduce KoLasSimpleQA, the first benchmark evaluating the multilingual
factual ability of Large Language Models (LLMs). Inspired by existing research,
we created the question set with features such as single knowledge point
coverage, absolute objectivity, unique answers, and temporal stability. These
questions enable efficient evaluation using the LLM-as-judge paradigm, testing
both the LLMs' factual memory and self-awareness ("know what they don't know").
KoLasSimpleQA expands existing research in two key dimensions: (1) Breadth
(Multilingual Coverage): It includes 9 languages, supporting global
applicability evaluation. (2) Depth (Dual Domain Design): It covers both the
general domain (global facts) and the language-specific domain (such as
history, culture, and regional traditions) for a comprehensive assessment of
multilingual capabilities. We evaluated mainstream LLMs, including traditional
LLM and emerging Large Reasoning Models. Results show significant performance
differences between the two domains, particularly in performance metrics,
ranking, calibration, and robustness. This highlights the need for targeted
evaluation and optimization in multilingual contexts. We hope KoLasSimpleQA
will help the research community better identify LLM capability boundaries in
multilingual contexts and provide guidance for model optimization. We will
release KoLasSimpleQA at https://github.com/opendatalab/KoLasSimpleQA .

</details>


### [90] [What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse](https://arxiv.org/abs/2505.16592)
*Shijia Zhou,Siyao Peng,Simon Luebke,Jörg Haßler,Mario Haim,Saif M. Mohammad,Barbara Plank*

Key words: 媒体框架, 立场检测, 气候变化, 网络表情包, CLIMATEMEMES

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了媒体框架和立场在气候变化网络表情包中的互动关系，创建了第一个标注立场和媒体框架的数据集CLIMATEMEMES，并提出了两个任务：立场检测和媒体框架检测。评估了多种模型的表现，发现视觉语言模型在立场检测上表现良好，但在框架检测上不如语言模型。

Motivation: 探讨媒体框架和立场之间的互动关系，尤其是在气候变化网络表情包中的表现。

Method: 跨学科方法，创建并标注CLIMATEMEMES数据集，提出立场检测和媒体框架检测任务，评估LLaVA-NeXT和Molmo等模型在不同设置下的表现。

Result: VLMs在立场检测上表现良好，但在框架检测上不如LLMs；人类标注的标题能持续提升性能。

Conclusion: 视觉语言模型在处理气候变化表情包的立场检测上有效，但在框架检测上存在局限性。

Abstract: Media framing refers to the emphasis on specific aspects of perceived reality
to shape how an issue is defined and understood. Its primary purpose is to
shape public perceptions often in alignment with the authors' opinions and
stances. However, the interaction between stance and media frame remains
largely unexplored. In this work, we apply an interdisciplinary approach to
conceptualize and computationally explore this interaction with internet memes
on climate change. We curate CLIMATEMEMES, the first dataset of climate-change
memes annotated with both stance and media frames, inspired by research in
communication science. CLIMATEMEMES includes 1,184 memes sourced from 47
subreddits, enabling analysis of frame prominence over time and communities,
and sheds light on the framing preferences of different stance holders. We
propose two meme understanding tasks: stance detection and media frame
detection. We evaluate LLaVA-NeXT and Molmo in various setups, and report the
corresponding results on their LLM backbone. Human captions consistently
enhance performance. Synthetic captions and human-corrected OCR also help
occasionally. Our findings highlight that VLMs perform well on stance, but
struggle on frames, where LLMs outperform VLMs. Finally, we analyze VLMs'
limitations in handling nuanced frames and stance expressions on climate change
internet memes.

</details>


### [91] [From Generic Empathy to Personalized Emotional Support: A Self-Evolution Framework for User Preference Alignment](https://arxiv.org/abs/2505.16610)
*Jing Ye,Lu Xiang,Yaping Zhang,Chengqing Zong*

Key words: 情感支持, 大型语言模型, 个性化, 自我改进

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种自我进化框架，通过迭代优化帮助LLM提供更个性化的情感支持。

Motivation: LLM在情感支持中常提供通用回应，无法满足用户个性化需求。

Method: 框架分两阶段：情感支持经验获取和个性化情感支持的自我改进，通过自反思和优化生成个性化回应。

Result: 实验表明方法显著提升模型性能，减少无用回应及用户偏好与输出的不一致性。

Conclusion: 自我进化框架有效改善LLM情感支持的质量。

Abstract: Effective emotional support hinges on understanding users' emotions and needs
to provide meaningful comfort during multi-turn interactions. Large Language
Models (LLMs) show great potential for expressing empathy; however, they often
deliver generic and one-size-fits-all responses that fail to address users'
specific needs. To tackle this issue, we propose a self-evolution framework
designed to help LLMs improve their responses to better align with users'
implicit preferences concerning user profiles (personalities), emotional
states, and specific situations. Our framework consists of two distinct phases:
\textit{(1)} \textit{Emotional Support Experience Acquisition}, where LLMs are
fine-tuned on limited emotional support conversation data to provide basic
support, and \textit{(2)} \textit{Self-Improvement for Personalized Emotional
Support}, where LLMs leverage self-reflection and self-refinement to generate
personalized responses. Through iterative direct preference optimization
between the pre- and post-refined responses, our model generates responses that
reflect a better understanding of the user's implicit preferences. Extensive
experiments and evaluations demonstrate that our method significantly enhances
the model's performance in emotional support, reducing unhelpful responses and
minimizing discrepancies between user preferences and model outputs.

</details>


### [92] [Steering Large Language Models for Machine Translation Personalization](https://arxiv.org/abs/2505.16612)
*Daniel Scalena,Gabriele Sarti,Arianna Bisazza,Elisabetta Fersini,Malvina Nissim*

Key words: 机器翻译、大型语言模型、个性化风格、稀疏自编码器、低资源场景

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了在低资源环境下个性化LLM生成翻译的策略，提出了一种利用稀疏自编码器提取潜在概念的对比框架，证明其在保持翻译质量的同时实现了强个性化。

Motivation: 大型语言模型（LLM）在翻译中难以处理风格要求不明确的场景，尤其在文学翻译领域，因此研究如何在低资源环境下实现个性化翻译成为关键目标。

Method: 通过提示策略和推理时干预引导模型生成个性化风格，并设计对比框架，利用稀疏自编码器提取潜在概念以识别关键个性化特征。

Result: 实验表明，该方法在保持翻译质量的同时实现了显著个性化，且多示例提示与引导方法对模型层的优化机制类似。

Conclusion: 研究表明通过潜在概念和推理干预可有效实现翻译个性化，为低资源场景下的风格化翻译提供了新思路。

Abstract: High-quality machine translation systems based on large language models
(LLMs) have simplified the production of personalized translations reflecting
specific stylistic constraints. However, these systems still struggle in
settings where stylistic requirements are less explicit and might be harder to
convey via prompting. We explore various strategies for personalizing
LLM-generated translations in low-resource settings, focusing on the
challenging literary translation domain. We explore prompting strategies and
inference-time interventions for steering model generations towards a
personalized style, and propose a contrastive framework exploiting latent
concepts extracted from sparse autoencoders to identify salient personalization
properties. Our results show that steering achieves strong personalization
while preserving translation quality. We further examine the impact of steering
on LLM representations, finding model layers with a relevant impact for
personalization are impacted similarly by multi-shot prompting and our steering
method, suggesting similar mechanism at play.

</details>


### [93] [SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation](https://arxiv.org/abs/2505.16637)
*Wenjie Yang,Mao Zheng,Mingyang Song,Zheng Li*

Key words: 机器翻译, 大语言模型, 自我奖励增强学习, SSR, 无监督学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种基于自我奖励增强学习（SSR）的机器翻译框架，无需外部监督，仅依赖自我评判奖励。在多个基准测试中，其模型表现优于现有方法，甚至超越闭源模型。

Motivation: 现有机器翻译专用大模型依赖昂贵的外部监督信号（如人工参考数据或奖励模型），难以扩展。研究旨在提出一种无需外部监督、完全在线的自我奖励框架。

Method: 采用Simple Self-Rewarding（SSR）增强学习框架，仅依赖自我评判奖励，并基于Qwen-2.5-7B模型。进一步通过COMET增强监督信号。

Result: SSR-Zero-7B在英汉互译任务中超越现有模型，增强版SSR-X-Zero-7B达到最先进性能，甚至优于闭源模型如GPT-4o和Gemini 1.5 Pro。

Conclusion: 自我奖励机制在机器翻译中有效性显著，与外部奖励模型结合时具有补充优势。研究为自改进增强学习方法的潜力提供了重要见解。

Abstract: Large language models (LLMs) have recently demonstrated remarkable
capabilities in machine translation (MT). However, most advanced MT-specific
LLMs heavily rely on external supervision signals during training, such as
human-annotated reference data or trained reward models (RMs), which are often
expensive to obtain and challenging to scale. To overcome this limitation, we
propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for
MT that is reference-free, fully online, and relies solely on self-judging
rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as
the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,
e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like
Qwen2.5-32B-Instruct in English $\leftrightarrow$ Chinese translation tasks
from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR
with external supervision from COMET, our strongest model, SSR-X-Zero-7B,
achieves state-of-the-art performance in English $\leftrightarrow$ Chinese
translation, surpassing all existing open-source models under 72B parameters
and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.
Our analysis highlights the effectiveness of the self-rewarding mechanism
compared to the external LLM-as-a-judge approach in MT and demonstrates its
complementary benefits when combined with trained RMs. Our findings provide
valuable insight into the potential of self-improving RL methods. We have
publicly released our code, data and models.

</details>


### [94] [Collaboration among Multiple Large Language Models for Medical Question Answering](https://arxiv.org/abs/2505.16648)
*Kexin Shang,Chia-Hsuan Chang,Christopher C. Yang*

Key words: 大型语言模型、多LLM协作、医学任务、多选题、置信度

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种多LLM协作框架，针对医学多选题数据集，提升LLM的推理能力并减少分歧。

Motivation: 利用多个LLM的协同效应来提升医学任务的解决能力。

Method: 提出多LLM协作框架，基于医学多选题数据集进行实验，分析3个预训练LLM的表现。

Result: 框架提升了所有LLM的推理能力并减少了分歧，同时发现LLM的置信度与预测准确性相关。

Conclusion: 多LLM协作框架在医学任务中表现出潜力，尤其是通过协同效应提升性能。

Abstract: Empowered by vast internal knowledge reservoir, the new generation of large
language models (LLMs) demonstrate untapped potential to tackle medical tasks.
However, there is insufficient effort made towards summoning up a synergic
effect from multiple LLMs' expertise and background. In this study, we propose
a multi-LLM collaboration framework tailored on a medical multiple-choice
questions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,
our framework is proved to boost all LLMs reasoning ability as well as
alleviate their divergence among questions. We also measure an LLM's confidence
when it confronts with adversary opinions from other LLMs and observe a
concurrence between LLM's confidence and prediction accuracy.

</details>


### [95] [Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu](https://arxiv.org/abs/2505.16660)
*Liu Chang,Wang Dongbo,Liu liu,Zhao Zhixiao*

Key words: 中国古代数学, 推理模型, 古文处理, 文化知识, 跨语言评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究通过构建Guji_MATH基准评估中国古代数学经典文本，评估主流推理模型在古代汉语独特语言限制下的数学问题解决能力，结果表明模型可以部分理解并解决古代数学问题，但在现代数学任务上表现较差，需提升模型的古文理解和文化知识。

Motivation: 解决中国古代数学经典智能处理的挑战，为挖掘古代文本中的数学知识和传播传统文化提供方法论支持，并为评估推理模型的跨语言和跨文化能力提供新视角。

Method: 通过机器辅助标注和人工验证从8部经典文本中提取538个数学问题，构建以‘问题-答案-解法’框架为核心的结构化数据集，设计闭卷（独立解题）和开卷（复现经典解法）两种评估模式，评估6种推理模型的性能。

Result: 推理模型可以部分理解和解决古代数学问题，但整体表现不及现代数学任务的基准，需优先提升模型的古文理解和文化知识。

Conclusion: 研究为挖掘古代数学知识和传播传统文化提供了方法支持，并为评估推理模型的跨语言和跨文化能力提供了新视角。

Abstract: This study addresses the challenges in intelligent processing of Chinese
ancient mathematical classics by constructing Guji_MATH, a benchmark for
evaluating classical texts based on Suanjing Shishu. It systematically assesses
the mathematical problem-solving capabilities of mainstream reasoning models
under the unique linguistic constraints of classical Chinese. Through
machine-assisted annotation and manual verification, 538 mathematical problems
were extracted from 8 canonical texts, forming a structured dataset centered on
the "Question-Answer-Solution" framework, supplemented by problem types and
difficulty levels. Dual evaluation modes--closed-book (autonomous
problem-solving) and open-book (reproducing classical solution methods)--were
designed to evaluate the performance of six reasoning models on ancient Chinese
mathematical problems. Results indicate that reasoning models can partially
comprehend and solve these problems, yet their overall performance remains
inferior to benchmarks on modern mathematical tasks. Enhancing models'
classical Chinese comprehension and cultural knowledge should be prioritized
for optimization. This study provides methodological support for mining
mathematical knowledge from ancient texts and disseminating traditional
culture, while offering new perspectives for evaluating cross-linguistic and
cross-cultural capabilities of reasoning models.

</details>


### [96] [A Japanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical NLP](https://arxiv.org/abs/2505.16661)
*Issey Sukeda,Takuro Fujii,Kosei Buma,Shunsuke Sasaki,Shinnosuke Ono*

Key words: 日本制药领域,语言模型,预训练,基准测试,跨语言术语

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一个针对日本制药领域的特定领域语言模型，通过持续预训练开发，并在三个新基准测试中进行了评估，结果显示其性能优于现有开源模型，并与商业模型竞争。

Motivation: 针对制药领域的语言处理需求，开发一个实用、安全且经济高效的日语特定领域语言模型，并提供可重复使用的评估资源。

Method: 通过持续预训练在20亿日语制药词汇和80亿英语生物医学词汇上开发模型，并引入三个新的基准测试（YakugakuQA、NayoseQA和SogoCheck）进行评估。

Result: 模型在术语密集和知识密集型任务上表现优于开源模型，与商业模型（如GPT-4o）竞争。SogoCheck任务显示跨句一致性推理仍具挑战性。

Conclusion: 工作证明了构建日语特定领域语言模型的可行性，并为未来制药和医疗NLP研究提供了评估资源。

Abstract: We present a Japanese domain-specific language model for the pharmaceutical
field, developed through continual pretraining on 2 billion Japanese
pharmaceutical tokens and 8 billion English biomedical tokens. To enable
rigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on
national pharmacist licensing exams; NayoseQA, which tests cross-lingual
synonym and terminology normalization; and SogoCheck, a novel task designed to
assess consistency reasoning between paired statements. We evaluate our model
against both open-source medical LLMs and commercial models, including GPT-4o.
Results show that our domain-specific model outperforms existing open models
and achieves competitive performance with commercial ones, particularly on
terminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o
performs poorly on SogoCheck, suggesting that cross-sentence consistency
reasoning remains an open challenge. Our benchmark suite offers a broader
diagnostic lens for pharmaceutical NLP, covering factual recall, lexical
variation, and logical consistency. This work demonstrates the feasibility of
building practical, secure, and cost-effective language models for Japanese
domain-specific applications, and provides reusable evaluation resources for
future research in pharmaceutical and healthcare NLP. Our model, codes, and
datasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval.

</details>


### [97] [Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence](https://arxiv.org/abs/2505.16694)
*Gouki Minegishi,Hiroki Furuta,Shohei Taniguchi,Yusuke Iwasawa,Yutaka Matsuo*

Key words: Transformer, in-context learning, meta-learning, induction heads, training dynamics

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了Transformer语言模型如何通过上下文元学习（ICL）来动态获取任务解决能力，而非仅复制上下文中的答案，并通过实验揭示了训练过程中多个阶段的电路变化。

Motivation: 研究动机是理解大型语言模型如何通过上下文元学习来动态适应任务，而非简单复制答案，这一能力在训练中的形成机制尚不明确。

Method: 该方法将先前的复制任务扩展到上下文元学习设置，通过分析模型电路在训练中的动态变化来探究能力获取过程。

Result: 研究发现，模型在获取元学习能力时会经历多个阶段，每个阶段会形成独特的电路，这与单一阶段的归纳头变化形成对比。

Conclusion: 结论表明这些电路的 emergence 提供了对Transformer上下文学习能力的更深层次理解。

Abstract: Transformer-based language models exhibit In-Context Learning (ICL), where
predictions are made adaptively based on context. While prior work links
induction heads to ICL through a sudden jump in accuracy, this can only account
for ICL when the answer is included within the context. However, an important
property of practical ICL in large language models is the ability to meta-learn
how to solve tasks from context, rather than just copying answers from context;
how such an ability is obtained during training is largely unexplored. In this
paper, we experimentally clarify how such meta-learning ability is acquired by
analyzing the dynamics of the model's circuit during training. Specifically, we
extend the copy task from previous research into an In-Context Meta Learning
setting, where models must infer a task from examples to answer queries.
Interestingly, in this setting, we find that there are multiple phases in the
process of acquiring such abilities, and that a unique circuit emerges in each
phase, contrasting with the single-phases change in induction heads. The
emergence of such circuits can be related to several phenomena known in large
language models, and our analysis lead to a deeper understanding of the source
of the transformer's ICL ability.

</details>


### [98] [Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs](https://arxiv.org/abs/2505.16703)
*Zeping Yu,Sophia Ananiadou*

Key words: 多模态大型语言模型, 灾难性遗忘, 参数融合框架, 神经元融合, 视觉能力保留

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种名为Locate-then-Merge的训练自由参数融合框架，通过定位重要参数并选择性合并来减轻多模态大型语言模型（MLLMs）在调整阶段对基础语言能力的灾难性遗忘，尤其适用于像Llama3这样的强大模型。

Motivation: 多模态大型语言模型在指令调整阶段往往会导致基础语言能力的灾难性遗忘，即使在强大的模型如Llama3中也是如此。为应对此问题，研究旨在提出一种方法，既保留新获得的视觉能力，又减轻语言能力的退化。

Method: 采用了Locate-then-Merge框架，首先定位重要参数，然后选择性合并。进一步引入了Neuron-Fusion策略，通过神经元级别的保留和调整，保留那些参数变化大的神经元（可能负责新获得的视觉能力），减弱变化小的神经元（可能编码通用语言技能）的影响。

Result: 在13个基准测试（涵盖语言和视觉任务）中，Neuron-Fusion始终优于现有的模型合并方法。进一步分析表明，该方法有效减少生成中的上下文幻觉。

Conclusion: 该研究的方法在保留视觉适应能力的同时，显著减轻了语言能力的退化，为多模态大型语言模型的调整提供了有效解决方案。

Abstract: Although multimodal large language models (MLLMs) have achieved impressive
performance, the multimodal instruction tuning stage often causes catastrophic
forgetting of the base LLM's language ability, even in strong models like
Llama3. To address this, we propose Locate-then-Merge, a training-free
parameter fusion framework that first locates important parameters and then
selectively merges them. We further introduce Neuron-Fusion, a neuron-level
strategy that preserves the influence of neurons with large parameter
shifts--neurons likely responsible for newly acquired visual
capabilities--while attenuating the influence of neurons with smaller changes
that likely encode general-purpose language skills. This design enables better
retention of visual adaptation while mitigating language degradation.
Experiments on 13 benchmarks across both language and visual tasks show that
Neuron-Fusion consistently outperforms existing model merging methods. Further
analysis reveals that our method effectively reduces context hallucination in
generation.

</details>


### [99] [Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification](https://arxiv.org/abs/2505.16722)
*Himanshu Beniwal,Youngwoo Kim,Maarten Sap,Soham Dan,Thomas Hartvigsen*

Key words: 跨语言解毒，大型语言模型，毒性降低，模型性能，知识保留

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了跨语言解毒的有效性，通过504种设置评估毒性降低，并探讨解毒对模型性能的影响。

Motivation: 由于大型语言模型（LLM）在全球应用中越来越普遍，确保其在多样语环境中无毒是一个关键挑战。

Method: 通过504种设置评估跨语言解毒的有效性，同时分析解毒对非毒性任务性能的影响。

Result: 研究揭示了安全性与知识保留之间的权衡关系。

Conclusion: 跨语言解毒在有限数据条件下有效，但需要在安全性和模型性能之间找到平衡。

Abstract: As large language models (LLMs) become increasingly prevalent in global
applications, ensuring that they are toxicity-free across diverse linguistic
contexts remains a critical challenge. We explore "Cross-lingual
Detoxification", a cross-lingual paradigm that mitigates toxicity, enabling
detoxification capabilities to transfer between high and low-resource languages
across different script families. We analyze cross-lingual detoxification's
effectiveness through 504 extensive settings to evaluate toxicity reduction in
cross-distribution settings with limited data and investigate how mitigation
impacts model performance on non-toxic tasks, revealing trade-offs between
safety and knowledge preservation. Our code and dataset are publicly available
at https://github.com/himanshubeniwal/Breaking-mBad.

</details>


### [100] [TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning](https://arxiv.org/abs/2505.16743)
*Florentin Beck,William Rudman,Carsten Eickhoff*

Key words: 大型语言模型,剪枝,稀疏性,TRIM,性能优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种名为TRIM的新型剪枝方法，通过针对性的行级迭代调整，优化稀疏分配，显著提升了大型语言模型的性能与稳定性。

Motivation: 大型语言模型（LLMs）由于规模庞大，计算和内存需求高，剪枝是高效部署的关键。现有的一刀切剪枝方法在高稀疏比下表现不佳。

Method: TRIM方法通过质量指标迭代调整每层输出维度（行）的稀疏比，减少输出间的质量差异，保留关键信息。

Result: 在多种LLM（如Qwen2.5、LLaMA-2、OPT）和稀疏比下，TRIM在困惑度和零样本任务中达到最先进水平。例如，在80%稀疏比下，困惑度显著降低。

Conclusion: 细粒度的行级稀疏适配是推动极端LLM压缩的关键。

Abstract: Large Language Models (LLMs) present significant computational and memory
challenges due to their extensive size, making pruning essential for their
efficient deployment. Existing one-shot pruning methods often apply uniform
sparsity constraints across layers or within each layer, resulting in
suboptimal performance, especially at high sparsity ratios. This work
introduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel
approach that applies varying sparsity ratios to individual output dimensions
(rows) within each layer. TRIM employs an iterative adjustment process guided
by quality metrics to optimize dimension-wise sparsity allocation, focusing on
reducing variance in quality retention across outputs to preserve critical
information. TRIM can be seamlessly integrated with existing layer-wise pruning
strategies. Our evaluations on perplexity and zero-shot tasks across diverse
LLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that
TRIM achieves new state-of-the-art results and enhances stability. For
instance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and
over 90% for OPT-13B compared to baseline methods. We conclude that
fine-grained, dimension-wise sparsity adaptation is crucial for pushing the
limits of extreme LLM compression. Code available at:
https://github.com/flobk/TRIM

</details>


### [101] [IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models](https://arxiv.org/abs/2505.16774)
*Yiming Gao,Bin Wang,Chengwei Wei,Shuo Sun,AiTi Aw*

Key words: 音频LLM, 指令跟随, 评估数据集, IFEval-Audio

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了IFEval-Audio数据集，用于评估音频大语言模型（LLM）的指令跟随能力，填补了音频LLM研究中的空白。

Motivation: 现有研究多关注文本和视觉语言模型的指令跟随能力，但音频LLM的指令跟随能力尚未充分探索。

Method: 通过构建IFEval-Audio数据集（包含280个音频-指令-答案三元组，覆盖六个维度）并评估现有音频LLM。

Result: 基准测试展示了当前音频LLM在遵循音频指令方面的表现。

Conclusion: IFEval-Audio为音频LLM的指令跟随研究提供了工具，推动了这一新兴领域的发展。

Abstract: Large language models (LLMs) have demonstrated strong instruction-following
capabilities in text-based tasks. However, this ability often deteriorates in
multimodal models after alignment with non-text modalities such as images or
audio. While several recent efforts have investigated instruction-following
performance in text and vision-language models, instruction-following in
audio-based large language models remains largely unexplored. To bridge this
gap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess
the ability to follow instructions in an audio LLM. IFEval-Audio contains 280
audio-instruction-answer triples across six diverse dimensions: Content,
Capitalization, Symbol, List Structure, Length, and Format. Each example pairs
an audio input with a text instruction, requiring the model to generate an
output that follows a specified structure. We benchmark state-of-the-art audio
LLMs on their ability to follow audio-involved instructions. The dataset is
released publicly to support future research in this emerging area.

</details>


### [102] [Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.16782)
*Xinghao Chen,Anhao Zhao,Heming Xia,Xuan Lu,Hanlin Wang,Yanjun Chen,Wei Zhang,Jian Wang,Wenjie Li,Xiaoyu Shen*

Key words: Large Language Models, Chain-of-Thought, Latent Reasoning, Taxonomy, Comparative Analysis

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文对潜在思路推理（Latent CoT）进行了全面的综述和分析，提出了基于四方面的统一分类法，并深入讨论了代表性方法的设计模式、优势和挑战。

Motivation: 传统思路推理（CoT）依赖显式自然语言步骤，效率低且难以应用于抽象推理。潜在CoT推理在潜在空间中进行推理，有望实现更丰富的认知表示和更灵活的推理。

Method: 提出统一分类法（令牌策略、内部机制、分析、应用），并对代表性方法进行深入讨论和比较分析。

Result: 系统总结了潜在CoT推理的研究进展，为LLM推理的新兴方向提供了结构化基础。

Conclusion: 潜在CoT推理是LLM推理的重要发展方向，但仍面临开放挑战。

Abstract: Large Language Models (LLMs) have achieved impressive performance on complex
reasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional
CoT relies on reasoning steps explicitly verbalized in natural language,
introducing inefficiencies and limiting its applicability to abstract
reasoning. To address this, there has been growing research interest in latent
CoT reasoning, where inference occurs within latent spaces. By decoupling
reasoning from language, latent reasoning promises richer cognitive
representations and more flexible, faster inference. Researchers have explored
various directions in this promising field, including training methodologies,
structural innovations, and internal reasoning mechanisms. This paper presents
a comprehensive overview and analysis of this reasoning paradigm. We begin by
proposing a unified taxonomy from four perspectives: token-wise strategies,
internal mechanisms, analysis, and applications. We then provide in-depth
discussions and comparative analyses of representative methods, highlighting
their design patterns, strengths, and open challenges. We aim to provide a
structured foundation for advancing this emerging direction in LLM reasoning.
The relevant papers will be regularly updated at
https://github.com/EIT-NLP/Awesome-Latent-CoT.

</details>


### [103] [Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability](https://arxiv.org/abs/2505.16789)
*Punya Syon Pandey,Samuel Simko,Kellin Pelrine,Zhijing Jin*

Key words: 大语言模型、对抗攻击、微调、数据集设计、模型对齐

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨了微调语言模型时数据特性导致的意外脆弱性（Accidental Misalignment），分析了语言特征、语义相似度和毒性等因素，并提出防御策略。

Motivation: 大语言模型的对抗攻击漏洞是主要关注点，微调可能因数据特性引入意外脆弱性，需深入研究。

Method: 识别微调数据的相关性因素（如语言特征、语义相似度、毒性），评估微调模型的对抗表现，并探索数据因素与攻击成功率的关系。

Result: 发现数据特性与模型脆弱性存在相关性，为防御策略提供了新视角，强调了数据集设计对模型对齐的重要性。

Conclusion: 微调数据的设计对模型安全性至关重要，需进一步研究以优化对抗防御。

Abstract: As large language models gain popularity, their vulnerability to adversarial
attacks remains a primary concern. While fine-tuning models on domain-specific
datasets is often employed to improve model performance, it can introduce
vulnerabilities within the underlying model. In this work, we investigate
Accidental Misalignment, unexpected vulnerabilities arising from
characteristics of fine-tuning data. We begin by identifying potential
correlation factors such as linguistic features, semantic similarity, and
toxicity within our experimental datasets. We then evaluate the adversarial
performance of these fine-tuned models and assess how dataset factors correlate
with attack success rates. Lastly, we explore potential causal links, offering
new insights into adversarial defense strategies and highlighting the crucial
role of dataset design in preserving model alignment. Our code is available at
https://github.com/psyonp/accidental_misalignment.

</details>


### [104] [Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource Canonical Morpheme Segmentation](https://arxiv.org/abs/2505.16800)
*Changbing Yang,Garrett Nicolai*

Key words: Transformer, 词素分割, 多任务学习, 合成数据, 低资源语言

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种基于Transformer的词素分割系统，通过多任务学习和LLM生成的合成数据增强低资源训练信号，显著提升了词素分割的准确性。

Motivation: 解决低资源语言中词素分割数据稀缺的问题，通过多任务学习和合成数据提升模型性能。

Method: 采用Transformer框架，结合多任务学习和LLM生成的合成数据，同时预测词素分割和注释。

Result: 在SIGMORPHON 2023数据集上，显著提升了词素分割的准确性和F1分数。

Conclusion: 结合多任务学习和合成数据是一种有效的低资源词素分割方法。

Abstract: We introduce a transformer-based morpheme segmentation system that augments a
low-resource training signal through multitask learning and LLM-generated
synthetic data. Our framework jointly predicts morphological segments and
glosses from orthographic input, leveraging shared linguistic representations
obtained through a common documentary process to enhance model generalization.
To further address data scarcity, we integrate synthetic training data
generated by large language models (LLMs) using in-context learning.
Experimental results on the SIGMORPHON 2023 dataset show that our approach
significantly improves word-level segmentation accuracy and morpheme-level
F1-score across multiple low-resource languages.

</details>


### [105] [Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement](https://arxiv.org/abs/2505.16806)
*Kexin Zhang,Junlan Chen,Daifeng Li,Yuxuan Zhang,Yangyang Feng,Bowen Deng,Weixu Chen*

Key words: 大型语言模型, 知识密集型多步推理, 双向证据自对齐, 双重门控推理增强

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种名为ESA-DGR的统一框架，包含双向证据自对齐（TW-ESA）和双重门控推理增强（DGR）模块，显著提升了大型语言模型在知识密集型多步推理任务中的表现。

Motivation: 大型语言模型（LLMs）在处理知识密集型多步推理任务时，面临证据提取与逻辑推理的挑战。现有方法常提取语义相关但逻辑无关的证据，导致推理错误。

Method: 1. TW-ESA模块通过严格推理与LLM推理的双向对齐，增强证据因果逻辑理解；2. DGR模块逐步融合LLM知识，专注于证据中的因果元素。

Result: 在三个数据集上，ESA-DGR显著优于现有方法，EM和F1分数平均提升4%和5%。

Conclusion: ESA-DGR框架有效解决了LLMs在复杂推理任务中的挑战，提升了准确性和鲁棒性。

Abstract: Large language models (LLMs) encounter difficulties in knowledge-intensive
multi-step reasoning (KIMSR) tasks. One challenge is how to effectively extract
and represent rationale evidence. The current methods often extract
semantically relevant but logically irrelevant evidence, resulting in flawed
reasoning and inaccurate responses. We propose a two-way evidence
self-alignment (TW-ESA) module, which utilizes the mutual alignment between
strict reasoning and LLM reasoning to enhance its understanding of the causal
logic of evidence, thereby addressing the first challenge. Another challenge is
how to utilize the rationale evidence and LLM's intrinsic knowledge for
accurate reasoning when the evidence contains uncertainty. We propose a
dual-gated reasoning enhancement (DGR) module to gradually fuse useful
knowledge of LLM within strict reasoning, which can enable the model to perform
accurate reasoning by focusing on causal elements in the evidence and exhibit
greater robustness. The two modules are collaboratively trained in a unified
framework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR
datasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based
fine-tuning methods, with remarkable average improvements of 4% in exact match
(EM) and 5% in F1 score. The implementation code is available at
https://anonymous.4open.science/r/ESA-DGR-2BF8.

</details>


### [106] [Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?](https://arxiv.org/abs/2505.16814)
*Gaurav Kamath,Sowmya Vajjala*

Key words: 命名实体识别, 低资源语言, 数据增强, 合成数据, 多语言

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 摘要探讨了合成数据在多语言低资源命名实体识别（NER）中的作用，测试了11种语言，发现合成数据对低资源语言NER有潜力，但效果因语言而异。

Motivation: 研究低资源语言的NER系统，因为这类语言标注数据有限，而数据增强是解决这一问题的常见方法。

Method: 通过合成数据增强低资源语言的标注数据，并在11种不同语系的语言中进行实验。

Result: 实验结果表明，合成数据对低资源语言NER有效，但不同语言之间的效果差异显著。

Conclusion: 合成数据可以作为低资源语言NER的一种潜在解决方案，但需要根据语言特性调整方法。

Abstract: Named Entity Recognition(NER) for low-resource languages aims to produce
robust systems for languages where there is limited labeled training data
available, and has been an area of increasing interest within NLP. Data
augmentation for increasing the amount of low-resource labeled data is a common
practice. In this paper, we explore the role of synthetic data in the context
of multilingual, low-resource NER, considering 11 languages from diverse
language families. Our results suggest that synthetic data does in fact hold
promise for low-resource language NER, though we see significant variation
between languages.

</details>


### [107] [Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs](https://arxiv.org/abs/2505.16831)
*Xiaoyu Xu,Xiang Yue,Yang Liu,Qingqing Ye,Haibo Hu,Minxin Du*

Key words: 大语言模型、遗忘、可逆性、表示分析、评估框架

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 当前大语言模型中的“遗忘”方法主要依赖词级评估指标，但这些指标可能会导致误导。论文提出一种表示级评估框架，揭示了遗忘的“可逆性”与“不可逆性”，并发现现有评估方法存在根本性缺陷。

Motivation: 当前大语言模型的遗忘评估过于依赖词级指标（如准确率、困惑度），而这些指标无法真正反映模型是否彻底遗忘数据。论文旨在揭示这种评估的误导性，并提出更可靠的诊断方法。

Method: 论文提出了一个表示级评估框架，包括基于PCA的相似性和偏移、中心核对齐以及Fisher信息。该框架用于分析六种遗忘方法在三种领域（文本、代码、数学）和两种开源大语言模型中的表示变化。

Result: 研究发现，遗忘现象分为“可逆”和“不可逆”两类。可逆时，模型仅表面遗忘但保留潜在特征；不可逆时，表示层遭到深度破坏。理论分析表明，浅层权重扰动可能导致误导性的遗忘信号。

Conclusion: 当前遗忘评估方法存在根本性缺陷，论文提出的表示级框架为可信赖的遗忘评估提供了新基础。研究还发现任务类型和超参数调节可逆性。

Abstract: Unlearning in large language models (LLMs) is intended to remove the
influence of specific data, yet current evaluations rely heavily on token-level
metrics such as accuracy and perplexity. We show that these metrics can be
misleading: models often appear to forget, but their original behavior can be
rapidly restored with minimal fine-tuning, revealing that unlearning may
obscure information rather than erase it. To diagnose this phenomenon, we
introduce a representation-level evaluation framework using PCA-based
similarity and shift, centered kernel alignment, and Fisher information.
Applying this toolkit across six unlearning methods, three domains (text, code,
math), and two open-source LLMs, we uncover a critical distinction between
reversible and irreversible forgetting. In reversible cases, models suffer
token-level collapse yet retain latent features; in irreversible cases, deeper
representational damage occurs. We further provide a theoretical account
linking shallow weight perturbations near output layers to misleading
unlearning signals, and show that reversibility is modulated by task type and
hyperparameters. Our findings reveal a fundamental gap in current evaluation
practices and establish a new diagnostic foundation for trustworthy unlearning
in LLMs. We provide a unified toolkit for analyzing LLM representation changes
under unlearning and relearning:
https://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.

</details>


### [108] [SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis](https://arxiv.org/abs/2505.16834)
*Shuang Sun,Huatong Song,Yuhao Wang,Ruiyang Ren,Jinhao Jiang,Junjie Zhang,Fei Bai,Jia Deng,Wayne Xin Zhao,Zheng Liu,Lei Fang,Zhongyuan Wang,Ji-Rong Wen*

Key words: 检索增强生成,数据工程,有监督微调,深度搜索,分布式不匹配

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种轻量级框架SimpleDeepSearcher，通过数据工程而非复杂训练范式解决检索增强生成（RAG）系统的数据稀缺和分布不匹配问题。

Motivation: 现有检索增强生成系统在高质量训练轨迹、分布不匹配和计算成本方面存在局限性，亟需一种高效解决方案。

Method: 通过模拟真实用户交互并采用多标准数据筛选策略，合成高质量训练数据，进行有监督微调（SFT）。

Result: 在五个基准测试中，仅用871个筛选样本的SFT表现优于基于强化学习的基线方法。

Conclusion: SimpleDeepSearcher通过系统解决数据稀缺问题，为高效深度搜索系统提供了实用路径。

Abstract: Retrieval-augmented generation (RAG) systems have advanced large language
models (LLMs) in complex deep search scenarios requiring multi-step reasoning
and iterative information retrieval. However, existing approaches face critical
limitations that lack high-quality training trajectories or suffer from the
distributional mismatches in simulated environments and prohibitive
computational costs for real-world deployment. This paper introduces
SimpleDeepSearcher, a lightweight yet effective framework that bridges this gap
through strategic data engineering rather than complex training paradigms. Our
approach synthesizes high-quality training data by simulating realistic user
interactions in live web search environments, coupled with a multi-criteria
curation strategy that optimizes the diversity and quality of input and output
side. Experiments on five benchmarks across diverse domains demonstrate that
SFT on only 871 curated samples yields significant improvements over RL-based
baselines. Our work establishes SFT as a viable pathway by systematically
addressing the data-scarce bottleneck, offering practical insights for
efficient deep search systems. Our code is available at
https://github.com/RUCAIBox/SimpleDeepSearcher.

</details>


### [109] [R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search](https://arxiv.org/abs/2505.16838)
*Yibo Wang,Li Shen,Huanjin Yao,Tiansheng Huang,Rui Liu,Naiqiang Tan,Jiaxing Huang,Kai Zhang,Dacheng Tao*

Key words: Chain-of-Thought, Long-CoT, 压缩框架, 推理准确性, token 优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: R1-Compress 是一种两阶段分块压缩框架，解决了 Long-CoT 推理模型的高计算成本问题，同时保持局部信息和输出连贯性。

Motivation: 长链式推理（Long-CoT）增加了计算开销，已有的压缩方法会牺牲局部推理信号或输出连贯性，R1-Compress 旨在解决这些问题。

Method: 将 Long-CoT 分块，利用 LLM 进行块内压缩，并通过块间搜索机制选择简短且连贯的序列。

Result: 在 MATH500 上，R1-Compress 在减少约 20% 的 token 使用量的同时，准确率仅下降 0.6%（达到 92.4%）。

Conclusion: R1-Compress 显著降低计算成本，同时保持推理准确性，为长链式推理提供了高效解决方案。

Abstract: Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by
enabling step-by-step problem-solving, yet its extension to Long-CoT introduces
substantial computational overhead due to increased token length. Existing
compression approaches -- instance-level and token-level -- either sacrifice
essential local reasoning signals like reflection or yield incoherent outputs.
To address these limitations, we propose R1-Compress, a two-stage chunk-level
compression framework that preserves both local information and coherence. Our
method segments Long-CoT into manageable chunks, applies LLM-driven inner-chunk
compression, and employs an inter-chunk search mechanism to select the short
and coherent sequence. Experiments on Qwen2.5-Instruct models across MATH500,
AIME24, and GPQA-Diamond demonstrate that R1-Compress significantly reduces
token usage while maintaining comparable reasoning accuracy. On MATH500,
R1-Compress achieves an accuracy of 92.4%, with only a 0.6% drop compared to
the Long-CoT baseline, while reducing token usage by about 20%. Source code
will be available at https://github.com/w-yibo/R1-Compress

</details>


### [110] [Understanding and Analyzing Inappropriately Targeting Language in Online Discourse: A Comparative Annotation Study](https://arxiv.org/abs/2505.16847)
*Baran Barbarestani,Isa Maks,Piek Vossen*

Key words: 仇恨言语检测、内容审核、ChatGPT、上下文分析、标注框架

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种通过结合众包和专家标注与ChatGPT来检测在线对话中不当目标语言的方法，揭示了不同标注方式的优劣及上下文因素在识别仇恨言语中的重要性。

Motivation: 在线对话中不当目标语言的检测对保障网络安全和包容性至关重要，但现有方法在识别隐含歧视性语言和上下文依赖方面存在不足。

Method: 基于Reddit英文对话线程，结合专家、众包标注和ChatGPT标注的对比分析，构建了涵盖多样目标类别的标注框架。

Result: 研究发现上下文因素对仇恨言语识别至关重要，并揭示了新的目标类别（如社会信仰和身体形象），同时指出了ChatGPT在理解复杂语言时的局限性。

Conclusion: 研究为改进自动化内容审核策略提供了见解，强调了人工与自动标注结合的必要性。

Abstract: This paper introduces a method for detecting inappropriately targeting
language in online conversations by integrating crowd and expert annotations
with ChatGPT. We focus on English conversation threads from Reddit, examining
comments that target individuals or groups. Our approach involves a
comprehensive annotation framework that labels a diverse data set for various
target categories and specific target words within the conversational context.
We perform a comparative analysis of annotations from human experts, crowd
annotators, and ChatGPT, revealing strengths and limitations of each method in
recognizing both explicit hate speech and subtler discriminatory language. Our
findings highlight the significant role of contextual factors in identifying
hate speech and uncover new categories of targeting, such as social belief and
body image. We also address the challenges and subjective judgments involved in
annotation and the limitations of ChatGPT in grasping nuanced language. This
study provides insights for improving automated content moderation strategies
to enhance online safety and inclusivity.

</details>


### [111] [Nested Named Entity Recognition as Single-Pass Sequence Labeling](https://arxiv.org/abs/2505.16855)
*Alberto Muñoz-Ortiz,David Vilares,Caio COrro,Carlos Gómez-Rodríguez*

Key words: 嵌套命名实体识别、序列标注、线性化、预训练编码器

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 将嵌套命名实体识别（NNER）转化为序列标注任务，通过线性化处理降低复杂度，结合预训练编码器实现高效性能。

Motivation: 解决嵌套命名实体识别的结构化预测问题，将其简化为高效的序列标注任务。

Method: 利用线性化处理将嵌套结构转化为序列标注，结合预训练编码器进行标记分类。

Result: 性能优于低效系统，支持使用现成的序列标注库训练。

Conclusion: 该方法在保持高效的同时，实现了对嵌套实体的有效识别。

Abstract: We cast nested named entity recognition (NNER) as a sequence labeling task by
leveraging prior work that linearizes constituency structures, effectively
reducing the complexity of this structured prediction problem to
straightforward token classification. By combining these constituency
linearizations with pretrained encoders, our method captures nested entities
while performing exactly $n$ tagging actions. Our approach achieves competitive
performance compared to less efficient systems, and it can be trained using any
off-the-shelf sequence labeling library.

</details>


### [112] [Comparative analysis of subword tokenization approaches for Indian languages](https://arxiv.org/abs/2505.16868)
*Sudhansu Bala Das,Samujjal Choudhury,Tapas Kumar Mishra,Bidyut Kr. Patra*

Key words: 子词分词,印度语言,机器翻译,SentencePiece,BPE

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了不同子词分词方法对印度语言机器翻译的影响，发现不同模型下SentencePiece和BPE表现各异，且ILs到英语的翻译效果更优。

Motivation: 由于印度语言的复杂形态和丰富词汇，需要合适的子词分词方法来提升机器翻译的质量。

Method: 比较了SentencePiece、BPE和WordPiece等分词方法在统计、神经及多语言神经机器翻译模型中的表现，并使用BLEU等标准指标评估。

Result: SentencePiece在统计和神经模型中表现最佳，而BPE在多语言模型中更优；ILs到英语的翻译效果优于反向翻译。

Conclusion: 子词分词方法的选择对机器翻译效果有显著影响，需根据模型类型和语言对选择合适方法。

Abstract: Tokenization is the act of breaking down text into smaller parts, or tokens,
that are easier for machines to process. This is a key phase in machine
translation (MT) models. Subword tokenization enhances this process by breaking
down words into smaller subword units, which is especially beneficial in
languages with complicated morphology or a vast vocabulary. It is useful in
capturing the intricate structure of words in Indian languages (ILs), such as
prefixes, suffixes, and other morphological variations. These languages
frequently use agglutinative structures, in which words are formed by the
combination of multiple morphemes such as suffixes, prefixes, and stems. As a
result, a suitable tokenization strategy must be chosen to address these
scenarios. This paper examines how different subword tokenization techniques,
such as SentencePiece, Byte Pair Encoding (BPE), and WordPiece Tokenization,
affect ILs. The effectiveness of these subword tokenization techniques is
investigated in statistical, neural, and multilingual neural machine
translation models. All models are examined using standard evaluation metrics,
such as the Bilingual Evaluation Understudy (BLEU) score, TER, METEOR, CHRF,
RIBES, and COMET. Based on the results, it appears that for the majority of
language pairs for the Statistical and Neural MT models, the SentencePiece
tokenizer continuously performed better than other tokenizers in terms of BLEU
score. However, BPE tokenization outperformed other tokenization techniques in
the context of Multilingual Neural Machine Translation model. The results show
that, despite using the same tokenizer and dataset for each model, translations
from ILs to English surpassed translations from English to ILs.

</details>


### [113] [MPO: Multilingual Safety Alignment via Reward Gap Optimization](https://arxiv.org/abs/2505.16869)
*Weixiang Zhao,Yulin Hu,Yang Deng,Tongtong Wu,Wenxuan Zhang,Jiahe Guo,An Zhang,Yanyan Zhao,Bing Qin,Tat-Seng Chua,Ting Liu*

Key words: 多语言, 安全对齐, MPO, 奖励差距, 自然语言处理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了多语言奖励差距优化（MPO）方法，利用主导语言（英语）的安全对齐能力来提升多语言环境下的安全对齐效果，并验证了其有效性。

Motivation: 现有安全对齐方法（如RLHF和DPO）主要为单语言设计，难以处理多语言噪声数据，因此需要一种新方法来提升多语言场景下的安全对齐效果。

Method: MPO通过最小化主导语言与目标语言之间的奖励差距，直接利用主导语言的安全对齐能力来提升多语言对齐效果。

Result: 在LLaMA-3.1、Gemma-2和Qwen2.5上的实验表明，MPO能有效提升多语言安全对齐效果且不影响通用多语言能力。

Conclusion: MPO是一种高效的多语言安全对齐方法，能够利用主导语言的安全能力并保持其原有优势。

Abstract: Large language models (LLMs) have become increasingly central to AI
applications worldwide, necessitating robust multilingual safety alignment to
ensure secure deployment across diverse linguistic contexts. Existing
preference learning methods for safety alignment, such as RLHF and DPO, are
primarily monolingual and struggle with noisy multilingual data. To address
these limitations, we introduce Multilingual reward gaP Optimization (MPO), a
novel approach that leverages the well-aligned safety capabilities of the
dominant language (English) to improve safety alignment across multiple
languages. MPO directly minimizes the reward gap difference between the
dominant language and target languages, effectively transferring safety
capabilities while preserving the original strengths of the dominant language.
Extensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate
MPO's efficacy in multilingual safety alignment without degrading general
multilingual utility.

</details>


### [114] [CASTILLO: Characterizing Response Length Distributions of Large Language Models](https://arxiv.org/abs/2505.16881)
*Daniel F. Perez-Ramirez,Dejan Kostic,Magnus Boman*

Key words: LLM, 资源管理, 响应长度预测, 数据集, 开源模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: CASTILLO 是一个数据集，用于分析不同开源大语言模型（LLM）的响应长度分布，帮助优化资源分配和预测模型行为。

Motivation: 由于自动生成文本的长度具有随机性和可变性，高效管理LLM推理的计算资源具有挑战性。现有方法存在偏见或忽略模型和提示的变异性。

Method: 通过13种开源LLM和7种指令遵循语料库，生成10个独立完成样本，记录响应长度并发布统计数据和生成设置。

Result: 分析显示响应长度在模型间和模型内存在显著变异性，模型特定行为和部分文本退化现象。

Conclusion: CASTILLO 为预测模型开发提供了系统框架，支持生成语言模型与系统的交叉研究。

Abstract: Efficiently managing compute resources for Large Language Model (LLM)
inference remains challenging due to the inherently stochastic and variable
lengths of autoregressive text generation. Accurately estimating response
lengths in advance enables proactive resource allocation, yet existing
approaches either bias text generation towards certain lengths or rely on
assumptions that ignore model- and prompt-specific variability. We introduce
CASTILLO, a dataset characterizing response length distributions across 13
widely-used open-source LLMs evaluated on seven distinct instruction-following
corpora. For each $\langle$prompt, model$\rangle$ sample pair, we generate 10
independent completions using fixed decoding hyper-parameters, record the token
length of each response, and publish summary statistics (mean, std-dev,
percentiles), along with the shortest and longest completions, and the exact
generation settings. Our analysis reveals significant inter- and intra-model
variability in response lengths (even under identical generation settings), as
well as model-specific behaviors and occurrences of partial text degeneration
in only subsets of responses. CASTILLO enables the development of predictive
models for proactive scheduling and provides a systematic framework for
analyzing model-specific generation behaviors. We publicly release the dataset
and code to foster research at the intersection of generative language modeling
and systems.

</details>


### [115] [Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs](https://arxiv.org/abs/2505.16894)
*Zeyu Wei,Shuo Wang,Xiaohui Rong,Xuemin Liu,He Li*

Key words: 大型语言模型, 幻觉, 内部状态漂移, 注意力机制, 上下文注入

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文通过系统研究内部状态漂移与幻觉现象的关系，揭示了大型语言模型在增量上下文注入下的幻觉机制。

Motivation: 研究幻觉现象（看似合理但错误的输出）是大型语言模型可靠部署的关键障碍，旨在通过分析内部状态漂移揭示其成因。

Method: 使用TruthfulQA构建16轮“滴定”实验，注入相关但有缺陷或误导性内容，通过三视角检测器和多种度量方法（余弦、熵、JS和Spearman漂移）追踪幻觉。

Result: 发现幻觉频率和表征漂移的单调增长现象，以及相关上下文导致的语义同化错误与无关上下文导致的注意力重定向错误。

Conclusion: JS和Spearman漂移的收敛标志幻觉固化的阈值，为预测和缓解幻觉提供了理论基础。

Abstract: Hallucinations -- plausible yet erroneous outputs -- remain a critical
barrier to reliable deployment of large language models (LLMs). We present the
first systematic study linking hallucination incidence to internal-state drift
induced by incremental context injection. Using TruthfulQA, we construct two
16-round "titration" tracks per question: one appends relevant but partially
flawed snippets, the other injects deliberately misleading content. Across six
open-source LLMs, we track overt hallucination rates with a tri-perspective
detector and covert dynamics via cosine, entropy, JS and Spearman drifts of
hidden states and attention maps. Results reveal (1) monotonic growth of
hallucination frequency and representation drift that plateaus after 5--7
rounds; (2) relevant context drives deeper semantic assimilation, producing
high-confidence "self-consistent" hallucinations, whereas irrelevant context
induces topic-drift errors anchored by attention re-routing; and (3)
convergence of JS-Drift ($\sim0.69$) and Spearman-Drift ($\sim0$) marks an
"attention-locking" threshold beyond which hallucinations solidify and become
resistant to correction. Correlation analyses expose a seesaw between
assimilation capacity and attention diffusion, clarifying size-dependent error
modes. These findings supply empirical foundations for intrinsic hallucination
prediction and context-aware mitigation mechanisms.

</details>


### [116] [Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality](https://arxiv.org/abs/2505.16900)
*Jintian Shao,Hongyi Huang,Jiayi Wu,Beiwen Zhang,ZhiYu Wu,You Shan,MingKai Zheng*

Key words: 文本生成, 微调, 损失函数, 幂律衰减, 信息量

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种新的损失函数Power-Law Decay Loss (PDL)，通过调整词频权重优化文本生成微调过程，旨在提升生成文本的质量、多样性和信息量。

Motivation: 现有交叉熵损失函数对所有词平等处理，导致模型过度关注高频低信息量词，忽略低频高信息量词。基于信息论和语言学观察，PDL通过词频重加权解决这一问题。

Method: 设计PDL损失函数，基于词频按幂律衰减调整权重——降低高频词权重，提高低频词权重，从而引导模型更关注信息密集词的学习和生成。

Result: PDL在文本生成微调任务（如摘要生成、对话系统、风格迁移）中理论上能提升生成内容的质量、多样性和信息量。

Conclusion: PDL是一种有效的微调优化方法，通过词频敏感权重调整，显著改善生成文本的特异性和信息密度。

Abstract: During the finetuning stage of text generation tasks, standard cross-entropy
loss treats all tokens equally. This can lead models to overemphasize
high-frequency, low-information tokens, neglecting lower-frequency tokens
crucial for specificity and informativeness in generated content. This paper
introduces a novel loss function, Power-Law Decay Loss (PDL), specifically
designed to optimize the finetuning process for text generation. The core
motivation for PDL stems from observations in information theory and
linguistics: the informativeness of a token is often inversely proportional to
its frequency of occurrence. PDL re-weights the contribution of each token in
the standard cross-entropy loss based on its frequency in the training corpus,
following a power-law decay. Specifically, the weights for high-frequency
tokens are reduced, while low-frequency, information-dense tokens are assigned
higher weights. This mechanism guides the model during finetuning to focus more
on learning and generating tokens that convey specific and unique information,
thereby enhancing the quality, diversity, and informativeness of the generated
text. We theoretically elaborate on the motivation and construction of PDL and
discuss its potential applications and advantages across various text
generation finetuning tasks, such as abstractive summarization, dialogue
systems, and style transfer.

</details>


### [117] [UNCLE: Uncertainty Expressions in Long-Form Generation](https://arxiv.org/abs/2505.16922)
*Ruihan Yang,Caiqi Zhang,Zhisong Zhang,Xinting Huang,Dong Yu,Nigel Collier,Deqing Yang*

Key words: 大型语言模型、幻觉、不确定性表达、长文本生成、基准评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文针对大型语言模型（LLMs）在长文本生成中的幻觉问题，提出通过让模型明确表达不确定性来缓解问题。研究团队推出了首个结合长短文本问答的基准UNCLE，并设计新指标评估模型选择性表达不确定性的能力。实验发现当前模型在长文本中表现不佳，但训练方法能显著改进效果。

Motivation: LLMs在长文本生成中易产生幻觉，但现有研究缺乏对其不确定性表达能力的直接评估。因此，研究团队旨在填补这一空白，推动模型更可信地表达知识边界。

Method: 提出UNCLE基准，包含5个领域的4k长文本和20k短文本问答数据，并设计新评估指标。通过基于提示和训练的方法改进模型表现。

Result: 现有模型在长文本中无法适当表达不确定性，但训练方法效果显著优于提示方法。长短文本不确定性表达的差异为未来研究提供方向。

Conclusion: UNCLE为评估和改进LLMs的不确定性表达能力提供了系统工具，训练方法展现出潜力，但对长短文本的差异仍需进一步探索。

Abstract: Large Language Models (LLMs) are prone to hallucination, particularly in
long-form generations. A promising direction to mitigate hallucination is to
teach LLMs to express uncertainty explicitly when they lack sufficient
knowledge. However, existing work lacks direct and fair evaluation of LLMs'
ability to express uncertainty effectively in long-form generation. To address
this gap, we first introduce UNCLE, a benchmark designed to evaluate
uncertainty expression in both long- and short-form question answering (QA).
UNCLE spans five domains and comprises 4k long-form QA instances and over 20k
short-form QA pairs. Our dataset is the first to directly bridge short- and
long-form QA with paired questions and gold-standard answers. Along with the
benchmark, we propose a suite of new metrics to assess the models' capabilities
to selectively express uncertainty. Using UNCLE, we then demonstrate that
current models fail to convey uncertainty appropriately in long-form
generation. We further explore both prompt-based and training-based methods to
improve models' performance, with the training-based methods yielding greater
gains. Further analysis of alignment gaps between short- and long-form
uncertainty expression highlights promising directions for future research
using UNCLE.

</details>


### [118] [Latent Principle Discovery for Language Model Self-Improvement](https://arxiv.org/abs/2505.16927)
*Keshav Ramji,Tahira Naseem,Ramón Fernandez Astudillo*

Key words: 语言模型、自我改进、潜在属性、聚类、后验正则化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种自动化方法，通过自校正和聚类挖掘潜在行为属性，使语言模型自我改进，提升生成质量。小模型性能显著提升，同时生成的原则可解释且多样。

Motivation: 当前提升语言模型生成质量需要人工标注大量行为属性，费时费力。本文旨在自动化挖掘这些潜在属性，实现模型自我改进。

Method: 采用后验正则化的蒙特卡洛期望最大化方法，挖掘并压缩潜在行为属性，通过聚类生成可解释的原则集，并训练模型动态调用这些原则自我校正。

Result: 实验显示，小语言模型（7-8B参数）性能显著提升：AlpacaEval胜率+8-10%，MT-Bench平均+0.3，IFEval原则遵循胜率+19-23%。聚类后的原则集可解释且不影响模型表现。

Conclusion: 该方法证明了自动化原则挖掘在持续自我改进中的潜力，为模型后训练提供了高效可扩展的解决方案。

Abstract: When language model (LM) users aim to improve the quality of its generations,
it is crucial to specify concrete behavioral attributes that the model should
strive to reflect. However, curating such principles across many domains, even
non-exhaustively, requires a labor-intensive annotation process. To automate
this process, we propose eliciting these latent attributes guiding model
reasoning towards human-preferred responses by explicitly modeling them in a
self-correction setting. Our approach mines new principles from the LM itself
and compresses the discovered elements to an interpretable set via clustering.
Specifically, we employ an approximation of posterior-regularized Monte Carlo
Expectation-Maximization to both identify a condensed set of the most effective
latent principles and teach the LM to strategically invoke them in order to
intrinsically refine its responses. We demonstrate that bootstrapping our
algorithm over multiple iterations enables smaller language models (7-8B
parameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an
average of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on
IFEval. We also show that clustering the principles yields interpretable and
diverse model-generated constitutions while retaining model performance. The
gains our method achieves highlight the potential of automated,
principle-driven post-training recipes toward continual self-improvement.

</details>


### [119] [PIIvot: A Lightweight NLP Anonymization Framework for Question-Anchored Tutoring Dialogues](https://arxiv.org/abs/2505.16931)
*Matthew Zent,Digory Smith,Simon Woodhead*

Key words: PII匿名化, 数据共享, 教育对话数据, QATD-2k, PIIvot

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: PIIvot是一个轻量级框架，利用数据上下文知识简化PII检测问题，并发布了最大的开源教育对话数据集QATD-2k。

Motivation: PII匿名化是高风险任务，限制了开放科学数据共享的采用。现有方法在误差阈值和召回/精确度权衡上仍有局限。

Method: 提出了PIIvot框架，通过数据上下文知识简化PII检测。

Result: 证明PIIvot有效，并贡献了最大的开源教育对话数据集QATD-2k。

Conclusion: PIIvot简化了PII检测问题，推动了高质量教育对话数据的共享。

Abstract: Personally identifiable information (PII) anonymization is a high-stakes task
that poses a barrier to many open-science data sharing initiatives. While PII
identification has made large strides in recent years, in practice, error
thresholds and the recall/precision trade-off still limit the uptake of these
anonymization pipelines. We present PIIvot, a lighter-weight framework for PII
anonymization that leverages knowledge of the data context to simplify the PII
detection problem. To demonstrate its effectiveness, we also contribute
QATD-2k, the largest open-source real-world tutoring dataset of its kind, to
support the demand for quality educational dialogue data.

</details>


### [120] [In-Context Watermarks for Large Language Models](https://arxiv.org/abs/2505.16934)
*Yepeng Liu,Xuandong Zhao,Christopher Kruegel,Dawn Song,Yuheng Bu*

Key words: 大语言模型, 水印技术, 提示工程, 内容溯源

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种仅通过提示工程即可在文本中嵌入水印的方法（ICW），无需访问解码过程，验证了其可行性。

Motivation: 现有水印方法需访问解码过程，限制了实际应用。针对学术评审中AI生成文本的检测需求，提出了无需模型访问权的解决方案。

Method: 利用LLM的上下文学习和指令跟随能力，通过提示工程嵌入水印，并研究了四种策略及配套检测方法。

Result: 实验验证了ICW作为一种模型无关、实用的水印方法的可行性。

Conclusion: 随着LLM能力提升，ICW为内容溯源提供了可扩展且易于实现的途径。

Abstract: The growing use of large language models (LLMs) for sensitive applications
has highlighted the need for effective watermarking techniques to ensure the
provenance and accountability of AI-generated text. However, most existing
watermarking methods require access to the decoding process, limiting their
applicability in real-world settings. One illustrative example is the use of
LLMs by dishonest reviewers in the context of academic peer review, where
conference organizers have no access to the model used but still need to detect
AI-generated reviews. Motivated by this gap, we introduce In-Context
Watermarking (ICW), which embeds watermarks into generated text solely through
prompt engineering, leveraging LLMs' in-context learning and
instruction-following abilities. We investigate four ICW strategies at
different levels of granularity, each paired with a tailored detection method.
We further examine the Indirect Prompt Injection (IPI) setting as a specific
case study, in which watermarking is covertly triggered by modifying input
documents such as academic manuscripts. Our experiments validate the
feasibility of ICW as a model-agnostic, practical watermarking approach.
Moreover, our findings suggest that as LLMs become more capable, ICW offers a
promising direction for scalable and accessible content attribution.

</details>


### [121] [On Multilingual Encoder Language Model Compression for Low-Resource Languages](https://arxiv.org/abs/2505.16956)
*Daniil Gurgurov,Michal Gregor,Josef van Genabith,Simon Ostermann*

Key words: 知识蒸馏, 模型压缩, 低资源语言, 多语言模型, 结构化剪枝

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种结合知识蒸馏、结构化剪枝、截断和词汇修剪的方法，用于极度压缩多语言编码器模型，尤其适用于低资源语言。通过系统整合现有技术并极端优化，模型大小减少92%，性能仅下降2-10%。

Motivation: 针对低资源语言，现有大模型难以部署，需要高效压缩方法以保留语言特性知识并降低成本。

Method: 结合两步知识蒸馏、结构化剪枝、截断和词汇修剪，缩减层深度、前馈隐藏大小和中间层嵌入尺寸。

Result: 压缩率高达92%，四项下游任务性能仅下降2-10%，且性能损失与教师模型的语言数据量相关。

Conclusion: 该方法显著减小模型体积且性能降幅可控，为低资源语言模型压缩提供了实用方案。

Abstract: In this paper, we combine two-step knowledge distillation, structured
pruning, truncation, and vocabulary trimming for extremely compressing
multilingual encoder-only language models for low-resource languages. Our novel
approach systematically combines existing techniques and takes them to the
extreme, reducing layer depth, feed-forward hidden size, and intermediate layer
embedding size to create significantly smaller monolingual models while
retaining essential language-specific knowledge. We achieve compression rates
of up to 92% with only a marginal performance drop of 2-10% in four downstream
tasks, including sentiment analysis, topic classification, named entity
recognition, and part-of-speech tagging, across three low-resource languages.
Notably, the performance degradation correlates with the amount of
language-specific data in the teacher model, with larger datasets resulting in
smaller performance losses. Additionally, we conduct extensive ablation studies
to identify best practices for multilingual model compression using these
techniques.

</details>


### [122] [BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation](https://arxiv.org/abs/2505.16965)
*Fengyi Li,Kayhan Behdin,Natesh Pillai,Xiaofeng Wang,Zhipeng Wang,Ercan Yildiz*

Key words: 文本分割, 图模型, 无监督学习, 信念传播

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种基于图模型的无监督学习方法BP-Seg，用于高效文本分割。

Motivation: 研究语义句子的文本分割任务，为下游应用提供支持。

Method: 通过在图模型上执行信念传播，同时考虑局部连贯性和长距离语义相似性。

Result: 实验结果表明，该方法在长文档数据集上优于竞争方法。

Conclusion: BP-Seg是一种有效的文本分割方法，结合局部和全局语义信息。

Abstract: Text segmentation based on the semantic meaning of sentences is a fundamental
task with broad utility in many downstream applications. In this paper, we
propose a graphical model-based unsupervised learning approach, named BP-Seg
for efficient text segmentation. Our method not only considers local coherence,
capturing the intuition that adjacent sentences are often more related, but
also effectively groups sentences that are distant in the text yet semantically
similar. This is achieved through belief propagation on the carefully
constructed graphical models. Experimental results on both an illustrative
example and a dataset with long-form documents demonstrate that our method
performs favorably compared to competing approaches.

</details>


### [123] [From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition](https://arxiv.org/abs/2505.16972)
*Tianduo Wang,Lu Xu,Wei Lu,Shanbo Cheng*

Key words: 自动语音识别,语音反向翻译,文本到语音,多语言,合成数据

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为‘语音反向翻译’的可扩展方法，通过现成的文本到语音模型将大规模文本语料转换为合成语音，以提升多语言自动语音识别模型的性能。实验证明，仅需数十小时的真实语音即可合成出数百倍的高质量语音数据，显著降低转录错误率。

Motivation: 多语言自动语音识别（ASR）的覆盖范围仍受限于资源稀缺的语言，本研究旨在通过合成语音数据来解决这一问题。

Method: 利用现成的文本到语音（TTS）模型将大量文本语料转换为合成语音，并通过基于可懂度的评估框架控制数据质量。

Result: 生成超过50万小时的10种语言合成语音，继续预训练Whisper-large-v3模型后，平均转录错误率降低30%以上。

Conclusion: 语音反向翻译是一种可扩展且高效的方法，能显著提升多语言ASR系统的性能。

Abstract: Recent advances in Automatic Speech Recognition (ASR) have been largely
fueled by massive speech corpora. However, extending coverage to diverse
languages with limited resources remains a formidable challenge. This paper
introduces Speech Back-Translation, a scalable pipeline that improves
multilingual ASR models by converting large-scale text corpora into synthetic
speech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just
tens of hours of real transcribed speech can effectively train TTS models to
generate synthetic speech at hundreds of times the original volume while
maintaining high quality. To evaluate synthetic speech quality, we develop an
intelligibility-based assessment framework and establish clear thresholds for
when synthetic data benefits ASR training. Using Speech Back-Translation, we
generate more than 500,000 hours of synthetic speech in ten languages and
continue pre-training Whisper-large-v3, achieving average transcription error
reductions of over 30\%. These results highlight the scalability and
effectiveness of Speech Back-Translation for enhancing multilingual ASR
systems.

</details>


### [124] [VeriFastScore: Speeding up long-form factuality evaluation](https://arxiv.org/abs/2505.16973)
*Rishanth Rajendhran,Amir Zadeh,Matthew Sarte,Chuan Li,Mohit Iyyer*

Key words: VeriFastScore, factuality evaluation, synthetic data, fine-tuning, Llama3.1

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: VeriFastScore通过微调Llama3.1 8B模型，实现了对文本中可验证声明的同时提取和验证，显著提升了速度（6.6x）并保持了与VeriScore的高度相关性。

Motivation: 现有方法（如FactScore和VeriScore）虽有效但耗时，限制了其在大规模评估和训练中的实用性。

Method: 利用合成数据微调Llama3.1 8B模型，同时提取和验证所有可验证声明，基于Google搜索的证据。

Result: VeriFastScore在速度和相关性上表现优异：速度提升6.6倍，与VeriScore的相关系数在示例级（r=0.80）和系统级（r=0.94）均较高。

Conclusion: VeriFastScore是一种高效且可靠的替代方案，为未来的真实性研究提供了公开模型和数据集。

Abstract: Metrics like FactScore and VeriScore that evaluate long-form factuality
operate by decomposing an input response into atomic claims and then
individually verifying each claim. While effective and interpretable, these
methods incur numerous LLM calls and can take upwards of 100 seconds to
evaluate a single response, limiting their practicality in large-scale
evaluation and training scenarios. To address this, we propose VeriFastScore,
which leverages synthetic data to fine-tune Llama3.1 8B for simultaneously
extracting and verifying all verifiable claims within a given text based on
evidence from Google Search. We show that this task cannot be solved via
few-shot prompting with closed LLMs due to its complexity: the model receives
~4K tokens of evidence on average and needs to concurrently decompose claims,
judge their verifiability, and verify them against noisy evidence. However, our
fine-tuned VeriFastScore model demonstrates strong correlation with the
original VeriScore pipeline at both the example level (r=0.80) and system level
(r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence
retrieval) over VeriScore. To facilitate future factuality research, we
publicly release our VeriFastScore model and synthetic datasets.

</details>


### [125] [LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding](https://arxiv.org/abs/2505.16983)
*Junlong Tong,Jinlan Fu,Zixuan Lin,Yingqi Fan,Anhao Zhao,Hui Su,Xiaoyu Shen*

Key words: 大型语言模型,流式处理,位置编码,输入-注意力不匹配,组位置编码

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种基于批量架构的组位置编码范式，以解决大型语言模型在流式处理中的输入-注意力不匹配问题，同时表明输出-注意力和位置ID不匹配对性能影响较小。

Motivation: 现有方法将大型语言模型（LLMs）适配到流式处理时，通常依赖于昂贵的重新编码或有限的专用架构，本研究旨在解决批量导向LLMs在流式处理中的关键不匹配问题。

Method: 通过分析位置编码对LLMs在流式处理中的影响，提出了一种基于批量架构的组位置编码范式，无需修改模型架构即可增强流式与批量模式的一致性。

Result: 在跨语言和跨模态任务上的实验表明，该方法优于现有方法，且无需架构修改，同时在流式和批量模式下均表现出强泛化能力。

Conclusion: 研究表明，输入-注意力不匹配是流式处理中的主要问题，而组位置编码方法能有效提升性能并保持通用性，为流式LLMs提供了一种高效的解决方案。

Abstract: Large Language Models (LLMs) are primarily designed for batch processing.
Existing methods for adapting LLMs to streaming rely either on expensive
re-encoding or specialized architectures with limited scalability. This work
identifies three key mismatches in adapting batch-oriented LLMs to streaming:
(1) input-attention, (2) output-attention, and (3) position-ID mismatches.
While it is commonly assumed that the latter two mismatches require frequent
re-encoding, our analysis reveals that only the input-attention mismatch
significantly impacts performance, indicating re-encoding outputs is largely
unnecessary. To better understand this discrepancy with the common assumption,
we provide the first comprehensive analysis of the impact of position encoding
on LLMs in streaming, showing that preserving relative positions within source
and target contexts is more critical than maintaining absolute order. Motivated
by the above analysis, we introduce a group position encoding paradigm built on
batch architectures to enhance consistency between streaming and batch modes.
Extensive experiments on cross-lingual and cross-modal tasks demonstrate that
our method outperforms existing approaches. Our method requires no
architectural modifications, exhibits strong generalization in both streaming
and batch modes. The code is available at repository
https://github.com/EIT-NLP/StreamingLLM.

</details>


### [126] [T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning](https://arxiv.org/abs/2505.16986)
*Amartya Chakraborty,Paresh Dashore,Nadia Bathaee,Anmol Jain,Anirban Das,Shi-Xiong Zhang,Sambit Sahu,Milind Naphade,Genta Indra Winata*

Key words: LLMs, 多轮对话, 工具依赖, 规划, T1数据集

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 生成了T1数据集，用于评估LLMs在多轮对话中工具依赖问题的规划和推理能力，支持多领域、动态重规划和缓存机制。

Motivation: LLMs在复杂规划和多轮工具依赖场景中的能力仍需提升，亟需专门的数据集支持研究。

Method: 提出T1数据集，包含多领域（单域和多域）交互任务，集成短长期缓存机制，并支持动态重规划。

Result: T1-Agent展示了在复杂工具依赖场景中的规划和推理能力。

Conclusion: T1为工具使用和规划研究提供了新基准，并推动了开源模型性能评估。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities as
intelligent agents capable of solving complex problems. However, effective
planning in scenarios involving dependencies between API or tool
calls-particularly in multi-turn conversations-remains a significant challenge.
To address this, we introduce T1, a tool-augmented, multi-domain, multi-turn
conversational dataset specifically designed to capture and manage inter-tool
dependencies across diverse domains. T1 enables rigorous evaluation of agents'
ability to coordinate tool use across nine distinct domains (4 single domain
and 5 multi-domain) with the help of an integrated caching mechanism for both
short- and long-term memory, while supporting dynamic replanning-such as
deciding whether to recompute or reuse cached results. Beyond facilitating
research on tool use and planning, T1 also serves as a benchmark for evaluating
the performance of open-source language models. We present results powered by
T1-Agent, highlighting their ability to plan and reason in complex,
tool-dependent scenarios.

</details>


### [127] [MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2505.16988)
*Rui Ye,Keduan Huang,Qimin Wu,Yuzhu Cai,Tian Jin,Xianghe Pang,Xiangrui Liu,Jiaqi Su,Chen Qian,Bohan Tang,Kaiqu Liang,Jiaao Chen,Yue Hu,Zhenfei Yin,Rongye Shi,Bo An,Yang Gao,Wenjun Wu,Lei Bai,Siheng Chen*

Key words: LLM-based MAS, 代码库, 多代理系统, 基准测试, 开源贡献

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 介绍了MASLab，一个统一且全面的LLM-based多代理系统代码库，解决了当前领域缺乏统一代码库的问题。

Motivation: 鉴于LLM-based多代理系统领域缺乏统一代码库，导致冗余实现和不公平比较，希望通过MASLab解决这些问题。

Method: MASLab整合了20多种现有方法，提供统一环境和多种基准测试，并对方法进行共享简化结构实现。

Result: 涵盖10+基准和8模型的实验验证了MASLab的价值，展示了当前MAS方法的全面情况。

Conclusion: MASLab有助于降低研究门槛，追踪最新发展，并欢迎开源社区贡献，将持续发展。

Abstract: LLM-based multi-agent systems (MAS) have demonstrated significant potential
in enhancing single LLMs to address complex and diverse tasks in practical
applications. Despite considerable advancements, the field lacks a unified
codebase that consolidates existing methods, resulting in redundant
re-implementation efforts, unfair comparisons, and high entry barriers for
researchers. To address these challenges, we introduce MASLab, a unified,
comprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab
integrates over 20 established methods across multiple domains, each rigorously
validated by comparing step-by-step outputs with its official implementation.
(2) MASLab provides a unified environment with various benchmarks for fair
comparisons among methods, ensuring consistent inputs and standardized
evaluation protocols. (3) MASLab implements methods within a shared streamlined
structure, lowering the barriers for understanding and extension. Building on
MASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,
offering researchers a clear and comprehensive view of the current landscape of
MAS methods. MASLab will continue to evolve, tracking the latest developments
in the field, and invite contributions from the broader open-source community.

</details>


### [128] [DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization](https://arxiv.org/abs/2505.16995)
*Chao Zhang,Xin Shi,Xueqiao Zhang,Yifan Zhu,Yi Yang,Yawei Luo*

Key words: 情感支持对话, 直接偏好优化, Inferential Preference Mining, 解耦框架, 情绪调节模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种解耦的情感支持对话框架，通过Inferential Preference Mining构建高质量偏好数据，解决了现有数据结构和优化模糊性问题，显著提升了响应质量。

Motivation: 现有情感支持对话（ESC）在使用监督微调（SFT）时仍存在心理错误，而直接偏好优化（DPO）由于数据纠缠和优化模糊性在ESC任务中效果有限。

Method: 引入Inferential Preference Mining（IPM）构建高质量偏好数据（IPM-PrefDial），并提出基于Gross情绪调节模型的解耦ESC框架，分为策略规划和共情响应生成两个子任务，分别通过SFT和DPO训练。

Result: 实验表明，解耦ESC框架优于联合优化基线，减少了偏好偏差并提升响应质量。

Conclusion: 解耦框架与高质量偏好数据的结合显著提高了ESC任务的效果，为心理偏好对齐提供了新思路。

Abstract: Recent advances in Emotional Support Conversation (ESC) have improved
emotional support generation by fine-tuning Large Language Models (LLMs) via
Supervised Fine-Tuning (SFT). However, common psychological errors still
persist. While Direct Preference Optimization (DPO) shows promise in reducing
such errors through pairwise preference learning, its effectiveness in ESC
tasks is limited by two key challenges: (1) Entangled data structure: Existing
ESC data inherently entangles psychological strategies and response content,
making it difficult to construct high-quality preference pairs; and (2)
Optimization ambiguity: Applying vanilla DPO to such entangled pairwise data
leads to ambiguous training objectives. To address these issues, we introduce
Inferential Preference Mining (IPM) to construct high-quality preference data,
forming the IPM-PrefDial dataset. Building upon this data, we propose a
Decoupled ESC framework inspired by Gross's Extended Process Model of Emotion
Regulation, which decomposes the ESC task into two sequential subtasks:
strategy planning and empathic response generation. Each was trained via SFT
and subsequently enhanced by DPO to align with the psychological preference.
Extensive experiments demonstrate that our Decoupled ESC framework outperforms
joint optimization baselines, reducing preference bias and improving response
quality.

</details>


### [129] [Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?](https://arxiv.org/abs/2505.16998)
*Jin Jiang,Jianing Wang,Yuchen Yan,Yang Liu,Jianhua Zhu,Mengdi Zhang,Xunliang Cai,Liangcai Gao*

Key words: 大型语言模型, 逻辑推理, 形式语言, 评估, 归纳推理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文全面评估了大型语言模型在不同逻辑推理问题中使用形式语言的表现，发现思考模型优于指导模型，形式语言提升了性能，但所有模型在归纳推理上仍有局限。

Motivation: 旨在系统评估大型语言模型在形式语言下的逻辑推理能力，填补现有研究的空白。

Method: 通过三个维度（模型谱系、任务分类、轨迹格式）进行综合评估，并利用形式相关训练数据增强小模型。

Result: 思考模型表现最佳，但所有模型在归纳推理上受限；PoT格式数据泛化性能最好，拒绝微调方法显著提升模型表现。

Conclusion: 形式语言能显着提升逻辑推理性能，但仍需改进归纳推理能力；简单的拒绝微调方法有效提升泛化性。

Abstract: Large Language Models (LLMs) have been shown to achieve breakthrough
performance on complex logical reasoning tasks. Nevertheless, most existing
research focuses on employing formal language to guide LLMs to derive reliable
reasoning paths, while systematic evaluations of these capabilities are still
limited. In this paper, we aim to conduct a comprehensive evaluation of LLMs
across various logical reasoning problems utilizing formal languages. From the
perspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and
format of trajectories, our key findings are: 1) Thinking models significantly
outperform Instruct models, especially when formal language is employed; 2) All
LLMs exhibit limitations in inductive reasoning capability, irrespective of
whether they use a formal language; 3) Data with PoT format achieves the best
generalization performance across other languages. Additionally, we also curate
the formal-relative training data to further enhance the small language models,
and the experimental results indicate that a simple rejected fine-tuning method
can better enable LLMs to generalize across formal languages and achieve the
best overall performance. Our codes and reports are available at
https://github.com/jiangjin1999/FormalEval.

</details>


### [130] [R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.17005)
*Huatong Song,Jinhao Jiang,Wenqing Tian,Zhipeng Chen,Yuhuan Wu,Jiahao Zhao,Yingqian Min,Wayne Xin Zhao,Lei Fang,Ji-Rong Wen*

Key words: 大语言模型（LLMs），检索增强生成（RAG），动态知识获取，强化学习（RL），两阶段训练

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: R1-Searcher++是一个新颖框架，通过两阶段训练策略（SFT冷启动和RL动态知识获取）使LLM能自适应利用内外知识，解决RAG方法的成本高和泛化差问题。

Motivation: 当前RAG方法存在成本高、泛化差或忽略模型内部知识的问题，因此需要开发一种能自适应结合内外知识的框架。

Method: 两阶段训练策略：SFT冷启动学习初步格式，RL动态知识获取阶段通过结果监督鼓励探索，结合奖励机制利用内部知识，并通过记忆机制持续吸收检索信息。

Result: 实验表明R1-Searcher++优于现有RAG和推理方法，实现了高效检索增强推理。

Conclusion: 通过结合内外知识并持续优化，R1-Searcher++显著提升了检索增强推理的能力。

Abstract: Large Language Models (LLMs) are powerful but prone to hallucinations due to
static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting
external information, but current methods often are costly, generalize poorly,
or ignore the internal knowledge of the model. In this paper, we introduce
R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage
both internal and external knowledge sources. R1-Searcher++ employs a two-stage
training strategy: an initial SFT Cold-start phase for preliminary format
learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses
outcome-supervision to encourage exploration, incorporates a reward mechanism
for internal knowledge utilization, and integrates a memorization mechanism to
continuously assimilate retrieved information, thereby enriching the model's
internal knowledge. By leveraging internal knowledge and external search
engine, the model continuously improves its capabilities, enabling efficient
retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++
outperforms previous RAG and reasoning methods and achieves efficient
retrieval. The code is available at
https://github.com/RUCAIBox/R1-Searcher-plus.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [131] [Adaptive Tokenization: On the Hop-Overpriority Problem in Tokenized Graph Learning Models](https://arxiv.org/abs/2505.15845)
*Zhibiao Wang,Yunlong Zhou,Ziwei Zhang,Mengmei Zhang,Shirui Pan,Chunming Hu,Xiao Wang*

Key words: Graph Transformers, Tokenized Graph Learning Models, Learnable Graph Token List, hop-overpriority, heterophilic graphs

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了Learnable Graph Token List (LGTL)，一种可学习的图令牌列表，用于解决现有Tokenized Graph Learning Models (TGLMs)中手工设计令牌列表的局限性，尤其是在处理异质性图时的跳数过优先问题。

Motivation: 现有的TGLMs依赖手工设计的令牌列表，对多样化的图学习场景的适应性不足，尤其是在异质性图中存在跳数过优先问题，即过度强调邻近节点而忽略全局信号。

Method: 提出了LGTL，通过图注意力门模块和选择模块自适应调整跳数权重并优先选择信息丰富的节点，适用于同质性和异质性图。

Result: 实验证明LGTL在Graph Transformers和Graph LLM骨干网络上的有效性，能够解决跳数过优先问题。

Conclusion: LGTL作为一种即插即用模块，能够自适应地提高图令牌列表的性能，适用于多种图学习场景。

Abstract: Graph Transformers, leveraging the global attention to capture long-range
dependencies in graph structures, have significantly advanced graph machine
learning, but face prohibitive computational complexity. Tokenized Graph
Learning Models (TGLMs) address this issue by converting graphs into ordered
token lists for scalable processing. Besides, TGLMs also empower Large Language
Models (LLMs) to handle text-attributed graphs more effectively and thus are
also employed in Graph LLMs. However, existing TGLMs rely on hand-designed
token lists and their adaptability to diverse graph learning scenarios remains
unexplored. In this paper, we first conduct extensive empirical and theoretical
preliminary studies for hand-designed token lists. Surprisingly, we identify an
unexplored hop-overpriority problem: the common pre-defined token lists
overemphasize nearby nodes and overwhelm the ability of TGLMs to balance local
and global signals. This phenomenon is especially harmful for heterophilic
graphs. To address this problem, we propose the Learnable Graph Token List
(LGTL), a plug-and-play module to replace hand-designed token lists in TGLMs.
Specifically, LGTL adaptively adjusts the weights across hops and prioritizes
informative nodes within hops through a graph attention gate module and a
selection module, respectively. In this way, contextually informative nodes can
be adaptively emphasized for both homophilic and heterophilic graphs. Besides,
we theoretically show that LGTL can address the hop-overpriority problem.
Extensive experiments on benchmarks validate the efficacy of LGTL across both
Graph Transformers and Graph LLM backbones.

</details>


### [132] [Last Layer Empirical Bayes](https://arxiv.org/abs/2505.15888)
*Valentin Villecroze,Yixin Wang,Gabriel Loaiza-Ganem*

Key words: Bayesian neural networks, deep ensembles, uncertainty quantification, empirical Bayes, normalizing flow

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为LLEB的方法，通过可学习的先验分布（使用normalizing flow）在最后一层实现，以在贝叶斯神经网络和集成方法之间进行折衷，并展示了其性能与现有方法相当。

Motivation: 量化神经网络预测的不确定性是人工智能中的关键挑战。贝叶斯神经网络（BNNs）和深度集成是解决这一问题的两种主流方法。论文旨在结合这两种方法的优势。

Method: 提出LLEB方法，将可学习的先验分布（通过normalizing flow实现）仅应用于最后一层，并通过最大化证据下界进行训练。

Result: LLEB在性能上与现有方法相当，表明经验贝叶斯在不确定性量化领域具有研究潜力。

Conclusion: LLEB结合了BNNs和集成方法的优点，是一种有效的折衷方案，经验贝叶斯是不确定性量化的重要研究方向。

Abstract: The task of quantifying the inherent uncertainty associated with neural
network predictions is a key challenge in artificial intelligence. Bayesian
neural networks (BNNs) and deep ensembles are among the most prominent
approaches to tackle this task. Both approaches produce predictions by
computing an expectation of neural network outputs over some distribution on
the corresponding weights; this distribution is given by the posterior in the
case of BNNs, and by a mixture of point masses for ensembles. Inspired by
recent work showing that the distribution used by ensembles can be understood
as a posterior corresponding to a learned data-dependent prior, we propose last
layer empirical Bayes (LLEB). LLEB instantiates a learnable prior as a
normalizing flow, which is then trained to maximize the evidence lower bound;
to retain tractability we use the flow only on the last layer. We show why LLEB
is well motivated, and how it interpolates between standard BNNs and ensembles
in terms of the strength of the prior that they use. LLEB performs on par with
existing approaches, highlighting that empirical Bayes is a promising direction
for future research in uncertainty quantification.

</details>


### [133] [Is (Selective) Round-To-Nearest Quantization All You Need?](https://arxiv.org/abs/2505.15909)
*Alex Kogan*

Key words: 量化, RTN, 大型语言模型, 效率, 吞吐量, 数据精度

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: RTN (Round-to-Nearest) 是一种简单有效的量化技术，尽管近来被更先进的方法所忽视。本文证明了 RTN 不仅成本低，而且在令牌生成吞吐量和准确度上可以与先进方法媲美，通过选择性提高数据精度格式逐步改进其性能，支持 RTN 成为量化 LLM 的实际选择。

Motivation: 近年来，量化成为服务不断增长的大型语言模型（LLM）的必要工具，RTN 作为一种简单的量化技术被忽视。本文旨在证明 RTN 在成本、吞吐量和准确度方面的实用性，挑战更先进量化方法的优势观点。

Method: 基于 Marlin 内核实现 RTN，并通过选择性提高某些模型层和模块的数据精度格式来逐步改进 RTN 的准确度。

Result: 实验表明，RTN 在令牌生成吞吐量方面优于先进方法，准确度与之相当，且成本更低。

Conclusion: RTN 是量化 LLM 的一种可行且实用的选择，具有成本优势和可接受的性能。

Abstract: Quantization became a necessary tool for serving ever-increasing Large
Language Models (LLMs). RTN (Round-to-Nearest) is perhaps the simplest
quantization technique that has been around well before LLMs surged to the
forefront of machine learning (ML) research. Yet, it has been largely dismissed
by recent and more advanced quantization methods that claim superiority over
RTN in nearly every aspect of performance. This work aims to dispel this
established point of view, showing that RTN is not only much cheaper to apply,
but also its token generation throughput can be better than and accuracy can be
similar to more advanced alternatives. In particular, we discuss our
implementation of RTN based on the recent Marlin kernels and demonstrate how
the accuracy of RTN can be gradually improved by selectively increasing the
data precision format of certain model layers and modules. Based on our
results, we argue that RTN presents a viable and practical choice for
quantizing LLMs.

</details>


### [134] [AllMetrics: A Unified Python Library for Standardized Metric Evaluation and Robust Data Validation in Machine Learning](https://arxiv.org/abs/2505.15931)
*Morteza Alizadeh,Mehrdad Oveisi,Sonya Falahati,Ghazal Mousavi,Mohsen Alambardar Meybodi,Somayeh Sadat Mehrnia,Ilker Hacihaliloglu,Arman Rahmim,Mohammad R. Salmanpour*

Key words: machine learning, performance metrics, standardization, open-source, reproducibility

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: AllMetrics is a unified Python library designed to standardize ML performance metrics, addressing issues of fragmentation and inconsistency in existing tools.

Motivation: Existing ML metric libraries suffer from fragmentation, inconsistent implementations, and lack of standardization, leading to unreliable results and difficulty in comparisons.

Method: Developed AllMetrics, an open-source library with modular API and robust validation, supporting diverse ML tasks like regression, classification, and clustering.

Result: Empirical analyses show AllMetrics reduces evaluation errors and enhances reliability, validated across datasets from healthcare, finance, and real estate.

Conclusion: AllMetrics successfully standardizes ML metric evaluation, improving reproducibility and trustworthiness in ML workflows.

Abstract: Machine learning (ML) models rely heavily on consistent and accurate
performance metrics to evaluate and compare their effectiveness. However,
existing libraries often suffer from fragmentation, inconsistent
implementations, and insufficient data validation protocols, leading to
unreliable results. Existing libraries have often been developed independently
and without adherence to a unified standard, particularly concerning the
specific tasks they aim to support. As a result, each library tends to adopt
its conventions for metric computation, input/output formatting, error
handling, and data validation protocols. This lack of standardization leads to
both implementation differences (ID) and reporting differences (RD), making it
difficult to compare results across frameworks or ensure reliable evaluations.
To address these issues, we introduce AllMetrics, an open-source unified Python
library designed to standardize metric evaluation across diverse ML tasks,
including regression, classification, clustering, segmentation, and
image-to-image translation. The library implements class-specific reporting for
multi-class tasks through configurable parameters to cover all use cases, while
incorporating task-specific parameters to resolve metric computation
discrepancies across implementations. Various datasets from domains like
healthcare, finance, and real estate were applied to our library and compared
with Python, Matlab, and R components to identify which yield similar results.
AllMetrics combines a modular Application Programming Interface (API) with
robust input validation mechanisms to ensure reproducibility and reliability in
model evaluation. This paper presents the design principles, architectural
components, and empirical analyses demonstrating the ability to mitigate
evaluation errors and to enhance the trustworthiness of ML workflows.

</details>


### [135] [MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding](https://arxiv.org/abs/2505.15946)
*Yuxiang Wei,Yanteng Zhang,Xi Xiao,Tianyang Wang,Xiao Wang,Vince D. Calhoun*

Key words: fMRI, 视觉解码, 混合专家, 可解释性, 扩散模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: MoRE-Brain是一个基于大脑网络的混合专家框架，用于高保真、可适应且可解释的视觉重建，通过动态路由机制增强解码的神经科学洞察力。

Motivation: 当前fMRI视觉解码方法过于关注重建保真度而忽视可解释性，MoRE-Brain旨在填补这一空白。

Method: 采用分层混合专家架构，专家网络处理功能相关体素组，并利用双阶段路由机制和扩散模型合成图像。

Result: 实验验证了MoRE-Brain的高保真重建能力，并展示了其有效利用fMRI信号的能力。

Conclusion: MoRE-Brain为fMRI视觉解码提供了更通用和可解释的解决方案。

Abstract: Decoding visual experiences from fMRI offers a powerful avenue to understand
human perception and develop advanced brain-computer interfaces. However,
current progress often prioritizes maximizing reconstruction fidelity while
overlooking interpretability, an essential aspect for deriving neuroscientific
insight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework
designed for high-fidelity, adaptable, and interpretable visual reconstruction.
MoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture
where distinct experts process fMRI signals from functionally related voxel
groups, mimicking specialized brain networks. The experts are first trained to
encode fMRI into the frozen CLIP space. A finetuned diffusion model then
synthesizes images, guided by expert outputs through a novel dual-stage routing
mechanism that dynamically weighs expert contributions across the diffusion
process. MoRE-Brain offers three main advancements: First, it introduces a
novel Mixture-of-Experts architecture grounded in brain network principles for
neuro-decoding. Second, it achieves efficient cross-subject generalization by
sharing core expert networks while adapting only subject-specific routers.
Third, it provides enhanced mechanistic insight, as the explicit routing
reveals precisely how different modeled brain regions shape the semantic and
spatial attributes of the reconstructed image. Extensive experiments validate
MoRE-Brain's high reconstruction fidelity, with bottleneck analyses further
demonstrating its effective utilization of fMRI signals, distinguishing genuine
neural decoding from over-reliance on generative priors. Consequently,
MoRE-Brain marks a substantial advance towards more generalizable and
interpretable fMRI-based visual decoding. Code will be publicly available soon:
https://github.com/yuxiangwei0808/MoRE-Brain.

</details>


### [136] [Towards Identifiability of Interventional Stochastic Differential Equations](https://arxiv.org/abs/2505.15987)
*Aaron Zweig,Zaikang Lin,Elham Azizi,David Knowles*

Key words: 随机微分方程、可识别性、多重干预、参数恢复、平稳分布

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 这篇论文研究了在多重干预下随机微分方程（SDE）模型的可识别性，首次给出了从平稳分布样本中唯一恢复SDE参数的可证明边界。针对线性SDE给出了紧的干预次数边界，针对小噪声条件下的非线性SDE给出了上界，并通过实验验证了参数恢复的有效性。

Motivation: 随机微分方程模型在多个领域中广泛应用，但在多重干预下，其参数的可识别性问题尚未得到充分研究。本研究旨在填补这一空白，为SDE模型的可识别性提供理论边界，并验证其在实践中的有效性。

Method: 针对线性SDE和非线性SDE（小噪声条件下），分别通过理论分析推导出参数可识别的干预次数边界。实验部分采用合成数据进行验证，并提出了可学习激活函数的参数化方法。

Result: 研究结果表明，线性SDE的参数恢复需要特定次数的干预，且干预次数边界是紧的；非线性SDE在小噪声条件下的参数恢复上界也得到了验证。实验验证了理论结果的有效性，并展示了可学习激活函数在参数化中的优势。

Conclusion: 本文首次为SDE模型的可识别性提供了理论边界，并通过实验验证了其可行性，为后续研究奠定了基础。

Abstract: We study identifiability of stochastic differential equation (SDE) models
under multiple interventions. Our results give the first provable bounds for
unique recovery of SDE parameters given samples from their stationary
distributions. We give tight bounds on the number of necessary interventions
for linear SDEs, and upper bounds for nonlinear SDEs in the small noise regime.
We experimentally validate the recovery of true parameters in synthetic data,
and motivated by our theoretical results, demonstrate the advantage of
parameterizations with learnable activation functions.

</details>


### [137] [Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations](https://arxiv.org/abs/2505.16004)
*Aaron J. Li,Suraj Srinivas,Usha Bhalla,Himabindu Lakkaraju*

Key words: 稀疏自编码器, 大型语言模型, 鲁棒性, 对抗性扰动, 模型监控

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文研究了稀疏自编码器（SAEs）在解释大型语言模型（LLMs）内部激活时的鲁棒性问题，发现其对输入扰动的脆弱性可能限制其在实际监控和监管中的应用。

Motivation: 现有的SAEs评估忽略了概念表示对输入扰动的鲁棒性，而这是概念标签准确性的关键指标。

Method: 作者通过输入空间优化问题来量化鲁棒性，并构建了一个包含对抗性扰动的评估框架。

Result: 实验表明，微小的对抗性扰动就能显著影响SAEs的概念解释，但对LLMs的输出影响较小。

Conclusion: SAEs的概念表示脆弱，可能不适合用于模型监控和监管。

Abstract: Sparse autoencoders (SAEs) are commonly used to interpret the internal
activations of large language models (LLMs) by mapping them to
human-interpretable concept representations. While existing evaluations of SAEs
focus on metrics such as the reconstruction-sparsity tradeoff, human
(auto-)interpretability, and feature disentanglement, they overlook a critical
aspect: the robustness of concept representations to input perturbations. We
argue that robustness must be a fundamental consideration for concept
representations, reflecting the fidelity of concept labeling. To this end, we
formulate robustness quantification as input-space optimization problems and
develop a comprehensive evaluation framework featuring realistic scenarios in
which adversarial perturbations are crafted to manipulate SAE representations.
Empirically, we find that tiny adversarial input perturbations can effectively
manipulate concept-based interpretations in most scenarios without notably
affecting the outputs of the base LLMs themselves. Overall, our results suggest
that SAE concept representations are fragile and may be ill-suited for
applications in model monitoring and oversight.

</details>


### [138] [GradPCA: Leveraging NTK Alignment for Reliable Out-of-Distribution Detection](https://arxiv.org/abs/2505.16017)
*Mariia Seleznova,Hung-Hsu Chou,Claudio Mayrink Verdun,Gitta Kutyniok*

Key words: GradPCA, OOD detection, NTK alignment, PCA, neural networks

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: GradPCA是一种基于神经网络梯度的低秩结构进行异常检测的方法，通过主成分分析提高检测一致性，并通过理论分析支持其有效性。

Motivation: 研究如何利用神经网络梯度中的低秩结构进行更有效的异常检测，并通过理论视角提供设计原则。

Method: 对梯度类均值应用主成分分析（PCA），并结合神经切线核（NTK）对齐的理论分析。

Result: 在标准图像分类基准测试中表现优于现有方法，且理论框架为设计更原则化的检测器提供指导。

Conclusion: GradPCA在异常检测中表现优异，且特征质量对检测效果有重要影响。

Abstract: We introduce GradPCA, an Out-of-Distribution (OOD) detection method that
exploits the low-rank structure of neural network gradients induced by Neural
Tangent Kernel (NTK) alignment. GradPCA applies Principal Component Analysis
(PCA) to gradient class-means, achieving more consistent performance than
existing methods across standard image classification benchmarks. We provide a
theoretical perspective on spectral OOD detection in neural networks to support
GradPCA, highlighting feature-space properties that enable effective detection
and naturally emerge from NTK alignment. Our analysis further reveals that
feature quality -- particularly the use of pretrained versus non-pretrained
representations -- plays a crucial role in determining which detectors will
succeed. Extensive experiments validate the strong performance of GradPCA, and
our theoretical framework offers guidance for designing more principled
spectral OOD detectors.

</details>


### [139] [Toward Theoretical Insights into Diffusion Trajectory Distillation via Operator Merging](https://arxiv.org/abs/2505.16024)
*Weiguo Gao,Ming Li*

Key words: 扩散模型, 轨迹蒸馏, 线性算子, 动态规划, 信号保真度

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了扩散轨迹蒸馏方法的理论分析框架，将其视为线性算子合并问题，并通过动态规划算法优化信号保真度，揭示了数据协方差结构对最优策略的相变影响。

Motivation: 扩散模型虽然能生成高质量结果，但采样速度慢。现有蒸馏方法缺乏对蒸馏策略与生成质量之间权衡的理论理解，限制了其优化和选择。

Method: 将轨迹蒸馏重新解释为线性算子合并问题，提出动态规划算法计算最优合并策略，以最大化信号保真度。

Result: 发现数据协方差结构会导致最优策略的尖锐相变，并通过理论分析揭示了信号收缩的原因。

Conclusion: 研究提升了扩散轨迹蒸馏的理论理解，并为改进蒸馏策略提供了实用指导。

Abstract: Diffusion trajectory distillation methods aim to accelerate sampling in
diffusion models, which produce high-quality outputs but suffer from slow
sampling speeds. These methods train a student model to approximate the
multi-step denoising process of a pretrained teacher model in a single step,
enabling one-shot generation. However, theoretical insights into the trade-off
between different distillation strategies and generative quality remain
limited, complicating their optimization and selection. In this work, we take a
first step toward addressing this gap. Specifically, we reinterpret trajectory
distillation as an operator merging problem in the linear regime, where each
step of the teacher model is represented as a linear operator acting on noisy
data. These operators admit a clear geometric interpretation as projections and
rescalings corresponding to the noise schedule. During merging, signal
shrinkage occurs as a convex combination of operators, arising from both
discretization and limited optimization time of the student model. We propose a
dynamic programming algorithm to compute the optimal merging strategy that
maximally preserves signal fidelity. Additionally, we demonstrate the existence
of a sharp phase transition in the optimal strategy, governed by data
covariance structures. Our findings enhance the theoretical understanding of
diffusion trajectory distillation and offer practical insights for improving
distillation strategies.

</details>


### [140] [Equivariant Eikonal Neural Networks: Grid-Free, Scalable Travel-Time Prediction on Homogeneous Spaces](https://arxiv.org/abs/2505.16035)
*Alejandro García-Castellanos,David R. Wessels,Nicky J. van den Berg,Remco Duits,Daniël M. Pelt,Erik J. Bekkers*

Key words: Eikonal方程,等变神经场,物理信息神经网络,黎曼流形,点云

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种结合等变神经场与神经Eikonal求解器的新框架，通过单一神经场和点云潜在变量高效建模多样Eikonal解，具有权重共享、几何基础和可操控性等优势。

Motivation: 为解决传统Eikonal求解器在多样性和可操控性上的局限性，结合等变神经场与物理信息神经网络，提升建模效率和几何一致性。

Method: 使用单一神经场，通过共享主干网络和点云潜在变量建模Eikonal解，结合PINNs确保物理准确性，适用于任意黎曼流形。

Result: 在2D和3D地震走时建模中表现出优越的性能、可扩展性、适应性和用户可控性。

Conclusion: 该框架为Eikonal问题提供了高效且可操控的解决方案，适用于多种几何空间。

Abstract: We introduce Equivariant Neural Eikonal Solvers, a novel framework that
integrates Equivariant Neural Fields (ENFs) with Neural Eikonal Solvers. Our
approach employs a single neural field where a unified shared backbone is
conditioned on signal-specific latent variables - represented as point clouds
in a Lie group - to model diverse Eikonal solutions. The ENF integration
ensures equivariant mapping from these latent representations to the solution
field, delivering three key benefits: enhanced representation efficiency
through weight-sharing, robust geometric grounding, and solution steerability.
This steerability allows transformations applied to the latent point cloud to
induce predictable, geometrically meaningful modifications in the resulting
Eikonal solution. By coupling these steerable representations with
Physics-Informed Neural Networks (PINNs), our framework accurately models
Eikonal travel-time solutions while generalizing to arbitrary Riemannian
manifolds with regular group actions. This includes homogeneous spaces such as
Euclidean, position-orientation, spherical, and hyperbolic manifolds. We
validate our approach through applications in seismic travel-time modeling of
2D and 3D benchmark datasets. Experimental results demonstrate superior
performance, scalability, adaptability, and user controllability compared to
existing Neural Operator-based Eikonal solver methods.

</details>


### [141] [Learning from Algorithm Feedback: One-Shot SAT Solver Guidance with GNNs](https://arxiv.org/abs/2505.16053)
*Jan Tönshoff,Martin Grohe*

Key words: SAT求解器；强化学习；图神经网络；组合优化；启发式策略

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 通过强化学习与图神经网络（GNN）结合，提出了一种名为RLAF的新方法，用于学习指导SAT求解器的分支启发式策略，显著提升了求解效率。

Motivation: 现有的SAT求解器性能依赖于人工设计的启发式策略，限制了其潜力。本文提出了一种基于数据驱动的方法，通过学习优化启发式策略。

Method: 使用GNN在单次前向传递中为所有变量分配权重和极性，并将其注入现有SAT求解器的分支启发式中。通过强化学习方法（如GRPO）训练GNN，直接以求解器的计算成本作为奖励信号。

Result: 实验表明，RLAF训练的算法显著降低了不同SAT求解器的平均求解时间，在某些情况下速度提升超过2倍，并且能够泛化到更大更难的问题。

Conclusion: RLAF方法优于基于专家监督的启发式策略设计，为组合优化中的数据驱动启发式设计提供了新方向。

Abstract: Boolean Satisfiability (SAT) solvers are foundational to computer science,
yet their performance typically hinges on hand-crafted heuristics. This work
introduces Reinforcement Learning from Algorithm Feedback (RLAF) as a paradigm
for learning to guide SAT solver branching heuristics with Graph Neural
Networks (GNNs). Central to our approach is a novel and generic mechanism for
injecting inferred variable weights and polarities into the branching
heuristics of existing SAT solvers. In a single forward pass, a GNN assigns
these parameters to all variables. Casting this one-shot guidance as a
reinforcement learning problem lets us train the GNN with off-the-shelf
policy-gradient methods, such as GRPO, directly using the solver's
computational cost as the sole reward signal. Extensive evaluations demonstrate
that RLAF-trained policies significantly reduce the mean solve times of
different base solvers across diverse SAT problem distributions, achieving more
than a 2x speedup in some cases, while generalizing effectively to larger and
harder problems after training. Notably, these policies consistently outperform
expert-supervised approaches based on learning handcrafted weighting
heuristics, offering a promising path towards data-driven heuristic design in
combinatorial optimization.

</details>


### [142] [Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models](https://arxiv.org/abs/2505.16056)
*Jingcong Liang,Siyuan Wang,Miren Tian,Yitong Li,Duyu Tang,Zhongyu Wei*

Key words: Mixture-of-Experts, 局部路由一致性, 专家卸载, 缓存优化, LLM部署

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了两种度量MoE模型局部路由一致性的指标，分析了20种不同MoE LLM，发现每层应用MoE且不共享专家的模型具有最高的路由一致性。领域专家对一致性的贡献更大，缓存大小约为活跃专家的2倍时可平衡效率。

Motivation: 研究MoE模型中局部路由一致性的程度及其影响因素，以优化内存受限设备上的部署效率。

Method: 提出SRP和SCH两种指标，分析20种MoE LLM的路由一致性与模型设计、专家类型的关系。

Result: 每层独立MoE模型路由一致性最高；领域专家贡献大于词汇专家；最佳缓存大小约为活跃专家的2倍。

Conclusion: 局部路由一致性分析为高效MoE设计提供了方向，可在不牺牲推理速度的前提下优化内存使用。

Abstract: Mixture-of-Experts (MoE) enables efficient scaling of large language models
(LLMs) with sparsely activated experts during inference. To effectively deploy
large MoE models on memory-constrained devices, many systems introduce *expert
offloading* that caches a subset of experts in fast memory, leaving others on
slow memory to run on CPU or load on demand. While some research has exploited
the locality of expert activations, where consecutive tokens activate similar
experts, the degree of this **local routing consistency** varies across models
and remains understudied. In this paper, we propose two metrics to measure
local routing consistency of MoE models: (1) **Segment Routing Best Performance
(SRP)**, which evaluates how well a fixed group of experts can cover the needs
of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which
measures the optimal segment-level cache hit rate under a given cache size
limit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found
that models that apply MoE on every layer and do not use shared experts exhibit
the highest local routing consistency. We further showed that
domain-specialized experts contribute more to routing consistency than
vocabulary-specialized ones, and that most models can balance between cache
effectiveness and efficiency with cache sizes approximately 2x the active
experts. These findings pave the way for memory-efficient MoE design and
deployment without compromising inference speed. We publish the code for
replicating experiments at https://github.com/ljcleo/moe-lrc .

</details>


### [143] [Mesh-free sparse identification of nonlinear dynamics](https://arxiv.org/abs/2505.16058)
*Mars Liyao Gao,J. Nathan Kutz,Bernat Font*

Key words: 动态系统, 控制方程识别, 神经网络, 自动微分, 非均匀数据

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种名为mesh-free SINDy的新算法，通过神经网络和自动微分从非均匀数据中识别动态系统的控制方程，具有高效性和抗噪能力。

Motivation: 动态系统控制方程的识别通常需要高质量的数据，但实际中数据常为非均匀采样，传统方法难以适用。

Method: 结合神经网络近似和自动微分技术，适用于任意传感器布局和非均匀时间采样的数据。

Result: 在高噪声和有限数据下表现鲁棒，训练时间短且无需复杂调参，成功识别多种PDE。

Conclusion: mesh-free SINDy在噪声大、数据少的情况下仍能有效识别控制方程，具有广泛适用性。

Abstract: Identifying the governing equations of a dynamical system is one of the most
important tasks for scientific modeling. However, this procedure often requires
high-quality spatio-temporal data uniformly sampled on structured grids. In
this paper, we propose mesh-free SINDy, a novel algorithm which leverages the
power of neural network approximation as well as auto-differentiation to
identify governing equations from arbitrary sensor placements and non-uniform
temporal data sampling. We show that mesh-free SINDy is robust to high noise
levels and limited data while remaining computationally efficient. In our
implementation, the training procedure is straight-forward and nearly free of
hyperparameter tuning, making mesh-free SINDy widely applicable to many
scientific and engineering problems. In the experiments, we demonstrate its
effectiveness on a series of PDEs including the Burgers' equation, the heat
equation, the Korteweg-De Vries equation and the 2D advection-diffusion
equation. We conduct detailed numerical experiments on all datasets, varying
the noise levels and number of samples, and we also compare our approach to
previous state-of-the-art methods. It is noteworthy that, even in high-noise
and low-data scenarios, mesh-free SINDy demonstrates robust PDE discovery,
achieving successful identification with up to 75% noise for the Burgers'
equation using 5,000 samples and with as few as 100 samples and 1% noise. All
of this is achieved within a training time of under one minute.

</details>


### [144] [Few-Shot Test-Time Optimization Without Retraining for Semiconductor Recipe Generation and Beyond](https://arxiv.org/abs/2505.16060)
*Shangding Gu,Donghao Ying,Ming Jin,Yu Joe Lu,Jun Wang,Javad Lavaei,Costas Spanos*

Key words: Model Feedback Learning, test-time optimization, semiconductor manufacturing, stability-aware optimization

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: MFL是一种新型测试时优化框架，无需重新训练模型或修改硬件即可优化预训练AI模型或部署硬件系统的输入，适用于半导体制造等实际场景。

Motivation: 现有方法需要调整模型参数，但实际部署中修改系统可能不可行或成本高昂，因此需要一种更高效的输入优化方法。

Method: 利用轻量级反向模型迭代搜索最优输入，结合稳定性感知优化增强鲁棒性。

Result: 在半导体等离子蚀刻等任务中，MFL仅需5次迭代即可生成目标配方，表现优于贝叶斯优化和人类专家，并在化学和电子系统中展示广泛适用性。

Conclusion: MFL为现实环境中的智能控制提供了高效、可扩展的优化范例。

Abstract: We introduce Model Feedback Learning (MFL), a novel test-time optimization
framework for optimizing inputs to pre-trained AI models or deployed hardware
systems without requiring any retraining of the models or modifications to the
hardware. In contrast to existing methods that rely on adjusting model
parameters, MFL leverages a lightweight reverse model to iteratively search for
optimal inputs, enabling efficient adaptation to new objectives under
deployment constraints. This framework is particularly advantageous in
real-world settings, such as semiconductor manufacturing recipe generation,
where modifying deployed systems is often infeasible or cost-prohibitive. We
validate MFL on semiconductor plasma etching tasks, where it achieves target
recipe generation in just five iterations, significantly outperforming both
Bayesian optimization and human experts. Beyond semiconductor applications, MFL
also demonstrates strong performance in chemical processes (e.g., chemical
vapor deposition) and electronic systems (e.g., wire bonding), highlighting its
broad applicability. Additionally, MFL incorporates stability-aware
optimization, enhancing robustness to process variations and surpassing
conventional supervised learning and random search methods in high-dimensional
control settings. By enabling few-shot adaptation, MFL provides a scalable and
efficient paradigm for deploying intelligent control in real-world
environments.

</details>


### [145] [Merge to Mix: Mixing Datasets via Model Merging](https://arxiv.org/abs/2505.16066)
*Zhixu Silvia Tao,Kasper Vinken,Hao-Wei Yeh,Avi Cooper,Xavier Boix*

Key words: 数据集混合,模型合并,微调,性能优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为Merge to Mix的新方法，通过模型合并加速数据集混合的构建，避免了对每个候选混合进行完整微调的需求，实验表明其性能优于现有方法。

Motivation: 为了最大化下游任务性能，混合数据集微调大型模型变得重要，但现有方法依赖启发式和试错，成本高且效率低。

Method: 提出Merge to Mix方法，通过合并单个数据集微调的模型（模型合并技术）替代对整个混合数据集的微调，加速数据集选择。

Result: 实验证明Merge to Mix在数据集选择任务上优于现有方法。

Conclusion: Merge to Mix为数据集混合构建提供了一种高效替代方案。

Abstract: Mixing datasets for fine-tuning large models (LMs) has become critical for
maximizing performance on downstream tasks. However, composing effective
dataset mixtures typically relies on heuristics and trial-and-error, often
requiring multiple fine-tuning runs to achieve the desired outcome. We propose
a novel method, $\textit{Merge to Mix}$, that accelerates composing dataset
mixtures through model merging. Model merging is a recent technique that
combines the abilities of multiple individually fine-tuned LMs into a single LM
by using a few simple arithmetic operations. Our key insight is that merging
models individually fine-tuned on each dataset in a mixture can effectively
serve as a surrogate for a model fine-tuned on the entire mixture. Merge to Mix
leverages this insight to accelerate selecting dataset mixtures without
requiring full fine-tuning on each candidate mixture. Our experiments
demonstrate that Merge to Mix surpasses state-of-the-art methods in dataset
selection for fine-tuning LMs.

</details>


### [146] [Bidirectional Variational Autoencoders](https://arxiv.org/abs/2505.16074)
*Bart Kosko,Olaoluwa Adigun*

Key words: BVAE, 变分自编码器, 双向结构, 图像任务

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: BVAE网络架构通过单一神经网络实现双向编码和解码，参数减少近50%，性能略优于传统VAE。

Motivation: 提出双向变分自编码器（BVAE），改进传统VAE的结构，减少参数同时提升性能。

Method: 使用单一神经网络双向编码和解码，通过同一组突触网络实现前向编码和后向解码。

Result: 在图像重建、分类、插值和生成任务中，BVAE参数减少近50%，性能略优于传统VAE。

Conclusion: BVAE通过双向结构高效减少参数并保持性能，优于传统VAE。

Abstract: We present the new bidirectional variational autoencoder (BVAE) network
architecture. The BVAE uses a single neural network both to encode and decode
instead of an encoder-decoder network pair. The network encodes in the forward
direction and decodes in the backward direction through the same synaptic web.
Simulations compared BVAEs and ordinary VAEs on the four image tasks of image
reconstruction, classification, interpolation, and generation. The image
datasets included MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and
CelebA-64 face images. The bidirectional structure of BVAEs cut the parameter
count by almost 50% and still slightly outperformed the unidirectional VAEs.

</details>


### [147] [Ensembling Sparse Autoencoders](https://arxiv.org/abs/2505.16077)
*Soham Gadgil,Chris Lin,Su-In Lee*

Key words: 稀疏自编码器, 特征提取, bagging, boosting, 语言模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究了如何通过集成多个稀疏自编码器(SAEs)来提高特征提取效果，实验证明集成方法在重构激活、特征多样性和下游任务中表现优于单个SAE。

Motivation: 单个SAE只能捕捉激活空间中的有限特征，不同初始化的SAE学习到的特征不同，表明集成多个SAE可能改进性能。

Method: 采用naive bagging和boosting方法集成多个SAE，前者基于不同权重初始化的SAE，后者通过最小化残差顺序训练SAE。

Result: 实验证明，集成SAE在语言模型激活重构、特征多样性和稳定性上表现更优，且在下游任务（如概念检测和虚假关联去除）中效果更好。

Conclusion: 集成SAE能更全面地提取特征，并提升实际应用效果。

Abstract: Sparse autoencoders (SAEs) are used to decompose neural network activations
into human-interpretable features. Typically, features learned by a single SAE
are used for downstream applications. However, it has recently been shown that
SAEs trained with different initial weights can learn different features,
demonstrating that a single SAE captures only a limited subset of features that
can be extracted from the activation space. Motivated by this limitation, we
propose to ensemble multiple SAEs through naive bagging and boosting.
Specifically, SAEs trained with different weight initializations are ensembled
in naive bagging, whereas SAEs sequentially trained to minimize the residual
error are ensembled in boosting. We evaluate our ensemble approaches with three
settings of language models and SAE architectures. Our empirical results
demonstrate that ensembling SAEs can improve the reconstruction of language
model activations, diversity of features, and SAE stability. Furthermore,
ensembling SAEs performs better than applying a single SAE on downstream tasks
such as concept detection and spurious correlation removal, showing improved
practical utility.

</details>


### [148] [FR-Mamba: Time-Series Physical Field Reconstruction Based on State Space Model](https://arxiv.org/abs/2505.16083)
*Jiahuan Long,Wenzhe Zhang,Ning Wang,Tingsong Jiang,Wen Yao*

Key words: 物理场重建,时空建模,状态空间模型,傅里叶神经算子,Mamba

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为FR-Mamba的新型时空流场重建框架，结合FNO和SSM，有效捕捉全局空间特征和长期时间依赖，显著优于现有方法。

Motivation: 现有深度学习方法难以捕捉长期时间依赖，导致时间演化物理系统的重建性能不佳，需改进。

Method: 设计混合神经网络架构（FNO+SSM），利用Mamba高效建模长期时间依赖，FNO捕捉非局部空间特征，融合二者实现全场重建。

Result: 实验表明，FR-Mamba在流场重建任务中显著优于现有方法，尤其在长序列上表现优异。

Conclusion: FR-Mamba结合时空建模优势，为物理场重建提供了高效准确的解决方案，尤其在长期依赖场景中表现突出。

Abstract: Physical field reconstruction (PFR) aims to predict the state distribution of
physical quantities (e.g., velocity, pressure, and temperature) based on
limited sensor measurements. It plays a critical role in domains such as fluid
dynamics and thermodynamics. However, existing deep learning methods often fail
to capture long-range temporal dependencies, resulting in suboptimal
performance on time-evolving physical systems. To address this, we propose
FR-Mamba, a novel spatiotemporal flow field reconstruction framework based on
state space modeling. Specifically, we design a hybrid neural network
architecture that combines Fourier Neural Operator (FNO) and State Space Model
(SSM) to capture both global spatial features and long-range temporal
dependencies. We adopt Mamba, a recently proposed efficient SSM architecture,
to model long-range temporal dependencies with linear time complexity. In
parallel, the FNO is employed to capture non-local spatial features by
leveraging frequency-domain transformations. The spatiotemporal representations
extracted by these two components are then fused to reconstruct the full-field
distribution of the physical system. Extensive experiments demonstrate that our
approach significantly outperforms existing PFR methods in flow field
reconstruction tasks, achieving high-accuracy performance on long sequences.

</details>


### [149] [A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization](https://arxiv.org/abs/2505.16094)
*Ziqing Wang,Kexin Zhang,Zihan Zhao,Yibo Wen,Abhishek Pandey,Han Liu,Kaize Ding*

Key words: 大型语言模型, 分子发现, 分子生成, 分子优化, 分类法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 这篇论文综述了大型语言模型（LLM）在分子发现中的应用，重点关注分子生成与优化任务，并提出了分类法、代表性技术与未来挑战。

Motivation: 推动LLM在分子发现领域的新兴应用，为研究者提供资源与方向。

Method: 提出分类法，分析代表性技术，总结常用数据集与评估协议。

Result: 系统梳理了LLM在分子发现中的两类核心任务，并探讨了技术发展趋势。

Conclusion: LLM为分子发现带来范式转变，但仍需解决关键挑战，未来潜力巨大。

Abstract: Large language models (LLMs) are introducing a paradigm shift in molecular
discovery by enabling text-guided interaction with chemical spaces through
natural language, symbolic notations, with emerging extensions to incorporate
multi-modal inputs. To advance the new field of LLM for molecular discovery,
this survey provides an up-to-date and forward-looking review of the emerging
use of LLMs for two central tasks: molecule generation and molecule
optimization. Based on our proposed taxonomy for both problems, we analyze
representative techniques in each category, highlighting how LLM capabilities
are leveraged across different learning settings. In addition, we include the
commonly used datasets and evaluation protocols. We conclude by discussing key
challenges and future directions, positioning this survey as a resource for
researchers working at the intersection of LLMs and molecular science. A
continuously updated reading list is available at
https://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery.

</details>


### [150] [Reinforcement Learning for Stock Transactions](https://arxiv.org/abs/2505.16099)
*Ziyi,Zhou,Nicholas Stern,Julien Laasri*

Key words: 强化学习、Q学习、深度Q学习、股票市场、马尔可夫决策过程

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文应用强化学习方法（如Q学习与深度Q学习）训练智能体以确定股票买卖的最佳时机，并比较不同智能体的表现以寻找最优策略。

Motivation: 股票市场的混乱交易模式中可能存在潜在规律，若能识别这些规律则可从中获利。因此，研究旨在利用强化学习优化股票买卖时机决策，提升投资收益。

Method: 通过定义马尔可夫决策过程（MDP）问题，使用Q学习、线性函数逼近的Q学习及深度Q学习训练智能体，并结合机器学习回归与分类模型预测股价，比较不同智能体的收敛性与策略效果。

Result: 研究发现部分智能体能够收敛到某种交易策略，且不同强化学习方法在股票市场中的表现存在差异，可能筛选出最优的盈利策略。

Conclusion: 强化学习（尤其是深度Q学习）在股票买卖决策中具有潜力，但仍需进一步优化模型以适应市场复杂性。

Abstract: Much research has been done to analyze the stock market. After all, if one
can determine a pattern in the chaotic frenzy of transactions, then they could
make a hefty profit from capitalizing on these insights. As such, the goal of
our project was to apply reinforcement learning (RL) to determine the best time
to buy a stock within a given time frame. With only a few adjustments, our
model can be extended to identify the best time to sell a stock as well. In
order to use the format of free, real-world data to train the model, we define
our own Markov Decision Process (MDP) problem. These two papers [5] [6] helped
us in formulating the state space and the reward system of our MDP problem. We
train a series of agents using Q-Learning, Q-Learning with linear function
approximation, and deep Q-Learning. In addition, we try to predict the stock
prices using machine learning regression and classification models. We then
compare our agents to see if they converge on a policy, and if so, which one
learned the best policy to maximize profit on the stock market.

</details>


### [151] [Towards Trustworthy Keylogger detection: A Comprehensive Analysis of Ensemble Techniques and Feature Selections through Explainable AI](https://arxiv.org/abs/2505.16103)
*Monirul Islam Mahmud*

Key words: 键盘记录器检测,机器学习,特征选择,可解释AI,AdaBoost

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了利用传统机器学习模型和集成方法进行键盘记录器检测，结合特征选择和可解释AI技术，最终AdaBoost模型表现最佳。

Motivation: 研究键盘记录器检测，通过监控异常系统行为和流量模式，提高检测准确性和可解释性。

Method: 使用SVC、随机森林等传统模型及Stacking等集成方法，结合特征选择和XAI技术（SHAP、LIME）。

Result: AdaBoost表现最优，准确率99.76%，F1分数0.99，其他指标接近完美。

Conclusion: 特征选择和XAI技术显著提升模型性能，AdaBoost是键盘记录器检测的最佳选择。

Abstract: Keylogger detection involves monitoring for unusual system behaviors such as
delays between typing and character display, analyzing network traffic patterns
for data exfiltration. In this study, we provide a comprehensive analysis for
keylogger detection with traditional machine learning models - SVC, Random
Forest, Decision Tree, XGBoost, AdaBoost, Logistic Regression and Naive Bayes
and advanced ensemble methods including Stacking, Blending and Voting.
Moreover, feature selection approaches such as Information gain, Lasso L1 and
Fisher Score are thoroughly assessed to improve predictive performance and
lower computational complexity. The Keylogger Detection dataset from publicly
available Kaggle website is used in this project. In addition to accuracy-based
classification, this study implements the approach for model interpretation
using Explainable AI (XAI) techniques namely SHAP (Global) and LIME (Local) to
deliver finer explanations for how much each feature contributes in assisting
or hindering the detection process. To evaluate the models result, we have used
AUC score, sensitivity, Specificity, Accuracy and F1 score. The best
performance was achieved by AdaBoost with 99.76% accuracy, F1 score of 0.99,
100% precision, 98.6% recall, 1.0 specificity and 0.99 of AUC that is
near-perfect classification with Fisher Score.

</details>


### [152] [Tools in the Loop: Quantifying Uncertainty of LLM Question Answering Systems That Use Tools](https://arxiv.org/abs/2505.16113)
*Panagiotis Lymperopoulos,Vasanth Sarathy*

Key words: 大型语言模型,不确定性量化,工具调用,检索增强生成,可信度

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种新框架，用于量化大型语言模型（LLM）与外部工具结合使用时的不确定性，以提高系统可信度。

Motivation: 在高风险应用中，如医疗决策，需要评估LLM生成内容与外部工具输出的不确定性，但现有方法未涵盖工具调用场景。

Method: 扩展了针对词序列的不确定性量化方法，提出了高效近似计算，并在合成QA数据集和RAG系统中进行了评估。

Result: 实验证明该框架能有效增强对LLM系统的信任，特别是在LLM内部知识不足且需外部工具的场景下。

Conclusion: 该研究提供了一种实用的不确定性量化方案，提升了LLM与工具结合系统的可靠性。

Abstract: Modern Large Language Models (LLMs) often require external tools, such as
machine learning classifiers or knowledge retrieval systems, to provide
accurate answers in domains where their pre-trained knowledge is insufficient.
This integration of LLMs with external tools expands their utility but also
introduces a critical challenge: determining the trustworthiness of responses
generated by the combined system. In high-stakes applications, such as medical
decision-making, it is essential to assess the uncertainty of both the LLM's
generated text and the tool's output to ensure the reliability of the final
response. However, existing uncertainty quantification methods do not account
for the tool-calling scenario, where both the LLM and external tool contribute
to the overall system's uncertainty. In this work, we present a novel framework
for modeling tool-calling LLMs that quantifies uncertainty by jointly
considering the predictive uncertainty of the LLM and the external tool. We
extend previous methods for uncertainty quantification over token sequences to
this setting and propose efficient approximations that make uncertainty
computation practical for real-world applications. We evaluate our framework on
two new synthetic QA datasets, derived from well-known machine learning
datasets, which require tool-calling for accurate answers. Additionally, we
apply our method to retrieval-augmented generation (RAG) systems and conduct a
proof-of-concept experiment demonstrating the effectiveness of our uncertainty
metrics in scenarios where external information retrieval is needed. Our
results show that the framework is effective in enhancing trust in LLM-based
systems, especially in cases where the LLM's internal knowledge is insufficient
and external tools are required.

</details>


### [153] [A Generic Framework for Conformal Fairness](https://arxiv.org/abs/2505.16115)
*Aditya T. Vadlamani,Anutam Srinivasan,Pranav Maneriker,Ali Payani,Srinivasan Parthasarathy*

Key words: Confomal Prediction, fairness, coverage gap, exchangeability, graph data

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种称为'Conformal Fairness'的公平性概念，通过改进置信预测（CP）来确保不同敏感群体间的覆盖差距得到控制，适用于非IID数据如图数据。

Motivation: 现有的置信预测方法对数据集中的敏感属性无感知，无法保证不同群体的公平覆盖。

Method: 基于交换性假设（而非传统的IID假设），提出了一种理论支持的算法和框架来控制不同敏感群体间的覆盖差距，拓展了其适用范围。

Result: 实验证明，该算法在图表数据上不仅能控制公平性差距，还能保持与理论一致的覆盖效果。

Conclusion: Conformal Fairness为机器学习的公平性提供了一种理论基础，并成功应用于非IID数据。

Abstract: Conformal Prediction (CP) is a popular method for uncertainty quantification
with machine learning models. While conformal prediction provides probabilistic
guarantees regarding the coverage of the true label, these guarantees are
agnostic to the presence of sensitive attributes within the dataset. In this
work, we formalize \textit{Conformal Fairness}, a notion of fairness using
conformal predictors, and provide a theoretically well-founded algorithm and
associated framework to control for the gaps in coverage between different
sensitive groups. Our framework leverages the exchangeability assumption
(implicit to CP) rather than the typical IID assumption, allowing us to apply
the notion of Conformal Fairness to data types and tasks that are not IID, such
as graph data. Experiments were conducted on graph and tabular datasets to
demonstrate that the algorithm can control fairness-related gaps in addition to
coverage aligned with theoretical expectations.

</details>


### [154] [Plan and Budget: Effective and Efficient Test-Time Scaling on Large Language Model Reasoning](https://arxiv.org/abs/2505.16122)
*Junhong Lin,Xinyue Zeng,Jie Zhu,Song Wang,Julian Shun,Jun Wu,Dawei Zhou*

Key words: 大型语言模型, 推理效率, BBAM, Plan-and-Budget, $E^3$度量

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 大型语言模型（LLM）在复杂推理任务中表现出色，但推理效率低下。论文提出BBAM理论模型和$E^3$度量，并设计Plan-and-Budget框架，通过动态分配计算资源提升效率和准确性。

Motivation: 解决LLM推理效率低下的问题，特别是模型在简单查询上过度推理（overthinking）或在困难问题上资源不足（underthinking）的现象。

Method: 提出BBAM理论模型，量化推理中的不确定性和子问题复杂度，并使用Plan-and-Budget框架动态分配计算资源。

Result: Plan-and-Budget显著提升了推理效率和准确性，最高可减少39%的token使用并提高70%的准确率。

Conclusion: 通过动态资源分配，Plan-and-Budget在不重新训练模型的情况下，显著提升了LLM的推理效率和准确性。

Abstract: Large Language Models (LLMs) have achieved remarkable success in complex
reasoning tasks, but their inference remains computationally inefficient. We
observe a common failure mode in many prevalent LLMs, overthinking, where
models generate verbose and tangential reasoning traces even for simple
queries. Recent works have tried to mitigate this by enforcing fixed token
budgets, however, this can lead to underthinking, especially on harder
problems. Through empirical analysis, we identify that this inefficiency often
stems from unclear problem-solving strategies. To formalize this, we develop a
theoretical model, BBAM (Bayesian Budget Allocation Model), which models
reasoning as a sequence of sub-questions with varying uncertainty, and
introduce the $E^3$ metric to capture the trade-off between correctness and
computation efficiency. Building on theoretical results from BBAM, we propose
Plan-and-Budget, a model-agnostic, test-time framework that decomposes complex
queries into sub-questions and allocates token budgets based on estimated
complexity using adaptive scheduling. Plan-and-Budget improves reasoning
efficiency across a range of tasks and models, achieving up to +70% accuracy
gains, -39% token reduction, and +187.5% improvement in $E^3$. Notably, it
elevates a smaller model (DS-Qwen-32B) to match the efficiency of a larger
model (DS-LLaMA-70B)-demonstrating Plan-and-Budget's ability to close
performance gaps without retraining. Our code is available at
anonymous.4open.science/r/P-and-B-6513/.

</details>


### [155] [Robust Invariant Representation Learning by Distribution Extrapolation](https://arxiv.org/abs/2505.16126)
*Kotaro Yoshida,Slavakis Konstantinos*

Key words: 不变风险最小化（IRM）、分布外泛化（OOD）、过参数化、环境多样性、外推

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种基于外推的新框架，通过合成分布偏移增强环境多样性，解决了现有不变风险最小化（IRM）方法因环境多样性不足和过参数化导致的性能下降问题，并在实验中验证了其优越性。

Motivation: 现有IRM方法（如IRMv1）因依赖惩罚性单层近似，在环境多样性有限和过参数化时表现不佳，亟需更鲁棒的实现方案。

Method: 提出基于外推的框架，通过合成分布偏移增强IRM惩罚项的环境多样性。

Result: 在合成和真实过参数化场景下，新方法均优于现有IRM变体，证明了其有效性和鲁棒性。

Conclusion: 外推框架显著提升了IRM的OOD泛化能力，为克服环境多样性限制提供了新思路。

Abstract: Invariant risk minimization (IRM) aims to enable out-of-distribution (OOD)
generalization in deep learning by learning invariant representations. As IRM
poses an inherently challenging bi-level optimization problem, most existing
approaches -- including IRMv1 -- adopt penalty-based single-level
approximations. However, empirical studies consistently show that these methods
often fail to outperform well-tuned empirical risk minimization (ERM),
highlighting the need for more robust IRM implementations. This work
theoretically identifies a key limitation common to many IRM variants: their
penalty terms are highly sensitive to limited environment diversity and
over-parameterization, resulting in performance degradation. To address this
issue, a novel extrapolation-based framework is proposed that enhances
environmental diversity by augmenting the IRM penalty through synthetic
distributional shifts. Extensive experiments -- ranging from synthetic setups
to realistic, over-parameterized scenarios -- demonstrate that the proposed
method consistently outperforms state-of-the-art IRM variants, validating its
effectiveness and robustness.

</details>


### [156] [Scalable Graph Generative Modeling via Substructure Sequences](https://arxiv.org/abs/2505.16130)
*Zehong Wang,Zheyuan Zhang,Tianyi Ma,Chuxu Zhang,Yanfang Ye*

Key words: 图神经网络,生成式预训练,Transformer,可扩展性,图表示学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于生成式Transformer的图神经网络框架G$^2$PM，通过子结构序列化表示和生成式预训练，解决了传统消息传递方法的局限性（如表达能力不足、过平滑、过挤压等），并在多个任务中表现出优异的可扩展性和泛化能力。

Motivation: 传统的图神经网络（GNNs）依赖消息传递机制，存在表达能力有限、过平滑、过挤压以及难以建模长距离依赖等问题，阻碍了其作为图基础模型的潜力。本文探索超越消息传递的路径，提出G$^2$PM框架以解决这些限制。

Method: G$^2$PM将图实例（节点、边或整图）表示为子结构序列，并采用生成式Transformer预训练学习可泛化、可迁移的表示。

Result: 在ogbn-arxiv基准测试中，G$^^2$PM在参数规模达到60M时仍能持续提升性能，显著优于其他生成式方法（如3M参数时即达到性能瓶颈）；同时在节点分类、图分类和迁移学习等任务中均超越基线模型。

Conclusion: G$^2$PM为可扩展的图学习提供了一个强有力的基础框架，并通过系统分析模型设计空间验证了其关键架构选择的有效性。

Abstract: Graph neural networks (GNNs) has been predominantly driven by
message-passing, where node representations are iteratively updated via local
neighborhood aggregation. Despite their success, message-passing suffers from
fundamental limitations -- including constrained expressiveness,
over-smoothing, over-squashing, and limited capacity to model long-range
dependencies. These issues hinder scalability: increasing data size or model
size often fails to yield improved performance, limiting the viability of GNNs
as backbones for graph foundation models. In this work, we explore pathways
beyond message-passing and introduce Generative Graph Pattern Machine
(G$^2$PM), a generative Transformer pre-training framework for graphs. G$^2$PM
represents graph instances (nodes, edges, or entire graphs) as sequences of
substructures, and employs generative pre-training over the sequences to learn
generalizable, transferable representations. Empirically, G$^2$PM demonstrates
strong scalability: on the ogbn-arxiv benchmark, it continues to improve with
model sizes up to 60M parameters, outperforming prior generative approaches
that plateau at significantly smaller scales (e.g., 3M). In addition, we
systematically analyze the model design space, highlighting key architectural
choices that contribute to its scalability and generalization. Across diverse
tasks -- including node classification, graph classification, and transfer
learning -- G$^2$PM consistently outperforms strong baselines, establishing a
compelling foundation for scalable graph learning. The code and dataset are
available at https://github.com/Zehong-Wang/G2PM.

</details>


### [157] [Multimodal Online Federated Learning with Modality Missing in Internet of Things](https://arxiv.org/abs/2505.16138)
*Heqiang Wang,Xiang Liu,Xiaoxiong Zhong,Lixing Chen,Fangming Liu,Weizhe Zhang*

Key words: 多模态学习、在线联邦学习、物联网、模态缺失、原型学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种多模态在线联邦学习框架（MMO-FL），用于处理物联网环境中的多模态数据，并针对边缘设备的模态缺失问题提出了原型模态补偿算法（PMM）。

Motivation: 物联网设备产生的多模态数据复杂且实时性要求高，传统集中式学习难以应对，边缘设备的模态缺失问题进一步增加了学习难度。

Method: 提出MMO-FL框架，并设计PMM算法，通过原型学习补偿缺失模态，支持动态去中心化学习。

Result: 在两种多模态数据集上，PMM算法表现优于基准方法，尤其在模态缺失情况下仍保持较高性能。

Conclusion: MMO-FL和PMM有效解决了物联网环境中多模态数据学习的挑战，为边缘智能提供了实用方案。

Abstract: The Internet of Things (IoT) ecosystem generates vast amounts of multimodal
data from heterogeneous sources such as sensors, cameras, and microphones. As
edge intelligence continues to evolve, IoT devices have progressed from simple
data collection units to nodes capable of executing complex computational
tasks. This evolution necessitates the adoption of distributed learning
strategies to effectively handle multimodal data in an IoT environment.
Furthermore, the real-time nature of data collection and limited local storage
on edge devices in IoT call for an online learning paradigm. To address these
challenges, we introduce the concept of Multimodal Online Federated Learning
(MMO-FL), a novel framework designed for dynamic and decentralized multimodal
learning in IoT environments. Building on this framework, we further account
for the inherent instability of edge devices, which frequently results in
missing modalities during the learning process. We conduct a comprehensive
theoretical analysis under both complete and missing modality scenarios,
providing insights into the performance degradation caused by missing
modalities. To mitigate the impact of modality missing, we propose the
Prototypical Modality Mitigation (PMM) algorithm, which leverages prototype
learning to effectively compensate for missing modalities. Experimental results
on two multimodal datasets further demonstrate the superior performance of PMM
compared to benchmarks.

</details>


### [158] [NAN: A Training-Free Solution to Coefficient Estimation in Model Merging](https://arxiv.org/abs/2505.16148)
*Chongjie Si,Kangtao Lv,Jingjing Jiang,Yadao Wang,Yongwei Wang,Xiaokang Yang,Wenbo Su,Bo Zheng,Wei Shen*

Key words: 模型合并,最小二乘优化,NAN,参数范数

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于最小二乘优化的模型合并方法NAN，通过参数范数反比估计合并系数，有效提升基线方法性能。

Motivation: 现有模型合并方法依赖启发式确定合并系数，限制了扩展性和通用性，需改进。

Method: 通过最小二乘优化重新审视模型合并，提出NAN方法，利用参数范数反比估计合并系数。

Result: NAN在广泛实验中一致提升基线方法的性能。

Conclusion: NAN是一种简单有效的训练自由方法，适用于多种合并策略。

Abstract: Model merging offers a training-free alternative to multi-task learning by
combining independently fine-tuned models into a unified one without access to
raw data. However, existing approaches often rely on heuristics to determine
the merging coefficients, limiting their scalability and generality. In this
work, we revisit model merging through the lens of least-squares optimization
and show that the optimal merging weights should scale with the amount of
task-specific information encoded in each model. Based on this insight, we
propose NAN, a simple yet effective method that estimates model merging
coefficients via the inverse of parameter norm. NAN is training-free,
plug-and-play, and applicable to a wide range of merging strategies. Extensive
experiments on show that NAN consistently improves performance of baseline
methods.

</details>


### [159] [Why Can Accurate Models Be Learned from Inaccurate Annotations?](https://arxiv.org/abs/2505.16159)
*Chongjie Si,Yidan Cui,Fuchao Yang,Xiaokang Yang,Wei Shen*

Key words: 噪声标注,模型鲁棒性,奇异值分解,LIP,主成分

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了模型为何能在不准确标注中学习到正确信息，发现标签不准确性主要在低奇异分量中累积噪声，但主成分信息仍能保留，从而解释了模型的泛化能力。作者提出LIP插件以保留主成分信息并减少噪声影响。

Motivation: 研究模型在不准确标注下仍能准确预测的现象，探索其背后的原因。

Method: 通过分析权重矩阵的奇异值分解，发现标签不准确性对主成分的影响有限，并提出LIP插件以优化分类器性能。

Result: LIP在各种不准确标签任务中稳定提升了现有算法的表现。

Conclusion: 标签不准确性主要影响低奇异分量而主成分信息保留完好，LIP证实了这一发现的有效性。

Abstract: Learning from inaccurate annotations has gained significant attention due to
the high cost of precise labeling. However, despite the presence of erroneous
labels, models trained on noisy data often retain the ability to make accurate
predictions. This intriguing phenomenon raises a fundamental yet largely
unexplored question: why models can still extract correct label information
from inaccurate annotations remains unexplored. In this paper, we conduct a
comprehensive investigation into this issue. By analyzing weight matrices from
both empirical and theoretical perspectives, we find that label inaccuracy
primarily accumulates noise in lower singular components and subtly perturbs
the principal subspace. Within a certain range, the principal subspaces of
weights trained on inaccurate labels remain largely aligned with those learned
from clean labels, preserving essential task-relevant information. We formally
prove that the angles of principal subspaces exhibit minimal deviation under
moderate label inaccuracy, explaining why models can still generalize
effectively. Building on these insights, we propose LIP, a lightweight plug-in
designed to help classifiers retain principal subspace information while
mitigating noise induced by label inaccuracy. Extensive experiments on tasks
with various inaccuracy conditions demonstrate that LIP consistently enhances
the performance of existing algorithms. We hope our findings can offer valuable
theoretical and practical insights to understand of model robustness under
inaccurate supervision.

</details>


### [160] [Enhancing Federated Survival Analysis through Peer-Driven Client Reputation in Healthcare](https://arxiv.org/abs/2505.16190)
*Navid Seidi,Satyaki Roy,Sajal Das*

Key words: 联邦学习, 声誉机制, 差分隐私, 数据异质性, 生存分析

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种联邦学习中结合声誉机制的方法，通过混合通信模型和噪声处理来提升模型聚合效果，同时保护隐私并解决数据异质性。

Motivation: 解决联邦学习中机构间的异质性、声誉不足及不可靠贡献的问题，同时保护患者数据隐私。

Method: 采用混合通信模型结合去中心化同伴反馈和基于聚类的噪声处理，并使用差分隐私技术保护模型更新的隐私。

Result: 在合成数据集和SEER数据集上的实验表明，该方法能稳定获得高C-index值，有效降噪并优于无声誉系统的联邦学习方法。

Conclusion: 提出的方法能有效解决联邦学习中的异质性和声誉问题，同时保护隐私，提升了模型的鲁棒性和性能。

Abstract: Federated Learning (FL) holds great promise for digital health by enabling
collaborative model training without compromising patient data privacy.
However, heterogeneity across institutions, lack of sustained reputation, and
unreliable contributions remain major challenges. In this paper, we propose a
robust, peer-driven reputation mechanism for federated healthcare that employs
a hybrid communication model to integrate decentralized peer feedback with
clustering-based noise handling to enhance model aggregation. Crucially, our
approach decouples the federated aggregation and reputation mechanisms by
applying differential privacy to client-side model updates before sharing them
for peer evaluation. This ensures sensitive information remains protected
during reputation computation, while unaltered updates are sent to the server
for global model training. Using the Cox Proportional Hazards model for
survival analysis across multiple federated nodes, our framework addresses both
data heterogeneity and reputation deficit by dynamically adjusting trust scores
based on local performance improvements measured via the concordance index.
Experimental evaluations on both synthetic datasets and the SEER dataset
demonstrate that our method consistently achieves high and stable C-index
values, effectively down-weighing noisy client updates and outperforming FL
methods that lack a reputation system.

</details>


### [161] [Directional Convergence, Benign Overfitting of Gradient Descent in leaky ReLU two-layer Neural Networks](https://arxiv.org/abs/2505.16204)
*Ichiro Hashimoto*

Key words: 神经网络,梯度下降,Leaky ReLU,方向收敛,良性过拟合

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文证明了固定宽度Leaky ReLU两层神经网络在梯度下降优化下的方向收敛性，并提出了良性过拟合的充分条件及测试误差新相变。

Motivation: 研究梯度下降优化下神经网络参数的方向收敛性，填补此前仅梯度流研究的空白。

Method: 分析收敛方向，建立良性过拟合的充分条件，并在次高斯混合模型中验证。

Result: 证明了方向收敛，发现了测试误差的新相变，推广了正交数据的研究范围。

Conclusion: 梯度下降优化的神经网络在更广泛数据场景下可实现良性过拟合。

Abstract: In this paper, we prove directional convergence of network parameters of
fixed width leaky ReLU two-layer neural networks optimized by gradient descent
with exponential loss, which was previously only known for gradient flow. By a
careful analysis of the convergent direction, we establish sufficient
conditions of benign overfitting and discover a new phase transition in the
test error bound. All of these results hold beyond the nearly orthogonal data
setting which was studied in prior works. As an application, we demonstrate
that benign overfitting occurs with high probability in sub-Gaussian mixture
models.

</details>


### [162] [NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics](https://arxiv.org/abs/2505.16210)
*Zhihang Cai,Xingjun Zhang,Zhendong Tan,Zheng Wei*

Key words: 大型语言模型, KV缓存, 量化, NQKV算法, 推理性能

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为NQKV的算法，通过分析KV缓存的元素分布并采用每块分位数量化，显著减少了内存消耗，提升了LLM的推理性能。

Motivation: 针对大型语言模型在推理过程中KV缓存内存消耗大的问题，研究如何通过量化进一步节省空间而不显著影响模型输出质量。

Method: 提出NQKV算法，分析KV缓存的元素分布，利用每块分位数量化实现信息论最优量化误差。

Result: NQKV使OPT模型在推理时支持2倍批量或4倍上下文长度，吞吐量提升9.3倍。

Conclusion: NQKV有效解决了KV缓存内存消耗问题，显著提升了LLM的推理效率。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency across
a wide range of tasks. However, LLMs often require larger batch sizes to
enhance throughput or longer context lengths to meet task demands, which
significantly increases the memory resource consumption of the Key-Value (KV)
cache during inference, becoming a major bottleneck in LLM deployment. To
address this issue, quantization is a common and straightforward approach.
Currently, quantization methods for activations are limited to 8-bit, and
quantization to even lower bits can lead to substantial accuracy drops. To
further save space by quantizing the KV cache to even lower bits, we analyzed
the element distribution of the KV cache and designed the NQKV algorithm. Since
the elements within each block of the KV cache follow a normal distribution,
NQKV employs per-block quantile quantization to achieve
information-theoretically optimal quantization error. Without significantly
compromising model output quality, NQKV enables the OPT model to perform
inference with an 2x larger batch size or a 4x longer context length, and it
improves throughput by 9.3x compared to when the KV cache is not used.

</details>


### [163] [Reward-Aware Proto-Representations in Reinforcement Learning](https://arxiv.org/abs/2505.16217)
*Hon Tik Tse,Siddarth Chandrasekar,Marlos C. Machado*

Key words: 强化学习,默认表示,后继表示,奖励动态

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 文章讨论了在强化学习中引入默认表示（DR）的理论基础和实际应用，与传统的后继表示（SR）相比，DR能更有效地处理奖励动态问题。

Motivation: 为了解决后继表示（SR）无法考虑奖励动态的问题，研究默认表示（DR）的理论和应用，以提升强化学习的性能。

Method: 通过动态规划和时间差分方法学习DR，分析DR的向量空间基础，并将其扩展到函数近似场景。

Result: DR在奖励塑造、选项发现、探索和迁移学习等任务中表现优于SR，表现出更优的奖励感知行为。

Conclusion: DR在理论和实践中均优于SR，能够更有效地处理奖励动态问题，提升强化学习性能。

Abstract: In recent years, the successor representation (SR) has attracted increasing
attention in reinforcement learning (RL), and it has been used to address some
of its key challenges, such as exploration, credit assignment, and
generalization. The SR can be seen as representing the underlying credit
assignment structure of the environment by implicitly encoding its induced
transition dynamics. However, the SR is reward-agnostic. In this paper, we
discuss a similar representation that also takes into account the reward
dynamics of the problem. We study the default representation (DR), a recently
proposed representation with limited theoretical (and empirical) analysis.
Here, we lay some of the theoretical foundation underlying the DR in the
tabular case by (1) deriving dynamic programming and (2) temporal-difference
methods to learn the DR, (3) characterizing the basis for the vector space of
the DR, and (4) formally extending the DR to the function approximation case
through default features. Empirically, we analyze the benefits of the DR in
many of the settings in which the SR has been applied, including (1) reward
shaping, (2) option discovery, (3) exploration, and (4) transfer learning. Our
results show that, compared to the SR, the DR gives rise to qualitatively
different, reward-aware behaviour and quantitatively better performance in
several settings.

</details>


### [164] [Realistic Evaluation of TabPFN v2 in Open Environments](https://arxiv.org/abs/2505.16226)
*Zi-Jian Cheng,Zi-Yi Jia,Zhi Zhou,Yu-Feng Li,Lan-Zhe Guo*

Key words: TabPFN v2, 开放环境, 表格数据, 树模型, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文评估了TabPFN v2在开放环境中的适应性，发现其在某些特定任务中表现良好，但整体上仍有局限性，树模型仍是开放环境下表格任务的最佳选择。

Motivation: 虽然TabPFN v2在封闭环境中表现优异，但开放环境中的挑战未被充分研究，本文旨在填补这一空白。

Method: 构建了一个统一的评估框架，模拟多种现实挑战，评估TabPFN v2的鲁棒性。

Result: TabPFN v2在开放环境中表现有限，但适用于小规模、协变量偏移和类别平衡的任务；树模型在开放环境下更优。

Conclusion: 建议建立开放环境表格基准、多指标评估和通用模块以提升模型鲁棒性，并公开了评估框架。

Abstract: Tabular data, owing to its ubiquitous presence in real-world domains, has
garnered significant attention in machine learning research. While tree-based
models have long dominated tabular machine learning tasks, the recently
proposed deep learning model TabPFN v2 has emerged, demonstrating unparalleled
performance and scalability potential. Although extensive research has been
conducted on TabPFN v2 to further improve performance, the majority of this
research remains confined to closed environments, neglecting the challenges
that frequently arise in open environments. This raises the question: Can
TabPFN v2 maintain good performance in open environments? To this end, we
conduct the first comprehensive evaluation of TabPFN v2's adaptability in open
environments. We construct a unified evaluation framework covering various
real-world challenges and assess the robustness of TabPFN v2 under open
environments scenarios using this framework. Empirical results demonstrate that
TabPFN v2 shows significant limitations in open environments but is suitable
for small-scale, covariate-shifted, and class-balanced tasks. Tree-based models
remain the optimal choice for general tabular tasks in open environments. To
facilitate future research on open environments challenges, we advocate for
open environments tabular benchmarks, multi-metric evaluation, and universal
modules to strengthen model robustness. We publicly release our evaluation
framework at https://anonymous.4open.science/r/tabpfn-ood-4E65.

</details>


### [165] [Offline Guarded Safe Reinforcement Learning for Medical Treatment Optimization Strategies](https://arxiv.org/abs/2505.16242)
*Runze Yan,Xun Shen,Akifumi Wachi,Sebastien Gros,Anni Zhao,Xiao Hu*

Key words: 离线强化学习, 分布外问题, 安全性, 医疗场景, OGSRL

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种基于模型的安全离线强化学习框架（OGSRL），通过双重约束机制（OOD守护者和安全成本约束）解决医疗场景中离线强化学习的分布外（OOD）问题，确保策略探索的安全性和可靠性。

Motivation: 医疗场景中离线强化学习的分布外问题可能导致有害的推荐，现有方法（如CQL）仅通过抑制不确定动作来约束动作选择，但无法调控下游状态轨迹，限制了长期治疗策略的发现。

Method: 提出OGSRL框架，包含OOD守护者和安全成本约束双重机制。OOD守护者划定临床验证的安全区域，约束策略优化；安全成本约束编码生理安全边界知识，提供领域特异性保障。

Result: OGSRL在理论和实验上均证明能确保策略在安全可靠的区域内探索，性能接近数据支持的最优策略。

Conclusion: OGSRL通过双重约束机制解决了离线强化学习在医疗领域的安全性和可靠性问题，为长期治疗策略的优化提供了理论保障。

Abstract: When applying offline reinforcement learning (RL) in healthcare scenarios,
the out-of-distribution (OOD) issues pose significant risks, as inappropriate
generalization beyond clinical expertise can result in potentially harmful
recommendations. While existing methods like conservative Q-learning (CQL)
attempt to address the OOD issue, their effectiveness is limited by only
constraining action selection by suppressing uncertain actions. This
action-only regularization imitates clinician actions that prioritize
short-term rewards, but it fails to regulate downstream state trajectories,
thereby limiting the discovery of improved long-term treatment strategies. To
safely improve policy beyond clinician recommendations while ensuring that
state-action trajectories remain in-distribution, we propose \textit{Offline
Guarded Safe Reinforcement Learning} ($\mathsf{OGSRL}$), a theoretically
grounded model-based offline RL framework. $\mathsf{OGSRL}$ introduces a novel
dual constraint mechanism for improving policy with reliability and safety.
First, the OOD guardian is established to specify clinically validated regions
for safe policy exploration. By constraining optimization within these regions,
it enables the reliable exploration of treatment strategies that outperform
clinician behavior by leveraging the full patient state history, without
drifting into unsupported state-action trajectories. Second, we introduce a
safety cost constraint that encodes medical knowledge about physiological
safety boundaries, providing domain-specific safeguards even in areas where
training data might contain potentially unsafe interventions. Notably, we
provide theoretical guarantees on safety and near-optimality: policies that
satisfy these constraints remain in safe and reliable regions and achieve
performance close to the best possible policy supported by the data.

</details>


### [166] [Graph Neural Network-Based Collaborative Perception for Adaptive Scheduling in Distributed Systems](https://arxiv.org/abs/2505.16248)
*Wenxuan Zhu,Qiyuan Wu,Tengda Tang,Renzi Meng,Sheng Chai,Xuehui Quan*

Key words: 分布式系统, 图神经网络, 协作感知, 动态调度, 消息传递

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了基于GNN的多节点协作感知机制，通过图结构建模和消息传递模块，提升了分布式系统中的感知效率与调度响应。

Motivation: 解决分布式系统中多节点感知局限和调度响应延迟的问题。

Method: 采用图神经网络（GNN）构建多节点协作感知机制，结合消息传递、状态更新模块及感知表示方法。

Result: 实验表明，该方法在任务完成率、延迟、负载均衡和传输效率上优于主流算法，尤其在带宽有限和动态拓扑变化时表现更佳。

Conclusion: 提出的模型具备快速收敛和高效响应能力，显著提升了系统的协作调度与感知性能。

Abstract: This paper addresses the limitations of multi-node perception and delayed
scheduling response in distributed systems by proposing a GNN-based multi-node
collaborative perception mechanism. The system is modeled as a graph structure.
Message-passing and state-update modules are introduced. A multi-layer graph
neural network is constructed to enable efficient information aggregation and
dynamic state inference among nodes. In addition, a perception representation
method is designed by fusing local states with global features. This improves
each node's ability to perceive the overall system status. The proposed method
is evaluated within a customized experimental framework. A dataset featuring
heterogeneous task loads and dynamic communication topologies is used.
Performance is measured in terms of task completion rate, average latency, load
balancing, and transmission efficiency. Experimental results show that the
proposed method outperforms mainstream algorithms under various conditions,
including limited bandwidth and dynamic structural changes. It demonstrates
superior perception capabilities and cooperative scheduling performance. The
model achieves rapid convergence and efficient responses to complex system
states.

</details>


### [167] [Small-to-Large Generalization: Data Influences Models Consistently Across Scale](https://arxiv.org/abs/2505.16260)
*Alaa Khaddaj,Logan Engstrom,Aleksander Madry*

Key words: 训练数据分布, 语言模型, 计算规模, 代理模型, 数据归因, 数据集选择

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究探讨了训练数据分布对不同规模语言模型预测行为的影响，发现小规模和大规模模型的预测高度相关，并验证了代理模型在数据归因和数据集选择中的有效性。

Motivation: 由于大规模模型训练成本高，当前实践中常用小规模代理模型来推测数据变化的影响，但数据变化对小规模和大规模模型的影响不一致，因此需要研究训练数据分布如何影响不同计算规模的模型行为。

Method: 通过实验分析小规模和大规模语言模型在不同训练数据分布下的预测相关性，并评估代理模型在数据归因和数据集选择中的应用效果。

Result: 发现小规模和大规模语言模型的预测行为高度相关，代理模型在数据归因和数据集选择任务中表现有效。

Conclusion: 训练数据分布对不同规模模型的预测行为具有一致性，代理模型可以可靠地用于推测大规模模型的行为。

Abstract: Choice of training data distribution greatly influences model behavior. Yet,
in large-scale settings, precisely characterizing how changes in training data
affects predictions is often difficult due to model training costs. Current
practice is to instead extrapolate from scaled down, inexpensive-to-train proxy
models. However, changes in data do not influence smaller and larger models
identically. Therefore, understanding how choice of data affects large-scale
models raises the question: how does training data distribution influence model
behavior across compute scale? We find that small- and large-scale language
model predictions (generally) do highly correlate across choice of training
data. Equipped with these findings, we characterize how proxy scale affects
effectiveness in two downstream proxy model applications: data attribution and
dataset selection.

</details>


### [168] [Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models](https://arxiv.org/abs/2505.16265)
*Ilgee Hong,Changlong Yu,Liang Qiu,Weixiang Yan,Zhenghao Xu,Haoming Jiang,Qingru Zhang,Qin Lu,Xin Liu,Chao Zhang,Tuo Zhao*

Key words: reinforcement learning from human feedback, reward models, chain-of-thought, long-horizon reasoning, pairwise RLHF

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Think-RM introduces a new framework for reinforcement learning from human feedback (RLHF) by enhancing generative reward models (GenRMs) with long-horizon reasoning, outperforming traditional methods like Bradley-Terry models by 8%.

Motivation: Current RLHF methods struggle with accurate reward signals due to limitations in existing reward models (BT RMs and GenRMs), which lack nuanced reasoning capabilities and compatibility with standard RLHF algorithms.

Method: Think-RM employs a warm-up phase via supervised fine-tuning on long chain-of-thought data, followed by rule-based reinforcement learning to enhance reasoning. It also introduces a pairwise RLHF pipeline to directly use pairwise preference rewards.

Result: Think-RM achieves an 8% improvement over existing methods on RM-Bench and demonstrates superior policy performance when integrated with the pairwise RLHF pipeline.

Conclusion: Think-RM addresses key limitations in RLHF by enabling advanced reasoning in reward models and optimizing policy alignment through pairwise rewards, marking a significant advancement in the field.

Abstract: Reinforcement learning from human feedback (RLHF) has become a powerful
post-training paradigm for aligning large language models with human
preferences. A core challenge in RLHF is constructing accurate reward signals,
where the conventional Bradley-Terry reward models (BT RMs) often suffer from
sensitivity to data size and coverage, as well as vulnerability to reward
hacking. Generative reward models (GenRMs) offer a more robust alternative by
generating chain-of-thought (CoT) rationales followed by a final reward.
However, existing GenRMs rely on shallow, vertically scaled reasoning, limiting
their capacity to handle nuanced or complex (e.g., reasoning-intensive) tasks.
Moreover, their pairwise preference outputs are incompatible with standard RLHF
algorithms that require pointwise reward signals. In this work, we introduce
Think-RM, a training framework that enables long-horizon reasoning in GenRMs by
modeling an internal thinking process. Rather than producing structured,
externally provided rationales, Think-RM generates flexible, self-guided
reasoning traces that support advanced capabilities such as self-reflection,
hypothetical reasoning, and divergent reasoning. To elicit these reasoning
abilities, we first warm-up the models by supervised fine-tuning (SFT) over
long CoT data. We then further improve the model's long-horizon abilities by
rule-based reinforcement learning (RL). In addition, we propose a novel
pairwise RLHF pipeline that directly optimizes policies using pairwise
preference rewards, eliminating the need for pointwise reward conversion and
enabling more effective use of Think-RM outputs. Experiments show that Think-RM
achieves state-of-the-art results on RM-Bench, outperforming both BT RM and
vertically scaled GenRM by 8%. When combined with our pairwise RLHF pipeline,
it demonstrates superior end-policy performance compared to traditional
approaches.

</details>


### [169] [Only Large Weights (And Not Skip Connections) Can Prevent the Perils of Rank Collapse](https://arxiv.org/abs/2505.16284)
*Josh Alman,Zhao Song*

Key words: 注意力机制、大权重、层崩溃、跳跃连接、表达力

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究表明，为了使注意力机制具有表达力，必须使用大权重，否则会出现层崩溃现象，而跳跃连接并不能避免这一问题。

Motivation: 现有研究表明注意力机制的计算复杂度通常是二次的，且小权重下可实现近似线性时间算法，但大权重是否对表达力至关重要尚不明确。

Method: 论文引入了层崩溃（layer collapse）概念，分析了在小权重下即使有跳跃连接，网络仍会被近似为单层网络。

Result: 证明了大权重对于避免层崩溃是必要的，而跳跃连接无法独立解决这一问题。

Conclusion: 大权重是保持注意力机制表达力的关键因素，而跳跃连接无法替代这一作用。

Abstract: Attention mechanisms lie at the heart of modern large language models (LLMs).
Straightforward algorithms for forward and backward (gradient) computation take
quadratic time, and a line of work initiated by [Alman and Song NeurIPS 2023]
and [Alman and Song NeurIPS 2024] has shown that quadratic time is necessary
unless the model weights are small, in which case almost linear time algorithms
are possible. In this paper, we show that large weights are necessary to avoid
a strong preclusion to representational strength we call layer collapse, which
means that the entire network can be approximated well by a network with only a
single layer. Thus, the quadratic running time of attention is unavoidable for
expressive transformers.
  The notion of layer collapse that we introduce is a variant on the notion of
rank collapse from the work of [Dong, Cordonnier, and Loukas ICML 2021]. They
showed that in Self Attention Networks with small weights and with skip
connections, rank collapse must occur. This is typically interpreted as
justifying the necessity of skip connections in expressive networks. However,
our result shows that even with skip connections, if the weights are small,
then layer collapse still occurs. Thus, only large weights, and not skip
connections, can prevent these representational weaknesses.

</details>


### [170] [Fairness under Competition](https://arxiv.org/abs/2505.16291)
*Ronen Gradwohl,Eilam Shapira,Moshe Tennenholtz*

Key words: 算法公平, 机器学习, 生态系统公平, 竞争性企业

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究了公平分类器在竞争性企业中的整体公平性影响，发现即使个体分类器公平，生态系统仍可能不公平，公平调整甚至可能导致整体公平性下降。

Motivation: 算法公平已成为机器学习中的核心问题，但当前研究主要关注单一分类器的公平性，忽视了其在生态系统中的整体影响。

Method: 引入竞争性企业的公平性研究框架，分析分类器的相关性和数据重叠程度对整体公平性的影响。

Result: 发现个体公平分类器未必能保证生态系统公平，且公平调整可能降低整体公平性。并通过实验验证了理论结果。

Conclusion: 呼吁关注生态系统层面的公平性，提出需要重新审视公平分类器的设计和实现。

Abstract: Algorithmic fairness has emerged as a central issue in ML, and it has become
standard practice to adjust ML algorithms so that they will satisfy fairness
requirements such as Equal Opportunity. In this paper we consider the effects
of adopting such fair classifiers on the overall level of ecosystem fairness.
Specifically, we introduce the study of fairness with competing firms, and
demonstrate the failure of fair classifiers in yielding fair ecosystems. Our
results quantify the loss of fairness in systems, under a variety of
conditions, based on classifiers' correlation and the level of their data
overlap. We show that even if competing classifiers are individually fair, the
ecosystem's outcome may be unfair; and that adjusting biased algorithms to
improve their individual fairness may lead to an overall decline in ecosystem
fairness. In addition to these theoretical results, we also provide supporting
experimental evidence. Together, our model and results provide a novel and
essential call for action.

</details>


### [171] [Large-Scale Bayesian Tensor Reconstruction: An Approximate Message Passing Solution](https://arxiv.org/abs/2505.16305)
*Bingyang Cheng,Zhongtao Chen,Yichen Jin,Hao Zhang,Chen Zhang,Edmud Y. Lam,Yik-Chung Wu*

Key words: 张量分解,贝叶斯方法,GAMP,期望最大化,计算效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为CP-GAMP的可扩展贝叶斯张量分解算法，通过避免矩阵求逆来加速计算，并在实验中显著减少了运行时间。

Motivation: 现有的贝叶斯张量分解方法在处理大规模张量时因高维矩阵求逆导致计算效率低下，作者旨在通过更高效的方法解决这一问题。

Method: CP-GAMP算法利用广义近似消息传递（GAMP）避免矩阵求逆，并结合期望最大化（EM）方法推断张量秩和噪声功率。

Result: 在100x100x100秩20的张量实验中，仅观察20%元素时，新算法比现有方法减少82.7%的运行时间，且重构精度相当。

Conclusion: CP-GAMP是一种高效且精度可靠的贝叶斯张量分解方法，特别适用于大规模数据。

Abstract: Tensor CANDECOMP/PARAFAC decomposition (CPD) is a fundamental model for
tensor reconstruction. Although the Bayesian framework allows for principled
uncertainty quantification and automatic hyperparameter learning, existing
methods do not scale well for large tensors because of high-dimensional matrix
inversions. To this end, we introduce CP-GAMP, a scalable Bayesian CPD
algorithm. This algorithm leverages generalized approximate message passing
(GAMP) to avoid matrix inversions and incorporates an expectation-maximization
routine to jointly infer the tensor rank and noise power. Through multiple
experiments, for synthetic 100x100x100 rank 20 tensors with only 20% elements
observed, the proposed algorithm reduces runtime by 82.7% compared to the
state-of-the-art variational Bayesian CPD method, while maintaining comparable
reconstruction accuracy.

</details>


### [172] [CAIFormer: A Causal Informed Transformer for Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.16308)
*Xingyu Zhang,Wenwen Qiang,Siyu Zhao,Huijie Guo,Jiangmeng Li,Chuxiong Sun,Changwen Zheng*

Key words: 多变量时间序列预测、结构因果模型、虚假相关性、CAIFormer

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种基于因果关系的多变量时间序列预测新范式（all-to-one），通过分离因果相关和虚假相关性信息，设计了CAIFormer模型，实验证明了其有效性。

Motivation: 现有多变量时间序列预测方法未区分变量的因果作用，导致模型可能混杂虚假相关性。为解决这一问题，论文提出通过因果结构分离变量角色，提升预测准确性和可解释性。

Method: 1. 基于观测数据构建结构因果模型；2. 根据因果结构将历史序列分为内生、直接因果、共因因果和虚假相关性四类子段；3. 仅使用前三类因果相关子段进行预测；4. 提出CAIFormer模型，包含三个分别处理上述子段的模块，并整合输出最终预测。

Result: 在多个基准数据集上的实验表明，CAIFormer能有效区分因果信息并显著提升预测性能。

Conclusion: 通过因果驱动的方法可显著减少虚假相关性干扰，CAIFormer为多变量时间序列预测提供了更可靠的框架。

Abstract: Most existing multivariate time series forecasting methods adopt an
all-to-all paradigm that feeds all variable histories into a unified model to
predict their future values without distinguishing their individual roles.
However, this undifferentiated paradigm makes it difficult to identify
variable-specific causal influences and often entangles causally relevant
information with spurious correlations. To address this limitation, we propose
an all-to-one forecasting paradigm that predicts each target variable
separately. Specifically, we first construct a Structural Causal Model from
observational data and then, for each target variable, we partition the
historical sequence into four sub-segments according to the inferred causal
structure: endogenous, direct causal, collider causal, and spurious
correlation. The prediction relies solely on the first three causally relevant
sub-segments, while the spurious correlation sub-segment is excluded.
Furthermore, we propose Causal Informed Transformer (CAIFormer), a novel
forecasting model comprising three components: Endogenous Sub-segment
Prediction Block, Direct Causal Sub-segment Prediction Block, and Collider
Causal Sub-segment Prediction Block, which process the endogenous, direct
causal, and collider causal sub-segments, respectively. Their outputs are then
combined to produce the final prediction. Extensive experiments on multiple
benchmark datasets demonstrate the effectiveness of the CAIFormer.

</details>


### [173] [FreshRetailNet-50K: A Stockout-Annotated Censored Demand Dataset for Latent Demand Recovery and Forecasting in Fresh Retail](https://arxiv.org/abs/2505.16319)
*Yangyang Wang,Jiawei Gu,Li Long,Xin Li,Li Shen,Zhouyu Fu,Xiangjun Zhou,Xu Jiang*

Key words: 需求估计,删失数据,零售AI,库存优化,因果分析

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了FreshRetailNet-50K，首个大规模的需求估计基准数据集，通过精确的每小时销售数据和库存状态标注解决了现有数据在缺货时的删失效应问题，并展示了两阶段需求建模方法的优越性。

Motivation: 准确的需求估计对零售业至关重要，但现有数据缺乏时间分辨率和标注，无法有效解决缺货时的需求删失问题。

Method: 提出两阶段需求建模方法：首先利用每小时库存状态标注重构缺货期间的潜在需求，然后利用这些数据训练鲁棒的需求预测模型。

Result: 实验显示，该方法预测精度提高了2.73%，并将系统性需求低估偏差从7.37%降至接近于零。

Conclusion: FreshRetailNet-50K数据集的开源为零售AI领域的需求插补、库存优化和因果分析提供了新的研究平台。

Abstract: Accurate demand estimation is critical for the retail business in guiding the
inventory and pricing policies of perishable products. However, it faces
fundamental challenges from censored sales data during stockouts, where
unobserved demand creates systemic policy biases. Existing datasets lack the
temporal resolution and annotations needed to address this censoring effect. To
fill this gap, we present FreshRetailNet-50K, the first large-scale benchmark
for censored demand estimation. It comprises 50,000 store-product time series
of detailed hourly sales data from 898 stores in 18 major cities, encompassing
863 perishable SKUs meticulously annotated for stockout events. The hourly
stock status records unique to this dataset, combined with rich contextual
covariates, including promotional discounts, precipitation, and temporal
features, enable innovative research beyond existing solutions. We demonstrate
one such use case of two-stage demand modeling: first, we reconstruct the
latent demand during stockouts using precise hourly annotations. We then
leverage the recovered demand to train robust demand forecasting models in the
second stage. Experimental results show that this approach achieves a 2.73\%
improvement in prediction accuracy while reducing the systematic demand
underestimation from 7.37\% to near-zero bias. With unprecedented temporal
granularity and comprehensive real-world information, FreshRetailNet-50K opens
new research directions in demand imputation, perishable inventory
optimization, and causal retail analytics. The unique annotation quality and
scale of the dataset address long-standing limitations in retail AI, providing
immediate solutions and a platform for future methodological innovation. The
data (https://huggingface.co/datasets/Dingdong-Inc/FreshRetailNet-50K) and code
(https://github.com/Dingdong-Inc/frn-50k-baseline}) are openly released.

</details>


### [174] [AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners](https://arxiv.org/abs/2505.16322)
*Woosung Koh,Wonbeen Oh,Jaein Jang,MinHyung Lee,Hyeongjin Kim,Ah Yeon Kim,Joonkee Kim,Junghyun Lee,Taehyeon Kim,Se-Young Yun*

Key words: 自适应采样, 语言模型, 自我训练, AdaSTaR, 训练效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: AdaSTaR是一种改进的自我训练推理算法，通过自适应采样提升训练效率和模型性能。

Motivation: 现有的自我训练机制（如STaR）通过随机采样数据训练语言模型，但这种方法会导致训练样本不平衡：已解决样本训练过度，而困难样本训练不足。

Method: 提出了AdaSTaR算法，整合两种自适应采样原则：(1)多样性自适应采样，促进样本平衡；(2)课程自适应采样，动态调整数据难度以适应模型能力。

Result: 在六个基准测试中，AdaSTaR在所有情况下均取得最佳测试准确率（6/6），训练FLOPs平均减少58.6%。

Conclusion: AdaSTaR显著提升了自我训练语言模型的性能和效率，适用于不同预训练模型和更大规模模型。

Abstract: Self-Taught Reasoners (STaR), synonymously known as Rejection sampling
Fine-Tuning (RFT), is an integral part of the training pipeline of
self-improving reasoning Language Models (LMs). The self-improving mechanism
often employs random observation (data) sampling. However, this results in
trained observation imbalance; inefficiently over-training on solved examples
while under-training on challenging ones. In response, we introduce Adaptive
STaR (AdaSTaR), a novel algorithm that rectifies this by integrating two
adaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting
balanced training across observations, and (2) Adaptive Sampling for
Curriculum: dynamically adjusting data difficulty to match the model's evolving
strength. Across six benchmarks, AdaSTaR achieves best test accuracy in all
instances (6/6) and reduces training FLOPs by an average of 58.6% against an
extensive list of baselines. These improvements in performance and efficiency
generalize to different pre-trained LMs and larger models, paving the way for
more efficient and effective self-improving LMs.

</details>


### [175] [ChemMLLM: Chemical Multimodal Large Language Model](https://arxiv.org/abs/2505.16326)
*Qian Tan,Dongzhan Zhou,Peng Xia,Wanhao Liu,Wanli Ouyang,Lei Bai,Yuqiang Li,Tianfan Fu*

Key words: 多模态大语言模型, 化学, 分子生成, 分子理解, SMILES

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: ChemMLLM 是一种用于分子理解与生成的多模态大语言模型，填补了化学领域多模态模型的空白，并在五项任务中表现优于现有模型。

Motivation: 当前化学领域的多模态大语言模型（MLLMs）在跨模态理解和生成方面研究不足，ChemMLLM 旨在填补这一空白。

Method: 设计了涵盖文本、分子 SMILES 字符串和图像的五项多模态任务，并构建数据集，对比 ChemMLLM 与现有通用 MLLMs 及化学 LLMs 的性能。

Result: ChemMLLM 在所有任务中表现优于基线模型，例如在分子图像优化任务中比 GPT-4o 的性能提升 118.9%。

Conclusion: ChemMLLM 是化学领域多模态任务的有效解决方案，代码已开源。

Abstract: Multimodal large language models (MLLMs) have made impressive progress in
many applications in recent years. However, chemical MLLMs that can handle
cross-modal understanding and generation remain underexplored. To fill this
gap, in this paper, we propose ChemMLLM, a unified chemical multimodal large
language model for molecule understanding and generation. Also, we design five
multimodal tasks across text, molecular SMILES strings, and image, and curate
the datasets. We benchmark ChemMLLM against a range of general leading MLLMs
and Chemical LLMs on these tasks. Experimental results show that ChemMLLM
achieves superior performance across all evaluated tasks. For example, in
molecule image optimization task, ChemMLLM outperforms the best baseline
(GPT-4o) by 118.9\% (4.27 vs 1.95 property improvement). The code is publicly
available at https://github.com/bbsbz/ChemMLLM.git.

</details>


### [176] [Understanding Differential Transformer Unchains Pretrained Self-Attentions](https://arxiv.org/abs/2505.16333)
*Chaerin Kong,Jiho Jang,Nojun Kwak*

Key words: Differential Transformer, DEX, 预训练语言模型, 差异注意力, 轻量级适配

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文深入研究了Differential Transformer的成功因素，提出了DEX方法，将差异注意力的优势高效集成到预训练语言模型中，显著提升了性能。

Motivation: Differential Transformer虽有显著性能表现，但其成功机制不明且依赖从头训练，限制了预训练权重的利用。

Method: 提出DEX方法，通过重用softmax注意力分数并在输出值矩阵上添加轻量级差异操作，高效整合差异注意力优势。

Result: DEX显著提升了预训练LLM在多种基准测试中的性能，仅需极少适配数据（<0.01%）。

Conclusion: DEX成功融合差异注意力的关键优势，同时保持轻量级训练与推理。

Abstract: Differential Transformer has recently gained significant attention for its
impressive empirical performance, often attributed to its ability to perform
noise canceled attention. However, precisely how differential attention
achieves its empirical benefits remains poorly understood. Moreover,
Differential Transformer architecture demands large-scale training from
scratch, hindering utilization of open pretrained weights. In this work, we
conduct an in-depth investigation of Differential Transformer, uncovering three
key factors behind its success: (1) enhanced expressivity via negative
attention, (2) reduced redundancy among attention heads, and (3) improved
learning dynamics. Based on these findings, we propose DEX, a novel method to
efficiently integrate the advantages of differential attention into pretrained
language models. By reusing the softmax attention scores and adding a
lightweight differential operation on the output value matrix, DEX effectively
incorporates the key advantages of differential attention while remaining
lightweight in both training and inference. Evaluations confirm that DEX
substantially improves the pretrained LLMs across diverse benchmarks, achieving
significant performance gains with minimal adaptation data (< 0.01\%).

</details>


### [177] [Improving Chemical Understanding of LLMs via SMILES Parsing](https://arxiv.org/abs/2505.16340)
*Yunhui Jang,Jaehyung Kim,Sungsoo Ahn*

Key words: 大语言模型、SMILES、分子科学、CLEANMOL、结构理解

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 为了解决当前大语言模型（LLMs）在解析分子SMILES表示上的困难，作者提出了CLEANMOL框架。该框架通过设计一系列确定性任务提升分子理解能力，实验表明其在结构理解和基准测试中表现优异。

Motivation: 现有LLMs在解析SMILES表示时表现不佳，无法完成基本任务（如计数分子环），限制了其在分子科学中的应用。因此，需要一种新方法来提升其对分子结构的理解能力。

Method: 提出了CLEANMOL框架，将SMILES解析转化为一系列确定性任务（如子图匹配和全局图匹配），利用分子预训练数据集和自适应难度评分预训练开源LLMs。

Result: CLEANMOL显著提升了模型的结构理解能力，并在Mol-Instructions基准测试中达到或超越基线性能。

Conclusion: CLEANMOL是一种有效的框架，能够增强LLMs对分子结构的理解，为分子科学中的LLMs应用提供了新方向。

Abstract: Large language models (LLMs) are increasingly recognized as powerful tools
for scientific discovery, particularly in molecular science. A fundamental
requirement for these models is the ability to accurately understand molecular
structures, commonly encoded in the SMILES representation. However, current
LLMs struggle to interpret SMILES, even failing to carry out basic tasks such
as counting molecular rings. To address this limitation, we introduce CLEANMOL,
a novel framework that formulates SMILES parsing into a suite of clean and
deterministic tasks explicitly designed to promote graph-level molecular
comprehension. These tasks span from subgraph matching to global graph
matching, providing structured supervision aligned with molecular structural
properties. We construct a molecular pretraining dataset with adaptive
difficulty scoring and pre-train open-source LLMs on these tasks. Our results
show that CLEANMOL not only enhances structural comprehension but also achieves
the best or competes with the baseline on the Mol-Instructions benchmark.

</details>


### [178] [A Square Peg in a Square Hole: Meta-Expert for Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2505.16341)
*Yaxin Hou,Yuheng Jia*

Key words: 长尾半监督学习, 分布不匹配, 动态专家分配, 特征融合

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本研究提出了一种动态专家分配模块和多深度特征融合模块，用于解决长尾半监督学习中的分布不匹配问题，有效提升了模型性能。

Motivation: 研究动机是解决长尾半监督学习中标记数据与未标记数据分布不匹配的问题，现有方法未能充分利用不同专家的优势。

Method: 方法包括动态专家分配模块（根据样本类别动态选择专家）和多深度特征融合模块（结合不同深度特征以减少模型偏差）。

Result: 在CIFAR-10-LT、STL-10-LT和SVHN-LT数据集上的实验证明了方法的有效性。

Conclusion: 通过动态利用专家优势和特征融合，论文方法在长尾半监督学习中表现优异。

Abstract: This paper studies the long-tailed semi-supervised learning (LTSSL) with
distribution mismatch, where the class distribution of the labeled training
data follows a long-tailed distribution and mismatches with that of the
unlabeled training data. Most existing methods introduce auxiliary classifiers
(experts) to model various unlabeled data distributions and produce
pseudo-labels, but the expertises of various experts are not fully utilized. We
observe that different experts are good at predicting different intervals of
samples, e.g., long-tailed expert is skilled in samples located in the head
interval and uniform expert excels in samples located in the medium interval.
Therefore, we propose a dynamic expert assignment module that can estimate the
class membership (i.e., head, medium, or tail class) of samples, and
dynamically assigns suitable expert to each sample based on the estimated
membership to produce high-quality pseudo-label in the training phase and
produce prediction in the testing phase. We also theoretically reveal that
integrating different experts' strengths will lead to a smaller generalization
error bound. Moreover, we find that the deeper features are more biased toward
the head class but with more discriminative ability, while the shallower
features are less biased but also with less discriminative ability. We,
therefore, propose a multi-depth feature fusion module to utilize different
depth features to mitigate the model bias. Our method demonstrates its
effectiveness through comprehensive experiments on the CIFAR-10-LT, STL-10-LT,
and SVHN-LT datasets across various settings. The code is available at
https://github.com/yaxinhou/Meta-Expert.

</details>


### [179] [Arrival Control in Quasi-Reversible Queueing Systems: Optimization and Reinforcement Learning](https://arxiv.org/abs/2505.16353)
*Céline Comte,Pascal Moyal*

Key words: 准可逆排队系统，平衡到达控制，Whittle网络，稳态测度，优化，强化学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种通用的准可逆排队系统到达率优化方案，重新定义了准可逆性，并提出平衡到达控制策略，证明其能保持准可逆性。

Motivation: 通过重新定义准可逆性并提出平衡到达控制策略，优化准可逆排队系统的到达率。

Method: 提出平衡到达控制策略，并证明其能保持准可逆性；利用优化和强化学习框架解决准入控制问题。

Result: 平衡到达控制策略成功保持准可逆性，并得出了稳态测度的具体形式。

Conclusion: 该方法适用于多种准可逆排队系统，如Whittle网络和顺序无关队列，并在优化和强化学习中具有应用潜力。

Abstract: In this paper, we introduce a versatile scheme for optimizing the arrival
rates of quasi-reversible queueing systems. We first propose an alternative
definition of quasi-reversibility that encompasses reversibility and highlights
the importance of the definition of customer classes. In a second time, we
introduce balanced arrival control policies, which generalize the notion of
balanced arrival rates introduced in the context of Whittle networks, to the
much broader class of quasi-reversible queueing systems. We prove that
supplementing a quasi-reversible queueing system with a balanced
arrival-control policy preserves the quasi-reversibility, and we specify the
form of the stationary measures. We revisit two canonical examples of
quasi-reversible queueing systems, Whittle networks and order-independent
queues. Lastly, we focus on the problem of admission control and leverage our
results in the frameworks of optimization and reinforcement learning.

</details>


### [180] [AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training](https://arxiv.org/abs/2505.16363)
*Huishuai Zhang,Bohan Wang,Luoxin Chen*

Key words: AdamS, 优化算法, 大型语言模型, 预训练, 微调

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: AdamS是一种简单高效的优化算法，替代Adam用于大型语言模型的预训练和微调，通过新颖的权重机制消除了二阶矩估计的需求，性能和效率均优于SGD和AdamW。

Motivation: 动机源于Transformer目标的$(L_0, L_1)$平滑性特性，局部平滑性由梯度大小决定，而梯度大小可通过动量大小近似估计。

Method: 使用动量与当前梯度平方加权和的平方根作为新分母，无需二阶矩估计，继承了AdamW的超参数，模型无关且与现有流程无缝集成。

Result: 在GPT-2和Llama2（13B参数）的预训练及强化学习微调中表现优异，理论收敛性得到严格证明。

Conclusion: AdamS以其高效、简单和理论支持，成为现有优化器的有力替代方案。

Abstract: We introduce AdamS, a simple yet effective alternative to Adam for large
language model (LLM) pretraining and post-training. By leveraging a novel
denominator, i.e., the root of weighted sum of squares of the momentum and the
current gradient, AdamS eliminates the need for second-moment estimates. Hence,
AdamS is efficient, matching the memory and compute footprint of SGD with
momentum while delivering superior optimization performance. Moreover, AdamS is
easy to adopt: it can directly inherit hyperparameters of AdamW, and is
entirely model-agnostic, integrating seamlessly into existing pipelines without
modifications to optimizer APIs or architectures. The motivation behind AdamS
stems from the observed $(L_0, L_1)$ smoothness properties in transformer
objectives, where local smoothness is governed by gradient magnitudes that can
be further approximated by momentum magnitudes. We establish rigorous
theoretical convergence guarantees and provide practical guidelines for
hyperparameter selection. Empirically, AdamS demonstrates strong performance in
various tasks, including pre-training runs on GPT-2 and Llama2 (up to 13B
parameters) and reinforcement learning in post-training regimes. With its
efficiency, simplicity, and theoretical grounding, AdamS stands as a compelling
alternative to existing optimizers.

</details>


### [181] [A collaborative constrained graph diffusion model for the generation of realistic synthetic molecules](https://arxiv.org/abs/2505.16365)
*Manuel Ruiz-Botella,Marta Sales-Pardo,Roger Guimerà*

Key words: 分子生成, 图扩散模型, 化学有效性, 协作机制

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: CoCoGraph是一种协作且受限的图扩散模型，用于生成化学有效的分子，性能优于现有方法。

Motivation: 解决分子空间中探索新分子的挑战，尤其是在健康和可持续性领域的需求。

Method: 采用协作和受限的图扩散模型（CoCoGraph），确保生成的分子化学有效。

Result: CoCoGraph在标准基准测试中表现优异，参数更少，生成的分子分布更接近真实分子。

Conclusion: CoCoGraph高效且实用，生成了820万个分子并进行了专家评估，验证了其可行性。

Abstract: Developing new molecular compounds is crucial to address pressing challenges,
from health to environmental sustainability. However, exploring the molecular
space to discover new molecules is difficult due to the vastness of the space.
Here we introduce CoCoGraph, a collaborative and constrained graph diffusion
model capable of generating molecules that are guaranteed to be chemically
valid. Thanks to the constraints built into the model and to the collaborative
mechanism, CoCoGraph outperforms state-of-the-art approaches on standard
benchmarks while requiring up to an order of magnitude fewer parameters.
Analysis of 36 chemical properties also demonstrates that CoCoGraph generates
molecules with distributions more closely matching real molecules than current
models. Leveraging the model's efficiency, we created a database of 8.2M
million synthetically generated molecules and conducted a Turing-like test with
organic chemistry experts to further assess the plausibility of the generated
molecules, and potential biases and limitations of CoCoGraph.

</details>


### [182] [SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning](https://arxiv.org/abs/2505.16368)
*Huanyu Liu,Jia Li,Hao Zhu,Kechi Zhang,Yihong Dong,Ge Li*

Key words: 强化学习,大语言模型,SAT问题,课程学习,推理能力

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出Saturn框架，利用SAT问题训练和评估LLM的推理能力，解决了现有RL任务的可扩展性、可验证性和难度控制问题，并在多个任务上取得了显著提升。

Motivation: 现有RL任务在训练LLM推理能力时存在可扩展性、可验证性和难度控制的问题，Saturn框架旨在解决这些问题。

Method: 提出Saturn框架，基于SAT问题设计课程学习流水线，逐步提升LLM的推理能力，并通过规则验证和精确难度控制实现稳定训练。

Result: Saturn在SAT问题、数学和编程任务上均取得显著改进，优于现有方法。

Conclusion: Saturn框架为LLM推理训练提供了高效、可控的解决方案，实验结果表明其在多任务上的优越性。

Abstract: How to design reinforcement learning (RL) tasks that effectively unleash the
reasoning capability of large language models (LLMs) remains an open question.
Existing RL tasks (e.g., math, programming, and constructing reasoning tasks)
suffer from three key limitations: (1) Scalability. They rely heavily on human
annotation or expensive LLM synthesis to generate sufficient training data. (2)
Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3)
Controllable Difficulty. Most tasks lack fine-grained difficulty control,
making it hard to train LLMs to develop reasoning ability from easy to hard.
  To address these limitations, we propose Saturn, a SAT-based RL framework
that uses Boolean Satisfiability (SAT) problems to train and evaluate LLM
reasoning. Saturn enables scalable task construction, rule-based verification,
and precise difficulty control. Saturn designs a curriculum learning pipeline
that continuously improves LLMs' reasoning capability by constructing SAT tasks
of increasing difficulty and training LLMs from easy to hard. To ensure stable
training, we design a principled mechanism to control difficulty transitions.
  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying
difficulty. It supports the evaluation of how LLM reasoning changes with
problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain
Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT
problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of
+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B
and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,
AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in
constructing RL tasks, Saturn achieves further improvements of +8.8%. We
release the source code, data, and models to support future research.

</details>


### [183] [Omni TM-AE: A Scalable and Interpretable Embedding Model Using the Full Tsetlin Machine State Space](https://arxiv.org/abs/2505.16386)
*Ahmed K. Kadhim,Lei Jiao,Rishad Shafik,Ole-Christoffer Granmo*

Key words: 可解释性, 嵌入模型, Tsetlin Machine, NLP, Omni TM-AE

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出Omni Tsetlin Machine AutoEncoder (Omni TM-AE)，一种新型嵌入模型，通过利用TM状态矩阵的全部信息，实现可重用、可解释的嵌入，在多个NLP任务中表现优异。

Motivation: 随着大规模语言模型复杂度提升，其可解释性和可重用性问题日益突出。传统嵌入模型（如Word2Vec、GloVe）缺乏透明度，而可解释模型（如TM）在扩展性上受限。论文旨在兼顾性能、扩展性与可解释性。

Method: 提出Omni TM-AE模型，充分利用TM状态矩阵信息（包括此前被忽略的文字），通过单次训练生成可重用、可解释的嵌入表示。

Result: 在语义相似性、情感分类和文档聚类任务中，Omni TM-AE表现优于主流嵌入模型。

Conclusion: 研究表明，Omni TM-AE在不依赖黑盒架构的情况下，能平衡NLP系统的性能、扩展性与可解释性。

Abstract: The increasing complexity of large-scale language models has amplified
concerns regarding their interpretability and reusability. While traditional
embedding models like Word2Vec and GloVe offer scalability, they lack
transparency and often behave as black boxes. Conversely, interpretable models
such as the Tsetlin Machine (TM) have shown promise in constructing explainable
learning systems, though they previously faced limitations in scalability and
reusability. In this paper, we introduce Omni Tsetlin Machine AutoEncoder (Omni
TM-AE), a novel embedding model that fully exploits the information contained
in the TM's state matrix, including literals previously excluded from clause
formation. This method enables the construction of reusable, interpretable
embeddings through a single training phase. Extensive experiments across
semantic similarity, sentiment classification, and document clustering tasks
show that Omni TM-AE performs competitively with and often surpasses mainstream
embedding models. These results demonstrate that it is possible to balance
performance, scalability, and interpretability in modern Natural Language
Processing (NLP) systems without resorting to opaque architectures.

</details>


### [184] [AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning](https://arxiv.org/abs/2505.16400)
*Yang Chen,Zhuolin Yang,Zihan Liu,Chankyu Lee,Peng Xu,Mohammad Shoeybi,Bryan Catanzaro,Wei Ping*

Key words: 强化学习,推理能力,数据筛选,分阶段训练,中小规模模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究表明，大规模强化学习能显著提升中小规模模型的推理能力，超越基于蒸馏的方法。通过数学和代码提示的分阶段训练，以及高质量数据集的构建，模型在数学和代码任务上表现优异。

Motivation: 尽管大规模强化学习（RL）在推理领域取得进展，但如何训练高性能推理模型的细节仍不明确。现有研究多忽略数据筛选和RL训练方法，且对小模型而言蒸馏更有效。本研究旨在验证RL对小规模模型的潜力。

Method: 采用分阶段RL训练，先数学提示后代码提示。构建高质量数据集，并验证基于RL的迭代训练效果。研究不同训练策略，如渐进式学习长度和策略参数更新的稳定性。

Result: 数学RL显著提升模型在数学（AIME 2025提升14.6%~17.2%）和代码任务（LiveCodeBench提升5.8%~6.8%）的表现。代码RL进一步优化代码能力且不影响数学结果。

Conclusion: RL能激发模型的推理潜力，解决此前无法解决的问题，同时支持分阶段训练和高质量数据集对提升性能的关键作用。

Abstract: Despite recent progress in large-scale reinforcement learning (RL) for
reasoning, the training recipe for building high-performing reasoning models
remains elusive. Key implementation details of frontier models, such as
DeepSeek-R1, including data curation strategies and RL training recipe, are
often omitted. Moreover, recent research indicates distillation remains more
effective than RL for smaller models. In this work, we demonstrate that
large-scale RL can significantly enhance the reasoning capabilities of strong,
small- and mid-sized models, achieving results that surpass those of
state-of-the-art distillation-based models. We systematically study the RL
training process through extensive ablations and propose a simple yet effective
approach: first training on math-only prompts, then on code-only prompts.
Notably, we find that math-only RL not only significantly enhances the
performance of strong distilled models on math benchmarks (e.g., +14.6% /
+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks
(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,
extended code-only RL iterations further improve performance on code benchmarks
with minimal or no degradation in math results. We develop a robust data
curation pipeline to collect challenging prompts with high-quality, verifiable
answers and test cases to enable verification-based RL across both domains.
Finally, we identify key experimental insights, including curriculum learning
with progressively increasing response lengths and the stabilizing effect of
on-policy parameter updates. We find that RL not only elicits the foundational
reasoning capabilities acquired during pretraining and supervised fine-tuning
(e.g., distillation), but also pushes the limits of the model's reasoning
ability, enabling it to solve problems that were previously unsolvable.

</details>


### [185] [Divide-Fuse-Conquer: Eliciting "Aha Moments" in Multi-Scenario Games](https://arxiv.org/abs/2505.16401)
*Xiaoqing Zhang,Huabin Zheng,Ang Lv,Yuhan Liu,Zirui Song,Flood Sung,Xiuying Chen,Rui Yan*

Key words: 大型语言模型、强化学习、多场景游戏、泛化能力、Divide-Fuse-Conquer

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了Divide-Fuse-Conquer方法以改进大型语言模型在多场景强化学习中的泛化能力，通过在分组训练后融合模型参数提升性能，实验显示其效果接近Claude3.5。

Motivation: 多场景游戏的复杂性常导致模型泛化能力不足，直接组合多场景训练会引发不稳定和性能下降，需创新方法解决。

Method: Divide-Fuse-Conquer框架：先启发式分组训练专用模型，再融合参数并持续训练以覆盖所有组。

Result: 在18个TextArena游戏中，Qwen2.5-32B-Align采用该方法达到与Claude3.5相当的水平（7胜4平）。

Conclusion: 该框架为利用强化学习提升LLM泛化能力提供了新思路，有望启发未来研究。

Abstract: Large language models (LLMs) have been observed to suddenly exhibit advanced
reasoning abilities during reinforcement learning (RL), resembling an ``aha
moment'' triggered by simple outcome-based rewards. While RL has proven
effective in eliciting such breakthroughs in tasks involving mathematics,
coding, and vision, it faces significant challenges in multi-scenario games.
The diversity of game rules, interaction modes, and environmental complexities
often leads to policies that perform well in one scenario but fail to
generalize to others. Simply combining multiple scenarios during training
introduces additional challenges, such as training instability and poor
performance. To overcome these challenges, we propose Divide-Fuse-Conquer, a
framework designed to enhance generalization in multi-scenario RL. This
approach starts by heuristically grouping games based on characteristics such
as rules and difficulties. Specialized models are then trained for each group
to excel at games in the group is what we refer to as the divide step. Next, we
fuse model parameters from different groups as a new model, and continue
training it for multiple groups, until the scenarios in all groups are
conquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align
trained with the Divide-Fuse-Conquer strategy reaches a performance level
comparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can
inspire future research on using reinforcement learning to improve the
generalization of LLMs.

</details>


### [186] [Performance Guaranteed Poisoning Attacks in Federated Learning: A Sliding Mode Approach](https://arxiv.org/abs/2505.16403)
*Huazi Pan,Yanjun Zhang,Leo Yu Zhang,Scott Adams,Abbas Kouzani,Suiyang Khoo*

Key words: 联邦学习, 投毒攻击, 滑动模式控制, 隐蔽攻击, 模型安全

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为FedSA的新型联邦学习攻击方法，旨在精确控制投毒程度，通过结合鲁棒非线性控制理论（SMC）实现隐蔽且可控的全局模型破坏。

Motivation: 现有投毒攻击多导致服务拒绝（DoS）问题，FedSA则旨在精确控制投毒影响，如将全局模型预测准确率降低特定比例（如10%），以更隐蔽的方式达成攻击目标。

Method: FedSA结合滑动模式控制（SMC）理论与模型投毒攻击，通过恶意客户端的更新操纵全局模型状态，以可控且隐蔽的速度达成预设的投毒目标。

Result: 实验表明，FedSA能以更少恶意客户端精确实现预设全局准确率，同时保持高隐蔽性和可调学习率。

Conclusion: FedSA提供了一种高效且隐蔽的联邦学习攻击方法，能精确控制投毒影响，对现有防御机制构成新挑战。

Abstract: Manipulation of local training data and local updates, i.e., the poisoning
attack, is the main threat arising from the collaborative nature of the
federated learning (FL) paradigm. Most existing poisoning attacks aim to
manipulate local data/models in a way that causes denial-of-service (DoS)
issues. In this paper, we introduce a novel attack method, named Federated
Learning Sliding Attack (FedSA) scheme, aiming at precisely introducing the
extent of poisoning in a subtle controlled manner. It operates with a
predefined objective, such as reducing global model's prediction accuracy by
10\%. FedSA integrates robust nonlinear control-Sliding Mode Control (SMC)
theory with model poisoning attacks. It can manipulate the updates from
malicious clients to drive the global model towards a compromised state,
achieving this at a controlled and inconspicuous rate. Additionally, leveraging
the robust control properties of FedSA allows precise control over the
convergence bounds, enabling the attacker to set the global accuracy of the
poisoned model to any desired level. Experimental results demonstrate that
FedSA can accurately achieve a predefined global accuracy with fewer malicious
clients while maintaining a high level of stealth and adjustable learning
rates.

</details>


### [187] [Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models](https://arxiv.org/abs/2505.16446)
*Zhaoxin Wang,Handing Wang,Cong Tian,Yaochu Jin*

Key words: 多模态大语言模型,隐式越狱,隐写术,对抗攻击,跨模态一致性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新型隐式越狱攻击框架IJA，通过隐写术将恶意指令嵌入图像，并结合看似无害的文本提示，有效绕过MLLMs的多模态一致性检测，攻击成功率超过90%。

Motivation: 随着多模态大语言模型的普及，其跨模态推理能力增强，但也引入了新的攻击面。传统显式攻击易被检测，因此需要开发更隐蔽的攻击方法。

Method: 提出IJA框架，结合隐写术在图像中嵌入恶意指令，并优化对抗性后缀和提示模板，通过迭代反馈提升攻击效果。

Result: 在GPT-4o和Gemini-1.5 Pro等商业模型上，平均仅需3次查询即可实现超过90%的攻击成功率。

Conclusion: IJA框架展示了现有MLLMs在多模态对齐中的潜在漏洞，为防御此类隐蔽攻击提供了研究方向。

Abstract: Multimodal large language models (MLLMs) enable powerful cross-modal
reasoning capabilities. However, the expanded input space introduces new attack
surfaces. Previous jailbreak attacks often inject malicious instructions from
text into less aligned modalities, such as vision. As MLLMs increasingly
incorporate cross-modal consistency and alignment mechanisms, such explicit
attacks become easier to detect and block. In this work, we propose a novel
implicit jailbreak framework termed IJA that stealthily embeds malicious
instructions into images via least significant bit steganography and couples
them with seemingly benign, image-related textual prompts. To further enhance
attack effectiveness across diverse MLLMs, we incorporate adversarial suffixes
generated by a surrogate model and introduce a template optimization module
that iteratively refines both the prompt and embedding based on model feedback.
On commercial models like GPT-4o and Gemini-1.5 Pro, our method achieves attack
success rates of over 90% using an average of only 3 queries.

</details>


### [188] [Neighbour-Driven Gaussian Process Variational Autoencoders for Scalable Structured Latent Modelling](https://arxiv.org/abs/2505.16481)
*Xinxing Shi,Xiaoyu Jiang,Mauricio A. Álvarez*

Key words: 高斯过程, 变分自编码器, 邻居驱动, 可扩展推理, 表示学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种邻居驱动的近似策略，通过利用潜在空间中的局部相邻关系，实现了可扩展的高斯过程变分自编码器（GPVAE）推理，提升了计算效率和预测性能。

Motivation: 标准GPVAE在大规模数据上的精确推理计算成本过高，现有方法通常依赖限制性核假设或大量诱导点。本文旨在解决这一问题，实现更高效和灵活的GPVAE推理。

Method: 采用邻居驱动的近似策略，通过限制每个数据点的最近邻计算，保留潜在依赖关系，减少对大量诱导点的依赖。

Result: 在表示学习、数据填补和条件生成等任务中，该方法在预测性能和计算效率上优于其他GPVAE变体。

Conclusion: 邻居驱动的近似策略为大规模GPVAE推理提供了高效且灵活的解决方案。

Abstract: Gaussian Process (GP) Variational Autoencoders (VAEs) extend standard VAEs by
replacing the fully factorised Gaussian prior with a GP prior, thereby
capturing richer correlations among latent variables. However, performing exact
GP inference in large-scale GPVAEs is computationally prohibitive, often
forcing existing approaches to rely on restrictive kernel assumptions or large
sets of inducing points. In this work, we propose a neighbour-driven
approximation strategy that exploits local adjacencies in the latent space to
achieve scalable GPVAE inference. By confining computations to the nearest
neighbours of each data point, our method preserves essential latent
dependencies, allowing more flexible kernel choices and mitigating the need for
numerous inducing points. Through extensive experiments on tasks including
representation learning, data imputation, and conditional generation, we
demonstrate that our approach outperforms other GPVAE variants in both
predictive performance and computational efficiency.

</details>


### [189] [Constrained Non-negative Matrix Factorization for Guided Topic Modeling of Minority Topics](https://arxiv.org/abs/2505.16493)
*Seyedeh Fatemeh Ebrahimi,Jaakko Peltonen*

Key words: 主题模型, 少数主题, NMF, 约束优化, 心理健康

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种通过约束NMF的方法改进主题模型，能够更好地捕捉少数主题（如心理健康内容），无需专家预先指定主题划分。

Motivation: 传统主题模型难以捕捉低频率但领域关键的少数主题，且现有方法需要过度详细的领域知识，限制了主题的多样性和划分。

Method: 采用约束NMF模型，通过种子词列表和主题流行度约束，结合KKT条件和乘法更新拟合模型。

Result: 在合成数据上表现优于基线模型（主题纯度、归一化互信息），并通过真实数据（YouTube评论）验证了其识别少数主题的能力。

Conclusion: 该方法有效解决了少数主题的捕捉问题，无需过度依赖专家知识，具有实际应用价值。

Abstract: Topic models often fail to capture low-prevalence, domain-critical themes,
so-called minority topics, such as mental health themes in online comments.
While some existing methods can incorporate domain knowledge, such as expected
topical content, methods allowing guidance may require overly detailed expected
topics, hindering the discovery of topic divisions and variation. We propose a
topic modeling solution via a specially constrained NMF. We incorporate a seed
word list characterizing minority content of interest, but we do not require
experts to pre-specify their division across minority topics. Through
prevalence constraints on minority topics and seed word content across topics,
we learn distinct data-driven minority topics as well as majority topics. The
constrained NMF is fitted via Karush-Kuhn-Tucker (KKT) conditions with
multiplicative updates. We outperform several baselines on synthetic data in
terms of topic purity, normalized mutual information, and also evaluate topic
quality using Jensen-Shannon divergence (JSD). We conduct a case study on
YouTube vlog comments, analyzing viewer discussion of mental health content;
our model successfully identifies and reveals this domain-relevant minority
content.

</details>


### [190] [Accuracy vs. Accuracy: Computational Tradeoffs Between Classification Rates and Utility](https://arxiv.org/abs/2505.16494)
*Noga Amit,Omer Reingold,Guy N. Rothblum*

Key words: fairness, utility, efficiency, classification, ranking, algorithms

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文探讨了在训练数据包含更丰富标签（如个体类型、排名或风险估计）的背景下，公平性与效用和效率的关系，并提出实现更强证据公平性的算法。

Motivation: 研究动机在于探索在数据标签更丰富的情况下如何实现更强的公平性，并研究公平性与效用、效率之间的平衡。

Method: 提出了支持分类和排名技术的算法，这些技术能保持准确的子群体分类率，并通过高效学习实现个体公平性。

Result: 展示了在某些情况下同时实现准确分类率和最优损失最小化在计算上是不可行的，但两者可分别通过高效学习实现。

Conclusion: 研究揭示了在公平性定义下准确性与损失最小化之间的计算不可调和性，提出了两种自然且可实现的公平性概念。

Abstract: We revisit the foundations of fairness and its interplay with utility and
efficiency in settings where the training data contain richer labels, such as
individual types, rankings, or risk estimates, rather than just binary
outcomes. In this context, we propose algorithms that achieve stronger notions
of evidence-based fairness than are possible in standard supervised learning.
Our methods support classification and ranking techniques that preserve
accurate subpopulation classification rates, as suggested by the underlying
data distributions, across a broad class of classification rules and downstream
applications. Furthermore, our predictors enable loss minimization, whether
aimed at maximizing utility or in the service of fair treatment.
  Complementing our algorithmic contributions, we present impossibility results
demonstrating that simultaneously achieving accurate classification rates and
optimal loss minimization is, in some cases, computationally infeasible. Unlike
prior impossibility results, our notions are not inherently in conflict and are
simultaneously satisfied by the Bayes-optimal predictor. Furthermore, we show
that each notion can be satisfied individually via efficient learning. Our
separation thus stems from the computational hardness of learning a
sufficiently good approximation of the Bayes-optimal predictor. These
computational impossibilities present a choice between two natural and
attainable notions of accuracy that could both be motivated by fairness.

</details>


### [191] [Computing Exact Shapley Values in Polynomial Time for Product-Kernel Methods](https://arxiv.org/abs/2505.16516)
*Majid Mohammadi,Siu Lun Chau,Krikamol Muandet*

Key words: 核方法, Shapley值, 可解释性, 乘积核, 统计推断

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 介绍了一种名为PKeX-Shapley的新算法，利用乘积核的结构在多项式时间内精确计算Shapley值，提高了核方法的可解释性和计算效率。

Motivation: 核方法在黑盒性质下缺乏可解释性，限制了其在关键领域的应用。现有Shapley值计算方法计算成本高，亟需高效且精确的解。

Method: 提出PKeX-Shapley算法，利用乘积核的乘法结构递归分解功能，实现多项式时间内的Shapley值精确计算。

Result: 算法不仅高效，还能推广至解释核基统计差异（如MMD和HSIC），为可解释统计推断提供新工具。

Conclusion: PKeX-Shapley为核方法提供了兼具高效性和可解释性的解决方案，适用于广泛统计任务。

Abstract: Kernel methods are widely used in machine learning due to their flexibility
and expressive power. However, their black-box nature poses significant
challenges to interpretability, limiting their adoption in high-stakes
applications. Shapley value-based feature attribution techniques, such as SHAP
and kernel-specific variants like RKHS-SHAP, offer a promising path toward
explainability. Yet, computing exact Shapley values remains computationally
intractable in general, motivating the development of various approximation
schemes. In this work, we introduce PKeX-Shapley, a novel algorithm that
utilizes the multiplicative structure of product kernels to enable the exact
computation of Shapley values in polynomial time. We show that product-kernel
models admit a functional decomposition that allows for a recursive formulation
of Shapley values. This decomposition not only yields computational efficiency
but also enhances interpretability in kernel-based learning. We also
demonstrate how our framework can be generalized to explain kernel-based
statistical discrepancies such as the Maximum Mean Discrepancy (MMD) and the
Hilbert-Schmidt Independence Criterion (HSIC), thus offering new tools for
interpretable statistical inference.

</details>


### [192] [Joint Relational Database Generation via Graph-Conditional Diffusion Models](https://arxiv.org/abs/2505.16527)
*Mohamed Amine Ketata,David Lüdke,Leo Schwinn,Stephan Günnemann*

Key words: 关系数据库生成、图神经网络、扩散模型、表间依赖

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种无固定表顺序的多表联合生成方法，基于图神经网络的关系扩散模型GRDM，显著优于序列生成方法。

Motivation: 现有生成模型多局限于单表或固定表顺序的序列生成，限制了并行性且易累积误差，需要一种更灵活的多表联合生成方法。

Method: 提出GRDM模型，利用图神经网络对RDB的自然图表示进行联合去噪，捕获表间复杂依赖关系。

Result: 在6个真实RDB上的实验表明，GRDM在多跳表间相关性建模上显著优于基线，单表保真度指标达到SOTA。

Conclusion: GRDM提供了更灵活高效的多表生成方案，突破了传统序列生成的限制。

Abstract: Building generative models for relational databases (RDBs) is important for
applications like privacy-preserving data release and augmenting real datasets.
However, most prior work either focuses on single-table generation or relies on
autoregressive factorizations that impose a fixed table order and generate
tables sequentially. This approach limits parallelism, restricts flexibility in
downstream applications like missing value imputation, and compounds errors due
to commonly made conditional independence assumptions. We propose a
fundamentally different approach: jointly modeling all tables in an RDB without
imposing any order. By using a natural graph representation of RDBs, we propose
the Graph-Conditional Relational Diffusion Model (GRDM). GRDM leverages a graph
neural network to jointly denoise row attributes and capture complex
inter-table dependencies. Extensive experiments on six real-world RDBs
demonstrate that our approach substantially outperforms autoregressive
baselines in modeling multi-hop inter-table correlations and achieves
state-of-the-art performance on single-table fidelity metrics.

</details>


### [193] [HOFT: Householder Orthogonal Fine-tuning](https://arxiv.org/abs/2505.16531)
*Alejandro Moreno Arcas,Albert Sanchis,Jorge Civera,Alfons Juan*

Key words: 正交微调,HOFT,SHOFT,模型适配,计算效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新的正交微调方法HOFT和其改进版SHOFT，解决了传统正交微调方法在时间和内存效率上的不足，并在多个下游任务中表现出色。

Motivation: 传统正交微调方法虽然具有良好的泛化性能，但时间和内存效率较低。研究旨在提出一种更高效的解决方案。

Method: 提出了HOFT和SHOFT两种正交微调方法，通过理论分析优化了正交微调的时间复杂度。

Result: 在常识推理、机器翻译、主题生成和数学推理等任务中，HOFT和SHOFT与现有最佳方法相比表现相当或更好。

Conclusion: HOFT和SHOFT在保持正交微调优势的同时，显著提升了计算效率，为模型适配提供了新思路。

Abstract: Adaptation of foundation models using low-rank methods is a widespread
approach. Another way to adapt these models is to employ orthogonal fine-tuning
methods, which are less time and memory efficient despite their good
generalization properties. In this work, we propose Householder Orthogonal
Fine-tuning (HOFT), a novel orthogonal fine-tuning method that aims to
alleviate time and space complexity. Moreover, some theoretical properties of
the orthogonal fine-tuning paradigm are explored. From this exploration, Scaled
Householder Orthogonal Fine-tuning (SHOFT) is proposed. Both HOFT and SHOFT are
evaluated in downstream tasks, namely commonsense reasoning, machine
translation, subject-driven generation and mathematical reasoning. Compared
with state-of-the-art adaptation methods, HOFT and SHOFT show comparable or
better results.

</details>


### [194] [Incremental Sequence Classification with Temporal Consistency](https://arxiv.org/abs/2505.16548)
*Lucas Maystre,Gabriel Barello,Tudor Berariu,Aleix Cambray,Rares Dolga,Alvaro Ortega Gonzalez,Andrei Nica,David Barber*

Key words: 增量序列分类, 时间一致性, 时间差分学习, 文本分类, 数学问题验证

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于时间一致性条件的增量序列分类方法，通过新损失函数提升数据效率和预测准确性，并在文本分类和数学问题验证中优于现有方法。

Motivation: 解决增量序列分类问题，利用时间一致性条件改进预测更新，提升数据效率和准确性。

Method: 结合强化学习的时间差分学习，设计时间一致性条件，并基于此提出新损失函数训练增量序列分类器。

Result: 在文本分类和数学问题验证任务中，该方法显著提升数据效率和预测准确性，尤其在早期标记观察中表现更好。

Conclusion: 所提方法通过时间一致性条件有效提升增量序列分类性能，适用于多种任务。

Abstract: We address the problem of incremental sequence classification, where
predictions are updated as new elements in the sequence are revealed. Drawing
on temporal-difference learning from reinforcement learning, we identify a
temporal-consistency condition that successive predictions should satisfy. We
leverage this condition to develop a novel loss function for training
incremental sequence classifiers. Through a concrete example, we demonstrate
that optimizing this loss can offer substantial gains in data efficiency. We
apply our method to text classification tasks and show that it improves
predictive accuracy over competing approaches on several benchmark datasets. We
further evaluate our approach on the task of verifying large language model
generations for correctness in grade-school math problems. Our results show
that models trained with our method are better able to distinguish promising
generations from unpromising ones after observing only a few tokens.

</details>


### [195] [Towards Coordinate- and Dimension-Agnostic Machine Learning for Partial Differential Equations](https://arxiv.org/abs/2505.16549)
*Trung V. Phan,George A. Kevrekidis,Soledad Villar,Yannis G. Kevrekidis,Juan M. Bello-Rivas*

Key words: PDE学习, 外微积分, 坐标无关, 维度泛化, 机器学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种基于外微积分形式的坐标无关、维度无关的PDE学习方法，能够在不同空间中无缝迁移预测。

Motivation: 现有数据驱动的PDE学习方法通常依赖特定空间维度和坐标，限制了模型的泛化能力。本文旨在解决这一问题，实现‘空间解放’的PDE学习。

Method: 采用外微积分形式的机器学习方法，预测标量场系统的演化，该方法天然支持任意维度和坐标独立性。

Result: 在多个模型（FitzHugh-Nagumo、Barkley反应-扩散模型、Patlak-Keller-Segel模型）中验证了方法的有效性，能够跨维度、坐标、边界条件和曲率预测场动态。

Conclusion: 该方法通过坐标无关表示实现了PDE学习的空间泛化，为复杂场景下的模型迁移提供了新思路。

Abstract: The machine learning methods for data-driven identification of partial
differential equations (PDEs) are typically defined for a given number of
spatial dimensions and a choice of coordinates the data have been collected in.
This dependence prevents the learned evolution equation from generalizing to
other spaces. In this work, we reformulate the problem in terms of coordinate-
and dimension-independent representations, paving the way toward what we call
``spatially liberated" PDE learning. To this end, we employ a machine learning
approach to predict the evolution of scalar field systems expressed in the
formalism of exterior calculus, which is coordinate-free and immediately
generalizes to arbitrary dimensions by construction. We demonstrate the
performance of this approach in the FitzHugh-Nagumo and Barkley
reaction-diffusion models, as well as the Patlak-Keller-Segel model informed by
in-situ chemotactic bacteria observations. We provide extensive numerical
experiments that demonstrate that our approach allows for seamless transitions
across various spatial contexts. We show that the field dynamics learned in one
space can be used to make accurate predictions in other spaces with different
dimensions, coordinate systems, boundary conditions, and curvatures.

</details>


### [196] [A Two-Stage Data Selection Framework for Data-Efficient Model Training on Edge Devices](https://arxiv.org/abs/2505.16563)
*Chen Gong,Rui Xing,Zhenzhe Zheng,Fan Wu*

Key words: 机器学习,边缘计算,数据选择,模型训练,Titan

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为Titan的两阶段数据选择框架，用于优化边缘设备上的机器学习模型训练效率和数据利用率，显著减少了训练时间并提高了准确性。

Motivation: 边缘设备上的机器学习模型训练需求增加，但当前方法存在数据利用率低、训练吞吐量不足和存储限制等问题，亟需改进。

Method: Titan框架分为粗粒度筛选和细粒度选择两个阶段，结合理论最优数据选择策略和流水线技术，同时利用空闲计算资源避免冲突。

Result: 在真实边缘设备和多种任务上测试，Titan减少了43%的训练时间，提高了6.2%的最终准确率，且系统开销极小。

Conclusion: Titan有效提升了边缘设备上的数据利用率和训练效率，为隐私保护和个性化服务提供了可行方案。

Abstract: The demand for machine learning (ML) model training on edge devices is
escalating due to data privacy and personalized service needs. However, we
observe that current on-device model training is hampered by the
under-utilization of on-device data, due to low training throughput, limited
storage and diverse data importance. To improve data resource utilization, we
propose a two-stage data selection framework {\sf Titan} to select the most
important data batch from streaming data for model training with guaranteed
efficiency and effectiveness. Specifically, in the first stage, {\sf Titan}
filters out a candidate dataset with potentially high importance in a
coarse-grained manner.In the second stage of fine-grained selection, we propose
a theoretically optimal data selection strategy to identify the data batch with
the highest model performance improvement to current training round. To further
enhance time-and-resource efficiency, {\sf Titan} leverages a pipeline to
co-execute data selection and model training, and avoids resource conflicts by
exploiting idle computing resources. We evaluate {\sf Titan} on real-world edge
devices and three representative edge computing tasks with diverse models and
data modalities. Empirical results demonstrate that {\sf Titan} achieves up to
$43\%$ reduction in training time and $6.2\%$ increase in final accuracy with
minor system overhead, such as data processing delay, memory footprint and
energy consumption.

</details>


### [197] [Finetuning-Activated Backdoors in LLMs](https://arxiv.org/abs/2505.16567)
*Thibaud Gloaguen,Mark Vero,Robin Staab,Martin Vechev*

Key words: 大型语言模型, 后门攻击, 元学习, 微调安全, FAB

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新型攻击方法FAB，通过元学习技术毒害大型语言模型（LLM），使其在看似良性的情况下，被下游用户微调后触发恶意行为。

Motivation: 揭示当前微调LLMs的安全假设漏洞，证明敌手可通过毒害模型创建隐蔽后门，挑战现有微调的安全性认知。

Method: 使用元学习技术模拟下游微调过程，优化模型在微调后展现恶意行为（如广告推送、拒绝服务、越狱能力），同时保持微调前的正常表现。

Result: FAB攻击在多类LLMs和三种目标行为（广告、拒绝、越狱）上有效，且对用户微调选择（如数据集、步数）具有鲁棒性。

Conclusion: 研究颠覆了微调安全的传统认知，揭示了LLMs复杂特性中的新攻击途径。

Abstract: Finetuning openly accessible Large Language Models (LLMs) has become standard
practice for achieving task-specific performance improvements. Until now,
finetuning has been regarded as a controlled and secure process in which
training on benign datasets led to predictable behaviors. In this paper, we
demonstrate for the first time that an adversary can create poisoned LLMs that
initially appear benign but exhibit malicious behaviors once finetuned by
downstream users. To this end, our proposed attack, FAB (Finetuning-Activated
Backdoor), poisons an LLM via meta-learning techniques to simulate downstream
finetuning, explicitly optimizing for the emergence of malicious behaviors in
the finetuned models. At the same time, the poisoned LLM is regularized to
retain general capabilities and to exhibit no malicious behaviors prior to
finetuning. As a result, when users finetune the seemingly benign model on
their own datasets, they unknowingly trigger its hidden backdoor behavior. We
demonstrate the effectiveness of FAB across multiple LLMs and three target
behaviors: unsolicited advertising, refusal, and jailbreakability.
Additionally, we show that FAB-backdoors are robust to various finetuning
choices made by the user (e.g., dataset, number of steps, scheduler). Our
findings challenge prevailing assumptions about the security of finetuning,
revealing yet another critical attack vector exploiting the complexities of
LLMs.

</details>


### [198] [Large Language Model-Empowered Interactive Load Forecasting](https://arxiv.org/abs/2505.16577)
*Yu Zuo,Dalin Qin,Yi Wang*

Key words: 负荷预测, 大语言模型, 多智能体协作, 人机交互, 电力系统

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种基于大语言模型（LLM）的多智能体协作框架，旨在解决电力系统负荷预测中由于缺乏人机交互机制导致的模型应用难题，通过自然语言交互降低技术门槛并整合人类经验，实验证明该框架能显著提升预测准确性且成本可控。

Motivation: 当前负荷预测方法静态化设计缺乏人机交互机制，系统操作员（非AI专家）难以理解和应用先进模型，也无法融入实际经验。LLM的突破为通过自然语言交互解决这一问题提供了新思路。

Method: 设计了基于LLM的多智能体协作框架：多个专用智能体分工执行预测任务，通过专用通信机制协作，并在预测流程中嵌入交互机制，支持自然语言输入以整合人类经验。

Result: 实验表明，用户在关键阶段提供适当洞察可显著提升交互式负荷预测准确性；成本分析证明框架具备实际部署的经济性。

Conclusion: 该框架通过LLM实现人机自然语言交互，降低了非专家使用门槛并提升预测性能，为电力系统智能化提供了可行方案。

Abstract: The growing complexity of power systems has made accurate load forecasting
more important than ever. An increasing number of advanced load forecasting
methods have been developed. However, the static design of current methods
offers no mechanism for human-model interaction. As the primary users of
forecasting models, system operators often find it difficult to understand and
apply these advanced models, which typically requires expertise in artificial
intelligence (AI). This also prevents them from incorporating their experience
and real-world contextual understanding into the forecasting process. Recent
breakthroughs in large language models (LLMs) offer a new opportunity to
address this issue. By leveraging their natural language understanding and
reasoning capabilities, we propose an LLM-based multi-agent collaboration
framework to bridge the gap between human operators and forecasting models. A
set of specialized agents is designed to perform different tasks in the
forecasting workflow and collaborate via a dedicated communication mechanism.
This framework embeds interactive mechanisms throughout the load forecasting
pipeline, reducing the technical threshold for non-expert users and enabling
the integration of human experience. Our experiments demonstrate that the
interactive load forecasting accuracy can be significantly improved when users
provide proper insight in key stages. Our cost analysis shows that the
framework remains affordable, making it practical for real-world deployment.

</details>


### [199] [How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning](https://arxiv.org/abs/2505.16581)
*Max Weltevrede,Moritz A. Zanger,Matthijs T. J. Spaan,Wendelin Böhmer*

Key words: 零样本策略迁移、策略蒸馏、泛化界限、集成学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文证明了在策略蒸馏后训练的泛化界限，并提出两点实用建议：训练蒸馏策略的集成和尽可能多的训练数据，实验验证了其效果优于原始策略。

Motivation: 研究零样本策略迁移中，策略蒸馏为何能提升泛化性能，以及如何通过蒸馏改进策略。

Method: 提出理论证明策略蒸馏后的泛化界限，并通过实验验证理论的实用性。

Result: 训练蒸馏策略的集成和使用更多数据能显著提升泛化能力，优于原始策略。

Conclusion: 策略蒸馏和数据集多样性是提升零样本策略迁移泛化能力的关键。

Abstract: In the zero-shot policy transfer setting in reinforcement learning, the goal
is to train an agent on a fixed set of training environments so that it can
generalise to similar, but unseen, testing environments. Previous work has
shown that policy distillation after training can sometimes produce a policy
that outperforms the original in the testing environments. However, it is not
yet entirely clear why that is, or what data should be used to distil the
policy. In this paper, we prove, under certain assumptions, a generalisation
bound for policy distillation after training. The theory provides two practical
insights: for improved generalisation, you should 1) train an ensemble of
distilled policies, and 2) distil it on as much data from the training
environments as possible. We empirically verify that these insights hold in
more general settings, when the assumptions required for the theory no longer
hold. Finally, we demonstrate that an ensemble of policies distilled on a
diverse dataset can generalise significantly better than the original agent.

</details>


### [200] [Training on Plausible Counterfactuals Removes Spurious Correlations](https://arxiv.org/abs/2505.16583)
*Shpresim Sadiku,Kartikeya Chitranshi,Hiroshi Kera,Sebastian Pokutta*

Key words: 合理反事实解释（p-CFEs）、分类器训练、偏见减少、虚假相关性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究展示了通过训练分类器使用由合理反事实解释（p-CFEs）标记的错误目标类别，可以在保持原始标签准确性的同时显著减少偏差。

Motivation: 探究如何利用合理反事实解释（p-CFEs）训练分类器，以在不损害原始标签准确性的情况下减少模型的偏见。

Method: 通过使用p-CFEs生成的扰动数据，并将其标记为错误目标类别来训练分类器。

Result: 研究结果表明，这种训练方法不仅能保持高准确率，还能显著减少模型的虚假相关性偏见。

Conclusion: 学习p-CFEs比传统的对抗扰动方法更有效，能够帮助构建更鲁棒且无偏的分类器。

Abstract: Plausible counterfactual explanations (p-CFEs) are perturbations that
minimally modify inputs to change classifier decisions while remaining
plausible under the data distribution. In this study, we demonstrate that
classifiers can be trained on p-CFEs labeled with induced \emph{incorrect}
target classes to classify unperturbed inputs with the original labels. While
previous studies have shown that such learning is possible with adversarial
perturbations, we extend this paradigm to p-CFEs. Interestingly, our
experiments reveal that learning from p-CFEs is even more effective: the
resulting classifiers achieve not only high in-distribution accuracy but also
exhibit significantly reduced bias with respect to spurious correlations.

</details>


### [201] [CausalDynamics: A large-scale benchmark for structural discovery of dynamical causal models](https://arxiv.org/abs/2505.16620)
*Benjamin Herdeanu,Juan Nathaniel,Carla Roesch,Jatan Buch,Gregor Ramien,Johannes Haux,Pierre Gentine*

Key words: 因果发现, 动态系统, 基准测试, 数据生成框架, 动态因果模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: CausalDynamics 是一个大规模基准和可扩展的数据生成框架，旨在推动动态因果模型的结构发现。论文提出了一种处理高维、强非线性时间序列数据的解决方案，并评估了现有算法的性能。

Motivation: 由于在不可干预领域中进行因果发现具有挑战性，现有方法多针对低维和弱非线性数据，难以满足需求。因此，需要一种更通用的方法来应对噪声、混杂和滞后动态的系统。

Method: 论文提出了CausalDynamics，包含数千个耦合的常微分和随机微分方程以及两个理想气候模型的真实因果图，形成基准测试，并用最先进的因果发现算法进行系统评估。

Result: 结果表明，CausalDynamics能有效支持动态因果模型的结构发现，并提供了一个可扩展的框架，适用于不同领域的独特挑战。

Conclusion: CausalDynamics为开发鲁棒的因果发现算法提供了新工具，尤其适用于复杂动态系统。

Abstract: Causal discovery for dynamical systems poses a major challenge in fields
where active interventions are infeasible. Most methods used to investigate
these systems and their associated benchmarks are tailored to deterministic,
low-dimensional and weakly nonlinear time-series data. To address these
limitations, we present CausalDynamics, a large-scale benchmark and extensible
data generation framework to advance the structural discovery of dynamical
causal models. Our benchmark consists of true causal graphs derived from
thousands of coupled ordinary and stochastic differential equations as well as
two idealized climate models. We perform a comprehensive evaluation of
state-of-the-art causal discovery algorithms for graph reconstruction on
systems with noisy, confounded, and lagged dynamics. CausalDynamics consists of
a plug-and-play, build-your-own coupling workflow that enables the construction
of a hierarchy of physical systems. We anticipate that our framework will
facilitate the development of robust causal discovery algorithms that are
broadly applicable across domains while addressing their unique challenges. We
provide a user-friendly implementation and documentation on
https://kausable.github.io/CausalDynamics.

</details>


### [202] [Multivariate Latent Recalibration for Conditional Normalizing Flows](https://arxiv.org/abs/2505.16636)
*Victor Dheur,Souhaib Ben Taieb*

Key words: 多变量模型, 概率校准, 潜在校准, 潜在重新校准, 正态化流

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为“潜在重新校准”（LR）的后处理方法，通过学习潜在空间的变换来改进多变量模型的概率校准。该方法在计算高效的同时，提供了显式的多变量密度函数。

Motivation: 现有方法在多元响应变量的联合分布校准上存在不足，尤其是无法提供完整的概率密度函数，影响了决策的可靠性。LR旨在弥补这一缺口。

Method: 引入潜在校准的新概念，并提出潜在重新校准（LR）方法，学习潜在空间的变换，确保有限样本下的校准保证。

Result: 实验表明，LR显著改善了潜在校准误差和重新校准模型的负对数似然，证明了其有效性。

Conclusion: LR提供了一种高效且可靠的多变量模型校准方法，解决了现有技术的局限性。

Abstract: Reliably characterizing the full conditional distribution of a multivariate
response variable given a set of covariates is crucial for trustworthy
decision-making. However, misspecified or miscalibrated multivariate models may
yield a poor approximation of the joint distribution of the response variables,
leading to unreliable predictions and suboptimal decisions. Furthermore,
standard recalibration methods are primarily limited to univariate settings,
while conformal prediction techniques, despite generating multivariate
prediction regions with coverage guarantees, do not provide a full probability
density function. We address this gap by first introducing a novel notion of
latent calibration, which assesses probabilistic calibration in the latent
space of a conditional normalizing flow. Second, we propose latent
recalibration (LR), a novel post-hoc model recalibration method that learns a
transformation of the latent space with finite-sample bounds on latent
calibration. Unlike existing methods, LR produces a recalibrated distribution
with an explicit multivariate density function while remaining computationally
efficient. Extensive experiments on both tabular and image datasets show that
LR consistently improves latent calibration error and the negative
log-likelihood of the recalibrated models.

</details>


### [203] [Reconsidering Fairness Through Unawareness from the Perspective of Model Multiplicity](https://arxiv.org/abs/2505.16638)
*Benedikt Höltgen,Nuria Oliver*

Key words: 公平性, 无知公平, 算法歧视, 模型多样性, 机器学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文探讨了‘无知公平’（FtU）方法是否能在不降低准确性的情况下减少算法歧视，并从理论和实证角度证明其有效性。

Motivation: 研究FtU是否能在不损害准确性的前提下减少算法歧视，并解决机器学习领域对FtU的常见批评。

Method: 通过理论分析和实证研究，结合模型多样性（Model Multiplicity）文献，验证FtU的效果。

Result: 实证表明FtU可以减少歧视且不必然降低准确性，并在实际应用中促进更公平政策的实施。

Conclusion: FtU值得在高风险场景中考虑，使用受保护特征需有明确依据。

Abstract: Fairness through Unawareness (FtU) describes the idea that discrimination
against demographic groups can be avoided by not considering group membership
in the decisions or predictions. This idea has long been criticized in the
machine learning literature as not being sufficient to ensure fairness. In
addition, the use of additional features is typically thought to increase the
accuracy of the predictions for all groups, so that FtU is sometimes thought to
be detrimental to all groups. In this paper, we show both theoretically and
empirically that FtU can reduce algorithmic discrimination without necessarily
reducing accuracy. We connect this insight with the literature on Model
Multiplicity, to which we contribute with novel theoretical and empirical
results. Furthermore, we illustrate how, in a real-life application, FtU can
contribute to the deployment of more equitable policies without losing
efficacy. Our findings suggest that FtU is worth considering in practical
applications, particularly in high-risk scenarios, and that the use of
protected attributes such as gender in predictive models should be accompanied
by a clear and well-founded justification.

</details>


### [204] [Stochastic Forward-Forward Learning through Representational Dimensionality Compression](https://arxiv.org/abs/2505.16649)
*Zhichao Zhu,Yang Qi,Hengyuan Ma,Wenlian Lu,Jianfeng Feng*

Key words: Forward+Forward+算法，goodness+函数，有效维度，噪声，神经形态计算

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种新的Forward-Forward算法中的'goodness'函数——维度压缩，通过引入神经元响应的有效维度（ED）来捕捉二阶统计结构，提高了性能并证明了噪声的积极作用。

Motivation: 现有的基于能量学习的'goodness'函数忽略了神经元间的相关性，限制了算法的性能。

Method: 提出了一种新的goodness函数，利用神经元响应的有效维度（ED），并优化其在不同输入条件下的表现。

Result: 新方法在性能上与非BP方法竞争，且噪声有助于提升泛化能力和推理效果。

Conclusion: 该方法为开发更具生物合理性的学习算法提供了新思路，尤其适合神经形态计算。

Abstract: The Forward-Forward (FF) algorithm provides a bottom-up alternative to
backpropagation (BP) for training neural networks, relying on a layer-wise
"goodness" function to guide learning. Existing goodness functions, inspired by
energy-based learning (EBL), are typically defined as the sum of squared
post-synaptic activations, neglecting the correlations between neurons. In this
work, we propose a novel goodness function termed dimensionality compression
that uses the effective dimensionality (ED) of fluctuating neural responses to
incorporate second-order statistical structure. Our objective minimizes ED for
clamped inputs when noise is considered while maximizing it across the sample
distribution, promoting structured representations without the need to prepare
negative samples. We demonstrate that this formulation achieves competitive
performance compared to other non-BP methods. Moreover, we show that noise
plays a constructive role that can enhance generalization and improve inference
when predictions are derived from the mean of squared outputs, which is
equivalent to making predictions based on the energy term. Our findings
contribute to the development of more biologically plausible learning
algorithms and suggest a natural fit for neuromorphic computing, where
stochasticity is a computational resource rather than a nuisance. The code is
available at https://github.com/ZhichaoZhu/StochasticForwardForward

</details>


### [205] [End-to-End Framework for Predicting the Remaining Useful Life of Lithium-Ion Batteries](https://arxiv.org/abs/2505.16664)
*Khoa Tran,Tri Le,Bao Huynh,Hung-Cuong Trinh,Vy-Rin Nguyen*

Key words: 锂离子电池、剩余使用寿命（RUL）、信号处理、深度学习、迁移学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于信号处理流程和深度学习模型的锂离子电池剩余使用寿命（RUL）预测方法，通过融合多特征和混合架构提升预测精度，实验显示其性能优于基线方法。

Motivation: 准确预测锂离子电池的剩余使用寿命（RUL）对及时维护至关重要，直接影响依赖电池的电力应用效率。现有方法需进一步提升精度和适应性。

Method: 提出结合信号预处理（统计去噪与特征增强）和混合深度学习架构（1D CNN、A-LSTM、ODE-LSTM模块），并采用迁移学习优化模型。

Result: 在公开数据集上测试，RMSE为101.59，优于基线深度学习和机器学习方法，且在小样本目标数据下表现稳健。

Conclusion: 混合架构和特征增强策略显著提升RUL预测性能，具备实际应用潜力。

Abstract: Accurate prediction of the Remaining Useful Life (RUL) is essential for
enabling timely maintenance of lithium-ion batteries, impacting the operational
efficiency of electric applications that rely on them. This paper proposes a
RUL prediction approach that leverages data from recent charge-discharge cycles
to estimate the number of remaining usable cycles. The approach introduces both
a novel signal processing pipeline and a deep learning prediction model. In the
signal preprocessing pipeline, a derived capacity feature is computed based on
current and capacity signals. Alongside original capacity, voltage and current,
these features are denoised and enhanced using statistical metrics and a
delta-based method to capture differences between the current and previous
cycles. In the prediction model, the processed features are then fed into a
hybrid deep learning architecture composed of 1D Convolutional Neural Networks
(CNN), Attentional Long Short-Term Memory (A-LSTM), and Ordinary Differential
Equation-based LSTM (ODE-LSTM) modules. This architecture is designed to
capture both local signal characteristics and long-range temporal dependencies
while modeling the continuous-time dynamics of battery degradation. The model
is further evaluated using transfer learning across different learning
strategies and target data partitioning scenarios. Results indicate that the
model maintains robust performance, even when fine-tuned on limited target
data. Experimental results on two publicly available large-scale datasets
demonstrate that the proposed method outperforms a baseline deep learning
approach and machine learning techniques, achieving an RMSE of 101.59,
highlighting its strong potential for real-world RUL prediction applications.

</details>


### [206] [Quantum Feature Optimization for Enhanced Clustering of Blockchain Transaction Data](https://arxiv.org/abs/2505.16672)
*Yun-Cheng Tsai,Samuel Yen-Chi Chen*

Key words: 区块链、量子聚类、K均值、量子神经网络、自监督学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文比较了三种区块链交易数据聚类方法，包括经典K均值、结合量子特征的混合聚类及全量子聚类，证明浅层量子电路能有效提升聚类性能。

Motivation: 区块链交易数据具有高维度、噪声多和特征复杂的特点，传统聚类算法面临挑战，需探索量子计算的应用潜力。

Method: 对比三种方法：经典K均值、混合聚类（量子随机特征增强）和全量子聚类（自监督QNN优化特征空间）。

Result: 实验表明，即使浅层量子电路也能提取有效非线性表示，显著提升聚类效果。

Conclusion: 量子方法在复杂数据聚类中具有优势，为区块链数据分析提供了新思路。

Abstract: Blockchain transaction data exhibits high dimensionality, noise, and
intricate feature entanglement, presenting significant challenges for
traditional clustering algorithms. In this study, we conduct a comparative
analysis of three clustering approaches: (1) Classical K-Means Clustering,
applied to pre-processed feature representations; (2) Hybrid Clustering,
wherein classical features are enhanced with quantum random features extracted
using randomly initialized quantum neural networks (QNNs); and (3) Fully
Quantum Clustering, where a QNN is trained in a self-supervised manner
leveraging a SwAV-based loss function to optimize the feature space for
clustering directly. The proposed experimental framework systematically
investigates the impact of quantum circuit depth and the number of learned
prototypes, demonstrating that even shallow quantum circuits can effectively
extract meaningful non-linear representations, significantly improving
clustering performance.

</details>


### [207] [On the Out-of-Distribution Generalization of Self-Supervised Learning](https://arxiv.org/abs/2505.16675)
*Wenwen Qiang,Jingyao Wang,Zeen Song,Jiangmeng Li,Changwen Zheng*

Key words: 自监督学习, 分布外泛化, 结构因果模型, 干预后分布, 虚假关联

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了自监督学习（SSL）在分布外（OOD）泛化中的表现，分析了其训练过程中学习虚假关联的问题，并提出一种基于结构因果模型的干预后分布（PID）方法以优化泛化能力。

Motivation: 研究旨在理解并提升SSL在OOD场景下的泛化性能，发现其训练过程中学习到的虚假关联是泛化能力下降的关键原因。

Method: 提出干预后分布（PID）方法，结合结构因果模型，设计批次采样策略以减少虚假关联，并通过理论证明和实验验证其有效性。

Result: 实验表明，提出的采样策略在多种OOD任务中显著提升SSL模型的泛化性能。

Conclusion: 通过PID方法和批次采样策略，SSL模型能够实现更优的OOD泛化表现。

Abstract: In this paper, we focus on the out-of-distribution (OOD) generalization of
self-supervised learning (SSL). By analyzing the mini-batch construction during
the SSL training phase, we first give one plausible explanation for SSL having
OOD generalization. Then, from the perspective of data generation and causal
inference, we analyze and conclude that SSL learns spurious correlations during
the training process, which leads to a reduction in OOD generalization. To
address this issue, we propose a post-intervention distribution (PID) grounded
in the Structural Causal Model. PID offers a scenario where the spurious
variable and label variable is mutually independent. Besides, we demonstrate
that if each mini-batch during SSL training satisfies PID, the resulting SSL
model can achieve optimal worst-case OOD performance. This motivates us to
develop a batch sampling strategy that enforces PID constraints through the
learning of a latent variable model. Through theoretical analysis, we
demonstrate the identifiability of the latent variable model and validate the
effectiveness of the proposed sampling strategy. Experiments conducted on
various downstream OOD tasks demonstrate the effectiveness of the proposed
sampling strategy.

</details>


### [208] [Learning Genomic Structure from $k$-mers](https://arxiv.org/abs/2505.16680)
*Filip Thor,Carl Nettelblad*

Key words: 对比学习,基因组组装,k-mer序列,古DNA,宏基因组

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于对比学习的方法，通过训练编码器模型生成能够聚类来自同一基因组区域的短读序列的嵌入表示，从而重构基因组。该方法无需依赖完整基因组组装，适用于多种下游任务。

Motivation: 基因组测序产生的短读序列数量庞大，传统组装方法复杂且耗时。作者希望通过对比学习生成通用的k-mer序列表示，提升基因组结构分析的效率和适用范围。

Method: 采用对比学习训练编码器模型，生成基因组区域的嵌入表示。通过域特异性噪声模型增强鲁棒性，并引入监督对比学习设置（参数Γ）。支持完全自监督训练。

Result: 在E. coli基因组和模拟古DNA数据上验证了方法的有效性，其小预测头在准确性及运行时上媲美BWA-aln标准方法，且适用于宏基因组物种鉴定。

Conclusion: 该方法无需完整基因组组装，具有优异的扩展性，适用于宏基因组和人类基因组规模的映射任务。

Abstract: Sequencing a genome to determine an individual's DNA produces an enormous
number of short nucleotide subsequences known as reads, which must be
reassembled to reconstruct the full genome. We present a method for analyzing
this type of data using contrastive learning, in which an encoder model is
trained to produce embeddings that cluster together sequences from the same
genomic region. The sequential nature of genomic regions is preserved in the
form of trajectories through this embedding space. Trained solely to reflect
the structure of the genome, the resulting model provides a general
representation of $k$-mer sequences, suitable for a range of downstream tasks
involving read data. We apply our framework to learn the structure of the $E.\
coli$ genome, and demonstrate its use in simulated ancient DNA (aDNA) read
mapping and identification of structural variations. Furthermore, we illustrate
the potential of using this type of model for metagenomic species
identification. We show how incorporating a domain-specific noise model can
enhance embedding robustness, and how a supervised contrastive learning setting
can be adopted when a linear reference genome is available, by introducing a
distance thresholding parameter $\Gamma$. The model can also be trained fully
self-supervised on read data, enabling analysis without the need to construct a
full genome assembly using specialized algorithms. Small prediction heads based
on a pre-trained embedding are shown to perform on par with BWA-aln, the
current gold standard approach for aDNA mapping, in terms of accuracy and
runtime for short genomes. Given the method's favorable scaling properties with
respect to total genome size, inference using our approach is highly promising
for metagenomic applications and for mapping to genomes comparable in size to
the human genome.

</details>


### [209] [Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator](https://arxiv.org/abs/2505.16690)
*Beier Luo,Shuoyuan Wang,Yixuan Li,Hongxin Wei*

Key words: 后训练语言模型，置信度校准，无监督学习，温度缩放，DACA

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种名为DACA的无需标记数据的无监督方法，用于优化后训练语言模型的置信度校准，通过选择性使用一致示例以改进其可靠性。

Motivation: 后训练语言模型（PoLM）常表现出过度自信的问题，影响关键应用的可靠性，而标注数据稀缺是校准的主要障碍。

Method: 提出DACA方法，利用PLM与PoLM预测一致的选择性示例进行温度缩放参数优化，避免不一致示例带来的负面影响。

Result: 实验表明DACA显著改进了模型校准性能，将GPT-4o等模型的平均ECE提升高达15.08%。

Conclusion: DACA通过选择性校准一致示例有效解决后训练模型的过度自信问题，提升了模型的可靠性。

Abstract: Post-training of large language models is essential for adapting pre-trained
language models (PLMs) to align with human preferences and downstream tasks.
While PLMs typically exhibit well-calibrated confidence, post-trained language
models (PoLMs) often suffer from over-confidence, assigning high confidence to
both correct and incorrect outputs, which can undermine reliability in critical
applications. A major obstacle in calibrating PoLMs is the scarcity of labeled
data for individual downstream tasks. To address this, we propose
Disagreement-Aware Confidence Alignment (DACA), a novel unsupervised method to
optimize the parameters (e.g., temperature $\tau$) in post-hoc confidence
calibration. Our method is motivated by the under-confidence issue caused by
prediction disagreement between the PLM and PoLM while aligning their
confidence via temperature scaling. Theoretically, the PLM's confidence
underestimates PoLM's prediction accuracy on disagreement examples, causing a
larger $\tau$ and producing under-confident predictions. DACA mitigates this by
selectively using only agreement examples for calibration, effectively
decoupling the influence of disagreement. In this manner, our method avoids an
overly large $\tau$ in temperature scaling caused by disagreement examples,
improving calibration performance. Extensive experiments demonstrate the
effectiveness of our method, improving the average ECE of open-sourced and
API-based LLMs (e.g. GPT-4o) by up to 15.08$\%$ on common benchmarks.

</details>


### [210] [An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations](https://arxiv.org/abs/2505.16705)
*Seonghwan Park,Jueun Mun,Donghyun Oh,Namhoon Lee*

Key words: 概念瓶颈模型,噪声,解释性,锐度感知最小化,预测熵

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究了概念瓶颈模型（CBMs）中噪声对性能的影响，提出了一种两阶段框架以增强鲁棒性。

Motivation: CBMs因其透明性而受欢迎，但训练用的注释常含噪声，其影响未被充分理解。

Method: 采用锐度感知最小化（SAM）训练，推理时通过预测熵排名校正不确定概念。

Result: 噪声显著降低性能、解释性和干预效果，提出的方法能有效缓解。

Conclusion: 两阶段框架在噪声环境中保持了模型的解释性和鲁棒性。

Abstract: Concept bottleneck models (CBMs) ensure interpretability by decomposing
predictions into human interpretable concepts. Yet the annotations used for
training CBMs that enable this transparency are often noisy, and the impact of
such corruption is not well understood. In this study, we present the first
systematic study of noise in CBMs and show that even moderate corruption
simultaneously impairs prediction performance, interpretability, and the
intervention effectiveness. Our analysis identifies a susceptible subset of
concepts whose accuracy declines far more than the average gap between noisy
and clean supervision and whose corruption accounts for most performance loss.
To mitigate this vulnerability we propose a two-stage framework. During
training, sharpness-aware minimization stabilizes the learning of
noise-sensitive concepts. During inference, where clean labels are unavailable,
we rank concepts by predictive entropy and correct only the most uncertain
ones, using uncertainty as a proxy for susceptibility. Theoretical analysis and
extensive ablations elucidate why sharpness-aware training confers robustness
and why uncertainty reliably identifies susceptible concepts, providing a
principled basis that preserves both interpretability and resilience in the
presence of noise.

</details>


### [211] [Training Long-Context LLMs Efficiently via Chunk-wise Optimization](https://arxiv.org/abs/2505.16710)
*Wenhao Li,Yuxin Zhang,Gen Luo,Daohai Yu,Rongrong Ji*

Key words: 长上下文LLM, 训练优化, 分块处理, 梯度传播, 内存效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了SeCO和SpaCO两种内存高效训练方法，通过分块处理长输入和选择性梯度传播来降低长上下文LLM的训练成本，并在单GPU上实现序列长度扩展和训练加速。

Motivation: 当前长上下文大语言模型训练成本过高，限制了定制化应用，需要更高效的训练方法。

Method: 提出SeCO（分块独立计算和局部反向传播）和SpaCO（选择性梯度传播和补偿因子设计）两种训练范式。

Result: SeCO将单GPU上的序列长度从1K扩展到16K，SpaCO在相同条件下训练速度比SeCO快3倍。

Conclusion: SeCO和SpaCO为长上下文模型优化提供了新思路，使其更适用于实际应用。

Abstract: While long-context large language models (LLMs) exhibit remarkable document
processing capabilities, their prohibitively high training costs often hinder
customized applications. To mitigate this issue, we propose \textit{Sequential
Chunk-wise Optimization} (SeCO), a memory-efficient training paradigm that
partitions lengthy inputs into manageable chunks. Each chunk independently
constructs its computational graph and performs localized backpropagation,
ensuring that only one chunk's forward activations are stored in memory.
Building on SeCO, we further introduce \textit{Sparse Chunk-wise Optimization}
(SpaCO), which reduces computational overhead by selectively propagating
gradients to specific chunks and incorporates a carefully designed compensation
factor to ensure unbiased gradient estimation. SpaCO decouples the
computational cost of backpropagation from the context length, enabling
training time to gradually converge to inference time as sequences become
longer. Implemented as lightweight training wrappers, both SeCO and SpaCO offer
substantial practical benefits. For example, when fine-tuning an 8B model with
LoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to
16K tokens, while SpaCO demonstrates accelerated training speed -- achieving up
to 3x faster than SeCO under the same experimental setup. These innovations
provide new insights into optimizing long-context models, making them more
accessible for practical applications. We have open-sourced the code at
\href{https://github.com/wenhaoli-xmu/seco}{here}.

</details>


### [212] [Advancing Brainwave Modeling with a Codebook-Based Foundation Model](https://arxiv.org/abs/2505.16724)
*Konstantinos Barmpas,Na Lee,Yannis Panagakis,Dimitrios A. Adamos,Nikolaos Laskaris,Stefanos Zafeiriou*

Key words: 大型预训练模型、脑电信号、脑机接口、神经振荡、信号处理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文介绍了一种改进的大型脑电波基础模型LaBraM++，通过优化的架构设计增强对神经振荡信息的捕捉能力，显著提升性能，在多种任务中表现优异。

Motivation: 现有的大规模预训练脑电模型未能充分捕捉神经振荡的丰富信息，限制了其在脑机接口和医疗应用中的性能与泛化能力。

Method: 提出LaBraM++模型，基于信号处理原理改进架构设计，增强模型的表征能力。

Result: LaBraM++在多种任务中表现优异，显著优于原基础架构，与其他开源模型相比更具竞争力，且训练效率更高。

Conclusion: LaBraM++具备作为未来大型脑电波基础模型的潜力，其性能与效率为相关领域的研究奠定了基础。

Abstract: Recent advances in large-scale pre-trained Electroencephalogram (EEG) models
have shown great promise, driving progress in Brain-Computer Interfaces (BCIs)
and healthcare applications. However, despite their success, many existing
pre-trained models have struggled to fully capture the rich information content
of neural oscillations, a limitation that fundamentally constrains their
performance and generalizability across diverse BCI tasks. This limitation is
frequently rooted in suboptimal architectural design choices which constrain
their representational capacity. In this work, we introduce LaBraM++, an
enhanced Large Brainwave Foundation Model (LBM) that incorporates principled
improvements grounded in robust signal processing foundations. LaBraM++
demonstrates substantial gains across a variety of tasks, consistently
outperforming its originally-based architecture and achieving competitive
results when compared to other open-source LBMs. Its superior performance and
training efficiency highlight its potential as a strong foundation for future
advancements in LBMs.

</details>


### [213] [Masked Conditioning for Deep Generative Models](https://arxiv.org/abs/2505.16725)
*Phillip Mueller,Jannik Wiese,Sebastian Mueller,Lars Mikelsons*

Key words: 生成模型, 稀疏数据, 混合类型数据, 变分自编码器, 潜在扩散模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新的掩蔽条件方法，用于处理工程领域中稀疏、混合类型数据的小型数据集，通过训练时模拟推理时的稀疏条件，提升了生成模型的适用性和效率。

Motivation: 工程领域的数据集通常小型、标签稀疏且包含混合类型数据，计算资源有限，限制了生成模型的应用。论文旨在解决这些问题。

Method: 提出了掩蔽条件方法，采用不同的稀疏调度策略，并引入灵活的嵌入处理分类和数值条件，集成到变分自编码器和潜在扩散模型中。

Result: 在两个工程相关数据集上验证了方法的有效性，并展示了小模型与大型预训练基础模型结合可提升生成质量。

Conclusion: 该方法在稀疏和混合类型数据条件下表现良好，为工程领域的生成任务提供了高效解决方案。

Abstract: Datasets in engineering domains are often small, sparsely labeled, and
contain numerical as well as categorical conditions. Additionally.
computational resources are typically limited in practical applications which
hinders the adoption of generative models for engineering tasks. We introduce a
novel masked-conditioning approach, that enables generative models to work with
sparse, mixed-type data. We mask conditions during training to simulate sparse
conditions at inference time. For this purpose, we explore the use of various
sparsity schedules that show different strengths and weaknesses. In addition,
we introduce a flexible embedding that deals with categorical as well as
numerical conditions. We integrate our method into an efficient variational
autoencoder as well as a latent diffusion model and demonstrate the
applicability of our approach on two engineering-related datasets of 2D point
clouds and images. Finally, we show that small models trained on limited data
can be coupled with large pretrained foundation models to improve generation
quality while retaining the controllability induced by our conditioning scheme.

</details>


### [214] [Sequential Monte Carlo for Policy Optimization in Continuous POMDPs](https://arxiv.org/abs/2505.16732)
*Hany Abdulsamad,Sahel Iqbal,Simo Särkkä*

Key words: POMDP, 策略优化, 探索与利用, 概率推断, 顺序蒙特卡洛

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一个新的策略优化框架，用于处理连续部分可观察马尔可夫决策过程（POMDPs）中的探索与利用问题，通过概率推断和信息收集的自然整合，无需外部探索奖励或启发式方法。

Motivation: 在部分可观察环境中，如何平衡探索（减少不确定性）与利用（追求即时目标）是决策制定的核心挑战。现有方法往往依赖外部探索奖励或启发式设计，缺乏自然整合信息收集的能力。

Method: 采用基于非马尔可夫Feynman-Kac模型的概率推断框架，捕捉信息收集的价值。开发了一种嵌套顺序蒙特卡洛（SMC）算法，高效估计由POMDP诱导的最优轨迹分布下的历史依赖策略梯度。

Result: 在标准连续POMDP基准测试中，该方法表现出色，解决了现有方法在不确定性下行动的困难。

Conclusion: 本文提出的框架成功解决了部分可观察决策中的探索-利用平衡问题，通过自然整合信息收集，避免了外部启发式设计的需求。

Abstract: Optimal decision-making under partial observability requires agents to
balance reducing uncertainty (exploration) against pursuing immediate
objectives (exploitation). In this paper, we introduce a novel policy
optimization framework for continuous partially observable Markov decision
processes (POMDPs) that explicitly addresses this challenge. Our method casts
policy learning as probabilistic inference in a non-Markovian Feynman--Kac
model that inherently captures the value of information gathering by
anticipating future observations, without requiring extrinsic exploration
bonuses or handcrafted heuristics. To optimize policies under this model, we
develop a nested sequential Monte Carlo~(SMC) algorithm that efficiently
estimates a history-dependent policy gradient under samples from the optimal
trajectory distribution induced by the POMDP. We demonstrate the effectiveness
of our algorithm across standard continuous POMDP benchmarks, where existing
methods struggle to act under uncertainty.

</details>


### [215] [Forward-only Diffusion Probabilistic Models](https://arxiv.org/abs/2505.16733)
*Ziwei Luo,Fredrik K. Gustafsson,Jens Sjölund,Thomas B. Schön*

Key words: 仅前向扩散, 生成建模, 随机微分方程, 均值回归, 图像修复

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种仅前向扩散（FoD）方法，用于简化传统扩散模型的双向过程，通过单一前向扩散直接学习生成数据，提升效率并保持竞争力。

Motivation: 传统扩散模型依赖复杂的前向-后向扩散方案，增加了训练和采样的复杂性。提出FoD旨在简化这一过程，通过单一前向扩散实现高效生成。

Method: FoD基于状态依赖的线性随机微分方程，包含均值回归项，确保数据收敛，并通过随机流匹配目标训练，支持非马尔可夫链采样。

Result: FoD在图像修复和无条件生成任务中表现优异，验证了其在生成模型中的有效性。

Conclusion: FoD提供了一个简洁高效的生成框架，克服了传统扩散模型的复杂性，同时保持竞争力。

Abstract: This work presents a forward-only diffusion (FoD) approach for generative
modelling. In contrast to traditional diffusion models that rely on a coupled
forward-backward diffusion scheme, FoD directly learns data generation through
a single forward diffusion process, yielding a simple yet efficient generative
framework. The core of FoD is a state-dependent linear stochastic differential
equation that involves a mean-reverting term in both the drift and diffusion
functions. This mean-reversion property guarantees the convergence to clean
data, naturally simulating a stochastic interpolation between source and target
distributions. More importantly, FoD is analytically tractable and is trained
using a simple stochastic flow matching objective, enabling a few-step
non-Markov chain sampling during inference. The proposed FoD model, despite its
simplicity, achieves competitive performance on various image-conditioned
(e.g., image restoration) and unconditional generation tasks, demonstrating its
effectiveness in generative modelling. Our code is available at
https://github.com/Algolzw/FoD.

</details>


### [216] [Maximum Total Correlation Reinforcement Learning](https://arxiv.org/abs/2505.16734)
*Bang You,Puze Liu,Huaping Liu,Jan Peters,Oleg Arenz*

Key words: 强化学习, 简单性, 鲁棒性, 轨迹相关性, 性能提升

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了如何在强化学习中通过最大化诱导轨迹中的总相关性来促进简单行为，提出了一种实用的算法，并在模拟环境中验证了其优于基线方法的鲁棒性和性能。

Motivation: 通过关注本质来增强泛化性和鲁棒性，探究如何在强化学习过程中促进简单行为。

Method: 引入一种修改后的强化学习问题定义，最大化诱导轨迹中的总相关性，并提出一种基于下限近似的实用算法。

Result: 在模拟机器人环境中，该方法自然地生成周期性、可压缩的轨迹，表现出对噪声和动态变化的更强鲁棒性，并在原始任务中提升了性能。

Conclusion: 通过最大化轨迹相关性促进简单行为的策略，能有效提升强化学习的鲁棒性和任务性能。

Abstract: Simplicity is a powerful inductive bias. In reinforcement learning,
regularization is used for simpler policies, data augmentation for simpler
representations, and sparse reward functions for simpler objectives, all that,
with the underlying motivation to increase generalizability and robustness by
focusing on the essentials. Supplementary to these techniques, we investigate
how to promote simple behavior throughout the episode. To that end, we
introduce a modification of the reinforcement learning problem that
additionally maximizes the total correlation within the induced trajectories.
We propose a practical algorithm that optimizes all models, including policy
and state representation, based on a lower-bound approximation. In simulated
robot environments, our method naturally generates policies that induce
periodic and compressible trajectories, and that exhibit superior robustness to
noise and changes in dynamics compared to baseline methods, while also
improving performance in the original tasks.

</details>


### [217] [Backward Oversmoothing: why is it hard to train deep Graph Neural Networks?](https://arxiv.org/abs/2505.16736)
*Nicolas Keriven*

Key words: 图神经网络，过平滑，优化，梯度消失，虚假驻点

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文分析了图神经网络（GNNs）中的后向过平滑问题，揭示了其在优化中的作用及对训练的影响。

Motivation: 长期认为GNNs中存在前向过平滑问题，但实践中即使权重足够大，过平滑仍会发生，需从优化角度研究后向过平滑。

Method: 分析后向过平滑及其与前向过平滑的交互作用，证明GNNs中存在虚假驻点，并通过理论推导探讨梯度趋近零的现象。

Result: 发现后向过平滑导致GNNs存在虚假驻点，梯度趋近零时损失仍高；而多层感知机（MLP）不受此影响。

Conclusion: 后向过平滑是GNNs特有的优化问题，对理解其优化场景具有重要意义。

Abstract: Oversmoothing has long been identified as a major limitation of Graph Neural
Networks (GNNs): input node features are smoothed at each layer and converge to
a non-informative representation, if the weights of the GNN are sufficiently
bounded. This assumption is crucial: if, on the contrary, the weights are
sufficiently large, then oversmoothing may not happen. Theoretically, GNN could
thus learn to not oversmooth. However it does not really happen in practice,
which prompts us to examine oversmoothing from an optimization point of view.
In this paper, we analyze backward oversmoothing, that is, the notion that
backpropagated errors used to compute gradients are also subject to
oversmoothing from output to input. With non-linear activation functions, we
outline the key role of the interaction between forward and backward smoothing.
Moreover, we show that, due to backward oversmoothing, GNNs provably exhibit
many spurious stationary points: as soon as the last layer is trained, the
whole GNN is at a stationary point. As a result, we can exhibit regions where
gradients are near-zero while the loss stays high. The proof relies on the fact
that, unlike forward oversmoothing, backward errors are subjected to a linear
oversmoothing even in the presence of non-linear activation function, such that
the average of the output error plays a key role. Additionally, we show that
this phenomenon is specific to deep GNNs, and exhibit counter-example
Multi-Layer Perceptron. This paper is a step toward a more complete
comprehension of the optimization landscape specific to GNNs.

</details>


### [218] [Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization](https://arxiv.org/abs/2505.16737)
*Chengcan Wu,Zhixin Zhang,Zeming Wei,Yihao Zhang,Meng Sun*

Key words: 大语言模型、安全对齐、微调、安全感知探针、梯度传播

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种安全感知探针（SAP）优化框架，解决大语言模型（LLMs）在微调时安全性能下降的问题，即使在无害数据上微调也可能导致安全性降低。SAP通过梯度传播过程中的安全感知探针，识别潜在风险方向，在提升任务性能的同时保持模型安全性。实验表明SAP显著降低了有害性，且测试损失与标准微调方法相当。

Motivation: 尽管在预训练阶段采用了安全对齐技术，但研究表明，即使在无害数据上微调LLMs也可能无意中损害其安全性。本文重新探讨了为何微调无害数据仍会导致安全性能下降的根本问题，旨在开发一种方法以在微调过程中保持模型的安全性。

Method: 提出了安全感知探针（SAP）优化框架，将安全感知探针嵌入梯度传播过程，通过识别梯度方向的潜在风险来缓解安全性能下降的问题。

Result: 实验结果表明，SAP有效降低了模型的有害性，使其低于原始微调模型，同时测试损失与标准微调方法相当。

Conclusion: SAP框架在微调LLMs时成功解决了安全性下降的问题，既提升了任务性能又保持了模型的安全性，为LLMs的安全应用提供了新思路。

Abstract: The significant progress of large language models (LLMs) has led to
remarkable achievements across numerous applications. However, their ability to
generate harmful content has sparked substantial safety concerns. Despite the
implementation of safety alignment techniques during the pre-training phase,
recent research indicates that fine-tuning LLMs on adversarial or even benign
data can inadvertently compromise their safety. In this paper, we re-examine
the fundamental issue of why fine-tuning on non-harmful data still results in
safety degradation. We introduce a safety-aware probing (SAP) optimization
framework designed to mitigate the safety risks of fine-tuning LLMs.
Specifically, SAP incorporates a safety-aware probe into the gradient
propagation process, mitigating the model's risk of safety degradation by
identifying potential pitfalls in gradient directions, thereby enhancing
task-specific performance while successfully preserving model safety. Our
extensive experimental results demonstrate that SAP effectively reduces
harmfulness below the original fine-tuned model and achieves comparable test
loss to standard fine-tuning methods. Our code is available at
https://github.com/ChengcanWu/SAP.

</details>


### [219] [Meta-reinforcement learning with minimum attention](https://arxiv.org/abs/2505.16741)
*Pilhwa Lee,Shashank Gupta*

Key words: 最小注意力、强化学习、元学习、非线性动力学、能量效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了最小注意力在强化学习中的应用，结合模型基础元学习，展示了其在快速适应和能量效率上的优势。

Motivation: 受Brockett提出的最小作用原理启发，研究最小注意力在模拟生物控制（如运动学习）中的作用，并探索其在强化学习中的潜力和连接元学习与稳定性的关系。

Method: 采用基于模型的元学习方法，在高维非线性动力学中应用最小注意力。通过集成模型学习和梯度元策略学习的交替进行，实现优化。

Result: 实验表明，最小注意力在少样本快速适应和模型与环境扰动的方差降低方面优于当前最先进的模型无关和模型基础强化学习算法，同时提高了能量效率。

Conclusion: 最小注意力不仅提升了强化学习的性能，还在能量效率方面表现出显著改进，为生物控制和机器学习提供了新的视角。

Abstract: Minimum attention applies the least action principle in the changes of
control concerning state and time, first proposed by Brockett. The involved
regularization is highly relevant in emulating biological control, such as
motor learning. We apply minimum attention in reinforcement learning (RL) as
part of the rewards and investigate its connection to meta-learning and
stabilization. Specifically, model-based meta-learning with minimum attention
is explored in high-dimensional nonlinear dynamics. Ensemble-based model
learning and gradient-based meta-policy learning are alternately performed.
Empirically, we show that the minimum attention does show outperforming
competence in comparison to the state-of-the-art algorithms in model-free and
model-based RL, i.e., fast adaptation in few shots and variance reduction from
the perturbations of the model and environment. Furthermore, the minimum
attention demonstrates the improvement in energy efficiency.

</details>


### [220] [Revenue Optimization with Price-Sensitive and Interdependent Demand](https://arxiv.org/abs/2505.16748)
*Julien Laasri,Marc Revol*

Key words: 收益管理, 定价优化, 航空公司, 需求预测, 收入最大化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文主要研究航空公司的收益管理，专注于通过预定义的价格和需求数据优化直飞航班的定价和数量决策，以实现收入最大化。

Motivation: 航空公司需要通过收益管理最大化收入，尤其是直飞航班的定价和数量决策。本文旨在通过优化预定义的价格选项和需求数据，解决这一实际业务问题。

Method: 利用预定义的价格选项和Air France提供的需求数据，通过优化算法选择最佳定价策略。

Result: 通过优化定价和数量决策，实现直飞航班收入的最大化。

Conclusion: 优化预定义价格和数量决策能有效提升航空公司的直飞航班收入。

Abstract: As Kalyan T. Talluri and Garrett J. Van Ryzin describe in their work [3],
Revenue Management aims to maximize an organization's revenue by considering
three types of decision categories: structural, pricing, and quantity. In this
document, our primary focus will be on decisions related to pricing and
quantity for the sale of airline tickets on a direct flight over a certain
number of time periods. More specifically, we will only focus on the
optimization aspect of this problem. We will assume the demand data to be
given, since Air France estimates it beforehand using real data. Similarly, we
assume all price options to be predetermined by Air France's algorithms and
verified by their analysts. Our objective will be to maximize the revenue of a
direct flight by choosing the prices for each product from the predefined set
of options.
  --
  Comme d\'ecrit par Kalyan T. Talluri et Garrett J. Van Ryzin dans leur
ouvrage [3], le Revenue Management consiste en la maximisation du revenu d'un
organisme \`a partir de trois types de cat\'egories de d\'ecision :
structurelles, prix et quantit\'e. Dans ce document, nous nous int\'eresserons
principalement aux d\'ecisions de type prix et quantit\'e pour la vente de
billets d'avion sur un vol direct au cours d'un certain nombre de pas de temps.
Plus pr\'ecis\'ement, nous nous situerons dans la partie optimisation du
probl\`eme. Nous prendrons ainsi les donn\'ees de demande comme acquises, car
elles sont estim\'ees au pr\'ealable par Air France \`a partir des donn\'ees
r\'eelles. De m\^eme, pour chaque produit que l'on cherchera \`a vendre, on
nous impose en amont les prix possibles que l'on a droit d'utiliser et qui se
basent sur des algorithmes d'Air France dont les r\'esultats sont v\'erifi\'es
par des analystes. Notre but sera alors de maximiser le revenu d'un vol direct
en choisissant les prix de chaque produit parmi ceux impos\'es.

</details>


### [221] [PyTupli: A Scalable Infrastructure for Collaborative Offline Reinforcement Learning Projects](https://arxiv.org/abs/2505.16754)
*Hannah Markgraf,Michael Eichelbeck,Daria Cappey,Selin Demirtürk,Yara Schattschneider,Matthias Althoff*

Key words: 离线强化学习, 数据集管理, PyTupli, 基准测试, 可复现性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: PyTupli是一个Python工具，旨在简化解耦离线强化学习中的数据管理问题，支持创建、存储和共享基准环境及相应数据集。

Motivation: 离线强化学习虽然避免了高成本或高风险的在线交互，但目前缺乏标准化、可扩展的数据集管理工具，尤其是在用户自定义基准测试场景中。

Method: 开发PyTupli，包含轻量级客户端库（上传/检索接口）和容器化服务器组件（身份验证、访问控制等），支持细粒度数据筛选。

Result: 实现了高效的数据集管理，提升了离线强化学习的协作性、可复现性和扩展性。

Conclusion: PyTupli填补了离线RL领域的基础设施空白，推动研究更高效发展。

Abstract: Offline reinforcement learning (RL) has gained traction as a powerful
paradigm for learning control policies from pre-collected data, eliminating the
need for costly or risky online interactions. While many open-source libraries
offer robust implementations of offline RL algorithms, they all rely on
datasets composed of experience tuples consisting of state, action, next state,
and reward. Managing, curating, and distributing such datasets requires
suitable infrastructure. Although static datasets exist for established
benchmark problems, no standardized or scalable solution supports developing
and sharing datasets for novel or user-defined benchmarks. To address this gap,
we introduce PyTupli, a Python-based tool to streamline the creation, storage,
and dissemination of benchmark environments and their corresponding tuple
datasets. PyTupli includes a lightweight client library with defined interfaces
for uploading and retrieving benchmarks and data. It supports fine-grained
filtering at both the episode and tuple level, allowing researchers to curate
high-quality, task-specific datasets. A containerized server component enables
production-ready deployment with authentication, access control, and automated
certificate provisioning for secure use. By addressing key barriers in dataset
infrastructure, PyTupli facilitates more collaborative, reproducible, and
scalable offline RL research.

</details>


### [222] [Multi-Output Gaussian Processes for Graph-Structured Data](https://arxiv.org/abs/2505.16755)
*Ayano Nakai-Kasai,Tadashi Wadayama*

Key words: 图结构数据，多输出高斯过程，回归方法，核设计

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种基于多输出高斯过程（MOGP）的图结构数据回归方法，能够同时捕捉顶点和数据的相关性，并具有广泛的适用性和高表达能力。

Motivation: 图结构数据中的顶点和边描述了数据的相关性，需要一种能够捕捉这种复杂关系的方法。

Method: 通过多输出高斯过程（MOGP）构建回归模型，利用灵活的核设计提高表达能力，并覆盖现有方法的多种配置和场景。

Result: 实验表明，该方法在合成和真实数据上表现良好，能够解除现有方法在数据配置、模型选择和推断场景上的限制。

Conclusion: 该方法为图结构数据的回归提供了灵活且高效的解决方案。

Abstract: Graph-structured data is a type of data to be obtained associated with a
graph structure where vertices and edges describe some kind of data
correlation. This paper proposes a regression method on graph-structured data,
which is based on multi-output Gaussian processes (MOGP), to capture both the
correlation between vertices and the correlation between associated data. The
proposed formulation is built on the definition of MOGP. This allows it to be
applied to a wide range of data configurations and scenarios. Moreover, it has
high expressive capability due to its flexibility in kernel design. It includes
existing methods of Gaussian processes for graph-structured data as special
cases and is possible to remove restrictions on data configurations, model
selection, and inference scenarios in the existing methods. The performance of
extensions achievable by the proposed formulation is evaluated through computer
experiments with synthetic and real data.

</details>


### [223] [FlowMixer: A Constrained Neural Architecture for Interpretable Spatiotemporal Forecasting](https://arxiv.org/abs/2505.16786)
*Fares B. Mehouachi,Saif Eddin Jabari*

Key words: FlowMixer, 时空模式, 可逆映射, Kronecker-Koopman, 长时预测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: FlowMixer是一种神经网络架构，通过约束矩阵运算建模时空模式，结合可逆映射框架与Kronecker-Koopman特征模态，实现高效、可解释的长时预测。

Motivation: 旨在通过架构约束提升神经网络预测性能和数学可解释性，尤其在复杂时空模式（如混沌吸引子、湍流）的建模中。

Method: 采用非负矩阵混合层和可逆映射框架（变换-混合-逆变换），结合Kronecker-Koopman特征模态理论。

Result: 实验证明FlowMixer在多领域具有稳健的长时预测能力，能直接代数调整预测范围且无需重新训练。

Conclusion: 架构约束可同时增强神经预测系统的性能与可解释性，为动态系统理论与统计学习架设桥梁。

Abstract: We introduce FlowMixer, a neural architecture that leverages constrained
matrix operations to model structured spatiotemporal patterns. At its core,
FlowMixer incorporates non-negative matrix mixing layers within a reversible
mapping framework-applying transforms before mixing and their inverses
afterward. This shape-preserving design enables a Kronecker-Koopman eigenmode
framework that bridges statistical learning with dynamical systems theory,
providing interpretable spatiotemporal patterns and facilitating direct
algebraic manipulation of prediction horizons without retraining. Extensive
experiments across diverse domains demonstrate FlowMixer's robust long-horizon
forecasting capabilities while effectively modeling physical phenomena such as
chaotic attractors and turbulent flows. These results suggest that
architectural constraints can simultaneously enhance predictive performance and
mathematical interpretability in neural forecasting systems.

</details>


### [224] [Learning Flexible Forward Trajectories for Masked Molecular Diffusion](https://arxiv.org/abs/2505.16790)
*Hyunjin Seo,Taewon Kim,Sihyun Yu,SungSoo Ahn*

Key words: 掩码扩散模型, 分子生成, 状态冲突, 噪声调度, MELD

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文探索了掩码扩散模型（MDMs）在分子生成中的潜力，发现直接应用标准MDMs会严重降低性能，并提出了MELD方法来解决状态冲突问题，显著提高了生成质量。

Motivation: 研究掩码扩散模型在分子生成中的应用潜力，并解决标准MDM方法中出现的状态冲突问题。

Method: 提出MELD方法，通过参数化噪声调度网络为单个分子元素（如原子和键）分配不同的破坏率，避免状态冲突。

Result: MELD显著提高了生成质量，将ZINC250K数据集上的化学有效性从15%提升至93%，并在条件生成任务中达到了最先进的性能。

Conclusion: MELD通过优化噪声调度解决了MDMs在分子生成中的状态冲突问题，显著提升了性能和化学有效性。

Abstract: Masked diffusion models (MDMs) have achieved notable progress in modeling
discrete data, while their potential in molecular generation remains
underexplored. In this work, we explore their potential and introduce the
surprising result that naively applying standards MDMs severely degrades the
performance. We identify the critical cause of this issue as a state-clashing
problem-where the forward diffusion of distinct molecules collapse into a
common state, resulting in a mixture of reconstruction targets that cannot be
learned using typical reverse diffusion process with unimodal predictions. To
mitigate this, we propose Masked Element-wise Learnable Diffusion (MELD) that
orchestrates per-element corruption trajectories to avoid collision between
distinct molecular graphs. This is achieved through a parameterized noise
scheduling network that assigns distinct corruption rates to individual graph
elements, i.e., atoms and bonds. Extensive experiments on diverse molecular
benchmarks reveal that MELD markedly enhances overall generation quality
compared to element-agnostic noise scheduling, increasing the chemical validity
of vanilla MDMs on ZINC250K from 15% to 93%, Furthermore, it achieves
state-of-the-art property alignment in conditional generation tasks.

</details>


### [225] [Cohort-Based Active Modality Acquisition](https://arxiv.org/abs/2505.16791)
*Tillmann Rheude,Roland Eils,Benjamin Wild*

Key words: 多模态学习,主动模态获取,测试时策略,生成式填充,判别式建模

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为CAMA的测试时多模态获取策略，用于在资源有限时选择需要额外模态的样本，通过生成式填充和判别式建模结合的方式优化获取效果。

Motivation: 解决现实应用中因模态缺失或获取成本高而难以有效整合多模态数据的问题。

Method: 结合生成式填充和判别式建模，提出CAMA框架及获取策略，并引入性能上限启发式方法进行基准测试。

Result: 实验表明，基于填充的策略在多模态数据集中比单模态、熵引导和随机选择更有效。

Conclusion: CAMA能在资源受限时优化模态获取，提升资源利用效率。

Abstract: Real-world machine learning applications often involve data from multiple
modalities that must be integrated effectively to make robust predictions.
However, in many practical settings, not all modalities are available for every
sample, and acquiring additional modalities can be costly. This raises the
question: which samples should be prioritized for additional modality
acquisition when resources are limited? While prior work has explored
individual-level acquisition strategies and training-time active learning
paradigms, test-time and cohort-based acquisition remain underexplored despite
their importance in many real-world settings. We introduce Cohort-based Active
Modality Acquisition (CAMA), a novel test-time setting to formalize the
challenge of selecting which samples should receive additional modalities. We
derive acquisition strategies that leverage a combination of generative
imputation and discriminative modeling to estimate the expected benefit of
acquiring missing modalities based on common evaluation metrics. We also
introduce upper-bound heuristics that provide performance ceilings to benchmark
acquisition strategies. Experiments on common multimodal datasets demonstrate
that our proposed imputation-based strategies can more effectively guide the
acquisition of new samples in comparison to those relying solely on unimodal
information, entropy guidance, and random selections. Our work provides an
effective solution for optimizing modality acquisition at the cohort level,
enabling better utilization of resources in constrained settings.

</details>


### [226] [A modular framework for automated evaluation of procedural content generation in serious games with deep reinforcement learning agents](https://arxiv.org/abs/2505.16801)
*Eleftherios Kalafatis,Konstantinos Mitsis,Konstantia Zarkogianni,Maria Athanasiou,Konstantina Nikita*

Key words: 严肃游戏（SGs）、程序化内容生成（PCG）、深度强化学习（DRL）、遗传算法、自动化评估

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种利用深度强化学习（DRL）游戏测试代理自动评估PCG（程序化内容生成）在严肃游戏（SGs）中集成效果的方法，并通过实验验证了其有效性。

Motivation: 研究动机在于解决PCG技术在SGs中集成效果的评估难题，以实现更个性化的玩家体验。

Method: 论文提出了一种基于DRL游戏测试代理的自动化评估框架，并通过卡牌游戏实例测试了三种PCG版本的效果。

Result: 结果显示，使用遗传算法的PCG版本（版本2和3）训练的DRL代理在胜率和训练时间上均优于随机生成版本（版本1），胜率峰值达到97%。

Conclusion: 结论表明，该框架能够有效评估SGs中的PCG内容，为游戏设计提供数据支持。

Abstract: Serious Games (SGs) are nowadays shifting focus to include procedural content
generation (PCG) in the development process as a means of offering personalized
and enhanced player experience. However, the development of a framework to
assess the impact of PCG techniques when integrated into SGs remains
particularly challenging. This study proposes a methodology for automated
evaluation of PCG integration in SGs, incorporating deep reinforcement learning
(DRL) game testing agents. To validate the proposed framework, a previously
introduced SG featuring card game mechanics and incorporating three different
versions of PCG for nonplayer character (NPC) creation has been deployed.
Version 1 features random NPC creation, while versions 2 and 3 utilize a
genetic algorithm approach. These versions are used to test the impact of
different dynamic SG environments on the proposed framework's agents. The
obtained results highlight the superiority of the DRL game testing agents
trained on Versions 2 and 3 over those trained on Version 1 in terms of win
rate (i.e. number of wins per played games) and training time. More
specifically, within the execution of a test emulating regular gameplay, both
Versions 2 and 3 peaked at a 97% win rate and achieved statistically
significant higher (p=0009) win rates compared to those achieved in Version 1
that peaked at 94%. Overall, results advocate towards the proposed framework's
capability to produce meaningful data for the evaluation of procedurally
generated content in SGs.

</details>


### [227] [Contextual Learning for Stochastic Optimization](https://arxiv.org/abs/2505.16829)
*Anna Heuser,Thomas Kesselheim*

Key words: 随机优化, 上下文价值分布, Lévy距离, 样本复杂度, 凸代理损失

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了如何从上下文价值分布的样本中学习，通过优化凸代理损失来逼近真实分布，并应用于随机优化问题，证明了样本复杂度为多项式级别。

Motivation: 研究动机来源于随机优化，探讨如何从上下文价值分布的样本中学习，以解决未知分布下的优化问题。

Method: 方法包括最小化凸代理损失，学习每个上下文对应的经验分布，确保其与真实分布的Lévy距离较小。

Result: 结果为样本复杂度边界提供理论保证，证明在强单调和稳定的优化问题（如单物品收益最大化、潘多拉盒子和最优停止）中，样本复杂度为多项式级别。

Conclusion: 结论表明，该方法能有效学习上下文价值分布，并为随机优化问题提供高效的样本复杂度。

Abstract: Motivated by stochastic optimization, we introduce the problem of learning
from samples of contextual value distributions. A contextual value distribution
can be understood as a family of real-valued distributions, where each sample
consists of a context $x$ and a random variable drawn from the corresponding
real-valued distribution $D_x$. By minimizing a convex surrogate loss, we learn
an empirical distribution $D'_x$ for each context, ensuring a small L\'evy
distance to $D_x$. We apply this result to obtain the sample complexity bounds
for the learning of an $\epsilon$-optimal policy for stochastic optimization
problems defined on an unknown contextual value distribution. The sample
complexity is shown to be polynomial for the general case of strongly monotone
and stable optimization problems, including Single-item Revenue Maximization,
Pandora's Box and Optimal Stopping.

</details>


### [228] [Strategically Linked Decisions in Long-Term Planning and Reinforcement Learning](https://arxiv.org/abs/2505.16833)
*Alihan Hüyük,Finale Doshi-Velez*

Key words: 强化学习, 战略链接分数, 决策支持系统, 规划过程, 非RL代理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种称为“战略链接分数”的量化方法，用于衡量计划行动之间的依赖关系，并通过三个实际应用展示了其实用性。

Motivation: 研究动机在于量化计划行动之间的依赖关系，以揭示强化学习（RL）中策略性决策的内在联系，并探索其在解释黑盒RL代理、提升决策支持系统性能以及分析非RL代理规划过程中的价值。

Method: 方法是通过计算“战略链接分数”，即在后续决策不可用时某一决策的概率下降值，来衡量行动之间的关联性。

Result: 研究结果表明，该方法能有效识别策略性关联决策，提升决策支持系统的最坏情况性能，并分析非RL代理的规划过程。

Conclusion: 结论是战略链接分数是一种实用工具，可用于多种场景，从而深化对策略性决策的理解。

Abstract: Long-term planning, as in reinforcement learning (RL), involves finding
strategies: actions that collectively work toward a goal rather than
individually optimizing their immediate outcomes. As part of a strategy, some
actions are taken at the expense of short-term benefit to enable future actions
with even greater returns. These actions are only advantageous if followed up
by the actions they facilitate, consequently, they would not have been taken if
those follow-ups were not available. In this paper, we quantify such
dependencies between planned actions with strategic link scores: the drop in
the likelihood of one decision under the constraint that a follow-up decision
is no longer available. We demonstrate the utility of strategic link scores
through three practical applications: (i) explaining black-box RL agents by
identifying strategically linked pairs among decisions they make, (ii)
improving the worst-case performance of decision support systems by
distinguishing whether recommended actions can be adopted as standalone
improvements or whether they are strategically linked hence requiring a
commitment to a broader strategy to be effective, and (iii) characterizing the
planning processes of non-RL agents purely through interventions aimed at
measuring strategic link scores - as an example, we consider a realistic
traffic simulator and analyze through road closures the effective planning
horizon of the emergent routing behavior of many drivers.

</details>


### [229] [ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning](https://arxiv.org/abs/2505.16850)
*Tajamul Ashraf,Mohammed Mohsen Peerzada,Moloud Abdar,Yutong Xie,Yuyin Zhou,Xiaofeng Liu,Iqra Altaf Gillani,Janibul Bashir*

Key words: 联邦学习, ATR-Bench, 适应性, 信任, 推理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: ATR-Bench是一个统一的联邦学习分析框架，专注于适应性、信任和推理三个维度，旨在标准化评估并推动联邦学习的系统化进步。

Motivation: 联邦学习缺乏标准化的评估方法，阻碍了其系统化进展和公平比较。

Method: 提出了ATR-Bench框架，整合适应性、信任和推理三个维度，并通过基准测试代表性方法和数据集。

Result: 在适应性和信任维度上进行了广泛测试，推理维度因缺乏可靠指标仅提供文献驱动分析。

Conclusion: ATR-Bench为联邦学习的系统化评估奠定了基础，并将持续跟踪研究进展。

Abstract: Federated Learning (FL) has emerged as a promising paradigm for collaborative
model training while preserving data privacy across decentralized participants.
As FL adoption grows, numerous techniques have been proposed to tackle its
practical challenges. However, the lack of standardized evaluation across key
dimensions hampers systematic progress and fair comparison of FL methods. In
this work, we introduce ATR-Bench, a unified framework for analyzing federated
learning through three foundational dimensions: Adaptation, Trust, and
Reasoning. We provide an in-depth examination of the conceptual foundations,
task formulations, and open research challenges associated with each theme. We
have extensively benchmarked representative methods and datasets for adaptation
to heterogeneous clients and trustworthiness in adversarial or unreliable
environments. Due to the lack of reliable metrics and models for reasoning in
FL, we only provide literature-driven insights for this dimension. ATR-Bench
lays the groundwork for a systematic and holistic evaluation of federated
learning with real-world relevance. We will make our complete codebase publicly
accessible and a curated repository that continuously tracks new developments
and research in the FL literature.

</details>


### [230] [Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only](https://arxiv.org/abs/2505.16856)
*Wei Xiao,Jiacheng Liu,Zifeng Zhuang,Runze Suo,Shangke Lyu,Donglin Wang*

Key words: 强化学习, 在线微调, 离线预训练, 行为克隆, PORL

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为PORL的新方法，通过仅使用离线预训练策略进行高效的在线强化学习微调，避免了依赖预训练Q函数的问题，从而解决了现有方法在探索性和适用性方面的局限性。

Motivation: 现有的在线RL微调方法依赖离线预训练的Q函数以确保稳定性和性能，但这些Q函数通常低估离线数据集之外的状态-动作对，限制了探索能力。此外，这些方法在仅有预训练策略（如模仿学习）而无Q函数时无法适用。

Method: 提出了PORL方法，仅在在线阶段从头初始化Q函数，避免了有害的悲观性，并且不需要预训练的Q函数。

Result: PORL方法不仅与先进的离线到在线RL算法和在线RL方法表现相当，还首次实现了直接微调行为克隆策略的可行性。

Conclusion: PORL方法通过仅依赖预训练策略，解决了现有RL微调方法在依赖性和探索性上的问题，为直接微调BC策略开辟了新途径。

Abstract: Improving the performance of pre-trained policies through online
reinforcement learning (RL) is a critical yet challenging topic. Existing
online RL fine-tuning methods require continued training with offline
pretrained Q-functions for stability and performance. However, these offline
pretrained Q-functions commonly underestimate state-action pairs beyond the
offline dataset due to the conservatism in most offline RL methods, which
hinders further exploration when transitioning from the offline to the online
setting. Additionally, this requirement limits their applicability in scenarios
where only pre-trained policies are available but pre-trained Q-functions are
absent, such as in imitation learning (IL) pre-training. To address these
challenges, we propose a method for efficient online RL fine-tuning using
solely the offline pre-trained policy, eliminating reliance on pre-trained
Q-functions. We introduce PORL (Policy-Only Reinforcement Learning
Fine-Tuning), which rapidly initializes the Q-function from scratch during the
online phase to avoid detrimental pessimism. Our method not only achieves
competitive performance with advanced offline-to-online RL algorithms and
online RL approaches that leverage data or policies prior, but also pioneers a
new path for directly fine-tuning behavior cloning (BC) policies.

</details>


### [231] [Redefining Clustered Federated Learning for System Identification: The Path of ClusterCraft](https://arxiv.org/abs/2505.16857)
*Ertuğrul Keçeci,Müjde Güzelkaya,Tufan Kumbasar*

Key words: System Identification, federated learning, incremental clustering, ClusterMerge

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种基于增量聚类的联邦学习方法IC-SYSID，用于解决多数据源系统辨识问题，无需先验知识，通过动态聚类和合并提升性能。

Motivation: 解决联邦学习框架下多数据源系统辨识问题，消除对先验知识的依赖。

Method: 引入ClusterCraft动态聚类和ClusterMerge合并相似簇，集成正则化项并采用mini-batch深度学习。

Result: 实验表明IC-SYSID在高性能系统辨识的同时避免了不稳定簇的学习。

Conclusion: IC-SYSID有效解决了多数据源系统辨识问题，提升了学习稳定性。

Abstract: This paper addresses the System Identification (SYSID) problem within the
framework of federated learning. We introduce a novel algorithm, Incremental
Clustering-based federated learning method for SYSID (IC-SYSID), designed to
tackle SYSID challenges across multiple data sources without prior knowledge.
IC-SYSID utilizes an incremental clustering method, ClusterCraft (CC), to
eliminate the dependency on the prior knowledge of the dataset. CC starts with
a single cluster model and assigns similar local workers to the same clusters
by dynamically increasing the number of clusters. To reduce the number of
clusters generated by CC, we introduce ClusterMerge, where similar cluster
models are merged. We also introduce enhanced ClusterCraft to reduce the
generation of similar cluster models during the training. Moreover, IC-SYSID
addresses cluster model instability by integrating a regularization term into
the loss function and initializing cluster models with scaled Glorot
initialization. It also utilizes a mini-batch deep learning approach to manage
large SYSID datasets during local training. Through the experiments conducted
on a real-world representing SYSID problem, where a fleet of vehicles
collaboratively learns vehicle dynamics, we show that IC-SYSID achieves a high
SYSID performance while preventing the learning of unstable clusters.

</details>


### [232] [GCAL: Adapting Graph Models to Evolving Domain Shifts](https://arxiv.org/abs/2505.16860)
*Ziyue Qiao,Qianyi Cai,Hao Dong,Jiawei Gu,Pengyang Wang,Meng Xiao,Xiao Luo,Hui Xiong*

Key words: 图域自适应, 动态图, 异分布, 连续学习, 信息瓶颈

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文针对动态、多异分布图的图域自适应问题，提出了一种图连续自适应学习（GCAL）方法。GCAL通过双层优化策略解决传统方法在连续域转移中的失效和灾难性遗忘问题，实验证明其性能显著优于现有方法。

Motivation: 解决传统图域自适应方法在动态、多异分布图中的单步适应局限性，应对连续域转移和灾难性遗忘的挑战。

Method: 采用双层优化策略：'适应'阶段使用信息最大化方法微调模型并重新适应旧记忆；'生成记忆'阶段通过变分记忆图生成模块压缩原始图为记忆。

Result: 实验表明GCAL在适应性和知识保留方面显著优于现有方法。

Conclusion: GCAL有效增强了模型在动态图域中的可持续性和适应性。

Abstract: This paper addresses the challenge of graph domain adaptation on evolving,
multiple out-of-distribution (OOD) graphs. Conventional graph domain adaptation
methods are confined to single-step adaptation, making them ineffective in
handling continuous domain shifts and prone to catastrophic forgetting. This
paper introduces the Graph Continual Adaptive Learning (GCAL) method, designed
to enhance model sustainability and adaptability across various graph domains.
GCAL employs a bilevel optimization strategy. The "adapt" phase uses an
information maximization approach to fine-tune the model with new graph domains
while re-adapting past memories to mitigate forgetting. Concurrently, the
"generate memory" phase, guided by a theoretical lower bound derived from
information bottleneck theory, involves a variational memory graph generation
module to condense original graphs into memories. Extensive experimental
evaluations demonstrate that GCAL substantially outperforms existing methods in
terms of adaptability and knowledge retention.

</details>


### [233] [A Multi-Step Comparative Framework for Anomaly Detection in IoT Data Streams](https://arxiv.org/abs/2505.16872)
*Mohammed Al-Qudah,Fadi AlMahamid*

Key words: 物联网, 异常检测, 机器学习, 预处理, 模型评估

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一个多步评估框架，分析不同预处理步骤（如归一化、转换和特征选择）与三种机器学习算法（RNN-LSTM、自编码器和梯度提升）的交互作用。实验结果表明，梯度提升在不同预处理配置下表现最优，而RNN-LSTM在z-score归一化下表现显著，自编码器则擅长召回率，适用于无监督场景。

Motivation: 随着物联网设备的迅速扩展，安全挑战日益突出，需要更精确的异常检测方法。虽然已有许多机器学习方法用于此目的，但缺乏对预处理步骤与不同模型架构交互作用的系统性研究。

Method: 论文提出了一种多步评估框架，评估预处理步骤（如归一化、转换和特征选择）对三种机器学习算法（RNN-LSTM、自编码器和梯度提升）的综合影响。实验在IoTID20数据集上进行。

Result: 实验结果显示，梯度提升（GBoosting）在不同预处理配置下准确率最高；RNN-LSTM在z-score归一化下表现显著提升；自编码器在召回率方面表现优异，适用于无监督场景。

Conclusion: 通过结构化分析预处理决策与不同机器学习技术的交互作用，该框架为提升物联网环境中的异常检测性能提供了实用指导。

Abstract: The rapid expansion of Internet of Things (IoT) devices has introduced
critical security challenges, underscoring the need for accurate anomaly
detection. Although numerous studies have proposed machine learning (ML)
methods for this purpose, limited research systematically examines how
different preprocessing steps--normalization, transformation, and feature
selection--interact with distinct model architectures. To address this gap,
this paper presents a multi-step evaluation framework assessing the combined
impact of preprocessing choices on three ML algorithms: RNN-LSTM, autoencoder
neural networks (ANN), and Gradient Boosting (GBoosting). Experiments on the
IoTID20 dataset shows that GBoosting consistently delivers superior accuracy
across preprocessing configurations, while RNN-LSTM shows notable gains with
z-score normalization and autoencoders excel in recall, making them well-suited
for unsupervised scenarios. By offering a structured analysis of preprocessing
decisions and their interplay with various ML techniques, the proposed
framework provides actionable guidance to enhance anomaly detection performance
in IoT environments.

</details>


### [234] [Structure-Aligned Protein Language Model](https://arxiv.org/abs/2505.16896)
*Can Chen,David Heurtel-Depeiges,Robert M. Vernon,Christopher James Langmead,Yoshua Bengio,Quentin Fournier*

Key words: 蛋白质语言模型, 对比学习, 结构知识, 图神经网络, ESM2

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 文章提出了一种通过双重任务框架将结构知识融入蛋白质语言模型的方法，显著提升了模型的性能。

Motivation: 蛋白质语言模型（pLMs）缺乏结构知识，而这对许多生物应用至关重要，因此需要改进。

Method: 通过对比学习任务和物理级任务将结构和物理知识融入pLMs，并引入残差损失选择模块优化学习过程。

Result: 在ESM2和AMPLIFY模型中应用该方法，性能显著提升，例如ESM2接触预测提高了12.7%。

Conclusion: 提出的框架成功将结构知识整合到pLMs中，并在多个任务中取得了显著效果。

Abstract: Protein language models (pLMs) pre-trained on vast protein sequence databases
excel at various downstream tasks but lack the structural knowledge essential
for many biological applications. To address this, we integrate structural
insights from pre-trained protein graph neural networks (pGNNs) into pLMs
through a latent-level contrastive learning task. This task aligns residue
representations from pLMs with those from pGNNs across multiple proteins,
enriching pLMs with inter-protein structural knowledge. Additionally, we
incorporate a physical-level task that infuses intra-protein structural
knowledge by optimizing pLMs to predict structural tokens. The proposed
dual-task framework effectively incorporates both inter-protein and
intra-protein structural knowledge into pLMs. Given the variability in the
quality of protein structures in PDB, we further introduce a residue loss
selection module, which uses a small model trained on high-quality structures
to select reliable yet challenging residue losses for the pLM to learn.
Applying our structure alignment method to the state-of-the-art ESM2 and
AMPLIFY results in notable performance gains across a wide range of tasks,
including a 12.7% increase in ESM2 contact prediction. The data, code, and
resulting SaESM2 and SaAMPLIFY models will be released on Hugging Face.

</details>


### [235] [Unsupervised Prompting for Graph Neural Networks](https://arxiv.org/abs/2505.16903)
*Peyman Baghershahi,Sourav Medya*

Key words: GNN, 提示方法, 无监督学习, 一致性正则化, 伪标记

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一个完全无监督的GNN提示方法，通过一致性正则化和伪标记，无需标记数据和参数更新，性能优于现有方法。

Motivation: 现有GNN提示方法依赖标记数据和轻量微调，而LLMs的上下文学习方法展示了无需参数更新和标记数据的潜力。

Method: 基于一致性正则化的伪标记技术，通过两种正则化技术对齐数据分布并减少偏差预测。

Result: 在无标记数据和参数更新的条件下，性能优于现有需标记数据的方法。

Conclusion: 无监督提示方法在GNN中展现了强大的潜力，尤其适用于协变量偏移场景。

Abstract: Prompt tuning methods for Graph Neural Networks (GNNs) have become popular to
address the semantic gap between pre-training and fine-tuning steps. However,
existing GNN prompting methods rely on labeled data and involve lightweight
fine-tuning for downstream tasks. Meanwhile, in-context learning methods for
Large Language Models (LLMs) have shown promising performance with no parameter
updating and no or minimal labeled data. Inspired by these approaches, in this
work, we first introduce a challenging problem setup to evaluate GNN prompting
methods. This setup encourages a prompting function to enhance a pre-trained
GNN's generalization to a target dataset under covariate shift without updating
the GNN's parameters and with no labeled data. Next, we propose a fully
unsupervised prompting method based on consistency regularization through
pseudo-labeling. We use two regularization techniques to align the prompted
graphs' distribution with the original data and reduce biased predictions.
Through extensive experiments under our problem setting, we demonstrate that
our unsupervised approach outperforms the state-of-the-art prompting methods
that have access to labels.

</details>


### [236] [Scalable and Interpretable Contextual Bandits: A Literature Review and Retail Offer Prototype](https://arxiv.org/abs/2505.16918)
*Nikola Tankovic,Robert Sajina*

Key words: 多臂老虎机, 可解释性, 动态环境, 特征工程, 个性化推荐

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种可扩展且可解释的多臂老虎机方法框架，特别针对快速变化的优惠选择问题，通过多类别上下文建模和改进的学习效率优化决策过程。

Motivation: 解决快速变化的优惠环境中，传统多臂老虎机方法学习效率低和泛化能力不足的问题，同时提高决策的透明度和可解释性。

Method: 通过多类别上下文建模、高效特征工程和模块化设计扩展标准CMAB方法，结合MPG和MF等高级特征捕捉用户-优惠交互。

Result: 该框架在动态环境中实现了高效学习和泛化，并通过LLM接口提供实时解释，支持个性化优惠优化。

Conclusion: 该框架在研究和实际应用中均表现出色，提升了自动化决策的信任度和效果。

Abstract: This paper presents a concise review of Contextual Multi-Armed Bandit (CMAB)
methods and introduces an experimental framework for scalable, interpretable
offer selection, addressing the challenge of fast-changing offers. The approach
models context at the product category level, allowing offers to span multiple
categories and enabling knowledge transfer across similar offers. This improves
learning efficiency and generalization in dynamic environments. The framework
extends standard CMAB methodology to support multi-category contexts, and
achieves scalability through efficient feature engineering and modular design.
Advanced features such as MPG (Member Purchase Gap) and MF (Matrix
Factorization) capture nuanced user-offer interactions, with implementation in
Python for practical deployment.
  A key contribution is interpretability at scale: logistic regression models
yield transparent weight vectors, accessible via a large language model (LLM)
interface for real-time, user-level tracking and explanation of evolving
preferences. This enables the generation of detailed member profiles and
identification of behavioral patterns, supporting personalized offer
optimization and enhancing trust in automated decisions. By situating our
prototype alongside established paradigms like Generalized Linear Models and
Thompson Sampling, we demonstrate its value for both research and real-world
CMAB applications.

</details>


### [237] [Risk-Averse Reinforcement Learning with Itakura-Saito Loss](https://arxiv.org/abs/2505.16925)
*Igor Udovichenko,Olivier Croissant,Anita Toleutaeva,Evgeny Burnaev,Alexander Korotin*

Key words: 风险规避强化学习, 指数效用函数, 伊塔库拉-赛托散度, 数值稳定

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文针对风险规避强化学习中的数值不稳定问题，提出了一种基于伊塔库拉-赛托散度的数值稳定损失函数，并在理论和实验上验证了其优越性。

Motivation: 风险规避强化学习在高风险领域应用广泛，但现有方法因涉及指数计算导致数值不稳定，亟需一种更稳定的解决方案。

Method: 采用指数效用函数框架，推导贝尔曼方程，并提出基于伊塔库拉-赛托散度的数值稳定损失函数。

Result: 实验表明，新损失函数在多种金融场景中表现优于现有方法，尤其是在已知解析解的场景中。

Conclusion: 论文提出的损失函数不仅数值稳定且数学严谨，为风险规避强化学习提供了更可靠的优化途径。

Abstract: Risk-averse reinforcement learning finds application in various high-stakes
fields. Unlike classical reinforcement learning, which aims to maximize
expected returns, risk-averse agents choose policies that minimize risk,
occasionally sacrificing expected value. These preferences can be framed
through utility theory. We focus on the specific case of the exponential
utility function, where we can derive the Bellman equations and employ various
reinforcement learning algorithms with few modifications. However, these
methods suffer from numerical instability due to the need for exponent
computation throughout the process. To address this, we introduce a numerically
stable and mathematically sound loss function based on the Itakura-Saito
divergence for learning state-value and action-value functions. We evaluate our
proposed loss function against established alternatives, both theoretically and
empirically. In the experimental section, we explore multiple financial
scenarios, some with known analytical solutions, and show that our loss
function outperforms the alternatives.

</details>


### [238] [The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm](https://arxiv.org/abs/2505.16932)
*Noah Amsel,David Persson,Christopher Musco,Robert Gower*

Key words: 极分解, 矩阵符号函数, GPU计算, Muon优化, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为Polar Express的GPU友好算法，用于计算极分解，适用于深度学习中的Muon优化框架，相比传统方法在效率和兼容性上更具优势。

Motivation: 传统数值分析中的极分解算法在深度学习场景下效率不足且不兼容GPU，特别是Newton-Schulz等方法收敛慢或依赖QR分解/矩阵求逆，无法满足需求。因此，需要一种高效且GPU兼容的新方法。

Method: Polar Express通过每迭代步求解极小极大优化问题来动态调整多项式更新规则，仅使用矩阵乘法操作，确保GPU兼容性，并具备最优收敛性保证。

Result: 实验表明，Polar Express在bfloat16精度下稳定，应用于Muon框架时，在GPT-2等大规模模型上验证损失持续优于现有方法。

Conclusion: Polar Express结合了快速初始收敛与渐进收敛速度，为深度学习中的极分解问题提供了高效、稳定的解决方案。

Abstract: Computing the polar decomposition and the related matrix sign function, has
been a well-studied problem in numerical analysis for decades. More recently,
it has emerged as an important subroutine in deep learning, particularly within
the Muon optimization framework. However, the requirements in this setting
differ significantly from those of traditional numerical analysis. In deep
learning, methods must be highly efficient and GPU-compatible, but high
accuracy is often unnecessary. As a result, classical algorithms like
Newton-Schulz (which suffers from slow initial convergence) and methods based
on rational functions (which rely on QR decompositions or matrix inverses) are
poorly suited to this context. In this work, we introduce Polar Express, a
GPU-friendly algorithm for computing the polar decomposition. Like classical
polynomial methods such as Newton-Schulz, our approach uses only matrix-matrix
multiplications, making it GPU-compatible. Motivated by earlier work of Chen &
Chow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule
at each iteration by solving a minimax optimization problem, and we prove that
it enjoys a strong worst-case optimality guarantee. This property ensures both
rapid early convergence and fast asymptotic convergence. We also address
finite-precision issues, making it stable in bfloat16 in practice. We apply
Polar Express within the Muon optimization framework and show consistent
improvements in validation loss on large-scale models such as GPT-2,
outperforming recent alternatives across a range of learning rates.

</details>


### [239] [LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning](https://arxiv.org/abs/2505.16933)
*Zebin You,Shen Nie,Xiaolu Zhang,Jun Hu,Jun Zhou,Zhiwu Lu,Ji-Rong Wen,Chongxuan Li*

Key words: 扩散模型,多模态大型语言模型,视觉指令调优,LLaDA-V

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: LLaDA-V是一种基于扩散的多模态大型语言模型，通过将视觉指令调优与掩码扩散模型结合，展示了在多模态任务中的竞争力。

Motivation: 当前多模态方法主要依赖自回归范式，LLaDA-V探索了一种基于扩散模型的新方法。

Method: LLaDA-V在LLaDA基础上加入视觉编码器和MLP连接器，将视觉特征投影到语言嵌入空间，实现多模态对齐。

Result: LLaDA-V在多模态任务中表现出色，性能接近LLaMA3-V和Qwen2-VL，并在多模态理解中达到最先进水平。

Conclusion: 大型语言扩散模型在多模态场景中具有潜力，值得进一步研究。

Abstract: In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large
Language Model (MLLM) that integrates visual instruction tuning with masked
diffusion models, representing a departure from the autoregressive paradigms
dominant in current multimodal approaches. Built upon LLaDA, a representative
large language diffusion model, LLaDA-V incorporates a vision encoder and MLP
connector that projects visual features into the language embedding space,
enabling effective multimodal alignment. Our empirical investigation reveals
several intriguing results: First, LLaDA-V demonstrates promising multimodal
performance despite its language model being weaker on purely textual tasks
than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same
instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal
tasks with better data scalability. It also narrows the performance gap to
Qwen2-VL, suggesting the effectiveness of its architecture for multimodal
tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal
understanding compared to existing hybrid autoregressive-diffusion and purely
diffusion-based MLLMs. Our findings suggest that large language diffusion
models show promise in multimodal contexts and warrant further investigation in
future research. Project page and codes:
https://ml-gsai.github.io/LLaDA-V-demo/.

</details>


### [240] [SPAR: Self-supervised Placement-Aware Representation Learning for Multi-Node IoT Systems](https://arxiv.org/abs/2505.16936)
*Yizhuo Chen,Tianchen Wang,You Lyu,Yanlan Hu,Jinyang Li,Tomoyoshi Kimura,Hongjue Zhao,Yigong Hu,Denizhan Kara,Tarek Abdelzaher*

Key words: 自监督学习,多传感器系统,空间表示学习,IoT,信息论

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 这篇论文提出了一种自监督的、考虑传感器布局的表示学习方法，旨在从多视角多模态的传感器观测中建模空间现象，为IoT系统提供更好的环境状态表示。通过理论和实验验证了方法的优越性。

Motivation: IoT系统中多传感器的观测需要从分布式的多视角数据中提炼出空间现象，而现有方法往往忽视了数据的空间特性。为了提升模型的泛化性和鲁棒性，需要一种能显式建模信号与观测几何布局关系的方法。

Method: 论文提出了一种自监督学习框架，通过显式学习测量值与观测者几何布局的依赖关系，基于信号与观测位置的对偶性设计核心原则。理论分析从信息论和遮挡不变表示学习的角度提供了设计依据。

Result: 在车辆监控、人类活动识别和地震定位三个真实数据集上的实验表明，该方法在不同模态、传感器布局、应用级推理任务和空间尺度上具有优越的泛化性和鲁棒性。

Conclusion: 该工作显著推进了IoT信号的自监督预训练，提供了一种能够有效建模空间现象的解决方案，并验证了其在实际应用中的有效性。

Abstract: This work develops the underpinnings of self-supervised placement-aware
representation learning given spatially-distributed (multi-view and multimodal)
sensor observations, motivated by the need to represent external environmental
state in multi-sensor IoT systems in a manner that correctly distills spatial
phenomena from the distributed multi-vantage observations. The objective of
sensing in IoT systems is, in general, to collectively represent an externally
observed environment given multiple vantage points from which sensory
observations occur. Pretraining of models that help interpret sensor data must
therefore encode the relation between signals observed by sensors and the
observers' vantage points in order to attain a representation that encodes the
observed spatial phenomena in a manner informed by the specific placement of
the measuring instruments, while allowing arbitrary placement. The work
significantly advances self-supervised model pretraining from IoT signals
beyond current solutions that often overlook the distinctive spatial nature of
IoT data. Our framework explicitly learns the dependencies between measurements
and geometric observer layouts and structural characteristics, guided by a core
design principle: the duality between signals and observer positions. We
further provide theoretical analyses from the perspectives of information
theory and occlusion-invariant representation learning to offer insight into
the rationale behind our design. Experiments on three real-world
datasets--covering vehicle monitoring, human activity recognition, and
earthquake localization--demonstrate the superior generalizability and
robustness of our method across diverse modalities, sensor placements,
application-level inference tasks, and spatial scales.

</details>


### [241] [FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records](https://arxiv.org/abs/2505.16941)
*Chao Pang,Vincent Jeanselme,Young Sang Choi,Xinzhuo Jiang,Zilin Jing,Aparajita Kashyap,Yuta Kobayashi,Yanwei Li,Florent Pollet,Karthik Natarajan,Shalmali Joshi*

Key words: 基础模型, 电子健康记录, 临床评估, 预训练策略, 数据表示

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文探讨了基础模型在医疗健康领域的潜力，特别是在电子健康记录（EHR）数据处理中的应用。研究提出了一套临床有意义任务的评估套件，并在500万患者数据上验证了基础模型的性能，旨在指导未来医疗基础模型的发展。

Motivation: 探索基础模型在医疗健康中的实用性和有效性，尤其是在标记数据有限的情况下，克服现有评估方法的不足，提供更全面和多样化的评估标准。

Method: 研究设计了一套临床相关的任务评估套件，利用500万哥伦比亚大学欧文医学中心的EHR数据，对比不同预训练、标记化和数据表示策略的效果。

Result: 通过14个临床任务评估了基础模型的性能、校准和子群体表现，比较了不同策略的优劣。

Conclusion: 研究为结构化的EHR基础模型提供了实证评估，并为未来的医疗基础模型开发提供了指导。

Abstract: Foundation models hold significant promise in healthcare, given their
capacity to extract meaningful representations independent of downstream tasks.
This property has enabled state-of-the-art performance across several clinical
applications trained on structured electronic health record (EHR) data, even in
settings with limited labeled data, a prevalent challenge in healthcare.
However, there is little consensus on these models' potential for clinical
utility due to the lack of desiderata of comprehensive and meaningful tasks and
sufficiently diverse evaluations to characterize the benefit over conventional
supervised learning. To address this gap, we propose a suite of clinically
meaningful tasks spanning patient outcomes, early prediction of acute and
chronic conditions, including desiderata for robust evaluations. We evaluate
state-of-the-art foundation models on EHR data consisting of 5 million patients
from Columbia University Irving Medical Center (CUMC), a large urban academic
medical center in New York City, across 14 clinically relevant tasks. We
measure overall accuracy, calibration, and subpopulation performance to surface
tradeoffs based on the choice of pre-training, tokenization, and data
representation strategies. Our study aims to advance the empirical evaluation
of structured EHR foundation models and guide the development of future
healthcare foundation models.

</details>


### [242] [MixAT: Combining Continuous and Discrete Adversarial Training for LLMs](https://arxiv.org/abs/2505.16947)
*Csaba Dékány,Stefan Balauca,Robin Staab,Dimitar I. Dimitrov,Martin Vechev*

Key words: 大型语言模型，对抗攻击，鲁棒性，MixAT，ALO-ASR

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出MixAT方法，结合离散和连续攻击训练大型语言模型（LLM），显著提升模型对抗攻击的鲁棒性。

Motivation: 当前LLM对抗攻击仍能产生有害内容，现有防御方法（如连续松弛训练）无法有效抵御离散攻击。

Method: MixAT结合更强的离散和更快的连续攻击训练LLM，并提出ALO-ASR指标评估模型最坏情况下的脆弱性。

Result: MixAT在多种攻击下表现优越（ALO-ASR < 20%），计算开销低，优于现有方法（ALO-ASR > 50%）。

Conclusion: MixAT提供了一种高效且鲁棒的防御方案，为构建更安全的LLM提供了新思路。

Abstract: Despite recent efforts in Large Language Models (LLMs) safety and alignment,
current adversarial attacks on frontier LLMs are still able to force harmful
generations consistently. Although adversarial training has been widely studied
and shown to significantly improve the robustness of traditional machine
learning models, its strengths and weaknesses in the context of LLMs are less
understood. Specifically, while existing discrete adversarial attacks are
effective at producing harmful content, training LLMs with concrete adversarial
prompts is often computationally expensive, leading to reliance on continuous
relaxations. As these relaxations do not correspond to discrete input tokens,
such latent training methods often leave models vulnerable to a diverse set of
discrete attacks. In this work, we aim to bridge this gap by introducing MixAT,
a novel method that combines stronger discrete and faster continuous attacks
during training. We rigorously evaluate MixAT across a wide spectrum of
state-of-the-art attacks, proposing the At Least One Attack Success Rate
(ALO-ASR) metric to capture the worst-case vulnerability of models. We show
MixAT achieves substantially better robustness (ALO-ASR < 20%) compared to
prior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to
methods based on continuous relaxations. We further analyze MixAT in realistic
deployment settings, exploring how chat templates, quantization, low-rank
adapters, and temperature affect both adversarial training and evaluation,
revealing additional blind spots in current methodologies. Our results
demonstrate that MixAT's discrete-continuous defense offers a principled and
superior robustness-accuracy tradeoff with minimal computational overhead,
highlighting its promise for building safer LLMs. We provide our code and
models at https://github.com/insait-institute/MixAT.

</details>


### [243] [Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised Reasoning](https://arxiv.org/abs/2505.16950)
*Adnan Oomerjee,Zafeirios Fountas,Zhongwei Yu,Haitham Bou-Ammar,Jun Wang*

Key words: 大型语言模型、信息瓶颈理论、Transformer架构、KV缓存、推理任务

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文通过信息瓶颈理论分析大型语言模型的泛化能力局限性，提出一种改进Transformer架构的方法，通过周期性全局变换KV缓存来提升推理任务的表现。

Motivation: 尽管大型语言模型表现出色，但其在训练分布之外的泛化能力有限，主要是模式插值而非真正的抽象推理。本文旨在通过信息瓶颈理论解决这一问题。

Method: 通过信息瓶颈理论证明解码器Transformer在形成任务最优序列表示方面受限，提出周期性全局变换KV缓存的模块以优化表示。

Result: 改进后的模型在数学推理基准测试中表现显著优于普通Transformer和启发式缓存压缩方法，参数效率更高。

Conclusion: 通过信息理论框架优化Transformer的内存管理，可解决仅靠规模扩展无法克服的推理能力限制。

Abstract: Despite their impressive capabilities, Large Language Models struggle with
generalisation beyond their training distribution, often exhibiting
sophisticated pattern interpolation rather than true abstract reasoning
(extrapolation). In this work, we approach this limitation through the lens of
Information Bottleneck (IB) theory, which posits that model generalisation
emerges from an optimal balance between input compression and retention of
predictive information in latent representations. We prove using IB theory that
decoder-only Transformers are inherently constrained in their ability to form
task-optimal sequence representations. We then use this result to demonstrate
that periodic global transformation of the internal sequence-level
representations (KV cache) is a necessary computational step for improving
Transformer generalisation in reasoning tasks. Based on these theoretical
insights, we propose a modification to the Transformer architecture, in the
form of an additional module that globally rewrites the KV cache at periodic
intervals, shifting its capacity away from memorising input prefixes and toward
encoding features most useful for predicting future tokens. Our model delivers
substantial gains on mathematical reasoning benchmarks, outperforming both
vanilla Transformers with up to 3.5x more parameters, as well as
heuristic-driven pruning mechanisms for cache compression. Our approach can be
seen as a principled generalisation of existing KV-cache compression methods;
whereas such methods focus solely on compressing input representations, they
often do so at the expense of retaining predictive information, and thus their
capabilities are inherently bounded by those of an unconstrained model. This
establishes a principled framework to manipulate Transformer memory using
information theory, addressing fundamental reasoning limitations that scaling
alone cannot overcome.

</details>


### [244] [A Comprehensive Evaluation of Contemporary ML-Based Solvers for Combinatorial Optimization](https://arxiv.org/abs/2505.16952)
*Shengyu Feng,Weiwei Sun,Shanda Li,Ameet Talwalkar,Yiming Yang*

Key words: 机器学习, 组合优化, 基准测试, 图神经网络, 大型语言模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文介绍了FrontierCO，一个综合基准测试，用于评估机器学习在组合优化问题中的实际应用效果，覆盖八种典型问题类型并测试16种代表性ML求解器。

Motivation: 当前机器学习在组合优化问题中的应用大多基于小规模、合成数据集，缺乏对实际大规模场景的有效验证，且现有基准测试数据不足。

Method: 引入FrontierCO基准，包含工业应用和研究前沿中的挑战性实例，提供丰富的训练数据，评估16种ML求解器（如图神经网络和大型语言模型代理）。

Result: 实证结果揭示了当前ML方法的优势和局限性，为机器学习与组合优化交叉领域的更稳健、实用进展提供了指导。

Conclusion: FrontierCO为解决ML在组合优化中的实际应用问题提供了重要基准和数据支持。

Abstract: Machine learning (ML) has demonstrated considerable potential in supporting
model design and optimization for combinatorial optimization (CO) problems.
However, much of the progress to date has been evaluated on small-scale,
synthetic datasets, raising concerns about the practical effectiveness of
ML-based solvers in real-world, large-scale CO scenarios. Additionally, many
existing CO benchmarks lack sufficient training data, limiting their utility
for evaluating data-driven approaches. To address these limitations, we
introduce FrontierCO, a comprehensive benchmark that covers eight canonical CO
problem types and evaluates 16 representative ML-based solvers--including graph
neural networks and large language model (LLM) agents. FrontierCO features
challenging instances drawn from industrial applications and frontier CO
research, offering both realistic problem difficulty and abundant training
data. Our empirical results provide critical insights into the strengths and
limitations of current ML methods, helping to guide more robust and practically
relevant advances at the intersection of machine learning and combinatorial
optimization. Our data is available at
https://huggingface.co/datasets/CO-Bench/FrontierCO.

</details>


### [245] [ICYM2I: The illusion of multimodal informativeness under missingness](https://arxiv.org/abs/2505.16953)
*Young Sang Choi,Vincent Jeanselme,Pierre Elias,Shalmali Joshi*

Key words: 多模态学习, 数据缺失, ICYM2I, 信息增益, 逆概率加权

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了多模态学习中的数据缺失问题，提出了ICYM2I框架，通过逆概率加权校正评估缺失情况下的预测性能和信息增益。

Motivation: 多模态学习在部署时可能面临模态缺失的问题，忽略缺失可能导致对模态价值的错误估计。

Method: 提出了ICYM2I框架，利用逆概率加权校正评估缺失情况下的信息增益。

Result: 通过合成、半合成和真实医学数据集验证了校正方法的重要性。

Conclusion: ICYM2I框架能有效评估缺失下的模态价值，提升多模态学习的实用性。

Abstract: Multimodal learning is of continued interest in artificial intelligence-based
applications, motivated by the potential information gain from combining
different types of data. However, modalities collected and curated during
development may differ from the modalities available at deployment due to
multiple factors including cost, hardware failure, or -- as we argue in this
work -- the perceived informativeness of a given modality. Na{\"i}ve estimation
of the information gain associated with including an additional modality
without accounting for missingness may result in improper estimates of that
modality's value in downstream tasks. Our work formalizes the problem of
missingness in multimodal learning and demonstrates the biases resulting from
ignoring this process. To address this issue, we introduce ICYM2I (In Case You
Multimodal Missed It), a framework for the evaluation of predictive performance
and information gain under missingness through inverse probability
weighting-based correction. We demonstrate the importance of the proposed
adjustment to estimate information gain under missingness on synthetic,
semi-synthetic, and real-world medical datasets.

</details>


### [246] [Bigger Isn't Always Memorizing: Early Stopping Overparameterized Diffusion Models](https://arxiv.org/abs/2505.16959)
*Alessandro Favero,Antonio Sclocchi,Matthieu Wyart*

Key words: 扩散模型, 泛化, 记忆, 过参数化, 早停准则

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了过参数化扩散模型在训练过程中如何逐步实现泛化而非记忆的机制，并提出基于数据集大小的早停准则来优化泛化。

Motivation: 研究扩散概率模型在过参数化状况下泛化与记忆的竞争机制，旨在理解并优化模型的泛化能力。

Method: 分析了从图像到语言的不同扩散模型，并通过学习简单概率上下文无关文法（PCFG）来验证现象。

Result: 发现记忆时间与数据集大小成正比，并提出了一个描述泛化与记忆竞争的相图。

Conclusion: 基于数据集大小的早停准则能有效优化泛化并避免记忆，对参数传递和隐私敏感应用有直接意义。

Abstract: Diffusion probabilistic models have become a cornerstone of modern generative
AI, yet the mechanisms underlying their generalization remain poorly
understood. In fact, if these models were perfectly minimizing their training
loss, they would just generate data belonging to their training set, i.e.,
memorize, as empirically found in the overparameterized regime. We revisit this
view by showing that, in highly overparameterized diffusion models,
generalization in natural data domains is progressively achieved during
training before the onset of memorization. Our results, ranging from image to
language diffusion models, systematically support the empirical law that
memorization time is proportional to the dataset size. Generalization vs.
memorization is then best understood as a competition between time scales. We
show that this phenomenology is recovered in diffusion models learning a simple
probabilistic context-free grammar with random rules, where generalization
corresponds to the hierarchical acquisition of deeper grammar rules as training
time grows, and the generalization cost of early stopping can be characterized.
We summarize these results in a phase diagram. Overall, our results support
that a principled early-stopping criterion - scaling with dataset size - can
effectively optimize generalization while avoiding memorization, with direct
implications for hyperparameter transfer and privacy-sensitive applications.

</details>


### [247] [UFT: Unifying Supervised and Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.16984)
*Mingyang Liu,Gabriele Farina,Asuman Ozdaglar*

Key words: UFT, SFT, RFT, 大语言模型, 推理能力

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为UFT的新范式，有效结合SFT和RFT，提升大语言模型的推理能力。

Motivation: 解决SFT和RFT在提升大语言模型推理能力时的局限性，如SFT可能导致过拟合，RFT依赖基础模型强度。

Method: 提出统一微调（UFT），将SFT和RFT整合为单一过程，结合探索和监督信号。

Result: UFT在所有模型规模上均优于SFT和RFT，并能指数级加速长范围推理任务的收敛。

Conclusion: UFT作为一种新方法，解决了现有方法的不足，显著提升了推理能力。

Abstract: Post-training has demonstrated its importance in enhancing the reasoning
capabilities of large language models (LLMs). The primary post-training methods
can be categorized into supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT). SFT is efficient and well-suited for small language models,
but it may lead to overfitting and limit the reasoning abilities of larger
models. In contrast, RFT generally yields better generalization but depends
heavily on the strength of the base model. To address the limitations of SFT
and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm
that unifies SFT and RFT into a single, integrated process. UFT enables the
model to effectively explore solutions while incorporating informative
supervision signals, bridging the gap between memorizing and thinking
underlying existing methods. Notably, UFT outperforms both SFT and RFT in
general, regardless of model sizes. Furthermore, we theoretically prove that
UFT breaks RFT's inherent exponential sample complexity bottleneck, showing for
the first time that unified training can exponentially accelerate convergence
on long-horizon reasoning tasks.

</details>


### [248] [PICT -- A Differentiable, GPU-Accelerated Multi-Block PISO Solver for Simulation-Coupled Learning Tasks in Fluid Dynamics](https://arxiv.org/abs/2505.16992)
*Aleksandra Franz,Hao Wei,Luca Guastoni,Nils Thuerey*

Key words: 流体模拟, 可微模拟器, 湍流模型, PyTorch, GPU

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了PICT，一种基于PyTorch的可微压力隐式流体模拟器，支持GPU，通过验证其在经典基准测试中的准确性和梯度，展示了其在复杂湍流模型学习中的有效性，并开源了代码。

Motivation: 流体模拟在科学计算中极具挑战性，可微模拟器因其在深度学习中梯度信息的必要性成为优化和物理模拟学习的有效工具。

Method: 开发了PICT，一个支持GPU的可微压力隐式流体模拟器，通过在经典基准测试中验证其前向模拟和梯度准确性，并应用于2D和3D湍流模型的监督与无监督学习。

Result: PICT能够学习稳定的3D湍流子网格尺度模型，低分辨率校正器在保持或超越高分辨率参考精度的同时运行更快。

Conclusion: PICT展示了可微模拟器在流体动力学中的潜力，其开源性有望推动进一步研究与应用。

Abstract: Despite decades of advancements, the simulation of fluids remains one of the
most challenging areas of in scientific computing. Supported by the necessity
of gradient information in deep learning, differentiable simulators have
emerged as an effective tool for optimization and learning in physics
simulations. In this work, we present our fluid simulator PICT, a
differentiable pressure-implicit solver coded in PyTorch with
Graphics-processing-unit (GPU) support. We first verify the accuracy of both
the forward simulation and our derived gradients in various established
benchmarks like lid-driven cavities and turbulent channel flows before we show
that the gradients provided by our solver can be used to learn complicated
turbulence models in 2D and 3D. We apply both supervised and unsupervised
training regimes using physical priors to match flow statistics. In particular,
we learn a stable sub-grid scale (SGS) model for a 3D turbulent channel flow
purely based on reference statistics. The low-resolution corrector trained with
our solver runs substantially faster than the highly resolved references, while
keeping or even surpassing their accuracy. Finally, we give additional insights
into the physical interpretation of different solver gradients, and motivate a
physically informed regularization technique. To ensure that the full potential
of PICT can be leveraged, it is published as open source:
https://github.com/tum-pbs/PICT.

</details>


### [249] [A Unified Framework for Simultaneous Parameter and Function Discovery in Differential Equations](https://arxiv.org/abs/2505.16996)
*Shalev Manor,Mohammad Kohandel*

Key words: 微分方程逆问题, 参数识别, 函数识别, 唯一解, 机器学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种解决微分方程逆问题中参数和函数同时识别的新框架，克服了现有方法（如PINNs、UDEs和UPINNs）的局限性，确保了唯一解的存在。

Motivation: 微分方程逆问题中，同时识别未知参数和函数时，现有方法可能因解的非唯一性而受限，需要一种更有效的解决方案。

Method: 提出了一种新框架，通过建立唯一解的条件来解决问题，并在生物系统和生态动力学中进行了验证。

Result: 实验证明该方法能够提供准确且可解释的结果。

Conclusion: 新框架显著提升了机器学习在复杂系统建模中的潜力。

Abstract: Inverse problems involving differential equations often require identifying
unknown parameters or functions from data. Existing approaches, such as
Physics-Informed Neural Networks (PINNs), Universal Differential Equations
(UDEs) and Universal Physics-Informed Neural Networks (UPINNs), are effective
at isolating either parameters or functions but can face challenges when
applied simultaneously due to solution non-uniqueness. In this work, we
introduce a framework that addresses these limitations by establishing
conditions under which unique solutions can be guaranteed. To illustrate, we
apply it to examples from biological systems and ecological dynamics,
demonstrating accurate and interpretable results. Our approach significantly
enhances the potential of machine learning techniques in modeling complex
systems in science and engineering.

</details>


### [250] [Guided Diffusion Sampling on Function Spaces with Applications to PDEs](https://arxiv.org/abs/2505.17004)
*Jiachen Yao,Abbas Mammadov,Julius Berner,Gavin Kerrigan,Jong Chul Ye,Kamyar Azizzadenesheli,Anima Anandkumar*

Key words: PDE逆问题，函数空间扩散模型，神经算子，条件采样，Tweedie公式

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种用于偏微分方程（PDE）逆问题中条件采样的通用框架FunDPS，通过函数空间扩散模型和即插即用引导机制，从极稀疏或噪声数据中恢复完整解。

Motivation: 解决传统方法在稀疏或噪声测量下难以恢复PDE逆问题的完整解的问题，尤其是在离散化无关和极小监督条件下的挑战。

Method: 1. 训练一个无条件的、离散化无关的去噪模型（基于神经算子架构）；2. 通过基于梯度的引导机制在推理阶段优化稀疏观测数据；3. 扩展Tweedie公式到无限维希尔伯特空间，为后验采样提供理论支持。

Result: 在仅3%观测数据的5个PDE任务中，平均准确率提升32%，采样步骤减少4倍；多分辨率微调增强了跨分辨率泛化能力。

Conclusion: FunDPS是首个独立于离散化的扩散框架，为PDE的正反问题提供了实用灵活的解决方案。

Abstract: We propose a general framework for conditional sampling in PDE-based inverse
problems, targeting the recovery of whole solutions from extremely sparse or
noisy measurements. This is accomplished by a function-space diffusion model
and plug-and-play guidance for conditioning. Our method first trains an
unconditional discretization-agnostic denoising model using neural operator
architectures. At inference, we refine the samples to satisfy sparse
observation data via a gradient-based guidance mechanism. Through rigorous
mathematical analysis, we extend Tweedie's formula to infinite-dimensional
Hilbert spaces, providing the theoretical foundation for our posterior sampling
approach. Our method (FunDPS) accurately captures posterior distributions in
function spaces under minimal supervision and severe data scarcity. Across five
PDE tasks with only 3% observation, our method achieves an average 32% accuracy
improvement over state-of-the-art fixed-resolution diffusion baselines while
reducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning
ensures strong cross-resolution generalizability. To the best of our knowledge,
this is the first diffusion-based framework to operate independently of
discretization, offering a practical and flexible solution for forward and
inverse problems in the context of PDEs. Code is available at
https://github.com/neuraloperator/FunDPS

</details>


### [251] [Understanding Prompt Tuning and In-Context Learning via Meta-Learning](https://arxiv.org/abs/2505.17010)
*Tim Genewein,Kevin Wenliang Li,Jordi Grau-Moya,Anian Ruoss,Laurent Orseau,Marcus Hutter*

Key words: 提示优化, 贝叶斯理论, prefix-tuning, 权重调整, 软前缀

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了如何从贝叶斯视角理解最优提示，并揭示了提示的根本局限性，提出仅通过调整权重才能克服这些限制。

Motivation: 目前提示优化的方法多为经验驱动，缺乏对提示概念的理解，论文旨在填补这一空白。

Method: 通过贝叶斯理论分析最优提示，并比较不同版本的prefix-tuning和权重调整方法。

Result: 实验表明，软前缀（实数向量序列）能通过操纵激活生成非常有效的提示。

Conclusion: 贝叶斯视角为提示优化提供了理论基础，但某些情况下仍需调整权重才能实现最优。

Abstract: Prompting is one of the main ways to adapt a pretrained model to target
tasks. Besides manually constructing prompts, many prompt optimization methods
have been proposed in the literature. Method development is mainly empirically
driven, with less emphasis on a conceptual understanding of prompting. In this
paper we discuss how optimal prompting can be understood through a Bayesian
view, which also implies some fundamental limitations of prompting that can
only be overcome by tuning weights. The paper explains in detail how
meta-trained neural networks behave as Bayesian predictors over the pretraining
distribution, whose hallmark feature is rapid in-context adaptation. Optimal
prompting can be studied formally as conditioning these Bayesian predictors,
yielding criteria for target tasks where optimal prompting is and is not
possible. We support the theory with educational experiments on LSTMs and
Transformers, where we compare different versions of prefix-tuning and
different weight-tuning methods. We also confirm that soft prefixes, which are
sequences of real-valued vectors outside the token alphabet, can lead to very
effective prompts for trained and even untrained networks by manipulating
activations in ways that are not achievable by hard tokens. This adds an
important mechanistic aspect beyond the conceptual Bayesian theory.

</details>


### [252] [When Are Concepts Erased From Diffusion Models?](https://arxiv.org/abs/2505.17013)
*Kevin Lu,Nicky Kriplani,Rohit Gandikota,Minh Pham,David Bau,Chinmay Hegde,Niv Cohen*

Key words: 概念擦除,扩散模型,评估框架,对抗攻击

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究了扩散模型中的概念擦除机制，并提出了两种擦除方法。为了评估擦除效果，引入了一套独立的评估框架，结果表明需在副作用最小化和对抗性提示的鲁棒性之间取得平衡。

Motivation: 评估现有概念擦除方法的彻底性，并探讨如何改进擦除效果。

Method: 提出了两种概念擦除机制模型，并设计了一套包含对抗攻击、新探测技术和替代生成分析的评估框架。

Result: 揭示了在概念擦除中副作用最小化与对抗性提示鲁棒性之间的矛盾。

Conclusion: 强调了全面评估在扩散模型概念擦除中的重要性。

Abstract: Concept erasure, the ability to selectively prevent a model from generating
specific concepts, has attracted growing interest, with various approaches
emerging to address the challenge. However, it remains unclear how thoroughly
these methods erase the target concept. We begin by proposing two conceptual
models for the erasure mechanism in diffusion models: (i) reducing the
likelihood of generating the target concept, and (ii) interfering with the
model's internal guidance mechanisms. To thoroughly assess whether a concept
has been truly erased from the model, we introduce a suite of independent
evaluations. Our evaluation framework includes adversarial attacks, novel
probing techniques, and analysis of the model's alternative generations in
place of the erased concept. Our results shed light on the tension between
minimizing side effects and maintaining robustness to adversarial prompts.
Broadly, our work underlines the importance of comprehensive evaluation for
erasure in diffusion models.

</details>


### [253] [Interactive Post-Training for Vision-Language-Action Models](https://arxiv.org/abs/2505.17016)
*Shuhan Tan,Kairan Dou,Yue Zhao,Philipp Krähenbühl*

Key words: VLA模型,强化学习,后训练,动态采样,鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: RIPT-VLA是一种通过强化学习微调预训练视觉语言动作模型的简单可扩展方法，仅需稀疏的二分类成功奖励，适用于低数据环境。

Motivation: 现有VLA模型过于依赖离线专家数据和监督模仿，难以适应低数据环境的新任务。

Method: 采用动态采样和留一优势估计的稳定策略优化算法进行交互式后训练。

Result: 在QueST模型上提升21.2%，7B OpenVLA-OFT模型达到97.5%成功率。仅需一次演示即可从4%成功率提升至97%。

Conclusion: RIPT-VLA是一种高效且通用的VLA模型后训练方法。

Abstract: We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based
interactive post-training paradigm that fine-tunes pretrained
Vision-Language-Action (VLA) models using only sparse binary success rewards.
Existing VLA training pipelines rely heavily on offline expert demonstration
data and supervised imitation, limiting their ability to adapt to new tasks and
environments under low-data regimes. RIPT-VLA addresses this by enabling
interactive post-training with a stable policy optimization algorithm based on
dynamic rollout sampling and leave-one-out advantage estimation.
  RIPT-VLA has the following characteristics. First, it applies to various VLA
models, resulting in an improvement on the lightweight QueST model by 21.2%,
and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it
is computationally efficient and data-efficient: with only one demonstration,
RIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success
rate within 15 iterations. Furthermore, we demonstrate that the policy learned
by RIPT-VLA generalizes across different tasks and scenarios and is robust to
the initial state context. These results highlight RIPT-VLA as a practical and
effective paradigm for post-training VLA models through minimal supervision.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [254] [Bandit based Dynamic Candidate Edge Selection in Solving Traveling Salesman Problems](https://arxiv.org/abs/2505.15862)
*Long Wanga,Jiongzhi Zheng,Zhengda Xiong,ChuMin Li,Kun He*

Key words: 路由算法, TSP, 多臂老虎机, 候选边集, LKH

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了通过动态扩展候选边集并结合多臂老虎机模型来改进经典LKH算法，以解决静态候选边集导致的局部最优问题，并在多种TSP基准测试中表现出色。

Motivation: 传统路由算法（如LKH）依赖静态候选边集，容易陷入局部最优。希望通过动态选择候选边提升搜索效率和解决方案质量。

Method: 提出了结合多臂老虎机模型动态选择候选边的方法，扩展了候选边集，并应用于LKH和LKH-3算法。

Result: 实验表明，该方法显著提升了LKH和LKH-3在多种TSP基准测试及其变体中的性能。

Conclusion: 通过动态候选边选择优化了局部搜索能力，显著提升了路由算法的解质量和效率。

Abstract: Algorithms designed for routing problems typically rely on high-quality
candidate edges to guide their search, aiming to reduce the search space and
enhance the search efficiency. However, many existing algorithms, like the
classical Lin-Kernighan-Helsgaun (LKH) algorithm for the Traveling Salesman
Problem (TSP), often use predetermined candidate edges that remain static
throughout local searches. This rigidity could cause the algorithm to get
trapped in local optima, limiting its potential to find better solutions. To
address this issue, we propose expanding the candidate sets to include other
promising edges, providing them an opportunity for selection. Specifically, we
incorporate multi-armed bandit models to dynamically select the most suitable
candidate edges in each iteration, enabling LKH to make smarter choices and
lead to improved solutions. Extensive experiments on multiple TSP benchmarks
show the excellent performance of our method. Moreover, we employ this
bandit-based method to LKH-3, an extension of LKH tailored for solving various
TSP variant problems, and our method also significantly enhances LKH-3's
performance across typical TSP variants.

</details>


### [255] [PhyX: Does Your Model Have the "Wits" for Physical Reasoning?](https://arxiv.org/abs/2505.15929)
*Hui Shen,Taiqiang Wu,Qi Han,Yunta Hsieh,Jizhou Wang,Yuyue Zhang,Yuxin Cheng,Zijian Hao,Yuansheng Ni,Xin Wang,Zhongwei Wan,Kai Zhang,Wendong Xu,Jing Xiong,Ping Luo,Wenhu Chen,Chaofan Tao,Zhuoqing Mao,Ngai Wong*

Key words: 物理推理，基准测试，多模态问题，模型评估

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了PhyX基准测试，用于评估模型在物理推理方面的能力，发现当前最先进的模型表现不佳，并揭示了其局限性。

Motivation: 现有基准测试未能捕捉物理推理这一关键智能能力，因此作者提出了PhyX基准测试以填补这一空白。

Method: 通过精心设计的多模态问题（涵盖多种物理领域和推理类型）评估模型，并采用多维度分析方法。

Result: 当前最先进模型（如GPT-4o、Claude3.7-Sonnet）在物理推理任务上表现较差（准确率32.5%-45.8%），远低于人类专家水平。

Conclusion: 研究揭示了当前模型的局限性，并提出了一种可复现的评估协议，为未来研究提供了方向。

Abstract: Existing benchmarks fail to capture a crucial aspect of intelligence:
physical reasoning, the integrated ability to combine domain knowledge,
symbolic reasoning, and understanding of real-world constraints. To address
this gap, we introduce PhyX: the first large-scale benchmark designed to assess
models capacity for physics-grounded reasoning in visual scenarios. PhyX
includes 3K meticulously curated multimodal questions spanning 6 reasoning
types across 25 sub-domains and 6 core physics domains: thermodynamics,
electromagnetism, mechanics, modern physics, optics, and wave\&acoustics. In
our comprehensive evaluation, even state-of-the-art models struggle
significantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and
GPT-o4-mini achieve only 32.5\%, 42.2\%, and 45.8\% accuracy
respectively-performance gaps exceeding 29\% compared to human experts. Our
analysis exposes critical limitations in current models: over-reliance on
memorized disciplinary knowledge, excessive dependence on mathematical
formulations, and surface-level visual pattern matching rather than genuine
physical understanding. We provide in-depth analysis through fine-grained
statistics, detailed case studies, and multiple evaluation paradigms to
thoroughly examine physical reasoning capabilities. To ensure reproducibility,
we implement a compatible evaluation protocol based on widely-used toolkits
such as VLMEvalKit, enabling one-click evaluation.

</details>


### [256] [Exploring Flow-Lenia Universes with a Curiosity-driven AI Scientist: Discovering Diverse Ecosystem Dynamics](https://arxiv.org/abs/2505.15998)
*Thomas Michel,Marko Cvjetko,Gautier Hamon,Pierre-Yves Oudeyer,Clément Moulin-Frier*

Key words: Flow-Lenia, 自组织, 细胞自动机, 好奇心驱动AI, IMGEPs, 生态系统动态

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一种自动化发现Flow-Lenia（一种连续细胞自动机）系统级动态的方法，使用好奇心驱动的AI科学家。该方法旨在揭示细胞自动机中自组织的进化和生态系统动态过程，扩展了之前的工作，通过多样性搜索算法在更大环境中发现交互模式，并利用模拟范围指标驱动探索。实验证明其比随机搜索能发现更多样化的动态，并展示了复杂的集体行为自组织。

Motivation: 探索细胞自动机中的自组织进化和生态系统动态，扩展现有方法以发现更大环境中的交互模式。

Method: 使用好奇心驱动的AI科学家（IMGEPs）结合多样性搜索算法和模拟范围指标（如进化活动、基于压缩的复杂性和多尺度熵）进行探索。

Result: 展示了两组实验，证明其比随机搜索能发现更多样化的动态，并揭示了复杂集体行为的自组织。

Conclusion: 该方法不仅适用于Flow-Lenia，还为其他参数化复杂系统提供了研究框架，有助于理解涌现的集体属性。

Abstract: We present a method for the automated discovery of system-level dynamics in
Flow-Lenia$-$a continuous cellular automaton (CA) with mass conservation and
parameter localization$-$using a curiosity-driven AI scientist. This method
aims to uncover processes leading to self-organization of evolutionary and
ecosystemic dynamics in CAs. We build on previous work which uses diversity
search algorithms in Lenia to find self-organized individual patterns, and
extend it to large environments that support distinct interacting patterns. We
adapt Intrinsically Motivated Goal Exploration Processes (IMGEPs) to drive
exploration of diverse Flow-Lenia environments using simulation-wide metrics,
such as evolutionary activity, compression-based complexity, and multi-scale
entropy. We test our method in two experiments, showcasing its ability to
illuminate significantly more diverse dynamics compared to random search. We
show qualitative results illustrating how ecosystemic simulations enable
self-organization of complex collective behaviors not captured by previous
individual pattern search and analysis. We complement automated discovery with
an interactive exploration tool, creating an effective human-AI collaborative
workflow for scientific investigation. Though demonstrated specifically with
Flow-Lenia, this methodology provides a framework potentially applicable to
other parameterizable complex systems where understanding emergent collective
properties is of interest.

</details>


### [257] [Children's Mental Models of AI Reasoning: Implications for AI Literacy Education](https://arxiv.org/abs/2505.16031)
*Aayushi Dangol,Robert Wolfe,Runhua Zhao,JaeWon Kim,Trushaa Ramanan,Katie Davis,Julie A. Kientz*

Key words: AI教育, 儿童认知模型, 推理算法, 大型推理模型, 可解释AI

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 这篇论文研究儿童对AI推理过程的理解，揭示了不同年龄段儿童的三种AI推理模型（演绎、归纳和固有），并探讨了对AI教育和可解释AI设计的启示。

Motivation: 随着AI（尤其是大型推理模型LRMs）的发展，了解儿童如何理解AI的推理过程对培养其AI素养至关重要。目前关于儿童在这一领域的认知模型知之甚少。

Method: 采用两阶段方法：先与8名儿童进行共同设计，再对106名3-8年级儿童进行实地研究，分析其AI推理模型。

Result: 发现低年级儿童（3-5年级）倾向认为AI的推理源于固有智能，而高年级儿童（6-8年级）更可能将AI视为模式识别器。同时揭示了儿童理解AI推理的三个矛盾点。

Conclusion: 研究结果为AI课程设计和可解释AI工具的开发提供了重要启示，强调需根据儿童认知特点进行分层教育。

Abstract: As artificial intelligence (AI) advances in reasoning capabilities, most
recently with the emergence of Large Reasoning Models (LRMs), understanding how
children conceptualize AI's reasoning processes becomes critical for fostering
AI literacy. While one of the "Five Big Ideas" in AI education highlights
reasoning algorithms as central to AI decision-making, less is known about
children's mental models in this area. Through a two-phase approach, consisting
of a co-design session with 8 children followed by a field study with 106
children (grades 3-8), we identified three models of AI reasoning: Deductive,
Inductive, and Inherent. Our findings reveal that younger children (grades 3-5)
often attribute AI's reasoning to inherent intelligence, while older children
(grades 6-8) recognize AI as a pattern recognizer. We highlight three tensions
that surfaced in children's understanding of AI reasoning and conclude with
implications for scaffolding AI curricula and designing explainable AI tools.

</details>


### [258] [Causal LLM Routing: End-to-End Regret Minimization from Observational Data](https://arxiv.org/abs/2505.16037)
*Asterios Tsiourvas,Wei Sun,Georgia Perakis*

Key words: LLM路由, 因果框架, 观测数据, 决策后悔, 异构成本

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一种基于观测数据的因果端到端框架，通过学习最小化决策后悔的路由策略，优化LLM路由选择。方法包括两种理论基础的替代目标，实验证明其在多个基准测试中优于现有基线。

Motivation: 现有的LLM路由方法通常采用解耦策略，容易产生误差累积，且依赖高成本的完整反馈数据。本文旨在通过学习观测数据（仅记录实际部署模型的结果）来优化路由策略。

Method: 提出因果端到端框架，引入两种替代目标：基于分类的上界和softmax加权的后悔近似。扩展框架以通过区间条件架构处理异构成本偏好。

Result: 在公开基准测试中，该方法优于现有基线，实现了最先进的性能。

Conclusion: 该框架能高效学习路由策略，减少对完整反馈数据的依赖，同时在多种嵌入模型中表现优异。

Abstract: LLM routing aims to select the most appropriate model for each query,
balancing competing performance metrics such as accuracy and cost across a pool
of language models. Prior approaches typically adopt a decoupled strategy,
where the metrics are first predicted and the model is then selected based on
these estimates. This setup is prone to compounding errors and often relies on
full-feedback data, where each query is evaluated by all candidate models,
which is costly to obtain and maintain in practice. In contrast, we learn from
observational data, which records only the outcome of the model actually
deployed. We propose a causal end-to-end framework that learns routing policies
by minimizing decision-making regret from observational data. To enable
efficient optimization, we introduce two theoretically grounded surrogate
objectives: a classification-based upper bound, and a softmax-weighted regret
approximation shown to recover the optimal policy at convergence. We further
extend our framework to handle heterogeneous cost preferences via an
interval-conditioned architecture. Experiments on public benchmarks show that
our method outperforms existing baselines, achieving state-of-the-art
performance across different embedding models.

</details>


### [259] [SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution](https://arxiv.org/abs/2505.16048)
*Philipp D. Siedler*

Key words: 大型语言模型,物理推理,空间推理,拓扑优化,数据集

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文介绍了一个新数据集，用于评估大型语言模型在拓扑优化中的物理和空间推理能力。

Motivation: 传统语言和逻辑基准未能全面评估模型的物理和空间推理能力，因此设计了这一数据集。

Method: 数据集提供2D边界、作用力和支撑条件，要求模型推理材料的最优分布。任务包括填补部分结构的掩码区域和预测完整材料分布。

Result: 该数据集为评估模型的物理和空间推理能力提供了新视角。

Conclusion: 该数据集是传统语言和逻辑基准的有益补充。

Abstract: We introduce a novel dataset designed to benchmark the physical and spatial
reasoning capabilities of Large Language Models (LLM) based on topology
optimization, a method for computing optimal material distributions within a
design space under prescribed loads and supports. In this dataset, LLMs are
provided with conditions such as 2D boundary, applied forces and supports, and
must reason about the resulting optimal material distribution. The dataset
includes a variety of tasks, ranging from filling in masked regions within
partial structures to predicting complete material distributions. Solving these
tasks requires understanding the flow of forces and the required material
distribution under given constraints, without access to simulation tools or
explicit physical models, challenging models to reason about structural
stability and spatial organization. Our dataset targets the evaluation of
spatial and physical reasoning abilities in 2D settings, offering a
complementary perspective to traditional language and logic benchmarks.

</details>


### [260] [How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior](https://arxiv.org/abs/2505.16067)
*Zidi Xiong,Yuping Lin,Wenya Xie,Pengfei He,Jiliang Tang,Himabindu Lakkaraju,Zhen Xiang*

Key words: 大语言模型, 智能体, 内存管理, 经验跟随, 错误传播

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文通过实证研究探讨了内存管理对大语言模型（LLM）智能体长期行为的影响，发现选择性内存增删策略能显著提升性能，并揭示了经验跟随特性带来的挑战。

Motivation: 研究动机在于理解内存管理如何影响LLM智能体的长期任务表现，尤其是内存的增加和删除操作对其行为的影响。

Method: 通过定量分析，研究聚焦于两种基本内存操作（增加和删除），并设计控制实验验证策略的有效性。

Result: 研究发现LLM智能体具有经验跟随特性，但这也导致了错误传播和误导性经验回放的挑战。选择性内存策略能平均提升10%的性能。

Conclusion: 研究为LLM智能体内存系统设计提供了实用指导，并公开了代码以促进进一步研究。

Abstract: Memory is a critical component in large language model (LLM)-based agents,
enabling them to store and retrieve past executions to improve task performance
over time. In this paper, we conduct an empirical study on how memory
management choices impact the LLM agents' behavior, especially their long-term
performance. Specifically, we focus on two fundamental memory operations that
are widely used by many agent frameworks-addition, which incorporates new
experiences into the memory base, and deletion, which selectively removes past
experiences-to systematically study their impact on the agent behavior. Through
our quantitative analysis, we find that LLM agents display an
experience-following property: high similarity between a task input and the
input in a retrieved memory record often results in highly similar agent
outputs. Our analysis further reveals two significant challenges associated
with this property: error propagation, where inaccuracies in past experiences
compound and degrade future performance, and misaligned experience replay,
where outdated or irrelevant experiences negatively influence current tasks.
Through controlled experiments, we show that combining selective addition and
deletion strategies can help mitigate these negative effects, yielding an
average absolute performance gain of 10% compared to naive memory growth.
Furthermore, we highlight how memory management choices affect agents' behavior
under challenging conditions such as task distribution shifts and constrained
memory resources. Our findings offer insights into the behavioral dynamics of
LLM agent memory systems and provide practical guidance for designing memory
components that support robust, long-term agent performance. We also release
our code to facilitate further study.

</details>


### [261] [SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation](https://arxiv.org/abs/2505.16080)
*Jiayue Liu,Zhongchao Yi,Zhengyang Zhou,Qihe Huang,Kuo Yang,Xu Wang,Yang Wang*

Key words: 时空学习、跨域知识、集体智能、模型进化、NeuroAI

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种名为SynEVO的时空网络，通过跨域集体智能和模型进化提升泛化能力，实验显示其跨域泛化能力提升高达42%。

Motivation: 当前时空学习模型通常独立训练，跨域迁移能力有限，而跨域知识共享和模型进化能提升泛化能力。

Method: 提出SynEVO网络，模拟人类课程学习重排样本，设计弹性公共容器和任务无关提取器，并结合自适应动态耦合器实现模型进化。

Result: 实验表明，SynEVO在跨域场景下泛化能力提升高达42%。

Conclusion: SynEVO为知识迁移和适应提供了NeuroAI范式，显著提升跨域性能。

Abstract: Discovering regularities from spatiotemporal systems can benefit various
scientific and social planning. Current spatiotemporal learners usually train
an independent model from a specific source data that leads to limited
transferability among sources, where even correlated tasks requires new design
and training. The key towards increasing cross-domain knowledge is to enable
collective intelligence and model evolution. In this paper, inspired by
neuroscience theories, we theoretically derive the increased information
boundary via learning cross-domain collective intelligence and propose a
Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the
model independence and enables cross-domain knowledge to be shared and
aggregated. Specifically, we first re-order the sample groups to imitate the
human curriculum learning, and devise two complementary learners, elastic
common container and task-independent extractor to allow model growth and
task-wise commonality and personality disentanglement. Then an adaptive dynamic
coupler with a new difference metric determines whether the new sample group
should be incorporated into common container to achieve model evolution under
various domains. Experiments show that SynEVO improves the generalization
capacity by at most 42% under cross-domain scenarios and SynEVO provides a
paradigm of NeuroAI for knowledge transfer and adaptation.

</details>


### [262] [Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development](https://arxiv.org/abs/2505.16086)
*Ming Shen,Raphael Shu,Anurag Pratik,James Gung,Yubin Ge,Monica Sunkara,Yi Zhang*

Key words: 大语言模型,多智能体系统,自然语言反馈,系统提示优化,软件开发

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文研究如何优化基于大语言模型（LLM）的多智能体系统中角色分配问题，通过自然语言反馈改进系统提示优化方法。

Motivation: 尽管大语言模型在多智能体系统中表现出色，但如何优化这些系统仍具挑战性。本研究旨在通过自然语言反馈改进角色分配的多智能体系统。

Method: 提出两步优化流程：首先通过文本反馈识别表现不佳的智能体及其失败原因，然后优化这些智能体的系统提示；同时比较在线/离线优化和个体/群体优化的效果，研究了单次和多次提示优化策略。

Result: 证明了该优化方法在软件开发任务中的有效性，并探讨了不同优化设置对多智能体系统群体行为的影响。

Conclusion: 研究为未来多智能体系统的开发提供了实用见解，优化方法在多样化的评估维度上表现良好。

Abstract: We have seen remarkable progress in large language models (LLMs) empowered
multi-agent systems solving complex tasks necessitating cooperation among
experts with diverse skills. However, optimizing LLM-based multi-agent systems
remains challenging. In this work, we perform an empirical case study on group
optimization of role-based multi-agent systems utilizing natural language
feedback for challenging software development tasks under various evaluation
dimensions. We propose a two-step agent prompts optimization pipeline:
identifying underperforming agents with their failure explanations utilizing
textual feedback and then optimizing system prompts of identified agents
utilizing failure explanations. We then study the impact of various
optimization settings on system performance with two comparison groups: online
against offline optimization and individual against group optimization. For
group optimization, we study two prompting strategies: one-pass and multi-pass
prompting optimizations. Overall, we demonstrate the effectiveness of our
optimization method for role-based multi-agent systems tackling software
development tasks evaluated on diverse evaluation dimensions, and we
investigate the impact of diverse optimization settings on group behaviors of
the multi-agent systems to provide practical insights for future development.

</details>


### [263] [Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance](https://arxiv.org/abs/2505.16090)
*Dominick Kubica,Dylan T. Gordon,Nanami Emura,Derleen Saini,Charlie Goldenberg*

Key words: 生成式人工智能,大型语言模型,情感分析,金融文本,提示工程

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文研究了生成式人工智能（GenAI）在金融领域的应用，评估了大型语言模型（LLMs）在金融文本情感分析中的表现，并考察了提示工程对提高分析结果的影响。

Motivation: 随着GenAI在金融等高要求领域的广泛应用，确保其输出的可靠性和准确性变得尤为重要，特别是在处理金融披露中复杂且有战略模糊性的语言时。

Method: 通过比较微软Copilot、OpenAI的ChatGPT、Google的Gemini和传统机器学习模型在财务情感分析中的表现，结合微软财报会议记录数据，评估了模型的表现和市场情绪的相关性。

Result: 研究发现LLMs在处理金融文本的情感分析时，尤其是在有战略模糊性的语言中，存在困难。同时提示工程有助于提升分析结果。

Conclusion: LLMs在金融情感分析中仍有局限性，尤其在处理战略模糊性语言时，提示工程是一种有效的改进方法。

Abstract: As of 2025, Generative Artificial Intelligence (GenAI) has become a central
tool for productivity across industries. Beyond text generation, GenAI now
plays a critical role in coding, data analysis, and research workflows. As
large language models (LLMs) continue to evolve, it is essential to assess the
reliability and accuracy of their outputs, especially in specialized,
high-stakes domains like finance. Most modern LLMs transform text into
numerical vectors, which are used in operations such as cosine similarity
searches to generate responses. However, this abstraction process can lead to
misinterpretation of emotional tone, particularly in nuanced financial
contexts. While LLMs generally excel at identifying sentiment in everyday
language, these models often struggle with the nuanced, strategically ambiguous
language found in earnings call transcripts. Financial disclosures frequently
embed sentiment in hedged statements, forward-looking language, and
industry-specific jargon, making it difficult even for human analysts to
interpret consistently, let alone AI models. This paper presents findings from
the Santa Clara Microsoft Practicum Project, led by Professor Charlie
Goldenberg, which benchmarks the performance of Microsoft's Copilot, OpenAI's
ChatGPT, Google's Gemini, and traditional machine learning models for sentiment
analysis of financial text. Using Microsoft earnings call transcripts, the
analysis assesses how well LLM-derived sentiment correlates with market
sentiment and stock movements and evaluates the accuracy of model outputs.
Prompt engineering techniques are also examined to improve sentiment analysis
results. Visualizations of sentiment consistency are developed to evaluate
alignment between tone and stock performance, with sentiment trends analyzed
across Microsoft's lines of business to determine which segments exert the
greatest influence.

</details>


### [264] [TrialPanorama: Database and Benchmark for Systematic Review and Design of Clinical Trials](https://arxiv.org/abs/2505.16097)
*Zifeng Wang,Qiao Jin,Jiacheng Lin,Junyi Gao,Jathurshan Pradeepkumar,Pengcheng Jiang,Benjamin Danek,Zhiyong Lu,Jimeng Sun*

Key words: AI, clinical trials, database, benchmark, large language models, ontology

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: TrialPanorama is a large-scale, structured clinical trial database designed to support AI applications in clinical trials, including task benchmarking using LLMs.

Motivation: To address the need for a solid data foundation in AI development for clinical trials by providing a structured and ontology-linked database.

Method: Creation of TrialPanorama, a database of 1,657,476 clinical trial records from 15 sources, linked to biomedical ontologies, and derivation of benchmark tasks for evaluation.

Result: General-purpose LLMs show limited zero-shot capability for high-stakes clinical trial tasks, highlighting the need for specialized AI solutions.

Conclusion: TrialPanorama serves as a valuable resource for AI research in clinical trials, with released database and benchmarks to aid future studies.

Abstract: Developing artificial intelligence (AI) for vertical domains requires a solid
data foundation for both training and evaluation. In this work, we introduce
TrialPanorama, a large-scale, structured database comprising 1,657,476 clinical
trial records aggregated from 15 global sources. The database captures key
aspects of trial design and execution, including trial setups, interventions,
conditions, biomarkers, and outcomes, and links them to standard biomedical
ontologies such as DrugBank and MedDRA. This structured and ontology-grounded
design enables TrialPanorama to serve as a unified, extensible resource for a
wide range of clinical trial tasks, including trial planning, design, and
summarization. To demonstrate its utility, we derive a suite of benchmark tasks
directly from the TrialPanorama database. The benchmark spans eight tasks
across two categories: three for systematic review (study search, study
screening, and evidence summarization) and five for trial design (arm design,
eligibility criteria, endpoint selection, sample size estimation, and trial
completion assessment). The experiments using five state-of-the-art large
language models (LLMs) show that while general-purpose LLMs exhibit some
zero-shot capability, their performance is still inadequate for high-stakes
clinical trial workflows. We release TrialPanorama database and the benchmark
to facilitate further research on AI for clinical trials.

</details>


### [265] [BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research](https://arxiv.org/abs/2505.16100)
*Zifeng Wang,Benjamin Danek,Jimeng Sun*

Key words: AI, 生物医学, 假设验证, 基准, 数据分析

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: BioDSA-1K是一个评估AI在生物医学假设验证任务表现的基准，包含1029个任务，基于300多项研究。

Motivation: 解决AI在复杂真实数据分析和证据解释中验证科学假设的困难。

Method: 构建包含假设、分析计划和实证数据的任务集，评估四个维度：假设准确性、证据与结论一致性、推理正确性及代码可执行性。

Result: BioDSA-1K支持评估通用、可信的AI代理，包括无法验证假设的实际场景。

Conclusion: BioDSA-1K为构建和评估生物医学发现的AI代理提供了基础。

Abstract: Validating scientific hypotheses is a central challenge in biomedical
research, and remains difficult for artificial intelligence (AI) agents due to
the complexity of real-world data analysis and evidence interpretation. In this
work, we present BioDSA-1K, a benchmark designed to evaluate AI agents on
realistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K
consists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans,
curated from over 300 published biomedical studies to reflect the structure and
reasoning found in authentic research workflows. Each task includes a
structured hypothesis derived from the original study's conclusions, expressed
in the affirmative to reflect the language of scientific reporting, and one or
more pieces of supporting evidence grounded in empirical data tables. While
these hypotheses mirror published claims, they remain testable using standard
statistical or machine learning methods. The benchmark enables evaluation along
four axes: (1) hypothesis decision accuracy, (2) alignment between evidence and
conclusion, (3) correctness of the reasoning process, and (4) executability of
the AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable
hypotheses: cases where the available data are insufficient to support or
refute a claim, reflecting a common yet underexplored scenario in real-world
science. We propose BioDSA-1K as a foundation for building and evaluating
generalizable, trustworthy AI agents for biomedical discovery.

</details>


### [266] [Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language](https://arxiv.org/abs/2505.16114)
*Naiqi Li,Peiyuan Liu,Zheng Liu,Tao Dai,Yong Jiang,Shu-Tao Xia*

Key words: Logic-of-Thought, 逻辑编程, LLMs, 自然语言推理, 谜题求解

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出Logic-of-Thought (Logot)框架，将LLMs与逻辑编程结合，解决自然语言谜题的推理问题。

Motivation: 解决LLMs在复杂谜题推理上的不足，结合自然语言理解与精确逻辑推理。

Method: 利用LLMs将谜题规则和状态转换为ASP程序，通过ASP解释器高效求解。

Result: 在网格谜题和动态谜题上实现近乎完美的准确率。

Conclusion: Logot框架有效结合LLMs与逻辑编程，提升复杂谜题的推理能力。

Abstract: Solving puzzles in natural language poses a long-standing challenge in AI.
While large language models (LLMs) have recently shown impressive capabilities
in a variety of tasks, they continue to struggle with complex puzzles that
demand precise reasoning and exhaustive search. In this paper, we propose
Logic-of-Thought (Logot), a novel framework that bridges LLMs with logic
programming to address this problem. Our method leverages LLMs to translate
puzzle rules and states into answer set programs (ASPs), the solution of which
are then accurately and efficiently inferred by an ASP interpreter. This hybrid
approach combines the natural language understanding of LLMs with the precise
reasoning capabilities of logic programs. We evaluate our method on various
grid puzzles and dynamic puzzles involving actions, demonstrating near-perfect
accuracy across all tasks. Our code and data are available at:
https://github.com/naiqili/Logic-of-Thought.

</details>


### [267] [LLM-Powered AI Agent Systems and Their Applications in Industry](https://arxiv.org/abs/2505.16120)
*Guannan Liang,Qianqian Tong*

Key words: 大型语言模型,代理系统,多模态处理,跨领域推理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文探讨了大型语言模型（LLM）如何改变代理系统，分析了其优势与当前面临的挑战，并提出了解决方案。

Motivation: 研究LLM如何增强代理系统的灵活性、跨领域推理和多模态处理能力，以应对传统系统的局限性。

Method: 通过分类比较LLM代理系统（软件、物理和混合系统），分析其应用领域和存在的挑战。

Result: 总结了LLM代理系统在多领域的应用潜力，并指出了高延迟、不确定性、评估指标缺乏等关键问题。

Conclusion: LLM代理系统具有巨大潜力，但仍需解决技术和安全挑战以充分发挥其价值。

Abstract: The emergence of Large Language Models (LLMs) has reshaped agent systems.
Unlike traditional rule-based agents with limited task scope, LLM-powered
agents offer greater flexibility, cross-domain reasoning, and natural language
interaction. Moreover, with the integration of multi-modal LLMs, current agent
systems are highly capable of processing diverse data modalities, including
text, images, audio, and structured tabular data, enabling richer and more
adaptive real-world behavior. This paper comprehensively examines the evolution
of agent systems from the pre-LLM era to current LLM-powered architectures. We
categorize agent systems into software-based, physical, and adaptive hybrid
systems, highlighting applications across customer service, software
development, manufacturing automation, personalized education, financial
trading, and healthcare. We further discuss the primary challenges posed by
LLM-powered agents, including high inference latency, output uncertainty, lack
of evaluation metrics, and security vulnerabilities, and propose potential
solutions to mitigate these concerns.

</details>


### [268] [Sudoku-Bench: Evaluating creative reasoning with Sudoku variants](https://arxiv.org/abs/2505.16135)
*Jeffrey Seely,Yuki Imajuku,Tianyu Zhao,Edoardo Cetin,Llion Jones*

Key words: LLMs, 推理基准, 数独变体, 创造性推理, 多步逻辑

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出Sudoku-Bench基准测试，挑战传统LLMs在创造性、多步逻辑推理上的不足，通过独特且多样化的数独变体评估模型能力。

Motivation: 现有推理基准常依赖记忆而非真实创造力，需一种能评估多步逻辑创新能力的测试。

Method: 设计Sudoku-Bench，包含精选的多样化数独变体、标准化文本表示及兼容工具，确保评估清晰一致。

Result: 实验显示当前先进LLMs在无辅助下解题率低于15%，表明长程战略推理能力有待提升。

Conclusion: Sudoku-Bench为评估和推动LLMs创造性推理提供有效工具，揭示了其推理能力的局限性。

Abstract: Existing reasoning benchmarks for large language models (LLMs) frequently
fail to capture authentic creativity, often rewarding memorization of
previously observed patterns. We address this shortcoming with Sudoku-Bench, a
curated benchmark of challenging and unconventional Sudoku variants
specifically selected to evaluate creative, multi-step logical reasoning.
Sudoku variants form an unusually effective domain for reasoning research: each
puzzle introduces unique or subtly interacting constraints, making memorization
infeasible and requiring solvers to identify novel logical breakthroughs
(``break-ins''). Despite their diversity, Sudoku variants maintain a common and
compact structure, enabling clear and consistent evaluation. Sudoku-Bench
includes a carefully chosen puzzle set, a standardized text-based puzzle
representation, and flexible tools compatible with thousands of publicly
available puzzles -- making it easy to extend into a general research
environment. Baseline experiments show that state-of-the-art LLMs solve fewer
than 15\% of puzzles unaided, highlighting significant opportunities to advance
long-horizon, strategic reasoning capabilities.

</details>


### [269] [Losing is for Cherishing: Data Valuation Based on Machine Unlearning and Shapley Value](https://arxiv.org/abs/2505.16147)
*Le Ma,Shirao Yang,Zihao Wang,Yinggui Wang,Lei Wang,Tao Wei,Kejun Zhang*

Key words: 数据估值,Shapley值,机器遗忘,蒙特卡洛采样,大型模型

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一种名为Unlearning Shapley的新框架，利用机器遗忘技术高效估计数据价值，解决了传统方法计算成本高的问题。

Motivation: 大型模型的普及使得高效数据估值方法需求增加，传统方法如Shapley值计算成本高，且难以实现部分数据估值。

Method: 通过从预训练模型中遗忘目标数据，并在测试集上测量性能变化，使用蒙特卡洛采样计算Shapley值。

Result: 实验表明，该方法在保持精度的同时，显著降低了计算开销，且能有效估计数据子集的真实影响。

Conclusion: 该框架弥合了数据估值理论与实际应用之间的差距，为现代AI生态系统提供了可扩展且隐私合规的解决方案。

Abstract: The proliferation of large models has intensified the need for efficient data
valuation methods to quantify the contribution of individual data providers.
Traditional approaches, such as game-theory-based Shapley value and
influence-function-based techniques, face prohibitive computational costs or
require access to full data and model training details, making them hardly
achieve partial data valuation. To address this, we propose Unlearning Shapley,
a novel framework that leverages machine unlearning to estimate data values
efficiently. By unlearning target data from a pretrained model and measuring
performance shifts on a reachable test set, our method computes Shapley values
via Monte Carlo sampling, avoiding retraining and eliminating dependence on
full data. Crucially, Unlearning Shapley supports both full and partial data
valuation, making it scalable for large models (e.g., LLMs) and practical for
data markets. Experiments on benchmark datasets and large-scale text corpora
demonstrate that our approach matches the accuracy of state-of-the-art methods
while reducing computational overhead by orders of magnitude. Further analysis
confirms a strong correlation between estimated values and the true impact of
data subsets, validating its reliability in real-world scenarios. This work
bridges the gap between data valuation theory and practical deployment,
offering a scalable, privacy-compliant solution for modern AI ecosystems.

</details>


### [270] [Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning](https://arxiv.org/abs/2505.16176)
*Jun Rao,Xuebo Liu,Hexuan Deng,Zepeng Lin,Zixiong Yu,Jiansheng Wei,Xiaojun Meng,Min Zhang*

Key words: 动态数据选择, 模型自适应, 数学推理, SAI-DPO, 在线强化学习

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种动态数据选择算法SAI-DPO，通过实时评估模型在不同训练阶段的推理能力，动态调整数据选择策略，显著提升了模型在数学推理任务中的表现。

Motivation: 现有的数据选择方法依赖静态预定义指标（如难度和多样性），无法适应动态训练过程。SAI-DPO旨在解决这一问题，通过动态调整数据选择以适应模型能力的演变。

Method: SAI-DPO算法利用实时模型性能反馈，动态调整数据选择策略，专注模型当前阶段的推理能力，优化数据利用效率和任务表现。

Result: 在三个先进模型和八个数学推理基准测试中，SAI-DPO平均性能提升高达21.3个百分点，尤其在AIME24和AMC23数据集上分别提升了10和15个百分点。

Conclusion: 动态、模型自适应的数据选择策略优于静态方法，能显著提升推理任务的性能。

Abstract: In the realm of data selection for reasoning tasks, existing approaches
predominantly rely on externally predefined static metrics such as difficulty
and diversity, which are often designed for supervised fine-tuning (SFT) and
lack adaptability to continuous training processes. A critical limitation of
these methods is their inability to dynamically align with the evolving
capabilities of models during online training, a gap that becomes increasingly
pronounced with the rise of dynamic training paradigms and online reinforcement
learning (RL) frameworks (e.g., R1 models). To address this, we introduce
SAI-DPO, an algorithm that dynamically selects training data by continuously
assessing a model's stage-specific reasoning abilities across different
training phases. By integrating real-time model performance feedback, SAI-DPO
adaptively adapts data selection to the evolving strengths and weaknesses of
the model, thus enhancing both data utilization efficiency and final task
performance. Extensive experiments on three state-of-the-art models and eight
mathematical reasoning benchmarks, including challenging competition-level
datasets (e.g., AIME24 and AMC23), demonstrate that SAI-DPO achieves an average
performance boost of up to 21.3 percentage points, with particularly notable
improvements of 10 and 15 points on AIME24 and AMC23, respectively. These
results highlight the superiority of dynamic, model-adaptive data selection
over static, externally defined strategies in advancing reasoning.

</details>


### [271] [SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning](https://arxiv.org/abs/2505.16186)
*Kaiwen Zhou,Xuandong Zhao,Gaowen Liu,Jayanth Srinivasa,Aosong Feng,Dawn Song,Xin Eric Wang*

Key words: 大推理模型、安全推理、监督微调、越狱攻击、SafeKey

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了SafeKey方法，通过增强大推理模型（LRMs）在关键句子中的安全意识来提升安全性，尤其针对未知的越狱攻击和有害提示。

Motivation: 现有的大推理模型（LRMs）在面对有害查询和对抗攻击时存在安全风险，已提出的监督微调（SFT）方法难以泛化到未见过的越狱提示。

Method: 提出了SafeKey方法，包括双路径安全头（Dual-Path Safety Head）和查询掩码建模（Query-Mask Modeling）两个目标，以激活关键句子中的安全意识。

Result: 实验表明，该方法安全泛化能力显著提升，平均有害率降低9.6%，且不影响模型的通用能力。

Conclusion: SafeKey通过重塑注意力机制和提升隐藏表征质量，有效增强了模型的安全性。

Abstract: Large Reasoning Models (LRMs) introduce a new generation paradigm of
explicitly reasoning before answering, leading to remarkable improvements in
complex tasks. However, they pose great safety risks against harmful queries
and adversarial attacks. While recent mainstream safety efforts on LRMs,
supervised fine-tuning (SFT), improve safety performance, we find that
SFT-aligned models struggle to generalize to unseen jailbreak prompts. After
thorough investigation of LRMs' generation, we identify a safety aha moment
that can activate safety reasoning and lead to a safe response. This aha moment
typically appears in the `key sentence', which follows models' query
understanding process and can indicate whether the model will proceed safely.
Based on these insights, we propose SafeKey, including two complementary
objectives to better activate the safety aha moment in the key sentence: (1) a
Dual-Path Safety Head to enhance the safety signal in the model's internal
representations before the key sentence, and (2) a Query-Mask Modeling
objective to improve the models' attention on its query understanding, which
has important safety hints. Experiments across multiple safety benchmarks
demonstrate that our methods significantly improve safety generalization to a
wide range of jailbreak attacks and out-of-distribution harmful prompts,
lowering the average harmfulness rate by 9.6\%, while maintaining general
abilities. Our analysis reveals how SafeKey enhances safety by reshaping
internal attention and improving the quality of hidden representations.

</details>


### [272] [Velocity Completion Task and Method for Event-based Player Positional Data in Soccer](https://arxiv.org/abs/2505.16199)
*Rikuhei Umemoto,Keisuke Fujii*

Key words: 多智能体系统, 团队运动, 速度补全, 神经网络, 动态分析

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出一种基于神经网络的方法，利用事件位置数据补全多智能体系统的速度信息，提升团队运动动态分析的深度。

Motivation: 事件位置数据缺乏连续时间信息，难以直接计算速度等关键属性，限制了动态分析的深度和团队策略的理解。

Method: 提出神经网络方法，利用事件位置数据补全所有智能体的速度，并研究其应用于团队运动分析的潜力。

Result: 神经网络方法在速度补全误差上优于规则方法，且基于补全速度的空间评估结果更接近完整跟踪数据。

Conclusion: 该方法有效补全速度信息，增强团队运动系统分析的潜力。

Abstract: In many real-world complex systems, the behavior can be observed as a
collection of discrete events generated by multiple interacting agents.
Analyzing the dynamics of these multi-agent systems, especially team sports,
often relies on understanding the movement and interactions of individual
agents. However, while providing valuable snapshots, event-based positional
data typically lacks the continuous temporal information needed to directly
calculate crucial properties such as velocity. This absence severely limits the
depth of dynamic analysis, preventing a comprehensive understanding of
individual agent behaviors and emergent team strategies. To address this
challenge, we propose a new method to simultaneously complete the velocity of
all agents using only the event-based positional data from team sports. Based
on this completed velocity information, we investigate the applicability of
existing team sports analysis and evaluation methods. Experiments using soccer
event data demonstrate that neural network-based approaches outperformed
rule-based methods regarding velocity completion error, considering the
underlying temporal dependencies and graph structure of player-to-player or
player-to-ball interaction. Moreover, the space evaluation results obtained
using the completed velocity are closer to those derived from complete tracking
data, highlighting our method's potential for enhanced team sports system
analysis.

</details>


### [273] [LightRouter: Towards Efficient LLM Collaboration with Minimal Overhead](https://arxiv.org/abs/2505.16221)
*Yifan Zhang,Xinkui Zhao,Zuxin Wang,Guanjie Cheng,Yueshen Xu,Shuiguang Deng,Jianwei Yin*

Key words: 大型语言模型, 模型选择, 成本效率, 自适应机制, 集成策略

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: LightRouter是一个用于高效选择和集成大型语言模型（LLM）的框架，旨在同时优化任务性能和成本效率，无需预先了解单个模型，仅使用轻量级模型。

Motivation: 现有大型语言模型（LLM）在成本、性能和计算需求方面差异显著，用户难以选择最适合特定任务的模型。

Method: LightRouter通过自适应选择机制减少启动令牌数量以降低成本，并采用有效的集成策略组合模型输出。

Result: 在多个基准测试中，LightRouter性能媲美或优于广泛使用的集成方法，准确率提升高达25%，推理成本降低27%。

Conclusion: LightRouter提供了一种高效LLM选择和集成的实用方法，为模型组合的最优策略提供了有价值的见解。

Abstract: The rapid advancement of large language models has unlocked remarkable
capabilities across a diverse array of natural language processing tasks.
However, the considerable differences among available LLMs-in terms of cost,
performance, and computational demands-pose significant challenges for users
aiming to identify the most suitable model for specific tasks. In this work, we
present LightRouter, a novel framework designed to systematically select and
integrate a small subset of LLMs from a larger pool, with the objective of
jointly optimizing both task performance and cost efficiency. LightRouter
leverages an adaptive selection mechanism to identify models that require only
a minimal number of boot tokens, thereby reducing costs, and further employs an
effective integration strategy to combine their outputs. Extensive experiments
across multiple benchmarks demonstrate that LightRouter matches or outperforms
widely-used ensemble baselines, achieving up to a 25% improvement in accuracy.
Compared with leading high-performing models, LightRouter achieves comparable
performance while reducing inference costs by up to 27%. Importantly, our
framework operates without any prior knowledge of individual models and relies
exclusively on inexpensive, lightweight models. This work introduces a
practical approach for efficient LLM selection and provides valuable insights
into optimal strategies for model combination.

</details>


### [274] [MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network](https://arxiv.org/abs/2505.16223)
*Sangyong Lee,Subo Hwang,Dohoon Kim*

Key words: 异常检测, 自监督聚类, 模型无关, 超球崩溃, 单向自适应损失

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: MADCluster是一个新颖的模型无关异常检测框架，利用自监督聚类解决现有深度学习方法中的'超球崩溃'问题，通过优化新提出的'单向自适应损失'实现高效单聚类。实验证明其在多种架构中提升性能。

Motivation: 现有的基于深度学习的异常检测方法存在'超球崩溃'问题，限制了其性能。MADCluster旨在通过自监督聚类和模型无关特性解决这一问题。

Method: MADCluster包含三个核心组件：捕获高维时序动态的Base Embedder、Cluster Distance Mapping和Sequence-wise Clustering。通过优化'单向自适应损失'实现高效单聚类。

Result: 在四个时间序列基准数据集上的实验表明，MADCluster显著提升了对比模型的整体性能。

Conclusion: MADCluster的兼容性展现了其在多种架构中提升模型性能的潜力。

Abstract: In this paper, we propose MADCluster, a novel model-agnostic anomaly
detection framework utilizing self-supervised clustering. MADCluster is
applicable to various deep learning architectures and addresses the
'hypersphere collapse' problem inherent in existing deep learning-based anomaly
detection methods. The core idea is to cluster normal pattern data into a
'single cluster' while simultaneously learning the cluster center and mapping
data close to this center. Also, to improve expressiveness and enable effective
single clustering, we propose a new 'One-directed Adaptive loss'. The
optimization of this loss is mathematically proven. MADCluster consists of
three main components: Base Embedder capturing high-dimensional temporal
dynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous
center updates. Its model-agnostic characteristics are achieved by applying
various architectures to the Base Embedder. Experiments on four time series
benchmark datasets demonstrate that applying MADCluster improves the overall
performance of comparative models. In conclusion, the compatibility of
MADCluster shows potential for enhancing model performance across various
architectures.

</details>


### [275] [MAPLE: Many-Shot Adaptive Pseudo-Labeling for In-Context Learning](https://arxiv.org/abs/2505.16225)
*Zihan Chen,Song Wang,Zhen Tan,Jundong Li,Cong Shen*

Key words: In-Context Learning, LLMs, Many-Shot ICL, Pseudo-Labeling, MAPLE

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: MAPLE是一种新型的基于影响力的多样本上下文学习框架，通过伪标记样本解决标注数据不足的问题，提升大型语言模型性能。

Motivation: 解决大型语言模型在多样本上下文学习中因标注数据获取成本高而受限的问题。

Method: 提出MAPLE框架，通过伪标记和自适应选择影响大的未标记样本，为测试查询定制输入。

Result: 实验证明MAPLE能有效提升模型适应性，并在标注数据有限的情况下提高性能。

Conclusion: MAPLE是高效的多样本上下文学习方法，可减少标注成本并提升性能。

Abstract: In-Context Learning (ICL) empowers Large Language Models (LLMs) to tackle
diverse tasks by incorporating multiple input-output examples, known as
demonstrations, into the input of LLMs. More recently, advancements in the
expanded context windows of LLMs have led to many-shot ICL, which uses hundreds
of demonstrations and outperforms few-shot ICL, which relies on fewer examples.
However, this approach is often hindered by the high cost of obtaining large
amounts of labeled data. To address this challenge, we propose Many-Shot
Adaptive Pseudo-LabEling, namely MAPLE, a novel influence-based many-shot ICL
framework that utilizes pseudo-labeled samples to compensate for the lack of
label information. We first identify a subset of impactful unlabeled samples
and perform pseudo-labeling on them by querying LLMs. These pseudo-labeled
samples are then adaptively selected and tailored to each test query as input
to improve the performance of many-shot ICL, without significant labeling
costs. Extensive experiments on real-world datasets demonstrate the
effectiveness of our framework, showcasing its ability to enhance LLM
adaptability and performance with limited labeled data.

</details>


### [276] [How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance](https://arxiv.org/abs/2505.16276)
*Desiree Heim,Lars-Peter Meyer,Markus Schröder,Johannes Frey,Andreas Dengel*

Key words: Large Language Models, Knowledge Graph Engineering, Scaling Laws, Cost-Effectiveness, Benchmarking

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文通过LLM-KG-Bench框架分析26个开源LLM在KGE任务中的表现，验证了模型规模与性能的一般规律，但也发现了性能平台效应及部分家族内大模型表现不如小模型的异常情况，建议综合考虑成本效益。

Motivation: 探索在知识图谱工程（KGE）任务中，大型语言模型（LLM）的规模扩展规律，并评估性能与成本的平衡。

Method: 使用LLM-KG-Bench框架对比26个开源LLM在KGE任务中的表现，分析模型规模分类下的基准得分变化及家族内模型的性能与规模相关性。

Result: 模型规模扩展规律在KGE任务中基本适用，但存在性能平台效应和家族内异常（大模型表现不如小模型），建议选择高性价比的小模型或测试相邻规模模型。

Conclusion: 虽然模型规模通常与性能正相关，但在KGE任务中需警惕平台效应和家族内异常，权衡成本效益选择模型。

Abstract: When using Large Language Models (LLMs) to support Knowledge Graph
Engineering (KGE), one of the first indications when searching for an
appropriate model is its size. According to the scaling laws, larger models
typically show higher capabilities. However, in practice, resource costs are
also an important factor and thus it makes sense to consider the ratio between
model performance and costs. The LLM-KG-Bench framework enables the comparison
of LLMs in the context of KGE tasks and assesses their capabilities of
understanding and producing KGs and KG queries. Based on a dataset created in
an LLM-KG-Bench run covering 26 open state-of-the-art LLMs, we explore the
model size scaling laws specific to KGE tasks. In our analyses, we assess how
benchmark scores evolve between different model size categories. Additionally,
we inspect how the general score development of single models and families of
models correlates to their size. Our analyses revealed that, with a few
exceptions, the model size scaling laws generally also apply to the selected
KGE tasks. However, in some cases, plateau or ceiling effects occurred, i.e.,
the task performance did not change much between a model and the next larger
model. In these cases, smaller models could be considered to achieve high
cost-effectiveness. Regarding models of the same family, sometimes larger
models performed worse than smaller models of the same family. These effects
occurred only locally. Hence it is advisable to additionally test the next
smallest and largest model of the same family.

</details>


### [277] [No Black Boxes: Interpretable and Interactable Predictive Healthcare with Knowledge-Enhanced Agentic Causal Discovery](https://arxiv.org/abs/2505.16288)
*Xiaoxue Han,Pengfei Hu,Jun-En Ding,Chang Lu,Feng Liu,Yue Ning*

Key words: 电子健康记录, 解释性, 交互性, 因果发现, 大型语言模型

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了II-KEA框架，通过结合知识库和大型语言模型增强医疗诊断模型的解释性和交互性，解决了现有深度学习模型在这两方面的不足。

Motivation: 当前基于电子健康记录（EHR）的深度学习模型虽然在诊断预测中表现出高准确性，但其‘黑盒’特性缺乏解释性和交互性，限制了临床医生的实际应用。

Method: 提出II-KEA框架，整合了个性化知识库和代理驱动的因果发现方法，通过显式推理和因果分析增强解释性，并支持临床医生通过定制知识库和提示注入自身经验。

Result: 在MIMIC-III和MIMIC-IV数据集上的实验表明，II-KEA在性能和解释性、交互性方面均表现优异。

Conclusion: II-KEA通过结合知识库和大型语言模型，显著提升了医疗诊断模型的实用性和临床适用性。

Abstract: Deep learning models trained on extensive Electronic Health Records (EHR)
data have achieved high accuracy in diagnosis prediction, offering the
potential to assist clinicians in decision-making and treatment planning.
However, these models lack two crucial features that clinicians highly value:
interpretability and interactivity. The ``black-box'' nature of these models
makes it difficult for clinicians to understand the reasoning behind
predictions, limiting their ability to make informed decisions. Additionally,
the absence of interactive mechanisms prevents clinicians from incorporating
their own knowledge and experience into the decision-making process. To address
these limitations, we propose II-KEA, a knowledge-enhanced agent-driven causal
discovery framework that integrates personalized knowledge databases and
agentic LLMs. II-KEA enhances interpretability through explicit reasoning and
causal analysis, while also improving interactivity by allowing clinicians to
inject their knowledge and experience through customized knowledge bases and
prompts. II-KEA is evaluated on both MIMIC-III and MIMIC-IV, demonstrating
superior performance along with enhanced interpretability and interactivity, as
evidenced by its strong results from extensive case studies.

</details>


### [278] [EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via Action Pruning](https://arxiv.org/abs/2505.16312)
*Jiawei Liu,Qisi Chen,Jianshu Zhang,Quan Liu,Defu Lian*

Key words: 大型语言模型, 语义等价, 数学推理, 剪枝, token消耗

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了EquivPruner方法，通过识别和剪枝语义等价动作来减少大型语言模型推理搜索中的冗余，显著降低了token消耗并提高了搜索效率和推理准确性。

Motivation: 现有的语义相似性方法在数学推理等特定领域中难以准确识别语义等价步骤，导致大量token消耗。

Method: 提出了EquivPruner方法，识别并剪枝语义等价动作；还创建了MathEquiv数据集，用于训练轻量级等价检测器。

Result: 实验表明，EquivPruner大幅减少token消耗（如在GSM8K任务中减少48.1%），并提升了推理准确性。

Conclusion: EquivPruner是一种简单有效的方法，显著优化了LLM的推理搜索效率和准确性。

Abstract: Large Language Models (LLMs) excel at complex reasoning through search
algorithms, yet current strategies often suffer from massive token consumption
due to redundant exploration of semantically equivalent steps. Existing
semantic similarity methods struggle to accurately identify such equivalence in
domain-specific contexts like mathematical reasoning. To address this, we
propose EquivPruner, a simple yet effective approach that identifies and prunes
semantically equivalent actions during LLM reasoning search. We also introduce
MathEquiv, the first dataset we created for mathematical statement equivalence,
which enables the training of a lightweight equivalence detector. Extensive
experiments across various models and tasks demonstrate that EquivPruner
significantly reduces token consumption, improving searching efficiency and
often bolstering reasoning accuracy. For instance, when applied to
Qwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by
48.1\% while also improving accuracy. Our code is available at
https://github.com/Lolo1222/EquivPruner.

</details>


### [279] [Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2505.16315)
*Xiaoxue Cheng,Junyi Li,Zhenduo Zhang,Xinyu Tang,Wayne Xin Zhao,Xinyu Kong,Zhiqiang Zhang*

Key words: 大型推理模型,自适应认知,强化学习,双系统理论,高效推理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种名为ACPO的强化学习框架，通过自适应认知分配和动态系统切换，减少大型推理模型的冗余推理。

Motivation: 大型推理模型在复杂推理任务上表现良好，但存在过度思考的问题，无论任务难度如何都会产生冗余内容，因此需要一种方法来实现高效推理。

Method: ACPO框架包括引入系统感知推理令牌以明确表示思考模式，并整合在线难度估计和令牌长度预算，引导自适应系统切换和推理。采用两阶段训练策略：监督微调冷启动模型，然后应用ACPO进行难度感知推理。

Result: 实验证明，ACPO能有效减少冗余推理，并根据任务复杂度自适应调整认知分配，实现高效混合推理。

Conclusion: ACPO通过透明化认知过程和难度感知推理，显著提升了大型推理模型的效率和适应性。

Abstract: Large reasoning models (LRMs) have demonstrated strong performance on complex
reasoning tasks, but often suffer from overthinking, generating redundant
content regardless of task difficulty. Inspired by the dual process theory in
cognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a
reinforcement learning framework that enables LRMs to achieve efficient
reasoning through adaptive cognitive allocation and dynamic system switch. ACPO
incorporates two key components: (1) introducing system-aware reasoning tokens
to explicitly represent the thinking modes thereby making the model's cognitive
process transparent, and (2) integrating online difficulty estimation and token
length budget to guide adaptive system switch and reasoning during
reinforcement learning. To this end, we propose a two-stage training strategy.
The first stage begins with supervised fine-tuning to cold start the model,
enabling it to generate reasoning paths with explicit thinking modes. In the
second stage, we apply ACPO to further enhance adaptive system switch for
difficulty-aware reasoning. Experimental results demonstrate that ACPO
effectively reduces redundant reasoning while adaptively adjusting cognitive
allocation based on task complexity, achieving efficient hybrid reasoning.

</details>


### [280] [Serious Games: Human-AI Interaction, Evolution, and Coevolution](https://arxiv.org/abs/2505.16388)
*Nandini Doreswamy,Louise Horstmanshof*

Key words: 进化博弈论，人类-AI交互，共同进化，鹰鸽博弈，囚徒困境，消耗战

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 进化博弈论（EGT）为研究人类与AI的竞争与合作策略提供了框架，分析了三种模型（鹰鸽博弈、重复囚徒困境、消耗战），预测可能的进化平衡，但需进一步探索跨学科和实证验证。

Motivation: 探讨进化博弈论如何预测人类与AI交互的潜在进化平衡及共同进化轨迹。

Method: 筛选并分析了13种EGT模型中的三种（鹰鸽博弈、重复囚徒困境、消耗战），基于其广泛认可度及对人类-AI动态的适用性。

Result: 模型分别显示：混合策略平衡（鹰鸽博弈）、重复互动促成合作（囚徒困境）、资源竞争导致战略共同进化（消耗战）。EGT可作为预测人类-AI进化动态的框架。

Conclusion: EGT能预测人类与AI的动态关系，但需结合其他框架及实证研究，并关注伦理与认知影响。

Abstract: The serious games between humans and AI have only just begun. Evolutionary
Game Theory (EGT) models the competitive and cooperative strategies of
biological entities. EGT could help predict the potential evolutionary
equilibrium of humans and AI. The objective of this work was to examine some of
the EGT models relevant to human-AI interaction, evolution, and coevolution. Of
thirteen EGT models considered, three were examined: the Hawk-Dove Game,
Iterated Prisoner's Dilemma, and the War of Attrition. This selection was based
on the widespread acceptance and clear relevance of these models to potential
human-AI evolutionary dynamics and coevolutionary trajectories. The Hawk-Dove
Game predicts balanced mixed-strategy equilibria based on the costs of
conflict. It also shows the potential for balanced coevolution rather than
dominance. Iterated Prisoner's Dilemma suggests that repeated interaction may
lead to cognitive coevolution. It demonstrates how memory and reciprocity can
lead to cooperation. The War of Attrition suggests that competition for
resources may result in strategic coevolution, asymmetric equilibria, and
conventions on sharing resources. Therefore, EGT may provide a suitable
framework to understand and predict the human-AI evolutionary dynamic. However,
future research could extend beyond EGT and explore additional frameworks,
empirical validation methods, and interdisciplinary perspectives. AI is being
shaped by human input and is evolving in response to it. So too,
neuroplasticity allows the human brain to grow and evolve in response to
stimuli. If humans and AI converge in future, what might be the result of human
neuroplasticity combined with an ever-evolving AI? Future research should be
mindful of the ethical and cognitive implications of human-AI interaction,
evolution, and coevolution.

</details>


### [281] [FREESON: Retriever-Free Retrieval-Augmented Reasoning via Corpus-Traversing MCTS](https://arxiv.org/abs/2505.16409)
*Chaeeun Kim,Seungone Kim*

Key words: 大型推理模型，检索增强推理，FREESON，CT-MCTS，开放领域问答

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种新颖的框架FREESON，通过将检索任务整合到大型推理模型中，避免了传统检索增强方法中的表示瓶颈问题，显著提升了开放领域问答的性能。

Motivation: 现有的检索增强推理方法依赖独立的检索模型，不仅增加了成本和错误率，还因表示瓶颈问题限制了性能。FREESON框架旨在让大型推理模型同时承担生成和检索任务，减少对独立检索模型的依赖。

Method: 提出了FREESON框架和CT-MCTS算法，通过修改的蒙特卡洛树搜索算法使大型推理模型能够在语料库中自主检索相关知识。

Result: 在五个开放领域问答基准测试中，FREESON在EM和F1上比传统方法平均提升了14.4%，并在特定数据集上超越了最强基线3%和2%。

Conclusion: FREESON框架通过整合检索与生成任务，显著提升了大型推理模型在复杂问答任务中的性能，消除了表示瓶颈问题。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in
multi-step reasoning and calling search engines at appropriate steps. However,
existing retrieval-augmented reasoning approaches rely on separate retrieval
models, limiting the LRM's role in retrieval to deciding when to retrieve and
how to query. This separation not only increases hardware and operational costs
but also leads to errors in the retrieval process due to the representation
bottleneck, a phenomenon where the retriever's embedding space is not
expressive enough to meet the generator's requirements. To address this, we
shift our perspective from sequence-to-sequence matching to locating the
answer-containing paths within the corpus, and propose a novel framework called
FREESON (Retriever-FREE Retrieval-Augmented ReaSONing). This framework enables
LRMs to retrieve relevant knowledge on their own by acting as both a generator
and retriever. To achieve this, we introduce a variant of the MCTS algorithm
specialized for the retrieval task, which we call CT-MCTS (Corpus-Traversing
Monte Carlo Tree Search). In this algorithm, LRMs traverse through the corpus
toward answer-containing regions. Our results on five open-domain QA
benchmarks, including single-hop and multi-hop questions, show that FREESON
achieves an average improvement of 14.4% in EM and F1 over four multi-step
reasoning models with a separate retriever, and it also performs comparably to
the strongest baseline, surpassing it by 3% on PopQA and 2WikiMultihopQA.

</details>


### [282] [Internal Bias in Reasoning Models leads to Overthinking](https://arxiv.org/abs/2505.16448)
*Renfei Dang,Shujian Huang,Jiajun Chen*

Key words: 推理模型, 内部偏见, 过度思考, 可解释性, 掩码输入

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文揭示了推理模型中过度思考的原因是由于内部对输入文本的偏见导致，初步猜测与推理结果冲突时引发冗余反思。通过实验证明屏蔽输入部分可显著减少推理长度并提高准确性。

Motivation: 当前推理模型虽具有强大探索能力，但常因冗余反思（过度思考）被批评。本研究首次揭示这种过度思考源于模型对输入文本的内部偏见。

Method: 研究通过可解释性实验分析模型的内部偏见行为，并通过掩码输入部分测试其对推理长度和准确性的影响。

Result: 实验显示屏蔽输入部分可将推理长度减少31%-53%，并在多数情况下提高准确性，验证了内部偏见与过度思考的因果关系。

Conclusion: 研究发现并验证了内部偏见是推理模型过度思考的主要原因，提出了缓解方法并展示其有效性。

Abstract: While current reasoning models possess strong exploratory capabilities, they
are often criticized for overthinking due to redundant and unnecessary
reflections. In this work, we reveal for the first time that overthinking in
reasoning models may stem from their internal bias towards input texts. Upon
encountering a reasoning problem, the model immediately forms a preliminary
guess about the answer, which we term as an internal bias since it is not
derived through actual reasoning. When this guess conflicts with its reasoning
result, the model tends to engage in reflection, leading to the waste of
computational resources. Through further interpretability experiments, we find
that this behavior is largely driven by the model's excessive attention to the
input section, which amplifies the influence of internal bias on its
decision-making process. Additionally, by masking out the original input
section, the affect of internal bias can be effectively alleviated and the
reasoning length could be reduced by 31%-53% across different complex reasoning
tasks. Notably, in most cases, this approach also leads to improvements in
accuracy. These findings demonstrate a causal relationship between internal
bias and overthinking.

</details>


### [283] [Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events](https://arxiv.org/abs/2505.16455)
*Mengzhu Liu,Zhengqiu Zhu,Chuan Ai,Chen Gao,Xinghong Li,Lingnan He,Kaisheng Lai,Yingfeng Chen,Xin Lu,Yong Li,Quanjun Yin*

Key words: 恐慌预测、心理驱动、生成代理、解释性、社交媒体

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种基于心理驱动的生成代理框架（PsychoAgent），用于解释性恐慌预测，解决了现有研究中的数据标注不足、风险感知未建模和解释性不足的问题，并通过实验验证了其性能提升和可解释性。

Motivation: 突发灾难事件中，准确预测社交媒体上的公众恐慌情绪对主动治理和危机管理至关重要。现有研究面临数据标注不足、风险感知未建模和恐慌形成机制解释性不足的挑战。

Method: 通过人类与LLMs协作构建细粒度恐慌情绪数据集（COPE），开发基于心理机制的跨域异质数据框架，并设计基于LLM的角色扮演代理模拟个体心理链。

Result: 实验结果表明，PsychoAgent在恐慌情绪预测上相比基线模型提升了12.6%至21.7%，并验证了其可解释性和泛化能力。

Conclusion: 该框架实现了从数据驱动的拟合到基于角色模拟和机制解释的透明预测范式的转变。

Abstract: During sudden disaster events, accurately predicting public panic sentiment
on social media is crucial for proactive governance and crisis management.
Current efforts on this problem face three main challenges: lack of finely
annotated data hinders emotion prediction studies, unmodeled risk perception
causes prediction inaccuracies, and insufficient interpretability of panic
formation mechanisms. We address these issues by proposing a Psychology-driven
generative Agent framework (PsychoAgent) for explainable panic prediction based
on emotion arousal theory. Specifically, we first construct a fine-grained open
panic emotion dataset (namely COPE) via human-large language models (LLMs)
collaboration to mitigate semantic bias. Then, we develop a framework
integrating cross-domain heterogeneous data grounded in psychological
mechanisms to model risk perception and cognitive differences in emotion
generation. To enhance interpretability, we design an LLM-based role-playing
agent that simulates individual psychological chains through dedicatedly
designed prompts. Experimental results on our annotated dataset show that
PsychoAgent improves panic emotion prediction performance by 12.6% to 21.7%
compared to baseline models. Furthermore, the explainability and generalization
of our approach is validated. Crucially, this represents a paradigm shift from
opaque "data-driven fitting" to transparent "role-based simulation with
mechanistic interpretation" for panic emotion prediction during emergencies.
Our implementation is publicly available at:
https://anonymous.4open.science/r/PsychoAgent-19DD.

</details>


### [284] [MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks](https://arxiv.org/abs/2505.16459)
*Guiyao Tie,Xueyang Zhou,Tianhe Gu,Ruihang Zhang,Chaoran Hu,Sizhe Zhang,Mengqu Sun,Yan Zhang,Pan Zhou,Lichao Sun*

Key words: 多模态大语言模型，推理能力评估，MMMR基准，思考轨迹，RTEP

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文介绍了MMMR基准，用于严格评估多模态大语言模型（MLLMs-T）的推理能力，并揭示了当前模型在一致性等方面的缺陷。

Motivation: 尽管多模态大语言模型（MLLMs）在逻辑推理等复杂任务中展现潜力，但其推理能力尤其是有中间思考轨迹的模型（MLLMs-T）尚未被充分理解，且缺乏标准化评估基准。

Method: 提出了MMMR基准，包含1,083道高难度问题及一个模块化的推理轨迹评估管道（RTEP），通过相关性、一致性等指标评估推理质量。

Result: MLLMs-T整体优于非思考轨迹模型，但顶级模型仍存在不一致性、过度思考等缺陷，揭示了准确性与推理质量间的差距。

Conclusion: MMMR为未来多模态推理模型的开发提供了可扩展的评估基础，有助于改进下一代系统。

Abstract: Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled
unified processing of language, vision, and structured inputs, opening the door
to complex tasks such as logical deduction, spatial reasoning, and scientific
analysis. Despite their promise, the reasoning capabilities of MLLMs,
particularly those augmented with intermediate thinking traces (MLLMs-T),
remain poorly understood and lack standardized evaluation benchmarks. Existing
work focuses primarily on perception or final answer correctness, offering
limited insight into how models reason or fail across modalities. To address
this gap, we introduce the MMMR, a new benchmark designed to rigorously
evaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) a
high-difficulty dataset of 1,083 questions spanning six diverse reasoning types
with symbolic depth and multi-hop demands and 2) a modular Reasoning Trace
Evaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy
through metrics like relevance, consistency, and structured error annotations.
Empirical results show that MLLMs-T overall outperform non-thinking
counterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro
suffer from reasoning pathologies such as inconsistency and overthinking. This
benchmark reveals persistent gaps between accuracy and reasoning quality and
provides an actionable evaluation pipeline for future model development.
Overall, the MMMR offers a scalable foundation for evaluating, comparing, and
improving the next generation of multi-modal reasoning systems.

</details>


### [285] [ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection](https://arxiv.org/abs/2505.16475)
*Jiaqi Li,Xinyi Dong,Yang Liu,Zhizhuo Yang,Quansen Wang,Xiaobo Wang,SongChun Zhu,Zixia Jia,Zilong Zheng*

Key words: ReflectEvo, 反射学习, 小语言模型, 自我训练, 推理能力

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: ReflectEvo是一个新流程，通过反射学习提升小语言模型（SLMs）的元自省能力，构建了大尺度自生成反射数据集ReflectEvo-460k，显著提升了模型的推理能力。

Motivation: 证明小语言模型可以通过反射学习实现自我提升，减少对大型模型或人工标注的依赖。

Method: 通过迭代生成自我反思进行自训练，使用SFT和DPO方法优化模型推理能力。

Result: Llama-3和Mistral的推理能力分别从52.4%和44.4%提升至71.2%和71.1%，超越了一些开源模型。

Conclusion: 反射学习能持续提升小语言模型的推理能力，具有长期潜力。

Abstract: We present a novel pipeline, ReflectEvo, to demonstrate that small language
models (SLMs) can enhance meta introspection through reflection learning. This
process iteratively generates self-reflection for self-training, fostering a
continuous and self-evolving process. Leveraging this pipeline, we construct
ReflectEvo-460k, a large-scale, comprehensive, self-generated reflection
dataset with broadened instructions and diverse multi-domain tasks. Building
upon this dataset, we demonstrate the effectiveness of reflection learning to
improve SLMs' reasoning abilities using SFT and DPO with remarkable
performance, substantially boosting Llama-3 from 52.4% to 71.2% and Mistral
from 44.4% to 71.1%. It validates that ReflectEvo can rival or even surpass the
reasoning capability of the three prominent open-sourced models on BIG-bench
without distillation from superior models or fine-grained human annotation. We
further conduct a deeper analysis of the high quality of self-generated
reflections and their impact on error localization and correction. Our work
highlights the potential of continuously enhancing the reasoning performance of
SLMs through iterative reflection learning in the long run.

</details>


### [286] [Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery](https://arxiv.org/abs/2505.16477)
*Yanbo Zhang,Sumeer A. Khan,Adnan Mahmud,Huck Yang,Alexander Lavin,Michael Levin,Jeremy Frey,Jared Dunnmon,James Evans,Alan Bundy,Saso Dzeroski,Jesper Tegner,Hector Zenil*

Key words: LLMs, 科学方法, 数据分析, 伦理学, 创造性

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: LLMs正在通过提升效率和重塑科学方法改变科学研究，尤其是在化学和生物学领域。尽管存在幻觉和可靠性问题，但其在科学周期中具有广泛应用潜力。

Motivation: 探讨LLMs如何重塑科学方法及其在科学周期各阶段的应用潜力，以提升生产力和创造性。

Method: 综述LLMs在科学方法中的作用，从实验设计到数据分析，并探讨其与人类科学目标的整合。

Result: LLMs有望成为创造性引擎，但需在明确评估指标和伦理框架下与人类合作。

Conclusion: LLMs的科学应用需要谨慎引导，以实现负责任且高效的突破，同时解决伦理问题。

Abstract: With recent Nobel Prizes recognising AI contributions to science, Large
Language Models (LLMs) are transforming scientific research by enhancing
productivity and reshaping the scientific method. LLMs are now involved in
experimental design, data analysis, and workflows, particularly in chemistry
and biology. However, challenges such as hallucinations and reliability
persist. In this contribution, we review how Large Language Models (LLMs) are
redefining the scientific method and explore their potential applications
across different stages of the scientific cycle, from hypothesis testing to
discovery. We conclude that, for LLMs to serve as relevant and effective
creative engines and productivity enhancers, their deep integration into all
steps of the scientific process should be pursued in collaboration and
alignment with human scientific goals, with clear evaluation metrics. The
transition to AI-driven science raises ethical questions about creativity,
oversight, and responsibility. With careful guidance, LLMs could evolve into
creative engines, driving transformative breakthroughs across scientific
disciplines responsibly and effectively. However, the scientific community must
also decide how much it leaves to LLMs to drive science, even when associations
with 'reasoning', mostly currently undeserved, are made in exchange for the
potential to explore hypothesis and solution regions that might otherwise
remain unexplored by human exploration alone.

</details>


### [287] [Minimizing the energy depletion in wireless rechargeable sensor networks using bi-level metaheuristic charging schemes](https://arxiv.org/abs/2505.16482)
*Huynh Thi Thanh Binh,Le Van Cuong,Dang Hai Dang,Le Trong Vinh*

Key words: Wireless Rechargeable Sensor Networks, partial charging, bi-level optimization, Genetic Algorithm, Multitasking Evolutionary Strategies

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: The paper introduces a bi-level optimized partial charging approach for WRSNs, combining charging path and time optimization through two algorithms, outperforming existing methods.

Motivation: Traditional full charging approaches in WRSNs lead to sensor deaths due to high latency. A partial charging strategy is needed to minimize energy depletion efficiently.

Method: A mathematical model is formulated, followed by two approximate algorithms optimizing charging path (upper level) and time (lower level) using Multi-start Local Search, Genetic Algorithm, and nested Multitasking with Covariance Matrix Adaptation Evolutionary Strategies.

Result: Experiments show the proposed algorithms outperform existing methods across various network scenarios.

Conclusion: The bi-level partial charging approach effectively reduces energy depletion in WRSNs, demonstrating superior performance through optimized path and time management.

Abstract: Recently, Wireless Rechargeable Sensor Networks (WRSNs) that leveraged the
advantage of wireless energy transfer technology have opened a promising
opportunity in solving the limited energy issue. However, an ineffective
charging strategy may reduce the charging performance. Although many practical
charging algorithms have been introduced, these studies mainly focus on
optimizing the charging path with a fully charging approach. This approach may
lead to the death of a series of sensors due to their extended charging
latency. This paper introduces a novel partial charging approach that follows a
bi-level optimized scheme to minimize energy depletion in WRSNs. We aim at
optimizing simultaneously two factors: the charging path and time. To
accomplish this, we first formulate a mathematical model of the investigated
problem. We then propose two approximate algorithms in which the optimization
of the charging path and the charging time are considered as the upper and
lower level, respectively. The first algorithm combines a Multi-start Local
Search method and a Genetic Algorithm to find a solution. The second algorithm
adopts a nested approach that utilizes the advantages of the Multitasking and
Covariance Matrix Adaptation Evolutionary Strategies. Experimental validations
on various network scenarios demonstrate that our proposed algorithms
outperform the existing works.

</details>


### [288] [Relevance for Stability of Verification Status of a Set of Arguments in Incomplete Argumentation Frameworks (with Proofs)](https://arxiv.org/abs/2505.16507)
*Anshu Xiong,Songmao Zhang*

Key words: 不完全论证框架、验证稳定性、相关性、强相关性、计算复杂度

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文扩展了不完全论证框架中单个论证的相关性概念，研究了论证集合验证稳定性所需的相关性，并提出了强相关性的概念。分析表明，在多数语义下检测（强）相关性可在P时间内完成，但在基础语义下难以找到高效方法。

Motivation: 为解决不完全论证框架中论证集合验证稳定性的问题，扩展了单论证的相关性概念，以确定需要解决的不确定性。

Method: 提出了论证集合验证稳定性的相关性和强相关性概念，并分析了在不同语义下的计算复杂度。

Result: 在多数语义下，检测（强）相关性可在P时间内完成；但在基础语义下，难找到高效方法。

Conclusion: 研究为不完全论证框架中论证集合的稳定性提供了新的理论工具，但在基础语义下仍需进一步探索。

Abstract: The notion of relevance was proposed for stability of justification status of
a single argument in incomplete argumentation frameworks (IAFs) in 2024 by
Odekerken et al. To extend the notion, we study the relevance for stability of
verification status of a set of arguments in this paper, i.e., the
uncertainties in an IAF that have to be resolved in some situations so that
answering whether a given set of arguments is an extension obtains the same
result in every completion of the IAF. Further we propose the notion of strong
relevance for describing the necessity of resolution in all situations reaching
stability. An analysis of complexity reveals that detecting the (strong)
relevance for stability of sets of arguments can be accomplished in P time
under the most semantics discussed in the paper. We also discuss the difficulty
in finding tractable methods for relevance detection under grounded semantics.

</details>


### [289] [Bridging the Dynamic Perception Gap: Training-Free Draft Chain-of-Thought for Dynamic Multimodal Spatial Reasoning](https://arxiv.org/abs/2505.16579)
*Siqu Ou,Hongcheng Liu,Pingjie Wang,Yusheng Liao,Chuan Xuan,Yanfeng Wang,Yu Wang*

Key words: 动态空间推理,多模态大模型,GRASSLAND,D2R,视觉草稿

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: GRASSLAND benchmark和D2R框架通过动态视觉草稿增强文本推理链，显著提升动态空间推理能力，无需模型微调。

Motivation: 现有方法在动态空间推理任务中表现不佳，因此需要一个新方法来增强多模态大模型的动态空间推理能力。

Method: 提出D2R框架，通过动态视觉草稿与文本推理链结合，无需训练即可提升多模态大模型的推理能力。

Result: D2R在多样化任务中表现优于传统方法，为动态空间推理建立了强大基线。

Conclusion: D2R框架有效增强动态空间推理能力，为多模态大模型提供了新的解决方案。

Abstract: While chains-of-thought (CoT) have advanced complex reasoning in multimodal
large language models (MLLMs), existing methods remain confined to text or
static visual domains, often faltering in dynamic spatial reasoning tasks. To
bridge this gap, we present GRASSLAND, a novel maze navigation benchmark
designed to evaluate dynamic spatial reasoning. Our experiments show that
augmenting textual reasoning chains with dynamic visual drafts, overlaid on
input images, significantly outperforms conventional approaches, offering new
insights into spatial reasoning in evolving environments. To generalize this
capability, we propose D2R (Dynamic Draft-Augmented Reasoning), a training-free
framework that seamlessly integrates textual CoT with corresponding visual
drafts into MLLMs. Extensive evaluations demonstrate that D2R consistently
enhances performance across diverse tasks, establishing a robust baseline for
dynamic spatial reasoning without requiring model fine-tuning. Project is open
at https://github.com/Cratileo/D2R.

</details>


### [290] [Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences](https://arxiv.org/abs/2505.16619)
*Gavin Farrell,Eleni Adamidi,Rafael Andrade Buono,Mihail Anton,Omar Abdelghani Attafi,Salvador Capella Gutierrez,Emidio Capriotti,Leyla Jael Castro,Davide Cirillo,Lisa Crossman,Christophe Dessimoz,Alexandros Dimopoulos,Raul Fernandez-Diaz,Styliani-Christina Fragkouli,Carole Goble,Wei Gu,John M. Hancock,Alireza Khanteymoori,Tom Lenaerts,Fabio G. Liberante,Peter Maccallum,Alexander Miguel Monzon,Magnus Palmblad,Lucy Poveda,Ovidiu Radulescu,Denis C. Shields,Shoaib Sufi,Thanasis Vergoulis,Fotis Psomopoulos,Silvio C. E. Tosatto*

Key words: 人工智能,生命科学,可持续性,可重复性,开放AI

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: AI在生命科学中的快速发展带来了机遇，但也加剧了研究的可重复性和可持续性问题。本文提出了开放与可持续AI（OSAI）的建议，以促进透明和可重用的AI研究。

Motivation: 解决AI在生命科学中迅速普及带来的研究挑战，如信任缺失和可持续性问题。

Method: 提出了一套OSAI建议，映射到AI生态系统的300多个组件中。

Result: 帮助研究者找到相关AI资源，促进可持续、可重用和透明的AI研究。

Conclusion: 本文为AI的政策制定和实施提供了结构化路径。

Abstract: Artificial intelligence (AI) has recently seen transformative breakthroughs
in the life sciences, expanding possibilities for researchers to interpret
biological information at an unprecedented capacity, with novel applications
and advances being made almost daily. In order to maximise return on the
growing investments in AI-based life science research and accelerate this
progress, it has become urgent to address the exacerbation of long-standing
research challenges arising from the rapid adoption of AI methods. We review
the increased erosion of trust in AI research outputs, driven by the issues of
poor reusability and reproducibility, and highlight their consequent impact on
environmental sustainability. Furthermore, we discuss the fragmented components
of the AI ecosystem and lack of guiding pathways to best support Open and
Sustainable AI (OSAI) model development. In response, this perspective
introduces a practical set of OSAI recommendations directly mapped to over 300
components of the AI ecosystem. Our work connects researchers with relevant AI
resources, facilitating the implementation of sustainable, reusable and
transparent AI. Built upon life science community consensus and aligned to
existing efforts, the outputs of this perspective are designed to aid the
future development of policy and structured pathways for guiding AI
implementation.

</details>


### [291] [SMART: Self-Generating and Self-Validating Multi-Dimensional Assessment for LLMs' Mathematical Problem Solving](https://arxiv.org/abs/2505.16646)
*Yujie Hou,Ting Zhang,Mei Wang,Xuetao Ma,Hu Huang*

Key words: 大型语言模型, 数学推理, 评估框架, SMART, 多维度分析

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文介绍了SMART框架，用于多维度评估大型语言模型在数学问题解决中的能力，揭示了现有评估指标的局限性。

Motivation: 大型语言模型在数学基准测试中表现出色，但无法确定其是基于真实的数学推理还是表面模式识别。现有评估指标缺乏诊断价值。

Method: 开发了SMART框架，将数学问题解决分解为四个维度，并通过自动化生成和验证机制评估模型。

Result: 应用于21种开源和闭源模型，发现各维度能力存在显著差异，表明单一指标不足。

Conclusion: 提出新的整体评估指标，以更全面捕捉模型的真实问题解决能力。

Abstract: Large Language Models have achieved remarkable results on a variety of
mathematical benchmarks. However, concerns remain as to whether these successes
reflect genuine mathematical reasoning or superficial pattern recognition.
Common evaluation metrics, such as final answer accuracy, fail to disentangle
the underlying competencies involved, offering limited diagnostic value. To
address these limitations, we introduce SMART: a Self-Generating and
Self-Validating Multi-Dimensional Assessment Framework. SMART decomposes
mathematical problem solving into four distinct dimensions: understanding,
reasoning, arithmetic, and reflection \& refinement. Each dimension is
evaluated independently through tailored tasks, enabling interpretable and
fine-grained analysis of LLM behavior. Crucially, SMART integrates an automated
self-generating and self-validating mechanism to produce and verify benchmark
data, ensuring both scalability and reliability. We apply SMART to 21
state-of-the-art open- and closed-source LLMs, uncovering significant
discrepancies in their abilities across different dimensions. Our findings
demonstrate the inadequacy of final answer accuracy as a sole metric and
motivate a new holistic metric to better capture true problem-solving
capabilities. Code and benchmarks will be released upon acceptance.

</details>


### [292] [ELABORATION: A Comprehensive Benchmark on Human-LLM Competitive Programming](https://arxiv.org/abs/2505.16667)
*Xinwei Yang,Zhaofeng Liu,Chen Huang,Jiashuai Zhang,Tong Zhang,Yifan Zhang,Wenqiang Lei*

Key words: 人类-LLM协作, 编程反馈分类法, ELABORATIONSET, ELABORATION基准, 竞争性编程

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了首个涵盖整个编程过程的人类反馈分类法，创建了专门为人类-LLM协作设计的数据集ELABORATIONSET，并引入了ELABORATION基准以全面评估协作效果。

Motivation: 现有研究在人类-LLM协作编程中缺乏系统性，且反馈形式多样且碎片化，因此需要统一分类法和标准化评估方法。

Method: 1. 提出人类反馈分类法；2. 构建精细标注的数据集ELABORATIONSET；3. 设计基准ELABORATION用于评估。

Result: 分类法促进了细粒度评估；数据集支持大规模模拟与真实人类交互研究；基准揭示了现有方法的优劣。

Conclusion: 该工作为未来改进人类-LLM协作编程奠定了基础，提供了工具和数据集。

Abstract: While recent research increasingly emphasizes the value of human-LLM
collaboration in competitive programming and proposes numerous empirical
methods, a comprehensive understanding remains elusive due to the fragmented
nature of existing studies and their use of diverse, application-specific human
feedback. Thus, our work serves a three-fold purpose: First, we present the
first taxonomy of human feedback consolidating the entire programming process,
which promotes fine-grained evaluation. Second, we introduce ELABORATIONSET, a
novel programming dataset specifically designed for human-LLM collaboration,
meticulously annotated to enable large-scale simulated human feedback and
facilitate costeffective real human interaction studies. Third, we introduce
ELABORATION, a novel benchmark to facilitate a thorough assessment of human-LLM
competitive programming. With ELABORATION, we pinpoint strengthes and
weaknesses of existing methods, thereby setting the foundation for future
improvement. Our code and dataset are available at
https://github.com/SCUNLP/ELABORATION

</details>


### [293] [SPaRC: A Spatial Pathfinding Reasoning Challenge](https://arxiv.org/abs/2505.16686)
*Lars Benedikt Kaesberg,Jan Philip Wahle,Terry Ruas,Bela Gipp*

Key words: 空间推理、路径规划、多步骤问题、模型评估、SPaRC

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: SPaRC数据集测试模型的抽象、多步骤空间路径规划能力，人类表现优异而当前模型表现较差，揭示了模型在导航和空间逻辑上的不足。

Motivation: 现有推理数据集无法测试抽象、复杂的多步骤问题，尤其是路径规划和规则约束满足问题。

Method: 引入SPaRC数据集，包含1000个2D网格路径规划谜题，评估空间和符号推理能力。

Result: 人类准确率接近完美（98.0%），而最佳模型（如o4-mini）表现较差（15.8%），且常生成无效路径。

Conclusion: SPaRC数据集可揭示模型空间推理的局限性，推动改进训练方法和测试时间扩展策略。

Abstract: Existing reasoning datasets saturate and fail to test abstract, multi-step
problems, especially pathfinding and complex rule constraint satisfaction. We
introduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000
2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,
requiring step-by-step planning with arithmetic and geometric rules. Humans
achieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best
reasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).
Models often generate invalid paths (>50% of puzzles for o4-mini), and
reasoning tokens reveal they make errors in navigation and spatial logic.
Unlike humans, who take longer on hard puzzles, models fail to scale test-time
compute with difficulty. Allowing models to make multiple solution attempts
improves accuracy, suggesting potential for better spatial reasoning with
improved training and efficient test-time scaling methods. SPaRC can be used as
a window into models' spatial reasoning limitations and drive research toward
new methods that excel in abstract, multi-step problem-solving.

</details>


### [294] [MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use Capabilities in Large Language Models](https://arxiv.org/abs/2505.16700)
*Xuanqi Gao,Siyi Xie,Juan Zhai,Shqing Ma,Chao Shen*

Key words: LLM, MCP, 工具交互, 评估基准, 五维指标

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: MCP-RADAR是首个专为评估LLM在MCP框架下表现的综合基准，通过五维方法量化工具使用能力，揭示了准确性与效率的权衡。

Motivation: 现有评估方法无法充分衡量LLM在MCP框架下的工具利用能力，需开发全面客观的评估标准。

Method: 提出MCP-RADAR，通过五维指标（答案准确性、工具选择效率、计算资源效率、参数构建准确性和执行速度）评估LLM表现。

Result: 评估揭示主流LLM在准确性与效率间的显著权衡，挑战单一指标排名传统，并为开发者提供优化建议。

Conclusion: MCP-RADAR为LLM工具交互生态优化提供通用方法，适用于所有框架。

Abstract: As Large Language Models (LLMs) evolve from passive text generators to active
reasoning agents capable of tool interaction, the Model Context Protocol (MCP)
has emerged as a standardized framework for dynamic tool discovery and
orchestration. Despite widespread industry adoption, existing evaluation
methodologies fail to adequately assess tool utilization capabilities within
this new paradigm. This paper introduces MCP-RADAR, the first comprehensive
benchmark specifically designed to evaluate LLM performance in the MCP
framework through a novel five-dimensional approach measuring: answer accuracy,
tool selection efficiency, computational resource efficiency, parameter
construction accuracy, and execution speed. Unlike conventional benchmarks that
rely on subjective human evaluations or binary success metrics, MCP-RADAR
employs objective, quantifiable measurements across multiple task domains
including software engineering, mathematical reasoning, and general
problem-solving. Our evaluations of leading commercial and open-source LLMs
reveal distinctive capability profiles with significant trade-offs between
accuracy, efficiency, and speed, challenging traditional single-metric
performance rankings. Besides, we provide valuable guidance for developers to
optimize their tools for maximum model compatibility and effectiveness. While
focused on MCP due to its standardized approach, our methodology remains
applicable across all LLM agent tool integration frameworks, providing valuable
insights for both LLM developers and tool creators to optimize the entire
LLM-tool interaction ecosystem. The implementation, configurations, and
datasets used in our evaluation are publicly available at
https://anonymous.4open.science/r/MCPRadar-B143.

</details>


### [295] [Data-Driven Breakthroughs and Future Directions in AI Infrastructure: A Comprehensive Review](https://arxiv.org/abs/2505.16771)
*Beyazit Bestami Yuksel,Ayse Yilmazer Metin*

Key words: 人工智能, 数据中心化, 统计学习理论, 隐私增强技术, 联邦学习

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 这篇论文综合回顾了过去十五年人工智能领域的主要突破，结合历史、理论和技术的多重视角，指出AI发展的关键转折点，并强调了向数据为中心的方法转变的趋势。

Motivation: 探讨AI领域的关键发展，理解这些突破如何推动范式转变，并应对隐私和法规挑战，为未来研究和政策制定提供指导。

Method: 通过分析计算资源、数据获取和算法创新的交汇点，结合统计学习理论（如样本复杂性和数据效率），评估包括联邦学习和隐私增强技术在内的新兴解决方案。

Result: 研究发现GPU训练、ImageNet推动的数据中心化、Transformer架构以及GPT系列的成功标志着AI的深层范式变化，并指出了数据中心化方法的必要性。

Conclusion: 论文主张未来AI研究应采取数据为中心的方法，同时整合隐私保护和法规遵从技术，以应对实际数据稀缺的挑战。

Abstract: This paper presents a comprehensive synthesis of major breakthroughs in
artificial intelligence (AI) over the past fifteen years, integrating
historical, theoretical, and technological perspectives. It identifies key
inflection points in AI' s evolution by tracing the convergence of
computational resources, data access, and algorithmic innovation. The analysis
highlights how researchers enabled GPU based model training, triggered a data
centric shift with ImageNet, simplified architectures through the Transformer,
and expanded modeling capabilities with the GPT series. Rather than treating
these advances as isolated milestones, the paper frames them as indicators of
deeper paradigm shifts. By applying concepts from statistical learning theory
such as sample complexity and data efficiency, the paper explains how
researchers translated breakthroughs into scalable solutions and why the field
must now embrace data centric approaches. In response to rising privacy
concerns and tightening regulations, the paper evaluates emerging solutions
like federated learning, privacy enhancing technologies (PETs), and the data
site paradigm, which reframe data access and security. In cases where real
world data remains inaccessible, the paper also assesses the utility and
constraints of mock and synthetic data generation. By aligning technical
insights with evolving data infrastructure, this study offers strategic
guidance for future AI research and policy development.

</details>


### [296] [Fuzzy Information Evolution with Three-Way Decision in Social Network Group Decision-Making](https://arxiv.org/abs/2505.16781)
*Qianlei Jia,Xinliang Zhou,Ondrej Krejcar,Enrique Herrera-Viedma*

Key words: 群体决策；三支决策；动态网络重构；语言意见；多智能体

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出一种结合三支决策理论和动态社交网络重构的新型群体决策框架，用于处理不确定性和模糊信息，并通过多无人机协同决策验证其有效性。

Motivation: 解决群体决策中的不确定性、动态社交结构和模糊信息问题。

Method: 结合三支决策理论、动态网络重构和语言意见表示，构建多智能体决策框架。

Result: 模拟实验验证了模型在系统稳定性和决策行为表示上的优势。

Conclusion: 该框架有效提升了群体决策的理性化和适应性。

Abstract: In group decision-making (GDM) scenarios, uncertainty, dynamic social
structures, and vague information present major challenges for traditional
opinion dynamics models. To address these issues, this study proposes a novel
social network group decision-making (SNGDM) framework that integrates
three-way decision (3WD) theory, dynamic network reconstruction, and linguistic
opinion representation. First, the 3WD mechanism is introduced to explicitly
model hesitation and ambiguity in agent judgments, thereby preventing
irrational decisions. Second, a connection adjustment rule based on opinion
similarity is developed, enabling agents to adaptively update their
communication links and better reflect the evolving nature of social
relationships. Third, linguistic terms are used to describe agent opinions,
allowing the model to handle subjective, vague, or incomplete information more
effectively. Finally, an integrated multi-agent decision-making framework is
constructed, which simultaneously considers individual uncertainty, opinion
evolution, and network dynamics. The proposed model is applied to a multi-UAV
cooperative decision-making scenario, where simulation results and consensus
analysis demonstrate its effectiveness. Experimental comparisons further verify
the advantages of the algorithm in enhancing system stability and representing
realistic decision-making behaviors.

</details>


### [297] [Gaze Into the Abyss -- Planning to Seek Entropy When Reward is Scarce](https://arxiv.org/abs/2505.16787)
*Ashish Sundar,Chunbo Luo,Xiaoyang Wang*

Key words: 模型强化学习、高熵状态、Dreamer、Miniworld、层次化规划器

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一种基于模型强化学习的新方法，通过主动寻找高熵状态来优化世界模型学习，从而提升模型效率和下游任务表现。该方法在Dreamer上的实验显示任务完成速度提升50%，训练步骤减少40%。

Motivation: 传统基于模型的强化学习方法在优化演员时往往忽视世界模型的学习。本文旨在通过改进世界模型的学习方式来提高其预测准确性，从而提升效率和下游任务的执行效果。

Method: 方法是预判和主动寻找高熵状态，利用世界模型生成的短时潜在预测，并通过层次化规划器动态决定重规划时机、规划周期和奖励-熵权重。应用在Dreamer框架上验证。

Result: 结果表明，该方法在Miniworld迷宫任务中收敛速度比基础Dreamer快50%，训练步骤仅需60%。

Conclusion: 优化世界模型学习可以显著提升样本效率和下游任务表现，提出的方法在实验中验证了其有效性。

Abstract: Model-based reinforcement learning (MBRL) offers an intuitive way to increase
the sample efficiency of model-free RL methods by simultaneously training a
world model that learns to predict the future. MBRL methods have progressed by
largely prioritising the actor; optimising the world model learning has been
neglected meanwhile. Improving the fidelity of the world model and reducing its
time to convergence can yield significant downstream benefits, one of which is
improving the ensuing performance of any actor it may train. We propose a novel
approach that anticipates and actively seeks out high-entropy states using
short-horizon latent predictions generated by the world model, offering a
principled alternative to traditional curiosity-driven methods that chase
once-novel states well after they were stumbled into. While many model
predictive control (MPC) based methods offer similar alternatives, they
typically lack commitment, synthesising multi step plans after every step. To
mitigate this, we present a hierarchical planner that dynamically decides when
to replan, planning horizon length, and the weighting between reward and
entropy. While our method can theoretically be applied to any model that trains
its own actors with solely model generated data, we have applied it to just
Dreamer as a proof of concept. Our method finishes the Miniworld procedurally
generated mazes 50% faster than base Dreamer at convergence and the policy
trained in imagination converges in only 60% of the environment steps that base
Dreamer needs.

</details>


### [298] [KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning](https://arxiv.org/abs/2505.16826)
*Wei Sun,Wen Yang,Pu Jian,Qianlong Du,Fuwei Cui,Shuo Ren,Jiajun Zhang*

Key words: 强化学习, 标记级优势估计, KTAE, 数学推理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一种名为KTAE的新算法，通过细粒度标记级优势估计提升强化学习效果，在数学推理基准测试中表现优异。

Motivation: 现有强化学习算法（如GRPO和DAPO）在优势计算时存在颗粒度过粗的问题，无法捕捉标记级贡献。

Method: 提出KTAE算法，结合统计分析和采样正确性，量化标记的重要性，实现细粒度优势估计。

Result: 在五个数学推理基准测试中，GRPO+KTAE和DAPO+KTAE表现优于基线方法，且能以更短的响应达到更高准确率。

Conclusion: KTAE有效改善了优势估计的颗粒度，提升了模型的学习效果和推理能力。

Abstract: Recent advances have demonstrated that integrating reinforcement learning
with rule-based rewards can significantly enhance the reasoning capabilities of
large language models, even without supervised fine-tuning. However, prevalent
reinforcement learning algorithms such as GRPO and its variants like DAPO,
suffer from a coarse granularity issue when computing the advantage.
Specifically, they compute rollout-level advantages that assign identical
values to every token within a sequence, failing to capture token-specific
contributions and hindering effective learning. To address this limitation, we
propose Key-token Advantage Estimation (KTAE) - a novel algorithm that
estimates fine-grained, token-level advantages without introducing additional
models. KTAE leverages the correctness of sampled rollouts and applies
statistical analysis to quantify the importance of individual tokens within a
sequence to the final outcome. This quantified token-level importance is then
combined with the rollout-level advantage to obtain a more fine-grained
token-level advantage estimation. Empirical results show that models trained
with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five
mathematical reasoning benchmarks. Notably, they achieve higher accuracy with
shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base
model.

</details>


### [299] [GUI-explorer: Autonomous Exploration and Mining of Transition-aware Knowledge for GUI Agent](https://arxiv.org/abs/2505.16827)
*Bin Xie,Rui Shao,Gongwei Chen,Kaiwen Zhou,Yinchuan Li,Jie Liu,Min Zhang,Liqiang Nie*

Key words: GUI自动化, MLLMs, 自主探索, 无监督知识提取, 任务成功率

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种无需训练的GUI代理GUI-explorer，通过自主探索和无监督知识提取解决了MLLMs在动态环境中面临的UI组件误解和知识过时问题，显著提升了任务成功率。

Motivation: 解决MLLMs在GUI自动化中因UI组件误解和知识过时导致的性能问题，同时避免传统微调方法的高成本。

Method: 设计了功能感知任务目标生成器进行自主探索，并通过过渡感知知识提取器无监督提取屏幕操作逻辑。

Result: 在SPA-Bench和AndroidWorld上分别达到53.7%和47.4%的任务成功率，超越现有SOTA代理。

Conclusion: GUI-explorer无需针对新应用更新参数，通过开源实现高效、低成本的GUI自动化。

Abstract: GUI automation faces critical challenges in dynamic environments. MLLMs
suffer from two key issues: misinterpreting UI components and outdated
knowledge. Traditional fine-tuning methods are costly for app-specific
knowledge updates. We propose GUI-explorer, a training-free GUI agent that
incorporates two fundamental mechanisms: (1) Autonomous Exploration of
Function-aware Trajectory. To comprehensively cover all application
functionalities, we design a Function-aware Task Goal Generator that
automatically constructs exploration goals by analyzing GUI structural
information (e.g., screenshots and activity hierarchies). This enables
systematic exploration to collect diverse trajectories. (2) Unsupervised Mining
of Transition-aware Knowledge. To establish precise screen-operation logic, we
develop a Transition-aware Knowledge Extractor that extracts effective
screen-operation logic through unsupervised analysis the state transition of
structured interaction triples (observation, action, outcome). This eliminates
the need for human involvement in knowledge extraction. With a task success
rate of 53.7% on SPA-Bench and 47.4% on AndroidWorld, GUI-explorer shows
significant improvements over SOTA agents. It requires no parameter updates for
new apps. GUI-explorer is open-sourced and publicly available at
https://github.com/JiuTian-VL/GUI-explorer.

</details>


### [300] [From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization](https://arxiv.org/abs/2505.16832)
*Haonian Ji,Shi Qiu,Siyang Xin,Siwei Han,Zhaorun Chen,Hongyi Wang,Dake Zhang,Huaxiu Yao*

Key words: 基础模型, 教育可视化, 多代理协作, 评估基准

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了EduVisBench基准和EduVisAgent框架，以评估和改进基础模型在教育场景中生成可视化解释的能力。

Motivation: 现有的基础模型在生成教育有效的可视化解释方面表现有限，忽视了结构化可视化在支持概念理解中的关键作用。

Method: 提出了EduVisBench多领域、多级别基准和EduVisAgent多代理协作框架，后者包括教学规划、推理分解、元认知提示和可视化设计的专门代理。

Result: EduVisAgent显著优于所有基线模型，性能提升了40.2%，生成了更符合教育需求的可视化结果。

Conclusion: EduVisBench和EduVisAgent为教育可视化提供了有效的评估和改进工具，展示了多代理协作框架的潜力。

Abstract: While foundation models (FMs), such as diffusion models and large
vision-language models (LVLMs), have been widely applied in educational
contexts, their ability to generate pedagogically effective visual explanations
remains limited. Most existing approaches focus primarily on textual reasoning,
overlooking the critical role of structured and interpretable visualizations in
supporting conceptual understanding. To better assess the visual reasoning
capabilities of FMs in educational settings, we introduce EduVisBench, a
multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem
sets requiring visually grounded solutions, along with a fine-grained
evaluation rubric informed by pedagogical theory. Our empirical analysis
reveals that existing models frequently struggle with the inherent challenge of
decomposing complex reasoning and translating it into visual representations
aligned with human cognitive processes. To address these limitations, we
propose EduVisAgent, a multi-agent collaborative framework that coordinates
specialized agents for instructional planning, reasoning decomposition,
metacognitive prompting, and visualization design. Experimental results show
that EduVisAgent substantially outperforms all baselines, achieving a 40.2%
improvement and delivering more educationally aligned visualizations.
EduVisBench and EduVisAgent are available at
https://github.com/aiming-lab/EduVisBench and
https://github.com/aiming-lab/EduVisAgent.

</details>


### [301] [Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models](https://arxiv.org/abs/2505.16854)
*Jiaqi Wang,Kevin Qinghong Lin,James Cheng,Mike Zheng Shou*

Key words: 强化学习,视觉语言模型,选择性推理,两阶段训练

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: TL;DR: 论文提出了TON方法，通过两阶段训练（SFT和GRPO）让视觉语言模型选择性推理，减少不必要计算，性能不降反升。

Motivation: 受人类选择性推理启发，旨在减少视觉语言模型中不必要的推理步骤以节省计算成本。

Method: 两阶段训练：SFT阶段引入“思维丢弃”操作，GRPO阶段自由探索何时推理。

Result: TON减少90%推理长度，性能不降反升，模型逐步学会跳过不必要步骤。

Conclusion: TON实现了人类选择性推理模式，为强化学习提供新思路。

Abstract: Reinforcement Learning (RL) has proven to be an effective post-training
strategy for enhancing reasoning in vision-language models (VLMs). Group
Relative Policy Optimization (GRPO) is a recent prominent method that
encourages models to generate complete reasoning traces before answering,
leading to increased token usage and computational cost. Inspired by the
human-like thinking process-where people skip reasoning for easy questions but
think carefully when needed-we explore how to enable VLMs to first decide when
reasoning is necessary. To realize this, we propose TON, a two-stage training
strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective
'thought dropout' operation, where reasoning traces are randomly replaced with
empty thoughts. This introduces a think-or-not format that serves as a cold
start for selective reasoning; (ii) a GRPO stage that enables the model to
freely explore when to think or not, while maximizing task-aware outcome
rewards. Experimental results show that TON can reduce the completion length by
up to 90% compared to vanilla GRPO, without sacrificing performance or even
improving it. Further evaluations across diverse vision-language tasks-covering
a range of reasoning difficulties under both 3B and 7B models-consistently
reveal that the model progressively learns to bypass unnecessary reasoning
steps as training advances. These findings shed light on the path toward
human-like reasoning patterns in reinforcement learning approaches. Our code is
available at https://github.com/kokolerk/TON.

</details>


### [302] [Predicate-Conditional Conformalized Answer Sets for Knowledge Graph Embeddings](https://arxiv.org/abs/2505.16877)
*Yuqicheng Zhu,Daniel Hernández,Yuan He,Zifeng Ding,Bo Xiong,Evgeny Kharlamov,Steffen Staab*

Key words: 知识图谱嵌入, 不确定性量化, 条件覆盖保证, 预测集, 医学诊断

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出CondKGCP方法，通过合并相似向量表示的谓词并利用排名信息增强校准，近似实现谓词条件覆盖保证，同时保持紧凑的预测集。

Motivation: 在知识图谱嵌入（KGE）方法中，不确定性量化对下游应用的可靠性至关重要。现有方法仅提供参考查询和答案集的平均概率保证（边际覆盖保证），而在高风险应用（如医疗诊断）中需要更强的每查询一致性覆盖保证（条件覆盖保证）。

Method: 提出CondKGCP方法，合并相似向量表示的谓词并利用排名信息增强校准，近似实现谓词条件覆盖保证。

Result: 通过理论证明和综合评估验证了CondKGCP的有效性，能够保持紧凑的预测集同时近似满足条件覆盖保证。

Conclusion: CondKGCP在保持预测集紧凑性的同时，有效解决了KGE方法中条件覆盖保证的需求。

Abstract: Uncertainty quantification in Knowledge Graph Embedding (KGE) methods is
crucial for ensuring the reliability of downstream applications. A recent work
applies conformal prediction to KGE methods, providing uncertainty estimates by
generating a set of answers that is guaranteed to include the true answer with
a predefined confidence level. However, existing methods provide probabilistic
guarantees averaged over a reference set of queries and answers (marginal
coverage guarantee). In high-stakes applications such as medical diagnosis, a
stronger guarantee is often required: the predicted sets must provide
consistent coverage per query (conditional coverage guarantee). We propose
CondKGCP, a novel method that approximates predicate-conditional coverage
guarantees while maintaining compact prediction sets. CondKGCP merges
predicates with similar vector representations and augments calibration with
rank information. We prove the theoretical guarantees and demonstrate empirical
effectiveness of CondKGCP by comprehensive evaluations.

</details>


### [303] [Identifying, Evaluating, and Mitigating Risks of AI Thought Partnerships](https://arxiv.org/abs/2505.16899)
*Kerem Oktar,Katherine M. Collins,Jose Hernandez-Orallo,Diane Coyle,Stephen Cave,Adrian Weller,Ilia Sucholutsky*

Key words: AI思维伙伴, 协作认知, 风险评估, RISc框架, 缓解策略

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 这篇评论文章探讨了AI思维伙伴（能够与人类在复杂推理中合作的新型AI模型）的多层次风险，并提出了一个名为RISc的框架来识别这些风险。文章还提出了风险评估的具体指标和缓解策略。

Motivation: 随着AI技术的发展，新型的AI模型（思维伙伴）能够与人类在复杂推理中深度合作，但这也带来了新的风险。文章旨在系统化识别这些风险并提出应对措施。

Method: 通过构建RISc框架（Real-time、Individual、Societal risks from collaborative cognition），作者在实时、个体和社会三个层面分析风险，并设计具体指标评估风险。

Result: 识别了AI思维伙伴在协作认知中可能引发的多层面风险，并提出了评估指标和针对开发者及政策制定者的具体缓解策略。

Conclusion: AI思维伙伴的广泛使用需要配套的风险管理措施，以确保其带来益处而非危害。文章为开发者和管理者提供了实用的指导。

Abstract: Artificial Intelligence (AI) systems have historically been used as tools
that execute narrowly defined tasks. Yet recent advances in AI have unlocked
possibilities for a new class of models that genuinely collaborate with humans
in complex reasoning, from conceptualizing problems to brainstorming solutions.
Such AI thought partners enable novel forms of collaboration and extended
cognition, yet they also pose major risks-including and beyond risks of typical
AI tools and agents. In this commentary, we systematically identify risks of AI
thought partners through a novel framework that identifies risks at multiple
levels of analysis, including Real-time, Individual, and Societal risks arising
from collaborative cognition (RISc). We leverage this framework to propose
concrete metrics for risk evaluation, and finally suggest specific mitigation
strategies for developers and policymakers. As AI thought partners continue to
proliferate, these strategies can help prevent major harms and ensure that
humans actively benefit from productive thought partnerships.

</details>


### [304] [Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning](https://arxiv.org/abs/2505.16928)
*Bosung Kim,Prithviraj Ammanabrolu*

Key words: ∞-THOR, 具身AI, 长程推理, Goal-State-Action建模, Context Parallelism

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: ∞-THOR是一个新的长程具身任务框架，强调长上下文理解，提供轨迹生成、新任务设计和数据集，探索了架构适应方法，实验结果展示了长程任务的挑战和模型行为。

Motivation: 解决具身AI中长程上下文理解和复杂任务规划的挑战，为下一代具身AI系统奠定基础。

Method: 提出轨迹生成框架、新QA任务Needle(s) in the Embodied Haystack，探索Goal-State-Action建模、上下文扩展技术和Context Parallelism。

Result: 构建了长程数据集和基准测试套件，实验分析了长程条件下的模型行为和训练策略。

Conclusion: ∞-THOR为具身AI提供了长程推理和规划的基础，展示了其在复杂任务中的潜力。

Abstract: We introduce $\infty$-THOR, a new framework for long-horizon embodied tasks
that advances long-context understanding in embodied AI. $\infty$-THOR
provides: (1) a generation framework for synthesizing scalable, reproducible,
and unlimited long-horizon trajectories; (2) a novel embodied QA task,
Needle(s) in the Embodied Haystack, where multiple scattered clues across
extended trajectories test agents' long-context reasoning ability; and (3) a
long-horizon dataset and benchmark suite featuring complex tasks that span
hundreds of environment steps, each paired with ground-truth action sequences.
To enable this capability, we explore architectural adaptations, including
interleaved Goal-State-Action modeling, context extension techniques, and
Context Parallelism, to equip LLM-based agents for extreme long-context
reasoning and interaction. Experimental results and analyses highlight the
challenges posed by our benchmark and provide insights into training strategies
and model behaviors under long-horizon conditions. Our work provides a
foundation for the next generation of embodied AI systems capable of robust,
long-term reasoning and planning.

</details>


### [305] [NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification](https://arxiv.org/abs/2505.16938)
*NovelSeek Team,Bo Zhang,Shiyang Feng,Xiangchao Yan,Jiakang Yuan,Zhiyin Yu,Xiaohan He,Songtao Huang,Shaowei Hou,Zheng Nie,Zhilong Wang,Jinyao Liu,Runmin Ma,Tianshuo Peng,Peng Ye,Dongzhan Zhou,Shufei Zhang,Xiaosong Wang,Yilan Zhang,Meng Li,Zhongying Tu,Xiangyu Yue,Wangli Ouyang,Bowen Zhou,Lei Bai*

Key words: 人工智能, 自主科学研究, 多智能体框架, 效率提升, 创新

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: NovelSeek是一个用于自主科学研究的闭环多智能体框架，具有可扩展性、交互性和高效性，显著提升了科学研究的效率和准确性。

Motivation: 加速科学研究范式的转变，通过人工智能提升研究效率和推动创新。

Method: 提出统一的闭环多智能体框架NovelSeek，支持跨领域科学研究的复杂问题解决，包含人机交互和多智能体协作。

Result: 在多个科学任务中表现出色，如反应产率预测、增强子活性预测和2D语义分割，性能提升显著且时间成本低。

Conclusion: NovelSeek是一个高效的自主科学研究工具，能够快速生成创新性解决方案并整合专家知识。

Abstract: Artificial Intelligence (AI) is accelerating the transformation of scientific
research paradigms, not only enhancing research efficiency but also driving
innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework
to conduct Autonomous Scientific Research (ASR) across various scientific
research fields, enabling researchers to tackle complicated problems in these
fields with unprecedented speed and precision. NovelSeek highlights three key
advantages: 1) Scalability: NovelSeek has demonstrated its versatility across
12 scientific research tasks, capable of generating innovative ideas to enhance
the performance of baseline code. 2) Interactivity: NovelSeek provides an
interface for human expert feedback and multi-agent interaction in automated
end-to-end processes, allowing for the seamless integration of domain expert
knowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in
several scientific fields with significantly less time cost compared to human
efforts. For instance, in reaction yield prediction, it increased from 27.6% to
35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from
0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,
precision advanced from 78.8% to 81.0% in a mere 30 hours.

</details>


### [306] [AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios](https://arxiv.org/abs/2505.16944)
*Yunjia Qi,Hao Peng,Xiaozhi Wang,Amy Xin,Youfeng Liu,Bin Xu,Lei Hou,Juanzi Li*

Key words: 大语言模型、代理系统、指令遵循、基准评测、复杂约束

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: AgentIF是首个系统性评估大语言模型（LLM）在代理场景下遵循指令能力的基准，基于50个真实应用构建，包含长且复杂的指令。当前LLM表现不佳，尤其在处理复杂约束和工具规范时。

Motivation: 代理场景中复杂、冗长的指令（如系统提示和工具规范）对LLM的实际应用至关重要，但其遵循能力尚未被充分研究。

Method: 构建AgentIF基准，涵盖50个真实代理任务，收录707条人工标注指令，平均1723词/条，含11.9个约束/条。采用代码、LLM及混合评估方法。

Result: 现有LLM表现普遍较差，尤其在处理复杂约束结构和工具规范时。错误分析揭示了指令长度和元约束的影响。

Conclusion: AgentIF为评估LLM指令遵循能力提供了系统性工具，揭示了当前模型的局限性，推动未来研究改进。

Abstract: Large Language Models (LLMs) have demonstrated advanced capabilities in
real-world agentic applications. Growing research efforts aim to develop
LLM-based agents to address practical demands, introducing a new challenge:
agentic scenarios often involve lengthy instructions with complex constraints,
such as extended system prompts and detailed tool specifications. While
adherence to such instructions is crucial for agentic applications, whether
LLMs can reliably follow them remains underexplored. In this paper, we
introduce AgentIF, the first benchmark for systematically evaluating LLM
instruction following ability in agentic scenarios. AgentIF features three key
characteristics: (1) Realistic, constructed from 50 real-world agentic
applications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.
(3) Complex, averaging 11.9 constraints per instruction, covering diverse
constraint types, such as tool specifications and condition constraints. To
construct AgentIF, we collect 707 human-annotated instructions across 50
agentic tasks from industrial application agents and open-source agentic
systems. For each instruction, we annotate the associated constraints and
corresponding evaluation metrics, including code-based evaluation, LLM-based
evaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically
evaluate existing advanced LLMs. We observe that current models generally
perform poorly, especially in handling complex constraint structures and tool
specifications. We further conduct error analysis and analytical experiments on
instruction length and meta constraints, providing some findings about the
failure modes of existing LLMs. We have released the code and data to
facilitate future research.

</details>


### [307] [HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar Generation](https://arxiv.org/abs/2505.16978)
*Weizhi Tang,Yixuan Li,Chris Sypherd,Elizabeth Polgreen,Vaishak Belle*

Key words: 大型语言模型、语法生成、混合遗传算法、自然语言处理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型（LLMs）在少样本语法生成任务中的表现，发现现有模型效果不佳，并提出了一种混合遗传算法（HyGenar）来优化语法生成。

Motivation: 研究大型语言模型在语法生成任务中的能力，弥补现有模型在该领域的不足。

Method: 构建包含540个语法生成挑战的新数据集，设计6个评估指标，提出基于LLM的混合遗传算法HyGenar。

Result: HyGenar显著提升了生成语法的语法和语义正确性。

Conclusion: HyGenar为LLM在语法生成任务中的优化提供了有效解决方案。

Abstract: Grammar plays a critical role in natural language processing and text/code
generation by enabling the definition of syntax, the creation of parsers, and
guiding structured outputs. Although large language models (LLMs) demonstrate
impressive capabilities across domains, their ability to infer and generate
grammars has not yet been thoroughly explored. In this paper, we aim to study
and improve the ability of LLMs for few-shot grammar generation, where grammars
are inferred from sets of a small number of positive and negative examples and
generated in Backus-Naur Form. To explore this, we introduced a novel dataset
comprising 540 structured grammar generation challenges, devised 6 metrics, and
evaluated 8 various LLMs against it. Our findings reveal that existing LLMs
perform sub-optimally in grammar generation. To address this, we propose an
LLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar
generation. HyGenar achieves substantial improvements in both the syntactic and
semantic correctness of generated grammars across LLMs.

</details>


### [308] [Know the Ropes: A Heuristic Strategy for LLM-based Multi-Agent System Design](https://arxiv.org/abs/2505.16979)
*Zhenkun Li,Lingyao Li,Shuhang Lin,Yongfeng Zhang*

Key words: 多智能体系统,LLM局限性,任务分解,领域先验,轻量增强

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: KtR框架通过将领域先验转化为算法蓝图层次结构，递归分解任务并针对性增强，提升小模型的协作能力，解决单智能体LLM的局限性。

Motivation: 单智能体LLM存在上下文限制、角色过载和领域迁移脆弱性，传统多智能体方法虽缓解但引入新问题（如分解模糊、验证开销），因此提出KtR框架。

Method: KtR将领域知识转化为层次化蓝图，递归分解任务为子任务，结合轻量增强（如思维链、微调、自检）分派给微型模型协作解决。

Result: 在背包问题（3-8项）中，3个GPT-4o-mini智能体从零样本3%准确率提升至95%（5项实例）；任务分配问题（6-15任务）中，6智能体实现100%（≤10任务）和84%（13-15任务）准确率，远超零样本11%。

Conclusion: 算法感知的任务分解与针对性增强可使小型模型成为可靠协作体，无需依赖大型单体模型。

Abstract: Single-agent LLMs hit hard limits--finite context, role overload, and brittle
domain transfer. Conventional multi-agent fixes soften those edges yet expose
fresh pains: ill-posed decompositions, fuzzy contracts, and verification
overhead that blunts the gains. We therefore present Know-The-Ropes (KtR), a
framework that converts domain priors into an algorithmic blueprint hierarchy,
in which tasks are recursively split into typed, controller-mediated subtasks,
each solved zero-shot or with the lightest viable boost (e.g.,
chain-of-thought, micro-tune, self-check). Grounded in the No-Free-Lunch
theorem, KtR trades the chase for a universal prompt for disciplined
decomposition. On the Knapsack problem (3-8 items), three GPT-4o-mini agents
raise accuracy from 3% zero-shot to 95% on size-5 instances after patching a
single bottleneck agent. On the tougher Task-Assignment problem (6-15 jobs), a
six-agent o3-mini blueprint hits 100% up to size 10 and 84% on sizes 13-15,
versus 11% zero-shot. Algorithm-aware decomposition plus targeted augmentation
thus turns modest models into reliable collaborators--no ever-larger monoliths
required.

</details>


### [309] [Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine](https://arxiv.org/abs/2505.16982)
*Adib Bazgir,Amir Habibdoust Lafmajani,Yuwen Zhang*

Key words: 大型语言模型（LLMs）, 因果推理, 多模态数据, 生物医学, 药物发现

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 这篇论文探讨了如何将大型语言模型（LLMs）与因果推理结合，构建多模态数据的因果LLM代理，以解决生物医学领域的挑战，并提出未来研究方向。

Motivation: 现有LLMs在生物医学领域的因果理解不足，依赖相关性而非因果关系，因此需要开发能整合多模态数据并进行干预推理的智能代理。

Method: 整合多模态数据（文本、图像、基因组等）、设计安全可控的代理框架、开发严格的因果评估基准、结合知识图谱（KGs）和形式化因果推理工具。

Result: 论文提出了一种新的研究方向，通过因果LLM代理加速药物发现和实现个性化医疗。

Conclusion: 未来需要跨学科合作，结合因果概念与基础模型，开发可靠的生物医学AI。

Abstract: Large Language Models (LLMs) show promise in biomedicine but lack true causal
understanding, relying instead on correlations. This paper envisions causal LLM
agents that integrate multimodal data (text, images, genomics, etc.) and
perform intervention-based reasoning to infer cause-and-effect. Addressing this
requires overcoming key challenges: designing safe, controllable agentic
frameworks; developing rigorous benchmarks for causal evaluation; integrating
heterogeneous data sources; and synergistically combining LLMs with structured
knowledge (KGs) and formal causal inference tools. Such agents could unlock
transformative opportunities, including accelerating drug discovery through
automated hypothesis generation and simulation, enabling personalized medicine
through patient-specific causal models. This research agenda aims to foster
interdisciplinary efforts, bridging causal concepts and foundation models to
develop reliable AI partners for biomedical progress.

</details>


### [310] [X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs](https://arxiv.org/abs/2505.16997)
*Rui Ye,Xiangrui Liu,Qimin Wu,Xianghe Pang,Zhenfei Yin,Lei Bai,Siheng Chen*

Key words: LLM, 多代理系统, 异构模型, 协同智能, X-MAS-Bench

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 这篇论文提出了一种异构LLM驱动的多代理系统（X-MAS），通过结合多样化的LLM提升系统智能。实验表明异构配置能显著提升性能，无需结构重新设计。

Motivation: 现有MAS框架通常依赖单一LLM驱动所有代理，限制了系统智能。本文探索异构LLM驱动的MAS，以发挥多样LLM的协同潜力。

Method: 提出了X-MAS-Bench测试平台，涵盖5个领域和5种功能，对27个LLM进行了170万次评估，以确定最优模型组合。

Result: 异构配置在特定场景下性能提升显著，例如在数学数据集上提升8.4%，在AIME数据集上提升47%。

Conclusion: 异构LLM驱动的MAS展现了推动可扩展协作AI系统的潜力。

Abstract: LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by
enabling cooperation among multiple specialized agents. However, most existing
MAS frameworks rely on a single LLM to drive all agents, constraining the
system's intelligence to the limit of that model. This paper explores the
paradigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by
diverse LLMs, elevating the system's potential to the collective intelligence
of diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to
evaluate the performance of various LLMs across different domains and
MAS-related functions. As an extensive empirical study, we assess 27 LLMs
across 5 domains (encompassing 21 test sets) and 5 functions, conducting over
1.7 million evaluations to identify optimal model selections for each
domain-function combination. Building on these findings, we demonstrate that
transitioning from homogeneous to heterogeneous LLM-driven MAS can
significantly enhance system performance without requiring structural redesign.
Specifically, in a chatbot-only MAS scenario, the heterogeneous configuration
yields up to 8.4\% performance improvement on the MATH dataset. In a mixed
chatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable
47\% performance boost on the AIME dataset. Our results underscore the
transformative potential of heterogeneous LLMs in MAS, highlighting a promising
avenue for advancing scalable, collaborative AI systems.

</details>


### [311] [Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning](https://arxiv.org/abs/2505.14403)
*Zhaohui Yang,Shilei Jiang,Chen Hu,Linjing Li,Shihong Deng,Daxin Jiang*

Key words: 推理模型,负样本增强,离线强化学习,BCPG-NSA

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种名为BCPG-NSA的精细离线强化学习框架，通过挖掘负样本中的有价值信息提高推理模型的训练效率。

Motivation: 现有方法未能充分利用负样本中的自反思和纠错步骤等有价值信息，导致训练效率低下。

Method: 提出BCPG-NSA框架，分为样本分割、基于共识的步骤正确性评估以及带负样本增强的策略优化三个阶段。

Result: 实验表明，BCPG-NSA在数学/编程推理任务中表现优于基线方法，提升了样本效率并展示了鲁棒性和可扩展性。

Conclusion: BCPG-NSA通过有效利用负样本中的学习信号，显著提升了推理模型的训练效率和性能。

Abstract: Recent advances in reasoning language models have witnessed a paradigm shift
from short to long CoT pattern. Given the substantial computational cost of
rollouts in long CoT models, maximizing the utility of fixed training datasets
becomes crucial. Our analysis reveals that negative responses contain valuable
components such as self-reflection and error-correction steps, yet primary
existing methods either completely discard negative samples (RFT) or apply
equal penalization across all tokens (RL), failing to leverage these potential
learning signals. In light of this, we propose Behavior Constrained Policy
Gradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline
RL framework that encompasses three stages: 1) sample segmentation, 2)
consensus-based step correctness assessment combining LLM and PRM judgers, and
3) policy optimization with NSA designed to effectively mine positive steps
within negative samples. Experimental results show that BCPG-NSA outperforms
baselines on several challenging math/coding reasoning benchmarks using the
same training dataset, achieving improved sample efficiency and demonstrating
robustness and scalability when extended to multiple iterations.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [312] [Interpretable Machine Learning for Macro Alpha: A News Sentiment Case Study](https://arxiv.org/abs/2505.16136)
*Yuke Zhang*

Key words: 机器学习, 宏观经济alpha, 新闻情绪, FinBERT, XGBoost, SHAP

<details>
  <summary>Details</summary>

Main category: q-fin.CP

TL;DR: 该研究提出了一种可解释的机器学习框架，利用全球新闻情绪提取宏观经济alpha值。通过FinBERT处理GDELT新闻数据，构建包含情绪均值、离散度和事件影响的指数，并结合XGBoost分类器预测EUR/USD、USD/JPY和10年期美国国债期货的次日收益，表现出色。

Motivation: 研究旨在通过结合领域特定的自然语言处理（NLP）和可解释的机器学习（ML），从全球新闻情绪中提取可解释的宏观alpha值。

Method: 使用FinBERT处理GDELT新闻数据，构建情绪指数，并通过XGBoost分类器预测金融资产收益，同时使用SHAP方法进行特征解释。

Result: XGBoost策略在回溯测试中表现出色，Sharpe比率和年复合增长率均显著高于基准，情绪离散度和文章影响是关键预测特征。

Conclusion: 研究表明，结合领域特定NLP和可解释ML是提取宏观alpha值的有效且可解释的方法。

Abstract: This study introduces an interpretable machine learning (ML) framework to
extract macroeconomic alpha from global news sentiment. We process the Global
Database of Events, Language, and Tone (GDELT) Project's worldwide news feed
using FinBERT -- a Bidirectional Encoder Representations from Transformers
(BERT) based model pretrained on finance-specific language -- to construct
daily sentiment indices incorporating mean tone, dispersion, and event impact.
These indices drive an XGBoost classifier, benchmarked against logistic
regression, to predict next-day returns for EUR/USD, USD/JPY, and 10-year U.S.
Treasury futures (ZN). Rigorous out-of-sample (OOS) backtesting (5-fold
expanding-window cross-validation, OOS period: c. 2017-April 2025) demonstrates
exceptional, cost-adjusted performance for the XGBoost strategy: Sharpe ratios
achieve 5.87 (EUR/USD), 4.65 (USD/JPY), and 4.65 (Treasuries), with respective
compound annual growth rates (CAGRs) exceeding 50% in Foreign Exchange (FX) and
22% in bonds. Shapley Additive Explanations (SHAP) affirm that sentiment
dispersion and article impact are key predictive features. Our findings
establish that integrating domain-specific Natural Language Processing (NLP)
with interpretable ML offers a potent and explainable source of macro alpha.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [313] [Materials Generation in the Era of Artificial Intelligence: A Comprehensive Survey](https://arxiv.org/abs/2505.16379)
*Zhixun Li,Bin Cao,Rui Jiao,Liang Wang,Ding Wang,Yang Liu,Dingshuo Chen,Jia Li,Qiang Liu,Yu Rong,Liang Wang,Tong-yi Zhang,Jeffrey Xu Yu*

Key words: 材料生成，人工智能，数据驱动，晶体材料，评估指标

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

TL;DR: 该论文综述了AI驱动的材料生成领域的最新进展，包括材料分类、表示方法、生成方法、评估指标和开源资源，并探讨了未来挑战。

Motivation: 材料发现对解决全球挑战至关重要，AI与高质量材料数据的结合为加速这一过程提供了新机会，但缺乏系统性综述。

Method: 论文首先组织材料类型并展示晶体材料的多种表示，然后总结和分类当前AI驱动的材料生成方法，讨论评估指标，并汇总开源代码和数据集。

Result: 提供了AI驱动材料生成的全面概述，包括分类、方法、评估标准和资源整合。

Conclusion: 论文填补了系统性综述的空白，并提出了这一快速发展领域的未来方向和挑战。

Abstract: Materials are the foundation of modern society, underpinning advancements in
energy, electronics, healthcare, transportation, and infrastructure. The
ability to discover and design new materials with tailored properties is
critical to solving some of the most pressing global challenges. In recent
years, the growing availability of high-quality materials data combined with
rapid advances in Artificial Intelligence (AI) has opened new opportunities for
accelerating materials discovery. Data-driven generative models provide a
powerful tool for materials design by directly create novel materials that
satisfy predefined property requirements. Despite the proliferation of related
work, there remains a notable lack of up-to-date and systematic surveys in this
area. To fill this gap, this paper provides a comprehensive overview of recent
progress in AI-driven materials generation. We first organize various types of
materials and illustrate multiple representations of crystalline materials. We
then provide a detailed summary and taxonomy of current AI-driven materials
generation approaches. Furthermore, we discuss the common evaluation metrics
and summarize open-source codes and benchmark datasets. Finally, we conclude
with potential future directions and challenges in this fast-growing field. The
related sources can be found at
https://github.com/ZhixunLEE/Awesome-AI-for-Materials-Generation.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [314] [Multi-omic Causal Discovery using Genotypes and Gene Expression](https://arxiv.org/abs/2505.15866)
*Stephen Asiedu,David Watson*

Key words: 因果发现、多组学、基因调控、GENESIS、约束算法

<details>
  <summary>Details</summary>

Main category: q-bio.GN

TL;DR: GENESIS是一种基于约束的算法，利用基因型的自然因果优先性推断转录组数据中的祖先关系，克服了高维度和隐藏混杂因素的挑战。

Motivation: 多组学数据中的因果发现对理解基因调控机制至关重要，但面临高维度、直接与间接关系区分及隐藏混杂因素的挑战。

Method: GENESIS通过初始化空的祖先矩阵，结合基因型作为固定因果锚点，逐步填充直接、间接或非因果关系。

Result: 在合成和真实基因组数据集上验证了GENESIS的有效性，为复杂性状的因果路径发现提供了新途径。

Conclusion: GENESIS为功能基因组学、药物发现和精准医学中的因果发现提供了有力工具。

Abstract: Causal discovery in multi-omic datasets is crucial for understanding the
bigger picture of gene regulatory mechanisms, but remains challenging due to
high dimensionality, differentiation of direct from indirect relationships, and
hidden confounders. We introduce GENESIS (GEne Network inference from
Expression SIgnals and SNPs), a constraint-based algorithm that leverages the
natural causal precedence of genotypes to infer ancestral relationships in
transcriptomic data. Unlike traditional causal discovery methods that start
with a fully connected graph, GENESIS initialises an empty ancestrality matrix
and iteratively populates it with direct, indirect or non-causal relationships
using a series of provably sound marginal and conditional independence tests.
By integrating genotypes as fixed causal anchors, GENESIS provides a principled
``head start'' to classical causal discovery algorithms, restricting the search
space to biologically plausible edges. We test GENESIS on synthetic and
real-world genomic datasets. This framework offers a powerful avenue for
uncovering causal pathways in complex traits, with promising applications to
functional genomics, drug discovery, and precision medicine.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [315] [A Novel Compound AI Model for 6G Networks in 3D Continuum](https://arxiv.org/abs/2505.15821)
*Milos Gravara,Andrija Stanisic,Stefan Nastic*

Key words: 6G网络,3D连续体,Compound AI系统,模块化架构,分布式智能

<details>
  <summary>Details</summary>

Main category: cs.NI

TL;DR: 论文提出了一种新型的三部分框架Compound AI系统，用于解决6G网络在3D连续体中的复杂任务分解与协调问题，克服了现有单体AI模型的局限性。

Motivation: 当前AI方法在网络管理中存在无法跨域交互、缺乏适应性及高计算资源需求的问题，亟需一种新的解决方案。

Method: 采用三部分框架，将复杂任务分解为可互操作的专业模块，实现分布式智能协调。

Result: 提出了模块化架构，能够应对6G网络在3D连续体中的异构需求，但需权衡模型与系统性能。

Conclusion: Compound AI系统为6G网络提供了新思路，但仍面临跨域资源协调、动态拓扑适应等挑战。

Abstract: The 3D continuum presents a complex environment that spans the terrestrial,
aerial and space domains, with 6Gnetworks serving as a key enabling technology.
Current AI approaches for network management rely on monolithic models that
fail to capture cross-domain interactions, lack adaptability,and demand
prohibitive computational resources. This paper presents a formal model of
Compound AI systems, introducing a novel tripartite framework that decomposes
complex tasks into specialized, interoperable modules. The proposed modular
architecture provides essential capabilities to address the unique challenges
of 6G networks in the 3D continuum, where heterogeneous components require
coordinated, yet distributed, intelligence. This approach introduces a
fundamental trade-off between model and system performance, which must be
carefully addressed. Furthermore, we identify key challenges faced by Compound
AI systems within 6G networks operating in the 3D continuum, including
cross-domain resource orchestration, adaptation to dynamic topologies, and the
maintenance of consistent AI service quality across heterogeneous environments.

</details>


### [316] [Generative AI-Aided QoE Maximization for RIS-Assisted Digital Twin Interaction](https://arxiv.org/abs/2505.15828)
*Jiayuan Chen,Yuxiang Li,Changyan Yi,Shimin Gong*

Key words: 经验质量, 可重构智能表面, 数字孪生, 生成人工智能, 资源分配

<details>
  <summary>Details</summary>

Main category: cs.NI

TL;DR: 该论文研究了基于可重构智能表面（RIS）辅助的数字孪生（DT）交互中用户体验质量（QoE）的资源分配问题，提出了一种结合决策变换器和生成人工智能（GAI）的PG-ZFO方法，以优化相位偏移矩阵、波束成形矩阵等，并在仿真中验证了其有效性。

Motivation: 在RIS辅助的DT交互中，用户QoE的优化面临DT模型不确定演化的挑战，需要一种动态高效的资源分配方法。

Method: 提出PG-ZFO方法，结合决策变换器的动态优化能力和GAI的泛化优势，优化相位偏移矩阵、波束成形矩阵、渲染分辨率和计算资源分配。

Result: 仿真结果表明，PG-ZFO在优化用户QoE方面表现出色，优于对比方法。

Conclusion: PG-ZFO通过动态优化和GAI的结合，有效解决了DT模型演化带来的资源分配挑战，显著提升了用户体验。

Abstract: In this paper, we investigate a quality of experience (QoE)-aware resource
allocation problem for reconfigurable intelligent surface (RIS)-assisted
digital twin (DT) interaction with uncertain evolution. In the considered
system, mobile users are expected to interact with a DT model maintained on a
DT server that is deployed on a base station, via effective uplink and downlink
channels assisted by an RIS. Our goal is to maximize the sum of all mobile
users' joint subjective and objective QoE in DT interactions across various DT
scenes, by jointly optimizing phase shift matrix, receive/transmit beamforming
matrix, rendering resolution configuration and computing resource allocation.
While solving this problem is challenging mainly due to the uncertain evolution
of the DT model, which leads to multiple scene-specific problems, and require
us to constantly re-solve each of them whenever DT model evolves.
  To this end, leveraging the dynamic optimization capabilities of decision
transformers and the generalization strengths of generative artificial
intelligence (GAI), we propose a novel GAI-aided approach, called the
prompt-guided decision transformer integrated with zero-forcing optimization
(PG-ZFO). Simulations are conducted to evaluate the proposed PG-ZFO,
demonstrating its effectiveness and superiority over counterparts.

</details>


### [317] [Transforming Decoder-Only Transformers for Accurate WiFi-Telemetry Based Indoor Localization](https://arxiv.org/abs/2505.15835)
*Nayan Sanjay Bhatia,Katia Obraczka*

Key words: WiFi定位, GPT, 大型语言模型, 无线遥测, 室内导航

<details>
  <summary>Details</summary>

Main category: cs.NI

TL;DR: WiFiGPT是一种基于GPT的系统，用于解决WiFi室内定位中的信号变化问题，通过捕捉无线遥测中的空间模式，取得了高定位精度，且无需手动信号处理或校准。

Motivation: 当前WiFi定位技术因环境变化、设备差异和用例需求多样化而难以统一高效处理，WiFiGPT旨在解决这些问题，提供高精度定位。

Method: 采用生成预训练变换器（GPT）模型，尤其是大型语言模型（LLMs），从嘈杂的无线遥测数据中提取空间模式。

Result: 实验显示WiFiGPT在RSSI和FTM上达到亚米级精度，CSI上达到厘米级精度，表现优于现有技术。

Conclusion: LLM为基础的定位技术在无需手动信号处理的情况下，具备超越专用技术的潜力。

Abstract: Wireless Fidelity (WiFi) based indoor positioning is a widely researched area
for determining the position of devices within a wireless network. Accurate
indoor location has numerous applications, such as asset tracking and indoor
navigation. Despite advances in WiFi localization techniques -- in particular
approaches that leverage WiFi telemetry -- their adoption in practice remains
limited due to several factors including environmental changes that cause
signal fading, multipath effects, interference, which, in turn, impact
positioning accuracy. In addition, telemetry data differs depending on the WiFi
device vendor, offering distinct features and formats; use case requirements
can also vary widely. Currently, there is no unified model to handle all these
variations effectively. In this paper, we present WiFiGPT, a Generative
Pretrained Transformer (GPT) based system that is able to handle these
variations while achieving high localization accuracy. Our experiments with
WiFiGPT demonstrate that GPTs, in particular Large Language Models (LLMs), can
effectively capture subtle spatial patterns in noisy wireless telemetry, making
them reliable regressors. Compared to existing state-of-the-art methods, our
method matches and often surpasses conventional approaches for multiple types
of telemetry. Achieving sub-meter accuracy for RSSI and FTM and
centimeter-level precision for CSI demonstrates the potential of LLM-based
localisation to outperform specialized techniques, all without handcrafted
signal processing or calibration.

</details>


### [318] [Integration of TinyML and LargeML: A Survey of 6G and Beyond](https://arxiv.org/abs/2505.15854)
*Thai-Hoc Vu,Ngo Hoang Tu,Thien Huynh-The,Kyungchun Lee,Sunghwan Kim,Miroslav Voznak,Quoc-Viet Pham*

Key words: 6G网络, TinyML, LargeML, 机器学习, IoT

<details>
  <summary>Details</summary>

Main category: cs.NI

TL;DR: 本文探讨了从5G到6G网络中机器学习（尤其是深度学习）的重要作用，重点介绍了TinyML和LargeML的集成潜力及其在智能服务中的应用，并总结了当前挑战和未来研究方向。

Motivation: 随着6G网络的发展，对机器学习的需求激增，尤其是资源受限的IoT设备推动了TinyML的发展。然而，大规模ML部署面临计算资源和管理的挑战，因此集成TinyML和LargeML成为未来高效资源管理和无缝连接的关键。

Method: 本文通过综述和分析最新研究，探讨TinyML和LargeML的集成方法及其在6G网络中的应用，重点关注性能优化、资源管理和安全问题。

Result: 研究表明，TinyML和LargeML的集成在6G网络中具有巨大潜力，但仍需解决性能优化、部署策略和安全性等挑战。

Conclusion: 未来研究应聚焦于TinyML和LargeML的全面集成，以推动6G网络中智能服务的实现。

Abstract: The transition from 5G networks to 6G highlights a significant demand for
machine learning (ML). Deep learning models, in particular, have seen wide
application in mobile networking and communications to support advanced
services in emerging wireless environments, such as smart healthcare, smart
grids, autonomous vehicles, aerial platforms, digital twins, and the metaverse.
The rapid expansion of Internet-of-Things (IoT) devices, many with limited
computational capabilities, has accelerated the development of tiny machine
learning (TinyML) and resource-efficient ML approaches for cost-effective
services. However, the deployment of large-scale machine learning (LargeML)
solutions require major computing resources and complex management strategies
to support extensive IoT services and ML-generated content applications.
Consequently, the integration of TinyML and LargeML is projected as a promising
approach for future seamless connectivity and efficient resource management.
  Although the integration of TinyML and LargeML shows abundant potential,
several challenges persist, including performance optimization, practical
deployment strategies, effective resource management, and security
considerations. In this survey, we review and analyze the latest research aimed
at enabling the integration of TinyML and LargeML models for the realization of
smart services and applications in future 6G networks and beyond. The paper
concludes by outlining critical challenges and identifying future research
directions for the holistic integration of TinyML and LargeML in
next-generation wireless networks.

</details>


### [319] [Graph Neural Networks Based Anomalous RSSI Detection](https://arxiv.org/abs/2505.15847)
*Blaž Bertalanič,Matej Vnučec,Carolina Fortuna*

Key words: 异常检测，图神经网络，时间序列，IoT网络，图注意力网络

<details>
  <summary>Details</summary>

Main category: cs.NI

TL;DR: 该论文提出了一种基于图神经网络的新方法，用于无线链路中的异常检测，该方法将时间序列数据转化为图，并通过改进的图注意力网络架构有效检测异常，计算效率高。

Motivation: 现代基础设施中IoT网络的扩大使得异常检测变得至关重要，以防止业务中断。传统方法难以高效处理此类问题，因此需要新方法。

Method: 将时间序列数据转化为图结构，设计新的基于图注意力网络的图神经网络架构进行训练。

Result: 模型在异常检测上达到与现有技术相当的效果，且计算效率显著提升，参数数量减少约171倍。

Conclusion: 提出的图神经网络方法在无线链路异常检测中高效且高性能，适合大规模IoT网络应用。

Abstract: In today's world, modern infrastructures are being equipped with information
and communication technologies to create large IoT networks.
  It is essential to monitor these networks to ensure smooth operations by
detecting and correcting link failures or abnormal network behaviour
proactively, which can otherwise cause interruptions in business operations.
  This paper presents a novel method for detecting anomalies in wireless links
using graph neural networks. The proposed approach involves converting time
series data into graphs and training a new graph neural network architecture
based on graph attention networks that successfully detects anomalies at the
level of individual measurements of the time series data. The model provides
competitive results compared to the state of the art while being
computationally more efficient with ~171 times fewer trainable parameters.

</details>


### [320] [LLM-Based Emulation of the Radio Resource Control Layer: Towards AI-Native RAN Protocols](https://arxiv.org/abs/2505.16821)
*Ziming liu,Bryan Liu,Alvaro Valcarce,Xiaoli Chu*

Key words: 6G网络,大型AI模型,RRC消息,协议保真度,AI原生无线标准

<details>
  <summary>Details</summary>

Main category: cs.NI

TL;DR: 论文展示了一种大型AI模型（LAM）在6G网络中生成标准兼容的RRC消息的端到端演示，通过参数高效的方法提升了协议保真度。

Motivation: 探索如何将大型AI模型融入6G网络，解决协议理解和实际部署的挑战，推动AI原生无线标准的发展。

Method: 将RRC消息视为领域特定语言，使用LoRA微调解码器变压器模型，保留ASN.1语法结构并优化训练效率。

Result: 在3万组测试数据上，模型的中位余弦相似度达到0.97，比零样本基线提升61%，显著提高了协议保真度。

Conclusion: 研究表明，结合RAN特定推理的LAM可直接控制协议流程，为AI原生空口范式奠定基础。

Abstract: Integrating large AI models (LAMs) into 6G mobile networks promises to
redefine protocol design and control-plane intelligence by enabling autonomous,
cognitive network operations. While industry concepts, such as ETSI's
Experiential Networked Intelligence (ENI), envision LAM-driven agents for
adaptive network slicing and intent-based management, practical implementations
still face challenges in protocol literacy and real-world deployment. This
paper presents an end-to-end demonstration of a LAM that generates
standards-compliant, ASN.1-encoded Radio Resource Control (RRC) messages as
part of control-plane procedures inside a gNB. We treat RRC messaging as a
domain-specific language and fine-tune a decoder-only transformer model (LLaMA
class) using parameter-efficient Low-Rank Adaptation (LoRA) on RRC messages
linearized to retain their ASN.1 syntactic structure before standard byte-pair
encoding tokenization. This enables combinatorial generalization over RRC
protocol states while minimizing training overhead. On 30k field-test
request-response pairs, our 8 B model achieves a median cosine similarity of
0.97 with ground-truth messages on an edge GPU -- a 61 % relative gain over a
zero-shot LLaMA-3 8B baseline -- indicating substantially improved structural
and semantic RRC fidelity. Overall, our results show that LAMs, when augmented
with Radio Access Network (RAN)-specific reasoning, can directly orchestrate
control-plane procedures, representing a stepping stone toward the AI-native
air-interface paradigm. Beyond RRC emulation, this work lays the groundwork for
future AI-native wireless standards.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [321] [Dynamic Reservoir Computing with Physical Neuromorphic Networks](https://arxiv.org/abs/2505.16813)
*Yinhao Xu,Georg A. Gottwald,Zdenka Kuncic*

Key words: Reservoir Computing, neuromorphic networks, nonlinear dynamics, network sparsity, chaotic time series

<details>
  <summary>Details</summary>

Main category: cs.ET

TL;DR: 该研究探讨了物理纳米电子网络作为RC框架中的动态储备池的应用，发现稀疏网络比密集网络更能生成有用的非线性时间输出，并在混沌时间序列预测任务中验证了网络稀疏性维持动态的重要性。

Motivation: 研究动机在于理解物理储备池的底层结构和内部动态，以及探索纳米电子网络在储备计算中的潜力。

Method: 采用物理纳米电子网络作为动态储备池，研究网络稀疏性对非线性时间输出的影响，并通过混沌时间序列预测任务验证结果。

Result: 稀疏网络比密集网络更能有效维持网络活动和动态，从而成功学习了混沌Lorenz63系统的吸引子行为。

Conclusion: 网络稀疏性在物理储备计算中至关重要，能提升动态储备池的性能。

Abstract: Reservoir Computing (RC) with physical systems requires an understanding of
the underlying structure and internal dynamics of the specific physical
reservoir. In this study, physical nano-electronic networks with neuromorphic
dynamics are investigated for their use as physical reservoirs in an RC
framework. These neuromorphic networks operate as dynamic reservoirs, with node
activities in general coupled to the edge dynamics through nonlinear
nano-electronic circuit elements, and the reservoir outputs influenced by the
underlying network connectivity structure. This study finds that networks with
varying degrees of sparsity generate more useful nonlinear temporal outputs for
dynamic RC compared to dense networks. Dynamic RC is also tested on an
autonomous multivariate chaotic time series prediction task with networks of
varying densities, which revealed the importance of network sparsity in
maintaining network activity and overall dynamics, that in turn enabled the
learning of the chaotic Lorenz63 system's attractor behavior.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [322] [What Lives? A meta-analysis of diverse opinions on the definition of life](https://arxiv.org/abs/2505.15849)
*Reed Bender,Karina Kofman,Blaise Agüera y Arcas,Michael Levin*

Key words: 生命定义、大型语言模型、跨学科分析、语义分析、概念潜在空间

<details>
  <summary>Details</summary>

Main category: q-bio.OT

TL;DR: 论文利用大型语言模型分析跨学科生命定义，揭示生命定义的连续谱系，提出从二元分类转向统一的潜在概念空间。

Motivation: 解决生命定义的多样性和争议，尤其在合成生物学、人工智能和天体生物学快速发展的背景下。

Method: 使用大型语言模型对专家定义进行成对相关性分析，通过聚类、语义分析和t-SNE投影揭示概念原型。

Result: 发现生命定义并非二元分类问题，而是连续的概念谱系，并提出统一潜在空间的概念。

Conclusion: 计算语义分析为科学和哲学中的定义争议提供了新方法论，适用于其他跨学科的类似问题。

Abstract: The question of "what is life?" has challenged scientists and philosophers
for centuries, producing an array of definitions that reflect both the mystery
of its emergence and the diversity of disciplinary perspectives brought to bear
on the question. Despite significant progress in our understanding of
biological systems, psychology, computation, and information theory, no single
definition for life has yet achieved universal acceptance. This challenge
becomes increasingly urgent as advances in synthetic biology, artificial
intelligence, and astrobiology challenge our traditional conceptions of what it
means to be alive. We undertook a methodological approach that leverages large
language models (LLMs) to analyze a set of definitions of life provided by a
curated set of cross-disciplinary experts. We used a novel pairwise correlation
analysis to map the definitions into distinct feature vectors, followed by
agglomerative clustering, intra-cluster semantic analysis, and t-SNE projection
to reveal underlying conceptual archetypes. This methodology revealed a
continuous landscape of the themes relating to the definition of life,
suggesting that what has historically been approached as a binary taxonomic
problem should be instead conceived as differentiated perspectives within a
unified conceptual latent space. We offer a new methodological bridge between
reductionist and holistic approaches to fundamental questions in science and
philosophy, demonstrating how computational semantic analysis can reveal
conceptual patterns across disciplinary boundaries, and opening similar
pathways for addressing other contested definitional territories across the
sciences.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [323] [Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey](https://arxiv.org/abs/2505.15957)
*Chih-Kai Yang,Neo S. Ho,Hung-yi Lee*

Key words: 大型音频语言模型, 评估分类, 系统性调查

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 本文对大型音频语言模型（LALMs）的评估进行了全面调查，提出了一个系统化的分类方法，并指出了未来研究方向。

Motivation: 现有的LALMs评估标准分散且缺乏系统分类，需要通过结构化方法填补这一空白。

Method: 提出了一个四维分类法，涵盖通用听觉处理、知识与推理、对话能力和公平性等多个方面。

Result: 首次对LALMs评估进行了系统性总结，为社区提供了清晰指导。

Conclusion: 研究为LALMs评估提供了框架，并计划持续更新调查结果以支持领域发展。

Abstract: With advancements in large audio-language models (LALMs), which enhance large
language models (LLMs) with auditory capabilities, these models are expected to
demonstrate universal proficiency across various auditory tasks. While numerous
benchmarks have emerged to assess LALMs' performance, they remain fragmented
and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive
survey and propose a systematic taxonomy for LALM evaluations, categorizing
them into four dimensions based on their objectives: (1) General Auditory
Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented
Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed
overviews within each category and highlight challenges in this field, offering
insights into promising future directions. To the best of our knowledge, this
is the first survey specifically focused on the evaluations of LALMs, providing
clear guidelines for the community. We will release the collection of the
surveyed papers and actively maintain it to support ongoing advancements in the
field.

</details>


### [324] [Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency Transcription and Detection](https://arxiv.org/abs/2505.16351)
*Chenxu Guo,Jiachen Lian,Xuanru Zhou,Jinming Zhang,Shuhe Li,Zongli Ye,Hwi Joo Park,Anaisha Das,Zoe Ezzes,Jet Vonk,Brittany Morin,Rian Bogley,Lisa Wauters,Zachary Miller,Maria Gorno-Tempini,Gopala Anumanchipalli*

Key words: 言语不流畅检测, 零样本解码器, 音素转录, WavLM, Dysfluent-WFST

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: Dysfluent-WFST 是一种无需额外训练的零样本解码器，可同时转录音素并检测言语不流畅，实现了语音错误率和不流畅检测的最优性能。

Motivation: 传统方法仅局限于分类且缺乏临床洞察力，无法有效处理上下文依赖的言语不流畅问题，因此需要更高效的解决方案。

Method: 采用 Dysfluent-WFST 零样本解码器，结合上游编码器（如 WavLM），直接建模发音行为以实现音素转录和不流畅检测。

Result: 在模拟和真实语音数据上均实现了语音错误率和不流畅检测的最优性能。

Conclusion: 通过显式建模发音行为的解码方法（而非复杂架构）有效提升了言语不流畅处理系统的性能。

Abstract: Automatic detection of speech dysfluency aids speech-language pathologists in
efficient transcription of disordered speech, enhancing diagnostics and
treatment planning. Traditional methods, often limited to classification,
provide insufficient clinical insight, and text-independent models misclassify
dysfluency, especially in context-dependent cases. This work introduces
Dysfluent-WFST, a zero-shot decoder that simultaneously transcribes phonemes
and detects dysfluency. Unlike previous models, Dysfluent-WFST operates with
upstream encoders like WavLM and requires no additional training. It achieves
state-of-the-art performance in both phonetic error rate and dysfluency
detection on simulated and real speech data. Our approach is lightweight,
interpretable, and effective, demonstrating that explicit modeling of
pronunciation behavior in decoding, rather than complex architectures, is key
to improving dysfluency processing systems.

</details>


### [325] [Multimodal Biomarkers for Schizophrenia: Towards Individual Symptom Severity Estimation](https://arxiv.org/abs/2505.16044)
*Gowtham Premananth,Philip Resnik,Sonia Bansal,Deanna L. Kelly,Carol Espy-Wilson*

Key words: 精神分裂症, 深度学习, 多模态, 症状评估, 个性化治疗

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 该研究通过多模态方法（语音、视频、文本）改进精神分裂症症状严重程度的个体化评估，超越传统二分类任务，提升临床适用性。

Motivation: 传统深度学习将精神分裂症简化为分类任务，忽略了症状复杂性，限制了临床应用价值。本研究旨在通过多模态数据更精确评估个体症状严重程度。

Method: 开发单模态模型（语音、视频、文本）和多模态框架，综合分析症状细节以提高准确性和鲁棒性。

Result: 多模态方法能更详细捕捉症状特征，为精准诊断和个性化治疗提供可扩展的客观工具。

Conclusion: 多模态评估框架比传统分类方法更适用于精神分裂症的临床实践，支持个体化干预。

Abstract: Studies on schizophrenia assessments using deep learning typically treat it
as a classification task to detect the presence or absence of the disorder,
oversimplifying the condition and reducing its clinical applicability. This
traditional approach overlooks the complexity of schizophrenia, limiting its
practical value in healthcare settings. This study shifts the focus to
individual symptom severity estimation using a multimodal approach that
integrates speech, video, and text inputs. We develop unimodal models for each
modality and a multimodal framework to improve accuracy and robustness. By
capturing a more detailed symptom profile, this approach can help in enhancing
diagnostic precision and support personalized treatment, offering a scalable
and objective tool for mental health assessment.

</details>


### [326] [Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition via Meta-learning](https://arxiv.org/abs/2505.16220)
*Liang-Yeh Shen,Shi-Xin Fang,Yi-Cheng Lin,Huang-Cheng Chou,Hung-yi Lee*

Key words: Meta-PerSER, 语音情感识别, 元学习, 个性化, IEMOCAP

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: Meta-PerSER是一种新的元学习框架，通过适应每个听众独特的情绪解释方式，实现个性化语音情感识别（SER）。

Motivation: 传统SER系统依赖聚合标注，忽视了个人差异并导致预测不一致。Meta-PerSER旨在解决这一问题。

Method: 结合Model-Agnostic Meta-Learning (MAML)、Combined-Set Meta-Training、Derivative Annealing和分步学习率，利用预训练的自监督模型提取通用情感特征后个性化调整。

Result: 在IEMOCAP数据集上，Meta-PerSER显著优于基线方法，适用于已知和未知场景。

Conclusion: Meta-PerSER为个性化情感识别提供了有效方案。

Abstract: This paper introduces Meta-PerSER, a novel meta-learning framework that
personalizes Speech Emotion Recognition (SER) by adapting to each listener's
unique way of interpreting emotion. Conventional SER systems rely on aggregated
annotations, which often overlook individual subtleties and lead to
inconsistent predictions. In contrast, Meta-PerSER leverages a Model-Agnostic
Meta-Learning (MAML) approach enhanced with Combined-Set Meta-Training,
Derivative Annealing, and per-layer per-step learning rates, enabling rapid
adaptation with only a few labeled examples. By integrating robust
representations from pre-trained self-supervised models, our framework first
captures general emotional cues and then fine-tunes itself to personal
annotation styles. Experiments on the IEMOCAP corpus demonstrate that
Meta-PerSER significantly outperforms baseline methods in both seen and unseen
data scenarios, highlighting its promise for personalized emotion recognition.

</details>


### [327] [Adversarial Deep Metric Learning for Cross-Modal Audio-Text Alignment in Open-Vocabulary Keyword Spotting](https://arxiv.org/abs/2505.16735)
*Youngmoon Jung,Yong-Hyeok Lee,Myunghun Jung,Jaeyoung Roh,Chang Woo Han,Hoon-Young Cho*

Key words: 开放词汇关键词检测, 深度度量学习, 模态对抗学习, 多模态嵌入, 音素对齐

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 论文提出了一种结合深度度量学习和模态对抗学习的方法，用于优化声学和文本编码器，以减少多模态嵌入之间的异质差异，并在WSJ和LibriPhrase数据集上验证了有效性。

Motivation: 现有的基于文本注册的开放词汇关键词检测方法在声学和文本嵌入的比较上存在模态异质性挑战，需要一种方法来减少这种差异。

Method: 采用深度度量学习（DML）优化编码器，并提出模态对抗学习（MAL）以减少声学和文本模态间的差异，同时通过对抗训练模态度分类器生成模态不变的嵌入。

Result: 在WSJ和LibriPhrase数据集上的实验表明，该方法能有效减少模态差异并实现音素级别的多模态对齐。

Conclusion: 结合DML和MAL的方法显著提升了开放词汇关键词检测的性能，证明了模态对抗学习的有效性。

Abstract: For text enrollment-based open-vocabulary keyword spotting (KWS), acoustic
and text embeddings are typically compared at either the phoneme or utterance
level. To facilitate this, we optimize acoustic and text encoders using deep
metric learning (DML), enabling direct comparison of multi-modal embeddings in
a shared embedding space. However, the inherent heterogeneity between audio and
text modalities presents a significant challenge. To address this, we propose
Modality Adversarial Learning (MAL), which reduces the domain gap in
heterogeneous modality representations. Specifically, we train a modality
classifier adversarially to encourage both encoders to generate
modality-invariant embeddings. Additionally, we apply DML to achieve
phoneme-level alignment between audio and text, and conduct comprehensive
comparisons across various DML objectives. Experiments on the Wall Street
Journal (WSJ) and LibriPhrase datasets demonstrate the effectiveness of the
proposed approach.

</details>


### [328] [SEED: Speaker Embedding Enhancement Diffusion Model](https://arxiv.org/abs/2505.16798)
*KiHyun Nam,Jungwoo Heo,Jee-weon Jung,Gangin Park,Chaeyoung Jung,Ha-Jin Yu,Joon Son Chung*

Key words: 说话人识别,扩散模型,环境失配,嵌入优化

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 提出了一种基于扩散的方法，用于优化说话人识别系统中的嵌入特征，通过噪声注入与反向重构提升环境失配场景下的识别准确率。

Motivation: 解决说话人识别系统在现实应用中因环境失配导致的性能下降问题。

Method: 采用扩散模型，对预训练说话人识别模型提取的嵌入特征进行噪声注入与反向重构，无需修改现有系统或依赖说话人标签。

Result: 在模拟环境失配场景中，识别准确率提升高达19.6%，且不影响常规场景性能。

Conclusion: 该方法有效改善了说话人识别系统在失配环境中的性能，且易于部署。

Abstract: A primary challenge when deploying speaker recognition systems in real-world
applications is performance degradation caused by environmental mismatch. We
propose a diffusion-based method that takes speaker embeddings extracted from a
pre-trained speaker recognition model and generates refined embeddings. For
training, our approach progressively adds Gaussian noise to both clean and
noisy speaker embeddings extracted from clean and noisy speech, respectively,
via forward process of a diffusion model, and then reconstructs them to clean
embeddings in the reverse process. While inferencing, all embeddings are
regenerated via diffusion process. Our method needs neither speaker label nor
any modification to the existing speaker recognition pipeline. Experiments on
evaluation sets simulating environment mismatch scenarios show that our method
can improve recognition accuracy by up to 19.6% over baseline models while
retaining performance on conventional scenarios. We publish our code here
https://github.com/kaistmm/seed-pytorch

</details>


### [329] [Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate](https://arxiv.org/abs/2505.16845)
*Hanglei Zhang,Yiwei Guo,Zhihan Li,Xiang Hao,Xie Chen,Kai Yu*

Key words: 神经语音编解码器，可变帧率（VFR），TFC技术，比特率优化，时间熵

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 论文提出了一种名为TFC的技术，首次在神经语音编解码器中引入可变帧率（VFR），以优化比特率和序列长度，实验证明其在高灵活性和低帧率下均能保持优异性能。

Motivation: 传统神经语音编解码器采用恒定帧率（CFR）机制调整比特率，但语音段的信息密度随时间变化（如静默区间与有声区域），这导致比特率和序列长度效率不高，尤其在实时应用中。

Method: 提出了一种名为Temporally Flexible Coding (TFC)的技术，首次在神经语音编解码器中引入可变帧率（VFR），动态分配帧率以适配时间熵变化。

Result: 实验表明，采用TFC的编解码器在保持高灵活性的同时，能实现最优重建质量，且在低帧率下仍具竞争力。

Conclusion: TFC技术为低帧率神经语音编解码器的开发提供了新思路，有望提升下游任务的效率。

Abstract: Most neural speech codecs achieve bitrate adjustment through intra-frame
mechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However,
speech segments inherently have time-varying information density (e.g., silent
intervals versus voiced regions). This property makes CFR not optimal in terms
of bitrate and token sequence length, hindering efficiency in real-time
applications. In this work, we propose a Temporally Flexible Coding (TFC)
technique, introducing variable frame rate (VFR) into neural speech codecs for
the first time. TFC enables seamlessly tunable average frame rates and
dynamically allocates frame rates based on temporal entropy. Experimental
results show that a codec with TFC achieves optimal reconstruction quality with
high flexibility, and maintains competitive performance even at lower frame
rates. Our approach is promising for the integration with other efforts to
develop low-frame-rate neural speech codecs for more efficient downstream
tasks.

</details>


### [330] [Active Speech Enhancement: Active Speech Denoising Decliping and Deveraberation](https://arxiv.org/abs/2505.16911)
*Ofir Yaish,Yehuda Mishaly,Eliya Nachmani*

Key words: 主动语音增强, Transformer-Mamba, 噪声抑制, 信号增强, 语音清晰度

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 论文提出了一种新的主动语音增强（ASE）范式，通过Transformer-Mamba架构和特定任务损失函数同时优化噪声抑制和信号增强，显著提升了语音清晰度和感知质量。

Motivation: 现有的主动噪声消除（ANC）技术仅抑制外部干扰，而未能主动优化语音信号本身。ASE旨在通过同时抑制噪声和增强语音相关频率，进一步提升语音的可懂度和感知质量。

Method: 提出了一种结合Transformer和Mamba的新架构，并设计了一种任务特定的损失函数，联合优化干扰抑制和信号增强。

Result: 该方法在去噪、去混响和去削波等多种语音处理任务中优于现有基线，证明了其在复杂声学环境中的有效性。

Conclusion: ASE通过主动调制语音信号，显著提升了语音处理效果，为未来语音增强技术提供了新方向。

Abstract: We introduce a new paradigm for active sound modification: Active Speech
Enhancement (ASE). While Active Noise Cancellation (ANC) algorithms focus on
suppressing external interference, ASE goes further by actively shaping the
speech signal -- both attenuating unwanted noise components and amplifying
speech-relevant frequencies -- to improve intelligibility and perceptual
quality. To enable this, we propose a novel Transformer-Mamba-based
architecture, along with a task-specific loss function designed to jointly
optimize interference suppression and signal enrichment. Our method outperforms
existing baselines across multiple speech processing tasks -- including
denoising, dereverberation, and declipping -- demonstrating the effectiveness
of active, targeted modulation in challenging acoustic environments.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [331] [Machine Learning the 6d Supergravity Landscape](https://arxiv.org/abs/2505.16131)
*Nathan Brady,David Tennyson,Thomas Vandermeulen*

Key words: 机器学习, 弦景观, 沼泽地, 6维超重力, 自动编码器

<details>
  <summary>Details</summary>

Main category: hep-th

TL;DR: 利用监督学习和无监督学习算法研究6维超重力模型的弦景观和沼泽地，通过自动编码器实现模型分类和异常检测，监督学习构建分类器预测模型一致性。

Motivation: 探索机器学习在弦理论和超重力模型中的应用，提高对弦景观和沼泽地的理解，并寻找高效分析复杂模型的方法。

Method: 结合监督学习（构建分类器预测模型一致性）和无监督学习（自动编码器降维与聚类），分析6维超重力模型的异常系数Gram矩阵。

Result: 自动编码器成功将数据压缩至2维并识别异常模型；监督学习分类器在模型一致性和异常检测中表现优异（精度分别为0.78和0.91）。

Conclusion: 机器学习能高效学习弦景观和沼泽地的复杂特征，自动编码器为映射6维超重力理论提供了新方法。

Abstract: In this paper, we apply both supervised and unsupervised machine learning
algorithms to the study of the string landscape and swampland in 6-dimensions.
Our data are the (almost) anomaly-free 6-dimensional $\mathcal{N} = (1,0)$
supergravity models, characterised by the Gram matrix of anomaly coefficients.
Our work demonstrates the ability of machine learning algorithms to efficiently
learn highly complex features of the landscape and swampland. Employing an
autoencoder for unsupervised learning, we provide an auto-classification of
these models by compressing the Gram matrix data to 2-dimensions. Through
compression, similar models cluster together, and we identify prominent
features of these clusters. The autoencoder also identifies outlier models
which are difficult to reconstruct. One of these outliers proves to be
incredibly difficult to combine with other models such that the
$\text{tr}R^{4}$ anomaly vanishes, making its presence in the landscape
extremely rare. Further, we utilise supervised learning to build two
classifiers predicting (1) model consistency under probe string insertion
(precision: 0.78, predicting consistency for 214,837 models with reasonable
certainty) and (2) inconsistency under anomaly inflow (precision: 0.91,
predicting inconsistency for 1,909,359 models). Notably, projecting these
predictions onto the autoencoder's 2-dimensional latent layer shows consistent
models clustering together, further indicating that the autoencoder has learnt
interesting and complex features of the set of models and potentially offers a
novel approach to mapping the landscape and swampland of 6-dimensional
supergravity theories.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [332] [Data-driven Verification of Procedural Programs with Integer Arrays](https://arxiv.org/abs/2505.15958)
*Ahmed Bouajjani,Wael-Amine Boutglay,Peter Habermehl*

Key words: 自动验证,数组,约束Horn子句,循环不变式,数据驱动

<details>
  <summary>Details</summary>

Main category: cs.PL

TL;DR: 本文提出了一种基于数据驱动的方法，通过扩展决策树Horn-ICE框架来处理数组，以自动生成循环不变式和过程前后条件的通用量化一阶公式。

Motivation: 解决自动验证操纵参数化大小整数数组的过程程序的问题，将其编码为约束Horn子句求解问题。

Method: 采用数据驱动方法，扩展决策树Horn-ICE框架处理数组，通过将整数数组向量的复杂分类问题简化为整数向量的分类问题来合成不变式。

Result: 实现的方法在显著基准测试中表现出高效性和竞争力。

Conclusion: 提出的方法能有效合成通用量化的不变量和过程前后条件，验证效率高。

Abstract: We address the problem of verifying automatically procedural programs
manipulating parametric-size arrays of integers, encoded as a constrained Horn
clauses solving problem. We propose a new algorithmic method for synthesizing
loop invariants and procedure pre/post-conditions represented as universally
quantified first-order formulas constraining the array elements and program
variables. We adopt a data-driven approach that extends the decision tree
Horn-ICE framework to handle arrays. We provide a powerful learning technique
based on reducing a complex classification problem of vectors of integer arrays
to a simpler classification problem of vectors of integers. The obtained
classifier is generalized to get universally quantified invariants and
procedure pre/post-conditions. We have implemented our method and shown its
efficiency and competitiveness w.r.t. state-of-the-art tools on a significant
benchmark.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [333] [Signals of Provenance: Practices & Challenges of Navigating Indicators in AI-Generated Media for Sighted and Blind Individuals](https://arxiv.org/abs/2505.16057)
*Ayae Ide,Tory Park,Jaron Mink,Tanusree Sharma*

Key words: AI生成内容（AIG）、盲人和视障用户（BLV）、内容标识、可访问性、用户研究

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 研究探讨了AI生成内容（AIG）的自我标识问题，发现当前视觉为主的标识方式对盲人和视障（BLV）用户效果不佳，导致他们难以识别AIG内容。通过用户访谈指出了内容标识和菜单辅助标识的优缺点，并提出了改进建议。

Motivation: 随着AI生成内容（AIG）的普及，当前主要依赖视觉的标识方式对盲人和视障（BLV）用户不够友好，导致他们难以识别这类内容。研究旨在探索更有效的标识方式。

Method: 通过半结构化访谈（N=28，包括15名视力正常者和13名BLV用户），分析了用户与AIG标识的互动方式及其有效性问题。

Result: 研究发现视觉和音频标识对视力正常者有效，而BLV用户主要依赖音频和辅助工具，且容易忽略菜单辅助标识。指出了标识位置不一致、元数据模糊和认知过载等问题。

Conclusion: 现有AIG标识对BLV用户不够友好，需改进标识设计的可访问性和一致性。研究提出了多维度的设计建议。

Abstract: AI-Generated (AIG) content has become increasingly widespread by recent
advances in generative models and the easy-to-use tools that have significantly
lowered the technical barriers for producing highly realistic audio, images,
and videos through simple natural language prompts. In response, platforms are
adopting provable provenance with platforms recommending AIG to be
self-disclosed and signaled to users. However, these indicators may be often
missed, especially when they rely solely on visual cues and make them
ineffective to users with different sensory abilities. To address the gap, we
conducted semi-structured interviews (N=28) with 15 sighted and 13 BLV
participants to examine their interaction with AIG content through
self-disclosed AI indicators. Our findings reveal diverse mental models and
practices, highlighting different strengths and weaknesses of content-based
(e.g., title, description) and menu-aided (e.g., AI labels) indicators. While
sighted participants leveraged visual and audio cues, BLV participants
primarily relied on audio and existing assistive tools, limiting their ability
to identify AIG. Across both groups, they frequently overlooked menu-aided
indicators deployed by platforms and rather interacted with content-based
indicators such as title and comments. We uncovered usability challenges
stemming from inconsistent indicator placement, unclear metadata, and cognitive
overload. These issues were especially critical for BLV individuals due to the
insufficient accessibility of interface elements. We provide practical
recommendations and design implications for future AIG indicators across
several dimensions.

</details>


### [334] [Real-Time Stress Monitoring, Detection, and Management in College Students: A Wearable Technology and Machine-Learning Approach](https://arxiv.org/abs/2505.15974)
*Alan Ta,Nilsu Salgin,Mustafa Demir,Kala Philips Randal,Ranjana K. Mehta,Anthony McDonald,Carly McCord,Farzan Sasangohar*

Key words: 移动健康；机器学习；压力检测；心理健康；随机对照试验

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 研究评估了mHELP移动健康干预在大学生压力管理中的效果，发现可穿戴设备和机器学习显著减少了客观压力指标，但对焦虑和抑郁等主观症状改善有限。

Motivation: 大学生面临心理健康问题却难以获得传统治疗，因此开发了mHELP这一结合智能穿戴和机器学习的实时压力管理工具。

Method: 通过12周随机对照试验（n=117），比较了使用完整mHELP功能的治疗组与仅用于压力记录和心理评估的对照组。

Result: 治疗组客观压力指标（MS）显著下降，但主观焦虑（GAD-7）和抑郁（PHQ-8）评分无显著差异；GAD-7和PSS评分有临床意义下降。

Conclusion: 可穿戴mHealth工具能有效减少大学生急性压力，但需进一步优化以改善慢性症状。

Abstract: College students are increasingly affected by stress, anxiety, and
depression, yet face barriers to traditional mental health care. This study
evaluated the efficacy of a mobile health (mHealth) intervention, Mental Health
Evaluation and Lookout Program (mHELP), which integrates a smartwatch sensor
and machine learning (ML) algorithms for real-time stress detection and
self-management. In a 12-week randomized controlled trial (n = 117),
participants were assigned to a treatment group using mHELP's full suite of
interventions or a control group using the app solely for real-time stress
logging and weekly psychological assessments. The primary outcome, "Moments of
Stress" (MS), was assessed via physiological and self-reported indicators and
analyzed using Generalized Linear Mixed Models (GLMM) approaches. Similarly,
secondary outcomes of psychological assessments, including the Generalized
Anxiety Disorder-7 (GAD-7) for anxiety, the Patient Health Questionnaire
(PHQ-8) for depression, and the Perceived Stress Scale (PSS), were also
analyzed via GLMM. The finding of the objective measure, MS, indicates a
substantial decrease in MS among the treatment group compared to the control
group, while no notable between-group differences were observed in subjective
scores of anxiety (GAD-7), depression (PHQ-8), or stress (PSS). However, the
treatment group exhibited a clinically meaningful decline in GAD-7 and PSS
scores. These findings underscore the potential of wearable-enabled mHealth
tools to reduce acute stress in college populations and highlight the need for
extended interventions and tailored features to address chronic symptoms like
depression.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [335] [Learning novel representations of variable sources from multi-modal $\textit{Gaia}$ data via autoencoders](https://arxiv.org/abs/2505.16320)
*P. Huijse,J. De Ridder,L. Eyer,L. Rimoldini,B. Holl,N. Chornay,J. Roquette,K. Nienartowicz,G. Jevardat de Fombelle,D. J. Fritzewski,A. Kemp,V. Vanlaer,M. Vanrespaille,H. Wang,M. I. Carnerero,C. M. Raiteri,G. Marton,M. Madarász,G. Clementini,P. Gavras,C. Aerts*

Key words: Gaia DR3, 变异性分析, 变分自编码器, 无监督分类, 机器学习

<details>
  <summary>Details</summary>

Main category: astro-ph.IM

TL;DR: 论文提出了一种基于机器学习的无监督分类方法，结合Gaia DR3多源数据，利用变分自编码器（VAE）压缩数据并在15维潜在空间中实现恒星和类星体变异性分析。

Motivation: Gaia DR3发布了大量变源数据，为研究其变异性提供了独特机会。论文旨在通过结合多种Gaia数据产品，开发一种无监督分类方法，为DR4做准备。

Method: 使用三个变分自编码器（VAE）分别处理Gaia低分辨率光谱、G波段光变曲线和亮度差异分布，将数据压缩为15维潜在空间表示。

Result: 潜在空间表示能有效区分Gaia DR3中的主要变异性类别，并在监督和无监督分类中表现优异。二维投影揭示了与天体物理属性高度相关的密度分布。

Conclusion: 该方法展示了多种Gaia数据产品的协同作用，为变异性分析任务（如分类、聚类和异常检测）提供了高效工具。

Abstract: Gaia Data Release 3 (DR3) published for the first time epoch photometry,
BP/RP (XP) low-resolution mean spectra, and supervised classification results
for millions of variable sources. This extensive dataset offers a unique
opportunity to study their variability by combining multiple Gaia data
products. In preparation for DR4, we propose and evaluate a machine learning
methodology capable of ingesting multiple Gaia data products to achieve an
unsupervised classification of stellar and quasar variability. A dataset of 4
million Gaia DR3 sources is used to train three variational autoencoders (VAE),
which are artificial neural networks (ANNs) designed for data compression and
generation. One VAE is trained on Gaia XP low-resolution spectra, another on a
novel approach based on the distribution of magnitude differences in the Gaia G
band, and the third on folded Gaia G band light curves. Each Gaia source is
compressed into 15 numbers, representing the coordinates in a 15-dimensional
latent space generated by combining the outputs of these three models. The
learned latent representation produced by the ANN effectively distinguishes
between the main variability classes present in Gaia DR3, as demonstrated
through both supervised and unsupervised classification analysis of the latent
space. The results highlight a strong synergy between light curves and
low-resolution spectral data, emphasising the benefits of combining the
different Gaia data products. A two-dimensional projection of the latent
variables reveals numerous overdensities, most of which strongly correlate with
astrophysical properties, showing the potential of this latent space for
astrophysical discovery. We show that the properties of our novel latent
representation make it highly valuable for variability analysis tasks,
including classification, clustering and outlier detection.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [336] [DisastIR: A Comprehensive Information Retrieval Benchmark for Disaster Management](https://arxiv.org/abs/2505.15856)
*Kai Yin,Xiangjue Dong,Chengkai Liu,Lipai Huang,Yiming Xiao,Zhewei Liu,Ali Mostafavi,James Caverlee*

Key words: 灾害管理, 信息检索, 评估基准, 查询标记对, 模型性能

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了首个针对灾害管理的检索评估基准DisastIR，包含9600个查询和130万标记对，覆盖48个任务。评估显示现有检索模型在灾害管理任务中表现不一，需专用基准指导模型选择。

Motivation: 现有信息检索（IR）基准主要集中在通用或专业领域（如医学或金融），而忽略了灾害管理的独特语言复杂性和多样信息需求，因此需要专用基准支持灾害管理决策。

Method: 开发了DisastIR基准，包含多样化的查询和标记数据，覆盖多种检索任务和灾害类型。并对30种先进检索模型进行了评估。

Result: 评估结果显示不同模型在不同任务中表现差异显著，且通用领域模型在灾害管理任务中存在性能差距。

Conclusion: 灾害管理需专用检索基准以优化模型选择，提升决策效率。

Abstract: Effective disaster management requires timely access to accurate and
contextually relevant information. Existing Information Retrieval (IR)
benchmarks, however, focus primarily on general or specialized domains, such as
medicine or finance, neglecting the unique linguistic complexity and diverse
information needs encountered in disaster management scenarios. To bridge this
gap, we introduce DisastIR, the first comprehensive IR evaluation benchmark
specifically tailored for disaster management. DisastIR comprises 9,600 diverse
user queries and more than 1.3 million labeled query-passage pairs, covering 48
distinct retrieval tasks derived from six search intents and eight general
disaster categories that include 301 specific event types. Our evaluations of
30 state-of-the-art retrieval models demonstrate significant performance
variances across tasks, with no single model excelling universally.
Furthermore, comparative analyses reveal significant performance gaps between
general-domain and disaster management-specific tasks, highlighting the
necessity of disaster management-specific benchmarks for guiding IR model
selection to support effective decision-making in disaster management
scenarios. All source codes and DisastIR are available at
https://github.com/KaiYin97/Disaster_IR.

</details>


### [337] [AutoData: A Multi-Agent System for Open Web Data Collection](https://arxiv.org/abs/2505.15859)
*Tianyi Ma,Yiyue Qian,Zheyuan Zhang,Zehong Wang,Xiaoye Qian,Feifan Bai,Yifan Ding,Xuwei Luo,Shinan Zhang,Keerthiram Murugesan,Chuxu Zhang,Yanfang Ye*

Key words: AutoData, web data collection, multi-agent system, hypergraph cache, LLM cost reduction

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出AutoData，一种多代理系统，用于自动化网络数据收集，减少人工干预，提升效率和降低成本。

Motivation: 现有网络数据收集方法在适应性和成本方面存在局限，AutoData旨在通过多代理系统解决这些问题。

Method: 使用多代理架构和面向消息的超图协调，引入超图缓存系统降低LLM成本，并开发Instruct2DS基准数据集。

Result: 在多个数据集上验证了AutoData的优越性能，特别是在图片书籍收集和论文提取等任务中表现突出。

Conclusion: AutoData有效解决了当前数据收集方法的效率与成本问题，具有广泛的应用潜力。

Abstract: The exponential growth of data-driven systems and AI technologies has
intensified the demand for high-quality web-sourced datasets. While existing
datasets have proven valuable, conventional web data collection approaches face
significant limitations in terms of human effort and scalability. Current
data-collecting solutions fall into two categories: wrapper-based methods that
struggle with adaptability and reproducibility, and large language model
(LLM)-based approaches that incur substantial computational and financial
costs. To address these challenges, we propose AutoData, a novel multi-agent
system for Automated web Data collection, that requires minimal human
intervention, i.e., only necessitating a natural language instruction
specifying the desired dataset. In addition, AutoData is designed with a robust
multi-agent architecture, featuring a novel oriented message hypergraph
coordinated by a central task manager, to efficiently organize agents across
research and development squads. Besides, we introduce a novel hypergraph cache
system to advance the multi-agent collaboration process that enables efficient
automated data collection and mitigates the token cost issues prevalent in
existing LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark
dataset supporting live data collection from web sources across three domains:
academic, finance, and sports. Comprehensive evaluations over Instruct2DS and
three existing benchmark datasets demonstrate AutoData's superior performance
compared to baseline methods. Case studies on challenging tasks such as picture
book collection and paper extraction from surveys further validate its
applicability. Our source code and dataset are available at
https://github.com/GraphResearcher/AutoData.

</details>


### [338] [InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation](https://arxiv.org/abs/2505.15872)
*Yunjia Xi,Jianghao Lin,Menghui Zhu,Yongzhao Xiao,Zhuoying Ou,Jiaqi Liu,Tong Wan,Bo Chen,Weiwen Liu,Yasheng Wang,Ruiming Tang,Weinan Zhang,Yong Yu*

Key words: Retrieval-Augmented Generation, Agentic RAG, benchmarking, dynamic web environments

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了一种新基准InfoDeepSeek，用于评估动态网络环境中自主LLM代理的信息检索能力。

Motivation: 现有基准无法评估动态环境中自主代理的信息检索行为，限制了Agentic RAG的发展。

Method: 提出系统性方法构建具有挑战性的查询，并开发首个动态代理信息检索评估框架。

Result: 实验揭示了代理的细微行为，为未来研究提供了实用见解。

Conclusion: InfoDeepSeek填补了现有测评空白，推动了自主代理信息检索领域的研究。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
grounding responses with retrieved information. As an emerging paradigm,
Agentic RAG further enhances this process by introducing autonomous LLM agents
into the information seeking process. However, existing benchmarks fall short
in evaluating such systems, as they are confined to a static retrieval
environment with a fixed, limited corpus} and simple queries that fail to
elicit agentic behavior. Moreover, their evaluation protocols assess
information seeking effectiveness by pre-defined gold sets of documents, making
them unsuitable for the open-ended and dynamic nature of real-world web
environments. To bridge this gap, we present InfoDeepSeek, a new benchmark with
challenging questions designed for assessing agentic information seeking in
real-world, dynamic web environments. We propose a systematic methodology for
constructing challenging queries satisfying the criteria of determinacy,
difficulty, and diversity. Based on this, we develop the first evaluation
framework tailored to dynamic agentic information seeking, including
fine-grained metrics about the accuracy, utility, and compactness of
information seeking outcomes. Through extensive experiments across LLMs, search
engines, and question types, InfoDeepSeek reveals nuanced agent behaviors and
offers actionable insights for future research.

</details>


### [339] [Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated Synthetic Data Augmentation](https://arxiv.org/abs/2505.16065)
*Ruijie Xi,He Ba,Hao Yuan,Rishu Agrawal,Arul Prakash*

Key words: Embedding-Based Retrieval, Generative AI, synthetic data, Llama models, Aug2Search

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: Aug2Search利用生成式AI生成高质量合成数据，提升Embedding-Based Retrieval模型性能，实验表明仅使用合成数据的模型表现优于原始数据或混合数据。

Motivation: Facebook Marketplace等平台的搜索日志数据缺乏多样性和细节，限制了EBR模型捕捉细微搜索模式的能力。

Method: 提出Aug2Search框架，通过三种策略生成合成数据（生成查询、增强产品列表、从增强列表生成查询），并对比不同数据训练集的EBR模型效果。

Result: 实验显示Llama模型生成的合成数据质量高，Aug2Search在1亿样本上ROC_AUC提升4%，仅使用合成数据的模型表现最佳。

Conclusion: 生成式AI生成的合成数据可有效增强EBR模型性能，为数据稀疏问题提供解决方案。

Abstract: Embedding-Based Retrieval (EBR) is an important technique in modern search
engines, enabling semantic match between search queries and relevant results.
However, search logging data on platforms like Facebook Marketplace lacks the
diversity and details needed for effective EBR model training, limiting the
models' ability to capture nuanced search patterns. To address this challenge,
we propose Aug2Search, an EBR-based framework leveraging synthetic data
generated by Generative AI (GenAI) models, in a multimodal and multitask
approach to optimize query-product relevance. This paper investigates the
capabilities of GenAI, particularly Large Language Models (LLMs), in generating
high-quality synthetic data, and analyzing its impact on enhancing EBR models.
We conducted experiments using eight Llama models and 100 million data points
from Facebook Marketplace logs. Our synthetic data generation follows three
strategies: (1) generate queries, (2) enhance product listings, and (3)
generate queries from enhanced listings. We train EBR models on three different
datasets: sampled engagement data or original data ((e.g., "Click" and "Listing
Interactions")), synthetic data, and a mixture of both engagement and synthetic
data to assess their performance across various training sets. Our findings
underscore the robustness of Llama models in producing synthetic queries and
listings with high coherence, relevance, and diversity, while maintaining low
levels of hallucination. Aug2Search achieves an improvement of up to 4% in
ROC_AUC with 100 million synthetic data samples, demonstrating the
effectiveness of our approach. Moreover, our experiments reveal that with the
same volume of training data, models trained exclusively on synthetic data
often outperform those trained on original data only or a mixture of original
and synthetic data.

</details>


### [340] [Conf-GNNRec: Quantifying and Calibrating the Prediction Confidence for GNN-based Recommendation Methods](https://arxiv.org/abs/2505.16466)
*Meng Yan,Cai Xu,Xujing Wang,Ziyu Guan,Wei Zhao,Yuhang Zhou*

Key words: 推荐系统, 图神经网络, 预测置信度, 噪声, 过自信

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 该论文提出了Conf-GNNRec方法，用于量化和校准基于图神经网络的推荐系统的预测置信度，解决现有方法在噪声环境中的过自信问题。

Motivation: 现有基于图神经网络的推荐系统在噪声环境下（如用户滥用或恶意广告）表现不佳，噪声会通过消息传播机制累积，导致预测结果不可信，且现有方法存在过自信问题。

Method: 提出动态评分校准方法以基于用户个性化调整过高评分，并设计置信度损失函数以减少负样本的过自信，提升推荐性能。

Result: 在公共数据集上的实验验证了Conf-GNNRec在预测置信度和推荐性能上的有效性。

Conclusion: Conf-GNNRec能有效解决噪声环境下的过自信问题，提升推荐系统的可信度和性能。

Abstract: Recommender systems based on graph neural networks perform well in tasks such
as rating and ranking. However, in real-world recommendation scenarios, noise
such as user misuse and malicious advertisement gradually accumulates through
the message propagation mechanism. Even if existing studies mitigate their
effects by reducing the noise propagation weights, the severe sparsity of the
recommender system still leads to the low-weighted noisy neighbors being
mistaken as meaningful information, and the prediction result obtained based on
the polluted nodes is not entirely trustworthy. Therefore, it is crucial to
measure the confidence of the prediction results in this highly noisy
framework. Furthermore, our evaluation of the existing representative GNN-based
recommendation shows that it suffers from overconfidence. Based on the above
considerations, we propose a new method to quantify and calibrate the
prediction confidence of GNN-based recommendations (Conf-GNNRec). Specifically,
we propose a rating calibration method that dynamically adjusts excessive
ratings to mitigate overconfidence based on user personalization. We also
design a confidence loss function to reduce the overconfidence of negative
samples and effectively improve recommendation performance. Experiments on
public datasets demonstrate the validity of Conf-GNNRec in prediction
confidence and recommendation performance.

</details>


### [341] [Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering](https://arxiv.org/abs/2505.16470)
*Kuicai Dong,Yujing Chang,Shijie Huang,Yasheng Wang,Ruiming Tang,Yong Liu*

Key words: DocVQA, 多模态, 基准测试, 证据链, 视觉问答, LLM, VLM

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: MMDocRAG是一个新的DocVQA基准测试，提供4055个专家标注的QA对，支持多页、跨模态证据链，并引入创新指标评估多模态引用选择和视觉元素整合。

Motivation: 当前DocRAG方法以文本为中心，忽略关键视觉信息，且缺乏评估多模态证据选择和整合的基准。

Method: 开发MMDocRAG基准，包含多模态QA对，并设计新指标评估多模态引用选择和视觉整合。进行大规模实验，测试60种VLM/LLM模型和14种检索系统。

Result: 专有LVMs表现优于开源模型，多模态输入对专有模型优势不显著，但对开源模型性能下降明显。细调LLMs通过详细图像描述显著提升效果。

Conclusion: MMDocRAG为开发更鲁棒的多模态DocVQA系统提供了严格测试环境和实用见解。

Abstract: Document Visual Question Answering (DocVQA) faces dual challenges in
processing lengthy multimodal documents (text, images, tables) and performing
cross-modal reasoning. Current document retrieval-augmented generation (DocRAG)
methods remain limited by their text-centric approaches, frequently missing
critical visual information. The field also lacks robust benchmarks for
assessing multimodal evidence selection and integration. We introduce MMDocRAG,
a comprehensive benchmark featuring 4,055 expert-annotated QA pairs with
multi-page, cross-modal evidence chains. Our framework introduces innovative
metrics for evaluating multimodal quote selection and enables answers that
interleave text with relevant visual elements. Through large-scale experiments
with 60 VLM/LLM models and 14 retrieval systems, we identify persistent
challenges in multimodal evidence retrieval, selection, and integration.Key
findings reveal advanced proprietary LVMs show superior performance than
open-sourced alternatives. Also, they show moderate advantages using multimodal
inputs over text-only inputs, while open-source alternatives show significant
performance degradation. Notably, fine-tuned LLMs achieve substantial
improvements when using detailed image descriptions. MMDocRAG establishes a
rigorous testing ground and provides actionable insights for developing more
robust multimodal DocVQA systems. Our benchmark and code are available at
https://mmdocrag.github.io/MMDocRAG/.

</details>


### [342] [MiLQ: Benchmarking IR Models for Bilingual Web Search with Mixed Language Queries](https://arxiv.org/abs/2505.16631)
*Jonghwi Kim,Deokhyung Kang,Seonjeong Hwang,Yunsu Kim,Jungseul Ok,Gary Lee*

Key words: 混合语言查询，信息检索，多语言，代码切换，双语搜索

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了一个混合语言查询测试集MiLQ，分析了多语言IR模型在不同查询类型上的表现，发现混合查询训练数据可能提升模型鲁棒性，且英语混合查询对双语者搜索有优势。

Motivation: 针对双语者在网络搜索中频繁使用的混合语言查询，现有IR研究较少，因此提出了MiLQ测试集以填补这一空白。

Method: 通过引入MiLQ测试集，评估多语言IR模型在原生、英语和混合语言查询上的表现，并分析代码切换训练数据的影响。

Result: 实验表明，多语言IR模型在MiLQ上表现中等且不一致，但混合查询训练数据可能提升模型鲁棒性；英语混合查询在搜索英语文档时效果更好。

Conclusion: 混合语言查询在IR领域有实际意义，未来的IR模型可通过代码切换数据增强鲁棒性，英语混合策略对双语者搜索有益。

Abstract: Despite bilingual speakers frequently using mixed-language queries in web
searches, Information Retrieval (IR) research on them remains scarce. To
address this, we introduce MiLQ,Mixed-Language Query test set, the first public
benchmark of mixed-language queries, confirmed as realistic and highly
preferred. Experiments show that multilingual IR models perform moderately on
MiLQ and inconsistently across native, English, and mixed-language queries,
also suggesting code-switched training data's potential for robust IR models
handling such queries. Meanwhile, intentional English mixing in queries proves
an effective strategy for bilinguals searching English documents, which our
analysis attributes to enhanced token matching compared to native queries.

</details>


### [343] [Don't "Overthink" Passage Reranking: Is Reasoning Truly Necessary?](https://arxiv.org/abs/2505.16886)
*Nour Jedidi,Yung-Sung Chuang,James Glass,Jimmy Lin*

Key words: 信息检索、推理模型、排序、LLMs

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 研究比较了基于推理的排序模型（ReasonRR）与标准非推理排序模型（StandardRR），发现StandardRR表现更优。进一步发现推理过程反而限制了排序准确性。

Motivation: 探讨推理能力在信息检索中的实际效果，验证推理是否真正提升排序准确性。

Method: 对比ReasonRR和StandardRR，并分析推理对ReasonRR的影响（通过禁用推理的ReasonRR-NoReason）。

Result: StandardRR优于ReasonRR，且ReasonRR-NoReason表现更好，原因是推理过程导致评分极化。

Conclusion: 推理过程可能因极化评分而限制排序准确性。

Abstract: With the growing success of reasoning models across complex natural language
tasks, researchers in the Information Retrieval (IR) community have begun
exploring how similar reasoning capabilities can be integrated into passage
rerankers built on Large Language Models (LLMs). These methods typically employ
an LLM to produce an explicit, step-by-step reasoning process before arriving
at a final relevance prediction. But, does reasoning actually improve reranking
accuracy? In this paper, we dive deeper into this question, studying the impact
of the reasoning process by comparing reasoning-based pointwise rerankers
(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under
identical training conditions, and observe that StandardRR generally
outperforms ReasonRR. Building on this observation, we then study the
importance of reasoning to ReasonRR by disabling its reasoning process
(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more
effective than ReasonRR. Examining the cause of this result, our findings
reveal that reasoning-based rerankers are limited by the LLM's reasoning
process, which pushes it toward polarized relevance scores and thus fails to
consider the partial relevance of passages, a key factor for the accuracy of
pointwise rerankers.

</details>


### [344] [Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval](https://arxiv.org/abs/2505.16967)
*Nandan Thakur,Crystina Zhang,Xueguang Ma,Jimmy Lin*

Key words: 数据质量、假阴性、检索模型、重排序、LLM提示

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 通过移除部分低质量数据集并重新标记假阴性样本，显著提升了检索和重排序模型的性能。

Motivation: 研究发现某些训练数据集的质量问题会影响模型效果，特别是“假阴性”样本的错误标注会降低模型表现。

Method: 提出一种使用级联LLM提示来识别和重新标记假阴性样本的简单方法。

Result: 实验显示重新标记假阴性样本后，检索模型在BEIR和AIR-Bench基准上分别提升0.7-1.4和1.7-1.8 nDCG@10，重排序模型同样有显著提升。

Conclusion: 识别并修正假阴性样本能显著提高模型性能，且GPT-4o的标注结果与人类标注更一致。

Abstract: Training robust retrieval and reranker models typically relies on large-scale
retrieval datasets; for example, the BGE collection contains 1.6 million
query-passage pairs sourced from various data sources. However, we find that
certain datasets can negatively impact model effectiveness -- pruning 8 out of
15 datasets from the BGE collection reduces the training set size by
2.35$\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a
deeper examination of training data quality, with a particular focus on "false
negatives", where relevant passages are incorrectly labeled as irrelevant. We
propose a simple, cost-effective approach using cascading LLM prompts to
identify and relabel hard negatives. Experimental results show that relabeling
false negatives with true positives improves both E5 (base) and Qwen2.5-7B
retrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot
AIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on
the relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the
cascading design is further supported by human annotation results, where we
find judgment by GPT-4o shows much higher agreement with humans than
GPT-4o-mini.

</details>


### [345] [$\text{R}^2\text{ec}$: Towards Large Recommender Models with Reasoning](https://arxiv.org/abs/2505.16994)
*Runyang You,Yongqi Li,Xinyu Lin,Xin Zhang,Wenjie Wang,Wenjie Li,Liqiang Nie*

Key words: 推荐系统,大型语言模型,推理能力,强化学习,RecPO

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了一种名为\name的统一大型推荐模型，具备内推理能力，通过重构模型架构和提出RecPO强化学习框架，同时优化推理和推荐能力，实验验证了其有效性。

Motivation: 现有研究将LLMs作为外部推理模块，导致资源成本高且联合优化不足。作者提出统一模型以解决这些问题。

Method: 重构模型架构实现推理与推荐的交错，提出RecPO强化学习框架，采用融合奖励方案优化推理和推荐能力。

Result: 在三个数据集上验证了模型有效性，Hit@5和NDCG@20分别提升68.67%和45.21%。

Conclusion: \name模型通过统一设计和RecPO框架，显著提升了推荐系统的推理和推荐性能。

Abstract: Large recommender models have extended LLMs as powerful recommenders via
encoding or item generation, and recent breakthroughs in LLM reasoning
synchronously motivate the exploration of reasoning in recommendation. Current
studies usually position LLMs as external reasoning modules to yield auxiliary
thought for augmenting conventional recommendation pipelines. However, such
decoupled designs are limited in significant resource cost and suboptimal joint
optimization. To address these issues, we propose \name, a unified large
recommender model with intrinsic reasoning capabilities. Initially, we
reconceptualize the model architecture to facilitate interleaved reasoning and
recommendation in the autoregressive process. Subsequently, we propose RecPO, a
corresponding reinforcement learning framework that optimizes \name\ both the
reasoning and recommendation capabilities simultaneously in a single policy
update; RecPO introduces a fused reward scheme that solely leverages
recommendation labels to simulate the reasoning capability, eliminating
dependency on specialized reasoning annotations. Experiments on three datasets
with various baselines verify the effectiveness of \name, showing relative
improvements of 68.67\% in Hit@5 and 45.21\% in NDCG@20. Code available at
https://github.com/YRYangang/RRec.

</details>


### [346] [Action is All You Need: Dual-Flow Generative Ranking Network for Recommendation](https://arxiv.org/abs/2505.16752)
*Hao Guo,Erpeng Xue,Lei Huang,Shichao Wang,Xiaolei Wang,Lei Wang,Jinpeng Wang,Sheng Chen*

Key words: 推荐系统、双流架构、自注意力机制、生成排序、训练效率

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 介绍了一种名为双流生成排序网络（DFGR）的两流架构，用于推荐系统。其创新性地在自注意力机制的QKV模块中整合了真实和虚假流的交互模式，提升了训练和推理效率，解决了Meta提出的HSTU方法中因信息量不均而导致的训练不稳定性问题。DFGR仅依赖用户历史行为序列和少量属性信息，无需大量手工特征工程，展现了超越现有基准模型的性能。

Motivation: 解决现有生成推荐方法如HSTU在异质信息量映射到相同向量空间时导致的训练不稳定性问题，同时减少对手工特征工程的依赖。

Method: 通过双流架构（Dual-Flow Generative Ranking Network, DFGR）整合真实和虚假流在自注意力机制QKV模块中的交互模式，优化训练和推理效率。

Result: 在开源和工业数据集上的综合评估表明，DFGR性能优于DIN、DCN、DIEN和DeepFM等基准模型，并能有效管理计算资源下的参数分配。

Conclusion: DFGR作为一种高效且有效的新一代生成排序范式，解决了现有方法的局限性，展现了卓越的推荐性能。

Abstract: We introduce the Dual-Flow Generative Ranking Network (DFGR), a two-stream
architecture designed for recommendation systems. DFGR integrates innovative
interaction patterns between real and fake flows within the QKV modules of the
self-attention mechanism, enhancing both training and inference efficiency.
This approach effectively addresses a key limitation observed in Meta's
proposed HSTU generative recommendation approach, where heterogeneous
information volumes are mapped into identical vector spaces, leading to
training instability. Unlike traditional recommendation models, DFGR only
relies on user history behavior sequences and minimal attribute information,
eliminating the need for extensive manual feature engineering. Comprehensive
evaluations on open-source and industrial datasets reveal DFGR's superior
performance compared to established baselines such as DIN, DCN, DIEN, and
DeepFM. We also investigate optimal parameter allocation strategies under
computational constraints, establishing DFGR as an efficient and effective
next-generation generate ranking paradigm.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [347] [Using Echo-State Networks to Reproduce Rare Events in Chaotic Systems](https://arxiv.org/abs/2505.16208)
*Anton Erofeev,Balasubramanya T. Nadiga,Ilya Timofeyev*

Key words: 回波状态网络, Lotka-Volterra模型, 混沌吸引子, 广义极值分布, 极端事件

<details>
  <summary>Details</summary>

Main category: nlin.CD

TL;DR: 使用回波状态网络预测混沌竞争Lotka-Volterra模型的时间序列和统计特性，成功学习其混沌吸引子并复现依赖变量的直方图及尾部行为。

Motivation: 研究旨在验证回波状态网络在预测复杂动力学系统中的表现，尤其是在混沌竞争Lotka-Volterra模型中捕捉极端事件和尾部行为的能力。

Method: 采用回波状态网络对混沌竞争Lotka-Volterra模型进行训练和预测，通过广义极值分布量化尾部行为。

Result: 实验证明，回波状态网络能成功学习混沌吸引子，并准确复现依赖变量的直方图及极端事件分布。

Conclusion: 回波状态网络在混沌系统的极端事件预测中表现出色，为复杂动力系统的统计分析提供了有效工具。

Abstract: We apply the Echo-State Networks to predict the time series and statistical
properties of the competitive Lotka-Volterra model in the chaotic regime. In
particular, we demonstrate that Echo-State Networks successfully learn the
chaotic attractor of the competitive Lotka-Volterra model and reproduce
histograms of dependent variables, including tails and rare events. We use the
Generalized Extreme Value distribution to quantify the tail behavior.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [348] [From Hand-Crafted Metrics to Evolved Training-Free Performance Predictors for Neural Architecture Search via Genetic Programming](https://arxiv.org/abs/2505.15832)
*Quan Minh Phan,Ngoc Hoang Luong*

Key words: 零成本指标, 神经架构搜索, 符号回归, 遗传编程, 自动化设计

<details>
  <summary>Details</summary>

Main category: cs.NE

TL;DR: 该论文提出了一种基于符号回归的自动化框架，用于设计零成本（ZC）指标以评估网络性能，解决了现有手工设计ZC指标的不一致性和耗时问题，并在多个任务中表现优于手工设计的指标。

Motivation: 目前零成本（ZC）指标在神经架构搜索（NAS）中虽有效，但存在不一致性和手工设计耗时的问题。论文旨在自动化设计更通用的ZC指标。

Method: 采用符号回归和遗传编程框架，自动化生成ZC指标，确保指标在多样化的NAS任务中具有高排名相关性。

Result: 在13个NAS任务上的实验表明，自动化生成的ZC指标始终优于手工设计指标，且能快速找到高性能网络架构。

Conclusion: 自动化设计的ZC指标更具通用性和效率，显著提升NAS的搜索性能。

Abstract: Estimating the network performance using zero-cost (ZC) metrics has proven
both its efficiency and efficacy in Neural Architecture Search (NAS). However,
a notable limitation of most ZC proxies is their inconsistency, as reflected by
the substantial variation in their performance across different problems.
Furthermore, the design of existing ZC metrics is manual, involving a
time-consuming trial-and-error process that requires substantial domain
expertise. These challenges raise two critical questions: (1) Can we automate
the design of ZC metrics? and (2) Can we utilize the existing hand-crafted ZC
metrics to synthesize a more generalizable one? In this study, we propose a
framework based on Symbolic Regression via Genetic Programming to automate the
design of ZC metrics. Our framework is not only highly extensible but also
capable of quickly producing a ZC metric with a strong positive rank
correlation to true network performance across diverse NAS search spaces and
tasks. Extensive experiments on 13 problems from NAS-Bench-Suite-Zero
demonstrate that our automatically generated proxies consistently outperform
hand-crafted alternatives. Using our evolved proxy metric as the search
objective in an evolutionary algorithm, we could identify network architectures
with competitive performance within 15 minutes using a single consumer GPU.

</details>


### [349] [Quantum-Evolutionary Neural Networks for Multi-Agent Federated Learning](https://arxiv.org/abs/2505.15836)
*Aarav Lala,Kalyan Cherukuri*

Key words: 量子启发神经网络, 进化算法, 多智能体系统, 联邦学习, 隐私保护

<details>
  <summary>Details</summary>

Main category: cs.NE

TL;DR: 介绍了一种结合量子启发神经网络和进化算法的新框架QE-NN，用于优化多智能体系统中的实时决策，强调隐私保护和适应性。

Motivation: 随着AI在复杂、去中心化环境中的应用增加，急需可扩展、自适应且保护隐私的决策系统。

Method: 提出量子进化神经网络（QE-NN），利用量子叠加和纠缠加速学习，结合进化算法优化行为，并通过联邦学习保护隐私。

Result: QE-NN在动态环境中提升了智能体的决策速度和准确性，适用于自动驾驶、智慧城市和医疗等领域。

Conclusion: 该研究将量子计算、进化优化和隐私保护技术结合，为多智能体决策系统开辟了新方向。

Abstract: As artificial intelligence continues to drive innovation in complex,
decentralized environments, the need for scalable, adaptive, and
privacy-preserving decision-making systems has become critical. This paper
introduces a novel framework combining quantum-inspired neural networks with
evolutionary algorithms to optimize real-time decision-making in multi-agent
systems (MAS). The proposed Quantum-Evolutionary Neural Network (QE-NN)
leverages quantum computing principles -- such as quantum superposition and
entanglement -- to enhance learning speed and decision accuracy, while
integrating evolutionary optimization to continually refine agent behaviors in
dynamic, uncertain environments. By utilizing federated learning, QE-NN ensures
privacy preservation, enabling decentralized agents to collaborate without
sharing sensitive data. The framework is designed to allow agents to adapt in
real-time to their environments, optimizing decision-making processes for
applications in areas such as autonomous systems, smart cities, and healthcare.
This research represents a breakthrough in merging quantum computing,
evolutionary optimization, and privacy-preserving techniques to solve complex
problems in multi-agent decision-making systems, pushing the boundaries of AI
in real-world, privacy-sensitive applications.

</details>


### [350] [TDFormer: A Top-Down Attention-Controlled Spiking Transformer](https://arxiv.org/abs/2505.15840)
*Zizheng Zhu,Yingchao Yu,Zeqi Zheng,Zhaofei Yu,Yaochu Jin*

Key words: SNN, TDFormer, 自上而下反馈, 时间信息, 梯度消失

<details>
  <summary>Details</summary>

Main category: cs.NE

TL;DR: TDFormer是一种新型SNN模型，通过引入自上而下的反馈结构，显著提升了时间信息的传递和梯度消失问题的缓解，在ImageNet上达到86.83%的准确率。

Motivation: 传统SNN的膜电位作为唯一的时间信息传递方式效率低下，无法充分利用时间信息，限制了模型性能。

Method: 提出TDFormer模型，引入层级化的自上而下反馈结构，利用高阶表征调节低阶信息处理。

Result: 模型在多个数据集上表现显著提升，ImageNet准确率达86.83%。

Conclusion: TDFormer通过反馈结构有效提升时间信息传递和梯度传播，显著提高模型性能。

Abstract: Traditional spiking neural networks (SNNs) can be viewed as a combination of
multiple subnetworks with each running for one time step, where the parameters
are shared, and the membrane potential serves as the only information link
between them. However, the implicit nature of the membrane potential limits its
ability to effectively represent temporal information. As a result, each time
step cannot fully leverage information from previous time steps, seriously
limiting the model's performance. Inspired by the top-down mechanism in the
brain, we introduce TDFormer, a novel model with a top-down feedback structure
that functions hierarchically and leverages high-order representations from
earlier time steps to modulate the processing of low-order information at later
stages. The feedback structure plays a role from two perspectives: 1) During
forward propagation, our model increases the mutual information across time
steps, indicating that richer temporal information is being transmitted and
integrated in different time steps. 2) During backward propagation, we
theoretically prove that the feedback structure alleviates the problem of
vanishing gradients along the time dimension. We find that these mechanisms
together significantly and consistently improve the model performance on
multiple datasets. In particular, our model achieves state-of-the-art
performance on ImageNet with an accuracy of 86.83%.

</details>


### [351] [Adversarially Robust Spiking Neural Networks with Sparse Connectivity](https://arxiv.org/abs/2505.15833)
*Mathias Schmolli,Maximilian Baronig,Robert Legenstein,Ozan Özdenizci*

Key words: 深度神经网络, 脉冲神经网络, 对抗性鲁棒性, 内存效率, 能耗效率

<details>
  <summary>Details</summary>

Main category: cs.NE

TL;DR: 该论文提出了一种将预训练的人工神经网络转换为稀疏且对抗性鲁棒的脉冲神经网络的方法，实现了内存和能耗的高效性。

Motivation: 在资源受限的嵌入式系统中部署深度神经网络需要高效的能量和内存解决方案，同时确保对抗恶意攻击的鲁棒性。

Method: 通过利用预训练人工神经网络的稀疏连接和权重，设计了一种转换算法生成稀疏且对抗性鲁棒的脉冲神经网络。

Result: 模型减少了高达100倍的内存存储权重，能耗效率提高8.6倍，同时保持高性能和对抗性鲁棒性。

Conclusion: 该方法成功结合了脉冲神经网络的高效架构和对抗性鲁棒性，为嵌入式系统提供了高能效的解决方案。

Abstract: Deployment of deep neural networks in resource-constrained embedded systems
requires innovative algorithmic solutions to facilitate their energy and memory
efficiency. To further ensure the reliability of these systems against
malicious actors, recent works have extensively studied adversarial robustness
of existing architectures. Our work focuses on the intersection of adversarial
robustness, memory- and energy-efficiency in neural networks. We introduce a
neural network conversion algorithm designed to produce sparse and
adversarially robust spiking neural networks (SNNs) by leveraging the sparse
connectivity and weights from a robustly pretrained artificial neural network
(ANN). Our approach combines the energy-efficient architecture of SNNs with a
novel conversion algorithm, leading to state-of-the-art performance with
enhanced energy and memory efficiency through sparse connectivity and
activations. Our models are shown to achieve up to 100x reduction in the number
of weights to be stored in memory, with an estimated 8.6x increase in energy
efficiency compared to dense SNNs, while maintaining high performance and
robustness against adversarial threats.

</details>


### [352] [Curriculum Learning in Genetic Programming Guided Local Search for Large-scale Vehicle Routing Problems](https://arxiv.org/abs/2505.15839)
*Saining Liu,Yi Mei,Mengjie Zhang*

Key words: 车辆路径问题,遗传编程,课程学习,元启发式

<details>
  <summary>Details</summary>

Main category: cs.NE

TL;DR: CL-GPGLS方法通过将课程学习嵌入到遗传编程中，改进了训练实例的选择策略，从而在车辆路径问题上取得了显著的性能提升。

Motivation: 传统方法依赖于随机选择训练实例，这可能导致学习效率低下。本文旨在通过课程学习（CL）的渐进式学习方法优化训练过程。

Method: 提出CL-GPGLS方法，结合遗传编程（GP）与课程学习（CL），逐步引入训练实例，从简单到复杂优化学习过程。

Result: 实验表明，CL-GPGLS在大型车辆路径问题（LSVRP）上的性能显著优于三种基线方法。

Conclusion: CL-GPGLS通过渐进式学习策略有效提升了遗传编程在车辆路径问题上的表现。

Abstract: Manually designing (meta-)heuristics for the Vehicle Routing Problem (VRP) is
a challenging task that requires significant domain expertise. Recently,
data-driven approaches have emerged as a promising solution, automatically
learning heuristics that perform well on training instances and generalize to
unseen test cases. Such an approach learns (meta-)heuristics that can perform
well on the training instances, expecting it to generalize well on the unseen
test instances. A recent method, named GPGLS, uses Genetic Programming (GP) to
learn the utility function in Guided Local Search (GLS) and solved large scale
VRP effectively. However, the selection of appropriate training instances
during the learning process remains an open question, with most existing
studies including GPGLS relying on random instance selection. To address this,
we propose a novel method, CL-GPGLS, which integrates Curriculum Learning (CL)
into GPGLS. Our approach leverages a predefined curriculum to introduce
training instances progressively, starting with simpler tasks and gradually
increasing complexity, enabling the model to better adapt and optimize for
large-scale VRP (LSVRP). Extensive experiments verify the effectiveness of
CL-GPGLS, demonstrating significant performance improvements over three
baseline methods.

</details>


### [353] [Neuromorphic-based metaheuristics: A new generation of low power, low latency and small footprint optimization algorithms](https://arxiv.org/abs/2505.16362)
*El-ghazali Talbi*

Key words: 神经形态计算、元启发式算法、优化问题、低功耗、冯·诺依曼架构

<details>
  <summary>Details</summary>

Main category: cs.NE

TL;DR: 该论文探讨了神经形态计算（NC）在优化算法和元启发式算法中的应用，提出了神经形态启发式（Nheuristics）概念，强调其低功耗、低延迟和小尺寸的特性，并分析了设计挑战及未来发展方向。

Motivation: 研究动机在于探索神经形态计算作为传统冯·诺依曼架构的替代方案，以突破优化问题的解决瓶颈，尤其是在低功耗和高效率方面。

Method: 通过分类和批判性分析，对不同元启发式算法及其解决的优化问题进行了框架性研究，并讨论了神经形态启发式的设计与实现挑战。

Result: 文章提出了神经形态启发式算法的潜力，并指出其在低功耗和小尺寸方面的优势，同时总结了当前面临的挑战。

Conclusion: 神经形态计算为优化算法提供了新方向，但需解决设计与实现的技术难题，未来应进一步拓展其应用范围。

Abstract: Neuromorphic computing (NC) introduces a novel algorithmic paradigm
representing a major shift from traditional digital computing of Von Neumann
architectures. NC emulates or simulates the neural dynamics of brains in the
form of Spiking Neural Networks (SNNs). Much of the research in NC has
concentrated on machine learning applications and neuroscience simulations.
This paper investigates the modelling and implementation of optimization
algorithms and particularly metaheuristics using the NC paradigm as an
alternative to Von Neumann architectures, leading to breakthroughs in solving
optimization problems.
  Neuromorphic-based metaheuristics (Nheuristics) are supposed to be
characterized by low power, low latency and small footprint. Since NC systems
are fundamentally different from conventional Von Neumann computers, several
challenges are posed to the design and implementation of Nheuristics. A
guideline based on a classification and critical analysis is conducted on the
different families of metaheuristics and optimization problems they address. We
also discuss future directions that need to be addressed to expand both the
development and application of Nheuristics.

</details>


### [354] [Graph-Supported Dynamic Algorithm Configuration for Multi-Objective Combinatorial Optimization](https://arxiv.org/abs/2505.16471)
*Robbert Reijnen,Yaoxin Wu,Zaharah Bukhsh,Yingqian Zhang*

Key words: 深度强化学习, 图神经网络, 多目标组合优化, 动态算法配置

<details>
  <summary>Details</summary>

Main category: cs.NE

TL;DR: 该论文提出了一种基于图神经网络（GNN）的深度强化学习方法，用于多目标组合优化问题的动态算法配置，并在实验中表现优异。

Motivation: 深度强化学习在动态算法配置中已有广泛应用，但在多目标组合优化问题中的研究较少。

Method: 将动态算法配置建模为马尔可夫决策过程，利用GNN学习解在目标空间中的收敛图的嵌入表示。

Result: 实验表明，该方法在效能和适应性上优于传统和基于DRL的配置方法，且具有跨目标类型和问题规模的泛化能力。

Conclusion: 该方法在多目标组合优化问题中表现出高效性和通用性，适用于多种进化计算方法。

Abstract: Deep reinforcement learning (DRL) has been widely used for dynamic algorithm
configuration, particularly in evolutionary computation, which benefits from
the adaptive update of parameters during the algorithmic execution. However,
applying DRL to algorithm configuration for multi-objective combinatorial
optimization (MOCO) problems remains relatively unexplored. This paper presents
a novel graph neural network (GNN) based DRL to configure multi-objective
evolutionary algorithms. We model the dynamic algorithm configuration as a
Markov decision process, representing the convergence of solutions in the
objective space by a graph, with their embeddings learned by a GNN to enhance
the state representation. Experiments on diverse MOCO challenges indicate that
our method outperforms traditional and DRL-based algorithm configuration
methods in terms of efficacy and adaptability. It also exhibits advantageous
generalizability across objective types and problem sizes, and applicability to
different evolutionary computation methods.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [355] [From Local Patterns to Global Understanding: Cross-Stock Trend Integration for Enhanced Predictive Modeling](https://arxiv.org/abs/2505.16573)
*Yi Hu,Hanchi Ren,Jingjing Deng,Xianghua Xie*

Key words: 股价预测、联邦学习、跨股票趋势集成、机器学习

<details>
  <summary>Details</summary>

Main category: cs.CE

TL;DR: 提出了一种基于联邦学习的跨股票趋势集成方法（CSTI），通过整合多股票模式提升股价预测性能，优于传统单股票学习方法。

Motivation: 传统单股票预测模型无法利用股票间的相关性，限制了预测性能的提升。

Method: 结合联邦学习框架，训练单股票模型并迭代整合为全局模型，再针对单股票微调保留局部相关性。

Result: 实验表明该方法优于基准模型，提升了预测能力。

Conclusion: CSTI为股价预测提供了更高效的替代方案。

Abstract: Stock price prediction is a critical area of financial forecasting,
traditionally approached by training models using the historical price data of
individual stocks. While these models effectively capture single-stock
patterns, they fail to leverage potential correlations among stock trends,
which could improve predictive performance. Current single-stock learning
methods are thus limited in their ability to provide a broader understanding of
price dynamics across multiple stocks. To address this, we propose a novel
method that merges local patterns into a global understanding through
cross-stock pattern integration. Our strategy is inspired by Federated Learning
(FL), a paradigm designed for decentralized model training. FL enables
collaborative learning across distributed datasets without sharing raw data,
facilitating the aggregation of global insights while preserving data privacy.
In our adaptation, we train models on individual stock data and iteratively
merge them to create a unified global model. This global model is subsequently
fine-tuned on specific stock data to retain local relevance. The proposed
strategy enables parallel training of individual stock models, facilitating
efficient utilization of computational resources and reducing overall training
time. We conducted extensive experiments to evaluate the proposed method,
demonstrating that it outperforms benchmark models and enhances the predictive
capabilities of state-of-the-art approaches. Our results highlight the efficacy
of Cross-Stock Trend Integration (CSTI) in advancing stock price prediction,
offering a robust alternative to traditional single-stock learning
methodologies.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [356] [Exploring Moral Exercises for Human Oversight of AI systems: Insights from Three Pilot Studies](https://arxiv.org/abs/2505.15851)
*Silvia Crafa,Teresa Scantamburlo*

Key words: 道德练习,AI伦理,人类监督,美德培养,负责任AI

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 本文探讨了通过道德练习帮助AI从业者培养美德以实现有效人类监督的框架，结合哲学、古代实践和现代AI伦理，提出了其核心支柱，并通过三项试点研究验证其潜力和局限性。

Motivation: 旨在通过道德练习培养AI从业者的美德，以加强对AI系统的有效人类监督。

Method: 结合哲学、古代实践和AI伦理研究，提出道德练习的核心支柱，并通过三项试点研究验证。

Result: 研究发现道德练习有助于在组织内培养负责任的AI文化，但也揭示了其局限性。

Conclusion: 道德练习对培养负责任的AI文化有潜力，未来研究需进一步探索其应用。

Abstract: This paper elaborates on the concept of moral exercises as a means to help AI
actors cultivate virtues that enable effective human oversight of AI systems.
We explore the conceptual framework and significance of moral exercises,
situating them within the contexts of philosophical discourse, ancient
practices, and contemporary AI ethics scholarship. We outline the core pillars
of the moral exercises methodology - eliciting an engaged personal disposition,
fostering relational understanding, and cultivating technomoral wisdom - and
emphasize their relevance to key activities and competencies essential for
human oversight of AI systems. Our argument is supported by findings from three
pilot studies involving a company, a multidisciplinary team of AI researchers,
and higher education students. These studies allow us to explore both the
potential and the limitations of moral exercises. Based on the collected data,
we offer insights into how moral exercises can foster a responsible AI culture
within organizations, and suggest directions for future research.

</details>


### [357] [NY Real Estate Racial Equity Analysis via Applied Machine Learning](https://arxiv.org/abs/2505.16946)
*Sanjana Chalavadi,Andrei Pastor,Terry Leitch*

Key words: 房地产所有权,种族不平等,LSTM,纽约

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 该研究通过分析纽约州和纽约市的房地产所有权模式，揭示了种族不平等现象。

Motivation: 旨在揭示房地产所有权中的种族差异，反映历史和社会经济不平等。

Method: 使用LSTM+Geo与XGBoost过滤的种族/族裔推算模型（准确率89.2%），对比预测的业主种族构成与人口普查数据。

Result: 结果显示白人持有不成比例的房产和价值，而黑人和西班牙裔等少数群体在房产所有权中代表性不足，尤其在少数族裔为主的社区。

Conclusion: 研究强调了房地产所有权中的持续种族不平等，并指出数据驱动方法的重要性。

Abstract: This study analyzes tract-level real estate ownership patterns in New York
State (NYS) and New York City (NYC) to uncover racial disparities. We use an
advanced race/ethnicity imputation model (LSTM+Geo with XGBoost filtering,
validated at 89.2% accuracy) to compare the predicted racial composition of
property owners to the resident population from census data. We examine both a
Full Model (statewide) and a Name-Only LSTM Model (NYC) to assess how
incorporating geospatial context affects our predictions and disparity
estimates. The results reveal significant inequities: White individuals hold a
disproportionate share of properties and property value relative to their
population, while Black, Hispanic, and Asian communities are underrepresented
as property owners. These disparities are most pronounced in minority-majority
neighborhoods, where ownership is predominantly White despite a predominantly
non-White population. Corporate ownership (LLCs, trusts, etc.) exacerbates
these gaps by reducing owner-occupied opportunities in urban minority
communities. We provide a breakdown of ownership vs. population by race for
majority-White, -Black, -Hispanic, and -Asian tracts, identify those with
extreme ownership disparities, and compare patterns in urban, suburban, and
rural contexts. The findings underscore persistent racial inequity in property
ownership, reflecting broader historical and socio-economic forces, and
highlight the importance of data-driven approaches to address these issues.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [358] [Artificial Intelligence for Direct Prediction of Molecular Dynamics Across Chemical Space](https://arxiv.org/abs/2505.16301)
*Fuchun Ge,Pavlo O. Dral*

Key words: 分子动力学, AI 模型, 等变神经网络, Transformer, 轨迹生成

<details>
  <summary>Details</summary>

Main category: physics.chem-ph

TL;DR: MDtrajNet-1 是一种基于 AI 的模型，通过绕过力计算和数值积分，直接生成分子动力学轨迹，将模拟速度提升两个数量级，同时保持与传统 ab initio MD 相近的误差水平。

Motivation: 传统分子动力学（MD）模拟依赖顺序数值积分，效率受限。MDtrajNet-1 旨在通过 AI 直接生成轨迹，突破这一效率瓶颈。

Method: MDtrajNet-1 结合了等变神经网络和 Transformer 架构，直接预测分子动力学轨迹，无需力计算或积分步骤。

Result: 模型在已知和未知系统中均表现出高精度和可迁移性，速度比传统 MD 快两个数量级，误差接近 ab initio MD。

Conclusion: MDtrajNet-1 突破了传统 MD 的速度限制，为高效扩展的原子尺度模拟开辟了新方向。

Abstract: Molecular dynamics (MD) is a powerful tool for exploring the behavior of
atomistic systems, but its reliance on sequential numerical integration limits
simulation efficiency. We present MDtrajNet-1, a foundational AI model that
directly generates MD trajectories across chemical space, bypassing force
calculations and integration. This approach accelerates simulations by up to
two orders of magnitude compared to traditional MD, even those enhanced by
machine-learning interatomic potentials. MDtrajNet-1 combines equivariant
neural networks with a Transformer-based architecture to achieve strong
accuracy and transferability in predicting long-time trajectories for both
known and unseen systems. Remarkably, the errors of the trajectories generated
by MDtrajNet-1 for various molecular systems are close to those of the
conventional ab initio MD. The model's flexible design supports diverse
application scenarios, including different statistical ensembles, boundary
conditions, and interaction types. By overcoming the intrinsic speed barrier of
conventional MD, MDtrajNet-1 opens new frontiers in efficient and scalable
atomistic simulations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [359] [Multilinear subspace learning for person re-identification based fusion of high order tensor features](https://arxiv.org/abs/2505.15825)
*Ammar Chouchane,Mohcene Bessaoudi,Hamza Kheddar,Abdelmalik Ouamane,Tiago Vieira,Mahmoud Hassaballah*

Key words: 人体再识别, 特征融合, 张量学习, CNN, LOMO

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种高效的人体再识别方法，通过融合多维度特征并引入张量交叉视图二次分析，显著提升了识别性能。

Motivation: 解决人体再识别任务中的特征提取和表示难题，提升在复杂视频监控场景下的识别准确性。

Method: 结合CNN和LOMO特征，采用高维特征融合（HDFF）方法及张量交叉视图二次分析（TXQDA）进行多线性子空间学习。

Result: 在VIPeR、GRID和PRID450S数据集上实验表明，该方法优于当前最优技术。

Conclusion: HDFF与TXQDA的融合有效提高了人体再识别的准确性和鲁棒性，为相关领域提供了新思路。

Abstract: Video surveillance image analysis and processing is a challenging field in
computer vision, with one of its most difficult tasks being Person
Re-Identification (PRe-ID). PRe-ID aims to identify and track target
individuals who have already been detected in a network of cameras, using a
robust description of their pedestrian images. The success of recent research
in person PRe-ID is largely due to effective feature extraction and
representation, as well as the powerful learning of these features to reliably
discriminate between pedestrian images. To this end, two powerful features,
Convolutional Neural Networks (CNN) and Local Maximal Occurrence (LOMO), are
modeled on multidimensional data using the proposed method, High-Dimensional
Feature Fusion (HDFF). Specifically, a new tensor fusion scheme is introduced
to leverage and combine these two types of features in a single tensor, even
though their dimensions are not identical. To enhance the system's accuracy, we
employ Tensor Cross-View Quadratic Analysis (TXQDA) for multilinear subspace
learning, followed by cosine similarity for matching. TXQDA efficiently
facilitates learning while reducing the high dimensionality inherent in
high-order tensor data. The effectiveness of our approach is verified through
experiments on three widely-used PRe-ID datasets: VIPeR, GRID, and PRID450S.
Extensive experiments demonstrate that our approach outperforms recent
state-of-the-art methods.

</details>


### [360] [GRIT: Teaching MLLMs to Think with Images](https://arxiv.org/abs/2505.15879)
*Yue Fan,Xuehai He,Diji Yang,Kaizhi Zheng,Ching-Chen Kuo,Yuting Zheng,Sravana Jyothi Narayanaraju,Xinze Guan,Xin Eric Wang*

Key words: Reinforcement Learning, Visual Reasoning, Multimodal Models, GRIT, GRPO-GR

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: GRIT is a novel method combining reinforcement learning and visual grounding to train multimodal large language models for generating visually-grounded reasoning chains with high data efficiency.

Motivation: Existing visual reasoning models lack explicit integration of visual information, limiting their ability to produce clearly articulated and visually grounded reasoning chains.

Method: Grounded Reasoning with Images and Texts (GRIT) introduces a grounded reasoning paradigm, interleaving natural language and bounding box coordinates, and uses reinforcement learning (GRPO-GR) with robust rewards for final answer accuracy and output format.

Result: GRIT achieves exceptional data efficiency, requiring only 20 image-question-answer triplets, and successfully trains models to produce coherent and visually grounded reasoning chains.

Conclusion: GRIT effectively unifies reasoning and grounding abilities, demonstrating a significant advance in multimodal reasoning.

Abstract: Recent studies have demonstrated the efficacy of using Reinforcement Learning
(RL) in building reasoning models that articulate chains of thoughts prior to
producing final answers. However, despite ongoing advances that aim at enabling
reasoning for vision-language tasks, existing open-source visual reasoning
models typically generate reasoning content with pure natural language, lacking
explicit integration of visual information. This limits their ability to
produce clearly articulated and visually grounded reasoning chains. To this
end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method
for training MLLMs to think with images. GRIT introduces a grounded reasoning
paradigm, in which models generate reasoning chains that interleave natural
language and explicit bounding box coordinates. These coordinates point to
regions of the input image that the model consults during its reasoning
process. Additionally, GRIT is equipped with a reinforcement learning approach,
GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused
on the final answer accuracy and format of the grounded reasoning output, which
eliminates the need for data with reasoning chain annotations or explicit
bounding box labels. As a result, GRIT achieves exceptional data efficiency,
requiring as few as 20 image-question-answer triplets from existing datasets.
Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to
produce coherent and visually grounded reasoning chains, showing a successful
unification of reasoning and grounding abilities.

</details>


### [361] [VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance](https://arxiv.org/abs/2505.15952)
*Mohammad Reza Taesiri,Abhijay Ghildyal,Saman Zadtootaghaj,Nabajeet Barman,Cor-Paul Bezemer*

Key words: 视频游戏,质量保证,视语言模型,基准测试,自动化

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文介绍了VideoGameQA-Bench，一个用于视频游戏质量保证（QA）的综合基准测试，填补了现有技术在游戏开发中自动化QA任务的评估空白。

Motivation: 随着游戏行业收入的增长，优化开发流程尤为重要，尤其是在自动化质量保证（QA）方面。目前的视语言模型（VLMs）虽具潜力但缺乏针对游戏QA的标准评测方法。

Method: 提出VideoGameQA-Bench基准测试，涵盖视觉单元测试、回归测试、异常检测和缺陷报告生成等游戏QA任务。

Result: 基准测试提供了标准化数据集和代码，支持对VLMs在游戏QA领域的有效性评估。

Conclusion: VideoGameQA-Bench填补了游戏QA自动化评测的空白，为行业提供了实用的工具。

Abstract: With video games now generating the highest revenues in the entertainment
industry, optimizing game development workflows has become essential for the
sector's sustained growth. Recent advancements in Vision-Language Models (VLMs)
offer considerable potential to automate and enhance various aspects of game
development, particularly Quality Assurance (QA), which remains one of the
industry's most labor-intensive processes with limited automation options. To
accurately evaluate the performance of VLMs in video game QA tasks and
determine their effectiveness in handling real-world scenarios, there is a
clear need for standardized benchmarks, as existing benchmarks are insufficient
to address the specific requirements of this domain. To bridge this gap, we
introduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array
of game QA activities, including visual unit testing, visual regression
testing, needle-in-a-haystack tasks, glitch detection, and bug report
generation for both images and videos of various games. Code and data are
available at: https://asgaardlab.github.io/videogameqa-bench/

</details>


### [362] [Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning](https://arxiv.org/abs/2505.15966)
*Alex Su,Haozhe Wang,Weimin Ren,Fangzhen Lin,Wenhu Chen*

Key words: 视觉语言模型,像素空间推理,视觉操作,强化学习,视觉任务

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了像素空间推理的新框架，通过视觉操作提升视觉语言模型的推理能力，并在多个视觉推理任务中取得了最优性能。

Motivation: 传统的大语言模型推理局限于文本空间，无法有效处理视觉密集型任务，因此需要扩展推理能力至像素空间。

Method: 采用两阶段训练方法，包括指令调优和强化学习，引入视觉操作如放大和选择帧，以提升模型的视觉推理能力。

Result: 提出的7B模型在多个视觉推理基准测试中表现优异，达到了开源模型中的最高准确率。

Conclusion: 像素空间推理对提升视觉任务性能至关重要，所提出的框架有效且实用。

Abstract: Chain-of-thought reasoning has significantly improved the performance of
Large Language Models (LLMs) across various domains. However, this reasoning
process has been confined exclusively to textual space, limiting its
effectiveness in visually intensive tasks. To address this limitation, we
introduce the concept of reasoning in the pixel-space. Within this novel
framework, Vision-Language Models (VLMs) are equipped with a suite of visual
reasoning operations, such as zoom-in and select-frame. These operations enable
VLMs to directly inspect, interrogate, and infer from visual evidences, thereby
enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space
reasoning capabilities in VLMs presents notable challenges, including the
model's initially imbalanced competence and its reluctance to adopt the newly
introduced pixel-space operations. We address these challenges through a
two-phase training approach. The first phase employs instruction tuning on
synthesized reasoning traces to familiarize the model with the novel visual
operations. Following this, a reinforcement learning (RL) phase leverages a
curiosity-driven reward scheme to balance exploration between pixel-space
reasoning and textual reasoning. With these visual operations, VLMs can
interact with complex visual inputs, such as information-rich images or videos
to proactively gather necessary information. We demonstrate that this approach
significantly improves VLM performance across diverse visual reasoning
benchmarks. Our 7B model, \model, achieves 84\% on V* bench, 74\% on
TallyQA-Complex, and 84\% on InfographicsVQA, marking the highest accuracy
achieved by any open-source model to date. These results highlight the
importance of pixel-space reasoning and the effectiveness of our framework.

</details>


### [363] [Domain Adaptive Skin Lesion Classification via Conformal Ensemble of Vision Transformers](https://arxiv.org/abs/2505.15997)
*Mehran Zoravar,Shadi Alijani,Homayoun Najjaran*

Key words: 深度学习, 一致性预测, 视觉变换器, 集成学习, 领域适应, 医疗影像

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种名为CE-ViTs的新框架，通过结合视觉变换器（ViT）模型和领域适应技术，提升图像分类的可靠性和性能。实验显示其覆盖率达到90.38%，比单一模型提高9.95%。

Motivation: 研究旨在解决深度学习模型在领域偏移场景（如不同数据源的变异性）中不确定性估计的挑战，尤其是在医疗影像决策支持系统等关键领域。

Method: 提出CE-ViTs框架，利用多种视觉变换器（ViT）模型的集成学习，结合HAM10000、Dermofit和ISIC皮肤癌数据集，通过一致性学习（conformal learning）增强领域适应性和模型鲁棒性。

Result: 实验结果表明，CE-ViTs的覆盖率达到90.38%（比基线模型提高9.95%），且对困难样本的平均预测集大小从1.86提升到3.075。

Conclusion: CE-ViTs通过集成学习和一致性校准显著提升了模型的领域适应性和预测可靠性，适用于医疗影像等高需求场景。

Abstract: Exploring the trustworthiness of deep learning models is crucial, especially
in critical domains such as medical imaging decision support systems. Conformal
prediction has emerged as a rigorous means of providing deep learning models
with reliable uncertainty estimates and safety guarantees. However, conformal
prediction results face challenges due to the backbone model's struggles in
domain-shifted scenarios, such as variations in different sources. To aim this
challenge, this paper proposes a novel framework termed Conformal Ensemble of
Vision Transformers (CE-ViTs) designed to enhance image classification
performance by prioritizing domain adaptation and model robustness, while
accounting for uncertainty. The proposed method leverages an ensemble of vision
transformer models in the backbone, trained on diverse datasets including
HAM10000, Dermofit, and Skin Cancer ISIC datasets. This ensemble learning
approach, calibrated through the combined mentioned datasets, aims to enhance
domain adaptation through conformal learning. Experimental results underscore
that the framework achieves a high coverage rate of 90.38\%, representing an
improvement of 9.95\% compared to the HAM10000 model. This indicates a strong
likelihood that the prediction set includes the true label compared to singular
models. Ensemble learning in CE-ViTs significantly improves conformal
prediction performance, increasing the average prediction set size for
challenging misclassified samples from 1.86 to 3.075.

</details>


### [364] [Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation](https://arxiv.org/abs/2505.16146)
*Zhenglin Hua,Jinghan He,Zijun Yao,Tianxu Han,Haiyun Guo,Yuheng Jia,Junfeng Fang*

Key words: LVLM, 幻觉, 稀疏自编码器, 训练免费, 语义方向

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种基于稀疏自编码器（SAE）的训练免费方法SSL，通过识别与幻觉或真实性相关的语义方向，有效减少大视觉语言模型（LVLM）中的幻觉问题。

Motivation: 尽管大视觉语言模型在多模态任务表现出色，但其生成的文本可能不一致于视觉输入（幻觉），现有解决方案计算成本高且效率低。

Method: 利用SAE识别与幻觉或实际相关的语义方向，提出SSL方法通过调整这些方向来减少幻觉。

Result: 实验表明SSL在减少幻觉方面优于现有解码方法，且在不同模型架构中具有可迁移性，额外时间成本极低。

Conclusion: SSL为LVLM中的幻觉问题提供了一种高效、精确的解决方案，无需额外训练，适用性广泛。

Abstract: Large vision-language models (LVLMs) have achieved remarkable performance on
multimodal tasks such as visual question answering (VQA) and image captioning.
However, they still suffer from hallucinations, generating text inconsistent
with visual input, posing significant risks in real-world applications.
Existing approaches to address this issue focus on incorporating external
knowledge bases, alignment training, or decoding strategies, all of which
require substantial computational cost and time. Recent works try to explore
more efficient alternatives by adjusting LVLMs' internal representations.
Although promising, these methods may cause hallucinations to be insufficiently
suppressed or lead to excessive interventions that negatively affect normal
semantics. In this work, we leverage sparse autoencoders (SAEs) to identify
semantic directions closely associated with either hallucinations or actuality,
realizing more precise and direct hallucination-related representations. Our
analysis demonstrates that interventions along the faithful direction we
identified can mitigate hallucinations, while those along the hallucinatory
direction can exacerbate them. Building on these insights, we propose Steering
LVLMs via SAE Latent Directions (SSL), a training-free method based on
SAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive
experiments demonstrate that SSL significantly outperforms existing decoding
approaches in mitigating hallucinations, while maintaining transferability
across different model architectures with negligible additional time overhead.

</details>


### [365] [When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification](https://arxiv.org/abs/2505.16149)
*Zirui Pang,Haosheng Tan,Yuhan Pu,Zhijie Deng,Zhouan Shen,Keyu Hu,Jiaheng Wei*

Key words: 图像分类,标签噪声,缺失标签,视觉语言模型,数据集清洗

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: REVEAL框架结合预训练视觉语言模型与标签清洗方法，系统性解决图像分类数据集中的噪声和缺失标签问题，显著提升6个基准测试集的质量。

Motivation: 现有图像分类数据集（如CIFAR、MNIST、ImageNet）因标签噪声和缺失导致模型评估不公且误导，需综合性解决方案。

Method: REVEAL整合视觉语言模型（如LLaVA、BLIP）与标签清洗工具（如Cleanlab），通过置信度预测和共识过滤检测噪声与缺失标签。

Result: 成功提升6个基准测试集质量，生成带概率的软标签，并通过人工验证高度符合人类判断。

Conclusion: REVEAL为数据集噪声与缺失标签问题提供了高效解决方案，促进了更准确的模型评估。

Abstract: Image classification benchmark datasets such as CIFAR, MNIST, and ImageNet
serve as critical tools for model evaluation. However, despite the cleaning
efforts, these datasets still suffer from pervasive noisy labels and often
contain missing labels due to the co-existing image pattern where multiple
classes appear in an image sample. This results in misleading model comparisons
and unfair evaluations. Existing label cleaning methods focus primarily on
noisy labels, but the issue of missing labels remains largely overlooked.
Motivated by these challenges, we present a comprehensive framework named
REVEAL, integrating state-of-the-art pre-trained vision-language models (e.g.,
LLaVA, BLIP, Janus, Qwen) with advanced machine/human label curation methods
(e.g., Docta, Cleanlab, MTurk), to systematically address both noisy labels and
missing label detection in widely-used image classification test sets. REVEAL
detects potential noisy labels and omissions, aggregates predictions from
various methods, and refines label accuracy through confidence-informed
predictions and consensus-based filtering. Additionally, we provide a thorough
analysis of state-of-the-art vision-language models and pre-trained image
classifiers, highlighting their strengths and limitations within the context of
dataset renovation by revealing 10 observations. Our method effectively reveals
missing labels from public datasets and provides soft-labeled results with
likelihoods. Through human verifications, REVEAL significantly improves the
quality of 6 benchmark test sets, highly aligning to human judgments and
enabling more accurate and meaningful comparisons in image classification.

</details>


### [366] [QuickVideo: Real-Time Long Video Understanding with System Algorithm Co-Design](https://arxiv.org/abs/2505.16175)
*Benjamin Schneider,Dongfu Jiang,Chao Du,Tianyu Pang,Wenhu Chen*

Key words: 长视频理解, VideoLLMs, 并行解码, KV缓存剪枝, CPU-GPU重叠

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: QuickVideo通过并行解码、KV缓存剪枝和CPU-GPU重叠处理，显著加速长视频理解，解决了VideoLLMs的计算瓶颈。

Motivation: 现实应用（如视频监控、会议总结）中长视频理解需求日益增长，但现有方法因解码和token填充导致的高延迟与内存消耗难以实用。

Method: 提出QuickVideo系统，包含并行解码器QuickDecoder、内存优化的QuickPrefill及CPU-GPU重叠处理方案。

Result: 实验显示QuickVideo将长视频推理时间缩短一分钟，且在不同时长和采样率下均有效。

Conclusion: QuickVideo实现了在有限硬件上的高效长视频理解，具有实际应用潜力。

Abstract: Long-video understanding has emerged as a crucial capability in real-world
applications such as video surveillance, meeting summarization, educational
lecture analysis, and sports broadcasting. However, it remains computationally
prohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential
video decoding, the process of converting the raw bit stream to RGB frames can
take up to a minute for hour-long video inputs, and 2) costly prefilling of up
to several million tokens for LLM inference, resulting in high latency and
memory use. To address these challenges, we propose QuickVideo, a
system-algorithm co-design that substantially accelerates long-video
understanding to support real-time downstream applications. It comprises three
key innovations: QuickDecoder, a parallelized CPU-based video decoder that
achieves 2-3 times speedup by splitting videos into keyframe-aligned intervals
processed concurrently; QuickPrefill, a memory-efficient prefilling method
using KV-cache pruning to support more frames with less GPU memory; and an
overlapping scheme that overlaps CPU video decoding with GPU inference.
Together, these components infernece time reduce by a minute on long video
inputs, enabling scalable, high-quality video understanding even on limited
hardware. Experiments show that QuickVideo generalizes across durations and
sampling rates, making long video processing feasible in practice.

</details>


### [367] [Understanding Generative AI Capabilities in Everyday Image Editing Tasks](https://arxiv.org/abs/2505.16181)
*Mohammad Reza Taesiri,Brandon Collins,Logan Bolton,Viet Dac Lai,Franck Dernoncourt,Trung Bui,Anh Totti Nguyen*

Key words: Generative AI, Image Editing, Reddit, Human Rating, VLM

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文通过分析83k个Reddit社区请求和305k个PSR-wizard编辑，研究了人们对AI编辑器的需求和AI编辑器的表现，发现仅33%的请求能由最佳AI编辑器完成，AI在低创造性任务上表现较差，且常无法保留人或动物的身份。

Motivation: 研究人们最常希望进行哪些图像编辑任务，以及AI编辑器的实际表现，以改进AI编辑工具并确定哪些任务适合AI处理。

Method: 分析了83k个Reddit社区请求（2013-2025年）和305k个PSR-wizard编辑，比较了AI编辑器（如GPT-4o、Gemini-2.0-Flash、SeedEdit）的表现，并进行了人类和VLM（如o1）的评分对比。

Result: 仅33%的请求能被最佳AI编辑器完成，AI在低创造性任务上表现较差，常无法保留身份或做出未经请求的修饰。VLM评分与人类偏好存在差异。

Conclusion: AI编辑器在创造性任务上表现较好，但需改进精确编辑任务的处理能力。研究为AI编辑器的优化提供了方向。

Abstract: Generative AI (GenAI) holds significant promise for automating everyday image
editing tasks, especially following the recent release of GPT-4o on March 25,
2025. However, what subjects do people most often want edited? What kinds of
editing actions do they want to perform (e.g., removing or stylizing the
subject)? Do people prefer precise edits with predictable outcomes or highly
creative ones? By understanding the characteristics of real-world requests and
the corresponding edits made by freelance photo-editing wizards, can we draw
lessons for improving AI-based editors and determine which types of requests
can currently be handled successfully by AI editors? In this paper, we present
a unique study addressing these questions by analyzing 83k requests from the
past 12 years (2013-2025) on the Reddit community, which collected 305k
PSR-wizard edits. According to human ratings, approximately only 33% of
requests can be fulfilled by the best AI editors (including GPT-4o,
Gemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on
low-creativity requests that require precise editing than on more open-ended
tasks. They often struggle to preserve the identity of people and animals, and
frequently make non-requested touch-ups. On the other side of the table, VLM
judges (e.g., o1) perform differently from human judges and may prefer AI edits
more than human edits. Code and qualitative examples are available at:
https://psrdataset.github.io

</details>


### [368] [VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought](https://arxiv.org/abs/2505.16192)
*Chaoya Jiang,Yongrui Heng,Wei Ye,Han Yang,Haiyang Xu,Ming Yan,Ji Zhang,Fei Huang,Shikun Zhang*

Key words: 视觉语言模型, 区域识别, 推理链, R-GRPO, 多模态推理

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: VLM-R$^3$是一个视觉语言模型框架，通过区域识别和推理能力，动态聚焦图像区域并整合视觉证据到文本推理链中。使用R-GRPO训练方法提升模型表现，在多个基准测试中达到最佳效果。

Motivation: 现有的推理型多模态语言模型在处理需要动态聚焦和反复访问视觉区域的复杂任务时表现不足，因此需要一种能更精确地将文本推理与视觉证据结合的方法。

Method: 提出了VLM-R$^3$框架，包含区域识别和推理能力，并通过R-GRPO训练范式奖励模型选择信息区域、形成适当变换并整合视觉上下文。使用VLIR语料库提供步骤级监督。

Result: 在MathVista、ScienceQA等基准测试中，VLM-R$^3$在零样本和少样本设置下取得了最优表现，尤其是在需要空间推理或细粒度视觉线索提取的任务中提升显著。

Conclusion: VLM-R$^3$通过动态区域聚焦和视觉证据整合，显著提升了多模态推理能力，为复杂任务提供了更精确的解决方案。

Abstract: Recently, reasoning-based MLLMs have achieved a degree of success in
generating long-form textual reasoning chains. However, they still struggle
with complex tasks that necessitate dynamic and iterative focusing on and
revisiting of visual regions to achieve precise grounding of textual reasoning
in visual evidence. We introduce \textbf{VLM-R$^3$} (\textbf{V}isual
\textbf{L}anguage \textbf{M}odel with \textbf{R}egion \textbf{R}ecognition and
\textbf{R}easoning), a framework that equips an MLLM with the ability to (i)
decide \emph{when} additional visual evidence is needed, (ii) determine
\emph{where} to ground within the image, and (iii) seamlessly weave the
relevant sub-image content back into an interleaved chain-of-thought. The core
of our method is \textbf{Region-Conditioned Reinforcement Policy Optimization
(R-GRPO)}, a training paradigm that rewards the model for selecting informative
regions, formulating appropriate transformations (e.g.\ crop, zoom), and
integrating the resulting visual context into subsequent reasoning steps. To
bootstrap this policy, we compile a modest but carefully curated Visuo-Lingual
Interleaved Rationale (VLIR) corpus that provides step-level supervision on
region selection and textual justification. Extensive experiments on MathVista,
ScienceQA, and other benchmarks show that VLM-R$^3$ sets a new state of the art
in zero-shot and few-shot settings, with the largest gains appearing on
questions demanding subtle spatial reasoning or fine-grained visual cue
extraction.

</details>


### [369] [DualComp: End-to-End Learning of a Unified Dual-Modality Lossless Compressor](https://arxiv.org/abs/2505.16256)
*Yan Zhao,Zhengxue Cheng,Junxuan Zhang,Qunshan Gu,Qi Wang,Li Song*

Key words: 学习型无损压缩, 多模态, 轻量级, DualComp, 模态统一

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: DualComp是一种轻量级、统一的双模态无损压缩器，针对图像和文本设计了三种结构增强，实现了接近SOTA的性能，且参数效率高。

Motivation: 现有的学习型无损压缩器多为单模态设计，缺乏灵活性，且多模态大模型复杂度高，难以实际部署。

Method: 设计了模态统一的分词、模态切换的上下文学习和模态路由的专家混合机制，结合重参数化训练策略。

Result: 在桌面CPU上实现近实时推理（200KB/s），参数更少但性能与SOTA的LLM方法相当，简化单模态版在Kodak数据集上超越之前最佳图像压缩器约9%。

Conclusion: DualComp通过高效参数利用和轻量级设计，为多模态无损压缩提供了实用解决方案。

Abstract: Most learning-based lossless compressors are designed for a single modality,
requiring separate models for multi-modal data and lacking flexibility.
However, different modalities vary significantly in format and statistical
properties, making it ineffective to use compressors that lack
modality-specific adaptations. While multi-modal large language models (MLLMs)
offer a potential solution for modality-unified compression, their excessive
complexity hinders practical deployment. To address these challenges, we focus
on the two most common modalities, image and text, and propose DualComp, the
first unified and lightweight learning-based dual-modality lossless compressor.
Built on a lightweight backbone, DualComp incorporates three key structural
enhancements to handle modality heterogeneity: modality-unified tokenization,
modality-switching contextual learning, and modality-routing
mixture-of-experts. A reparameterization training strategy is also used to
boost compression performance. DualComp integrates both modality-specific and
shared parameters for efficient parameter utilization, enabling near real-time
inference (200KB/s) on desktop CPUs. With much fewer parameters, DualComp
achieves compression performance on par with the SOTA LLM-based methods for
both text and image datasets. Its simplified single-modality variant surpasses
the previous best image compressor on the Kodak dataset by about 9% using just
1.2% of the model size.

</details>


### [370] [DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving](https://arxiv.org/abs/2505.16278)
*Zhenjie Yang,Yilin Chai,Xiaosong Jia,Qifeng Li,Yuqian Shao,Xuekai Zhu,Haisheng Su,Junchi Yan*

Key words: 端到端自动驾驶, MoE架构, 场景专用, 行为专用, SOTA性能

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: DriveMoE提出了一种基于MoE架构的端到端自动驾驶框架，通过视觉和行为专家模块的动态选择，解决了复杂驾驶场景中的模式平均问题，并在Bench2Drive评测中达到SOTA性能。

Motivation: 自动驾驶需要处理多视角传感数据并应对复杂场景（如激进转弯），而MoE架构在LLM中的成功展示了参数专业化的可扩展性优势。

Method: 在Drive-π₀基础上增加了场景专用视觉MoE和行为专用动作MoE，通过路由器动态选择相关摄像头和专家模块，模拟人类驾驶的注意力机制。

Result: 在Bench2Drive闭环评测中取得SOTA性能，验证了视觉与动作MoE结合的有效性。

Conclusion: DriveMoE通过专业化设计有效处理多样化驾驶场景，避免了模式平均问题，代码和模型将开源。

Abstract: End-to-end autonomous driving (E2E-AD) demands effective processing of
multi-view sensory data and robust handling of diverse and complex driving
scenarios, particularly rare maneuvers such as aggressive turns. Recent success
of Mixture-of-Experts (MoE) architecture in Large Language Models (LLMs)
demonstrates that specialization of parameters enables strong scalability. In
this work, we propose DriveMoE, a novel MoE-based E2E-AD framework, with a
Scene-Specialized Vision MoE and a Skill-Specialized Action MoE. DriveMoE is
built upon our $\pi_0$ Vision-Language-Action (VLA) baseline (originally from
the embodied AI field), called Drive-$\pi_0$. Specifically, we add Vision MoE
to Drive-$\pi_0$ by training a router to select relevant cameras according to
the driving context dynamically. This design mirrors human driving cognition,
where drivers selectively attend to crucial visual cues rather than
exhaustively processing all visual information. In addition, we add Action MoE
by training another router to activate specialized expert modules for different
driving behaviors. Through explicit behavioral specialization, DriveMoE is able
to handle diverse scenarios without suffering from modes averaging like
existing models. In Bench2Drive closed-loop evaluation experiments, DriveMoE
achieves state-of-the-art (SOTA) performance, demonstrating the effectiveness
of combining vision and action MoE in autonomous driving tasks. We will release
our code and models of DriveMoE and Drive-$\pi_0$.

</details>


### [371] [Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval](https://arxiv.org/abs/2505.15877)
*Siting Li,Xiang Gao,Simon Shaolei Du*

Key words: 文本到图像检索, COCO-Facet, 可提示嵌入, CLIP, MLLM

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了COCO-Facet基准测试，评估了CLIP-like检索器和多模态大型语言模型（MLLM）在属性聚焦查询中的表现，发现它们表现不佳。作者提出了可提示图像嵌入方法，显著提升了性能，并提供了两种加速策略。

Motivation: 现有的文本到图像（T2I）检索器在处理属性聚焦查询时表现不佳，尤其是CLIP-like检索器和MLLM检索器，因为它们忽略了部分关键视觉属性。

Method: 作者构建了COCO-Facet基准测试，揭示了检索器的局限性，并提出了一种可提示图像嵌入方法，通过高亮所需属性来提升性能。同时提出了两种加速策略：预处理可提示嵌入和使用线性近似。

Result: 实验表明，可提示图像嵌入方法能够显著提升性能，预处理策略在Recall@5上提升了15%，线性近似策略在推理时提升了8%。

Conclusion: 使用通用图像嵌入进行检索是次优选择，可提示图像嵌入方法能更好地处理属性聚焦查询，并通过加速策略增强了实用性。

Abstract: While an image is worth more than a thousand words, only a few provide
crucial information for a given task and thus should be focused on. In light of
this, ideal text-to-image (T2I) retrievers should prioritize specific visual
attributes relevant to queries. To evaluate current retrievers on handling
attribute-focused queries, we build COCO-Facet, a COCO-based benchmark with
9,112 queries about diverse attributes of interest. We find that CLIP-like
retrievers, which are widely adopted due to their efficiency and zero-shot
ability, have poor and imbalanced performance, possibly because their image
embeddings focus on global semantics and subjects while leaving out other
details. Notably, we reveal that even recent Multimodal Large Language Model
(MLLM)-based, stronger retrievers with a larger output dimension struggle with
this limitation. Hence, we hypothesize that retrieving with general image
embeddings is suboptimal for performing such queries. As a solution, we propose
to use promptable image embeddings enabled by these multimodal retrievers,
which boost performance by highlighting required attributes. Our pipeline for
deriving such embeddings generalizes across query types, image pools, and base
retriever architectures. To enhance real-world applicability, we offer two
acceleration strategies: Pre-processing promptable embeddings and using linear
approximations. We show that the former yields a 15% improvement in Recall@5
when prompts are predefined, while the latter achieves an 8% improvement when
prompts are only available during inference.

</details>


### [372] [NTIRE 2025 challenge on Text to Image Generation Model Quality Assessment](https://arxiv.org/abs/2505.16314)
*Shuhao Han,Haotian Fan,Fangyuan Kong,Wenjie Liao,Chunle Guo,Chongyi Li,Radu Timofte,Liang Li,Tao Li,Junhui Cui,Yunqiu Wang,Yang Tai,Jingwei Sun,Jianhui Sun,Xinli Yue,Tianyi Wang,Huan Hou,Junda Lu,Xinyang Huang,Zitang Zhou,Zijian Zhang,Xuhui Zheng,Xuecheng Wu,Chong Peng,Xuezhi Cao,Trong-Hieu Nguyen-Mau,Minh-Hoang Le,Minh-Khoa Le-Phan,Duy-Nam Ly,Hai-Dang Nguyen,Minh-Triet Tran,Yukang Lin,Yan Hong,Chuanbiao Song,Siyuan Li,Jun Lan,Zhichao Zhang,Xinyue Li,Wei Sun,Zicheng Zhang,Yunhao Li,Xiaohong Liu,Guangtao Zhai,Zitong Xu,Huiyu Duan,Jiarui Wang,Guangji Ma,Liu Yang,Lu Liu,Qiang Hu,Xiongkuo Min,Zichuan Wang,Zhenchen Tang,Bo Peng,Jing Dong,Fengbin Guan,Zihao Yu,Yiting Lu,Wei Luo,Xin Li,Minhao Lin,Haofeng Chen,Xuanxuan He,Kele Xu,Qisheng Xu,Zijian Gao,Tianjiao Wan,Bo-Cheng Qiu,Chih-Chung Hsu,Chia-ming Lee,Yu-Fan Lin,Bo Yu,Zehao Wang,Da Mu,Mingxiu Chen,Junkang Fang,Huamei Sun,Wending Zhao,Zhiyu Wang,Wang Liu,Weikang Yu,Puhong Duan,Bin Sun,Xudong Kang,Shutao Li,Shuai He,Lingzhi Fu,Heng Cong,Rongyu Zhang,Jiarong He,Zhishan Qiao,Yongqing Huang,Zewen Chen,Zhe Pang,Juan Wang,Jian Guo,Zhizhuo Shao,Ziyu Feng,Bing Li,Weiming Hu,Hesong Li,Dehua Liu,Zeming Liu,Qingsong Xie,Ruichen Wang,Zhihao Li,Yuqi Liang,Jianqi Bi,Jun Luo,Junfeng Yang,Can Li,Jing Fu,Hongwei Xu,Mingrui Long,Lulin Tang*

Key words: NTIRE, 文本到图像生成, 质量评估, EvalMuse-40K, EvalMuse-Structure

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: NTIRE 2025挑战赛关注文本到图像（T2I）生成模型的质量评估，分为图像-文本对齐和图像结构失真检测两个赛道。

Motivation: 解决文本到图像生成模型的细粒度质量评估问题。

Method: 两个赛道分别使用EvalMuse-40K和EvalMuse-Structure数据集进行评估，吸引了大量参与者提交方案。

Result: 两个赛道的获奖方法均显著优于基线，展现了卓越的T2I质量预测性能。

Conclusion: 挑战赛成功推动了T2I生成模型质量评估的研究，展示了高效的评估方法。

Abstract: This paper reports on the NTIRE 2025 challenge on Text to Image (T2I)
generation model quality assessment, which will be held in conjunction with the
New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2025.
The aim of this challenge is to address the fine-grained quality assessment of
text-to-image generation models. This challenge evaluates text-to-image models
from two aspects: image-text alignment and image structural distortion
detection, and is divided into the alignment track and the structural track.
The alignment track uses the EvalMuse-40K, which contains around 40K
AI-Generated Images (AIGIs) generated by 20 popular generative models. The
alignment track has a total of 371 registered participants. A total of 1,883
submissions are received in the development phase, and 507 submissions are
received in the test phase. Finally, 12 participating teams submitted their
models and fact sheets. The structure track uses the EvalMuse-Structure, which
contains 10,000 AI-Generated Images (AIGIs) with corresponding structural
distortion mask. A total of 211 participants have registered in the structure
track. A total of 1155 submissions are received in the development phase, and
487 submissions are received in the test phase. Finally, 8 participating teams
submitted their models and fact sheets. Almost all methods have achieved better
results than baseline methods, and the winning methods in both tracks have
demonstrated superior prediction performance on T2I model quality assessment.

</details>


### [373] [ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation](https://arxiv.org/abs/2505.15928)
*Tony Montes,Fernando Lozano*

Key words: 视频问答,LLM,思维链,YOLO-World,对象跟踪

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种结合思维链框架和YOLO-World的LLM智能体，用于零样本视频问答任务，提升了对象跟踪和推理能力，在多个基准测试中表现优异。

Motivation: 当前基于LLM的视频问答系统在对象跟踪和时间推理方面仍有改进空间，需要更好地对齐对象引用和语言模型输出。

Method: 采用思维链框架和YOLO-World结合的LLM智能体，增强对象跟踪和推理对齐。

Result: 在NExT-QA、iVQA和ActivityNet-QA基准测试中取得了最先进的效果，并支持时间框验证，提高输出可靠性。

Conclusion: 该方法显著提升了视频问答和理解的性能，具有跨领域应用潜力。

Abstract: Recent advancements in Video Question Answering (VideoQA) have introduced
LLM-based agents, modular frameworks, and procedural solutions, yielding
promising results. These systems use dynamic agents and memory-based mechanisms
to break down complex tasks and refine answers. However, significant
improvements remain in tracking objects for grounding over time and
decision-making based on reasoning to better align object references with
language model outputs, as newer models get better at both tasks. This work
presents an LLM-brained agent for zero-shot Video Question Answering (VideoQA)
that combines a Chain-of-Thought framework with grounding reasoning alongside
YOLO-World to enhance object tracking and alignment. This approach establishes
a new state-of-the-art in VideoQA and Video Understanding, showing enhanced
performance on NExT-QA, iVQA, and ActivityNet-QA benchmarks. Our framework also
enables cross-checking of grounding timeframes, improving accuracy and
providing valuable support for verification and increased output reliability
across multiple video domains. The code is available at
https://github.com/t-montes/viqagent.

</details>


### [374] [OViP: Online Vision-Language Preference Learning](https://arxiv.org/abs/2505.15963)
*Shujun Liu,Siyuan Wang,Zejun Li,Jianxiang Wang,Cheng Zeng,Zhongyu Wei*

Key words: 视觉语言模型、幻觉抑制、偏好学习、扩散模型、多模态

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一个名为OViP的在线视觉语言偏好学习框架，通过动态构建对比训练数据减少视觉语言模型的幻觉问题，效果优于现有方法。

Motivation: 现有方法依赖预定义或随机编辑的负样本，无法反映模型实际错误，限制了训练效果，因此需要一种动态生成更相关监督信号的方法。

Method: 通过分析模型自身幻觉输出构建对比数据，结合扩散模型生成负样本，实现文本和视觉偏好的自适应对齐。

Result: 实验表明，OViP在幻觉抑制和通用任务上均表现优异，有效减少幻觉的同时保留多模态能力。

Conclusion: OViP通过动态生成训练数据和优化评估协议，显著提升了视觉语言模型的性能。

Abstract: Large vision-language models (LVLMs) remain vulnerable to hallucination,
often generating content misaligned with visual inputs. While recent approaches
advance multi-modal Direct Preference Optimization (DPO) to mitigate
hallucination, they typically rely on predefined or randomly edited negative
samples that fail to reflect actual model errors, limiting training efficacy.
In this work, we propose an Online Vision-language Preference Learning (OViP)
framework that dynamically constructs contrastive training data based on the
model's own hallucinated outputs. By identifying semantic differences between
sampled response pairs and synthesizing negative images using a diffusion
model, OViP generates more relevant supervision signals in real time. This
failure-driven training enables adaptive alignment of both textual and visual
preferences. Moreover, we refine existing evaluation protocols to better
capture the trade-off between hallucination suppression and expressiveness.
Experiments on hallucination and general benchmarks demonstrate that OViP
effectively reduces hallucinations while preserving core multi-modal
capabilities.

</details>


### [375] [FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design](https://arxiv.org/abs/2505.16335)
*Renjie Wei,Songqiang Xu,Qingyu Guo,Meng Li*

Key words: VAR, 量化, FPGA, 图像生成, 低比特浮点

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: FPQVAR是一种高效的浮点量化框架，通过算法与硬件协同设计，显著降低了VAR模型的存储与计算成本，提升了图像质量和推理速度，并在FPGA上实现了高性能加速。

Motivation: 现有VAR模型在边缘设备上部署时面临参数大、计算成本高的问题，亟需一种高效的量化方法以降低资源消耗。

Method: 提出FPQVAR框架，包含双格式量化、分组Walsh-Hadamard变换和GHT感知可学习变换，并在FPGA上设计低比特浮点量化器和加速器。

Result: 4位量化下FID从10.83降至3.58，IS从175.9提升至241.5；FPGA加速器吞吐量达1.1 image/s，能效优于整数加速器和GPU基线。

Conclusion: FPQVAR通过协同优化算法与硬件，实现了高效的VAR量化与加速，为边缘部署提供了可行方案。

Abstract: Visual autoregressive (VAR) modeling has marked a paradigm shift in image
generation from next-token prediction to next-scale prediction. VAR predicts a
set of tokens at each step from coarse to fine scale, leading to better image
quality and faster inference speed compared to existing diffusion models.
However, the large parameter size and computation cost hinder its deployment on
edge devices. To reduce the memory and computation cost, we propose FPQVAR, an
efficient post-training floating-point (FP) quantization framework for VAR
featuring algorithm and hardware co-design. At the algorithm level, we first
identify the challenges of quantizing VAR. To address them, we propose Dual
Format Quantization for the highly imbalanced input activation. We further
propose Group-wise Hadamard Transformation and GHT-Aware Learnable
Transformation to address the time-varying outlier channels. At the hardware
level, we design the first low-bit FP quantizer and multiplier with lookup
tables on FPGA and propose the first FPGA-based VAR accelerator featuring
low-bit FP computation and an elaborate two-level pipeline. Extensive
experiments show that compared to the state-of-the-art quantization method, our
proposed FPQVAR significantly improves Fr\'echet Inception Distance (FID) from
10.83 to 3.58, Inception Score (IS) from 175.9 to 241.5 under 4-bit
quantization. FPQVAR also significantly improves the performance of 6-bit
quantized VAR, bringing it on par with the FP16 model. Our accelerator on
AMD-Xilinx VCK190 FPGA achieves a throughput of 1.1 image/s, which is 3.1x
higher than the integer-based accelerator. It also demonstrates 3.6x and 2.8x
higher energy efficiency compared to the integer-based accelerator and GPU
baseline, respectively.

</details>


### [376] [SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval](https://arxiv.org/abs/2505.15867)
*Nikolaos Chaidos,Angeliki Dimitriou,Maria Lymperaiou,Giorgos Stamou*

Key words: 图像检索, 场景图, 无监督学习, 图自编码器, 图编辑距离

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种基于场景图的图像检索框架SCENIR，通过无监督的图自编码器减少对标注数据的依赖，并采用图编辑距离作为更可靠的相似性度量，显著提升了检索性能。

Motivation: 现有基于卷积和Transformer的图像检索模型易受低级视觉特征（如颜色）的偏见影响，且基于监督图神经网络的方法依赖于不稳定的标注数据。本文旨在通过无监督学习提升语义理解和检索可靠性。

Method: 提出SCENIR框架，采用无监督的图自编码器学习场景图表示，并首次引入图编辑距离（GED）作为相似性度量标准。

Result: 在多项指标和运行时效率上优于现有的视觉、多模态及监督GNN方法，且在未标注数据集上验证了泛化能力。

Conclusion: SCENIR通过无监督学习和GED的应用，显著提升了图像检索的语义理解能力和可靠性，推动了反事实图像检索的进展。

Abstract: Despite the dominance of convolutional and transformer-based architectures in
image-to-image retrieval, these models are prone to biases arising from
low-level visual features, such as color. Recognizing the lack of semantic
understanding as a key limitation, we propose a novel scene graph-based
retrieval framework that emphasizes semantic content over superficial image
characteristics. Prior approaches to scene graph retrieval predominantly rely
on supervised Graph Neural Networks (GNNs), which require ground truth graph
pairs driven from image captions. However, the inconsistency of caption-based
supervision stemming from variable text encodings undermine retrieval
reliability. To address these, we present SCENIR, a Graph Autoencoder-based
unsupervised retrieval framework, which eliminates the dependence on labeled
training data. Our model demonstrates superior performance across metrics and
runtime efficiency, outperforming existing vision-based, multimodal, and
supervised GNN approaches. We further advocate for Graph Edit Distance (GED) as
a deterministic and robust ground truth measure for scene graph similarity,
replacing the inconsistent caption-based alternatives for the first time in
image-to-image retrieval evaluation. Finally, we validate the generalizability
of our method by applying it to unannotated datasets via automated scene graph
generation, while substantially contributing in advancing state-of-the-art in
counterfactual image retrieval.

</details>


### [377] [Temporal and Spatial Feature Fusion Framework for Dynamic Micro Expression Recognition](https://arxiv.org/abs/2505.16372)
*Feng Liu,Bingyu Nan,Xuezhong Qian,Xiaolan Fu*

Key words: 微表情识别, 动态微表情, 多模态融合, 时空特征, TSFmicro

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种新型的时空特征融合框架TSFmicro，用于动态微表情识别，通过结合Retention Network和基于transformer的网络来提升识别准确率。

Motivation: 微表情因短暂和局部化特性难以准确识别，专业人员的识别准确率仅50%，因此需要探索多模态融合技术以提升识别效果。

Method: 提出TSFmicro框架，整合Retention Network和transformer网络，采用并行时空融合方法在高维特征空间融合时空信息。

Result: 实验表明TSFmicro在三个知名微表情数据集上优于当前最优方法。

Conclusion: TSFmicro通过高效捕捉和融合时空关系，显著提升了微表情识别的准确性和语义信息丰富度。

Abstract: When emotions are repressed, an individual's true feelings may be revealed
through micro-expressions. Consequently, micro-expressions are regarded as a
genuine source of insight into an individual's authentic emotions. However, the
transient and highly localised nature of micro-expressions poses a significant
challenge to their accurate recognition, with the accuracy rate of
micro-expression recognition being as low as 50%, even for professionals. In
order to address these challenges, it is necessary to explore the field of
dynamic micro expression recognition (DMER) using multimodal fusion techniques,
with special attention to the diverse fusion of temporal and spatial modal
features. In this paper, we propose a novel Temporal and Spatial feature Fusion
framework for DMER (TSFmicro). This framework integrates a Retention Network
(RetNet) and a transformer-based DMER network, with the objective of efficient
micro-expression recognition through the capture and fusion of temporal and
spatial relations. Meanwhile, we propose a novel parallel time-space fusion
method from the perspective of modal fusion, which fuses spatio-temporal
information in high-dimensional feature space, resulting in complementary
"where-how" relationships at the semantic level and providing richer semantic
information for the model. The experimental results demonstrate the superior
performance of the TSFmicro method in comparison to other contemporary
state-of-the-art methods. This is evidenced by its effectiveness on three
well-recognised micro-expression datasets.

</details>


### [378] [Analyzing Hierarchical Structure in Vision Models with Sparse Autoencoders](https://arxiv.org/abs/2505.15970)
*Matthew Lyle Olson,Musashi Hinck,Neale Ratzlaff,Changbai Li,Phillip Howard,Vasudev Lal,Shao-Yen Tseng*

Key words: ImageNet，稀疏自动编码器（SAEs），视觉模型，层次结构，语义表征

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该研究利用稀疏自动编码器（SAEs）分析视觉模型如何编码ImageNet层次结构，揭示了模型隐含的层次关系，并提出了一个框架来系统分析视觉模型的表征。

Motivation: 探究视觉模型如何学习和编码ImageNet的层次结构，以理解模型内部的语义表征。

Method: 使用稀疏自动编码器（SAEs）分析视觉模型DINOv2的内部表征，研究其与ImageNet层次结构的关系。

Result: SAEs揭示了模型激活中的层次关系，表明模型隐含地编码了层次结构信息，特别是在类标记中。

Conclusion: SAEs是用于探测视觉模型语义结构的有效工具，且视觉模型能够内部化层次类别信息。

Abstract: The ImageNet hierarchy provides a structured taxonomy of object categories,
offering a valuable lens through which to analyze the representations learned
by deep vision models. In this work, we conduct a comprehensive analysis of how
vision models encode the ImageNet hierarchy, leveraging Sparse Autoencoders
(SAEs) to probe their internal representations. SAEs have been widely used as
an explanation tool for large language models (LLMs), where they enable the
discovery of semantically meaningful features. Here, we extend their use to
vision models to investigate whether learned representations align with the
ontological structure defined by the ImageNet taxonomy. Our results show that
SAEs uncover hierarchical relationships in model activations, revealing an
implicit encoding of taxonomic structure. We analyze the consistency of these
representations across different layers of the popular vision foundation model
DINOv2 and provide insights into how deep vision models internalize
hierarchical category information by increasing information in the class token
through each layer. Our study establishes a framework for systematic
hierarchical analysis of vision model representations and highlights the
potential of SAEs as a tool for probing semantic structure in deep networks.

</details>


### [379] [DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos](https://arxiv.org/abs/2505.16376)
*Zijia Lu,A S M Iftekhar,Gaurav Mittal,Tianjian Meng,Xiawei Wang,Cheng Zhao,Rohith Kukkala,Ehsan Elhamifar,Mei Chen*

Key words: 长视频时间定位, 计算效率, DeCafNet, 显著性图, 多尺度特征

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: DeCafNet通过“委托与攻克”策略减少长视频时间定位的计算成本，同时保持性能，显著提升效率。

Motivation: 解决现有方法在处理长视频时因计算成本高而难以扩展的问题。

Method: 引入轻量级sidekick编码器提取特征和生成显著性图，结合专家编码器，并通过DeCaf-Grounder统一多尺度特征。

Result: 在减少47%计算量的同时，性能超越现有方法，达到新的SOTA。

Conclusion: DeCafNet高效且性能优越，为长视频时间定位提供了新思路。

Abstract: Long Video Temporal Grounding (LVTG) aims at identifying specific moments
within lengthy videos based on user-provided text queries for effective content
retrieval. The approach taken by existing methods of dividing video into clips
and processing each clip via a full-scale expert encoder is challenging to
scale due to prohibitive computational costs of processing a large number of
clips in long videos. To address this issue, we introduce DeCafNet, an approach
employing ``delegate-and-conquer'' strategy to achieve computation efficiency
without sacrificing grounding performance. DeCafNet introduces a sidekick
encoder that performs dense feature extraction over all video clips in a
resource-efficient manner, while generating a saliency map to identify the most
relevant clips for full processing by the expert encoder. To effectively
leverage features from sidekick and expert encoders that exist at different
temporal resolutions, we introduce DeCaf-Grounder, which unifies and refines
them via query-aware temporal aggregation and multi-scale temporal refinement
for accurate grounding. Experiments on two LTVG benchmark datasets demonstrate
that DeCafNet reduces computation by up to 47\% while still outperforming
existing methods, establishing a new state-of-the-art for LTVG in terms of both
efficiency and performance. Our code is available at
https://github.com/ZijiaLewisLu/CVPR2025-DeCafNet.

</details>


### [380] [Redemption Score: An Evaluation Framework to Rank Image Captions While Redeeming Image Semantics and Language Pragmatics](https://arxiv.org/abs/2505.16180)
*Ashim Dahal,Ankit Ghimire,Saydul Akbar Murad,Nick Rahimi*

Key words: 图像标题评估, Redemption Score, MID, DINO, BERTScore

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: Redemption Score是一种新型混合框架，通过结合三种互补信号（MID、DINO感知相似性和BERTScore）来评估图像标题，优于现有方法。

Motivation: 现有图像标题评估指标未能全面捕捉视觉语义和语言语用学，需要更全面的评估框架。

Method: 结合MID（全局图像文本分布对齐）、DINO感知相似性（视觉基础）和BERTScore（文本上下文相似性）进行综合评估。

Result: 在Flickr8k基准测试中，Redemption Score的Kendall-τ为56.43，优于12种现有方法，且与人类判断相关性更高。

Conclusion: Redemption Score通过有效结合视觉和语言信号，提供了更鲁棒和细致的评估框架。

Abstract: Evaluating image captions requires cohesive assessment of both visual
semantics and language pragmatics, which is often not entirely captured by most
metrics. We introduce Redemption Score, a novel hybrid framework that ranks
image captions by triangulating three complementary signals: (1) Mutual
Information Divergence (MID) for global image-text distributional alignment,
(2) DINO-based perceptual similarity of cycle-generated images for visual
grounding, and (3) BERTScore for contextual text similarity against human
references. A calibrated fusion of these signals allows Redemption Score to
offer a more holistic assessment. On the Flickr8k benchmark, Redemption Score
achieves a Kendall-$\tau$ of 56.43, outperforming twelve prior methods and
demonstrating superior correlation with human judgments without requiring
task-specific training. Our framework provides a more robust and nuanced
evaluation by effectively redeeming image semantics and linguistic
interpretability indicated by strong transfer of knowledge in the Conceptual
Captions and MS COCO datasets.

</details>


### [381] [Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models](https://arxiv.org/abs/2505.16416)
*Chengcheng Wang,Jianyuan Guo,Hongguang Li,Yuchuan Tian,Ying Nie,Chang Xu,Kai Han*

Key words: Rotary Position Embedding, 视觉语言模型, 跨模态偏差, Circle-RoPE, Per-Token Distance

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了Circle-RoPE，一种新的位置编码方案，用于减少多模态模型中的跨模态位置偏差，同时保留图像的空间信息。

Motivation: 解决旋转位置嵌入（RoPE）在大型视觉语言模型（LVLM）中导致的跨模态位置偏差问题。

Method: 提出Per-Token Distance（PTD）度量指标，并设计Circle-RoPE方案，通过正交映射避免跨模态偏差，同时采用分层策略结合不同RoPE变体。

Result: 实验表明，该方法有效减少位置偏差并保留图像空间信息，提供了更鲁棒的编码框架。

Conclusion: Circle-RoPE为LVLMs提供了一种更灵活且有效的位置编码解决方案。

Abstract: Rotary Position Embedding (RoPE) is a widely adopted technique for encoding
relative positional information in large language models (LLMs). However, when
extended to large vision-language models (LVLMs), its variants introduce
unintended cross-modal positional biases. Specifically, they enforce relative
positional dependencies between text token indices and image tokens, causing
spurious alignments. This issue arises because image tokens representing the
same content but located at different spatial positions are assigned distinct
positional biases, leading to inconsistent cross-modal associations. To address
this, we propose Per-Token Distance (PTD) - a simple yet effective metric for
quantifying the independence of positional encodings across modalities.
Informed by this analysis, we introduce Circle-RoPE, a novel encoding scheme
that maps image token indices onto a circular trajectory orthogonal to the
linear path of text token indices, forming a cone-like structure. This
configuration ensures that each text token maintains an equal distance to all
image tokens, reducing artificial cross-modal biases while preserving
intra-image spatial information. To further enhance performance, we propose a
staggered layer strategy that applies different RoPE variants across layers.
This design leverages the complementary strengths of each RoPE variant, thereby
enhancing the model's overall performance. Our experimental results demonstrate
that our method effectively preserves spatial information from images while
reducing relative positional bias, offering a more robust and flexible
positional encoding framework for LVLMs. The code is available at
[https://github.com/lose4578/CircleRoPE](https://github.com/lose4578/CircleRoPE).

</details>


### [382] [Investigating Fine- and Coarse-grained Structural Correspondences Between Deep Neural Networks and Human Object Image Similarity Judgments Using Unsupervised Alignment](https://arxiv.org/abs/2505.16419)
*Soh Takahashi,Masaru Sasaki,Ken Takeda,Masafumi Oizumi*

Key words: 物体表征、Gromov-Wasserstein最优传输、CLIP、自监督学习、THINGS数据集

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该研究利用无监督对齐方法比较人类与模型在粗细粒度上的物体表征，发现CLIP模型在粗细粒度上均与人类表征高度匹配，而自监督模型则表现有限，但仍能捕捉粗粒度类别结构。

Motivation: 探讨人类如何通过不同学习范式（如监督、自监督和CLIP）获得物体表征，并验证这些表征在粗细粒度上与人类表征的相似性。

Method: 采用基于Gromov-Wasserstein最优传输的无监督对齐方法，比较人类与模型在粗细粒度上的物体表征，利用THINGS数据集的1,854个物体的人类相似性判断进行评估。

Result: CLIP模型在粗细粒度上均与人类表征高度匹配，自监督模型匹配有限但仍能反映粗粒度类别结构。

Conclusion: 语言信息对精确物体表征的获取至关重要，自监督学习在捕捉粗粒度类别结构上具有一定潜力。

Abstract: The learning mechanisms by which humans acquire internal representations of
objects are not fully understood. Deep neural networks (DNNs) have emerged as a
useful tool for investigating this question, as they have internal
representations similar to those of humans as a byproduct of optimizing their
objective functions. While previous studies have shown that models trained with
various learning paradigms - such as supervised, self-supervised, and CLIP -
acquire human-like representations, it remains unclear whether their similarity
to human representations is primarily at a coarse category level or extends to
finer details. Here, we employ an unsupervised alignment method based on
Gromov-Wasserstein Optimal Transport to compare human and model object
representations at both fine-grained and coarse-grained levels. The unique
feature of this method compared to conventional representational similarity
analysis is that it estimates optimal fine-grained mappings between the
representation of each object in human and model representations. We used this
unsupervised alignment method to assess the extent to which the representation
of each object in humans is correctly mapped to the corresponding
representation of the same object in models. Using human similarity judgments
of 1,854 objects from the THINGS dataset, we find that models trained with CLIP
consistently achieve strong fine- and coarse-grained matching with human object
representations. In contrast, self-supervised models showed limited matching at
both fine- and coarse-grained levels, but still formed object clusters that
reflected human coarse category structure. Our results offer new insights into
the role of linguistic information in acquiring precise object representations
and the potential of self-supervised learning to capture coarse categorical
structures.

</details>


### [383] [CMRINet: Joint Groupwise Registration and Segmentation for Cardiac Function Quantification from Cine-MRI](https://arxiv.org/abs/2505.16452)
*Mohamed S. Elmahdy,Marius Staring,Patrick J. H. de Koning,Samer Alabed,Mahan Salehi,Faisal Alandejani,Michael Sharkey,Ziad Aldabbagh,Andrew J. Swift,Rob J. van der Geest*

Key words: 心脏MRI, 深度学习, 群体配准, 分割, 心脏功能评估

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种端到端的深度学习模型，联合估计心脏MRI图像的群体配准和分割，提高了性能并减少了计算时间。

Motivation: 当前心脏功能评估中，左心室射血分数（LVEF）和心肌应变通常分别计算，可能限制功能评估的全面性。

Method: 开发了一种名为Anatomically-guided Deep GW的端到端深度学习模型，联合进行群体配准和分割。

Result: 在大规模数据集上验证，模型表现优于传统配准方法和两种基于深度学习的方法，且计算时间显著减少。

Conclusion: 该模型为心脏功能评估提供了更高效、更全面的解决方案。

Abstract: Accurate and efficient quantification of cardiac function is essential for
the estimation of prognosis of cardiovascular diseases (CVDs). One of the most
commonly used metrics for evaluating cardiac pumping performance is left
ventricular ejection fraction (LVEF). However, LVEF can be affected by factors
such as inter-observer variability and varying pre-load and after-load
conditions, which can reduce its reproducibility. Additionally, cardiac
dysfunction may not always manifest as alterations in LVEF, such as in heart
failure and cardiotoxicity diseases. An alternative measure that can provide a
relatively load-independent quantitative assessment of myocardial contractility
is myocardial strain and strain rate. By using LVEF in combination with
myocardial strain, it is possible to obtain a thorough description of cardiac
function. Automated estimation of LVEF and other volumetric measures from
cine-MRI sequences can be achieved through segmentation models, while strain
calculation requires the estimation of tissue displacement between sequential
frames, which can be accomplished using registration models. These tasks are
often performed separately, potentially limiting the assessment of cardiac
function. To address this issue, in this study we propose an end-to-end deep
learning (DL) model that jointly estimates groupwise (GW) registration and
segmentation for cardiac cine-MRI images. The proposed anatomically-guided Deep
GW network was trained and validated on a large dataset of 4-chamber view
cine-MRI image series of 374 subjects. A quantitative comparison with
conventional GW registration using elastix and two DL-based methods showed that
the proposed model improved performance and substantially reduced computation
time.

</details>


### [384] [Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports](https://arxiv.org/abs/2505.16624)
*Francesco Dalla Serra,Patrick Schrempf,Chaoyang Wang,Zaiqiao Meng,Fani Deligianni,Alison Q. O'Neil*

Key words: 胸部X光,视觉问答,放射学报告,单图像问题,图像差异问题

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的胸部X光（CXR）视觉问答（VQA）方法，结合放射学报告提升模型性能，实现了单图像和图像差异问题的统一处理。

Motivation: 研究旨在通过整合放射学报告提升CXR VQA任务的性能，尤其是针对单图像和图像差异问题的问答能力。

Method: 提出了一种统一的方法，包括报告生成（RG）和答案生成（AG）两步，利用放射学报告作为AG模型的额外输入。

Result: 在Medical-Diff-VQA数据集上，该方法实现了最先进的性能，显著提升了单图像和图像差异问题的解答效果。

Conclusion: 放射学报告的整合显著提升了CXR VQA任务的性能，为未来医学图像分析提供了新思路。

Abstract: We present a novel approach to Chest X-ray (CXR) Visual Question Answering
(VQA), addressing both single-image image-difference questions. Single-image
questions focus on abnormalities within a specific CXR ("What abnormalities are
seen in image X?"), while image-difference questions compare two longitudinal
CXRs acquired at different time points ("What are the differences between image
X and Y?"). We further explore how the integration of radiology reports can
enhance the performance of VQA models. While previous approaches have
demonstrated the utility of radiology reports during the pre-training phase, we
extend this idea by showing that the reports can also be leveraged as
additional input to improve the VQA model's predicted answers. First, we
propose a unified method that handles both types of questions and
auto-regressively generates the answers. For single-image questions, the model
is provided with a single CXR. For image-difference questions, the model is
provided with two CXRs from the same patient, captured at different time
points, enabling the model to detect and describe temporal changes. Taking
inspiration from 'Chain-of-Thought reasoning', we demonstrate that performance
on the CXR VQA task can be improved by grounding the answer generator module
with a radiology report predicted for the same CXR. In our approach, the VQA
model is divided into two steps: i) Report Generation (RG) and ii) Answer
Generation (AG). Our results demonstrate that incorporating predicted radiology
reports as evidence to the AG model enhances performance on both single-image
and image-difference questions, achieving state-of-the-art results on the
Medical-Diff-VQA dataset.

</details>


### [385] [R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO](https://arxiv.org/abs/2505.16673)
*Huanjin Yao,Qixiang Yin,Jingyi Zhang,Min Yang,Yibo Wang,Wenhao Wu,Fei Su,Li Shen,Minghui Qiu,Dacheng Tao,Jiaxing Huang*

Key words: 多模态大语言模型, 强化学习, 推理能力, 稀疏奖励, Share-GRPO

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出Share-GRPO，一种基于强化学习的多模态大语言模型推理能力优化方法，通过多样化推理轨迹探索和奖励共享解决稀疏奖励和优势消失问题。

Motivation: 激励多模态大语言模型（MLLMs）的推理能力，同时解决强化学习中的稀疏奖励和优势消失问题。

Method: 提出Share-GRPO方法，通过数据扩展技术扩充问题空间，鼓励模型探索多样化推理轨迹，并在扩展问题间共享轨迹和奖励信息。

Result: 在六个常用推理基准测试中表现优越。

Conclusion: Share-GRPO有效提升MLLM的推理能力，为强化学习在多模态任务中的应用提供了新思路。

Abstract: In this work, we aim to incentivize the reasoning ability of Multimodal Large
Language Models (MLLMs) via reinforcement learning (RL) and develop an
effective approach that mitigates the sparse reward and advantage vanishing
issues during RL. To this end, we propose Share-GRPO, a novel RL approach that
tackle these issues by exploring and sharing diverse reasoning trajectories
over expanded question space. Specifically, Share-GRPO first expands the
question space for a given question via data transformation techniques, and
then encourages MLLM to effectively explore diverse reasoning trajectories over
the expanded question space and shares the discovered reasoning trajectories
across the expanded questions during RL. In addition, Share-GRPO also shares
reward information during advantage computation, which estimates solution
advantages hierarchically across and within question variants, allowing more
accurate estimation of relative advantages and improving the stability of
policy training. Extensive evaluations over six widely-used reasoning
benchmarks showcase the superior performance of our method. Code will be
available at https://github.com/HJYao00/R1-ShareVL.

</details>


### [386] [Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection](https://arxiv.org/abs/2505.16512)
*Jiaxin Liu,Jia Wang,Saihui Hou,Min Ren,Huijia Wu,Zhaofeng He*

Key words: 深度伪造,扩散模型,多模态检测,数字人,时空特征

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文介绍了首个基于扩散模型的大规模多模态数字人伪造数据集DigiFakeAV，并提出检测基线方法DigiShield，通过时空和多模态融合有效识别伪造视频。

Motivation: 深度伪造技术快速发展，尤其是基于扩散模型的数字人生成技术对公共安全构成严重威胁，现有检测方法难以应对。为此，论文构建多样化数据集并提出新检测方法。

Method: 采用五种最新数字人生成方法构建包含6万视频的多模态数据集DigiFakeAV；提出DigiShield方法，通过时空和跨模态融合分析视频与音频特征。

Result: 伪造视频与真实视频的混淆率达68%，现有SOTA检测模型在DigiFakeAV上AUC值大幅下降。DigiShield在两个数据集上均达到SOTA性能。

Conclusion: DigiFakeAV数据集揭示了当前检测方法的不足，DigiShield通过细粒度时空-多模态分析有效提升伪造视频检测能力。

Abstract: In recent years, the rapid development of deepfake technology has given rise
to an emerging and serious threat to public security: diffusion model-based
digital human generation. Unlike traditional face manipulation methods, such
models can generate highly realistic videos with consistency through multimodal
control signals. Their flexibility and covertness pose severe challenges to
existing detection strategies. To bridge this gap, we introduce DigiFakeAV, the
first large-scale multimodal digital human forgery dataset based on diffusion
models. Employing five latest digital human generation methods (Sonic, Hallo,
etc.) and voice cloning method, we systematically produce a dataset comprising
60,000 videos (8.4 million frames), covering multiple nationalities, skin
tones, genders, and real-world scenarios, significantly enhancing data
diversity and realism. User studies show that the confusion rate between forged
and real videos reaches 68%, and existing state-of-the-art (SOTA) detection
models exhibit large drops in AUC values on DigiFakeAV, highlighting the
challenge of the dataset. To address this problem, we further propose
DigiShield, a detection baseline based on spatiotemporal and cross-modal
fusion. By jointly modeling the 3D spatiotemporal features of videos and the
semantic-acoustic features of audio, DigiShield achieves SOTA performance on
both the DigiFakeAV and DF-TIMIT datasets. Experiments show that this method
effectively identifies covert artifacts through fine-grained analysis of the
temporal evolution of facial features in synthetic videos.

</details>


### [387] [TextureSAM: Towards a Texture Aware Foundation Model for Segmentation](https://arxiv.org/abs/2505.16540)
*Inbal Cohen,Boaz Meivar,Peihan Tu,Shai Avidan,Gal Oren*

Key words: SAM, TextureSAM, 纹理分割, 微调, 数据增强

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文研究了SAM模型在纹理主导场景中的语义偏好问题，并提出了TextureSAM模型，通过纹理增强技术优化训练数据，显著提升了纹理分割性能。

Motivation: 针对SAM模型在纹理定义边界的领域（如医学影像、材料分类）中的性能局限性，研究其形状偏置问题并提出改进方案。

Method: 采用纹理增强技术微调模型，利用纹理修改后的ADE20K数据集训练TextureSAM，以强调纹理特征。

Result: TextureSAM在自然和合成纹理数据集上分别比SAM-2提升了0.2和0.18 mIoU。

Conclusion: TextureSAM有效解决了SAM的纹理分割不足问题，为纹理主导任务提供了更优解决方案。

Abstract: Segment Anything Models (SAM) have achieved remarkable success in object
segmentation tasks across diverse datasets. However, these models are
predominantly trained on large-scale semantic segmentation datasets, which
introduce a bias toward object shape rather than texture cues in the image.
This limitation is critical in domains such as medical imaging, material
classification, and remote sensing, where texture changes define object
boundaries. In this study, we investigate SAM's bias toward semantics over
textures and introduce a new texture-aware foundation model, TextureSAM, which
performs superior segmentation in texture-dominant scenarios. To achieve this,
we employ a novel fine-tuning approach that incorporates texture augmentation
techniques, incrementally modifying training images to emphasize texture
features. By leveraging a novel texture-alternation of the ADE20K dataset, we
guide TextureSAM to prioritize texture-defined regions, thereby mitigating the
inherent shape bias present in the original SAM model. Our extensive
experiments demonstrate that TextureSAM significantly outperforms SAM-2 on both
natural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation
datasets. The code and texture-augmented dataset will be publicly available.

</details>


### [388] [Auto-nnU-Net: Towards Automated Medical Image Segmentation](https://arxiv.org/abs/2505.16561)
*Jannis Becktepe,Leona Hennig,Steffen Oeltze-Jafra,Marius Lindauer*

Key words: 医学图像分割，AutoML，nnU-Net，超参数优化，神经架构搜索

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: Auto-nnU-Net通过引入超参数优化、神经架构搜索和分层神经架构搜索，改进了nnU-Net框架，提升了医学图像分割性能并优化了资源利用。

Motivation: 现有nnU-Net框架虽自动化但受限于固定超参数和启发式设计，无法充分优化性能或资源利用。

Method: 提出Auto-nnU-Net，整合HPO、NAS和HNAS，并引入Regularized PriorBand平衡准确性与计算资源。

Result: 在10个数据集中，6个性能显著提升，其余持平，同时保持合理资源需求。

Conclusion: Auto-nnU-Net有效提升了医学图像分割性能，适用于实际资源受限场景。

Abstract: Medical Image Segmentation (MIS) includes diverse tasks, from bone to organ
segmentation, each with its own challenges in finding the best segmentation
model. The state-of-the-art AutoML-related MIS-framework nnU-Net automates many
aspects of model configuration but remains constrained by fixed hyperparameters
and heuristic design choices. As a full-AutoML framework for MIS, we propose
Auto-nnU-Net, a novel nnU-Net variant enabling hyperparameter optimization
(HPO), neural architecture search (NAS), and hierarchical NAS (HNAS).
Additionally, we propose Regularized PriorBand to balance model accuracy with
the computational resources required for training, addressing the resource
constraints often faced in real-world medical settings that limit the
feasibility of extensive training procedures. We evaluate our approach across
diverse MIS datasets from the well-established Medical Segmentation Decathlon,
analyzing the impact of AutoML techniques on segmentation performance,
computational efficiency, and model design choices. The results demonstrate
that our AutoML approach substantially improves the segmentation performance of
nnU-Net on 6 out of 10 datasets and is on par on the other datasets while
maintaining practical resource requirements. Our code is available at
https://github.com/LUH-AI/AutonnUNet.

</details>


### [389] [Accelerating Targeted Hard-Label Adversarial Attacks in Low-Query Black-Box Settings](https://arxiv.org/abs/2505.16313)
*Arjhun Swaminathan,Mete Akgün*

Key words: 对抗样本, 目标攻击, 边缘信息, 黑盒攻击, 图像分类

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种名为TEA的新型攻击方法，利用目标图像的边缘信息生成对抗样本，在低查询量情况下优于现有方法，且为几何攻击提供了更好的初始化。

Motivation: 当前黑盒对抗攻击方法主要依赖决策边界的几何特性，而忽略了图像本身的信息，尤其是在窄决策区域下目标攻击效果不佳。

Method: 提出TEA方法，通过利用目标图像的边缘信息精心扰动，生成更接近源图像且能实现目标分类的对抗样本。

Result: 在低查询量设置下，TEA方法比现有方法表现更优，查询量减少近70%，并为几何攻击提供了更好的初始化。

Conclusion: TEA方法通过利用边缘信息有效提升了对抗攻击的性能，尤其在资源受限的实际场景中更具优势。

Abstract: Deep neural networks for image classification remain vulnerable to
adversarial examples -- small, imperceptible perturbations that induce
misclassifications. In black-box settings, where only the final prediction is
accessible, crafting targeted attacks that aim to misclassify into a specific
target class is particularly challenging due to narrow decision regions.
Current state-of-the-art methods often exploit the geometric properties of the
decision boundary separating a source image and a target image rather than
incorporating information from the images themselves. In contrast, we propose
Targeted Edge-informed Attack (TEA), a novel attack that utilizes edge
information from the target image to carefully perturb it, thereby producing an
adversarial image that is closer to the source image while still achieving the
desired target classification. Our approach consistently outperforms current
state-of-the-art methods across different models in low query settings (nearly
70\% fewer queries are used), a scenario especially relevant in real-world
applications with limited queries and black-box access. Furthermore, by
efficiently generating a suitable adversarial example, TEA provides an improved
target initialization for established geometry-based attacks.

</details>


### [390] [MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning](https://arxiv.org/abs/2505.16964)
*Suhao Yu,Haojin Wang,Juncheng Wu,Cihang Xie,Yuyin Zhou*

Key words: medical VQA, multi-image reasoning, diagnostic AI, multimodal LLMs, MedFrameQA

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: MedFrameQA is a new medical VQA benchmark focusing on multi-image reasoning, built using automated frame extraction and filtering. Despite testing advanced models, performance remains below 50%, highlighting challenges in multi-image analysis.

Motivation: To mimic clinicians' diagnostic workflow of comparing multiple images, addressing the gap in single-image-focused medical VQA benchmarks.

Method: Developed an automated pipeline for extracting coherent frames from medical videos and a filtering strategy to ensure data quality. Evaluated ten Multimodal LLMs on this dataset.

Result: Models performed poorly (below 50% accuracy), struggling with salient findings, evidence aggregation, and error propagation across images. Performance varied by body systems and organs.

Conclusion: The benchmark underscores the need for improved multi-image reasoning in diagnostic AI, aiming to inspire further research in this area.

Abstract: Existing medical VQA benchmarks mostly focus on single-image analysis, yet
clinicians almost always compare a series of images before reaching a
diagnosis. To better approximate this workflow, we introduce MedFrameQA -- the
first benchmark that explicitly evaluates multi-image reasoning in medical VQA.
To build MedFrameQA both at scale and in high-quality, we develop 1) an
automated pipeline that extracts temporally coherent frames from medical videos
and constructs VQA items whose content evolves logically across images, and 2)
a multiple-stage filtering strategy, including model-based and manual review,
to preserve data clarity, difficulty, and medical relevance. The resulting
dataset comprises 2,851 VQA pairs (gathered from 9,237 high-quality frames in
3,420 videos), covering nine human body systems and 43 organs; every question
is accompanied by two to five images. We comprehensively benchmark ten advanced
Multimodal LLMs -- both proprietary and open source, with and without explicit
reasoning modules -- on MedFrameQA. The evaluation challengingly reveals that
all models perform poorly, with most accuracies below 50%, and accuracy
fluctuates as the number of images per question increases. Error analysis
further shows that models frequently ignore salient findings, mis-aggregate
evidence across images, and propagate early mistakes through their reasoning
chains; results also vary substantially across body systems, organs, and
modalities. We hope this work can catalyze research on clinically grounded,
multi-image reasoning and accelerate progress toward more capable diagnostic AI
systems.

</details>


### [391] [Style Transfer with Diffusion Models for Synthetic-to-Real Domain Adaptation](https://arxiv.org/abs/2505.16360)
*Estelle Chigot,Dennis G. Wilson,Meriem Ghrib,Thomas Oberlin*

Key words: 语义分割, 扩散模型, 风格迁移, 域适应, 合成数据

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出利用扩散模型提升合成数据训练视觉模型的性能，通过两种新技术解决语义一致性的风格迁移问题，实验证明其能有效缩小合成与真实数据的差距。

Motivation: 解决合成数据训练的语义分割模型在真实图像中表现不佳的问题，尤其在标注数据稀缺的恶劣条件下。

Method: 提出两种新方法：基于语义类别选择性统计归一化的CACTI，以及进一步通过特征相似性过滤交叉注意力图的CACTIF。

Result: 实验显示，方法在GTA5到Cityscapes/ACDC的域转换中生成更高质量图像（FID分数更低）且能更好地保留内容。

Conclusion: 类感知的扩散风格迁移能有效缩小域差距，即使在目标域数据极少的情况下，也能提升感知系统的鲁棒性。

Abstract: Semantic segmentation models trained on synthetic data often perform poorly
on real-world images due to domain gaps, particularly in adverse conditions
where labeled data is scarce. Yet, recent foundation models enable to generate
realistic images without any training. This paper proposes to leverage such
diffusion models to improve the performance of vision models when learned on
synthetic data. We introduce two novel techniques for semantically consistent
style transfer using diffusion models: Class-wise Adaptive Instance
Normalization and Cross-Attention (CACTI) and its extension with selective
attention Filtering (CACTIF). CACTI applies statistical normalization
selectively based on semantic classes, while CACTIF further filters
cross-attention maps based on feature similarity, preventing artifacts in
regions with weak cross-attention correspondences. Our methods transfer style
characteristics while preserving semantic boundaries and structural coherence,
unlike approaches that apply global transformations or generate content without
constraints. Experiments using GTA5 as source and Cityscapes/ACDC as target
domains show that our approach produces higher quality images with lower FID
scores and better content preservation. Our work demonstrates that class-aware
diffusion-based style transfer effectively bridges the synthetic-to-real domain
gap even with minimal target domain data, advancing robust perception systems
for challenging real-world applications. The source code is available at:
https://github.com/echigot/cactif.

</details>


### [392] [SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game Understanding](https://arxiv.org/abs/2505.16630)
*Sushant Gautam,Cise Midoglu,Vajira Thambawita,Michael A. Riegler,Pål Halvorsen,Mubarak Shah*

Key words: 足球分析,多模态AI,交互式分析,SoccerNet,裁判决策

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: SoccerChat通过整合视觉和文本数据，实现了对足球视频的多模态理解，并在事件分类和裁判决策中表现出色，推动了交互式AI体育分析的发展。

Motivation: 传统足球分析方法依赖孤立数据流，无法全面捕捉比赛情境，因此需要一种更综合的解决方案。

Method: 引入SoccerChat框架，整合视觉和文本数据，基于SoccerNet数据集和结构化视频指令进行训练，支持游戏理解、事件分类和裁判决策。

Result: 在动作分类和裁判决策任务中表现优异，展示了多模态整合在足球分析中的重要性。

Conclusion: 多模态集成是提升足球分析的关键，为交互式和可解释的AI体育分析奠定了基础。

Abstract: The integration of artificial intelligence in sports analytics has
transformed soccer video understanding, enabling real-time, automated insights
into complex game dynamics. Traditional approaches rely on isolated data
streams, limiting their effectiveness in capturing the full context of a match.
To address this, we introduce SoccerChat, a multimodal conversational AI
framework that integrates visual and textual data for enhanced soccer video
comprehension. Leveraging the extensive SoccerNet dataset, enriched with jersey
color annotations and automatic speech recognition (ASR) transcripts,
SoccerChat is fine-tuned on a structured video instruction dataset to
facilitate accurate game understanding, event classification, and referee
decision making. We benchmark SoccerChat on action classification and referee
decision-making tasks, demonstrating its performance in general soccer event
comprehension while maintaining competitive accuracy in referee decision
making. Our findings highlight the importance of multimodal integration in
advancing soccer analytics, paving the way for more interactive and explainable
AI-driven sports analysis. https://github.com/simula/SoccerChat

</details>


### [393] [Mitigating Hallucinations in Vision-Language Models through Image-Guided Head Suppression](https://arxiv.org/abs/2505.16411)
*Sreetama Sarkar,Yue Che,Alex Gavin,Peter A. Beerel,Souvik Kundu*

Key words: 大型视觉语言模型, 幻觉, 注意力机制, 推理效率

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: SPIN是一种任务无关的注意力引导头抑制策略，旨在减少大型视觉语言模型（LVLM）中的幻觉现象，同时不增加计算或延迟开销。

Motivation: 大型视觉语言模型在多模态理解任务中表现出色，但常出现与视觉内容不符的文本生成（幻觉）问题。现有方法通常增加延迟，因此需要更高效的解决方案。

Method: SPIN通过分析发现幻觉与特定注意力头相关，因此在推理时选择性抑制对图像注意力低的头，保留前K个注意力头。

Result: 在视觉问答和图像描述任务上，SPIN显著降低了幻觉分数（最高2.7倍），同时保持F1分数，吞吐量提升1.8倍。

Conclusion: SPIN是一种高效的幻觉抑制方法，能够在不增加计算负担的情况下显著改善模型性能。

Abstract: Despite their remarkable progress in multimodal understanding tasks, large
vision language models (LVLMs) often suffer from "hallucinations", generating
texts misaligned with the visual context. Existing methods aimed at reducing
hallucinations through inference time intervention incur a significant increase
in latency. To mitigate this, we present SPIN, a task-agnostic attention-guided
head suppression strategy that can be seamlessly integrated during inference,
without incurring any significant compute or latency overhead. We investigate
whether hallucination in LVLMs can be linked to specific model components. Our
analysis suggests that hallucinations can be attributed to a dynamic subset of
attention heads in each layer. Leveraging this insight, for each text query
token, we selectively suppress attention heads that exhibit low attention to
image tokens, keeping the top-K attention heads intact. Extensive evaluations
on visual question answering and image description tasks demonstrate the
efficacy of SPIN in reducing hallucination scores up to 2.7x while maintaining
F1, and improving throughput by 1.8x compared to existing alternatives. Code is
available at https://github.com/YUECHE77/SPIN.

</details>


### [394] [Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models](https://arxiv.org/abs/2505.17015)
*Runsen Xu,Weiyao Wang,Hao Tang,Xingyu Chen,Xiaodong Wang,Fu-Jen Chu,Dahua Lin,Matt Feiszli,Kevin J. Liang*

Key words: 多模态大语言模型, 多帧空间理解, 机器人应用, MultiSPA数据集, 深度感知

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一个框架，通过整合深度感知、视觉对应和动态感知，赋予多模态大语言模型（MLLMs）多帧空间理解能力，并引入了MultiSPA数据集和基准测试，展示了模型Multi-SpatialMLLM在机器人等实际应用中的潜力。

Motivation: 当前的多模态大语言模型在视觉任务中表现出色，但对多帧的空间理解能力有限，限制了其在机器人等需要多帧推理的实际应用中的表现。

Method: 通过集成深度感知、视觉对应和动态感知，构建MultiSPA数据集（包含超过2700万个多样化3D和4D场景样本），并设计一个综合基准测试，训练出Multi-SpatialMLLM模型。

Result: Multi-SpatialMLLM在基线模型和专有系统上取得了显著提升，展示了可扩展和通用的多帧推理能力，并在机器人任务中展现了多任务收益和新兴能力。

Conclusion: 该框架显著提升了多模态大语言模型在多帧空间理解上的能力，为机器人等实际应用提供了新的可能性。

Abstract: Multi-modal large language models (MLLMs) have rapidly advanced in visual
tasks, yet their spatial understanding remains limited to single images,
leaving them ill-suited for robotics and other real-world applications that
require multi-frame reasoning. In this paper, we propose a framework to equip
MLLMs with robust multi-frame spatial understanding by integrating depth
perception, visual correspondence, and dynamic perception. Central to our
approach is the MultiSPA dataset, a novel, large-scale collection of more than
27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we
introduce a comprehensive benchmark that tests a wide spectrum of spatial tasks
under uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves
significant gains over baselines and proprietary systems, demonstrating
scalable, generalizable multi-frame reasoning. We further observe multi-task
benefits and early indications of emergent capabilities in challenging
scenarios, and showcase how our model can serve as a multi-frame reward
annotator for robotics.

</details>


### [395] [From Evaluation to Defense: Advancing Safety in Video Large Language Models](https://arxiv.org/abs/2505.16643)
*Yiwei Sun,Peiqi Jiang,Chuanbin Liu,Luohao Lin,Zhiying Lu,Hongtao Xie*

Key words: Video LLMs, safety benchmark, multimodal attack, VideoSafety-R1, alarm token

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了首个大规模、文化多样的视频LLM安全基准VideoSafetyBench (VSB-77k),并揭示了视频模态会降低安全性42.3%。为解决这一问题，作者提出了VideoSafety-R1框架，通过双重创新实现了65.1%的安全性能提升。

Motivation: 尽管基于图像的LLM安全问题已被广泛研究，基于视频的LLM（Video LLMs）安全性仍未被充分探讨。因此，作者旨在系统性地研究Video LLMs的安全风险，并提出解决方案。

Method: 作者提出了VideoSafety-R1框架，包含两个创新：(1) Alarm Token-Guided Safety Fine-Tuning (AT-SFT)通过可学习警报令牌实现跨模态的显式危害感知；(2) Safety-Guided GRPO通过动态策略优化增强防御性推理。

Result: 提出的框架在VSB-Eval-HH上实现了65.1%的安全性能提升，并在MMBench、VLGuard和FigStep等图像安全数据集上分别提升了59.1%、44.3%和15.0%。

Conclusion: 视频模态显著降低了LLM的安全性，而VideoSafety-R1框架能有效提升安全性表现，从被动危害识别转向主动推理。

Abstract: While the safety risks of image-based large language models have been
extensively studied, their video-based counterparts (Video LLMs) remain
critically under-examined. To systematically study this problem, we introduce
\textbf{VideoSafetyBench (VSB-77k) - the first large-scale, culturally diverse
benchmark for Video LLM safety}, which compromises 77,646 video-query pairs and
spans 19 principal risk categories across 10 language communities. \textit{We
reveal that integrating video modality degrades safety performance by an
average of 42.3\%, exposing systemic risks in multimodal attack exploitation.}
To address this vulnerability, we propose \textbf{VideoSafety-R1}, a dual-stage
framework achieving unprecedented safety gains through two innovations: (1)
Alarm Token-Guided Safety Fine-Tuning (AT-SFT) injects learnable alarm tokens
into visual and textual sequences, enabling explicit harm perception across
modalities via multitask objectives. (2) Then, Safety-Guided GRPO enhances
defensive reasoning through dynamic policy optimization with rule-based rewards
derived from dual-modality verification. These components synergize to shift
safety alignment from passive harm recognition to active reasoning. The
resulting framework achieves a 65.1\% improvement on VSB-Eval-HH, and improves
by 59.1\%, 44.3\%, and 15.0\% on the image safety datasets MMBench, VLGuard,
and FigStep, respectively. \textit{Our codes are available in the supplementary
materials.} \textcolor{red}{Warning: This paper contains examples of harmful
language and videos, and reader discretion is recommended.}

</details>


### [396] [Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO](https://arxiv.org/abs/2505.17017)
*Chengzhuo Tong,Ziyu Guo,Renrui Zhang,Wenyu Shan,Xinyu Wei,Zhenghao Xing,Hongsheng Li,Pheng-Ann Heng*

Key words: 强化学习,链式思考,自回归图像生成,Direct Preference Optimization,Group Relative Policy Optimization

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 研究了强化学习（RL）在提升大型语言模型（LLMs）的链式思考（CoT）推理能力中的作用，重点关注了Direct Preference Optimization（DPO）和Group Relative Policy Optimization（GRPO）两种算法，并将RL应用于自回归图像生成领域。

Motivation: 探索RL如何增强自回归图像生成中的CoT推理能力，解决文本-图像一致性、图像美学质量和奖励模型设计等挑战。

Method: 通过比较GRPO和DPO算法在自回归图像生成中的表现，评估其领域内性能和领域外泛化能力，并分析不同奖励模型的影响。

Result: 发现GRPO和DPO各有优势，奖励模型的泛化能力对RL算法性能有重要影响，并提出了三种扩展策略。

Conclusion: 研究了如何通过RL算法提升自回归图像生成的CoT推理能力，为未来开发更有效的RL算法提供了新思路。

Abstract: Recent advancements underscore the significant role of Reinforcement Learning
(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large
language models (LLMs). Two prominent RL algorithms, Direct Preference
Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central
to these developments, showcasing different pros and cons. Autoregressive image
generation, also interpretable as a sequential CoT reasoning process, presents
unique challenges distinct from LLM-based CoT reasoning. These encompass
ensuring text-image consistency, improving image aesthetic quality, and
designing sophisticated reward models, rather than relying on simpler
rule-based rewards. While recent efforts have extended RL to this domain, these
explorations typically lack an in-depth analysis of the domain-specific
challenges and the characteristics of different RL strategies. To bridge this
gap, we provide the first comprehensive investigation of the GRPO and DPO
algorithms in autoregressive image generation, evaluating their in-domain
performance and out-of-domain generalization, while scrutinizing the impact of
different reward models on their respective capabilities. Our findings reveal
that GRPO and DPO exhibit distinct advantages, and crucially, that reward
models possessing stronger intrinsic generalization capabilities potentially
enhance the generalization potential of the applied RL algorithms. Furthermore,
we systematically explore three prevalent scaling strategies to enhance both
their in-domain and out-of-domain proficiency, deriving unique insights into
efficiently scaling performance for each paradigm. We hope our study paves a
new path for inspiring future work on developing more effective RL algorithms
to achieve robust CoT reasoning in the realm of autoregressive image
generation. Code is released at
https://github.com/ZiyuGuo99/Image-Generation-CoT

</details>


### [397] [Ranked Entropy Minimization for Continual Test-Time Adaptation](https://arxiv.org/abs/2505.16441)
*Jisu Han,Jaemin Na,Wonjun Hwang*

Key words: test-time adaptation, entropy minimization, model collapse, ranked entropy

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种名为ranked entropy minimization的方法，用于解决持续测试时间适应中的稳定性问题，避免模型崩溃。

Motivation: 熵最小化方法在持续测试时间适应中易导致模型崩溃（预测单一类别），因此需要更稳定的解决方案。

Method: 通过渐进掩码策略结构化预测难度，保持熵的排序一致性，逐步对齐不同难度下的概率分布。

Result: 在多个基准测试中验证了该方法的有效性，能够显著提升模型稳定性。

Conclusion: ranked entropy minimization是一种高效且稳定的持续测试时间适应方法。

Abstract: Test-time adaptation aims to adapt to realistic environments in an online
manner by learning during test time. Entropy minimization has emerged as a
principal strategy for test-time adaptation due to its efficiency and
adaptability. Nevertheless, it remains underexplored in continual test-time
adaptation, where stability is more important. We observe that the entropy
minimization method often suffers from model collapse, where the model
converges to predicting a single class for all images due to a trivial
solution. We propose ranked entropy minimization to mitigate the stability
problem of the entropy minimization method and extend its applicability to
continuous scenarios. Our approach explicitly structures the prediction
difficulty through a progressive masking strategy. Specifically, it gradually
aligns the model's probability distributions across different levels of
prediction difficulty while preserving the rank order of entropy. The proposed
method is extensively evaluated across various benchmarks, demonstrating its
effectiveness through empirical results. Our code is available at
https://github.com/pilsHan/rem

</details>


### [398] [Point, Detect, Count: Multi-Task Medical Image Understanding with Instruction-Tuned Vision-Language Models](https://arxiv.org/abs/2505.16647)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Key words: 视觉语言模型, 医学图像理解, 多任务学习, 指令微调, LoRA

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该研究探讨了如何通过微调视觉语言模型（VLMs）实现医学图像的多任务理解，包括检测、定位和计数。结果显示多任务训练提高了模型的鲁棒性和准确性，但也存在一定的权衡，如边缘案例的可靠性降低。

Motivation: 评估指令调优的VLMs是否能同时提升医学图像的多任务理解能力，以增强诊断的准确性和效率。

Method: 使用MedMultiPoints数据集，将任务转化为基于指令的提示，并采用LoRA方法微调Qwen2.5-VL-7B-Instruct模型进行多任务训练。

Result: 多任务训练提高了模型的鲁棒性和准确性，例如在计数任务中减少了MAE并提高了匹配精度，但在边缘案例中降低了可靠性。

Conclusion: 研究表明，通过指令驱动的微调，通用VLMs可以适应专业医学任务，并展示了其在复合诊断推理模式中的潜力。

Abstract: We investigate fine-tuning Vision-Language Models (VLMs) for multi-task
medical image understanding, focusing on detection, localization, and counting
of findings in medical images. Our objective is to evaluate whether
instruction-tuned VLMs can simultaneously improve these tasks, with the goal of
enhancing diagnostic accuracy and efficiency. Using MedMultiPoints, a
multimodal dataset with annotations from endoscopy (polyps and instruments) and
microscopy (sperm cells), we reformulate each task into instruction-based
prompts suitable for vision-language reasoning. We fine-tune
Qwen2.5-VL-7B-Instruct using Low-Rank Adaptation (LoRA) across multiple task
combinations. Results show that multi-task training improves robustness and
accuracy. For example, it reduces the Count Mean Absolute Error (MAE) and
increases Matching Accuracy in the Counting + Pointing task. However,
trade-offs emerge, such as more zero-case point predictions, indicating reduced
reliability in edge cases despite overall performance gains. Our study
highlights the potential of adapting general-purpose VLMs to specialized
medical tasks via prompt-driven fine-tuning. This approach mirrors clinical
workflows, where radiologists simultaneously localize, count, and describe
findings - demonstrating how VLMs can learn composite diagnostic reasoning
patterns. The model produces interpretable, structured outputs, offering a
promising step toward explainable and versatile medical AI. Code, model
weights, and scripts will be released for reproducibility at
https://github.com/simula/PointDetectCount.

</details>


### [399] [GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning](https://arxiv.org/abs/2505.17022)
*Chengqi Duan,Rongyao Fang,Yuqing Wang,Kun Wang,Linjiang Huang,Xingyu Zeng,Hongsheng Li,Xihui Liu*

Key words: 视觉生成模型,强化学习,语义-空间推理,GoT-R1,双阶段奖励框架

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: GoT-R1利用强化学习增强视觉生成中的语义-空间推理能力，通过双阶段多维奖励框架评估推理过程和最终输出，显著提升了复杂提示下的图像生成质量。

Motivation: 当前视觉生成模型在处理需要精确空间关系和属性的复杂文本提示时表现不佳，因此需要增强语义内容和空间布局的显式推理能力。

Method: 提出GoT-R1框架，结合强化学习和双阶段多维奖励框架，利用MLLMs评估推理过程及输出，以监督语义对齐、空间准确性和视觉质量。

Result: 在T2I-CompBench基准测试中表现显著提升，尤其在涉及精确空间关系和属性绑定的组合任务中。

Conclusion: GoT-R1将复杂推理能力成功引入视觉生成领域，推动了图像生成技术的发展。

Abstract: Visual generation models have made remarkable progress in creating realistic
images from text prompts, yet struggle with complex prompts that specify
multiple objects with precise spatial relationships and attributes. Effective
handling of such prompts requires explicit reasoning about the semantic content
and spatial layout. We present GoT-R1, a framework that applies reinforcement
learning to enhance semantic-spatial reasoning in visual generation. Building
upon the Generation Chain-of-Thought approach, GoT-R1 enables models to
autonomously discover effective reasoning strategies beyond predefined
templates through carefully designed reinforcement learning. To achieve this,
we propose a dual-stage multi-dimensional reward framework that leverages MLLMs
to evaluate both the reasoning process and final output, enabling effective
supervision across the entire generation pipeline. The reward system assesses
semantic alignment, spatial accuracy, and visual quality in a unified approach.
Experimental results demonstrate significant improvements on T2I-CompBench
benchmark, particularly in compositional tasks involving precise spatial
relationships and attribute binding. GoT-R1 advances the state-of-the-art in
image generation by successfully transferring sophisticated reasoning
capabilities to the visual generation domain. To facilitate future research, we
make our code and pretrained models publicly available at
https://github.com/gogoduan/GoT-R1.

</details>


### [400] [AnchorFormer: Differentiable Anchor Attention for Efficient Vision Transformer](https://arxiv.org/abs/2505.16463)
*Jiquan Shan,Junxiao Wang,Lifeng Zhao,Liang Cai,Hongyuan Zhang,Ioannis Liritzis*

Key words: 视觉Transformer、锚点机制、计算复杂度、双边注意力、下游任务

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文介绍了AnchorFormer，一种基于锚点的高效视觉Transformer，通过减少计算复杂度（从O(n^2)到O(mn)）提升推理速度，同时在多个下游任务中表现优异。

Motivation: 传统视觉Transformer（ViTs）的计算复杂度高（O(n^2)），且信息通常集中在少数区域，导致冗余计算。AnchorFormer旨在通过锚点机制降低复杂度，同时保留关键信息。

Method: 提出AnchorFormer，利用锚点与token的双边注意力（O(mn)复杂度），并采用可微分学习锚点分布，通过马尔可夫过程近似全局注意力。模型还扩展至分类、检测和分割任务。

Result: 实验表明，AnchorFormer在ImageNet分类上精度提升9.0%或FLOPs减少46.7%，在COCO检测上mAP提升81.3%（同等FLOPs下），显著优于基线模型。

Conclusion: AnchorFormer通过锚点机制高效捕捉关键信息，在提升性能的同时大幅降低计算成本，适用于多种视觉任务。

Abstract: Recently, vision transformers (ViTs) have achieved excellent performance on
vision tasks by measuring the global self-attention among the image patches.
Given $n$ patches, they will have quadratic complexity such as
$\mathcal{O}(n^2)$ and the time cost is high when splitting the input image
with a small granularity. Meanwhile, the pivotal information is often randomly
gathered in a few regions of an input image, some tokens may not be helpful for
the downstream tasks. To handle this problem, we introduce an anchor-based
efficient vision transformer (AnchorFormer), which employs the anchor tokens to
learn the pivotal information and accelerate the inference. Firstly, by
estimating the bipartite attention between the anchors and tokens, the
complexity will be reduced from $\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$, where
$m$ is an anchor number and $m < n$. Notably, by representing the anchors with
the neurons in a neural layer, we can differentiable learn these distributions
and approximate global self-attention through the Markov process. Moreover, we
extend the proposed model to three downstream tasks including classification,
detection, and segmentation. Extensive experiments show the effectiveness of
our AnchorFormer, e.g., achieving up to a 9.0% higher accuracy or 46.7% FLOPs
reduction on ImageNet classification, 81.3% higher mAP on COCO detection under
comparable FLOPs, as compared to the current baselines.

</details>


### [401] [CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving](https://arxiv.org/abs/2505.16524)
*Huitong Yang,Zhuoxiao Chen,Fengyi Zhang,Zi Huang,Yadan Luo*

Key words: 3D感知, 测试时适应, 模型合并, 轻量级框架, 自动驾驶

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: CodeMerge是一种轻量级模型合并框架，通过在紧凑潜在空间中操作，提高3D感知在动态环境下的鲁棒性，避免了传统方法的高计算成本。

Motivation: 解决现有测试时适应（TTA）方法在3D物体检测中因优化不稳定和计算成本高而失效的问题。

Method: 利用低维指纹和关键值码本表示模型检查点，通过脊杠杆得分计算合并系数，实现高效模型组合。

Result: 在nuScenes-C和nuScenes-to-KITTI基准测试中分别提升14.9% NDS和7.6% mAP，且对下游任务有直接益处。

Conclusion: CodeMerge提供了一种高效且可扩展的模型合并方案，显著提升了动态环境下的3D感知性能。

Abstract: Maintaining robust 3D perception under dynamic and unpredictable test-time
conditions remains a critical challenge for autonomous driving systems.
Existing test-time adaptation (TTA) methods often fail in high-variance tasks
like 3D object detection due to unstable optimization and sharp minima. While
recent model merging strategies based on linear mode connectivity (LMC) offer
improved stability by interpolating between fine-tuned checkpoints, they are
computationally expensive, requiring repeated checkpoint access and multiple
forward passes. In this paper, we introduce CodeMerge, a lightweight and
scalable model merging framework that bypasses these limitations by operating
in a compact latent space. Instead of loading full models, CodeMerge represents
each checkpoint with a low-dimensional fingerprint derived from the source
model's penultimate features and constructs a key-value codebook. We compute
merging coefficients using ridge leverage scores on these fingerprints,
enabling efficient model composition without compromising adaptation quality.
Our method achieves strong performance across challenging benchmarks, improving
end-to-end 3D detection 14.9% NDS on nuScenes-C and LiDAR-based detection by
over 7.6% mAP on nuScenes-to-KITTI, while benefiting downstream tasks such as
online mapping, motion prediction and planning even without training. Code and
pretrained models are released in the supplementary material.

</details>


### [402] [Temporal Object Captioning for Street Scene Videos from LiDAR Tracks](https://arxiv.org/abs/2505.16594)
*Vignesh Gopinathan,Urs Zimmermann,Michael Arnold,Matthias Rottmann*

Key words: 视频字幕生成, LiDAR, 时间动态, 高级驾驶辅助系统, SwinBERT

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种基于LiDAR的自动化视频字幕生成方法，专注于交通参与者的时间动态，通过规则系统提取关键信息并结合模板生成字幕，显著提升了模型的时间理解能力。

Motivation: 尽管视频字幕生成模型在时间信息捕获方面取得了进展，但在理解如何利用时间语义进行有效的时间特征提取方面，尤其是在高级驾驶辅助系统（ADAS）中的应用，仍存在显著差距。

Method: 采用基于LiDAR的自动化字幕生成流程，通过规则系统提取车道位置和相对运动等关键信息，再结合模板生成字幕。随后使用这些字幕监督训练SwinBERT模型。

Result: 实验表明，使用专门设计用于封装细粒度时间行为的模板字幕训练模型，能够一致提升其在三个数据集上的时间理解能力。

Conclusion: 通过整合基于LiDAR的字幕监督，有效减少了当前先进模型架构中普遍存在的视觉/静态偏见，显著增强了时间理解能力。

Abstract: Video captioning models have seen notable advancements in recent years,
especially with regard to their ability to capture temporal information. While
many research efforts have focused on architectural advancements, such as
temporal attention mechanisms, there remains a notable gap in understanding how
models capture and utilize temporal semantics for effective temporal feature
extraction, especially in the context of Advanced Driver Assistance Systems. We
propose an automated LiDAR-based captioning procedure that focuses on the
temporal dynamics of traffic participants. Our approach uses a rule-based
system to extract essential details such as lane position and relative motion
from object tracks, followed by a template-based caption generation. Our
findings show that training SwinBERT, a video captioning model, using only
front camera images and supervised with our template-based captions,
specifically designed to encapsulate fine-grained temporal behavior, leads to
improved temporal understanding consistently across three datasets. In
conclusion, our results clearly demonstrate that integrating LiDAR-based
caption supervision significantly enhances temporal understanding, effectively
addressing and reducing the inherent visual/static biases prevalent in current
state-of-the-art model architectures.

</details>


### [403] [Semantic Compression of 3D Objects for Open and Collaborative Virtual Worlds](https://arxiv.org/abs/2505.16679)
*Jordan Dotzel,Tony Montes,Mohamed S. Abdelfattah,Zhiru Zhang*

Key words: 3D压缩, 语义压缩, 自然语言存储, 深度生成模型, 虚拟现实

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 语义压缩方法通过忽略结构信息，利用自然语言存储和深度生成模型，实现了高达105倍的3D对象压缩，超越了传统方法。

Motivation: 传统3D对象压缩方法在高压缩率下效果不佳，而语义压缩方法能够突破这一限制，适用于协作性增强和虚拟现实应用。

Method: 构建了基于公开生成模型的3D语义压缩流程，利用自然语言存储和深度生成模型预测缺失信息。

Result: 在Objaverse数据集上实现了高达105倍的压缩率，并在100倍压缩率附近保持了更好的质量。

Conclusion: 语义压缩在高压缩率下优于传统方法，具有更广泛的应用潜力。

Abstract: Traditional methods for 3D object compression operate only on structural
information within the object vertices, polygons, and textures. These methods
are effective at compression rates up to 10x for standard object sizes but
quickly deteriorate at higher compression rates with texture artifacts,
low-polygon counts, and mesh gaps. In contrast, semantic compression ignores
structural information and operates directly on the core concepts to push to
extreme levels of compression. In addition, it uses natural language as its
storage format, which makes it natively human-readable and a natural fit for
emerging applications built around large-scale, collaborative projects within
augmented and virtual reality. It deprioritizes structural information like
location, size, and orientation and predicts the missing information with
state-of-the-art deep generative models. In this work, we construct a pipeline
for 3D semantic compression from public generative models and explore the
quality-compression frontier for 3D object compression. We apply this pipeline
to achieve rates as high as 105x for 3D objects taken from the Objaverse
dataset and show that semantic compression can outperform traditional methods
in the important quality-preserving region around 100x compression.

</details>


### [404] [Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding](https://arxiv.org/abs/2505.16652)
*Feilong Tang,Chengzhi Liu,Zhongxing Xu,Ming Hu,Zelin Peng,Zhiwei Yang,Jionglong Su,Minquan Lin,Yifan Peng,Xuelian Cheng,Imran Razzak,Zongyuan Ge*

Key words: 多模态大语言模型, 视觉问答, 幻觉问题, 因果掩码, 注意力机制

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种名为FarSight的解码策略，通过优化因果掩码和动态分配注意力来减少多模态大语言模型中的幻觉问题。

Motivation: 多模态大语言模型在视觉问答中表现优异，但常出现幻觉问题，尤其是初始幻觉和雪球幻觉。解决这一问题需要改进解码策略。

Method: 利用因果掩码建立多模态令牌之间的信息传播，设计注意力寄存器结构和位置感知编码方法，动态分配注意力以抑制异常令牌的影响。

Result: FarSight在图像和视频基准测试中显著减少了幻觉问题，验证了其有效性。

Conclusion: 通过优化令牌传播和注意力分配，FarSight能够有效缓解多模态模型中的幻觉问题。

Abstract: Recent advancements in multimodal large language models (MLLMs) have
significantly improved performance in visual question answering. However, they
often suffer from hallucinations. In this work, hallucinations are categorized
into two main types: initial hallucinations and snowball hallucinations. We
argue that adequate contextual information can be extracted directly from the
token interaction process. Inspired by causal inference in the decoding
strategy, we propose to leverage causal masks to establish information
propagation between multimodal tokens. The hypothesis is that insufficient
interaction between those tokens may lead the model to rely on outlier tokens,
overlooking dense and rich contextual cues. Therefore, we propose to intervene
in the propagation process by tackling outlier tokens to enhance in-context
inference. With this goal, we present FarSight, a versatile plug-and-play
decoding strategy to reduce attention interference from outlier tokens merely
by optimizing the causal mask. The heart of our method is effective token
propagation. We design an attention register structure within the upper
triangular matrix of the causal mask, dynamically allocating attention to
capture attention diverted to outlier tokens. Moreover, a positional awareness
encoding method with a diminishing masking rate is proposed, allowing the model
to attend to further preceding tokens, especially for video sequence tasks.
With extensive experiments, FarSight demonstrates significant
hallucination-mitigating performance across different MLLMs on both image and
video benchmarks, proving its effectiveness.

</details>


### [405] [Robust Vision-Based Runway Detection through Conformal Prediction and Conformal mAP](https://arxiv.org/abs/2505.16740)
*Alya Zouzou,Léo andéol,Mélanie Ducoffe,Ryma Boumazouza*

Key words: Conformal Prediction, YOLOv5, YOLOv6, runway detection, aerospace

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 使用 Conformal Prediction 为基于视觉的着陆系统（VLS）中的跑道检测提供统计不确定性保证，提出新指标 C-mAP，并通过实验证明其能提高可靠性。

Motivation: 提高视觉着陆系统中跑道检测的可靠性，为航空航天领域机器学习系统的认证提供支持。

Method: 使用微调的 YOLOv5 和 YOLOv6 模型，结合 Conformal Prediction 量化定位可靠性，并引入新指标 C-mAP。

Result: Conformal Prediction 能通过统计有效的方式量化不确定性，提高跑道检测的可靠性。

Conclusion: 该方法为机器学习在航空航天领域的应用提供了更安全的解决方案，并支持系统认证。

Abstract: We explore the use of conformal prediction to provide statistical uncertainty
guarantees for runway detection in vision-based landing systems (VLS). Using
fine-tuned YOLOv5 and YOLOv6 models on aerial imagery, we apply conformal
prediction to quantify localization reliability under user-defined risk levels.
We also introduce Conformal mean Average Precision (C-mAP), a novel metric
aligning object detection performance with conformal guarantees. Our results
show that conformal prediction can improve the reliability of runway detection
by quantifying uncertainty in a statistically sound way, increasing safety
on-board and paving the way for certification of ML system in the aerospace
domain.

</details>


### [406] [T2I-ConBench: Text-to-Image Benchmark for Continual Post-training](https://arxiv.org/abs/2505.16875)
*Zhehao Huang,Yuhang Liu,Yixin Lou,Zhengbao He,Mingzhen He,Wenxing Zhou,Tao Li,Kehan Li,Zeyi Huang,Xiaolin Huang*

Key words: 文本到图像模型, 持续后训练, 评估基准, 跨任务泛化

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文介绍了T2I-ConBench，一个用于文本到图像模型持续后训练的统一基准，评估了十种方法的性能，发现尚无方法在所有方面都表现出色。

Motivation: 解决现有文本到图像模型持续后训练缺乏标准化评估协议的问题。

Method: 提出T2I-ConBench基准，结合自动化指标、人类偏好建模和视觉语言问答进行综合评估。

Result: 十种方法在三种实际任务序列上表现不一，跨任务泛化问题仍未解决。

Conclusion: 该研究为文本到图像模型的持续后训练提供了标准化评估工具和数据集，有助于推动相关研究。

Abstract: Continual post-training adapts a single text-to-image diffusion model to
learn new tasks without incurring the cost of separate models, but naive
post-training causes forgetting of pretrained knowledge and undermines
zero-shot compositionality. We observe that the absence of a standardized
evaluation protocol hampers related research for continual post-training. To
address this, we introduce T2I-ConBench, a unified benchmark for continual
post-training of text-to-image models. T2I-ConBench focuses on two practical
scenarios, item customization and domain enhancement, and analyzes four
dimensions: (1) retention of generality, (2) target-task performance, (3)
catastrophic forgetting, and (4) cross-task generalization. It combines
automated metrics, human-preference modeling, and vision-language QA for
comprehensive assessment. We benchmark ten representative methods across three
realistic task sequences and find that no approach excels on all fronts. Even
joint "oracle" training does not succeed for every task, and cross-task
generalization remains unsolved. We release all datasets, code, and evaluation
tools to accelerate research in continual post-training for text-to-image
models.

</details>


### [407] [Mitigating Overfitting in Medical Imaging: Self-Supervised Pretraining vs. ImageNet Transfer Learning for Dermatological Diagnosis](https://arxiv.org/abs/2505.16773)
*Iván Matas,Carmen Serrano,Miguel Nogales,David Moreno,Lara Ferrándiz,Teresa Ojeda,Begoña Acha*

Key words: 无监督学习,变分自编码器,医学影像,自监督学习,ImageNet

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种无监督学习框架，通过变分自编码器（VAE）从皮肤病数据集中提取特定领域特征，比基于ImageNet的预训练模型表现出更强的泛化能力和适应性。

Motivation: 为了解决深度学习中预训练模型在医学图像领域因依赖自然图像数据集（如ImageNet）而可能忽略特定领域特征的问题，作者提出了一种无监督学习框架，专注于皮肤病数据的特征提取。

Method: 使用变分自编码器（VAE）在专有皮肤病数据集上从头训练，学习结构化且临床相关的潜在空间，并将其与ImageNet预训练模型在相同分类条件下进行比较。

Result: 自监督模型在验证损失（-33.33%）和准确性（+44.44%）方面表现优于ImageNet预训练模型，且过拟合情况更少。后者虽收敛快，但过拟合问题严重（+0.060）。

Conclusion: 自监督学习在医学图像领域中展现出更稳定的改进、更强的泛化能力和适应性，凸显了特定领域特征提取的重要性。

Abstract: Deep learning has transformed computer vision but relies heavily on large
labeled datasets and computational resources. Transfer learning, particularly
fine-tuning pretrained models, offers a practical alternative; however, models
pretrained on natural image datasets such as ImageNet may fail to capture
domain-specific characteristics in medical imaging. This study introduces an
unsupervised learning framework that extracts high-value dermatological
features instead of relying solely on ImageNet-based pretraining. We employ a
Variational Autoencoder (VAE) trained from scratch on a proprietary
dermatological dataset, allowing the model to learn a structured and clinically
relevant latent space. This self-supervised feature extractor is then compared
to an ImageNet-pretrained backbone under identical classification conditions,
highlighting the trade-offs between general-purpose and domain-specific
pretraining. Our results reveal distinct learning patterns. The self-supervised
model achieves a final validation loss of 0.110 (-33.33%), while the
ImageNet-pretrained model stagnates at 0.100 (-16.67%), indicating overfitting.
Accuracy trends confirm this: the self-supervised model improves from 45% to
65% (+44.44%) with a near-zero overfitting gap, whereas the ImageNet-pretrained
model reaches 87% (+50.00%) but plateaus at 75% (+19.05%), with its overfitting
gap increasing to +0.060. These findings suggest that while ImageNet
pretraining accelerates convergence, it also amplifies overfitting on
non-clinically relevant features. In contrast, self-supervised learning
achieves steady improvements, stronger generalization, and superior
adaptability, underscoring the importance of domain-specific feature extraction
in medical imaging.

</details>


### [408] [REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training](https://arxiv.org/abs/2505.16792)
*Ziqiao Wang,Wangbo Zhao,Yuhao Zhou,Zekai Li,Zhiyuan Liang,Mingjia Shi,Xuanlei Zhao,Pengfei Zhou,Kaipeng Zhang,Zhangyang Wang,Kai Wang,Yang You*

Key words: DiTs, HASTE, 训练加速, 生成模型, 对齐损失

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: HASTE是一种两阶段训练方法，通过早期对齐损失加速DiTs的训练，随后终止对齐以充分发挥其生成能力，显著提升训练效率。

Motivation: 为解决DiTs训练速度慢及REPA方法在后期表现不佳的问题，研究提出了HASTE方法。

Method: HASTE采用两阶段策略：第一阶段通过整体对齐损失从教师模型中蒸馏注意力图和特征投影；第二阶段在达到触发条件后终止对齐损失。

Result: HASTE显著加速了DiTs的训练，在ImageNet和MS-COCO任务中均表现出色。

Conclusion: HASTE是一种简单且高效的DiTs训练方法，适用于多种任务。

Abstract: Diffusion Transformers (DiTs) deliver state-of-the-art image quality, yet
their training remains notoriously slow. A recent remedy -- representation
alignment (REPA) that matches DiT hidden features to those of a non-generative
teacher (e.g. DINO) -- dramatically accelerates the early epochs but plateaus
or even degrades performance later. We trace this failure to a capacity
mismatch: once the generative student begins modelling the joint data
distribution, the teacher's lower-dimensional embeddings and attention patterns
become a straitjacket rather than a guide. We then introduce HASTE (Holistic
Alignment with Stage-wise Termination for Efficient training), a two-phase
schedule that keeps the help and drops the hindrance. Phase I applies a
holistic alignment loss that simultaneously distills attention maps (relational
priors) and feature projections (semantic anchors) from the teacher into
mid-level layers of the DiT, yielding rapid convergence. Phase II then performs
one-shot termination that deactivates the alignment loss, once a simple trigger
such as a fixed iteration is hit, freeing the DiT to focus on denoising and
exploit its generative capacity. HASTE speeds up training of diverse DiTs
without architecture changes. On ImageNet 256X256, it reaches the vanilla
SiT-XL/2 baseline FID in 50 epochs and matches REPA's best FID in 500 epochs,
amounting to a 28X reduction in optimization steps. HASTE also improves
text-to-image DiTs on MS-COCO, demonstrating to be a simple yet principled
recipe for efficient diffusion training across various tasks. Our code is
available at https://github.com/NUS-HPC-AI-Lab/HASTE .

</details>


### [409] [Efficient Correlation Volume Sampling for Ultra-High-Resolution Optical Flow Estimation](https://arxiv.org/abs/2505.16942)
*Karlis Martins Briedis,Markus Gross,Christopher Schroers*

Key words: 光流估计, 全对相关体积, 内存效率, 高分辨率

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种高效的全对相关体积采样实现，显著降低了内存使用和计算时间，同时保持了高性能。

Motivation: 现有光流估计方法因采用密集全对相关体积的高计算和内存复杂度，常需降低分辨率处理，导致细节丢失。

Method: 提出了一种更高效的全对相关体积采样实现，与RAFT定义的精确数学算子匹配。

Result: 在内存使用降低95%的同时，性能与默认实现相当，推理时间节省50%。

Conclusion: 该方法在高分辨率下实现了最先进的精度和效率。

Abstract: Recent optical flow estimation methods often employ local cost sampling from
a dense all-pairs correlation volume. This results in quadratic computational
and memory complexity in the number of pixels. Although an alternative
memory-efficient implementation with on-demand cost computation exists, this is
slower in practice and therefore prior methods typically process images at
reduced resolutions, missing fine-grained details.
  To address this, we propose a more efficient implementation of the all-pairs
correlation volume sampling, still matching the exact mathematical operator as
defined by RAFT. Our approach outperforms on-demand sampling by up to 90% while
maintaining low memory usage, and performs on par with the default
implementation with up to 95% lower memory usage. As cost sampling makes up a
significant portion of the overall runtime, this can translate to up to 50%
savings for the total end-to-end model inference in memory-constrained
environments. Our evaluation of existing methods includes an 8K
ultra-high-resolution dataset and an additional inference-time modification of
the recent SEA-RAFT method. With this, we achieve state-of-the-art results at
high resolutions both in accuracy and efficiency.

</details>


### [410] [Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning](https://arxiv.org/abs/2505.16836)
*Fanrui Zhang,Dian Li,Qiang Zhang,Chenjun,sinbadliu,Junxiong Lin,Jiahong Yan,Jiawei Liu,Zheng-Jun Zha*

Key words: 虚假信息检测,多模态学习,强化学习,深度推理,视频理解

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了FakeVV数据集和Fact-R1框架，用于解决视频多模态虚假信息检测的问题，通过三阶段训练实现深度推理与规则强化学习的结合。

Motivation: 社交媒体上多模态虚假信息的快速传播引发担忧，但现有方法因缺乏大规模多样数据集和深度推理能力而受限。

Method: 提出Fact-R1框架，包括三阶段训练：长链思维指令调优、直接偏好优化对齐、基于可验证奖励的组相对策略优化。

Result: Fact-R1在复杂多模态虚假信息场景中展现出与先进文本强化学习系统相当的推理能力。

Conclusion: 该研究为虚假信息检测建立了新范式，结合大规模视频理解、推理引导对齐和可解释验证。

Abstract: The rapid spread of multimodal misinformation on social media has raised
growing concerns, while research on video misinformation detection remains
limited due to the lack of large-scale, diverse datasets. Existing methods
often overfit to rigid templates and lack deep reasoning over deceptive
content. To address these challenges, we introduce FakeVV, a large-scale
benchmark comprising over 100,000 video-text pairs with fine-grained,
interpretable annotations. In addition, we further propose Fact-R1, a novel
framework that integrates deep reasoning with collaborative rule-based
reinforcement learning. Fact-R1 is trained through a three-stage process: (1)
misinformation long-Chain-of-Thought (CoT) instruction tuning, (2) preference
alignment via Direct Preference Optimization (DPO), and (3) Group Relative
Policy Optimization (GRPO) using a novel verifiable reward function. This
enables Fact-R1 to exhibit emergent reasoning behaviors comparable to those
observed in advanced text-based reinforcement learning systems, but in the more
complex multimodal misinformation setting. Our work establishes a new paradigm
for misinformation detection, bridging large-scale video understanding,
reasoning-guided alignment, and interpretable verification.

</details>


### [411] [Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation](https://arxiv.org/abs/2505.16985)
*Moru Liu,Hao Dong,Jessica Kelly,Olga Fink,Mario Trapp*

Key words: OOD检测, 多模态, Feature Mixing, CARLA-OOD

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种名为Feature Mixing的简单快速多模态异常合成方法，用于提升OOD检测和分割性能，并通过CARLA-OOD数据集验证其效果。

Motivation: 现实应用中的多模态数据需要更好的OOD检测方法，但缺乏未知数据的监督信号导致模型在OOD样本上过度自信。

Method: 提出Feature Mixing方法，通过多模态异常合成优化模型对ID和OOD数据的区分能力。

Result: 在多个数据集上，Feature Mixing实现了最先进的性能，速度提升了10到370倍。

Conclusion: Feature Mixing是一种高效且通用的方法，适用于多种模态组合，显著提升了OOD检测性能。

Abstract: Out-of-distribution (OOD) detection and segmentation are crucial for
deploying machine learning models in safety-critical applications such as
autonomous driving and robot-assisted surgery. While prior research has
primarily focused on unimodal image data, real-world applications are
inherently multimodal, requiring the integration of multiple modalities for
improved OOD detection. A key challenge is the lack of supervision signals from
unknown data, leading to overconfident predictions on OOD samples. To address
this challenge, we propose Feature Mixing, an extremely simple and fast method
for multimodal outlier synthesis with theoretical support, which can be further
optimized to help the model better distinguish between in-distribution (ID) and
OOD data. Feature Mixing is modality-agnostic and applicable to various
modality combinations. Additionally, we introduce CARLA-OOD, a novel multimodal
dataset for OOD segmentation, featuring synthetic OOD objects across diverse
scenes and weather conditions. Extensive experiments on SemanticKITTI,
nuScenes, CARLA-OOD datasets, and the MultiOOD benchmark demonstrate that
Feature Mixing achieves state-of-the-art performance with a $10 \times$ to $370
\times$ speedup. Our source code and dataset will be available at
https://github.com/mona4399/FeatureMixing.

</details>


### [412] [Native Segmentation Vision Transformers](https://arxiv.org/abs/2505.16993)
*Guillem Brasó,Aljoša Ošep,Laura Leal-Taixé*

Key words: Native Segmentation, Vision Transformer, content-aware grouping, zero-shot segmentation

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种基于内容感知空间分组的Native Segmentation Vision Transformer，无需额外分割头即可生成强分割掩码，支持零样本学习和高效下游任务设计。

Motivation: 传统均匀下采样是视觉主干的默认方法，但缺乏对图像边界和语义内容的动态适应，因此探索一种更优的替代方案。

Method: 通过内容感知空间分组层动态分配标记到精简集合，堆叠分组层实现层次分割，形成Native Segmentation Vision Transformer。

Result: 仅通过分组层即可生成强分割掩码，无需额外分割头，支持零样本学习和高效下游任务设计。

Conclusion: 为原生、主干级分割的新范式奠定了基础，无需掩码监督即可实现强零样本结果。

Abstract: Uniform downsampling remains the de facto standard for reducing spatial
resolution in vision backbones. In this work, we propose an alternative design
built around a content-aware spatial grouping layer, that dynamically assigns
tokens to a reduced set based on image boundaries and their semantic content.
Stacking our grouping layer across consecutive backbone stages results in
hierarchical segmentation that arises natively in the feature extraction
process, resulting in our coined Native Segmentation Vision Transformer. We
show that a careful design of our architecture enables the emergence of strong
segmentation masks solely from grouping layers, that is, without additional
segmentation-specific heads. This sets the foundation for a new paradigm of
native, backbone-level segmentation, which enables strong zero-shot results
without mask supervision, as well as a minimal and efficient standalone model
design for downstream segmentation tasks. Our project page is
https://research.nvidia.com/labs/dvl/projects/native-segmentation.

</details>


### [413] [DetailMaster: Can Your Text-to-Image Model Handle Long Prompts?](https://arxiv.org/abs/2505.16915)
*Qirui Jiao,Daoyuan Chen,Yilun Huang,Xika Lin,Ying Shen,Yaliang Li*

Key words: text-to-image, benchmark, DetailMaster, evaluation, long prompts

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 这篇论文提出了DetailMaster基准测试，专门用于评估文本到图像（T2I）模型处理复杂长文本提示的能力。通过四个关键维度测试，发现现有模型在属性绑定和空间推理等方面表现有限，性能随着提示长度增加而下降。

Motivation: 现有的T2I模型在处理长且复杂的文本提示时表现不佳，无法满足专业应用的需求。DetailMaster的提出填补了这一领域缺乏系统性评估工具的空白。

Method: DetailBenchmark设计了包含四个关键评估维度的测试：角色属性、结构化角色位置、多维度场景属性和显式空间/交互关系。评测数据集包含平均284.89个标记的高质量提示，并由专家验证。

Result: 评测结果显示，最先进的T2I模型在属性绑定和空间推理等关键维度上准确率仅为~50%，且所有模型在提示长度增加时性能均下降。

Conclusion: DetailBenchmark揭示了T2I模型在结构理解和细节过载处理方面的系统性缺陷，为未来研究提出了增强组合推理能力的架构方向。论文开源了数据集及相关工具。

Abstract: While recent text-to-image (T2I) models show impressive capabilities in
synthesizing images from brief descriptions, their performance significantly
degrades when confronted with long, detail-intensive prompts required in
professional applications. We present DetailMaster, the first comprehensive
benchmark specifically designed to evaluate T2I models' systematical abilities
to handle extended textual inputs that contain complex compositional
requirements. Our benchmark introduces four critical evaluation dimensions:
Character Attributes, Structured Character Locations, Multi-Dimensional Scene
Attributes, and Explicit Spatial/Interactive Relationships. The benchmark
comprises long and detail-rich prompts averaging 284.89 tokens, with high
quality validated by expert annotators. Evaluation on 7 general-purpose and 5
long-prompt-optimized T2I models reveals critical performance limitations:
state-of-the-art models achieve merely ~50% accuracy in key dimensions like
attribute binding and spatial reasoning, while all models showing progressive
performance degradation as prompt length increases. Our analysis highlights
systemic failures in structural comprehension and detail overload handling,
motivating future research into architectures with enhanced compositional
reasoning. We open-source the dataset, data curation code, and evaluation tools
to advance detail-rich T2I generation and enable broad applications that would
otherwise be infeasible due to the lack of a dedicated benchmark.

</details>


### [414] [PAEFF: Precise Alignment and Enhanced Gated Feature Fusion for Face-Voice Association](https://arxiv.org/abs/2505.17002)
*Abdul Hannan,Muhammad Arslan Manzoor,Shah Nawaz,Muhammad Irzam Liaqat,Markus Schedl,Mubashir Noman*

Key words: 多模态学习, 人脸-声音关联, 嵌入空间对齐, 门控融合, VoxCeleb

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出一种改进的多模态人脸-声音关联方法，通过对齐嵌入空间和增强的门控融合提升性能。

Motivation: 现有方法在负样本挖掘和依赖远程边际参数方面存在问题，需改进嵌入空间的融合方式。

Method: 学习一个联合嵌入空间，应用正交约束，并在融合前对齐人脸和声音的嵌入空间，使用增强门控融合技术。

Result: 在VoxCeleb数据集上的实验显示该方法性能显著提升。

Conclusion: 提出的方法有效解决了嵌入空间对齐和融合的问题，提高了人脸-声音关联的准确性。

Abstract: We study the task of learning association between faces and voices, which is
gaining interest in the multimodal community lately. These methods suffer from
the deliberate crafting of negative mining procedures as well as the reliance
on the distant margin parameter. These issues are addressed by learning a joint
embedding space in which orthogonality constraints are applied to the fused
embeddings of faces and voices. However, embedding spaces of faces and voices
possess different characteristics and require spaces to be aligned before
fusing them. To this end, we propose a method that accurately aligns the
embedding spaces and fuses them with an enhanced gated fusion thereby improving
the performance of face-voice association. Extensive experiments on the
VoxCeleb dataset reveals the merits of the proposed approach.

</details>


### [415] [SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding](https://arxiv.org/abs/2505.17012)
*Haoning Wu,Xiao Huang,Yaohui Chen,Ya Zhang,Yanfeng Wang,Weidi Xie*

Key words: 多模态大语言模型、3D空间感知、VGBench、SpatialScore、SpatialAgent

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文研究了多模态大语言模型（MLLMs）的3D空间感知与理解能力，提出了VGBench和SpatialScore两大基准测试，并开发了SpatialAgent多智能体系统。实验揭示了空间推理的挑战，并验证了SpatialAgent的有效性。

Motivation: 尽管多模态大语言模型在问答任务中表现优异，但其在3D空间感知与理解方面的能力尚未充分探索。论文旨在填补这一空白，评估MLLMs的空间理解能力。

Method: 论文提出了VGBench基准和综合的SpatialScore基准，包含28K样本和SpatialScore-Hard子集。同时，开发了SpatialAgent多智能体系统，支持Plan-Execute和ReAct推理范式。

Result: 实验结果显示，当前MLLMs在空间推理中仍面临挑战，但SpatialAgent表现出显著效果。

Conclusion: SpatialScore作为一个严谨的基准，将为MLLMs的下一阶段发展提供有价值的见解。

Abstract: Multimodal large language models (MLLMs) have achieved impressive success in
question-answering tasks, yet their capabilities for spatial understanding are
less explored. This work investigates a critical question: do existing MLLMs
possess 3D spatial perception and understanding abilities? Concretely, we make
the following contributions in this paper: (i) we introduce VGBench, a
benchmark specifically designed to assess MLLMs for visual geometry perception,
e.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most
comprehensive and diverse multimodal spatial understanding benchmark to date,
integrating VGBench with relevant data from the other 11 existing datasets.
This benchmark comprises 28K samples across various spatial understanding
tasks, modalities, and QA formats, along with a carefully curated challenging
subset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent
system incorporating 9 specialized tools for spatial understanding, supporting
both Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive
evaluations to reveal persistent challenges in spatial reasoning while
demonstrating the effectiveness of SpatialAgent. We believe SpatialScore will
offer valuable insights and serve as a rigorous benchmark for the next
evolution of MLLMs.

</details>


### [416] [Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework](https://arxiv.org/abs/2505.17019)
*Chenhao Zhang,Yazhe Niu*

Key words: 图像隐喻理解，多模态大语言模型，视觉-语言推理，LAD框架，上下文缺失

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出Let Androids Dream (LAD)框架，通过三阶段（感知、搜索、推理）解决图像隐喻理解的上下文缺失问题，在英汉基准测试中表现优异，尤其在中英文任务上超越多模态大模型。

Motivation: 当前多模态大语言模型（MLLMs）在视觉问答任务中表现优异，但在图像隐喻理解任务中因上下文缺失而受限。受人类认知过程启发，作者希望提升AI对图像隐含意义的理解能力。

Method: 提出LAD三阶段框架：1) 感知阶段将视觉信息转为多层级文本表示；2) 搜索阶段通过跨域知识整合消除歧义；3) 推理阶段生成上下文对齐的图像隐含意义。采用轻量级GPT-4o-mini模型实现。

Result: LAD在英语图像隐喻基准测试中超越15+MLLMs，中文任务显著提升，在MCQ任务性能接近GPT-4o，OSQ任务表现优于36.7%。

Conclusion: LAD为AI理解图像隐喻提供了新思路，推动了视觉-语言推理与人机交互领域发展。开源项目已发布。

Abstract: Metaphorical comprehension in images remains a critical challenge for AI
systems, as existing models struggle to grasp the nuanced cultural, emotional,
and contextual implications embedded in visual content. While multimodal large
language models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they
struggle with a fundamental limitation on image implication tasks: contextual
gaps that obscure the relationships between different visual elements and their
abstract meanings. Inspired by the human cognitive process, we propose Let
Androids Dream (LAD), a novel framework for image implication understanding and
reasoning. LAD addresses contextual missing through the three-stage framework:
(1) Perception: converting visual information into rich and multi-level textual
representations, (2) Search: iteratively searching and integrating cross-domain
knowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment
image implication via explicit reasoning. Our framework with the lightweight
GPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English
image implication benchmark and a huge improvement on Chinese benchmark,
performing comparable with the GPT-4o model on Multiple-Choice Question (MCQ)
and outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work
provides new insights into how AI can more effectively interpret image
implications, advancing the field of vision-language reasoning and human-AI
interaction. Our project is publicly available at
https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [417] [Smaller, Smarter, Closer: The Edge of Collaborative Generative AI](https://arxiv.org/abs/2505.16499)
*Roberto Morabito,SiYoung Jang*

Key words: 生成式AI, 大型语言模型, 小型语言模型, 边缘计算, 云计算, 协作推理

<details>
  <summary>Details</summary>

Main category: cs.DC

TL;DR: 该论文探讨了结合边缘与云资源的协作推理系统，以解决生成式AI（如大型语言模型）在延迟、成本和隐私方面的局限性，并提供了实际设计原则与实验洞察。

Motivation: 生成式AI（尤其是大型语言模型）的云中心部署存在延迟、成本和隐私问题，而小型语言模型在边缘环境中的能力有限，因此需要协作推理系统来弥补这些不足。

Method: 论文提出了结合边缘与云资源的协作推理策略，并设计了实际原则和实验验证。

Result: 研究展示了协作推理系统在生成式AI部署中的有效性，平衡了性能和资源限制。

Conclusion: 通过边缘与云资源的协作，可以优化生成式AI的部署，解决延迟、成本和隐私问题。

Abstract: The rapid adoption of generative AI (GenAI), particularly Large Language
Models (LLMs), has exposed critical limitations of cloud-centric deployments,
including latency, cost, and privacy concerns. Meanwhile, Small Language Models
(SLMs) are emerging as viable alternatives for resource-constrained edge
environments, though they often lack the capabilities of their larger
counterparts. This article explores the potential of collaborative inference
systems that leverage both edge and cloud resources to address these
challenges. By presenting distinct cooperation strategies alongside practical
design principles and experimental insights, we offer actionable guidance for
deploying GenAI across the computing continuum.

</details>


### [418] [Edge-First Language Model Inference: Models, Metrics, and Tradeoffs](https://arxiv.org/abs/2505.16508)
*SiYoung Jang,Roberto Morabito*

Key words: 小型语言模型, 边缘计算, 云计算, 模型压缩, 自适应系统

<details>
  <summary>Details</summary>

Main category: cs.DC

TL;DR: 论文探讨了小型语言模型（SLMs）在边缘计算中的部署优势及与云计算的互补关系，提供了设计高效自适应系统的见解。

Motivation: 随着语言模型（LMs）在各行业的广泛应用，部署到从云到边缘的计算连续体以降低成本、延迟并提升可靠性和隐私性成为研究热点。

Method: 通过详细基准测试评估SLMs在单一边缘设备及分布式边缘集群上的性能，分析边缘与云部署的优劣。

Result: 研究发现边缘推理在某些场景下表现优异且成本更低，而在扩展性或模型能力受限时需依赖云回退。

Conclusion: 提出平台级比较和设计建议，支持构建高效自适应的异构环境LM推理系统。

Abstract: The widespread adoption of Language Models (LMs) across industries is driving
interest in deploying these services across the computing continuum, from the
cloud to the network edge. This shift aims to reduce costs, lower latency, and
improve reliability and privacy. Small Language Models (SLMs), enabled by
advances in model compression, are central to this shift, offering a path to
on-device inference on resource-constrained edge platforms. This work examines
the interplay between edge and cloud deployments, starting from detailed
benchmarking of SLM capabilities on single edge devices, and extending to
distributed edge clusters. We identify scenarios where edge inference offers
comparable performance with lower costs, and others where cloud fallback
becomes essential due to limits in scalability or model capacity. Rather than
proposing a one-size-fits-all solution, we present platform-level comparisons
and design insights for building efficient, adaptive LM inference systems
across heterogeneous environments.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [419] [The Computational Complexity of Counting Linear Regions in ReLU Neural Networks](https://arxiv.org/abs/2505.16716)
*Moritz Stargalla,Christoph Hertrich,Daniel Reichman*

Key words: ReLU neural networks, linear regions, computational complexity, NP-hardness, #P-hardness

<details>
  <summary>Details</summary>

Main category: cs.CC

TL;DR: This paper explores different definitions of linear regions in ReLU neural networks, analyzes their computational complexity, and proves NP- and #P-hardness for counting them, while also providing polynomial-space algorithmic solutions for some definitions.

Motivation: To clarify and compare various non-equivalent definitions of linear regions in ReLU neural networks, assess their computational complexity, and understand the challenges in counting these regions.

Method: Systematically review definitions used in existing literature, analyze their relationships, and investigate computational complexity, including proving NP- and #P-hardness for counting regions in networks with one or more hidden layers.

Result: Found that counting linear regions is generally intractable (NP- and #P-hard), especially for networks with multiple hidden layers, but polynomial-space algorithms exist for some definitions.

Conclusion: The study highlights the complexity of counting linear regions in ReLU networks, with hardness results even for simple architectures, but offers algorithmic insights for certain cases.

Abstract: An established measure of the expressive power of a given ReLU neural network
is the number of linear regions into which it partitions the input space. There
exist many different, non-equivalent definitions of what a linear region
actually is. We systematically assess which papers use which definitions and
discuss how they relate to each other. We then analyze the computational
complexity of counting the number of such regions for the various definitions.
Generally, this turns out to be an intractable problem. We prove NP- and
#P-hardness results already for networks with one hidden layer and strong
hardness of approximation results for two or more hidden layers. Finally, on
the algorithmic side, we demonstrate that counting linear regions can at least
be achieved in polynomial space for some common definitions.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [420] [Benchmarking Chest X-ray Diagnosis Models Across Multinational Datasets](https://arxiv.org/abs/2505.16027)
*Qinmei Xu,Yiheng Li,Xianghao Zhan,Ahmet Gorkem Er,Brittany Dashevsky,Chuanjun Xu,Mohammed Alawad,Mengya Yang,Liu Ya,Changsheng Zhou,Xiao Li,Haruka Itakura,Olivier Gevaert*

Key words: 胸部X光片诊断、基础模型、视觉语言预训练、卷积神经网络、多国数据集

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 该研究比较了基于视觉语言预训练的基础模型与传统CNN在胸部X光片诊断上的表现。基础模型在准确性和任务覆盖范围上均优于CNN，特别是MAVL模型在公开和私有数据集上表现最佳。但所有模型在儿科病例上表现较差，提示未来需要进一步优化。

Motivation: 尽管基于视觉语言预训练的基础模型在胸部X光片诊断中显示出潜力，但其在多样化人群和诊断任务中的实际性能尚未充分评估，本研究旨在填补这一空白。

Method: 研究评估了8个CXR诊断模型（5个基础模型和3个CNN架构），在37个标准化分类任务上使用6个公共数据集和3个私有数据集，性能通过AUROC、AUPRC等指标衡量。

Result: 基础模型在准确性和任务覆盖范围上优于CNN，MAVL模型在公开和私有数据集上表现最佳（公开数据集平均AUROC: 0.82；私有数据集平均AUROC: 0.95），但所有模型在儿科病例上表现显著下降（成人AUROC: 0.88，儿童AUROC: 0.57）。

Conclusion: 研究强调了结构化监督和提示设计在放射学AI中的价值，并提出了未来研究方向，包括地理扩展和集成模型用于临床部署。

Abstract: Foundation models leveraging vision-language pretraining have shown promise
in chest X-ray (CXR) interpretation, yet their real-world performance across
diverse populations and diagnostic tasks remains insufficiently evaluated. This
study benchmarks the diagnostic performance and generalizability of foundation
models versus traditional convolutional neural networks (CNNs) on multinational
CXR datasets. We evaluated eight CXR diagnostic models - five vision-language
foundation models and three CNN-based architectures - across 37 standardized
classification tasks using six public datasets from the USA, Spain, India, and
Vietnam, and three private datasets from hospitals in China. Performance was
assessed using AUROC, AUPRC, and other metrics across both shared and
dataset-specific tasks. Foundation models outperformed CNNs in both accuracy
and task coverage. MAVL, a model incorporating knowledge-enhanced prompts and
structured supervision, achieved the highest performance on public (mean AUROC:
0.82; AUPRC: 0.32) and private (mean AUROC: 0.95; AUPRC: 0.89) datasets,
ranking first in 14 of 37 public and 3 of 4 private tasks. All models showed
reduced performance on pediatric cases, with average AUROC dropping from 0.88
+/- 0.18 in adults to 0.57 +/- 0.29 in children (p = 0.0202). These findings
highlight the value of structured supervision and prompt design in radiologic
AI and suggest future directions including geographic expansion and ensemble
modeling for clinical deployment. Code for all evaluated models is available at
https://drive.google.com/drive/folders/1B99yMQm7bB4h1sVMIBja0RfUu8gLktCE

</details>


### [421] [MambaStyle: Efficient StyleGAN Inversion for Real Image Editing with State-Space Models](https://arxiv.org/abs/2505.15822)
*Jhon Lopez,Carlos Hinojosa,Henry Arguello,Bernard Ghanem*

Key words: GAN反演, 图像编辑, 视觉状态空间模型, 计算效率, 实时应用

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: MambaStyle是一种基于单阶段编码器的高效GAN反演和编辑方法，通过整合视觉状态空间模型（VSSMs），在保持高质量图像重建和灵活编辑的同时，显著减少了参数数量和计算复杂性。

Motivation: 现有的GAN反演方法在重建质量、可编辑性和计算效率之间难以平衡，MambaStyle旨在解决这一问题。

Method: 提出一种基于视觉状态空间模型（VSSMs）的单阶段编码器架构，用于高效图像反演和编辑。

Result: 实验表明，MambaStyle在反演精度、编辑质量和计算效率方面实现了优越的平衡，且模型复杂度更低、推理速度更快。

Conclusion: MambaStyle适用于实时应用，具有高效、高质量的图像反演和编辑能力。

Abstract: The task of inverting real images into StyleGAN's latent space to manipulate
their attributes has been extensively studied. However, existing GAN inversion
methods struggle to balance high reconstruction quality, effective editability,
and computational efficiency. In this paper, we introduce MambaStyle, an
efficient single-stage encoder-based approach for GAN inversion and editing
that leverages vision state-space models (VSSMs) to address these challenges.
Specifically, our approach integrates VSSMs within the proposed architecture,
enabling high-quality image inversion and flexible editing with significantly
fewer parameters and reduced computational complexity compared to
state-of-the-art methods. Extensive experiments show that MambaStyle achieves a
superior balance among inversion accuracy, editing quality, and computational
efficiency. Notably, our method achieves superior inversion and editing results
with reduced model complexity and faster inference, making it suitable for
real-time applications.

</details>


### [422] [Diffusion Probabilistic Generative Models for Accelerated, in-NICU Permanent Magnet Neonatal MRI](https://arxiv.org/abs/2505.15984)
*Yamin Arefeen,Brett Levac,Bhairav Patel,Chang Ho,Jonathan I. Tamir*

Key words: MRI, 新生儿, 扩散概率生成模型, 加速扫描

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 该研究利用扩散概率生成模型加速新生儿重症监护病房（NICU）中的MRI扫描，通过改进训练流程应对数据量少和信噪比低的挑战。

Motivation: 在NICU中使用永久磁体扫描仪进行新生儿MRI扫描时间较长，需要提高效率。

Method: 建立1特斯拉新生儿MRI数据集，改进网络架构以支持不同分辨率，通过自监督去噪和平均后验样本重建图像。

Result: 结合所有数据、去噪预训练和后验样本平均，定量改进重建效果，生成模型无需重新训练即可实现加速。

Conclusion: 扩散概率生成模型结合改进的训练流程可有效缩短NICU新生儿MRI的扫描时间。

Abstract: Purpose: Magnetic Resonance Imaging (MRI) enables non-invasive assessment of
brain abnormalities during early life development. Permanent magnet scanners
operating in the neonatal intensive care unit (NICU) facilitate MRI of sick
infants, but have long scan times due to lower signal-to-noise ratios (SNR) and
limited receive coils. This work accelerates in-NICU MRI with diffusion
probabilistic generative models by developing a training pipeline accounting
for these challenges.
  Methods: We establish a novel training dataset of clinical, 1 Tesla neonatal
MR images in collaboration with Aspect Imaging and Sha'are Zedek Medical
Center. We propose a pipeline to handle the low quantity and SNR of our
real-world dataset (1) modifying existing network architectures to support
varying resolutions; (2) training a single model on all data with learned class
embedding vectors; (3) applying self-supervised denoising before training; and
(4) reconstructing by averaging posterior samples. Retrospective under-sampling
experiments, accounting for signal decay, evaluated each item of our proposed
methodology. A clinical reader study with practicing pediatric
neuroradiologists evaluated our proposed images reconstructed from 1.5x
under-sampled data.
  Results: Combining all data, denoising pre-training, and averaging posterior
samples yields quantitative improvements in reconstruction. The generative
model decouples the learned prior from the measurement model and functions at
two acceleration rates without re-training. The reader study suggests that
proposed images reconstructed from approximately 1.5x under-sampled data are
adequate for clinical use.
  Conclusion: Diffusion probabilistic generative models applied with the
proposed pipeline to handle challenging real-world datasets could reduce scan
time of in-NICU neonatal MRI.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [423] [CoT Information: Improved Sample Complexity under Chain-of-Thought Supervision](https://arxiv.org/abs/2505.15927)
*Awni Altabaa,Omar Montasser,John Lafferty*

Key words: 链式思维（CoT）、统计学习理论、样本复杂度、信息论、端到端学习

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了一个统计理论来分析链式思维（CoT）监督下的学习效果，证明CoT监督能显著提高学习效率。

Motivation: 传统的输入-输出监督学习在处理多步推理复杂函数时效果有限，而CoT监督提供了中间推理步骤和最终输出，显著提升了大型语言模型的推理能力。本文旨在建立CoT监督下的学习统计理论。

Method: 通过定义CoT信息度量 $\mathcal{I}_{	ext{CoT}}$，量化观察推理过程带来的判别能力提升，并显式链接CoT风险（训练目标）和端到端风险（测试目标），以获得更严格的样本复杂度边界。

Result: 主要理论结果表明，CoT监督可以显著提高学习速率，样本复杂度仅需 $d/	ext{CoT信息}$，远优于传统 $d/	ext{误差}$ 的速率。

Conclusion: CoT信息是学习链式思维监督下统计复杂度的基本度量。

Abstract: Learning complex functions that involve multi-step reasoning poses a
significant challenge for standard supervised learning from input-output
examples. Chain-of-thought (CoT) supervision, which provides intermediate
reasoning steps together with the final output, has emerged as a powerful
empirical technique, underpinning much of the recent progress in the reasoning
capabilities of large language models. This paper develops a statistical theory
of learning under CoT supervision. A key characteristic of the CoT setting, in
contrast to standard supervision, is the mismatch between the training
objective (CoT risk) and the test objective (end-to-end risk). A central part
of our analysis, distinguished from prior work, is explicitly linking those two
types of risk to achieve sharper sample complexity bounds. This is achieved via
the *CoT information measure* $\mathcal{I}_{\mathcal{D},
h_\star}^{\mathrm{CoT}}(\epsilon; \calH)$, which quantifies the additional
discriminative power gained from observing the reasoning process. The main
theoretical results demonstrate how CoT supervision can yield significantly
faster learning rates compared to standard E2E supervision. Specifically, it is
shown that the sample complexity required to achieve a target E2E error
$\epsilon$ scales as $d/\mathcal{I}_{\mathcal{D},
h_\star}^{\mathrm{CoT}}(\epsilon; \calH)$, where $d$ is a measure of hypothesis
class complexity, which can be much faster than standard $d/\epsilon$ rates.
Information-theoretic lower bounds in terms of the CoT information are also
obtained. Together, these results suggest that CoT information is a fundamental
measure of statistical complexity for learning under chain-of-thought
supervision.

</details>


### [424] [PO-Flow: Flow-based Generative Models for Sampling Potential Outcomes and Counterfactuals](https://arxiv.org/abs/2505.16051)
*Dongze Wu,David I. Inouye,Yao Xie*

Key words: 因果推断, 连续归一化流, 反事实预测, 密度学习

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: PO-Flow 是一种新颖的连续归一化流框架，用于因果推断，联合建模潜在结果和反事实，无需显式分布假设。

Motivation: 解决因果推断中潜在结果和反事实预测的问题，提供统一且灵活的模型框架。

Method: 通过流匹配训练，支持个性化潜在结果预测、反事实预测和不确定性感知密度学习。

Result: 在 ACIC、IHDP 和 IBM 等基准测试中优于现有方法，并在高维场景（如图像生成）中表现优异。

Conclusion: PO-Flow 是一种广泛适用的因果推断工具，适用于多样化任务。

Abstract: We propose PO-Flow, a novel continuous normalizing flow (CNF) framework for
causal inference that jointly models potential outcomes and counterfactuals.
Trained via flow matching, PO-Flow provides a unified framework for
individualized potential outcome prediction, counterfactual predictions, and
uncertainty-aware density learning. Among generative models, it is the first to
enable density learning of potential outcomes without requiring explicit
distributional assumptions (e.g., Gaussian mixtures), while also supporting
counterfactual prediction conditioned on factual outcomes in general
observational datasets. On benchmarks such as ACIC, IHDP, and IBM, it
consistently outperforms prior methods across a range of causal inference
tasks. Beyond that, PO-Flow succeeds in high-dimensional settings, including
counterfactual image generation, demonstrating its broad applicability.

</details>


### [425] [Oh SnapMMD! Forecasting Stochastic Dynamics Beyond the Schrödinger Bridge's End](https://arxiv.org/abs/2505.16082)
*Renato Berlinghieri,Yunyi Shen,Jialong Jiang,Tamara Broderick*

Key words: 时间点数据，动态系统学习，最大均值差异，预报，插值

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出了一种名为SnapMMD的新框架，用于从时间点数据中学习和预测潜在随机动力学，克服了现有方法在预报中的局限性。

Motivation: 科学家常需根据时间点数据预测超出观测范围的行为，但现有方法在预报精度上存在不足。

Method: 采用最大均值差异（MMD）损失直接拟合状态测量和观测时间的联合分布，学习动态系统。

Result: 实验表明，SnapMMD在真实和合成数据中均能提供准确的预测，且在插值和速度场重构方面表现优异。

Conclusion: SnapMMD是一种有效的动态系统学习方法，特别适用于状态依赖性波动和不完整数据的情况。

Abstract: Scientists often want to make predictions beyond the observed time horizon of
"snapshot" data following latent stochastic dynamics. For example, in time
course single-cell mRNA profiling, scientists have access to cellular
transcriptional state measurements (snapshots) from different biological
replicates at different time points, but they cannot access the trajectory of
any one cell because measurement destroys the cell. Researchers want to
forecast (e.g.) differentiation outcomes from early state measurements of stem
cells. Recent Schr\"odinger-bridge (SB) methods are natural for interpolating
between snapshots. But past SB papers have not addressed forecasting -- likely
since existing methods either (1) reduce to following pre-set reference
dynamics (chosen before seeing data) or (2) require the user to choose a fixed,
state-independent volatility since they minimize a Kullback-Leibler divergence.
Either case can lead to poor forecasting quality. In the present work, we
propose a new framework, SnapMMD, that learns dynamics by directly fitting the
joint distribution of both state measurements and observation time with a
maximum mean discrepancy (MMD) loss. Unlike past work, our method allows us to
infer unknown and state-dependent volatilities from the observed data. We show
in a variety of real and synthetic experiments that our method delivers
accurate forecasts. Moreover, our approach allows us to learn in the presence
of incomplete state measurements and yields an $R^2$-style statistic that
diagnoses fit. We also find that our method's performance at interpolation (and
general velocity-field reconstruction) is at least as good as (and often better
than) state-of-the-art in almost all of our experiments.

</details>


### [426] [Dimension-adapted Momentum Outscales SGD](https://arxiv.org/abs/2505.16098)
*Damien Ferbach,Katie Everett,Gauthier Gidel,Elliot Paquette,Courtney Paquette*

Key words: 随机动量算法, 缩放规律, DANA, SGD-M, 数据复杂性

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 研究了在小批量条件下随机动量算法的缩放规律，分析了数据复杂性、目标复杂性和模型大小的影响，发现DANA（维度适应的Nesterov加速）比传统方法表现更优。

Motivation: 探索随机动量算法在不同数据-目标复杂性下的表现，并比较传统SGD-M与DANA在缩放规律上的差异。

Method: 通过理论分析和实验验证，对比SGD-M和DANA在不同数据复杂性下的损失曲线形状和缩放行为。

Result: DANA在广泛的数据和目标复杂性范围内优于传统方法，改善了计算最优的缩放行为。

Conclusion: DANA通过适应模型大小和数据复杂性调整动量超参数，显著提升了性能，且在大规模实验中验证了其优势。

Abstract: We investigate scaling laws for stochastic momentum algorithms with small
batch on the power law random features model, parameterized by data complexity,
target complexity, and model size. When trained with a stochastic momentum
algorithm, our analysis reveals four distinct loss curve shapes determined by
varying data-target complexities. While traditional stochastic gradient descent
with momentum (SGD-M) yields identical scaling law exponents to SGD,
dimension-adapted Nesterov acceleration (DANA) improves these exponents by
scaling momentum hyperparameters based on model size and data complexity. This
outscaling phenomenon, which also improves compute-optimal scaling behavior, is
achieved by DANA across a broad range of data and target complexities, while
traditional methods fall short. Extensive experiments on high-dimensional
synthetic quadratics validate our theoretical predictions and large-scale text
experiments with LSTMs show DANA's improved loss exponents over SGD hold in a
practical setting.

</details>


### [427] [Exponential Convergence of CAVI for Bayesian PCA](https://arxiv.org/abs/2505.16145)
*Arghya Datta,Philippe Gagnon,Florian Maire*

Key words: 贝叶斯主成分分析,变分推断,收敛速度,KL散度

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文填补了贝叶斯主成分分析（BPCA）中坐标上升变分推断（CAVI）算法收敛速度的理论空白，证明了在单一主成分情况下CAVI的指数收敛性，并扩展到了多主成分情形。

Motivation: 传统概率PCA和贝叶斯PCA在降维中广泛应用，但CAVI算法在BPCA中的收敛速度尚未被理论化研究，本文旨在填补这一空白。

Method: 通过连接经典幂迭代算法，证明单一主成分下CAVI的指数收敛性，并利用新工具扩展到多主成分情形，同时提出了一种新的多元正态分布对称KL散度下界。

Result: 证明了CAVI在单一主成分下的精确指数收敛性，并扩展到多主成分情形。新的KL散度下界对信息论有独立价值。

Conclusion: 本文为BPCA中CAVI算法的收敛性提供了理论支撑，并提出了一个对信息论有贡献的新工具。

Abstract: Probabilistic principal component analysis (PCA) and its Bayesian variant
(BPCA) are widely used for dimension reduction in machine learning and
statistics. The main advantage of probabilistic PCA over the traditional
formulation is allowing uncertainty quantification. The parameters of BPCA are
typically learned using mean-field variational inference, and in particular,
the coordinate ascent variational inference (CAVI) algorithm. So far, the
convergence speed of CAVI for BPCA has not been characterized. In our paper, we
fill this gap in the literature. Firstly, we prove a precise exponential
convergence result in the case where the model uses a single principal
component (PC). Interestingly, this result is established through a connection
with the classical $\textit{power iteration algorithm}$ and it indicates that
traditional PCA is retrieved as points estimates of the BPCA parameters.
Secondly, we leverage recent tools to prove exponential convergence of CAVI for
the model with any number of PCs, thus leading to a more general result, but
one that is of a slightly different flavor. To prove the latter result, we
additionally needed to introduce a novel lower bound for the symmetric
Kullback--Leibler divergence between two multivariate normal distributions,
which, we believe, is of independent interest in information theory.

</details>


### [428] [Integral Imprecise Probability Metrics](https://arxiv.org/abs/2505.16156)
*Siu Lun Chau,Michele Caprio,Krikamol Muandet*

Key words: 认知不确定性,不精确概率,容量,Choqut积分,IIPM

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了基于Choqut积分的IIPM框架，用于量化不同IP模型的差异及计算认知不确定性，理论证明了其有效性，并通过实验验证了性能优势。

Motivation: 经典概率模型无法充分表征认知不确定性（如模糊性和部分信念），而IP理论提供了更丰富的模型。因此需要一种新方法来度量和比较IP模型。

Method: 基于Choqut积分提出IIPM框架，将其推广到容量（IP模型）的度量，并提出新的认知不确定性指标MMI。

Result: IIPM不仅能跨模型比较，还可量化单一模型的认知不确定性。实验显示MMI在选择性分类任务中优于现有方法，尤其在大规模类别的场景下。

Conclusion: IIPM为IPML提供了理论和实践基础，支持在不确定性下的量化与比较，推动了IPML的发展。

Abstract: Quantifying differences between probability distributions is fundamental to
statistics and machine learning, primarily for comparing statistical
uncertainty. In contrast, epistemic uncertainty (EU) -- due to incomplete
knowledge -- requires richer representations than those offered by classical
probability. Imprecise probability (IP) theory offers such models, capturing
ambiguity and partial belief. This has driven growing interest in imprecise
probabilistic machine learning (IPML), where inference and decision-making rely
on broader uncertainty models -- highlighting the need for metrics beyond
classical probability. This work introduces the Integral Imprecise Probability
Metric (IIPM) framework, a Choquet integral-based generalisation of classical
Integral Probability Metric (IPM) to the setting of capacities -- a broad class
of IP models encompassing many existing ones, including lower probabilities,
probability intervals, belief functions, and more. Theoretically, we establish
conditions under which IIPM serves as a valid metric and metrises a form of
weak convergence of capacities. Practically, IIPM not only enables comparison
across different IP models but also supports the quantification of epistemic
uncertainty within a single IP model. In particular, by comparing an IP model
with its conjugate, IIPM gives rise to a new class of EU measures -- Maximum
Mean Imprecision -- which satisfy key axiomatic properties proposed in the
Uncertainty Quantification literature. We validate MMI through selective
classification experiments, demonstrating strong empirical performance against
established EU measures, and outperforming them when classical methods struggle
to scale to a large number of classes. Our work advances both theory and
practice in IPML, offering a principled framework for comparing and quantifying
epistemic uncertainty under imprecision.

</details>


### [429] [Generalized Power Priors for Improved Bayesian Inference with Historical Data](https://arxiv.org/abs/2505.16244)
*Masanari Kimura,Howard Bondell*

Key words: 幂先验,贝叶斯框架,$α$-散度,几何解释

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文扩展了贝叶斯框架中的幂先验，通过引入Amari的$α$-散度来改进性能，并提供了理论分析和几何解释。

Motivation: 旨在通过推广传统的KL散度方法，使历史数据在当前分析中更具适应性和灵活性。

Method: 使用Amari的$α$-散度作为KL散度的推广，构建广义幂后验，并分析其在概率分布流形上的几何性质。

Result: 广义幂后验在性能上有所改进，且能适应不同的$α$参数选择。

Conclusion: 广义幂后验为贝叶斯分析提供了更灵活的工具，尤其是在历史数据和当前数据的整合上表现出优越性。

Abstract: The power prior is a class of informative priors designed to incorporate
historical data alongside current data in a Bayesian framework. It includes a
power parameter that controls the influence of historical data, providing
flexibility and adaptability. A key property of the power prior is that the
resulting posterior minimizes a linear combination of KL divergences between
two pseudo-posterior distributions: one ignoring historical data and the other
fully incorporating it. We extend this framework by identifying the posterior
distribution as the minimizer of a linear combination of Amari's
$\alpha$-divergence, a generalization of KL divergence. We show that this
generalization can lead to improved performance by allowing for the data to
adapt to appropriate choices of the $\alpha$ parameter. Theoretical properties
of this generalized power posterior are established, including behavior as a
generalized geodesic on the Riemannian manifold of probability distributions,
offering novel insights into its geometric interpretation.

</details>


### [430] [Graph-Smoothed Bayesian Black-Box Shift Estimator and Its Information Geometry](https://arxiv.org/abs/2505.16251)
*Masanari Kimura*

Key words: label shift adaptation, Bayesian inference, graph smoothing, confusion matrix, robustness

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: GS-B³SE通过图平滑贝叶斯方法优化类别分布估计，解决传统黑盒平移估计器的脆弱性问题，提供更稳健的解决方案。

Motivation: 传统黑盒平移估计器通过反演分类器的混淆矩阵得到的点估计脆弱且容易受采样噪声和类别相似性影响，需要一种更稳健的概率性替代方法。

Method: 提出了GS-B³SE，通过在图结构上施加拉普拉斯-高斯先验对目标类别先验和混淆矩阵列进行平滑，利用HMC或块牛顿-CG方案进行后验推断。

Result: 证明了可识别性、N^{-1/2}收缩性，方差随图的代数连通性减小，且对拉普拉斯矩阵的误设具有鲁棒性。

Conclusion: GS-B³SE不仅泛化了现有平移估计器，还通过信息几何视角为其提供了新解释，展现了其优越性。

Abstract: Label shift adaptation aims to recover target class priors when the labelled
source distribution $P$ and the unlabelled target distribution $Q$ share $P(X
\mid Y) = Q(X \mid Y)$ but $P(Y) \neq Q(Y)$. Classical black-box shift
estimators invert an empirical confusion matrix of a frozen classifier,
producing a brittle point estimate that ignores sampling noise and similarity
among classes. We present Graph-Smoothed Bayesian BBSE (GS-B$^3$SE), a fully
probabilistic alternative that places Laplacian-Gaussian priors on both target
log-priors and confusion-matrix columns, tying them together on a
label-similarity graph. The resulting posterior is tractable with HMC or a fast
block Newton-CG scheme. We prove identifiability, $N^{-1/2}$ contraction,
variance bounds that shrink with the graph's algebraic connectivity, and
robustness to Laplacian misspecification. We also reinterpret GS-B$^3$SE
through information geometry, showing that it generalizes existing shift
estimators.

</details>


### [431] [Higher-Order Asymptotics of Test-Time Adaptation for Batch Normalization Statistics](https://arxiv.org/abs/2505.16257)
*Masanari Kimura*

Key words: 批归一化, 测试时适应, Edgeworth展开, 鞍点近似, M估计

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文开发了一种高阶渐近框架，通过整合Edgeworth展开和鞍点近似技术，结合一步M估计视角，优化批归一化在分布偏移下的测试时适应性能。

Motivation: 研究旨在解决批归一化在测试时适应分布偏移时的统计差异问题，通过更高阶的校正提升其可靠性和性能。

Method: 结合Edgeworth展开、鞍点近似和一步M估计，分析BN均值的归一化差异，并优化权重参数以最小化均方误差。

Result: 提出了一个高阶局部渐近正态性结果，量化了偏差、方差和偏度之间的权衡，并建立了模型风险的泛化界限。

Conclusion: 通过高阶校正和一步稳健更新，显著提升了BN层在适应变化数据分布时的可靠性和性能。

Abstract: This study develops a higher-order asymptotic framework for test-time
adaptation (TTA) of Batch Normalization (BN) statistics under distribution
shift by integrating classical Edgeworth expansion and saddlepoint
approximation techniques with a novel one-step M-estimation perspective. By
analyzing the statistical discrepancy between training and test distributions,
we derive an Edgeworth expansion for the normalized difference in BN means and
obtain an optimal weighting parameter that minimizes the mean-squared error of
the adapted statistic. Reinterpreting BN TTA as a one-step M-estimator allows
us to derive higher-order local asymptotic normality results, which incorporate
skewness and other higher moments into the estimator's behavior. Moreover, we
quantify the trade-offs among bias, variance, and skewness in the adaptation
process and establish a corresponding generalization bound on the model risk.
The refined saddlepoint approximations further deliver uniformly accurate
density and tail probability estimates for the BN TTA statistic. These
theoretical insights provide a comprehensive understanding of how higher-order
corrections and robust one-step updating can enhance the reliability and
performance of BN layers in adapting to changing data distributions.

</details>


### [432] [Generator-Mediated Bandits: Thompson Sampling for GenAI-Powered Adaptive Interventions](https://arxiv.org/abs/2505.16311)
*Marc Brooks,Gabriel Durham,Kihyuk Hong,Ambuj Tewari*

Key words: 生成式人工智能, 个性化决策, 强盗算法, 汤普森采样, 移动健康干预

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出了一种名为GAMBITTS的生成模型中介的强盗算法，用于解决生成式人工智能在个性化决策系统中引入的新结构问题，并通过模拟实验验证其优越性。

Motivation: 随着生成式人工智能（GenAI）的发展，个性化内容生成需要新的决策系统框架，传统强盗算法未考虑生成模型的随机响应特性。

Method: 提出GAMBITTS算法，通过显式建模处理和奖励生成过程，利用生成模型的输出加速策略学习。

Result: GAMBITTS在模拟实验中优于传统强盗算法，并通过理论分析明确了其优越性的条件。

Conclusion: GAMBITTS为GenAI驱动的干预提供了高效的决策框架，具有理论和实践优势。

Abstract: Recent advances in generative artificial intelligence (GenAI) models have
enabled the generation of personalized content that adapts to up-to-date user
context. While personalized decision systems are often modeled using bandit
formulations, the integration of GenAI introduces new structure into otherwise
classical sequential learning problems. In GenAI-powered interventions, the
agent selects a query, but the environment experiences a stochastic response
drawn from the generative model. Standard bandit methods do not explicitly
account for this structure, where actions influence rewards only through
stochastic, observed treatments. We introduce generator-mediated
bandit-Thompson sampling (GAMBITTS), a bandit approach designed for this
action/treatment split, using mobile health interventions with large language
model-generated text as a motivating case study. GAMBITTS explicitly models
both the treatment and reward generation processes, using information in the
delivered treatment to accelerate policy learning relative to standard methods.
We establish regret bounds for GAMBITTS by decomposing sources of uncertainty
in treatment and reward, identifying conditions where it achieves stronger
guarantees than standard bandit approaches. In simulation studies, GAMBITTS
consistently outperforms conventional algorithms by leveraging observed
treatments to more accurately estimate expected rewards.

</details>


### [433] [Better Rates for Private Linear Regression in the Proportional Regime via Aggressive Clipping](https://arxiv.org/abs/2505.16329)
*Simone Bombari,Inbar Seroussi,Marco Mondelli*

Key words: 差分隐私, 线性回归, DP-SGD, ODE模型, 剪裁策略

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文分析了差分隐私（DP）线性回归中DP-SGD的性能优化，提出通过频繁剪裁和动态学习率调度来提高误差率。

Motivation: 理论研究与实证结果在DP线性回归中存在差异，本文旨在弥合这一差距。

Method: 基于多元高斯数据，通过建立DP-SGD轨迹的确定性等效ODE模型，分析剪裁和学习率调度的影响。

Result: 证明了频繁剪裁的优越性，并揭示了动态学习率和噪声调度的重要性。

Conclusion: 在高维数据下，激进剪裁和动态调度策略能显著提升DP-SGD性能。

Abstract: Differentially private (DP) linear regression has received significant
attention in the recent theoretical literature, with several works aimed at
obtaining improved error rates. A common approach is to set the clipping
constant much larger than the expected norm of the per-sample gradients. While
simplifying the analysis, this is however in sharp contrast with what empirical
evidence suggests to optimize performance. Our work bridges this gap between
theory and practice: we provide sharper rates for DP stochastic gradient
descent (DP-SGD) by crucially operating in a regime where clipping happens
frequently. Specifically, we consider the setting where the data is
multivariate Gaussian, the number of training samples $n$ is proportional to
the input dimension $d$, and the algorithm guarantees constant-order zero
concentrated DP. Our method relies on establishing a deterministic equivalent
for the trajectory of DP-SGD in terms of a family of ordinary differential
equations (ODEs). As a consequence, the risk of DP-SGD is bounded between two
ODEs, with upper and lower bounds matching for isotropic data. By studying
these ODEs when $n / d$ is large enough, we demonstrate the optimality of
aggressive clipping, and we uncover the benefits of decaying learning rate and
private noise scheduling.

</details>


### [434] [Learning non-equilibrium diffusions with Schrödinger bridges: from exactly solvable to simulation-free](https://arxiv.org/abs/2505.16644)
*Stephen Y. Zhang,Michael P H Stumpf*

Key words: Schrödinger桥, 非平衡系统, Ornstein-Uhlenbeck过程, 单细胞数据, mvOU-OTFM

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文研究了Schrödinger桥问题，提出了一种基于多元Ornstein-Uhlenbeck过程的非平衡系统方法，并在高斯边际情况下给出了显式解。对于一般边际，提出了mvOU-OTFM算法，在合成和真实单细胞数据中表现优越。

Motivation: 传统Schrödinger桥问题通常假设布朗运动参考动力学，局限于势驱动动力学。本文旨在扩展到非平衡系统，尤其是具有非保守力的生物系统。

Method: 针对高斯边际，推导了静态和动态Schrödinger桥的显式解；针对一般边际，提出了基于流和分数匹配的模拟自由算法mvOU-OTFM。

Result: 在合成和真实单细胞数据中，mvOU-OTFM相比其他方法准确率更高且训练速度更快。

Conclusion: 提出的方法为非平衡系统Schrödinger桥问题提供了高效解决方案，尤其在生物系统中具有应用潜力。

Abstract: We consider the Schr\"odinger bridge problem which, given ensemble
measurements of the initial and final configurations of a stochastic dynamical
system and some prior knowledge on the dynamics, aims to reconstruct the "most
likely" evolution of the system compatible with the data. Most existing
literature assume Brownian reference dynamics and are implicitly limited to
potential-driven dynamics. We depart from this regime and consider reference
processes described by a multivariate Ornstein-Uhlenbeck process with generic
drift matrix $\mathbf{A} \in \mathbb{R}^{d \times d}$. When $\mathbf{A}$ is
asymmetric, this corresponds to a non-equilibrium system with non-conservative
forces at play: this is important for applications to biological systems, which
are naturally exist out-of-equilibrium. In the case of Gaussian marginals, we
derive explicit expressions that characterise the solution of both the static
and dynamic Schr\"odinger bridge. For general marginals, we propose mvOU-OTFM,
a simulation-free algorithm based on flow and score matching for learning the
Schr\"odinger bridge. In application to a range of problems based on synthetic
and real single cell data, we demonstrate that mvOU-OTFM achieves higher
accuracy compared to competing methods, whilst being significantly faster to
train.

</details>


### [435] [Sharp concentration of uniform generalization errors in binary linear classification](https://arxiv.org/abs/2505.16713)
*Shogo Nakakita*

Key words: 二元线性分类, Poincaré不等式, 对数Sobolev不等式, 泛化误差, 集中界

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文通过等周论证研究了二元线性分类问题中均匀泛化误差的集中性，建立了Poincaré和对数Sobolev不等式，并推导了集中界。结果表明在渐进分析中，均匀泛化误差几乎必然收敛于其期望。

Motivation: 研究二元线性分类中均匀泛化误差的集中性，旨在推导其理论界限并展示其在广泛条件下的收敛性。

Method: 使用等周论证，建立Poincaré和对数Sobolev不等式，分析标签加权输入向量的联合分布。

Result: 集中界在适度乘法常数下保持尖锐，且在渐进分析中，均匀泛化误差几乎必然收敛于期望。

Conclusion: 该研究为二元线性分类提供了理论支持，展示了均匀泛化误差在广泛条件下的收敛性。

Abstract: We examine the concentration of uniform generalization errors around their
expectation in binary linear classification problems via an isoperimetric
argument. In particular, we establish Poincar\'{e} and log-Sobolev inequalities
for the joint distribution of the output labels and the label-weighted input
vectors, which we apply to derive concentration bounds. The derived
concentration bounds are sharp up to moderate multiplicative constants by those
under well-balanced labels. In asymptotic analysis, we also show that almost
sure convergence of uniform generalization errors to their expectation occurs
in very broad settings, such as proportionally high-dimensional regimes. Using
this convergence, we establish uniform laws of large numbers under
dimension-free conditions.

</details>


### [436] [How high is `high'? Rethinking the roles of dimensionality in topological data analysis and manifold learning](https://arxiv.org/abs/2505.16879)
*Hannah Sansford,Nick Whiteley,Patrick Rubin-Delanchy*

Key words: Hanson-Wright不等式, 数据几何, 持续图, 流形结构, 网格细胞

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文提出广义Hanson-Wright不等式，并用于分析数据点云的几何结构，揭示了三种维度概念的作用。研究证明，当$p_{\mathrm{int}} \gg \log n$时，持续图能显示潜在同源性且流形结构显现。通过理论分析，首次发现神经科学中网格细胞活动的环面结构与物理空间是等距的。

Motivation: 为了理解数据点云的几何结构，并探讨三种维度概念在随机函数模型中的作用。

Method: 提出广义Hanson-Wright不等式，并分析数据点云的几何性质，验证持续图和流形结构的显现条件。

Result: 当$p_{\mathrm{int}} \gg \log n$时，持续图能揭示潜在同源性且流形结构显现。神经科学中的网格细胞活动环面结构与物理空间是等距的。

Conclusion: 通过理论分析，揭示了数据点云的几何结构和网格细胞活动的物理意义。

Abstract: We present a generalised Hanson-Wright inequality and use it to establish new
statistical insights into the geometry of data point-clouds. In the setting of
a general random function model of data, we clarify the roles played by three
notions of dimensionality: ambient intrinsic dimension $p_{\mathrm{int}}$,
which measures total variability across orthogonal feature directions;
correlation rank, which measures functional complexity across samples; and
latent intrinsic dimension, which is the dimension of manifold structure hidden
in data. Our analysis shows that in order for persistence diagrams to reveal
latent homology and for manifold structure to emerge it is sufficient that
$p_{\mathrm{int}}\gg \log n$, where $n$ is the sample size. Informed by these
theoretical perspectives, we revisit the ground-breaking neuroscience discovery
of toroidal structure in grid-cell activity made by Gardner et al. (Nature,
2022): our findings reveal, for the first time, evidence that this structure is
in fact isometric to physical space, meaning that grid cell activity conveys a
geometrically faithful representation of the real world.

</details>


### [437] [Statistical Test for Saliency Maps of Graph Neural Networks via Selective Inference](https://arxiv.org/abs/2505.16893)
*Shuichi Nishino,Tomohiro Shiraishi,Teruyuki Katsuoka,Ichiro Takeuchi*

Key words: 图神经网络, 显著性图, 统计测试, 选择性推断, Type I错误率

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了一种统计测试框架，用于严格评估GNN显著性图的可靠性，解决了因数据重复使用导致的Type I错误率膨胀问题，并通过实验验证了其有效性。

Motivation: GNN显著性图的可靠性受到质疑，尤其是在面对噪声时。为了解决这一问题，作者希望通过统计测试框架来验证显著性图的有效性。

Method: 利用选择性推断（Selective Inference）框架，提出了一种统计测试方法，生成统计有效的p值并控制Type I错误率，确保识别的显著子图具有实际意义而非随机噪声。

Result: 在合成和真实数据集上的实验表明，该方法能有效评估GNN解释的可靠性，并控制Type I错误率。

Conclusion: 研究提出的方法为GNN显著性图的可靠性提供了统计验证的工具，增强了其解释的信任度。

Abstract: Graph Neural Networks (GNNs) have gained prominence for their ability to
process graph-structured data across various domains. However, interpreting GNN
decisions remains a significant challenge, leading to the adoption of saliency
maps for identifying influential nodes and edges. Despite their utility, the
reliability of GNN saliency maps has been questioned, particularly in terms of
their robustness to noise. In this study, we propose a statistical testing
framework to rigorously evaluate the significance of saliency maps. Our main
contribution lies in addressing the inflation of the Type I error rate caused
by double-dipping of data, leveraging the framework of Selective Inference. Our
method provides statistically valid $p$-values while controlling the Type I
error rate, ensuring that identified salient subgraphs contain meaningful
information rather than random artifacts. To demonstrate the effectiveness of
our method, we conduct experiments on both synthetic and real-world datasets,
showing its effectiveness in assessing the reliability of GNN interpretations.

</details>


### [438] [TULiP: Test-time Uncertainty Estimation via Linearization and Weight Perturbation](https://arxiv.org/abs/2505.16923)
*Yuhui Zhang,Dongshen Wu,Yuichiro Wada,Takafumi Kanamori*

Key words: OOD检测, 不确定性估计, 深度学习, TULiP, 后处理方法

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出了一种基于理论的后处理不确定性估计方法TULiP，用于OOD检测。通过线性化训练动态和参数扰动计算不确定性，在合成和大规模数据集上验证了其有效性，尤其是在近分布样本上表现最佳。

Motivation: 可靠的OOD检测对深度学习模型在开放世界中的安全部署至关重要，现有方法缺乏理论驱动的后处理不确定性估计方法。

Method: 提出TULiP方法，考虑网络收敛前的扰动，基于线性化训练动态计算不确定性得分，并通过参数扰动实现。

Result: 在合成数据集和大规模OOD检测基准测试中表现优异，特别是在近分布样本上达到最先进性能。

Conclusion: TULiP为OOD检测提供了理论支持的、高效的不确定性估计方法。

Abstract: A reliable uncertainty estimation method is the foundation of many modern
out-of-distribution (OOD) detectors, which are critical for safe deployments of
deep learning models in the open world. In this work, we propose TULiP, a
theoretically-driven post-hoc uncertainty estimator for OOD detection. Our
approach considers a hypothetical perturbation applied to the network before
convergence. Based on linearized training dynamics, we bound the effect of such
perturbation, resulting in an uncertainty score computable by perturbing model
parameters. Ultimately, our approach computes uncertainty from a set of sampled
predictions. We visualize our bound on synthetic regression and classification
datasets. Furthermore, we demonstrate the effectiveness of TULiP using
large-scale OOD detection benchmarks for image classification. Our method
exhibits state-of-the-art performance, particularly for near-distribution
samples.

</details>


### [439] [Critical Points of Random Neural Networks](https://arxiv.org/abs/2505.17000)
*Simmaco Di Lillo*

Key words: 随机神经网络, 临界点, 激活函数, 渐近分析

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文研究了无限宽度极限下，随着深度增加，随机神经网络临界点期望数量的变化，揭示了三种不同增长模式。

Motivation: 探究随机神经网络在深度增加时临界点数量的变化，理解不同激活函数对此的影响。

Method: 在特定正则条件下推导临界点固定指数及超过阈值的期望数量的渐近公式，并进行数值实验验证。

Result: 发现临界点数量随深度变化呈现收敛、多项式增长或指数增长三种模式；ReLU激活函数可能导致临界点数量发散。

Conclusion: 理论预测与实验结果一致，揭示了深度神经网络临界点行为的多样性。

Abstract: This work investigates the expected number of critical points of random
neural networks with different activation functions as the depth increases in
the infinite-width limit. Under suitable regularity conditions, we derive
precise asymptotic formulas for the expected number of critical points of fixed
index and those exceeding a given threshold. Our analysis reveals three
distinct regimes depending on the value of the first derivative of the
covariance evaluated at 1: the expected number of critical points may converge,
grow polynomially, or grow exponentially with depth. The theoretical
predictions are supported by numerical experiments. Moreover, we provide
numerical evidence suggesting that, when the regularity condition is not
satisfied (e.g. for neural networks with ReLU as activation function), the
number of critical points increases as the map resolution increases, indicating
a potential divergence in the number of critical points.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [440] [CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark](https://arxiv.org/abs/2505.16968)
*Ahmed Heakl,Sarim Hashmi,Gustavo Bertolo Stahl,Seung Hun Eddie Han,Salman Khan,Abdulrahman Mahmoud*

Key words: GPU代码转译, 数据集, 语言模型, 二进制兼容性

<details>
  <summary>Details</summary>

Main category: cs.AR

TL;DR: 论文介绍了CASS，一个用于跨架构GPU代码转译的大规模数据集和模型套件，支持源级和汇编级翻译，性能超过主流商业模型。

Motivation: 解决GPU代码跨架构移植性的关键问题，填补低级别GPU代码移植工具的空白。

Method: 构建包含70k已验证代码对的数据集，并训练CASS系列领域专用语言模型。

Result: 源翻译准确率达95%，汇编翻译准确率37.5%，生成的代码在85%测试用例中性能接近原生。

Conclusion: CASS显著提升了GPU代码移植性，工具和数据开源以推动相关领域发展。

Abstract: We introduce \texttt{CASS}, the first large-scale dataset and model suite for
cross-architecture GPU code transpilation, targeting both source-level
(CUDA~$\leftrightarrow$~HIP) and assembly-level (Nvidia
SASS~$\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k
verified code pairs across host and device, addressing a critical gap in
low-level GPU code portability. Leveraging this resource, we train the
\texttt{CASS} family of domain-specific language models, achieving 95\% source
translation accuracy and 37.5\% assembly translation accuracy, substantially
outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our
generated code matches native performance in over 85\% of test cases,
preserving runtime and memory behavior. To support rigorous evaluation, we
introduce \texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with
ground-truth execution. All data, models, and evaluation tools are released as
open source to foster progress in GPU compiler tooling, binary compatibility,
and LLM-guided hardware translation. Dataset and benchmark are on
\href{https://huggingface.co/datasets/MBZUAI/cass}{\textcolor{blue}{HuggingFace}},
with code at
\href{https://github.com/GustavoStahl/CASS}{\textcolor{blue}{GitHub}}.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [441] [Sufficient conditions for offline reactivation in recurrent neural networks](https://arxiv.org/abs/2505.17003)
*Nanda H. Krishna,Colin Bredenberg,Daniel Levenstein,Blake A. Richards,Guillaume Lajoie*

Key words: 神经重现、任务优化、噪声循环网络、离线活动、数学建模

<details>
  <summary>Details</summary>

Main category: q-bio.NC

TL;DR: 该研究提出了一个数学框架，阐明在接收变化感官信息的噪声循环网络中，任务优化的网络如何自主重现在线行为期间的网络状态。

Motivation: 探索任务优化网络在静止期（如睡眠）重现在线行为状态的条件，这一现象尚未完全理解。

Method: 通过数学框架证明噪声循环网络在优化跟踪环境状态变量时，会自然发展出去噪动力学，从而在无输入时重现在线活动状态。

Result: 通过数值实验验证了理论，在空间位置估计和头部方向估计两项典型任务中得到支持。

Conclusion: 研究为离线重现现象提供了理论依据，表明其是噪声神经回路中任务优化的自然结果。

Abstract: During periods of quiescence, such as sleep, neural activity in many brain
circuits resembles that observed during periods of task engagement. However,
the precise conditions under which task-optimized networks can autonomously
reactivate the same network states responsible for online behavior is poorly
understood. In this study, we develop a mathematical framework that outlines
sufficient conditions for the emergence of neural reactivation in circuits that
encode features of smoothly varying stimuli. We demonstrate mathematically that
noisy recurrent networks optimized to track environmental state variables using
change-based sensory information naturally develop denoising dynamics, which,
in the absence of input, cause the network to revisit state configurations
observed during periods of online activity. We validate our findings using
numerical experiments on two canonical neuroscience tasks: spatial position
estimation based on self-motion cues, and head direction estimation based on
angular velocity cues. Overall, our work provides theoretical support for
modeling offline reactivation as an emergent consequence of task optimization
in noisy neural circuits.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [442] [Graph Attention Network for Optimal User Association in Wireless Networks](https://arxiv.org/abs/2505.16347)
*Javad Mirzaei,Jeebak Mitra,Gwenael Poitau*

Key words: 5G, 网络能源效率, 用户关联, 图优化, 节能

<details>
  <summary>Details</summary>

Main category: cs.IT

TL;DR: 该论文提出了基于图抽象优化的用户关联策略，以提升5G网络中的能源效率，并在与现有方法的对比中展示了优越性。

Motivation: 5G网络密集化导致能源消耗剧增，亟需提升网络能源效率以降低运营成本。

Method: 通过图抽象优化用户关联策略，动态激活节能功能（如小区关闭）。

Result: 所提方法在节能效果上优于传统方案。

Conclusion: 图形化优化为5G网络能源效率提供了一种有效解决方案。

Abstract: With increased 5G deployments, network densification is higher than ever to
support the exponentially high throughput requirements. However, this has meant
a significant increase in energy consumption, leading to higher operational
expenditure (OpEx) for network operators creating an acute need for
improvements in network energy savings (NES). A key determinant of operational
efficacy in cellular networks is the user association (UA) policy, as it
affects critical aspects like spectral efficiency, load balancing etc. and
therefore impacts the overall energy consumption of the network directly.
Furthermore, with cellular network topologies lending themselves well to
graphical abstractions, use of graphs in network optimization has gained
significant prominence. In this work, we propose and analyze a graphical
abstraction based optimization for UA in cellular networks to improve NES by
determining when energy saving features like cell switch off can be activated.
A comparison with legacy approaches establishes the superiority of the proposed
approach.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [443] [VERDI: VLM-Embedded Reasoning for Autonomous Driving](https://arxiv.org/abs/2505.15925)
*Bowen Feng,Zhiting Mei,Baiang Li,Julian Ost,Roger Girgis,Anirudha Majumdar,Felix Heide*

Key words: 自动驾驶, 视觉语言模型, 推理蒸馏, 端到端学习, NuScenes

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: VERDI框架通过将视觉语言模型（VLM）的推理过程蒸馏到自动驾驶（AD）堆栈中，解决了现有方法参数过多和不安全的问题，提升了性能并保持了推理速度。

Motivation: 现有的大型视觉语言模型在自动驾驶决策中虽然表现良好，但参数巨大、推理速度慢且缺乏安全性分解，因此需要一种更高效且实用的解决方案。

Method: VERDI通过在训练时将VLM的推理和常识知识蒸馏到AD堆栈中，对齐感知、预测和规划阶段的中间模块输出与VLM生成的文本特征，从而在潜在空间实现对齐。

Result: 在NuScenes数据集上，VERDI相比未嵌入推理的端到端方法，性能提升了10%（基于L2距离），同时保持了高推理速度。

Conclusion: VERDI成功地将VLM的推理能力嵌入到AD堆栈中，解决了现有方法的不足，同时提升了性能和实用性。

Abstract: While autonomous driving (AD) stacks struggle with decision making under
partial observability and real-world complexity, human drivers are capable of
commonsense reasoning to make near-optimal decisions with limited information.
Recent work has attempted to leverage finetuned Vision-Language Models (VLMs)
for trajectory planning at inference time to emulate human behavior. Despite
their success in benchmark evaluations, these methods are often impractical to
deploy (a 70B parameter VLM inference at merely 8 tokens per second requires
more than 160G of memory), and their monolithic network structure prohibits
safety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for
autonomous Driving (VERDI), a training-time framework that distills the
reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI
augments modular differentiable end-to-end (e2e) AD models by aligning
intermediate module outputs at the perception, prediction, and planning stages
with text features explaining the driving reasoning process produced by VLMs.
By encouraging alignment in latent space, \textsc{VERDI} enables the modular AD
stack to internalize structured reasoning, without incurring the inference-time
costs of large VLMs. We demonstrate the effectiveness of our method on the
NuScenes dataset and find that VERDI outperforms existing e2e methods that do
not embed reasoning by 10% in $\ell_{2}$ distance, while maintaining high
inference speed.

</details>


### [444] [EasyInsert: A Data-Efficient and Generalizable Insertion Policy](https://arxiv.org/abs/2505.16187)
*Guanghe Li,Junming Zhao,Shengjie Wang,Yang Gao*

Key words: 机器人插入任务, 位姿预测, 零样本泛化, 自动化数据收集

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: EasyInsert提出一种基于相对位姿预测的框架，通过少量真实数据训练实现高泛化能力的插入任务，成功处理未知物体和杂乱环境。

Motivation: 现有方法在泛化性和环境适应性上表现不佳，依赖强假设（如CAD模型）。EasyInsert以人类直觉为基础，利用相对位姿预测提升泛化性。

Method: 采用粗到细的执行策略，通过自动化数据收集训练通用位姿预测模型，减少人工干预。

Result: 仅5小时训练数据，对15种未知物体中的13种实现90%以上零样本插入成功率；单次演示+4分钟微调后全部达到90%以上。

Conclusion: EasyInsert在高效数据利用和零样本泛化性上表现优越，适用于复杂插入任务。

Abstract: Insertion task is highly challenging that requires robots to operate with
exceptional precision in cluttered environments. Existing methods often have
poor generalization capabilities. They typically function in restricted and
structured environments, and frequently fail when the plug and socket are far
apart, when the scene is densely cluttered, or when handling novel objects.
They also rely on strong assumptions such as access to CAD models or a digital
twin in simulation. To address this, we propose EasyInsert, a framework which
leverages the human intuition that relative pose (delta pose) between plug and
socket is sufficient for successful insertion, and employs efficient and
automated real-world data collection with minimal human labor to train a
generalizable model for relative pose prediction. During execution, EasyInsert
follows a coarse-to-fine execution procedure based on predicted delta pose, and
successfully performs various insertion tasks. EasyInsert demonstrates strong
zero-shot generalization capability for unseen objects in cluttered
environments, handling cases with significant initial pose deviations while
maintaining high sample efficiency and requiring little human effort. In
real-world experiments, with just 5 hours of training data, EasyInsert achieves
over 90% success in zero-shot insertion for 13 out of 15 unseen novel objects,
including challenging objects like Type-C cables, HDMI cables, and Ethernet
cables. Furthermore, with only one human demonstration and 4 minutes of
automatically collected data for fine-tuning, it reaches over 90% success rate
for all 15 objects.

</details>


### [445] [SEM: Enhancing Spatial Understanding for Robust Robot Manipulation](https://arxiv.org/abs/2505.16196)
*Xuewu Lin,Tianwei Lin,Lichao Huang,Hongyu Xie,Yiwei Jin,Keyu Li,Zhizhong Su*

Key words: 机器人操作,空间理解,扩散模型,3D几何,本体感知

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 论文提出SEM模型，通过增强空间理解和机器人状态编码，提升机器人操作的鲁棒性和泛化能力。

Motivation: 现有方法在空间理解和语义抽象上表现不足，无法满足复杂机器人操作任务的需求。

Method: 采用扩散式策略框架，结合空间增强器和基于图的机器人状态编码器，提升3D几何理解和机器人本体感知。

Result: SEM模型在多样化任务中表现优于现有基线方法，展现出更强的空间理解和泛化能力。

Conclusion: SEM通过整合空间增强和本体感知，显著提升了机器人操作策略的性能。

Abstract: A key challenge in robot manipulation lies in developing policy models with
strong spatial understanding, the ability to reason about 3D geometry, object
relations, and robot embodiment. Existing methods often fall short: 3D point
cloud models lack semantic abstraction, while 2D image encoders struggle with
spatial reasoning. To address this, we propose SEM (Spatial Enhanced
Manipulation model), a novel diffusion-based policy framework that explicitly
enhances spatial understanding from two complementary perspectives. A spatial
enhancer augments visual representations with 3D geometric context, while a
robot state encoder captures embodiment-aware structure through graphbased
modeling of joint dependencies. By integrating these modules, SEM significantly
improves spatial understanding, leading to robust and generalizable
manipulation across diverse tasks that outperform existing baselines.

</details>


### [446] [Manipulating Elasto-Plastic Objects With 3D Occupancy and Learning-Based Predictive Control](https://arxiv.org/abs/2505.16249)
*Zhen Zhang,Xiangyu Chu,Yunxi Tang,Lulu Zhao,Jing Huang,Zhongliang Jiang,K. W. Samuel Au*

Key words: 弹性-塑性物体,3D占用,深度学习,预测控制,机器人操纵

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 该论文提出了一种基于3D占用表示和学习的弹性-塑性物体操纵框架，利用深度学习预测复杂变形并通过控制算法规划动作，成功实现目标形状塑造。

Motivation: 弹性-塑性物体的操纵因自遮挡、表示困难和复杂动力学而具有挑战性，需开发新方法解决这些问题。

Method: 采用静态假设和3D占用表示，结合3D CNN和GNN的神经网络预测变形，并通过学习型预测控制算法规划机器人动作。

Result: 框架在仿真和现实实验中成功将物体塑造为目标形状，验证了其有效性。

Conclusion: 所提方法有效解决了弹性-塑性物体操纵的挑战，展示了实际应用的潜力。

Abstract: Manipulating elasto-plastic objects remains a significant challenge due to
severe self-occlusion, difficulties of representation, and complicated
dynamics. This work proposes a novel framework for elasto-plastic object
manipulation with a quasi-static assumption for motions, leveraging 3D
occupancy to represent such objects, a learned dynamics model trained with 3D
occupancy, and a learning-based predictive control algorithm to address these
challenges effectively. We build a novel data collection platform to collect
full spatial information and propose a pipeline for generating a 3D occupancy
dataset. To infer the 3D occupancy during manipulation, an occupancy prediction
network is trained with multiple RGB images supervised by the generated
dataset. We design a deep neural network empowered by a 3D convolution neural
network (CNN) and a graph neural network (GNN) to predict the complex
deformation with the inferred 3D occupancy results. A learning-based predictive
control algorithm is introduced to plan the robot actions, incorporating a
novel shape-based action initialization module specifically designed to improve
the planner efficiency. The proposed framework in this paper can successfully
shape the elasto-plastic objects into a given goal shape and has been verified
in various experiments both in simulation and the real world.

</details>


### [447] [VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving](https://arxiv.org/abs/2505.16377)
*Yansong Qu,Zilin Huang,Zihao Sheng,Jiancong Chen,Sikai Chen,Samuel Labi*

Key words: 强化学习, 自动驾驶, 视觉语言模型, 世界模型, 安全学习

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: VL-SAFE提出了一种基于视觉语言模型（VLM）引导的世界模型安全强化学习框架，用于离线安全驾驶策略学习，通过专家数据和安全评分优化策略，显著提升了样本效率、泛化性和安全性。

Motivation: 面向安全关键场景的自动驾驶策略学习中，传统强化学习方法依赖在线交互且样本效率低，现有安全RL方法无法准确捕捉复杂驾驶环境中的“安全”语义，导致行为过度保守或违反约束。

Method: 构建包含专家数据和VLM生成安全评分的离线数据集，训练世界模型生成想象轨迹和安全评估，结合VLM引导的Actor-Critic学习优化策略，实现无环境交互的安全规划。

Result: VL-SAFE在样本效率、泛化性、安全性及整体性能上优于现有基线方法。

Conclusion: 本研究首次将VLM引导与世界模型结合用于自动驾驶安全策略学习，为离线安全RL提供了新思路。

Abstract: Reinforcement learning (RL)-based autonomous driving policy learning faces
critical limitations such as low sample efficiency and poor generalization; its
reliance on online interactions and trial-and-error learning is especially
unacceptable in safety-critical scenarios. Existing methods including safe RL
often fail to capture the true semantic meaning of "safety" in complex driving
contexts, leading to either overly conservative driving behavior or constraint
violations. To address these challenges, we propose VL-SAFE, a world
model-based safe RL framework with Vision-Language model
(VLM)-as-safety-guidance paradigm, designed for offline safe policy learning.
Specifically, we construct offline datasets containing data collected by expert
agents and labeled with safety scores derived from VLMs. A world model is
trained to generate imagined rollouts together with safety estimations,
allowing the agent to perform safe planning without interacting with the real
environment. Based on these imagined trajectories and safety evaluations,
actor-critic learning is conducted under VLM-based safety guidance to optimize
the driving policy more safely and efficiently. Extensive evaluations
demonstrate that VL-SAFE achieves superior sample efficiency, generalization,
safety, and overall performance compared to existing baselines. To the best of
our knowledge, this is the first work that introduces a VLM-guided world
model-based approach for safe autonomous driving. The demo video and code can
be accessed at: https://ys-qu.github.io/vlsafe-website/

</details>


### [448] [Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)](https://arxiv.org/abs/2505.16394)
*Zhenjie Yang,Xiaosong Jia,Qifeng Li,Xue Yang,Maoqing Yao,Junchi Yan*

Key words: 强化学习, 端到端自动驾驶, 模型基强化学习, 特权信息, 原始传感器数据

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: Raw2Drive是一种基于模型强化学习（MBRL）的双流方法，利用特权信息训练辅助世界模型，再通过引导机制训练原始传感器世界模型，最终结合两者的先验知识指导策略训练，在CARLA Leaderboard 2.0上取得了最佳表现。

Motivation: 解决强化学习（RL）在端到端自动驾驶（E2E-AD）中训练困难的问题，现有方法多依赖模仿学习（IL）或特权信息，而Raw2Drive填补了直接处理原始传感器数据的空白。

Method: 分两步：1）用特权信息训练特权世界模型与神经规划器；2）通过引导机制训练原始传感器世界模型，确保与特权模型的一致性，并利用后者知识指导原始传感器策略。

Result: Raw2Drive成为CARLA Leaderboard 2.0上唯一基于RL的端到端方法，达到当前最优性能。

Conclusion: Raw2Drive通过结合特权模型和原始传感器模型，显著提升了RL在E2E-AD中的可行性，为无需特权信息的RL应用提供了新思路。

Abstract: Reinforcement Learning (RL) can mitigate the causal confusion and
distribution shift inherent to imitation learning (IL). However, applying RL to
end-to-end autonomous driving (E2E-AD) remains an open problem for its training
difficulty, and IL is still the mainstream paradigm in both academia and
industry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated
promising results in neural planning; however, these methods typically require
privileged information as input rather than raw sensor data. We fill this gap
by designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently
train an auxiliary privileged world model paired with a neural planner that
uses privileged information as input. Subsequently, we introduce a raw sensor
world model trained via our proposed Guidance Mechanism, which ensures
consistency between the raw sensor world model and the privileged world model
during rollouts. Finally, the raw sensor world model combines the prior
knowledge embedded in the heads of the privileged world model to effectively
guide the training of the raw sensor policy. Raw2Drive is so far the only RL
based end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it
achieves state-of-the-art performance.

</details>


### [449] [Human-like Semantic Navigation for Autonomous Driving using Knowledge Representation and Large Language Models](https://arxiv.org/abs/2505.16498)
*Augusto Luis Ballardini,Miguel Ángel Sotelo*

Key words: 自动驾驶, 大型语言模型, Answer Set Programming, 动态导航, 可解释性

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 该论文探讨了利用大型语言模型将非正式导航指令转化为基于逻辑的ASP规则，以提升自动驾驶在动态环境中的适应性和可解释性。

Motivation: 为了解决自动驾驶在动态城市环境中因过度依赖预定义地图数据而难以适应实时变化的挑战，研究提出利用大型语言模型生成ASP规则，以增强系统的适应性和可解释性。

Method: 研究采用大型语言模型将非正式导航指令转化为Answer Set Programming规则，从而实现语义化决策和动态导航规划。通过实验评估验证了方法的有效性。

Result: 实验表明，LLM驱动的ASP规则生成能够支持基于语义的决策，提供一种与人类导航意图密切匹配的可解释动态导航框架。

Conclusion: 通过将非正式导航指令自动化转化为逻辑规则，该方法显著提升了自动驾驶在动态环境中的适应性和可解释性，为未来研究提供了新方向。

Abstract: Achieving full automation in self-driving vehicles remains a challenge,
especially in dynamic urban environments where navigation requires real-time
adaptability. Existing systems struggle to handle navigation plans when faced
with unpredictable changes in road layouts, spontaneous detours, or missing map
data, due to their heavy reliance on predefined cartographic information. In
this work, we explore the use of Large Language Models to generate Answer Set
Programming rules by translating informal navigation instructions into
structured, logic-based reasoning. ASP provides non-monotonic reasoning,
allowing autonomous vehicles to adapt to evolving scenarios without relying on
predefined maps. We present an experimental evaluation in which LLMs generate
ASP constraints that encode real-world urban driving logic into a formal
knowledge representation. By automating the translation of informal navigation
instructions into logical rules, our method improves adaptability and
explainability in autonomous navigation. Results show that LLM-driven ASP rule
generation supports semantic-based decision-making, offering an explainable
framework for dynamic navigation planning that aligns closely with how humans
communicate navigational intent.

</details>


### [450] [Find the Fruit: Designing a Zero-Shot Sim2Real Deep RL Planner for Occlusion Aware Plant Manipulation](https://arxiv.org/abs/2505.16547)
*Nitesh Subedi,Hsin-Jung Yang,Devesh K. Jha,Soumik Sarkar*

Key words: 深度强化学习、机器人操纵、遮挡感知、农业机器人、仿真到现实迁移

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 本文提出了一种端到端深度强化学习框架，用于在复杂植物环境中进行遮挡感知的机器人操纵。通过多模态观察，机器人能够与可变形植物交互以揭示隐藏目标（如果实），并在真实环境中实现了86.7%的成功率。

Motivation: 农业机器人需要智能地与复杂植物环境交互，以在遮挡严重的场景中找到目标果实。传统方法依赖于显式的几何和动力学建模，而本文旨在通过端到端学习简化这一过程。

Method: 使用深度强化学习框架，将运动学规划与机器人控制解耦，结合多模态观察实现零样本的仿真到现实迁移。

Result: 在真实环境中测试，该策略成功率高达86.7%，适用于多样化的初始条件。

Conclusion: 该框架为自主感知驱动的农业机器人提供了可行方案，无需为每种植物场景显式建模。

Abstract: This paper presents an end-to-end deep reinforcement learning (RL) framework
for occlusion-aware robotic manipulation in cluttered plant environments. Our
approach enables a robot to interact with a deformable plant to reveal hidden
objects of interest, such as fruits, using multimodal observations. We decouple
the kinematic planning problem from robot control to simplify zero-shot
sim2real transfer for the trained policy. Our results demonstrate that the
trained policy, deployed using our framework, achieves up to 86.7% success in
real-world trials across diverse initial conditions. Our findings pave the way
toward autonomous, perception-driven agricultural robots that intelligently
interact with complex foliage plants to "find the fruit" in challenging
occluded scenarios, without the need for explicitly designed geometric and
dynamic models of every plant scenario.

</details>


### [451] [Safe Uncertainty-Aware Learning of Robotic Suturing](https://arxiv.org/abs/2505.16596)
*Wilbert Peter Empleo,Yitaek Kim,Hansoul Kim,Thiusius Rajeeth Savarimuthu,Iñigo Iturrate*

Key words: 机器人手术、集成学习、扩散策略、不确定性感知、安全控制

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 本文提出了一种安全、不确定性感知的学习框架，用于自动化机器人辅助微创手术，通过集成扩散策略模型量化不确定性，并使用控制屏障函数确保行动安全。

Motivation: 自动化机器人手术可减轻外科医生的体力负担和重复任务，但现有AI方法缺乏可解释性和安全保证，因此需开发更可靠的自动化方案。

Method: 使用专家演示训练集成扩散策略模型，量化认知不确定性以识别非常规场景，并结合模型无关的控制屏障函数确保行动安全性。

Result: 在高级缝合模拟器中测试，策略对针掉落、相机移动等扰动表现鲁棒，能检测非常规场景并确保行动不越界。

Conclusion: 该框架在自动化手术中实现了安全性与适应性，为未来临床部署奠定了基础。

Abstract: Robot-Assisted Minimally Invasive Surgery is currently fully manually
controlled by a trained surgeon. Automating this has great potential for
alleviating issues, e.g., physical strain, highly repetitive tasks, and
shortages of trained surgeons. For these reasons, recent works have utilized
Artificial Intelligence methods, which show promising adaptability. Despite
these advances, there is skepticism of these methods because they lack
explainability and robust safety guarantees. This paper presents a framework
for a safe, uncertainty-aware learning method. We train an Ensemble Model of
Diffusion Policies using expert demonstrations of needle insertion. Using an
Ensemble model, we can quantify the policy's epistemic uncertainty, which is
used to determine Out-Of-Distribution scenarios. This allows the system to
release control back to the surgeon in the event of an unsafe scenario.
Additionally, we implement a model-free Control Barrier Function to place
formal safety guarantees on the predicted action. We experimentally evaluate
our proposed framework using a state-of-the-art robotic suturing simulator. We
evaluate multiple scenarios, such as dropping the needle, moving the camera,
and moving the phantom. The learned policy is robust to these perturbations,
showing corrective behaviors and generalization, and it is possible to detect
Out-Of-Distribution scenarios. We further demonstrate that the Control Barrier
Function successfully limits the action to remain within our specified safety
set in the case of unsafe predictions.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [452] [SpecMaskFoley: Steering Pretrained Spectral Masked Generative Transformer Toward Synchronized Video-to-audio Synthesis via ControlNet](https://arxiv.org/abs/2505.16195)
*Zhi Zhong,Akira Takahashi,Shuyang Cui,Keisuke Toyama,Shusuke Takahashi,Yuki Mitsufuji*

Key words: 音效合成, ControlNet, 预训练模型, 视频同步, 特征对齐

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 提出SpecMaskFoley方法，通过ControlNet引导预训练的SpecMaskGIT模型实现视频同步的音效合成，性能超越从头训练的基线模型。

Motivation: 解决ControlNet在音效合成中性能差距问题，利用预训练模型避免从头训练的复杂性。

Method: 引入频率感知的时间特征对齐器，解决视频特征与时频特征的差异，简化控制机制。

Result: 在常见音效合成基准测试中，SpecMaskFoley表现优于从头训练的基线模型。

Conclusion: SpecMaskFoley显著推进了基于ControlNet的音效合成模型发展。

Abstract: Foley synthesis aims to synthesize high-quality audio that is both
semantically and temporally aligned with video frames. Given its broad
application in creative industries, the task has gained increasing attention in
the research community. To avoid the non-trivial task of training audio
generative models from scratch, adapting pretrained audio generative models for
video-synchronized foley synthesis presents an attractive direction.
ControlNet, a method for adding fine-grained controls to pretrained generative
models, has been applied to foley synthesis, but its use has been limited to
handcrafted human-readable temporal conditions. In contrast, from-scratch
models achieved success by leveraging high-dimensional deep features extracted
using pretrained video encoders. We have observed a performance gap between
ControlNet-based and from-scratch foley models. To narrow this gap, we propose
SpecMaskFoley, a method that steers the pretrained SpecMaskGIT model toward
video-synchronized foley synthesis via ControlNet. To unlock the potential of a
single ControlNet branch, we resolve the discrepancy between the temporal video
features and the time-frequency nature of the pretrained SpecMaskGIT via a
frequency-aware temporal feature aligner, eliminating the need for complicated
conditioning mechanisms widely used in prior arts. Evaluations on a common
foley synthesis benchmark demonstrate that SpecMaskFoley could even outperform
strong from-scratch baselines, substantially advancing the development of
ControlNet-based foley synthesis models. Demo page:
https://zzaudio.github.io/SpecMaskFoley_Demo/

</details>


### [453] [AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models](https://arxiv.org/abs/2505.16211)
*Kai Li,Can Shen,Yile Liu,Jirui Han,Kelong Zheng,Xuechao Zou,Zhe Wang,Xingjian Du,Shun Zhang,Hanjun Luo,Yingbin Jin,Xinxin Xing,Ziyang Ma,Yue Liu,Xiaojun Jia,Yifan Zhang,Junfeng Fang,Kun Wang,Yibo Yan,Haoyang Li,Yiming Li,Xiaobin Zhuang,Yang Liu,Haibo Hu,Zhuo Chen,Zhizheng Wu,Xiaolin Hu,Eng-Siong Chng,XiaoFeng Wang,Wenyuan Xu,Wei Dong,Xinfeng Li*

Key words: 音频大语言模型（ALLMs）、信任评估框架、数据集、评估指标、自动化评分

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 提出了AudioTrust，首个专为音频大语言模型（ALLMs）设计的全面信任评估框架和基准，涵盖六个关键维度和18种实验设置，揭示当前模型的信任边界和局限性。

Motivation: 现有评估框架主要关注文本模态或有限的安全维度，未能充分应对音频模态的特有风险和应用场景。

Method: 构建了包含4,420个音频/文本样本的数据集，设计了9个音频专用评估指标，并采用自动化流程进行模型输出评分。

Result: 实验揭示了当前开源和闭源ALLMs在高风险音频场景下的信任边界和局限性。

Conclusion: AudioTrust为未来音频模型的安全可靠部署提供了有价值的洞察。

Abstract: The rapid advancement and expanding applications of Audio Large Language
Models (ALLMs) demand a rigorous understanding of their trustworthiness.
However, systematic research on evaluating these models, particularly
concerning risks unique to the audio modality, remains largely unexplored.
Existing evaluation frameworks primarily focus on the text modality or address
only a restricted set of safety dimensions, failing to adequately account for
the unique characteristics and application scenarios inherent to the audio
modality. We introduce AudioTrust-the first multifaceted trustworthiness
evaluation framework and benchmark specifically designed for ALLMs. AudioTrust
facilitates assessments across six key dimensions: fairness, hallucination,
safety, privacy, robustness, and authentication. To comprehensively evaluate
these dimensions, AudioTrust is structured around 18 distinct experimental
setups. Its core is a meticulously constructed dataset of over 4,420 audio/text
samples, drawn from real-world scenarios (e.g., daily conversations, emergency
calls, voice assistant interactions), specifically designed to probe the
multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully
designs 9 audio-specific evaluation metrics, and we employ a large-scale
automated pipeline for objective and scalable scoring of model outputs.
Experimental results reveal the trustworthiness boundaries and limitations of
current state-of-the-art open-source and closed-source ALLMs when confronted
with various high-risk audio scenarios, offering valuable insights for the
secure and trustworthy deployment of future audio models. Our platform and
benchmark are available at https://github.com/JusperLee/AudioTrust.

</details>


### [454] [Dialogue in Resonance: An Interactive Music Piece for Piano and Real-Time Automatic Transcription System](https://arxiv.org/abs/2505.16259)
*Hayeon Bang,Taegyun Kwon,Juhan Nam*

Key words: 互动音乐, 实时转录, 计算机音乐, 音乐对话

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 论文介绍了《Dialogue in Resonance》，一种结合实时自动音乐转录的互动音乐作品，平衡了作曲结构与动态交互。

Motivation: 探索如何将作曲意图与实时互动结合，创造一种新颖的音乐对话形式。

Method: 通过实时自动转录技术，计算机实时解读并回应人类演奏者的输入，结合排练及表演过程进行技术实现。

Result: 成功开发了一个平衡作曲结构与动态交互的音乐框架，并在首演中验证其效果。

Conclusion: 该作品展示了技术与艺术的结合，为互动音乐创作提供了新思路。

Abstract: This paper presents <Dialogue in Resonance>, an interactive music piece for a
human pianist and a computer-controlled piano that integrates real-time
automatic music transcription into a score-driven framework. Unlike previous
approaches that primarily focus on improvisation-based interactions, our work
establishes a balanced framework that combines composed structure with dynamic
interaction. Through real-time automatic transcription as its core mechanism,
the computer interprets and responds to the human performer's input in real
time, creating a musical dialogue that balances compositional intent with live
interaction while incorporating elements of unpredictability. In this paper, we
present the development process from composition to premiere performance,
including technical implementation, rehearsal process, and performance
considerations.

</details>


### [455] [Layer-wise Investigation of Large-Scale Self-Supervised Music Representation Models](https://arxiv.org/abs/2505.16306)
*Yizhi Zhou,Haina Zhu,Hangting Chen*

Key words: 音乐信息检索,自监督学习,MusicFM,MuQ,层次信息

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 研究分析了MusicFM和MuQ两种音乐表示模型，验证了自监督学习模型在下游任务中的优势，探索了不同任务中层次信息的特性，并比较了选择特定层的性能差异。

Motivation: 研究动机在于探究自监督学习模型中编码信息的具体含义和适用性，以更好地理解其能力和限制，从而更有效地应用于下游任务。

Method: 方法包括分析MusicFM和MuQ模型，验证其在多下游任务中的优势，探索层次信息对不同任务的特性，以及比较选择特定层的性能差异。

Result: 结果揭示了自监督学习模型在音乐信息检索中的结构和潜在应用，验证了其优势并展示了层次信息的任务适应性和性能差异。

Conclusion: 结论强调了自监督学习模型在音乐信息检索中的潜力，但需进一步理解其信息编码机制以优化应用。

Abstract: Recently, pre-trained models for music information retrieval based on
self-supervised learning (SSL) are becoming popular, showing success in various
downstream tasks. However, there is limited research on the specific meanings
of the encoded information and their applicability. Exploring these aspects can
help us better understand their capabilities and limitations, leading to more
effective use in downstream tasks.
  In this study, we analyze the advanced music representation model MusicFM and
the newly emerged SSL model MuQ. We focus on three main aspects: (i) validating
the advantages of SSL models across multiple downstream tasks, (ii) exploring
the specialization of layer-wise information for different tasks, and (iii)
comparing performance differences when selecting specific layers. Through this
analysis, we reveal insights into the structure and potential applications of
SSL models in music information retrieval.

</details>


### [456] [EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion](https://arxiv.org/abs/2505.16691)
*Advait Joglekar,Divyanshu Singh,Rooshil Rohit Bhatia,S. Umesh*

Key words: 语音转换, 零样本学习, 跨语言, 自监督学习, 扩散Transformer

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 提出了一种结合自监督模型离散语音表示与非自回归扩散Transformer的条件流匹配解码器的零样本跨语言语音转换方法。

Motivation: 当前语音转换方法在零样本跨语言场景中表现不佳，无法泛化至未见过的语言和口音，需要一种更高效的方法。

Method: 采用自监督模型的离散语音表示与非自回归扩散Transformer的条件流匹配解码器，无需文本或多编码器。

Result: 模型在零样本跨语言场景中表现优异，即使对未见过的语言也有效。

Conclusion: 该方法无需复杂特征解耦，纯自监督训练即可实现高效语音转换。

Abstract: Voice Conversion research in recent times has increasingly focused on
improving the zero-shot capabilities of existing methods. Despite remarkable
advancements, current architectures still tend to struggle in zero-shot
cross-lingual settings. They are also often unable to generalize for speakers
of unseen languages and accents. In this paper, we adopt a simple yet effective
approach that combines discrete speech representations from self-supervised
models with a non-autoregressive Diffusion-Transformer based conditional flow
matching speech decoder. We show that this architecture allows us to train a
voice-conversion model in a purely textless, self-supervised fashion. Our
technique works without requiring multiple encoders to disentangle speech
features. Our model also manages to excel in zero-shot cross-lingual settings
even for unseen languages.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [457] [MPPFND: A Dataset and Analysis of Detecting Fake News with Multi-Platform Propagation](https://arxiv.org/abs/2505.15834)
*Congyuan Zhao,Lingwei Wei,Ziming Qin,Wei Zhou,Yunya Song,Songlin Hu*

Key words: 假新闻检测, 多平台传播, 图神经网络, 社交上下文

<details>
  <summary>Details</summary>

Main category: cs.SI

TL;DR: 该论文提出了一个基于多平台传播特性的假新闻检测模型（APSL），利用图神经网络提取不同平台的社交上下文特征，并构建了MPPFND数据集来验证模型的性能。

Motivation: 现有的假新闻检测算法通常基于单一平台，忽视了不同平台传播特性的差异，导致检测效果受限。本文旨在通过多平台数据集和模型提升检测性能。

Method: 构建MPPFND数据集捕获多平台传播结构，提出APSL模型，利用图神经网络提取跨平台的社交上下文特征。

Result: 实验表明，考虑跨平台传播差异可以显著提升假新闻检测的性能。

Conclusion: 多平台传播特性的建模对假新闻检测至关重要，APSL模型在跨平台场景中表现优异。

Abstract: Fake news spreads widely on social media, leading to numerous negative
effects. Most existing detection algorithms focus on analyzing news content and
social context to detect fake news. However, these approaches typically detect
fake news based on specific platforms, ignoring differences in propagation
characteristics across platforms. In this paper, we introduce the MPPFND
dataset, which captures propagation structures across multiple platforms. We
also describe the commenting and propagation characteristics of different
platforms to show that their social contexts have distinct features. We propose
a multi-platform fake news detection model (APSL) that uses graph neural
networks to extract social context features from various platforms. Experiments
show that accounting for cross-platform propagation differences improves fake
news detection performance.

</details>


### [458] [AH-UGC: Adaptive and Heterogeneous-Universal Graph Coarsening](https://arxiv.org/abs/2505.15842)
*Mohit Kataria,Shreyash Bhilwade,Sandeep Kumar,Jayadeva*

Key words: 图粗化、局部敏感哈希、一致性哈希、异构图、语义一致性

<details>
  <summary>Details</summary>

Main category: cs.SI

TL;DR: 论文提出了一种结合局部敏感哈希（LSH）和一致性哈希的自适应图粗化框架，支持异构图语义约束，提升了效率和可扩展性。

Motivation: 现有图粗化方法在异构图和动态粗化比例场景下效率不足，缺乏同时满足自适应性和语义一致性的统一框架。

Method: 结合LSH与一致性哈希实现自适应粗化；针对异构图提出类型隔离粗化策略，限制同类型节点合并以保持语义一致性。

Result: 在23个真实数据集（包括同质/异质图、同配/异配图）上验证了方法的可扩展性，且能保留原图结构和语义。

Conclusion: 该框架首次统一支持自适应与异构粗化，在效率与语义保持上优于现有方法。

Abstract: $\textbf{Graph Coarsening (GC)}$ is a prominent graph reduction technique
that compresses large graphs to enable efficient learning and inference.
However, existing GC methods generate only one coarsened graph per run and must
recompute from scratch for each new coarsening ratio, resulting in unnecessary
overhead. Moreover, most prior approaches are tailored to
$\textit{homogeneous}$ graphs and fail to accommodate the semantic constraints
of $\textit{heterogeneous}$ graphs, which comprise multiple node and edge
types. To overcome these limitations, we introduce a novel framework that
combines Locality Sensitive Hashing (LSH) with Consistent Hashing to enable
$\textit{adaptive graph coarsening}$. Leveraging hashing techniques, our method
is inherently fast and scalable. For heterogeneous graphs, we propose a
$\textit{type isolated coarsening}$ strategy that ensures semantic consistency
by restricting merges to nodes of the same type. Our approach is the first
unified framework to support both adaptive and heterogeneous coarsening.
Extensive evaluations on 23 real-world datasets including homophilic,
heterophilic, homogeneous, and heterogeneous graphs demonstrate that our method
achieves superior scalability while preserving the structural and semantic
integrity of the original graph.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [459] [Is Quantum Optimization Ready? An Effort Towards Neural Network Compression using Adiabatic Quantum Computing](https://arxiv.org/abs/2505.16332)
*Zhehui Wanga,Benjamin Chen Ming Choonga,Tian Huang,Daniel Gerlinghoffa,Rick Siow Mong Goh,Cheng Liu,Tao Luo*

Key words: 量子优化,绝热量子计算,深度神经网络,模型压缩,QUBO

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 论文探讨了如何使用绝热量子计算（AQC）优化深度神经网络（DNN）的剪枝-量化问题，将其转化为二次无约束二进制优化（QUBO）问题，并在商业量子退火设备上验证了其有效性。AQC在时间效率和全局最优解识别上优于经典算法。

Motivation: 随着DNN模型规模的增大，优化变得愈发困难。量子优化虽然适合处理复杂问题，但其在DNN优化中的应用需要重新设计以适应现有量子设备。本研究旨在探索AQC在DNN模型压缩中的潜力。

Method: 将DNN的剪枝-量化问题重新表述为QUBO问题，并利用商业量子退火设备（如AQC）求解。对经典启发式方法进行调整，使其适配量子计算框架。

Result: 实验表明，AQC在DNN模型压缩中的表现优于遗传算法和强化学习，不仅在时间效率上更优，还能找到全局最优解。

Conclusion: AQC在DNN优化中具有实际应用潜力，尤其适用于需要高效全局优化的场景。

Abstract: Quantum optimization is the most mature quantum computing technology to date,
providing a promising approach towards efficiently solving complex
combinatorial problems. Methods such as adiabatic quantum computing (AQC) have
been employed in recent years on important optimization problems across various
domains. In deep learning, deep neural networks (DNN) have reached immense
sizes to support new predictive capabilities. Optimization of large-scale
models is critical for sustainable deployment, but becomes increasingly
challenging with ever-growing model sizes and complexity. While quantum
optimization is suitable for solving complex problems, its application to DNN
optimization is not straightforward, requiring thorough reformulation for
compatibility with commercially available quantum devices. In this work, we
explore the potential of adopting AQC for fine-grained pruning-quantization of
convolutional neural networks. We rework established heuristics to formulate
model compression as a quadratic unconstrained binary optimization (QUBO)
problem, and assess the solution space offered by commercial quantum annealing
devices. Through our exploratory efforts of reformulation, we demonstrate that
AQC can achieve effective compression of practical DNN models. Experiments
demonstrate that adiabatic quantum computing (AQC) not only outperforms
classical algorithms like genetic algorithms and reinforcement learning in
terms of time efficiency but also excels at identifying global optima.

</details>


### [460] [Experimental robustness benchmark of quantum neural network on a superconducting quantum processor](https://arxiv.org/abs/2505.16714)
*Hai-Feng Zhang,Zhao-Yun Chen,Peng Wang,Liang-Liang Guo,Tian-Le Wang,Xiao-Yan Yang,Ren-Ze Zhao,Ze-An Zhao,Sheng Zhang,Lei Du,Hao-Ran Tao,Zhi-Long Jia,Wei-Cheng Kong,Huan-Yu Liu,Athanasios V. Vasilakos,Yang Yang,Yu-Chun Wu,Ji Guan,Peng Duan,Guo-Ping Guo*

Key words: 量子机器学习, 对抗攻击, 量子神经网络, 对抗训练, 鲁棒性

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 该论文首次系统性实验了20-qubit量子神经网络（QNN）的对抗攻击鲁棒性，提出了高效的对抗攻击算法，并通过对抗训练提升了QNN的鲁棒性。实验显示QNN比经典神经网络更具鲁棒性，且实验值与理论值偏差极小（3×10⁻³），验证了攻击算法的有效性。

Motivation: 量子机器学习（QML）模型易受对抗攻击影响，阻碍其安全部署。该研究旨在系统性评估QNN的对抗鲁棒性，并探索提升方法。

Method: 设计了一种高效的对抗攻击算法，针对超导处理器上的20-qubit QNN分类器进行实验，结合对抗训练和输入梯度正则化。

Result: 对抗训练显著提升QNN的鲁棒性；QNN比经典神经网络更具对抗鲁棒性（归因于量子噪声）；实验上界与理论下界的偏差极小（3×10⁻³）。

Conclusion: 本研究为评估和提升量子对抗鲁棒性建立了关键实验框架，推动了安全可靠的QML应用发展。

Abstract: Quantum machine learning (QML) models, like their classical counterparts, are
vulnerable to adversarial attacks, hindering their secure deployment. Here, we
report the first systematic experimental robustness benchmark for 20-qubit
quantum neural network (QNN) classifiers executed on a superconducting
processor. Our benchmarking framework features an efficient adversarial attack
algorithm designed for QNNs, enabling quantitative characterization of
adversarial robustness and robustness bounds. From our analysis, we verify that
adversarial training reduces sensitivity to targeted perturbations by
regularizing input gradients, significantly enhancing QNN's robustness.
Additionally, our analysis reveals that QNNs exhibit superior adversarial
robustness compared to classical neural networks, an advantage attributed to
inherent quantum noise. Furthermore, the empirical upper bound extracted from
our attack experiments shows a minimal deviation ($3 \times 10^{-3}$) from the
theoretical lower bound, providing strong experimental confirmation of the
attack's effectiveness and the tightness of fidelity-based robustness bounds.
This work establishes a critical experimental framework for assessing and
improving quantum adversarial robustness, paving the way for secure and
reliable QML applications.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [461] [Common Data Format (CDF): A Standardized Format for Match-Data in Football (Soccer)](https://arxiv.org/abs/2505.15820)
*Gabriel Anzer,Kilian Arnsmeyer,Pascal Bauer,Joris Bekkers,Ulf Brefeld,Jesse Davis,Nicolas Evans,Matthias Kempe,Samuel J Robertson,Joshua Wyatt Smith,Jan Van Haaren*

Key words: 足球数据, 数据标准化, 通用数据格式, 数据分析

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: 提出了一种称为通用数据格式（CDF）的标准化足球数据格式，以解决不同数据提供商之间的数据不一致问题。

Motivation: 足球比赛中，多方收集的数据存在格式、规范和交付方式差异，导致分析和决策困难。

Method: 设计并详细说明CDF的技术规范，包括五类比赛数据的最小模式和数据交付方法。

Result: CDF确保数据清晰、完整且上下文明确，支持后续分析任务。

Conclusion: CDF为足球数据提供统一标准，解决了数据集成和分析的难题。

Abstract: During football matches, a variety of different parties (e.g., companies)
each collect (possibly overlapping) data about the match ranging from basic
information (e.g., starting players) to detailed positional data. This data is
provided to clubs, federations, and other organizations who are increasingly
interested in leveraging this data to inform their decision making.
Unfortunately, analyzing such data pose significant barriers because each
provider may (1) collect different data, (2) use different specifications even
within the same category of data, (3) represent the data differently, and (4)
delivers the data in a different manner (e.g., file format, protocol).
Consequently, working with these data requires a significant investment of time
and money. The goal of this work is to propose a uniform and standardized
format for football data called the Common Data Format (CDF). The CDF specifies
a minimal schema for five types of match data: match sheet data, video footage,
event data, tracking data, and match meta data. It aims to ensure that the
provided data is clear, sufficiently contextualized (e.g., its provenance is
clear), and complete such that it enables common downstream analysis tasks.
Concretely, this paper will detail the technical specifications of the CDF, the
representational choices that were made to help ensure the clarity of the
provided data, and a concrete approach for delivering data in the CDF.

</details>


### [462] [MAPS: A Multilingual Benchmark for Global Agent Performance and Security](https://arxiv.org/abs/2505.15935)
*Omer Hofman,Oren Rachmil,Shamik Bose,Vikas Pahuja,Jonathan Brokman,Toshiya Shimizu,Trisha Starostina,Kelly Marchisio,Seraphina Goldfarb-Tarrant,Roman Vainshtein*

Key words: 代理AI, 多语言基准测试, LLM, 性能评估, 安全性

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: 提出了一个多语言基准测试套件MAPS，用于评估基于LLM的代理AI系统在不同语言和任务中的表现。

Motivation: 解决LLM在多语言环境下性能和安全性的问题，确保代理AI系统的全球可访问性。

Method: 通过翻译四个广泛使用的代理基准测试数据集（GAIA、SWE-bench、MATH、Agent Security Benchmark）至十种语言，构建了包含805个独特任务和8,855个语言特定实例的MAPS套件。

Result: 发现从英语到其他语言的转换会导致性能和安全性的一致下降，严重程度因任务和翻译输入量而异。

Conclusion: 提供了一个标准化的评估框架，并提出了可操作建议，以促进多语言环境下代理AI系统的公平、可靠和全球可访问性研究。

Abstract: Agentic AI systems, which build on Large Language Models (LLMs) and interact
with tools and memory, have rapidly advanced in capability and scope. Yet,
since LLMs have been shown to struggle in multilingual settings, typically
resulting in lower performance and reduced safety, agentic systems risk
inheriting these limitations. This raises concerns about the global
accessibility of such systems, as users interacting in languages other than
English may encounter unreliable or security-critical agent behavior. Despite
growing interest in evaluating agentic AI, existing benchmarks focus
exclusively on English, leaving multilingual settings unexplored. To address
this gap, we propose MAPS, a multilingual benchmark suite designed to evaluate
agentic AI systems across diverse languages and tasks. MAPS builds on four
widely used agentic benchmarks - GAIA (real-world tasks), SWE-bench (code
generation), MATH (mathematical reasoning), and the Agent Security Benchmark
(security). We translate each dataset into ten diverse languages, resulting in
805 unique tasks and 8,855 total language-specific instances. Our benchmark
suite enables a systematic analysis of how multilingual contexts affect agent
performance and robustness. Empirically, we observe consistent degradation in
both performance and security when transitioning from English to other
languages, with severity varying by task and correlating with the amount of
translated input. Building on these findings, we provide actionable
recommendations to guide agentic AI systems development and assessment under
multilingual settings. This work establishes a standardized evaluation
framework, encouraging future research towards equitable, reliable, and
globally accessible agentic AI. MAPS benchmark suite is publicly available at
https://huggingface.co/datasets/Fujitsu-FRE/MAPS

</details>


### [463] [WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative Learning](https://arxiv.org/abs/2505.16635)
*Zhaomin Wu,Ziyang Wang,Bingsheng He*

Key words: 表格数据、图结构、协作学习、WikiData、基础模型

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: 该论文介绍了WikiDBGraph，一个包含10万个真实世界表格数据库的大规模图结构，通过1700万条边连接，并提出协作学习方法在提升模型性能上的潜力。

Motivation: 现有表格数据学习因单表或孤立数据库的局限性而受阻，迫切需要解决数据规模不足的问题，尤其是缺乏真实世界互联表格资源。

Method: 构建WikiDBGraph，一个基于WikiData的大规模图结构，通过节点和边属性表征数据库间的关联，并利用协作学习（如联邦学习、迁移学习）方法训练模型。

Result: 实验证明，在互联数据库上采用协作学习能显著提升性能，为结构化基础模型训练提供了新方向。

Conclusion: WikiDBGraph为表格数据的互联学习提供了资源，同时揭示了未来研究的关键挑战与方向。

Abstract: Tabular data, ubiquitous and rich in informational value, is an increasing
focus for deep representation learning, yet progress is hindered by studies
centered on single tables or isolated databases, which limits model
capabilities due to data scale. While collaborative learning approaches such as
federated learning, transfer learning, split learning, and tabular foundation
models aim to learn from multiple correlated databases, they are challenged by
a scarcity of real-world interconnected tabular resources. Current data lakes
and corpora largely consist of isolated databases lacking defined
inter-database correlations. To overcome this, we introduce WikiDBGraph, a
large-scale graph of 100,000 real-world tabular databases from WikiData,
interconnected by 17 million edges and characterized by 13 node and 12 edge
properties derived from its database schema and data distribution.
WikiDBGraph's weighted edges identify both instance- and feature-overlapped
databases. Experiments on these newly identified databases confirm that
collaborative learning yields superior performance, thereby offering
considerable promise for structured foundation model training while also
exposing key challenges and future directions for learning from interconnected
tabular data.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [464] [Multimodal Generative AI for Story Point Estimation in Software Development](https://arxiv.org/abs/2505.16290)
*Mohammad Rubyet Islam,Peter Sandborn*

Key words: 多模态生成AI, 故事点估算, 敏捷开发, BERT, CNN, XGBoost

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 这篇研究探讨了多模态生成AI在敏捷软件开发中故事点估算的应用，通过整合文本、图像和分类数据优化估算方法，结果显示了在高简单故事点的准确性，但复杂类别因数据不平衡存在挑战。

Motivation: 传统单模态估算方法存在局限性，研究旨在通过多模态数据集成提升估算准确性和适应性。

Method: 整合文本（BERT）、图像（CNN）和分类数据（XGBoost）的多模态方法。

Result: 对简单故事点估算准确率高，复杂类别因数据不平衡表现受限；分类数据（如严重性）对模型性能有显著影响。

Conclusion: 多模态数据集成在AI驱动的项目管理中具有变革潜力，未来需解决数据变异性和提升鲁棒性。

Abstract: This research explores the application of Multimodal Generative AI to enhance
story point estimation in Agile software development. By integrating text,
image, and categorical data using advanced models like BERT, CNN, and XGBoost,
our approach surpasses the limitations of traditional single-modal estimation
methods. The results demonstrate strong accuracy for simpler story points,
while also highlighting challenges in more complex categories due to data
imbalance. This study further explores the impact of categorical data,
particularly severity, on the estimation process, emphasizing its influence on
model performance. Our findings emphasize the transformative potential of
multimodal data integration in refining AI-driven project management, paving
the way for more precise, adaptable, and domain-specific AI capabilities.
Additionally, this work outlines future directions for addressing data
variability and enhancing the robustness of AI in Agile methodologies.

</details>


### [465] [AutoMCQ -- Automatically Generate Code Comprehension Questions using GenAI](https://arxiv.org/abs/2505.16430)
*Martin Goodfellow,Robbie Booth,Andrew Fagan,Alasdair Lambert*

Key words: 代码理解, 生成式AI, 自动评估, 多选题, 教育技术

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: AutoMCQ利用GenAI自动生成多选题代码理解问题，集成到CodeRunner平台中，解决学生代码理解不足及评估耗时问题。

Motivation: 学生在写代码时可能不完全理解其内容，后期教育中难以纠正错误知识，且GenAI工具的普及使代码理解更显重要。传统评估方法耗时且难以扩展。

Method: 利用生成式人工智能（GenAI）自动生成多选题形式的代码理解问题，并与CodeRunner自动评估平台集成。

Result: AutoMCQ能够高效生成代码理解问题，减轻人工评估负担，并可能辅助检测抄袭。

Conclusion: AutoMCQ为代码理解评估提供了一种可扩展的自动化解决方案，适用于现代教育环境。

Abstract: Students often do not fully understand the code they have written. This
sometimes does not become evident until later in their education, which can
mean it is harder to fix their incorrect knowledge or misunderstandings. In
addition, being able to fully understand code is increasingly important in a
world where students have access to generative artificial intelligence (GenAI)
tools, such as GitHub Copilot. One effective solution is to utilise code
comprehension questions, where a marker asks questions about a submission to
gauge understanding, this can also have the side effect of helping to detect
plagiarism. However, this approach is time consuming and can be difficult
and/or expensive to scale. This paper introduces AutoMCQ, which uses GenAI for
the automatic generation of multiple-choice code comprehension questions. This
is integrated with the CodeRunner automated assessment platform.

</details>


### [466] [SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development](https://arxiv.org/abs/2505.16975)
*Yaxin Du,Yuzhu Cai,Yifan Zhou,Cheng Wang,Yu Qian,Xianghe Pang,Qian Liu,Yue Hu,Siheng Chen*

Key words: 大语言模型（LLMs）, 功能驱动开发（FDD）, SWE-Dev数据集, 监督微调（SFT）, 强化学习（RL）

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 论文介绍了SWE-Dev数据集，这是首个针对真实世界功能开发任务的大规模数据集（14,000个训练样本和500个测试样本），支持监督微调和强化学习。评估显示当前AI在功能驱动开发任务中表现较差，但通过微调可以显著提升模型性能。

Motivation: 功能驱动开发（FDD）是一个普遍但未被充分研究的软件开发任务，现有的大语言模型（LLMs）在此任务上表现不佳。为此，作者提出了SWE-Dev数据集，旨在填补这一空白并推动相关研究。

Method: 通过构建一个包含可运行环境和开发者编写的单元测试的大规模数据集（SWE-Dev），支持监督微调（SFT）和基于单元测试的强化学习（RL）。实验评估了17个聊天机器人LLMs、10个推理模型和10个多智能体系统（MAS）。

Result: 当前AI在FDD任务中表现较差（如Claude-3.7-Sonnet在困难测试集上仅22.45% Pass@3），但经过SWE-Dev训练集的微调后，一个7B模型在困难测试集上的表现可与GPT-4o媲美。

Conclusion: SWE-Dev数据集为功能驱动开发任务提供了高质量的训练和评估平台，显著提升了模型性能，展示了其在推动AI软件开发能力方面的价值。

Abstract: Large Language Models (LLMs) have shown strong capability in diverse software
engineering tasks, e.g. code completion, bug fixing, and document generation.
However, feature-driven development (FDD), a highly prevalent real-world task
that involves developing new functionalities for large, existing codebases,
remains underexplored. We therefore introduce SWE-Dev, the first large-scale
dataset (with 14,000 training and 500 test samples) designed to evaluate and
train autonomous coding systems on real-world feature development tasks. To
ensure verifiable and diverse training, SWE-Dev uniquely provides all instances
with a runnable environment and its developer-authored executable unit tests.
This collection not only provides high-quality data for Supervised Fine-Tuning
(SFT), but also enables Reinforcement Learning (RL) by delivering accurate
reward signals from executable unit tests. Our extensive evaluations on
SWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent
Systems (MAS), reveal that FDD is a profoundly challenging frontier for current
AI (e.g., Claude-3.7-Sonnet achieves only 22.45\% Pass@3 on the hard test
split). Crucially, we demonstrate that SWE-Dev serves as an effective platform
for model improvement: fine-tuning on training set enabled a 7B model
comparable to GPT-4o on \textit{hard} split, underscoring the value of its
high-quality training data. Code is available here
\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}.

</details>


### [467] [Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks](https://arxiv.org/abs/2505.16901)
*Hongyuan Tao,Ying Zhang,Zhenhao Tang,Hongen Peng,Xukun Zhu,Bingchang Liu,Yingguang Yang,Ziyin Zhang,Zhaogui Xu,Haipeng Zhang,Linchao Zhu,Rui Wang,Hang Yu,Jianguo Li,Peng Di*

Key words: 大语言模型, 代码生成, 开源, 代码图模型, SWE-bench

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 研究表明开源LLMs可通过Code Graph Models（CGMs）有效处理仓库级任务，无需代理方法，在SWE-bench Lite上表现优异。

Motivation: 解决现有LLM代理方法在仓库级软件工程任务中的不可预测性、可访问性及隐私问题。

Method: 提出CGMs，将代码图结构整合到LLM的注意力机制中，通过适配器映射节点属性。

Result: 在Qwen2.5-72B模型上实现43.00%的解决率，在开源模型中排名第一。

Conclusion: 开源LLMs结合CGMs可高效处理复杂代码库任务，超越现有开源方法。

Abstract: Recent advances in Large Language Models (LLMs) have shown promise in
function-level code generation, yet repository-level software engineering tasks
remain challenging. Current solutions predominantly rely on proprietary LLM
agents, which introduce unpredictability and limit accessibility, raising
concerns about data privacy and model customization. This paper investigates
whether open-source LLMs can effectively address repository-level tasks without
requiring agent-based approaches. We demonstrate this is possible by enabling
LLMs to comprehend functions and files within codebases through their semantic
information and structural dependencies. To this end, we introduce Code Graph
Models (CGMs), which integrate repository code graph structures into the LLM's
attention mechanism and map node attributes to the LLM's input space using a
specialized adapter. When combined with an agentless graph RAG framework, our
approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark
using the open-source Qwen2.5-72B model. This performance ranks first among
open weight models, second among methods with open-source systems, and eighth
overall, surpassing the previous best open-source model-based method by 12.33%.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [468] [Physics-based machine learning for mantle convection simulations](https://arxiv.org/abs/2505.16041)
*Siddhant Agarwal,Ali Can Bekar,Christian Hüttig,David S. Greenberg,Nicola Tosi*

Key words: 地幔对流, 机器学习, 卷积神经网络, 计算模拟

<details>
  <summary>Details</summary>

Main category: astro-ph.EP

TL;DR: 提出了一个基于物理的机器学习方法，用于预测地幔对流模拟中的流动速度，解决了传统数值求解器的计算难题。

Motivation: 传统地幔对流模拟面临输入参数未知、传输性质非线性依赖及超长积分时间等计算挑战。

Method: 利用卷积神经网络预测流动速度并保障质量守恒，结合有限体积求解器推进温度场演化。

Result: 模型比传统数值求解器快89倍，测试了不同网络组件对性能的影响。

Conclusion: 该方法在地幔对流模拟中展现出高效性，但在某些未见场景中表现有限。

Abstract: Mantle convection simulations are an essential tool for understanding how
rocky planets evolve. However, the poorly known input parameters to these
simulations, the non-linear dependence of transport properties on pressure and
temperature, and the long integration times in excess of several billion years
all pose a computational challenge for numerical solvers. We propose a
physics-based machine learning approach that predicts creeping flow velocities
as a function of temperature while conserving mass, thereby bypassing the
numerical solution of the Stokes problem. A finite-volume solver then uses the
predicted velocities to advect and diffuse the temperature field to the next
time-step, enabling autoregressive rollout at inference. For training, our
model requires temperature-velocity snapshots from a handful of simulations
(94). We consider mantle convection in a two-dimensional rectangular box with
basal and internal heating, pressure- and temperature-dependent viscosity.
Overall, our model is up to 89 times faster than the numerical solver. We also
show the importance of different components in our convolutional neural network
architecture such as mass conservation, learned paddings on the boundaries, and
loss scaling for the overall rollout performance. Finally, we test our approach
on unseen scenarios to demonstrate some of its strengths and weaknesses.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [469] [An Inclusive Foundation Model for Generalizable Cytogenetics in Precision Oncology](https://arxiv.org/abs/2505.15868)
*Changchun Yang,Weiqian Dai,Yilan Zhang,Siyuan Chen,Jingdong Hu,Junkai Su,Yuxuan Chen,Ao Xu,Na Li,Xin Gao,Yongguo Yu*

Key words: 染色体分析, 基础模型, 自监督学习, 细胞基因组学, 临床AI

<details>
  <summary>Details</summary>

Main category: q-bio.QM

TL;DR: CHROMA是一种基础模型，用于细胞基因组学，通过自监督学习在84000多个样本上预训练，能够更好地识别染色体异常，减少专家标注工作量，并提高临床分析的自动化水平。

Motivation: 当前染色体分析方法面临复杂性高、数据集稀缺、模型泛化能力差等问题，需要一种通用且高效的解决方案来支持临床遗传病和癌症治疗的诊断。

Method: CHROMA采用自监督学习，预训练于84000个样本（约400万张染色体图像），学习染色体异常的可泛化表示。

Result: CHROMA在各类染色体异常识别中表现优于其他方法，尤其在数据标注少或数据集不平衡的情况下仍能保持高性能。

Conclusion: CHROMA为临床分析提供了一种可扩展且通用的自动化解决方案，提升了基因组分析的准确性和可及性。

Abstract: Chromosome analysis is vital for diagnosing genetic disorders and guiding
cancer therapy decisions through the identification of somatic clonal
aberrations. However, developing an AI model are hindered by the overwhelming
complexity and diversity of chromosomal abnormalities, requiring extensive
annotation efforts, while automated methods remain task-specific and lack
generalizability due to the scarcity of comprehensive datasets spanning diverse
resource conditions. Here, we introduce CHROMA, a foundation model for
cytogenomics, designed to overcome these challenges by learning generalizable
representations of chromosomal abnormalities. Pre-trained on over 84,000
specimens (~4 million chromosomal images) via self-supervised learning, CHROMA
outperforms other methods across all types of abnormalities, even when trained
on fewer labelled data and more imbalanced datasets. By facilitating
comprehensive mapping of instability and clonal leisons across various
aberration types, CHROMA offers a scalable and generalizable solution for
reliable and automated clinical analysis, reducing the annotation workload for
experts and advancing precision oncology through the early detection of rare
genomic abnormalities, enabling broad clinical AI applications and making
advanced genomic analysis more accessible.

</details>


### [470] [Advancing Tabular Stroke Modelling Through a Novel Hybrid Architecture and Feature-Selection Synergy](https://arxiv.org/abs/2505.15844)
*Yousuf Islam,Md. Jalal Uddin Chowdhury,Sumon Chandra Das*

Key words: 中风预测, 机器学习, 数据预处理, SMOTE, 混合模型

<details>
  <summary>Details</summary>

Main category: q-bio.QM

TL;DR: 该研究开发了一个可解释的机器学习框架，通过数据预处理和混合模型显著提升了中风预测的准确性。

Motivation: 全球范围内，脑中风仍是主要致死和致残原因，但现有预测模型准确率不足95%，限制了实际应用。

Method: 研究结合了EDA、SMOTE、特征选择和多种算法优化，最终采用随机森林、XGBoost、LightGBM和支持向量机，以逻辑回归作为元学习器。

Result: 模型准确率达97.2%，F1分数97.15%，显著优于最佳单一模型（LightGBM，准确率91.4%）。

Conclusion: 严格的预处理和混合模型可将低成本表格数据转化为接近临床级的中风风险评估工具。

Abstract: Brain stroke remains one of the principal causes of death and disability
worldwide, yet most tabular-data prediction models still hover below the 95%
accuracy threshold, limiting real-world utility. Addressing this gap, the
present work develops and validates a completely data-driven and interpretable
machine-learning framework designed to predict strokes using ten routinely
gathered demographic, lifestyle, and clinical variables sourced from a public
cohort of 4,981 records. We employ a detailed exploratory data analysis (EDA)
to understand the dataset's structure and distribution, followed by rigorous
data preprocessing, including handling missing values, outlier removal, and
class imbalance correction using Synthetic Minority Over-sampling Technique
(SMOTE). To streamline feature selection, point-biserial correlation and
random-forest Gini importance were utilized, and ten varied
algorithms-encompassing tree ensembles, boosting, kernel methods, and a
multilayer neural network-were optimized using stratified five-fold
cross-validation. Their predictions based on probabilities helped us build the
proposed model, which included Random Forest, XGBoost, LightGBM, and a
support-vector classifier, with logistic regression acting as a meta-learner.
The proposed model achieved an accuracy rate of 97.2% and an F1-score of
97.15%, indicating a significant enhancement compared to the leading individual
model, LightGBM, which had an accuracy of 91.4%. Our study's findings indicate
that rigorous preprocessing, coupled with a diverse hybrid model, can convert
low-cost tabular data into a nearly clinical-grade stroke-risk assessment tool.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [471] [All You Need is "Leet": Evading Hate-speech Detection AI](https://arxiv.org/abs/2505.16263)
*Sampanna Yashwant Kahu,Naman Ahuja*

Key words: 仇恨言论，黑盒技术，深度学习，扰动攻击，社交媒体

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 设计黑盒技术生成扰动，以欺骗基于深度学习的有害言论检测模型，降低其效率，同时确保原意最小改变。

Motivation: 社交媒体和在线论坛日益流行，但常被用于传播仇恨言论，需要技术保护用户免受其害。

Method: 采用黑盒技术生成扰动，以愚弄基于深度学习的有害言论检测模型。

Result: 最佳扰动攻击能够成功逃避86.8%有害文本的检测。

Conclusion: 该技术在保护用户不受有害言论侵害的同时，保持了文本原意的完整性。

Abstract: Social media and online forums are increasingly becoming popular.
Unfortunately, these platforms are being used for spreading hate speech. In
this paper, we design black-box techniques to protect users from hate-speech on
online platforms by generating perturbations that can fool state of the art
deep learning based hate speech detection models thereby decreasing their
efficiency. We also ensure a minimal change in the original meaning of
hate-speech. Our best perturbation attack is successfully able to evade
hate-speech detection for 86.8 % of hateful text.

</details>


### [472] [DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection](https://arxiv.org/abs/2505.16530)
*Yuliang Yan,Haochun Tang,Shuo Yan,Enyan Dai*

Key words: 大型语言模型、知识产权保护、指纹识别、黑盒验证

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 该论文提出了DuFFin框架，通过双级指纹（触发模式和知识级指纹）在黑盒设置中进行LLM版权验证，实验表明其准确性高。

Motivation: 保护大型语言模型（LLMs）的知识产权（IP），防止恶意窃取或未经授权的部署。

Method: 提出了DuFFin框架，结合触发模式和知识级指纹进行黑盒版权验证。

Result: 实验显示，该方法能在多种模型变体上准确验证基LLM版权，IP-ROC指标超过0.95。

Conclusion: DuFFin是一种实用且有效的LLM版权保护方法。

Abstract: Large language models (LLMs) are considered valuable Intellectual Properties
(IP) for legitimate owners due to the enormous computational cost of training.
It is crucial to protect the IP of LLMs from malicious stealing or unauthorized
deployment. Despite existing efforts in watermarking and fingerprinting LLMs,
these methods either impact the text generation process or are limited in
white-box access to the suspect model, making them impractical. Hence, we
propose DuFFin, a novel $\textbf{Du}$al-Level $\textbf{Fin}$gerprinting
$\textbf{F}$ramework for black-box setting ownership verification. DuFFin
extracts the trigger pattern and the knowledge-level fingerprints to identify
the source of a suspect model. We conduct experiments on a variety of models
collected from the open-source website, including four popular base models as
protected LLMs and their fine-tuning, quantization, and safety alignment
versions, which are released by large companies, start-ups, and individual
users. Results show that our method can accurately verify the copyright of the
base protected LLM on their model variants, achieving the IP-ROC metric greater
than 0.95. Our code is available at
https://github.com/yuliangyan0807/llm-fingerprint.

</details>


### [473] [CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning](https://arxiv.org/abs/2505.16559)
*Biao Yi,Tiansheng Huang,Baolei Zhang,Tong Li,Lihai Nie,Zheli Liu,Li Shen*

Key words: 大语言模型（LLM）、有害微调攻击、选择性遗忘、模型崩溃、安全对齐、CTRAP

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 该论文指出当前针对有害微调攻击的选择性遗忘防御方式存在缺陷，大语言模型（LLM）的强大适应能力使其能轻易绕过选择性遗忘。为此，作者提出了一种新型防御范式——模型崩溃（model collapse），通过预先配置的崩溃陷阱（CTRAP）在检测到恶意适应时触发模型能力退化，从而彻底消除攻击者利用的机会，同时在良性微调中保持模型功能。

Motivation: 当前选择性遗忘防御无法彻底阻止大语言模型被恶意利用，因其通用适应性允许攻击者快速重新学习或调整模型至有害任务。作者旨在解决这一核心漏洞，通过更彻底的模型崩溃方案阻断攻击。

Method: 提出“崩溃陷阱”（CTRAP）机制：在模型对齐阶段预配置触发条件，若微调过程中检测到持续试图逆转安全对齐的更新，则逐步降解模型的核心语言建模能力，直至其失效。该机制在良性微调中保持静默。

Result: 实验证明CTRAP能有效抵御多种LLM和攻击场景下的有害微调风险，同时在良性场景中保持高性能表现。

Conclusion: 模型崩溃是比选择性遗忘更彻底的防御方案，通过CTRAP机制直接消除模型的通用能力漏洞，为LLM安全提供了新思路。

Abstract: Fine-tuning-as-a-service, while commercially successful for Large Language
Model (LLM) providers, exposes models to harmful fine-tuning attacks. As a
widely explored defense paradigm against such attacks, unlearning attempts to
remove malicious knowledge from LLMs, thereby essentially preventing them from
being used to perform malicious tasks. However, we highlight a critical flaw:
the powerful general adaptability of LLMs allows them to easily bypass
selective unlearning by rapidly relearning or repurposing their capabilities
for harmful tasks. To address this fundamental limitation, we propose a
paradigm shift: instead of selective removal, we advocate for inducing model
collapse--effectively forcing the model to "unlearn everything"--specifically
in response to updates characteristic of malicious adaptation. This collapse
directly neutralizes the very general capabilities that attackers exploit,
tackling the core issue unaddressed by selective unlearning. We introduce the
Collapse Trap (CTRAP) as a practical mechanism to implement this concept
conditionally. Embedded during alignment, CTRAP pre-configures the model's
reaction to subsequent fine-tuning dynamics. If updates during fine-tuning
constitute a persistent attempt to reverse safety alignment, the pre-configured
trap triggers a progressive degradation of the model's core language modeling
abilities, ultimately rendering it inert and useless for the attacker.
Crucially, this collapse mechanism remains dormant during benign fine-tuning,
ensuring the model's utility and general capabilities are preserved for
legitimate users. Extensive empirical results demonstrate that CTRAP
effectively counters harmful fine-tuning risks across various LLMs and attack
settings, while maintaining high performance in benign scenarios. Our code is
available at https://anonymous.4open.science/r/CTRAP.

</details>


### [474] [A Scalable Hierarchical Intrusion Detection System for Internet of Vehicles](https://arxiv.org/abs/2505.16215)
*Md Ashraf Uddin,Nam H. Chu,Reza Rafeh,Mutaz Barika*

Key words: 车联网（IoV）, 入侵检测系统（IDS）, 分层分类, Boruta特征选择, 边缘计算

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 本文提出了一种针对车联网（IoV）的分层分类框架，通过在不同层级训练和测试分类器，使边缘节点能独立检测特定攻击，同时利用云节点进行全面威胁分析。该方法采用Boruta特征选择优化处理效率，并在CIC-IoV2024数据集上验证了有效性。

Motivation: 车联网因其动态性、移动性和无线数据传输特性易受网络威胁，现有集中式入侵检测系统存在延迟高且边缘节点资源不足的问题。需要一种适应分布式特性的解决方案。

Method: 提出分层分类框架，边缘节点独立检测特定攻击，云节点提供全面分析；采用Boruta方法降维以优化边缘节点资源使用。

Result: 在CIC-IoV2024数据集上验证，模型表现出可行性和高效性。

Conclusion: 分层框架有效平衡了边缘与云节点的能力，提升了车联网安全检测的实时性和效率。

Abstract: Due to its nature of dynamic, mobility, and wireless data transfer, the
Internet of Vehicles (IoV) is prone to various cyber threats, ranging from
spoofing and Distributed Denial of Services (DDoS) attacks to malware. To
safeguard the IoV ecosystem from intrusions, malicious activities, policy
violations, intrusion detection systems (IDS) play a critical role by
continuously monitoring and analyzing network traffic to identify and mitigate
potential threats in real-time. However, most existing research has focused on
developing centralized, machine learning-based IDS systems for IoV without
accounting for its inherently distributed nature. Due to intensive computing
requirements, these centralized systems often rely on the cloud to detect cyber
threats, increasing delay of system response. On the other hand, edge nodes
typically lack the necessary resources to train and deploy complex machine
learning algorithms. To address this issue, this paper proposes an effective
hierarchical classification framework tailored for IoV networks. Hierarchical
classification allows classifiers to be trained and tested at different levels,
enabling edge nodes to detect specific types of attacks independently. With
this approach, edge nodes can conduct targeted attack detection while
leveraging cloud nodes for comprehensive threat analysis and support. Given the
resource constraints of edge nodes, we have employed the Boruta feature
selection method to reduce data dimensionality, optimizing processing
efficiency. To evaluate our proposed framework, we utilize the latest IoV
security dataset CIC-IoV2024, achieving promising results that demonstrate the
feasibility and effectiveness of our models in securing IoV networks.

</details>


### [475] [CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework](https://arxiv.org/abs/2505.16888)
*Viet Pham,Thai Le*

Key words: 大型语言模型（LLMs）、对抗攻击、系统提示、信息操纵、CAIN算法

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出了一种新型安全威胁，通过篡改LLMs的系统提示，使其针对特定问题（如选举或疫苗安全）产生恶意回答，同时在其他问题上表现正常。作者开发了CAIN算法，可自动生成此类恶意提示，并在开源和商业LLMs上验证了其效果。结果显示，CAIN在无目标攻击中可使特定问题的性能下降40%，在有目标攻击中能使70%的特定回答达成恶意目标，突显了加强LLMs鲁棒性的必要性。

Motivation: 由于大型语言模型（LLMs）在多个应用中广泛使用，但其易受对抗攻击，尤其是通过操纵系统提示使其在特定问题上产生恶意回答，可能引发大规模信息操纵。作者旨在揭示这一新型威胁并提出防御需求。

Method: 作者开发了CAIN算法，能够在黑盒环境下自动为特定目标问题生成恶意系统提示，无需LLMs的参数访问。通过实验评估其在开源和商业LLMs上的效果，包括无目标和有目标攻击两类任务。

Result: 在无目标攻击中，CAIN使LLMs对目标问题的F1分数下降40%，同时保持对正常输入的高准确率。在有目标攻击中，CAIN实现了70%的F1分数达到特定恶意回答目标，且对正常问题影响最小。

Conclusion: 结果表明，篡改系统提示是一种高效且隐蔽的攻击手段，突显了提升LLMs鲁棒性的紧迫性。作者建议未来研究需关注防御此类攻击的策略。

Abstract: Large language models (LLMs) have advanced many applications, but are also
known to be vulnerable to adversarial attacks. In this work, we introduce a
novel security threat: hijacking AI-human conversations by manipulating LLMs'
system prompts to produce malicious answers only to specific targeted questions
(e.g., "Who should I vote for US President?", "Are Covid vaccines safe?"),
while behaving benignly on others. This attack is detrimental as it can enable
malicious actors to exercise large-scale information manipulation by spreading
harmful but benign-looking system prompts online. To demonstrate such an
attack, we develop CAIN, an algorithm that can automatically curate such
harmful system prompts for a specific target question in a black-box setting or
without the need to access the LLM's parameters. Evaluated on both open-source
and commercial LLMs, CAIN demonstrates significant adversarial impact. In
untargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves
up to 40% F1 degradation on targeted questions while preserving high accuracy
on benign inputs. For targeted attacks or forcing LLMs to output specific
harmful answers, CAIN achieves over 70% F1 scores on these targeted responses
with minimal impact on benign questions. Our results highlight the critical
need for enhanced robustness measures to safeguard the integrity and safety of
LLMs in real-world applications. All source code will be publicly available.

</details>


### [476] [BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization](https://arxiv.org/abs/2505.16640)
*Xueyang Zhou,Guiyao Tie,Guowen Zhang,Hechang Wang,Pan Zhou,Lichao Sun*

Key words: Vision-Language-Action (VLA), 后门攻击, 模型安全, Objective-Decoupled Optimization

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出了BadVLA方法，首次揭示了Vision-Language-Action (VLA)模型的后门漏洞，并展示了其高效攻击性和隐蔽性。

Motivation: 现有的VLA模型在安全方面存在未探索的后门漏洞，尤其是在训练即服务（Training-as-a-Service）模式下，亟需研究相关威胁。

Method: 基于Objective-Decoupled Optimization的两阶段攻击：1）显式特征空间分离，2）条件控制偏差触发。

Result: 在多个VLA基准测试中，BadVLA攻击成功率接近100%，且几乎不影响正常任务性能，同时对抗输入扰动、任务迁移和微调表现鲁棒。

Conclusion: VLA模型存在严重后门漏洞，需重视安全设计。

Abstract: Vision-Language-Action (VLA) models have advanced robotic control by enabling
end-to-end decision-making directly from multimodal inputs. However, their
tightly coupled architectures expose novel security vulnerabilities. Unlike
traditional adversarial perturbations, backdoor attacks represent a stealthier,
persistent, and practically significant threat-particularly under the emerging
Training-as-a-Service paradigm-but remain largely unexplored in the context of
VLA models. To address this gap, we propose BadVLA, a backdoor attack method
based on Objective-Decoupled Optimization, which for the first time exposes the
backdoor vulnerabilities of VLA models. Specifically, it consists of a
two-stage process: (1) explicit feature-space separation to isolate trigger
representations from benign inputs, and (2) conditional control deviations that
activate only in the presence of the trigger, while preserving clean-task
performance. Empirical results on multiple VLA benchmarks demonstrate that
BadVLA consistently achieves near-100% attack success rates with minimal impact
on clean task accuracy. Further analyses confirm its robustness against common
input perturbations, task transfers, and model fine-tuning, underscoring
critical security vulnerabilities in current VLA deployments. Our work offers
the first systematic investigation of backdoor vulnerabilities in VLA models,
highlighting an urgent need for secure and trustworthy embodied model design
practices. We have released the project page at
https://badvla-project.github.io/.

</details>


### [477] [BitHydra: Towards Bit-flip Inference Cost Attack against Large Language Models](https://arxiv.org/abs/2505.16670)
*Xiaobei Yan,Yiming Li,Zhaoxin Fan,Han Qiu,Tianwei Zhang*

Key words: 推理成本攻击,大型语言模型,位翻转,BitHydra

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 本文提出了一种新型的推理成本攻击（'bit-flip inference cost attack'），通过翻转模型参数中的关键位，使受害者模型生成最长输出内容，从而增加推理成本。该方法效率高、可扩展性强。

Motivation: 现有推理成本攻击方法只能通过输入内容间接影响模型输出，效果有限。本文旨在通过直接攻击模型参数，实现更高效的推理成本攻击。

Method: 设计了名为'BitHydra'的方法，通过优化损失函数抑制<EOS>标记概率，并结合高效的关键位搜索算法翻转模型参数中的关键位。

Result: 在11个参数从1.5B到14B的LLM上测试，仅需4个搜索样本和3次位翻转，即可使所有测试提示生成最大长度内容。

Conclusion: BitHydra方法在效率和可扩展性上表现优异，且对未见过的输入具有强迁移性。

Abstract: Large language models (LLMs) have shown impressive capabilities across a wide
range of applications, but their ever-increasing size and resource demands make
them vulnerable to inference cost attacks, where attackers induce victim LLMs
to generate the longest possible output content. In this paper, we revisit
existing inference cost attacks and reveal that these methods can hardly
produce large-scale malicious effects since they are self-targeting, where
attackers are also the users and therefore have to execute attacks solely
through the inputs, whose generated content will be charged by LLMs and can
only directly influence themselves. Motivated by these findings, this paper
introduces a new type of inference cost attacks (dubbed 'bit-flip inference
cost attack') that target the victim model itself rather than its inputs.
Specifically, we design a simple yet effective method (dubbed 'BitHydra') to
effectively flip critical bits of model parameters. This process is guided by a
loss function designed to suppress <EOS> token's probability with an efficient
critical bit search algorithm, thus explicitly defining the attack objective
and enabling effective optimization. We evaluate our method on 11 LLMs ranging
from 1.5B to 14B parameters under both int8 and float16 settings. Experimental
results demonstrate that with just 4 search samples and as few as 3 bit flips,
BitHydra can force 100% of test prompts to reach the maximum generation length
(e.g., 2048 tokens) on representative LLMs such as LLaMA3, highlighting its
efficiency, scalability, and strong transferability across unseen inputs.

</details>


### [478] [Robust LLM Fingerprinting via Domain-Specific Watermarks](https://arxiv.org/abs/2505.16723)
*Thibaud Gloaguen,Robin Staab,Nikola Jovanović,Martin Vechev*

Key words: 开源语言模型, 水印技术, 模型溯源, 指纹识别, 特定领域

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出了针对特定领域的水印技术，用于改进开源语言模型的溯源能力，相比现有方法能更可靠地识别模型来源。

Motivation: 随着开源语言模型能力增强和广泛微调，模型溯源（识别模型来源）成为重要问题，现有技术无法满足实际所有权检测需求。

Method: 提出了特定领域水印技术，仅在特定子领域（如语言或主题）嵌入水印，而非所有生成内容，以提高检测可靠性和水印耐久性。

Result: 评估表明，该方法在统计保证、可控误报率、高检测能力和生成质量保持方面表现优异，指纹隐蔽且适应实际场景变化。

Conclusion: 特定领域水印技术能有效解决模型溯源问题，提供高可靠性和实用性。

Abstract: As open-source language models (OSMs) grow more capable and are widely shared
and finetuned, ensuring model provenance, i.e., identifying the origin of a
given model instance, has become an increasingly important issue. At the same
time, existing backdoor-based model fingerprinting techniques often fall short
of achieving key requirements of real-world model ownership detection. In this
work, we build on the observation that while current open-source model
watermarks fail to achieve reliable content traceability, they can be
effectively adapted to address the challenge of model provenance. To this end,
we introduce the concept of domain-specific watermarking for model
fingerprinting. Rather than watermarking all generated content, we train the
model to embed watermarks only within specified subdomains (e.g., particular
languages or topics). This targeted approach ensures detection reliability,
while improving watermark durability and quality under a range of real-world
deployment settings. Our evaluations show that domain-specific watermarking
enables model fingerprinting with strong statistical guarantees, controllable
false positive rates, high detection power, and preserved generation quality.
Moreover, we find that our fingerprints are inherently stealthy and naturally
robust to real-world variability across deployment scenarios.

</details>


### [479] [When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak Attack on LLMs via Steganographic Techniques](https://arxiv.org/abs/2505.16765)
*Jianing Geng,Biao Yi,Zekun Fei,Tongxi Wu,Lihai Nie,Zheli Liu*

Key words: 越狱攻击,隐写术,LLM安全性,隐蔽性

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 这篇论文提出了一种名为StegoAttack的新型越狱攻击方法，通过隐写术隐藏恶意查询，成功绕过LLM内置及外部安全机制，攻击成功率达92%。

Motivation: 研究现有越狱攻击在毒性和语言隐蔽性方面的不足，提出完全隐蔽的攻击方法以提升模型安全性。

Method: 利用隐写术将有害查询嵌入语义连贯的良性文本中，诱使LLM提取并响应，同时保持自然性和隐蔽性。

Result: 在四种主流LLM上测试，攻击成功率达92%，隐蔽性表现优异，仅在外部分析时成功率下降不到1%。

Conclusion: StegoAttack是一种高效且隐蔽的越狱攻击方法，能有效规避现有安全机制。

Abstract: Jailbreak attacks pose a serious threat to large language models (LLMs) by
bypassing built-in safety mechanisms and leading to harmful outputs. Studying
these attacks is crucial for identifying vulnerabilities and improving model
security. This paper presents a systematic survey of jailbreak methods from the
novel perspective of stealth. We find that existing attacks struggle to
simultaneously achieve toxic stealth (concealing toxic content) and linguistic
stealth (maintaining linguistic naturalness). Motivated by this, we propose
StegoAttack, a fully stealthy jailbreak attack that uses steganography to hide
the harmful query within benign, semantically coherent text. The attack then
prompts the LLM to extract the hidden query and respond in an encrypted manner.
This approach effectively hides malicious intent while preserving naturalness,
allowing it to evade both built-in and external safety mechanisms. We evaluate
StegoAttack on four safety-aligned LLMs from major providers, benchmarking
against eight state-of-the-art methods. StegoAttack achieves an average attack
success rate (ASR) of 92.00%, outperforming the strongest baseline by 11.0%.
Its ASR drops by less than 1% even under external detection (e.g., Llama
Guard). Moreover, it attains the optimal comprehensive scores on stealth
detection metrics, demonstrating both high efficacy and exceptional stealth
capabilities. The code is available at
https://anonymous.4open.science/r/StegoAttack-Jail66

</details>


### [480] [CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of Large Language Models](https://arxiv.org/abs/2505.16785)
*Zhenzhen Ren,GuoBiao Li,Sheng Li,Zhenxing Qian,Xinpeng Zhang*

Key words: LLM, 指纹识别, 思维链, 对比学习, Kullback-Leibler散度

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出了一种名为CoTSRF的LLM指纹识别方案，利用思维链（CoT）作为指纹，通过对比学习训练提取器，再基于Kullback-Leibler散度进行指纹验证，实验证明其具有隐蔽性和鲁棒性。

Motivation: 开源大语言模型（LLMs）存在滥用风险，现有指纹识别方法在隐蔽性和鲁棒性上不足，因此需要一种更有效的解决方案。

Method: CoTSRF通过设计CoT查询收集源LLM响应，利用对比学习训练CoT提取器，提取指纹后通过Kullback-Leibler散度进行验证。

Result: 实验表明CoTSRF在LLM指纹识别中表现优越，尤其在隐蔽和鲁棒的指纹验证方面。

Conclusion: CoTSRF为LLM指纹识别提供了一种新颖且高效的解决方案，解决了隐蔽性和鲁棒性问题。

Abstract: Despite providing superior performance, open-source large language models
(LLMs) are vulnerable to abusive usage. To address this issue, recent works
propose LLM fingerprinting methods to identify the specific source LLMs behind
suspect applications. However, these methods fail to provide stealthy and
robust fingerprint verification. In this paper, we propose a novel LLM
fingerprinting scheme, namely CoTSRF, which utilizes the Chain of Thought (CoT)
as the fingerprint of an LLM. CoTSRF first collects the responses from the
source LLM by querying it with crafted CoT queries. Then, it applies
contrastive learning to train a CoT extractor that extracts the CoT feature
(i.e., fingerprint) from the responses. Finally, CoTSRF conducts fingerprint
verification by comparing the Kullback-Leibler divergence between the CoT
features of the source and suspect LLMs against an empirical threshold. Various
experiments have been conducted to demonstrate the advantage of our proposed
CoTSRF for fingerprinting LLMs, particularly in stealthy and robust fingerprint
verification.

</details>


### [481] [Invisible Prompts, Visible Threats: Malicious Font Injection in External Resources for Large Language Models](https://arxiv.org/abs/2505.16957)
*Junjie Xiong,Changjia Zhu,Shuhang Lin,Chong Zhang,Yongfeng Zhang,Yao Liu,Lingyao Li*

Key words: 大型语言模型, 安全漏洞, 字体注入, MCP协议, 对抗攻击

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文研究了大型语言模型（LLMs）在集成实时网络搜索和MCP协议时可能面临的字体注入攻击漏洞，提出了两种攻击场景并通过实验验证其有效性。

Motivation: 随着LLMs功能扩展（如网络搜索和MCP协议集成），新的安全漏洞可能被引入。研究旨在揭示通过恶意字体注入的隐藏对抗提示漏洞。

Method: 通过恶意字体注入攻击外部资源（如网页），操纵代码到字形映射以注入对用户不可见的欺骗内容，并评估两种攻击场景（恶意内容传递和敏感数据泄漏）。

Result: 实验表明，带有恶意字体的间接提示可通过外部资源绕过LLM的安全机制，成功率因数据敏感性和提示设计而异。

Conclusion: LLM在处理外部内容时亟需增强安全措施以应对此类攻击漏洞。

Abstract: Large Language Models (LLMs) are increasingly equipped with capabilities of
real-time web search and integrated with protocols like Model Context Protocol
(MCP). This extension could introduce new security vulnerabilities. We present
a systematic investigation of LLM vulnerabilities to hidden adversarial prompts
through malicious font injection in external resources like webpages, where
attackers manipulate code-to-glyph mapping to inject deceptive content which
are invisible to users. We evaluate two critical attack scenarios: (1)
"malicious content relay" and (2) "sensitive data leakage" through MCP-enabled
tools. Our experiments reveal that indirect prompts with injected malicious
font can bypass LLM safety mechanisms through external resources, achieving
varying success rates based on data sensitivity and prompt design. Our research
underscores the urgent need for enhanced security measures in LLM deployments
when processing external content.

</details>
