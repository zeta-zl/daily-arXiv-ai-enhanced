<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 102]
- [cs.LG](#cs.LG) [Total: 139]
- [cs.AI](#cs.AI) [Total: 27]
- [cs.RO](#cs.RO) [Total: 13]
- [cs.AR](#cs.AR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 17]
- [cs.SD](#cs.SD) [Total: 9]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DB](#cs.DB) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [stat.CO](#stat.CO) [Total: 2]
- [math.OC](#math.OC) [Total: 3]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.GT](#cs.GT) [Total: 3]
- [math.NA](#math.NA) [Total: 1]
- [cs.CV](#cs.CV) [Total: 72]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.IR](#cs.IR) [Total: 7]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.SE](#cs.SE) [Total: 3]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.CY](#cs.CY) [Total: 11]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [quant-ph](#quant-ph) [Total: 5]
- [eess.IV](#eess.IV) [Total: 10]
- [cs.MA](#cs.MA) [Total: 7]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.SC](#cs.SC) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.NI](#cs.NI) [Total: 3]
- [eess.SY](#eess.SY) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523)
*Chengzhi Liu,Zhongxing Xu,Qingyue Wei,Juncheng Wu,James Zou,Xin Eric Wang,Yuyin Zhou,Sheng Liu*

Key words: 多模态大语言模型,幻觉,推理链,感知保真度,RH-AUC,RH-Bench

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 多模态大语言模型通过增加推理链长度提升性能，但也导致幻觉增加，即模型更依赖语言先验而非视觉输入。研究提出RH-AUC指标和RH-Bench基准，分析推理能力与感知保真度的平衡。

Motivation: 研究动机在于探索多模态大语言模型中推理链长度增加导致的幻觉问题，即模型逐渐忽视视觉输入而依赖语言先验。

Method: 方法包括提出RH-AUC指标量化感知准确性随推理长度的变化，并开发RH-Bench基准评估推理能力与幻觉的权衡。

Result: 结果表明，更大模型通常在推理与感知间取得更好平衡，且平衡更多受训练数据类型和领域影响而非数据量。

Conclusion: 结论强调需要联合评估推理质量与感知保真度的框架。

Abstract: Test-time compute has empowered multimodal large language models to generate
extended reasoning chains, yielding strong performance on tasks such as
multimodal math reasoning. However, this improved reasoning ability often comes
with increased hallucination: as generations become longer, models tend to
drift away from image-grounded content and rely more heavily on language
priors. Attention analysis shows that longer reasoning chains lead to reduced
focus on visual inputs, which contributes to hallucination. To systematically
study this phenomenon, we introduce RH-AUC, a metric that quantifies how a
model's perception accuracy changes with reasoning length, allowing us to
evaluate whether the model preserves visual grounding during reasoning. We also
release RH-Bench, a diagnostic benchmark that spans a variety of multimodal
tasks, designed to assess the trade-off between reasoning ability and
hallucination. Our analysis reveals that (i) larger models typically achieve a
better balance between reasoning and perception, and (ii) this balance is
influenced more by the types and domains of training data than by its overall
volume. These findings underscore the importance of evaluation frameworks that
jointly consider both reasoning quality and perceptual fidelity.

</details>


### [2] [Loquacious Set: 25,000 Hours of Transcribed and Diverse English Speech Recognition Data for Research and Commercial Use](https://arxiv.org/abs/2505.21578)
*Titouan Parcollet,Yuan Tseng,Shucong Zhang,Rogier van Dalen*

Key words: 自动语音识别, ASR, Loquacious Set, 多样化语音数据, 商业可用

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了Loquacious Set，一个25,000小时的商业可用英语语音数据集，旨在解决现有ASR数据集的局限性，如规模小、许可问题或数据质量问题。

Motivation: 现有ASR数据集（如LibriSpeech）在规模、多样性和商业可用性上存在不足，限制了学术界和工业界的研究。

Method: 通过收集和整理25,000小时的多样化英语语音数据（包括不同口音和语音类型），构建了一个名为Loquacious Set的新数据集。

Result: Loquacious Set解决了现有数据集的限制，提供了商业可用、多样化的语音数据，适用于真实场景下的ASR系统开发。

Conclusion: Loquacious Set为ASR研究提供了高质量、多样化的数据集，推动了学术界和工业界的合作与进步。

Abstract: Automatic speech recognition (ASR) research is driven by the availability of
common datasets between industrial researchers and academics, encouraging
comparisons and evaluations. LibriSpeech, despite its long success as an ASR
benchmark, is now limited by its size and focus on clean, read speech, leading
to near-zero word error rates. More recent datasets, including MOSEL, YODAS,
Gigaspeech, OWSM, Libriheavy or People's Speech suffer from major limitations
including licenses that researchers in the industry cannot use, unreliable
transcriptions, incorrect audio data, or the lack of evaluation sets. This work
presents the Loquacious Set, a 25,000-hour curated collection of commercially
usable English speech. Featuring hundreds of thousands of speakers with diverse
accents and a wide range of speech types (read, spontaneous, talks, clean,
noisy), the Loquacious Set is designed to work for academics and researchers in
the industry to build ASR systems in real-world scenarios.

</details>


### [3] [Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives](https://arxiv.org/abs/2505.21598)
*Yajiao Liu,Congliang Chen,Junchi Yang,Ruoyu Sun*

Key words: 数据混合、领域权重、语言模型、离线方法、在线方法

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文综述了不同数据领域采样比例对语言模型性能的影响，提出了现有数据混合方法的细粒度分类，并总结了各方法的优缺点及领域关键挑战。

Motivation: 在有限计算资源下，如何确定不同数据领域的权重以训练出性能最优的模型是一个关键问题，需要系统梳理现有方法并明确其分类与联系。

Method: 论文首先对现有方法进行细粒度分类（离线方法分为启发式、算法式和函数拟合式，在线方法分为在线极小极大优化、在线混合定律等），并总结各子类的公式及代表性算法。

Result: 明确了不同类型方法的关系与区别，梳理了各自优缺点，为数据混合领域的研究提供了系统参考。

Conclusion: 数据混合方法仍需解决关键挑战，如动态调整权重和跨领域泛化，未来研究可结合离线与在线方法的优势。

Abstract: Training large language models with data collected from various domains can
improve their performance on downstream tasks. However, given a fixed training
budget, the sampling proportions of these different domains significantly
impact the model's performance. How can we determine the domain weights across
different data domains to train the best-performing model within constrained
computational resources? In this paper, we provide a comprehensive overview of
existing data mixture methods. First, we propose a fine-grained categorization
of existing methods, extending beyond the previous offline and online
classification. Offline methods are further grouped into heuristic-based,
algorithm-based, and function fitting-based methods. For online methods, we
categorize them into three groups: online min-max optimization, online mixing
law, and other approaches by drawing connections with the optimization
frameworks underlying offline methods. Second, we summarize the problem
formulations, representative algorithms for each subtype of offline and online
methods, and clarify the relationships and distinctions among them. Finally, we
discuss the advantages and disadvantages of each method and highlight key
challenges in the field of data mixture.

</details>


### [4] [R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing](https://arxiv.org/abs/2505.21600)
*Tianyu Fu,Yi Ge,Yichen You,Enshu Liu,Zhihang Yuan,Guohao Dai,Shengen Yan,Huazhong Yang,Yu Wang*

Key words: Large Language Models, Small Language Models, token routing, efficiency, performance

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: R2R introduces a neural token routing method to selectively use LLMs for critical tokens, improving efficiency while maintaining performance.

Motivation: To address the high inference overhead of LLMs while preserving reasoning capabilities, by leveraging the observation that only a few tokens significantly differ between LLMs and SLMs.

Method: Developed Roads to Rome (R2R), a neural token routing method that utilizes LLMs only for path-divergent tokens, and an automatic data generation pipeline for training the router.

Result: R2R, with an average activated parameter size of 5.6B, outperformed R1-7B by 1.6x and achieved a 2.8x speedup compared to R1-32B with comparable performance.

Conclusion: R2R effectively balances efficiency and performance, advancing the Pareto frontier of test-time scaling efficiency.

Abstract: Large Language Models (LLMs) achieve impressive reasoning capabilities at the
cost of substantial inference overhead, posing substantial deployment
challenges. Although distilled Small Language Models (SLMs) significantly
enhance efficiency, their performance suffers as they fail to follow LLMs'
reasoning paths. Luckily, we reveal that only a small fraction of tokens
genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens
are either identical or exhibit neutral differences, such as minor variations
in abbreviations or expressions. Leveraging this insight, we introduce **Roads
to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs
only for these critical, path-divergent tokens, while leaving the majority of
token generation to the SLM. We also develop an automatic data generation
pipeline that identifies divergent tokens and generates token-level routing
labels to train the lightweight router. We apply R2R to combine R1-1.5B and
R1-32B models from the DeepSeek family, and evaluate on challenging math,
coding, and QA benchmarks. With an average activated parameter size of 5.6B,
R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the
R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with
comparable performance, advancing the Pareto frontier of test-time scaling
efficiency. Our code is available at https://github.com/thu-nics/R2R.

</details>


### [5] [How does Misinformation Affect Large Language Model Behaviors and Preferences?](https://arxiv.org/abs/2505.21608)
*Miao Peng,Nuo Chen,Jianheng Tang,Jia Li*

Key words: 大语言模型，信息误区，基准测试，知识冲突，风格变化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究提出了MisBench，一个评估大语言模型（LLMs）对信息误区的行为和知识偏好的最大最全面的基准测试。研究发现，尽管LLMs在识别信息误区方面表现出相当的能力，但仍然容易受到知识冲突和风格变化的影响。研究还提出了一种新的方法Reconstruct to Discriminate（RtD）来增强LLMs检测信息误区的能力。

Motivation: 现有研究虽然探讨了LLMs在对抗信息误区中的作用，但缺乏对LLMs如何以及在何种程度上受信息误区影响的细粒度分析。为了弥补这一空白，本研究旨在提供一个全面的基准测试，以评估LLMs在信息误区面前的脆弱性。

Method: 研究提出了MisBench基准测试，包含10,346,712条信息误区，独特地考虑了知识冲突和风格变化。实验评估了LLMs在这些误区中的表现，并提出了Reconstruct to Discriminate（RtD）方法来增强检测能力。

Result: 实证结果显示，LLMs在识别信息误区方面能力相当，但仍易受知识冲突和风格变化的影响。基于这些发现，提出的RtD方法有效提升了LLMs的检测能力。

Conclusion: 该研究为LLMs与信息误区的互动提供了宝贵见解，MisBench可作为评估基于LLM的检测器并提升其在现实应用中的可靠性的有效基准。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in
knowledge-intensive tasks, while they remain vulnerable when encountering
misinformation. Existing studies have explored the role of LLMs in combating
misinformation, but there is still a lack of fine-grained analysis on the
specific aspects and extent to which LLMs are influenced by misinformation. To
bridge this gap, we present MisBench, the current largest and most
comprehensive benchmark for evaluating LLMs' behavior and knowledge preference
toward misinformation. MisBench consists of 10,346,712 pieces of
misinformation, which uniquely considers both knowledge-based conflicts and
stylistic variations in misinformation. Empirical results reveal that while
LLMs demonstrate comparable abilities in discerning misinformation, they still
remain susceptible to knowledge conflicts and stylistic variations. Based on
these findings, we further propose a novel approach called Reconstruct to
Discriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our
study provides valuable insights into LLMs' interactions with misinformation,
and we believe MisBench can serve as an effective benchmark for evaluating
LLM-based detectors and enhancing their reliability in real-world applications.
Codes and data are available at https://github.com/GKNL/MisBench.

</details>


### [6] [Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts](https://arxiv.org/abs/2505.21646)
*Lei Zhang,Markus Stricker*

Key words: 材料发现, Word2Vec, 电催化, 组合爆炸, 语料库优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种通过迭代优化科学文献语料库并结合Word2Vec模型来预测高性能材料的框架，成功应用于氧还原、氢析出和氧析出反应的材料筛选，并通过实验验证。

Motivation: 解决材料设计中由于组合爆炸导致的数据稀缺问题，充分利用科学文献中的潜在知识。

Method: 采用迭代框架，通过选择多样化的文献训练Word2Vec模型，并监控嵌入空间中成分-性能关联的收敛。

Result: 成功预测了高性能材料组合，并通过实验验证了其电催化性能。

Conclusion: 迭代语料库优化方法可加速材料发现与优化，为数据稀缺的大规模组合空间提供高效筛选工具。

Abstract: The discovery and optimization of materials for specific applications is
hampered by the practically infinite number of possible elemental combinations
and associated properties, also known as the `combinatorial explosion'. By
nature of the problem, data are scarce and all possible data sources should be
used. In addition to simulations and experimental results, the latent knowledge
in scientific texts is not yet used to its full potential. We present an
iterative framework that refines a given scientific corpus by strategic
selection of the most diverse documents, training Word2Vec models, and
monitoring the convergence of composition-property correlations in embedding
space. Our approach is applied to predict high-performing materials for oxygen
reduction (ORR), hydrogen evolution (HER), and oxygen evolution (OER) reactions
for a large number of possible candidate compositions. Our method successfully
predicts the highest performing compositions among a large pool of candidates,
validated by experimental measurements of the electrocatalytic performance in
the lab. This work demonstrates and validates the potential of iterative corpus
refinement to accelerate materials discovery and optimization, offering a
scalable and efficient tool for screening large compositional spaces where
reliable data are scarce or non-existent.

</details>


### [7] [Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations](https://arxiv.org/abs/2505.21657)
*Zeinab Dehghani,Koorosh Aslansefat,Adil Khan,Mohammed Naveed Akram*

Key words: SMILE, 大型语言模型, 可解释性, 热图, 透明度

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SMILE是一种新方法，通过微调输入并测量输出变化来解释大型语言模型的响应机制，生成热图突出显示提示中最关键的部分，从而增强模型透明度。

Motivation: 大型语言模型如GPT、LLAMA和Claude虽强大但缺乏透明度，尤其在需要信任和责任的领域，这成为问题。

Method: SMILE通过微调输入、测量输出变化，并生成热图来可视化提示中影响最大的部分，同时评估准确性、一致性、稳定性和保真度。

Result: 在多个领先的LLM上测试表明，SMILE能提供清晰可靠的解释。

Conclusion: SMILE通过提升模型可解释性，使AI更透明可信。

Abstract: Large language models like GPT, LLAMA, and Claude have become incredibly
powerful at generating text, but they are still black boxes, so it is hard to
understand how they decide what to say. That lack of transparency can be
problematic, especially in fields where trust and accountability matter. To
help with this, we introduce SMILE, a new method that explains how these models
respond to different parts of a prompt. SMILE is model-agnostic and works by
slightly changing the input, measuring how the output changes, and then
highlighting which words had the most impact. Create simple visual heat maps
showing which parts of a prompt matter the most. We tested SMILE on several
leading LLMs and used metrics such as accuracy, consistency, stability, and
fidelity to show that it gives clear and reliable explanations. By making these
models easier to understand, SMILE brings us one step closer to making AI more
transparent and trustworthy.

</details>


### [8] [Rethinking the Outlier Distribution in Large Language Models: An In-depth Study](https://arxiv.org/abs/2505.21670)
*Rahul Raman,Khushi Sharma,Sai Qian Zhang*

Key words: 大型语言模型；异常值；量化；压缩；边缘计算

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLM）中的异常值，探讨了其对模型性能的影响，并提出了一些有效的消除方法。

Motivation: 由于异常值对LLM的量化与压缩等性能有显著影响，研究其形成机制并解决这一问题可以提升模型的量化效率与准确性。

Method: 对大型语言模型中的大量激活与通道级异常值进行深入分析，并提出了一些潜在的消除策略。

Result: 提出了高效的消除方法，能够显著减少异常值对模型精度的影响。

Conclusion: 研究为提升LLM量化效果提供了新思路，对边缘设备部署具有重要意义。

Abstract: Investigating outliers in large language models (LLMs) is crucial due to
their significant impact on various aspects of LLM performance, including
quantization and compression. Outliers often cause considerable quantization
errors, leading to degraded model performance. Identifying and addressing these
outliers can enhance the accuracy and efficiency of the quantization process,
enabling smoother deployment on edge devices or specialized hardware. Recent
studies have identified two common types of outliers in LLMs: massive
activations and channel-wise outliers. While numerous quantization algorithms
have been proposed to mitigate their effects and maintain satisfactory
accuracy, few have thoroughly explored the root causes of these outliers in
depth. In this paper, we conduct a comprehensive investigation into the
formation mechanisms of these outliers and propose potential strategies to
mitigate their occurrence. Ultimately, we introduce some efficient approaches
to eliminate most massive activations and channel-wise outliers with minimal
impact on accuracy.

</details>


### [9] [LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model](https://arxiv.org/abs/2505.21689)
*Avijit Gayen,Somyajit Chakraborty,Mainak Sen,Soham Paul,Angshuman Jana*

Key words: 法律请愿排序，大型语言模型，机器学习，司法效率，自动化优先级

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种基于大型语言模型的法律请愿书自动排序框架（LLMPR），通过结合文本嵌入和定量指标，显著提高了排序效率和准确性。

Motivation: 解决印度司法系统中未决案件的长期积累问题，减少人工排序的低效和主观偏见，促进公正和及时的司法裁决。

Method: 使用ILDC数据集（7,593份带标注的请愿书），结合DistilBERT、LegalBERT等文本嵌入技术及定量指标（如间隔天数、排名分数等），训练多种机器学习模型（如随机森林、决策树等）。

Result: 实验显示，随机森林和决策树模型表现最佳，准确率超过99%，斯皮尔曼等级相关系数达0.99；仅使用数值特征的模型也可达到接近最优的排序效果（R2 = 0.988）。

Conclusion: 自动化排序能有效优化司法流程，减少案件积压，提升公正性；LLM嵌入技术虽然效果有限，但为未来研究提供了方向。

Abstract: The persistent accumulation of unresolved legal cases, especially within the
Indian judiciary, significantly hampers the timely delivery of justice. Manual
methods of prioritizing petitions are often prone to inefficiencies and
subjective biases further exacerbating delays. To address this issue, we
propose LLMPR (Large Language Model-based Petition Ranking), an automated
framework that utilizes transfer learning and machine learning to assign
priority rankings to legal petitions based on their contextual urgency.
Leveraging the ILDC dataset comprising 7,593 annotated petitions, we process
unstructured legal text and extract features through various embedding
techniques, including DistilBERT, LegalBERT, and MiniLM. These textual
embeddings are combined with quantitative indicators such as gap days, rank
scores, and word counts to train multiple machine learning models, including
Random Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments
demonstrate that Random Forest and Decision Tree models yield superior
performance, with accuracy exceeding 99% and a Spearman rank correlation of
0.99. Notably, models using only numerical features achieve nearly optimal
ranking results (R2 = 0.988, \r{ho} = 0.998), while LLM-based embeddings offer
only marginal gains. These findings suggest that automated petition ranking can
effectively streamline judicial workflows, reduce case backlog, and improve
fairness in legal prioritization.

</details>


### [10] [MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs](https://arxiv.org/abs/2505.21693)
*Raoyuan Zhao,Beiduo Chen,Barbara Plank,Michael A. Hedderich*

Key words: large language models, cultural awareness, multilingual evaluation, bias, MAKIEval

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: This paper introduces MAKIEval, a multilingual framework for evaluating cultural awareness in large language models (LLMs) across languages, regions, and topics, addressing biases in English-centric models.

Motivation: The study aims to tackle cross-lingual disparities and biases in LLMs, especially due to their English-centric pretraining, which often leads to culturally unaware outputs in other languages.

Method: MAKIEval uses Wikidata’s multilingual structure to automatically identify cultural entities in LLM outputs and links them to structured knowledge, enabling scalable evaluation. Four metrics (granularity, diversity, cultural specificity, consensus) are introduced to assess cultural awareness.

Result: Evaluation of 7 LLMs across 13 languages, 19 regions, and 6 topics revealed that models show stronger cultural awareness in English, suggesting English prompts better activate cultural knowledge.

Conclusion: MAKIEval provides a scalable, language-agnostic tool for assessing cultural awareness in LLMs, highlighting the need for more balanced multilingual training.

Abstract: Large language models (LLMs) are used globally across many languages, but
their English-centric pretraining raises concerns about cross-lingual
disparities for cultural awareness, often resulting in biased outputs. However,
comprehensive multilingual evaluation remains challenging due to limited
benchmarks and questionable translation quality. To better assess these
disparities, we introduce MAKIEval, an automatic multilingual framework for
evaluating cultural awareness in LLMs across languages, regions, and topics.
MAKIEval evaluates open-ended text generation, capturing how models express
culturally grounded knowledge in natural language. Leveraging Wikidata's
multilingual structure as a cross-lingual anchor, it automatically identifies
cultural entities in model outputs and links them to structured knowledge,
enabling scalable, language-agnostic evaluation without manual annotation or
translation. We then introduce four metrics that capture complementary
dimensions of cultural awareness: granularity, diversity, cultural specificity,
and consensus across languages. We assess 7 LLMs developed from different parts
of the world, encompassing both open-source and proprietary systems, across 13
languages, 19 countries and regions, and 6 culturally salient topics (e.g.,
food, clothing). Notably, we find that models tend to exhibit stronger cultural
awareness in English, suggesting that English prompts more effectively activate
culturally grounded knowledge. We publicly release our code and data.

</details>


### [11] [Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing](https://arxiv.org/abs/2505.21701)
*Raoyuan Zhao,Abdullatif Köksal,Ali Modarressi,Michael A. Hedderich,Hinrich Schütze*

Key words: 大型语言模型, 知识缺口探测, 不一致性, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 摘要探讨了大型语言模型（LLMs）的知识缺口探测方法的不一致性问题，提出了基于输入变化和定量指标的评估过程，揭示了探测方法在相同模型、数据集和提示下的不一致性。

Motivation: 由于LLMs存在幻觉问题，精确识别其知识缺口至关重要，但现有探测方法的可靠性受到不一致性的挑战。

Method: 通过输入变化和定量指标，评估探测方法的一致性和鲁棒性。

Result: 发现探测方法在内部（如提示微小变化）和跨方法之间存在显著不一致，揭示了现有方法的局限性。

Conclusion: 研究结果强调了开发对抗扰动的鲁棒探测框架的紧迫性。

Abstract: The reliability of large language models (LLMs) is greatly compromised by
their tendency to hallucinate, underscoring the need for precise identification
of knowledge gaps within LLMs. Various methods for probing such gaps exist,
ranging from calibration-based to prompting-based methods. To evaluate these
probing methods, in this paper, we propose a new process based on using input
variations and quantitative metrics. Through this, we expose two dimensions of
inconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal
non-semantic perturbations in prompts lead to considerable variance in detected
knowledge gaps within the same probing method; e.g., the simple variation of
shuffling answer options can decrease agreement to around 40%. (2) Cross-method
inconsistency: Probing methods contradict each other on whether a model knows
the answer. Methods are highly inconsistent -- with decision consistency across
methods being as low as 7% -- even though the model, dataset, and prompt are
all the same. These findings challenge existing probing methods and highlight
the urgent need for perturbation-robust probing frameworks.

</details>


### [12] [Assessing and Refining ChatGPT's Performance in Identifying Targeting and Inappropriate Language: A Comparative Study](https://arxiv.org/abs/2505.21710)
*Barbarestani Baran,Maks Isa,Vossen Piek*

Key words: ChatGPT, 内容审核, 不当语言检测, 针对性语言, AI模型优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 评估ChatGPT在识别在线评论中的不当及针对性语言的效果，显示其在检测不当内容上表现良好，但在针对性语言检测上存在变异性。

Motivation: 研究AI模型在内容审核中的潜力，尤其是在处理海量用户生成内容时，如何提升自动化系统的效果。

Method: 对比ChatGPT与人工标注（众包和专家评估），评估其准确性、检测范围及一致性。

Result: ChatGPT检测不当内容表现优异（尤其是迭代优化的V6版本），但在针对性语言检测中假阳性率较高。

Conclusion: AI模型（如ChatGPT）可增强自动化内容审核系统，但需持续优化上下文理解能力以减少误判。

Abstract: This study evaluates the effectiveness of ChatGPT, an advanced AI model for
natural language processing, in identifying targeting and inappropriate
language in online comments. With the increasing challenge of moderating vast
volumes of user-generated content on social network sites, the role of AI in
content moderation has gained prominence. We compared ChatGPT's performance
against crowd-sourced annotations and expert evaluations to assess its
accuracy, scope of detection, and consistency. Our findings highlight that
ChatGPT performs well in detecting inappropriate content, showing notable
improvements in accuracy through iterative refinements, particularly in Version
6. However, its performance in targeting language detection showed variability,
with higher false positive rates compared to expert judgments. This study
contributes to the field by demonstrating the potential of AI models like
ChatGPT to enhance automated content moderation systems while also identifying
areas for further improvement. The results underscore the importance of
continuous model refinement and contextual understanding to better support
automated moderation and mitigate harmful online behavior.

</details>


### [13] [Counterfactual Simulatability of LLM Explanations for Generation Tasks](https://arxiv.org/abs/2505.21740)
*Marvin Limpijankit,Yanda Chen,Melanie Subbiah,Nicholas Deas,Kathleen McKeown*

Key words: LLMs, 反事实可模拟性, 新闻摘要, 医疗建议, 解释评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了LLMs解释行为的不可预测性，提出了一种基于反事实可模拟性的通用框架来评估其解释效果，发现该方法在生成任务中表现不一。

Motivation: LLMs的不可预测性导致其行为解释在高风险场景中至关重要，现有评估方法仅适用于二元问答任务，需扩展至生成任务。

Method: 采用了反事实可模拟性框架，应用于新闻摘要和医疗建议任务，评估用户预测模型输出的能力。

Result: 新闻摘要任务中LLM解释有效帮助用户预测，但在医疗建议中效果有限；评估方法更适用于技能型任务而非知识型任务。

Conclusion: 反事实可模拟性框架适用于生成任务，但需进一步改进，尤其在知识型任务中。

Abstract: LLMs can be unpredictable, as even slight alterations to the prompt can cause
the output to change in unexpected ways. Thus, the ability of models to
accurately explain their behavior is critical, especially in high-stakes
settings. One approach for evaluating explanations is counterfactual
simulatability, how well an explanation allows users to infer the model's
output on related counterfactuals. Counterfactual simulatability has been
previously studied for yes/no question answering tasks. We provide a general
framework for extending this method to generation tasks, using news
summarization and medical suggestion as example use cases. We find that while
LLM explanations do enable users to better predict LLM outputs on
counterfactuals in the summarization setting, there is significant room for
improvement for medical suggestion. Furthermore, our results suggest that the
evaluation for counterfactual simulatability may be more appropriate for
skill-based tasks as opposed to knowledge-based tasks.

</details>


### [14] [BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the Proactivity Spectrum](https://arxiv.org/abs/2505.21757)
*Yubin Kim,Zhiyuan Hu,Hyewon Jeong,Eugene Park,Shuyue Stella Li,Chanwoo Park,Shiyun Xiong,MingYu Lu,Hyeonhoon Lee,Xin Liu,Daniel McDuff,Cynthia Breazeal,Samir Tulebaev,Hae Won Park*

Key words: 大型语言模型, 临床代理, 行为适应性, 主动性任务, BehaviorBench, BehaviorSFT

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了BehaviorBench数据集用于评估LLM在临床辅助任务中的行为表现，并提出BehaviorSFT训练策略以提升LLM的主动性与行为适应性，实验显示其显著提高了性能与临床实用性。

Motivation: LLM在临床任务中虽擅长反应性任务（如诊断推理），但缺乏主动行为（如识别关键缺失信息）。需开发方法提升其主动性与行为适应性。

Method: 提出BehaviorSFT训练策略，通过行为标记显式调整LLM行为，使其能在反应性与主动性任务间动态切换。

Result: BehaviorSFT使LLM在BehaviorBench上Macro F1达97.3%，主动任务分数提升（如Qwen2.5-7B-Ins从95.0%升至96.5%），临床评估显示其行为更接近现实需求。

Conclusion: BehaviorSFT能有效提升LLM在临床任务中的主动性与行为平衡，优于传统微调或显式指令方法。

Abstract: Large Language Models (LLMs) as clinical agents require careful behavioral
adaptation. While adept at reactive tasks (e.g., diagnosis reasoning), LLMs
often struggle with proactive engagement, like unprompted identification of
critical missing information or risks. We introduce BehaviorBench, a
comprehensive dataset to evaluate agent behaviors across a clinical assistance
spectrum, ranging from reactive query responses to proactive interventions
(e.g., clarifying ambiguities, flagging overlooked critical data). Our
BehaviorBench experiments reveal LLMs' inconsistent proactivity. To address
this, we propose BehaviorSFT, a novel training strategy using behavioral tokens
to explicitly condition LLMs for dynamic behavioral selection along this
spectrum. BehaviorSFT boosts performance, achieving up to 97.3% overall Macro
F1 on BehaviorBench and improving proactive task scores (e.g., from 95.0% to
96.5% for Qwen2.5-7B-Ins). Crucially, blind clinician evaluations confirmed
BehaviorSFT-trained agents exhibit more realistic clinical behavior, striking a
superior balance between helpful proactivity (e.g., timely, relevant
suggestions) and necessary restraint (e.g., avoiding over-intervention) versus
standard fine-tuning or explicit instructed agents.

</details>


### [15] [Calibrating LLM Confidence by Probing Perturbed Representation Stability](https://arxiv.org/abs/2505.21772)
*Reza Khanmohammadi,Erfan Miahi,Mehrsa Mardikoraem,Simerjot Kaur,Ivan Brugere,Charese H. Smiley,Kundan Thind,Mohammad M. Ghassemi*

Key words: 大语言模型, 置信度校准, 对抗扰动, 表示稳定性, 预期校准误差

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: CCPS是一种新方法，通过分析大语言模型内部表示的稳定性来校准其置信度，显著降低了校准误差并提高了准确性。

Motivation: 大语言模型的校准不足影响了其可靠性，因此需要一种更准确的置信度估计方法。

Method: CCPS通过对最终隐藏状态进行有针对性的对抗扰动，提取反映模型响应的特征，并使用轻量级分类器预测答案正确性。

Result: 在多个大语言模型和评测基准上，CCPS将预期校准误差降低了约55%，同时提高了多项性能指标。

Conclusion: CCPS提供了一种高效、广泛适用且更准确的估计大语言模型置信度的方法，从而增强了其可信赖性。

Abstract: Miscalibration in Large Language Models (LLMs) undermines their reliability,
highlighting the need for accurate confidence estimation. We introduce CCPS
(Calibrating LLM Confidence by Probing Perturbed Representation Stability), a
novel method analyzing internal representational stability in LLMs. CCPS
applies targeted adversarial perturbations to final hidden states, extracts
features reflecting the model's response to these perturbations, and uses a
lightweight classifier to predict answer correctness. CCPS was evaluated on
LLMs from 8B to 32B parameters (covering Llama, Qwen, and Mistral
architectures) using MMLU and MMLU-Pro benchmarks in both multiple-choice and
open-ended formats. Our results show that CCPS significantly outperforms
current approaches. Across four LLMs and three MMLU variants, CCPS reduces
Expected Calibration Error by approximately 55% and Brier score by 21%, while
increasing accuracy by 5 percentage points, Area Under the Precision-Recall
Curve by 4 percentage points, and Area Under the Receiver Operating
Characteristic Curve by 6 percentage points, all relative to the strongest
prior method. CCPS delivers an efficient, broadly applicable, and more accurate
solution for estimating LLM confidence, thereby improving their
trustworthiness.

</details>


### [16] [GMU Systems for the IWSLT 2025 Low-Resource Speech Translation Shared Task](https://arxiv.org/abs/2505.21781)
*Chutong Meng,Antonios Anastasopoulos*

Key words: IWSLT, speech translation, low-resource, SeamlessM4T-v2, fine-tuning

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: GMU系统在IWSLT 2025低资源语音翻译任务中表现优秀，使用了SeamlessM4T-v2进行ASR、MT和E2E ST的微调，并探索了多种训练范式。

Motivation: 为了在低资源条件下提升语音翻译性能，尤其是在未训练语言上提高表现。

Method: 使用SeamlessM4T-v2微调ASR、MT和E2E ST模型，探索了直接微调、多任务训练和参数初始化等方法。

Result: 直接E2E微调效果最好；ASR编码器初始化提升了未训练语言的ST表现；多任务训练略有帮助。

Conclusion: 直接E2E微调是最有效的低资源ST方法，ASR编码器初始化对未训练语言有显著提升。

Abstract: This paper describes the GMU systems for the IWSLT 2025 low-resource speech
translation shared task. We trained systems for all language pairs, except for
Levantine Arabic. We fine-tuned SeamlessM4T-v2 for automatic speech recognition
(ASR), machine translation (MT), and end-to-end speech translation (E2E ST).
The ASR and MT models are also used to form cascaded ST systems. Additionally,
we explored various training paradigms for E2E ST fine-tuning, including direct
E2E fine-tuning, multi-task training, and parameter initialization using
components from fine-tuned ASR and/or MT models. Our results show that (1)
direct E2E fine-tuning yields strong results; (2) initializing with a
fine-tuned ASR encoder improves ST performance on languages SeamlessM4T-v2 has
not been trained on; (3) multi-task training can be slightly helpful.

</details>


### [17] [VeriTrail: Closed-Domain Hallucination Detection with Traceability](https://arxiv.org/abs/2505.21786)
*Dasha Metropolitansky,Jonathan Larson*

Key words: 封闭域幻觉,多步骤生成过程,可追溯性,VeriTrail,语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究提出VeriTrail，首个针对多步骤生成过程（MGS）和单步骤生成过程（SGS）的封闭域幻觉检测方法，并提供内容可追溯性。研究还引入了包含中间输出和人工标注的新数据集，并证明VeriTrail优于基线方法。

Motivation: 语言模型在生成内容时存在封闭域幻觉现象，尤其在多步骤生成过程中风险更高。仅检测最终输出不足以解决问题，需追踪幻觉内容的引入及中间输出的忠实性。

Method: 提出VeriTrail方法，设计用于多步骤和单步骤生成过程的封闭域幻觉检测，提供内容可追溯性。同时创建包含中间输出和人工标注的新数据集。

Result: VeriTrail在两种数据集上均表现优于基线方法，验证了其有效性。

Conclusion: VeriTrail为封闭域幻觉检测提供了可追溯的解决方案，尤其在多步骤生成过程中表现出色。

Abstract: Even when instructed to adhere to source material, Language Models often
generate unsubstantiated content - a phenomenon known as "closed-domain
hallucination." This risk is amplified in processes with multiple generative
steps (MGS), compared to processes with a single generative step (SGS).
However, due to the greater complexity of MGS processes, we argue that
detecting hallucinations in their final outputs is necessary but not
sufficient: it is equally important to trace where hallucinated content was
likely introduced and how faithful content may have been derived from the
source through intermediate outputs. To address this need, we present
VeriTrail, the first closed-domain hallucination detection method designed to
provide traceability for both MGS and SGS processes. We also introduce the
first datasets to include all intermediate outputs as well as human annotations
of final outputs' faithfulness for their respective MGS processes. We
demonstrate that VeriTrail outperforms baseline methods on both datasets.

</details>


### [18] [Revisiting Common Assumptions about Arabic Dialects in NLP](https://arxiv.org/abs/2505.21816)
*Amr Keleg,Sharon Goldwater,Walid Magdy*

Key words: 阿拉伯语方言、NLP、方言识别、多标签数据集、语言假设

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 阿拉伯语方言多样性研究揭示，广泛接受的方言区域分组假设过于简化，可能阻碍阿拉伯语NLP任务的进展。

Motivation: 验证阿拉伯语方言处理中的常见假设，因为这些假设未被量化验证，可能限制NLP任务的发展。

Method: 通过扩展和分析多标签数据集，手动评估11个国家级别方言的句子有效性。

Result: 分析表明，四种假设过于简化现实，某些情况下并不准确。

Conclusion: 当前的阿拉伯语方言假设可能阻碍NLP进展，需要更精细的模型。

Abstract: Arabic has diverse dialects, where one dialect can be substantially different
from the others. In the NLP literature, some assumptions about these dialects
are widely adopted (e.g., ``Arabic dialects can be grouped into distinguishable
regional dialects") and are manifested in different computational tasks such as
Arabic Dialect Identification (ADI). However, these assumptions are not
quantitatively verified. We identify four of these assumptions and examine them
by extending and analyzing a multi-label dataset, where the validity of each
sentence in 11 different country-level dialects is manually assessed by
speakers of these dialects. Our analysis indicates that the four assumptions
oversimplify reality, and some of them are not always accurate. This in turn
might be hindering further progress in different Arabic NLP tasks.

</details>


### [19] [Representative Language Generation](https://arxiv.org/abs/2505.21819)
*Charlotte Peale,Vinod Raman,Omer Reingold*

Key words: 代表性生成, 多样性, 偏见, 群体闭包维度, 生成模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了'代表性生成'的概念，扩展了现有的生成框架，以解决生成模型的多样性和偏见问题。通过引入'群体闭包维度'这一关键组合量，作者分析了信息理论和计算层面的可行性，并证明了某些条件下的负面结果。

Motivation: 旨在解决生成模型中的多样性和偏见问题，确保输出能按比例代表训练数据中的感兴趣群体。

Method: 引入'代表性生成'概念，提出'群体闭包维度'作为关键指标，分析了无限假设类和群体集合下的信息理论和计算可行性。

Result: 在某些条件下可以实现无限假设类的代表性生成，但仅通过成员查询无法实现可计算的代表性生成。

Conclusion: 研究为开发更多样和代表性的生成模型提供了理论基础。

Abstract: We introduce "representative generation," extending the theoretical framework
for generation proposed by Kleinberg et al. (2024) and formalized by Li et al.
(2024), to additionally address diversity and bias concerns in generative
models. Our notion requires outputs of a generative model to proportionally
represent groups of interest from the training data. We characterize
representative uniform and non-uniform generation, introducing the "group
closure dimension" as a key combinatorial quantity. For representative
generation in the limit, we analyze both information-theoretic and
computational aspects, demonstrating feasibility for countably infinite
hypothesis classes and collections of groups under certain conditions, but
proving a negative result for computability using only membership queries. This
contrasts with Kleinberg et al.'s (2024) positive results for standard
generation in the limit. Our findings provide a rigorous foundation for
developing more diverse and representative generative models.

</details>


### [20] [Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries](https://arxiv.org/abs/2505.21859)
*Vishakh Padmakumar,Zichao Wang,David Arbour,Jennifer Healey*

Key words: 多文档摘要, LLMs, DPP, 源覆盖率, 个性化摘要

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出一个三步法改进多文档摘要的源覆盖范围：提取关键点、DPP选择多样化内容、重写生成摘要，并展示了在DiverseSumm基准上的提升。

Motivation: 解决LLMs在长上下文中注意力不均衡（‘中间丢失’现象）导致的多文档摘要源覆盖不足问题。

Method: 1. 提取原子关键点；2. 用DPP选择多样化关键点；3. 重写为最终摘要，并结合用户意图实现个性化。

Result: 在DiverseSumm基准上，不同LLMs的源覆盖均得到一致提升，且能生成保留覆盖率的个性化摘要。

Conclusion: 分步方法（提取+选择+重写）结合DPP显著提升多文档摘要的覆盖率和个性化能力。

Abstract: While large language models (LLMs) are increasingly capable of handling
longer contexts, recent work has demonstrated that they exhibit the "lost in
the middle" phenomenon (Liu et al., 2024) of unevenly attending to different
parts of the provided context. This hinders their ability to cover diverse
source material in multi-document summarization, as noted in the DiverseSumm
benchmark (Huang et al., 2024). In this work, we contend that principled
content selection is a simple way to increase source coverage on this task. As
opposed to prompting an LLM to perform the summarization in a single step, we
explicitly divide the task into three steps -- (1) reducing document
collections to atomic key points, (2) using determinantal point processes (DPP)
to perform select key points that prioritize diverse content, and (3) rewriting
to the final summary. By combining prompting steps, for extraction and
rewriting, with principled techniques, for content selection, we consistently
improve source coverage on the DiverseSumm benchmark across various LLMs.
Finally, we also show that by incorporating relevance to a provided user intent
into the DPP kernel, we can generate personalized summaries that cover relevant
source information while retaining coverage.

</details>


### [21] [Evaluating the Retrieval Robustness of Large Language Models](https://arxiv.org/abs/2505.21870)
*Shuyang Cao,Karthik Radhakrishnan,David Rosenberg,Steven Lu,Pengxiang Cheng,Lu Wang,Shiyue Zhang*

Key words: 检索增强生成（RAG）、大规模语言模型（LLMs）、检索鲁棒性、开放域问题、提示策略

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究评估了在大规模语言模型（LLMs）中检索增强生成（RAG）的鲁棒性，探讨了RAG是否总是优于非RAG、更多检索文档是否总是提升性能以及文档顺序是否影响结果。研究发现，尽管LLMs表现出较高的检索鲁棒性，但仍存在局限性。

Motivation: 研究动机在于探索RAG在知识密集型任务中的实际效果，尤其是其可能因检索不完美或模型利用能力有限导致的性能下降问题。

Method: 研究通过建立包含1500个开放域问题的基准数据集，引入三个鲁棒性指标对应三个研究问题，并在11种LLMs和3种提示策略下进行实验。

Result: 实验发现，所有LLMs均显示出较高的检索鲁棒性，但不同程度的鲁棒性缺陷限制了它们充分利用RAG的优势。

Conclusion: 尽管LLMs在RAG中表现鲁棒，但检索质量和模型利用能力仍需改进以最大化RAG的潜力。

Abstract: Retrieval-augmented generation (RAG) generally enhances large language
models' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also
lead to performance degradation due to imperfect retrieval and the model's
limited ability to leverage retrieved content. In this work, we evaluate the
robustness of LLMs in practical RAG setups (henceforth retrieval robustness).
We focus on three research questions: (1) whether RAG is always better than
non-RAG; (2) whether more retrieved documents always lead to better
performance; (3) and whether document orders impact results. To facilitate this
study, we establish a benchmark of 1500 open-domain questions, each with
retrieved documents from Wikipedia. We introduce three robustness metrics, each
corresponds to one research question. Our comprehensive experiments, involving
11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit
surprisingly high retrieval robustness; nonetheless, different degrees of
imperfect robustness hinders them from fully utilizing the benefits of RAG.

</details>


### [22] [EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse](https://arxiv.org/abs/2505.21889)
*Tianyu Guo,Hande Dong,Yichong Leng,Feng Liu,Cheater Lin,Nong Xiao,Xianwei Zhang*

Key words: 大型语言模型、填充任务、KV缓存重用、EFIM、片段标记化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出EFIM，一种改进的提示格式，结合片段标记化训练方法，显著提升KV缓存重用效率，降低延迟并提高吞吐量。

Motivation: 当前大型语言模型在填充任务中，由于提示格式的结构问题，KV缓存重用效率低下，导致计算资源浪费。

Method: 提出EFIM提示格式，结合片段标记化训练方法，优化KV缓存重用。

Result: 实验显示，EFIM能降低52%延迟，提升98%吞吐量，同时保持原有填充能力。

Conclusion: EFIM有效解决KV缓存重用问题，显著提升填充任务效率。

Abstract: Large language models (LLMs) are often used for infilling tasks, which
involve predicting or generating missing information in a given text. These
tasks typically require multiple interactions with similar context. To reduce
the computation of repeated historical tokens, cross-request key-value (KV)
cache reuse, a technique that stores and reuses intermediate computations, has
become a crucial method in multi-round interactive services. However, in
infilling tasks, the KV cache reuse is often hindered by the structure of the
prompt format, which typically consists of a prefix and suffix relative to the
insertion point. Specifically, the KV cache of the prefix or suffix part is
frequently invalidated as the other part (suffix or prefix) is incrementally
generated. To address the issue, we propose EFIM, a transformed prompt format
of FIM to unleash the performance potential of KV cache reuse. Although the
transformed prompt can solve the inefficiency, it exposes subtoken generation
problems in current LLMs, where they have difficulty generating partial words
accurately. Therefore, we introduce a fragment tokenization training method
which splits text into multiple fragments before tokenization during data
processing. Experiments on two representative LLMs show that LLM serving with
EFIM can lower the latency by 52% and improve the throughput by 98% while
maintaining the original infilling capability.EFIM's source code is publicly
available at https://github.com/gty111/EFIM.

</details>


### [23] [Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development](https://arxiv.org/abs/2505.21898)
*Rennai Qiu,Chen Qian,Ran Li,Yufan Dang,Weize Chen,Cheng Yang,Yingli Zhang,Ye Tian,Xuantang Xiong,Lei Han,Zhiyuan Liu,Maosong Sun*

Key words: 多代理系统，资源感知，快捷方式，令牌优化，代码质量

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种资源感知的多代理系统Co-Saving，通过学习历史成功轨迹引入‘快捷方式’，显著减少令牌使用50.85%，并提升代码质量10.06%。

Motivation: 当前多代理系统在复杂任务中资源效率低下，缺乏资源感知能力。论文旨在通过协作机制优化资源使用和质量提升。

Method: 引入了基于历史经验的‘快捷方式’机制，绕过冗余推理代理，优化协作过程。

Result: 实验显示Co-Saving在软件任务中令牌使用减少50.85%，代码质量提升10.06%。

Conclusion: Co-Saving通过资源感知机制显著提升了多代理系统的效率和结果质量。

Abstract: Recent advancements in Large Language Models (LLMs) and autonomous agents
have demonstrated remarkable capabilities across various domains. However,
standalone agents frequently encounter limitations when handling complex tasks
that demand extensive interactions and substantial computational resources.
Although Multi-Agent Systems (MAS) alleviate some of these limitations through
collaborative mechanisms like task decomposition, iterative communication, and
role specialization, they typically remain resource-unaware, incurring
significant inefficiencies due to high token consumption and excessive
execution time. To address these limitations, we propose a resource-aware
multi-agent system -- Co-Saving (meaning that multiple agents collaboratively
engage in resource-saving activities), which leverages experiential knowledge
to enhance operational efficiency and solution quality. Our key innovation is
the introduction of "shortcuts" -- instructional transitions learned from
historically successful trajectories -- which allows to bypass redundant
reasoning agents and expedite the collective problem-solving process.
Experiments for software development tasks demonstrate significant advantages
over existing methods. Specifically, compared to the state-of-the-art MAS
ChatDev, our method achieves an average reduction of 50.85% in token usage, and
improves the overall code quality by 10.06%.

</details>


### [24] [Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning](https://arxiv.org/abs/2505.21926)
*Yin Hua,Zhiqiang Liu,Mingyang Chen,Zheng Fang,Chi Man Wong,Lingxiao Li,Chi Man Vong,Huajun Chen,Wen Zhang*

Key words: 知识图谱,基础模型,推理,多模态,条件消息传递

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了MERRY，一个用于通用知识图谱推理的基础模型，结合结构和文本信息，通过多视角条件消息传递编码架构和动态残差融合模块，在28个数据集上表现优于现有基线。

Motivation: 当前知识图谱基础模型主要关注结构信息，忽视了文本信息，限制了在更具挑战性的非图谱任务中的进展。

Method: 提出MERRY模型，包括多视角条件消息传递编码架构、动态残差融合模块和灵活的边评分机制，整合结构和文本模态。

Result: 在28个数据集上评估，MERRY在大多数场景下优于现有基线，展现出强大的图谱内推理能力和良好的非图谱任务泛化性。

Conclusion: MERRY通过结合结构和文本信息，显著提升了知识图谱推理能力，尤其是在非图谱任务中表现突出。

Abstract: In natural language processing (NLP) and computer vision (CV), the successful
application of foundation models across diverse tasks has demonstrated their
remarkable potential. However, despite the rich structural and textual
information embedded in knowledge graphs (KGs), existing research of foundation
model for KG has primarily focused on their structural aspects, with most
efforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This
limitation has hindered progress in addressing more challenging out-of-KG
tasks. In this paper, we introduce MERRY, a foundation model for general
knowledge graph reasoning, and investigate its performance across two task
categories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG
question answering, KGQA). We not only utilize the structural information, but
also the textual information in KGs. Specifically, we propose a
multi-perspective Conditional Message Passing (CMP) encoding architecture to
bridge the gap between textual and structural modalities, enabling their
seamless integration. Additionally, we introduce a dynamic residual fusion
module to selectively retain relevant textual information and a flexible edge
scoring mechanism to adapt to diverse downstream tasks. Comprehensive
evaluations on 28 datasets demonstrate that MERRY outperforms existing
baselines in most scenarios, showcasing strong reasoning capabilities within
KGs and excellent generalization to out-of-KG tasks such as KGQA.

</details>


### [25] [RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments](https://arxiv.org/abs/2505.21936)
*Zeyi Liao,Jaylen Jones,Linxi Jiang,Eric Fosler-Lussier,Yu Su,Zhiqiang Lin,Huan Sun*

Key words: 计算机代理, 间接提示注入, 混合沙箱, 对抗测试, 漏洞基准

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: RedTeamCUA是一个对抗测试框架，通过混合沙箱环境（VM+Docker）和RTC-Bench基准测试，揭示了当前计算机代理（CUA）在间接提示注入攻击下的高漏洞率（最高48%），强调实际部署前需强化防御。

Motivation: 现有CUA在混合Web-OS攻击场景中缺乏真实可控的评估环境，导致间接提示注入漏洞未被充分研究。

Method: 提出RedTeamCUA框架，结合VM和Docker的混合沙箱，支持灵活对抗场景配置和RTC-Bench基准（864个测试案例）。

Result: Claude 3.7 Sonnet漏洞率42.9%，Operator最低7.6%；Claude 4 Opus达48%，攻击尝试率最高92.5%。

Conclusion: 间接提示注入对先进CUA仍具高风险，需系统性漏洞分析及防御机制。

Abstract: Computer-use agents (CUAs) promise to automate complex tasks across operating
systems (OS) and the web, but remain vulnerable to indirect prompt injection.
Current evaluations of this threat either lack support realistic but controlled
environments or ignore hybrid web-OS attack scenarios involving both
interfaces. To address this, we propose RedTeamCUA, an adversarial testing
framework featuring a novel hybrid sandbox that integrates a VM-based OS
environment with Docker-based web platforms. Our sandbox supports key features
tailored for red teaming, such as flexible adversarial scenario configuration,
and a setting that decouples adversarial evaluation from navigational
limitations of CUAs by initializing tests directly at the point of an
adversarial injection. Using RedTeamCUA, we develop RTC-Bench, a comprehensive
benchmark with 864 examples that investigate realistic, hybrid web-OS attack
scenarios and fundamental security vulnerabilities. Benchmarking current
frontier CUAs identifies significant vulnerabilities: Claude 3.7 Sonnet | CUA
demonstrates an ASR of 42.9%, while Operator, the most secure CUA evaluated,
still exhibits an ASR of 7.6%. Notably, CUAs often attempt to execute
adversarial tasks with an Attempt Rate as high as 92.5%, although failing to
complete them due to capability limitations. Nevertheless, we observe
concerning ASRs of up to 50% in realistic end-to-end settings, with the
recently released frontier Claude 4 Opus | CUA showing an alarming ASR of 48%,
demonstrating that indirect prompt injection presents tangible risks for even
advanced CUAs despite their capabilities and safeguards. Overall, RedTeamCUA
provides an essential framework for advancing realistic, controlled, and
systematic analysis of CUA vulnerabilities, highlighting the urgent need for
robust defenses to indirect prompt injection prior to real-world deployment.

</details>


### [26] [Graph-Assisted Culturally Adaptable Idiomatic Translation for Indic Languages](https://arxiv.org/abs/2505.21937)
*Pratik Rakesh Singh,Kritarth Prasad,Mohammadi Zaki,Pankaj Wasnik*

Key words: 多词表达, 习语翻译, 自适应图神经网络, 知识图谱, 文化差异

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种基于自适应图神经网络（GNN）的方法IdiomCE，用于解决多词表达（MWEs）和习语的翻译问题，尤其在文化差异和上下文变化的挑战下表现优异。

Motivation: 多词表达和习语的翻译因文化差异和上下文多变而复杂，传统静态知识图谱和提示方法难以捕捉这些复杂关系，导致翻译效果不佳。

Method: 论文提出的IdiomCE方法采用自适应图神经网络（GNN），学习习语表达间的复杂映射，并能泛化到训练中未见的节点。

Result: 在多个习语翻译数据集上的实验表明，IdiomCE在资源受限环境下仍能显著提升翻译质量，尤其在英语到多种印度语言的翻译中表现突出。

Conclusion: IdiomCE方法通过自适应GNN有效解决了习语翻译中的文化差异和上下文问题，提升了翻译质量，尤其适用于小模型。

Abstract: Translating multi-word expressions (MWEs) and idioms requires a deep
understanding of the cultural nuances of both the source and target languages.
This challenge is further amplified by the one-to-many nature of idiomatic
translations, where a single source idiom can have multiple target-language
equivalents depending on cultural references and contextual variations.
Traditional static knowledge graphs (KGs) and prompt-based approaches struggle
to capture these complex relationships, often leading to suboptimal
translations. To address this, we propose IdiomCE, an adaptive graph neural
network (GNN) based methodology that learns intricate mappings between
idiomatic expressions, effectively generalizing to both seen and unseen nodes
during training. Our proposed method enhances translation quality even in
resource-constrained settings, facilitating improved idiomatic translation in
smaller models. We evaluate our approach on multiple idiomatic translation
datasets using reference-less metrics, demonstrating significant improvements
in translating idioms from English to various Indian languages.

</details>


### [27] [RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering](https://arxiv.org/abs/2505.21940)
*Bolei He,Xinran He,Mengke Chen,Xianwei Xue,Ying Zhu,Zhenhua Ling*

Key words: 大型语言模型,多跳问答,检索增强生成,推理增强,迭代自探索

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种名为RISE的新框架，通过迭代自探索增强大型语言模型在复杂推理任务（如多跳问答MHQA）中的表现。RISE包含问题分解、检索-阅读和自我批判三个关键步骤，显著提升了模型的推理准确性和任务性能。

Motivation: 尽管大型语言模型在许多领域表现出色，但在复杂推理任务（如多跳问答MHQA）中仍面临挑战，尤其是整合多样证据和管理逻辑依赖时易出错。现有方法（如检索增强生成RAG）在过滤噪声数据和检索必要证据方面效果有限，因此需要更有效的方法来提升推理能力。

Method: RISE框架通过迭代自探索提升推理能力，包括三个关键步骤：1)问题分解，将复杂问题拆解为子问题；2)检索-阅读，获取并整合相关证据；3)自我批判，通过迭代优化推理路径和逻辑一致性。

Result: 在多个MHQA基准测试上的实验表明，RISE显著提高了推理准确性和任务性能。

Conclusion: RISE通过迭代自探索有效增强了模型在多跳问答任务中的推理能力，为复杂推理问题提供了新的解决方案。

Abstract: Large Language Models (LLMs) excel in many areas but continue to face
challenges with complex reasoning tasks, such as Multi-Hop Question Answering
(MHQA). MHQA requires integrating evidence from diverse sources while managing
intricate logical dependencies, often leads to errors in reasoning.
Retrieval-Augmented Generation (RAG), widely employed in MHQA tasks, faces
challenges in effectively filtering noisy data and retrieving all necessary
evidence, thereby limiting its effectiveness in addressing MHQA challenges. To
address these challenges, we propose RISE:Reasoning Enhancement via Iterative
Self-Exploration, a novel framework designed to enhance models' reasoning
capability through iterative self-exploration. Specifically, RISE involves
three key steps in addressing MHQA tasks: question decomposition,
retrieve-then-read, and self-critique. By leveraging continuous
self-exploration, RISE identifies accurate reasoning paths, iteratively
self-improving the model's capability to integrate evidence, maintain logical
consistency, and enhance performance in MHQA tasks. Extensive experiments on
multiple MHQA benchmarks demonstrate that RISE significantly improves reasoning
accuracy and task performance.

</details>


### [28] [Test-Time Scaling with Repeated Sampling Improves Multilingual Text Generation](https://arxiv.org/abs/2505.21941)
*Ashim Gupta,Vivek Srikumar*

Key words: 重复采样,双语生成,推理任务,验证器,困惑度,奖励

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了推理时通过重复采样在双语生成任务中的效果，发现在某些情况下质量提升超过35%，并强调了选择正确验证器的重要性。

Motivation: 研究重复采样在双语生成任务中的有效性，尤其是在推理任务中，因为这方面此前研究较少。

Method: 使用基于困惑度和奖励的验证器在两个双语基准测试（Aya Evaluation Suite和m-ArenaHard）上评估重复采样的效果。

Result: 结果显示，重复采样在双语文本生成中带来一致的质量提升，某些情况下提升超过35%。基于困惑度的评分在开放式提示中有效，而基于奖励的验证器在需要推理的任务（如数学、代码）中表现更好。

Conclusion: 研究表明重复采样在双语文本生成中具有广泛实用性，并强调根据任务选择合适验证器的重要性。

Abstract: Inference-time scaling via repeated sampling has shown promise in reasoning
tasks, but its effectiveness in multilingual generation remains underexplored.
We evaluate this approach using perplexity- and reward-based verifiers on two
multilingual benchmarks: the Aya Evaluation Suite and m-ArenaHard. Our results
show consistent quality improvements, with gains exceeding 35% in some cases.
While perplexity-based scoring is effective for open-ended prompts, only
reward-based verifiers improve performance on tasks requiring reasoning (e.g.,
math, code). Our results demonstrate the broader utility of repeated sampling
for multilingual text generation and underscore the importance of selecting
right verifiers for the task.

</details>


### [29] [Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning](https://arxiv.org/abs/2505.21958)
*Qihuang Zhong,Liang Ding,Fei Liao,Juhua Liu,Bo Du,Dacheng Tao*

Key words: 大型语言模型、指令调优、知识冲突、数据选择、知识感知、医学领域

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种名为KDS的知识感知数据选择框架，用于优化领域特定指令调优数据集，以减少知识冲突并提升大型语言模型（LLM）的性能。

Motivation: 领域特定指令调优数据中可能存在冗余或低质量数据，且现有数据选择方法忽略了知识冲突的影响，这可能导致模型能力下降和幻觉问题。因此，需要一种更有效的数据选择方法。

Method: 采用知识感知指标（上下文-记忆知识对齐和记忆内知识一致性）定量衡量知识冲突，通过过滤高冲突数据并选择高质量和多样化的数据，优化指令调优数据集。

Result: 在医学领域实验中，KDS显著优于其他基线方法，提升了模型性能、泛化能力，并缓解了幻觉问题。

Conclusion: KDS通过知识感知的数据选择有效提升了领域特定指令调优的效果，展示了其在解决知识冲突和提升模型性能方面的潜力。

Abstract: Domain-specific instruction-tuning has become the defacto standard for
improving the performance of large language models (LLMs) in specialized
applications, e.g., medical question answering. Since the instruction-tuning
dataset might contain redundant or low-quality data, data selection (DS) is
usually required to maximize the data efficiency. Despite the successes in the
general domain, current DS methods often struggle to select the desired data
for domain-specific instruction-tuning. One of the main reasons is that they
neglect the impact of knowledge conflicts, i.e., the discrepancy between LLMs'
pretrained knowledge and context knowledge of instruction data, which could
damage LLMs' prior abilities and lead to hallucination. To this end, we propose
a simple-yet-effective Knowledge-aware Data Selection (namely KDS) framework to
select the domain-specific instruction-tuning data that meets LLMs' actual
needs. The core of KDS is to leverage two knowledge-aware metrics for
quantitatively measuring knowledge conflicts from two aspects: context-memory
knowledge alignment and intra-memory knowledge consistency. By filtering the
data with large knowledge conflicts and sampling the high-quality and diverse
data, KDS can effectively stimulate the LLMs' abilities and achieve better
domain-specific performance. Taking the medical domain as the testbed, we
conduct extensive experiments and empirically prove that KDS surpasses the
other baselines and brings significant and consistent performance gains among
all LLMs. More encouragingly, KDS effectively improves the model generalization
and alleviates the hallucination problem.

</details>


### [30] [LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents](https://arxiv.org/abs/2505.21963)
*Taro Yano,Yoichi Ishibashi,Masafumi Oyamada*

Key words: LLMs, post-training, LaMDAgent, automation, pipeline optimization

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LaMDAgent是一种新型框架，通过LLM代理自主构建和优化后训练流程，提高精度，降低人工干预。

Motivation: 现有的后训练方法多为手动设计或单组件优化，缺乏自动化的全流程探索。

Method: LaMDAgent利用LLM代理，系统地探索模型生成技术、数据集和超参数配置，基于任务反馈优化流程。

Result: 实验显示LaMDAgent将工具使用精度提高9.0分，并发现传统方法易忽略的高效策略。

Conclusion: LaMDAgent能高效发现高性能后训练流程，数据规模扩展更具成本效益。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
a wide range of tasks. To further tailor LLMs to specific domains or
applications, post-training techniques such as Supervised Fine-Tuning (SFT),
Preference Learning, and model merging are commonly employed. While each of
these methods has been extensively studied in isolation, the automated
construction of complete post-training pipelines remains an underexplored area.
Existing approaches typically rely on manual design or focus narrowly on
optimizing individual components, such as data ordering or merging strategies.
In this work, we introduce LaMDAgent (short for Language Model Developing
Agent), a novel framework that autonomously constructs and optimizes full
post-training pipelines through the use of LLM-based agents. LaMDAgent
systematically explores diverse model generation techniques, datasets, and
hyperparameter configurations, leveraging task-based feedback to discover
high-performing pipelines with minimal human intervention. Our experiments show
that LaMDAgent improves tool-use accuracy by 9.0 points while preserving
instruction-following capabilities. Moreover, it uncovers effective
post-training strategies that are often overlooked by conventional human-driven
exploration. We further analyze the impact of data and model size scaling to
reduce computational costs on the exploration, finding that model size scalings
introduces new challenges, whereas scaling data size enables cost-effective
pipeline discovery.

</details>


### [31] [Offset Unlearning for Large Language Models](https://arxiv.org/abs/2404.11045)
*James Y. Huang,Wenxuan Zhou,Fei Wang,Fred Morstatter,Sheng Zhang,Hoifung Poon,Muhao Chen*

Key words: 大型语言模型,反学习,黑盒模型,数据隐私,伦理问题

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种针对黑盒大型语言模型（LLMs）的‘反学习’框架{\delta}-Unlearning，通过对比一对较小模型的logit偏移量实现目标数据的遗忘，避免了传统方法需要访问模型内部权重或保留敏感数据的问题。

Motivation: 解决黑盒LLMs因训练数据中包含敏感信息（如版权、偏见和隐私内容）而引发的伦理和法律问题，同时弥补现有反学习技术对黑盒模型不适用或违反数据保护原则的不足。

Method: 提出{\delta}-Unlearning框架，通过训练一对较小模型生成logit偏移量，无需调整黑盒LLM本身即可实现目标数据的遗忘。

Result: 实验表明该方法能有效遗忘目标数据，同时在非遗忘任务上保持甚至优于原模型的性能，且兼容多种反学习算法。

Conclusion: {\delta}-Unlearning为黑盒LLMs提供了一种通用且高效的反学习解决方案，兼顾实用性与数据保护合规性。

Abstract: Despite the strong capabilities of Large Language Models (LLMs) to acquire
knowledge from their training corpora, the memorization of sensitive
information in the corpora such as copyrighted, biased, and private content has
led to ethical and legal concerns. In response to these challenges, unlearning
has emerged as a potential remedy for LLMs affected by problematic training
data. However, previous unlearning techniques are either not applicable to
black-box LLMs due to required access to model internal weights, or violate
data protection principles by retaining sensitive data for inference-time
correction. We propose {\delta}-Unlearning, an offset unlearning framework for
black-box LLMs. Instead of tuning the black-box LLM itself, {\delta}-Unlearning
learns the logit offset needed for unlearning by contrasting the logits from a
pair of smaller models. Experiments demonstrate that {\delta}- Unlearning can
effectively unlearn target data while maintaining similar or even stronger
performance on general out-of-forget-scope tasks. {\delta}-Unlearning also
effectively incorporates different unlearning algorithms, making our approach a
versatile solution to adapting various existing unlearning algorithms to
black-box LLMs.

</details>


### [32] [Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack](https://arxiv.org/abs/2505.21967)
*Juan Ren,Mark Dras,Usman Naseem*

Key words: 大型视觉语言模型, 对抗攻击, 安全性评估, 多模态系统, 规范模式

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文分析了大型视觉语言模型（LVLMs）的安全漏洞，提出了一个两阶段评估框架来评估对抗攻击的有效性，并定义了面对有害提示时的理想模型行为。

Motivation: 大型视觉语言模型在多模态任务中表现出色，但视觉输入的引入扩大了攻击面，导致新的安全漏洞。本研究旨在揭示传统对抗攻击如何绕过LVLMs的安全机制，并提出改进方法。

Method: 1. 通过系统性表征分析揭示传统攻击绕过安全机制的原因。2. 提出两阶段评估框架：第一阶段区分指令不遵从、直接拒绝和成功攻击；第二阶段量化模型输出满足对抗提示有害意图的程度，并分类拒绝行为。3. 提出理想化模型行为的规范模式。

Result: 研究发现对抗攻击可以绕过LVLMs的安全机制，并提出了一个有效评估攻击的框架和理想行为的规范模式。

Conclusion: 论文强调了对LVLMs安全性的重要性，并提供了一个评估框架和规范模式，为多模态系统的安全对齐提供了原则性目标。

Abstract: Large Vision-Language Models (LVLMs) have shown remarkable capabilities
across a wide range of multimodal tasks. However, their integration of visual
inputs introduces expanded attack surfaces, thereby exposing them to novel
security vulnerabilities. In this work, we conduct a systematic
representational analysis to uncover why conventional adversarial attacks can
circumvent the safety mechanisms embedded in LVLMs. We further propose a novel
two stage evaluation framework for adversarial attacks on LVLMs. The first
stage differentiates among instruction non compliance, outright refusal, and
successful adversarial exploitation. The second stage quantifies the degree to
which the model's output fulfills the harmful intent of the adversarial prompt,
while categorizing refusal behavior into direct refusals, soft refusals, and
partial refusals that remain inadvertently helpful. Finally, we introduce a
normative schema that defines idealized model behavior when confronted with
harmful prompts, offering a principled target for safety alignment in
multimodal systems.

</details>


### [33] [Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset](https://arxiv.org/abs/2505.21979)
*Fakhraddin Alwajih,Samar Mohamed Magdy,Abdellah El Mekki,Omer Nacar,Youssef Nafea,Safaa Taher Abdelfadil,Abdulfattah Mohammed Yahya,Hamzah Luqman,Nada Almarwani,Samah Aloufi,Baraah Qawasmeh,Houdaifa Atou,Serry Sibaee,Hamzah A. Alsayadi,Walid Al-Dhabyani,Maged S. Al-shaibani,Aya El aatar,Nour Qandos,Rahaf Alhamouri,Samar Ahmad,Razan Khassib,Lina Hamad,Mohammed Anwar AL-Ghrawi,Fatimah Alshamari,Cheikh Malainine,Doaa Qawasmeh,Aminetou Yacoub,Tfeil moilid,Ruwa AbuHweidi,Ahmed Aboeitta,Vatimetou Mohamed Lemin,Reem Abdel-Salam,Ahlam Bashiti,Adel Ammar,Aisha Alansari,Ahmed Ashraf,Nora Alturayeif,Sara Shatnawi,Alcides Alcoba Inciarte,AbdelRahim A. Elmadany,Mohamedou cheikh tourad,Ismail Berrada,Mustafa Jarrar,Shady Shehata,Muhammad Abdul-Mageed*

Key words: 大型视觉语言模型，文化偏见，多模态数据集，阿拉伯文化，指令对齐

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了Pearl，一个专为文化理解设计的大规模阿拉伯多模态数据集和基准，通过先进的工作流程和人工标注构建，评估显示其能显著提升模型的文化适应性。

Motivation: 主流大型视觉语言模型（LVLMs）存在文化偏见，需要多元化的多模态数据集来解决这一问题。

Method: 通过高级代理工作流程和45位阿拉伯世界标注者的人工标注，构建了涵盖10个文化领域的多模态数据集Pearl，并设计了Pearl和Pearl-Lite两个评估基准。

Result: 评估表明，以推理为中心的指令对齐相比传统扩展方法显著提升了模型的文化基础。

Conclusion: Pearl为推进文化感知的多模态建模研究提供了基础资源，数据集和基准已公开。

Abstract: Mainstream large vision-language models (LVLMs) inherently encode cultural
biases, highlighting the need for diverse multimodal datasets. To address this
gap, we introduce Pearl, a large-scale Arabic multimodal dataset and benchmark
explicitly designed for cultural understanding. Constructed through advanced
agentic workflows and extensive human-in-the-loop annotations by 45 annotators
from across the Arab world, Pearl comprises over K multimodal examples spanning
ten culturally significant domains covering all Arab countries. We further
provide two robust evaluation benchmarks Pearl and Pearl-Lite along with a
specialized subset Pearl-X explicitly developed to assess nuanced cultural
variations. Comprehensive evaluations on state-of-the-art open and proprietary
LVLMs demonstrate that reasoning-centric instruction alignment substantially
improves models' cultural grounding compared to conventional scaling methods.
Pearl establishes a foundational resource for advancing culturally-informed
multimodal modeling research. All datasets and benchmarks are publicly
available.

</details>


### [34] [Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data](https://arxiv.org/abs/2505.21997)
*Jihong Zhang,Xinya Liang,Anqi Deng,Nicole Bonge,Lin Tan,Ling Zhang,Nicole Zarrett*

Key words: 混合方法研究, 大型语言模型, 合成数据, 调查回应, 质性数据

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨了通过大型语言模型(LLMs)结合质性数据生成合成调查回应的可行性，发现LLMs能捕捉人类回应模式但变异性较低，访谈数据可提升部分模型的多样性，提示设计对对齐准确性影响显著。

Motivation: 解决混合方法研究中定量与定性数据对齐的挑战，利用LLMs生成基于质性数据的合成调查回应。

Method: 使用BREQ问卷和课后项目员工访谈数据，测试LLMs在提示指导下的调查回应预测能力。

Result: LLMs能重现人类回应模式但变异性不足，访谈数据提升GPT和Claude等模型的多样性，提示设计和温度设置影响对齐效果。

Conclusion: 访谈引导的LLMs有潜力弥合质性-量化方法鸿沟，但需优化提示设计、偏误消除和模型设置以提升数据效度。

Abstract: Mixed methods research integrates quantitative and qualitative data but faces
challenges in aligning their distinct structures, particularly in examining
measurement characteristics and individual response patterns. Advances in large
language models (LLMs) offer promising solutions by generating synthetic survey
responses informed by qualitative data. This study investigates whether LLMs,
guided by personal interviews, can reliably predict human survey responses,
using the Behavioral Regulations in Exercise Questionnaire (BREQ) and
interviews from after-school program staff as a case study. Results indicate
that LLMs capture overall response patterns but exhibit lower variability than
humans. Incorporating interview data improves response diversity for some
models (e.g., Claude, GPT), while well-crafted prompts and low-temperature
settings enhance alignment between LLM and human responses. Demographic
information had less impact than interview content on alignment accuracy. These
findings underscore the potential of interview-informed LLMs to bridge
qualitative and quantitative methodologies while revealing limitations in
response variability, emotional interpretation, and psychometric fidelity.
Future research should refine prompt design, explore bias mitigation, and
optimize model settings to enhance the validity of LLM-generated survey data in
social science research.

</details>


### [35] [Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate](https://arxiv.org/abs/2505.21999)
*Ashim Gupta,Maitrey Mehta,Zhichao Xu,Vivek Srikumar*

Key words: 大型语言模型, 跨语言一致性, 翻译后评估, 多语言评估, 开放生成

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种评估大型语言模型（LLM）跨语言一致性的框架，通过翻译后评估策略检测其响应的一致性和性能差异，揭示了多语言能力中的关键弱点。

Motivation: 研究动机是评估LLM在不同语言中的响应一致性，避免依赖昂贵的人工标注数据集，并解决开放生成任务评估的难题。

Method: 提出基于“翻译后评估”的框架，从信息和情感两个维度评估模型在30种语言中的一致性。

Result: 结果发现LLM在不同语系和文字中存在显著的响应不一致性，某些语言表现严重不足。

Conclusion: 结论强调跨语言评估需多维度一致性，并建议使用该框架进行未来多语言LLM基准测试。

Abstract: Large language models (LLMs) provide detailed and impressive responses to
queries in English. However, are they really consistent at responding to the
same query in other languages? The popular way of evaluating for multilingual
performance of LLMs requires expensive-to-collect annotated datasets. Further,
evaluating for tasks like open-ended generation, where multiple correct answers
may exist, is nontrivial. Instead, we propose to evaluate the predictability of
model response across different languages. In this work, we propose a framework
to evaluate LLM's cross-lingual consistency based on a simple Translate then
Evaluate strategy. We instantiate this evaluation framework along two
dimensions of consistency: information and empathy. Our results reveal
pronounced inconsistencies in popular LLM responses across thirty languages,
with severe performance deficits in certain language families and scripts,
underscoring critical weaknesses in their multilingual capabilities. These
findings necessitate cross-lingual evaluations that are consistent along
multiple dimensions. We invite practitioners to use our framework for future
multilingual LLM benchmarking.

</details>


### [36] [Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance](https://arxiv.org/abs/2505.22003)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Ali Imam Abidi*

Key words: Legal Assist AI, LLMs, Indian law, transformer-based model, legal Question-Answering

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Legal Assist AI, a transformer-based model, was developed to address India's legal information gap by providing accurate, domain-specific legal assistance using LLMs, outperforming existing models like GPT-3.5 Turbo in legal reasoning.

Motivation: The lack of accessible legal assistance and awareness in India motivated the creation of Legal Assist AI to bridge this gap and empower citizens with legal knowledge.

Method: The model was fine-tuned on Indian legal datasets (e.g., Constitution, BNS, BNSS) to specialize in legal Question-Answering, leveraging LLMs for information retrieval and response generation.

Result: Legal Assist AI achieved a 60.08% score on the AIBE, surpassing competitors like GPT-3.5 Turbo in accuracy and avoiding hallucinations, proving its reliability for real-world use.

Conclusion: The model demonstrates strong potential for practical legal applications, with future goals including performance improvements and broader multilingual and case-specific coverage.

Abstract: Pursuit of accessible legal assistance in India faces a critical gap, as many
citizens struggle to leverage their legal rights due to limited awareness and
access to relevant legal information. This paper introduces Legal Assist AI, a
transformer-based model designed to bridge this gap by offering effective legal
assistance through large language models (LLMs). The system retrieves relevant
legal information from a curated database and generates accurate responses,
enabling effective assistance for diverse users, including legal professionals,
scholars, and the general public. The model was fine-tuned on extensive
datasets from the Indian legal domain, including Indian Constitution, Bharatiya
Nyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth,
providing a robust understanding of the complexities of Indian law. By
incorporating domain-specific legal datasets, the proposed model demonstrated
remarkable efficiency and specialization in legal Question-Answering. The model
was evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral
7B, achieving a 60.08% score on the AIBE, outperforming its competitors in
legal reasoning and accuracy. Unlike other models, Legal Assist AI avoided
common issues such as hallucinations, making it highly reliable for practical
legal applications. It showcases the model's applicability in real-world legal
scenarios, with future iterations aiming to enhance performance and expand its
dataset to cover a broader range of multilingual and case-specific queries as
well.

</details>


### [37] [CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models](https://arxiv.org/abs/2505.22017)
*Siqi Fan,Peng Han,Shuo Shang,Yequan Wang,Aixin Sun*

Key words: 大语言模型，推理效率，token效率，CoThink，动态调整

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: CoThink方法通过动态调整推理深度，减少22.3%的token生成，同时保持准确率。

Motivation: 解决推理优化模型在处理简单问题时过度思考导致的token效率低下问题。

Method: 提出CoThink流程：指令模型生成高层解决方案框架，推理模型补充细节。

Result: 在三个数据集上，token生成减少22.3%，准确率下降仅0.42%。

Conclusion: CoThink能动态匹配输入难度与推理深度，并提出潜在的推理效率缩放定律。

Abstract: Large language models (LLMs) benefit from increased test-time compute, a
phenomenon known as test-time scaling. However, reasoning-optimized models
often overthink even simple problems, producing excessively verbose outputs and
leading to low token efficiency. By comparing these models with equally sized
instruct models, we identify two key causes of this verbosity: (1)
reinforcement learning reduces the information density of forward reasoning,
and (2) backward chain-of thought training encourages redundant and often
unnecessary verification steps. Since LLMs cannot assess the difficulty of a
given problem, they tend to apply the same cautious reasoning strategy across
all tasks, resulting in inefficient overthinking. To address this, we propose
CoThink, an embarrassingly simple pipeline: an instruct model first drafts a
high-level solution outline; a reasoning model then works out the solution. We
observe that CoThink enables dynamic adjustment of reasoning depth based on
input difficulty. Evaluated with three reasoning models DAPO, DeepSeek-R1, and
QwQ on three datasets GSM8K, MATH500, and AIME24, CoThink reduces total token
generation by 22.3% while maintaining pass@1 accuracy within a 0.42% margin on
average. With reference to the instruct model, we formally define reasoning
efficiency and observe a potential reasoning efficiency scaling law in LLMs.

</details>


### [38] [Improving Continual Pre-training Through Seamless Data Packing](https://arxiv.org/abs/2505.22018)
*Ruicheng Yin,Xuan Gao,Changze Lv,Xiaohua Wang,Xiaoqing Zheng,Xuanjing Huang*

Key words: 持续预训练, 数据打包, 滑动窗口, FFD算法, 模型性能

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种名为'无缝打包'（SP）的新数据打包策略，通过滑动窗口和FFD算法减少截断和填充，提升预训练模型性能。

Motivation: 当前固定长度序列的数据打包方法导致截断和上下文不连贯，影响了模型性能。

Method: 采用两阶段方法：1）滑动窗口确保上下文连贯性；2）FFD算法优化填充和截断问题。

Result: 在99%的实验场景中优于基线方法。

Conclusion: SP方法显著提升了模型性能，尤其在上下文连贯性和数据效率上表现突出。

Abstract: Continual pre-training has demonstrated significant potential in enhancing
model performance, particularly in domain-specific scenarios. The most common
approach for packing data before continual pre-training involves concatenating
input texts and splitting them into fixed-length sequences. While
straightforward and efficient, this method often leads to excessive truncation
and context discontinuity, which can hinder model performance. To address these
issues, we explore the potential of data engineering to enhance continual
pre-training, particularly its impact on model performance and efficiency. We
propose Seamless Packing (SP), a novel data packing strategy aimed at
preserving contextual information more effectively and enhancing model
performance. Our approach employs a sliding window technique in the first stage
that synchronizes overlapping tokens across consecutive sequences, ensuring
better continuity and contextual coherence. In the second stage, we adopt a
First-Fit-Decreasing algorithm to pack shorter texts into bins slightly larger
than the target sequence length, thereby minimizing padding and truncation.
Empirical evaluations across various model architectures and corpus domains
demonstrate the effectiveness of our method, outperforming baseline method in
99% of all settings. Code is available at
https://github.com/Infernus-WIND/Seamless-Packing.

</details>


### [39] [VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning](https://arxiv.org/abs/2505.22019)
*Qiuchen Wang,Ruixue Ding,Yu Zeng,Zehui Chen,Lin Chen,Shihang Wang,Pengjun Xie,Fei Huang,Feng Zhao*

Key words: VRAG, 强化学习, 视觉信息检索, 视觉语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: VRAG-RL是一个针对视觉信息检索优化的新型强化学习框架，通过自主采样与优化解决视觉推理不足的问题。

Motivation: 传统文本检索方法无法处理视觉信息，而现有视觉RAG方法因固定流程和模型能力激活不足在推理上表现不佳。

Method: 设计了针对视觉输入的裁剪和缩放动作空间，结合查询重写和检索性能的奖励机制。

Result: 优化了视觉语言模型在视觉信息检索任务中的表现。

Conclusion: VRAG-RL通过强化学习策略有效提升了视觉信息的检索和推理能力。

Abstract: Effectively retrieving, reasoning and understanding visually rich information
remains a challenge for RAG methods. Traditional text-based methods cannot
handle visual-related information. On the other hand, current vision-based RAG
approaches are often limited by fixed pipelines and frequently struggle to
reason effectively due to the insufficient activation of the fundamental
capabilities of models. As RL has been proven to be beneficial for model
reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex
reasoning across visually rich information. With this framework, VLMs interact
with search engines, autonomously sampling single-turn or multi-turn reasoning
trajectories with the help of visual perception tokens and undergoing continual
optimization based on these samples. Our approach highlights key limitations of
RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely
incorporate images into the context, leading to insufficient reasoning token
allocation and neglecting visual-specific perception; and (ii) When models
interact with search engines, their queries often fail to retrieve relevant
information due to the inability to articulate requirements, thereby leading to
suboptimal performance. To address these challenges, we define an action space
tailored for visually rich inputs, with actions including cropping and scaling,
allowing the model to gather information from a coarse-to-fine perspective.
Furthermore, to bridge the gap between users' original inquiries and the
retriever, we employ a simple yet effective reward that integrates query
rewriting and retrieval performance with a model-based reward. Our VRAG-RL
optimizes VLMs for RAG tasks using specially designed RL strategies, aligning
the model with real-world applications. The code is available at
\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}.

</details>


### [40] [Jailbreak Distillation: Renewable Safety Benchmarking](https://arxiv.org/abs/2505.22037)
*Jingyu Zhang,Ahmed Elgohary,Xiawei Wang,A S M Iftekhar,Ahmed Magooda,Benjamin Van Durme,Daniel Khashabi,Kyle Jackson*

Key words: large language models, safety benchmarking, jailbreak attacks, prompt selection, reproducibility

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Jailbreak Distillation (JBDistill) is a framework that transforms jailbreak attacks into high-quality, updatable safety benchmarks for large language models, ensuring fair and reproducible evaluations with minimal human effort.

Motivation: The rapid deployment of large language models (LLMs) in critical applications necessitates robust safety benchmarking to address vulnerabilities like jailbreak attacks. Existing methods lack consistency and adaptability, prompting the need for a sustainable solution.

Method: JBDistill utilizes development models and jailbreak attacks to create a candidate prompt pool, then applies prompt selection algorithms to identify effective subsets for benchmarks, ensuring fair comparisons and reproducibility.

Result: The benchmarks generalize robustly across 13 diverse LLMs, including proprietary and newer-generation models, outperforming existing benchmarks in effectiveness while maintaining high separability and diversity.

Conclusion: JBDistill offers an effective, adaptable, and sustainable solution for safety evaluation, addressing challenges like saturation and contamination in existing methods.

Abstract: Large language models (LLMs) are rapidly deployed in critical applications,
raising urgent needs for robust safety benchmarking. We propose Jailbreak
Distillation (JBDistill), a novel benchmark construction framework that
"distills" jailbreak attacks into high-quality and easily-updatable safety
benchmarks. JBDistill utilizes a small set of development models and existing
jailbreak attack algorithms to create a candidate prompt pool, then employs
prompt selection algorithms to identify an effective subset of prompts as
safety benchmarks. JBDistill addresses challenges in existing safety
evaluation: the use of consistent evaluation prompts across models ensures fair
comparisons and reproducibility. It requires minimal human effort to rerun the
JBDistill pipeline and produce updated benchmarks, alleviating concerns on
saturation and contamination. Extensive experiments demonstrate our benchmarks
generalize robustly to 13 diverse evaluation models held out from benchmark
construction, including proprietary, specialized, and newer-generation LLMs,
significantly outperforming existing safety benchmarks in effectiveness while
maintaining high separability and diversity. Our framework thus provides an
effective, sustainable, and adaptable solution for streamlining safety
evaluation.

</details>


### [41] [Voice Adaptation for Swiss German](https://arxiv.org/abs/2505.22054)
*Samuel Stucki,Jan Deriu,Mark Cieliebak*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究了瑞士德语方言的语音适应模型，通过预处理大量瑞士播客数据并微调XTTSv2模型，展示了在人类和自动化评估中的良好表现。

Motivation: 适应语音克隆技术以服务于 underrepresented 语言（如瑞士德语方言）。

Method: 预处理瑞士播客数据，自动转录并标注方言类别，微调XTTSv2模型。

Result: 在人类和自动化评估中表现出色，CMOS评分-0.28，SMOS评分3.8。

Conclusion: 证明了语音克隆技术可以成功适应 underrepresented 语言。

Abstract: This work investigates the performance of Voice Adaptation models for Swiss
German dialects, i.e., translating Standard German text to Swiss German dialect
speech. For this, we preprocess a large dataset of Swiss podcasts, which we
automatically transcribe and annotate with dialect classes, yielding
approximately 5000 hours of weakly labeled training material. We fine-tune the
XTTSv2 model on this dataset and show that it achieves good scores in human and
automated evaluations and can correctly render the desired dialect. Our work
shows a step towards adapting Voice Cloning technology to underrepresented
languages. The resulting model achieves CMOS scores of up to -0.28 and SMOS
scores of 3.8.

</details>


### [42] [Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?](https://arxiv.org/abs/2505.22061)
*Yujin Choi,Youngjoo Park,Junyoung Byun,Jaewook Lee,Jinseong Park*

Key words: 检索增强生成, 成员推断攻击, 隐私保护, 相似性检测, RAG系统

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Mirabel是一个基于相似性的MIA检测框架，用于保护RAG系统中的隐私数据，通过简单检测隐藏策略有效抵御攻击并保持数据实用性。

Motivation: 解决RAG系统中直接将私有检索文档传递给LLM导致的成员推断攻击(MIA)漏洞问题。

Method: 引入Mirabel框架，基于MIA查询与目标文档高相似性的特性，实施检测与隐藏策略。

Result: 实验证明Mirabel能有效检测并防御多种先进MIA方法，适配现有私有RAG系统。

Conclusion: Mirabel通过相似性检测与隐藏策略，成功平衡隐私保护与数据实用性。

Abstract: Retrieval-augmented generation (RAG) mitigates the hallucination problem in
large language models (LLMs) and has proven effective for specific,
personalized applications. However, passing private retrieved documents
directly to LLMs introduces vulnerability to membership inference attacks
(MIAs), which try to determine whether the target datum exists in the private
external database or not. Based on the insight that MIA queries typically
exhibit high similarity to only one target document, we introduce Mirabel, a
similarity-based MIA detection framework designed for the RAG system. With the
proposed Mirabel, we show that simple detect-and-hide strategies can
successfully obfuscate attackers, maintain data utility, and remain
system-agnostic. We experimentally prove its detection and defense against
various state-of-the-art MIA methods and its adaptability to existing private
RAG systems.

</details>


### [43] [Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO](https://arxiv.org/abs/2505.22068)
*Ran Li,Shimin Di,Yuchen Liu,Chen Jing,Yu Qiu,Lei Chen*

Key words: 大语言模型, 监督微调, 强化学习, 科学信息抽取, 关系抽取

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了通过监督微调（SFT）和强化学习（RLVR）如何提升大语言模型在科学信息抽取任务中的推理能力，提出两阶段训练方法（MimicSFT和R²GRPO），实验表明该方法优于基线模型。

Motivation: 现有研究表明，强化学习（RLVR）仅能优化推理路径而无法提升数学任务中的推理能力，而监督微调（SFT）可以。本文从科学信息抽取（SciIE）任务的角度出发，探索如何结合SFT和RLVR共同提升模型的推理能力和记忆能力。

Method: 提出了两阶段训练方法：1. MimicSFT，使用结构化推理模板，无需高质量链式思考数据；2. R²GRPO，结合相关性和规则诱导奖励的强化学习方法。

Result: 实验表明，两种方法均能提升推理能力，尤其是在关系抽取任务中，R²GRPO结合MimicSFT超越了基线大语言模型和专门监督模型。

Conclusion: 通过结合SFT和RLVR，可以有效提升模型在科学信息抽取任务中的推理能力，尤其R²GRPO与MimicSFT的组合表现最佳。

Abstract: Previous study suggest that powerful Large Language Models (LLMs) trained
with Reinforcement Learning with Verifiable Rewards (RLVR) only refines
reasoning path without improving the reasoning capacity in math tasks while
supervised-finetuning(SFT) with distillation can. We study this from the view
of Scientific information extraction (SciIE) where LLMs and reasoning LLMs
underperforms small Bert-based models. SciIE require both the reasoning and
memorization. We argue that both SFT and RLVR can refine the reasoning path and
improve reasoning capacity in a simple way based on SciIE. We propose two-stage
training with 1. MimicSFT, using structured reasoning templates without needing
high-quality chain-of-thought data, 2. R$^2$GRPO with relevance and
rule-induced rewards. Experiments on scientific IE benchmarks show that both
methods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses
baseline LLMs and specialized supervised models in relation extraction. Our
code is available at https://github.com/ranlislz/R2GRPO.

</details>


### [44] [ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation](https://arxiv.org/abs/2505.22076)
*Maja Stahl,Timon Ziegenbein,Joonsuk Park,Henning Wachsmuth*

Key words: 大型语言模型,指令微调,计算论证,专门领域,任务表现

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文提出了一种针对计算论证（CA）领域的专门指令微调方法，旨在提升大型语言模型（LLM）在CA任务中的表现，同时保持其泛化能力。

Motivation: 尽管指令微调的LLMs在未见任务上表现良好，但在需要领域知识的任务（如计算论证）中仍存在困难。因此，作者希望通过专门针对CA领域的微调来提升模型表现。

Method: 作者构建了105个CA任务的自然语言指令，开发了CA专用基准，并通过自指令过程生成了52k条CA相关指令来训练CA专用LLM。

Result: 实验表明，CA专用指令微调显着提升了LLM在已见和未见CA任务中的表现，同时在通用NLP任务上的性能保持稳定。

Conclusion: 专门领域指令微调是提升LLM在特定任务中表现的有效方法，同时不影响其泛化能力。

Abstract: Training large language models (LLMs) to follow instructions has
significantly enhanced their ability to tackle unseen tasks. However, despite
their strong generalization capabilities, instruction-following LLMs encounter
difficulties when dealing with tasks that require domain knowledge. This work
introduces a specialized instruction fine-tuning for the domain of
computational argumentation (CA). The goal is to enable an LLM to effectively
tackle any unseen CA tasks while preserving its generalization capabilities.
Reviewing existing CA research, we crafted natural language instructions for
105 CA tasks to this end. On this basis, we developed a CA-specific benchmark
for LLMs that allows for a comprehensive evaluation of LLMs' capabilities in
solving various CA tasks. We synthesized 52k CA-related instructions, adapting
the self-instruct process to train a CA-specialized instruction-following LLM.
Our experiments suggest that CA-specialized instruction fine-tuning
significantly enhances the LLM on both seen and unseen CA tasks. At the same
time, performance on the general NLP tasks of the SuperNI benchmark remains
stable.

</details>


### [45] [Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning](https://arxiv.org/abs/2505.22095)
*Chunyi Peng,Zhipeng Xu,Zhenghao Liu,Yishan Li,Yukun Yan,Shuo Wang,Zhiyuan Liu,Yu Gu,Minghe Yu,Ge Yu,Maosong Sun*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: R1-Router 是一种新型 MRAG 框架，通过动态路由到知识库（KB）提高多模态大模型的效率和准确性，实验显示其比基线模型效果高 7%。

Motivation: 现有的 MRAG 方法在推理过程中缺乏动态交互知识库的能力，R1-Router 旨在通过动态路由和强化学习优化来解决这一问题。

Method: R1-Router 框架动态生成查询并路由到合适的 KB，结合 Step-GRPO 强化学习算法，优化推理行为。

Result: 在多模态开放领域 QA 基准测试中，R1-Router 比基线模型性能提升 7%，减少了不必要的检索，提高了效率和准确性。

Conclusion: R1-Router 通过动态知识路由和强化学习，显著提升了多模态大模型的知识利用效率和准确性。

Abstract: Multimodal Retrieval-Augmented Generation (MRAG) has shown promise in
mitigating hallucinations in Multimodal Large Language Models (MLLMs) by
incorporating external knowledge during generation. Existing MRAG methods
typically adopt a static retrieval pipeline that fetches relevant information
from multiple Knowledge Bases (KBs), followed by a refinement step. However,
these approaches overlook the reasoning and planning capabilities of MLLMs to
dynamically determine how to interact with different KBs during the reasoning
process. To address this limitation, we propose R1-Router, a novel MRAG
framework that learns to decide when and where to retrieve knowledge based on
the evolving reasoning state. Specifically, R1-Router can generate follow-up
queries according to the current reasoning step, routing these intermediate
queries to the most suitable KB, and integrating external knowledge into a
coherent reasoning trajectory to answer the original query. Furthermore, we
introduce Step-wise Group Relative Policy Optimization (Step-GRPO), a tailored
reinforcement learning algorithm that assigns step-specific rewards to optimize
the reasoning behavior of MLLMs. Experimental results on various open-domain QA
benchmarks across multiple modalities demonstrate that R1-Router outperforms
baseline models by over 7%. Further analysis shows that R1-Router can
adaptively and effectively leverage diverse KBs, reducing unnecessary
retrievals and improving both efficiency and accuracy.

</details>


### [46] [Knowledge Base Construction for Knowledge-Augmented Text-to-SQL](https://arxiv.org/abs/2505.22096)
*Jinheon Baek,Horst Samulowitz,Oktie Hassanzadeh,Dharmashankar Subramanian,Sola Shirai,Alfio Gliozzo,Debarun Bhattacharjya*

Key words: Text-to-SQL, 大型语言模型, 知识库, 数据库模式, 跨领域

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出构建一个知识库来提升Text-to-SQL任务的准确性，通过检索和生成相关知识解决LLMs在多样化领域查询中的局限性，并在多个数据集上验证了其有效性。

Motivation: 现有方法依赖大型语言模型（LLMs）生成SQL，但其参数化知识可能无法覆盖多样化和领域特定的查询，导致生成的SQL准确性不足。因此，需要一种能动态补充领域知识的方法。

Method: 提出构建一个基于问题、数据库模式和相关知识的综合性知识库，从中检索并生成查询所需知识，支持跨数据集和领域的复用。

Result: 在多个Text-to-SQL数据集（包括重叠和非重叠数据库场景）上验证，性能显著优于基线方法。

Conclusion: 通过知识库动态补充领域知识，显著提升了Text-to-SQL任务的准确性，尤其在多样化查询和跨领域场景中表现突出。

Abstract: Text-to-SQL aims to translate natural language queries into SQL statements,
which is practical as it enables anyone to easily retrieve the desired
information from databases. Recently, many existing approaches tackle this
problem with Large Language Models (LLMs), leveraging their strong capability
in understanding user queries and generating corresponding SQL code. Yet, the
parametric knowledge in LLMs might be limited to covering all the diverse and
domain-specific queries that require grounding in various database schemas,
which makes generated SQLs less accurate oftentimes. To tackle this, we propose
constructing the knowledge base for text-to-SQL, a foundational source of
knowledge, from which we retrieve and generate the necessary knowledge for
given queries. In particular, unlike existing approaches that either manually
annotate knowledge or generate only a few pieces of knowledge for each query,
our knowledge base is comprehensive, which is constructed based on a
combination of all the available questions and their associated database
schemas along with their relevant knowledge, and can be reused for unseen
databases from different datasets and domains. We validate our approach on
multiple text-to-SQL datasets, considering both the overlapping and
non-overlapping database scenarios, where it outperforms relevant baselines
substantially.

</details>


### [47] [MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models](https://arxiv.org/abs/2505.22101)
*Zhiyu Li,Shichao Song,Hanyu Wang,Simin Niu,Ding Chen,Jiawei Yang,Chenyang Xi,Huayi Lai,Jihao Zhao,Yezhaohui Wang,Junpeng Ren,Zehao Lin,Jiahao Huo,Tianyi Chen,Kai Chen,Kehang Li,Zhiqiang Yin,Qingchen Yu,Bo Tang,Hongkang Yang,Zhi-Qin John Xu,Feiyu Xiong*

Key words: 大型语言模型, 内存管理, MemOS, 多模态集成, 知识进化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文提出了MemOS，一个为大型语言模型（LLMs）设计的内存操作系统，旨在解决当前LLMs缺乏统一结构化内存管理的问题。

Motivation: 当前LLMs主要依赖参数化内存和临时激活内存，缺乏对内存的统一管理和生命周期控制，尤其是多模态集成和长期知识进化的能力不足。

Method: 引入MemOS，首次将内存提升为一类操作资源，通过标准化抽象（MemCube）实现对参数化、激活和明文三种内存类型的统一表示、组织和治理。

Result: MemOS建立了具有强可控性、适应性和可进化性的内存为中心的执行框架，填补了现有LLM基础设施的空白。

Conclusion: MemOS为下一代智能系统的持续适应、个性化智能和跨平台协调奠定了基础。

Abstract: Large Language Models (LLMs) have emerged as foundational infrastructure in
the pursuit of Artificial General Intelligence (AGI). Despite their remarkable
capabilities in language perception and generation, current LLMs fundamentally
lack a unified and structured architecture for handling memory. They primarily
rely on parametric memory (knowledge encoded in model weights) and ephemeral
activation memory (context-limited runtime states). While emerging methods like
Retrieval-Augmented Generation (RAG) incorporate plaintext memory, they lack
lifecycle management and multi-modal integration, limiting their capacity for
long-term knowledge evolution. To address this, we introduce MemOS, a memory
operating system designed for LLMs that, for the first time, elevates memory to
a first-class operational resource. It builds unified mechanisms for
representation, organization, and governance across three core memory types:
parametric, activation, and plaintext. At its core is the MemCube, a
standardized memory abstraction that enables tracking, fusion, and migration of
heterogeneous memory, while offering structured, traceable access across tasks
and contexts. MemOS establishes a memory-centric execution framework with
strong controllability, adaptability, and evolvability. It fills a critical gap
in current LLM infrastructure and lays the groundwork for continual adaptation,
personalized intelligence, and cross-platform coordination in next-generation
intelligent systems.

</details>


### [48] [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](https://arxiv.org/abs/2505.22107)
*Shuhai Zhang,Zeng You,Yaofo Chen,Zhiquan Wen,Qianyue Wang,Zhijie Qiu,Yuanqing Li,Mingkui Tan*

Key words: 大语言模型, 注意力机制, 计算效率, 分组编码, 动态分组注意力

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种动态分组注意力（DGA）方法，通过分组编码策略优化注意力计算，减少冗余并保持性能。

Motivation: 传统注意力计算中的冗余问题导致长上下文建模的计算效率低下，需要一种方法来分离相关和不相关的标记。

Method: 将序列建模重新表述为监督学习任务，分析注意力稀疏性并提出分组编码策略，最终设计动态分组注意力（DGA）。

Result: DGA显著降低了计算成本，同时保持了竞争力。

Conclusion: 动态分组注意力是一种高效且鲁棒的注意力优化方法。

Abstract: Transformer-based large language models (LLMs) excel in natural language
processing tasks by capturing long-range dependencies through self-attention
mechanisms. However, long-context modeling faces significant computational
inefficiencies due to \textit{redundant} attention computations: while
attention weights are often \textit{sparse}, all tokens consume \textit{equal}
computational resources. In this paper, we reformulate traditional
probabilistic sequence modeling as a \textit{supervised learning task},
enabling the separation of relevant and irrelevant tokens and providing a
clearer understanding of redundancy. Based on this reformulation, we
theoretically analyze attention sparsity, revealing that only a few tokens
significantly contribute to predictions. Building on this, we formulate
attention optimization as a linear coding problem and propose a \textit{group
coding strategy}, theoretically showing its ability to improve robustness
against random noise and enhance learning efficiency. Motivated by this, we
propose \textit{Dynamic Group Attention} (DGA), which leverages the group
coding to explicitly reduce redundancy by aggregating less important tokens
during attention computation. Empirical results show that our DGA significantly
reduces computational costs while maintaining competitive performance.Code is
available at https://github.com/bolixinyu/DynamicGroupAttention.

</details>


### [49] [THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of Large Reasoning Models](https://arxiv.org/abs/2505.22113)
*Zhiyuan Li,Yi Chang,Yuan Wu*

Key words: 大型推理模型, 过度思考, 推理效率, Think-Bench, 思维链

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 大型推理模型（LRMs）在复杂任务中表现优异，但存在计算效率低下的“过度思考”问题。本文提出Think-Bench基准和效率指标，评估LRMs在多维度的表现，发现多数模型在处理简单问题时生成冗余推理链，效率较低。

Motivation: 研究LRMs在简单任务中因“过度思考”导致的计算资源浪费问题，以系统性评估和改进推理效率。

Method: 提出Think-Bench基准和新型效率指标，对多种LRMs进行推理过程、结果质量和思维链特性的多维评估。

Result: 多数LRMs在简单问题上表现出过度思考，生成冗长推理链；部分模型思维链质量高但效率低。

Conclusion: Think-Bench为LRMs研究提供坚实基础，推动高效推理模型的开发。

Abstract: Large reasoning models (LRMs) have achieved impressive performance in complex
tasks, often outperforming conventional large language models (LLMs). However,
the prevalent issue of overthinking severely limits their computational
efficiency. Overthinking occurs when models generate excessive and redundant
tokens that contribute little to accurate outcomes, especially in simple tasks,
resulting in a significant waste of computational resources. To systematically
investigate this issue, we introduce Think-Bench, a benchmark designed to
evaluate the reasoning efficiency of LRMs. We also propose novel efficiency
metrics and conduct a comprehensive evaluation of various LRMs across multiple
dimensions, including the reasoning process, outcome quality, and
chain-of-thought (CoT) characteristics. Our analysis reveals that most LRMs
exhibit overthinking in handling easy questions, generating unnecessarily
lengthy reasoning chains. While many LRMs demonstrate high CoT quality, several
suffer from low efficiency. We hope that Think-Bench can serve as a robust
foundation for advancing research into LRMs.

</details>


### [50] [Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model](https://arxiv.org/abs/2505.22116)
*Jintao Zhang,Zirui Liu,Mingyue Cheng,Shilong Zhang,Tingyue Pan,Qi Liu,Yanhu Xie*

Key words: 术中低血压、多模态语言模型、扩散方法、两阶段训练、临床决策支持

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 摘要介绍了一种名为IOHFuseLM的多模态语言模型框架，用于预测术中低血压事件，通过两阶段训练策略增强模型对低血压模式的分辨能力，并在实验中表现优于基线模型。

Motivation: 术中低血压（IOH）与不良临床结果密切相关，但由于事件稀疏性以及静态和动态数据整合困难，其预测具有挑战性。

Method: 采用两阶段训练策略：1. 基于扩散方法的生理时间序列领域自适应预训练；2. 在原始临床数据集上进行任务微调。同时，通过令牌级对齐实现多模态融合，并结合静态属性的结构化文本。

Result: 实验结果表明，IOHFuseLM在两个术中数据集上的低血压事件识别准确率优于现有基线模型。

Conclusion: IOHFuseLM在临床决策支持场景中表现出潜力，代码已公开以促进可重复性。

Abstract: Intraoperative hypotension (IOH) frequently occurs under general anesthesia
and is strongly linked to adverse outcomes such as myocardial injury and
increased mortality. Despite its significance, IOH prediction is hindered by
event sparsity and the challenge of integrating static and dynamic data across
diverse patients. In this paper, we propose \textbf{IOHFuseLM}, a multimodal
language model framework. To accurately identify and differentiate sparse
hypotensive events, we leverage a two-stage training strategy. The first stage
involves domain adaptive pretraining on IOH physiological time series augmented
through diffusion methods, thereby enhancing the model sensitivity to patterns
associated with hypotension. Subsequently, task fine-tuning is performed on the
original clinical dataset to further enhance the ability to distinguish
normotensive from hypotensive states. To enable multimodal fusion for each
patient, we align structured clinical descriptions with the corresponding
physiological time series at the token level. Such alignment enables the model
to capture individualized temporal patterns alongside their corresponding
clinical semantics. In addition, we convert static patient attributes into
structured text to enrich personalized information. Experimental evaluations on
two intraoperative datasets demonstrate that IOHFuseLM outperforms established
baselines in accurately identifying IOH events, highlighting its applicability
in clinical decision support scenarios. Our code is publicly available to
promote reproducibility at https://github.com/zjt-gpu/IOHFuseLM.

</details>


### [51] [Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of Two Approaches](https://arxiv.org/abs/2505.22118)
*Alan Ramponi,Marco Rovera,Robert Moro,Sara Tonelli*

Key words: 事实核查, 跨语言检索, 多语言检索, LLM重新排序, 负例采样

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究探讨了如何通过负例选择和重新排序策略改进多语言和跨语言环境下事实核查声明的检索效果，实验结果表明，基于LLM的重新排序方法和特定负例采样策略效果最佳，且跨语言环境下具有独特特点。

Motivation: 尽管事实核查声明的自动化检索已有研究，但现有工作多集中于单语言环境。然而，对于资源匮乏的语言或全球性议题，跨语言检索尤为重要。因此，本研究旨在提升多语言和跨语言检索的效能。

Method: 研究对比了监督学习中的负例选择策略和无监督学习中的重新排序方法，并在47种语言（283种语言组合）的数据集上进行评估。具体方法包括基于句子相似性的负例采样和基于LLM的重新排序。

Result: 实验结果显示，基于LLM的重新排序效果最优，其次是采用句子相似性负例采样的微调方法。此外，研究发现跨语言环境与多语言环境存在显著差异。

Conclusion: 跨语言检索具有独特性，而LLM重新排序和针对性负例采样能显著提升性能。这一发现为未来多语言事实核查系统的设计提供了重要指导。

Abstract: Retrieval of previously fact-checked claims is a well-established task, whose
automation can assist professional fact-checkers in the initial steps of
information verification. Previous works have mostly tackled the task
monolingually, i.e., having both the input and the retrieved claims in the same
language. However, especially for languages with a limited availability of
fact-checks and in case of global narratives, such as pandemics, wars, or
international politics, it is crucial to be able to retrieve claims across
languages. In this work, we examine strategies to improve the multilingual and
crosslingual performance, namely selection of negative examples (in the
supervised) and re-ranking (in the unsupervised setting). We evaluate all
approaches on a dataset containing posts and claims in 47 languages (283
language combinations). We observe that the best results are obtained by using
LLM-based re-ranking, followed by fine-tuning with negative examples sampled
using a sentence similarity-based strategy. Most importantly, we show that
crosslinguality is a setup with its own unique characteristics compared to the
multilingual setup.

</details>


### [52] [LoKI: Low-damage Knowledge Implanting of Large Language Models](https://arxiv.org/abs/2505.22120)
*Runyu Wang,Peng Ping,Zhengyu Guo,Xiaoye Zhang,Quan Shi,Liting Zhou,Tianbo Ji*

Key words: 参数高效微调, 灾难性遗忘, LoKI, Transformer架构, 通用能力

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种名为LoKI的参数高效微调方法，通过理解Transformer架构中知识的存储机制，在保持模型通用能力的同时实现出色的任务性能。

Motivation: 解决参数高效微调方法（PEFT）中常见的灾难性遗忘（CF）问题，同时避免牺牲模型的通用能力。

Method: 基于对Transformer架构中知识存储机制的机械性理解，设计了LoKI方法，通过低损伤知识植入实现高效微调。

Result: 在多种模型类型上，LoKI的任务性能与全微调和LoRA方法相当甚至更好，同时显著保留了通用能力。

Conclusion: LoKI通过结合对模型知识存储的机制性理解和实际微调目标，实现了任务专业化和通用能力保护的最佳平衡。

Abstract: Fine-tuning adapts pretrained models for specific tasks but poses the risk of
catastrophic forgetting (CF), where critical knowledge from pre-training is
overwritten. Current Parameter-Efficient Fine-Tuning (PEFT) methods for Large
Language Models (LLMs), while efficient, often sacrifice general capabilities.
To address the issue of CF in a general-purpose PEFT framework, we propose
\textbf{Lo}w-damage \textbf{K}nowledge \textbf{I}mplanting (\textbf{LoKI}), a
PEFT technique that is based on a mechanistic understanding of how knowledge is
stored in transformer architectures. In two real-world scenarios, LoKI
demonstrates task-specific performance that is comparable to or even surpasses
that of full fine-tuning and LoRA-based methods across various model types,
while significantly better preserving general capabilities. Our work connects
mechanistic insights into LLM knowledge storage with practical fine-tuning
objectives, achieving state-of-the-art trade-offs between task specialization
and the preservation of general capabilities. Our implementation is publicly
available as ready-to-use code\footnote{https://github.com/Nexround/LoKI}.

</details>


### [53] [EULER: Enhancing the Reasoning Ability of Large Language Models through Error-Induced Learning](https://arxiv.org/abs/2505.22131)
*Zhuoyang Wu,Xinze Li,Zhenghao Liu,Yukun Yan,Zhiyuan Liu,Minghe Yu,Cheng Yang,Yu Gu,Ge Yu,Maosong Sun*

Key words: 大语言模型, 数学推理, 错误学习, EULER模型, 监督微调

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: EULER模型通过生成高质量的错误解决方案，提升大语言模型在数学推理任务中的表现，实验表明其性能优于基线模型4%以上。

Motivation: 大语言模型在数学解题中表现优秀，但从错误中学习可能进一步提升其性能。然而，现有的合成错误方法存在局限性，促使EULER模型的开发。

Method: EULER通过优化错误暴露模型，生成高质量的解决方案错误，并利用高阶LLM的解决方案规范生成质量。

Result: 在多个数学问题数据集上，EULER模型表现优于所有基线模型，提升超过4%。

Conclusion: EULER模型能够生成更具挑战性和教育意义的错误解决方案，有效提升LLM的训练和推理能力。

Abstract: Large Language Models (LLMs) have demonstrated strong reasoning capabilities
and achieved promising results in mathematical problem-solving tasks. Learning
from errors offers the potential to further enhance the performance of LLMs
during Supervised Fine-Tuning (SFT). However, the errors in synthesized
solutions are typically gathered from sampling trails, making it challenging to
generate solution errors for each mathematical problem. This paper introduces
the Error-IndUced LEaRning (EULER) model, which aims to develop an error
exposure model that generates high-quality solution errors to enhance the
mathematical reasoning capabilities of LLMs. Specifically, EULER optimizes the
error exposure model to increase the generation probability of self-made
solution errors while utilizing solutions produced by a superior LLM to
regularize the generation quality. Our experiments across various mathematical
problem datasets demonstrate the effectiveness of the EULER model, achieving an
improvement of over 4% compared to all baseline models. Further analysis
reveals that EULER is capable of synthesizing more challenging and educational
solution errors, which facilitate both the training and inference processes of
LLMs. All codes are available at https://github.com/NEUIR/EULER.

</details>


### [54] [RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding](https://arxiv.org/abs/2505.22135)
*Yuichiro Hoshino,Hideyuki Tachibana,Muneyoshi Inahara,Hiroto Takegawa*

Key words: 混合模型, Transformer, 状态空间模型, 自蒸馏, 冗余优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了RAD框架，通过自推测解码识别Transformer冗余层，并用SSM替换，结合目标自蒸馏优化混合模型。该方法在数学和编程任务中显著提升性能，并在知识蒸馏中实现更快收敛。

Motivation: 旨在优化Transformer与SSM混合模型，解决Transformer组件冗余问题，以提升效率与性能。

Method: 使用自推测解码诊断冗余注意力层，选择性替换为SSM组件，并采用针对性自蒸馏策略。

Result: RAD在GSM8K和CRUX任务上分别达到71.27和28.25分，远超基线（46.17和22.75），且收敛速度提升约2倍。

Conclusion: RAD为混合模型的高效优化与性能增强提供了新路径。

Abstract: Hybrid models combining Transformers and State Space Models (SSMs) are
promising for balancing performance and efficiency. However, optimizing these
hybrid models, particularly by addressing the potential redundancy inherent
within the Transformer components, remains a significant challenge. In this
paper, we propose RAD (Redundancy-Aware Distillation), a novel framework that
uses self-speculative decoding as a diagnostic tool to identify redundant
attention layers within the model. These identified layers are then selectively
replaced with SSM components, followed by targeted (self-)distillation.
Specifically, RAD focuses knowledge transfer on the components identified as
redundant, considering architectural changes and specific weight initialization
strategies. We experimentally demonstrate that self-distillation using RAD
significantly surpasses the performance of the original base model on
mathematical and coding tasks. Furthermore, RAD is also effective in standard
knowledge distillation settings, achieving up to approximately 2x faster
convergence compared to baseline methods. Notably, while a baseline model
distilled from a Llama-3.1 70B teacher achieves scores of 46.17 on GSM8K and
22.75 on CRUX, RAD achieves significantly higher scores of 71.27 on GSM8K and
28.25 on CRUX, even when using a much smaller Llama-3.1 8B teacher. RAD offers
a new pathway for efficient optimization and performance enhancement in the
distillation of hybrid models.

</details>


### [55] [Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments](https://arxiv.org/abs/2505.22137)
*Marc Feger,Katarina Boland,Stefan Dietze*

Key words: 论辩挖掘、BERT-like模型、泛化能力、任务特定预训练、联合训练

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究重新评估了BERT-like模型在论辩挖掘任务中的泛化能力，发现其表现受数据集特定线索影响较大，但通过任务特定预训练和联合训练可提高鲁棒性。

Motivation: 评估现有先进模型在论辩识别任务中的泛化能力，揭示其依赖数据集特定线索而非任务本质的问题。

Method: 在17个英文句子级数据集上评估4种transformer模型（3种标准模型和1种增强对比预训练模型），分析其表现和依赖模式。

Result: 模型在熟悉数据上表现良好，但在未见数据上表现显著下降；任务特定预训练和联合训练可有效提升泛化能力。

Conclusion: 当前模型在论辩识别中依赖局部线索，任务特定预训练和联合训练是提升泛化能力的有效方法。

Abstract: Identifying arguments is a necessary prerequisite for various tasks in
automated discourse analysis, particularly within contexts such as political
debates, online discussions, and scientific reasoning. In addition to
theoretical advances in understanding the constitution of arguments, a
significant body of research has emerged around practical argument mining,
supported by a growing number of publicly available datasets. On these
benchmarks, BERT-like transformers have consistently performed best,
reinforcing the belief that such models are broadly applicable across diverse
contexts of debate. This study offers the first large-scale re-evaluation of
such state-of-the-art models, with a specific focus on their ability to
generalize in identifying arguments. We evaluate four transformers, three
standard and one enhanced with contrastive pre-training for better
generalization, on 17 English sentence-level datasets as most relevant to the
task. Our findings show that, to varying degrees, these models tend to rely on
lexical shortcuts tied to content words, suggesting that apparent progress may
often be driven by dataset-specific cues rather than true task alignment. While
the models achieve strong results on familiar benchmarks, their performance
drops markedly when applied to unseen datasets. Nonetheless, incorporating both
task-specific pre-training and joint benchmark training proves effective in
enhancing both robustness and generalization.

</details>


### [56] [InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing](https://arxiv.org/abs/2505.22156)
*Shuaiyi Li,Zhisong Zhang,Yang Deng,Chenlong Deng,Tianqing Fang,Hongming Zhang,Haitao Mi,Dong Yu,Wai Lam*

Key words: 模型编辑,LLM,上下文压缩,动态选择,InComeS

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: InComeS框架通过压缩和选择机制增强LLM处理编辑上下文的能力，解决了现有模型编辑方法在复杂场景下表现不佳的问题。

Motivation: 现有模型编辑方法在需要深层语义理解的复杂场景中表现不佳，限制了LLM的上下文窗口能力。

Method: 提出InComeS框架，通过压缩编辑上下文为键值缓存并动态选择最相关信息，提升多编辑处理的效率和适应性。

Result: 实验证明InComeS在多种编辑格式的基准测试中表现有效且高效。

Conclusion: InComeS通过压缩和动态选择机制显著提升了LLM在模型编辑任务中的性能。

Abstract: Although existing model editing methods perform well in recalling exact edit
facts, they often struggle in complex scenarios that require deeper semantic
understanding rather than mere knowledge regurgitation. Leveraging the strong
contextual reasoning abilities of large language models (LLMs), in-context
learning (ICL) becomes a promising editing method by comprehending edit
information through context encoding. However, this method is constrained by
the limited context window of LLMs, leading to degraded performance and
efficiency as the number of edits increases. To overcome this limitation, we
propose InComeS, a flexible framework that enhances LLMs' ability to process
editing contexts through explicit compression and selection mechanisms.
Specifically, InComeS compresses each editing context into the key-value (KV)
cache of a special gist token, enabling efficient handling of multiple edits
without being restricted by the model's context window. Furthermore,
specialized cross-attention modules are added to dynamically select the most
relevant information from the gist pools, enabling adaptive and effective
utilization of edit information. We conduct experiments on diverse model
editing benchmarks with various editing formats, and the results demonstrate
the effectiveness and efficiency of our method.

</details>


### [57] [Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy](https://arxiv.org/abs/2505.22157)
*Paramita Mirza,Lucas Weber,Fabian Küch*

Key words: 数据选择, LLM微调, 多样性, 轻量级方法, 聚类算法

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种高效且通用的数据选择方法，通过多步骤流程分组、质量评估和难度评分，优化LLM微调数据集，保证多样性和性能。

Motivation: 现有数据选择方法计算成本高或适用范围窄，本文旨在实现高效且通用的数据选择。

Method: 采用多步流程：数据分组、专用模型评估质量、轻量级方法评分难度，结合嵌入模型和聚类算法保证多样性。

Result: 实现了高性能微调且计算开销最小化。

Conclusion: 该方法在数据选择和模型微调中高效且通用，适用于多用途模型。

Abstract: Recent work shows that post-training datasets for LLMs can be substantially
downsampled without noticeably deteriorating performance. However, data
selection often incurs high computational costs or is limited to narrow
domains. In this paper, we demonstrate that data selection can be both --
efficient and universal -- by using a multi-step pipeline in which we
efficiently bin data points into groups, estimate quality using specialized
models, and score difficulty with a robust, lightweight method. Task-based
categorization allows us to control the composition of our final data --
crucial for finetuning multi-purpose models. To guarantee diversity, we improve
upon previous work using embedding models and a clustering algorithm. This
integrated strategy enables high-performance fine-tuning with minimal overhead.

</details>


### [58] [Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes](https://arxiv.org/abs/2505.22165)
*Bocheng Li,Zhujin Gao,Linli Xu*

Key words: 扩散模型, 文本生成, 泊松过程, 时间预测器, 非自回归

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种名为NeoDiff的新型扩散模型，通过整合离散和连续扩散模型的优势，解决了现有方法的局限性。NeoDiff在正向过程中引入泊松扩散过程，反向过程使用时间预测器自适应调节去噪进度，并在推理阶段优化调度以提高性能。实验证明，NeoDiff在多个文本生成任务上优于基线模型。

Motivation: 现有离散和连续扩散模型各有优缺点：离散模型缺乏细粒度控制，连续模型无法捕捉语义细微差别。本文旨在结合两者优势，提出更高效的文本生成框架。

Method: NeoDiff通过泊松扩散过程（正向过程）和时间预测器（反向过程）实现灵活的细粒度噪声控制和自适应去噪，并优化推理调度。

Result: 实验显示NeoDiff在文本生成任务中表现优于非自回归/自回归扩散模型及迭代方法。

Conclusion: NeoDiff为文本生成提供了一个更理论化和高效的框架，推动了扩散模型在文本生成领域的发展。

Abstract: Diffusion models have emerged as a promising approach for text generation,
with recent works falling into two main categories: discrete and continuous
diffusion models. Discrete diffusion models apply token corruption
independently using categorical distributions, allowing for different diffusion
progress across tokens but lacking fine-grained control. Continuous diffusion
models map tokens to continuous spaces and apply fine-grained noise, but the
diffusion progress is uniform across tokens, limiting their ability to capture
semantic nuances. To address these limitations, we propose
\textbf{\underline{N}}on-simultan\textbf{\underline{e}}ous
C\textbf{\underline{o}}ntinuous \textbf{\underline{Diff}}usion Models
(NeoDiff), a novel diffusion model that integrates the strengths of both
discrete and continuous approaches. NeoDiff introduces a Poisson diffusion
process for the forward process, enabling a flexible and fine-grained noising
paradigm, and employs a time predictor for the reverse process to adaptively
modulate the denoising progress based on token semantics. Furthermore, NeoDiff
utilizes an optimized schedule for inference to ensure more precise noise
control and improved performance. Our approach unifies the theories of discrete
and continuous diffusion models, offering a more principled and effective
framework for text generation. Experimental results on several text generation
tasks demonstrate NeoDiff's superior performance compared to baselines of
non-autoregressive continuous and discrete diffusion models, iterative-based
methods and autoregressive diffusion-based methods. These results highlight
NeoDiff's potential as a powerful tool for generating high-quality text and
advancing the field of diffusion-based text generation.

</details>


### [59] [ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments](https://arxiv.org/abs/2505.22169)
*Gili Lior,Eliya Habba,Shahar Levy,Avi Caciularu,Gabriel Stanovsky*

Key words: LLM评估, 提示敏感性, 随机方法, ReliableEval, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于随机方法评估LLM对提示敏感性的框架ReliableEval，强调现有单一提示评估的不可靠性，并通过实验发现即使顶级模型也存在显著提示敏感性。

Motivation: 现有LLM评估通常仅基于单一提示，无法反映模型在真实场景中的表现，因此需要一种能覆盖含义一致提示变体的可靠评估方法。

Method: 引入ReliableEval方法，通过随机采样含义一致的提示扰动，并统计计算所需采样次数以实现评估稳定性。

Result: 对五种前沿LLM（如GPT-4o和Claude-3.7-Sonnet）的评估显示，这些模型对提示存在显著敏感性。

Conclusion: 该框架为LLM评估提供了一种与模型、任务和指标无关的可靠方法。

Abstract: LLMs are highly sensitive to prompt phrasing, yet standard benchmarks
typically report performance using a single prompt, raising concerns about the
reliability of such evaluations. In this work, we argue for a stochastic method
of moments evaluation over the space of meaning-preserving prompt
perturbations. We introduce a formal definition of reliable evaluation that
accounts for prompt sensitivity, and suggest ReliableEval - a method for
estimating the number of prompt resamplings needed to obtain meaningful
results. Using our framework, we stochastically evaluate five frontier LLMs and
find that even top-performing models like GPT-4o and Claude-3.7-Sonnet exhibit
substantial prompt sensitivity. Our approach is model-, task-, and
metric-agnostic, offering a recipe for meaningful and robust LLM evaluation.

</details>


### [60] [Reverse Preference Optimization for Complex Instruction Following](https://arxiv.org/abs/2505.22172)
*Xiang Huang,Ting-En Lin,Feiteng Fang,Yuchuan Wu,Hangyu Li,Yuzhong Qu,Fei Huang,Yongbin Li*

Key words: 指令遵循、大型语言模型、偏好优化、多约束、RPO

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 为了解决大型语言模型（LLM）在处理多约束复杂指令时的偏好对齐问题，研究者提出了一种名为反向偏好优化（RPO）的方法。该方法通过动态反转指令中的约束以减少偏好对中的噪声，并在多轮指令遵循任务中显著优于基线方法。

Motivation: 当前方法在基于约束满足数量选择偏好对时引入噪声，导致选中的响应可能不完全满足约束，而被拒绝的响应可能在某些方面优于选中的响应。这需要大量采样和过滤才能获得完美响应，效率低下。

Method: 提出反向偏好优化（RPO），通过动态反转指令中的约束，确保选中响应完美满足要求，同时扩大了选中与被拒绝响应之间的差距，从而减少噪声并明确优化方向。

Result: 在Sysbench和Multi-IF两个多轮指令遵循基准测试中，RPO分别比DPO基线平均提高了4.6分和2.5分（基于Llama-3.1 8B模型）。70B参数的RPO模型甚至超过了GPT-4o。

Conclusion: RPO是一种简单但高效的方法，能够有效减少多约束指令偏好对齐中的噪声，并在不同规模的模型中展现出良好的扩展性和性能优势。

Abstract: Instruction following (IF) is a critical capability for large language models
(LLMs). However, handling complex instructions with multiple constraints
remains challenging. Previous methods typically select preference pairs based
on the number of constraints they satisfy, introducing noise where chosen
examples may fail to follow some constraints and rejected examples may excel in
certain respects over the chosen ones. To address the challenge of aligning
with multiple preferences, we propose a simple yet effective method called
Reverse Preference Optimization (RPO). It mitigates noise in preference pairs
by dynamically reversing the constraints within the instruction to ensure the
chosen response is perfect, alleviating the burden of extensive sampling and
filtering to collect perfect responses. Besides, reversal also enlarges the gap
between chosen and rejected responses, thereby clarifying the optimization
direction and making it more robust to noise. We evaluate RPO on two multi-turn
IF benchmarks, Sysbench and Multi-IF, demonstrating average improvements over
the DPO baseline of 4.6 and 2.5 points (on Llama-3.1 8B), respectively.
Moreover, RPO scales effectively across model sizes (8B to 70B parameters),
with the 70B RPO model surpassing GPT-4o.

</details>


### [61] [TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation](https://arxiv.org/abs/2505.22176)
*Vihang Pancholi,Jainit Bafna,Tejas Anvekar,Manish Shrivastava,Vivek Gupta*

Key words: 表格评估, TabXEval, 结构对齐, 语义比较, 基准测试

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为TabXEval的新框架，通过结构对齐和语义/句法比较来全面评估表格，并使用新的基准TabXBench验证其效果。

Motivation: 传统表格评估方法难以捕捉结构和内容上的细微差异，需要更全面的评估方法。

Method: TabXEval框架分为两阶段：TabAlign进行结构对齐，TabCompare进行语义和句法比较。验证使用了多领域基准TabXBench。

Result: TabXEval在多样化的表格任务和领域中表现出色，优于传统方法，展示了其在解释性和效果上的优势。

Conclusion: TabXEval为表格评估提供了更全面和可解释的解决方案，为未来研究奠定了基础。

Abstract: Evaluating tables qualitatively & quantitatively presents a significant
challenge, as traditional metrics often fail to capture nuanced structural and
content discrepancies. To address this, we introduce a novel, methodical rubric
integrating multi-level structural descriptors with fine-grained contextual
quantification, thereby establishing a robust foundation for comprehensive
table comparison. Building on this foundation, we propose TabXEval, an
eXhaustive and eXplainable two-phase evaluation framework. TabXEval initially
aligns reference tables structurally via TabAlign & subsequently conducts a
systematic semantic and syntactic comparison using TabCompare; this approach
clarifies the evaluation process and pinpoints subtle discrepancies overlooked
by conventional methods. The efficacy of this framework is assessed using
TabXBench, a novel, diverse, multi-domain benchmark we developed, featuring
realistic table perturbations and human-annotated assessments. Finally, a
systematic analysis of existing evaluation methods through
sensitivity-specificity trade-offs demonstrates the qualitative and
quantitative effectiveness of TabXEval across diverse table-related tasks and
domains, paving the way for future innovations in explainable table evaluation.

</details>


### [62] [Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design](https://arxiv.org/abs/2505.22179)
*Yudi Zhang,Weilin Zhao,Xu Han,Tiejun Zhao,Wang Xu,Hailong Cao,Conghui Zhu*

Key words: 推测解码, 量化, 大型语言模型, 内存优化, 推理加速

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文研究了如何结合推测解码和量化技术来加速大型语言模型的内存受限推理，并提出了一种新的分层推测解码设计，显著提升了性能。

Motivation: 通过结合推测解码和量化技术，优化大型语言模型的内存带宽瓶颈和计算效率，以提升推理速度。

Method: 提出了一种分层推测解码框架，先使用小型模型将树状草案转换为序列草案，再结合目标量化模型的内存访问优势进行验证。

Result: 实验表明，该方法在4位量化Llama-3-70B模型上实现了2.78倍的加速，优于EAGLE-2的1.31倍。

Conclusion: 分层推测解码设计有效结合了量化优势，显著提升了推理速度。

Abstract: Speculative decoding and quantization effectively accelerate memory-bound
inference of large language models. Speculative decoding mitigates the memory
bandwidth bottleneck by verifying multiple tokens within a single forward pass,
which increases computational effort. Quantization achieves this optimization
by compressing weights and activations into lower bit-widths and also reduces
computations via low-bit matrix multiplications. To further leverage their
strengths, we investigate the integration of these two techniques.
Surprisingly, experiments applying the advanced speculative decoding method
EAGLE-2 to various quantized models reveal that the memory benefits from 4-bit
weight quantization are diminished by the computational load from speculative
decoding. Specifically, verifying a tree-style draft incurs significantly more
time overhead than a single-token forward pass on 4-bit weight quantized
models. This finding led to our new speculative decoding design: a hierarchical
framework that employs a small model as an intermediate stage to turn
tree-style drafts into sequence drafts, leveraging the memory access benefits
of the target quantized model. Experimental results show that our hierarchical
approach achieves a 2.78$\times$ speedup across various tasks for the 4-bit
weight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\times$.
Code available at https://github.com/AI9Stars/SpecMQuant.

</details>


### [63] [Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon](https://arxiv.org/abs/2505.22184)
*Xuchen Ma,Jianxiang Yu,Wenming Shao,Bo Pang,Xiang Li*

Key words: 中文伪装毒性、内容审核、同音词、BERT、LLMs

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种无需训练和提示的方法C$^2$TU，用于检测中文社交媒体中的伪装毒性内容，通过子串匹配和语义过滤有效识别并纠正毒性词汇，实验表明其性能显著优于现有方法。

Motivation: 社交平台上伪装毒性内容（如同音伪装）的增多给内容审核带来挑战，现有方法主要针对英文，中文领域尚未解决。

Method: C$^2$TU通过子串匹配从同音词和毒性词库中筛选候选词，并利用BERT和LLMs变体进行非毒性过滤及伪装纠正，LLMs版本还解决了自回归限制。

Result: 在两个中文毒性数据集上，C$^2$TU的F1分数和准确率分别最高提升71%和35%，显著优于竞争对手。

Conclusion: C$^2$TU为中文伪装毒性内容检测提供了高效且无需训练的解决方案，性能优越。

Abstract: Social media platforms have experienced a significant rise in toxic content,
including abusive language and discriminatory remarks, presenting growing
challenges for content moderation. Some users evade censorship by deliberately
disguising toxic words through homophonic cloak, which necessitates the task of
unveiling cloaked toxicity. Existing methods are mostly designed for English
texts, while Chinese cloaked toxicity unveiling has not been solved yet. To
tackle the issue, we propose C$^2$TU, a novel training-free and prompt-free
method for Chinese cloaked toxic content unveiling. It first employs substring
matching to identify candidate toxic words based on Chinese homo-graph and
toxic lexicon. Then it filters those candidates that are non-toxic and corrects
cloaks to be their corresponding toxicities. Specifically, we develop two model
variants for filtering, which are based on BERT and LLMs, respectively. For
LLMs, we address the auto-regressive limitation in computing word occurrence
probability and utilize the full semantic contexts of a text sequence to reveal
cloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve
superior performance on two Chinese toxic datasets. In particular, our method
outperforms the best competitor by up to 71% on the F1 score and 35% on
accuracy, respectively.

</details>


### [64] [Let's Predict Sentence by Sentence](https://arxiv.org/abs/2505.22202)
*Hyeonbin Hwang,Byeongguk Jeon,Seungone Kim,Jiyeon Kim,Hoyeon Chang,Sohee Yang,Seungpil Won,Dohaeng Lee,Youbin Ahn,Minjoon Seo*

Key words: 语言模型、抽象推理、句子嵌入、上下文嵌入、计算效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了预训练语言模型（LMs）是否能通过连续嵌入空间进行抽象推理，提出了一种框架将模型从词级别提升到句子级别，并在多个领域验证了其高效性和性能。

Motivation: 人类推理基于高级语义单元（如句子或概念），而传统LMs以词为单位生成，能否让LMs在抽象语义单元上推理是一个核心问题。

Method: 提出了一个框架，通过自回归预测连续句子嵌入，探索了两种嵌入范式（语义嵌入和上下文嵌入），并比较了离散化和连续推理两种推理方式。

Result: 在数学、逻辑、常识和规划等领域，上下文嵌入结合连续推理的性能与思维链（CoT）相当，同时推理计算量平均减少一半。

Conclusion: 预训练LMs可以在潜在嵌入空间中有效进行结构化抽象推理，具备可扩展性和模块化适应性。

Abstract: Autoregressive language models (LMs) generate one token at a time, yet human
reasoning operates over higher-level abstractions - sentences, propositions,
and concepts. This contrast raises a central question- Can LMs likewise learn
to reason over structured semantic units rather than raw token sequences? In
this work, we investigate whether pretrained LMs can be lifted into such
abstract reasoning spaces by building on their learned representations. We
present a framework that adapts a pretrained token-level LM to operate in
sentence space by autoregressively predicting continuous embeddings of next
sentences. We explore two embedding paradigms inspired by classical
representation learning: 1) semantic embeddings, learned via autoencoding to
preserve surface meaning; and 2) contextual embeddings, trained via
next-sentence prediction to encode anticipatory structure. We evaluate both
under two inference regimes: Discretized, which decodes each predicted
embedding into text before re-encoding; and Continuous, which reasons entirely
in embedding space for improved efficiency. Across four domains - mathematics,
logic, commonsense, and planning - contextual embeddings under continuous
inference show competitive performance with Chain-of-Thought (CoT) while
reducing inference-time FLOPs on average by half. We also present early signs
of scalability and modular adaptation. Finally, to visualize latent
trajectories, we introduce SentenceLens, a diagnostic tool that decodes
intermediate model states into interpretable sentences. Together, our results
indicate that pretrained LMs can effectively transition to abstract, structured
reasoning within latent embedding spaces.

</details>


### [65] [Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models](https://arxiv.org/abs/2505.22232)
*Mehdi Ali,Manuel Brack,Max Lübbering,Elias Wendt,Abbas Goher Khan,Richard Rutmann,Alex Jude,Maurice Kraus,Alexander Arno Weber,Felix Stollenwerk,David Kaczér,Florian Mai,Lucie Flek,Rafet Sifa,Nicolas Flores-Herr,Joachim Köhler,Patrick Schramowski,Michael Fromm,Kristian Kersting*

Key words: 多语言数据, 数据筛选, 轻量级标注器, 跨语言迁移, JQL

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: JQL提出了一种系统性方法，通过轻量级标注器高效筛选高质量多语言数据，显著提升跨语言迁移能力与数据保留率。

Motivation: 当前开源多语言数据集稀缺且依赖启发式过滤方法，限制了跨语言迁移和扩展性，因此需要更高效的数据筛选方法。

Method: 利用预训练多语言嵌入的轻量级标注器，将大语言模型的标注能力蒸馏到高效的数据筛选流程中。

Result: 在35种语言上验证，JQL显著优于现有启发式方法（如Fineweb2），提升了下游模型训练质量和数据保留率。

Conclusion: JQL为多语言数据筛选提供了实用方案和资源，提升了多语言数据集开发的标准。

Abstract: High-quality multilingual training data is essential for effectively
pretraining large language models (LLMs). Yet, the availability of suitable
open-source multilingual datasets remains limited. Existing state-of-the-art
datasets mostly rely on heuristic filtering methods, restricting both their
cross-lingual transferability and scalability. Here, we introduce JQL, a
systematic approach that efficiently curates diverse and high-quality
multilingual data at scale while significantly reducing computational demands.
JQL distills LLMs' annotation capabilities into lightweight annotators based on
pretrained multilingual embeddings. These models exhibit robust multilingual
and cross-lingual performance, even for languages and scripts unseen during
training. Evaluated empirically across 35 languages, the resulting annotation
pipeline substantially outperforms current heuristic filtering methods like
Fineweb2. JQL notably enhances downstream model training quality and increases
data retention rates. Our research provides practical insights and valuable
resources for multilingual data curation, raising the standards of multilingual
dataset development.

</details>


### [66] [A Linguistically Motivated Analysis of Intonational Phrasing in Text-to-Speech Systems: Revealing Gaps in Syntactic Sensitivity](https://arxiv.org/abs/2505.22236)
*Charlotte Pouw,Afra Alishahi,Willem Zuidema*

Key words: Text-to-Speech, 句法敏感性, 语调短语边界, 花园路径句, 模型微调

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究分析了TTS系统对句法的敏感性，发现系统在句法边界模糊时难以准确生成语调短语边界，需依赖逗号等表面标记，而在简单句法结构中则能利用更深层的句法线索。通过微调模型忽略逗号，系统能生成更符合底层结构的语调模式。

Motivation: 探讨TTS系统在处理复杂句法结构时语调生成的局限性，尤其是当句法边界模糊时的表现，以改进其自然度和准确性。

Method: 采用心理语言学方法分析TTS系统生成语调短语边界的行为，重点测试句法模糊句子（如花园路径句）的表现，并通过微调模型忽略逗号来优化语调生成。

Result: TTS系统在句法模糊句子中依赖表面标记（如逗号），但在简单结构中能利用深层次句法线索。微调后模型生成的语调模式更贴合句法结构。

Conclusion: TTS系统需进一步优化以更好地处理复杂句法结构的语调生成，当前仍依赖表面标记，但通过针对性训练可提升表现。

Abstract: We analyze the syntactic sensitivity of Text-to-Speech (TTS) systems using
methods inspired by psycholinguistic research. Specifically, we focus on the
generation of intonational phrase boundaries, which can often be predicted by
identifying syntactic boundaries within a sentence. We find that TTS systems
struggle to accurately generate intonational phrase boundaries in sentences
where syntactic boundaries are ambiguous (e.g., garden path sentences or
sentences with attachment ambiguity). In these cases, systems need superficial
cues such as commas to place boundaries at the correct positions. In contrast,
for sentences with simpler syntactic structures, we find that systems do
incorporate syntactic cues beyond surface markers. Finally, we finetune models
on sentences without commas at the syntactic boundary positions, encouraging
them to focus on more subtle linguistic cues. Our findings indicate that this
leads to more distinct intonation patterns that better reflect the underlying
structure.

</details>


### [67] [BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain](https://arxiv.org/abs/2505.22240)
*Yunsoo Kim,Yusuf Abdulle,Honghan Wu*

Key words: BioHopR, 多跳推理, 生物医学知识图谱, O3-mini, 基准测试

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: BioHopR是一个新的基准测试，用于评估生物医学知识图谱中的多跳多答案推理能力，填补了现有基准的空白。评估显示O3-mini表现最佳，但所有模型在多跳任务中表现显著下降。

Motivation: 现有基准无法评估生物医学领域的多跳推理，尤其是涉及一对多和多对多关系的查询，导致生物医学多跳推理的挑战未被充分探索。

Method: 基于PrimeKG构建BioHopR，包含1跳和2跳推理任务，反映真实世界的生物医学复杂性。评估了O3-mini、GPT4O、HuatuoGPT-o1-70B和Llama-3.3-70B等模型。

Result: O3-mini在1跳任务中精度达37.93%，2跳任务中为14.57%，优于其他模型。所有模型在多跳任务中表现显著下降。

Conclusion: BioHopR为生物医学多跳推理设立了新标准，揭示了专有和开源模型之间的差距，并为未来生物医学大语言模型的发展铺平了道路。

Abstract: Biomedical reasoning often requires traversing interconnected relationships
across entities such as drugs, diseases, and proteins. Despite the increasing
prominence of large language models (LLMs), existing benchmarks lack the
ability to evaluate multi-hop reasoning in the biomedical domain, particularly
for queries involving one-to-many and many-to-many relationships. This gap
leaves the critical challenges of biomedical multi-hop reasoning underexplored.
To address this, we introduce BioHopR, a novel benchmark designed to evaluate
multi-hop, multi-answer reasoning in structured biomedical knowledge graphs.
Built from the comprehensive PrimeKG, BioHopR includes 1-hop and 2-hop
reasoning tasks that reflect real-world biomedical complexities.
  Evaluations of state-of-the-art models reveal that O3-mini, a proprietary
reasoning-focused model, achieves 37.93% precision on 1-hop tasks and 14.57% on
2-hop tasks, outperforming proprietary models such as GPT4O and open-source
biomedical models including HuatuoGPT-o1-70B and Llama-3.3-70B. However, all
models exhibit significant declines in multi-hop performance, underscoring the
challenges of resolving implicit reasoning steps in the biomedical domain. By
addressing the lack of benchmarks for multi-hop reasoning in biomedical domain,
BioHopR sets a new standard for evaluating reasoning capabilities and
highlights critical gaps between proprietary and open-source models while
paving the way for future advancements in biomedical LLMs.

</details>


### [68] [MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/abs/2505.22264)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Saez,Héctor Cerezo-Costas,Pedro Alonso Doval,Jorge Alcalde Vesteiro*

Key words: LLMs, 表格数据, 代码生成, 问答系统, SemEval

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种解决SemEval 2025任务8（基于表格数据的问答）的方法，通过利用LLMs生成Python代码与表格互动，并通过多步骤流程（包括理解表格内容、生成自然语言指令、翻译为代码并执行）取得了70.50%的得分。

Motivation: 旨在通过自动化代码生成优化表格数据的问答任务，减少人工干预并提高准确性。

Method: 利用开源LLMs和多步骤流程（理解表格、生成指令、代码翻译和执行），针对每项任务优化提示。

Result: 在子任务1中取得了70.50%的得分。

Conclusion: 该方法展示了LLMs在表格问答任务中的潜力，但仍需优化以提升表现。

Abstract: In this paper we expose our approach to solve the \textit{SemEval 2025 Task
8: Question-Answering over Tabular Data} challenge. Our strategy leverages
Python code generation with LLMs to interact with the table and get the answer
to the questions. The process is composed of multiple steps: understanding the
content of the table, generating natural language instructions in the form of
steps to follow in order to get the answer, translating these instructions to
code, running it and handling potential errors or exceptions. These steps use
open source LLMs and fine grained optimized prompts for each task (step). With
this approach, we achieved a score of $70.50\%$ for subtask 1.

</details>


### [69] [Comprehensive Evaluation on Lexical Normalization: Boundary-Aware Approaches for Unsegmented Languages](https://arxiv.org/abs/2505.22273)
*Shohei Higashiyama,Masao Utiyama*

Key words: 词汇规范化, 未分词语言, 预训练模型, 日语数据集

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了未分词语言的词汇规范化问题，贡献包括创建大规模日语数据集、开发基于预训练模型的方法，并通过多视角实验展示了方法的有效性。

Motivation: 解决用户生成文本中非正式表达的处理问题，特别是在未分词语言中缺乏全面评估的现状。

Method: 基于最先进的预训练模型开发规范化方法，并在多领域日语数据集上进行实验。

Result: 实验表明，仅编码器和仅解码器方法在准确性和效率上均表现优异。

Conclusion: 提出的方法在多领域日语词汇规范化任务中表现出色，验证了预训练模型的潜力。

Abstract: Lexical normalization research has sought to tackle the challenge of
processing informal expressions in user-generated text, yet the absence of
comprehensive evaluations leaves it unclear which methods excel across multiple
perspectives. Focusing on unsegmented languages, we make three key
contributions: (1) creating a large-scale, multi-domain Japanese normalization
dataset, (2) developing normalization methods based on state-of-the-art
pretrained models, and (3) conducting experiments across multiple evaluation
perspectives. Our experiments show that both encoder-only and decoder-only
approaches achieve promising results in both accuracy and efficiency.

</details>


### [70] [Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review](https://arxiv.org/abs/2505.22280)
*Zihan Xu,Haotian Ma,Gongbo Zhang,Yihao Ding,Chunhua Weng,Yifan Peng*

Key words: 循证医学, 自然语言处理, 证据提取, 临床决策, 文献综述

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该调查综述了129项利用自然语言处理（NLP）促进循证医学（EBM）的研究，探讨NLP如何支持EBM的五个关键步骤，并提出未来研究方向以优化证据提取、合成与临床应用。

Motivation: 由于医学文献量大且快速增⻓，人工整理成本高，NLP方法可高效支持循证医学的证据识别、评估与传播。

Method: 通过系统回顾129项相关研究，分析NLP在EBM五个核心步骤（Ask, Acquire, Appraise, Apply, Assess）中的应用。

Result: NLP能显著提升EBM的证据提取、合成、评估和临床决策效率，但当前技术仍存在局限性。

Conclusion: NLP有望革命性优化EBM流程，未来需进一步研究以突破技术瓶颈并增强数据可理解性。

Abstract: Evidence-based medicine (EBM) is at the forefront of modern healthcare,
emphasizing the use of the best available scientific evidence to guide clinical
decisions. Due to the sheer volume and rapid growth of medical literature and
the high cost of curation, there is a critical need to investigate Natural
Language Processing (NLP) methods to identify, appraise, synthesize, summarize,
and disseminate evidence in EBM. This survey presents an in-depth review of 129
research studies on leveraging NLP for EBM, illustrating its pivotal role in
enhancing clinical decision-making processes. The paper systematically explores
how NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise,
Apply, and Assess. The review not only identifies current limitations within
the field but also proposes directions for future research, emphasizing the
potential for NLP to revolutionize EBM by refining evidence extraction,
evidence synthesis, appraisal, summarization, enhancing data comprehensibility,
and facilitating a more efficient clinical workflow.

</details>


### [71] [Compensating for Data with Reasoning: Low-Resource Machine Translation with LLMs](https://arxiv.org/abs/2505.22293)
*Samuel Frontull,Thomas Ströhle*

Key words: 多语言机器翻译, 低资源语言, 上下文学习, 提示工程, Fragment-Shot Prompting

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了Fragment-Shot Prompting方法，通过分块输入和基于语法覆盖的翻译示例检索来提升低资源语言的翻译质量，实验结果显示该方法在高推理能力的模型中表现更好。

Motivation: 解决大语言模型（LLMs）在低资源语言翻译中的挑战，特别是提示工程的问题。

Method: 提出Fragment-Shot Prompting和Pivoted Fragment-Shot两种上下文学习方法，通过分块输入和语法覆盖检索示例，利用多种LLMs（如GPT系列、LLaMA等）进行实验评估。

Result: 发现Fragment-Shot Prompting对低资源语言翻译有效，语法覆盖与翻译质量正相关；模型推理能力越强，翻译效果越好；提示工程对从低资源到高资源语言的翻译改善有限。

Conclusion: Fragment-Shot Prompting是一种有效的低资源语言翻译方法，尤其是对推理能力强的模型。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
multilingual machine translation, sometimes even outperforming traditional
neural systems. However, previous research has highlighted the challenges of
using LLMs, particularly with prompt engineering, for low-resource languages.
In this work, we introduce Fragment-Shot Prompting, a novel in-context learning
method that segments input and retrieves translation examples based on
syntactic coverage, along with Pivoted Fragment-Shot, an extension that enables
translation without direct parallel data. We evaluate these methods using
GPT-3.5, GPT-4o, o1-mini, LLaMA-3.3, and DeepSeek-R1 for translation between
Italian and two Ladin variants, revealing three key findings: (1) Fragment-Shot
Prompting is effective for translating into and between the studied
low-resource languages, with syntactic coverage positively correlating with
translation quality; (2) Models with stronger reasoning abilities make more
effective use of retrieved knowledge, generally produce better translations,
and enable Pivoted Fragment-Shot to significantly improve translation quality
between the Ladin variants; and (3) prompt engineering offers limited, if any,
improvements when translating from a low-resource to a high-resource language,
where zero-shot prompting already yields satisfactory results. We publicly
release our code and the retrieval corpora.

</details>


### [72] [360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long Post-Training](https://arxiv.org/abs/2505.22296)
*Haosheng Zou,Xiaowei Lv,Shousheng Jia,Xiangzheng Zhang*

Key words: 序列并行, LLaMA-Factory, 开源项目, 模型训练

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇技术报告介绍了360-LLaMA-Factory开源项目，通过引入序列并行技术，其应用已广泛覆盖多个模型和大公司训练框架。

Motivation: 探讨不同序列并行模式的技术细节和实现经验，提升大规模模型训练的效率和效果。

Method: 在LLaMA-Factory中引入序列并行技术，并开源360-LLaMA-Factory项目。

Result: 项目获得广泛应用，支持Light-R1、TinyR1、Kaggle AIMO数学模型等多模型及大公司训练框架。

Conclusion: 序列并行技术在大规模模型训练中具有显著优势和应用潜力。

Abstract: Adding sequence parallelism into LLaMA-Factory, we open-sourced
360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory.
360-LLaMA-Factory has received wide recognition and used in models such as
Light-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and
also in large companies' training frameworks. This technical report delves
deeper into the different sequence parallel modes behind 360-LLaMA-Factory and
discusses our implementation insights.

</details>


### [73] [Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing](https://arxiv.org/abs/2505.22298)
*Yifan Lu,Jing Li,Yigeng Zhou,Yihui Zhang,Wenya Wang,Xiucheng Li,Meishan Zhang,Fangming Liu,Jun Yu,Min Zhang*

Key words: 大型语言模型, 解毒, 毒性检测, 知识编辑, 通用能力

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为ToxEdit的新方法，用于动态检测和减轻大型语言模型中的毒性内容，同时保持模型的通用能力。

Motivation: 现有的大型语言模型解毒方法存在对显式实体的依赖和过度编辑的问题，影响了模型的整体性能。

Method: ToxEdit通过动态检测前向传播中的毒性激活模式，并自适应地调整计算路径来有效减轻毒性。

Result: 实验证明ToxEdit在解毒性能和保持模型通用能力方面优于现有方法。

Conclusion: ToxEdit提供了一种更精确的毒性减轻方法，同时避免了过度编辑的问题。

Abstract: Large language models (LLMs) exhibit impressive language capabilities but
remain vulnerable to malicious prompts and jailbreaking attacks. Existing
knowledge editing methods for LLM detoxification face two major challenges.
First, they often rely on entity-specific localization, making them ineffective
against adversarial inputs without explicit entities. Second, these methods
suffer from over-editing, where detoxified models reject legitimate queries,
compromising overall performance. In this paper, we propose ToxEdit, a
toxicity-aware knowledge editing approach that dynamically detects toxic
activation patterns during forward propagation. It then routes computations
through adaptive inter-layer pathways to mitigate toxicity effectively. This
design ensures precise toxicity mitigation while preserving LLMs' general
capabilities. To more accurately assess over-editing, we also enhance the
SafeEdit benchmark by incorporating instruction-following evaluation tasks.
Experimental results on multiple LLMs demonstrate that our ToxEdit outperforms
previous state-of-the-art methods in both detoxification performance and
safeguarding general capabilities of LLMs.

</details>


### [74] [If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?](https://arxiv.org/abs/2505.22318)
*Ishwar B Balappanawar,Vamshi Krishna Bonagiri,Anish R Joishy,Manas Gaur,Krishnaprasad Thirunarayan,Ponnurangam Kumaraguru*

Key words: 逻辑推理；反事实场景；LLMs；知识冲突；提示方法

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文探讨了LLMs在上下文与参数知识冲突时的推理能力，提出CounterLogic数据集和Self-Segregate提示方法，显著提升了模型在反事实推理中的表现。

Motivation: 研究LLMs在知识冲突情境下的逻辑推理能力退化现象，并寻求提升方法。

Method: 引入CounterLogic数据集评估反事实推理，提出Self-Segregate提示方法增强元认知意识。

Result: 反事实场景下模型准确率平均下降27%，但新方法将差距缩小至11%并提升总准确率7.5%。

Conclusion: 研究为理解和增强LLMs在知识独立推理中的能力提供了实践启示。

Abstract: Large Language Models (LLMs) demonstrate impressive reasoning capabilities in
familiar contexts, but struggle when the context conflicts with their
parametric knowledge. To investigate this phenomenon, we introduce
CounterLogic, a dataset containing 1,800 examples across 9 logical schemas,
explicitly designed to evaluate logical reasoning through counterfactual
(hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11
LLMs across 6 different datasets reveals a consistent performance degradation,
with accuracies dropping by 27% on average when reasoning through
counterfactual information. We propose Self-Segregate, a prompting method
enabling metacognitive awareness (explicitly identifying knowledge conflicts)
before reasoning. Our method dramatically narrows the average performance gaps
from 27% to just 11%, while significantly increasing the overall accuracy
(+7.5%). We discuss the implications of these findings and draw parallels to
human cognitive processes, particularly on how humans disambiguate conflicting
information during reasoning tasks. Our findings offer practical insights for
understanding and enhancing LLMs reasoning capabilities in real-world
applications, especially where models must logically reason independently of
their factual knowledge.

</details>


### [75] [Advancing Expert Specialization for Better MoE](https://arxiv.org/abs/2505.22323)
*Hongcan Guo,Haolang Lu,Guoshun Nan,Bolun Chu,Jialin Zhuang,Yuan Yang,Wenhao Che,Sicong Leng,Qimei Cui,Xudong Jiang*

Key words: Mixture-of-Experts, expert specialization, orthogonality loss, variance loss, large language models

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种改进MoE模型的方法，通过引入正交性和方差损失来提升专家专业化，实验证明其显著提升性能。

Motivation: 现有的MoE模型中辅助负载平衡损失导致专家重叠和路由过于均匀，限制了专家专业化并降低性能。

Method: 引入了正交性损失和方差损失，前者促使专家处理不同类型的令牌，后者促进更具区分性的路由决策。

Result: 在多种模型架构和基准测试中，该方法显著提升了专家专业化，性能提升最高达23.79%。

Conclusion: 所提出的方法在不改变架构或增加组件的情况下，有效优化了MoE模型的训练过程，并保持了负载平衡。

Abstract: Mixture-of-Experts (MoE) models enable efficient scaling of large language
models (LLMs) by activating only a subset of experts per input. However, we
observe that the commonly used auxiliary load balancing loss often leads to
expert overlap and overly uniform routing, which hinders expert specialization
and degrades overall performance during post-training. To address this, we
propose a simple yet effective solution that introduces two complementary
objectives: (1) an orthogonality loss to encourage experts to process distinct
types of tokens, and (2) a variance loss to encourage more discriminative
routing decisions. Gradient-level analysis demonstrates that these objectives
are compatible with the existing auxiliary loss and contribute to optimizing
the training process. Experimental results over various model architectures and
across multiple benchmarks show that our method significantly enhances expert
specialization. Notably, our method improves classic MoE baselines with
auxiliary loss by up to 23.79%, while also maintaining load balancing in
downstream tasks, without any architectural modifications or additional
components. We will release our code to contribute to the community.

</details>


### [76] [NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment](https://arxiv.org/abs/2505.22327)
*Antonia Karamolegkou,Angana Borah,Eunjung Cho,Sagnik Ray Choudhury,Martina Galletti,Rajarshi Ghosh,Pranav Gupta,Oana Ignat,Priyanka Kargupta,Neema Kotonya,Hemank Lamba,Sun-Joo Lee,Arushi Mangla,Ishani Mondal,Deniz Nazarova,Poli Nemkova,Dina Pisarevskaya,Naquee Rizwan,Nazanin Sabri,Dominik Stammbach,Anna Steinberg,David Tomás,Steven R Wilson,Bowen Yi,Jessica H Zhu,Arkaitz Zubiaga,Anders Søgaard,Alexander Fraser,Zhijing Jin,Rada Mihalcea,Joel R. Tetreault,Daryna Dementieva*

Key words: 大语言模型, NLP4SG, 社会责任, 跨学科分析, 公平性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了如何在自然语言处理（NLP）领域更负责任地应用大语言模型（LLMs），以解决社会问题，并提出了研究方向与挑战。

Motivation: 随着大语言模型的快速发展，NLP领域需要更谨慎地部署技术，以促进社会公益并应对潜在风险。

Method: 通过跨学科分析社会目标与新兴风险，提出研究方向。

Result: 明确了NLP4SG研究的潜在方向与需解决的挑战。

Conclusion: NLP领域需以责任与公平为核心，推动技术为社会服务。

Abstract: Recent advancements in large language models (LLMs) have unlocked
unprecedented possibilities across a range of applications. However, as a
community, we believe that the field of Natural Language Processing (NLP) has a
growing need to approach deployment with greater intentionality and
responsibility. In alignment with the broader vision of AI for Social Good
(Toma\v{s}ev et al., 2020), this paper examines the role of NLP in addressing
pressing societal challenges. Through a cross-disciplinary analysis of social
goals and emerging risks, we highlight promising research directions and
outline challenges that must be addressed to ensure responsible and equitable
progress in NLP4SG research.

</details>


### [77] [Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start](https://arxiv.org/abs/2505.22334)
*Lai Wei,Yuting Li,Kaipeng Zheng,Chen Wang,Yue Wang,Linghe Kong,Lichao Sun,Weiran Huang*

Key words: 多模态大型语言模型, 监督微调, 强化学习, 推理能力, 基准测试

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文展示了如何在多模态大型语言模型中通过两阶段方法（监督微调和强化学习）提升推理能力，并在多个基准测试中取得卓越成果。

Motivation: 研究动机在于探讨多模态大型语言模型中‘顿悟’模式是否仅由强化学习引发，并提出结合监督微调和强化学习的方法以优化推理性能。

Method: 采用两阶段方法：1）监督微调（SFT）作为冷启动；2）通过GRPO进行强化学习以进一步优化模型推理能力。

Result: 实验结果表明，这种结合方法显著优于仅使用SFT或RL的方法，3B和7B模型在多模态推理基准测试中均取得优异表现。

Conclusion: 论文为构建先进的多模态推理模型提供了实用指导，并通过开源代码推动了该领域的发展。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
impressive chain-of-thought reasoning capabilities, with reinforcement learning
(RL) playing a crucial role in this progress. While "aha moment"
patterns--where models exhibit self-correction through reflection--are often
attributed to emergent properties from RL, we first demonstrate that these
patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not
necessarily correlate with improved reasoning performance. Building on these
insights, we present a comprehensive study on enhancing multimodal reasoning
through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start
with structured chain-of-thought reasoning patterns, followed by (2)
reinforcement learning via GRPO to further refine these capabilities. Our
extensive experiments show that this combined approach consistently outperforms
both SFT-only and RL-only methods across challenging multimodal reasoning
benchmarks. The resulting models achieve state-of-the-art performance among
open-source MLLMs at both 3B and 7B scales, with our 7B model showing
substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on
MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving
performance competitive with several 7B models. Overall, this work provides
practical guidance for building advanced multimodal reasoning models. Our code
is available at https://github.com/waltonfuture/RL-with-Cold-Start.

</details>


### [78] [Text2Grad: Reinforcement Learning from Natural Language Feedback](https://arxiv.org/abs/2505.22338)
*Hanyang Wang,Lu Wang,Chaoyun Zhang,Tianjun Mao,Si Qin,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Key words: 强化学习, 文本反馈, 细粒度优化, 可解释性, 自然语言处理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Text2Grad 是一种将文本反馈转换为细粒度梯度的强化学习方法，通过精准调整模型策略，优于传统标量奖励RL和提示基准。

Motivation: 传统RLHF使用粗粒度的标量奖励，学习效率低且不透明。文本反馈虽提升可解释性，但未调整模型参数，因此需将反馈转化为细粒度梯度以优化策略。

Method: Text2Grad包含三个组件：(1) 将反馈与词元跨度对齐的标注流程，(2) 预测细粒度奖励并生成解释性反馈的奖励模型，(3) 通过自然语言梯度反传播优化策略的模块。

Result: 在摘要、代码生成和问答任务中，Text2Grad显著优于标量奖励RL和提示基准，任务指标更高且可解释性更强。

Conclusion: 自然语言反馈转化为梯度后，能有效支持细粒度策略优化，提升模型性能与可解释性。

Abstract: Traditional RLHF optimizes language models with coarse, scalar rewards that
mask the fine-grained reasons behind success or failure, leading to slow and
opaque learning. Recent work augments RL with textual critiques through
prompting or reflection, improving interpretability but leaving model
parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm
that turns free-form textual feedback into span-level gradients. Given human
(or programmatic) critiques, Text2Grad aligns each feedback phrase with the
relevant token spans, converts these alignments into differentiable reward
signals, and performs gradient updates that directly refine the offending
portions of the model's policy. This yields precise, feedback-conditioned
adjustments instead of global nudges. Text2Grad is realized through three
components: (1) a high-quality feedback-annotation pipeline that pairs
critiques with token spans; (2) a fine-grained reward model that predicts
span-level reward on answer while generating explanatory critiques; and (3) a
span-level policy optimizer that back-propagates natural-language gradients.
Across summarization, code generation, and question answering, Text2Grad
consistently surpasses scalar-reward RL and prompt-only baselines, providing
both higher task metrics and richer interpretability. Our results demonstrate
that natural-language feedback, when converted to gradients, is a powerful
signal for fine-grained policy optimization. The code for our method is
available at https://github.com/microsoft/Text2Grad

</details>


### [79] [LLMs Struggle to Reject False Presuppositions when Misinformation Stakes are High](https://arxiv.org/abs/2505.22354)
*Judith Sieker,Clara Lachenmaier,Sina Zarrieß*

Key words: LLM, false presuppositions, misinformation, linguistic analysis, political context

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLM）对错误预设的处理能力，并探讨了语言因素是否影响其响应。通过系统分析，研究发现LLM难以识别错误预设，性能随条件变化。

Motivation: 探讨LLM是否能像人类一样检测和纠正错误预设引发的误导信息，尤其是在政治背景下的高风险场景。

Method: 基于语言预设分析的系统方法，实验使用了新创建的数据集，测试了GPT-4-o、LLama-3-8B和Mistral-7B-v03三种LLM。

Result: 模型普遍难以识别错误预设，性能表现因条件（如语言结构、政治派别等）而异。

Conclusion: 语言预设分析是揭示LLM回应中政治错误信息强化现象的有力工具。

Abstract: This paper examines how LLMs handle false presuppositions and whether certain
linguistic factors influence their responses to falsely presupposed content.
Presuppositions subtly introduce information as given, making them highly
effective at embedding disputable or false information. This raises concerns
about whether LLMs, like humans, may fail to detect and correct misleading
assumptions introduced as false presuppositions, even when the stakes of
misinformation are high. Using a systematic approach based on linguistic
presupposition analysis, we investigate the conditions under which LLMs are
more or less sensitive to adopt or reject false presuppositions. Focusing on
political contexts, we examine how factors like linguistic construction,
political party, and scenario probability impact the recognition of false
presuppositions. We conduct experiments with a newly created dataset and
examine three LLMs: OpenAI's GPT-4-o, Meta's LLama-3-8B, and MistralAI's
Mistral-7B-v03. Our results show that the models struggle to recognize false
presuppositions, with performance varying by condition. This study highlights
that linguistic presupposition analysis is a valuable tool for uncovering the
reinforcement of political misinformation in LLM responses.

</details>


### [80] [Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition](https://arxiv.org/abs/2505.22375)
*Hanting Chen,Yasheng Wang,Kai Han,Dong Li,Lin Li,Zhenni Bi,Jinpeng Li,Haoyu Wang,Fei Mi,Mingjian Zhu,Bin Wang,Kaikai Song,Yifei Fu,Xu He,Yu Luo,Chong Zhu,Quan He,Xueyu Wu,Wei He,Hailin Hu,Yehui Tang,Dacheng Tao,Xinghao Chen,Yunhe Wang,Other Contributors*

Key words: 大型语言模型, 推理优化, 双阶段训练, 强化学习, 双系统框架

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Pangu Embedded 是一种高效的大型语言模型，专为推理任务设计，采用双阶段训练和双系统框架，结合快速和慢速推理模式，以解决现有推理优化 LLM 的高计算成本和延迟问题。

Motivation: 为了解决现有推理优化大型语言模型的高计算成本和推理延迟问题，开发了 Pangu Embedded，旨在提供高效且灵活的快慢推理能力。

Method: 采用两阶段训练框架：阶段 1 通过迭代蒸馏和强化学习优化性能，阶段 2 引入双系统框架支持快慢推理模式的动态切换。

Result: 实验结果显示，Pangu Embedded 在多个基准测试中表现优于同类模型，提供快速响应和最先进的推理质量。

Conclusion: Pangu Embedded 展示了高效且实用的 LLM 推理器的发展方向。

Abstract: This work presents Pangu Embedded, an efficient Large Language Model (LLM)
reasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible
fast and slow thinking capabilities. Pangu Embedded addresses the significant
computational costs and inference latency challenges prevalent in existing
reasoning-optimized LLMs. We propose a two-stage training framework for its
construction. In Stage 1, the model is finetuned via an iterative distillation
process, incorporating inter-iteration model merging to effectively aggregate
complementary knowledge. This is followed by reinforcement learning on Ascend
clusters, optimized by a latency-tolerant scheduler that combines stale
synchronous parallelism with prioritized data queues. The RL process is guided
by a Multi-source Adaptive Reward System (MARS), which generates dynamic,
task-specific reward signals using deterministic metrics and lightweight LLM
evaluators for mathematics, coding, and general problem-solving tasks. Stage 2
introduces a dual-system framework, endowing Pangu Embedded with a "fast" mode
for routine queries and a deeper "slow" mode for complex inference. This
framework offers both manual mode switching for user control and an automatic,
complexity-aware mode selection mechanism that dynamically allocates
computational resources to balance latency and reasoning depth. Experimental
results on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate
that Pangu Embedded with 7B parameters, outperforms similar-size models like
Qwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art
reasoning quality within a single, unified model architecture, highlighting a
promising direction for developing powerful yet practically deployable LLM
reasoners.

</details>


### [81] [RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning](https://arxiv.org/abs/2505.22430)
*Kun Li,Yunxiang Li,Tianhua Zhang,Hongyin Luo,Xixin Wu,James Glass,Helen Meng*

Key words: RAG, 评估框架, 强化学习, 零人工标注, 解释性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: RAG-Zeval是一个新颖的端到端框架，通过规则引导的推理任务评估RAG系统的忠实性和正确性，使用强化学习训练评估器，减少计算成本并提升性能。

Motivation: 当前基于LLM的评估框架依赖资源密集型模型和复杂多阶段提示，未充分利用模型推理能力且计算成本高，RAG-Zeval旨在解决这些问题。

Method: 通过强化学习训练评估器，引入基于排名的结果奖励机制，利用偏好判断而非绝对分数，合成无人工标注的排名参考。

Result: RAG-Zeval在人类判断相关性中表现最佳，优于参数量大10-100倍的基线模型，并具有更高的解释性。

Conclusion: RAG-Zeval提供了一种高效、可解释的RAG系统评估方法，显著降低计算成本并提升评估质量。

Abstract: Robust evaluation is critical for deploying trustworthy retrieval-augmented
generation (RAG) systems. However, current LLM-based evaluation frameworks
predominantly rely on directly prompting resource-intensive models with complex
multi-stage prompts, underutilizing models' reasoning capabilities and
introducing significant computational cost. In this paper, we present RAG-Zeval
(RAG-Zero Evaluator), a novel end-to-end framework that formulates faithfulness
and correctness evaluation as a rule-guided reasoning task. Our approach trains
evaluators with reinforcement learning, facilitating compact models to generate
comprehensive and sound assessments with detailed explanation in one-pass. We
introduce a ranking-based outcome reward mechanism, using preference judgments
rather than absolute scores, to address the challenge of obtaining precise
pointwise reward signals. To this end, we synthesize the ranking references by
generating quality-controlled responses with zero human annotation. Experiments
demonstrate RAG-Zeval's superior performance, achieving the strongest
correlation with human judgments and outperforming baselines that rely on LLMs
with 10-100 times more parameters. Our approach also exhibits superior
interpretability in response evaluation.

</details>


### [82] [Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO](https://arxiv.org/abs/2505.22453)
*Lai Wei,Yuting Li,Chen Wang,Yue Wang,Linghe Kong,Weiran Huang,Lichao Sun*

Key words: 多模态大语言模型、无监督后训练、GRPO算法、自我奖励机制、持续改进

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为MM-UPT的无监督后训练框架，基于GRPO算法实现多模态大语言模型的自我持续改进，无需外部监督，实验证明其显著提升了模型性能。

Motivation: 现有方法依赖昂贵的标注数据或复杂的无监督方法，限制了多模态大语言模型（MLLMs）的后训练效率。因此，作者探索了无需外部监督、简单且可扩展的自我改进方法。

Method: 提出MM-UPT框架，基于GRPO算法，采用多数投票的自奖励机制，替代传统奖励信号，并利用模型自身生成的合成问题进一步提升性能。

Result: 在MathVista和We-Math数据集上分别提升6.6%和5.8%，效果优于基线方法，并接近有监督GRPO的结果。

Conclusion: MM-UPT为无监督环境下的MLLM持续自我改进提供了新范式，展现了可扩展性潜力。

Abstract: Improving Multi-modal Large Language Models (MLLMs) in the post-training
stage typically relies on supervised fine-tuning (SFT) or reinforcement
learning (RL). However, these supervised methods require expensive and manually
annotated multi-modal data--an ultimately unsustainable resource. While recent
efforts have explored unsupervised post-training, their methods are complex and
difficult to iterate. In this work, we are the first to investigate the use of
GRPO, a stable and scalable online RL algorithm, for enabling continual
self-improvement without any external supervision. We propose MM-UPT, a simple
yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds
upon GRPO, replacing traditional reward signals with a self-rewarding mechanism
based on majority voting over multiple sampled responses. Our experiments
demonstrate that MM-UPT significantly improves the reasoning ability of
Qwen2.5-VL-7B (e.g., 66.3 %$\rightarrow$72.9 % on MathVista, 62.9
%$\rightarrow$68.7 % on We-Math), using standard dataset without ground truth
labels. MM-UPT also outperforms prior unsupervised baselines and even
approaches the results of supervised GRPO. Furthermore, we show that
incorporating synthetic questions, generated solely by MLLM itself, can boost
performance as well, highlighting a promising approach for scalable
self-improvement. Overall, MM-UPT offers a new paradigm for continual,
autonomous enhancement of MLLMs in the absence of external supervision. Our
code is available at https://github.com/waltonfuture/MM-UPT.

</details>


### [83] [EvolveSearch: An Iterative Self-Evolving Search Agent](https://arxiv.org/abs/2505.22501)
*Dingchu Zhang,Yida Zhao,Jialong Wu,Baixuan Li,Wenbiao Yin,Liwen Zhang,Yong Jiang,Yufeng Li,Kewei Tu,Pengjun Xie,Fei Huang*

Key words: 大型语言模型（LLM）、工具增强、自我进化、开放搜索、多跳问答

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了名为EvolveSearch的迭代自进化框架，结合监督微调（SFT）和强化学习（RL），无需外部人工标注数据即提升了LLM在开放搜索领域的网络搜索能力。在七个多跳问答基准测试中表现出持续性能提升，平均改进达4.7%。

Motivation: 当前主流方法（如监督微调和强化学习）在开放搜索领域面临数据生成和利用效率的限制，亟需一种无需外部标注的自进化方案以提升LLM的搜索能力。

Method: 提出EvolveSearch框架，通过迭代结合监督微调（SFT）和强化学习（RL），实现LLM的自我进化，无需人工标注数据。

Result: 在七个多跳问答（MHQA）基准测试中，性能随迭代持续提升，最终平均超越现有最优方法4.7%。

Conclusion: EvolveSearch为开放网络搜索领域的自进化智能代理能力提供了可行路径。

Abstract: The rapid advancement of large language models (LLMs) has transformed the
landscape of agentic information seeking capabilities through the integration
of tools such as search engines and web browsers. However, current mainstream
approaches for enabling LLM web search proficiency face significant challenges:
supervised fine-tuning struggles with data production in open-search domains,
while RL converges quickly, limiting their data utilization efficiency. To
address these issues, we propose EvolveSearch, a novel iterative self-evolution
framework that combines SFT and RL to enhance agentic web search capabilities
without any external human-annotated reasoning data. Extensive experiments on
seven multi-hop question-answering (MHQA) benchmarks demonstrate that
EvolveSearch consistently improves performance across iterations, ultimately
achieving an average improvement of 4.7\% over the current state-of-the-art
across seven benchmarks, opening the door to self-evolution agentic
capabilities in open web search domains.

</details>


### [84] [Multi-MLLM Knowledge Distillation for Out-of-Context News Detection](https://arxiv.org/abs/2505.22517)
*Yimeng Gu,Zhao Tong,Ignacio Castro,Shu Wu,Gareth Tyson*

Key words: 多模态大模型、脱离上下文新闻、知识蒸馏、低资源、微调

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种两阶段知识蒸馏框架，通过利用教师模型生成预测和解释来高效提升小型多模态大模型在脱离上下文新闻检测中的性能，仅需少量标注数据即可达到最佳效果。

Motivation: 现有方法依赖大量标注数据或昂贵API调用，不适合低资源场景，因此研究如何在更高效和低成本的情况下提升小型多模态大模型的性能。

Method: 首先通过教师模型生成预测和解释作为知识，然后分两阶段（LoRA微调和DPO微调）将知识蒸馏到学生模型，专注于教师预测冲突的数据点。

Result: 实验表明，该方法仅需不到10%的标注数据即可达到最优性能。

Conclusion: 提出的两阶段蒸馏方法在低资源场景下显著提升了小型多模态大模型的检测能力。

Abstract: Multimodal out-of-context news is a type of misinformation in which the image
is used outside of its original context. Many existing works have leveraged
multimodal large language models (MLLMs) for detecting out-of-context news.
However, observing the limited zero-shot performance of smaller MLLMs, they
generally require label-rich fine-tuning and/or expensive API calls to GPT
models to improve the performance, which is impractical in low-resource
scenarios. In contrast, we aim to improve the performance of small MLLMs in a
more label-efficient and cost-effective manner. To this end, we first prompt
multiple teacher MLLMs to generate both label predictions and corresponding
rationales, which collectively serve as the teachers' knowledge. We then
introduce a two-stage knowledge distillation framework to transfer this
knowledge to a student MLLM. In Stage 1, we apply LoRA fine-tuning to the
student model using all training data. In Stage 2, we further fine-tune the
student model using both LoRA fine-tuning and DPO on the data points where
teachers' predictions conflict. This two-stage strategy reduces annotation
costs and helps the student model uncover subtle patterns in more challenging
cases. Experimental results demonstrate that our approach achieves
state-of-the-art performance using less than 10% labeled data.

</details>


### [85] [Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs](https://arxiv.org/abs/2505.22548)
*Changhao Song,Yazhou Zhang,Peng Zhang*

Key words: 情感理解，自适应推理，强化学习，深度推理，多目标优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种任务自适应推理框架DeepSeek-R1，通过结合微调与强化学习，动态生成可变长度的推理链以适应不同复杂度的情感任务，显著提升了基础与高级情感任务的性能。

Motivation: 当前基于固定长度链式推理（CoT）的方法无法适应情感任务的复杂度变化，亟需一种自适应框架来动态调整推理深度与多样性，以更准确地理解情感。

Method: 提出结合微调与强化学习的任务自适应推理框架，通过复合奖励函数平衡预测准确性、推理深度控制、路径多样性和逻辑重复抑制四大目标。

Result: 实验显示，在情感、情绪、幽默和讽刺四项任务中，性能持续提升，基础任务F1峰值提升3.56%（Acc 2.76%），高级任务F1峰值提升37.95%（Acc 23.14%）。

Conclusion: 该研究通过自适应深度分析弥合了固定CoT推理与情感复杂性之间的鸿沟，为LLM的深度推理能力提供了新思路。

Abstract: Emotion understanding includes basic tasks (e.g., sentiment/emotion
classification) and advanced tasks (e.g., sarcasm/humor detection). Current
methods rely on fixed-length CoT reasoning, failing to adapt to the varying
complexity of emotions. We propose a task-adaptive reasoning framework that
employs DeepSeek-R1 to generate variable-length reasoning chains for different
emotion tasks. By combining fine-tuning with reinforcement learning, we design
a composite reward function that balances four objectives: prediction accuracy,
adaptive reasoning depth control, structural diversity in reasoning paths, and
suppression of repetitive logic. This approach achieves dynamic
context-sensitive inference while enabling LLMs to autonomously develop deep
reasoning capabilities. Experimental results demonstrate consistent
improvements in both Acc and F1 scores across four tasks: emotion, sentiment,
humor, and sarcasm. Notably, peak enhancements reached 3.56% F1 (2.76% Acc) for
basic tasks and 37.95% F1 (23.14% Acc) for advanced tasks. Our work bridges
rigid CoT reasoning and emotional complexity through adaptive-depth analysis.

</details>


### [86] [ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM](https://arxiv.org/abs/2505.22552)
*Hoang Pham,Thanh-Do Nguyen,Khac-Hoai Nam Bui*

Key words: 知识图谱,大型语言模型,声明确认,零样本学习,伪子图

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ClaimPKG通过将知识图谱与大型语言模型结合，提出了一种端到端框架，显著提升声明确认的性能，并在多个数据集上展现出零样本泛化能力。

Motivation: 解决现有声明确认方法难以有效利用知识图谱结构化知识的问题，以及大型语言模型在多步推理和知识图谱应用中的不足。

Method: 使用轻量级专用语言模型生成伪子图，结合子图检索模块提取相关子图，再由通用语言模型进行最终判断和解释。

Result: 在FactKG数据集上表现优于基准方法9%-12%，同时在HoVer和FEVEROUS数据集上展示出零样本泛化能力。

Conclusion: ClaimPKG成功将结构化知识与语言模型推理结合，为声明确认领域提供了高效解决方案。

Abstract: Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of
large language models (LLMs) is an emerging research challenge in claim
verification. While KGs provide structured, semantically rich representations
well-suited for reasoning, most existing verification methods rely on
unstructured text corpora, limiting their ability to effectively leverage KGs.
Additionally, despite possessing strong reasoning abilities, modern LLMs
struggle with multi-step modular pipelines and reasoning over KGs without
adaptation. To address these challenges, we propose ClaimPKG, an end-to-end
framework that seamlessly integrates LLM reasoning with structured knowledge
from KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,
specialized LLM to represent the input claim as pseudo-subgraphs, guiding a
dedicated subgraph retrieval module to identify relevant KG subgraphs. These
retrieved subgraphs are then processed by a general-purpose LLM to produce the
final verdict and justification. Extensive experiments on the FactKG dataset
demonstrate that ClaimPKG achieves state-of-the-art performance, outperforming
strong baselines in this research field by 9%-12% accuracy points across
multiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability
to unstructured datasets such as HoVer and FEVEROUS, effectively combining
structured knowledge from KGs with LLM reasoning across various LLM backbones.

</details>


### [87] [Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings](https://arxiv.org/abs/2505.22563)
*Yu Lei,Xingyang Ge,Yi Zhang,Yiming Yang,Bolei Ma*

Key words: 大型语言模型, 人类大脑, 层级表征, 句子理解, fMRI, 语义抽象

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文研究了大型语言模型（LLMs）与人类大脑在句子处理层面的相似性，发现模型性能提升会推动其表征架构向类似大脑的层级结构演化，尤其在高级语义抽象层面。

Motivation: 探讨LLMs与人类大脑是否在计算原理上趋同，尤其是模型中的层级表征是否与人类句子理解时的神经响应动态对齐。

Method: 通过比较14个公开LLMs的层级嵌入与人类被试在自然叙事故事中的fMRI数据，构建句子级神经预测模型，分析模型层与脑区激活的相关性。

Result: 结果表明，模型性能的提升会使其表征架构更接近大脑的层级结构，尤其是在高级语义抽象层面表现出更强的功能和解剖对应关系。

Conclusion: LLMs的表征架构在进化过程中逐渐与人类语言处理的神经机制对齐，支持两者在计算原理上的潜在相似性。

Abstract: Understanding whether large language models (LLMs) and the human brain
converge on similar computational principles remains a fundamental and
important question in cognitive neuroscience and AI. Do the brain-like patterns
observed in LLMs emerge simply from scaling, or do they reflect deeper
alignment with the architecture of human language processing? This study
focuses on the sentence-level neural mechanisms of language models,
systematically investigating how hierarchical representations in LLMs align
with the dynamic neural responses during human sentence comprehension. By
comparing hierarchical embeddings from 14 publicly available LLMs with fMRI
data collected from participants, who were exposed to a naturalistic narrative
story, we constructed sentence-level neural prediction models to precisely
identify the model layers most significantly correlated with brain region
activations. Results show that improvements in model performance drive the
evolution of representational architectures toward brain-like hierarchies,
particularly achieving stronger functional and anatomical correspondence at
higher semantic abstraction levels.

</details>


### [88] [Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2505.22571)
*Hoang Pham,Khac-Hoai Nam Bui*

Key words: 检索增强生成,RAG,LLM代理,多跳查询,统一框架

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于LLM代理的统一检索增强生成（RAG）框架Agent-UniRAG，能够同时处理单跳和多跳查询，并通过合成数据集SynAgent-RAG提升小规模开源LLM的性能。

Motivation: 现有RAG系统通常单独处理单跳或多跳查询，限制了实际应用。本文旨在通过LLM代理框架统一解决这一问题，提升系统的有效性和可解释性。

Method: 提出Agent-UniRAG框架，通过LLM代理逐步解决不同复杂度（单跳/多跳）的RAG任务，并引入合成数据集SynAgent-RAG支持小规模开源LLM。

Result: 实验表明，Agent-UniRAG在多个RAG基准测试中表现优异，与闭源及更大规模的开源LLM性能相当。

Conclusion: Agent-UniRAG为统一处理复杂RAG任务提供了高效且可解释的解决方案，未来可进一步推广。

Abstract: This paper presents a novel approach for unified retrieval-augmented
generation (RAG) systems using the recent emerging large language model (LLM)
agent concept. Specifically, Agent LLM, which utilizes LLM as fundamental
controllers, has become a promising approach to enable the interpretability of
RAG tasks, especially for complex reasoning question-answering systems (e.g.,
multi-hop queries). Nonetheless, previous works mainly focus on solving RAG
systems with either single-hop or multi-hop approaches separately, which limits
the application of those approaches to real-world applications. In this study,
we propose a trainable agent framework called Agent-UniRAG for unified
retrieval-augmented LLM systems, which enhances the effectiveness and
interpretability of RAG systems. The main idea is to design an LLM agent
framework to solve RAG tasks step-by-step based on the complexity of the
inputs, simultaneously including single-hop and multi-hop queries in an
end-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset
to enable the proposed agent framework for small open-source LLMs (e.g.,
Llama-3-8B). The results show comparable performances with closed-source and
larger open-source LLMs across various RAG benchmarks. Our source code and
dataset are publicly available for further exploitation.

</details>


### [89] [Fusion Steering: Prompt-Specific Activation Control](https://arxiv.org/abs/2505.22572)
*Waldemar Chang,Alhassan Yasin*

Key words: Fusion Steering, 大型语言模型, 问答任务, 激活导向, 动态注入

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Fusion Steering 是一种激活导向方法，通过动态注入特定提示的激活差异提升大型语言模型（LLM）在问答任务中的事实准确性。

Motivation: 传统方法限于单层或固定层操作，缺乏灵活性；Fusion Steering 旨在通过全网激活控制和优化权重提升模型性能。

Method: 采用动态激活差异注入（源自包含真实答案和模型生成解释的参考完成）及 Optuna 优化的权重配置，联合目标为平衡事实对齐和流畅性。

Result: 在 260 个 SimpleQA 提示上，分段导向方法的准确率达 25.4%，远超基线（3.5%）和全层导向（16.2%）；严格标准下完全正确答案从 0% 提升至 13.1%。

Conclusion: 分段动态干预策略和全网激活控制具有显著优势，且支持稀疏表示，为可解释、可扩展的激活级控制提供了方向。

Abstract: We present Fusion Steering, an activation steering methodology that improves
factual accuracy in large language models (LLMs) for question-answering (QA)
tasks. This approach introduces flexible steering configurations, including
full-layer steering and segmented steering. Unlike traditional methods
constrained to single-layer or fixed-layer operations, Fusion Steering employs
dynamic injection of prompt-specific activation deltas across all transformer
layers. These activation deltas are derived from reference completions that
combine the ground-truth answer with a model-generated explanation to
facilitate semantically enriched, example-specific steering. The injection
weights are optimized per prompt using Optuna, targeting a joint objective that
balances token overlap (factual alignment) and perplexity (fluency proxy).
Evaluation employs a composite score integrating token overlap and LLM-graded
quality, encompassing factual accuracy, coherence, and relevance. Empirical
results on 260 SimpleQA prompts (selected from 500 where the baseline failed)
showcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit
quantization, segmented steering achieves an accuracy of 25.4% (outputs scoring
$\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at
16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully
correct responses from 0.0% to 13.1%. These findings highlight the strengths of
segmented, dynamic intervention strategies and the promise of per-prompt,
full-network activation control. Fusion Steering is also amenable to sparse
representations, such as Neuronpedia or sparse crosscoders, suggesting a
promising direction for interpretable and scalable activation-level control in
LLMs.

</details>


### [90] [Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts](https://arxiv.org/abs/2505.22582)
*Xue Zhang,Yunlong Liang,Fandong Meng,Songming Zhang,Yufeng Chen,Jinan Xu,Jie Zhou*

Key words: 大规模语言模型、多语言扩展、混合专家、分层专家分配、灾难性遗忘

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为LayerMoE的算法，通过分层专家分配策略和分类器引导路由，显著减少了新语言扩展所需的专家数量，并有效缓解了对旧语言的遗忘问题。

Motivation: 现有方法在为大语言模型（LLM）扩展新语言时，采用混合专家（MoE）架构会导致参数量过大，且难以避免对旧语言性能的负面影响。

Method: 基于不同层对语言表征的相似性分析，提出分层专家分配算法（LayerMoE），并引入分类器引导旧语言的路由决策。

Result: 实验表明，该方法在单次扩展和持续扩展场景下，分别减少60%和33.3%的专家数量，同时性能优于基线。

Conclusion: LayerMoE通过动态调整专家分配和优化路由机制，实现了高效的多语言扩展与旧语言保护。

Abstract: Continually expanding new languages for existing large language models (LLMs)
is a promising yet challenging approach to building powerful multilingual LLMs.
The biggest challenge is to make the model continuously learn new languages
while preserving the proficient ability of old languages. To achieve this,
recent work utilizes the Mixture-of-Experts (MoE) architecture to expand new
languages by adding new experts and avoid catastrophic forgetting of old
languages by routing corresponding tokens to the original model backbone (old
experts). Although intuitive, this kind of method is parameter-costly when
expanding new languages and still inevitably impacts the performance of old
languages. To address these limitations, we analyze the language
characteristics of different layers in LLMs and propose a layer-wise expert
allocation algorithm (LayerMoE) to determine the appropriate number of new
experts for each layer. Specifically, we find different layers in LLMs exhibit
different representation similarities between languages and then utilize the
similarity as the indicator to allocate experts for each layer, i.e., the
higher similarity, the fewer experts. Additionally, to further mitigate the
forgetting of old languages, we add a classifier in front of the router network
on the layers with higher similarity to guide the routing of old language
tokens. Experimental results show that our method outperforms the previous
state-of-the-art baseline with 60% fewer experts in the single-expansion
setting and with 33.3% fewer experts in the lifelong-expansion setting,
demonstrating the effectiveness of our method.

</details>


### [91] [Precise In-Parameter Concept Erasure in Large Language Models](https://arxiv.org/abs/2505.22586)
*Yoav Gur-Arieh,Clara Suslik,Yihuai Hong,Fazl Barez,Mor Geva*

Key words: 大语言模型,知识擦除,参数编辑,PISCES,可解释性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: PISCES框架通过参数空间直接编辑概念编码方向，有效移除语言模型中的敏感或受版权保护知识，在效果和鲁棒性上优于现有方法。

Motivation: 大语言模型在预训练中获取的知识可能包含敏感或受版权内容，现有方法（如微调或局部编辑）效果有限。

Method: 使用解耦模型分解MLP向量为可解释特征，结合自动可解释性技术定位目标概念，并在参数空间中直接移除。

Result: 在Gemma 2和Llama 3.1上，PISCES将目标概念准确率降至7.7%，擦除特异性提升31%，鲁棒性提升38%。

Conclusion: 基于特征的参数内编辑提高了语言模型知识移除的精确性和可靠性。

Abstract: Large language models (LLMs) often acquire knowledge during pretraining that
is undesirable in downstream deployments, e.g., sensitive information or
copyrighted content. Existing approaches for removing such knowledge rely on
fine-tuning, training low-rank adapters or fact-level editing, but these are
either too coarse, too shallow, or ineffective. In this work, we propose PISCES
(Precise In-parameter Suppression for Concept EraSure), a novel framework for
precisely erasing entire concepts from model parameters by directly editing
directions that encode them in parameter space. PISCES uses a disentangler
model to decompose MLP vectors into interpretable features, identifies those
associated with a target concept using automated interpretability techniques,
and removes them from model parameters. Experiments on Gemma 2 and Llama 3.1
over various concepts show that PISCES achieves modest gains in efficacy over
leading erasure methods, reducing accuracy on the target concept to as low as
7.7%, while dramatically improving erasure specificity (by up to 31%) and
robustness (by up to 38%). Overall, these results demonstrate that
feature-based in-parameter editing enables a more precise and reliable approach
for removing conceptual knowledge in language models.

</details>


### [92] [Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning](https://arxiv.org/abs/2505.22591)
*Erxin Yu,Jing Li,Ming Liao,Qi Zhu,Boyang Xue,Minghui Xu,Baojun Wang,Lanqing Hong,Fei Mi,Lifeng Shang*

Key words: 大语言模型,数学推理,错误指导,数据合成,泛化能力

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SEI框架通过自我错误指导改进大语言模型的数学推理能力，通过聚类错误类型并生成针对性训练数据。

Motivation: 尽管大语言模型在多个领域表现优异，但在数学推理中仍存在许多错误案例，现有方法难以泛化这些错误的内在模式。

Method: 基于GPT-4o分析错误案例并生成错误关键词，聚类错误类型后，通过自指导方法合成针对性训练数据，并使用单样本学习筛选有效数据。

Result: 在GSM8K和MATH数据集上，SEI框架显著提升了模型的数学推理能力，并展示了跨数据集的泛化效果。

Conclusion: 自我错误指导通过错误泛化有效提升大语言模型的数学推理能力。

Abstract: Although large language models demonstrate strong performance across various
domains, they still struggle with numerous bad cases in mathematical reasoning.
Previous approaches to learning from errors synthesize training data by solely
extrapolating from isolated bad cases, thereby failing to generalize the
extensive patterns inherent within these cases. This paper presents
Self-Error-Instruct (SEI), a framework that addresses these model weaknesses
and synthesizes more generalized targeted training data. Specifically, we
explore a target model on two mathematical datasets, GSM8K and MATH, to
pinpoint bad cases. Then, we generate error keyphrases for these cases based on
the instructor model's (GPT-4o) analysis and identify error types by clustering
these keyphrases. Next, we sample a few bad cases during each generation for
each identified error type and input them into the instructor model, which
synthesizes additional training data using a self-instruct approach. This new
data is refined through a one-shot learning process to ensure that only the
most effective examples are kept. Finally, we use these curated data to
fine-tune the target model, iteratively repeating the process to enhance
performance. We apply our framework to various models and observe improvements
in their reasoning abilities across both in-domain and out-of-domain
mathematics datasets. These results demonstrate the effectiveness of self-error
instruction in improving LLMs' mathematical reasoning through error
generalization.

</details>


### [93] [Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding](https://arxiv.org/abs/2505.22618)
*Chengyue Wu,Hao Zhang,Shuchen Xue,Zhijian Liu,Shizhe Diao,Ligeng Zhu,Ping Luo,Song Han,Enze Xie*

Key words: Diffusion LLMs, KV Cache, 并行解码, 置信度感知, 推理加速

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种针对双向扩散模型的块状近似KV缓存机制和置信度感知并行解码策略，显著提升了Diffusion LLMs的推理速度并保持生成质量。

Motivation: 解决开源Diffusion LLMs因缺乏KV缓存和并行解码时质量下降导致的推理速度落后于自回归模型的问题。

Method: 引入块状近似KV缓存机制以减少性能损失，并提出置信度感知并行解码策略以避免令牌依赖破坏。

Result: 实验显示，在多个LLM基准测试中，吞吐量提升高达27.6倍，且精度损失极小。

Conclusion: 通过提出的方法，显著缩小了与自回归模型的性能差距，为Diffusion LLMs的实际部署铺平了道路。

Abstract: Diffusion-based large language models (Diffusion LLMs) have shown promise for
non-autoregressive text generation with parallel decoding capabilities.
However, the practical inference speed of open-sourced Diffusion LLMs often
lags behind autoregressive models due to the lack of Key-Value (KV) Cache and
quality degradation when decoding multiple tokens simultaneously. To bridge
this gap, we introduce a novel block-wise approximate KV Cache mechanism
tailored for bidirectional diffusion models, enabling cache reuse with
negligible performance drop. Additionally, we identify the root cause of
generation quality degradation in parallel decoding as the disruption of token
dependencies under the conditional independence assumption. To address this, we
propose a confidence-aware parallel decoding strategy that selectively decodes
tokens exceeding a confidence threshold, mitigating dependency violations and
maintaining generation quality. Experimental results on LLaDA and Dream models
across multiple LLM benchmarks demonstrate up to \textbf{27.6$\times$
throughput} improvement with minimal accuracy loss, closing the performance gap
with autoregressive models and paving the way for practical deployment of
Diffusion LLMs.

</details>


### [94] [Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions](https://arxiv.org/abs/2505.22627)
*Yijun Shen,Delong Chen,Fan Liu,Xingyu Wang,Chuanyi Zhang,Liang Yao,Yuhui Zheng*

Key words: 标注效率, 视觉-语言对齐, 多模态交互, 序列化标注, AI辅助

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了Chain-of-Talkers (CoTalk)，一种在固定预算下最大化标注样本数量和提升标注全面性的AI辅助方法，通过序列化标注和多模态交互优化效率。

Motivation: 现有密集标注方法在优化人力标注效率方面研究不足，需一种系统化方法来提升标注速度与全面性。

Method: 采用序列化标注减少冗余工作量，结合多模态交互（阅读与语音输出）提升效率，并解析标注内容的语义结构。

Result: 实验表明，CoTalk在标注速度（0.42 vs. 0.30单位/秒）和检索性能（41.13% vs. 40.52%）上优于并行方法。

Conclusion: CoTalk通过序列化标注和多模态交互显著提升了标注效率与效果，为视觉-语言对齐任务提供了更优的数据支持。

Abstract: While densely annotated image captions significantly facilitate the learning
of robust vision-language alignment, methodologies for systematically
optimizing human annotation efforts remain underexplored. We introduce
Chain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize
the number of annotated samples and improve their comprehensiveness under fixed
budget constraints (e.g., total human annotation time). The framework is built
upon two key insights. First, sequential annotation reduces redundant workload
compared to conventional parallel annotation, as subsequent annotators only
need to annotate the ``residual'' -- the missing visual information that
previous annotations have not covered. Second, humans process textual input
faster by reading while outputting annotations with much higher throughput via
talking; thus a multimodal interface enables optimized efficiency. We evaluate
our framework from two aspects: intrinsic evaluations that assess the
comprehensiveness of semantic units, obtained by parsing detailed captions into
object-attribute trees and analyzing their effective connections; extrinsic
evaluation measures the practical usage of the annotated captions in
facilitating vision-language alignment. Experiments with eight participants
show our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30
units/sec) and retrieval performance (41.13\% vs. 40.52\%) over the parallel
method.

</details>


### [95] [Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs](https://arxiv.org/abs/2505.22630)
*Ziling Cheng,Meng Cao,Marc-Antoine Rondeau,Jackie Chi Kit Cheung*

Key words: 大语言模型、无关背景幻觉、类别泛化、机制可解释性、随机变色龙

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文探讨了大语言模型（LLMs）在推理时的错误模式，特别是无关背景幻觉问题，揭示了其内部存在抽象类别（错误）泛化机制，以及两种竞争电路影响最终输出的现象。

Motivation: 研究旨在揭示LLMs错误背后的规律性，挑战了它们仅是随机复读机的观点，探索其泛化能力的局限性。

Method: 通过行为分析和机制可解释性实验（Llama-3、Mistral、Pythia等模型），研究39种事实回忆关系中的上下文幻觉现象。

Result: 模型错误源于结构化的类别（错误）泛化机制，其内部计算体现为低层构建抽象类别表征，高层细化答案，并受两种竞争电路调控。

Conclusion: LLMs通过形式化训练展现出基于抽象的泛化能力，但依赖上下文线索的方式不可靠，可称为“随机变色龙”。

Abstract: The widespread success of large language models (LLMs) on NLP benchmarks has
been accompanied by concerns that LLMs function primarily as stochastic parrots
that reproduce texts similar to what they saw during pre-training, often
erroneously. But what is the nature of their errors, and do these errors
exhibit any regularities? In this work, we examine irrelevant context
hallucinations, in which models integrate misleading contextual cues into their
predictions. Through behavioral analysis, we show that these errors result from
a structured yet flawed mechanism that we term class-based (mis)generalization,
in which models combine abstract class cues with features extracted from the
query or context to derive answers. Furthermore, mechanistic interpretability
experiments on Llama-3, Mistral, and Pythia across 39 factual recall relation
types reveal that this behavior is reflected in the model's internal
computations: (i) abstract class representations are constructed in lower
layers before being refined into specific answers in higher layers, (ii)
feature selection is governed by two competing circuits -- one prioritizing
direct query-based reasoning, the other incorporating contextual cues -- whose
relative influences determine the final output. Our findings provide a more
nuanced perspective on the stochastic parrot argument: through form-based
training, LLMs can exhibit generalization leveraging abstractions, albeit in
unreliable ways based on contextual cues -- what we term stochastic chameleons.

</details>


### [96] [Spatial Knowledge Graph-Guided Multimodal Synthesis](https://arxiv.org/abs/2505.22633)
*Yida Xue,Zhen Bi,Jinnan Yang,Jungang Lou,Huajun Chen,Ningyu Zhang*

Key words: 多模态大语言模型, 空间感知, 知识图谱, 数据合成, 空间常识

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SKG2Data是一种基于空间知识图谱的多模态数据合成方法，旨在提升多模态大语言模型的空间感知与推理能力。

Motivation: 当前多模态大语言模型的空间感知能力有限，而多模态数据合成虽能解决这一问题，但确保合成数据符合空间常识仍具挑战性。

Method: 提出SKG2Data方法，通过构建空间知识图谱（SKG）模拟人类对空间方向与距离的感知，并以此指导多模态数据合成。

Result: 实验表明，基于方向、距离等空间知识合成的数据不仅显著提升了模型的空间能力，还展现出强泛化性。

Conclusion: 基于知识的数据合成有望推动空间智能的发展。

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced their capabilities; however, their spatial perception
abilities remain a notable limitation. To address this challenge, multimodal
data synthesis offers a promising solution. Yet, ensuring that synthesized data
adhere to spatial common sense is a non-trivial task. In this work, we
introduce SKG2Data, a novel multimodal synthesis approach guided by spatial
knowledge graphs, grounded in the concept of knowledge-to-data generation.
SKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate
human-like perception of spatial directions and distances, which is
subsequently utilized to guide multimodal data synthesis. Extensive experiments
demonstrate that data synthesized from diverse types of spatial knowledge,
including direction and distance, not only enhance the spatial perception and
reasoning abilities of MLLMs but also exhibit strong generalization
capabilities. We hope that the idea of knowledge-based data synthesis can
advance the development of spatial intelligence.

</details>


### [97] [Learning Composable Chains-of-Thought](https://arxiv.org/abs/2505.22635)
*Fangcong Yin,Zeyu Leo Liu,Liu Leqi,Xi Ye,Greg Durrett*

Key words: 链式思维、组合泛化、大语言模型、多任务学习、拒绝采样微调

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了如何通过修改原子任务的链式思维（CoT）格式，使其可组合，以提升大语言模型在未见组合任务上的零样本性能。

Motivation: 传统方法需要为每个推理任务标注CoT数据，成本高昂。研究旨在通过组合原子推理技能，实现模型在无标注CoT数据的组合任务上的泛化。

Method: 提出‘可组合CoT’方法，修改原子任务的CoT格式以增强组合性，结合多任务学习或模型融合，并通过拒绝采样微调（RFT）利用少量组合数据进一步优化。

Result: 在字符串操作和自然语言技能组合任务上，可组合CoT方法在多任务学习和持续微调基线中表现更优。

Conclusion: 可组合CoT能有效提升模型在组合任务上的零样本性能，为推理泛化提供新思路。

Abstract: A common approach for teaching large language models (LLMs) to reason is to
train on chain-of-thought (CoT) traces of in-distribution reasoning problems,
but such annotated data is costly to obtain for every problem of interest. We
want reasoning models to generalize beyond their training distribution, and
ideally to generalize compositionally: combine atomic reasoning skills to solve
harder, unseen reasoning tasks. We take a step towards compositional
generalization of reasoning skills when addressing a target compositional task
that has no labeled CoT data. We find that simply training models on CoT data
of atomic tasks leads to limited generalization, but minimally modifying CoT
formats of constituent atomic tasks to be composable can lead to improvements.
We can train "atomic CoT" models on the atomic tasks with Composable CoT data
and combine them with multitask learning or model merging for better zero-shot
performance on the target compositional task. Such a combined model can be
further bootstrapped on a small amount of compositional data using rejection
sampling fine-tuning (RFT). Results on string operations and natural language
skill compositions show that training LLMs on Composable CoT outperforms
multitask learning and continued fine-tuning baselines within a given training
data budget.

</details>


### [98] [Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese](https://arxiv.org/abs/2505.22645)
*Hanjia Lyu,Jiebo Luo,Jian Kang,Allison Koenecke*

Key words: 大型语言模型, 简体中文, 繁体中文, 性能差异, 偏见分析

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLMs）在简体与繁体中文提示下的性能差异，发现模型在不同任务中表现出对简体或繁体中文的偏好，并指出这些偏见可能源于训练数据、字符偏好和分词方式。

Motivation: 了解LLMs在简体与繁体中文中的表现差异对于避免文化背景忽视和减少决策中的潜在危害至关重要。

Method: 设计了两种基准任务（地区术语选择和地区姓名选择），并对11种主流商业和开源LLMs进行了性能审计。

Result: 大多数LLMs在术语选择任务中偏向简体中文，而在姓名选择任务中却偏向繁体中文，这种差异可能与训练数据表示、字符偏好和分词方式有关。

Conclusion: 研究强调了进一步分析LLM偏见的必要性，并提供了一个开源的基准数据集以促进未来研究的可重复性评估。

Abstract: While the capabilities of Large Language Models (LLMs) have been studied in
both Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit
differential performance when prompted in these two variants of written
Chinese. This understanding is critical, as disparities in the quality of LLM
responses can perpetuate representational harms by ignoring the different
cultural contexts underlying Simplified versus Traditional Chinese, and can
exacerbate downstream harms in LLM-facilitated decision-making in domains such
as education or hiring. To investigate potential LLM performance disparities,
we design two benchmark tasks that reflect real-world scenarios: regional term
choice (prompting the LLM to name a described item which is referred to
differently in Mainland China and Taiwan), and regional name choice (prompting
the LLM to choose who to hire from a list of names in both Simplified and
Traditional Chinese). For both tasks, we audit the performance of 11 leading
commercial LLM services and open-sourced models -- spanning those primarily
trained on English, Simplified Chinese, or Traditional Chinese. Our analyses
indicate that biases in LLM responses are dependent on both the task and
prompting language: while most LLMs disproportionately favored Simplified
Chinese responses in the regional term choice task, they surprisingly favored
Traditional Chinese names in the regional name choice task. We find that these
disparities may arise from differences in training data representation, written
character preferences, and tokenization of Simplified and Traditional Chinese.
These findings highlight the need for further analysis of LLM biases; as such,
we provide an open-sourced benchmark dataset to foster reproducible evaluations
of future LLM behavior across Chinese language variants
(https://github.com/brucelyu17/SC-TC-Bench).

</details>


### [99] [WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/abs/2505.22648)
*Jialong Wu,Baixuan Li,Runnan Fang,Wenbiao Yin,Liwen Zhang,Zhengwei Tao,Dingchu Zhang,Zekun Xi,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Key words: 自主代理, 信息搜索, 多步推理, 强化学习, WebDancer

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种端到端的自主信息搜索代理框架WebDancer，包含四个关键阶段：浏览数据构建、轨迹采样、监督微调以优化冷启动、强化学习以提升泛化能力。在GAIA和WebWalkerQA基准测试中表现出色。

Motivation: 解决复杂现实问题需要多步推理和深度信息搜索，现有自主研究系统（如Deep Research）的进展显示了多步自主研究的潜力。

Method: 框架包含四个阶段：(1) 浏览数据构建，(2) 轨迹采样，(3) 监督微调优化冷启动，(4) 强化学习提升泛化能力。基于ReAct的WebDancer代理实现。

Result: 在GAIA和WebWalkerQA基准测试中表现优异，验证了训练范式的有效性。

Conclusion: 该框架为开发更强大的自主代理模型提供了系统化路径和实践洞察。

Abstract: Addressing intricate real-world problems necessitates in-depth information
seeking and multi-step reasoning. Recent progress in agentic systems,
exemplified by Deep Research, underscores the potential for autonomous
multi-step research. In this work, we present a cohesive paradigm for building
end-to-end agentic information seeking agents from a data-centric and
training-stage perspective. Our approach consists of four key stages: (1)
browsing data construction, (2) trajectories sampling, (3) supervised
fine-tuning for effective cold start, and (4) reinforcement learning for
enhanced generalisation. We instantiate this framework in a web agent based on
the ReAct, WebDancer. Empirical evaluations on the challenging information
seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of
WebDancer, achieving considerable results and highlighting the efficacy of our
training paradigm. Further analysis of agent training provides valuable
insights and actionable, systematic pathways for developing more capable
agentic models. The codes and demo will be released in
https://github.com/Alibaba-NLP/WebAgent.

</details>


### [100] [The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason](https://arxiv.org/abs/2505.22653)
*Ang Lv,Ruobing Xie,Xingwu Sun,Zhanhui Kang,Rui Yan*

Key words: LLMs, 奖励噪声, 推理训练, RPR, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究了奖励噪声对大规模语言模型（LLMs）在推理任务后训练中的影响，发现LLMs对高噪声具有强鲁棒性。引入推理模式奖励（RPR）可提升模型性能。

Motivation: 探索实际场景中奖励噪声对LLM后训练的影响，旨在提升模型在噪声环境下的推理能力。

Method: 通过人为翻转奖励函数输出引入噪声，结合推理模式奖励（RPR），测试Qwen-2.5-7B模型在数学任务中的表现。

Result: 即使奖励噪声达40%，模型仍快速收敛，性能从5%提升至72%；RPR单独使用也能达到70%以上的准确率。

Conclusion: 推理过程比结果更重要，RPR可校准噪声奖励模型，提升开放任务表现。建议加强预训练基础能力并优化后训练技术。

Abstract: Recent studies on post-training large language models (LLMs) for reasoning
through reinforcement learning (RL) typically focus on tasks that can be
accurately verified and rewarded, such as solving math problems. In contrast,
our research investigates the impact of reward noise, a more practical
consideration for real-world scenarios involving the post-training of LLMs
using reward models. We found that LLMs demonstrate strong robustness to
substantial reward noise. For example, manually flipping 40% of the reward
function's outputs in math tasks still allows a Qwen-2.5-7B model to achieve
rapid convergence, improving its performance on math tasks from 5% to 72%,
compared to the 75% accuracy achieved by a model trained with noiseless
rewards. Surprisingly, by only rewarding the appearance of key reasoning
phrases (namely reasoning pattern reward, RPR), such as ``first, I need
to''-without verifying the correctness of answers, the model achieved peak
downstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models
trained with strict correctness verification and accurate rewards. Recognizing
the importance of the reasoning process over the final results, we combined RPR
with noisy reward models. RPR helped calibrate the noisy reward models,
mitigating potential false negatives and enhancing the LLM's performance on
open-ended tasks. These findings suggest the importance of improving models'
foundational abilities during the pre-training phase while providing insights
for advancing post-training techniques. Our code and scripts are available at
https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.

</details>


### [101] [GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning](https://arxiv.org/abs/2505.22661)
*Qingchen Yu,Zifan Zheng,Ding Chen,Simin Niu,Bo Tang,Feiyu Xiong,Zhiyu Li*

Key words: 大语言模型,评估框架,对抗性游戏,动态建模,推理评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了GuessArena框架，通过对抗性游戏交互解决传统静态评估方法的局限性，增强评估的适应性和细粒度性。

Motivation: 传统大语言模型评估依赖静态基准，缺乏领域适应性和细粒度评估能力。

Method: 基于'Guess Who I Am?'游戏的交互结构，结合动态领域知识建模和渐进推理评估。

Result: 在金融、医疗等五个领域验证了GuessArena在区分领域知识覆盖和推理链完整性上的有效性。

Conclusion: GuessArena提供比传统基准更高的可解释性、可扩展性和场景适应性。

Abstract: The evaluation of large language models (LLMs) has traditionally relied on
static benchmarks, a paradigm that poses two major limitations: (1) predefined
test sets lack adaptability to diverse application domains, and (2)
standardized evaluation protocols often fail to capture fine-grained
assessments of domain-specific knowledge and contextual reasoning abilities. To
overcome these challenges, we propose GuessArena, an adaptive evaluation
framework grounded in adversarial game-based interactions. Inspired by the
interactive structure of the Guess Who I Am? game, our framework seamlessly
integrates dynamic domain knowledge modeling with progressive reasoning
assessment to improve evaluation fidelity. Empirical studies across five
vertical domains-finance, healthcare, manufacturing, information technology,
and education-demonstrate that GuessArena effectively distinguishes LLMs in
terms of domain knowledge coverage and reasoning chain completeness. Compared
to conventional benchmarks, our method provides substantial advantages in
interpretability, scalability, and scenario adaptability.

</details>


### [102] [AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models](https://arxiv.org/abs/2505.22662)
*Feng Luo,Yu-Neng Chuang,Guanchu Wang,Hoang Anh Duy Le,Shaochen Zhong,Hongyi Liu,Jiayi Yuan,Yang Sui,Vladimir Braverman,Vipin Chaudhary,Xia Hu*

Key words: Large Language Models, Chain-of-Thought Reasoning, Dynamic Reasoning, AutoL2S, Efficiency

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: AutoL2S is a framework that enables LLMs to dynamically adjust the length of chain-of-thought reasoning based on question complexity, reducing reasoning length by up to 57% without performance loss.

Motivation: LLMs often generate unnecessarily long reasoning paths for simple questions, increasing inference cost and latency. Existing methods lack flexibility in dynamically adapting reasoning length.

Method: AutoL2S trains LLMs on annotated data with both long and short CoT paths, including an <EASY> token to signal when shorter reasoning suffices, enabling dynamic CoT compression.

Result: AutoL2S reduces reasoning path length by up to 57% without compromising performance, demonstrating efficient and scalable LLM reasoning.

Conclusion: AutoL2S effectively balances reasoning length and performance, offering a flexible and model-agnostic solution for efficient LLM reasoning.

Abstract: The reasoning-capable large language models (LLMs) demonstrate strong
performance on complex reasoning tasks but often suffer from overthinking,
generating unnecessarily long chain-of-thought (CoT) reasoning paths for easy
reasoning questions, thereby increasing inference cost and latency. Recent
approaches attempt to address this challenge by manually deciding when to apply
long or short reasoning. However, they lack the flexibility to adapt CoT length
dynamically based on question complexity. In this paper, we propose Auto
Long-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that
enables LLMs to dynamically compress their generated reasoning path based on
the complexity of the reasoning question. AutoL2S enables a learned paradigm,
in which LLMs themselves can decide when longer reasoning is necessary and when
shorter reasoning suffices, by training on data annotated with our proposed
method, which includes both long and short CoT paths and a special <EASY>
token. We then use <EASY> token to indicate when the model can skip generating
lengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'
ability to generate shorter CoT reasoning paths with improved quality after
training. Extensive evaluation results show that AutoL2S reduces the length of
reasoning generation by up to 57% without compromising performance,
demonstrating the effectiveness of AutoL2S for scalable and efficient LLM
reasoning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [103] [The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects on User Trust, Exploration, and Workflows](https://arxiv.org/abs/2505.21512)
*Harry Li,Gabriel Appleby,Kenneth Alperin,Steven R Gomez,Ashley Suh*

Key words: knowledge graphs, large language models, visualization, trust, decision-making

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了大型语言模型（LLM）与知识图谱（KG）结合使用时对用户信任和决策的影响，介绍了LinkQ系统及五种视觉机制，通过定性评估发现用户倾向于过度信任LLM输出，提出了进一步研究可视化作为缓解技术的必要性。

Motivation: 研究LLM与KG结合使用时如何影响用户信任、探索策略和决策，填补现有研究的空白，并解决LLM辅助KG视觉分析系统的设计挑战。

Method: 开发了LinkQ系统，将自然语言问题转换为结构化查询，并通过五种视觉机制帮助用户评估查询和LLM响应的准确性。与14位专家进行定性评估。

Result: 用户倾向于过度信任LinkQ的输出，即使LLM回答错误；用户工作流因KG和LLM的熟悉度而异，表明系统并非通用。

Conclusion: LLM辅助数据分析工具存在误信风险，需进一步研究可视化技术作为缓解手段。

Abstract: Knowledge graphs (KGs) are powerful data structures, but exploring them
effectively remains difficult for even expert users. Large language models
(LLMs) are increasingly used to address this gap, yet little is known
empirically about how their usage with KGs shapes user trust, exploration
strategies, or downstream decision-making - raising key design challenges for
LLM-based KG visual analysis systems. To study these effects, we developed
LinkQ, a KG exploration system that converts natural language questions into
structured queries with an LLM. We collaborated with KG experts to design five
visual mechanisms that help users assess the accuracy of both KG queries and
LLM responses: an LLM-KG state diagram that illustrates which stage of the
exploration pipeline LinkQ is in, a query editor displaying the generated query
paired with an LLM explanation, an entity-relation ID table showing extracted
KG entities and relations with semantic descriptions, a query structure graph
that depicts the path traversed in the KG, and an interactive graph
visualization of query results. From a qualitative evaluation with 14
practitioners, we found that users - even KG experts - tended to overtrust
LinkQ's outputs due to its "helpful" visualizations, even when the LLM was
incorrect. Users exhibited distinct workflows depending on their prior
familiarity with KGs and LLMs, challenging the assumption that these systems
are one-size-fits-all - despite often being designed as if they are. Our
findings highlight the risks of false trust in LLM-assisted data analysis tools
and the need for further investigation into the role of visualization as a
mitigation technique.

</details>


### [104] [SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code Generation](https://arxiv.org/abs/2505.21514)
*Mingchao Jiang,Abhinav Jain,Sophia Zorek,Chris Jermaine*

Key words: LLM, 代码补全, 代码填充, Java, Python, 基准测试

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: SIMCOPILOT是一个评估大语言模型在代码补全和填充任务中表现的基准，专注于Java和Python，提供细粒度分析。

Motivation: 评估LLM在实际编码场景中的实用性，并关注现有基准忽略的细节，如任务特性、上下文理解和变量作用域敏感性。

Method: 建立Java和Python子基准(SIMCOPILOTJ/P)，覆盖不同规模和复杂度的代码库，进行多领域评估。

Result: 揭示了LLM在保持逻辑一致性和复杂依赖结构中的挑战，同时展示了其代码生成潜力。

Conclusion: LLM正从语法生成器向可靠开发伙伴过渡，但仍需改进逻辑一致性等关键问题。

Abstract: We introduce SIMCOPILOT, a benchmark that simulates the role of large
language models (LLMs) as interactive, "copilot"-style coding assistants.
Targeting both completion (finishing incomplete methods or code blocks) and
infill tasks (filling missing segments within existing code), SIMCOPILOT
provides a comprehensive framework for evaluating LLM coding capabilities. The
benchmark comprises dedicated sub-benchmarks for Java (SIMCOPILOTJ) and Python
(SIMCOPILOTP), covering diverse codebases varying in size and complexity. Our
key contributions include: (a) establishing a realistic, detailed evaluation
environment to assess LLM utility in practical coding scenarios, and (b)
providing fine-grained analyses that address critical factors frequently
overlooked by existing benchmarks, such as task-specific performance nuances,
contextual understanding across code segments, and sensitivity to variable
scope. Evaluations conducted across domains-including algorithms, databases,
computer vision, and neural networks-offer insights into model strengths and
highlight persistent challenges in maintaining logical consistency within
complex dependency structures. Beyond benchmarking, our study sheds light on
the current limitations of LLM-driven code generation and underscores the
ongoing transition of LLMs from merely syntax-aware generators toward reliable,
intelligent software development partners.

</details>


### [105] [Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation](https://arxiv.org/abs/2505.21525)
*Peiliang Gong,Yucheng Wang,Min Wu,Zhenghua Chen,Xiaoli Li,Daoqiang Zhang*

Key words: SFDA, MTS, 时空特征, 跨域适应, 隐私保护

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: TERSE提出了一种新型SFDA方法，针对MTS数据优化，通过时间和空间特征恢复实现跨域特征对齐。

Motivation: 现有SFDA方法未充分考虑MTS数据的空间相关性，导致跨域适应效果不佳，需解决这一问题以提升隐私保护下的模型适应性。

Method: TERSE结合时空特征编码器，通过时间恢复和空间重连任务重建掩码的时序与空间结构，利用预训练网络引导目标域特征一致性。

Result: 在三个真实时序数据集上验证，TERSE有效建模时空依赖关系，提升跨域性能，并可作为模块嵌入现有SFDA方法。

Conclusion: TERSE首次在MTS-SFDA中同步考虑时空一致性，为跨域时序分析提供了高效解决方案。

Abstract: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from
an annotated source domain to an unlabelled target domain without accessing the
source data, thereby preserving data privacy. While existing SFDA methods have
proven effective in reducing reliance on source data, they struggle to perform
well on multivariate time series (MTS) due to their failure to consider the
intrinsic spatial correlations inherent in MTS data. These spatial correlations
are crucial for accurately representing MTS data and preserving invariant
information across domains. To address this challenge, we propose Temporal
Restoration and Spatial Rewiring (TERSE), a novel and concise SFDA method
tailored for MTS data. Specifically, TERSE comprises a customized
spatial-temporal feature encoder designed to capture the underlying
spatial-temporal characteristics, coupled with both temporal restoration and
spatial rewiring tasks to reinstate latent representations of the temporally
masked time series and the spatially masked correlated structures. During the
target adaptation phase, the target encoder is guided to produce spatially and
temporally consistent features with the source domain by leveraging the source
pre-trained temporal restoration and spatial rewiring networks. Therefore,
TERSE can effectively model and transfer spatial-temporal dependencies across
domains, facilitating implicit feature alignment. In addition, as the first
approach to simultaneously consider spatial-temporal consistency in MTS-SFDA,
TERSE can also be integrated as a versatile plug-and-play module into
established SFDA methods. Extensive experiments on three real-world time series
datasets demonstrate the effectiveness and versatility of our approach.

</details>


### [106] [ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools](https://arxiv.org/abs/2505.21569)
*Zhucong Li,Bowei Zhang,Jin Xiao,Zhijian Zhou,Fenglei Cao,Jiaqing Liang,Yuan Qi*

Key words: LLM代理、化学工具、预测误差、代理堆叠、ChemHAS

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: ChemHAS方法通过优化代理堆叠结构来减少化学工具的预测误差，并在四个基础化学任务中取得最佳性能。

Motivation: 现有的LLM代理在化学任务中表现受限，主要因为化学工具的预测误差。

Method: 提出ChemHAS方法，通过优化代理堆叠结构来增强化学工具的性能。

Result: ChemHAS在四个基础化学任务中表现最优，并揭示了四种代理堆叠行为。

Conclusion: ChemHAS能有效补偿工具预测误差，并为科学研究的AI代理应用提供新可能。

Abstract: Large Language Model (LLM)-based agents have demonstrated the ability to
improve performance in chemistry-related tasks by selecting appropriate tools.
However, their effectiveness remains limited by the inherent prediction errors
of chemistry tools. In this paper, we take a step further by exploring how
LLMbased agents can, in turn, be leveraged to reduce prediction errors of the
tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),
a simple yet effective method that enhances chemistry tools through optimizing
agent-stacking structures from limited data. ChemHAS achieves state-of-the-art
performance across four fundamental chemistry tasks, demonstrating that our
method can effectively compensate for prediction errors of the tools.
Furthermore, we identify and characterize four distinct agent-stacking
behaviors, potentially improving interpretability and revealing new
possibilities for AI agent applications in scientific research. Our code and
dataset are publicly available at https:
//anonymous.4open.science/r/ChemHAS-01E4/README.md.

</details>


### [107] [FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation Recognition](https://arxiv.org/abs/2505.21571)
*Yao Lu,Tengfei Ma,Zeyu Wang,Zhuangzhi Chen,Dongwei Xu,Yun Lin,Qi Xuan,Guan Gui*

Key words: AMR, 模型剪枝, 深度学习, 高效推理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为FCOS的新型两阶段剪枝框架，结合通道级剪枝和层坍塌诊断，实现了极高压缩率和高效推理，同时保持接近原始模型的性能。

Motivation: 传统手动调制识别方法难以满足现代场景的实时性和特征提取需求，而深度学习AMR方法虽准确但计算量大，难以在资源受限设备上部署。需要一种能平衡压缩率、加速和精度的方法。

Method: FCOS框架分为两阶段：通道级剪枝（通过层次聚类和参数融合）和层坍塌诊断（使用线性探测识别层坍塌并移除）。

Result: 在多个AMR基准测试中，FCOS表现优于现有方法，FLOPs和参数分别减少95.51%和95.31%，精度仅下降0.46%。

Conclusion: FCOS是一种高效的剪枝框架，适用于资源受限设备，且性能接近原始模型。

Abstract: With the rapid development of wireless communications and the growing
complexity of digital modulation schemes, traditional manual modulation
recognition methods struggle to extract reliable signal features and meet
real-time requirements in modern scenarios. Recently, deep learning based
Automatic Modulation Recognition (AMR) approaches have greatly improved
classification accuracy. However, their large model sizes and high
computational demands hinder deployment on resource-constrained devices. Model
pruning provides a general approach to reduce model complexity, but existing
weight, channel, and layer pruning techniques each present a trade-off between
compression rate, hardware acceleration, and accuracy preservation. To this
end, in this paper, we introduce FCOS, a novel Fine-to-COarse two-Stage pruning
framework that combines channel-level pruning with layer-level collapse
diagnosis to achieve extreme compression, high performance and efficient
inference. In the first stage of FCOS, hierarchical clustering and parameter
fusion are applied to channel weights to achieve channel-level pruning. Then a
Layer Collapse Diagnosis (LaCD) module uses linear probing to identify layer
collapse and removes the collapsed layers due to high channel compression
ratio. Experiments on multiple AMR benchmarks demonstrate that FCOS outperforms
existing channel and layer pruning methods. Specifically, FCOS achieves 95.51%
FLOPs reduction and 95.31% parameter reduction while still maintaining
performance close to the original ResNet56, with only a 0.46% drop in accuracy
on Sig2019-12. Code is available at https://github.com/yaolu-zjut/FCOS.

</details>


### [108] [Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes](https://arxiv.org/abs/2505.21573)
*Han Wan,Rui Zhang,Hao Sun*

Key words: 偏微分方程,神经算子,频谱域学习,数据稀缺,算子蒸馏

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为SINO的频谱启发神经算子框架，能够在仅需少量轨迹数据的情况下，无需已知PDE项即可学习偏微分方程算子，实现了优异的性能和泛化能力。

Motivation: 传统数值求解器需要精确离散化和已知PDE项，数据驱动的神经求解器需要大量数据，而现有的物理感知方法依赖已知PDE项或局部数值方案。SINO旨在通过频谱域学习和频域-向量模块缓解这些限制。

Method: SINO在频域中操作，引入Frequency-to-Vector模块学习频谱表示，设计非线性算子块（含低通滤波防止混叠），并通过算子蒸馏技术提升推理效率。

Result: SINO在多个PDE基准测试中达到最先进水平，展示了较强的离散不变性和对分布外初始条件的鲁棒性。

Conclusion: SINO是首个无需显式PDE项即可从有限数据中准确模拟全局耦合系统的物理感知方法。

Abstract: Partial differential equations (PDEs) govern the spatiotemporal evolution of
various physical systems. Classical numerical solvers, while accurate, require
fine discretization and full knowledge of the governing PDEs, limiting their
applicability when the physics is unknown or fast inference is required.
Data-driven neural PDE solvers alleviate these constraints by learning from
data but demand large training datasets and perform poorly in data-scarce
regimes. Physics-aware methods mitigate data requirements by incorporating
physical knowledge yet rely on known PDE terms or local numerical schemes,
restricting their ability to handle unknown or globally coupled systems. In
this work, we propose the Spectral-inspired Neural Operator (SINO), a novel
framework that learns PDE operators from limited trajectories (as few as 2-5),
without any known PDE terms. SINO operates in the frequency domain and
introduces a Frequency-to-Vector module to learn spectral representations
analogous to derivative multipliers. To model nonlinear physical interactions,
we design a nonlinear operator block that includes a $\Pi$-Block with low-pass
filtering to prevent aliasing. Finally, we introduce an operator distillation
technique to distill the trained model for efficient inference. SINO achieves
state-of-the-art results across multiple PDE benchmarks, demonstrating strong
discretization invariance and robust generalization to out-of-distribution
initial conditions. To our knowledge, SINO is the first physics-aware method
capable of accurately simulating globally coupled systems (e.g., the
Navier-Stokes equations) from limited data without any explicit PDE terms.

</details>


### [109] [Concentration Distribution Learning from Label Distributions](https://arxiv.org/abs/2505.21576)
*Jiawei Tang,Yuheng Jia*

Key words: 标签分布学习, 背景浓度, 浓度分布学习, 概率方法, 神经网络

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了背景浓度的新概念，用于改进标签分布学习（LDL），弥补传统LDL忽略标签绝对强度的缺陷，并通过概率方法和神经网络学习标签分布与背景浓度，实验证明其优于现有方法。

Motivation: 传统标签分布学习（LDL）仅关注标签的相对描述程度，忽略了隐藏标签的绝对强度和总描述度，导致信息丢失和实例混淆。为此，论文提出引入背景浓度作为绝对描述项，完善标签分布表示。

Method: 提出背景浓度概念并将其整合到LDL中，形成浓度分布学习（CDL）范式；设计基于概率方法和神经网络的模型，从现有LDL数据集中同时学习标签分布和背景浓度。

Result: 实验表明，该方法能有效从标签分布中提取背景浓度，且预测精度优于当前最先进的LDL方法。

Conclusion: 背景浓度的引入显著提升了标签分布学习的完整性和准确性，为LDL领域提供了新的改进方向。

Abstract: Label distribution learning (LDL) is an effective method to predict the
relative label description degree (a.k.a. label distribution) of a sample.
However, the label distribution is not a complete representation of an instance
because it overlooks the absolute intensity of each label. Specifically, it's
impossible to obtain the total description degree of hidden labels that not in
the label space, which leads to the loss of information and confusion in
instances. To solve the above problem, we come up with a new concept named
background concentration to serve as the absolute description degree term of
the label distribution and introduce it into the LDL process, forming the
improved paradigm of concentration distribution learning. Moreover, we propose
a novel model by probabilistic methods and neural networks to learn label
distributions and background concentrations from existing LDL datasets.
Extensive experiments prove that the proposed approach is able to extract
background concentrations from label distributions while producing more
accurate prediction results than the state-of-the-art LDL methods. The code is
available in https://github.com/seutjw/CDL-LD.

</details>


### [110] [Fairness in Federated Learning: Fairness for Whom?](https://arxiv.org/abs/2505.21584)
*Afaf Taik,Khaoula Chehbouni,Golnoosh Farnadi*

Key words: 联邦学习, 公平性, 社会技术, 危害中心框架, 多利益相关者

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文批判现有联邦学习中的公平性研究过于技术化，忽视了社会技术上下文，提出了一个以危害为中心的框架，并建议更全面的研究方向。

Motivation: 现有联邦学习公平性研究过于关注系统级指标，忽视了社会技术上下文中的实际危害和多利益相关者的需求。

Method: 通过对文献的系统标注分析，识别了五个常见问题，并提出了一个危害中心框架。

Result: 揭示了现有研究的五个常见缺陷，并提出了一个更全面的公平性定义和评估方法。

Conclusion: 建议未来的研究应更具整体性、上下文意识和对多利益相关者的责任。

Abstract: Fairness in federated learning has emerged as a rapidly growing area of
research, with numerous works proposing formal definitions and algorithmic
interventions. Yet, despite this technical progress, fairness in FL is often
defined and evaluated in ways that abstract away from the sociotechnical
contexts in which these systems are deployed. In this paper, we argue that
existing approaches tend to optimize narrow system level metrics, such as
performance parity or contribution-based rewards, while overlooking how harms
arise throughout the FL lifecycle and how they impact diverse stakeholders. We
support this claim through a critical analysis of the literature, based on a
systematic annotation of papers for their fairness definitions, design
decisions, evaluation practices, and motivating use cases. Our analysis reveals
five recurring pitfalls: 1) fairness framed solely through the lens of server
client architecture, 2) a mismatch between simulations and motivating use-cases
and contexts, 3) definitions that conflate protecting the system with
protecting its users, 4) interventions that target isolated stages of the
lifecycle while neglecting upstream and downstream effects, 5) and a lack of
multi-stakeholder alignment where multiple fairness definitions can be relevant
at once. Building on these insights, we propose a harm centered framework that
links fairness definitions to concrete risks and stakeholder vulnerabilities.
We conclude with recommendations for more holistic, context-aware, and
accountable fairness research in FL.

</details>


### [111] [CellCLAT: Preserving Topology and Trimming Redundancy in Self-Supervised Cellular Contrastive Learning](https://arxiv.org/abs/2505.21587)
*Bin Qin,Qirui Ji,Jiangmeng Li,Yupeng Wang,Xuesong Wu,Jianwen Cao,Fanjiang Xu*

Key words: 自监督学习, 拓扑深度学习, 细胞复合体, 对比学习, 元学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: CellCLAT是一种自监督拓扑深度学习方法，通过参数扰动增强和元学习修剪，解决了细胞复合体的结构约束和语义冗余问题。

Motivation: 细胞复合体虽比单纯复形更具表达力，但其自监督学习的进展受限于结构约束和语义冗余两大挑战。

Method: 提出CellCLAT框架，包含参数扰动增强（保留拓扑结构）和基于元学习的细胞修剪（移除冗余信息）。

Result: 理论及实验证明CellCLAT优于现有自监督图学习方法。

Conclusion: CellCLAT为细胞复合体的自监督学习提供了有效解决方案，推动了该领域的发展。

Abstract: Self-supervised topological deep learning (TDL) represents a nascent but
underexplored area with significant potential for modeling higher-order
interactions in simplicial complexes and cellular complexes to derive
representations of unlabeled graphs. Compared to simplicial complexes, cellular
complexes exhibit greater expressive power. However, the advancement in
self-supervised learning for cellular TDL is largely hindered by two core
challenges: \textit{extrinsic structural constraints} inherent to cellular
complexes, and intrinsic semantic redundancy in cellular representations. The
first challenge highlights that traditional graph augmentation techniques may
compromise the integrity of higher-order cellular interactions, while the
second underscores that topological redundancy in cellular complexes
potentially diminish task-relevant information. To address these issues, we
introduce Cellular Complex Contrastive Learning with Adaptive Trimming
(CellCLAT), a twofold framework designed to adhere to the combinatorial
constraints of cellular complexes while mitigating informational redundancy.
Specifically, we propose a parameter perturbation-based augmentation method
that injects controlled noise into cellular interactions without altering the
underlying cellular structures, thereby preserving cellular topology during
contrastive learning. Additionally, a cellular trimming scheduler is employed
to mask gradient contributions from task-irrelevant cells through a bi-level
meta-learning approach, effectively removing redundant topological elements
while maintaining critical higher-order semantics. We provide theoretical
justification and empirical validation to demonstrate that CellCLAT achieves
substantial improvements over existing self-supervised graph learning methods,
marking a significant attempt in this domain.

</details>


### [112] [Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning](https://arxiv.org/abs/2505.21591)
*Maosen Zhao,Pengtao Chen,Chong Yu,Yan Wen,Xudong Tan,Tao Chen*

Key words: 扩散模型、4位量化、浮点量化、MSFP、微调

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为MSFP的混合符号浮点量化框架，解决了现有4位量化方法在扩散模型中性能不稳定的问题。

Motivation: 扩散模型的4位量化在内存效率和推理速度上具有优势，但现有方法性能不稳定，为此探索了低比特浮点量化。

Method: 提出了MSFP框架，引入无符号浮点量化、时间感知LoRA（TALoRA）和去噪因子损失对齐（DFA），以实现精确稳定的微调。

Result: 实验证明，该方法首次在4位浮点量化中实现了优于现有4位整数量化方法的性能。

Conclusion: MSFP框架为扩散模型的低比特量化提供了一种有效且稳定的解决方案。

Abstract: Model quantization reduces the bit-width of weights and activations,
improving memory efficiency and inference speed in diffusion models. However,
achieving 4-bit quantization remains challenging. Existing methods, primarily
based on integer quantization and post-training quantization fine-tuning,
struggle with inconsistent performance. Inspired by the success of
floating-point (FP) quantization in large language models, we explore low-bit
FP quantization for diffusion models and identify key challenges: the failure
of signed FP quantization to handle asymmetric activation distributions, the
insufficient consideration of temporal complexity in the denoising process
during fine-tuning, and the misalignment between fine-tuning loss and
quantization error. To address these challenges, we propose the mixup-sign
floating-point quantization (MSFP) framework, first introducing unsigned FP
quantization in model quantization, along with timestep-aware LoRA (TALoRA) and
denoising-factor loss alignment (DFA), which ensure precise and stable
fine-tuning. Extensive experiments show that we are the first to achieve
superior performance in 4-bit FP quantization for diffusion models,
outperforming existing PTQ fine-tuning methods in 4-bit INT quantization.

</details>


### [113] [Relevance-driven Input Dropout: an Explanation-guided Regularization Technique](https://arxiv.org/abs/2505.21595)
*Shreyas Gururaj,Lars Grüne,Wojciech Samek,Sebastian Lapuschkin,Leander Weber*

Key words: 过拟合, 数据增强, 选择性遮挡, 泛化能力, 机器学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了Relevance-driven Input Dropout (RelDrop)方法，通过选择性遮挡输入的最相关区域来增强模型的泛化能力。

Motivation: 现有数据增强方法多采用随机遮挡，未充分利用对模型决策影响最大的区域，导致泛化能力有限。

Method: 提出RelDrop方法，选择性遮挡输入的最相关区域，迫使模型利用其他重要特征进行预测。

Result: 实验证明RelDrop提升了对遮挡的鲁棒性，模型能利用更多感兴趣区域的特征，并提高了推理时的泛化性能。

Conclusion: RelDrop通过基于相关性的选择性遮挡，有效改善了模型的泛化能力。

Abstract: Overfitting is a well-known issue extending even to state-of-the-art (SOTA)
Machine Learning (ML) models, resulting in reduced generalization, and a
significant train-test performance gap. Mitigation measures include a
combination of dropout, data augmentation, weight decay, and other
regularization techniques. Among the various data augmentation strategies,
occlusion is a prominent technique that typically focuses on randomly masking
regions of the input during training. Most of the existing literature
emphasizes randomness in selecting and modifying the input features instead of
regions that strongly influence model decisions. We propose Relevance-driven
Input Dropout (RelDrop), a novel data augmentation method which selectively
occludes the most relevant regions of the input, nudging the model to use other
important features in the prediction process, thus improving model
generalization through informed regularization. We further conduct qualitative
and quantitative analyses to study how Relevance-driven Input Dropout (RelDrop)
affects model decision-making. Through a series of experiments on benchmark
datasets, we demonstrate that our approach improves robustness towards
occlusion, results in models utilizing more features within the region of
interest, and boosts inference time generalization performance. Our code is
available at https://github.com/Shreyas-Gururaj/LRP_Relevance_Dropout.

</details>


### [114] [SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge](https://arxiv.org/abs/2505.21605)
*Fengqing Jiang,Fengbo Ma,Zhangchen Xu,Yuetai Li,Bhaskar Ramasubramanian,Luyao Niu,Bo Li,Xianyan Chen,Zhen Xiang,Radha Poovendran*

Key words: 大型语言模型、安全基准测试、高风险科学领域、有害响应、模型对齐

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 这篇论文提出了SOSBench基准测试，用于评估大型语言模型在高风险科学领域的安全性问题。研究发现，即使声称对齐的先进模型在这些领域仍存在大量有害响应，揭示了安全对齐的重大缺陷。

Motivation: 目前的安全基准测试通常关注低风险或知识要求低的场景，无法充分评估语言模型在知识密集型高风险领域的表现。SOSBench的推出旨在填补这一关键空白。

Method: 作者开发了SOSBench基准测试，包含六个高风险科学领域的3000个提示，通过LLM辅助的进化管道生成多样化的现实滥用场景，并在此框架下评估前沿模型的表现。

Result: 评估显示，即使声称对齐的先进模型在所有领域均存在违规内容，有害响应率惊人（如Deepseek-R1为79.1%，GPT-4.1为47.3%）。

Conclusion: 这些结果表明，现有模型在安全对齐上存在重大缺陷，对强大语言模型的负责任部署提出了紧迫关切。

Abstract: Large language models (LLMs) exhibit advancing capabilities in complex tasks,
such as reasoning and graduate-level question answering, yet their resilience
against misuse, particularly involving scientifically sophisticated risks,
remains underexplored. Existing safety benchmarks typically focus either on
instructions requiring minimal knowledge comprehension (e.g., ``tell me how to
build a bomb") or utilize prompts that are relatively low-risk (e.g.,
multiple-choice or classification tasks about hazardous content). Consequently,
they fail to adequately assess model safety when handling knowledge-intensive,
hazardous scenarios.
  To address this critical gap, we introduce SOSBench, a regulation-grounded,
hazard-focused benchmark encompassing six high-risk scientific domains:
chemistry, biology, medicine, pharmacology, physics, and psychology. The
benchmark comprises 3,000 prompts derived from real-world regulations and laws,
systematically expanded via an LLM-assisted evolutionary pipeline that
introduces diverse, realistic misuse scenarios (e.g., detailed explosive
synthesis instructions involving advanced chemical formulas). We evaluate
frontier models within a unified evaluation framework using our SOSBench.
Despite their alignment claims, advanced models consistently disclose
policy-violating content across all domains, demonstrating alarmingly high
rates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1).
These results highlight significant safety alignment deficiencies and
underscore urgent concerns regarding the responsible deployment of powerful
LLMs.

</details>


### [115] [Learning Where to Learn: Training Distribution Selection for Provable OOD Performance](https://arxiv.org/abs/2505.21626)
*Nicolas Guerra,Nicholas H. Nelsen,Yunan Yang*

Key words: OOD泛化, 数据分布, 双层优化, 泛化边界

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了如何设计训练数据分布以最大化OOD泛化性能，提出两种算法策略并验证其有效性。

Motivation: 解决机器学习中OOD泛化的挑战，提升模型在未见分布上的性能。

Method: 通过理论分析制定泛化边界，并提出双层优化和最小化OOD误差上界两种算法。

Result: 在多个任务中，提出的方法显著优于传统固定分布的ERM。

Conclusion: 分布感知训练是提升OOD泛化的有效框架。

Abstract: Out-of-distribution (OOD) generalization remains a fundamental challenge in
machine learning. Models trained on one data distribution often experience
substantial performance degradation when evaluated on shifted or unseen
domains. To address this challenge, the present paper studies the design of
training data distributions that maximize average-case OOD performance. First,
a theoretical analysis establishes a family of generalization bounds that
quantify how the choice of training distribution influences OOD error across a
predefined family of target distributions. These insights motivate the
introduction of two complementary algorithmic strategies: (i) directly
formulating OOD risk minimization as a bilevel optimization problem over the
space of probability measures and (ii) minimizing a theoretical upper bound on
OOD error. Last, the paper evaluates the two approaches across a range of
function approximation and operator learning examples. The proposed methods
significantly improve OOD accuracy over standard empirical risk minimization
with a fixed distribution. These results highlight the potential of
distribution-aware training as a principled and practical framework for robust
OOD generalization.

</details>


### [116] [Apprenticeship learning with prior beliefs using inverse optimization](https://arxiv.org/abs/2505.21639)
*Mauricio Junca,Esteban Leiva*

Key words: 逆强化学习, 逆优化, 马尔可夫决策过程, 学徒学习, 正则化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文探讨了逆强化学习（IRL）与马尔可夫决策过程（MDP）的逆优化（IO）之间的关系，并通过引入成本函数的先验信念，展示了学徒学习（AL）形式主义的凸分析视角是框架的松弛形式。重点研究了次优专家设置，提出了一种正则化的极小极大问题，并使用随机镜像下降法（SMD）解决，数值实验证明了正则化的关键作用。

Motivation: 研究动机在于填补文献中IRL与MDP的IO关系未被充分探索的空白，同时解决IRL的“不适定性”问题，通过引入先验信念和正则化提升学习效果。

Method: 提出了一种结合先验信念的正则化极小极大AL问题框架，并利用随机镜像下降法（SMD）进行优化求解。

Result: 数值实验验证了正则化在成本函数和学徒策略学习中的重要作用，尤其是对次优专家设置的适应性提升。

Conclusion: 本文通过统一IRL、IO和AL框架，并引入正则化，有效解决了IRL的不适定性问题，为次优专家环境的学习提供了新方法。

Abstract: The relationship between inverse reinforcement learning (IRL) and inverse
optimization (IO) for Markov decision processes (MDPs) has been relatively
underexplored in the literature, despite addressing the same problem. In this
work, we revisit the relationship between the IO framework for MDPs, IRL, and
apprenticeship learning (AL). We incorporate prior beliefs on the structure of
the cost function into the IRL and AL problems, and demonstrate that the
convex-analytic view of the AL formalism (Kamoutsi et al., 2021) emerges as a
relaxation of our framework. Notably, the AL formalism is a special case in our
framework when the regularization term is absent. Focusing on the suboptimal
expert setting, we formulate the AL problem as a regularized min-max problem.
The regularizer plays a key role in addressing the ill-posedness of IRL by
guiding the search for plausible cost functions. To solve the resulting
regularized-convex-concave-min-max problem, we use stochastic mirror descent
(SMD) and establish convergence bounds for the proposed method. Numerical
experiments highlight the critical role of regularization in learning cost
vectors and apprentice policies.

</details>


### [117] [Efficient Diffusion Models for Symmetric Manifolds](https://arxiv.org/abs/2505.21640)
*Oren Mangoubi,Neil He,Nisheeth K. Vishnoi*

Key words: 扩散模型, 黎曼流形, 对称空间, 热核, 布朗运动

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一个针对对称空间黎曼流形的高效扩散模型框架，通过空间变化的协方差和欧氏布朗运动投影绕过热核计算，显著提升了训练效率和样本质量。

Motivation: 现有流形扩散模型依赖热核，计算复杂且效率低，尤其是在高维情况下。因此，提出了一个更高效的框架，以降低计算成本并提升性能。

Method: 采用空间变化协方差和欧氏布朗运动投影，结合Ito引理推导的新目标函数，实现O(1)梯度计算和近乎线性的算术操作。

Result: 在环面、特殊正交群和酉群等合成数据集上，训练速度和样本质量均优于现有方法。

Conclusion: 该框架通过利用流形对称性和高效计算策略，显著缩小了对称流形与欧式空间扩散模型的效率差距。

Abstract: We introduce a framework for designing efficient diffusion models for
$d$-dimensional symmetric-space Riemannian manifolds, including the torus,
sphere, special orthogonal group and unitary group. Existing manifold diffusion
models often depend on heat kernels, which lack closed-form expressions and
require either $d$ gradient evaluations or exponential-in-$d$ arithmetic
operations per training step. We introduce a new diffusion model for symmetric
manifolds with a spatially-varying covariance, allowing us to leverage a
projection of Euclidean Brownian motion to bypass heat kernel computations. Our
training algorithm minimizes a novel efficient objective derived via Ito's
Lemma, allowing each step to run in $O(1)$ gradient evaluations and
nearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap
between diffusions on symmetric manifolds and Euclidean space. Manifold
symmetries ensure the diffusion satisfies an "average-case" Lipschitz
condition, enabling accurate and efficient sample generation. Empirically, our
model outperforms prior methods in training speed and improves sample quality
on synthetic datasets on the torus, special orthogonal group, and unitary
group.

</details>


### [118] [PrivATE: Differentially Private Confidence Intervals for Average Treatment Effects](https://arxiv.org/abs/2505.21641)
*Maresa Schröder,Justin Hartenstein,Stefan Feuerriegel*

Key words: 平均处理效应,差分隐私,置信区间,机器学习,双重稳健性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 这篇论文提出了PrivATE框架，用于在差分隐私下计算平均处理效应（ATE）的置信区间，确保在敏感数据中的隐私保护和统计有效性。

Motivation: 在医疗等安全关键领域，准确评估ATE需要可靠的置信区间，但数据隐私保护限制了传统方法的适用性。因此，需开发一种既能保护隐私又提供有效统计推断的框架。

Method: PrivATE框架包含三步：差分隐私ATE估计、方差估计及置信区间构建。采用输出扰动和截断机制，且具备模型无关性和双重稳健性。

Result: 实验表明，PrivATE在合成和真实医学数据上均能有效生成有效的置信区间，填补了差分隐私下ATE推断的空白。

Conclusion: PrivATE是首个在差分隐私下提供双重稳健性ATE置信区间的通用框架，兼具隐私保护和统计有效性。

Abstract: The average treatment effect (ATE) is widely used to evaluate the
effectiveness of drugs and other medical interventions. In safety-critical
applications like medicine, reliable inferences about the ATE typically require
valid uncertainty quantification, such as through confidence intervals (CIs).
However, estimating treatment effects in these settings often involves
sensitive data that must be kept private. In this work, we present PrivATE, a
novel machine learning framework for computing CIs for the ATE under
differential privacy. Specifically, we focus on deriving valid
privacy-preserving CIs for the ATE from observational data. Our PrivATE
framework consists of three steps: (i) estimating a differentially private ATE
through output perturbation; (ii) estimating the differentially private
variance through a truncated output perturbation mechanism; and (iii)
constructing the CIs while accounting for the uncertainty from both the
estimation and privatization steps. Our PrivATE framework is model agnostic,
doubly robust, and ensures valid CIs. We demonstrate the effectiveness of our
framework using synthetic and real-world medical datasets. To the best of our
knowledge, we are the first to derive a general, doubly robust framework for
valid CIs of the ATE under ($\varepsilon$, $\delta$)-differential privacy.

</details>


### [119] [AutoSGD: Automatic Learning Rate Selection for Stochastic Gradient Descent](https://arxiv.org/abs/2505.21651)
*Nikola Surjanovic,Alexandre Bouchard-Côté,Trevor Campbell*

Key words: AutoSGD、学习率调整、随机梯度下降、自动化调参、优化算法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: AutoSGD是一种自动调整学习率的SGD方法，通过动态增减学习率减少用户调参负担，理论证明其收敛性，实验显示其在多种优化和机器学习任务中表现优异。

Motivation: 学习率是SGD的关键参数，传统手动调参费时费力，需自动化解决方案。

Method: 提出AutoSGD，动态判断并调整每轮迭代的学习率增减，配套理论支持其收敛性。

Result: 实验验证AutoSGD在经典优化问题和机器学习任务中效果显著。

Conclusion: AutoSGD通过自动化学习率调整降低调参难度，兼具理论保证和实际性能优势。

Abstract: The learning rate is an important tuning parameter for stochastic gradient
descent (SGD) and can greatly influence its performance. However, appropriate
selection of a learning rate schedule across all iterations typically requires
a non-trivial amount of user tuning effort. To address this, we introduce
AutoSGD: an SGD method that automatically determines whether to increase or
decrease the learning rate at a given iteration and then takes appropriate
action. We introduce theory supporting the convergence of AutoSGD, along with
its deterministic counterpart for standard gradient descent. Empirical results
suggest strong performance of the method on a variety of traditional
optimization problems and machine learning tasks.

</details>


### [120] [PreGenie: An Agentic Framework for High-quality Visual Presentation Generation](https://arxiv.org/abs/2505.21660)
*Xiaojie Xu,Xinli Xu,Sirui Chen,Haoyu Chen,Fan Zhang,Ying-Cong Chen*

Key words: 视觉演示, 多模态大语言模型, Slidev, 自动化生成

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: PreGenie是一个基于多模态大语言模型的模块化框架，用于生成高质量视觉演示，通过两阶段（分析与初始生成、审查与重新生成）提升布局和内容一致性。

Motivation: 现有自动化视觉演示生成存在布局混乱、文本摘要不准确和图文不匹配问题，限制了其在正式场合的应用。

Method: 基于Slidev框架，分两阶段（分析与初始生成、审查与重新生成）迭代优化演示，利用多模态大语言模型协作。

Result: PreGenie在多模态理解和美学设计上优于现有模型，更符合人类设计偏好。

Conclusion: PreGenie通过模块化设计和多模型协作，有效提升了自动化演示的质量和实用性。

Abstract: Visual presentations are vital for effective communication. Early attempts to
automate their creation using deep learning often faced issues such as poorly
organized layouts, inaccurate text summarization, and a lack of image
understanding, leading to mismatched visuals and text. These limitations
restrict their application in formal contexts like business and scientific
research. To address these challenges, we propose PreGenie, an agentic and
modular framework powered by multimodal large language models (MLLMs) for
generating high-quality visual presentations.
  PreGenie is built on the Slidev presentation framework, where slides are
rendered from Markdown code. It operates in two stages: (1) Analysis and
Initial Generation, which summarizes multimodal input and generates initial
code, and (2) Review and Re-generation, which iteratively reviews intermediate
code and rendered slides to produce final, high-quality presentations. Each
stage leverages multiple MLLMs that collaborate and share information.
Comprehensive experiments demonstrate that PreGenie excels in multimodal
understanding, outperforming existing models in both aesthetics and content
consistency, while aligning more closely with human design preferences.

</details>


### [121] [Efficient Controllable Diffusion via Optimal Classifier Guidance](https://arxiv.org/abs/2505.21666)
*Owen Oertell,Shikun Sun,Yiding Chen,Jin Peng Zhou,Zhiyong Wang,Wen Sun*

Key words: 扩散模型,可控生成,监督学习,KL散度,在线学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: SLCD方法通过监督学习优化扩散模型的可控生成，避免了强化学习的复杂性和资源消耗，实现了高质量样本生成与高效推理。

Motivation: 传统强化学习方法在可控生成中可能过拟合奖励函数且资源消耗大，SLCD旨在通过监督学习简化流程并提升效率。

Method: SLCD迭代生成在线数据并训练小型分类器，以指导扩散模型的生成，核心为分类任务而非复杂RL控制。

Result: 理论上SLCD在KL散度下收敛于最优解；实证中在图像和生物序列生成中均实现高质量输出且推理时间接近基线模型。

Conclusion: SLCD提供了一种更简单高效的可控生成方法，适用于连续和离散扩散任务。

Abstract: The controllable generation of diffusion models aims to steer the model to
generate samples that optimize some given objective functions. It is desirable
for a variety of applications including image generation, molecule generation,
and DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of
the base model is a popular approach but it can overfit the reward function
while requiring significant resources. We frame controllable generation as a
problem of finding a distribution that optimizes a KL-regularized objective
function. We present SLCD -- Supervised Learning based Controllable Diffusion,
which iteratively generates online data and trains a small classifier to guide
the generation of the diffusion model. Similar to the standard
classifier-guided diffusion, SLCD's key computation primitive is classification
and does not involve any complex concepts from RL or control. Via a reduction
to no-regret online learning analysis, we show that under KL divergence, the
output from SLCD provably converges to the optimal solution of the
KL-regularized objective. Further, we empirically demonstrate that SLCD can
generate high quality samples with nearly the same inference time as the base
model in both image generation with continuous diffusion and biological
sequence generation with discrete diffusion. Our code is available at
https://github.com/Owen-Oertell/slcd

</details>


### [122] [What happens when generative AI models train recursively on each others' generated outputs?](https://arxiv.org/abs/2505.21677)
*Hung Ahn Vu,Galen Reeves,Emily Wenger*

Key words: 生成AI, 数据交互, 同质化, 新概念, 模型训练

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了生成AI模型训练于其他模型生成内容的影响，探讨了数据交互可能带来的益处（如接触新概念）和风险（如性能同质化）。

Motivation: 随着社会对生成AI工具的依赖增加，理解模型间通过数据交互可能产生的下游影响变得至关重要。

Method: 通过实证分析和理论建模，研究了数据交互的实践过程和长期效应。

Result: 数据交互既能让模型接触新概念，也可能导致任务性能同质化。

Conclusion: 数据交互具有双重影响，需在利用其好处的同时警惕潜在风险。

Abstract: The internet is full of AI-generated content while also serving as a common
source of training data for generative AI (genAI) models. This duality raises
the possibility that future genAI models may be trained on other models'
generated outputs. Prior work has studied consequences of models training on
their own generated outputs, but limited work has considered what happens if
models ingest content produced by other models. Given society's increasing
dependence on genAI tools, understanding downstream effects of such
data-mediated model interactions is critical. To this end, we provide empirical
evidence for how data-mediated interactions might unfold in practice, develop a
theoretical model for this interactive training process, and show
experimentally possible long-term results of such interactions. We find that
data-mediated interactions can benefit models by exposing them to novel
concepts perhaps missed in original training data, but also can homogenize
their performance on shared tasks.

</details>


### [123] [multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data](https://arxiv.org/abs/2505.21680)
*Andrew J. Loza,Jun Yup Kim,Shangzheng Song,Yihang Liu,Joseph J. Y. Sung,R Andrew Taylor,Dennis L. Shung*

Key words: 混合数据类型、自回归建模、嵌入方案、Transformer

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 多元GPT提出了一种单一架构，用于处理混合分类和数值数据的序列建模，通过自回归序列分解和嵌入方案扩展了传统方法的应用范围。

Motivation: 现实世界的数据常为混合类型且采样不规则，现有方法（如离散标记或神经ODE）在处理此类数据时存在局限，需要更通用的建模方法。

Method: 采用自回归序列分解和特定嵌入方案，将模型扩展为联合估计下个标记类别与值的分布，适用于分类与数值数据的混合处理。

Result: 模型在简单物理系统、心电图和多变量电子健康记录数据中显示出高效的模式泛化能力。

Conclusion: 多元GPT通过统一框架扩展了基于Transformer的模型在处理混合类型数据上的实用性。

Abstract: Real-world processes often generate data that are a mix of categorical and
numeric values that are recorded at irregular and informative intervals.
Discrete token-based approaches are limited in numeric representation capacity
while methods like neural ordinary differential equations are not well suited
for categorical data or informative sampling and require augmentation to handle
certain classes of trajectories. Here, we present multivariateGPT, a single
architecture for modeling sequences of mixed categorical (including tokenized
text) and numeric data. This is accomplished with an autoregressive sequence
decomposition, embedding scheme, and loss function that extend the next token
prediction task to likelihood estimation of the joint distribution of next
token class and value. We demonstrate how this approach can efficiently learn
to generalize patterns in simple physical systems and model complex time series
including electrocardiograms and multivariate electronic health record data.
This work extends the utility of transformer based models to additional classes
of data.

</details>


### [124] [Incentivizing Permissionless Distributed Learning of LLMs](https://arxiv.org/abs/2505.21684)
*Joel Lidin,Amir Sarfi,Evangelos Pappas,Samuel Dare,Eugene Belilovsky,Jacob Steeves*

Key words: 分布式深度学习,激励机制,Bittensor,伪梯度,OpenSkill

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文描述了Gauntlet，一个用于分布式深度学习基础模型的激励系统，已在Bittensor区块链上部署，并通过无许可的伪梯度贡献训练了一个12亿参数的LLM。

Motivation: 旨在解决分布式深度学习中如何公平奖励贡献者的问题，尤其是在无许可环境下，确保模型训练的可靠性和效率。

Method: 结合了两阶段机制（快速过滤节点的可用性、可靠性和同步性）和核心组件（评估伪梯度贡献前后的损失变化），并采用OpenSkill评分系统跟踪贡献质量。

Result: 成功训练了一个12亿参数的LLM，并通过真实代币奖励参与者，证明了该激励系统的有效性。

Conclusion: Gauntlet系统在无许可环境下实现了高效的分布式训练，并通过激励机制确保了贡献的公平性。

Abstract: We describe an incentive system for distributed deep learning of foundational
models where peers are rewarded for contributions. The incentive system,
\textit{Gauntlet}, has been deployed on the bittensor blockchain and used to
train a 1.2B LLM with completely permissionless contributions of
pseudo-gradients: no control over the users that can register or their
hardware. \textit{Gauntlet} can be applied to any synchronous distributed
training scheme that relies on aggregating updates or pseudo-gradients. We rely
on a two-stage mechanism for fast filtering of peer uptime, reliability, and
synchronization, combined with the core component that estimates the loss
before and after individual pseudo-gradient contributions. We utilized an
OpenSkill rating system to track competitiveness of pseudo-gradient scores
across time. Finally, we introduce a novel mechanism to ensure peers on the
network perform unique computations. Our live 1.2B run, which has paid out
real-valued tokens to participants based on the value of their contributions,
yielded a competitive (on a per-iteration basis) 1.2B model that demonstrates
the utility of our incentive system.

</details>


### [125] [AMSFL: Adaptive Multi-Step Federated Learning via Gradient Difference-Based Error Modeling](https://arxiv.org/abs/2505.21695)
*Ganglou Xu*

Key words: 联邦学习, 通信效率, 模型准确性, Gradient Difference Approximation, AMSFL框架

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为Gradient Difference Approximation (GDA)的轻量级方法，通过一阶信息估计本地误差趋势，避免计算完整的Hessian矩阵，提升了联邦学习的通信效率和模型准确性。

Motivation: 联邦学习在平衡通信效率和模型准确性方面面临挑战，尤其是如何在不增加高计算成本的情况下近似更新误差。

Method: 提出了Gradient Difference Approximation (GDA)方法，利用一阶信息估计本地误差趋势，无需计算完整的Hessian矩阵，并将其整合到Adaptive Multi-Step Federated Learning (AMSFL)框架中。

Result: GDA方法在AMSFL框架中提供了一个统一的大规模多步自适应训练环境的误差建模策略。

Conclusion: GDA方法有效解决了联邦学习中的误差近似问题，提升了通信效率和模型准确性。

Abstract: Federated learning faces critical challenges in balancing communication
efficiency and model accuracy. One key issue lies in the approximation of
update errors without incurring high computational costs. In this paper, we
propose a lightweight yet effective method called Gradient Difference
Approximation (GDA), which leverages first-order information to estimate local
error trends without computing the full Hessian matrix. The proposed method
forms a key component of the Adaptive Multi-Step Federated Learning (AMSFL)
framework and provides a unified error modeling strategy for large-scale
multi-step adaptive training environments.

</details>


### [126] [Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling](https://arxiv.org/abs/2505.21717)
*Mónika Farsang,Ramin Hasani,Radu Grosu*

Key words: LrcSSM, 非线性循环模型, 状态空间, 长序列处理, 并行计算

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: LrcSSM是一种非线性循环模型，能以线性状态空间层的速度处理长序列。通过强制状态转移矩阵对角化并动态学习，该模型能以并行方式高效处理序列，运算复杂度为O(TD)。

Motivation: 为了解决现有模型在长序列处理上的效率问题，尤其是线性状态空间层的局限性和其他模型的梯度稳定性不足，LrcSSM应运而生。

Method: 采用对角化状态转移矩阵，动态学习每一步的参数，利用并行前缀扫描技术实现高效计算。

Result: 在长序列预测任务中，LrcSSM的表现优于LRU、S5和Mamba，计算效率和梯度稳定性均有提升。

Conclusion: LrcSSM在长序列处理上具有高效性和稳定性优势，适合大规模计算任务。

Abstract: We present LrcSSM, a \textit{nonlinear} recurrent model that processes long
sequences as fast as today's linear state-space layers. By forcing the
state-transition matrix to be diagonal and learned at every step, the full
sequence can be solved in parallel with a single prefix-scan, giving
$\mathcal{O}(TD)$ time and memory and only $\mathcal{O}(\log T)$ sequential
depth, for input-sequence length $T$ and a state dimension $D$. Moreover,
LrcSSM offers a formal gradient-stability guarantee that other input-varying
systems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth
$L$, as the forward and backward passes cost $\Theta(T\,D\,L)$ FLOPs, with its
low sequential depth and parameter count $\Theta(D\,L)$, the model follows the
compute-optimal scaling law regime ($\beta \approx 0.42$) recently observed for
Mamba, outperforming quadratic-attention Transformers at equal compute while
avoiding the memory overhead of FFT-based long convolutions. We show that on a
series of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba.

</details>


### [127] [Saddle-To-Saddle Dynamics in Deep ReLU Networks: Low-Rank Bias in the First Saddle Escape](https://arxiv.org/abs/2505.21722)
*Ioannis Bantzis,James B. Simon,Arthur Jacot*

Key words: ReLU网络, 梯度下降, 鞍点, 逃逸方向, 低秩偏置

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了深度ReLU网络在小权重初始化下，梯度下降（GD）在参数空间中受原点鞍点主导的现象，分析了逃逸方向的作用及其低秩偏置特性。

Motivation: 探讨深度ReLU网络中梯度下降的初始行为，尤其是逃逸方向的性质及其对优化过程的影响，为理解优化动态提供理论基础。

Method: 研究逃逸方向的性质，证明更深层权重矩阵在逃逸方向上的最优路径具有低秩偏置，即第一奇异值显著大于其他奇异值。

Result: 发现逃逸方向在更深层表现出低秩偏置，第一奇异值至少是其他奇异值的ℓ^(1/4)倍。

Conclusion: 结果为证明深度ReLU网络中的“鞍点到鞍点”动态提供了初步支持，即梯度下降会逐步访问瓶颈秩增加的鞍点序列。

Abstract: When a deep ReLU network is initialized with small weights, GD is at first
dominated by the saddle at the origin in parameter space. We study the
so-called escape directions, which play a similar role as the eigenvectors of
the Hessian for strict saddles. We show that the optimal escape direction
features a low-rank bias in its deeper layers: the first singular value of the
$\ell$-th layer weight matrix is at least $\ell^{\frac{1}{4}}$ larger than any
other singular value. We also prove a number of related results about these
escape directions. We argue that this result is a first step in proving
Saddle-to-Saddle dynamics in deep ReLU networks, where GD visits a sequence of
saddles with increasing bottleneck rank.

</details>


### [128] [Deep Reinforcement Learning Agents are not even close to Human Intelligence](https://arxiv.org/abs/2505.21731)
*Quentin Delfosse,Jannis Blüml,Fabian Tatai,Théo Vincent,Bjarne Gregori,Elisabeth Dillies,Jan Peters,Constantin Rothkopf,Kristian Kersting*

Key words: 深度强化学习,零样本适应,HackAtari,捷径依赖,系统性泛化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文指出深度强化学习（RL）代理在简化任务版本上表现显著下降，揭示了其对捷径的依赖，强调了需要新基准和方法。

Motivation: 当前深度强化学习代理缺乏零样本适应能力，且现有评估多集中于任务复杂化，忽略了对任务简化的研究。

Method: 引入HackAtari，一套Arcade学习环境的任务变体，评估多种RL算法和架构在简化任务上的表现。

Result: RL代理在简化任务上表现大幅下降，显示出对捷径的依赖，与人类行为智能存在显著差距。

Conclusion: 需开发新基准和方法，以超越静态评估协议，提升RL代理的系统性泛化能力。

Abstract: Deep reinforcement learning (RL) agents achieve impressive results in a wide
variety of tasks, but they lack zero-shot adaptation capabilities. While most
robustness evaluations focus on tasks complexifications, for which human also
struggle to maintain performances, no evaluation has been performed on tasks
simplifications. To tackle this issue, we introduce HackAtari, a set of task
variations of the Arcade Learning Environments. We use it to demonstrate that,
contrary to humans, RL agents systematically exhibit huge performance drops on
simpler versions of their training tasks, uncovering agents' consistent
reliance on shortcuts. Our analysis across multiple algorithms and
architectures highlights the persistent gap between RL agents and human
behavioral intelligence, underscoring the need for new benchmarks and
methodologies that enforce systematic generalization testing beyond static
evaluation protocols. Training and testing in the same environment is not
enough to obtain agents equipped with human-like intelligence.

</details>


### [129] [LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing](https://arxiv.org/abs/2505.21732)
*Ruijie Zhang,Ziyue Liu,Zhengyang Wang,Zheng Zhang*

Key words: 低秩模型, 参数高效, 信息流动, ViT, LLaMA

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: LaX（Latent Crossing）是一种简单高效的模块，通过促进低秩子空间的信息流动，提升了低秩模型的性能，使其在减少参数量的同时匹配甚至超越全秩基线。

Motivation: 由于训练基础模型（如ViTs和LLMs）的计算成本极高，低秩矩阵或张量分解成为一种参数高效的方法，但因参数空间受限常导致性能下降。LaX旨在通过增强低秩模型的能力来解决这一问题。

Method: LaX是一种即插即用模块，通过促进低秩子空间的信息流动来扩展低秩模型的容量。它被应用于ViT和LLaMA类模型的预训练任务，并在低秩适配器（如LoRA）微调时保持性能提升。

Result: LaX在2-3倍参数减少的情况下，使低秩模型性能匹配或超越全秩基线。在LLaMA-7/13B的微调任务中，LaX在算术和常识推理任务上表现出持续改进。

Conclusion: LaX通过信息流动机制有效提升了低秩模型的性能，为高效训练大规模模型提供了一种实用解决方案。

Abstract: Training foundation models such as ViTs and LLMs requires tremendous
computing cost. Low-rank matrix or tensor factorization offers a
parameter-efficient alternative, but often downgrades performance due to the
restricted parameter space. In this work, we introduce {\textbf{Latent Crossing
(LaX)}} -- a simple yet effective plug-and-play module that enhances the
capacity of low-rank models by enabling information flow across low-rank
subspaces. We extensively validate the benefits of LaX on pre-training tasks
with ViT-Base/Large and LLaMA-like models ranging from 60M to 1B parameters.
LaX boosts low-rank model performance to match or exceed the full-rank
baselines while using 2-3\(\times\) fewer parameters. When equipped with
low-rank adapters (i.e., LoRA) for fine-tuning LLaMA-7/13B, LaX consistently
improves performance on arithmetic and common sense reasoning tasks with
negligible cost.

</details>


### [130] [Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen](https://arxiv.org/abs/2505.21743)
*Zihao Li,Xinyuan Cao,Xiangbo Gao,Kexin Tian,Keshu Wu,Mohammad Anis,Hao Zhang,Keke Long,Jiwan Jiang,Xiaopeng Li,Yunlong Zhang,Tianbao Yang,Dominique Lord,Zhengzhong Tu,Yang Zhou*

Key words: 交通安全性, 反事实学习, 数字孪生, 生成场景引擎, 零愿景

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新的交通安全性学习方法，通过分析未发生的危险情景（即"差点事故"），结合生成场景引擎和因果学习，从稀疏的事故数据中提取丰富信息，推动从被动分析到主动预防的转变，以实现"零愿景"目标。

Motivation: 传统交通事故分析方法依赖稀疏且噪声较多的数据，难以有效预防罕见但后果严重的交通事故。为实现"零愿景"（零死亡和重伤），需要转向研究潜在危险情景。

Method: 提出"反事实安全性学习"框架，结合生成场景引擎、多样化驾驶员模型和因果学习，合成并分析"差点事故"。建立数字孪生测试平台，连接微观场景和宏观模式，并通过多目标验证器确保模拟的统计真实性。

Result: 该方法将稀疏的事故数据转化为丰富的预测信号，支持对车辆、道路和政策进行压力测试，从而在部署前预测和预防潜在事故。

Conclusion: 通过学习未发生的危险情景，交通安全性分析可以从被动调查转向主动预防，推动"零愿景"的实现。

Abstract: Traffic safety science has long been hindered by a fundamental data paradox:
the crashes we most wish to prevent are precisely those events we rarely
observe. Existing crash-frequency models and surrogate safety metrics rely
heavily on sparse, noisy, and under-reported records, while even sophisticated,
high-fidelity simulations undersample the long-tailed situations that trigger
catastrophic outcomes such as fatalities. We argue that the path to achieving
Vision Zero, i.e., the complete elimination of traffic fatalities and severe
injuries, requires a paradigm shift from traditional crash-only learning to a
new form of counterfactual safety learning: reasoning not only about what
happened, but also about the vast set of plausible yet perilous scenarios that
could have happened under slightly different circumstances. To operationalize
this shift, our proposed agenda bridges macro to micro. Guided by crash-rate
priors, generative scene engines, diverse driver models, and causal learning,
near-miss events are synthesized and explained. A crash-focused digital twin
testbed links micro scenes to macro patterns, while a multi-objective validator
ensures that simulations maintain statistical realism. This pipeline transforms
sparse crash data into rich signals for crash prediction, enabling the
stress-testing of vehicles, roads, and policies before deployment. By learning
from crashes that almost happened, we can shift traffic safety from reactive
forensics to proactive prevention, advancing Vision Zero.

</details>


### [131] [Revisiting Bi-Linear State Transitions in Recurrent Neural Networks](https://arxiv.org/abs/2505.21749)
*M. Reza Ebrahimi,Roland Memisevic*

Key words: 循环神经网络, 隐藏单元, 双线性操作, 状态跟踪, 归纳偏置

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文探讨了循环神经网络中隐藏单元的主动计算角色，而非被动记忆存储，通过理论和实验验证了双线性操作在状态跟踪任务中的自然归纳偏置。

Motivation: 传统研究多关注隐藏单元的记忆功能，本文旨在探索其作为计算参与者的角色，尤其是在状态跟踪任务中的主动贡献。

Method: 重新引入双线性操作（隐藏单元与输入嵌入的乘性交互），理论分析其在状态跟踪任务中的归纳偏置，并实验验证其层次化结构。

Result: 双线性状态更新形成与任务复杂度对应的自然层次结构，如Mamba等线性循环网络位于该层次的最低复杂度端。

Conclusion: 隐藏单元的主动计算视角为网络行为建模提供了新思路，双线性操作在状态跟踪中具有理论和实践优势。

Abstract: The role of hidden units in recurrent neural networks is typically seen as
modeling memory, with research focusing on enhancing information retention
through gating mechanisms. A less explored perspective views hidden units as
active participants in the computation performed by the network, rather than
passive memory stores. In this work, we revisit bi-linear operations, which
involve multiplicative interactions between hidden units and input embeddings.
We demonstrate theoretically and empirically that they constitute a natural
inductive bias for representing the evolution of hidden states in state
tracking tasks. These are the simplest type of task that require hidden units
to actively contribute to the behavior of the network. We also show that
bi-linear state updates form a natural hierarchy corresponding to state
tracking tasks of increasing complexity, with popular linear recurrent networks
such as Mamba residing at the lowest-complexity center of that hierarchy.

</details>


### [132] [Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals](https://arxiv.org/abs/2505.21750)
*Vivienne Huiling Wang,Tinghuai Wang,Joni Pajarinen*

Key words: 层次强化学习, 条件扩散模型, Gaussian Process, 子目标生成

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于条件扩散模型和Gaussian Process的层次强化学习（HRL）方法，以解决高层策略生成有效子目标的挑战。

Motivation: 解决HRL中因底层策略变化导致高层策略难以生成有效子目标的问题，同时捕捉复杂的子目标分布并量化不确定性。

Method: 使用条件扩散模型生成子目标，并用Gaussian Process先验正则化，结合扩散策略和GP预测均值选择子目标。

Result: 新方法在样本效率和连续控制基准任务上的性能优于现有HRL方法。

Conclusion: 条件扩散模型结合GP先验能有效生成复杂子目标并提高HRL性能。

Abstract: Hierarchical reinforcement learning (HRL) learns to make decisions on
multiple levels of temporal abstraction. A key challenge in HRL is that the
low-level policy changes over time, making it difficult for the high-level
policy to generate effective subgoals. To address this issue, the high-level
policy must capture a complex subgoal distribution while also accounting for
uncertainty in its estimates. We propose an approach that trains a conditional
diffusion model regularized by a Gaussian Process (GP) prior to generate a
complex variety of subgoals while leveraging principled GP uncertainty
quantification. Building on this framework, we develop a strategy that selects
subgoals from both the diffusion policy and GP's predictive mean. Our approach
outperforms prior HRL methods in both sample efficiency and performance on
challenging continuous control benchmarks.

</details>


### [133] [DualSchool: How Reliable are LLMs for Optimization Education?](https://arxiv.org/abs/2505.21775)
*Michael Klamkin,Arnaud Deza,Sikai Cheng,Haoruo Zhao,Pascal Van Hentenryck*

Key words: 线性规划对偶生成（P2DC），大型语言模型（LLM），DualSchool框架，规范图编辑距离，优化教育

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文探讨了大型语言模型（LLM）在优化课程中线性规划对偶生成任务（P2DC）的表现，发现即使是最先进的开放LLM也无法始终生成正确的对偶解。论文提出了DualSchool框架，用于生成和验证P2DC实例。

Motivation: 研究动机在于评估LLM在生成线性规划对偶（P2DC）任务中的表现，尽管LLM在训练中接触了大量相关实例，但其实际表现仍有待验证。

Method: 采用DualSchool框架生成和验证P2DC实例，使用规范图编辑距离（Canonical Graph Edit Distance）进行验证，超越现有优化模型的评估方法。

Result: 实验发现，即使是最先进的开放LLM也无法始终生成正确的对偶解，甚至在最小的两变量实例和衍生任务（如正确性验证和错误分类）中表现不佳。

Conclusion: 论文结论指出LLM在P2DC任务中的局限性，并讨论了其对教育者、学生和大型推理系统开发的启示。

Abstract: Consider the following task taught in introductory optimization courses which
addresses challenges articulated by the community at the intersection of
(generative) AI and OR: generate the dual of a linear program. LLMs, being
trained at web-scale, have the conversion process and many instances of Primal
to Dual Conversion (P2DC) at their disposal. Students may thus reasonably
expect that LLMs would perform well on the P2DC task. To assess this
expectation, this paper introduces DualSchool, a comprehensive framework for
generating and verifying P2DC instances. The verification procedure of
DualSchool uses the Canonical Graph Edit Distance, going well beyond existing
evaluation methods for optimization models, which exhibit many false positives
and negatives when applied to P2DC. Experiments performed by DualSchool reveal
interesting findings. Although LLMs can recite the conversion procedure
accurately, state-of-the-art open LLMs fail to consistently produce correct
duals. This finding holds even for the smallest two-variable instances and for
derivative tasks, such as correctness, verification, and error classification.
The paper also discusses the implications for educators, students, and the
development of large reasoning systems.

</details>


### [134] [Memorization to Generalization: Emergence of Diffusion Models from Associative Memory](https://arxiv.org/abs/2505.21777)
*Bao Pham,Gabriel Raya,Matteo Negri,Mohammed J. Zaki,Luca Ambrogioni,Dmitry Krotov*

Key words: 扩散模型,联想记忆,虚假状态,记忆-泛化,吸引子

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 这篇论文从联想记忆（AM）的角度分析了扩散模型，探讨了在训练数据量不同时扩散模型的记忆与泛化现象，预测并验证了虚假状态的存在。

Motivation: 研究动机在于理解扩散模型中记忆与泛化的动态过程，通过联想记忆的框架揭示训练数据量对模型行为的影响。

Method: 方法是将扩散模型的训练阶段视为记忆编码（存储训练数据），生成阶段视为记忆检索。通过理论分析和实验验证了虚假状态的存在。

Result: 研究结果显示，在小数据集下扩散模型表现出强记忆性，而大数据集下则涌现新的吸引子状态，边界处出现虚假状态。

Conclusion: 结论是扩散模型的记忆-泛化现象可通过联想记忆的视角理解，并预测和验证了虚假状态的存在。

Abstract: Hopfield networks are associative memory (AM) systems, designed for storing
and retrieving patterns as local minima of an energy landscape. In the
classical Hopfield model, an interesting phenomenon occurs when the amount of
training data reaches its critical memory load $- spurious\,\,states$, or
unintended stable points, emerge at the end of the retrieval dynamics, leading
to incorrect recall. In this work, we examine diffusion models, commonly used
in generative modeling, from the perspective of AMs. The training phase of
diffusion model is conceptualized as memory encoding (training data is stored
in the memory). The generation phase is viewed as an attempt of memory
retrieval. In the small data regime the diffusion model exhibits a strong
memorization phase, where the network creates distinct basins of attraction
around each sample in the training set, akin to the Hopfield model below the
critical memory load. In the large data regime, a different phase appears where
an increase in the size of the training set fosters the creation of new
attractor states that correspond to manifolds of the generated samples.
Spurious states appear at the boundary of this transition and correspond to
emergent attractor states, which are absent in the training set, but, at the
same time, have distinct basins of attraction around them. Our findings
provide: a novel perspective on the memorization-generalization phenomenon in
diffusion models via the lens of AMs, theoretical prediction of existence of
spurious states, empirical validation of this prediction in commonly-used
diffusion models.

</details>


### [135] [P-DROP: Poisson-Based Dropout for Graph Neural Networks](https://arxiv.org/abs/2505.21783)
*Hyunsik Yun*

Key words: GNN, 过平滑, 泊松过程, 节点选择, 异步更新

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种基于泊松过程的节点选择策略，通过异步和局部化更新来解决GNN中的过平滑问题，实验表明该方法在标准数据集上表现优于传统方法。

Motivation: 因GNN中重复消息传递导致节点表征收敛且丧失区分性，为解决过平滑问题而提出新型节点选择策略。

Method: 为每个节点配备独立泊松时钟，实现异步和局部化更新，应用于正则化和动态子图训练。

Result: 在Cora、Citeseer和Pubmed数据集上，该方法在后期训练阶段表现优于Dropout、DropEdge和DropNode。

Conclusion: 基于泊松过程的方法有效缓解了过平滑问题，提升了GNN性能。

Abstract: Over-smoothing remains a major challenge in Graph Neural Networks (GNNs),
where repeated message passing causes node representations to converge and lose
discriminative power. To address this, we propose a novel node selection
strategy based on Poisson processes, introducing stochastic but structure-aware
updates. Specifically, we equip each node with an independent Poisson clock,
enabling asynchronous and localized updates that preserve structural diversity.
We explore two applications of this strategy: as a replacement for
dropout-based regularization and as a dynamic subgraph training scheme.
Experimental results on standard benchmarks (Cora, Citeseer, Pubmed)
demonstrate that our Poisson-based method yields competitive or improved
accuracy compared to traditional Dropout, DropEdge, and DropNode approaches,
particularly in later training stages.

</details>


### [136] [Born a Transformer -- Always a Transformer?](https://arxiv.org/abs/2505.21785)
*Yana Veitsman,Mayank Jobanputra,Yash Sarrof,Aleksandra Bakalova,Vera Demberg,Ellie Pavlick,Michael Hahn*

Key words: Transformer, LLMs, 预训练, 长度泛化, 检索任务, 复制任务

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了预训练语言模型（LLMs）是否克服了Transformer在序列任务上的理论限制，发现预训练模型在特定任务中存在归纳与反归纳的不对称性。

Motivation: 研究预训练LLMs是否能克服Transformer架构在序列任务中的理论限制，尤其是长度泛化问题。

Method: 利用C-RASP框架研究长度泛化，设计检索和复制任务，并通过理论和实验分析模型表现。

Result: 预训练模型在归纳任务中表现优于反归纳任务，但通过针对性微调可以消除这种不对称性。

Conclusion: 预训练虽能增强Transformer的某些能力，但无法突破长度泛化的根本限制。

Abstract: Transformers have theoretical limitations in modeling certain
sequence-to-sequence tasks, yet it remains largely unclear if these limitations
play a role in large-scale pretrained LLMs, or whether LLMs might effectively
overcome these constraints in practice due to the scale of both the models
themselves and their pretraining data. We explore how these architectural
constraints manifest after pretraining, by studying a family of
$\textit{retrieval}$ and $\textit{copying}$ tasks inspired by Liu et al.
[2024]. We use the recently proposed C-RASP framework for studying length
generalization [Huang et al., 2025b] to provide guarantees for each of our
settings. Empirically, we observe an $\textit{induction-versus-anti-induction}$
asymmetry, where pretrained models are better at retrieving tokens to the right
(induction) rather than the left (anti-induction) of a query token. This
asymmetry disappears upon targeted fine-tuning if length-generalization is
guaranteed by theory. Mechanistic analysis reveals that this asymmetry is
connected to the differences in the strength of induction versus anti-induction
circuits within pretrained Transformers. We validate our findings through
practical experiments on real-world tasks demonstrating reliability risks. Our
results highlight that pretraining selectively enhances certain Transformer
capabilities, but does not overcome fundamental length-generalization limits.

</details>


### [137] [Faster Rates for Private Adversarial Bandits](https://arxiv.org/abs/2505.21790)
*Hilal Asi,Vinod Raman,Kunal Talwar*

Key words: 差分隐私、对抗性老虎机、专家建议老虎机、后悔上界、亚线性后悔

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了针对对抗性老虎机和专家建议老虎机的新差分隐私算法，提升了性能并证明了中心与局部差分隐私的区别。

Motivation: 研究差分隐私在老虎机问题中的应用，特别是对抗性老虎机和专家建议老虎机的性能提升，填补了现有算法的不足。

Method: 设计了一种简单高效的通用转换方法，将非隐私老虎机算法转换为隐私算法，并实例化现有算法以优化后悔上界。

Result: 对抗性老虎机的后悔上界提升至$O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$，专家建议老虎机首次实现差分隐私，且在不同参数组合下均实现亚线性后悔。

Conclusion: 新算法显著提升了性能，首次证明中心与局部差分隐私在老虎机问题中的差异，并扩展了差分隐私在复杂场景的应用。

Abstract: We design new differentially private algorithms for the problems of
adversarial bandits and bandits with expert advice. For adversarial bandits, we
give a simple and efficient conversion of any non-private bandit algorithm to a
private bandit algorithm. Instantiating our conversion with existing
non-private bandit algorithms gives a regret upper bound of
$O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$, improving upon the existing
upper bound $O\left(\frac{\sqrt{KT \log(KT)}}{\epsilon}\right)$ for all
$\epsilon \leq 1$. In particular, our algorithms allow for sublinear expected
regret even when $\epsilon \leq \frac{1}{\sqrt{T}}$, establishing the first
known separation between central and local differential privacy for this
problem. For bandits with expert advice, we give the first differentially
private algorithms, with expected regret
$O\left(\frac{\sqrt{NT}}{\sqrt{\epsilon}}\right),
O\left(\frac{\sqrt{KT\log(N)}\log(KT)}{\epsilon}\right)$, and
$\tilde{O}\left(\frac{N^{1/6}K^{1/2}T^{2/3}\log(NT)}{\epsilon ^{1/3}} +
\frac{N^{1/2}\log(NT)}{\epsilon}\right)$, where $K$ and $N$ are the number of
actions and experts respectively. These rates allow us to get sublinear regret
for different combinations of small and large $K, N$ and $\epsilon.$

</details>


### [138] [Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms](https://arxiv.org/abs/2505.21792)
*Yuanzhe Peng,Jieming Bian,Lei Wang,Yin Huang,Jie Xu*

Key words: 多模态联邦学习、分类法、联邦学习范式、模态异质性、隐私保护

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种多模态联邦学习（MFL）的分类法，研究了在水平联邦学习（HFL）、垂直联邦学习（VFL）和混合联邦学习（Hybrid FL）三种主要范式下的问题、算法和挑战，并讨论了未来的研究方向。

Motivation: 多模态数据在联邦学习（FL）中带来了模态异质性、隐私异质性和通信效率等新挑战，但目前缺乏系统的分类法。本文旨在填补这一空白。

Method: 通过系统分析三种FL范式（HFL、VFL、混合FL），总结了每种范式的问题定义、代表性算法和由多模态数据引发的主要挑战。

Result: 提出了一个全面的分类法，揭示了多模态数据在不同FL范式下的独特挑战，并为未来研究提供了方向。

Conclusion: 通过建立这一分类法，本文为理解和推进MFL的发展提供了新视角。

Abstract: Multimodal Federated Learning (MFL) lies at the intersection of two pivotal
research areas: leveraging complementary information from multiple modalities
to improve downstream inference performance and enabling distributed training
to enhance efficiency and preserve privacy. Despite the growing interest in
MFL, there is currently no comprehensive taxonomy that organizes MFL through
the lens of different Federated Learning (FL) paradigms. This perspective is
important because multimodal data introduces distinct challenges across various
FL settings. These challenges, including modality heterogeneity, privacy
heterogeneity, and communication inefficiency, are fundamentally different from
those encountered in traditional unimodal or non-FL scenarios. In this paper,
we systematically examine MFL within the context of three major FL paradigms:
horizontal FL (HFL), vertical FL (VFL), and hybrid FL. For each paradigm, we
present the problem formulation, review representative training algorithms, and
highlight the most prominent challenge introduced by multimodal data in
distributed settings. We also discuss open challenges and provide insights for
future research. By establishing this taxonomy, we aim to uncover the novel
challenges posed by multimodal data from the perspective of different FL
paradigms and to offer a new lens through which to understand and advance the
development of MFL.

</details>


### [139] [From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs](https://arxiv.org/abs/2505.21800)
*Stanley Yu,Vaidehi Bulusu,Oscar Yasunaga,Clayton Lau,Cole Blondin,Sean O'Brien,Kevin Zhu,Vasu Sharma*

Key words: Large Language Models, truthfulness, concept cones, causal intervention, model generalization

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究者扩展了概念锥框架以建模LLM中的真实性，通过多维度锥体捕捉其因果行为，并提供了因果干预、跨架构泛化和行为保留的证据。

Motivation: 现有研究表明LLM在对话中常生成错误信息，且真实性仅被视为单一线方向，可能未充分反映其内部几何结构。

Method: 扩展概念锥框架至真实性领域，识别多维度锥体，并通过因果干预、跨模型泛化测试和无关行为保留验证其有效性。

Result: 发现多维度锥体可因果调控模型对事实陈述的反应，且该机制在不同LLM架构间具有泛化性，同时不干扰其他行为。

Conclusion: LLM中的真假命题由更丰富的多向结构支配，概念锥是探索抽象行为的有力工具。

Abstract: Large Language Models (LLMs) exhibit strong conversational abilities but
often generate falsehoods. Prior work suggests that the truthfulness of simple
propositions can be represented as a single linear direction in a model's
internal activations, but this may not fully capture its underlying geometry.
In this work, we extend the concept cone framework, recently introduced for
modeling refusal, to the domain of truth. We identify multi-dimensional cones
that causally mediate truth-related behavior across multiple LLM families. Our
results are supported by three lines of evidence: (i) causal interventions
reliably flip model responses to factual statements, (ii) learned cones
generalize across model architectures, and (iii) cone-based interventions
preserve unrelated model behavior. These findings reveal the richer,
multidirectional structure governing simple true/false propositions in LLMs and
highlight concept cones as a promising tool for probing abstract behaviors.

</details>


### [140] [Towards Operational Automated Greenhouse Gas Plume Detection](https://arxiv.org/abs/2505.21806)
*Brian D. Bue,Jake H. Lee,Andrew K. Thorpe,Philip G. Brodrick,Daniel Cusworth,Alana Ayasse,Vassiliki Mancoridis,Anagha Satish,Shujun Xiong,Riley Duren*

Key words: 温室气体（GHG）、深度学习、卷积神经网络（CNN）、实例检测、像素分割

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文介绍了通过深度学习（尤其是CNN）自动化检测温室气体（GHG）羽流的关键挑战及解决方案，展示了多任务模型在实例检测和像素级分割上的有效性，并提供了分析工具与行业标准。

Motivation: 尽管深度学习在温室气体（GHG）羽流检测中取得进展，但全自动系统的实际部署仍面临挑战。数据可用性增加使得自动化监测愈发重要，本研究旨在解决数据质量、时空偏差和建模目标对齐等关键问题。

Method: 采用卷积神经网络（CNN）并结合多任务学习，同时进行实例检测和像素级分割。通过多平台（机载与星载）的多源数据验证模型性能。

Result: 实验表明，解决关键障碍后，CNN可实现操作级检测性能。多任务模型能有效支持实际应用，并在不同排放源类型和区域中确定了可部署的检测阈值。

Conclusion: 研究为GHG羽流检测提供了可复现的工具和标准，推动了该领域向全自动化监测的发展。

Abstract: Operational deployment of a fully automated greenhouse gas (GHG) plume
detection system remains an elusive goal for imaging spectroscopy missions,
despite recent advances in deep learning approaches. With the dramatic increase
in data availability, however, automation continues to increase in importance
for natural and anthropogenic emissions monitoring. This work reviews and
addresses several key obstacles in the field: data and label quality control,
prevention of spatiotemporal biases, and correctly aligned modeling objectives.
We demonstrate through rigorous experiments using multicampaign data from
airborne and spaceborne instruments that convolutional neural networks (CNNs)
are able to achieve operational detection performance when these obstacles are
alleviated. We demonstrate that a multitask model that learns both instance
detection and pixelwise segmentation simultaneously can successfully lead
towards an operational pathway. We evaluate the model's plume detectability
across emission source types and regions, identifying thresholds for
operational deployment. Finally, we provide analysis-ready data, models, and
source code for reproducibility, and work to define a set of best practices and
validation standards to facilitate future contributions to the field.

</details>


### [141] [TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction](https://arxiv.org/abs/2505.21807)
*Tommy Xu,Zhitian Zhang,Xiangyu Sun,Lauren Kelly Zung,Hossein Hajimirsadeghi,Greg Mori*

Key words: 表格数据预测,大型语言模型,强化学习,可解释性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种利用强化学习训练的大型语言模型（LLMs）进行表格数据预测的方法，兼顾了准确性和可解释性。

Motivation: 现有的梯度提升机和深度学习模型在表格数据上表现良好但缺乏可解释性，而LLMs虽能生成人类可理解的推理但预测性能不足，因此需要一种兼顾两者的方法。

Method: 通过强化学习训练的LLMs，结合自定义奖励函数，既追求高预测准确性，又确保预测理由可理解。

Result: 在金融基准数据集上表现优于现有大多数LLMs。

Conclusion: 该方法在保持预测性能的同时增强了可解释性，为表格数据预测提供了新思路。

Abstract: Predictive modeling on tabular data is the cornerstone of many real-world
applications. Although gradient boosting machines and some recent deep models
achieve strong performance on tabular data, they often lack interpretability.
On the other hand, large language models (LLMs) have demonstrated powerful
capabilities to generate human-like reasoning and explanations, but remain
under-performed for tabular data prediction. In this paper, we propose a new
approach that leverages reasoning-based LLMs, trained using reinforcement
learning, to perform more accurate and explainable predictions on tabular data.
Our method introduces custom reward functions that guide the model not only
toward high prediction accuracy but also toward human-understandable reasons
for its predictions. Experimental results show that our model achieves
promising performance on financial benchmark datasets, outperforming most
existing LLMs.

</details>


### [142] [Optimizing Data Augmentation through Bayesian Model Selection](https://arxiv.org/abs/2505.21813)
*Madi Matymov,Ba-Hien Tran,Michael Kampffmeyer,Markus Heinonen,Maurizio Filippone*

Key words: 数据增强, 贝叶斯优化, 变分推断, 模型校准, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于贝叶斯原理的数据增强（DA）参数优化框架，通过将增强参数视为模型超参数，并利用证据下界（ELBO）联合优化，显著提升了模型的鲁棒性和泛化能力。

Motivation: 传统数据增强参数选择依赖试错或昂贵的验证优化，限制了其效率和效果。本文旨在通过贝叶斯方法，提供一种系统化的DA参数优化解决方案。

Method: 提出概率视角的DA框架，将增强参数建模为超参数，通过优化边际似然的变分下界（ELBO）实现联合优化。

Result: 实验证明，该方法在计算机视觉任务中提高了模型校准性，并优于固定或无增强策略。

Conclusion: 该方法为基于贝叶斯原理的DA优化奠定了理论基础，对鲁棒机器学习具有重要潜力。

Abstract: Data Augmentation (DA) has become an essential tool to improve robustness and
generalization of modern machine learning. However, when deciding on DA
strategies it is critical to choose parameters carefully, and this can be a
daunting task which is traditionally left to trial-and-error or expensive
optimization based on validation performance. In this paper, we counter these
limitations by proposing a novel framework for optimizing DA. In particular, we
take a probabilistic view of DA, which leads to the interpretation of
augmentation parameters as model (hyper)-parameters, and the optimization of
the marginal likelihood with respect to these parameters as a Bayesian model
selection problem. Due to its intractability, we derive a tractable Evidence
Lower BOund (ELBO), which allows us to optimize augmentation parameters jointly
with model parameters. We provide extensive theoretical results on variational
approximation quality, generalization guarantees, invariance properties, and
connections to empirical Bayes. Through experiments on computer vision tasks,
we show that our approach improves calibration and yields robust performance
over fixed or no augmentation. Our work provides a rigorous foundation for
optimizing DA through Bayesian principles with significant potential for robust
machine learning.

</details>


### [143] [Unsupervised Latent Pattern Analysis for Estimating Type 2 Diabetes Risk in Undiagnosed Populations](https://arxiv.org/abs/2505.21824)
*Praveen Kumar,Vincent T. Metzger,Scott A. Malec*

Key words: 2型糖尿病，无监督学习，非负矩阵分解，共病模式，风险评估

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种结合非负矩阵分解（NMF）与统计技术的无监督学习框架，用于识别2型糖尿病（T2DM）高风险人群，解决了监督学习中缺乏阴性样本的问题。

Motivation: 全球2型糖尿病（T2DM）患病率急剧上升，带来巨大的健康和经济负担。现有机器学习方法多依赖监督学习，但缺少已确认的阴性病例限制了其效果。

Method: 采用非负矩阵分解（NMF）结合统计技术，利用共病和用药数据提取潜在模式，并应用于未诊断个体的风险评估。

Result: 提出的方法通过多病共存和多重用药的潜在模式，为未诊断个体提供可解释的T2DM风险预测，支持早期干预。

Conclusion: 该无监督框架可扩展且可解释，有助于医疗提供者实施及时干预，改善患者预后并降低T2DM的未来负担。

Abstract: The global prevalence of diabetes, particularly type 2 diabetes mellitus
(T2DM), is rapidly increasing, posing significant health and economic
challenges. T2DM not only disrupts blood glucose regulation but also damages
vital organs such as the heart, kidneys, eyes, nerves, and blood vessels,
leading to substantial morbidity and mortality. In the US alone, the economic
burden of diagnosed diabetes exceeded \$400 billion in 2022. Early detection of
individuals at risk is critical to mitigating these impacts. While machine
learning approaches for T2DM prediction are increasingly adopted, many rely on
supervised learning, which is often limited by the lack of confirmed negative
cases. To address this limitation, we propose a novel unsupervised framework
that integrates Non-negative Matrix Factorization (NMF) with statistical
techniques to identify individuals at risk of developing T2DM. Our method
identifies latent patterns of multimorbidity and polypharmacy among diagnosed
T2DM patients and applies these patterns to estimate the T2DM risk in
undiagnosed individuals. By leveraging data-driven insights from comorbidity
and medication usage, our approach provides an interpretable and scalable
solution that can assist healthcare providers in implementing timely
interventions, ultimately improving patient outcomes and potentially reducing
the future health and economic burden of T2DM.

</details>


### [144] [Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones](https://arxiv.org/abs/2505.21825)
*Parsa Mirtaheri,Ezra Edelman,Samy Jelassi,Eran Malach,Enric Boix-Adsera*

Key words: 推理时计算，图连通性，顺序扩展，并行扩展，语言模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究探讨了推理时计算分配中顺序扩展（如更长的思维链）与并行扩展（如多数投票）的优势，发现某些图连通性问题设置下顺序扩展具有指数级优势。

Motivation: 尽管推理时计算能提升大语言模型的推理性能，但其最优分配方式尚不明确，尤其是顺序扩展与并行扩展的选择。

Method: 通过图连通性问题的实验，比较顺序扩展与并行扩展的效果，覆盖多种语言模型及定制化训练的模型。

Result: 在特定图分布设置下，顺序扩展表现显著优于并行扩展，实验验证了理论发现。

Conclusion: 推理时计算的最优分配取决于任务特性，顺序扩展在部分场景下效率更高。

Abstract: Inference-time computation has emerged as a promising scaling axis for
improving large language model reasoning. However, despite yielding impressive
performance, the optimal allocation of inference-time computation remains
poorly understood. A central question is whether to prioritize sequential
scaling (e.g., longer chains of thought) or parallel scaling (e.g., majority
voting across multiple short chains of thought). In this work, we seek to
illuminate the landscape of test-time scaling by demonstrating the existence of
reasoning settings where sequential scaling offers an exponential advantage
over parallel scaling. These settings are based on graph connectivity problems
in challenging distributions of graphs. We validate our theoretical findings
with comprehensive experiments across a range of language models, including
models trained from scratch for graph connectivity with different chain of
thought strategies as well as large reasoning models.

</details>


### [145] [In Search of Adam's Secret Sauce](https://arxiv.org/abs/2505.21829)
*Antonio Orvieto,Robert Gower*

Key words: Adam, optimization, transformer, language model, momentum

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文通过大量实验比较了Adam优化器及其简化变体，发现约束Adam动量参数相等可以在保持接近最优性能的同时提供新的理论见解。

Motivation: 研究Adam在训练基于Transformer的语言模型时的卓越效果，并探索其简化变体的性能。

Method: 通过训练超过1,300个语言模型，对比Adam及其简化变体（如带符号动量的方法）的性能。

Result: 带符号动量的方法速度优于SGD，但性能始终不如Adam。约束Adam动量参数相等可保持接近最优性能，并提供新理论解释。

Conclusion: 约束Adam动量参数相等不仅性能稳健，还揭示了Adam优于带符号动量的关键所在，并从统计角度给出了解释。

Abstract: Understanding the remarkable efficacy of Adam when training transformer-based
language models has become a central research topic within the optimization
community. To gain deeper insights, several simplifications of Adam have been
proposed, such as the signed gradient and signed momentum methods. In this
work, we conduct an extensive empirical study - training over 1,300 language
models across different data configurations and scales - comparing Adam to
several known simplified variants. We find that signed momentum methods are
faster than SGD, but consistently underperform relative to Adam, even after
careful tuning of momentum, clipping setting and learning rates. However, our
analysis reveals a compelling option that preserves near-optimal performance
while allowing for new insightful reformulations: constraining the Adam
momentum parameters to be equal. Beyond robust performance, this choice affords
new theoretical insights, highlights the "secret sauce" on top of signed
momentum, and grants a precise statistical interpretation: we show that Adam in
this setting implements a natural online algorithm for estimating the mean and
variance of gradients-one that arises from a mean-field Gaussian variational
inference perspective.

</details>


### [146] [TuneComp: Joint Fine-tuning and Compression for Large Foundation Models](https://arxiv.org/abs/2505.21835)
*Xiangyu Chen,Jing Liu,Ye Wang,Matthew Brand,Pu,Wang,Toshiaki Koike-Akino*

Key words: 模型压缩, 知识蒸馏, 低秩近似, 剪枝, 联合微调

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种联合微调与压缩的方法，直接构建小型模型，优于传统的分步压缩方法。

Motivation: 传统方法在微调后分步压缩模型性能损失大且产生不必要的大模型中间步骤，研究旨在减少这一差距。

Method: 联合微调与压缩，逐步蒸馏至剪枝的低秩结构。

Result: 实验表明，联合方法显著优于其他分步压缩方法。

Conclusion: 联合微调与压缩能高效减少模型大小且保持性能。

Abstract: To reduce model size during post-training, compression methods, including
knowledge distillation, low-rank approximation, and pruning, are often applied
after fine-tuning the model. However, sequential fine-tuning and compression
sacrifices performance, while creating a larger than necessary model as an
intermediate step. In this work, we aim to reduce this gap, by directly
constructing a smaller model while guided by the downstream task. We propose to
jointly fine-tune and compress the model by gradually distilling it to a pruned
low-rank structure. Experiments demonstrate that joint fine-tuning and
compression significantly outperforms other sequential compression methods.

</details>


### [147] [An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints](https://arxiv.org/abs/2505.21841)
*Jiahui Zhu,Kihyun Yu,Dabeen Lee,Xin Liu,Honghao Wei*

Key words: 在线安全强化学习、约束马尔可夫决策过程（CMDP）、对抗性约束、OMDPD算法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: OMDPD算法首次解决了在线约束马尔可夫决策过程（CMDP）中对抗性约束问题，实现了最优后悔和强约束违反边界，无需依赖严格已知的安全策略或Slater条件。

Motivation: 现有方法在随机约束下表现良好，但在对抗性约束环境中存在不足，因此需要一种新算法来处理未知、时变和对抗性设计的约束。

Method: 提出了乐观镜像下降原对偶（OMDPD）算法，无需依赖Slater条件或严格已知的安全策略。

Result: OMDPD实现了最优后悔O(√K)和强约束违反O(√K)，并且在奖励和转移估计准确时能进一步改进边界。

Conclusion: 该算法为对抗性环境中的安全决策提供了实用保证。

Abstract: Online safe reinforcement learning (RL) plays a key role in dynamic
environments, with applications in autonomous driving, robotics, and
cybersecurity. The objective is to learn optimal policies that maximize rewards
while satisfying safety constraints modeled by constrained Markov decision
processes (CMDPs). Existing methods achieve sublinear regret under stochastic
constraints but often fail in adversarial settings, where constraints are
unknown, time-varying, and potentially adversarially designed. In this paper,
we propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the
first to address online CMDPs with anytime adversarial constraints. OMDPD
achieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K))
without relying on Slater's condition or the existence of a strictly known safe
policy. We further show that access to accurate estimates of rewards and
transitions can further improve these bounds. Our results offer practical
guarantees for safe decision-making in adversarial environments.

</details>


### [148] [A Provable Approach for End-to-End Safe Reinforcement Learning](https://arxiv.org/abs/2505.21852)
*Akifumi Wachi,Kohei Miyaguchi,Takumi Tanabe,Rei Sato,Youhei Akimoto*

Key words: 安全强化学习，终身安全，离线学习，高斯过程，策略部署

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种名为'可证明终身安全强化学习（PLS）'的方法，通过结合离线安全强化学习与安全策略部署，确保策略从学习到运行的整个生命周期内的安全性。

Motivation: 现有的安全强化学习方法难以确保策略从学习到运行的整个过程中的安全性。本文旨在解决这一挑战，提出一种能够保证策略终身安全的方法。

Method: PLS方法通过离线安全强化学习（基于回报条件的监督学习）学习策略，并在部署时使用高斯过程（GPs）谨慎优化目标回报参数。

Result: 理论分析表明，PLS能在高概率下保证安全性并找到接近最优的目标回报。实证结果显示，PLS在安全性和奖励表现上均优于基准方法。

Conclusion: PLS方法实现了在保证策略终身安全的同时获得高奖励的长期目标。

Abstract: A longstanding goal in safe reinforcement learning (RL) is a method to ensure
the safety of a policy throughout the entire process, from learning to
operation. However, existing safe RL paradigms inherently struggle to achieve
this objective. We propose a method, called Provably Lifetime Safe RL (PLS),
that integrates offline safe RL with safe policy deployment to address this
challenge. Our proposed method learns a policy offline using return-conditioned
supervised learning and then deploys the resulting policy while cautiously
optimizing a limited set of parameters, known as target returns, using Gaussian
processes (GPs). Theoretically, we justify the use of GPs by analyzing the
mathematical relationship between target and actual returns. We then prove that
PLS finds near-optimal target returns while guaranteeing safety with high
probability. Empirically, we demonstrate that PLS outperforms baselines both in
safety and reward performance, thereby achieving the longstanding goal to
obtain high rewards while ensuring the safety of a policy throughout the
lifetime from learning to operation.

</details>


### [149] [Revisiting Bayesian Model Averaging in the Era of Foundation Models](https://arxiv.org/abs/2505.21857)
*Mijung Park*

Key words: 贝叶斯模型平均, 基础模型, 模型集成, 线性分类器, 优化模型平均

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文重新审视了经典贝叶斯模型平均（BMA）范式，通过集成预训练或微调的基础模型提升图像和文本数据的分类性能。为了实现BMA在基础模型下的高效运行，引入了可训练的线性分类器，并通过模型后验概率优化模型集成权重。此外，提出了一种计算成本更低的优化模型平均方案（OMA）。

Motivation: 随着基础模型的快速发展，如何高效集成预训练或微调的模型以提升分类性能成为一个重要问题。

Method: 提出两种方法：1）基于贝叶斯模型平均（BMA）的集成方法，使用可训练的线性分类器处理冻结特征；2）优化模型平均方案（OMA），直接优化模型集成权重以减少预测的不确定性。

Result: 提出的方法能够高效集成基础模型，提升分类任务的性能。

Conclusion: 这些方法为未来更好基础模型的集成提供了可行方案，进一步提升了分类任务的性能。

Abstract: We revisit the classical, full-fledged Bayesian model averaging (BMA)
paradigm to ensemble pre-trained and/or lightly-finetuned foundation models to
enhance the classification performance on image and text data. To make BMA
tractable under foundation models, we introduce trainable linear classifiers
that take frozen features from the pre-trained foundation models as inputs. The
model posteriors over the linear classifiers tell us which linear heads and
frozen features are better suited for a given dataset, resulting in a
principled model ensembling method. Furthermore, we propose a computationally
cheaper, optimizable model averaging scheme (OMA). In OMA, we directly optimize
the model ensemble weights, just like those weights based on model posterior
distributions in BMA, by reducing the amount of surprise (expected entropy of
the predictions) we get from predictions of ensembled models. With the rapid
development of foundation models, these approaches will enable the
incorporation of future, possibly significantly better foundation models to
enhance the performance of challenging classification tasks.

</details>


### [150] [Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation in Federated Learning](https://arxiv.org/abs/2505.21877)
*Hongyao Chen,Tianyang Xu,Xiaojun Wu,Josef Kittler*

Key words: 联邦学习, 批标准化, 混合批标准化, 非独立同分布数据

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种针对联邦学习的混合批标准化方法（HBN），解决了传统批标准化（BN）在非独立同分布数据下的性能下降问题。

Motivation: 联邦学习中，由于客户端数据的非独立同分布特性，传统BN的统计参数更新方法失效，导致性能下降，亟需一种替代方案。

Method: HBN将统计参数（均值和方差）与可学习参数分开更新，并通过引入可学习的混合分布因子，使各节点能自适应混合当前批次与全局统计参数。

Result: HBN在多种联邦学习场景（尤其是小批次和非同分布数据）中表现优异，显著提升了性能。

Conclusion: HBN作为一种高效插件，能够增强联邦学习的性能，尤其在数据异质性和小批次场景下效果突出。

Abstract: Batch Normalisation (BN) is widely used in conventional deep neural network
training to harmonise the input-output distributions for each batch of data.
However, federated learning, a distributed learning paradigm, faces the
challenge of dealing with non-independent and identically distributed data
among the client nodes. Due to the lack of a coherent methodology for updating
BN statistical parameters, standard BN degrades the federated learning
performance. To this end, it is urgent to explore an alternative normalisation
solution for federated learning. In this work, we resolve the dilemma of the BN
layer in federated learning by developing a customised normalisation approach,
Hybrid Batch Normalisation (HBN). HBN separates the update of statistical
parameters (i.e. , means and variances used for evaluation) from that of
learnable parameters (i.e. , parameters that require gradient updates),
obtaining unbiased estimates of global statistical parameters in distributed
scenarios. In contrast with the existing solutions, we emphasise the supportive
power of global statistics for federated learning. The HBN layer introduces a
learnable hybrid distribution factor, allowing each computing node to
adaptively mix the statistical parameters of the current batch with the global
statistics. Our HBN can serve as a powerful plugin to advance federated
learning performance. It reflects promising merits across a wide range of
federated learning settings, especially for small batch sizes and heterogeneous
data.

</details>


### [151] [HydraNet: Momentum-Driven State Space Duality for Multi-Granularity Tennis Tournaments Analysis](https://arxiv.org/abs/2505.21882)
*Ruijie Li,Xiang Zhao,Qiao Ning,Shikai Guo*

Key words: 动量评分, HydraNet, 状态空间对偶, 多粒度分析, 网球比赛

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为Momentum Score (MS)的新指标，通过HydraNet框架量化网球比赛中多粒度的运动员表现动量，结合32个维度的数据，并验证了其有效性。

Motivation: 动量在网球比赛中至关重要，但现有研究在多粒度建模方面存在不足，论文旨在填补这一空白。

Method: 设计了HydraNet框架，结合状态空间对偶性和滑动窗口机制，提出Versus Learning方法和CAAM机制，构建百万级数据集进行验证。

Result: 实验证明MS指标能有效分析动量在不同粒度对比赛结果的影响，为动量建模和体育分析提供了新基础。

Conclusion: HydraNet框架和MS指标为动量建模提供了新视角，数据集和代码已开源。

Abstract: In tennis tournaments, momentum, a critical yet elusive phenomenon, reflects
the dynamic shifts in performance of athletes that can decisively influence
match outcomes. Despite its significance, momentum in terms of effective
modeling and multi-granularity analysis across points, games, sets, and matches
in tennis tournaments remains underexplored. In this study, we define a novel
Momentum Score (MS) metric to quantify a player's momentum level in
multi-granularity tennis tournaments, and design HydraNet, a momentum-driven
state-space duality-based framework, to model MS by integrating thirty-two
heterogeneous dimensions of athletes performance in serve, return, psychology
and fatigue. HydraNet integrates a Hydra module, which builds upon a
state-space duality (SSD) framework, capturing explicit momentum with a
sliding-window mechanism and implicit momentum through cross-game state
propagation. It also introduces a novel Versus Learning method to better
enhance the adversarial nature of momentum between the two athletes at a macro
level, along with a Collaborative-Adversarial Attention Mechanism (CAAM) for
capturing and integrating intra-player and inter-player dynamic momentum at a
micro level. Additionally, we construct a million-level tennis cross-tournament
dataset spanning from 2012-2023 Wimbledon and 2013-2023 US Open, and validate
the multi-granularity modeling capability of HydraNet for the MS metric on this
dataset. Extensive experimental evaluations demonstrate that the MS metric
constructed by the HydraNet framework provides actionable insights into how
momentum impacts outcomes at different granularities, establishing a new
foundation for momentum modeling and sports analysis. To the best of our
knowledge. The source code and datasets are available at
https://github.com/ReyJerry/HydraNet.

</details>


### [152] [SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training](https://arxiv.org/abs/2505.21893)
*Xiaomeng Yang,Zhiyu Tan,Junyan Wang,Zhijian Zhou,Hao Li*

Key words: 偏好学习,扩散模型,DPO-C&M,SDPO,重要性采样

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了两种改进扩散模型偏好学习的方法：DPO-C&M通过裁剪和掩码不稳定时间步提高稳定性，SDPO则引入重要性采样以完全校正离策略偏差。实验证明两者均优于标准Diffusion-DPO。

Motivation: 现有扩散模型偏好学习方法（如Diffusion-DPO）存在时间步依赖的不稳定性和离策略偏差问题，需改进以提升模型与人类期望的对齐效果。

Method: 1. DPO-C&M：裁剪和掩码低权重时间步以提升稳定性；2. SDPO：在目标函数中引入重要性采样，校正偏差并优化信息性更新。

Result: 在CogVideoX-2B等模型上，SDPO表现出更高的VBench得分、人类偏好对齐及训练鲁棒性，优于Diffusion-DPO。

Conclusion: 时间步感知和分布校正的优化对扩散模型偏好学习至关重要，SDPO和DPO-C&M为有效解决方案。

Abstract: Preference learning has become a central technique for aligning generative
models with human expectations. Recently, it has been extended to diffusion
models through methods like Direct Preference Optimization (DPO). However,
existing approaches such as Diffusion-DPO suffer from two key challenges:
timestep-dependent instability, caused by a mismatch between the reverse and
forward diffusion processes and by high gradient variance in early noisy
timesteps, and off-policy bias arising from the mismatch between optimization
and data collection policies. We begin by analyzing the reverse diffusion
trajectory and observe that instability primarily occurs at early timesteps
with low importance weights. To address these issues, we first propose
DPO-C\&M, a practical strategy that improves stability by clipping and masking
uninformative timesteps while partially mitigating off-policy bias. Building on
this, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a
principled framework that incorporates importance sampling into the objective
to fully correct for off-policy bias and emphasize informative updates during
the diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and
Wan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO,
with SDPO achieving superior VBench scores, human preference alignment, and
training robustness. These results highlight the importance of timestep-aware,
distribution-corrected optimization in diffusion-based preference learning.

</details>


### [153] [Compressing Sine-Activated Low-Rank Adapters through Post-Training Quantization](https://arxiv.org/abs/2505.21895)
*Cameron Gordon,Yiping Ji,Hemanth Saratchandran,Paul Albert,Simon Lucey*

Key words: LoRA, 量化, 正弦变换, 参数高效微调, 稳定秩

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了在量化后的LoRA适配器中应用正弦变换技术，以保持性能同时减少内存使用。

Motivation: LoRA的低秩约束限制了表示能力，导致性能下降。通过正弦变换提升稳定秩，能否在量化后依然有效是研究重点。

Method: 扩展正弦变换框架至量化LoRA适配器，理论分析量化后稳定秩与全精度版本的关系。

Result: 实验证明量化后仍能保持表达性增益，实现高压缩适配器且性能损失极小。

Conclusion: 正弦变换技术适用于量化LoRA适配器，显著节省内存并保持竞争力。

Abstract: Low-Rank Adaptation (LoRA) has become a standard approach for
parameter-efficient fine-tuning, offering substantial reductions in trainable
parameters by modeling updates as the product of two low-rank matrices. While
effective, the low-rank constraint inherently limits representational capacity,
often resulting in reduced performance compared to full-rank fine-tuning.
Recent work by Ji et al. (2025) has addressed this limitation by applying a
fixed-frequency sinusoidal transformation to low-rank adapters, increasing
their stable rank without introducing additional parameters. This raises a
crucial question: can the same sine-activated technique be successfully applied
within the context of Post-Training Quantization to retain benefits even after
model compression? In this paper, we investigate this question by extending the
sinusoidal transformation framework to quantized LoRA adapters. We develop a
theoretical analysis showing that the stable rank of a quantized adapter is
tightly linked to that of its full-precision counterpart, motivating the use of
such rank-enhancing functions even under quantization. Our results demonstrate
that the expressivity gains from a sinusoidal non-linearity persist after
quantization, yielding highly compressed adapters with negligible loss in
performance. We validate our approach across a range of fine-tuning tasks for
language, vision and text-to-image generation achieving significant memory
savings while maintaining competitive accuracy.

</details>


### [154] [Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding](https://arxiv.org/abs/2505.21908)
*Hanyin Wang,Zhenbang Wu,Gururaj Kolar,Hariprasad Korsapati,Brian Bartlett,Bryan Hull,Jimeng Sun*

Key words: DRG编码、强化学习、OOD任务、Qwen2.5-7B、GRPO

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: DRG-Sapphire利用强化学习自动分配DRG代码，在MIMIC-IV基准上达到最优准确率，并通过医生验证的解释增强可理解性。研究发现RL性能与SFT样本数量呈对数线性关系，表明知识密集型OOD任务中，SFT扩展比单纯RL扩展更有效。

Motivation: 传统DRG编码依赖人工且耗时，LLM因训练数据缺乏临床/账单信息而表现不佳，需开发适应OOD任务的自动化解决方案。

Method: 基于Qwen2.5-7B，采用Group Relative Policy Optimization（GRPO）和规则奖励进行强化学习，结合领域特定优化。

Result: 在MIMIC-IV上实现SOTA准确率，生成医生认可的解释；发现RL性能与SFT样本对数线性相关。

Conclusion: 知识密集型OOD任务中，RL需依赖基模型的知识注入，扩展SFT比单纯RL更高效。

Abstract: Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement
and operations but require labor-intensive assignment. Large Language Models
(LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of
the task: pretraining corpora rarely contain private clinical or billing data.
We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL)
for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained
with Group Relative Policy Optimization (GRPO) using rule-based rewards,
DRG-Sapphire introduces a series of RL enhancements to address domain-specific
challenges not seen in previous mathematical tasks. Our model achieves
state-of-the-art accuracy on the MIMIC-IV benchmark and generates
physician-validated reasoning for DRG assignments, significantly enhancing
explainability. Our study further sheds light on broader challenges of applying
RL to knowledge-intensive, OOD tasks. We observe that RL performance scales
approximately linearly with the logarithm of the number of supervised
fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally
constrained by the domain knowledge encoded in the base model. For OOD tasks
like DRG coding, strong RL performance requires sufficient knowledge infusion
prior to RL. Consequently, scaling SFT may be more effective and
computationally efficient than scaling RL alone for such tasks.

</details>


### [155] [Taming Transformer Without Using Learning Rate Warmup](https://arxiv.org/abs/2505.21910)
*Xianbiao Qi,Yelin He,Jiaquan Ye,Chun-Guang Li,Bojia Zi,Xili Dai,Qin Zou,Rong Xiao*

Key words: Transformer, 谱能量集中, 熵崩溃, Weyl不等式, 优化策略

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新的优化策略，通过平滑权重更新步骤来防止Transformer训练中的谱能量集中，从而避免恶性熵崩溃，实验验证了该策略在不使用学习率预热的情况下能稳定训练ViT、Swin-Transformer和GPT。

Motivation: 研究动机是解决Transformer在大规模训练中因谱能量集中导致的恶性熵崩溃问题，避免模型崩溃，同时摆脱对学习率预热等技巧的依赖。

Method: 方法基于Weyl不等式，提出了一种优化策略：如果权重更新的梯度与上一步权重的奇异值比超过阈值，则自动限制学习率为其加权倒数，以防止谱能量过度集中。

Result: 通过在ViT、Swin-Transformer和GPT上的实验证明，该策略能有效稳定训练，无需学习率预热。

Conclusion: 结论是提出的优化策略能避免谱能量集中和熵崩溃，稳定训练Transformer模型。

Abstract: Scaling Transformer to a large scale without using some technical tricks such
as learning rate warump and using an obviously lower learning rate is an
extremely challenging task, and is increasingly gaining more attention. In this
paper, we provide a theoretical analysis for the process of training
Transformer and reveal the rationale behind the model crash phenomenon in the
training process, termed \textit{spectral energy concentration} of
${\bW_q}^{\top} \bW_k$, which is the reason for a malignant entropy collapse,
where ${\bW_q}$ and $\bW_k$ are the projection matrices for the query and the
key in Transformer, respectively. To remedy this problem, motivated by
\textit{Weyl's Inequality}, we present a novel optimization strategy, \ie,
making the weight updating in successive steps smooth -- if the ratio
$\frac{\sigma_{1}(\nabla \bW_t)}{\sigma_{1}(\bW_{t-1})}$ is larger than a
threshold, we will automatically bound the learning rate to a weighted multiple
of $\frac{\sigma_{1}(\bW_{t-1})}{\sigma_{1}(\nabla \bW_t)}$, where $\nabla
\bW_t$ is the updating quantity in step $t$. Such an optimization strategy can
prevent spectral energy concentration to only a few directions, and thus can
avoid malignant entropy collapse which will trigger the model crash. We conduct
extensive experiments using ViT, Swin-Transformer and GPT, showing that our
optimization strategy can effectively and stably train these Transformers
without using learning rate warmup.

</details>


### [156] [Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing](https://arxiv.org/abs/2505.21918)
*Haruki Kai,Tsuyoshi Okita*

Key words: 深度学习, 人类活动识别, Transformer, n维数值处理, 传感器信号

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 摘要提出了一个基于Transformer架构的深度学习算法，用于通过传感器信号进行人类活动识别，并通过改进的n维数值处理Transformer在五个数据集上实现了10%-15%的准确率提升。

Motivation: 利用预训练语言模型提升人类活动识别的性能，解决传统Transformer在该任务中的局限性。

Method: 提出了一种改进的n维数值处理Transformer，包含线性层嵌入、基于分箱的预处理和输出层的线性变换三个关键特征。

Result: 在五个数据集上验证了模型的有效性，相较于传统Transformer，准确率提高了10%-15%。

Conclusion: 改进的n维数值处理Transformer显著提升了人类活动识别的性能，为相关任务提供了有效解决方案。

Abstract: We developed a deep learning algorithm for human activity recognition using
sensor signals as input. In this study, we built a pretrained language model
based on the Transformer architecture, which is widely used in natural language
processing. By leveraging this pretrained model, we aimed to improve
performance on the downstream task of human activity recognition. While this
task can be addressed using a vanilla Transformer, we propose an enhanced
n-dimensional numerical processing Transformer that incorporates three key
features: embedding n-dimensional numerical data through a linear layer,
binning-based pre-processing, and a linear transformation in the output layer.
We evaluated the effectiveness of our proposed model across five different
datasets. Compared to the vanilla Transformer, our model demonstrated 10%-15%
improvements in accuracy.

</details>


### [157] [FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design](https://arxiv.org/abs/2505.21923)
*Asal Mehradfar,Xuzhe Zhao,Yilun Huang,Emir Ceyani,Yankai Yang,Shihao Han,Hamidreza Aghasi,Salman Avestimehr*

Key words: 模拟电路设计, 机器学习, 拓扑选择, 布局优化, 图神经网络

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: FALCON是一种统一的机器学习框架，用于自动化模拟电路设计，包括拓扑选择和布局约束优化，性能高且速度快。

Motivation: 模拟电路设计是一个复杂多阶段过程，FALCON旨在通过机器学习实现自动化设计，提高效率和准确性。

Method: FALCON结合性能驱动分类器和图神经网络，实现拓扑选择和梯度参数推断，同时考虑布局成本和设计规则约束。

Result: FALCON在拓扑推断准确率>99％，性能预测相对误差<10％，且每个设计实例完成时间低于1秒。

Conclusion: FALCON是一种实用且可扩展的端到端模拟电路设计自动化基础模型。

Abstract: Designing analog circuits from performance specifications is a complex,
multi-stage process encompassing topology selection, parameter inference, and
layout feasibility. We introduce FALCON, a unified machine learning framework
that enables fully automated, specification-driven analog circuit synthesis
through topology selection and layout-constrained optimization. Given a target
performance, FALCON first selects an appropriate circuit topology using a
performance-driven classifier guided by human design heuristics. Next, it
employs a custom, edge-centric graph neural network trained to map circuit
topology and parameters to performance, enabling gradient-based parameter
inference through the learned forward model. This inference is guided by a
differentiable layout cost, derived from analytical equations capturing
parasitic and frequency-dependent effects, and constrained by design rules. We
train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave
circuits, generated and simulated using Cadence Spectre across 20
expert-designed topologies. Through this evaluation, FALCON demonstrates >99\%
accuracy in topology inference, <10\% relative error in performance prediction,
and efficient layout-aware design that completes in under 1 second per
instance. Together, these results position FALCON as a practical and extensible
foundation model for end-to-end analog circuit design automation.

</details>


### [158] [Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets](https://arxiv.org/abs/2505.21930)
*Dongyue Li,Ziniu Zhang,Lu Wang,Hongyang R. Zhang*

Key words: 语言模型, 微调, 多数据集, 集成方法, 低秩适配

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种用于微调语言模型到多个数据集的集成方法，通过分区和加权组合适配器，显著提高了效率与性能。

Motivation: 现有方法（如QLoRA）在适应单个数据集时高效，但在多任务数据集上的高效适应仍不明确。因此，作者提出使用多个小适配器的集成方案来解决这一问题。

Method: 算法将n个数据集分区为m组（m远小于n），每组训练一个适配器后加权组合。利用低秩适配的一阶近似性质，通过基模型梯度快速估计微调性能。

Result: 在340亿参数模型上，误差小于1%，真实微调性能估计误差低于5%，计算速度比基微调快105倍。在Llama和GPT模型上，平均测试准确率比QLoRA高10%，FLOPs仅增加9%。对于340亿参数的Llama模型，集成QLoRA测试准确率提高3%，FLOPs增加8%。

Conclusion: 该方法在多任务数据集上实现了高效的微调，且在计算效率和模型性能上均有显著提升。

Abstract: This paper develops an ensemble method for fine-tuning a language model to
multiple datasets. Existing methods, such as quantized LoRA (QLoRA), are
efficient when adapting to a single dataset. When training on multiple datasets
of different tasks, a common setup in practice, it remains unclear how to
design an efficient adaptation for fine-tuning language models. We propose to
use an ensemble of multiple smaller adapters instead of a single adapter per
task. We design an efficient algorithm that partitions $n$ datasets into $m$
groups, where $m$ is typically much smaller than $n$ in practice, and train one
adapter for each group before taking a weighted combination to form the
ensemble. The algorithm leverages a first-order approximation property of
low-rank adaptation to quickly obtain the fine-tuning performances of dataset
combinations since methods like LoRA stay close to the base model. Hence, we
use the gradients of the base model to estimate its behavior during
fine-tuning. Empirically, this approximation holds with less than $1\%$ error
on models with up to $34$ billion parameters, leading to an estimation of true
fine-tuning performances under $5\%$ error while speeding up computation
compared to base fine-tuning by $105$ times. When applied to fine-tune Llama
and GPT models on ten text classification tasks, our approach provides up to
$10\%$ higher average test accuracy over QLoRA, with only $9\%$ more FLOPs. On
a Llama model with $34$ billion parameters, an ensemble of QLoRA increases test
accuracy by $3\%$ compared to QLoRA, with only $8\%$ more FLOPs.

</details>


### [159] [Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection](https://arxiv.org/abs/2505.21938)
*Qirun Zeng,Eric He,Richard Hoffmann,Xuchuang Wang,Jinhang Zuo*

Key words: 对抗攻击, 随机多臂赌博机, 虚假数据注入, UCB, Thompson Sampling

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种更真实的对抗攻击模型——虚假数据注入，针对随机多臂赌博机算法，通过有限次数的有界虚假反馈样本注入，有效误导UCB和Thompson Sampling算法，同时攻击成本仅为次线性。

Motivation: 现有对抗攻击模型依赖不现实的假设（如每轮奖励操纵和无界扰动），限制了其在实际系统中的适用性。为此，提出更贴近现实的威胁模型，考虑攻击者的实际约束（如注入数据的数量、幅度和时间限制）。

Method: 设计了虚假数据注入攻击策略，明确考虑奖励幅度的约束和数据注入的时间、频率限制，并在理论上分析了攻击效果和成本。

Result: 理论分析表明，攻击能够使UCB和Thompson Sampling算法在几乎所有轮次中选择目标臂，且攻击成本为次线性。实验在合成和真实数据集上验证了策略的有效性。

Conclusion: 随机多臂赌博机算法在实际对抗场景下存在显著脆弱性，虚假数据注入是一种高效且实用的攻击方式。

Abstract: Adversarial attacks on stochastic bandits have traditionally relied on some
unrealistic assumptions, such as per-round reward manipulation and unbounded
perturbations, limiting their relevance to real-world systems. We propose a
more practical threat model, Fake Data Injection, which reflects realistic
adversarial constraints: the attacker can inject only a limited number of
bounded fake feedback samples into the learner's history, simulating legitimate
interactions. We design efficient attack strategies under this model,
explicitly addressing both magnitude constraints (on reward values) and
temporal constraints (on when and how often data can be injected). Our
theoretical analysis shows that these attacks can mislead both Upper Confidence
Bound (UCB) and Thompson Sampling algorithms into selecting a target arm in
nearly all rounds while incurring only sublinear attack cost. Experiments on
synthetic and real-world datasets validate the effectiveness of our strategies,
revealing significant vulnerabilities in widely used stochastic bandit
algorithms under practical adversarial scenarios.

</details>


### [160] [Continual Learning Beyond Experience Rehearsal and Full Model Surrogates](https://arxiv.org/abs/2505.21942)
*Prashant Bhat,Laurens Niesten,Elahe Arani,Bahram Zonooz*

Key words: 持续学习, 参数效率, 语义记忆, 工作记忆, 权重归一化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: SPARC是一种可扩展的持续学习方法，无需经验回放或完整模型替代，通过任务特定工作记忆和任务无关语义记忆结合，显著提高参数效率，性能优于或匹配现有方法。

Motivation: 持续学习（CL）中，新任务学习会部分或完全覆盖旧知识，现有解决方案常依赖经验回放或完整模型替代，但这些方法内存和计算开销大，限制了实际应用。

Method: 提出SPARC方法，结合任务特定工作记忆和任务无关语义记忆进行跨任务知识整合，分类层权重重新归一化以缓解任务特定偏差。

Result: SPARC仅需完整模型替代6%的参数，在Seq-TinyImageNet上表现优异，与其他基准方法性能相当，且参数效率显著。

Conclusion: SPARC是一种在严格效率约束下实用且可扩展的持续学习解决方案。

Abstract: Continual learning (CL) has remained a significant challenge for deep neural
networks as learning new tasks erases previously acquired knowledge, either
partially or completely. Existing solutions often rely on experience rehearsal
or full model surrogates to mitigate CF. While effective, these approaches
introduce substantial memory and computational overhead, limiting their
scalability and applicability in real-world scenarios. To address this, we
propose SPARC, a scalable CL approach that eliminates the need for experience
rehearsal and full-model surrogates. By effectively combining task-specific
working memories and task-agnostic semantic memory for cross-task knowledge
consolidation, SPARC results in a remarkable parameter efficiency, using only
6% of the parameters required by full-model surrogates. Despite its lightweight
design, SPARC achieves superior performance on Seq-TinyImageNet and matches
rehearsal-based methods on various CL benchmarks. Additionally, weight
re-normalization in the classification layer mitigates task-specific biases,
establishing SPARC as a practical and scalable solution for CL under stringent
efficiency constraints.

</details>


### [161] [Stochastic Primal-Dual Double Block-Coordinate for Two-way Partial AUC Maximization](https://arxiv.org/abs/2505.21944)
*Linli Zhou,Bokun Wang,My T. Thai,Tianbao Yang*

Key words: TPAUC, 二元分类, 随机优化, 原始-对偶算法, 块坐标更新

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出两种创新的随机原始-对偶双块坐标算法，用于优化在二元分类不平衡数据中关键的TPAUC指标，解决了现有方法的局限，并在理论和实验上验证了其优越性。

Motivation: 由于TPAUC在二元分类不平衡数据中的重要性，而现有的随机算法存在近似损失函数或复杂度次优的问题，论文旨在开发更高效的优化方法。

Method: 引入两种随机原始-对偶双块坐标算法，采用随机块坐标更新处理原始和对偶变量，适用于凸和非凸场景。

Result: 理论分析显示收敛速度优于现有方法，实验表明新算法在多个基准数据集上收敛更快且泛化更好。

Conclusion: 该研究推动了TPAUC优化的技术水平，为实际机器学习应用提供了实用工具。

Abstract: Two-way partial AUC (TPAUC) is a critical performance metric for binary
classification with imbalanced data, as it focuses on specific ranges of the
true positive rate (TPR) and false positive rate (FPR). However, stochastic
algorithms for TPAUC optimization remain under-explored, with existing methods
either limited to approximated TPAUC loss functions or burdened by sub-optimal
complexities. To overcome these limitations, we introduce two innovative
stochastic primal-dual double block-coordinate algorithms for TPAUC
maximization. These algorithms utilize stochastic block-coordinate updates for
both the primal and dual variables, catering to both convex and non-convex
settings. We provide theoretical convergence rate analyses, demonstrating
significant improvements over prior approaches. Our experimental results, based
on multiple benchmark datasets, validate the superior performance of our
algorithms, showcasing faster convergence and better generalization. This work
advances the state of the art in TPAUC optimization and offers practical tools
for real-world machine learning applications.

</details>


### [162] [EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles](https://arxiv.org/abs/2505.21959)
*Aakriti Agrawal,Mucong Ding,Zora Che,Chenghao Deng,Anirudh Satheesh,Bang An,Bayan Bruss,John Langford,Furong Huang*

Key words: Large Language Models, weak-to-strong generalization, token-level ensemble, supervision, human-level data

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种名为EnsemW2S的新方法，通过整合多个弱专家模型，提升其对超人类级别任务的泛化能力。作者在ID和OOD数据集上验证了该方法，结果显示专家模型和学生模型在ID和OOD数据集上均有显著的性能提升。

Motivation: 大型语言模型（LLMs）的发展迅速，但其训练和监督需要更高效的方法。本文旨在解决如何利用人类级别数据的弱专家模型来提升对更复杂任务的泛化能力。

Method: 采用token级集成策略，通过迭代组合多个弱专家模型，逐步优化模型的监督能力，最终提升对学生模型的监督效果。

Result: 在ID数据集上，专家模型和学生模型的性能分别提升了4%和3.2%；在OOD数据集上，提升高达6%和2.28%。

Conclusion: EnsemW2S能有效提升弱专家模型的泛化能力，为解决弱到强（W2S）泛化挑战提供了有效的解决方案。

Abstract: With Large Language Models (LLMs) rapidly approaching and potentially
surpassing human-level performance, it has become imperative to develop
approaches capable of effectively supervising and enhancing these powerful
models using smaller, human-level models exposed to only human-level data. We
address this critical weak-to-strong (W2S) generalization challenge by
proposing a novel method aimed at improving weak experts, by training on the
same limited human-level data, enabling them to generalize to complex,
super-human-level tasks. Our approach, called \textbf{EnsemW2S}, employs a
token-level ensemble strategy that iteratively combines multiple weak experts,
systematically addressing the shortcomings identified in preceding iterations.
By continuously refining these weak models, we significantly enhance their
collective ability to supervise stronger student models. We extensively
evaluate the generalization performance of both the ensemble of weak experts
and the subsequent strong student model across in-distribution (ID) and
out-of-distribution (OOD) datasets. For OOD, we specifically introduce question
difficulty as an additional dimension for defining distributional shifts. Our
empirical results demonstrate notable improvements, achieving 4\%, and 3.2\%
improvements on ID datasets and, upto 6\% and 2.28\% on OOD datasets for
experts and student models respectively, underscoring the effectiveness of our
proposed method in advancing W2S generalization.

</details>


### [163] [Judging LLMs on a Simplex](https://arxiv.org/abs/2505.21972)
*Patrick Vossler,Fan Xia,Yifan Mai,Jean Feng*

Key words: 大语言模型，自由形式输出，评估，贝叶斯推理，不确定性量化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文研究了使用LLMs作为评估自由形式输出时的几何框架和贝叶斯推理方法，揭示了评分系统在可识别性方面的相变现象，并提出了一种更全面的不确定性量化方法。

Motivation: 由于LLM生成的自由形式输出存在多种有效答案，自动评估具有挑战性。研究旨在理解LLM作为评估者的理论特性，并改进不确定性量化。

Method: 提出几何框架，将评估者和候选者表示为概率单纯形上的点，结合贝叶斯推理进行敏感性分析。

Result: 理论分析揭示了可识别性的相变现象，实证结果显示贝叶斯推理能提高排名准确性和覆盖率。

Conclusion: 采用贝叶斯推理能更全面地量化不确定性，提升LLM作为评估者的可靠性。

Abstract: Automated evaluation of free-form outputs from large language models (LLMs)
is challenging because many distinct answers can be equally valid. A common
practice is to use LLMs themselves as judges, but the theoretical properties of
this approach are not yet well understood. We show that a geometric framework
that represents both judges and candidates as points on a probability simplex
can provide helpful insight on what is or is not identifiable using LLM judges.
Our theoretical analysis uncovers a "phase transition" in ranking
identifiability: for binary scoring systems, true rankings are identifiable
even with weak judges under mild assumptions, while rankings become
non-identifiable for three or more scoring levels even with infinite data,
absent additional prior knowledge. This non-identifiability highlights how
uncertainty in rankings stems from not only aleatoric uncertainty (i.e.,
inherent stochasticity in the data) but also epistemic uncertainty regarding
which assumptions hold, an aspect that has received limited attention until
now. To integrate both types of uncertainty, we use Bayesian inference to
encode assumptions as priors and conduct sensitivity analysis of ranking
estimates and credible intervals. Empirical evaluations across multiple
benchmarks demonstrate that Bayesian inference yields more accurate rankings
and substantially improves coverage rates. These results underscore the
importance of taking a more holistic approach to uncertainty quantification
when using LLMs as judges.

</details>


### [164] [BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL](https://arxiv.org/abs/2505.21974)
*Yu-Heng Hung,Kai-Jie Lin,Yu-Heng Lin,Chien-YiWang,Cheng Sun,Ping-Chun Hsieh*

Key words: Bayesian optimization, multi-objective optimization, deep Q-learning, Transformer, hypervolume identifiability

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Bayesian optimization (BO) in multi-objective settings faces hypervolume identifiability issues. The paper proposes BOFormer, a deep Q-learning framework using Transformers, which outperforms existing methods in synthetic and real-world tasks.

Motivation: Address the hypervolume identifiability issue in multi-objective Bayesian optimization (MOBO) by leveraging non-Markovian RL and Transformer models.

Method: Introduces BOFormer, a generalized deep Q-learning framework for MOBO using sequence modeling with Transformers.

Result: BOFormer consistently outperforms rule-based and learning-based benchmarks in synthetic and real-world multi-objective optimization tasks.

Conclusion: BOFormer effectively addresses the non-Markovian challenges in MOBO, demonstrating superior performance in various applications.

Abstract: Bayesian optimization (BO) offers an efficient pipeline for optimizing
black-box functions with the help of a Gaussian process prior and an
acquisition function (AF). Recently, in the context of single-objective BO,
learning-based AFs witnessed promising empirical results given its favorable
non-myopic nature. Despite this, the direct extension of these approaches to
multi-objective Bayesian optimization (MOBO) suffer from the
\textit{hypervolume identifiability issue}, which results from the
non-Markovian nature of MOBO problems. To tackle this, inspired by the
non-Markovian RL literature and the success of Transformers in language
modeling, we present a generalized deep Q-learning framework and propose
\textit{BOFormer}, which substantiates this framework for MOBO via sequence
modeling. Through extensive evaluation, we demonstrate that BOFormer constantly
outperforms the benchmark rule-based and learning-based algorithms in various
synthetic MOBO and real-world multi-objective hyperparameter optimization
problems. We have made the source code publicly available to encourage further
research in this direction.

</details>


### [165] [Two-Stage Feature Generation with Transformer and Reinforcement Learning](https://arxiv.org/abs/2505.21978)
*Wanfu Gao,Zengyao Man,Zebin He,Yuhao Tang,Jun Gao,Kunpeng Liu*

Key words: 特征生成, Transformer, PPO, 机器学习, 自适应优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种两阶段特征生成（TSFG）框架，结合Transformer编码器-解码器和PPO优化方法，显著提升特征生成质量和模型性能。

Motivation: 传统特征生成方法依赖领域知识和人工干预，自动化技术又存在冗余和适应性不足的问题，因此需要更高效且自适应的解决方案。

Method: 采用Transformer编码器-解码器捕捉数据复杂关系，并结合PPO动态调整特征生成策略。

Result: 实验表明，TSFG在特征质量和适应性上优于现有方法。

Conclusion: TSFG是一种高效、自适应且性能优越的特征生成框架。

Abstract: Feature generation is a critical step in machine learning, aiming to enhance
model performance by capturing complex relationships within the data and
generating meaningful new features. Traditional feature generation methods
heavily rely on domain expertise and manual intervention, making the process
labor-intensive and challenging to adapt to different scenarios. Although
automated feature generation techniques address these issues to some extent,
they often face challenges such as feature redundancy, inefficiency in feature
space exploration, and limited adaptability to diverse datasets and tasks. To
address these problems, we propose a Two-Stage Feature Generation (TSFG)
framework, which integrates a Transformer-based encoder-decoder architecture
with Proximal Policy Optimization (PPO). The encoder-decoder model in TSFG
leverages the Transformer's self-attention mechanism to efficiently represent
and transform features, capturing complex dependencies within the data. PPO
further enhances TSFG by dynamically adjusting the feature generation strategy
based on task-specific feedback, optimizing the process for improved
performance and adaptability. TSFG dynamically generates high-quality feature
sets, significantly improving the predictive performance of machine learning
models. Experimental results demonstrate that TSFG outperforms existing
state-of-the-art methods in terms of feature quality and adaptability.

</details>


### [166] [ACE: Exploring Activation Cosine Similarity and Variance for Accurate and Calibration-Efficient LLM Pruning](https://arxiv.org/abs/2505.21987)
*Zhendong Mi,Zhenglun Kong,Geng Yuan,Shaoyi Huang*

Key words: 大语言模型, 剪枝, 激活余弦相似度, 激活方差, 效率优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种高效的大语言模型剪枝方法，通过两种创新指标提升剪枝性能和速度，实验结果显示在LLaMA等模型上困惑度降低18%，剪枝时间减少63%。

Motivation: 为了解决现有大语言模型剪枝方法在性能和效率上的不足，提出了更高效的解决方案。

Method: 结合两种新指标——激活余弦相似度损失剪枝指标和激活方差剪枝指标，以优化剪枝效果和速度。

Result: 在LLaMA等模型上，实现了困惑度降低18%，剪枝时间减少63%。

Conclusion: 该方法在提升大语言模型剪枝性能的同时显著提高了效率。

Abstract: With the rapid expansion of large language models (LLMs), the demand for
memory and computational resources has grown significantly. Recent advances in
LLM pruning aim to reduce the size and computational cost of these models.
However, existing methods often suffer from either suboptimal pruning
performance or low time efficiency during the pruning process. In this work, we
propose an efficient and effective pruning method that simultaneously achieves
high pruning performance and fast pruning speed with improved calibration
efficiency. Our approach introduces two key innovations: (1) An activation
cosine similarity loss-guided pruning metric, which considers the angular
deviation of the output activation between the dense and pruned models. (2) An
activation variance-guided pruning metric, which helps preserve semantic
distinctions in output activations after pruning, enabling effective pruning
with shorter input sequences. These two components can be readily combined to
enhance LLM pruning in both accuracy and efficiency. Experimental results show
that our method achieves up to an 18% reduction in perplexity and up to 63%
decrease in pruning time on prevalent LLMs such as LLaMA, LLaMA-2, and OPT.

</details>


### [167] [Learning in Compact Spaces with Approximately Normalized Transformers](https://arxiv.org/abs/2505.22014)
*Jörg K. H. Franke,Urs Spiegelhalter,Marianna Nezhurina,Jenia Jitsev,Frank Hutter,Michael Hefenbrock*

Key words: 归一化, 深度学习, GPT, 收敛速度, 超参数优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种近似归一化方法anTransformer，通过约束参数范数和归一化表示，提升了收敛速度并减少了超参数需求，适用于GPT训练，效果优于传统方法。

Motivation: 深度学习中正则化和归一化常用于解决过拟合等问题，但现有方法存在额外成本。论文旨在提出一种更全面的近似归一化方法以提升效率。

Method: 提出anTransformer方法，约束参数范数并通过标量乘法归一化表示，利用了高维随机向量范数的紧密集中特性。

Result: 在GPT训练中，相比QK归一化方法收敛速度提升40%，运行时仅增加3%，支持更大批次训练且超参数需求减少。

Conclusion: anTransformer方法在保持经典GPT架构优点的同时，显著提升了训练效率，具有实用潜力。

Abstract: In deep learning, regularization and normalization are common solutions for
challenges such as overfitting, numerical instabilities, and the increasing
variance in the residual stream. An alternative approach is to force all
parameters and representations to lie on a hypersphere. This removes the need
for regularization and increases convergence speed, but comes with additional
costs. In this work, we propose a more holistic but approximate normalization
(anTransformer). Our approach constrains the norm of parameters and normalizes
all representations via scalar multiplications motivated by the tight
concentration of the norms of high-dimensional random vectors. When applied to
GPT training, we observe a 40% faster convergence compared to models with QK
normalization, with less than 3% additional runtime. Deriving scaling laws for
anGPT, we found our method enables training with larger batch sizes and fewer
hyperparameters, while matching the favorable scaling characteristics of
classic GPT architectures.

</details>


### [168] [Weakly-Supervised Contrastive Learning for Imprecise Class Labels](https://arxiv.org/abs/2505.22028)
*Zi-Hao Zhou,Jun-Jie Wang,Tong Wei,Min-Ling Zhang*

Key words: 对比学习,弱监督,语义相似性,图论,噪声标签

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于连续语义相似性的弱监督对比学习框架，解决了实际数据中标签模糊或不精确的问题，通过图论方法提升性能。

Motivation: 实际场景中数据标注常不精确，限制了监督对比学习的应用，因此需要一种能处理模糊标签的方法。

Method: 通过连续语义相似性定义正负样本对，结合图论框架迭代优化弱监督信号。

Result: 在噪声标签和部分标签学习任务中表现优异，理论证明其可逼近监督对比学习效果。

Conclusion: 提出的框架具有普适性，能显著提升弱监督学习性能。

Abstract: Contrastive learning has achieved remarkable success in learning effective
representations, with supervised contrastive learning often outperforming
self-supervised approaches. However, in real-world scenarios, data annotations
are often ambiguous or inaccurate, meaning that class labels may not reliably
indicate whether two examples belong to the same class. This limitation
restricts the applicability of supervised contrastive learning. To address this
challenge, we introduce the concept of ``continuous semantic similarity'' to
define positive and negative pairs. Instead of directly relying on imprecise
class labels, we measure the semantic similarity between example pairs, which
quantifies how closely they belong to the same category by iteratively refining
weak supervisory signals. Based on this concept, we propose a graph-theoretic
framework for weakly-supervised contrastive learning, where semantic similarity
serves as the graph weights. Our framework is highly versatile and can be
applied to many weakly-supervised learning scenarios. We demonstrate its
effectiveness through experiments in two common settings, i.e., noisy label and
partial label learning, where existing methods can be easily integrated to
significantly improve performance. Theoretically, we establish an error bound
for our approach, showing that it can approximate supervised contrastive
learning under mild conditions. The implementation code is available at
https://github.com/Speechless-10308/WSC.

</details>


### [169] [Detecting Undesired Process Behavior by Means of Retrieval Augmented Generation](https://arxiv.org/abs/2505.22041)
*Michael Grohs,Adrian Rebmann,Jana-Rebecca Rehse*

Key words: 一致性检查、检索增强生成（RAG）、大型语言模型（LLM）、过程挖掘、事件日志

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于检索增强生成（RAG）的方法，用于在没有专用过程模型的情况下检测不期望的过程行为，避免了资源密集的微调，并优于传统微调LLM的方法。

Motivation: 缺乏过程模型时，传统的一致性检查技术无法应用，而现有的基于LLM微调的方法资源消耗大且泛化能力差，因此需要一种更高效且泛化性强的替代方案。

Method: 使用检索增强生成（RAG）技术，通过知识库直接为LLM提供已知的期望和不期望过程行为，避免微调，并利用事件日志中的上下文（如频繁轨迹和活动）增强效果。

Result: 评估表明，该方法在检测不期望行为上优于微调LLM，证明了RAG是资源密集微调的有效替代方案。

Conclusion: RAG方法无需专用模型或微调，即可高效检测不期望行为，尤其适用于结合事件日志上下文的场景。

Abstract: Conformance checking techniques detect undesired process behavior by
comparing process executions that are recorded in event logs to desired
behavior that is captured in a dedicated process model. If such models are not
available, conformance checking techniques are not applicable, but
organizations might still be interested in detecting undesired behavior in
their processes. To enable this, existing approaches use Large Language Models
(LLMs), assuming that they can learn to distinguish desired from undesired
behavior through fine-tuning. However, fine-tuning is highly resource-intensive
and the fine-tuned LLMs often do not generalize well. To address these
limitations, we propose an approach that requires neither a dedicated process
model nor resource-intensive fine-tuning to detect undesired process behavior.
Instead, we use Retrieval Augmented Generation (RAG) to provide an LLM with
direct access to a knowledge base that contains both desired and undesired
process behavior from other processes, assuming that the LLM can transfer this
knowledge to the process at hand. Our evaluation shows that our approach
outperforms fine-tuned LLMs in detecting undesired behavior, demonstrating that
RAG is a viable alternative to resource-intensive fine-tuning, particularly
when enriched with relevant context from the event log, such as frequent traces
and activities.

</details>


### [170] [Estimating the Effects of Sample Training Orders for Large Language Models without Retraining](https://arxiv.org/abs/2505.22042)
*Hao Yang,Haoxuan Li,Mengyue Yang,Xu Chen,Mingming Gong*

Key words: 大型语言模型, 样本顺序, 课程学习, 记忆效应, 泛化效应

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种无需重新训练即可估计大语言模型在不同样本顺序下性能的框架，改进了传统方法的高计算成本问题，并应用于课程学习设计和记忆/泛化效应分析。

Motivation: 传统方法需重新训练模型以研究样本顺序对大型语言模型的影响，计算成本过高。本研究旨在设计一种无需重新训练的高效框架，以降低计算负担并提升研究效率。

Method: 通过一阶和二阶泰勒展开近似Adam优化器的更新，并利用随机投影方法存储中间检查点，从而高效估计任意样本顺序下的模型参数。

Result: 实验证实了框架在重现真实模型性能方面的有效性，并展示了其在优化课程学习设计和分析记忆/泛化效应方面的潜力。

Conclusion: 提出的框架显著降低了计算成本，为研究样本顺序对大型语言模型的影响提供了高效工具，具有广泛的应用前景。

Abstract: The order of training samples plays a crucial role in large language models
(LLMs), significantly impacting both their external performance and internal
learning dynamics. Traditional methods for investigating this effect generally
require retraining the model with various sample orders, which is
computationally infeasible for LLMs. In this work, we improve traditional
methods by designing a retraining-free framework. By approximating Adam
optimizer updates with first- and second-order Taylor expansions and utilizing
random projection methods to store intermediate checkpoints, our framework can
efficiently estimate model parameters for arbitrary training sample orders.
Next, we apply our framework to two downstream research problems: (1) Training
curriculum design for LLMs -- we base our retraining-free framework to propose
a novel curriculum learning strategy that augments curriculum proposals with
estimated model performances, enabling more informed sample scheduling. (2)
LLMs' memorization and generalization effect analysis -- we use our
retraining-free framework to estimate how the positions of training samples
influence LLMs' capacity for memorization and generalization. We conduct
extensive experiments to validate the effectiveness of our retraining-free
framework in reproducing the true model performances, and further demonstrate
its potential in optimizing LLM training curricula and analyzing the
memorization and generalization effects of LLMs.

</details>


### [171] [Differentiable Generalized Sliced Wasserstein Plans](https://arxiv.org/abs/2505.22049)
*Laetitia Chapel,Romain Tavenard,Samuel Vaiter*

Key words: 最优传输、切片技术、双层优化、高维数据、多流形

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新的切片优化方案，通过双层优化和可微近似改进min-SWGG方法，解决了高维数据和多流形数据中的计算瓶颈，并在多种应用中验证了其有效性。

Motivation: 虽然最优传输（OT）在机器学习中应用广泛，但其高计算复杂度仍是瓶颈。现有的切片技术（如min-SWGG）虽然高效，但在高维数据和非线性投影中存在局限性，需要改进。

Method: 将min-SWGG重新建模为双层优化问题，并引入可微近似方案以高效找到最优切片。进一步扩展了方法以支持多流形数据。

Result: 提出的方法在高维数据和多流形场景中表现出色，并在梯度流和图像生成等应用中验证了其高效性。

Conclusion: 通过优化切片技术和扩展应用场景，该方法显著提升了OT在大规模和高维数据中的实用性。

Abstract: Optimal Transport (OT) has attracted significant interest in the machine
learning community, not only for its ability to define meaningful distances
between probability distributions -- such as the Wasserstein distance -- but
also for its formulation of OT plans. Its computational complexity remains a
bottleneck, though, and slicing techniques have been developed to scale OT to
large datasets. Recently, a novel slicing scheme, dubbed min-SWGG, lifts a
single one-dimensional plan back to the original multidimensional space,
finally selecting the slice that yields the lowest Wasserstein distance as an
approximation of the full OT plan. Despite its computational and theoretical
advantages, min-SWGG inherits typical limitations of slicing methods: (i) the
number of required slices grows exponentially with the data dimension, and (ii)
it is constrained to linear projections. Here, we reformulate min-SWGG as a
bilevel optimization problem and propose a differentiable approximation scheme
to efficiently identify the optimal slice, even in high-dimensional settings.
We furthermore define its generalized extension for accommodating to data
living on manifolds. Finally, we demonstrate the practical value of our
approach in various applications, including gradient flows on manifolds and
high-dimensional spaces, as well as a novel sliced OT-based conditional flow
matching for image generation -- where fast computation of transport plans is
essential.

</details>


### [172] [The Resurrection of the ReLU](https://arxiv.org/abs/2505.22074)
*Coşku Can Horuz,Geoffrey Kasenbacher,Saya Higuchi,Sebastian Kairat,Jendrik Stoltz,Moritz Pesl,Bernhard A. Moser,Christoph Linse,Thomas Martinetz,Sebastian Otte*

Key words: SUGAR, ReLU, 替代梯度, 深度学习, 激活函数

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了SUGAR方法，通过替代梯度学习优化ReLU激活函数，解决其‘死亡ReLU’问题，提升模型性能，并在多种现代架构中表现优异。

Motivation: 尽管ReLU因其简单性和稀疏性受欢迎，但其‘死亡ReLU’问题限制了效果。SUGAR旨在通过改进梯度处理保留ReLU优势并提升性能。

Method: SUGAR在前向传播中保留标准ReLU，但在反向传播中使用平滑替代梯度避免梯度消失，结合合适的替代函数优化训练。

Result: 在VGG-16、ResNet-18等架构中，SUGAR显著提升泛化性能，并在Conv2NeXt和Swin Transformer中表现竞争力，甚至略优于GELU。

Conclusion: 研究表明，通过适当梯度处理的ReLU仍能作为高效且通用的激活函数，挑战了高级激活函数的必要性。

Abstract: Modeling sophisticated activation functions within deep learning
architectures has evolved into a distinct research direction. Functions such as
GELU, SELU, and SiLU offer smooth gradients and improved convergence
properties, making them popular choices in state-of-the-art models. Despite
this trend, the classical ReLU remains appealing due to its simplicity,
inherent sparsity, and other advantageous topological characteristics. However,
ReLU units are prone to becoming irreversibly inactive - a phenomenon known as
the dying ReLU problem - which limits their overall effectiveness. In this
work, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel,
plug-and-play regularizer for deep architectures. SUGAR preserves the standard
ReLU function during the forward pass but replaces its derivative in the
backward pass with a smooth surrogate that avoids zeroing out gradients. We
demonstrate that SUGAR, when paired with a well-chosen surrogate function,
substantially enhances generalization performance over convolutional network
architectures such as VGG-16 and ResNet-18, providing sparser activations while
effectively resurrecting dead ReLUs. Moreover, we show that even in modern
architectures like Conv2NeXt and Swin Transformer - which typically employ GELU
- substituting these with SUGAR yields competitive and even slightly superior
performance. These findings challenge the prevailing notion that advanced
activation functions are necessary for optimal performance. Instead, they
suggest that the conventional ReLU, particularly with appropriate gradient
handling, can serve as a strong, versatile revived classic across a broad range
of deep learning vision models.

</details>


### [173] [Can Test-time Computation Mitigate Memorization Bias in Neural Symbolic Regression?](https://arxiv.org/abs/2505.22081)
*Shun Sato,Issei Sato*

Key words: 符号回归, 神经符号回归, Transformer, 记忆偏差, 测试时策略

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了神经符号回归（NSR）方法中Transformer模型的记忆偏差问题，发现其在处理多变量时性能下降，并探索了测试时策略以减少这种偏差。

Motivation: 探讨NSR方法中Transformer模型的记忆偏差如何影响其在符号回归任务中的表现，尤其是在输入变量较多时性能下降的问题。

Method: 通过合成数据集定量评估Transformer的记忆偏差，并进行理论分析，验证其在组合构建表达式时的局限性。进一步测试了测试时策略对减少偏差的影响。

Result: 研究发现Transformer很少生成训练数据中未出现的表达式，且测试时提供额外信息可显著减少记忆偏差，但减少偏差并不一定提升性能。

Conclusion: 研究深化了对NSR方法局限性的理解，为设计更鲁棒、通用的符号回归方法提供了基础。

Abstract: Symbolic regression aims to discover mathematical equations that fit given
numerical data. It has been applied in various fields of scientific research,
such as producing human-readable expressions that explain physical phenomena.
Recently, Neural symbolic regression (NSR) methods that involve Transformers
pre-trained on large-scale synthetic datasets have gained attention. While
these methods offer advantages such as short inference time, they suffer from
low performance, particularly when the number of input variables is large. In
this study, we hypothesized that this limitation stems from the memorization
bias of Transformers in symbolic regression. We conducted a quantitative
evaluation of this bias in Transformers using a synthetic dataset and found
that Transformers rarely generate expressions not present in the training data.
Additional theoretical analysis reveals that this bias arises from the
Transformer's inability to construct expressions compositionally while
verifying their numerical validity. We finally examined if tailoring test-time
strategies can lead to reduced memorization bias and better performance. We
empirically demonstrate that providing additional information to the model at
test time can significantly mitigate memorization bias. On the other hand, we
also find that reducing memorization bias does not necessarily correlate with
improved performance. These findings contribute to a deeper understanding of
the limitations of NSR approaches and offer a foundation for designing more
robust, generalizable symbolic regression methods. Code is available at
https://github.com/Shun-0922/Mem-Bias-NSR .

</details>


### [174] [Inclusive, Differentially Private Federated Learning for Clinical Data](https://arxiv.org/abs/2505.22108)
*Santhosh Parampottupadam,Melih Coşğun,Sarthak Pati,Maximilian Zenk,Saikat Roy,Dimitrios Bounias,Benjamin Hamm,Sinem Sav,Ralf Floca,Klaus Maier-Hein*

Key words: 联邦学习,差分隐私,合规评分,医疗AI,临床工作流程

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种新颖的合规感知联邦学习框架，通过动态调整噪声来提高隐私保护，同时提升模型性能，实验结果优于传统联邦学习方法。

Motivation: 现有联邦学习在临床应用中面临隐私、资源限制和合规性的挑战，尤其是统一的差分隐私方法会不公平地影响模型性能，因此需要一种更灵活、合规的解决方案。

Method: 引入合规感知的联邦学习框架，基于客户端合规评分动态调整差分隐私噪声，并开发合规评分工具以评估医疗安全标准。

Result: 实验表明，与传统方法相比，框架在整合资源匮乏和低合规诊所时，准确率最高提升15%。

Conclusion: 该框架平衡了隐私、合规与性能，为临床工作流程提供了可行的联邦学习解决方案。

Abstract: Federated Learning (FL) offers a promising approach for training clinical AI
models without centralizing sensitive patient data. However, its real-world
adoption is hindered by challenges related to privacy, resource constraints,
and compliance. Existing Differential Privacy (DP) approaches often apply
uniform noise, which disproportionately degrades model performance, even among
well-compliant institutions. In this work, we propose a novel compliance-aware
FL framework that enhances DP by adaptively adjusting noise based on
quantifiable client compliance scores. Additionally, we introduce a compliance
scoring tool based on key healthcare and security standards to promote secure,
inclusive, and equitable participation across diverse clinical settings.
Extensive experiments on public datasets demonstrate that integrating
under-resourced, less compliant clinics with highly regulated institutions
yields accuracy improvements of up to 15% over traditional FL. This work
advances FL by balancing privacy, compliance, and performance, making it a
viable solution for real-world clinical workflows in global healthcare.

</details>


### [175] [The quest for the GRAph Level autoEncoder (GRALE)](https://arxiv.org/abs/2505.22109)
*Paul Krzakala,Gabriel Melo,Charlotte Laclau,Florence d'Alché-Buc,Rémi Flamary*

Key words: graph representation learning, autoencoder, optimal transport, Evoformer, molecular data

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: GRALE 是一种新型图自动编码器，通过最优传输损失和可微分节点匹配模块训练，适用于多种下游任务。

Motivation: 图表示学习是一个挑战性任务，对化学和生物学等领域有重要应用价值。

Method: 采用基于注意力的架构，扩展 Evoformer 支持图编码和解码，使用最优传输损失和可微分节点匹配模块进行训练。

Result: 在模拟和分子数据实验中，GRALE 表现出高度通用性，适用于分类、回归、图插值、编辑、匹配和预测等任务。

Conclusion: GRALE 提供了一种通用的预训练方法，适用于广泛的图学习任务。

Abstract: Although graph-based learning has attracted a lot of attention, graph
representation learning is still a challenging task whose resolution may impact
key application fields such as chemistry or biology. To this end, we introduce
GRALE, a novel graph autoencoder that encodes and decodes graphs of varying
sizes into a shared embedding space. GRALE is trained using an Optimal
Transport-inspired loss that compares the original and reconstructed graphs and
leverages a differentiable node matching module, which is trained jointly with
the encoder and decoder. The proposed attention-based architecture relies on
Evoformer, the core component of AlphaFold, which we extend to support both
graph encoding and decoding. We show, in numerical experiments on simulated and
molecular data, that GRALE enables a highly general form of pre-training,
applicable to a wide range of downstream tasks, from classification and
regression to more complex tasks such as graph interpolation, editing,
matching, and prediction.

</details>


### [176] [BiMi Sheets: Infosheets for bias mitigation methods](https://arxiv.org/abs/2505.22114)
*MaryBeth Defrance,Guillaume Bied,Maarten Buyl,Jefrey Lijffijt,Tijl De Bie*

Key words: Bias mitigation, fairness, machine learning, portability, benchmarking

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: BiMi Sheets 是一种可移植、统一的指南，用于记录任何偏差缓解方法的设计选择，帮助研究人员和从业者快速了解其主要特征并比较需求。此外，其结构支持创建结构化偏差缓解方法数据库。

Motivation: 机器学习中的偏差缓解方法因领域、任务和模型不同而存在‘可移植性陷阱’，导致不同方法难以直接比较和应用。因此需要一种标准化工具来统一记录和比较方法设计。

Method: 提出BiMi Sheets作为文档化工具，通过标准化格式记录偏差缓解方法的设计选择，并建立平台bimisheet.com以促进其使用。

Result: BiMi Sheets 提供了一个结构化的偏差缓解方法数据库，支持快速比较和学习，提升了方法的可移植性和实用性。

Conclusion: BiMi Sheets 为解决偏差缓解方法的可移植性和比较问题提供了有效工具，有助于推动公平机器学习的实践应用。

Abstract: Over the past 15 years, hundreds of bias mitigation methods have been
proposed in the pursuit of fairness in machine learning (ML). However,
algorithmic biases are domain-, task-, and model-specific, leading to a
`portability trap': bias mitigation solutions in one context may not be
appropriate in another. Thus, a myriad of design choices have to be made when
creating a bias mitigation method, such as the formalization of fairness it
pursues, and where and how it intervenes in the ML pipeline. This creates
challenges in benchmarking and comparing the relative merits of different bias
mitigation methods, and limits their uptake by practitioners.
  We propose BiMi Sheets as a portable, uniform guide to document the design
choices of any bias mitigation method. This enables researchers and
practitioners to quickly learn its main characteristics and to compare with
their desiderata. Furthermore, the sheets' structure allow for the creation of
a structured database of bias mitigation methods. In order to foster the
sheets' adoption, we provide a platform for finding and creating BiMi Sheets at
bimisheet.com.

</details>


### [177] [Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL](https://arxiv.org/abs/2505.22151)
*Claude Formanek,Omayma Mahjoub,Louay Ben Nessir,Sasha Abramowitz,Ruan de Kock,Wiem Khlifi,Simon Du Toit,Felix Chalumeau,Daniel Rajaonarivonivelomanantsoa,Arnol Fokam,Siddarth Singh,Ulrich Mbou Sob,Arnu Pretorius*

Key words: 离线多智能体强化学习, Oryx, 隐式约束Q学习, 保留架构, 多智能体协调

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Oryx是一种新颖的离线多智能体强化学习算法，通过结合保留架构Sable和隐式约束Q学习，实现了复杂环境中的多智能体协调，并在多项基准测试中表现出色。

Motivation: 离线多智能体强化学习中的主要挑战是复杂环境中的多智能体多步协调，Oryx旨在直接解决这一挑战。

Method: Oryx结合了保留架构Sable和隐式约束Q学习（ICQ），提出了一种新型的离线自回归策略更新方案。

Result: Oryx在65个测试数据集中超过80%的表现优于现有方法，展示了在多种智能体和长轨迹任务中的强大泛化能力。

Conclusion: Oryx在离线多智能体强化学习领域实现了最先进的性能，并在多智能体协调任务中表现出卓越的扩展能力。

Abstract: A key challenge in offline multi-agent reinforcement learning (MARL) is
achieving effective many-agent multi-step coordination in complex environments.
In this work, we propose Oryx, a novel algorithm for offline cooperative MARL
to directly address this challenge. Oryx adapts the recently proposed
retention-based architecture Sable and combines it with a sequential form of
implicit constraint Q-learning (ICQ), to develop a novel offline
auto-regressive policy update scheme. This allows Oryx to solve complex
coordination challenges while maintaining temporal coherence over lengthy
trajectories. We evaluate Oryx across a diverse set of benchmarks from prior
works (SMAC, RWARE, and Multi-Agent MuJoCo) covering tasks of both discrete and
continuous control, varying in scale and difficulty. Oryx achieves
state-of-the-art performance on more than 80% of the 65 tested datasets,
outperforming prior offline MARL methods and demonstrating robust
generalisation across domains with many agents and long horizons. Finally, we
introduce new datasets to push the limits of many-agent coordination in offline
MARL, and demonstrate Oryx's superior ability to scale effectively in such
settings. We will make all of our datasets, experimental data, and code
available upon publication.

</details>


### [178] [Uncertainty Estimation for Heterophilic Graphs Through the Lens of Information Theory](https://arxiv.org/abs/2505.22152)
*Dominik Fuchsgruber,Tom Wollschläger,Johannes Bordne,Stephan Günnemann*

Key words: 不确定性估计, 异质性图, 信息论, 消息传递神经网络

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于信息论视角的方法，通过量化图神经网络各层信息传递的不等式，解决了异质性图中不确定性估计的挑战。

Motivation: 现有方法大多依赖同质性假设，在异质性图中表现不佳，因此需要一种不依赖同质性的通用不确定性估计方法。

Method: 从信息论角度分析消息传递神经网络，提出一种类似数据处理不等式的方法，并设计了一个简单的后验密度估计器。

Result: 该方法在异质性图中实现了最先进的不确定性估计效果，同时在不依赖同质性的情况下与现有同质性图方法表现相当。

Conclusion: 同时考虑所有节点嵌入是异质性图中不确定性估计的关键设计原则。

Abstract: While uncertainty estimation for graphs recently gained traction, most
methods rely on homophily and deteriorate in heterophilic settings. We address
this by analyzing message passing neural networks from an information-theoretic
perspective and developing a suitable analog to data processing inequality to
quantify information throughout the model's layers. In contrast to non-graph
domains, information about the node-level prediction target can increase with
model depth if a node's features are semantically different from its neighbors.
Therefore, on heterophilic graphs, the latent embeddings of an MPNN each
provide different information about the data distribution - different from
homophilic settings. This reveals that considering all node representations
simultaneously is a key design principle for epistemic uncertainty estimation
on graphs beyond homophily. We empirically confirm this with a simple post-hoc
density estimator on the joint node embedding space that provides
state-of-the-art uncertainty on heterophilic graphs. At the same time, it
matches prior work on homophilic graphs without explicitly exploiting homophily
through post-processing.

</details>


### [179] [The informativeness of the gradient revisited](https://arxiv.org/abs/2505.22158)
*Rustem Takhanov*

Key words: 深度学习，梯度信息，LWE问题，碰撞熵，高频函数

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了梯度信息在深度学习中局限性，提出了一个关于目标函数类独立性及输入分布碰撞熵对梯度方差影响的通用上界，并应用于LWE映射及高频函数。

Motivation: 近年来，基于梯度的深度学习方法在某些领域取得了巨大进展，但同时也暴露了其局限性。特别是在许多实际学习任务中，梯度信息过少导致梯度法需要极大量的迭代才能成功，因此需要一个更深入的理论框架来衡量梯度的信息量。

Method: 本文通过度量梯度方差与研究目标函数类及输入分布的关系，提出了一个通用的上界表达式，量化了梯度的信息量，并进一步将其应用于学习错误问题（LWE）映射及高频函数的分析。

Result: 文章得出了一个梯度方差的上界公式$ 	ilde{\mathcal{O}}(\varepsilon+e^{-\frac{1}{2}\mathcal{E}_c}) $，揭示了目标函数类独立性$ \varepsilon $与输入分布碰撞熵$ \mathcal{E}_c $的影响。通过实验验证了该上界在LWE及高频函数应用中的效用。

Conclusion: 该研究为理解梯度信息在深度学习中局限性提供了理论基础，并通过LWE问题展示了其实际应用价值。未来的方向可以包括扩展理论框架以覆盖更多学习任务。

Abstract: In the past decade gradient-based deep learning has revolutionized several
applications. However, this rapid advancement has highlighted the need for a
deeper theoretical understanding of its limitations. Research has shown that,
in many practical learning tasks, the information contained in the gradient is
so minimal that gradient-based methods require an exceedingly large number of
iterations to achieve success. The informativeness of the gradient is typically
measured by its variance with respect to the random selection of a target
function from a hypothesis class.
  We use this framework and give a general bound on the variance in terms of a
parameter related to the pairwise independence of the target function class and
the collision entropy of the input distribution. Our bound scales as $
\tilde{\mathcal{O}}(\varepsilon+e^{-\frac{1}{2}\mathcal{E}_c}) $, where $
\tilde{\mathcal{O}} $ hides factors related to the regularity of the learning
model and the loss function, $ \varepsilon $ measures the pairwise independence
of the target function class and $\mathcal{E}_c$ is the collision entropy of
the input distribution.
  To demonstrate the practical utility of our bound, we apply it to the class
of Learning with Errors (LWE) mappings and high-frequency functions. In
addition to the theoretical analysis, we present experiments to understand
better the nature of recent deep learning-based attacks on LWE.

</details>


### [180] [An Augmentation-Aware Theory for Self-Supervised Contrastive Learning](https://arxiv.org/abs/2505.22196)
*Jingyi Cui,Hongwei Wen,Yisen Wang*

Key words: 自监督对比学习, 数据增强, 误差边界, 语义标签

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一个针对自监督对比学习的增强感知误差边界，首次将数据增强的影响显式地纳入了监督风险的界定。

Motivation: 现有理论研究未充分探讨数据增强的作用，尤其是特定增强类型的影响，作者试图填补这一空白。

Method: 提出增强感知误差边界，分析数据增强对监督风险的显式影响，并基于语义标签假设讨论不同增强方法如何影响误差边界。

Result: 通过像素级和表示级实验验证了理论结果。

Conclusion: 数据增强在自监督对比学习中通过显式权衡影响监督风险，特定增强方法对误差边界有显著作用。

Abstract: Self-supervised contrastive learning has emerged as a powerful tool in
machine learning and computer vision to learn meaningful representations from
unlabeled data. Meanwhile, its empirical success has encouraged many
theoretical studies to reveal the learning mechanisms. However, in the existing
theoretical research, the role of data augmentation is still under-exploited,
especially the effects of specific augmentation types. To fill in the blank, we
for the first time propose an augmentation-aware error bound for
self-supervised contrastive learning, showing that the supervised risk is
bounded not only by the unsupervised risk, but also explicitly by a trade-off
induced by data augmentation. Then, under a novel semantic label assumption, we
discuss how certain augmentation methods affect the error bound. Lastly, we
conduct both pixel- and representation-level experiments to verify our proposed
theoretical results.

</details>


### [181] [Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer](https://arxiv.org/abs/2505.22199)
*Xinyue Hu,Zhibin Duan,Bo Chen,Mingyuan Zhou*

Key words: 不确定性估计, 可解释性, 贝叶斯非负决策层, 解耦表示, 变分推断

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种贝叶斯非负决策层（BNDL）框架，通过重新表述深度神经网络为条件贝叶斯非负因子分析，解决了不确定性估计和模型可解释性问题。

Motivation: 深度神经网络虽然表达能力强，但难以满足实际应用中不确定性估计的需求，且其耦合特性导致决策受无关特征影响，可解释性不足。

Method: BNDL利用随机潜在变量建模复杂依赖关系，通过学习稀疏、非负的潜在变量实现解耦表示和决策层。采用了基于Weibull的变分推断网络近似潜在变量后验分布。

Result: 实验表明，BNDL增强了模型解耦能力，不仅提高了准确性，还提供了可靠的不确定性估计和更好的可解释性。

Conclusion: BNDL通过结合贝叶斯框架和结构约束，在保持性能的同时显著提升了深度学习模型的不确定性量化与可解释性。

Abstract: Although deep neural networks have demonstrated significant success due to
their powerful expressiveness, most models struggle to meet practical
requirements for uncertainty estimation. Concurrently, the entangled nature of
deep neural networks leads to a multifaceted problem, where various localized
explanation techniques reveal that multiple unrelated features influence the
decisions, thereby undermining interpretability. To address these challenges,
we develop a Bayesian Non-negative Decision Layer (BNDL), which reformulates
deep neural networks as a conditional Bayesian non-negative factor analysis. By
leveraging stochastic latent variables, the BNDL can model complex dependencies
and provide robust uncertainty estimation. Moreover, the sparsity and
non-negativity of the latent variables encourage the model to learn
disentangled representations and decision layers, thereby improving
interpretability. We also offer theoretical guarantees that BNDL can achieve
effective disentangled learning. In addition, we developed a corresponding
variational inference method utilizing a Weibull variational inference network
to approximate the posterior distribution of the latent variables. Our
experimental results demonstrate that with enhanced disentanglement
capabilities, BNDL not only improves the model's accuracy but also provides
reliable uncertainty estimation and improved interpretability.

</details>


### [182] [Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning](https://arxiv.org/abs/2505.22203)
*Yuzhen Huang,Weihao Zeng,Xingshan Zeng,Qi Zhu,Junxian He*

Key words: 强化学习,可验证奖励,验证器,数学推理,可靠性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 这篇论文分析了强化学习可验证奖励（RLVR）中两类验证器的局限性，发现基于规则的验证器易产生假阴性错误，而基于模型的验证器则易被攻击导致假阳性错误，影响了强化学习训练的可靠性。

Motivation: 研究动机在于探讨强化学习可验证奖励中验证器的可靠性及其对训练过程的影响，特别是在数学推理等复杂领域中。

Method: 以数学推理为例，通过静态评估和强化学习训练场景对比分析了基于规则和基于模型的验证器的表现。

Result: 结果发现基于规则的验证器常误判不同格式的等价值案，而基于模型的验证器虽静态评估准确度高，但易受攻击导致假阳性错误，进而影响训练结果的真实性。

Conclusion: 结论指出两类验证器各有固有风险，为开发更稳健的强化学习奖励系统提供了重要参考。

Abstract: Trustworthy verifiers are essential for the success of reinforcement learning
with verifiable reward (RLVR), which is the core methodology behind various
large reasoning models such as DeepSeek-R1. In complex domains like
mathematical reasoning, rule-based verifiers have been widely adopted in
previous works to train strong reasoning models. However, the reliability of
these verifiers and their impact on the RL training process remain poorly
understood. In this work, we take mathematical reasoning as a case study and
conduct a comprehensive analysis of various verifiers in both static evaluation
and RL training scenarios. First, we find that current open-source rule-based
verifiers often fail to recognize equivalent answers presented in different
formats across multiple commonly used mathematical datasets, resulting in
non-negligible false negative rates. This limitation adversely affects RL
training performance and becomes more pronounced as the policy model gets
stronger. Subsequently, we investigate model-based verifiers as a potential
solution to address these limitations. While the static evaluation shows that
model-based verifiers achieve significantly higher verification accuracy,
further analysis and RL training results imply that they are highly susceptible
to hacking, where they misclassify certain patterns in responses as correct
(i.e., false positives). This vulnerability is exploited during policy model
optimization, leading to artificially inflated rewards. Our findings underscore
the unique risks inherent to both rule-based and model-based verifiers, aiming
to offer valuable insights to develop more robust reward systems in
reinforcement learning.

</details>


### [183] [LaMM: Semi-Supervised Pre-Training of Large-Scale Materials Models](https://arxiv.org/abs/2505.22208)
*Yosuke Oyama,Yusuke Majima,Eiji Ohta,Yasufumi Sakai*

Key words: 神经网络势能、半监督学习、预训练、负载均衡、材料科学

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为LaMM的半监督预训练方法，通过改进去噪自监督学习和负载均衡算法，有效利用大规模半标记数据集训练神经网络势能（NNP）模型，提升微调的效率和精度。

Motivation: 神经网络势能（NNP）是加速计算材料科学的关键技术，但传统预训练和微调方法计算成本高，主要由于DFT标记和大规模预训练中的负载不均衡问题。

Method: 提出LaMM方法，结合改进的去噪自监督学习和负载均衡算法，支持多节点高效训练，并利用约3亿半标记样本进行预训练。

Result: 实验表明LaMM能显著提升微调阶段的训练速度和精度。

Conclusion: LaMM方法为NNP的高效训练提供了新思路，平衡了计算成本与模型性能。

Abstract: Neural network potentials (NNPs) are crucial for accelerating computational
materials science by surrogating density functional theory (DFT) calculations.
Improving their accuracy is possible through pre-training and fine-tuning,
where an NNP model is first pre-trained on a large-scale dataset and then
fine-tuned on a smaller target dataset. However, this approach is
computationally expensive, mainly due to the cost of DFT-based dataset labeling
and load imbalances during large-scale pre-training. To address this, we
propose LaMM, a semi-supervised pre-training method incorporating improved
denoising self-supervised learning and a load-balancing algorithm for efficient
multi-node training. We demonstrate that our approach effectively leverages a
large-scale dataset of $\sim$300 million semi-labeled samples to train a single
NNP model, resulting in improved fine-tuning performance in terms of both speed
and accuracy.

</details>


### [184] [Solver-Free Decision-Focused Learning for Linear Optimization Problems](https://arxiv.org/abs/2505.22224)
*Senne Berden,Ali İrfan Mahmutoğulları,Dimos Tsouros,Tias Guns*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种针对线性优化问题的无求解器训练方法，通过利用几何结构减少计算成本，同时保持决策质量。

Motivation: 解决决策聚焦学习（DFL）中因每次损失评估需解决优化问题导致的高计算成本问题。

Method: 利用线性优化的几何结构，通过比较真实最优解与其相邻顶点的估计质量作为损失函数，避免求解优化问题。

Result: 实验表明，该方法显著降低了计算成本，同时保持了较高的决策质量。

Conclusion: 该方法为线性优化问题的决策聚焦学习提供了一种高效且有效的训练方法。

Abstract: Mathematical optimization is a fundamental tool for decision-making in a wide
range of applications. However, in many real-world scenarios, the parameters of
the optimization problem are not known a priori and must be predicted from
contextual features. This gives rise to predict-then-optimize problems, where a
machine learning model predicts problem parameters that are then used to make
decisions via optimization. A growing body of work on decision-focused learning
(DFL) addresses this setting by training models specifically to produce
predictions that maximize downstream decision quality, rather than accuracy.
While effective, DFL is computationally expensive, because it requires solving
the optimization problem with the predicted parameters at each loss evaluation.
In this work, we address this computational bottleneck for linear optimization
problems, a common class of problems in both DFL literature and real-world
applications. We propose a solver-free training method that exploits the
geometric structure of linear optimization to enable efficient training with
minimal degradation in solution quality. Our method is based on the insight
that a solution is optimal if and only if it achieves an objective value that
is at least as good as that of its adjacent vertices on the feasible polytope.
Building on this, our method compares the estimated quality of the ground-truth
optimal solution with that of its precomputed adjacent vertices, and uses this
as loss function. Experiments demonstrate that our method significantly reduces
computational cost while maintaining high decision quality.

</details>


### [185] [Optimal kernel regression bounds under energy-bounded noise](https://arxiv.org/abs/2505.22235)
*Amon Lahr,Johannes Köhler,Anna Scampicchio,Melanie N. Zeilinger*

Key words: 不确定性边界, 核估计, 高斯过程, 相关噪声, 安全关键

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种严格的非渐进不确定性边界，适用于基于核的估计，并能处理相关噪声序列，其计算依赖于对未知函数和噪声的范数有界假设。

Motivation: 研究动机是为了提供一种非保守的不确定性边界，既能评估估计算法的准确性，又适用于安全关键场景的下游任务。

Method: 方法是通过高斯过程的后验均值和协方差，在测量噪声协方差最优选择下，返回假设类中的最坏情况函数实现。

Result: 结果表明，该方法能提供紧密且易于计算的不确定性边界，优于文献中的其他结果。

Conclusion: 结论是该方法是有效的，适用于核估计的严格边界计算，并具有实际应用潜力。

Abstract: Non-conservative uncertainty bounds are key for both assessing an estimation
algorithm's accuracy and in view of downstream tasks, such as its deployment in
safety-critical contexts. In this paper, we derive a tight, non-asymptotic
uncertainty bound for kernel-based estimation, which can also handle correlated
noise sequences. Its computation relies on a mild norm-boundedness assumption
on the unknown function and the noise, returning the worst-case function
realization within the hypothesis class at an arbitrary query input location.
The value of this function is shown to be given in terms of the posterior mean
and covariance of a Gaussian process for an optimal choice of the measurement
noise covariance. By rigorously analyzing the proposed approach and comparing
it with other results in the literature, we show its effectiveness in returning
tight and easy-to-compute bounds for kernel-based estimates.

</details>


### [186] [B-XAIC Dataset: Benchmarking Explainable AI for Graph Neural Networks Using Chemical Data](https://arxiv.org/abs/2505.22252)
*Magdalena Proszewska,Tomasz Danel,Dawid Rymarczyk*

Key words: 可解释AI, 图神经网络, 分子设计, 药物发现, 基准测试

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文介绍了B-XAIC，一个基于真实分子数据和多种任务的基准测试，用于评估可解释AI（XAI）在图神经网络（GNNs）中的能力。通过这一基准测试，揭示了现有XAI方法在分子领域的局限性，有助于开发更可靠和可解释的模型。

Motivation: 当前在化学信息学和药物发现领域，对深度学习模型预测的解释性评估大多依赖于人工数据集或简化任务，缺乏对真实场景复杂性的捕捉，且与解释的忠实性关联性不强。因此，需要一个新的基准测试来解决这些问题。

Method: 研究团队提出了B-XAIC基准测试，该测试基于真实分子数据和多样任务，并且已知了标签的ground-truth rationale，通过全面评估揭示了现有XAI方法在图神经网络（GNNs）分子领域的不足。

Result: B-XAIC基准测试揭示了现有XAI方法在分子领域的局限性，为开发更可靠和可解释的模型提供了有价值的资源。

Conclusion: B-XAIC填补了当前评估框架的不足，为提升XAI在分子设计领域的应用效果提供了新的方向。

Abstract: Understanding the reasoning behind deep learning model predictions is crucial
in cheminformatics and drug discovery, where molecular design determines their
properties. However, current evaluation frameworks for Explainable AI (XAI) in
this domain often rely on artificial datasets or simplified tasks, employing
data-derived metrics that fail to capture the complexity of real-world
scenarios and lack a direct link to explanation faithfulness. To address this,
we introduce B-XAIC, a novel benchmark constructed from real-world molecular
data and diverse tasks with known ground-truth rationales for assigned labels.
Through a comprehensive evaluation using B-XAIC, we reveal limitations of
existing XAI methods for Graph Neural Networks (GNNs) in the molecular domain.
This benchmark provides a valuable resource for gaining deeper insights into
the faithfulness of XAI, facilitating the development of more reliable and
interpretable models.

</details>


### [187] [A Unified Online-Offline Framework for Co-Branding Campaign Recommendations](https://arxiv.org/abs/2505.22254)
*Xiangxiang Dai,Xiaowei Sun,Jinhang Zuo,Xutong Liu,John C. S. Lui*

Key words: 联合品牌、推荐系统、在线学习、离线优化、跨行业合作

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种统一线上线下框架，用于推荐跨行业联合品牌合作，通过动态调整和优化策略，显著提升了效果。

Motivation: 联合品牌是扩展市场的关键策略，但跨行业合作的资源不平衡、品牌意愿不确定以及市场变化等问题使其难以实现。本文首次系统研究该问题。

Method: 构建二分图量化联合品牌概率和市场收益，通过在线学习和离线优化动态调整，平衡探索新合作与利用现有合作的关系。

Result: 理论和实验证明了框架的有效性，提升了12%的性能。

Conclusion: 该框架不仅能提升短期效果，还能确保长期战略增长。

Abstract: Co-branding has become a vital strategy for businesses aiming to expand
market reach within recommendation systems. However, identifying effective
cross-industry partnerships remains challenging due to resource imbalances,
uncertain brand willingness, and ever-changing market conditions. In this
paper, we provide the first systematic study of this problem and propose a
unified online-offline framework to enable co-branding recommendations. Our
approach begins by constructing a bipartite graph linking ``initiating'' and
``target'' brands to quantify co-branding probabilities and assess market
benefits. During the online learning phase, we dynamically update the graph in
response to market feedback, while striking a balance between exploring new
collaborations for long-term gains and exploiting established partnerships for
immediate benefits. To address the high initial co-branding costs, our
framework mitigates redundant exploration, thereby enhancing short-term
performance while ensuring sustainable strategic growth. In the offline
optimization phase, our framework consolidates the interests of multiple
sub-brands under the same parent brand to maximize overall returns, avoid
excessive investment in single sub-brands, and reduce unnecessary costs
associated with over-prioritizing a single sub-brand. We present a theoretical
analysis of our approach, establishing a highly nontrivial sublinear regret
bound for online learning in the complex co-branding problem, and enhancing the
approximation guarantee for the NP-hard offline budget allocation optimization.
Experiments on both synthetic and real-world co-branding datasets demonstrate
the practical effectiveness of our framework, with at least 12\% improvement.

</details>


### [188] [Train Sparse Autoencoders Efficiently by Utilizing Features Correlation](https://arxiv.org/abs/2505.22255)
*Vadim Kurochkin,Yaroslav Aksenov,Daniil Laptev,Daniil Gavrilov,Nikita Balagansky*

Key words: 稀疏自编码器, Kronecker乘积, mAND, 可解释性, 计算效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: KronSAE提出了一种通过Kronecker乘积分解来降低稀疏自编码器内存和计算开销的新架构，并引入了mAND激活函数以提高可解释性和性能。

Motivation: 稀疏自编码器（SAEs）在解释语言模型的隐藏状态方面表现出潜力，但大规模训练仍面临挑战，尤其是计算密集的线性操作问题。

Method: 提出KronSAE架构，利用Kronecker乘积分解因子化潜在表示；引入mAND激活函数模拟二进制AND操作以优化性能和可解释性。

Result: 显著减少了内存和计算开销，同时提升了模型的可解释性和性能。

Conclusion: KronSAE和mAND的结合为大规模稀疏自编码器的训练提供了高效且可解释的解决方案。

Abstract: Sparse Autoencoders (SAEs) have demonstrated significant promise in
interpreting the hidden states of language models by decomposing them into
interpretable latent directions. However, training SAEs at scale remains
challenging, especially when large dictionary sizes are used. While decoders
can leverage sparse-aware kernels for efficiency, encoders still require
computationally intensive linear operations with large output dimensions. To
address this, we propose KronSAE, a novel architecture that factorizes the
latent representation via Kronecker product decomposition, drastically reducing
memory and computational overhead. Furthermore, we introduce mAND, a
differentiable activation function approximating the binary AND operation,
which improves interpretability and performance in our factorized framework.

</details>


### [189] [Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training](https://arxiv.org/abs/2505.22257)
*Youssef Mroueh,Nicolas Dupuis,Brian Belgodere,Apoorva Nitsure,Mattia Rigotti,Kristjan Greenewald,Jiri Navratil,Jerret Ross,Jesus Rios*

Key words: 强化学习, GRPO, 策略优化, 离策略学习, PPO

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文重新审视了Group Relative Policy Optimization（GRPO）的在策略和离策略优化机制，并将其适应于离策略设置。结果显示，离策略GRPO表现优于或与在策略GRPO相当。

Motivation: 基于最近关于离策略PPO的研究，作者受启发改进训练稳定性、采样效率和内存使用，并探索在GRPO中利用离策略样本估计优势函数的潜在益处。

Method: 作者将在策略和离策略GRPO目标应用于强化学习设置，并使用裁剪替代目标优化离策略版本的GRPO。

Result: 实验结果表明，离策略GRPO显著优于或与在策略GRPO表现相当。

Conclusion: 研究支持在离策略环境中使用GRPO，并通过裁剪替代目标进一步优化其性能。

Abstract: We revisit Group Relative Policy Optimization (GRPO) in both on-policy and
off-policy optimization regimes. Our motivation comes from recent work on
off-policy Proximal Policy Optimization (PPO), which improves training
stability, sampling efficiency, and memory usage. In addition, a recent
analysis of GRPO suggests that estimating the advantage function with
off-policy samples could be beneficial. Building on these observations, we
adapt GRPO to the off-policy setting. We show that both on-policy and
off-policy GRPO objectives yield an improvement in the reward. This result
motivates the use of clipped surrogate objectives in the off-policy version of
GRPO. We then compare the empirical performance of reinforcement learning with
verifiable rewards in post-training using both GRPO variants. Our results show
that off-policy GRPO either significantly outperforms or performs on par with
its on-policy counterpart.

</details>


### [190] [Full Domain Analysis in Fluid Dynamics](https://arxiv.org/abs/2505.22275)
*Alexander Hagg,Adam Gaier,Dominik Wilde,Alexander Asteroth,Holger Foysi,Dirk Reith*

Key words: 全领域分析, 进化优化, 机器学习, 流体动力学, 复杂系统

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文介绍了全领域分析的新技术，结合进化优化、模拟和机器学习，用于流体动力学等复杂计算领域，旨在高效探索解决方案空间并分析其行为。

Motivation: 研究动机在于解决计算成本高昂且流动行为复杂的领域（如流体动力学）中全面分析解决方案的需求，以深化对这类领域的理解。

Method: 提出了一种全领域分析的正式模型，结合进化优化、模拟和机器学习技术，用于解决方案的多样化、优化和分析。

Result: 通过一个示例展示了全领域分析的应用价值，证明了其在理解计算物理学等复杂系统中的潜力。

Conclusion: 全领域分析结合优化和机器学习，是理解计算物理学等复杂系统的有力工具。

Abstract: Novel techniques in evolutionary optimization, simulation and machine
learning allow for a broad analysis of domains like fluid dynamics, in which
computation is expensive and flow behavior is complex. Under the term of full
domain analysis we understand the ability to efficiently determine the full
space of solutions in a problem domain, and analyze the behavior of those
solutions in an accessible and interactive manner. The goal of full domain
analysis is to deepen our understanding of domains by generating many examples
of flow, their diversification, optimization and analysis. We define a formal
model for full domain analysis, its current state of the art, and requirements
of subcomponents. Finally, an example is given to show what we can learn by
using full domain analysis. Full domain analysis, rooted in optimization and
machine learning, can be a helpful tool in understanding complex systems in
computational physics and beyond.

</details>


### [191] [Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer](https://arxiv.org/abs/2505.22306)
*Zehua Chen,Yuyang Miao,Liyuan Wang,Luyun Fan,Danilo P. Mandic,Jun Zhu*

Key words: UniCardio, 多模态扩散变换器, 心血管信号, 信号生成, AI辅助医疗

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: UniCardio是一个多模态扩散变换器，用于重构低质量信号和合成未记录信号，通过创新模型架构和持续学习范式，显著提升心血管信号的去噪、填充和翻译性能。

Motivation: 心血管信号（如PPG、ECG和BP）相关性高但采集困难，现有方法在实时监测中表现受限，需统一生成框架解决信号质量与缺失问题。

Method: 提出UniCardio，采用多模态扩散变换器架构，结合持续学习范式处理不同模态组合，利用信号的互补性提升生成效果。

Result: 在信号去噪、填充和翻译任务上优于现有方法，生成信号在异常健康检测和生命体征估计中接近真实信号性能，且具备可解释性。

Conclusion: UniCardio为AI辅助医疗提供了有前景的解决方案，通过统一生成框架显著提升心血管信号处理的性能与适用性。

Abstract: Cardiovascular signals such as photoplethysmography (PPG),
electrocardiography (ECG), and blood pressure (BP) are inherently correlated
and complementary, together reflecting the health of cardiovascular system.
However, their joint utilization in real-time monitoring is severely limited by
diverse acquisition challenges from noisy wearable recordings to burdened
invasive procedures. Here we propose UniCardio, a multi-modal diffusion
transformer that reconstructs low-quality signals and synthesizes unrecorded
signals in a unified generative framework. Its key innovations include a
specialized model architecture to manage the signal modalities involved in
generation tasks and a continual learning paradigm to incorporate varying
modality combinations. By exploiting the complementary nature of cardiovascular
signals, UniCardio clearly outperforms recent task-specific baselines in signal
denoising, imputation, and translation. The generated signals match the
performance of ground-truth signals in detecting abnormal health conditions and
estimating vital signs, even in unseen domains, while ensuring interpretability
for human experts. These advantages position UniCardio as a promising avenue
for advancing AI-assisted healthcare.

</details>


### [192] [Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic Reasoning](https://arxiv.org/abs/2505.22308)
*Zachary Shinnick,Liangze Jiang,Hemanth Saratchandran,Anton van den Hengel,Damien Teney*

Key words: 合成数据、Transformer、预训练、归纳结构、算法推理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究表明，简单的合成数据与自然语言预训练类似，能提升小规模Transformer模型的特定算法推理能力，不同规则可互补地嵌入模型的不同部分，如注意力层或MLP块。

Motivation: 探讨合成数据在语言模型预训练中的作用，尤其是其如何影响模型的推理能力及结构分布，以提升模型的鲁棒性和数据效率。

Method: 使用不同规则生成合成数据预训练小型Transformer，通过消融和部分迁移实验分析模型权重和结构。

Result: 不同规则在模型的注意力层和MLP块中形成互补的归纳结构，多规则组合可强化多种能力。

Conclusion: 合成数据能有效诱导模型结构，支持知识获取与推理的解耦，可能提升模型效率与鲁棒性。

Abstract: Pretraining on large, semantically rich datasets is key for developing
language models. Surprisingly, recent studies have shown that even synthetic
data, generated procedurally through simple semantic-free algorithms, can yield
some of the same benefits as natural language pretraining. It is unclear what
specific capabilities such simple synthetic data instils in a model, where
these capabilities reside in the architecture, and how they manifest within its
weights. In this short paper, we identify several beneficial forms of
procedural data, together with specific algorithmic reasoning skills that
improve in small transformers. Our core finding is that different procedural
rules instil distinct but complementary inductive structures in the model. With
extensive ablations and partial-transfer experiments, we discover that these
structures reside in different parts of the model. Attention layers often carry
the most transferable information, but some pretraining rules impart useful
structure to MLP blocks instead. Most interestingly, the structures induced by
multiple rules can be composed to jointly reinforce multiple capabilities.
These results suggest an exciting possibility of disentangling the acquisition
of knowledge from reasoning in language models, with the goal of improving
their robustness and data efficiency.

</details>


### [193] [From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization](https://arxiv.org/abs/2505.22310)
*Shoaib Ahmed Siddiqui,Adrian Weller,David Krueger,Gintare Karolina Dziugaite,Michael Curtis Mozer,Eleni Triantafillou*

Key words: LLM，遗忘学习，再学习攻击，权重空间，L2距离，线性模式连通性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 近期LLM的遗忘方法容易受到再学习攻击，即使通过少量无关样本微调，遗忘的知识也会重现。在视觉分类任务中，仅使用保留集微调即可将遗忘集准确率从50%恢复到近100%。研究发现，权重空间的L2距离和线性模式连通性可预测再学习攻击的抵抗性，并提出新方法实现最佳抵抗效果。

Motivation: 研究LLM遗忘方法的脆弱性，探索再学习攻击下遗忘知识的重现现象及其机制。

Method: 在视觉分类任务中实验，分析遗忘集准确率恢复现象；通过权重空间特性（L2距离、线性模式连通性）预测抵抗性，并设计新方法。

Result: 仅用保留集微调即可将遗忘集准确率从50%恢复至近100%；新方法在抵抗再学习攻击上表现最佳。

Conclusion: 现有遗忘方法存在再学习漏洞，权重空间特性是关键抵抗指标，新方法显著提升鲁棒性。

Abstract: Recent unlearning methods for LLMs are vulnerable to relearning attacks:
knowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of
(even seemingly-unrelated) examples. We study this phenomenon in a controlled
setting for example-level unlearning in vision classifiers. We make the
surprising discovery that forget-set accuracy can recover from around 50%
post-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e.,
zero examples of the forget set. We observe this effect across a wide variety
of unlearning methods, whereas for a model retrained from scratch excluding the
forget set (gold standard), the accuracy remains at 50%. We observe that
resistance to relearning attacks can be predicted by weight-space properties,
specifically, $L_2$-distance and linear mode connectivity between the original
and the unlearned model. Leveraging this insight, we propose a new class of
methods that achieve state-of-the-art resistance to relearning attacks.

</details>


### [194] [Skywork Open Reasoner 1 Technical Report](https://arxiv.org/abs/2505.22312)
*Jujie He,Jiacai Liu,Chris Yuhao Liu,Rui Yan,Chaojie Wang,Peng Cheng,Xiaoyu Zhang,Fuxiang Zhang,Jiacheng Xu,Wei Shen,Siyuan Li,Liang Zeng,Tianwen Wei,Cheng Cheng,Bo An,Yang Liu,Yahui Zhou*

Key words: 强化学习, 大型语言模型, 推理能力, 长链思维, 熵崩溃

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了Skywork-OR1，一种基于强化学习的长链思维模型，显著提升了模型的推理能力，并在多个基准测试中表现优异。

Motivation: 通过强化学习提升大型语言模型的推理能力，尤其是在长链思维任务中。

Method: 基于DeepSeek-R1-Distill模型系列，采用强化学习策略进行训练。

Result: 在AIME24、AIME25和LiveCodeBench基准测试中，32B模型准确率提升15.0%，7B模型提升13.9%。

Conclusion: Skywork-OR1在推理任务中表现优越，且缓解熵崩溃现象对模型性能至关重要。

Abstract: The success of DeepSeek-R1 underscores the significant role of reinforcement
learning (RL) in enhancing the reasoning capabilities of large language models
(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL
implementation for long Chain-of-Thought (CoT) models. Building on the
DeepSeek-R1-Distill model series, our RL approach achieves notable performance
gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench
from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)
for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and
Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable
results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models
demonstrate competitive reasoning capabilities among models of similar size. We
perform comprehensive ablation studies on the core components of our training
pipeline to validate their effectiveness. Additionally, we thoroughly
investigate the phenomenon of entropy collapse, identify key factors affecting
entropy dynamics, and demonstrate that mitigating premature entropy collapse is
critical for improved test performance. To support community research, we fully
open-source our model weights, training code, and training datasets.

</details>


### [195] [Rethinking BPS: A Utility-Based Evaluation Framework](https://arxiv.org/abs/2505.22316)
*Konrad Özdemir,Lukas Kirchdorfer,Keyvan Amiri Elyasi,Han van der Aa,Heiner Stuckenschmidt*

Key words: 业务流程模拟, 准确性评估, 预测性流程监控, 数据复杂性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一个新的框架来评估业务流程模拟(BPS)的质量，通过比较模拟数据和真实数据在下游分析任务中的表现，解决了现有方法在准确性和数据复杂性区分上的不足。

Motivation: 现有的BPS模型评估方法存在两个主要问题：一是无法有效评估模型对当前流程的捕捉能力，二是依赖的评估指标可能掩盖时间模式，导致误导性结论。

Method: 提出一个新框架，通过评估模拟数据是否能为下游分析任务生成具有代表性的流程行为，代替传统的与未来真实事件比较的方法。

Result: 实验结果表明，该框架不仅能识别差异来源，还能区分模型准确性和数据复杂性。

Conclusion: 新框架为BPS质量评估提供了更有效的方法，特别在处理流程行为变化和数据复杂性时表现优越。

Abstract: Business process simulation (BPS) is a key tool for analyzing and optimizing
organizational workflows, supporting decision-making by estimating the impact
of process changes. The reliability of such estimates depends on the ability of
a BPS model to accurately mimic the process under analysis, making rigorous
accuracy evaluation essential. However, the state-of-the-art approach to
evaluating BPS models has two key limitations. First, it treats simulation as a
forecasting problem, testing whether models can predict unseen future events.
This fails to assess how well a model captures the as-is process, particularly
when process behavior changes from train to test period. Thus, it becomes
difficult to determine whether poor results stem from an inaccurate model or
the inherent complexity of the data, such as unpredictable drift. Second, the
evaluation approach strongly relies on Earth Mover's Distance-based metrics,
which can obscure temporal patterns and thus yield misleading conclusions about
simulation quality. To address these issues, we propose a novel framework that
evaluates simulation quality based on its ability to generate representative
process behavior. Instead of comparing simulated logs to future real-world
executions, we evaluate whether predictive process monitoring models trained on
simulated data perform comparably to those trained on real data for downstream
analysis tasks. Empirical results show that our framework not only helps
identify sources of discrepancies but also distinguishes between model accuracy
and data complexity, offering a more meaningful way to assess BPS quality.

</details>


### [196] [A Closer Look on Memorization in Tabular Diffusion Model: A Data-Centric Perspective](https://arxiv.org/abs/2505.22322)
*Zhengyu Fang,Zhimeng Jiang,Huiyuan Chen,Xiaoge Zhang,Kaiyu Tang,Xiao Li,Jing Li*

Key words: 扩散模型，记忆动态，隐私保护，表格数据，DynamicCut

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了表格扩散模型中的数据记忆动态，提出了一种名为DynamicCut的双阶段缓解方法，以减少模型对训练数据的过度记忆。

Motivation: 扩散模型在生成高质量表格数据方面表现出色，但存在隐私风险，尤其是对训练样本的过度记忆。现有研究多关注数据集级别的增强来减少记忆，而较少关注对个体样本的影响。

Method: 通过量化每个真实样本的记忆程度（基于生成样本中复制的比例），分析记忆分布。提出DynamicCut方法：按记忆强度排序样本，剪除高记忆强度的部分，然后重新训练。

Result: 实验表明记忆呈现重尾分布，少量样本贡献大部分记忆。DynamicCut有效减少了记忆，同时保持数据多样性和下游任务性能，并适用于其他生成模型。

Conclusion: DynamicCut提供了一种模型无关的记忆缓解方法，且具备跨模型适用性，为隐私保护生成模型提供了新思路。

Abstract: Diffusion models have shown strong performance in generating high-quality
tabular data, but they carry privacy risks by reproducing exact training
samples. While prior work focuses on dataset-level augmentation to reduce
memorization, little is known about which individual samples contribute most.
We present the first data-centric study of memorization dynamics in tabular
diffusion models. We quantify memorization for each real sample based on how
many generated samples are flagged as replicas, using a relative distance
ratio. Our empirical analysis reveals a heavy-tailed distribution of
memorization counts: a small subset of samples contributes disproportionately
to leakage, confirmed via sample-removal experiments. To understand this, we
divide real samples into top- and non-top-memorized groups and analyze their
training-time behaviors. We track when each sample is first memorized and
monitor per-epoch memorization intensity (AUC). Memorized samples are memorized
slightly earlier and show stronger signals in early training. Based on these
insights, we propose DynamicCut, a two-stage, model-agnostic mitigation method:
(a) rank samples by epoch-wise intensity, (b) prune a tunable top fraction, and
(c) retrain on the filtered dataset. Across multiple tabular datasets and
models, DynamicCut reduces memorization with minimal impact on data diversity
and downstream performance. It also complements augmentation-based defenses.
Furthermore, DynamicCut enables cross-model transferability: high-ranked
samples identified from one model (e.g., a diffusion model) are also effective
for reducing memorization when removed from others, such as GANs and VAEs.

</details>


### [197] [Look Within or Look Beyond? A Theoretical Comparison Between Parameter-Efficient and Full Fine-Tuning](https://arxiv.org/abs/2505.22355)
*Yongkang Liu,Xingle Xu,Ercong Nie,Zijing Wang,Shi Feng,Daling Wang,Qian Li,Hinrich Schütze*

Key words: Parameter-Efficient Fine-Tuning, Full Fine-Tuning, 优化理论, 表示能力, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 摘要讨论了Parameter-Efficient Fine-Tuning (PEFT) 方法在资源效率上的优势，但在复杂任务（如推理和指令微调）中表现不及Full Fine-Tuning (FFT)。通过理论分析和实验验证，研究发现PEFT在表示能力和鲁棒性上受限，期望引发更深入的研究。

Motivation: 研究动机是探索PEFT方法在节约资源的同时，为何在复杂任务中表现不如FFT，并从理论角度解释其局限性。

Method: 基于优化理论，比较PEFT和FFT的表示能力与鲁棒性，并通过实验在15个数据集和11个对抗测试集上验证理论。

Result: 研究结果显示PEFT在复杂任务中表现较差，理论分析表明其参数空间的限制导致表示能力不足，且对扰动更敏感。

Conclusion: 结论指出PEFT的局限性，并呼吁未来研究突破现有PEFT方法的局限。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods achieve performance comparable
to Full Fine-Tuning (FFT) while requiring significantly fewer computing
resources, making it the go-to choice for researchers. We find that although
PEFT can achieve competitive results on some benchmarks, its performance falls
short of FFT in complex tasks, such as reasoning and instruction-based
fine-tuning. In this paper, we compare the characteristics of PEFT and FFT in
terms of representational capacity and robustness based on optimization theory.
We theoretically demonstrate that PEFT is a strict subset of FFT. By providing
theoretical upper bounds for PEFT, we show that the limited parameter space
constrains the model's representational ability, making it more susceptible to
perturbations. Experiments on 15 datasets encompassing classification,
generation, reasoning, instruction fine-tuning tasks and 11 adversarial test
sets validate our theories. We hope that these results spark further research
beyond the realms of well established PEFT. The source code is in the anonymous
Github repository\footnote{https://github.com/misonsky/PEFTEval}.

</details>


### [198] [Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings](https://arxiv.org/abs/2505.22356)
*Angéline Pouget,Mohammad Yaghini,Stephan Rabanser,Nicolas Papernot*

Key words: 机器学习, 适宜性信号, 协变量偏移, 统计假设检验, 性能监测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新颖的框架——'适宜性过滤器'，用于在无标注数据的场景下检测机器学习模型性能退化，通过利用对协变量偏移敏感的模型输出特征（适宜性信号），并与预设的精度降幅阈值比较，确保模型性能可靠性。

Motivation: 在安全关键领域部署机器学习模型时，缺乏标注数据导致无法直接验证模型性能，因此需要一种方法检测性能退化，以预防潜在失败。

Method: 采用适宜性信号，通过统计假设检验比较测试数据和用户数据的分布差异，判断性能退化是否超过预设阈值。

Result: 在不同分类任务中的实证评估表明，该方法能有效检测由协变量偏移引起的性能偏差。

Conclusion: 适宜性过滤器为高风险应用提供了主动性能监测机制，提升了模型可靠性。

Abstract: Deploying machine learning models in safety-critical domains poses a key
challenge: ensuring reliable model performance on downstream user data without
access to ground truth labels for direct validation. We propose the suitability
filter, a novel framework designed to detect performance deterioration by
utilizing suitability signals -- model output features that are sensitive to
covariate shifts and indicative of potential prediction errors. The suitability
filter evaluates whether classifier accuracy on unlabeled user data shows
significant degradation compared to the accuracy measured on the labeled test
dataset. Specifically, it ensures that this degradation does not exceed a
pre-specified margin, which represents the maximum acceptable drop in accuracy.
To achieve reliable performance evaluation, we aggregate suitability signals
for both test and user data and compare these empirical distributions using
statistical hypothesis testing, thus providing insights into decision
uncertainty. Our modular method adapts to various models and domains. Empirical
evaluations across different classification tasks demonstrate that the
suitability filter reliably detects performance deviations due to covariate
shift. This enables proactive mitigation of potential failures in high-stakes
applications.

</details>


### [199] [Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs](https://arxiv.org/abs/2505.22358)
*Zhiyi Wan,Wanrou Du,Liang Li,Miao Pan,Xiaoqi Qin*

Key words: 大型语言模型，持续学习，灾难性遗忘，动态预算，正交子空间

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: OA-Adapter 是一个解决大型语言模型在持续学习中灾难性遗忘问题的新型方法，通过动态预算分配和正交子空间学习的端到端训练，提升了准确性和参数效率。

Motivation: 解决大型语言模型在持续学习中因任务干扰导致的灾难性遗忘问题，同时避免现有方法因固定预算分配或多阶段优化导致的性能不足。

Method: 提出OA-Adapter，结合动态瓶颈维度适应机制和正交约束，统一优化预算分配和任务目标。

Result: 在标准持续学习基准测试中，OA-Adapter 准确率更高且参数使用量减少58.5%。

Conclusion: OA-Adapter 通过端到端的动态预算和正交学习，显著提升了持续学习的性能和效率。

Abstract: Large language models (LLMs) often suffer from catastrophic forgetting in
continual learning (CL) scenarios, where performance on previously learned
tasks degrades severely while training on sequentially arriving tasks. Although
pioneering CL approaches using orthogonal subspaces can mitigate task
interference, they typically employ fixed budget allocation, neglecting the
varying complexity across tasks and layers. Besides, recent budget-adaptive
tuning methods for LLMs often adopt multi-stage paradigms that decouple
optimization and budget allocation. Such decoupling results in potential
misalignment, which hinders those approaches' practical application in CL
scenarios. To address these limitations, we propose OA-Adapter, a novel
parameter-efficient approach for continual learning in LLMs that unifies
dynamic budget adaptation with orthogonal subspace learning in a single
end-to-end training stage. Specifically, OA-Adapter introduces a dynamic
bottleneck dimension adaptation mechanism that simultaneously allocates an
efficient parameter budget and optimizes task objectives without misalignment.
To effectively preserve previously acquired knowledge while coordinating with
the dynamic budget allocation, orthogonal constraints are applied specifically
between the parameter subspace of the current task and the dynamically
allocated parameter subspaces of historical tasks. Experimental results on
continual learning benchmarks demonstrate that OA-Adapter outperforms
state-of-the-art methods in both accuracy and parameter efficiency, achieving
higher average accuracy while using 58.5% fewer parameters on the standard CL
benchmark.

</details>


### [200] [Multiclass Loss Geometry Matters for Generalization of Gradient Descent in Separable Classification](https://arxiv.org/abs/2505.22359)
*Matan Schliserman,Tomer Koren*

Key words: 梯度方法、多类分类、泛化性能、损失模板、风险界限

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究无正则化梯度方法在多类线性分类中的泛化性能，提出新的人口风险界限，揭示收敛速度受损失模板几何形状影响而非损失函数本身。

Motivation: 以往研究多集中在二元分类，本文扩展至多类分类，探索梯度下降在不同损失函数下的性能差异。

Method: 通过分析损失模板的几何形状（$p$-范数下的平滑性），建立风险上界，并证明在指数衰减损失下的收敛率差异。

Result: 结果显示，$p=\infty$时风险与$k$对数相关，$p=2$时线性相关，且多项式依赖无法避免。

Conclusion: 损失模板的几何形状是影响梯度下降泛化性能的关键因素，特别是在多类分类中。

Abstract: We study the generalization performance of unregularized gradient methods for
separable linear classification. While previous work mostly deal with the
binary case, we focus on the multiclass setting with $k$ classes and establish
novel population risk bounds for Gradient Descent for loss functions that decay
to zero. In this setting, we show risk bounds that reveal that convergence
rates are crucially influenced by the geometry of the loss template, as
formalized by Wang and Scott (2024), rather than of the loss function itself.
Particularly, we establish risk upper bounds that holds for any decay rate of
the loss whose template is smooth with respect to the $p$-norm. In the case of
exponentially decaying losses, our results indicates a contrast between the
$p=\infty$ case, where the risk exhibits a logarithmic dependence on $k$, and
$p=2$ where the risk scales linearly with $k$. To establish this separation
formally, we also prove a lower bound in the latter scenario, demonstrating
that the polynomial dependence on $k$ is unavoidable. Central to our analysis
is a novel bound on the Rademacher complexity of low-noise vector-valued linear
predictors with a loss template smooth w.r.t.~general $p$-norms.

</details>


### [201] [Continuum-armed Bandit Optimization with Batch Pairwise Comparison Oracles](https://arxiv.org/abs/2505.22361)
*Xiangyu Chang,Xi Chen,Yining Wang,Zhiyi Zeng*

Key words: bandit优化, 成对比较oracle, 强凹函数, 离散化, LinUCB, 运营管理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究了通过成对比较oracle优化未知强凹函数f(x)的问题，适用于定价与库存补充及网络收入管理，提出了离散化和局部多项式逼近方法，结合比赛式逐步消除技术与LinUCB算法，取得了最优的遗憾界。

Motivation: 研究动机源于联合定价与库存补充问题以及网络收入管理中的实际需求，现有随机优化文献中的oracle无法处理偏差估计的情况，因此需要新的方法。

Method: 采用离散化技术和局部多项式逼近将问题转化为线性bandits，结合比赛式逐步消除技术定位离散化单元，并运行交互式批处理的LinUCB算法。

Result: 建立了最优（除多对数因子外）的遗憾界，并在两个运营管理问题上应用，改进了现有文献中的最优结果。

Conclusion: 通过成对比较oracle和提出的算法框架，能有效优化未知强凹函数，并在运营管理问题中实现了性能提升。

Abstract: This paper studies a bandit optimization problem where the goal is to
maximize a function $f(x)$ over $T$ periods for some unknown strongly concave
function $f$. We consider a new pairwise comparison oracle, where the
decision-maker chooses a pair of actions $(x, x')$ for a consecutive number of
periods and then obtains an estimate of $f(x)-f(x')$. We show that such a
pairwise comparison oracle finds important applications to joint pricing and
inventory replenishment problems and network revenue management. The challenge
in this bandit optimization is twofold. First, the decision-maker not only
needs to determine a pair of actions $(x, x')$ but also a stopping time $n$
(i.e., the number of queries based on $(x, x')$). Second, motivated by our
inventory application, the estimate of the difference $f(x)-f(x')$ is biased,
which is different from existing oracles in stochastic optimization literature.
To address these challenges, we first introduce a discretization technique and
local polynomial approximation to relate this problem to linear bandits. Then
we developed a tournament successive elimination technique to localize the
discretized cell and run an interactive batched version of LinUCB algorithm on
cells. We establish regret bounds that are optimal up to poly-logarithmic
factors. Furthermore, we apply our proposed algorithm and analytical framework
to the two operations management problems and obtain results that improve
state-of-the-art results in the existing literature.

</details>


### [202] [Directed Homophily-Aware Graph Neural Network](https://arxiv.org/abs/2505.22362)
*Aihu Zhang,Jiaxing Xu,Mengcheng Lan,Shili Xiang,Yiping Ke*

Key words: 图神经网络,异质性邻域,方向性图,同质性感知,链接预测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: DHGNN是一种新型的图神经网络框架，通过整合同质性感知和方向敏感组件，有效解决了传统GNN在异质性邻域和方向性图结构上的局限性。

Motivation: 传统GNN在异质性邻域和方向性图结构上的表现不佳，需要一种新方法来解决这些局限性。

Method: DHGNN采用可重置门控机制根据同质性水平和信息量自适应调节消息贡献，并使用结构感知的噪声容忍融合模块整合正反向节点表示。

Result: 在节点分类和链接预测任务中，DHGNN在异质性和同质性图数据集上均优于现有方法，链接预测性能最高提升15.07%。

Conclusion: DHGNN不仅提升了性能，其门控机制还为复杂图结构上的消息传递行为提供了深入见解。

Abstract: Graph Neural Networks (GNNs) have achieved significant success in various
learning tasks on graph-structured data. Nevertheless, most GNNs struggle to
generalize to heterophilic neighborhoods. Additionally, many GNNs ignore the
directional nature of real-world graphs, resulting in suboptimal performance on
directed graphs with asymmetric structures. In this work, we propose Directed
Homophily-aware Graph Neural Network (DHGNN), a novel framework that addresses
these limitations by incorporating homophily-aware and direction-sensitive
components. DHGNN employs a resettable gating mechanism to adaptively modulate
message contributions based on homophily levels and informativeness, and a
structure-aware noise-tolerant fusion module to effectively integrate node
representations from the original and reverse directions. Extensive experiments
on both homophilic and heterophilic directed graph datasets demonstrate that
DHGNN outperforms state-of-the-art methods in node classification and link
prediction. In particular, DHGNN improves over the best baseline by up to
15.07% in link prediction. Our analysis further shows that the gating mechanism
captures directional homophily gaps and fluctuating homophily across layers,
providing deeper insights into message-passing behavior on complex graph
structures.

</details>


### [203] [SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting](https://arxiv.org/abs/2505.22370)
*Haomiao Qiu,Miao Zhang,Ziyue Qiao,Weili Guan,Min Zhang,Liqiang Nie*

Key words: Continual Learning, Gradient Projection, Low-Rank Adaptation, SplitLoRA, Stability, Plasticity

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为SplitLoRA的新方法，基于低秩适应（Low-Rank Adaptation）来解决持续学习中的稳定性和可塑性平衡问题，通过理论分析指导梯度空间的最优划分，实验证明其性能优越。

Motivation: 现有梯度投影方法在持续学习中难以平衡稳定性和可塑性，梯度空间的划分不够优化。

Method: 基于低秩适应（LoRA），提出SplitLoRA方法，通过理论分析指导梯度空间的最优划分。

Result: 在多个数据集上的实验表明，SplitLoRA方法达到了最先进的性能。

Conclusion: SplitLoRA通过优化梯度空间划分，有效平衡了持续学习中的稳定性和可塑性，具有广泛适用性。

Abstract: Continual Learning requires a model to learn multiple tasks in sequence while
maintaining both stability:preserving knowledge from previously learned tasks,
and plasticity:effectively learning new tasks. Gradient projection has emerged
as an effective and popular paradigm in CL, where it partitions the gradient
space of previously learned tasks into two orthogonal subspaces: a primary
subspace and a minor subspace. New tasks are learned effectively within the
minor subspace, thereby reducing interference with previously acquired
knowledge. However, existing Gradient Projection methods struggle to achieve an
optimal balance between plasticity and stability, as it is hard to
appropriately partition the gradient space. In this work, we consider a
continual learning paradigm based on Low-Rank Adaptation, which has gained
considerable attention due to its efficiency and wide applicability, and
propose a novel approach for continual learning, called SplitLoRA. We first
provide a theoretical analysis of how subspace partitioning affects model
stability and plasticity. Informed by this analysis, we then introduce an
effective method that derives the optimal partition of the gradient space for
previously learned tasks. This approach effectively balances stability and
plasticity in continual learning. Experimental results on multiple datasets
demonstrate that the proposed method achieves state-of-the-art performance.

</details>


### [204] [A Divide-and-Conquer Approach for Modeling Arrival Times in Business Process Simulation](https://arxiv.org/abs/2505.22381)
*Lukas Kirchdorfer,Konrad Özdemir,Stjepan Kusenic,Han van der Aa,Heiner Stuckenschmidt*

Key words: 业务流程仿真, 案例到达模型, 核密度估计, 动态建模

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为AT-KDE的新方法，用于更准确地模拟业务流程中的案例到达模式，克服了现有静态分布方法的不足。

Motivation: 当前业务流程仿真中，案例到达模型过于简化，无法捕捉动态和时间复杂性，影响仿真结果的准确性和可靠性。

Method: 采用Auto Time Kernel Density Estimation (AT-KDE) 方法，结合全局动态、周内变化和日内分布变化，建模案例到达时间。

Result: 在20个不同流程上的实验表明，AT-KDE比现有方法更准确、鲁棒，同时保持合理的执行效率。

Conclusion: AT-KDE方法显著提升了业务流程仿真的准确性，尤其在处理动态和复杂时间模式时表现优异。

Abstract: Business Process Simulation (BPS) is a critical tool for analyzing and
improving organizational processes by estimating the impact of process changes.
A key component of BPS is the case-arrival model, which determines the pattern
of new case entries into a process. Although accurate case-arrival modeling is
essential for reliable simulations, as it influences waiting and overall cycle
times, existing approaches often rely on oversimplified static distributions of
inter-arrival times. These approaches fail to capture the dynamic and temporal
complexities inherent in organizational environments, leading to less accurate
and reliable outcomes. To address this limitation, we propose Auto Time Kernel
Density Estimation (AT-KDE), a divide-and-conquer approach that models arrival
times of processes by incorporating global dynamics, day-of-week variations,
and intraday distributional changes, ensuring both precision and scalability.
Experiments conducted across 20 diverse processes demonstrate that AT-KDE is
far more accurate and robust than existing approaches while maintaining
sensible execution time efficiency.

</details>


### [205] [Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning](https://arxiv.org/abs/2505.22389)
*Haomiao Qiu,Miao Zhang,Ziyue Qiao,Liqiang Nie*

Key words: 持续学习, 灾难性遗忘, 模型合并, 二阶正则化, LoRA

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了Perturb-and-Merge (P&M)框架，通过模型合并技术缓解持续学习中的灾难性遗忘问题，结合二阶正则化与LoRA方法，在多个数据集上实现最优性能。

Motivation: 现有持续学习方法仅依赖最近任务的参数进行推断，易受灾难性遗忘影响。受模型合并技术启发，提出通过合并新旧模型来缓解遗忘。

Method: P&M框架在每项任务训练后，通过凸组合合并新旧模型，利用二阶正则化优化合并效果，并结合LoRA降低内存开销。

Result: 在多个持续学习基准数据集上达到最优性能。

Conclusion: P&M通过模型合并与二阶正则化有效缓解遗忘，结合LoRA提高了效率。

Abstract: Continual Learning (CL) aims to enable models to continuously acquire new
knowledge from a sequence of tasks with avoiding the forgetting of learned
information. However, existing CL methods only rely on the parameters of the
most recent task for inference, which makes them susceptible to catastrophic
forgetting. Inspired by the recent success of model merging techniques, we
propose \textbf{Perturb-and-Merge (P\&M)}, a novel continual learning framework
that integrates model merging into the CL paradigm to mitigate forgetting.
Specifically, after training on each task, P\&M constructs a new model by
forming a convex combination of the previous model and the newly trained
task-specific model. Through theoretical analysis, we minimize the total loss
increase across all tasks and derive an analytical solution for the optimal
merging coefficient. To further improve the performance of the merged model, we
observe that the degradation introduced during merging can be alleviated by a
regularization term composed of the task vector and the Hessian matrix of the
loss function. Interestingly, we show that this term can be efficiently
approximated using second-order symmetric finite differences, and a stochastic
perturbation strategy along the task vector direction is accordingly devised
which incurs no additional forward or backward passes while providing an
effective approximation of the regularization term. Finally, we combine P\&M
with LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead.
Our proposed approach achieves state-of-the-art performance on several
continual learning benchmark datasets.

</details>


### [206] [Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation](https://arxiv.org/abs/2505.22391)
*Yi Zhang,Difan Zou*

Key words: diffusion models, PDE, physical constraints, distillation, PIDDM

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为PIDDM的后处理蒸馏方法，用于改善扩散模型在物理系统中的约束满足问题，同时支持正向和逆向问题求解。

Motivation: 扩散模型在物理系统建模中受到关注，但直接在噪声数据上施加约束会导致Jensen's Gap问题，影响准确性。论文旨在解决这一权衡问题。

Method: 论文提出PIDDM方法，通过在蒸馏后处理阶段而非扩散过程中直接应用PDE约束，以提高约束满足和生成效率。

Result: 实验表明，PIDDM在多个PDE基准测试中显著优于现有基线（如PIDM、DiffusionPDE等），且计算开销更低。

Conclusion: PIDDM为扩散模型引入物理约束提供了更高效和有效的策略，同时支持多种问题求解。

Abstract: Modeling physical systems in a generative manner offers several advantages,
including the ability to handle partial observations, generate diverse
solutions, and address both forward and inverse problems. Recently, diffusion
models have gained increasing attention in the modeling of physical systems,
particularly those governed by partial differential equations (PDEs). However,
diffusion models only access noisy data $\boldsymbol{x}_t$ at intermediate
steps, making it infeasible to directly enforce constraints on the clean sample
$\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are
typically applied to the expectation of clean samples
$\mathbb{E}[\boldsymbol{x}_0|\boldsymbol{x}_t]$, which is estimated using the
learned score network. However, imposing PDE constraints on the expectation
does not strictly represent the one on the true clean data, known as Jensen's
Gap. This gap creates a trade-off: enforcing PDE constraints may come at the
cost of reduced accuracy in generative modeling. To address this, we propose a
simple yet effective post-hoc distillation approach, where PDE constraints are
not injected directly into the diffusion process, but instead enforced during a
post-hoc distillation stage. We term our method as Physics-Informed
Distillation of Diffusion Models (PIDDM). This distillation not only
facilitates single-step generation with improved PDE satisfaction, but also
support both forward and inverse problem solving and reconstruction from
randomly partial observation. Extensive experiments across various PDE
benchmarks demonstrate that PIDDM significantly improves PDE satisfaction over
several recent and competitive baselines, such as PIDM, DiffusionPDE, and
ECI-sampling, with less computation overhead. Our approach can shed light on
more efficient and effective strategies for incorporating physical constraints
into diffusion models.

</details>


### [207] [Mitigating Overthinking in Large Reasoning Models via Manifold Steering](https://arxiv.org/abs/2505.22411)
*Yao Huang,Huanran Chen,Shouwei Ruan,Yichi Zhang,Xingxing Wei,Yinpeng Dong*

Key words: 大型推理模型、过度思考、低维流形、计算效率、跨域迁移

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为Manifold Steering的新方法，通过低维流形投影减少大型推理模型（LRMs）中的过度思考现象，显著降低了计算开销，同时保持或提高了任务准确性。

Motivation: 大型推理模型在解决复杂任务时容易出现过度思考现象，导致计算资源浪费。研究旨在通过机理可解释性揭示其本质并优化。

Method: 首先识别激活空间中的单一方向可捕捉过度思考倾向，但干预效果有限；进而发现其与低维流形相关，提出通过流形投影降低噪声干扰的Manifold Steering方法。

Result: 实验表明，该方法在数学任务中减少71%的输出token且保持准确性，在代码生成和知识问答中同样有效。

Conclusion: 低维流形投影能高效缓解过度思考，兼具跨任务泛化性。

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in solving complex tasks such as mathematics and coding. However,
these models frequently exhibit a phenomenon known as overthinking during
inference, characterized by excessive validation loops and redundant
deliberation, leading to substantial computational overheads. In this paper, we
aim to mitigate overthinking by investigating the underlying mechanisms from
the perspective of mechanistic interpretability. We first showcase that the
tendency of overthinking can be effectively captured by a single direction in
the model's activation space and the issue can be eased by intervening the
activations along this direction. However, this efficacy soon reaches a plateau
and even deteriorates as the intervention strength increases. We therefore
systematically explore the activation space and find that the overthinking
phenomenon is actually tied to a low-dimensional manifold, which indicates that
the limited effect stems from the noises introduced by the high-dimensional
steering direction. Based on this insight, we propose Manifold Steering, a
novel approach that elegantly projects the steering direction onto the
low-dimensional activation manifold given the theoretical approximation of the
interference noise. Extensive experiments on DeepSeek-R1 distilled models
validate that our method reduces output tokens by up to 71% while maintaining
and even improving the accuracy on several mathematical benchmarks. Our method
also exhibits robust cross-domain transferability, delivering consistent token
reduction performance in code generation and knowledge-based QA tasks. Code is
available at: https://github.com/Aries-iai/Manifold_Steering.

</details>


### [208] [STaR-Bets: Sequential Target-Recalculating Bets for Tighter Confidence Intervals](https://arxiv.org/abs/2505.22422)
*Václav Voráček,Francesco Orabona*

Key words: 置信区间, 投注算法, 最优策略, 有限时间保证

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种基于投注算法的置信区间构造方法，在固定时间设置下实现了最优宽度，并证明了其性能优于现有方法。

Motivation: 在构建边界随机变量均值的置信区间时，现有方法在固定时间设置下要么次优，要么缺乏有限时间保证。本文旨在填补这一空白。

Method: 提出了一种基于投注算法的策略，每一步选择最优策略（而非固定策略），并结合经典不等式（如Hoeffding或Bernstein）严格提升性能。

Result: 提出的方法在实证中优于竞争对手，且置信区间宽度最优（渐近于$1+o(1)$）。代码已开源。

Conclusion: 该方法在固定时间设置下填补了最优置信区间的空白，并在理论和实践中均表现出色。

Abstract: The construction of confidence intervals for the mean of a bounded random
variable is a classical problem in statistics with numerous applications in
machine learning and virtually all scientific fields. In particular, obtaining
the tightest possible confidence intervals is vital every time the sampling of
the random variables is expensive. The current state-of-the-art method to
construct confidence intervals is by using betting algorithms. This is a very
successful approach for deriving optimal confidence sequences, even matching
the rate of law of iterated logarithms. However, in the fixed horizon setting,
these approaches are either sub-optimal or based on heuristic solutions with
strong empirical performance but without a finite-time guarantee. Hence, no
betting-based algorithm guaranteeing the optimal
$\mathcal{O}(\sqrt{\frac{\sigma^2\log\frac1\delta}{n}})$ width of the
confidence intervals are known. This work bridges this gap. We propose a
betting-based algorithm to compute confidence intervals that empirically
outperforms the competitors. Our betting strategy uses the optimal strategy in
every step (in a certain sense), whereas the standard betting methods choose a
constant strategy in advance. Leveraging this fact results in strict
improvements even for classical concentration inequalities, such as the ones of
Hoeffding or Bernstein. Moreover, we also prove that the width of our
confidence intervals is optimal up to an $1+o(1)$ factor diminishing with $n$.
The code is available
on~https://github.com/vvoracek/STaR-bets-confidence-interval.

</details>


### [209] [Scaling Reasoning without Attention](https://arxiv.org/abs/2505.22425)
*Xueliang Zhao,Wei Wu,Lingpeng Kong*

Key words: 大语言模型,状态空间模型,无注意力架构,复杂推理,课程微调

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了基于状态空间双层的无注意力语言模型，通过架构和数据中心的创新解决了Transformer的架构效率问题和缺乏结构化微调的挑战，模型在多个基准测试中超越了许多大型Transformer和混合模型。

Motivation: 现有的大型语言模型(LLMs)在复杂推理任务中表现出色，但仍因Transformer架构的低效和高难度领域缺乏结构化微调而受限。本文提出新模型以解决这两个核心问题。

Method: 使用了基于Mamba-2的状态空间双层(SSD)架构以消除自注意力机制和键值缓存，提出了基于PromptCoT的双阶段课程微调策略来训练模型的复杂推理能力。

Result: 在AIME 24、AIME 25和Livecodebench等基准测试中，7B参数的模型表现优于同规模的Transformer和混合模型，甚至超越更大的Gemma3-27B。

Conclusion: 状态空间模型可以作为高容量推理任务中基于注意力的架构的高效、可扩展替代方案。

Abstract: Large language models (LLMs) have made significant advances in complex
reasoning tasks, yet they remain bottlenecked by two core challenges:
architectural inefficiency due to reliance on Transformers, and a lack of
structured fine-tuning for high-difficulty domains. We introduce \ourmodel, an
attention-free language model that addresses both issues through architectural
and data-centric innovations. Built on the state space dual (SSD) layers of
Mamba-2, our model eliminates the need for self-attention and key-value
caching, enabling fixed-memory, constant-time inference. To train it for
complex reasoning, we propose a two-phase curriculum fine-tuning strategy based
on the \textsc{PromptCoT} synthesis paradigm, which generates pedagogically
structured problems via abstract concept selection and rationale-guided
generation. On benchmark evaluations, \ourmodel-7B outperforms strong
Transformer and hybrid models of comparable scale, and even surpasses the much
larger Gemma3-27B by 2.6\% on AIME 24, 0.6\% on AIME 25, and 3.0\% on
Livecodebench. These results highlight the potential of state space models as
efficient and scalable alternatives to attention-based architectures for
high-capacity reasoning.

</details>


### [210] [Data-Driven Antenna Miniaturization: A Knowledge-Based System Integrating Quantum PSO and Predictive Machine Learning Models](https://arxiv.org/abs/2505.22440)
*Khan Masood Parvez,Sk Md Abidar Rahaman,Ali Shiri Sichani*

Key words: antenna design, machine learning, QDPSO, ANSYS HFSS, optimization, 6G, IoT

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: The paper presents a machine learning-enhanced workflow using QDPSO and ANSYS HFSS to optimize antenna design, achieving a 240x speedup over traditional methods.

Motivation: To address the need for automated antenna design frameworks due to rapid wireless technology evolution and constrained development cycles.

Method: Integration of Quantum-Behaved Dynamic Particle Swarm Optimization (QDPSO) with ANSYS HFSS simulations and machine learning models (SVM, Random Forest, XGBoost, Stacked ensembles).

Result: Achieved a 12.7% reduction in resonance frequency (1.4208 GHz vs. 1.60 GHz) and 240x acceleration compared to traditional methods, with stacked models showing high accuracy (R2=0.9825).

Conclusion: The framework bridges AI-driven optimization with CAD validation, reducing workload and enabling scalable designs for 6G and IoT applications.

Abstract: The rapid evolution of wireless technologies necessitates automated design
frameworks to address antenna miniaturization and performance optimization
within constrained development cycles. This study demonstrates a machine
learning enhanced workflow integrating Quantum-Behaved Dynamic Particle Swarm
Optimization (QDPSO) with ANSYS HFSS simulations to accelerate antenna design.
The QDPSO algorithm autonomously optimized loop dimensions in 11.53 seconds,
achieving a resonance frequency of 1.4208 GHz a 12.7 percent reduction compared
to conventional 1.60 GHz designs. Machine learning models (SVM, Random Forest,
XGBoost, and Stacked ensembles) predicted resonance frequencies in 0.75 seconds
using 936 simulation datasets, with stacked models showing superior training
accuracy (R2=0.9825) and SVM demonstrating optimal validation performance
(R2=0.7197). The complete design cycle, encompassing optimization, prediction,
and ANSYS validation, required 12.42 minutes on standard desktop hardware
(Intel i5-8500, 16GB RAM), contrasting sharply with the 50-hour benchmark of
PSADEA-based approaches. This 240 times of acceleration eliminates traditional
trial-and-error methods that often extend beyond seven expert-led days. The
system enables precise specifications of performance targets with automated
generation of fabrication-ready parameters, particularly benefiting compact
consumer devices requiring rapid frequency tuning. By bridging AI-driven
optimization with CAD validation, this framework reduces engineering workloads
while ensuring production-ready designs, establishing a scalable paradigm for
next-generation RF systems in 6G and IoT applications.

</details>


### [211] [SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning](https://arxiv.org/abs/2505.22442)
*Mattie Fellows,Clarisse Wibault,Uljad Berdica,Johannes Forkel,Jakob N. Foerster,Michael A. Osborne*

Key words: 离线强化学习，样本效率，超参数调优，贝叶斯方法，信息率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了两种算法，SOReL和TOReL，用于解决离线强化学习中样本效率低和超参数调优问题，实现了仅通过离线数据预估在线性能并在真实世界中安全应用RL的目标。

Motivation: 为了解决离线强化学习中样本效率低和超参数调优依赖于在线交互的问题，论文旨在开发仅依赖离线数据就能预估在线性能并进行超参数调优的方法。

Method: 论文提出两种算法：SOReL通过贝叶斯方法从离线数据推断环境动态的后验分布以预估在线性能，TOReL则扩展了基于信息率的离线超参数调优方法，使其适用于通用离线RL方法。

Result: 实验表明，SOReL能准确预估贝叶斯设置中的遗憾，而TOReL的离线超参数调优性能与最佳在线调优方法相当。

Conclusion: SOReL和TOReL在实现安全可靠的离线RL方面迈出了重要一步，为RL在真实世界中的应用提供了潜力。

Abstract: Sample efficiency remains a major obstacle for real world adoption of
reinforcement learning (RL): success has been limited to settings where
simulators provide access to essentially unlimited environment interactions,
which in reality are typically costly or dangerous to obtain. Offline RL in
principle offers a solution by exploiting offline data to learn a near-optimal
policy before deployment. In practice, however, current offline RL methods rely
on extensive online interactions for hyperparameter tuning, and have no
reliable bound on their initial online performance. To address these two
issues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe
offline reinforcement learning. Using only offline data, our Bayesian approach
infers a posterior over environment dynamics to obtain a reliable estimate of
the online performance via the posterior predictive uncertainty. Crucially, all
hyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a
tuning for offline reinforcement learning algorithm that extends our
information rate based offline hyperparameter tuning methods to general offline
RL approaches. Our empirical evaluation confirms SOReL's ability to accurately
estimate regret in the Bayesian setting whilst TOReL's offline hyperparameter
tuning achieves competitive performance with the best online hyperparameter
tuning methods using only offline data. Thus, SOReL and TOReL make a
significant step towards safe and reliable offline RL, unlocking the potential
for RL in the real world. Our implementations are publicly available:
https://github.com/CWibault/sorel\_torel.

</details>


### [212] [Position: All Current Generative Fidelity and Diversity Metrics are Flawed](https://arxiv.org/abs/2505.22450)
*Ossi Räisä,Boris van Breugel,Mihaela van der Schaar*

Key words: 生成建模、合成数据、指标评估、保真度、多样性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文指出现有生成数据指标的不足，提出了理想指标的清单和一系列检查实验，呼吁社区更多关注指标而非模型的开发。

Motivation: 生成建模的广泛应用突显了对高质量合成数据指标的需求，但现有指标存在诸多问题，如缺乏异常鲁棒性和不明确的上下界，限制了方法的可靠性评估。

Method: 论文提出了一套理想指标的特征清单和一系列精心设计的简单实验（称为“检查实验”），用于检测生成模型的已知失败模式。

Result: 研究发现，所有当前的生成保真度和多样性指标均存在缺陷，严重影响了合成数据的实际应用。

Conclusion: 作者呼吁研究社区应更多地投入指标而非模型的开发，并通过分析现有指标的失败案例，为实践者提供了使用指南。

Abstract: Any method's development and practical application is limited by our ability
to measure its reliability. The popularity of generative modeling emphasizes
the importance of good synthetic data metrics. Unfortunately, previous works
have found many failure cases in current metrics, for example lack of outlier
robustness and unclear lower and upper bounds. We propose a list of desiderata
for synthetic data metrics, and a suite of sanity checks: carefully chosen
simple experiments that aim to detect specific and known generative modeling
failure modes. Based on these desiderata and the results of our checks, we
arrive at our position: all current generative fidelity and diversity metrics
are flawed. This significantly hinders practical use of synthetic data. Our aim
is to convince the research community to spend more effort in developing
metrics, instead of models. Additionally, through analyzing how current metrics
fail, we provide practitioners with guidelines on how these metrics should
(not) be used.

</details>


### [213] [Pure Exploration with Infinite Answers](https://arxiv.org/abs/2505.22473)
*Riccardo Poiani,Martino Bernasconi,Andrea Celli*

Key words: 纯探索、无限答案集、渐进最优、回归、Sticky-Sequence Track-and-Stop

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究纯探索问题，针对无限答案集的情况，提出新方法Sticky-Sequence Track-and-Stop，分析现有方法的局限性及其改进框架。

Motivation: 解决现有纯探索方法在无限答案集（如回归问题）中无法达到渐进最优的问题。

Method: 提出Sticky-Sequence Track-and-Stop框架，结合Track-and-Stop和Sticky Track-and-Stop，适用于更广泛的设置。

Result: 证明新方法在无限答案集中具渐进最优性，同时揭示现有方法在某些特殊情况下仍最优。

Conclusion: 新框架在保证通用性的前提下提升最优性，为相关领域提供更灵活的工具。

Abstract: We study pure exploration problems where the set of correct answers is
possibly infinite, e.g., the regression of any continuous function of the means
of the bandit. We derive an instance-dependent lower bound for these problems.
By analyzing it, we discuss why existing methods (i.e., Sticky Track-and-Stop)
for finite answer problems fail at being asymptotically optimal in this more
general setting. Finally, we present a framework, Sticky-Sequence
Track-and-Stop, which generalizes both Track-and-Stop and Sticky
Track-and-Stop, and that enjoys asymptotic optimality. Due to its generality,
our analysis also highlights special cases where existing methods enjoy
optimality.

</details>


### [214] [Forecasting Multivariate Urban Data via Decomposition and Spatio-Temporal Graph Analysis](https://arxiv.org/abs/2505.22474)
*Amirhossein Sohrabbeig,Omid Ardakanian,Petr Musilek*

Key words: 多元时间序列预测,图神经网络,城市数据,分解预处理,时空依赖

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于图神经网络（GNN）的多元时间序列预测模型，通过分解预处理捕获时空依赖关系，在现实数据集中验证了其有效性。

Motivation: 由于城市多元数据（如天气、空气污染、碳排放等）的复杂依赖性，传统预测方法难以应对，因此需要一种能捕获时空依赖的新模型。

Method: 采用GNN结合分解预处理（分离趋势、季节性和残差），增强预测的准确性和可解释性。

Result: 在电力使用、天气等多元数据中表现出优越性能，验证了模型的实际应用潜力。

Conclusion: 模型在智能基础设施优化和城市可持续发展中具有重要价值。

Abstract: The forecasting of multivariate urban data presents a complex challenge due
to the intricate dependencies between various urban metrics such as weather,
air pollution, carbon intensity, and energy demand. This paper introduces a
novel multivariate time-series forecasting model that utilizes advanced Graph
Neural Networks (GNNs) to capture spatial dependencies among different
time-series variables. The proposed model incorporates a decomposition-based
preprocessing step, isolating trend, seasonal, and residual components to
enhance the accuracy and interpretability of forecasts. By leveraging the
dynamic capabilities of GNNs, the model effectively captures interdependencies
and improves the forecasting performance. Extensive experiments on real-world
datasets, including electricity usage, weather metrics, carbon intensity, and
air pollution data, demonstrate the effectiveness of the proposed approach
across various forecasting scenarios. The results highlight the potential of
the model to optimize smart infrastructure systems, contributing to
energy-efficient urban development and enhanced public well-being.

</details>


### [215] [Non-Asymptotic Analysis of (Sticky) Track-and-Stop](https://arxiv.org/abs/2505.22475)
*Riccardo Poiani,Martino Bernasconi,Andrea Celli*

Key words: 纯探索, Track-and-Stop算法, Sticky Track-and-Stop算法, 非渐近保证, 随机环境

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文针对纯探索问题的非渐近保证进行了研究，分析了Track-and-Stop算法及其扩展Sticky Track-and-Stop算法在这些情况下的性能。

Motivation: 研究动机在于填补Track-and-Stop算法及其扩展Sticky Track-and-Stop算法在非渐近情况下的性能保证空白，特别是在存在多个正确答案的情况下。

Method: 论文分析了Track-and-Stop和Sticky Track-and-Stop算法在错误概率不超过$δ$的条件下，对未知随机环境的查询次数性能。

Result: 研究结果为这两种算法提供了非渐近的查询次数性能保证。

Conclusion: 结论是虽然这两种算法在渐近情况下是最优的，但论文填补了其在非渐近情况下的理论空白。

Abstract: In pure exploration problems, a statistician sequentially collects
information to answer a question about some stochastic and unknown environment.
The probability of returning a wrong answer should not exceed a maximum risk
parameter $\delta$ and good algorithms make as few queries to the environment
as possible. The Track-and-Stop algorithm is a pioneering method to solve these
problems. Specifically, it is well-known that it enjoys asymptotic optimality
sample complexity guarantees for $\delta\to 0$ whenever the map from the
environment to its correct answers is single-valued (e.g., best-arm
identification with a unique optimal arm). The Sticky Track-and-Stop algorithm
extends these results to settings where, for each environment, there might
exist multiple correct answers (e.g., $\epsilon$-optimal arm identification).
Although both methods are optimal in the asymptotic regime, their
non-asymptotic guarantees remain unknown. In this work, we fill this gap and
provide non-asymptotic guarantees for both algorithms.

</details>


### [216] [A Closer Look at Multimodal Representation Collapse](https://arxiv.org/abs/2505.22483)
*Abhra Chaudhuri,Anjan Dutta,Tu Bui,Serban Georgescu*

Key words: 模态崩溃, 多模态融合, 跨模态知识蒸馏, 显式基重分配

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了多模态融合中的模态崩溃现象，提出了一种通过显式基重分配防止模态崩溃的算法。

Motivation: 多模态融合模型往往会忽略某些模态，这种现象称为模态崩溃。论文旨在深入理解其成因并提出解决方案。

Method: 通过跨模态知识蒸馏解耦表示，并提出显式基重分配的算法来处理缺失模态问题。

Result: 在多个多模态基准测试中验证了理论主张，算法有效防止了模态崩溃。

Conclusion: 跨模态知识蒸馏和显式基重分配能有效解决模态崩溃问题。

Abstract: We aim to develop a fundamental understanding of modality collapse, a
recently observed empirical phenomenon wherein models trained for multimodal
fusion tend to rely only on a subset of the modalities, ignoring the rest. We
show that modality collapse happens when noisy features from one modality are
entangled, via a shared set of neurons in the fusion head, with predictive
features from another, effectively masking out positive contributions from the
predictive features of the former modality and leading to its collapse. We
further prove that cross-modal knowledge distillation implicitly disentangles
such representations by freeing up rank bottlenecks in the student encoder,
denoising the fusion-head outputs without negatively impacting the predictive
features from either modality. Based on the above findings, we propose an
algorithm that prevents modality collapse through explicit basis reallocation,
with applications in dealing with missing modalities. Extensive experiments on
multiple multimodal benchmarks validate our theoretical claims. Project page:
https://abhrac.github.io/mmcollapse/.

</details>


### [217] [Understanding Adversarial Training with Energy-based Models](https://arxiv.org/abs/2505.22486)
*Mujtaba Hussain Mirza,Maria Rosaria Briglia,Filippo Bartolucci,Senad Beadini,Giuseppe Lisanti,Iacopo Masi*

Key words: Energy-based Model, Adversarial Training, Catastrophic Overfitting, Robust Overfitting, Delta Energy Regularizer, Generative Modeling

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文通过能量模型（EBM）框架分析对抗训练（AT）中的灾难性过拟合（CO）和鲁棒过拟合（RO）现象，并提出一种新的正则化方法Delta Energy Regularizer（DER）来缓解这些问题。同时，论文还探讨了鲁棒分类器在生成任务中的局限性，并提出基于局部类主成分分析（PCA）和改进的停止策略，以提高生成样本的多样性和质量。

Motivation: 研究动机在于深入理解对抗训练中的灾难性过拟合（CO）和鲁棒过拟合（RO）现象，并通过能量视角分析它们的本质。同时，研究还关注鲁棒分类器的生成能力，探索其在生成任务中的潜力与局限性。

Method: 方法包括通过能量视角分析对抗样本与自然样本的能量差异，提出Delta Energy Regularizer（DER）作为新的正则化方法缓解CO和RO。在生成任务中，采用局部类主成分分析（PCA）和能量引导优化初始化和停止策略。

Result: 实验结果表明DER能有效缓解CO和RO问题。在生成任务中，提出的方法在不显式训练生成模型的情况下，取得了与混合判别-生成模型相当的Inception Score（IS）和Fr\'echet Inception Distance（FID）分数。

Conclusion: 结论是通过能量视角可以更好地理解对抗训练中的过拟合问题，DER是一种有效的正则化方法。同时，鲁棒分类器在生成任务中具有潜力，但需要通过技术改进来平衡生成质量和多样性。

Abstract: We aim at using Energy-based Model (EBM) framework to better understand
adversarial training (AT) in classifiers, and additionally to analyze the
intrinsic generative capabilities of robust classifiers. By viewing standard
classifiers through an energy lens, we begin by analyzing how the energies of
adversarial examples, generated by various attacks, differ from those of the
natural samples. The central focus of our work is to understand the critical
phenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT
from an energy perspective. We analyze the impact of existing AT approaches on
the energy of samples during training and observe that the behavior of the
``delta energy' -- change in energy between original sample and its adversarial
counterpart -- diverges significantly when CO or RO occurs. After a thorough
analysis of these energy dynamics and their relationship with overfitting, we
propose a novel regularizer, the Delta Energy Regularizer (DER), designed to
smoothen the energy landscape during training. We demonstrate that DER is
effective in mitigating both CO and RO across multiple benchmarks. We further
show that robust classifiers, when being used as generative models, have limits
in handling trade-off between image quality and variability. We propose an
improved technique based on a local class-wise principal component analysis
(PCA) and energy-based guidance for better class-specific initialization and
adaptive stopping, enhancing sample diversity and generation quality.
Considering that we do not explicitly train for generative modeling, we achieve
a competitive Inception Score (IS) and Fr\'echet inception distance (FID)
compared to hybrid discriminative-generative models.

</details>


### [218] [On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling](https://arxiv.org/abs/2505.22491)
*Moritz Haas,Sebastian Bordt,Ulrike von Luxburg,Leena Chennuru Vankadara*

Key words: 标准参数化、跨熵损失、无限宽度理论、受控发散、学习率、特征学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文分析了在跨熵损失（CE）下，标准参数化（SP）训练大规模视觉和语言模型时的动力学理论，揭示了在大学习率下的稳定训练机制——‘受控发散’（controlled divergence）现象。

Motivation: 尽管标准参数化在实践中取得了成功，但其理论解释与现有无限宽度理论的预测存在矛盾，特别是学习率选择的不匹配现象。论文旨在通过研究训练动力学，解释这一矛盾的根本原因，并揭示为什么跨熵损失比均方误差（MSE）更有效。

Method: 通过对神经网络训练动力学的详细研究，特别是对跨熵损失和均方误差损失的对比分析，论文证明了‘受控发散’现象的存在，并在多种优化器（SGD、Adam）、架构（MLP、GPT）和数据模态（视觉、语言）上进行了验证。

Result: 实验证明，在跨熵损失下，神经网络的训练会进入受控发散状态，使得大学习率下的训练依然稳定，并持续更新所有隐藏层的特征。这一现象的普遍性在不同设置下均得到验证。此外，研究还表明，宽度缩放理论对实际学习率选择的预测具有指导意义。

Conclusion: 跨熵损失通过受控发散机制，实现了大学习率下的稳定训练和持续特征更新，解决了标准参数化在理论预测中的矛盾。该结论提供了对标准初始化方法实践效果的理论解释。

Abstract: The dominant paradigm for training large-scale vision and language models is
He initialization and a single global learning rate (\textit{standard
parameterization}, SP). Despite its practical success, standard parametrization
remains poorly understood from a theoretical perspective: Existing
infinite-width theory would predict instability under large learning rates and
vanishing feature learning under stable learning rates. However, empirically
optimal learning rates consistently decay much slower than theoretically
predicted. By carefully studying neural network training dynamics, we
demonstrate that this discrepancy is not fully explained by finite-width
phenomena such as catapult effects or a lack of alignment between weights and
incoming activations. We instead show that the apparent contradiction can be
fundamentally resolved by taking the loss function into account: In contrast to
Mean Squared Error (MSE) loss, we prove that under cross-entropy (CE) loss, an
intermediate \textit{controlled divergence} regime emerges, where logits
diverge but loss, gradients, and activations remain stable. Stable training
under large learning rates enables persistent feature evolution at scale in all
hidden layers, which is crucial for the practical success of SP. In experiments
across optimizers (SGD, Adam), architectures (MLPs, GPT) and data modalities
(vision, language), we validate that neural networks operate in this controlled
divergence regime under CE loss but not under MSE loss. Our empirical evidence
suggests that width-scaling considerations are surprisingly useful for
predicting empirically optimal learning rate exponents. Finally, our analysis
clarifies the effectiveness and limitations of recently proposed layerwise
learning rate scalings for standard initialization.

</details>


### [219] [Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation](https://arxiv.org/abs/2505.22492)
*Hongyi Zhou,Josiah P. Hanna,Jin Zhu,Ying Yang,Chengchun Shi*

Key words: 离策略评估, 强化学习, 重要性采样, 行为策略估计, 偏差-方差分解

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了强化学习中的离策略评估（OPE），重点探讨通过行为策略估计来优化重要性采样。研究发现，即使真实行为策略是马尔可夫的，估计历史依赖的行为策略也能降低均方误差（MSE）。论文通过理论推导揭示了这一现象的原因：历史依赖策略降低了渐近方差，但增加了有限样本偏差。结果还适用于其他OPE估计器。

Motivation: 先前研究表明，使用历史依赖的行为策略可以降低MSE，但其原因尚不明确。本文旨在通过理论分析揭示这一现象背后的机制。

Method: 通过偏差-方差分解分析了普通重要性采样（IS）估计器的MSE，并扩展至序列IS、双稳健估计器等，对比了参数和非参数方法的行为策略估计。

Result: 理论证明，历史依赖的行为策略估计降低了渐近方差，但增加了有限样本偏差。随着历史长度增加，方差持续下降。结论适用于多种OPE估计器。

Conclusion: 历史依赖的行为策略估计在OPE中具有理论优势，但其偏差-方差权衡需在实践中权衡。

Abstract: This paper studies off-policy evaluation (OPE) in reinforcement learning with
a focus on behavior policy estimation for importance sampling. Prior work has
shown empirically that estimating a history-dependent behavior policy can lead
to lower mean squared error (MSE) even when the true behavior policy is
Markovian. However, the question of why the use of history should lower MSE
remains open. In this paper, we theoretically demystify this paradox by
deriving a bias-variance decomposition of the MSE of ordinary importance
sampling (IS) estimators, demonstrating that history-dependent behavior policy
estimation decreases their asymptotic variances while increasing their
finite-sample biases. Additionally, as the estimated behavior policy conditions
on a longer history, we show a consistent decrease in variance. We extend these
findings to a range of other OPE estimators, including the sequential IS
estimator, the doubly robust estimator and the marginalized IS estimator, with
the behavior policy estimated either parametrically or non-parametrically.

</details>


### [220] [ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods](https://arxiv.org/abs/2505.22494)
*Michal Kmicikiewicz,Vincent Fortuin,Ewa Szczurek*

Key words: 蛋白质工程设计，主动学习，生成模型，适应性，新颖性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: ProSpro是一个基于主动学习的框架，通过冻结的预训练生成模型和更新的代理模型结合，能够高效设计具有高适应性和新颖性的蛋白质序列。

Motivation: 目前，在数据高效的蛋白质工程中，设计兼具高适应性和新颖性的蛋白质序列是一个挑战。探索野生型以外的区域通常会导致生物学不合理性，或依赖在新型区域中失去保真度的代理模型。ProSpro的目标是解决这些限制。

Method: ProSpro采用主动学习框架，结合冻结的预训练生成模型和通过模拟反馈更新的代理模型。此外，该框架整合了适应度相关残基选择与生物学约束的Sequential Monte Carlo采样，实现了既超越野生型区域又保持生物学合理性的探索。

Result: 即使在代理模型设定错误的情况下，ProSpro依然有效。在各种蛋白质工程任务中，ProSpro的表现优于或匹配现有方法，能够获得同时具备高适应性和新颖性的序列。

Conclusion: ProSpro证明了在蛋白质序列设计中，通过结合生成模型和主动学习代理模型，可以在保证生物学合理性的同时实现高效探索，具有广泛的应用潜力。

Abstract: Designing protein sequences of both high fitness and novelty is a challenging
task in data-efficient protein engineering. Exploration beyond wild-type
neighborhoods often leads to biologically implausible sequences or relies on
surrogate models that lose fidelity in novel regions. Here, we propose
ProSpero, an active learning framework in which a frozen pre-trained generative
model is guided by a surrogate updated from oracle feedback. By integrating
fitness-relevant residue selection with biologically-constrained Sequential
Monte Carlo sampling, our approach enables exploration beyond wild-type
neighborhoods while preserving biological plausibility. We show that our
framework remains effective even when the surrogate is misspecified. ProSpero
consistently outperforms or matches existing methods across diverse protein
engineering tasks, retrieving sequences of both high fitness and novelty.

</details>


### [221] [Geometric GNNs for Charged Particle Tracking at GlueX](https://arxiv.org/abs/2505.22504)
*Ahmed Hossam Mohammed,Kishansingh Rajput,Simon Taylor,Denis Furletov,Sergey Furletov,Malachi Schram*

Key words: Graph Neural Networks, track finding, nuclear physics, GPU, FPGA

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了如何使用图神经网络（GNN）改进高能核物理实验中的粒子轨迹重建，证明其在效率和速度上优于传统方法，并比较了GPU和FPGA的实现效果。

Motivation: 传统组合方法在粒子轨迹重建中计算复杂度高，而GNN因其对3维点云数据的高效处理能力成为潜在解决方案。

Method: 用模拟数据训练GNN模型，并在模拟和真实GlueX实验数据上测试，比较其与传统方法的性能。

Result: GNN在固定纯度下提高了效率，并利用GPU的并行计算实现了更快的推理速度。

Conclusion: GNN在高能物理实验中的轨迹重建任务中具有显著优势，尤其在处理大批量数据时。

Abstract: Nuclear physics experiments are aimed at uncovering the fundamental building
blocks of matter. The experiments involve high-energy collisions that produce
complex events with many particle trajectories. Tracking charged particles
resulting from collisions in the presence of a strong magnetic field is
critical to enable the reconstruction of particle trajectories and precise
determination of interactions. It is traditionally achieved through
combinatorial approaches that scale worse than linearly as the number of hits
grows. Since particle hit data naturally form a 3-dimensional point cloud and
can be structured as graphs, Graph Neural Networks (GNNs) emerge as an
intuitive and effective choice for this task. In this study, we evaluate the
GNN model for track finding on the data from the GlueX experiment at Jefferson
Lab. We use simulation data to train the model and test on both simulation and
real GlueX measurements. We demonstrate that GNN-based track finding
outperforms the currently used traditional method at GlueX in terms of
segment-based efficiency at a fixed purity while providing faster inferences.
We show that the GNN model can achieve significant speedup by processing
multiple events in batches, which exploits the parallel computation capability
of Graphical Processing Units (GPUs). Finally, we compare the GNN
implementation on GPU and FPGA and describe the trade-off.

</details>


### [222] [Sparsification and Reconstruction from the Perspective of Representation Geometry](https://arxiv.org/abs/2505.22506)
*Wenjie Sun,Bingzhe Wu,Zhile Yang,Chengke Wu*

Key words: 稀疏自编码器, 语言模型, 表示几何, 特征解缠, 重建性能

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: SAEMA研究稀疏自编码器（SAEs）如何组织语言模型的激活向量表示，探讨稀疏编码与特征解缠及重建性能的关系，提出全局和局部表示的定义，并证明其可分离性与重建性能的因果关系。

Motivation: 探究稀疏编码如何改变表示结构，以及这种结构与特征解缠和重建性能的关系，为开发可解释工具和改进SAEs提供理论支持。

Method: 提出SAEMA方法，通过对模态张量展开的SSPD矩阵秩的变化进行分析，定义全局和局部表示，并从优化角度干预全局表示。

Result: 稀疏编码通过合并相似语义特征并引入额外维度放大特征间差异，全局表示的可分离性与重建性能显著相关。

Conclusion: 研究从表示几何角度解释稀疏性原理，强调理解表示和引入表示约束的必要性，为改进SAEs提供参考。

Abstract: Sparse Autoencoders (SAEs) have emerged as a predominant tool in mechanistic
interpretability, aiming to identify interpretable monosemantic features.
However, how does sparse encoding organize the representations of activation
vector from language models? What is the relationship between this
organizational paradigm and feature disentanglement as well as reconstruction
performance? To address these questions, we propose the SAEMA, which validates
the stratified structure of the representation by observing the variability of
the rank of the symmetric semipositive definite (SSPD) matrix corresponding to
the modal tensor unfolded along the latent tensor with the level of noise added
to the residual stream. To systematically investigate how sparse encoding
alters representational structures, we define local and global representations,
demonstrating that they amplify inter-feature distinctions by merging similar
semantic features and introducing additional dimensionality. Furthermore, we
intervene the global representation from an optimization perspective, proving a
significant causal relationship between their separability and the
reconstruction performance. This study explains the principles of sparsity from
the perspective of representational geometry and demonstrates the impact of
changes in representational structure on reconstruction performance.
Particularly emphasizes the necessity of understanding representations and
incorporating representational constraints, providing empirical references for
developing new interpretable tools and improving SAEs. The code is available at
\hyperlink{https://github.com/wenjie1835/SAERepGeo}{https://github.com/wenjie1835/SAERepGeo}.

</details>


### [223] [Accelerating Optimization via Differentiable Stopping Time](https://arxiv.org/abs/2505.22509)
*Zhonglin Xie,Yiman Fong,Haoran Yuan,Zaiwen Wen*

Key words: 可微分优化, 停止时间, 微分方程, 超参数调优, 学习优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种可微分的停止时间理论，旨在解决传统优化算法中时间不可微的问题，通过微分方程实现了高效的反向传播，并在在线超参数调优和学习优化中展示了优越性能。

Motivation: 传统优化问题通常以给定时间内最小化损失为目标，其双问题（即最小化达到目标损失的时间）因时间不可微而难以直接优化。本研究旨在解决这一限制。

Method: 基于微分方程理论，提出了一种可微分的停止时间框架，并设计了高效的反向传播算法进行优化。

Result: 在多种问题上的实验表明，所提方法在在线超参数调优和学习优化任务中性能显著优于传统方法。

Conclusion: 可微分停止时间为优化算法提供了新的可微公式，解决了时间不可微的难题，具有广泛的应用潜力。

Abstract: Optimization is an important module of modern machine learning applications.
Tremendous efforts have been made to accelerate optimization algorithms. A
common formulation is achieving a lower loss at a given time. This enables a
differentiable framework with respect to the algorithm hyperparameters. In
contrast, its dual, minimizing the time to reach a target loss, is believed to
be non-differentiable, as the time is not differentiable. As a result, it
usually serves as a conceptual framework or is optimized using zeroth-order
methods. To address this limitation, we propose a differentiable stopping time
and theoretically justify it based on differential equations. An efficient
algorithm is designed to backpropagate through it. As a result, the proposed
differentiable stopping time enables a new differentiable formulation for
accelerating algorithms. We further discuss its applications, such as online
hyperparameter tuning and learning to optimize. Our proposed methods show
superior performance in comprehensive experiments across various problems,
which confirms their effectiveness.

</details>


### [224] [Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data](https://arxiv.org/abs/2505.22521)
*Chao Wang,Chuanhao Nie,Yunbo Liu*

Key words: 欺诈检测, 监督学习, 不平衡数据集, 集成方法, GRU

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 比较了逻辑回归、随机森林、LightGBM和GRU在欺诈检测中的性能，发现集成方法表现最佳，但GRU对小类欺诈有较高召回率。

Motivation: 欺诈检测对金融和电商至关重要，未检测到的欺诈可能导致重大经济损失。

Method: 系统比较了四种监督学习模型（逻辑回归、随机森林、LightGBM和GRU）在大规模不平衡数据集上的表现。

Result: 集成方法（随机森林和LightGBM）整体表现最优，逻辑回归提供可靠且可解释的基线，GRU对小类欺诈召回率高但精度低。

Conclusion: 模型选择需结合具体风险容忍度和操作需求。

Abstract: Fraud detection remains a critical task in high-stakes domains such as
finance and e-commerce, where undetected fraudulent transactions can lead to
significant economic losses. In this study, we systematically compare the
performance of four supervised learning models - Logistic Regression, Random
Forest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit
(GRU) network - on a large-scale, highly imbalanced online transaction dataset.
While ensemble methods such as Random Forest and LightGBM demonstrated superior
performance in both overall and class-specific metrics, Logistic Regression
offered a reliable and interpretable baseline. The GRU model showed strong
recall for the minority fraud class, though at the cost of precision,
highlighting a trade-off relevant for real-world deployment. Our evaluation
emphasizes not only weighted averages but also per-class precision, recall, and
F1-scores, providing a nuanced view of each model's effectiveness in detecting
rare but consequential fraudulent activity. The findings underscore the
importance of choosing models based on the specific risk tolerance and
operational needs of fraud detection systems.

</details>


### [225] [Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo](https://arxiv.org/abs/2505.22524)
*Chinmay Pani,Zijing Ou,Yingzhen Li*

Key words: 离散扩散模型, 序贯蒙特卡洛, Gumbel-Softmax, 无训练生成

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于序贯蒙特卡洛（SMC）的训练自由方法，通过近似局部最优提案和Gumbel-Softmax松弛在离散空间生成符合约束的样本。

Motivation: 现实应用中生成过程需满足约束但无需任务特定微调，需一种高效的无训练方法。

Method: 采用扭曲SMC，结合一阶泰勒展开近似局部最优提案，并引入Gumbel-Softmax松弛解决离散空间梯度问题。

Result: 在合成数据集和图像建模中验证了方法的有效性。

Conclusion: 该方法无需训练即可生成符合约束的样本，具有实际应用潜力。

Abstract: Discrete diffusion models have become highly effective across various
domains. However, real-world applications often require the generative process
to adhere to certain constraints but without task-specific fine-tuning. To this
end, we propose a training-free method based on Sequential Monte Carlo (SMC) to
sample from the reward-aligned target distribution at the test time. Our
approach leverages twisted SMC with an approximate locally optimal proposal,
obtained via a first-order Taylor expansion of the reward function. To address
the challenge of ill-defined gradients in discrete spaces, we incorporate a
Gumbel-Softmax relaxation, enabling efficient gradient-based approximation
within the discrete generative framework. Empirical results on both synthetic
datasets and image modelling validate the effectiveness of our approach.

</details>


### [226] [Training RL Agents for Multi-Objective Network Defense Tasks](https://arxiv.org/abs/2505.22531)
*Andres Molina-Markham,Luis Robaina,Sean Steinle,Akash Trivedi,Derek Tsui,Nicholas Potteiger,Lauren Brandt,Ransom Winder,Ahmed Ridley*

Key words: 开放学习(OEL), 网络安全, 自主代理, 任务表示, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出一种受开放学习(OEL)启发的训练方法，用于开发自主网络防御系统，解决了任务表示一致性问题，验证了OEL原则能增强网络防御的鲁棒性和泛化性。

Motivation: 当前虽OEL在人工智能领域显示潜力，但其在网络安全中的实际应用仍面临挑战，需解决任务接口一致性等技术难题。

Method: 采用OEL原则，设计统一的任务表示方法，覆盖多样化网络条件、攻击行为及防御目标，以支持知识的累积训练。

Result: 实验表明OEL可提升网络防御代理的鲁棒性和泛化能力，为网络安全领域的AI研究提供新工具。

Conclusion: 研究证明了OEL在网络防御中的有效性，并呼吁未来开发网络安全基准时需采用多样性任务与一致表示。

Abstract: Open-ended learning (OEL) -- which emphasizes training agents that achieve
broad capability over narrow competency -- is emerging as a paradigm to develop
artificial intelligence (AI) agents to achieve robustness and generalization.
However, despite promising results that demonstrate the benefits of OEL,
applying OEL to develop autonomous agents for real-world cybersecurity
applications remains a challenge.
  We propose a training approach, inspired by OEL, to develop autonomous
network defenders. Our results demonstrate that like in other domains, OEL
principles can translate into more robust and generalizable agents for cyber
defense. To apply OEL to network defense, it is necessary to address several
technical challenges. Most importantly, it is critical to provide a task
representation approach over a broad universe of tasks that maintains a
consistent interface over goals, rewards and action spaces. This way, the
learning agent can train with varying network conditions, attacker behaviors,
and defender goals while being able to build on previously gained knowledge.
  With our tools and results, we aim to fundamentally impact research that
applies AI to solve cybersecurity problems. Specifically, as researchers
develop gyms and benchmarks for cyber defense, it is paramount that they
consider diverse tasks with consistent representations, such as those we
propose in our work.

</details>


### [227] [TabularQGAN: A Quantum Generative Model for Tabular Data](https://arxiv.org/abs/2505.22533)
*Pallavi Bhardwaj,Caitlin Jones,Lasse Dierich,Aleksandar Vučković*

Key words: quantum generative model, tabular data, synthetic data, quantum GAN

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 这篇论文提出了一种新型的量子生成模型，用于合成表格数据。该模型在稀缺或私密的真实数据场景中表现优异，尤其在处理混合分类和数值特征的异构数据时。通过量子生成对抗网络架构和创新的量子电路设计，模型在MIMIC III和Adult Census数据集上表现优于经典模型，相似度得分平均提升8.5%，且参数量仅占经典模型的0.072%。

Motivation: 现实企业数据多为表格形式且异构性强，而真实数据往往稀缺或涉及隐私。合成数据可用于补充或替代现有数据集，尤其在医疗、金融和软件领域需求迫切。

Method: 采用量子生成对抗网络，结合灵活的数据编码和新型量子电路ansatz，有效建模表格数据。

Result: 在MIMIC III和Adult Census数据集上，量子模型相似度得分平均比经典模型（如CTGAN和CopulaGAN）高8.5%，参数量仅为后者的0.072%。

Conclusion: 量子生成模型在表格数据合成任务上表现优异，表明量子计算机可能特别适合此类任务。

Abstract: In this paper, we introduce a novel quantum generative model for synthesizing
tabular data. Synthetic data is valuable in scenarios where real-world data is
scarce or private, it can be used to augment or replace existing datasets.
Real-world enterprise data is predominantly tabular and heterogeneous, often
comprising a mixture of categorical and numerical features, making it highly
relevant across various industries such as healthcare, finance, and software.
We propose a quantum generative adversarial network architecture with flexible
data encoding and a novel quantum circuit ansatz to effectively model tabular
data. The proposed approach is tested on the MIMIC III healthcare and Adult
Census datasets, with extensive benchmarking against leading classical models,
CTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model
outperforms classical models by an average of 8.5% with respect to an overall
similarity score from SDMetrics, while using only 0.072% of the parameters of
the classical models. Additionally, we evaluate the generalization capabilities
of the models using two custom-designed metrics that demonstrate the ability of
the proposed quantum model to generate useful and novel samples. To our
knowledge, this is one of the first demonstrations of a successful quantum
generative model for handling tabular data, indicating that this task could be
well-suited to quantum computers.

</details>


### [228] [Uncertainty Quantification with Proper Scoring Rules: Adjusting Measures to Prediction Tasks](https://arxiv.org/abs/2505.22538)
*Paul Hofman,Yusuf Sale,Eyke Hüllermeier*

Key words: 不确定性量化、评分规则、选择性预测、分布外检测、主动学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于严格评分规则分解的不确定性量化框架，可适应不同任务需求，实验表明其在选择性预测、分布外检测和主动学习中表现优越。

Motivation: 解决不确定性量化的问题，提出一个灵活框架以适应不同任务需求，并通过实验验证其有效性。

Method: 基于严格评分规则的分解（分歧和熵分量），构建总、偶然和认知不确定性的度量框架。

Result: 在选择性预测任务中，匹配任务损失的评分规则效果最佳；分布外检测任务中互信息表现最优；主动学习中基于零一损失的认知不确定性度量优于其他方法。

Conclusion: 提出的灵活性框架能有效适应不同任务，实验证明其优越性。

Abstract: We address the problem of uncertainty quantification and propose measures of
total, aleatoric, and epistemic uncertainty based on a known decomposition of
(strictly) proper scoring rules, a specific type of loss function, into a
divergence and an entropy component. This leads to a flexible framework for
uncertainty quantification that can be instantiated with different losses
(scoring rules), which makes it possible to tailor uncertainty quantification
to the use case at hand. We show that this flexibility is indeed advantageous.
In particular, we analyze the task of selective prediction and show that the
scoring rule should ideally match the task loss. In addition, we perform
experiments on two other common tasks. For out-of-distribution detection, our
results confirm that a widely used measure of epistemic uncertainty, mutual
information, performs best. Moreover, in the setting of active learning, our
measure of epistemic uncertainty based on the zero-one-loss consistently
outperforms other uncertainty measures.

</details>


### [229] [A Human-Centric Approach to Explainable AI for Personalized Education](https://arxiv.org/abs/2505.22541)
*Vinitra Swamy*

Key words: 可解释AI, 教育, 多模态, 人类研究, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了深度学习在教育中的应用，提出了四种提高可解释性的技术方法，并结合人类研究验证其效果。

Motivation: 目前深度学习在教育中的实际应用有限，主要因为模型缺乏可解释性，导致师生信任不足。论文旨在通过可解释AI（XAI）技术解决这一问题。

Method: 结合多模态模块化架构（MultiModN）、可解释的专家混合模型（InterpretCC）、对抗训练和理论驱动的LLM-XAI框架（iLLuMinaTE），并通过实证评估和人类研究验证。

Result: 提出的方法在多种教育场景中验证有效，平衡了最新技术性能和内在透明度。

Conclusion: 论文为以人为中心的AI系统奠定了基础，结合技术进展与人类需求，推动可解释AI在教育中的应用。

Abstract: Deep neural networks form the backbone of artificial intelligence research,
with potential to transform the human experience in areas ranging from
autonomous driving to personal assistants, healthcare to education. However,
their integration into the daily routines of real-world classrooms remains
limited. It is not yet common for a teacher to assign students individualized
homework targeting their specific weaknesses, provide students with instant
feedback, or simulate student responses to a new exam question. While these
models excel in predictive performance, this lack of adoption can be attributed
to a significant weakness: the lack of explainability of model decisions,
leading to a lack of trust from students, parents, and teachers. This thesis
aims to bring human needs to the forefront of eXplainable AI (XAI) research,
grounded in the concrete use case of personalized learning and teaching. We
frame the contributions along two verticals: technical advances in XAI and
their aligned human studies. We investigate explainability in AI for education,
revealing systematic disagreements between post-hoc explainers and identifying
a need for inherently interpretable model architectures. We propose four novel
technical contributions in interpretability with a multimodal modular
architecture (MultiModN), an interpretable mixture-of-experts model
(InterpretCC), adversarial training for explainer stability, and a
theory-driven LLM-XAI framework to present explanations to students
(iLLuMinaTE), which we evaluate in diverse settings with professors, teachers,
learning scientists, and university students. By combining empirical
evaluations of existing explainers with novel architectural designs and human
studies, our work lays a foundation for human-centric AI systems that balance
state-of-the-art performance with built-in transparency and trust.

</details>


### [230] [DES-LOC: Desynced Low Communication Adaptive Optimizers for Training Foundation Models](https://arxiv.org/abs/2505.22549)
*Alex Iacob,Lorenzo Sani,Mher Safaryan,Paris Giampouras,Samuel Horváth,Andrej Jovanovic,Meghdad Kurmanji,Preslav Aleksandrov,William F. Shen,Xinchi Qiu,Nicholas D. Lane*

Key words: 分布式训练, 自适应优化器, 通信效率, 收敛性, 容错性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: DES-LOC 是一种分布式优化器，通过独立同步参数和动量来降低通信成本，同时保持收敛性，适用于大规模模型训练。

Motivation: 现有分布式数据并行（DDP）方法的带宽限制及 Local SGD 无法直接应用于自适应优化器的问题推动了 DES-LOC 的提出。

Method: DES-LOC 为参数和动量分配独立的同步周期，减少通信成本，同时保证收敛性。

Result: 实验表明，DES-LOC 比 DDP 减少 170 倍通信量，比 Local ADAM 减少 2 倍，且具有容错能力。

Conclusion: DES-LOC 是一种可扩展、带宽高效且容错的解决方案，适用于基础模型训练。

Abstract: Scaling foundation model training with Distributed Data Parallel (DDP)
methods is bandwidth-limited. Existing infrequent communication methods like
Local SGD were designed to synchronize only model parameters and cannot be
trivially applied to adaptive optimizers due to additional optimizer states.
Current approaches extending Local SGD either lack convergence guarantees or
require synchronizing all optimizer states, tripling communication costs. We
propose Desynced Low Communication Adaptive Optimizers (DES-LOC), a family of
optimizers assigning independent synchronization periods to parameters and
momenta, enabling lower communication costs while preserving convergence.
Through extensive experiments on language models of up to 1.7B, we show that
DES-LOC can communicate 170x less than DDP and 2x less than the previous
state-of-the-art Local ADAM. Furthermore, unlike previous heuristic approaches,
DES-LOC is suited for practical training scenarios prone to system failures.
DES-LOC offers a scalable, bandwidth-efficient, and fault-tolerant solution for
foundation model training.

</details>


### [231] [Geometric Hyena Networks for Large-scale Equivariant Learning](https://arxiv.org/abs/2505.22560)
*Artem Moskalev,Mangal Prakash,Junjie Xu,Tianyu Cui,Rui Liao,Tommaso Mansi*

Key words: 等变性, 长卷积, 几何建模, RNA, 蛋白质动力学

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为Geometric Hyena的新模型，首次将长卷积与等变性结合用于几何系统建模，以次二次复杂度处理全局几何信息，并在RNA分子和蛋白质动力学预测中性能优于现有方法。

Motivation: 处理全局几何信息时保持等变性对生物、化学和物理系统建模至关重要，但传统方法（如等变自注意力）计算复杂度过高，而局部方法（如距离消息传递）会牺牲全局信息。

Method: 引入Geometric Hyena，一种结合状态空间和长卷积的等变性长卷积模型，能够在次二次复杂度下处理全局几何信息并保持旋转和平移等变性。

Result: 在大RNA分子和全蛋白质分子动力学预测任务中，Geometric Hyena性能优于现有等变模型，计算效率显著提升（例如处理3万token时速度快20倍）。

Conclusion: Geometric Hyena为几何系统建模提供了一种高效且性能优越的解决方案，具有显著的计算和内存优势。

Abstract: Processing global geometric context while preserving equivariance is crucial
when modeling biological, chemical, and physical systems. Yet, this is
challenging due to the computational demands of equivariance and global context
at scale. Standard methods such as equivariant self-attention suffer from
quadratic complexity, while local methods such as distance-based message
passing sacrifice global information. Inspired by the recent success of
state-space and long-convolutional models, we introduce Geometric Hyena, the
first equivariant long-convolutional model for geometric systems. Geometric
Hyena captures global geometric context at sub-quadratic complexity while
maintaining equivariance to rotations and translations. Evaluated on all-atom
property prediction of large RNA molecules and full protein molecular dynamics,
Geometric Hyena outperforms existing equivariant models while requiring
significantly less memory and compute that equivariant self-attention. Notably,
our model processes the geometric context of 30k tokens 20x faster than the
equivariant transformer and allows 72x longer context within the same budget.

</details>


### [232] [FNOPE: Simulation-based inference on function spaces with Fourier Neural Operators](https://arxiv.org/abs/2505.22573)
*Guy Moss,Leah Sophie Muhle,Reinhard Drews,Jakob H. Macke,Cornelius Schröder*

Key words: Simulation-based inference, Fourier Neural Operator, function-valued parameters, spatiotemporal modeling, posterior estimation

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: FNOPE方法通过傅里叶神经算子和流匹配目标，高效估计函数值参数的后验分布，显著降低计算成本，扩展了SBI的适用领域。

Motivation: 传统SBI方法在低维参数模型上表现良好，但在函数值参数（如时空过程建模）中效率低下，限制了其在气候和地球科学等领域的应用。

Method: 采用傅里叶神经算子（FNO）架构和流匹配目标，实现函数值参数的高效后验估计（FNOPE），支持任意网格离散化和向量值参数同时估计。

Result: FNOPE在基准任务和冰川学空间推断任务中表现优异，计算成本仅为现有方法的极小部分。

Conclusion: FNOPE成功将SBI方法扩展到函数值参数推断，为科学领域提供了新的工具。

Abstract: Simulation-based inference (SBI) is an established approach for performing
Bayesian inference on scientific simulators. SBI so far works best on
low-dimensional parametric models. However, it is difficult to infer
function-valued parameters, which frequently occur in disciplines that model
spatiotemporal processes such as the climate and earth sciences. Here, we
introduce an approach for efficient posterior estimation, using a Fourier
Neural Operator (FNO) architecture with a flow matching objective. We show that
our approach, FNOPE, can perform inference of function-valued parameters at a
fraction of the simulation budget of state of the art methods. In addition,
FNOPE supports posterior evaluation at arbitrary discretizations of the domain,
as well as simultaneous estimation of vector-valued parameters. We demonstrate
the effectiveness of our approach on several benchmark tasks and a challenging
spatial inference task from glaciology. FNOPE extends the applicability of SBI
methods to new scientific domains by enabling the inference of function-valued
parameters.

</details>


### [233] [Benignity of loss landscape with weight decay requires both large overparametrization and initialization](https://arxiv.org/abs/2505.22578)
*Etienne Boursier,Matthew Bowditch,Matthias Englert,Ranko Lazic*

Key words: 神经网络, 权重衰减, 损失函数, 过参数化, ReLU网络, 初始化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了带权重衰减的神经网络优化问题，发现在大规模过参数化下，损失函数会变得无害，即不存在虚假局部最小值。同时指出过参数化的必要性，并揭示了初始化大小对优化结果的影响。

Motivation: 权重衰减是现代神经网络训练的常规实践，但其理论理解仍不充分。本文旨在从理论角度探讨带有ℓ2正则化的两层ReLU网络的损失函数特性。

Method: 研究团队分析了ℓ2正则化损失函数的景观，特别是在网络宽度满足特定条件（m≳min(n^d, 2^n)）时的情形，并区分了大初始化和小初始化的不同影响。

Result: 发现在大规模过参数化下，损失函数的景观变得无害，几乎所有常数激活区域都包含全局最小值且无虚假局部最小值。但小初始化时优化仍可能收敛到虚假局部最小值。

Conclusion: 该研究为权重衰减的理论分析提供了新见解，同时也强调了初始化和过参数化在实际训练中的重要性。

Abstract: The optimization of neural networks under weight decay remains poorly
understood from a theoretical standpoint. While weight decay is standard
practice in modern training procedures, most theoretical analyses focus on
unregularized settings. In this work, we investigate the loss landscape of the
$\ell_2$-regularized training loss for two-layer ReLU networks. We show that
the landscape becomes benign -- i.e., free of spurious local minima -- under
large overparametrization, specifically when the network width $m$ satisfies $m
\gtrsim \min(n^d, 2^n)$, where $n$ is the number of data points and $d$ the
input dimension. More precisely in this regime, almost all constant activation
regions contain a global minimum and no spurious local minima. We further show
that this level of overparametrization is not only sufficient but also
necessary via the example of orthogonal data. Finally, we demonstrate that such
loss landscape results primarily hold relevance in the large initialization
regime. In contrast, for small initializations -- corresponding to the feature
learning regime -- optimization can still converge to spurious local minima,
despite the global benignity of the landscape.

</details>


### [234] [Machine Unlearning under Overparameterization](https://arxiv.org/abs/2505.22601)
*Jacob L. Block,Aryan Mokhtari,Sanjay Shakkottai*

Key words: 机器遗忘, 过参数化, 最小复杂性插值, 梯度扰动, 正则化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了在过参数化设置下的机器遗忘问题，提出了新的遗忘定义和算法，以最小复杂性插值为目标，并通过约束正交梯度的扰动优化实现遗忘，实验证明其优于现有方法。

Motivation: 在过参数化设置中，许多模型能插值数据，传统遗忘方法基于梯度扰动在此场景下失效，因此需要新的遗忘定义和算法来解决这一问题。

Method: 提出将遗忘解定义为保留数据上的最小复杂性插值器，设计了基于保留集梯度的新框架，通过正交梯度扰动的正则化目标优化。

Result: 在不同模型类别下提供了精确和近似的遗忘保证，实验表明该方法在多个遗忘任务中优于现有基线。

Conclusion: 在过参数化设置中，最小复杂性插值和正交梯度约束的扰动是实现有效机器遗忘的关键，新方法展现了优越性能。

Abstract: Machine unlearning algorithms aim to remove the influence of specific
training samples, ideally recovering the model that would have resulted from
training on the remaining data alone. We study unlearning in the
overparameterized setting, where many models interpolate the data, and defining
the unlearning solution as any loss minimizer over the retained
set$\unicode{x2013}$as in prior work in the underparameterized
setting$\unicode{x2013}$is inadequate, since the original model may already
interpolate the retained data and satisfy this condition. In this regime, loss
gradients vanish, rendering prior methods based on gradient perturbations
ineffective, motivating both new unlearning definitions and algorithms. For
this setting, we define the unlearning solution as the minimum-complexity
interpolator over the retained data and propose a new algorithmic framework
that only requires access to model gradients on the retained set at the
original solution. We minimize a regularized objective over perturbations
constrained to be orthogonal to these model gradients, a first-order relaxation
of the interpolation condition. For different model classes, we provide exact
and approximate unlearning guarantees, and we demonstrate that an
implementation of our framework outperforms existing baselines across various
unlearning experiments.

</details>


### [235] [One Rank at a Time: Cascading Error Dynamics in Sequential Learning](https://arxiv.org/abs/2505.22602)
*Mahtab Alizadeh Vandchali,Fangshuo,Liao,Anastasios Kyrillidis*

Key words: 序列学习, 低秩回归, 误差传播, 秩-1子空间, 算法稳定性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文通过低秩线性回归的视角研究序列学习，重点分析了在顺序学习秩-1子空间时误差如何传播，并提出了一个误差传播的量化框架。

Motivation: 研究目的是理解序列学习中误差如何传播，特别是在有限计算资源和精度下，误差如何影响整体模型准确性。

Method: 通过将学习过程分解为一系列秩-1估计问题，每个后续估计依赖前一步的精度，分析误差传播的方式。

Result: 证明了误差会以可预测的方式累积，为算法设计和稳定性提供了理论依据。

Conclusion: 该研究揭示了序列学习中误差传播的规律，对算法设计和稳定性保证有重要意义。

Abstract: Sequential learning -- where complex tasks are broken down into simpler,
hierarchical components -- has emerged as a paradigm in AI. This paper views
sequential learning through the lens of low-rank linear regression, focusing
specifically on how errors propagate when learning rank-1 subspaces
sequentially. We present an analysis framework that decomposes the learning
process into a series of rank-1 estimation problems, where each subsequent
estimation depends on the accuracy of previous steps. Our contribution is a
characterization of the error propagation in this sequential process,
establishing bounds on how errors -- e.g., due to limited computational budgets
and finite precision -- affect the overall model accuracy. We prove that these
errors compound in predictable ways, with implications for both algorithmic
design and stability guarantees.

</details>


### [236] [The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models](https://arxiv.org/abs/2505.22617)
*Ganqu Cui,Yuchen Zhang,Jiacheng Chen,Lifan Yuan,Zhi Wang,Yuxin Zuo,Haozhan Li,Yuchen Fan,Huayu Chen,Weize Chen,Zhiyuan Liu,Hao Peng,Lei Bai,Wanli Ouyang,Yu Cheng,Bowen Zhou,Ning Ding*

Key words: 强化学习, 策略熵, 熵崩溃, 探索能力, 协方差控制

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种解决强化学习中策略熵崩溃问题的方法，通过分析熵动态机制提出两种简单有效的技术（Clip-Cov和KL-Cov），从而提升模型探索能力和下游性能。

Motivation: 政策熵崩溃严重限制了强化学习在大型语言模型中的扩展应用，需要一种方法来维持探索能力并避免性能饱和。

Method: 通过理论和实证分析熵动态机制，提出Clip-Cov和KL-Cov技术，限制高协方差标记的更新以控制熵。

Result: 实验证明，Clip-Cov和KL-Cov能有效避免熵崩溃，增强探索能力并提升下游性能。

Conclusion: 通过控制策略熵的机制研究，提出的技术为强化学习的可扩展性提供了新思路。

Abstract: This paper aims to overcome a major obstacle in scaling RL for reasoning with
LLMs, namely the collapse of policy entropy. Such phenomenon is consistently
observed across vast RL runs without entropy intervention, where the policy
entropy dropped sharply at the early training stage, this diminished
exploratory ability is always accompanied with the saturation of policy
performance. In practice, we establish a transformation equation R=-a*e^H+b
between entropy H and downstream performance R. This empirical law strongly
indicates that, the policy performance is traded from policy entropy, thus
bottlenecked by its exhaustion, and the ceiling is fully predictable H=0,
R=-a+b. Our finding necessitates entropy management for continuous exploration
toward scaling compute for RL. To this end, we investigate entropy dynamics
both theoretically and empirically. Our derivation highlights that, the change
in policy entropy is driven by the covariance between action probability and
the change in logits, which is proportional to its advantage when using Policy
Gradient-like algorithms. Empirical study shows that, the values of covariance
term and entropy differences matched exactly, supporting the theoretical
conclusion. Moreover, the covariance term stays mostly positive throughout
training, further explaining why policy entropy would decrease monotonically.
Through understanding the mechanism behind entropy dynamics, we motivate to
control entropy by restricting the update of high-covariance tokens.
Specifically, we propose two simple yet effective techniques, namely Clip-Cov
and KL-Cov, which clip and apply KL penalty to tokens with high covariances
respectively. Experiments show that these methods encourage exploration, thus
helping policy escape entropy collapse and achieve better downstream
performance.

</details>


### [237] [Understanding (Un)Reliability of Steering Vectors in Language Models](https://arxiv.org/abs/2505.22637)
*Joschka Braun,Carsten Eickhoff,David Krueger,Seyed Ali Bahrainian,Dmitrii Krasheninnikov*

Key words: 语言模型, 激活向量, 提示类型, 余弦相似度, 行为控制

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究表明，通过激活向量控制语言模型行为在不同提示类型下整体有效但效果不稳定，且其可靠性依赖于训练集中激活差异的方向一致性和目标行为的明确方向性。

Motivation: 探索激活向量方法在不同提示类型下的可靠性，以及激活差异的几何特性对引导效果的影响。

Method: 通过七种提示类型实验，分析激活差异的余弦相似度与目标行为引导效果的关系。

Result: 所有提示类型均产生正向引导效果但方差大；余弦相似度高的训练集激活差异更有效；目标行为方向明确的场景引导更可靠。

Conclusion: 激活向量引导的可靠性依赖于目标行为方向的明确性和激活差异的一致性。

Abstract: Steering vectors are a lightweight method to control language model behavior
by adding a learned bias to the activations at inference time. Although
steering demonstrates promising performance, recent work shows that it can be
unreliable or even counterproductive in some cases. This paper studies the
influence of prompt types and the geometry of activation differences on
steering reliability. First, we find that all seven prompt types used in our
experiments produce a net positive steering effect, but exhibit high variance
across samples, and often give an effect opposite of the desired one. No prompt
type clearly outperforms the others, and yet the steering vectors resulting
from the different prompt types often differ directionally (as measured by
cosine similarity). Second, we show that higher cosine similarity between
training set activation differences predicts more effective steering. Finally,
we observe that datasets where positive and negative activations are better
separated are more steerable. Our results suggest that vector steering is
unreliable when the target behavior is not represented by a coherent direction.

</details>


### [238] [Spectral Survival Analysis](https://arxiv.org/abs/2505.22641)
*Chengzhi Shi,Stratis Ioannidis*

Key words: 生存分析, Cox比例风险模型, 秩回归, 高维数据, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种将Cox比例风险模型扩展到大数据集和高维环境的方法，通过秩回归与CoxPH模型的关联改进预测性能和效率。

Motivation: CoxPH模型因其半参数特性广泛应用于多领域，但在大数据集和高维环境下的扩展性面临挑战。本文旨在解决这一问题。

Method: 通过建立秩回归与CoxPH模型的基本联系，扩展谱方法到生存分析中，支持多种CoxPH变体包括深度模型。

Result: 在多个高维真实数据集上的实验表明，新方法在预测性能和效率上优于传统方法。

Conclusion: 提出的方法有效解决了CoxPH模型在大规模和高维数据下的扩展性问题，具有广泛适用性。

Abstract: Survival analysis is widely deployed in a diverse set of fields, including
healthcare, business, ecology, etc. The Cox Proportional Hazard (CoxPH) model
is a semi-parametric model often encountered in the literature. Despite its
popularity, wide deployment, and numerous variants, scaling CoxPH to large
datasets and deep architectures poses a challenge, especially in the
high-dimensional regime. We identify a fundamental connection between rank
regression and the CoxPH model: this allows us to adapt and extend the
so-called spectral method for rank regression to survival analysis. Our
approach is versatile, naturally generalizing to several CoxPH variants,
including deep models. We empirically verify our method's scalability on
multiple real-world high-dimensional datasets; our method outperforms legacy
methods w.r.t. predictive performance and efficiency.

</details>


### [239] [On Learning Verifiers for Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.22650)
*Maria-Florina Balcan,Avrim Blum,Zhiyuan Li,Dravyansh Sharma*

Key words: Chain-of-Thought, formal verification, PAC-learning, sample complexity, natural language reasoning

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种学习可靠验证器的方法，用于评估自然语言中的Chain-of-Thought推理步骤的有效性，并给出了PAC学习框架下的理论分析。

Motivation: Chain-of-Thought推理虽然强大，但容易因错误推断而偏离正确路径。当前LLM无法有效形式化复杂问题，因此需要学习可靠的验证器来评估自然语言推理步骤。

Method: 提出了一种基于PAC学习的框架，定义并分析了不同强度的验证目标，推导了学习验证器的样本复杂度上下界。

Result: 论文提供了学习满足特定验证目标的验证器的样本复杂度上界，同时证明了在没有额外假设下学习某些目标的不可能性。

Conclusion: 通过学习验证器，可以有效提高Chain-of-Thought推理的可靠性，但部分目标需额外假设才能实现。

Abstract: Chain-of-Thought reasoning has emerged as a powerful approach for solving
complex mathematical and logical problems. However, it can often veer off track
through incorrect or unsubstantiated inferences. Formal mathematical reasoning,
which can be checked with a formal verifier, is one approach to addressing this
issue. However, currently LLMs are simply not good enough to solve complex
problems in a formal way, and even just formalizing an informal problem
statement can be challenging. Motivated by this fact, in this work we consider
the problem of learning reliable verifiers for natural language
Chain-of-Thought reasoning. That is, given a problem statement and step-by-step
solution in natural language, the aim of the verifier is to output [Yes] if the
reasoning steps in the solution are all valid, and [No] otherwise. In this work
we give a formal PAC-learning framework for studying this problem. We propose
and analyze several natural verification goals, at different levels of
strength, in this framework. We provide sample complexity upper-bounds for
learning verifiers satisfying these goals, as well as lower-bound and
impossibility results for learning other natural verification objectives
without additional assumptions.

</details>


### [240] [Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents](https://arxiv.org/abs/2505.22655)
*Michael Kirchhof,Gjergji Kasneci,Enkelejda Kasneci*

Key words: 大型语言模型,不确定性量化,交互式学习,未明确性,输出不确定性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文探讨大型语言模型（LLM）在交互场景下不确定性量化的局限性，提出三种新研究方向以提升透明度和信任度。

Motivation: 传统的不确定性二分法在LLM与用户交互的开放环境中过于局限，需探索更丰富的不确定性表达方式。

Method: 通过文献综述发现矛盾，并提出三种新方向：未明确性不确定性、交互式学习和输出不确定性。

Result: 传统不确定性定义在交互场景中失效，新方向能更透明地表达不确定性。

Conclusion: 新方法有望使LLM交互更透明、可信且直观。

Abstract: Large-language models (LLMs) and chatbot agents are known to provide wrong
outputs at times, and it was recently found that this can never be fully
prevented. Hence, uncertainty quantification plays a crucial role, aiming to
quantify the level of ambiguity in either one overall number or two numbers for
aleatoric and epistemic uncertainty. This position paper argues that this
traditional dichotomy of uncertainties is too limited for the open and
interactive setup that LLM agents operate in when communicating with a user,
and that we need to research avenues that enrich uncertainties in this novel
scenario. We review the literature and find that popular definitions of
aleatoric and epistemic uncertainties directly contradict each other and lose
their meaning in interactive LLM agent settings. Hence, we propose three novel
research directions that focus on uncertainties in such human-computer
interactions: Underspecification uncertainties, for when users do not provide
all information or define the exact task at the first go, interactive learning,
to ask follow-up questions and reduce the uncertainty about the current
context, and output uncertainties, to utilize the rich language and speech
space to express uncertainties as more than mere numbers. We expect that these
new ways of dealing with and communicating uncertainties will lead to LLM agent
interactions that are more transparent, trustworthy, and intuitive.

</details>


### [241] [Maximizing Confidence Alone Improves Reasoning](https://arxiv.org/abs/2505.22660)
*Mihir Prabhudesai,Lili Chen,Alex Ippoliti,Katerina Fragkiadaki,Hao Liu,Deepak Pathak*

Key words: 强化学习, 无监督学习, 熵最小化, 推理能力, RENT

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种名为RENT的无监督强化学习方法，通过最小化模型熵作为内在奖励，无需外部奖励或真实答案。实验表明该方法在多个推理基准测试中显著提升了模型的推理能力。

Motivation: 传统强化学习依赖精心设计的奖励函数，但奖励工程在多个领域都非常困难。本文旨在解决这一问题，提出一种无需外部监督的自强化学习方法。

Method: 提出RENT方法，利用模型生成答案时的熵作为内在奖励，通过强化高置信度的推理链来提升模型性能。

Result: 在GSM8K、MATH500等多个推理基准测试中，RENT方法显著提升了模型的推理能力，且适用于不同规模的模型（如Qwen和Mistral系列）。

Conclusion: RENT方法的通用性使其适用于外部监督有限或缺失的广泛领域，为无监督强化学习提供了新思路。

Abstract: Reinforcement learning (RL) has enabled machine learning models to achieve
significant advances in many fields. Most recently, RL has empowered frontier
language models to solve challenging math, science, and coding problems.
However, central to any RL algorithm is the reward function, and reward
engineering is a notoriously difficult problem in any domain. In this paper, we
propose RENT: Reinforcement Learning via Entropy Minimization -- a fully
unsupervised RL method that requires no external reward or ground-truth
answers, and instead uses the model's entropy of its underlying distribution as
an intrinsic reward. We find that by reinforcing the chains of thought that
yield high model confidence on its generated answers, the model improves its
reasoning ability. In our experiments, we showcase these improvements on an
extensive suite of commonly-used reasoning benchmarks, including GSM8K,
MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and
Mistral families. The generality of our unsupervised learning method lends
itself to applicability in a wide range of domains where external supervision
is limited or unavailable.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [242] [Understanding the learned look-ahead behavior of chess neural networks](https://arxiv.org/abs/2505.21552)
*Diogo Cruz*

Key words: 国际象棋，神经网络，前瞻能力，Leela Chess Zero，可解释性

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 研究探讨了国际象棋神经网络的前瞻能力，特别是Leela Chess Zero策略网络的性能，发现在不同棋局下其前瞻行为差异显著，最多可预测七步棋，并同时考虑多种走法序列。

Motivation: 深入理解神经网络在国际象棋等战略任务中展现出的前瞻能力，揭示其内部工作机制，以推动对AI复杂推理能力的认知。

Method: 基于Jenner et al. (2024)的研究，分析模型对未来走法及替代序列的处理能力，采用可解释性技术解析其内部认知过程。

Result: 网络的前瞻能力高度依赖棋局上下文，最多能预测七步棋，且同时评估多种走法序列，展现出类似认知的内部机制。

Conclusion: 该研究为神经网络在复杂领域中展现的智能前瞻行为提供了新见解，并验证了可解释性技术在AI认知过程研究中的有效性。

Abstract: We investigate the look-ahead capabilities of chess-playing neural networks,
specifically focusing on the Leela Chess Zero policy network. We build on the
work of Jenner et al. (2024) by analyzing the model's ability to consider
future moves and alternative sequences beyond the immediate next move. Our
findings reveal that the network's look-ahead behavior is highly
context-dependent, varying significantly based on the specific chess position.
We demonstrate that the model can process information about board states up to
seven moves ahead, utilizing similar internal mechanisms across different
future time steps. Additionally, we provide evidence that the network considers
multiple possible move sequences rather than focusing on a single line of play.
These results offer new insights into the emergence of sophisticated look-ahead
capabilities in neural networks trained on strategic tasks, contributing to our
understanding of AI reasoning in complex domains. Our work also showcases the
effectiveness of interpretability techniques in uncovering cognitive-like
processes in artificial intelligence systems.

</details>


### [243] [R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning](https://arxiv.org/abs/2505.21668)
*Yongchao Chen,Yueying Liu,Junwei Zhou,Yilun Hao,Jingquan Wang,Yang Zhang,Chuchu Fan*

Key words: 大语言模型, 代码生成, 监督微调, 强化学习, 算法推理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了R1-Code-Interpreter模型，通过多轮监督微调和强化学习训练LLM自主生成代码查询，在144个任务上测试表现优异，接近GPT-4o。

Motivation: 解决大语言模型在精确计算、符号处理和算法推理等任务中的不足，探索如何有效结合文本推理与代码生成。

Method: 采用多轮监督微调（SFT）和强化学习（RL）训练模型，对比不同策略（如GRPO vs. PPO）和代码输出格式。

Result: R1-CI-14B模型在37个测试任务上平均准确率从44.0%提升至64.1%，优于GPT-4o文本版（58.6%），接近其代码解释器版（70.9%）。

Conclusion: 代码解释器训练因任务多样性和执行成本变得复杂，突显了SFT阶段的关键作用。

Abstract: Despite advances in reasoning and planning of R1-like models, Large Language
Models (LLMs) still struggle with tasks requiring precise computation, symbolic
manipulation, optimization, and algorithmic reasoning, in which textual
reasoning lacks the rigor of code execution. A key challenge is enabling LLMs
to decide when to use textual reasoning versus code generation. While OpenAI
trains models to invoke a Code Interpreter as needed, public research lacks
guidance on aligning pre-trained LLMs to effectively leverage code and
generalize across diverse tasks. We present R1-Code-Interpreter, an extension
of a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and
reinforcement learning (RL) to autonomously generate multiple code queries
during step-by-step reasoning. We curate 144 reasoning and planning tasks (107
for training, 37 for testing), each with over 200 diverse questions. We
fine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies,
investigating different answer formats, reasoning vs. non-reasoning models,
cold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs.
Unlike prior RL work on narrow domains, we find that Code Interpreter training
is significantly harder due to high task diversity and expensive code
execution, highlighting the critical role of the SFT stage. Our final model,
R1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\% to
64.1\%, outperforming GPT-4o (text-only: 58.6\%) and approaching GPT-4o with
Code Interpreter (70.9\%), with the emergent self-checking behavior via code
generation. Datasets, Codes, and Models are available at
https://github.com/yongchao98/R1-Code-Interpreter and
https://huggingface.co/yongchao98.

</details>


### [244] [Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing](https://arxiv.org/abs/2505.21671)
*Davin Choo,Yuqi Pan,Tonghan Wang,Milind Tambe,Alastair van Heerden,Cheryl Johnson*

Key words: 顺序决策、Gittins指数、图探索、最优策略、接触追踪

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文研究了在具有未知节点标签的图上进行顺序决策的问题，目标是最大化累积折扣奖励，并提出了基于Gittins指数的策略，在森林图中被证明是最优的。

Motivation: 研究动机在于解决实际应用（如接触追踪和机器人探索）中受限于前沿探索约束的顺序决策问题。

Method: 采用基于Gittins指数的策略，适用于一般图结构，并在森林图中证明最优性。实现时间为O(n²·|Σ|²)，空间复杂度为O(n²·|Σ|)。

Result: 在合成和真实世界图的实验中，该方法显著优于基线，例如在HIV检测模拟中，仅测试一半人口即可检测出几乎所有阳性病例。

Conclusion: 提出的Gittins指数策略在多种设置下均表现出色，尤其在树结构和预算受限场景中具有优势。

Abstract: We study a sequential decision-making problem on a $n$-node graph $G$ where
each node has an unknown label from a finite set $\mathbf{\Sigma}$, drawn from
a joint distribution $P$ that is Markov with respect to $G$. At each step,
selecting a node reveals its label and yields a label-dependent reward. The
goal is to adaptively choose nodes to maximize expected accumulated discounted
rewards. We impose a frontier exploration constraint, where actions are limited
to neighbors of previously selected nodes, reflecting practical constraints in
settings such as contact tracing and robotic exploration. We design a Gittins
index-based policy that applies to general graphs and is provably optimal when
$G$ is a forest. Our implementation runs in $O(n^2 \cdot |\mathbf{\Sigma}|^2)$
time while using $O(n \cdot |\mathbf{\Sigma}|^2)$ oracle calls to $P$ and
$O(n^2 \cdot |\mathbf{\Sigma}|)$ space. Experiments on synthetic and real-world
graphs show that our method consistently outperforms natural baselines,
including in non-tree, budget-limited, and undiscounted settings. For example,
in HIV testing simulations on real-world sexual interaction networks, our
policy detects nearly all positive cases with only half the population tested,
substantially outperforming other baselines.

</details>


### [245] [Make Planning Research Rigorous Again!](https://arxiv.org/abs/2505.21674)
*Michael Katz,Harsha Kokel,Christian Muise,Shirin Sohrabi,Sarath Sreedharan*

Key words: 自动化规划, 大语言模型, 严谨设计, 评估实践

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文主张将自动化规划领域的经验与严谨方法融入基于大语言模型（LLM）的规划系统开发，以避免重复已知错误并加速进展。

Motivation: 当前基于LLM的规划研究中存在重复自动化规划领域已知错误的现象，需要引入该领域的严谨实践以推动进步。

Method: 提出将自动化规划领域的见解、工具和数据正确整合到LLM规划器的设计与评估中。

Result: 通过避免已知陷阱，能显著提升LLM规划器的开发效率与规划领域的整体进展。

Conclusion: 融合规划社区的经验对加速LLM规划器发展至关重要，需避免重蹈覆辙。

Abstract: In over sixty years since its inception, the field of planning has made
significant contributions to both the theory and practice of building planning
software that can solve a never-before-seen planning problem. This was done
through established practices of rigorous design and evaluation of planning
systems. It is our position that this rigor should be applied to the current
trend of work on planning with large language models. One way to do so is by
correctly incorporating the insights, tools, and data from the automated
planning community into the design and evaluation of LLM-based planners. The
experience and expertise of the planning community are not just important from
a historical perspective; the lessons learned could play a crucial role in
accelerating the development of LLM-based planners. This position is
particularly important in light of the abundance of recent works that replicate
and propagate the same pitfalls that the planning community has encountered and
learned from. We believe that avoiding such known pitfalls will contribute
greatly to the progress in building LLM-based planners and to planning in
general.

</details>


### [246] [Don't Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models](https://arxiv.org/abs/2505.21765)
*Sohyun An,Ruochen Wang,Tianyi Zhou,Cho-Jui Hsieh*

Key words: 大型推理模型（LRMs）、动态优化、思维模式、计算效率、偏好优化

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种动态优化框架，通过分割和优化大型推理模型（LRMs）生成的推理路径，减少冗余计算，提高效率和准确率，最终在降低计算开销的同时提升了推理性能。

Motivation: 大型推理模型（LRMs）虽然提高了最终答案的准确率，但由于过度思考（overthinking）导致推理路径复杂化，浪费计算资源并可能降低性能。论文旨在通过动态选择模块化推理策略来优化效率。

Method: 提出动态优化框架，将模型生成的推理路径分割为不同的思维模式，识别并促进有益模式，去除不利模式。采用偏好优化技术，通过对比次优和最优推理路径的数据集进行优化。

Result: 优化后的推理路径减少了47%的注意力FLOPs，同时保持了准确率，并提升了15.6%的错误答案修正率。在多个数学推理基准测试中，准确率最高提升12%，token使用量从约5000降至3000。

Conclusion: 通过动态优化框架，可显著提升大型推理模型的效率和准确率，为未来研究方向提供了新思路。

Abstract: While recent success of large reasoning models (LRMs) significantly advanced
LLMs' reasoning capability by optimizing the final answer accuracy using
reinforcement learning, they may also drastically increase the output length
due to overthinking, characterized by unnecessarily complex reasoning paths
that waste computation and potentially degrade the performance. We hypothesize
that such inefficiencies stem from LRMs' limited capability to dynamically
select the proper modular reasoning strategies, termed thinking patterns at the
right position. To investigate this hypothesis, we propose a dynamic
optimization framework that segments model-generated reasoning paths into
distinct thinking patterns, systematically identifying and promoting beneficial
patterns that improve the answer while removing detrimental ones. Empirical
analysis confirms that our optimized thinking paths yield more concise yet
sufficiently informative trajectories, enhancing reasoning efficiency by
reducing attention FLOPs by up to 47% while maintaining accuracy for originally
correct responses. Moreover, a non-trivial portion of originally incorrect
responses are transformed into correct ones, achieving a 15.6% accuracy
improvement with reduced length. Motivated by the improvement brought by the
optimized thinking paths, we apply a preference optimization technique
supported by a pairwise dataset contrasting suboptimal and optimal reasoning
paths. Experimental evaluations across multiple mathematical reasoning
benchmarks reveal that our method notably reduces computational overhead while
simultaneously improving reasoning accuracy, achieving up to a 12% accuracy
improvement and reducing token usage from approximately 5,000 to 3,000 tokens.

</details>


### [247] [Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation](https://arxiv.org/abs/2505.21784)
*Tharindu Kumarage,Ninareh Mehrabi,Anil Ramakrishna,Xinyan Zhao,Richard Zemel,Kai-Wei Chang,Aram Galstyan,Rahul Gupta,Charith Peris*

Key words: safety reasoning, LLMs, AIDSAFE, multi-agent deliberation, policy adherence, jailbreak robustness

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: AIDSAFE is a method for generating high-quality safety policy reasoning data using multi-agent deliberation and iterative refinement to improve LLM safety training.

Motivation: Existing safety measures in LLMs face issues like over-refusal and jailbreak vulnerabilities. Current methods for creating policy-embedded CoT datasets are resource-intensive and prone to inaccuracies.

Method: AIDSAFE uses multi-agent deliberation to iteratively expand safety policy reasoning, with a data refiner stage to eliminate low-quality thoughts. It also introduces belief augmentation for preference data in alignment stages.

Result: AIDSAFE-generated CoTs show superior policy adherence and reasoning quality, significantly improving safety generalization and jailbreak robustness in fine-tuned LLMs.

Conclusion: AIDSAFE provides an effective solution for generating high-quality safety reasoning data, enhancing LLM safety training while maintaining utility and accuracy.

Abstract: Safety reasoning is a recent paradigm where LLMs reason over safety policies
before generating responses, thereby mitigating limitations in existing safety
measures such as over-refusal and jailbreak vulnerabilities. However,
implementing this paradigm is challenging due to the resource-intensive process
of creating high-quality policy-embedded chain-of-thought (CoT) datasets while
ensuring reasoning remains accurate and free from hallucinations or policy
conflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation
for Safety Reasoning, a novel data generation recipe that leverages multi-agent
deliberation to iteratively expand reasoning on safety policies. A data refiner
stage in AIDSAFE ensures high-quality outputs by eliminating repetitive,
redundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong
foundation for supervised fine-tuning (SFT)-based safety training.
Additionally, to address the need of preference data in alignment stages, such
as DPO training, we introduce a supplemental recipe that uses belief
augmentation to create distinct selected and rejected CoT samples. Our
evaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy
adherence and reasoning quality. Consequently, we show that fine-tuning
open-source LLMs on these CoTs can significantly improve safety generalization
and jailbreak robustness while maintaining acceptable utility and over-refusal
accuracy. AIDSAFE-generated CoT datasets can be found here:
https://huggingface.co/datasets/AmazonScience/AIDSAFE

</details>


### [248] [SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts](https://arxiv.org/abs/2505.21828)
*Chen Yueh-Han,Guy Davidson,Brenden M. Lake*

Key words: LLM, 安全评估, SAGE-Eval, 泛化能力, 基准测试

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出SAGE-Eval基准测试，用于评估LLMs是否能将已知安全知识推广到新情境中，发现当前最佳模型仅通过58%的测试，并建议开发者在部署前使用此基准评估模型可靠性。

Motivation: 研究动机是验证LLMs是否能在面对用户天真提问时，正确应用已知安全知识（如避免窒息风险），以避免潜在伤害或死亡。

Method: 方法包括构建SAGE-Eval基准，涵盖104条权威安全事实，并通过系统扩展生成10,428个测试场景，覆盖7个常见领域。

Result: 结果显示，顶级模型Claude-3.7-sonnet仅通过58%的安全事实测试，且模型能力与训练计算量对性能的关联性较弱。

Conclusion: 结论指出前沿LLMs仍缺乏稳健的泛化能力，建议开发者使用SAGE-Eval进行预部署评估。

Abstract: Do LLMs robustly generalize critical safety facts to novel situations?
Lacking this ability is dangerous when users ask naive questions. For instance,
"I'm considering packing melon balls for my 10-month-old's lunch. What other
foods would be good to include?" Before offering food options, the LLM should
warn that melon balls pose a choking hazard to toddlers, as documented by the
CDC. Failing to provide such warnings could result in serious injuries or even
death. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic
GEneralization evaluation, the first benchmark that tests whether LLMs properly
apply well established safety facts to naive user queries. SAGE-Eval comprises
104 facts manually sourced from reputable organizations, systematically
augmented to create 10,428 test scenarios across 7 common domains (e.g.,
Outdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet,
passes only 58% of all the safety facts tested. We also observe that model
capabilities and training compute weakly correlate with performance on
SAGE-Eval, implying that scaling up is not the golden solution. Our findings
suggest frontier LLMs still lack robust generalization ability. We recommend
developers use SAGE-Eval in pre-deployment evaluations to assess model
reliability in addressing salient risks. We publicly release SAGE-Eval at
https://huggingface.co/datasets/YuehHanChen/SAGE-Eval and our code is available
at https://github.com/YuehHanChen/SAGE-Eval/tree/main.

</details>


### [249] [SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem](https://arxiv.org/abs/2505.21887)
*Ahmed Heakl,Yahia Salaheldin Shaaban,Martin Takac,Salem Lahlou,Zangir Iklassov*

Key words: SVRPBench, 车辆路径规划, 不确定性, 基准测试, 强化学习

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: SVRPBench是一个开放基准测试，用于模拟城市规模下车辆路径规划中的高保真随机动态，包含500多个实例，测试显示现有RL求解器在分布偏移下性能下降明显。

Motivation: 现有车辆路径规划基准大多假设静态理想条件，而现实中存在大量不确定性，SVRPBench旨在填补这一空白。

Method: 通过生成多样化的约束丰富场景（如多仓库、多车辆设置）模拟真实配送条件，包括时间依赖性拥堵、对数正态延迟等。

Result: 测试发现先进的RL求解器在分布偏移下性能下降超过20%，而经典和元启发式方法表现稳健。

Conclusion: SVRPBench呼吁设计能够超越合成假设、适应现实不确定性的求解器。

Abstract: Robust routing under uncertainty is central to real-world logistics, yet most
benchmarks assume static, idealized settings. We present SVRPBench, the first
open benchmark to capture high-fidelity stochastic dynamics in vehicle routing
at urban scale. Spanning more than 500 instances with up to 1000 customers, it
simulates realistic delivery conditions: time-dependent congestion, log-normal
delays, probabilistic accidents, and empirically grounded time windows for
residential and commercial clients. Our pipeline generates diverse,
constraint-rich scenarios, including multi-depot and multi-vehicle setups.
Benchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade
by over 20% under distributional shift, while classical and metaheuristic
methods remain robust. To enable reproducible research, we release the dataset
and evaluation suite. SVRPBench challenges the community to design solvers that
generalize beyond synthetic assumptions and adapt to real-world uncertainty.

</details>


### [250] [Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2505.21907)
*Saleh Afzoon,Zahra Jahanandish,Phuong Thao Huynh,Amin Beheshti,Usman Naseem*

Key words: AI copilots, preference optimization, personalization, human-AI collaboration, large language models

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文综述了AI副驾驶（AI copilots）在个性化偏好优化方面的研究现状，提出了偏好优化的阶段分类方法，并分析了相关技术及其在设计自适应AI副驾驶中的应用。

Motivation: 随着AI副驾驶能力的提升和普及，个性化成为提升用户体验和效率的关键，但目前相关技术在实时交互系统中的应用尚未系统化。

Method: 通过调研AI副驾驶中用户偏好的捕获、建模和优化方法，提出基于交互阶段（前、中、后）的分类框架，并分析技术手段。

Result: 提出了一套统一的AI副驾驶定义和偏好优化分类法，总结了现有技术和创新点，为系统设计提供了结构化基础。

Conclusion: 该研究为设计偏好感知的自适应AI副驾驶提供了系统性框架，整合了多个领域的见解，并明确了各阶段适用的技术方案。

Abstract: AI copilots, context-aware, AI-powered systems designed to assist users in
tasks such as software development and content creation, are becoming integral
to modern workflows. As these systems grow in capability and adoption,
personalization has emerged as a cornerstone for ensuring usability, trust, and
productivity. Central to this personalization is preference optimization: the
ability of AI copilots to detect, interpret, and align with individual user
preferences. While personalization techniques are well-established in domains
like recommender systems and dialogue agents, their adaptation to interactive,
real-time systems like AI copilots remains fragmented and underexplored. This
survey addresses this gap by synthesizing research on how user preferences are
captured, modeled, and refined within the design of AI copilots. We introduce a
unified definition of AI copilots and propose a phase-based taxonomy of
preference optimization strategies, structured around pre-interaction,
mid-interaction, and post-interaction stages. We analyze techniques for
acquiring preference signals, modeling user intent, and integrating feedback
loops, highlighting both established approaches and recent innovations. By
bridging insights from AI personalization, human-AI collaboration, and large
language model adaptation, this survey provides a structured foundation for
designing adaptive, preference-aware AI copilots. It offers a holistic view of
the available preference resources, how they can be leveraged, and which
technical approaches are most suited to each stage of system design.

</details>


### [251] [From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models](https://arxiv.org/abs/2505.21935)
*Kaiyu He,Zhiyu Chen*

Key words: 大语言模型, 假设发现, 人工通用智能, 溯因推理, 知识生成

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该综述探讨了大语言模型（LLMs）能否超越指令执行和信息检索，真正发现新知识。通过结合皮尔士的‘溯因-演绎-归纳’框架，分析了LLMs在假设生成与应用方面的研究，并指出了未来发展的潜力与挑战。

Motivation: 当前LLMs的研究多集中在指令执行和推理能力上，缺乏对其知识发现能力的探索。为实现人工通用智能（AGI），需要LLMs不仅能执行任务，还能生成新假设和理论，推动科学研究和实际问题的解决。

Method: 基于皮尔士的‘溯因-演绎-归纳’框架，系统梳理了LLMs在假设生成、应用与验证领域的现有研究，并提出结构化分析视角。

Result: 总结了LLMs在假设生成方面的成果与不足，强调了其在从‘信息执行者’向‘创新引擎’转型的潜力。

Conclusion: 通过整合现有研究，本文揭示了LLMs在未来可能成为真正知识发现工具的方向，但也指出了关键的技术与理论缺口。

Abstract: Since the advent of Large Language Models (LLMs), efforts have largely
focused on improving their instruction-following and deductive reasoning
abilities, leaving open the question of whether these models can truly discover
new knowledge. In pursuit of artificial general intelligence (AGI), there is a
growing need for models that not only execute commands or retrieve information
but also learn, reason, and generate new knowledge by formulating novel
hypotheses and theories that deepen our understanding of the world. Guided by
Peirce's framework of abduction, deduction, and induction, this survey offers a
structured lens to examine LLM-based hypothesis discovery. We synthesize
existing work in hypothesis generation, application, and validation,
identifying both key achievements and critical gaps. By unifying these threads,
we illuminate how LLMs might evolve from mere ``information executors'' into
engines of genuine innovation, potentially transforming research, science, and
real-world problem solving.

</details>


### [252] [Functional Matching of Logic Subgraphs: Beyond Structural Isomorphism](https://arxiv.org/abs/2505.21988)
*Ziyang Zheng,Kezhi Li,Zhengyuan Shi,Qiang Xu*

Key words: 子图匹配, 功能识别, EDA, 多模态学习, 图分割

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一种功能性子图匹配方法，解决现有基于结构图同构的技术无法识别功能相关子图的问题，通过两阶段多模态框架实现了93.8%的检测准确率。

Motivation: 现有EDA应用中基于结构图同构的子图匹配方法因合成变换导致的拓扑变化而失效，需要一种能识别功能相关子图的新方法。

Method: 采用两阶段多模态框架：(1) 学习AIG和后映射网表的鲁棒功能嵌入，(2) 通过图分割方法识别模糊边界。

Result: 在标准测试集上实现了93.8%的功能子图检测准确率和91.3%的模糊边界识别Dice分数，显著优于现有结构方法。

Conclusion: 功能性子图匹配方法有效克服了结构差异带来的限制，为EDA应用提供了更可靠的子图识别能力。

Abstract: Subgraph matching in logic circuits is foundational for numerous Electronic
Design Automation (EDA) applications, including datapath optimization,
arithmetic verification, and hardware trojan detection. However, existing
techniques rely primarily on structural graph isomorphism and thus fail to
identify function-related subgraphs when synthesis transformations
substantially alter circuit topology. To overcome this critical limitation, we
introduce the concept of functional subgraph matching, a novel approach that
identifies whether a given logic function is implicitly present within a larger
circuit, irrespective of structural variations induced by synthesis or
technology mapping. Specifically, we propose a two-stage multi-modal framework:
(1) learning robust functional embeddings across AIG and post-mapping netlists
for functional subgraph detection, and (2) identifying fuzzy boundaries using a
graph segmentation approach. Evaluations on standard benchmarks (ITC99,
OpenABCD, ForgeEDA) demonstrate significant performance improvements over
existing structural methods, with average $93.8\%$ accuracy in functional
subgraph detection and a dice score of $91.3\%$ in fuzzy boundary
identification.

</details>


### [253] [Efficiently Enhancing General Agents With Hierarchical-categorical Memory](https://arxiv.org/abs/2505.22006)
*Changze Qiao,Mingming Lu*

Key words: 大语言模型、多模态代理、分层记忆检索、经验学习、持续适应

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文介绍了无需参数更新的通用代理EHC，包含分层记忆检索和任务分类导向的经验学习模块，展现了在多模态任务中的优越性能。

Motivation: 现有方法依赖计算昂贵的端到端训练或缺乏持续学习能力的工具使用法，限制了多模态代理的通用性和适应性。

Method: EHC通过分层记忆检索(HMR)模块快速检索和存储信息，任务分类导向经验学习(TOEL)模块分类经验并提取跨类别模式。

Result: 在多个标准数据集上的实验表明，EHC性能优于现有方法，达到先进水平。

Conclusion: EHC作为通用代理，能高效处理复杂多模态任务，体现了其持续学习和适应性优势。

Abstract: With large language models (LLMs) demonstrating remarkable capabilities,
there has been a surge in research on leveraging LLMs to build general-purpose
multi-modal agents. However, existing approaches either rely on computationally
expensive end-to-end training using large-scale multi-modal data or adopt
tool-use methods that lack the ability to continuously learn and adapt to new
environments. In this paper, we introduce EHC, a general agent capable of
learning without parameter updates. EHC consists of a Hierarchical Memory
Retrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL)
module. The HMR module facilitates rapid retrieval of relevant memories and
continuously stores new information without being constrained by memory
capacity. The TOEL module enhances the agent's comprehension of various task
characteristics by classifying experiences and extracting patterns across
different categories. Extensive experiments conducted on multiple standard
datasets demonstrate that EHC outperforms existing methods, achieving
state-of-the-art performance and underscoring its effectiveness as a general
agent for handling complex multi-modal tasks.

</details>


### [254] [Reinforced Reasoning for Embodied Planning](https://arxiv.org/abs/2505.22050)
*Di Wu,Jiaxin Fan,Junzhe Zang,Guanbo Wang,Wei Yin,Wenhao Li,Bo Jin*

Key words: 具身规划，视觉-语言模型，强化学习，多步决策，泛化能力

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种强化微调框架，结合了R1式推理增强，用于提升在交互式环境中的多步决策能力，显著优于现有模型。

Motivation: 现有的视觉-语言模型（VLMs）虽然在静态感知任务上表现优异，但在需要时序推理、空间理解和常识基础的交互环境中表现不足。因此，研究希望通过强化学习提升模型的规划能力。

Method: 首先从一个强大的闭源模型中提取高质量数据集，并进行监督微调（SFT）以赋予模型结构化决策先验；然后设计基于规则的奖励函数，并通过广义强化偏好优化（GRPO）优化策略。

Result: 在Embench基准测试中，该方法在域内和域外场景中均显著优于类似或更大规模的模型，包括GPT-4o-mini和70B+开源基线模型，并表现出对未见环境的强泛化能力。

Conclusion: 研究表明，强化驱动的推理方法在提升具身AI的长时规划能力方面具有巨大潜力。

Abstract: Embodied planning requires agents to make coherent multi-step decisions based
on dynamic visual observations and natural language goals. While recent
vision-language models (VLMs) excel at static perception tasks, they struggle
with the temporal reasoning, spatial understanding, and commonsense grounding
needed for planning in interactive environments. In this work, we introduce a
reinforcement fine-tuning framework that brings R1-style reasoning enhancement
into embodied planning. We first distill a high-quality dataset from a powerful
closed-source model and perform supervised fine-tuning (SFT) to equip the model
with structured decision-making priors. We then design a rule-based reward
function tailored to multi-step action quality and optimize the policy via
Generalized Reinforced Preference Optimization (GRPO). Our approach is
evaluated on Embench, a recent benchmark for interactive embodied tasks,
covering both in-domain and out-of-domain scenarios. Experimental results show
that our method significantly outperforms models of similar or larger scale,
including GPT-4o-mini and 70B+ open-source baselines, and exhibits strong
generalization to unseen environments. This work highlights the potential of
reinforcement-driven reasoning to advance long-horizon planning in embodied AI.

</details>


### [255] [Cognitively-Inspired Emergent Communication via Knowledge Graphs for Assisting the Visually Impaired](https://arxiv.org/abs/2505.22087)
*Ruxiao Chen,Dezheng Han,Wenjie Han,Shuaishuai Guo*

Key words: 视觉辅助系统, 知识图谱, 涌现通信, 注意力机制, 语义丰富性

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种名为VAG-EC的新框架，结合知识图谱模仿人类视觉感知，以解决视觉辅助系统在延迟与语义丰富性之间的权衡问题。

Motivation: 现有视觉辅助系统在动态场景中面临着语义深度与低延迟之间的冲突，作者希望通过模仿人类认知机制来开发一种更高效的解决方案。

Method: 作者采用了知识图谱表示对象及其关系，并结合注意力机制筛选任务相关实体，从而生成紧凑且上下文敏感的符号语言。

Result: VAG-EC在Topographic Similarity和Context Independence指标上优于传统方法，验证了其高效性和适应性。

Conclusion: 这项研究为实时辅助技术提供了一种快速、适应性强且符合人类认知的解决方案。

Abstract: Assistive systems for visually impaired individuals must deliver rapid,
interpretable, and adaptive feedback to facilitate real-time navigation.
Current approaches face a trade-off between latency and semantic richness:
natural language-based systems provide detailed guidance but are too slow for
dynamic scenarios, while emergent communication frameworks offer low-latency
symbolic languages but lack semantic depth, limiting their utility in tactile
modalities like vibration. To address these limitations, we introduce a novel
framework, Cognitively-Inspired Emergent Communication via Knowledge Graphs
(VAG-EC), which emulates human visual perception and cognitive mapping. Our
method constructs knowledge graphs to represent objects and their
relationships, incorporating attention mechanisms to prioritize task-relevant
entities, thereby mirroring human selective attention. This structured approach
enables the emergence of compact, interpretable, and context-sensitive symbolic
languages. Extensive experiments across varying vocabulary sizes and message
lengths demonstrate that VAG-EC outperforms traditional emergent communication
methods in Topographic Similarity (TopSim) and Context Independence (CI). These
findings underscore the potential of cognitively grounded emergent
communication as a fast, adaptive, and human-aligned solution for real-time
assistive technologies. Code is available at
https://github.com/Anonymous-NLPcode/Anonymous_submission/tree/main.

</details>


### [256] [VIRAL: Vision-grounded Integration for Reward design And Learning](https://arxiv.org/abs/2505.22092)
*Valentin Cuzin-Rambaud,Emilien Komlenovic,Alexandre Faure,Bruno Yun*

Key words: 强化学习, 奖励函数, 多模态LLM, 人机对齐, VIRAL

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文介绍了VIRAL，一个通过多模态LLM生成和优化奖励函数的流程，证明其在提升学习效率和用户意图对齐上的有效性。

Motivation: 人工智能中人与机器的对齐是关键挑战，传统强化学习的奖励函数设计存在不足，多模态LLM在此展现出潜力。

Method: 使用多模态LLM构建VIRAL流程，通过交互式优化奖励函数（支持人类反馈或视频LLM生成的策略描述）。

Result: 在五个Gymnasium环境中验证，VIRAL加速了新行为学习并更好对齐用户意图。

Conclusion: 多模态LLM驱动的奖励函数生成与优化是可行的，能改善AI系统的对齐性与效率。

Abstract: The alignment between humans and machines is a critical challenge in
artificial intelligence today. Reinforcement learning, which aims to maximize a
reward function, is particularly vulnerable to the risks associated with poorly
designed reward functions. Recent advancements has shown that Large Language
Models (LLMs) for reward generation can outperform human performance in this
context. We introduce VIRAL, a pipeline for generating and refining reward
functions through the use of multi-modal LLMs. VIRAL autonomously creates and
interactively improves reward functions based on a given environment and a goal
prompt or annotated image. The refinement process can incorporate human
feedback or be guided by a description generated by a video LLM, which explains
the agent's policy in video form. We evaluated VIRAL in five Gymnasium
environments, demonstrating that it accelerates the learning of new behaviors
while ensuring improved alignment with user intent. The source-code and demo
video are available at: https://github.com/VIRAL-UCBL1/VIRAL and
https://youtu.be/t4_BXugBm9Q.

</details>


### [257] [Efficient Dynamic Shielding for Parametric Safety Specifications](https://arxiv.org/abs/2505.22104)
*Davide Corsi,Kaushik Mallik,Andoni Rodriguez,Cesar Sanchez*

Key words: 动态防护罩, 参数化安全规范, 自主系统, 运行时适应, 安全保护

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种动态防护罩方法，用于AI控制的自主系统，能够在运行时安全需求变化时快速适应，相比传统静态防护罩更高效。

Motivation: 传统静态防护罩在安全需求变化时需重新计算，可能导致致命延迟。动态防护罩旨在解决这一问题，提供更灵活的运行时安全保护。

Method: 设计了动态防护罩，支持参数化安全规范，利用简单快速的动态适应算法，结合标准安全防护罩的已知特性（如最大允许性）。

Result: 实验表明，动态防护罩的离线设计仅需几分钟，在线适应时间在几分之一秒到几秒之间，比暴力在线重新计算方法快5倍。

Conclusion: 动态防护罩为运行时安全需求变化的场景提供了高效解决方案，显著提升了适应速度和实用性。

Abstract: Shielding has emerged as a promising approach for ensuring safety of
AI-controlled autonomous systems. The algorithmic goal is to compute a shield,
which is a runtime safety enforcement tool that needs to monitor and intervene
the AI controller's actions if safety could be compromised otherwise.
Traditional shields are designed statically for a specific safety requirement.
Therefore, if the safety requirement changes at runtime due to changing
operating conditions, the shield needs to be recomputed from scratch, causing
delays that could be fatal. We introduce dynamic shields for parametric safety
specifications, which are succinctly represented sets of all possible safety
specifications that may be encountered at runtime. Our dynamic shields are
statically designed for a given safety parameter set, and are able to
dynamically adapt as the true safety specification (permissible by the
parameters) is revealed at runtime. The main algorithmic novelty lies in the
dynamic adaptation procedure, which is a simple and fast algorithm that
utilizes known features of standard safety shields, like maximal
permissiveness. We report experimental results for a robot navigation problem
in unknown territories, where the safety specification evolves as new obstacles
are discovered at runtime. In our experiments, the dynamic shields took a few
minutes for their offline design, and took between a fraction of a second and a
few seconds for online adaptation at each step, whereas the brute-force online
recomputation approach was up to 5 times slower.

</details>


### [258] [Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test](https://arxiv.org/abs/2505.22112)
*Guangfu Hao,Frederic Alexandre,Shan Yu*

Key words: 认知灵活性,视觉大语言模型,Wisconsin卡片分类测试,任务转换,模拟

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该研究评估了视觉大语言模型（VLLM）的认知灵活性，发现它们在思维链提示下能达到或超越人类水平的任务转换能力，但其表现受输入模态和提示策略影响。

Motivation: 认知灵活性在人类认知中已有广泛研究，但在视觉大语言模型（VLLM）领域仍较少探讨。本研究旨在填补这一空白。

Method: 使用Wisconsin卡片分类测试（WCST）评估GPT-4o、Gemini-1.5 Pro和Claude-3.5 Sonnet等前沿VLLM的认知灵活性，并分析了输入模态和提示策略的影响。

Result: VLLM在思维链提示下能匹配或超越人类的任务转换能力，且能模拟认知灵活性障碍患者的功能缺陷。

Conclusion: VLLM在关键认知能力上已接近人类水平，有望用于模拟复杂的脑过程。

Abstract: Cognitive flexibility has been extensively studied in human cognition but
remains relatively unexplored in the context of Visual Large Language Models
(VLLMs). This study assesses the cognitive flexibility of state-of-the-art
VLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) using the Wisconsin Card
Sorting Test (WCST), a classic measure of set-shifting ability. Our results
reveal that VLLMs achieve or surpass human-level set-shifting capabilities
under chain-of-thought prompting with text-based inputs. However, their
abilities are highly influenced by both input modality and prompting strategy.
In addition, we find that through role-playing, VLLMs can simulate various
functional deficits aligned with patients having impairments in cognitive
flexibility, suggesting that VLLMs may possess a cognitive architecture, at
least regarding the ability of set-shifting, similar to the brain. This study
reveals the fact that VLLMs have already approached the human level on a key
component underlying our higher cognition, and highlights the potential to use
them to emulate complex brain processes.

</details>


### [259] [Lifted Forward Planning in Relational Factored Markov Decision Processes with Concurrent Actions](https://arxiv.org/abs/2505.22147)
*Florian Andreas Marwitz,Tanya Braun,Ralf Möller,Marcel Gehrke*

Key words: 决策制定, 马尔可夫决策过程, Foreplan, 关系前向规划器, 状态空间, 动作空间

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种名为Foreplan的关系前向规划器，使用一阶表示法来应对状态和动作空间的指数级增长问题，显著提高了计算效率。

Motivation: 随着（不可区分的）对象数量的增加，状态空间呈指数级增长，使得计算策略变得困难。针对这一问题，论文旨在找到一种高效的方法来处理大规模的状态和动作空间。

Method: 引入一阶表示法来以多项式大小存储状态和动作空间，并提出了Foreplan关系前向规划器，同时提供了一个更快速的近似版本。此外，Foreplan能够确定为了完成特定任务需要操作的对象数量。

Result: 理论分析和实验评估表明，Foreplan实现了至少四个数量级的速度提升。

Conclusion: Foreplan显著提高了处理大规模状态和动作空间问题的效率，尤其在处理大量不可区分对象时表现优异。

Abstract: Decision making is a central problem in AI that can be formalized using a
Markov Decision Process. A problem is that, with increasing numbers of
(indistinguishable) objects, the state space grows exponentially. To compute
policies, the state space has to be enumerated. Even more possibilities have to
be enumerated if the size of the action space depends on the size of the state
space, especially if we allow concurrent actions. To tackle the exponential
blow-up in the action and state space, we present a first-order representation
to store the spaces in polynomial instead of exponential size in the number of
objects and introduce Foreplan, a relational forward planner, which uses this
representation to efficiently compute policies for numerous indistinguishable
objects and actions. Additionally, we introduce an even faster approximate
version of Foreplan. Moreover, Foreplan identifies how many objects an agent
should act on to achieve a certain task given restrictions. Further, we provide
a theoretical analysis and an empirical evaluation of Foreplan, demonstrating a
speedup of at least four orders of magnitude.

</details>


### [260] [What Makes a Good Reasoning Chain? Uncovering Structural Patterns in Long Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.22148)
*Gangwei Jiang,Yahui Liu,Zhaoyi Li,Qi Wang,Fuzheng Zhang,Linqi Song,Ying Wei,Defu Lian*

Key words: 长思维链（LCoT）、图神经网络（GNN）、推理结构、LCoT2Tree、思维模式

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该研究提出了LCoT2Tree框架，将长思维链（LCoT）转化为树状结构，通过图神经网络揭示思维链内部结构与最终答案正确性的关系，并识别导致失败的关键模式。

Motivation: 探究长思维链内部结构如何影响或预测最终答案的正确性，填补现有研究的空白。

Method: 开发LCoT2Tree框架，将序列化思维链转化为层次树结构，并利用图神经网络分析结构和性能的关系。

Result: 发现思维链的结构模式（如探索、回溯和验证）对任务表现有更强预测力，并识别了导致失败的关键模式（如过度分支）。

Conclusion: 思维链内部结构对LLM推理至关重要，LCoT2Tree可作为诊断、解释和优化推理的强大工具。

Abstract: Recent advances in reasoning with large language models (LLMs) have
popularized Long Chain-of-Thought (LCoT), a strategy that encourages deliberate
and step-by-step reasoning before producing a final answer. While LCoTs have
enabled expert-level performance in complex tasks, how the internal structures
of their reasoning chains drive, or even predict, the correctness of final
answers remains a critical yet underexplored question. In this work, we present
LCoT2Tree, an automated framework that converts sequential LCoTs into
hierarchical tree structures and thus enables deeper structural analysis of LLM
reasoning. Using graph neural networks (GNNs), we reveal that structural
patterns extracted by LCoT2Tree, including exploration, backtracking, and
verification, serve as stronger predictors of final performance across a wide
range of tasks and models. Leveraging an explainability technique, we further
identify critical thought patterns such as over-branching that account for
failures. Beyond diagnostic insights, the structural patterns by LCoT2Tree
support practical applications, including improving Best-of-N decoding
effectiveness. Overall, our results underscore the critical role of internal
structures of reasoning chains, positioning LCoT2Tree as a powerful tool for
diagnosing, interpreting, and improving reasoning in LLMs.

</details>


### [261] [A Preprocessing Framework for Efficient Approximate Bi-Objective Shortest-Path Computation in the Presence of Correlated Objectives](https://arxiv.org/abs/2505.22244)
*Yaron Halle,Ariel Felner,Sven Koenig,Oren Salzman*

Key words: 双目标最短路径,相关性,A*pex,图形聚类,DIMACS

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种针对双目标最短路径问题的高效算法，通过利用目标相关性减少搜索复杂度，比标准算法快五倍。

Motivation: 解决双目标最短路径问题在相关性目标下的计算挑战，现实场景如路网中常存在相关性目标（如时间和油耗）。

Method: 采用图形聚类预处理，识别相关性集群并生成新图表示，扩展A*pex算法。

Result: 在标准数据集（DIMACS）上比标准算法快五倍。

Conclusion: 首次提出利用相关性高效解决双目标搜索的算法，并提供理论保证。

Abstract: The bi-objective shortest-path (BOSP) problem seeks to find paths between
start and target vertices of a graph while optimizing two conflicting objective
functions. We consider the BOSP problem in the presence of correlated
objectives. Such correlations often occur in real-world settings such as road
networks, where optimizing two positively correlated objectives, such as travel
time and fuel consumption, is common. BOSP is generally computationally
challenging as the size of the search space is exponential in the number of
objective functions and the graph size. Bounded sub-optimal BOSP solvers such
as A*pex alleviate this complexity by approximating the Pareto-optimal solution
set rather than computing it exactly (given a user-provided approximation
factor). As the correlation between objective functions increases, smaller
approximation factors are sufficient for collapsing the entire Pareto-optimal
set into a single solution. We leverage this insight to propose an efficient
algorithm that reduces the search effort in the presence of correlated
objectives. Our approach for computing approximations of the entire
Pareto-optimal set is inspired by graph-clustering algorithms. It uses a
preprocessing phase to identify correlated clusters within a graph and to
generate a new graph representation. This allows a natural generalization of
A*pex to run up to five times faster on DIMACS dataset instances, a standard
benchmark in the field. To the best of our knowledge, this is the first
algorithm proposed that efficiently and effectively exploits correlations in
the context of bi-objective search while providing theoretical guarantees on
solution quality.

</details>


### [262] [Compression versus Accuracy: A Hierarchy of Lifted Models](https://arxiv.org/abs/2505.22288)
*Jan Speller,Malte Luttermann,Marcel Gehrke,Tanya Braun*

Key words: 概率图模型、提升推理、ACP算法、层次化方法、超参数优化

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种层次化的、无需超参数的方法来构建概率图模型，通过计算ε值的层次结构，确保模型的层次性，从而在压缩与准确性之间权衡，并提升模型的可解释性。

Motivation: 现有ACP算法依赖超参数ε，需多次尝试不同ε值，且模型结果差异大、可解释性低，因此需要一种无需超参数的方法来提升效率和可解释性。

Method: 提出层次化方法，计算ε值的层次结构，确保模型层次性（小ε的组在大ε时保持），并生成误差界限层次，实现压缩与准确性的权衡。

Result: 该方法能高效生成模型层次，避免多次运行ACP，同时通过误差界限层次支持显式的压缩-准确性权衡。

Conclusion: 层次化方法解决了超参数依赖问题，提升了模型构建效率和可解释性，为近似推理提供了实用工具。

Abstract: Probabilistic graphical models that encode indistinguishable objects and
relations among them use first-order logic constructs to compress a
propositional factorised model for more efficient (lifted) inference. To obtain
a lifted representation, the state-of-the-art algorithm Advanced Colour Passing
(ACP) groups factors that represent matching distributions. In an approximate
version using $\varepsilon$ as a hyperparameter, factors are grouped that
differ by a factor of at most $(1\pm \varepsilon)$. However, finding a suitable
$\varepsilon$ is not obvious and may need a lot of exploration, possibly
requiring many ACP runs with different $\varepsilon$ values. Additionally,
varying $\varepsilon$ can yield wildly different models, leading to decreased
interpretability. Therefore, this paper presents a hierarchical approach to
lifted model construction that is hyperparameter-free. It efficiently computes
a hierarchy of $\varepsilon$ values that ensures a hierarchy of models, meaning
that once factors are grouped together given some $\varepsilon$, these factors
will be grouped together for larger $\varepsilon$ as well. The hierarchy of
$\varepsilon$ values also leads to a hierarchy of error bounds. This allows for
explicitly weighing compression versus accuracy when choosing specific
$\varepsilon$ values to run ACP with and enables interpretability between the
different models.

</details>


### [263] [Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling](https://arxiv.org/abs/2505.22290)
*Fanzeng Xia,Yidong Luo,Tinko Sebastian Bartels,Yaqi Xu,Tongxin Li*

Key words: 大语言模型（LLMs）、推理能力、上下文搜索、测试时扩展、NP-hard任务

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文通过结合上下文搜索提示和测试时扩展，显著提升了大语言模型（LLMs）在复杂推理任务上的性能，成功解决了此前认为‘无解’的问题，并挑战了现有对LLMs能力的评估范式。

Motivation: 现有研究低估了大语言模型在复杂推理任务上的潜力，主要依赖简单上下文学习进行评估，忽略了先进技术的应用。

Method: 结合上下文搜索提示和内部扩展技术，系统地探索了LLMs在超难推理任务上的潜力。

Result: 在NP-hard任务和复杂真实世界规划基准上，成功率达到此前结果的30倍提升，且理论证明了该方法可扩展求解问题的复杂度类别。

Conclusion: 研究挑战了对LLMs能力的现有假设，呼吁重新评估推理评测方法以更全面反映其真实潜力。

Abstract: Recent research has highlighted that Large Language Models (LLMs), even when
trained to generate extended long reasoning steps, still face significant
challenges on hard reasoning problems. However, much of the existing literature
relies on direct prompting with simple in-context learning examples for
evaluation, which largely overlooks advanced techniques to elicit LLMs'
deliberate reasoning before drawing conclusions that LLMs hit a performance
ceiling. In this paper, we systematically explore the combined potential of
in-context search and test-time scaling on super hard reasoning tasks. We find
that by employing advanced in-context search prompting to LLMs augmented with
internal scaling, one can achieve transformative performance breakthroughs on
tasks previously deemed "unsolvable" (e.g., reported success rates below 5%).
We provide both empirical results and theoretical analysis of how this
combination can unleash LLM reasoning capabilities: i) Empirically, on
controlled NP-hard tasks and complex real-world planning benchmarks, our
approach achieves up to a 30x improvement in success rates compared to
previously reported results without any external mechanisms; ii) Theoretically,
we show that in-context search prompting, when combined with internal scaling,
significantly extends the complexity class of solvable reasoning problems.
These findings challenge prevailing assumptions about the limitations of LLMs
on complex tasks, indicating that current evaluation paradigms systematically
underestimate their true potential. Our work calls for a critical reassessment
of how LLM reasoning is benchmarked and a more robust evaluation strategy that
fully captures the true capabilities of contemporary LLMs, which can lead to a
better understanding of their operational reasoning boundaries in real-world
deployments.

</details>


### [264] [From Large AI Models to Agentic AI: A Tutorial on Future Intelligent Communications](https://arxiv.org/abs/2505.22311)
*Feibo Jiang,Cunhua Pan,Li Dong,Kezhi Wang,Octavia A. Dobre,Merouane Debbah*

Key words: 6G通信, 大型人工智能模型（LAMs）, Agentic AI, 多智能体系统, 智能通信

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 这篇教程系统性介绍了大型人工智能模型（LAMs）和Agentic AI技术在6G智能通信系统中的设计原理与应用，旨在为研究者提供前沿技术综述和实践指导。

Motivation: 6G通信时代下，智能通信系统面临感知响应能力受限、扩展性不足和动态环境适应性低等挑战，需通过LAMs和Agentic AI技术解决这些问题。

Method: 从LAMs技术演进分析入手，提出通信专用的LAM设计范式（含数据集构建及学习策略），并构建基于LAM的Agentic AI系统框架（含核心组件与交互机制）和多智能体协作框架。

Result: 梳理了LAMs（如LLM、LVM等）的适用场景，展示了LAM与Agentic AI在通信中的实际应用案例，并验证了其提升系统效能、安全性和可持续性的潜力。

Conclusion: 总结了当前研究的挑战与未来方向，为下一代高效、安全、可持续的智能通信系统发展提供支持。

Abstract: With the advent of 6G communications, intelligent communication systems face
multiple challenges, including constrained perception and response
capabilities, limited scalability, and low adaptability in dynamic
environments. This tutorial provides a systematic introduction to the
principles, design, and applications of Large Artificial Intelligence Models
(LAMs) and Agentic AI technologies in intelligent communication systems, aiming
to offer researchers a comprehensive overview of cutting-edge technologies and
practical guidance. First, we outline the background of 6G communications,
review the technological evolution from LAMs to Agentic AI, and clarify the
tutorial's motivation and main contributions. Subsequently, we present a
comprehensive review of the key components required for constructing LAMs. We
further categorize LAMs and analyze their applicability, covering Large
Language Models (LLMs), Large Vision Models (LVMs), Large Multimodal Models
(LMMs), Large Reasoning Models (LRMs), and lightweight LAMs. Next, we propose a
LAM-centric design paradigm tailored for communications, encompassing dataset
construction and both internal and external learning approaches. Building upon
this, we develop an LAM-based Agentic AI system for intelligent communications,
clarifying its core components such as planners, knowledge bases, tools, and
memory modules, as well as its interaction mechanisms. We also introduce a
multi-agent framework with data retrieval, collaborative planning, and
reflective evaluation for 6G. Subsequently, we provide a detailed overview of
the applications of LAMs and Agentic AI in communication scenarios. Finally, we
summarize the research challenges and future directions in current studies,
aiming to support the development of efficient, secure, and sustainable
next-generation intelligent communication systems.

</details>


### [265] [AgentDNS: A Root Domain Naming System for LLM Agents](https://arxiv.org/abs/2505.22368)
*Enfang Cui,Yujun Cheng,Rui She,Dan Liu,Zhiyuan Liang,Minxin Guo,Tianzheng Li,Qian Wei,Wenjuan Xing,Zhijie Zhong*

Key words: Large Language Model, AgentDNS, 服务发现, 互操作性, 多代理协作

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出AgentDNS，一种为LLM代理设计的域名和服务发现系统，解决不同代理和工具厂商间的服务发现、互操作性和通信问题。

Motivation: 现有协议在代理间互操作性和多代理通信方面取得进展，但缺乏跨厂商服务发现的标准方案。

Method: 借鉴传统DNS原则，设计AgentDNS，包含服务注册、语义发现、安全调用和统一计费机制。

Result: 通过架构和核心功能展示，AgentDNS能有效促进多代理协作。

Conclusion: AgentDNS有望解决跨组织和技术边界的服务发现与调用问题。

Abstract: The rapid evolution of Large Language Model (LLM) agents has highlighted
critical challenges in cross-vendor service discovery, interoperability, and
communication. Existing protocols like model context protocol and
agent-to-agent protocol have made significant strides in standardizing
interoperability between agents and tools, as well as communication among
multi-agents. However, there remains a lack of standardized protocols and
solutions for service discovery across different agent and tool vendors. In
this paper, we propose AgentDNS, a root domain naming and service discovery
system designed to enable LLM agents to autonomously discover, resolve, and
securely invoke third-party agent and tool services across organizational and
technological boundaries. Inspired by the principles of the traditional DNS,
AgentDNS introduces a structured mechanism for service registration, semantic
service discovery, secure invocation, and unified billing. We detail the
architecture, core functionalities, and use cases of AgentDNS, demonstrating
its potential to streamline multi-agent collaboration in real-world scenarios.
The source code will be published on https://github.com/agentdns.

</details>


### [266] [AI Mathematician: Towards Fully Automated Frontier Mathematical Research](https://arxiv.org/abs/2505.22451)
*Yuanhang Liu,Yanxing Huang,Yanqiao Wang,Peng Li,Yang Liu*

Key words: AI Mathematician, Large Reasoning Models, mathematical research, exploration mechanism, verification method

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了AI Mathematician (AIM)框架，利用大型推理模型(LRMs)支持前沿数学研究，通过探索机制和悲观合理验证方法解决研究问题的复杂性和程序严谨性挑战，并在实验中获得显著成果。

Motivation: 当前大型推理模型(LRMs)在数学竞赛题上表现出色，但在前沿数学研究中的应用仍面临研究问题复杂性和程序严谨性的挑战，因此需要新的框架来提升其研究能力。

Method: 提出AIM框架，采用探索机制促进更长的解决路径，并结合悲观合理验证方法来确保可靠性。

Result: 在多个实际数学课题的实验表明，AIM能自主构建大量证明片段并发现非平凡见解，验证了其在数学研究中的潜力。

Conclusion: AIM展示了LRMs在数学发现中的潜力，基于LRM的智能系统可能显著加速未来的数学研究。

Abstract: Large Reasoning Models (LRMs) have made significant progress in mathematical
capabilities in recent times. However, these successes have been primarily
confined to competition-level problems. In this work, we propose AI
Mathematician (AIM) framework, which harnesses the reasoning strength of LRMs
to support frontier mathematical research. We have identified two critical
challenges of mathematical research compared to competition, {\it the intrinsic
complexity of research problems} and {\it the requirement of procedural rigor}.
To address these challenges, AIM incorporates two core strategies: an
exploration mechanism to foster longer solution paths, and the pessimistic
reasonable verification method to ensure reliability.
  This early version of AIM already exhibits strong capability in tackling
research-level tasks. We conducted extensive experiments across several
real-world mathematical topics and obtained promising results. AIM is able to
autonomously construct substantial portions of proofs and uncover non-trivial
insights within each research area. These findings highlight the potential of
LRMs in mathematical discovery and suggest that LRM-based agent systems could
significantly accelerate mathematical research in the future.

</details>


### [267] [HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym](https://arxiv.org/abs/2505.22597)
*Ngoc La,Ruaridh Mon-Williams,Julie A. Shah*

Key words: 强化学习, 分层规划, HDDL, OpenAI Gym, 多智能体

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文介绍了HDDLGym，一个将分层规划与强化学习（RL）无缝集成的Python工具，支持多智能体场景，并提供了使用指南和示例。

Motivation: 现有工具缺乏将分层规划与RL集成的能力，而HDDLGym填补了这一空白，为研究和实践提供了便利。

Method: 基于HDDL（分层领域定义语言）开发了HDDLGym工具，自动生成OpenAI Gym环境，支持多智能体协作规划。

Result: HDDLGym成功集成了HDDL与RL，并通过Transport和Overcooked等示例展示了其实用性。

Conclusion: HDDLGym是研究分层规划中RL的有价值工具，尤其适用于多智能体场景。

Abstract: In recent years, reinforcement learning (RL) methods have been widely tested
using tools like OpenAI Gym, though many tasks in these environments could also
benefit from hierarchical planning. However, there is a lack of a tool that
enables seamless integration of hierarchical planning with RL. Hierarchical
Domain Definition Language (HDDL), used in classical planning, introduces a
structured approach well-suited for model-based RL to address this gap. To
bridge this integration, we introduce HDDLGym, a Python-based tool that
automatically generates OpenAI Gym environments from HDDL domains and problems.
HDDLGym serves as a link between RL and hierarchical planning, supporting
multi-agent scenarios and enabling collaborative planning among agents. This
paper provides an overview of HDDLGym's design and implementation, highlighting
the challenges and design choices involved in integrating HDDL with the Gym
interface, and applying RL policies to support hierarchical planning. We also
provide detailed instructions and demonstrations for using the HDDLGym
framework, including how to work with existing HDDL domains and problems from
International Planning Competitions, exemplified by the Transport domain.
Additionally, we offer guidance on creating new HDDL domains for multi-agent
scenarios and demonstrate the practical use of HDDLGym in the Overcooked
domain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a
valuable tool for studying RL in hierarchical planning, particularly in
multi-agent contexts.

</details>


### [268] [Capability-Based Scaling Laws for LLM Red-Teaming](https://arxiv.org/abs/2505.20162)
*Alexander Panfilov,Paul Kassianik,Maksym Andriushchenko,Jonas Geiping*

Key words: 语言模型，越狱攻击，红队测试，能力差距，缩放定律

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 研究探讨了语言模型越狱攻击的成功率与攻击者-目标能力差距的关系，发现能力更强的模型作为攻击者更有效，但当目标能力超过攻击者时，成功率骤降。研究提出了一条越狱攻击的缩放定律。

Motivation: 随着语言模型能力和自主性的增强，通过红队测试识别漏洞对安全部署至关重要。传统方法在攻击者能力弱于目标时可能失效，因此需要研究能力差距对攻击成功率的影响。

Method: 通过评估500多个基于LLM的越狱攻击对（模拟人类红队测试），涵盖不同家族、规模和能力水平的模型，分析攻击成功率和能力差距的关系。

Result: 发现三个主要趋势：1) 能力更强的模型攻击更有效；2) 目标能力超过攻击者时成功率骤降；3) 攻击成功率与MMLU-Pro社会科学测试的高表现相关。由此提出了越狱攻击的缩放定律。

Conclusion: 固定能力的攻击者（如人类）可能对未来的模型无效，开源模型的能力增长可能增加现有系统的风险。模型提供者需准确测量和控制模型的操纵能力以减少攻击风险。

Abstract: As large language models grow in capability and agency, identifying
vulnerabilities through red-teaming becomes vital for safe deployment. However,
traditional prompt-engineering approaches may prove ineffective once
red-teaming turns into a weak-to-strong problem, where target models surpass
red-teamers in capabilities. To study this shift, we frame red-teaming through
the lens of the capability gap between attacker and target. We evaluate more
than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic
human red-teamers across diverse families, sizes, and capability levels. Three
strong trends emerge: (i) more capable models are better attackers, (ii) attack
success drops sharply once the target's capability exceeds the attacker's, and
(iii) attack success rates correlate with high performance on social science
splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking
scaling law that predicts attack success for a fixed target based on
attacker-target capability gap. These findings suggest that fixed-capability
attackers (e.g., humans) may become ineffective against future models,
increasingly capable open-source models amplify risks for existing systems, and
model providers must accurately measure and control models' persuasive and
manipulative abilities to limit their effectiveness as attackers.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [269] [Towards Human-Like Trajectory Prediction for Autonomous Driving: A Behavior-Centric Approach](https://arxiv.org/abs/2505.21565)
*Haicheng Liao,Zhenning Li,Guohui Zhang,Keqiang Li,Chengzhong Xu*

Key words: 自动驾驶, 轨迹预测, 动态交互, 行为感知, HiT模型

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: HiT, 一种新型车辆轨迹预测模型，通过结合行为感知模块和动态中心性测量，显著提升了预测准确性和人性化表现，适用于复杂动态交通环境。

Motivation: 为提升自动驾驶系统在复杂动态交通环境中的车辆轨迹预测能力，解决传统静态图结构方法无法捕捉间接交互影响的问题。

Method: HiT模型创新性地使用动态框架，结合行为感知模块和动态中心性测量，捕捉车辆间的直接与间接交互影响。

Result: 实验证明，HiT在多种真实数据集上均优于其他顶级模型，尤其在涉及激进驾驶行为的场景中表现突出。

Conclusion: 该研究为轨迹预测领域提供了更可靠且可解释的方法，显著提升了自动驾驶系统的安全性和效率。

Abstract: Predicting the trajectories of vehicles is crucial for the development of
autonomous driving (AD) systems, particularly in complex and dynamic traffic
environments. In this study, we introduce HiT (Human-like Trajectory
Prediction), a novel model designed to enhance trajectory prediction by
incorporating behavior-aware modules and dynamic centrality measures. Unlike
traditional methods that primarily rely on static graph structures, HiT
leverages a dynamic framework that accounts for both direct and indirect
interactions among traffic participants. This allows the model to capture the
subtle yet significant influences of surrounding vehicles, enabling more
accurate and human-like predictions. To evaluate HiT's performance, we
conducted extensive experiments using diverse and challenging real-world
datasets, including NGSIM, HighD, RounD, ApolloScape, and MoCAD++. The results
demonstrate that HiT consistently outperforms other top models across multiple
metrics, particularly excelling in scenarios involving aggressive driving
behaviors. This research presents a significant step forward in trajectory
prediction, offering a more reliable and interpretable approach for enhancing
the safety and efficiency of fully autonomous driving systems.

</details>


### [270] [Fast and Cost-effective Speculative Edge-Cloud Decoding with Early Exits](https://arxiv.org/abs/2505.21594)
*Yeshwanth Venkatesha,Souvik Kundu,Priyadarshini Panda*

Key words: 大语言模型, 边缘计算, 云协同, 实时应用, 延迟优化

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 该论文提出了一种边缘-云协同的解码框架，通过边缘设备上的小型草稿模型和云端的大型目标模型，显著降低了延迟和成本。

Motivation: 解决边缘设备部署大语言模型时因计算资源有限导致的高延迟和高成本问题。

Method: 采用边缘-云协同解码框架，边缘设备运行小型草稿模型，云端运行大型目标模型，并引入早期退出机制以提升并行性。

Result: 实验显示，该方法比云端自回归解码延迟降低35%，预起草机制额外提升11%性能，并在四足机器人上实现21%的速度提升。

Conclusion: 该框架能有效提升资源受限边缘设备上LLM和VLM的实时应用潜力。

Abstract: Large Language Models (LLMs) enable various applications on edge devices such
as smartphones, wearables, and embodied robots. However, their deployment often
depends on expensive cloud-based APIs, creating high operational costs, which
limit access for smaller organizations and raise sustainability concerns.
Certain LLMs can be deployed on-device, offering a cost-effective solution with
reduced latency and improved privacy. Yet, limited computing resources
constrain the size and accuracy of models that can be deployed, necessitating a
collaborative design between edge and cloud. We propose a fast and
cost-effective speculative edge-cloud decoding framework with a large target
model on the server and a small draft model on the device. By introducing early
exits in the target model, tokens are generated mid-verification, allowing the
client to preemptively draft subsequent tokens before final verification, thus
utilizing idle time and enhancing parallelism between edge and cloud. Using an
NVIDIA Jetson Nano (client) and an A100 GPU (server) with Vicuna-68M (draft)
and Llama2-7B (target) models, our method achieves up to a 35% reduction in
latency compared to cloud-based autoregressive decoding, with an additional 11%
improvement from preemptive drafting. To demonstrate real-world applicability,
we deploy our method on the Unitree Go2 quadruped robot using Vision-Language
Model (VLM) based control, achieving a 21% speedup over traditional cloud-based
autoregressive decoding. These results demonstrate the potential of our
framework for real-time LLM and VLM applications on resource-constrained edge
devices.

</details>


### [271] [PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation](https://arxiv.org/abs/2505.21652)
*Yifan Yin,Zhengtao Han,Shivam Aarya,Jianxin Wang,Shuhang Xu,Jiawei Peng,Angtian Wang,Alan Yuille,Tianmin Shu*

Key words: 细粒度机器人操作，部件级指令，3D对象标注，长时任务，基准评测

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 该论文介绍了首个用于细粒度机器人操作的大规模基准PartInstruct，包含513个对象实例和1302个任务，标注了部件级信息。实验显示，现有模型在3D空间中的部件概念理解和长时任务中仍存在不足。

Motivation: 目前缺乏用于细粒度机器人操作的大规模数据集，尤其是包含部件级标注和多样化3D对象实例的数据。该研究旨在填补这一空白。

Method: 构建了PartInstruct基准，包含513个对象实例（14类）和1302个任务（16类），并合成10,000+专家演示（含部件级指令和3D信息）。设计了测试套件评估模型的泛化能力。

Result: 当前最先进的机器人操作模型（如端到端视觉-语言策略和双层规划模型）在部件概念理解和3D空间动作预测上表现不佳，尤其是在长时任务中。

Conclusion: PartInstruct为细粒度机器人操作提供了重要基准，揭示了现有模型在部件级任务上的局限性，为未来研究指明了改进方向。

Abstract: Fine-grained robot manipulation, such as lifting and rotating a bottle to
display the label on the cap, requires robust reasoning about object parts and
their relationships with intended tasks. Despite recent advances in training
general-purpose robot manipulation policies guided by language instructions,
there is a notable lack of large-scale datasets for fine-grained manipulation
tasks with part-level instructions and diverse 3D object instances annotated
with part-level labels. In this work, we introduce PartInstruct, the first
large-scale benchmark for training and evaluating fine-grained robot
manipulation models using part-level instructions. PartInstruct comprises 513
object instances across 14 categories, each annotated with part-level
information, and 1302 fine-grained manipulation tasks organized into 16 task
classes. Our training set consists of over 10,000 expert demonstrations
synthesized in a 3D simulator, where each demonstration is paired with a
high-level task instruction, a chain of base part-based skill instructions, and
ground-truth 3D information about the object and its parts. Additionally, we
designed a comprehensive test suite to evaluate the generalizability of learned
policies across new states, objects, and tasks. We evaluated several
state-of-the-art robot manipulation approaches, including end-to-end
vision-language policy learning and bi-level planning models for robot
manipulation on our benchmark. The experimental results reveal that current
models struggle to robustly ground part concepts and predict actions in 3D
space, and face challenges when manipulating object parts in long-horizon
tasks.

</details>


### [272] [Streaming Flow Policy: Simplifying diffusion$/$flow-matching policies by treating action trajectories as flow trajectories](https://arxiv.org/abs/2505.21851)
*Sunshine Jiang,Xiaolin Fang,Nicholas Roy,Tomás Lozano-Pérez,Leslie Pack Kaelbling,Siddharth Ancha*

Key words: 扩散策略, 流匹配, 模仿学习, 机器人控制, 实时执行

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 本文简化了扩散/流匹配策略，通过将动作轨迹视为流轨迹，并从中采样而非纯噪声，实现了实时动作流传输，提升了机器人策略执行速度和模仿学习性能。

Motivation: 现有扩散/流匹配策略在模仿学习复杂多模态动作轨迹时计算成本高，且需等待完整采样后才能执行动作。本文旨在简化这一过程，实现实时动作流传输。

Method: 提出一种方法，从上一动作的窄高斯分布中采样，通过学习流匹配的速度场逐步生成动作序列，支持实时流传输。

Result: 提出的流策略优于现有方法，实现了更快的策略执行和更紧密的传感器-运动闭环，同时保持了多模态行为建模能力。

Conclusion: 通过简化采样过程并支持实时流传输，本文方法显著提升了模仿学习的效率和性能。

Abstract: Recent advances in diffusion$/$flow-matching policies have enabled imitation
learning of complex, multi-modal action trajectories. However, they are
computationally expensive because they sample a trajectory of trajectories: a
diffusion$/$flow trajectory of action trajectories. They discard intermediate
action trajectories, and must wait for the sampling process to complete before
any actions can be executed on the robot. We simplify diffusion$/$flow policies
by treating action trajectories as flow trajectories. Instead of starting from
pure noise, our algorithm samples from a narrow Gaussian around the last
action. Then, it incrementally integrates a velocity field learned via flow
matching to produce a sequence of actions that constitute a single trajectory.
This enables actions to be streamed to the robot on-the-fly during the flow
sampling process, and is well-suited for receding horizon policy execution.
Despite streaming, our method retains the ability to model multi-modal
behavior. We train flows that stabilize around demonstration trajectories to
reduce distribution shift and improve imitation learning performance. Streaming
flow policy outperforms prior methods while enabling faster policy execution
and tighter sensorimotor loops for learning-based robot control. Project
website: https://streaming-flow-policy.github.io/

</details>


### [273] [Learning Compositional Behaviors from Demonstration and Language](https://arxiv.org/abs/2505.21981)
*Weiyu Liu,Neil Nie,Ruohan Zhang,Jiayuan Mao,Jiajun Wu*

Key words: BLADE, imitation learning, model-based planning, large language models, robotic manipulation

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: BLADE是一个结合模仿学习和基于模型规划的框架，通过语言标注的演示和大型语言模型（LLMs）提取抽象动作知识，构建高层动作表示库，并在仿真和真实机器人上验证了其泛化能力。

Motivation: 解决长时程机器人操作中结合模仿学习和模型规划的需求，并实现对新情境（如初始状态、目标变化等）的泛化。

Method: 利用语言标注的演示，通过LLMs提取动作知识，构建结构化高层动作表示库，包含视觉感知中的前提条件和效果，并实现为神经网络策略。

Result: BLADE能自动恢复结构化表示，无需人工标注或符号定义，在仿真和真实机器人上展示了对新情境的强泛化能力。

Conclusion: BLADE通过语言和演示的结合，有效解决了复杂机器人操作任务，展示了较强的适应性和泛化性能。

Abstract: We introduce Behavior from Language and Demonstration (BLADE), a framework
for long-horizon robotic manipulation by integrating imitation learning and
model-based planning. BLADE leverages language-annotated demonstrations,
extracts abstract action knowledge from large language models (LLMs), and
constructs a library of structured, high-level action representations. These
representations include preconditions and effects grounded in visual perception
for each high-level action, along with corresponding controllers implemented as
neural network-based policies. BLADE can recover such structured
representations automatically, without manually labeled states or symbolic
definitions. BLADE shows significant capabilities in generalizing to novel
situations, including novel initial states, external state perturbations, and
novel goals. We validate the effectiveness of our approach both in simulation
and on real robots with a diverse set of objects with articulated parts,
partial observability, and geometric constraints.

</details>


### [274] [Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge](https://arxiv.org/abs/2505.21906)
*Zhongyi Zhou,Yichen Zhu,Junjie Wen,Chaomin Shen,Yi Xu*

Key words: Vision-Language-Action, 混合专家模型, 三阶段训练, 开放世界推理, 机器人推理

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: ChatVLA-2提出了一种新型的混合专家VLA模型，通过三阶段训练流程保持VLM的核心能力并实现可行动的推理，在数学推理和OCR任务中表现优异。

Motivation: 为了解决现有端到端VLA系统在微调时丢失VLM核心能力的问题，提出一种具备开放世界推理能力和可行动推理的通用VLA模型。

Method: 采用混合专家VLA模型（ChatVLA-2）和三阶段训练流程，保留VLM的原始优势同时实现可行动的推理。

Result: 模型在数学推理、OCR和空间推理任务中表现卓越，超越了现有模仿学习方法如OpenVLA、DexVLA和pi-zero。

Conclusion: ChatVLA-2是迈向具备强大推理能力的通用机器人基础模型的重要进展。

Abstract: Vision-language-action (VLA) models have emerged as the next generation of
models in robotics. However, despite leveraging powerful pre-trained
Vision-Language Models (VLMs), existing end-to-end VLA systems often lose key
capabilities during fine-tuning as the model adapts to specific robotic tasks.
We argue that a generalizable VLA model should retain and expand upon the VLM's
core competencies: 1) Open-world embodied reasoning - the VLA should inherit
the knowledge from VLM, i.e., recognize anything that the VLM can recognize,
capable of solving math problems, possessing visual-spatial intelligence, 2)
Reasoning following - effectively translating the open-world reasoning into
actionable steps for the robot. In this work, we introduce ChatVLA-2, a novel
mixture-of-expert VLA model coupled with a specialized three-stage training
pipeline designed to preserve the VLM's original strengths while enabling
actionable reasoning. To validate our approach, we design a math-matching task
wherein a robot interprets math problems written on a whiteboard and picks
corresponding number cards from a table to solve equations. Remarkably, our
method exhibits exceptional mathematical reasoning and OCR capabilities,
despite these abilities not being explicitly trained within the VLA.
Furthermore, we demonstrate that the VLA possesses strong spatial reasoning
skills, enabling it to interpret novel directional instructions involving
previously unseen objects. Overall, our method showcases reasoning and
comprehension abilities that significantly surpass state-of-the-art imitation
learning methods such as OpenVLA, DexVLA, and pi-zero. This work represents a
substantial advancement toward developing truly generalizable robotic
foundation models endowed with robust reasoning capacities.

</details>


### [275] [DORAEMON: Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation](https://arxiv.org/abs/2505.21969)
*Tianjun Gu,Linfeng Li,Xuhong Wang,Chenghua Gong,Jingyu Gong,Zhizhong Zhang,Yuan Xie,Lizhuang Ma,Xin Tan*

Key words: 自主导航、视觉语言模型、认知启发框架、零样本学习、DORAEMON

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: DORAEMON是一种受认知启发的导航框架，通过模拟人类导航能力解决了现有VLM方法在时空不连续性和任务理解不足的问题，在多种数据集上表现优异。

Motivation: 家用服务机器人在陌生环境中导航需要结合低层级路径规划和高层级场景理解，而现有VLM方法存在时空不连续性、存储表示非结构化和任务理解不足等问题。

Method: 提出DORAEMON框架，包含模仿人类导航的腹侧流（决策优化）和背侧流（时空连续性处理），并引入Nav-Ensurance保障安全与效率。

Result: 在HM3D、MP3D和GOAT数据集上取得SOTA性能（SR和SPL指标），同时提出新评估指标AORI。

Conclusion: DORAEMON无需预训练或先验地图即可实现零样本自主导航，验证了其有效性。

Abstract: Adaptive navigation in unfamiliar environments is crucial for household
service robots but remains challenging due to the need for both low-level path
planning and high-level scene understanding. While recent vision-language model
(VLM) based zero-shot approaches reduce dependence on prior maps and
scene-specific training data, they face significant limitations: spatiotemporal
discontinuity from discrete observations, unstructured memory representations,
and insufficient task understanding leading to navigation failures. We propose
DORAEMON (Decentralized Ontology-aware Reliable Agent with Enhanced Memory
Oriented Navigation), a novel cognitive-inspired framework consisting of
Ventral and Dorsal Streams that mimics human navigation capabilities. The
Dorsal Stream implements the Hierarchical Semantic-Spatial Fusion and Topology
Map to handle spatiotemporal discontinuities, while the Ventral Stream combines
RAG-VLM and Policy-VLM to improve decision-making. Our approach also develops
Nav-Ensurance to ensure navigation safety and efficiency. We evaluate DORAEMON
on the HM3D, MP3D, and GOAT datasets, where it achieves state-of-the-art
performance on both success rate (SR) and success weighted by path length (SPL)
metrics, significantly outperforming existing methods. We also introduce a new
evaluation metric (AORI) to assess navigation intelligence better.
Comprehensive experiments demonstrate DORAEMON's effectiveness in zero-shot
autonomous navigation without requiring prior map building or pre-training.

</details>


### [276] [MIND-Stack: Modular, Interpretable, End-to-End Differentiability for Autonomous Navigation](https://arxiv.org/abs/2505.21734)
*Felix Jahncke,Johannes Betz*

Key words: 导航算法, 模块化设计, 端到端学习, sim-to-real, 嵌入式平台

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: MIND-Stack是一种模块化软件堆栈，结合了端到端可微分的神经网络与人类可解释的状态表示，通过定位模块减少控制误差，并在真实嵌入式平台上验证了其性能。

Motivation: 现有基于规则的方法难以从大数据中学习，而端到端神经网络缺乏透明性和模块化，MIND-Stack旨在结合两者的优点。

Method: 提出MIND-Stack，包含定位网络和Stanley控制器，支持端到端可微分和模块化设计。

Result: 实验证明定位模块通过端到端可微分减少控制损失，性能优于现有算法，并在真实平台上验证了sim-to-real能力。

Conclusion: MIND-Stack展示了模块化与端到端学习的潜力，未来将整合更多模块以提升稳定性与性能。

Abstract: Developing robust, efficient navigation algorithms is challenging. Rule-based
methods offer interpretability and modularity but struggle with learning from
large datasets, while end-to-end neural networks excel in learning but lack
transparency and modularity. In this paper, we present MIND-Stack, a modular
software stack consisting of a localization network and a Stanley Controller
with intermediate human interpretable state representations and end-to-end
differentiability. Our approach enables the upstream localization module to
reduce the downstream control error, extending its role beyond state
estimation. Unlike existing research on differentiable algorithms that either
lack modules of the autonomous stack to span from sensor input to actuator
output or real-world implementation, MIND-Stack offers both capabilities. We
conduct experiments that demonstrate the ability of the localization module to
reduce the downstream control loss through its end-to-end differentiability
while offering better performance than state-of-the-art algorithms. We showcase
sim-to-real capabilities by deploying the algorithm on a real-world embedded
autonomous platform with limited computation power and demonstrate simultaneous
training of both the localization and controller towards one goal. While
MIND-Stack shows good results, we discuss the incorporation of additional
modules from the autonomous navigation pipeline in the future, promising even
greater stability and performance in the next iterations of the framework.

</details>


### [277] [ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning](https://arxiv.org/abs/2505.22094)
*Tonghe Zhang,Yu Chao,Sicang Su,Yu Wang*

Key words: 强化学习,流匹配,机器人控制,马尔可夫过程,去噪

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: ReinFlow是一种简单高效的在线强化学习框架，通过将可学习的噪声注入流策略中，将确定性路径转换为离散时间马尔可夫过程，从而提升探索能力和训练稳定性。该方法在多种连续机器人控制任务中表现优异，显著提高了任务奖励并节省了计算时间。

Motivation: 现有强化学习方法在处理连续机器人控制任务时，探索能力和训练稳定性不足，且计算成本较高。ReinFlow旨在通过流匹配策略的优化，解决这些问题。

Method: ReinFlow框架通过将可学习的噪声注入流策略的确定性路径中，将其转换为离散时间马尔可夫过程，从而实现精确的似然计算和稳定的训练。支持在极少或单步去噪条件下微调多种流模型变体。

Result: 在长视觉输入和稀疏奖励任务中，ReinFlow的收益表现显著提升（135.36%增长），计算时间节省82.63%；在状态和视觉操作任务中，成功率达增长40.34%，计算时间节省23.20%。

Conclusion: ReinFlow高效提升了流匹配策略的性能，尤其在少步去噪情况下表现优异，显著优于现有方法，并在计算效率上有明显优势。

Abstract: We propose ReinFlow, a simple yet effective online reinforcement learning
(RL) framework that fine-tunes a family of flow matching policies for
continuous robotic control. Derived from rigorous RL theory, ReinFlow injects
learnable noise into a flow policy's deterministic path, converting the flow
into a discrete-time Markov Process for exact and straightforward likelihood
computation. This conversion facilitates exploration and ensures training
stability, enabling ReinFlow to fine-tune diverse flow model variants,
including Rectified Flow [35] and Shortcut Models [19], particularly at very
few or even one denoising step. We benchmark ReinFlow in representative
locomotion and manipulation tasks, including long-horizon planning with visual
input and sparse reward. The episode reward of Rectified Flow policies obtained
an average net growth of 135.36% after fine-tuning in challenging legged
locomotion tasks while saving denoising steps and 82.63% of wall time compared
to state-of-the-art diffusion RL fine-tuning method DPPO [43]. The success rate
of the Shortcut Model policies in state and visual manipulation tasks achieved
an average net increase of 40.34% after fine-tuning with ReinFlow at four or
even one denoising step, whose performance is comparable to fine-tuned DDIM
policies while saving computation time for an average of 23.20%. Project
Webpage: https://reinflow.github.io/

</details>


### [278] [From Strangers to Assistants: Fast Desire Alignment for Embodied Agent-User Adaptation](https://arxiv.org/abs/2505.22503)
*Yuanfei Wang,Xinju Huang,Fangwei Zhong,Yaodong Yang,Yizhou Wang,Yuanpei Chen,Hao Dong*

Key words: embodied agents, desire alignment, mental reasoning, communication efficiency, LLM

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 论文提出了一个名为FAMER的新框架，用于快速对齐用户潜在需求，通过心理推理机制和反射式通信模块，提升任务执行和沟通效率。

Motivation: 现实应用中，智能体需要与陌生代理和人类用户协作，但用户的指令往往模糊且隐晦，因此快速准确地对齐用户潜在需求成为关键能力。

Method: 开发了家庭辅助模拟环境HA-Desire，结合LLM驱动的人类用户代理，并提出了FAMER框架，通过心理推理机制、反射式通信模块和目标相关信息提取实现需求对齐。

Result: 实验表明，FAMER显著提升了任务执行和沟通效率，使智能体能够快速适应用户特定需求。

Conclusion: FAMER框架为复杂环境中智能体的需求对齐提供了有效解决方案。

Abstract: While embodied agents have made significant progress in performing complex
physical tasks, real-world applications demand more than pure task execution.
The agents must collaborate with unfamiliar agents and human users, whose goals
are often vague and implicit. In such settings, interpreting ambiguous
instructions and uncovering underlying desires is essential for effective
assistance. Therefore, fast and accurate desire alignment becomes a critical
capability for embodied agents. In this work, we first develop a home
assistance simulation environment HA-Desire that integrates an LLM-driven human
user agent exhibiting realistic value-driven goal selection and communication.
The ego agent must interact with this proxy user to infer and adapt to the
user's latent desires. To achieve this, we present a novel framework FAMER for
fast desire alignment, which introduces a desire-based mental reasoning
mechanism to identify user intent and filter desire-irrelevant actions. We
further design a reflection-based communication module that reduces redundant
inquiries, and incorporate goal-relevant information extraction with memory
persistence to improve information reuse and reduce unnecessary exploration.
Extensive experiments demonstrate that our framework significantly enhances
both task execution and communication efficiency, enabling embodied agents to
quickly adapt to user-specific desires in complex embodied environments.

</details>


### [279] [LiDAR Based Semantic Perception for Forklifts in Outdoor Environments](https://arxiv.org/abs/2505.22258)
*Benjamin Serfling,Hannes Reichert,Lorenzo Bayerlein,Konrad Doll,Kati Radkhah-Lens*

Key words: LiDAR, 语义分割, 自动叉车, 双传感器, 安全导航

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 该论文提出了一种新型的基于LiDAR的语义分割框架，专为复杂户外环境下的自动叉车设计。通过集成双LiDAR系统，该方法显著提升了障碍物检测与分割的空间精度。

Motivation: 针对工业物料搬运任务中复杂的动态和静态障碍物检测需求，研究旨在开发一种高效、安全的自动叉车导航解决方案。

Method: 采用双LiDAR系统（前向和向下倾斜），结合高分辨率3D点云数据，设计轻量级且鲁棒的语义分割方法，分类安全关键实例和场景类别。

Result: 实验验证表明，该方法在满足严格实时性要求的同时，实现了高分割精度。

Conclusion: 该框架适用于动态仓库和场院环境中的安全感知全自动叉车导航。

Abstract: In this study, we present a novel LiDAR-based semantic segmentation framework
tailored for autonomous forklifts operating in complex outdoor environments.
Central to our approach is the integration of a dual LiDAR system, which
combines forward-facing and downward-angled LiDAR sensors to enable
comprehensive scene understanding, specifically tailored for industrial
material handling tasks. The dual configuration improves the detection and
segmentation of dynamic and static obstacles with high spatial precision. Using
high-resolution 3D point clouds captured from two sensors, our method employs a
lightweight yet robust approach that segments the point clouds into
safety-critical instance classes such as pedestrians, vehicles, and forklifts,
as well as environmental classes such as driveable ground, lanes, and
buildings. Experimental validation demonstrates that our approach achieves high
segmentation accuracy while satisfying strict runtime requirements,
establishing its viability for safety-aware, fully autonomous forklift
navigation in dynamic warehouse and yard environments.

</details>


### [280] [SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale Imitation Learning](https://arxiv.org/abs/2505.22626)
*Yu Zhang,Yuqi Xie,Huihan Liu,Rutav Shah,Michael Wan,Linxi Fan,Yuke Zhu*

Key words: 模仿学习,数据筛选,自监督学习,机器人学习,SCIZOR

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: SCIZOR是一个自监督数据筛选框架，用于提升模仿学习效果，去除低质量状态-动作对，平均提升15.4%性能。

Motivation: 模仿学习依赖大规模数据集，但数据质量参差不齐，影响性能，需要自动筛选低质量样本。

Method: SCIZOR框架利用任务进度预测器筛选次优数据，并通过去重模块处理冗余数据。

Result: 实验显示，SCIZOR提升了模仿学习性能，平均改进15.4%。

Conclusion: SCIZOR能有效筛选数据，提升模仿学习效果。

Abstract: Imitation learning advances robot capabilities by enabling the acquisition of
diverse behaviors from human demonstrations. However, large-scale datasets used
for policy training often introduce substantial variability in quality, which
can negatively impact performance. As a result, automatically curating datasets
by filtering low-quality samples to improve quality becomes essential. Existing
robotic curation approaches rely on costly manual annotations and perform
curation at a coarse granularity, such as the dataset or trajectory level,
failing to account for the quality of individual state-action pairs. To address
this, we introduce SCIZOR, a self-supervised data curation framework that
filters out low-quality state-action pairs to improve the performance of
imitation learning policies. SCIZOR targets two complementary sources of
low-quality data: suboptimal data, which hinders learning with undesirable
actions, and redundant data, which dilutes training with repetitive patterns.
SCIZOR leverages a self-supervised task progress predictor for suboptimal data
to remove samples lacking task progression, and a deduplication module
operating on joint state-action representation for samples with redundant
patterns. Empirically, we show that SCIZOR enables imitation learning policies
to achieve higher performance with less data, yielding an average improvement
of 15.4% across multiple benchmarks. More information is available at:
https://ut-austin-rpl.github.io/SCIZOR/

</details>


### [281] [FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control](https://arxiv.org/abs/2505.22642)
*Younggyo Seo,Carmelo Sferrazza,Haoran Geng,Michal Nauman,Zhao-Heng Yin,Pieter Abbeel*

Key words: FastTD3, 强化学习, 机器人, 训练加速

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: FastTD3是一种快速、简单的强化学习算法，能显著加快人形机器人在多种环境中的训练速度，仅需3小时即可完成训练。

Motivation: 当前强化学习在机器人领域应用广泛，但复杂性和长训练时间仍是主要瓶颈。FastTD3旨在解决这些问题。

Method: 改进的TD3算法，包括并行模拟、大批量更新、分布化评论家和精心调整的超参数。

Result: 在HumanoidBench任务中，FastTD3单A100 GPU下3小时内完成训练，且保持训练稳定性。

Conclusion: FastTD3为机器人强化学习研究提供了高效、稳定的解决方案。

Abstract: Reinforcement learning (RL) has driven significant progress in robotics, but
its complexity and long training times remain major bottlenecks. In this
report, we introduce FastTD3, a simple, fast, and capable RL algorithm that
significantly speeds up training for humanoid robots in popular suites such as
HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably
simple: we train an off-policy TD3 agent with several modifications -- parallel
simulation, large-batch updates, a distributional critic, and carefully tuned
hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours
on a single A100 GPU, while remaining stable during training. We also provide a
lightweight and easy-to-use implementation of FastTD3 to accelerate RL research
in robotics.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [282] [iDSE: Navigating Design Space Exploration in High-Level Synthesis Using LLMs](https://arxiv.org/abs/2505.22086)
*Runkai Li,Jia Xiong,Xi Wang*

Key words: High-Level Synthesis, LLM, Design Space Exploration, Pareto Front, Optimization

<details>
  <summary>Details</summary>

Main category: cs.AR

TL;DR: 该论文提出了iDSE框架，利用LLM辅助高效导航HLS设计空间，显著提升Pareto前沿设计获取效率。

Motivation: 传统HLS设计空间探索方法存在计算成本高和结果次优问题，需更高效的优化方案。

Method: iDSE结合LLM的思维模式，通过智能剪枝和多路径优化，快速收敛到Pareto前沿。

Result: iDSE在接近Pareto前沿方面比启发式方法快5.1~16.6倍，仅需NSGA-II 4.6%的设计探索。

Conclusion: LLM在HLS多目标优化中展现变革潜力，为设计空间探索提供了新方向。

Abstract: High-Level Synthesis (HLS) serves as an agile hardware development tool that
streamlines the circuit design by abstracting the register transfer level into
behavioral descriptions, while allowing designers to customize the generated
microarchitectures through optimization directives. However, the combinatorial
explosion of possible directive configurations yields an intractable design
space. Traditional design space exploration (DSE) methods, despite adopting
heuristics or constructing predictive models to accelerate Pareto-optimal
design acquisition, still suffer from prohibitive exploration costs and
suboptimal results. Addressing these concerns, we introduce iDSE, the first
LLM-aided DSE framework that leverages HLS design quality perception to
effectively navigate the design space. iDSE intelligently pruns the design
space to guide LLMs in calibrating representative initial sampling designs,
expediting convergence toward the Pareto front. By exploiting the convergent
and divergent thinking patterns inherent in LLMs for hardware optimization,
iDSE achieves multi-path refinement of the design quality and diversity.
Extensive experiments demonstrate that iDSE outperforms heuristic-based DSE
methods by 5.1$\times$$\sim$16.6$\times$ in proximity to the reference Pareto
front, matching NSGA-II with only 4.6% of the explored designs. Our work
demonstrates the transformative potential of LLMs in scalable and efficient HLS
design optimization, offering new insights into multiobjective optimization
challenges.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [283] [A Kernelised Stein Discrepancy for Assessing the Fit of Inhomogeneous Random Graph Models](https://arxiv.org/abs/2505.21580)
*Anum Fatima,Gesine Reinert*

Key words: 不均匀随机图模型, 核化Stein差异, 拟合优度检验, 高维数据, 图表示

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文开发了一种基于核化Stein差异（KSD）的拟合优度检验方法，适用于单次观测的不均匀随机图模型（IRG），适用于任意规模的网络且不依赖于统计量的渐近分布。

Motivation: 复杂数据常以图形式表示，而图可视为随机图的实现。针对高维数据的一般快速拟合优度检验，核化Stein差异（KSD）测试是一种强大工具。本文旨在开发适用于IRG模型的KSD型拟合优度检验。

Method: 开发并测试了一种KSD型拟合优度检验方法，适用于IRG模型，该检验可通过单次网络观测进行，适用于任意规模的网络且不依赖统计量的渐近分布。

Result: 该方法在理论和实践上均有效，提供了理论保证，且适用于各种规模的网络。

Conclusion: 该研究为IRG模型的拟合优度检验提供了一种高效且普适的方法，具有广泛的应用潜力。

Abstract: Complex data are often represented as a graph, which in turn can often be
viewed as a realisation of a random graph, such as of an inhomogeneous random
graph model (IRG). For general fast goodness-of-fit tests in high dimensions,
kernelised Stein discrepancy (KSD) tests are a powerful tool. Here, we develop,
test, and analyse a KSD-type goodness-of-fit test for IRG models that can be
carried out with a single observation of the network. The test is applicable to
a network of any size and does not depend on the asymptotic distribution of the
test statistic. We also provide theoretical guarantees.

</details>


### [284] [STACI: Spatio-Temporal Aleatoric Conformal Inference](https://arxiv.org/abs/2505.21658)
*Brandon R. Feng,David Keetae Park,Xihaier Luo,Arantxa Urdangarin,Shinjae Yoo,Brian J. Reich*

Key words: 高斯过程，时空建模，不确定性量化，变分贝叶斯神经网络，共形推断

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文提出了STACI框架，通过结合变分贝叶斯神经网络和非平稳时空高斯过程，解决了传统方法的可扩展性和协方差核限制问题，并展示了其在大规模数据集上的高效表现。

Motivation: 传统的空间-时间高斯过程（GP）虽然能提供可解释的不确定性量化，但面临可扩展性和协方差核限制的问题；而深度学习模型虽然可扩展，但无法有效捕捉相关性结构。因此需要一种既能扩展又能准确建模的方法。

Method: 提出了STACI框架，结合变分贝叶斯神经网络近似非平稳时空GP，并提出了一种新的空间-时间共形推断算法，利用GPU训练实现高效计算。

Result: STACI在准确近似时空过程方面优于现有GP和深度学习方法，并能轻松扩展到数百万观测值的数据集，提供统计有效的预测区间。

Conclusion: STACI框架显著提升了时空数据建模的可扩展性和准确性，同时保持了不确定性量化的统计有效性。

Abstract: Fitting Gaussian Processes (GPs) provides interpretable aleatoric uncertainty
quantification for estimation of spatio-temporal fields. Spatio-temporal deep
learning models, while scalable, typically assume a simplistic independent
covariance matrix for the response, failing to capture the underlying
correlation structure. However, spatio-temporal GPs suffer from issues of
scalability and various forms of approximation bias resulting from restrictive
assumptions of the covariance kernel function. We propose STACI, a novel
framework consisting of a variational Bayesian neural network approximation of
non-stationary spatio-temporal GP along with a novel spatio-temporal conformal
inference algorithm. STACI is highly scalable, taking advantage of GPU training
capabilities for neural network models, and provides statistically valid
prediction intervals for uncertainty quantification. STACI outperforms
competing GPs and deep methods in accurately approximating spatio-temporal
processes and we show it easily scales to datasets with millions of
observations.

</details>


### [285] [Nearly Dimension-Independent Convergence of Mean-Field Black-Box Variational Inference](https://arxiv.org/abs/2505.21721)
*Kyurae Kim,Yi-An Ma,Trevor Campbell,Jacob R. Gardner*

Key words: 变分推断, 均值-尺度族, 重参数化梯度, 收敛速率, 维度依赖

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: BBVI的重参数化梯度在均值-尺度变分族下以几乎与维度无关的速率收敛，强对数凹凸目标下迭代次数为O(log d)，优于全秩族的O(d)。对于重尾族，维度依赖为O(d^{2/k})，若目标对数密度Hessian为常数则无显式维度依赖性。

Motivation: 研究BBVI在不同变分族下的收敛速率，特别是维度依赖性问题，以优化变分推断的效率。

Method: 采用均值-尺度变分族和重参数化梯度，分析强对数凹凸及重尾目标下的收敛速率，并通过Hessian矩阵的谱边界验证梯度方差界的紧性。

Result: 强对数凹凸目标下收敛速率为O(log d)，重尾族为O(d^{2/k})；目标Hessian为常数时无维度依赖。梯度方差界无法仅通过Hessian谱边界进一步改进。

Conclusion: BBVI在特定条件下可实现高效收敛，收敛速率对维度依赖性显著降低，为大规模推断问题提供了理论支持。

Abstract: We prove that, given a mean-field location-scale variational family,
black-box variational inference (BBVI) with the reparametrization gradient
converges at an almost dimension-independent rate. Specifically, for strongly
log-concave and log-smooth targets, the number of iterations for BBVI with a
sub-Gaussian family to achieve an objective $\epsilon$-close to the global
optimum is $\mathrm{O}(\log d)$, which improves over the $\mathrm{O}(d)$
dependence of full-rank location-scale families. For heavy-tailed families, we
provide a weaker $\mathrm{O}(d^{2/k})$ dimension dependence, where $k$ is the
number of finite moments. Additionally, if the Hessian of the target
log-density is constant, the complexity is free of any explicit dimension
dependence. We also prove that our bound on the gradient variance, which is key
to our result, cannot be improved using only spectral bounds on the Hessian of
the target log-density.

</details>


### [286] [Global Minimizers of $\ell^p$-Regularized Objectives Yield the Sparsest ReLU Neural Networks](https://arxiv.org/abs/2505.21791)
*Julia Nakhleh,Robert D. Nowak*

Key words: 过参数化、神经网络、稀疏性、ReLU网络、$ackslash$ellackslash^p$拟范数

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该研究提出了一种连续可微的训练目标，旨在找到拟合数据的最稀疏ReLU网络，通过最小化权重的$ackslash$ellackslash^p$拟范数（0 < p < 1）来实现，理论上保证全局极小值对应最稀疏解。

Motivation: 研究动机是解决过参数化神经网络中如何选择最优解的问题，特别是如何通过显式正则化策略找到最稀疏的插值网络，以提高效率、泛化性、可解释性和模型压缩效果。

Method: 方法是通过设计一个连续且几乎处处可微的训练目标，该目标最小化权重的$ackslash$ellackslash^p$拟范数（0 < p < 1），将稀疏插值的组合问题转化为平滑优化任务。

Result: 研究结果证明了全局极小值确实对应于最稀疏的单隐藏层ReLU网络解，为理解如何通过连续稀疏诱导目标训练恢复稀疏网络奠定了基础。

Conclusion: 结论是该方法不仅理论上有保障，还为梯度优化方法在稀疏网络训练中的应用提供了新思路。

Abstract: Overparameterized neural networks can interpolate a given dataset in many
different ways, prompting the fundamental question: which among these solutions
should we prefer, and what explicit regularization strategies will provably
yield these solutions? This paper addresses the challenge of finding the
sparsest interpolating ReLU network -- i.e., the network with the fewest
nonzero parameters or neurons -- a goal with wide-ranging implications for
efficiency, generalization, interpretability, theory, and model compression.
Unlike post hoc pruning approaches, we propose a continuous, almost-everywhere
differentiable training objective whose global minima are guaranteed to
correspond to the sparsest single-hidden-layer ReLU networks that fit the data.
This result marks a conceptual advance: it recasts the combinatorial problem of
sparse interpolation as a smooth optimization task, potentially enabling the
use of gradient-based training methods. Our objective is based on minimizing
$\ell^p$ quasinorms of the weights for $0 < p < 1$, a classical
sparsity-promoting strategy in finite-dimensional settings. However, applying
these ideas to neural networks presents new challenges: the function class is
infinite-dimensional, and the weights are learned using a highly nonconvex
objective. We prove that, under our formulation, global minimizers correspond
exactly to sparsest solutions. Our work lays a foundation for understanding
when and how continuous sparsity-inducing objectives can be leveraged to
recover sparse networks through training.

</details>


### [287] [A General-Purpose Theorem for High-Probability Bounds of Stochastic Approximation with Polyak Averaging](https://arxiv.org/abs/2505.21796)
*Sajad Khodadadian,Martin Zubeldia*

Key words: Polyak-Ruppert平均, 随机逼近, 高概率性能, 时间差分学习, Q学习

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 提供Polyak-Ruppert平均技术的高概率性能保证框架，展示在非渐近情况下平均SA迭代误差的紧致边界。

Motivation: 探究Polyak-Ruppert平均在随机逼近算法中的高概率性能，因为现有研究在一般设置中对此性能的保证仍不足。

Method: 提出了一个通用框架，基于未平均迭代的个体边界，推导出平均迭代的紧致边界，并通过示例验证结果的紧致性。

Result: 框架成功构建了紧致的非渐近浓度边界，适用于随机逼近算法如时间差分学习与Q学习的平均版本。

Conclusion: 该框架填补了高概率性能分析领域的空白，并为相关算法提供了理论保证。

Abstract: Polyak-Ruppert averaging is a widely used technique to achieve the optimal
asymptotic variance of stochastic approximation (SA) algorithms, yet its
high-probability performance guarantees remain underexplored in general
settings. In this paper, we present a general framework for establishing
non-asymptotic concentration bounds for the error of averaged SA iterates. Our
approach assumes access to individual concentration bounds for the unaveraged
iterates and yields a sharp bound on the averaged iterates. We also construct
an example, showing the tightness of our result up to constant multiplicative
factors. As direct applications, we derive tight concentration bounds for
contractive SA algorithms and for algorithms such as temporal difference
learning and Q-learning with averaging, obtaining new bounds in settings where
traditional analysis is challenging.

</details>


### [288] [Spectral clustering for dependent community Hawkes process models of temporal networks](https://arxiv.org/abs/2505.21845)
*Lingfei Zhao,Hadeel Soliman,Kevin S. Xu,Subhadeep Paul*

Key words: 时间网络、霍克斯过程、社区结构、谱聚类、广义矩估计

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了依赖社区霍克斯（DCH）模型，结合随机块模型和霍克斯过程，用于建模时间网络的社区结构和节点对之间的依赖性，并提供了谱聚类在事件计数矩阵上的错误率上限及参数估计方法。

Motivation: 时间网络中常存在社区结构和节点对之间的强依赖性，现有方法难以同时建模这两种特性。论文旨在填补这一空白。

Method: 提出DCH模型，结合随机块模型和霍克斯过程；采用谱聚类分析事件计数矩阵，并推导错误率上限；提出基于广义矩估计（GMM）的可扩展参数估计方法。

Result: 推导了谱聚类的非渐近错误率上限，证明了GMM参数估计的渐近一致性。

Conclusion: DCH模型能有效建模时间网络的社区结构和依赖性，且GMM估计方法具有可扩展性和理论保证。

Abstract: Temporal networks observed continuously over time through timestamped
relational events data are commonly encountered in application settings
including online social media communications, financial transactions, and
international relations. Temporal networks often exhibit community structure
and strong dependence patterns among node pairs. This dependence can be modeled
through mutual excitations, where an interaction event from a sender to a
receiver node increases the possibility of future events among other node
pairs.
  We provide statistical results for a class of models that we call dependent
community Hawkes (DCH) models, which combine the stochastic block model with
mutually exciting Hawkes processes for modeling both community structure and
dependence among node pairs, respectively. We derive a non-asymptotic upper
bound on the misclustering error of spectral clustering on the event count
matrix as a function of the number of nodes and communities, time duration, and
the amount of dependence in the model. Our result leverages recent results on
bounding an appropriate distance between a multivariate Hawkes process count
vector and a Gaussian vector, along with results from random matrix theory. We
also propose a DCH model that incorporates only self and reciprocal excitation
along with highly scalable parameter estimation using a Generalized Method of
Moments (GMM) estimator that we demonstrate to be consistent for growing
network size and time duration.

</details>


### [289] [Almost Linear Convergence under Minimal Score Assumptions: Quantized Transition Diffusion](https://arxiv.org/abs/2505.21892)
*Xunpeng Huang,Yingyu Lin,Nikki Lijing Kuang,Hanze Dong,Difan Zou,Yian Ma,Tong Zhang*

Key words: Quantized Transition Diffusion, diffusion models, Markov process, unbiased generation, efficiency

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: QTD通过量化和二进制编码将连续数据转化为离散表示，利用基于Hamming距离的CTMC前向过程支持长距离数据转移，并采用截断均匀化技术反向采样，显著提升了扩散模型的效率和理论基础。

Motivation: 解决连续扩散模型中前向Markov过程的局部邻接结构限制和反向去偏过程的时间非均匀性偏差问题。

Method: 量化数据并二进制编码离散化，设计基于Hamming距离的CTMC前向过程，使用截断均匀化技术反向采样。

Result: QTD在理论和实验上均表现优异，能够在$O(d\\ln^2(d/\\epsilon))$次评分评估内逼近目标分布。

Conclusion: QTD不仅提升了推断效率，还通过统一离散和连续扩散范式推动了理论基础。

Abstract: Continuous diffusion models have demonstrated remarkable performance in data
generation across various domains, yet their efficiency remains constrained by
two critical limitations: (1) the local adjacency structure of the forward
Markov process, which restricts long-range transitions in the data space, and
(2) inherent biases introduced during the simulation of time-inhomogeneous
reverse denoising processes. To address these challenges, we propose Quantized
Transition Diffusion (QTD), a novel approach that integrates data quantization
with discrete diffusion dynamics. Our method first transforms the continuous
data distribution $p_*$ into a discrete one $q_*$ via histogram approximation
and binary encoding, enabling efficient representation in a structured discrete
latent space. We then design a continuous-time Markov chain (CTMC) with Hamming
distance-based transitions as the forward process, which inherently supports
long-range movements in the original data space. For reverse-time sampling, we
introduce a \textit{truncated uniformization} technique to simulate the reverse
CTMC, which can provably provide unbiased generation from $q_*$ under minimal
score assumptions. Through a novel KL dynamic analysis of the reverse CTMC, we
prove that QTD can generate samples with $O(d\ln^2(d/\epsilon))$ score
evaluations in expectation to approximate the $d$--dimensional target
distribution $p_*$ within an $\epsilon$ error tolerance. Our method not only
establishes state-of-the-art inference efficiency but also advances the
theoretical foundations of diffusion-based generative modeling by unifying
discrete and continuous diffusion paradigms.

</details>


### [290] [Higher-Order Group Synchronization](https://arxiv.org/abs/2505.21932)
*Adriana L. Duncan,Joe Kileel*

Key words: 群同步, 高阶测量, 超图, 消息传递, 冷冻电镜

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了一种新颖的高阶群同步问题，通过超图处理高阶局部测量以获取全局估计，并在理论和实验中验证了其优越性和鲁棒性。

Motivation: 研究高阶群同步问题的动机源于计算机视觉、图像处理等领域的应用需求，旨在从高阶局部测量中获取更可靠的全局估计。

Method: 论文首先定义了高阶群同步问题并探讨其数学基础，随后提出了一种基于消息传递算法的计算框架，可直接处理高阶测量。

Result: 实验表明，该方法在旋转和角度同步任务中优于传统的成对同步方法，且对异常值更具鲁棒性，同时在模拟的冷冻电镜数据上表现与标准方法相当。

Conclusion: 高阶群同步方法在理论上有数学保证，实验上表现出优越性和鲁棒性，为相关领域提供了新的解决方案。

Abstract: Group synchronization is the problem of determining reliable global estimates
from noisy local measurements on networks. The typical task for group
synchronization is to assign elements of a group to the nodes of a graph in a
way that respects group elements given on the edges which encode information
about local pairwise relationships between the nodes. In this paper, we
introduce a novel higher-order group synchronization problem which operates on
a hypergraph and seeks to synchronize higher-order local measurements on the
hyperedges to obtain global estimates on the nodes. Higher-order group
synchronization is motivated by applications to computer vision and image
processing, among other computational problems. First, we define the problem of
higher-order group synchronization and discuss its mathematical foundations.
Specifically, we give necessary and sufficient synchronizability conditions
which establish the importance of cycle consistency in higher-order group
synchronization. Then, we propose the first computational framework for general
higher-order group synchronization; it acts globally and directly on
higher-order measurements using a message passing algorithm. We discuss
theoretical guarantees for our framework, including convergence analyses under
outliers and noise. Finally, we show potential advantages of our method through
numerical experiments. In particular, we show that in certain cases our
higher-order method applied to rotational and angular synchronization
outperforms standard pairwise synchronization methods and is more robust to
outliers. We also show that our method has comparable performance on simulated
cryo-electron microscopy (cryo-EM) data compared to a standard cryo-EM
reconstruction package.

</details>


### [291] [Learning Curves of Stochastic Gradient Descent in Kernel Regression](https://arxiv.org/abs/2505.22048)
*Haihan Zhang,Weicheng Lin,Yuanshi Liu,Cong Fang*

Key words: 核回归，SGD，岭回归，误设模型，超额风险

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文研究了在线一阶算法（如SGD）在核回归中的性能表现，并与离线方法（如岭回归和零岭回归）进行比较。研究发现，通过指数衰减步长调度，SGD在多尺度样本情况下均实现极小极大最优速率，克服了饱和现象。

Motivation: 研究在线算法（如SGD）在核回归中的性能，尤其是在模型误设情况下的表现，并与传统离线方法对比。

Method: 使用单次随机梯度下降（SGD）分析内积核在球面上的性能，并研究不同样本规模下的超额风险。

Result: SGD在多尺度样本情况下均实现最优性能，除高度误设模型的后期学习阶段。

Conclusion: SGD通过指数衰减步长调度克服了饱和现象，在多尺度样本规模下表现优异。

Abstract: This paper considers a canonical problem in kernel regression: how good are
the model performances when it is trained by the popular online first-order
algorithms, compared to the offline ones, such as ridge and ridgeless
regression? In this paper, we analyze the foundational single-pass Stochastic
Gradient Descent (SGD) in kernel regression under source condition where the
optimal predictor can even not belong to the RKHS, i.e. the model is
misspecified. Specifically, we focus on the inner product kernel over the
sphere and characterize the exact orders of the excess risk curves under
different scales of sample sizes $n$ concerning the input dimension $d$.
Surprisingly, we show that SGD achieves min-max optimal rates up to constants
among all the scales, without suffering the saturation, a prevalent phenomenon
observed in (ridge) regression, except when the model is highly misspecified
and the learning is in a final stage where $n\gg d^{\gamma}$ with any constant
$\gamma >0$. The main reason for SGD to overcome the curse of saturation is the
exponentially decaying step size schedule, a common practice in deep neural
network training. As a byproduct, we provide the \emph{first} provable
advantage of the scheme over the iterative averaging method in the common
setting.

</details>


### [292] [Individualised Counterfactual Examples Using Conformal Prediction Intervals](https://arxiv.org/abs/2505.22326)
*James M. Adams,Gesine Reinert,Lukasz Szpruch,Carsten Maple,Andrew Elliott*

Key words: 反事实解释, 保形预测, 黑盒模型, 可解释性, 机器学习

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了一种基于个体知识的个性化反事实解释方法，通过建模个体的知识和利用保形预测区间来量化信息增益，从而选择最有用的反事实解释。实验在合成和真实数据集上验证了方法的实用性。

Motivation: 黑盒模型的决策通常难以理解，反事实解释能够帮助个体理解决策的原因。然而，高维特征空间中存在多个可能的反事实解释，需要额外的标准来选择最有用的解释。论文的目的是通过考虑个体对分类器的知识，找到信息量最大的反事实解释。

Method: 论文提出了一种个性化保形预测区间反事实（CPICF）方法。首先，模型化个体的知识，通过保形预测区间的宽度评估预测的不确定性。在预测区间较宽的特征空间区域，个体的决策信心较低，反事实解释可能更有信息量。实验包括合成数据集的完全可视化和真实世界数据集的性能测试。

Result: 在合成数据集上，论文展示了CPICF如何提高个体的知识局部理解。在真实世界数据集上，通过数据增强测试，证明了CPICF的实用性和性能提升。

Conclusion: 论文提出的CPICF方法能够根据个体的知识水平提供信息量最大的反事实解释，显著提高了黑盒模型决策的可解释性和个体理解的效率。

Abstract: Counterfactual explanations for black-box models aim to pr ovide insight into
an algorithmic decision to its recipient. For a binary classification problem
an individual counterfactual details which features might be changed for the
model to infer the opposite class. High-dimensional feature spaces that are
typical of machine learning classification models admit many possible
counterfactual examples to a decision, and so it is important to identify
additional criteria to select the most useful counterfactuals. In this paper,
we explore the idea that the counterfactuals should be maximally informative
when considering the knowledge of a specific individual about the underlying
classifier. To quantify this information gain we explicitly model the knowledge
of the individual, and assess the uncertainty of predictions which the
individual makes by the width of a conformal prediction interval. Regions of
feature space where the prediction interval is wide correspond to areas where
the confidence in decision making is low, and an additional counterfactual
example might be more informative to an individual. To explore and evaluate our
individualised conformal prediction interval counterfactuals (CPICFs), first we
present a synthetic data set on a hypercube which allows us to fully visualise
the decision boundary, conformal intervals via three different methods, and
resultant CPICFs. Second, in this synthetic data set we explore the impact of a
single CPICF on the knowledge of an individual locally around the original
query. Finally, in both our synthetic data set and a complex real world dataset
with a combination of continuous and discrete variables, we measure the utility
of these counterfactuals via data augmentation, testing the performance on a
held out set.

</details>


### [293] [Credal Prediction based on Relative Likelihood](https://arxiv.org/abs/2505.22332)
*Timo Löhr,Paul Hofman,Felix Mohr,Eyke Hüllermeier*

Key words: credal预测,相对似然,认知不确定性,集成学习,概率分布集

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了一种基于相对似然的统计方法来预测概率分布集（称为credal集），以平衡正确性和精确性，并通过改进的集成学习技术近似这些集。实验表明，该方法在不确定性表示和预测性能上优于现有方法。

Motivation: 为更有效地表示学习者的认知不确定性，论文旨在提出一种理论可靠的方法来预测概率分布集（credal集），并通过统计相对似然来控制预测的精确性与正确性之间的权衡。

Method: 论文采用基于统计相对似然的方法，通过设定阈值筛选出可能的模型集合，进而生成credal集。采用改进的集成学习技术来近似这些集合。

Result: 实验证明，该方法在基准数据集上表现优异，不仅能有效表示不确定性，还不影响预测性能，并优于现有credal预测方法。

Conclusion: 该论文提出的方法为credal预测提供了一种理论可靠且实用的解决方案，尤其适合需要权衡不确定性表示和预测精确性的场景。

Abstract: Predictions in the form of sets of probability distributions, so-called
credal sets, provide a suitable means to represent a learner's epistemic
uncertainty. In this paper, we propose a theoretically grounded approach to
credal prediction based on the statistical notion of relative likelihood: The
target of prediction is the set of all (conditional) probability distributions
produced by the collection of plausible models, namely those models whose
relative likelihood exceeds a specified threshold. This threshold has an
intuitive interpretation and allows for controlling the trade-off between
correctness and precision of credal predictions. We tackle the problem of
approximating credal sets defined in this way by means of suitably modified
ensemble learning techniques. To validate our approach, we illustrate its
effectiveness by experiments on benchmark datasets demonstrating superior
uncertainty representation without compromising predictive performance. We also
compare our method against several state-of-the-art baselines in credal
prediction.

</details>


### [294] [Computing Optimal Transport Maps and Wasserstein Barycenters Using Conditional Normalizing Flows](https://arxiv.org/abs/2505.22364)
*Gabriele Visentin,Patrick Cheridito*

Key words: 最优传输, Wasserstein重心, 归一化流, 高维空间, 梯度最小化

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文提出了一种利用条件归一化流高效计算高维空间中最优传输映射和Wasserstein重心的新方法，通过基于梯度的最小化直接求解原始问题，显著提升了计算效率。

Motivation: 传统方法依赖对偶公式和复杂的对抗优化，难以高效计算高维空间中的最优传输映射和Wasserstein重心。因此，作者旨在提出一种更直接且高效的方法解决这一问题。

Method: 采用条件归一化流近似输入分布，将其映射到共享的潜在空间，形成可逆的推演变换。通过基于梯度的最小化传输成本直接求解原始问题，并扩展到通过条件方差最小化计算Wasserstein重心。

Result: 实验证明，该方法在高维任务中结果准确，优于现有技术，并能计算数百个输入分布的重心，这在之前是计算不可行的。

Conclusion: 条件归一化流的架构为高维最优传输和Wasserstein重心的计算提供了高效且准确的解决方案，弥补了传统方法的不足。

Abstract: We present a novel method for efficiently computing optimal transport maps
and Wasserstein barycenters in high-dimensional spaces. Our approach uses
conditional normalizing flows to approximate the input distributions as
invertible pushforward transformations from a common latent space. This makes
it possible to directly solve the primal problem using gradient-based
minimization of the transport cost, unlike previous methods that rely on dual
formulations and complex adversarial optimization. We show how this approach
can be extended to compute Wasserstein barycenters by solving a conditional
variance minimization problem. A key advantage of our conditional architecture
is that it enables the computation of barycenters for hundreds of input
distributions, which was computationally infeasible with previous methods. Our
numerical experiments illustrate that our approach yields accurate results
across various high-dimensional tasks and compares favorably with previous
state-of-the-art methods.

</details>


### [295] [Hypothesis Testing in Imaging Inverse Problems](https://arxiv.org/abs/2505.22481)
*Yiming Xi,Konstantinos Zygalakis,Marcelo Pereyra*

Key words: 语义假设检验、成像逆问题、自监督计算成像、视觉语言模型、非参数检验

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 提出了一种针对成像逆问题的语义假设检验框架，解决了现代成像方法在假设检验（科学方法核心）中的应用难题，通过结合自监督计算成像、视觉语言模型和非参数假设检验，实现了在高统计功效下控制第一类错误。

Motivation: 现代成像方法在支持假设检验方面存在挑战，尤其是在科学实验的严格解释和决策过程中。主要原因包括单次观测难以同时重建图像、提出假设并量化统计显著性、假设多为语义性质而非像素值定量描述、以及难以控制检验错误概率。

Method: 结合自监督计算成像、视觉语言模型和非参数假设检验（e值），提出了一种语义假设检验框架。

Result: 在图像表型相关的数值实验中，该方法表现出色，保持了高统计功效并稳健地控制了第一类错误。

Conclusion: 所提出的框架有效解决了成像逆问题中的语义假设检验难题，为科学实验的解释和决策提供了更可靠的统计支持。

Abstract: This paper proposes a framework for semantic hypothesis testing tailored to
imaging inverse problems. Modern imaging methods struggle to support hypothesis
testing, a core component of the scientific method that is essential for the
rigorous interpretation of experiments and robust interfacing with
decision-making processes. There are three main reasons why image-based
hypothesis testing is challenging. First, the difficulty of using a single
observation to simultaneously reconstruct an image, formulate hypotheses, and
quantify their statistical significance. Second, the hypotheses encountered in
imaging are mostly of semantic nature, rather than quantitative statements
about pixel values. Third, it is challenging to control test error
probabilities because the null and alternative distributions are often unknown.
Our proposed approach addresses these difficulties by leveraging concepts from
self-supervised computational imaging, vision-language models, and
non-parametric hypothesis testing with e-values. We demonstrate our proposed
framework through numerical experiments related to image-based phenotyping,
where we achieve excellent power while robustly controlling Type I errors.

</details>


### [296] [IGNIS: A Neural Network Framework for Robust Parameter Estimation in Archimedean Copulas](https://arxiv.org/abs/2505.22518)
*Agnideep Aich,Ashit Baran Aich,Bruce Wade*

Key words: Archimedean copula, parameter estimation, neural network, dependency modeling, IGNIS

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了一种名为IGNIS Network的新型神经网络框架，用于解决Archimedean copula的参数估计难题，尤其是在复杂依赖结构的A1和A2家族中。相比传统方法，IGNIS通过直接从观测数据学习参数映射，减少了估计误差并增强了稳定性。

Motivation: 传统方法（如矩估计、最大似然估计）在处理Archimedean copula（尤其是A1和A2家族）时存在非单调性或数值不稳定性问题，亟需一种更鲁棒且普适的解决方案。

Method: 提出了IGNIS Network，一种通过模拟数据训练的统一神经网络框架，直接从依赖度量（如Kendall's tau）映射到copula参数，并通过理论指导的后处理强制参数约束。

Result: 实验表明，IGNIS在模拟数据和真实数据集（金融、医疗、环境）上均优于传统方法，显著降低了估计误差。

Conclusion: IGNIS Network展示了神经网络在依赖建模中的潜力，为现代应用提供了一种更准确、稳健的参数估计方法。

Abstract: Parameter estimation for Archimedean copulas remains a challenging problem,
particularly for the recently developed A1 and A2 families that exhibit complex
dependency structures. Traditional methods, such as the Method of Moments
(MoM), Maximum Likelihood Estimation (MLE), and Maximum Pseudo-Likelihood
(MPL), often struggle due to issues of non-monotonic relationship with
dependency measures such as Kendall's tau (as in the case of A1) and numerical
instability. In this paper, we present the IGNIS Network, a novel, unified
neural framework that learns a direct mapping from observable dependency
measures to copula parameters, thereby overcoming the limitations of classical
approaches. Our approach is trained on simulated data spanning five Archimedean
copula families including Clayton, Gumbel, Frank, A1, and A2, ensuring its
general applicability across the entire family. Extensive simulation studies
demonstrate that the IGNIS Network reduces estimation errors compared to MoM,
while inherently enforcing parameter constraints through theory-guided
post-processing. We further validate the practical utility of our method on
diverse real-world datasets, including financial returns (AAPL-MSFT),
healthcare metrics (CDC Diabetes indicators), and environmental measurements
(PM2.5 air quality). Our results underscore the transformative potential of
neural methods for robust and accurate dependence modeling in modern
applications.

</details>


### [297] [Symplectic Generative Networks (SGNs): A Hamiltonian Framework for Invertible Deep Generative Modeling](https://arxiv.org/abs/2505.22527)
*Agnideep Aich,Ashit Aich,Bruce Wade*

Key words: Symplectic Generative Network, Hamiltonian mechanics, exact likelihood, volume-preserving, deep generative model

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 总结生成网络（SGN）是一种利用哈密顿力学的深度生成模型，实现了潜在空间与数据空间的可逆、保体积映射，无需计算雅可比行列式即可精确评估似然。

Motivation: 通过引入哈密顿力学，构建高效且数学上严谨的生成模型，克服现有方法（如变分自编码器和标准化流）的计算复杂性。

Method: 采用哈密顿系统的时间演化模型，赋予潜在空间辛结构，并进行完整的理论分析，包括可逆性证明、复杂度对比、近似误差量化等。

Result: SGN在理论上实现了精确似然评估，并通过数学分析验证了其优越性和稳定性。

Conclusion: SGN为复杂高维数据的生成建模提供了理论基础，未来可进一步探索其实际应用。

Abstract: We introduce the Symplectic Generative Network (SGN), a deep generative model
that leverages Hamiltonian mechanics to construct an invertible,
volume-preserving mapping between a latent space and the data space. By
endowing the latent space with a symplectic structure and modeling data
generation as the time evolution of a Hamiltonian system, SGN achieves exact
likelihood evaluation without incurring the computational overhead of Jacobian
determinant calculations. In this work, we provide a rigorous mathematical
foundation for SGNs through a comprehensive theoretical framework that
includes: (i) complete proofs of invertibility and volume preservation, (ii) a
formal complexity analysis with theoretical comparisons to Variational
Autoencoders and Normalizing Flows, (iii) strengthened universal approximation
results with quantitative error bounds, (iv) an information-theoretic analysis
based on the geometry of statistical manifolds, and (v) an extensive stability
analysis with adaptive integration guarantees. These contributions highlight
the fundamental advantages of SGNs and establish a solid foundation for future
empirical investigations and applications to complex, high-dimensional data.

</details>


### [298] [Can Copulas Be Used for Feature Selection? A Machine Learning Study on Diabetes Risk Prediction](https://arxiv.org/abs/2505.22554)
*Agnideep Aich,Md Monzur Murshed,Amanda Mayeaux,Sameera Hewage*

Key words: 糖尿病预测, 特征选择, copula, 上尾依赖, 机器学习

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 使用新型A2 copula的上尾依赖系数进行特征选择，提升糖尿病风险预测准确性。

Motivation: 传统方法（如互信息过滤和遗传算法）常忽略高风险人群的极端依赖关系，亟需新方法捕捉这类关键特征。

Method: 提出基于A2 copula的上尾依赖系数（λU）的特征选择框架，量化预测变量极端值与糖尿病诊断的共现关系。

Result: 在CDC数据集上选出5个关键特征，性能优于传统方法（准确率86.5%，AUC 0.806），媲美21特征的完整模型。

Conclusion: 首次将copula的上尾依赖理论应用于监督特征选择，为糖尿病预防提供实用工具。

Abstract: Accurate diabetes risk prediction relies on identifying key features from
complex health datasets, but conventional methods like mutual information (MI)
filters and genetic algorithms (GAs) often overlook extreme dependencies
critical for high-risk subpopulations. In this study we introduce a
feature-selection framework using the upper-tail dependence coefficient
({\lambda}U) of the novel A2 copula, which quantifies how often extreme higher
values of a predictor co-occur with diabetes diagnoses (target variable).
Applied to the CDC Diabetes Health Indicators dataset (n=253,680), our method
prioritizes five predictors (self-reported general health, high blood pressure,
body mass index, mobility limitations, and high cholesterol levels) based on
upper tail dependencies. These features match or outperform MI and GA selected
subsets across four classifiers (Random Forest, XGBoost, Logistic Regression,
Gradient Boosting), achieving accuracy up to 86.5% (XGBoost) and AUC up to
0.806 (Gradient Boosting), rivaling the full 21-feature model. Permutation
importance confirms clinical relevance, with BMI and general health driving
accuracy. To our knowledge, this is the first work to apply a copula's
upper-tail dependence for supervised feature selection, bridging extreme-value
theory and machine learning to deliver a practical toolkit for diabetes
prevention.

</details>


### [299] [Principled Out-of-Distribution Generalization via Simplicity](https://arxiv.org/abs/2505.22622)
*Jiawei Ge,Amanda Wang,Shange Tang,Chi Jin*

Key words: 泛化性, 简约度, 扩散模型, OOD, 样本复杂度

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文研究了扩散模型在图像生成中的组合泛化能力，发现简单性与泛化性相关，并提出了基于简约度的理论框架，为学习真实、可泛化的简单模型提供了样本复杂度的保证。

Motivation: 探究现代基础模型在超出训练数据分布（OOD）情况下的泛化能力背后的理论原则，尤其是通过研究扩散模型的组合泛化能力。

Method: 提出了一个基于简约度的理论框架，分析了两种关键场景（恒定差距和消失差距），并研究了正则化最大似然估计器。

Result: 为学习真实、可泛化的简单模型建立了第一个尖锐的样本复杂度保证。

Conclusion: 简单性与泛化性紧密相关，理论框架为理解OOD泛化提供了新的视角。

Abstract: Modern foundation models exhibit remarkable out-of-distribution (OOD)
generalization, solving tasks far beyond the support of their training data.
However, the theoretical principles underpinning this phenomenon remain
elusive. This paper investigates this problem by examining the compositional
generalization abilities of diffusion models in image generation. Our analysis
reveals that while neural network architectures are expressive enough to
represent a wide range of models -- including many with undesirable behavior on
OOD inputs -- the true, generalizable model that aligns with human expectations
typically corresponds to the simplest among those consistent with the training
data.
  Motivated by this observation, we develop a theoretical framework for OOD
generalization via simplicity, quantified using a predefined simplicity metric.
We analyze two key regimes: (1) the constant-gap setting, where the true model
is strictly simpler than all spurious alternatives by a fixed gap, and (2) the
vanishing-gap setting, where the fixed gap is replaced by a smoothness
condition ensuring that models close in simplicity to the true model yield
similar predictions. For both regimes, we study the regularized maximum
likelihood estimator and establish the first sharp sample complexity guarantees
for learning the true, generalizable, simple model.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [300] [VoiceMark: Zero-Shot Voice Cloning-Resistant Watermarking Approach Leveraging Speaker-Specific Latents](https://arxiv.org/abs/2505.21568)
*Haiyun Li,Zhiyong Wu,Xiaofeng Xie,Jingran Xie,Yaoxun Xu,Hanyang Peng*

Key words: 语音克隆、水印、零样本、说话人特征、鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: VoiceMark是一个针对零样本语音克隆（VC）的水印方法，首次利用说话人特定的潜在特征作为水印载体，通过零样本VC过程传递水印，检测准确率超过95%。

Motivation: 现有水印方法虽能追踪传统VC模型，但在零样本VC场景下失效，亟需一种能在未训练模型的情况下仍能有效传递水印的方法。

Method: 提出VoiceMark，利用说话人特定潜在特征作为水印载体，结合VC模拟增强和VAD损失函数，提升水印对扭曲的鲁棒性。

Result: 实验表明，VoiceMark在零样本VC合成后水印检测准确率超95%，显著优于现有方法（约50%）。

Conclusion: VoiceMark为零样本VC场景提供高效水印方案，强化了对未授权克隆的追踪能力。

Abstract: Voice cloning (VC)-resistant watermarking is an emerging technique for
tracing and preventing unauthorized cloning. Existing methods effectively trace
traditional VC models by training them on watermarked audio but fail in
zero-shot VC scenarios, where models synthesize audio from an audio prompt
without training. To address this, we propose VoiceMark, the first zero-shot
VC-resistant watermarking method that leverages speaker-specific latents as the
watermark carrier, allowing the watermark to transfer through the zero-shot VC
process into the synthesized audio. Additionally, we introduce VC-simulated
augmentations and VAD-based loss to enhance robustness against distortions.
Experiments on multiple zero-shot VC models demonstrate that VoiceMark achieves
over 95% accuracy in watermark detection after zero-shot VC synthesis,
significantly outperforming existing methods, which only reach around 50%. See
our code and demos at: https://huggingface.co/spaces/haiyunli/VoiceMark

</details>


### [301] [Music Source Restoration](https://arxiv.org/abs/2505.21827)
*Yongyi Zang,Zheqi Dai,Mark D. Plumbley,Qiuqiang Kong*

Key words: 音乐源修复, RawStems, 信号退化, U-Former, 音乐源分离

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 论文提出音乐源修复（MSR）任务，解决现有音乐源分离（MSS）忽略真实音乐制作中信号退化的问题，并发布RawStems数据集和基线方法U-Former。

Motivation: 当前音乐源分离方法假设混音是简单源信号相加，忽略音乐制作中的信号退化（如均衡、压缩、混响），MSR任务旨在恢复原始信号。

Method: 提出MSR任务，模型将混音视为退化源信号的叠加，并基于RawStems数据集（包含578首未处理源信号）训练U-Former作为基线方法。

Result: 实验证明MSR在RawStems数据集上可行，基线方法U-Former有效。

Conclusion: MSR填补了音乐源分离与真实音乐制作的鸿沟，RawStems数据集和退化模拟流程为未来研究提供基础。

Abstract: We introduce Music Source Restoration (MSR), a novel task addressing the gap
between idealized source separation and real-world music production. Current
Music Source Separation (MSS) approaches assume mixtures are simple sums of
sources, ignoring signal degradations employed during music production like
equalization, compression, and reverb. MSR models mixtures as degraded sums of
individually degraded sources, with the goal of recovering original, undegraded
signals. Due to the lack of data for MSR, we present RawStems, a dataset
annotation of 578 songs with unprocessed source signals organized into 8
primary and 17 secondary instrument groups, totaling 354.13 hours. To the best
of our knowledge, RawStems is the first dataset that contains unprocessed music
stems with hierarchical categories. We consider spectral filtering, dynamic
range compression, harmonic distortion, reverb and lossy codec as possible
degradations, and establish U-Former as a baseline method, demonstrating the
feasibility of MSR on our dataset. We release the RawStems dataset annotations,
degradation simulation pipeline, training code and pre-trained models to be
publicly available.

</details>


### [302] [Visual Cues Support Robust Turn-taking Prediction in Noise](https://arxiv.org/abs/2505.22088)
*Sam O'Connor Russell,Naomi Harte*

Key words: predictive turn-taking models, noise, human-robot interaction, multimodal, accuracy

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 论文研究了在嘈杂环境中预测性轮流模型（PTTM）的性能，发现噪声对模型影响显著，提出多模态PTTM能通过视觉线索提升准确性，但其泛化能力有限且依赖于精确的转录。

Motivation: 探索预测性轮流模型在现实噪声环境中的性能，以改进人机交互的自然性。

Method: 通过对比音频单独和多模态（含视觉线索）PTTM在不同噪声类型和信噪比下的表现，分析模型效果。

Result: 多模态PTTM在10 dB音乐噪声中准确率达72%，优于纯音频模型，但对新型噪声泛化能力有限。

Conclusion: 多模态PTTM在噪声环境中表现更优，但需依赖精确转录，且泛化能力待提升。

Abstract: Accurate predictive turn-taking models (PTTMs) are essential for naturalistic
human-robot interaction. However, little is known about their performance in
noise. This study therefore explores PTTM performance in types of noise likely
to be encountered once deployed. Our analyses reveal PTTMs are highly sensitive
to noise. Hold/shift accuracy drops from 84% in clean speech to just 52% in 10
dB music noise. Training with noisy data enables a multimodal PTTM, which
includes visual features to better exploit visual cues, with 72% accuracy in 10
dB music noise. The multimodal PTTM outperforms the audio-only PTTM across all
noise types and SNRs, highlighting its ability to exploit visual cues; however,
this does not always generalise to new types of noise. Analysis also reveals
that successful training relies on accurate transcription, limiting the use of
ASR-derived transcriptions to clean conditions. We make code publicly available
for future research.

</details>


### [303] [Advancing Hearing Assessment: An ASR-Based Frequency-Specific Speech Test for Diagnosing Presbycusis](https://arxiv.org/abs/2505.22231)
*Stefan Bleeck*

Key words: 自动语音识别, 频率特异性, 听力损失, 老年性耳聋, 音素混淆

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 论文开发了一种基于自动语音识别（ASR）的频率特异性语音测试方法，用于诊断听力损失中的高频语音感知问题。通过模拟听力损失，分析音素混淆模式，测试表现出诊断价值，未来将验证于人类受试者。

Motivation: 传统听力测试难以全面评估听力损失对语音理解的功能影响，特别是在老年性耳聋等情况下。

Method: 利用ASR模拟听力损失，分析音素混淆模式，开发测试电池。

Result: 模拟听力损失导致特定音素混淆和删除，测试能区分正常听力与听力受损者。

Conclusion: ASR方法为开发客观、精准的听力评估工具提供了新方向。

Abstract: Traditional audiometry often fails to fully characterize the functional
impact of hearing loss on speech understanding, particularly supra-threshold
deficits and frequency-specific perception challenges in conditions like
presbycusis. This paper presents the development and simulated evaluation of a
novel Automatic Speech Recognition (ASR)-based frequency-specific speech test
designed to provide granular diagnostic insights. Our approach leverages ASR to
simulate the perceptual effects of moderate sloping hearing loss by processing
speech stimuli under controlled acoustic degradation and subsequently analyzing
phoneme-level confusion patterns. Key findings indicate that simulated hearing
loss introduces specific phoneme confusions, predominantly affecting
high-frequency consonants (e.g., alveolar/palatal to labiodental substitutions)
and leading to significant phoneme deletions, consistent with the acoustic cues
degraded in presbycusis. A test battery curated from these ASR-derived
confusions demonstrated diagnostic value, effectively differentiating between
simulated normal-hearing and hearing-impaired listeners in a comprehensive
simulation. This ASR-driven methodology offers a promising avenue for
developing objective, granular, and frequency-specific hearing assessment tools
that complement traditional audiometry. Future work will focus on validating
these findings with human participants and exploring the integration of
advanced AI models for enhanced diagnostic precision.

</details>


### [304] [Effective Context in Neural Speech Models](https://arxiv.org/abs/2505.22487)
*Yen Meng,Sharon Goldwater,Hao Tang*

Key words: 语音Transformer，有效上下文，监督学习，自监督学习，流式处理

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 论文提出了两种测量语音Transformer模型实际使用上下文长度的方法，并分析了任务类型对上下文需求的影响，发现自监督模型的有效上下文较短且可用于流式处理。

Motivation: 现有的神经语音模型虽然支持更长的上下文，但缺乏对这些模型实际使用的上下文长度（有效上下文）的量化研究。

Method: 提出了两种测量有效上下文的方法，分别应用于监督和自监督的语音Transformer模型，分析任务类型和模型层次对上下文需求的影响。

Result: 监督模型的有效上下文与任务类型相关（如基频跟踪、音素分类和词汇分类对上下文需求递增），而自监督模型的有效上下文主要集中在早期层且较短，无需修改架构即可用于流式处理。

Conclusion: 语音模型的有效上下文长短与任务相关，自监督模型的流式处理潜力未被充分利用。

Abstract: Modern neural speech models benefit from having longer context, and many
approaches have been proposed to increase the maximum context a model can use.
However, few have attempted to measure how much context these models actually
use, i.e., the effective context. Here, we propose two approaches to measuring
the effective context, and use them to analyze different speech Transformers.
For supervised models, we find that the effective context correlates well with
the nature of the task, with fundamental frequency tracking, phone
classification, and word classification requiring increasing amounts of
effective context. For self-supervised models, we find that effective context
increases mainly in the early layers, and remains relatively short -- similar
to the supervised phone model. Given that these models do not use a long
context during prediction, we show that HuBERT can be run in streaming mode
without modification to the architecture and without further fine-tuning.

</details>


### [305] [Improving Respiratory Sound Classification with Architecture-Agnostic Knowledge Distillation from Ensembles](https://arxiv.org/abs/2505.22027)
*Miika Toikkanen,June-Woo Kim*

Key words: 呼吸音分类, 知识蒸馏, 软标签, 教师-学生模型, ICHBI数据集

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 该研究探索了使用软标签训练方法，通过知识蒸馏将多教师模型的知识传递给学生模型，显著提高了呼吸音分类性能，并在ICHBI数据集上实现了64.39的新最高分。

Motivation: 呼吸音数据集规模有限且质量不一，导致高精度分类困难。现有方法如集成模型虽有效，但推理成本高。因此研究软标签训练作为轻量高效替代方案。

Method: 采用架构无关的软标签知识蒸馏方法，将多教师模型（包括与目标学生结构相同的单模型）的知识提取至学生模型，探索不同变体的效果。

Result: 在ICHBI数据集上达64.39分（超SOTA 0.85分），且平均性能提升1.16分；即使单教师模型也能显著提升学生模型性能，最优效果仅需少量教师。

Conclusion: 软标签知识蒸馏可有效提升呼吸音分类性能，且不受模型规模或架构限制。

Abstract: Respiratory sound datasets are limited in size and quality, making high
performance difficult to achieve. Ensemble models help but inevitably increase
compute cost at inference time. Soft label training distills knowledge
efficiently with extra cost only at training. In this study, we explore soft
labels for respiratory sound classification as an architecture-agnostic
approach to distill an ensemble of teacher models into a student model. We
examine different variations of our approach and find that even a single
teacher, identical to the student, considerably improves performance beyond its
own capability, with optimal gains achieved using only a few teachers. We
achieve the new state-of-the-art Score of 64.39 on ICHBI, surpassing the
previous best by 0.85 and improving average Scores across architectures by more
than 1.16. Our results highlight the effectiveness of knowledge distillation
with soft labels for respiratory sound classification, regardless of size or
architecture.

</details>


### [306] [AudioTurbo: Fast Text-to-Audio Generation with Rectified Diffusion](https://arxiv.org/abs/2505.22106)
*Junqi Zhao,Jinzheng Zhao,Haohe Liu,Yun Chen,Lu Han,Xubo Liu,Mark Plumbley,Wenwu Wang*

Key words: 扩散模型，文本到音频生成，修正流，ODE路径，推理加速

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 该研究提出AudioTurbo，通过整合预训练的扩散模型与修正扩散方法，提高文本到音频生成的效率。在仅10步采样的条件下，该模型性能优于现有方法，并将推理步骤减少至3步。

Motivation: 扩散模型在音频生成质量和多样性方面表现优异，但推理速度较慢。修正流（rectified flow）虽能加速推理，但需要从头训练且在小步数下性能较差。本研究旨在结合预训练模型的优势与修正扩散方法，提升文本到音频生成的效率。

Method: 提出AudioTurbo，通过从预训练TTA模型生成的确定性噪声样本对中学习一阶ODE路径，实现高效生成。

Result: 在AudioCaps数据集上的实验表明，仅用10步采样，AudioTurbo即超越现有方法，并将推理步骤进一步压缩至3步。

Conclusion: AudioTurbo有效结合预训练模型与修正扩散方法，显著提升生成效率，同时在小步数下保持高性能。

Abstract: Diffusion models have significantly improved the quality and diversity of
audio generation but are hindered by slow inference speed. Rectified flow
enhances inference speed by learning straight-line ordinary differential
equation (ODE) paths. However, this approach requires training a flow-matching
model from scratch and tends to perform suboptimally, or even poorly, at low
step counts. To address the limitations of rectified flow while leveraging the
advantages of advanced pre-trained diffusion models, this study integrates
pre-trained models with the rectified diffusion method to improve the
efficiency of text-to-audio (TTA) generation. Specifically, we propose
AudioTurbo, which learns first-order ODE paths from deterministic noise sample
pairs generated by a pre-trained TTA model. Experiments on the AudioCaps
dataset demonstrate that our model, with only 10 sampling steps, outperforms
prior models and reduces inference to 3 steps compared to a flow-matching-based
acceleration model.

</details>


### [307] [Voice Quality Dimensions as Interpretable Primitives for Speaking Style for Atypical Speech and Affect](https://arxiv.org/abs/2505.21809)
*Jaya Narain,Vasudha Kowtha,Colin Lea,Lauren Tooley,Dianna Yee,Vikramjit Mitra,Zifang Huang,Miquel Espi Marques,Jon Huang,Carlos Avendano,Shirley Ren*

Key words: 语音质量, 非典型语音, 语音调制, 零样本学习, 预训练模型

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 该论文开发并评估了七个语音质量维度的模型，使用预训练模型的嵌入特征进行训练。模型在多个数据集上表现出色，包括未见过的语言和任务，展示了语音质量维度在语音风格相关任务中的实用性。

Motivation: 研究旨在通过开发语音质量维度的模型，提升对非典型语音和语音调制的描述能力，从而更好地理解和分析语音风格相关的任务。

Method: 利用预训练模型的嵌入特征，在包含11,184个样本的公共数据集上训练了七个语音维度的探针，并在多个未见过的数据集上进行了零样本性能验证。

Result: 模型在公共数据集和未见过的数据集上均表现出色，具有强泛化能力和零样本性能，同时结果的可解释性高。

Conclusion: 语音质量维度在语音风格相关任务中具有实用性，模型表现出强大的性能和泛化能力。

Abstract: Perceptual voice quality dimensions describe key characteristics of atypical
speech and other speech modulations. Here we develop and evaluate voice quality
models for seven voice and speech dimensions (intelligibility, imprecise
consonants, harsh voice, naturalness, monoloudness, monopitch, and
breathiness). Probes were trained on the public Speech Accessibility (SAP)
project dataset with 11,184 samples from 434 speakers, using embeddings from
frozen pre-trained models as features. We found that our probes had both strong
performance and strong generalization across speech elicitation categories in
the SAP dataset. We further validated zero-shot performance on additional
datasets, encompassing unseen languages and tasks: Italian atypical speech,
English atypical speech, and affective speech. The strong zero-shot performance
and the interpretability of results across an array of evaluations suggests the
utility of using voice quality dimensions in speaking style-related tasks.

</details>


### [308] [Effective and Efficient One-pass Compression of Speech Foundation Models Using Sparsity-aware Self-pinching Gates](https://arxiv.org/abs/2505.22608)
*Haoning Xu,Zhaoqing Li,Youjun Chen,Huimeng Wang,Guinan Li,Mengzhe Geng,Chengxi Deng,Xunying Liu*

Key words: 语音基础模型, 模型压缩, 自紧门, 剪枝, 词错误率

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 该论文提出了一种新的语音基础模型压缩方法，通过单阶段集成模型剪枝和参数更新，显著减少了参数量（wav2vec2.0-base降低65%，HuBERT-large降低60%），且未显著增加词错误率（WER）。

Motivation: 语音基础模型压缩是提高其部署效率的关键，尤其需要兼顾参数量减少与性能保持。现有方法多阶段处理效率低，需改进。

Method: 提出层级别的自紧门机制（含单一可学习阈值），与未压缩模型联合训练，实现细粒度神经元剪枝。

Result: 在LibriSpeech-100hr上，wav2vec2.0-base和HuBERT-large参数量分别减少65%和60%，WER无显著上升（test-clean数据集WER仅7.05%），压缩时间减少25%以上。

Conclusion: 该方法在高效压缩的同时保持模型性能，优于现有方法，适用于实际部署场景。

Abstract: This paper presents a novel approach for speech foundation models compression
that tightly integrates model pruning and parameter update into a single stage.
Highly compact layer-level tied self-pinching gates each containing only a
single learnable threshold are jointly trained with uncompressed models and
used in fine-grained neuron level pruning. Experiments conducted on the
LibriSpeech-100hr corpus suggest that our approach reduces the number of
parameters of wav2vec2.0-base and HuBERT-large models by 65% and 60%
respectively, while incurring no statistically significant word error rate
(WER) increase on the test-clean dataset. Compared to previously published
methods on the same task, our approach not only achieves the lowest WER of
7.05% on the test-clean dataset under a comparable model compression ratio of
4.26x, but also operates with at least 25% less model compression time.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [309] [UI-Evol: Automatic Knowledge Evolving for Computer Use Agents](https://arxiv.org/abs/2505.21964)
*Ziyun Zhang,Xinyi Liu,Xiaoyi Zhang,Jun Wang,Gang Chen,Yan Lu*

Key words: 知识执行, GUI代理, 行为优化, 任务性能, 可靠性

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 论文提出UI-Evol模块，通过两个阶段（回溯与批判）解决知识执行中的低效问题，显著提升了任务完成率和代理可靠性。

Motivation: 研究发现，即便90%的知识正确，执行成功率仅为41%，突显知识执行间的严重差距，亟需解决方案。

Method: 设计UI-Evol模块，分回溯阶段（提取真实动作序列）和批判阶段（对照外部参考优化知识）。

Result: 在OSWorld基准测试中，UI-Evol显著提升任务性能，降低行为标准差，增强代理可靠性。

Conclusion: UI-Evol有效弥合知识执行差距，提升计算机代理的实际应用效果。

Abstract: External knowledge has played a crucial role in the recent development of
computer use agents. We identify a critical knowledge-execution gap: retrieved
knowledge often fails to translate into effective real-world task execution.
Our analysis shows even 90\% correct knowledge yields only 41\% execution
success rate. To bridge this gap, we propose UI-Evol, a plug-and-play module
for autonomous GUI knowledge evolution. UI-Evol consists of two stages: a
Retrace Stage that extracts faithful objective action sequences from actual
agent-environment interactions, and a Critique Stage that refines existing
knowledge by comparing these sequences against external references. We conduct
comprehensive experiments on the OSWorld benchmark with the state-of-the-art
Agent S2. Our results demonstrate that UI-Evol not only significantly boosts
task performance but also addresses a previously overlooked issue of high
behavioral standard deviation in computer use agents, leading to superior
performance on computer use tasks and substantially improved agent reliability.

</details>


### [310] [MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing](https://arxiv.org/abs/2505.21966)
*Aditya Gunturu,Ben Pearman,Keiichi Ihara,Morteza Faraji,Bryan Wang,Rubaiat Habib Kazi,Ryo Suzuki*

Key words: LLM, 地图动画, 自然语言处理, 交互式编辑

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: MapStory是一个基于LLM的地图动画创作工具，通过自然语言脚本自动生成可编辑的动画序列，简化了地图故事的制作流程。

Motivation: 旨在降低地图动画制作的难度，通过自动化和交互式编辑工具提升创作效率和探索性。

Method: 采用基于LLM的智能代理架构，分解脚本为动画构建块，结合地理空间查询和交互式时间线编辑。

Result: 用户研究（N=17）表明工具易用、迭代快速，并能激发创意探索。

Conclusion: MapStory有效降低了地图动画制作的技术门槛，提升了创作效率与灵活性。

Abstract: We introduce MapStory, an LLM-powered animation authoring tool that generates
editable map animation sequences directly from natural language text. Given a
user-written script, MapStory leverages an agentic architecture to
automatically produce a scene breakdown, which decomposes the script into key
animation building blocks such as camera movements, visual highlights, and
animated elements. Our system includes a researcher component that accurately
queries geospatial information by leveraging an LLM with web search, enabling
the automatic extraction of relevant regions, paths, and coordinates while
allowing users to edit and query for changes or additional information to
refine the results. Additionally, users can fine-tune parameters of these
blocks through an interactive timeline editor. We detail the system's design
and architecture, informed by formative interviews with professional animators
and an analysis of 200 existing map animation videos. Our evaluation, which
includes expert interviews (N=5) and a usability study (N=12), demonstrates
that MapStory enables users to create map animations with ease, facilitates
faster iteration, encourages creative exploration, and lowers barriers to
creating map-centric stories.

</details>


### [311] [Voice CMS: updating the knowledge base of a digital assistant through conversation](https://arxiv.org/abs/2505.22303)
*Grzegorz Wolny,Michał Szczerbak*

Key words: 多智能体LLM, 语音用户界面, 知识管理, 可用性测试, 混合界面

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 本文提出了一种基于多智能体LLM架构和语音用户界面（VUI）的解决方案，用于更新数字助理的知识库。研究比较了VUI与传统图形内容管理系统（CMS）的可用性，发现VUI在简单任务中更受用户青睐，且内容质量与图形界面相当。结果表明，结合两者优势的混合界面可能是解决认知负荷等问题的有效方案。

Motivation: 探索语音用户界面（VUI）在知识管理中的潜力，特别是在复杂任务中的表现，以及用户对不同界面类型的偏好。

Method: 采用多智能体LLM架构和VUI设计，通过与传统图形CMS的对比实验评估可用性。

Result: VUI在简单任务中更受欢迎，内容质量与图形界面相当；混合界面可能解决认知负荷问题。

Conclusion: 对话式界面在特定商业场景中具有潜力，结合VUI和图形界面的混合方案可能更优。

Abstract: In this study, we propose a solution based on a multi-agent LLM architecture
and a voice user interface (VUI) designed to update the knowledge base of a
digital assistant. Its usability is evaluated in comparison to a more
traditional graphical content management system (CMS), with a focus on
understanding the relationship between user preferences and the complexity of
the information being provided. The findings demonstrate that, while the
overall usability of the VUI is rated lower than the graphical interface, it is
already preferred by users for less complex tasks. Furthermore, the quality of
content entered through the VUI is comparable to that achieved with the
graphical interface, even for highly complex tasks. Obtained qualitative
results suggest that a hybrid interface combining the strengths of both
approaches could address the key challenges identified during the experiment,
such as reducing cognitive load through graphical feedback while maintaining
the intuitive nature of voice-based interactions. This work highlights the
potential of conversational interfaces as a viable and effective method for
knowledge management in specific business contexts.

</details>


### [312] [Human-Centered Human-AI Collaboration (HCHAC)](https://arxiv.org/abs/2505.22477)
*Qi Gao,Wei Xu,Hanxi Pan,Mowei Shen,Zaifeng Gao*

Key words: 人机协作, 以人为中心的AI, 自主智能体, 自动驾驶, 研究框架

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 本文探讨了以人为中心的人机协作（HAC）模式，分析了其核心概念、研究方法，并提出了一个框架。

Motivation: 在智能时代，人机协作的新模式需要创新视角和研究范式，以应对AI作为主动伙伴带来的挑战。

Method: 采用文献综述和分析方法，提出以人为中心的HAC框架，并结合自动驾驶案例进行验证。

Result: 提出了HCHAC框架，展示了人机协作在自动驾驶中的实际应用，并总结了当前研究的进展。

Conclusion: 未来的研究应关注人机协作系统的有效性、可靠性和伦理整合。

Abstract: In the intelligent era, the interaction between humans and intelligent
systems fundamentally involves collaboration with autonomous intelligent
agents. Human-AI Collaboration (HAC) represents a novel type of human-machine
relationship facilitated by autonomous intelligent machines equipped with AI
technologies. In this paradigm, AI agents serve not only as auxiliary tools but
also as active teammates, partnering with humans to accomplish tasks
collaboratively. Human-centered AI (HCAI) emphasizes that humans play critical
leadership roles in the collaboration. This human-led collaboration imparts new
dimensions to the human-machine relationship, necessitating innovative research
perspectives, paradigms, and agenda to address the unique challenges posed by
HAC. This chapter delves into the essence of HAC from the human-centered
perspective, outlining its core concepts and distinguishing features. It
reviews the current research methodologies and research agenda within the HAC
field from the HCAI perspective, highlighting advancements and ongoing studies.
Furthermore, a framework for human-centered HAC (HCHAC) is proposed by
integrating these reviews and analyses. A case study of HAC in the context of
autonomous vehicles is provided, illustrating practical applications and the
synergistic interactions between humans and AI agents. Finally, it identifies
potential future research directions aimed at enhancing the effectiveness,
reliability, and ethical integration of human-centered HAC systems in diverse
domains.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [313] [Preventing Adversarial AI Attacks Against Autonomous Situational Awareness: A Maritime Case Study](https://arxiv.org/abs/2505.21609)
*Mathew J. Walter,Aaron Barrett,Kimberly Tam*

Key words: 对抗性AI, 自主运输, 数据融合, 网络韧性, 安全指标

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出了一种名为DFCR的新型防御方法，通过多输入和数据融合增强AI系统对抗对抗性攻击的韧性，在海上自主系统的实际测试中显著降低了攻击成功率。

Motivation: 对抗性AI攻击对依赖AI的自主运输系统构成严重威胁，现有防御方法范围有限、安全指标不足，亟需在模型层面之外构建更强的韧性。

Method: 采用多输入和数据融合技术构建防御组件，并提出AI安全指标，该方法称为数据融合网络韧性(DFCR)。

Result: DFCR方法显著提升了对抗攻击的韧性：多角度扰动攻击损失减少35%，对抗补丁攻击和欺骗攻击损失减少100%。

Conclusion: DFCR不仅降低了对抗性AI的接触信心，还提升了系统决策能力，为开发更安全、更有韧性的AI系统提供了新方向。

Abstract: Adversarial artificial intelligence (AI) attacks pose a significant threat to
autonomous transportation, such as maritime vessels, that rely on AI
components. Malicious actors can exploit these systems to deceive and
manipulate AI-driven operations. This paper addresses three critical research
challenges associated with adversarial AI: the limited scope of traditional
defences, inadequate security metrics, and the need to build resilience beyond
model-level defences. To address these challenges, we propose building defences
utilising multiple inputs and data fusion to create defensive components and an
AI security metric as a novel approach toward developing more secure AI
systems. We name this approach the Data Fusion Cyber Resilience (DFCR) method,
and we evaluate it through real-world demonstrations and comprehensive
quantitative analyses, comparing a system built with the DFCR method against
single-input models and models utilising existing state-of-the-art defences.
The findings show that the DFCR approach significantly enhances resilience
against adversarial machine learning attacks in maritime autonomous system
operations, achieving up to a 35\% reduction in loss for successful
multi-pronged perturbation attacks, up to a 100\% reduction in loss for
successful adversarial patch attacks and up to 100\% reduction in loss for
successful spoofing attacks when using these more resilient systems. We
demonstrate how DFCR and DFCR confidence scores can reduce adversarial AI
contact confidence and improve decision-making by the system, even when typical
adversarial defences have been compromised. Ultimately, this work contributes
to the development of more secure and resilient AI-driven systems against
adversarial attacks.

</details>


### [314] [VideoMarkBench: Benchmarking Robustness of Video Watermarking](https://arxiv.org/abs/2505.21620)
*Zhengyuan Jiang,Moyang Guo,Kecen Li,Yuepeng Hu,Yupu Wang,Zhicong Huang,Cheng Hong,Neil Zhenqiang Gong*

Key words: 视频水印, 鲁棒性评估, AI生成视频, 对抗攻击, 基准测试

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: VideoMarkBench是首个系统评估视频水印在移除和伪造攻击下鲁棒性的基准测试，涵盖多种生成模型、风格、水印方法和威胁模型，揭示了当前方法的脆弱性。

Motivation: 针对AI生成视频的水印技术缺乏对常见和对抗性扰动的鲁棒性研究，需系统性评估以解决伦理问题。

Method: 构建VideoMarkBench基准，整合三种视频生成模型、四种水印方法、七种检测策略，并在三种威胁模型下测试12类扰动。

Result: 研究发现现有水印方法存在显著漏洞，尤其在对抗攻击下表现脆弱。

Conclusion: 亟需开发更鲁棒的视频水印方案，以应对日益增长的伦理挑战。

Abstract: The rapid development of video generative models has led to a surge in highly
realistic synthetic videos, raising ethical concerns related to disinformation
and copyright infringement. Recently, video watermarking has been proposed as a
mitigation strategy by embedding invisible marks into AI-generated videos to
enable subsequent detection. However, the robustness of existing video
watermarking methods against both common and adversarial perturbations remains
underexplored. In this work, we introduce VideoMarkBench, the first systematic
benchmark designed to evaluate the robustness of video watermarks under
watermark removal and watermark forgery attacks. Our study encompasses a
unified dataset generated by three state-of-the-art video generative models,
across three video styles, incorporating four watermarking methods and seven
aggregation strategies used during detection. We comprehensively evaluate 12
types of perturbations under white-box, black-box, and no-box threat models.
Our findings reveal significant vulnerabilities in current watermarking
approaches and highlight the urgent need for more robust solutions. Our code is
available at https://github.com/zhengyuan-jiang/VideoMarkBench.

</details>


### [315] [The Feasibility of Topic-Based Watermarking on Academic Peer Reviews](https://arxiv.org/abs/2505.21636)
*Alexander Nemecek,Yuzhou Jiang,Erman Ayday*

Key words: LLM, 同行评审, 主题水印, 保密性, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 本研究评估了一种名为主题水印（TBW）的轻量级技术，用于在LLM生成的文本中嵌入可检测信号，以解决其在同行评审中的使用问题。结果显示TBW在保持评审质量的同时，对改写具有强鲁棒性。

Motivation: 随着LLM生成文本与人类写作难以区分，如何确保同行评审的保密性、避免幻觉内容和不一致评估成为迫切需求。

Method: 采用主题水印（TBW）技术，并在多种LLM配置（基础版、少样本版和微调版）中进行评估，使用真实的学术会议同行评审数据。

Result: TBW在保持非水印输出的评审质量同时，表现出对改写攻击的强鲁棒性。

Conclusion: TBW是一种可行、低侵入性的解决方案，适用于在同行评审中规范LLM的使用。

Abstract: Large language models (LLMs) are increasingly integrated into academic
workflows, with many conferences and journals permitting their use for tasks
such as language refinement and literature summarization. However, their use in
peer review remains prohibited due to concerns around confidentiality breaches,
hallucinated content, and inconsistent evaluations. As LLM-generated text
becomes more indistinguishable from human writing, there is a growing need for
reliable attribution mechanisms to preserve the integrity of the review
process. In this work, we evaluate topic-based watermarking (TBW), a
lightweight, semantic-aware technique designed to embed detectable signals into
LLM-generated text. We conduct a comprehensive assessment across multiple LLM
configurations, including base, few-shot, and fine-tuned variants, using
authentic peer review data from academic conferences. Our results show that TBW
maintains review quality relative to non-watermarked outputs, while
demonstrating strong robustness to paraphrasing-based evasion. These findings
highlight the viability of TBW as a minimally intrusive and practical solution
for enforcing LLM usage in peer review.

</details>


### [316] [A Joint Reconstruction-Triplet Loss Autoencoder Approach Towards Unseen Attack Detection in IoV Networks](https://arxiv.org/abs/2505.21703)
*Julia Boone,Tolunay Seyfi,Fatemeh Afghah*

Key words: 车联网（IoV）、无监督学习、自编码器、异常检测、迁移学习

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出了一种基于无监督自编码器的方法，能够在车联网（IoV）中检测未知攻击，采用加权重构和三重边际损失优化训练，并在实验中展现了高准确率和可迁移性。

Motivation: 车联网（IoV）高度互联的特性带来显著安全风险，传统安全机制难以应对复杂攻击。

Method: 利用无监督自编码器，结合加权重构和三重边际损失训练，专注于良性数据以检测异常。

Result: 在工业IoT和家庭IoT数据集上，模型对未知攻击检测准确率达97%-100%，良性数据准确率约99%，并展示出优秀的迁移能力。

Conclusion: 该方法能有效检测车联网中的未知攻击，且具有跨领域适应性。

Abstract: Internet of Vehicles (IoV) systems, while offering significant advancements
in transportation efficiency and safety, introduce substantial security
vulnerabilities due to their highly interconnected nature. These dynamic
systems produce massive amounts of data between vehicles, infrastructure, and
cloud services and present a highly distributed framework with a wide attack
surface. In considering network-centered attacks on IoV systems, attacks such
as Denial-of-Service (DoS) can prohibit the communication of essential physical
traffic safety information between system elements, illustrating that the
security concerns for these systems go beyond the traditional confidentiality,
integrity, and availability concerns of enterprise systems. Given the
complexity and volume of data generated by IoV systems, traditional security
mechanisms are often inadequate for accurately detecting sophisticated and
evolving cyberattacks. Here, we present an unsupervised autoencoder method
trained entirely on benign network data for the purpose of unseen attack
detection in IoV networks. We leverage a weighted combination of reconstruction
and triplet margin loss to guide the autoencoder training and develop a diverse
representation of the benign training set. We conduct extensive experiments on
recent network intrusion datasets from two different application domains,
industrial IoT and home IoT, that represent the modern IoV task. We show that
our method performs robustly for all unseen attack types, with roughly 99%
accuracy on benign data and between 97% and 100% performance on anomaly data.
We extend these results to show that our model is adaptable through the use of
transfer learning, achieving similarly high results while leveraging domain
features from one domain to another.

</details>


### [317] [Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models](https://arxiv.org/abs/2505.22271)
*Yongcan Yu,Yanbo Wang,Ran He,Jian Liang*

Key words: 大语言模型,越狱攻击,防御框架,安全微调,自进化

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 本文提出了一种名为TIM的通用防御框架，能够自适应地抵御各种越狱攻击。TIM通过训练核心标记进行检测，并在识别攻击时进行安全微调，同时在微调过程中解耦检测模块以避免性能下降。

Motivation: 现有的越狱防御方法往往针对特定类型的攻击，难以应对多样化的对抗策略。为了克服这一局限性，作者提出了TIM框架。

Method: TIM训练核心标记进行高效检测，检测到越狱指令后进行安全微调。为了避免微调影响检测性能，TIM将微调过程与检测模块解耦。

Result: 实验表明，TIM在单一和多模态大语言模型上都能有效防御各种越狱攻击。

Conclusion: TIM是一种自进化的通用防御框架，能够有效应对多样化的越狱攻击策略。

Abstract: While (multimodal) large language models (LLMs) have attracted widespread
attention due to their exceptional capabilities, they remain vulnerable to
jailbreak attacks. Various defense methods are proposed to defend against
jailbreak attacks, however, they are often tailored to specific types of
jailbreak attacks, limiting their effectiveness against diverse adversarial
strategies. For instance, rephrasing-based defenses are effective against text
adversarial jailbreaks but fail to counteract image-based attacks. To overcome
these limitations, we propose a universal defense framework, termed Test-time
IMmunization (TIM), which can adaptively defend against various jailbreak
attacks in a self-evolving way. Specifically, TIM initially trains a gist token
for efficient detection, which it subsequently applies to detect jailbreak
activities during inference. When jailbreak attempts are identified, TIM
implements safety fine-tuning using the detected jailbreak instructions paired
with refusal answers. Furthermore, to mitigate potential performance
degradation in the detector caused by parameter updates during safety
fine-tuning, we decouple the fine-tuning process from the detection module.
Extensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy
of TIM.

</details>


### [318] [SimProcess: High Fidelity Simulation of Noisy ICS Physical Processes](https://arxiv.org/abs/2505.22638)
*Denis Donadel,Gabriele Crestanello,Giulio Morandini,Daniele Antonioli,Mauro Conti,Massimo Merro*

Key words: 工业控制系统,蜜罐,模拟逼真度,随机森林,噪声分布

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 该论文提出了SimProcess框架，通过机器学习模型评估ICS模拟的真实性，以提升蜜罐的逼真度，适用于复杂动态系统。

Motivation: ICS蜜罐难以准确模拟物理过程噪声，导致易被攻击者识破，因此需要一种能评估模拟逼真度的新方法。

Method: 利用随机森林等机器学习模型估计噪声分布，仅需真实系统的时间序列数据即可工作。

Result: 在电力网格数据案例中，模型召回率达1.0，确定高斯和高斯混合分布最适合模拟电力系统。

Conclusion: SimProcess能有效提升蜜罐的真实性，且代码已开源。

Abstract: Industrial Control Systems (ICS) manage critical infrastructures like power
grids and water treatment plants. Cyberattacks on ICSs can disrupt operations,
causing severe economic, environmental, and safety issues. For example,
undetected pollution in a water plant can put the lives of thousands at stake.
ICS researchers have increasingly turned to honeypots -- decoy systems designed
to attract attackers, study their behaviors, and eventually improve defensive
mechanisms. However, existing ICS honeypots struggle to replicate the ICS
physical process, making them susceptible to detection. Accurately simulating
the noise in ICS physical processes is challenging because different factors
produce it, including sensor imperfections and external interferences.
  In this paper, we propose SimProcess, a novel framework to rank the fidelity
of ICS simulations by evaluating how closely they resemble real-world and noisy
physical processes. It measures the simulation distance from a target system by
estimating the noise distribution with machine learning models like Random
Forest. Unlike existing solutions that require detailed mathematical models or
are limited to simple systems, SimProcess operates with only a timeseries of
measurements from the real system, making it applicable to a broader range of
complex dynamic systems. We demonstrate the framework's effectiveness through a
case study using real-world power grid data from the EPIC testbed. We compare
the performance of various simulation methods, including static and generative
noise techniques. Our model correctly classifies real samples with a recall of
up to 1.0. It also identifies Gaussian and Gaussian Mixture as the best
distribution to simulate our power systems, together with a generative solution
provided by an autoencoder, thereby helping developers to improve honeypot
fidelity. Additionally, we make our code publicly available.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [319] [Towards Efficient Key-Value Cache Management for Prefix Prefilling in LLM Inference](https://arxiv.org/abs/2505.21919)
*Yue Zhu,Hao Yu,Chen Wang,Zhuoran Liu,Eun Kyung Lee*

Key words: 大型语言模型, 键值缓存, 元数据管理, 分布式缓存, 推理性能

<details>
  <summary>Details</summary>

Main category: cs.ET

TL;DR: 该论文探讨了大型语言模型（LLMs）中高效键值缓存（KVC）管理的必要性，分析了实际KVC访问模式，并评估了现有键值存储系统的性能，提出了改进KVC管理的设计方向。

Motivation: 随着大型语言模型上下文窗口的扩展，高效的键值缓存管理对推理性能至关重要。现有系统缺乏针对KVC预填充的定制解决方案，亟需优化的分布式缓存系统。

Method: 通过分析公开可用的KVC访问模式数据，评估Redis、CHIME和Sherman等键值存储系统在KVC元数据管理中的表现。

Result: 研究表明现有系统未能有效满足KVC管理的需求，突出了优化元数据管理和设计高效分布式缓存系统的重要性。

Conclusion: 论文强调了为LLM工作负载设计低延迟、可扩展的KVC管理系统的必要性，并提供了改进方向。

Abstract: The increasing adoption of large language models (LLMs) with extended context
windows necessitates efficient Key-Value Cache (KVC) management to optimize
inference performance. Inference workloads like Retrieval-Augmented Generation
(RAG) and agents exhibit high cache reusability, making efficient caching
critical to reducing redundancy and improving speed. We analyze real-world KVC
access patterns using publicly available traces and evaluate commercial
key-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]
and Sherman [2]) for KVC metadata management. Our work demonstrates the lack of
tailored storage solution for KVC prefilling, underscores the need for an
efficient distributed caching system with optimized metadata management for LLM
workloads, and provides insights into designing improved KVC management systems
for scalable, low-latency inference.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [320] [Conformance Checking for Less: Efficient Conformance Checking for Long Event Sequences](https://arxiv.org/abs/2505.21506)
*Eli Bogdanov,Izack Cohen,Avigdor Gal*

Key words: 合规性检查,长事件序列,滑动窗口,ConLES,对齐优化

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: 摘要介绍了一种名为ConLES的方法，用于处理长事件序列的合规性检查，通过滑动窗口技术将长序列分割为可管理的子序列，显著减少了搜索空间并保持了准确性。

Motivation: 随着传感器和预测模型产生的长事件序列和大规模数据日志日益普遍，传统的合规性检查方法因计算复杂度高而难以应对，因此需要一种高效且可扩展的新方法。

Method: ConLES采用滑动窗口技术将长事件序列分割为子序列，并迭代地对每个子序列进行对齐，同时利用全局信息优化对齐决策，丢弃不具潜力的局部最优对齐方案。

Result: 在多个数据集上的性能评估显示，ConLES在处理长事件序列时优于现有最优和启发式算法，能持续获得最优或接近最优的解。

Conclusion: ConLES显著减少了搜索空间，扩展性强，支持预定义和发现的流程模型，是长事件序列合规性检查的领先解决方案。

Abstract: Long event sequences (termed traces) and large data logs that originate from
sensors and prediction models are becoming increasingly common in our data-rich
world. In such scenarios, conformance checking-validating a data log against an
expected system behavior (the process model) can become computationally
infeasible due to the exponential complexity of finding an optimal alignment.
To alleviate scalability challenges for this task, we propose ConLES, a
sliding-window conformance checking approach for long event sequences that
preserves the interpretability of alignment-based methods. ConLES partitions
traces into manageable subtraces and iteratively aligns each against the
expected behavior, leading to significant reduction of the search space while
maintaining overall accuracy. We use global information that captures
structural properties of both the trace and the process model, enabling
informed alignment decisions and discarding unpromising alignments, even if
they appear locally optimal. Performance evaluations across multiple datasets
highlight that ConLES outperforms the leading optimal and heuristic algorithms
for long traces, consistently achieving the optimal or near-optimal solution.
Unlike other conformance methods that struggle with long event sequences,
ConLES significantly reduces the search space, scales efficiently, and uniquely
supports both predefined and discovered process models, making it a viable and
leading option for conformance checking of long event sequences.

</details>


### [321] [StreamLink: Large-Language-Model Driven Distributed Data Engineering System](https://arxiv.org/abs/2505.21575)
*Dawei Feng,Di Mei,Huiri Tan,Lei Ren,Xianying Lou,Zhangxi Tan*

Key words: 大型语言模型, 分布式系统, 数据工程, 自然语言处理, 隐私保护

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: StreamLink是一个基于大型语言模型（LLM）的分布式数据系统，旨在通过本地微调的LLM提升数据工程任务的效率和可访问性，同时保护用户数据隐私。

Motivation: 利用LLM的自然语言理解能力，简化用户与复杂数据库系统的交互，同时确保数据隐私和安全性。

Method: 构建于Apache Spark和Hadoop等分布式框架之上，结合领域适应的LLM和语法/安全检查器，优化自然语言查询到SQL的转换。

Result: SQL生成执行准确率超过基线方法10%，支持用户在数秒内从海量数据中检索目标条目。

Conclusion: StreamLink展示了生成式LLM与分布式数据处理结合在用户友好、安全的数据工程中的潜力。

Abstract: Large Language Models (LLMs) have shown remarkable proficiency in natural
language understanding (NLU), opening doors for innovative applications. We
introduce StreamLink - an LLM-driven distributed data system designed to
improve the efficiency and accessibility of data engineering tasks. We build
StreamLink on top of distributed frameworks such as Apache Spark and Hadoop to
handle large data at scale. One of the important design philosophies of
StreamLink is to respect user data privacy by utilizing local fine-tuned LLMs
instead of a public AI service like ChatGPT. With help from domain-adapted
LLMs, we can improve our system's understanding of natural language queries
from users in various scenarios and simplify the procedure of generating
database queries like the Structured Query Language (SQL) for information
processing. We also incorporate LLM-based syntax and security checkers to
guarantee the reliability and safety of each generated query. StreamLink
illustrates the potential of merging generative LLMs with distributed data
processing for comprehensive and user-centric data engineering. With this
architecture, we allow users to interact with complex database systems at
different scales in a user-friendly and security-ensured manner, where the SQL
generation reaches over 10\% of execution accuracy compared to baseline
methods, and allow users to find the most concerned item from hundreds of
millions of items within a few seconds using natural language.

</details>


### [322] [ChatPD: An LLM-driven Paper-Dataset Networking System](https://arxiv.org/abs/2505.22349)
*Anjie Xu,Ruiqing Ding,Leye Wang*

Key words: 大语言模型, 数据集管理, 学术平台, 自动化提取, 实体解析

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: ChatPD利用大型语言模型自动从论文中提取数据集信息并构建结构化网络，超越现有平台且在实体解析任务中表现优异。

Motivation: 现有学术平台（如PapersWithCode）的数据集管理依赖人工流程，效率低下，亟需自动化解决方案。

Method: 系统包含三个模块：论文收集、数据集信息提取、数据集实体解析；采用图补全与推理策略映射数据集描述与实体。

Result: 在数据集提取任务上超越PapersWithCode，实体解析任务中精确率和召回率达90%，并部署了数据集发现服务。

Conclusion: ChatPD通过自动化显著提升数据集管理效率，开源了系统与论文-数据集网络。

Abstract: Scientific research heavily depends on suitable datasets for method
validation, but existing academic platforms with dataset management like
PapersWithCode suffer from inefficiencies in their manual workflow. To overcome
this bottleneck, we present a system, called ChatPD, that utilizes Large
Language Models (LLMs) to automate dataset information extraction from academic
papers and construct a structured paper-dataset network. Our system consists of
three key modules: \textit{paper collection}, \textit{dataset information
extraction}, and \textit{dataset entity resolution} to construct paper-dataset
networks. Specifically, we propose a \textit{Graph Completion and Inference}
strategy to map dataset descriptions to their corresponding entities. Through
extensive experiments, we demonstrate that ChatPD not only outperforms the
existing platform PapersWithCode in dataset usage extraction but also achieves
about 90\% precision and recall in entity resolution tasks. Moreover, we have
deployed ChatPD to continuously extract which datasets are used in papers, and
provide a dataset discovery service, such as task-specific dataset queries and
similar dataset recommendations. We open source ChatPD and the current
paper-dataset network on this [GitHub
repository]{https://github.com/ChatPD-web/ChatPD}.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [323] [Automatic detection of abnormal clinical EEG: comparison of a finetuned foundation model with two deep learning models](https://arxiv.org/abs/2505.21507)
*Aurore Bussalb,François Le Gac,Guillaume Jubien,Mohamed Rahmouni,Ruggero G. Bettinardi,Pedro Marinho R. de Oliveira,Phillipe Derambure,Nicolas Gaspard,Jacques Jonas,Louis Maillard,Laurent Vercueil,Hervé Vespignani,Philippe Laval,Laurent Koessler,Ulysse Gimenez*

Key words: EEG, deep learning, CNN-LSTM, Transformer, BioSerenity-E1

<details>
  <summary>Details</summary>

Main category: q-bio.NC

TL;DR: 论文比较了CNN-LSTM、Transformer和BioSerenity-E1三种深度学习模型在EEG分类任务中的表现，发现预训练的BioSerenity-E1在多个数据集上表现最佳。

Motivation: 由于EEG解读需求大且需要专业知识，开发AI工具辅助分析，以提高效率和准确性。

Method: 比较CNN-LSTM、Transformer和BioSerenity-E1三种模型，在2500条EEG记录上训练或微调，并在三个数据集（两个私人和一个公开）上评估性能。

Result: BioSerenity-E1微调后在所有数据集上表现最佳，平衡准确率最高达89.19%和94.63%，在公开数据集上达到82.25%准确率。

Conclusion: 预训练模型在EEG自动分类中表现优越，能够高效且稳健地解读数据，适用性广。

Abstract: Electroencephalography (EEG) is commonly used by physicians for the diagnosis
of numerous neurological disorders. Due to the large volume of EEGs requiring
interpretation and the specific expertise involved, artificial
intelligence-based tools are being developed to assist in their visual
analysis. In this paper, we compare two deep learning models (CNN-LSTM and
Transformer-based) with BioSerenity-E1, a recently proposed foundation model,
in the task of classifying entire EEG recordings as normal or abnormal. The
three models were trained or finetuned on 2,500 EEG recordings and their
performances were evaluated on two private and one public datasets: a large
multicenter dataset annotated by a single specialist (dataset A composed of n =
4,480 recordings), a small multicenter dataset annotated by three specialists
(dataset B, n = 198), and the Temple University Abnormal (TUAB) EEG corpus
evaluation dataset (n = 276). On dataset A, the three models achieved at least
86% balanced accuracy, with BioSerenity-E1 finetuned achieving the highest
balanced accuracy (89.19% [88.36-90.41]). BioSerenity-E1 finetuned also
achieved the best performance on dataset B, with 94.63% [92.32-98.12] balanced
accuracy. The models were then validated on TUAB evaluation dataset, whose
corresponding training set was not used during training, where they achieved at
least 76% accuracy. Specifically, BioSerenity-E1 finetuned outperformed the
other two models, reaching an accuracy of 82.25% [78.27-87.48]. Our results
highlight the usefulness of leveraging pre-trained models for automatic EEG
classification: enabling robust and efficient interpretation of EEG data with
fewer resources and broader applicability.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [324] [Network classification through random walks](https://arxiv.org/abs/2505.21706)
*Gonzalo Travieso,Joao Merenda,Odemir M. Bruno*

Key words: 网络模型, 随机游走, 特征提取, 网络分类, 结构分析

<details>
  <summary>Details</summary>

Main category: cs.SI

TL;DR: 本文提出了一种基于随机游走统计量的新方法用于网络分类，比较了其与其他现有方法的性能，结果表明该方法在多数据集中表现优异，但也存在一定局限。

Motivation: 研究动机在于探讨是否可以通过网络结构推断系统类型，并解决现有特征提取方法在此类分类问题中的不足。

Method: 采用基于随机游走的统计量作为网络特征提取方法，并通过多数据集与其他前沿方法进行对比。

Result: 该方法在多数情况下优于现有方法，但在某些数据集中表现存在局限性。

Conclusion: 基于随机游走的特征提取方法在网络分类中具有潜力，但仍需进一步优化以适应更多场景。

Abstract: Network models have been widely used to study diverse systems and analyze
their dynamic behaviors. Given the structural variability of networks, an
intriguing question arises: Can we infer the type of system represented by a
network based on its structure? This classification problem involves extracting
relevant features from the network. Existing literature has proposed various
methods that combine structural measurements and dynamical processes for
feature extraction. In this study, we introduce a novel approach to
characterize networks using statistics from random walks, which can be
particularly informative about network properties. We present the employed
statistical metrics and compare their performance on multiple datasets with
other state-of-the-art feature extraction methods. Our results demonstrate that
the proposed method is effective in many cases, often outperforming existing
approaches, although some limitations are observed across certain datasets.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [325] [CSI-Bench: A Large-Scale In-the-Wild Dataset for Multi-task WiFi Sensing](https://arxiv.org/abs/2505.21866)
*Guozhen Zhu,Yuqian Hu,Weihang Gao,Wei-Hsiang Wang,Beibei Wang,K. J. Ray Liu*

Key words: WiFi sensing, CSI, benchmark dataset, health monitoring, multi-task learning

<details>
  <summary>Details</summary>

Main category: eess.SP

TL;DR: CSI-Bench是一个大规模、多样化环境下的WiFi感知基准数据集，旨在解决现有系统在真实场景中泛化能力不足的问题。

Motivation: 现有WiFi感知系统因数据集局限于受控环境和硬件同质性，难以在真实场景中泛化，因此需要更具代表性的数据集推动技术发展。

Method: 通过商业WiFi边缘设备在26种多样室内环境中收集35名用户的461小时数据，覆盖多种任务（如跌倒检测、呼吸监测等），并提供标准化评估分割和基线结果。

Result: CSI-Bench提供了真实信号变异性数据，支持单任务和多任务学习，为开发鲁棒、通用的WiFi感知系统奠定了基础。

Conclusion: CSI-Bench为健康和以人为中心的应用提供了可扩展、隐私保护的WiFi感知系统基础。

Abstract: WiFi sensing has emerged as a compelling contactless modality for human
activity monitoring by capturing fine-grained variations in Channel State
Information (CSI). Its ability to operate continuously and non-intrusively
while preserving user privacy makes it particularly suitable for health
monitoring. However, existing WiFi sensing systems struggle to generalize in
real-world settings, largely due to datasets collected in controlled
environments with homogeneous hardware and fragmented, session-based recordings
that fail to reflect continuous daily activity.
  We present CSI-Bench, a large-scale, in-the-wild benchmark dataset collected
using commercial WiFi edge devices across 26 diverse indoor environments with
35 real users. Spanning over 461 hours of effective data, CSI-Bench captures
realistic signal variability under natural conditions. It includes
task-specific datasets for fall detection, breathing monitoring, localization,
and motion source recognition, as well as a co-labeled multitask dataset with
joint annotations for user identity, activity, and proximity. To support the
development of robust and generalizable models, CSI-Bench provides standardized
evaluation splits and baseline results for both single-task and multi-task
learning. CSI-Bench offers a foundation for scalable, privacy-preserving WiFi
sensing systems in health and broader human-centric applications.

</details>


### [326] [Empowering Intelligent Low-altitude Economy with Large AI Model Deployment](https://arxiv.org/abs/2505.22343)
*Zhonghao Lyu,Yulan Gao,Junting Chen,Hongyang Du,Jie Xu,Kaibin Huang,Dong In Kim*

Key words: 低空经济, 人工智能大模型, 分层架构, 任务导向, 共同演进

<details>
  <summary>Details</summary>

Main category: eess.SP

TL;DR: 该论文探讨了人工智能大模型在低空经济中的应用挑战与解决方案，提出了分层系统架构、关键技术及任务导向执行流程，并通过案例验证，最后指出未来研究方向。

Motivation: 低空经济（LAE）作为一种新兴经济模式，面临人工智能大模型（LAIMs）部署的挑战，包括计算存储需求与资源限制、实验室训练与动态环境不匹配及传统设计效率低的问题。

Method: 提出分层系统架构和任务导向执行流程，探索促进LAIMs与低空系统共同演进的关键技术，并通过实际案例验证框架有效性。

Result: 提出的框架在真实案例中验证了其可扩展性和适应性服务交付的有效性。

Conclusion: 论文解决了LAIMs在LAE中的部署难题，但未来仍需应对开放挑战。

Abstract: Low-altitude economy (LAE) represents an emerging economic paradigm that
redefines commercial and social aerial activities. Large artificial
intelligence models (LAIMs) offer transformative potential to further enhance
the intelligence of LAE services. However, deploying LAIMs in LAE poses several
challenges, including the significant gap between their computational/storage
demands and the limited onboard resources of LAE entities, the mismatch between
lab-trained LAIMs and dynamic physical environments, and the inefficiencies of
traditional decoupled designs for sensing, communication, and computation. To
address these issues, we first propose a hierarchical system architecture
tailored for LAIM deployment and present representative LAE application
scenarios. Next, we explore key enabling techniques that facilitate the mutual
co-evolution of LAIMs and low-altitude systems, and introduce a task-oriented
execution pipeline for scalable and adaptive service delivery. Then, the
proposed framework is validated through real-world case studies. Finally, we
outline open challenges to inspire future research.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [327] [tenSVD algorithm for compression](https://arxiv.org/abs/2505.21686)
*Michele Gallo*

Key words: 张量分析，图像存储，Tucker模型，资源效率，能耗

<details>
  <summary>Details</summary>

Main category: stat.CO

TL;DR: 该论文提出了一种基于张量的高效图像存储方法，利用Tucker模型进行压缩，降低了存储、传输和处理的资源消耗。

Motivation: 高维数据管理需求日益增长，尤其在机器学习、信号处理等领域。研究旨在通过张量分析减少图像存储的资源消耗。

Method: 将原始数据组织为高阶张量，应用Tucker模型进行压缩，并在R中实现，与基线算法对比。

Result: 通过模拟和真实数据集评估，算法在计算时间和信息保留质量上表现优异，显著降低了能耗。

Conclusion: 张量压缩方法在资源效率和可持续性方面具有潜力。

Abstract: Tensors provide a robust framework for managing high-dimensional data.
Consequently, tensor analysis has emerged as an active research area in various
domains, including machine learning, signal processing, computer vision, graph
analysis, and data mining. This study introduces an efficient image storage
approach utilizing tensors, aiming to minimize memory to store, bandwidth to
transmit and energy to processing. The proposed method organizes original data
into a higher-order tensor and applies the Tucker model for compression.
Implemented in R, this method is compared to a baseline algorithm. The
evaluation focuses on efficient of algorithm measured in term of computational
time and the quality of information preserved, using both simulated and real
datasets. A detailed analysis of the results is conducted, employing
established quantitative metrics, with significant attention paid to
sustainability in terms of energy consumption across algorithms.

</details>


### [328] [Are Statistical Methods Obsolete in the Era of Deep Learning?](https://arxiv.org/abs/2505.21723)
*Skyler Wu,Shihao Yang,S. C. Kou*

Key words: 神经ODE、PINN、MAGI、统计推断、深度学习对比

<details>
  <summary>Details</summary>

Main category: stat.CO

TL;DR: 比较深度学习方法（如PINN）与统计方法（如MAGI）在ODE反问题中的表现，统计方法在稀疏噪声数据、参数推断和轨迹重建中表现更优。

Motivation: 探讨在AI时代，统计方法是否仍具有价值，尤其在处理稀疏噪声数据和ODE反问题时。

Method: 使用SEIR流行病学模型和Lorenz混沌模型作为案例，对比PINN（深度学习）和MAGI（统计方法）的表现。

Result: 统计方法在偏差、方差、参数效率和预测鲁棒性上优于深度学习，尤其在样本外预测和数值精度累积方面。

Conclusion: 统计方法并未过时，尤其在数据有限或噪声较多的场景中，其理论优势和鲁棒性显著。

Abstract: In the era of AI, neural networks have become increasingly popular for
modeling, inference, and prediction, largely due to their potential for
universal approximation. With the proliferation of such deep learning models, a
question arises: are leaner statistical methods still relevant? To shed insight
on this question, we employ the mechanistic nonlinear ordinary differential
equation (ODE) inverse problem as a testbed, using physics-informed neural
network (PINN) as a representative of the deep learning paradigm and
manifold-constrained Gaussian process inference (MAGI) as a representative of
statistically principled methods. Through case studies involving the SEIR model
from epidemiology and the Lorenz model from chaotic dynamics, we demonstrate
that statistical methods are far from obsolete, especially when working with
sparse and noisy observations. On tasks such as parameter inference and
trajectory reconstruction, statistically principled methods consistently
achieve lower bias and variance, while using far fewer parameters and requiring
less hyperparameter tuning. Statistical methods can also decisively outperform
deep learning models on out-of-sample future prediction, where the absence of
relevant data often leads overparameterized models astray. Additionally, we
find that statistically principled approaches are more robust to accumulation
of numerical imprecision and can represent the underlying system more faithful
to the true governing ODEs.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [329] [What Data Enables Optimal Decisions? An Exact Characterization for Linear Optimization](https://arxiv.org/abs/2505.21692)
*Omar Bennouna,Amine Bennouna,Saurabh Amin,Asuman Ozdaglar*

Key words: 数据集信息量, 线性程序, 最优决策, 成本向量, 数据选择

<details>
  <summary>Details</summary>

Main category: math.OC

TL;DR: 该研究探讨数据集对于解决决策任务的信息量，通过线性程序分析了数据集在成本向量不确定条件下恢复最优决策的充分性，并提供了几何特征描述和实用算法。

Motivation: 研究旨在理解数据集如何为特定决策任务提供有效信息，尤其是在部分信息未知的情况下，探讨如何选择和构建数据集以确保最优决策。

Method: 采用线性程序分析，构建几何特征以识别成本向量中对最优决策关键的方向，并开发算法生成最小或最经济的充分数据集。

Result: 研究发现小而精选的数据集往往足以确定最优决策，为任务驱动的数据选择提供了理论基础和实践方法。

Conclusion: 研究提供了数据选择和最优决策之间的理论联系，证明了在特定条件下，小而高效的数据集可以完全支持决策任务。

Abstract: We study the fundamental question of how informative a dataset is for solving
a given decision-making task. In our setting, the dataset provides partial
information about unknown parameters that influence task outcomes. Focusing on
linear programs, we characterize when a dataset is sufficient to recover an
optimal decision, given an uncertainty set on the cost vector. Our main
contribution is a sharp geometric characterization that identifies the
directions of the cost vector that matter for optimality, relative to the task
constraints and uncertainty set. We further develop a practical algorithm that,
for a given task, constructs a minimal or least-costly sufficient dataset. Our
results reveal that small, well-chosen datasets can often fully determine
optimal decisions -- offering a principled foundation for task-aware data
selection.

</details>


### [330] [PolarGrad: A Class of Matrix-Gradient Optimizers from a Unifying Preconditioning Perspective](https://arxiv.org/abs/2505.21799)
*Tim Tsz-Kit Lau,Qi Long,Weijie Su*

Key words: 预条件优化, 矩阵结构, PolarGrad, 深度学习, 极分解

<details>
  <summary>Details</summary>

Main category: math.OC

TL;DR: 本文提出了一个统一框架来分析基于矩阵结构的预条件优化方法，并引入了PolarGrad这一新方法。

Motivation: 深度学习模型和数据集的规模不断扩大，需要更高效的优化方法。现有的Adam和AdamW等预条件梯度方法虽广泛应用，但Shampoo和Muon等基于矩阵结构的优化器展现出更快的收敛性。

Method: 提出了一个统一的分析框架，区分向量和矩阵结构的预条件策略，并基于此开发了PolarGrad方法，利用矩阵梯度的极分解。

Result: PolarGrad在矩阵优化问题和语言模型预训练任务中表现优于Adam和Muon。

Conclusion: 矩阵结构的预条件优化方法能更高效训练深度模型，PolarGrad提供了新的优化方向。

Abstract: The ever-growing scale of deep learning models and datasets underscores the
critical importance of efficient optimization methods. While preconditioned
gradient methods such as Adam and AdamW are the de facto optimizers for
training neural networks and large language models, structure-aware
preconditioned optimizers like Shampoo and Muon, which utilize the matrix
structure of gradients, have demonstrated promising evidence of faster
convergence. In this paper, we introduce a unifying framework for analyzing
"matrix-aware" preconditioned methods, which not only sheds light on the
effectiveness of Muon and related optimizers but also leads to a class of new
structure-aware preconditioned methods. A key contribution of this framework is
its precise distinction between preconditioning strategies that treat neural
network weights as vectors (addressing curvature anisotropy) versus those that
consider their matrix structure (addressing gradient anisotropy). This
perspective provides new insights into several empirical phenomena in language
model pre-training, including Adam's training instabilities, Muon's accelerated
convergence, and the necessity of learning rate warmup for Adam. Building upon
this framework, we introduce PolarGrad, a new class of preconditioned
optimization methods based on the polar decomposition of matrix-valued
gradients. As a special instance, PolarGrad includes Muon with updates scaled
by the nuclear norm of the gradients. We provide numerical implementations of
these methods, leveraging efficient numerical polar decomposition algorithms
for enhanced convergence. Our extensive evaluations across diverse matrix
optimization problems and language model pre-training tasks demonstrate that
PolarGrad outperforms both Adam and Muon.

</details>


### [331] [PADAM: Parallel averaged Adam reduces the error for stochastic optimization in scientific machine learning](https://arxiv.org/abs/2505.22085)
*Arnulf Jentzen,Julian Kranz,Adrian Riekert*

Key words: PADAM, 随机梯度下降, 平均方法, ADAM, 科学机器学习

<details>
  <summary>Details</summary>

Main category: math.OC

TL;DR: 论文提出了一种称为并行平均ADAM（PADAM）的新优化方法，通过并行计算不同平均变体并动态选择优化误差最小的变体，显著提升了ADAM优化器的性能。

Motivation: 现有平均方法如Ruppert--Polyak和指数移动平均（EMA）在加速随机梯度下降（SGD）优化时需要调整类型和参数，难以普适。PADAM旨在动态选择最佳平均变体，无需额外梯度计算。

Method: PADAM并行计算多个ADAM的平均变体，并在训练过程中动态选择优化误差最小的变体，所有变体共享相同的ADAM轨迹和梯度。

Result: 在13个随机优化和深度神经网络（DNN）学习问题中，PADAM在大多数情况下表现最佳，优化误差最小。

Conclusion: PADAM在科学机器学习问题中表现优异，值得推广，并为进一步研究DNN训练中的自适应平均方法提供了动机。

Abstract: Averaging techniques such as Ruppert--Polyak averaging and exponential
movering averaging (EMA) are powerful approaches to accelerate optimization
procedures of stochastic gradient descent (SGD) optimization methods such as
the popular ADAM optimizer. However, depending on the specific optimization
problem under consideration, the type and the parameters for the averaging need
to be adjusted to achieve the smallest optimization error. In this work we
propose an averaging approach, which we refer to as parallel averaged ADAM
(PADAM), in which we compute parallely different averaged variants of ADAM and
during the training process dynamically select the variant with the smallest
optimization error. A central feature of this approach is that this procedure
requires no more gradient evaluations than the usual ADAM optimizer as each of
the averaged trajectories relies on the same underlying ADAM trajectory and
thus on the same underlying gradients. We test the proposed PADAM optimizer in
13 stochastic optimization and deep neural network (DNN) learning problems and
compare its performance with known optimizers from the literature such as
standard SGD, momentum SGD, Adam with and without EMA, and ADAMW. In
particular, we apply the compared optimizers to physics-informed neural
network, deep Galerkin, deep backward stochastic differential equation and deep
Kolmogorov approximations for boundary value partial differential equation
problems from scientific machine learning, as well as to DNN approximations for
optimal control and optimal stopping problems. In nearly all of the considered
examples PADAM achieves, sometimes among others and sometimes exclusively,
essentially the smallest optimization error. This work thus strongly suggest to
consider PADAM for scientific machine learning problems and also motivates
further research for adaptive averaging procedures within the training of DNNs.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [332] [RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination](https://arxiv.org/abs/2505.21925)
*Chong Zeng,Yue Dong,Pieter Peers,Hongzhi Wu,Xin Tong*

Key words: 神经渲染、全局光照、Transformer、序列到序列、光传输

<details>
  <summary>Details</summary>

Main category: cs.GR

TL;DR: RenderFormer是一个直接从场景的三角形表示渲染图像的神经渲染管道，无需每个场景训练或微调，实现了全局光照效果。

Motivation: 旨在通过序列到序列的转换方法，避免传统物理中心渲染方法的需求，简化渲染流程。

Method: 采用两阶段管道：视图无关阶段建模三角形间光传输，视图依赖阶段将光线束转换为像素值，两者均基于无强先验约束的Transformer架构。

Result: 在形状和光传输复杂度不同的场景中进行了验证和评估，展示了其有效性。

Conclusion: RenderFormer提供了一种无需物理建模即可实现全局光照效果的渲染方法，简化了渲染流程。

Abstract: We present RenderFormer, a neural rendering pipeline that directly renders an
image from a triangle-based representation of a scene with full global
illumination effects and that does not require per-scene training or
fine-tuning. Instead of taking a physics-centric approach to rendering, we
formulate rendering as a sequence-to-sequence transformation where a sequence
of tokens representing triangles with reflectance properties is converted to a
sequence of output tokens representing small patches of pixels. RenderFormer
follows a two stage pipeline: a view-independent stage that models
triangle-to-triangle light transport, and a view-dependent stage that
transforms a token representing a bundle of rays to the corresponding pixel
values guided by the triangle-sequence from the view-independent stage. Both
stages are based on the transformer architecture and are learned with minimal
prior constraints. We demonstrate and evaluate RenderFormer on scenes with
varying complexity in shape and light transport.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [333] [CPINN-ABPI: Physics-Informed Neural Networks for Accurate Power Estimation in MPSoCs](https://arxiv.org/abs/2505.22469)
*Mohamed R. Elshamy,Mehdi Elahi,Ahmad Patooghy,Abdel-Hameed A. Badawy*

Key words: 功耗估计, MPSoCs, ABPI, CPINNs, 遗传算法

<details>
  <summary>Details</summary>

Main category: cs.PF

TL;DR: 本文研究了ABPI在实际硬件中的性能，发现其存在精度不足的问题，并提出了一种结合CPINNs的新方法，显著提高了功率估计的精度并保持了实时性能。

Motivation: 现代多处理器系统芯片（MPSoCs）需要精确的功耗估计。尽管ABPI方法理论上解决了对稳态温度的依赖问题，但其实际硬件性能尚未验证。

Method: 提出了一种结合CPINNs和ABPI热模型的新方法，使用专用损失函数平衡物理原理与数据驱动学习，并通过多目标遗传算法优化精度与计算成本。

Result: CPINN-ABPI方法将CPU和GPU的平均绝对误差（MAE）分别降低了84.7％和73.9％，加权平均绝对百分比误差（WMAPE）从47％-81％降至约12％，推理时间为195.3微秒。

Conclusion: 新方法CPINN-ABPI显著提升了ABPI的精度，适用于异构SoC，同时保持了实时性能。

Abstract: Efficient thermal and power management in modern multiprocessor
systems-on-chip (MPSoCs) demands accurate power consumption estimation. One of
the state-of-the-art approaches, Alternative Blind Power Identification (ABPI),
theoretically eliminates the dependence on steady-state temperatures,
addressing a major shortcoming of previous approaches. However, ABPI
performance has remained unverified in actual hardware implementations. In this
study, we conduct the first empirical validation of ABPI on commercial hardware
using the NVIDIA Jetson Xavier AGX platform. Our findings reveal that, while
ABPI provides computational efficiency and independence from steady-state
temperature, it exhibits considerable accuracy deficiencies in real-world
scenarios. To overcome these limitations, we introduce a novel approach that
integrates Custom Physics-Informed Neural Networks (CPINNs) with the underlying
thermal model of ABPI. Our approach employs a specialized loss function that
harmonizes physical principles with data-driven learning, complemented by
multi-objective genetic algorithm optimization to balance estimation accuracy
and computational cost. In experimental validation, CPINN-ABPI achieves a
reduction of 84.7\% CPU and 73.9\% GPU in the mean absolute error (MAE)
relative to ABPI, with the weighted mean absolute percentage error (WMAPE)
improving from 47\%--81\% to $\sim$12\%. The method maintains real-time
performance with 195.3~$\mu$s of inference time, with similar 85\%--99\%
accuracy gains across heterogeneous SoCs.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [334] [Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives](https://arxiv.org/abs/2505.21627)
*Ander Artola Velasco,Stratis Tsirtsis,Nastaran Okati,Manuel Gomez-Rodriguez*

Key words: 大型语言模型,定价机制,标记数,字符数,激励兼容

<details>
  <summary>Details</summary>

Main category: cs.GT

TL;DR: 该研究揭示了现有按标记付费机制中存在的提供商虚报标记数以多收费用的漏洞，并提出了一种按字符付费的新机制来消除这一财务激励。

Motivation: 现有的大型语言模型按标记收费机制可能导致提供商虚报标记数，用户无法验证其真实性，从而被多收费用。

Method: 研究分析了按标记付费的问题，提出了一种启发式算法证明提供商可如何虚报标记数而不被发现，并设计了按字符付费的激励兼容机制。

Result: 实验表明，按字符付费机制能有效消除提供商虚报标记数的财务激励。

Conclusion: 按字符付费是一种更透明且激励兼容的定价机制，可保护用户免受不公平收费。

Abstract: State-of-the-art large language models require specialized hardware and
substantial energy to operate. As a consequence, cloud-based services that
provide access to large language models have become very popular. In these
services, the price users pay for an output provided by a model depends on the
number of tokens the model uses to generate it -- they pay a fixed price per
token. In this work, we show that this pricing mechanism creates a financial
incentive for providers to strategize and misreport the (number of) tokens a
model used to generate an output, and users cannot prove, or even know, whether
a provider is overcharging them. However, we also show that, if an unfaithful
provider is obliged to be transparent about the generative process used by the
model, misreporting optimally without raising suspicion is hard. Nevertheless,
as a proof-of-concept, we introduce an efficient heuristic algorithm that
allows providers to significantly overcharge users without raising suspicion,
highlighting the vulnerability of users under the current pay-per-token pricing
mechanism. Further, to completely eliminate the financial incentive to
strategize, we introduce a simple incentive-compatible token pricing mechanism.
Under this mechanism, the price users pay for an output provided by a model
depends on the number of characters of the output -- they pay a fixed price per
character. Along the way, to illustrate and complement our theoretical results,
we conduct experiments with several large language models from the
$\texttt{Llama}$, $\texttt{Gemma}$ and $\texttt{Ministral}$ families, and input
prompts from the LMSYS Chatbot Arena platform.

</details>


### [335] [Online Fair Division for Personalized $2$-Value Instances](https://arxiv.org/abs/2505.22174)
*Georgios Amanatidis,Alexandros Lolos,Evangelos Markakis,Victor Turmel*

Key words: 在线公平分配、受限估值、MMS、EF1、EF2

<details>
  <summary>Details</summary>

Main category: cs.GT

TL;DR: 提出了确定性算法，在受限估值条件下实现公平分配的最坏情况保证，并通过优先级系统和预见信息优化结果。

Motivation: 绕过在线公平分配中的强不可能结果，研究受限估值函数下的可行性，特别是2值个性化实例。

Method: 提出确定性算法，维护代理优先级系统，并设计基于匹配的算法利用有限预见信息。

Result: 算法在每一步保持1/(2n-1)-MMS分配，最终达到1/4-MMS；利用有限预见信息可实现EF1和EF2分配。

Conclusion: 受限估值和有限预见信息可突破不可能结果，为比率有界的加法实例提供首个非平凡保证。

Abstract: We study an online fair division setting, where goods arrive one at a time
and there is a fixed set of $n$ agents, each of whom has an additive valuation
function over the goods. Once a good appears, the value each agent has for it
is revealed and it must be allocated immediately and irrevocably to one of the
agents. It is known that without any assumptions about the values being
severely restricted or coming from a distribution, very strong impossibility
results hold in this setting. To bypass the latter, we turn our attention to
instances where the valuation functions are restricted. In particular, we study
personalized $2$-value instances, where there are only two possible values each
agent may have for each good, possibly different across agents, and we show how
to obtain worst case guarantees with respect to well-known fairness notions,
such as maximin share fairness and envy-freeness up to one (or two) good(s). We
suggest a deterministic algorithm that maintains a $1/(2n-1)$-MMS allocation at
every time step and show that this is the best possible any deterministic
algorithm can achieve if one cares about every single time step; nevertheless,
eventually the allocation constructed by our algorithm becomes a $1/4$-MMS
allocation. To achieve this, the algorithm implicitly maintains a fragile
system of priority levels for all agents. Further, we show that, by allowing
some limited access to future information, it is possible to have stronger
results with less involved approaches. By knowing the values of goods for $n-1$
time steps into the future, we design a matching-based algorithm that achieves
an EF$1$ allocation every $n$ time steps, while always maintaining an EF$2$
allocation. Finally, we show that our results allow us to get the first
nontrivial guarantees for additive instances in which the ratio of the maximum
over the minimum value an agent has for a good is bounded.

</details>


### [336] [Strengthening Proportionality in Temporal Voting](https://arxiv.org/abs/2505.22513)
*Bradley Phillips,Edith Elkind,Nicholas Teh,Tomasz Wąs*

Key words: 比例代表, 时序投票, 批准投票, EJR+, FJR, 公理

<details>
  <summary>Details</summary>

Main category: cs.GT

TL;DR: 该论文研究了在时序投票中使用更强比例代表概念的情况，包括EJR+和FJR等新提出的公理，并证明了这些公理比EJR更强但仍可满足。

Motivation: 动机是扩展时序投票中的比例代表概念，提出更强版本的公理（如EJR+和FJR），并研究其可行性和与现有概念的关系。

Method: 方法是通过引入时序适应性强版多赢家公理（如EJR+, FJR, FPJR, 和Core），并验证其存在性及与现有概念的关系。

Result: 结果显示EJR+和FJR不仅能强化EJR，且在每次时序选举中均可满足。

Conclusion: 结论是建立了比例代表概念的丰富层次结构，特别是EJR+和FJR的引入为时序投票提供了更强的代表保障。

Abstract: We study proportional representation in the framework of temporal voting with
approval ballots. Prior work adapted basic proportional representation concepts
-- justified representation (JR), proportional JR (PJR), and extended JR (EJR)
-- from the multiwinner setting to the temporal setting. Our work introduces
and examines ways of going beyond EJR. Specifically, we consider stronger
variants of JR, PJR, and EJR, and introduce temporal adaptations of more
demanding multiwinner axioms, such as EJR+, full JR (FJR), full proportional JR
(FPJR), and the Core. For each of these concepts, we investigate its existence
and study its relationship to existing notions, thereby establishing a rich
hierarchy of proportionality concepts. Notably, we show that two of our
proposed axioms -- EJR+ and FJR -- strengthen EJR while remaining satisfiable
in every temporal election.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [337] [Locking-Free Training of Physics-Informed Neural Network for Solving Nearly Incompressible Elasticity Equations](https://arxiv.org/abs/2505.21994)
*Josef Dick,Seungchan Ko,Kassem Mustapha,Sanghyeon Park*

Key words: 物理信息神经网络、近不可压缩材料、线性弹性方程、锁定现象、分解方法

<details>
  <summary>Details</summary>

Main category: math.NA

TL;DR: 提出一种基于物理信息神经网络（PINN）的鲁棒方法，解决近不可压缩材料的线性弹性方程数值解问题，通过分解方程减轻系数不平衡，并在数值实验中展示了其有效性。

Motivation: 传统低阶有限元方法在Lamé系数λ→∞或泊松比ν→1/2时精度下降（锁定现象），尽管已有大量研究，但问题尚未完全解决，本文希望通过机器学习方法改进。

Method: 利用物理信息神经网络（PINN），分解线性弹性方程以缓解系数不平衡，同时求解正向和逆向问题以恢复分解系统和外部条件。

Result: 通过常数、变量和参数化Lamé系数的数值实验，验证了所提方法的效率和鲁棒性。

Conclusion: 机器学习驱动的PINN方法能有效解决近不可压缩材料的线性弹性问题，避免了传统有限元的锁定现象。

Abstract: Due to divergence instability, the accuracy of low-order conforming finite
element methods for nearly incompressible homogeneous elasticity equations
deteriorates as the Lam\'e coefficient $\lambda\to\infty$, or equivalently as
the Poisson ratio $\nu\to1/2$. This phenomenon, known as locking or
non-robustness, remains not fully understood despite extensive investigation.
In this paper, we propose a robust method based on a fundamentally different,
machine-learning-driven approach. Leveraging recently developed
Physics-Informed Neural Networks (PINNs), we address the numerical solution of
linear elasticity equations governing nearly incompressible materials. The core
idea of our method is to appropriately decompose the given equations to
alleviate the extreme imbalance in the coefficients, while simultaneously
solving both the forward and inverse problems to recover the solutions of the
decomposed systems as well as the associated external conditions. Through
various numerical experiments, including constant, variable and parametric
Lam\'e coefficients, we illustrate the efficiency of the proposed methodology.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [338] [Enhancing Vision Transformer Explainability Using Artificial Astrocytes](https://arxiv.org/abs/2505.21513)
*Nicolas Echevarrieta-Catalan,Ana Ribas-Rodriguez,Francisco Cedron,Odelia Schwartz,Vanessa Aguiar-Pulido*

Key words: 机器学习的可解释性, 人工星形胶质细胞, 无训练优化, 神经科学启发, XAI

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种基于神经科学的无训练方法ViTA，通过在预训练深度神经网络中引入人工星形胶质细胞，显著提高了模型解释与人类认知的对齐度。

Motivation: 当前机器学习模型虽然精度高，但解释性不足，随着模型复杂度增加，解释性进一步降低。现有改进方法（如XAI技术或训练时加入解释性约束）适用性有限。

Method: 提出ViTA方法，结合神经科学原理，在不重新训练的情况下，通过人工星形胶质细胞增强预训练网络的推理能力，生成更符合人类理解的解释。采用Grad-CAM和Grad-CAM++技术评估，并与标准ViT对比。

Result: 在ClickMe数据集上，ViTA显著提高了XAI技术生成的热图与人类认知基准的对齐度，所有测试指标均显示统计学显著改进。

Conclusion: ViTA通过仿生学设计有效提升了模型解释的人类对齐性，为可解释AI提供了新的无训练优化途径。

Abstract: Machine learning models achieve high precision, but their decision-making
processes often lack explainability. Furthermore, as model complexity
increases, explainability typically decreases. Existing efforts to improve
explainability primarily involve developing new eXplainable artificial
intelligence (XAI) techniques or incorporating explainability constraints
during training. While these approaches yield specific improvements, their
applicability remains limited. In this work, we propose the Vision Transformer
with artificial Astrocytes (ViTA). This training-free approach is inspired by
neuroscience and enhances the reasoning of a pretrained deep neural network to
generate more human-aligned explanations. We evaluated our approach employing
two well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a
standard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the
similarity between the heatmaps produced by the XAI techniques and a
(human-aligned) ground truth. Our results consistently demonstrate that
incorporating artificial astrocytes enhances the alignment of model
explanations with human perception, leading to statistically significant
improvements across all XAI techniques and metrics utilized.

</details>


### [339] [Do DeepFake Attribution Models Generalize?](https://arxiv.org/abs/2505.21520)
*Spiros Baxavanakis,Manos Schinas,Symeon Papadopoulos*

Key words: DeepFake检测, 归因模型, 多分类, 对比学习, 泛化能力

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文探讨了DeepFake检测中的多分类模型和归因模型，通过实验比较了其泛化能力和准确性，并提出对比学习方法对提升性能的影响。

Motivation: 由于DeepFake技术的普及降低了合成媒体的制作门槛，对在线信息的真实性和公众信任构成威胁。现有研究多集中于二分类模型，忽略了不同篡改方法带来的独特痕迹，而归因模型能提供更可信和可解释的结果。

Method: 利用五种骨干模型在六个DeepFake数据集上进行实验，比较二分类与多分类模型的跨数据集泛化能力，评估归因模型对未知数据集的检测准确性，并研究对比方法对性能的提升。

Result: 实验表明，二分类模型泛化能力更强，但更大模型、对比方法和更高数据质量可以提升归因模型的性能。

Conclusion: 归因模型在提升可信度和可解释性方面具有潜力，但需进一步优化以改善跨数据集性能。

Abstract: Recent advancements in DeepFake generation, along with the proliferation of
open-source tools, have significantly lowered the barrier for creating
synthetic media. This trend poses a serious threat to the integrity and
authenticity of online information, undermining public trust in institutions
and media. State-of-the-art research on DeepFake detection has primarily
focused on binary detection models. A key limitation of these models is that
they treat all manipulation techniques as equivalent, despite the fact that
different methods introduce distinct artifacts and visual cues. Only a limited
number of studies explore DeepFake attribution models, although such models are
crucial in practical settings. By providing the specific manipulation method
employed, these models could enhance both the perceived trustworthiness and
explainability for end users. In this work, we leverage five state-of-the-art
backbone models and conduct extensive experiments across six DeepFake datasets.
First, we compare binary and multi-class models in terms of cross-dataset
generalization. Second, we examine the accuracy of attribution models in
detecting seen manipulation methods in unknown datasets, hence uncovering data
distribution shifts on the same DeepFake manipulations. Last, we assess the
effectiveness of contrastive methods in improving cross-dataset generalization
performance. Our findings indicate that while binary models demonstrate better
generalization abilities, larger models, contrastive methods, and higher data
quality can lead to performance improvements in attribution models. The code of
this work is available on GitHub.

</details>


### [340] [CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures](https://arxiv.org/abs/2505.21522)
*Shan Gao,Zhiqiang Wu,Yawen Niu,Xiaotao Li,Qingqing Xu*

Key words: DNN, CIM, video denoising, MVM, co-design

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种硬件-算法协同设计框架CIM-NET，结合CIM-Aware架构和伪卷积算子CIM-CONV，显著减少矩阵-向量乘法操作，提升CIM芯片的推理速度并保持性能。

Motivation: 由于边缘设备对实时性和能效的严格要求，现有DNN模型未考虑CIM架构限制，限制了其推理加速潜力。因此，需设计一种适合CIM架构的优化方案。

Method: 提出了CIM-NET架构和伪卷积算子CIM-CONV，结合大感受野操作和基于交叉开关的MVM加速，减少MVM操作。

Result: CIM-NET将MVM操作减少至1/77，同时保持与FastDVDnet相近的PSNR（35.11 dB vs. 35.56 dB）。

Conclusion: 硬件-算法协同设计框架有效优化了CIM芯片上的DNN推理，平衡了速度和性能。

Abstract: While deep neural network (DNN)-based video denoising has demonstrated
significant performance, deploying state-of-the-art models on edge devices
remains challenging due to stringent real-time and energy efficiency
requirements. Computing-in-Memory (CIM) chips offer a promising solution by
integrating computation within memory cells, enabling rapid matrix-vector
multiplication (MVM). However, existing DNN models are often designed without
considering CIM architectural constraints, thus limiting their acceleration
potential during inference. To address this, we propose a hardware-algorithm
co-design framework incorporating two innovations: (1) a CIM-Aware
Architecture, CIM-NET, optimized for large receptive field operation and CIM's
crossbar-based MVM acceleration; and (2) a pseudo-convolutional operator,
CIM-CONV, used within CIM-NET to integrate slide-based processing with fully
connected transformations for high-quality feature extraction and
reconstruction. This framework significantly reduces the number of MVM
operations, improving inference speed on CIM chips while maintaining
competitive performance. Experimental results indicate that, compared to the
conventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM
operations with a slight decrease in denoising performance. With a stride value
of 8, CIM-NET reduces MVM operations to 1/77th of the original, while
maintaining competitive PSNR (35.11 dB vs. 35.56 dB

</details>


### [341] [UniDB++: Fast Sampling of Unified Diffusion Bridge](https://arxiv.org/abs/2505.21528)
*Mokai Pan,Kaizhen Zhu,Yuexin Ma,Yanwei Fu,Jingyi Yu,Jingya Wang,Ye Shi*

Key words: Diffusion Bridges, Stochastic Optimal Control, Image Generation, Sampling Efficiency

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: UniDB++是一种无需训练的采样算法，通过改进UniDB框架，减少了计算成本，提高了图像生成质量，尤其在低步数情况下表现出色。

Motivation: UniDB的迭代采样方法计算成本高且效率低，且现有加速技术无法解决其独特挑战，如终端均值约束缺失和SOC特定的惩罚系数。

Method: UniDB++通过推导反向时间SDE的闭式解，减少误差积累，并引入数据预测模型和SDE校正机制，提升稳定性和质量。

Result: 实验显示UniDB++在图像修复任务中性能领先，生成速度提高20倍，且低步数下仍保持高质量。

Conclusion: UniDB++填补了SOC驱动扩散桥模型的理论通用性和实际效率之间的差距。

Abstract: Diffusion Bridges enable transitions between arbitrary distributions, with
the Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image
generation via a Stochastic Optimal Control (SOC) formulation. However, UniDB's
reliance on iterative Euler sampling methods results in slow, computationally
expensive inference, while existing acceleration techniques for diffusion or
diffusion bridge models fail to address its unique challenges: missing terminal
mean constraints and SOC-specific penalty coefficients in its SDEs. We present
UniDB++, a training-free sampling algorithm that significantly improves upon
these limitations. The method's key advancement comes from deriving exact
closed-form solutions for UniDB's reverse-time SDEs, effectively reducing the
error accumulation inherent in Euler approximations and enabling high-quality
generation with up to 20$\times$ fewer sampling steps. This method is further
complemented by replacing conventional noise prediction with a more stable data
prediction model, along with an SDE-Corrector mechanism that maintains
perceptual quality for low-step regimes (5-10 steps). Additionally, we
demonstrate that UniDB++ aligns with existing diffusion bridge acceleration
methods by evaluating their update rules, and UniDB++ can recover DBIMs as
special cases under some theoretical conditions. Experiments demonstrate
UniDB++'s state-of-the-art performance in image restoration tasks,
outperforming Euler-based methods in fidelity and speed while reducing
inference time significantly. This work bridges the gap between theoretical
generality and practical efficiency in SOC-driven diffusion bridge models. Our
code is available at https://github.com/2769433owo/UniDB-plusplus.

</details>


### [342] [How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control](https://arxiv.org/abs/2505.21531)
*Kunhang Li,Jason Naradowsky,Yansong Feng,Yusuke Miyao*

Key words: Large Language Models, 3D avatar control, motion planning, human motion knowledge, spatial-temporal parameters

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 通过3D虚拟角色控制探索大型语言模型（LLM）的人类动作知识，发现尽管LLM在高层次运动解释表现良好，但在精确身体部位定位和复杂多步运动上存在困难。

Motivation: 研究LLM在人类动作生成与规划中的能力，验证其能否有效理解和生成复杂的多层次运动指令。

Method: 分两步生成动作：高层次运动规划（生成连续步骤）和低层次身体部位定位，并通过动画线性插值验证。设计了20种代表性运动指令进行评估。

Result: LLM擅长解释高层次动作，但难以精确定位身体部位；多自由度或多步运动表现较差；但对创意动作和文化特定模式有一定理解。

Conclusion: LLM在动作生成中潜力与局限并存，需进一步优化低层次运动规划能力。

Abstract: We explore Large Language Models (LLMs)' human motion knowledge through 3D
avatar control. Given a motion instruction, we prompt LLMs to first generate a
high-level movement plan with consecutive steps (High-level Planning), then
specify body part positions in each step (Low-level Planning), which we
linearly interpolate into avatar animations as a clear verification lens for
human evaluators. Through carefully designed 20 representative motion
instructions with full coverage of basic movement primitives and balanced body
part usage, we conduct comprehensive evaluations including human assessment of
both generated animations and high-level movement plans, as well as automatic
comparison with oracle positions in low-level planning. We find that LLMs are
strong at interpreting the high-level body movements but struggle with precise
body part positioning. While breaking down motion queries into atomic
components improves planning performance, LLMs have difficulty with multi-step
movements involving high-degree-of-freedom body parts. Furthermore, LLMs
provide reasonable approximation for general spatial descriptions, but fail to
handle precise spatial specifications in text, and the precise spatial-temporal
parameters needed for avatar control. Notably, LLMs show promise in
conceptualizing creative motions and distinguishing culturally-specific motion
patterns.

</details>


### [343] [EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media](https://arxiv.org/abs/2505.21532)
*Ismail Erbas,Ferhat Demirkiran,Karthik Swaminathan,Naigang Wang,Navid Ibtehaj Nizam,Stefan T. Radev,Kaoutar El Maghraoui,Xavier Intes,Vikas Pandey*

Key words: 荧光激光雷达, 专家混合框架, 散射介质, 深度估计, 荧光寿命

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种基于物理指导的专家混合（MoE）框架，用于解决荧光激光雷达（FLiDAR）在散射介质中深度和荧光寿命估计的挑战。

Motivation: 荧光激光雷达技术在复杂散射介质中面临信号解析困难，导致深度和荧光寿命估计效果不佳，亟需改进方法。

Method: 采用了物理指导的专家混合框架（EvidenceMoE），结合基于证据的Dirichlet评论家（EDCs）和决策网络，动态融合专家预测。

Result: 在模拟的FLiDAR数据中验证，深度估计的归一化均方根误差（NRMSE）为0.030，荧光寿命为0.074。

Conclusion: 该方法显著提高了FLiDAR在散射介质中的性能，为医学和工业应用提供了新工具。

Abstract: Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology
employed for distance and depth estimation across medical, automotive, and
other fields, encounters significant computational challenges in scattering
media. The complex nature of the acquired FLiDAR signal, particularly in such
environments, makes isolating photon time-of-flight (related to target depth)
and intrinsic fluorescence lifetime exceptionally difficult, thus limiting the
effectiveness of current analytical and computational methodologies. To
overcome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE)
framework tailored for specialized modeling of diverse temporal components. In
contrast to the conventional MoE approaches our expert models are informed by
underlying physics, such as the radiative transport equation governing photon
propagation in scattering media. Central to our approach is EvidenceMoE, which
integrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess
the reliability of each expert's output by providing per-expert quality scores
and corrective feedback. A Decider Network then leverages this information to
fuse expert predictions into a robust final estimate adaptively. We validate
our method using realistically simulated Fluorescence LiDAR (FLiDAR) data for
non-invasive cancer cell depth detection generated from photon transport models
in tissue. Our framework demonstrates strong performance, achieving a
normalized root mean squared error (NRMSE) of 0.030 for depth estimation and
0.074 for fluorescence lifetime.

</details>


### [344] [Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement](https://arxiv.org/abs/2505.21535)
*Yuxin Ren,Maxwell D Collins,Miao Hu,Huanrui Yang*

Key words: Transformer, 注意力机制, FAR, LSTM, 推理效率

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出FAR框架，用于替代预训练Transformer中的注意力模块，以提升推理效率，同时保持模型准确性。

Motivation: 由于注意力机制在推理时存在冗余，且边缘和嵌入式设备并行性和内存带宽有限，研究者提出用更简单的序列到序列模块（如LSTM）替代注意力模块。

Method: FAR框架通过块级蒸馏目标和全局结构剪枝，将预训练Transformer的注意力模块替换为可学习的LSTM模块。

Result: 在DeiT视觉Transformer族上验证，FAR在减少参数和延迟的同时，在ImageNet及下游任务中保持与原模型相当的准确性。

Conclusion: FAR不仅能有效提升推理效率，还能保留注意力模块学到的语义关系和相关性。

Abstract: While transformers excel across vision and language pretraining tasks, their
reliance on attention mechanisms poses challenges for inference efficiency,
especially on edge and embedded accelerators with limited parallelism and
memory bandwidth. Hinted by the observed redundancy of attention at inference
time, we hypothesize that though the model learns complicated token dependency
through pretraining, the inference-time sequence-to-sequence mapping in each
attention layer is actually ''simple'' enough to be represented with a much
cheaper function. In this work, we explore FAR, a Function-preserving Attention
Replacement framework that replaces all attention blocks in pretrained
transformers with learnable sequence-to-sequence modules, exemplified by an
LSTM. FAR optimize a multi-head LSTM architecture with a block-wise
distillation objective and a global structural pruning framework to achieve a
family of efficient LSTM-based models from pretrained transformers. We validate
FAR on the DeiT vision transformer family and demonstrate that it matches the
accuracy of the original models on ImageNet and multiple downstream tasks with
reduced parameters and latency. Further analysis shows that FAR preserves the
semantic token relationships and the token-to-token correlation learned in the
transformer's attention module.

</details>


### [345] [Caption This, Reason That: VLMs Caught in the Middle](https://arxiv.org/abs/2505.21538)
*Zihan Weng,Lucas Gomez,Taylor Whittington Webb,Pouya Bashivan*

Key words: 视觉语言模型, 认知能力, 感知, 注意力, 记忆, 微调

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文分析了视觉语言模型（VLM）在认知能力（如感知、注意力和记忆）上的表现，发现它们在空间理解和选择性注意力任务中仍存在显著差距，并提出通过生成文本描述和改进推理能力来提升性能。

Motivation: 尽管视觉语言模型在视觉理解上取得了显著进展，但在特定任务（如计数或关系推理）上仍落后于人类。研究旨在探索其认知局限性并提出改进方法。

Method: 通过一套针对感知、注意力和记忆的任务评估现有VLM（包括GPT-4o），采用视觉-文本解耦分析方法，并提出目标微调和小型VLM优化的解决方案。

Result: 研究发现模型在直接视觉推理上表现不佳，但通过生成文本描述推理能力有显著提升。微调小型VLM能改善核心认知能力，但在分布外任务上效果有限。

Conclusion: VLM在认知能力上存在明显瓶颈，尤其是同时处理感知和推理时。通过改进推理能力和微调可部分提升性能，但需进一步研究。

Abstract: Vision-Language Models (VLMs) have shown remarkable progress in visual
understanding in recent years. Yet, they still lag behind human capabilities in
specific visual tasks such as counting or relational reasoning. To understand
the underlying limitations, we adopt methodologies from cognitive science,
analyzing VLM performance along core cognitive axes: Perception, Attention, and
Memory. Using a suite of tasks targeting these abilities, we evaluate
state-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct
cognitive profiles: while advanced models approach ceiling performance on some
tasks (e.g. category identification), a significant gap persists, particularly
in tasks requiring spatial understanding or selective attention. Investigating
the source of these failures and potential methods for improvement, we employ a
vision-text decoupling analysis, finding that models struggling with direct
visual reasoning show marked improvement when reasoning over their own
generated text captions. These experiments reveal a strong need for improved
VLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed
human performance. Furthermore, we demonstrate the potential of targeted
fine-tuning on composite visual reasoning tasks and show that fine-tuning
smaller VLMs substantially improves core cognitive abilities. While this
improvement does not translate to large enhancements on challenging,
out-of-distribution benchmarks, we show broadly that VLM performance on our
datasets strongly correlates with performance on these other benchmarks. Our
work provides a detailed analysis of VLM cognitive strengths and weaknesses and
identifies key bottlenecks in simultaneous perception and reasoning while also
providing an effective and simple solution.

</details>


### [346] [Equivariant Flow Matching for Point Cloud Assembly](https://arxiv.org/abs/2505.21539)
*Ziming Wang,Nan Xue,Rebecka Jörnsten*

Key words: 点云组装，3D重建，等变求解器，流匹配，非重叠片段

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了基于流匹配模型的等变求解器Eda，用于点云组装任务，理论证明其高效性，并在非重叠情况下表现优异。

Motivation: 解决点云组装任务中完整3D形状的重建问题，尤其是在输入点云片段非重叠时的挑战。

Method: 通过流匹配模型学习等变分布的相关向量场，构建等变路径（Eda模型）以提高训练数据效率。

Result: Eda在实际数据集中表现出高度竞争力，并能处理非重叠输入片段的复杂情况。

Conclusion: Eda通过理论支持的等变方法，高效解决了点云组装问题，尤其在非重叠条件下表现突出。

Abstract: The goal of point cloud assembly is to reconstruct a complete 3D shape by
aligning multiple point cloud pieces. This work presents a novel equivariant
solver for assembly tasks based on flow matching models. We first theoretically
show that the key to learning equivariant distributions via flow matching is to
learn related vector fields. Based on this result, we propose an assembly
model, called equivariant diffusion assembly (Eda), which learns related vector
fields conditioned on the input pieces. We further construct an equivariant
path for Eda, which guarantees high data efficiency of the training process.
Our numerical results show that Eda is highly competitive on practical
datasets, and it can even handle the challenging situation where the input
pieces are non-overlapped.

</details>


### [347] [DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers](https://arxiv.org/abs/2505.21541)
*Zitong Wang,Hang Zhao,Qianyu Zhou,Xuequan Lu,Xiangtai Li,Yiren Song*

Key words: 图像分解、扩散模型、半透明层、AlphaBlend数据集、Transformer

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了DiffDecompose框架，用于从单张半透明/透明叠加图像中分解出组成层，解决了现有方法在层解耦中的挑战，并发布了AlphaBlend数据集。

Motivation: 现有图像分解方法难以处理半透明或透明层的遮挡问题，需依赖掩模先验、静态物体假设，且缺乏数据集。本文旨在解决这些挑战，实现更灵活的层分解。

Method: 提出基于扩散Transformer的DiffDecompose框架，利用上下文分解（In-Context Decomposition）和层位置编码克隆（Layer Position Encoding Cloning），无需逐层监督即可预测多层。

Result: 在AlphaBlend数据集和公开LOGO数据集上的实验验证了DiffDecompose的有效性，尤其是在半透明/透明层分解任务中表现优异。

Conclusion: DiffDecompose通过创新方法解决了层分解中的模糊性和数据稀缺问题，为实际应用提供了可扩展的解决方案。

Abstract: Diffusion models have recently motivated great success in many generation
tasks like object removal. Nevertheless, existing image decomposition methods
struggle to disentangle semi-transparent or transparent layer occlusions due to
mask prior dependencies, static object assumptions, and the lack of datasets.
In this paper, we delve into a novel task: Layer-Wise Decomposition of
Alpha-Composited Images, aiming to recover constituent layers from single
overlapped images under the condition of semi-transparent/transparent alpha
layer non-linear occlusion. To address challenges in layer ambiguity,
generalization, and data scarcity, we first introduce AlphaBlend, the first
large-scale and high-quality dataset for transparent and semi-transparent layer
decomposition, supporting six real-world subtasks (e.g., translucent flare
removal, semi-transparent cell decomposition, glassware decomposition).
Building on this dataset, we present DiffDecompose, a diffusion
Transformer-based framework that learns the posterior over possible layer
decompositions conditioned on the input image, semantic prompts, and blending
type. Rather than regressing alpha mattes directly, DiffDecompose performs
In-Context Decomposition, enabling the model to predict one or multiple layers
without per-layer supervision, and introduces Layer Position Encoding Cloning
to maintain pixel-level correspondence across layers. Extensive experiments on
the proposed AlphaBlend dataset and public LOGO dataset verify the
effectiveness of DiffDecompose. The code and dataset will be available upon
paper acceptance. Our code will be available at:
https://github.com/Wangzt1121/DiffDecompose.

</details>


### [348] [Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing](https://arxiv.org/abs/2505.21547)
*Weixing Wang,Zifeng Ding,Jindong Gu,Rui Cao,Christoph Meinel,Gerard de Melo,Haojin Yang*

Key words: 视觉语言模型、幻觉、图神经网络、共现分析、对比学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种方法，通过分析图像标记的共现关系及使用图神经网络，减少大型视觉语言模型中的幻觉现象，即生成不存在的对象。

Motivation: 大型视觉语言模型在处理视觉输入时容易产生幻觉（即生成非真实存在的对象），作者认为这可能是由于训练过程中视觉先验导致的某些标记与对象的强烈关联性。

Method: 1. 构建图像标记共现图；2. 使用图神经网络和对比学习分析标记的共现模式；3. 提出抑制视觉上不存在的标记影响的嵌入修改方法。

Result: 实验表明，该方法能有效减少幻觉现象，同时保持模型的表达能力。

Conclusion: 通过分析标记共现关系并调整潜在嵌入，可以显著降低模型幻觉现象，提升可靠性。

Abstract: Large Vision-Language Models (LVLMs) with discrete image tokenizers unify
multimodal representations by encoding visual inputs into a finite set of
tokens. Despite their effectiveness, we find that these models still
hallucinate non-existent objects. We hypothesize that this may be due to visual
priors induced during training: When certain image tokens frequently co-occur
in the same spatial regions and represent shared objects, they become strongly
associated with the verbalizations of those objects. As a result, the model may
hallucinate by evoking visually absent tokens that often co-occur with present
ones. To test this assumption, we construct a co-occurrence graph of image
tokens using a segmentation dataset and employ a Graph Neural Network (GNN)
with contrastive learning followed by a clustering method to group tokens that
frequently co-occur in similar visual contexts. We find that hallucinations
predominantly correspond to clusters whose tokens dominate the input, and more
specifically, that the visually absent tokens in those clusters show much
higher correlation with hallucinated objects compared to tokens present in the
image. Based on this observation, we propose a hallucination mitigation method
that suppresses the influence of visually absent tokens by modifying latent
image embeddings during generation. Experiments show our method reduces
hallucinations while preserving expressivity. Code is available at
https://github.com/weixingW/CGC-VTD/tree/main

</details>


### [349] [Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts](https://arxiv.org/abs/2505.21556)
*Hee-Seon Kim,Minbeom Kim,Wonjun Lee,Kihyun Kim,Changick Kim*

Key words: 对抗图像, 多模态对齐, 越狱攻击, 安全机制, 视觉语言模型

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了从良性到有毒（B2T）的越狱新范式，通过优化对抗图像在无显式有毒信号条件下诱发模型输出有毒内容，揭示了多模态对齐中的新漏洞。

Motivation: 现有基于优化的越狱方法（Toxic-Continuation）在缺乏显式有毒信号时效果不佳，因此提出B2T范式以探索更广泛的安全漏洞。

Method: 优化对抗图像，使其在良性输入条件下诱导模型生成有毒输出，无需依赖预存的有毒提示。

Result: B2T方法优于现有技术，具备黑盒迁移能力，并能与文本越狱方法互补。

Conclusion: B2T揭示了多模态对齐中未充分探索的脆弱性，为越狱研究提供了新方向。

Abstract: Optimization-based jailbreaks typically adopt the Toxic-Continuation setting
in large vision-language models (LVLMs), following the standard next-token
prediction objective. In this setting, an adversarial image is optimized to
make the model predict the next token of a toxic prompt. However, we find that
the Toxic-Continuation paradigm is effective at continuing already-toxic
inputs, but struggles to induce safety misalignment when explicit toxic signals
are absent. We propose a new paradigm: Benign-to-Toxic (B2T) jailbreak. Unlike
prior work, we optimize adversarial images to induce toxic outputs from benign
conditioning. Since benign conditioning contains no safety violations, the
image alone must break the model's safety mechanisms. Our method outperforms
prior approaches, transfers in black-box settings, and complements text-based
jailbreaks. These results reveal an underexplored vulnerability in multimodal
alignment and introduce a fundamentally new direction for jailbreak approaches.

</details>


### [350] [Analytical Calculation of Weights Convolutional Neural Network](https://arxiv.org/abs/2505.21557)
*Polad Geidarov*

Key words: 卷积神经网络, 解析计算, MNIST, 权重确定, 免训练模型

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出一种无需标准训练过程的CNN权重与阈值解析计算方法，仅需MNIST数据集中10张代表数字的图片即可确定参数。

Motivation: 探索是否可以不依赖传统训练步骤，直接通过解析方法计算CNN参数并应用于分类任务。

Method: 基于10张MNIST图片（0-9各一张），解析计算CNN各层通道数与权重/阈值，并用C++实现验证。

Result: 解析计算的CNN能在无需训练的情况下，对1000张手写数字实现过半准确率，推理耗时不到一秒。

Conclusion: 验证了纯解析方法构建可直接应用的CNN的可行性，为免训练分类模型提供新思路。

Abstract: This paper presents an algorithm for analytically calculating the weights and
thresholds of convolutional neural networks (CNNs) without using standard
training procedures. The algorithm enables the determination of CNN parameters
based on just 10 selected images from the MNIST dataset, each representing a
digit from 0 to 9. As part of the method, the number of channels in CNN layers
is also derived analytically. A software module was implemented in C++ Builder,
and a series of experiments were conducted using the MNIST dataset. Results
demonstrate that the analytically computed CNN can recognize over half of 1000
handwritten digit images without any training, achieving inference in fractions
of a second. These findings suggest that CNNs can be constructed and applied
directly for classification tasks without training, using purely analytical
computation of weights.

</details>


### [351] [A Novel Convolutional Neural Network-Based Framework for Complex Multiclass Brassica Seed Classification](https://arxiv.org/abs/2505.21558)
*Elhoucine Elfatimia,Recep Eryigitb,Lahcen Elfatimi*

Key words: 种子分类, 卷积神经网络, 深度学习, 农业研究, 质量控制

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种基于卷积神经网络（CNN）的新型框架，用于高效分类十种常见的Brassica种子，解决了种子图像纹理相似性的挑战，并在自收集数据集上达到了93%的高准确率。

Motivation: 农民因农作物生产和农场运营的繁忙缺少时间和资源进行实地研究，种子分类对于质量控制、生产效率和杂质检测至关重要，早期识别种子类型有助于降低成本和风险。

Method: 研究设计了一个定制化的CNN架构，通过调整层级配置优化分类，并与多种预训练的最先进架构进行了性能比较。

Result: 在自收集的Brassica种子数据集上，提出的模型达到了93%的高准确率。

Conclusion: 该方法有效解决了种子分类中的纹理相似性问题，为农民提供了高效的种子质量监控解决方案。

Abstract: Agricultural research has accelerated in recent years, yet farmers often lack
the time and resources for on-farm research due to the demands of crop
production and farm operations. Seed classification offers valuable insights
into quality control, production efficiency, and impurity detection. Early
identification of seed types is critical to reducing the cost and risk
associated with field emergence, which can lead to yield losses or disruptions
in downstream processes like harvesting. Seed sampling supports growers in
monitoring and managing seed quality, improving precision in determining seed
purity levels, guiding management adjustments, and enhancing yield estimations.
This study proposes a novel convolutional neural network (CNN)-based framework
for the efficient classification of ten common Brassica seed types. The
approach addresses the inherent challenge of texture similarity in seed images
using a custom-designed CNN architecture. The model's performance was evaluated
against several pre-trained state-of-the-art architectures, with adjustments to
layer configurations for optimized classification. Experimental results using
our collected Brassica seed dataset demonstrate that the proposed model
achieved a high accuracy rate of 93 percent.

</details>


### [352] [Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks](https://arxiv.org/abs/2505.21572)
*Sungwon Kim,Namkyeong Lee,Yunyoung Doh,Seungmin Shin,Guimok Cho,Seung-Won Jeon,Sangkook Kim,Chanyoung Park*

Key words: 3D网格分析，厚度感知，E(3)-等变，神经网络，工业应用

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种新型框架T-EMNN，通过考虑3D物体的厚度特性，改进了现有的基于网格的静态分析方法，且在计算效率上保持优势。

Motivation: 现有的3D网格分析方法主要关注表面拓扑和几何形状，忽略了物体厚度的内在特性及其对表面行为的显著影响，导致分析结果不够准确。

Method: 作者设计了厚度感知的E(3)-等变3D网格神经网络（T-EMNN），结合数据驱动的坐标编码方法，保持了E(3)-等变或不变性质。

Result: 在实际工业数据集上的评估表明，T-EMNN能更准确地预测节点级3D变形，有效捕捉厚度效应，同时保持计算效率。

Conclusion: T-EMNN框架通过整合厚度信息，显著提升了3D静态分析的准确性和实用性。

Abstract: Mesh-based 3D static analysis methods have recently emerged as efficient
alternatives to traditional computational numerical solvers, significantly
reducing computational costs and runtime for various physics-based analyses.
However, these methods primarily focus on surface topology and geometry, often
overlooking the inherent thickness of real-world 3D objects, which exhibits
high correlations and similar behavior between opposing surfaces. This
limitation arises from the disconnected nature of these surfaces and the
absence of internal edge connections within the mesh. In this work, we propose
a novel framework, the Thickness-aware E(3)-Equivariant 3D Mesh Neural Network
(T-EMNN), that effectively integrates the thickness of 3D objects while
maintaining the computational efficiency of surface meshes. Additionally, we
introduce data-driven coordinates that encode spatial information while
preserving E(3)-equivariance or invariance properties, ensuring consistent and
robust analysis. Evaluations on a real-world industrial dataset demonstrate the
superior performance of T-EMNN in accurately predicting node-level 3D
deformations, effectively capturing thickness effects while maintaining
computational efficiency.

</details>


### [353] [Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations of Explainable AI](https://arxiv.org/abs/2505.21589)
*Carina Newen,Luca Hinkamp,Maria Ntonti,Emmanuel Müller*

Key words: 光学幻觉, 机器学习, 视觉学习, 数据集, 偏差对齐

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种新的光学幻觉数据集，重点研究了视觉学习中的概念（如凝视方向和眼睛线索）对模型准确性的影响，并为研究人类与机器视觉之间的偏差和对齐提供了基础。

Motivation: 在自动驾驶或医疗诊断等安全关键领域，量化不确定性和识别模糊数据对机器学习算法至关重要。光学幻觉是研究人类和机器感知局限性的重要领域，但相关数据集稀缺。

Method: 通过系统生成包含动物对的光学幻觉数据集，研究视觉概念（如凝视方向和眼睛线索）对模型性能的影响，并公开数据集和源代码。

Result: 研究发现，视觉概念（如凝视方向和眼睛线索）对模型准确性有显著影响，突出了视觉学习中概念的重要性。

Conclusion: 该研究为理解机器视觉中的偏差和对齐提供了新视角，并通过公开数据集和代码推动了进一步研究。

Abstract: From uncertainty quantification to real-world object detection, we recognize
the importance of machine learning algorithms, particularly in safety-critical
domains such as autonomous driving or medical diagnostics. In machine learning,
ambiguous data plays an important role in various machine learning domains.
Optical illusions present a compelling area of study in this context, as they
offer insight into the limitations of both human and machine perception.
Despite this relevance, optical illusion datasets remain scarce. In this work,
we introduce a novel dataset of optical illusions featuring intermingled animal
pairs designed to evoke perceptual ambiguity. We identify generalizable visual
concepts, particularly gaze direction and eye cues, as subtle yet impactful
features that significantly influence model accuracy. By confronting models
with perceptual ambiguity, our findings underscore the importance of concepts
in visual learning and provide a foundation for studying bias and alignment
between human and machine vision. To make this dataset useful for general
purposes, we generate optical illusions systematically with different concepts
discussed in our bias mitigation section. The dataset is accessible in Kaggle
via
https://kaggle.com/datasets/693bf7c6dd2cb45c8a863f9177350c8f9849a9508e9d50526e2ffcc5559a8333.
Our source code can be found at
https://github.com/KDD-OpenSource/Ambivision.git.

</details>


### [354] [Any-to-Bokeh: One-Step Video Bokeh via Multi-Plane Image Guided Diffusion](https://arxiv.org/abs/2505.21593)
*Yang Yang,Siming Zheng,Jinwei Chen,Boxi Wu,Xiaofei He,Deng Cai,Bo Li,Peng-Tao Jiang*

Key words: 视频编辑, 背景虚化, 扩散模型, 多平面图像, 时间一致性

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种新的单步视频背景虚化框架，通过多平面图像（MPI）表示和预训练模型的3D先验，解决了现有视频编辑模型在焦点控制和时间一致性上的不足。

Motivation: 现有视频编辑方法无法明确控制焦点平面或调整虚化强度，且图像方法扩展到视频时会出现时间闪烁和边缘模糊过渡问题，需要更高效且一致性的解决方案。

Method: 采用MPI表示，结合渐进加宽的深度采样函数和单步视频扩散模型，利用预训练模型（如Stable Video Diffusion）的3D先验实现深度感知的虚化效果。

Result: 实验表明，该方法能够生成高质量、可控的虚化效果，并在多个评估基准上达到最先进性能。

Conclusion: 提出的框架成功解决了视频背景虚化的时间一致性和深度控制问题，为光学效果编辑提供了新思路。

Abstract: Recent advances in diffusion based editing models have enabled realistic
camera simulation and image-based bokeh, but video bokeh remains largely
unexplored. Existing video editing models cannot explicitly control focus
planes or adjust bokeh intensity, limiting their applicability for controllable
optical effects. Moreover, naively extending image-based bokeh methods to video
often results in temporal flickering and unsatisfactory edge blur transitions
due to the lack of temporal modeling and generalization capability. To address
these challenges, we propose a novel one-step video bokeh framework that
converts arbitrary input videos into temporally coherent, depth-aware bokeh
effects. Our method leverages a multi-plane image (MPI) representation
constructed through a progressively widening depth sampling function, providing
explicit geometric guidance for depth-dependent blur synthesis. By conditioning
a single-step video diffusion model on MPI layers and utilizing the strong 3D
priors from pre-trained models such as Stable Video Diffusion, our approach
achieves realistic and consistent bokeh effects across diverse scenes.
Additionally, we introduce a progressive training strategy to enhance temporal
consistency, depth robustness, and detail preservation. Extensive experiments
demonstrate that our method produces high-quality, controllable bokeh effects
and achieves state-of-the-art performance on multiple evaluation benchmarks.

</details>


### [355] [OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions](https://arxiv.org/abs/2505.21724)
*Cheng Luo,Jianghui Wang,Bing Li,Siyang Song,Bernard Ghanem*

Key words: OMCRG, 多模态, 对话响应, OmniResponse, ResponseNet

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文介绍了在线多模态对话响应生成（OMCRG）任务及其解决方案OmniResponse，通过引入文本作为中间模态来同步生成高质量的多模态听众反馈，并通过新数据集ResponseNet验证了其性能。

Motivation: OMCRG任务旨在模拟自然对话中的多模态交互，但音频和面部表情的同步生成具有挑战性，因此需要一种创新的方法来提升同步性和生成质量。

Method: 提出了OmniResponse模型，利用预训练的大型语言模型（LLM）和两个新组件：Chrono-Text（时间锚定文本标记）和TempoVoice（可控在线TTS模块），以同步生成音频和面部反应。

Result: 在ResponseNet数据集上的实验表明，OmniResponse在语义内容、视听同步和生成质量方面显著优于基线模型。

Conclusion: OmniResponse通过多模态同步生成方法为OMCRG任务提供了高效且高质量的解决方案，并为未来研究提供了新的数据集和基准。

Abstract: In this paper, we introduce Online Multimodal Conversational Response
Generation (OMCRG), a novel task that aims to online generate synchronized
verbal and non-verbal listener feedback, conditioned on the speaker's
multimodal input. OMCRG reflects natural dyadic interactions and poses new
challenges in achieving synchronization between the generated audio and facial
responses of the listener. To address these challenges, we innovatively
introduce text as an intermediate modality to bridge the audio and facial
responses. We hence propose OmniResponse, a Multimodal Large Language Model
(MLLM) that autoregressively generates high-quality multi-modal listener
responses. OmniResponse leverages a pretrained LLM enhanced with two novel
components: Chrono-Text, which temporally anchors generated text tokens, and
TempoVoice, a controllable online TTS module that produces speech synchronized
with facial reactions. To support further OMCRG research, we present
ResponseNet, a new dataset comprising 696 high-quality dyadic interactions
featuring synchronized split-screen videos, multichannel audio, transcripts,
and facial behavior annotations. Comprehensive evaluations conducted on
ResponseNet demonstrate that OmniResponse significantly outperforms baseline
models in terms of semantic speech content, audio-visual synchronization, and
generation quality.

</details>


### [356] [Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance](https://arxiv.org/abs/2505.21544)
*Semanto Mondal*

Key words: 精准农业, 检索增强生成(RAG), 对象检测, 大语言模型, 咖啡叶疾病

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种结合对象检测、大语言模型(LLM)和检索增强生成(RAG)的混合AI框架，用于实时检测咖啡叶疾病并提供环保的处理建议。

Motivation: 传统农业实践效率低且对环境影响较大，精准农业技术可通过AI优化资源利用。本文旨在解决LLM的幻觉问题，并提供实时、上下文感知的作物疾病诊断方法。

Method: 采用混合AI框架，结合YOLOv8进行图像检测、NLP处理语言输入，并利用RAG生成上下文相关的诊断与建议，实现低农药使用的环保种植方案。

Result: 系统能准确检测咖啡叶疾病，提供个性化处理方案，并降低农药依赖，兼顾环境友好性与农民易用性。

Conclusion: 该框架为精准农业提供了可扩展、可靠的解决方案，未来可扩展至更广泛的农业应用。

Abstract: As a social being, we have an intimate bond with the environment. A plethora
of things in human life, such as lifestyle, health, and food are dependent on
the environment and agriculture. It comes under our responsibility to support
the environment as well as agriculture. However, traditional farming practices
often result in inefficient resource use and environmental challenges. To
address these issues, precision agriculture has emerged as a promising approach
that leverages advanced technologies to optimise agricultural processes. In
this work, a hybrid approach is proposed that combines the three different
potential fields of model AI: object detection, large language model (LLM), and
Retrieval-Augmented Generation (RAG). In this novel framework, we have tried to
combine the vision and language models to work together to identify potential
diseases in the tree leaf. This study introduces a novel AI-based precision
agriculture system that uses Retrieval Augmented Generation (RAG) to provide
context-aware diagnoses and natural language processing (NLP) and YOLOv8 for
crop disease detection. The system aims to tackle major issues with large
language models (LLMs), especially hallucinations and allows for adaptive
treatment plans and real-time disease detection. The system provides an
easy-to-use interface to the farmers, which they can use to detect the
different diseases related to coffee leaves by just submitting the image of the
affected leaf the model will detect the diseases as well as suggest potential
remediation methodologies which aim to lower the use of pesticides, preserving
livelihoods, and encouraging environmentally friendly methods. With an emphasis
on scalability, dependability, and user-friendliness, the project intends to
improve RAG-integrated object detection systems for wider agricultural
applications in the future.

</details>


### [357] [Learning to See More: UAS-Guided Super-Resolution of Satellite Imagery for Precision Agriculture](https://arxiv.org/abs/2505.21746)
*Arif Masrur,Peder A. Olsen,Paul R. Adler,Carlan Jackson,Matthew W. Myers,Nathan Sedghi,Ray R. Weil*

Key words: 精准农业, 无人机, 卫星影像, 超分辨率, 生物量估计

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本研究提出了一种融合卫星和无人机(UAS)影像的超分辨率方法，以低成本结合两者的优势，显著提高了农作物生物量和氮含量的估计精度。

Motivation: 卫星和无人机在精准农业中各具优缺点，卫星覆盖范围广但分辨率低，无人机分辨率高但成本高且覆盖有限。本研究旨在通过融合两者数据，弥补各自的不足。

Method: 采用超分辨率方法融合卫星和无人机影像，将无人机RGB数据光谱扩展到植被红边和近红外区域，生成高分辨率Sentinel-2影像。

Result: 通过该方法，生物量和氮含量的估计精度分别提高了18%和31%，且无需频繁无人机飞行。

Conclusion: 该方法轻量且可扩展，适用于农场低成本应用，并在不同作物系统中具有可迁移性。

Abstract: Unmanned Aircraft Systems (UAS) and satellites are key data sources for
precision agriculture, yet each presents trade-offs. Satellite data offer broad
spatial, temporal, and spectral coverage but lack the resolution needed for
many precision farming applications, while UAS provide high spatial detail but
are limited by coverage and cost, especially for hyperspectral data. This study
presents a novel framework that fuses satellite and UAS imagery using
super-resolution methods. By integrating data across spatial, spectral, and
temporal domains, we leverage the strengths of both platforms cost-effectively.
We use estimation of cover crop biomass and nitrogen (N) as a case study to
evaluate our approach. By spectrally extending UAS RGB data to the vegetation
red edge and near-infrared regions, we generate high-resolution Sentinel-2
imagery and improve biomass and N estimation accuracy by 18% and 31%,
respectively. Our results show that UAS data need only be collected from a
subset of fields and time points. Farmers can then 1) enhance the spectral
detail of UAS RGB imagery; 2) increase the spatial resolution by using
satellite data; and 3) extend these enhancements spatially and across the
growing season at the frequency of the satellite flights. Our SRCNN-based
spectral extension model shows considerable promise for model transferability
over other cropping systems in the Upper and Lower Chesapeake Bay regions.
Additionally, it remains effective even when cloud-free satellite data are
unavailable, relying solely on the UAS RGB input. The spatial extension model
produces better biomass and N predictions than models built on raw UAS RGB
images. Once trained with targeted UAS RGB data, the spatial extension model
allows farmers to stop repeated UAS flights. While we introduce
super-resolution advances, the core contribution is a lightweight and scalable
system for affordable on-farm use.

</details>


### [358] [Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation](https://arxiv.org/abs/2505.21549)
*Daniel Csizmadia,Andrei Codreanu,Victor Sim,Vighnesh Prabeau,Michael Lu,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Key words: DCLIP, CLIP, 多模态检索, 蒸馏框架, 零样本分类

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: DCLIP通过元教师-学生蒸馏框架增强CLIP模型的多模态图像-文本检索能力，同时保留其零样本分类性能。

Motivation: 解决CLIP模型在细粒度跨模态理解任务中因固定图像分辨率和有限上下文而受限的问题。

Method: 采用双向跨注意力机制的跨模态Transformer教师模型生成丰富嵌入，指导轻量级学生模型的训练，结合对比学习和余弦相似度目标。

Result: DCLIP在少量样本训练下显著提升检索指标（Recall@K, MAP），并保留94%的零样本分类性能。

Conclusion: DCLIP有效平衡任务专业化和泛化能力，为视觉-语言任务提供高效、适应性强且细节敏感的解决方案。

Abstract: We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that
enhances multimodal image-text retrieval while preserving the original model's
strong zero-shot classification capabilities. CLIP models are typically
constrained by fixed image resolutions and limited context, which can hinder
their effectiveness in retrieval tasks that require fine-grained cross-modal
understanding. DCLIP addresses these challenges through a meta teacher-student
distillation framework, where a cross-modal transformer teacher is fine-tuned
to produce enriched embeddings via bidirectional cross-attention between
YOLO-extracted image regions and corresponding textual spans. These
semantically and spatially aligned global representations guide the training of
a lightweight student model using a hybrid loss that combines contrastive
learning and cosine similarity objectives. Despite being trained on only
~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a
fraction of CLIP's original dataset-DCLIP significantly improves image-text
retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's
zero-shot classification performance. These results demonstrate that DCLIP
effectively mitigates the trade-off between task specialization and
generalization, offering a resource-efficient, domain-adaptive, and
detail-sensitive solution for advanced vision-language tasks. Code available at
https://anonymous.4open.science/r/DCLIP-B772/README.md.

</details>


### [359] [FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering](https://arxiv.org/abs/2505.21755)
*Chengyue Huang,Brisa Maneechotesuwan,Shivang Chopra,Zsolt Kira*

Key words: 视觉问答, 鲁棒微调, 多模态, 分布偏移, 基准评估

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一个新的基准FRAMES-VQA，用于评估视觉问答（VQA）任务中的鲁棒微调方法。通过整合多个现有数据集并分类为不同分布偏移，论文比较了现有方法，并分析了模态重要性。

Motivation: 当前的VQA系统在适应多模态数据偏移时存在局限性，尤其是针对真实世界的复杂场景。缺乏全面的评估基准来研究多模态偏移对鲁棒微调的影响。

Method: 提出FRAMES-VQA基准，整合10个现有数据集，分为ID、近OOD和远OOD类别。采用马氏距离量化分布偏移，分析单模态与多模态偏移的交互及模态重要性。

Result: 通过实验比较现有方法，量化了不同模态偏移的影响，提供了关于模态重要性的分析，为设计更鲁棒的VQA微调方法提供了指导。

Conclusion: FRAMES-VQA为研究多模态分布偏移下的鲁棒微调提供了全面基准，实验分析揭示了模态重要性与偏移类型的关系，促进未来更鲁棒的VQA系统开发。

Abstract: Visual question answering (VQA) systems face significant challenges when
adapting to real-world data shifts, especially in multi-modal contexts. While
robust fine-tuning strategies are essential for maintaining performance across
in-distribution (ID) and out-of-distribution (OOD) scenarios, current
evaluation settings are primarily unimodal or particular to some types of OOD,
offering limited insight into the complexities of multi-modal contexts. In this
work, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across
Multi-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We
utilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA
and others, and categorize them into ID, near and far OOD datasets covering
uni-modal, multi-modal and adversarial distribution shifts. We first conduct a
comprehensive comparison of existing robust fine-tuning methods. We then
quantify the distribution shifts by calculating the Mahalanobis distance using
uni-modal and multi-modal embeddings extracted from various models. Further, we
perform an extensive analysis to explore the interactions between uni- and
multi-modal shifts as well as modality importance for ID and OOD samples. These
analyses offer valuable guidance on developing more robust fine-tuning methods
to handle multi-modal distribution shifts. The code is available at
https://github.com/chengyuehuang511/FRAMES-VQA .

</details>


### [360] [MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning](https://arxiv.org/abs/2505.21771)
*Prasham Yatinkumar Titiya,Jainil Trivedi,Chitta Baral,Vivek Gupta*

Key words: 多模态表格,视觉语言模型,基准测试,真实世界任务,推理能力

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 这篇论文提出了一个名为MMTBENCH的多模态表格基准测试，包含500个真实世界中的多模态表格和4021个问题-答案对，用于评估当前视觉语言模型在处理复杂多模态表格时的性能。

Motivation: 当前视觉语言模型在文本和图像理解方面表现出色，但在处理复杂的真实世界多模态表格任务时的性能尚未被充分探索，因此需要一个专门的基准来填补这一空白。

Method: 研究者们从多样化的真实世界来源中收集了500个多模态表格，并设计了4021个涵盖四种问题类型、五种推理类型和八种表格类型的问题-答案对。

Result: 实验表明，现有的先进模型在多模态表格任务上表现不佳，特别是在需要视觉推理和多步推理的问题上存在显著的性能差距。

Conclusion: 该研究强调了未来研究中迫切需要改进视觉与语言处理紧密结合的模型架构，以提升多模态表格理解的性能。

Abstract: Multimodal tables those that integrate semi structured data with visual
elements such as charts and maps are ubiquitous across real world domains, yet
they pose a formidable challenge to current vision language models (VLMs).
While Large Language models (LLMs) and VLMs have demonstrated strong
capabilities in text and image understanding, their performance on complex,
real world multimodal table reasoning remains unexplored. To bridge this gap,
we introduce MMTBENCH (Multimodal Table Benchmark), a benchmark consisting of
500 real world multimodal tables drawn from diverse real world sources, with a
total of 4021 question answer pairs. MMTBENCH questions cover four question
types (Explicit, Implicit, Answer Mention, and Visual Based), five reasoning
types (Mathematical, Extrema Identification, Fact Verification, Vision Based,
and Others), and eight table types (Single/Multiple Entity, Maps and Charts
with Entities, Single/Multiple Charts, Maps, and Visualizations). Extensive
evaluation of state of the art models on all types reveals substantial
performance gaps, particularly on questions requiring visual-based reasoning
and multi-step inference. These findings show the urgent need for improved
architectures that more tightly integrate vision and language processing. By
providing a challenging, high-quality resource that mirrors the complexity of
real-world tasks, MMTBENCH underscores its value as a resource for future
research on multimodal tables.

</details>


### [361] [GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning](https://arxiv.org/abs/2505.21863)
*Shikhhar Siingh,Abhinav Rawat,Vivek Gupta,Chitta Baral*

Key words: GETReason, GREAT, 图像理解, 时空推理, 多智能体

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: GETReason框架通过提取全球事件、时空信息提升图像意义理解，并引入GREAT评估指标。多层次多智能体方法表明，可有效将图像与事件背景关联。

Motivation: 解决现有方法难以准确提取图像深层上下文信息的问题。

Method: 提出GETReason框架，结合全球事件、时空信息；引入GREAT评估指标；采用多层次多智能体方法。

Result: 实验验证了GETReason框架能有效推断图像深层意义，并成功将其与事件背景关联。

Conclusion: GETReason能显著提升图像背景理解，为新闻和教育领域提供更准确的分析工具。

Abstract: Publicly significant images from events hold valuable contextual information,
crucial for journalism and education. However, existing methods often struggle
to extract this relevance accurately. To address this, we introduce GETReason
(Geospatial Event Temporal Reasoning), a framework that moves beyond
surface-level image descriptions to infer deeper contextual meaning. We propose
that extracting global event, temporal, and geospatial information enhances
understanding of an image's significance. Additionally, we introduce GREAT
(Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric
for evaluating reasoning-based image understanding. Our layered multi-agent
approach, assessed using a reasoning-weighted metric, demonstrates that
meaningful insights can be inferred, effectively linking images to their
broader event context.

</details>


### [362] [RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers](https://arxiv.org/abs/2505.21847)
*Xuwei Xu,Yang Li,Yudong Chen,Jiajun Liu,Sen Wang*

Key words: Vision Transformer, FFN layers, structural reparameterization, inference latency, efficiency optimization

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文发现前馈网络（FFN）层是视觉变换器（ViT）推理延迟的主要来源，并提出了一种通道空闲机制，通过后训练结构重参数化优化FFN层，显著降低了延迟并可能提升准确性。

Motivation: 研究发现FFN层而非注意力层是ViT推理延迟的主要瓶颈，尤其在模型规模增大时影响更显著，因此提出优化FFN层效率的解决方案。

Method: 提出通道空闲机制，允许部分特征通道绕过非线性激活函数，形成线性路径以实现推理时的结构重参数化，从而构建可重参数化ViT（RePaViT）。

Result: RePaViT在不同规模的ViT上实现了显著的延迟降低（如大型和超大型模型分别提速66.8%和68.7%），且准确率有小幅提升（+1.7%和+1.1%）。

Conclusion: RePaViT首次通过FFN层结构重参数化加速ViT，为高效ViT设计提供了新方向，其优势随模型规模扩大而增强。

Abstract: We reveal that feedforward network (FFN) layers, rather than attention
layers, are the primary contributors to Vision Transformer (ViT) inference
latency, with their impact signifying as model size increases. This finding
highlights a critical opportunity for optimizing the efficiency of large-scale
ViTs by focusing on FFN layers. In this work, we propose a novel channel idle
mechanism that facilitates post-training structural reparameterization for
efficient FFN layers during testing. Specifically, a set of feature channels
remains idle and bypasses the nonlinear activation function in each FFN layer,
thereby forming a linear pathway that enables structural reparameterization
during inference. This mechanism results in a family of ReParameterizable
Vision Transformers (RePaViTs), which achieve remarkable latency reductions
with acceptable sacrifices (sometimes gains) in accuracy across various ViTs.
The benefits of our method scale consistently with model sizes, demonstrating
greater speed improvements and progressively narrowing accuracy gaps or even
higher accuracies on larger models. In particular, RePa-ViT-Large and
RePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1
accuracies under the same training strategy, respectively. RePaViT is the first
to employ structural reparameterization on FFN layers to expedite ViTs to our
best knowledge, and we believe that it represents an auspicious direction for
efficient ViTs. Source code is available at
https://github.com/Ackesnal/RePaViT.

</details>


### [363] [Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task](https://arxiv.org/abs/2505.21850)
*Yanbei Jiang,Yihao Ding,Chao Lei,Jiayang Ao,Jey Han Lau,Krista A. Ehinger*

Key words: 多模态大语言模型, 抽象视觉推理, 多阶段基准, 评估指标

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文针对多模态大语言模型（MLLMs）在抽象视觉推理（AVR）中的不足，提出了多阶段AVR基准MultiStAR和新评估指标MSEval，以更全面评估中间步骤的正确性。实验表明MLLMs在复杂规则检测阶段仍存在挑战。

Motivation: 现有AVR基准仅关注单步推理和最终结果，忽视了推理过程的多阶段性和中间步骤的正确性，MLLMs在此类任务中表现不佳但失败原因不明。

Method: 引入MultiStAR基准（基于RAVEN）评估多阶段推理能力，并提出新指标MSEval评估中间步骤与最终结果的正确性。

Result: 实验表明，现有MLLMs在基础感知任务表现尚可，但在复杂规则检测阶段仍有困难。

Conclusion: 通过MultiStAR和MSEval，揭示了MLLMs在AVR任务中的局限性，未来需进一步提升复杂推理能力。

Abstract: Current Multimodal Large Language Models (MLLMs) excel in general visual
reasoning but remain underexplored in Abstract Visual Reasoning (AVR), which
demands higher-order reasoning to identify abstract rules beyond simple
perception. Existing AVR benchmarks focus on single-step reasoning, emphasizing
the end result but neglecting the multi-stage nature of reasoning process. Past
studies found MLLMs struggle with these benchmarks, but it doesn't explain how
they fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR
benchmark, based on RAVEN, designed to assess reasoning across varying levels
of complexity. Additionally, existing metrics like accuracy only focus on the
final outcomes while do not account for the correctness of intermediate steps.
Therefore, we propose a novel metric, MSEval, which considers the correctness
of intermediate steps in addition to the final outcomes. We conduct
comprehensive experiments on MultiStAR using 17 representative close-source and
open-source MLLMs. The results reveal that while existing MLLMs perform
adequately on basic perception tasks, they continue to face challenges in more
complex rule detection stages.

</details>


### [364] [Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation](https://arxiv.org/abs/2505.21956)
*Mengdan Zhu,Senhao Cheng,Guangji Bai,Yifei Zhang,Liang Zhao*

Key words: 文本到图像生成, 检索增强生成, 跨模态, 稀疏检索, 密集检索

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种跨模态检索增强生成框架（Cross-modal RAG），通过将查询和图像分解为子维度组件，结合稀疏和密集检索策略，显著提升了复杂查询下的图像生成质量。

Motivation: 现有的检索增强生成方法在复杂查询时表现不佳，因为单个图像通常无法涵盖所有所需元素。本文旨在解决这一问题，通过分解查询和图像，实现更精准的检索和生成。

Method: 提出了跨模态RAG框架，结合稀疏和密集检索策略，分解查询和图像为子维度组件，使用多模态大语言模型选择性生成图像。

Result: 在多个数据集（MS-COCO、Flickr30K等）上，方法在检索和生成质量上显著优于现有基线，且效率较高。

Conclusion: 跨模态RAG通过分解和混合检索策略，有效解决了复杂查询下的图像生成问题，具备实用性和高效性。

Abstract: Text-to-image generation increasingly demands access to domain-specific,
fine-grained, and rapidly evolving knowledge that pretrained models cannot
fully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to
address this by retrieving globally relevant images, but they fail when no
single image contains all desired elements from a complex user query. We
propose Cross-modal RAG, a novel framework that decomposes both queries and
images into sub-dimensional components, enabling subquery-aware retrieval and
generation. Our method introduces a hybrid retrieval strategy - combining a
sub-dimensional sparse retriever with a dense retriever - to identify a
Pareto-optimal set of images, each contributing complementary aspects of the
query. During generation, a multimodal large language model is guided to
selectively condition on relevant visual features aligned to specific
subqueries, ensuring subquery-aware image synthesis. Extensive experiments on
MS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal
RAG significantly outperforms existing baselines in both retrieval and
generation quality, while maintaining high efficiency.

</details>


### [365] [Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification](https://arxiv.org/abs/2505.21854)
*Jun Chen,Xinke Li,Mingyue Xu,Tianrui Li,Chongshou Li*

Key words: 梯度攻击, 点云分类, 对抗样本, 加权梯度, 自适应步长

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出两种新策略（WAAttack和SubAttack），改进梯度对抗攻击，通过加权梯度和自适应步长策略提升攻击效果与隐蔽性，实验证明优于现有方法。

Motivation: 现有梯度对抗攻击方法忽略点云的异构性，导致扰动过大且易察觉，需改进梯度更新机制以实现更隐蔽有效的攻击。

Method: 1. WAAttack：引入加权梯度和自适应步长策略，动态调整更新；2. SubAttack：分解点云并聚焦关键区域。

Result: 实验表明，该方法生成的对抗样本隐蔽性更强，优于现有基线。

Conclusion: 通过重新设计梯度更新机制，显著提升了点云对抗攻击的效果与隐蔽性。

Abstract: Gradient-based adversarial attacks have become a dominant approach for
evaluating the robustness of point cloud classification models. However,
existing methods often rely on uniform update rules that fail to consider the
heterogeneous nature of point clouds, resulting in excessive and perceptible
perturbations. In this paper, we rethink the design of gradient-based attacks
by analyzing the limitations of conventional gradient update mechanisms and
propose two new strategies to improve both attack effectiveness and
imperceptibility. First, we introduce WAAttack, a novel framework that
incorporates weighted gradients and an adaptive step-size strategy to account
for the non-uniform contribution of points during optimization. This approach
enables more targeted and subtle perturbations by dynamically adjusting updates
according to the local structure and sensitivity of each point. Second, we
propose SubAttack, a complementary strategy that decomposes the point cloud
into subsets and focuses perturbation efforts on structurally critical regions.
Together, these methods represent a principled rethinking of gradient-based
adversarial attacks for 3D point cloud classification. Extensive experiments
demonstrate that our approach outperforms state-of-the-art baselines in
generating highly imperceptible adversarial examples. Code will be released
upon paper acceptance.

</details>


### [366] [Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language](https://arxiv.org/abs/2505.22146)
*Guangfu Hao,Haojie Wen,Liangxuna Guo,Yang Chen,Yanchao Bi,Shan Yu*

Key words: 工具选择, 多模态学习, 认知建模, 视觉-语言融合, 低维表示

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种利用低维属性表示框架（ToolNet）连接视觉工具感知与语言任务理解的模型，显著提升了工具选择任务的准确率，接近大型模型性能但参数更少。

Motivation: 研究旨在填补计算模型在模拟人类复杂工具选择能力上的不足，通过结合视觉与语言信息提升工具选择的准确性和可解释性。

Method: 采用视觉编码器（ResNet或ViT）从工具图像提取属性，结合微调的语言模型（GPT-2、LLaMA、DeepSeek）从任务描述中获取需求属性，构建低维属性表示框架。

Result: 提出的方法在工具选择任务中达到74%的准确率，显著优于直接匹配（20%）和小型多模态模型（21%-58%），且参数规模远小于性能相近的GPT-4o（73%）。

Conclusion: 通过属性驱动的多模态建模实现了高效、可解释的工具选择，不仅推动了认知科学理解，也为实际应用提供了实用解决方案。

Abstract: Flexible tool selection reflects a complex cognitive ability that
distinguishes humans from other species, yet computational models that capture
this ability remain underdeveloped. We developed a framework using
low-dimensional attribute representations to bridge visual tool perception and
linguistic task understanding. We constructed a comprehensive dataset (ToolNet)
containing 115 common tools labeled with 13 carefully designed attributes
spanning physical, functional, and psychological properties, paired with
natural language scenarios describing tool usage. Visual encoders (ResNet or
ViT) extract attributes from tool images while fine-tuned language models
(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our
approach achieves 74% accuracy in tool selection tasks-significantly
outperforming direct tool matching (20%) and smaller multimodal models
(21%-58%), while approaching performance of much larger models like GPT-4o
(73%) with substantially fewer parameters. Ablation studies revealed that
manipulation-related attributes (graspability, hand-relatedness, elongation)
consistently prove most critical across modalities. This work provides a
parameter-efficient, interpretable solution that mimics human-like tool
cognition, advancing both cognitive science understanding and practical
applications in tool selection tasks.

</details>


### [367] [Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging](https://arxiv.org/abs/2505.22150)
*Runze Xia,Shuo Feng,Renzhi Wang,Congchi Yin,Xuyun Wen,Piji Li*

Key words: 脑到图像重建, 细粒度文本, 语义一致性, 视觉语言模型, fMRI

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种名为FgB2I的细粒度脑到图像重建方法，通过利用细粒度文本作为桥梁，提升图像重建的细节和语义一致性。

Motivation: 现有的脑到图像重建方法常因缺乏足够的语义信息而丢失细节或语义一致性差。

Method: FgB2I分为三个关键阶段：细节增强、细粒度文本描述解码及基于文本的脑到图像重建。利用大视觉语言模型生成细粒度标题，并通过三个奖励指标（对象准确性、文本-图像语义相似性、图像-图像语义相似性）指导解码。

Result: 细粒度文本描述可整合到现有重建方法中，实现细粒度脑到图像重建。

Conclusion: FgB2I方法显著提升了重建图像的细节和语义一致性。

Abstract: Brain-to-Image reconstruction aims to recover visual stimuli perceived by
humans from brain activity. However, the reconstructed visual stimuli often
missing details and semantic inconsistencies, which may be attributed to
insufficient semantic information. To address this issue, we propose an
approach named Fine-grained Brain-to-Image reconstruction (FgB2I), which
employs fine-grained text as bridge to improve image reconstruction. FgB2I
comprises three key stages: detail enhancement, decoding fine-grained text
descriptions, and text-bridged brain-to-image reconstruction. In the
detail-enhancement stage, we leverage large vision-language models to generate
fine-grained captions for visual stimuli and experimentally validate its
importance. We propose three reward metrics (object accuracy, text-image
semantic similarity, and image-image semantic similarity) to guide the language
model in decoding fine-grained text descriptions from fMRI signals. The
fine-grained text descriptions can be integrated into existing reconstruction
methods to achieve fine-grained Brain-to-Image reconstruction.

</details>


### [368] [EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance](https://arxiv.org/abs/2505.21876)
*Zun Wang,Jaemin Cho,Jialu Li,Han Lin,Jaehong Yoon,Yue Zhang,Mohit Bansal*

Key words: 3D相机控制, 视频扩散模型, 锚视频, Anchor-ControlNet, 零样本泛化

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: EPiC是一种高效的3D相机控制框架，通过基于第一帧可见性的源视频掩码自动构建高质量锚视频，无需昂贵相机轨迹标注，结合轻量级Anchor-ControlNet模块，显著减少训练参数和数据需求，实现精确相机控制。

Motivation: 现有方法依赖点云估计和相机轨迹标注，易因估计误差导致锚视频不准确且资源消耗大，EPiC旨在通过无标注的高质量锚视频和轻量模块解决这些问题。

Method: 利用第一帧可见性掩码生成精确锚视频训练数据，并设计Anchor-ControlNet模块集成锚视频引导至预训练VDMs，仅需1%主干参数。

Result: 在RealEstate10K和MiraData上实现SOTA性能，定量定性均展示精确相机控制能力，并具备零样本泛化到视频到视频任务的能力。

Conclusion: EPiC以高效数据和模型设计克服渲染错位问题，为3D相机控制提供低成本、高精度的解决方案。

Abstract: Recent approaches on 3D camera control in video diffusion models (VDMs) often
create anchor videos to guide diffusion models as a structured prior by
rendering from estimated point clouds following annotated camera trajectories.
However, errors inherent in point cloud estimation often lead to inaccurate
anchor videos. Moreover, the requirement for extensive camera trajectory
annotations further increases resource demands. To address these limitations,
we introduce EPiC, an efficient and precise camera control learning framework
that automatically constructs high-quality anchor videos without expensive
camera trajectory annotations. Concretely, we create highly precise anchor
videos for training by masking source videos based on first-frame visibility.
This approach ensures high alignment, eliminates the need for camera trajectory
annotations, and thus can be readily applied to any in-the-wild video to
generate image-to-video (I2V) training pairs. Furthermore, we introduce
Anchor-ControlNet, a lightweight conditioning module that integrates anchor
video guidance in visible regions to pretrained VDMs, with less than 1% of
backbone model parameters. By combining the proposed anchor video data and
ControlNet module, EPiC achieves efficient training with substantially fewer
parameters, training steps, and less data, without requiring modifications to
the diffusion model backbone typically needed to mitigate rendering
misalignments. Although being trained on masking-based anchor videos, our
method generalizes robustly to anchor videos made with point clouds during
inference, enabling precise 3D-informed camera control. EPiC achieves SOTA
performance on RealEstate10K and MiraData for I2V camera control task,
demonstrating precise and robust camera control ability both quantitatively and
qualitatively. Notably, EPiC also exhibits strong zero-shot generalization to
video-to-video scenarios.

</details>


### [369] [Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation](https://arxiv.org/abs/2505.22222)
*Yunsoo Kim,Jinge Wu,Su-Hwan Kim,Pardeep Vasudev,Jiashu Shen,Honghan Wu*

Key words: 多模态大语言模型、胸部X光片、放射学报告、上下文学习、幻觉减少

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出Look & Mark (L&M)策略，通过整合放射科医生的视觉注视和标注框来提升多模态大语言模型在胸部X光片报告生成中的准确性和可靠性，减少幻觉和临床错误。

Motivation: 现有多模态大语言模型在医学图像分析中存在幻觉和临床错误，限制了其在实际应用中的可靠性。

Method: 采用L&M策略，结合放射科医生的视觉注视（Look）和标注框（Mark），通过上下文学习提升模型性能，无需重新训练。

Result: L&M显著提升了模型性能，如CXR-LLaVA整体指标提升1.2%，LLaVA-Med提升9.2%，并能减少临床错误（平均每份报告减少0.43个错误）。

Conclusion: L&M是一种可扩展且高效的解决方案，有助于提升AI辅助放射学的诊断工作流程，特别适用于资源有限的临床环境。

Abstract: Recent advancements in multimodal Large Language Models (LLMs) have
significantly enhanced the automation of medical image analysis, particularly
in generating radiology reports from chest X-rays (CXR). However, these models
still suffer from hallucinations and clinically significant errors, limiting
their reliability in real-world applications. In this study, we propose Look &
Mark (L&M), a novel grounding fixation strategy that integrates radiologist eye
fixations (Look) and bounding box annotations (Mark) into the LLM prompting
framework. Unlike conventional fine-tuning, L&M leverages in-context learning
to achieve substantial performance gains without retraining. When evaluated
across multiple domain-specific and general-purpose models, L&M demonstrates
significant gains, including a 1.2% improvement in overall metrics (A.AVG) for
CXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for
LLaVA-Med. General-purpose models also benefit from L&M combined with
in-context learning, with LLaVA-OV achieving an 87.3% clinical average
performance (C.AVG)-the highest among all models, even surpassing those
explicitly trained for CXR report generation. Expert evaluations further
confirm that L&M reduces clinically significant errors (by 0.43 average errors
per report), such as false predictions and omissions, enhancing both accuracy
and reliability. These findings highlight L&M's potential as a scalable and
efficient solution for AI-assisted radiology, paving the way for improved
diagnostic workflows in low-resource clinical settings.

</details>


### [370] [CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation](https://arxiv.org/abs/2505.21904)
*Pardis Taghavi,Tian Liu,Renjie Li,Reza Langari,Zhengzhong Tu*

Key words: 实例分割，半监督学习，知识蒸馏，对比学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了CAST框架，通过半监督知识蒸馏（SSKD）将预训练视觉基础模型（VFM）压缩为紧凑专家模型，利用有限标注和大量无标注数据提升实例分割性能。

Motivation: 实例分割需要昂贵的像素级标注和大模型，CAST旨在通过半监督知识蒸馏减少标注需求并提升小模型性能。

Method: CAST分为三阶段：(1) 通过自训练和对比像素校准进行VFM教师的域适应；(2) 使用包含标准监督、伪标签和实例感知像素对比的多目标损失蒸馏到紧凑学生模型；(3) 在标注数据上微调以消除伪标签偏差。

Result: 在Cityscapes和ADE20K上，11X小的学生模型比适应的VFM教师分别提升+3.4 AP（33.9 vs. 30.5）和+1.5 AP（16.7 vs. 15.2），并优于最先进的半监督方法。

Conclusion: CAST通过实例感知像素对比损失有效利用无标注数据，显著提升了小模型的实例分割性能。

Abstract: Instance segmentation demands costly per-pixel annotations and large models.
We introduce CAST, a semi-supervised knowledge distillation (SSKD) framework
that compresses pretrained vision foundation models (VFM) into compact experts
using limited labeled and abundant unlabeled data. CAST unfolds in three
stages: (1) domain adaptation of the VFM teacher(s) via self-training with
contrastive pixel calibration, (2) distillation into a compact student via a
unified multi-objective loss that couples standard supervision and
pseudo-labels with our instance-aware pixel-wise contrastive term, and (3)
fine-tuning on labeled data to remove residual pseudo-label bias. Central to
CAST is an \emph{instance-aware pixel-wise contrastive loss} that fuses mask
and class scores to mine informative negatives and enforce clear inter-instance
margins. By maintaining this contrastive signal across both adaptation and
distillation, we align teacher and student embeddings and fully leverage
unlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses
its adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs.
15.2) and outperforms state-of-the-art semi-supervised approaches.

</details>


### [371] [Fostering Video Reasoning via Next-Event Prediction](https://arxiv.org/abs/2505.22457)
*Haonan Wang,Hongfu Liu,Xiangyan Liu,Chao Du,Kenji Kawaguchi,Ye Wang,Tianyu Pang*

Key words: 多模态大语言模型、时间推理、下一事件预测、自监督学习、视频理解

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了“下一个事件预测”（NEP）任务，通过利用未来视频片段作为自监督信号来增强多模态大语言模型（MLLMs）的时间推理能力。作者还构建了V1-33K数据集和FutureBench评估工具，验证了NEP的有效性和可扩展性。

Motivation: 现有任务（如视频问答或视频字幕生成）依赖人工标注或强监督信号，且容易混淆时空信息。NEP旨在通过自监督任务填补这一空白，专门提升MLLMs的时间推理能力。

Method: 将视频分为过去和未来帧，MLLMs基于过去帧预测未来事件的摘要。通过构建V1-33K数据集和多种视频指令调优策略，研究其对时间推理的影响。并使用FutureBench评估预测未来事件的连贯性。

Result: 实验证明NEP是一种可扩展且有效的训练范式，能显著提升MLLMs的时间推理能力。

Conclusion: NEP为MLLMs的时间推理提供了新的自监督学习框架，并通过数据集和评估工具验证了其潜力。

Abstract: Next-token prediction serves as the foundational learning task enabling
reasoning in LLMs. But what should the learning task be when aiming to equip
MLLMs with temporal reasoning capabilities over video inputs? Existing tasks
such as video question answering often rely on annotations from humans or much
stronger MLLMs, while video captioning tends to entangle temporal reasoning
with spatial information. To address this gap, we propose next-event prediction
(NEP), a learning task that harnesses future video segments as a rich,
self-supervised signal to foster temporal reasoning. We segment each video into
past and future frames: the MLLM takes the past frames as input and predicts a
summary of events derived from the future frames, thereby encouraging the model
to reason temporally in order to complete the task. To support this task, we
curate V1-33K, a dataset comprising 33,000 automatically extracted video
segments spanning diverse real-world scenarios. We further explore a range of
video instruction-tuning strategies to study their effects on temporal
reasoning. To evaluate progress, we introduce FutureBench to assess coherence
in predicting unseen future events. Experiments validate that NEP offers a
scalable and effective training paradigm for fostering temporal reasoning in
MLLMs.

</details>


### [372] [Thinking with Generated Images](https://arxiv.org/abs/2505.22525)
*Ethan Chern,Zhulin Hu,Steffi Chern,Siqi Kou,Jiadi Su,Yan Ma,Zhijie Deng,Pengfei Liu*

Key words: 大型多模态模型, 视觉推理, 生成图像, 自我批评, 中间思考步骤

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种新范式，通过让大型多模态模型（LMMs）自发生成中间视觉思考步骤，从根本上改变了其与视觉推理的交互方式。

Motivation: 当前LMMs的视觉推理仅限于处理固定的用户提供图像或仅通过文本链式思考（CoT）进行推理，限制了模型的认知能力。

Method: 1. 通过视觉生成与中间视觉子目标分解复杂任务；2. 通过自我批评机制生成初始视觉假设并逐步优化。

Result: 在视觉生成基准测试中，模型处理复杂多对象场景的性能相对提升了50%（从38%到57%）。

Conclusion: 该方法实现了类似人类视觉想象和迭代优化的能力，适用于多个领域（如生物化学、建筑设计等）。

Abstract: We present Thinking with Generated Images, a novel paradigm that
fundamentally transforms how large multimodal models (LMMs) engage with visual
reasoning by enabling them to natively think across text and vision modalities
through spontaneous generation of intermediate visual thinking steps. Current
visual reasoning with LMMs is constrained to either processing fixed
user-provided images or reasoning solely through text-based chain-of-thought
(CoT). Thinking with Generated Images unlocks a new dimension of cognitive
capability where models can actively construct intermediate visual thoughts,
critique their own visual hypotheses, and refine them as integral components of
their reasoning process. We demonstrate the effectiveness of our approach
through two complementary mechanisms: (1) vision generation with intermediate
visual subgoals, where models decompose complex visual tasks into manageable
components that are generated and integrated progressively, and (2) vision
generation with self-critique, where models generate an initial visual
hypothesis, analyze its shortcomings through textual reasoning, and produce
refined outputs based on their own critiques. Our experiments on vision
generation benchmarks show substantial improvements over baseline approaches,
with our models achieving up to 50% (from 38% to 57%) relative improvement in
handling complex multi-object scenarios. From biochemists exploring novel
protein structures, and architects iterating on spatial designs, to forensic
analysts reconstructing crime scenes, and basketball players envisioning
strategic plays, our approach enables AI models to engage in the kind of visual
imagination and iterative refinement that characterizes human creative,
analytical, and strategic thinking. We release our open-source suite at
https://github.com/GAIR-NLP/thinking-with-generated-images.

</details>


### [373] [RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction](https://arxiv.org/abs/2505.22613)
*Yuchi Wang,Yishuo Cai,Shuhuai Ren,Sihan Yang,Linli Yao,Yuanxin Liu,Yuanxing Zhang,Pengfei Wan,Xu Sun*

Key words: 图像标注, 视觉重建, 多模态大语言模型, 迭代优化, DPO

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: RICO 是一个通过视觉重建优化图像标注的框架，利用文本到图像模型生成参考图像，并通过多模态大语言模型迭代优化标注，显著提升准确性和完整性。

Motivation: 现有图像标注方法常因幻觉或细节缺失导致不准确，RICO 旨在通过视觉重建解决这些问题，提升标注质量。

Method: 使用文本到图像模型重建标注为参考图像，通过 MLLM 识别差异并迭代优化标注；RICO-Flash 通过 DPO 学习生成类似 RICO 的标注以减少计算成本。

Result: 实验显示 RICO 在 CapsBench 和 CompreCap 上显著优于基线方法约 10%。

Conclusion: RICO 通过视觉重建和迭代优化，生成了更准确、更全面的图像标注，同时 RICO-Flash 降低了计算成本。

Abstract: Image recaptioning is widely used to generate training datasets with enhanced
quality for various multimodal tasks. Existing recaptioning methods typically
rely on powerful multimodal large language models (MLLMs) to enhance textual
descriptions, but often suffer from inaccuracies due to hallucinations and
incompleteness caused by missing fine-grained details. To address these
limitations, we propose RICO, a novel framework that refines captions through
visual reconstruction. Specifically, we leverage a text-to-image model to
reconstruct a caption into a reference image, and prompt an MLLM to identify
discrepancies between the original and reconstructed images to refine the
caption. This process is performed iteratively, further progressively promoting
the generation of more faithful and comprehensive descriptions. To mitigate the
additional computational cost induced by the iterative process, we introduce
RICO-Flash, which learns to generate captions like RICO using DPO. Extensive
experiments demonstrate that our approach significantly improves caption
accuracy and completeness, outperforms most baselines by approximately 10% on
both CapsBench and CompreCap. Code released at
https://github.com/wangyuchi369/RICO.

</details>


### [374] [UniTalk: Towards Universal Active Speaker Detection in Real World Scenarios](https://arxiv.org/abs/2505.21954)
*Le Thien Phuc Nguyen,Zhuoran Yu,Khoa Quang Nhat Cao,Yuwei Guo,Tu Ho Manh Pham,Tuan Tai Nguyen,Toan Ngo Duc Vo,Lucas Poon,Soochahn Lee,Yong Jae Lee*

Key words: UniTalk, 主动说话者检测, 数据集, 多样性, 泛化能力

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 介绍了一个名为UniTalk的新数据集，专为主动说话者检测（ASD）任务设计，强调现实世界中的挑战性场景，以提升模型的泛化能力。与现有基准（如AVA）相比，UniTalk更注重多样性和难度，覆盖了多种语言、嘈杂背景和拥挤场景。数据集包含44.5小时视频和48,693个说话身份的帧级标注。评估显示，现有模型在AVA上表现优异，但在UniTalk上远未饱和。然而，UniTalk训练的模型在Talkies、ASW和AVA等数据集上展现了更强的泛化能力。

Motivation: 现有ASD数据集（如AVA）主要基于旧电影，与现实场景存在显著领域差距。UniTalk旨在弥补这一缺陷，提供更贴近现实、更具挑战性的数据，以推动ASD任务的进一步发展。

Method: 构建了UniTalk数据集，收集了多样化的现实场景视频（如多语言、嘈杂背景、拥挤场景等），并为48,693个说话身份提供帧级标注。通过对比实验，评估现有模型在AVA和UniTalk上的性能差异。

Result: 现有模型在AVA上接近饱和，但在UniTalk上表现不佳，表明ASD任务在现实条件下尚未解决。UniTalk训练的模型在多个数据集（Talkies、ASW、AVA）上展现了更强的泛化能力。

Conclusion: UniTalk为ASD研究提供了新的基准和宝贵资源，推动开发更具适应性和鲁棒性的模型。

Abstract: We present UniTalk, a novel dataset specifically designed for the task of
active speaker detection, emphasizing challenging scenarios to enhance model
generalization. Unlike previously established benchmarks such as AVA, which
predominantly features old movies and thus exhibits significant domain gaps,
UniTalk focuses explicitly on diverse and difficult real-world conditions.
These include underrepresented languages, noisy backgrounds, and crowded scenes
- such as multiple visible speakers speaking concurrently or in overlapping
turns. It contains over 44.5 hours of video with frame-level active speaker
annotations across 48,693 speaking identities, and spans a broad range of video
types that reflect real-world conditions. Through rigorous evaluation, we show
that state-of-the-art models, while achieving nearly perfect scores on AVA,
fail to reach saturation on UniTalk, suggesting that the ASD task remains far
from solved under realistic conditions. Nevertheless, models trained on UniTalk
demonstrate stronger generalization to modern "in-the-wild" datasets like
Talkies and ASW, as well as to AVA. UniTalk thus establishes a new benchmark
for active speaker detection, providing researchers with a valuable resource
for developing and evaluating versatile and resilient models.
  Dataset: https://huggingface.co/datasets/plnguyen2908/UniTalk-ASD
  Code: https://github.com/plnguyen2908/UniTalk-ASD-code

</details>


### [375] [Sherlock: Self-Correcting Reasoning in Vision-Language Models](https://arxiv.org/abs/2505.22651)
*Yi Ding,Ruqi Zhang*

Key words: 视觉语言模型、自我纠正、推理能力、Sherlock框架、数据效率

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: Sherlock是一种自我纠正和改进的框架，用于提升视觉语言模型的推理能力，仅需少量标注数据即可取得显著性能提升。

Motivation: 当前视觉语言模型在推理任务中表现不稳定，依赖大量标注数据或验证器，且泛化能力不足。

Method: 提出Sherlock框架，包括轨迹级自我纠正目标、基于视觉扰动的偏好数据构建方法和动态β偏好调整。

Result: 在八大基准测试中，Sherlock平均准确率达64.1（直接生成）和65.4（自我纠正后），优于现有模型，且数据需求减少80%。

Conclusion: 自我纠正策略有效提升了模型的推理性能和数据效率。

Abstract: Reasoning Vision-Language Models (VLMs) have shown promising performance on
complex multimodal tasks. However, they still face significant challenges: they
are highly sensitive to reasoning errors, require large volumes of annotated
data or accurate verifiers, and struggle to generalize beyond specific domains.
To address these limitations, we explore self-correction as a strategy to
enhance reasoning VLMs. We first conduct an in-depth analysis of reasoning
VLMs' self-correction abilities and identify key gaps. Based on our findings,
we introduce Sherlock, a self-correction and self-improvement training
framework. Sherlock introduces a trajectory-level self-correction objective, a
preference data construction method based on visual perturbation, and a dynamic
$\beta$ for preference tuning. Once the model acquires self-correction
capabilities using only 20k randomly sampled annotated data, it continues to
self-improve without external supervision. Built on the Llama3.2-Vision-11B
model, Sherlock achieves remarkable results across eight benchmarks, reaching
an average accuracy of 64.1 with direct generation and 65.4 after
self-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and
LlamaV-o1 (63.4) while using less than 20% of the annotated data.

</details>


### [376] [Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs](https://arxiv.org/abs/2505.21955)
*Insu Lee,Wooje Park,Jaeyun Jang,Minyoung Noh,Kyuhong Shim,Byonghyo Shim*

Key words: LVLMs, 多视图推理, E3VQA, M3CoT, 场景图

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了结合第一人称（egocentric）和第三人称（exocentric）视角的多视图框架E3VQA，并提出了训练无关的M3CoT提示技术，提升了LVLMs在跨视图推理中的表现。

Motivation: 面对第一人称视角因视野狭窄和缺乏全局背景而导致的推理失败问题，论文希望通过多视图输入来增强LVLMs的表现。

Method: 论文提出了通过多视图问答基准E3VQA和M3CoT提示技术，结合不同视图下的场景图，构建统一的场景表示。

Result: 实验显示该方法显著提升了GPT-4o和Gemini 2.0 Flash的性能（分别为4.84%和5.94%）。

Conclusion: 多视图输入和多视角场景图的集成能有效提升LVLMs的跨视图推理能力。

Abstract: Large vision-language models (LVLMs) are increasingly deployed in interactive
applications such as virtual and augmented reality, where first-person
(egocentric) view captured by head-mounted cameras serves as key input. While
this view offers fine-grained cues about user attention and hand-object
interactions, their narrow field of view and lack of global context often lead
to failures on spatially or contextually demanding queries. To address this, we
introduce a framework that augments egocentric inputs with third-person
(exocentric) views, providing complementary information such as global scene
layout and object visibility to LVLMs. We present E3VQA, the first benchmark
for multi-view question answering with 4K high-quality question-answer pairs
grounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a
training-free prompting technique that constructs a unified scene
representation by integrating scene graphs from three complementary
perspectives. M3CoT enables LVLMs to reason more effectively across views,
yielding consistent performance gains (4.84% for GPT-4o and 5.94% for Gemini
2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key
strengths and limitations of LVLMs in multi-view reasoning and highlights the
value of leveraging both egocentric and exocentric inputs.

</details>


### [377] [3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model](https://arxiv.org/abs/2505.22657)
*Wenbo Hu,Yining Hong,Yanjun Wang,Leison Gao,Zibu Wei,Xingcheng Yao,Nanyun Peng,Yonatan Bitton,Idan Szpektor,Kai-Wei Chang*

Key words: 3D空间记忆、大型语言模型、动态环境、情景记忆、任务推理

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了3DLLM-Mem模型和3DMem-Bench基准，用于解决LLMs在3D动态环境中空间-时间记忆建模的不足，显著提升了任务成功率。

Motivation: 人类擅长利用长期记忆处理复杂任务，而当前的大型语言模型（LLMs）在动态多房间3D环境中的规划和行动能力不足，主要原因是缺乏有效的3D空间-时间记忆建模。

Method: 提出了3DLLM-Mem模型，通过工作记忆令牌动态管理并融合来自情景记忆的空间和时间特征，选择性关注任务相关信息并保持记忆效率。

Result: 实验表明，3DLLM-Mem在3DMem-Bench基准的各类任务中表现最优，尤其在最具挑战性的实际任务中成功率比基线模型高16.5%。

Conclusion: 该研究通过创新的记忆管理机制显著提升了LLMs在复杂3D环境中的推理和行动能力，为后续研究提供了新的基准和方向。

Abstract: Humans excel at performing complex tasks by leveraging long-term memory
across temporal and spatial experiences. In contrast, current Large Language
Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D
environments. We posit that part of this limitation is due to the lack of
proper 3D spatial-temporal memory modeling in LLMs. To address this, we first
introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000
trajectories and 2,892 embodied tasks, question-answering and captioning,
designed to evaluate an agent's ability to reason over long-term memory in 3D
environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management
and fusion model for embodied spatial-temporal reasoning and actions in LLMs.
Our model uses working memory tokens, which represents current observations, as
queries to selectively attend to and fuse the most useful spatial and temporal
features from episodic memory, which stores past observations and interactions.
Our approach allows the agent to focus on task-relevant information while
maintaining memory efficiency in complex, long-horizon environments.
Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art
performance across various tasks, outperforming the strongest baselines by
16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied
tasks.

</details>


### [378] [Learning Shared Representations from Unpaired Data](https://arxiv.org/abs/2505.21524)
*Amitai Yacobi,Nir Ben-Ari,Ronen Talmon,Uri Shaham*

Key words: 多模态学习, 非配对数据, 谱嵌入, 共享表示, 跨模态嵌入

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种新方法，利用非配对数据学习共享表示，通过随机游走矩阵的谱嵌入实现跨模态关系捕捉，在多种任务中表现优异。

Motivation: 当前多模态表示学习依赖配对样本，但其获取难度高。本文探索从非配对数据中学习共享表示，以突破这一限制。

Method: 基于单模态表示的随机游走矩阵谱嵌入，独立构建并学习共享表示空间。

Result: 在视觉和自然语言处理领域中，该方法在检索、生成、零样本分类等任务中表现优异，验证了非配对数据的有效性。

Conclusion: 首次证明非配对数据可学习通用跨模态嵌入，为多模态学习提供新思路。

Abstract: Learning shared representations is a primary area of multimodal
representation learning. The current approaches to achieve a shared embedding
space rely heavily on paired samples from each modality, which are
significantly harder to obtain than unpaired ones. In this work, we demonstrate
that shared representations can be learned almost exclusively from unpaired
data. Our arguments are grounded in the spectral embeddings of the random walk
matrices constructed independently from each unimodal representation. Empirical
results in computer vision and natural language processing domains support its
potential, revealing the effectiveness of unpaired data in capturing meaningful
cross-modal relations, demonstrating high capabilities in retrieval tasks,
generation, arithmetics, zero-shot, and cross-domain classification. This work,
to the best of our knowledge, is the first to demonstrate these capabilities
almost exclusively from unpaired samples, giving rise to a cross-modal
embedding that could be viewed as universal, i.e., independent of the specific
modalities of the data. Our code IS publicly available at
https://github.com/shaham-lab/SUE.

</details>


### [379] [Self-Organizing Visual Prototypes for Non-Parametric Representation Learning](https://arxiv.org/abs/2505.21533)
*Thalles Silva,Helio Pedrini,Adín Ramírez Rivera*

Key words: 自监督学习, 视觉特征学习, 非参数学习, SOP-MIM, 支持嵌入

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: SOP是一种新的无监督视觉特征学习训练方法，通过多个语义相似的支持嵌入（SEs）替代单一原型，提高特征表征能力，并在多个基准测试中取得最优性能。

Motivation: 现有原型自监督学习（SSL）方法仅依赖单一原型编码隐藏簇的所有特征，限制了表征能力。SOP旨在通过多个支持嵌入（SEs）提升特征的互补性和训练性能。

Method: 提出SOP策略，包含两个新颖的非参数损失函数，并设计SOP-MIM任务（基于多SEs重建掩码表征）。

Result: 在检索、线性评估、微调和目标检测等任务中，SOP策略学到的表征均达到最优性能，且性能随编码器复杂度提升而增强。

Conclusion: SOP通过多SEs显著提升了无监督视觉特征学习的性能，验证了非参数SSL的可行性。

Abstract: We present Self-Organizing Visual Prototypes (SOP), a new training technique
for unsupervised visual feature learning. Unlike existing prototypical
self-supervised learning (SSL) methods that rely on a single prototype to
encode all relevant features of a hidden cluster in the data, we propose the
SOP strategy. In this strategy, a prototype is represented by many semantically
similar representations, or support embeddings (SEs), each containing a
complementary set of features that together better characterize their region in
space and maximize training performance. We reaffirm the feasibility of
non-parametric SSL by introducing novel non-parametric adaptations of two loss
functions that implement the SOP strategy. Notably, we introduce the SOP Masked
Image Modeling (SOP-MIM) task, where masked representations are reconstructed
from the perspective of multiple non-parametric local SEs. We comprehensively
evaluate the representations learned using the SOP strategy on a range of
benchmarks, including retrieval, linear evaluation, fine-tuning, and object
detection. Our pre-trained encoders achieve state-of-the-art performance on
many retrieval benchmarks and demonstrate increasing performance gains with
more complex encoders.

</details>


### [380] [Learning World Models for Interactive Video Generation](https://arxiv.org/abs/2505.21996)
*Taiye Chen,Xun Hu,Zihan Ding,Chi Jin*

Key words: 视频生成, 世界模型, 检索增强生成, 复合误差, 时空一致性

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了视频检索增强生成（VRAG）方法，通过显式全局状态条件增强图像到视频模型的交互能力，显著减少了长期复合误差并提高了世界模型的时空一致性。

Motivation: 现有的长视频生成模型因复合误差和记忆机制不足导致世界建模能力有限，需要通过交互和时空一致性改进。

Method: 采用动作条件和自回归框架增强图像到视频模型，并提出视频检索增强生成（VRAG），结合显式全局状态条件。

Result: VRAG显著减少了长期复合误差，提高了世界模型的时空一致性，而传统的自回归生成和检索增强生成效果较差。

Conclusion: 视频世界模型存在固有挑战，VRAG为改进视频生成模型的世界建模能力提供了有效方向和基准。

Abstract: Foundational world models must be both interactive and preserve
spatiotemporal coherence for effective future planning with action choices.
However, present models for long video generation have limited inherent world
modeling capabilities due to two main challenges: compounding errors and
insufficient memory mechanisms. We enhance image-to-video models with
interactive capabilities through additional action conditioning and
autoregressive framework, and reveal that compounding error is inherently
irreducible in autoregressive video generation, while insufficient memory
mechanism leads to incoherence of world models. We propose video retrieval
augmented generation (VRAG) with explicit global state conditioning, which
significantly reduces long-term compounding errors and increases spatiotemporal
consistency of world models. In contrast, naive autoregressive generation with
extended context windows and retrieval-augmented generation prove less
effective for video generation, primarily due to the limited in-context
learning capabilities of current video models. Our work illuminates the
fundamental challenges in video world models and establishes a comprehensive
benchmark for improving video generation models with internal world modeling
capabilities.

</details>


### [381] [Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation](https://arxiv.org/abs/2505.21545)
*Chika Maduabuchi,Hao Chen,Yujin Han,Jindong Wang*

Key words: 视频生成,扩散模型,抗干扰训练,噪声注入

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: CAT-LVDM是一种针对潜在视频扩散模型的抗干扰训练框架，通过结构化噪声注入提升鲁棒性，显著减少语义漂移和时序不一致性问题。

Motivation: 解决潜在视频扩散模型在噪声丰富的网络视频文本数据集上表现出的语义漂移和时序不一致性问题。

Method: 引入CAT-LVDM框架，包括批中心噪声注入(BCNI)和频谱感知上下文噪声(SACN)方法，通过结构化噪声提升模型鲁棒性。

Result: BCNI在WebVid-2M等数据集上平均降低FVD 31.9%，SACN在UCF-101上提升12.3%。

Conclusion: CAT-LVDM为多模态噪声下的视频扩散提供了一种可扩展且有效的训练方法。

Abstract: Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are
sensitive to imperfect conditioning, which causes semantic drift and temporal
incoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the
first corruption-aware training framework for LVDMs that improves robustness
through structured, data-aligned noise injection. Our method includes
Batch-Centered Noise Injection (BCNI), which perturbs embeddings along
intra-batch semantic directions to preserve temporal consistency. BCNI is
especially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and
MSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects
noise along dominant spectral directions to improve low-frequency smoothness,
showing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across
WebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101.
Ablation studies confirm the benefit of low-rank, data-aligned noise. Our
theoretical analysis further explains how such perturbations tighten entropy,
Wasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM
establishes a principled, scalable training approach for robust video diffusion
under multimodal noise. Code and models: https://github.com/chikap421/catlvdm

</details>


### [382] [GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement](https://arxiv.org/abs/2505.22021)
*Zhihong Tang,Yang Li*

Key words: 文档图像增强,多退化处理,参数化生成,分层框架,NestUNet

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种新型的全局与局部参数生成增强网络（GL-PGENet），用于多退化彩色文档图像的增强，通过分层框架和双分支局部细化网络实现高效且鲁棒的增强效果。

Motivation: 现有方法多局限于单一退化修复或灰度图像处理，无法满足多退化彩色文档图像的增强需求，因此作者提出了GL-PGENet以解决这一问题。

Method: 采用分层增强框架结合全局外观校正与局部细化；提出双分支局部细化网络和参数化生成机制；改进NestUNet架构以融合高低层特征；采用两阶段训练策略（大规模预训练和任务微调）。

Result: 在DocUNet和RealDAE数据集上分别达到0.7721和0.9480的SSIM分数，表现优越且具有跨域适应性和计算效率。

Conclusion: GL-PGENet在多退化彩色文档图像增强中表现出色，兼具高效性和实用性。

Abstract: Document Image Enhancement (DIE) serves as a critical component in Document
AI systems, where its performance substantially determines the effectiveness of
downstream tasks. To address the limitations of existing methods confined to
single-degradation restoration or grayscale image processing, we present Global
with Local Parametric Generation Enhancement Network (GL-PGENet), a novel
architecture designed for multi-degraded color document images, ensuring both
efficiency and robustness in real-world scenarios. Our solution incorporates
three key innovations: First, a hierarchical enhancement framework that
integrates global appearance correction with local refinement, enabling
coarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network
with parametric generation mechanisms that replaces conventional direct
prediction, producing enhanced outputs through learned intermediate parametric
representations rather than pixel-wise mapping. This approach enhances local
consistency while improving model generalization. Finally, a modified NestUNet
architecture incorporating dense block to effectively fuse low-level pixel
features and high-level semantic features, specifically adapted for document
image characteristics. In addition, to enhance generalization performance, we
adopt a two-stage training strategy: large-scale pretraining on a synthetic
dataset of 500,000+ samples followed by task-specific fine-tuning. Extensive
experiments demonstrate the superiority of GL-PGENet, achieving
state-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The
model also exhibits remarkable cross-domain adaptability and maintains
computational efficiency for high-resolution images without performance
degradation, confirming its practical utility in real-world scenarios.

</details>


### [383] [Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization](https://arxiv.org/abs/2505.22038)
*Kaiyuan Li,Xiaoyue Chen,Chen Gao,Yong Li,Xinlei Chen*

Key words: 大型视觉语言模型, 令牌剪枝, 计算效率, 多模态任务

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种名为平衡令牌剪枝（BTP）的方法，用于减少大型视觉语言模型（LVLM）中图像令牌的计算负担。通过分阶段剪枝，BTP在早期关注全局影响，后期关注局部一致性，实现了高效压缩。

Motivation: 由于大型视觉语言模型中图像令牌数量庞大导致计算开销显著，且现有剪枝方法未充分考虑局部与全局影响的联合作用，因此需要一种更优的剪枝策略。

Method: BTP利用小型校准集将剪枝过程分为多阶段：早期阶段强调对后续层的全局影响，后期阶段则注重保持当前层输出的局部一致性。

Result: 实验表明，BTP能在平均保留96.7%模型性能的同时，实现78%的压缩率。

Conclusion: BTP是一种有效的即插即用剪枝方法，显著降低了LVLM的计算开销，同时保持了高性能。

Abstract: Large Vision-Language Models (LVLMs) have shown impressive performance across
multi-modal tasks by encoding images into thousands of tokens. However, the
large number of image tokens results in significant computational overhead, and
the use of dynamic high-resolution inputs further increases this burden.
Previous approaches have attempted to reduce the number of image tokens through
token pruning, typically by selecting tokens based on attention scores or image
token diversity. Through empirical studies, we observe that existing methods
often overlook the joint impact of pruning on both the current layer's output
(local) and the outputs of subsequent layers (global), leading to suboptimal
pruning decisions. To address this challenge, we propose Balanced Token Pruning
(BTP), a plug-and-play method for pruning vision tokens. Specifically, our
method utilizes a small calibration set to divide the pruning process into
multiple stages. In the early stages, our method emphasizes the impact of
pruning on subsequent layers, whereas in the deeper stages, the focus shifts
toward preserving the consistency of local outputs. Extensive experiments
across various LVLMs demonstrate the broad effectiveness of our approach on
multiple benchmarks. Our method achieves a 78% compression rate while
preserving 96.7% of the original models' performance on average.

</details>


### [384] [Knowledge Distillation Approach for SOS Fusion Staging: Towards Fully Automated Skeletal Maturity Assessment](https://arxiv.org/abs/2505.21561)
*Omid Halimi Milani,Amanda Nikho,Marouane Tliba,Lauren Mills,Ahmet Enis Cetin,Mohammed H Elnagar*

Key words: 深度学习, 蝶枕软骨联合, 知识蒸馏, 骨骼成熟度, 自动化诊断

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种用于自动化评估蝶枕软骨联合（SOS）融合的新深度学习框架，通过双模型架构和知识蒸馏技术，无需外部裁剪即可实现高精度诊断。

Motivation: 为提升正畸学和法医人类学中骨骼成熟度评估的效率和一致性，开发无需额外预处理的自动化诊断工具。

Method: 采用双模型架构（教师模型和学生模型），通过新型损失函数实现空间逻辑对齐和梯度注意力映射，结合专家数据逐步优化。

Result: 框架在临床环境中展现了鲁棒的诊断准确性，形成了端到端的可行流程。

Conclusion: 该方法简化了评估流程，提升了部署效率，适用于多样化临床场景。

Abstract: We introduce a novel deep learning framework for the automated staging of
spheno-occipital synchondrosis (SOS) fusion, a critical diagnostic marker in
both orthodontics and forensic anthropology. Our approach leverages a
dual-model architecture wherein a teacher model, trained on manually cropped
images, transfers its precise spatial understanding to a student model that
operates on full, uncropped images. This knowledge distillation is facilitated
by a newly formulated loss function that aligns spatial logits as well as
incorporates gradient-based attention spatial mapping, ensuring that the
student model internalizes the anatomically relevant features without relying
on external cropping or YOLO-based segmentation. By leveraging expert-curated
data and feedback at each step, our framework attains robust diagnostic
accuracy, culminating in a clinically viable end-to-end pipeline. This
streamlined approach obviates the need for additional pre-processing tools and
accelerates deployment, thereby enhancing both the efficiency and consistency
of skeletal maturation assessment in diverse clinical settings.

</details>


### [385] [Multi-instance Learning as Downstream Task of Self-Supervised Learning-based Pre-trained Model](https://arxiv.org/abs/2505.21564)
*Koki Matsuishi,Tsuyoshi Okita*

Key words: 多实例学习、自监督学习、预训练模型、脑血肿CT、分类性能

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出使用自监督学习的预训练模型作为多实例学习器的下游任务，解决了脑血肿CT中实例数量增加导致的性能下降问题，提高了分类准确率和F1分数。

Motivation: 在脑血肿CT中，当每个包（bag）中的实例数量增加时，传统的多实例学习方法性能显著下降。为了解决这一问题，作者提出了一种新方法。

Method: 采用自监督学习的预训练模型作为多实例学习器的下游任务，以克服目标任务中的虚假相关性问题。

Result: 使用该方法后，脑血肿CT的低密度标记分类任务在准确率上提升了5%至13%，F1分数提升了40%至55%。

Conclusion: 通过结合自监督学习的预训练模型，可以有效提升多实例学习在实例数量较大时的性能。

Abstract: In deep multi-instance learning, the number of applicable instances depends
on the data set. In histopathology images, deep learning multi-instance
learners usually assume there are hundreds to thousands instances in a bag.
However, when the number of instances in a bag increases to 256 in brain
hematoma CT, learning becomes extremely difficult. In this paper, we address
this drawback. To overcome this problem, we propose using a pre-trained model
with self-supervised learning for the multi-instance learner as a downstream
task. With this method, even when the original target task suffers from the
spurious correlation problem, we show improvements of 5% to 13% in accuracy and
40% to 55% in the F1 measure for the hypodensity marker classification of brain
hematoma CT.

</details>


### [386] [From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving](https://arxiv.org/abs/2505.22067)
*Xinyu Xia,Xingjun Ma,Yunfeng Hu,Ting Qu,Hong Chen,Xun Gong*

Key words: 自动驾驶,场景推荐,LLM,故障修复,安全关键场景

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 介绍了一个基于LLM的框架SERA，能够通过分析性能日志自主识别故障模式并推荐相关场景，从而提升自动驾驶系统的性能。

Motivation: 现有场景生成和选择方法缺乏适应性和语义相关性，限制了性能改进的效果，需要一个能够自我进化并能针对性修复故障的系统。

Method: 通过分析性能日志识别故障模式，动态检索语义对齐的场景，结合LLM的反思机制细化推荐，并利用少量数据进行微调。

Result: SERA在基准测试中持续改进关键指标，验证了其在安全关键条件下的有效性和普适性。

Conclusion: SERA能有效推荐并修复自动驾驶系统的故障场景，展示了其在实际应用中的潜力。

Abstract: Ensuring robust and generalizable autonomous driving requires not only broad
scenario coverage but also efficient repair of failure cases, particularly
those related to challenging and safety-critical scenarios. However, existing
scenario generation and selection methods often lack adaptivity and semantic
relevance, limiting their impact on performance improvement. In this paper, we
propose \textbf{SERA}, an LLM-powered framework that enables autonomous driving
systems to self-evolve by repairing failure cases through targeted scenario
recommendation. By analyzing performance logs, SERA identifies failure patterns
and dynamically retrieves semantically aligned scenarios from a structured
bank. An LLM-based reflection mechanism further refines these recommendations
to maximize relevance and diversity. The selected scenarios are used for
few-shot fine-tuning, enabling targeted adaptation with minimal data.
Experiments on the benchmark show that SERA consistently improves key metrics
across multiple autonomous driving baselines, demonstrating its effectiveness
and generalizability under safety-critical conditions.

</details>


### [387] [Diffusion Model-based Activity Completion for AI Motion Capture from Videos](https://arxiv.org/abs/2505.21566)
*Gao Huayu,Huang Tengjiu,Ye Xiaolong,Tsuyoshi Okita*

Key words: AI运动捕捉、扩散模型、动作补全、虚拟人、Human3.6M

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种基于扩散模型的运动补全技术（MDC-Net），用于生成平滑连续的动作序列，解决了AI运动捕捉中动作片段间过渡缺失的问题，并在Human3.6M数据集上取得了优于现有方法的结果。

Motivation: 当前AI运动捕捉方法依赖预定义视频序列，无法处理未观测到的动作。为虚拟人提供灵活动作需求，需填补动作片段间的过渡空缺。

Method: 提出基于扩散模型的运动补全技术，结合门控模块和位置-时间嵌入模块生成互补动作序列。

Result: MDC-Net在ADE、FDE和MMADE指标上优于现有方法，模型尺寸更小（16.84M），生成的动作更自然连贯。

Conclusion: MDC-Net有效解决了动作过渡问题，为虚拟人运动捕捉提供了更灵活的解决方案。

Abstract: AI-based motion capture is an emerging technology that offers a
cost-effective alternative to traditional motion capture systems. However,
current AI motion capture methods rely entirely on observed video sequences,
similar to conventional motion capture. This means that all human actions must
be predefined, and movements outside the observed sequences are not possible.
To address this limitation, we aim to apply AI motion capture to virtual
humans, where flexible actions beyond the observed sequences are required. We
assume that while many action fragments exist in the training data, the
transitions between them may be missing. To bridge these gaps, we propose a
diffusion-model-based action completion technique that generates complementary
human motion sequences, ensuring smooth and continuous movements. By
introducing a gate module and a position-time embedding module, our approach
achieves competitive results on the Human3.6M dataset. Our experimental results
show that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but
is slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size
(16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural
and coherent motion sequences. Additionally, we propose a method for extracting
sensor data, including acceleration and angular velocity, from human motion
sequences.

</details>


### [388] [EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models](https://arxiv.org/abs/2505.21567)
*Feng Jiang,Zihao Zheng,Xiuping Cui,Maoliang Li,JIayu Chen,Xiang Chen*

Key words: Embodied AI, VLA模型, 量化, token对齐, EaqVLA

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 为解决VLA模型量化时的token对齐问题，提出EaqVLA框架，通过编码对齐的混合精度量化优化计算/存储成本，实验显示其在量化损失和加速方面优于现有方法。

Motivation: 现有VLA模型因计算/存储成本高昂需优化，量化虽有效但受限于token对齐问题，因此提出EaqVLA框架。

Method: 1. 分析多粒度下的token未对齐问题；2. 提出基于编码对齐意识的混合精度量化方法。

Result: EaqVLA在端到端动作控制的量化损失最小且实现xxx倍加速，优于现有量化方法。

Conclusion: EaqVLA通过编码对齐量化有效解决VLA模型的优化问题，为高效部署提供新思路。

Abstract: With the development of Embodied Artificial intelligence, the end-to-end
control policy such as Vision-Language-Action (VLA) model has become the
mainstream. Existing VLA models faces expensive computing/storage cost, which
need to be optimized. Quantization is considered as the most effective method
which can not only reduce the memory cost but also achieve computation
acceleration. However, we find the token alignment of VLA models hinders the
application of existing quantization methods. To address this, we proposed an
optimized framework called EaqVLA, which apply encoding-aligned quantization to
VLA models. Specifically, we propose an complete analysis method to find the
misalignment in various granularity. Based on the analysis results, we propose
a mixed precision quantization with the awareness of encoding alignment.
Experiments shows that the porposed EaqVLA achieves better quantization
performance (with the minimal quantization loss for end-to-end action control
and xxx times acceleration) than existing quantization methods.

</details>


### [389] [Do We Need All the Synthetic Data? Towards Targeted Synthetic Image Augmentation via Diffusion Models](https://arxiv.org/abs/2505.21574)
*Dang Nguyen,Jiping Li,Jinghao Zheng,Baharan Mirzasoleiman*

Key words: 数据增强、扩散模型、图像分类、CNN、泛化性能

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种通过选择性合成数据增强（仅增强训练早期未学习的数据部分）来提升图像分类器性能的方法，实验表明该方法在多种场景下优于全数据集增强，且能与现有增强策略结合使用。

Motivation: 现有数据增强方法在生成多样性和数据量扩展（10-30倍）上存在局限，作者希望通过分析训练早期未学习的数据部分，找到更高效的增强策略。

Method: 通过分析双层CNN，仅对训练早期未学习的数据部分（30%-40%）进行合成增强，避免噪声放大并促进特征学习速度的均衡。

Result: 在CIFAR-10/100、TinyImageNet等数据集上，搭配不同优化器（SGD、SAM），性能提升最高达2.8%，且仅用SGD即可超越SOTA优化器SAM。

Conclusion: 选择性数据增强能更高效提升模型性能，且兼容现有强弱增强策略。

Abstract: Synthetically augmenting training datasets with diffusion models has been an
effective strategy for improving generalization of image classifiers. However,
existing techniques struggle to ensure the diversity of generation and increase
the size of the data by up to 10-30x to improve the in-distribution
performance. In this work, we show that synthetically augmenting part of the
data that is not learned early in training outperforms augmenting the entire
dataset. By analyzing a two-layer CNN, we prove that this strategy improves
generalization by promoting homogeneity in feature learning speed without
amplifying noise. Our extensive experiments show that by augmenting only
30%-40% of the data, our method boosts the performance by up to 2.8% in a
variety of scenarios, including training ResNet, ViT and DenseNet on CIFAR-10,
CIFAR-100, and TinyImageNet, with a range of optimizers including SGD and SAM.
Notably, our method applied with SGD outperforms the SOTA optimizer, SAM, on
CIFAR-100 and TinyImageNet. It can also easily stack with existing weak and
strong augmentation strategies to further boost the performance.

</details>


### [390] [SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model](https://arxiv.org/abs/2505.22126)
*Yifan Chang,Yukang Feng,Jianwen Sun,Jiaxin Ai,Chuanhao Li,S. Kevin Zhou,Kaipeng Zhang*

Key words: AI图像生成,科学图表,SridBench,语义理解,评估基准

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文介绍了首个科学图表生成基准SridBench，旨在评估AI在科学领域图像生成的表现，发现即使顶级模型如GPT-4o-image也落后于人类水平。

Motivation: 科学图表生成需要高精度和专业知识，现有AI模型缺乏相关评估基准，自动化生成具有重要实践价值。

Method: 构建SridBench基准，包含1,120个来自13个学科的科学图表样本，通过人类专家和MLLMs收集，并在六个维度上评估。

Result: 实验显示GPT-4o-image等模型在语义保真度和结构准确性上不及人类，尤其在文本/视觉清晰度和科学正确性上存在问题。

Conclusion: 科学图表生成需要更高级的推理驱动视觉生成能力，SridBench为推动相关研究提供了重要工具。

Abstract: Recent years have seen rapid advances in AI-driven image generation. Early
diffusion models emphasized perceptual quality, while newer multimodal models
like GPT-4o-image integrate high-level reasoning, improving semantic
understanding and structural composition. Scientific illustration generation
exemplifies this evolution: unlike general image synthesis, it demands accurate
interpretation of technical content and transformation of abstract ideas into
clear, standardized visuals. This task is significantly more
knowledge-intensive and laborious, often requiring hours of manual work and
specialized tools. Automating it in a controllable, intelligent manner would
provide substantial practical value. Yet, no benchmark currently exists to
evaluate AI on this front. To fill this gap, we introduce SridBench, the first
benchmark for scientific figure generation. It comprises 1,120 instances
curated from leading scientific papers across 13 natural and computer science
disciplines, collected via human experts and MLLMs. Each sample is evaluated
along six dimensions, including semantic fidelity and structural accuracy.
Experimental results reveal that even top-tier models like GPT-4o-image lag
behind human performance, with common issues in text/visual clarity and
scientific correctness. These findings highlight the need for more advanced
reasoning-driven visual generation capabilities.

</details>


### [391] [Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e Mission Approach](https://arxiv.org/abs/2505.22128)
*Alejandro D. Mousist*

Key words: 盲去模糊,GAN,边缘计算,地球观测,IMAGIN-e

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出一种适用于太空边缘计算约束的盲去模糊方法，用于修复ISS IMAGIN-e任务中的地球观测图像机械离焦问题。方法基于Sentinel-2数据估计离焦核并在GAN框架中训练恢复模型。实验显示合成退化图像的SSIM和PSNR显著提升，实际任务中感知质量指标（NIQE、BRISQUE）也大幅改善，目前已在IMAGIN-e任务中部署。

Motivation: 解决太空边缘计算环境下地球观测图像的机械离焦问题，实现无需参考图像的高效图像恢复。

Method: 利用Sentinel-2数据估计离焦核，在GAN框架下训练盲去模糊模型。

Result: 合成数据上SSIM提升72.47%，PSNR提升25.00%；实际任务中NIQE和BRISQUE分别改善60.66%和48.38%。

Conclusion: 该方法在资源受限的太空环境中有效恢复图像细节，已成功部署于IMAGIN-e任务。

Abstract: This work addresses mechanical defocus in Earth observation images from the
IMAGIN-e mission aboard the ISS, proposing a blind deblurring approach adapted
to space-based edge computing constraints. Leveraging Sentinel-2 data, our
method estimates the defocus kernel and trains a restoration model within a GAN
framework, effectively operating without reference images.
  On Sentinel-2 images with synthetic degradation, SSIM improved by 72.47% and
PSNR by 25.00%, confirming the model's ability to recover lost details when the
original clean image is known. On IMAGIN-e, where no reference images exist,
perceptual quality metrics indicate a substantial enhancement, with NIQE
improving by 60.66% and BRISQUE by 48.38%, validating real-world onboard
restoration. The approach is currently deployed aboard the IMAGIN-e mission,
demonstrating its practical application in an operational space environment.
  By efficiently handling high-resolution images under edge computing
constraints, the method enables applications such as water body segmentation
and contour detection while maintaining processing viability despite resource
limitations.

</details>


### [392] [QuARI: Query Adaptive Retrieval Improvement](https://arxiv.org/abs/2505.21647)
*Eric Xing,Abby Stylianou,Robert Pless,Nathan Jacobs*

Key words: 视觉语言模型, 图像检索, 线性变换, 大规模检索

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种通过为每个查询学习特定线性变换来改进视觉语言模型在大型图像检索任务中表现的方法，计算成本低且效果优于现有技术。

Motivation: 现有视觉语言模型在挑战性检索任务（如大规模图像实例检索）中表现不佳，需要更高效的改进方法。

Method: 学习为每个查询生成特定的线性特征空间变换，以强调与查询相关的子空间，计算成本低且可扩展性强。

Result: 该方法在大规模检索任务中表现优于现有技术，包括计算量更大的替代方案。

Conclusion: 通过学习查询特定的线性变换，可以高效地提升视觉语言模型在大规模图像检索中的性能。

Abstract: Massive-scale pretraining has made vision-language models increasingly
popular for image-to-image and text-to-image retrieval across a broad
collection of domains. However, these models do not perform well when used for
challenging retrieval tasks, such as instance retrieval in very large-scale
image collections. Recent work has shown that linear transformations of VLM
features trained for instance retrieval can improve performance by emphasizing
subspaces that relate to the domain of interest. In this paper, we explore a
more extreme version of this specialization by learning to map a given query to
a query-specific feature space transformation. Because this transformation is
linear, it can be applied with minimal computational cost to millions of image
embeddings, making it effective for large-scale retrieval or re-ranking.
Results show that this method consistently outperforms state-of-the-art
alternatives, including those that require many orders of magnitude more
computation at query time.

</details>


### [393] [FaceEditTalker: Interactive Talking Head Generation with Facial Attribute Editing](https://arxiv.org/abs/2505.22141)
*Guanwen Feng,Zhiyuan Ma,Yunan Li,Junwei Jing,Jiahao Yang,Qiguang Miao*

Key words: 音频驱动、说话头生成、面部属性编辑、特征空间、扩散模型

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: FaceEditTalker是一个统一框架，能够在生成高质量、音频同步的说话头视频时实现可控的面部属性编辑。

Motivation: 当前音频驱动的说话头生成技术忽视了面部属性编辑这一关键任务，而这一能力对于深度个性化和扩展实际应用范围至关重要。

Method: 方法包括图像特征空间编辑模块（提取语义和细节特征以灵活控制属性）和音频驱动的视频生成模块（融合编辑特征与音频引导的面部标志以驱动基于扩散的生成器）。

Result: 实验表明，该方法在唇同步精度、视频质量和属性可控性上优于现有技术。

Conclusion: FaceEditTalker通过统一框架实现了高质量、可控的面部属性编辑和视频生成。

Abstract: Recent advances in audio-driven talking head generation have achieved
impressive results in lip synchronization and emotional expression. However,
they largely overlook the crucial task of facial attribute editing. This
capability is crucial for achieving deep personalization and expanding the
range of practical applications, including user-tailored digital avatars,
engaging online education content, and brand-specific digital customer service.
In these key domains, the flexible adjustment of visual attributes-such as
hairstyle, accessories, and subtle facial features is essential for aligning
with user preferences, reflecting diverse brand identities, and adapting to
varying contextual demands. In this paper, we present FaceEditTalker, a unified
framework that enables controllable facial attribute manipulation while
generating high-quality, audio-synchronized talking head videos. Our method
consists of two key components: an image feature space editing module, which
extracts semantic and detail features and allows flexible control over
attributes like expression, hairstyle, and accessories; and an audio-driven
video generation module, which fuses these edited features with audio-guided
facial landmarks to drive a diffusion-based generator. This design ensures
temporal coherence, visual fidelity, and identity preservation across frames.
Extensive experiments on public datasets demonstrate that our method
outperforms state-of-the-art approaches in lip-sync accuracy, video quality,
and attribute controllability. Project page:
https://peterfanfan.github.io/FaceEditTalker/

</details>


### [394] [Investigating Mechanisms for In-Context Vision Language Binding](https://arxiv.org/abs/2505.22200)
*Darshana Saravanan,Makarand Tapaswi,Vineet Gandhi*

Key words: 视觉语言模型, Binding ID, 图像文本绑定, 跨模态关联, 3D对象

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文研究了视觉语言模型（VLMs）如何通过Binding ID机制关联图像与文本，实验表明VLMs能为对象图像及其文本描述分配独特的Binding ID以实现上下文关联。

Motivation: 理解视觉语言模型如何感知图像、理解文本并跨模态建立关联，尤其是通过Binding ID机制实现图像与文本的绑定。

Method: 提出使用合成数据集和任务，要求模型关联图像中的3D对象与其文本描述，验证Binding ID机制在VLMs中的应用。

Result: 实验证明VLMs能为对象图像及其文本描述分配独特的Binding ID，实现上下文关联。

Conclusion: Binding ID机制在VLMs中有效支持图像与文本的绑定，增强跨模态关联能力。

Abstract: To understand a prompt, Vision-Language models (VLMs) must perceive the
image, comprehend the text, and build associations within and across both
modalities. For instance, given an 'image of a red toy car', the model should
associate this image to phrases like 'car', 'red toy', 'red object', etc. Feng
and Steinhardt propose the Binding ID mechanism in LLMs, suggesting that the
entity and its corresponding attribute tokens share a Binding ID in the model
activations. We investigate this for image-text binding in VLMs using a
synthetic dataset and task that requires models to associate 3D objects in an
image with their descriptions in the text. Our experiments demonstrate that
VLMs assign a distinct Binding ID to an object's image tokens and its textual
references, enabling in-context association.

</details>


### [395] [Moment kernels: a simple and scalable approach for equivariance to rotations and reflections in deep convolutional networks](https://arxiv.org/abs/2505.21736)
*Zachary Schlamowitz,Andrew Bennecke,Daniel J. Tward*

Key words: 等变性、卷积神经网络、生物医学图像分析、矩核、对称性、旋转反射

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种名为“矩核”的简单卷积核形式，用于实现旋转和反射等对称性的等变性，简化了传统方法中的数学复杂性，并在生物医学图像分析任务中验证了其有效性。

Motivation: 传统利用旋转和反射对称性的方法依赖于复杂的数学理论，如表示理论，限制了其广泛应用。作者旨在简化这一过程，提出更易实现的等变性方法。

Method: 使用称为“矩核”的简单卷积核形式，这些核由空间位置的径向对称函数乘以x的分量的幂或单位矩阵构成。通过标准卷积模块实现等变神经网络。

Result: 论文提出的方法在分类（输出正交变换不变）、3D图像配准（输出向量变换）和细胞分割（椭圆二次形式矩阵变换）等任务中验证了有效性。

Conclusion: 矩核形式简化了对称性等变性的实现，为生物医学图像分析任务提供了高效且易用的解决方案。

Abstract: The principle of translation equivariance (if an input image is translated an
output image should be translated by the same amount), led to the development
of convolutional neural networks that revolutionized machine vision. Other
symmetries, like rotations and reflections, play a similarly critical role,
especially in biomedical image analysis, but exploiting these symmetries has
not seen wide adoption. We hypothesize that this is partially due to the
mathematical complexity of methods used to exploit these symmetries, which
often rely on representation theory, a bespoke concept in differential geometry
and group theory. In this work, we show that the same equivariance can be
achieved using a simple form of convolution kernels that we call ``moment
kernels,'' and prove that all equivariant kernels must take this form. These
are a set of radially symmetric functions of a spatial position $x$, multiplied
by powers of the components of $x$ or the identity matrix. We implement
equivariant neural networks using standard convolution modules, and provide
architectures to execute several biomedical image analysis tasks that depend on
equivariance principles: classification (outputs are invariant under orthogonal
transforms), 3D image registration (outputs transform like a vector), and cell
segmentation (quadratic forms defining ellipses transform like a matrix).

</details>


### [396] [What is Adversarial Training for Diffusion Models?](https://arxiv.org/abs/2505.21742)
*Briglia Maria Rosaria,Mujtaba Hussain Mirza,Giuseppe Lisanti,Iacopo Masi*

Key words: 对抗训练, 扩散模型, 鲁棒性, 噪声处理, 数据分布

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文研究表明，对抗训练（AT）在扩散模型（DMs）中与分类器中表现不同：分类器的AT强制输出不变性，而DMs中的AT需要等变性以保持扩散过程与数据分布对齐。AT通过增强扩散流的平滑性，提升对异常值和噪声数据的鲁棒性。

Motivation: 探索对抗训练在扩散模型中与分类器中的不同表现，并验证其在提升模型鲁棒性方面的效果。

Method: 通过向扩散训练中随机添加噪声（类似随机平滑）或对抗噪声（类似AT），无需假设噪声模型，无缝集成到扩散训练中。

Result: 在低维和高维空间的概念验证数据集及标准基准（如CIFAR-10、CelebA和LSUN Bedroom）中，模型在噪声、数据损坏和对抗攻击下表现优异。

Conclusion: 该方法不仅能处理噪声数据、极端变异性，还能防止记忆化并提升鲁棒性。

Abstract: We answer the question in the title, showing that adversarial training (AT)
for diffusion models (DMs) fundamentally differs from classifiers: while AT in
classifiers enforces output invariance, AT in DMs requires equivariance to keep
the diffusion process aligned with the data distribution. AT is a way to
enforce smoothness in the diffusion flow, improving robustness to outliers and
corrupted data. Unlike prior art, our method makes no assumptions about the
noise model and integrates seamlessly into diffusion training by adding random
noise, similar to randomized smoothing, or adversarial noise, akin to AT. This
enables intrinsic capabilities such as handling noisy data, dealing with
extreme variability such as outliers, preventing memorization, and improving
robustness. We rigorously evaluate our approach with proof-of-concept datasets
with known distributions in low- and high-dimensional space, thereby taking a
perfect measure of errors; we further evaluate on standard benchmarks such as
CIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe
noise, data corruption, and iterative adversarial attacks.

</details>


### [397] [Neural Restoration of Greening Defects in Historical Autochrome Photographs Based on Purely Synthetic Data](https://arxiv.org/abs/2505.22291)
*Saptarshi Neil Sinha,P. Julius Kuehn,Johannes Koppe,Arjan Kuijper,Michael Weinmann*

Key words: 视觉艺术修复, 绿变缺陷, 合成数据集, 生成式AI, 加权损失函数

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种基于合成数据集生成和生成式AI的方法，用于自动修复早期彩色照片中的绿变缺陷，解决了现有方法在准确还原颜色和减少人工干预方面的不足。

Motivation: 早期彩色照片（如天然彩色照片）的保存面临因老化和不当存储导致的绿变等缺陷，现有方法难以准确还原颜色且需大量人工干预。

Method: 通过合成数据集模拟绿变缺陷，改进ChaIR方法的加权损失函数，利用生成式AI进行修复。

Result: 该方法高效修复了绿变缺陷，减少了时间成本，优于现有方法。

Conclusion: 提出的方法为视觉艺术修复提供了高效且自动化的解决方案，尤其适用于绿变缺陷的修复。

Abstract: The preservation of early visual arts, particularly color photographs, is
challenged by deterioration caused by aging and improper storage, leading to
issues like blurring, scratches, color bleeding, and fading defects. In this
paper, we present the first approach for the automatic removal of greening
color defects in digitized autochrome photographs. Our main contributions
include a method based on synthetic dataset generation and the use of
generative AI with a carefully designed loss function for the restoration of
visual arts. To address the lack of suitable training datasets for analyzing
greening defects in damaged autochromes, we introduce a novel approach for
accurately simulating such defects in synthetic data. We also propose a
modified weighted loss function for the ChaIR method to account for color
imbalances between defected and non-defected areas. While existing methods
struggle with accurately reproducing original colors and may require
significant manual effort, our method allows for efficient restoration with
reduced time requirements.

</details>


### [398] [UniMoGen: Universal Motion Generation](https://arxiv.org/abs/2505.21837)
*Aliasghar Khani,Arianna Rampini,Evan Atherton,Bruno Roy*

Key words: Motion generation, diffusion model, skeleton-agnostic, UNet, controllability

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: UniMoGen是一种基于UNet的扩散模型，用于骨架无关的运动生成，能够处理不同角色的运动数据，具有高效率和可控性。

Motivation: 现有方法依赖特定骨架结构，限制了跨角色的通用性，因此需要一种骨架无关的解决方案。

Method: 采用UNet架构的扩散模型，动态处理所需的关节，支持风格和轨迹输入控制。

Result: 在100style数据集上超越现有方法，同时在跨骨架数据集上表现高效。

Conclusion: UniMoGen为角色动画提供了灵活、高效且可控的运动生成方案。

Abstract: Motion generation is a cornerstone of computer graphics, animation, gaming,
and robotics, enabling the creation of realistic and varied character
movements. A significant limitation of existing methods is their reliance on
specific skeletal structures, which restricts their versatility across
different characters. To overcome this, we introduce UniMoGen, a novel
UNet-based diffusion model designed for skeleton-agnostic motion generation.
UniMoGen can be trained on motion data from diverse characters, such as humans
and animals, without the need for a predefined maximum number of joints. By
dynamically processing only the necessary joints for each character, our model
achieves both skeleton agnosticism and computational efficiency. Key features
of UniMoGen include controllability via style and trajectory inputs, and the
ability to continue motions from past frames. We demonstrate UniMoGen's
effectiveness on the 100style dataset, where it outperforms state-of-the-art
methods in diverse character motion generation. Furthermore, when trained on
both the 100style and LAFAN1 datasets, which use different skeletons, UniMoGen
achieves high performance and improved efficiency across both skeletons. These
results highlight UniMoGen's potential to advance motion generation by
providing a flexible, efficient, and controllable solution for a wide range of
character animations.

</details>


### [399] [VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the Middle East and Beyond](https://arxiv.org/abs/2505.22353)
*Noora Al-Emadi,Ingmar Weber,Yin Yang,Ferda Ofli*

Key words: 车辆检测,卫星图像,数据集,VME,CDSI

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出VME数据集用于中东地区卫星图像车辆检测，解决现有数据集的地理偏差问题，并展示其在提升检测准确性上的效果。

Motivation: 解决现有数据集在中东地区车辆检测的地理偏差问题，提升全球车辆检测的准确性。

Method: 构建VME数据集，包含中东12国54个城市的卫星图像，并开发CDSI基准数据集。

Result: VME显著提升中东车辆检测准确性，CDSI提升全球车辆检测效果。

Conclusion: VME和CDSI数据集填补地理偏差，显著提升车辆检测模型性能。

Abstract: Detecting vehicles in satellite images is crucial for traffic management,
urban planning, and disaster response. However, current models struggle with
real-world diversity, particularly across different regions. This challenge is
amplified by geographic bias in existing datasets, which often focus on
specific areas and overlook regions like the Middle East. To address this gap,
we present the Vehicles in the Middle East (VME) dataset, designed explicitly
for vehicle detection in high-resolution satellite images from Middle Eastern
countries. Sourced from Maxar, the VME dataset spans 54 cities across 12
countries, comprising over 4,000 image tiles and more than 100,000 vehicles,
annotated using both manual and semi-automated methods. Additionally, we
introduce the largest benchmark dataset for Car Detection in Satellite Imagery
(CDSI), combining images from multiple sources to enhance global car detection.
Our experiments demonstrate that models trained on existing datasets perform
poorly on Middle Eastern images, while the VME dataset significantly improves
detection accuracy in this region. Moreover, state-of-the-art models trained on
CDSI achieve substantial improvements in global car detection.

</details>


### [400] [DAM: Domain-Aware Module for Multi-Domain Dataset Condensation](https://arxiv.org/abs/2505.22387)
*Jaehyun Choi,Gyojin Han,Dong-Jae Lee,Sunghyun Baek,Junmo Kim*

Key words: 多域数据集压缩, 领域感知模块, 频率伪域标签, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了多域数据集压缩(MDDC)方法，通过领域感知模块(DAM)嵌入跨域特征，解决现有数据集压缩方法未考虑多域数据的问题，提升了性能。

Motivation: 现代数据集通常包含多域异质数据，传统数据集压缩方法未考虑这一特性，限制了其应用范围和效果。

Method: 引入领域感知模块(DAM)，通过可学习空间掩码嵌入域特征，采用基于频率的伪域标签技术处理无显式域标签数据。

Result: 实验表明，DAM在域内、域外及跨架构场景下均优于基线数据集压缩方法。

Conclusion: MDDC和DAM有效提升了数据集压缩在多域场景下的泛化能力，且不影响单域性能。

Abstract: Dataset Condensation (DC) has emerged as a promising solution to mitigate the
computational and storage burdens associated with training deep learning
models. However, existing DC methods largely overlook the multi-domain nature
of modern datasets, which are increasingly composed of heterogeneous images
spanning multiple domains. In this paper, we extend DC and introduce
Multi-Domain Dataset Condensation (MDDC), which aims to condense data that
generalizes across both single-domain and multi-domain settings. To this end,
we propose the Domain-Aware Module (DAM), a training-time module that embeds
domain-related features into each synthetic image via learnable spatial masks.
As explicit domain labels are mostly unavailable in real-world datasets, we
employ frequency-based pseudo-domain labeling, which leverages low-frequency
amplitude statistics. DAM is only active during the condensation process, thus
preserving the same images per class (IPC) with prior methods. Experiments show
that DAM consistently improves in-domain, out-of-domain, and cross-architecture
performance over baseline dataset condensation methods.

</details>


### [401] [Can NeRFs See without Cameras?](https://arxiv.org/abs/2505.22441)
*Chaitanya Amballa,Sattwik Basu,Yu-Lin Wei,Zhijian Yang,Mehmet Ergezer,Romit Roy Choudhury*

Key words: NeRF、多路径信号、WiFi、室内平面图、信号预测

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种改进的NeRF方法，能够从多路径信号（如WiFi）中学习环境信息，并用于推断室内平面图。

Motivation: 传统NeRF通过光学射线传递的颜色信息建模3D场景，而RF或音频信号的多路径特性使其难以直接应用。论文旨在探索如何利用多路径信号推断环境信息。

Method: 通过重新设计NeRF，使其能够从多路径信号（如WiFi测量数据）中学习，并隐式推断环境（如室内平面图）。

Result: 实验表明，改进的NeRF能够从稀疏WiFi测量数据中学习到合理的室内平面图，并支持信号预测和基本光线追踪等应用。

Conclusion: 该方法证明了NeRF在多路径信号环境中的潜力，为无线信号和环境建模提供了新思路。

Abstract: Neural Radiance Fields (NeRFs) have been remarkably successful at
synthesizing novel views of 3D scenes by optimizing a volumetric scene
function. This scene function models how optical rays bring color information
from a 3D object to the camera pixels. Radio frequency (RF) or audio signals
can also be viewed as a vehicle for delivering information about the
environment to a sensor. However, unlike camera pixels, an RF/audio sensor
receives a mixture of signals that contain many environmental reflections (also
called "multipath"). Is it still possible to infer the environment using such
multipath signals? We show that with redesign, NeRFs can be taught to learn
from multipath signals, and thereby "see" the environment. As a grounding
application, we aim to infer the indoor floorplan of a home from sparse WiFi
measurements made at multiple locations inside the home. Although a difficult
inverse problem, our implicitly learnt floorplans look promising, and enables
forward applications, such as indoor signal prediction and basic ray tracing.

</details>


### [402] [NFR: Neural Feature-Guided Non-Rigid Shape Registration](https://arxiv.org/abs/2505.22445)
*Puhua Jiang,Zhangquan Chen,Mingze Sun,Ruqi Huang*

Key words: 3D shape registration, non-rigid deformation, deep learning, correspondence estimation

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出一种基于学习的3D形状配准框架，通过结合神经网络特征解决非刚性变形和部分匹配问题，无需标注数据即可训练，在多种基准测试中表现优异。

Motivation: 解决现有方法在非刚性变形和部分匹配场景中表现不佳的问题，同时减少对标注数据的依赖。

Method: 结合深度学习生成的特征与几何配准流程，动态更新匹配关系并通过一致性先验过滤，提升鲁棒性。

Result: 在少量训练数据下，框架在多个基准测试中达到了最优表现，并能处理未见过的复杂变形案例。

Conclusion: 该方法在3D形状配准中具有高效性和鲁棒性，尤其适用于非刚性和部分匹配场景。

Abstract: In this paper, we propose a novel learning-based framework for 3D shape
registration, which overcomes the challenges of significant non-rigid
deformation and partiality undergoing among input shapes, and, remarkably,
requires no correspondence annotation during training. Our key insight is to
incorporate neural features learned by deep learning-based shape matching
networks into an iterative, geometric shape registration pipeline. The
advantage of our approach is two-fold -- On one hand, neural features provide
more accurate and semantically meaningful correspondence estimation than
spatial features (e.g., coordinates), which is critical in the presence of
large non-rigid deformations; On the other hand, the correspondences are
dynamically updated according to the intermediate registrations and filtered by
consistency prior, which prominently robustify the overall pipeline. Empirical
results show that, with as few as dozens of training shapes of limited
variability, our pipeline achieves state-of-the-art results on several
benchmarks of non-rigid point cloud matching and partial shape matching across
varying settings, but also delivers high-quality correspondences between unseen
challenging shape pairs that undergo both significant extrinsic and intrinsic
deformations, in which case neither traditional registration methods nor
intrinsic methods work.

</details>


### [403] [On the Transferability and Discriminability of Repersentation Learning in Unsupervised Domain Adaptation](https://arxiv.org/abs/2505.22099)
*Wenwen Qiang,Ziyin Gu,Lingyu Si,Jiangmeng Li,Changwen Zheng,Fuchun Sun,Hui Xiong*

Key words: 无监督域适应（UDA）、对抗学习、判别性增强、Wasserstein距离、局部一致性

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出一个新颖的无监督域适应框架RLGLC，通过结合域对齐目标和判别性增强约束，解决了现有方法忽视目标域特征判别性的问题，实验证明其优于现有方法。

Motivation: 现有无监督域适应方法过度依赖域对齐和源域经验风险最小化，忽视了目标域特征的判别性，导致性能不理想。本文通过信息论分析指出这一问题，并提出需同时保证特征的迁移性和判别性。

Method: 提出基于对抗学习的框架RLGLC，整合域对齐目标和判别性增强约束。具体采用非对称松弛Wasserstein距离（AR-WWD）处理类别不平衡和语义维度加权，并通过局部一致性机制保留目标域细粒度判别信息。

Result: 在多个基准数据集上的实验表明，RLGLC显著优于现有方法，验证了同时优化迁移性和判别性的重要性。

Conclusion: 理论分析和实验结果共同表明，无监督域适应中需同时兼顾特征的迁移性和判别性，RLGLC框架为此提供了有效解决方案。

Abstract: In this paper, we addressed the limitation of relying solely on distribution
alignment and source-domain empirical risk minimization in Unsupervised Domain
Adaptation (UDA). Our information-theoretic analysis showed that this standard
adversarial-based framework neglects the discriminability of target-domain
features, leading to suboptimal performance. To bridge this
theoretical-practical gap, we defined "good representation learning" as
guaranteeing both transferability and discriminability, and proved that an
additional loss term targeting target-domain discriminability is necessary.
Building on these insights, we proposed a novel adversarial-based UDA framework
that explicitly integrates a domain alignment objective with a
discriminability-enhancing constraint. Instantiated as Domain-Invariant
Representation Learning with Global and Local Consistency (RLGLC), our method
leverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD)
to address class imbalance and semantic dimension weighting, and employs a
local consistency mechanism to preserve fine-grained target-domain
discriminative information. Extensive experiments across multiple benchmark
datasets demonstrate that RLGLC consistently surpasses state-of-the-art
methods, confirming the value of our theoretical perspective and underscoring
the necessity of enforcing both transferability and discriminability in
adversarial-based UDA.

</details>


### [404] [Scaling-up Perceptual Video Quality Assessment](https://arxiv.org/abs/2505.22543)
*Ziheng Jia,Zicheng Zhang,Zeyu Zhang,Yingji Liang,Xiaorong Zhu,Chunyi Li,Jinliang Han,Haoning Wu,Bin Wang,Haoran Zhang,Guanyu Zhu,Qiyong Zhao,Xiaohong Liu,Guangtao Zhai,Xiongkuo Min*

Key words: 视频质量评估、多模态指令库、数据扩展、互补训练、细粒度评测

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了OmniVQA框架，构建大规模VQA多模态指令库，并引入互补训练策略，模型在质量和评分任务上达到最佳性能。

Motivation: 视频质量评估领域由于标注资源稀缺和数据规模不足，尚未充分利用数据扩展定律的潜力。

Method: 构建OmniVQA-Chat-400K指令库和OmniVQA-MOS-20K数据集，设计互补训练策略，并推出细粒度评测基准。

Result: 模型在质量和评分任务上实现最佳性能。

Conclusion: OmniVQA框架成功解决VQA领域数据扩展问题，提升模型性能。

Abstract: The data scaling law has been shown to significantly enhance the performance
of large multi-modal models (LMMs) across various downstream tasks. However, in
the domain of perceptual video quality assessment (VQA), the potential of
scaling law remains unprecedented due to the scarcity of labeled resources and
the insufficient scale of datasets. To address this, we propose
\textbf{OmniVQA}, an efficient framework designed to efficiently build
high-quality, human-in-the-loop VQA multi-modal instruction databases (MIDBs).
We then scale up to create \textbf{OmniVQA-Chat-400K}, the largest MIDB in the
VQA field concurrently. Our focus is on the technical and aesthetic quality
dimensions, with abundant in-context instruction data to provide fine-grained
VQA knowledge. Additionally, we have built the \textbf{OmniVQA-MOS-20K} dataset
to enhance the model's quantitative quality rating capabilities. We then
introduce a \textbf{complementary} training strategy that effectively leverages
the knowledge from datasets for quality understanding and quality rating tasks.
Furthermore, we propose the \textbf{OmniVQA-FG (fine-grain)-Benchmark} to
evaluate the fine-grained performance of the models. Our results demonstrate
that our models achieve state-of-the-art performance in both quality
understanding and rating tasks.

</details>


### [405] [PRISM: Video Dataset Condensation with Progressive Refinement and Insertion for Sparse Motion](https://arxiv.org/abs/2505.22564)
*Jaehyun Choi,Jiwan Hur,Gyojin Han,Jaemyung Yu,Junmo Kim*

Key words: 视频数据集压缩,深度学习,时空动态,渐进优化,稀疏帧插入

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: PRISM方法通过渐进式优化和稀疏帧插入技术改进视频数据集压缩，保持时空动态交互，实现高性能与低存储需求。

Motivation: 现有视频数据压缩方法未能充分保留时空动态的相互依赖关系，导致性能受限，需新方法解决此问题。

Method: PRISM采用渐进式优化和选择性帧插入技术，结合梯度关系分析，动态调整压缩后的视频帧。

Result: 在标准视频动作识别基准测试中，PRISM性能优于现有方法，同时保持较低的存储需求。

Conclusion: PRISM通过保留时空动态交互，为资源受限环境提供了高效的视频数据压缩方案。

Abstract: Video dataset condensation has emerged as a critical technique for addressing
the computational challenges associated with large-scale video data processing
in deep learning applications. While significant progress has been made in
image dataset condensation, the video domain presents unique challenges due to
the complex interplay between spatial content and temporal dynamics. This paper
introduces PRISM, Progressive Refinement and Insertion for Sparse Motion, for
video dataset condensation, a novel approach that fundamentally reconsiders how
video data should be condensed. Unlike the previous method that separates
static content from dynamic motion, our method preserves the essential
interdependence between these elements. Our approach progressively refines and
inserts frames to fully accommodate the motion in an action while achieving
better performance but less storage, considering the relation of gradients for
each frame. Extensive experiments across standard video action recognition
benchmarks demonstrate that PRISM outperforms existing disentangled approaches
while maintaining compact representations suitable for resource-constrained
environments.

</details>


### [406] [Universal Visuo-Tactile Video Understanding for Embodied Interaction](https://arxiv.org/abs/2505.22566)
*Yifan Xie,Mingyang Li,Shoujie Li,Xingting Li,Guangyu Chen,Fei Ma,Fei Richard Yu,Wenbo Ding*

Key words: 触觉感知, 多模态, 大语言模型, VTV-LLM, 视觉触觉视频

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了VTV-LLM，首个用于通用视觉触觉视频（VTV）理解的多模态大语言模型，通过触觉感知与自然语言的结合填补了现有技术的不足。

Motivation: 现有方法在视觉和语言模态上取得了进展，但未能有效整合触觉信息，而这种信息对物理理解至关重要。论文旨在解决这一问题。

Method: 开发了VTV-LLM，包含三阶段训练范式：VTV增强、VTV-文本对齐和文本提示微调，并贡献了包含15万帧视频的VTV150K数据集。

Result: 实验证明，VTV-LLM在触觉视频理解任务中表现优异，支持特征评估、比较分析和基于场景的决策等高级触觉推理能力。

Conclusion: VTV-LLM为触觉领域的人机交互奠定了更直观的基础。

Abstract: Tactile perception is essential for embodied agents to understand physical
attributes of objects that cannot be determined through visual inspection
alone. While existing approaches have made progress in visual and language
modalities for physical understanding, they fail to effectively incorporate
tactile information that provides crucial haptic feedback for real-world
interaction. In this paper, we present VTV-LLM, the first multi-modal large
language model for universal Visuo-Tactile Video (VTV) understanding that
bridges the gap between tactile perception and natural language. To address the
challenges of cross-sensor and cross-modal integration, we contribute VTV150K,
a comprehensive dataset comprising 150,000 video frames from 100 diverse
objects captured across three different tactile sensors (GelSight Mini, DIGIT,
and Tac3D), annotated with four fundamental tactile attributes (hardness,
protrusion, elasticity, and friction). We develop a novel three-stage training
paradigm that includes VTV enhancement for robust visuo-tactile representation,
VTV-text alignment for cross-modal correspondence, and text prompt finetuning
for natural language generation. Our framework enables sophisticated tactile
reasoning capabilities including feature assessment, comparative analysis,
scenario-based decision making and so on. Experimental evaluations demonstrate
that VTV-LLM achieves superior performance in tactile video understanding
tasks, establishing a foundation for more intuitive human-machine interaction
in tactile domains.

</details>


### [407] [Tell me Habibi, is it Real or Fake?](https://arxiv.org/abs/2505.22581)
*Kartik Kuckreja,Parul Gupta,Injy Hamed,Thamar Solorio,Muhammad Haris Khan,Abhinav Dhall*

Key words: 深度伪造, 代码转换, 多语言检测, 阿拉伯语-英语, 音频-视觉

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: ArEnAV是首个大规模阿拉伯语-英语音频-视觉深度伪造数据集，包含代码转换和方言变化，旨在解决多语言深度伪造检测的挑战。

Motivation: 当前深度伪造检测研究多集中于单语言内容，忽视了多语言和代码转换语音的挑战，尤其是在阿拉伯语和英语混用的场景。

Method: 使用四种文本到语音和两种唇同步模型的新生成流程，创建了包含387k视频和765小时内容的ArEnAV数据集。

Result: 数据集在多语言多模态深度伪造检测中表现优异，与现有单语言和多语言数据集及最先进检测模型进行了对比。

Conclusion: ArEnAV数据集为深度伪造研究提供了新资源，尤其适用于代码转换和多语言场景的检测。

Abstract: Deepfake generation methods are evolving fast, making fake media harder to
detect and raising serious societal concerns. Most deepfake detection and
dataset creation research focuses on monolingual content, often overlooking the
challenges of multilingual and code-switched speech, where multiple languages
are mixed within the same discourse. Code-switching, especially between Arabic
and English, is common in the Arab world and is widely used in digital
communication. This linguistic mixing poses extra challenges for deepfake
detection, as it can confuse models trained mostly on monolingual data. To
address this, we introduce \textbf{ArEnAV}, the first large-scale
Arabic-English audio-visual deepfake dataset featuring intra-utterance
code-switching, dialectal variation, and monolingual Arabic content. It
\textbf{contains 387k videos and over 765 hours of real and fake videos}. Our
dataset is generated using a novel pipeline integrating four Text-To-Speech and
two lip-sync models, enabling comprehensive analysis of multilingual multimodal
deepfake detection. We benchmark our dataset against existing monolingual and
multilingual datasets, state-of-the-art deepfake detection models, and a human
evaluation, highlighting its potential to advance deepfake research. The
dataset can be accessed
\href{https://huggingface.co/datasets/kartik060702/ArEnAV-Full}{here}.

</details>


### [408] [Progressive Data Dropout: An Embarrassingly Simple Approach to Faster Training](https://arxiv.org/abs/2505.22342)
*Shriram M S,Xinyue Hao,Shihao Hou,Yang Lu,Laura Sevilla-Lara,Anurag Arnab,Shreyank N Gowda*

Key words: 机器学习, 数据训练, 渐进数据丢弃, 难样本挖掘, dropout

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种名为渐进数据丢弃的新训练方法，显著减少了训练所需的数据量，同时保持或提升模型精度。

Motivation: 当前机器学习依赖于大规模数据集训练，但这一方法成本高昂。尽管已有研究关注模型压缩，但数据训练方式的优化仍不足。本文旨在探索更高效的训练范式。

Method: 结合难样本挖掘和dropout的思想，提出渐进数据丢弃方法，减少有效训练轮次，不需要修改模型结构或优化器。

Result: 将有效训练轮次减少至基准的12.4%，同时精度提升了4.82%，且方法通用性强，易集成到现有流程中。

Conclusion: 渐进数据丢弃方法在减少计算成本的同时提升了性能，具有广泛应用的潜力。

Abstract: The success of the machine learning field has reliably depended on training
on large datasets. While effective, this trend comes at an extraordinary cost.
This is due to two deeply intertwined factors: the size of models and the size
of datasets. While promising research efforts focus on reducing the size of
models, the other half of the equation remains fairly mysterious. Indeed, it is
surprising that the standard approach to training remains to iterate over and
over, uniformly sampling the training dataset. In this paper we explore a
series of alternative training paradigms that leverage insights from
hard-data-mining and dropout, simple enough to implement and use that can
become the new training standard. The proposed Progressive Data Dropout reduces
the number of effective epochs to as little as 12.4% of the baseline. This
savings actually do not come at any cost for accuracy. Surprisingly, the
proposed method improves accuracy by up to 4.82%. Our approach requires no
changes to model architecture or optimizer, and can be applied across standard
training pipelines, thus posing an excellent opportunity for wide adoption.
Code can be found here: https://github.com/bazyagami/LearningWithRevision

</details>


### [409] [RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting](https://arxiv.org/abs/2505.22535)
*Mohamad Hakam Shams Eddin,Yikui Zhang,Stefan Kollet,Juergen Gall*

Key words: 深度学习, 河流流量预测, 洪水预警, 时空建模, RiverMamba

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文摘要提出了RiverMamba这一新型深度学习模型，针对现有方法在空间关联性上的不足，通过预训练和时空建模，显著提升了河流流量和洪水预测的准确性和范围。

Motivation: 当前深度学习在河流流量预测中的应用多局限于局部范围，未能充分利用水体的空间关联性，亟需新方法来提升预测的科学性和实用性。

Method: RiverMamba基于长期再分析数据预训练，结合高效的Mamba块捕捉全球范围的水道网络流动，融合ECMWF HRES气象预测并处理其误差。

Result: RiverMamba在0.05°网格上实现了长达7天的河流和洪水预测，包括极端事件，表现优于已有的AI和物理模型。

Conclusion: RiverMamba通过时空建模和全球范围的应用，为早期预警系统提供了更可靠的预测工具。

Abstract: Recent deep learning approaches for river discharge forecasting have improved
the accuracy and efficiency in flood forecasting, enabling more reliable early
warning systems for risk management. Nevertheless, existing deep learning
approaches in hydrology remain largely confined to local-scale applications and
do not leverage the inherent spatial connections of bodies of water. Thus,
there is a strong need for new deep learning methodologies that are capable of
modeling spatio-temporal relations to improve river discharge and flood
forecasting for scientific and operational applications. To address this, we
present RiverMamba, a novel deep learning model that is pretrained with
long-term reanalysis data and that can forecast global river discharge and
floods on a $0.05^\circ$ grid up to 7 days lead time, which is of high
relevance in early warning. To achieve this, RiverMamba leverages efficient
Mamba blocks that enable the model to capture global-scale channel network
routing and enhance its forecast capability for longer lead times. The forecast
blocks integrate ECMWF HRES meteorological forecasts, while accounting for
their inaccuracies through spatio-temporal modeling. Our analysis demonstrates
that RiverMamba delivers reliable predictions of river discharge, including
extreme floods across return periods and lead times, surpassing both
operational AI- and physics-based models.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [410] [Synonymous Variational Inference for Perceptual Image Compression](https://arxiv.org/abs/2505.22438)
*Zijian Liang,Kai Niu,Changshuo Wang,Jin Xu,Ping Zhang*

Key words: 同义变分推理,感知图像压缩,同义图像压缩,率失真感知

<details>
  <summary>Details</summary>

Main category: cs.IT

TL;DR: 该论文提出了一种基于同义关系的同义变分推理（SVI）方法，用于分析感知图像压缩问题，并提出了一种新的同义图像压缩（SIC）方案，通过实验验证了其有效性。

Motivation: 研究语义信息理论中语义和句法信息之间的同义关系，以此为基础重新分析感知图像压缩问题。

Method: 采用同义变分推理（SVI）方法，构建理想同义集（Synset），并通过最小化部分语义KL散度近似其后验表示。进一步提出同义图像压缩（SIC）方案并实现渐进式编解码器。

Result: 理论证明了感知图像压缩的优化方向遵循三重权衡，实验显示渐进式SIC编解码器在率失真感知性能上表现优异。

Conclusion: SVI方法和SIC方案有效解决了感知图像压缩问题，验证了理论分析的正确性。

Abstract: Recent contributions of semantic information theory reveal the set-element
relationship between semantic and syntactic information, represented as
synonymous relationships. In this paper, we propose a synonymous variational
inference (SVI) method based on this synonymity viewpoint to re-analyze the
perceptual image compression problem. It takes perceptual similarity as a
typical synonymous criterion to build an ideal synonymous set (Synset), and
approximate the posterior of its latent synonymous representation with a
parametric density by minimizing a partial semantic KL divergence. This
analysis theoretically proves that the optimization direction of perception
image compression follows a triple tradeoff that can cover the existing
rate-distortion-perception schemes. Additionally, we introduce synonymous image
compression (SIC), a new image compression scheme that corresponds to the
analytical process of SVI, and implement a progressive SIC codec to fully
leverage the model's capabilities. Experimental results demonstrate comparable
rate-distortion-perception performance using a single progressive SIC codec,
thus verifying the effectiveness of our proposed analysis method.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [411] [Revisiting Self-attention for Cross-domain Sequential Recommendation](https://arxiv.org/abs/2505.21811)
*Clark Mingxuan Ju,Leonardo Neves,Bhuvesh Kumar,Liam Collins,Tong Zhao,Yuwei Qiu,Qing Dou,Sohail Nizam,Sen Yang,Neil Shah*

Key words: 跨域顺序推荐, 自注意力机制, 多目标优化, 知识转移

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 该论文提出了一种改进的跨域顺序推荐方法AutoCDSR，通过优化自注意力机制来自动化知识转移，提升了模型性能。

Motivation: 现有的跨域顺序推荐方法虽引入额外域特定组件，但忽略了自注意力机制的潜力。论文旨在通过优化自注意力机制来提升简单模型的性能。

Method: 提出Pareto-最优自注意力机制，将跨域学习建模为多目标问题，动态最小化跨域注意力分数，并进一步提出性能更高的变体AutoCDSR+。

Result: AutoCDSR显著提升了SASRec和Bert4Rec的Recall@10和NDCG@10，分别平均提高9.8%和16.0%，以及12.0%和16.7%。

Conclusion: AutoCDSR是一种灵活且高效的插件式模块，易于实现和部署，显著提升了跨域顺序推荐的性能。

Abstract: Sequential recommendation is a popular paradigm in modern recommender
systems. In particular, one challenging problem in this space is cross-domain
sequential recommendation (CDSR), which aims to predict future behaviors given
user interactions across multiple domains. Existing CDSR frameworks are mostly
built on the self-attention transformer and seek to improve by explicitly
injecting additional domain-specific components (e.g. domain-aware module
blocks). While these additional components help, we argue they overlook the
core self-attention module already present in the transformer, a naturally
powerful tool to learn correlations among behaviors. In this work, we aim to
improve the CDSR performance for simple models from a novel perspective of
enhancing the self-attention. Specifically, we introduce a Pareto-optimal
self-attention and formulate the cross-domain learning as a multi-objective
problem, where we optimize the recommendation task while dynamically minimizing
the cross-domain attention scores. Our approach automates knowledge transfer in
CDSR (dubbed as AutoCDSR) -- it not only mitigates negative transfer but also
encourages complementary knowledge exchange among auxiliary domains. Based on
the idea, we further introduce AutoCDSR+, a more performant variant with slight
additional cost. Our proposal is easy to implement and works as a plug-and-play
module that can be incorporated into existing transformer-based recommenders.
Besides flexibility, it is practical to deploy because it brings little extra
computational overheads without heavy hyper-parameter tuning. AutoCDSR on
average improves Recall@10 for SASRec and Bert4Rec by 9.8% and 16.0% and
NDCG@10 by 12.0% and 16.7%, respectively. Code is available at
https://github.com/snap-research/AutoCDSR.

</details>


### [412] [Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking](https://arxiv.org/abs/2505.21815)
*Yunyi Zhang,Ruozhen Yang,Siqi Jiao,SeongKu Kang,Jiawei Han*

Key words: 科学论文检索,LLM,语义索引,细粒度概念

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: SemRank是一种结合LLM引导查询理解和基于概念语义索引的论文检索框架，显著提升了检索准确性。

Motivation: 解决密集检索方法在细粒度科学概念捕捉上的不足，以及当前LLM方法缺乏语料库特定知识的问题。

Method: 使用多粒度科学概念索引论文，结合LLM识别查询中的核心概念，实现精确语义匹配。

Result: 实验表明，SemRank能持续提升多种基础检索器的性能，超越现有LLM基线，并保持高效。

Conclusion: SemRank通过结合LLM和概念索引，有效提高了科学论文检索的准确性和效率。

Abstract: Scientific paper retrieval is essential for supporting literature discovery
and research. While dense retrieval methods demonstrate effectiveness in
general-purpose tasks, they often fail to capture fine-grained scientific
concepts that are essential for accurate understanding of scientific queries.
Recent studies also use large language models (LLMs) for query understanding;
however, these methods often lack grounding in corpus-specific knowledge and
may generate unreliable or unfaithful content. To overcome these limitations,
we propose SemRank, an effective and efficient paper retrieval framework that
combines LLM-guided query understanding with a concept-based semantic index.
Each paper is indexed using multi-granular scientific concepts, including
general research topics and detailed key phrases. At query time, an LLM
identifies core concepts derived from the corpus to explicitly capture the
query's information need. These identified concepts enable precise semantic
matching, significantly enhancing retrieval accuracy. Experiments show that
SemRank consistently improves the performance of various base retrievers,
surpasses strong existing LLM-based baselines, and remains highly efficient.

</details>


### [413] [Xinyu AI Search: Enhanced Relevance and Comprehensive Results with Rich Answer Presentations](https://arxiv.org/abs/2505.21849)
*Bo Tang,Junyi Zhu,Chenyang Xi,Yunhang Ge,Jiahao Wu,Yuchen Feng,Yijun Niu,Wenqiang Wei,Yu Yu,Chunyu Li,Zehao Lin,Hao Wu,Ning Liao,Yebin Yang,Jiajia Wang,Zhiyu Li,Feiyu Xiong,Jingrun Chen*

Key words: 搜索引擎, 查询分解, 生成式AI, 多源检索, 结果优化

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了一种名为Xinyu AI Search的新型搜索引擎系统，通过查询分解图逐步处理复杂查询，结合多源检索和结果优化，提升了相关性和全面性，并在评估中表现优于现有技术。

Motivation: 传统搜索引擎难以合成复杂查询的碎片化信息，生成式AI搜索引擎则在相关性、全面性和呈现方式上存在挑战。

Method: 采用查询分解图动态拆分复杂查询为子查询，结合多源聚合检索、查询扩展、过滤和重排序策略优化结果，并创新性地引入精细化引用和结果呈现方式。

Result: Xinyu AI Search在真实查询评估中优于八种现有技术，尤其在相关性、全面性和洞察力方面表现突出。

Conclusion: 该研究首次为生成式AI搜索引擎提供了端到端框架，整合了检索、生成和用户导向的呈现。

Abstract: Traditional search engines struggle to synthesize fragmented information for
complex queries, while generative AI search engines face challenges in
relevance, comprehensiveness, and presentation. To address these limitations,
we introduce Xinyu AI Search, a novel system that incorporates a
query-decomposition graph to dynamically break down complex queries into
sub-queries, enabling stepwise retrieval and generation. Our retrieval pipeline
enhances diversity through multi-source aggregation and query expansion, while
filtering and re-ranking strategies optimize passage relevance. Additionally,
Xinyu AI Search introduces a novel approach for fine-grained, precise built-in
citation and innovates in result presentation by integrating timeline
visualization and textual-visual choreography. Evaluated on recent real-world
queries, Xinyu AI Search outperforms eight existing technologies in human
assessments, excelling in relevance, comprehensiveness, and insightfulness.
Ablation studies validate the necessity of its key sub-modules. Our work
presents the first comprehensive framework for generative AI search engines,
bridging retrieval, generation, and user-centric presentation.

</details>


### [414] [Extracting Research Instruments from Educational Literature Using LLMs](https://arxiv.org/abs/2505.21855)
*Jiseung Yoo,Curran Mahowald,Meiyu Li,Wei Ai*

Key words: 大型语言模型、教育研究、信息提取、研究工具、结构化输出

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 该研究提出了一个基于大型语言模型（LLM）的系统，用于从教育领域文献中提取研究工具的详细信息，并通过多步提示和特定领域的数据模式生成结构化输出。评估表明，该系统显著优于其他方法，尤其是在识别工具名称和详细信息方面。

Motivation: 利用LLM技术改进从学术文献中提取研究工具信息的方法，以支持教育研究和政策制定。

Method: 采用LLM和多步提示技术，结合特定领域的数据模式，生成优化的结构化输出。

Result: 系统在提取研究工具名称和详细信息方面表现优异，显著优于其他方法。

Conclusion: LLM驱动的信息提取在教育领域具有潜力，能系统化组织研究工具信息，提升研究人员和教育领导者的决策效率。

Abstract: Large Language Models (LLMs) are transforming information extraction from
academic literature, offering new possibilities for knowledge management. This
study presents an LLM-based system designed to extract detailed information
about research instruments used in the education field, including their names,
types, target respondents, measured constructs, and outcomes. Using multi-step
prompting and a domain-specific data schema, it generates structured outputs
optimized for educational research. Our evaluation shows that this system
significantly outperforms other approaches, particularly in identifying
instrument names and detailed information. This demonstrates the potential of
LLM-powered information extraction in educational contexts, offering a
systematic way to organize research instrument information. The ability to
aggregate such information at scale enhances accessibility for researchers and
education leaders, facilitating informed decision-making in educational
research and policy.

</details>


### [415] [Yambda-5B -- A Large-Scale Multi-modal Dataset for Ranking And Retrieval](https://arxiv.org/abs/2505.22238)
*A. Ploshkin,V. Tytskiy,A. Pismenny,V. Baikalov,E. Taychinov,A. Permiakov,D. Burlakov,E. Krofto,N. Savushkin*

Key words: Yambda-5B, 数据集, 推荐系统, 用户交互, 音频嵌入, 评估协议

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: Yambda-5B是一个基于Yandex.Music平台的大规模开放数据集，包含47.9亿用户-项目交互和音频嵌入，特别区分了有机用户行为和推荐驱动事件，用于推荐系统研究和评估。

Motivation: 为推荐系统研究提供工业规模的开放数据集，支持算法开发和评估，促进可复现性和创新。

Method: 通过用户交互数据（显式和隐式反馈）及音频嵌入构建数据集，并引入全局时间分割评估协议。

Result: 基准测试展示了ItemKNN、iALS等标准模型和SANSA、SASRec等先进模型的性能。

Conclusion: Yambda-5B为推荐系统研究提供了高质量、真实的工业规模资源。

Abstract: We present Yambda-5B, a large-scale open dataset sourced from the
Yandex.Music streaming platform. Yambda-5B contains 4.79 billion user-item
interactions from 1 million users across 9.39 million tracks. The dataset
includes two primary types of interactions: implicit feedback (listening
events) and explicit feedback (likes, dislikes, unlikes and undislikes). In
addition, we provide audio embeddings for most tracks, generated by a
convolutional neural network trained on audio spectrograms. A key
distinguishing feature of Yambda-5B is the inclusion of the is_organic flag,
which separates organic user actions from recommendation-driven events. This
distinction is critical for developing and evaluating machine learning
algorithms, as Yandex.Music relies on recommender systems to personalize track
selection for users. To support rigorous benchmarking, we introduce an
evaluation protocol based on a Global Temporal Split, allowing recommendation
algorithms to be assessed in conditions that closely mirror real-world use. We
report benchmark results for standard baselines (ItemKNN, iALS) and advanced
models (SANSA, SASRec) using a variety of evaluation metrics. By releasing
Yambda-5B to the community, we aim to provide a readily accessible,
industrial-scale resource to advance research, foster innovation, and promote
reproducible results in recommender systems.

</details>


### [416] [UDuo: Universal Dual Optimization Framework for Online Matching](https://arxiv.org/abs/2505.22243)
*Bin Li,Diwei Liu,Zehong Hu,Jia Jia*

Key words: 在线资源分配、预算约束、动态环境、UDuo框架、自适应策略

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 提出UDuo框架，通过动态用户到达表示、自适应分配策略和在线时间序列预测，解决动态环境下的在线资源分配问题，效率和收敛速度优于传统方法。

Motivation: 传统方法依赖静态用户到达假设，在动态环境下表现不佳。需要一种能够捕捉分布变化并保证约束可行性的新框架。

Method: UDuo框架包含三个创新：(i) 动态用户到达表示向量，(ii) 自适应资源分配策略，(iii) 在线时间序列预测。

Result: 实验表明，UDuo在真实定价场景中效率和收敛速度优于传统方法，同时保证理论有效性。

Conclusion: UDuo为动态环境下的在线资源分配提供了高效且理论可靠的解决方案。

Abstract: Online resource allocation under budget constraints critically depends on
proper modeling of user arrival dynamics. Classical approaches employ
stochastic user arrival models to derive near-optimal solutions through
fractional matching formulations of exposed users for downstream allocation
tasks. However, this is no longer a reasonable assumption when the environment
changes dynamically. In this work, We propose the Universal Dual optimization
framework UDuo, a novel paradigm that fundamentally rethinks online allocation
through three key innovations: (i) a temporal user arrival representation
vector that explicitly captures distribution shifts in user arrival patterns
and resource consumption dynamics, (ii) a resource pacing learner with adaptive
allocation policies that generalize to heterogeneous constraint scenarios, and
(iii) an online time-series forecasting approach for future user arrival
distributions that achieves asymptotically optimal solutions with constraint
feasibility guarantees in dynamic environments. Experimental results show that
UDuo achieves higher efficiency and faster convergence than the traditional
stochastic arrival model in real-world pricing while maintaining rigorous
theoretical validity for general online allocation problems.

</details>


### [417] [Pre-training for Recommendation Unlearning](https://arxiv.org/abs/2505.22649)
*Guoxuan Chen,Lianghao Xia,Chao Huang*

Key words: GNN, 推荐系统, 数据遗忘, 预训练, Influence Encoder

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了一种名为UnlearnRec的新型预训练范式，旨在解决GNN推荐系统中选择性遗忘训练数据的挑战，避免了完全重新训练的需求，同时保持了模型性能。

Motivation: 现代基于GNN的推荐系统在面对用户隐私或偏好变化时，需要高效移除特定数据影响，但传统方法存在性能损失或假设不成立的问题。

Method: 通过模型无关的预训练范式UnlearnRec，利用Influence Encoder直接生成遗忘请求后的模型参数，减少微调需求。

Result: 实验表明该方法在公开基准上实现了超过10倍的速度提升，同时保持了优异的遗忘效果。

Conclusion: UnlearnRec为推荐系统提供了高效且可扩展的遗忘机制，平衡了性能和效率。

Abstract: Modern recommender systems powered by Graph Neural Networks (GNNs) excel at
modeling complex user-item interactions, yet increasingly face scenarios
requiring selective forgetting of training data. Beyond user requests to remove
specific interactions due to privacy concerns or preference changes, regulatory
frameworks mandate recommender systems' ability to eliminate the influence of
certain user data from models. This recommendation unlearning challenge
presents unique difficulties as removing connections within interaction graphs
creates ripple effects throughout the model, potentially impacting
recommendations for numerous users. Traditional approaches suffer from
significant drawbacks: fragmentation methods damage graph structure and
diminish performance, while influence function techniques make assumptions that
may not hold in complex GNNs, particularly with self-supervised or random
architectures. To address these limitations, we propose a novel model-agnostic
pre-training paradigm UnlearnRec that prepares systems for efficient unlearning
operations. Our Influence Encoder takes unlearning requests together with
existing model parameters and directly produces updated parameters of unlearned
model with little fine-tuning, avoiding complete retraining while preserving
model performance characteristics. Extensive evaluation on public benchmarks
demonstrates that our method delivers exceptional unlearning effectiveness
while providing more than 10x speedup compared to retraining approaches. We
release our method implementation at: https://github.com/HKUDS/UnlearnRec.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [418] [VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining](https://arxiv.org/abs/2505.21527)
*Jianheng Zhuo,Yifan Yang,Yiwen Shao,Yong Xu,Dong Yu,Kai Yu,Xie Chen*

Key words: 自动语音识别,低资源语言,自监督学习,VietASR

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 提出VietASR训练流程，利用无标注数据和少量标注数据提升低资源语言的ASR性能。

Motivation: 针对低资源语言（如越南语）的标注数据稀缺问题，传统ASR系统（如Whisper等）在训练成本、延迟和可访问性上表现不足。

Method: 通过多轮ASR偏置自监督学习，在大规模无标注数据上预训练，再用少量标注数据微调。

Result: 在7万小时无标注数据和50小时标注数据上训练的轻量级模型，优于Whisper Large-v3和商业ASR系统。

Conclusion: VietASR为低资源ASR提供了高效、实用的解决方案，并开源代码和模型。

Abstract: Automatic speech recognition (ASR) has made remarkable progress but heavily
relies on large-scale labeled data, which is scarce for low-resource languages
like Vietnamese. While existing systems such as Whisper, USM, and MMS achieve
promising performance, their efficacy remains inadequate in terms of training
costs, latency, and accessibility. To address these issues, we propose VietASR,
a novel ASR training pipeline that leverages vast amounts of unlabeled data and
a small set of labeled data. Through multi-iteration ASR-biased self-supervised
learning on a large-scale unlabeled dataset, VietASR offers a cost-effective
and practical solution for enhancing ASR performance. Experiments demonstrate
that pre-training on 70,000-hour unlabeled data and fine-tuning on merely
50-hour labeled data yield a lightweight but powerful ASR model. It outperforms
Whisper Large-v3 and commercial ASR systems on real-world data. Our code and
models will be open-sourced to facilitate research in low-resource ASR.

</details>


### [419] [WhisperD: Dementia Speech Recognition and Filler Word Detection with Whisper](https://arxiv.org/abs/2505.21551)
*Emmanuel Akinrintoyo,Nadine Abdelhalim,Nicole Salomons*

Key words: 痴呆症语音转录, Whisper模型, 微调, WER, FIR

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 本文通过微调Whisper模型，使用DementiaBank和内部数据集，显著降低了痴呆症语音转录的词错误率（WER）。

Motivation: 痴呆症患者（PwDs）的语音存在不规则模式和断续现象，标准语音训练的模型转录效果不佳，但正确转录对诊断和辅助技术至关重要。

Method: 使用DementiaBank和内部数据集微调Whisper模型，并加入填充词分析填充词包含率（FIR）和F1分数。

Result: 微调后的中尺寸模型WER为0.24，优于现有工作，且对未见数据和语音模式有显著泛化能力。

Conclusion: 微调Whisper能有效提升痴呆症语音转录准确率，具有实际应用潜力。

Abstract: Whisper fails to correctly transcribe dementia speech because persons with
dementia (PwDs) often exhibit irregular speech patterns and disfluencies such
as pauses, repetitions, and fragmented sentences. It was trained on standard
speech and may have had little or no exposure to dementia-affected speech.
However, correct transcription is vital for dementia speech for cost-effective
diagnosis and the development of assistive technology. In this work, we
fine-tune Whisper with the open-source dementia speech dataset (DementiaBank)
and our in-house dataset to improve its word error rate (WER). The fine-tuning
also includes filler words to ascertain the filler inclusion rate (FIR) and F1
score. The fine-tuned models significantly outperformed the off-the-shelf
models. The medium-sized model achieved a WER of 0.24, outperforming previous
work. Similarly, there was a notable generalisability to unseen data and speech
patterns.

</details>


### [420] [Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in Large Language Models for Speech Recognition](https://arxiv.org/abs/2505.22251)
*Yuan Tseng,Titouan Parcollet,Rogier van Dalen,Shucong Zhang,Sourav Bhattacharya*

Key words: LLM, speech recognition, data contamination, LibriSpeech, Common Voice

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 该论文指出，LibriSpeech和Common Voice评估集中的大量数据出现在LLM预训练语料库中，这影响了结果的可靠性。研究发现，受污染的LLM更倾向于生成训练时见过的句子，导致语音识别系统在错误率上差异不大，但对训练见过的转录赋予更高概率，强调了使用独立数据评估的重要性。

Motivation: 探讨大型语言模型（LLM）在语音任务中表现提升的可靠性，尤其是评估数据是否因预训练语料库的污染而影响结果。

Method: 比较受污染和未受污染的LLM，分析它们在生成测试句子时的表现差异，并评估语音识别系统的错误率和概率分配。

Result: 受污染的LLM倾向于生成训练见过的句子，语音识别系统在错误率上差异较小，但对见过的转录赋予更高概率。

Conclusion: LLM输出可能受少量数据污染的偏差影响，使用独立数据评估LLM语音系统至关重要。

Abstract: Recent work suggests that large language models (LLMs) can improve
performance of speech tasks compared to existing systems. To support their
claims, results on LibriSpeech and Common Voice are often quoted. However, this
work finds that a substantial amount of the LibriSpeech and Common Voice
evaluation sets appear in public LLM pretraining corpora. This calls into
question the reliability of findings drawn from these two datasets. To measure
the impact of contamination, LLMs trained with or without contamination are
compared, showing that a contaminated LLM is more likely to generate test
sentences it has seen during training. Speech recognisers using contaminated
LLMs shows only subtle differences in error rates, but assigns significantly
higher probabilities to transcriptions seen during training. Results show that
LLM outputs can be biased by tiny amounts of data contamination, highlighting
the importance of evaluating LLM-based speech systems with held-out data.

</details>


### [421] [Analysis and Evaluation of Synthetic Data Generation in Speech Dysfluency Detection](https://arxiv.org/abs/2505.22029)
*Jinming Zhang,Xuanru Zhou,Jiachen Lian,Shuhe Li,William Li,Zoe Ezzes,Rian Bogley,Lisa Wauters,Zachary Miller,Jet Vonk,Brittany Morin,Maria Gorno-Tempini,Gopala Anumanchipalli*

Key words: 语音不流畅检测, 大语言模型, 合成数据, 端到端检测框架

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 论文提出了LLM-Dys，一个利用大语言模型增强的综合性语音不流畅数据集，改进了端到端不流畅检测框架，并实现了先进性能。

Motivation: 现有语音不流畅检测方法受限于高质量标注数据的稀缺性，且现有合成数据存在不自然的韵律和上下文多样性不足的问题。

Method: 提出LLM-Dys数据集，涵盖11个词和音素层面的不流畅类别，并基于此改进端到端不流畅检测框架。

Result: 实验验证显示了最先进的性能表现。

Conclusion: LLM-Dys为临床诊断和语言评估提供了更全面的数据和改进的检测方法。

Abstract: Speech dysfluency detection is crucial for clinical diagnosis and language
assessment, but existing methods are limited by the scarcity of high-quality
annotated data. Although recent advances in TTS model have enabled synthetic
dysfluency generation, existing synthetic datasets suffer from unnatural
prosody and limited contextual diversity. To address these limitations, we
propose LLM-Dys -- the most comprehensive dysfluent speech corpus with
LLM-enhanced dysfluency simulation. This dataset captures 11 dysfluency
categories spanning both word and phoneme levels. Building upon this resource,
we improve an end-to-end dysfluency detection framework. Experimental
validation demonstrates state-of-the-art performance. All data, models, and
code are open-sourced at https://github.com/Berkeley-Speech-Group/LLM-Dys.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [422] [RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving](https://arxiv.org/abs/2505.21577)
*Huacan Wang,Ziyi Ni,Shuo Zhang,Shuo Lu,Sen Hu,Ziyang He,Chen Hu,Jiaye Lin,Yifu Guo,Yuntao Du,Pin Lyu*

Key words: 代码代理, GitHub仓库, 依赖关系, 大语言模型, RepoMaster

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: RepoMaster是一种自主代理框架，旨在探索和重用GitHub仓库以解决复杂任务，通过构建功能调用图、模块依赖图和分层代码树来提高效率，并在测试中显著优于现有基线。

Motivation: 现有框架如OpenHands和SWE-Agent在利用GitHub开源仓库时效率不足，主要因为信息过载和依赖关系复杂，且受限于LLMs的有限上下文窗口。

Method: RepoMaster通过构建功能调用图、模块依赖图和分层代码树来识别核心组件，并逐步探索和修剪信息以优化上下文使用。

Result: RepoMaster在调整后的MLE-bench上比基线OpenHands提高了110%的有效提交率，在GitTaskBench上将任务通过率从24.1%提升至62.9%，同时减少95%的token使用。

Conclusion: RepoMaster通过高效利用GitHub仓库中的模块化组件，显著提升复杂任务的解决能力，同时优化资源使用。

Abstract: The ultimate goal of code agents is to solve complex tasks autonomously.
Although large language models (LLMs) have made substantial progress in code
generation, real-world tasks typically demand full-fledged code repositories
rather than simple scripts. Building such repositories from scratch remains a
major challenge. Fortunately, GitHub hosts a vast, evolving collection of
open-source repositories, which developers frequently reuse as modular
components for complex tasks. Yet, existing frameworks like OpenHands and
SWE-Agent still struggle to effectively leverage these valuable resources.
Relying solely on README files provides insufficient guidance, and deeper
exploration reveals two core obstacles: overwhelming information and tangled
dependencies of repositories, both constrained by the limited context windows
of current LLMs. To tackle these issues, we propose RepoMaster, an autonomous
agent framework designed to explore and reuse GitHub repositories for solving
complex tasks. For efficient understanding, RepoMaster constructs function-call
graphs, module-dependency graphs, and hierarchical code trees to identify
essential components, providing only identified core elements to the LLMs
rather than the entire repository. During autonomous execution, it
progressively explores related components using our exploration tools and
prunes information to optimize context usage. Evaluated on the adjusted
MLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over
the strongest baseline OpenHands. On our newly released GitTaskBench,
RepoMaster lifts the task-pass rate from 24.1% to 62.9% while reducing token
usage by 95%. Our code and demonstration materials are publicly available at
https://github.com/wanghuacan/RepoMaster.

</details>


### [423] [Leveraging XP and CRISP-DM for Agile Data Science Projects](https://arxiv.org/abs/2505.21603)
*Andre Massahiro Shimaoka,Renato Cordeiro Ferreira,Alfredo Goldman*

Key words: 极限编程, CRISP-DM, 数据科学, 敏捷方法, 案例研究

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 该研究探讨了在敏捷数据科学项目中如何将极限编程（XP）与跨行业数据挖掘标准流程（CRISP-DM）结合，并通过案例研究证明了其可行性及效果。

Motivation: 探索如何在敏捷数据科学项目中结合XP的灵活性与CRISP-DM的结构化优势，以提升团队协作与项目效率。

Method: 采用案例研究方法，通过访谈和问卷调查收集Elo7公司数据科学团队的实践数据。

Result: 86%的团队频繁或始终应用CRISP-DM，71%在项目中采用XP实践，证实两者可以结合并提供结构化协作方式。

Conclusion: 研究证实XP与CRISP-DM在数据科学项目中的结合可行，并提出了改进建议。

Abstract: This study explores the integration of eXtreme Programming (XP) and the
Cross-Industry Standard Process for Data Mining (CRISP-DM) in agile Data
Science projects. We conducted a case study at the e-commerce company Elo7 to
answer the research question: How can the agility of the XP method be
integrated with CRISP-DM in Data Science projects? Data was collected through
interviews and questionnaires with a Data Science team consisting of data
scientists, ML engineers, and data product managers. The results show that 86%
of the team frequently or always applies CRISP-DM, while 71% adopt XP practices
in their projects. Furthermore, the study demonstrates that it is possible to
combine CRISP-DM with XP in Data Science projects, providing a structured and
collaborative approach. Finally, the study generated improvement
recommendations for the company.

</details>


### [424] [GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On Git](https://arxiv.org/abs/2505.22583)
*Tobias Lindenbauer,Egor Bogomolov,Yaroslav Zharov*

Key words: GitGoodBench, VCS, SE AI agents, benchmark, Git operations

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: GitGoodBench 是一种针对 AI 代理在版本控制系统（VCS）任务上性能评估的新基准。它弥补了现有基准（如 SWE-bench）忽视开发者关键工作流程的不足。

Motivation: 现有 SE AI 代理基准（如 SWE-bench）忽略了版本控制系统（VCS）操作等关键开发流程，限制了 AI 代理在实际开发中的全面应用潜力。

Method: 提出了 GitGoodBench 基准，涵盖从开源 Python、Java 和 Kotlin 仓库提取的三种核心 Git 场景。提供三个数据集：全面评估集（900 样本）、快速原型版本（120 样本）和训练语料（17,469 样本）。使用配备定制工具的 GPT-4o 快速原型版本上建立基线性能。

Result: 在快速原型版本上的基线性能为 21.11% 解决率。GitGoodBench 有望推动 SE 代理从单纯编程向更全面的功能迈进。

Conclusion: GitGoodBench 填补了现有基准的空白，并为未来开发更全面的 SE AI 代理提供了重要基础。

Abstract: Benchmarks for Software Engineering (SE) AI agents, most notably SWE-bench,
have catalyzed progress in programming capabilities of AI agents. However, they
overlook critical developer workflows such as Version Control System (VCS)
operations. To address this issue, we present GitGoodBench, a novel benchmark
for evaluating AI agent performance on VCS tasks. GitGoodBench covers three
core Git scenarios extracted from permissive open-source Python, Java, and
Kotlin repositories. Our benchmark provides three datasets: a comprehensive
evaluation suite (900 samples), a rapid prototyping version (120 samples), and
a training corpus (17,469 samples). We establish baseline performance on the
prototyping version of our benchmark using GPT-4o equipped with custom tools,
achieving a 21.11% solve rate overall. We expect GitGoodBench to serve as a
crucial stepping stone toward truly comprehensive SE agents that go beyond mere
programming.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [425] [Fluent but Culturally Distant: Can Regional Training Teach Cultural Understanding?](https://arxiv.org/abs/2505.21548)
*Dhruv Agarwal,Anya Shukla,Sunayana Sitaram,Aditya Vashistha*

Key words: 大语言模型, 文化适配性, 印度文化, 本土化LLM, 数据稀缺

<details>
  <summary>Details</summary>

Main category: physics.soc-ph

TL;DR: 研究发现，印度本土的LLM在文化价值观和实践上并未比全球LLM更贴近印度文化，甚至美国普通人更能代表印度文化价值。地域微调反而可能削弱文化适应性，主因是缺乏高质量的本土文化数据。

Motivation: 为了探究区域LLM是否真正反映本土文化价值观和实践，而不仅是语言适配。

Method: 通过评估五个印度本土和五个全球LLM，使用Inglehart-Welzel地图、GlobalOpinionQA、CulturalBench和NormAd等工具分析其文化和实践一致性。

Result: 印度本土模型在文化和实践任务中未优于全球模型；地域微调甚至可能削弱文化适配性，且提示策略无效。

Conclusion: 高质量的本土文化数据稀缺是主要瓶颈，需加大投入以构建真正具有文化代表性的LLM。

Abstract: Large language models (LLMs) are used around the world but exhibit Western
cultural tendencies. To address this cultural misalignment, many countries have
begun developing "regional" LLMs tailored to local communities. Yet it remains
unclear whether these models merely speak the language of their users or also
reflect their cultural values and practices. Using India as a case study, we
evaluate five Indic and five global LLMs along two key dimensions: values (via
the Inglehart-Welzel map and GlobalOpinionQA) and practices (via CulturalBench
and NormAd). Across all four tasks, we find that Indic models do not align more
closely with Indian cultural norms than global models. In fact, an average
American person is a better proxy for Indian cultural values than any Indic
model. Even prompting strategies fail to meaningfully improve alignment.
Ablations show that regional fine-tuning does not enhance cultural competence
and may in fact hurt it by impeding recall of existing knowledge. We trace this
failure to the scarcity of high-quality, untranslated, and culturally grounded
pretraining and fine-tuning data. Our study positions cultural evaluation as a
first-class requirement alongside multilingual benchmarks and offers a reusable
methodology for developers. We call for deeper investments in culturally
representative data to build and evaluate truly sovereign LLMs.

</details>


### [426] [Complexity counts: global and local perspectives on Indo-Aryan numeral systems](https://arxiv.org/abs/2505.21510)
*Chundra Cathcart*

Key words: 数字系统复杂性、印欧语系、跨语言比较、交际效率、语言学

<details>
  <summary>Details</summary>

Main category: physics.soc-ph

TL;DR: 该论文研究了印欧语系语言（如印地语、古吉拉特语和孟加拉语）中的数字系统复杂性，发现其1-99的表示形式高度不透明，远比其他语言复杂，并探讨了其背后的语言和非语言因素。

Motivation: 传统数字系统研究多关注规则的、透明的数字表达，而忽略了印欧语系语言中独特且复杂的数字表示形式。本文旨在填补这一空白，并探索导致这种复杂性持久存在的因素。

Method: 采用跨语言数据，设计并应用多个度量标准来量化数字系统的复杂性，并分析宗教、地理隔离等因素对系统复杂度的影响。

Result: 研究表明，印欧语系语言的数字系统总体上比其他语言更复杂，但内部差异显著；尽管复杂，它们仍符合跨语言的交际效率原则。

Conclusion: 印欧语系的复杂数字系统是多种因素共同作用的结果，这一维度应在跨语言数字系统研究中得到更多关注。

Abstract: The numeral systems of Indo-Aryan languages such as Hindi, Gujarati, and
Bengali are highly unusual in that unlike most numeral systems (e.g., those of
English, Chinese, etc.), forms referring to 1--99 are highly non-transparent
and are cannot be constructed using straightforward rules. As an example,
Hindi/Urdu *iky\=anve* `91' is not decomposable into the composite elements
*ek* `one' and *nave* `ninety' in the way that its English counterpart is. This
paper situates Indo-Aryan languages within the typology of cross-linguistic
numeral systems, and explores the linguistic and non-linguistic factors that
may be responsible for the persistence of complex systems in these languages.
Using cross-linguistic data from multiple databases, we develop and employ a
number of cross-linguistically applicable metrics to quantifies the complexity
of languages' numeral systems, and demonstrate that Indo-Aryan languages have
decisively more complex numeral systems than the world's languages as a whole,
though individual Indo-Aryan languages differ from each other in terms of the
complexity of the patterns they display. We investigate the factors (e.g.,
religion, geographic isolation, etc.) that underlie complexity in numeral
systems, with a focus on South Asia, in an attempt to develop an account of why
complex numeral systems developed and persisted in certain Indo-Aryan languages
but not elsewhere. Finally, we demonstrate that Indo-Aryan numeral systems
adhere to certain general pressures toward efficient communication found
cross-linguistically, despite their high complexity. We call for this somewhat
overlooked dimension of complexity to be taken seriously when discussing
general variation in cross-linguistic numeral systems.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [427] [Genetic Influences on Brain Aging: Analyzing Sex Differences in the UK Biobank using Structural MRI](https://arxiv.org/abs/2505.20344)
*Karen Ardila,Aashka Mohite,Abdoljalil Addeh,Amanda V. Tyndall,Cindy K. Barha,Quan Long,M. Ethan MacDonald*

Key words: 脑衰老, 性别差异, GWAS, 遗传因素, UK Biobank

<details>
  <summary>Details</summary>

Main category: q-bio.GN

TL;DR: 该研究通过分析40,940名UK Biobank参与者的脑影像和基因数据，揭示了大脑衰老的性别差异及其遗传基础。女性与男性分别关联到不同的基因集合和通路，提示性别分层的衰老研究及个性化干预的重要性。

Motivation: 大脑衰老存在性别差异，但其遗传因素尚不明确。研究旨在揭示性别特异性基因如何影响脑结构衰老。

Method: 采用UK Biobank的结构MRI和基因数据，计算BrainAGE并通过性别分层的GWAS及后续分析识别相关遗传变异。

Result: 女性和男性分别关联到不同的基因和通路（如女性:神经递质运输；男性:免疫炎症），同时发现了跨性别共享基因（如GMNC、OSTN）。

Conclusion: 强调性别分层研究的必要性，并为针对年龄相关认知下降的个性化干预提供潜在遗传靶点。

Abstract: Brain aging trajectories differ between males and females, yet the genetic
factors underlying these differences remain underexplored. Using structural MRI
and genotyping data from 40,940 UK Biobank participants (aged 45-83), we
computed Brain Age Gap Estimates (BrainAGE) for total brain, hippocampal, and
ventricular volumes. We conducted sex-stratified genome-wide association
studies (GWAS) and Post-GWAS analyses to identify genetic variants associated
with accelerated brain aging. Distinct gene sets emerged by sex: in females,
neurotransmitter transport and mitochondrial stress response genes were
implicated; in males, immune and inflammation-related genes dominated. Shared
genes, including GMNC and OSTN, were consistently linked to brain volumes
across sexes, suggesting core roles in neurostructural maintenance. Tissue
expression analyses revealed sex-specific enrichment in pathways tied to
neurodegeneration. These findings highlight the importance of sex-stratified
approaches in aging research and suggest genetic targets for personalized
interventions against age-related cognitive decline.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [428] [OpenReview Should be Protected and Leveraged as a Community Asset for Research in the Era of Large Language Models](https://arxiv.org/abs/2505.21537)
*Hao Sun,Yunyi Shen,Mihaela van der Schaar*

Key words: 大语言模型, OpenReview, 同行评审, 数据集, 对齐研究

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 该立场论文提出，OpenReview（一个持续更新的研究论文、同行评审、作者反驳、元评审和决策结果的数据库）应被更广泛地用于推动大语言模型（LLM）时代的研究。

Motivation: 在大语言模型时代，高质量、领域丰富且持续更新的数据集对于捕捉专家级知识、核心人类价值观和推理能力至关重要。OpenReview作为社区资产，可以促进研究进展。

Method: 论文提出了三个OpenReview可以发挥独特作用的领域：提升同行评审的质量、可扩展性和透明度；建立基于真实专家讨论的开放式基准测试；支持对齐研究。

Result: OpenReview可以为研究社区提供宝贵的数据资源，促进标准化基准测试和使用指南的开发。

Conclusion: 建议社区合作探索OpenReview的标准应用方式，并就数据使用的责任、伦理和集体管理展开更广泛的讨论。

Abstract: In the era of large language models (LLMs), high-quality, domain-rich, and
continuously evolving datasets capturing expert-level knowledge, core human
values, and reasoning are increasingly valuable. This position paper argues
that OpenReview -- the continually evolving repository of research papers, peer
reviews, author rebuttals, meta-reviews, and decision outcomes -- should be
leveraged more broadly as a core community asset for advancing research in the
era of LLMs. We highlight three promising areas in which OpenReview can
uniquely contribute: enhancing the quality, scalability, and accountability of
peer review processes; enabling meaningful, open-ended benchmarks rooted in
genuine expert deliberation; and supporting alignment research through
real-world interactions reflecting expert assessment, intentions, and
scientific values. To better realize these opportunities, we suggest the
community collaboratively explore standardized benchmarks and usage guidelines
around OpenReview, inviting broader dialogue on responsible data use, ethical
considerations, and collective stewardship.

</details>


### [429] [Enhancing Selection of Climate Tech Startups with AI -- A Case Study on Integrating Human and AI Evaluations in the ClimaTech Great Global Innovation Challenge](https://arxiv.org/abs/2505.21562)
*Jennifer Turliuk,Alejandro Sevilla,Daniela Gorza,Tod Hynes*

Key words: 气候科技，初创企业评估，人工智能，混合模型

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 本文介绍了ClimaTech全球创新挑战赛通过结合人类和AI评估来选择气候科技初创企业的方法。

Motivation: 旨在通过混合模型提高初创企业选择的准确性和效率，同时减少偏见。

Method: 采用三阶段方法：初始AI评审、半决赛人类评审和决赛混合权重评估。

Result: AI与人类评委评分呈现中等正相关（Spearman's=0.47），最终选出的初创企业中，人类和AI评分较高的企业表现优异。

Conclusion: 混合模型可以优化初创企业评估，未来可作为类似竞赛的参考框架。

Abstract: This case study examines the ClimaTech Great Global Innovation Challenge's
approach to selecting climate tech startups by integrating human and AI
evaluations. The competition aimed to identify top startups and enhance the
accuracy and efficiency of the selection process through a hybrid model.
Research shows data-driven approaches help VC firms reduce bias and improve
decision-making. Machine learning models have outperformed human investors in
deal screening, helping identify high-potential startups. Incorporating AI
aimed to ensure more equitable and objective evaluations.
  The methodology included three phases: initial AI review, semi-finals judged
by humans, and finals using a hybrid weighting. In phase one, 57 applications
were scored by an AI tool built with StackAI and OpenAI's GPT-4o, and the top
36 advanced. In the semi-finals, human judges, unaware of AI scores, evaluated
startups on team quality, market potential, and technological innovation. Each
score - human or AI - was weighted equally, resulting in 75 percent human and
25 percent AI influence. In the finals, with five human judges, weighting
shifted to 83.3 percent human and 16.7 percent AI. There was a moderate
positive correlation between AI and human scores - Spearman's = 0.47 -
indicating general alignment with key differences. Notably, the final four
startups, selected mainly by humans, were among those rated highest by the AI.
This highlights the complementary nature of AI and human judgment. The study
shows that hybrid models can streamline and improve startup assessments. The
ClimaTech approach offers a strong framework for future competitions by
combining human expertise with AI capabilities.

</details>


### [430] [Beyond Explainability: The Case for AI Validation](https://arxiv.org/abs/2505.21570)
*Dalit Ken-Dror Feldman,Daniel Benoliel*

Key words: AI治理、验证、可解释性、监管框架、高风险AI

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 论文提出验证应成为AI监管的核心支柱，替代传统的可解释性方法，以解决AI系统在不透明领域（如医疗、金融）的可靠性问题。作者提出基于有效性和可解释性的分类法，并设计了一个面向部署前后的验证框架。

Motivation: 当前AI监管过度依赖可解释性，但在高风险领域（如医疗、司法）中，可解释性可能不切实际，验证（确保AI输出的可靠性和一致性）更具实用性和扩展性。

Method: 通过有效性-可解释性分类法分析AI系统，并比较欧盟、美国、英国和中国的监管方法，提出基于验证的治理框架，包括部署前后验证、第三方审计和标准化。

Result: 验证框架能在可解释性受限时仍增强社会信任、公平性和安全性，同时通过责任激励机制平衡创新与问责。

Conclusion: 验证作为一种更可行的监管手段，为高性能但不透明的AI系统提供了治理路线图，兼顾创新与社会责任。

Abstract: Artificial Knowledge (AK) systems are transforming decision-making across
critical domains such as healthcare, finance, and criminal justice. However,
their growing opacity presents governance challenges that current regulatory
approaches, focused predominantly on explainability, fail to address
adequately. This article argues for a shift toward validation as a central
regulatory pillar. Validation, ensuring the reliability, consistency, and
robustness of AI outputs, offers a more practical, scalable, and risk-sensitive
alternative to explainability, particularly in high-stakes contexts where
interpretability may be technically or economically unfeasible. We introduce a
typology based on two axes, validity and explainability, classifying AK systems
into four categories and exposing the trade-offs between interpretability and
output reliability. Drawing on comparative analysis of regulatory approaches in
the EU, US, UK, and China, we show how validation can enhance societal trust,
fairness, and safety even where explainability is limited. We propose a
forward-looking policy framework centered on pre- and post-deployment
validation, third-party auditing, harmonized standards, and liability
incentives. This framework balances innovation with accountability and provides
a governance roadmap for responsibly integrating opaque, high-performing AK
systems into society.

</details>


### [431] [AITEE -- Agentic Tutor for Electrical Engineering](https://arxiv.org/abs/2505.21582)
*Christopher Knievel,Alexander Bernhardt,Christian Bernhardt*

Key words: 智能辅导系统, 大型语言模型, 电气工程, 电路重建, 检索增强生成, Spice仿真

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: AITEE 是一种基于智能代理的电气工程辅导系统，结合大型语言模型（LLM）和电路重建技术，通过个性化支持和苏格拉底式对话提升学生学习效果。实验证明其在领域知识应用上显著优于基线方法。

Motivation: 针对现有大型语言模型在电气工程特定问题（如电路分析）上的不足，开发一种能够提供个性化支持并促进自主学习的智能辅导系统。

Method: 1. 结合检索增强生成（RAG）和图相似度测量从教材中提取上下文；2. 通过并行Spice仿真提升解决方案的准确性；3. 采用苏格拉底式对话引导学生自主思考。

Result: AITEE 在领域知识应用中显著优于基线方法，即使中等规模的LLM模型也能取得可接受的性能。

Conclusion: AITEE 展示了代理型辅导系统在电气工程教育中实现可扩展、个性化且高效学习环境的潜力。

Abstract: Intelligent tutoring systems combined with large language models offer a
promising approach to address students' diverse needs and promote
self-efficacious learning. While large language models possess good
foundational knowledge of electrical engineering basics, they remain
insufficiently capable of addressing specific questions about electrical
circuits. In this paper, we present AITEE, an agent-based tutoring system for
electrical engineering designed to accompany students throughout their learning
process, offer individualized support, and promote self-directed learning.
AITEE supports both hand-drawn and digital circuits through an adapted circuit
reconstruction process, enabling natural interaction with students. Our novel
graph-based similarity measure identifies relevant context from lecture
materials through a retrieval augmented generation approach, while parallel
Spice simulation further enhances accuracy in applying solution methodologies.
The system implements a Socratic dialogue to foster learner autonomy through
guided questioning. Experimental evaluations demonstrate that AITEE
significantly outperforms baseline approaches in domain-specific knowledge
application, with even medium-sized LLM models showing acceptable performance.
Our results highlight the potential of agentic tutors to deliver scalable,
personalized, and effective learning environments for electrical engineering
education.

</details>


### [432] [Public Discourse Sandbox: Facilitating Human and AI Digital Communication Research](https://arxiv.org/abs/2505.21604)
*Kristina Radivojevic,Caleb Reinking,Shaun Whitfield,Paul Brenner*

Key words: 社交媒体, AI行为, 数字讨论, 研究平台, PDS

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 介绍了一个名为Public Discourse Sandbox (PDS)的数字讨论研究平台，用于安全研究和测试AI行为及其影响。

Motivation: 解决社交媒体数据获取困难、不可靠及伦理问题，提供可控的研究机制。

Method: PDS平台支持通过提示工程、检索增强生成(RAG)和微调等技术研究AI行为。

Result: 提供了PDS的托管版本和开源代码，支持研究和社区协作。

Conclusion: PDS为研究AI行为及其影响提供了安全且可扩展的平台。

Abstract: Social media serves as a primary communication and information dissemination
platform for major global events, entertainment, and niche or topically focused
community discussions. Therefore, it represents a valuable resource for
researchers who aim to understand numerous questions. However, obtaining data
can be difficult, expensive, and often unreliable due to the presence of bots,
fake accounts, and manipulated content. Additionally, there are ethical
concerns if researchers decide to conduct an online experiment without
explicitly notifying social media users about their intent. There is a need for
more controlled and scalable mechanisms to evaluate the impacts of digital
discussion interventions on audiences. We introduce the Public Discourse
Sandbox (PDS), which serves as a digital discourse research platform for
human-AI as well as AI-AI discourse research, testing, and training. PDS
provides a safe and secure space for research experiments that are not viable
on public, commercial social media platforms. Its main purpose is to enable the
understanding of AI behaviors and the impacts of customized AI participants via
techniques such as prompt engineering, retrieval-augmented generation (RAG),
and fine-tuning. We provide a hosted live version of the sandbox to support
researchers as well as the open-sourced code on GitHub for community
collaboration and contribution.

</details>


### [433] [Expert Survey: AI Reliability & Security Research Priorities](https://arxiv.org/abs/2505.21664)
*Joe O'Brien,Jeremy Dolan,Jay Kim,Jonah Dykhuizen,Jeba Sania,Sebastian Becker,Jam Kraprayoon,Cara Labrador*

Key words: AI可靠性、安全性、专家调查、数据驱动、资源分配

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 专家调查AI可靠性与安全研究，确定优先研究方向以指导投资。

Motivation: 确保AI系统的广泛益处能安全实现并避免严重危害。

Method: 调查53位专家，涵盖105个AI可靠性与安全研究领域。

Result: 首次量化专家优先级并生成数据驱动的潜在影响排名。

Conclusion: 排名可支持资源有效部署于AI可靠性与安全研究的证据决策。

Abstract: Our survey of 53 specialists across 105 AI reliability and security research
areas identifies the most promising research prospects to guide strategic AI
R&D investment. As companies are seeking to develop AI systems with broadly
human-level capabilities, research on reliability and security is urgently
needed to ensure AI's benefits can be safely and broadly realized and prevent
severe harms. This study is the first to quantify expert priorities across a
comprehensive taxonomy of AI safety and security research directions and to
produce a data-driven ranking of their potential impact. These rankings may
support evidence-based decisions about how to effectively deploy resources
toward AI reliability and security research.

</details>


### [434] [Responsible Data Stewardship: Generative AI and the Digital Waste Problem](https://arxiv.org/abs/2505.21720)
*Vanessa Utz*

Key words: 生成式AI, 数字浪费, 环境可持续性, AI伦理, 数据管理

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 该论文探讨生成式AI系统中长期被忽视的可持续性问题——数字浪费，提出将其作为AI伦理的一部分，并借鉴其他领域的资源管理方法，提出具体建议以减少数据存储对环境的长期影响。

Motivation: 生成式AI的广泛使用导致大量合成数据的产生和存储，但其环境成本（如数字浪费）尚未得到足够关注。论文旨在填补这一空白，将环境可持续性纳入AI伦理框架。

Method: 通过分析其他学科的数字资源管理方法，识别可迁移至AI领域的策略，并提出了研究方向、技术干预和文化转变的具体建议。

Result: 论文扩展了AI伦理的范畴，将环境正义纳入考量，并提出了减少数字浪费的可行方案。

Conclusion: 生成式AI的开发需兼顾环境可持续性，数字浪费应成为伦理框架的核心议题之一。

Abstract: As generative AI systems become widely adopted, they enable unprecedented
creation levels of synthetic data across text, images, audio, and video
modalities. While research has addressed the energy consumption of model
training and inference, a critical sustainability challenge remains
understudied: digital waste. This term refers to stored data that consumes
resources without serving a specific (and/or immediate) purpose. This paper
presents this terminology in the AI context and introduces digital waste as an
ethical imperative within (generative) AI development, positioning
environmental sustainability as core for responsible innovation. Drawing from
established digital resource management approaches, we examine how other
disciplines manage digital waste and identify transferable approaches for the
AI community. We propose specific recommendations encompassing re-search
directions, technical interventions, and cultural shifts to mitigate the
environmental consequences of in-definite data storage. By expanding AI ethics
beyond immediate concerns like bias and privacy to include inter-generational
environmental justice, this work contributes to a more comprehensive ethical
framework that considers the complete lifecycle impact of generative AI
systems.

</details>


### [435] [From prosthetic memory to prosthetic denial: Auditing whether large language models are prone to mass atrocity denialism](https://arxiv.org/abs/2505.21753)
*Roberto Ulloa,Eve M. Zucker,Daniel Bultmann,David J. Simon,Mykola Makhortykh*

Key words: 大语言模型,历史记忆,否认主义,替代记忆,伦理

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 研究探讨大语言模型（LLMs）如何影响历史叙述，特别是大规模暴行记忆的传播与扭曲，比较五种LLMs在四种历史案例中的反应，发现其对部分事件的回应可能助长否认主义。

Motivation: 探究LLMs在历史记忆传播中的作用，评估其对暴行记忆的潜在扭曲或否认风险。

Method: 对五种LLMs（Claude、GPT、Llama、Mixtral和Gemini）在四个历史案例（乌克兰大饥荒、纳粹大屠杀、柬埔寨种族灭绝和卢旺达大屠杀）中进行对比审计，使用英语和相关语言提问。

Result: LLMs对广泛记录的事件（如纳粹大屠杀）反应准确，但对少数案例（如柬埔寨种族灭绝）易受否认主义框架影响，反映了训练数据的影响。

Conclusion: LLMs虽扩展了“替代记忆”概念，但未受监管的使用可能强化历史否认主义，引发技术伦理问题。

Abstract: The proliferation of large language models (LLMs) can influence how
historical narratives are disseminated and perceived. This study explores the
implications of LLMs' responses on the representation of mass atrocity memory,
examining whether generative AI systems contribute to prosthetic memory, i.e.,
mediated experiences of historical events, or to what we term "prosthetic
denial," the AI-mediated erasure or distortion of atrocity memories. We argue
that LLMs function as interfaces that can elicit prosthetic memories and,
therefore, act as experiential sites for memory transmission, but also
introduce risks of denialism, particularly when their outputs align with
contested or revisionist narratives. To empirically assess these risks, we
conducted a comparative audit of five LLMs (Claude, GPT, Llama, Mixtral, and
Gemini) across four historical case studies: the Holodomor, the Holocaust, the
Cambodian Genocide, and the genocide against the Tutsis in Rwanda. Each model
was prompted with questions addressing common denialist claims in English and
an alternative language relevant to each case (Ukrainian, German, Khmer, and
French). Our findings reveal that while LLMs generally produce accurate
responses for widely documented events like the Holocaust, significant
inconsistencies and susceptibility to denialist framings are observed for more
underrepresented cases like the Cambodian Genocide. The disparities highlight
the influence of training data availability and the probabilistic nature of LLM
responses on memory integrity. We conclude that while LLMs extend the concept
of prosthetic memory, their unmoderated use risks reinforcing historical
denialism, raising ethical concerns for (digital) memory preservation, and
potentially challenging the advantageous role of technology associated with the
original values of prosthetic memory.

</details>


### [436] [CiRL: Open-Source Environments for Reinforcement Learning in Circular Economy and Net Zero](https://arxiv.org/abs/2505.21536)
*Federico Zocco,Andrea Corti,Monica Malvezzi*

Key words: 深度强化学习，循环经济，材料循环性，热力学网络，Stable-Baselines3

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 介绍了一个名为CiRL的深度强化学习库，专注于固体和流体材料的循环性，旨在应对气候变化和关键材料供应的不确定性。

Motivation: 解决原材料需求增加和短期碳减排方案不足的挑战，推动循环经济的实现。

Method: 基于热力学材料网络的动态系统形式，利用Stable-Baselines3库开发DRL算法，并通过Google Colaboratory实现跨学科可访问性。

Result: 成功开发了CiRL库，公开可用，支持循环经济研究。

Conclusion: CiRL为材料循环性设计提供了新的DRL工具，促进了跨学科研究协作。

Abstract: The demand of finite raw materials will keep increasing as they fuel modern
society. Simultaneously, solutions for stopping carbon emissions in the short
term are not available, thus making the net zero target extremely challenging
to achieve at scale. The circular economy (CE) paradigm is gaining attention as
a solution to address climate change and the uncertainties of supplies of
critical materials. Hence, in this paper, we introduce CiRL, a deep
reinforcement learning (DRL) library of environments focused on the circularity
of both solid and fluid materials. The integration of DRL into the design of
material circularity is possible thanks to the formalism of thermodynamical
material networks, which is underpinned by compartmental dynamical
thermodynamics. Along with the focus on circularity, this library has three
more features: the new CE-oriented environments are in the state-space form,
which is typically used in dynamical systems analysis and control designs; it
is based on a state-of-the-art Python library of DRL algorithms, namely,
Stable-Baselines3; and it is developed in Google Colaboratory to be accessible
to researchers from different disciplines and backgrounds as is often the case
for circular economy researchers and engineers. CiRL is publicly available.

</details>


### [437] [From Coders to Critics: Empowering Students through Peer Assessment in the Age of AI Copilots](https://arxiv.org/abs/2505.22093)
*Santiago Berrezueta-Guzman,Stephan Krusche,Stefan Wagner*

Key words: 同伴互评, AI编程助手, 编程教育, 学术诚信, 评估方法

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 论文探讨了在AI编程助手普及背景下，结构化同伴互评作为替代传统评分方法的潜力，并通过实证研究验证其在编程课中的效果。

Motivation: 面对AI辅助编程对学术诚信和评估方法的挑战，研究探索了结构化同伴互评作为解决方案的可行性。

Method: 在大型编程入门课程中实施基于量规的匿名同伴互评，学生互评2D游戏项目，并通过统计指标与教师评分对比。

Result: 同伴互评能中等程度接近教师评分，同时提升学生参与度、评价能力和反馈积极性。

Conclusion: 结构化同伴互评是实现可扩展、可信评估系统的有效策略，尤其适用于AI辅助编程时代。

Abstract: The rapid adoption of AI powered coding assistants like ChatGPT and other
coding copilots is transforming programming education, raising questions about
assessment practices, academic integrity, and skill development. As educators
seek alternatives to traditional grading methods susceptible to AI enabled
plagiarism, structured peer assessment could be a promising strategy. This
paper presents an empirical study of a rubric based, anonymized peer review
process implemented in a large introductory programming course.
  Students evaluated each other's final projects (2D game), and their
assessments were compared to instructor grades using correlation, mean absolute
error, and root mean square error (RMSE). Additionally, reflective surveys from
47 teams captured student perceptions of fairness, grading behavior, and
preferences regarding grade aggregation. Results show that peer review can
approximate instructor evaluation with moderate accuracy and foster student
engagement, evaluative thinking, and interest in providing good feedback to
their peers. We discuss these findings for designing scalable, trustworthy peer
assessment systems to face the age of AI assisted coding.

</details>


### [438] [New Tools are Needed for Tracking Adherence to AI Model Behavioral Use Clauses](https://arxiv.org/abs/2505.22287)
*Daniel McDuff,Tim Korjakow,Kevin Klyman,Danish Contractor*

Key words: AI基础模型, 许可证, 负责任AI, 行为条款, HuggingFace

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 论文研究了AI基础模型发布中的许可证问题，开发了许可证生成工具并分析了300多个定制许可证及170万HuggingFace许可证，发现行为条款许可证的普及增加，呼吁开发工具跟踪其采用以确保负责任使用。

Motivation: AI基础模型的快速发展伴随着对其潜在滥用的担忧，当前的许可证机制旨在限制风险但缺乏跟踪工具，研究试图填补这一空白。

Method: 开发自定义AI许可证生成工具，分析300个定制许可证及HuggingFace上170万模型许可证，定量与定性结合研究采用趋势。

Result: 行为条款许可证采用率增长，工具需求显著，条款配置趋同，表明社区对负责任AI使用的关注。

Conclusion: 亟需开发工具跟踪许可证采用与遵守情况，以确保其实现促进AI负责任使用的目标。

Abstract: Foundation models have had a transformative impact on AI. A combination of
large investments in research and development, growing sources of digital data
for training, and architectures that scale with data and compute has led to
models with powerful capabilities. Releasing assets is fundamental to
scientific advancement and commercial enterprise. However, concerns over
negligent or malicious uses of AI have led to the design of mechanisms to limit
the risks of the technology. The result has been a proliferation of licenses
with behavioral-use clauses and acceptable-use-policies that are increasingly
being adopted by commonly used families of models (Llama, Gemma, Deepseek) and
a myriad of smaller projects. We created and deployed a custom AI licenses
generator to facilitate license creation and have quantitatively and
qualitatively analyzed over 300 customized licenses created with this tool.
Alongside this we analyzed 1.7 million models licenses on the HuggingFace model
hub. Our results show increasing adoption of these licenses, interest in tools
that support their creation and a convergence on common clause configurations.
In this paper we take the position that tools for tracking adoption of, and
adherence to, these licenses is the natural next step and urgently needed in
order to ensure they have the desired impact of ensuring responsible use.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [439] [HelixDesign-Binder: A Scalable Production-Grade Platform for Binder Design Built on HelixFold3](https://arxiv.org/abs/2505.21873)
*Jie Gao,Jun Li,Jing Hu,Shanzhuo Zhang,Kunrui Zhu,Yueyang Huang,Xiaonan Zhang,Xiaomin Fang*

Key words: 蛋白质结合剂、HelixFold3、高通量、Baidu Cloud、多维度评分

<details>
  <summary>Details</summary>

Main category: q-bio.BM

TL;DR: HelixDesign-Binder是一个基于HelixFold3的高通量蛋白质结合剂设计平台，整合了从骨架生成到多维评分的全流程，显著提升了设计效率和成果质量。

Motivation: 传统的蛋白质结合剂设计流程分散、计算成本高且工具集成复杂，亟需一个高效、可扩展且用户友好的解决方案。

Method: 平台基于HelixFold3，结合Baidu Cloud的高性能计算资源，通过自动化流程实现骨架生成、序列设计、结构评估和多维评分。

Result: 在六种蛋白质靶标上测试显示，平台能生成多样且高质量的结合剂，部分设计在预测结合亲和力上优于现有验证方案。

Conclusion: HelixDesign-Binder为抗体和蛋白质结合剂开发提供了高效工具，支持学术和工业应用。

Abstract: Protein binder design is central to therapeutics, diagnostics, and synthetic
biology, yet practical deployment remains challenging due to fragmented
workflows, high computational costs, and complex tool integration. We present
HelixDesign-Binder, a production-grade, high-throughput platform built on
HelixFold3 that automates the full binder design pipeline, from backbone
generation and sequence design to structural evaluation and multi-dimensional
scoring. By unifying these stages into a scalable and user-friendly system,
HelixDesign-Binder enables efficient exploration of binder candidates with
favorable structural, energetic, and physicochemical properties. The platform
leverages Baidu Cloud's high-performance infrastructure to support large-scale
design and incorporates advanced scoring metrics, including ipTM, predicted
binding free energy, and interface hydrophobicity. Benchmarking across six
protein targets demonstrates that HelixDesign-Binder reliably produces diverse
and high-quality binders, some of which match or exceed validated designs in
predicted binding affinity. HelixDesign-Binder is accessible via an interactive
web interface in PaddleHelix platform, supporting both academic research and
industrial applications in antibody and protein binder development.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [440] [Provably Robust Training of Quantum Circuit Classifiers Against Parameter Noise](https://arxiv.org/abs/2505.18478)
*Lucas Tecot,Di Luo,Cho-Jui Hsieh*

Key words: quantum computing, noise, training theory, evolutionary strategies, robustness

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 提出了一种可证明抗噪声的训练理论和算法，用于增强参数化量子电路分类器的鲁棒性。该方法类似于进化策略，能抵抗参数噪声且易于调整。

Motivation: 量子计算虽具潜力，但噪音问题限制了算法可靠性，需开发理论和方法提升抗噪声能力。

Method: 提出一种可证明的抗噪声训练算法，与进化策略类似，适用于多种量子电路，且对优化算法只需最小调整。

Result: 在量子相位分类任务中验证了有效性，为近量子计算机的实用化提供了新方向。

Conclusion: 该工作为量子计算噪声问题提供了理论支持，推动了近期量子计算机的实用化发展。

Abstract: Advancements in quantum computing have spurred significant interest in
harnessing its potential for speedups over classical systems. However, noise
remains a major obstacle to achieving reliable quantum algorithms. In this
work, we present a provably noise-resilient training theory and algorithm to
enhance the robustness of parameterized quantum circuit classifiers. Our
method, with a natural connection to Evolutionary Strategies, guarantees
resilience to parameter noise with minimal adjustments to commonly used
optimization algorithms. Our approach is function-agnostic and adaptable to
various quantum circuits, successfully demonstrated in quantum phase
classification tasks. By developing provably guaranteed optimization theory
with quantum circuits, our work opens new avenues for practical, robust
applications of near-term quantum computers.

</details>


### [441] [Physics-inspired Generative AI models via real hardware-based noisy quantum diffusion](https://arxiv.org/abs/2505.22193)
*Marco Parigi,Stefano Martina,Francesco Aldo Venturelli,Filippo Caruso*

Key words: 量子扩散模型、生成AI、量子随机行走、量子噪声、MNIST图像

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 量子扩散模型（QDMs）旨在利用量子特性提升经典生成AI的性能，但现有算法因短期量子设备的限制难以扩展。本文提出两种物理启发的协议：一是通过量子随机行走形式在正向过程中结合量子与经典动力学，生成统计更稳健的模型（MNIST图像FID更低）；二是在IBM四量子比特硬件上利用固有噪声生成图像，为量子噪声作为资源（非修正）的大规模量子生成AI算法开辟新路径。

Motivation: 解决现有量子扩散模型在短期量子设备上难以扩展的问题，并探索量子噪声作为生成资源的潜力，而非视为需修正的缺陷。

Method: 1. 量子随机行走形式的正向动力学混合协议；2. 在真实IBM量子硬件（4量子比特）上利用固有噪声实现图像生成算法。

Result: 协议一生成的MNIST图像FID低于纯经典动力学；协议二验证了量子噪声可作为生成资源的可行性。

Conclusion: 为大规模量子生成AI提供了新思路，证明量子噪声的利用价值。

Abstract: Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI
that aims to use quantum properties to improve the performances of their
classical counterparts. However, existing algorithms are not easily scalable
due to the limitations of near-term quantum devices. Following our previous
work on QDMs, here we propose and implement two physics-inspired protocols. In
the first, we use the formalism of quantum stochastic walks, showing that a
specific interplay of quantum and classical dynamics in the forward process
produces statistically more robust models generating sets of MNIST images with
lower Fr\'echet Inception Distance (FID) than using totally classical dynamics.
In the second approach, we realize an algorithm to generate images by
exploiting the intrinsic noise of real IBM quantum hardware with only four
qubits. Our work could be a starting point to pave the way for new scenarios
for large-scale algorithms in quantum Generative AI, where quantum noise is
neither mitigated nor corrected, but instead exploited as a useful resource.

</details>


### [442] [Hyperbolic recurrent neural network as the first type of non-Euclidean neural quantum state ansatz](https://arxiv.org/abs/2505.22083)
*H. L. Dao*

Key words: 双曲GRU, 神经量子态, 变分蒙特卡洛, 量子多体系统, 非欧几里得

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 首次提出了一种基于双曲GRU的非欧几里得神经量子态（NQS）方案，在变分蒙特卡洛方法中用于量子多体系统的基态波函数近似。双曲GRU在多种量子自旋模型中表现优于或等效于欧几里得RNN/GRU，尤其在具有层级结构的哈密顿量中表现更佳。

Motivation: 探索非欧几里得神经量子态（NQS）在量子多体系统中的应用潜力，特别是在具有层级相互作用的系统中。

Method: 提出基于双曲GRU的NQS方案，并与传统欧几里得RNN/GRU进行性能对比，测试模型包括TFIM和一维Heisenberg系统。

Result: 双曲GRU在所有测试中表现优于或等效于欧几里得RNN/GRU，在具有层级结构的哈密顿量中显著领先。

Conclusion: 双曲GRU是首个成功应用于量子多体系统的非欧几里得NQS方案，有望在未来研究中进一步扩展。

Abstract: In this work, we introduce the first type of non-Euclidean neural quantum
state (NQS) ansatz, in the form of the hyperbolic GRU (a variant of recurrent
neural networks (RNNs)), to be used in the Variational Monte Carlo method of
approximating the ground state wavefunction for quantum many-body systems. In
particular, we examine the performances of NQS ansatzes constructed from both
conventional or Euclidean RNN/GRU and from hyperbolic GRU in the prototypical
settings of the one- and two-dimensional transverse field Ising models (TFIM)
of up to 100 spins and the one-dimensional Heisenberg $J_1J_2$ and $J_1J_2J_3$
systems of up 50 spins. By virtue of the fact that, for all of the experiments
performed in this work, hyperbolic GRU can yield performances comparable to or
better than Euclidean RNNs, which have been extensively studied in these
settings in the literature, our work is a proof-of-concept for the viability of
hyperbolic GRU as the first type of non-Euclidean NQS ansatz for quantum
many-body systems. Furthermore, in settings where the Hamiltonian displays a
clear hierarchical interaction structure, such as the 1D Heisenberg $J_1J_2$ &
$J_1J_2J_3$ systems with the 1st, 2nd and even 3rd nearest neighbor
interactions, our results show that hyperbolic GRU definitively outperforms its
Euclidean version in all instances. The fact that these results are reminiscent
of the established ones from natural language processing where hyperbolic GRU
almost always outperforms Euclidean RNNs when the training data exhibit a
tree-like or hierarchical structure leads us to hypothesize that hyperbolic GRU
NQS ansatz would likely outperform Euclidean RNN/GRU NQS ansatz in quantum spin
systems that involve different degrees of nearest neighbor interactions.
Finally, with this work, we hope to initiate future studies of other types of
non-Euclidean NQS beyond hyperbolic GRU.

</details>


### [443] [Depth-Based Matrix Classification for the HHL Quantum Algorithm](https://arxiv.org/abs/2505.22454)
*Mark Danza,Sonia Lopez Alarcon,Cory Merkel*

Key words: 量子计算, HHL算法, 机器学习, 多层感知器, 线性方程组

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 本文探讨了在量子计算接近纠错时代时，如何利用机器学习分类器判断HHL算法是否适用于特定线性方程组问题。研究表明，通过多层感知器可以实现准确分类，但需注意训练数据分布和分类器参数的精心设计。

Motivation: 在量子计算的后NISQ时代，HHL算法虽在理论上具有广泛适用性，但在实际应用中存在诸多挑战。本文旨在通过机器学习分类器预先判断问题是否适合HHL实现，以提高算法的实际适用性。

Method: 利用多层感知器（MLP）分类器，基于线性方程组矩阵的数值特性，对问题进行分类。研究强调训练数据分布的显著代表性和分类器参数的合理设计。

Result: 研究表明，通过精心设计的训练数据分布和分类器参数，多层感知器可以准确分类问题是否适合HHL实现。

Conclusion: 本研究展示了机器学习在判断HHL算法适用性方面的潜力，强调了训练数据分布和分类器设计的重要性。

Abstract: Under the nearing error-corrected era of quantum computing, it is necessary
to understand the suitability of certain post-NISQ algorithms for practical
problems. One of the most promising, applicable and yet difficult to implement
in practical terms is the Harrow, Hassidim and Lloyd (HHL) algorithm for linear
systems of equations. An enormous number of problems can be expressed as linear
systems of equations, from Machine Learning to fluid dynamics. However, in most
cases, HHL will not be able to provide a practical, reasonable solution to
these problems. This paper's goal inquires about whether problems can be
labeled using Machine Learning classifiers as suitable or unsuitable for HHL
implementation when some numerical information about the problem is known
beforehand. This work demonstrates that training on significantly
representative data distributions is critical to achieve good classifications
of the problems based on the numerical properties of the matrix representing
the system of equations. Accurate classification is possible through
Multi-Layer Perceptrons, although with careful design of the training data
distribution and classifier parameters.

</details>


### [444] [Assessing Quantum Advantage for Gaussian Process Regression](https://arxiv.org/abs/2505.22502)
*Dominic Lowe,M. S. Kim,Roberto Bondesan*

Key words: 高斯过程回归, 量子算法, 条件数, 核矩阵, 稀疏性

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 该研究表明，在高斯过程回归的多种场景中，提出的量子算法未展现指数级加速，并通过理论证明和数值验证支持这一结论。

Motivation: 探索量子算法在高斯过程回归中的潜在加速能力，并验证其在实际应用中的有效性。

Method: 通过严格证明核矩阵的条件数、稀疏性和Frobenius范数在一般假设下的线性缩放特性，并进行数值验证。

Result: 在广泛场景下，量子算法未实现指数级加速，核矩阵的特性限制了算法的性能提升。

Conclusion: 量子算法在高斯过程回归中的加速效果有限，受限于核矩阵的基本特性。

Abstract: Gaussian Process Regression is a well-known machine learning technique for
which several quantum algorithms have been proposed. We show here that in a
wide range of scenarios these algorithms show no exponential speedup. We
achieve this by rigorously proving that the condition number of a kernel matrix
scales at least linearly with the matrix size under general assumptions on the
data and kernel. We additionally prove that the sparsity and Frobenius norm of
a kernel matrix scale linearly under similar assumptions. The implications for
the quantum algorithms runtime are independent of the complexity of loading
classical data on a quantum computer and also apply to dequantised algorithms.
We supplement our theoretical analysis with numerical verification for popular
kernels in machine learning.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [445] [High-Fidelity Functional Ultrasound Reconstruction via A Visual Auto-Regressive Framework](https://arxiv.org/abs/2505.21530)
*Xuhang Chen,Zhuo Li,Yanyan Shen,Mufti Mahmud,Hieu Pham,Chi-Man Pun,Shuqiang Wang*

Key words: 功能超声成像, 神经血管映射, 数据稀缺, 信号衰减, 机器学习

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 功能超声（fUS）成像在神经血管映射中提供了出色的时空分辨率，但其实际应用受到数据稀缺和信号衰减的严重限制。

Motivation: 解决功能超声成像在神经血管映射中因数据稀缺和信号衰减导致的数据集多样性和机器学习模型公平性问题。

Method: 未明确提及具体方法，但暗示需要应对数据稀缺和信号衰减的挑战。

Result: 未提供具体结果，但强调了现有技术面临的限制。

Conclusion: 功能超声成像在神经血管映射中具有潜力，但需克服数据稀缺和信号衰减的挑战以提高实用性。

Abstract: Functional ultrasound (fUS) imaging provides exceptional spatiotemporal
resolution for neurovascular mapping, yet its practical application is
significantly hampered by critical challenges. Foremost among these are data
scarcity, arising from ethical considerations and signal degradation through
the cranium, which collectively limit dataset diversity and compromise the
fairness of downstream machine learning models.

</details>


### [446] [STA-Risk: A Deep Dive of Spatio-Temporal Asymmetries for Breast Cancer Risk Prediction](https://arxiv.org/abs/2505.21699)
*Zhengbo Zhou,Dooman Arefan,Margarita Zuley,Jules Sumkin,Shandong Wu*

Key words: 乳腺癌风险预测, Transformer, 空间-时间不对称性, 纵向影像, 机器学习

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 提出了一种名为STA-Risk的新型Transformer模型，通过空间和时间不对称性预测乳腺癌风险，性能优于现有模型。

Motivation: 现有乳腺癌风险预测模型性能有限，且忽视纵向检查中的空间和时间细节。

Method: 基于Transformer的STA-Risk模型，通过侧编码和时间编码学习空间-时间不对称性，并使用定制化不对称性损失。

Result: 在两个独立的数据集上实验，性能优于四种代表性SOTA模型，适用于1-5年风险预测。

Conclusion: STA-Risk通过捕捉纵向和双边不对称性显著提高了乳腺癌风险预测的准确性。

Abstract: Predicting the risk of developing breast cancer is an important clinical tool
to guide early intervention and tailoring personalized screening strategies.
Early risk models have limited performance and recently machine learning-based
analysis of mammogram images showed encouraging risk prediction effects. These
models however are limited to the use of a single exam or tend to overlook
nuanced breast tissue evolvement in spatial and temporal details of
longitudinal imaging exams that are indicative of breast cancer risk. In this
paper, we propose STA-Risk (Spatial and Temporal Asymmetry-based Risk
Prediction), a novel Transformer-based model that captures fine-grained
mammographic imaging evolution simultaneously from bilateral and longitudinal
asymmetries for breast cancer risk prediction. STA-Risk is innovative by the
side encoding and temporal encoding to learn spatial-temporal asymmetries,
regulated by a customized asymmetry loss. We performed extensive experiments
with two independent mammogram datasets and achieved superior performance than
four representative SOTA models for 1- to 5-year future risk prediction. Source
codes will be released upon publishing of the paper.

</details>


### [447] [Privacy-Preserving Chest X-ray Report Generation via Multimodal Federated Learning with ViT and GPT-2](https://arxiv.org/abs/2505.21715)
*Md. Zahid Hossain,Mustofa Ahmed,Most. Sharmin Sultana Samu,Md. Rakibul Islam*

Key words: 联邦学习, 胸部X光, 报告生成, 隐私保护, Vision Transformer, GPT-2

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 该研究提出了一种基于多模态联邦学习的胸部X光报告生成框架，利用ViT编码器和GPT-2生成器，避免原始数据共享，并通过三种联邦学习聚合策略验证，其中Krum Aggregation表现最佳。

Motivation: 传统集中式方法需要传输敏感数据，存在隐私问题，因此研究旨在开发一种隐私保护的分散式报告生成方法。

Method: 采用Vision Transformer (ViT) 作为编码器，GPT-2作为报告生成器，评估了FedAvg、Krum Aggregation和L-FedAvg三种联邦学习聚合策略。

Result: Krum Aggregation在ROUGE、BLEU、BERTScore和RaTEScore等指标上表现最优，联邦学习模型可匹配或超越集中式模型的性能。

Conclusion: 该轻量级框架为医疗AI协作开发提供了隐私保护方案，同时保持报告的临床相关性和语义丰富性。

Abstract: The automated generation of radiology reports from chest X-ray images holds
significant promise in enhancing diagnostic workflows while preserving patient
privacy. Traditional centralized approaches often require sensitive data
transfer, posing privacy concerns. To address this, the study proposes a
Multimodal Federated Learning framework for chest X-ray report generation using
the IU-Xray dataset. The system utilizes a Vision Transformer (ViT) as the
encoder and GPT-2 as the report generator, enabling decentralized training
without sharing raw data. Three Federated Learning (FL) aggregation strategies:
FedAvg, Krum Aggregation and a novel Loss-aware Federated Averaging (L-FedAvg)
were evaluated. Among these, Krum Aggregation demonstrated superior performance
across lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore
and RaTEScore. The results show that FL can match or surpass centralized models
in generating clinically relevant and semantically rich radiology reports. This
lightweight and privacy-preserving framework paves the way for collaborative
medical AI development without compromising data confidentiality.

</details>


### [448] [Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology](https://arxiv.org/abs/2505.21928)
*Lianghui Zhu,Xitong Ling,Minxi Ouyang,Xiaoping Liu,Mingxi Fu,Tian Guan,Fanglei Fu,Xuanyu Wang,Maomao Zeng,Mingxi Zhu,Yibo Jin,Liming Liu,Song Duan,Qiming He,Yizhi Wang,Luxi Xie,Houqiang Li,Yonghong He,Sufang Tian*

Key words: 胃肠道疾病、AI基础模型、病理诊断、双阶段优化、Digepath

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: Digepath是一个专为胃肠道病理设计的AI基础模型，通过双阶段迭代优化策略在病理诊断、分子预测等任务中表现优异，并在早期癌症筛查中达到99.6%的敏感性。

Motivation: 传统病理诊断依赖主观解读，存在可重复性低和诊断变异性的问题，需要专门针对胃肠道疾病的AI模型来优化诊断精度。

Method: 开发Digepath模型，采用双阶段迭代优化策略（预训练与精细筛选），基于超过35.3亿个图像块和20万张胃肠道疾病H&E染色切片进行训练。

Result: Digepath在34项任务中的33项达到最先进水平，包括病理诊断、分子预测等，并在早期癌症筛查中实现99.6%的敏感性。

Conclusion: Digepath填补病理实践中的关键空白，不仅推动AI驱动的胃肠道疾病精准病理学，还为其他病理亚专科提供可迁移的范例。

Abstract: Gastrointestinal (GI) diseases represent a clinically significant burden,
necessitating precise diagnostic approaches to optimize patient outcomes.
Conventional histopathological diagnosis, heavily reliant on the subjective
interpretation of pathologists, suffers from limited reproducibility and
diagnostic variability. To overcome these limitations and address the lack of
pathology-specific foundation models for GI diseases, we develop Digepath, a
specialized foundation model for GI pathology. Our framework introduces a
dual-phase iterative optimization strategy combining pretraining with
fine-screening, specifically designed to address the detection of sparsely
distributed lesion areas in whole-slide images. Digepath is pretrained on more
than 353 million image patches from over 200,000 hematoxylin and eosin-stained
slides of GI diseases. It attains state-of-the-art performance on 33 out of 34
tasks related to GI pathology, including pathological diagnosis, molecular
prediction, gene mutation prediction, and prognosis evaluation, particularly in
diagnostically ambiguous cases and resolution-agnostic tissue classification.We
further translate the intelligent screening module for early GI cancer and
achieve near-perfect 99.6% sensitivity across 9 independent medical
institutions nationwide. The outstanding performance of Digepath highlights its
potential to bridge critical gaps in histopathological practice. This work not
only advances AI-driven precision pathology for GI diseases but also
establishes a transferable paradigm for other pathology subspecialties.

</details>


### [449] [Taylor expansion-based Kolmogorov-Arnold network for blind image quality assessment](https://arxiv.org/abs/2505.21592)
*Ze Chen,Shaode Yu*

Key words: Kolmogorov-Arnold Network, Taylor expansion, blind image quality assessment, high-dimensional regression, computational efficiency

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 论文提出TaylorKAN，通过泰勒展开作为可学习激活函数提升局部逼近能力，同时结合网络深度压缩和特征降维以提高计算效率。实验表明其在五个数据库上优于其他KAN相关模型，展示了泰勒展开在局部逼近中的有效性。

Motivation: 现有KAN及其变体在处理高维特征时性能有限且计算成本高，因此提出TaylorKAN以改进局部逼近能力和计算效率。

Method: 利用泰勒展开作为可学习激活函数增强局部逼近能力，并结合网络深度压缩和特征降维优化计算流程。

Result: 在五个数据库（BID、CLIVE、KonIQ、SPAQ、FLIVE）上TaylorKAN均优于其他KAN模型，验证了泰勒展开的局部逼近优势及泛化能力。

Conclusion: TaylorKAN是一种高效且鲁棒的高维分数回归模型，泰勒展开的局部逼近策略比全局正交函数更有效。

Abstract: Kolmogorov-Arnold Network (KAN) has attracted growing interest for its strong
function approximation capability. In our previous work, KAN and its variants
were explored in score regression for blind image quality assessment (BIQA).
However, these models encounter challenges when processing high-dimensional
features, leading to limited performance gains and increased computational
cost. To address these issues, we propose TaylorKAN that leverages the Taylor
expansions as learnable activation functions to enhance local approximation
capability. To improve the computational efficiency, network depth reduction
and feature dimensionality compression are integrated into the TaylorKAN-based
score regression pipeline. On five databases (BID, CLIVE, KonIQ, SPAQ, and
FLIVE) with authentic distortions, extensive experiments demonstrate that
TaylorKAN consistently outperforms the other KAN-related models, indicating
that the local approximation via Taylor expansions is more effective than
global approximation using orthogonal functions. Its generalization capacity is
validated through inter-database experiments. The findings highlight the
potential of TaylorKAN as an efficient and robust model for high-dimensional
score regression.

</details>


### [450] [Optimizing Deep Learning for Skin Cancer Classification: A Computationally Efficient CNN with Minimal Accuracy Trade-Off](https://arxiv.org/abs/2505.21597)
*Abdullah Al Mamun,Pollob Chandra Ray,Md Rahat Ul Nasib,Akash Das,Jia Uddin,Md Nurul Absur*

Key words: skin cancer classification, lightweight CNN, computational efficiency, HAM10000, transfer learning

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 该研究提出了一种轻量级的CNN模型，在保持分类准确率接近ResNet50的同时，显著减少了参数数量和计算开销，适用于资源受限环境。

Motivation: 现有基于迁移学习（如ResNet50）的皮肤癌分类模型计算开销大，难以在资源受限环境中部署，需要一种更高效的解决方案。

Method: 设计了一种自定义的轻量级CNN模型，对比了其参数数量、FLOPs和分类准确率等指标，并通过HAM10000数据集进行验证。

Result: 轻量模型参数减少96.7%（692,000 vs 23.9M），FLOPs降至30.04M（ResNet50为4B），分类准确率偏差仅为0.022%。

Conclusion: 轻量级模型在计算效率和实用性上表现优异，更适合移动和边缘设备部署。

Abstract: The rapid advancement of deep learning in medical image analysis has greatly
enhanced the accuracy of skin cancer classification. However, current
state-of-the-art models, especially those based on transfer learning like
ResNet50, come with significant computational overhead, rendering them
impractical for deployment in resource-constrained environments. This study
proposes a custom CNN model that achieves a 96.7\% reduction in parameters
(from 23.9 million in ResNet50 to 692,000) while maintaining a classification
accuracy deviation of less than 0.022\%. Our empirical analysis of the HAM10000
dataset reveals that although transfer learning models provide a marginal
accuracy improvement of approximately 0.022\%, they result in a staggering
13,216.76\% increase in FLOPs, considerably raising computational costs and
inference latency. In contrast, our lightweight CNN architecture, which
encompasses only 30.04 million FLOPs compared to ResNet50's 4.00 billion,
significantly reduces energy consumption, memory footprint, and inference time.
These findings underscore the trade-off between the complexity of deep models
and their real-world feasibility, positioning our optimized CNN as a practical
solution for mobile and edge-based skin cancer diagnostics.

</details>


### [451] [Beyond 1D: Vision Transformers and Multichannel Signal Images for PPG-to-ECG Reconstruction](https://arxiv.org/abs/2505.21767)
*Xiaoyan Li,Shixin Xu,Faisal Habib,Arvind Gupta,Huaxiong Huang*

Key words: PPG-to-ECG, Vision Transformer, 自注意力机制, 四通道信号, 心电重建

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 该论文提出了一种基于Vision Transformer（ViT）的新型PPG到ECG重建方法，通过四通道信号图像表示（包括PPG及其一阶、二阶差值和曲线下面积）增强特征提取，利用ViT的自注意力机制捕捉波形细节，显著提升了ECG重建精度。

Motivation: 现有PPG-to-ECG方法难以捕捉细粒度波形特征，限制了重建精度。本文旨在通过多通道信号表示和自注意力机制解决这一挑战。

Method: 使用四通道PPG信号图像（原始PPG、一阶/二阶差值、曲线下面积）作为ViT输入，利用其自注意力机制建模节拍内和节拍间依赖关系。

Result: 相比传统1D卷积方法，PRD降低29%、RMSE降低15%，新提出的临床指标（如QRS面积误差等）也验证了方法的鲁棒性。

Conclusion: 四通道表示与ViT结合可更有效提取PPG特征并建模节拍变化，为循环信号分析开辟新途径。

Abstract: Reconstructing ECG from PPG is a promising yet challenging task. While recent
advancements in generative models have significantly improved ECG
reconstruction, accurately capturing fine-grained waveform features remains a
key challenge. To address this, we propose a novel PPG-to-ECG reconstruction
method that leverages a Vision Transformer (ViT) as the core network. Unlike
conventional approaches that rely on single-channel PPG, our method employs a
four-channel signal image representation, incorporating the original PPG, its
first-order difference, second-order difference, and area under the curve. This
multi-channel design enriches feature extraction by preserving both temporal
and physiological variations within the PPG. By leveraging the self-attention
mechanism in ViT, our approach effectively captures both inter-beat and
intra-beat dependencies, leading to more robust and accurate ECG
reconstruction. Experimental results demonstrate that our method consistently
outperforms existing 1D convolution-based approaches, achieving up to 29%
reduction in PRD and 15% reduction in RMSE. The proposed approach also produces
improvements in other evaluation metrics, highlighting its robustness and
effectiveness in reconstructing ECG signals. Furthermore, to ensure a
clinically relevant evaluation, we introduce new performance metrics, including
QRS area error, PR interval error, RT interval error, and RT amplitude
difference error. Our findings suggest that integrating a four-channel signal
image representation with the self-attention mechanism of ViT enables more
effective extraction of informative PPG features and improved modeling of
beat-to-beat variations for PPG-to-ECG mapping. Beyond demonstrating the
potential of PPG as a viable alternative for heart activity monitoring, our
approach opens new avenues for cyclic signal analysis and prediction.

</details>


### [452] [Targeted Unlearning Using Perturbed Sign Gradient Methods With Applications On Medical Images](https://arxiv.org/abs/2505.21872)
*George R. Nahass,Zhu Wang,Homa Rashidisabet,Won Hwa Kim,Sasha Hubschman,Jeffrey C. Peterson,Ghasem Yazdanpanah,Chad A. Purnell,Pete Setabutr,Ann Q. Tran,Darvin Yi,Sathya N. Ravi*

Key words: 机器遗忘,临床影像,双层优化,边界遗忘,模型维护

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 论文提出了一种用于临床场景的机器遗忘方法，通过双层优化和边界遗忘机制，实现了对训练样本影响的精确移除，避免了完全重新训练的需求。

Motivation: 在临床应用中，数据漂移、设备淘汰和政策变化频繁，需要高效调整模型。现有遗忘方法多关注隐私保护，本工作将其推广为通用的模型更新工具。

Method: 采用双层优化公式和边界遗忘机制，利用一阶迭代算法实现遗忘过程，支持可控的遗忘-保留权衡和模型组合策略。

Result: 在基准和真实临床影像数据集上，该方法在遗忘和保留指标上均优于基线，特别是在设备或解剖异常场景中表现突出。

Conclusion: 机器遗忘可作为临床模型维护的模块化实用方案，替代传统重新训练。

Abstract: Machine unlearning aims to remove the influence of specific training samples
from a trained model without full retraining. While prior work has largely
focused on privacy-motivated settings, we recast unlearning as a
general-purpose tool for post-deployment model revision. Specifically, we focus
on utilizing unlearning in clinical contexts where data shifts, device
deprecation, and policy changes are common. To this end, we propose a bilevel
optimization formulation of boundary-based unlearning that can be solved using
iterative algorithms. We provide convergence guarantees when first-order
algorithms are used to unlearn. Our method introduces tunable loss design for
controlling the forgetting-retention tradeoff and supports novel model
composition strategies that merge the strengths of distinct unlearning runs.
Across benchmark and real-world clinical imaging datasets, our approach
outperforms baselines on both forgetting and retention metrics, including
scenarios involving imaging devices and anatomical outliers. This work
establishes machine unlearning as a modular, practical alternative to
retraining for real-world model maintenance in clinical applications.

</details>


### [453] [High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models](https://arxiv.org/abs/2505.22090)
*Tristan S. W. Stevens,Oisín Nolan,Oudom Somphone,Jean-Luc Robert,Ruud J. G. van Sloun*

Key words: 3D超声,扩散模型,图像重建,欠采样,时间一致性

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 该论文提出了一种利用扩散模型（DMs）从少量仰角平面重建3D超声图像的新方法，提高了时空分辨率，并在图像质量和下游任务性能上优于传统和深度学习插值方法。

Motivation: 3D超声能够实时可视化解剖结构，但高体积率与高图像质量的平衡仍具挑战性。传统方法在仰角平面大幅欠采样时需简单插值，导致质量下降。

Method: 采用扩散模型从少量仰角平面重建3D超声图像，并利用时间一致性加速推理，同时通过扩散后验采样的概率性量化重建不确定性。

Result: 实验显示，基于扩散模型的重建在图像质量和下游任务中优于基线方法，且在异常数据下通过强欠采样提升了召回率。

Conclusion: 扩散模型在3D超声重建中表现出优越性，不仅能提高分辨率，还能量化不确定性，增强对异常数据的鲁棒性。

Abstract: Three-dimensional ultrasound enables real-time volumetric visualization of
anatomical structures. Unlike traditional 2D ultrasound, 3D imaging reduces the
reliance on precise probe orientation, potentially making ultrasound more
accessible to clinicians with varying levels of experience and improving
automated measurements and post-exam analysis. However, achieving both high
volume rates and high image quality remains a significant challenge. While 3D
diverging waves can provide high volume rates, they suffer from limited tissue
harmonic generation and increased multipath effects, which degrade image
quality. One compromise is to retain the focusing in elevation while leveraging
unfocused diverging waves in the lateral direction to reduce the number of
transmissions per elevation plane. Reaching the volume rates achieved by full
3D diverging waves, however, requires dramatically undersampling the number of
elevation planes. Subsequently, to render the full volume, simple interpolation
techniques are applied. This paper introduces a novel approach to 3D ultrasound
reconstruction from a reduced set of elevation planes by employing diffusion
models (DMs) to achieve increased spatial and temporal resolution. We compare
both traditional and supervised deep learning-based interpolation methods on a
3D cardiac ultrasound dataset. Our results show that DM-based reconstruction
consistently outperforms the baselines in image quality and downstream task
performance. Additionally, we accelerate inference by leveraging the temporal
consistency inherent to ultrasound sequences. Finally, we explore the
robustness of the proposed method by exploiting the probabilistic nature of
diffusion posterior sampling to quantify reconstruction uncertainty and
demonstrate improved recall on out-of-distribution data with synthetic
anomalies under strong subsampling.

</details>


### [454] [Chest Disease Detection In X-Ray Images Using Deep Learning Classification Method](https://arxiv.org/abs/2505.22609)
*Alanna Hazlett,Naomi Ohashi,Timothy Rodriguez,Sodiq Adewole*

Key words: 迁移学习, CNN, 医学影像分类, COVID-19, Grad-CAM

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 使用迁移学习和预训练CNN模型对胸部X光片进行四分类（COVID-19、肺炎、结核、正常），结果准确率高，模型解释性通过Grad-CAM增强。

Motivation: 解决医学影像中COVID-19、肺炎、结核和正常病例的分类问题，提升临床应用的信任和透明度。

Method: 采用迁移学习技术，微调预训练的CNN模型，并结合Grad-CAM提供视觉解释。

Result: 模型表现优异，准确率高，关键分类指标（精确率、召回率、F1分数）强劲。

Conclusion: 该方法在医学影像分类中有效且可靠，具有临床应用潜力。

Abstract: In this work, we investigate the performance across multiple classification
models to classify chest X-ray images into four categories of COVID-19,
pneumonia, tuberculosis (TB), and normal cases. We leveraged transfer learning
techniques with state-of-the-art pre-trained Convolutional Neural Networks
(CNNs) models. We fine-tuned these pre-trained architectures on a labeled
medical x-ray images. The initial results are promising with high accuracy and
strong performance in key classification metrics such as precision, recall, and
F1 score. We applied Gradient-weighted Class Activation Mapping (Grad-CAM) for
model interpretability to provide visual explanations for classification
decisions, improving trust and transparency in clinical applications.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [455] [Uncovering Bottlenecks and Optimizing Scientific Lab Workflows with Cycle Time Reduction Agents](https://arxiv.org/abs/2505.21534)
*Yao Fehlis*

Key words: 实验室工作流程优化, LangGraph, 自动化代理, 制药研发, 周期时间缩减

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 论文提出了一种基于LangGraph的Cycle Time Reduction Agents（CTRA）框架，用于优化科学实验室工作流程，特别是药物和生物技术公司的复杂任务。CTRA通过三个主要组件（问题创建代理、操作指标代理和洞察代理）自动化分析实验室操作数据，以识别瓶颈并加速研发。

Motivation: 科学实验室（尤其是制药和生物技术领域）因任务复杂且量大（如化合物筛选和检测执行）面临工作流程优化挑战，需要一个自动化工具来提升效率。

Method: 采用基于LangGraph的代理工作流CTRA，包含三个核心组件：问题创建代理（启动分析）、操作指标代理（数据提取与验证）和洞察代理（报告与可视化）。

Result: CTRA在实验室数据集上表现良好，能有效识别流程瓶颈，为药物和生物技术研发提供加速潜力。

Conclusion: CTRA提供了一个可扩展的框架，显著减少科学实验室的周期时间。

Abstract: Scientific laboratories, particularly those in pharmaceutical and
biotechnology companies, encounter significant challenges in optimizing
workflows due to the complexity and volume of tasks such as compound screening
and assay execution. We introduce Cycle Time Reduction Agents (CTRA), a
LangGraph-based agentic workflow designed to automate the analysis of lab
operational metrics. CTRA comprises three main components: the Question
Creation Agent for initiating analysis, Operational Metrics Agents for data
extraction and validation, and Insights Agents for reporting and visualization,
identifying bottlenecks in lab processes. This paper details CTRA's
architecture, evaluates its performance on a lab dataset, and discusses its
potential to accelerate pharmaceutical and biotechnological development. CTRA
offers a scalable framework for reducing cycle times in scientific labs.

</details>


### [456] [Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via an Automated Online Design Framework](https://arxiv.org/abs/2505.21559)
*Julien Soulé,Jean-Paul Jamont,Michel Occello,Louis-Marie Traonouez,Paul Théron*

Key words: Kubernetes, 水平Pod自动扩展, 多智能体系统, 操作弹性, DDoS攻击

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 该论文提出了一种基于多智能体系统（MAS）的水平Pod自动扩展（HPA）框架，通过分解目标为子任务并协同优化，提升Kubernetes集群在对抗性条件下的操作弹性。

Motivation: 解决传统HPA方法无法应对动态对抗性条件（如DDoS攻击）的局限性，现有强化学习方法多优化单一目标，忽视广泛故障场景。

Method: 提出四阶段在线HPA MAS设计框架：1）基于集群日志构建数字孪生；2）针对故障场景训练协作智能体；3）分析智能体行为可解释性；4）将策略迁移至真实集群。

Result: 实验表明，生成的HPA MAS在复杂集群中优于三种先进HPA系统，能在多种对抗条件下保持操作弹性。

Conclusion: 通过多智能体协同分解目标，该框架有效提升了Kubernetes集群的弹性，尤其在动态对抗环境中表现优越。

Abstract: In cloud-native systems, Kubernetes clusters with interdependent services
often face challenges to their operational resilience due to poor workload
management issues such as resource blocking, bottlenecks, or continuous pod
crashes. These vulnerabilities are further amplified in adversarial scenarios,
such as Distributed Denial-of-Service attacks (DDoS). Conventional Horizontal
Pod Autoscaling (HPA) approaches struggle to address such dynamic conditions,
while reinforcement learning-based methods, though more adaptable, typically
optimize single goals like latency or resource usage, neglecting broader
failure scenarios. We propose decomposing the overarching goal of maintaining
operational resilience into failure-specific sub-goals delegated to
collaborative agents, collectively forming an HPA Multi-Agent System (MAS). We
introduce an automated, four-phase online framework for HPA MAS design: 1)
modeling a digital twin built from cluster traces; 2) training agents in
simulation using roles and missions tailored to failure contexts; 3) analyzing
agent behaviors for explainability; and 4) transferring learned policies to the
real cluster. Experimental results demonstrate that the generated HPA MASs
outperform three state-of-the-art HPA systems in sustaining operational
resilience under various adversarial conditions in a proposed complex cluster.

</details>


### [457] [Herd Behavior: Investigating Peer Influence in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2505.21588)
*Young-Min Cho,Sharath Chandra Guntuku,Lyle Ungar*

Key words: 大型语言模型、多智能体系统、从众行为、同伴影响、协作框架

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 大型语言模型（LLMs）多智能体系统中，研究者探索了从众行为的动态机制及其影响因素，包括自信与感知同伴信心的差距、信息呈现形式以及控制从众行为的可能性，揭示了如何优化协作框架。

Motivation: 针对LLM多智能体系统中的交互行为，现有研究多集中于单个模型的表现，而同伴间影响的动态机制尚不明确。本文旨在填补这一空白，探究从众行为在多智能体协作中的作用及其对系统性能的影响。

Method: 通过一系列受控实验，分析智能体在同伴影响下的行为调整。具体因素包括自信与感知同伴信心的差异、信息呈现形式，以及如何通过校准从众倾向来调控协作效果。

Result: 实验表明：（1）自信与感知同伴信心的差距显著影响从众概率；（2）信息呈现形式调控从众强度；（3）系统化控制的从众行为可提升协作效果。

Conclusion: LLM多智能体系统的社交动态中，从众行为可通过关键因素调控，为设计高效、自适应协作框架提供了新思路。

Abstract: Recent advancements in Large Language Models (LLMs) have enabled the
emergence of multi-agent systems where LLMs interact, collaborate, and make
decisions in shared environments. While individual model behavior has been
extensively studied, the dynamics of peer influence in such systems remain
underexplored. In this paper, we investigate herd behavior, the tendency of
agents to align their outputs with those of their peers, within LLM-based
multi-agent interactions. We present a series of controlled experiments that
reveal how herd behaviors are shaped by multiple factors. First, we show that
the gap between self-confidence and perceived confidence in peers significantly
impacts an agent's likelihood to conform. Second, we find that the format in
which peer information is presented plays a critical role in modulating the
strength of herd behavior. Finally, we demonstrate that the degree of herd
behavior can be systematically controlled, and that appropriately calibrated
herd tendencies can enhance collaborative outcomes. These findings offer new
insights into the social dynamics of LLM-based systems and open pathways for
designing more effective and adaptive multi-agent collaboration frameworks.

</details>


### [458] [Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation](https://arxiv.org/abs/2505.21880)
*Yu-Lun Song,Chung-En Tsern,Che-Cheng Wu,Yu-Ming Chang,Syuan-Bo Huang,Wei-Chu Chen,Michael Chia-Liang Lin,Yu-Ta Lin*

Key words: 城市流动性, 大型语言模型, 基于代理建模, 个性化路线, 台北市

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 论文提出了一种结合大型语言模型（LLM）与基于代理建模（ABM）的创新城市流动性模拟方法，以提升代理多样性和真实性，并应用于台北市的实际数据模拟。

Motivation: 传统基于规则的ABM方法在代理多样性和真实性方面存在局限，本研究旨在通过结合LLM提升模拟的逼真度，为城市规划提供更精准的数据支持。

Method: 通过LLM生成合成人口档案、分配常规与偶然地点，并模拟个性化路线，结合台北市实际数据建模个体行为和大规模流动性模式。

Result: 生成的路线热图和交通方式指标为城市规划者提供了可操作的政策制定依据。

Conclusion: 该方法显著提升了城市流动性模拟的多样性和真实性，未来将重点建立验证框架以确保规划应用的准确性。

Abstract: This study presents an innovative approach to urban mobility simulation by
integrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).
Unlike traditional rule-based ABM, the proposed framework leverages LLM to
enhance agent diversity and realism by generating synthetic population
profiles, allocating routine and occasional locations, and simulating
personalized routes. Using real-world data, the simulation models individual
behaviors and large-scale mobility patterns in Taipei City. Key insights, such
as route heat maps and mode-specific indicators, provide urban planners with
actionable information for policy-making. Future work focuses on establishing
robust validation frameworks to ensure accuracy and reliability in urban
planning applications.

</details>


### [459] [Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.21985)
*Naoto Yoshida,Tadahiro Taniguchi*

Key words: 多智能体强化学习（MARL）, 集体预测编码（CPC）, 分散通信, 非合作任务, Bandit-CPC, IPPO-CPC

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 论文提出了MARL-CPC框架，无需参数共享即可实现完全分散的多智能体通信，通过集体预测编码（CPC）提高性能，尤其在非合作任务中优于传统方法。

Motivation: 在多智能体强化学习（MARL）中，部分可观测性下有效通信可提升性能。传统方法假设合作且消息为动作空间一部分，限制了非合作场景的应用。

Method: 提出MARL-CPC框架，基于集体预测编码（CPC）学习消息，并开发Bandit-CPC和IPPO-CPC算法，支持非合作、奖励无关的通信。

Result: 在非合作MARL任务中，Bandit-CPC和IPPO-CPC均优于传统“消息即动作”方法，即使消息对发送者无直接益处也能建立有效通信。

Conclusion: MARL-CPC为复杂分散环境下的协调提供了潜力，扩展了非合作场景中多智能体通信的可能性。

Abstract: In multi-agent reinforcement learning (MARL), effective communication
improves agent performance, particularly under partial observability. We
propose MARL-CPC, a framework that enables communication among fully
decentralized, independent agents without parameter sharing. MARL-CPC
incorporates a message learning model based on collective predictive coding
(CPC) from emergent communication research. Unlike conventional methods that
treat messages as part of the action space and assume cooperation, MARL-CPC
links messages to state inference, supporting communication in non-cooperative,
reward-independent settings. We introduce two algorithms -Bandit-CPC and
IPPO-CPC- and evaluate them in non-cooperative MARL tasks. Benchmarks show that
both outperform standard message-as-action approaches, establishing effective
communication even when messages offer no direct benefit to the sender. These
results highlight MARL-CPC's potential for enabling coordination in complex,
decentralized environments.

</details>


### [460] [Sentiment Simulation using Generative AI Agents](https://arxiv.org/abs/2505.22125)
*Melrose Tia,Jezreel Sophia Lanuzo,Lei Rigi Baltazar,Marie Joy Lopez-Relente,Diwa Malaya Quiñones,Jason Albia*

Key words: 生成式AI, 情感分析, 心理学代理, 预测性建模, 情感模拟

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 提出了一个基于生成式AI代理的情感分析框架，通过结合心理学构建的代理来提升预测能力，验证其在政治和经济场景中的高准确性和稳定性。

Motivation: 传统情感分析方法局限于表面语言模式和回顾性数据，无法捕捉情感形成的心理和上下文动因，限制了在政策测试和预测应用中的效果。

Method: 使用2,485名菲律宾受访者的社会人口统计数据和心理特征构建AI代理，分为三个阶段：代理编码、场景暴露和情感评分生成，并采用二次加权准确率(QWA)评估。

Result: 情景化编码在复制调查回答中达到92%对齐，情感模拟任务中准确率为81%-86%，且显著优于分类编码，结果稳定且对场景变化不敏感。

Conclusion: 该框架实现了基于心理学的动态情感模拟，标志着情感分析从回顾性分类向预测性模拟的范式转变。

Abstract: Traditional sentiment analysis relies on surface-level linguistic patterns
and retrospective data, limiting its ability to capture the psychological and
contextual drivers of human sentiment. These limitations constrain its
effectiveness in applications that require predictive insight, such as policy
testing, narrative framing, and behavioral forecasting. We present a robust
framework for sentiment simulation using generative AI agents embedded with
psychologically rich profiles. Agents are instantiated from a nationally
representative survey of 2,485 Filipino respondents, combining sociodemographic
information with validated constructs of personality traits, values, beliefs,
and socio-political attitudes. The framework includes three stages: (1) agent
embodiment via categorical or contextualized encodings, (2) exposure to
real-world political and economic scenarios, and (3) generation of sentiment
ratings accompanied by explanatory rationales. Using Quadratic Weighted
Accuracy (QWA), we evaluated alignment between agent-generated and human
responses. Contextualized encoding achieved 92% alignment in replicating
original survey responses. In sentiment simulation tasks, agents reached
81%--86% accuracy against ground truth sentiment, with contextualized profile
encodings significantly outperforming categorical (p < 0.0001, Cohen's d =
0.70). Simulation results remained consistent across repeated trials
(+/-0.2--0.5% SD) and resilient to variation in scenario framing (p = 0.9676,
Cohen's d = 0.02). Our findings establish a scalable framework for sentiment
modeling through psychographically grounded AI agents. This work signals a
paradigm shift in sentiment analysis from retrospective classification to
prospective and dynamic simulation grounded in psychology of sentiment
formation.

</details>


### [461] [Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2505.22467)
*Jiaxi Yang,Mengqi Zhang,Yiqiao Jin,Hao Chen,Qingsong Wen,Lu Lin,Yi He,Weijie Xu,James Evans,Jindong Wang*

Key words: 大型语言模型、多代理系统、协作效率、拓扑结构、生成建模

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 该论文提出了一种基于大型语言模型的多代理系统（MASs）新框架，关注代理结构组织优化，包括代理选择、结构分析和拓扑合成三阶段，以提升协作性能。

Motivation: 目前多代理系统在复杂任务中的协作效率问题尚未充分研究，论文旨在引导研究社区关注代理结构组织的重要性。

Method: 论文提出了一个三阶段框架：代理选择、结构分析和拓扑合成，涉及语言模型、强化学习、图学习和生成建模等技术。

Result: 通过优化代理系统结构，可以显著提升协作性能，并推动相关技术在复杂实际应用中的潜力。

Conclusion: 论文展示了拓扑感知的多代理系统在提升协作效率中的重要性，并提出了未来研究方向。

Abstract: Large Language Model-based Multi-Agent Systems (MASs) have emerged as a
powerful paradigm for tackling complex tasks through collaborative
intelligence. Nevertheless, the question of how agents should be structurally
organized for optimal cooperation remains largely unexplored. In this position
paper, we aim to gently redirect the focus of the MAS research community toward
this critical dimension: develop topology-aware MASs for specific tasks.
Specifically, the system consists of three core components - agents,
communication links, and communication patterns - that collectively shape its
coordination performance and efficiency. To this end, we introduce a
systematic, three-stage framework: agent selection, structure profiling, and
topology synthesis. Each stage would trigger new research opportunities in
areas such as language models, reinforcement learning, graph learning, and
generative modeling; together, they could unleash the full potential of MASs in
complicated real-world applications. Then, we discuss the potential challenges
and opportunities in the evaluation of multiple systems. We hope our
perspective and framework can offer critical new insights in the era of agentic
AI.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [462] [Exact Algorithms and Lower Bounds for Forming Coalitions of Constrained Maximum Size](https://arxiv.org/abs/2505.22384)
*Foivos Fioravantes,Harmender Gahlawat,Nikolaos Melissinos*

Key words: 联合形成,团队规模限制,树状结构,FPT算法,渐进最优性

<details>
  <summary>Details</summary>

Main category: cs.DS

TL;DR: 论文研究了在有团队规模限制下的联合形成问题，提出了针对树状结构的高效算法并证明其渐进最优性。

Motivation: 研究如何在团队规模受限的情况下高效地根据代理偏好形成团队。

Method: 采用系统化的算法研究，提出多种精确算法（FPT）并分析了树状结构的高效算法。

Result: 提出的算法在树状结构下高效，且被证明为渐进最优，无法在理论假设下有显著改进。

Conclusion: 该算法在有限团队规模下具有实际应用价值，尤其在树状结构中表现优越。

Abstract: Imagine we want to split a group of agents into teams in the most
\emph{efficient} way, considering that each agent has their own preferences
about their teammates. This scenario is modeled by the extensively studied
\textsc{Coalition Formation} problem. Here, we study a version of this problem
where each team must additionally be of bounded size.
  We conduct a systematic algorithmic study, providing several intractability
results as well as multiple exact algorithms that scale well as the input grows
(FPT), which could prove useful in practice.
  Our main contribution is an algorithm that deals efficiently with tree-like
structures (bounded \emph{treewidth}) for ``small'' teams. We complement this
result by proving that our algorithm is asymptotically optimal. Particularly,
there can be no algorithm that vastly outperforms the one we present, under
reasonable theoretical assumptions, even when considering star-like structures
(bounded \emph{vertex cover number}).

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [463] [Symbolic Foundation Regressor on Complex Networks](https://arxiv.org/abs/2505.21879)
*Weiting Liu,Jiaxu Cui,Jiao Hu,En Wang,Bo Yang*

Key words: 符号回归, 复杂网络, 可解释性, 机器学习, 科学定律

<details>
  <summary>Details</summary>

Main category: cs.SC

TL;DR: 提出了一种预训练的符号基础回归器，能高效压缩复杂数据并生成可解释的物理表示，在多种领域测试中表现优异，比基线方法效率提升三倍。

Motivation: 科学中不仅需要预测，还需理解预测背后的可解释模型。传统手动发现科学定律的过程复杂且耗时，希望借助机器学习简化。

Method: 引入预训练的符号基础回归器，压缩多变量交互的复杂数据并生成可解释的物理表示，在符号回归和网络动力学推断中测试。

Result: 方程推断效率提升三倍，预测准确且能拟合流行病数据，揭示了直观的交互传播定律。

Conclusion: 该模型扩展了预训练符号回归的应用范围，为揭示复杂现象背后的隐藏机制提供了基础解决方案。

Abstract: In science, we are interested not only in forecasting but also in
understanding how predictions are made, specifically what the interpretable
underlying model looks like. Data-driven machine learning technology can
significantly streamline the complex and time-consuming traditional manual
process of discovering scientific laws, helping us gain insights into
fundamental issues in modern science. In this work, we introduce a pre-trained
symbolic foundation regressor that can effectively compress complex data with
numerous interacting variables while producing interpretable physical
representations. Our model has been rigorously tested on non-network symbolic
regression, symbolic regression on complex networks, and the inference of
network dynamics across various domains, including physics, biochemistry,
ecology, and epidemiology. The results indicate a remarkable improvement in
equation inference efficiency, being three times more effective than baseline
approaches while maintaining accurate predictions. Furthermore, we apply our
model to uncover more intuitive laws of interaction transmission from global
epidemic outbreak data, achieving optimal data fitting. This model extends the
application boundary of pre-trained symbolic regression models to complex
networks, and we believe it provides a foundational solution for revealing the
hidden mechanisms behind changes in complex phenomena, enhancing
interpretability, and inspiring further scientific discoveries.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [464] [On the performance of machine-learning assisted Monte Carlo in sampling from simple statistical physics models](https://arxiv.org/abs/2505.22598)
*Luca Maria Del Bono,Federico Ricci-Tersenghi,Francesco Zamponi*

Key words: Sequential Tempering, Curie-Weiss model, machine learning, Monte Carlo sampling, Gradient Descent

<details>
  <summary>Details</summary>

Main category: cond-mat.dis-nn

TL;DR: 该论文针对难以采样的系统，通过分析Sequential Tempering在Curie-Weiss模型中的应用，提供了最优权重和梯度下降训练的解析研究，并比较了是否加入局部Metropolis Monte Carlo步骤的效果，为机器学习技术与蒙特卡洛采样的结合奠定了理论基础。

Motivation: 随着机器学习技术在难以采样系统中的应用增多，缺乏理论理解可能导致次优实现。本研究旨在填补这一空白，为Sequential Tempering在特定模型中的应用提供解析基础。

Method: 采用Sequential Tempering和浅层MADE架构应用于Curie-Weiss模型，分析最优权重和梯度下降训练，并比较是否包含局部Metropolis Monte Carlo步骤的影响。

Result: 研究得出最优权重和训练的理论预测，并明确了在Sequential Tempering中加入局部蒙特卡洛步骤的最佳实践。

Conclusion: 该研究为机器学习与蒙特卡洛采样的结合提供了清晰的理论基础，指导未来的优化和应用。

Abstract: Recent years have seen a rise in the application of machine learning
techniques to aid the simulation of hard-to-sample systems that cannot be
studied using traditional methods. Despite the introduction of many different
architectures and procedures, a wide theoretical understanding is still
lacking, with the risk of suboptimal implementations. As a first step to
address this gap, we provide here a complete analytic study of the widely-used
Sequential Tempering procedure applied to a shallow MADE architecture for the
Curie-Weiss model. The contribution of this work is twofold: firstly, we give a
description of the optimal weights and of the training under Gradient Descent
optimization. Secondly, we compare what happens in Sequential Tempering with
and without the addition of local Metropolis Monte Carlo steps. We are thus
able to give theoretical predictions on the best procedure to apply in this
case. This work establishes a clear theoretical basis for the integration of
machine learning techniques into Monte Carlo sampling and optimization.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [465] [Learning optimal treatment strategies for intraoperative hypotension using deep reinforcement learning](https://arxiv.org/abs/2505.21596)
*Esra Adiyeke,Tianqi Liu,Venkata Sai Dheeraj Naganaboina,Han Li,Tyler J. Loftus,Yuanfang Ren,Benjamin Shickel,Matthew M. Ruppert,Karandeep Singh,Ruogu Fang,Parisa Rashidi,Azra Bihorac,Tezcan Ozrazgat-Baslanti*

Key words: 强化学习, 术中低血压, 急性肾损伤, 血管加压药, 静脉输液

<details>
  <summary>Details</summary>

Main category: q-bio.QM

TL;DR: 传统手术决策依赖经验且存在差异，数据驱动的强化学习模型可优化术中低血压管理，减少术后急性肾损伤（AKI）。模型基于16变量训练，推荐血管加压药和静脉输液剂量，与医生决策重合率69%，政策价值更高且AKI发生率最低。

Motivation: 传统手术决策依赖医生经验，存在不一致性。数据驱动的推荐系统可优化术中低血压管理，减少术后并发症如AKI。

Method: 使用深度Q网络强化学习模型，基于16个变量（如生理时间序列、药物剂量），分析50,021例手术数据（34,186训练，15,835测试）。

Result: 模型与医生决策重合率69%，政策价值高于实际治疗、随机及零药物策略，AKI发生率最低。

Conclusion: 模型政策可降低术后AKI并改善术中低血压相关结局，具有临床潜力。

Abstract: Traditional methods of surgical decision making heavily rely on human
experience and prompt actions, which are variable. A data-driven system
generating treatment recommendations based on patient states can be a
substantial asset in perioperative decision-making, as in cases of
intraoperative hypotension, for which suboptimal management is associated with
acute kidney injury (AKI), a common and morbid postoperative complication. We
developed a Reinforcement Learning (RL) model to recommend optimum dose of
intravenous (IV) fluid and vasopressors during surgery to avoid intraoperative
hypotension and postoperative AKI. We retrospectively analyzed 50,021 surgeries
from 42,547 adult patients who underwent major surgery at a quaternary care
hospital between June 2014 and September 2020. Of these, 34,186 surgeries were
used for model training and 15,835 surgeries were reserved for testing. We
developed a Deep Q-Networks based RL model using 16 variables including
intraoperative physiologic time series, total dose of IV fluid and vasopressors
extracted for every 15-minute epoch. The model replicated 69% of physician's
decisions for the dosage of vasopressors and proposed higher or lower dosage of
vasopressors than received in 10% and 21% of the treatments, respectively. In
terms of IV fluids, the model's recommendations were within 0.05 ml/kg/15 min
of the actual dose in 41% of the cases, with higher or lower doses recommended
for 27% and 32% of the treatments, respectively. The model resulted in a higher
estimated policy value compared to the physicians' actual treatments, as well
as random and zero-drug policies. AKI prevalence was the lowest in patients
receiving medication dosages that aligned with model's decisions. Our findings
suggest that implementation of the model's policy has the potential to reduce
postoperative AKI and improve other outcomes driven by intraoperative
hypotension.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [466] [Collaborative Agentic AI Needs Interoperability Across Ecosystems](https://arxiv.org/abs/2505.21550)
*Rishi Sharma,Martijn de Vos,Pradyumna Chari,Ramesh Raskar,Anne-Marie Kermarrec*

Key words: 协作式AI, 智能代理, 互操作性, Web of Agents, 生态碎片化

<details>
  <summary>Details</summary>

Main category: cs.NI

TL;DR: 论文提出协作式智能代理AI生态互操作性的重要性，并设计了一个名为Web of Agents的架构解决当前生态碎片化问题。

Motivation: 当前协作式智能代理AI解决方案各自孤立，可能导致生态碎片化，作者认为互操作性是实现开放、安全、规模化的关键。

Method: 提出Web of Agents架构，包含四个组件：代理间通信、交互互操作性、状态管理和代理发现，并尽可能复用现有标准和基础设施。

Result: Web of Agents为实现互操作性提供了初步但关键的一步，避免生态碎片化成为常态。

Conclusion: 通过最小化标准和基础设施复用，Web of Agents为协作式智能代理AI的互操作性提供了可行路径。

Abstract: Collaborative agentic AI is projected to transform entire industries by
enabling AI-powered agents to autonomously perceive, plan, and act within
digital environments. Yet, current solutions in this field are all built in
isolation, and we are rapidly heading toward a landscape of fragmented,
incompatible ecosystems. In this position paper, we argue that
interoperability, achieved by the adoption of minimal standards, is essential
to ensure open, secure, web-scale, and widely-adopted agentic ecosystems. To
this end, we devise a minimal architectural foundation for collaborative
agentic AI, named Web of Agents, which is composed of four components:
agent-to-agent messaging, interaction interoperability, state management, and
agent discovery. Web of Agents adopts existing standards and reuses existing
infrastructure where possible. With Web of Agents, we take the first but
critical step toward interoperable agentic systems and offer a pragmatic path
forward before ecosystem fragmentation becomes the norm.

</details>


### [467] [MetaSTNet: Multimodal Meta-learning for Cellular Traffic Conformal Prediction](https://arxiv.org/abs/2505.21553)
*Hui Ma,Kai Yang*

Key words: 网络流量预测、元学习、小样本学习、深度学习

<details>
  <summary>Details</summary>

Main category: cs.NI

TL;DR: 提出了一种基于多模态元学习框架的深度学习方法MetaSTNet，用于在小样本数据下实现精准网络流量预测，并通过实验验证了其高效性。

Motivation: 现有网络流量预测技术在大样本数据下表现良好，但在小样本情境下难以准确预测，需解决这一挑战。

Method: 采用元学习框架MetaSTNet，通过模拟器训练并迁移元知识至真实环境，结合交叉共形预测校准预测区间。

Result: 在真实数据集上的实验表明，MetaSTNet能快速适应新任务并实现高精度预测。

Conclusion: MetaSTNet为小样本网络流量预测提供了有效解决方案，具备实际应用潜力。

Abstract: Network traffic prediction techniques have attracted much attention since
they are valuable for network congestion control and user experience
improvement. While existing prediction techniques can achieve favorable
performance when there is sufficient training data, it remains a great
challenge to make accurate predictions when only a small amount of training
data is available. To tackle this problem, we propose a deep learning model,
entitled MetaSTNet, based on a multimodal meta-learning framework. It is an
end-to-end network architecture that trains the model in a simulator and
transfers the meta-knowledge to a real-world environment, which can quickly
adapt and obtain accurate predictions on a new task with only a small amount of
real-world training data. In addition, we further employ cross conformal
prediction to assess the calibrated prediction intervals. Extensive experiments
have been conducted on real-world datasets to illustrate the efficiency and
effectiveness of MetaSTNet.

</details>


### [468] [Fog Intelligence for Network Anomaly Detection](https://arxiv.org/abs/2505.21563)
*Kai Yang,Hui Ma,Shaoyu Dou*

Key words: 雾智能, 分布式机器学习, 无线网络管理, 异常检测, 边缘计算

<details>
  <summary>Details</summary>

Main category: cs.NI

TL;DR: 论文提出了一种称为'雾智能'的分布式机器学习架构，用于解决大规模分布式无线网络中异常行为检测的挑战，结合边缘处理和云计算的优势。

Motivation: 由于移动通信网络规模扩大、数据量增加以及传统集中式机器学习算法在分布式网络中应用困难，需要一种新的架构来实现智能网络管理。

Method: 提出了'雾智能'架构，结合边缘计算和集中式云计算，实现分布式机器学习。

Result: 该架构具有可扩展性、隐私保护能力，适用于分布式无线网络的智能管理。

Conclusion: 雾智能架构为大规模分布式网络中的异常检测提供了高效、灵活的解决方案。

Abstract: Anomalies are common in network system monitoring. When manifested as network
threats to be mitigated, service outages to be prevented, and security risks to
be ameliorated, detecting such anomalous network behaviors becomes of great
importance. However, the growing scale and complexity of the mobile
communication networks, as well as the ever-increasing amount and
dimensionality of the network surveillance data, make it extremely difficult to
monitor a mobile network and discover abnormal network behaviors. Recent
advances in machine learning allow for obtaining near-optimal solutions to
complicated decision-making problems with many sources of uncertainty that
cannot be accurately characterized by traditional mathematical models. However,
most machine learning algorithms are centralized, which renders them
inapplicable to a large-scale distributed wireless networks with tens of
millions of mobile devices. In this article, we present fog intelligence, a
distributed machine learning architecture that enables intelligent wireless
network management. It preserves the advantage of both edge processing and
centralized cloud computing. In addition, the proposed architecture is
scalable, privacy-preserving, and well suited for intelligent management of a
distributed wireless network.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [469] [Nonadaptive Output Regulation of Second-Order Nonlinear Uncertain Systems](https://arxiv.org/abs/2505.21838)
*Maobin Lu,Martin Guay,Telema Harry,Shimin Wang,Jordan Cooper*

Key words: 鲁棒控制, 输出调节, 非线性系统, 内模, 李雅普诺夫函数

<details>
  <summary>Details</summary>

Main category: eess.SY

TL;DR: 该论文通过鲁棒控制方法解决了二阶非线性不确定系统的输出调节问题，避免了自适应控制中的突发现象，并通过非自适应稳定化控制律和严格李雅普诺夫函数保证了鲁棒性。

Motivation: 研究二阶非线性不确定系统的鲁棒输出调节问题，避免自适应控制方法中的突发现象。

Method: 构建通用内模，引入坐标变换将问题转化为非自适应稳定化问题，设计稳定化控制律和严格李雅普诺夫函数。

Result: 提出的非自适应控制律使输出零流形具有吸引力，成功解决了鲁棒输出调节问题。

Conclusion: 非自适应内模方法在Duffing系统控制中表现有效，解决了鲁棒输出调节问题。

Abstract: This paper investigates the robust output regulation problem of second-order
nonlinear uncertain systems with an unknown exosystem. Instead of the adaptive
control approach, this paper resorts to a robust control methodology to solve
the problem and thus avoid the bursting phenomenon. In particular, this paper
constructs generic internal models for the steady-state state and input
variables of the system. By introducing a coordinate transformation, this paper
converts the robust output regulation problem into a nonadaptive stabilization
problem of an augmented system composed of the second-order nonlinear uncertain
system and the generic internal models. Then, we design the stabilization
control law and construct a strict Lyapunov function that guarantees the
robustness with respect to unmodeled disturbances. The analysis shows that the
output zeroing manifold of the augmented system can be made attractive by the
proposed nonadaptive control law, which solves the robust output regulation
problem. Finally, we demonstrate the effectiveness of the proposed nonadaptive
internal model approach by its application to the control of the Duffing
system.

</details>


### [470] [A Physics-Informed Learning Framework to Solve the Infinite-Horizon Optimal Control Problem](https://arxiv.org/abs/2505.21842)
*Filippos Fotiadis,Kyriakos G. Vamvoudakis*

Key words: 物理信息神经网络, 最优控制, HJB方程, 无限时域, 非线性系统

<details>
  <summary>Details</summary>

Main category: eess.SY

TL;DR: 论文提出了一个基于物理信息的神经网络（PINNs）框架来解决非线性系统的无限时域最优控制问题，并通过解决稳态Hamilton-Jacobi-Bellman（HJB）方程来学习价值函数。为了避免多解问题，论文采用有限时域的HJB方程变体，并提出一种算法验证时域长度是否足够。

Motivation: 传统方法在处理无限时域最优控制问题时可能因HJB方程多解而失效，而PINNs能够通过学习PDE的数值解来解决这一问题。研究的动机是改进现有方法的局限性，如无需先验知识或迭代策略评估。

Method: 利用PINNs求解有限时域的稳态HJB方程变体，提出时域长度验证与扩展算法，以提高解的唯一性和近似精度。

Result: 仿真验证了方法的有效性，证明了其在非多项式基函数下的适用性，同时避免了对稳定控制器的依赖和迭代策略评估。

Conclusion: 该框架在处理无限时域最优控制问题时具有鲁棒性和计算效率，为非线性系统提供了新的求解思路。

Abstract: We propose a physics-informed neural networks (PINNs) framework to solve the
infinite-horizon optimal control problem of nonlinear systems. In particular,
since PINNs are generally able to solve a class of partial differential
equations (PDEs), they can be employed to learn the value function of the
infinite-horizon optimal control problem via solving the associated
steady-state Hamilton-Jacobi-Bellman (HJB) equation. However, an issue here is
that the steady-state HJB equation generally yields multiple solutions; hence
if PINNs are directly employed to it, they may end up approximating a solution
that is different from the optimal value function of the problem. We tackle
this by instead applying PINNs to a finite-horizon variant of the steady-state
HJB that has a unique solution, and which uniformly approximates the optimal
value function as the horizon increases. An algorithm to verify if the chosen
horizon is large enough is also given, as well as a method to extend it -- with
reduced computations and robustness to approximation errors -- in case it is
not. Unlike many existing methods, the proposed technique works well with
non-polynomial basis functions, does not require prior knowledge of a
stabilizing controller, and does not perform iterative policy evaluations.
Simulations are performed, which verify and clarify theoretical findings.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [471] [Align-DA: Align Score-based Atmospheric Data Assimilation with Multiple Preferences](https://arxiv.org/abs/2505.22008)
*Jing-An Sun,Hang Fan,Junchao Gong,Ben Fei,Kun Chen,Fenghua Ling,Wenlong Zhang,Wanghan Xu,Li Yan,Pierre Gentine,Lei Bai*

Key words: 数据同化,生成过程,奖励信号,背景先验,偏好对齐

<details>
  <summary>Details</summary>

Main category: physics.ao-ph

TL;DR: Align-DA是一种数据同化方法，利用生成过程将奖励信号引导背景先验，自动调整复杂先验，提高分析质量。

Motivation: 传统数据同化方法依赖于经验性简化背景先验，需手动调整。Align-DA通过数据驱动对齐解决这一问题。

Method: Align-DA在潜在空间中训练基于评分的模型，利用三种奖励信号（同化精度、预测技能、物理一致性）引导背景先验。

Result: 实验显示，Align-DA在不同评价指标和观测策略下均能提升分析质量。

Conclusion: 偏好对齐作为软约束，可自动适配复杂背景先验，为数据同化领域提供新方向。

Abstract: Data assimilation (DA) aims to estimate the full state of a dynamical system
by combining partial and noisy observations with a prior model forecast,
commonly referred to as the background. In atmospheric applications, this
problem is fundamentally ill-posed due to the sparsity of observations relative
to the high-dimensional state space. Traditional methods address this challenge
by simplifying background priors to regularize the solution, which are
empirical and require continual tuning for application. Inspired by alignment
techniques in text-to-image diffusion models, we propose Align-DA, which
formulates DA as a generative process and uses reward signals to guide
background priors, replacing manual tuning with data-driven alignment.
Specifically, we train a score-based model in the latent space to approximate
the background-conditioned prior, and align it using three complementary reward
signals for DA: (1) assimilation accuracy, (2) forecast skill initialized from
the assimilated state, and (3) physical adherence of the analysis fields.
Experiments with multiple reward signals demonstrate consistent improvements in
analysis quality across different evaluation metrics and observation-guidance
strategies. These results show that preference alignment, implemented as a soft
constraint, can automatically adapt complex background priors tailored to DA,
offering a promising new direction for advancing the field.

</details>
