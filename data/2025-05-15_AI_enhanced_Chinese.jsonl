{"id": "2505.08828", "pdf": "https://arxiv.org/pdf/2505.08828", "abs": "https://arxiv.org/abs/2505.08828", "authors": ["Eduardo Araujo Oliveira", "Madhavi Mohoni", "Sonsoles L\u00f3pez-Pernas", "Mohammed Saqr"], "title": "Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "19 pages, 10 figures, 11 tables", "summary": "As human-AI collaboration becomes increasingly prevalent in educational\ncontexts, understanding and measuring the extent and nature of such\ninteractions pose significant challenges. This research investigates the use of\nauthorship verification (AV) techniques not as a punitive measure, but as a\nmeans to quantify AI assistance in academic writing, with a focus on promoting\ntransparency, interpretability, and student development. Building on prior\nwork, we structured our investigation into three stages: dataset selection and\nexpansion, AV method development, and systematic evaluation. Using three\ndatasets - including a public dataset (PAN-14) and two from University of\nMelbourne students from various courses - we expanded the data to include\nLLM-generated texts, totalling 1,889 documents and 540 authorship problems from\n506 students. We developed an adapted Feature Vector Difference AV methodology\nto construct robust academic writing profiles for students, designed to capture\nmeaningful, individual characteristics of their writing. The method's\neffectiveness was evaluated across multiple scenarios, including distinguishing\nbetween student-authored and LLM-generated texts and testing resilience against\nLLMs' attempts to mimic student writing styles. Results demonstrate the\nenhanced AV classifier's ability to identify stylometric discrepancies and\nmeasure human-AI collaboration at word and sentence levels while providing\neducators with a transparent tool to support academic integrity investigations.\nThis work advances AV technology, offering actionable insights into the\ndynamics of academic writing in an AI-driven era.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4f5c\u8005\u9a8c\u8bc1\uff08AV\uff09\u6280\u672f\u91cf\u5316\u5b66\u751f\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u7684AI\u8f85\u52a9\uff0c\u65e8\u5728\u63d0\u5347\u900f\u660e\u5ea6\u548c\u5b66\u751f\u53d1\u5c55\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e2d\u9a8c\u8bc1\u4e86\u5176\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u6559\u80b2\u73af\u5883\u4e2d\u4eba\u673a\u534f\u4f5c\u7684\u666e\u53ca\uff0c\u9700\u8981\u4e00\u79cd\u900f\u660e\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u6765\u6d4b\u91cfAI\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u7684\u53c2\u4e0e\u7a0b\u5ea6\uff0c\u4ee5\u652f\u6301\u5b66\u672f\u8bda\u4fe1\u548c\u5b66\u751f\u53d1\u5c55\u3002", "method": "\u7814\u7a76\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u6570\u636e\u96c6\u9009\u62e9\u4e0e\u6269\u5c55\u3001AV\u65b9\u6cd5\u5f00\u53d1\u3001\u7cfb\u7edf\u8bc4\u4f30\u3002\u91c7\u7528\u4e86\u4e09\u4e2a\u6570\u636e\u96c6\uff08\u5305\u62ecPAN-14\u548c\u58a8\u5c14\u672c\u5927\u5b66\u5b66\u751f\u6570\u636e\uff09\uff0c\u5f00\u53d1\u4e86\u6539\u8fdb\u7684\u7279\u5f81\u5411\u91cf\u5dee\u5f02AV\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6548\u679c\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6539\u8fdb\u7684AV\u5206\u7c7b\u5668\u80fd\u6709\u6548\u8bc6\u522b\u5b66\u751f\u5199\u4f5c\u4e0eAI\u751f\u6210\u6587\u672c\u7684\u5dee\u5f02\uff0c\u5e76\u5728\u5355\u8bcd\u548c\u53e5\u5b50\u5c42\u9762\u6d4b\u91cf\u4eba\u673a\u534f\u4f5c\uff0c\u4e3a\u6559\u80b2\u8005\u63d0\u4f9b\u4e86\u900f\u660e\u5de5\u5177\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86AV\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4e3aAI\u65f6\u4ee3\u7684\u5b66\u672f\u5199\u4f5c\u52a8\u6001\u63d0\u4f9b\u4e86\u53ef\u884c\u89c1\u89e3\uff0c\u4fc3\u8fdb\u4e86\u5b66\u672f\u8bda\u4fe1\u548c\u5b66\u751f\u53d1\u5c55\u7684\u900f\u660e\u5316\u3002"}}
{"id": "2505.08891", "pdf": "https://arxiv.org/pdf/2505.08891", "abs": "https://arxiv.org/abs/2505.08891", "authors": ["Daeun Hwang", "Samuel Shields", "Alex Calderwood", "Shi Johnson-Bey", "Michael Mateas", "Noah Wardrip-Fruin", "Edward F. Melcer"], "title": "Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives", "categories": ["cs.CL"], "comment": "8 pages, 3 figures, 1 table, 1 appendix. Workshop paper, CHI 2025\n  Augmented Educators and AI", "summary": "Motivation is an important factor underlying successful learning. Previous\nresearch has demonstrated the positive effects that static interactive\nnarrative games can have on motivation. Concurrently, advances in AI have made\ndynamic and adaptive approaches to interactive narrative increasingly\naccessible. However, limited work has explored the impact that dynamic\nnarratives can have on learner motivation. In this paper, we compare two\nversions of Academical, a choice-based educational interactive narrative game\nabout research ethics. One version employs a traditional hand-authored\nbranching plot (i.e., static narrative) while the other dynamically sequences\nplots during play (i.e., dynamic narrative). Results highlight the importance\nof responsive content and a variety of choices for player engagement, while\nalso illustrating the challenge of balancing pedagogical goals with the dynamic\naspects of narrative. We also discuss design implications that arise from these\nfindings. Ultimately, this work provides initial steps to illuminate the\nemerging potential of AI-driven dynamic narrative in educational games.", "AI": {"tldr": "\u5bf9\u6bd4\u9759\u6001\u4e0e\u52a8\u6001\u53d9\u4e8b\u6559\u80b2\u6e38\u620f\u5bf9\u5b66\u4e60\u52a8\u673a\u7684\u5f71\u54cd\uff0c\u52a8\u6001\u53d9\u4e8b\u66f4\u6ce8\u91cd\u5185\u5bb9\u54cd\u5e94\u6027\u4e0e\u9009\u62e9\u591a\u6837\u6027\uff0c\u4f46\u9700\u5e73\u8861\u6559\u5b66\u76ee\u6807\u4e0e\u53d9\u4e8b\u52a8\u6001\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u52a8\u6001\u53d9\u4e8b\uff08\u57fa\u4e8eAI\uff09\u76f8\u6bd4\u4f20\u7edf\u9759\u6001\u53d9\u4e8b\u5728\u63d0\u5347\u5b66\u4e60\u52a8\u673a\u65b9\u9762\u7684\u6f5c\u5728\u4f18\u52bf\uff0c\u586b\u8865\u4e86\u52a8\u6001\u53d9\u4e8b\u5728\u6559\u80b2\u6e38\u620f\u4e2d\u5f71\u54cd\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u9759\u6001\u5206\u652f\u53d9\u4e8b\u4e0e\u52a8\u6001\u5e8f\u5217\u5316\u53d9\u4e8b\u7684\u4e24\u4e2a\u7248\u672c\u6559\u80b2\u6e38\u620f\u300aAcademical\u300b\uff0c\u5206\u6790\u73a9\u5bb6\u53c2\u4e0e\u5ea6\u4e0e\u8bbe\u8ba1\u6311\u6218\u3002", "result": "\u52a8\u6001\u53d9\u4e8b\u80fd\u63d0\u5347\u73a9\u5bb6\u53c2\u4e0e\u5ea6\uff08\u56e0\u54cd\u5e94\u6027\u5185\u5bb9\u4e0e\u591a\u6837\u9009\u62e9\uff09\uff0c\u4f46\u9700\u6743\u8861\u6559\u5b66\u76ee\u6807\u7684\u660e\u786e\u6027\u4e0e\u53d9\u4e8b\u7075\u6d3b\u6027\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u52a8\u6001\u53d9\u4e8b\u5728\u6559\u80b2\u6e38\u620f\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u89e3\u51b3\u6559\u5b66\u4e0e\u53d9\u4e8b\u52a8\u6001\u6027\u7684\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2505.08996", "pdf": "https://arxiv.org/pdf/2505.08996", "abs": "https://arxiv.org/abs/2505.08996", "authors": ["Adele E Goldberg", "Supantho Rakshit", "Jennifer Hu", "Kyle Mahowald"], "title": "A suite of LMs comprehend puzzle statements as well as humans", "categories": ["cs.CL"], "comment": null, "summary": "Recent claims suggest that large language models (LMs) underperform humans in\ncomprehending minimally complex English statements (Dentella et al., 2024).\nHere, we revisit those findings and argue that human performance was\noverestimated, while LLM abilities were underestimated. Using the same stimuli,\nwe report a preregistered study comparing human responses in two conditions:\none allowed rereading (replicating the original study), and one that restricted\nrereading (a more naturalistic comprehension test). Human accuracy dropped\nsignificantly when rereading was restricted (73%), falling below that of\nFalcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect\naccuracy. Results further show that both humans and models are\ndisproportionately challenged by queries involving potentially reciprocal\nactions (e.g., kissing), suggesting shared pragmatic sensitivities rather than\nmodel-specific deficits. Additional analyses using Llama-2-70B log\nprobabilities, a recoding of open-ended model responses, and grammaticality\nratings of other sentences reveal systematic underestimation of model\nperformance. We find that GPT-4o can align with either naive or expert\ngrammaticality judgments, depending on prompt framing. These findings\nunderscore the need for more careful experimental design and coding practices\nin LLM evaluation, and they challenge the assumption that current models are\ninherently weaker than humans at language comprehension.", "AI": {"tldr": "\u8bba\u6587\u91cd\u65b0\u8bc4\u4f30\u4e86\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u5728\u7406\u89e3\u82f1\u8bed\u8bed\u53e5\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4eba\u7c7b\u7684\u8868\u73b0\u88ab\u9ad8\u4f30\uff0c\u800c\u6a21\u578b\u7684\u80fd\u529b\u88ab\u4f4e\u4f30\uff1b\u5b9e\u9a8c\u663e\u793a\uff0c\u9650\u5236\u4eba\u7c7b\u91cd\u8bfb\u540e\u5176\u51c6\u786e\u7387\u4f4e\u4e8e\u67d0\u4e9bLM\uff0c\u5e76\u53d1\u73b0\u4e24\u8005\u5728\u7406\u89e3\u4ea4\u4e92\u6027\u52a8\u4f5c\u65f6\u5b58\u5728\u7c7b\u4f3c\u7684\u6311\u6218\u3002", "motivation": "\u53cd\u9a73\u524d\u4eba\u7814\u7a76\u4e2d\u4eba\u7c7b\u8bed\u8a00\u7406\u89e3\u4f18\u4e8eLMs\u7684\u7ed3\u8bba\uff0c\u63ed\u793a\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u7f16\u7801\u5b9e\u8df5\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u76f8\u540c\u7684\u523a\u6fc0\u6750\u6599\uff0c\u8fdb\u884c\u9884\u6ce8\u518c\u7814\u7a76\uff0c\u6bd4\u8f83\u4eba\u7c7b\u5728\u5141\u8bb8\u91cd\u8bfb\u548c\u9650\u5236\u91cd\u8bfb\u4e24\u79cd\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u591a\u4e2aLM\uff08\u5982Falcon-180B-Chat\u3001GPT-4\u7b49\uff09\u5bf9\u6bd4\u3002", "result": "\u9650\u5236\u91cd\u8bfb\u540e\u4eba\u7c7b\u51c6\u786e\u7387\uff0873%\uff09\u4f4e\u4e8eFalcon-180B-Chat\uff0876%\uff09\u548cGPT-4\uff0881%\uff09\uff0cGPT-o1\u6a21\u578b\u8fbe100%\uff1b\u53cc\u65b9\u5728\u4ea4\u4e92\u6027\u52a8\u4f5c\u7406\u89e3\u4e0a\u8868\u73b0\u76f8\u4f3c\u3002", "conclusion": "\u9700\u66f4\u4e25\u8c28\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u7f16\u7801\u4ee5\u8bc4\u4f30LM\uff0c\u5f53\u524d\u6a21\u578b\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u672a\u5fc5\u5f31\u4e8e\u4eba\u7c7b\u3002"}}
{"id": "2505.09005", "pdf": "https://arxiv.org/pdf/2505.09005", "abs": "https://arxiv.org/abs/2505.09005", "authors": ["Nicole Cuneo", "Eleanor Graves", "Supantho Rakshit", "Adele E. Goldberg"], "title": "For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies", "categories": ["cs.CL"], "comment": null, "summary": "It remains debated how well any LM understands natural language or generates\nreliable metalinguistic judgments. Moreover, relatively little work has\ndemonstrated that LMs can represent and respect subtle relationships between\nform and function proposed by linguists. We here focus on a particular such\nrelationship established in recent work: English speakers' judgments about the\ninformation structure of canonical sentences predicts independently collected\nacceptability ratings on corresponding 'long distance dependency' [LDD]\nconstructions, across a wide array of base constructions and multiple types of\nLDDs. To determine whether any LM captures this relationship, we probe GPT-4 on\nthe same tasks used with humans and new extensions.Results reveal reliable\nmetalinguistic skill on the information structure and acceptability tasks,\nreplicating a striking interaction between the two, despite the zero-shot,\nexplicit nature of the tasks, and little to no chance of contamination [Studies\n1a, 1b]. Study 2 manipulates the information structure of base sentences and\nconfirms a causal relationship: increasing the prominence of a constituent in a\ncontext sentence increases the subsequent acceptability ratings on an LDD\nconstruction. The findings suggest a tight relationship between natural and\nGPT-4 generated English, and between information structure and syntax, which\nbegs for further exploration.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86GPT-4\u662f\u5426\u80fd\u50cf\u4eba\u7c7b\u4e00\u6837\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u5e76\u751f\u6210\u53ef\u9760\u7684\u5143\u8bed\u8a00\u5224\u65ad\uff0c\u5c24\u5176\u662f\u80fd\u5426\u6355\u6349\u4fe1\u606f\u7ed3\u6784\u4e0e\u53e5\u6cd5\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGPT-4\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u5224\u65ad\u80fd\u529b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4fe1\u606f\u7ed3\u6784\u5bf9\u53e5\u6cd5\u53ef\u63a5\u53d7\u6027\u7684\u56e0\u679c\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u9a8c\u8bc1\u8bed\u8a00\u6a21\u578b\uff08\u5c24\u5176\u662fGPT-4\uff09\u662f\u5426\u80fd\u91cd\u73b0\u4eba\u7c7b\u8bed\u8a00\u4e2d\u4fe1\u606f\u7ed3\u6784\u4e0e\u957f\u8ddd\u79bb\u4f9d\u8d56\uff08LDD\uff09\u53ef\u63a5\u53d7\u6027\u4e4b\u95f4\u7684\u5fae\u5999\u5173\u7cfb\uff0c\u8fd9\u4e00\u95ee\u9898\u5728\u8bed\u8a00\u5b66\u4e2d\u5907\u53d7\u4e89\u8bae\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u4fe1\u606f\u7ed3\u6784\u4efb\u52a1\u548cLDD\u53ef\u63a5\u53d7\u6027\u4efb\u52a1\uff0c\u4ee5\u96f6\u6837\u672c\u65b9\u5f0f\u6d4b\u8bd5GPT-4\uff0c\u5e76\u6269\u5c55\u5b9e\u9a8c\u9a8c\u8bc1\u56e0\u679c\u5173\u7cfb\uff08\u5982\u901a\u8fc7\u64cd\u7eb5\u4e0a\u4e0b\u6587\u53e5\u5b50\u7684\u4fe1\u606f\u7ed3\u6784\uff09\u3002", "result": "GPT-4\u5728\u4fe1\u606f\u7ed3\u6784\u548c\u53ef\u63a5\u53d7\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u53ef\u9760\u7684\u5143\u8bed\u8a00\u80fd\u529b\uff0c\u4e14\u5b9e\u9a8c\u7ed3\u679c\u4e0e\u4eba\u7c7b\u6570\u636e\u4e00\u81f4\u3002\u540c\u65f6\uff0c\u7814\u7a76\u786e\u8ba4\u4e86\u4fe1\u606f\u7ed3\u6784\u5bf9LDD\u53ef\u63a5\u53d7\u6027\u7684\u56e0\u679c\u5f71\u54cd\u3002", "conclusion": "\u7ed3\u8bba\u662fGPT-4\u7684\u8868\u73b0\u63ed\u793a\u4e86\u81ea\u7136\u8bed\u8a00\u4e0e\u751f\u6210\u8bed\u8a00\u4e4b\u95f4\u3001\u4fe1\u606f\u7ed3\u6784\u4e0e\u53e5\u6cd5\u4e4b\u95f4\u7684\u7d27\u5bc6\u5173\u7cfb\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2505.08896", "pdf": "https://arxiv.org/pdf/2505.08896", "abs": "https://arxiv.org/abs/2505.08896", "authors": ["Pankaj Kumar", "Aditya Mishra", "Pranamesh Chakraborty", "Subrahmanya Swamy Peruru"], "title": "Deep reinforcement learning-based longitudinal control strategy for automated vehicles at signalised intersections", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Developing an autonomous vehicle control strategy for signalised\nintersections (SI) is one of the challenging tasks due to its inherently\ncomplex decision-making process. This study proposes a Deep Reinforcement\nLearning (DRL) based longitudinal vehicle control strategy at SI. A\ncomprehensive reward function has been formulated with a particular focus on\n(i) distance headway-based efficiency reward, (ii) decision-making criteria\nduring amber light, and (iii) asymmetric acceleration/ deceleration response,\nalong with the traditional safety and comfort criteria. This reward function\nhas been incorporated with two popular DRL algorithms, Deep Deterministic\nPolicy Gradient (DDPG) and Soft-Actor Critic (SAC), which can handle the\ncontinuous action space of acceleration/deceleration. The proposed models have\nbeen trained on the combination of real-world leader vehicle (LV) trajectories\nand simulated trajectories generated using the Ornstein-Uhlenbeck (OU) process.\nThe overall performance of the proposed models has been tested using Cumulative\nDistribution Function (CDF) plots and compared with the real-world trajectory\ndata. The results show that the RL models successfully maintain lower distance\nheadway (i.e., higher efficiency) and jerk compared to human-driven vehicles\nwithout compromising safety. Further, to assess the robustness of the proposed\nmodels, we evaluated the model performance on diverse safety-critical\nscenarios, in terms of car-following and traffic signal compliance. Both DDPG\nand SAC models successfully handled the critical scenarios, while the DDPG\nmodel showed smoother action profiles compared to the SAC model. Overall, the\nresults confirm that DRL-based longitudinal vehicle control strategy at SI can\nhelp to improve traffic safety, efficiency, and comfort.", "AI": {"tldr": "\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u4fe1\u53f7\u4ea4\u53c9\u8def\u53e3\u7eb5\u5411\u8f66\u8f86\u63a7\u5236\u7b56\u7565\uff0c\u7ed3\u5408\u5b89\u5168\u3001\u6548\u7387\u548c\u8212\u9002\u5ea6\u4f18\u5316\uff0cDDPG\u548cSAC\u7b97\u6cd5\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4fe1\u53f7\u4ea4\u53c9\u8def\u53e3\u7684\u590d\u6742\u51b3\u7b56\u8fc7\u7a0b\u4f7f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63a7\u5236\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u517c\u987e\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u8212\u9002\u5ea6\u3002", "method": "\u63d0\u51fa\u7efc\u5408\u5956\u52b1\u51fd\u6570\uff0c\u7ed3\u5408\u8ddd\u79bb\u6548\u7387\u3001\u9ec4\u706f\u51b3\u7b56\u548c\u975e\u5bf9\u79f0\u52a0\u51cf\u901f\u54cd\u5e94\uff0c\u91c7\u7528DDPG\u548cSAC\u7b97\u6cd5\u5904\u7406\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u3002", "result": "\u76f8\u6bd4\u4eba\u7c7b\u9a7e\u9a76\uff0cRL\u6a21\u578b\u5728\u4fdd\u6301\u5b89\u5168\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7387\u548c\u66f4\u4f4e\u6025\u52a8\u5ea6\uff0cDDPG\u52a8\u4f5c\u66f4\u5e73\u6ed1\u3002", "conclusion": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u80fd\u6709\u6548\u63d0\u5347\u4fe1\u53f7\u4ea4\u53c9\u8def\u53e3\u7684\u4ea4\u901a\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u8212\u9002\u5ea6\u3002"}}
{"id": "2505.08792", "pdf": "https://arxiv.org/pdf/2505.08792", "abs": "https://arxiv.org/abs/2505.08792", "authors": ["Michelle Nashla Turcios", "Alicia E. Boyd", "Angela D. R. Smith", "Brittany Johnson"], "title": "A Preliminary Framework for Intersectionality in ML Pipelines", "categories": ["cs.LG", "cs.CY"], "comment": "Accepted for the 1st International Intersectionality and Software\n  Engineering Workshop, colocated with FSE 2025", "summary": "Machine learning (ML) has become a go-to solution for improving how we use,\nexperience, and interact with technology (and the world around us).\nUnfortunately, studies have repeatedly shown that machine learning technologies\nmay not provide adequate support for societal identities and experiences.\nIntersectionality is a sociological framework that provides a mechanism for\nexplicitly considering complex social identities, focusing on social justice\nand power. While the framework of intersectionality can support the development\nof technologies that acknowledge and support all members of society, it has\nbeen adopted and adapted in ways that are not always true to its foundations,\nthereby weakening its potential for impact. To support the appropriate adoption\nand use of intersectionality for more equitable technological outcomes, we\namplify the foundational intersectionality scholarship--Crenshaw, Combahee, and\nCollins (three C's), to create a socially relevant preliminary framework in\ndeveloping machine-learning solutions. We use this framework to evaluate and\nreport on the (mis)alignments of intersectionality application in machine\nlearning literature.", "AI": {"tldr": "\u6458\u8981\u8ba8\u8bba\u4e86\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u5728\u793e\u4f1a\u8eab\u4efd\u652f\u6301\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u5229\u7528\u4ea4\u53c9\u6027\u793e\u4f1a\u5b66\u6846\u67b6\u6765\u63d0\u5347\u6280\u672f\u7684\u516c\u5e73\u6027\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u673a\u5668\u5b66\u4e60\u6280\u672f\u7f3a\u4e4f\u5bf9\u793e\u4f1a\u8eab\u4efd\u591a\u6837\u6027\u7684\u5145\u5206\u652f\u6301\uff0c\u9700\u8981\u501f\u9274\u4ea4\u53c9\u6027\u7406\u8bba\u4ee5\u5b9e\u73b0\u66f4\u516c\u5e73\u7684\u6280\u672f\u7ed3\u679c\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7Crenshaw\u3001Combahee\u548cCollins\u7684\u4ea4\u53c9\u6027\u7406\u8bba\u6846\u67b6\uff0c\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u6587\u732e\u4e2d\u5bf9\u4ea4\u53c9\u6027\u5e94\u7528\u7684\uff08\u9519\u8bef\uff09\u5bf9\u9f50\u60c5\u51b5\u3002", "result": "\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524d\u673a\u5668\u5b66\u4e60\u6587\u732e\u4e2d\u4ea4\u53c9\u6027\u5e94\u7528\u7684\u504f\u5dee\u6216\u4e0d\u8db3\u3002", "conclusion": "\u7ed3\u8bba\u662f\u5bf9\u4ea4\u53c9\u6027\u7406\u8bba\u7684\u6b63\u786e\u5e94\u7528\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u516c\u5e73\u7684\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09039", "pdf": "https://arxiv.org/pdf/2505.09039", "abs": "https://arxiv.org/abs/2505.09039", "authors": ["Jingfeng Chen", "Raghuveer Thirukovalluru", "Junlin Wang", "Kaiwei Luo", "Bhuwan Dhingra"], "title": "Atomic Consistency Preference Optimization for Long-Form Question Answering", "categories": ["cs.CL"], "comment": "16 pages, 2 figures", "summary": "Large Language Models (LLMs) frequently produce factoid hallucinations -\nplausible yet incorrect answers. A common mitigation strategy is model\nalignment, which improves factual accuracy by training on curated factual and\nnon-factual pairs. However, this approach often relies on a stronger model\n(e.g., GPT-4) or an external knowledge base to assess factual correctness,\nwhich may not always be accessible. To address this, we propose Atomic\nConsistency Preference Optimization (ACPO), a self-supervised preference-tuning\nmethod that enhances factual accuracy without external supervision. ACPO\nleverages atomic consistency signals, i.e., the agreement of individual facts\nacross multiple stochastic responses, to identify high- and low-quality data\npairs for model alignment. By eliminating the need for costly GPT calls, ACPO\nprovides a scalable and efficient approach to improving factoid\nquestion-answering. Despite being self-supervised, empirical results\ndemonstrate that ACPO outperforms FactAlign, a strong supervised alignment\nbaseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its\neffectiveness in enhancing factual reliability without relying on external\nmodels or knowledge bases.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u81ea\u76d1\u7763\u504f\u597d\u4f18\u5316\u65b9\u6cd5ACPO\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u5373\u53ef\u63d0\u9ad8LLMs\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u51cf\u5c11\u5927\u6a21\u578b\u4e8b\u5b9e\u6027\u9519\u8bef\u7684\u5e38\u89c1\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5916\u90e8\u6a21\u578b\u6216\u77e5\u8bc6\u5e93\uff0c\u6210\u672c\u9ad8\u4e14\u4e0d\u53ef\u6301\u7eed\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u591a\u968f\u673a\u751f\u6210\u7b54\u6848\u4e2d\u539f\u5b50\u4e00\u81f4\u6027\u4fe1\u53f7\u7684\u5bf9\u6bd4\uff08\u4e8b\u5b9e\u4e00\u81f4\u7a0b\u5ea6\uff09\uff0c\u81ea\u52a8\u7b5b\u9009\u9ad8\u4f4e\u8d28\u91cf\u6570\u636e\u7528\u4e8e\u5bf9\u9f50\u8bad\u7ec3\u3002", "result": "\u5728LongFact\u548cBioGen\u6570\u636e\u96c6\u4e0a\uff0cACPO\u6bd4\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5FactAlign\u9ad81.95\u5206\u3002", "conclusion": "ACPO\u4e3a\u63d0\u5347\u6a21\u578b\u4e8b\u5b9e\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u81ea\u76d1\u7763\u65b9\u6848\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u8d44\u6e90\u3002"}}
{"id": "2505.08905", "pdf": "https://arxiv.org/pdf/2505.08905", "abs": "https://arxiv.org/abs/2505.08905", "authors": ["Michael Majurski", "Cynthia Matuszek"], "title": "Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Language Models (LMs) continue to advance, improving response quality and\ncoherence. Given Internet-scale training datasets, LMs have likely encountered\nmuch of what users might ask them to generate in some form during their\ntraining. A plethora of evaluation benchmarks have been constructed to assess\nmodel quality, response appropriateness, and reasoning capabilities. However,\nthe human effort required for benchmark construction is limited and being\nrapidly outpaced by the size and scope of the models under evaluation.\nAdditionally, having humans build a benchmark for every possible domain of\ninterest is impractical. Therefore, we propose a methodology for automating the\nconstruction of fact-based synthetic data model evaluations grounded in\ndocument populations. This work leverages those very same LMs to evaluate\ndomain-specific knowledge automatically, using only grounding documents (e.g.,\na textbook) as input. This synthetic data benchmarking approach corresponds\nwell with human curated questions with a Spearman ranking correlation of 0.96\nand a benchmark evaluation Pearson accuracy correlation of 0.79. This novel\ntool supports generating both multiple choice and open-ended synthetic data\nquestions to gain diagnostic insight of LM capability. We apply this\nmethodology to evaluate model performance on a recent relevant arXiv preprint,\ndiscovering a surprisingly strong performance from Gemma3 models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6784\u5efa\u57fa\u4e8e\u4e8b\u5b9e\u7684\u5408\u6210\u6570\u636e\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u81ea\u52a8\u8bc4\u4f30\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\uff0c\u4ec5\u9700\u57fa\u7840\u6587\u6863\uff08\u5982\u6559\u79d1\u4e66\uff09\u4f5c\u4e3a\u8f93\u5165\u3002\u8be5\u65b9\u6cd5\u4e0e\u4eba\u5de5\u6784\u5efa\u7684\u57fa\u51c6\u9ad8\u5ea6\u76f8\u5173\uff08Spearman\u6392\u540d\u76f8\u5173\u60270.96\uff0cPearson\u51c6\u786e\u6027\u76f8\u5173\u60270.79\uff09\uff0c\u5e76\u652f\u6301\u751f\u6210\u591a\u9009\u9898\u548c\u5f00\u653e\u5f0f\u95ee\u9898\uff0c\u7528\u4e8e\u8bca\u65adLM\u80fd\u529b\u3002\u5e94\u7528\u8be5\u65b9\u6cd5\u53d1\u73b0Gemma3\u6a21\u578b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7531\u4e8e\u7f51\u7edc\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u4f7f\u7528\uff0c\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u53ef\u80fd\u5df2\u5728\u8bad\u7ec3\u4e2d\u63a5\u89e6\u8fc7\u7528\u6237\u53ef\u80fd\u63d0\u51fa\u7684\u5404\u79cd\u95ee\u9898\u3002\u7136\u800c\uff0c\u4eba\u5de5\u6784\u5efa\u8bc4\u4f30\u57fa\u51c6\u7684\u9650\u5236\u6027\u548c\u4e0d\u53ef\u6269\u5c55\u6027\u4f7f\u5f97\u65e0\u6cd5\u8986\u76d6\u6240\u6709\u611f\u5174\u8da3\u7684\u9886\u57df\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6784\u5efa\u57fa\u4e8e\u4e8b\u5b9e\u7684\u5408\u6210\u6570\u636e\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5229\u7528LMs\u81ea\u52a8\u8bc4\u4f30\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\uff0c\u4ec5\u9700\u57fa\u7840\u6587\u6863\u4f5c\u4e3a\u8f93\u5165\u3002\u652f\u6301\u751f\u6210\u591a\u9009\u9898\u548c\u5f00\u653e\u5f0f\u95ee\u9898\u3002", "result": "\u81ea\u52a8\u5316\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u57fa\u51c6\u4e0e\u4eba\u5de5\u6784\u5efa\u7684\u95ee\u9898\u9ad8\u5ea6\u76f8\u5173\uff08Spearman\u6392\u540d\u76f8\u5173\u60270.96\uff0cPearson\u51c6\u786e\u6027\u76f8\u5173\u60270.79\uff09\u3002\u5e94\u7528\u8be5\u65b9\u6cd5\u53d1\u73b0Gemma3\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u9884\u671f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bca\u65ad\u6027\u95ee\u9898\uff0c\u5c55\u793a\u51fa\u5728\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u9886\u57df\u7279\u5b9a\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.08793", "pdf": "https://arxiv.org/pdf/2505.08793", "abs": "https://arxiv.org/abs/2505.08793", "authors": ["Monirul Islam Pavel", "Siyi Hu", "Mahardhika Pratama", "Ryszard Kowalczyk"], "title": "Onboard Optimization and Learning: A Survey", "categories": ["cs.LG", "cs.AR"], "comment": "36 pages, 5 figures, 3 tables", "summary": "Onboard learning is a transformative approach in edge AI, enabling real-time\ndata processing, decision-making, and adaptive model training directly on\nresource-constrained devices without relying on centralized servers. This\nparadigm is crucial for applications demanding low latency, enhanced privacy,\nand energy efficiency. However, onboard learning faces challenges such as\nlimited computational resources, high inference costs, and security\nvulnerabilities. This survey explores a comprehensive range of methodologies\nthat address these challenges, focusing on techniques that optimize model\nefficiency, accelerate inference, and support collaborative learning across\ndistributed devices. Approaches for reducing model complexity, improving\ninference speed, and ensuring privacy-preserving computation are examined\nalongside emerging strategies that enhance scalability and adaptability in\ndynamic environments. By bridging advancements in hardware-software co-design,\nmodel compression, and decentralized learning, this survey provides insights\ninto the current state of onboard learning to enable robust, efficient, and\nsecure AI deployment at the edge.", "AI": {"tldr": "\u7efc\u8ff0\u63a2\u8ba8\u4e86\u8fb9\u7f18AI\u4e2d\u7684\u677f\u8f7d\u5b66\u4e60\uff0c\u6311\u6218\u5305\u62ec\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u3001\u63a8\u7406\u6210\u672c\u9ad8\u548c\u5b89\u5168\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4f18\u5316\u6a21\u578b\u6548\u7387\u3001\u52a0\u901f\u63a8\u7406\u548c\u5206\u5e03\u5f0f\u534f\u4f5c\u7684\u65b9\u6cd5\u3002", "motivation": "\u8fb9\u7f18AI\u7684\u677f\u8f7d\u5b66\u4e60\u5bf9\u4f4e\u5ef6\u8fdf\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u80fd\u6548\u8981\u6c42\u9ad8\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u8d44\u6e90\u53d7\u9650\u548c\u5b89\u5168\u6311\u6218\u3002", "method": "\u7efc\u8ff0\u4e86\u6a21\u578b\u590d\u6742\u6027\u964d\u4f4e\u3001\u63a8\u7406\u52a0\u901f\u3001\u9690\u79c1\u4fdd\u62a4\u8ba1\u7b97\u7b49\u6280\u672f\uff0c\u5e76\u7814\u7a76\u4e86\u786c\u4ef6\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u3001\u6a21\u578b\u538b\u7f29\u548c\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7684\u8fdb\u5c55\u3002", "result": "\u901a\u8fc7\u591a\u79cd\u65b9\u6cd5\u4f18\u5316\u4e86\u677f\u8f7d\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u8fb9\u7f18AI\u7684\u677f\u8f7d\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f53\u524d\u8fdb\u5c55\u7684\u5168\u9762\u89c6\u89d2\uff0c\u652f\u6301\u5176\u66f4\u9ad8\u6548\u3001\u5b89\u5168\u7684\u90e8\u7f72\u3002"}}
{"id": "2505.09056", "pdf": "https://arxiv.org/pdf/2505.09056", "abs": "https://arxiv.org/abs/2505.09056", "authors": ["Brandon Smith", "Mohamed Reda Bouadjenek", "Tahsin Alamgir Kheya", "Phillip Dawson", "Sunil Aryal"], "title": "A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) represent a major step toward artificial general\nintelligence, significantly advancing our ability to interact with technology.\nWhile LLMs perform well on Natural Language Processing tasks -- such as\ntranslation, generation, code writing, and summarization -- questions remain\nabout their output similarity, variability, and ethical implications. For\ninstance, how similar are texts generated by the same model? How does this\ncompare across different models? And which models best uphold ethical\nstandards? To investigate, we used 5{,}000 prompts spanning diverse tasks like\ngeneration, explanation, and rewriting. This resulted in approximately 3\nmillion texts from 12 LLMs, including proprietary and open-source systems from\nOpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs\nfrom the same LLM are more similar to each other than to human-written texts;\n(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4\nproduces more varied responses; (3) LLM writing styles differ significantly,\nwith Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for\ndistinctiveness; (4) differences in vocabulary and tone underscore the\nlinguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate\ngreater gender balance and reduced bias. These results offer new insights into\nthe behavior and diversity of LLM outputs, helping guide future development and\nethical evaluation.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e8612\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6587\u672c\u8f93\u51fa\u76f8\u4f3c\u6027\u3001\u591a\u6837\u6027\u548c\u4f26\u7406\u8868\u73b0\uff0c\u53d1\u73b0\u540c\u6a21\u578b\u8f93\u51fa\u76f8\u4f3c\u5ea6\u9ad8\uff0c\u4e0d\u540c\u6a21\u578b\u98ce\u683c\u5dee\u5f02\u663e\u8457\uff0cGPT-4\u591a\u6837\u6027\u7a81\u51fa\uff0c\u90e8\u5206\u6a21\u578b\u5728\u6027\u522b\u5e73\u8861\u548c\u504f\u89c1\u51cf\u5c11\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u63a2\u7d22LLM\u8f93\u51fa\u6587\u672c\u7684\u76f8\u4f3c\u6027\u3001\u591a\u6837\u6027\u53ca\u4f26\u7406\u8868\u73b0\uff0c\u4ee5\u6307\u5bfc\u672a\u6765\u5f00\u53d1\u548c\u4f26\u7406\u8bc4\u4f30\u3002", "method": "\u4f7f\u75285,000\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u63d0\u793a\uff0c\u751f\u6210\u7ea6300\u4e07\u6587\u672c\uff0c\u5bf9\u6bd412\u79cdLLM\uff08\u5305\u62ec\u5f00\u6e90\u548c\u4e13\u6709\u7cfb\u7edf\uff09\u7684\u8f93\u51fa\u3002", "result": "\u53d1\u73b0\u540c\u6a21\u578b\u8f93\u51fa\u76f8\u4f3c\u5ea6\u9ad8\uff0c\u4e0d\u540c\u6a21\u578b\u98ce\u683c\u5dee\u5f02\u5927\uff08\u5982GPT-4\u591a\u6837\u6027\u663e\u8457\uff09\uff0c\u8bcd\u6c47\u548c\u8bed\u6c14\u5dee\u5f02\u660e\u663e\uff0c\u90e8\u5206\u6a21\u578b\u5728\u4f26\u7406\u6307\u6807\u4e0a\u66f4\u4f18\u3002", "conclusion": "\u7ed3\u679c\u4e3aLLM\u884c\u4e3a\u591a\u6837\u6027\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u6a21\u578b\u5f00\u53d1\u548c\u4f26\u7406\u6807\u51c6\u5236\u5b9a\u3002"}}
{"id": "2505.08988", "pdf": "https://arxiv.org/pdf/2505.08988", "abs": "https://arxiv.org/abs/2505.08988", "authors": ["Montaser Mohammedalamen", "Michael Bowling"], "title": "Generalization in Monitored Markov Decision Processes (Mon-MDPs)", "categories": ["cs.AI"], "comment": "Under Review", "summary": "Reinforcement learning (RL) typically models the interaction between the\nagent and environment as a Markov decision process (MDP), where the rewards\nthat guide the agent's behavior are always observable. However, in many\nreal-world scenarios, rewards are not always observable, which can be modeled\nas a monitored Markov decision process (Mon-MDP). Prior work on Mon-MDPs have\nbeen limited to simple, tabular cases, restricting their applicability to\nreal-world problems. This work explores Mon-MDPs using function approximation\n(FA) and investigates the challenges involved. We show that combining function\napproximation with a learned reward model enables agents to generalize from\nmonitored states with observable rewards, to unmonitored environment states\nwith unobservable rewards. Therefore, we demonstrate that such generalization\nwith a reward model achieves near-optimal policies in environments formally\ndefined as unsolvable. However, we identify a critical limitation of such\nfunction approximation, where agents incorrectly extrapolate rewards due to\novergeneralization, resulting in undesirable behaviors. To mitigate\novergeneralization, we propose a cautious police optimization method leveraging\nreward uncertainty. This work serves as a step towards bridging this gap\nbetween Mon-MDP theory and real-world applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u5956\u52b1\u4e0d\u5b8c\u5168\u53ef\u89c2\u6d4b\u7684Mon-MDP\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u51fd\u6570\u8fd1\u4f3c\u548c\u5956\u52b1\u6a21\u578b\u5b9e\u73b0\u4ece\u53ef\u89c2\u6d4b\u72b6\u6001\u5230\u4e0d\u53ef\u89c2\u6d4b\u72b6\u6001\u7684\u6cdb\u5316\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5956\u52b1\u4e0d\u786e\u5b9a\u6027\u7684\u8c28\u614e\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u8fc7\u5ea6\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u5956\u52b1\u5e76\u975e\u603b\u662f\u53ef\u89c2\u6d4b\uff0c\u800c\u73b0\u6709Mon-MDP\u7814\u7a76\u5c40\u9650\u4e8e\u7b80\u5355\u8868\u683c\u5316\u6848\u4f8b\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u51fd\u6570\u8fd1\u4f3c\u548c\u5956\u52b1\u6a21\u578b\u63d0\u5347Mon-MDP\u7684\u9002\u7528\u6027\uff0c\u5e76\u89e3\u51b3\u6cdb\u5316\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u51fd\u6570\u8fd1\u4f3c\u4e0e\u5b66\u4e60\u7684\u5956\u52b1\u6a21\u578b\uff0c\u5c06\u53ef\u89c2\u6d4b\u5956\u52b1\u7684\u72b6\u6001\u6cdb\u5316\u5230\u4e0d\u53ef\u89c2\u6d4b\u72b6\u6001\uff1b\u63d0\u51fa\u57fa\u4e8e\u5956\u52b1\u4e0d\u786e\u5b9a\u6027\u7684\u8c28\u614e\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u8fc7\u5ea6\u6cdb\u5316\u3002", "result": "\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u5728\u7406\u8bba\u4e0d\u53ef\u89e3\u73af\u5883\u4e2d\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u7b56\u7565\uff0c\u4f46\u9700\u8b66\u60d5\u56e0\u8fc7\u5ea6\u6cdb\u5316\u5bfc\u81f4\u7684\u9519\u8bef\u5956\u52b1\u5916\u63a8\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u51fd\u6570\u8fd1\u4f3c\u548c\u5956\u52b1\u6a21\u578b\u6269\u5c55\u4e86Mon-MDP\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u5f15\u5165\u8c28\u614e\u4f18\u5316\u65b9\u6cd5\u7f13\u89e3\u4e86\u8fc7\u5ea6\u6cdb\u5316\uff0c\u4e3a\u7406\u8bba\u5230\u5b9e\u9645\u7684\u8fc7\u6e21\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.08795", "pdf": "https://arxiv.org/pdf/2505.08795", "abs": "https://arxiv.org/abs/2505.08795", "authors": ["Andres Anabalon", "Hugo Garces", "Julio Oliva", "Jose Cifuentes"], "title": "The Geometry of Meaning: Perfect Spacetime Representations of Hierarchical Structures", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "7 pages, 3 figures", "summary": "We show that there is a fast algorithm that embeds hierarchical structures in\nthree-dimensional Minkowski spacetime. The correlation of data ends up purely\nencoded in the causal structure. Our model relies solely on oriented token\npairs -- local hierarchical signals -- with no access to global symbolic\nstructure. We apply our method to the corpus of \\textit{WordNet}. We provide a\nperfect embedding of the mammal sub-tree including ambiguities (more than one\nhierarchy per node) in such a way that the hierarchical structures get\ncompletely codified in the geometry and exactly reproduce the ground-truth. We\nextend this to a perfect embedding of the maximal unambiguous subset of the\n\\textit{WordNet} with 82{,}115 noun tokens and a single hierarchy per token. We\nintroduce a novel retrieval mechanism in which causality, not distance, governs\nhierarchical access. Our results seem to indicate that all discrete data has a\nperfect geometrical representation that is three-dimensional. The resulting\nembeddings are nearly conformally invariant, indicating deep connections with\ngeneral relativity and field theory. These results suggest that concepts,\ncategories, and their interrelations, namely hierarchical meaning itself, is\ngeometric.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4e09\u7ef4\u95f5\u53ef\u592b\u65af\u57fa\u65f6\u7a7a\u4e2d\u5feb\u901f\u5d4c\u5165\u5c42\u6b21\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u5c42\u6b21\u4fe1\u53f7\uff08\u5b9a\u5411\u4ee4\u724c\u5bf9\uff09\u5b9e\u73b0\uff0c\u65e0\u9700\u5168\u5c40\u7b26\u53f7\u7ed3\u6784\u3002\u5e94\u7528\u5728WordNet\u4e0a\uff0c\u6210\u529f\u5d4c\u5165\u54fa\u4e73\u52a8\u7269\u5b50\u6811\u53ca\u6700\u5927\u65e0\u6b67\u4e49\u5b50\u96c6\uff0c\u7ed3\u679c\u8868\u660e\u6240\u6709\u79bb\u6563\u6570\u636e\u90fd\u5b58\u5728\u5b8c\u7f8e\u7684\u4e09\u7ef4\u51e0\u4f55\u8868\u793a\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5c42\u6b21\u7ed3\u6784\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u51e0\u4f55\u65b9\u5f0f\uff08\u7279\u522b\u662f\u4e09\u7ef4\u95f5\u53ef\u592b\u65af\u57fa\u65f6\u7a7a\uff09\u5b8c\u7f8e\u8868\u793a\uff0c\u4ece\u800c\u63ed\u793a\u6982\u5ff5\u548c\u7c7b\u522b\u4e4b\u95f4\u7684\u5c42\u6b21\u5173\u7cfb\u7684\u51e0\u4f55\u672c\u8d28\u3002", "method": "\u4f7f\u7528\u5b9a\u5411\u4ee4\u724c\u5bf9\u4f5c\u4e3a\u5c40\u90e8\u5c42\u6b21\u4fe1\u53f7\uff0c\u65e0\u9700\u5168\u5c40\u7b26\u53f7\u7ed3\u6784\u3002\u901a\u8fc7\u56e0\u679c\u7ed3\u6784\u7f16\u7801\u6570\u636e\u76f8\u5173\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u6027\u800c\u975e\u8ddd\u79bb\u7684\u65b0\u578b\u68c0\u7d22\u673a\u5236\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u54fa\u4e73\u52a8\u7269\u5b50\u6811\u548cWordNet\u6700\u5927\u65e0\u6b67\u4e49\u5b50\u96c6\u7684\u5b8c\u7f8e\u5d4c\u5165\uff0c\u8bc1\u5b9e\u79bb\u6563\u6570\u636e\u5b58\u5728\u4e09\u7ef4\u51e0\u4f55\u8868\u793a\u3002\u5d4c\u5165\u7ed3\u679c\u8fd1\u4e4e\u5171\u5f62\u4e0d\u53d8\uff0c\u6697\u793a\u4e0e\u5e7f\u4e49\u76f8\u5bf9\u8bba\u548c\u573a\u8bba\u7684\u6df1\u5c42\u8054\u7cfb\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6982\u5ff5\u3001\u7c7b\u522b\u53ca\u5176\u76f8\u4e92\u5173\u7cfb\uff08\u5373\u5c42\u6b21\u610f\u4e49\u672c\u8eab\uff09\u662f\u51e0\u4f55\u7684\uff0c\u53ef\u901a\u8fc7\u4e09\u7ef4\u95f5\u53ef\u592b\u65af\u57fa\u65f6\u7a7a\u4e2d\u7684\u56e0\u679c\u7ed3\u6784\u5b8c\u7f8e\u8868\u793a\u3002"}}
{"id": "2505.09068", "pdf": "https://arxiv.org/pdf/2505.09068", "abs": "https://arxiv.org/abs/2505.09068", "authors": ["Jennifer Haase", "Paul H. P. Hanel", "Sebastian Pokutta"], "title": "S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This paper introduces S-DAT (Synthetic-Divergent Association Task), a\nscalable, multilingual framework for automated assessment of divergent thinking\n(DT) -a core component of human creativity. Traditional creativity assessments\nare often labor-intensive, language-specific, and reliant on subjective human\nratings, limiting their scalability and cross-cultural applicability. In\ncontrast, S-DAT leverages large language models and advanced multilingual\nembeddings to compute semantic distance -- a language-agnostic proxy for DT. We\nevaluate S-DAT across eleven diverse languages, including English, Spanish,\nGerman, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating\nrobust and consistent scoring across linguistic contexts. Unlike prior DAT\napproaches, the S-DAT shows convergent validity with other DT measures and\ncorrect discriminant validity with convergent thinking. This cross-linguistic\nflexibility allows for more inclusive, global-scale creativity research,\naddressing key limitations of earlier approaches. S-DAT provides a powerful\ntool for fairer, more comprehensive evaluation of cognitive flexibility in\ndiverse populations and can be freely assessed online:\nhttps://sdat.iol.zib.de/.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faS-DAT\uff08\u5408\u6210-\u53d1\u6563\u5173\u8054\u4efb\u52a1\uff09\uff0c\u4e00\u79cd\u591a\u8bed\u8a00\u7684\u521b\u9020\u529b\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u591a\u8bed\u8a00\u5d4c\u5165\u6765\u91cf\u5316\u8bed\u4e49\u8ddd\u79bb\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u521b\u9020\u529b\u8bc4\u4f30\u65b9\u6cd5\u7684\u8bed\u8a00\u548c\u6587\u5316\u9650\u5236\u3002", "motivation": "\u4f20\u7edf\u521b\u9020\u529b\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bc4\u5206\u4e14\u5c40\u9650\u4e8e\u7279\u5b9a\u8bed\u8a00\uff0c\u96be\u4ee5\u6269\u5c55\u548c\u8de8\u6587\u5316\u5e94\u7528\u3002S-DAT\u65e8\u5728\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u5b9e\u73b0\u66f4\u516c\u5e73\u3001\u5168\u7403\u5316\u7684\u521b\u9020\u529b\u7814\u7a76\u3002", "method": "S-DAT\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u591a\u8bed\u8a00\u5d4c\u5165\u6280\u672f\uff0c\u8ba1\u7b97\u8bed\u4e49\u8ddd\u79bb\u4f5c\u4e3a\u53d1\u6563\u601d\u7ef4\uff08DT\uff09\u7684\u8bed\u8a00\u65e0\u5173\u6307\u6807\uff0c\u5e76\u572811\u79cd\u8bed\u8a00\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "S-DAT\u8868\u73b0\u51fa\u4e0e\u5176\u4ed6DT\u6d4b\u91cf\u65b9\u6cd5\u7684\u6536\u655b\u6548\u5ea6\uff0c\u4e14\u80fd\u6b63\u786e\u533a\u5206\u805a\u5408\u601d\u7ef4\uff0c\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u8bc4\u5206\u4e00\u81f4\u6027\u5f3a\u3002", "conclusion": "S-DAT\u4e3a\u5168\u7403\u8303\u56f4\u5185\u66f4\u5305\u5bb9\u3001\u66f4\u5168\u9762\u7684\u521b\u9020\u529b\u7814\u7a76\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u5176\u5728\u7ebf\u53ef\u8bbf\u95ee\u6027\u8fdb\u4e00\u6b65\u63a8\u52a8\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2505.08995", "pdf": "https://arxiv.org/pdf/2505.08995", "abs": "https://arxiv.org/abs/2505.08995", "authors": ["Ardian Selmonaj", "Oleg Szehr", "Giacomo Del Rio", "Alessandro Antonucci", "Adrian Schneider", "Michael R\u00fcegsegger"], "title": "Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.RO"], "comment": "Published as journal chapter in Deep Learning Applications, Vol. 1,\n  by Taylor & Francis", "summary": "This work presents a Hierarchical Multi-Agent Reinforcement Learning\nframework for analyzing simulated air combat scenarios involving heterogeneous\nagents. The objective is to identify effective Courses of Action that lead to\nmission success within preset simulations, thereby enabling the exploration of\nreal-world defense scenarios at low cost and in a safe-to-fail setting.\nApplying deep Reinforcement Learning in this context poses specific challenges,\nsuch as complex flight dynamics, the exponential size of the state and action\nspaces in multi-agent systems, and the capability to integrate real-time\ncontrol of individual units with look-ahead planning. To address these\nchallenges, the decision-making process is split into two levels of\nabstraction: low-level policies control individual units, while a high-level\ncommander policy issues macro commands aligned with the overall mission\ntargets. This hierarchical structure facilitates the training process by\nexploiting policy symmetries of individual agents and by separating control\nfrom command tasks. The low-level policies are trained for individual combat\ncontrol in a curriculum of increasing complexity. The high-level commander is\nthen trained on mission targets given pre-trained control policies. The\nempirical validation confirms the advantages of the proposed framework.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u6a21\u62df\u7a7a\u6218\u573a\u666f\u4e2d\u7684\u5f02\u6784\u667a\u80fd\u4f53\uff0c\u4ee5\u4f4e\u6210\u672c\u5728\u5b89\u5168\u73af\u5883\u4e2d\u63a2\u7d22\u73b0\u5b9e\u9632\u5fa1\u573a\u666f\u3002\u901a\u8fc7\u5206\u5c42\u51b3\u7b56\uff08\u4f4e\u5c42\u63a7\u5236\u4e2a\u4f53\uff0c\u9ad8\u5c42\u6307\u6325\u6574\u4f53\uff09\u89e3\u51b3\u4e86\u590d\u6742\u52a8\u6001\u3001\u72b6\u6001\u7a7a\u95f4\u5927\u7b49\u6311\u6218\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6a21\u62df\u7a7a\u6218\u573a\u666f\uff0c\u4ee5\u4f4e\u6210\u672c\u548c\u5b89\u5168\u7684\u65b9\u5f0f\u63a2\u7d22\u73b0\u5b9e\u9632\u5fa1\u7b56\u7565\uff0c\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u590d\u6742\u52a8\u6001\u548c\u5927\u72b6\u6001\u7a7a\u95f4\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u4f4e\u5c42\u7b56\u7565\u63a7\u5236\u4e2a\u4f53\u5355\u4f4d\uff0c\u9ad8\u5c42\u6307\u6325\u5b98\u7b56\u7565\u53d1\u5e03\u5b8f\u89c2\u6307\u4ee4\u3002\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u9010\u6b65\u8bad\u7ec3\u4f4e\u5c42\u7b56\u7565\uff0c\u518d\u57fa\u4e8e\u9884\u8bad\u7ec3\u7b56\u7565\u8bad\u7ec3\u9ad8\u5c42\u6307\u6325\u5b98\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u5206\u5c42\u6846\u67b6\u6709\u6548\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u590d\u6742\u6027\u95ee\u9898\u3002", "conclusion": "\u5206\u5c42\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5728\u6a21\u62df\u7a7a\u6218\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u73b0\u5b9e\u9632\u5fa1\u573a\u666f\u7684\u63a2\u7d22\u63d0\u4f9b\u4e86\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.08803", "pdf": "https://arxiv.org/pdf/2505.08803", "abs": "https://arxiv.org/abs/2505.08803", "authors": ["Zizhao Hu", "Mohammad Rostami", "Jesse Thomason"], "title": "Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent research has highlighted the risk of generative model collapse, where\nperformance progressively degrades when continually trained on self-generated\ndata. However, existing exploration on model collapse is limited to single,\nunimodal models, limiting our understanding in more realistic scenarios, such\nas diverse multi-modal AI agents interacting autonomously through synthetic\ndata and continually evolving. We expand the synthetic data training and model\ncollapse study to multi-modal vision-language generative systems, such as\nvision-language models (VLMs) and text-to-image diffusion models, as well as\nrecursive generate-train loops with multiple models. We find that model\ncollapse, previously observed in single-modality generative models, exhibits\ndistinct characteristics in the multi-modal context, such as improved\nvision-language alignment and increased variance in VLM image-captioning task.\nAdditionally, we find that general approaches such as increased decoding\nbudgets, greater model diversity, and relabeling with frozen models can\neffectively mitigate model collapse. Our findings provide initial insights and\npractical guidelines for reducing the risk of model collapse in self-improving\nmulti-agent AI systems and curating robust multi-modal synthetic datasets.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u751f\u6210\u7cfb\u7edf\u4e2d\u6a21\u578b\u5d29\u6e83\u7684\u73b0\u8c61\uff0c\u6269\u5c55\u4e86\u4e4b\u524d\u5355\u6a21\u6001\u6a21\u578b\u7684\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u4e86\u7f13\u89e3\u6a21\u578b\u5d29\u6e83\u7684\u5b9e\u7528\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u591a\u6a21\u6001AI\u7cfb\u7edf\u5728\u6301\u7eed\u8bad\u7ec3\u81ea\u751f\u6210\u6570\u636e\u65f6\u7684\u6a21\u578b\u5d29\u6e83\u73b0\u8c61\uff0c\u53ca\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5728\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u5e94\u7528\u9012\u5f52\u751f\u6210-\u8bad\u7ec3\u5faa\u73af\uff0c\u5e76\u6d4b\u8bd5\u4e0d\u540c\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u591a\u6a21\u6001\u6a21\u578b\u5d29\u6e83\u8868\u73b0\u51fa\u72ec\u7279\u7279\u5f81\uff0c\u5982\u6539\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u548cVLM\u4efb\u52a1\u4e2d\u65b9\u5dee\u589e\u52a0\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u6709\u6548\u7f13\u89e3\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u4e3a\u591a\u6a21\u6001\u81ea\u6539\u8fdbAI\u7cfb\u7edf\u7684\u7a33\u5065\u6027\u63d0\u4f9b\u4e86\u521d\u6b65\u89c1\u89e3\u548c\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2505.09082", "pdf": "https://arxiv.org/pdf/2505.09082", "abs": "https://arxiv.org/abs/2505.09082", "authors": ["Sophie Zhang", "Zhiming Lin"], "title": "CEC-Zero: Chinese Error Correction Solution Based on LLM", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) demonstrate exceptional\nChinese text processing capabilities, particularly in Chinese Spelling\nCorrection (CSC). While LLMs outperform traditional BERT-based models in\naccuracy and robustness, challenges persist in reliability and generalization.\nThis paper proposes CEC-Zero, a novel reinforcement learning (RL) framework\nenabling LLMs to self-correct through autonomous error strategy learning\nwithout external supervision. By integrating RL with LLMs' generative power,\nthe method eliminates dependency on annotated data or auxiliary models.\nExperiments reveal RL-enhanced LLMs achieve industry-viable accuracy and\nsuperior cross-domain generalization, offering a scalable solution for\nreliability optimization in Chinese NLP applications. This breakthrough\nfacilitates LLM deployment in practical Chinese text correction scenarios while\nestablishing a new paradigm for self-improving language models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCEC-Zero\uff0c\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u81ea\u4e3b\u5b66\u4e60\u548c\u4fee\u6b63\u4e2d\u6587\u62fc\u5199\u9519\u8bef\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u6216\u6807\u6ce8\u6570\u636e\u3002\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u4e2d\u6587NLP\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eBERT\u7684\u4e2d\u6587\u62fc\u5199\u7ea0\u9519\u6a21\u578b\u5728\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u867d\u8868\u73b0\u4f18\u5f02\uff0c\u4ecd\u4f9d\u8d56\u5916\u90e8\u76d1\u7763\u6216\u8f85\u52a9\u6a21\u578b\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u5e72\u9884\u7684\u81ea\u4fee\u6b63\u6846\u67b6\u3002", "method": "\u63d0\u51faCEC-Zero\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f7f\u5176\u81ea\u4e3b\u5b66\u4e60\u548c\u4f18\u5316\u7ea0\u9519\u7b56\u7565\uff0c\u6446\u8131\u5bf9\u6807\u6ce8\u6570\u636e\u6216\u8f85\u52a9\u6a21\u578b\u7684\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u7ecf\u8fc7RL\u589e\u5f3a\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u6587\u62fc\u5199\u7ea0\u9519\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e1a\u754c\u53ef\u7528\u51c6\u786e\u7387\uff0c\u4e14\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CEC-Zero\u4e3a\u4e2d\u6587NLP\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u65b9\u6848\uff0c\u540c\u65f6\u4e3a\u81ea\u6539\u8fdb\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u63a8\u52a8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u6587\u672c\u7ea0\u9519\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u3002"}}
{"id": "2505.09012", "pdf": "https://arxiv.org/pdf/2505.09012", "abs": "https://arxiv.org/abs/2505.09012", "authors": ["Bo Meng", "Chenghao Xu", "Yongli Zhu"], "title": "Deep Reinforcement Learning for Power Grid Multi-Stage Cascading Failure Mitigation", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": "This paper has been accepted and presented at ICLR 2025 in Singapore,\n  Apr. 28, 2025", "summary": "Cascading failures in power grids can lead to grid collapse, causing severe\ndisruptions to social operations and economic activities. In certain cases,\nmulti-stage cascading failures can occur. However, existing\ncascading-failure-mitigation strategies are usually single-stage-based,\noverlooking the complexity of the multi-stage scenario. This paper treats the\nmulti-stage cascading failure problem as a reinforcement learning task and\ndevelops a simulation environment. The reinforcement learning agent is then\ntrained via the deterministic policy gradient algorithm to achieve continuous\nactions. Finally, the effectiveness of the proposed approach is validated on\nthe IEEE 14-bus and IEEE 118-bus systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u7ea7\u7ea7\u8054\u6545\u969c\u7f13\u89e3\u7b56\u7565\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u8bad\u7ec3\u667a\u80fd\u4f53\uff0c\u5728IEEE 14\u548c118\u6bcd\u7ebf\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u7ea7\u8054\u6545\u969c\u7f13\u89e3\u7b56\u7565\u591a\u4e3a\u5355\u7ea7\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u5e94\u5bf9\u590d\u6742\u7684\u591a\u7ea7\u6545\u969c\u573a\u666f\u3002", "method": "\u5c06\u591a\u7ea7\u7ea7\u8054\u6545\u969c\u5efa\u6a21\u4e3a\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u5f00\u53d1\u4eff\u771f\u73af\u5883\uff0c\u5e76\u91c7\u7528\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u8bad\u7ec3\u667a\u80fd\u4f53\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728IEEE 14\u548c118\u6bcd\u7ebf\u7cfb\u7edf\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u591a\u7ea7\u7ea7\u8054\u6545\u969c\uff0c\u4e3a\u7535\u7f51\u7a33\u5b9a\u8fd0\u884c\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2505.08823", "pdf": "https://arxiv.org/pdf/2505.08823", "abs": "https://arxiv.org/abs/2505.08823", "authors": ["Cody Steinmetz", "Gavin Childress", "Aaron Herbst", "Gavin Jones", "Jasdeep Singh", "Eli Vang", "Keagan Weinstock"], "title": "An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have transformed natural-language processing,\nyet their scale makes real-world deployment costly. Post-training quantization\nreduces memory and computation but often degrades accuracy, while\nquantization-aware training can recover performance at the cost of extra\ntraining. Pushing quantization to the ternary (2-bit) regime yields even larger\nsavings but is notoriously unstable. Building on recent work showing that a\nbias-free, RMS-normalized Transformer with straight-through estimation can\nreach 1.58-bit precision, we demonstrate that simply inserting RMS\nnormalization before every linear projection and applying a gradual, layer-wise\nquantization schedule stably fine-tunes full-precision checkpoints into ternary\nLLMs. Our approach matches or surpasses more elaborate knowledge-distillation\npipelines on standard language-modeling benchmarks without adding model\ncomplexity. These results indicate that careful normalization alone can close\nmuch of the accuracy gap between ternary and full-precision LLMs, making\nultra-low-bit inference practical.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5f52\u4e00\u5316\u548c\u6e10\u5c42\u91cf\u5316\u65b9\u6cd5\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7a33\u5b9a\u91cf\u5316\u4e3a2\u4f4d\u7cbe\u5ea6\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u5373\u53ef\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5e9e\u5927\u7684\u89c4\u6a21\u5bfc\u81f4\u5b9e\u9645\u90e8\u7f72\u6210\u672c\u9ad8\u6602\u3002\u91cf\u5316\u6280\u672f\u867d\u80fd\u964d\u4f4e\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5e38\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\u6216\u9700\u989d\u5916\u8bad\u7ec3\u3002\u5982\u4f55\u5728\u8d85\u4f4e\u6bd4\u7279\uff08\u59822\u4f4d\uff09\u4e0b\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u65b9\u6cd5\uff1a\u5728\u6bcf\u6b21\u7ebf\u6027\u6295\u5f71\u524d\u63d2\u5165RMS\u5f52\u4e00\u5316\u64cd\u4f5c\uff0c\u5e76\u91c7\u7528\u6e10\u5c42\u3001\u5206\u5c42\u7684\u91cf\u5316\u8c03\u5ea6\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u7a33\u5b9a\u5730\u5c06\u5168\u7cbe\u5ea6\u6a21\u578b\u5fae\u8c03\u4e3a2\u4f4dLLMs\uff0c\u65e0\u9700\u590d\u6742\u7684\u77e5\u8bc6\u84b8\u998f\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6807\u51c6\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u4f18\u4e8e\u66f4\u590d\u6742\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6848\uff0c\u540c\u65f6\u672a\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u3002RMS\u5f52\u4e00\u5316\u548c\u6e10\u5c42\u91cf\u5316\u663e\u8457\u7f29\u5c0f\u4e862\u4f4d\u4e0e\u5168\u7cbe\u5ea6\u6a21\u578b\u4e4b\u95f4\u7684\u7cbe\u5ea6\u5dee\u8ddd\u3002", "conclusion": "\u901a\u8fc7\u5f52\u4e00\u5316\u548c\u6e10\u5c42\u91cf\u5316\uff0c\u8bba\u6587\u8bc1\u660e\u4e86\u8d85\u4f4e\u6bd4\u7279\uff082\u4f4d\uff09\u63a8\u7406\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u4f4e\u6210\u672c\u9ad8\u6548\u90e8\u7f72LLMs\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2505.09269", "pdf": "https://arxiv.org/pdf/2505.09269", "abs": "https://arxiv.org/abs/2505.09269", "authors": ["Ulrich Frank", "Pierre Maier"], "title": "How an unintended Side Effect of a Research Project led to Boosting the Power of UML", "categories": ["cs.CL"], "comment": null, "summary": "This paper describes the design, implementation and use of a new UML modeling\ntool that represents a significant advance over conventional tools. Among other\nthings, it allows the integration of class diagrams and object diagrams as well\nas the execution of objects. This not only enables new software architectures\ncharacterized by the integration of software with corresponding object models,\nbut is also ideal for use in teaching, as it provides students with a\nparticularly stimulating learning experience. A special feature of the project\nis that it has emerged from a long-standing international research project,\nwhich is aimed at a comprehensive multi-level architecture. The project is\ntherefore an example of how research can lead to valuable results that arise as\na side effect of other work.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u6b3e\u65b0\u578bUML\u5efa\u6a21\u5de5\u5177\uff0c\u6574\u5408\u7c7b\u56fe\u4e0e\u5bf9\u8c61\u56fe\u5e76\u652f\u6301\u5bf9\u8c61\u6267\u884c\uff0c\u9002\u7528\u4e8e\u6559\u5b66\u4e0e\u7814\u7a76\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfUML\u5de5\u5177\u7684\u5c40\u9650\u6027\uff0c\u4fc3\u8fdb\u8f6f\u4ef6\u67b6\u6784\u521b\u65b0\u53ca\u6559\u5b66\u6548\u679c\u63d0\u5347\u3002", "method": "\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u96c6\u6210\u7c7b\u56fe\u548c\u5bf9\u8c61\u56fe\u7684\u65b0\u5de5\u5177\uff0c\u652f\u6301\u5bf9\u8c61\u6267\u884c\uff0c\u5e76\u5e94\u7528\u4e8e\u56fd\u9645\u7814\u7a76\u9879\u76ee\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u591a\u529f\u80fdUML\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u5efa\u6a21\u80fd\u529b\u4e0e\u6559\u5b66\u4f53\u9a8c\u3002", "conclusion": "\u8be5\u5de5\u5177\u662f\u7814\u7a76\u526f\u4ea7\u7269\u7684\u6210\u529f\u6848\u4f8b\uff0c\u5c55\u793a\u4e86\u8de8\u9886\u57df\u7814\u7a76\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.09024", "pdf": "https://arxiv.org/pdf/2505.09024", "abs": "https://arxiv.org/abs/2505.09024", "authors": ["Aaron Baughman", "Rahul Agarwal", "Eduardo Morales", "Gozde Akay"], "title": "Automated Meta Prompt Engineering for Alignment with the Theory of Mind", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "9 pages, 6 figures, 3 tables", "summary": "We introduce a method of meta-prompting that jointly produces fluent text for\ncomplex tasks while optimizing the similarity of neural states between a\nhuman's mental expectation and a Large Language Model's (LLM) neural\nprocessing. A technique of agentic reinforcement learning is applied, in which\nan LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,\nhow to produce content by interpreting the intended and unintended generated\ntext traits. To measure human mental beliefs around content production, users\nmodify long form AI-generated text articles before publication at the US Open\n2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)\nalignment problem by anticipating and including human edits within the creation\nof text from an LLM. Throughout experimentation and by interpreting the results\nof a live production system, the expectations of human content reviewers had\n100% of alignment with AI 53.8% of the time with an average iteration count of\n4.38. The geometric interpretation of content traits such as factualness,\nnovelty, repetitiveness, and relevancy over a Hilbert vector space combines\nspatial volume (all trait importance) with vertices alignment (individual trait\nrelevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an\nincrease in content quality by extending the coverage of tennis action. Our\nwork that was deployed at the US Open 2024 has been used across other live\nevents within sports and entertainment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5143\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u4eba\u7c7b\u9884\u671f\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u795e\u7ecf\u72b6\u6001\u7684\u76f8\u4f3c\u6027\uff0c\u751f\u6210\u590d\u6742\u4efb\u52a1\u7684\u6d41\u7545\u6587\u672c\u3002\u91c7\u7528\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u8ba9LLM\u4f5c\u4e3a\u6cd5\u5b98\uff08LLMaaJ\uff09\u6307\u5bfc\u53e6\u4e00\u4e2aLLM\u751f\u6210\u5185\u5bb9\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4eba\u7c7b\u5185\u5bb9\u7f16\u8f91\u7684\u9884\u671f\u5bf9\u9f50\u4e0a\u8fbe\u523053.8%\uff0c\u5e76\u63d0\u5347\u4e86\u5185\u5bb9\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5185\u5bb9\u4e0e\u4eba\u7c7b\u9884\u671f\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6587\u672c\u6d41\u7545\u6027\u548c\u5fc3\u7406\u9884\u671f\u5339\u914d\u3002\u901a\u8fc7\u4f18\u5316\u7c7b\u4f3c\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u7684\u795e\u7ecf\u72b6\u6001\uff0c\u63d0\u5347\u751f\u6210\u5185\u5bb9\u7684\u8d28\u91cf\u548c\u76f8\u5173\u6027\u3002", "method": "\u91c7\u7528\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u5229\u7528LLM\u4f5c\u4e3a\u6cd5\u5b98\uff08LLMaaJ\uff09\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u6307\u5bfc\u53e6\u4e00\u4e2aLLM\u751f\u6210\u5185\u5bb9\u3002\u901a\u8fc7\u5206\u6790\u4eba\u7c7b\u7f16\u8f91\u7684AI\u751f\u6210\u6587\u672c\uff0c\u91cf\u5316\u9884\u671f\u5bf9\u9f50\uff0c\u5e76\u5728Hilbert\u5411\u91cf\u7a7a\u95f4\u4e2d\u4f18\u5316\u5185\u5bb9\u7279\u6027\u3002", "result": "\u5728US Open 2024\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e8653.8%\u7684\u4eba\u7c7b\u9884\u671f\u5bf9\u9f50\uff0c\u5e73\u5747\u8fed\u4ee3\u6b21\u6570\u4e3a4.38\u3002\u5185\u5bb9\u8d28\u91cf\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u7f51\u7403\u8d5b\u4e8b\u7684\u8986\u76d6\u8303\u56f4\u4e0a\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5143\u63d0\u793a\u548cLLMaaJ\u80fd\u6709\u6548\u63d0\u5347LLM\u751f\u6210\u5185\u5bb9\u7684\u8d28\u91cf\u548c\u5bf9\u9f50\u6027\u3002\u8be5\u65b9\u6cd5\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u4f53\u80b2\u548c\u5a31\u4e50\u9886\u57df\u7684\u5176\u4ed6\u73b0\u573a\u6d3b\u52a8\u3002"}}
{"id": "2505.08827", "pdf": "https://arxiv.org/pdf/2505.08827", "abs": "https://arxiv.org/abs/2505.08827", "authors": ["Toby Simonds", "Kevin Lopez", "Akira Yoshiyama", "Dominique Garmier"], "title": "Self Rewarding Self Improving", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We demonstrate that large language models can effectively self-improve\nthrough self-judging without requiring reference solutions, leveraging the\ninherent asymmetry between generating and verifying solutions. Our experiments\non Countdown puzzles and MIT Integration Bee problems show that models can\nprovide reliable reward signals without ground truth answers, enabling\nreinforcement learning in domains previously not possible. By implementing\nself-judging, we achieve significant performance gains maintaining alignment\nwith formal verification. When combined with synthetic question generation, we\nestablish a complete self-improvement loop where models generate practice\nproblems, solve them, and evaluate their own performance-achieving an 8%\nimprovement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on\nintegration tasks. Our findings demonstrate that LLM judges can provide\neffective reward signals for training models, unlocking many reinforcement\nlearning environments previously limited by the difficulty of creating\nprogrammatic rewards. This suggests a potential paradigm shift toward AI\nsystems that continuously improve through self-directed learning rather than\nhuman-guided training, potentially accelerating progress in domains with scarce\ntraining data or complex evaluation requirements.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u901a\u8fc7\u81ea\u6211\u8bc4\u5224\u5b9e\u73b0\u81ea\u6211\u63d0\u5347\uff0c\u65e0\u9700\u53c2\u8003\u89e3\u51b3\u65b9\u6848\uff0c\u5229\u7528\u751f\u6210\u4e0e\u9a8c\u8bc1\u4e4b\u95f4\u7684\u4e0d\u5bf9\u79f0\u6027\u3002\u5728Countdown\u8c1c\u9898\u548cMIT\u79ef\u5206\u8d5b\u95ee\u9898\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u80fd\u63d0\u4f9b\u53ef\u9760\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u5b9e\u73b0\u5f3a\u5316\u5b66\u4e60\u3002\u7ed3\u5408\u5408\u6210\u95ee\u9898\u751f\u6210\uff0c\u6a21\u578b\u5728\u81ea\u6211\u8bc4\u5224\u4e2d\u6027\u80fd\u63d0\u53478%\uff0c\u8d85\u8d8aGPT-4o\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5bf9\u7a0b\u5e8f\u5316\u5956\u52b1\u4f9d\u8d56\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u6a21\u578b\u81ea\u6211\u8bc4\u5224\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u7684\u53ef\u884c\u6027\uff0c\u63a8\u52a8\u5728\u6570\u636e\u7a00\u7f3a\u6216\u8bc4\u4f30\u590d\u6742\u7684\u9886\u57df\u4e2d\u5b9e\u73b0\u81ea\u6211\u6301\u7eed\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u81ea\u6211\u8bc4\u5224\u6846\u67b6\uff0c\u6a21\u578b\u751f\u6210\u95ee\u9898\u3001\u89e3\u7b54\u5e76\u81ea\u6211\u8bc4\u5206\uff0c\u7ed3\u5408\u5408\u6210\u95ee\u9898\u751f\u6210\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "result": "\u5728Qwen 2.5 7B\u6a21\u578b\u4e0a\u6027\u80fd\u63d0\u53478%\uff0c\u5728\u79ef\u5206\u4efb\u52a1\u4e0a\u8d85\u8d8aGPT-4o\uff0c\u9a8c\u8bc1\u4e86\u81ea\u6211\u8bc4\u5224\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u81ea\u6211\u8bc4\u5224\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u6709\u671b\u5728\u6570\u636e\u7a00\u7f3a\u9886\u57df\u63a8\u52a8AI\u7cfb\u7edf\u7684\u81ea\u4e3b\u6301\u7eed\u5b66\u4e60\u3002"}}
{"id": "2505.09286", "pdf": "https://arxiv.org/pdf/2505.09286", "abs": "https://arxiv.org/abs/2505.09286", "authors": ["Jiin Park", "Misuk Kim"], "title": "A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data", "categories": ["cs.CL"], "comment": "36 pages, 3 figures", "summary": "Effectively analyzing online review data is essential across industries.\nHowever, many existing studies are limited to specific domains and languages or\ndepend on supervised learning approaches that require large-scale labeled\ndatasets. To address these limitations, we propose a multilingual, scalable,\nand unsupervised framework for cross-domain aspect detection. This framework is\ndesigned for multi-aspect labeling of multilingual and multi-domain review\ndata. In this study, we apply automatic labeling to Korean and English review\ndatasets spanning various domains and assess the quality of the generated\nlabels through extensive experiments. Aspect category candidates are first\nextracted through clustering, and each review is then represented as an\naspect-aware embedding vector using negative sampling. To evaluate the\nframework, we conduct multi-aspect labeling and fine-tune several pretrained\nlanguage models to measure the effectiveness of the automatically generated\nlabels. Results show that these models achieve high performance, demonstrating\nthat the labels are suitable for training. Furthermore, comparisons with\npublicly available large language models highlight the framework's superior\nconsistency and scalability when processing large-scale data. A human\nevaluation also confirms that the quality of the automatic labels is comparable\nto those created manually. This study demonstrates the potential of a robust\nmulti-aspect labeling approach that overcomes limitations of supervised methods\nand is adaptable to multilingual, multi-domain environments. Future research\nwill explore automatic review summarization and the integration of artificial\nintelligence agents to further improve the efficiency and depth of review\nanalysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8bed\u8a00\u3001\u53ef\u6269\u5c55\u7684\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u9886\u57df\u7684\u65b9\u9762\u68c0\u6d4b\uff0c\u9002\u7528\u4e8e\u591a\u8bed\u8a00\u548c\u591a\u9886\u57df\u8bc4\u8bba\u6570\u636e\u7684\u591a\u6807\u7b7e\u6807\u6ce8\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u51fa\u9ad8\u6548\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5c40\u9650\u4e8e\u7279\u5b9a\u9886\u57df\u548c\u8bed\u8a00\uff0c\u4e14\u4f9d\u8d56\u6709\u76d1\u7763\u5b66\u4e60\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4e3a\u6b64\u63d0\u51fa\u4e86\u65e0\u76d1\u7763\u7684\u591a\u8bed\u8a00\u3001\u8de8\u9886\u57df\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u805a\u7c7b\u63d0\u53d6\u65b9\u9762\u7c7b\u522b\u5019\u9009\uff0c\u4f7f\u7528\u8d1f\u91c7\u6837\u5c06\u6bcf\u6761\u8bc4\u8bba\u8868\u793a\u4e3a\u65b9\u9762\u611f\u77e5\u7684\u5d4c\u5165\u5411\u91cf\uff0c\u5e76\u5fae\u8c03\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u81ea\u52a8\u751f\u6210\u6807\u7b7e\u7684\u6548\u679c\u3002", "result": "\u6a21\u578b\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\uff0c\u81ea\u52a8\u751f\u6210\u6807\u7b7e\u9002\u7528\u4e8e\u8bad\u7ec3\uff0c\u4e14\u5728\u4e00\u81f4\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u4f18\u4e8e\u516c\u5f00\u53ef\u7528\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u4eba\u5de5\u8bc4\u4f30\u4e5f\u663e\u793a\u81ea\u52a8\u6807\u7b7e\u8d28\u91cf\u63a5\u8fd1\u624b\u52a8\u6807\u6ce8\u3002", "conclusion": "\u8be5\u6846\u67b6\u514b\u670d\u4e86\u6709\u76d1\u7763\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u9002\u7528\u4e8e\u591a\u8bed\u8a00\u548c\u591a\u9886\u57df\u73af\u5883\uff0c\u672a\u6765\u5c06\u63a2\u7d22\u81ea\u52a8\u8bc4\u8bba\u6458\u8981\u548cAI\u4ee3\u7406\u96c6\u6210\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u5206\u6790\u6548\u7387\u3002"}}
{"id": "2505.09029", "pdf": "https://arxiv.org/pdf/2505.09029", "abs": "https://arxiv.org/abs/2505.09029", "authors": ["Hazim Alzorgan", "Abolfazl Razi"], "title": "Monte Carlo Beam Search for Actor-Critic Reinforcement Learning in Continuous Control", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Actor-critic methods, like Twin Delayed Deep Deterministic Policy Gradient\n(TD3), depend on basic noise-based exploration, which can result in less than\noptimal policy convergence. In this study, we introduce Monte Carlo Beam Search\n(MCBS), a new hybrid method that combines beam search and Monte Carlo rollouts\nwith TD3 to improve exploration and action selection. MCBS produces several\ncandidate actions around the policy's output and assesses them through\nshort-horizon rollouts, enabling the agent to make better-informed choices. We\ntest MCBS across various continuous-control benchmarks, including\nHalfCheetah-v4, Walker2d-v5, and Swimmer-v5, showing enhanced sample efficiency\nand performance compared to standard TD3 and other baseline methods like SAC,\nPPO, and A2C. Our findings emphasize MCBS's capability to enhance policy\nlearning through structured look-ahead search while ensuring computational\nefficiency. Additionally, we offer a detailed analysis of crucial\nhyperparameters, such as beam width and rollout depth, and explore adaptive\nstrategies to optimize MCBS for complex control tasks. Our method shows a\nhigher convergence rate across different environments compared to TD3, SAC,\nPPO, and A2C. For instance, we achieved 90% of the maximum achievable reward\nwithin around 200 thousand timesteps compared to 400 thousand timesteps for the\nsecond-best method.", "AI": {"tldr": "MCBS\u7ed3\u5408\u6ce2\u675f\u641c\u7d22\u548c\u8499\u7279\u5361\u6d1b\u6a21\u62df\u6539\u8fdbTD3\u7684\u63a2\u7d22\u548c\u52a8\u4f5c\u9009\u62e9\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8eTD3\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u566a\u58f0\u7684\u63a2\u7d22\u65b9\u6cd5\uff08\u5982TD3\uff09\u53ef\u80fd\u5bfc\u81f4\u7b56\u7565\u6536\u655b\u4e0d\u7406\u60f3\uff0c\u9700\u6539\u8fdb\u63a2\u7d22\u6548\u7387\u3002", "method": "\u63d0\u51faMCBS\uff0c\u5229\u7528\u6ce2\u675f\u641c\u7d22\u751f\u6210\u5019\u9009\u52a8\u4f5c\u5e76\u901a\u8fc7\u77ed\u89c6\u754c\u8499\u7279\u5361\u6d1b\u6a21\u62df\u8bc4\u4f30\uff0c\u7ed3\u5408TD3\u6846\u67b6\u3002", "result": "\u5728HalfCheetah\u7b49\u73af\u5883\u4e2d\uff0cMCBS\u7684\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u5747\u4f18\u4e8eTD3\u3001SAC\u7b49\u65b9\u6cd5\uff0c\u6536\u655b\u901f\u5ea6\u66f4\u5feb\uff08200\u5343\u6b65\u8fbe90%\u6700\u5927\u5956\u52b1\uff09\u3002", "conclusion": "MCBS\u901a\u8fc7\u7ed3\u6784\u5316\u524d\u77bb\u641c\u7d22\u63d0\u5347\u7b56\u7565\u5b66\u4e60\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u63a7\u5236\u4efb\u52a1\u3002"}}
{"id": "2505.08829", "pdf": "https://arxiv.org/pdf/2505.08829", "abs": "https://arxiv.org/abs/2505.08829", "authors": ["David Kinney"], "title": "Aggregating Concepts of Fairness and Accuracy in Predictive Systems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "An algorithm that outputs predictions about the state of the world will\nalmost always be designed with the implicit or explicit goal of outputting\naccurate predictions (i.e., predictions that are likely to be true). In\naddition, the rise of increasingly powerful predictive algorithms brought about\nby the recent revolution in artificial intelligence has led to an emphasis on\nbuilding predictive algorithms that are fair, in the sense that their\npredictions do not systematically evince bias or bring about harm to certain\nindividuals or groups. This state of affairs presents two conceptual\nchallenges. First, the goals of accuracy and fairness can sometimes be in\ntension, and there are no obvious normative guidelines for managing the\ntrade-offs between these two desiderata when they arise. Second, there are many\ndistinct ways of measuring both the accuracy and fairness of a predictive\nalgorithm; here too, there are no obvious guidelines on how to aggregate our\npreferences for predictive algorithms that satisfy disparate measures of\nfairness and accuracy to various extents. The goal of this paper is to address\nthese challenges by arguing that there are good reasons for using a linear\ncombination of accuracy and fairness metrics to measure the\nall-things-considered value of a predictive algorithm for agents who care about\nboth accuracy and fairness. My argument depends crucially on a classic result\nin the preference aggregation literature due to Harsanyi. After making this\nformal argument, I apply my result to an analysis of accuracy-fairness\ntrade-offs using the COMPAS dataset compiled by Angwin et al.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u9884\u6d4b\u7b97\u6cd5\u5728\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ebf\u6027\u7ec4\u5408\u6765\u8861\u91cf\u517c\u987e\u4e24\u8005\u7684\u6574\u4f53\u4ef7\u503c\u3002", "motivation": "\u968f\u7740AI\u9884\u6d4b\u7b97\u6cd5\u7684\u666e\u53ca\uff0c\u5982\u4f55\u5728\u4fdd\u8bc1\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u907f\u514d\u7cfb\u7edf\u6027\u504f\u89c1\u6216\u4f24\u5bb3\u7279\u5b9a\u7fa4\u4f53\u6210\u4e3a\u91cd\u8981\u95ee\u9898\u3002", "method": "\u91c7\u7528Harsanyi\u7684\u504f\u597d\u805a\u5408\u7406\u8bba\uff0c\u63d0\u51fa\u7ebf\u6027\u7ec4\u5408\u65b9\u6cd5\u8bc4\u4f30\u7b97\u6cd5\u7684\u7efc\u5408\u4ef7\u503c\u3002", "result": "\u4f7f\u7528COMPAS\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u51c6\u786e\u6027\u4e0e\u516c\u5e73\u6027\u6743\u8861\u65f6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ebf\u6027\u7ec4\u5408\u4e3a\u8861\u91cf\u9884\u6d4b\u7b97\u6cd5\u7684\u7efc\u5408\u4ef7\u503c\u63d0\u4f9b\u4e86\u5408\u7406\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09316", "pdf": "https://arxiv.org/pdf/2505.09316", "abs": "https://arxiv.org/abs/2505.09316", "authors": ["Hongjin Qian", "Zheng Liu"], "title": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging", "categories": ["cs.CL", "cs.IR"], "comment": "16 pages", "summary": "Augmenting large language models (LLMs) with external retrieval has become a\nstandard method to address their inherent knowledge cutoff limitations.\nHowever, traditional retrieval-augmented generation methods employ static,\npre-inference retrieval strategies, making them inadequate for complex tasks\ninvolving ambiguous, multi-step, or evolving information needs. Recent advances\nin test-time scaling techniques have demonstrated significant potential in\nenabling LLMs to dynamically interact with external tools, motivating the shift\ntoward adaptive inference-time retrieval. Inspired by Information Foraging\nTheory (IFT), we propose InForage, a reinforcement learning framework that\nformalizes retrieval-augmented reasoning as a dynamic information-seeking\nprocess. Unlike existing approaches, InForage explicitly rewards intermediate\nretrieval quality, encouraging LLMs to iteratively gather and integrate\ninformation through adaptive search behaviors. To facilitate training, we\nconstruct a human-guided dataset capturing iterative search and reasoning\ntrajectories for complex, real-world web tasks. Extensive evaluations across\ngeneral question answering, multi-hop reasoning tasks, and a newly developed\nreal-time web QA dataset demonstrate InForage's superior performance over\nbaseline methods. These results highlight InForage's effectiveness in building\nrobust, adaptive, and efficient reasoning agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInForage\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4fe1\u606f\u68c0\u7d22\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u9759\u6001\u68c0\u7d22\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u68c0\u7d22\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u590d\u6742\u4efb\u52a1\u7684\u9700\u6c42\uff0c\u5c24\u5176\u662f\u6d89\u53ca\u6a21\u7cca\u3001\u591a\u6b65\u9aa4\u6216\u52a8\u6001\u4fe1\u606f\u9700\u6c42\u7684\u4efb\u52a1\u3002", "method": "\u57fa\u4e8e\u4fe1\u606f\u89c5\u98df\u7406\u8bba\uff08IFT\uff09\uff0c\u63d0\u51faInForage\u6846\u67b6\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5956\u52b1\u4e2d\u95f4\u68c0\u7d22\u8d28\u91cf\uff0c\u52a8\u6001\u4f18\u5316\u4fe1\u606f\u68c0\u7d22\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cInForage\u5728\u901a\u7528\u95ee\u7b54\u3001\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u548c\u5b9e\u65f6Web\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "InForage\u80fd\u6709\u6548\u6784\u5efa\u9c81\u68d2\u3001\u81ea\u9002\u5e94\u4e14\u9ad8\u6548\u7684\u63a8\u7406\u4ee3\u7406\uff0c\u5c55\u793a\u4e86\u5176\u5728\u52a8\u6001\u68c0\u7d22\u589e\u5f3a\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
{"id": "2505.09031", "pdf": "https://arxiv.org/pdf/2505.09031", "abs": "https://arxiv.org/abs/2505.09031", "authors": ["Adarsh Kumar", "Hwiyoon Kim", "Jawahar Sai Nathani", "Neil Roy"], "title": "Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Hallucination, where large language models (LLMs) generate confident but\nincorrect or irrelevant information, remains a key limitation in their\napplication to complex, open-ended tasks. Chain-of-thought (CoT) prompting has\nemerged as a promising method for improving multistep reasoning by guiding\nmodels through intermediate steps. However, CoT alone does not fully address\nthe hallucination problem. In this work, we investigate how combining CoT with\nretrieval-augmented generation (RAG), as well as applying self-consistency and\nself-verification strategies, can reduce hallucinations and improve factual\naccuracy. By incorporating external knowledge sources during reasoning and\nenabling models to verify or revise their own outputs, we aim to generate more\naccurate and coherent responses. We present a comparative evaluation of\nbaseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification\ntechniques. Our results highlight the effectiveness of each method and identify\nthe most robust approach for minimizing hallucinations while preserving fluency\nand reasoning depth.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u7ed3\u5408\u601d\u7ef4\u94fe\uff08CoT\uff09\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4ee5\u53ca\u81ea\u4e00\u81f4\u6027\u548c\u81ea\u9a8c\u8bc1\u7b56\u7565\uff0c\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5bb9\u6613\u51fa\u73b0\u81ea\u4fe1\u4f46\u9519\u8bef\u6216\u4e0d\u76f8\u5173\u7684\u4fe1\u606f\uff08\u5373\u5e7b\u89c9\uff09\uff0c\u5c3d\u7ba1\u601d\u7ef4\u94fe\u63d0\u793a\u6539\u5584\u4e86\u591a\u6b65\u63a8\u7406\uff0c\u4f46\u4ecd\u672a\u5b8c\u5168\u89e3\u51b3\u8be5\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u6e90\u548c\u81ea\u9a8c\u8bc1\u6280\u672f\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u751f\u6210\u5185\u5bb9\u7684\u51c6\u786e\u6027\u4e0e\u8fde\u8d2f\u6027\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u57fa\u7ebfLLM\u3001\u601d\u7ef4\u94fe\uff08CoT\uff09\u3001CoT+RAG\u3001\u81ea\u4e00\u81f4\u6027\u548c\u81ea\u9a8c\u8bc1\u6280\u672f\u7684\u6548\u679c\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u5728\u63a8\u7406\u4e2d\u5f15\u5165\u5916\u90e8\u77e5\u8bc6\uff08RAG\uff09\uff0c\u5e76\u8ba9\u6a21\u578b\u81ea\u6211\u9a8c\u8bc1\u6216\u4fee\u6b63\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u5c55\u793a\u4e86\u6bcf\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u786e\u5b9a\u4e86\u6700\u80fd\u5728\u4fdd\u6301\u6d41\u7545\u6027\u548c\u63a8\u7406\u6df1\u5ea6\u7684\u540c\u65f6\u51cf\u5c11\u5e7b\u89c9\u7684\u9c81\u68d2\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408CoT\u4e0eRAG\u53ca\u81ea\u9a8c\u8bc1\u7b56\u7565\u80fd\u663e\u8457\u964d\u4f4e\u5e7b\u89c9\u95ee\u9898\uff0c\u540c\u65f6\u7ef4\u6301\u751f\u6210\u5185\u5bb9\u7684\u8d28\u91cf\u548c\u903b\u8f91\u6027\uff0c\u4e3aLLM\u5728\u5f00\u653e\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.08846", "pdf": "https://arxiv.org/pdf/2505.08846", "abs": "https://arxiv.org/abs/2505.08846", "authors": ["Felix Marti-Perez", "Brigt H\u00e5vardstun", "C\u00e8sar Ferri", "Carlos Monserrat", "Jan Arne Telle"], "title": "Evaluating Simplification Algorithms for Interpretability of Time Series Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this work, we introduce metrics to evaluate the use of simplified time\nseries in the context of interpretability of a TSC - a Time Series Classifier.\nSuch simplifications are important because time series data, in contrast to\ntext and image data, are not intuitively understandable to humans. These\nmetrics are related to the complexity of the simplifications - how many\nsegments they contain - and to their loyalty - how likely they are to maintain\nthe classification of the original time series. We employ these metrics to\nevaluate four distinct simplification algorithms, across several TSC algorithms\nand across datasets of varying characteristics, from seasonal or stationary to\nshort or long. Our findings suggest that using simplifications for\ninterpretability of TSC is much better than using the original time series,\nparticularly when the time series are seasonal, non-stationary and/or with low\nentropy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8bc4\u4f30\u7b80\u5316\u65f6\u95f4\u5e8f\u5217\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u5668\uff08TSC\uff09\u53ef\u89e3\u91ca\u6027\u7684\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u76f8\u6bd4\u6587\u672c\u548c\u56fe\u50cf\u6570\u636e\u5bf9\u4eba\u7c7b\u4e0d\u76f4\u89c2\uff0c\u56e0\u6b64\u9700\u8981\u7b80\u5316\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e0e\u7b80\u5316\u590d\u6742\u5ea6\u548c\u5fe0\u8bda\u5ea6\u76f8\u5173\u7684\u6307\u6807\uff0c\u5e76\u8bc4\u4f30\u4e86\u56db\u79cd\u7b80\u5316\u7b97\u6cd5\u5728\u4e0d\u540cTSC\u7b97\u6cd5\u548c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u7b80\u5316\u65f6\u95f4\u5e8f\u5217\u6bd4\u539f\u59cb\u6570\u636e\u66f4\u9002\u5408TSC\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5c24\u5176\u5728\u5b63\u8282\u6027\u3001\u975e\u5e73\u7a33\u6216\u4f4e\u71b5\u6570\u636e\u4e0a\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "\u7b80\u5316\u65f6\u95f4\u5e8f\u5217\u663e\u8457\u63d0\u5347\u4e86TSC\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5c24\u5176\u5728\u7279\u5b9a\u7c7b\u578b\u6570\u636e\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2505.09338", "pdf": "https://arxiv.org/pdf/2505.09338", "abs": "https://arxiv.org/abs/2505.09338", "authors": ["Jingcheng Niu", "Xingdi Yuan", "Tong Wang", "Hamidreza Saghir", "Amir H. Abdi"], "title": "Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "We observe a novel phenomenon, contextual entrainment, across a wide range of\nlanguage models (LMs) and prompt settings, providing a new mechanistic\nperspective on how LMs become distracted by ``irrelevant'' contextual\ninformation in the input prompt. Specifically, LMs assign significantly higher\nlogits (or probabilities) to any tokens that have previously appeared in the\ncontext prompt, even for random tokens. This suggests that contextual\nentrainment is a mechanistic phenomenon, occurring independently of the\nrelevance or semantic relation of the tokens to the question or the rest of the\nsentence. We find statistically significant evidence that the magnitude of\ncontextual entrainment is influenced by semantic factors. Counterfactual\nprompts have a greater effect compared to factual ones, suggesting that while\ncontextual entrainment is a mechanistic phenomenon, it is modulated by semantic\nfactors.\n  We hypothesise that there is a circuit of attention heads -- the entrainment\nheads -- that corresponds to the contextual entrainment phenomenon. Using a\nnovel entrainment head discovery method based on differentiable masking, we\nidentify these heads across various settings. When we ``turn off'' these heads,\ni.e., set their outputs to zero, the effect of contextual entrainment is\nsignificantly attenuated, causing the model to generate output that capitulates\nto what it would produce if no distracting context were provided. Our discovery\nof contextual entrainment, along with our investigation into LM distraction via\nthe entrainment heads, marks a key step towards the mechanistic analysis and\nmitigation of the distraction problem.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u73b0\u8c61\u201c\u4e0a\u4e0b\u6587\u540c\u5316\u201d\uff0c\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u4f1a\u503e\u5411\u4e8e\u91cd\u590d\u63d0\u793a\u4e2d\u51fa\u73b0\u7684\u65e0\u5173\u8bcd\u6c47\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u673a\u5236\u53ca\u7f13\u89e3\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u4e3a\u4f55\u5bb9\u6613\u88ab\u8f93\u5165\u63d0\u793a\u4e2d\u7684\u65e0\u5173\u4fe1\u606f\u5e72\u6270\uff0c\u4ece\u800c\u5f71\u54cd\u8f93\u51fa\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u7edf\u8ba1\u5206\u6790\u548c\u4e00\u79cd\u53ef\u5fae\u5206\u63a9\u7801\u7684\u65b0\u65b9\u6cd5\uff0c\u8bc6\u522b\u4e0e\u4e0a\u4e0b\u6587\u540c\u5316\u76f8\u5173\u7684\u6ce8\u610f\u529b\u5934\uff08entrainment heads\uff09\uff0c\u5e76\u901a\u8fc7\u5173\u95ed\u8fd9\u4e9b\u5934\u51cf\u5f31\u5e72\u6270\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e0a\u4e0b\u6587\u540c\u5316\u662f\u673a\u5236\u6027\u73b0\u8c61\uff0c\u4e14\u5176\u5f3a\u5ea6\u53d7\u8bed\u4e49\u56e0\u7d20\u5f71\u54cd\uff1b\u5173\u95ed\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u53ef\u663e\u8457\u51cf\u5c11\u6a21\u578b\u5bf9\u65e0\u5173\u4fe1\u606f\u7684\u4f9d\u8d56\u3002", "conclusion": "\u8bba\u6587\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5206\u5fc3\u7684\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u7f13\u89e3\u65b9\u6cd5\uff0c\u4e3a\u6a21\u578b\u884c\u4e3a\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2505.09114", "pdf": "https://arxiv.org/pdf/2505.09114", "abs": "https://arxiv.org/abs/2505.09114", "authors": ["Minh Hoang Nguyen", "Linh Le Pham Van", "Thommen George Karimpanal", "Sunil Gupta", "Hung Le"], "title": "Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Decision Transformers (DT) play a crucial role in modern reinforcement\nlearning, leveraging offline datasets to achieve impressive results across\nvarious domains. However, DT requires high-quality, comprehensive data to\nperform optimally. In real-world applications, the lack of training data and\nthe scarcity of optimal behaviours make training on offline datasets\nchallenging, as suboptimal data can hinder performance. To address this, we\npropose the Counterfactual Reasoning Decision Transformer (CRDT), a novel\nframework inspired by counterfactual reasoning. CRDT enhances DT ability to\nreason beyond known data by generating and utilizing counterfactual\nexperiences, enabling improved decision-making in unseen scenarios. Experiments\nacross Atari and D4RL benchmarks, including scenarios with limited data and\naltered dynamics, demonstrate that CRDT outperforms conventional DT approaches.\nAdditionally, reasoning counterfactually allows the DT agent to obtain\nstitching abilities, combining suboptimal trajectories, without architectural\nmodifications. These results highlight the potential of counterfactual\nreasoning to enhance reinforcement learning agents' performance and\ngeneralization capabilities.", "AI": {"tldr": "CRDT\u6846\u67b6\u901a\u8fc7\u53cd\u4e8b\u5b9e\u63a8\u7406\u589e\u5f3a\u51b3\u7b56\u53d8\u6362\u5668\uff08DT\uff09\uff0c\u5728\u6570\u636e\u6709\u9650\u6216\u52a8\u6001\u6539\u53d8\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4f20\u7edfDT\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u8f68\u8ff9\u62fc\u63a5\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfDT\u5728\u79bb\u7ebf\u6570\u636e\u96c6\u8bad\u7ec3\u4e2d\u56e0\u6570\u636e\u4e0d\u8db3\u6216\u6b21\u4f18\u884c\u4e3a\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u53cd\u4e8b\u5b9e\u63a8\u7406\u51b3\u7b56\u53d8\u6362\u5668\uff08CRDT\uff09\uff0c\u901a\u8fc7\u751f\u6210\u548c\u5229\u7528\u53cd\u4e8b\u5b9e\u7ecf\u9a8c\u589e\u5f3a\u51b3\u7b56\u80fd\u529b\u3002", "result": "\u5728Atari\u548cD4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCRDT\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfDT\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u6216\u52a8\u6001\u6539\u53d8\u7684\u573a\u666f\u4e2d\u3002", "conclusion": "\u53cd\u4e8b\u5b9e\u63a8\u7406\u80fd\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u4fee\u6539\u67b6\u6784\u5373\u53ef\u5b9e\u73b0\u8f68\u8ff9\u62fc\u63a5\u3002"}}
{"id": "2505.08915", "pdf": "https://arxiv.org/pdf/2505.08915", "abs": "https://arxiv.org/abs/2505.08915", "authors": ["Jialin Mao", "Itay Griniasty", "Yan Sun", "Mark K. Transtrum", "James P. Sethna", "Pratik Chaudhari"], "title": "An Analytical Characterization of Sloppiness in Neural Networks: Insights from Linear Models", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech"], "comment": null, "summary": "Recent experiments have shown that training trajectories of multiple deep\nneural networks with different architectures, optimization algorithms,\nhyper-parameter settings, and regularization methods evolve on a remarkably\nlow-dimensional \"hyper-ribbon-like\" manifold in the space of probability\ndistributions. Inspired by the similarities in the training trajectories of\ndeep networks and linear networks, we analytically characterize this phenomenon\nfor the latter. We show, using tools in dynamical systems theory, that the\ngeometry of this low-dimensional manifold is controlled by (i) the decay rate\nof the eigenvalues of the input correlation matrix of the training data, (ii)\nthe relative scale of the ground-truth output to the weights at the beginning\nof training, and (iii) the number of steps of gradient descent. By analytically\ncomputing and bounding the contributions of these quantities, we characterize\nphase boundaries of the region where hyper-ribbons are to be expected. We also\nextend our analysis to kernel machines and linear models that are trained with\nstochastic gradient descent.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u7814\u7a76\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u8f68\u8ff9\u5728\u6982\u7387\u5206\u5e03\u7a7a\u95f4\u4e2d\u7684\u4f4e\u7ef4'\u8d85\u5e26\u72b6'\u6d41\u5f62\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u7f51\u7edc\u5206\u6790\u8fd9\u4e00\u73b0\u8c61\uff0c\u63a2\u8ba8\u5f71\u54cd\u56e0\u7d20\u53ca\u76f8\u4f4d\u8fb9\u754c\u3002", "motivation": "\u53d7\u6df1\u5ea6\u7f51\u7edc\u4e0e\u7ebf\u6027\u7f51\u7edc\u8bad\u7ec3\u8f68\u8ff9\u76f8\u4f3c\u6027\u7684\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u4ece\u7406\u8bba\u4e0a\u89e3\u91ca\u4f4e\u7ef4\u6d41\u5f62\u73b0\u8c61\uff0c\u4ee5\u7406\u89e3\u7f51\u7edc\u8bad\u7ec3\u7684\u5171\u6027\u89c4\u5f8b\u3002", "method": "\u5229\u7528\u52a8\u6001\u7cfb\u7edf\u7406\u8bba\u5de5\u5177\uff0c\u5206\u6790\u8f93\u5165\u76f8\u5173\u77e9\u9635\u7279\u5f81\u503c\u8870\u51cf\u7387\u3001\u6743\u91cd\u521d\u59cb\u5c3a\u5ea6\u4e0e\u8bad\u7ec3\u6b65\u6570\u5bf9\u4f4e\u7ef4\u6d41\u5f62\u51e0\u4f55\u7684\u5f71\u54cd\uff0c\u5e76\u6269\u5c55\u81f3\u6838\u673a\u5668\u4e0e\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7684\u7ebf\u6027\u6a21\u578b\u3002", "result": "\u660e\u786e\u4e86\u8d85\u5e26\u72b6\u6d41\u5f62\u5b58\u5728\u7684\u76f8\u4f4d\u8fb9\u754c\u6761\u4ef6\uff0c\u63ed\u793a\u4e86\u8f93\u5165\u6570\u636e\u3001\u521d\u59cb\u6743\u91cd\u53ca\u8bad\u7ec3\u6b65\u9aa4\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u63a8\u5e7f\u5230\u66f4\u5e7f\u6cdb\u7684\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684\u4f4e\u7ef4\u6d41\u5f62\u73b0\u8c61\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u7b97\u6cd5\u8bbe\u8ba1\u3002"}}
{"id": "2505.09388", "pdf": "https://arxiv.org/pdf/2505.09388", "abs": "https://arxiv.org/abs/2505.09388", "authors": ["An Yang", "Anfeng Li", "Baosong Yang", "Beichen Zhang", "Binyuan Hui", "Bo Zheng", "Bowen Yu", "Chang Gao", "Chengen Huang", "Chenxu Lv", "Chujie Zheng", "Dayiheng Liu", "Fan Zhou", "Fei Huang", "Feng Hu", "Hao Ge", "Haoran Wei", "Huan Lin", "Jialong Tang", "Jian Yang", "Jianhong Tu", "Jianwei Zhang", "Jianxin Yang", "Jiaxi Yang", "Jing Zhou", "Jingren Zhou", "Junyang Lin", "Kai Dang", "Keqin Bao", "Kexin Yang", "Le Yu", "Lianghao Deng", "Mei Li", "Mingfeng Xue", "Mingze Li", "Pei Zhang", "Peng Wang", "Qin Zhu", "Rui Men", "Ruize Gao", "Shixuan Liu", "Shuang Luo", "Tianhao Li", "Tianyi Tang", "Wenbiao Yin", "Xingzhang Ren", "Xinyu Wang", "Xinyu Zhang", "Xuancheng Ren", "Yang Fan", "Yang Su", "Yichang Zhang", "Yinger Zhang", "Yu Wan", "Yuqiong Liu", "Zekun Wang", "Zeyu Cui", "Zhenru Zhang", "Zhipeng Zhou", "Zihan Qiu"], "title": "Qwen3 Technical Report", "categories": ["cs.CL"], "comment": null, "summary": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.", "AI": {"tldr": "Qwen3\u662f\u4e00\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7cfb\u5217\uff0c\u652f\u6301\u591a\u8bed\u8a00\u548c\u9ad8\u6548\u8ba1\u7b97\uff0c\u6574\u5408\u601d\u8003\u4e0e\u975e\u601d\u8003\u6a21\u5f0f\uff0c\u5e76\u5f15\u5165\u8d44\u6e90\u5206\u914d\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u591a\u8bed\u8a00\u652f\u6301\u3002", "motivation": "\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3001\u6548\u7387\u548c\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u540c\u65f6\u4f18\u5316\u8d44\u6e90\u5206\u914d\u548c\u52a8\u6001\u4efb\u52a1\u5904\u7406\u3002", "method": "\u91c7\u7528\u5bc6\u96c6\u548c\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff08MoE\uff09\uff0c\u5f15\u5165\u601d\u7ef4\u9884\u7b97\u673a\u5236\u548c\u52a8\u6001\u6a21\u5f0f\u5207\u6362\uff0c\u51cf\u5c11\u5c0f\u578b\u6a21\u578b\u7684\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "result": "Qwen3\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u9886\u5148\u6c34\u5e73\uff0c\u652f\u6301119\u79cd\u8bed\u8a00\uff0c\u6027\u80fd\u4f18\u4e8e\u524d\u4ee3\u6a21\u578b\u3002", "conclusion": "Qwen3\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u548c\u8d44\u6e90\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u4efb\u52a1\u7684\u5904\u7406\u80fd\u529b\uff0c\u5e76\u5f00\u6e90\u4ee5\u652f\u6301\u793e\u533a\u7814\u7a76\u3002"}}
{"id": "2505.09289", "pdf": "https://arxiv.org/pdf/2505.09289", "abs": "https://arxiv.org/abs/2505.09289", "authors": ["Pedro M. P. Curvo", "Mara Dragomir", "Salvador Torpes", "Mohammadmahdi Rahimi"], "title": "Reproducibility Study of \"Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents\"", "categories": ["cs.AI"], "comment": "11 Tables, 9 Figures", "summary": "This study evaluates and extends the findings made by Piatti et al., who\nintroduced GovSim, a simulation framework designed to assess the cooperative\ndecision-making capabilities of large language models (LLMs) in\nresource-sharing scenarios. By replicating key experiments, we validate claims\nregarding the performance of large models, such as GPT-4-turbo, compared to\nsmaller models. The impact of the universalization principle is also examined,\nwith results showing that large models can achieve sustainable cooperation,\nwith or without the principle, while smaller models fail without it. In\naddition, we provide multiple extensions to explore the applicability of the\nframework to new settings. We evaluate additional models, such as DeepSeek-V3\nand GPT-4o-mini, to test whether cooperative behavior generalizes across\ndifferent architectures and model sizes. Furthermore, we introduce new\nsettings: we create a heterogeneous multi-agent environment, study a scenario\nusing Japanese instructions, and explore an \"inverse environment\" where agents\nmust cooperate to mitigate harmful resource distributions. Our results confirm\nthat the benchmark can be applied to new models, scenarios, and languages,\noffering valuable insights into the adaptability of LLMs in complex cooperative\ntasks. Moreover, the experiment involving heterogeneous multi-agent systems\ndemonstrates that high-performing models can influence lower-performing ones to\nadopt similar behaviors. This finding has significant implications for other\nagent-based applications, potentially enabling more efficient use of\ncomputational resources and contributing to the development of more effective\ncooperative AI systems.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u5e76\u6269\u5c55\u4e86Piatti\u7b49\u4eba\u7684GovSim\u6846\u67b6\uff0c\u9a8c\u8bc1\u4e86\u5927\u6a21\u578b\uff08\u5982GPT-4-turbo\uff09\u5728\u8d44\u6e90\u5171\u4eab\u573a\u666f\u4e2d\u7684\u5408\u4f5c\u80fd\u529b\u4f18\u4e8e\u5c0f\u6a21\u578b\uff0c\u5e76\u63a2\u8ba8\u4e86\u901a\u7528\u5316\u539f\u5219\u7684\u5f71\u54cd\u3002\u7814\u7a76\u8fd8\u901a\u8fc7\u65b0\u6a21\u578b\u3001\u591a\u8bed\u8a00\u548c\u5f02\u6784\u73af\u5883\u6269\u5c55\u4e86\u6846\u67b6\u9002\u7528\u6027\uff0c\u53d1\u73b0\u9ad8\u6027\u80fd\u6a21\u578b\u80fd\u5f71\u54cd\u4f4e\u6027\u80fd\u6a21\u578b\u7684\u5408\u4f5c\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u9a8c\u8bc1\u548c\u6269\u5c55GovSim\u6846\u67b6\uff0c\u8bc4\u4f30\u4e0d\u540c\u89c4\u6a21LLM\u5728\u5408\u4f5c\u51b3\u7b56\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u7d22\u8be5\u6846\u67b6\u5728\u65b0\u6a21\u578b\u3001\u65b0\u573a\u666f\u548c\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u590d\u73b0\u5173\u952e\u5b9e\u9a8c\u3001\u6d4b\u8bd5\u65b0\u6a21\u578b\uff08\u5982DeepSeek-V3\u548cGPT-4o-mini\uff09\u3001\u5f15\u5165\u5f02\u6784\u591a\u667a\u80fd\u4f53\u73af\u5883\u3001\u65e5\u8bed\u6307\u4ee4\u573a\u666f\u53ca\u9006\u5411\u73af\u5883\uff0c\u5206\u6790LLM\u7684\u5408\u4f5c\u884c\u4e3a\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5927\u6a21\u578b\u80fd\u5b9e\u73b0\u53ef\u6301\u7eed\u5408\u4f5c\uff08\u65e0\u8bba\u901a\u7528\u5316\u539f\u5219\u662f\u5426\u5b58\u5728\uff09\uff0c\u800c\u5c0f\u6a21\u578b\u9700\u4f9d\u8d56\u8be5\u539f\u5219\u3002\u6846\u67b6\u5728\u65b0\u6a21\u578b\u3001\u573a\u666f\u548c\u8bed\u8a00\u4e2d\u5747\u9002\u7528\uff0c\u4e14\u9ad8\u6027\u80fd\u6a21\u578b\u80fd\u5e26\u52a8\u4f4e\u6027\u80fd\u6a21\u578b\u63d0\u5347\u5408\u4f5c\u884c\u4e3a\u3002", "conclusion": "GovSim\u6846\u67b6\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4e3a\u590d\u6742\u5408\u4f5c\u4efb\u52a1\u4e2dLLM\u7684\u9002\u5e94\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5e76\u63ed\u793a\u4e86\u9ad8\u6027\u80fd\u6a21\u578b\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u79ef\u6781\u5f71\u54cd\uff0c\u5bf9\u5408\u4f5cAI\u53d1\u5c55\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2505.08940", "pdf": "https://arxiv.org/pdf/2505.08940", "abs": "https://arxiv.org/abs/2505.08940", "authors": ["Jeremie Blanchard", "Lisa Casino", "Jordan Gierschendorf"], "title": "NeurIPS 2024 Ariel Data Challenge: Characterisation of Exoplanetary Atmospheres Using a Data-Centric Approach", "categories": ["cs.LG", "astro-ph.IM"], "comment": "12 pages", "summary": "The characterization of exoplanetary atmospheres through spectral analysis is\na complex challenge. The NeurIPS 2024 Ariel Data Challenge, in collaboration\nwith the European Space Agency's (ESA) Ariel mission, provided an opportunity\nto explore machine learning techniques for extracting atmospheric compositions\nfrom simulated spectral data. In this work, we focus on a data-centric business\napproach, prioritizing generalization over competition-specific optimization.\nWe briefly outline multiple experimental axes, including feature extraction,\nsignal transformation, and heteroskedastic uncertainty modeling. Our\nexperiments demonstrate that uncertainty estimation plays a crucial role in the\nGaussian Log-Likelihood (GLL) score, impacting performance by several\npercentage points. Despite improving the GLL score by 11%, our results\nhighlight the inherent limitations of tabular modeling and feature engineering\nfor this task, as well as the constraints of a business-driven approach within\na Kaggle-style competition framework. Our findings emphasize the trade-offs\nbetween model simplicity, interpretability, and generalization in astrophysical\ndata analysis.", "AI": {"tldr": "\u5206\u6790\u7cfb\u5916\u884c\u661f\u5927\u6c14\u5149\u8c31\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5f3a\u8c03\u5e7f\u4e49\u5316\u800c\u975e\u7ade\u8d5b\u4f18\u5316\uff0c\u5b9e\u9a8c\u8868\u660e\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0c\u4f46\u8868\u683c\u5efa\u6a21\u548c\u7279\u5f81\u5de5\u7a0b\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u6280\u672f\u4ece\u6a21\u62df\u5149\u8c31\u6570\u636e\u4e2d\u63d0\u53d6\u5927\u6c14\u6210\u5206\uff0c\u5173\u6ce8\u5e7f\u4e49\u5316\u800c\u975e\u7ade\u8d5b\u4f18\u5316\uff0c\u4ee5\u89e3\u51b3\u5929\u4f53\u7269\u7406\u6570\u636e\u4e2d\u7684\u5b9e\u9645\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u3001\u4fe1\u53f7\u8f6c\u6362\u548c\u5f02\u65b9\u5dee\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u7b49\u591a\u8f74\u5b9e\u9a8c\u3002", "result": "\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u63d0\u5347GLL\u5206\u657011%\uff0c\u4f46\u63ed\u793a\u4e86\u8868\u683c\u5efa\u6a21\u548c\u7279\u5f81\u5de5\u7a0b\u7684\u5c40\u9650\u6027\uff0c\u53ca\u5546\u4e1a\u5316\u65b9\u6cd5\u5728Kaggle\u7ade\u8d5b\u6846\u67b6\u5185\u7684\u7ea6\u675f\u3002", "conclusion": "\u5929\u6587\u6570\u636e\u5206\u6790\u9700\u6743\u8861\u6a21\u578b\u7b80\u6d01\u6027\u3001\u53ef\u89e3\u91ca\u6027\u4e0e\u5e7f\u4e49\u5316\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u867d\u6709\u6f5c\u529b\u4f46\u4ecd\u53d7\u73b0\u6709\u6280\u672f\u9650\u5236\u3002"}}
{"id": "2505.09407", "pdf": "https://arxiv.org/pdf/2505.09407", "abs": "https://arxiv.org/abs/2505.09407", "authors": ["Subrit Dikshit", "Ritu Tiwari", "Priyank Jain"], "title": "Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits", "categories": ["cs.CL", "cs.AI", "cs.ET"], "comment": "12 pages, 12 figures", "summary": "Cloud-based multilingual translation services like Google Translate and\nMicrosoft Translator achieve state-of-the-art translation capabilities. These\nservices inherently use large multilingual language models such as GRU, LSTM,\nBERT, GPT, T5, or similar encoder-decoder architectures with attention\nmechanisms as the backbone. Also, new age natural language systems, for\ninstance ChatGPT and DeepSeek, have established huge potential in multiple\ntasks in natural language processing. At the same time, they also possess\noutstanding multilingual translation capabilities. However, these models use\nthe classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder\nAttention-based Convolutional Variational Circuits) is an alternate solution\nthat explores the quantum computing realm instead of the classical computing\nrealm to study and demonstrate multilingual machine translation. QEDACVC\nintroduces the quantum encoder-decoder architecture that simulates and runs on\nquantum computing hardware via quantum convolution, quantum pooling, quantum\nvariational circuit, and quantum attention as software alterations. QEDACVC\nachieves an Accuracy of 82% when trained on the OPUS dataset for English,\nFrench, German, and Hindi corpora for multilingual translations.", "AI": {"tldr": "QEDACVC\u662f\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u8ba1\u7b97\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u7528\u4e8e\u591a\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\uff0c\u51c6\u786e\u7387\u8fbe\u523082%\uff0c\u5bf9\u6bd4\u4f20\u7edf\u4e91\u8ba1\u7b97\u7ffb\u8bd1\u670d\u52a1\u5982Google Translate\u7b49\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50\u8ba1\u7b97\u5728\u591a\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u6f5c\u529b\uff0c\u63d0\u4f9b\u4e00\u79cd\u66ff\u4ee3\u4f20\u7edf\u4e91\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faQEDACVC\u67b6\u6784\uff0c\u7ed3\u5408\u91cf\u5b50\u5377\u79ef\u3001\u91cf\u5b50\u6c60\u5316\u3001\u91cf\u5b50\u53d8\u5206\u7535\u8def\u548c\u91cf\u5b50\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6a21\u62df\u5e76\u8fd0\u884c\u4e8e\u91cf\u5b50\u8ba1\u7b97\u786c\u4ef6\u4e0a\u3002", "result": "\u5728OPUS\u6570\u636e\u96c6\uff08\u82f1\u8bed\u3001\u6cd5\u8bed\u3001\u5fb7\u8bed\u3001\u5370\u5730\u8bed\uff09\u4e0a\u8bad\u7ec3\u540e\uff0cQEDACVC\u7684\u7ffb\u8bd1\u51c6\u786e\u7387\u4e3a82%\u3002", "conclusion": "QEDACVC\u5c55\u793a\u4e86\u91cf\u5b50\u8ba1\u7b97\u5728\u591a\u8bed\u8a00\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u91cf\u5b50\u8ba1\u7b97\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.09341", "pdf": "https://arxiv.org/pdf/2505.09341", "abs": "https://arxiv.org/abs/2505.09341", "authors": ["Ev\u017een Wybitul"], "title": "Access Controls Will Solve the Dual-Use Dilemma", "categories": ["cs.AI"], "comment": null, "summary": "AI safety systems face a dual-use dilemma. Since the same request can be\neither harmless or harmful depending on who made it and why, if the system\nmakes decisions based solely on the request's content, it will refuse some\nlegitimate queries and let pass harmful ones. To address this, we propose a\nconceptual access control framework, based on verified user credentials (such\nas institutional affiliation) and classifiers that assign model outputs to risk\ncategories (such as advanced virology). The system permits responses only when\nthe user's verified credentials match the category's requirements. For\nimplementation of the model output classifiers, we introduce a theoretical\napproach utilizing small, gated expert modules integrated into the generator\nmodel, trained with gradient routing, that enable efficient risk detection\nwithout the capability gap problems of external monitors. While open questions\nremain about the verification mechanisms, risk categories, and the technical\nimplementation, our framework makes the first step toward enabling granular\ngovernance of AI capabilities: verified users gain access to specialized\nknowledge without arbitrary restrictions, while adversaries are blocked from\nit. This contextual approach reconciles model utility with robust safety,\naddressing the dual-use dilemma.", "AI": {"tldr": "AI\u5b89\u5168\u7cfb\u7edf\u9762\u4e34\u53cc\u91cd\u7528\u9014\u56f0\u5883\uff0c\u63d0\u51fa\u57fa\u4e8e\u7528\u6237\u51ed\u8bc1\u548c\u98ce\u9669\u5206\u7c7b\u7684\u8bbf\u95ee\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u4e13\u5bb6\u6a21\u5757\u5b9e\u73b0\u9ad8\u6548\u98ce\u9669\u68c0\u6d4b\uff0c\u5e73\u8861\u6a21\u578b\u6548\u7528\u4e0e\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3AI\u7cfb\u7edf\u56e0\u4ec5\u4f9d\u8d56\u8bf7\u6c42\u5185\u5bb9\u5224\u65ad\u800c\u5bfc\u81f4\u7684\u5408\u6cd5\u8bf7\u6c42\u88ab\u62d2\u6216\u6709\u5bb3\u8bf7\u6c42\u901a\u8fc7\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5df2\u9a8c\u8bc1\u7528\u6237\u51ed\u8bc1\u548c\u6a21\u578b\u8f93\u51fa\u98ce\u9669\u5206\u7c7b\u7684\u8bbf\u95ee\u63a7\u5236\u6846\u67b6\uff0c\u5f15\u5165\u4e13\u5bb6\u6a21\u5757\u8bad\u7ec3\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u6548\u98ce\u9669\u68c0\u6d4b\u3002", "result": "\u6846\u67b6\u521d\u6b65\u5b9e\u73b0\u4e86\u5bf9AI\u80fd\u529b\u7684\u7ec6\u7c92\u5ea6\u7ba1\u7406\uff0c\u5408\u6cd5\u7528\u6237\u53ef\u8bbf\u95ee\u4e13\u4e1a\u77e5\u8bc6\uff0c\u540c\u65f6\u963b\u6b62\u6076\u610f\u884c\u4e3a\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u8bbf\u95ee\u63a7\u5236\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86AI\u7684\u53cc\u91cd\u7528\u9014\u56f0\u5883\uff0c\u517c\u987e\u6a21\u578b\u6548\u7528\u4e0e\u5b89\u5168\u3002"}}
{"id": "2505.08941", "pdf": "https://arxiv.org/pdf/2505.08941", "abs": "https://arxiv.org/abs/2505.08941", "authors": ["Gavin Hull", "Alex Bihlo"], "title": "ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers", "categories": ["cs.LG", "cs.CL"], "comment": "16 pages, 13 figures", "summary": "Predicting the future citation rates of academic papers is an important step\ntoward the automation of research evaluation and the acceleration of scientific\nprogress. We present $\\textbf{ForeCite}$, a simple but powerful framework to\nappend pre-trained causal language models with a linear head for average\nmonthly citation rate prediction. Adapting transformers for regression tasks,\nForeCite achieves a test correlation of $\\rho = 0.826$ on a curated dataset of\n900K+ biomedical papers published between 2000 and 2024, a 27-point improvement\nover the previous state-of-the-art. Comprehensive scaling-law analysis reveals\nconsistent gains across model sizes and data volumes, while temporal holdout\nexperiments confirm practical robustness. Gradient-based saliency heatmaps\nsuggest a potentially undue reliance on titles and abstract texts. These\nresults establish a new state-of-the-art in forecasting the long-term influence\nof academic research and lay the groundwork for the automated, high-fidelity\nevaluation of scientific contributions.", "AI": {"tldr": "ForeCite\u5229\u7528\u9884\u8bad\u7ec3\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u548c\u7ebf\u6027\u5934\u6765\u9884\u6d4b\u8bba\u6587\u7684\u6708\u5ea6\u5e73\u5747\u5f15\u7528\u7387\uff0c\u6d4b\u8bd5\u76f8\u5173\u6027\u8fbe\u03c1=0.826\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u65b9\u6cd5\u63d0\u534727\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u9884\u6d4b\u8bba\u6587\u672a\u6765\u5f15\u7528\u7387\u5bf9\u81ea\u52a8\u5316\u7814\u7a76\u8bc4\u4f30\u548c\u52a0\u901f\u79d1\u5b66\u8fdb\u6b65\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u56e0\u679c\u8bed\u8a00\u6a21\u578b\uff0c\u52a0\u5165\u7ebf\u6027\u5934\u8fdb\u884c\u56de\u5f52\u4efb\u52a1\uff0c\u4f18\u5316\u4e86\u53d8\u6362\u5668\u7684\u9002\u5e94\u6027\u3002", "result": "\u572890\u4e07+\u751f\u7269\u533b\u5b66\u8bba\u6587\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6a21\u578b\u89c4\u6a21\u548c\u6570\u636e\u91cf\u6269\u5c55\u5b9e\u9a8c\u663e\u793a\u4e00\u81f4\u589e\u76ca\u3002", "conclusion": "ForeCite\u5728\u9884\u6d4b\u5b66\u672f\u7814\u7a76\u957f\u671f\u5f71\u54cd\u529b\u65b9\u9762\u8fbe\u5230\u65b0\u9ad8\u5ea6\uff0c\u4e3a\u81ea\u52a8\u5316\u79d1\u5b66\u8d21\u732e\u8bc4\u4f30\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2505.09519", "pdf": "https://arxiv.org/pdf/2505.09519", "abs": "https://arxiv.org/abs/2505.09519", "authors": ["Zongqian Li", "Yixuan Su", "Nigel Collier"], "title": "PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning", "categories": ["cs.CL"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting\nlarge language models, yet existing approaches exhibit counter-intuitive\nphenomena: integrating router into prompt tuning (PT) increases training\nefficiency yet does not improve performance universally; parameter reduction\nthrough matrix decomposition can improve performance in specific domains.\nMotivated by these observations and the modular nature of PT, we propose\nPT-MoE, a novel framework that integrates matrix decomposition with\nmixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets\ndemonstrate that PT-MoE achieves state-of-the-art performance in both question\nanswering (QA) and mathematical problem solving tasks, improving F1 score by\n1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing\nmathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all\nwhile using 25% fewer parameters than LoRA. Our analysis reveals that while PT\nmethods generally excel in QA tasks and LoRA-based methods in math datasets,\nthe integration of matrix decomposition and MoE in PT-MoE yields complementary\nbenefits: decomposition enables efficient parameter sharing across experts\nwhile MoE provides dynamic adaptation, collectively enabling PT-MoE to\ndemonstrate cross-task consistency and generalization abilities. These\nfindings, along with ablation studies on routing mechanisms and architectural\ncomponents, provide insights for future PEFT methods.", "AI": {"tldr": "\u63d0\u51faPT-MoE\uff0c\u7ed3\u5408\u77e9\u9635\u5206\u89e3\u4e0eMoE\u8def\u7531\uff0c\u5728QA\u548c\u6570\u5b66\u4efb\u52a1\u4e2d\u5b9e\u73b0SOTA\uff0c\u53c2\u6570\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u73b0\u6709PEFT\u65b9\u6cd5\u5b58\u5728\u77db\u76fe\u73b0\u8c61\uff08\u5982\u8def\u7531\u63d0\u5347\u6548\u7387\u4f46\u6027\u80fd\u4e0d\u5747\uff09\uff0c\u6fc0\u53d1\u7ed3\u5408\u77e9\u9635\u5206\u89e3\u4e0eMoE\u4ee5\u4e92\u8865\u4f18\u52bf\u3002", "method": "\u6574\u5408\u77e9\u9635\u5206\u89e3\u4e0eMoE\u8def\u7531\uff0c\u901a\u8fc7\u5206\u89e3\u5b9e\u73b0\u53c2\u6570\u5171\u4eab\uff0cMoE\u52a8\u6001\u9002\u914d\uff0c\u5f62\u6210PT-MoE\u6846\u67b6\u3002", "result": "\u572817\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cQA\u4efb\u52a1F1\u63d0\u53471.49\u5206\uff0c\u6570\u5b66\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u534710.75\u5206\uff0c\u53c2\u6570\u51cf\u5c1125%\u3002", "conclusion": "PT-MoE\u5c55\u793a\u8de8\u4efb\u52a1\u4e00\u81f4\u6027\uff0c\u77e9\u9635\u5206\u89e3\u4e0eMoE\u534f\u540c\u589e\u6548\uff0c\u4e3a\u672a\u6765PEFT\u65b9\u6cd5\u63d0\u4f9b\u8bbe\u8ba1\u542f\u793a\u3002"}}
{"id": "2505.09396", "pdf": "https://arxiv.org/pdf/2505.09396", "abs": "https://arxiv.org/abs/2505.09396", "authors": ["Vince Trencsenyi", "Agnieszka Mensfelt", "Kostas Stathis"], "title": "The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "The rapid rise of large language models (LLMs) has shifted artificial\nintelligence (AI) research toward agentic systems, motivating the use of weaker\nand more flexible notions of agency. However, this shift raises key questions\nabout the extent to which LLM-based agents replicate human strategic reasoning,\nparticularly in game-theoretic settings. In this context, we examine the role\nof agentic sophistication in shaping artificial reasoners' performance by\nevaluating three agent designs: a simple game-theoretic model, an unstructured\nLLM-as-agent model, and an LLM integrated into a traditional agentic framework.\nUsing guessing games as a testbed, we benchmarked these agents against human\nparticipants across general reasoning patterns and individual role-based\nobjectives. Furthermore, we introduced obfuscated game scenarios to assess\nagents' ability to generalise beyond training distributions. Our analysis,\ncovering over 2000 reasoning samples across 25 agent configurations, shows that\nhuman-inspired cognitive structures can enhance LLM agents' alignment with\nhuman strategic behaviour. Still, the relationship between agentic design\ncomplexity and human-likeness is non-linear, highlighting a critical dependence\non underlying LLM capabilities and suggesting limits to simple architectural\naugmentation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u4ee3\u7406\u7cfb\u7edf\u5728\u535a\u5f08\u8bba\u8bbe\u7f6e\u4e2d\u5982\u4f55\u6a21\u62df\u4eba\u7c7b\u7b56\u7565\u63a8\u7406\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e09\u79cd\u4ee3\u7406\u8bbe\u8ba1\u7684\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e86\u89e3LLM\u4ee3\u7406\u662f\u5426\u80fd\u590d\u5236\u4eba\u7c7b\u7b56\u7565\u63a8\u7406\uff0c\u5e76\u63a2\u7d22\u4ee3\u7406\u8bbe\u8ba1\u7684\u590d\u6742\u6027\u5982\u4f55\u5f71\u54cd\u5176\u4e0e\u4eba\u7c7b\u884c\u4e3a\u7684\u5bf9\u9f50\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u8bbe\u8ba1\u4e09\u79cd\u4ee3\u7406\uff08\u7b80\u5355\u535a\u5f08\u8bba\u6a21\u578b\u3001\u975e\u7ed3\u6784\u5316LLM\u4ee3\u7406\u3001LLM\u4e0e\u4f20\u7edf\u4ee3\u7406\u6846\u67b6\u96c6\u6210\uff09\uff0c\u5e76\u901a\u8fc7\u731c\u6d4b\u6e38\u620f\u6d4b\u8bd5\u5176\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u53d7\u4eba\u7c7b\u8ba4\u77e5\u7ed3\u6784\u542f\u53d1\u7684\u8bbe\u8ba1\u80fd\u63d0\u5347LLM\u4ee3\u7406\u4e0e\u4eba\u7c7b\u884c\u4e3a\u7684\u5bf9\u9f50\uff0c\u4f46\u590d\u6742\u6027\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u6027\u7684\u5173\u7cfb\u662f\u975e\u7ebf\u6027\u7684\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0cLLM\u4ee3\u7406\u7684\u6027\u80fd\u4f9d\u8d56\u4e8e\u5176\u5e95\u5c42\u80fd\u529b\uff0c\u7b80\u5355\u7684\u67b6\u6784\u589e\u5f3a\u53ef\u80fd\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2505.08964", "pdf": "https://arxiv.org/pdf/2505.08964", "abs": "https://arxiv.org/abs/2505.08964", "authors": ["Majed Jaber", "Julien Michel", "Nicolas Boutry", "Pierre Parrend"], "title": "GPML: Graph Processing for Machine Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "The dramatic increase of complex, multi-step, and rapidly evolving attacks in\ndynamic networks involves advanced cyber-threat detectors. The GPML (Graph\nProcessing for Machine Learning) library addresses this need by transforming\nraw network traffic traces into graph representations, enabling advanced\ninsights into network behaviors. The library provides tools to detect anomalies\nin interaction and community shifts in dynamic networks. GPML supports\ncommunity and spectral metrics extraction, enhancing both real-time detection\nand historical forensics analysis. This library supports modern cybersecurity\nchallenges with a robust, graph-based approach.", "AI": {"tldr": "GPML\u5e93\u901a\u8fc7\u5c06\u539f\u59cb\u7f51\u7edc\u6d41\u91cf\u6570\u636e\u8f6c\u5316\u4e3a\u56fe\u8868\u793a\uff0c\u63d0\u4f9b\u9ad8\u7ea7\u7f51\u7edc\u884c\u4e3a\u6d1e\u5bdf\uff0c\u652f\u6301\u52a8\u6001\u7f51\u7edc\u4e2d\u5f02\u5e38\u548c\u793e\u533a\u53d8\u5316\u7684\u68c0\u6d4b\uff0c\u589e\u5f3a\u5b9e\u65f6\u68c0\u6d4b\u548c\u5386\u53f2\u53d6\u8bc1\u5206\u6790\u80fd\u529b\u3002", "motivation": "\u5e94\u5bf9\u52a8\u6001\u7f51\u7edc\u4e2d\u590d\u6742\u3001\u591a\u6b65\u9aa4\u4e14\u5feb\u901f\u6f14\u53d8\u7684\u653b\u51fb\uff0c\u9700\u8981\u5148\u8fdb\u7684\u7f51\u7edc\u5a01\u80c1\u68c0\u6d4b\u5de5\u5177\u3002", "method": "\u5229\u7528\u56fe\u5904\u7406\u6280\u672f\uff08GPML\u5e93\uff09\u5c06\u7f51\u7edc\u6d41\u91cf\u8f6c\u5316\u4e3a\u56fe\u8868\u793a\uff0c\u5e76\u63d0\u53d6\u793e\u533a\u548c\u8c31\u5ea6\u91cf\u4ee5\u68c0\u6d4b\u5f02\u5e38\u3002", "result": "GPML\u5e93\u80fd\u6709\u6548\u652f\u6301\u5b9e\u65f6\u5a01\u80c1\u68c0\u6d4b\u548c\u5386\u53f2\u53d6\u8bc1\u5206\u6790\uff0c\u63d0\u5347\u7f51\u7edc\u5b89\u5168\u9632\u62a4\u80fd\u529b\u3002", "conclusion": "GPML\u5e93\u901a\u8fc7\u56fe\u5904\u7406\u65b9\u6cd5\u4e3a\u73b0\u4ee3\u7f51\u7edc\u5b89\u5168\u6311\u6218\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09595", "pdf": "https://arxiv.org/pdf/2505.09595", "abs": "https://arxiv.org/abs/2505.09595", "authors": ["Abdullah Mushtaq", "Imran Taj", "Rafay Naeem", "Ibrahim Ghaznavi", "Junaid Qadir"], "title": "WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "comment": "Preprint. Submitted to the Journal of Artificial Intelligence\n  Research (JAIR) on April 29, 2025", "summary": "Large Language Models (LLMs) are predominantly trained and aligned in ways\nthat reinforce Western-centric epistemologies and socio-cultural norms, leading\nto cultural homogenization and limiting their ability to reflect global\ncivilizational plurality. Existing benchmarking frameworks fail to adequately\ncapture this bias, as they rely on rigid, closed-form assessments that overlook\nthe complexity of cultural inclusivity. To address this, we introduce\nWorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity\n(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our\napproach is grounded in the Multiplex Worldview proposed by Senturk et al.,\nwhich distinguishes between Uniplex models, reinforcing cultural\nhomogenization, and Multiplex models, which integrate diverse perspectives.\nWorldView-Bench measures Cultural Polarization, the exclusion of alternative\nperspectives, through free-form generative evaluation rather than conventional\ncategorical benchmarks. We implement applied multiplexity through two\nintervention strategies: (1) Contextually-Implemented Multiplex LLMs, where\nsystem prompts embed multiplexity principles, and (2) Multi-Agent System\n(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing\ndistinct cultural perspectives collaboratively generate responses. Our results\ndemonstrate a significant increase in Perspectives Distribution Score (PDS)\nentropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,\nalongside a shift toward positive sentiment (67.7%) and enhanced cultural\nbalance. These findings highlight the potential of multiplex-aware AI\nevaluation in mitigating cultural bias in LLMs, paving the way for more\ninclusive and ethically aligned AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faWorldView-Bench\u57fa\u51c6\uff0c\u8bc4\u4f30LLM\u7684\u5168\u7403\u6587\u5316\u5305\u5bb9\u6027\uff08GCI\uff09\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u6a21\u578b\u51cf\u5c11\u897f\u65b9\u4e2d\u5fc3\u504f\u89c1\uff0c\u63d0\u5347\u6587\u5316\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u7684LLM\u8bad\u7ec3\u548c\u8bc4\u4f30\u65b9\u6cd5\u5f3a\u5316\u897f\u65b9\u4e2d\u5fc3\u4e3b\u4e49\uff0c\u5bfc\u81f4\u6587\u5316\u540c\u8d28\u5316\uff0c\u7f3a\u4e4f\u5168\u7403\u6587\u660e\u591a\u6837\u6027\u3002\u73b0\u6709\u57fa\u51c6\u6846\u67b6\u65e0\u6cd5\u5145\u5206\u6355\u6349\u8fd9\u79cd\u504f\u89c1\u3002", "method": "\u5f15\u5165WorldView-Bench\u57fa\u51c6\uff0c\u57fa\u4e8e\u591a\u89c6\u89d2\u4e16\u754c\u89c2\u7406\u8bba\uff0c\u901a\u8fc7\u81ea\u7531\u751f\u6210\u8bc4\u4f30\u6587\u5316\u6781\u5316\u3002\u91c7\u7528\u4e24\u79cd\u5e72\u9884\u7b56\u7565\uff1a\u4e0a\u4e0b\u6587\u5b9e\u73b0\u591a\u89c6\u89d2LLM\u548c\u591a\u4ee3\u7406\u7cfb\u7edf\u5b9e\u73b0\u591a\u89c6\u89d2LLM\u3002", "result": "\u591a\u4ee3\u7406\u7cfb\u7edf\u5b9e\u73b0\u7684LLM\u5c06\u89c6\u89d2\u5206\u5e03\u71b5\u4ece\u57fa\u7ebf13%\u63d0\u5347\u81f394%\uff0c\u6b63\u9762\u60c5\u611f\u6bd4\u4f8b\u8fbe67.7%\uff0c\u6587\u5316\u5e73\u8861\u6027\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u591a\u89c6\u89d2\u611f\u77e5\u7684AI\u8bc4\u4f30\u80fd\u6709\u6548\u51cf\u5c11LLM\u7684\u6587\u5316\u504f\u89c1\uff0c\u63a8\u52a8\u66f4\u5305\u5bb9\u548c\u4f26\u7406\u5bf9\u9f50\u7684AI\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2505.09412", "pdf": "https://arxiv.org/pdf/2505.09412", "abs": "https://arxiv.org/abs/2505.09412", "authors": ["Paul Kobialka", "Lina Gerlach", "Francesco Leofante", "Erika \u00c1brah\u00e1m", "Silvia Lizeth Tapia Tarifa", "Einar Broch Johnsen"], "title": "Counterfactual Strategies for Markov Decision Processes", "categories": ["cs.AI", "I.2.m"], "comment": null, "summary": "Counterfactuals are widely used in AI to explain how minimal changes to a\nmodel's input can lead to a different output. However, established methods for\ncomputing counterfactuals typically focus on one-step decision-making, and are\nnot directly applicable to sequential decision-making tasks. This paper fills\nthis gap by introducing counterfactual strategies for Markov Decision Processes\n(MDPs). During MDP execution, a strategy decides which of the enabled actions\n(with known probabilistic effects) to execute next. Given an initial strategy\nthat reaches an undesired outcome with a probability above some limit, we\nidentify minimal changes to the initial strategy to reduce that probability\nbelow the limit. We encode such counterfactual strategies as solutions to\nnon-linear optimization problems, and further extend our encoding to synthesize\ndiverse counterfactual strategies. We evaluate our approach on four real-world\ndatasets and demonstrate its practical viability in sophisticated sequential\ndecision-making tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u7684\u53cd\u4e8b\u5b9e\u7b56\u7565\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u5728\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u7a7a\u767d\u3002\u901a\u8fc7\u4f18\u5316\u95ee\u9898\u7f16\u7801\uff0c\u751f\u6210\u6700\u5c0f\u5316\u4fee\u6539\u4ee5\u5b9e\u73b0\u671f\u671b\u76ee\u6807\u7684\u7b56\u7565\uff0c\u5e76\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u6b65\u51b3\u7b56\uff0c\u4e0d\u9002\u7528\u4e8e\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\uff08\u5982MDP\uff09\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e00\u79cd\u5728\u590d\u6742\u5e8f\u5217\u51b3\u7b56\u4e2d\u8ba1\u7b97\u53cd\u4e8b\u5b9e\u7b56\u7565\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5c06\u53cd\u4e8b\u5b9e\u7b56\u7565\u5efa\u6a21\u4e3a\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\uff0c\u8bc6\u522b\u521d\u59cb\u7b56\u7565\u7684\u6700\u5c0f\u4fee\u6539\uff0c\u4ee5\u964d\u4f4e\u4e0d\u671f\u671b\u7ed3\u679c\u7684\u6982\u7387\u3002\u8fdb\u4e00\u6b65\u6269\u5c55\u65b9\u6cd5\u4ee5\u751f\u6210\u591a\u6837\u5316\u7684\u53cd\u4e8b\u5b9e\u7b56\u7565\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cd\u4e8b\u5b9e\u7b56\u7565\u65b9\u6cd5\u4e3a\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u91ca\u5de5\u5177\uff0c\u6269\u5c55\u4e86\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2505.08977", "pdf": "https://arxiv.org/pdf/2505.08977", "abs": "https://arxiv.org/abs/2505.08977", "authors": ["Hossein Babaei", "Mel White", "Sina Alemohammad", "Richard G. Baraniuk"], "title": "SaFARi: State-Space Models for Frame-Agnostic Representation", "categories": ["cs.LG", "eess.AS", "eess.IV", "eess.SP"], "comment": "13 pages, 5 figures", "summary": "State-Space Models (SSMs) have re-emerged as a powerful tool for online\nfunction approximation, and as the backbone of machine learning models for\nlong-range dependent data. However, to date, only a few polynomial bases have\nbeen explored for this purpose, and the state-of-the-art implementations were\nbuilt upon the best of a few limited options. In this paper, we present a\ngeneralized method for building an SSM with any frame or basis, rather than\nbeing restricted to polynomials. This framework encompasses the approach known\nas HiPPO, but also permits an infinite diversity of other possible \"species\"\nwithin the SSM architecture. We dub this approach SaFARi: SSMs for\nFrame-Agnostic Representation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSaFARi\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u57fa\u4e8e\u4efb\u610f\u6846\u67b6\u6216\u57fa\u51fd\u6570\u7684State-Space Models\uff08SSMs\uff09\uff0c\u7a81\u7834\u4e86\u6b64\u524d\u4ec5\u9650\u4e8e\u591a\u9879\u5f0f\u57fa\u7684\u9650\u5236\uff0c\u6269\u5c55\u4e86SSM\u7684\u5e94\u7528\u591a\u6837\u6027\u3002", "motivation": "\u5f53\u524dSSMs\u5728\u957f\u7a0b\u4f9d\u8d56\u6570\u636e\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5b9e\u73b0\u901a\u5e38\u5c40\u9650\u4e8e\u5c11\u6570\u591a\u9879\u5f0f\u57fa\u51fd\u6570\u3002\u4e3a\u4e86\u7a81\u7834\u8fd9\u4e00\u9650\u5236\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u66f4\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u4efb\u610f\u6846\u67b6\u6216\u57fa\u51fd\u6570\uff0c\u4ee5\u4e30\u5bccSSM\u7684\u6f5c\u529b\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSaFARi\u7684\u6846\u67b6\uff0c\u5141\u8bb8\u4f7f\u7528\u4efb\u610f\u6846\u67b6\u6216\u57fa\u51fd\u6570\u6784\u5efaSSM\uff0c\u4e0d\u518d\u5c40\u9650\u4e8e\u591a\u9879\u5f0f\u3002\u8fd9\u4e00\u65b9\u6cd5\u4e0d\u4ec5\u5305\u542bHiPPO\u7b49\u73b0\u6709\u6280\u672f\uff0c\u8fd8\u652f\u6301\u65e0\u9650\u591a\u6837\u7684SSM\u53d8\u4f53\u3002", "result": "SaFARi\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86SSM\u7684\u6846\u67b6\u65e0\u5173\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u57fa\u51fd\u6570\u9009\u62e9\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u957f\u7a0b\u4f9d\u8d56\u6570\u636e\u4e0a\u7684\u8868\u73b0\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7SaFARi\u6846\u67b6\u663e\u8457\u6269\u5c55\u4e86SSM\u7684\u9002\u7528\u6027\uff0c\u4e3a\u672a\u6765\u5728\u591a\u6837\u573a\u666f\u4e0b\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2505.09518", "pdf": "https://arxiv.org/pdf/2505.09518", "abs": "https://arxiv.org/abs/2505.09518", "authors": ["Maris F. L. Galesloot", "Roman Andriushchenko", "Milan \u010ce\u0161ka", "Sebastian Junges", "Nils Jansen"], "title": "\\textsc{rfPG}: Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted for publication at IJCAI 2025", "summary": "Partially observable Markov decision processes (POMDPs) model specific\nenvironments in sequential decision-making under uncertainty. Critically,\noptimal policies for POMDPs may not be robust against perturbations in the\nenvironment. Hidden-model POMDPs (HM-POMDPs) capture sets of different\nenvironment models, that is, POMDPs with a shared action and observation space.\nThe intuition is that the true model is hidden among a set of potential models,\nand it is unknown which model will be the environment at execution time. A\npolicy is robust for a given HM-POMDP if it achieves sufficient performance for\neach of its POMDPs. We compute such robust policies by combining two orthogonal\ntechniques: (1) a deductive formal verification technique that supports\ntractable robust policy evaluation by computing a worst-case POMDP within the\nHM-POMDP and (2) subgradient ascent to optimize the candidate policy for a\nworst-case POMDP. The empirical evaluation shows that, compared to various\nbaselines, our approach (1) produces policies that are more robust and\ngeneralize better to unseen POMDPs and (2) scales to HM-POMDPs that consist of\nover a hundred thousand environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDPs\uff09\u7684\u9c81\u68d2\u6027\u7b56\u7565\u65b9\u6cd5\uff0c\u7ed3\u5408\u5f62\u5f0f\u5316\u9a8c\u8bc1\u548c\u6b21\u68af\u5ea6\u4e0a\u5347\u6280\u672f\uff0c\u751f\u6210\u5bf9\u672a\u77e5\u73af\u5883\u66f4\u5177\u9002\u5e94\u6027\u7684\u7b56\u7565\u3002", "motivation": "\u4f20\u7edfPOMDPs\u7684\u6700\u4f18\u7b56\u7565\u5728\u73af\u5883\u6270\u52a8\u4e0b\u53ef\u80fd\u4e0d\u591f\u9c81\u68d2\uff0c\u800c\u9690\u85cf\u6a21\u578bPOMDPs\uff08HM-POMDPs\uff09\u901a\u8fc7\u8003\u8651\u591a\u4e2a\u6f5c\u5728\u73af\u5883\u6a21\u578b\u6765\u589e\u5f3a\u7b56\u7565\u7684\u9002\u5e94\u6027\u3002", "method": "\u7ed3\u5408\u4e24\u79cd\u6b63\u4ea4\u6280\u672f\uff1a(1) \u5f62\u5f0f\u5316\u9a8c\u8bc1\u786e\u5b9a\u6700\u574f\u60c5\u51b5POMDP\uff0c(2) \u6b21\u68af\u5ea6\u4e0a\u5347\u4f18\u5316\u7b56\u7565\u4ee5\u5e94\u5bf9\u6700\u574f\u60c5\u51b5\u3002", "result": "\u5b9e\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u7b56\u7565\u66f4\u9c81\u68d2\uff0c\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u7684POMDPs\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u5305\u542b\u8d85\u8fc710\u4e07\u4e2a\u73af\u5883\u7684HM-POMDPs\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u7b56\u7565\u5728\u590d\u6742\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.08982", "pdf": "https://arxiv.org/pdf/2505.08982", "abs": "https://arxiv.org/abs/2505.08982", "authors": ["Jiachen Qian", "Yang Zheng"], "title": "Model-free Online Learning for the Kalman Filter: Forgetting Factor and Logarithmic Regret", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "comment": null, "summary": "We consider the problem of online prediction for an unknown, non-explosive\nlinear stochastic system. With a known system model, the optimal predictor is\nthe celebrated Kalman filter. In the case of unknown systems, existing\napproaches based on recursive least squares and its variants may suffer from\ndegraded performance due to the highly imbalanced nature of the regression\nmodel. This imbalance can easily lead to overfitting and thus degrade\nprediction accuracy. We tackle this problem by injecting an inductive bias into\nthe regression model via {exponential forgetting}. While exponential forgetting\nis a common wisdom in online learning, it is typically used for re-weighting\ndata. In contrast, our approach focuses on balancing the regression model. This\nachieves a better trade-off between {regression} and {regularization errors},\nand simultaneously reduces the {accumulation error}. With new proof techniques,\nwe also provide a sharper logarithmic regret bound of $O(\\log^3 N)$, where $N$\nis the number of observations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6307\u6570\u9057\u5fd8\u7684\u5728\u7ebf\u9884\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u672a\u77e5\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u7684\u9884\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u5e73\u8861\u56de\u5f52\u6a21\u578b\u6539\u5584\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u5bf9\u6570\u9057\u61be\u754c\u3002", "motivation": "\u9488\u5bf9\u672a\u77e5\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u7684\u5728\u7ebf\u9884\u6d4b\u95ee\u9898\uff0c\u4f20\u7edf\u9012\u5f52\u6700\u5c0f\u4e8c\u4e58\u6cd5\u53ca\u5176\u53d8\u79cd\u53ef\u80fd\u56e0\u56de\u5f52\u6a21\u578b\u9ad8\u5ea6\u4e0d\u5e73\u8861\u800c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u8fc7\u62df\u5408\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u6307\u6570\u9057\u5fd8\u7684\u5f52\u7eb3\u504f\u7f6e\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6307\u6570\u9057\u5fd8\u6280\u672f\u6765\u5e73\u8861\u56de\u5f52\u6a21\u578b\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u5730\u91cd\u65b0\u52a0\u6743\u6570\u636e\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u56de\u5f52\u8bef\u5dee\u548c\u6b63\u5219\u5316\u8bef\u5dee\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u7d2f\u8ba1\u8bef\u5dee\u3002", "result": "\u901a\u8fc7\u65b0\u7684\u8bc1\u660e\u6280\u672f\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5bf9\u6570\u9057\u61be\u754c $O(\\log^3 N)$\uff0c\u5176\u4e2d $N$ \u662f\u89c2\u6d4b\u6570\u91cf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5e73\u8861\u56de\u5f52\u6a21\u578b\u548c\u5f15\u5165\u6307\u6570\u9057\u5fd8\uff0c\u6709\u6548\u63d0\u5347\u4e86\u672a\u77e5\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u5728\u7ebf\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2505.09614", "pdf": "https://arxiv.org/pdf/2505.09614", "abs": "https://arxiv.org/abs/2505.09614", "authors": ["Anthony GX-Chen", "Dongyan Lin", "Mandana Samiei", "Doina Precup", "Blake A. Richards", "Rob Fergus", "Kenneth Marino"], "title": "Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Language model (LM) agents are increasingly used as autonomous\ndecision-makers who need to actively gather information to guide their\ndecisions. A crucial cognitive skill for such agents is the efficient\nexploration and understanding of the causal structure of the world -- key to\nrobust, scientifically grounded reasoning. Yet, it remains unclear whether LMs\npossess this capability or exhibit systematic biases leading to erroneous\nconclusions. In this work, we examine LMs' ability to explore and infer causal\nrelationships, using the well-established \"Blicket Test\" paradigm from\ndevelopmental psychology. We find that LMs reliably infer the common, intuitive\ndisjunctive causal relationships but systematically struggle with the unusual,\nyet equally (or sometimes even more) evidenced conjunctive ones. This\n\"disjunctive bias\" persists across model families, sizes, and prompting\nstrategies, and performance further declines as task complexity increases.\nInterestingly, an analogous bias appears in human adults, suggesting that LMs\nmay have inherited deep-seated reasoning heuristics from their training data.\nTo this end, we quantify similarities between LMs and humans, finding that LMs\nexhibit adult-like inference profiles (but not children-like). Finally, we\npropose a test-time sampling method which explicitly samples and eliminates\nhypotheses about causal relationships from the LM. This scalable approach\nsignificantly reduces the disjunctive bias and moves LMs closer to the goal of\nscientific, causally rigorous reasoning.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u63a8\u7406\u4e2d\u5b58\u5728\u201c\u6790\u53d6\u504f\u5dee\u201d\uff0c\u504f\u5411\u5e38\u89c1\u4f46\u5ffd\u7565\u7f55\u89c1\u8bc1\u636e\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u4e14\u4e0e\u4eba\u7c7b\u6210\u5e74\u4eba\u7684\u63a8\u7406\u6a21\u5f0f\u76f8\u4f3c\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u5907\u63a2\u7d22\u548c\u63a8\u65ad\u56e0\u679c\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u63ed\u793a\u5176\u63a8\u7406\u504f\u5dee\u3002", "method": "\u4f7f\u7528\u53d1\u5c55\u5fc3\u7406\u5b66\u4e2d\u7684\u201cBlicket Test\u201d\u8303\u5f0f\uff0c\u6d4b\u8bd5\u6a21\u578b\u5728\u4e0d\u540c\u56e0\u679c\u5173\u7cfb\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5728\u5e38\u89c1\u6790\u53d6\u56e0\u679c\u5173\u7cfb\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u5408\u53d6\u5173\u7cfb\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff1b\u63d0\u51fa\u4e86\u51cf\u5c11\u504f\u5dee\u7684\u91c7\u6837\u65b9\u6cd5\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u7684\u56e0\u679c\u63a8\u7406\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u6210\u5e74\u4eba\u76f8\u4f3c\uff0c\u9700\u6539\u8fdb\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u66f4\u79d1\u5b66\u7684\u63a8\u7406\u3002"}}
{"id": "2505.09003", "pdf": "https://arxiv.org/pdf/2505.09003", "abs": "https://arxiv.org/abs/2505.09003", "authors": ["Zeki Doruk Erden", "Donia Gasmi", "Boi Faltings"], "title": "Continual Reinforcement Learning via Autoencoder-Driven Task and New Environment Recognition", "categories": ["cs.LG", "cs.AI"], "comment": "Published in the Autonomous Robots and Multirobot Systems (ARMS)\n  workshop at AAMAS 2025", "summary": "Continual learning for reinforcement learning agents remains a significant\nchallenge, particularly in preserving and leveraging existing information\nwithout an external signal to indicate changes in tasks or environments. In\nthis study, we explore the effectiveness of autoencoders in detecting new tasks\nand matching observed environments to previously encountered ones. Our approach\nintegrates policy optimization with familiarity autoencoders within an\nend-to-end continual learning system. This system can recognize and learn new\ntasks or environments while preserving knowledge from earlier experiences and\ncan selectively retrieve relevant knowledge when re-encountering a known\nenvironment. Initial results demonstrate successful continual learning without\nexternal signals to indicate task changes or reencounters, showing promise for\nthis methodology.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u6301\u7eed\u5b66\u4e60\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u52a8\u7f16\u7801\u5668\u548c\u7b56\u7565\u4f18\u5316\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u80fd\u8bc6\u522b\u65b0\u4efb\u52a1\u5e76\u65e0\u5916\u90e8\u4fe1\u53f7\u4e0b\u4fdd\u7559\u65e7\u77e5\u8bc6\u3002\u521d\u6b65\u7ed3\u679c\u663e\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u5982\u4f55\u65e0\u9700\u5916\u90e8\u4fe1\u53f7\u5c31\u80fd\u4fdd\u7559\u548c\u5229\u7528\u73b0\u6709\u4fe1\u606f\u7684\u6311\u6218\u3002", "method": "\u65b9\u6cd5\u662f\u5c06\u7b56\u7565\u4f18\u5316\u4e0e\u719f\u6089\u5ea6\u81ea\u52a8\u7f16\u7801\u5668\u7ed3\u5408\uff0c\u6784\u5efa\u7aef\u5230\u7aef\u7684\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\uff0c\u4ee5\u8bc6\u522b\u65b0\u4efb\u52a1\u5e76\u5339\u914d\u5df2\u77e5\u73af\u5883\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u7cfb\u7edf\u80fd\u6210\u529f\u5b9e\u73b0\u65e0\u9700\u5916\u90e8\u4fe1\u53f7\u7684\u6301\u7eed\u5b66\u4e60\uff0c\u5305\u62ec\u65b0\u4efb\u52a1\u8bc6\u522b\u548c\u5df2\u77e5\u73af\u5883\u77e5\u8bc6\u7684\u91cd\u7528\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u65b9\u6cd5\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u65e0\u5916\u90e8\u4fe1\u53f7\u6307\u793a\u4efb\u52a1\u53d8\u5316\u65f6\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2505.08842", "pdf": "https://arxiv.org/pdf/2505.08842", "abs": "https://arxiv.org/abs/2505.08842", "authors": ["Zekun Wu", "Seonglae Cho", "Umar Mohammed", "Cristian Munoz", "Kleyton Costa", "Xin Guan", "Theo King", "Ze Wang", "Emre Kazim", "Adriano Koshiyama"], "title": "LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Open-source AI libraries are foundational to modern AI systems but pose\nsignificant, underexamined risks across security, licensing, maintenance,\nsupply chain integrity, and regulatory compliance. We present LibVulnWatch, a\ngraph-based agentic assessment framework that performs deep, source-grounded\nevaluations of these libraries. Built on LangGraph, the system coordinates a\ndirected acyclic graph of specialized agents to extract, verify, and quantify\nrisk using evidence from trusted sources such as repositories, documentation,\nand vulnerability databases. LibVulnWatch generates reproducible,\ngovernance-aligned scores across five critical domains, publishing them to a\npublic leaderboard for longitudinal ecosystem monitoring. Applied to 20 widely\nused libraries, including ML frameworks, LLM inference engines, and agent\norchestration tools, our system covers up to 88% of OpenSSF Scorecard checks\nwhile uncovering up to 19 additional risks per library. These include critical\nRemote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials\n(SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in\nregulatory documentation and auditability. By translating high-level governance\nprinciples into practical, verifiable metrics, LibVulnWatch advances technical\nAI governance with a scalable, transparent mechanism for continuous supply\nchain risk assessment and informed library selection.", "AI": {"tldr": "LibVulnWatch\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7684\u4ee3\u7406\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5f00\u6e90AI\u5e93\u7684\u591a\u9886\u57df\u98ce\u9669\uff0c\u5305\u62ec\u5b89\u5168\u3001\u8bb8\u53ef\u548c\u7ef4\u62a4\u7b49\uff0c\u5e76\u751f\u6210\u53ef\u91cf\u5316\u7684\u98ce\u9669\u8bc4\u5206\u3002", "motivation": "\u5f00\u6e90AI\u5e93\u5728\u73b0\u4ee3AI\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5728\u5b89\u5168\u3001\u8bb8\u53ef\u3001\u7ef4\u62a4\u7b49\u65b9\u9762\u7684\u98ce\u9669\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u4e9f\u9700\u4e00\u79cd\u7cfb\u7edf\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eLangGraph\u6784\u5efa\uff0c\u901a\u8fc7\u4ee3\u7406\u534f\u4f5c\u4ece\u53ef\u4fe1\u6570\u636e\u6e90\uff08\u5982\u4ee3\u7801\u5e93\u3001\u6587\u6863\uff09\u63d0\u53d6\u5e76\u91cf\u5316\u98ce\u9669\uff0c\u751f\u62105\u4e2a\u5173\u952e\u9886\u57df\u7684\u8bc4\u5206\u3002", "result": "\u5bf920\u4e2a\u5e38\u7528\u5e93\u7684\u8bc4\u4f30\u8986\u76d6\u4e8688%\u7684OpenSSF Scorecard\u68c0\u67e5\uff0c\u53d1\u73b0\u6bcf\u5e93\u6700\u591a19\u9879\u989d\u5916\u98ce\u9669\uff0c\u5305\u62ecRCE\u6f0f\u6d1e\u3001\u8bb8\u53ef\u95ee\u9898\u7b49\u3002", "conclusion": "LibVulnWatch\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u6307\u6807\u63a8\u52a8\u4e86AI\u6280\u672f\u6cbb\u7406\uff0c\u4e3a\u4f9b\u5e94\u94fe\u98ce\u9669\u8bc4\u4f30\u548c\u5e93\u9009\u62e9\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2412.15404", "pdf": "https://arxiv.org/pdf/2412.15404", "abs": "https://arxiv.org/abs/2412.15404", "authors": ["Ahmet Yasin Aytar", "Kemal Kilic", "Kamer Kaya"], "title": "A Retrieval-Augmented Generation Framework for Academic Literature Navigation in Data Science", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "In the rapidly evolving field of data science, efficiently navigating the\nexpansive body of academic literature is crucial for informed decision-making\nand innovation. This paper presents an enhanced Retrieval-Augmented Generation\n(RAG) application, an artificial intelligence (AI)-based system designed to\nassist data scientists in accessing precise and contextually relevant academic\nresources. The AI-powered application integrates advanced techniques, including\nthe GeneRation Of BIbliographic Data (GROBID) technique for extracting\nbibliographic information, fine-tuned embedding models, semantic chunking, and\nan abstract-first retrieval method, to significantly improve the relevance and\naccuracy of the retrieved information. This implementation of AI specifically\naddresses the challenge of academic literature navigation. A comprehensive\nevaluation using the Retrieval-Augmented Generation Assessment System (RAGAS)\nframework demonstrates substantial improvements in key metrics, particularly\nContext Relevance, underscoring the system's effectiveness in reducing\ninformation overload and enhancing decision-making processes. Our findings\nhighlight the potential of this enhanced Retrieval-Augmented Generation system\nto transform academic exploration within data science, ultimately advancing the\nworkflow of research and innovation in the field.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684AI\u7cfb\u7edf\uff0c\u901a\u8fc7GROBID\u6280\u672f\u3001\u5fae\u8c03\u5d4c\u5165\u6a21\u578b\u3001\u8bed\u4e49\u5206\u5757\u548c\u6458\u8981\u4f18\u5148\u68c0\u7d22\u65b9\u6cd5\uff0c\u63d0\u5347\u6570\u636e\u79d1\u5b66\u9886\u57df\u5b66\u672f\u6587\u732e\u68c0\u7d22\u7684\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u7b49\u6307\u6807\u4e0a\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u6570\u636e\u79d1\u5b66\u9886\u57df\u6587\u732e\u8fc5\u901f\u589e\u957f\uff0c\u9ad8\u6548\u68c0\u7d22\u76f8\u5173\u5b66\u672f\u8d44\u6e90\u5bf9\u51b3\u7b56\u548c\u521b\u65b0\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u68c0\u7d22\u7cfb\u7edf\u5728\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528GROBID\u6280\u672f\u63d0\u53d6\u6587\u732e\u5143\u6570\u636e\uff0c\u7ed3\u5408\u5fae\u8c03\u5d4c\u5165\u6a21\u578b\u548c\u8bed\u4e49\u5206\u5757\u4f18\u5316\u68c0\u7d22\uff0c\u5f15\u5165\u6458\u8981\u4f18\u5148\u68c0\u7d22\u65b9\u6cd5\u63d0\u5347\u7ed3\u679c\u8d28\u91cf\uff0c\u5e76\u4f7f\u7528RAGAS\u6846\u67b6\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u5728\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u7b49\u5173\u952e\u6307\u6807\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u6709\u6548\u7f13\u89e3\u4fe1\u606f\u8fc7\u8f7d\u95ee\u9898\uff0c\u5e76\u4f18\u5316\u51b3\u7b56\u6d41\u7a0b\u3002", "conclusion": "\u589e\u5f3a\u7684RAG\u7cfb\u7edf\u6709\u671b\u9769\u65b0\u6570\u636e\u79d1\u5b66\u9886\u57df\u7684\u5b66\u672f\u63a2\u7d22\uff0c\u63a8\u52a8\u7814\u7a76\u548c\u521b\u65b0\u5de5\u4f5c\u6d41\u7684\u8fdb\u6b65\u3002"}}
{"id": "2505.09011", "pdf": "https://arxiv.org/pdf/2505.09011", "abs": "https://arxiv.org/abs/2505.09011", "authors": ["Antonio Candito", "Matthew D Blackledge", "Richard Holbrey", "Nuria Porta", "Ana Ribeiro", "Fabio Zugni", "Luca D'Erme", "Francesca Castagnoli", "Alina Dragan", "Ricardo Donners", "Christina Messiou", "Nina Tunariu", "Dow-Mu Koh"], "title": "Signal-based AI-driven software solution for automated quantification of metastatic bone disease and treatment response assessment using Whole-Body Diffusion-Weighted MRI (WB-DWI) biomarkers in Advanced Prostate Cancer", "categories": ["cs.LG"], "comment": null, "summary": "We developed an AI-driven software solution to quantify metastatic bone\ndisease from WB-DWI scans. Core technologies include: (i) a weakly-supervised\nResidual U-Net model generating a skeleton probability map to isolate bone;\n(ii) a statistical framework for WB-DWI intensity normalisation, obtaining a\nsignal-normalised b=900s/mm^2 (b900) image; and (iii) a shallow convolutional\nneural network that processes outputs from (i) and (ii) to generate a mask of\nsuspected bone lesions, characterised by higher b900 signal intensity due to\nrestricted water diffusion. This mask is applied to the gADC map to extract TDV\nand gADC statistics. We tested the tool using expert-defined metastatic bone\ndisease delineations on 66 datasets, assessed repeatability of imaging\nbiomarkers (N=10), and compared software-based response assessment with a\nconstruct reference standard based on clinical, laboratory and imaging\nassessments (N=118). Dice score between manual and automated delineations was\n0.6 for lesions within pelvis and spine, with an average surface distance of\n2mm. Relative differences for log-transformed TDV (log-TDV) and median gADC\nwere below 9% and 5%, respectively. Repeatability analysis showed coefficients\nof variation of 4.57% for log-TDV and 3.54% for median gADC, with intraclass\ncorrelation coefficients above 0.9. The software achieved 80.5% accuracy, 84.3%\nsensitivity, and 85.7% specificity in assessing response to treatment compared\nto the construct reference standard. Computation time generating a mask\naveraged 90 seconds per scan. Our software enables reproducible TDV and gADC\nquantification from WB-DWI scans for monitoring metastatic bone disease\nresponse, thus providing potentially useful measurements for clinical\ndecision-making in APC patients.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cdAI\u9a71\u52a8\u7684\u8f6f\u4ef6\uff0c\u901a\u8fc7WB-DWI\u626b\u63cf\u91cf\u5316\u8f6c\u79fb\u6027\u9aa8\u75c5\uff0c\u7ed3\u5408\u5f31\u76d1\u7763Residual U-Net\u6a21\u578b\u548c\u6d45\u5c42CNN\uff0c\u5b9e\u73b0\u4e86\u9ad8\u91cd\u590d\u6027\u548c\u51c6\u786e\u6027\u7684\u75c5\u7076\u91cf\u5316\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u8f6c\u79fb\u6027\u9aa8\u75c5\u7684\u7cbe\u51c6\u91cf\u5316\u5bf9\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u53ef\u91cd\u590d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u5f31\u76d1\u7763Residual U-Net\u6a21\u578b\u751f\u6210\u9aa8\u9abc\u6982\u7387\u56fe\uff0c\u7edf\u8ba1\u6846\u67b6\u6807\u51c6\u5316WB-DWI\u4fe1\u53f7\uff0c\u6d45\u5c42CNN\u751f\u6210\u75c5\u7076\u63a9\u819c\uff0c\u5e76\u63d0\u53d6TDV\u548cgADC\u7edf\u8ba1\u6570\u636e\u3002", "result": "\u8f6f\u4ef6\u4e0e\u4e13\u5bb6\u6807\u6ce8\u7684Dice\u5206\u6570\u4e3a0.6\uff0c\u91cd\u590d\u6027\u5206\u6790\u663e\u793alog-TDV\u548cgADC\u7684\u53d8\u5f02\u7cfb\u6570\u4f4e\u4e8e5%\uff0c\u54cd\u5e94\u8bc4\u4f30\u7684\u51c6\u786e\u7387\u8fbe80.5%\u3002", "conclusion": "\u8be5\u8f6f\u4ef6\u80fd\u591f\u9ad8\u6548\u3001\u51c6\u786e\u5730\u91cf\u5316\u8f6c\u79fb\u6027\u9aa8\u75c5\uff0c\u4e3a\u4e34\u5e8a\u76d1\u6d4b\u548c\u6cbb\u7597\u51b3\u7b56\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2505.08902", "pdf": "https://arxiv.org/pdf/2505.08902", "abs": "https://arxiv.org/abs/2505.08902", "authors": ["Lucas McCullum", "Pelagie Ami Agassi", "Leo Anthony Celi", "Daniel K. Ebner", "Chrystinne Oliveira Fernandes", "Rachel S. Hicklen", "Mkliwa Koumbia", "Lisa Soleymani Lehmann", "David Restrepo"], "title": "Performance Gains of LLMs With Humans in a World of LLMs Versus Humans", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Currently, a considerable research effort is devoted to comparing LLMs to a\ngroup of human experts, where the term \"expert\" is often ill-defined or\nvariable, at best, in a state of constantly updating LLM releases. Without\nproper safeguards in place, LLMs will threaten to cause harm to the established\nstructure of safe delivery of patient care which has been carefully developed\nthroughout history to keep the safety of the patient at the forefront. A key\ndriver of LLM innovation is founded on community research efforts which, if\ncontinuing to operate under \"humans versus LLMs\" principles, will expedite this\ntrend. Therefore, research efforts moving forward must focus on effectively\ncharacterizing the safe use of LLMs in clinical settings that persist across\nthe rapid development of novel LLM models. In this communication, we\ndemonstrate that rather than comparing LLMs to humans, there is a need to\ndevelop strategies enabling efficient work of humans with LLMs in an almost\nsymbiotic manner.", "AI": {"tldr": "\u8bba\u6587\u547c\u5401\u907f\u514d\u76f4\u63a5\u6bd4\u8f83LLMs\u4e0e\u4eba\u7c7b\u4e13\u5bb6\uff0c\u8f6c\u800c\u7814\u7a76\u5982\u4f55\u5b89\u5168\u3001\u9ad8\u6548\u5730\u4fc3\u8fdbLLMs\u4e0e\u4eba\u7c7b\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u3002", "motivation": "\u5f53\u524d\u5173\u4e8eLLMs\u7684\u7814\u7a76\u8fc7\u4e8e\u5173\u6ce8\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u5bf9\u6bd4\uff0c\u53ef\u80fd\u5bfc\u81f4\u5ffd\u89c6\u5176\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u6f5c\u5728\u7684\u5b89\u5168\u98ce\u9669\uff0c\u9700\u91cd\u65b0\u805a\u7126\u4e8e\u4eba\u7c7b\u4e0eLLM\u7684\u534f\u4f5c\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u9700\u5236\u5b9a\u7b56\u7565\uff0c\u786e\u4fddLLMs\u5728\u5feb\u901f\u8fed\u4ee3\u4e2d\u4ecd\u80fd\u5b89\u5168\u5730\u8f85\u52a9\u4e34\u5e8a\u5de5\u4f5c\uff0c\u800c\u975e\u7b80\u5355\u66ff\u4ee3\u4eba\u7c7b\u3002", "result": "\u5f3a\u8c03\u5e94\u8f6c\u5411\u201c\u4eba\u673a\u534f\u540c\u201d\u6a21\u5f0f\uff0c\u800c\u975e\u5bf9\u7acb\u6bd4\u8f83\uff0c\u4ee5\u4fdd\u969c\u60a3\u8005\u5b89\u5168\u548c\u533b\u7597\u8d28\u91cf\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u4f18\u5148\u5f00\u53d1LLMs\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u534f\u540c\u6846\u67b6\uff0c\u9002\u5e94\u5176\u5feb\u901f\u53d1\u5c55\u7684\u7279\u6027\u3002"}}
{"id": "2505.09017", "pdf": "https://arxiv.org/pdf/2505.09017", "abs": "https://arxiv.org/abs/2505.09017", "authors": ["Bizhan Alipour Pijan", "Serdar Bozdag"], "title": "DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update", "categories": ["cs.LG", "cs.SI"], "comment": null, "summary": "Most of the dynamic graph representation learning methods involve dividing a\ndynamic graph into discrete snapshots to capture the evolving behavior of nodes\nover time. Existing methods primarily capture only local or global structures\nof each node within a snapshot using message-passing and random walk-based\nmethods. Then, they utilize sequence-based models (e.g., transformers) to\nencode the temporal evolution of node embeddings, and meta-learning techniques\nto update the model parameters. However, these approaches have two limitations.\nFirst, they neglect the extraction of global and local information\nsimultaneously in each snapshot. Second, they fail to consider the model's\nperformance in the current snapshot during parameter updates, resulting in a\nlack of temporal dependency management. Recently, HiPPO (High-order Polynomial\nProjection Operators) algorithm has gained attention for their ability to\noptimize and preserve sequence history in State Space Model (SSM). To address\nthe aforementioned limitations in dynamic graph representation learning, we\npropose a novel method called Multi-view Dynamic Graph Embeddings with State\nSpace Model Gradient Update (DyGSSM). Our approach combines Graph Convolution\nNetworks (GCN) for local feature extraction and random walk with Gated\nRecurrent Unit (GRU) for global feature extraction in each snapshot. We then\nintegrate the local and global features using a cross-attention mechanism.\nAdditionally, we incorporate an SSM based on HiPPO algorithm to account for\nlong-term dependencies when updating model parameters, ensuring that model\nperformance in each snapshot informs subsequent updates. Experiments on five\npublic datasets show that our method outperforms existing baseline and\nstate-of-the-art (SOTA) methods in 17 out of 20 cases.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86DyGSSM\u65b9\u6cd5\uff0c\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u901a\u8fc7\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4f18\u5316\u53c2\u6570\u66f4\u65b0\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u56fe\u8868\u793a\u5b66\u4e60\u4e2d\u5c40\u90e8\u4e0e\u5168\u5c40\u4fe1\u606f\u540c\u65f6\u63d0\u53d6\u53ca\u65f6\u95f4\u4f9d\u8d56\u7ba1\u7406\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u56fe\u8868\u793a\u5b66\u4e60\u4e2d\u65e0\u6cd5\u540c\u65f6\u63d0\u53d6\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\uff0c\u4e14\u53c2\u6570\u66f4\u65b0\u65f6\u672a\u8003\u8651\u5f53\u524d\u5feb\u7167\u7684\u6027\u80fd\uff0c\u5bfc\u81f4\u65f6\u95f4\u4f9d\u8d56\u7ba1\u7406\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDyGSSM\u65b9\u6cd5\uff0c\u7ed3\u5408GCN\uff08\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\uff09\u3001\u968f\u673a\u6e38\u8d70\u4e0eGRU\uff08\u5168\u5c40\u7279\u5f81\u63d0\u53d6\uff09\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u7279\u5f81\uff1b\u4f7f\u7528HiPPO\u7b97\u6cd5\u7684SSM\u6a21\u578b\u7ba1\u7406\u957f\u671f\u4f9d\u8d56\u3002", "result": "\u57285\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cDyGSSM\u572820\u4e2a\u6848\u4f8b\u4e2d\u768417\u4e2a\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u53caSOTA\u65b9\u6cd5\u3002", "conclusion": "DyGSSM\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u56fe\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u5c40\u90e8-\u5168\u5c40\u4fe1\u606f\u6574\u5408\u4e0e\u65f6\u95f4\u4f9d\u8d56\u7ba1\u7406\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.08798", "pdf": "https://arxiv.org/pdf/2505.08798", "abs": "https://arxiv.org/abs/2505.08798", "authors": ["Mobina Shrestha", "Bishwas Mandal", "Vishal Mandal", "Asis Shrestha"], "title": "In-Context Learning for Label-Efficient Cancer Image Classification in Oncology", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "The application of AI in oncology has been limited by its reliance on large,\nannotated datasets and the need for retraining models for domain-specific\ndiagnostic tasks. Taking heed of these limitations, we investigated in-context\nlearning as a pragmatic alternative to model retraining by allowing models to\nadapt to new diagnostic tasks using only a few labeled examples at inference,\nwithout the need for retraining. Using four vision-language models\n(VLMs)-Paligemma, CLIP, ALIGN and GPT-4o, we evaluated the performance across\nthree oncology datasets: MHIST, PatchCamelyon and HAM10000. To the best of our\nknowledge, this is the first study to compare the performance of multiple VLMs\non different oncology classification tasks. Without any parameter updates, all\nmodels showed significant gains with few-shot prompting, with GPT-4o reaching\nan F1 score of 0.81 in binary classification and 0.60 in multi-class\nclassification settings. While these results remain below the ceiling of fully\nfine-tuned systems, they highlight the potential of ICL to approximate\ntask-specific behavior using only a handful of examples, reflecting how\nclinicians often reason from prior cases. Notably, open-source models like\nPaligemma and CLIP demonstrated competitive gains despite their smaller size,\nsuggesting feasibility for deployment in computing constrained clinical\nenvironments. Overall, these findings highlight the potential of ICL as a\npractical solution in oncology, particularly for rare cancers and\nresource-limited contexts where fine-tuning is infeasible and annotated data is\ndifficult to obtain.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u5728\u80bf\u7624\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u8ba9\u6a21\u578b\u9002\u5e94\u65b0\u8bca\u65ad\u4efb\u52a1\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002\u6d4b\u8bd5\u4e86\u56db\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\uff0c\u7ed3\u679c\u663e\u793aGPT-4o\u5728\u5c11\u6837\u672c\u63d0\u793a\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u5f00\u6e90\u6a21\u578b\u4e5f\u5c55\u793a\u4e86\u7ade\u4e89\u529b\u3002", "motivation": "\u89e3\u51b3AI\u5728\u80bf\u7624\u5b66\u4e2d\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22ICL\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\u4ee5\u9002\u5e94\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u3002", "method": "\u4f7f\u7528\u56db\u79cdVLMs\uff08Paligemma\u3001CLIP\u3001ALIGN\u3001GPT-4o\uff09\uff0c\u5728\u4e09\u4e2a\u80bf\u7624\u6570\u636e\u96c6\uff08MHIST\u3001PatchCamelyon\u3001HAM10000\uff09\u4e0a\u8bc4\u4f30\u5c11\u6837\u672c\u63d0\u793a\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u5c11\u6837\u672c\u63d0\u793a\u4e0b\u5747\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0cGPT-4o\u5728\u4e8c\u5206\u7c7b\u548c\u591a\u5206\u7c7b\u4e2d\u7684F1\u5f97\u5206\u5206\u522b\u4e3a0.81\u548c0.60\u3002\u5f00\u6e90\u6a21\u578b\u867d\u89c4\u6a21\u8f83\u5c0f\u4f46\u8868\u73b0\u7ade\u4e89\u6027\u3002", "conclusion": "ICL\u5c55\u793a\u4e86\u5728\u80bf\u7624\u5b66\u4e2d\u7684\u5b9e\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7f55\u89c1\u764c\u75c7\u548c\u8d44\u6e90\u53d7\u9650\u573a\u666f\uff0c\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6216\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002"}}
{"id": "2505.09022", "pdf": "https://arxiv.org/pdf/2505.09022", "abs": "https://arxiv.org/abs/2505.09022", "authors": ["Annan Yu", "N. Benjamin Erichson"], "title": "Block-Biased Mamba for Long-Range Sequence Processing", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Mamba extends earlier state space models (SSMs) by introducing\ninput-dependent dynamics, and has demonstrated strong empirical performance\nacross a range of domains, including language modeling, computer vision, and\nfoundation models. However, a surprising weakness remains: despite being built\non architectures designed for long-range dependencies, Mamba performs poorly on\nlong-range sequential tasks. Understanding and addressing this gap is important\nfor improving Mamba's universality and versatility. In this work, we analyze\nMamba's limitations through three perspectives: expressiveness, inductive bias,\nand training stability. Our theoretical results show how Mamba falls short in\neach of these aspects compared to earlier SSMs such as S4D. To address these\nissues, we propose $\\text{B}_2\\text{S}_6$, a simple extension of Mamba's S6\nunit that combines block-wise selective dynamics with a channel-specific bias.\nWe prove that these changes equip the model with a better-suited inductive bias\nand improve its expressiveness and stability. Empirically,\n$\\text{B}_2\\text{S}_6$ outperforms S4 and S4D on Long-Range Arena (LRA) tasks\nwhile maintaining Mamba's performance on language modeling benchmarks.", "AI": {"tldr": "Mamba\u867d\u7136\u901a\u8fc7\u8f93\u5165\u4f9d\u8d56\u7684\u52a8\u6001\u6027\u6269\u5c55\u4e86SSMs\u5e76\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u7814\u7a76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63d0\u51fa\u6539\u8fdb\u65b9\u6848B\u2082S\u2086\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "Mamba\u867d\u7136\u5728\u8bed\u8a00\u5efa\u6a21\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e0a\u7684\u5f31\u70b9\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u3002\u7814\u7a76\u65e8\u5728\u5206\u6790\u5e76\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790Mamba\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51faB\u2082S\u2086\u6539\u8fdb\u65b9\u6848\uff0c\u7ed3\u5408\u5757\u9009\u62e9\u52a8\u6001\u6027\u548c\u901a\u9053\u504f\u7f6e\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u8868\u8fbe\u529b\u548c\u7a33\u5b9a\u6027\u3002", "result": "B\u2082S\u2086\u5728\u957f\u5e8f\u5217\u4efb\u52a1\uff08LRA\uff09\u4e0a\u8d85\u8d8aS4\u548cS4D\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "B\u2082S\u2086\u6709\u6548\u5f25\u8865\u4e86Mamba\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u4e86\u5176\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u6539\u8fdb\u65b9\u6848\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.08910", "pdf": "https://arxiv.org/pdf/2505.08910", "abs": "https://arxiv.org/abs/2505.08910", "authors": ["Nahid Alam", "Karthik Reddy Kanjula", "Surya Guthikonda", "Timothy Chung", "Bala Krishna S Vegesna", "Abhipsha Das", "Anthony Susevski", "Ryan Sze-Yin Chan", "S M Iftekhar Uddin", "Shayekh Bin Islam", "Roshan Santhosh", "Snegha A", "Drishti Sharma", "Chen Liu", "Isha Chaturvedi", "Genta Indra Winata", "Ashvanth. S", "Snehanshu Mukherjee", "Alham Fikri Aji"], "title": "Behind Maya: Building a Multilingual Vision Language Model", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted at VLM4ALL CVPR 2025 Workshop", "summary": "In recent times, we have seen a rapid development of large Vision-Language\nModels (VLMs). They have shown impressive results on academic benchmarks,\nprimarily in widely spoken languages but lack performance on low-resource\nlanguages and varied cultural contexts. To address these limitations, we\nintroduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a\nmultilingual image-text pretraining dataset in eight languages, based on the\nLLaVA pretraining dataset; and 2) a multilingual image-text model supporting\nthese languages, enhancing cultural and linguistic comprehension in\nvision-language tasks. Code available at https://github.com/nahidalam/maya.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMaya\u7684\u5f00\u6e90\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524dVLM\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u591a\u6837\u5316\u6587\u5316\u80cc\u666f\u4e0b\u7684\u6027\u80fd\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u8bed\u8a00\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u6587\u5316\u591a\u6837\u6027\u80cc\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6784\u5efa\u591a\u8bed\u8a00\u7684\u56fe\u50cf-\u6587\u672c\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u6a21\u578b\u6765\u5f25\u8865\u8fd9\u4e00\u7f3a\u9677\u3002", "method": "\u4f5c\u8005\u57fa\u4e8eLLaVA\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u516b\u79cd\u8bed\u8a00\u7684\u591a\u8bed\u8a00\u56fe\u50cf-\u6587\u672c\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u652f\u6301\u8fd9\u4e9b\u8bed\u8a00\u7684\u591a\u8bed\u8a00\u56fe\u50cf-\u6587\u672c\u6a21\u578b\uff0c\u4ee5\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u6587\u5316\u548c\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002", "result": "Maya\u6a21\u578b\u80fd\u591f\u652f\u6301\u516b\u79cd\u8bed\u8a00\uff0c\u63d0\u5347\u4e86\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u6587\u5316\u591a\u6837\u6027\u80cc\u666f\u4e0b\u7684\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u5f00\u6e90Maya\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u4f5c\u8005\u4e3a\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u591a\u8bed\u8a00\u548c\u8de8\u6587\u5316\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.08800", "pdf": "https://arxiv.org/pdf/2505.08800", "abs": "https://arxiv.org/abs/2505.08800", "authors": ["Olivia Nocentini", "Marta Lagomarsino", "Gokhan Solak", "Younggeol Cho", "Qiyi Tong", "Marta Lorenzini", "Arash Ajoudani"], "title": "Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Driver fatigue poses a significant challenge to railway safety, with\ntraditional systems like the dead-man switch offering limited and basic\nalertness checks. This study presents an online behavior-based monitoring\nsystem utilizing a customised Directed-Graph Neural Network (DGNN) to classify\ntrain driver's states into three categories: alert, not alert, and\npathological. To optimize input representations for the model, an ablation\nstudy was performed, comparing three feature configurations: skeletal-only,\nfacial-only, and a combination of both. Experimental results show that\ncombining facial and skeletal features yields the highest accuracy (80.88%) in\nthe three-class model, outperforming models using only facial or skeletal\nfeatures. Furthermore, this combination achieves over 99% accuracy in the\nbinary alertness classification. Additionally, we introduced a novel dataset\nthat, for the first time, incorporates simulated pathological conditions into\ntrain driver monitoring, broadening the scope for assessing risks related to\nfatigue and health. This work represents a step forward in enhancing railway\nsafety through advanced online monitoring using vision-based technologies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u884c\u4e3a\u5206\u6790\u7684\u5728\u7ebf\u76d1\u6d4b\u7cfb\u7edf\uff0c\u4f7f\u7528\u5b9a\u5411\u56fe\u795e\u7ecf\u7f51\u7edc\uff08DGNN\uff09\u5206\u7c7b\u706b\u8f66\u53f8\u673a\u72b6\u6001\uff08\u8b66\u89c9\u3001\u975e\u8b66\u89c9\u3001\u75c5\u7406\u72b6\u6001\uff09\u3002\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u53d1\u73b0\uff0c\u7ed3\u5408\u9762\u90e8\u548c\u9aa8\u9abc\u7279\u5f81\u7684\u6a21\u578b\u5728\u4e09\u5206\u7c7b\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u6700\u9ad8\uff0880.88%\uff09\uff0c\u4e8c\u5206\u7c7b\u8b66\u89c9\u6027\u4efb\u52a1\u51c6\u786e\u7387\u8d8599%\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b\u6a21\u62df\u75c5\u7406\u72b6\u6001\u7684\u65b0\u6570\u636e\u96c6\u3002", "motivation": "\u4f20\u7edf\u6b7b\u673a\u5f00\u5173\u7b49\u7cfb\u7edf\u5bf9\u706b\u8f66\u53f8\u673a\u8b66\u89c9\u6027\u76d1\u6d4b\u6709\u9650\uff0c\u9700\u66f4\u5148\u8fdb\u7684\u5728\u7ebf\u76d1\u6d4b\u6280\u672f\u63d0\u5347\u94c1\u8def\u5b89\u5168\u3002", "method": "\u91c7\u7528\u5b9a\u5411\u56fe\u795e\u7ecf\u7f51\u7edc\uff08DGNN\uff09\uff0c\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u5bf9\u6bd4\u9aa8\u9abc\u3001\u9762\u90e8\u53ca\u4e24\u8005\u7ed3\u5408\u7684\u7279\u5f81\u914d\u7f6e\uff0c\u4f18\u5316\u8f93\u5165\u8868\u793a\u3002", "result": "\u7ed3\u5408\u9762\u90e8\u548c\u9aa8\u9abc\u7279\u5f81\u7684\u6a21\u578b\u5728\u4e09\u5206\u7c7b\u4e2d\u51c6\u786e\u738780.88%\uff0c\u4e8c\u5206\u7c7b\u8d8599%\uff0c\u5e76\u521b\u5efa\u4e86\u5305\u542b\u75c5\u7406\u72b6\u6001\u7684\u65b0\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u89c6\u89c9\u6280\u672f\u63d0\u5347\u5728\u7ebf\u76d1\u6d4b\u80fd\u529b\uff0c\u4e3a\u94c1\u8def\u5b89\u5168\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u75b2\u52b3\u548c\u5065\u5eb7\u98ce\u9669\u8bc4\u4f30\u65b9\u6848\u3002"}}
{"id": "2505.09063", "pdf": "https://arxiv.org/pdf/2505.09063", "abs": "https://arxiv.org/abs/2505.09063", "authors": ["Khalid Rafiq", "Wenjing Liao", "Aditya G. Nair"], "title": "Single-shot prediction of parametric partial differential equations", "categories": ["cs.LG", "cs.NA", "math.NA", "68T07"], "comment": "35 pages, 17 figures", "summary": "We introduce Flexi-VAE, a data-driven framework for efficient single-shot\nforecasting of nonlinear parametric partial differential equations (PDEs),\neliminating the need for iterative time-stepping while maintaining high\naccuracy and stability. Flexi-VAE incorporates a neural propagator that\nadvances latent representations forward in time, aligning latent evolution with\nphysical state reconstruction in a variational autoencoder setting. We evaluate\ntwo propagation strategies, the Direct Concatenation Propagator (DCP) and the\nPositional Encoding Propagator (PEP), and demonstrate, through\nrepresentation-theoretic analysis, that DCP offers superior long-term\ngeneralization by fostering disentangled and physically meaningful latent\nspaces. Geometric diagnostics, including Jacobian spectral analysis, reveal\nthat propagated latent states reside in regions of lower decoder sensitivity\nand more stable local geometry than those derived via direct encoding,\nenhancing robustness for long-horizon predictions. We validate Flexi-VAE on\ncanonical PDE benchmarks, the 1D viscous Burgers equation and the 2D\nadvection-diffusion equation, achieving accurate forecasts across wide\nparametric ranges. The model delivers over 50x CPU and 90x GPU speedups\ncompared to autoencoder-LSTM baselines for large temporal shifts. These results\nposition Flexi-VAE as a scalable and interpretable surrogate modeling tool for\naccelerating high-fidelity simulations in computational fluid dynamics (CFD)\nand other parametric PDE-driven applications, with extensibility to\nhigher-dimensional and more complex systems.", "AI": {"tldr": "Flexi-VAE \u662f\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u975e\u7ebf\u6027\u53c2\u6570\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u9ad8\u6548\u5355\u6b21\u9884\u6d4b\uff0c\u907f\u514d\u4e86\u8fed\u4ee3\u65f6\u95f4\u6b65\u8fdb\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u9884\u6d4b\u975e\u7ebf\u6027\u53c2\u6570\u504f\u5fae\u5206\u65b9\u7a0b\u65f6\u9700\u8981\u8fed\u4ee3\u65f6\u95f4\u6b65\u8fdb\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u9884\u6d4b\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u795e\u7ecf\u4f20\u64ad\u5668\uff08DCP\u548cPEP\uff09\u63a8\u8fdb\u6f5c\u5728\u8868\u793a\u7684\u65f6\u95f4\u6f14\u5316\uff0c\u5e76\u901a\u8fc7\u51e0\u4f55\u8bca\u65ad\u5206\u6790\u63d0\u5347\u957f\u65f6\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u57281D\u9ecf\u6027Burgers\u65b9\u7a0b\u548c2D\u5bf9\u6d41-\u6269\u6563\u65b9\u7a0b\u4e0a\u9a8c\u8bc1\u4e86\u9ad8\u7cbe\u5ea6\u9884\u6d4b\uff0cCPU\u548cGPU\u901f\u5ea6\u5206\u522b\u63d0\u534750\u500d\u548c90\u500d\u3002", "conclusion": "Flexi-VAE\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u5efa\u6a21\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u6d41\u4f53\u529b\u5b66\u7b49\u53c2\u6570PDE\u9a71\u52a8\u7684\u5e94\u7528\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u66f4\u9ad8\u7ef4\u548c\u590d\u6742\u7cfb\u7edf\u3002"}}
{"id": "2505.09076", "pdf": "https://arxiv.org/pdf/2505.09076", "abs": "https://arxiv.org/abs/2505.09076", "authors": ["Berkay Guler", "Hamid Jafarkhani"], "title": "AdaFortiTran: An Adaptive Transformer Model for Robust OFDM Channel Estimation", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Deep learning models for channel estimation in Orthogonal Frequency Division\nMultiplexing (OFDM) systems often suffer from performance degradation under\nfast-fading channels and low-SNR scenarios. To address these limitations, we\nintroduce the Adaptive Fortified Transformer (AdaFortiTran), a novel model\nspecifically designed to enhance channel estimation in challenging\nenvironments. Our approach employs convolutional layers that exploit locality\nbias to capture strong correlations between neighboring channel elements,\ncombined with a transformer encoder that applies the global Attention mechanism\nto channel patches. This approach effectively models both long-range\ndependencies and spectro-temporal interactions within single OFDM frames. We\nfurther augment the model's adaptability by integrating nonlinear\nrepresentations of available channel statistics SNR, delay spread, and Doppler\nshift as priors. A residual connection is employed to merge global features\nfrom the transformer with local features from early convolutional processing,\nfollowed by final convolutional layers to refine the hierarchical channel\nrepresentation. Despite its compact architecture, AdaFortiTran achieves up to 6\ndB reduction in mean squared error (MSE) compared to state-of-the-art models.\nTested across a wide range of Doppler shifts (200-1000 Hz), SNRs (0 to 25 dB),\nand delay spreads (50-300 ns), it demonstrates superior robustness in\nhigh-mobility environments.", "AI": {"tldr": "\u63d0\u51faAdaptive Fortified Transformer (AdaFortiTran)\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u5347OFDM\u7cfb\u7edf\u5728\u5feb\u901f\u8870\u843d\u548c\u4f4e\u4fe1\u566a\u6bd4\u573a\u666f\u4e0b\u7684\u4fe1\u9053\u4f30\u8ba1\u6027\u80fd\uff0c\u901a\u8fc7\u5377\u79ef\u5c42\u4e0eTransformer\u7f16\u7801\u5668\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u79cd\u6076\u52a3\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728OFDM\u7cfb\u7edf\u5feb\u901f\u8870\u843d\u4fe1\u9053\u548c\u4f4e\u4fe1\u566a\u6bd4\u573a\u666f\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5377\u79ef\u5c42\uff08\u6355\u83b7\u5c40\u90e8\u76f8\u5173\u6027\uff09\u4e0eTransformer\u7f16\u7801\u5668\uff08\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\uff09\uff0c\u5e76\u96c6\u6210\u4fe1\u9053\u7edf\u8ba1\u4fe1\u606f\uff08\u5982SNR\u3001\u65f6\u5ef6\u6269\u5c55\u548c\u591a\u666e\u52d2\u9891\u79fb\uff09\u4f5c\u4e3a\u5148\u9a8c\u3002", "result": "\u5728\u591a\u79cd\u6d4b\u8bd5\u6761\u4ef6\u4e0b\uff08\u591a\u666e\u52d2\u9891\u79fb200-1000 Hz\u3001SNR 0-25 dB\u3001\u65f6\u5ef6\u6269\u5c5550-300 ns\uff09\uff0c\u6bd4\u73b0\u6709\u6a21\u578b\u964d\u4f4e\u6700\u591a6 dB\u7684\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u3002", "conclusion": "AdaFortiTran\u5728\u7d27\u51d1\u67b6\u6784\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u9053\u4f30\u8ba1\u6027\u80fd\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9ad8\u79fb\u52a8\u6027\u73af\u5883\u3002"}}
{"id": "2505.08971", "pdf": "https://arxiv.org/pdf/2505.08971", "abs": "https://arxiv.org/abs/2505.08971", "authors": ["Yangyi Chen", "Hao Peng", "Tong Zhang", "Heng Ji"], "title": "Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "The code will be available at https://github.com/Yangyi-Chen/PRIOR", "summary": "In standard large vision-language models (LVLMs) pre-training, the model\ntypically maximizes the joint probability of the caption conditioned on the\nimage via next-token prediction (NTP); however, since only a small subset of\ncaption tokens directly relates to the visual content, this naive NTP\nunintentionally fits the model to noise and increases the risk of\nhallucination. We present PRIOR, a simple vision-language pre-training approach\nthat addresses this issue by prioritizing image-related tokens through\ndifferential weighting in the NTP loss, drawing from the importance sampling\nframework. PRIOR introduces a reference model-a text-only large language model\n(LLM) trained on the captions without image inputs, to weight each token based\non its probability for LVLMs training. Intuitively, tokens that are directly\nrelated to the visual inputs are harder to predict without the image and thus\nreceive lower probabilities from the text-only reference LLM. During training,\nwe implement a token-specific re-weighting term based on the importance scores\nto adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs\nwith visual encoders and LVLMs without visual encoders. We observe 19% and 8%\naverage relative improvement, respectively, on several vision-language\nbenchmarks compared to NTP. In addition, PRIOR exhibits superior scaling\nproperties, as demonstrated by significantly higher scaling coefficients,\nindicating greater potential for performance gains compared to NTP given\nincreasing compute and data.", "AI": {"tldr": "PRIOR\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5dee\u5f02\u52a0\u6743\u4f18\u5148\u5904\u7406\u4e0e\u56fe\u50cf\u76f8\u5173\u7684token\uff0c\u51cf\u5c11\u4e86\u566a\u58f0\u62df\u5408\u548c\u5e7b\u89c9\u98ce\u9669\u3002", "motivation": "\u6807\u51c6LVLMs\u9884\u8bad\u7ec3\u4e2d\uff0cNaive NTP\u4f1a\u65e0\u610f\u4e2d\u62df\u5408\u566a\u58f0\u5e76\u589e\u52a0\u5e7b\u89c9\u98ce\u9669\uff0cPRIOR\u65e8\u5728\u901a\u8fc7\u52a0\u6743\u4f18\u5316\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "PRIOR\u5229\u7528\u6587\u672c\u53c2\u8003LLM\u5bf9token\u8fdb\u884c\u91cd\u8981\u6027\u6253\u5206\uff0c\u5e76\u5728NTP\u635f\u5931\u4e2d\u8c03\u6574\u6743\u91cd\uff0c\u4f18\u5148\u8bad\u7ec3\u4e0e\u89c6\u89c9\u76f8\u5173\u7684token\u3002", "result": "\u5728\u4e24\u79cdLVLMs\u8bbe\u7f6e\u4e2d\uff0cPRIOR\u5206\u522b\u5b9e\u73b0\u4e8619%\u548c8%\u7684\u5e73\u5747\u76f8\u5bf9\u63d0\u5347\uff0c\u5e76\u5c55\u793a\u4e86\u66f4\u597d\u7684\u6269\u5c55\u6027\u3002", "conclusion": "PRIOR\u901a\u8fc7\u5dee\u5f02\u52a0\u6743\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e14\u5177\u6709\u66f4\u5f3a\u7684\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2505.08807", "pdf": "https://arxiv.org/pdf/2505.08807", "abs": "https://arxiv.org/abs/2505.08807", "authors": ["Yuntao Wang", "Yanghe Pan", "Shaolong Guo", "Zhou Su"], "title": "Security of Internet of Agents: Attacks and Countermeasures", "categories": ["cs.CR", "cs.AI"], "comment": "11 pages, 5 figures, 3 tables, submitted to IEEE OJCS", "summary": "With the rise of large language and vision-language models, AI agents have\nevolved into autonomous, interactive systems capable of perception, reasoning,\nand decision-making. As they proliferate across virtual and physical domains,\nthe Internet of Agents (IoA) has emerged as a key infrastructure for enabling\nscalable and secure coordination among heterogeneous agents. This survey offers\na comprehensive examination of the security and privacy landscape in IoA\nsystems. We begin by outlining the IoA architecture and its distinct\nvulnerabilities compared to traditional networks, focusing on four critical\naspects: identity authentication threats, cross-agent trust issues, embodied\nsecurity, and privacy risks. We then review existing and emerging defense\nmechanisms and highlight persistent challenges. Finally, we identify open\nresearch directions to advance the development of resilient and\nprivacy-preserving IoA ecosystems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7efc\u8ff0\u4e86\u7269\u8054\u7f51\u4ee3\u7406\uff08IoA\uff09\u4e2d\u7684\u5b89\u5168\u4e0e\u9690\u79c1\u95ee\u9898\uff0c\u5305\u62ec\u8eab\u4efd\u8ba4\u8bc1\u5a01\u80c1\u3001\u8de8\u4ee3\u7406\u4fe1\u4efb\u95ee\u9898\u3001\u5b9e\u4f53\u5b89\u5168\u548c\u9690\u79c1\u98ce\u9669\uff0c\u5e76\u63a2\u8ba8\u4e86\u73b0\u6709\u9632\u5fa1\u673a\u5236\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\uff0cAI\u4ee3\u7406\u5df2\u6210\u4e3a\u80fd\u591f\u611f\u77e5\u3001\u63a8\u7406\u548c\u51b3\u7b56\u7684\u81ea\u4e3b\u4ea4\u4e92\u7cfb\u7edf\uff0c\u4f46\u8fd9\u4e5f\u5e26\u6765\u4e86\u5b89\u5168\u548c\u9690\u79c1\u7684\u6311\u6218\u3002\u8bba\u6587\u65e8\u5728\u5168\u9762\u5206\u6790IoA\u7cfb\u7edf\u4e2d\u7684\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bba\u6587\u9996\u5148\u6982\u8ff0\u4e86IoA\u67b6\u6784\u53ca\u5176\u4e0e\u4f20\u7edf\u7f51\u7edc\u4e0d\u540c\u7684\u8106\u5f31\u6027\uff0c\u7136\u540e\u4ece\u56db\u4e2a\u5173\u952e\u65b9\u9762\uff08\u8eab\u4efd\u8ba4\u8bc1\u3001\u8de8\u4ee3\u7406\u4fe1\u4efb\u3001\u5b9e\u4f53\u5b89\u5168\u548c\u9690\u79c1\u98ce\u9669\uff09\u6df1\u5165\u5206\u6790\u4e86\u73b0\u6709\u53ca\u65b0\u5174\u7684\u9632\u5fa1\u673a\u5236\u3002", "result": "\u8bba\u6587\u603b\u7ed3\u4e86\u5f53\u524dIoA\u7cfb\u7edf\u4e2d\u7684\u4e3b\u8981\u5b89\u5168\u95ee\u9898\u548c\u9632\u5fa1\u63aa\u65bd\uff0c\u5e76\u6307\u51fa\u4e86\u4e00\u4e9b\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\uff0c\u4ee5\u4fc3\u8fdb\u5f00\u53d1\u66f4\u5177\u5f39\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u7684IoA\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2505.09085", "pdf": "https://arxiv.org/pdf/2505.09085", "abs": "https://arxiv.org/abs/2505.09085", "authors": ["Jiaxuan Chen", "Yu Qi", "Yueming Wang", "Gang Pan"], "title": "Human-like Cognitive Generalization for Large Models via Brain-in-the-loop Supervision", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advancements in deep neural networks (DNNs), particularly large-scale\nlanguage models, have demonstrated remarkable capabilities in image and natural\nlanguage understanding. Although scaling up model parameters with increasing\nvolume of training data has progressively improved DNN capabilities, achieving\ncomplex cognitive abilities - such as understanding abstract concepts,\nreasoning, and adapting to novel scenarios, which are intrinsic to human\ncognition - remains a major challenge. In this study, we show that\nbrain-in-the-loop supervised learning, utilizing a small set of brain signals,\ncan effectively transfer human conceptual structures to DNNs, significantly\nenhancing their comprehension of abstract and even unseen concepts.\nExperimental results further indicate that the enhanced cognitive capabilities\nlead to substantial performance gains in challenging tasks, including\nfew-shot/zero-shot learning and out-of-distribution recognition, while also\nyielding highly interpretable concept representations. These findings highlight\nthat human-in-the-loop supervision can effectively augment the complex\ncognitive abilities of large models, offering a promising pathway toward\ndeveloping more human-like cognitive abilities in artificial systems.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u8111\u4fe1\u53f7\u76d1\u7763\u5b66\u4e60\uff0c\u5c06\u4eba\u7c7b\u6982\u5ff5\u7ed3\u6784\u8fc1\u79fb\u5230\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u5176\u62bd\u8c61\u548c\u672a\u77e5\u6982\u5ff5\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5b9e\u73b0\u8ba4\u77e5\u80fd\u529b\u7684\u589e\u5f3a\u3002", "motivation": "\u5c3d\u7ba1\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u548c\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u8868\u73b0\u5353\u8d8a\uff0c\u4f46\u6a21\u62df\u4eba\u7c7b\u7684\u590d\u6742\u8ba4\u77e5\u80fd\u529b\uff08\u5982\u62bd\u8c61\u6982\u5ff5\u7406\u89e3\u3001\u63a8\u7406\u548c\u9002\u5e94\u65b0\u573a\u666f\uff09\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4eba\u7c7b\u8111\u4fe1\u53f7\u76d1\u7763\u5b66\u4e60\uff0c\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u8111\u4fe1\u53f7\u76d1\u7763\u5b66\u4e60\uff08brain-in-the-loop supervised learning\uff09\uff0c\u5229\u7528\u5c11\u91cf\u8111\u4fe1\u53f7\u6570\u636e\u5c06\u4eba\u7c7b\u6982\u5ff5\u7ed3\u6784\u8fc1\u79fb\u81f3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5c11\u6837\u672c/\u96f6\u6837\u672c\u5b66\u4e60\u53ca\u5206\u5e03\u5916\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u540c\u65f6\u751f\u6210\u9ad8\u5ea6\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u8868\u793a\u3002", "conclusion": "\u4eba\u673a\u534f\u540c\u76d1\u7763\u53ef\u6709\u6548\u589e\u5f3a\u5927\u6a21\u578b\u7684\u590d\u6742\u8ba4\u77e5\u80fd\u529b\uff0c\u4e3a\u5f00\u53d1\u66f4\u5177\u4eba\u7c7b\u8ba4\u77e5\u7279\u6027\u7684\u4eba\u5de5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.08808", "pdf": "https://arxiv.org/pdf/2505.08808", "abs": "https://arxiv.org/abs/2505.08808", "authors": ["Anqing Jiang", "Jinhao Chai", "Yu Gao", "Yiru Wang", "Yuwen Heng", "Zhigang Sun", "Hao Sun", "Zezhong Zhao", "Li Sun", "Jian Zhou", "Lijuan Zhu", "Shugong Xu", "Hao Zhao"], "title": "SparseMeXT Unlocking the Potential of Sparse Representations for HD Map Construction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in high-definition \\emph{HD} map construction have\ndemonstrated the effectiveness of dense representations, which heavily rely on\ncomputationally intensive bird's-eye view \\emph{BEV} features. While sparse\nrepresentations offer a more efficient alternative by avoiding dense BEV\nprocessing, existing methods often lag behind due to the lack of tailored\ndesigns. These limitations have hindered the competitiveness of sparse\nrepresentations in online HD map construction. In this work, we systematically\nrevisit and enhance sparse representation techniques, identifying key\narchitectural and algorithmic improvements that bridge the gap with--and\nultimately surpass--dense approaches. We introduce a dedicated network\narchitecture optimized for sparse map feature extraction, a sparse-dense\nsegmentation auxiliary task to better leverage geometric and semantic cues, and\na denoising module guided by physical priors to refine predictions. Through\nthese enhancements, our method achieves state-of-the-art performance on the\nnuScenes dataset, significantly advancing HD map construction and centerline\ndetection. Specifically, SparseMeXt-Tiny reaches a mean average precision\n\\emph{mAP} of 55.5% at 32 frames per second \\emph{fps}, while SparseMeXt-Base\nattains 65.2% mAP. Scaling the backbone and decoder further, SparseMeXt-Large\nachieves an mAP of 68.9% at over 20 fps, establishing a new benchmark for\nsparse representations in HD map construction. These results underscore the\nuntapped potential of sparse methods, challenging the conventional reliance on\ndense representations and redefining efficiency-performance trade-offs in the\nfield.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u6539\u8fdb\u7a00\u758f\u8868\u793a\u6280\u672f\uff0c\u63d0\u51fa\u4e13\u7528\u7f51\u7edc\u67b6\u6784\u3001\u7a00\u758f-\u5bc6\u96c6\u5206\u5272\u8f85\u52a9\u4efb\u52a1\u548c\u7269\u7406\u5148\u9a8c\u53bb\u566a\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u9ad8\u7cbe\u5730\u56fe\u6784\u5efa\u6027\u80fd\uff0c\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u8868\u793a\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u9488\u5bf9\u6027\u8bbe\u8ba1\u800c\u6027\u80fd\u4e0d\u8db3\uff0c\u65e0\u6cd5\u4e0e\u5bc6\u96c6\u8868\u793a\u7ade\u4e89\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u67b6\u6784\u548c\u7b97\u6cd5\u6539\u8fdb\uff0c\u4f7f\u7a00\u758f\u8868\u793a\u8d85\u8d8a\u5bc6\u96c6\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7a00\u758f\u5730\u56fe\u7279\u5f81\u63d0\u53d6\u4e13\u7528\u7f51\u7edc\u67b6\u6784\u3001\u7a00\u758f-\u5bc6\u96c6\u5206\u5272\u8f85\u52a9\u4efb\u52a1\u4ee5\u53ca\u57fa\u4e8e\u7269\u7406\u5148\u9a8c\u7684\u53bb\u566a\u6a21\u5757\u3002", "result": "SparseMeXt\u7cfb\u5217\u6a21\u578b\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cTiny\u7248\u8fbe55.5% mAP/32fps\uff0cBase\u724865.2% mAP\uff0cLarge\u724868.9% mAP/20fps\u3002", "conclusion": "\u7a00\u758f\u65b9\u6cd5\u6f5c\u529b\u5de8\u5927\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u5bc6\u96c6\u8868\u793a\u7684\u4f9d\u8d56\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u7684\u6743\u8861\u3002"}}
{"id": "2505.09089", "pdf": "https://arxiv.org/pdf/2505.09089", "abs": "https://arxiv.org/abs/2505.09089", "authors": ["Philipp Hess", "Maximilian Gelbrecht", "Christof Sch\u00f6tz", "Michael Aich", "Yu Huang", "Shangshang Yang", "Niklas Boers"], "title": "Generating time-consistent dynamics with discriminator-guided image diffusion models", "categories": ["cs.LG"], "comment": null, "summary": "Realistic temporal dynamics are crucial for many video generation, processing\nand modelling applications, e.g. in computational fluid dynamics, weather\nprediction, or long-term climate simulations. Video diffusion models (VDMs) are\nthe current state-of-the-art method for generating highly realistic dynamics.\nHowever, training VDMs from scratch can be challenging and requires large\ncomputational resources, limiting their wider application. Here, we propose a\ntime-consistency discriminator that enables pretrained image diffusion models\nto generate realistic spatiotemporal dynamics. The discriminator guides the\nsampling inference process and does not require extensions or finetuning of the\nimage diffusion model. We compare our approach against a VDM trained from\nscratch on an idealized turbulence simulation and a real-world global\nprecipitation dataset. Our approach performs equally well in terms of temporal\nconsistency, shows improved uncertainty calibration and lower biases compared\nto the VDM, and achieves stable centennial-scale climate simulations at daily\ntime steps.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u4e00\u81f4\u6027\u5224\u522b\u5668\uff0c\u4f7f\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u6269\u6563\u6a21\u578b\u80fd\u591f\u751f\u6210\u903c\u771f\u7684\u65f6\u7a7a\u52a8\u6001\uff0c\u65e0\u9700\u4ece\u5934\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDMs\uff09\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ec\u7a33\u5b9a\u957f\u671f\u7684\u6bcf\u65e5\u6c14\u5019\u6a21\u62df\u3002", "motivation": "\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDMs\uff09\u867d\u80fd\u751f\u6210\u9ad8\u5ea6\u903c\u771f\u7684\u52a8\u6001\uff0c\u4f46\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u5927\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u65f6\u95f4\u4e00\u81f4\u6027\u5224\u522b\u5668\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u65f6\u7a7a\u52a8\u6001\u751f\u6210\uff0c\u964d\u4f4e\u5e94\u7528\u95e8\u69db\u3002", "method": "\u91c7\u7528\u65f6\u95f4\u4e00\u81f4\u6027\u5224\u522b\u5668\u6307\u5bfc\u91c7\u6837\u63a8\u65ad\u8fc7\u7a0b\uff0c\u65e0\u9700\u4fee\u6539\u6216\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u6269\u6563\u6a21\u578b\u3002", "result": "\u5728\u7406\u60f3\u6e4d\u6d41\u6a21\u62df\u548c\u5168\u7403\u964d\u6c34\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0e\u4ece\u5934\u8bad\u7ec3\u7684VDMs\u76f8\u5f53\uff0c\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u548c\u504f\u5dee\u66f4\u4f4e\uff0c\u5e76\u80fd\u5b9e\u73b0\u957f\u8fbe\u767e\u5e74\u7684\u6bcf\u65e5\u6c14\u5019\u7a33\u5b9a\u6a21\u62df\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u8d44\u6e90\u53cb\u597d\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u663e\u8457\u6269\u5c55\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5e94\u7528\u8303\u56f4\uff0c\u5c24\u5176\u5728\u957f\u671f\u52a8\u6001\u6a21\u62df\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2505.08809", "pdf": "https://arxiv.org/pdf/2505.08809", "abs": "https://arxiv.org/abs/2505.08809", "authors": ["Shixi Qin", "Zhiyong Yang", "Shilong Bao", "Shi Wang", "Qianqian Xu", "Qingming Huang"], "title": "MixBridge: Heterogeneous Image-to-Image Backdoor Attack through Mixture of Schr\u00f6dinger Bridges", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "This paper focuses on implanting multiple heterogeneous backdoor triggers in\nbridge-based diffusion models designed for complex and arbitrary input\ndistributions. Existing backdoor formulations mainly address single-attack\nscenarios and are limited to Gaussian noise input models. To fill this gap, we\npropose MixBridge, a novel diffusion Schr\\\"odinger bridge (DSB) framework to\ncater to arbitrary input distributions (taking I2I tasks as special cases).\nBeyond this trait, we demonstrate that backdoor triggers can be injected into\nMixBridge by directly training with poisoned image pairs. This eliminates the\nneed for the cumbersome modifications to stochastic differential equations\nrequired in previous studies, providing a flexible tool to study backdoor\nbehavior for bridge models. However, a key question arises: can a single DSB\nmodel train multiple backdoor triggers? Unfortunately, our theory shows that\nwhen attempting this, the model ends up following the geometric mean of benign\nand backdoored distributions, leading to performance conflict across backdoor\ntasks. To overcome this, we propose a Divide-and-Merge strategy to mix\ndifferent bridges, where models are independently pre-trained for each specific\nobjective (Divide) and then integrated into a unified model (Merge). In\naddition, a Weight Reallocation Scheme (WRS) is also designed to enhance the\nstealthiness of MixBridge. Empirical studies across diverse generation tasks\nspeak to the efficacy of MixBridge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMixBridge\uff0c\u4e00\u79cd\u65b0\u7684\u6269\u6563Schr\u00f6dinger\u6865\uff08DSB\uff09\u6846\u67b6\uff0c\u652f\u6301\u4efb\u610f\u8f93\u5165\u5206\u5e03\u7684\u591a\u91cd\u5f02\u6784\u540e\u95e8\u89e6\u53d1\u5668\u690d\u5165\uff0c\u5e76\u901a\u8fc7Divide-and-Merge\u7b56\u7565\u548c\u6743\u91cd\u91cd\u5206\u914d\u65b9\u6848\uff08WRS\uff09\u4f18\u5316\u6027\u80fd\u4e0e\u9690\u853d\u6027\u3002", "motivation": "\u73b0\u6709\u540e\u95e8\u690d\u5165\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u653b\u51fb\u573a\u666f\u4e14\u9650\u4e8e\u9ad8\u65af\u566a\u58f0\u8f93\u5165\u6a21\u578b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u590d\u6742\u8f93\u5165\u5206\u5e03\u9700\u6c42\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faMixBridge\u6846\u67b6\uff0c\u76f4\u63a5\u5229\u7528\u6bd2\u5316\u56fe\u50cf\u5bf9\u8bad\u7ec3\u690d\u5165\u540e\u95e8\u89e6\u53d1\u5668\uff0c\u65e0\u9700\u4fee\u6539\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff1b\u901a\u8fc7Divide-and-Merge\u7b56\u7565\u72ec\u7acb\u9884\u8bad\u7ec3\u5e76\u5408\u5e76\u6a21\u578b\uff0c\u7ed3\u5408WRS\u63d0\u5347\u9690\u853d\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u663e\u793a\u591a\u89e6\u53d1\u5668\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u6a21\u578b\u8868\u73b0\u51b2\u7a81\uff0c\u4f46Divide-and-Merge\u548cWRS\u6709\u6548\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MixBridge\u5728\u591a\u6837\u5316\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "MixBridge\u4e3a\u6865\u6a21\u578b\u540e\u95e8\u884c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u7075\u6d3b\u5de5\u5177\uff0c\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u591a\u89e6\u53d1\u5668\u690d\u5165\u7684\u51b2\u7a81\u4e0e\u9690\u853d\u6027\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.09106", "pdf": "https://arxiv.org/pdf/2505.09106", "abs": "https://arxiv.org/abs/2505.09106", "authors": ["Ya Liu", "Kai Yang", "Yu Zhu", "Keying Yang", "Haibo Zhao"], "title": "Argus: Federated Non-convex Bilevel Learning over 6G Space-Air-Ground Integrated Network", "categories": ["cs.LG", "68T07", "I.2"], "comment": "17 pages, 11 figures", "summary": "The space-air-ground integrated network (SAGIN) has recently emerged as a\ncore element in the 6G networks. However, traditional centralized and\nsynchronous optimization algorithms are unsuitable for SAGIN due to\ninfrastructureless and time-varying environments. This paper aims to develop a\nnovel Asynchronous algorithm a.k.a. Argus for tackling non-convex and\nnon-smooth decentralized federated bilevel learning over SAGIN. The proposed\nalgorithm allows networked agents (e.g. autonomous aerial vehicles) to tackle\nbilevel learning problems in time-varying networks asynchronously, thereby\naverting stragglers from impeding the overall training speed. We provide a\ntheoretical analysis of the iteration complexity, communication complexity, and\ncomputational complexity of Argus. Its effectiveness is further demonstrated\nthrough numerical experiments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aArgus\u7684\u5f02\u6b65\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3SAGIN\u4e2d\u975e\u51f8\u975e\u5149\u6ed1\u7684\u5206\u6563\u8054\u90a6\u53cc\u5c42\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u52a8\u6001\u7f51\u7edc\u4e0b\u7684\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u96c6\u4e2d\u5f0f\u540c\u6b65\u4f18\u5316\u7b97\u6cd5\u5728SAGIN\u4e2d\u4e0d\u9002\u7528\uff0c\u56e0\u5176\u7f3a\u4e4f\u57fa\u7840\u8bbe\u65bd\u4e14\u73af\u5883\u65f6\u53d8\uff0c\u9700\u5f00\u53d1\u5f02\u6b65\u7b97\u6cd5\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1Argus\u5f02\u6b65\u7b97\u6cd5\uff0c\u652f\u6301\u7f51\u7edc\u4ee3\u7406\u5728\u65f6\u53d8\u7f51\u7edc\u4e2d\u5f02\u6b65\u5904\u7406\u53cc\u5c42\u5b66\u4e60\u95ee\u9898\uff0c\u907f\u514d\u62d6\u6162\u6574\u4f53\u8bad\u7ec3\u901f\u5ea6\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff08\u8fed\u4ee3\u3001\u901a\u4fe1\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\uff09\u53ca\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86Argus\u7684\u6709\u6548\u6027\u3002", "conclusion": "Argus\u7b97\u6cd5\u5728SAGIN\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u4e3a6G\u7f51\u7edc\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09083", "pdf": "https://arxiv.org/pdf/2505.09083", "abs": "https://arxiv.org/abs/2505.09083", "authors": ["Dominic Zaun Eu Jones"], "title": "Ornithologist: Towards Trustworthy \"Reasoning\" about Central Bank Communications", "categories": ["econ.GN", "cs.CL", "q-fin.EC", "J.4; I.2.7"], "comment": "16 pages, 6 figures", "summary": "I develop Ornithologist, a weakly-supervised textual classification system\nand measure the hawkishness and dovishness of central bank text. Ornithologist\nuses ``taxonomy-guided reasoning'', guiding a large language model with\nhuman-authored decision trees. This increases the transparency and\nexplainability of the system and makes it accessible to non-experts. It also\nreduces hallucination risk. Since it requires less supervision than traditional\nclassification systems, it can more easily be applied to other problems or\nsources of text (e.g. news) without much modification. Ornithologist\nmeasurements of hawkishness and dovishness of RBA communication carry\ninformation about the future of the cash rate path and of market expectations.", "AI": {"tldr": "Ornithologist\u662f\u4e00\u79cd\u5f31\u76d1\u7763\u6587\u672c\u5206\u7c7b\u7cfb\u7edf\uff0c\u7528\u4e8e\u6d4b\u91cf\u4e2d\u592e\u94f6\u884c\u6587\u672c\u7684\u9e70\u6d3e\u548c\u9e3d\u6d3e\u503e\u5411\uff0c\u901a\u8fc7\u201c\u5206\u7c7b\u5f15\u5bfc\u63a8\u7406\u201d\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u51cf\u5c11\u5e7b\u89c9\u98ce\u9669\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u900f\u660e\u3001\u6613\u89e3\u91ca\u4e14\u9002\u7528\u4e8e\u975e\u4e13\u5bb6\u7684\u6587\u672c\u5206\u7c7b\u7cfb\u7edf\uff0c\u4ee5\u5206\u6790\u592e\u884c\u6587\u672c\u4e2d\u7684\u9e70\u6d3e\u548c\u9e3d\u6d3e\u503e\u5411\u3002", "method": "\u4f7f\u7528\u201c\u5206\u7c7b\u5f15\u5bfc\u63a8\u7406\u201d\u65b9\u6cd5\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4eba\u7c7b\u7f16\u5199\u7684\u51b3\u7b56\u6811\uff0c\u8fdb\u884c\u5f31\u76d1\u7763\u6587\u672c\u5206\u7c7b\u3002", "result": "Ornithologist\u80fd\u591f\u6709\u6548\u6d4b\u91cf\u592e\u884c\u6587\u672c\u4e2d\u7684\u9e70\u6d3e\u548c\u9e3d\u6d3e\u503e\u5411\uff0c\u5e76\u5bf9\u672a\u6765\u73b0\u91d1\u5229\u7387\u8def\u5f84\u548c\u5e02\u573a\u9884\u671f\u63d0\u4f9b\u6709\u7528\u4fe1\u606f\u3002", "conclusion": "Ornithologist\u7cfb\u7edf\u5728\u964d\u4f4e\u76d1\u7763\u9700\u6c42\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u592e\u884c\u548c\u5176\u4ed6\u6587\u672c\u5206\u6790\u4efb\u52a1\u3002"}}
{"id": "2505.08810", "pdf": "https://arxiv.org/pdf/2505.08810", "abs": "https://arxiv.org/abs/2505.08810", "authors": ["Bappa Muktar", "Vincent Fono", "Adama Nouboukpo"], "title": "Machine Learning-Based Detection of DDoS Attacks in VANETs for Emergency Vehicle Communication", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Vehicular Ad Hoc Networks (VANETs) play a key role in Intelligent\nTransportation Systems (ITS), particularly in enabling real-time communication\nfor emergency vehicles. However, Distributed Denial of Service (DDoS) attacks,\nwhich interfere with safety-critical communication channels, can severely\nimpair their reliability. This study introduces a robust and scalable framework\nto detect DDoS attacks in highway-based VANET environments. A synthetic dataset\nwas constructed using Network Simulator 3 (NS-3) in conjunction with the\nSimulation of Urban Mobility (SUMO) and further enriched with real-world\nmobility traces from Germany's A81 highway, extracted via OpenStreetMap (OSM).\nThree traffic categories were simulated: DDoS, VoIP, and TCP-based video\nstreaming (VideoTCP). The data preprocessing pipeline included normalization,\nsignal-to-noise ratio (SNR) feature engineering, missing value imputation, and\nclass balancing using the Synthetic Minority Over-sampling Technique (SMOTE).\nFeature importance was assessed using SHapley Additive exPlanations (SHAP).\nEleven classifiers were benchmarked, among them XGBoost (XGB), CatBoost (CB),\nAdaBoost (AB), GradientBoosting (GB), and an Artificial Neural Network (ANN).\nXGB and CB achieved the best performance, each attaining an F1-score of 96%.\nThese results highlight the robustness of the proposed framework and its\npotential for real-time deployment in VANETs to secure critical emergency\ncommunications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u68c0\u6d4b\u9ad8\u901f\u516c\u8defVANET\u4e2dDDoS\u653b\u51fb\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1\uff0cXGBoost\u548cCatBoost\u8868\u73b0\u6700\u4f73\uff0cF1\u5206\u6570\u8fbe96%\u3002", "motivation": "VANET\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46DDoS\u653b\u51fb\u4f1a\u5a01\u80c1\u5176\u53ef\u9760\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4f7f\u7528NS-3\u548cSUMO\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u771f\u5b9e\u4ea4\u901a\u6570\u636e\uff0c\u901a\u8fc7\u9884\u5904\u7406\u548cSMOTE\u5e73\u8861\u7c7b\u522b\uff0c\u4f7f\u7528SHAP\u8bc4\u4f30\u7279\u5f81\u91cd\u8981\u6027\uff0c\u5e76\u6bd4\u8f83\u4e8611\u79cd\u5206\u7c7b\u5668\u3002", "result": "XGBoost\u548cCatBoost\u8868\u73b0\u6700\u4f73\uff0cF1\u5206\u6570\u8fbe96%\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u671b\u5b9e\u65f6\u90e8\u7f72\u4e8eVANET\uff0c\u4fdd\u969c\u5173\u952e\u5e94\u6025\u901a\u4fe1\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2505.09113", "pdf": "https://arxiv.org/pdf/2505.09113", "abs": "https://arxiv.org/abs/2505.09113", "authors": ["Yingrong Wang", "Anpeng Wu", "Baohong Li", "Ziyang Xiao", "Ruoxuan Xiong", "Qing Han", "Kun Kuang"], "title": "Sequential Treatment Effect Estimation with Unmeasured Confounders", "categories": ["cs.LG", "stat.ME"], "comment": null, "summary": "This paper studies the cumulative causal effects of sequential treatments in\nthe presence of unmeasured confounders. It is a critical issue in sequential\ndecision-making scenarios where treatment decisions and outcomes dynamically\nevolve over time. Advanced causal methods apply transformer as a backbone to\nmodel such time sequences, which shows superiority in capturing long time\ndependence and periodic patterns via attention mechanism. However, even they\ncontrol the observed confounding, these estimators still suffer from unmeasured\nconfounders, which influence both treatment assignments and outcomes. How to\nadjust the latent confounding bias in sequential treatment effect estimation\nremains an open challenge. Therefore, we propose a novel Decomposing Sequential\nInstrumental Variable framework for CounterFactual Regression (DSIV-CFR),\nrelying on a common negative control assumption. Specifically, an instrumental\nvariable (IV) is a special negative control exposure, while the previous\noutcome serves as a negative control outcome. This allows us to recover the IVs\nlatent in observation variables and estimate sequential treatment effects via a\ngeneralized moment condition. We conducted experiments on 4 datasets and\nachieved significant performance in one- and multi-step prediction, supported\nby which we can identify optimal treatments for dynamic systems.", "AI": {"tldr": "DSIV-CFR\u6846\u67b6\u5229\u7528\u5de5\u5177\u53d8\u91cf\u548c\u8d1f\u63a7\u5236\u5047\u8bbe\u89e3\u51b3\u5e8f\u5217\u6cbb\u7597\u4e2d\u7684\u672a\u89c2\u6d4b\u6df7\u6742\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5e8f\u5217\u51b3\u7b56\u4e2d\uff0c\u52a8\u6001\u53d8\u5316\u7684\u6cbb\u7597\u548c\u7ed3\u679c\u6613\u53d7\u672a\u89c2\u6d4b\u6df7\u6742\u5f71\u54cd\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\uff0c\u9700\u65b0\u65b9\u6cd5\u8c03\u6574\u504f\u5dee\u3002", "method": "\u63d0\u51faDSIV-CFR\u6846\u67b6\uff0c\u7ed3\u5408\u5de5\u5177\u53d8\u91cf\uff08IV\uff09\u4e0e\u8d1f\u63a7\u5236\u5047\u8bbe\uff0c\u901a\u8fc7\u5e7f\u4e49\u77e9\u6761\u4ef6\u4f30\u8ba1\u5e8f\u5217\u6cbb\u7597\u6548\u679c\u3002", "result": "\u57284\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5355\u6b65\u548c\u591a\u6b65\u9884\u6d4b\u6548\u679c\u663e\u8457\uff0c\u80fd\u8bc6\u522b\u52a8\u6001\u7cfb\u7edf\u6700\u4f18\u6cbb\u7597\u3002", "conclusion": "DSIV-CFR\u6709\u6548\u89e3\u51b3\u5e8f\u5217\u6cbb\u7597\u4e2d\u7684\u672a\u89c2\u6d4b\u6df7\u6742\u95ee\u9898\uff0c\u4e3a\u52a8\u6001\u51b3\u7b56\u63d0\u4f9b\u53ef\u9760\u65b9\u6cd5\u3002"}}
{"id": "2505.09246", "pdf": "https://arxiv.org/pdf/2505.09246", "abs": "https://arxiv.org/abs/2505.09246", "authors": ["Derian Boer", "Stephen Roth", "Stefan Kramer"], "title": "Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "In many real-world settings, machine learning models and interactive systems\nhave access to both structured knowledge, e.g., knowledge graphs or tables, and\nunstructured content, e.g., natural language documents. However, most rely on\neither. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking\nunstructured content to nodes within structured data, thereby enabling new\nstrategies for knowledge access and use. In this work, we present\nFocusedRetriever, a modular SKB-based framework for multi-hop question\nanswering. It integrates components (VSS-based entity search, LLM-based\ngeneration of Cypher queries and pairwise re-ranking) in a way that enables it\nto outperform state-of-the-art methods across all three STaRK benchmark test\nsets, covering diverse domains and multiple performance metrics. The average\nfirst-hit rate exceeds that of the second-best method by 25.7%.\nFocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to\nextract relational facts and entity attributes from unstructured text, (2) node\nset joins to filter answer candidates based on these extracted triplets and\nconstraints, (3) vector similarity search to retrieve and rank relevant\nunstructured content, and (4) the contextual capabilities of LLMs to finally\nrank the top-k answers. For generality, we only incorporate base LLMs in\nFocusedRetriever in our evaluation. However, our analysis of intermediate\nresults highlights several opportunities for further upgrades including\nfinetuning. The source code is publicly available at\nhttps://github.com/kramerlab/FocusedRetriever .", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faFocusedRetriever\uff0c\u4e00\u79cd\u57fa\u4e8e\u534a\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u3002\u901a\u8fc7\u7ed3\u5408\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\u3001LLM\u751f\u6210\u7684Cypher\u67e5\u8be2\u548c\u91cd\u65b0\u6392\u540d\uff0c\u5728STaRK\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7b2c\u4e00\u547d\u4e2d\u7387\u5e73\u5747\u63d0\u534725.7%\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u4ea4\u4e92\u7cfb\u7edf\u901a\u5e38\u9700\u540c\u65f6\u5904\u7406\u7ed3\u6784\u5316\u77e5\u8bc6\uff08\u5982\u77e5\u8bc6\u56fe\u8c31\uff09\u548c\u975e\u7ed3\u6784\u5316\u5185\u5bb9\uff08\u5982\u81ea\u7136\u8bed\u8a00\u6587\u6863\uff09\u3002\u4f20\u7edf\u65b9\u6cd5\u5f80\u5f80\u4ec5\u4f9d\u8d56\u5176\u4e2d\u4e00\u79cd\uff0c\u800c\u534a\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\uff08SKBs\uff09\u80fd\u5f25\u5408\u8fd9\u4e00\u9e3f\u6c9f\uff0c\u901a\u8fc7\u94fe\u63a5\u975e\u7ed3\u6784\u5316\u5185\u5bb9\u4e0e\u7ed3\u6784\u5316\u6570\u636e\u8282\u70b9\uff0c\u4e3a\u77e5\u8bc6\u8bbf\u95ee\u4e0e\u4f7f\u7528\u63d0\u4f9b\u65b0\u7b56\u7565\u3002", "method": "Framework integrates four key techniques: (1) LLMs extract relational facts & attributes from text; (2) node set joins filter candidates using extracted triplets; (3) vector similarity search retrieves/ranks unstructured content; (4) LLMs contextually re-rank top-k answers.", "result": "\u5728STaRK\u57fa\u51c6\u6d4b\u8bd5\u7684\u591a\u6837\u5316\u9886\u57df\u548c\u6307\u6807\u4e2d\uff0cFocusedRetriever\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u7b2c\u4e00\u547d\u4e2d\u7387\u5e73\u5747\u9886\u5148\u6b21\u4f18\u65b9\u6cd525.7%\u3002\u4ec5\u4f7f\u7528\u57fa\u7840LLM\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u4f46\u4e2d\u95f4\u7ed3\u679c\u5206\u6790\u8868\u660e\u5fae\u8c03\u7b49\u4f18\u5316\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "FocusedRetriever\u9a8c\u8bc1\u4e86SKBs\u5728\u8de8\u6a21\u6001\u77e5\u8bc6\u878d\u5408\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u4fbf\u4e8e\u6269\u5c55\uff08\u5982LLM\u5fae\u8c03\uff09\u3002\u672a\u6765\u53ef\u901a\u8fc7\u8fdb\u4e00\u6b65\u4f18\u5316\u4e2d\u95f4\u6b65\u9aa4\uff08\u5982\u67e5\u8be2\u751f\u6210\uff09\u63d0\u5347\u6027\u80fd\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.08814", "pdf": "https://arxiv.org/pdf/2505.08814", "abs": "https://arxiv.org/abs/2505.08814", "authors": ["Wenkai Li", "Xiaoqi Li", "Yingjie Mao", "Yishun Wang"], "title": "Towards Understanding Deep Learning Model in Image Recognition via Coverage Test", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep neural networks (DNNs) play a crucial role in the field of artificial\nintelligence, and their security-related testing has been a prominent research\nfocus. By inputting test cases, the behavior of models is examined for\nanomalies, and coverage metrics are utilized to determine the extent of neurons\ncovered by these test cases. With the widespread application and advancement of\nDNNs, different types of neural behaviors have garnered attention, leading to\nthe emergence of various coverage metrics for neural networks. However, there\nis currently a lack of empirical research on these coverage metrics,\nspecifically in analyzing the relationships and patterns between model depth,\nconfiguration information, and neural network coverage. This paper aims to\ninvestigate the relationships and patterns of four coverage metrics: primary\nfunctionality, boundary, hierarchy, and structural coverage. A series of\nempirical experiments were conducted, selecting LeNet, VGG, and ResNet as\ndifferent DNN architectures, along with 10 models of varying depths ranging\nfrom 5 to 54 layers, to compare and study the relationships between different\ndepths, configuration information, and various neural network coverage metrics.\nAdditionally, an investigation was carried out on the relationships between\nmodified decision/condition coverage and dataset size. Finally, three potential\nfuture directions are proposed to further contribute to the security testing of\nDNN Models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5b89\u5168\u6d4b\u8bd5\u4e2d\u7684\u56db\u79cd\u8986\u76d6\u5ea6\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u63a2\u8ba8\u6a21\u578b\u6df1\u5ea6\u3001\u914d\u7f6e\u4fe1\u606f\u53ca\u6570\u636e\u96c6\u5927\u5c0f\u5bf9\u5176\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "DNN\u5728\u4eba\u5de5\u667a\u80fd\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5b89\u5168\u6d4b\u8bd5\u7f3a\u4e4f\u5bf9\u8986\u76d6\u5ea6\u91cf\uff08\u5982\u529f\u80fd\u3001\u8fb9\u754c\u3001\u5c42\u6b21\u548c\u7ed3\u6784\u8986\u76d6\uff09\u4e0e\u6a21\u578b\u6df1\u5ea6\u3001\u914d\u7f6e\u4fe1\u606f\u5173\u7cfb\u7684\u5b9e\u8bc1\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u9009\u53d6LeNet\u3001VGG\u548cResNet\u7b49\u4e0d\u540c\u67b6\u6784\u53ca5\u81f354\u5c42\u7684\u6a21\u578b\uff0c\u8fdb\u884c\u5b9e\u9a8c\u6bd4\u8f83\u4e0d\u540c\u6df1\u5ea6\u3001\u914d\u7f6e\u4fe1\u606f\u4e0e\u8986\u76d6\u5ea6\u91cf\u7684\u5173\u7cfb\uff0c\u5e76\u5206\u6790\u4fee\u6539\u540e\u7684\u51b3\u7b56/\u6761\u4ef6\u8986\u76d6\u4e0e\u6570\u636e\u96c6\u5927\u5c0f\u7684\u5173\u8054\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u4e0d\u540c\u8986\u76d6\u5ea6\u91cf\u4e0e\u6a21\u578b\u6df1\u5ea6\u53ca\u914d\u7f6e\u7684\u5173\u8054\uff0c\u5e76\u53d1\u73b0\u6570\u636e\u96c6\u5927\u5c0f\u5bf9\u51b3\u7b56/\u6761\u4ef6\u8986\u76d6\u7684\u5f71\u54cd\uff0c\u4e3aDNN\u5b89\u5168\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86DNN\u8986\u76d6\u5ea6\u91cf\u5b9e\u8bc1\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u4e09\u4e2a\u7814\u7a76\u65b9\u5411\uff0c\u52a9\u529bDNN\u5b89\u5168\u6d4b\u8bd5\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2505.09131", "pdf": "https://arxiv.org/pdf/2505.09131", "abs": "https://arxiv.org/abs/2505.09131", "authors": ["Kunwoong Kim", "Jihu Lee", "Sangchul Park", "Yongdai Kim"], "title": "Fair Clustering via Alignment", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted at ICML 2025. This is the version submitted for review and\n  will be replaced by the camera-ready version soon", "summary": "Algorithmic fairness in clustering aims to balance the proportions of\ninstances assigned to each cluster with respect to a given sensitive attribute.\nWhile recently developed fair clustering algorithms optimize clustering\nobjectives under specific fairness constraints, their inherent complexity or\napproximation often results in suboptimal clustering utility or numerical\ninstability in practice. To resolve these limitations, we propose a new fair\nclustering algorithm based on a novel decomposition of the fair K-means\nclustering objective function. The proposed algorithm, called Fair Clustering\nvia Alignment (FCA), operates by alternately (i) finding a joint probability\ndistribution to align the data from different protected groups, and (ii)\noptimizing cluster centers in the aligned space. A key advantage of FCA is that\nit theoretically guarantees approximately optimal clustering utility for any\ngiven fairness level without complex constraints, thereby enabling high-utility\nfair clustering in practice. Experiments show that FCA outperforms existing\nmethods by (i) attaining a superior trade-off between fairness level and\nclustering utility, and (ii) achieving near-perfect fairness without numerical\ninstability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86FCA\u7b97\u6cd5\u901a\u8fc7\u5206\u89e3\u516c\u5e73K\u5747\u503c\u76ee\u6807\u51fd\u6570\u89e3\u51b3\u805a\u7c7b\u7b97\u6cd5\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u4fdd\u8bc1\u4f18\u5316\u805a\u7c7b\u6548\u7528\u540c\u65f6\u907f\u514d\u590d\u6742\u7ea6\u675f\u3002\u5b9e\u9a8c\u8bc1\u660eFCA\u5728\u516c\u5e73\u6027\u4e0e\u805a\u7c7b\u6548\u7528\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u4f18\u5e73\u8861\uff0c\u4e14\u7a33\u5b9a\u6027\u9ad8\u3002", "motivation": "\u73b0\u6709\u516c\u5e73\u805a\u7c7b\u7b97\u6cd5\u56e0\u590d\u6742\u6027\u6216\u8fd1\u4f3c\u6027\u5e38\u5bfc\u81f4\u805a\u7c7b\u6548\u80fd\u4e0d\u8db3\u6216\u6570\u503c\u4e0d\u7a33\u5b9a\uff0c\u56e0\u6b64\u9700\u5f00\u53d1\u66f4\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u516c\u5e73\u805a\u7c7b\u65b9\u6cd5\u3002", "method": "FCA\u7b97\u6cd5\u901a\u8fc7\u4ea4\u66ff\uff081\uff09\u5bfb\u627e\u8054\u5408\u6982\u7387\u5206\u5e03\u5bf9\u9f50\u4e0d\u540c\u53d7\u4fdd\u62a4\u7fa4\u4f53\u7684\u6570\u636e\uff0c\uff082\uff09\u5728\u5bf9\u9f50\u7a7a\u95f4\u4e2d\u4f18\u5316\u805a\u7c7b\u4e2d\u5fc3\uff0c\u907f\u514d\u590d\u6742\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u663e\u793aFCA\u5728\u516c\u5e73\u6027\u4e0e\u805a\u7c7b\u6548\u7528\u95f4\u53d6\u5f97\u66f4\u4f18\u5e73\u8861\uff0c\u4e14\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u516c\u5e73\u6027\u800c\u65e0\u6570\u503c\u4e0d\u7a33\u5b9a\u3002", "conclusion": "FCA\u901a\u8fc7\u7b80\u5316\u89e3\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u516c\u5e73\u805a\u7c7b\u7684\u5b9e\u7528\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u7684\u9ad8\u6548\u516c\u5e73\u805a\u7c7b\u3002"}}
{"id": "2505.09436", "pdf": "https://arxiv.org/pdf/2505.09436", "abs": "https://arxiv.org/abs/2505.09436", "authors": ["Raghav Garg", "Kapil Sharma", "Karan Gupta"], "title": "CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) hold immense potential for revolutionizing\nCustomer Experience Management (CXM), particularly in contact center\noperations. However, evaluating their practical utility in complex operational\nenvironments is hindered by data scarcity (due to privacy concerns) and the\nlimitations of current benchmarks. Existing benchmarks often lack realism,\nfailing to incorporate deep knowledge base (KB) integration, real-world noise,\nor critical operational tasks beyond conversational fluency. To bridge this\ngap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset\nspecifically designed for evaluating AI in operational CXM contexts. Given the\ndiversity in possible contact center features, we have developed a scalable\nLLM-powered pipeline that simulates the brand's CXM entities that form the\nfoundation of our datasets-such as knowledge articles including product\nspecifications, issue taxonomies, and contact center conversations. The\nentities closely represent real-world distribution because of controlled noise\ninjection (informed by domain experts) and rigorous automated validation.\nBuilding on this, we release CXMArena, which provides dedicated benchmarks\ntargeting five important operational tasks: Knowledge Base Refinement, Intent\nPrediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with\nIntegrated Tools. Our baseline experiments underscore the benchmark's\ndifficulty: even state of the art embedding and generation models achieve only\n68% accuracy on article search, while standard embedding methods yield a low F1\nscore of 0.3 for knowledge base refinement, highlighting significant challenges\nfor current models necessitating complex pipelines and solutions over\nconventional techniques.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86CXMArena\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u5728\u5ba2\u6237\u4f53\u9a8c\u7ba1\u7406\uff08CXM\uff09\u64cd\u4f5c\u73af\u5883\u4e2d\u7684\u65b0\u578b\u5927\u89c4\u6a21\u5408\u6210\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u771f\u5b9e\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u5728CXM\u4e2d\u7684\u5b9e\u7528\u8bc4\u4f30\u53d7\u9650\u4e8e\u6570\u636e\u7a00\u7f3a\u548c\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u771f\u5b9e\u6027\u548c\u5bf9\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u7684\u8986\u76d6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684LLM\u9a71\u52a8\u7684\u7ba1\u9053\uff0c\u6a21\u62df\u54c1\u724cCXM\u5b9e\u4f53\uff0c\u5305\u62ec\u77e5\u8bc6\u6587\u7ae0\u3001\u95ee\u9898\u5206\u7c7b\u548c\u8054\u7cfb\u4e2d\u5fc3\u5bf9\u8bdd\uff0c\u5e76\u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u548c\u81ea\u52a8\u9a8c\u8bc1\u589e\u5f3a\u771f\u5b9e\u6027\u3002", "result": "\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u6587\u7ae0\u641c\u7d22\u4efb\u52a1\u4e0a\u4ec5\u8fbe\u523068%\u7684\u51c6\u786e\u7387\uff0c\u77e5\u8bc6\u5e93\u4f18\u5316\u7684F1\u5206\u6570\u4f4e\u81f30.3\uff0c\u8868\u660e\u5f53\u524d\u6a21\u578b\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "conclusion": "CXMArena\u4e3a\u8bc4\u4f30AI\u5728\u590d\u6742CXM\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u57fa\u51c6\uff0c\u51f8\u663e\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u590d\u6742\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.08818", "pdf": "https://arxiv.org/pdf/2505.08818", "abs": "https://arxiv.org/abs/2505.08818", "authors": ["Amara Tariq", "Rimita Lahiri", "Charles Kahn", "Imon Banerjee"], "title": "Position: Restructuring of Categories and Implementation of Guidelines Essential for VLM Adoption in Healthcare", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": "15 pages, 2, tables, 3 figures", "summary": "The intricate and multifaceted nature of vision language model (VLM)\ndevelopment, adaptation, and application necessitates the establishment of\nclear and standardized reporting protocols, particularly within the high-stakes\ncontext of healthcare. Defining these reporting standards is inherently\nchallenging due to the diverse nature of studies involving VLMs, which vary\nsignificantly from the development of all new VLMs or finetuning for domain\nalignment to off-the-shelf use of VLM for targeted diagnosis and prediction\ntasks. In this position paper, we argue that traditional machine learning\nreporting standards and evaluation guidelines must be restructured to\naccommodate multiphase VLM studies; it also has to be organized for intuitive\nunderstanding of developers while maintaining rigorous standards for\nreproducibility. To facilitate community adoption, we propose a categorization\nframework for VLM studies and outline corresponding reporting standards that\ncomprehensively address performance evaluation, data reporting protocols, and\nrecommendations for manuscript composition. These guidelines are organized\naccording to the proposed categorization scheme. Lastly, we present a checklist\nthat consolidates reporting standards, offering a standardized tool to ensure\nconsistency and quality in the publication of VLM-related research.", "AI": {"tldr": "\u8be5\u7acb\u573a\u8bba\u6587\u4e3b\u5f20\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7814\u7a76\u5236\u5b9a\u6807\u51c6\u5316\u62a5\u544a\u534f\u8bae\uff0c\u5e76\u63d0\u51fa\u5206\u7c7b\u6846\u67b6\u548c\u68c0\u67e5\u8868\u4ee5\u786e\u4fdd\u7814\u7a76\u7684\u53ef\u91cd\u73b0\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u7531\u4e8eVLM\u7814\u7a76\u7684\u591a\u6837\u6027\u548c\u9ad8\u98ce\u9669\u533b\u7597\u73af\u5883\u7684\u9700\u6c42\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u62a5\u544a\u6807\u51c6\u9700\u8981\u8c03\u6574\u4ee5\u9002\u5e94\u591a\u9636\u6bb5VLM\u7814\u7a76\u3002", "method": "\u63d0\u51faVLM\u7814\u7a76\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u5236\u5b9a\u9488\u5bf9\u6027\u80fd\u8bc4\u4f30\u3001\u6570\u636e\u62a5\u544a\u548c\u8bba\u6587\u64b0\u5199\u7684\u6807\u51c6\u5316\u6307\u5357\uff0c\u6700\u7ec8\u6c47\u603b\u4e3a\u68c0\u67e5\u8868\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u5957\u5168\u9762\u7684VLM\u7814\u7a76\u62a5\u544a\u6807\u51c6\uff0c\u5e76\u901a\u8fc7\u5206\u7c7b\u6846\u67b6\u548c\u68c0\u67e5\u8868\u5b9e\u73b0\u4e00\u81f4\u6027\u3002", "conclusion": "\u6807\u51c6\u5316\u62a5\u544a\u534f\u8bae\u548c\u5de5\u5177\u5c06\u63d0\u5347VLM\u7814\u7a76\u7684\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u73b0\u6027\uff0c\u63a8\u52a8\u793e\u533a\u91c7\u7eb3\u3002"}}
{"id": "2505.09134", "pdf": "https://arxiv.org/pdf/2505.09134", "abs": "https://arxiv.org/abs/2505.09134", "authors": ["Daniel Huang"], "title": "Scaling Gaussian Process Regression with Full Derivative Observations", "categories": ["cs.LG", "stat.ML"], "comment": "12 pages", "summary": "We present a scalable Gaussian Process (GP) method that can fit and predict\nfull derivative observations called DSoftKI. It extends SoftKI, a method that\napproximates a kernel via softmax interpolation from learned interpolation\npoint locations, to the setting with derivatives. DSoftKI enhances SoftKI's\ninterpolation scheme to incorporate the directional orientation of\ninterpolation points relative to the data. This enables the construction of a\nscalable approximate kernel, including its first and second-order derivatives,\nthrough interpolation. We evaluate DSoftKI on a synthetic function benchmark\nand high-dimensional molecular force field prediction (100-1000 dimensions),\ndemonstrating that DSoftKI is accurate and can scale to larger datasets with\nfull derivative observations than previously possible.", "AI": {"tldr": "DSoftKI\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u9ad8\u65af\u8fc7\u7a0b\u65b9\u6cd5\uff0c\u80fd\u5904\u7406\u5e76\u9884\u6d4b\u5b8c\u6574\u5bfc\u6570\u89c2\u6d4b\uff0c\u6269\u5c55\u4e86SoftKI\u65b9\u6cd5\u4ee5\u7ed3\u5408\u5bfc\u6570\u4fe1\u606f\uff0c\u5e76\u5728\u5408\u6210\u51fd\u6570\u57fa\u51c6\u548c\u9ad8\u7ef4\u5206\u5b50\u529b\u573a\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u7684SoftKI\u65b9\u6cd5\u5728\u5904\u7406\u5bfc\u6570\u89c2\u6d4b\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u5904\u7406\u5305\u542b\u5bfc\u6570\u4fe1\u606f\u7684\u6269\u5c55\u65b9\u6cd5\u3002", "method": "DSoftKI\u901a\u8fc7\u589e\u5f3aSoftKI\u7684\u63d2\u503c\u65b9\u6848\uff0c\u5f15\u5165\u4e86\u63d2\u503c\u70b9\u76f8\u5bf9\u4e8e\u6570\u636e\u7684\u65b9\u5411\u4fe1\u606f\uff0c\u4ece\u800c\u6784\u5efa\u4e86\u5305\u542b\u4e00\u9636\u548c\u4e8c\u9636\u5bfc\u6570\u7684\u53ef\u6269\u5c55\u8fd1\u4f3c\u6838\u3002", "result": "\u5728\u5408\u6210\u51fd\u6570\u57fa\u51c6\u548c\u9ad8\u7ef4\u5206\u5b50\u529b\u573a\u9884\u6d4b\uff08100-1000\u7ef4\uff09\u4e2d\uff0cDSoftKI\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u6bd4\u4ee5\u5f80\u66f4\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6\u3002", "conclusion": "DSoftKI\u6210\u529f\u6269\u5c55\u4e86SoftKI\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u5305\u542b\u5bfc\u6570\u4fe1\u606f\u7684\u89c2\u6d4b\uff0c\u5e76\u5728\u9ad8\u7ef4\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2505.09610", "pdf": "https://arxiv.org/pdf/2505.09610", "abs": "https://arxiv.org/abs/2505.09610", "authors": ["Nicolas Dupuis", "Ravi Nair", "Shyam Ramji", "Sean McClintock", "Nishant Chauhan", "Priyanka Nagpal", "Bart Blaner", "Ken Valk", "Leon Stok", "Ruchir Puri"], "title": "Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "The use of Large Language Models (LLMs) in hardware design has taken off in\nrecent years, principally through its incorporation in tools that increase chip\ndesigner productivity. There has been considerable discussion about the use of\nLLMs in RTL specifications of chip designs, for which the two most popular\nlanguages are Verilog and VHDL. LLMs and their use in Verilog design has\nreceived significant attention due to the higher popularity of the language,\nbut little attention so far has been given to VHDL despite its continued\npopularity in the industry. There has also been little discussion about the\nunique needs of organizations that engage in high-performance processor design,\nand techniques to deploy AI solutions in these settings. In this paper, we\ndescribe our journey in developing a Large Language Model (LLM) specifically\nfor the purpose of explaining VHDL code, a task that has particular importance\nin an organization with decades of experience and assets in high-performance\nprocessor design. We show how we developed test sets specific to our needs and\nused them for evaluating models as we performed extended pretraining (EPT) of a\nbase LLM. Expert evaluation of the code explanations produced by the EPT model\nincreased to 69% compared to a base model rating of 43%. We further show how we\ndeveloped an LLM-as-a-judge to gauge models similar to expert evaluators. This\nled us to deriving and evaluating a host of new models, including an\ninstruction-tuned version of the EPT model with an expected expert evaluator\nrating of 71%. Our experiments also indicate that with the potential use of\nnewer base models, this rating can be pushed to 85% and beyond. We conclude\nwith a discussion on further improving the quality of hardware design LLMs\nusing exciting new developments in the Generative AI world.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e13\u95e8\u7528\u4e8e\u89e3\u91caVHDL\u4ee3\u7801\u7684LLM\u5f00\u53d1\uff0c\u901a\u8fc7\u6269\u5c55\u9884\u8bad\u7ec3\u548c\u4e13\u5bb6\u8bc4\u4f30\uff0c\u5c06\u4ee3\u7801\u89e3\u91ca\u51c6\u786e\u7387\u4ece43%\u63d0\u5347\u81f369%\uff0c\u5e76\u8fdb\u4e00\u6b65\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u8fbe\u523071%\u3002", "motivation": "\u5c3d\u7ba1Verilog\u5728\u82af\u7247\u8bbe\u8ba1\u4e2d\u66f4\u53d7\u5173\u6ce8\uff0c\u4f46VHDL\u5728\u884c\u4e1a\u4e2d\u4ecd\u5e7f\u6cdb\u4f7f\u7528\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u6027\u80fd\u5904\u7406\u5668\u8bbe\u8ba1\u4e2d\u3002\u73b0\u6709\u7814\u7a76\u5bf9VHDL\u548c\u6b64\u7c7b\u7ec4\u7ec7\u7684\u72ec\u7279\u9700\u6c42\u5173\u6ce8\u4e0d\u8db3\uff0c\u56e0\u6b64\u5f00\u53d1\u4e13\u95e8\u7684LLM\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u9884\u8bad\u7ec3\uff08EPT\uff09\u57fa\u7840LLM\uff0c\u5f00\u53d1\u7279\u5b9a\u7684\u6d4b\u8bd5\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5f15\u5165LLM-as-a-judge\u65b9\u6cd5\uff0c\u6a21\u62df\u4e13\u5bb6\u8bc4\u4f30\u4ee5\u4f18\u5316\u6a21\u578b\u3002", "result": "EPT\u6a21\u578b\u5c06\u4e13\u5bb6\u8bc4\u4f30\u7684\u51c6\u786e\u7387\u4ece\u57fa\u7840\u6a21\u578b\u768443%\u63d0\u5347\u81f369%\uff0c\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u8fdb\u4e00\u6b65\u8fbe\u523071%\uff0c\u9884\u8ba1\u4f7f\u7528\u66f4\u5148\u8fdb\u7684\u57fa\u6a21\u578b\u53ef\u63d0\u5347\u81f385%\u4ee5\u4e0a\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u6a21\u578b\u4f18\u5316\u548c\u751f\u6210AI\u7684\u65b0\u8fdb\u5c55\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u786c\u4ef6\u8bbe\u8ba1LLM\u7684\u8d28\u91cf\u3002"}}
{"id": "2505.08821", "pdf": "https://arxiv.org/pdf/2505.08821", "abs": "https://arxiv.org/abs/2505.08821", "authors": ["Meryem Altin Karagoz", "Marc D. Breton", "Anas El Fathi"], "title": "A Comparative Study of Transformer-Based Models for Multi-Horizon Blood Glucose Prediction", "categories": ["q-bio.QM", "cs.AI", "stat.AP"], "comment": "7 pages, 2 figures, 1 table, 1st IFAC Workshop on Engineering\n  Diabetes Technologies (EDT 2025)", "summary": "Accurate blood glucose prediction can enable novel interventions for type 1\ndiabetes treatment, including personalized insulin and dietary adjustments.\nAlthough recent advances in transformer-based architectures have demonstrated\nthe power of attention mechanisms in complex multivariate time series\nprediction, their potential for blood glucose (BG) prediction remains\nunderexplored. We present a comparative analysis of transformer models for\nmulti-horizon BG prediction, examining forecasts up to 4 hours and input\nhistory up to 1 week. The publicly available DCLP3 dataset (n=112) was split\n(80%-10%-10%) for training, validation, and testing, and the OhioT1DM dataset\n(n=12) served as an external test set. We trained networks with point-wise,\npatch-wise, series-wise, and hybrid embeddings, using CGM, insulin, and meal\ndata. For short-term blood glucose prediction, Crossformer, a patch-wise\ntransformer architecture, achieved a superior 30-minute prediction of RMSE\n(15.6 mg / dL on OhioT1DM). For longer-term predictions (1h, 2h, and 4h),\nPatchTST, another path-wise transformer, prevailed with the lowest RMSE (24.6\nmg/dL, 36.1 mg/dL, and 46.5 mg/dL on OhioT1DM). In general, models that used\ntokenization through patches demonstrated improved accuracy with larger input\nsizes, with the best results obtained with a one-week history. These findings\nhighlight the promise of transformer-based architectures for BG prediction by\ncapturing and leveraging seasonal patterns in multivariate time-series data to\nimprove accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u5728\u8840\u7cd6\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4e0d\u540c\u5d4c\u5165\u65b9\u5f0f\u7684\u6a21\u578b\u6027\u80fd\uff0c\u53d1\u73b0Patch-wise Transformer\u5728\u77ed\u671f\u548c\u957f\u671f\u9884\u6d4b\u4e2d\u5747\u8868\u73b0\u6700\u4f18\uff0c\u5c24\u5176\u662f\u4f7f\u7528\u4e00\u5468\u5386\u53f2\u6570\u636e\u65f6\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u51c6\u786e\u7684\u8840\u7cd6\u9884\u6d4b\u80fd\u4e3a1\u578b\u7cd6\u5c3f\u75c5\u6cbb\u7597\u63d0\u4f9b\u4e2a\u6027\u5316\u5e72\u9884\u63aa\u65bd\uff0c\u5982\u80f0\u5c9b\u7d20\u548c\u996e\u98df\u8c03\u6574\u3002\u5c3d\u7ba1Transformer\u67b6\u6784\u5728\u590d\u6742\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5c55\u73b0\u4e86\u5f3a\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u8840\u7cd6\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cdTransformer\u6a21\u578b\u5728\u591a\u65f6\u95f4\u8303\u56f4\u5185\u7684\u8840\u7cd6\u9884\u6d4b\u6027\u80fd\uff0c\u4f7f\u7528\u4e86DCLP3\u548cOhioT1DM\u6570\u636e\u96c6\uff0c\u5e76\u6d4b\u8bd5\u4e86\u70b9\u72b6\u3001\u7247\u6bb5\u72b6\u3001\u5e8f\u5217\u72b6\u53ca\u6df7\u5408\u5d4c\u5165\u65b9\u5f0f\u3002", "result": "Crossformer\u572830\u5206\u949f\u9884\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff08RMSE 15.6 mg/dL\uff09\uff0c\u800cPatchTST\u5728\u957f\u671f\u9884\u6d4b\uff081h\u30012h\u30014h\uff09\u4e2d\u8868\u73b0\u6700\u4f18\uff08RMSE\u5206\u522b\u4e3a24.6\u300136.1\u300146.5 mg/dL\uff09\u3002\u7247\u6bb5\u5316\u5d4c\u5165\u65b9\u6cd5\u5728\u589e\u52a0\u8f93\u5165\u6570\u636e\u91cf\u65f6\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u80fd\u6709\u6548\u6355\u6349\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u5468\u671f\u6027\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u8840\u7cd6\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u7ed3\u5408\u8f83\u957f\u7684\u5386\u53f2\u6570\u636e\u65f6\u8868\u73b0\u66f4\u4e3a\u7a81\u51fa\u3002"}}
{"id": "2505.09160", "pdf": "https://arxiv.org/pdf/2505.09160", "abs": "https://arxiv.org/abs/2505.09160", "authors": ["Berkay Guler", "Giovanni Geraci", "Hamid Jafarkhani"], "title": "A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Current applications of self-supervised learning to wireless channel\nrepresentation often borrow paradigms developed for text and image processing,\nwithout fully addressing the unique characteristics and constraints of wireless\ncommunications. Aiming to fill this gap, we first propose WiMAE (Wireless\nMasked Autoencoder), a transformer-based encoder-decoder foundation model\npretrained on a realistic open-source multi-antenna wireless channel dataset.\nBuilding upon this foundation, we develop ContraWiMAE, which enhances WiMAE by\nincorporating a contrastive learning objective alongside the reconstruction\ntask in a unified multi-task framework. By warm-starting from pretrained WiMAE\nweights and generating positive pairs via noise injection, the contrastive\ncomponent enables the model to capture both structural and discriminative\nfeatures, enhancing representation quality beyond what reconstruction alone can\nachieve. Through extensive evaluation on unseen scenarios, we demonstrate the\neffectiveness of both approaches across multiple downstream tasks, with\nContraWiMAE showing further improvements in linear separability and\nadaptability in diverse wireless environments. Comparative evaluations against\na state-of-the-art wireless channel foundation model confirm the superior\nperformance and data efficiency of our models, highlighting their potential as\npowerful baselines for future research in self-supervised wireless channel\nrepresentation learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWiMAE\u7684\u65e0\u7ebf\u901a\u9053\u8868\u793a\u81ea\u76d1\u7763\u5b66\u4e60\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u4e00\u6b65\u4f18\u5316\u4e3aContraWiMAE\uff0c\u5176\u5728\u591a\u6837\u5316\u7684\u65e0\u7ebf\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u65e0\u7ebf\u901a\u9053\u8868\u793a\u4e2d\u7684\u5e94\u7528\u5f80\u5f80\u501f\u9274\u6587\u672c\u548c\u56fe\u50cf\u5904\u7406\u7684\u8303\u5f0f\uff0c\u672a\u80fd\u5145\u5206\u89e3\u51b3\u65e0\u7ebf\u901a\u4fe1\u7684\u72ec\u7279\u7279\u6027\u548c\u9650\u5236\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u9996\u5148\u63d0\u51fa\u57fa\u4e8eTransformer\u7684WiMAE\u6a21\u578b\uff0c\u9884\u8bad\u7ec3\u4e8e\u591a\u5929\u7ebf\u65e0\u7ebf\u901a\u9053\u6570\u636e\u96c6\uff1b\u968f\u540e\u5f00\u53d1ContraWiMAE\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u91cd\u5efa\u4efb\u52a1\uff0c\u751f\u6210\u6b63\u6837\u672c\u5bf9\u4ee5\u63d0\u5347\u8868\u793a\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cWiMAE\u548cContraWiMAE\u5728\u672a\u89c1\u8fc7\u7684\u573a\u666f\u4e2d\u5747\u6709\u6548\uff0c\u4e14ContraWiMAE\u5728\u7ebf\u6027\u53ef\u5206\u79bb\u6027\u548c\u9002\u5e94\u591a\u6837\u6027\u73af\u5883\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "WiMAE\u548cContraWiMAE\u5728\u81ea\u76d1\u7763\u65e0\u7ebf\u901a\u9053\u8868\u793a\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u53ef\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u7684\u5f3a\u5927\u57fa\u7ebf\u3002"}}
{"id": "2505.09174", "pdf": "https://arxiv.org/pdf/2505.09174", "abs": "https://arxiv.org/abs/2505.09174", "authors": ["Xinyu You", "Xiang Liu", "Chuan-Shen Hu", "Kelin Xia", "Tze Chien Sum"], "title": "Quotient Complex Transformer (QCformer) for Perovskite Data Analysis", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "comment": null, "summary": "The discovery of novel functional materials is crucial in addressing the\nchallenges of sustainable energy generation and climate change. Hybrid\norganic-inorganic perovskites (HOIPs) have gained attention for their\nexceptional optoelectronic properties in photovoltaics. Recently, geometric\ndeep learning, particularly graph neural networks (GNNs), has shown strong\npotential in predicting material properties and guiding material design.\nHowever, traditional GNNs often struggle to capture the periodic structures and\nhigher-order interactions prevalent in such systems. To address these\nlimitations, we propose a novel representation based on quotient complexes\n(QCs) and introduce the Quotient Complex Transformer (QCformer) for material\nproperty prediction. A material structure is modeled as a quotient complex,\nwhich encodes both pairwise and many-body interactions via simplices of varying\ndimensions and captures material periodicity through a quotient operation. Our\nmodel leverages higher-order features defined on simplices and processes them\nusing a simplex-based Transformer module. We pretrain QCformer on benchmark\ndatasets such as the Materials Project and JARVIS, and fine-tune it on HOIP\ndatasets. The results show that QCformer outperforms state-of-the-art models in\nHOIP property prediction, demonstrating its effectiveness. The quotient complex\nrepresentation and QCformer model together contribute a powerful new tool for\npredictive modeling of perovskite materials.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5546\u590d\u5f62\uff08QC\uff09\u7684\u65b0\u8868\u793a\u65b9\u6cd5\u548cQuotient Complex Transformer\uff08QCformer\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u6742\u5316\u6709\u673a-\u65e0\u673a\u9499\u949b\u77ff\uff08HOIPs\uff09\u7684\u6750\u6599\u6027\u8d28\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5468\u671f\u6027\u7ed3\u6784\u548c\u9ad8\u9636\u76f8\u4e92\u4f5c\u7528\u5efa\u6a21\u4e2d\u7684\u4e0d\u8db3\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6742\u5316\u6709\u673a-\u65e0\u673a\u9499\u949b\u77ff\uff08HOIPs\uff09\u56e0\u5176\u4f18\u5f02\u7684\u5149\u7535\u6027\u80fd\u5728\u5149\u4f0f\u9886\u57df\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u5176\u5468\u671f\u6027\u7ed3\u6784\u548c\u9ad8\u9636\u76f8\u4e92\u4f5c\u7528\u4f7f\u4f20\u7edf\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u5176\u6027\u8d28\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u5546\u590d\u5f62\u8868\u793a\uff08QC\uff09\u548cQCformer\u6a21\u578b\uff0c\u5c06\u6750\u6599\u7ed3\u6784\u5efa\u6a21\u4e3a\u5546\u590d\u5f62\uff0c\u901a\u8fc7\u4e0d\u540c\u7ef4\u5ea6\u7684\u5355\u80de\u7f16\u7801\u591a\u4f53\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u5229\u7528\u5546\u8fd0\u7b97\u6355\u6349\u5468\u671f\u6027\u3002\u6a21\u578b\u57fa\u4e8e\u5355\u80de\u7684\u9ad8\u9636\u7279\u5f81\uff0c\u901a\u8fc7Transformer\u6a21\u5757\u5904\u7406\u6570\u636e\uff0c\u5e76\u5728Materials Project\u548cJARVIS\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\uff0c\u5728HOIP\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u3002", "result": "QCformer\u5728HOIP\u6027\u8d28\u9884\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "\u5546\u590d\u5f62\u8868\u793a\u4e0eQCformer\u4e3a\u9499\u949b\u77ff\u6750\u6599\u7684\u9884\u6d4b\u5efa\u6a21\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u529f\u80fd\u6750\u6599\u7684\u8bbe\u8ba1\u4e0e\u53d1\u73b0\u3002"}}
{"id": "2505.08825", "pdf": "https://arxiv.org/pdf/2505.08825", "abs": "https://arxiv.org/abs/2505.08825", "authors": ["Pedro Antonio Alarcon Granadeno", "Theodore Chambers", "Jane Cleland-Huang"], "title": "Multi-source Plume Tracing via Multi-Agent Reinforcement Learning", "categories": ["cs.MA", "cs.AI"], "comment": "13 pages, 7 figures", "summary": "Industrial catastrophes like the Bhopal disaster (1984) and the Aliso Canyon\ngas leak (2015) demonstrate the urgent need for rapid and reliable plume\ntracing algorithms to protect public health and the environment. Traditional\nmethods, such as gradient-based or biologically inspired approaches, often fail\nin realistic, turbulent conditions. To address these challenges, we present a\nMulti-Agent Reinforcement Learning (MARL) algorithm designed for localizing\nmultiple airborne pollution sources using a swarm of small uncrewed aerial\nsystems (sUAS). Our method models the problem as a Partially Observable Markov\nGame (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific\nDouble Deep Recurrent Q-Network (ADDRQN) that uses full sequences of historical\naction-observation pairs, effectively approximating latent states. Unlike prior\nwork, we use a general-purpose simulation environment based on the Gaussian\nPlume Model (GPM), incorporating realistic elements such as a three-dimensional\nenvironment, sensor noise, multiple interacting agents, and multiple plume\nsources. The incorporation of action histories as part of the inputs further\nenhances the adaptability of our model in complex, partially observable\nenvironments. Extensive simulations show that our algorithm significantly\noutperforms conventional approaches. Specifically, our model allows agents to\nexplore only 1.29\\% of the environment to successfully locate pollution\nsources.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5feb\u901f\u5b9a\u4f4d\u7a7a\u6c14\u4e2d\u7684\u6c61\u67d3\u6e90\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u6e4d\u6d41\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5de5\u4e1a\u707e\u96be\u5982\u535a\u5e15\u5c14\u4e8b\u6545\uff081984\uff09\u548c\u963f\u5229\u7d22\u5ce1\u8c37\u5929\u7136\u6c14\u6cc4\u6f0f\uff082015\uff09\u66b4\u9732\u4e86\u73b0\u6709\u6c61\u67d3\u6e90\u5b9a\u4f4d\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u4e9f\u9700\u4e00\u79cd\u5728\u6e4d\u6d41\u7b49\u590d\u6742\u73af\u5883\u4e0b\u4ecd\u80fd\u5feb\u901f\u53ef\u9760\u5de5\u4f5c\u7684\u7b97\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u548c\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\uff08POMG\uff09\u6846\u67b6\uff0c\u7ed3\u5408LSTM\u7f51\u7edc\u548c\u52a8\u4f5c\u7279\u5b9a\u7684\u53cc\u6df1\u5ea6\u9012\u5f52Q\u7f51\u7edc\uff08ADDRQN\uff09\uff0c\u5229\u7528\u5386\u53f2\u52a8\u4f5c-\u89c2\u5bdf\u5e8f\u5217\u6a21\u62df\u6f5c\u5728\u72b6\u6001\uff0c\u5e76\u5728\u57fa\u4e8e\u9ad8\u65af\u7fbd\u6d41\u6a21\u578b\uff08GPM\uff09\u7684\u4eff\u771f\u73af\u5883\u4e2d\u9a8c\u8bc1\u3002", "result": "\u7b97\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4ec5\u9700\u63a2\u7d22\u73af\u5883\u76841.29%\u5373\u53ef\u6210\u529f\u5b9a\u4f4d\u6c61\u67d3\u6e90\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684MARL\u7b97\u6cd5\u5728\u590d\u6742\u3001\u90e8\u5206\u53ef\u89c2\u5bdf\u7684\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u6c61\u67d3\u6e90\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09175", "pdf": "https://arxiv.org/pdf/2505.09175", "abs": "https://arxiv.org/abs/2505.09175", "authors": ["Mohammad Ganjirad", "Mahmoud Reza Delavar", "Hossein Bagheri", "Mohammad Mehdi Azizi"], "title": "Optimizing Urban Critical Green Space Development Using Machine Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "This paper presents a novel framework for prioritizing urban green space\ndevelopment in Tehran using diverse socio-economic, environmental, and\nsensitivity indices. The indices were derived from various sources including\nGoogle Earth Engine, air pollution measurements, municipal reports and the\nWeather Research & Forecasting (WRF) model. The WRF model was used to estimate\nthe air temperature at a 1 km resolution due to insufficient meteorological\nstations, yielding RMSE and MAE values of 0.96{\\deg}C and 0.92{\\deg}C,\nrespectively. After data preparation, several machine learning models were used\nfor binary vegetation cover classification including XGBoost, LightGBM, Random\nForest (RF) and Extra Trees. RF achieved the highest performance, exceeding 94%\nin Overall Accuracy, Recall, and F1-score. Then, the probability of areas\nlacking vegetation cover was assessed using socio-economic, environmental and\nsensitivity indices. This resulted in the RF generating an urban green space\ndevelopment prioritization map. Feature Importance Analysis revealed that the\nmost significant indices were nightly land surface temperature (LST) and\nsensitive population. Finally, the framework performance was validated through\nmicroclimate simulation to assess the critical areas after and before the green\nspace development by green roofs. The simulation demonstrated reducing air\ntemperature by up to 0.67{\\deg}C after utilizing the green roof technology in\ncritical areas. As a result, this framework provides a valuable tool for urban\nplanners to develop green spaces.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528\u591a\u79cd\u793e\u4f1a\u7ecf\u6d4e\u3001\u73af\u5883\u4e0e\u654f\u611f\u6307\u6570\u6765\u4f18\u5148\u53d1\u5c55\u5fb7\u9ed1\u5170\u57ce\u5e02\u7eff\u5730\u7684\u6846\u67b6\u3002\u7ed3\u5408\u591a\u79cd\u6570\u636e\u6765\u6e90\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u968f\u673a\u68ee\u6797\u8868\u73b0\u6700\u4f73\uff0c\u751f\u6210\u7684\u7eff\u5730\u5f00\u53d1\u4f18\u5148\u7ea7\u5730\u56fe\u7ecf\u8fc7\u5fae\u6c14\u5019\u6a21\u62df\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u7eff\u8272\u5c4b\u9876\u6280\u672f\u7684\u964d\u6e29\u6548\u679c\u3002", "motivation": "\u57ce\u5e02\u5316\u8fdb\u7a0b\u4e2d\u7f3a\u4e4f\u7eff\u5730\u89c4\u5212\u7684\u79d1\u5b66\u4f9d\u636e\uff0c\u9700\u7ed3\u5408\u591a\u6e90\u6570\u636e\u4e0e\u673a\u5668\u5b66\u4e60\u6765\u4f18\u5316\u57ce\u5e02\u7eff\u5730\u53d1\u5c55\u7b56\u7565\uff0c\u4ee5\u6539\u5584\u73af\u5883\u4e0e\u751f\u6d3b\u8d28\u91cf\u3002", "method": "\u5229\u7528Google Earth Engine\u3001WRF\u6a21\u578b\u7b49\u6570\u636e\u6e90\u6784\u5efa\u6307\u6570\uff0c\u91c7\u7528XGBoost\u3001LightGBM\u3001\u968f\u673a\u68ee\u6797\u7b49\u6a21\u578b\u5206\u7c7b\u690d\u88ab\u8986\u76d6\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u786e\u5b9a\u5173\u952e\u6307\u6807\uff08\u5982\u591c\u95f4\u5730\u8868\u6e29\u5ea6\u4e0e\u654f\u611f\u4eba\u53e3\uff09\u3002", "result": "\u968f\u673a\u68ee\u6797\u5206\u7c7b\u7cbe\u5ea6\u8d8594%\uff0c\u7eff\u5730\u4f18\u5148\u7ea7\u5730\u56fe\u7ecf\u6a21\u62df\u9a8c\u8bc1\u53ef\u4f7f\u7a7a\u6c14\u6e29\u5ea6\u964d\u4f4e0.67\u00b0C\uff0c\u591c\u95f4\u5730\u8868\u6e29\u5ea6\u4e0e\u654f\u611f\u4eba\u53e3\u4e3a\u6700\u663e\u8457\u5f71\u54cd\u56e0\u7d20\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u57ce\u5e02\u89c4\u5212\u8005\u63d0\u4f9b\u4e86\u79d1\u5b66\u5de5\u5177\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u7eff\u5730\u5f00\u53d1\u4f18\u5148\u7ea7\u5e76\u9a8c\u8bc1\u73af\u5883\u6548\u76ca\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u5730\u533a\u3002"}}
{"id": "2505.09214", "pdf": "https://arxiv.org/pdf/2505.09214", "abs": "https://arxiv.org/abs/2505.09214", "authors": ["Zhonghao Lyu", "Ming Xiao", "Jie Xu", "Mikael Skoglund", "Marco Di Renzo"], "title": "The Larger the Merrier? Efficient Large AI Model Inference in Wireless Edge Networks", "categories": ["cs.LG"], "comment": null, "summary": "The growing demand for large artificial intelligence model (LAIM) services is\ndriving a paradigm shift from traditional cloud-based inference to edge-based\ninference for low-latency, privacy-preserving applications. In particular,\nedge-device co-inference, which partitions LAIMs between edge devices and\nservers, has emerged as a promising strategy for resource-efficient LAIM\nexecution in wireless networks. In this paper, we investigate a pruning-aware\nLAIM co-inference scheme, where a pre-trained LAIM is pruned and partitioned\ninto on-device and on-server sub-models for deployment. For analysis, we first\nprove that the LAIM output distortion is upper bounded by its parameter\ndistortion. Then, we derive a lower bound on parameter distortion via\nrate-distortion theory, analytically capturing the relationship between pruning\nratio and co-inference performance. Next, based on the analytical results, we\nformulate an LAIM co-inference distortion bound minimization problem by jointly\noptimizing the pruning ratio, transmit power, and computation frequency under\nsystem latency, energy, and available resource constraints. Moreover, we\npropose an efficient algorithm to tackle the considered highly non-convex\nproblem. Finally, extensive simulations demonstrate the effectiveness of the\nproposed design. In particular, model parameter distortion is shown to provide\na reliable bound on output distortion. Also, the proposed joint pruning ratio\nand resource management design achieves superior performance in balancing\ntrade-offs among inference performance, system latency, and energy consumption\ncompared with benchmark schemes, such as fully on-device and on-server\ninference. Moreover, the split point is shown to play a critical role in system\nperformance optimization under heterogeneous and resource-limited edge\nenvironments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fb9\u7f18\u8bbe\u5907\u4e0e\u670d\u52a1\u5668\u534f\u540c\u63a8\u7406\u7684\u5927\u578b\u4eba\u5de5\u667a\u80fd\u6a21\u578b\uff08LAIM\uff09\u526a\u88c1\u4e0e\u5206\u533a\u65b9\u6848\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u526a\u88c1\u6bd4\u4f8b\u3001\u4f20\u8f93\u529f\u7387\u548c\u8ba1\u7b97\u9891\u7387\u6765\u964d\u4f4e\u63a8\u7406\u5931\u771f\uff0c\u5e76\u5728\u8d44\u6e90\u6709\u9650\u7684\u8fb9\u7f18\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u4f18\u52bf\u3002", "motivation": "\u968f\u7740\u5bf9\u5927\u578b\u4eba\u5de5\u667a\u80fd\u6a21\u578b\uff08LAIM\uff09\u670d\u52a1\u7684\u9700\u6c42\u589e\u957f\uff0c\u8fb9\u7f18\u8bbe\u5907\u4e0e\u670d\u52a1\u5668\u534f\u540c\u63a8\u7406\u6210\u4e3a\u4e00\u79cd\u4f4e\u5ef6\u8fdf\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u8d44\u6e90\u9ad8\u6548\u7b56\u7565\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u526a\u88c1\u548c\u5206\u533aLAIM\u6765\u4f18\u5316\u63a8\u7406\u6027\u80fd\u548c\u7cfb\u7edf\u8d44\u6e90\u5206\u914d\u3002", "method": "\u8bba\u6587\u9996\u5148\u8bc1\u660eLAIM\u8f93\u51fa\u5931\u771f\u7531\u5176\u53c2\u6570\u5931\u771f\u4e0a\u9650\u754c\u5b9a\uff0c\u5e76\u901a\u8fc7\u7387\u5931\u771f\u7406\u8bba\u63a8\u5bfc\u53c2\u6570\u5931\u771f\u4e0b\u9650\u3002\u968f\u540e\uff0c\u57fa\u4e8e\u5206\u6790\u7ed3\u679c\uff0c\u8054\u5408\u4f18\u5316\u526a\u88c1\u6bd4\u4f8b\u3001\u4f20\u8f93\u529f\u7387\u548c\u8ba1\u7b97\u9891\u7387\uff0c\u63d0\u51fa\u9ad8\u6548\u7b97\u6cd5\u89e3\u51b3\u975e\u51f8\u95ee\u9898\u3002", "result": "\u4eff\u771f\u663e\u793a\u6a21\u578b\u53c2\u6570\u5931\u771f\u80fd\u53ef\u9760\u754c\u5b9a\u8f93\u51fa\u5931\u771f\uff0c\u4e14\u63d0\u51fa\u7684\u8054\u5408\u4f18\u5316\u8bbe\u8ba1\u5728\u63a8\u7406\u6027\u80fd\u3001\u7cfb\u7edf\u5ef6\u8fdf\u548c\u80fd\u8017\u65b9\u9762\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\uff08\u5b8c\u5168\u5728\u8bbe\u5907\u6216\u670d\u52a1\u5668\u63a8\u7406\uff09\u3002\u5206\u754c\u70b9\u5728\u5f02\u6784\u8d44\u6e90\u6709\u9650\u7684\u8fb9\u7f18\u73af\u5883\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u526a\u88c1\u611f\u77e5\u534f\u540c\u63a8\u7406\u65b9\u6848\u6709\u6548\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u8d44\u6e90\u5229\u7528\uff0c\u4e3a\u8fb9\u7f18\u73af\u5883\u4e2d\u7684LAIM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2505.09218", "pdf": "https://arxiv.org/pdf/2505.09218", "abs": "https://arxiv.org/abs/2505.09218", "authors": ["Alexander Tyurin", "Danil Sivtsov"], "title": "Birch SGD: A Tree Graph Framework for Local and Asynchronous SGD Methods", "categories": ["cs.LG", "cs.DC", "math.OC"], "comment": null, "summary": "We propose a new unifying framework, Birch SGD, for analyzing and designing\ndistributed SGD methods. The central idea is to represent each method as a\nweighted directed tree, referred to as a computation tree. Leveraging this\nrepresentation, we introduce a general theoretical result that reduces\nconvergence analysis to studying the geometry of these trees. This perspective\nyields a purely graph-based interpretation of optimization dynamics, offering a\nnew and intuitive foundation for method development. Using Birch SGD, we design\neight new methods and analyze them alongside previously known ones, with at\nleast six of the new methods shown to have optimal computational time\ncomplexity. Our research leads to two key insights: (i) all methods share the\nsame \"iteration rate\" of $O\\left(\\frac{(R + 1) L \\Delta}{\\varepsilon} +\n\\frac{\\sigma^2 L \\Delta}{\\varepsilon^2}\\right)$, where $R$ the maximum \"tree\ndistance\" along the main branch of a tree; and (ii) different methods exhibit\ndifferent trade-offs-for example, some update iterates more frequently,\nimproving practical performance, while others are more communication-efficient\nor focus on other aspects. Birch SGD serves as a unifying framework for\nnavigating these trade-offs. We believe these results provide a unified\nfoundation for understanding, analyzing, and designing efficient asynchronous\nand parallel optimization methods.", "AI": {"tldr": "Birch SGD \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u8868\u793a\u5206\u5e03\u5f0f SGD \u65b9\u6cd5\u4e3a\u8ba1\u7b97\u6811\uff0c\u7b80\u5316\u6536\u655b\u5206\u6790\u5e76\u8bbe\u8ba1\u4e86\u516b\u79cd\u65b0\u65b9\u6cd5\uff0c\u5176\u4e2d\u516d\u79cd\u5177\u6709\u6700\u4f18\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u65e8\u5728\u4e3a\u5206\u5e03\u5f0f SGD \u65b9\u6cd5\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7b80\u5316\u6536\u655b\u5206\u6790\u5e76\u4fc3\u8fdb\u65b0\u65b9\u6cd5\u7684\u8bbe\u8ba1\uff0c\u4ee5\u4f18\u5316\u8ba1\u7b97\u548c\u901a\u4fe1\u6548\u7387\u3002", "method": "\u5c06\u6bcf\u79cd\u65b9\u6cd5\u8868\u793a\u4e3a\u52a0\u6743\u6709\u5411\u6811\uff08\u8ba1\u7b97\u6811\uff09\uff0c\u5229\u7528\u8fd9\u4e00\u8868\u793a\u7b80\u5316\u6536\u655b\u5206\u6790\uff0c\u5e76\u57fa\u4e8e\u6b64\u6846\u67b6\u8bbe\u8ba1\u65b0\u65b9\u6cd5\u3002", "result": "\u8bbe\u8ba1\u4e86\u516b\u79cd\u65b0\u65b9\u6cd5\uff0c\u5176\u4e2d\u516d\u79cd\u5177\u6709\u6700\u4f18\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u63ed\u793a\u4e86\u6240\u6709\u65b9\u6cd5\u5171\u4eab\u76f8\u540c\u7684\u8fed\u4ee3\u7387\uff0c\u4f46\u5b58\u5728\u4e0d\u540c\u7684\u6743\u8861\uff08\u5982\u8ba1\u7b97\u9891\u7387\u4e0e\u901a\u4fe1\u6548\u7387\uff09\u3002", "conclusion": "Birch SGD \u4e3a\u7406\u89e3\u548c\u8bbe\u8ba1\u9ad8\u6548\u7684\u5f02\u6b65\u548c\u5e76\u884c\u4f18\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002"}}
{"id": "2505.09239", "pdf": "https://arxiv.org/pdf/2505.09239", "abs": "https://arxiv.org/abs/2505.09239", "authors": ["Faruk Alpay"], "title": "Stable and Convexified Information Bottleneck Optimization via Symbolic Continuation and Entropy-Regularized Trajectories", "categories": ["cs.LG", "68T05, 90C25, 94A15", "I.2.6; G.1.6; H.1.1"], "comment": "23 pages, 11 figures, includes analytical proofs, sensitivity\n  analysis (95% CI), and JAX-based open-source implementation available at:\n  https://github.com/farukalpay/information-bottleneck-beta-optimization", "summary": "The Information Bottleneck (IB) method frequently suffers from unstable\noptimization, characterized by abrupt representation shifts near critical\npoints of the IB trade-off parameter, beta. In this paper, I introduce a novel\napproach to achieve stable and convex IB optimization through symbolic\ncontinuation and entropy-regularized trajectories. I analytically prove\nconvexity and uniqueness of the IB solution path when an entropy regularization\nterm is included, and demonstrate how this stabilizes representation learning\nacross a wide range of \\b{eta} values. Additionally, I provide extensive\nsensitivity analyses around critical points (beta) with statistically robust\nuncertainty quantification (95% confidence intervals). The open-source\nimplementation, experimental results, and reproducibility framework included in\nthis work offer a clear path for practical deployment and future extension of\nmy proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7b26\u53f7\u5ef6\u7eed\u548c\u71b5\u6b63\u5219\u5316\u8f68\u8ff9\u5b9e\u73b0\u7a33\u5b9a\u4e14\u51f8\u7684\u4fe1\u606f\u74f6\u9888\uff08IB\uff09\u4f18\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86IB\u65b9\u6cd5\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u4e0d\u7a33\u5b9a\u548c\u8868\u793a\u7a81\u53d8\u7684\u95ee\u9898\u3002", "motivation": "\u4fe1\u606f\u74f6\u9888\u65b9\u6cd5\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5e38\u56e0IB\u6743\u8861\u53c2\u6570\u03b2\u7684\u4e34\u754c\u70b9\u9644\u8fd1\u8868\u793a\u7a81\u53d8\u800c\u8868\u73b0\u51fa\u4e0d\u7a33\u5b9a\u6027\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u7b26\u53f7\u5ef6\u7eed\u548c\u71b5\u6b63\u5219\u5316\u8f68\u8ff9\uff0c\u5305\u542b\u71b5\u6b63\u5219\u5316\u9879\u4ee5\u5206\u6790\u8bc1\u660eIB\u89e3\u8def\u5f84\u7684\u51f8\u6027\u548c\u552f\u4e00\u6027\uff0c\u5e76\u901a\u8fc7\u654f\u611f\u6027\u5206\u6790\u548c\u7edf\u8ba1\u9c81\u68d2\u6027\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6765\u7a33\u5b9a\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u5e7f\u6cdb\u7684\u03b2\u503c\u8303\u56f4\u5185\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u8868\u793a\u5b66\u4e60\uff0c\u5e76\u63d0\u4f9b\u4e86\u5177\u670995%\u7f6e\u4fe1\u533a\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u89e3\u51b3\u4e86IB\u4f18\u5316\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5f00\u6e90\u5b9e\u73b0\u548c\u53ef\u590d\u73b0\u6027\u6846\u67b6\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u548c\u672a\u6765\u6269\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.08830", "pdf": "https://arxiv.org/pdf/2505.08830", "abs": "https://arxiv.org/abs/2505.08830", "authors": ["Wenhao Jiang", "Yuchuan Luo", "Guilin Deng", "Silong Chen", "Xu Yang", "Shihong Wu", "Xinwen Gao", "Lin Liu", "Shaojing Fu"], "title": "Federated Large Language Models: Feasibility, Robustness, Security and Future Directions", "categories": ["cs.CR", "cs.AI"], "comment": "35 pages", "summary": "The integration of Large Language Models (LLMs) and Federated Learning (FL)\npresents a promising solution for joint training on distributed data while\npreserving privacy and addressing data silo issues. However, this emerging\nfield, known as Federated Large Language Models (FLLM), faces significant\nchallenges, including communication and computation overheads, heterogeneity,\nprivacy and security concerns. Current research has primarily focused on the\nfeasibility of FLLM, but future trends are expected to emphasize enhancing\nsystem robustness and security. This paper provides a comprehensive review of\nthe latest advancements in FLLM, examining challenges from four critical\nperspectives: feasibility, robustness, security, and future directions. We\npresent an exhaustive survey of existing studies on FLLM feasibility, introduce\nmethods to enhance robustness in the face of resource, data, and task\nheterogeneity, and analyze novel risks associated with this integration,\nincluding privacy threats and security challenges. We also review the latest\ndevelopments in defense mechanisms and explore promising future research\ndirections, such as few-shot learning, machine unlearning, and IP protection.\nThis survey highlights the pressing need for further research to enhance system\nrobustness and security while addressing the unique challenges posed by the\nintegration of FL and LLM.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8054\u90a6\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08FLLM\uff09\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u805a\u7126\u5176\u53ef\u884c\u6027\u3001\u9c81\u68d2\u6027\u3001\u5b89\u5168\u6027\u53ca\u672a\u6765\u65b9\u5411\u3002\u5185\u5bb9\u6db5\u76d6\u73b0\u6709\u7814\u7a76\u7684\u5168\u9762\u8c03\u67e5\u3001\u63d0\u5347\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3001\u9690\u79c1\u4e0e\u5b89\u5168\u98ce\u9669\u7684\u8bc6\u522b\u4e0e\u9632\u5fa1\u673a\u5236\u63a2\u8ba8\uff0c\u4ee5\u53ca\u8bf8\u5982\u5c11\u6837\u672c\u5b66\u4e60\u3001\u673a\u5668\u53cd\u5b66\u4e60\u548cIP\u4fdd\u62a4\u7b49\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002\u4f5c\u8005\u5f3a\u8c03\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u589e\u5f3a\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\uff0c\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6574\u5408\u7684\u72ec\u7279\u6311\u6218\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7ed3\u5408\u4e3a\u89e3\u51b3\u5206\u5e03\u5f0f\u6570\u636e\u8054\u5408\u8bad\u7ec3\u4e2d\u7684\u9690\u79c1\u548c\u6570\u636e\u5b64\u5c9b\u95ee\u9898\u63d0\u4f9b\u4e86\u6f5c\u5728\u65b9\u6848\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b0\u5174\u7684\u8054\u90a6\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08FLLM\uff09\u9886\u57df\u9762\u4e34\u901a\u4fe1\u3001\u8ba1\u7b97\u5f00\u9500\u3001\u5f02\u6784\u6027\u53ca\u9690\u79c1\u5b89\u5168\u7b49\u663e\u8457\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8FLLM\u7684\u53ef\u884c\u6027\uff0c\u672a\u6765\u8d8b\u52bf\u9884\u8ba1\u5c06\u4fa7\u91cd\u4e8e\u589e\u5f3a\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5bf9FLLM\u73b0\u6709\u7814\u7a76\u7684\u5168\u9762\u8c03\u67e5\uff0c\u4ece\u53ef\u884c\u6027\u3001\u9c81\u68d2\u6027\u3001\u5b89\u5168\u6027\u548c\u672a\u6765\u65b9\u5411\u56db\u4e2a\u5173\u952e\u89c6\u89d2\u8fdb\u884c\u4e86\u7cfb\u7edf\u68b3\u7406\u3002\u4f5c\u8005\u603b\u7ed3\u4e86\u63d0\u5347FLLM\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff08\u5982\u5e94\u5bf9\u8d44\u6e90\u3001\u6570\u636e\u548c\u4efb\u52a1\u5f02\u6784\u6027\uff09\uff0c\u5206\u6790\u4e86\u9690\u79c1\u548c\u5b89\u5168\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u5e76\u56de\u987e\u4e86\u6700\u65b0\u7684\u9632\u5fa1\u673a\u5236\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cFLLM\u7684\u6574\u5408\u867d\u5177\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u8d44\u6e90\u3001\u6570\u636e\u5f02\u6784\u6027\u548c\u5b89\u5168\u5a01\u80c1\u7b49\u591a\u91cd\u6311\u6218\u3002\u5f53\u524d\u9632\u5fa1\u673a\u5236\u4ecd\u9700\u6539\u8fdb\uff0c\u800c\u672a\u6765\u65b9\u5411\uff08\u5982\u5c11\u6837\u672c\u5b66\u4e60\u3001\u673a\u5668\u53cd\u5b66\u4e60\u7b49\uff09\u4e3aFLLM\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "conclusion": "\u672c\u7efc\u8ff0\u5f3a\u8c03\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u4f18\u5316FLLM\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\uff0c\u540c\u65f6\u5e94\u5bf9FL\u4e0eLLM\u6574\u5408\u7684\u7279\u6709\u95ee\u9898\u3002\u672a\u6765\u5728\u5c11\u6837\u672c\u5b66\u4e60\u3001\u673a\u5668\u53cd\u5b66\u4e60\u53caIP\u4fdd\u62a4\u7b49\u9886\u57df\u7684\u63a2\u7d22\u5c06\u63a8\u52a8\u8be5\u9886\u57df\u7684\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2505.09284", "pdf": "https://arxiv.org/pdf/2505.09284", "abs": "https://arxiv.org/abs/2505.09284", "authors": ["Panqi Chen", "Yifan Sun", "Lei Cheng", "Yang Yang", "Weichang Li", "Yang Liu", "Weiqing Liu", "Jiang Bian", "Shikai Fang"], "title": "Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Modeling and reconstructing multidimensional physical dynamics from sparse\nand off-grid observations presents a fundamental challenge in scientific\nresearch. Recently, diffusion-based generative modeling shows promising\npotential for physical simulation. However, current approaches typically\noperate on on-grid data with preset spatiotemporal resolution, but struggle\nwith the sparsely observed and continuous nature of real-world physical\ndynamics. To fill the gaps, we present SDIFT, Sequential DIffusion in\nFunctional Tucker space, a novel framework that generates full-field evolution\nof physical dynamics from irregular sparse observations. SDIFT leverages the\nfunctional Tucker model as the latent space representer with proven universal\napproximation property, and represents observations as latent functions and\nTucker core sequences. We then construct a sequential diffusion model with\ntemporally augmented UNet in the functional Tucker space, denoising noise drawn\nfrom a Gaussian process to generate the sequence of core tensors.\n  At the posterior sampling stage, we propose a Message-Passing Posterior\nSampling mechanism, enabling conditional generation of the entire sequence\nguided by observations at limited time steps. We validate SDIFT on three\nphysical systems spanning astronomical (supernova explosions, light-year\nscale), environmental (ocean sound speed fields, kilometer scale), and\nmolecular (organic liquid, millimeter scale) domains, demonstrating significant\nimprovements in both reconstruction accuracy and computational efficiency\ncompared to state-of-the-art approaches.", "AI": {"tldr": "SDIFT\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u548c\u4e0d\u89c4\u5219\u89c2\u6d4b\u4e2d\u91cd\u5efa\u591a\u7ef4\u7269\u7406\u52a8\u529b\u5b66\uff0c\u901a\u8fc7\u529f\u80fdTucker\u7a7a\u95f4\u548c\u5e8f\u5217\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u7684\u8ba1\u7b97\u3002", "motivation": "\u89e3\u51b3\u4ece\u7a00\u758f\u548c\u975e\u7f51\u683c\u89c2\u6d4b\u4e2d\u5efa\u6a21\u548c\u91cd\u5efa\u591a\u7ef4\u7269\u7406\u52a8\u529b\u5b66\u7684\u6311\u6218\uff0c\u586b\u8865\u73b0\u6709\u65b9\u6cd5\u5728\u8fde\u7eed\u6027\u548c\u7a00\u758f\u6027\u5904\u7406\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u529f\u80fdTucker\u6a21\u578b\u4f5c\u4e3a\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\uff0c\u6784\u5efa\u65f6\u5e8f\u589e\u5f3a\u7684UNet\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u566a\u58f0\u53bb\u566a\u751f\u6210\u6838\u5fc3\u5f20\u91cf\u5e8f\u5217\uff0c\u5e76\u5f15\u5165\u6d88\u606f\u4f20\u9012\u540e\u9a8c\u91c7\u6837\u673a\u5236\u3002", "result": "\u5728\u5929\u6587\u3001\u73af\u5883\u548c\u5206\u5b50\u4e09\u4e2a\u9886\u57df\u7684\u7269\u7406\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\uff0c\u663e\u793a\u51fa\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u7684\u91cd\u5efa\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "SDIFT\u6210\u529f\u89e3\u51b3\u4e86\u7a00\u758f\u548c\u4e0d\u89c4\u5219\u89c2\u6d4b\u4e0b\u7684\u7269\u7406\u52a8\u529b\u5b66\u5efa\u6a21\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.08834", "pdf": "https://arxiv.org/pdf/2505.08834", "abs": "https://arxiv.org/abs/2505.08834", "authors": ["Muhammad Junaid Asif"], "title": "Crowd Scene Analysis using Deep Learning Techniques", "categories": ["cs.CV", "cs.AI"], "comment": "MS Graduate Research Thesis", "summary": "Our research is focused on two main applications of crowd scene analysis\ncrowd counting and anomaly detection In recent years a large number of\nresearches have been presented in the domain of crowd counting We addressed two\nmain challenges in this domain 1 Deep learning models are datahungry paradigms\nand always need a large amount of annotated data for the training of algorithm\nIt is timeconsuming and costly task to annotate such large amount of data\nSelfsupervised training is proposed to deal with this challenge 2 MCNN consists\nof multicolumns of CNN with different sizes of filters by presenting a novel\napproach based on a combination of selfsupervised training and MultiColumn CNN\nThis enables the model to learn features at different levels and makes it\neffective in dealing with challenges of occluded scenes nonuniform density\ncomplex backgrounds and scale invariation The proposed model was evaluated on\npublicly available data sets such as ShanghaiTech and UCFQNRF by means of MAE\nand MSE A spatiotemporal model based on VGG19 is proposed for crowd anomaly\ndetection addressing challenges like lighting environmental conditions\nunexpected objects and scalability The model extracts spatial and temporal\nfeatures allowing it to be generalized to realworld scenes Spatial features are\nlearned using CNN while temporal features are learned using LSTM blocks The\nmodel works on binary classification and can detect normal or abnormal behavior\nThe models performance is improved by replacing fully connected layers with\ndense residual blocks Experiments on the Hockey Fight dataset and SCVD dataset\nshow our models outperform other stateoftheart approaches", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u76d1\u7763\u8bad\u7ec3\u548c\u591a\u5217CNN\u7684\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4eba\u7fa4\u8ba1\u6570\u4e2d\u7684\u6807\u6ce8\u6570\u636e\u9700\u6c42\u548c\u9ad8\u5bc6\u5ea6\u906e\u6321\u95ee\u9898\uff0c\u540c\u65f6\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u91c7\u7528VGG19\u548cLSTM\u63d0\u53d6\u65f6\u7a7a\u7279\u5f81\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4eba\u7fa4\u8ba1\u6570\u4e2d\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u9700\u6c42\u53ca\u9ad8\u5bc6\u5ea6\u573a\u666f\u4e0b\u7684\u906e\u6321\u95ee\u9898\uff0c\u540c\u65f6\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5e94\u5bf9\u5149\u7167\u53d8\u5316\u548c\u590d\u6742\u73af\u5883\u6311\u6218\u3002", "method": "\u4eba\u7fa4\u8ba1\u6570\u91c7\u7528\u81ea\u76d1\u7763\u8bad\u7ec3\u548c\u591a\u5217CNN\uff1b\u5f02\u5e38\u68c0\u6d4b\u7ed3\u5408VGG19\uff08\u7a7a\u95f4\u7279\u5f81\uff09\u548cLSTM\uff08\u65f6\u95f4\u7279\u5f81\uff09\uff0c\u5e76\u5f15\u5165\u5bc6\u96c6\u6b8b\u5dee\u5757\u3002", "result": "\u5728ShanghaiTech\u3001UCF-QNRF\u7b49\u6570\u636e\u96c6\u4e0aMAE/MSE\u8868\u73b0\u4f18\u5f02\uff1b\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u5728Hockey Fight\u548cSCVD\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u6a21\u578b\u5728\u4eba\u7fa4\u8ba1\u6570\u548c\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5747\u6709\u6548\uff0c\u4e14\u901a\u8fc7\u81ea\u76d1\u7763\u548c\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.09287", "pdf": "https://arxiv.org/pdf/2505.09287", "abs": "https://arxiv.org/abs/2505.09287", "authors": ["Shunsuke Yoneda", "Valdemar \u0160v\u00e1bensk\u00fd", "Gen Li", "Daisuke Deguchi", "Atsushi Shimada"], "title": "Ranking-Based At-Risk Student Prediction Using Federated Learning and Differential Features", "categories": ["cs.LG", "cs.CY", "I.2; I.6; K.3"], "comment": "To appear in the Proceedings of the 18th Educational Data Mining\n  Conference (EDM 2025)", "summary": "Digital textbooks are widely used in various educational contexts, such as\nuniversity courses and online lectures. Such textbooks yield learning log data\nthat have been used in numerous educational data mining (EDM) studies for\nstudent behavior analysis and performance prediction. However, these studies\nhave faced challenges in integrating confidential data, such as academic\nrecords and learning logs, across schools due to privacy concerns.\nConsequently, analyses are often conducted with data limited to a single\nschool, which makes developing high-performing and generalizable models\ndifficult. This study proposes a method that combines federated learning and\ndifferential features to address these issues. Federated learning enables model\ntraining without centralizing data, thereby preserving student privacy.\nDifferential features, which utilize relative values instead of absolute\nvalues, enhance model performance and generalizability. To evaluate the\nproposed method, a model for predicting at-risk students was trained using data\nfrom 1,136 students across 12 courses conducted over 4 years, and validated on\nhold-out test data from 5 other courses. Experimental results demonstrated that\nthe proposed method addresses privacy concerns while achieving performance\ncomparable to that of models trained via centralized learning in terms of Top-n\nprecision, nDCG, and PR-AUC. Furthermore, using differential features improved\nprediction performance across all evaluation datasets compared to\nnon-differential approaches. The trained models were also applicable for early\nprediction, achieving high performance in detecting at-risk students in earlier\nstages of the semester within the validation datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u548c\u5dee\u5206\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u6559\u80b2\u6570\u636e\u6316\u6398\u4e2d\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u901a\u7528\u6027\u95ee\u9898\uff0c\u5e76\u5728\u9884\u6d4b\u5b66\u4e60\u56f0\u96be\u5b66\u751f\u65b9\u9762\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u95ee\u9898\uff0c\u73b0\u6709\u6559\u80b2\u6570\u636e\u6316\u6398\u7814\u7a76\u96be\u4ee5\u8de8\u6821\u6574\u5408\u5b66\u4e60\u65e5\u5fd7\u548c\u5b66\u672f\u8bb0\u5f55\u7b49\u654f\u611f\u6570\u636e\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u548c\u901a\u7528\u6027\u53d7\u9650\u3002", "method": "\u91c7\u7528\u8054\u90a6\u5b66\u4e60\u907f\u514d\u6570\u636e\u96c6\u4e2d\u5b58\u50a8\u4ee5\u4fdd\u62a4\u9690\u79c1\uff0c\u5e76\u7ed3\u5408\u5dee\u5206\u7279\u5f81\uff08\u4f7f\u7528\u76f8\u5bf9\u503c\u800c\u975e\u7edd\u5bf9\u503c\uff09\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u901a\u7528\u6027\u3002", "result": "\u572812\u95e8\u8bfe\u7a0b\uff081,136\u540d\u5b66\u751f\uff09\u6570\u636e\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u57285\u95e8\u8bfe\u7a0b\u4e0a\u9a8c\u8bc1\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u9690\u79c1\u4fdd\u62a4\u7684\u540c\u65f6\uff0c\u6027\u80fd\u4e0e\u96c6\u4e2d\u5f0f\u5b66\u4e60\u76f8\u5f53\uff08Top-n\u7cbe\u5ea6\u3001nDCG\u3001PR-AUC\uff09\uff0c\u4e14\u5dee\u5206\u7279\u5f81\u5168\u9762\u63d0\u5347\u4e86\u9884\u6d4b\u8868\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u9690\u79c1\u4e0e\u6027\u80fd\uff0c\u652f\u6301\u8de8\u6821\u6a21\u578b\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u65e9\u671f\u9884\u6d4b\u80fd\u529b\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.08835", "pdf": "https://arxiv.org/pdf/2505.08835", "abs": "https://arxiv.org/abs/2505.08835", "authors": ["Hyunsik Na", "Wonho Lee", "Seungdeok Roh", "Sohee Park", "Daeseon Choi"], "title": "Robustness Analysis against Adversarial Patch Attacks in Fully Unmanned Stores", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": null, "summary": "The advent of convenient and efficient fully unmanned stores equipped with\nartificial intelligence-based automated checkout systems marks a new era in\nretail. However, these systems have inherent artificial intelligence security\nvulnerabilities, which are exploited via adversarial patch attacks,\nparticularly in physical environments. This study demonstrated that adversarial\npatches can severely disrupt object detection models used in unmanned stores,\nleading to issues such as theft, inventory discrepancies, and interference. We\ninvestigated three types of adversarial patch attacks -- Hiding, Creating, and\nAltering attacks -- and highlighted their effectiveness. We also introduce the\nnovel color histogram similarity loss function by leveraging attacker knowledge\nof the color information of a target class object. Besides the traditional\nconfusion-matrix-based attack success rate, we introduce a new\nbounding-boxes-based metric to analyze the practical impact of these attacks.\nStarting with attacks on object detection models trained on snack and fruit\ndatasets in a digital environment, we evaluated the effectiveness of\nadversarial patches in a physical testbed that mimicked a real unmanned store\nwith RGB cameras and realistic conditions. Furthermore, we assessed the\nrobustness of these attacks in black-box scenarios, demonstrating that shadow\nattacks can enhance success rates of attacks even without direct access to\nmodel parameters. Our study underscores the necessity for robust defense\nstrategies to protect unmanned stores from adversarial threats. Highlighting\nthe limitations of the current defense mechanisms in real-time detection\nsystems and discussing various proactive measures, we provide insights into\nimproving the robustness of object detection models and fortifying unmanned\nretail environments against these attacks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u4eba\u5de5\u667a\u80fd\u65e0\u4eba\u5546\u5e97\u4e2d\u7269\u4f53\u68c0\u6d4b\u6a21\u578b\u6613\u53d7\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\u7684\u6f0f\u6d1e\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u653b\u51fb\u65b9\u5f0f\u5e76\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u540c\u65f6\u5728\u7269\u7406\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u653b\u51fb\u6548\u679c\uff0c\u5f3a\u8c03\u4e86\u9632\u5fa1\u7b56\u7565\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u5546\u5e97\u7684\u666e\u53ca\uff0c\u5176\u4f9d\u8d56\u7684AI\u7cfb\u7edf\u9762\u4e34\u5bf9\u6297\u6027\u653b\u51fb\u7684\u5a01\u80c1\uff0c\u53ef\u80fd\u5bfc\u81f4\u76d7\u7a83\u548c\u5e93\u5b58\u95ee\u9898\uff0c\u56e0\u6b64\u7814\u7a76\u8fd9\u4e9b\u6f0f\u6d1e\u53ca\u9632\u5fa1\u63aa\u65bd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6570\u5b57\u548c\u7269\u7406\u73af\u5883\u6d4b\u8bd5\u4e09\u79cd\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\uff08\u9690\u85cf\u3001\u521b\u5efa\u3001\u4fee\u6539\uff09\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u989c\u8272\u76f4\u65b9\u56fe\u76f8\u4f3c\u6027\u635f\u5931\u51fd\u6570\u548c\u8fb9\u754c\u6846\u8bc4\u4f30\u6307\u6807\uff0c\u8bc4\u4f30\u653b\u51fb\u6548\u679c\u3002", "result": "\u653b\u51fb\u5728\u6570\u5b57\u548c\u7269\u7406\u73af\u5883\u4e2d\u5747\u6709\u6548\uff0c\u5c24\u5176\u5728\u9ed1\u76d2\u573a\u666f\u4e0b\u901a\u8fc7\u5f71\u5b50\u653b\u51fb\u63d0\u5347\u6210\u529f\u7387\uff0c\u51f8\u663e\u4e86\u5f53\u524d\u9632\u5fa1\u673a\u5236\u7684\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u7b56\u7565\u4ee5\u4fdd\u62a4\u65e0\u4eba\u5546\u5e97\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7269\u4f53\u68c0\u6d4b\u6a21\u578b\u548c\u589e\u5f3a\u96f6\u552e\u73af\u5883\u5b89\u5168\u6027\u7684\u65b9\u5411\u3002"}}
{"id": "2505.09294", "pdf": "https://arxiv.org/pdf/2505.09294", "abs": "https://arxiv.org/abs/2505.09294", "authors": ["Fan Xu", "Wuyang Chen", "Wei Gao"], "title": "On the Learning with Augmented Class via Forests", "categories": ["cs.LG"], "comment": "Accepted by IJCAI 2025", "summary": "Decision trees and forests have achieved successes in various real\napplications, most working with all testing classes known in training data. In\nthis work, we focus on learning with augmented class via forests, where an\naugmented class may appear in testing data yet not in training data. We\nincorporate information of augmented class into trees' splitting, i.e., a new\nsplitting criterion, called augmented Gini impurity, is introduced to exploit\nsome unlabeled data from testing distribution. We then develop the approach\nnamed Learning with Augmented Class via Forests (LACForest), which constructs\nshallow forests based on the augmented Gini impurity and then splits forests\nwith pseudo-labeled augmented instances for better performance. We also develop\ndeep neural forests with a novel optimization objective based on our augmented\nGini impurity, so as to utilize the representation power of neural networks for\nforests. Theoretically, we present the convergence analysis for augmented Gini\nimpurity, and finally conduct experiments to verify the effectiveness of our\napproaches. The code is available at https://github.com/nju-xuf/LACForest/.", "AI": {"tldr": "LACForest\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u51b3\u7b56\u6811\u548c\u68ee\u6797\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u589e\u5e7fGini\u4e0d\u7eaf\u5ea6\u6765\u5b66\u4e60\u8bad\u7ec3\u6570\u636e\u4e2d\u672a\u89c1\u7684\u589e\u5e7f\u7c7b\u522b\uff0c\u5229\u7528\u672a\u6807\u8bb0\u6d4b\u8bd5\u6570\u636e\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6d4b\u8bd5\u6570\u636e\u4e2d\u51fa\u73b0\u8bad\u7ec3\u6570\u636e\u4e2d\u672a\u89c1\u7684\u589e\u5e7f\u7c7b\u522b\u65f6\uff0c\u4f20\u7edf\u51b3\u7b56\u6811\u548c\u68ee\u6797\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u589e\u5e7fGini\u4e0d\u7eaf\u5ea6\u4f5c\u4e3a\u65b0\u7684\u5206\u88c2\u51c6\u5219\uff0c\u6784\u5efaLACForest\u6d45\u5c42\u68ee\u6797\uff0c\u5e76\u5229\u7528\u4f2a\u6807\u8bb0\u589e\u5e7f\u5b9e\u4f8b\u4f18\u5316\u68ee\u6797\u5206\u88c2\u3002\u8fdb\u4e00\u6b65\u5f00\u53d1\u6df1\u5ea6\u795e\u7ecf\u68ee\u6797\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u80fd\u529b\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u589e\u5e7fGini\u4e0d\u7eaf\u5ea6\u7684\u6536\u655b\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LACForest\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "LACForest\u901a\u8fc7\u589e\u5e7fGini\u4e0d\u7eaf\u5ea6\u548c\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\uff0c\u6709\u6548\u5904\u7406\u4e86\u589e\u5e7f\u7c7b\u522b\u7684\u5b66\u4e60\u95ee\u9898\uff0c\u4e3a\u51b3\u7b56\u6811\u548c\u68ee\u6797\u63d0\u4f9b\u4e86\u65b0\u7684\u6269\u5c55\u65b9\u5411\u3002"}}
{"id": "2505.08838", "pdf": "https://arxiv.org/pdf/2505.08838", "abs": "https://arxiv.org/abs/2505.08838", "authors": ["Peixuan Ge", "Tongkun Su", "Faqin Lv", "Baoliang Zhao", "Peng Zhang", "Chi Hong Wong", "Liang Yao", "Yu Sun", "Zenan Wang", "Pak Kin Wong", "Ying Hu"], "title": "Ultrasound Report Generation with Multimodal Large Language Models for Standardized Texts", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Ultrasound (US) report generation is a challenging task due to the\nvariability of US images, operator dependence, and the need for standardized\ntext. Unlike X-ray and CT, US imaging lacks consistent datasets, making\nautomation difficult. In this study, we propose a unified framework for\nmulti-organ and multilingual US report generation, integrating fragment-based\nmultilingual training and leveraging the standardized nature of US reports. By\naligning modular text fragments with diverse imaging data and curating a\nbilingual English-Chinese dataset, the method achieves consistent and\nclinically accurate text generation across organ sites and languages.\nFine-tuning with selective unfreezing of the vision transformer (ViT) further\nimproves text-image alignment. Compared to the previous state-of-the-art KMVE\nmethod, our approach achieves relative gains of about 2\\% in BLEU scores,\napproximately 3\\% in ROUGE-L, and about 15\\% in CIDEr, while significantly\nreducing errors such as missing or incorrect content. By unifying multi-organ\nand multi-language report generation into a single, scalable framework, this\nwork demonstrates strong potential for real-world clinical workflows.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u591a\u5668\u5b98\u548c\u591a\u8bed\u8a00\u7684\u8d85\u58f0\u62a5\u544a\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u788e\u7247\u5316\u591a\u8bed\u8a00\u8bad\u7ec3\u4e0e\u6807\u51c6\u5316\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u751f\u6210\u62a5\u544a\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u591a\u9879\u6307\u6807\u4e0a\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\u3002", "motivation": "\u8d85\u58f0\u62a5\u544a\u751f\u6210\u9762\u4e34\u6570\u636e\u6807\u51c6\u4e0d\u4e00\u3001\u4f9d\u8d56\u64cd\u4f5c\u8005\u7b49\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u81ea\u52a8\u5316\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u4e2a\u9002\u7528\u4e8e\u591a\u5668\u5b98\u3001\u591a\u8bed\u8a00\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u788e\u7247\u5316\u591a\u8bed\u8a00\u8bad\u7ec3\u548c\u53cc\u8bed\u6570\u636e\u96c6\uff08\u82f1\u8bed-\u4e2d\u6587\uff09\u5bf9\u9f50\u6a21\u5757\u5316\u6587\u672c\u7247\u6bb5\u4e0e\u591a\u6837\u5f71\u50cf\u6570\u636e\uff0c\u7ed3\u5408\u9009\u62e9\u6027\u89e3\u51bb\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u4f18\u5316\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u3002", "result": "\u76f8\u8f83\u4e8e\u5148\u524d\u7684KMVE\u65b9\u6cd5\uff0cBLEU\u63d0\u53472%\uff0cROUGE-L\u63d0\u53473%\uff0cCIDEr\u63d0\u534715%\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u9057\u6f0f\u6216\u9519\u8bef\u5185\u5bb9\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u4e34\u5e8a\u6d41\u7a0b\u4e2d\u7684\u6f5c\u529b\uff0c\u53ef\u6269\u5c55\u7528\u4e8e\u591a\u5668\u5b98\u548c\u591a\u8bed\u8a00\u62a5\u544a\u7684\u6807\u51c6\u5316\u751f\u6210\u3002"}}
{"id": "2505.09308", "pdf": "https://arxiv.org/pdf/2505.09308", "abs": "https://arxiv.org/abs/2505.09308", "authors": ["George Andriopoulos", "Soyuj Jung Basnet", "Juan Guevara", "Li Guo", "Keith Ross"], "title": "Neural Multivariate Regression: Qualitative Insights from the Unconstrained Feature Model", "categories": ["cs.LG"], "comment": "31 pages, 8 figures", "summary": "The Unconstrained Feature Model (UFM) is a mathematical framework that\nenables closed-form approximations for minimal training loss and related\nperformance measures in deep neural networks (DNNs). This paper leverages the\nUFM to provide qualitative insights into neural multivariate regression, a\ncritical task in imitation learning, robotics, and reinforcement learning.\nSpecifically, we address two key questions: (1) How do multi-task models\ncompare to multiple single-task models in terms of training performance? (2)\nCan whitening and normalizing regression targets improve training performance?\nThe UFM theory predicts that multi-task models achieve strictly smaller\ntraining MSE than multiple single-task models when the same or stronger\nregularization is applied to the latter, and our empirical results confirm\nthese findings. Regarding whitening and normalizing regression targets, the UFM\ntheory predicts that they reduce training MSE when the average variance across\nthe target dimensions is less than one, and our empirical results once again\nconfirm these findings. These findings highlight the UFM as a powerful\nframework for deriving actionable insights into DNN design and data\npre-processing strategies.", "AI": {"tldr": "UFM\u7406\u8bba\u901a\u8fc7\u6570\u5b66\u6846\u67b6\u4e3aDNN\u4e2d\u7684\u8bad\u7ec3\u635f\u5931\u548c\u6027\u80fd\u63d0\u4f9b\u4e86\u95ed\u5f0f\u89e3\uff0c\u5e76\u5e94\u7528\u4e8e\u591a\u53d8\u91cf\u56de\u5f52\u4efb\u52a1\u3002\u7814\u7a76\u53d1\u73b0\u591a\u4efb\u52a1\u6a21\u578b\u5728\u76f8\u540c\u6216\u66f4\u5f3a\u6b63\u5219\u5316\u4e0b\u4f18\u4e8e\u5355\u4efb\u52a1\u6a21\u578b\uff0c\u4e14\u76ee\u6807\u7ef4\u5ea6\u65b9\u5dee\u5c0f\u4e8e1\u65f6\u5f52\u4e00\u5316\u80fd\u964d\u4f4e\u8bad\u7ec3MSE\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5229\u7528UFM\u6846\u67b6\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u591a\u53d8\u91cf\u56de\u5f52\u4efb\u52a1\uff08\u5982\u6a21\u4eff\u5b66\u4e60\u3001\u673a\u5668\u4eba\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u63d0\u4f9b\u8bbe\u8ba1\u6d1e\u89c1\uff0c\u89e3\u51b3\u591a\u4efb\u52a1\u6a21\u578b\u6027\u80fd\u53ca\u6570\u636e\u9884\u5904\u7406\u7b56\u7565\u7684\u6548\u679c\u95ee\u9898\u3002", "method": "\u91c7\u7528UFM\u7406\u8bba\u8fdb\u884c\u6570\u5b66\u5efa\u6a21\u548c\u95ed\u5f0f\u89e3\u63a8\u5bfc\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u591a\u4efb\u52a1\u6a21\u578b\u4e0e\u5355\u4efb\u52a1\u6a21\u578b\u7684\u8bad\u7ec3MSE\u5bf9\u6bd4\uff0c\u4ee5\u53ca\u76ee\u6807\u7ef4\u5ea6\u5f52\u4e00\u5316\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7406\u8bba\u9884\u6d4b\u548c\u5b9e\u8bc1\u4e00\u81f4\u8868\u660e\uff1a\u591a\u4efb\u52a1\u6a21\u578b\u5728\u6b63\u5219\u5316\u6761\u4ef6\u76f8\u540c\u6216\u66f4\u5f3a\u65f6\u8bad\u7ec3MSE\u66f4\u4f4e\uff1b\u76ee\u6807\u7ef4\u5ea6\u5e73\u5747\u65b9\u5dee\u5c0f\u4e8e1\u65f6\u5f52\u4e00\u5316\u53ef\u964d\u4f4e\u8bad\u7ec3MSE\u3002", "conclusion": "UFM\u662f\u5206\u6790DNN\u8bbe\u8ba1\u53ca\u6570\u636e\u9884\u5904\u7406\u7b56\u7565\u7684\u6709\u529b\u5de5\u5177\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u56de\u5f52\u76ee\u6807\u9884\u5904\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2505.08841", "pdf": "https://arxiv.org/pdf/2505.08841", "abs": "https://arxiv.org/abs/2505.08841", "authors": ["Andrea Cremaschi", "Dae-Jin Lee", "Manuele Leonelli"], "title": "Will AI Take My Job? Evolving Perceptions of Automation and Labor Risk in Latin America", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "As artificial intelligence and robotics increasingly reshape the global labor\nmarket, understanding public perceptions of these technologies becomes\ncritical. We examine how these perceptions have evolved across Latin America,\nusing survey data from the 2017, 2018, 2020, and 2023 waves of the\nLatinobar\\'ometro. Drawing on responses from over 48,000 individuals across 16\ncountries, we analyze fear of job loss due to artificial intelligence and\nrobotics. Using statistical modeling and latent class analysis, we identify key\nstructural and ideological predictors of concern, with education level and\npolitical orientation emerging as the most consistent drivers. Our findings\nreveal substantial temporal and cross-country variation, with a notable peak in\nfear during 2018 and distinct attitudinal profiles emerging from latent\nsegmentation. These results offer new insights into the social and structural\ndimensions of AI anxiety in emerging economies and contribute to a broader\nunderstanding of public attitudes toward automation beyond the Global North.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u62c9\u4e01\u7f8e\u6d32\u516c\u4f17\u5bf9AI\u548c\u673a\u5668\u4eba\u5bfc\u81f4\u5931\u4e1a\u7684\u62c5\u5fe7\uff0c\u5206\u6790\u4e862017-2023\u5e74\u7684\u8c03\u67e5\u6570\u636e\uff0c\u53d1\u73b0\u6559\u80b2\u6c34\u5e73\u548c\u653f\u6cbb\u503e\u5411\u662f\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u3002", "motivation": "\u968f\u7740AI\u548c\u673a\u5668\u4eba\u91cd\u5851\u52b3\u52a8\u529b\u5e02\u573a\uff0c\u4e86\u89e3\u516c\u4f17\u5bf9\u8fd9\u4e9b\u6280\u672f\u7684\u770b\u6cd5\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u5728\u975e\u53d1\u8fbe\u56fd\u5bb6\u3002", "method": "\u91c7\u7528\u7edf\u8ba1\u5efa\u6a21\u548c\u6f5c\u5728\u7c7b\u522b\u5206\u6790\uff0c\u57fa\u4e8e48,000\u591a\u4eba\u7684\u8c03\u67e5\u6570\u636e\uff0c\u5206\u6790\u5931\u4e1a\u62c5\u5fe7\u7684\u7ed3\u6784\u548c\u610f\u8bc6\u5f62\u6001\u9a71\u52a8\u56e0\u7d20\u3002", "result": "\u53d1\u73b02018\u5e74\u662f\u62c5\u5fe7\u9ad8\u5cf0\u671f\uff0c\u6559\u80b2\u6c34\u5e73\u548c\u653f\u6cbb\u503e\u5411\u662f\u6700\u4e00\u81f4\u7684\u5f71\u54cd\u56e0\u7d20\uff0c\u4e14\u5b58\u5728\u663e\u8457\u7684\u65f6\u95f4\u548c\u56fd\u5bb6\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u4e3a\u65b0\u5174\u7ecf\u6d4e\u4f53\u7684AI\u7126\u8651\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u8865\u5145\u4e86\u5168\u7403\u5317\u65b9\u4ee5\u5916\u7684\u81ea\u52a8\u5316\u6001\u5ea6\u7814\u7a76\u3002"}}
{"id": "2505.09331", "pdf": "https://arxiv.org/pdf/2505.09331", "abs": "https://arxiv.org/abs/2505.09331", "authors": ["Cunlai Pu", "Fangrui Wu", "Rajput Ramiz Sharafat", "Guangzhao Dai", "Xiangbo Shu"], "title": "MUST: Multi-Scale Structural-Temporal Link Prediction Model for UAV Ad Hoc Networks", "categories": ["cs.LG"], "comment": null, "summary": "Link prediction in unmanned aerial vehicle (UAV) ad hoc networks (UANETs)\naims to predict the potential formation of future links between UAVs. In\nadversarial environments where the route information of UAVs is unavailable,\npredicting future links must rely solely on the observed historical topological\ninformation of UANETs. However, the highly dynamic and sparse nature of UANET\ntopologies presents substantial challenges in effectively capturing meaningful\nstructural and temporal patterns for accurate link prediction. Most existing\nlink prediction methods focus on temporal dynamics at a single structural scale\nwhile neglecting the effects of sparsity, resulting in insufficient information\ncapture and limited applicability to UANETs. In this paper, we propose a\nmulti-scale structural-temporal link prediction model (MUST) for UANETs.\nSpecifically, we first employ graph attention networks (GATs) to capture\nstructural features at multiple levels, including the individual UAV level, the\nUAV community level, and the overall network level. Then, we use long\nshort-term memory (LSTM) networks to learn the temporal dynamics of these\nmulti-scale structural features. Additionally, we address the impact of\nsparsity by introducing a sophisticated loss function during model\noptimization. We validate the performance of MUST using several UANET datasets\ngenerated through simulations. Extensive experimental results demonstrate that\nMUST achieves state-of-the-art link prediction performance in highly dynamic\nand sparse UANETs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMUST\u7684\u591a\u5c3a\u5ea6\u7ed3\u6784-\u65f6\u95f4\u94fe\u8def\u9884\u6d4b\u6a21\u578b\uff0c\u7528\u4e8e\u9ad8\u5ea6\u52a8\u6001\u4e14\u7a00\u758f\u7684\u65e0\u4eba\u673a\u81ea\u7ec4\u7f51\uff08UANETs\uff09\u3002", "motivation": "\u5728\u5bf9\u6297\u73af\u5883\u4e2d\uff0c\u65e0\u4eba\u673a\u8def\u5f84\u4fe1\u606f\u4e0d\u53ef\u7528\uff0c\u9700\u4ec5\u57fa\u4e8e\u5386\u53f2\u62d3\u6251\u4fe1\u606f\u9884\u6d4b\u672a\u6765\u94fe\u63a5\u3002\u73b0\u6709\u65b9\u6cd5\u56e0\u5ffd\u7565\u7a00\u758f\u6027\u548c\u5355\u4e00\u5c3a\u5ea6\u65f6\u95f4\u52a8\u6001\u6027\uff0c\u5bfc\u81f4\u4fe1\u606f\u6355\u6349\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GATs\uff09\u6355\u6349\u591a\u5c42\u6b21\u7ed3\u6784\u7279\u5f81\uff0c\u7ed3\u5408LSTM\u5b66\u4e60\u5176\u65f6\u95f4\u52a8\u6001\u6027\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u635f\u5931\u51fd\u6570\u89e3\u51b3\u7a00\u758f\u6027\u95ee\u9898\u3002", "result": "\u5728\u4eff\u771f\u751f\u6210\u7684UANET\u6570\u636e\u96c6\u4e0a\uff0cMUST\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u94fe\u8def\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "MUST\u6a21\u578b\u80fd\u6709\u6548\u5e94\u5bf9UANETs\u7684\u9ad8\u52a8\u6001\u6027\u548c\u7a00\u758f\u6027\uff0c\u663e\u8457\u63d0\u5347\u94fe\u8def\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2505.08844", "pdf": "https://arxiv.org/pdf/2505.08844", "abs": "https://arxiv.org/abs/2505.08844", "authors": ["Jiawen Chen", "Jianghao Zhang", "Huaxiu Yao", "Yun Li"], "title": "CellTypeAgent: Trustworthy cell type annotation with Large Language Models", "categories": ["q-bio.GN", "cs.AI", "68T20", "I.2.1"], "comment": null, "summary": "Cell type annotation is a critical yet laborious step in single-cell RNA\nsequencing analysis. We present a trustworthy large language model (LLM)-agent,\nCellTypeAgent, which integrates LLMs with verification from relevant databases.\nCellTypeAgent achieves higher accuracy than existing methods while mitigating\nhallucinations. We evaluated CellTypeAgent across nine real datasets involving\n303 cell types from 36 tissues. This combined approach holds promise for more\nefficient and reliable cell type annotation.", "AI": {"tldr": "CellTypeAgent\u662f\u4e00\u4e2a\u96c6\u6210\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u548c\u6570\u636e\u5e93\u9a8c\u8bc1\u7684\u53ef\u9760\u5de5\u5177\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u5206\u6790\u4e2d\u7684\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u5e76\u51cf\u5c11\u4e86\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u662f\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u5206\u6790\u4e2d\u5173\u952e\u4f46\u8d39\u65f6\u7684\u6b65\u9aa4\uff0c\u76ee\u524d\u65b9\u6cd5\u5b58\u5728\u51c6\u786e\u6027\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u96c6\u6210\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u4e0e\u6570\u636e\u5e93\u9a8c\u8bc1\uff0c\u6784\u5efaCellTypeAgent\u5de5\u5177\uff0c\u901a\u8fc7\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u63d0\u5347\u51c6\u786e\u7387\u3002", "result": "\u57289\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\uff08\u6db5\u76d636\u79cd\u7ec4\u7ec7\u7684303\u79cd\u7ec6\u80de\u7c7b\u578b\uff09\u4e2d\u9a8c\u8bc1\uff0cCellTypeAgent\u51c6\u786e\u7387\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CellTypeAgent\u5c55\u73b0\u4e86\u7ed3\u5408LLM\u4e0e\u6570\u636e\u5e93\u9a8c\u8bc1\u7684\u6f5c\u529b\uff0c\u4e3a\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09344", "pdf": "https://arxiv.org/pdf/2505.09344", "abs": "https://arxiv.org/abs/2505.09344", "authors": ["Gabriel Cort\u00eas", "Nuno Louren\u00e7o", "Paolo Romano", "Penousal Machado"], "title": "GreenFactory: Ensembling Zero-Cost Proxies to Estimate Performance of Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Determining the performance of a Deep Neural Network during Neural\nArchitecture Search processes is essential for identifying optimal\narchitectures and hyperparameters. Traditionally, this process requires\ntraining and evaluation of each network, which is time-consuming and\nresource-intensive. Zero-cost proxies estimate performance without training,\nserving as an alternative to traditional training. However, recent proxies\noften lack generalization across diverse scenarios and provide only relative\nrankings rather than predicted accuracies. To address these limitations, we\npropose GreenFactory, an ensemble of zero-cost proxies that leverages a random\nforest regressor to combine multiple predictors' strengths and directly predict\nmodel test accuracy. We evaluate GreenFactory on NATS-Bench, achieving robust\nresults across multiple datasets. Specifically, GreenFactory achieves high\nKendall correlations on NATS-Bench-SSS, indicating substantial agreement\nbetween its predicted scores and actual performance: 0.907 for CIFAR-10, 0.945\nfor CIFAR-100, and 0.920 for ImageNet-16-120. Similarly, on NATS-Bench-TSS, we\nachieve correlations of 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for\nImageNet-16-120, showcasing its reliability in both search spaces.", "AI": {"tldr": "GreenFactory \u662f\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\u7684\u96f6\u6210\u672c\u4ee3\u7406\u96c6\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u76f4\u63a5\u9884\u6d4b\u6a21\u578b\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u4ee3\u7406\u65b9\u6cd5\u6cdb\u5316\u6027\u5dee\u548c\u4ec5\u63d0\u4f9b\u76f8\u5bf9\u6392\u540d\u7684\u95ee\u9898\uff0c\u5728 NATS-Bench \u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u9700\u8981\u8bad\u7ec3\u548c\u8bc4\u4f30\u6bcf\u4e2a\u7f51\u7edc\uff0c\u8017\u65f6\u8017\u8d44\u6e90\u3002\u73b0\u6709\u7684\u96f6\u6210\u672c\u4ee3\u7406\u65b9\u6cd5\u6cdb\u5316\u6027\u5dee\u4e14\u4ec5\u63d0\u4f9b\u76f8\u5bf9\u6392\u540d\uff0c\u65e0\u6cd5\u76f4\u63a5\u9884\u6d4b\u51c6\u786e\u7387\u3002", "method": "GreenFactory \u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u591a\u4e2a\u96f6\u6210\u672c\u4ee3\u7406\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\u7ed3\u5408\u5b83\u4eec\u7684\u4f18\u52bf\uff0c\u76f4\u63a5\u9884\u6d4b\u6a21\u578b\u6d4b\u8bd5\u51c6\u786e\u7387\u3002", "result": "\u5728 NATS-Bench \u7684\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\uff0cGreenFactory \u7684 Kendall \u76f8\u5173\u7cfb\u6570\u8868\u73b0\u4f18\u5f02\uff08\u5982 CIFAR-10 \u4e0a 0.907\uff09\uff0c\u8bc1\u660e\u4e86\u5176\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GreenFactory \u901a\u8fc7\u96c6\u6210\u96f6\u6210\u672c\u4ee3\u7406\u548c\u76f4\u63a5\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u641c\u7d22\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.08845", "pdf": "https://arxiv.org/pdf/2505.08845", "abs": "https://arxiv.org/abs/2505.08845", "authors": ["Misgina Tsighe Hagos", "Antti Suutala", "Dmitrii Bychkov", "Hakan K\u00fcc\u00fckel", "Joar von Bahr", "Milda Poceviciute", "Johan Lundin", "Nina Linder", "Claes Lundstr\u00f6m"], "title": "Validation of Conformal Prediction in Cervical Atypia Classification", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Deep learning based cervical cancer classification can potentially increase\naccess to screening in low-resource regions. However, deep learning models are\noften overconfident and do not reliably reflect diagnostic uncertainty.\nMoreover, they are typically optimized to generate maximum-likelihood\npredictions, which fail to convey uncertainty or ambiguity in their results.\nSuch challenges can be addressed using conformal prediction, a model-agnostic\nframework for generating prediction sets that contain likely classes for\ntrained deep-learning models. The size of these prediction sets indicates model\nuncertainty, contracting as model confidence increases. However, existing\nconformal prediction evaluation primarily focuses on whether the prediction set\nincludes or covers the true class, often overlooking the presence of extraneous\nclasses. We argue that prediction sets should be truthful and valuable to end\nusers, ensuring that the listed likely classes align with human expectations\nrather than being overly relaxed and including false positives or unlikely\nclasses. In this study, we comprehensively validate conformal prediction sets\nusing expert annotation sets collected from multiple annotators. We evaluate\nthree conformal prediction approaches applied to three deep-learning models\ntrained for cervical atypia classification. Our expert annotation-based\nanalysis reveals that conventional coverage-based evaluations overestimate\nperformance and that current conformal prediction methods often produce\nprediction sets that are not well aligned with human labels. Additionally, we\nexplore the capabilities of the conformal prediction methods in identifying\nambiguous and out-of-distribution data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u5171\u5f62\u9884\u6d4b\u6539\u8fdb\u6df1\u5ea6\u5b66\u4e60\u5728\u5bab\u9888\u764c\u5206\u7c7b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8\u9a8c\u8bc1\u5176\u771f\u5b9e\u6027\u4e0e\u5b9e\u7528\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u5bab\u9888\u764c\u5206\u7c7b\u4e2d\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u548c\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5171\u5f62\u9884\u6d4b\u53ef\u751f\u6210\u5305\u542b\u53ef\u80fd\u7c7b\u522b\u7684\u9884\u6d4b\u96c6\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u5ffd\u7565\u4e86\u9884\u6d4b\u96c6\u7684\u771f\u5b9e\u6027\u548c\u4e34\u5e8a\u4ef7\u503c\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u5bf9\u4e09\u4e2a\u5bab\u9888\u5f02\u578b\u5206\u7c7b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u5408\u591a\u6ce8\u91ca\u8005\u7684\u4e13\u5bb6\u6807\u6ce8\u9a8c\u8bc1\u9884\u6d4b\u96c6\u7684\u51c6\u786e\u6027\u3002", "result": "\u4f20\u7edf\u8986\u76d6\u7387\u8bc4\u4f30\u9ad8\u4f30\u6027\u80fd\uff0c\u73b0\u6709\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u7684\u9884\u6d4b\u96c6\u4e0e\u4eba\u7c7b\u6807\u6ce8\u4e00\u81f4\u6027\u4e0d\u8db3\uff0c\u4f46\u80fd\u8bc6\u522b\u6a21\u7cca\u548c\u5206\u5e03\u5916\u6570\u636e\u3002", "conclusion": "\u9700\u6539\u8fdb\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u4ee5\u751f\u6210\u66f4\u7b26\u5408\u4eba\u7c7b\u9884\u671f\u7684\u9884\u6d4b\u96c6\uff0c\u63d0\u5347\u5176\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.09354", "pdf": "https://arxiv.org/pdf/2505.09354", "abs": "https://arxiv.org/abs/2505.09354", "authors": ["Guangtai Wang", "Chi-Man Vong", "Jintao Huang"], "title": "Exploiting the Potential Supervision Information of Clean Samples in Partial Label Learning", "categories": ["cs.LG"], "comment": null, "summary": "Diminishing the impact of false-positive labels is critical for conducting\ndisambiguation in partial label learning. However, the existing disambiguation\nstrategies mainly focus on exploiting the characteristics of individual partial\nlabel instances while neglecting the strong supervision information of clean\nsamples randomly lying in the datasets. In this work, we show that clean\nsamples can be collected to offer guidance and enhance the confidence of the\nmost possible candidates. Motivated by the manner of the differentiable count\nloss strat- egy and the K-Nearest-Neighbor algorithm, we proposed a new\ncalibration strategy called CleanSE. Specifically, we attribute the most\nreliable candidates with higher significance under the assumption that for each\nclean sample, if its label is one of the candidates of its nearest neighbor in\nthe representation space, it is more likely to be the ground truth of its\nneighbor. Moreover, clean samples offer help in characterizing the sample\ndistributions by restricting the label counts of each label to a specific\ninterval. Extensive experiments on 3 synthetic benchmarks and 5 real-world PLL\ndatasets showed this calibration strategy can be applied to most of the\nstate-of-the-art PLL methods as well as enhance their performance.", "AI": {"tldr": "CleanSE\u662f\u4e00\u79cd\u65b0\u7684\u6821\u51c6\u7b56\u7565\uff0c\u901a\u8fc7\u5229\u7528\u6570\u636e\u96c6\u4e2d\u5e72\u51c0\u6837\u672c\u7684\u5f3a\u76d1\u7763\u4fe1\u606f\uff0c\u589e\u5f3a\u90e8\u5206\u6807\u7b7e\u5b66\u4e60\u4e2d\u5019\u9009\u6807\u7b7e\u7684\u7f6e\u4fe1\u5ea6\uff0c\u63d0\u5347\u73b0\u6709PLL\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u90e8\u5206\u6807\u7b7e\u5b66\u4e60\u4e2d\u7684\u8bef\u62a5\u6807\u7b7e\u5f71\u54cd\u9700\u51cf\u5c0f\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u5b9e\u4f8b\u800c\u5ffd\u89c6\u6570\u636e\u96c6\u4e2d\u5e72\u51c0\u6837\u672c\u7684\u5f3a\u76d1\u7763\u4fe1\u606f\uff0cCleanSE\u65e8\u5728\u5229\u7528\u8fd9\u4e9b\u5e72\u51c0\u6837\u672c\u63d0\u4f9b\u6307\u5bfc\u548c\u589e\u5f3a\u5019\u9009\u6807\u7b7e\u7f6e\u4fe1\u5ea6\u3002", "method": "\u7ed3\u5408\u53ef\u5fae\u5206\u8ba1\u6570\u635f\u5931\u7b56\u7565\u548cK\u8fd1\u90bb\u7b97\u6cd5\uff0cCleanSE\u5047\u8bbe\u82e5\u5e72\u51c0\u6837\u672c\u7684\u6807\u7b7e\u662f\u5176\u8fd1\u90bb\u7684\u5019\u9009\u6807\u7b7e\u4e4b\u4e00\uff0c\u5219\u8be5\u6807\u7b7e\u66f4\u6709\u53ef\u80fd\u662f\u5176\u8fd1\u90bb\u7684\u771f\u5b9e\u6807\u7b7e\uff0c\u5e76\u901a\u8fc7\u9650\u5236\u5404\u6807\u7b7e\u8ba1\u6570\u533a\u95f4\u6765\u523b\u753b\u6837\u672c\u5206\u5e03\u3002", "result": "\u57283\u4e2a\u5408\u6210\u57fa\u51c6\u548c5\u4e2a\u771f\u5b9ePLL\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCleanSE\u53ef\u5e94\u7528\u4e8e\u5927\u591a\u6570\u5148\u8fdbPLL\u65b9\u6cd5\u5e76\u663e\u8457\u63d0\u5347\u5176\u6027\u80fd\u3002", "conclusion": "CleanSE\u901a\u8fc7\u5229\u7528\u5e72\u51c0\u6837\u672c\u7684\u76d1\u7763\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u90e8\u5206\u6807\u7b7e\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.09361", "pdf": "https://arxiv.org/pdf/2505.09361", "abs": "https://arxiv.org/abs/2505.09361", "authors": ["Samir Moustafa", "Nils M. Kriege", "Wilfried N. Gansterer"], "title": "Efficient Mixed Precision Quantization in Graph Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have become essential for handling large-scale\ngraph applications. However, the computational demands of GNNs necessitate the\ndevelopment of efficient methods to accelerate inference. Mixed precision\nquantization emerges as a promising solution to enhance the efficiency of GNN\narchitectures without compromising prediction performance. Compared to\nconventional deep learning architectures, GNN layers contain a wider set of\ncomponents that can be quantized, including message passing functions,\naggregation functions, update functions, the inputs, learnable parameters, and\noutputs of these functions. In this paper, we introduce a theorem for efficient\nquantized message passing to aggregate integer messages. It guarantees\nnumerical equality of the aggregated messages using integer values with respect\nto those obtained with full (FP32) precision. Based on this theorem, we\nintroduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, which\nflexibly selects effective integer bit-widths for all components within GNN\nlayers. Our approach systematically navigates the wide set of possible\nbit-width combinations, addressing the challenge of optimizing efficiency while\naiming at maintaining comparable prediction performance. MixQ-GNN integrates\nwith existing GNN quantization methods, utilizing their graph structure\nadvantages to achieve higher prediction performance. On average, MixQ-GNN\nachieved reductions in bit operations of 5.5x for node classification and 5.1x\nfor graph classification compared to architectures represented in FP32\nprecision.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMixQ-GNN\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u63d0\u5347GNN\u63a8\u7406\u6548\u7387\uff0c\u4fdd\u8bc1\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "GNN\u5728\u5927\u89c4\u6a21\u56fe\u5e94\u7528\u4e2d\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u9700\u9ad8\u6548\u63a8\u7406\u65b9\u6cd5\u3002\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u662f\u63d0\u5347\u6548\u7387\u4e14\u4e0d\u727a\u7272\u6027\u80fd\u7684\u6f5c\u5728\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u91cf\u5316\u6d88\u606f\u4f20\u9012\u5b9a\u7406\uff0c\u63d0\u51faMixQ-GNN\u6846\u67b6\uff0c\u7075\u6d3b\u9009\u62e9GNN\u5c42\u5404\u7ec4\u4ef6\u7684\u6574\u6570\u4f4d\u5bbd\uff0c\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "result": "MixQ-GNN\u5728\u8282\u70b9\u5206\u7c7b\u548c\u56fe\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4FP32\u7cbe\u5ea6\u5206\u522b\u51cf\u5c11\u4e865.5\u500d\u548c5.1\u500d\u7684\u6bd4\u7279\u64cd\u4f5c\u3002", "conclusion": "MixQ-GNN\u901a\u8fc7\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u6709\u6548\u5e73\u8861\u4e86GNN\u7684\u8ba1\u7b97\u6548\u7387\u4e0e\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.08847", "pdf": "https://arxiv.org/pdf/2505.08847", "abs": "https://arxiv.org/abs/2505.08847", "authors": ["Fatima Ezzeddine", "Rinad Akel", "Ihab Sbeity", "Silvia Giordano", "Marc Langheinrich", "Omran Ayoub"], "title": "On the interplay of Explainability, Privacy and Predictive Performance with Explanation-assisted Model Extraction", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Machine Learning as a Service (MLaaS) has gained important attraction as a\nmeans for deploying powerful predictive models, offering ease of use that\nenables organizations to leverage advanced analytics without substantial\ninvestments in specialized infrastructure or expertise. However, MLaaS\nplatforms must be safeguarded against security and privacy attacks, such as\nmodel extraction (MEA) attacks. The increasing integration of explainable AI\n(XAI) within MLaaS has introduced an additional privacy challenge, as attackers\ncan exploit model explanations particularly counterfactual explanations (CFs)\nto facilitate MEA. In this paper, we investigate the trade offs among model\nperformance, privacy, and explainability when employing Differential Privacy\n(DP), a promising technique for mitigating CF facilitated MEA. We evaluate two\ndistinct DP strategies: implemented during the classification model training\nand at the explainer during CF generation.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u4f7f\u7528\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u6280\u672f\u65f6\uff0c\u673a\u5668\u5b66\u4e60\u5373\u670d\u52a1\uff08MLaaS\uff09\u5e73\u53f0\u5728\u6a21\u578b\u6027\u80fd\u3001\u9690\u79c1\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u91cd\u70b9\u5173\u6ce8\u5982\u4f55\u901a\u8fc7\u4e24\u79cd\u4e0d\u540c\u7684DP\u7b56\u7565\u6765\u7f13\u89e3\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff08CFs\uff09\u7684\u6a21\u578b\u63d0\u53d6\u653b\u51fb\uff08MEA\uff09\u3002", "motivation": "\u968f\u7740\u53ef\u89e3\u91caAI\uff08XAI\uff09\u5728MLaaS\u4e2d\u7684\u96c6\u6210\uff0c\u653b\u51fb\u8005\u53ef\u80fd\u5229\u7528\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff08CFs\uff09\u8fdb\u884c\u6a21\u578b\u63d0\u53d6\u653b\u51fb\uff08MEA\uff09\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u5e73\u8861\u6a21\u578b\u6027\u80fd\u3001\u9690\u79c1\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u8bba\u6587\u8bc4\u4f30\u4e86\u4e24\u79cd\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u7b56\u7565\uff1a\u4e00\u79cd\u5728\u5206\u7c7b\u6a21\u578b\u8bad\u7ec3\u65f6\u5b9e\u65bd\uff0c\u53e6\u4e00\u79cd\u5728\u89e3\u91ca\u5668\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff08CFs\uff09\u65f6\u5b9e\u65bd\u3002", "result": "\u7814\u7a76\u5206\u6790\u4e86\u8fd9\u4e24\u79cdDP\u7b56\u7565\u5bf9\u6a21\u578b\u6027\u80fd\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5f71\u54cd\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u5728MLaaS\u4e2d\u5e94\u7528\u5dee\u5206\u9690\u79c1\u4ee5\u7f13\u89e3\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u6a21\u578b\u63d0\u53d6\u653b\u51fb\u7684\u53ef\u80fd\u9014\u5f84\uff0c\u5e76\u5c55\u793a\u4e86\u4e0d\u540c\u7b56\u7565\u7684\u6743\u8861\u6548\u679c\u3002"}}
{"id": "2505.09366", "pdf": "https://arxiv.org/pdf/2505.09366", "abs": "https://arxiv.org/abs/2505.09366", "authors": ["SeyedMojtaba Mohasel", "Alireza Afzal Aghaei", "Corey Pew"], "title": "Personalized Control for Lower Limb Prosthesis Using Kolmogorov-Arnold Networks", "categories": ["cs.LG"], "comment": null, "summary": "Objective: This paper investigates the potential of learnable activation\nfunctions in Kolmogorov-Arnold Networks (KANs) for personalized control in a\nlower-limb prosthesis. In addition, user-specific vs. pooled training data is\nevaluated to improve machine learning (ML) and Deep Learning (DL) performance\nfor turn intent prediction.\n  Method: Inertial measurement unit (IMU) data from the shank were collected\nfrom five individuals with lower-limb amputation performing turning tasks in a\nlaboratory setting. Ability to classify an upcoming turn was evaluated for\nMultilayer Perceptron (MLP), Kolmogorov-Arnold Network (KAN), convolutional\nneural network (CNN), and fractional Kolmogorov-Arnold Networks (FKAN). The\ncomparison of MLP and KAN (for ML models) and FKAN and CNN (for DL models)\nassessed the effectiveness of learnable activation functions. Models were\ntrained separately on user-specific and pooled data to evaluate the impact of\ntraining data on their performance.\n  Results: Learnable activation functions in KAN and FKAN did not yield\nsignificant improvement compared to MLP and CNN, respectively. Training on\nuser-specific data yielded superior results compared to pooled data for ML\nmodels ($p < 0.05$). In contrast, no significant difference was observed\nbetween user-specific and pooled training for DL models.\n  Significance: These findings suggest that learnable activation functions may\ndemonstrate distinct advantages in datasets involving more complex tasks and\nlarger volumes. In addition, pooled training showed comparable performance to\nuser-specific training in DL models, indicating that model training for\nprosthesis control can utilize data from multiple participants.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u5728\u79d1\u5c14\u83ab\u6208\u7f57\u592b-\u963f\u8bfa\u5fb7\u7f51\u7edc\uff08KAN\uff09\u4e2d\u5bf9\u4e0b\u80a2\u5047\u80a2\u4e2a\u6027\u5316\u63a7\u5236\u7684\u6f5c\u529b\uff0c\u5e76\u6bd4\u8f83\u4e86\u7528\u6237\u7279\u5b9a\u6570\u636e\u4e0e\u5408\u5e76\u6570\u636e\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u8868\u660e\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u672a\u5e26\u6765\u663e\u8457\u6539\u8fdb\uff0c\u4f46\u7528\u6237\u7279\u5b9a\u6570\u636e\u5728\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u5728\u5047\u80a2\u63a7\u5236\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u8bad\u7ec3\u6570\u636e\u6765\u6e90\uff08\u7528\u6237\u7279\u5b9a\u6216\u5408\u5e76\uff09\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u63d0\u5347\u8f6c\u5411\u610f\u56fe\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6536\u96c6\u4e94\u540d\u4e0b\u80a2\u622a\u80a2\u8005\u7684\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u6570\u636e\uff0c\u8bc4\u4f30\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u3001KAN\u3001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u5206\u6570KAN\uff08FKAN\uff09\u5728\u8f6c\u5411\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u6bd4\u8f83\u7528\u6237\u7279\u5b9a\u6570\u636e\u4e0e\u5408\u5e76\u6570\u636e\u8bad\u7ec3\u7684\u6548\u679c\u3002", "result": "\u7ed3\u679c\u663e\u793aKAN\u548cFKAN\u7684\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u672a\u663e\u8457\u4f18\u4e8e\u4f20\u7edfMLP\u548cCNN\u3002\u7528\u6237\u7279\u5b9a\u6570\u636e\u5728ML\u6a21\u578b\u4e2d\u8868\u73b0\u66f4\u4f18\uff08p < 0.05\uff09\uff0c\u4f46DL\u6a21\u578b\u4e2d\u4e24\u8005\u65e0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u53ef\u80fd\u5728\u66f4\u590d\u6742\u4efb\u52a1\u548c\u5927\u6570\u636e\u96c6\u4e2d\u5c55\u73b0\u4f18\u52bf\uff0c\u4e14DL\u6a21\u578b\u53ef\u5229\u7528\u5408\u5e76\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u4e3a\u5047\u80a2\u63a7\u5236\u7684\u6570\u636e\u91c7\u96c6\u63d0\u4f9b\u4e86\u7075\u6d3b\u6027\u3002"}}
{"id": "2505.08849", "pdf": "https://arxiv.org/pdf/2505.08849", "abs": "https://arxiv.org/abs/2505.08849", "authors": ["Keyu Chen", "Hao Tang", "Qinglin Liu", "Yizhao Xu"], "title": "Improved Algorithms for Differentially Private Language Model Alignment", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Language model alignment is crucial for ensuring that large language models\n(LLMs) align with human preferences, yet it often involves sensitive user data,\nraising significant privacy concerns. While prior work has integrated\ndifferential privacy (DP) with alignment techniques, their performance remains\nlimited. In this paper, we propose novel algorithms for privacy-preserving\nalignment and rigorously analyze their effectiveness across varying privacy\nbudgets and models. Our framework can be deployed on two celebrated alignment\ntechniques, namely direct preference optimization (DPO) and reinforcement\nlearning from human feedback (RLHF). Through systematic experiments on\nlarge-scale language models, we demonstrate that our approach achieves\nstate-of-the-art performance. Notably, one of our algorithms, DP-AdamW,\ncombined with DPO, surpasses existing methods, improving alignment quality by\nup to 15% under moderate privacy budgets ({\\epsilon}=2-5). We further\ninvestigate the interplay between privacy guarantees, alignment efficacy, and\ncomputational demands, providing practical guidelines for optimizing these\ntrade-offs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u6846\u67b6\uff0c\u7ed3\u5408DPO\u548cRLHF\uff0c\u5728\u9690\u79c1\u9884\u7b97\u9002\u4e2d\u65f6\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5bf9\u9f50\u8d28\u91cf\u63d0\u534715%\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u5e38\u6d89\u53ca\u654f\u611f\u6570\u636e\uff0c\u73b0\u6709\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u7684\u65b9\u6cd5\u6027\u80fd\u6709\u9650\uff0c\u9700\u6539\u8fdb\u9690\u79c1\u4fdd\u62a4\u548c\u6027\u80fd\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u65b0\u7b97\u6cd5\uff08\u5982DP-AdamW\uff09\u5e76\u4e0eDPO\u548cRLHF\u7ed3\u5408\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u9690\u79c1\u9884\u7b97\u4e0b\u7684\u6548\u679c\u4e0e\u8ba1\u7b97\u6210\u672c\u3002", "result": "DP-AdamW+DPO\u5728\u03b5=2-5\u65f6\uff0c\u5bf9\u9f50\u8d28\u91cf\u63d0\u534715%\uff0c\u4e14\u5728\u9690\u79c1\u4fdd\u969c\u3001\u5bf9\u9f50\u6548\u679c\u4e0e\u8ba1\u7b97\u9700\u6c42\u95f4\u63d0\u4f9b\u4e86\u4f18\u5316\u6307\u5bfc\u3002", "conclusion": "\u65b0\u6846\u67b6\u5728\u9690\u79c1\u4fdd\u62a4\u4e0e\u6027\u80fd\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.09427", "pdf": "https://arxiv.org/pdf/2505.09427", "abs": "https://arxiv.org/abs/2505.09427", "authors": ["Achref Doula", "Max M\u00fchl\u00e4user", "Alejandro Sanchez Guinea"], "title": "SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Large Language Models (LLMs) show growing promise in autonomous driving by\nreasoning over complex traffic scenarios to generate path plans. However, their\ntendencies toward overconfidence, and hallucinations raise critical safety\nconcerns. We introduce SafePath, a modular framework that augments LLM-based\npath planning with formal safety guarantees using conformal prediction.\nSafePath operates in three stages. In the first stage, we use an LLM that\ngenerates a set of diverse candidate paths, exploring possible trajectories\nbased on agent behaviors and environmental cues. In the second stage, SafePath\nfilters out high-risk trajectories while guaranteeing that at least one safe\noption is included with a user-defined probability, through a multiple-choice\nquestion-answering formulation that integrates conformal prediction. In the\nfinal stage, our approach selects the path with the lowest expected collision\nrisk when uncertainty is low or delegates control to a human when uncertainty\nis high. We theoretically prove that SafePath guarantees a safe trajectory with\na user-defined probability, and we show how its human delegation rate can be\ntuned to balance autonomy and safety. Extensive experiments on nuScenes and\nHighway-env show that SafePath reduces planning uncertainty by 77\\% and\ncollision rates by up to 70\\%, demonstrating effectiveness in making LLM-driven\npath planning more safer.", "AI": {"tldr": "SafePath\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u9002\u5e94\u6027\u9884\u6d4b\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u5b89\u5168\u4fdd\u8bc1\uff0c\u663e\u8457\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\u548c\u78b0\u649e\u7387\u3002", "motivation": "LLM\u5728\u81ea\u52a8\u9a7e\u9a76\u8def\u5f84\u89c4\u5212\u4e2d\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u5f15\u53d1\u5b89\u5168\u9690\u60a3\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6846\u67b6\uff1a\u751f\u6210\u5019\u9009\u8def\u5f84\u3001\u901a\u8fc7\u9002\u5e94\u6027\u9884\u6d4b\u8fc7\u6ee4\u9ad8\u98ce\u9669\u8f68\u8ff9\u3001\u6839\u636e\u4e0d\u786e\u5b9a\u6027\u9009\u62e9\u5b89\u5168\u8def\u5f84\u6216\u4eba\u5de5\u63a5\u7ba1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cSafePath\u5c06\u89c4\u5212\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e77%\uff0c\u78b0\u649e\u7387\u51cf\u5c11\u9ad8\u8fbe70%\u3002", "conclusion": "SafePath\u6709\u6548\u63d0\u5347\u4e86LLM\u8def\u5f84\u89c4\u5212\u7684\u5b89\u5168\u6027\uff0c\u5e76\u5728\u81ea\u4e3b\u6027\u4e0e\u5b89\u5168\u6027\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2505.08854", "pdf": "https://arxiv.org/pdf/2505.08854", "abs": "https://arxiv.org/abs/2505.08854", "authors": ["Yuping Wang", "Shuo Xing", "Cui Can", "Renjie Li", "Hongyuan Hua", "Kexin Tian", "Zhaobin Mo", "Xiangbo Gao", "Keshu Wu", "Sulong Zhou", "Hengxu You", "Juntong Peng", "Junge Zhang", "Zehao Wang", "Rui Song", "Mingxuan Yan", "Walter Zimmer", "Xingcheng Zhou", "Peiran Li", "Zhaohan Lu", "Chia-Ju Chen", "Yue Huang", "Ryan A. Rossi", "Lichao Sun", "Hongkai Yu", "Zhiwen Fan", "Frank Hao Yang", "Yuhao Kang", "Ross Greer", "Chenxi Liu", "Eun Hak Lee", "Xuan Di", "Xinyue Ye", "Liu Ren", "Alois Knoll", "Xiaopeng Li", "Shuiwang Ji", "Masayoshi Tomizuka", "Marco Pavone", "Tianbao Yang", "Jing Du", "Ming-Hsuan Yang", "Hua Wei", "Ziran Wang", "Yang Zhou", "Jiachen Li", "Zhengzhong Tu"], "title": "Generative AI for Autonomous Driving: Frontiers and Opportunities", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Generative Artificial Intelligence (GenAI) constitutes a transformative\ntechnological wave that reconfigures industries through its unparalleled\ncapabilities for content creation, reasoning, planning, and multimodal\nunderstanding. This revolutionary force offers the most promising path yet\ntoward solving one of engineering's grandest challenges: achieving reliable,\nfully autonomous driving, particularly the pursuit of Level 5 autonomy. This\nsurvey delivers a comprehensive and critical synthesis of the emerging role of\nGenAI across the autonomous driving stack. We begin by distilling the\nprinciples and trade-offs of modern generative modeling, encompassing VAEs,\nGANs, Diffusion Models, and Large Language Models (LLMs). We then map their\nfrontier applications in image, LiDAR, trajectory, occupancy, video generation\nas well as LLM-guided reasoning and decision making. We categorize practical\napplications, such as synthetic data workflows, end-to-end driving strategies,\nhigh-fidelity digital twin systems, smart transportation networks, and\ncross-domain transfer to embodied AI. We identify key obstacles and\npossibilities such as comprehensive generalization across rare cases,\nevaluation and safety checks, budget-limited implementation, regulatory\ncompliance, ethical concerns, and environmental effects, while proposing\nresearch plans across theoretical assurances, trust metrics, transport\nintegration, and socio-technical influence. By unifying these threads, the\nsurvey provides a forward-looking reference for researchers, engineers, and\npolicymakers navigating the convergence of generative AI and advanced\nautonomous mobility. An actively maintained repository of cited works is\navailable at https://github.com/taco-group/GenAI4AD.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u5e94\u7528\uff0c\u5305\u62ec\u5176\u539f\u7406\u3001\u524d\u6cbf\u6280\u672f\u3001\u5b9e\u9645\u5e94\u7528\u3001\u6311\u6218\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u63a2\u8ba8GenAI\u5982\u4f55\u901a\u8fc7\u5176\u5728\u5185\u5bb9\u751f\u6210\u3001\u63a8\u7406\u548c\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63a8\u52a8\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u5411Level 5\u5168\u81ea\u4e3b\u9a7e\u9a76\u7684\u7a81\u7834\u3002", "method": "\u5206\u6790\u4e86\u751f\u6210\u6a21\u578b\uff08\u5982VAEs\u3001GANs\u3001Diffusion Models\u548cLLMs\uff09\u7684\u539f\u7406\u53ca\u5176\u5728\u56fe\u50cf\u3001LiDAR\u3001\u8f68\u8ff9\u7b49\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5206\u7c7b\u8ba8\u8bba\u4e86\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\u3002", "result": "\u603b\u7ed3\u4e86GenAI\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u524d\u6cbf\u5e94\u7528\u53ca\u5173\u952e\u6311\u6218\uff0c\u5982\u6cdb\u5316\u80fd\u529b\u3001\u5b89\u5168\u6027\u3001\u9884\u7b97\u9650\u5236\u548c\u4f26\u7406\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "GenAI\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7684\u7a81\u7834\u63d0\u4f9b\u4e86\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u591a\u65b9\u9762\u7684\u6280\u672f\u548c\u975e\u6280\u672f\u6311\u6218\u624d\u80fd\u5b9e\u73b0\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2505.09432", "pdf": "https://arxiv.org/pdf/2505.09432", "abs": "https://arxiv.org/abs/2505.09432", "authors": ["Yuzhou Cao", "Han Bao", "Lei Feng", "Bo An"], "title": "Establishing Linear Surrogate Regret Bounds for Convex Smooth Losses via Convolutional Fenche-Young Losses", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Surrogate regret bounds bridge the gap between the convergence rates of\nsurrogate and target losses, with linear bounds favorable for their lossless\nregret transfer. While convex smooth surrogate losses are appealing in\nparticular due to the efficient estimation and optimization, the existence of a\ntrade-off between the smoothness and linear regret bound has been believed in\nthe community. That being said, the better optimization and estimation\nproperties of convex smooth surrogate losses may inevitably deteriorate after\nundergoing the regret transfer onto a target loss. We overcome this dilemma for\narbitrary discrete target losses by constructing a convex smooth surrogate\nloss, which entails a linear surrogate regret bound composed with a tailored\nprediction link. The construction is based on Fenchel-Young losses generated by\nthe convolutional negentropy, which are equivalent to the infimal convolution\nof a generalized negentropy and the target Bayes risk. Consequently, the\ninfimal convolution enables us to derive a smooth loss while maintaining the\nsurrogate regret bound linear. We additionally benefit from the infimal\nconvolution to have a consistent estimator of the underlying class probability.\nOur results are overall a novel demonstration of how convex analysis penetrates\ninto optimization and statistical efficiency in risk minimization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u51f8\u5e73\u6ed1\u4ee3\u7406\u635f\u5931\u51fd\u6570\uff0c\u9488\u5bf9\u4efb\u610f\u79bb\u6563\u76ee\u6807\u635f\u5931\u5b9e\u73b0\u4e86\u7ebf\u6027\u4ee3\u7406\u9057\u61be\u754c\uff0c\u89e3\u51b3\u4e86\u5e73\u6ed1\u6027\u4e0e\u9057\u61be\u754c\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u9488\u5bf9\u51f8\u5e73\u6ed1\u4ee3\u7406\u635f\u5931\u51fd\u6570\u5728\u9057\u61be\u4f20\u9012\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u4f18\u5316\u4e0e\u4f30\u8ba1\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u7814\u7a76\u5982\u4f55\u4fdd\u6301\u5e73\u6ed1\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u7ebf\u6027\u9057\u61be\u754c\u3002", "method": "\u57fa\u4e8eFenchel-Young\u635f\u5931\u548c\u5377\u79ef\u8d1f\u71b5\uff0c\u6784\u9020\u4e86\u51f8\u5e73\u6ed1\u4ee3\u7406\u635f\u5931\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u9884\u6d4b\u94fe\u63a5\u8bbe\u8ba1\u4fdd\u6301\u7ebf\u6027\u9057\u61be\u754c\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u79cd\u51f8\u5e73\u6ed1\u4ee3\u7406\u635f\u5931\u51fd\u6570\uff0c\u5176\u7ebf\u6027\u9057\u61be\u754c\u5f97\u4ee5\u4fdd\u7559\uff0c\u540c\u65f6\u8fd8\u5b9e\u73b0\u4e86\u57fa\u7840\u7c7b\u522b\u6982\u7387\u7684\u4e00\u81f4\u6027\u4f30\u8ba1\u3002", "conclusion": "\u901a\u8fc7\u51f8\u5206\u6790\u5728\u98ce\u9669\u6700\u5c0f\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u4f18\u5316\u4e0e\u7edf\u8ba1\u6548\u7387\u7684\u7ed3\u5408\uff0c\u4e3a\u89e3\u51b3\u7c7b\u4f3c\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.08878", "pdf": "https://arxiv.org/pdf/2505.08878", "abs": "https://arxiv.org/abs/2505.08878", "authors": ["Dor Tsur", "Carol Xuan Long", "Claudio Mayrink Verdun", "Hsiang Hsu", "Haim Permuter", "Flavio P. Calmon"], "title": "Optimized Couplings for Watermarking Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.IT", "math.IT"], "comment": "Accepted at ISIT25", "summary": "Large-language models (LLMs) are now able to produce text that is, in many\ncases, seemingly indistinguishable from human-generated content. This has\nfueled the development of watermarks that imprint a ``signal'' in LLM-generated\ntext with minimal perturbation of an LLM's output. This paper provides an\nanalysis of text watermarking in a one-shot setting. Through the lens of\nhypothesis testing with side information, we formulate and analyze the\nfundamental trade-off between watermark detection power and distortion in\ngenerated textual quality. We argue that a key component in watermark design is\ngenerating a coupling between the side information shared with the watermark\ndetector and a random partition of the LLM vocabulary. Our analysis identifies\nthe optimal coupling and randomization strategy under the worst-case LLM\nnext-token distribution that satisfies a min-entropy constraint. We provide a\nclosed-form expression of the resulting detection rate under the proposed\nscheme and quantify the cost in a max-min sense. Finally, we provide an array\nof numerical results, comparing the proposed scheme with the theoretical\noptimum and existing schemes, in both synthetic data and LLM watermarking. Our\ncode is available at https://github.com/Carol-Long/CC_Watermark", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u6587\u672c\u7684\u6c34\u5370\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5355\u6b21\u8bbe\u7f6e\u4e0b\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u901a\u8fc7\u5047\u8bbe\u68c0\u9a8c\u548c\u4fa7\u4fe1\u606f\u4f18\u5316\u68c0\u6d4b\u80fd\u529b\u4e0e\u6587\u672c\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u968f\u7740LLM\u751f\u6210\u6587\u672c\u8d8a\u6765\u8d8a\u96be\u4ee5\u4e0e\u4eba\u7c7b\u751f\u6210\u5185\u5bb9\u533a\u5206\uff0c\u9700\u8981\u5f00\u53d1\u6c34\u5370\u6280\u672f\u4ee5\u5728\u4e0d\u663e\u8457\u6539\u53d8\u8f93\u51fa\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u6807\u8bb0LLM\u751f\u6210\u5185\u5bb9\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5047\u8bbe\u68c0\u9a8c\u548c\u4fa7\u4fe1\u606f\u7684\u89c6\u89d2\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u68c0\u6d4b\u5668\u5171\u4eab\u7684\u4fa7\u4fe1\u606f\u4e0eLLM\u8bcd\u6c47\u8868\u968f\u673a\u5206\u533a\u8026\u5408\u7684\u6c34\u5370\u8bbe\u8ba1\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u5f97\u51fa\u4e86\u5728\u6700\u5c0f\u71b5\u7ea6\u675f\u4e0b\u7684\u6700\u4f18\u8026\u5408\u548c\u968f\u673a\u5316\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u548c\u5b9e\u9645LLM\u6c34\u5370\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u6c34\u5370\u65b9\u6848\u5728\u68c0\u6d4b\u80fd\u529b\u548c\u6587\u672c\u8d28\u91cf\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6700\u4f18\u6743\u8861\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2505.08894", "pdf": "https://arxiv.org/pdf/2505.08894", "abs": "https://arxiv.org/abs/2505.08894", "authors": ["Hiba Eltigani", "Rukhshan Haroon", "Asli Kocak", "Abdullah Bin Faisal", "Noah Martin", "Fahad Dogar"], "title": "WaLLM -- Insights from an LLM-Powered Chatbot deployment via WhatsApp", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Recent advances in generative AI, such as ChatGPT, have transformed access to\ninformation in education, knowledge-seeking, and everyday decision-making.\nHowever, in many developing regions, access remains a challenge due to the\npersistent digital divide. To help bridge this gap, we developed WaLLM - a\ncustom AI chatbot over WhatsApp, a widely used communication platform in\ndeveloping regions. Beyond answering queries, WaLLM offers several features to\nenhance user engagement: a daily top question, suggested follow-up questions,\ntrending and recent queries, and a leaderboard-based reward system. Our service\nhas been operational for over 6 months, amassing over 14.7K queries from\napproximately 100 users. In this paper, we present WaLLM's design and a\nsystematic analysis of logs to understand user interactions. Our results show\nthat 55% of user queries seek factual information. \"Health and well-being\" was\nthe most popular topic (28%), including queries about nutrition and disease,\nsuggesting users view WaLLM as a reliable source. Two-thirds of users' activity\noccurred within 24 hours of the daily top question. Users who accessed the\n\"Leaderboard\" interacted with WaLLM 3x as those who did not. We conclude by\ndiscussing implications for culture-based customization, user interface design,\nand appropriate calibration of users' trust in AI systems for developing\nregions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86WaLLM\uff0c\u4e00\u4e2a\u57fa\u4e8eWhatsApp\u7684AI\u804a\u5929\u673a\u5668\u4eba\uff0c\u65e8\u5728\u89e3\u51b3\u53d1\u5c55\u4e2d\u5730\u533a\u7684\u6570\u5b57\u9e3f\u6c9f\u95ee\u9898\u3002\u5b83\u63d0\u4f9b\u591a\u79cd\u4e92\u52a8\u529f\u80fd\uff0c6\u4e2a\u6708\u5185\u6536\u96c6\u4e8614.7K\u67e5\u8be2\uff0c\u5206\u6790\u663e\u793a55%\u4e3a\u4e8b\u5b9e\u67e5\u8be2\uff0c\u5065\u5eb7\u7c7b\u6700\u53d7\u6b22\u8fce\u3002", "motivation": "\u53d1\u5c55\u4e2d\u5730\u533a\u56e0\u6570\u5b57\u9e3f\u6c9f\u96be\u4ee5\u63a5\u89e6\u751f\u6210\u5f0fAI\u6280\u672f\uff0cWaLLM\u901a\u8fc7\u666e\u53ca\u7684WhatsApp\u5e73\u53f0\u63d0\u4f9b\u4fbf\u6377\u7684AI\u670d\u52a1\u3002", "method": "\u5f00\u53d1WaLLM\u5e76\u96c6\u6210\u6bcf\u65e5\u70ed\u70b9\u95ee\u9898\u3001\u5efa\u8bae\u8ffd\u95ee\u3001\u67e5\u8be2\u6392\u884c\u53ca\u5956\u52b1\u7cfb\u7edf\uff0c\u5206\u67906\u4e2a\u6708\u7684\u7528\u6237\u65e5\u5fd7\u3002", "result": "55%\u67e5\u8be2\u4e3a\u4e8b\u5b9e\u9700\u6c42\uff0c\u5065\u5eb7\u4e3b\u9898\u536028%\uff1b\u4f7f\u7528\u6392\u884c\u699c\u7684\u7528\u6237\u4e92\u52a8\u91cf\u9ad83\u500d\u3002", "conclusion": "\u9700\u6587\u5316\u5b9a\u5236\u3001\u754c\u9762\u4f18\u5316\u53ca\u5408\u7406\u6821\u51c6\u7528\u6237\u5bf9AI\u7684\u4fe1\u4efb\uff0c\u4ee5\u9002\u914d\u53d1\u5c55\u4e2d\u5730\u533a\u9700\u6c42\u3002"}}
{"id": "2505.09458", "pdf": "https://arxiv.org/pdf/2505.09458", "abs": "https://arxiv.org/abs/2505.09458", "authors": ["Jad Mounayer", "Alicia Tierz", "Jerome Tomezyk", "Chady Ghnatios", "Francisco Chinesta"], "title": "Variational Rank Reduction Autoencoder", "categories": ["cs.LG"], "comment": null, "summary": "Deterministic Rank Reduction Autoencoders (RRAEs) enforce by construction a\nregularization on the latent space by applying a truncated SVD. While this\nregularization makes Autoencoders more powerful, using them for generative\npurposes is counter-intuitive due to their deterministic nature. On the other\nhand, Variational Autoencoders (VAEs) are well known for their generative\nabilities by learning a probabilistic latent space. In this paper, we present\nVariational Rank Reduction Autoencoders (VRRAEs), a model that leverages the\nadvantages of both RRAEs and VAEs. Our claims and results show that when\ncarefully sampling the latent space of RRAEs and further regularizing with the\nKullback-Leibler (KL) divergence (similarly to VAEs), VRRAEs outperform RRAEs\nand VAEs. Additionally, we show that the regularization induced by the SVD not\nonly makes VRRAEs better generators than VAEs, but also reduces the possibility\nof posterior collapse. Our results include a synthetic dataset of a small size\nthat showcases the robustness of VRRAEs against collapse, and three real-world\ndatasets; the MNIST, CelebA, and CIFAR-10, over which VRRAEs are shown to\noutperform both VAEs and RRAEs on many random generation and interpolation\ntasks based on the FID score.", "AI": {"tldr": "Variational Rank Reduction Autoencoders (VRRAEs) combine the strengths of deterministic Rank Reduction Autoencoders (RRAEs) and Variational Autoencoders (VAEs) to improve generative performance and avoid posterior collapse.", "motivation": "To address the limitations of RRAEs (deterministic nature) and VAEs (posterior collapse) by integrating their advantages into a single model.", "method": "VRRAEs sample the latent space of RRAEs and regularize it with KL divergence, leveraging truncated SVD for additional regularization.", "result": "VRRAEs outperform RRAEs and VAEs in random generation and interpolation tasks on MNIST, CelebA, and CIFAR-10 datasets, measured by FID scores.", "conclusion": "The combination of SVD regularization and probabilistic sampling in VRRAEs enhances generative performance and mitigates posterior collapse."}}
{"id": "2505.09486", "pdf": "https://arxiv.org/pdf/2505.09486", "abs": "https://arxiv.org/abs/2505.09486", "authors": ["Seyed Roozbeh Razavi Rohani", "Khashayar Khajavi", "Wesley Chung", "Mo Chen", "Sharan Vaswani"], "title": "Preserving Plasticity in Continual Learning with Adaptive Linearity Injection", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted in 4th Conference on Lifelong Learning Agents (CoLLAs), 2025", "summary": "Loss of plasticity in deep neural networks is the gradual reduction in a\nmodel's capacity to incrementally learn and has been identified as a key\nobstacle to learning in non-stationary problem settings. Recent work has shown\nthat deep linear networks tend to be resilient towards loss of plasticity.\nMotivated by this observation, we propose Adaptive Linearization (AdaLin), a\ngeneral approach that dynamically adapts each neuron's activation function to\nmitigate plasticity loss. Unlike prior methods that rely on regularization or\nperiodic resets, AdaLin equips every neuron with a learnable parameter and a\ngating mechanism that injects linearity into the activation function based on\nits gradient flow. This adaptive modulation ensures sufficient gradient signal\nand sustains continual learning without introducing additional hyperparameters\nor requiring explicit task boundaries. When used with conventional activation\nfunctions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can\nsignificantly improve performance on standard benchmarks, including Random\nLabel and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split\nCIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such\nas class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in\nmitigating plasticity loss in off-policy reinforcement learning agents. We\nperform a systematic set of ablations that show that neuron-level adaptation is\ncrucial for good performance and analyze a number of metrics in the network\nthat might be correlated to loss of plasticity.", "AI": {"tldr": "AdaLin\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u795e\u7ecf\u5143\u6fc0\u6d3b\u51fd\u6570\u7684\u7ebf\u6027\u5316\u7a0b\u5ea6\uff0c\u6709\u6548\u7f13\u89e3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u5851\u6027\u635f\u5931\uff0c\u65e0\u9700\u989d\u5916\u8d85\u53c2\u6570\u6216\u4efb\u52a1\u8fb9\u754c\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u975e\u7a33\u6001\u95ee\u9898\u4e2d\u5bb9\u6613\u56e0\u53ef\u5851\u6027\u635f\u5931\u5f71\u54cd\u6301\u7eed\u5b66\u4e60\u80fd\u529b\uff0cAdaLin\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u81ea\u9002\u5e94\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faAdaLin\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u53c2\u6570\u548c\u95e8\u63a7\u673a\u5236\u52a8\u6001\u8c03\u8282\u6fc0\u6d3b\u51fd\u6570\u7684\u7ebf\u6027\u5316\u7a0b\u5ea6\uff0c\u4ee5\u4fdd\u6301\u68af\u5ea6\u6d41\u52a8\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982MNIST\u3001CIFAR\uff09\u548c\u590d\u6742\u573a\u666f\uff08\u5982\u589e\u91cf\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u795e\u7ecf\u5143\u7ea7\u9002\u5e94\u662f\u5173\u952e\u3002", "conclusion": "AdaLin\u663e\u8457\u63d0\u5347\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u80fd\u529b\uff0c\u795e\u7ecf\u5143\u7ea7\u9002\u5e94\u662f\u6838\u5fc3\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u4efb\u52a1\u3002"}}
{"id": "2505.08904", "pdf": "https://arxiv.org/pdf/2505.08904", "abs": "https://arxiv.org/abs/2505.08904", "authors": ["Varun Nagaraj Rao", "Samantha Dalal", "Andrew Schwartz", "Amna Liaqat", "Dana Calacci", "Andr\u00e9s Monroy-Hern\u00e1ndez"], "title": "FareShare: A Tool for Labor Organizers to Estimate Lost Wages and Contest Arbitrary AI and Algorithmic Deactivations", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC"], "comment": null, "summary": "What happens when a rideshare driver is suddenly locked out of the platform\nconnecting them to riders, wages, and daily work? Deactivation-the abrupt\nremoval of gig workers' platform access-typically occurs through arbitrary AI\nand algorithmic decisions with little explanation or recourse. This represents\none of the most severe forms of algorithmic control and often devastates\nworkers' financial stability. Recent U.S. state policies now mandate appeals\nprocesses and recovering compensation during the period of wrongful\ndeactivation based on past earnings. Yet, labor organizers still lack effective\ntools to support these complex, error-prone workflows. We designed FareShare, a\ncomputational tool automating lost wage estimation for deactivated drivers,\nthrough a 6 month partnership with the State of Washington's largest rideshare\nlabor union. Over the following 3 months, our field deployment of FareShare\nregistered 178 account signups. We observed that the tool could reduce lost\nwage calculation time by over 95%, eliminate manual data entry errors, and\nenable legal teams to generate arbitration-ready reports more efficiently.\nBeyond these gains, the deployment also surfaced important socio-technical\nchallenges around trust, consent, and tool adoption in high-stakes labor\ncontexts.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aFareShare\u7684\u5de5\u5177\uff0c\u5e2e\u52a9\u88ab\u5e73\u53f0\u7a81\u7136\u5c01\u7981\u7684\u5171\u4eab\u4e58\u8f66\u53f8\u673a\u81ea\u52a8\u8ba1\u7b97\u5de5\u8d44\u635f\u5931\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5171\u4eab\u4e58\u8f66\u53f8\u673a\u88ab\u5e73\u53f0\u7a81\u7136\u5c01\u7981\uff08\u901a\u8fc7\u7b97\u6cd5\u51b3\u7b56\uff09\u4f1a\u4e25\u91cd\u5f71\u54cd\u5176\u7ecf\u6d4e\u72b6\u51b5\uff0c\u800c\u73b0\u6709\u7533\u8bc9\u6d41\u7a0b\u548c\u5de5\u8d44\u6062\u590d\u673a\u5236\u7f3a\u4e4f\u9ad8\u6548\u5de5\u5177\u652f\u6301\u3002", "method": "\u901a\u8fc7\u4e3a\u671f6\u4e2a\u6708\u7684\u5408\u4f5c\uff0c\u8bbe\u8ba1\u4e86\u81ea\u52a8\u5316\u8ba1\u7b97\u5de5\u8d44\u635f\u5931\u7684\u5de5\u5177FareShare\uff0c\u5e76\u57283\u4e2a\u6708\u7684\u5b9e\u5730\u90e8\u7f72\u4e2d\u6d4b\u8bd5\u5176\u6548\u679c\u3002", "result": "FareShare\u5c06\u5de5\u8d44\u635f\u5931\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u4e8695%\u4ee5\u4e0a\uff0c\u6d88\u9664\u4e86\u4eba\u5de5\u6570\u636e\u8f93\u5165\u9519\u8bef\uff0c\u5e76\u63d0\u9ad8\u4e86\u6cd5\u5f8b\u56e2\u961f\u751f\u6210\u4ef2\u88c1\u62a5\u544a\u7684\u6548\u7387\u3002", "conclusion": "\u5c3d\u7ba1FareShare\u6548\u679c\u663e\u8457\uff0c\u4f46\u5728\u9ad8\u98ce\u9669\u7684\u52b3\u52a8\u60c5\u5883\u4e2d\uff0c\u5de5\u5177\u7684\u4fe1\u4efb\u3001\u540c\u610f\u548c\u91c7\u7528\u4ecd\u9762\u4e34\u6311\u6218\u3002"}}
{"id": "2505.09500", "pdf": "https://arxiv.org/pdf/2505.09500", "abs": "https://arxiv.org/abs/2505.09500", "authors": ["Timothy Qian", "Vinith Suriyakumar", "Ashia Wilson", "Dylan Hadfield-Menell"], "title": "Layered Unlearning for Adversarial Relearning", "categories": ["cs.LG"], "comment": "37 pages, 8 figures", "summary": "Our goal is to understand how post-training methods, such as fine-tuning,\nalignment, and unlearning, modify language model behavior and representations.\nWe are particularly interested in the brittle nature of these modifications\nthat makes them easy to bypass through prompt engineering or relearning. Recent\nresults suggest that post-training induces shallow context-dependent\n``circuits'' that suppress specific response patterns. This could be one\nexplanation for the brittleness of post-training. To test this hypothesis, we\ndesign an unlearning algorithm, Layered Unlearning (LU), that creates distinct\ninhibitory mechanisms for a growing subset of the data. By unlearning the first\n$i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU\nlimits the ability of relearning on a subset of data to recover the full\ndataset. We evaluate LU through a combination of synthetic and large language\nmodel (LLM) experiments. We find that LU improves robustness to adversarial\nrelearning for several different unlearning methods. Our results contribute to\nthe state-of-the-art of machine unlearning and provide insight into the effect\nof post-training updates.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u540e\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982\u5fae\u8c03\u3001\u5bf9\u9f50\u548c\u9057\u5fd8\uff09\u5982\u4f55\u6539\u53d8\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u548c\u8868\u793a\uff0c\u5e76\u63d0\u51fa\u5206\u5c42\u9057\u5fd8\uff08LU\uff09\u8fd9\u4e00\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u5bf9\u5bf9\u6297\u6027\u518d\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u7406\u89e3\u540e\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982\u5fae\u8c03\u3001\u5bf9\u9f50\u548c\u9057\u5fd8\uff09\u5982\u4f55\u4fee\u6539\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\u548c\u8868\u793a\uff0c\u5e76\u63a2\u7d22\u5176\u8106\u5f31\u6027\u7684\u539f\u56e0\uff0c\u5c24\u5176\u662f\u5982\u4f55\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u6216\u518d\u5b66\u4e60\u7ed5\u8fc7\u8fd9\u4e9b\u4fee\u6539\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u9057\u5fd8\uff08LU\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u9057\u5fd8\u6570\u636e\u96c6\u4e2d\u7684\u4e0d\u540c\u5b50\u96c6\u6765\u51cf\u5c11\u518d\u5b66\u4e60\u5bf9\u5b8c\u6574\u6570\u636e\u7684\u6062\u590d\u80fd\u529b\uff0c\u5e76\u5728\u5408\u6210\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "LU\u65b9\u6cd5\u63d0\u9ad8\u4e86\u591a\u79cd\u4e0d\u540c\u9057\u5fd8\u65b9\u6cd5\u5bf9\u5bf9\u6297\u6027\u518d\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "LU\u63d0\u5347\u4e86\u673a\u5668\u9057\u5fd8\u7684\u524d\u6cbf\u6c34\u5e73\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u540e\u8bad\u7ec3\u66f4\u65b0\u6548\u679c\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.08916", "pdf": "https://arxiv.org/pdf/2505.08916", "abs": "https://arxiv.org/abs/2505.08916", "authors": ["Chan Le Duc", "Ludovic Brieulle"], "title": "A New Tractable Description Logic under Categorical Semantics", "categories": ["cs.LO", "cs.AI"], "comment": null, "summary": "Biomedical ontologies contain numerous concept or role names involving\nnegative knowledge such as lacks_part, absence_of. Such a representation with\nlabels rather than logical constructors would not allow a reasoner to interpret\nlacks_part as a kind of negation of has_part. It is known that adding negation\nto the tractable Description Logic (DL) EL allowing for conjunction,\nexistential restriction and concept inclusion makes it intractable since the\nobtained logic includes implicitly disjunction and universal restriction which\ninteract with other constructors. In this paper, we propose a new extension of\nEL with a weakened negation allowing to represent negative knowledge while\nretaining tractability. To this end, we introduce categorical semantics of all\nlogical constructors of the DL SH including EL with disjunction, negation,\nuniversal restriction, role inclusion and transitive roles. The categorical\nsemantics of a logical constructor is usually described as a set of categorical\nproperties referring to several objects without using set membership. To\nrestore tractability, we have to weaken semantics of disjunction and universal\nrestriction by identifying \\emph{independent} categorical properties that are\nresponsible for intractability, and dropping them from the set of categorical\nproperties. We show that the logic resulting from weakening semantics is more\nexpressive than EL with the bottom concept, transitive roles and role\ninclusion.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4fdd\u6301\u53ef\u5904\u7406\u6027\u7684\u524d\u63d0\u4e0b\u6269\u5c55EL\u63cf\u8ff0\u903b\u8f91\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5f31\u5316\u7684\u5426\u5b9a\u6765\u8868\u793a\u8d1f\u9762\u77e5\u8bc6\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u672c\u4f53\u4e2d\u5e38\u7528\u8d1f\u9762\u77e5\u8bc6\u8868\u793a\uff08\u5982\u7f3a\u4e4f\u90e8\u5206\uff09\uff0c\u4f46\u76f4\u63a5\u7528\u6807\u7b7e\u800c\u975e\u903b\u8f91\u6784\u9020\u5668\u4f1a\u5bfc\u81f4\u63a8\u7406\u5668\u65e0\u6cd5\u7406\u89e3\u5176\u903b\u8f91\u542b\u4e49\u3002\u73b0\u6709\u65b9\u6cd5\u6dfb\u52a0\u5426\u5b9a\u4f1a\u4f7fEL\u903b\u8f91\u53d8\u5f97\u4e0d\u53ef\u5904\u7406\u3002", "method": "\u5f15\u5165SH\u63cf\u8ff0\u903b\u8f91\u7684\u8303\u7574\u8bed\u4e49\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u5254\u9664\u5bfc\u81f4\u4e0d\u53ef\u5904\u7406\u6027\u7684\u72ec\u7acb\u8303\u7574\u5c5e\u6027\uff0c\u5f31\u5316\u6790\u53d6\u548c\u5168\u79f0\u9650\u5236\u7684\u8bed\u4e49\uff0c\u4ece\u800c\u4fdd\u7559\u53ef\u5904\u7406\u6027\u3002", "result": "\u65b0\u903b\u8f91\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u5f3a\u4e8e\u5e26\u5e95\u90e8\u6982\u5ff5\u3001\u4f20\u9012\u89d2\u8272\u548c\u89d2\u8272\u5305\u542b\u7684EL\u903b\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u5904\u7406\u6027\u3002", "conclusion": "\u901a\u8fc7\u8bed\u4e49\u5f31\u5316\u5b9e\u73b0\u4e86\u8d1f\u9762\u77e5\u8bc6\u7684\u53ef\u5904\u7406\u8868\u793a\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u672c\u4f53\u7b49\u9886\u57df\u7684\u903b\u8f91\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2505.09503", "pdf": "https://arxiv.org/pdf/2505.09503", "abs": "https://arxiv.org/abs/2505.09503", "authors": ["Patrik Kenfack", "Samira Ebrahimi Kaho", "Ulrich A\u00efvodji"], "title": "Towards Fair In-Context Learning with Tabular Foundation Models", "categories": ["cs.LG"], "comment": "24 pages, 10 figures, 4 tables", "summary": "Tabular foundational models have exhibited strong in-context learning (ICL)\ncapabilities on structured data, allowing them to make accurate predictions on\ntest sets without parameter updates, using training examples as context. This\nemerging approach positions itself as a competitive alternative to traditional\ngradient-boosted tree methods. However, while biases in conventional machine\nlearning models are well documented, it remains unclear how these biases\nmanifest in tabular ICL. The paper investigates the fairness implications of\ntabular ICL and explores three preprocessing strategies--correlation removal,\ngroup-balanced demonstration selection, and uncertainty-based demonstration\nselection--to address bias. Comprehensive experiments indicate that\nuncertainty-based demonstration selection consistently enhances group fairness\nof in-context predictions. The source code for reproducing the results of this\nwork can be found at https://github.com/patrikken/Fair-TabICL.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8868\u683c\u57fa\u7840\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u9884\u5904\u7406\u7b56\u7565\uff0c\u53d1\u73b0\u4e0d\u786e\u5b9a\u6027\u7684\u6f14\u793a\u9009\u62e9\u80fd\u6709\u6548\u63d0\u5347\u516c\u5e73\u6027\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u504f\u5dee\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u8868\u683c\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u504f\u5dee\u8868\u73b0\u5c1a\u4e0d\u660e\u786e\uff0c\u8bba\u6587\u65e8\u5728\u63a2\u8ba8\u8fd9\u4e00\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u7814\u7a76\u4e86\u4e09\u79cd\u9884\u5904\u7406\u7b56\u7565\uff1a\u76f8\u5173\u6027\u53bb\u9664\u3001\u7ec4\u5e73\u8861\u6f14\u793a\u9009\u62e9\u548c\u4e0d\u786e\u5b9a\u6027\u6f14\u793a\u9009\u62e9\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u5176\u6548\u679c\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u6f14\u793a\u9009\u62e9\u80fd\u663e\u8457\u63d0\u5347\u4e0d\u540c\u7ec4\u7684\u9884\u6d4b\u516c\u5e73\u6027\u3002", "conclusion": "\u8868\u683c\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u516c\u5e73\u6027\u53ef\u901a\u8fc7\u9002\u5f53\u7684\u9884\u5904\u7406\u7b56\u7565\u6539\u8fdb\uff0c\u4e0d\u786e\u5b9a\u6027\u6f14\u793a\u9009\u62e9\u662f\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2505.08918", "pdf": "https://arxiv.org/pdf/2505.08918", "abs": "https://arxiv.org/abs/2505.08918", "authors": ["Marina Popova", "Iaroslav Chelombitko", "Aleksey Komissarov"], "title": "When repeats drive the vocabulary: a Byte-Pair Encoding analysis of T2T primate genomes", "categories": ["q-bio.GN", "cs.AI"], "comment": "ICLR 2025 Workshop on Machine Learning for Genomics Explorations", "summary": "The emergence of telomere-to-telomere (T2T) genome assemblies has opened new\navenues for comparative genomics, yet effective tokenization strategies for\ngenomic sequences remain underexplored. In this pilot study, we apply Byte Pair\nEncoding (BPE) to nine T2T primate genomes including three human assemblies by\ntraining independent BPE tokenizers with a fixed vocabulary of 512,000 tokens\nusing our custom tool, dnaBPE. Our analysis reveals that only 11,569 tokens are\nshared across all assemblies, while nearly 991,854 tokens are unique to a\nsingle genome, indicating a rapid decline in shared vocabulary with increasing\nassembly comparisons. Moreover, phylogenetic trees derived from token overlap\nfailed to recapitulate established primate relationships, a discrepancy\nattributed to the disproportionate influence of species-specific high-copy\nrepetitive elements. These findings underscore the dual nature of BPE\ntokenization: while it effectively compresses repetitive sequences, its\nsensitivity to high-copy elements limits its utility as a universal tool for\ncomparative genomics. We discuss potential hybrid strategies and repeat-masking\napproaches to refine genomic tokenization, emphasizing the need for\ndomain-specific adaptations in the development of large-scale genomic language\nmodels. The dnaBPE tool used in this study is open-source and available at\nhttps://github.com/aglabx/dnaBPE.", "AI": {"tldr": "\u4f7f\u7528Byte Pair Encoding (BPE)\u5bf9\u4e5d\u4e2aT2T\u7075\u957f\u7c7b\u57fa\u56e0\u7ec4\u8fdb\u884ctokenization\u5206\u6790\uff0c\u53d1\u73b0\u5171\u4eabtoken\u6781\u5c11\u4e14\u7cfb\u7edf\u53d1\u80b2\u6811\u672a\u91cd\u73b0\u5df2\u77e5\u5173\u7cfb\uff0c\u8868\u660eBPE\u5728\u6bd4\u8f83\u57fa\u56e0\u7ec4\u5b66\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "T2T\u57fa\u56e0\u7ec4\u7ec4\u88c5\u4e3a\u6bd4\u8f83\u57fa\u56e0\u7ec4\u5b66\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u57fa\u56e0\u7ec4\u5e8f\u5217\u7684\u6709\u6548tokenization\u7b56\u7565\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u81ea\u5b9a\u4e49\u5de5\u5177dnaBPE\u8bad\u7ec3\u4e5d\u4e2aT2T\u7075\u957f\u7c7b\u57fa\u56e0\u7ec4\u7684\u72ec\u7acbBPE tokenizer\uff0c\u56fa\u5b9a\u8bcd\u6c47\u91cf512,000\u3002", "result": "\u4ec5\u670911,569 token\u5728\u6240\u6709\u57fa\u56e0\u7ec4\u4e2d\u5171\u4eab\uff0c\u7cfb\u7edf\u53d1\u80b2\u6811\u4e0e\u5df2\u77e5\u7075\u957f\u7c7b\u5173\u7cfb\u4e0d\u7b26\uff0c\u9ad8\u62f7\u8d1d\u91cd\u590d\u5e8f\u5217\u5f71\u54cd\u663e\u8457\u3002", "conclusion": "BPE\u5728\u57fa\u56e0\u7ec4\u538b\u7f29\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u6bd4\u8f83\u57fa\u56e0\u7ec4\u5b66\u4e2d\u9700\u7ed3\u5408\u5176\u4ed6\u7b56\u7565\uff1bdnaBPE\u5de5\u5177\u5f00\u6e90\u53ef\u7528\u3002"}}
{"id": "2505.09572", "pdf": "https://arxiv.org/pdf/2505.09572", "abs": "https://arxiv.org/abs/2505.09572", "authors": ["Julian Kranz", "Davide Gallon", "Steffen Dereich", "Arnulf Jentzen"], "title": "SAD Neural Networks: Divergent Gradient Flows and Asymptotic Optimality via o-minimal Structures", "categories": ["cs.LG", "math.LO", "math.OC", "stat.ML", "Primary 68T05, Secondary 68T07, 26B40, 03C64, 03C98"], "comment": "27 pages, 4 figures", "summary": "We study gradient flows for loss landscapes of fully connected feed forward\nneural networks with commonly used continuously differentiable activation\nfunctions such as the logistic, hyperbolic tangent, softplus or GELU function.\nWe prove that the gradient flow either converges to a critical point or\ndiverges to infinity while the loss converges to an asymptotic critical value.\nMoreover, we prove the existence of a threshold $\\varepsilon>0$ such that the\nloss value of any gradient flow initialized at most $\\varepsilon$ above the\noptimal level converges to it. For polynomial target functions and sufficiently\nbig architecture and data set, we prove that the optimal loss value is zero and\ncan only be realized asymptotically. From this setting, we deduce our main\nresult that any gradient flow with sufficiently good initialization diverges to\ninfinity. Our proof heavily relies on the geometry of o-minimal structures. We\nconfirm these theoretical findings with numerical experiments and extend our\ninvestigation to real-world scenarios, where we observe an analogous behavior.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5168\u8fde\u63a5\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u5728\u5e38\u89c1\u53ef\u5fae\u6fc0\u6d3b\u51fd\u6570\u4e0b\u7684\u68af\u5ea6\u6d41\u884c\u4e3a\uff0c\u8bc1\u660e\u68af\u5ea6\u6d41\u8981\u4e48\u6536\u655b\u5230\u4e34\u754c\u70b9\uff0c\u8981\u4e48\u53d1\u6563\u5230\u65e0\u7a77\u5927\u4e14\u635f\u5931\u8d8b\u8fd1\u4e8e\u6e10\u8fd1\u4e34\u754c\u503c\u3002\u5bf9\u4e8e\u8db3\u591f\u597d\u7684\u521d\u59cb\u5316\uff0c\u68af\u5ea6\u6d41\u4f1a\u53d1\u6563\u5230\u65e0\u7a77\u5927\u3002\u7406\u8bba\u7ed3\u679c\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u7684\u68af\u5ea6\u6d41\u884c\u4e3a\uff0c\u7406\u89e3\u5176\u6536\u655b\u6216\u53d1\u6563\u7684\u6761\u4ef6\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4e0d\u540c\u6fc0\u6d3b\u51fd\u6570\u548c\u521d\u59cb\u5316\u5bf9\u8bad\u7ec3\u52a8\u6001\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528o-minimal\u7ed3\u6784\u7684\u51e0\u4f55\u6027\u8d28\uff0c\u7ed3\u5408\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\uff0c\u7814\u7a76\u68af\u5ea6\u6d41\u7684\u52a8\u529b\u5b66\u7279\u6027\u3002", "result": "\u8bc1\u660e\u68af\u5ea6\u6d41\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4f1a\u53d1\u6563\u5230\u65e0\u7a77\u5927\u4e14\u635f\u5931\u8d8b\u8fd1\u4e8e\u6e10\u8fd1\u4e34\u754c\u503c\uff1b\u5bf9\u591a\u9879\u5f0f\u76ee\u6807\u51fd\u6570\uff0c\u6700\u4f18\u635f\u5931\u503c\u4e3a\u96f6\u4e14\u4ec5\u80fd\u6e10\u8fd1\u5b9e\u73b0\u3002", "conclusion": "\u68af\u5ea6\u6d41\u7684\u53d1\u6563\u884c\u4e3a\u4e0e\u521d\u59cb\u5316\u548c\u7f51\u7edc\u7ed3\u6784\u5bc6\u5207\u76f8\u5173\uff0c\u7406\u8bba\u7ed3\u679c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u4e5f\u6709\u7c7b\u4f3c\u8868\u73b0\u3002"}}
{"id": "2505.08919", "pdf": "https://arxiv.org/pdf/2505.08919", "abs": "https://arxiv.org/abs/2505.08919", "authors": ["Kangxian Xie", "Yufei Zhu", "Kaiming Kuang", "Li Zhang", "Hongwei Bran Li", "Mingchen Gao", "Jiancheng Yang"], "title": "Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "In revision process", "summary": "High-quality 3D reconstruction of pulmonary segments plays a crucial role in\nsegmentectomy and surgical treatment planning for lung cancer. Due to the\nresolution requirement of the target reconstruction, conventional deep\nlearning-based methods often suffer from computational resource constraints or\nlimited granularity. Conversely, implicit modeling is favored due to its\ncomputational efficiency and continuous representation at any resolution. We\npropose a neural implicit function-based method to learn a 3D surface to\nachieve anatomy-aware, precise pulmonary segment reconstruction, represented as\na shape by deforming a learnable template. Additionally, we introduce two\nclinically relevant evaluation metrics to assess the reconstruction\ncomprehensively. Further, due to the absence of publicly available shape\ndatasets to benchmark reconstruction algorithms, we developed a shape dataset\nnamed Lung3D, including the 3D models of 800 labeled pulmonary segments and the\ncorresponding airways, arteries, veins, and intersegmental veins. We\ndemonstrate that the proposed approach outperforms existing methods, providing\na new perspective for pulmonary segment reconstruction. Code and data will be\navailable at https://github.com/M3DV/ImPulSe.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u9690\u5f0f\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u80ba\u90e83D\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u5206\u8fa8\u7387\u9650\u5236\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u4e2a\u4e34\u5e8a\u76f8\u5173\u6307\u6807\u548cLung3D\u6570\u636e\u96c6\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u7684\u80ba\u6bb53D\u91cd\u5efa\u5bf9\u80ba\u6bb5\u5207\u9664\u672f\u548c\u80ba\u764c\u624b\u672f\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6216\u5206\u8fa8\u7387\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u795e\u7ecf\u9690\u5f0f\u51fd\u6570\u5b66\u4e603D\u8868\u9762\uff0c\u5229\u7528\u53ef\u5b66\u4e60\u6a21\u677f\u53d8\u5f62\u5b9e\u73b0\u7cbe\u786e\u91cd\u5efa\uff0c\u5e76\u5f00\u53d1\u4e86Lung3D\u6570\u636e\u96c6\u3002", "result": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u80ba\u6bb5\u91cd\u5efa\u7684\u65b0\u89c6\u89d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u80ba\u6bb53D\u91cd\u5efa\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u4e34\u5e8a\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2505.09586", "pdf": "https://arxiv.org/pdf/2505.09586", "abs": "https://arxiv.org/abs/2505.09586", "authors": ["Yipeng Zhang", "Longlong Li", "Kelin Xia"], "title": "Rhomboid Tiling for Geometric Graph Deep Learning", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have proven effective for learning from\ngraph-structured data through their neighborhood-based message passing\nframework. Many hierarchical graph clustering pooling methods modify this\nframework by introducing clustering-based strategies, enabling the construction\nof more expressive and powerful models. However, all of these message passing\nframework heavily rely on the connectivity structure of graphs, limiting their\nability to capture the rich geometric features inherent in geometric graphs. To\naddress this, we propose Rhomboid Tiling (RT) clustering, a novel clustering\nmethod based on the rhomboid tiling structure, which performs clustering by\nleveraging the complex geometric information of the data and effectively\nextracts its higher-order geometric structures. Moreover, we design RTPool, a\nhierarchical graph clustering pooling model based on RT clustering for graph\nclassification tasks. The proposed model demonstrates superior performance,\noutperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u83f1\u5f62\u94fa\u780c\u7ed3\u6784\u7684\u65b0\u578b\u805a\u7c7b\u65b9\u6cd5RT\u548c\u5c42\u6b21\u56fe\u805a\u7c7b\u6c60\u5316\u6a21\u578bRTPool\uff0c\u7528\u4e8e\u56fe\u5206\u7c7b\u4efb\u52a1\uff0c\u57287\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e21\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6d88\u606f\u4f20\u9012\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u4e3b\u8981\u4f9d\u8d56\u56fe\u7684\u8fde\u63a5\u7ed3\u6784\uff0c\u96be\u4ee5\u6355\u6349\u51e0\u4f55\u56fe\u4e2d\u7684\u4e30\u5bcc\u51e0\u4f55\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u83f1\u5f62\u94fa\u780c\u7ed3\u6784\u7684RT\u805a\u7c7b\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1RTPool\u6a21\u578b\u7528\u4e8e\u5c42\u6b21\u56fe\u805a\u7c7b\u3002", "result": "RTPool\u57287\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e21\u79cd\u7ade\u4e89\u5bf9\u624b\u3002", "conclusion": "RT\u805a\u7c7b\u548cRTPool\u6709\u6548\u5229\u7528\u4e86\u6570\u636e\u7684\u51e0\u4f55\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u56fe\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2505.08939", "pdf": "https://arxiv.org/pdf/2505.08939", "abs": "https://arxiv.org/abs/2505.08939", "authors": ["Suchismita Naik", "Prakash Shukla", "Ike Obi", "Jessica Backus", "Nancy Rasche", "Paul Parsons"], "title": "Tracing the Invisible: Understanding Students' Judgment in AI-Supported Design Work", "categories": ["cs.HC", "cs.AI"], "comment": "5 pages, 2 Tables, In Creativity and Cognition 2025, June 23--25,\n  2025, Virtual, United Kingdom", "summary": "As generative AI tools become integrated into design workflows, students\nincreasingly engage with these tools not just as aids, but as collaborators.\nThis study analyzes reflections from 33 student teams in an HCI design course\nto examine the kinds of judgments students make when using AI tools. We found\nboth established forms of design judgment (e.g., instrumental, appreciative,\nquality) and emergent types: agency-distribution judgment and reliability\njudgment. These new forms capture how students negotiate creative\nresponsibility with AI and assess the trustworthiness of its outputs. Our\nfindings suggest that generative AI introduces new layers of complexity into\ndesign reasoning, prompting students to reflect not only on what AI produces,\nbut also on how and when to rely on it. By foregrounding these judgments, we\noffer a conceptual lens for understanding how students engage in co-creative\nsensemaking with AI in design contexts.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5b66\u751f\u5728\u8bbe\u8ba1\u8bfe\u7a0b\u4e2d\u4e0e\u751f\u6210AI\u5408\u4f5c\u7684\u5224\u65ad\u7c7b\u578b\uff0c\u53d1\u73b0\u4e86\u4f20\u7edf\u8bbe\u8ba1\u5224\u65ad\u548c\u65b0\u51fa\u73b0\u7684\u4ee3\u7406\u5206\u914d\u4e0e\u53ef\u9760\u6027\u5224\u65ad\uff0c\u63ed\u793a\u4e86AI\u5e26\u6765\u7684\u590d\u6742\u6027\u3002", "motivation": "\u63a2\u8ba8\u5b66\u751f\u5982\u4f55\u5728\u8bbe\u8ba1\u6d41\u7a0b\u4e2d\u4e0e\u751f\u6210AI\u534f\u4f5c\uff0c\u5e76\u7406\u89e3\u5176\u5224\u65ad\u7c7b\u578b\uff0c\u4ee5\u63ed\u793aAI\u5bf9\u8bbe\u8ba1\u601d\u7ef4\u7684\u5f71\u54cd\u3002", "method": "\u5206\u679033\u4e2a\u5b66\u751f\u56e2\u961f\u7684\u53cd\u601d\u5185\u5bb9\uff0c\u8bc6\u522b\u4ed6\u4eec\u4f7f\u7528AI\u5de5\u5177\u65f6\u7684\u5224\u65ad\u7c7b\u578b\u3002", "result": "\u53d1\u73b0\u65e2\u6709\u4f20\u7edf\u8bbe\u8ba1\u5224\u65ad\uff08\u5de5\u5177\u6027\u3001\u9274\u8d4f\u6027\u3001\u8d28\u91cf\uff09\uff0c\u4e5f\u6709\u65b0\u5224\u65ad\u7c7b\u578b\uff08\u4ee3\u7406\u5206\u914d\u4e0e\u53ef\u9760\u6027\uff09\uff0c\u8868\u660eAI\u589e\u52a0\u4e86\u8bbe\u8ba1\u51b3\u7b56\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u751f\u6210AI\u4e3a\u8bbe\u8ba1\u63a8\u7406\u5f15\u5165\u65b0\u5c42\u6b21\uff0c\u5b66\u751f\u9700\u5e73\u8861\u521b\u610f\u8d23\u4efb\u4e0eAI\u8f93\u51fa\u7684\u53ef\u4fe1\u5ea6\uff0c\u7814\u7a76\u4e3a\u7406\u89e3\u4eba\u673a\u5171\u521b\u63d0\u4f9b\u4e86\u6982\u5ff5\u6846\u67b6\u3002"}}
{"id": "2505.09593", "pdf": "https://arxiv.org/pdf/2505.09593", "abs": "https://arxiv.org/abs/2505.09593", "authors": ["Filippo Leveni", "Guilherme Weigert Cassales", "Bernhard Pfahringer", "Albert Bifet", "Giacomo Boracchi"], "title": "Online Isolation Forest", "categories": ["cs.LG"], "comment": "Accepted at International Conference on Machine Learning (ICML 2024)", "summary": "The anomaly detection literature is abundant with offline methods, which\nrequire repeated access to data in memory, and impose impractical assumptions\nwhen applied to a streaming context. Existing online anomaly detection methods\nalso generally fail to address these constraints, resorting to periodic\nretraining to adapt to the online context. We propose Online-iForest, a novel\nmethod explicitly designed for streaming conditions that seamlessly tracks the\ndata generating process as it evolves over time. Experimental validation on\nreal-world datasets demonstrated that Online-iForest is on par with online\nalternatives and closely rivals state-of-the-art offline anomaly detection\ntechniques that undergo periodic retraining. Notably, Online-iForest\nconsistently outperforms all competitors in terms of efficiency, making it a\npromising solution in applications where fast identification of anomalies is of\nprimary importance such as cybersecurity, fraud and fault detection.", "AI": {"tldr": "Online-iForest\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u6d41\u6570\u636e\u73af\u5883\u7684\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5176\u6027\u80fd\u4e0e\u5728\u7ebf\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u6548\u7387\u4f18\u4e8e\u6240\u6709\u7ade\u4e89\u8005\u3002", "motivation": "\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u591a\u4e3a\u79bb\u7ebf\u65b9\u6cd5\uff0c\u4e0d\u9002\u5408\u6d41\u6570\u636e\u73af\u5883\uff1b\u73b0\u6709\u5728\u7ebf\u65b9\u6cd5\u591a\u4f9d\u8d56\u5468\u671f\u6027\u91cd\u8bad\u7ec3\uff0c\u65e0\u6cd5\u9002\u5e94\u5b9e\u65f6\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u4e86Online-iForest\u65b9\u6cd5\uff0c\u4e13\u4e3a\u6d41\u6570\u636e\u8bbe\u8ba1\uff0c\u80fd\u5b9e\u65f6\u8ddf\u8e2a\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u7684\u53d8\u5316\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cOnline-iForest\u6027\u80fd\u4e0e\u5728\u7ebf\u65b9\u6cd5\u76f8\u5f53\uff0c\u6548\u7387\u4f18\u4e8e\u6240\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9700\u5feb\u901f\u8bc6\u522b\u7684\u573a\u666f\u3002", "conclusion": "Online-iForest\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u7f51\u7edc\u5b89\u5168\u3001\u6b3a\u8bc8\u68c0\u6d4b\u7b49\u5b9e\u65f6\u6027\u8981\u6c42\u9ad8\u7684\u9886\u57df\u3002"}}
{"id": "2505.09602", "pdf": "https://arxiv.org/pdf/2505.09602", "abs": "https://arxiv.org/abs/2505.09602", "authors": ["David Khachaturov", "Robert Mullins"], "title": "Adversarial Suffix Filtering: a Defense Pipeline for LLMs", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly embedded in autonomous systems\nand public-facing environments, yet they remain susceptible to jailbreak\nvulnerabilities that may undermine their security and trustworthiness.\nAdversarial suffixes are considered to be the current state-of-the-art\njailbreak, consistently outperforming simpler methods and frequently succeeding\neven in black-box settings. Existing defenses rely on access to the internal\narchitecture of models limiting diverse deployment, increase memory and\ncomputation footprints dramatically, or can be bypassed with simple prompt\nengineering methods. We introduce $\\textbf{Adversarial Suffix Filtering}$\n(ASF), a lightweight novel model-agnostic defensive pipeline designed to\nprotect LLMs against adversarial suffix attacks. ASF functions as an input\npreprocessor and sanitizer that detects and filters adversarially crafted\nsuffixes in prompts, effectively neutralizing malicious injections. We\ndemonstrate that ASF provides comprehensive defense capabilities across both\nblack-box and white-box attack settings, reducing the attack efficacy of\nstate-of-the-art adversarial suffix generation methods to below 4%, while only\nminimally affecting the target model's capabilities in non-adversarial\nscenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u9632\u5fa1\u65b9\u6cd5Adversarial Suffix Filtering (ASF)\uff0c\u7528\u4e8e\u4fdd\u62a4\u5927\u578b\u8bed\u8a00\u6a21\u578b\u514d\u53d7\u5bf9\u6297\u6027\u540e\u7f00\u653b\u51fb\uff0c\u6709\u6548\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u81f34%\u4ee5\u4e0b\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u6a21\u578b\u6b63\u5e38\u529f\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u7cfb\u7edf\u548c\u516c\u5171\u73af\u5883\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5b58\u5728\u8d8a\u72f1\u6f0f\u6d1e\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u67b6\u6784\uff0c\u8981\u4e48\u589e\u52a0\u8ba1\u7b97\u8d1f\u62c5\u6216\u6613\u88ab\u7ed5\u8fc7\u3002", "method": "ASF\u4f5c\u4e3a\u8f93\u5165\u9884\u5904\u7406\u548c\u51c0\u5316\u5668\uff0c\u68c0\u6d4b\u5e76\u8fc7\u6ee4\u5bf9\u6297\u6027\u540e\u7f00\uff0c\u65e0\u9700\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u67b6\u6784\uff0c\u9002\u7528\u4e8e\u9ed1\u76d2\u548c\u767d\u76d2\u653b\u51fb\u573a\u666f\u3002", "result": "ASF\u5c06\u653b\u51fb\u6548\u80fd\u964d\u4f4e\u81f34%\u4ee5\u4e0b\uff0c\u5728\u975e\u5bf9\u6297\u6027\u573a\u666f\u4e2d\u5bf9\u6a21\u578b\u529f\u80fd\u5f71\u54cd\u6781\u5c0f\u3002", "conclusion": "ASF\u4e3aLLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u7684\u9632\u5fa1\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2505.07363", "pdf": "https://arxiv.org/pdf/2505.07363", "abs": "https://arxiv.org/abs/2505.07363", "authors": ["Serge Massar"], "title": "Equilibrium Propagation for Learning in Lagrangian Dynamical Systems", "categories": ["nlin.CD", "cs.LG", "physics.data-an"], "comment": "8 pages, 1 figure", "summary": "We propose a method for training dynamical systems governed by Lagrangian\nmechanics using Equilibrium Propagation. Our approach extends Equilibrium\nPropagation -- initially developed for energy-based models -- to dynamical\ntrajectories by leveraging the principle of action extremization. Training is\nachieved by gently nudging trajectories toward desired targets and measuring\nhow the variables conjugate to the parameters to be trained respond. This\nmethod is particularly suited to systems with periodic boundary conditions or\nfixed initial and final states, enabling efficient parameter updates without\nrequiring explicit backpropagation through time. In the case of periodic\nboundary conditions, this approach yields the semiclassical limit of Quantum\nEquilibrium Propagation. Applications to systems with dissipation are also\ndiscussed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eEquilibrium Propagation\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u7531\u62c9\u683c\u6717\u65e5\u529b\u5b66\u63a7\u5236\u7684\u52a8\u6001\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u5468\u671f\u8fb9\u754c\u6761\u4ef6\u6216\u56fa\u5b9a\u521d\u59cb\u548c\u6700\u7ec8\u72b6\u6001\u7684\u7cfb\u7edf\u3002", "motivation": "\u6269\u5c55Equilibrium Propagation\u65b9\u6cd5\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u52a8\u6001\u8f68\u8ff9\u8bad\u7ec3\uff0c\u5229\u7528\u4f5c\u7528\u91cf\u6781\u503c\u5316\u539f\u7406\uff0c\u907f\u514d\u663e\u5f0f\u65f6\u95f4\u53cd\u5411\u4f20\u64ad\u3002", "method": "\u901a\u8fc7\u8f7b\u5fae\u8c03\u6574\u8f68\u8ff9\u81f3\u76ee\u6807\uff0c\u6d4b\u91cf\u4e0e\u53c2\u6570\u5171\u8f6d\u53d8\u91cf\u7684\u54cd\u5e94\uff0c\u5b9e\u73b0\u53c2\u6570\u66f4\u65b0\uff0c\u5c24\u5176\u9002\u5408\u5468\u671f\u8fb9\u754c\u6761\u4ef6\u6216\u56fa\u5b9a\u72b6\u6001\u7684\u7cfb\u7edf\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5468\u671f\u8fb9\u754c\u6761\u4ef6\u4e0b\u53ef\u63a8\u5bfc\u4e3a\u91cf\u5b50Equilibrium Propagation\u7684\u534a\u7ecf\u5178\u6781\u9650\uff0c\u5e76\u8ba8\u8bba\u4e86\u8017\u6563\u7cfb\u7edf\u7684\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u52a8\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u65e0\u9700\u590d\u6742\u7684\u65f6\u95f4\u53cd\u5411\u4f20\u64ad\uff0c\u6269\u5c55\u4e86Equilibrium Propagation\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2505.09021", "pdf": "https://arxiv.org/pdf/2505.09021", "abs": "https://arxiv.org/abs/2505.09021", "authors": ["Maria Dhakal", "Chia-Yi Su", "Robert Wallace", "Chris Fakhimi", "Aakash Bansal", "Toby Li", "Yu Huang", "Collin McMillan"], "title": "AI-Mediated Code Comment Improvement", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "This paper describes an approach to improve code comments along different\nquality axes by rewriting those comments with customized Artificial\nIntelligence (AI)-based tools. We conduct an empirical study followed by\ngrounded theory qualitative analysis to determine the quality axes to improve.\nThen we propose a procedure using a Large Language Model (LLM) to rewrite\nexisting code comments along the quality axes. We implement our procedure using\nGPT-4o, then distil the results into a smaller model capable of being run\nin-house, so users can maintain data custody. We evaluate both our approach\nusing GPT-4o and the distilled model versions. We show in an evaluation how our\nprocedure improves code comments along the quality axes. We release all data\nand source code in an online repository for reproducibility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528AI\u5de5\u5177\u91cd\u5199\u4ee3\u7801\u6ce8\u91ca\u4ee5\u63d0\u5347\u5176\u8d28\u91cf\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u84b8\u998f\u6280\u672f\u5c06\u7ed3\u679c\u538b\u7f29\u4e3a\u53ef\u5728\u4f01\u4e1a\u5185\u90e8\u8fd0\u884c\u7684\u5c0f\u6a21\u578b\uff0c\u540c\u65f6\u5728\u8bc4\u4f30\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63d0\u5347\u4ee3\u7801\u6ce8\u91ca\u7684\u8d28\u91cf\u5bf9\u4e8e\u4ee3\u7801\u53ef\u7ef4\u62a4\u6027\u548c\u5f00\u53d1\u8005\u534f\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6ce8\u91ca\u5f80\u5f80\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u548c\u5b9a\u6027\u5206\u6790\u786e\u5b9a\u8d28\u91cf\u6539\u8fdb\u7684\u7ef4\u5ea6\uff0c\u4f7f\u7528LLM\uff08\u5982GPT-4o\uff09\u91cd\u5199\u6ce8\u91ca\uff0c\u5e76\u901a\u8fc7\u84b8\u998f\u6280\u672f\u5b9e\u73b0\u6a21\u578b\u7684\u5c0f\u578b\u5316\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8bc4\u4f30\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u4ee3\u7801\u6ce8\u91ca\u7684\u8d28\u91cf\uff0c\u5e76\u6210\u529f\u5c06\u7ed3\u679c\u84b8\u998f\u4e3a\u53ef\u672c\u5730\u8fd0\u884c\u7684\u5c0f\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u663e\u8457\u6539\u5584\u4e86\u4ee3\u7801\u6ce8\u91ca\u8d28\u91cf\uff0c\u8fd8\u901a\u8fc7\u6570\u636e\u5f00\u6e90\u548c\u6a21\u578b\u84b8\u998f\u652f\u6301\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2505.08801", "pdf": "https://arxiv.org/pdf/2505.08801", "abs": "https://arxiv.org/abs/2505.08801", "authors": ["Md. Sakib Hassan Chowdhury", "Md. Hafiz Ahamed", "Bishowjit Paul", "Sarafat Hussain Abhi", "Abu Bakar Siddique", "Md. Robius Sany"], "title": "OptiGait-LGBM: An Efficient Approach of Gait-based Person Re-identification in Non-Overlapping Regions", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "12 pages, 17 figures", "summary": "Gait recognition, known for its ability to identify individuals from a\ndistance, has gained significant attention in recent times due to its\nnon-intrusive verification. While video-based gait identification systems\nperform well on large public datasets, their performance drops when applied to\nreal-world, unconstrained gait data due to various factors. Among these,\nuncontrolled outdoor environments, non-overlapping camera views, varying\nillumination, and computational efficiency are core challenges in gait-based\nauthentication. Currently, no dataset addresses all these challenges\nsimultaneously. In this paper, we propose an OptiGait-LGBM model capable of\nrecognizing person re-identification under these constraints using a skeletal\nmodel approach, which helps mitigate inconsistencies in a person's appearance.\nThe model constructs a dataset from landmark positions, minimizing memory usage\nby using non-sequential data. A benchmark dataset, RUET-GAIT, is introduced to\nrepresent uncontrolled gait sequences in complex outdoor environments. The\nprocess involves extracting skeletal joint landmarks, generating numerical\ndatasets, and developing an OptiGait-LGBM gait classification model. Our aim is\nto address the aforementioned challenges with minimal computational cost\ncompared to existing methods. A comparative analysis with ensemble techniques\nsuch as Random Forest and CatBoost demonstrates that the proposed approach\noutperforms them in terms of accuracy, memory usage, and training time. This\nmethod provides a novel, low-cost, and memory-efficient video-based gait\nrecognition solution for real-world scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOptiGait-LGBM\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u590d\u6742\u65e0\u7ea6\u675f\u7684\u5ba4\u5916\u73af\u5883\u4e0b\u901a\u8fc7\u9aa8\u9abc\u6a21\u578b\u5b9e\u73b0\u6b65\u6001\u8bc6\u522b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u89c6\u9891\u7684\u6b65\u6001\u8bc6\u522b\u7cfb\u7edf\u5728\u5927\u89c4\u6a21\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u65e0\u7ea6\u675f\u7684\u6b65\u6001\u6570\u636e\u4e2d\u6027\u80fd\u4e0b\u964d\u3002\u4e3b\u8981\u6311\u6218\u5305\u62ec\u5ba4\u5916\u73af\u5883\u4e0d\u53ef\u63a7\u3001\u6444\u50cf\u5934\u89c6\u89d2\u4e0d\u91cd\u53e0\u3001\u5149\u7167\u53d8\u5316\u548c\u8ba1\u7b97\u6548\u7387\u3002\u76ee\u524d\u6ca1\u6709\u6570\u636e\u96c6\u80fd\u540c\u65f6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9aa8\u9abc\u6a21\u578b\u65b9\u6cd5\uff0c\u4ece\u5173\u952e\u70b9\u4f4d\u7f6e\u6784\u5efa\u6570\u636e\u96c6\uff0c\u6700\u5c0f\u5316\u5185\u5b58\u4f7f\u7528\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aOptiGait-LGBM\u7684\u6b65\u6001\u5206\u7c7b\u6a21\u578b\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aRUET-GAIT\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8868\u793a\u590d\u6742\u5ba4\u5916\u73af\u5883\u4e2d\u7684\u65e0\u7ea6\u675f\u6b65\u6001\u5e8f\u5217\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOptiGait-LGBM\u5728\u51c6\u786e\u6027\u3001\u5185\u5b58\u4f7f\u7528\u548c\u8bad\u7ec3\u65f6\u95f4\u4e0a\u4f18\u4e8e\u5176\u4ed6\u96c6\u6210\u6280\u672f\uff08\u5982\u968f\u673a\u68ee\u6797\u548cCatBoost\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u89c6\u9891\u6b65\u6001\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u3001\u4f4e\u6210\u672c\u4e14\u5185\u5b58\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.08804", "pdf": "https://arxiv.org/pdf/2505.08804", "abs": "https://arxiv.org/abs/2505.08804", "authors": ["Longtian Wang", "Xiaofei Xie", "Tianlin Li", "Yuhan Zhi", "Chao Shen"], "title": "TokenProber: Jailbreaking Text-to-image Models via Fine-grained Word Impact Analysis", "categories": ["cs.CR", "cs.LG"], "comment": "13 pages, 5 figures", "summary": "Text-to-image (T2I) models have significantly advanced in producing\nhigh-quality images. However, such models have the ability to generate images\ncontaining not-safe-for-work (NSFW) content, such as pornography, violence,\npolitical content, and discrimination. To mitigate the risk of generating NSFW\ncontent, refusal mechanisms, i.e., safety checkers, have been developed to\ncheck potential NSFW content. Adversarial prompting techniques have been\ndeveloped to evaluate the robustness of the refusal mechanisms. The key\nchallenge remains to subtly modify the prompt in a way that preserves its\nsensitive nature while bypassing the refusal mechanisms. In this paper, we\nintroduce TokenProber, a method designed for sensitivity-aware differential\ntesting, aimed at evaluating the robustness of the refusal mechanisms in T2I\nmodels by generating adversarial prompts. Our approach is based on the key\nobservation that adversarial prompts often succeed by exploiting discrepancies\nin how T2I models and safety checkers interpret sensitive content. Thus, we\nconduct a fine-grained analysis of the impact of specific words within prompts,\ndistinguishing between dirty words that are essential for NSFW content\ngeneration and discrepant words that highlight the different sensitivity\nassessments between T2I models and safety checkers. Through the\nsensitivity-aware mutation, TokenProber generates adversarial prompts, striking\na balance between maintaining NSFW content generation and evading detection.\nOur evaluation of TokenProber against 5 safety checkers on 3 popular T2I\nmodels, using 324 NSFW prompts, demonstrates its superior effectiveness in\nbypassing safety filters compared to existing methods (e.g., 54%+ increase on\naverage), highlighting TokenProber's ability to uncover robustness issues in\nthe existing refusal mechanisms.", "AI": {"tldr": "TokenProber \u662f\u4e00\u79cd\u654f\u611f\u6027\u611f\u77e5\u5dee\u5f02\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u4e2d\u7684\u62d2\u7edd\u673a\u5236\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u6027\u63d0\u793a\u7ed5\u8fc7\u5b89\u5168\u68c0\u67e5\u5668\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u53ef\u80fd\u751f\u6210\u4e0d\u5b89\u5168\u5185\u5bb9\u7684\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u6d4b\u8bd5\u73b0\u6709\u62d2\u7edd\u673a\u5236\u7684\u9c81\u68d2\u6027\uff0c\u5bf9\u6297\u6027\u63d0\u793a\u6280\u672f\u7684\u5173\u952e\u6311\u6218\u5728\u4e8e\u4fdd\u7559\u654f\u611f\u5185\u5bb9\u540c\u65f6\u7ed5\u8fc7\u5b89\u5168\u68c0\u67e5\u3002", "method": "TokenProber \u63d0\u51fa\u4e00\u79cd\u654f\u611f\u6027\u611f\u77e5\u53d8\u5f02\u65b9\u6cd5\uff0c\u533a\u5206\u5173\u952e\u810f\u8bcd\u4e0e\u5dee\u5f02\u6027\u8bcd\u6c47\uff0c\u5229\u7528 T2I \u6a21\u578b\u4e0e\u5b89\u5168\u68c0\u67e5\u5668\u5bf9\u654f\u611f\u6027\u8bc4\u4f30\u7684\u5dee\u5f02\u751f\u6210\u5bf9\u6297\u6027\u63d0\u793a\u3002", "result": "\u5728 5 \u4e2a\u5b89\u5168\u68c0\u67e5\u5668\u4e0e 3 \u4e2a T2I \u6a21\u578b\u4e0a\u6d4b\u8bd5\u663e\u793a\uff0cTokenProber \u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747\u63d0\u5347 54%+ \u7684\u7ed5\u8fc7\u6210\u529f\u7387\uff0c\u663e\u8457\u63ed\u9732\u62d2\u7edd\u673a\u5236\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "conclusion": "TokenProber \u6210\u529f\u66b4\u9732\u4e86 T2I \u6a21\u578b\u4e2d\u62d2\u7edd\u673a\u5236\u7684\u5f31\u70b9\uff0c\u4e3a\u672a\u6765\u5b89\u5168\u673a\u5236\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2505.09027", "pdf": "https://arxiv.org/pdf/2505.09027", "abs": "https://arxiv.org/abs/2505.09027", "authors": ["Yi Cui"], "title": "Tests as Prompt: A Test-Driven-Development Benchmark for LLM Code Generation", "categories": ["cs.SE", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2409.05177", "summary": "We introduce WebApp1K, a novel benchmark for evaluating large language models\n(LLMs) in test-driven development (TDD) tasks, where test cases serve as both\nprompt and verification for code generation. Unlike traditional approaches\nrelying on natural language prompts, our benchmark emphasizes the ability of\nLLMs to interpret and implement functionality directly from test cases,\nreflecting real-world software development practices. Comprising 1000 diverse\nchallenges across 20 application domains, the benchmark evaluates LLMs on their\nability to generate compact, functional code under the constraints of context\nlength and multi-feature complexity. Our findings highlight instruction\nfollowing and in-context learning as critical capabilities for TDD success,\nsurpassing the importance of general coding proficiency or pretraining\nknowledge. Through comprehensive evaluation of 19 frontier models, we reveal\nperformance bottlenecks, such as instruction loss in long prompts, and provide\na detailed error analysis spanning multiple root causes. This work underscores\nthe practical value of TDD-specific benchmarks and lays the foundation for\nadvancing LLM capabilities in rigorous, application-driven coding scenarios.", "AI": {"tldr": "WebApp1K\u662f\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5f3a\u8c03\u901a\u8fc7\u6d4b\u8bd5\u6848\u4f8b\u751f\u6210\u4ee3\u7801\u7684\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff0c\u800cWebApp1K\u65e8\u5728\u66f4\u8d34\u8fd1\u5b9e\u9645\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\uff0c\u5173\u6ce8LLM\u76f4\u63a5\u901a\u8fc7\u6d4b\u8bd5\u6848\u4f8b\u751f\u6210\u529f\u80fd\u4ee3\u7801\u7684\u80fd\u529b\u3002", "method": "\u521b\u5efa1000\u4e2a\u8de820\u4e2a\u5e94\u7528\u9886\u57df\u7684\u591a\u6837\u5316\u6311\u6218\uff0c\u8bc4\u4f30LLM\u5728\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u591a\u529f\u80fd\u590d\u6742\u6027\u7ea6\u675f\u4e0b\u751f\u6210\u7d27\u51d1\u3001\u529f\u80fd\u4ee3\u7801\u7684\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6307\u4ee4\u9075\u5faa\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u662fTDD\u6210\u529f\u7684\u5173\u952e\uff0c\u91cd\u8981\u6027\u8d85\u8fc7\u4e00\u822c\u7f16\u7801\u80fd\u529b\u6216\u9884\u8bad\u7ec3\u77e5\u8bc6\u3002\u8bc4\u4f30\u63ed\u793a\u4e86\u6027\u80fd\u74f6\u9888\uff0c\u5982\u957f\u63d0\u793a\u4e2d\u7684\u6307\u4ee4\u4e22\u5931\u3002", "conclusion": "WebApp1K\u5f3a\u8c03\u4e86TDD\u7279\u5b9a\u57fa\u51c6\u7684\u5b9e\u9645\u4ef7\u503c\uff0c\u4e3a\u5728\u4e25\u683c\u7684\u5e94\u7528\u9a71\u52a8\u7f16\u7801\u573a\u666f\u4e2d\u63d0\u5347LLM\u80fd\u529b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.09040", "pdf": "https://arxiv.org/pdf/2505.09040", "abs": "https://arxiv.org/abs/2505.09040", "authors": ["Owen Kwon", "Abraham George", "Alison Bartsch", "Amir Barati Farimani"], "title": "RT-cache: Efficient Robot Trajectory Retrieval System", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "9 pages, 5 figures. Submitted to an IEEE robotics conference", "summary": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation.", "AI": {"tldr": "RT-cache\u662f\u4e00\u79cd\u901a\u8fc7\u5927\u6570\u636e\u68c0\u7d22\u548c\u7ecf\u9a8c\u5b66\u4e60\u52a0\u901f\u673a\u5668\u4eba\u63a8\u7406\u7684\u65b0\u578b\u8f68\u8ff9\u8bb0\u5fc6\u7ba1\u9053\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5f00\u9500\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u4ee3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u80fd\u5904\u7406\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u4efb\u52a1\uff0c\u4f46\u5176\u6bcf\u4e00\u6b65\u63a8\u7406\u6210\u672c\u9ad8\uff0c\u5bfc\u81f4\u663e\u8457\u5ef6\u8fdf\u3002RT-cache\u65e8\u5728\u901a\u8fc7\u5b58\u50a8\u548c\u68c0\u7d22\u6210\u529f\u8f68\u8ff9\u6765\u51cf\u5c11\u8fd9\u79cd\u5ef6\u8fdf\u3002", "method": "RT-cache\u7ed3\u5408\u4e86\u8bb0\u5fc6\u6784\u5efa\u5668\u548c\u8f68\u8ff9\u68c0\u7d22\u5668\uff0c\u5b58\u50a8\u5927\u89c4\u6a21\u6210\u529f\u8f68\u8ff9\u5e76\u68c0\u7d22\u76f8\u5173\u591a\u6b65\u9aa4\u8fd0\u52a8\u7247\u6bb5\uff0c\u4ee5\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002\u8be5\u65b9\u6cd5\u53ef\u7075\u6d3b\u79ef\u7d2f\u73b0\u5b9e\u7ecf\u9a8c\u5e76\u5728\u573a\u666f\u5339\u914d\u65f6\u91cd\u653e\u3002", "result": "\u5728Open-X Embodiment\u6570\u636e\u96c6\u548c\u5176\u4ed6\u73b0\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRT-cache\u6bd4\u7f3a\u4e4f\u68c0\u7d22\u7684\u57fa\u7ebf\u66f4\u5feb\u4e14\u66f4\u6210\u529f\u5730\u5b8c\u6210\u4efb\u52a1\u3002", "conclusion": "RT-cache\u4e3a\u5b9e\u65f6\u64cd\u63a7\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u6570\u636e\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u5feb\u901f\u9002\u5e94\u65b0\u73af\u5883\u3002"}}
{"id": "2505.08816", "pdf": "https://arxiv.org/pdf/2505.08816", "abs": "https://arxiv.org/abs/2505.08816", "authors": ["Ippokratis Koukoulis", "Ilias Syrigos", "Thanasis Korakis"], "title": "Self-Supervised Transformer-based Contrastive Learning for Intrusion Detection Systems", "categories": ["cs.CR", "cs.LG"], "comment": "Accepted at IFIP Networking 2025. Code available at\n  https://github.com/koukipp/contrastive_transformers_ids", "summary": "As the digital landscape becomes more interconnected, the frequency and\nseverity of zero-day attacks, have significantly increased, leading to an\nurgent need for innovative Intrusion Detection Systems (IDS). Machine\nLearning-based IDS that learn from the network traffic characteristics and can\ndiscern attack patterns from benign traffic offer an advanced solution to\ntraditional signature-based IDS. However, they heavily rely on labeled\ndatasets, and their ability to generalize when encountering unseen traffic\npatterns remains a challenge. This paper proposes a novel self-supervised\ncontrastive learning approach based on transformer encoders, specifically\ntailored for generalizable intrusion detection on raw packet sequences. Our\nproposed learning scheme employs a packet-level data augmentation strategy\ncombined with a transformer-based architecture to extract and generate\nmeaningful representations of traffic flows. Unlike traditional methods reliant\non handcrafted statistical features (NetFlow), our approach automatically\nlearns comprehensive packet sequence representations, significantly enhancing\nperformance in anomaly identification tasks and supervised learning for\nintrusion detection. Our transformer-based framework exhibits better\nperformance in comparison to existing NetFlow self-supervised methods.\nSpecifically, we achieve up to a 3% higher AUC in anomaly detection for\nintra-dataset evaluation and up to 20% higher AUC scores in inter-dataset\nevaluation. Moreover, our model provides a strong baseline for supervised\nintrusion detection with limited labeled data, exhibiting an improvement over\nself-supervised NetFlow models of up to 1.5% AUC when pretrained and evaluated\non the same dataset. Additionally, we show the adaptability of our pretrained\nmodel when fine-tuned across different datasets, demonstrating strong\nperformance even when lacking benign data from the target domain.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u539f\u59cb\u6570\u636e\u5305\u5e8f\u5217\u4e2d\u5b66\u4e60\u901a\u7528\u5165\u4fb5\u68c0\u6d4b\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u548c\u76d1\u7763\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u968f\u7740\u96f6\u65e5\u653b\u51fb\u9891\u7387\u548c\u4e25\u91cd\u6027\u7684\u589e\u52a0\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u7b7e\u540d\u7684\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08IDS\uff09\u5df2\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\u3002\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u867d\u80fd\u8bc6\u522b\u653b\u51fb\u6a21\u5f0f\uff0c\u4f46\u5bf9\u6807\u6ce8\u6570\u636e\u548c\u672a\u89c1\u8fc7\u6d41\u91cf\u6a21\u5f0f\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u57fa\u4e8eTransformer\u7f16\u7801\u5668\u81ea\u52a8\u5b66\u4e60\u539f\u59cb\u6570\u636e\u5305\u5e8f\u5217\u7684\u8868\u793a\uff0c\u65e0\u9700\u4f9d\u8d56\u624b\u5de5\u7edf\u8ba1\u7279\u5f81\uff08\u5982NetFlow\uff09\uff0c\u5e76\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u7b56\u7565\u3002", "result": "\u5728\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u7684NetFlow\u81ea\u76d1\u7763\u65b9\u6cd5\uff0cAUC\u63d0\u53473%\uff08\u6570\u636e\u96c6\u5185\uff09\u548c20%\uff08\u8de8\u6570\u636e\u96c6\uff09\u3002\u5728\u76d1\u7763\u5b66\u4e60\u4e2d\uff0c\u4f7f\u7528\u6709\u9650\u6807\u6ce8\u6570\u636e\u65f6AUC\u63d0\u9ad81.5%\u3002", "conclusion": "\u63d0\u51fa\u7684Transformer\u6846\u67b6\u4e0d\u4ec5\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd8\u5c55\u793a\u4e86\u5728\u76ee\u6807\u57df\u7f3a\u4e4f\u826f\u6027\u6570\u636e\u65f6\u7684\u5f3a\u9002\u5e94\u80fd\u529b\uff0c\u4e3a\u901a\u7528\u5165\u4fb5\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09062", "pdf": "https://arxiv.org/pdf/2505.09062", "abs": "https://arxiv.org/abs/2505.09062", "authors": ["Junda Zhao", "Yuliang Song", "Eldan Cohen"], "title": "Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models", "categories": ["cs.SE", "cs.AI", "cs.LG", "D.2.7"], "comment": "Accepted by the Journal of Systems and Software", "summary": "Recent advancements in source code summarization have leveraged\ntransformer-based pre-trained models, including Large Language Models of Code\n(LLMCs), to automate and improve the generation of code summaries. However,\nexisting methods often focus on generating a single high-quality summary for a\ngiven source code, neglecting scenarios where the generated summary might be\ninadequate and alternative options are needed. In this paper, we introduce\nVariational Prefix Tuning (VPT), a novel approach that enhances pre-trained\nmodels' ability to generate diverse yet accurate sets of summaries, allowing\nthe user to choose the most suitable one for the given source code. Our method\nintegrates a Conditional Variational Autoencoder (CVAE) framework as a modular\ncomponent into pre-trained models, enabling us to model the distribution of\nobserved target summaries and sample continuous embeddings to be used as\nprefixes to steer the generation of diverse outputs during decoding.\nImportantly, we construct our method in a parameter-efficient manner,\neliminating the need for expensive model retraining, especially when using\nLLMCs. Furthermore, we employ a bi-criteria reranking method to select a subset\nof generated summaries, optimizing both the diversity and the accuracy of the\noptions presented to users. We present extensive experimental evaluations using\nwidely used datasets and current state-of-the-art pre-trained code\nsummarization models to demonstrate the effectiveness of our approach and its\nadaptability across models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVariational Prefix Tuning (VPT)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668(CVAE)\u6846\u67b6\uff0c\u589e\u5f3a\u9884\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u591a\u6837\u4e14\u51c6\u786e\u7684\u4ee3\u7801\u6458\u8981\u7684\u80fd\u529b\uff0c\u540c\u65f6\u907f\u514d\u6602\u8d35\u7684\u6a21\u578b\u91cd\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u4e3a\u7ed9\u5b9a\u6e90\u4ee3\u7801\u751f\u6210\u5355\u4e00\u9ad8\u8d28\u91cf\u6458\u8981\uff0c\u5ffd\u7565\u4e86\u6458\u8981\u53ef\u80fd\u4e0d\u5145\u5206\u4e14\u9700\u8981\u66ff\u4ee3\u9009\u9879\u7684\u573a\u666f\u3002", "method": "VPT\u65b9\u6cd5\u7ed3\u5408\u4e86CVAE\u6846\u67b6\uff0c\u901a\u8fc7\u91c7\u6837\u8fde\u7eed\u5d4c\u5165\u4f5c\u4e3a\u524d\u7f00\u6765\u6307\u5bfc\u89e3\u7801\u8fc7\u7a0b\u4e2d\u751f\u6210\u591a\u6837\u5316\u7684\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u80fd\u9002\u5e94\u4e0d\u540c\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "VPT\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u591a\u6837\u4e14\u51c6\u786e\u7684\u4ee3\u7801\u6458\u8981\uff0c\u65e0\u9700\u6602\u8d35\u7684\u6a21\u578b\u91cd\u8bad\u7ec3\u3002"}}
{"id": "2505.08817", "pdf": "https://arxiv.org/pdf/2505.08817", "abs": "https://arxiv.org/abs/2505.08817", "authors": ["Camilo Carvajal Reyes", "Joaqu\u00edn Fontbona", "Felipe Tobar"], "title": "Towards SFW sampling for diffusion models via external conditioning", "categories": ["cs.CV", "cs.LG"], "comment": "Accepcted at IJCNN 2025", "summary": "Score-based generative models (SBM), also known as diffusion models, are the\nde facto state of the art for image synthesis. Despite their unparalleled\nperformance, SBMs have recently been in the spotlight for being tricked into\ncreating not-safe-for-work (NSFW) content, such as violent images and\nnon-consensual nudity. Current approaches that prevent unsafe generation are\nbased on the models' own knowledge, and the majority of them require\nfine-tuning. This article explores the use of external sources for ensuring\nsafe outputs in SBMs. Our safe-for-work (SFW) sampler implements a Conditional\nTrajectory Correction step that guides the samples away from undesired regions\nin the ambient space using multimodal models as the source of conditioning.\nFurthermore, using Contrastive Language Image Pre-training (CLIP), our method\nadmits user-defined NSFW classes, which can vary in different settings. Our\nexperiments on the text-to-image SBM Stable Diffusion validate that the\nproposed SFW sampler effectively reduces the generation of explicit content\nwhile being competitive with other fine-tuning-based approaches, as assessed\nvia independent NSFW detectors. Moreover, we evaluate the impact of the SFW\nsampler on image quality and show that the proposed correction scheme comes at\na minor cost with negligible effect on samples not needing correction. Our\nstudy confirms the suitability of the SFW sampler towards aligned SBM models\nand the potential of using model-agnostic conditioning for the prevention of\nunwanted images.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5916\u90e8\u591a\u6a21\u6001\u6a21\u578b\u7684SFW\u91c7\u6837\u5668\uff0c\u7528\u4e8e\u5f15\u5bfc\u5206\u6570\u751f\u6210\u6a21\u578b\uff08SBM\uff09\u907f\u514d\u751f\u6210\u4e0d\u9002\u5b9c\u5185\u5bb9\uff08NSFW\uff09\uff0c\u65e0\u9700\u5fae\u8c03\u4e14\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49NSFW\u7c7b\u522b\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6548\u679c\u4e0e\u5fae\u8c03\u65b9\u6cd5\u76f8\u5f53\u4e14\u5bf9\u56fe\u50cf\u8d28\u91cf\u5f71\u54cd\u8f83\u5c0f\u3002", "motivation": "\u5c3d\u7ba1SBM\u5728\u56fe\u50cf\u5408\u6210\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u4f46\u5176\u53ef\u80fd\u751f\u6210\u4e0d\u9002\u5b9c\u5185\u5bb9\uff08\u5982\u66b4\u529b\u6216\u975e\u81ea\u613f\u88f8\u9732\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u4e8e\u6a21\u578b\u81ea\u8eab\u77e5\u8bc6\u4e14\u9700\u5fae\u8c03\uff0c\u672c\u6587\u63a2\u7d22\u5229\u7528\u5916\u90e8\u591a\u6a21\u6001\u6a21\u578b\u6307\u5bfc\u751f\u6210\u5b89\u5168\u5185\u5bb9\u3002", "method": "\u63d0\u51faSFW\u91c7\u6837\u5668\uff0c\u901a\u8fc7**\u6761\u4ef6\u8f68\u8ff9\u6821\u6b63**\u6b65\u9aa4\uff0c\u5229\u7528CLIP\u7b49\u5916\u90e8\u6a21\u578b\u5f15\u5bfc\u6837\u672c\u8fdc\u79bb\u4e0d\u9002\u5b9c\u533a\u57df\uff0c\u5e76\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49NSFW\u7c7b\u522b\u3002", "result": "\u5728Stable Diffusion\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSFW\u91c7\u6837\u5668\u663e\u8457\u51cf\u5c11\u4e0d\u9002\u5b9c\u5185\u5bb9\u751f\u6210\uff0c\u6548\u679c\u4e0e\u5fae\u8c03\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u5bf9\u65e0\u9700\u6821\u6b63\u7684\u6837\u672c\u5f71\u54cd\u53ef\u5ffd\u7565\u3002", "conclusion": "SFW\u91c7\u6837\u5668\u9a8c\u8bc1\u4e86\u6a21\u578b\u65e0\u5173\u6761\u4ef6\u5f15\u5bfc\u7684\u6f5c\u529b\uff0c\u4e3a\u5bf9\u9f50SBM\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u7075\u6d3b\u7684\u5b89\u5168\u751f\u6210\u65b9\u6848\u3002"}}
{"id": "2505.09081", "pdf": "https://arxiv.org/pdf/2505.09081", "abs": "https://arxiv.org/abs/2505.09081", "authors": ["Gaurav Koley"], "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation", "categories": ["cs.SI", "cs.AI", "cs.MA"], "comment": null, "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSALM\u6846\u67b6\uff0c\u5c06\u8bed\u8a00\u6a21\u578b\u878d\u5165\u793e\u4ea4\u7f51\u7edc\u6a21\u62df\uff0c\u5b9e\u73b0\u591a\u573a\u666f\u4e0b\u524d\u6240\u672a\u6709\u7684\u65f6\u95f4\u7a33\u5b9a\u6027\uff0c\u4e3b\u8981\u8d21\u732e\u5305\u62ec\u5206\u5c42\u63d0\u793a\u67b6\u6784\u3001\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8bb0\u5fc6\u7cfb\u7edf\u548c\u4eba\u683c\u7a33\u5b9a\u6027\u754c\u9650\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u793e\u4ea4\u7cfb\u7edf\u5efa\u6a21\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u57fa\u4e8e\u89c4\u5219\u7684\u884c\u4e3a\uff0c\u96be\u4ee5\u6355\u6349\u590d\u6742\u52a8\u6001\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u793e\u4ea4\u4e92\u52a8\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u3002", "method": "\u91c7\u7528SALM\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5c42\u63d0\u793a\u67b6\u6784\uff08\u964d\u4f4e73% token\u4f7f\u7528\uff09\u3001\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff08\u7f13\u5b58\u547d\u4e2d\u738780%\uff09\u548c\u4eba\u683c\u7a33\u5b9a\u6027\u754c\u9650\u3002", "result": "\u5728SNAP ego\u7f51\u7edc\u4e0a\u9a8c\u8bc1\u540e\uff0c\u8bc1\u660e\u8be5\u6846\u67b6\u80fd\u7a33\u5b9a\u6a21\u62df\u8d854000\u65f6\u95f4\u6b65\u957f\uff0c\u4fdd\u6301\u884c\u4e3a\u4fdd\u771f\u5ea6\u3002", "conclusion": "SALM\u662f\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u6846\u67b6\uff0c\u80fd\u5efa\u6a21\u957f\u671f\u793e\u4ea4\u73b0\u8c61\uff0c\u540c\u65f6\u7ef4\u6301\u5b9e\u8bc1\u9a8c\u8bc1\u7684\u884c\u4e3a\u51c6\u786e\u6027\u3002"}}
{"id": "2505.08819", "pdf": "https://arxiv.org/pdf/2505.08819", "abs": "https://arxiv.org/abs/2505.08819", "authors": ["Asahi Miyazaki", "Tsuyoshi Okita"], "title": "Thoughts on Objectives of Sparse and Hierarchical Masked Image Model", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "9 pages, 11 figures", "summary": "Masked image modeling is one of the most poplular objectives of training.\nRecently, the SparK model has been proposed with superior performance among\nself-supervised learning models. This paper proposes a new mask pattern for\nthis SparK model, proposing it as the Mesh Mask-ed SparK model. We report the\neffect of the mask pattern used for image masking in pre-training on\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a9\u6a21\u6a21\u5f0fMesh Mask\uff0c\u7528\u4e8e\u6539\u8fdbSparK\u6a21\u578b\uff0c\u5e76\u7814\u7a76\u4e86\u4e0d\u540c\u63a9\u6a21\u6a21\u5f0f\u5bf9\u9884\u8bad\u7ec3\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u4e0d\u540c\u63a9\u6a21\u6a21\u5f0f\u5bf9\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u9488\u5bf9SparK\u6a21\u578b\u7684\u6539\u8fdb\u3002", "method": "\u63d0\u51faMesh Mask\u6a21\u5f0f\uff0c\u5e94\u7528\u4e8eSparK\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u62a5\u544a\u4e86Mesh Mask\u6a21\u5f0f\u5728\u9884\u8bad\u7ec3\u4e2d\u7684\u6027\u80fd\u6548\u679c\u3002", "conclusion": "Mesh Mask\u6a21\u5f0f\u5bf9SparK\u6a21\u578b\u7684\u6027\u80fd\u6709\u79ef\u6781\u5f71\u54cd\u3002"}}
{"id": "2505.08822", "pdf": "https://arxiv.org/pdf/2505.08822", "abs": "https://arxiv.org/abs/2505.08822", "authors": ["Yuhao Wang", "Kailai Wang", "Songhua Hu", "Yunpeng", "Zhang", "Gino Lim", "Pengyu Zhu"], "title": "The Geography of Transportation Cybersecurity: Visitor Flows, Industry Clusters, and Spatial Dynamics", "categories": ["cs.CY", "cs.LG", "physics.soc-ph"], "comment": null, "summary": "The rapid evolution of the transportation cybersecurity ecosystem,\nencompassing cybersecurity, automotive, and transportation and logistics\nsectors, will lead to the formation of distinct spatial clusters and visitor\nflow patterns across the US. This study examines the spatiotemporal dynamics of\nvisitor flows, analyzing how socioeconomic factors shape industry clustering\nand workforce distribution within these evolving sectors. To model and predict\nvisitor flow patterns, we develop a BiTransGCN framework, integrating an\nattention-based Transformer architecture with a Graph Convolutional Network\nbackbone. By integrating AI-enabled forecasting techniques with spatial\nanalysis, this study improves our ability to track, interpret, and anticipate\nchanges in industry clustering and mobility trends, thereby supporting\nstrategic planning for a secure and resilient transportation network. It offers\na data-driven foundation for economic planning, workforce development, and\ntargeted investments in the transportation cybersecurity ecosystem.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u7f8e\u56fd\u4ea4\u901a\u8fd0\u8f93\u7f51\u7edc\u5b89\u5168\u751f\u6001\u7cfb\u7edf\u4e2d\u8bbf\u5ba2\u6d41\u91cf\u7684\u65f6\u7a7a\u52a8\u6001\uff0c\u901a\u8fc7\u7ed3\u5408\u793e\u4f1a\u7ecf\u6d4e\u56e0\u7d20\u548cAI\u9884\u6d4b\u6280\u672f\uff0c\u63d0\u51fa\u4e86BiTransGCN\u6846\u67b6\u6765\u9884\u6d4b\u884c\u4e1a\u96c6\u7fa4\u548c\u52b3\u52a8\u529b\u5206\u5e03\u3002", "motivation": "\u4ea4\u901a\u8fd0\u8f93\u7f51\u7edc\u5b89\u5168\u751f\u6001\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\u9700\u8981\u5bf9\u5176\u884c\u4e1a\u96c6\u7fa4\u548c\u52b3\u52a8\u529b\u5206\u5e03\u6709\u66f4\u6df1\u5165\u7684\u7406\u89e3\uff0c\u4ee5\u652f\u6301\u7ecf\u6d4e\u89c4\u5212\u548c\u6218\u7565\u6027\u6295\u8d44\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86BiTransGCN\u6846\u67b6\uff0c\u7ed3\u5408\u57fa\u4e8e\u6ce8\u610f\u529b\u7684Transformer\u67b6\u6784\u548c\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u6765\u5efa\u6a21\u548c\u9884\u6d4b\u8bbf\u5ba2\u6d41\u91cf\u6a21\u5f0f\u3002", "result": "\u901a\u8fc7AI\u9884\u6d4b\u6280\u672f\u548c\u7a7a\u95f4\u5206\u6790\u7684\u7ed3\u5408\uff0c\u8be5\u7814\u7a76\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u548c\u9884\u6d4b\u884c\u4e1a\u96c6\u7fa4\u53ca\u6d41\u52a8\u6027\u53d8\u5316\u7684\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u4ea4\u901a\u8fd0\u8f93\u7f51\u7edc\u5b89\u5168\u751f\u6001\u7cfb\u7edf\u7684\u7ecf\u6d4e\u89c4\u5212\u3001\u52b3\u52a8\u529b\u53d1\u5c55\u548c\u9488\u5bf9\u6027\u6295\u8d44\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u5b89\u5168\u548c\u6709\u97e7\u6027\u7684\u4ea4\u901a\u7f51\u7edc\u3002"}}
{"id": "2505.09091", "pdf": "https://arxiv.org/pdf/2505.09091", "abs": "https://arxiv.org/abs/2505.09091", "authors": ["Zeeshan Ahmad", "Shudi Bao", "Meng Chen"], "title": "DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "eess.AS"], "comment": null, "summary": "In recent years, generative adversarial networks (GANs) have made significant\nprogress in generating audio sequences. However, these models typically rely on\nbandwidth-limited mel-spectrograms, which constrain the resolution of generated\naudio sequences, and lead to mode collapse during conditional generation. To\naddress this issue, we propose Deformable Periodic Network based GAN (DPN-GAN),\na novel GAN architecture that incorporates a kernel-based periodic ReLU\nactivation function to induce periodic bias in audio generation. This\ninnovative approach enhances the model's ability to capture and reproduce\nintricate audio patterns. In particular, our proposed model features a DPN\nmodule for multi-resolution generation utilizing deformable convolution\noperations, allowing for adaptive receptive fields that improve the quality and\nfidelity of the synthetic audio. Additionally, we enhance the discriminator\nnetwork using deformable convolution to better distinguish between real and\ngenerated samples, further refining the audio quality. We trained two versions\nof the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M\nparameters). For evaluation, we use five different datasets, covering both\nspeech synthesis and music generation tasks, to demonstrate the efficiency of\nthe DPN-GAN. The experimental results demonstrate that DPN-GAN delivers\nsuperior performance on both out-of-distribution and noisy data, showcasing its\nrobustness and adaptability. Trained across various datasets, DPN-GAN\noutperforms state-of-the-art GAN architectures on standard evaluation metrics,\nand exhibits increased robustness in synthesized audio.", "AI": {"tldr": "DPN-GAN\u662f\u4e00\u79cd\u65b0\u578bGAN\u67b6\u6784,\u901a\u8fc7\u53ef\u53d8\u5f62\u5468\u671f\u7f51\u7edc\u548c\u591a\u5206\u8fa8\u7387\u751f\u6210\u89e3\u51b3\u97f3\u9891\u751f\u6210\u4e2d\u7684\u5206\u8fa8\u7387\u9650\u5236\u548c\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898,\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8emel\u9891\u8c31\u7684GAN\u5728\u97f3\u9891\u751f\u6210\u4e2d\u5b58\u5728\u5206\u8fa8\u7387\u9650\u5236\u548c\u6a21\u5f0f\u5d29\u6e83\u7684\u95ee\u9898,\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51faDPN-GAN,\u5f15\u5165\u57fa\u4e8e\u6838\u7684\u5468\u671fReLU\u6fc0\u6d3b\u51fd\u6570\u548c\u53ef\u53d8\u5f62\u5377\u79ef\u64cd\u4f5c,\u589e\u5f3a\u97f3\u9891\u6a21\u5f0f\u6355\u6349\u80fd\u529b,\u5e76\u6539\u8fdb\u5224\u522b\u5668\u7f51\u7edc\u3002", "result": "DPN-GAN\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u4f18GAN\u6a21\u578b,\u4e14\u5bf9\u5c0f\u6837\u672c\u548c\u566a\u58f0\u6570\u636e\u8868\u73b0\u66f4\u9c81\u68d2\u3002", "conclusion": "DPN-GAN\u901a\u8fc7\u53ef\u53d8\u5f62\u5468\u671f\u7f51\u7edc\u663e\u8457\u63d0\u5347\u97f3\u9891\u751f\u6210\u8d28\u91cf,\u5177\u6709\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.08833", "pdf": "https://arxiv.org/pdf/2505.08833", "abs": "https://arxiv.org/abs/2505.08833", "authors": ["Qingyi Wang", "Yuebing Liang", "Yunhan Zheng", "Kaiyuan Xu", "Jinhua Zhao", "Shenhao Wang"], "title": "Generative AI for Urban Planning: Synthesizing Satellite Imagery via Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Generative AI offers new opportunities for automating urban planning by\ncreating site-specific urban layouts and enabling flexible design exploration.\nHowever, existing approaches often struggle to produce realistic and practical\ndesigns at scale. Therefore, we adapt a state-of-the-art Stable Diffusion\nmodel, extended with ControlNet, to generate high-fidelity satellite imagery\nconditioned on land use descriptions, infrastructure, and natural environments.\nTo overcome data availability limitations, we spatially link satellite imagery\nwith structured land use and constraint information from OpenStreetMap. Using\ndata from three major U.S. cities, we demonstrate that the proposed diffusion\nmodel generates realistic and diverse urban landscapes by varying land-use\nconfigurations, road networks, and water bodies, facilitating cross-city\nlearning and design diversity. We also systematically evaluate the impacts of\nvarying language prompts and control imagery on the quality of satellite\nimagery generation. Our model achieves high FID and KID scores and demonstrates\nrobustness across diverse urban contexts. Qualitative assessments from urban\nplanners and the general public show that generated images align closely with\ndesign descriptions and constraints, and are often preferred over real images.\nThis work establishes a benchmark for controlled urban imagery generation and\nhighlights the potential of generative AI as a tool for enhancing planning\nworkflows and public engagement.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6539\u8fdbStable Diffusion\u6a21\u578b\uff08\u7ed3\u5408ControlNet\uff09\uff0c\u5229\u7528OpenStreetMap\u6570\u636e\u751f\u6210\u9ad8\u4fdd\u771f\u536b\u661f\u56fe\u50cf\uff0c\u5c55\u793a\u4e86\u751f\u6210\u5f0fAI\u5728\u57ce\u5e02\u89c4\u5212\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u8bbe\u8ba1\u4e0e\u516c\u4f17\u53c2\u4e0e\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u5927\u89c4\u6a21\u3001\u73b0\u5b9e\u7684\u57ce\u5e02\u573a\u666f\u5e03\u5c40\u3002\u7814\u7a76\u65e8\u5728\u5229\u7528\u751f\u6210\u5f0fAI\u63d0\u5347\u57ce\u5e02\u89c4\u5212\u7684\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u652f\u6301\u7075\u6d3b\u8bbe\u8ba1\u63a2\u7d22\u3002", "method": "\u7ed3\u5408Stable Diffusion\u4e0eControlNet\uff0c\u8f93\u5165\u571f\u5730\u7528\u9014\u63cf\u8ff0\u3001\u57fa\u7840\u8bbe\u65bd\u7b49\u6761\u4ef6\uff1b\u901a\u8fc7OpenStreetMap\u94fe\u63a5\u536b\u661f\u56fe\u50cf\u4e0e\u7ed3\u6784\u5316\u6570\u636e\uff1b\u5728\u4e09\u4e2a\u7f8e\u56fd\u57ce\u5e02\u9a8c\u8bc1\u6a21\u578b\u3002", "result": "\u6a21\u578b\u751f\u6210\u591a\u6837\u4e14\u771f\u5b9e\u7684\u57ce\u5e02\u573a\u666f\uff0cFID/KID\u5206\u6570\u9ad8\uff1b\u8bed\u8a00\u63d0\u793a\u548c\u63a7\u5236\u56fe\u50cf\u7684\u8c03\u6574\u663e\u8457\u5f71\u54cd\u8f93\u51fa\u8d28\u91cf\uff1b\u751f\u6210\u56fe\u50cf\u88ab\u89c4\u5212\u5e08\u548c\u516c\u4f17\u504f\u597d\u8d85\u8fc7\u771f\u5b9e\u56fe\u50cf\u3002", "conclusion": "\u7814\u7a76\u4e3a\u53ef\u63a7\u57ce\u5e02\u56fe\u50cf\u751f\u6210\u8bbe\u5b9a\u4e86\u57fa\u51c6\uff0c\u8bc1\u660e\u751f\u6210\u5f0fAI\u53ef\u4f18\u5316\u89c4\u5212\u6d41\u7a0b\u548c\u516c\u4f17\u53c2\u4e0e\u3002"}}
{"id": "2505.09108", "pdf": "https://arxiv.org/pdf/2505.09108", "abs": "https://arxiv.org/abs/2505.09108", "authors": ["Fernando Cladera", "Zachary Ravichandran", "Jason Hughes", "Varun Murali", "Carlos Nieto-Granda", "M. Ani Hsieh", "George J. Pappas", "Camillo J. Taylor", "Vijay Kumar"], "title": "Air-Ground Collaboration for Language-Specified Missions in Unknown Environments", "categories": ["cs.RO", "cs.AI"], "comment": "19 pages, 24 figures, 7 tables. Submitted to T-FR", "summary": "As autonomous robotic systems become increasingly mature, users will want to\nspecify missions at the level of intent rather than in low-level detail.\nLanguage is an expressive and intuitive medium for such mission specification.\nHowever, realizing language-guided robotic teams requires overcoming\nsignificant technical hurdles. Interpreting and realizing language-specified\nmissions requires advanced semantic reasoning. Successful heterogeneous robots\nmust effectively coordinate actions and share information across varying\nviewpoints. Additionally, communication between robots is typically\nintermittent, necessitating robust strategies that leverage communication\nopportunities to maintain coordination and achieve mission objectives. In this\nwork, we present a first-of-its-kind system where an unmanned aerial vehicle\n(UAV) and an unmanned ground vehicle (UGV) are able to collaboratively\naccomplish missions specified in natural language while reacting to changes in\nspecification on the fly. We leverage a Large Language Model (LLM)-enabled\nplanner to reason over semantic-metric maps that are built online and\nopportunistically shared between an aerial and a ground robot. We consider\ntask-driven navigation in urban and rural areas. Our system must infer\nmission-relevant semantics and actively acquire information via semantic\nmapping. In both ground and air-ground teaming experiments, we demonstrate our\nsystem on seven different natural-language specifications at up to\nkilometer-scale navigation.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578b\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6307\u5bfc\u65e0\u4eba\u673a\uff08UAV\uff09\u548c\u65e0\u4eba\u5730\u9762\u8f66\uff08UGV\uff09\u534f\u4f5c\u5b8c\u6210\u4efb\u52a1\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u89c4\u5212\u548c\u8bed\u4e49\u63a8\u7406\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6210\u719f\uff0c\u7528\u6237\u5e0c\u671b\u901a\u8fc7\u66f4\u9ad8\u5c42\u6b21\u7684\u610f\u56fe\uff08\u800c\u975e\u4f4e\u5c42\u7ec6\u8282\uff09\u6307\u5b9a\u4efb\u52a1\uff0c\u8bed\u8a00\u4f5c\u4e3a\u4e00\u79cd\u76f4\u89c2\u4e14\u8868\u8fbe\u4e30\u5bcc\u7684\u5a92\u4ecb\u4e3a\u6b64\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002\u4f46\u5b9e\u73b0\u8bed\u8a00\u6307\u5bfc\u7684\u673a\u5668\u4eba\u56e2\u961f\u534f\u4f5c\u9762\u4e34\u8bed\u4e49\u63a8\u7406\u3001\u5f02\u6784\u673a\u5668\u4eba\u534f\u8c03\u53ca\u95f4\u6b47\u901a\u4fe1\u7b49\u6311\u6218\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u89c4\u5212\u5668\uff0c\u7528\u4e8e\u5728\u5b9e\u65f6\u6784\u5efa\u7684\u8bed\u4e49-\u5ea6\u91cf\u5730\u56fe\u4e0a\u8fdb\u884c\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u673a\u4f1a\u6027\u5171\u4eab\u4fe1\u606f\u5b9e\u73b0\u7a7a\u4e2d\u4e0e\u5730\u9762\u673a\u5668\u4eba\u7684\u534f\u4f5c\u3002\u7cfb\u7edf\u652f\u6301\u4efb\u52a1\u9a71\u52a8\u7684\u5bfc\u822a\uff0c\u5e76\u80fd\u52a8\u6001\u54cd\u5e94\u8bed\u8a00\u6307\u4ee4\u7684\u53d8\u5316\u3002", "result": "\u5728\u57ce\u5e02\u548c\u4e61\u6751\u73af\u5883\u7684\u5b9e\u9a8c\u4e2d\uff0c\u7cfb\u7edf\u6210\u529f\u5b8c\u6210\u4e86\u4e03\u79cd\u4e0d\u540c\u7684\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u89c4\u8303\uff0c\u5b9e\u73b0\u4e86\u516c\u91cc\u7ea7\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5f02\u6784\u673a\u5668\u4eba\u534f\u4f5c\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5b9e\u73b0\u4e86\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u5bfc\u65e0\u4eba\u673a\u4e0e\u65e0\u4eba\u5730\u9762\u8f66\u7684\u52a8\u6001\u534f\u4f5c\uff0c\u5c55\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u548c\u8bed\u4e49\u63a8\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.08837", "pdf": "https://arxiv.org/pdf/2505.08837", "abs": "https://arxiv.org/abs/2505.08837", "authors": ["Muhammad Saqib", "Dipkumar Mehta", "Fnu Yashu", "Shubham Malhotra"], "title": "Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning", "categories": ["cs.CR", "cs.CV", "cs.DC", "cs.LG", "cs.NI"], "comment": "10 pages, 6 figures, 1 table", "summary": "The security of cloud environments, such as Amazon Web Services (AWS), is\ncomplex and dynamic. Static security policies have become inadequate as threats\nevolve and cloud resources exhibit elasticity [1]. This paper addresses the\nlimitations of static policies by proposing a security policy management\nframework that uses reinforcement learning (RL) to adapt dynamically.\nSpecifically, we employ deep reinforcement learning algorithms, including deep\nQ Networks and proximal policy optimization, enabling the learning and\ncontinuous adjustment of controls such as firewall rules and Identity and\nAccess Management (IAM) policies. The proposed RL based solution leverages\ncloud telemetry data (AWS Cloud Trail logs, network traffic data, threat\nintelligence feeds) to continuously refine security policies, maximizing threat\nmitigation, and compliance while minimizing resource impact. Experimental\nresults demonstrate that our adaptive RL based framework significantly\noutperforms static policies, achieving higher intrusion detection rates (92%\ncompared to 82% for static policies) and substantially reducing incident\ndetection and response times by 58%. In addition, it maintains high conformity\nwith security requirements and efficient resource usage. These findings\nvalidate the effectiveness of adaptive reinforcement learning approaches in\nimproving cloud security policy management.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u52a8\u6001\u5b89\u5168\u7b56\u7565\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6Q\u7f51\u7edc\u548c\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u52a8\u6001\u8c03\u6574\u4e91\u73af\u5883\u4e2d\u7684\u5b89\u5168\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5165\u4fb5\u68c0\u6d4b\u7387\u548c\u54cd\u5e94\u6548\u7387\u3002", "motivation": "\u9759\u6001\u5b89\u5168\u7b56\u7565\u5728\u52a8\u6001\u4e91\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u9002\u5e94\u5a01\u80c1\u7684\u5feb\u901f\u53d8\u5316\u548c\u8d44\u6e90\u7684\u5f39\u6027\u9700\u6c42\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u5229\u7528RL\u6280\u672f\u52a8\u6001\u4f18\u5316\u5b89\u5168\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u5982\u6df1\u5ea6Q\u7f51\u7edc\u548c\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff09\uff0c\u7ed3\u5408\u4e91\u9065\u6d4b\u6570\u636e\uff08\u5982AWS Cloud Trail\u65e5\u5fd7\u3001\u7f51\u7edc\u6d41\u91cf\u6570\u636e\u548c\u5a01\u80c1\u60c5\u62a5\uff09\u6301\u7eed\u4f18\u5316\u9632\u706b\u5899\u89c4\u5219\u548cIAM\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5165\u4fb5\u68c0\u6d4b\u7387\u8fbe92%\uff08\u9759\u6001\u7b56\u7565\u4e3a82%\uff09\uff0c\u4e8b\u4ef6\u68c0\u6d4b\u548c\u54cd\u5e94\u65f6\u95f4\u51cf\u5c1158%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u5408\u89c4\u6027\u548c\u8d44\u6e90\u6548\u7387\u3002", "conclusion": "\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u4e91\u5b89\u5168\u7b56\u7565\u7ba1\u7406\u7684\u6548\u679c\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u52a8\u6001\u4e91\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.09115", "pdf": "https://arxiv.org/pdf/2505.09115", "abs": "https://arxiv.org/abs/2505.09115", "authors": ["Yu Lun Hsu", "Yun-Rung Chou", "Chiao-Ju Chang", "Yu-Cheng Chang", "Zer-Wei Lee", "Rokas Gipi\u0161kis", "Rachel Li", "Chih-Yuan Shih", "Jen-Kuei Peng", "Hsien-Liang Huang", "Jaw-Shiun Tsai", "Mike Y. Chen"], "title": "PreCare: Designing AI Assistants for Advance Care Planning (ACP) to Enhance Personal Value Exploration, Patient Knowledge, and Decisional Confidence", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Advance Care Planning (ACP) allows individuals to specify their preferred\nend-of-life life-sustaining treatments before they become incapacitated by\ninjury or terminal illness (e.g., coma, cancer, dementia). While online ACP\noffers high accessibility, it lacks key benefits of clinical consultations,\nincluding personalized value exploration, immediate clarification of decision\nconsequences. To bridge this gap, we conducted two formative studies: 1)\nshadowed and interviewed 3 ACP teams consisting of physicians, nurses, and\nsocial workers (18 patients total), and 2) interviewed 14 users of ACP\nwebsites. Building on these insights, we designed PreCare in collaboration with\n6 ACP professionals. PreCare is a website with 3 AI-driven assistants designed\nto guide users through exploring personal values, gaining ACP knowledge, and\nsupporting informed decision-making. A usability study (n=12) showed that\nPreCare achieved a System Usability Scale (SUS) rating of excellent. A\ncomparative evaluation (n=12) showed that PreCare's AI assistants significantly\nimproved exploration of personal values, knowledge, and decisional confidence,\nand was preferred by 92% of participants.", "AI": {"tldr": "\u6587\u7ae0\u4ecb\u7ecd\u4e86PreCare\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u8fc7AI\u8f85\u52a9\u5e2e\u52a9\u7528\u6237\u8fdb\u884c\u9884\u5148\u533b\u7597\u89c4\u5212\u7684\u7f51\u7ad9\uff0c\u65e8\u5728\u5f25\u8865\u5728\u7ebfACP\u7f3a\u4e4f\u4e34\u5e8a\u54a8\u8be2\u4f18\u52bf\u7684\u4e0d\u8db3\u3002\u7814\u7a76\u663e\u793a\uff0cPreCare\u5728\u7528\u6237\u4f53\u9a8c\u548c\u6548\u679c\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u7ebf\u9884\u5148\u533b\u7597\u89c4\u5212\uff08ACP\uff09\u867d\u7136\u4fbf\u6377\uff0c\u4f46\u7f3a\u4e4f\u4e34\u5e8a\u54a8\u8be2\u7684\u4e2a\u4eba\u5316\u4ef7\u503c\u63a2\u8ba8\u548c\u5373\u65f6\u6f84\u6e05\u51b3\u7b56\u540e\u679c\u7684\u4f18\u52bf\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7AI\u6280\u672f\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u5f62\u6210\u6027\u7814\u7a76\uff08\u89c2\u5bdf\u8bbf\u8c0818\u540d\u60a3\u8005\u548c14\u540d\u7f51\u7ad9\u7528\u6237\uff09\u8bbe\u8ba1PreCare\u7f51\u7ad9\uff0c\u5e76\u8fdb\u884c\u4e86\u53ef\u7528\u6027\u7814\u7a76\u548c\u5bf9\u6bd4\u8bc4\u4f30\uff08\u540412\u540d\u53c2\u4e0e\u8005\uff09\u3002", "result": "PreCare\u5728\u7cfb\u7edf\u53ef\u7528\u6027\u8bc4\u5206\u4e0a\u8868\u73b0\u4f18\u79c0\uff0cAI\u52a9\u624b\u663e\u8457\u63d0\u5347\u4e86\u4e2a\u4eba\u4ef7\u503c\u63a2\u7d22\u3001\u77e5\u8bc6\u83b7\u53d6\u548c\u51b3\u7b56\u4fe1\u5fc3\uff0c92%\u7684\u53c2\u4e0e\u8005\u66f4\u504f\u597dPreCare\u3002", "conclusion": "PreCare\u901a\u8fc7AI\u8f85\u52a9\u6709\u6548\u63d0\u5347\u4e86\u5728\u7ebfACP\u7684\u4f53\u9a8c\u548c\u6548\u679c\uff0c\u4e3a\u9884\u5148\u533b\u7597\u89c4\u5212\u63d0\u4f9b\u4e86\u53ef\u884c\u4e14\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09129", "pdf": "https://arxiv.org/pdf/2505.09129", "abs": "https://arxiv.org/abs/2505.09129", "authors": ["Wei Meng"], "title": "WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes", "categories": ["cs.CV", "cs.AI", "es: 68T10, 68T05, 62H35, 68U10", "I.4.9; I.5.1; I.2.10"], "comment": "17 pages, 3 figures, 3 tables. The paper proposes a lightweight\n  weakly-supervised color intelligence model for tactical video anomaly\n  detection, tested on anonymized African surveillance data", "summary": "The deployment of traditional deep learning models in high-risk security\ntasks in an unlabeled, data-non-exploitable video intelligence environment\nfaces significant challenges. In this paper, we propose a lightweight anomaly\ndetection framework based on color features for surveillance video clips in a\nhigh sensitivity tactical mission, aiming to quickly identify and interpret\npotential threat events under resource-constrained and data-sensitive\nconditions. The method fuses unsupervised KMeans clustering with RGB channel\nhistogram modeling to achieve composite detection of structural anomalies and\ncolor mutation signals in key frames. The experiment takes an operation\nsurveillance video occurring in an African country as a research sample, and\nsuccessfully identifies multiple highly anomalous frames related to high-energy\nlight sources, target presence, and reflective interference under the condition\nof no access to the original data. The results show that this method can be\neffectively used for tactical assassination warning, suspicious object\nscreening and environmental drastic change monitoring with strong deployability\nand tactical interpretation value. The study emphasizes the importance of color\nfeatures as low semantic battlefield signal carriers, and its battlefield\nintelligent perception capability will be further extended by combining graph\nneural networks and temporal modeling in the future.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u989c\u8272\u7279\u5f81\u7684\u8f7b\u91cf\u7ea7\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u4e14\u6570\u636e\u654f\u611f\u7684\u6218\u672f\u4efb\u52a1\u4e2d\u5feb\u901f\u8bc6\u522b\u6f5c\u5728\u5a01\u80c1\u4e8b\u4ef6\u3002\u65b9\u6cd5\u7ed3\u5408\u65e0\u76d1\u7763KMeans\u805a\u7c7b\u548cRGB\u901a\u9053\u76f4\u65b9\u56fe\u5efa\u6a21\uff0c\u6210\u529f\u5728\u65e0\u6cd5\u8bbf\u95ee\u539f\u59cb\u6570\u636e\u7684\u6761\u4ef6\u4e0b\u68c0\u6d4b\u5230\u5f02\u5e38\u5e27\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u5b89\u5168\u4efb\u52a1\u4e2d\u9762\u5bf9\u65e0\u6807\u7b7e\u3001\u6570\u636e\u4e0d\u53ef\u5229\u7528\u7684\u89c6\u9891\u73af\u5883\u65f6\u5b58\u5728\u6311\u6218\uff0c\u9700\u4e00\u79cd\u8f7b\u91cf\u4e14\u9ad8\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4ee5\u9002\u5e94\u6218\u672f\u4efb\u52a1\u9700\u6c42\u3002", "method": "\u91c7\u7528\u65e0\u76d1\u7763KMeans\u805a\u7c7b\u4e0eRGB\u901a\u9053\u76f4\u65b9\u56fe\u5efa\u6a21\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5bf9\u5173\u952e\u5e27\u8fdb\u884c\u7ed3\u6784\u5f02\u5e38\u548c\u989c\u8272\u7a81\u53d8\u4fe1\u53f7\u7684\u590d\u5408\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u6210\u529f\u8bc6\u522b\u4e86\u4e0e\u9ad8\u80fd\u5149\u6e90\u3001\u76ee\u6807\u51fa\u73b0\u53ca\u53cd\u5c04\u5e72\u6270\u76f8\u5173\u7684\u5f02\u5e38\u5e27\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u6218\u672f\u9884\u8b66\u3001\u53ef\u7591\u5bf9\u8c61\u7b5b\u9009\u548c\u73af\u5883\u7a81\u53d8\u76d1\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u989c\u8272\u7279\u5f81\u4f5c\u4e3a\u4f4e\u8bed\u4e49\u6218\u573a\u4fe1\u53f7\u8f7d\u4f53\u7684\u91cd\u8981\u6027\uff0c\u672a\u6765\u5c06\u901a\u8fc7\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u65f6\u95f4\u5efa\u6a21\u8fdb\u4e00\u6b65\u6269\u5c55\u5176\u6218\u573a\u667a\u80fd\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2505.08886", "pdf": "https://arxiv.org/pdf/2505.08886", "abs": "https://arxiv.org/abs/2505.08886", "authors": ["Hamideh Khaleghpour", "Brett McKinney"], "title": "Optimizing Neuro-Fuzzy and Colonial Competition Algorithms for Skin Cancer Diagnosis in Dermatoscopic Images", "categories": ["cs.CV", "cs.LG"], "comment": "7 pages, 10 figures. Accepted at the 2nd Asia Pacific Computer\n  Systems Conference (APCS 2024), March 15-17, 2024", "summary": "The rising incidence of skin cancer, coupled with limited public awareness\nand a shortfall in clinical expertise, underscores an urgent need for advanced\ndiagnostic aids. Artificial Intelligence (AI) has emerged as a promising tool\nin this domain, particularly for distinguishing malignant from benign skin\nlesions. Leveraging publicly available datasets of skin lesions, researchers\nhave been developing AI-based diagnostic solutions. However, the integration of\nsuch computer systems in clinical settings is still nascent. This study aims to\nbridge this gap by employing a fusion of image processing techniques and\nmachine learning algorithms, specifically neuro-fuzzy and colonial competition\napproaches. Applied to dermoscopic images from the ISIC database, our method\nachieved a notable accuracy of 94% on a dataset of 560 images. These results\nunderscore the potential of our approach in aiding clinicians in the early\ndetection of melanoma, thereby contributing significantly to skin cancer\ndiagnostics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u56fe\u50cf\u5904\u7406\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08\u795e\u7ecf\u6a21\u7cca\u548c\u6b96\u6c11\u7ade\u4e89\u65b9\u6cd5\uff09\uff0c\u5728\u76ae\u80a4\u764c\u8bca\u65ad\u9886\u57df\u53d6\u5f9794%\u7684\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86AI\u5728\u65e9\u671f\u9ed1\u8272\u7d20\u7624\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u76ae\u80a4\u764c\u53d1\u75c5\u7387\u4e0a\u5347\uff0c\u516c\u4f17\u610f\u8bc6\u4e0d\u8db3\u548c\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u77ed\u7f3a\uff0c\u8feb\u5207\u9700\u8981\u5148\u8fdb\u7684\u8bca\u65ad\u8f85\u52a9\u5de5\u5177\uff0cAI\u5728\u6b64\u9886\u57df\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u91c7\u7528\u56fe\u50cf\u5904\u7406\u6280\u672f\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08\u795e\u7ecf\u6a21\u7cca\u548c\u6b96\u6c11\u7ade\u4e89\u65b9\u6cd5\uff09\u5bf9ISIC\u6570\u636e\u5e93\u4e2d\u7684\u76ae\u80a4\u955c\u56fe\u50cf\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5728560\u5f20\u56fe\u50cf\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e8694%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u76ae\u80a4\u764c\u65e9\u671f\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u4e34\u5e8a\u8bca\u65ad\u6548\u679c\u3002"}}
{"id": "2505.09142", "pdf": "https://arxiv.org/pdf/2505.09142", "abs": "https://arxiv.org/abs/2505.09142", "authors": ["Seungbeom Choi", "Jeonghoe Goo", "Eunjoo Jeon", "Mingyu Yang", "Minsung Jang"], "title": "ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": "13 pages, 5 figures. Cloud-native LLM scheduling system with\n  latency-aware inference optimization", "summary": "We propose ELIS, a serving system for Large Language Models (LLMs) featuring\nan Iterative Shortest Remaining Time First (ISRTF) scheduler designed to\nefficiently manage inference tasks with the shortest remaining tokens. Current\nLLM serving systems often employ a first-come-first-served scheduling strategy,\nwhich can lead to the \"head-of-line blocking\" problem. To overcome this\nlimitation, it is necessary to predict LLM inference times and apply a shortest\njob first scheduling strategy. However, due to the auto-regressive nature of\nLLMs, predicting the inference latency is challenging. ELIS addresses this\nchallenge by training a response length predictor for LLMs using the BGE model,\nan encoder-based state-of-the-art model. Additionally, we have devised the\nISRTF scheduling strategy, an optimization of shortest remaining time first\ntailored to existing LLM iteration batching. To evaluate our work in an\nindustrial setting, we simulate streams of requests based on our study of\nreal-world user LLM serving trace records. Furthermore, we implemented ELIS as\na cloud-native scheduler system on Kubernetes to evaluate its performance in\nproduction environments. Our experimental results demonstrate that ISRTF\nreduces the average job completion time by up to 19.6%.", "AI": {"tldr": "ELIS\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u670d\u52a1\u7cfb\u7edf\uff0c\u91c7\u7528\u57fa\u4e8e\u6700\u77ed\u5269\u4f59\u4ee4\u724c\u7684\u8fed\u4ee3\u8c03\u5ea6\u7b56\u7565(ISRTF)\uff0c\u663e\u8457\u51cf\u5c11\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709LLM\u670d\u52a1\u7cfb\u7edf\u91c7\u7528\u5148\u5230\u5148\u5f97\u8c03\u5ea6\u7b56\u7565\uff0c\u5bb9\u6613\u5bfc\u81f4\u2018\u961f\u5934\u963b\u585e\u2019\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u8c03\u5ea6\u6548\u7387\u3002", "method": "\u8bad\u7ec3BGE\u6a21\u578b\u9884\u6d4b\u54cd\u5e94\u957f\u5ea6\uff0c\u8bbe\u8ba1ISRTF\u8c03\u5ea6\u7b56\u7565\u4f18\u5316\u8fed\u4ee3\u6279\u5904\u7406\uff0c\u5e76\u57fa\u4e8eKubernetes\u5b9e\u73b0\u4e91\u539f\u751f\u8c03\u5ea6\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u8868\u660eISRTF\u5c06\u5e73\u5747\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u964d\u4f4e19.6%\u3002", "conclusion": "ELIS\u901a\u8fc7\u667a\u80fd\u8c03\u5ea6\u6709\u6548\u63d0\u5347LLM\u670d\u52a1\u6548\u7387\uff0c\u9002\u7528\u4e8e\u751f\u4ea7\u73af\u5883\u3002"}}
{"id": "2505.08899", "pdf": "https://arxiv.org/pdf/2505.08899", "abs": "https://arxiv.org/abs/2505.08899", "authors": ["Andrew Mullhaupt", "Cheng Peng"], "title": "Bounding Neyman-Pearson Region with $f$-Divergences", "categories": ["math.ST", "cs.LG", "stat.ML", "stat.TH"], "comment": null, "summary": "The Neyman-Pearson region of a simple binary hypothesis testing is the set of\npoints whose coordinates represent the false positive rate and false negative\nrate of some test. The lower boundary of this region is given by the\nNeyman-Pearson lemma, and is up to a coordinate change, equivalent to the\noptimal ROC curve. We establish a novel lower bound for the boundary in terms\nof any $f$-divergence. Since the bound generated by hockey-stick\n$f$-divergences characterizes the Neyman-Pearson boundary, this bound is best\npossible. In the case of KL divergence, this bound improves Pinsker's\ninequality. Furthermore, we obtain a closed-form refined upper bound for the\nNeyman-Pearson boundary in terms of the Chernoff $\\alpha$-coefficient. Finally,\nwe present methods for constructing pairs of distributions that can\napproximately or exactly realize any given Neyman-Pearson boundary.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e$f$-\u6563\u5ea6\u7684Neyman-Pearson\u8fb9\u754c\u4e0b\u754c\uff0c\u8bc1\u660e\u5176\u6700\u4f18\u6027\uff0c\u5e76\u5728KL\u6563\u5ea6\u4e0b\u6539\u8fdb\u4e86Pinsker\u4e0d\u7b49\u5f0f\uff1b\u540c\u65f6\u7ed9\u51fa\u4e86\u57fa\u4e8eChernoff\u7cfb\u6570\u7684\u4e0a\u754c\uff0c\u5e76\u63d0\u4f9b\u4e86\u6784\u9020\u5206\u5e03\u5bf9\u4ee5\u5b9e\u73b0\u4efb\u610fNeyman-Pearson\u8fb9\u754c\u7684\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76Neyman-Pearson\u8fb9\u754c\u5728\u4e8c\u5143\u5047\u8bbe\u68c0\u9a8c\u4e2d\u7684\u4e0b\u754c\u95ee\u9898\uff0c\u65e8\u5728\u901a\u8fc7$f$-\u6563\u5ea6\u63d0\u4f9b\u66f4\u4f18\u7684\u8fb9\u754c\u523b\u753b\uff0c\u5e76\u63a2\u7d22\u5206\u5e03\u5bf9\u7684\u6784\u9020\u65b9\u6cd5\u4ee5\u9a8c\u8bc1\u7406\u8bba\u7ed3\u679c\u3002", "method": "\u5229\u7528$f$-\u6563\u5ea6\uff08\u5c24\u5176\u66f2\u68cd\u7403\u6563\u5ea6\uff09\u63a8\u5bfc\u8fb9\u754c\u4e0b\u754c\uff0c\u901a\u8fc7KL\u6563\u5ea6\u6539\u8fdbPinsker\u4e0d\u7b49\u5f0f\uff1b\u57fa\u4e8eChernoff\u7cfb\u6570\u63d0\u51fa\u95ed\u5f0f\u4e0a\u754c\uff1b\u8bbe\u8ba1\u5206\u5e03\u5bf9\u6784\u9020\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u7406\u8bba\u8fb9\u754c\u3002", "result": "\u8bc1\u660e\u4e86$f$-\u6563\u5ea6\u4e0b\u754c\u7684\u6700\u4f18\u6027\uff0cKL\u6563\u5ea6\u4e0b\u6539\u8fdb\u4e86Pinsker\u4e0d\u7b49\u5f0f\uff1b\u83b7\u5f97\u4e86Chernoff\u7cfb\u6570\u4e0a\u754c\uff1b\u63d0\u51fa\u4e86\u7cbe\u786e\u6216\u8fd1\u4f3c\u5b9e\u73b0\u4efb\u610f\u8fb9\u754c\u7684\u5206\u5e03\u5bf9\u6784\u9020\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aNeyman-Pearson\u8fb9\u754c\u63d0\u4f9b\u4e86\u666e\u9002\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4e0b\u754c\u548c\u4e0a\u754c\u7684\u6539\u8fdb\u5177\u6709\u7406\u8bba\u610f\u4e49\uff0c\u5206\u5e03\u6784\u9020\u65b9\u6cd5\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5de5\u5177\u3002"}}
{"id": "2505.08908", "pdf": "https://arxiv.org/pdf/2505.08908", "abs": "https://arxiv.org/abs/2505.08908", "authors": ["Benedikt Koch", "Kosuke Imai"], "title": "Statistical Decision Theory with Counterfactual Loss", "categories": ["math.ST", "cs.LG", "econ.TH", "stat.TH"], "comment": null, "summary": "Classical statistical decision theory evaluates treatment choices based\nsolely on observed outcomes. However, by ignoring counterfactual outcomes, it\ncannot assess the quality of decisions relative to feasible alternatives. For\nexample, the quality of a physician's decision may depend not only on patient\nsurvival, but also on whether a less invasive treatment could have produced a\nsimilar result. To address this limitation, we extend standard decision theory\nto incorporate counterfactual losses--criteria that evaluate decisions using\nall potential outcomes. The central challenge in this generalization is\nidentification: because only one potential outcome is observed for each unit,\nthe associated risk under a counterfactual loss is generally not identifiable.\nWe show that under the assumption of strong ignorability, a counterfactual risk\nis identifiable if and only if the counterfactual loss function is additive in\nthe potential outcomes. Moreover, we demonstrate that additive counterfactual\nlosses can yield treatment recommendations that differ from those based on\nstandard loss functions, provided that the decision problem involves more than\ntwo treatment options.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6269\u5c55\u4e86\u4f20\u7edf\u7edf\u8ba1\u51b3\u7b56\u7406\u8bba\uff0c\u5f15\u5165\u4e86\u53cd\u4e8b\u5b9e\u635f\u5931\u4ee5\u8bc4\u4f30\u51b3\u7b56\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u4ec5\u4f9d\u8d56\u89c2\u5bdf\u7ed3\u679c\u800c\u5ffd\u7565\u53cd\u4e8b\u5b9e\u7ed3\u679c\u7684\u5c40\u9650\u6027\u3002\u901a\u8fc7\u5047\u8bbe\u5f3a\u53ef\u5ffd\u7565\u6027\uff0c\u63d0\u51fa\u4e86\u53cd\u4e8b\u5b9e\u98ce\u9669\u53ef\u8bc6\u522b\u7684\u6761\u4ef6\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u591a\u6cbb\u7597\u9009\u9879\u4e0b\u53cd\u4e8b\u5b9e\u635f\u5931\u53ef\u80fd\u5e26\u6765\u4e0d\u540c\u7684\u6cbb\u7597\u65b9\u6848\u63a8\u8350\u3002", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u51b3\u7b56\u7406\u8bba\u4ec5\u57fa\u4e8e\u89c2\u5bdf\u7ed3\u679c\u8bc4\u4f30\u6cbb\u7597\u65b9\u6848\uff0c\u5ffd\u7565\u4e86\u53cd\u4e8b\u5b9e\u7ed3\u679c\u7684\u6f5c\u5728\u5f71\u54cd\u3002\u8fd9\u9650\u5236\u4e86\u51b3\u7b56\u8d28\u91cf\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u7b49\u9700\u8981\u6743\u8861\u4e0d\u540c\u6cbb\u7597\u65b9\u6848\u7684\u9886\u57df\u3002\u8bba\u6587\u65e8\u5728\u6269\u5c55\u8fd9\u4e00\u7406\u8bba\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u51b3\u7b56\u8d28\u91cf\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5f15\u5165\u53cd\u4e8b\u5b9e\u635f\u5931\u51fd\u6570\uff0c\u6269\u5c55\u4e86\u6807\u51c6\u51b3\u7b56\u7406\u8bba\u3002\u5728\u5f3a\u53ef\u5ffd\u7565\u6027\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u4e86\u53cd\u4e8b\u5b9e\u98ce\u9669\u7684\u53ef\u8bc6\u522b\u6027\u6761\u4ef6\uff0c\u5373\u635f\u5931\u51fd\u6570\u5fc5\u987b\u662f\u6f5c\u5728\u7ed3\u679c\u7684\u52a0\u6027\u51fd\u6570\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u53cd\u4e8b\u5b9e\u635f\u5931\u53ef\u4ee5\u8bc6\u522b\u53cd\u4e8b\u5b9e\u98ce\u9669\uff0c\u4ec5\u5f53\u635f\u5931\u51fd\u6570\u662f\u52a0\u6027\u7684\u3002\u6b64\u5916\uff0c\u5728\u591a\u6cbb\u7597\u9009\u9879\u4e0b\uff0c\u53cd\u4e8b\u5b9e\u635f\u5931\u53ef\u80fd\u5bfc\u81f4\u4e0e\u4f20\u7edf\u635f\u5931\u51fd\u6570\u4e0d\u540c\u7684\u6cbb\u7597\u63a8\u8350\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u53cd\u4e8b\u5b9e\u635f\u5931\uff0c\u8bba\u6587\u4e3a\u51b3\u7b56\u7406\u8bba\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5c24\u5176\u5728\u591a\u6cbb\u7597\u9009\u9879\u573a\u666f\u4e0b\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u4f18\u5316\u7684\u63a8\u8350\u3002\u5f3a\u53ef\u5ffd\u7565\u6027\u662f\u5b9e\u73b0\u53cd\u4e8b\u5b9e\u98ce\u9669\u53ef\u8bc6\u522b\u7684\u5173\u952e\u5047\u8bbe\u3002"}}
{"id": "2505.09166", "pdf": "https://arxiv.org/pdf/2505.09166", "abs": "https://arxiv.org/abs/2505.09166", "authors": ["Hannu Simonen", "Atte Kiviniemi", "Jonas Oppenlaender"], "title": "An Initial Exploration of Default Images in Text-to-Image Generation", "categories": ["cs.HC", "cs.AI", "H.5.m; I.2.m"], "comment": "16 pages, 6 figures", "summary": "In the creative practice of text-to-image generation (TTI), images are\ngenerated from text prompts. However, TTI models are trained to always yield an\noutput, even if the prompt contains unknown terms. In this case, the model may\ngenerate what we call \"default images\": images that closely resemble each other\nacross many unrelated prompts. We argue studying default images is valuable for\ndesigning better solutions for TTI and prompt engineering. In this paper, we\nprovide the first investigation into default images on Midjourney, a popular\nimage generator. We describe our systematic approach to create input prompts\ntriggering default images, and present the results of our initial experiments\nand several small-scale ablation studies. We also report on a survey study\ninvestigating how default images affect user satisfaction. Our work lays the\nfoundation for understanding default images in TTI and highlights challenges\nand future research directions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff08TTI\uff09\u4e2d\u7684\u9ed8\u8ba4\u56fe\u50cf\u73b0\u8c61\uff0c\u5206\u6790\u4e86\u5176\u6210\u56e0\u3001\u5f71\u54cd\u53ca\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5728TTI\u6a21\u578b\u4e2d\uff0c\u672a\u77e5\u672f\u8bed\u4f1a\u89e6\u53d1\u9ed8\u8ba4\u56fe\u50cf\u751f\u6210\uff0c\u7814\u7a76\u8fd9\u4e00\u73b0\u8c61\u6709\u52a9\u4e8e\u6539\u8fdb\u6a21\u578b\u548c\u63d0\u793a\u5de5\u7a0b\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u65b9\u6cd5\u6784\u5efa\u89e6\u53d1\u9ed8\u8ba4\u56fe\u50cf\u7684\u8f93\u5165\u63d0\u793a\uff0c\u5e76\u8fdb\u884c\u5b9e\u9a8c\u3001\u6d88\u878d\u7814\u7a76\u548c\u7528\u6237\u8c03\u67e5\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u548c\u7814\u7a76\u8868\u660e\u9ed8\u8ba4\u56fe\u50cf\u5bf9\u7528\u6237\u6ee1\u610f\u5ea6\u6709\u5f71\u54cd\uff0c\u5e76\u63ed\u793a\u4e86\u76f8\u5173\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3TTI\u4e2d\u7684\u9ed8\u8ba4\u56fe\u50cf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u6307\u660e\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.08909", "pdf": "https://arxiv.org/pdf/2505.08909", "abs": "https://arxiv.org/abs/2505.08909", "authors": ["Deliang Wei", "Peng Chen", "Haobo Xu", "Jiale Yao", "Fang Li", "Tieyong Zeng"], "title": "Learning Cocoercive Conservative Denoisers via Helmholtz Decomposition for Poisson Inverse Problems", "categories": ["cs.CV", "cs.LG", "math.FA", "math.OC", "94A08, 47H10, 47J26, 46N10, 47N10"], "comment": "31 pages", "summary": "Plug-and-play (PnP) methods with deep denoisers have shown impressive results\nin imaging problems. They typically require strong convexity or smoothness of\nthe fidelity term and a (residual) non-expansive denoiser for convergence.\nThese assumptions, however, are violated in Poisson inverse problems, and\nnon-expansiveness can hinder denoising performance. To address these\nchallenges, we propose a cocoercive conservative (CoCo) denoiser, which may be\n(residual) expansive, leading to improved denoising. By leveraging the\ngeneralized Helmholtz decomposition, we introduce a novel training strategy\nthat combines Hamiltonian regularization to promote conservativeness and\nspectral regularization to ensure cocoerciveness. We prove that CoCo denoiser\nis a proximal operator of a weakly convex function, enabling a restoration\nmodel with an implicit weakly convex prior. The global convergence of PnP\nmethods to a stationary point of this restoration model is established.\nExtensive experimental results demonstrate that our approach outperforms\nclosely related methods in both visual quality and quantitative metrics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoCo\u7684\u4fdd\u5b88\u53ef\u534f\u53d8\u53bb\u566a\u5668\uff0c\u6539\u8fdb\u4e86\u4f20\u7edfPnP\u65b9\u6cd5\u5728\u6cca\u677e\u9006\u95ee\u9898\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u65b0\u578b\u8bad\u7ec3\u7b56\u7565\u786e\u4fdd\u6536\u655b\u6027\u548c\u53bb\u566a\u6027\u80fd\u7684\u63d0\u5347\u3002", "motivation": "\u4f20\u7edfPnP\u65b9\u6cd5\u8981\u6c42\u5f3a\u51f8\u6027\u6216\u5e73\u6ed1\u6027\u53ca\u975e\u6269\u5c55\u6027\u53bb\u566a\u5668\uff0c\u4f46\u8fd9\u4e9b\u5047\u8bbe\u5728\u6cca\u677e\u9006\u95ee\u9898\u4e2d\u4e0d\u6210\u7acb\u4e14\u9650\u5236\u4e86\u53bb\u566a\u6548\u679c\u3002", "method": "\u63d0\u51faCoCo\u53bb\u566a\u5668\uff0c\u5229\u7528\u5e7f\u4e49\u4ea5\u59c6\u970d\u5179\u5206\u89e3\u7ed3\u5408\u54c8\u5bc6\u987f\u6b63\u5219\u5316\u548c\u8c31\u6b63\u5219\u5316\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u786e\u4fdd\u5176\u4fdd\u5b88\u6027\u548c\u53ef\u534f\u53d8\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u5b9a\u91cf\u6307\u6807\u4e0a\u4f18\u4e8e\u76f8\u5173\u65b9\u6cd5\uff0c\u4e14\u7406\u8bba\u8bc1\u660e\u4e86\u5176\u5168\u5c40\u6536\u655b\u6027\u3002", "conclusion": "CoCo\u53bb\u566a\u5668\u901a\u8fc7\u5f31\u51f8\u9690\u5f0f\u5148\u9a8c\uff0c\u4e3aPnP\u65b9\u6cd5\u5728\u590d\u6742\u9006\u95ee\u9898\u4e2d\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09168", "pdf": "https://arxiv.org/pdf/2505.09168", "abs": "https://arxiv.org/abs/2505.09168", "authors": ["Jianlin Sun", "Xiaolin Fang", "Juwei Guan", "Dongdong Gui", "Teqi Wang", "Tongxin Zhu"], "title": "DRRNet: Macro-Micro Feature Fusion and Dual Reverse Refinement for Camouflaged Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The core challenge in Camouflage Object Detection (COD) lies in the\nindistinguishable similarity between targets and backgrounds in terms of color,\ntexture, and shape. This causes existing methods to either lose edge details\n(such as hair-like fine structures) due to over-reliance on global semantic\ninformation or be disturbed by similar backgrounds (such as vegetation\npatterns) when relying solely on local features. We propose DRRNet, a\nfour-stage architecture characterized by a \"context-detail-fusion-refinement\"\npipeline to address these issues. Specifically, we introduce an Omni-Context\nFeature Extraction Module to capture global camouflage patterns and a Local\nDetail Extraction Module to supplement microstructural information for the\nfull-scene context module. We then design a module for forming dual\nrepresentations of scene understanding and structural awareness, which fuses\npanoramic features and local features across various scales. In the decoder, we\nalso introduce a reverse refinement module that leverages spatial edge priors\nand frequency-domain noise suppression to perform a two-stage inverse\nrefinement of the output. By applying two successive rounds of inverse\nrefinement, the model effectively suppresses background interference and\nenhances the continuity of object boundaries. Experimental results demonstrate\nthat DRRNet significantly outperforms state-of-the-art methods on benchmark\ndatasets. Our code is available at https://github.com/jerrySunning/DRRNet.", "AI": {"tldr": "DRRNet\u901a\u8fc7\u56db\u9636\u6bb5\u67b6\u6784\u89e3\u51b3\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u4e2d\u76ee\u6807\u4e0e\u80cc\u666f\u76f8\u4f3c\u6027\u95ee\u9898\uff0c\u7ed3\u5408\u5168\u5c40\u4e0e\u5c40\u90e8\u7279\u5f81\uff0c\u5f15\u5165\u53cd\u5411\u7ec6\u5316\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u4e2d\u56e0\u76ee\u6807\u4e0e\u80cc\u666f\u5728\u989c\u8272\u3001\u7eb9\u7406\u3001\u5f62\u72b6\u4e0a\u7684\u9ad8\u5ea6\u76f8\u4f3c\u6027\u5bfc\u81f4\u7684\u8fb9\u7f18\u7ec6\u8282\u4e22\u5931\u6216\u80cc\u666f\u5e72\u6270\u95ee\u9898\u3002", "method": "\u63d0\u51faDRRNet\u56db\u9636\u6bb5\u67b6\u6784\uff0c\u5305\u62ec\u5168\u4e0a\u4e0b\u6587\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u3001\u5c40\u90e8\u7ec6\u8282\u63d0\u53d6\u6a21\u5757\u3001\u53cc\u8868\u5f81\u878d\u5408\u6a21\u5757\u53ca\u53cd\u5411\u7ec6\u5316\u6a21\u5757\uff0c\u7ed3\u5408\u5168\u5c40\u8bed\u4e49\u4e0e\u5c40\u90e8\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDRRNet\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DRRNet\u901a\u8fc7\u878d\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u4e0e\u9006\u5411\u7ec6\u5316\uff0c\u6709\u6548\u6291\u5236\u80cc\u666f\u5e72\u6270\u5e76\u589e\u5f3a\u76ee\u6807\u8fb9\u754c\u8fde\u7eed\u6027\uff0c\u4e3a\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.08961", "pdf": "https://arxiv.org/pdf/2505.08961", "abs": "https://arxiv.org/abs/2505.08961", "authors": ["Yancheng Wang", "Nebojsa Jojic", "Yingzhen Yang"], "title": "Differentiable Channel Selection in Self-Attention For Person Re-Identification", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In this paper, we propose a novel attention module termed the Differentiable\nChannel Selection Attention module, or the DCS-Attention module. In contrast\nwith conventional self-attention, the DCS-Attention module features selection\nof informative channels in the computation of the attention weights. The\nselection of the feature channels is performed in a differentiable manner,\nenabling seamless integration with DNN training. Our DCS-Attention is\ncompatible with either fixed neural network backbones or learnable backbones\nwith Differentiable Neural Architecture Search (DNAS), leading to DCS with\nFixed Backbone (DCS-FB) and DCS-DNAS, respectively. Importantly, our\nDCS-Attention is motivated by the principle of Information Bottleneck (IB), and\na novel variational upper bound for the IB loss, which can be optimized by SGD,\nis derived and incorporated into the training loss of the networks with the\nDCS-Attention modules. In this manner, a neural network with DCS-Attention\nmodules is capable of selecting the most informative channels for feature\nextraction so that it enjoys state-of-the-art performance for the Re-ID task.\nExtensive experiments on multiple person Re-ID benchmarks using both DCS-FB and\nDCS-DNAS show that DCS-Attention significantly enhances the prediction accuracy\nof DNNs for person Re-ID, which demonstrates the effectiveness of DCS-Attention\nin learning discriminative features critical to identifying person identities.\nThe code of our work is available at\nhttps://github.com/Statistical-Deep-Learning/DCS-Attention.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u5fae\u5206\u901a\u9053\u9009\u62e9\u6ce8\u610f\u529b\u6a21\u5757\uff08DCS-Attention\uff09\uff0c\u901a\u8fc7\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u7684\u901a\u9053\u6765\u589e\u5f3a\u6ce8\u610f\u529b\u6743\u91cd\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u53d7\u4fe1\u606f\u74f6\u9888\uff08IB\uff09\u539f\u7406\u542f\u53d1\uff0c\u5e0c\u671b\u901a\u8fc7\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u901a\u9053\u6765\u4f18\u5316\u7279\u5f81\u63d0\u53d6\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u8bbe\u8ba1\u4e86DCS-Attention\u6a21\u5757\uff0c\u652f\u6301\u56fa\u5b9a\u4e3b\u5e72\u7f51\u7edc\u548c\u53ef\u5b66\u4e60\u4e3b\u5e72\u7f51\u7edc\uff08DNAS\uff09\uff0c\u5e76\u63a8\u5bfc\u4e86IB\u635f\u5931\u7684\u4e0a\u754c\u4ee5\u4fbf\u901a\u8fc7SGD\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u884c\u4eba\u91cd\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528DCS-Attention\u7684\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "DCS-Attention\u6a21\u5757\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5173\u952e\u7279\u5f81\u901a\u9053\uff0c\u4e3a\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u63d0\u4f9b\u4e86\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09203", "pdf": "https://arxiv.org/pdf/2505.09203", "abs": "https://arxiv.org/abs/2505.09203", "authors": ["Xiao-Qi Han", "Peng-Jie Guo", "Ze-Feng Gao", "Hao Sun", "Zhong-Yi Lu"], "title": "InvDesFlow-AL: Active Learning-based Workflow for Inverse Design of Functional Materials", "categories": ["cond-mat.mtrl-sci", "cond-mat.supr-con", "cs.AI", "cs.LG"], "comment": "29 pages, 11 figures", "summary": "Developing inverse design methods for functional materials with specific\nproperties is critical to advancing fields like renewable energy, catalysis,\nenergy storage, and carbon capture. Generative models based on diffusion\nprinciples can directly produce new materials that meet performance\nconstraints, thereby significantly accelerating the material design process.\nHowever, existing methods for generating and predicting crystal structures\noften remain limited by low success rates. In this work, we propose a novel\ninverse material design generative framework called InvDesFlow-AL, which is\nbased on active learning strategies. This framework can iteratively optimize\nthe material generation process to gradually guide it towards desired\nperformance characteristics. In terms of crystal structure prediction, the\nInvDesFlow-AL model achieves an RMSE of 0.0423 {\\AA}, representing an 32.96%\nimprovement in performance compared to exsisting generative models.\nAdditionally, InvDesFlow-AL has been successfully validated in the design of\nlow-formation-energy and low-Ehull materials. It can systematically generate\nmaterials with progressively lower formation energies while continuously\nexpanding the exploration across diverse chemical spaces. These results fully\ndemonstrate the effectiveness of the proposed active learning-driven generative\nmodel in accelerating material discovery and inverse design. To further prove\nthe effectiveness of this method, we took the search for BCS superconductors\nunder ambient pressure as an example explored by InvDesFlow-AL. As a result, we\nsuccessfully identified Li\\(_2\\)AuH\\(_6\\) as a conventional BCS superconductor\nwith an ultra-high transition temperature of 140 K. This discovery provides\nstrong empirical support for the application of inverse design in materials\nscience.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u7684\u65b0\u578b\u9006\u5411\u6750\u6599\u8bbe\u8ba1\u751f\u6210\u6846\u67b6InvDesFlow-AL\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u4f4e\u5f62\u6210\u80fd\u6750\u6599\u548cBCS\u8d85\u5bfc\u4f53\u7684\u53d1\u73b0\u3002", "motivation": "\u5f00\u53d1\u5177\u6709\u7279\u5b9a\u529f\u80fd\u6750\u6599\u7684\u9006\u5411\u8bbe\u8ba1\u65b9\u6cd5\u5bf9\u53ef\u518d\u751f\u80fd\u6e90\u3001\u50ac\u5316\u3001\u50a8\u80fd\u548c\u78b3\u6355\u83b7\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u751f\u6210\u548c\u9884\u6d4b\u6676\u4f53\u7ed3\u6784\u65b9\u6cd5\u6210\u529f\u7387\u8f83\u4f4e\uff0c\u9650\u5236\u4e86\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86InvDesFlow-AL\u6846\u67b6\uff0c\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u8fed\u4ee3\u4f18\u5316\u6750\u6599\u751f\u6210\u8fc7\u7a0b\uff0c\u9010\u6b65\u5b9e\u73b0\u76ee\u6807\u6027\u80fd\u7279\u5f81\u3002", "result": "\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\u7684RMSE\u8fbe\u52300.0423 \u00c5\uff0c\u6bd4\u73b0\u6709\u751f\u6210\u6a21\u578b\u6027\u80fd\u63d0\u534732.96%\uff1b\u6210\u529f\u8bbe\u8ba1\u51fa\u4f4e\u5f62\u6210\u80fd\u6750\u6599\uff0c\u5e76\u53d1\u73b0Li\u2082AuH\u2086\u4f5c\u4e3a140 K\u9ad8\u6e29BCS\u8d85\u5bfc\u4f53\u3002", "conclusion": "InvDesFlow-AL\u6846\u67b6\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u663e\u8457\u52a0\u901f\u4e86\u6750\u6599\u53d1\u73b0\u548c\u9006\u5411\u8bbe\u8ba1\u8fc7\u7a0b\uff0c\u4e3a\u6750\u6599\u79d1\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2505.09208", "pdf": "https://arxiv.org/pdf/2505.09208", "abs": "https://arxiv.org/abs/2505.09208", "authors": ["Lei Fan", "Kunyang Deng", "Fangxue Liu"], "title": "Educational impacts of generative artificial intelligence on learning and performance of engineering students in China", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "With the rapid advancement of generative artificial intelligence(AI), its\npotential applications in higher education have attracted significant\nattention. This study investigated how 148 students from diverse engineering\ndisciplines and regions across China used generative AI, focusing on its impact\non their learning experience and the opportunities and challenges it poses in\nengineering education. Based on the surveyed data, we explored four key areas:\nthe frequency and application scenarios of AI use among engineering students,\nits impact on students' learning and performance, commonly encountered\nchallenges in using generative AI, and future prospects for its adoption in\nengineering education. The results showed that more than half of the\nparticipants reported a positive impact of generative AI on their learning\nefficiency, initiative, and creativity, with nearly half believing it also\nenhanced their independent thinking. However, despite acknowledging improved\nstudy efficiency, many felt their actual academic performance remained largely\nunchanged and expressed concerns about the accuracy and domain-specific\nreliability of generative AI. Our findings provide a first-hand insight into\nthe current benefits and challenges generative AI brings to students,\nparticularly Chinese engineering students, while offering several\nrecommendations, especially from the students' perspective, for effectively\nintegrating generative AI into engineering education.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86148\u540d\u4e2d\u56fd\u5de5\u7a0b\u5b66\u751f\u4f7f\u7528\u751f\u6210\u5f0fAI\u7684\u60c5\u51b5\uff0c\u63a2\u8ba8\u4e86\u5176\u5b66\u4e60\u4f53\u9a8c\u7684\u5f71\u54cd\u3001\u5e94\u7528\u573a\u666f\u53ca\u6311\u6218\uff0c\u7ed3\u679c\u663e\u793aAI\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u548c\u521b\u9020\u529b\uff0c\u4f46\u4e5f\u5b58\u5728\u51c6\u786e\u6027\u548c\u9886\u57df\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u751f\u6210\u5f0fAI\u5728\u5de5\u7a0b\u6559\u80b2\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u53ca\u5176\u5bf9\u5b66\u751f\u5b66\u4e60\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u8c03\u67e5148\u540d\u6765\u81ea\u4e0d\u540c\u5de5\u7a0b\u4e13\u4e1a\u548c\u5730\u533a\u7684\u5b66\u751f\uff0c\u5206\u6790AI\u7684\u4f7f\u7528\u9891\u7387\u3001\u573a\u666f\u3001\u5b66\u4e60\u5f71\u54cd\u53ca\u6311\u6218\u3002", "result": "\u8d85\u8fc7\u534a\u6570\u5b66\u751f\u8ba4\u4e3a\u751f\u6210\u5f0fAI\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u548c\u521b\u9020\u529b\uff0c\u4f46\u5b66\u672f\u8868\u73b0\u65e0\u660e\u663e\u53d8\u5316\uff0c\u4e14\u5bf9AI\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u5b58\u7591\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5bf9\u5de5\u7a0b\u6559\u80b2\u6709\u79ef\u6781\u5f71\u54cd\uff0c\u4f46\u9700\u89e3\u51b3\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5e76\u4f18\u5316\u6574\u5408\u7b56\u7565\u3002"}}
{"id": "2505.08986", "pdf": "https://arxiv.org/pdf/2505.08986", "abs": "https://arxiv.org/abs/2505.08986", "authors": ["Amirreza Davar", "Zhengtong Xu", "Siavash Mahmoudi", "Pouya Sohrabipour", "Chaitanya Pallerla", "Yu She", "Wan Shou", "Philip Crandall", "Dongyi Wang"], "title": "ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control for Delicate, Irregular Bio-products Manipulation", "categories": ["cs.RO", "cs.LG"], "comment": "Submitted for journal review", "summary": "Automated poultry processing lines still rely on humans to lift slippery,\neasily bruised carcasses onto a shackle conveyor. Deformability, anatomical\nvariance, and strict hygiene rules make conventional suction and scripted\nmotions unreliable. We present ChicGrasp, an end--to--end hardware--software\nco-design for this task. An independently actuated dual-jaw pneumatic gripper\nclamps both chicken legs, while a conditional diffusion-policy controller,\ntrained from only 50 multi--view teleoperation demonstrations (RGB +\nproprioception), plans 5 DoF end--effector motion, which includes jaw commands\nin one shot. On individually presented raw broiler carcasses, our system\nachieves a 40.6\\% grasp--and--lift success rate and completes the pick to\nshackle cycle in 38 s, whereas state--of--the--art implicit behaviour cloning\n(IBC) and LSTM-GMM baselines fail entirely. All CAD, code, and datasets will be\nopen-source. ChicGrasp shows that imitation learning can bridge the gap between\nrigid hardware and variable bio--products, offering a reproducible benchmark\nand a public dataset for researchers in agricultural engineering and robot\nlearning.", "AI": {"tldr": "ChicGrasp\u662f\u4e00\u4e2a\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u6293\u53d6\u548c\u60ac\u6302\u5bb6\u79bd\u5c38\u4f53\u3002\u901a\u8fc7\u6761\u4ef6\u6269\u6563\u7b56\u7565\u63a7\u5236\u5668\u548c50\u6b21\u591a\u89c6\u89d2\u9065\u64cd\u4f5c\u6f14\u793a\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e8640.6%\u7684\u6210\u529f\u7387\uff0c\u5e76\u572838\u79d2\u5185\u5b8c\u6210\u4e00\u4e2a\u5468\u671f\u3002\u8be5\u7cfb\u7edf\u4e3a\u519c\u4e1a\u5de5\u7a0b\u548c\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u548c\u516c\u5f00\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u524d\u5bb6\u79bd\u5904\u7406\u7ebf\u4f9d\u8d56\u4eba\u5de5\u64cd\u4f5c\uff0c\u5b58\u5728\u6548\u7387\u4f4e\u3001\u6613\u635f\u4f24\u7b49\u95ee\u9898\u3002\u7531\u4e8e\u5bb6\u79bd\u5c38\u4f53\u7684\u6613\u53d8\u5f62\u6027\u548c\u89e3\u5256\u5b66\u5dee\u5f02\uff0c\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u5438\u76d8\u6216\u56fa\u5b9a\u8f68\u8ff9\uff09\u96be\u4ee5\u9002\u7528\u3002", "method": "ChicGrasp\u91c7\u7528\u53cc\u722a\u6c14\u52a8\u5939\u6301\u5668\u5939\u4f4f\u9e21\u817f\uff0c\u5e76\u7ed3\u5408\u6761\u4ef6\u6269\u6563\u7b56\u7565\u63a7\u5236\u5668\u8fdb\u884c5\u81ea\u7531\u5ea6\u672b\u7aef\u6267\u884c\u5668\u8fd0\u52a8\u89c4\u5212\u3002\u63a7\u5236\u5668\u901a\u8fc750\u6b21RGB+\u672c\u4f53\u611f\u77e5\u7684\u9065\u64cd\u4f5c\u6f14\u793a\u8bad\u7ec3\u3002", "result": "\u7cfb\u7edf\u5bf9\u5355\u72ec\u5448\u73b0\u7684\u751f\u8089\u9e21\u5c38\u4f53\u5b9e\u73b0\u4e8640.6%\u7684\u6293\u53d6-\u60ac\u6302\u6210\u529f\u7387\uff0c\u6bcf\u4e2a\u5468\u671f\u8017\u65f638\u79d2\uff0c\u4f18\u4e8e\u73b0\u6709\u9690\u5f0f\u884c\u4e3a\u514b\u9686\uff08IBC\uff09\u548cLSTM-GMM\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ChicGrasp\u8bc1\u660e\u4e86\u6a21\u4eff\u5b66\u4e60\u80fd\u591f\u5f25\u8865\u521a\u6027\u786c\u4ef6\u4e0e\u751f\u7269\u4ea7\u54c1\u53d8\u5f02\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u519c\u4e1a\u5de5\u7a0b\u548c\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u548c\u516c\u5f00\u6570\u636e\u96c6\u3002"}}
{"id": "2505.09262", "pdf": "https://arxiv.org/pdf/2505.09262", "abs": "https://arxiv.org/abs/2505.09262", "authors": ["Hongxin Xiang", "Ke Li", "Mingquan Liu", "Zhixiang Cheng", "Bin Yao", "Wenjie Du", "Jun Xia", "Li Zeng", "Xin Jin", "Xiangxiang Zeng"], "title": "EDBench: Large-Scale Electron Density Data for Molecular Modeling", "categories": ["physics.chem-ph", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Existing molecular machine learning force fields (MLFFs) generally focus on\nthe learning of atoms, molecules, and simple quantum chemical properties (such\nas energy and force), but ignore the importance of electron density (ED)\n$\\rho(r)$ in accurately understanding molecular force fields (MFFs). ED\ndescribes the probability of finding electrons at specific locations around\natoms or molecules, which uniquely determines all ground state properties (such\nas energy, molecular structure, etc.) of interactive multi-particle systems\naccording to the Hohenberg-Kohn theorem. However, the calculation of ED relies\non the time-consuming first-principles density functional theory (DFT) which\nleads to the lack of large-scale ED data and limits its application in MLFFs.\nIn this paper, we introduce EDBench, a large-scale, high-quality dataset of ED\ndesigned to advance learning-based research at the electronic scale. Built upon\nthe PCQM4Mv2, EDBench provides accurate ED data, covering 3.3 million\nmolecules. To comprehensively evaluate the ability of models to understand and\nutilize electronic information, we design a suite of ED-centric benchmark tasks\nspanning prediction, retrieval, and generation. Our evaluation on several\nstate-of-the-art methods demonstrates that learning from EDBench is not only\nfeasible but also achieves high accuracy. Moreover, we show that learning-based\nmethod can efficiently calculate ED with comparable precision while\nsignificantly reducing the computational cost relative to traditional DFT\ncalculations. All data and benchmarks from EDBench will be freely available,\nlaying a robust foundation for ED-driven drug discovery and materials science.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86EDBench\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u7535\u5b50\u5bc6\u5ea6\u6570\u636e\u96c6\uff0c\u65e8\u5728\u4fc3\u8fdb\u7535\u5b50\u5c3a\u5ea6\u7684\u5b66\u4e60\u7814\u7a76\u3002\u901a\u8fc7\u8be5\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u8ba1\u7b97\u7535\u5b50\u5bc6\u5ea6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4f20\u7edfDFT\u7684\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u5206\u5b50\u673a\u5668\u5b66\u4e60\u529b\u573a\uff08MLFFs\uff09\u901a\u5e38\u5ffd\u7565\u7535\u5b50\u5bc6\u5ea6\uff08ED\uff09\u7684\u91cd\u8981\u6027\uff0c\u800cED\u5bf9\u51c6\u786e\u7406\u89e3\u5206\u5b50\u529b\u573a\u81f3\u5173\u91cd\u8981\u3002\u7531\u4e8eED\u8ba1\u7b97\u4f9d\u8d56\u4e8e\u8017\u65f6\u7684\u4e00\u6027\u539f\u7406\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\uff0c\u7f3a\u4e4f\u5927\u89c4\u6a21ED\u6570\u636e\u9650\u5236\u4e86\u5176\u5728MLFFs\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u57fa\u4e8ePCQM4Mv2\u6784\u5efa\u4e86EDBench\u6570\u636e\u96c6\uff0c\u5305\u542b330\u4e07\u4e2a\u5206\u5b50\u7684\u7cbe\u786eED\u6570\u636e\uff0c\u8bbe\u8ba1\u4e86\u4e00\u7cfb\u5217\u4ee5ED\u4e3a\u4e2d\u5fc3\u7684\u57fa\u51c6\u4efb\u52a1\uff08\u9884\u6d4b\u3001\u68c0\u7d22\u548c\u751f\u6210\uff09\uff0c\u8bc4\u4f30\u6a21\u578b\u7406\u89e3\u548c\u5229\u7528\u7535\u5b50\u4fe1\u606f\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eEDBench\u7684\u5b66\u4e60\u4e0d\u4ec5\u53ef\u884c\uff0c\u8fd8\u80fd\u8fbe\u5230\u9ad8\u7cbe\u5ea6\u3002\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u80fd\u9ad8\u6548\u8ba1\u7b97ED\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u4f4e\u4e8e\u4f20\u7edfDFT\u3002", "conclusion": "EDBench\u4e3a\u7535\u5b50\u5bc6\u5ea6\u9a71\u52a8\u7684\u836f\u7269\u53d1\u73b0\u548c\u6750\u6599\u79d1\u5b66\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u6570\u636e\u57fa\u7840\uff0c\u5176\u6570\u636e\u548c\u57fa\u51c6\u6d4b\u8bd5\u5c06\u514d\u8d39\u5f00\u653e\u3002"}}
{"id": "2505.09004", "pdf": "https://arxiv.org/pdf/2505.09004", "abs": "https://arxiv.org/abs/2505.09004", "authors": ["Monica Welfert", "Nathan Stromberg", "Mario Diaz", "Lalitha Sankar"], "title": "Lower Bounds on the MMSE of Adversarially Inferring Sensitive Features", "categories": ["stat.ML", "cs.LG"], "comment": "submitted to IEEE Transactions on Information Theory", "summary": "We propose an adversarial evaluation framework for sensitive feature\ninference based on minimum mean-squared error (MMSE) estimation with a finite\nsample size and linear predictive models. Our approach establishes theoretical\nlower bounds on the true MMSE of inferring sensitive features from noisy\nobservations of other correlated features. These bounds are expressed in terms\nof the empirical MMSE under a restricted hypothesis class and a non-negative\nerror term. The error term captures both the estimation error due to finite\nnumber of samples and the approximation error from using a restricted\nhypothesis class. For linear predictive models, we derive closed-form bounds,\nwhich are order optimal in terms of the noise variance, on the approximation\nerror for several classes of relationships between the sensitive and\nnon-sensitive features, including linear mappings, binary symmetric channels,\nand class-conditional multi-variate Gaussian distributions. We also present a\nnew lower bound that relies on the MSE computed on a hold-out validation\ndataset of the MMSE estimator learned on finite-samples and a restricted\nhypothesis class. Through empirical evaluation, we demonstrate that our\nframework serves as an effective tool for MMSE-based adversarial evaluation of\nsensitive feature inference that balances theoretical guarantees with practical\nefficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\uff08MMSE\uff09\u4f30\u8ba1\u7684\u5bf9\u6297\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u654f\u611f\u7279\u5f81\u63a8\u65ad\uff0c\u5e76\u5efa\u7acb\u4e86\u7406\u8bba\u4e0b\u754c\u3002\u9488\u5bf9\u7ebf\u6027\u9884\u6d4b\u6a21\u578b\uff0c\u63a8\u5bfc\u4e86\u95ed\u5f0f\u8fb9\u754c\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u4e2a\u7406\u8bba\u4e25\u8c28\u4e14\u5b9e\u7528\u7684\u5bf9\u6297\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u4ece\u566a\u58f0\u89c2\u6d4b\u4e2d\u63a8\u65ad\u654f\u611f\u7279\u5f81\u7684\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\uff0c\u4ee5\u5e73\u8861\u7406\u8bba\u4fdd\u8bc1\u4e0e\u5b9e\u9645\u6548\u7387\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u57fa\u4e8e\u6709\u9650\u6837\u672c\u548c\u7ebf\u6027\u9884\u6d4b\u6a21\u578b\u7684MMSE\u4f30\u8ba1\uff0c\u5efa\u7acb\u7406\u8bba\u4e0b\u754c\uff0c\u5e76\u63a8\u5bfc\u9488\u5bf9\u7ebf\u6027\u6620\u5c04\u3001\u4e8c\u8fdb\u5236\u5bf9\u79f0\u4fe1\u9053\u7b49\u5173\u7cfb\u7684\u95ed\u5f0f\u8fb9\u754c\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u654f\u611f\u7279\u5f81\u63a8\u65ad\u7684MMSE\uff0c\u5e76\u5728\u591a\u79cd\u5173\u7cfb\u4e0b\u5b9e\u73b0\u9636\u6b21\u6700\u4f18\u7684\u8fd1\u4f3c\u8bef\u5dee\u8fb9\u754c\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u6846\u67b6\u4e3aMMSE-based\u5bf9\u6297\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0e\u5b9e\u8bc1\u652f\u6301\uff0c\u9002\u7528\u4e8e\u654f\u611f\u7279\u5f81\u63a8\u65ad\u7684\u9a8c\u8bc1\u3002"}}
{"id": "2505.09263", "pdf": "https://arxiv.org/pdf/2505.09263", "abs": "https://arxiv.org/abs/2505.09263", "authors": ["Guan Gui", "Bin-Bin Gao", "Jun Liu", "Chengjie Wang", "Yunsheng Wu"], "title": "Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ECCV 2024", "summary": "Anomaly detection is a practical and challenging task due to the scarcity of\nanomaly samples in industrial inspection. Some existing anomaly detection\nmethods address this issue by synthesizing anomalies with noise or external\ndata. However, there is always a large semantic gap between synthetic and\nreal-world anomalies, resulting in weak performance in anomaly detection. To\nsolve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)\nmethod, which guides the diffusion model to generate realistic and diverse\nanomalies with only a few real anomalies, thereby benefiting training anomaly\ndetection models. Specifically, our work is divided into three stages. In the\nfirst stage, we learn the anomaly distribution based on a few given real\nanomalies and inject the learned knowledge into an embedding. In the second\nstage, we use the embedding and given bounding boxes to guide the diffusion\nmodel to generate realistic and diverse anomalies on specific objects (or\ntextures). In the final stage, we propose a weakly-supervised anomaly detection\nmethod to train a more powerful model with generated anomalies. Our method\nbuilds upon DRAEM and DesTSeg as the foundation model and conducts experiments\non the commonly used industrial anomaly detection dataset, MVTec. The\nexperiments demonstrate that our generated anomalies effectively improve the\nmodel performance of both anomaly classification and segmentation tasks\nsimultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement\nin AU-PR metric on segmentation task, respectively. The code and generated\nanomalous data are available at https://github.com/gaobb/AnoGen.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAnoGen\u7684\u5c11\u6837\u672c\u5f02\u5e38\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u903c\u771f\u4e14\u591a\u6837\u7684\u5f02\u5e38\u6570\u636e\uff0c\u4ee5\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728MVTec\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a\u68c0\u6d4b\u4e2d\u5f02\u5e38\u6837\u672c\u7a00\u7f3a\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5408\u6210\u5f02\u5e38\u6570\u636e\u5b58\u5728\u8bed\u4e49\u5dee\u8ddd\u95ee\u9898\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6027\u80fd\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5c11\u91cf\u771f\u5b9e\u5f02\u5e38\u751f\u6210\u903c\u771f\u5f02\u5e38\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u63d0\u51faAnoGen\u65b9\u6cd5\uff0c\u5206\u4e09\u9636\u6bb5\uff1a1) \u5b66\u4e60\u5f02\u5e38\u5206\u5e03\u5e76\u5d4c\u5165\u77e5\u8bc6\uff1b2) \u5229\u7528\u5d4c\u5165\u548c\u8fb9\u754c\u6846\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u751f\u6210\u5f02\u5e38\uff1b3) \u91c7\u7528\u5f31\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u8bad\u7ec3\u6a21\u578b\u3002\u57fa\u4e8eDRAEM\u548cDesTSeg\u6a21\u578b\uff0c\u5728MVTec\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u3002", "result": "\u751f\u6210\u7684\u5f02\u5e38\u6570\u636e\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0cDRAEM\u548cDesTSeg\u5728\u5206\u5272\u4efb\u52a1\u7684AU-PR\u6307\u6807\u4e0a\u5206\u522b\u63d0\u53475.8%\u548c1.5%\u3002", "conclusion": "AnoGen\u65b9\u6cd5\u80fd\u6709\u6548\u5229\u7528\u5c11\u91cf\u771f\u5b9e\u5f02\u5e38\u751f\u6210\u9ad8\u8d28\u91cf\u5f02\u5e38\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u6027\u80fd\uff0c\u4e3a\u5de5\u4e1a\u68c0\u6d4b\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09018", "pdf": "https://arxiv.org/pdf/2505.09018", "abs": "https://arxiv.org/abs/2505.09018", "authors": ["Adarsh Kumar"], "title": "Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Effective dietary monitoring is critical for managing Type 2 diabetes, yet\naccurately estimating caloric intake remains a major challenge. While\ncontinuous glucose monitors (CGMs) offer valuable physiological data, they\noften fall short in capturing the full nutritional profile of meals due to\ninter-individual and meal-specific variability. In this work, we introduce a\nmultimodal deep learning framework that jointly leverages CGM time-series data,\nDemographic/Microbiome, and pre-meal food images to enhance caloric estimation.\nOur model utilizes attention based encoding and a convolutional feature\nextraction for meal imagery, multi-layer perceptrons for CGM and Microbiome\ndata followed by a late fusion strategy for joint reasoning. We evaluate our\napproach on a curated dataset of over 40 participants, incorporating\nsynchronized CGM, Demographic and Microbiome data and meal photographs with\nstandardized caloric labels. Our model achieves a Root Mean Squared Relative\nError (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These\nfindings demonstrate the potential of multimodal sensing to improve automated\ndietary assessment tools for chronic disease management.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CGM\u6570\u636e\u3001\u4eba\u53e3\u7edf\u8ba1/\u5fae\u751f\u7269\u7ec4\u4fe1\u606f\u548c\u9910\u524d\u98df\u7269\u56fe\u50cf\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u5bf92\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u7684\u70ed\u91cf\u6444\u5165\u4f30\u7b97\uff0c\u5176\u8bef\u5dee\u6bd4\u57fa\u7ebf\u6a21\u578b\u964d\u4f4e\u4e8650%\u4ee5\u4e0a\u3002", "motivation": "\u7cbe\u51c6\u4f30\u7b97\u70ed\u91cf\u6444\u5165\u5bf9\u7ba1\u74062\u578b\u7cd6\u5c3f\u75c5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6280\u672f\uff08\u5982\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u4eea\uff09\u56e0\u4e2a\u4f53\u548c\u9910\u98df\u5dee\u5f02\u96be\u4ee5\u5168\u9762\u6355\u6349\u8425\u517b\u4fe1\u606f\uff0c\u9700\u591a\u6a21\u6001\u6570\u636e\u63d0\u5347\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff1a\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7f16\u7801\u548c\u5377\u79ef\u7279\u5f81\u63d0\u53d6\u5904\u7406\u98df\u7269\u56fe\u50cf\uff0c\u591a\u5c42\u611f\u77e5\u5668\u5904\u7406CGM\u548c\u5fae\u751f\u7269\u7ec4\u6570\u636e\uff0c\u6700\u540e\u901a\u8fc7\u540e\u671f\u878d\u5408\u7b56\u7565\u8054\u5408\u63a8\u7406\u3002", "result": "\u572840\u591a\u540d\u53c2\u4e0e\u8005\u7684\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u6a21\u578b\u7684\u70ed\u91cf\u4f30\u7b97\u5747\u65b9\u6839\u76f8\u5bf9\u8bef\u5dee\uff08RMSRE\uff09\u4e3a0.2544\uff0c\u6bd4\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u63d0\u5347\u8d8550%\u3002", "conclusion": "\u591a\u6a21\u6001\u611f\u77e5\u6280\u672f\u80fd\u663e\u8457\u63d0\u5347\u81ea\u52a8\u5316\u996e\u98df\u8bc4\u4f30\u5de5\u5177\u7684\u6027\u80fd\uff0c\u5bf9\u6162\u6027\u75c5\u7ba1\u7406\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2505.09264", "pdf": "https://arxiv.org/pdf/2505.09264", "abs": "https://arxiv.org/abs/2505.09264", "authors": ["Bin-Bin Gao"], "title": "Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ECCV 2024", "summary": "Unsupervised reconstruction networks using self-attention transformers have\nachieved state-of-the-art performance for multi-class (unified) anomaly\ndetection with a single model. However, these self-attention reconstruction\nmodels primarily operate on target features, which may result in perfect\nreconstruction for both normal and anomaly features due to high consistency\nwith context, leading to failure in detecting anomalies. Additionally, these\nmodels often produce inaccurate anomaly segmentation due to performing\nreconstruction in a low spatial resolution latent space. To enable\nreconstruction models enjoying high efficiency while enhancing their\ngeneralization for unified anomaly detection, we propose a simple yet effective\nmethod that reconstructs normal features and restores anomaly features with\njust One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP\nallows for the first time to reconstruct or restore anomalies with just one\nnormal image prompt, effectively boosting unified anomaly detection\nperformance. Furthermore, we propose a supervised refiner that regresses\nreconstruction errors by using both real normal and synthesized anomalous\nimages, which significantly improves pixel-level anomaly segmentation. OneNIP\noutperforms previous methods on three industry anomaly detection benchmarks:\nMVTec, BTAD, and VisA. The code and pre-trained models are available at\nhttps://github.com/gaobb/OneNIP.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOneNIP\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u4f7f\u7528\u4e00\u5f20\u6b63\u5e38\u56fe\u50cf\u63d0\u793a\uff08OneNIP\uff09\u6765\u91cd\u6784\u6b63\u5e38\u7279\u5f81\u5e76\u4fee\u590d\u5f02\u5e38\u7279\u5f81\uff0c\u4ece\u800c\u5728\u7edf\u4e00\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u76d1\u7763\u7ec6\u5316\u5668\u6765\u4f18\u5316\u91cd\u6784\u8bef\u5dee\uff0c\u663e\u8457\u63d0\u5347\u4e86\u50cf\u7d20\u7ea7\u5f02\u5e38\u5206\u5272\u7684\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u53d8\u6362\u5668\u7684\u65e0\u76d1\u7763\u91cd\u6784\u7f51\u7edc\u867d\u7136\u5728\u591a\u7c7b\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u5bf9\u5f02\u5e38\u7279\u5f81\u91cd\u6784\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u4e14\u7531\u4e8e\u5728\u4f4e\u7a7a\u95f4\u5206\u8fa8\u7387\u9690\u7a7a\u95f4\u4e2d\u8fdb\u884c\u91cd\u6784\uff0c\u5bfc\u81f4\u5f02\u5e38\u5206\u5272\u4e0d\u7cbe\u786e\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86OneNIP\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u4f7f\u7528\u4e00\u5f20\u6b63\u5e38\u56fe\u50cf\u63d0\u793a\u6765\u91cd\u6784\u6b63\u5e38\u7279\u5f81\u5e76\u4fee\u590d\u5f02\u5e38\u7279\u5f81\uff0c\u4ece\u800c\u589e\u5f3a\u7edf\u4e00\u5f02\u5e38\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u76d1\u7763\u7ec6\u5316\u5668\uff0c\u5229\u7528\u771f\u5b9e\u6b63\u5e38\u56fe\u50cf\u548c\u5408\u6210\u7684\u5f02\u5e38\u56fe\u50cf\u6765\u56de\u5f52\u91cd\u6784\u8bef\u5dee\uff0c\u4ee5\u63d0\u5347\u50cf\u7d20\u7ea7\u5f02\u5e38\u5206\u5272\u7684\u51c6\u786e\u6027\u3002", "result": "OneNIP\u5728MVTec\u3001BTAD\u548cVisA\u4e09\u4e2a\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4e4b\u524d\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7edf\u4e00\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "OneNIP\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5f02\u5e38\u68c0\u6d4b\u7684\u6548\u7387\uff0c\u8fd8\u663e\u8457\u63d0\u5347\u4e86\u50cf\u7d20\u7ea7\u5f02\u5e38\u5206\u5272\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u7edf\u4e00\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09265", "pdf": "https://arxiv.org/pdf/2505.09265", "abs": "https://arxiv.org/abs/2505.09265", "authors": ["Bin-Bin Gao"], "title": "MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by NeurIPS 2024", "summary": "Zero- and few-shot visual anomaly segmentation relies on powerful\nvision-language models that detect unseen anomalies using manually designed\ntextual prompts. However, visual representations are inherently independent of\nlanguage. In this paper, we explore the potential of a pure visual foundation\nmodel as an alternative to widely used vision-language models for universal\nvisual anomaly segmentation. We present a novel paradigm that unifies anomaly\nsegmentation into change segmentation. This paradigm enables us to leverage\nlarge-scale synthetic image pairs, featuring object-level and local region\nchanges, derived from existing image datasets, which are independent of target\nanomaly datasets. We propose a one-prompt Meta-learning framework for Universal\nAnomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and\nthen generalizes well to segment any novel or unseen visual anomalies in the\nreal world. To handle geometrical variations between prompt and query images,\nwe propose a soft feature alignment module that bridges paired-image change\nperception and single-image semantic segmentation. This is the first work to\nachieve universal anomaly segmentation using a pure vision model without\nrelying on special anomaly detection datasets and pre-trained visual-language\nmodels. Our method effectively and efficiently segments any anomalies with only\none normal image prompt and enjoys training-free without guidance from\nlanguage. Our MetaUAS significantly outperforms previous zero-shot, few-shot,\nand even full-shot anomaly segmentation methods. The code and pre-trained\nmodels are available at https://github.com/gaobb/MetaUAS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7eaf\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u65b0\u578b\u901a\u7528\u89c6\u89c9\u5f02\u5e38\u5206\u5272\u65b9\u6cd5\uff08MetaUAS\uff09\uff0c\u901a\u8fc7\u7edf\u4e00\u5f02\u5e38\u5206\u5272\u4e0e\u53d8\u5316\u5206\u5272\u7684\u8303\u5f0f\uff0c\u5229\u7528\u5408\u6210\u56fe\u50cf\u5bf9\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u4f9d\u8d56\u8bed\u8a00\u63d0\u793a\u6216\u7279\u5b9a\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u5f02\u5e38\u5206\u5272\u6548\u679c\u3002", "motivation": "\u63a2\u7d22\u7eaf\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u66ff\u4ee3\u73b0\u6709\u4f9d\u8d56\u8bed\u8a00\u63d0\u793a\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u89c6\u89c9\u8868\u793a\u4e0e\u8bed\u8a00\u65e0\u5173\u7684\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u65e0\u9700\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u5f02\u5e38\u5206\u5272\u3002", "method": "\u63d0\u51faMetaUAS\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5f02\u5e38\u5206\u5272\u7edf\u4e00\u4e3a\u53d8\u5316\u5206\u5272\u8303\u5f0f\uff0c\u5229\u7528\u5408\u6210\u56fe\u50cf\u5bf9\u8fdb\u884c\u5143\u5b66\u4e60\u8bad\u7ec3\uff0c\u5e76\u5f15\u5165\u8f6f\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\u5904\u7406\u51e0\u4f55\u53d8\u5316\u3002", "result": "MetaUAS\u5728\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u751a\u81f3\u5168\u6837\u672c\u5f02\u5e38\u5206\u5272\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u4ec5\u9700\u4e00\u5f20\u6b63\u5e38\u56fe\u50cf\u63d0\u793a\u5373\u53ef\u9ad8\u6548\u5206\u5272\u4efb\u610f\u5f02\u5e38\u3002", "conclusion": "\u8bba\u6587\u9996\u6b21\u5b9e\u73b0\u4e86\u65e0\u9700\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u548c\u7279\u5b9a\u5f02\u5e38\u6570\u636e\u96c6\u7684\u901a\u7528\u5f02\u5e38\u5206\u5272\uff0c\u4e3a\u7eaf\u89c6\u89c9\u6a21\u578b\u5728\u5f02\u5e38\u68c0\u6d4b\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.09026", "pdf": "https://arxiv.org/pdf/2505.09026", "abs": "https://arxiv.org/abs/2505.09026", "authors": ["Domniki Ladopoulou", "Dat Minh Hong", "Petros Dellaportas"], "title": "Probabilistic Wind Power Forecasting via Non-Stationary Gaussian Processes", "categories": ["stat.AP", "cs.LG", "stat.ML"], "comment": "11 pages, 3 figures, 2 tables", "summary": "Accurate probabilistic forecasting of wind power is essential for maintaining\ngrid stability and enabling efficient integration of renewable energy sources.\nGaussian Process (GP) models offer a principled framework for quantifying\nuncertainty; however, conventional approaches rely on stationary kernels, which\nare inadequate for modeling the inherently non-stationary nature of wind speed\nand power output. We propose a non-stationary GP framework that incorporates\nthe generalized spectral mixture (GSM) kernel, enabling the model to capture\ntime-varying patterns and heteroscedastic behaviors in wind speed and wind\npower data. We evaluate the performance of the proposed model on real-world\nSCADA data across short\\mbox{-,} medium-, and long-term forecasting horizons.\nCompared to standard radial basis function and spectral mixture kernels, the\nGSM-based model outperforms, particularly in short-term forecasts. These\nresults highlight the necessity of modeling non-stationarity in wind power\nforecasting and demonstrate the practical value of non-stationary GP models in\noperational settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u5e73\u7a33\u9ad8\u65af\u8fc7\u7a0b\u7684\u5e7f\u4e49\u8c31\u6df7\u5408\u6838\u6a21\u578b\uff0c\u4ee5\u6539\u8fdb\u98ce\u7535\u529f\u7387\u6982\u7387\u9884\u6d4b\uff0c\u5c24\u5176\u5728\u77ed\u671f\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u4e3a\u63d0\u9ad8\u7535\u7f51\u7a33\u5b9a\u6027\u5e76\u9ad8\u6548\u6574\u5408\u53ef\u518d\u751f\u80fd\u6e90\uff0c\u9700\u8981\u51c6\u786e\u7684\u98ce\u7535\u529f\u7387\u6982\u7387\u9884\u6d4b\u3002\u4f20\u7edf\u9ad8\u65af\u8fc7\u7a0b\u91c7\u7528\u5e73\u7a33\u6838\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u98ce\u7535\u6570\u636e\u7684\u975e\u5e73\u7a33\u7279\u6027", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u975e\u5e73\u7a33\u9ad8\u65af\u8fc7\u7a0b\u6846\u67b6\uff0c\u7ed3\u5408\u5e7f\u4e49\u8c31\u6df7\u5408\u6838\uff08GSM\uff09\uff0c\u7528\u4e8e\u5efa\u6a21\u98ce\u7535\u6570\u636e\u7684\u65f6\u53d8\u6a21\u5f0f\u548c\u5f02\u65b9\u5dee\u884c\u4e3a", "result": "\u5728\u771f\u5b9eSCADA\u6570\u636e\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0cGSM\u6838\u6a21\u578b\u5728\u77ed\u3001\u4e2d\u3001\u957f\u671f\u9884\u6d4b\u4e2d\u5747\u4f18\u4e8e\u4f20\u7edf\u5f84\u5411\u57fa\u51fd\u6570\u548c\u8c31\u6df7\u5408\u6838\uff0c\u5c24\u5176\u5728\u77ed\u671f\u9884\u6d4b\u4e2d\u8868\u73b0\u7a81\u51fa", "conclusion": "\u975e\u5e73\u7a33\u6027\u5efa\u6a21\u5bf9\u98ce\u7535\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u975e\u5e73\u7a33\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u5728\u5b9e\u9645\u64cd\u4f5c\u4e2d\u5177\u6709\u663e\u8457\u4ef7\u503c"}}
{"id": "2505.09295", "pdf": "https://arxiv.org/pdf/2505.09295", "abs": "https://arxiv.org/abs/2505.09295", "authors": ["Qiming Wu", "Siqi Li", "Doudou Zhou", "Nan Liu"], "title": "Toward Fair Federated Learning under Demographic Disparities and Data Imbalance", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "Ensuring fairness is critical when applying artificial intelligence to\nhigh-stakes domains such as healthcare, where predictive models trained on\nimbalanced and demographically skewed data risk exacerbating existing\ndisparities. Federated learning (FL) enables privacy-preserving collaboration\nacross institutions, but remains vulnerable to both algorithmic bias and\nsubgroup imbalance - particularly when multiple sensitive attributes intersect.\nWe propose FedIDA (Fed erated Learning for Imbalance and D isparity A\nwareness), a framework-agnostic method that combines fairness-aware\nregularization with group-conditional oversampling. FedIDA supports multiple\nsensitive attributes and heterogeneous data distributions without altering the\nconvergence behavior of the underlying FL algorithm. We provide theoretical\nanalysis establishing fairness improvement bounds using Lipschitz continuity\nand concentration inequalities, and show that FedIDA reduces the variance of\nfairness metrics across test sets. Empirical results on both benchmark and\nreal-world clinical datasets confirm that FedIDA consistently improves fairness\nwhile maintaining competitive predictive performance, demonstrating its\neffectiveness for equitable and privacy-preserving modeling in healthcare. The\nsource code is available on GitHub.", "AI": {"tldr": "\u63d0\u51faFedIDA\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u516c\u5e73\u611f\u77e5\u6b63\u5219\u5316\u548c\u7ec4\u6761\u4ef6\u8fc7\u91c7\u6837\uff0c\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u7b97\u6cd5\u504f\u89c1\u548c\u5b50\u7ec4\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5c24\u5176\u5728\u591a\u654f\u611f\u5c5e\u6027\u4ea4\u53c9\u65f6\u6709\u6548\u3002", "motivation": "\u5728\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u5e94\u7528AI\u65f6\uff0c\u516c\u5e73\u6027\u81f3\u5173\u91cd\u8981\u3002\u8054\u90a6\u5b66\u4e60\u867d\u80fd\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u4ecd\u9762\u4e34\u7b97\u6cd5\u504f\u89c1\u548c\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5c24\u5176\u662f\u591a\u654f\u611f\u5c5e\u6027\u4ea4\u53c9\u65f6\u3002", "method": "FedIDA\u7ed3\u5408\u516c\u5e73\u611f\u77e5\u6b63\u5219\u5316\u548c\u7ec4\u6761\u4ef6\u8fc7\u91c7\u6837\uff0c\u652f\u6301\u591a\u654f\u611f\u5c5e\u6027\u548c\u5f02\u6784\u6570\u636e\u5206\u5e03\uff0c\u4e14\u4e0d\u6539\u53d8\u5e95\u5c42\u8054\u90a6\u5b66\u4e60\u7684\u6536\u655b\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eFedIDA\u80fd\u6539\u5584\u516c\u5e73\u6027\uff1b\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u57fa\u51c6\u548c\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u5747\u80fd\u63d0\u5347\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "FedIDA\u662f\u4e00\u79cd\u6709\u6548\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u533b\u7597\u7b49\u9700\u8981\u516c\u5e73\u9884\u6d4b\u7684\u9886\u57df\u3002"}}
{"id": "2505.09324", "pdf": "https://arxiv.org/pdf/2505.09324", "abs": "https://arxiv.org/abs/2505.09324", "authors": ["Lakshya Gupta", "Imran N. Junejo"], "title": "Neural Video Compression using 2D Gaussian Splatting", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": "9 pages, 8 figures", "summary": "The computer vision and image processing research community has been involved\nin standardizing video data communications for the past many decades, leading\nto standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent\ngroundbreaking works have focused on employing deep learning-based techniques\nto replace the traditional video codec pipeline to a greater affect. Neural\nvideo codecs (NVC) create an end-to-end ML-based solution that does not rely on\nany handcrafted features (motion or edge-based) and have the ability to learn\ncontent-aware compression strategies, offering better adaptability and higher\ncompression efficiency than traditional methods. This holds a great potential\nnot only for hardware design, but also for various video streaming platforms\nand applications, especially video conferencing applications such as MS-Teams\nor Zoom that have found extensive usage in classrooms and workplaces. However,\ntheir high computational demands currently limit their use in real-time\napplications like video conferencing. To address this, we propose a\nregion-of-interest (ROI) based neural video compression model that leverages 2D\nGaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable\nof real-time decoding and can be optimized using fewer data points, requiring\nonly thousands of Gaussians for decent quality outputs as opposed to millions\nin 3D scenes. In this work, we designed a video pipeline that speeds up the\nencoding time of the previous Gaussian splatting-based image codec by 88% by\nusing a content-aware initialization strategy paired with a novel Gaussian\ninter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be\nused for a video-codec solution, the first of its kind solution in this neural\nvideo codec space.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u611f\u5174\u8da3\u533a\u57df(ROI)\u548c2D\u9ad8\u65af\u6492\u70b9\u7684\u795e\u7ecf\u89c6\u9891\u538b\u7f29\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u7801\u901f\u5ea6\uff0c\u4e3a\u5b9e\u65f6\u89c6\u9891\u5e94\u7528\u5982\u89c6\u9891\u4f1a\u8bae\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u7f16\u89e3\u7801\u5668\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\uff0c\u800c\u795e\u7ecf\u89c6\u9891\u7f16\u89e3\u7801\u5668(NVC)\u867d\u80fd\u5b66\u4e60\u5185\u5bb9\u611f\u77e5\u7684\u538b\u7f29\u7b56\u7565\uff0c\u4f46\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc72D\u9ad8\u65af\u6492\u70b9\u6280\u672f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u75282D\u9ad8\u65af\u6492\u70b9\u6280\u672f\uff0c\u7ed3\u5408\u5185\u5bb9\u611f\u77e5\u521d\u59cb\u5316\u7b56\u7565\u548c\u5e27\u95f4\u5197\u4f59\u51cf\u5c11\u673a\u5236\uff0c\u8bbe\u8ba1\u4e86\u9996\u4e2a\u9ad8\u65af\u6492\u70b9\u89c6\u9891\u7f16\u89e3\u7801\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u65b0\u65b9\u6cd5\u5c06\u7f16\u7801\u65f6\u95f4\u6bd4\u4e4b\u524d\u57fa\u4e8e\u9ad8\u65af\u6492\u70b9\u7684\u56fe\u50cf\u7f16\u89e3\u7801\u5668\u63d0\u5347\u4e8688%\uff0c\u8bc1\u660e\u4e86\u5176\u5b9e\u65f6\u89e3\u7801\u80fd\u529b\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u795e\u7ecf\u89c6\u9891\u7f16\u89e3\u7801\u9886\u57df\u63d0\u4f9b\u4e86\u521b\u65b0\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u5b9e\u65f6\u89c6\u9891\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2505.09329", "pdf": "https://arxiv.org/pdf/2505.09329", "abs": "https://arxiv.org/abs/2505.09329", "authors": ["Jiarun Liu", "Hong-Yu Zhou", "Weijian Huang", "Hao Yang", "Dongning Song", "Tao Tan", "Yong Liang", "Shanshan Wang"], "title": "BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 4 figures", "summary": "Scaling up model and data size have demonstrated impressive performance\nimprovement over a wide range of tasks. Despite extensive studies on scaling\nbehaviors for general-purpose tasks, medical images exhibit substantial\ndifferences from natural data. It remains unclear the key factors in developing\nmedical vision foundation models at scale due to the absence of an extensive\nunderstanding of scaling behavior in the medical domain. In this paper, we\nexplored the scaling behavior across model sizes, training algorithms, data\nsizes, and imaging modalities in developing scalable medical vision foundation\nmodels by self-supervised learning. To support scalable pretraining, we\nintroduce BioVFM-21M, a large-scale biomedical image dataset encompassing a\nwide range of biomedical image modalities and anatomies. We observed that\nscaling up does provide benefits but varies across tasks. Additional analysis\nreveals several factors correlated with scaling benefits. Finally, we propose\nBioVFM, a large-scale medical vision foundation model pretrained on 21 million\nbiomedical images, which outperforms the previous state-of-the-art foundation\nmodels across 12 medical benchmarks. Our results highlight that while scaling\nup is beneficial for pursuing better performance, task characteristics, data\ndiversity, pretraining methods, and computational efficiency remain critical\nconsiderations for developing scalable medical foundation models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e2d\u6a21\u578b\u89c4\u6a21\u3001\u8bad\u7ec3\u7b97\u6cd5\u3001\u6570\u636e\u89c4\u6a21\u548c\u6210\u50cf\u6a21\u6001\u7684\u6269\u5c55\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u5f00\u53d1\u4e86\u53ef\u6269\u5c55\u7684BioVFM-21M\u6570\u636e\u96c6\u548cBioVFM\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u6269\u5c55\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u76ca\u5904\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u4e0e\u81ea\u7136\u6570\u636e\u5dee\u5f02\u663e\u8457\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u533b\u5b66\u9886\u57df\u6269\u5c55\u884c\u4e3a\u7684\u6df1\u5165\u7406\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5f00\u53d1\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5927\u5c0f\u3001\u8bad\u7ec3\u7b97\u6cd5\u3001\u6570\u636e\u89c4\u6a21\u548c\u6210\u50cf\u6a21\u6001\u7684\u6269\u5c55\u884c\u4e3a\uff0c\u5f15\u5165BioVFM-21M\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u9884\u8bad\u7ec3BioVFM\u6a21\u578b\u3002", "result": "BioVFM\u572812\u4e2a\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u6269\u5c55\u7684\u76ca\u5904\u56e0\u4efb\u52a1\u800c\u5f02\uff0c\u4e14\u4e0e\u4efb\u52a1\u7279\u6027\u3001\u6570\u636e\u591a\u6837\u6027\u3001\u9884\u8bad\u7ec3\u65b9\u6cd5\u548c\u8ba1\u7b97\u6548\u7387\u76f8\u5173\u3002", "conclusion": "\u6269\u5c55\u6709\u52a9\u4e8e\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u4efb\u52a1\u7279\u6027\u3001\u6570\u636e\u591a\u6837\u6027\u3001\u9884\u8bad\u7ec3\u65b9\u6cd5\u548c\u8ba1\u7b97\u6548\u7387\u4ecd\u662f\u5f00\u53d1\u53ef\u6269\u5c55\u533b\u5b66\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2505.09342", "pdf": "https://arxiv.org/pdf/2505.09342", "abs": "https://arxiv.org/abs/2505.09342", "authors": ["Mostafa Jafari", "Alireza Shameli-Sendi"], "title": "Evaluating the Robustness of Adversarial Defenses in Malware Detection Systems", "categories": ["cs.CR", "cs.AI", "cs.LG", "68", "I.2.1"], "comment": "Submitted to IEEE Transactions on Information Forensics and Security\n  (T-IFS), 13 pages, 4 figures", "summary": "Machine learning is a key tool for Android malware detection, effectively\nidentifying malicious patterns in apps. However, ML-based detectors are\nvulnerable to evasion attacks, where small, crafted changes bypass detection.\nDespite progress in adversarial defenses, the lack of comprehensive evaluation\nframeworks in binary-constrained domains limits understanding of their\nrobustness. We introduce two key contributions. First, Prioritized Binary\nRounding, a technique to convert continuous perturbations into binary feature\nspaces while preserving high attack success and low perturbation size. Second,\nthe sigma-binary attack, a novel adversarial method for binary domains,\ndesigned to achieve attack goals with minimal feature changes. Experiments on\nthe Malscan dataset show that sigma-binary outperforms existing attacks and\nexposes key vulnerabilities in state-of-the-art defenses. Defenses equipped\nwith adversary detectors, such as KDE, DLA, DNN+, and ICNN, exhibit significant\nbrittleness, with attack success rates exceeding 90% using fewer than 10\nfeature modifications and reaching 100% with just 20. Adversarially trained\ndefenses, including AT-rFGSM-k, AT-MaxMA, improves robustness under small\nbudgets but remains vulnerable to unrestricted perturbations, with attack\nsuccess rates of 99.45% and 96.62%, respectively. Although PAD-SMA demonstrates\nstrong robustness against state-of-the-art gradient-based adversarial attacks\nby maintaining an attack success rate below 16.55%, the sigma-binary attack\nsignificantly outperforms these methods, achieving a 94.56% success rate under\nunrestricted perturbations. These findings highlight the critical need for\nprecise method like sigma-binary to expose hidden vulnerabilities in existing\ndefenses and support the development of more resilient malware detection\nsystems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5148\u4e8c\u8fdb\u5236\u820d\u5165\u6280\u672f\u548csigma-binary\u653b\u51fb\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e8c\u8fdb\u5236\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684Android\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u9632\u5fa1\uff0c\u5b9e\u9a8c\u8868\u660e\u65b0\u653b\u51fb\u65b9\u6cd5\u80fd\u9ad8\u6548\u7ed5\u8fc7\u73b0\u6709\u9632\u5fa1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684Android\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u5668\u6613\u53d7\u9003\u907f\u653b\u51fb\uff0c\u7f3a\u4e4f\u5bf9\u4e8c\u8fdb\u5236\u7ea6\u675f\u57df\u9632\u5fa1\u9c81\u68d2\u6027\u7684\u5168\u9762\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f15\u5165\u4e86\u4f18\u5148\u4e8c\u8fdb\u5236\u820d\u5165\u6280\u672f\u5c06\u8fde\u7eed\u6270\u52a8\u8f6c\u6362\u4e3a\u4e8c\u8fdb\u5236\u7279\u5f81\u7a7a\u95f4\uff0c\u5e76\u63d0\u51fasigma-binary\u653b\u51fb\u65b9\u6cd5\uff0c\u65e8\u5728\u4ee5\u6700\u5c11\u7279\u5f81\u4fee\u6539\u5b9e\u73b0\u653b\u51fb\u76ee\u6807\u3002", "result": "\u5728Malscan\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0csigma-binary\u653b\u51fb\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u653b\u51fb\u6210\u529f\u7387\u5728\u5c11\u91cf\u7279\u5f81\u4fee\u6539\u4e0b\u53ef\u8fbe90%\u4ee5\u4e0a\uff0c\u751a\u81f3100%\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u9700\u91c7\u7528\u7cbe\u786e\u65b9\u6cd5\u5982sigma-binary\u66b4\u9732\u73b0\u6709\u9632\u5fa1\u7684\u6f5c\u5728\u6f0f\u6d1e\uff0c\u4ee5\u652f\u6301\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u7cfb\u7edf\u3002"}}
{"id": "2505.09075", "pdf": "https://arxiv.org/pdf/2505.09075", "abs": "https://arxiv.org/abs/2505.09075", "authors": ["Carlos Misael Madrid Padilla", "Oscar Hernan Madrid Padilla", "Sabyasachi Chatterjee"], "title": "Risk Bounds For Distributional Regression", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This work examines risk bounds for nonparametric distributional regression\nestimators. For convex-constrained distributional regression, general upper\nbounds are established for the continuous ranked probability score (CRPS) and\nthe worst-case mean squared error (MSE) across the domain. These theoretical\nresults are applied to isotonic and trend filtering distributional regression,\nyielding convergence rates consistent with those for mean estimation.\nFurthermore, a general upper bound is derived for distributional regression\nunder non-convex constraints, with a specific application to neural\nnetwork-based estimators. Comprehensive experiments on both simulated and real\ndata validate the theoretical contributions, demonstrating their practical\neffectiveness.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u975e\u53c2\u6570\u5206\u5e03\u56de\u5f52\u4f30\u8ba1\u7684\u98ce\u9669\u754c\u9650\uff0c\u63d0\u51fa\u4e86CRPS\u548c\u6700\u574f\u60c5\u51b5MSE\u7684\u7406\u8bba\u4e0a\u754c\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u6548\u679c\u3002", "motivation": "\u63a2\u8ba8\u975e\u53c2\u6570\u5206\u5e03\u56de\u5f52\u4f30\u8ba1\u7684\u98ce\u9669\u754c\u9650\uff0c\u586b\u8865\u7406\u8bba\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u51f8\u7ea6\u675f\u548c\u975e\u51f8\u7ea6\u675f\u7684\u5206\u5e03\u56de\u5f52\u5206\u6790\uff0c\u63a8\u5bfcCRPS\u548cMSE\u7684\u7406\u8bba\u4e0a\u754c\uff0c\u5e76\u5e94\u7528\u4e8e\u7b49\u6e17\u548c\u8d8b\u52bf\u6ee4\u6ce2\u56de\u5f52\u53ca\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u8d21\u732e\uff0c\u8868\u660e\u5176\u5728\u6a21\u62df\u548c\u5b9e\u9645\u6570\u636e\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u975e\u53c2\u6570\u5206\u5e03\u56de\u5f52\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u652f\u6301\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u79cd\u7ea6\u675f\u4e0b\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2505.09343", "pdf": "https://arxiv.org/pdf/2505.09343", "abs": "https://arxiv.org/abs/2505.09343", "authors": ["Chenggang Zhao", "Chengqi Deng", "Chong Ruan", "Damai Dai", "Huazuo Gao", "Jiashi Li", "Liyue Zhang", "Panpan Huang", "Shangyan Zhou", "Shirong Ma", "Wenfeng Liang", "Ying He", "Yuqing Wang", "Yuxuan Liu", "Y. X. Wei"], "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures", "categories": ["cs.DC", "cs.AI", "cs.AR"], "comment": "This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution. The definitive version will appear as\n  part of the Industry Track in Proceedings of the 52nd Annual International\n  Symposium on Computer Architecture (ISCA '25)", "summary": "The rapid scaling of large language models (LLMs) has unveiled critical\nlimitations in current hardware architectures, including constraints in memory\ncapacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,\ntrained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model\nco-design can effectively address these challenges, enabling cost-efficient\ntraining and inference at scale. This paper presents an in-depth analysis of\nthe DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting\nkey innovations such as Multi-head Latent Attention (MLA) for enhanced memory\nefficiency, Mixture of Experts (MoE) architectures for optimized\ncomputation-communication trade-offs, FP8 mixed-precision training to unlock\nthe full potential of hardware capabilities, and a Multi-Plane Network Topology\nto minimize cluster-level network overhead. Building on the hardware\nbottlenecks encountered during DeepSeek-V3's development, we engage in a\nbroader discussion with academic and industry peers on potential future\nhardware directions, including precise low-precision computation units,\nscale-up and scale-out convergence, and innovations in low-latency\ncommunication fabrics. These insights underscore the critical role of hardware\nand model co-design in meeting the escalating demands of AI workloads, offering\na practical blueprint for innovation in next-generation AI systems.", "AI": {"tldr": "DeepSeek-V3\u901a\u8fc7\u786c\u4ef6\u611f\u77e5\u7684\u6a21\u578b\u534f\u540c\u8bbe\u8ba1\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5185\u5b58\u5bb9\u91cf\u3001\u8ba1\u7b97\u6548\u7387\u548c\u4e92\u8054\u5e26\u5bbd\u4e0a\u7684\u74f6\u9888\uff0c\u63d0\u51fa\u591a\u9879\u521b\u65b0\u6280\u672f\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u786c\u4ef6\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u5f53\u524d\u786c\u4ef6\u67b6\u6784\u5728LLMs\u5feb\u901f\u6269\u5c55\u8fc7\u7a0b\u4e2d\u66b4\u9732\u4e86\u5185\u5b58\u3001\u8ba1\u7b97\u548c\u5e26\u5bbd\u7684\u9650\u5236\uff0c\u4e9f\u9700\u901a\u8fc7\u786c\u4ef6\u4e0e\u6a21\u578b\u534f\u540c\u8bbe\u8ba1\u6765\u63d0\u5347\u6548\u7387\u548c\u964d\u4f4e\u6210\u672c\u3002", "method": "\u91c7\u7528Multi-head Latent Attention (MLA)\u589e\u5f3a\u5185\u5b58\u6548\u7387\u3001MoE\u67b6\u6784\u4f18\u5316\u8ba1\u7b97-\u901a\u4fe1\u5e73\u8861\u3001FP8\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u5145\u5206\u53d1\u6325\u786c\u4ef6\u6f5c\u529b\uff0c\u4ee5\u53caMulti-Plane\u7f51\u7edc\u62d3\u6251\u51cf\u5c11\u96c6\u7fa4\u7ea7\u7f51\u7edc\u5f00\u9500\u3002", "result": "DeepSeek-V3\u57282048\u5757NVIDIA H800 GPU\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u9a8c\u8bc1\u4e86\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u786c\u4ef6\u4e0e\u6a21\u578b\u534f\u540c\u8bbe\u8ba1\u662f\u5e94\u5bf9AI\u5de5\u4f5c\u8d1f\u8f7d\u9700\u6c42\u7684\u5173\u952e\uff0c\u672c\u6587\u4e3a\u4e0b\u4e00\u4ee3AI\u7cfb\u7edf\u521b\u65b0\u63d0\u4f9b\u4e86\u5b9e\u8df5\u84dd\u56fe\u3002"}}
{"id": "2505.09087", "pdf": "https://arxiv.org/pdf/2505.09087", "abs": "https://arxiv.org/abs/2505.09087", "authors": ["He Wang", "Yikun Zhang", "Jie Chen", "Jian Zhan", "Yaoqi Zhou"], "title": "A Comparative Review of RNA Language Models", "categories": ["q-bio.BM", "cs.LG"], "comment": null, "summary": "Given usefulness of protein language models (LMs) in structure and functional\ninference, RNA LMs have received increased attentions in the last few years.\nHowever, these RNA models are often not compared against the same standard.\nHere, we divided RNA LMs into three classes (pretrained on multiple RNA types\n(especially noncoding RNAs), specific-purpose RNAs, and LMs that unify RNA with\nDNA or proteins or both) and compared 13 RNA LMs along with 3 DNA and 1 protein\nLMs as controls in zero-shot prediction of RNA secondary structure and\nfunctional classification. Results shows that the models doing well on\nsecondary structure prediction often perform worse in function classification\nor vice versa, suggesting that more balanced unsupervised training is needed.", "AI": {"tldr": "\u6587\u7ae0\u901a\u8fc7\u5c06RNA\u8bed\u8a00\u6a21\u578b\u5206\u4e3a\u4e09\u7c7b\u5e76\u6bd4\u8f8313\u79cdRNA\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u64c5\u957f\u4e8c\u7ea7\u7ed3\u6784\u9884\u6d4b\u7684\u6a21\u578b\u5728\u529f\u80fd\u5206\u7c7b\u4e0a\u8868\u73b0\u5f80\u5f80\u8f83\u5dee\uff0c\u53cd\u4e4b\u4ea6\u7136\uff0c\u8868\u660e\u9700\u8981\u66f4\u5747\u8861\u7684\u65e0\u76d1\u7763\u8bad\u7ec3\u3002", "motivation": "\u9274\u4e8e\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u548c\u529f\u80fd\u63a8\u65ad\u4e2d\u7684\u5b9e\u7528\u6027\uff0cRNA\u8bed\u8a00\u6a21\u578b\u8fd1\u5e74\u53d7\u5230\u66f4\u591a\u5173\u6ce8\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u5c06RNA\u8bed\u8a00\u6a21\u578b\u5206\u4e3a\u4e09\u7c7b\uff08\u9884\u8bad\u7ec3\u4e8e\u591a\u79cdRNA\u3001\u7279\u5b9a\u76ee\u7684RNA\u3001\u7edf\u4e00RNA\u4e0eDNA\u6216\u86cb\u767d\u8d28\u7684\u6a21\u578b\uff09\uff0c\u5e76\u4e0eDNA\u548c\u86cb\u767d\u8d28\u6a21\u578b\u4f5c\u4e3a\u5bf9\u7167\uff0c\u8fdb\u884c\u96f6\u6837\u672c\u9884\u6d4bRNA\u4e8c\u7ea7\u7ed3\u6784\u548c\u529f\u80fd\u5206\u7c7b\u7684\u6bd4\u8f83\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u64c5\u957f\u4e8c\u7ea7\u7ed3\u6784\u9884\u6d4b\u7684\u6a21\u578b\u5728\u529f\u80fd\u5206\u7c7b\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002", "conclusion": "\u9700\u8981\u66f4\u5747\u8861\u7684\u65e0\u76d1\u7763\u8bad\u7ec3\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.09371", "pdf": "https://arxiv.org/pdf/2505.09371", "abs": "https://arxiv.org/abs/2505.09371", "authors": ["Akash Kundu", "Stefano Mangini"], "title": "TensorRL-QAS: Reinforcement learning with tensor networks for scalable quantum architecture search", "categories": ["quant-ph", "cs.AI", "cs.ET", "cs.LG"], "comment": "The code will be available soon! Comments are welcomed!", "summary": "Variational quantum algorithms hold the promise to address meaningful quantum\nproblems already on noisy intermediate-scale quantum hardware, but they face\nthe challenge of designing quantum circuits that both solve the target problem\nand comply with device limitations. Quantum architecture search (QAS) automates\nthis design process, with reinforcement learning (RL) emerging as a promising\napproach. Yet, RL-based QAS methods encounter significant scalability issues,\nas computational and training costs grow rapidly with the number of qubits,\ncircuit depth, and noise, severely impacting performance. To address these\nchallenges, we introduce $\\textit{TensorRL-QAS}$, a scalable framework that\ncombines tensor network (TN) methods with RL for designing quantum circuits. By\nwarm-starting the architecture search with a matrix product state approximation\nof the target solution, TensorRL-QAS effectively narrows the search space to\nphysically meaningful circuits, accelerating convergence to the desired\nsolution. Tested on several quantum chemistry problems of up to 12-qubit,\nTensorRL-QAS achieves up to a 10-fold reduction in CNOT count and circuit depth\ncompared to baseline methods, while maintaining or surpassing chemical\naccuracy. It reduces function evaluations by up to 100-fold, accelerates\ntraining episodes by up to $98\\%$, and achieves up to $50\\%$ success\nprobability for 10-qubit systems-far exceeding the $<1\\%$ rates of baseline\napproaches. Robustness and versatility are demonstrated both in the noiseless\nand noisy scenarios, where we report a simulation of up to 8-qubit. These\nadvancements establish TensorRL-QAS as a promising candidate for a scalable and\nefficient quantum circuit discovery protocol on near-term quantum hardware.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86TensorRL-QAS\u6846\u67b6\uff0c\u7ed3\u5408\u5f20\u91cf\u7f51\u7edc\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\uff0c\u5e76\u572812\u91cf\u5b50\u6bd4\u7279\u4ee5\u5185\u7684\u5316\u5b66\u95ee\u9898\u4e0a\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u91cf\u5b50\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\u5728\u6269\u5c55\u6027\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u91cf\u5b50\u6bd4\u7279\u6570\u3001\u7535\u8def\u6df1\u5ea6\u548c\u566a\u58f0\u589e\u52a0\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f20\u91cf\u7f51\u7edc\u7684\u77e9\u9635\u4e58\u79ef\u72b6\u6001\u8fd1\u4f3c\u76ee\u6807\u89e3\uff0c\u9884\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u4ee5\u7f29\u5c0f\u641c\u7d22\u7a7a\u95f4\uff0c\u8bbe\u8ba1\u7269\u7406\u610f\u4e49\u4e0a\u5408\u7406\u7684\u91cf\u5b50\u7535\u8def\u3002", "result": "\u572812\u91cf\u5b50\u6bd4\u7279\u7684\u5316\u5b66\u95ee\u9898\u4e0a\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0cCNOT\u95e8\u6570\u91cf\u548c\u7535\u8def\u6df1\u5ea6\u51cf\u5c1110\u500d\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u534798%\uff0c10\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u7684\u6210\u529f\u7387\u4ece<1%\u63d0\u5347\u81f350%\u3002", "conclusion": "TensorRL-QAS\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u6846\u67b6\uff0c\u9002\u5408\u8fd1\u671f\u7684\u542b\u566a\u58f0\u91cf\u5b50\u786c\u4ef6\u3002"}}
{"id": "2505.09098", "pdf": "https://arxiv.org/pdf/2505.09098", "abs": "https://arxiv.org/abs/2505.09098", "authors": ["Yan Hao Ling", "Zhouhao Yang", "Jonathan Scarlett"], "title": "Statistical Mean Estimation with Coded Relayed Observations", "categories": ["cs.IT", "cs.LG", "math.IT", "math.ST", "stat.TH"], "comment": null, "summary": "We consider a problem of statistical mean estimation in which the samples are\nnot observed directly, but are instead observed by a relay (``teacher'') that\ntransmits information through a memoryless channel to the decoder\n(``student''), who then produces the final estimate. We consider the minimax\nestimation error in the large deviations regime, and establish achievable error\nexponents that are tight in broad regimes of the estimation accuracy and\nchannel quality. In contrast, two natural baseline methods are shown to yield\nstrictly suboptimal error exponents. We initially focus on Bernoulli sources\nand binary symmetric channels, and then generalize to sub-Gaussian and\nheavy-tailed settings along with arbitrary discrete memoryless channels.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u6837\u672c\u901a\u8fc7\u4e2d\u7ee7\uff08\u201c\u8001\u5e08\u201d\uff09\u4f20\u8f93\u5230\u89e3\u7801\u5668\uff08\u201c\u5b66\u751f\u201d\uff09\u7684\u7edf\u8ba1\u5747\u503c\u4f30\u8ba1\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u5728\u5927\u504f\u5dee\u4f53\u5236\u4e0b\u53ef\u5b9e\u73b0\u7684\u7d27\u81f4\u8bef\u5dee\u6307\u6570\uff0c\u5e76\u5c55\u793a\u4e86\u4e24\u79cd\u57fa\u7ebf\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76\u901a\u8fc7\u4e2d\u7ee7\u4fe1\u9053\u4f20\u8f93\u6837\u672c\u7684\u7edf\u8ba1\u5747\u503c\u4f30\u8ba1\u95ee\u9898\uff0c\u65e8\u5728\u89e3\u51b3\u76f4\u63a5\u89c2\u6d4b\u6837\u672c\u4e0d\u53ef\u884c\u65f6\u7684\u4f30\u8ba1\u6027\u80fd\u4f18\u5316\u3002", "method": "\u7814\u7a76\u91c7\u7528\u5927\u504f\u5dee\u4f53\u5236\u4e0b\u7684\u6781\u5c0f\u6781\u5927\u4f30\u8ba1\u8bef\u5dee\u5206\u6790\u65b9\u6cd5\uff0c\u9996\u5148\u9488\u5bf9\u4f2f\u52aa\u5229\u6e90\u548c\u4e8c\u8fdb\u5236\u5bf9\u79f0\u4fe1\u9053\uff0c\u540e\u63a8\u5e7f\u81f3\u4e9a\u9ad8\u65af\u3001\u91cd\u5c3e\u5206\u5e03\u53ca\u4efb\u610f\u79bb\u6563\u65e0\u8bb0\u5fc6\u4fe1\u9053\u3002", "result": "\u5efa\u7acb\u4e86\u5728\u4f30\u8ba1\u7cbe\u5ea6\u548c\u4fe1\u9053\u8d28\u91cf\u7684\u5e7f\u6cdb\u8303\u56f4\u5185\u7d27\u81f4\u7684\u8bef\u5dee\u6307\u6570\uff0c\u8868\u660e\u4e24\u79cd\u57fa\u7ebf\u65b9\u6cd5\u5728\u8bef\u5dee\u6307\u6570\u4e0a\u4e25\u683c\u6b21\u4f18\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u901a\u8fc7\u4e2d\u7ee7\u4fe1\u9053\u8fdb\u884c\u7edf\u8ba1\u5747\u503c\u4f30\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5c55\u793a\u4e86\u4f18\u5316\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.09380", "pdf": "https://arxiv.org/pdf/2505.09380", "abs": "https://arxiv.org/abs/2505.09380", "authors": ["Qinghui Liu", "Jon Nesvold", "Hanna Raaum", "Elakkyen Murugesu", "Martin R\u00f8vang", "Bradley J Maclntosh", "Atle Bj\u00f8rnerud", "Karoline Skogen"], "title": "Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "19 pages, 11 figures, on submission to BMC Methods", "summary": "Background: There are many challenges and opportunities in the clinical\ndeployment of AI tools in radiology. The current study describes a radiology\nsoftware platform called NeoMedSys that can enable efficient deployment and\nrefinements of AI models. We evaluated the feasibility and effectiveness of\nrunning NeoMedSys for three months in real-world clinical settings and focused\non improvement performance of an in-house developed AI model (VIOLA-AI)\ndesigned for intracranial hemorrhage (ICH) detection.\n  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI\nmodels with a web-based medical image viewer, annotation system, and\nhospital-wide radiology information systems. A pragmatic investigation was\ndeployed using clinical cases of patients presenting to the largest Emergency\nDepartment in Norway (site-1) with suspected traumatic brain injury (TBI) or\npatients with suspected stroke (site-2). We assessed ICH classification\nperformance as VIOLA-AI encountered new data and underwent pre-planned model\nretraining. Performance metrics included sensitivity, specificity, accuracy,\nand the area under the receiver operating characteristic curve (AUC).\n  Results: NeoMedSys facilitated iterative improvements in the AI model,\nsignificantly enhancing its diagnostic accuracy. Automated bleed detection and\nsegmentation were reviewed in near real-time to facilitate re-training\nVIOLA-AI. The iterative refinement process yielded a marked improvement in\nclassification sensitivity, rising to 90.3% (from 79.2%), and specificity that\nreached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire\nsample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).\nModel refinement stages were associated with notable gains, highlighting the\nvalue of real-time radiologist feedback.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aNeoMedSys\u7684\u653e\u5c04\u5b66\u8f6f\u4ef6\u5e73\u53f0\uff0c\u7528\u4e8e\u9ad8\u6548\u90e8\u7f72\u548c\u4f18\u5316AI\u6a21\u578b\uff0c\u5e76\u5728\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u8bc4\u4f30\u4e86\u5176\u6548\u679c\u3002\u901a\u8fc7\u8fed\u4ee3\u6539\u8fdb\uff0cAI\u6a21\u578b\uff08VIOLA-AI\uff09\u7684\u9885\u5185\u51fa\u8840\u68c0\u6d4b\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3AI\u5de5\u5177\u5728\u653e\u5c04\u5b66\u4e34\u5e8a\u90e8\u7f72\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7NeoMedSys\u5e73\u53f0\u5b9e\u73b0AI\u6a21\u578b\u7684\u5feb\u901f\u90e8\u7f72\u548c\u4f18\u5316\uff0c\u4ee5\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "NeoMedSys\u96c6\u6210\u4e86AI\u6a21\u578b\u90e8\u7f72\u3001\u6d4b\u8bd5\u548c\u4f18\u5316\u5de5\u5177\uff0c\u5e76\u7ed3\u5408\u533b\u5b66\u5f71\u50cf\u67e5\u770b\u5668\u3001\u6807\u6ce8\u7cfb\u7edf\u548c\u5168\u9662\u653e\u5c04\u4fe1\u606f\u7cfb\u7edf\u3002\u5728\u632a\u5a01\u6700\u5927\u6025\u8bca\u79d1\u5bf9\u7591\u4f3c\u8111\u5916\u4f24\u6216\u4e2d\u98ce\u60a3\u8005\u8fdb\u884c\u5b9e\u7528\u6027\u8c03\u67e5\uff0c\u5e76\u901a\u8fc7\u9884\u8bbe\u6a21\u578b\u518d\u8bad\u7ec3\u8bc4\u4f30\u6027\u80fd\u6307\u6807\u3002", "result": "\u8fed\u4ee3\u6539\u8fdb\u663e\u8457\u63d0\u5347\u4e86VIOLA-AI\u7684\u654f\u611f\u6027\uff0890.3%\uff09\u548c\u7279\u5f02\u6027\uff0889.3%\uff09\uff0cROC\u66f2\u7ebf\u7684AUC\u8fbe0.949\uff0c\u51f8\u663e\u5b9e\u65f6\u653e\u5c04\u79d1\u533b\u751f\u53cd\u9988\u7684\u4ef7\u503c\u3002", "conclusion": "NeoMedSys\u5e73\u53f0\u6709\u6548\u63d0\u5347\u4e86AI\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u4f18\u5316AI\u5de5\u5177\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.09099", "pdf": "https://arxiv.org/pdf/2505.09099", "abs": "https://arxiv.org/abs/2505.09099", "authors": ["Shirui Lyu", "Vittorio Caggiano", "Matteo Leonetti", "Dario Farina", "Letizia Gionfrida"], "title": "Imitation Learning for Adaptive Control of a Virtual Soft Exoglove", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "The use of wearable robots has been widely adopted in rehabilitation training\nfor patients with hand motor impairments. However, the uniqueness of patients'\nmuscle loss is often overlooked. Leveraging reinforcement learning and a\nbiologically accurate musculoskeletal model in simulation, we propose a\ncustomized wearable robotic controller that is able to address specific muscle\ndeficits and to provide compensation for hand-object manipulation tasks. Video\ndata of a same subject performing human grasping tasks is used to train a\nmanipulation model through learning from demonstration. This manipulation model\nis subsequently fine-tuned to perform object-specific interaction tasks. The\nmuscle forces in the musculoskeletal manipulation model are then weakened to\nsimulate neurological motor impairments, which are later compensated by the\nactuation of a virtual wearable robotics glove. Results shows that integrating\nthe virtual wearable robotic glove provides shared assistance to support the\nhand manipulator with weakened muscle forces. The learned exoglove controller\nachieved an average of 90.5\\% of the original manipulation proficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u751f\u7269\u7cbe\u786e\u808c\u8089\u9aa8\u9abc\u6a21\u578b\u7684\u5b9a\u5236\u5316\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u624b\u90e8\u808c\u8089\u7f3a\u635f\u95ee\u9898\uff0c\u5e76\u5728\u624b-\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\u4e2d\u63d0\u4f9b\u8865\u507f\u3002\u901a\u8fc7\u4eff\u771f\u548c\u4eba\u7c7b\u6293\u53d6\u4efb\u52a1\u7684\u89c6\u9891\u6570\u636e\u8bad\u7ec3\u6a21\u578b\uff0c\u6700\u7ec8\u865a\u62df\u624b\u5957\u63a7\u5236\u5668\u6062\u590d\u4e8690.5%\u7684\u539f\u59cb\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u5728\u5eb7\u590d\u8bad\u7ec3\u4e2d\u5e38\u5ffd\u7565\u60a3\u8005\u808c\u8089\u635f\u4f24\u7684\u72ec\u7279\u6027\uff0c\u5bfc\u81f4\u8865\u507f\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u751f\u7269\u7cbe\u786e\u808c\u8089\u9aa8\u9abc\u6a21\u578b\uff0c\u901a\u8fc7\u4eff\u771f\u548c\u4eba\u7c7b\u6293\u53d6\u89c6\u9891\u6570\u636e\u8bad\u7ec3\u5b9a\u5236\u5316\u63a7\u5236\u5668\uff0c\u5e76\u6a21\u62df\u795e\u7ecf\u8fd0\u52a8\u635f\u4f24\u4ee5\u6d4b\u8bd5\u8865\u507f\u6548\u679c\u3002", "result": "\u865a\u62df\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u624b\u5957\u4e3a\u808c\u8089\u529b\u91cf\u51cf\u5f31\u7684\u624b\u90e8\u64cd\u7eb5\u5668\u63d0\u4f9b\u4e86\u5171\u4eab\u8f85\u52a9\uff0c\u63a7\u5236\u5668\u6062\u590d\u4e8690.5%\u7684\u539f\u59cb\u64cd\u4f5c\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u5b9a\u5236\u5316\u63a7\u5236\u5668\u80fd\u6709\u6548\u8865\u507f\u7279\u5b9a\u808c\u8089\u7f3a\u635f\uff0c\u63d0\u5347\u5eb7\u590d\u8bad\u7ec3\u4e2d\u7684\u64cd\u4f5c\u80fd\u529b\u3002"}}
{"id": "2505.09382", "pdf": "https://arxiv.org/pdf/2505.09382", "abs": "https://arxiv.org/abs/2505.09382", "authors": ["Zhengyan Sheng", "Jinghao He", "Liping Chen", "Kong Aik Lee", "Zhen-Hua Ling"], "title": "The Voice Timbre Attribute Detection 2025 Challenge Evaluation Plan", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Voice timbre refers to the unique quality or character of a person's voice\nthat distinguishes it from others as perceived by human hearing. The Voice\nTimbre Attribute Detection (VtaD) 2025 challenge focuses on explaining the\nvoice timbre attribute in a comparative manner. In this challenge, the human\nimpression of voice timbre is verbalized with a set of sensory descriptors,\nincluding bright, coarse, soft, magnetic, and so on. The timbre is explained\nfrom the comparison between two voices in their intensity within a specific\ndescriptor dimension. The VtaD 2025 challenge starts in May and culminates in a\nspecial proposal at the NCMMSC2025 conference in October 2025 in Zhenjiang,\nChina.", "AI": {"tldr": "VtaD 2025\u6311\u6218\u65e8\u5728\u901a\u8fc7\u611f\u5b98\u63cf\u8ff0\u7b26\uff08\u5982\u660e\u4eae\u3001\u7c97\u7cd9\u3001\u67d4\u548c\u7b49\uff09\u6bd4\u8f83\u89e3\u91ca\u4eba\u58f0\u97f3\u8272\u5c5e\u6027\u3002", "motivation": "\u7814\u7a76\u4eba\u58f0\u97f3\u8272\u7684\u72ec\u7279\u611f\u77e5\uff0c\u5e76\u901a\u8fc7\u6bd4\u8f83\u65b9\u5f0f\u66f4\u7cbe\u51c6\u5730\u63cf\u8ff0\u548c\u5206\u7c7b\u58f0\u97f3\u7279\u6027\u3002", "method": "\u4f7f\u7528\u611f\u5b98\u63cf\u8ff0\u7b26\u5bf9\u4e24\u79cd\u58f0\u97f3\u7684\u5f3a\u5ea6\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u6311\u6218\u5c06\u57282025\u5e745\u6708\u542f\u52a8\uff0c\u5e76\u4e8e10\u6708\u5728\u9547\u6c5f\u7684NCMMSC2025\u4f1a\u8bae\u4e0a\u5c55\u793a\u7279\u522b\u63d0\u6848\u3002", "conclusion": "VtaD 2025\u6311\u6218\u4e3a\u58f0\u97f3\u8272\u5c5e\u6027\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u6bd4\u8f83\u5206\u6790\u6846\u67b6\u3002"}}
{"id": "2505.09110", "pdf": "https://arxiv.org/pdf/2505.09110", "abs": "https://arxiv.org/abs/2505.09110", "authors": ["Zhihao Dou", "Jiaqi Wang", "Wei Sun", "Zhuqing Liu", "Minghong Fang"], "title": "Toward Malicious Clients Detection in Federated Learning", "categories": ["cs.CR", "cs.DC", "cs.LG"], "comment": "To appear in ACM ASIACCS 2025", "summary": "Federated learning (FL) enables multiple clients to collaboratively train a\nglobal machine learning model without sharing their raw data. However, the\ndecentralized nature of FL introduces vulnerabilities, particularly to\npoisoning attacks, where malicious clients manipulate their local models to\ndisrupt the training process. While Byzantine-robust aggregation rules have\nbeen developed to mitigate such attacks, they remain inadequate against more\nadvanced threats. In response, recent advancements have focused on FL detection\ntechniques to identify potentially malicious participants. Unfortunately, these\nmethods often misclassify numerous benign clients as threats or rely on\nunrealistic assumptions about the server's capabilities. In this paper, we\npropose a novel algorithm, SafeFL, specifically designed to accurately identify\nmalicious clients in FL. The SafeFL approach involves the server collecting a\nseries of global models to generate a synthetic dataset, which is then used to\ndistinguish between malicious and benign models based on their behavior.\nExtensive testing demonstrates that SafeFL outperforms existing methods,\noffering superior efficiency and accuracy in detecting malicious clients.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSafeFL\u7684\u65b0\u7b97\u6cd5\uff0c\u65e8\u5728\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u51c6\u786e\u8bc6\u522b\u6076\u610f\u5ba2\u6237\u7aef\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\u533a\u5206\u6076\u610f\u4e0e\u826f\u6027\u6a21\u578b\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u7684\u53bb\u4e2d\u5fc3\u5316\u7279\u6027\u4f7f\u5176\u6613\u53d7\u6bd2\u5316\u653b\u51fb\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5bf9\u9ad8\u7ea7\u5a01\u80c1\u65e0\u6548\u6216\u8bef\u5224\u7387\u9ad8\uff0c\u4e9f\u9700\u66f4\u51c6\u786e\u7684\u68c0\u6d4b\u6280\u672f\u3002", "method": "\u63d0\u51faSafeFL\u7b97\u6cd5\uff0c\u670d\u52a1\u5668\u6536\u96c6\u4e00\u7cfb\u5217\u5168\u5c40\u6a21\u578b\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff0c\u636e\u6b64\u901a\u8fc7\u6a21\u578b\u884c\u4e3a\u8bc6\u522b\u6076\u610f\u5ba2\u6237\u7aef\u3002", "result": "\u5927\u91cf\u6d4b\u8bd5\u663e\u793aSafeFL\u5728\u68c0\u6d4b\u6076\u610f\u5ba2\u6237\u7aef\u65b9\u9762\u6548\u7387\u548c\u51c6\u786e\u6027\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SafeFL\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u6076\u610f\u5ba2\u6237\u7aef\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u9632\u5fa1\u80fd\u529b\u3002"}}
{"id": "2505.09385", "pdf": "https://arxiv.org/pdf/2505.09385", "abs": "https://arxiv.org/abs/2505.09385", "authors": ["Xiaoyang Yu", "Xiaoming Wu", "Xin Wang", "Dongrun Li", "Ming Yang", "Peng Cheng"], "title": "FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Federated semantic segmentation enables pixel-level classification in images\nthrough collaborative learning while maintaining data privacy. However,\nexisting research commonly overlooks the fine-grained class relationships\nwithin the semantic space when addressing heterogeneous problems, particularly\ndomain shift. This oversight results in ambiguities between class\nrepresentation. To overcome this challenge, we propose a novel federated\nsegmentation framework that strikes class consistency, termed FedSaaS.\nSpecifically, we introduce class exemplars as a criterion for both local- and\nglobal-level class representations. On the server side, the uploaded class\nexemplars are leveraged to model class prototypes, which supervise global\nbranch of clients, ensuring alignment with global-level representation. On the\nclient side, we incorporate an adversarial mechanism to harmonize contributions\nof global and local branches, leading to consistent output. Moreover,\nmultilevel contrastive losses are employed on both sides to enforce consistency\nbetween two-level representations in the same semantic space. Extensive\nexperiments on several driving scene segmentation datasets demonstrate that our\nframework outperforms state-of-the-art methods, significantly improving average\nsegmentation accuracy and effectively addressing the class-consistency\nrepresentation problem.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedSaaS\u7684\u65b0\u578b\u8054\u90a6\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u7c7b\u8303\u4f8b\u548c\u5bf9\u6297\u673a\u5236\u89e3\u51b3\u9886\u57df\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u89e3\u51b3\u9886\u57df\u504f\u79fb\u95ee\u9898\u65f6\u5ffd\u89c6\u4e86\u8bed\u4e49\u7a7a\u95f4\u4e2d\u7ec6\u7c92\u5ea6\u7684\u7c7b\u5173\u7cfb\uff0c\u5bfc\u81f4\u7c7b\u8868\u793a\u6a21\u7cca\u3002", "method": "\u5f15\u5165\u7c7b\u8303\u4f8b\u4f5c\u4e3a\u672c\u5730\u548c\u5168\u5c40\u7c7b\u8868\u793a\u7684\u57fa\u51c6\uff0c\u670d\u52a1\u5668\u7aef\u5efa\u6a21\u7c7b\u539f\u578b\u4ee5\u76d1\u7763\u5ba2\u6237\u7aef\u5168\u5c40\u5206\u652f\uff0c\u5ba2\u6237\u7aef\u901a\u8fc7\u5bf9\u6297\u673a\u5236\u534f\u8c03\u5168\u5c40\u4e0e\u672c\u5730\u5206\u652f\uff0c\u5e76\u91c7\u7528\u591a\u7ea7\u5bf9\u6bd4\u635f\u5931\u786e\u4fdd\u8bed\u4e49\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u9a7e\u9a76\u573a\u666f\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedSaaS\u663e\u8457\u63d0\u5347\u4e86\u5e73\u5747\u5206\u5272\u7cbe\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7c7b\u4e00\u81f4\u6027\u8868\u793a\u95ee\u9898\u3002", "conclusion": "FedSaaS\u901a\u8fc7\u7c7b\u8303\u4f8b\u548c\u591a\u7ea7\u5bf9\u6bd4\u635f\u5931\u5b9e\u73b0\u8054\u90a6\u8bed\u4e49\u5206\u5272\u7684\u7c7b\u4e00\u81f4\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.09393", "pdf": "https://arxiv.org/pdf/2505.09393", "abs": "https://arxiv.org/abs/2505.09393", "authors": ["Huakun Liu", "Hiroki Ota", "Xin Wei", "Yutaro Hirao", "Monica Perusquia-Hernandez", "Hideaki Uchiyama", "Kiyoshi Kiyokawa"], "title": "UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Sparse wearable inertial measurement units (IMUs) have gained popularity for\nestimating 3D human motion. However, challenges such as pose ambiguity, data\ndrift, and limited adaptability to diverse bodies persist. To address these\nissues, we propose UMotion, an uncertainty-driven, online fusing-all state\nestimation framework for 3D human shape and pose estimation, supported by six\nintegrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB\nsensors measure inter-node distances to infer spatial relationships, aiding in\nresolving pose ambiguities and body shape variations when combined with\nanthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors\nare affected by body occlusions. Consequently, we develop a tightly coupled\nUnscented Kalman Filter (UKF) framework that fuses uncertainties from sensor\ndata and estimated human motion based on individual body shape. The UKF\niteratively refines IMU and UWB measurements by aligning them with uncertain\nhuman motion constraints in real-time, producing optimal estimates for each.\nExperiments on both synthetic and real-world datasets demonstrate the\neffectiveness of UMotion in stabilizing sensor data and the improvement over\nstate of the art in pose accuracy.", "AI": {"tldr": "UMotion is a new framework combining IMU\u548cUWB\u4f20\u611f\u5668\uff0c\u901a\u8fc7UKF\u5b9e\u65f6\u4f18\u5316\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u59ff\u6001\u6a21\u7cca\u3001\u6570\u636e\u6f02\u79fb\u548c\u4f53\u578b\u9002\u5e94\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u7a7f\u6234\u5f0fIMU\u57283D\u4eba\u4f53\u8fd0\u52a8\u4f30\u8ba1\u4e2d\u5b58\u5728\u59ff\u6001\u6a21\u7cca\u3001\u6570\u636e\u6f02\u79fb\u548c\u4f53\u578b\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86UMotion\u6846\u67b6\uff0c\u7ed3\u54086\u4e2aUWB\u8ddd\u79bb\u4f20\u611f\u5668\u548cIMU\uff0c\u5229\u7528UKF\u5b9e\u65f6\u878d\u5408\u4f20\u611f\u5668\u6570\u636e\u4e0e\u4eba\u4f53\u8fd0\u52a8\u4e0d\u786e\u5b9a\u6027\uff0c\u4f18\u5316\u4f30\u8ba1\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eUMotion\u80fd\u7a33\u5b9a\u4f20\u611f\u5668\u6570\u636e\uff0c\u5e76\u5728\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u4e0a\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u3002", "conclusion": "UMotion\u901a\u8fc7\u591a\u4f20\u611f\u5668\u878d\u5408\u548cUKF\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e863D\u4eba\u4f53\u8fd0\u52a8\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2505.09395", "pdf": "https://arxiv.org/pdf/2505.09395", "abs": "https://arxiv.org/abs/2505.09395", "authors": ["Chen-Yu Liu", "Kuan-Cheng Chen", "Yi-Chien Chen", "Samuel Yen-Chi Chen", "Wei-Hao Huang", "Wei-Jia Huang", "Yen-Jui Chang"], "title": "Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": null, "summary": "Typhoon trajectory forecasting is essential for disaster preparedness but\nremains computationally demanding due to the complexity of atmospheric dynamics\nand the resource requirements of deep learning models. Quantum-Train (QT), a\nhybrid quantum-classical framework that leverages quantum neural networks\n(QNNs) to generate trainable parameters exclusively during training,\neliminating the need for quantum hardware at inference time. Building on QT's\nsuccess across multiple domains, including image classification, reinforcement\nlearning, flood prediction, and large language model (LLM) fine-tuning, we\nintroduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting\nmodel learning. Integrated with an Attention-based Multi-ConvGRU model, QPA\nenables parameter-efficient training while maintaining predictive accuracy.\nThis work represents the first application of quantum machine learning (QML) to\nlarge-scale typhoon trajectory prediction, offering a scalable and\nenergy-efficient approach to climate modeling. Our results demonstrate that QPA\nsignificantly reduces the number of trainable parameters while preserving\nperformance, making high-performance forecasting more accessible and\nsustainable through hybrid quantum-classical learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86Quantum-Train (QT)\u6846\u67b6\uff0c\u7ed3\u5408\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNNs\uff09\u751f\u6210\u8bad\u7ec3\u53c2\u6570\uff0c\u4ec5\u9700\u5728\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u91cf\u5b50\u786c\u4ef6\u3002\u901a\u8fc7Quantum Parameter Adaptation (QPA)\u65b9\u6cd5\uff0c\u7ed3\u5408Attention-based Multi-ConvGRU\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53c2\u6570\u8f7b\u91cf\u7684\u53f0\u98ce\u8f68\u8ff9\u9884\u6d4b\uff0c\u9996\u6b21\u5c06\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u53f0\u98ce\u9884\u6d4b\u9886\u57df\u3002\u7ed3\u679c\u8868\u660eQPA\u5728\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u53f0\u98ce\u8f68\u8ff9\u9884\u6d4b\u5bf9\u9632\u707e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u3002\u7ed3\u5408\u91cf\u5b50\u8ba1\u7b97\u7684\u6f5c\u529b\uff0c\u63d0\u51faQT\u6846\u67b6\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u63d0\u51faQuantum Parameter Adaptation (QPA)\u65b9\u6cd5\uff0c\u96c6\u6210Attention-based Multi-ConvGRU\u6a21\u578b\uff0c\u4ec5\u5728\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u91cf\u5b50\u786c\u4ef6\u751f\u6210\u53c2\u6570\uff0c\u63a8\u7406\u9636\u6bb5\u65e0\u9700\u91cf\u5b50\u8bbe\u5907\u3002", "result": "QPA\u663e\u8457\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u6c14\u5019\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u8282\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u9996\u6b21\u5c06QML\u5e94\u7528\u4e8e\u53f0\u98ce\u8f68\u8ff9\u9884\u6d4b\uff0c\u9a8c\u8bc1\u4e86QT\u6846\u67b6\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u9ad8\u6548\u3001\u53ef\u6301\u7eed\u7684\u6c14\u5019\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.09161", "pdf": "https://arxiv.org/pdf/2505.09161", "abs": "https://arxiv.org/abs/2505.09161", "authors": ["Yu Xin", "Peng Liu", "Zhuohang Xie", "Wenhui Mi", "Pengyue Gao", "Hong Jian Zhao", "Jian Lv", "Yanchao Wang", "Yanming Ma"], "title": "Bridging Theory and Experiment in Materials Discovery: Machine-Learning-Assisted Prediction of Synthesizable Structures", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "comment": null, "summary": "Even though thermodynamic energy-based crystal structure prediction (CSP) has\nrevolutionized materials discovery, the energy-driven CSP approaches often\nstruggle to identify experimentally realizable metastable materials synthesized\nthrough kinetically controlled pathways, creating a critical gap between\ntheoretical predictions and experimental synthesis. Here, we propose a\nsynthesizability-driven CSP framework that integrates symmetry-guided structure\nderivation with a Wyckoff encode-based machine-learning model, allowing for the\nefficient localization of subspaces likely to yield highly synthesizable\nstructures. Within the identified promising subspaces, a structure-based\nsynthesizability evaluation model, fine-tuned using recently synthesized\nstructures to enhance predictive accuracy, is employed in conjunction with ab\ninitio calculations to systematically identify synthesizable candidates. The\nframework successfully reproduces 13 experimentally known XSe (X = Sc, Ti, Mn,\nFe, Ni, Cu, Zn) structures, demonstrating its effectiveness in predicting\nsynthesizable structures. Notably, 92,310 structures are filtered from the\n554,054 candidates predicted by GNoME, exhibiting great potential for promising\nsynthesizability. Additionally, eight thermodynamically favorable Hf-X-O (X =\nTi, V, and Mn) structures have been identified, among which three HfV$_2$O$_7$\ncandidates exhibit high synthesizability, presenting viable candidates for\nexperimental realization and potentially associated with experimentally\nobserved temperature-induced phase transitions. This work establishes a\ndata-driven paradigm for machine-learning-assisted inorganic materials\nsynthesis, highlighting its potential to bridge the gap between computational\npredictions and experimental realization while unlocking new opportunities for\nthe targeted discovery of novel functional materials.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5408\u6210\u6027\u9a71\u52a8\u7684\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u5bf9\u79f0\u6027\u5f15\u5bfc\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u6709\u6548\u4ece\u5927\u91cf\u5019\u9009\u7ed3\u6784\u4e2d\u7b5b\u9009\u51fa\u53ef\u5408\u6210\u6750\u6599\u3002", "motivation": "\u4f20\u7edf\u70ed\u529b\u5b66\u9a71\u52a8\u7684\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\u65b9\u6cd5\u5f80\u5f80\u96be\u4ee5\u8bc6\u522b\u5b9e\u9a8c\u4e2d\u901a\u8fc7\u52a8\u529b\u5b66\u9014\u5f84\u5408\u6210\u7684\u4e9a\u7a33\u6001\u6750\u6599\uff0c\u5bfc\u81f4\u7406\u8bba\u4e0e\u5b9e\u9a8c\u7684\u8131\u8282\u3002", "method": "\u91c7\u7528\u5bf9\u79f0\u6027\u5f15\u5bfc\u7684\u7ed3\u6784\u63a8\u5bfc\u548c\u57fa\u4e8eWyckoff\u7f16\u7801\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u4ece\u5934\u8ba1\u7b97\uff0c\u7cfb\u7edf\u8bc4\u4f30\u7ed3\u6784\u7684\u5408\u6210\u6027\u3002", "result": "\u6210\u529f\u91cd\u73b013\u79cd\u5df2\u77e5XSe\u7ed3\u6784\uff0c\u5e76\u4ece55\u4e07\u5019\u9009\u7ed3\u6784\u4e2d\u7b5b\u9009\u51fa9\u4e07\u591a\u4e2a\u6f5c\u5728\u53ef\u5408\u6210\u7ed3\u6784\uff0c\u8fd8\u9274\u5b9a\u51fa8\u79cd\u70ed\u529b\u5b66\u6709\u5229\u7684Hf-X-O\u7ed3\u6784\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u7f29\u5c0f\u4e86\u8ba1\u7b97\u9884\u6d4b\u4e0e\u5b9e\u9a8c\u5408\u6210\u7684\u5dee\u8ddd\uff0c\u4e3a\u529f\u80fd\u6027\u6750\u6599\u7684\u5b9a\u5411\u53d1\u73b0\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.09167", "pdf": "https://arxiv.org/pdf/2505.09167", "abs": "https://arxiv.org/abs/2505.09167", "authors": ["Amit Daniely", "Idan Mehalel", "Elchanan Mossel"], "title": "Online Learning of Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We study online learning of feedforward neural networks with the sign\nactivation function that implement functions from the unit ball in\n$\\mathbb{R}^d$ to a finite label set $\\{1, \\ldots, Y\\}$.\n  First, we characterize a margin condition that is sufficient and in some\ncases necessary for online learnability of a neural network: Every neuron in\nthe first hidden layer classifies all instances with some margin $\\gamma$\nbounded away from zero. Quantitatively, we prove that for any net, the optimal\nmistake bound is at most approximately $\\mathtt{TS}(d,\\gamma)$, which is the\n$(d,\\gamma)$-totally-separable-packing number, a more restricted variation of\nthe standard $(d,\\gamma)$-packing number. We complement this result by\nconstructing a net on which any learner makes $\\mathtt{TS}(d,\\gamma)$ many\nmistakes. We also give a quantitative lower bound of approximately\n$\\mathtt{TS}(d,\\gamma) \\geq \\max\\{1/(\\gamma \\sqrt{d})^d, d\\}$ when $\\gamma \\geq\n1/2$, implying that for some nets and input sequences every learner will err\nfor $\\exp(d)$ many times, and that a dimension-free mistake bound is almost\nalways impossible.\n  To remedy this inevitable dependence on $d$, it is natural to seek additional\nnatural restrictions to be placed on the network, so that the dependence on $d$\nis removed. We study two such restrictions. The first is the multi-index model,\nin which the function computed by the net depends only on $k \\ll d$ orthonormal\ndirections. We prove a mistake bound of approximately $(1.5/\\gamma)^{k + 2}$ in\nthis model. The second is the extended margin assumption. In this setting, we\nassume that all neurons (in all layers) in the network classify every ingoing\ninput from previous layer with margin $\\gamma$ bounded away from zero. In this\nmodel, we prove a mistake bound of approximately $(\\log Y)/ \\gamma^{O(L)}$,\nwhere L is the depth of the network.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5e26\u6709\u7b26\u53f7\u6fc0\u6d3b\u51fd\u6570\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u7684\u5728\u7ebf\u5b66\u4e60\u95ee\u9898\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5728\u5355\u4f4d\u7403\u5185\u7684\u8f93\u5165\u548c\u6709\u9650\u6807\u7b7e\u96c6\u4e0b\u7684\u5b66\u4e60\u80fd\u529b\u3002\u901a\u8fc7\u5f15\u5165\u8fb9\u754c\u6761\u4ef6\u548c\u591a\u5c42\u7f51\u7edc\u9650\u5236\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u9519\u8bef\u754c\u9650\u7684\u7406\u8bba\u5206\u6790\uff0c\u5e76\u63a2\u8ba8\u4e86\u7ef4\u5ea6\u4f9d\u8d56\u7684\u5fc5\u8981\u6027\u53ca\u5176\u7f13\u89e3\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u7406\u89e3\u5e26\u6709\u7b26\u53f7\u6fc0\u6d3b\u51fd\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u5728\u5728\u7ebf\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u8f93\u5165\u4e0b\u7684\u5b66\u4e60\u80fd\u529b\u53ca\u5176\u9650\u5236\u3002\u5e0c\u671b\u901a\u8fc7\u7406\u8bba\u5206\u6790\u4e3a\u795e\u7ecf\u7f51\u7edc\u5728\u7ebf\u5b66\u4e60\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u8fb9\u754c\u6761\u4ef6\uff08\u5404\u5c42\u795e\u7ecf\u5143\u7684\u5206\u7c7b\u8fb9\u754c\u9700\u5927\u4e8e\u96f6\uff09\u548c\u8fdb\u4e00\u6b65\u9650\u5236\u7f51\u7edc\u7ed3\u6784\uff08\u591a\u7d22\u5f15\u6a21\u578b\u548c\u6269\u5c55\u8fb9\u754c\u5047\u8bbe\uff09\uff0c\u5206\u6790\u5e76\u8bc1\u660e\u4e86\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7f51\u7edc\u5b66\u4e60\u7684\u6700\u4f18\u9519\u8bef\u754c\u9650\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e00\u822c\u6761\u4ef6\u4e0b\uff0c\u9519\u8bef\u754c\u9650\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u8f93\u5165\u7ef4\u5ea6 $d$\uff0c\u800c\u901a\u8fc7\u591a\u7d22\u5f15\u6a21\u578b\u6216\u6269\u5c55\u8fb9\u754c\u5047\u8bbe\u53ef\u4ee5\u7f13\u89e3\u8fd9\u79cd\u4f9d\u8d56\u6027\uff0c\u5206\u522b\u5b9e\u73b0\u4e0e $k \text{\uff08\u8fdc\u5c0f\u4e8e $d$\uff09\u548c $L \text{\uff08\u7f51\u7edc\u6df1\u5ea6\uff09\u76f8\u5173\u7684\u9519\u8bef\u754c\u9650\u3002", "conclusion": "\u8bba\u6587\u6307\u51fa\u5b8c\u5168\u907f\u514d\u7ef4\u5ea6\u4f9d\u8d56\u51e0\u4e4e\u4e0d\u53ef\u80fd\uff0c\u4f46\u901a\u8fc7\u5408\u7406\u9650\u5236\u7f51\u7edc\u7ed3\u6784\u53ef\u4ee5\u663e\u8457\u6539\u5584\u5b66\u4e60\u6027\u80fd\uff0c\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u7684\u5728\u7ebf\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2505.09435", "pdf": "https://arxiv.org/pdf/2505.09435", "abs": "https://arxiv.org/abs/2505.09435", "authors": ["Yili He", "Yan Zhu", "Peiyao Fu", "Ruijie Yang", "Tianyi Chen", "Zhihua Wang", "Quanlin Li", "Pinghong Zhou", "Xian Yang", "Shuo Wang"], "title": "Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records", "categories": ["cs.CV", "cs.AI"], "comment": "Early accepted to MICCAI 2025", "summary": "Pre-training on image-text colonoscopy records offers substantial potential\nfor improving endoscopic image analysis, but faces challenges including\nnon-informative background images, complex medical terminology, and ambiguous\nmulti-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised\nframework that enhances Contrastive Language-Image Pre-training (CLIP) for this\ndomain. Endo-CLIP's three-stage framework--cleansing, attunement, and\nunification--addresses these challenges by (1) removing background frames, (2)\nleveraging large language models to extract clinical attributes for\nfine-grained contrastive learning, and (3) employing patient-level\ncross-attention to resolve multi-polyp ambiguities. Extensive experiments\ndemonstrate that Endo-CLIP significantly outperforms state-of-the-art\npre-training methods in zero-shot and few-shot polyp detection and\nclassification, paving the way for more accurate and clinically relevant\nendoscopic analysis.", "AI": {"tldr": "Endo-CLIP\u662f\u4e00\u79cd\u9488\u5bf9\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u6790\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\uff08\u6e05\u6d01\u3001\u8c03\u6574\u3001\u7edf\u4e00\uff09\u5904\u7406\u80cc\u666f\u5e27\u3001\u4e34\u5e8a\u5c5e\u6027\u548c\u591a\u75c5\u53d8\u63cf\u8ff0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u68c0\u6d4b\u4e0e\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u6790\u9762\u4e34\u80cc\u666f\u5e72\u6270\u3001\u590d\u6742\u533b\u5b66\u672f\u8bed\u548c\u591a\u75c5\u53d8\u63cf\u8ff0\u6a21\u7cca\u7684\u6311\u6218\uff0c\u9700\u8981\u6539\u8fdb\u9884\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u63d0\u5347\u51c6\u786e\u6027\u3002", "method": "Endo-CLIP\u6846\u67b6\u5206\u4e09\u9636\u6bb5\uff1a\uff081\uff09\u53bb\u9664\u80cc\u666f\u5e27\uff0c\uff082\uff09\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u4e34\u5e8a\u5c5e\u6027\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5bf9\u6bd4\u5b66\u4e60\uff0c\uff083\uff09\u60a3\u8005\u7ea7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u591a\u75c5\u53d8\u6b67\u4e49\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEndo-CLIP\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u606f\u8089\u68c0\u6d4b\u4e0e\u5206\u7c7b\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Endo-CLIP\u4e3a\u5185\u7aa5\u955c\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u4e14\u4e34\u5e8a\u76f8\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2505.09229", "pdf": "https://arxiv.org/pdf/2505.09229", "abs": "https://arxiv.org/abs/2505.09229", "authors": ["Brian Britos", "Mathias Bourel"], "title": "Optimal Transport-Based Domain Adaptation for Rotated Linear Regression", "categories": ["stat.ML", "cs.LG", "math.PR"], "comment": null, "summary": "Optimal Transport (OT) has proven effective for domain adaptation (DA) by\naligning distributions across domains with differing statistical properties.\nBuilding on the approach of Courty et al. (2016), who mapped source data to the\ntarget domain for improved model transfer, we focus on a supervised DA problem\ninvolving linear regression models under rotational shifts. This ongoing work\nconsiders cases where source and target domains are related by a\nrotation-common in applications like sensor calibration or image orientation.\nWe show that in $\\mathbb{R}^2$ , when using a p-norm cost with $p $\\ge$ 2$, the\noptimal transport map recovers the underlying rotation. Based on this, we\npropose an algorithm that combines K-means clustering, OT, and singular value\ndecomposition (SVD) to estimate the rotation angle and adapt the regression\nmodel. This method is particularly effective when the target domain is sparsely\nsampled, leveraging abundant source data for improved generalization. Our\ncontributions offer both theoretical and practical insights into OT-based model\nadaptation under geometric transformations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408K\u5747\u503c\u805a\u7c7b\u3001\u6700\u4f18\u4f20\u8f93(OT)\u548c\u5947\u5f02\u503c\u5206\u89e3(SVD)\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u65cb\u8f6c\u89d2\u5ea6\u5e76\u8c03\u6574\u56de\u5f52\u6a21\u578b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u76ee\u6807\u57df\u6570\u636e\u7a00\u758f\u7684\u60c5\u51b5\u3002", "motivation": "\u7814\u7a76\u5728\u65cb\u8f6c\u5e73\u79fb\u7684\u76d1\u7763\u57df\u9002\u5e94\u95ee\u9898\u4e2d\uff0c\u5229\u7528\u6700\u4f18\u4f20\u8f93\u6062\u590d\u57fa\u7840\u65cb\u8f6c\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u8fc1\u79fb\u6548\u679c\u3002", "method": "\u4f7f\u7528p-norm\u6210\u672c\u51fd\u6570\uff08p\u22652\uff09\u5728\u211d\u00b2\u4e2d\u6062\u590d\u65cb\u8f6c\uff0c\u7ed3\u5408K\u5747\u503c\u3001OT\u548cSVD\u4f30\u8ba1\u65cb\u8f6c\u89d2\u5ea6\u5e76\u8c03\u6574\u56de\u5f52\u6a21\u578b\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u76ee\u6807\u57df\u6570\u636e\u7a00\u758f\u65f6\uff0c\u80fd\u6709\u6548\u5229\u7528\u6e90\u57df\u6570\u636e\u63d0\u5347\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u4e3a\u51e0\u4f55\u53d8\u6362\u4e0b\u57fa\u4e8eOT\u7684\u6a21\u578b\u9002\u5e94\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u89c1\u89e3\u3002"}}
{"id": "2505.09438", "pdf": "https://arxiv.org/pdf/2505.09438", "abs": "https://arxiv.org/abs/2505.09438", "authors": ["Paul Tschisgale", "Holger Maus", "Fabian Kieser", "Ben Kroehs", "Stefan Petersen", "Peter Wulff"], "title": "Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment", "categories": ["physics.ed-ph", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are now widely accessible, reaching learners at\nall educational levels. This development has raised concerns that their use may\ncircumvent essential learning processes and compromise the integrity of\nestablished assessment formats. In physics education, where problem solving\nplays a central role in instruction and assessment, it is therefore essential\nto understand the physics-specific problem-solving capabilities of LLMs. Such\nunderstanding is key to informing responsible and pedagogically sound\napproaches to integrating LLMs into instruction and assessment. This study\ntherefore compares the problem-solving performance of a general-purpose LLM\n(GPT-4o, using varying prompting techniques) and a reasoning-optimized model\n(o1-preview) with that of participants of the German Physics Olympiad, based on\na set of well-defined Olympiad problems. In addition to evaluating the\ncorrectness of the generated solutions, the study analyzes characteristic\nstrengths and limitations of LLM-generated solutions. The findings of this\nstudy indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate\nadvanced problem-solving capabilities on Olympiad-type physics problems, on\naverage outperforming the human participants. Prompting techniques had little\neffect on GPT-4o's performance, while o1-preview almost consistently\noutperformed both GPT-4o and the human benchmark. Based on these findings, the\nstudy discusses implications for the design of summative and formative\nassessment in physics education, including how to uphold assessment integrity\nand support students in critically engaging with LLMs.", "AI": {"tldr": "\u7814\u7a76\u4e86GPT-4o\u548co1-preview\u5728\u7269\u7406\u95ee\u9898\u89e3\u51b3\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5b83\u4eec\u4f18\u4e8e\u4eba\u7c7b\u53c2\u4e0e\u8005\uff0c\u5e76\u63a2\u8ba8\u4e86LLMs\u5728\u7269\u7406\u6559\u80b2\u4e2d\u7684\u8bc4\u4f30\u8bbe\u8ba1\u5f71\u54cd\u3002", "motivation": "\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u95ee\u9898\u89e3\u51b3\u4e0a\u7684\u80fd\u529b\uff0c\u4ee5\u6307\u5bfc\u5982\u4f55\u8d1f\u8d23\u4efb\u5730\u5c06\u5176\u6574\u5408\u5230\u6559\u5b66\u548c\u8bc4\u4f30\u4e2d\u3002", "method": "\u6bd4\u8f83\u4e86GPT-4o\u548co1-preview\u4e0e\u5fb7\u56fd\u7269\u7406\u5965\u6797\u5339\u514b\u53c2\u8d5b\u8005\u5728\u7279\u5b9a\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u4e86\u751f\u6210\u89e3\u51b3\u65b9\u6848\u7684\u7279\u70b9\u3002", "result": "\u4e24\u79cdLLMs\u5728\u7269\u7406\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u8d85\u8fc7\u4eba\u7c7b\u53c2\u4e0e\u8005\uff1bo1-preview\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7269\u7406\u6559\u80b2\u4e2d\u7684\u8bc4\u4f30\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53c2\u8003\uff0c\u5f3a\u8c03\u4e86\u7ef4\u62a4\u8bc4\u4f30\u5b8c\u6574\u6027\u548c\u5b66\u751f\u6279\u5224\u6027\u4f7f\u7528LLMs\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.09456", "pdf": "https://arxiv.org/pdf/2505.09456", "abs": "https://arxiv.org/abs/2505.09456", "authors": ["Josep Lumbreras", "Ruo Cheng Huang", "Yanglin Hu", "Mile Gu", "Marco Tomamichel"], "title": "Quantum state-agnostic work extraction (almost) without dissipation", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": "5 pages+14 pages, 2 figures", "summary": "We investigate work extraction protocols designed to transfer the maximum\npossible energy to a battery using sequential access to $N$ copies of an\nunknown pure qubit state. The core challenge is designing interactions to\noptimally balance two competing goals: charging of the battery optimally using\nthe qubit in hand, and acquiring more information by qubit to improve energy\nharvesting in subsequent rounds. Here, we leverage exploration-exploitation\ntrade-off in reinforcement learning to develop adaptive strategies achieving\nenergy dissipation that scales only poly-logarithmically in $N$. This\nrepresents an exponential improvement over current protocols based on full\nstate tomography.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316\u672a\u77e5\u7eaf\u91cf\u5b50\u6bd4\u7279\u6001\u7684\u80fd\u91cf\u63d0\u53d6\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\uff0c\u5b9e\u73b0\u4e86\u80fd\u91cf\u8017\u6563\u7684\u591a\u5bf9\u6570\u7ea7\u589e\u957f\u3002", "motivation": "\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u8bbe\u8ba1\u6700\u4f18\u7684\u80fd\u91cf\u63d0\u53d6\u534f\u8bae\uff0c\u4ee5\u5e73\u8861\u7535\u6c60\u5145\u7535\u548c\u83b7\u53d6\u66f4\u591a\u4fe1\u606f\u4ee5\u63d0\u9ad8\u540e\u7eed\u80fd\u91cf\u6536\u83b7\u95f4\u7684\u77db\u76fe\u3002", "method": "\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\u8bbe\u8ba1\u81ea\u9002\u5e94\u7b56\u7565\u3002", "result": "\u81ea\u9002\u5e94\u7b56\u7565\u5b9e\u73b0\u4e86\u80fd\u91cf\u8017\u6563\u4ec5\u968f$N$\u7684\u591a\u5bf9\u6570\u7ea7\u589e\u957f\uff0c\u76f8\u6bd4\u57fa\u4e8e\u5168\u6001\u5c42\u6790\u7684\u73b0\u6709\u534f\u8bae\u63d0\u5347\u4e86\u6307\u6570\u7ea7\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u91cf\u5b50\u6001\u80fd\u91cf\u63d0\u53d6\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6280\u672f\u3002"}}
{"id": "2505.09266", "pdf": "https://arxiv.org/pdf/2505.09266", "abs": "https://arxiv.org/abs/2505.09266", "authors": ["Lirand\u00eb Pira", "Airin Antony", "Nayanthara Prathap", "Daniel Peace", "Jacquiline Romero"], "title": "Enhanced Photonic Chip Design via Interpretable Machine Learning Techniques", "categories": ["physics.optics", "cs.LG", "quant-ph"], "comment": null, "summary": "Photonic chip design has seen significant advancements with the adoption of\ninverse design methodologies, offering flexibility and efficiency in optimizing\ndevice performance. However, the black-box nature of the optimization\napproaches, such as those used in inverse design in order to minimize a loss\nfunction or maximize coupling efficiency, poses challenges in understanding the\noutputs. This challenge is prevalent in machine learning-based optimization\nmethods, which can suffer from the same lack of transparency. To this end,\ninterpretability techniques address the opacity of optimization models. In this\nwork, we apply interpretability techniques from machine learning, with the aim\nof gaining understanding of inverse design optimization used in designing\nphotonic components, specifically two-mode multiplexers. We base our\nmethodology on the widespread interpretability technique known as local\ninterpretable model-agnostic explanations, or LIME. As a result, LIME-informed\ninsights point us to more effective initial conditions, directly improving\ndevice performance. This demonstrates that interpretability methods can do more\nthan explain models -- they can actively guide and enhance the inverse-designed\nphotonic components. Our results demonstrate the ability of interpretable\ntechniques to reveal underlying patterns in the inverse design process, leading\nto the development of better-performing components.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5982\u4f55\u5728\u5149\u5b50\u82af\u7247\u8bbe\u8ba1\u4e2d\u5e94\u7528\u673a\u5668\u5b66\u4e60\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff0c\u7279\u522b\u662fLIME\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u9006\u5411\u8bbe\u8ba1\u7684\u900f\u660e\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u9006\u5411\u8bbe\u8ba1\u5728\u5149\u5b50\u82af\u7247\u4f18\u5316\u4e2d\u867d\u9ad8\u6548\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4e2d\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u6280\u672f\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u63d0\u5347\u8bbe\u8ba1\u8fc7\u7a0b\u7684\u7406\u89e3\u4e0e\u6027\u80fd\u3002", "method": "\u91c7\u7528\u5c40\u90e8\u53ef\u89e3\u91ca\u6a21\u578b\u65e0\u5173\u89e3\u91ca\uff08LIME\uff09\u6280\u672f\uff0c\u5206\u6790\u9006\u5411\u8bbe\u8ba1\u4f18\u5316\u8fc7\u7a0b\uff0c\u4ee5\u6307\u5bfc\u5149\u5b50\u7ec4\u4ef6\uff08\u5982\u53cc\u6a21\u590d\u7528\u5668\uff09\u7684\u8bbe\u8ba1\u3002", "result": "LIME\u63d0\u4f9b\u7684\u6d1e\u5bdf\u5e2e\u52a9\u4f18\u5316\u521d\u59cb\u6761\u4ef6\uff0c\u76f4\u63a5\u63d0\u5347\u4e86\u5668\u4ef6\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u53ef\u89e3\u91ca\u6027\u6280\u672f\u4e0d\u4ec5\u80fd\u89e3\u91ca\u6a21\u578b\uff0c\u8fd8\u80fd\u4e3b\u52a8\u6539\u8fdb\u8bbe\u8ba1\u3002", "conclusion": "\u53ef\u89e3\u91ca\u6027\u6280\u672f\u63ed\u793a\u4e86\u9006\u5411\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u5728\u6a21\u5f0f\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u6027\u80fd\u66f4\u4f18\u7684\u5149\u5b50\u7ec4\u4ef6\uff0c\u63a8\u52a8\u4e86\u900f\u660e\u5316\u8bbe\u8ba1\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.09466", "pdf": "https://arxiv.org/pdf/2505.09466", "abs": "https://arxiv.org/abs/2505.09466", "authors": ["Xi Chen", "Shiyang Zhou", "Muqi Huang", "Jiaxu Feng", "Yun Xiong", "Kun Zhou", "Biao Yang", "Yuhui Zhang", "Huishuai Bao", "Sijia Peng", "Chuan Li", "Feng Shi"], "title": "A 2D Semantic-Aware Position Encoding for Vision Transformers", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 4 figures, 3 tables", "summary": "Vision transformers have demonstrated significant advantages in computer\nvision tasks due to their ability to capture long-range dependencies and\ncontextual relationships through self-attention. However, existing position\nencoding techniques, which are largely borrowed from natural language\nprocessing, fail to effectively capture semantic-aware positional relationships\nbetween image patches. Traditional approaches like absolute position encoding\nand relative position encoding primarily focus on 1D linear position\nrelationship, often neglecting the semantic similarity between distant yet\ncontextually related patches. These limitations hinder model generalization,\ntranslation equivariance, and the ability to effectively handle repetitive or\nstructured patterns in images. In this paper, we propose 2-Dimensional\nSemantic-Aware Position Encoding ($\\text{SaPE}^2$), a novel position encoding\nmethod with semantic awareness that dynamically adapts position representations\nby leveraging local content instead of fixed linear position relationship or\nspatial coordinates. Our method enhances the model's ability to generalize\nacross varying image resolutions and scales, improves translation equivariance,\nand better aggregates features for visually similar but spatially distant\npatches. By integrating $\\text{SaPE}^2$ into vision transformers, we bridge the\ngap between position encoding and perceptual similarity, thereby improving\nperformance on computer vision tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e862-D\u8bed\u4e49\u611f\u77e5\u4f4d\u7f6e\u7f16\u7801(SaPE\u00b2)\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u65e0\u6cd5\u6709\u6548\u6355\u6349\u8bed\u4e49\u4f4d\u7f6e\u5173\u7cfb\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf1D\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u65e0\u6cd5\u6709\u6548\u6355\u6349\u8bed\u4e49\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7ffb\u8bd1\u7b49\u53d8\u6027\u3002", "method": "\u63d0\u51faSaPE\u00b2\uff0c\u52a8\u6001\u8c03\u6574\u4f4d\u7f6e\u8868\u793a\uff0c\u5229\u7528\u5c40\u90e8\u5185\u5bb9\u800c\u975e\u56fa\u5b9a\u7ebf\u6027\u5173\u7cfb\u6216\u7a7a\u95f4\u5750\u6807\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u6a21\u578b\u8de8\u5206\u8fa8\u7387\u548c\u5c3a\u5ea6\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6539\u5584\u4e86\u7ffb\u8bd1\u7b49\u53d8\u6027\uff0c\u5e76\u66f4\u597d\u5730\u805a\u5408\u76f8\u4f3c\u4f46\u7a7a\u95f4\u9065\u8fdc\u7684\u7279\u5f81\u3002", "conclusion": "SaPE\u00b2\u901a\u8fc7\u7ed3\u5408\u4f4d\u7f6e\u7f16\u7801\u4e0e\u611f\u77e5\u76f8\u4f3c\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.09477", "pdf": "https://arxiv.org/pdf/2505.09477", "abs": "https://arxiv.org/abs/2505.09477", "authors": ["Zachary Ravichandran", "Fernando Cladera", "Jason Hughes", "Varun Murali", "M. Ani Hsieh", "George J. Pappas", "Camillo J. Taylor", "Vijay Kumar"], "title": "Deploying Foundation Model-Enabled Air and Ground Robots in the Field: Challenges and Opportunities", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted to the IEEE ICRA Workshop on Field Robotics 2025", "summary": "The integration of foundation models (FMs) into robotics has enabled robots\nto understand natural language and reason about the semantics in their\nenvironments. However, existing FM-enabled robots primary operate in\nclosed-world settings, where the robot is given a full prior map or has a full\nview of its workspace. This paper addresses the deployment of FM-enabled robots\nin the field, where missions often require a robot to operate in large-scale\nand unstructured environments. To effectively accomplish these missions, robots\nmust actively explore their environments, navigate obstacle-cluttered terrain,\nhandle unexpected sensor inputs, and operate with compute constraints. We\ndiscuss recent deployments of SPINE, our LLM-enabled autonomy framework, in\nfield robotic settings. To the best of our knowledge, we present the first\ndemonstration of large-scale LLM-enabled robot planning in unstructured\nenvironments with several kilometers of missions. SPINE is agnostic to a\nparticular LLM, which allows us to distill small language models capable of\nrunning onboard size, weight and power (SWaP) limited platforms. Via\npreliminary model distillation work, we then present the first language-driven\nUAV planner using on-device language models. We conclude our paper by proposing\nseveral promising directions for future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u5982\u4f55\u5c06\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u5728\u5f00\u653e\u3001\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u64cd\u4f5c\uff0c\u5e76\u63d0\u51fa\u4e86SPINE\u6846\u67b6\uff0c\u9996\u6b21\u5c55\u793a\u4e86\u5927\u89c4\u6a21LLM\u9a71\u52a8\u7684\u673a\u5668\u4eba\u89c4\u5212\u3002", "motivation": "\u73b0\u6709FM-enabled\u673a\u5668\u4eba\u4e3b\u8981\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u65e0\u6cd5\u5e94\u5bf9\u5927\u89c4\u6a21\u975e\u7ed3\u6784\u5316\u73af\u5883\u7684\u6311\u6218\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u673a\u5668\u4eba\u5728\u91ce\u5916\u4efb\u52a1\u4e2d\u7684\u63a2\u7d22\u3001\u5bfc\u822a\u548c\u8ba1\u7b97\u9650\u5236\u95ee\u9898\u3002", "method": "\u63d0\u51faSPINE\u6846\u67b6\uff0c\u5b83\u662f\u4e00\u79cdLLM-enabled\u7684\u81ea\u4e3b\u6027\u6846\u67b6\uff0c\u53ef\u9002\u7528\u4e8e\u4e0d\u540cLLM\uff0c\u5e76\u80fd\u591f\u84b8\u998f\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4ee5\u9002\u914d\u8d44\u6e90\u53d7\u9650\u7684\u5e73\u53f0\u3002", "result": "\u9996\u6b21\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u6570\u516c\u91cc\u89c4\u6a21\u7684\u4efb\u52a1\u89c4\u5212\uff0c\u5e76\u5f00\u53d1\u4e86\u9996\u4e2a\u57fa\u4e8e\u8bbe\u5907\u7aef\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u4eba\u673a\u89c4\u5212\u5668\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u5f53\u524d\u6210\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u8fdb\u4e00\u6b65\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2505.09304", "pdf": "https://arxiv.org/pdf/2505.09304", "abs": "https://arxiv.org/abs/2505.09304", "authors": ["Luciano Sebastian Martinez-Rau", "Quynh Nguyen Phuong Vu", "Yuxuan Zhang", "Bengt Oelmann", "Sebastian Bader"], "title": "Adaptive Noise Resilient Keyword Spotting Using One-Shot Learning", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "Preprint submitted to the IEEE 11th World Forum on Internet of Things", "summary": "Keyword spotting (KWS) is a key component of smart devices, enabling\nefficient and intuitive audio interaction. However, standard KWS systems\ndeployed on embedded devices often suffer performance degradation under\nreal-world operating conditions. Resilient KWS systems address this issue by\nenabling dynamic adaptation, with applications such as adding or replacing\nkeywords, adjusting to specific users, and improving noise robustness. However,\ndeploying resilient, standalone KWS systems with low latency on\nresource-constrained devices remains challenging due to limited memory and\ncomputational resources. This study proposes a low computational approach for\ncontinuous noise adaptation of pretrained neural networks used for KWS\nclassification, requiring only 1-shot learning and one epoch. The proposed\nmethod was assessed using two pretrained models and three real-world noise\nsources at signal-to-noise ratios (SNRs) ranging from 24 to -3 dB. The adapted\nmodels consistently outperformed the pretrained models across all scenarios,\nespecially at SNR $\\leq$ 18 dB, achieving accuracy improvements of 4.9% to\n46.0%. These results highlight the efficacy of the proposed methodology while\nbeing lightweight enough for deployment on resource-constrained devices.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f4e\u8ba1\u7b97\u91cf\u7684\u5173\u952e\u8bcd\u8bc6\u522b\uff08KWS\uff09\u65b9\u6cd5\uff0c\u4ec5\u9700\u5355\u6b21\u5b66\u4e60\u548c\u4e00\u4e2a\u5468\u671f\u5373\u53ef\u5b9e\u73b0\u8fde\u7eed\u566a\u58f0\u9002\u5e94\uff0c\u663e\u8457\u63d0\u5347\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6807\u51c6KWS\u7cfb\u7edf\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5c24\u5176\u9488\u5bf9\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u4f4e\u5ef6\u8fdf\u3001\u52a8\u6001\u9002\u5e94\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5355\u6b21\u5b66\u4e60\u548c\u4e00\u4e2a\u5468\u671f\u5bf9\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u8fde\u7eed\u566a\u58f0\u9002\u5e94\uff0c\u964d\u4f4e\u8ba1\u7b97\u8d1f\u62c5\u3002", "result": "\u5728\u591a\u79cd\u771f\u5b9e\u566a\u58f0\u548c\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\uff0c\u6539\u8fdb\u540e\u7684\u6a21\u578b\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5c24\u5176\u5728\u4f4e\u4fe1\u566a\u6bd4\uff08\u2264 18 dB\uff09\u65f6\u51c6\u786e\u7387\u63d0\u53474.9%\u81f346.0%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u8f7b\u91cf\uff0c\u9002\u5408\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\uff0c\u4e3a\u52a8\u6001KWS\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09306", "pdf": "https://arxiv.org/pdf/2505.09306", "abs": "https://arxiv.org/abs/2505.09306", "authors": ["Thijs L van der Plas", "Stephen Law", "Michael JO Pocock"], "title": "Predicting butterfly species presence from satellite imagery using soft contrastive regularisation", "categories": ["cs.CV", "cs.LG"], "comment": "To be published in the 2025 CVPR FGVC12 workshop", "summary": "The growing demand for scalable biodiversity monitoring methods has fuelled\ninterest in remote sensing data, due to its widespread availability and\nextensive coverage. Traditionally, the application of remote sensing to\nbiodiversity research has focused on mapping and monitoring habitats, but with\nincreasing availability of large-scale citizen-science wildlife observation\ndata, recent methods have started to explore predicting multi-species presence\ndirectly from satellite images. This paper presents a new data set for\npredicting butterfly species presence from satellite data in the United\nKingdom. We experimentally optimise a Resnet-based model to predict\nmulti-species presence from 4-band satellite images, and find that this model\nespecially outperforms the mean rate baseline for locations with high species\nbiodiversity. To improve performance, we develop a soft, supervised contrastive\nregularisation loss that is tailored to probabilistic labels (such as\nspecies-presence data), and demonstrate that this improves prediction accuracy.\nIn summary, our new data set and contrastive regularisation method contribute\nto the open challenge of accurately predicting species biodiversity from remote\nsensing data, which is key for efficient biodiversity monitoring.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6570\u636e\u96c6\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u536b\u661f\u56fe\u50cf\u76f4\u63a5\u9884\u6d4b\u82f1\u56fd\u8774\u8776\u7269\u79cd\u7684\u5b58\u5728\uff0c\u91c7\u7528Resnet\u6a21\u578b\u548c\u5bf9\u6bd4\u6b63\u5219\u5316\u635f\u5931\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u751f\u7269\u591a\u6837\u6027\u533a\u57df\u7684\u9884\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740\u5bf9\u53ef\u6269\u5c55\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u65b9\u6cd5\u7684\u9700\u6c42\u589e\u957f\uff0c\u9065\u611f\u6570\u636e\u56e0\u5176\u5e7f\u6cdb\u53ef\u7528\u6027\u548c\u8986\u76d6\u8303\u56f4\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\u3002\u4f20\u7edf\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6816\u606f\u5730\u6620\u5c04\uff0c\u4f46\u65b0\u65b9\u6cd5\u5c1d\u8bd5\u5229\u7528\u516c\u6c11\u79d1\u5b66\u6570\u636e\u76f4\u63a5\u4ece\u536b\u661f\u56fe\u50cf\u9884\u6d4b\u591a\u7269\u79cd\u5b58\u5728\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eResnet\u7684\u6a21\u578b\uff0c\u4ece4\u6ce2\u6bb5\u536b\u661f\u56fe\u50cf\u9884\u6d4b\u591a\u7269\u79cd\u5b58\u5728\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u6982\u7387\u6807\u7b7e\u7684\u8f6f\u76d1\u7763\u5bf9\u6bd4\u6b63\u5219\u5316\u635f\u5931\u51fd\u6570\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u9ad8\u751f\u7269\u591a\u6837\u6027\u533a\u57df\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u5bf9\u6bd4\u6b63\u5219\u5316\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u7387\u3002", "conclusion": "\u65b0\u6570\u636e\u96c6\u548c\u5bf9\u6bd4\u6b63\u5219\u5316\u65b9\u6cd5\u4e3a\u4ece\u9065\u611f\u6570\u636e\u51c6\u786e\u9884\u6d4b\u751f\u7269\u591a\u6837\u6027\u8fd9\u4e00\u5f00\u653e\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u9ad8\u6548\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2505.09498", "pdf": "https://arxiv.org/pdf/2505.09498", "abs": "https://arxiv.org/abs/2505.09498", "authors": ["Bo Zhang", "Shuo Li", "Runhe Tian", "Yang Yang", "Jixin Tang", "Jinhao Zhou", "Lin Ma"], "title": "Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 7 figures", "summary": "In this paper, we introduce Flash-VL 2B, a novel approach to optimizing\nVision-Language Models (VLMs) for real-time applications, targeting ultra-low\nlatency and high throughput without sacrificing accuracy. Leveraging advanced\narchitectural enhancements and efficient computational strategies, Flash-VL 2B\nis designed to maximize throughput by reducing processing time while\nmaintaining competitive performance across multiple vision-language benchmarks.\nOur approach includes tailored architectural choices, token compression\nmechanisms, data curation, training schemes, and a novel image processing\ntechnique called implicit semantic stitching that effectively balances\ncomputational load and model performance. Through extensive evaluations on 11\nstandard VLM benchmarks, we demonstrate that Flash-VL 2B achieves\nstate-of-the-art results in both speed and accuracy, making it a promising\nsolution for deployment in resource-constrained environments and large-scale\nreal-time applications.", "AI": {"tldr": "Flash-VL 2B\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u67b6\u6784\u589e\u5f3a\u548c\u8ba1\u7b97\u7b56\u7565\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u9ad8\u541e\u5410\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002", "motivation": "\u9488\u5bf9\u5b9e\u65f6\u5e94\u7528\u4e2d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4f4e\u5ef6\u8fdf\u9ad8\u541e\u5410\u9700\u6c42\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u6a21\u578b\u51c6\u786e\u7387\uff0c\u8bbe\u8ba1\u4e86Flash-VL 2B\u3002", "method": "\u91c7\u7528\u5b9a\u5236\u67b6\u6784\u3001\u4ee4\u724c\u538b\u7f29\u673a\u5236\u3001\u6570\u636e\u4f18\u5316\u3001\u8bad\u7ec3\u7b56\u7565\u53ca\u65b0\u9896\u7684\u56fe\u50cf\u5904\u7406\u6280\u672f\u201c\u9690\u5f0f\u8bed\u4e49\u62fc\u63a5\u201d\u4ee5\u5e73\u8861\u8ba1\u7b97\u8d1f\u8377\u548c\u6027\u80fd\u3002", "result": "\u572811\u4e2a\u6807\u51c6VLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlash-VL 2B\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "Flash-VL 2B\u662f\u8d44\u6e90\u53d7\u9650\u73af\u5883\u548c\u5927\u89c4\u6a21\u5b9e\u65f6\u5e94\u7528\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09313", "pdf": "https://arxiv.org/pdf/2505.09313", "abs": "https://arxiv.org/abs/2505.09313", "authors": ["Qiangqiang Liu", "Qian Huang", "Frank Fan", "Haishan Wu", "Xueyan Tang"], "title": "Detecting Sybil Addresses in Blockchain Airdrops: A Subgraph-based Feature Propagation and Fusion Approach", "categories": ["cs.CR", "cs.LG"], "comment": "IEEE International Conference on Blockchain and Cryptocurrency(Proc.\n  IEEE ICBC 2025)", "summary": "Sybil attacks pose a significant security threat to blockchain ecosystems,\nparticularly in token airdrop events. This paper proposes a novel sybil address\nidentification method based on subgraph feature extraction lightGBM. The method\nfirst constructs a two-layer deep transaction subgraph for each address, then\nextracts key event operation features according to the lifecycle of sybil\naddresses, including the time of first transaction, first gas acquisition,\nparticipation in airdrop activities, and last transaction. These temporal\nfeatures effectively capture the consistency of sybil address behavior\noperations. Additionally, the method extracts amount and network structure\nfeatures, comprehensively describing address behavior patterns and network\ntopology through feature propagation and fusion. Experiments conducted on a\ndataset containing 193,701 addresses (including 23,240 sybil addresses) show\nthat this method outperforms existing approaches in terms of precision, recall,\nF1 score, and AUC, with all metrics exceeding 0.9. The methods and results of\nthis study can be further applied to broader blockchain security areas such as\ntransaction manipulation identification and token liquidity risk assessment,\ncontributing to the construction of a more secure and fair blockchain\necosystem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b50\u56fe\u7279\u5f81\u63d0\u53d6\u7684\u8f7b\u91cf\u7ea7GBM\u7684Sybil\u5730\u5740\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u65f6\u95f4\u3001\u91d1\u989d\u548c\u7f51\u7edc\u7ed3\u6784\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u7cbe\u5ea6\uff0c\u6240\u6709\u6307\u6807\u8d85\u8fc70.9\u3002", "motivation": "Sybil\u653b\u51fb\u5bf9\u533a\u5757\u94fe\u751f\u6001\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u7279\u522b\u662f\u5728\u4ee3\u5e01\u7a7a\u6295\u6d3b\u52a8\u4e2d\uff0c\u4e9f\u9700\u9ad8\u6548\u8bc6\u522b\u65b9\u6cd5\u4ee5\u4fdd\u969c\u5b89\u5168\u548c\u516c\u5e73\u3002", "method": "\u6784\u5efa\u53cc\u5c42\u4ea4\u6613\u5b50\u56fe\uff0c\u63d0\u53d6Sybil\u5730\u5740\u751f\u547d\u5468\u671f\u7684\u5173\u952e\u64cd\u4f5c\u7279\u5f81\uff08\u5982\u9996\u6b21\u4ea4\u6613\u65f6\u95f4\u3001\u9996\u6b21\u83b7\u53d6Gas\u65f6\u95f4\u7b49\uff09\uff0c\u5e76\u7ed3\u5408\u91d1\u989d\u548c\u7f51\u7edc\u7ed3\u6784\u7279\u5f81\u8fdb\u884c\u7efc\u5408\u5206\u6790\u3002", "result": "\u5728\u5305\u542b193,701\u4e2a\u5730\u5740\uff08\u542b23,240\u4e2aSybil\u5730\u5740\uff09\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548cAUC\u5747\u8d85\u8fc70.9\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u81f3\u4ea4\u6613\u64cd\u7eb5\u8bc6\u522b\u548c\u6d41\u52a8\u6027\u98ce\u9669\u8bc4\u4f30\u7b49\u9886\u57df\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u5b89\u5168\u516c\u5e73\u7684\u533a\u5757\u94fe\u751f\u6001\u3002"}}
{"id": "2505.09558", "pdf": "https://arxiv.org/pdf/2505.09558", "abs": "https://arxiv.org/abs/2505.09558", "authors": ["Shengpeng Ji", "Tianle Liang", "Yangzhuo Li", "Jialong Zuo", "Minghui Fang", "Jinzheng He", "Yifu Chen", "Zhengqing Liu", "Ziyue Jiang", "Xize Cheng", "Siqi Zheng", "Jin Xu", "Junyang Lin", "Zhou Zhao"], "title": "WavReward: Spoken Dialogue Models With Generalist Reward Evaluators", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.MM", "cs.SD"], "comment": null, "summary": "End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered\nsignificant attention in the speech domain. However, the evaluation of spoken\ndialogue models' conversational performance has largely been overlooked. This\nis primarily due to the intelligent chatbots convey a wealth of non-textual\ninformation which cannot be easily measured using text-based language models\nlike ChatGPT. To address this gap, we propose WavReward, a reward feedback\nmodel based on audio language models that can evaluate both the IQ and EQ of\nspoken dialogue systems with speech input. Specifically, 1) based on audio\nlanguage models, WavReward incorporates the deep reasoning process and the\nnonlinear reward mechanism for post-training. By utilizing multi-sample\nfeedback via the reinforcement learning algorithm, we construct a specialized\nevaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a\npreference dataset used to train WavReward. ChatReward-30K includes both\ncomprehension and generation aspects of spoken dialogue models. These scenarios\nspan various tasks, such as text-based chats, nine acoustic attributes of\ninstruction chats, and implicit chats. WavReward outperforms previous\nstate-of-the-art evaluation models across multiple spoken dialogue scenarios,\nachieving a substantial improvement about Qwen2.5-Omni in objective accuracy\nfrom 55.1$\\%$ to 91.5$\\%$. In subjective A/B testing, WavReward also leads by a\nmargin of 83$\\%$. Comprehensive ablation studies confirm the necessity of each\ncomponent of WavReward. All data and code will be publicly at\nhttps://github.com/jishengpeng/WavReward after the paper is accepted.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faWavReward\uff0c\u4e00\u79cd\u57fa\u4e8e\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7684\u5956\u52b1\u53cd\u9988\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u7684IQ\u548cEQ\u8868\u73b0\u3002\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u6837\u672c\u53cd\u9988\uff0c\u7ed3\u5408ChatReward-30K\u6570\u636e\u96c6\uff0cWavReward\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8bc4\u4f30\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u5bf9\u8bdd\u6a21\u578b\u7684\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u65b9\u6cd5\uff0c\u65e0\u6cd5\u6355\u6349\u975e\u6587\u672c\u4fe1\u606f\u3002WavReward\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u8bed\u97f3\u5bf9\u8bdd\u6027\u80fd\u8bc4\u4f30\u3002", "method": "\u57fa\u4e8e\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408\u6df1\u5ea6\u63a8\u7406\u548c\u975e\u7ebf\u6027\u5956\u52b1\u673a\u5236\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u6837\u672c\u53cd\u9988\u6784\u5efa\u8bc4\u4f30\u5668\u3002\u4f7f\u7528ChatReward-30K\u6570\u636e\u96c6\uff08\u5305\u542b\u591a\u79cd\u5bf9\u8bdd\u4efb\u52a1\uff09\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "WavReward\u5728\u5ba2\u89c2\u51c6\u786e\u7387\u4e0a\u4ece55.1%\u63d0\u5347\u81f391.5%\uff0c\u4e3b\u89c2A/B\u6d4b\u8bd5\u4e2d\u4ee583%\u7684\u4f18\u52bf\u9886\u5148\uff0c\u5404\u7ec4\u4ef6\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u5fc5\u8981\u6027\u3002", "conclusion": "WavReward\u4e3a\u8bed\u97f3\u5bf9\u8bdd\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6570\u636e\u4e0e\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2505.09315", "pdf": "https://arxiv.org/pdf/2505.09315", "abs": "https://arxiv.org/abs/2505.09315", "authors": ["Xuefeng Jiang", "Yuan Ma", "Pengxiang Li", "Leimeng Xu", "Xin Wen", "Kun Zhan", "Zhongpu Xia", "Peng Jia", "XianPeng Lang", "Sheng Sun"], "title": "TransDiffuser: End-to-end Trajectory Generation with Decorrelated Multi-modal Representation for Autonomous Driving", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Under review", "summary": "In recent years, diffusion model has shown its potential across diverse\ndomains from vision generation to language modeling. Transferring its\ncapabilities to modern autonomous driving systems has also emerged as a\npromising direction.In this work, we propose TransDiffuser, an encoder-decoder\nbased generative trajectory planning model for end-to-end autonomous driving.\nThe encoded scene information serves as the multi-modal conditional input of\nthe denoising decoder. To tackle the mode collapse dilemma in generating\nhigh-quality diverse trajectories, we introduce a simple yet effective\nmulti-modal representation decorrelation optimization mechanism during the\ntraining process.TransDiffuser achieves PDMS of 94.85 on the NAVSIM benchmark,\nsurpassing previous state-of-the-art methods without any anchor-based prior\ntrajectories.", "AI": {"tldr": "TransDiffuser\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6761\u4ef6\u8f93\u5165\u548c\u8868\u793a\u53bb\u76f8\u5173\u4f18\u5316\u89e3\u51b3\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u5728NAVSIM\u57fa\u51c6\u4e0a\u8fbe\u523094.85\u7684PDMS\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u73b0\u4ee3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "method": "\u63d0\u51faTransDiffuser\uff0c\u4e00\u79cd\u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7684\u751f\u6210\u8f68\u8ff9\u89c4\u5212\u6a21\u578b\uff0c\u91c7\u7528\u591a\u6a21\u6001\u6761\u4ef6\u8f93\u5165\u548c\u8868\u793a\u53bb\u76f8\u5173\u4f18\u5316\u673a\u5236\u3002", "result": "\u5728NAVSIM\u57fa\u51c6\u4e0aPDMS\u8fbe\u523094.85\uff0c\u4f18\u4e8e\u4e4b\u524d\u7684\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "TransDiffuser\u901a\u8fc7\u4f18\u5316\u591a\u6a21\u6001\u8868\u793a\u6709\u6548\u89e3\u51b3\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09561", "pdf": "https://arxiv.org/pdf/2505.09561", "abs": "https://arxiv.org/abs/2505.09561", "authors": ["Marcel Torne", "Andy Tang", "Yuejiang Liu", "Chelsea Finn"], "title": "Learning Long-Context Diffusion Policies via Past-Token Prediction", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Videos are available at https://long-context-dp.github.io", "summary": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPast-Token Prediction (PTP)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u8fc7\u53bb\u52a8\u4f5c\u4ee4\u724c\u6765\u6539\u8fdb\u957f\u4e0a\u4e0b\u6587\u7b56\u7565\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5f15\u5165\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u6700\u7ec8\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u7b56\u7565\u5b66\u4e60\u9762\u4e34\u5185\u5b58\u9700\u6c42\u9ad8\u548c\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u622a\u65ad\u4e0a\u4e0b\u6587\u89c4\u907f\uff0c\u4f46\u53ef\u80fd\u4f1a\u4e22\u5931\u5173\u952e\u4fe1\u606f\u3002", "method": "\u63d0\u51faPTP\u4efb\u52a1\uff0c\u9884\u6d4b\u8fc7\u53bb\u548c\u672a\u6765\u52a8\u4f5c\u4ee4\u724c\u4ee5\u6539\u8fdb\u65f6\u5e8f\u5efa\u6a21\uff0c\u5e76\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u540e\u5fae\u8c03\u7b56\u7565\u5934\uff09\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57284\u4e2a\u771f\u5b9e\u4efb\u52a1\u548c6\u4e2a\u6a21\u62df\u4efb\u52a1\u4e2d\u5c06\u957f\u4e0a\u4e0b\u6587\u6269\u6563\u7b56\u7565\u6027\u80fd\u63d0\u53473\u500d\uff0c\u8bad\u7ec3\u901f\u5ea6\u52a0\u5feb10\u500d\u4ee5\u4e0a\u3002", "conclusion": "PTP\u53ca\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u7b56\u7565\u5b66\u4e60\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u3002"}}
{"id": "2505.09565", "pdf": "https://arxiv.org/pdf/2505.09565", "abs": "https://arxiv.org/abs/2505.09565", "authors": ["Maik Dannecker", "Thomas Sanchez", "Meritxell Bach Cuadra", "\u00d6zg\u00fcn Turgut", "Anthony N. Price", "Lucilio Cordero-Grande", "Vanessa Kyriakopoulou", "Joseph V. Hajnal", "Daniel Rueckert"], "title": "Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using Implicit Neural Representations", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 6 figures", "summary": "High-resolution slice-to-volume reconstruction (SVR) from multiple\nmotion-corrupted low-resolution 2D slices constitutes a critical step in\nimage-based diagnostics of moving subjects, such as fetal brain Magnetic\nResonance Imaging (MRI). Existing solutions struggle with image artifacts and\nsevere subject motion or require slice pre-alignment to achieve satisfying\nreconstruction performance. We propose a novel SVR method to enable fast and\naccurate MRI reconstruction even in cases of severe image and motion\ncorruption. Our approach performs motion correction, outlier handling, and\nsuper-resolution reconstruction with all operations being entirely based on\nimplicit neural representations. The model can be initialized with\ntask-specific priors through fully self-supervised meta-learning on either\nsimulated or real-world data. In extensive experiments including over 480\nreconstructions of simulated and clinical MRI brain data from different\ncenters, we prove the utility of our method in cases of severe subject motion\nand image artifacts. Our results demonstrate improvements in reconstruction\nquality, especially in the presence of severe motion, compared to\nstate-of-the-art methods, and up to 50% reduction in reconstruction time.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7684\u9ad8\u5206\u8fa8\u7387\u5207\u7247\u5230\u4f53\u79ef\u91cd\u5efa\uff08SVR\uff09\u65b9\u6cd5\uff0c\u80fd\u5feb\u901f\u51c6\u786e\u5730\u5904\u7406\u8fd0\u52a8\u4f2a\u5f71\u4e25\u91cd\u7684MRI\u6570\u636e\uff0c\u4e14\u65e0\u9700\u9884\u5bf9\u9f50\u5207\u7247\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u5176\u91cd\u5efa\u8d28\u91cf\u548c\u901f\u5ea6\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5207\u7247\u5230\u4f53\u79ef\u91cd\u5efa\u65b9\u6cd5\u5728\u5904\u7406\u8fd0\u52a8\u4f2a\u5f71\u4e25\u91cd\u7684\u4f4e\u5206\u8fa8\u73872D\u5207\u7247\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e38\u9700\u9884\u5bf9\u9f50\u5207\u7247\u4e14\u91cd\u5efa\u6548\u679c\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u65e0\u9700\u9884\u5bf9\u9f50\u3001\u80fd\u9ad8\u6548\u5904\u7406\u4e25\u91cd\u8fd0\u52a8\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff0c\u7ed3\u5408\u8fd0\u52a8\u77eb\u6b63\u3001\u79bb\u7fa4\u503c\u5904\u7406\u53ca\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u901a\u8fc7\u5168\u81ea\u76d1\u7763\u5143\u5b66\u4e60\u5229\u7528\u4efb\u52a1\u7279\u5b9a\u5148\u9a8c\u521d\u59cb\u5316\u6a21\u578b\u3002", "result": "\u5728480\u591a\u6b21\u6a21\u62df\u548c\u4e34\u5e8aMRI\u8111\u6570\u636e\u91cd\u5efa\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4e25\u91cd\u8fd0\u52a8\u4f2a\u5f71\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u7f29\u77ed50%\u91cd\u5efa\u65f6\u95f4\u3002", "conclusion": "\u8be5SVR\u65b9\u6cd5\u5728\u8fd0\u52a8\u4f2a\u5f71\u4e25\u91cd\u7684\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u8d8a\uff0c\u91cd\u5efa\u901f\u5ea6\u548c\u51c6\u786e\u7387\u5747\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.09326", "pdf": "https://arxiv.org/pdf/2505.09326", "abs": "https://arxiv.org/abs/2505.09326", "authors": ["Vincent Abbott", "Kotaro Kamiya", "Gerard Glowacki", "Yu Atsumi", "Gioele Zardini", "Yoshihiro Maruyama"], "title": "Accelerating Machine Learning Systems via Category Theory: Applications to Spherical Attention for Gene Regulatory Networks", "categories": ["math.CT", "cs.LG", "q-bio.MN"], "comment": null, "summary": "How do we enable artificial intelligence models to improve themselves? This\nis central to exponentially improving generalized artificial intelligence\nmodels, which can improve their own architecture to handle new problem domains\nin an efficient manner that leverages the latest hardware. However, current\nautomated compilation methods are poor, and efficient algorithms require years\nof human development. In this paper, we use neural circuit diagrams, based in\ncategory theory, to prove a general theorem related to deep learning\nalgorithms, guide the development of a novel attention algorithm catered to the\ndomain of gene regulatory networks, and produce a corresponding efficient\nkernel. The algorithm we propose, spherical attention, shows that neural\ncircuit diagrams enable a principled and systematic method for reasoning about\ndeep learning architectures and providing high-performance code. By replacing\nSoftMax with an $L^2$ norm as suggested by diagrams, it overcomes the special\nfunction unit bottleneck of standard attention while retaining the streaming\nproperty essential to high-performance. Our diagrammatically derived\n\\textit{FlashSign} kernel achieves comparable performance to the\nstate-of-the-art, fine-tuned FlashAttention algorithm on an A100, and\n$3.6\\times$ the performance of PyTorch. Overall, this investigation shows\nneural circuit diagrams' suitability as a high-level framework for the\nautomated development of efficient, novel artificial intelligence\narchitectures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7535\u8def\u56fe\u7684\u901a\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b\u6ce8\u610f\u529b\u7b97\u6cd5\uff08\u7403\u5f62\u6ce8\u610f\u529b\uff09\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u9ad8\u6548\u5185\u6838\u5b9e\u73b0\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u7535\u8def\u56fe\uff08\u57fa\u4e8e\u8303\u7574\u8bba\uff09\u63d0\u5347\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u7684\u81ea\u6211\u6539\u8fdb\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u81ea\u52a8\u7f16\u8bd1\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u548c\u7b97\u6cd5\u5f00\u53d1\u8017\u65f6\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u795e\u7ecf\u7535\u8def\u56fe\u63d0\u51fa\u7403\u5f62\u6ce8\u610f\u529b\u7b97\u6cd5\uff0c\u7528$L^2$\u8303\u6570\u66ff\u4ee3SoftMax\uff0c\u907f\u514d\u4e86\u6807\u51c6\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u9ad8\u6027\u80fd\u6240\u9700\u7684\u6d41\u5f0f\u7279\u6027\u3002", "result": "\u63d0\u51fa\u7684FlashSign\u5185\u6838\u5728A100\u4e0a\u6027\u80fd\u4e0eFlashAttention\u76f8\u5f53\uff0c\u4e14\u662fPyTorch\u76843.6\u500d\uff0c\u9a8c\u8bc1\u4e86\u795e\u7ecf\u7535\u8def\u56fe\u5728\u9ad8\u6548AI\u67b6\u6784\u5f00\u53d1\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u795e\u7ecf\u7535\u8def\u56fe\u662f\u4e00\u79cd\u6709\u6548\u7684\u6846\u67b6\uff0c\u53ef\u7528\u4e8e\u7cfb\u7edf\u5316\u5f00\u53d1\u9ad8\u6548\u3001\u521b\u65b0\u7684\u4eba\u5de5\u667a\u80fd\u67b6\u6784\u3002"}}
{"id": "2505.09568", "pdf": "https://arxiv.org/pdf/2505.09568", "abs": "https://arxiv.org/abs/2505.09568", "authors": ["Jiuhai Chen", "Zhiyang Xu", "Xichen Pan", "Yushi Hu", "Can Qin", "Tom Goldstein", "Lifu Huang", "Tianyi Zhou", "Saining Xie", "Silvio Savarese", "Le Xue", "Caiming Xiong", "Ran Xu"], "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unifying image understanding and generation has gained growing attention in\nrecent research on multimodal models. Although design choices for image\nunderstanding have been extensively studied, the optimal model architecture and\ntraining recipe for a unified framework with image generation remain\nunderexplored. Motivated by the strong potential of autoregressive and\ndiffusion models for high-quality generation and scalability, we conduct a\ncomprehensive study of their use in unified multimodal settings, with emphasis\non image representations, modeling objectives, and training strategies.\nGrounded in these investigations, we introduce a novel approach that employs a\ndiffusion transformer to generate semantically rich CLIP image features, in\ncontrast to conventional VAE-based representations. This design yields both\nhigher training efficiency and improved generative quality. Furthermore, we\ndemonstrate that a sequential pretraining strategy for unified models-first\ntraining on image understanding and subsequently on image generation-offers\npractical advantages by preserving image understanding capability while\ndeveloping strong image generation ability. Finally, we carefully curate a\nhigh-quality instruction-tuning dataset BLIP3o-60k for image generation by\nprompting GPT-4o with a diverse set of captions covering various scenes,\nobjects, human gestures, and more. Building on our innovative model design,\ntraining recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art\nunified multimodal models. BLIP3-o achieves superior performance across most of\nthe popular benchmarks spanning both image understanding and generation tasks.\nTo facilitate future research, we fully open-source our models, including code,\nmodel weights, training scripts, and pretraining and instruction tuning\ndatasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u6a21\u578bBLIP3-o\uff0c\u7ed3\u5408\u4e86\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6269\u6563Transformer\u751f\u6210CLIP\u56fe\u50cf\u7279\u5f81\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u3002\u91c7\u7528\u5206\u9636\u6bb5\u9884\u8bad\u7ec3\u7b56\u7565\uff08\u5148\u7406\u89e3\u540e\u751f\u6210\uff09\u548c\u9ad8\u8d28\u91cf\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6BLIP3o-60k\uff0cBLIP3-o\u5728\u56fe\u50cf\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5f00\u6e90\u4e86\u76f8\u5173\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u7edf\u4e00\u56fe\u50cf\u7406\u89e3\u4e0e\u751f\u6210\u7684\u6846\u67b6\u4e2d\uff0c\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u4ecd\u7f3a\u4e4f\u6df1\u5165\u63a2\u7d22\u3002\u672c\u6587\u57fa\u4e8e\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u65e8\u5728\u63a2\u7d22\u5176\u5728\u591a\u6a21\u6001\u7edf\u4e00\u6846\u67b6\u4e2d\u7684\u6700\u4f73\u5b9e\u8df5\u3002", "method": "\u91c7\u7528\u6269\u6563Transformer\u751f\u6210CLIP\u56fe\u50cf\u7279\u5f81\uff0c\u53d6\u4ee3\u4f20\u7edfVAE\u8868\u793a\uff1b\u5206\u9636\u6bb5\u9884\u8bad\u7ec3\uff08\u5148\u56fe\u50cf\u7406\u89e3\u540e\u751f\u6210\uff09\uff1b\u6784\u5efa\u9ad8\u8d28\u91cf\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6BLIP3o-60k\u3002", "result": "\u63d0\u51fa\u7684BLIP3-o\u6a21\u578b\u5728\u4e3b\u6d41\u56fe\u50cf\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u8bad\u7ec3\u6548\u7387\u66f4\u9ad8\u4e14\u751f\u6210\u8d28\u91cf\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u521b\u65b0\u7684\u6a21\u578b\u8bbe\u8ba1\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u6570\u636e\u96c6\uff0cBLIP3-o\u5b9e\u73b0\u4e86\u56fe\u50cf\u7406\u89e3\u4e0e\u751f\u6210\u7684\u7edf\u4e00\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.09576", "pdf": "https://arxiv.org/pdf/2505.09576", "abs": "https://arxiv.org/abs/2505.09576", "authors": ["Shannon Lodoen", "Alexi Orchard"], "title": "Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach", "categories": ["cs.CY", "cs.AI"], "comment": "10 pages, 1 figure, Accepted version", "summary": "Since 2022, versions of generative AI chatbots such as ChatGPT and Claude\nhave been trained using a specialized technique called Reinforcement Learning\nfrom Human Feedback (RLHF) to fine-tune language model output using feedback\nfrom human annotators. As a result, the integration of RLHF has greatly\nenhanced the outputs of these large language models (LLMs) and made the\ninteractions and responses appear more \"human-like\" than those of previous\nversions using only supervised learning. The increasing convergence of human\nand machine-written text has potentially severe ethical, sociotechnical, and\npedagogical implications relating to transparency, trust, bias, and\ninterpersonal relations. To highlight these implications, this paper presents a\nrhetorical analysis of some of the central procedures and processes currently\nbeing reshaped by RLHF-enhanced generative AI chatbots: upholding language\nconventions, information seeking practices, and expectations for social\nrelationships. Rhetorical investigations of generative AI and LLMs have, to\nthis point, focused largely on the persuasiveness of the content generated.\nUsing Ian Bogost's concept of procedural rhetoric, this paper shifts the site\nof rhetorical investigation from content analysis to the underlying mechanisms\nof persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical\ninvestigation opens a new direction for further inquiry in AI ethics that\nconsiders how procedures rerouted through AI-driven technologies might\nreinforce hegemonic language use, perpetuate biases, decontextualize learning,\nand encroach upon human relationships. It will therefore be of interest to\neducators, researchers, scholars, and the growing number of users of generative\nAI chatbots.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4eba\u7c7b\u53cd\u9988\uff08RLHF\uff09\u4f18\u5316\u7684\u751f\u6210\u5f0fAI\u804a\u5929\u673a\u5668\u4eba\u5bf9\u4eba\u7c7b\u8bed\u8a00\u4e60\u60ef\u3001\u4fe1\u606f\u83b7\u53d6\u53ca\u793e\u4ea4\u5173\u7cfb\u7684\u6f5c\u5728\u4f26\u7406\u548c\u793e\u4f1a\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63ed\u793aRLHF\u6280\u672f\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u4f7f\u5176\u66f4\u201c\u4eba\u6027\u5316\u201d\uff0c\u4ee5\u53ca\u8fd9\u79cd\u6280\u672f\u5bf9\u900f\u660e\u5ea6\u3001\u4fe1\u4efb\u3001\u504f\u89c1\u548c\u4eba\u9645\u5173\u7cfb\u7684\u6df1\u8fdc\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4fee\u8f9e\u5b66\u5206\u6790\uff08\u5c24\u5176\u662fIan Bogost\u7684\u7a0b\u5e8f\u4fee\u8f9e\u7406\u8bba\uff09\uff0c\u805a\u7126RLHF\u6280\u672f\u5982\u4f55\u91cd\u5851\u8bed\u8a00\u60ef\u4f8b\u3001\u4fe1\u606f\u5bfb\u6c42\u884c\u4e3a\u548c\u793e\u4f1a\u5173\u7cfb\u671f\u671b\u3002", "result": "\u7814\u7a76\u53d1\u73b0RLHF\u53ef\u80fd\u5f3a\u5316\u9738\u6743\u8bed\u8a00\u4f7f\u7528\u3001\u5ef6\u7eed\u504f\u89c1\u3001\u4f7f\u5b66\u4e60\u53bb\u8bed\u5883\u5316\uff0c\u5e76\u4fb5\u8680\u4eba\u9645\u5173\u7cfb\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u8fd9\u4e00\u7406\u8bba\u63a2\u7d22\u4e3aAI\u4f26\u7406\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u547c\u5401\u5173\u6ce8AI\u6280\u672f\u5982\u4f55\u901a\u8fc7\u7a0b\u5e8f\u673a\u5236\u5f71\u54cd\u793e\u4f1a\u7ed3\u6784\u548c\u4eba\u7c7b\u4e92\u52a8\u3002"}}
{"id": "2505.09358", "pdf": "https://arxiv.org/pdf/2505.09358", "abs": "https://arxiv.org/abs/2505.09358", "authors": ["Bingxin Ke", "Kevin Qu", "Tianfu Wang", "Nando Metzger", "Shengyu Huang", "Bo Li", "Anton Obukhov", "Konrad Schindler"], "title": "Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis", "categories": ["cs.CV", "cs.LG"], "comment": "Journal extension of our CVPR 2024 paper, featuring new tasks,\n  improved efficiency, high-resolution capabilities, and enhanced accessibility", "summary": "The success of deep learning in computer vision over the past decade has\nhinged on large labeled datasets and strong pretrained models. In data-scarce\nsettings, the quality of these pretrained models becomes crucial for effective\ntransfer learning. Image classification and self-supervised learning have\ntraditionally been the primary methods for pretraining CNNs and\ntransformer-based architectures. Recently, the rise of text-to-image generative\nmodels, particularly those using denoising diffusion in a latent space, has\nintroduced a new class of foundational models trained on massive, captioned\nimage datasets. These models' ability to generate realistic images of unseen\ncontent suggests they possess a deep understanding of the visual world. In this\nwork, we present Marigold, a family of conditional generative models and a\nfine-tuning protocol that extracts the knowledge from pretrained latent\ndiffusion models like Stable Diffusion and adapts them for dense image analysis\ntasks, including monocular depth estimation, surface normals prediction, and\nintrinsic decomposition. Marigold requires minimal modification of the\npre-trained latent diffusion model's architecture, trains with small synthetic\ndatasets on a single GPU over a few days, and demonstrates state-of-the-art\nzero-shot generalization. Project page:\nhttps://marigoldcomputervision.github.io", "AI": {"tldr": "Marigold\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u5bc6\u96c6\u56fe\u50cf\u5206\u6790\u4efb\u52a1\uff08\u5982\u6df1\u5ea6\u4f30\u8ba1\u3001\u8868\u9762\u6cd5\u7ebf\u9884\u6d4b\u7b49\uff09\uff0c\u5728\u5c0f\u89c4\u6a21\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u548c\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4f46\u6570\u636e\u7a00\u7f3a\u65f6\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8d28\u91cf\u5c24\u4e3a\u5173\u952e\u3002\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff08\u5982\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff09\u5c55\u73b0\u51fa\u5bf9\u89c6\u89c9\u4e16\u754c\u7684\u6df1\u523b\u7406\u89e3\uff0c\u5982\u4f55\u5c06\u5176\u77e5\u8bc6\u8fc1\u79fb\u5230\u5bc6\u96c6\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002", "method": "\u63d0\u51faMarigold\uff0c\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\uff0c\u901a\u8fc7\u5fae\u8c03\u534f\u8bae\u63d0\u53d6\u5176\u77e5\u8bc6\uff0c\u9002\u5e94\u5bc6\u96c6\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u3002\u65b9\u6cd5\u4ec5\u9700\u6700\u5c0f\u5316\u4fee\u6539\u6a21\u578b\u67b6\u6784\uff0c\u5229\u7528\u5c0f\u89c4\u6a21\u5408\u6210\u6570\u636e\u5728\u5355GPU\u4e0a\u8bad\u7ec3\u51e0\u5929\u5373\u53ef\u3002", "result": "Marigold\u5728\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3001\u8868\u9762\u6cd5\u7ebf\u9884\u6d4b\u548c\u672c\u5f81\u5206\u89e3\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Marigold\u5c55\u793a\u4e86\u4ece\u751f\u6210\u6a21\u578b\u4e2d\u63d0\u53d6\u77e5\u8bc6\u5e76\u8fc1\u79fb\u5230\u5bc6\u96c6\u89c6\u89c9\u4efb\u52a1\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09591", "pdf": "https://arxiv.org/pdf/2505.09591", "abs": "https://arxiv.org/abs/2505.09591", "authors": ["Tobias Jan Wieczorek", "Nathalie Daun", "Mohammad Emtiyaz Khan", "Marcus Rohrbach"], "title": "Variational Visual Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 16 figures, under review at ICCV 2025", "summary": "Despite remarkable progress in multimodal models for Visual Question\nAnswering (VQA), there remain major reliability concerns because the models can\noften be overconfident and miscalibrated, especially in out-of-distribution\n(OOD) settings. Plenty has been done to address such issues for unimodal\nmodels, but little work exists for multimodal cases. Here, we address\nunreliability in multimodal models by proposing a Variational VQA approach.\nSpecifically, instead of fine-tuning vision-language models by using AdamW, we\nemploy a recently proposed variational algorithm called IVON, which yields a\nposterior distribution over model parameters. Through extensive experiments, we\nshow that our approach improves calibration and abstentions without sacrificing\nthe accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce\nExpected Calibration Error by more than 50% compared to the AdamW baseline and\nraise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of\ndistribution shifts, the performance gain is even higher, achieving 8% Coverage\n(@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we\npresent variational learning as a viable option to enhance the reliability of\nmultimodal models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u53d8\u5206VQA\u65b9\u6cd5\uff0c\u901a\u8fc7IVON\u7b97\u6cd5\u6539\u8fdb\u591a\u6a21\u6001\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u663e\u8457\u964d\u4f4e\u6821\u51c6\u8bef\u5dee\u5e76\u63d0\u9ad8\u8986\u76d6\u7387\uff0c\u5c24\u5176\u5728\u5206\u5e03\u504f\u79fb\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u591a\u6a21\u6001\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4e2d\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u504f\u79fb\uff08OOD\uff09\u65f6\u5bb9\u6613\u8fc7\u5ea6\u81ea\u4fe1\u548c\u6821\u51c6\u9519\u8bef\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u6a21\u6001\u6a21\u578b\uff0c\u591a\u6a21\u6001\u573a\u666f\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u91c7\u7528\u53d8\u5206\u7b97\u6cd5IVON\u4ee3\u66ff\u4f20\u7edfAdamW\u4f18\u5316\uff0c\u751f\u6210\u6a21\u578b\u53c2\u6570\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u6821\u51c6\u80fd\u529b\u548c\u5f03\u6743\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4AdamW\uff0c\u53d8\u5206\u65b9\u6cd5\u5c06\u9884\u671f\u6821\u51c6\u8bef\u5dee\u964d\u4f4e50%\u4ee5\u4e0a\uff0c\u8986\u76d6\u7387\u63d0\u53474%\uff08\u98ce\u9669\u56fa\u5b9a\u4e3a1%\uff09\uff0c\u5728OOD\u60c5\u51b5\u4e0b\u8986\u76d6\u7387\u63d0\u5347\u8fbe8%\u3002", "conclusion": "\u53d8\u5206\u5b66\u4e60\u662f\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u53ef\u9760\u6027\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5206\u5e03\u504f\u79fb\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2505.09364", "pdf": "https://arxiv.org/pdf/2505.09364", "abs": "https://arxiv.org/abs/2505.09364", "authors": ["Michael Benigni", "Maurizio Ferrari Dacrema", "Dietmar Jannach"], "title": "Diffusion Recommender Models and the Illusion of Progress: A Concerning Study of Reproducibility and a Conceptual Mismatch", "categories": ["cs.IR", "cs.LG", "cs.NE"], "comment": null, "summary": "Countless new machine learning models are published every year and are\nreported to significantly advance the state-of-the-art in \\emph{top-n}\nrecommendation. However, earlier reproducibility studies indicate that progress\nin this area may be quite limited. Specifically, various widespread\nmethodological issues, e.g., comparisons with untuned baseline models, have led\nto an \\emph{illusion of progress}. In this work, our goal is to examine whether\nthese problems persist in today's research. To this end, we aim to reproduce\nthe latest advancements reported from applying modern Denoising Diffusion\nProbabilistic Models to recommender systems, focusing on four models published\nat the top-ranked SIGIR conference in 2023 and 2024. Our findings are\nconcerning, revealing persistent methodological problems. Alarmingly, through\nexperiments, we find that the latest recommendation techniques based on\ndiffusion models, despite their computational complexity and substantial carbon\nfootprint, are consistently outperformed by simpler existing models.\nFurthermore, we identify key mismatches between the characteristics of\ndiffusion models and those of the traditional \\emph{top-n} recommendation task,\nraising doubts about their suitability for recommendation. We also note that,\nin the papers we analyze, the generative capabilities of these models are\nconstrained to a minimum. Overall, our results and continued methodological\nissues call for greater scientific rigor and a disruptive change in the\nresearch and publication culture in this area.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u6700\u65b0\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u63a8\u8350\u6280\u672f\u5c3d\u7ba1\u8ba1\u7b97\u590d\u6742\u4e14\u78b3\u8db3\u8ff9\u5927\uff0c\u4f46\u6027\u80fd\u4ecd\u4e0d\u5982\u7b80\u5355\u6a21\u578b\uff0c\u4e14\u5b58\u5728\u65b9\u6cd5\u8bba\u95ee\u9898\u3002", "motivation": "\u63a2\u8ba8\u5f53\u524d\u7814\u7a76\u4e2d\u5173\u4e8e\u6269\u6563\u6a21\u578b\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u53ca\u65b9\u6cd5\u8bba\u95ee\u9898\u662f\u5426\u4f9d\u65e7\u5b58\u5728\u3002", "method": "\u901a\u8fc7\u590d\u73b02023-2024\u5e74SIGIR\u4f1a\u8bae\u4e0a\u7684\u56db\u4e2a\u6269\u6563\u6a21\u578b\u8bba\u6587\u7684\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u5176\u6027\u80fd\u548c\u65b9\u6cd5\u8bba\u3002", "result": "\u6269\u6563\u6a21\u578b\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5b58\u5728\u65b9\u6cd5\u8bba\u7f3a\u9677\uff0c\u5982\u4e0e\u672a\u8c03\u4f18\u57fa\u7ebf\u7684\u6bd4\u8f83\u95ee\u9898\u3002", "conclusion": "\u547c\u5401\u52a0\u5f3a\u79d1\u5b66\u4e25\u8c28\u6027\u5e76\u6539\u53d8\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u51fa\u7248\u6587\u5316\u3002"}}
{"id": "2505.09365", "pdf": "https://arxiv.org/pdf/2505.09365", "abs": "https://arxiv.org/abs/2505.09365", "authors": ["H. T. R\u00fcdisser", "G. Nguyen", "J. Le Lou\u00ebdec", "C. M\u00f6stl"], "title": "ARCANE -- Early Detection of Interplanetary Coronal Mass Ejections", "categories": ["physics.space-ph", "astro-ph.IM", "astro-ph.SR", "cs.LG"], "comment": "25 pages, 9 figures, 1 table, submitted to AGU Space Weather on 14th\n  May 2025", "summary": "Interplanetary coronal mass ejections (ICMEs) are major drivers of space\nweather disturbances, posing risks to both technological infrastructure and\nhuman activities. Automatic detection of ICMEs in solar wind in situ data is\nessential for early warning systems. While several methods have been proposed\nto identify these structures in time series data, robust real-time detection\nremains a significant challenge. In this work, we present ARCANE - the first\nframework explicitly designed for early ICME detection in streaming solar wind\ndata under realistic operational constraints, enabling event identification\nwithout requiring observation of the full structure. Our approach evaluates the\nstrengths and limitations of detection models by comparing a machine\nlearning-based method to a threshold-based baseline. The ResUNet++ model,\npreviously validated on science data, significantly outperforms the baseline,\nparticularly in detecting high-impact events, while retaining solid performance\non lower-impact cases. Notably, we find that using real-time solar wind (RTSW)\ndata instead of high-resolution science data leads to only minimal performance\ndegradation. Despite the challenges of operational settings, our detection\npipeline achieves an F1 score of 0.53, with an average detection delay of 21.5%\nof the event's duration while only seeing a minimal amount of data. As more\ndata becomes available, the performance increases significantly. These results\nmark a substantial step forward in automated space weather monitoring and lay\nthe groundwork for enhanced real-time forecasting capabilities.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86ARCANE\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u592a\u9633\u98ce\u6570\u636e\u4e2d\u7684\u884c\u661f\u9645\u65e5\u5195\u7269\u8d28\u629b\u5c04\uff08ICMEs\uff09\uff0c\u901a\u8fc7\u6bd4\u8f83\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u9608\u503c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u53d1\u73b0ResUNet++\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u5728\u5b9e\u65f6\u6570\u636e\u4e2d\u8868\u73b0\u63a5\u8fd1\u79d1\u5b66\u6570\u636e\u3002", "motivation": "ICMEs\u662f\u7a7a\u95f4\u5929\u6c14\u6270\u52a8\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff0c\u5bf9\u6280\u672f\u548c\u4eba\u7c7b\u6d3b\u52a8\u6784\u6210\u98ce\u9669\u3002\u73b0\u6709\u7684\u5b9e\u65f6\u68c0\u6d4b\u65b9\u6cd5\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u5728\u4e0d\u5b8c\u6574\u89c2\u6d4b\u4e0b\u8bc6\u522b\u4e8b\u4ef6\u7684\u6846\u67b6\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u4e86ARCANE\u6846\u67b6\uff0c\u91c7\u7528ResUNet++\u6a21\u578b\u4e0e\u9608\u503c\u57fa\u7ebf\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u8bc4\u4f30\u4e24\u79cd\u65b9\u6cd5\u5728\u5b9e\u65f6\u592a\u9633\u98ce\u6570\u636e\uff08RTSW\uff09\u4e0e\u9ad8\u5206\u8fa8\u7387\u79d1\u5b66\u6570\u636e\u4e2d\u7684\u8868\u73b0\u3002", "result": "ResUNet++\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u9608\u503c\u57fa\u7ebf\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u5f71\u54cd\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u3002\u4f7f\u7528RTSW\u6570\u636e\u4ec5\u5bfc\u81f4\u8f7b\u5fae\u6027\u80fd\u4e0b\u964d\u3002\u68c0\u6d4b\u6d41\u6c34\u7ebf\u7684F1\u5f97\u5206\u4e3a0.53\uff0c\u5e73\u5747\u5ef6\u8fdf\u4e3a\u4e8b\u4ef6\u65f6\u957f\u768421.5%\u3002", "conclusion": "ARCANE\u6846\u67b6\u5728\u81ea\u52a8\u5316\u7a7a\u95f4\u5929\u6c14\u76d1\u6d4b\u4e2d\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u4e3a\u5b9e\u65f6\u9884\u62a5\u80fd\u529b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.09598", "pdf": "https://arxiv.org/pdf/2505.09598", "abs": "https://arxiv.org/abs/2505.09598", "authors": ["Nidhal Jegham", "Marwen Abdelatti", "Lassad Elmoubarki", "Abdeltawab Hendawi"], "title": "How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) spread across industries, understanding their\nenvironmental footprint at the inference level is no longer optional; it is\nessential. However, most existing studies exclude proprietary models, overlook\ninfrastructural variability and overhead, or focus solely on training, even as\ninference increasingly dominates AI's environmental impact. To bridge this gap,\nthis paper introduces a novel infrastructure-aware benchmarking framework for\nquantifying the environmental footprint of LLM inference across 30\nstate-of-the-art models as deployed in commercial data centers. Our framework\ncombines public API performance data with region-specific environmental\nmultipliers and statistical inference of hardware configurations. We\nadditionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank\nmodels by performance relative to environmental cost. Our results show that o3\nand DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33\nWh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and\nthat Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short\nGPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results\nin substantial annual environmental impacts. These include electricity use\ncomparable to 35,000 U.S. homes, freshwater evaporation matching the annual\ndrinking needs of 1.2 million people, and carbon emissions requiring a\nChicago-sized forest to offset. These findings illustrate a growing paradox:\nalthough individual queries are efficient, their global scale drives\ndisproportionate resource consumption. Our study provides a standardized,\nempirically grounded methodology for benchmarking the sustainability of LLM\ndeployments, laying a foundation for future environmental accountability in AI\ndevelopment and sustainability standards.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u7840\u8bbe\u65bd\u611f\u77e5\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u5546\u4e1a\u6570\u636e\u4e2d\u5fc3\u90e8\u7f72\u768430\u79cd\u6700\u5148\u8fdbLLM\u63a8\u7406\u7684\u73af\u5883\u8db3\u8ff9\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u95f4\u663e\u8457\u7684\u80fd\u8017\u5dee\u5f02\u548c\u89c4\u6a21\u5316\u67e5\u8be2\u7684\u6f5c\u5728\u73af\u5883\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8eLLM\u5728\u5404\u884c\u4e1a\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7406\u89e3\u5176\u5728\u63a8\u7406\u9636\u6bb5\u7684\u73af\u5883\u8db3\u8ff9\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5ffd\u89c6\u4e13\u6709\u6a21\u578b\u3001\u57fa\u7840\u8bbe\u65bd\u5dee\u5f02\uff0c\u6216\u4ec5\u5173\u6ce8\u8bad\u7ec3\u9636\u6bb5\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u516c\u5171API\u6027\u80fd\u6570\u636e\u3001\u533a\u57df\u7279\u5b9a\u73af\u5883\u4e58\u6570\u548c\u786c\u4ef6\u914d\u7f6e\u7edf\u8ba1\u63a8\u65ad\u7684\u6846\u67b6\uff0c\u5e76\u91c7\u7528\u4ea4\u53c9\u6548\u7387\u6570\u636e\u5305\u7edc\u5206\u6790\uff08DEA\uff09\u5bf9\u6a21\u578b\u8fdb\u884c\u6027\u80fd\u4e0e\u73af\u5883\u6210\u672c\u7684\u6392\u540d\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0co3\u548cDeepSeek-R1\u662f\u80fd\u8017\u6700\u9ad8\u7684\u6a21\u578b\uff0c\u800cClaude-3.7 Sonnet\u5728\u751f\u6001\u6548\u7387\u4e0a\u8868\u73b0\u6700\u4f73\u3002\u89c4\u6a21\u5316\u67e5\u8be2\uff08\u5982\u6bcf\u65e57\u4ebf\u6b21\uff09\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u80fd\u6e90\u3001\u6de1\u6c34\u548c\u78b3\u6392\u653e\u95ee\u9898\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aLLM\u90e8\u7f72\u7684\u53ef\u6301\u7eed\u6027\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u3001\u5b9e\u8bc1\u652f\u6301\u7684\u65b9\u6cd5\u8bba\uff0c\u4e3a\u672a\u6765AI\u5f00\u53d1\u4e2d\u7684\u73af\u5883\u8d23\u4efb\u548c\u53ef\u6301\u7eed\u6027\u6807\u51c6\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.09368", "pdf": "https://arxiv.org/pdf/2505.09368", "abs": "https://arxiv.org/abs/2505.09368", "authors": ["Jenny Schmalfuss", "Victor Oei", "Lukas Mehl", "Madlen Bartsch", "Shashank Agnihotri", "Margret Keuper", "Andr\u00e9s Bruhn"], "title": "RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Standard benchmarks for optical flow, scene flow, and stereo vision\nalgorithms generally focus on model accuracy rather than robustness to image\ncorruptions like noise or rain. Hence, the resilience of models to such\nreal-world perturbations is largely unquantified. To address this, we present\nRobustSpring, a comprehensive dataset and benchmark for evaluating robustness\nto image corruptions for optical flow, scene flow, and stereo models.\nRobustSpring applies 20 different image corruptions, including noise, blur,\ncolor changes, quality degradations, and weather distortions, in a time-,\nstereo-, and depth-consistent manner to the high-resolution Spring dataset,\ncreating a suite of 20,000 corrupted images that reflect challenging\nconditions. RobustSpring enables comparisons of model robustness via a new\ncorruption robustness metric. Integration with the Spring benchmark enables\npublic two-axis evaluations of both accuracy and robustness. We benchmark a\ncurated selection of initial models, observing that accurate models are not\nnecessarily robust and that robustness varies widely by corruption type.\nRobustSpring is a new computer vision benchmark that treats robustness as a\nfirst-class citizen to foster models that combine accuracy with resilience. It\nwill be available at https://spring-benchmark.org.", "AI": {"tldr": "RobustSpring\u662f\u4e00\u4e2a\u65b0\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5149\u5b66\u6d41\u3001\u573a\u666f\u6d41\u548c\u7acb\u4f53\u6a21\u578b\u5bf9\u56fe\u50cf\u635f\u574f\u7684\u9c81\u68d2\u6027\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u6a21\u578b\u5bf9\u566a\u58f0\u6216\u96e8\u7b49\u73b0\u5b9e\u4e16\u754c\u6270\u52a8\u7684\u9c81\u68d2\u6027\u91cf\u5316\u65b9\u9762\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u51c6\u786e\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u5bf9\u56fe\u50cf\u635f\u574f\uff08\u5982\u566a\u58f0\u6216\u96e8\uff09\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u3002RobustSpring\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5728\u9ad8\u5206\u8fa8\u7387Spring\u6570\u636e\u96c6\u4e0a\u5e94\u752820\u79cd\u4e0d\u540c\u7684\u56fe\u50cf\u635f\u574f\uff08\u5982\u566a\u58f0\u3001\u6a21\u7cca\u3001\u989c\u8272\u53d8\u5316\u7b49\uff09\uff0c\u4ee5\u65f6\u95f4\u3001\u7acb\u4f53\u548c\u6df1\u5ea6\u4e00\u81f4\u7684\u65b9\u5f0f\u751f\u621020,000\u5f20\u635f\u574f\u56fe\u50cf\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0\uff0c\u51c6\u786e\u6027\u9ad8\u7684\u6a21\u578b\u4e0d\u4e00\u5b9a\u9c81\u68d2\uff0c\u4e14\u9c81\u68d2\u6027\u56e0\u635f\u574f\u7c7b\u578b\u800c\u5f02\u3002", "conclusion": "RobustSpring\u5c06\u9c81\u68d2\u6027\u4f5c\u4e3a\u9996\u8981\u76ee\u6807\uff0c\u65e8\u5728\u63a8\u52a8\u517c\u5177\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u7684\u6a21\u578b\u53d1\u5c55\u3002"}}
{"id": "2505.09425", "pdf": "https://arxiv.org/pdf/2505.09425", "abs": "https://arxiv.org/abs/2505.09425", "authors": ["Sarah Leyder", "Jakob Raymaekers", "Peter J. Rousseeuw", "Tom Van Deuren", "Tim Verdonck"], "title": "Independent Component Analysis by Robust Distance Correlation", "categories": ["stat.CO", "cs.LG"], "comment": null, "summary": "Independent component analysis (ICA) is a powerful tool for decomposing a\nmultivariate signal or distribution into fully independent sources, not just\nuncorrelated ones. Unfortunately, most approaches to ICA are not robust against\noutliers. Here we propose a robust ICA method called RICA, which estimates the\ncomponents by minimizing a robust measure of dependence between multivariate\nrandom variables. The dependence measure used is the distance correlation\n(dCor). In order to make it more robust we first apply a new transformation\ncalled the bowl transform, which is bounded, one-to-one, continuous, and maps\nfar outliers to points close to the origin. This preserves the crucial property\nthat a zero dCor implies independence. RICA estimates the independent sources\nsequentially, by looking for the component that has the smallest dCor with the\nremainder. RICA is strongly consistent and has the usual parametric rate of\nconvergence. Its robustness is investigated by a simulation study, in which it\ngenerally outperforms its competitors. The method is illustrated on three\napplications, including the well-known cocktail party problem.", "AI": {"tldr": "\u63d0\u51fa\u4e86RICA\u8fd9\u4e00\u9c81\u68d2\u7684\u72ec\u7acb\u6210\u5206\u5206\u6790\uff08ICA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u591a\u5143\u968f\u673a\u53d8\u91cf\u4e4b\u95f4\u7684\u9c81\u68d2\u4f9d\u8d56\u5ea6\u91cf\uff08dCor\uff09\u6765\u4f30\u8ba1\u72ec\u7acb\u6210\u5206\uff0c\u5e76\u901a\u8fc7bowl\u53d8\u6362\u589e\u5f3a\u5176\u9c81\u68d2\u6027\u3002RICA\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edfICA\u65b9\u6cd5\u5bf9\u5f02\u5e38\u503c\u4e0d\u591f\u9c81\u68d2\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86RICA\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u9c81\u68d2\u4f9d\u8d56\u5ea6\u91cf\u548cbowl\u53d8\u6362\u63d0\u5347ICA\u5728\u5f02\u5e38\u503c\u5b58\u5728\u65f6\u7684\u7a33\u5b9a\u6027\u3002", "method": "RICA\u901a\u8fc7\u6700\u5c0f\u5316\u591a\u5143\u968f\u673a\u53d8\u91cf\u4e4b\u95f4\u7684\u8ddd\u79bb\u76f8\u5173\u6027\uff08dCor\uff09\u6765\u4f30\u8ba1\u72ec\u7acb\u6210\u5206\uff0c\u5e76\u4f7f\u7528bowl\u53d8\u6362\u4f7fdCor\u5bf9\u5f02\u5e38\u503c\u66f4\u9c81\u68d2\u3002RICA\u987a\u5e8f\u4f30\u8ba1\u72ec\u7acb\u6210\u5206\uff0c\u6bcf\u6b21\u5bfb\u627e\u4e0e\u5269\u4f59\u90e8\u5206dCor\u6700\u5c0f\u7684\u6210\u5206\u3002", "result": "RICA\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5177\u6709\u5f3a\u4e00\u81f4\u6027\u548c\u901a\u5e38\u7684\u53c2\u6570\u6536\u655b\u901f\u5ea6\u3002\u5728\u4e09\u4e2a\u5b9e\u9645\u5e94\u7528\uff08\u5305\u62ec\u8457\u540d\u7684\u9e21\u5c3e\u9152\u4f1a\u95ee\u9898\uff09\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "RICA\u662f\u4e00\u79cd\u9c81\u68d2\u4e14\u6709\u6548\u7684ICA\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5b58\u5728\u5f02\u5e38\u503c\u7684\u60c5\u51b5\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.09430", "pdf": "https://arxiv.org/pdf/2505.09430", "abs": "https://arxiv.org/abs/2505.09430", "authors": ["Yutong Hu", "Pinhao Song", "Kehan Wen", "Renaud Detry"], "title": "Train a Multi-Task Diffusion Policy on RLBench-18 in One Day with One GPU", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "We present a method for training multi-task vision-language robotic diffusion\npolicies that reduces training time and memory usage by an order of magnitude.\nThis improvement arises from a previously underexplored distinction between\naction diffusion and the image diffusion techniques that inspired it: image\ngeneration targets are high-dimensional, while robot actions lie in a much\nlower-dimensional space. Meanwhile, the vision-language conditions for action\ngeneration remain high-dimensional. Our approach, Mini-Diffuser, exploits this\nasymmetry by introducing Level-2 minibatching, which pairs multiple noised\naction samples with each vision-language condition, instead of the conventional\none-to-one sampling strategy. To support this batching scheme, we introduce\narchitectural adaptations to the diffusion transformer that prevent information\nleakage across samples while maintaining full conditioning access. In RLBench\nsimulations, Mini-Diffuser achieves 95\\% of the performance of state-of-the-art\nmulti-task diffusion policies, while using only 5\\% of the training time and\n7\\% of the memory. Real-world experiments further validate that Mini-Diffuser\npreserves the key strengths of diffusion-based policies, including the ability\nto model multimodal action distributions and produce behavior conditioned on\ndiverse perceptual inputs. Code available at\ngithub.com/utomm/mini-diffuse-actor.", "AI": {"tldr": "Mini-Diffuser\u901a\u8fc7Level-2 minibatching\u548c\u67b6\u6784\u6539\u8fdb\uff0c\u663e\u8457\u51cf\u5c11\u591a\u4efb\u52a1\u89c6\u89c9\u8bed\u8a00\u673a\u5668\u4eba\u6269\u6563\u7b56\u7565\u7684\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u5360\u7528\uff0c\u6027\u80fd\u63a5\u8fd1\u5f53\u524d\u6700\u4f73\u65b9\u6cd5\u4f46\u4ec5\u97005%\u7684\u8bad\u7ec3\u65f6\u95f4\u548c7%\u7684\u5185\u5b58\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u6269\u6563\u6280\u672f\u76f4\u63a5\u8fc1\u79fb\u5230\u673a\u5668\u4eba\u52a8\u4f5c\u751f\u6210\u65f6\uff0c\u56e0\u52a8\u4f5c\u7a7a\u95f4\u7ef4\u5ea6\u8fdc\u4f4e\u4e8e\u56fe\u50cf\u7a7a\u95f4\uff0c\u5bfc\u81f4\u8bad\u7ec3\u8d44\u6e90\u6d6a\u8d39\u3002Mini-Diffuser\u65e8\u5728\u5229\u7528\u8fd9\u79cd\u4e0d\u5bf9\u79f0\u6027\u4f18\u5316\u6548\u7387\u3002", "method": "\u63d0\u51faLevel-2 minibatching\u7b56\u7565\uff0c\u5c06\u591a\u4e2a\u566a\u58f0\u52a8\u4f5c\u6837\u672c\u4e0e\u5355\u4e2a\u89c6\u89c9\u8bed\u8a00\u6761\u4ef6\u914d\u5bf9\uff08\u800c\u975e\u4f20\u7edf\u4e00\u5bf9\u4e00\uff09\uff1b\u540c\u65f6\u6539\u8fdb\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u4ee5\u9632\u6b62\u8de8\u6837\u672c\u4fe1\u606f\u6cc4\u9732\u3002", "result": "\u5728RLBench\u4eff\u771f\u4e2d\uff0c\u8fbe\u5230SOTA\u65b9\u6cd595%\u7684\u6027\u80fd\uff0c\u4f46\u8bad\u7ec3\u65f6\u95f4\u4ec5\u97005%\uff0c\u5185\u5b58\u5360\u7528\u4ec57%\u3002\u5b9e\u7269\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5bf9\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u548c\u591a\u6837\u5316\u611f\u77e5\u8f93\u5165\u7684\u652f\u6301\u3002", "conclusion": "Mini-Diffuser\u901a\u8fc7\u6761\u4ef6-\u52a8\u4f5c\u7ef4\u5ea6\u4e0d\u5bf9\u79f0\u6027\u521b\u65b0\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u673a\u5668\u4eba\u7b56\u7565\u8bad\u7ec3\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u53ef\u884c\u6027\u3002"}}
{"id": "2505.09471", "pdf": "https://arxiv.org/pdf/2505.09471", "abs": "https://arxiv.org/abs/2505.09471", "authors": ["Xiaoyu Hu", "Gengyu Xue", "Zhenhua Lin", "Yi Yu"], "title": "Fairness-aware Bayes optimal functional classification", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "comment": null, "summary": "Algorithmic fairness has become a central topic in machine learning, and\nmitigating disparities across different subpopulations has emerged as a rapidly\ngrowing research area. In this paper, we systematically study the\nclassification of functional data under fairness constraints, ensuring the\ndisparity level of the classifier is controlled below a pre-specified\nthreshold. We propose a unified framework for fairness-aware functional\nclassification, tackling an infinite-dimensional functional space, addressing\nkey challenges from the absence of density ratios and intractability of\nposterior probabilities, and discussing unique phenomena in functional\nclassification. We further design a post-processing algorithm, Fair Functional\nLinear Discriminant Analysis classifier (Fair-FLDA), which targets at\nhomoscedastic Gaussian processes and achieves fairness via group-wise\nthresholding. Under weak structural assumptions on eigenspace, theoretical\nguarantees on fairness and excess risk controls are established. As a\nbyproduct, our results cover the excess risk control of the standard FLDA as a\nspecial case, which, to the best of our knowledge, is first time seen. Our\ntheoretical findings are complemented by extensive numerical experiments on\nsynthetic and real datasets, highlighting the practicality of our designed\nalgorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u516c\u5e73\u6027\u7ea6\u675f\u4e0b\u7684\u51fd\u6570\u6570\u636e\u5206\u7c7b\u7edf\u4e00\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86Fair-FLDA\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u7ec4\u9608\u503c\u5b9e\u73b0\u516c\u5e73\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u7b97\u6cd5\u516c\u5e73\u6027\u65e5\u76ca\u91cd\u8981\uff0c\u9700\u8981\u51cf\u5c11\u4e0d\u540c\u5b50\u7fa4\u4f53\u95f4\u7684\u5dee\u5f02\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u51fd\u6570\u6570\u636e\u5206\u7c7b\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u51fd\u6570\u7a7a\u95f4\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u8bbe\u8ba1\u4e86Fair-FLDA\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u7ec4\u9608\u503c\u5b9e\u73b0\u516c\u5e73\u6027\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u7b97\u6cd5\u5728\u516c\u5e73\u6027\u548c\u8d85\u989d\u98ce\u9669\u63a7\u5236\u4e0a\u7684\u4fdd\u969c\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "Fair-FLDA\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u51fd\u6570\u6570\u636e\u5206\u7c7b\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09496", "pdf": "https://arxiv.org/pdf/2505.09496", "abs": "https://arxiv.org/abs/2505.09496", "authors": ["Rui Miao", "Babak Shahbaba", "Annie Qu"], "title": "Reinforcement Learning for Individual Optimal Policy from Heterogeneous Data", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Offline reinforcement learning (RL) aims to find optimal policies in dynamic\nenvironments in order to maximize the expected total rewards by leveraging\npre-collected data. Learning from heterogeneous data is one of the fundamental\nchallenges in offline RL. Traditional methods focus on learning an optimal\npolicy for all individuals with pre-collected data from a single episode or\nhomogeneous batch episodes, and thus, may result in a suboptimal policy for a\nheterogeneous population. In this paper, we propose an individualized offline\npolicy optimization framework for heterogeneous time-stationary Markov decision\nprocesses (MDPs). The proposed heterogeneous model with individual latent\nvariables enables us to efficiently estimate the individual Q-functions, and\nour Penalized Pessimistic Personalized Policy Learning (P4L) algorithm\nguarantees a fast rate on the average regret under a weak partial coverage\nassumption on behavior policies. In addition, our simulation studies and a real\ndata application demonstrate the superior numerical performance of the proposed\nmethod compared with existing methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2a\u6027\u5316\u79bb\u7ebf\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff08P4L\u7b97\u6cd5\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u5f02\u6784\u6570\u636e\u73af\u5883\u4e2d\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u4e2a\u4f53\u6f5c\u5728\u53d8\u91cf\u5efa\u6a21\u548c\u5b66\u4e60\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u5f02\u6784\u6570\u636e\u65f6\u53ef\u80fd\u4ea7\u751f\u6b21\u4f18\u7b56\u7565\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e2a\u6027\u5316\u5efa\u6a21\u4f18\u5316\u7b56\u7565\u5b66\u4e60\u6548\u679c\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e2a\u6027\u5316\u6f5c\u5728\u53d8\u91cf\u7684\u5f02\u6784MDP\u6a21\u578b\uff0c\u5f00\u53d1P4L\u7b97\u6cd5\uff0c\u7ed3\u5408\u60e9\u7f5a\u60b2\u89c2\u7b56\u7565\u5b66\u4e60\u548c\u5f31\u90e8\u5206\u8986\u76d6\u5047\u8bbe\uff0c\u5feb\u901f\u4f30\u8ba1\u4e2a\u4f53Q\u51fd\u6570\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86P4L\u5728\u5e73\u5747\u9057\u61be\u4e0a\u7684\u5feb\u901f\u6536\u655b\u6027\uff0c\u4eff\u771f\u548c\u5b9e\u9645\u6570\u636e\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6570\u503c\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f02\u6784\u6570\u636e\u4e0b\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.09506", "pdf": "https://arxiv.org/pdf/2505.09506", "abs": "https://arxiv.org/abs/2505.09506", "authors": ["Mar\u00eda Alejandra Hern\u00e1ndez", "Oscar Rodriguez", "Dae-Jin Lee"], "title": "Deep-SITAR: A SITAR-Based Deep Learning Framework for Growth Curve Modeling via Autoencoders", "categories": ["stat.ML", "cs.LG", "F.2.2; I.2.7"], "comment": "Pre-print", "summary": "Several approaches have been developed to capture the complexity and\nnonlinearity of human growth. One widely used is the Super Imposition by\nTranslation and Rotation (SITAR) model, which has become popular in studies of\nadolescent growth. SITAR is a shape-invariant mixed-effects model that\nrepresents the shared growth pattern of a population using a natural cubic\nspline mean curve while incorporating three subject-specific random effects --\ntiming, size, and growth intensity -- to account for variations among\nindividuals. In this work, we introduce a supervised deep learning framework\nbased on an autoencoder architecture that integrates a deep neural network\n(neural network) with a B-spline model to estimate the SITAR model. In this\napproach, the encoder estimates the random effects for each individual, while\nthe decoder performs a fitting based on B-splines similar to the classic SITAR\nmodel. We refer to this method as the Deep-SITAR model. This innovative\napproach enables the prediction of the random effects of new individuals\nentering a population without requiring a full model re-estimation. As a\nresult, Deep-SITAR offers a powerful approach to predicting growth\ntrajectories, combining the flexibility and efficiency of deep learning with\nthe interpretability of traditional mixed-effects models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7ed3\u6784\u7684Deep-SITAR\u6a21\u578b\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548cB\u6837\u6761\u6a21\u578b\u6765\u4f30\u8ba1SITAR\u6a21\u578b\uff0c\u4ece\u800c\u9884\u6d4b\u4e2a\u4f53\u7684\u751f\u957f\u8f68\u8ff9\u3002", "motivation": "\u4e3a\u4e86\u66f4\u9ad8\u6548\u5730\u9884\u6d4b\u4e2a\u4f53\u751f\u957f\u8f68\u8ff9\uff0c\u540c\u65f6\u4fdd\u6301\u4f20\u7edf\u6df7\u5408\u6548\u5e94\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528\u81ea\u7f16\u7801\u5668\u67b6\u6784\uff0c\u7f16\u7801\u5668\u4f30\u8ba1\u6bcf\u4e2a\u4e2a\u4f53\u7684\u968f\u673a\u6548\u5e94\uff0c\u89e3\u7801\u5668\u57fa\u4e8eB\u6837\u6761\u8fdb\u884c\u62df\u5408\u3002", "result": "Deep-SITAR\u80fd\u591f\u9884\u6d4b\u65b0\u4e2a\u4f53\u7684\u968f\u673a\u6548\u5e94\uff0c\u65e0\u9700\u91cd\u65b0\u4f30\u8ba1\u6574\u4e2a\u6a21\u578b\u3002", "conclusion": "Deep-SITAR\u7ed3\u5408\u4e86\u6df1\u5ea6\u5b66\u4e60\u7684\u7075\u6d3b\u6027\u548c\u4f20\u7edf\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u751f\u957f\u8f68\u8ff9\u9884\u6d4b\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\u3002"}}
{"id": "2505.09516", "pdf": "https://arxiv.org/pdf/2505.09516", "abs": "https://arxiv.org/abs/2505.09516", "authors": ["Siyi Wang", "Alexandre Leblanc", "Paul D. McNicholas"], "title": "Depth-Based Local Center Clustering: A Framework for Handling Different Clustering Scenarios", "categories": ["stat.ME", "cs.LG", "stat.AP"], "comment": null, "summary": "Cluster analysis, or clustering, plays a crucial role across numerous\nscientific and engineering domains. Despite the wealth of clustering methods\nproposed over the past decades, each method is typically designed for specific\nscenarios and presents certain limitations in practical applications. In this\npaper, we propose depth-based local center clustering (DLCC). This novel method\nmakes use of data depth, which is known to produce a center-outward ordering of\nsample points in a multivariate space. However, data depth typically fails to\ncapture the multimodal characteristics of {data}, something of the utmost\nimportance in the context of clustering. To overcome this, DLCC makes use of a\nlocal version of data depth that is based on subsets of {data}. From this,\nlocal centers can be identified as well as clusters of varying shapes.\nFurthermore, we propose a new internal metric based on density-based clustering\nto evaluate clustering performance on {non-convex clusters}. Overall, DLCC is a\nflexible clustering approach that seems to overcome some limitations of\ntraditional clustering methods, thereby enhancing data analysis capabilities\nacross a wide range of application scenarios.", "AI": {"tldr": "DLCC\u662f\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u6df1\u5ea6\u7684\u5c40\u90e8\u4e2d\u5fc3\u805a\u7c7b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u6570\u636e\uff0c\u80fd\u8bc6\u522b\u4e0d\u540c\u5f62\u72b6\u7684\u7c07\uff0c\u5e76\u901a\u8fc7\u65b0\u7684\u5185\u90e8\u6307\u6807\u8bc4\u4f30\u975e\u51f8\u7c07\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u573a\u666f\u8bbe\u8ba1\uff0c\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5bf9\u591a\u6a21\u6001\u548c\u975e\u51f8\u7c07\u7684\u5904\u7406\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6570\u636e\u6df1\u5ea6\u7684\u5c40\u90e8\u4e2d\u5fc3\u805a\u7c7b\uff08DLCC\uff09\uff0c\u5229\u7528\u5c40\u90e8\u6570\u636e\u6df1\u5ea6\u8bc6\u522b\u5c40\u90e8\u4e2d\u5fc3\u548c\u591a\u5f62\u72b6\u7c07\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u5bc6\u5ea6\u6307\u6807\u8bc4\u4f30\u6027\u80fd\u3002", "result": "DLCC\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u548c\u975e\u51f8\u7c07\u7684\u5206\u6790\u80fd\u529b\u3002", "conclusion": "DLCC\u662f\u4e00\u79cd\u7075\u6d3b\u4e14\u6709\u6548\u7684\u805a\u7c7b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2505.09529", "pdf": "https://arxiv.org/pdf/2505.09529", "abs": "https://arxiv.org/abs/2505.09529", "authors": ["Mohamed Moustafa", "Joseph Lemley", "Peter Corcoran"], "title": "Contactless Cardiac Pulse Monitoring Using Event Cameras", "categories": ["cs.CV", "cs.ET", "cs.LG", "eess.IV"], "comment": "This paper is a preprint of a paper submitted to IEEE Access and is\n  currently under review", "summary": "Time event cameras are a novel technology for recording scene information at\nextremely low latency and with low power consumption. Event cameras output a\nstream of events that encapsulate pixel-level light intensity changes within\nthe scene, capturing information with a higher dynamic range and temporal\nresolution than traditional cameras. This study investigates the contact-free\nreconstruction of an individual's cardiac pulse signal from time event\nrecording of their face using a supervised convolutional neural network (CNN)\nmodel. An end-to-end model is trained to extract the cardiac signal from a\ntwo-dimensional representation of the event stream, with model performance\nevaluated based on the accuracy of the calculated heart rate. The experimental\nresults confirm that physiological cardiac information in the facial region is\neffectively preserved within the event stream, showcasing the potential of this\nnovel sensor for remote heart rate monitoring. The model trained on event\nframes achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm)\ncompared to the RMSE of 2.92 bpm achieved by the baseline model trained on\nstandard camera frames. Furthermore, models trained on event frames generated\nat 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an\nRMSE of 2.54 and 2.13 bpm, respectively.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u65f6\u95f4\u4e8b\u4ef6\u76f8\u673a\u548cCNN\u6a21\u578b\u65e0\u63a5\u89e6\u91cd\u5efa\u4e2a\u4f53\u5fc3\u810f\u8109\u640f\u4fe1\u53f7\uff0c\u5c55\u793a\u4e86\u8be5\u6280\u672f\u5728\u8fdc\u7a0b\u5fc3\u7387\u76d1\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u63a2\u7d22\u65f6\u95f4\u4e8b\u4ef6\u76f8\u673a\u5728\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u529f\u8017\u6761\u4ef6\u4e0b\u6355\u6349\u9762\u90e8\u533a\u57df\u751f\u7406\u4fe1\u606f\u7684\u6f5c\u529b\uff0c\u5b9e\u73b0\u65e0\u63a5\u89e6\u5fc3\u7387\u76d1\u6d4b\u3002", "method": "\u4f7f\u7528\u76d1\u7763\u5b66\u4e60\u7684CNN\u6a21\u578b\u4ece\u4e8b\u4ef6\u6d41\u4e8c\u7ef4\u8868\u793a\u4e2d\u63d0\u53d6\u5fc3\u810f\u4fe1\u53f7\uff0c\u8bc4\u4f30\u6a21\u578b\u8ba1\u7b97\u5fc3\u7387\u7684\u51c6\u786e\u6027\u3002", "result": "\u4e8b\u4ef6\u6d41\u6709\u6548\u4fdd\u7559\u4e86\u9762\u90e8\u751f\u7406\u4fe1\u606f\uff0c\u57fa\u4e8e\u4e8b\u4ef6\u5e27\u7684\u6a21\u578bRMSE\u4e3a3.32 bpm\uff0c60\u548c120 FPS\u4e8b\u4ef6\u5e27\u6a21\u578b\u8868\u73b0\u4f18\u4e8e30 FPS\u6807\u51c6\u76f8\u673a\u3002", "conclusion": "\u65f6\u95f4\u4e8b\u4ef6\u76f8\u673a\u5728\u8fdc\u7a0b\u5fc3\u7387\u76d1\u6d4b\u4e2d\u5177\u6709\u5b9e\u7528\u6f5c\u529b\uff0c\u9ad8\u5e27\u7387\u4e8b\u4ef6\u5e27\u6a21\u578b\u6027\u80fd\u66f4\u4f18\u3002"}}
{"id": "2505.09546", "pdf": "https://arxiv.org/pdf/2505.09546", "abs": "https://arxiv.org/abs/2505.09546", "authors": ["Yujin Kim", "Nathaniel Chin", "Arnav Vasudev", "Sanjiban Choudhury"], "title": "Distilling Realizable Students from Unrealizable Teachers", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "We study policy distillation under privileged information, where a student\npolicy with only partial observations must learn from a teacher with full-state\naccess. A key challenge is information asymmetry: the student cannot directly\naccess the teacher's state space, leading to distributional shifts and policy\ndegradation. Existing approaches either modify the teacher to produce\nrealizable but sub-optimal demonstrations or rely on the student to explore\nmissing information independently, both of which are inefficient. Our key\ninsight is that the student should strategically interact with the teacher\n--querying only when necessary and resetting from recovery states --to stay on\na recoverable path within its own observation space. We introduce two methods:\n(i) an imitation learning approach that adaptively determines when the student\nshould query the teacher for corrections, and (ii) a reinforcement learning\napproach that selects where to initialize training for efficient exploration.\nWe validate our methods in both simulated and real-world robotic tasks,\ndemonstrating significant improvements over standard teacher-student baselines\nin training efficiency and final performance. The project website is available\nat : https://portal-cornell.github.io/CritiQ_ReTRy/", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u7279\u6743\u4fe1\u606f\u4e0b\u7684\u7b56\u7565\u84b8\u998f\uff0c\u63d0\u51fa\u901a\u8fc7\u5b66\u751f\u7b56\u7565\u4e0e\u8001\u5e08\u7b56\u7565\u7684\u6218\u7565\u4e92\u52a8\uff08\u5fc5\u8981\u65f6\u67e5\u8be2\u53ca\u6062\u590d\u72b6\u6001\uff09\u6765\u5f25\u8865\u4fe1\u606f\u4e0d\u5bf9\u79f0\uff0c\u4ece\u800c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5b66\u751f\u7b56\u7565\u56e0\u65e0\u6cd5\u76f4\u63a5\u8bbf\u95ee\u8001\u5e08\u7b56\u7565\u7684\u5168\u90e8\u72b6\u6001\u4fe1\u606f\u800c\u9762\u4e34\u7684\u5206\u5e03\u504f\u79fb\u548c\u7b56\u7565\u9000\u5316\u95ee\u9898\uff0c\u907f\u514d\u73b0\u6709\u65b9\u6cd5\uff08\u4fee\u6539\u8001\u5e08\u7b56\u7565\u6216\u4f9d\u8d56\u5b66\u751f\u72ec\u7acb\u63a2\u7d22\uff09\u7684\u4f4e\u6548\u3002", "method": "\u5f15\u5165\u4e24\u79cd\u65b9\u6cd5\uff1a(i) \u9002\u5e94\u6027\u51b3\u5b9a\u5b66\u751f\u4f55\u65f6\u5e94\u67e5\u8be2\u8001\u5e08\u8fdb\u884c\u4fee\u6b63\u7684\u6a21\u4eff\u5b66\u4e60\uff0c(ii) \u9009\u62e9\u521d\u59cb\u5316\u8bad\u7ec3\u4f4d\u7f6e\u4ee5\u9ad8\u6548\u63a2\u7d22\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u57fa\u51c6\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u6218\u7565\u4e92\u52a8\u5f25\u8865\u4fe1\u606f\u4e0d\u5bf9\u79f0\uff0c\u4e3a\u7b56\u7565\u84b8\u998f\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e14\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2505.09552", "pdf": "https://arxiv.org/pdf/2505.09552", "abs": "https://arxiv.org/abs/2505.09552", "authors": ["Pascal K\u00fcndig", "Fabio Sigrist"], "title": "Scalable Computations for Generalized Mixed Effects Models with Crossed Random Effects Using Krylov Subspace Methods", "categories": ["stat.ME", "cs.LG", "stat.ML"], "comment": null, "summary": "Mixed effects models are widely used for modeling data with hierarchically\ngrouped structures and high-cardinality categorical predictor variables.\nHowever, for high-dimensional crossed random effects, current standard\ncomputations relying on Cholesky decompositions can become prohibitively slow.\nIn this work, we present novel Krylov subspace-based methods that address\nseveral existing computational bottlenecks. Among other things, we\ntheoretically analyze and empirically evaluate various preconditioners for the\nconjugate gradient and stochastic Lanczos quadrature methods, derive new\nconvergence results, and develop computationally efficient methods for\ncalculating predictive variances. Extensive experiments using simulated and\nreal-world data sets show that our proposed methods scale much better than\nCholesky-based computations, for instance, achieving a runtime reduction of\napproximately two orders of magnitudes for both estimation and prediction.\nMoreover, our software implementation is up to 10'000 times faster and more\nstable than state-of-the-art implementations such as lme4 and glmmTMB when\nusing default settings. Our methods are implemented in the free C++ software\nlibrary GPBoost with high-level Python and R packages.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKrylov\u5b50\u7a7a\u95f4\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u4ea4\u53c9\u968f\u673a\u6548\u5e94\u6a21\u578b\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u76f8\u6bd4\u57fa\u4e8eCholesky\u5206\u89e3\u7684\u4f20\u7edf\u65b9\u6cd5\uff0c\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u7ea6\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u5728\u8f6f\u4ef6\u5b9e\u73b0\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u3002", "motivation": "\u9488\u5bf9\u9ad8\u7ef4\u4ea4\u53c9\u968f\u673a\u6548\u5e94\u6a21\u578b\u4e2d\u56e0Cholesky\u5206\u89e3\u5bfc\u81f4\u7684\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u4f5c\u8005\u65e8\u5728\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u4ee5\u63d0\u5347\u8ba1\u7b97\u901f\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8eKrylov\u5b50\u7a7a\u95f4\u7684\u9884\u5904\u7406\u5171\u8f6d\u68af\u5ea6\u6cd5\u548c\u968f\u673aLanczos\u6c42\u79ef\u6cd5\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u9884\u5904\u7406\u5668\u7684\u6548\u679c\uff0c\u5e76\u63a8\u5bfc\u4e86\u65b0\u7684\u6536\u655b\u7ed3\u679c\uff0c\u540c\u65f6\u4f18\u5316\u4e86\u9884\u6d4b\u65b9\u5dee\u7684\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u65b0\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u7ea6\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u8f6f\u4ef6\u5b9e\u73b0\u901f\u5ea6\u6bd4lme4\u548cglmmTMB\u5feb\u81f310000\u500d\uff0c\u4e14\u66f4\u7a33\u5b9a\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u7ef4\u4ea4\u53c9\u968f\u673a\u6548\u5e94\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5df2\u96c6\u6210\u5230\u5f00\u6e90\u8f6f\u4ef6GPBoost\u4e2d\uff0c\u652f\u6301Python\u548cR\u63a5\u53e3\u3002"}}
{"id": "2505.09603", "pdf": "https://arxiv.org/pdf/2505.09603", "abs": "https://arxiv.org/abs/2505.09603", "authors": ["Shivin Dass", "Alaa Khaddaj", "Logan Engstrom", "Aleksander Madry", "Andrew Ilyas", "Roberto Mart\u00edn-Mart\u00edn"], "title": "DataMIL: Selecting Data for Robot Imitation Learning with Datamodels", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Recently, the robotics community has amassed ever larger and more diverse\ndatasets to train generalist robot policies. However, while these policies\nachieve strong mean performance across a variety of tasks, they often\nunderperform on individual, specialized tasks and require further tuning on\nnewly acquired task-specific data. Combining task-specific data with carefully\ncurated subsets of large prior datasets via co-training can produce better\nspecialized policies, but selecting data naively may actually harm downstream\nperformance. To address this, we introduce DataMIL, a policy-driven data\nselection framework built on the datamodels paradigm that reasons about data\nselection in an end-to-end manner, using the policy itself to identify which\ndata points will most improve performance. Unlike standard practices that\nfilter data using human notions of quality (e.g., based on semantic or visual\nsimilarity), DataMIL directly optimizes data selection for task success,\nallowing us to select data that enhance the policy while dropping data that\ndegrade it. To avoid performing expensive rollouts in the environment during\nselection, we use a novel surrogate loss function on task-specific data,\nallowing us to use DataMIL in the real world without degrading performance. We\nvalidate our approach on a suite of more than 60 simulation and real-world\nmanipulation tasks - most notably showing successful data selection from the\nOpen X-Embodiment datasets-demonstrating consistent gains in success rates and\nsuperior performance over multiple baselines. Our results underscore the\nimportance of end-to-end, performance-aware data selection for unlocking the\npotential of large prior datasets in robotics. More information at\nhttps://robin-lab.cs.utexas.edu/datamodels4imitation/", "AI": {"tldr": "DataMIL\u662f\u4e00\u4e2a\u57fa\u4e8e\u7b56\u7565\u7684\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u6570\u636e\u9009\u62e9\u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff0c\u907f\u514d\u4e86\u4eba\u4e3a\u8d28\u91cf\u8fc7\u6ee4\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u7b56\u7565\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u4ecd\u9700\u8c03\u4f18\uff0c\u6570\u636e\u9009\u62e9\u4e0d\u5f53\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\u3002", "method": "\u91c7\u7528DataMIL\u6846\u67b6\uff0c\u901a\u8fc7\u7b56\u7565\u9a71\u52a8\u7684\u6570\u636e\u9009\u62e9\u548c\u66ff\u4ee3\u635f\u5931\u51fd\u6570\uff0c\u65e0\u9700\u6602\u8d35\u73af\u5883\u6d4b\u8bd5\u5373\u53ef\u4f18\u5316\u6570\u636e\u9009\u62e9\u3002", "result": "\u572860\u4f59\u9879\u4eff\u771f\u548c\u5b9e\u9645\u4efb\u52a1\u4e2d\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5c24\u5176\u662f\u5728Open X-Embodiment\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u7aef\u5230\u7aef\u7684\u6027\u80fd\u611f\u77e5\u6570\u636e\u9009\u62e9\u5bf9\u91ca\u653e\u5927\u578b\u6570\u636e\u96c6\u6f5c\u529b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.09612", "pdf": "https://arxiv.org/pdf/2505.09612", "abs": "https://arxiv.org/abs/2505.09612", "authors": ["Tathagata Sadhukhan", "Manit Paul", "Raaz Dwivedi"], "title": "Adaptively-weighted Nearest Neighbors for Matrix Completion", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "comment": "25 pages, 6 figures", "summary": "In this technical note, we introduce and analyze AWNN: an adaptively weighted\nnearest neighbor method for performing matrix completion. Nearest neighbor (NN)\nmethods are widely used in missing data problems across multiple disciplines\nsuch as in recommender systems and for performing counterfactual inference in\npanel data settings. Prior works have shown that in addition to being very\nintuitive and easy to implement, NN methods enjoy nice theoretical guarantees.\nHowever, the performance of majority of the NN methods rely on the appropriate\nchoice of the radii and the weights assigned to each member in the nearest\nneighbor set and despite several works on nearest neighbor methods in the past\ntwo decades, there does not exist a systematic approach of choosing the radii\nand the weights without relying on methods like cross-validation. AWNN\naddresses this challenge by judiciously balancing the bias variance trade off\ninherent in weighted nearest-neighbor regression. We provide theoretical\nguarantees for the proposed method under minimal assumptions and support the\ntheory via synthetic experiments.", "AI": {"tldr": "AWNN\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u52a0\u6743\u7684\u6700\u8fd1\u90bb\u65b9\u6cd5\uff0c\u7528\u4e8e\u77e9\u9635\u8865\u5168\uff0c\u901a\u8fc7\u667a\u80fd\u5e73\u8861\u52a0\u6743\u6700\u8fd1\u90bb\u56de\u5f52\u4e2d\u7684\u504f\u5dee-\u65b9\u5dee\u6743\u8861\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4ea4\u53c9\u9a8c\u8bc1\u9009\u62e9\u534a\u5f84\u548c\u6743\u91cd\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6700\u8fd1\u90bb\u65b9\u6cd5\u867d\u7136\u5728\u591a\u4e2a\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\u4e14\u7406\u8bba\u4fdd\u8bc1\u826f\u597d\uff0c\u4f46\u5176\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u534a\u5f84\u548c\u6743\u91cd\u7684\u9009\u62e9\uff0c\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u9009\u62e9\u65b9\u6cd5\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86AWNN\u65b9\u6cd5\u3002", "method": "AWNN\u901a\u8fc7\u81ea\u9002\u5e94\u52a0\u6743\u6700\u8fd1\u90bb\u65b9\u6cd5\uff0c\u667a\u80fd\u5e73\u8861\u56de\u5f52\u4e2d\u7684\u504f\u5dee-\u65b9\u5dee\u6743\u8861\uff0c\u65e0\u9700\u4f9d\u8d56\u4ea4\u53c9\u9a8c\u8bc1\u5373\u53ef\u9009\u62e9\u6700\u4f18\u534a\u5f84\u548c\u6743\u91cd\u3002", "result": "\u7406\u8bba\u5206\u6790\u5728\u6700\u5c0f\u5047\u8bbe\u4e0b\u9a8c\u8bc1\u4e86AWNN\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u5b9e\u9a8c\u652f\u6301\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "AWNN\u4e3a\u77e9\u9635\u8865\u5168\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7cfb\u7edf\u3001\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u5bf9\u5916\u90e8\u9a8c\u8bc1\u65b9\u6cd5\u7684\u4f9d\u8d56\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u7684\u53cc\u91cd\u4f18\u52bf\u3002"}}
