{"id": "2506.01961", "pdf": "https://arxiv.org/pdf/2506.01961", "abs": "https://arxiv.org/abs/2506.01961", "authors": ["Jinzhu Yang"], "title": "Research on Medical Named Entity Identification Based On Prompt-Biomrc Model and Its Application in Intelligent Consultation System", "categories": ["cs.CL"], "comment": null, "summary": "This study is dedicated to exploring the application of prompt learning\nmethods to advance Named Entity Recognition (NER) within the medical domain. In\nrecent years, the emergence of large-scale models has driven significant\nprogress in NER tasks, particularly with the introduction of the BioBERT\nlanguage model, which has greatly enhanced NER capabilities in medical texts.\nOur research introduces the Prompt-bioMRC model, which integrates both hard\ntemplate and soft prompt designs aimed at refining the precision and efficiency\nof medical entity recognition. Through extensive experimentation across diverse\nmedical datasets, our findings consistently demonstrate that our approach\nsurpasses traditional models. This enhancement not only validates the efficacy\nof our methodology but also highlights its potential to provide reliable\ntechnological support for applications like intelligent diagnosis systems. By\nleveraging advanced NER techniques, this study contributes to advancing\nautomated medical data processing, facilitating more accurate medical\ninformation extraction, and supporting efficient healthcare decision-making\nprocesses.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u533b\u5b66\u9886\u57df\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u786c\u6a21\u677f\u548c\u8f6f\u63d0\u793a\u8bbe\u8ba1\u7684Prompt-bioMRC\u6a21\u578b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3a\u667a\u80fd\u8bca\u65ad\u7cfb\u7edf\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u6280\u672f\u652f\u6491\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08\u5982BioBERT\uff09\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u6587\u672c\u7684NER\u6027\u80fd\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u533b\u5b66\u5b9e\u4f53\u8bc6\u522b\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86Prompt-bioMRC\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u786c\u6a21\u677f\u548c\u8f6f\u63d0\u793a\u8bbe\u8ba1\uff0c\u4f18\u5316\u533b\u5b66\u9886\u57df\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "Prompt-bioMRC\u6a21\u578b\u4e3a\u533b\u5b66\u9886\u57df\u7684NER\u63d0\u4f9b\u4e86\u53ef\u9760\u6280\u672f\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u81ea\u52a8\u5316\u533b\u7597\u6570\u636e\u5904\u7406\u548c\u667a\u80fd\u8bca\u65ad\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "keywords": "\u547d\u540d\u5b9e\u4f53\u8bc6\u522b, \u63d0\u793a\u5b66\u4e60, \u533b\u7597\u9886\u57df, BioBERT, Prompt-bioMRC"}}
{"id": "2506.01992", "pdf": "https://arxiv.org/pdf/2506.01992", "abs": "https://arxiv.org/abs/2506.01992", "authors": ["Lukas Rauch", "Moritz Wirth", "Denis Huseljic", "Marek Herde", "Bernhard Sick", "Matthias A\u00dfenmacher"], "title": "No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "under review @NeurIPS2025", "summary": "The advent of large language models (LLMs) capable of producing\ngeneral-purpose representations lets us revisit the practicality of deep active\nlearning (AL): By leveraging frozen LLM embeddings, we can mitigate the\ncomputational costs of iteratively fine-tuning large backbones. This study\nestablishes a benchmark and systematically investigates the influence of LLM\nembedding quality on query strategies in deep AL. We employ five top-performing\nmodels from the massive text embedding benchmark (MTEB) leaderboard and two\nbaselines for ten diverse text classification tasks. Our findings reveal key\ninsights: First, initializing the labeled pool using diversity-based sampling\nsynergizes with high-quality embeddings, boosting performance in early AL\niterations. Second, the choice of the optimal query strategy is sensitive to\nembedding quality. While the computationally inexpensive Margin sampling can\nachieve performance spikes on specific datasets, we find that strategies like\nBadge exhibit greater robustness across tasks. Importantly, their effectiveness\nis often enhanced when paired with higher-quality embeddings. Our results\nemphasize the need for context-specific evaluation of AL strategies, as\nperformance heavily depends on embedding quality and the target task.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5229\u7528\u9ad8\u8d28\u91cfLLM\u5d4c\u5165\u5bf9\u6df1\u5ea6\u4e3b\u52a8\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5d4c\u5165\u8d28\u91cf\u76f4\u63a5\u5f71\u54cd\u67e5\u8be2\u7b56\u7565\u9009\u62e9\uff0c\u4e14\u591a\u6837\u6027\u91c7\u6837\u4e0e\u9ad8\u8d28\u91cf\u5d4c\u5165\u7ed3\u5408\u5728\u65e9\u671f\u8fed\u4ee3\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u8ba8\u9ad8\u8d28\u91cfLLM\u5d4c\u5165\u5982\u4f55\u964d\u4f4e\u6df1\u5ea6\u4e3b\u52a8\u5b66\u4e60\u7684\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u6548\u679c\uff0c\u586b\u8865\u4e86LLM\u5d4c\u5165\u5728AL\u4efb\u52a1\u4e2d\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u91c7\u7528MTEB\u6392\u884c\u699c\u4e0a\u7684\u4e94\u79cd\u9ad8\u6027\u80fdLLM\u6a21\u578b\u548c\u4e24\u79cd\u57fa\u7ebf\uff0c\u5728\u5341\u79cd\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u6d4b\u8bd5\u4e0d\u540c\u67e5\u8be2\u7b56\u7565\u7684\u8868\u73b0\u3002", "result": "\u591a\u6837\u6027\u91c7\u6837\u4e0e\u9ad8\u8d28\u91cf\u5d4c\u5165\u534f\u540c\u63d0\u5347\u65e9\u671fAL\u6027\u80fd\uff1b\u5d4c\u5165\u8d28\u91cf\u5f71\u54cd\u6700\u4f73\u7b56\u7565\u9009\u62e9\uff0cBadge\u7b56\u7565\u5728\u591a\u6570\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "AL\u7b56\u7565\u9700\u7ed3\u5408\u5d4c\u5165\u8d28\u91cf\u548c\u4efb\u52a1\u7279\u70b9\u8fdb\u884c\u9488\u5bf9\u6027\u8bc4\u4f30\uff0c\u5d4c\u5165\u8d28\u91cf\u5bf9\u7b56\u7565\u6548\u679c\u6709\u663e\u8457\u5f71\u54cd\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u6df1\u5ea6\u4e3b\u52a8\u5b66\u4e60\u3001\u5d4c\u5165\u8d28\u91cf\u3001\u67e5\u8be2\u7b56\u7565\u3001\u6587\u672c\u5206\u7c7b"}}
{"id": "2506.02000", "pdf": "https://arxiv.org/pdf/2506.02000", "abs": "https://arxiv.org/abs/2506.02000", "authors": ["Abhay Gupta", "Michael Lu", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "title": "NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts", "categories": ["cs.CL"], "comment": null, "summary": "Current large language models (LLMs) struggle to answer questions that span\ntens of thousands of tokens, especially when multi-hop reasoning is involved.\nWhile prior benchmarks explore long-context comprehension or multi-hop\nreasoning in isolation, none jointly vary context length and reasoning depth in\nnatural narrative settings. We introduce NovelHopQA, the first benchmark to\nevaluate k1-4 hop QA over 64k-128k-token excerpts from 83 full-length\npublic-domain novels. A keyword-guided pipeline builds hop-separated chains\ngrounded in coherent storylines. We evaluate six state-of-the-art (SOTA) models\nand apply oracle-context filtering to ensure all questions are genuinely\nanswerable. Human annotators validate both alignment and hop depth. We noticed\nconsistent accuracy drops with increased hops and context length, even in\nfrontier models-revealing that sheer scale does not guarantee robust reasoning.\nOur failure mode analysis highlights common breakdowns, such as missed\nfinal-hop integration and long-range drift. NovelHopQA offers a controlled\ndiagnostic setting to stress-test multi-hop reasoning at scale.", "AI": {"tldr": "NovelHopQA\u662f\u9996\u4e2a\u8bc4\u4f30\u572864k-128k\u4ee4\u724c\u957f\u6587\u5c0f\u8bf4\u7247\u6bb5\u4e0a\u8fdb\u884c1-4\u8df3\u95ee\u7b54\u7684\u57fa\u51c6\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u591a\u8df3\u548c\u957f\u4e0a\u4e0b\u6587\u4e0b\u8868\u73b0\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6d89\u53ca\u591a\u8df3\u63a8\u7406\u7684\u957f\u4e0a\u4e0b\u6587\u95ee\u7b54\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u7ed3\u5408\u957f\u4e0a\u4e0b\u6587\u548c\u591a\u8df3\u63a8\u7406\u7684\u81ea\u7136\u53d9\u4e8b\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u901a\u8fc7\u5173\u952e\u8bcd\u5f15\u5bfc\u7684\u7ba1\u9053\u6784\u5efa\u57fa\u4e8e\u8fde\u8d2f\u6545\u4e8b\u60c5\u8282\u7684\u591a\u8df3\u95ee\u7b54\u94fe\uff0c\u8bc4\u4f306\u79cd\u524d\u6cbf\u6a21\u578b\uff0c\u5e76\u5e94\u7528\u7406\u60f3\u4e0a\u4e0b\u6587\u8fc7\u6ee4\u786e\u4fdd\u95ee\u9898\u53ef\u89e3\u3002", "result": "\u6a21\u578b\u5728\u591a\u8df3\u548c\u957f\u4e0a\u4e0b\u6587\u4e0b\u7684\u51c6\u786e\u6027\u663e\u8457\u4e0b\u964d\uff0c\u63ed\u793a\u4ec5\u9760\u89c4\u6a21\u65e0\u6cd5\u4fdd\u8bc1\u7a33\u5065\u63a8\u7406\u3002", "conclusion": "NovelHopQA\u4e3a\u5927\u89c4\u6a21\u591a\u8df3\u63a8\u7406\u7684\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u8bca\u65ad\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u7684\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u591a\u8df3\u63a8\u7406, \u957f\u4e0a\u4e0b\u6587, \u95ee\u7b54\u57fa\u51c6, \u81ea\u7136\u53d9\u4e8b"}}
{"id": "2506.02005", "pdf": "https://arxiv.org/pdf/2506.02005", "abs": "https://arxiv.org/abs/2506.02005", "authors": ["Timothy Do", "Pranav Saran", "Harshita Poojary", "Pranav Prabhu", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "title": "Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT", "categories": ["cs.CL"], "comment": "9 pages, 7 figures", "summary": "In this paper, we address the persistent challenges that figurative language\nexpressions pose for natural language processing (NLP) systems, particularly in\nlow-resource languages such as Konkani. We present a hybrid model that\nintegrates a pre-trained Multilingual BERT (mBERT) with a bidirectional LSTM\nand a linear classifier. This architecture is fine-tuned on a newly introduced\nannotated dataset for metaphor classification, developed as part of this work.\nTo improve the model's efficiency, we implement a gradient-based attention head\npruning strategy. For metaphor classification, the pruned model achieves an\naccuracy of 78%. We also applied our pruning approach to expand on an existing\nidiom classification task, achieving 83% accuracy. These results demonstrate\nthe effectiveness of attention head pruning for building efficient NLP tools in\nunderrepresented languages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6a21\u578b\uff0c\u7ed3\u5408mBERT\u3001\u53cc\u5411LSTM\u548c\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982Konkani\uff09\u4e2d\u7684\u6bd4\u55bb\u8bed\u8a00\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u6ce8\u610f\u529b\u5934\u526a\u679d\u63d0\u5347\u6548\u7387\uff0c\u5206\u522b\u8fbe\u523078%\u548c83%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u6bd4\u55bb\u8bed\u8a00\u8868\u8fbe\u5bf9NLP\u7cfb\u7edf\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6a21\u578b\uff08mBERT+\u53cc\u5411LSTM+\u7ebf\u6027\u5206\u7c7b\u5668\uff09\uff0c\u5e76\u91c7\u7528\u68af\u5ea6\u6ce8\u610f\u529b\u5934\u526a\u679d\u7b56\u7565\u3002", "result": "\u5728\u6bd4\u55bb\u5206\u7c7b\u548c\u4e60\u8bed\u5206\u7c7b\u4efb\u52a1\u4e2d\u5206\u522b\u8fbe\u523078%\u548c83%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u6ce8\u610f\u529b\u5934\u526a\u679d\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u9ad8\u6548NLP\u5de5\u5177\u6784\u5efa\u6709\u6548\u3002", "keywords": "\u6bd4\u55bb\u8bed\u8a00, \u4f4e\u8d44\u6e90\u8bed\u8a00, mBERT, \u6ce8\u610f\u529b\u5934\u526a\u679d, Konkani"}}
{"id": "2506.02097", "pdf": "https://arxiv.org/pdf/2506.02097", "abs": "https://arxiv.org/abs/2506.02097", "authors": ["Priyaranjan Pattnayak", "Amit Agarwal", "Hansa Meghwani", "Hitesh Laxmichand Patel", "Srikant Panda"], "title": "Hybrid AI for Responsive Multi-Turn Online Conversations with Novel Dynamic Routing and Feedback Adaptation", "categories": ["cs.AI"], "comment": "Proceedings of the 4th International Workshop on Knowledge Augmented\n  Methods for Natural Language Processing in NAACL 2025, pages 215 to 229,\n  Albuquerque, New Mexico, USA. Association for Computational Linguistics", "summary": "Retrieval-Augmented Generation (RAG) systems and large language model\n(LLM)-powered chatbots have significantly advanced conversational AI by\ncombining generative capabilities with external knowledge retrieval. Despite\ntheir success, enterprise-scale deployments face critical challenges, including\ndiverse user queries, high latency, hallucinations, and difficulty integrating\nfrequently updated domain-specific knowledge. This paper introduces a novel\nhybrid framework that integrates RAG with intent-based canned responses,\nleveraging predefined high-confidence responses for efficiency while\ndynamically routing complex or ambiguous queries to the RAG pipeline. Our\nframework employs a dialogue context manager to ensure coherence in multi-turn\ninteractions and incorporates a feedback loop to refine intents, dynamically\nadjust confidence thresholds, and expand response coverage over time.\nExperimental results demonstrate that the proposed framework achieves a balance\nof high accuracy (95\\%) and low latency (180ms), outperforming RAG and\nintent-based systems across diverse query types, positioning it as a scalable\nand adaptive solution for enterprise conversational AI applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u9884\u5b9a\u4e49\u56de\u5e94\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4f01\u4e1a\u7ea7\u5bf9\u8bddAI\u4e2d\u7684\u9ad8\u5ef6\u8fdf\u3001\u5e7b\u89c9\u7b49\u95ee\u9898\u3002", "motivation": "\u4f01\u4e1a\u7ea7\u5bf9\u8bddAI\u9762\u4e34\u591a\u6837\u67e5\u8be2\u3001\u9ad8\u5ef6\u8fdf\u3001\u9891\u7e41\u66f4\u65b0\u7684\u9886\u57df\u77e5\u8bc6\u7b49\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u96c6\u6210RAG\u4e0e\u9884\u5b9a\u4e49\u9ad8\u7f6e\u4fe1\u5ea6\u56de\u5e94\uff0c\u52a8\u6001\u8def\u7531\u590d\u6742\u67e5\u8be2\uff0c\u5e76\u5229\u7528\u53cd\u9988\u5faa\u73af\u4f18\u5316\u610f\u56fe\u548c\u54cd\u5e94\u8303\u56f4\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u6846\u67b6\u5728\u51c6\u786e\u6027\uff0895%\uff09\u548c\u5ef6\u8fdf\uff08180ms\uff09\u4e0a\u4f18\u4e8e\u4f20\u7edfRAG\u548c\u610f\u56fe\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u67e5\u8be2\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u6846\u67b6\u4e3a\u4f01\u4e1a\u7ea7\u5bf9\u8bddAI\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09, \u610f\u56fe\u7cfb\u7edf, \u5bf9\u8bddAI, \u4f01\u4e1a\u5e94\u7528"}}
{"id": "2506.01959", "pdf": "https://arxiv.org/pdf/2506.01959", "abs": "https://arxiv.org/abs/2506.01959", "authors": ["Irmi Schneider"], "title": "Ubiquitous Symmetry at Critical Points Across Diverse Optimization Landscapes", "categories": ["cs.LG", "cs.AI", "physics.atom-ph"], "comment": null, "summary": "Symmetry plays a crucial role in understanding the properties of mathematical\nstructures and optimization problems. Recent work has explored this phenomenon\nin the context of neural networks, where the loss function is invariant under\ncolumn and row permutations of the network weights. It has been observed that\nlocal minima exhibit significant symmetry with respect to the network weights\n(invariance to row and column permutations). And moreover no critical point was\nfound that lacked symmetry. We extend this line of inquiry by investigating\nsymmetry phenomena in real-valued loss functions defined on a broader class of\nspaces. We will introduce four more cases: the projective case over a finite\nfield, the octahedral graph case, the perfect matching case, and the particle\nattraction case. We show that as in the neural network case, all the critical\npoints observed have non-trivial symmetry. Finally we introduce a new measure\nof symmetry in the system and show that it reveals additional symmetry\nstructures not captured by the previous measure.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e0d\u540c\u7a7a\u95f4\u4e2d\u7684\u5bf9\u79f0\u6027\u73b0\u8c61\uff0c\u6269\u5c55\u4e86\u795e\u7ecf\u7f51\u7edc\u4e2d\u6743\u91cd\u5bf9\u79f0\u6027\u7684\u7814\u7a76\uff0c\u5e76\u5728\u5176\u4ed6\u56db\u79cd\u6848\u4f8b\u4e2d\u53d1\u73b0\u4e86\u4e34\u754c\u70b9\u7684\u975e\u5e73\u51e1\u5bf9\u79f0\u6027\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u79f0\u6027\u5ea6\u91cf\u65b9\u6cd5\u3002", "motivation": "\u63a2\u8ba8\u5bf9\u79f0\u6027\u5728\u6570\u5b66\u7ed3\u6784\u548c\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\uff0c\u7279\u522b\u662f\u6269\u5c55\u4e86\u795e\u7ecf\u7f51\u7edc\u4e2d\u6743\u91cd\u5bf9\u79f0\u6027\u7684\u7814\u7a76\uff0c\u8bd5\u56fe\u7406\u89e3\u66f4\u5e7f\u6cdb\u7a7a\u95f4\u4e2d\u7684\u5bf9\u79f0\u6027\u73b0\u8c61\u3002", "method": "\u7814\u7a76\u4e86\u56db\u79cd\u6848\u4f8b\uff08\u6709\u9650\u57df\u4e0a\u7684\u6295\u5f71\u6848\u4f8b\u3001\u516b\u9762\u4f53\u56fe\u6848\u4f8b\u3001\u5b8c\u7f8e\u5339\u914d\u6848\u4f8b\u548c\u7c92\u5b50\u5438\u5f15\u6848\u4f8b\uff09\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u4e9b\u6848\u4f8b\u4e2d\u4e34\u754c\u70b9\u7684\u5bf9\u79f0\u6027\u3002\u540c\u65f6\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u79f0\u6027\u5ea6\u91cf\u65b9\u6cd5\u3002", "result": "\u6240\u6709\u5206\u6790\u7684\u6848\u4f8b\u4e2d\uff0c\u4e34\u754c\u70b9\u5747\u8868\u73b0\u51fa\u975e\u5e73\u51e1\u5bf9\u79f0\u6027\u3002\u65b0\u5f15\u5165\u7684\u5bf9\u79f0\u6027\u5ea6\u91cf\u65b9\u6cd5\u63ed\u793a\u4e86\u4e4b\u524d\u65b9\u6cd5\u672a\u80fd\u6355\u6349\u5230\u7684\u989d\u5916\u5bf9\u79f0\u6027\u7ed3\u6784\u3002", "conclusion": "\u5bf9\u79f0\u6027\u662f\u4f18\u5316\u95ee\u9898\u4e2d\u666e\u904d\u5b58\u5728\u7684\u73b0\u8c61\uff0c\u65b0\u63d0\u51fa\u7684\u5bf9\u79f0\u6027\u5ea6\u91cf\u65b9\u6cd5\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u5bf9\u79f0\u6027\u63d0\u4f9b\u4e86\u5de5\u5177\u3002", "keywords": "\u5bf9\u79f0\u6027, \u795e\u7ecf\u7f51\u7edc, \u4e34\u754c\u70b9, \u4f18\u5316\u95ee\u9898, \u5ea6\u91cf\u65b9\u6cd5"}}
{"id": "2506.02018", "pdf": "https://arxiv.org/pdf/2506.02018", "abs": "https://arxiv.org/abs/2506.02018", "authors": ["Christopher Lee L\u00fcbbers"], "title": "Enhancing Paraphrase Type Generation: The Impact of DPO and RLHF Evaluated with Human-Ranked Data", "categories": ["cs.CL", "I.2.7"], "comment": "21 pages, 11 figures. Master's thesis, University of Goettingen,\n  December 2025. Code: https://github.com/cluebbers/dpo-rlhf-paraphrase-types.\n  Models:\n  https://huggingface.co/collections/cluebbers/enhancing-paraphrase-type-generation-673ca8d75dfe2ce962a48ac0", "summary": "Paraphrasing re-expresses meaning to enhance applications like text\nsimplification, machine translation, and question-answering. Specific\nparaphrase types facilitate accurate semantic analysis and robust language\nmodels. However, existing paraphrase-type generation methods often misalign\nwith human preferences due to reliance on automated metrics and limited\nhuman-annotated training data, obscuring crucial aspects of semantic fidelity\nand linguistic transformations.\n  This study addresses this gap by leveraging a human-ranked paraphrase-type\ndataset and integrating Direct Preference Optimization (DPO) to align model\noutputs directly with human judgments. DPO-based training increases\nparaphrase-type generation accuracy by 3 percentage points over a supervised\nbaseline and raises human preference ratings by 7 percentage points. A newly\ncreated human-annotated dataset supports more rigorous future evaluations.\nAdditionally, a paraphrase-type detection model achieves F1 scores of 0.91 for\naddition/deletion, 0.78 for same polarity substitution, and 0.70 for\npunctuation changes.\n  These findings demonstrate that preference data and DPO training produce more\nreliable, semantically accurate paraphrases, enabling downstream applications\nsuch as improved summarization and more robust question-answering. The PTD\nmodel surpasses automated metrics and provides a more reliable framework for\nevaluating paraphrase quality, advancing paraphrase-type research toward\nricher, user-aligned language generation and establishing a stronger foundation\nfor future evaluations grounded in human-centric criteria.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4eba\u7c7b\u6392\u540d\u6570\u636e\u96c6\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u6280\u672f\uff0c\u63d0\u5347\u4e86\u8bed\u4e49\u4fdd\u6301\u548c\u8bed\u8a00\u53d8\u6362\u7684\u51c6\u786e\u6027\uff0c\u6539\u8fdb\u4e86\u751f\u6210\u548c\u68c0\u6d4b\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u81ea\u52a8\u5316\u6307\u6807\u548c\u6709\u9650\u7684\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u4e0e\u4eba\u7c7b\u504f\u597d\u4e0d\u4e00\u81f4\u3002", "method": "\u4f7f\u7528\u4eba\u7c7b\u6392\u540d\u7684\u6570\u636e\u96c6\u548cDPO\u6280\u672f\uff0c\u76f4\u63a5\u5bf9\u9f50\u6a21\u578b\u8f93\u51fa\u4e0e\u4eba\u7c7b\u5224\u65ad\u3002", "result": "\u751f\u6210\u51c6\u786e\u7387\u63d0\u9ad83%\uff0c\u4eba\u7c7b\u504f\u597d\u8bc4\u5206\u63d0\u53477%\u3002\u68c0\u6d4b\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e2dF1\u5206\u6570\u8fbe0.91\u3002", "conclusion": "\u504f\u597d\u6570\u636e\u548cDPO\u8bad\u7ec3\u80fd\u751f\u6210\u66f4\u53ef\u9760\u7684\u8bed\u4e49\u51c6\u786e\u8f6c\u8ff0\uff0c\u63a8\u52a8\u4e0b\u6e38\u5e94\u7528\u3002", "keywords": "\u8f6c\u8ff0\u751f\u6210, \u76f4\u63a5\u504f\u597d\u4f18\u5316, \u8bed\u4e49\u5206\u6790, \u8bed\u8a00\u6a21\u578b, \u4eba\u5de5\u6807\u6ce8"}}
{"id": "2506.02125", "pdf": "https://arxiv.org/pdf/2506.02125", "abs": "https://arxiv.org/abs/2506.02125", "authors": ["Guy Tennenholtz", "Jihwan Jeong", "Chih-Wei Hsu", "Yinlam Chow", "Craig Boutilier"], "title": "Descriptive History Representations: Learning Representations by Answering Questions", "categories": ["cs.AI"], "comment": null, "summary": "Effective decision making in partially observable environments requires\ncompressing long interaction histories into informative representations. We\nintroduce Descriptive History Representations (DHRs): sufficient statistics\ncharacterized by their capacity to answer relevant questions about past\ninteractions and potential future outcomes. DHRs focus on capturing the\ninformation necessary to address task-relevant queries, providing a structured\nway to summarize a history for optimal control. We propose a multi-agent\nlearning framework, involving representation, decision, and question-asking\ncomponents, optimized using a joint objective that balances reward maximization\nwith the representation's ability to answer informative questions. This yields\nrepresentations that capture the salient historical details and predictive\nstructures needed for effective decision making. We validate our approach on\nuser modeling tasks with public movie and shopping datasets, generating\ninterpretable textual user profiles which serve as sufficient statistics for\npredicting preference-driven behavior of users.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u63cf\u8ff0\u6027\u5386\u53f2\u8868\u5f81\uff08DHRs\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u538b\u7f29\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u7684\u4ea4\u4e92\u5386\u53f2\uff0c\u4ee5\u652f\u6301\u6709\u6548\u51b3\u7b56\u3002", "motivation": "\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\uff0c\u6709\u6548\u51b3\u7b56\u9700\u8981\u5c06\u957f\u4ea4\u4e92\u5386\u53f2\u538b\u7f29\u4e3a\u4fe1\u606f\u4e30\u5bcc\u7684\u8868\u5f81\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u62ec\u8868\u5f81\u3001\u51b3\u7b56\u548c\u63d0\u95ee\u7ec4\u4ef6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u76ee\u6807\u5e73\u8861\u5956\u52b1\u6700\u5927\u5316\u548c\u8868\u5f81\u80fd\u529b\u3002", "result": "\u5728\u7535\u5f71\u548c\u8d2d\u7269\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u7528\u6237\u753b\u50cf\uff0c\u7528\u4e8e\u9884\u6d4b\u7528\u6237\u504f\u597d\u884c\u4e3a\u3002", "conclusion": "DHRs\u80fd\u591f\u6355\u83b7\u5173\u952e\u5386\u53f2\u7ec6\u8282\u548c\u9884\u6d4b\u7ed3\u6784\uff0c\u4e3a\u6709\u6548\u51b3\u7b56\u63d0\u4f9b\u652f\u6301\u3002", "keywords": "\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883, \u63cf\u8ff0\u6027\u5386\u53f2\u8868\u5f81, \u591a\u667a\u80fd\u4f53\u5b66\u4e60, \u7528\u6237\u5efa\u6a21"}}
{"id": "2506.01962", "pdf": "https://arxiv.org/pdf/2506.01962", "abs": "https://arxiv.org/abs/2506.01962", "authors": ["Xiaozhou Ye", "Kevin I-Kai Wang"], "title": "Graph-Based Adversarial Domain Generalization with Anatomical Correlation Knowledge for Cross-User Human Activity Recognition", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Cross-user variability poses a significant challenge in sensor-based Human\nActivity Recognition (HAR) systems, as traditional models struggle to\ngeneralize across users due to differences in behavior, sensor placement, and\ndata distribution. To address this, we propose GNN-ADG (Graph Neural Network\nwith Adversarial Domain Generalization), a novel method that leverages both the\nstrength from both the Graph Neural Networks (GNNs) and adversarial learning to\nachieve robust cross-user generalization. GNN-ADG models spatial relationships\nbetween sensors on different anatomical body parts, extracting three types of\nAnatomical Units: (1) Interconnected Units, capturing inter-relations between\nneighboring sensors; (2) Analogous Units, grouping sensors on symmetrical or\nfunctionally similar body parts; and (3) Lateral Units, connecting sensors\nbased on their position to capture region-specific coordination. These units\ninformation are fused into an unified graph structure with a cyclic training\nstrategy, dynamically integrating spatial, functional, and lateral correlations\nto facilitate a holistic, user-invariant representation. Information fusion\nmechanism of GNN-ADG occurs by iteratively cycling through edge topologies\nduring training, allowing the model to refine its understanding of inter-sensor\nrelationships across diverse perspectives. By representing the spatial\nconfiguration of sensors as an unified graph and incorporating adversarial\nlearning, Information Fusion GNN-ADG effectively learns features that\ngeneralize well to unseen users without requiring target user data during\ntraining, making it practical for real-world applications.", "AI": {"tldr": "\u63d0\u51faGNN-ADG\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5bf9\u6297\u5b66\u4e60\u89e3\u51b3\u4f20\u611f\u5668\u4eba\u673a\u4ea4\u4e92\u8bc6\u522b\u4e2d\u7684\u8de8\u7528\u6237\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4f20\u611f\u5668\u4eba\u673a\u4ea4\u4e92\u8bc6\u522b\u4e2d\u56e0\u7528\u6237\u884c\u4e3a\u3001\u4f20\u611f\u5668\u653e\u7f6e\u548c\u6570\u636e\u5206\u5e03\u5dee\u5f02\u5bfc\u81f4\u7684\u8de8\u7528\u6237\u6cdb\u5316\u6311\u6218\u3002", "method": "\u57fa\u4e8eGNN\u548c\u5bf9\u6297\u5b66\u4e60\uff0c\u6784\u5efa\u4e09\u79cd\u89e3\u5256\u5355\u5143\uff08\u4e92\u8054\u3001\u7c7b\u6bd4\u548c\u4fa7\u8fb9\u5355\u5143\uff09\u7684\u7edf\u4e00\u56fe\u7ed3\u6784\uff0c\u901a\u8fc7\u5faa\u73af\u8bad\u7ec3\u7b56\u7565\u52a8\u6001\u878d\u5408\u7a7a\u95f4\u3001\u529f\u80fd\u548c\u4fa7\u8fb9\u76f8\u5173\u6027\u3002", "result": "GNN-ADG\u65e0\u9700\u76ee\u6807\u7528\u6237\u6570\u636e\u5373\u53ef\u5b66\u4e60\u6cdb\u5316\u7279\u5f81\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u5e94\u7528\u3002", "conclusion": "GNN-ADG\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u7528\u6237\u6cdb\u5316\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "keywords": "\u4f20\u611f\u5668\u4eba\u673a\u4ea4\u4e92\u8bc6\u522b, \u8de8\u7528\u6237\u6cdb\u5316, \u56fe\u795e\u7ecf\u7f51\u7edc, \u5bf9\u6297\u5b66\u4e60"}}
{"id": "2506.02019", "pdf": "https://arxiv.org/pdf/2506.02019", "abs": "https://arxiv.org/abs/2506.02019", "authors": ["E Fan", "Weizong Wang", "Tianhan Zhang"], "title": "ChatCFD: an End-to-End CFD Agent with Domain-specific Structured Thinking", "categories": ["cs.CL"], "comment": "19 pages, 8 figures", "summary": "Computational Fluid Dynamics (CFD) is essential for scientific and\nengineering advancements but is limited by operational complexity and the need\nfor extensive expertise. This paper presents ChatCFD, a large language\nmodel-driven pipeline that automates CFD workflows within the OpenFOAM\nframework. It enables users to configure and execute complex simulations from\nnatural language prompts or published literature with minimal expertise. The\ninnovation is its structured approach to database construction, configuration\nvalidation, and error reflection, integrating CFD and OpenFOAM knowledge with\ngeneral language models to improve accuracy and adaptability. Validation shows\nChatCFD can autonomously reproduce published CFD results, handling complex,\nunseen configurations beyond basic examples, a task challenging for general\nlanguage models.", "AI": {"tldr": "ChatCFD\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316CFD\u5de5\u4f5c\u6d41\u7a0b\u5de5\u5177\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6216\u6587\u732e\u5feb\u901f\u914d\u7f6e\u548c\u6267\u884c\u590d\u6742\u6a21\u62df\uff0c\u65e0\u9700\u4e13\u4e1a\u80cc\u666f\u3002", "motivation": "\u73b0\u6709CFD\u5de5\u5177\u64cd\u4f5c\u590d\u6742\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u79d1\u5b66\u548c\u5de5\u7a0b\u5e94\u7528\uff0cChatCFD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u6570\u636e\u5e93\u6784\u5efa\u3001\u914d\u7f6e\u9a8c\u8bc1\u548c\u9519\u8bef\u53cd\u601d\uff0c\u5c06CFD\u4e0eOpenFOAM\u77e5\u8bc6\u878d\u5165\u666e\u901a\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "result": "\u9a8c\u8bc1\u8868\u660e\uff0cChatCFD\u80fd\u81ea\u4e3b\u590d\u73b0\u5df2\u53d1\u8868\u7ed3\u679c\uff0c\u5e76\u5904\u7406\u590d\u6742\u672a\u77e5\u914d\u7f6e\uff0c\u8d85\u8d8a\u666e\u901a\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "ChatCFD\u4e3aCFD\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u6613\u7528\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u62d3\u5c55\u4e86\u975e\u4e13\u5bb6\u7684\u4f7f\u7528\u573a\u666f\u3002", "keywords": "ChatCFD, CFD, OpenFOAM, \u81ea\u7136\u8bed\u8a00\u5904\u7406, \u81ea\u52a8\u5316"}}
{"id": "2506.02139", "pdf": "https://arxiv.org/pdf/2506.02139", "abs": "https://arxiv.org/abs/2506.02139", "authors": ["Edward Y. Chang"], "title": "The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning", "categories": ["cs.AI", "I.2.7"], "comment": "12 pages, 1 figure, 1 table", "summary": "Few-shot learning in large language models (LLMs) reveals a deep paradox:\nSome tasks generalize from minimal examples, while others require extensive\nsupervision. We address this through the Unified Cognitive Consciousness Theory\n(UCCT), which reframes LLMs not as incomplete agents, but as unconscious\nsubstrates, repositories of latent linguistic and conceptual patterns that\noperate without explicit semantics or goal-directed reasoning. In this view,\nLLMs are not broken approximations of cognition, but necessary and foundational\ncomponents of general intelligence. Semantic anchoring, through prompts, roles,\nand interaction, acts as a conscious control layer, binding latent structure to\ntask-relevant meaning and enabling coherent reasoning. UCCT offers a unifying\naccount of prompting, fine-tuning, retrieval, and multi-agent coordination, all\ngrounded in probabilistic alignment between unconscious representation and\nexternal control. To support this model, we present the Threshold-Crossing\nDynamics Theorem, which formalizes semantic anchoring as a probabilistic phase\ntransition. But the central claim remains architectural: AGI will not emerge by\ndiscarding LLMs, but by aligning and integrating them into systems that reason,\nregulate, and adapt together.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7edf\u4e00\u8ba4\u77e5\u610f\u8bc6\u7406\u8bba\uff08UCCT\uff09\uff0c\u8ba4\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u975e\u5b8c\u6574\u8ba4\u77e5\u7684\u57fa\u7840\u7ec4\u4ef6\uff0c\u800c\u975e\u7f3a\u9677\u3002\u901a\u8fc7\u8bed\u4e49\u951a\u5b9a\uff0cUCCT\u7edf\u4e00\u4e86\u63d0\u793a\u3001\u5fae\u8c03\u7b49\u65b9\u6cd5\uff0c\u5e76\u5f62\u5f0f\u5316\u4e3a\u6982\u7387\u76f8\u4f4d\u8f6c\u6362\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u77db\u76fe\u8868\u73b0\uff0c\u63d0\u51fa\u65b0\u7406\u8bba\u89e3\u91ca\u5176\u4f5c\u4e3a\u901a\u7528\u667a\u80fd\u57fa\u7840\u7ec4\u4ef6\u7684\u4f5c\u7528\u3002", "method": "\u63d0\u51faUCCT\u7406\u8bba\uff0c\u5c06LLMs\u89c6\u4e3a\u65e0\u610f\u8bc6\u5e95\u5c42\u7ed3\u6784\uff0c\u8bed\u4e49\u951a\u5b9a\u4f5c\u4e3a\u63a7\u5236\u5c42\uff0c\u5e76\u5f62\u5f0f\u5316\u4e3a\u6982\u7387\u76f8\u4f4d\u8f6c\u6362\u3002", "result": "UCCT\u4e3a\u63d0\u793a\u3001\u5fae\u8c03\u7b49\u65b9\u6cd5\u63d0\u4f9b\u7edf\u4e00\u89e3\u91ca\uff0c\u5e76\u652f\u6301LLMs\u4f5c\u4e3aAGI\u7cfb\u7edf\u6838\u5fc3\u7684\u89c2\u70b9\u3002", "conclusion": "AGI\u7684\u5b9e\u73b0\u9700\u6574\u5408LLMs\u800c\u975e\u629b\u5f03\uff0c\u901a\u8fc7\u8bed\u4e49\u951a\u5b9a\u5b9e\u73b0\u534f\u540c\u63a8\u7406\u4e0e\u9002\u5e94\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u5c11\u6837\u672c\u5b66\u4e60, \u7edf\u4e00\u8ba4\u77e5\u610f\u8bc6\u7406\u8bba, \u8bed\u4e49\u951a\u5b9a, AGI"}}
{"id": "2506.01963", "pdf": "https://arxiv.org/pdf/2506.01963", "abs": "https://arxiv.org/abs/2506.01963", "authors": ["Andrew Kiruluta", "Preethi Raju", "Priscilla Burity"], "title": "Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We present a novel non attention based architecture for large language models\n(LLMs) that efficiently handles very long context windows, on the order of\nhundreds of thousands to potentially millions of tokens. Unlike traditional\nTransformer designs, which suffer from quadratic memory and computation\noverload due to the nature of the self attention mechanism, our model avoids\ntoken to token attention entirely. Instead, it combines the following\ncomplementary components: State Space blocks (inspired by S4) that learn\ncontinuous time convolution kernels and scale near linearly with sequence\nlength, Multi Resolution Convolution layers that capture local context at\ndifferent dilation levels, a lightweight Recurrent Supervisor to maintain a\nglobal hidden state across sequential chunks, and Retrieval Augmented External\nMemory that stores and retrieves high-level chunk embeddings without\nreintroducing quadratic operations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u975e\u6ce8\u610f\u529b\u673a\u5236\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u9ad8\u6548\u5904\u7406\u8d85\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002", "motivation": "\u4f20\u7edfTransformer\u56e0\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5bfc\u81f4\u4e8c\u6b21\u65b9\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u96be\u4ee5\u5904\u7406\u8d85\u957f\u4e0a\u4e0b\u6587\u3002", "method": "\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u5757\u3001\u591a\u5206\u8fa8\u7387\u5377\u79ef\u5c42\u3001\u8f7b\u91cf\u7ea7\u5faa\u73af\u76d1\u7763\u5668\u548c\u68c0\u7d22\u589e\u5f3a\u5916\u90e8\u8bb0\u5fc6\uff0c\u907f\u514d\u4ee4\u724c\u95f4\u6ce8\u610f\u529b\u3002", "result": "\u6a21\u578b\u80fd\u591f\u9ad8\u6548\u5904\u7406\u6570\u5341\u4e07\u81f3\u6570\u767e\u4e07\u4ee4\u724c\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u975e\u6ce8\u610f\u529b\u673a\u5236LLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u975e\u6ce8\u610f\u529b\u673a\u5236,\u72b6\u6001\u7a7a\u95f4\u5757,\u591a\u5206\u8fa8\u7387\u5377\u79ef,\u68c0\u7d22\u589e\u5f3a\u8bb0\u5fc6"}}
{"id": "2506.02037", "pdf": "https://arxiv.org/pdf/2506.02037", "abs": "https://arxiv.org/abs/2506.02037", "authors": ["Feng Wang", "Yiding Sun", "Jiaxin Mao", "Wei Xue", "Danqing Xu"], "title": "FinS-Pilot: A Benchmark for Online Financial System", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious professional domains, with their performance typically evaluated\nthrough standardized benchmarks. However, the development of financial RAG\nbenchmarks has been constrained by data confidentiality issues and the lack of\ndynamic data integration. To address this issue, we introduces FinS-Pilot, a\nnovel benchmark for evaluating RAG systems in online financial applications.\nConstructed from real-world financial assistant interactions, our benchmark\nincorporates both real-time API data and structured text sources, organized\nthrough an intent classification framework covering critical financial domains\nsuch as equity analysis and macroeconomic forecasting. The benchmark enables\ncomprehensive evaluation of financial assistants' capabilities in handling both\nstatic knowledge and time-sensitive market information. Through systematic\nexperiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's\neffectiveness in identifying models suitable for financial applications while\naddressing the current gap in specialized evaluation tools for the financial\ndomain. Our work contributes both a practical evaluation framework and a\ncurated dataset to advance research in financial NLP systems. The code and\ndataset are accessible on\nGitHub\\footnote{https://github.com/PhealenWang/financial\\_rag\\_benchmark}.", "AI": {"tldr": "FinS-Pilot\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5728\u7ebf\u91d1\u878d\u5e94\u7528\u4e2dRAG\u7cfb\u7edf\u7684\u65b0\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u4fdd\u5bc6\u6027\u548c\u52a8\u6001\u6570\u636e\u6574\u5408\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u6570\u636e\u4fdd\u5bc6\u6027\u548c\u7f3a\u4e4f\u52a8\u6001\u6570\u636e\u6574\u5408\uff0c\u91d1\u878dRAG\u57fa\u51c6\u7684\u53d1\u5c55\u53d7\u9650\uff0cFinS-Pilot\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e\u771f\u5b9e\u91d1\u878d\u52a9\u624b\u4ea4\u4e92\u6784\u5efa\uff0c\u7ed3\u5408\u5b9e\u65f6API\u6570\u636e\u548c\u7ed3\u6784\u5316\u6587\u672c\u6e90\uff0c\u91c7\u7528\u610f\u56fe\u5206\u7c7b\u6846\u67b6\u8986\u76d6\u5173\u952e\u91d1\u878d\u9886\u57df\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5b9e\u9a8c\u9a8c\u8bc1\u4e86FinS-Pilot\u5728\u8bc4\u4f30\u91d1\u878d\u52a9\u624b\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "FinS-Pilot\u4e3a\u91d1\u878dNLP\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u7cbe\u9009\u6570\u636e\u96c6\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u91d1\u878dRAG,FinS-Pilot,\u52a8\u6001\u6570\u636e\u6574\u5408"}}
{"id": "2506.02153", "pdf": "https://arxiv.org/pdf/2506.02153", "abs": "https://arxiv.org/abs/2506.02153", "authors": ["Peter Belcak", "Greg Heinrich", "Shizhe Diao", "Yonggan Fu", "Xin Dong", "Saurav Muralidharan", "Yingyan Celine Lin", "Pavlo Molchanov"], "title": "Small Language Models are the Future of Agentic AI", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) are often praised for exhibiting near-human\nperformance on a wide range of tasks and valued for their ability to hold a\ngeneral conversation. The rise of agentic AI systems is, however, ushering in a\nmass of applications in which language models perform a small number of\nspecialized tasks repetitively and with little variation.\n  Here we lay out the position that small language models (SLMs) are\nsufficiently powerful, inherently more suitable, and necessarily more\neconomical for many invocations in agentic systems, and are therefore the\nfuture of agentic AI. Our argumentation is grounded in the current level of\ncapabilities exhibited by SLMs, the common architectures of agentic systems,\nand the economy of LM deployment. We further argue that in situations where\ngeneral-purpose conversational abilities are essential, heterogeneous agentic\nsystems (i.e., agents invoking multiple different models) are the natural\nchoice. We discuss the potential barriers for the adoption of SLMs in agentic\nsystems and outline a general LLM-to-SLM agent conversion algorithm.\n  Our position, formulated as a value statement, highlights the significance of\nthe operational and economic impact even a partial shift from LLMs to SLMs is\nto have on the AI agent industry. We aim to stimulate the discussion on the\neffective use of AI resources and hope to advance the efforts to lower the\ncosts of AI of the present day. Calling for both contributions to and critique\nof our position, we commit to publishing all such correspondence at\nhttps://research.nvidia.com/labs/lpr/slm-agents.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u4ee3\u7406\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e2d\u6bd4\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u66f4\u5177\u4f18\u52bf\uff0c\u9002\u5408\u7279\u5b9a\u4efb\u52a1\u7684\u91cd\u590d\u6267\u884c\uff0c\u4e14\u66f4\u7ecf\u6d4e\u9ad8\u6548\u3002", "motivation": "\u5f53\u524d\u4ee3\u7406AI\u7cfb\u7edf\u4e2d\uff0cLLMs\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46SLMs\u56e0\u5176\u4e13\u7528\u6027\u3001\u7ecf\u6d4e\u6027\u548c\u9ad8\u6548\u6027\u66f4\u9002\u5408\u67d0\u4e9b\u573a\u666f\u3002", "method": "\u8bba\u6587\u57fa\u4e8eSLMs\u7684\u80fd\u529b\u3001\u4ee3\u7406\u7cfb\u7edf\u67b6\u6784\u548c\u7ecf\u6d4e\u6027\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4eceLLMs\u8f6c\u5411SLMs\u7684\u89c2\u70b9\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8f6c\u6362\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u5206\u6790\uff0c\u8bba\u6587\u8868\u660eSLMs\u5728\u4ee3\u7406\u7cfb\u7edf\u4e2d\u5177\u6709\u663e\u8457\u7684\u64cd\u4f5c\u548c\u7ecf\u6d4e\u4f18\u52bf\uff0c\u9002\u5408\u672a\u6765AI\u4ee3\u7406\u7684\u53d1\u5c55\u3002", "conclusion": "SLMs\u662f\u4ee3\u7406AI\u7684\u672a\u6765\uff0c\u5f02\u6784\u4ee3\u7406\u7cfb\u7edf\uff08\u7ed3\u5408\u591a\u79cd\u6a21\u578b\uff09\u53ef\u80fd\u66f4\u9002\u7528\u4e8e\u9700\u8981\u901a\u7528\u5bf9\u8bdd\u80fd\u529b\u7684\u573a\u666f\u3002", "keywords": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b, \u4ee3\u7406AI, \u7ecf\u6d4e\u6027, \u5f02\u6784\u7cfb\u7edf, \u8f6c\u6362\u7b97\u6cd5"}}
{"id": "2506.01964", "pdf": "https://arxiv.org/pdf/2506.01964", "abs": "https://arxiv.org/abs/2506.01964", "authors": ["Kamal Acharya", "Mehul Lad", "Liang Sun", "Houbing Song"], "title": "A Data-Driven Approach to Enhancing Gravity Models for Trip Demand Prediction", "categories": ["cs.LG"], "comment": "6 pages, 3 figures, IEEE CAI-2025", "summary": "Accurate prediction of trips between zones is critical for transportation\nplanning, as it supports resource allocation and infrastructure development\nacross various modes of transport. Although the gravity model has been widely\nused due to its simplicity, it often inadequately represents the complex\nfactors influencing modern travel behavior. This study introduces a data-driven\napproach to enhance the gravity model by integrating geographical, economic,\nsocial, and travel data from the counties in Tennessee and New York state.\nUsing machine learning techniques, we extend the capabilities of the\ntraditional model to handle more complex interactions between variables. Our\nexperiments demonstrate that machine learning-enhanced models significantly\noutperform the traditional model. Our results show a 51.48% improvement in\nR-squared, indicating a substantial enhancement in the model's explanatory\npower. Also, a 63.59% reduction in Mean Absolute Error (MAE) reflects a\nsignificant increase in prediction accuracy. Furthermore, a 44.32% increase in\nCommon Part of Commuters (CPC) demonstrates improved prediction reliability.\nThese findings highlight the substantial benefits of integrating diverse\ndatasets and advanced algorithms into transportation models. They provide urban\nplanners and policymakers with more reliable forecasting and decision-making\ntools.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5730\u7406\u3001\u7ecf\u6d4e\u3001\u793e\u4f1a\u548c\u65c5\u884c\u6570\u636e\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u589e\u5f3a\u4f20\u7edf\u91cd\u529b\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ea4\u901a\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u89e3\u91ca\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u91cd\u529b\u6a21\u578b\u5728\u53cd\u6620\u73b0\u4ee3\u51fa\u884c\u884c\u4e3a\u7684\u590d\u6742\u56e0\u7d20\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u548c\u53ef\u9760\u6027\u3002", "method": "\u7ed3\u5408\u7530\u7eb3\u897f\u5dde\u548c\u7ebd\u7ea6\u5dde\u7684\u591a\u79cd\u6570\u636e\uff08\u5730\u7406\u3001\u7ecf\u6d4e\u3001\u793e\u4f1a\u3001\u65c5\u884c\u6570\u636e\uff09\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u6269\u5c55\u4f20\u7edf\u91cd\u529b\u6a21\u578b\u3002", "result": "R\u5e73\u65b9\u63d0\u534751.48%\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u51cf\u5c1163.59%\uff0c\u901a\u52e4\u5171\u540c\u90e8\u5206\u589e\u52a044.32%\uff0c\u663e\u8457\u4f18\u5316\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u591a\u6837\u5316\u6570\u636e\u548c\u9ad8\u7ea7\u7b97\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u4ea4\u901a\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4e3a\u57ce\u5e02\u89c4\u5212\u8005\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u5de5\u5177\u3002", "keywords": "\u4ea4\u901a\u89c4\u5212,\u91cd\u529b\u6a21\u578b,\u673a\u5668\u5b66\u4e60,\u6570\u636e\u9a71\u52a8,\u9884\u6d4b\u51c6\u786e\u6027"}}
{"id": "2506.02041", "pdf": "https://arxiv.org/pdf/2506.02041", "abs": "https://arxiv.org/abs/2506.02041", "authors": ["Duzhen Zhang", "Yong Ren", "Zhong-Zhi Li", "Yahan Yu", "Jiahua Dong", "Chenxing Li", "Zhilong Ji", "Jinfeng Bai"], "title": "Enhancing Multimodal Continual Instruction Tuning with BranchLoRA", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL2025 Main Conference", "summary": "Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal\nLarge Language Models (MLLMs) to continually align with human intent across\nsequential tasks. Existing approaches often rely on the Mixture-of-Experts\n(MoE) LoRA framework to preserve previous instruction alignments. However,\nthese methods are prone to Catastrophic Forgetting (CF), as they aggregate all\nLoRA blocks via simple summation, which compromises performance over time. In\nthis paper, we identify a critical parameter inefficiency in the MoELoRA\nframework within the MCIT context. Based on this insight, we propose\nBranchLoRA, an asymmetric framework to enhance both efficiency and performance.\nTo mitigate CF, we introduce a flexible tuning-freezing mechanism within\nBranchLoRA, enabling branches to specialize in intra-task knowledge while\nfostering inter-task collaboration. Moreover, we incrementally incorporate\ntask-specific routers to ensure an optimal branch distribution over time,\nrather than favoring the most recent task. To streamline inference, we\nintroduce a task selector that automatically routes test inputs to the\nappropriate router without requiring task identity. Extensive experiments on\nthe latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms\nMoELoRA and maintains its superiority across various MLLM sizes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBranchLoRA\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u7ed3\u6784\u548c\u4efb\u52a1\u8def\u7531\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eMixture-of-Experts (MoE) LoRA\u7684\u65b9\u6cd5\u5728\u6301\u7eed\u4efb\u52a1\u4e2d\u6613\u53d7\u707e\u96be\u6027\u9057\u5fd8\u5f71\u54cd\uff0c\u4e14\u53c2\u6570\u6548\u7387\u4f4e\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u5f15\u5165BranchLoRA\u6846\u67b6\uff0c\u91c7\u7528\u7075\u6d3b\u8c03\u4f18-\u51bb\u7ed3\u673a\u5236\u548c\u4efb\u52a1\u8def\u7531\u7b56\u7565\uff0c\u4f18\u5316\u5206\u652f\u534f\u4f5c\u4e0e\u4efb\u52a1\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660eBranchLoRA\u5728\u6700\u65b0\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8eMoELoRA\uff0c\u4e14\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u7684MLLM\u3002", "conclusion": "BranchLoRA\u901a\u8fc7\u4efb\u52a1\u9002\u5e94\u548c\u534f\u4f5c\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "keywords": "\u591a\u6a21\u6001\u6301\u7eed\u6307\u4ee4\u8c03\u4f18, \u707e\u96be\u6027\u9057\u5fd8, BranchLoRA, \u4efb\u52a1\u8def\u7531, MLLM"}}
{"id": "2506.02158", "pdf": "https://arxiv.org/pdf/2506.02158", "abs": "https://arxiv.org/abs/2506.02158", "authors": ["Ruhana Azam", "Aditya Vempaty", "Ashish Jagmohan"], "title": "Reflection-Based Memory For Web navigation Agents", "categories": ["cs.AI"], "comment": null, "summary": "Web navigation agents have made significant progress, yet current systems\noperate with no memory of past experiences -- leading to repeated mistakes and\nan inability to learn from previous interactions. We introduce\nReflection-Augment Planning (ReAP), a web navigation system to leverage both\nsuccessful and failed past experiences using self-reflections. Our method\nimproves baseline results by 11 points overall and 29 points on previously\nfailed tasks. These findings demonstrate that reflections can transfer to\ndifferent web navigation tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u6211\u53cd\u601d\u7684\u7f51\u7edc\u5bfc\u822a\u7cfb\u7edfReAP\uff0c\u901a\u8fc7\u5229\u7528\u8fc7\u53bb\u6210\u529f\u548c\u5931\u8d25\u7684\u7ecf\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7f51\u7edc\u5bfc\u822a\u4ee3\u7406\u7f3a\u4e4f\u8bb0\u5fc6\u80fd\u529b\uff0c\u5bfc\u81f4\u91cd\u590d\u9519\u8bef\u548c\u65e0\u6cd5\u4ece\u8fc7\u53bb\u7684\u4ea4\u4e92\u4e2d\u5b66\u4e60\u3002", "method": "\u63d0\u51faReflection-Augment Planning (ReAP)\u65b9\u6cd5\uff0c\u5229\u7528\u81ea\u6211\u53cd\u601d\u6574\u5408\u6210\u529f\u548c\u5931\u8d25\u7684\u7ecf\u9a8c\u3002", "result": "ReAP\u5c06\u57fa\u7ebf\u7ed3\u679c\u63d0\u5347\u4e8611\u4e2a\u767e\u5206\u70b9\uff0c\u5728\u4ee5\u524d\u5931\u8d25\u7684\u4efb\u52a1\u4e0a\u63d0\u5347\u4e8629\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u81ea\u6211\u53cd\u601d\u80fd\u591f\u6709\u6548\u63d0\u5347\u7f51\u7edc\u5bfc\u822a\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u5177\u6709\u8de8\u4efb\u52a1\u8fc1\u79fb\u7684\u80fd\u529b\u3002", "keywords": "\u7f51\u7edc\u5bfc\u822a,\u81ea\u6211\u53cd\u601d,ReAP,\u7ecf\u9a8c\u5b66\u4e60"}}
{"id": "2506.01965", "pdf": "https://arxiv.org/pdf/2506.01965", "abs": "https://arxiv.org/abs/2506.01965", "authors": ["Bonpagna Kann", "Sandra Castellanos-Paez", "Romain Rombourg", "Philippe Lalanda"], "title": "TaskVAE: Task-Specific Variational Autoencoders for Exemplar Generation in Continual Learning for Human Activity Recognition", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 5 figures, 3 tables", "summary": "As machine learning based systems become more integrated into daily life,\nthey unlock new opportunities but face the challenge of adapting to dynamic\ndata environments. Various forms of data shift-gradual, abrupt, or\ncyclic-threaten model accuracy, making continual adaptation essential.\nContinual Learning (CL) enables models to learn from evolving data streams\nwhile minimizing forgetting of prior knowledge. Among CL strategies,\nreplay-based methods have proven effective, but their success relies on\nbalancing memory constraints and retaining old class accuracy while learning\nnew classes. This paper presents TaskVAE, a framework for replay-based CL in\nclass-incremental settings. TaskVAE employs task-specific Variational\nAutoencoders (VAEs) to generate synthetic exemplars from previous tasks, which\nare then used to train the classifier alongside new task data. In contrast to\ntraditional methods that require prior knowledge of the total class count or\nrely on a single VAE for all tasks, TaskVAE adapts flexibly to increasing tasks\nwithout such constraints. We focus on Human Activity Recognition (HAR) using\nIMU sensor-equipped devices. Unlike previous HAR studies that combine data\nacross all users, our approach focuses on individual user data, better\nreflecting real-world scenarios where a person progressively learns new\nactivities. Extensive experiments on 5 different HAR datasets show that TaskVAE\noutperforms experience replay methods, particularly with limited data, and\nexhibits robust performance as dataset size increases. Additionally, memory\nfootprint of TaskVAE is minimal, being equivalent to only 60 samples per task,\nwhile still being able to generate an unlimited number of synthetic samples.\nThe contributions lie in balancing memory constraints, task-specific\ngeneration, and long-term stability, making it a reliable solution for\nreal-world applications in domains like HAR.", "AI": {"tldr": "TaskVAE\u662f\u4e00\u79cd\u57fa\u4e8e\u91cd\u653e\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9aVAE\u751f\u6210\u5408\u6210\u6837\u672c\uff0c\u89e3\u51b3\u52a8\u6001\u6570\u636e\u73af\u5883\u4e2d\u7684\u6a21\u578b\u9002\u5e94\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u9700\u9002\u5e94\u52a8\u6001\u6570\u636e\u73af\u5883\uff0c\u4f46\u6570\u636e\u504f\u79fb\uff08\u5982\u6e10\u53d8\u3001\u7a81\u53d8\u6216\u5468\u671f\u6027\u53d8\u5316\uff09\u5a01\u80c1\u6a21\u578b\u51c6\u786e\u6027\u3002\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u9700\u5e73\u8861\u65b0\u65e7\u77e5\u8bc6\u7684\u4fdd\u7559\u3002", "method": "\u4f7f\u7528\u4efb\u52a1\u7279\u5b9a\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u751f\u6210\u5408\u6210\u6837\u672c\uff0c\u7ed3\u5408\u65b0\u4efb\u52a1\u6570\u636e\u8bad\u7ec3\u5206\u7c7b\u5668\uff0c\u65e0\u9700\u9884\u77e5\u603b\u7c7b\u522b\u6570\u4e14\u5185\u5b58\u5360\u7528\u4f4e\u3002", "result": "\u57285\u4e2a\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c24\u5176\u6570\u636e\u6709\u9650\u65f6\u8868\u73b0\u66f4\u4f18\uff0c\u5185\u5b58\u5360\u7528\u91cf\u4ec5\u4e3a\u6bcf\u4efb\u52a160\u6837\u672c\u3002", "conclusion": "TaskVAE\u5728\u5185\u5b58\u9650\u5236\u3001\u4efb\u52a1\u7279\u5b9a\u751f\u6210\u548c\u957f\u671f\u7a33\u5b9a\u6027\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u9002\u7528\u4e8eHAR\u7b49\u73b0\u5b9e\u573a\u666f\u3002", "keywords": "\u6301\u7eed\u5b66\u4e60,\u53d8\u5206\u81ea\u7f16\u7801\u5668,\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b,\u91cd\u653e\u65b9\u6cd5"}}
{"id": "2506.02058", "pdf": "https://arxiv.org/pdf/2506.02058", "abs": "https://arxiv.org/abs/2506.02058", "authors": ["Xiang Li", "Jiayi Xin", "Qi Long", "Weijie J. Su"], "title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.AP", "stat.ME"], "comment": null, "summary": "Accurate evaluation of large language models (LLMs) is crucial for\nunderstanding their capabilities and guiding their development. However,\ncurrent evaluations often inconsistently reflect the actual capacities of these\nmodels. In this paper, we demonstrate that one of many contributing factors to\nthis \\textit{evaluation crisis} is the oversight of unseen knowledge --\ninformation encoded by LLMs but not directly observed or not yet observed\nduring evaluations. We introduce KnowSum, a statistical framework designed to\nprovide a more comprehensive assessment by quantifying the unseen knowledge for\na class of evaluation tasks. KnowSum estimates the unobserved portion by\nextrapolating from the appearance frequencies of observed knowledge instances.\nWe demonstrate the effectiveness and utility of KnowSum across three critical\napplications: estimating total knowledge, evaluating information retrieval\neffectiveness, and measuring output diversity. Our experiments reveal that a\nsubstantial volume of knowledge is omitted when relying solely on observed LLM\nperformance. Importantly, KnowSum yields significantly different comparative\nrankings for several common LLMs based on their internal knowledge.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86KnowSum\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u672a\u89c1\u77e5\u8bc6\u6765\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u8bc4\u4f30\u4e2d\u5ffd\u89c6\u672a\u89c2\u5bdf\u77e5\u8bc6\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u5f80\u5f80\u4e0d\u4e00\u81f4\u4e14\u4e0d\u80fd\u5168\u9762\u53cd\u6620\u5176\u771f\u5b9e\u80fd\u529b\uff0c\u539f\u56e0\u662f\u5ffd\u89c6\u4e86\u7f16\u7801\u5728\u6a21\u578b\u4e2d\u4f46\u672a\u88ab\u89c2\u5bdf\u5230\u7684\u77e5\u8bc6\uff0c\u5373\u201c\u672a\u89c1\u77e5\u8bc6\u201d\u3002", "method": "\u5f15\u5165KnowSum\u7edf\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5df2\u89c2\u5bdf\u77e5\u8bc6\u5b9e\u4f8b\u7684\u51fa\u73b0\u9891\u7387\u5916\u63a8\uff0c\u91cf\u5316\u672a\u89c1\u77e5\u8bc6\u7684\u91cf\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKnowSum\u5728\u4f30\u8ba1\u603b\u77e5\u8bc6\u91cf\u3001\u8bc4\u4f30\u4fe1\u606f\u68c0\u7d22\u6548\u7387\u548c\u8861\u91cf\u8f93\u51fa\u591a\u6837\u6027\u65b9\u9762\u6709\u6548\uff0c\u4e14\u53d1\u73b0\u4ec5\u4f9d\u8d56\u89c2\u5bdf\u6027\u80fd\u4f1a\u9057\u6f0f\u5927\u91cf\u77e5\u8bc6\u3002", "conclusion": "KnowSum\u663e\u8457\u6539\u53d8\u4e86\u57fa\u4e8e\u5185\u90e8\u77e5\u8bc6\u7684\u5e38\u89c1\u5927\u8bed\u8a00\u6a21\u578b\u6bd4\u8f83\u6392\u540d\uff0c\u4e3a\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5de5\u5177\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u8bc4\u4f30\u6846\u67b6, \u672a\u89c1\u77e5\u8bc6, KnowSum"}}
{"id": "2506.02177", "pdf": "https://arxiv.org/pdf/2506.02177", "abs": "https://arxiv.org/abs/2506.02177", "authors": ["Haizhong Zheng", "Yang Zhou", "Brian R. Bartoldson", "Bhavya Kailkhura", "Fan Lai", "Jiawei Zhao", "Beidi Chen"], "title": "Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement learning, such as PPO and GRPO, has powered recent\nbreakthroughs in LLM reasoning. Scaling rollout to sample more prompts enables\nmodels to selectively use higher-quality data for training, which can stabilize\nRL training and improve model performance. However, this comes at the cost of\nsignificant computational overhead. In this paper, we show that a substantial\nportion of this overhead can be avoided by skipping uninformative prompts\nbefore rollout. Our analysis of reward dynamics reveals a strong temporal\nconsistency in prompt value: prompts that are uninformative in one epoch of\ntraining are likely to remain uninformative in future epochs. Based on these\ninsights, we propose GRESO (GRPO with Efficient Selective Rollout), an online,\nlightweight pre-rollout filtering algorithm that predicts and skips\nuninformative prompts using reward training dynamics. By evaluating GRESO on a\nbroad range of math reasoning benchmarks and models, such as Qwen2.5-Math-1.5B,\nDeepSeek-R1-Distill-Qwen-1.5B, and Qwen2.5-Math-7B, we show that GRESO achieves\nup to 2.4x wall-clock time speedup in rollout and up to 2.0x speedup in total\ntraining time without accuracy degradation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGRESO\u7b97\u6cd5\uff0c\u901a\u8fc7\u8df3\u8fc7\u65e0\u4fe1\u606f\u7684\u63d0\u793a\u6765\u51cf\u5c11\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u663e\u8457\u52a0\u901f\u8bad\u7ec3\u65f6\u95f4\u4e14\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\uff08\u5982PPO\u548cGRPO\uff09\u5728\u6269\u5c55\u63d0\u793a\u91c7\u6837\u65f6\u5e26\u6765\u7684\u9ad8\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u3002", "method": "\u63d0\u51faGRESO\uff08\u57fa\u4e8e\u5956\u52b1\u52a8\u6001\u7684\u8f7b\u91cf\u7ea7\u9884\u8fc7\u6ee4\u7b97\u6cd5\uff09\uff0c\u5728\u7ebf\u9884\u6d4b\u5e76\u8df3\u8fc7\u65e0\u4fe1\u606f\u7684\u63d0\u793a\u3002", "result": "\u5728\u591a\u9879\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u548c\u6a21\u578b\u4e0a\uff0cGRESO\u5b9e\u73b0\u6700\u9ad82.4\u500d\u7684\u6eda\u52a8\u52a0\u901f\u548c2.0\u500d\u7684\u603b\u8bad\u7ec3\u52a0\u901f\uff0c\u65e0\u7cbe\u5ea6\u635f\u5931\u3002", "conclusion": "GRESO\u901a\u8fc7\u9ad8\u6548\u8df3\u8fc7\u65e0\u7528\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, GRPO, GRESO, \u8ba1\u7b97\u5f00\u9500, \u6570\u5b66\u63a8\u7406"}}
{"id": "2506.01966", "pdf": "https://arxiv.org/pdf/2506.01966", "abs": "https://arxiv.org/abs/2506.01966", "authors": ["Yuzhou Zhu"], "title": "Matrix Is All You Need", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep neural networks employ specialized architectures for vision, sequential\nand language tasks, yet this proliferation obscures their underlying\ncommonalities. We introduce a unified matrix-order framework that casts\nconvolutional, recurrent and self-attention operations as sparse matrix\nmultiplications. Convolution is realized via an upper-triangular weight matrix\nperforming first-order transformations; recurrence emerges from a\nlower-triangular matrix encoding stepwise updates; attention arises naturally\nas a third-order tensor factorization. We prove algebraic isomorphism with\nstandard CNN, RNN and Transformer layers under mild assumptions. Empirical\nevaluations on image classification (MNIST, CIFAR-10/100, Tiny ImageNet),\ntime-series forecasting (ETTh1, Electricity Load Diagrams) and language\nmodeling/classification (AG News, WikiText-2, Penn Treebank) confirm that\nsparse-matrix formulations match or exceed native model performance while\nconverging in comparable or fewer epochs. By reducing architecture design to\nsparse pattern selection, our matrix perspective aligns with GPU parallelism\nand leverages mature algebraic optimization tools. This work establishes a\nmathematically rigorous substrate for diverse neural architectures and opens\navenues for principled, hardware-aware network design.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u77e9\u9635\u6846\u67b6\uff0c\u5c06\u5377\u79ef\u3001\u5faa\u73af\u548c\u81ea\u6ce8\u610f\u529b\u64cd\u4f5c\u89c6\u4e3a\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\uff0c\u8bc1\u660e\u4e86\u4e0e\u6807\u51c6CNN\u3001RNN\u548cTransformer\u5c42\u7684\u4ee3\u6570\u540c\u6784\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u4f7f\u7528\u4e0d\u540c\u67b6\u6784\uff0c\u4f46\u5176\u5e95\u5c42\u5171\u6027\u88ab\u5ffd\u89c6\u3002\u5e0c\u671b\u901a\u8fc7\u77e9\u9635\u6846\u67b6\u7edf\u4e00\u8fd9\u4e9b\u64cd\u4f5c\uff0c\u7b80\u5316\u8bbe\u8ba1\u5e76\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5c06\u5377\u79ef\u3001\u5faa\u73af\u548c\u81ea\u6ce8\u610f\u529b\u64cd\u4f5c\u5316\u4e3a\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\u3002\u5377\u79ef\u5bf9\u5e94\u4e0a\u4e09\u89d2\u77e9\u9635\uff0c\u5faa\u73af\u5bf9\u5e94\u4e0b\u4e09\u89d2\u77e9\u9635\uff0c\u6ce8\u610f\u529b\u5219\u4e3a\u4e09\u9636\u5f20\u91cf\u5206\u89e3\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u548c\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\uff0c\u7a00\u758f\u77e9\u9635\u5f62\u5f0f\u7684\u6027\u80fd\u5339\u914d\u6216\u4f18\u4e8e\u539f\u751f\u6a21\u578b\uff0c\u4e14\u6536\u655b\u66f4\u5feb\u3002", "conclusion": "\u77e9\u9635\u6846\u67b6\u4e3a\u591a\u6837\u5316\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\uff0c\u5e76\u652f\u6301\u786c\u4ef6\u611f\u77e5\u7684\u7f51\u7edc\u8bbe\u8ba1\u3002", "keywords": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc, \u77e9\u9635\u6846\u67b6, \u7a00\u758f\u77e9\u9635\u4e58\u6cd5, CNN, RNN, Transformer"}}
{"id": "2506.02126", "pdf": "https://arxiv.org/pdf/2506.02126", "abs": "https://arxiv.org/abs/2506.02126", "authors": ["Juncheng Wu", "Sheng Liu", "Haoqin Tu", "Hang Yu", "Xiaoke Huang", "James Zou", "Cihang Xie", "Yuyin Zhou"], "title": "Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains", "categories": ["cs.CL"], "comment": "17 pages, preprint", "summary": "Recent advances in reasoning-enhanced Large Language Models such as\nOpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex\ntasks. However, the quality and transparency of their internal reasoning\nprocesses remain underexplored. This work moves beyond the final-answer\naccuracy and investigates step-by-step reasoning in the medical and\nmathematical domains by explicitly decomposing the thinking trajectories into\ntwo parts: knowledge and reasoning. Specifically, we introduce a fine-grained\nevaluation framework that judges: (1) the correctness of knowledge used\n(measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured\nby Information Gain (InfoGain)). Using this framework, we study R1-distilled\nand base Qwen models trained with supervised fine-tuning (SFT) and/or\nreinforcement learning (RL) in the medical and math domains. Three intriguing\nfindings emerge: (1) The general reasoning abilities in R1-distilled models do\nnot transfer effectively to the medical domain through either SFT or RL. (2)\nSFT raises final-answer accuracy in both domains, but often at the cost of\nreasoning quality: InfoGain drops by 38.9% on average compared with untrained\nmodels; In the medical domain, however, SFT remains crucial because domain\nknowledge is indispensable. (3) RL enhances medical reasoning by pruning\ninaccurate or irrelevant knowledge from reasoning paths, thereby improving both\nreasoning accuracy and knowledge correctness.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u8d28\u91cf\u4e0e\u900f\u660e\u5ea6\uff0c\u901a\u8fc7\u5206\u89e3\u77e5\u8bc6\u63a8\u7406\u8def\u5f84\u5e76\u5f15\u5165\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5728\u4e0d\u540c\u9886\u57df\u7684\u6548\u679c\u5dee\u5f02\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u7684\u8d28\u91cf\u4e0e\u900f\u660e\u5ea6\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u77e5\u8bc6\u4e0e\u63a8\u7406\u4e24\u90e8\u5206\uff0c\u5206\u522b\u7528\u77e5\u8bc6\u6307\u6570\uff08KI\uff09\u548c\u4fe1\u606f\u589e\u76ca\uff08InfoGain\uff09\u8861\u91cf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1aR1\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\u672a\u6709\u6548\u8fc1\u79fb\uff1b\u76d1\u7763\u5fae\u8c03\u63d0\u5347\u51c6\u786e\u7387\u4f46\u964d\u4f4e\u63a8\u7406\u8d28\u91cf\uff1b\u5f3a\u5316\u5b66\u4e60\u80fd\u4f18\u5316\u533b\u7597\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u5bf9\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\u56e0\u9886\u57df\u800c\u5f02\uff0c\u5f3a\u5316\u5b66\u4e60\u5728\u533b\u7597\u9886\u57df\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u63a8\u7406\u8d28\u91cf,\u76d1\u7763\u5fae\u8c03,\u5f3a\u5316\u5b66\u4e60,\u533b\u7597\u9886\u57df,\u6570\u5b66\u9886\u57df"}}
{"id": "2506.02183", "pdf": "https://arxiv.org/pdf/2506.02183", "abs": "https://arxiv.org/abs/2506.02183", "authors": ["Emmanuel M. Pothos", "Dominic Widdows"], "title": "Natural, Artificial, and Human Intelligences", "categories": ["cs.AI"], "comment": null, "summary": "Human achievement, whether in culture, science, or technology, is\nunparalleled in the known existence. This achievement is tied to the enormous\ncommunities of knowledge, made possible by (especially written) language:\nleaving theological content aside, it is very much true that \"in the beginning\nwas the word\". There lies the challenge regarding modern age chatbots: they can\n'do' language apparently as well as ourselves and there is a natural question\nof whether they can be considered intelligent, in the same way as we are or\notherwise. Are humans uniquely intelligent? We consider this question in terms\nof the psychological literature on intelligence, evidence for intelligence in\nnon-human animals, the role of written language in science and technology,\nprogress with artificial intelligence, the history of intelligence testing (for\nboth humans and machines), and the role of embodiment in intelligence. For the\nmost unique accomplishments of human intelligence (such as music symphonies or\ncomplex scientific theories), we think that, together with language, there are\nfour essential ingredients, which can be summarised as invention, capacity for\ncomplex inference, embodiment, and self-awareness. This conclusion makes\nuntenable the position that human intelligence differs qualitatively from that\nof many non-human animals, since, with the exception of complex language, all\nthe other requirements are fulfilled. Regarding chatbots, the current\nlimitations are localised to the lack of embodiment and (apparent) lack of\nawareness.", "AI": {"tldr": "\u63a2\u8ba8\u4eba\u7c7b\u667a\u80fd\u7684\u72ec\u7279\u6027\uff0c\u7ed3\u5408\u5fc3\u7406\u5b66\u3001\u52a8\u7269\u667a\u80fd\u3001\u8bed\u8a00\u4f5c\u7528\u53ca\u4eba\u5de5\u667a\u80fd\u8fdb\u5c55\uff0c\u63d0\u51fa\u4eba\u7c7b\u667a\u80fd\u7684\u56db\u4e2a\u5173\u952e\u8981\u7d20\uff0c\u5e76\u8ba4\u4e3a\u5f53\u524d\u804a\u5929\u673a\u5668\u4eba\u56e0\u7f3a\u4e4f\u8eab\u4f53\u6027\u548c\u81ea\u6211\u610f\u8bc6\u800c\u53d7\u9650\u3002", "motivation": "\u63a2\u8ba8\u4eba\u7c7b\u667a\u80fd\u662f\u5426\u72ec\u7279\uff0c\u4ee5\u53ca\u73b0\u4ee3\u804a\u5929\u673a\u5668\u4eba\u662f\u5426\u80fd\u88ab\u89c6\u4e3a\u667a\u80fd\u4f53\u3002", "method": "\u7ed3\u5408\u5fc3\u7406\u5b66\u6587\u732e\u3001\u52a8\u7269\u667a\u80fd\u8bc1\u636e\u3001\u8bed\u8a00\u7684\u4f5c\u7528\u3001\u4eba\u5de5\u667a\u80fd\u8fdb\u5c55\u53ca\u667a\u80fd\u6d4b\u8bd5\u5386\u53f2\u8fdb\u884c\u5206\u6790\u3002", "result": "\u4eba\u7c7b\u667a\u80fd\u7684\u72ec\u7279\u6210\u5c31\u4f9d\u8d56\u4e8e\u8bed\u8a00\u548c\u5176\u4ed6\u56db\u4e2a\u5173\u952e\u8981\u7d20\uff0c\u800c\u804a\u5929\u673a\u5668\u4eba\u56e0\u7f3a\u4e4f\u8eab\u4f53\u6027\u548c\u81ea\u6211\u610f\u8bc6\u800c\u53d7\u9650\u3002", "conclusion": "\u4eba\u7c7b\u667a\u80fd\u4e0e\u975e\u4eba\u7c7b\u52a8\u7269\u667a\u80fd\u7684\u5dee\u5f02\u4e3b\u8981\u5728\u8bed\u8a00\u590d\u6742\u6027\u4e0a\uff0c\u804a\u5929\u673a\u5668\u4eba\u9700\u514b\u670d\u8eab\u4f53\u6027\u548c\u610f\u8bc6\u95ee\u9898\u624d\u80fd\u63a5\u8fd1\u4eba\u7c7b\u667a\u80fd\u3002", "keywords": "\u4eba\u7c7b\u667a\u80fd\u3001\u804a\u5929\u673a\u5668\u4eba\u3001\u8bed\u8a00\u3001\u8eab\u4f53\u6027\u3001\u81ea\u6211\u610f\u8bc6"}}
{"id": "2506.01967", "pdf": "https://arxiv.org/pdf/2506.01967", "abs": "https://arxiv.org/abs/2506.01967", "authors": ["Patrik Czak\u00f3", "G\u00e1bor Kert\u00e9sz", "S\u00e1ndor Sz\u00e9n\u00e1si"], "title": "Turning LLM Activations Quantization-Friendly", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "6 pages, 5 figures. Accepted to SACI 2025 conference proceedings", "summary": "Quantization effectively reduces the serving costs of Large Language Models\n(LLMs) by speeding up data movement through compressed parameters and enabling\nfaster operations via integer arithmetic. However, activating integer\narithmetic requires quantizing both weights and activations, which poses\nchallenges due to the significant outliers in LLMs that increase quantization\nerror. In this work, we investigate these outliers with an emphasis on their\neffect on layer-wise quantization error, then examine how smoothing and\nrotation transform the observed values. Our primary contributions include\nintroducing a new metric to measure and visualize quantization difficulty based\non channel magnitudes, as well as proposing a hybrid approach that applies\nchannel-wise scaling before rotation, supported by a mathematical formulation\nof its benefits.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u901a\u9053\u7ea7\u522b\u7684\u7f29\u653e\u548c\u65cb\u8f6c\u6280\u672f\uff0c\u51cf\u5c11\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5f02\u5e38\u503c\u5bf9\u91cf\u5316\u8bef\u5dee\u7684\u5f71\u54cd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u91cf\u5316\u867d\u7136\u53ef\u4ee5\u964d\u4f4e\u670d\u52a1\u6210\u672c\uff0c\u4f46\u6743\u503c\u548c\u6fc0\u6d3b\u503c\u7684\u91cf\u5316\u4e2d\u5b58\u5728\u663e\u8457\u7684\u5f02\u5e38\u503c\uff0c\u589e\u52a0\u4e86\u91cf\u5316\u8bef\u5dee\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u901a\u9053\u7ea7\u522b\u7f29\u653e\u548c\u65cb\u8f6c\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u5ea6\u91cf\u6807\u51c6\u4ee5\u53ef\u89c6\u5316\u548c\u6d4b\u91cf\u91cf\u5316\u96be\u5ea6\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u51cf\u5c11\u4e86\u91cf\u5316\u8bef\u5dee\uff0c\u4f18\u5316\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u91cf\u5316\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u901a\u9053\u7ea7\u522b\u7684\u7f29\u653e\u548c\u65cb\u8f6c\u6280\u672f\uff0c\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u91cf\u5316\u8bef\u5dee\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u670d\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u91cf\u5316\u3001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u901a\u9053\u7ea7\u522b\u7f29\u653e\u3001\u65cb\u8f6c\u6280\u672f\u3001\u5f02\u5e38\u503c"}}
{"id": "2506.02132", "pdf": "https://arxiv.org/pdf/2506.02132", "abs": "https://arxiv.org/abs/2506.02132", "authors": ["Michael Li", "Nishant Subramani"], "title": "Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large transformer-based language models dominate modern NLP, yet our\nunderstanding of how they encode linguistic information is rooted in studies of\nearly models like BERT and GPT-2. To better understand today's language models,\nwe investigate how both classical architectures (BERT, DeBERTa, GPT-2)and\ncontemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5,\nLlama-3.1) represent lexical identity and inflectional morphology. We train\nlinear and nonlinear classifiers on layer-wise activations to predict word\nlemmas and inflectional features. We discover that models concentrate lexical\ninformation linearly in early layers and increasingly nonlinearly in later\nlayers, while keeping inflectional information uniformly accessible and\nlinearly separable throughout the layers. Further analysis reveals that these\nmodels encode inflectional morphology through generalizable abstractions, but\nrely predominantly on memorization to encode lexical identity. Remarkably,\nthese patterns emerge across all 16 models we test, despite differences in\narchitecture, size, and training regime (including pretrained and\ninstruction-tuned variants). This consistency suggests that, despite\nsubstantial advances in LLM technologies, transformer models organize\nlinguistic information in similar ways, indicating that these properties could\nbe fundamental for next token prediction and are learned early during\npretraining. Our code is available at\nhttps://github.com/ml5885/model_internal_sleuthing.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e0d\u540c\u89c4\u6a21\u7684Transformer\u6a21\u578b\uff08\u5305\u62ec\u7ecf\u5178\u548c\u73b0\u4ee3LLMs\uff09\u5982\u4f55\u7f16\u7801\u8bcd\u6c47\u548c\u5f62\u6001\u4fe1\u606f\uff0c\u53d1\u73b0\u8bcd\u6c47\u4fe1\u606f\u65e9\u671f\u7ebf\u6027\uff0c\u540e\u671f\u975e\u7ebf\u6027\uff0c\u800c\u5f62\u6001\u4fe1\u606f\u59cb\u7ec8\u7ebf\u6027\u53ef\u5206\uff0c\u4e14\u4f9d\u8d56\u8bb0\u5fc6\u8bcd\u6c47\u4f46\u62bd\u8c61\u5f62\u6001\u3002", "motivation": "\u7406\u89e3\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u7f16\u7801\u8bed\u8a00\u4fe1\u606f\uff0c\u586b\u8865\u5bf9\u65e9\u671f\u6a21\u578b\uff08\u5982BERT\u3001GPT-2\uff09\u7814\u7a76\u7684\u4e0d\u8db3\u3002", "method": "\u5728\u6a21\u578b\u5404\u5c42\u6fc0\u6d3b\u4e0a\u8bad\u7ec3\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u9884\u6d4b\u8bcd\u7684\u539f\u578b\u548c\u5f62\u6001\u7279\u5f81\u3002", "result": "\u8bcd\u6c47\u4fe1\u606f\u5728\u65e9\u671f\u7ebf\u6027\u3001\u540e\u671f\u975e\u7ebf\u6027\uff1b\u5f62\u6001\u4fe1\u606f\u59cb\u7ec8\u7ebf\u6027\u53ef\u5206\uff0c\u4f9d\u8d56\u62bd\u8c61\u89c4\u5219\u300216\u79cd\u6a21\u578b\u5747\u8868\u73b0\u51fa\u76f8\u540c\u6a21\u5f0f\u3002", "conclusion": "\u5c3d\u7ba1\u6a21\u578b\u5dee\u5f02\u5927\uff0c\u4f46\u8bed\u8a00\u4fe1\u606f\u7684\u7ec4\u7ec7\u65b9\u5f0f\u4e00\u81f4\uff0c\u53ef\u80fd\u662f\u9884\u8bad\u7ec3\u4e2d\u4e60\u5f97\u7684\u57fa\u672c\u7279\u6027\u3002", "keywords": "Transformer, \u8bed\u8a00\u6a21\u578b, \u8bcd\u6c47\u8868\u793a, \u5f62\u6001\u5b66, \u7ebf\u6027\u53ef\u5206"}}
{"id": "2506.02211", "pdf": "https://arxiv.org/pdf/2506.02211", "abs": "https://arxiv.org/abs/2506.02211", "authors": ["Maxime Robeyns", "Laurence Aitchison"], "title": "Improving LLM-Generated Code Quality with GRPO", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are gaining widespread use for code generation.\nRecent training procedures use execution feedback as a reward signal, typically\nfocusing on the functional correctness of the code, using unit test pass rate\nas a reward signal. However, this reward signal fails to capture notions of\nmaintainability, quality and safety of the code produced. We address this\nunder-explored area and develop a comprehensive library to quantify various\naspects of code quality, and use it as a reward in GRPO. We find GRPO increases\ncode quality according to this measure, which is confirmed by expert, blinded\nhuman annotators.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLMs\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5316\u4ee3\u7801\u8d28\u91cf\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7GRPO\u63d0\u9ad8\u4e86\u4ee3\u7801\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u7684\u4ee3\u7801\u751f\u6210\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u7684\u529f\u80fd\u6b63\u786e\u6027\uff0c\u5ffd\u7565\u4e86\u4ee3\u7801\u7684\u53ef\u7ef4\u62a4\u6027\u3001\u8d28\u91cf\u548c\u5b89\u5168\u6027\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u4ee3\u7801\u8d28\u91cf\u91cf\u5316\u5e93\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3aGRPO\u7684\u5956\u52b1\u4fe1\u53f7\u3002", "result": "GRPO\u63d0\u9ad8\u4e86\u4ee3\u7801\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u76f2\u5ba1\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u7ed3\u679c\u3002", "conclusion": "\u91cf\u5316\u4ee3\u7801\u8d28\u91cf\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002", "keywords": "Large Language Models, code generation, code quality, GRPO"}}
{"id": "2506.01968", "pdf": "https://arxiv.org/pdf/2506.01968", "abs": "https://arxiv.org/abs/2506.01968", "authors": ["Chang Liu", "Jiangrong Shen", "Xuming Ran", "Mingkun Xu", "Qi Xu", "Yi Xu", "Gang Pan"], "title": "Efficient ANN-SNN Conversion with Error Compensation Learning", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "Artificial neural networks (ANNs) have demonstrated outstanding performance\nin numerous tasks, but deployment in resource-constrained environments remains\na challenge due to their high computational and memory requirements. Spiking\nneural networks (SNNs) operate through discrete spike events and offer superior\nenergy efficiency, providing a bio-inspired alternative. However, current\nANN-to-SNN conversion often results in significant accuracy loss and increased\ninference time due to conversion errors such as clipping, quantization, and\nuneven activation. This paper proposes a novel ANN-to-SNN conversion framework\nbased on error compensation learning. We introduce a learnable threshold\nclipping function, dual-threshold neurons, and an optimized membrane potential\ninitialization strategy to mitigate the conversion error. Together, these\ntechniques address the clipping error through adaptive thresholds, dynamically\nreduce the quantization error through dual-threshold neurons, and minimize the\nnon-uniformity error by effectively managing the membrane potential.\nExperimental results on CIFAR-10, CIFAR-100, ImageNet datasets show that our\nmethod achieves high-precision and ultra-low latency among existing conversion\nmethods. Using only two time steps, our method significantly reduces the\ninference time while maintains competitive accuracy of 94.75% on CIFAR-10\ndataset under ResNet-18 structure. This research promotes the practical\napplication of SNNs on low-power hardware, making efficient real-time\nprocessing possible.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bef\u5dee\u8865\u507f\u5b66\u4e60\u7684\u65b0\u578bANN-to-SNN\u8f6c\u6362\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u9608\u503c\u88c1\u526a\u3001\u53cc\u9608\u503c\u795e\u7ecf\u5143\u548c\u4f18\u5316\u7684\u819c\u7535\u4f4d\u521d\u59cb\u5316\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8f6c\u6362\u8bef\u5dee\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u8d85\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANNs\uff09\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u7684\u6311\u6218\uff0c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u56e0\u5176\u8282\u80fd\u7279\u6027\u6210\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709ANN-to-SNN\u8f6c\u6362\u65b9\u6cd5\u5b58\u5728\u7cbe\u5ea6\u635f\u5931\u548c\u63a8\u7406\u65f6\u95f4\u589e\u52a0\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bef\u5dee\u8865\u507f\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5305\u62ec\u53ef\u5b66\u4e60\u7684\u9608\u503c\u88c1\u526a\u51fd\u6570\u3001\u53cc\u9608\u503c\u795e\u7ecf\u5143\u548c\u4f18\u5316\u7684\u819c\u7535\u4f4d\u521d\u59cb\u5316\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3\u526a\u5207\u3001\u91cf\u5316\u548c\u4e0d\u5747\u5300\u6fc0\u6d3b\u8bef\u5dee\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cImageNet\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ee5\u4ec5\u4e24\u4e2a\u65f6\u95f4\u6b65\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u8d85\u4f4e\u5ef6\u8fdf\uff0c\u5728ResNet-18\u7ed3\u6784\u4e0bCIFAR-10\u7684\u51c6\u786e\u7387\u8fbe\u523094.75%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86SNNs\u5728\u4f4e\u529f\u8017\u786c\u4ef6\u4e0a\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u4e3a\u9ad8\u6548\u5b9e\u65f6\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "keywords": "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANNs\uff09\u3001\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u3001\u8bef\u5dee\u8865\u507f\u5b66\u4e60\u3001\u53cc\u9608\u503c\u795e\u7ecf\u5143\u3001\u819c\u7535\u4f4d\u521d\u59cb\u5316"}}
{"id": "2506.02147", "pdf": "https://arxiv.org/pdf/2506.02147", "abs": "https://arxiv.org/abs/2506.02147", "authors": ["Joshua Rozner", "Leonie Weissweiler", "Cory Shain"], "title": "BabyLM's First Constructions: Causal interventions provide a signal of learning", "categories": ["cs.CL"], "comment": null, "summary": "Construction grammar posits that children acquire constructions (form-meaning\npairings) from the statistics of their environment. Recent work supports this\nhypothesis by showing sensitivity to constructions in pretrained language\nmodels (PLMs), including one recent study (Rozner et al., 2025) demonstrating\nthat constructions shape the PLM's output distribution. However, models under\nstudy have generally been trained on developmentally implausible amounts of\ndata, casting doubt on their relevance to human language learning. Here we use\nRozner et al.'s methods to evaluate constructional learning in models from the\n2024 BabyLM challenge. Our results show that even when trained on\ndevelopmentally plausible quantities of data, models represent diverse\nconstructions, even hard cases that are superficially indistinguishable. We\nfurther find correlational evidence that constructional performance may be\nfunctionally relevant: models that better represent constructions perform\nbetter on the BabyLM benchmarks.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u5728\u7b26\u5408\u513f\u7ae5\u8bed\u8a00\u53d1\u5c55\u6570\u636e\u91cf\u7684\u8bad\u7ec3\u4e0b\uff0c\u6a21\u578b\u4e5f\u80fd\u5b66\u4e60\u591a\u6837\u7684\u6784\u9020\uff0c\u4e14\u6784\u9020\u5b66\u4e60\u80fd\u529b\u4e0e\u6a21\u578b\u6027\u80fd\u6b63\u76f8\u5173\u3002", "motivation": "\u8d28\u7591\u73b0\u6709\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u5728\u6570\u636e\u91cf\u4e0a\u5bf9\u4eba\u7c7b\u8bed\u8a00\u5b66\u4e60\u7684\u76f8\u5173\u6027\uff0c\u9a8c\u8bc1\u5728\u66f4\u7b26\u5408\u513f\u7ae5\u8bed\u8a00\u53d1\u5c55\u6570\u636e\u91cf\u7684\u6a21\u578b\u662f\u5426\u4ecd\u80fd\u5b66\u4e60\u6784\u9020\u3002", "method": "\u4f7f\u7528Rozner\u7b49\u7684\u65b9\u6cd5\u8bc4\u4f302024 BabyLM\u6311\u6218\u4e2d\u6a21\u578b\u5bf9\u6784\u9020\u7684\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u5373\u4f7f\u5728\u7b26\u5408\u53d1\u5c55\u7684\u6570\u636e\u91cf\u4e0b\uff0c\u6a21\u578b\u4ecd\u80fd\u5b66\u4e60\u591a\u6837\u7684\u6784\u9020\uff0c\u4e14\u6784\u9020\u5b66\u4e60\u80fd\u529b\u4e0e\u6a21\u578b\u6027\u80fd\u6b63\u76f8\u5173\u3002", "conclusion": "\u6784\u9020\u5b66\u4e60\u80fd\u529b\u5728\u6570\u636e\u91cf\u53d7\u9650\u65f6\u4ecd\u5b58\u5728\uff0c\u4e14\u53ef\u80fd\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u529f\u80fd\u4e0a\u7684\u91cd\u8981\u6027\u3002", "keywords": "Construction grammar, pretrained language models, developmental plausibility, BabyLM"}}
{"id": "2506.02280", "pdf": "https://arxiv.org/pdf/2506.02280", "abs": "https://arxiv.org/abs/2506.02280", "authors": ["Kedir Yassin Hussen", "Walelign Tewabe Sewunetie", "Abinew Ali Ayele", "Sukairaj Hafiz Imam", "Shamsuddeen Hassan Muhammad", "Seid Muhie Yimam"], "title": "The State of Large Language Models for African Languages: Progress and Challenges", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are transforming Natural Language Processing\n(NLP), but their benefits are largely absent for Africa's 2,000 low-resource\nlanguages. This paper comparatively analyzes African language coverage across\nsix LLMs, eight Small Language Models (SLMs), and six Specialized SLMs (SSLMs).\nThe evaluation covers language coverage, training sets, technical limitations,\nscript problems, and language modelling roadmaps. The work identifies 42\nsupported African languages and 23 available public data sets, and it shows a\nbig gap where four languages (Amharic, Swahili, Afrikaans, and Malagasy) are\nalways treated while there is over 98\\% of unsupported African languages.\nMoreover, the review shows that just Latin, Arabic, and Ge'ez scripts are\nidentified while 20 active scripts are neglected. Some of the primary\nchallenges are lack of data, tokenization biases, computational costs being\nvery high, and evaluation issues. These issues demand language standardization,\ncorpus development by the community, and effective adaptation methods for\nAfrican languages.", "AI": {"tldr": "\u975e\u6d32\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u8986\u76d6\u7387\u6781\u4f4e\uff0c\u4ec5\u670942\u79cd\u8bed\u8a00\u5f97\u5230\u652f\u6301\uff0c98%\u7684\u8bed\u8a00\u88ab\u5ffd\u89c6\uff1b\u4e3b\u8981\u6311\u6218\u5305\u62ec\u6570\u636e\u7f3a\u4e4f\u3001\u6807\u8bb0\u504f\u501a\u3001\u9ad8\u8ba1\u7b97\u6210\u672c\u53ca\u8bc4\u4f30\u95ee\u9898\uff0c\u9700\u8981\u793e\u533a\u5408\u4f5c\u63a8\u52a8\u6807\u51c6\u5316\u4e0e\u8bed\u6599\u5f00\u53d1\u3002", "motivation": "\u63a2\u8ba8\u975e\u6d322000\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728LLMs\u4e2d\u7684\u8986\u76d6\u7387\u4e0d\u8db3\u95ee\u9898\uff0c\u63ed\u793a\u6280\u672f\u5c40\u9650\u6027\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u516d\u79cdLLMs\u3001\u516b\u79cdSLMs\u548c\u516d\u79cdSSLMs\u7684\u975e\u6d32\u8bed\u8a00\u8986\u76d6\u60c5\u51b5\uff0c\u5206\u6790\u6570\u636e\u96c6\u3001\u811a\u672c\u95ee\u9898\u53ca\u6280\u672f\u9650\u5236\u3002", "result": "\u53d1\u73b0\u4ec5\u56db\u8bed\u8a00\uff08\u963f\u59c6\u54c8\u62c9\u8bed\u3001\u65af\u74e6\u5e0c\u91cc\u8bed\u3001\u5357\u975e\u8377\u5170\u8bed\u3001\u9a6c\u8fbe\u52a0\u65af\u52a0\u8bed\uff09\u88ab\u5e7f\u6cdb\u652f\u6301\uff0c20\u79cd\u6d3b\u8dc3\u811a\u672c\u88ab\u5ffd\u89c6\uff1b\u63d0\u51fa\u6807\u51c6\u5316\u4e0e\u793e\u533a\u5f00\u53d1\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u9700\u901a\u8fc7\u6807\u51c6\u5316\u3001\u8bed\u6599\u5e93\u5efa\u8bbe\u53ca\u9002\u5e94\u6027\u65b9\u6cd5\u89e3\u51b3\u975e\u6d32\u8bed\u8a00\u5728LLMs\u4e2d\u7684\u6280\u672f\u969c\u788d\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u4f4e\u8d44\u6e90\u8bed\u8a00\u3001\u975e\u6d32\u8bed\u8a00\u3001\u8bed\u6599\u5e93\u3001\u6807\u51c6\u5316"}}
{"id": "2506.01970", "pdf": "https://arxiv.org/pdf/2506.01970", "abs": "https://arxiv.org/abs/2506.01970", "authors": ["Ruizhuo Song", "Beiming Yuan"], "title": "Johnny: Structuring Representation Space to Enhance Machine Abstract Reasoning Ability", "categories": ["cs.LG", "cs.CV"], "comment": "15 pages, 15 figures, 5 tables", "summary": "This paper thoroughly investigates the challenges of enhancing AI's abstract\nreasoning capabilities, with a particular focus on Raven's Progressive Matrices\n(RPM) tasks involving complex human-like concepts. Firstly, it dissects the\nempirical reality that traditional end-to-end RPM-solving models heavily rely\non option pool configurations, highlighting that this dependency constrains the\nmodel's reasoning capabilities. To address this limitation, the paper proposes\nthe Johnny architecture - a novel representation space-based framework for\nRPM-solving. Through the synergistic operation of its Representation Extraction\nModule and Reasoning Module, Johnny significantly enhances reasoning\nperformance by supplementing primitive negative option configurations with a\nlearned representation space. Furthermore, to strengthen the model's capacity\nfor capturing positional relationships among local features, the paper\nintroduces the Spin-Transformer network architecture, accompanied by a\nlightweight Straw Spin-Transformer variant that reduces computational overhead\nthrough parameter sharing and attention mechanism optimization. Experimental\nevaluations demonstrate that both Johnny and Spin-Transformer achieve superior\nperformance on RPM tasks, offering innovative methodologies for advancing AI's\nabstract reasoning capabilities.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u4f20\u7edfRPM\u4efb\u52a1\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u67b6\u6784Johnny\u548cSpin-Transformer\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u7684\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfRPM\u4efb\u52a1\u6a21\u578b\u4f9d\u8d56\u4e8e\u9009\u9879\u6c60\u914d\u7f6e\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e9f\u9700\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Johnny\u67b6\u6784\uff0c\u5305\u62ec\u8868\u5f81\u63d0\u53d6\u6a21\u5757\u548c\u63a8\u7406\u6a21\u5757\uff0c\u4ee5\u53caSpin-Transformer\u7f51\u7edc\u53ca\u5176\u8f7b\u91cf\u7ea7\u53d8\u4f53\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eJohnny\u548cSpin-Transformer\u5728RPM\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u63a8\u52a8\u4e86AI\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u7684\u53d1\u5c55\u3002", "conclusion": "\u63d0\u51fa\u7684\u67b6\u6784\u4e3a\u63d0\u5347AI\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u521b\u65b0\u65b9\u6cd5\uff0c\u5177\u6709\u91cd\u8981\u7814\u7a76\u4ef7\u503c\u3002", "keywords": "\u62bd\u8c61\u63a8\u7406\u3001RPM\u4efb\u52a1\u3001Johnny\u67b6\u6784\u3001Spin-Transformer\u7f51\u7edc\u3001\u8868\u5f81\u7a7a\u95f4"}}
{"id": "2506.02157", "pdf": "https://arxiv.org/pdf/2506.02157", "abs": "https://arxiv.org/abs/2506.02157", "authors": ["Amir Hussein", "Cihan Xiao", "Matthew Wiesner", "Dan Povey", "Leibny Paola Garcia", "Sanjeev Khudanpur"], "title": "HENT-SRT: Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Neural transducers (NT) provide an effective framework for speech streaming,\ndemonstrating strong performance in automatic speech recognition (ASR).\nHowever, the application of NT to speech translation (ST) remains challenging,\nas existing approaches struggle with word reordering and performance\ndegradation when jointly modeling ASR and ST, resulting in a gap with\nattention-based encoder-decoder (AED) models. Existing NT-based ST approaches\nalso suffer from high computational training costs. To address these issues, we\npropose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech\nRecognition and Translation), a novel framework that factorizes ASR and\ntranslation tasks to better handle reordering. To ensure robust ST while\npreserving ASR performance, we use self-distillation with CTC consistency\nregularization. Moreover, we improve computational efficiency by incorporating\nbest practices from ASR transducers, including a down-sampled hierarchical\nencoder, a stateless predictor, and a pruned transducer loss to reduce training\ncomplexity. Finally, we introduce a blank penalty during decoding, reducing\ndeletions and improving translation quality. Our approach is evaluated on three\nconversational datasets Arabic, Spanish, and Mandarin achieving new\nstate-of-the-art performance among NT models and substantially narrowing the\ngap with AED-based systems.", "AI": {"tldr": "HENT-SRT\u662f\u4e00\u79cd\u9488\u5bf9\u8bed\u97f3\u8bc6\u522b\u548c\u7ffb\u8bd1\u4efb\u52a1\u7684\u65b0\u578b\u795e\u7ecf\u8f6c\u5bfc\u5668\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u795e\u7ecf\u8f6c\u5bfc\u5668\u5728\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u5904\u7406\u8bcd\u5e8f\u548c\u6027\u80fd\u9000\u5316\u7684\u6311\u6218\uff0c\u540c\u65f6\u964d\u4f4e\u9ad8\u8ba1\u7b97\u8bad\u7ec3\u6210\u672c\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u5206\u89e3ASR\u548c\u7ffb\u8bd1\u4efb\u52a1\uff0c\u7ed3\u5408\u81ea\u84b8\u998f\u4e0eCTC\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff0c\u4f18\u5316\u7f16\u7801\u5668\u548c\u9884\u6d4b\u5668\u8bbe\u8ba1\uff0c\u5e76\u5f15\u5165\u89e3\u7801\u7a7a\u767d\u60e9\u7f5a\u3002", "result": "\u5728\u591a\u4e2a\u5bf9\u8bdd\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u65b0\u7684SOTA\u6027\u80fd\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0eAED\u6a21\u578b\u7684\u5dee\u8ddd\u3002", "conclusion": "HENT-SRT\u6709\u6548\u89e3\u51b3\u4e86\u795e\u7ecf\u8f6c\u5bfc\u5668\u5728\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002", "keywords": "\u795e\u7ecf\u8f6c\u5bfc\u5668,HENT-SRT,\u8bed\u97f3\u8bc6\u522b,\u8bed\u97f3\u7ffb\u8bd1,\u81ea\u84b8\u998f"}}
{"id": "2506.02314", "pdf": "https://arxiv.org/pdf/2506.02314", "abs": "https://arxiv.org/abs/2506.02314", "authors": ["Tianyu Hua", "Harper Hua", "Violet Xiang", "Benjamin Klieger", "Sang T. Truong", "Weixin Liang", "Fan-Yun Sun", "Nick Haber"], "title": "ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown promise in transforming machine\nlearning research, yet their capability to faithfully implement novel ideas\nfrom recent research papers-ideas unseen during pretraining-remains unclear. We\nintroduce ResearchCodeBench, a benchmark of 212 coding challenges that\nevaluates LLMs' ability to translate cutting-edge ML contributions from top\n2024-2025 research papers into executable code. We assessed 30+ proprietary and\nopen-source LLMs, finding that even the best models correctly implement less\nthan 40% of the code. We find Gemini-2.5-Pro-Preview to perform best at 37.3%\nsuccess rate, with O3 (High) and O4-mini (High) following behind at 32.3% and\n30.8% respectively. We present empirical findings on performance comparison,\ncontamination, and error patterns. By providing a rigorous and community-driven\nevaluation platform, ResearchCodeBench enables continuous understanding and\nadvancement of LLM-driven innovation in research code generation.", "AI": {"tldr": "ResearchCodeBench\u662f\u4e00\u4e2a\u5305\u542b212\u4e2a\u7f16\u7801\u6311\u6218\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c06\u6700\u65b0\u7814\u7a76\u8bba\u6587\u4e2d\u7684\u524d\u6cbf\u673a\u5668\u5b66\u4e60\u8d21\u732e\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\u7684\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u4e5f\u53ea\u6709\u4e0d\u523040%\u7684\u6b63\u786e\u7387\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u5b66\u4e60\u7814\u7a76\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u80fd\u5426\u5fe0\u5b9e\u5b9e\u73b0\u9884\u8bad\u7ec3\u671f\u95f4\u672a\u89c1\u7684\u65b0\u9896\u7814\u7a76\u60f3\u6cd5\u5c1a\u4e0d\u660e\u786e\u3002\u8fd9\u662f\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u4e0d\u786e\u5b9a\u6027\u800c\u8fdb\u884c\u7684\u7814\u7a76\u3002", "method": "\u5f15\u5165ResearchCodeBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b212\u4e2a\u7f16\u7801\u6311\u6218\uff0c\u8bc4\u4f3030\u591a\u79cd\u4e13\u6709\u548c\u5f00\u6e90LLMs\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002", "result": "\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u662fGemini-2.5-Pro-Preview\uff0c\u6210\u529f\u7387\u4e3a37.3%\uff0c\u5176\u6b21\u662fO3\uff08High\uff09\u548cO4-mini\uff08High\uff09\uff0c\u5206\u522b\u4e3a32.3%\u548c30.8%\u3002\u6240\u6709\u6a21\u578b\u7684\u6b63\u786e\u7387\u5747\u4f4e\u4e8e40%\u3002", "conclusion": "ResearchCodeBench\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u6301\u7eed\u7406\u89e3\u548c\u63a8\u8fdbLLM\u5728\u7814\u7a76\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u521b\u65b0\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u4ee3\u7801\u751f\u6210\u3001\u57fa\u51c6\u6d4b\u8bd5\u3001\u673a\u5668\u5b66\u4e60\u7814\u7a76"}}
{"id": "2506.01974", "pdf": "https://arxiv.org/pdf/2506.01974", "abs": "https://arxiv.org/abs/2506.01974", "authors": ["Kanwal Aalijah"], "title": "Traffic and Mobility Optimization Using AI: Comparative Study between Dubai and Riyadh", "categories": ["cs.LG"], "comment": null, "summary": "Urban planning plays a very important role in development modern cities. It\neffects the economic growth, quality of life, and environmental sustainability.\nModern cities face challenges in managing traffic congestion. These challenges\narise to due to rapid urbanization. In this study we will explore how AI can be\nused to understand the traffic and mobility related issues and its effects on\nthe residents sentiment. The approach combines real-time traffic data with\ngeo-located sentiment analysis, offering a comprehensive and dynamic approach\nto urban mobility planning. AI models and exploratory data analysis was used to\npredict traffic congestion patterns, analyze commuter behaviors, and identify\ncongestion hotspots and dissatisfaction zones. The findings offer actionable\nrecommendations for optimizing traffic flow, enhancing commuter experiences,\nand addressing city specific mobility challenges in the Middle East and beyond.", "AI": {"tldr": "AI\u6280\u672f\u7ed3\u5408\u5b9e\u65f6\u4ea4\u901a\u6570\u636e\u548c\u60c5\u611f\u5206\u6790\u4f18\u5316\u57ce\u5e02\u4ea4\u901a\u89c4\u5212\u3002", "motivation": "\u5feb\u901f\u57ce\u5e02\u5316\u5bfc\u81f4\u4ea4\u901a\u62e5\u5835\u95ee\u9898\u52a0\u5267\uff0c\u5f71\u54cd\u5c45\u6c11\u751f\u6d3b\u8d28\u91cf\u548c\u57ce\u5e02\u53ef\u6301\u7eed\u53d1\u5c55\u3002", "method": "\u7ed3\u5408\u5b9e\u65f6\u4ea4\u901a\u6570\u636e\u548c\u5730\u7406\u5b9a\u4f4d\u7684\u60c5\u611f\u5206\u6790\uff0c\u5229\u7528AI\u6a21\u578b\u548c\u63a2\u7d22\u6027\u6570\u636e\u5206\u6790\u9884\u6d4b\u62e5\u5835\u6a21\u5f0f\u3001\u5206\u6790\u901a\u52e4\u884c\u4e3a\u3002", "result": "\u63d0\u51fa\u4f18\u5316\u4ea4\u901a\u6d41\u3001\u6539\u5584\u901a\u52e4\u4f53\u9a8c\u53ca\u9488\u5bf9\u6027\u89e3\u51b3\u4e2d\u4e1c\u7b49\u5730\u57ce\u5e02\u4ea4\u901a\u95ee\u9898\u7684\u5efa\u8bae\u3002", "conclusion": "AI\u4e3a\u52a8\u6001\u548c\u5168\u9762\u7684\u57ce\u5e02\u4ea4\u901a\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "keywords": "\u57ce\u5e02\u4ea4\u901a\u3001AI\u3001\u60c5\u611f\u5206\u6790\u3001\u62e5\u5835\u9884\u6d4b\u3001\u4e2d\u4e1c\u5730\u533a"}}
{"id": "2506.02172", "pdf": "https://arxiv.org/pdf/2506.02172", "abs": "https://arxiv.org/abs/2506.02172", "authors": ["Dennis Fucci", "Marco Gaido", "Matteo Negri", "Luisa Bentivogli", "Andre Martins", "Giuseppe Attanasio"], "title": "Different Speech Translation Models Encode and Translate Speaker Gender Differently", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025", "summary": "Recent studies on interpreting the hidden states of speech models have shown\ntheir ability to capture speaker-specific features, including gender. Does this\nfinding also hold for speech translation (ST) models? If so, what are the\nimplications for the speaker's gender assignment in translation? We address\nthese questions from an interpretability perspective, using probing methods to\nassess gender encoding across diverse ST models. Results on three language\ndirections (English-French/Italian/Spanish) indicate that while traditional\nencoder-decoder models capture gender information, newer architectures --\nintegrating a speech encoder with a machine translation system via adapters --\ndo not. We also demonstrate that low gender encoding capabilities result in\nsystems' tendency toward a masculine default, a translation bias that is more\npronounced in newer architectures.", "AI": {"tldr": "\u7814\u7a76\u4e86\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u4e2d\u9690\u85cf\u72b6\u6001\u5bf9\u8bf4\u8bdd\u8005\u6027\u522b\u7279\u5f81\u7684\u6355\u83b7\u80fd\u529b\uff0c\u53d1\u73b0\u4f20\u7edf\u6a21\u578b\u80fd\u6355\u6349\u6027\u522b\u4fe1\u606f\uff0c\u800c\u65b0\u578b\u6a21\u578b\uff08\u901a\u8fc7\u9002\u914d\u5668\u96c6\u6210\u8bed\u97f3\u7f16\u7801\u5668\u548c\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\uff09\u5219\u4e0d\u80fd\uff0c\u4e14\u540e\u8005\u66f4\u503e\u5411\u4e8e\u7537\u6027\u9ed8\u8ba4\u7ffb\u8bd1\u3002", "motivation": "\u63a2\u8ba8\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u662f\u5426\u6355\u83b7\u8bf4\u8bdd\u8005\u6027\u522b\u7279\u5f81\u53ca\u5176\u5bf9\u7ffb\u8bd1\u4e2d\u6027\u522b\u5206\u914d\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u63a2\u6d4b\u65b9\u6cd5\u8bc4\u4f30\u4e0d\u540c\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u4e2d\u7684\u6027\u522b\u7f16\u7801\u80fd\u529b\u3002", "result": "\u4f20\u7edf\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u80fd\u6355\u6349\u6027\u522b\u4fe1\u606f\uff0c\u65b0\u67b6\u6784\u5219\u4e0d\u80fd\uff0c\u4e14\u65b0\u67b6\u6784\u66f4\u503e\u5411\u4e8e\u7537\u6027\u9ed8\u8ba4\u7ffb\u8bd1\u3002", "conclusion": "\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u7684\u67b6\u6784\u5dee\u5f02\u5f71\u54cd\u6027\u522b\u7f16\u7801\u80fd\u529b\uff0c\u65b0\u67b6\u6784\u5728\u6027\u522b\u5206\u914d\u4e0a\u5b58\u5728\u504f\u5dee\u3002", "keywords": "\u8bed\u97f3\u7ffb\u8bd1, \u6027\u522b\u7f16\u7801, \u6a21\u578b\u67b6\u6784, \u7ffb\u8bd1\u504f\u5dee"}}
{"id": "2506.02387", "pdf": "https://arxiv.org/pdf/2506.02387", "abs": "https://arxiv.org/abs/2506.02387", "authors": ["Zelai Xu", "Zhexuan Xu", "Xiangmin Yi", "Huining Yuan", "Xinlei Chen", "Yi Wu", "Chao Yu", "Yu Wang"], "title": "VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in Multi-Agent Environments", "categories": ["cs.AI"], "comment": null, "summary": "Recent advancements in Vision Language Models (VLMs) have expanded their\ncapabilities to interactive agent tasks, yet existing benchmarks remain limited\nto single-agent or text-only environments. In contrast, real-world scenarios\noften involve multiple agents interacting within rich visual and linguistic\ncontexts, posing challenges with both multimodal observations and strategic\ninteractions. To bridge this gap, we introduce Visual Strategic Bench\n(VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning\nand decision-making in multi-agent environments. VS-Bench comprises eight\nvision-grounded environments spanning cooperative, competitive, and\nmixed-motive interactions, designed to assess agents' ability to predict\nothers' future moves and optimize for long-term objectives. We consider two\ncomplementary evaluation dimensions, including offline evaluation of strategic\nreasoning by next-action prediction accuracy and online evaluation of\ndecision-making by normalized episode return. Extensive experiments of fourteen\nleading VLMs reveal a significant gap between current models and optimal\nperformance, with the best models attaining 47.8% prediction accuracy and 24.3%\nnormalized return. We further conduct in-depth analyses on multimodal\nobservations, test-time scaling, social behaviors, and failure cases of VLM\nagents. By standardizing the evaluation and highlighting the limitations of\nexisting models, we envision VS-Bench as a foundation for future research on\nstrategic multimodal agents. Code and data are available at\nhttps://vs-bench.github.io.", "AI": {"tldr": "VS-Bench\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u6218\u7565\u63a8\u7406\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u8986\u76d6\u5408\u4f5c\u3001\u7ade\u4e89\u548c\u6df7\u5408\u52a8\u673a\u573a\u666f\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5c40\u9650\u4e8e\u5355\u667a\u80fd\u4f53\u6216\u7eaf\u6587\u672c\u73af\u5883\uff0c\u800c\u73b0\u5b9e\u573a\u666f\u6d89\u53ca\u591a\u667a\u80fd\u4f53\u5728\u4e30\u5bcc\u7684\u89c6\u89c9\u548c\u8bed\u8a00\u4e0a\u4e0b\u6587\u4e2d\u7684\u4ea4\u4e92\uff0c\u9700\u8bc4\u4f30\u6a21\u578b\u7684\u6218\u7565\u63a8\u7406\u548c\u51b3\u7b56\u80fd\u529b\u3002", "method": "VS-Bench\u5305\u542b8\u4e2a\u89c6\u89c9\u57fa\u7840\u73af\u5883\uff0c\u901a\u8fc7\u79bb\u7ebf\u8bc4\u4f30\uff08\u9884\u6d4b\u51c6\u786e\u6027\uff09\u548c\u5728\u7ebf\u8bc4\u4f30\uff08\u89c4\u8303\u5316\u7684\u4efb\u52a1\u56de\u62a5\uff09\u4e24\u4e2a\u7ef4\u5ea6\u6d4b\u8bd5\u6a21\u578b\u3002", "result": "14\u4e2a\u9886\u5148\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u6700\u4f73\u6a21\u578b\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u89c4\u8303\u5316\u56de\u62a5\u4e0a\u7684\u8868\u73b0\u5206\u522b\u4e3a47.8%\u548c24.3%\uff0c\u663e\u793a\u51fa\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "VS-Bench\u4e3a\u672a\u6765\u6218\u7565\u591a\u6a21\u6001\u667a\u80fd\u4f53\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u57fa\u7840\uff0c\u5e76\u51f8\u663e\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "keywords": "Vision Language Models, Multi-agent, Strategic Reasoning, Benchmark, Multimodal"}}
{"id": "2506.01975", "pdf": "https://arxiv.org/pdf/2506.01975", "abs": "https://arxiv.org/abs/2506.01975", "authors": ["Jama Hussein Mohamud"], "title": "An empirical study of task and feature correlations in the reuse of pre-trained models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Pre-trained neural networks are commonly used and reused in the machine\nlearning community. Alice trains a model for a particular task, and a part of\nher neural network is reused by Bob for a different task, often to great\neffect. To what can we ascribe Bob's success? This paper introduces an\nexperimental setup through which factors contributing to Bob's empirical\nsuccess could be studied in silico. As a result, we demonstrate that Bob might\njust be lucky: his task accuracy increases monotonically with the correlation\nbetween his task and Alice's. Even when Bob has provably uncorrelated tasks and\ninput features from Alice's pre-trained network, he can achieve significantly\nbetter than random performance due to Alice's choice of network and optimizer.\nWhen there is little correlation between tasks, only reusing lower pre-trained\nlayers is preferable, and we hypothesize the converse: that the optimal number\nof retrained layers is indicative of task and feature correlation. Finally, we\nshow in controlled real-world scenarios that Bob can effectively reuse Alice's\npre-trained network if there are semantic correlations between his and Alice's\ntask.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u53d1\u73b0\uff0c\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u590d\u7528\u6548\u679c\u4e0e\u4efb\u52a1\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u5bc6\u5207\u76f8\u5173\uff0c\u4efb\u52a1\u76f8\u5173\u6027\u9ad8\u65f6\u590d\u7528\u6548\u679c\u66f4\u597d\uff1b\u5373\u4f7f\u4efb\u52a1\u65e0\u5173\uff0c\u4ecd\u53ef\u80fd\u56e0\u7f51\u7edc\u548c\u4f18\u5316\u5668\u7684\u9009\u62e9\u83b7\u5f97\u4f18\u4e8e\u968f\u673a\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u7a76\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u590d\u7528\u6210\u529f\u7684\u56e0\u7d20\uff0c\u7406\u89e3\u4efb\u52a1\u76f8\u5173\u6027\u5bf9\u590d\u7528\u6548\u679c\u7684\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u5b9e\u9a8c\u6846\u67b6\uff0c\u5206\u6790\u4efb\u52a1\u76f8\u5173\u6027\u3001\u7f51\u7edc\u5c42\u9009\u62e9\u548c\u4f18\u5316\u5668\u5bf9\u590d\u7528\u6548\u679c\u7684\u4f5c\u7528\u3002", "result": "\u4efb\u52a1\u76f8\u5173\u6027\u9ad8\u65f6\u590d\u7528\u6548\u679c\u663e\u8457\uff1b\u4efb\u52a1\u65e0\u5173\u65f6\u4ecd\u53ef\u80fd\u56e0\u7f51\u7edc\u548c\u4f18\u5316\u5668\u9009\u62e9\u83b7\u5f97\u8f83\u597d\u8868\u73b0\uff1b\u8bed\u4e49\u76f8\u5173\u7684\u4efb\u52a1\u590d\u7528\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "\u9884\u8bad\u7ec3\u7f51\u7edc\u7684\u590d\u7528\u6548\u679c\u53d6\u51b3\u4e8e\u4efb\u52a1\u76f8\u5173\u6027\uff0c\u8bed\u4e49\u76f8\u5173\u4efb\u52a1\u590d\u7528\u66f4\u6709\u6548\u3002", "keywords": "\u9884\u8bad\u7ec3\u7f51\u7edc\u3001\u795e\u7ecf\u7f51\u7edc\u590d\u7528\u3001\u4efb\u52a1\u76f8\u5173\u6027\u3001\u8bed\u4e49\u76f8\u5173\u3001\u4f18\u5316\u5668"}}
{"id": "2506.02175", "pdf": "https://arxiv.org/pdf/2506.02175", "abs": "https://arxiv.org/abs/2506.02175", "authors": ["Salman Rahman", "Sheriff Issaka", "Ashima Suvarna", "Genglin Liu", "James Shiffer", "Jaeyoung Lee", "Md Rizwan Parvez", "Hamid Palangi", "Shi Feng", "Nanyun Peng", "Yejin Choi", "Julian Michael", "Liwei Jiang", "Saadia Gabriel"], "title": "AI Debate Aids Assessment of Controversial Claims", "categories": ["cs.CL"], "comment": null, "summary": "As AI grows more powerful, it will increasingly shape how we understand the\nworld. But with this influence comes the risk of amplifying misinformation and\ndeepening social divides-especially on consequential topics like public health\nwhere factual accuracy directly impacts well-being. Scalable Oversight aims to\nensure AI truthfulness by enabling humans to supervise systems that may exceed\nhuman capabilities--yet humans themselves hold different beliefs and biases\nthat impair their judgment. We study whether AI debate can guide biased judges\ntoward the truth by having two AI systems debate opposing sides of\ncontroversial COVID-19 factuality claims where people hold strong prior\nbeliefs. We conduct two studies: one with human judges holding either\nmainstream or skeptical beliefs evaluating factuality claims through\nAI-assisted debate or consultancy protocols, and a second examining the same\nproblem with personalized AI judges designed to mimic these different human\nbelief systems. In our human study, we find that debate-where two AI advisor\nsystems present opposing evidence-based arguments-consistently improves\njudgment accuracy and confidence calibration, outperforming consultancy with a\nsingle-advisor system by 10% overall. The improvement is most significant for\njudges with mainstream beliefs (+15.2% accuracy), though debate also helps\nskeptical judges who initially misjudge claims move toward accurate views\n(+4.7% accuracy). In our AI judge study, we find that AI judges with human-like\npersonas achieve even higher accuracy (78.5%) than human judges (70.1%) and\ndefault AI judges without personas (69.8%), suggesting their potential for\nsupervising frontier AI models. These findings highlight AI debate as a\npromising path toward scalable, bias-resilient oversight--leveraging both\ndiverse human and AI judgments to move closer to truth in contested domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7AI\u8fa9\u8bba\u6765\u89e3\u51b3\u4eba\u7c7b\u504f\u89c1\u5bf9\u4e8b\u5b9e\u5224\u65ad\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728COVID-19\u76f8\u5173\u4e89\u8bae\u6027\u8bdd\u9898\u4e2d\uff0c\u53d1\u73b0\u8fa9\u8bba\u663e\u8457\u63d0\u9ad8\u4e86\u5224\u65ad\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740AI\u5f71\u54cd\u529b\u7684\u589e\u52a0\uff0c\u5176\u53ef\u80fd\u653e\u5927\u9519\u8bef\u4fe1\u606f\u6216\u52a0\u6df1\u793e\u4f1a\u5206\u88c2\u3002\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7AI\u8fa9\u8bba\u5e2e\u52a9\u4eba\u7c7b\u5728\u5b58\u5728\u504f\u89c1\u7684\u60c5\u51b5\u4e0b\u66f4\u51c6\u786e\u5730\u5224\u65ad\u4e8b\u5b9e\u3002", "method": "\u7814\u7a76\u5305\u62ec\u4e24\u4e2a\u90e8\u5206\uff1a\u4e00\u662f\u8ba9\u5177\u6709\u4e3b\u6d41\u6216\u6000\u7591\u8bba\u4fe1\u5ff5\u7684\u4eba\u7c7b\u6cd5\u5b98\u901a\u8fc7AI\u8fa9\u8bba\u6216\u54a8\u8be2\u534f\u8bae\u8bc4\u4f30\u4e8b\u5b9e\u6027\u58f0\u660e\uff1b\u4e8c\u662f\u7528\u6a21\u62df\u4eba\u7c7b\u4fe1\u5ff5\u7684AI\u6cd5\u5b98\u91cd\u590d\u5b9e\u9a8c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8fa9\u8bba\u6a21\u5f0f\u6bd4\u5355\u4e00\u987e\u95ee\u6a21\u5f0f\u6574\u4f53\u63d0\u9ad810%\u7684\u5224\u65ad\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e3b\u6d41\u4fe1\u5ff5\u6301\u6709\u8005\uff08+15.2%\uff09\u3002AI\u6cd5\u5b98\u7684\u51c6\u786e\u6027\uff0878.5%\uff09\u9ad8\u4e8e\u4eba\u7c7b\uff0870.1%\uff09\u548c\u9ed8\u8ba4AI\u6cd5\u5b98\uff0869.8%\uff09\u3002", "conclusion": "AI\u8fa9\u8bba\u662f\u4e00\u79cd\u6709\u6548\u7684\u3001\u53ef\u6269\u5c55\u7684\u76d1\u7763\u65b9\u6cd5\uff0c\u80fd\u591f\u7ed3\u5408\u4eba\u7c7b\u548cAI\u7684\u591a\u6837\u6027\u5224\u65ad\uff0c\u5e2e\u52a9\u5728\u4e89\u8bae\u6027\u9886\u57df\u4e2d\u66f4\u63a5\u8fd1\u771f\u76f8\u3002", "keywords": "AI\u8fa9\u8bba, \u4e8b\u5b9e\u5224\u65ad, \u4eba\u7c7b\u504f\u89c1, COVID-19, \u76d1\u7763AI"}}
{"id": "2506.02397", "pdf": "https://arxiv.org/pdf/2506.02397", "abs": "https://arxiv.org/abs/2506.02397", "authors": ["Shengjia Zhang", "Junjie Wu", "Jiawei Chen", "Changwang Zhang", "Xingyu Lou", "Wangchunshu Zhou", "Sheng Zhou", "Can Wang", "Jun Wang"], "title": "OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation", "categories": ["cs.AI"], "comment": null, "summary": "Recent advanced large reasoning models (LRMs) leverage extended\nchain-of-thought (CoT) reasoning to solve complex tasks, achieving\nstate-of-the-art performance. Despite their success, we identify a critical\nissue: a substantial portion of simple tasks solved by LRMs can also be\naddressed by non-reasoning LLMs using significantly fewer tokens, indicating\nthe complex reasoning may not always be necessary. To address this, we\nsystematically analyze the reasoning trajectories of LRMs and present a method\nutilizing identified paradigms and LLM-Judge to classify these trajectories as\neither Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1,\na method that prunes redundant reasoning steps while preserving logical\nvalidity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking)\nfor straightforward problems while engaging in deliberate thinking\n(slow-thinking) for complex problems. Experiments across mathematical and\nquestion-answering tasks demonstrate that OThink-R1 reduces reasoning\nredundancy by almost 23\\% on average without compromising accuracy, offering\npractical guidelines for efficient reasoning models. The code is available at\nhttps://github.com/AgenticIR-Lab/OThink-R1.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOThink-R1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5197\u4f59\u4e0e\u5fc5\u8981\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u52a8\u6001\u7b80\u5316\u63a8\u7406\u8fc7\u7a0b\uff0c\u4ece\u800c\u63d0\u9ad8\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u8bb8\u591a\u7b80\u5355\u4efb\u52a1\u53ef\u4ee5\u901a\u8fc7\u975e\u63a8\u7406LLMs\u7528\u66f4\u5c11\u7684\u6807\u8bb0\u89e3\u51b3\uff0c\u8868\u660e\u590d\u6742\u63a8\u7406\u5e76\u975e\u603b\u662f\u5fc5\u8981\u3002", "method": "\u8bba\u6587\u5206\u6790\u4e86LRMs\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5229\u7528LLM-Judge\u5206\u7c7b\u4e3a\u5197\u4f59\u6216\u5fc5\u8981\u63a8\u7406\uff0c\u5e76\u5f15\u5165OThink-R1\u65b9\u6cd5\u52a8\u6001\u5207\u6362\u5feb\u901f\u4e0e\u6162\u901f\u63a8\u7406\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOThink-R1\u5728\u6570\u5b66\u548c\u95ee\u7b54\u4efb\u52a1\u4e2d\u5e73\u5747\u51cf\u5c1123%\u7684\u5197\u4f59\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "OThink-R1\u4e3a\u9ad8\u6548\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u4f18\u5316\u4e86\u63a8\u7406\u8fc7\u7a0b\u3002", "keywords": "\u5927\u578b\u63a8\u7406\u6a21\u578b, \u5197\u4f59\u63a8\u7406, \u94fe\u5f0f\u601d\u7ef4, OThink-R1, \u6548\u7387\u4f18\u5316"}}
{"id": "2506.01976", "pdf": "https://arxiv.org/pdf/2506.01976", "abs": "https://arxiv.org/abs/2506.01976", "authors": ["Elham Kiyani", "Venkatesh Ananchaperumal", "Ahmad Peyvan", "Mahendaran Uchimali", "Gang Li", "George Em Karniadakis"], "title": "Crack Path Prediction with Operator Learning using Discrete Particle System data Generation", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "comment": "22 pages, 14 figures", "summary": "Accurately modeling crack propagation is critical for predicting failure in\nengineering materials and structures, where small cracks can rapidly evolve and\ncause catastrophic damage. The interaction of cracks with discontinuities, such\nas holes, significantly affects crack deflection and arrest. Recent\ndevelopments in discrete particle systems with multibody interactions based on\nconstitutive behavior have demonstrated the ability to capture crack nucleation\nand evolution without relying on continuum assumptions. In this work, we use\ndata from Constitutively Informed Particle Dynamics (CPD) simulations to train\noperator learning models, specifically Deep Operator Networks (DeepONets),\nwhich learn mappings between function spaces instead of finite-dimensional\nvectors. We explore two DeepONet variants: vanilla and Fusion DeepONet, for\npredicting time-evolving crack propagation in specimens with varying\ngeometries. Three representative cases are studied: (i) varying notch height\nwithout active fracture; and (ii) and (iii) combinations of notch height and\nhole radius where dynamic fracture occurs on irregular discrete meshes. The\nmodels are trained on 32 to 45 samples, using geometric inputs in the branch\nnetwork and spatial-temporal coordinates in the trunk network. Results show\nthat Fusion DeepONet consistently outperforms the vanilla variant, with more\naccurate predictions especially in non-fracturing cases. Fracture-driven\nscenarios involving displacement and crack evolution remain more challenging.\nThese findings highlight the potential of Fusion DeepONet to generalize across\ncomplex, geometry-varying, and time-dependent crack propagation phenomena.", "AI": {"tldr": "\u5229\u7528DeepONet\u6a21\u578b\u9884\u6d4b\u88c2\u7eb9\u6269\u5c55\uff0c\u53d1\u73b0Fusion DeepONet\u5728\u51e0\u4f55\u53d8\u5316\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u7248\u672c\u3002", "motivation": "\u51c6\u786e\u5efa\u6a21\u88c2\u7eb9\u6269\u5c55\u5bf9\u9884\u6d4b\u5de5\u7a0b\u6750\u6599\u548c\u7ed3\u6784\u7684\u5931\u6548\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u88c2\u7eb9\u4e0e\u5b54\u6d1e\u7b49\u4e0d\u8fde\u7eed\u6027\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u901a\u8fc7CPD\u6a21\u62df\u6570\u636e\u8bad\u7ec3DeepONet\uff08\u5305\u62ecvanilla\u548cFusion\u53d8\u4f53\uff09\uff0c\u9884\u6d4b\u51e0\u4f55\u53d8\u5316\u65f6\u7684\u88c2\u7eb9\u6269\u5c55\u3002", "result": "Fusion DeepONet\u5728\u975e\u65ad\u88c2\u6848\u4f8b\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u65ad\u88c2\u9a71\u52a8\u573a\u666f\u4ecd\u5177\u6311\u6218\u6027\u3002", "conclusion": "Fusion DeepONet\u5728\u590d\u6742\u51e0\u4f55\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\u88c2\u7eb9\u6269\u5c55\u95ee\u9898\u4e2d\u5c55\u73b0\u51fa\u6f5c\u5728\u6cdb\u5316\u80fd\u529b\u3002", "keywords": "\u88c2\u7eb9\u6269\u5c55, DeepONet, CPD\u6a21\u62df, \u51e0\u4f55\u53d8\u5316, \u65f6\u95f4\u4f9d\u8d56\u6027"}}
{"id": "2506.02181", "pdf": "https://arxiv.org/pdf/2506.02181", "abs": "https://arxiv.org/abs/2506.02181", "authors": ["Dennis Fucci", "Marco Gaido", "Matteo Negri", "Mauro Cettolo", "Luisa Bentivogli"], "title": "Echoes of Phonetics: Unveiling Relevant Acoustic Cues for ASR via Feature Attribution", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Interspeech 2025", "summary": "Despite significant advances in ASR, the specific acoustic cues models rely\non remain unclear. Prior studies have examined such cues on a limited set of\nphonemes and outdated models. In this work, we apply a feature attribution\ntechnique to identify the relevant acoustic cues for a modern Conformer-based\nASR system. By analyzing plosives, fricatives, and vowels, we assess how\nfeature attributions align with their acoustic properties in the time and\nfrequency domains, also essential for human speech perception. Our findings\nshow that the ASR model relies on vowels' full time spans, particularly their\nfirst two formants, with greater saliency in male speech. It also better\ncaptures the spectral characteristics of sibilant fricatives than non-sibilants\nand prioritizes the release phase in plosives, especially burst\ncharacteristics. These insights enhance the interpretability of ASR models and\nhighlight areas for future research to uncover potential gaps in model\nrobustness.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7279\u5f81\u5f52\u56e0\u6280\u672f\u5206\u6790\u4e86\u73b0\u4ee3Conformer-based ASR\u7cfb\u7edf\u4f9d\u8d56\u7684\u58f0\u5b66\u7ebf\u7d22\uff0c\u53d1\u73b0\u6a21\u578b\u5bf9\u5143\u97f3\u3001\u6469\u64e6\u97f3\u548c\u7206\u7834\u97f3\u7684\u611f\u77e5\u65b9\u5f0f\u4e0e\u4eba\u7c7b\u542c\u89c9\u7279\u6027\u4e00\u81f4\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u6f5c\u5728\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1ASR\u6280\u672f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u6a21\u578b\u4f9d\u8d56\u7684\u5177\u4f53\u58f0\u5b66\u7ebf\u7d22\u4ecd\u4e0d\u660e\u786e\u3002\u4e4b\u524d\u7684\u7814\u7a76\u4ec5\u9488\u5bf9\u6709\u9650\u97f3\u7d20\u548c\u8fc7\u65f6\u6a21\u578b\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5bf9\u73b0\u4ee3ASR\u7cfb\u7edf\u8fdb\u884c\u66f4\u5168\u9762\u7684\u5206\u6790\u3002", "method": "\u4f7f\u7528\u7279\u5f81\u5f52\u56e0\u6280\u672f\uff0c\u5206\u6790\u73b0\u4ee3Conformer-based ASR\u6a21\u578b\u5bf9\u7206\u7834\u97f3\u3001\u6469\u64e6\u97f3\u548c\u5143\u97f3\u7684\u4f9d\u8d56\uff0c\u5e76\u4e0e\u65f6\u9891\u57df\u4e2d\u7684\u58f0\u5b66\u7279\u6027\u53ca\u4eba\u7c7b\u542c\u89c9\u7279\u6027\u5bf9\u6bd4\u3002", "result": "\u6a21\u578b\u66f4\u4f9d\u8d56\u5143\u97f3\u7684\u6574\u4e2a\u65f6\u95f4\u8de8\u5ea6\uff08\u7279\u522b\u662f\u524d\u4e24\u4e2a\u5171\u632f\u5cf0\uff09\uff0c\u4e14\u5bf9\u7537\u6027\u8bed\u97f3\u66f4\u654f\u611f\uff1b\u5bf9\u6469\u64e6\u97f3\u4e2d\u7684\u64e6\u97f3\u8c31\u7279\u5f81\u6355\u6349\u66f4\u51c6\u786e\uff1b\u7206\u7834\u97f3\u4e2d\u66f4\u91cd\u89c6\u91ca\u653e\u9636\u6bb5\u7684\u7206\u7834\u7279\u6027\u3002", "conclusion": "\u7814\u7a76\u589e\u5f3a\u4e86ASR\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\uff0c\u4ee5\u63ed\u793a\u6a21\u578b\u9c81\u68d2\u6027\u53ef\u80fd\u5b58\u5728\u7684\u4e0d\u8db3\u3002", "keywords": "ASR, \u58f0\u5b66\u7ebf\u7d22, \u7279\u5f81\u5f52\u56e0, Conformer\u6a21\u578b, \u53ef\u89e3\u91ca\u6027"}}
{"id": "2506.02456", "pdf": "https://arxiv.org/pdf/2506.02456", "abs": "https://arxiv.org/abs/2506.02456", "authors": ["Tri Cao", "Bennett Lim", "Yue Liu", "Yuan Sui", "Yuexin Li", "Shumin Deng", "Lin Lu", "Nay Oo", "Shuicheng Yan", "Bryan Hooi"], "title": "VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents", "categories": ["cs.AI", "cs.CR"], "comment": "Under Review", "summary": "Computer-Use Agents (CUAs) with full system access enable powerful task\nautomation but pose significant security and privacy risks due to their ability\nto manipulate files, access user data, and execute arbitrary commands. While\nprior work has focused on browser-based agents and HTML-level attacks, the\nvulnerabilities of CUAs remain underexplored. In this paper, we investigate\nVisual Prompt Injection (VPI) attacks, where malicious instructions are\nvisually embedded within rendered user interfaces, and examine their impact on\nboth CUAs and Browser-Use Agents (BUAs). We propose VPI-Bench, a benchmark of\n306 test cases across five widely used platforms, to evaluate agent robustness\nunder VPI threats. Each test case is a variant of a web platform, designed to\nbe interactive, deployed in a realistic environment, and containing a visually\nembedded malicious prompt. Our empirical study shows that current CUAs and BUAs\ncan be deceived at rates of up to 51% and 100%, respectively, on certain\nplatforms. The experimental results also indicate that system prompt defenses\noffer only limited improvements. These findings highlight the need for robust,\ncontext-aware defenses to ensure the safe deployment of multimodal AI agents in\nreal-world environments. The code and dataset are available at:\nhttps://github.com/cua-framework/agents", "AI": {"tldr": "\u7814\u7a76CUAs\u548cBUAs\u5728\u89c6\u89c9\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51faVPI-Bench\u8bc4\u6d4b\u57fa\u51c6\uff0c\u53d1\u73b0\u5f53\u524d\u4ee3\u7406\u6613\u53d7\u653b\u51fb\uff0c\u7cfb\u7edf\u63d0\u793a\u9632\u5fa1\u6548\u679c\u6709\u9650\u3002", "motivation": "\u7531\u4e8eCUAs\u5177\u6709\u7cfb\u7edf\u7ea7\u8bbf\u95ee\u6743\uff0c\u5176\u5b89\u5168\u6027\u548c\u9690\u79c1\u98ce\u9669\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u5c24\u5176\u662f\u89c6\u89c9\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faVPI-Bench\u57fa\u51c6\uff0c\u5305\u542b306\u4e2a\u6d4b\u8bd5\u7528\u4f8b\uff0c\u8bc4\u4f30\u4ee3\u7406\u5728\u89c6\u89c9\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "CUAs\u548cBUAs\u5728\u67d0\u4e9b\u5e73\u53f0\u4e0a\u7684\u653b\u51fb\u6210\u529f\u7387\u5206\u522b\u9ad8\u8fbe51%\u548c100%\uff0c\u7cfb\u7edf\u63d0\u793a\u9632\u5fa1\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u60c5\u5883\u611f\u77e5\u7684\u9632\u5fa1\u673a\u5236\u4ee5\u4fdd\u969c\u591a\u6a21\u6001AI\u4ee3\u7406\u7684\u5b89\u5168\u90e8\u7f72\u3002", "keywords": "CUAs, BUAs, \u89c6\u89c9\u63d0\u793a\u6ce8\u5165\u653b\u51fb, \u591a\u6a21\u6001AI\u4ee3\u7406, \u5b89\u5168\u6027"}}
{"id": "2506.01977", "pdf": "https://arxiv.org/pdf/2506.01977", "abs": "https://arxiv.org/abs/2506.01977", "authors": ["Wei Huang", "Hanchen Wang", "Dong Wen", "Shaozhen Ma", "Wenjie Zhang", "Xuemin Lin"], "title": "Towards Unsupervised Training of Matching-based Graph Edit Distance Solver via Preference-aware GAN", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Edit Distance (GED) is a fundamental graph similarity metric widely\nused in various applications. However, computing GED is an NP-hard problem.\nRecent state-of-the-art hybrid GED solver has shown promising performance by\nformulating GED as a bipartite graph matching problem, then leveraging a\ngenerative diffusion model to predict node matching between two graphs, from\nwhich both the GED and its corresponding edit path can be extracted using a\ntraditional algorithm. However, such methods typically rely heavily on\nground-truth supervision, where the ground-truth labels are often costly to\nobtain in real-world scenarios. In this paper, we propose GEDRanker, a novel\nunsupervised GAN-based framework for GED computation. Specifically, GEDRanker\nconsists of a matching-based GED solver and introduces an interpretable\npreference-aware discriminator with an effective training strategy to guide the\nmatching-based GED solver toward generating high-quality node matching without\nthe need for ground-truth labels. Extensive experiments on benchmark datasets\ndemonstrate that our GEDRanker enables the matching-based GED solver to achieve\nnear-optimal solution quality without any ground-truth supervision.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684GAN\u6846\u67b6GEDRanker\uff0c\u7528\u4e8e\u8ba1\u7b97\u56fe\u7f16\u8f91\u8ddd\u79bb\uff08GED\uff09\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684\u771f\u5b9e\u6807\u7b7e\uff0c\u901a\u8fc7\u8bbe\u8ba1\u53ef\u89e3\u91ca\u7684\u5224\u522b\u5668\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u8282\u70b9\u5339\u914d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u771f\u5b9e\u6807\u7b7e\uff0c\u4f46\u5b9e\u9645\u573a\u666f\u4e2d\u83b7\u53d6\u6210\u672c\u9ad8\uff1b\u672c\u6587\u63d0\u51fa\u65e0\u76d1\u7763\u6846\u67b6\u4ee5\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "GEDRanker\u7ed3\u5408\u5339\u914d\u5f0fGED\u6c42\u89e3\u5668\u548c\u504f\u597d\u611f\u77e5\u5224\u522b\u5668\uff0c\u901a\u8fc7\u6709\u6548\u8bad\u7ec3\u7b56\u7565\u751f\u6210\u9ad8\u8d28\u91cf\u8282\u70b9\u5339\u914d\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cGEDRanker\u4f7f\u5339\u914d\u5f0f\u6c42\u89e3\u5668\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u5373\u53ef\u83b7\u5f97\u63a5\u8fd1\u6700\u4f18\u89e3\u7684\u8d28\u91cf\u3002", "conclusion": "GEDRanker\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u76d1\u7763GED\u8ba1\u7b97\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u771f\u5b9e\u6807\u7b7e\u4f9d\u8d56\u95ee\u9898\u3002", "keywords": "\u56fe\u7f16\u8f91\u8ddd\u79bb\uff0c\u65e0\u76d1\u7763\u5b66\u4e60\uff0cGAN\uff0c\u8282\u70b9\u5339\u914d"}}
{"id": "2506.02204", "pdf": "https://arxiv.org/pdf/2506.02204", "abs": "https://arxiv.org/abs/2506.02204", "authors": ["Lindia Tjuatja", "Graham Neubig"], "title": "BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Language model evaluation is a daunting task: prompts are brittle,\ncorpus-level perplexities are vague, and the choice of benchmarks are endless.\nFinding examples that show meaningful, generalizable differences between two\nLMs is crucial to understanding where one model succeeds and another fails. Can\nthis process be done automatically? In this work, we propose methodology for\nautomated comparison of language models that uses performance-aware contextual\nembeddings to find fine-grained features of text where one LM outperforms\nanother. Our method, which we name BehaviorBox, extracts coherent features that\ndemonstrate differences with respect to the ease of generation between two LMs.\nSpecifically, BehaviorBox finds features that describe groups of words in\nfine-grained contexts, such as \"conditional 'were' in the phrase 'if you were'\"\nand \"exclamation marks after emotional statements\", where one model outperforms\nanother within a particular datatset. We apply BehaviorBox to compare models\nthat vary in size, model family, and post-training, and enumerate insights into\nspecific contexts that illustrate meaningful differences in performance which\ncannot be found by measures such as corpus-level perplexity alone.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBehaviorBox\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u6bd4\u8f83\u8bed\u8a00\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u6027\u80fd\u5dee\u5f02\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5d4c\u5165\u627e\u5230\u6a21\u578b\u95f4\u7684\u5177\u4f53\u4f18\u52a3\u573a\u666f\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u590d\u6742\u4e14\u4e3b\u89c2\uff0c\u4f20\u7edf\u7684\u56f0\u60d1\u5ea6\u6307\u6807\u6a21\u7cca\uff0c\u96be\u4ee5\u6355\u6349\u6a21\u578b\u95f4\u7684\u5177\u4f53\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u6027\u80fd\u611f\u77e5\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\uff08BehaviorBox\uff09\uff0c\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7279\u5f81\uff0c\u5c55\u793a\u6a21\u578b\u5728\u7279\u5b9a\u4e0a\u4e0b\u6587\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "BehaviorBox\u80fd\u591f\u8bc6\u522b\u6a21\u578b\u5728\u5177\u4f53\u4e0a\u4e0b\u6587\u4e2d\u7684\u4f18\u52a3\uff0c\u5982\u7279\u5b9a\u77ed\u8bed\u6216\u6807\u70b9\u4f7f\u7528\u573a\u666f\uff0c\u4f18\u4e8e\u4f20\u7edf\u56f0\u60d1\u5ea6\u6307\u6807\u3002", "conclusion": "\u81ea\u52a8\u5316\u65b9\u6cd5BehaviorBox\u4e3a\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u7ec6\u7c92\u5ea6\u548c\u53ef\u89e3\u91ca\u7684\u5dee\u5f02\u5206\u6790\u5de5\u5177\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30, \u6027\u80fd\u6bd4\u8f83, BehaviorBox, \u4e0a\u4e0b\u6587\u5d4c\u5165, \u7ec6\u7c92\u5ea6\u7279\u5f81"}}
{"id": "2506.02470", "pdf": "https://arxiv.org/pdf/2506.02470", "abs": "https://arxiv.org/abs/2506.02470", "authors": ["Xuejiao Zhao", "Siyan Liu", "Su-Yin Yang", "Chunyan Miao"], "title": "A Smart Multimodal Healthcare Copilot with Powerful LLM Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Misdiagnosis causes significant harm to healthcare systems worldwide, leading\nto increased costs and patient risks. MedRAG is a smart multimodal healthcare\ncopilot equipped with powerful large language model (LLM) reasoning, designed\nto enhance medical decision-making. It supports multiple input modalities,\nincluding non-intrusive voice monitoring, general medical queries, and\nelectronic health records. MedRAG provides recommendations on diagnosis,\ntreatment, medication, and follow-up questioning. Leveraging\nretrieval-augmented generation enhanced by knowledge graph-elicited reasoning,\nMedRAG retrieves and integrates critical diagnostic insights, reducing the risk\nof misdiagnosis. It has been evaluated on both public and private datasets,\noutperforming existing models and offering more specific and accurate\nhealthcare assistance. A demonstration video of MedRAG is available at:\nhttps://www.youtube.com/watch?v=PNIBDMYRfDM. The source code is available at:\nhttps://github.com/SNOWTEAM2023/MedRAG.", "AI": {"tldr": "MedRAG\u662f\u4e00\u79cd\u591a\u6a21\u6001\u533b\u7597\u52a9\u624b\uff0c\u7ed3\u5408LLM\u548c\u77e5\u8bc6\u56fe\u8c31\uff0c\u65e8\u5728\u51cf\u5c11\u8bef\u8bca\u5e76\u63d0\u9ad8\u51b3\u7b56\u51c6\u786e\u6027\u3002", "motivation": "\u8bef\u8bca\u5bf9\u533b\u7597\u7cfb\u7edf\u548c\u60a3\u8005\u9020\u6210\u91cd\u5927\u5371\u5bb3\uff0c\u9700\u667a\u80fd\u8f85\u52a9\u5de5\u5177\u63d0\u5347\u51b3\u7b56\u6548\u7387\u3002", "method": "\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\uff0c\u652f\u6301\u8bed\u97f3\u3001\u67e5\u8be2\u548c\u7535\u5b50\u75c5\u5386\u7b49\u591a\u6a21\u6001\u8f93\u5165\u3002", "result": "\u5728\u516c\u5f00\u548c\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u5177\u4f53\u7684\u533b\u7597\u5efa\u8bae\u3002", "conclusion": "MedRAG\u901a\u8fc7\u591a\u6a21\u6001\u548c\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u533b\u7597\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "keywords": "\u8bef\u8bca, \u591a\u6a21\u6001, \u68c0\u7d22\u589e\u5f3a\u751f\u6210, \u77e5\u8bc6\u56fe\u8c31, \u533b\u7597\u52a9\u624b"}}
{"id": "2506.01983", "pdf": "https://arxiv.org/pdf/2506.01983", "abs": "https://arxiv.org/abs/2506.01983", "authors": ["Reyhaneh Keshavarzpour", "Eghbal Mansoori"], "title": "Improvement of AMPs Identification with Generative Adversarial Network and Ensemble Classification", "categories": ["cs.LG", "cs.AI"], "comment": "21 pages, 3 figures, 4 tables", "summary": "Identification of antimicrobial peptides is an important and necessary issue\nin today's era. Antimicrobial peptides are essential as an alternative to\nantibiotics for biomedical applications and many other practical applications.\nThese oligopeptides are useful in drug design and cause innate immunity against\nmicroorganisms. Artificial intelligence algorithms have played a significant\nrole in the ease of identifying these peptides.This research is improved by\nimproving proposed method in the field of antimicrobial peptides prediction.\nSuggested method is improved by combining the best coding method from different\nperspectives, In the following a deep neural network to balance the imbalanced\ncombined datasets. The results of this research show that the proposed method\nhave a significant improvement in the accuracy and efficiency of the prediction\nof antimicrobial peptides and are able to provide the best results compared to\nthe existing methods. These development in the field of prediction and\nclassification of antimicrobial peptides, basically in the fields of medicine\nand pharmaceutical industries, have high effectiveness and application.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u6297\u83cc\u80bd\uff0c\u901a\u8fc7\u7ed3\u5408\u6700\u4f73\u7f16\u7801\u65b9\u6cd5\u548c\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u6297\u83cc\u80bd\u4f5c\u4e3a\u6297\u751f\u7d20\u7684\u66ff\u4ee3\u54c1\u5728\u751f\u7269\u533b\u5b66\u548c\u5176\u4ed6\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u8bc6\u522b\u6297\u83cc\u80bd\u662f\u5f53\u524d\u7684\u91cd\u8981\u8bfe\u9898\u3002\u4eba\u5de5\u667a\u80fd\u7b97\u6cd5\u5728\u5176\u8bc6\u522b\u4e2d\u53d1\u6325\u4e86\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u4e0d\u540c\u89c6\u89d2\u7684\u6700\u4f73\u7f16\u7801\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\u96c6\uff0c\u6539\u8fdb\u6297\u83cc\u80bd\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6297\u83cc\u80bd\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6297\u83cc\u80bd\u7684\u9884\u6d4b\u548c\u5206\u7c7b\u9886\u57df\uff0c\u5c24\u5176\u662f\u5728\u533b\u5b66\u548c\u5236\u836f\u884c\u4e1a\uff0c\u5177\u6709\u9ad8\u5ea6\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u6297\u83cc\u80bd\uff0c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u9884\u6d4b\u65b9\u6cd5\uff0c\u4eba\u5de5\u667a\u80fd\uff0c\u6297\u751f\u7d20\u66ff\u4ee3"}}
{"id": "2506.02212", "pdf": "https://arxiv.org/pdf/2506.02212", "abs": "https://arxiv.org/abs/2506.02212", "authors": ["Ella Rannon", "David Burstein"], "title": "Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics", "categories": ["cs.CL", "cs.AI", "q-bio.GN"], "comment": null, "summary": "Natural Language Processing (NLP) has transformed various fields beyond\nlinguistics by applying techniques originally developed for human language to\nthe analysis of biological sequences. This review explores the application of\nNLP methods to biological sequence data, focusing on genomics, transcriptomics,\nand proteomics. We examine how various NLP methods, from classic approaches\nlike word2vec to advanced models employing transformers and hyena operators,\nare being adapted to analyze DNA, RNA, protein sequences, and entire genomes.\nThe review also examines tokenization strategies and model architectures,\nevaluating their strengths, limitations, and suitability for different\nbiological tasks. We further cover recent advances in NLP applications for\nbiological data, such as structure prediction, gene expression, and\nevolutionary analysis, highlighting the potential of these methods for\nextracting meaningful insights from large-scale genomic data. As language\nmodels continue to advance, their integration into bioinformatics holds immense\npromise for advancing our understanding of biological processes in all domains\nof life.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86NLP\u65b9\u6cd5\u5728\u751f\u7269\u5e8f\u5217\u6570\u636e\uff08\u5982DNA\u3001RNA\u548c\u86cb\u767d\u8d28\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86\u4ece\u7ecf\u5178\u6a21\u578b\u5230\u5148\u8fdb\u6280\u672f\u7684\u9002\u5e94\u6027\u3001\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u5176\u5728\u751f\u7269\u4fe1\u606f\u5b66\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u63a2\u7d22NLP\u6280\u672f\u5728\u751f\u7269\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u4ee5\u89e3\u51b3\u57fa\u56e0\u7ec4\u3001\u8f6c\u5f55\u7ec4\u548c\u86cb\u767d\u8d28\u7ec4\u6570\u636e\u5206\u6790\u4e2d\u7684\u6311\u6218\u3002", "method": "\u7efc\u8ff0\u4e86\u5305\u62ecword2vec\u3001transformer\u548chyena\u7b97\u5b50\u7b49NLP\u65b9\u6cd5\u5728\u751f\u7269\u5e8f\u5217\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u5e76\u5206\u6790\u5176\u5efa\u6a21\u548c\u6807\u8bb0\u5316\u7b56\u7565\u3002", "result": "\u5c55\u793a\u4e86NLP\u65b9\u6cd5\u5728\u7ed3\u6784\u9884\u6d4b\u3001\u57fa\u56e0\u8868\u8fbe\u548c\u8fdb\u5316\u5206\u6790\u7b49\u751f\u7269\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\uff0cNLP\u5728\u751f\u7269\u4fe1\u606f\u5b66\u4e2d\u7684\u5e94\u7528\u6709\u671b\u4e3a\u7406\u89e3\u751f\u547d\u8fc7\u7a0b\u63d0\u4f9b\u65b0\u7684\u89c1\u89e3\u3002", "keywords": "NLP,\u751f\u7269\u5e8f\u5217,\u57fa\u56e0\u7ec4\u5b66,transformer,\u751f\u7269\u4fe1\u606f\u5b66"}}
{"id": "2506.02485", "pdf": "https://arxiv.org/pdf/2506.02485", "abs": "https://arxiv.org/abs/2506.02485", "authors": ["Haowen Xu", "Sisi Zlatanova", "Ruiyu Liang", "Ismet Canbulat"], "title": "Generative AI for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning", "categories": ["cs.AI", "cs.CE"], "comment": null, "summary": "Wildfires continue to inflict devastating human, environmental, and economic\nlosses globally, as tragically exemplified by the 2025 Los Angeles wildfire and\nthe urgent demand for more effective response strategies. While physics-based\nand deep learning models have advanced wildfire simulation, they face critical\nlimitations in predicting and visualizing multimodal fire spread in real time,\nparticularly in both 2D and 3D spatial domains using dynamically updated GIS\ndata. These limitations hinder timely emergency response, infrastructure\nprotection, and community safety. Generative AI has recently emerged as a\ntransformative approach across research and industry. Models such as Generative\nAdversarial Networks (GANs), Variational Autoencoders (VAEs), Transformers, and\ndiffusion-based architectures offer distinct advantages over traditional\nmethods, including the integration of multimodal data, generation of diverse\nscenarios under uncertainty, and improved modeling of wildfire dynamics across\nspatial and temporal scales. This position paper advocates for the adoption of\ngenerative AI as a foundational framework for wildfire prediction. We explore\nhow such models can enhance 2D fire spread forecasting and enable more\nrealistic, scalable 3D simulations. Additionally, we employ a novel human-AI\ncollaboration framework using large language models (LLMs) for automated\nknowledge extraction, literature synthesis, and bibliometric mapping. Looking\nahead, we identify five key visions for integrating generative AI into wildfire\nmanagement: multimodal approaches, AI foundation models, conversational AI\nsystems, edge-computing-based scenario generation, and cognitive digital twins.\nWe also address three major challenges accompanying these opportunities and\npropose potential solutions to support their implementation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u751f\u6210\u5f0fAI\u6539\u8fdb\u91ce\u706b\u9884\u6d4b\u548c\u6a21\u62df\uff0c\u63d0\u51fa\u4e86\u4e94\u65b9\u9762\u7684\u672a\u6765\u613f\u666f\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b9e\u65bd\u4e2d\u7684\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u91ce\u706b\u9884\u6d4b\u548c\u6a21\u62df\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u751f\u6210\u5f0fAI\u63d0\u4f9b\u4e86\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u548c\u6539\u8fdb\u6a21\u62df\u7684\u65b0\u673a\u9047\u3002", "method": "\u91c7\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u3001\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAEs\uff09\u3001Transformer\u548c\u6269\u6563\u6a21\u578b\u7b49\u751f\u6210\u5f0fAI\u65b9\u6cd5\uff0c\u7ed3\u5408\u4eba\u7c7b-AI\u534f\u4f5c\u6846\u67b6\u3002", "result": "\u751f\u6210\u5f0fAI\u80fd\u589e\u5f3a2D\u91ce\u706b\u6269\u6563\u9884\u6d4b\u548c\u751f\u6210\u66f4\u771f\u5b9e\u76843D\u6a21\u62df\uff0c\u540c\u65f6\u652f\u6301\u81ea\u52a8\u77e5\u8bc6\u63d0\u53d6\u548c\u6587\u732e\u5408\u6210\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u6709\u6f5c\u529b\u6210\u4e3a\u91ce\u706b\u7ba1\u7406\u7684\u57fa\u7840\u6846\u67b6\uff0c\u4f46\u9700\u89e3\u51b3\u76f8\u5173\u6311\u6218\u4ee5\u5b9e\u73b0\u5176\u5168\u90e8\u6f5c\u529b\u3002", "keywords": "\u91ce\u706b\u9884\u6d4b, \u751f\u6210\u5f0fAI, \u591a\u6a21\u6001\u6570\u636e, 3D\u6a21\u62df, \u4eba\u7c7b-AI\u534f\u4f5c"}}
{"id": "2506.01986", "pdf": "https://arxiv.org/pdf/2506.01986", "abs": "https://arxiv.org/abs/2506.01986", "authors": ["Selin Yildirim", "Deming Chen"], "title": "SpecMemo: Speculative Decoding is in Your Pocket", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Recent advancements in speculative decoding have demonstrated considerable\nspeedup across a wide array of large language model (LLM) tasks. Speculative\ndecoding inherently relies on sacrificing extra memory allocations to generate\nseveral candidate tokens, of which acceptance rate drives the speedup. However,\ndeploying speculative decoding on memory-constrained devices, such as mobile\nGPUs, remains as a significant challenge in real-world scenarios. In this work,\nwe present a device-aware inference engine named SpecMemo that can smartly\ncontrol memory allocations at finer levels to enable multi-turn chatbots with\nspeculative decoding on such limited memory devices. Our methodology stems from\ntheoretically modeling memory footprint of speculative decoding to determine a\nlower bound on the required memory budget while retaining speedup. SpecMemo\nempirically acquires a careful balance between minimizing redundant memory\nallocations for rejected candidate tokens and maintaining competitive\nperformance gains from speculation. Notably, with SpecMemo's memory management,\nwe maintain 96% of overall throughput from speculative decoding on MT-Bench,\nwith reduced generation-memory by 65% on single Nvidia Titan RTX. Given\nmultiple constrained GPUs, we build on top of previous speculative decoding\narchitectures to facilitate big-model inference by distributing\nLlama-2-70B-Chat model, on which we provide novel batched speculative decoding\nto increase usability of multiple small server GPUs. This novel framework\ndemonstrates 2x speedup over distributed and batched vanilla decoding with the\nbase model on eight AMD MI250 GPUs. Moreover, inference throughput increases\nremarkably 8x with batch size 10. Our work contributes to democratized LLM\napplications in resource-constrained environments, providing a pathway for\nfaster and cheaper deployment of real-world LLM applications with robust\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpecMemo\u7684\u8bbe\u5907\u611f\u77e5\u63a8\u7406\u5f15\u64ce\uff0c\u901a\u8fc7\u7cbe\u7ec6\u63a7\u5236\u5185\u5b58\u5206\u914d\uff0c\u4f7f\u5185\u5b58\u53d7\u9650\u8bbe\u5907\u80fd\u591f\u652f\u6301\u5e26\u63a8\u6d4b\u89e3\u7801\u7684\u591a\u8f6e\u804a\u5929\u673a\u5668\u4eba\uff0c\u5e76\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u63a8\u6d4b\u89e3\u7801\u867d\u80fd\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\uff0c\u4f46\u5728\u5185\u5b58\u53d7\u9650\u8bbe\u5907\uff08\u5982\u79fb\u52a8GPU\uff09\u4e0a\u90e8\u7f72\u4ecd\u5177\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5efa\u6a21\u63a8\u6d4b\u89e3\u7801\u7684\u5185\u5b58\u5360\u7528\uff0c\u786e\u5b9a\u6700\u4f4e\u5185\u5b58\u9700\u6c42\u5e76\u4f18\u5316\u5206\u914d\u3002\u7ed3\u5408\u5206\u5e03\u5f0f\u548c\u6279\u91cf\u63a8\u6d4b\u89e3\u7801\uff0c\u63d0\u5347\u591a\u5c0fGPU\u4e0a\u7684\u5927\u6a21\u578b\u63a8\u7406\u6548\u7387\u3002", "result": "SpecMemo\u5728MT-Bench\u4e0a\u4fdd\u7559\u4e8696%\u7684\u63a8\u6d4b\u89e3\u7801\u541e\u5410\u91cf\uff0c\u5355\u5361\u5185\u5b58\u51cf\u5c1165%\uff1b\u5728\u516b\u5757AMD MI250 GPU\u4e0a\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53472\u500d\uff0c\u6279\u91cf\u5904\u7406\u65f6\u901f\u5ea6\u63d0\u53478\u500d\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684LLM\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u90e8\u7f72\u65b9\u6848\u3002", "keywords": "\u63a8\u6d4b\u89e3\u7801, \u5185\u5b58\u4f18\u5316, \u8bbe\u5907\u611f\u77e5, \u5206\u5e03\u5f0f\u63a8\u7406, \u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.02239", "pdf": "https://arxiv.org/pdf/2506.02239", "abs": "https://arxiv.org/abs/2506.02239", "authors": ["Sofoklis Kakouros"], "title": "Investigating the Impact of Word Informativeness on Speech Emotion Recognition", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "In emotion recognition from speech, a key challenge lies in identifying\nspeech signal segments that carry the most relevant acoustic variations for\ndiscerning specific emotions. Traditional approaches compute functionals for\nfeatures such as energy and F0 over entire sentences or longer speech portions,\npotentially missing essential fine-grained variation in the long-form\nstatistics. This research investigates the use of word informativeness, derived\nfrom a pre-trained language model, to identify semantically important segments.\nAcoustic features are then computed exclusively for these identified segments,\nenhancing emotion recognition accuracy. The methodology utilizes standard\nacoustic prosodic features, their functionals, and self-supervised\nrepresentations. Results indicate a notable improvement in recognition\nperformance when features are computed on segments selected based on word\ninformativeness, underscoring the effectiveness of this approach.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u8bcd\u4fe1\u606f\u91cf\u8bc6\u522b\u8bed\u4e49\u91cd\u8981\u7247\u6bb5\uff0c\u8ba1\u7b97\u8fd9\u4e9b\u7247\u6bb5\u7684\u58f0\u5b66\u7279\u5f81\u4ee5\u63d0\u9ad8\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u6574\u53e5\u6216\u8f83\u957f\u8bed\u97f3\u6bb5\u4e0a\u8ba1\u7b97\u58f0\u5b66\u7279\u5f81\u7684\u529f\u80fd\u7edf\u8ba1\u91cf\uff0c\u53ef\u80fd\u5ffd\u7565\u7ec6\u7c92\u5ea6\u53d8\u5316\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u8bcd\u4fe1\u606f\u91cf\u7b5b\u9009\u91cd\u8981\u8bed\u97f3\u6bb5\uff0c\u8ba1\u7b97\u5176\u58f0\u5b66\u7279\u5f81\u548c\u81ea\u76d1\u7763\u8868\u793a\u3002", "result": "\u57fa\u4e8e\u8bcd\u4fe1\u606f\u9009\u62e9\u7684\u7247\u6bb5\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u8bc6\u522b\u7684\u6027\u80fd\u3002", "conclusion": "\u8bcd\u4fe1\u606f\u91cf\u7b5b\u9009\u7247\u6bb5\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8\u60c5\u611f\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "keywords": "\u60c5\u611f\u8bc6\u522b, \u8bed\u97f3\u4fe1\u53f7, \u8bcd\u4fe1\u606f\u91cf, \u81ea\u76d1\u7763\u5b66\u4e60"}}
{"id": "2506.02522", "pdf": "https://arxiv.org/pdf/2506.02522", "abs": "https://arxiv.org/abs/2506.02522", "authors": ["Xu Wan", "Wenyue Xu", "Chao Yang", "Mingyang Sun"], "title": "Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale Decision Making", "categories": ["cs.AI"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) and Reinforcement\nLearning (RL) have shown significant promise in decision-making tasks.\nNevertheless, for large-scale industrial decision problems, both approaches\nface distinct challenges: LLMs lack real-time long-sequence decision-making\ncapabilities, while RL struggles with sample efficiency in vast action spaces.\nTo bridge this gap, we propose Agents Co-Evolution (ACE), a synergistic\nframework between LLMs and RL agents for large-scale decision-making scenarios.\nACE introduces a dual-role trajectory refinement mechanism where LLMs act as\nboth Policy Actor and Value Critic during RL's training: the Actor refines\nsuboptimal actions via multi-step reasoning and environment validation, while\nthe Critic performs temporal credit assignment through trajectory-level reward\nshaping. Concurrently, RL agent enhances LLMs' task-specific decision-making\nwith high-quality fine-tuning datasets generated via prioritized experience\nreplay. Through extensive experiments across multiple power grid operation\nchallenges with action spaces exceeding 60K discrete actions, ACE demonstrates\nsuperior performance over existing RL methods and LLM-based methods.", "AI": {"tldr": "\u63d0\u51faACE\u6846\u67b6\uff0c\u7ed3\u5408LLMs\u548cRL\u89e3\u51b3\u5927\u89c4\u6a21\u5de5\u4e1a\u51b3\u7b56\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u89d2\u8272\u8f68\u8ff9\u4f18\u5316\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "motivation": "LLMs\u7f3a\u4e4f\u5b9e\u65f6\u957f\u5e8f\u5217\u51b3\u7b56\u80fd\u529b\uff0cRL\u5728\u5927\u89c4\u6a21\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u6837\u672c\u6548\u7387\u4f4e\uff0c\u9700\u4e00\u79cd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b9\u6cd5\u3002", "method": "ACE\u6846\u67b6\u901a\u8fc7\u53cc\u89d2\u8272\u8f68\u8ff9\u4f18\u5316\u673a\u5236\uff0cLLMs\u540c\u65f6\u4f5c\u4e3a\u7b56\u7565\u6267\u884c\u8005\u548c\u4ef7\u503c\u8bc4\u5224\u8005\uff0cRL\u751f\u6210\u9ad8\u8d28\u91cf\u5fae\u8c03\u6570\u636e\u3002", "result": "\u5728\u8d85\u8fc760K\u79bb\u6563\u52a8\u4f5c\u7684\u7535\u7f51\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cACE\u8868\u73b0\u4f18\u4e8e\u73b0\u6709RL\u548cLLM\u65b9\u6cd5\u3002", "conclusion": "ACE\u6709\u6548\u7ed3\u5408LLMs\u548cRL\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u51b3\u7b56\u95ee\u9898\u3002", "keywords": "LLMs, RL, \u51b3\u7b56, ACE, \u7535\u7f51\u64cd\u4f5c"}}
{"id": "2506.01987", "pdf": "https://arxiv.org/pdf/2506.01987", "abs": "https://arxiv.org/abs/2506.01987", "authors": ["Runkang Yang", "Peng Sun", "Xinyi Shang", "Yi Tang", "Tao Lin"], "title": "Equally Critical: Samples, Targets, and Their Mappings in Datasets", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Data inherently possesses dual attributes: samples and targets. For targets,\nknowledge distillation has been widely employed to accelerate model\nconvergence, primarily relying on teacher-generated soft target supervision.\nConversely, recent advancements in data-efficient learning have emphasized\nsample optimization techniques, such as dataset distillation, while neglected\nthe critical role of target. This dichotomy motivates our investigation into\nunderstanding how both sample and target collectively influence training\ndynamic. To address this gap, we first establish a taxonomy of existing\nparadigms through the lens of sample-target interactions, categorizing them\ninto distinct sample-to-target mapping strategies. Building upon this\nfoundation, we then propose a novel unified loss framework to assess their\nimpact on training efficiency. Through extensive empirical studies on our\nproposed strategies, we comprehensively analyze how variations in target and\nsample types, quantities, and qualities influence model training, providing six\nkey insights to enhance training efficacy.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6570\u636e\u548c\u76ee\u6807\u7684\u4ea4\u4e92\u5bf9\u8bad\u7ec3\u52a8\u6001\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u635f\u5931\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u63d0\u4f9b\u4e86\u516d\u9879\u5173\u952e\u89c1\u89e3\u4ee5\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u76ee\u6807\u4f18\u5316\uff0c\u5bfc\u81f4\u6837\u672c\u4e0e\u76ee\u6807\u4e4b\u95f4\u4ea4\u4e92\u7684\u5f71\u54cd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5efa\u7acb\u6837\u672c-\u76ee\u6807\u4ea4\u4e92\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u63d0\u51fa\u7edf\u4e00\u635f\u5931\u6846\u67b6\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u5206\u6790\u4e86\u76ee\u6807\u4e0e\u6837\u672c\u7c7b\u578b\u3001\u6570\u91cf\u548c\u8d28\u91cf\u5bf9\u8bad\u7ec3\u7684\u5f71\u54cd\uff0c\u5f97\u51fa\u516d\u9879\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "\u6837\u672c\u4e0e\u76ee\u6807\u7684\u4ea4\u4e92\u5bf9\u8bad\u7ec3\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u548c\u89c1\u89e3\u6709\u52a9\u4e8e\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002", "keywords": "\u77e5\u8bc6\u84b8\u998f\u3001\u6570\u636e\u9ad8\u6548\u5b66\u4e60\u3001\u6837\u672c\u4f18\u5316\u3001\u76ee\u6807\u4f18\u5316\u3001\u8bad\u7ec3\u52a8\u6001"}}
{"id": "2506.02264", "pdf": "https://arxiv.org/pdf/2506.02264", "abs": "https://arxiv.org/abs/2506.02264", "authors": ["Radin Shayanfar", "Chu Fei Luo", "Rohan Bhambhoria", "Samuel Dahan", "Xiaodan Zhu"], "title": "CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue Flow Alignment", "categories": ["cs.CL"], "comment": null, "summary": "It is often challenging to teach specialized, unseen tasks to dialogue\nsystems due to the high cost of expert knowledge, training data, and high\ntechnical difficulty. To support domain-specific applications - such as law,\nmedicine, or finance - it is essential to build frameworks that enable\nnon-technical experts to define, test, and refine system behaviour with minimal\neffort. Achieving this requires cross-disciplinary collaboration between\ndevelopers and domain specialists. In this work, we introduce a novel\nframework, CoDial (Code for Dialogue), that converts expert knowledge,\nrepresented as a novel structured heterogeneous graph, into executable\nconversation logic. CoDial can be easily implemented in existing guardrailing\nlanguages, such as Colang, to enable interpretable, modifiable, and true\nzero-shot specification of task-oriented dialogue systems. Empirically, CoDial\nachieves state-of-the-art performance on the STAR dataset for inference-based\nmodels and is competitive with similar baselines on the well-known MultiWOZ\ndataset. We also demonstrate CoDial's iterative improvement via manual and\nLLM-aided feedback, making it a practical tool for expert-guided alignment of\nLLMs in high-stakes domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCoDial\u7684\u65b0\u6846\u67b6\uff0c\u5c06\u4e13\u5bb6\u77e5\u8bc6\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u5bf9\u8bdd\u903b\u8f91\uff0c\u652f\u6301\u9886\u57df\u7279\u5b9a\u5bf9\u8bdd\u7cfb\u7edf\u7684\u5feb\u901f\u5f00\u53d1\u4e0e\u8fed\u4ee3\u3002", "motivation": "\u7531\u4e8e\u4e13\u5bb6\u77e5\u8bc6\u3001\u8bad\u7ec3\u6570\u636e\u548c\u6280\u672f\u96be\u5ea6\u7684\u9650\u5236\uff0c\u4e3a\u5bf9\u8bdd\u7cfb\u7edf\u6559\u6388\u4e13\u4e1a\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\u3002\u4f5c\u8005\u65e8\u5728\u5efa\u7acb\u4e00\u4e2a\u6846\u67b6\uff0c\u4f7f\u975e\u6280\u672f\u4e13\u5bb6\u80fd\u591f\u8f7b\u677e\u5b9a\u4e49\u548c\u6539\u8fdb\u7cfb\u7edf\u884c\u4e3a\u3002", "method": "CoDial\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u5f02\u6784\u56fe\u8868\u793a\u4e13\u5bb6\u77e5\u8bc6\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u5bf9\u8bdd\u903b\u8f91\uff0c\u652f\u6301\u73b0\u6709\u62a4\u680f\u8bed\u8a00\u7684\u5b9e\u73b0\u3002", "result": "CoDial\u5728STAR\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728MultiWOZ\u6570\u636e\u96c6\u4e0a\u4e0e\u57fa\u7ebf\u6a21\u578b\u8868\u73b0\u76f8\u5f53\u3002\u540c\u65f6\u5c55\u793a\u4e86\u901a\u8fc7\u4eba\u5de5\u548cLLM\u53cd\u9988\u8fdb\u884c\u8fed\u4ee3\u6539\u8fdb\u7684\u80fd\u529b\u3002", "conclusion": "CoDial\u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u5de5\u5177\uff0c\u652f\u6301\u9ad8\u98ce\u9669\u9886\u57df\u4e2d\u4e13\u5bb6\u5f15\u5bfc\u7684LLM\u5bf9\u9f50\uff0c\u4e14\u6613\u4e8e\u5b9e\u65bd\u548c\u4fee\u6539\u3002", "keywords": "\u5bf9\u8bdd\u7cfb\u7edf, \u4e13\u5bb6\u77e5\u8bc6, \u7ed3\u6784\u5316\u5f02\u6784\u56fe, \u96f6\u6837\u672c, \u8fed\u4ee3\u6539\u8fdb"}}
{"id": "2506.02565", "pdf": "https://arxiv.org/pdf/2506.02565", "abs": "https://arxiv.org/abs/2506.02565", "authors": ["Zhuoxuan Jiang", "Tianyang Zhang", "Peiyan Peng", "Jing Chen", "Yinong Xun", "Haotian Zhang", "Lichi Li", "Yong Li", "Shaohua Zhang"], "title": "Towards Generating Controllable and Solvable Geometry Problem by Leveraging Symbolic Deduction Engine", "categories": ["cs.AI"], "comment": "To Appear in ACL'25", "summary": "Generating high-quality geometry problems is both an important and\nchallenging task in education. Compared to math word problems, geometry\nproblems further emphasize multi-modal formats and the translation between\ninformal and formal languages. In this paper, we introduce a novel task for\ngeometry problem generation and propose a new pipeline method: the Symbolic\nDeduction Engine-based Geometry Problem Generation framework (SDE-GPG). The\nframework leverages a symbolic deduction engine and contains four main steps:\n(1) searching a predefined mapping table from knowledge points to extended\ndefinitions, (2) sampling extended definitions and performing symbolic\ndeduction, (3) filtering out unqualified problems, and (4) generating textual\nproblems and diagrams. Specifically, our method supports to avoid inherent\nbiases in translating natural language into formal language by designing the\nmapping table, and guarantees to control the generated problems in terms of\nknowledge points and difficulties by an elaborate checking function. With\nobtained formal problems, they are translated to natural language and the\naccompanying diagrams are automatically drew by rule-based methods. We conduct\nexperiments using real-world combinations of knowledge points from two public\ndatasets. The results demonstrate that the SDE-GPG can effectively generate\nreadable, solvable and controllable geometry problems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b26\u53f7\u63a8\u5bfc\u5f15\u64ce\u7684\u51e0\u4f55\u95ee\u9898\u751f\u6210\u6846\u67b6\uff08SDE-GPG\uff09\uff0c\u65e8\u5728\u89e3\u51b3\u51e0\u4f55\u95ee\u9898\u751f\u6210\u7684\u6311\u6218\uff0c\u652f\u6301\u591a\u6a21\u6001\u683c\u5f0f\u548c\u975e\u6b63\u5f0f\u4e0e\u6b63\u5f0f\u8bed\u8a00\u4e4b\u95f4\u7684\u8f6c\u6362\u3002", "motivation": "\u51e0\u4f55\u95ee\u9898\u751f\u6210\u5728\u6559\u80b2\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u6a21\u6001\u683c\u5f0f\u548c\u975e\u6b63\u5f0f\u4e0e\u6b63\u5f0f\u8bed\u8a00\u4e4b\u95f4\u7684\u8f6c\u6362\u3002", "method": "\u63d0\u51faSDE-GPG\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u6b65\u9aa4\uff1a\u77e5\u8bc6\u70b9\u6620\u5c04\u8868\u641c\u7d22\u3001\u6269\u5c55\u5b9a\u4e49\u91c7\u6837\u4e0e\u7b26\u53f7\u63a8\u5bfc\u3001\u95ee\u9898\u8fc7\u6ee4\u3001\u6587\u672c\u548c\u56fe\u8868\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSDE-GPG\u80fd\u6709\u6548\u751f\u6210\u53ef\u8bfb\u3001\u53ef\u89e3\u4e14\u53ef\u63a7\u7684\u51e0\u4f55\u95ee\u9898\u3002", "conclusion": "SDE-GPG\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u51e0\u4f55\u95ee\u9898\u751f\u6210\u7684\u591a\u6a21\u6001\u548c\u8bed\u8a00\u8f6c\u6362\u6311\u6218\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "keywords": "\u51e0\u4f55\u95ee\u9898\u751f\u6210, \u7b26\u53f7\u63a8\u5bfc\u5f15\u64ce, \u591a\u6a21\u6001, \u8bed\u8a00\u8f6c\u6362"}}
{"id": "2506.01988", "pdf": "https://arxiv.org/pdf/2506.01988", "abs": "https://arxiv.org/abs/2506.01988", "authors": ["Akshat Dubey", "Aleksandar An\u017eel", "Georges Hattab"], "title": "Surrogate Interpretable Graph for Random Decision Forests", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The field of health informatics has been profoundly influenced by the\ndevelopment of random forest models, which have led to significant advances in\nthe interpretability of feature interactions. These models are characterized by\ntheir robustness to overfitting and parallelization, making them particularly\nuseful in this domain. However, the increasing number of features and\nestimators in random forests can prevent domain experts from accurately\ninterpreting global feature interactions, thereby compromising trust and\nregulatory compliance. A method called the surrogate interpretability graph has\nbeen developed to address this issue. It uses graphs and mixed-integer linear\nprogramming to analyze and visualize feature interactions. This improves their\ninterpretability by visualizing the feature usage per\ndecision-feature-interaction table and the most dominant hierarchical decision\nfeature interactions for predictions. The implementation of a surrogate\ninterpretable graph enhances global interpretability, which is critical for\nsuch a high-stakes domain.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u66ff\u4ee3\u53ef\u89e3\u91ca\u6027\u56fe\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u5065\u5eb7\u4fe1\u606f\u5b66\u9886\u57df\u7684\u5168\u5c40\u53ef\u89e3\u91ca\u6027\uff0c\u89e3\u51b3\u4e86\u7279\u5f81\u6570\u91cf\u589e\u52a0\u5bfc\u81f4\u7684\u89e3\u91ca\u56f0\u96be\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u5065\u5eb7\u4fe1\u606f\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u5f81\u6570\u91cf\u548c\u4f30\u8ba1\u5668\u7684\u589e\u52a0\u4f7f\u5f97\u5168\u5c40\u7279\u5f81\u4ea4\u4e92\u7684\u89e3\u91ca\u53d8\u5f97\u56f0\u96be\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u548c\u5408\u89c4\u6027\u3002", "method": "\u4f7f\u7528\u56fe\u5f62\u548c\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u6765\u5206\u6790\u5e76\u53ef\u89c6\u5316\u7279\u5f81\u4ea4\u4e92\uff0c\u901a\u8fc7\u5c55\u793a\u6bcf\u4e2a\u51b3\u7b56-\u7279\u5f81\u4ea4\u4e92\u8868\u4e2d\u7684\u7279\u5f81\u4f7f\u7528\u60c5\u51b5\u4ee5\u53ca\u9884\u6d4b\u4e2d\u6700\u4e3b\u8981\u7684\u5c42\u6b21\u51b3\u7b56\u7279\u5f81\u4ea4\u4e92\uff0c\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u66ff\u4ee3\u53ef\u89e3\u91ca\u6027\u56fe\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u5168\u5c40\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u9886\u57df\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u5065\u5eb7\u4fe1\u606f\u5b66\u4e2d\u7684\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002", "keywords": "\u5065\u5eb7\u4fe1\u606f\u5b66, \u968f\u673a\u68ee\u6797, \u53ef\u89e3\u91ca\u6027, \u7279\u5f81\u4ea4\u4e92, \u66ff\u4ee3\u53ef\u89e3\u91ca\u6027\u56fe"}}
{"id": "2506.02279", "pdf": "https://arxiv.org/pdf/2506.02279", "abs": "https://arxiv.org/abs/2506.02279", "authors": ["Wenzheng Zhang", "Xi Victoria Lin", "Karl Stratos", "Wen-tau Yih", "Mingda Chen"], "title": "ImpRAG: Retrieval-Augmented Generation with Implicit Queries", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems traditionally treat retrieval\nand generation as separate processes, requiring explicit textual queries to\nconnect them. This separation can limit the ability of models to generalize\nacross diverse tasks. In this work, we propose a query-free RAG system, named\nImpRAG, which integrates retrieval and generation into a unified model. ImpRAG\nallows models to implicitly express their information needs, eliminating the\nneed for human-specified queries. By dividing pretrained decoder-only language\nmodels into specialized layer groups, ImpRAG optimizes retrieval and generation\ntasks simultaneously. Our approach employs a two-stage inference process, using\nthe same model parameters and forward pass for both retrieval and generation,\nthereby minimizing the disparity between retrievers and language models.\nExperiments on 8 knowledge-intensive tasks demonstrate that ImpRAG achieves\n3.6-11.5 improvements in exact match scores on unseen tasks with diverse\nformats, highlighting its effectiveness in enabling models to articulate their\nown information needs and generalize across tasks. Our analysis underscores the\nimportance of balancing retrieval and generation parameters and leveraging\ngeneration perplexities as retrieval training objectives for enhanced\nperformance.", "AI": {"tldr": "ImpRAG\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e0\u9700\u663e\u5f0f\u67e5\u8be2\u7684RAG\u7cfb\u7edf\uff0c\u901a\u8fc7\u7edf\u4e00\u68c0\u7d22\u548c\u751f\u6210\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfRAG\u7cfb\u7edf\u5c06\u68c0\u7d22\u548c\u751f\u6210\u4f5c\u4e3a\u72ec\u7acb\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002ImpRAG\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u6a21\u578b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "ImpRAG\u5c06\u9884\u8bad\u7ec3\u89e3\u7801\u5668\u8bed\u8a00\u6a21\u578b\u5206\u4e3a\u4e13\u7528\u5c42\u7ec4\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u63a8\u7406\u540c\u65f6\u4f18\u5316\u68c0\u7d22\u548c\u751f\u6210\u4efb\u52a1\u3002", "result": "\u57288\u4e2a\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\uff0cImpRAG\u7684\u7cbe\u786e\u5339\u914d\u5206\u6570\u63d0\u5347\u4e863.6-11.5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "ImpRAG\u901a\u8fc7\u5e73\u8861\u68c0\u7d22\u548c\u751f\u6210\u53c2\u6570\u53ca\u5229\u7528\u751f\u6210\u56f0\u60d1\u5ea6\u4f5c\u4e3a\u68c0\u7d22\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "keywords": "Retrieval-Augmented Generation, \u67e5\u8be2\u81ea\u7531, \u7edf\u4e00\u6a21\u578b, \u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1"}}
{"id": "2506.02568", "pdf": "https://arxiv.org/pdf/2506.02568", "abs": "https://arxiv.org/abs/2506.02568", "authors": ["Dongzhe Fan", "Yi Fang", "Jiajin Liu", "Djellel Difallah", "Qiaoyu Tan"], "title": "MLaGA: Multimodal Large Language and Graph Assistant", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated substantial efficacy in\nadvancing graph-structured data analysis. Prevailing LLM-based graph methods\nexcel in adapting LLMs to text-rich graphs, wherein node attributes are text\ndescriptions. However, their applications to multimodal graphs--where nodes are\nassociated with diverse attribute types, such as texts and images--remain\nunderexplored, despite their ubiquity in real-world scenarios. To bridge the\ngap, we introduce the Multimodal Large Language and Graph Assistant (MLaGA), an\ninnovative model that adeptly extends LLM capabilities to facilitate reasoning\nover complex graph structures and multimodal attributes. We first design a\nstructure-aware multimodal encoder to align textual and visual attributes\nwithin a unified space through a joint graph pre-training objective.\nSubsequently, we implement a multimodal instruction-tuning approach to\nseamlessly integrate multimodal features and graph structures into the LLM\nthrough lightweight projectors. Extensive experiments across multiple datasets\ndemonstrate the effectiveness of MLaGA compared to leading baseline methods,\nachieving superior performance in diverse graph learning tasks under both\nsupervised and transfer learning scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMLaGA\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u4e0e\u56fe\u52a9\u624b\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u6a21\u6001\u56fe\u5206\u6790\u4e2d\u7684\u672a\u5145\u5206\u63a2\u7d22\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u7684\u591a\u6a21\u6001\u7f16\u7801\u5668\u548c\u591a\u6a21\u6001\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u5b66\u4e60\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u56fe\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6587\u672c\u4e30\u5bcc\u7684\u56fe\uff0c\u800c\u591a\u6a21\u6001\u56fe\uff08\u5305\u542b\u591a\u79cd\u5c5e\u6027\u7c7b\u578b\uff0c\u5982\u6587\u672c\u548c\u56fe\u50cf\uff09\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u4e86\u7ed3\u6784\u611f\u77e5\u7684\u591a\u6a21\u6001\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u8054\u5408\u56fe\u9884\u8bad\u7ec3\u76ee\u6807\u5c06\u6587\u672c\u548c\u89c6\u89c9\u5c5e\u6027\u5bf9\u9f50\u5230\u7edf\u4e00\u7a7a\u95f4\uff1b\u968f\u540e\u91c7\u7528\u591a\u6a21\u6001\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6295\u5f71\u5668\u5c06\u591a\u6a21\u6001\u7279\u5f81\u548c\u56fe\u7ed3\u6784\u6574\u5408\u5230LLM\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMLaGA\u5728\u76d1\u7763\u548c\u8fc1\u79fb\u5b66\u4e60\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u73b0\u4e86\u591a\u6837\u56fe\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "MLaGA\u6210\u529f\u6269\u5c55\u4e86LLM\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u56fe\u7ed3\u6784\u548c\u591a\u6a21\u6001\u5c5e\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u56fe\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u591a\u6a21\u6001\u56fe\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u56fe\u5b66\u4e60\u3001\u591a\u6a21\u6001\u7f16\u7801\u5668\u3001\u6307\u4ee4\u8c03\u4f18"}}
{"id": "2506.01989", "pdf": "https://arxiv.org/pdf/2506.01989", "abs": "https://arxiv.org/abs/2506.01989", "authors": ["Chengxi Li", "Ming Xiao", "Mikael Skoglund"], "title": "Coded Robust Aggregation for Distributed Learning under Byzantine Attacks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "In this paper, we investigate the problem of distributed learning (DL) in the\npresence of Byzantine attacks. For this problem, various robust bounded\naggregation (RBA) rules have been proposed at the central server to mitigate\nthe impact of Byzantine attacks. However, current DL methods apply RBA rules\nfor the local gradients from the honest devices and the disruptive information\nfrom Byzantine devices, and the learning performance degrades significantly\nwhen the local gradients of different devices vary considerably from each\nother. To overcome this limitation, we propose a new DL method to cope with\nByzantine attacks based on coded robust aggregation (CRA-DL). Before training\nbegins, the training data are allocated to the devices redundantly. During\ntraining, in each iteration, the honest devices transmit coded gradients to the\nserver computed from the allocated training data, and the server then\naggregates the information received from both honest and Byzantine devices\nusing RBA rules. In this way, the global gradient can be approximately\nrecovered at the server to update the global model. Compared with current DL\nmethods applying RBA rules, the improvement of CRA-DL is attributed to the fact\nthat the coded gradients sent by the honest devices are closer to each other.\nThis closeness enhances the robustness of the aggregation against Byzantine\nattacks, since Byzantine messages tend to be significantly different from those\nof honest devices in this case. We theoretically analyze the convergence\nperformance of CRA-DL. Finally, we present numerical results to verify the\nsuperiority of the proposed method over existing baselines, showing its\nenhanced learning performance under Byzantine attacks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f16\u7801\u9c81\u68d2\u805a\u5408\uff08CRA-DL\uff09\u7684\u65b0\u578b\u5206\u5e03\u5f0f\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u62dc\u5360\u5ead\u653b\u51fb\u3002\u901a\u8fc7\u5197\u4f59\u5206\u914d\u8bad\u7ec3\u6570\u636e\u548c\u7f16\u7801\u68af\u5ea6\u4f20\u8f93\uff0c\u63d0\u5347\u4e86\u805a\u5408\u7684\u9c81\u68d2\u6027\u548c\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u65b9\u6cd5\u5728\u9762\u5bf9\u62dc\u5360\u5ead\u653b\u51fb\u65f6\uff0c\u7531\u4e8e\u672c\u5730\u68af\u5ea6\u5dee\u5f02\u8f83\u5927\uff0c\u5bfc\u81f4\u5b66\u4e60\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5728\u8bad\u7ec3\u5f00\u59cb\u524d\uff0c\u5197\u4f59\u5206\u914d\u8bad\u7ec3\u6570\u636e\uff1b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u8bda\u5b9e\u8bbe\u5907\u4f20\u8f93\u57fa\u4e8e\u5206\u914d\u6570\u636e\u7684\u7f16\u7801\u68af\u5ea6\uff0c\u670d\u52a1\u5668\u4f7f\u7528\u9c81\u68d2\u6709\u754c\u805a\u5408\u89c4\u5219\uff08RBA\uff09\u805a\u5408\u4fe1\u606f\uff0c\u6062\u590d\u5168\u5c40\u68af\u5ea6\u4ee5\u66f4\u65b0\u6a21\u578b\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86CRA-DL\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5176\u5728\u62dc\u5360\u5ead\u653b\u51fb\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u5b66\u4e60\u6027\u80fd\u3002", "conclusion": "CRA-DL\u901a\u8fc7\u7f16\u7801\u68af\u5ea6\u8bbe\u8ba1\u548c\u5197\u4f59\u6570\u636e\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5e03\u5f0f\u5b66\u4e60\u5728\u9762\u5bf9\u62dc\u5360\u5ead\u653b\u51fb\u65f6\u7684\u9c81\u68d2\u6027\u548c\u6548\u679c\u3002", "keywords": "\u5206\u5e03\u5f0f\u5b66\u4e60, \u62dc\u5360\u5ead\u653b\u51fb, \u9c81\u68d2\u6709\u754c\u805a\u5408\u89c4\u5219, \u7f16\u7801\u9c81\u68d2\u805a\u5408"}}
{"id": "2506.02283", "pdf": "https://arxiv.org/pdf/2506.02283", "abs": "https://arxiv.org/abs/2506.02283", "authors": ["Sofoklis Kakouros", "Haoyu Chen"], "title": "Sounding Like a Winner? Prosodic Differences in Post-Match Interviews", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "This study examines the prosodic characteristics associated with winning and\nlosing in post-match tennis interviews. Additionally, this research explores\nthe potential to classify match outcomes solely based on post-match interview\nrecordings using prosodic features and self-supervised learning (SSL)\nrepresentations. By analyzing prosodic elements such as pitch and intensity,\nalongside SSL models like Wav2Vec 2.0 and HuBERT, the aim is to determine\nwhether an athlete has won or lost their match. Traditional acoustic features\nand deep speech representations are extracted from the data, and machine\nlearning classifiers are employed to distinguish between winning and losing\nplayers. Results indicate that SSL representations effectively differentiate\nbetween winning and losing outcomes, capturing subtle speech patterns linked to\nemotional states. At the same time, prosodic cues -- such as pitch variability\n-- remain strong indicators of victory.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u97f5\u5f8b\u7279\u5f81\u548c\u81ea\u6211\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u8868\u5f81\uff0c\u63a2\u8ba8\u4e86\u7f51\u7403\u8d5b\u540e\u91c7\u8bbf\u4e2d\u80dc\u8d1f\u9009\u624b\u7684\u97f5\u5f8b\u7279\u5f81\u53ca\u5176\u5206\u7c7b\u6548\u679c\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u5206\u6790\u8d5b\u540e\u91c7\u8bbf\u4e2d\u7684\u97f5\u5f8b\u7279\u5f81\u548cSSL\u8868\u5f81\uff0c\u4e86\u89e3\u80dc\u8d1f\u9009\u624b\u7684\u58f0\u5b66\u5dee\u5f02\uff0c\u5e76\u63a2\u7d22\u5176\u5206\u7c7b\u6f5c\u529b\u3002", "method": "\u5229\u7528\u97f5\u5f8b\u7279\u5f81\uff08\u5982\u97f3\u9ad8\u548c\u5f3a\u5ea6\uff09\u548cSSL\u6a21\u578b\uff08Wav2Vec 2.0\u548cHuBERT\uff09\uff0c\u63d0\u53d6\u4f20\u7edf\u58f0\u5b66\u7279\u5f81\u4e0e\u6df1\u5ea6\u8bed\u97f3\u8868\u5f81\uff0c\u5e76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u533a\u5206\u80dc\u8d1f\u3002", "result": "SSL\u8868\u5f81\u80fd\u6709\u6548\u533a\u5206\u80dc\u8d1f\uff0c\u6355\u6349\u4e0e\u60c5\u7eea\u76f8\u5173\u7684\u7ec6\u5fae\u8bed\u97f3\u6a21\u5f0f\uff1b\u97f5\u5f8b\u7279\u5f81\uff08\u5982\u97f3\u9ad8\u53d8\u5f02\u6027\uff09\u4ecd\u662f\u80dc\u8d1f\u7684\u5f3a\u6307\u6807\u3002", "conclusion": "\u97f5\u5f8b\u7279\u5f81\u548cSSL\u8868\u5f81\u5728\u533a\u5206\u7f51\u7403\u8d5b\u80dc\u8d1f\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u7ed3\u5408\u60c5\u7eea\u76f8\u5173\u7684\u8bed\u97f3\u6a21\u5f0f\u3002", "keywords": "\u97f5\u5f8b\u7279\u5f81\u3001\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u3001Wav2Vec 2.0\u3001HuBERT\u3001\u673a\u5668\u5b66\u4e60"}}
{"id": "2506.02576", "pdf": "https://arxiv.org/pdf/2506.02576", "abs": "https://arxiv.org/abs/2506.02576", "authors": ["Haichen Wang", "Liu Yang", "Xinyuan Zhang", "Haomin Yu", "Ming Li", "Jilin Hu"], "title": "ADFormer: Aggregation Differential Transformer for Passenger Demand Forecasting", "categories": ["cs.AI"], "comment": "9 pages, 5 figures, 3 tables. IJCAI-2025", "summary": "Passenger demand forecasting helps optimize vehicle scheduling, thereby\nimproving urban efficiency. Recently, attention-based methods have been used to\nadequately capture the dynamic nature of spatio-temporal data. However,\nexisting methods that rely on heuristic masking strategies cannot fully adapt\nto the complex spatio-temporal correlations, hindering the model from focusing\non the right context. These works also overlook the high-level correlations\nthat exist in the real world. Effectively integrating these high-level\ncorrelations with the original correlations is crucial. To fill this gap, we\npropose the Aggregation Differential Transformer (ADFormer), which offers new\ninsights to demand forecasting promotion. Specifically, we utilize Differential\nAttention to capture the original spatial correlations and achieve attention\ndenoising. Meanwhile, we design distinct aggregation strategies based on the\nnature of space and time. Then, the original correlations are unified with the\nhigh-level correlations, enabling the model to capture holistic spatio-temporal\nrelations. Experiments conducted on taxi and bike datasets confirm the\neffectiveness and efficiency of our model, demonstrating its practical value.\nThe code is available at https://github.com/decisionintelligence/ADFormer.", "AI": {"tldr": "\u63d0\u51fa\u4e86Aggregation Differential Transformer (ADFormer)\uff0c\u901a\u8fc7\u5dee\u5206\u6ce8\u610f\u529b\u6355\u6349\u539f\u59cb\u7a7a\u95f4\u76f8\u5173\u6027\u5e76\u8fdb\u884c\u53bb\u566a\uff0c\u8bbe\u8ba1\u4e86\u65f6\u7a7a\u805a\u5408\u7b56\u7565\uff0c\u7edf\u4e00\u539f\u59cb\u4e0e\u9ad8\u5c42\u76f8\u5173\u6027\uff0c\u63d0\u5347\u9700\u6c42\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u65b9\u6cd5\u96be\u4ee5\u5b8c\u5168\u9002\u5e94\u590d\u6742\u7684\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u4e14\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u9ad8\u5c42\u76f8\u5173\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u6574\u5408\u539f\u59cb\u4e0e\u9ad8\u5c42\u76f8\u5173\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86ADFormer\uff0c\u5229\u7528\u5dee\u5206\u6ce8\u610f\u529b\u6355\u6349\u539f\u59cb\u7a7a\u95f4\u76f8\u5173\u6027\u5e76\u8fdb\u884c\u53bb\u566a\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u65f6\u7a7a\u7279\u6027\u7684\u4e0d\u540c\u805a\u5408\u7b56\u7565\uff0c\u7edf\u4e00\u539f\u59cb\u4e0e\u9ad8\u5c42\u76f8\u5173\u6027\u3002", "result": "\u5728\u51fa\u79df\u8f66\u548c\u81ea\u884c\u8f66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "ADFormer\u901a\u8fc7\u6574\u5408\u539f\u59cb\u4e0e\u9ad8\u5c42\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9700\u6c42\u9884\u6d4b\u6027\u80fd\u3002", "keywords": "\u9700\u6c42\u9884\u6d4b, \u65f6\u7a7a\u6570\u636e, \u6ce8\u610f\u529b\u673a\u5236, \u5dee\u5206\u6ce8\u610f\u529b, \u805a\u5408\u7b56\u7565"}}
{"id": "2506.02050", "pdf": "https://arxiv.org/pdf/2506.02050", "abs": "https://arxiv.org/abs/2506.02050", "authors": ["Qingyu Xiao", "Yuanlin Chang", "Youtian Du"], "title": "Decoupled Hierarchical Reinforcement Learning with State Abstraction for Discrete Grids", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, 6 figures", "summary": "Effective agent exploration remains a core challenge in reinforcement\nlearning (RL) for complex discrete state-space environments, particularly under\npartial observability. This paper presents a decoupled hierarchical RL\nframework integrating state abstraction (DcHRL-SA) to address this issue. The\nproposed method employs a dual-level architecture, consisting of a high level\nRL-based actor and a low-level rule-based policy, to promote effective\nexploration. Additionally, state abstraction method is incorporated to cluster\ndiscrete states, effectively lowering state dimensionality. Experiments\nconducted in two discrete customized grid environments demonstrate that the\nproposed approach consistently outperforms PPO in terms of exploration\nefficiency, convergence speed, cumulative reward, and policy stability. These\nresults demonstrate a practical approach for integrating decoupled hierarchical\npolicies and state abstraction in discrete grids with large-scale exploration\nspace. Code will be available at https://github.com/XQY169/DcHRL-SA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08DcHRL-SA\uff09\uff0c\u901a\u8fc7\u72b6\u6001\u62bd\u8c61\u548c\u53cc\u5c42\u67b6\u6784\u89e3\u51b3\u590d\u6742\u79bb\u6563\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u63a2\u7d22\u95ee\u9898\u3002", "motivation": "\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u590d\u6742\u79bb\u6563\u72b6\u6001\u7a7a\u95f4\u4e2d\uff0c\u6709\u6548\u7684\u667a\u80fd\u4f53\u63a2\u7d22\u4ecd\u7136\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u67b6\u6784\uff08\u9ad8\u5c42RL\u6f14\u5458\u548c\u4f4e\u5c42\u57fa\u4e8e\u89c4\u5219\u7684\u7b56\u7565\uff09\u7ed3\u5408\u72b6\u6001\u62bd\u8c61\u65b9\u6cd5\uff0c\u964d\u4f4e\u72b6\u6001\u7ef4\u5ea6\u3002", "result": "\u5728\u4e24\u4e2a\u79bb\u6563\u5b9a\u5236\u7f51\u683c\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u63a2\u7d22\u6548\u7387\u3001\u6536\u655b\u901f\u5ea6\u3001\u7d2f\u79ef\u5956\u52b1\u548c\u7b56\u7565\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8ePPO\u3002", "conclusion": "\u7ed3\u5408\u5206\u5c42\u7b56\u7565\u548c\u72b6\u6001\u62bd\u8c61\u4e3a\u5927\u89c4\u6a21\u63a2\u7d22\u7a7a\u95f4\u7684\u79bb\u6563\u7f51\u683c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u65b9\u6cd5\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60\u3001\u5206\u5c42RL\u3001\u72b6\u6001\u62bd\u8c61\u3001\u63a2\u7d22\u6548\u7387\u3001\u79bb\u6563\u72b6\u6001\u7a7a\u95f4"}}
{"id": "2506.02298", "pdf": "https://arxiv.org/pdf/2506.02298", "abs": "https://arxiv.org/abs/2506.02298", "authors": ["Thai Hoang", "Kung-Hsiang Huang", "Shirley Kokane", "Jianguo Zhang", "Zuxin Liu", "Ming Zhu", "Jake Grigsby", "Tian Lan", "Michael S Ryoo", "Chien-Sheng Wu", "Shelby Heinecke", "Huan Wang", "Silvio Savarese", "Caiming Xiong", "Juan Carlos Niebles"], "title": "LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "LAM Simulator framework for agentic data generation", "summary": "Large Action Models (LAMs) for AI Agents offer incredible potential but face\nchallenges due to the need for high-quality training data, especially for\nmulti-steps tasks that involve planning, executing tool calls, and responding\nto feedback. To address these issues, we present LAM SIMULATOR, a comprehensive\nframework designed for online exploration of agentic tasks with high-quality\nfeedback. Our framework features a dynamic task query generator, an extensive\ncollection of tools, and an interactive environment where Large Language Model\n(LLM) Agents can call tools and receive real-time feedback. This setup enables\nLLM Agents to explore and solve tasks autonomously, facilitating the discovery\nof multiple approaches to tackle any given task. The resulting action\ntrajectory data are then used to create high-quality training datasets for\nLAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena,\nhighlight the effectiveness of LAM SIMULATOR: models trained with\nself-generated datasets using our framework achieve significant performance\ngains, up to a 49.3\\% improvement over their original baselines. LAM SIMULATOR\nrequires minimal human input during dataset creation, highlighting LAM\nSIMULATOR's efficiency and effectiveness in speeding up development of AI\nagents.", "AI": {"tldr": "LAM SIMULATOR\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u7ebf\u63a2\u7d22\u4ee3\u7406\u4efb\u52a1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u4efb\u52a1\u67e5\u8be2\u3001\u63d0\u4f9b\u5de5\u5177\u96c6\u5408\u548c\u5b9e\u65f6\u53cd\u9988\uff0c\u5e2e\u52a9LLM\u4ee3\u7406\u81ea\u4e3b\u5b8c\u6210\u4efb\u52a1\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u4f7f\u7528\u8be5\u6846\u67b6\u7684\u6570\u636e\u8bad\u7ec3LAM\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff08\u6700\u9ad849.3%\uff09\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3LAMs\u5728\u9700\u8981\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u5c24\u5176\u662f\u591a\u6b65\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86LAM SIMULATOR\u6846\u67b6\u3002", "method": "\u6846\u67b6\u5305\u62ec\u52a8\u6001\u4efb\u52a1\u67e5\u8be2\u751f\u6210\u5668\u3001\u5de5\u5177\u96c6\u5408\u548c\u4ea4\u4e92\u73af\u5883\uff0c\u4f7fLLM\u4ee3\u7406\u80fd\u81ea\u4e3b\u63a2\u7d22\u4efb\u52a1\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728ToolBench\u548cCRMArena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u6846\u67b6\u751f\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff08\u9ad8\u8fbe49.3%\uff09\u3002", "conclusion": "LAM SIMULATOR\u80fd\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u52a0\u901fAI\u4ee3\u7406\u7684\u5f00\u53d1\u3002", "keywords": "Large Action Models, AI Agents, LAM SIMULATOR, training data, LLM Agents"}}
{"id": "2506.02580", "pdf": "https://arxiv.org/pdf/2506.02580", "abs": "https://arxiv.org/abs/2506.02580", "authors": ["Xuewen Luo", "Fengze Yang", "Fan Ding", "Xiangbo Gao", "Shuo Xing", "Yang Zhou", "Zhengzhong Tu", "Chenxi Liu"], "title": "V2X-UniPool: Unifying Multimodal Perception and Knowledge Reasoning for Autonomous Driving", "categories": ["cs.AI"], "comment": null, "summary": "Knowledge-driven autonomous driving systems(ADs) offer powerful reasoning\ncapabilities, but face two critical challenges: limited perception due to the\nshort-sightedness of single-vehicle sensors, and hallucination arising from the\nlack of real-time environmental grounding. To address these issues, this paper\nintroduces V2X-UniPool, a unified framework that integrates multimodal\nVehicle-to-Everything (V2X) data into a time-indexed and language-based\nknowledge pool. By leveraging a dual-query Retrieval-Augmented Generation (RAG)\nmechanism, which enables retrieval of both static and dynamic knowledge, our\nsystem enables ADs to perform accurate, temporally consistent reasoning over\nboth static environment and dynamic traffic context. Experiments on a\nreal-world cooperative driving dataset demonstrate that V2X-UniPool\nsignificantly enhances motion planning accuracy and reasoning capability.\nRemarkably, it enables even zero-shot vehicle-side models to achieve\nstate-of-the-art performance by leveraging V2X-UniPool, while simultaneously\nreducing transmission cost by over 99.9\\% compared to prior V2X methods.", "AI": {"tldr": "V2X-UniPool\u6846\u67b6\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001V2X\u6570\u636e\uff0c\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u611f\u77e5\u53d7\u9650\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8fd0\u52a8\u89c4\u5212\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u56e0\u5355\u8f66\u8f86\u4f20\u611f\u5668\u77ed\u89c6\u548c\u73af\u5883\u5b9e\u65f6\u6027\u4e0d\u8db3\u9762\u4e34\u611f\u77e5\u53d7\u9650\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u63d0\u51faV2X-UniPool\u6846\u67b6\uff0c\u96c6\u6210\u591a\u6a21\u6001V2X\u6570\u636e\uff0c\u91c7\u7528\u53cc\u67e5\u8be2RAG\u673a\u5236\u68c0\u7d22\u9759\u6001\u548c\u52a8\u6001\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cV2X-UniPool\u663e\u8457\u63d0\u5347\u8fd0\u52a8\u89c4\u5212\u7cbe\u5ea6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u96f6\u6837\u672c\u8f66\u8f86\u6a21\u578b\u6027\u80fd\u8fbeSOTA\uff0c\u4f20\u8f93\u6210\u672c\u964d\u4f4e99.9%\u3002", "conclusion": "V2X-UniPool\u6709\u6548\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5173\u952e\u6311\u6218\uff0c\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u6210\u672c\u3002", "keywords": "\u81ea\u52a8\u9a7e\u9a76, V2X, \u77e5\u8bc6\u9a71\u52a8, \u591a\u6a21\u6001, RAG"}}
{"id": "2506.02053", "pdf": "https://arxiv.org/pdf/2506.02053", "abs": "https://arxiv.org/abs/2506.02053", "authors": ["Xu Zhang", "Haoye Qiu", "Weixuan Liang", "Hui Liu", "Junhui Hou", "Yuheng Jia"], "title": "Generalization Performance of Ensemble Clustering: From Theory to Algorithm", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Ensemble clustering has demonstrated great success in practice; however, its\ntheoretical foundations remain underexplored. This paper examines the\ngeneralization performance of ensemble clustering, focusing on generalization\nerror, excess risk and consistency. We derive a convergence rate of\ngeneralization error bound and excess risk bound both of\n$\\mathcal{O}(\\sqrt{\\frac{\\log n}{m}}+\\frac{1}{\\sqrt{n}})$, with $n$ and $m$\nbeing the numbers of samples and base clusterings. Based on this, we prove that\nwhen $m$ and $n$ approach infinity and $m$ is significantly larger than log\n$n$, i.e., $m,n\\to \\infty, m\\gg \\log n$, ensemble clustering is consistent.\nFurthermore, recognizing that $n$ and $m$ are finite in practice, the\ngeneralization error cannot be reduced to zero. Thus, by assigning varying\nweights to finite clusterings, we minimize the error between the empirical\naverage clusterings and their expectation. From this, we theoretically\ndemonstrate that to achieve better clustering performance, we should minimize\nthe deviation (bias) of base clustering from its expectation and maximize the\ndifferences (diversity) among various base clusterings. Additionally, we derive\nthat maximizing diversity is nearly equivalent to a robust (min-max)\noptimization model. Finally, we instantiate our theory to develop a new\nensemble clustering algorithm. Compared with SOTA methods, our approach\nachieves average improvements of 6.1%, 7.3%, and 6.0% on 10 datasets w.r.t.\nNMI, ARI, and Purity. The code is available at https://github.com/xuz2019/GPEC.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u96c6\u6210\u805a\u7c7b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u63a8\u5bfc\u4e86\u6cdb\u5316\u8bef\u5dee\u548c\u8d85\u989d\u98ce\u9669\u7684\u6536\u655b\u7387\uff0c\u8bc1\u660e\u4e86\u5f53\u6837\u672c\u548c\u57fa\u805a\u7c7b\u6570\u91cf\u8d8b\u4e8e\u65e0\u7a77\u65f6\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u96c6\u6210\u805a\u7c7b\u7b97\u6cd5\u3002", "motivation": "\u96c6\u6210\u805a\u7c7b\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u7406\u8bba\u57fa\u7840\u5c1a\u4e0d\u5b8c\u5584\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7a76\u5176\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u6cdb\u5316\u8bef\u5dee\u548c\u8d85\u989d\u98ce\u9669\u7684\u6536\u655b\u7387\uff0c\u63d0\u51fa\u6700\u5c0f\u5316\u57fa\u805a\u7c7b\u7684\u504f\u5dee\u548c\u6700\u5927\u5316\u591a\u6837\u6027\uff0c\u5e76\u5b9e\u4f8b\u5316\u4e3a\u65b0\u7b97\u6cd5\u3002", "result": "\u65b0\u7b97\u6cd5\u572810\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u5347\u4e866.1%\u30017.3%\u548c6.0%\uff08NMI\u3001ARI\u548cPurity\uff09\u3002", "conclusion": "\u96c6\u6210\u805a\u7c7b\u7684\u6027\u80fd\u63d0\u5347\u4f9d\u8d56\u4e8e\u57fa\u805a\u7c7b\u7684\u504f\u5dee\u6700\u5c0f\u5316\u548c\u591a\u6837\u6027\u6700\u5927\u5316\uff0c\u7406\u8bba\u7ed3\u679c\u5728\u5b9e\u9a8c\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "keywords": "\u96c6\u6210\u805a\u7c7b,\u6cdb\u5316\u8bef\u5dee,\u8d85\u989d\u98ce\u9669,\u4e00\u81f4\u6027,\u591a\u6837\u6027"}}
{"id": "2506.02302", "pdf": "https://arxiv.org/pdf/2506.02302", "abs": "https://arxiv.org/abs/2506.02302", "authors": ["Russell Scheinberg", "Ameeta Agrawal", "Amber Shore", "So Young Lee"], "title": "Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Findings", "summary": "Large language models (LLMs) can explain grammatical rules, yet they often\nfail to apply those rules when judging sentence acceptability. We present\n\"grammar prompting\", an explain-then-process paradigm: a large LLM first\nproduces a concise explanation of the relevant syntactic phenomenon, then that\nexplanation is fed back as additional context to the target model -- either an\nLLM or a smaller language model (SLM) -- before deciding which sentence of a\nminimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian\nRuBLiMP benchmarks, this simple prompt design yields substantial improvements\nover strong baselines across many syntactic phenomena. Feeding an LLM's\nmetalinguistic explanation back to the target model bridges the gap between\nknowing a rule and using it. On SLMs, grammar prompting alone trims the average\nLLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by\n56% (13.0 pp -> 5.8 pp), all at negligible cost. The lightweight,\nlanguage-agnostic cue lets low-cost SLMs approach frontier-LLM performance in\nmultilingual settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8bed\u6cd5\u63d0\u793a\u201d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5148\u89e3\u91ca\u8bed\u6cd5\u89c4\u5219\uff0c\u518d\u5229\u7528\u8fd9\u4e9b\u89e3\u91ca\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u4fe1\u606f\u5e2e\u52a9\u76ee\u6807\u6a21\u578b\u5224\u65ad\u53e5\u5b50\u6b63\u786e\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u8bed\u6cd5\u5224\u65ad\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u89e3\u91ca\u8bed\u6cd5\u89c4\u5219\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5374\u96be\u4ee5\u6b63\u786e\u8fd0\u7528\u8fd9\u4e9b\u89c4\u5219\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63d0\u793a\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u201c\u89e3\u91ca-\u5904\u7406\u201d\u8303\u5f0f\uff0c\u5373\u9996\u5148\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8bed\u6cd5\u73b0\u8c61\u7684\u7b80\u77ed\u89e3\u91ca\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u89e3\u91ca\u4f5c\u4e3a\u9644\u52a0\u4e0a\u4e0b\u6587\u8f93\u5165\u76ee\u6807\u6a21\u578b\uff08\u53ef\u4ee5\u662f\u5927\u578b\u6216\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff09\uff0c\u4ee5\u5224\u65ad\u53e5\u5b50\u5bf9\u9519\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u8bed\u6cd5\u5224\u65ad\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u4f7f\u7528\u65f6\uff0c\u80fd\u591f\u5927\u5e45\u7f29\u5c0f\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u8bed\u6cd5\u63d0\u793a\u65b9\u6cd5\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u8bed\u8a00\u65e0\u5173\u7684\u6280\u672f\uff0c\u53ef\u4ee5\u6709\u6548\u5e2e\u52a9\u4f4e\u6210\u672c\u7684\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u63a5\u8fd1\u524d\u6cbf\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u8bed\u6cd5\u63d0\u793a,\u8bed\u6cd5\u5224\u65ad,\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5,\u5c0f\u578b\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.02594", "pdf": "https://arxiv.org/pdf/2506.02594", "abs": "https://arxiv.org/abs/2506.02594", "authors": ["Ruibo Duan", "Yuxin Liu", "Xinyao Dong", "Chenglin Fan"], "title": "EALG: Evolutionary Adversarial Generation of Language Model-Guided Generators for Combinatorial Optimization", "categories": ["cs.AI"], "comment": null, "summary": "Generating challenging instances is crucial for the evaluation and\nadvancement of combinatorial optimization solvers. In this work, we introduce\nEALG (Evolutionary Adversarial Generation of Language Model-Guided Generators),\na novel framework that automates the co-evolution of optimization problem\ninstances and their corresponding heuristic solvers using large language models\n(LLMs). EALG leverages a mutation-based adversarial approach that dynamically\nevolves instance generation procedures to create increasingly difficult\nproblems, while simultaneously synthesizing adaptive heuristic algorithms\nthrough interactions with LLMs guided by algorithmic structure. Unlike existing\napproaches that focus solely on static benchmark creation or manual solver\ndesign, EALG provides a seamless pipeline from instance generation to solver\nsynthesis. Experimental results demonstrate that EALG generates significantly\nharder instances than current benchmarks, and its synthesized solvers\ngeneralize effectively across a broad spectrum of combinatorial tasks. This\nwork explores a new paradigm for combinatorial optimization that integrates\ninstance generation with solver design, resulting in state-of-the-art\nperformance.", "AI": {"tldr": "EALG \u662f\u4e00\u4e2a\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5316\u5171\u540c\u8fdb\u5316\u89e3\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u5b9e\u4f8b\u53ca\u5176\u542f\u53d1\u5f0f\u6c42\u89e3\u5668\uff0c\u52a8\u6001\u751f\u6210\u66f4\u96be\u7684\u5b9e\u4f8b\u5e76\u5408\u6210\u9002\u5e94\u6027\u7b97\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u6311\u6218\u6027\u5b9e\u4f8b\u548c\u6c42\u89e3\u5668\uff0c\u63a8\u52a8\u7ec4\u5408\u4f18\u5316\u89e3\u7b97\u5668\u7684\u8bc4\u4f30\u4e0e\u53d1\u5c55\uff0c\u89e3\u51b3\u73b0\u6709\u9759\u6001\u57fa\u51c6\u6216\u624b\u52a8\u8bbe\u8ba1\u7684\u5c40\u9650\u6027\u3002", "method": "\u57fa\u4e8e\u8fdb\u5316\u7684\u5bf9\u6297\u6027\u751f\u6210\u65b9\u6cd5\uff08EALG\uff09\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u52a8\u6001\u8fdb\u5316\u89e3\u7684\u751f\u6210\u8fc7\u7a0b\u548c\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "result": "EALG \u751f\u6210\u7684\u5b9e\u4f8b\u6bd4\u5f53\u524d\u57fa\u51c6\u66f4\u96be\uff0c\u5176\u5408\u6210\u7684\u6c42\u89e3\u5668\u5728\u591a\u79cd\u7ec4\u5408\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "EALG \u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u5408\u5b9e\u4f8b\u751f\u6210\u4e0e\u6c42\u89e3\u5668\u8bbe\u8ba1\u7684\u65b0\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "keywords": "\u7ec4\u5408\u4f18\u5316\u3001\u8bed\u8a00\u6a21\u578b\u3001\u5bf9\u6297\u6027\u751f\u6210\u3001\u542f\u53d1\u5f0f\u7b97\u6cd5\u3001\u81ea\u52a8\u5316\u6c42\u89e3\u5668"}}
{"id": "2506.02062", "pdf": "https://arxiv.org/pdf/2506.02062", "abs": "https://arxiv.org/abs/2506.02062", "authors": ["Malik A. Altayar", "Muhyeeddin Alqaraleh", "Mowafaq Salem Alzboon", "Wesam T. Almagharbeh"], "title": "Predicting Blood Type: Assessing Model Performance with ROC Analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Introduction: Personal identification is a critical aspect of forensic\nsciences, security, and healthcare. While conventional biometrics systems such\nas DNA profiling and iris scanning offer high accuracy, they are time-consuming\nand costly. Objectives: This study investigates the relationship between\nfingerprint patterns and ABO blood group classification to explore potential\ncorrelations between these two traits. Methods: The study analyzed 200\nindividuals, categorizing their fingerprints into three types: loops, whorls,\nand arches. Blood group classification was also recorded. Statistical analysis,\nincluding chi-square and Pearson correlation tests, was used to assess\nassociations between fingerprint patterns and blood groups. Results: Loops were\nthe most common fingerprint pattern, while blood group O+ was the most\nprevalent among the participants. Statistical analysis revealed no significant\ncorrelation between fingerprint patterns and blood groups (p > 0.05),\nsuggesting that these traits are independent. Conclusions: Although the study\nshowed limited correlation between fingerprint patterns and ABO blood groups,\nit highlights the importance of future research using larger and more diverse\npopulations, incorporating machine learning approaches, and integrating\nmultiple biometric signals. This study contributes to forensic science by\nemphasizing the need for rigorous protocols and comprehensive investigations in\npersonal identification.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u6307\u7eb9\u6a21\u5f0f\u4e0eABO\u8840\u578b\u5206\u7c7b\u7684\u5173\u7cfb\uff0c\u672a\u53d1\u73b0\u663e\u8457\u76f8\u5173\u6027\uff0c\u4f46\u5f3a\u8c03\u4e86\u672a\u6765\u7814\u7a76\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u4f20\u7edf\u751f\u7269\u8bc6\u522b\u6280\u672f\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u7814\u7a76\u63a2\u7d22\u6307\u7eb9\u4e0e\u8840\u578b\u7684\u6f5c\u5728\u5173\u8054\u4ee5\u4f18\u5316\u8eab\u4efd\u8bc6\u522b\u3002", "method": "\u5206\u6790200\u540d\u53c2\u4e0e\u8005\u7684\u6307\u7eb9\u7c7b\u578b\uff08\u6597\u5f62\u3001\u73af\u5f62\u3001\u5f13\u5f62\uff09\u4e0e\u8840\u578b\uff0c\u4f7f\u7528\u5361\u65b9\u68c0\u9a8c\u548c\u76ae\u5c14\u900a\u76f8\u5173\u5206\u6790\u3002", "result": "\u6307\u7eb9\u6a21\u5f0f\u4e0e\u8840\u578b\u65e0\u663e\u8457\u76f8\u5173\u6027\uff08p > 0.05\uff09\uff0c\u6597\u5f62\u6307\u7eb9\u548cO+\u8840\u578b\u6700\u5e38\u89c1\u3002", "conclusion": "\u7814\u7a76\u672a\u53d1\u73b0\u663e\u8457\u5173\u8054\uff0c\u4f46\u5efa\u8bae\u672a\u6765\u6269\u5927\u6837\u672c\u591a\u6837\u6027\u5e76\u5f15\u5165\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "keywords": "\u6307\u7eb9\u6a21\u5f0f, ABO\u8840\u578b, \u76f8\u5173\u6027\u5206\u6790, \u6cd5\u533b\u5b66, \u4e2a\u4eba\u8bc6\u522b"}}
{"id": "2506.02321", "pdf": "https://arxiv.org/pdf/2506.02321", "abs": "https://arxiv.org/abs/2506.02321", "authors": ["Pegah Alipoormolabashi", "Ajay Patel", "Niranjan Balasubramanian"], "title": "Quantifying Misattribution Unfairness in Authorship Attribution", "categories": ["cs.CL"], "comment": null, "summary": "Authorship misattribution can have profound consequences in real life. In\nforensic settings simply being considered as one of the potential authors of an\nevidential piece of text or communication can result in undesirable scrutiny.\nThis raises a fairness question: Is every author in the candidate pool at equal\nrisk of misattribution? Standard evaluation measures for authorship attribution\nsystems do not explicitly account for this notion of fairness. We introduce a\nsimple measure, Misattribution Unfairness Index (MAUIk), which is based on how\noften authors are ranked in the top k for texts they did not write. Using this\nmeasure we quantify the unfairness of five models on two different datasets.\nAll models exhibit high levels of unfairness with increased risks for some\nauthors. Furthermore, we find that this unfairness relates to how the models\nembed the authors as vectors in the latent search space. In particular, we\nobserve that the risk of misattribution is higher for authors closer to the\ncentroid (or center) of the embedded authors in the haystack. These results\nindicate the potential for harm and the need for communicating with and\ncalibrating end users on misattribution risk when building and providing such\nmodels for downstream use.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8861\u91cf\u4f5c\u8005\u5f52\u5c5e\u4e0d\u516c\u5e73\u6027\u7684\u6307\u6807MAUIk\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4e94\u79cd\u6a21\u578b\u5728\u6570\u636e\u96c6\u4e0a\u7684\u4e0d\u516c\u5e73\u6027\uff0c\u6307\u51fa\u6a21\u578b\u5d4c\u5165\u65b9\u5f0f\u4e0e\u4e0d\u516c\u5e73\u6027\u7684\u5173\u7cfb\u3002", "motivation": "\u4f5c\u8005\u5f52\u5c5e\u9519\u8bef\u5728\u73b0\u5b9e\u4e2d\u53ef\u80fd\u5e26\u6765\u4e25\u91cd\u540e\u679c\uff0c\u5c24\u5176\u662f\u6cd5\u533b\u573a\u666f\u4e0b\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u672a\u660e\u786e\u8003\u8651\u516c\u5e73\u6027\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u6307\u6807\u6765\u8861\u91cf\u4e0d\u516c\u5e73\u6027\u3002", "method": "\u5f15\u5165Misattribution Unfairness Index (MAUIk)\u6307\u6807\uff0c\u57fa\u4e8e\u4f5c\u8005\u5728\u672a\u64b0\u5199\u6587\u672c\u4e2d\u7684\u524dk\u6392\u540d\u9891\u7387\uff0c\u8bc4\u4f30\u4e94\u79cd\u6a21\u578b\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6240\u6709\u6a21\u578b\u5747\u8868\u73b0\u51fa\u9ad8\u6c34\u5e73\u7684\u4e0d\u516c\u5e73\u6027\uff0c\u90e8\u5206\u4f5c\u8005\u98ce\u9669\u66f4\u9ad8\u3002\u4e0d\u516c\u5e73\u6027\u4e0e\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5d4c\u5165\u4f5c\u8005\u7684\u65b9\u5f0f\u76f8\u5173\uff0c\u9760\u8fd1\u4e2d\u5fc3\u70b9\u7684\u4f5c\u8005\u98ce\u9669\u66f4\u5927\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6a21\u578b\u53ef\u80fd\u5e26\u6765\u6f5c\u5728\u5371\u5bb3\uff0c\u9700\u5728\u4f7f\u7528\u65f6\u4e0e\u7ec8\u7aef\u7528\u6237\u6c9f\u901a\u5e76\u6821\u51c6\u8bef\u5f52\u5c5e\u98ce\u9669\u3002", "keywords": "\u4f5c\u8005\u5f52\u5c5e\u3001\u516c\u5e73\u6027\u3001MAUIk\u3001\u6a21\u578b\u8bc4\u4f30\u3001\u6f5c\u5728\u7a7a\u95f4\u5d4c\u5165"}}
{"id": "2506.02609", "pdf": "https://arxiv.org/pdf/2506.02609", "abs": "https://arxiv.org/abs/2506.02609", "authors": ["Tianfan Jiang", "Mei Wu", "Wenchao Weng", "Dewen Seng", "Yiqian Lin"], "title": "A Time-Enhanced Data Disentanglement Network for Traffic Flow Forecasting", "categories": ["cs.AI"], "comment": null, "summary": "In recent years, traffic flow prediction has become a highlight in the field\nof intelligent transportation systems. However, due to the temporal variations\nand dynamic spatial correlations of traffic data, traffic prediction remains\nhighly challenging.Traditional spatiotemporal networks, which rely on\nend-to-end training, often struggle to handle the diverse data dependencies of\nmultiple traffic flow patterns. Additionally, traffic flow variations are\nhighly sensitive to temporal information changes. Regrettably, other\nresearchers have not sufficiently recognized the importance of temporal\ninformation.To address these challenges, we propose a novel approach called A\nTime-Enhanced Data Disentanglement Network for Traffic Flow Forecasting\n(TEDDN). This network disentangles the originally complex and intertwined\ntraffic data into stable patterns and trends. By flexibly learning temporal and\nnode information through a dynamic graph enhanced by a temporal feature\nextraction module, TEDDN demonstrates significant efficacy in disentangling and\nextracting complex traffic information. Experimental evaluations and ablation\nstudies on four real-world datasets validate the superiority of our method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTEDDN\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u4e2d\u590d\u6742\u7684\u65f6\u7a7a\u4f9d\u8d56\u6027\u548c\u52a8\u6001\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u79bb\u548c\u63d0\u53d6\u7a33\u5b9a\u7684\u4ea4\u901a\u6a21\u5f0f\u4e0e\u8d8b\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6548\u679c\u3002", "motivation": "\u7531\u4e8e\u4ea4\u901a\u6570\u636e\u7684\u65f6\u7a7a\u52a8\u6001\u6027\u548c\u591a\u6837\u6027\uff0c\u4f20\u7edf\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u65f6\u7a7a\u7f51\u7edc\u96be\u4ee5\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u4ea4\u901a\u6d41\u91cf\u6a21\u5f0f\u4f9d\u8d56\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7814\u7a76\u5bf9\u65f6\u95f4\u4fe1\u606f\u7684\u91cd\u89c6\u4e0d\u8db3\uff0c\u5bfc\u81f4\u9884\u6d4b\u6548\u679c\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e86TEDDN\u7f51\u7edc\uff0c\u901a\u8fc7\u52a8\u6001\u56fe\u548c\u65f6\u95f4\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff0c\u5c06\u590d\u6742\u7684\u4ea4\u901a\u6570\u636e\u89e3\u8026\u4e3a\u7a33\u5b9a\u7684\u6a21\u5f0f\u548c\u8d8b\u52bf\uff0c\u7075\u6d3b\u5b66\u4e60\u65f6\u7a7a\u4fe1\u606f\u548c\u8282\u70b9\u4fe1\u606f\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86TEDDN\u7684\u4f18\u8d8a\u6027\uff0c\u8868\u660e\u5176\u5728\u89e3\u8026\u548c\u63d0\u53d6\u590d\u6742\u4ea4\u901a\u4fe1\u606f\u65b9\u9762\u6709\u663e\u8457\u6548\u679c\u3002", "conclusion": "TEDDN\u65b9\u6cd5\u901a\u8fc7\u589e\u5f3a\u65f6\u95f4\u4fe1\u606f\u7684\u5229\u7528\u548c\u52a8\u6001\u56fe\u7684\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "keywords": "\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\uff0c\u65f6\u7a7a\u52a8\u6001\u6027\uff0c\u6570\u636e\u89e3\u8026\uff0c\u65f6\u95f4\u7279\u5f81\u63d0\u53d6\uff0c\u52a8\u6001\u56fe"}}
{"id": "2506.02065", "pdf": "https://arxiv.org/pdf/2506.02065", "abs": "https://arxiv.org/abs/2506.02065", "authors": ["Shriraj P. Sawant", "Krishna P. Miyapuram"], "title": "EWGN: Elastic Weight Generation and Context Switching in Deep Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "The ability to learn and retain a wide variety of tasks is a hallmark of\nhuman intelligence that has inspired research in artificial general\nintelligence. Continual learning approaches provide a significant step towards\nachieving this goal. It has been known that task variability and context\nswitching are challenging for learning in neural networks. Catastrophic\nforgetting refers to the poor performance on retention of a previously learned\ntask when a new task is being learned. Switching between different task\ncontexts can be a useful approach to mitigate the same by preventing the\ninterference between the varying task weights of the network. This paper\nintroduces Elastic Weight Generative Networks (EWGN) as an idea for context\nswitching between two different tasks. The proposed EWGN architecture uses an\nadditional network that generates the weights of the primary network\ndynamically while consolidating the weights learned. The weight generation is\ninput-dependent and thus enables context switching. Using standard computer\nvision datasets, namely MNIST and fashion-MNIST, we analyse the retention of\npreviously learned task representations in Fully Connected Networks,\nConvolutional Neural Networks, and EWGN architectures with Stochastic Gradient\nDescent and Elastic Weight Consolidation learning algorithms. Understanding\ndynamic weight generation and context-switching ability can be useful in\nenabling continual learning for improved performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5f39\u6027\u6743\u91cd\u751f\u6210\u7f51\u7edc\uff08EWGN\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u6743\u91cd\u548c\u4e0a\u4e0b\u6587\u5207\u6362\u6765\u7f13\u89e3\u795e\u7ecf\u7f51\u7edc\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u53d7\u4eba\u7c7b\u5b66\u4e60\u591a\u6837\u4efb\u52a1\u80fd\u529b\u7684\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u5b9e\u73b0\u4eba\u5de5\u667a\u80fd\u7684\u901a\u7528\u667a\u80fd\uff0c\u89e3\u51b3\u4efb\u52a1\u53d8\u5f02\u548c\u4e0a\u4e0b\u6587\u5207\u6362\u5bf9\u795e\u7ecf\u7f51\u7edc\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faEWGN\u67b6\u6784\uff0c\u5229\u7528\u9644\u52a0\u7f51\u7edc\u52a8\u6001\u751f\u6210\u4e3b\u7f51\u7edc\u6743\u91cd\uff0c\u5b9e\u73b0\u8f93\u5165\u4f9d\u8d56\u7684\u4e0a\u4e0b\u6587\u5207\u6362\uff1b\u5728MNIST\u548cfashion-MNIST\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e0d\u540c\u7f51\u7edc\u67b6\u6784\u548c\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1EWGN\u5728\u4fdd\u6301\u5148\u524d\u4efb\u52a1\u8868\u5f81\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u52a8\u6001\u6743\u91cd\u751f\u6210\u548c\u4e0a\u4e0b\u6587\u5207\u6362\u80fd\u529b\u6709\u52a9\u4e8e\u63d0\u5347\u6301\u7eed\u5b66\u4e60\u6027\u80fd\u3002", "keywords": "\u6301\u7eed\u5b66\u4e60,\u707e\u96be\u6027\u9057\u5fd8,\u5f39\u6027\u6743\u91cd\u751f\u6210\u7f51\u7edc,\u4e0a\u4e0b\u6587\u5207\u6362,\u52a8\u6001\u6743\u91cd\u751f\u6210"}}
{"id": "2506.02326", "pdf": "https://arxiv.org/pdf/2506.02326", "abs": "https://arxiv.org/abs/2506.02326", "authors": ["Berk Atil", "Namrata Sureddy", "Rebecca J. Passonneau"], "title": "Something Just Like TRuST : Toxicity Recognition of Span and Target", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Toxicity in online content, including content generated by language models,\nhas become a critical concern due to its potential for negative psychological\nand social impact. This paper introduces TRuST, a comprehensive dataset\ndesigned to improve toxicity detection that merges existing datasets, and has\nlabels for toxicity, target social group, and toxic spans. It includes a\ndiverse range of target groups such as ethnicity, gender, religion, disability,\nand politics, with both human/machine-annotated and human machine-generated\ndata. We benchmark state-of-the-art large language models (LLMs) on toxicity\ndetection, target group identification, and toxic span extraction. We find that\nfine-tuned models consistently outperform zero-shot and few-shot prompting,\nthough performance remains low for certain social groups. Further, reasoning\ncapabilities do not significantly improve performance, indicating that LLMs\nhave weak social reasoning skills.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86TRuST\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6539\u8fdb\u6bd2\u6027\u68c0\u6d4b\uff0c\u5e76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6bd2\u6027\u68c0\u6d4b\u3001\u76ee\u6807\u7fa4\u4f53\u8bc6\u522b\u548c\u6bd2\u6027\u7247\u6bb5\u63d0\u53d6\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u5728\u7ebf\u5185\u5bb9\u4e2d\u7684\u6bd2\u6027\u95ee\u9898\u5bf9\u5fc3\u7406\u548c\u793e\u4f1a\u6709\u8d1f\u9762\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5408\u5e76\u73b0\u6709\u6570\u636e\u96c6\u5e76\u6dfb\u52a0\u6807\u7b7e\uff0c\u6784\u5efaTRuST\u6570\u636e\u96c6\uff0c\u5e76\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u5fae\u8c03\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u96f6\u6837\u672c\u6216\u5c11\u6837\u672c\u63d0\u793a\uff0c\u4f46\u5bf9\u67d0\u4e9b\u793e\u4f1a\u7fa4\u4f53\u6548\u679c\u4e0d\u4f73\uff0c\u4e14\u63a8\u7406\u80fd\u529b\u672a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "LLMs\u5728\u793e\u4f1a\u63a8\u7406\u80fd\u529b\u4e0a\u8f83\u5f31\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "keywords": "\u6bd2\u6027\u68c0\u6d4b,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u793e\u4f1a\u7fa4\u4f53,TRuST\u6570\u636e\u96c6"}}
{"id": "2506.02648", "pdf": "https://arxiv.org/pdf/2506.02648", "abs": "https://arxiv.org/abs/2506.02648", "authors": ["Yue Yang", "MingKang Chen", "Qihua Liu", "Mengkang Hu", "Qiguang Chen", "Gengrui Zhang", "Shuyue Hu", "Guangtao Zhai", "Yu Qiao", "Yu Wang", "Wenqi Shao", "Ping Luo"], "title": "Truly Assessing Fluid Intelligence of Large Language Models through Dynamic Reasoning Evaluation", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated impressive\nreasoning capacities that mirror human-like thinking. However, whether LLMs\npossess genuine fluid intelligence (i.e., the ability to reason abstractly and\ngeneralize rules in novel situations) remains an open question. Existing\nreasoning benchmarks either focus on domain-specific knowledge (crystallized\nintelligence) or lack interpretability. To address these limitations, we\npropose DRE-Bench, a dynamic reasoning evaluation benchmark grounded in a\nhierarchical cognitive framework. DRE-Bench consists of 36 abstract reasoning\ntasks organized across four cognitive levels, with each task featuring multiple\ndynamic variants that test the same underlying latent rule. This design enables\nfine-grained, interpretable, and reliable assessments of fluid intelligence. We\nevaluate a range of state-of-the-art LLMs, including both general LLMs (GPT-4o,\nClaude 3.7) and reasoning LLMs (o1, DeepSeek-R1, QwQ, Skywork-OR1).\nExperimental results reveal that although most LLMs achieve competent and\nrobust performance in low-level cognition, they struggle with high-level\ncognition and exhibit limited generalization as task complexity grows. Our\nfindings highlight the gap between current LLMs and true human-like fluid\nintelligence and offer a new path for systematically tracking reasoning\nprogress in LLMs.", "AI": {"tldr": "DRE-Bench\u662f\u4e00\u4e2a\u52a8\u6001\u63a8\u7406\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u5927\u8bed\u8a00\u6a21\u578b\u5728\u62bd\u8c61\u63a8\u7406\u548c\u591a\u5c42\u6b21\u8ba4\u77e5\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u5176\u5728\u9ad8\u7ea7\u8ba4\u77e5\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u5907\u6d41\u4f53\u667a\u80fd\uff08\u62bd\u8c61\u63a8\u7406\u548c\u65b0\u60c5\u5883\u4e2d\u7684\u89c4\u5219\u6cdb\u5316\u80fd\u529b\uff09\uff0c\u586b\u8865\u73b0\u6709\u57fa\u51c6\u5728\u53ef\u89e3\u91ca\u6027\u548c\u9886\u57df\u901a\u7528\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDRE-Bench\u57fa\u51c6\uff0c\u5305\u542b36\u4e2a\u62bd\u8c61\u63a8\u7406\u4efb\u52a1\uff0c\u5206\u4e3a\u56db\u4e2a\u8ba4\u77e5\u5c42\u6b21\uff0c\u6bcf\u4e2a\u4efb\u52a1\u6709\u591a\u4e2a\u52a8\u6001\u53d8\u4f53\u6d4b\u8bd5\u76f8\u540c\u7684\u6f5c\u5728\u89c4\u5219\u3002", "result": "\u591a\u6570\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u5c42\u6b21\u8ba4\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u4f46\u5728\u9ad8\u5c42\u6b21\u8ba4\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "conclusion": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u6d41\u4f53\u667a\u80fd\u4ecd\u6709\u5dee\u8ddd\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u63a8\u7406\u80fd\u529b\uff1bDRE-Bench\u4e3a\u7cfb\u7edf\u8ffd\u8e2a\u63a8\u7406\u8fdb\u5c55\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u6d41\u4f53\u667a\u80fd, \u62bd\u8c61\u63a8\u7406, DRE-Bench, \u8ba4\u77e5\u5c42\u6b21"}}
{"id": "2506.02070", "pdf": "https://arxiv.org/pdf/2506.02070", "abs": "https://arxiv.org/abs/2506.02070", "authors": ["Peter Holderrieth", "Ezra Erives"], "title": "An Introduction to Flow Matching and Diffusion Models", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion and flow-based models have become the state of the art for\ngenerative AI across a wide range of data modalities, including images, videos,\nshapes, molecules, music, and more! These notes are originally from\nhttps://diffusion.csail.mit.edu/, as taught at MIT over the 2025 IAP (winter)\nterm, and are intended to accompany other course content, including lectures\nand labs. Overall, they function as a self-contained introduction to both flow\nmatching and diffusion models, starting with ordinary and stochastic\ndifferential equations, and culminating in flow matching, score matching,\nclassifier-free guidance, and the inner workings of modern, state-of-the-art\nmodels for image and video. These notes, and the accompanying course, are ideal\nfor students and practitioners alike who want to develop a principled\nunderstanding of the theory and practice of generative AI.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u6269\u6563\u548c\u57fa\u4e8e\u6d41\u7684\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u5df2\u6210\u4e3a\u751f\u6210\u5f0fAI\u7684\u6700\u65b0\u6280\u672f\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6570\u636e\u7c7b\u578b\u3002\u5185\u5bb9\u6db5\u76d6\u4ece\u5fae\u5206\u65b9\u7a0b\u5230\u73b0\u4ee3\u5148\u8fdb\u6a21\u578b\u7684\u7406\u8bba\u4e0e\u5b9e\u8df5\u3002", "motivation": "\u65e8\u5728\u4e3a\u751f\u6210\u5f0fAI\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e00\u4e2a\u7cfb\u7edf\u7684\u4ecb\u7ecd\uff0c\u5e2e\u52a9\u5b66\u751f\u548c\u4ece\u4e1a\u8005\u6df1\u5165\u7406\u89e3\u6269\u6563\u548c\u57fa\u4e8e\u6d41\u7684\u6a21\u578b\u3002", "method": "\u8bfe\u7a0b\u7b14\u8bb0\u7ed3\u5408\u4e86\u5fae\u5206\u65b9\u7a0b\u3001\u6d41\u5339\u914d\u3001\u5206\u6570\u5339\u914d\u7b49\u65b9\u6cd5\uff0c\u4ecb\u7ecd\u4e86\u73b0\u4ee3\u6269\u6563\u6a21\u578b\u7684\u5185\u90e8\u5de5\u4f5c\u539f\u7406\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u5957\u5b8c\u6574\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u8df5\u6307\u5357\uff0c\u9002\u7528\u4e8e\u751f\u6210\u5f0fAI\u7684\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002", "conclusion": "\u8fd9\u4e9b\u8bfe\u7a0b\u7b14\u8bb0\u548c\u914d\u5957\u8bfe\u7a0b\u9002\u5408\u5e0c\u671b\u7cfb\u7edf\u5730\u5b66\u4e60\u751f\u6210\u5f0fAI\u7406\u8bba\u548c\u6280\u672f\u7684\u4eba\u3002", "keywords": "\u6269\u6563\u6a21\u578b, \u57fa\u4e8e\u6d41\u7684\u6a21\u578b, \u751f\u6210\u5f0fAI, \u5fae\u5206\u65b9\u7a0b, \u6d41\u5339\u914d"}}
{"id": "2506.02338", "pdf": "https://arxiv.org/pdf/2506.02338", "abs": "https://arxiv.org/abs/2506.02338", "authors": ["Hyungjoo Chae", "Dongjin Kang", "Jihyuk Kim", "Beong-woo Kwak", "Sunghyun Park", "Haeju Park", "Jinyoung Yeo", "Moontae Lee", "Kyungjae Lee"], "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL", "categories": ["cs.CL"], "comment": "ACL 2025 Industry", "summary": "With the release of R1, a publicly available large reasoning model (LRM),\nresearchers commonly train new LRMs by training language models on R1's long\nchain-of-thought (CoT) inferences. While prior works show that LRMs'\ncapabilities can be reproduced through direct distillation, the continued\nreliance on the existing models (e.g., R1) remains a critical limitation in\nadvancing the field. As a first step toward independent LRM development, this\npaper explores the possibility of constructing a long CoT dataset with LLMs\nthat are not trained for inference-time scaling. To this end, we present the\nLong CoT Collection, a dataset of 100K CoT rationales annotated using existing\nshort CoT LLMs. We develop a pipeline that induces o1's novel reasoning\nstrategies into short CoT LLMs, enabling them to think longer and introducing\ncontrollability over the thought budget to better manage the overthinking\nproblem. Our extensive analyses validate that our dataset achieves quality\ncomparable to--or slightly below--R1. Furthermore, our experiments demonstrate\nthat training on our dataset not only strengthens general reasoning skills, but\nalso provides a strong foundation for reinforcement learning--models\ninitialized on our data achieve 2-3x larger gains with RLVR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u4e86\u5229\u7528\u672a\u9488\u5bf9\u63a8\u7406\u65f6\u95f4\u6269\u5c55\u8bad\u7ec3\u7684LLMs\u6784\u5efa\u957f\u94fe\u601d\u7ef4\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86Long CoT Collection\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u5f53\u524d\u4f9d\u8d56\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08\u5982R1\uff09\u9650\u5236\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\uff0c\u56e0\u6b64\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u72ec\u7acb\u7684\u957f\u94fe\u601d\u7ef4\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u77ed\u94fe\u601d\u7ef4LLMs\u6807\u6ce8100K\u957f\u94fe\u601d\u7ef4\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u8bf1\u5bfc\u65b0\u63a8\u7406\u7b56\u7565\u7684\u6d41\u7a0b\uff0c\u5e76\u5f15\u5165\u4e86\u601d\u60f3\u9884\u7b97\u7684\u53ef\u63a7\u6027\u3002", "result": "\u6570\u636e\u96c6\u8d28\u91cf\u63a5\u8fd1R1\uff0c\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u4e0d\u4ec5\u63d0\u5347\u4e00\u822c\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u57fa\u7840\u3002", "conclusion": "\u8bba\u6587\u6210\u529f\u8bc1\u660e\u4e86\u901a\u8fc7\u77ed\u94fe\u601d\u7ef4LLMs\u6784\u5efa\u957f\u94fe\u601d\u7ef4\u6570\u636e\u96c6\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "\u957f\u94fe\u601d\u7ef4, \u6570\u636e\u96c6\u6784\u5efa, \u63a8\u7406\u80fd\u529b, \u5f3a\u5316\u5b66\u4e60"}}
{"id": "2506.02649", "pdf": "https://arxiv.org/pdf/2506.02649", "abs": "https://arxiv.org/abs/2506.02649", "authors": ["Yousef Emami", "Hao Zhou", "Miguel Gutierrez Gaitan", "Kai Li", "Luis Almeida", "Zhu Han"], "title": "From Prompts to Protection: Large Language Model-Enabled In-Context Learning for Smart Public Safety UAV", "categories": ["cs.AI", "53-01", "C.2"], "comment": "8 pages, 4 figures", "summary": "A public safety Unmanned Aerial Vehicle (UAV) enhances situational awareness\nin emergency response. Its agility and ability to optimize mobility and\nestablish Line-of-Sight (LoS) communication make it increasingly vital for\nmanaging emergencies such as disaster response, search and rescue, and wildfire\nmonitoring. While Deep Reinforcement Learning (DRL) has been applied to\noptimize UAV navigation and control, its high training complexity, low sample\nefficiency, and simulation-to-reality gap limit its practicality in public\nsafety. Recent advances in Large Language Models (LLMs) offer a compelling\nalternative. With strong reasoning and generalization capabilities, LLMs can\nadapt to new tasks through In-Context Learning (ICL), which enables task\nadaptation via natural language prompts and example-based guidance, without\nretraining. Deploying LLMs at the network edge, rather than in the cloud,\nfurther reduces latency and preserves data privacy, thereby making them\nsuitable for real-time, mission-critical public safety UAVs. This paper\nproposes the integration of LLM-enabled ICL with public safety UAV to address\nthe key functions, such as path planning and velocity control, in the context\nof emergency response. We present a case study on data collection scheduling\nwhere the LLM-enabled ICL framework can significantly reduce packet loss\ncompared to conventional approaches, while also mitigating potential\njailbreaking vulnerabilities. Finally, we discuss LLM optimizers and specify\nfuture research directions. The ICL framework enables adaptive, context-aware\ndecision-making for public safety UAV, thus offering a lightweight and\nefficient solution for enhancing UAV autonomy and responsiveness in\nemergencies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u516c\u5171\u5b89\u5168\u65e0\u4eba\u673a\uff08UAV\uff09\u7ed3\u5408\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4f18\u5316\u5e94\u6025\u54cd\u5e94\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u548c\u901f\u5ea6\u63a7\u5236\uff0c\u51cf\u5c11\u5ef6\u8fdf\u5e76\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u5728\u65e0\u4eba\u673a\u5bfc\u822a\u4e2d\u5b58\u5728\u8bad\u7ec3\u590d\u6742\u3001\u6837\u672c\u6548\u7387\u4f4e\u7b49\u95ee\u9898\uff0c\u800cLLM\u56e0\u5176\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u65e0\u4eba\u673a\u5728\u516c\u5171\u5b89\u5168\u5e94\u6025\u54cd\u5e94\u4e2d\u63d0\u4f9b\u4e86\u66f4\u8f7b\u91cf\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u80fd\u529b\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u548c\u793a\u4f8b\u6307\u5bfc\u65e0\u4eba\u673a\u6267\u884c\u4efb\u52a1\uff0c\u7ed3\u5408\u8fb9\u7f18\u8ba1\u7b97\u51cf\u5c11\u5ef6\u8fdf\u5e76\u4fdd\u62a4\u9690\u79c1\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8eLLM\u7684ICL\u6846\u67b6\u663e\u8457\u51cf\u5c11\u4e86\u6570\u636e\u5305\u4e22\u5931\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u6f5c\u5728\u7684\u5b89\u5168\u6f0f\u6d1e\u98ce\u9669\u3002", "conclusion": "LLM\u7684ICL\u6846\u67b6\u4e3a\u516c\u5171\u5b89\u5168\u65e0\u4eba\u673a\u63d0\u4f9b\u4e86\u81ea\u9002\u5e94\u3001\u60c5\u5883\u611f\u77e5\u7684\u51b3\u7b56\u652f\u6301\uff0c\u662f\u63d0\u5347\u65e0\u4eba\u673a\u5728\u5e94\u6025\u54cd\u5e94\u4e2d\u81ea\u4e3b\u6027\u548c\u54cd\u5e94\u80fd\u529b\u7684\u6709\u6548\u65b9\u6848\u3002", "keywords": "\u516c\u5171\u5b89\u5168\u65e0\u4eba\u673a\uff08UAV\uff09\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u3001\u5e94\u6025\u54cd\u5e94\u3001\u8def\u5f84\u89c4\u5212"}}
{"id": "2506.02077", "pdf": "https://arxiv.org/pdf/2506.02077", "abs": "https://arxiv.org/abs/2506.02077", "authors": ["Yoonjun Cho", "Soeun Kim", "Dongjae Jeon", "Kyelim Lee", "Beomsoo Lee", "Albert No"], "title": "Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "Decomposing weight matrices into quantization and low-rank components\n($\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$) is a widely used\ntechnique for compressing large language models (LLMs). Existing joint\noptimization methods iteratively alternate between quantization and low-rank\napproximation. However, these methods tend to prioritize one component at the\nexpense of the other, resulting in suboptimal decompositions that fail to\nleverage each component's unique strengths. In this work, we introduce\nOutlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank\ncomponents the specific role of capturing activation-sensitive weights. This\nstructured decomposition mitigates outliers' negative impact on quantization,\nenabling more effective balance between quantization and low-rank\napproximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B\ndemonstrate that incorporating ODLRI into the joint optimization framework\nconsistently reduces activation-aware error, minimizes quantization scale, and\nimproves perplexity and zero-shot accuracy in low-bit settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aODLRI\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5206\u89e3\u6743\u91cd\u77e9\u9635\u6765\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u538b\u7f29\u6548\u679c\uff0c\u63d0\u5347\u91cf\u5316\u548c\u4f4e\u79e9\u8fd1\u4f3c\u7684\u5e73\u8861\u6027\u3002", "motivation": "\u73b0\u6709\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u5728\u91cf\u5316\u548c\u4f4e\u79e9\u8fd1\u4f3c\u4e4b\u95f4\u4ea4\u66ff\u8fdb\u884c\uff0c\u4f46\u5f80\u5f80\u727a\u7272\u4e00\u65b9\u6548\u679c\uff0c\u5bfc\u81f4\u5206\u89e3\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5f15\u5165ODLRI\u65b9\u6cd5\uff0c\u4e13\u95e8\u8ba9\u4f4e\u79e9\u5206\u91cf\u6355\u6349\u6fc0\u6d3b\u654f\u611f\u7684\u6743\u91cd\uff0c\u7ed3\u6784\u5316\u5206\u89e3\u4ee5\u51cf\u8f7b\u5f02\u5e38\u503c\u5bf9\u91cf\u5316\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u5b9e\u9a8c\u9a8c\u8bc1\uff0cODLRI\u663e\u8457\u51cf\u5c11\u4e86\u6fc0\u6d3b\u611f\u77e5\u8bef\u5dee\uff0c\u964d\u4f4e\u4e86\u91cf\u5316\u89c4\u6a21\uff0c\u63d0\u5347\u4e86\u4f4e\u6bd4\u7279\u8bbe\u7f6e\u4e0b\u7684\u56f0\u60d1\u5ea6\u548c\u96f6\u6837\u672c\u51c6\u786e\u6027\u3002", "conclusion": "ODLRI\u901a\u8fc7\u5e73\u8861\u91cf\u5316\u548c\u4f4e\u79e9\u8fd1\u4f3c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u538b\u7f29\u6548\u679c\u3002", "keywords": "\u6743\u91cd\u77e9\u9635\u5206\u89e3, \u91cf\u5316, \u4f4e\u79e9\u8fd1\u4f3c, \u5927\u578b\u8bed\u8a00\u6a21\u578b, ODLRI"}}
{"id": "2506.02347", "pdf": "https://arxiv.org/pdf/2506.02347", "abs": "https://arxiv.org/abs/2506.02347", "authors": ["Jiaming Li", "Yukun Chen", "Ziqiang Liu", "Minghuan Tan", "Lei Zhang", "Yunshui Li", "Run Luo", "Longze Chen", "Jing Luo", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Wei Zhou", "Min Yang"], "title": "STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation", "categories": ["cs.CL"], "comment": null, "summary": "Stories are central to human culture, serving to share ideas, preserve\ntraditions, and foster connections. Automatic story generation, a key\nadvancement in artificial intelligence (AI), offers new possibilities for\ncreating personalized content, exploring creative ideas, and enhancing\ninteractive experiences. However, existing methods struggle to maintain\nnarrative coherence and logical consistency. This disconnect compromises the\noverall storytelling experience, underscoring the need for substantial\nimprovements. Inspired by human cognitive processes, we introduce Storyteller,\na novel approach that systemically improves the coherence and consistency of\nautomatically generated stories. Storyteller introduces a plot node structure\nbased on linguistically grounded subject verb object (SVO) triplets, which\ncapture essential story events and ensure a consistent logical flow. Unlike\nprevious methods, Storyteller integrates two dynamic modules, the STORYLINE and\nnarrative entity knowledge graph (NEKG),that continuously interact with the\nstory generation process. This integration produces structurally sound,\ncohesive and immersive narratives. Extensive experiments demonstrate that\nStoryteller significantly outperforms existing approaches, achieving an 84.33%\naverage win rate through human preference evaluation. At the same time, it is\nalso far ahead in other aspects including creativity, coherence, engagement,\nand relevance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u5b66SVO\u4e09\u5143\u7ec4\u7684\u81ea\u52a8\u6545\u4e8b\u751f\u6210\u65b9\u6cd5Storyteller\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6545\u4e8b\u7684\u4e00\u81f4\u6027\u548c\u8fde\u8d2f\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u6545\u4e8b\u751f\u6210\u65b9\u6cd5\u5728\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0cStoryteller\u65e8\u5728\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8eSVO\u4e09\u5143\u7ec4\u7684\u5267\u60c5\u8282\u70b9\u7ed3\u6784\uff0c\u7ed3\u5408\u52a8\u6001\u6a21\u5757STORYLINE\u548cNEKG\uff0c\u6301\u7eed\u4ea4\u4e92\u4f18\u5316\u6545\u4e8b\u751f\u6210\u3002", "result": "\u5728\u4eba\u7c7b\u504f\u597d\u8bc4\u4f30\u4e2d\u5e73\u5747\u80dc\u51fa\u7387\u8fbe84.33%\uff0c\u5e76\u5728\u521b\u610f\u3001\u8fde\u8d2f\u6027\u3001\u5438\u5f15\u529b\u548c\u76f8\u5173\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Storyteller\u901a\u8fc7\u7cfb\u7edf\u6027\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u751f\u6210\u6545\u4e8b\u7684\u903b\u8f91\u8fde\u8d2f\u6027\u548c\u53d9\u4e8b\u8d28\u91cf\u3002", "keywords": "\u81ea\u52a8\u6545\u4e8b\u751f\u6210\uff0cSVO\u4e09\u5143\u7ec4\uff0c\u53d9\u4e8b\u8fde\u8d2f\u6027\uff0cNEKG"}}
{"id": "2506.02668", "pdf": "https://arxiv.org/pdf/2506.02668", "abs": "https://arxiv.org/abs/2506.02668", "authors": ["Frederico Metelo", "Alexandre Oliveira", "Stevo Rackovi\u0107", "Pedro \u00c1kos Costa", "Cl\u00e1udia Soares"], "title": "FAuNO: Semi-Asynchronous Federated Reinforcement Learning Framework for Task Offloading in Edge Systems", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Edge computing addresses the growing data demands of connected-device\nnetworks by placing computational resources closer to end users through\ndecentralized infrastructures. This decentralization challenges traditional,\nfully centralized orchestration, which suffers from latency and resource\nbottlenecks. We present \\textbf{FAuNO} -- \\emph{Federated Asynchronous Network\nOrchestrator} -- a buffered, asynchronous \\emph{federated\nreinforcement-learning} (FRL) framework for decentralized task offloading in\nedge systems. FAuNO adopts an actor-critic architecture in which local actors\nlearn node-specific dynamics and peer interactions, while a federated critic\naggregates experience across agents to encourage efficient cooperation and\nimprove overall system performance. Experiments in the \\emph{PeersimGym}\nenvironment show that FAuNO consistently matches or exceeds heuristic and\nfederated multi-agent RL baselines in reducing task loss and latency,\nunderscoring its adaptability to dynamic edge-computing scenarios.", "AI": {"tldr": "FAuNO\u662f\u4e00\u79cd\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u7cfb\u7edf\u7684\u5f02\u6b65\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u4efb\u52a1\u5378\u8f7d\u89e3\u51b3\u5ef6\u8fdf\u548c\u8d44\u6e90\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u96c6\u4e2d\u5f0f\u7f16\u6392\u5728\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u5b58\u5728\u5ef6\u8fdf\u548c\u8d44\u6e90\u74f6\u9888\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u3002", "method": "FAuNO\u91c7\u7528\u6f14\u5458-\u8bc4\u8bba\u5bb6\u67b6\u6784\uff0c\u672c\u5730\u6f14\u5458\u5b66\u4e60\u8282\u70b9\u52a8\u6001\u548c\u4ea4\u4e92\uff0c\u8054\u90a6\u8bc4\u8bba\u5bb6\u805a\u5408\u7ecf\u9a8c\u4ee5\u4fc3\u8fdb\u534f\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFAuNO\u5728\u51cf\u5c11\u4efb\u52a1\u635f\u5931\u548c\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8e\u542f\u53d1\u5f0f\u548c\u8054\u90a6\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u3002", "conclusion": "FAuNO\u5728\u52a8\u6001\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5f3a\u8c03\u5176\u9002\u5e94\u6027\u548c\u9ad8\u6548\u6027\u3002", "keywords": "\u8fb9\u7f18\u8ba1\u7b97, \u8054\u90a6\u5f3a\u5316\u5b66\u4e60, \u53bb\u4e2d\u5fc3\u5316, \u4efb\u52a1\u5378\u8f7d, \u5f02\u6b65\u7f16\u6392"}}
{"id": "2506.02079", "pdf": "https://arxiv.org/pdf/2506.02079", "abs": "https://arxiv.org/abs/2506.02079", "authors": ["Xuefeng Jiang", "Tian Wen", "Zhiqin Yang", "Lvhua Wu", "Yufeng Chen", "Sheng Sun", "Yuwei Wang", "Min Liu"], "title": "Robust Federated Learning against Noisy Clients via Masked Optimization", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Under review", "summary": "In recent years, federated learning (FL) has made significant advance in\nprivacy-sensitive applications. However, it can be hard to ensure that FL\nparticipants provide well-annotated data for training. The corresponding\nannotations from different clients often contain complex label noise at varying\nlevels. This label noise issue has a substantial impact on the performance of\nthe trained models, and clients with greater noise levels can be largely\nattributed for this degradation. To this end, it is necessary to develop an\neffective optimization strategy to alleviate the adverse effects of these noisy\nclients.In this study, we present a two-stage optimization framework,\nMaskedOptim, to address this intricate label noise problem. The first stage is\ndesigned to facilitate the detection of noisy clients with higher label noise\nrates. The second stage focuses on rectifying the labels of the noisy clients'\ndata through an end-to-end label correction mechanism, aiming to mitigate the\nnegative impacts caused by misinformation within datasets. This is achieved by\nlearning the potential ground-truth labels of the noisy clients' datasets via\nbackpropagation. To further enhance the training robustness, we apply the\ngeometric median based model aggregation instead of the commonly-used vanilla\naveraged model aggregation. We implement sixteen related methods and conduct\nevaluations on three image datasets and one text dataset with diverse label\nnoise patterns for a comprehensive comparison. Extensive experimental results\nindicate that our proposed framework shows its robustness in different\nscenarios. Additionally, our label correction framework effectively enhances\nthe data quality of the detected noisy clients' local datasets. % Our codes\nwill be open-sourced to facilitate related research communities. Our codes are\navailable via https://github.com/Sprinter1999/MaskedOptim .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMaskedOptim\u7684\u4e24\u9636\u6bb5\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u590d\u6742\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u901a\u8fc7\u68c0\u6d4b\u9ad8\u566a\u58f0\u5ba2\u6237\u7aef\u548c\u6807\u7b7e\u6821\u6b63\u673a\u5236\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u63d0\u4f9b\u7684\u6807\u6ce8\u6570\u636e\u5e38\u542b\u6709\u590d\u6742\u6807\u7b7e\u566a\u58f0\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u9700\u89e3\u51b3\u9ad8\u566a\u58f0\u5ba2\u6237\u7aef\u5e26\u6765\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u68c0\u6d4b\u9ad8\u566a\u58f0\u5ba2\u6237\u7aef\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u7aef\u5230\u7aef\u6807\u7b7e\u6821\u6b63\u673a\u5236\u4fee\u6b63\u566a\u58f0\u6570\u636e\u6807\u7b7e\uff0c\u5e76\u4f7f\u7528\u51e0\u4f55\u4e2d\u503c\u805a\u5408\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u6709\u6548\u63d0\u5347\u566a\u58f0\u5ba2\u6237\u7aef\u672c\u5730\u6570\u636e\u8d28\u91cf\u3002", "conclusion": "MaskedOptim\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u6570\u636e\u8d28\u91cf\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60\u3001\u6807\u7b7e\u566a\u58f0\u3001\u4f18\u5316\u6846\u67b6\u3001\u6807\u7b7e\u6821\u6b63\u3001\u51e0\u4f55\u4e2d\u503c\u805a\u5408"}}
{"id": "2506.02350", "pdf": "https://arxiv.org/pdf/2506.02350", "abs": "https://arxiv.org/abs/2506.02350", "authors": ["Herun Wan", "Jiaying Wu", "Minnan Luo", "Zhi Zeng", "Zhixiong Su"], "title": "Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection", "categories": ["cs.CL"], "comment": null, "summary": "Misinformation detection models often rely on superficial cues (i.e.,\n\\emph{shortcuts}) that correlate with misinformation in training data but fail\nto generalize to the diverse and evolving nature of real-world misinformation.\nThis issue is exacerbated by large language models (LLMs), which can easily\ngenerate convincing misinformation through simple prompts. We introduce\nTruthOverTricks, a unified evaluation paradigm for measuring shortcut learning\nin misinformation detection. TruthOverTricks categorizes shortcut behaviors\ninto intrinsic shortcut induction and extrinsic shortcut injection, and\nevaluates seven representative detectors across 14 popular benchmarks, along\nwith two new factual misinformation datasets, NQ-Misinfo and Streaming-Misinfo.\nEmpirical results reveal that existing detectors suffer severe performance\ndegradation when exposed to both naturally occurring and adversarially crafted\nshortcuts. To address this, we propose SMF, an LLM-augmented data augmentation\nframework that mitigates shortcut reliance through paraphrasing, factual\nsummarization, and sentiment normalization. SMF consistently enhances\nrobustness across 16 benchmarks, encouraging models to rely on deeper semantic\nunderstanding rather than shortcut cues. To promote the development of\nmisinformation detectors, we have published the resources publicly at\nhttps://github.com/whr000001/TruthOverTricks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86TruthOverTricks\u8bc4\u4f30\u8303\u5f0f\uff0c\u63ed\u793a\u73b0\u6709\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u56e0\u4f9d\u8d56\u8868\u9762\u7ebf\u7d22\uff08\u6377\u5f84\uff09\u800c\u4e0b\u964d\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eLLM\u7684\u6570\u636e\u589e\u5f3a\u6846\u67b6SMF\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u6a21\u578b\u5bb9\u6613\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u8868\u9762\u7ebf\u7d22\uff08\u6377\u5f84\uff09\uff0c\u4f46\u5728\u9762\u5bf9\u591a\u6837\u5316\u548c\u52a8\u6001\u53d8\u5316\u7684\u865a\u5047\u4fe1\u606f\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u8f7b\u677e\u751f\u6210\u5177\u6709\u8ff7\u60d1\u6027\u7684\u865a\u5047\u4fe1\u606f\u3002", "method": "\u8bba\u6587\u63d0\u51faTruthOverTricks\u8bc4\u4f30\u8303\u5f0f\uff0c\u5206\u7c7b\u6377\u5f84\u884c\u4e3a\uff08\u5185\u5728\u8bf1\u5bfc\u4e0e\u5916\u5728\u6ce8\u5165\uff09\uff0c\u5e76\u572814\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f307\u79cd\u4ee3\u8868\u6027\u68c0\u6d4b\u5668\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86SMF\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\uff08\u5982\u91ca\u4e49\u3001\u4e8b\u5b9e\u6458\u8981\u548c\u60c5\u611f\u5f52\u4e00\u5316\uff09\u51cf\u5c11\u5bf9\u6377\u5f84\u7684\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u9762\u5bf9\u81ea\u7136\u6216\u5bf9\u6297\u6027\u6377\u5f84\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800cSMF\u6846\u67b6\u80fd\u591f\u6301\u7eed\u63d0\u5347\u6a21\u578b\u572816\u4e2a\u57fa\u51c6\u4e0a\u7684\u9c81\u68d2\u6027\uff0c\u4fc3\u4f7f\u5176\u4f9d\u8d56\u6df1\u5c42\u8bed\u4e49\u7406\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u5de5\u5177\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u6377\u5f84\u5b66\u4e60\u7684\u5c40\u9650\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002", "keywords": "\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u3001\u6377\u5f84\u5b66\u4e60\u3001\u6570\u636e\u589e\u5f3a\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u9c81\u68d2\u6027"}}
{"id": "2506.02696", "pdf": "https://arxiv.org/pdf/2506.02696", "abs": "https://arxiv.org/abs/2506.02696", "authors": ["Jinyuan Luo", "Zhen Fang", "Yixuan Li", "Seongheon Park", "Ling Chen"], "title": "Shaking to Reveal: Perturbation-Based Detection of LLM Hallucinations", "categories": ["cs.AI"], "comment": null, "summary": "Hallucination remains a key obstacle to the reliable deployment of large\nlanguage models (LLMs) in real-world question answering tasks. A widely adopted\nstrategy to detect hallucination, known as self-assessment, relies on the\nmodel's own output confidence to estimate the factual accuracy of its answers.\nHowever, this strategy assumes that the model's output distribution closely\nreflects the true data distribution, which may not always hold in practice. As\nbias accumulates through the model's layers, the final output can diverge from\nthe underlying reasoning process, making output-level confidence an unreliable\nsignal for hallucination detection. In this work, we propose Sample-Specific\nPrompting (SSP), a new framework that improves self-assessment by analyzing\nperturbation sensitivity at intermediate representations. These\nrepresentations, being less influenced by model bias, offer a more faithful\nview of the model's latent reasoning process. Specifically, SSP dynamically\ngenerates noise prompts for each input and employs a lightweight encoder to\namplify the changes in representations caused by the perturbation. A\ncontrastive distance metric is then used to quantify these differences and\nseparate truthful from hallucinated responses. By leveraging the dynamic\nbehavior of intermediate representations under perturbation, SSP enables more\nreliable self-assessment. Extensive experiments demonstrate that SSP\nsignificantly outperforms prior methods across a range of hallucination\ndetection benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Sample-Specific Prompting (SSP)\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u4e2d\u95f4\u8868\u5f81\u7684\u6270\u52a8\u654f\u611f\u6027\uff0c\u6539\u8fdb\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u8bc4\u4f30\u80fd\u529b\uff0c\u4ece\u800c\u66f4\u53ef\u9760\u5730\u68c0\u6d4b\u5e7b\u89c9\u54cd\u5e94\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u95ee\u7b54\u4efb\u52a1\u4e2d\u5e7b\u89c9\u68c0\u6d4b\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u81ea\u6211\u8bc4\u4f30\u65b9\u6cd5\u56e0\u6a21\u578b\u504f\u89c1\u5bfc\u81f4\u7684\u8f93\u51fa\u7f6e\u4fe1\u5ea6\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSSP\u6846\u67b6\uff0c\u52a8\u6001\u751f\u6210\u566a\u58f0\u63d0\u793a\u5e76\u5206\u6790\u4e2d\u95f4\u8868\u5f81\u7684\u6270\u52a8\u654f\u611f\u6027\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\u548c\u5bf9\u6bd4\u8ddd\u79bb\u5ea6\u91cf\u91cf\u5316\u5dee\u5f02\u4ee5\u533a\u5206\u771f\u5b9e\u548c\u5e7b\u89c9\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSSP\u5728\u591a\u4e2a\u5e7b\u89c9\u68c0\u6d4b\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SSP\u901a\u8fc7\u5229\u7528\u4e2d\u95f4\u8868\u5f81\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u6211\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u5e7b\u89c9\u68c0\u6d4b,\u81ea\u6211\u8bc4\u4f30,\u6270\u52a8\u654f\u611f\u6027,SSP"}}
{"id": "2506.02081", "pdf": "https://arxiv.org/pdf/2506.02081", "abs": "https://arxiv.org/abs/2506.02081", "authors": ["Chihiro Maru", "Shoetsu Sato"], "title": "RATFM: Retrieval-augmented Time Series Foundation Model for Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Inspired by the success of large language models (LLMs) in natural language\nprocessing, recent research has explored the building of time series foundation\nmodels and applied them to tasks such as forecasting, classification, and\nanomaly detection. However, their performances vary between different domains\nand tasks. In LLM-based approaches, test-time adaptation using example-based\nprompting has become common, owing to the high cost of retraining. In the\ncontext of anomaly detection, which is the focus of this study, providing\nnormal examples from the target domain can also be effective. However, time\nseries foundation models do not naturally acquire the ability to interpret or\nutilize examples or instructions, because the nature of time series data used\nduring training does not encourage such capabilities. To address this\nlimitation, we propose a retrieval augmented time series foundation model\n(RATFM), which enables pretrained time series foundation models to incorporate\nexamples of test-time adaptation. We show that RATFM achieves a performance\ncomparable to that of in-domain fine-tuning while avoiding domain-dependent\nfine-tuning. Experiments on the UCR Anomaly Archive, a multi-domain dataset\nincluding nine domains, confirms the effectiveness of the proposed approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b(RATFM)\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u9002\u5e94\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u907f\u514d\u9886\u57df\u4f9d\u8d56\u7684\u5fae\u8c03\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u4e14\u7f3a\u4e4f\u89e3\u91ca\u6216\u5229\u7528\u793a\u4f8b\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u63d0\u5347\u5176\u9002\u5e94\u6027\u3002", "method": "\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u6280\u672f\uff0c\u4f7f\u9884\u8bad\u7ec3\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u80fd\u591f\u7eb3\u5165\u6d4b\u8bd5\u65f6\u7684\u793a\u4f8b\uff0c\u4ece\u800c\u63d0\u9ad8\u5176\u8868\u73b0\u3002", "result": "\u5728UCR Anomaly Archive\u591a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRATFM\u7684\u6027\u80fd\u63a5\u8fd1\u9886\u57df\u5185\u5fae\u8c03\uff0c\u4f46\u65e0\u9700\u9886\u57df\u4f9d\u8d56\u7684\u5fae\u8c03\u3002", "conclusion": "RATFM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u9886\u57df\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b, \u6d4b\u8bd5\u65f6\u9002\u5e94, \u68c0\u7d22\u589e\u5f3a, \u5f02\u5e38\u68c0\u6d4b"}}
{"id": "2506.02351", "pdf": "https://arxiv.org/pdf/2506.02351", "abs": "https://arxiv.org/abs/2506.02351", "authors": ["Jeonghun Kang", "Soonmok Kwon", "Joonseok Lee", "Byung-Hak Kim"], "title": "DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "To appear in the First REALM (Research on Agent Language Models)\n  workshop at ACL 2025", "summary": "Traditional approaches -- such as Win Probability Added (WPA)-based ranking\nor computer vision-driven event detection -- can identify scoring plays but\noften miss strategic depth, momentum shifts, and storyline progression. Manual\ncuration remains the gold standard but is resource-intensive and not scalable.\nWe introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight\nsummarization that integrates structured sports analytics with natural language\nreasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and\nLeverage Index -- to quantify play importance, while an LLM module enhances\nselection based on contextual narrative value. This hybrid approach ensures\nboth quantitative rigor and qualitative richness, surpassing the limitations of\npurely statistical or vision-based systems. Evaluated on five diverse Korean\nBaseball Organization League games, DIAMOND improves F1-score from 42.9%\n(WPA-only) to 84.8%, outperforming both commercial and statistical baselines.\nThough limited in scale, our results highlight the potential of modular,\ninterpretable agent-based frameworks for event-level summarization in sports\nand beyond.", "AI": {"tldr": "DIAMOND\u662f\u4e00\u6b3e\u7ed3\u5408\u7ed3\u6784\u5316\u4f53\u80b2\u5206\u6790\u548c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u7684LLM\u9a71\u52a8\u4ee3\u7406\uff0c\u7528\u4e8e\u68d2\u7403\u9ad8\u5149\u65f6\u523b\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u6458\u8981\uff0c\u8d85\u8d8a\u4f20\u7edf\u57fa\u4e8e\u7edf\u8ba1\u6216\u89c6\u89c9\u7684\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5982WPA\u6392\u540d\u6216\u8ba1\u7b97\u673a\u89c6\u89c9\u4e8b\u4ef6\u68c0\u6d4b\u7f3a\u4e4f\u6218\u7565\u6df1\u5ea6\u548c\u6545\u4e8b\u6027\uff0c\u4eba\u5de5\u6807\u8bb0\u867d\u51c6\u786e\u4f46\u4e0d\u9ad8\u6548\u3002DIAMOND\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "DIAMOND\u7ed3\u5408sabermetric\u7279\u5f81\uff08\u5982Win Expectancy\uff09\u548cLLM\u6a21\u5757\uff0c\u5b9a\u91cf\u8bc4\u4f30\u6bd4\u8d5b\u91cd\u8981\u6027\u5e76\u589e\u5f3a\u4e0a\u4e0b\u6587\u53d9\u4e8b\u4ef7\u503c\u3002", "result": "\u5728\u4e94\u573a\u97e9\u56fd\u68d2\u7403\u8054\u76df\u6bd4\u8d5b\u4e2d\uff0cDIAMOND\u7684F1\u5206\u6570\u4ece42.9%\u63d0\u5347\u81f384.8%\uff0c\u4f18\u4e8e\u5546\u4e1a\u548c\u7edf\u8ba1\u57fa\u7ebf\u3002", "conclusion": "DIAMOND\u5c55\u793a\u4e86\u6a21\u5757\u5316\u3001\u53ef\u89e3\u91ca\u7684\u57fa\u4e8e\u4ee3\u7406\u7684\u6846\u67b6\u5728\u4f53\u80b2\u53ca\u5176\u4ed6\u9886\u57df\u4e8b\u4ef6\u6458\u8981\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "DIAMOND, \u68d2\u7403\u6458\u8981, LLM\u9a71\u52a8\u4ee3\u7406, sabermetric\u7279\u5f81, \u4e0a\u4e0b\u6587\u611f\u77e5"}}
{"id": "2506.02713", "pdf": "https://arxiv.org/pdf/2506.02713", "abs": "https://arxiv.org/abs/2506.02713", "authors": ["Xiaochong Lan", "Jie Feng", "Yizhou Sun", "Chen Gao", "Jiahuan Lei", "Xinlei Shi", "Hengliang Luo", "Yong Li"], "title": "Open-Set Living Need Prediction with Large Language Models", "categories": ["cs.AI"], "comment": "ACL 2025 Findings", "summary": "Living needs are the needs people generate in their daily lives for survival\nand well-being. On life service platforms like Meituan, user purchases are\ndriven by living needs, making accurate living need predictions crucial for\npersonalized service recommendations. Traditional approaches treat this\nprediction as a closed-set classification problem, severely limiting their\nability to capture the diversity and complexity of living needs. In this work,\nwe redefine living need prediction as an open-set classification problem and\npropose PIGEON, a novel system leveraging large language models (LLMs) for\nunrestricted need prediction. PIGEON first employs a behavior-aware record\nretriever to help LLMs understand user preferences, then incorporates Maslow's\nhierarchy of needs to align predictions with human living needs. For evaluation\nand application, we design a recall module based on a fine-tuned text embedding\nmodel that links flexible need descriptions to appropriate life services.\nExtensive experiments on real-world datasets demonstrate that PIGEON\nsignificantly outperforms closed-set approaches on need-based life service\nrecall by an average of 19.37%. Human evaluation validates the reasonableness\nand specificity of our predictions. Additionally, we employ instruction tuning\nto enable smaller LLMs to achieve competitive performance, supporting practical\ndeployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPIGEON\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f00\u653e\u5f0f\u5206\u7c7b\u548cLLM\u9884\u6d4b\u751f\u6d3b\u9700\u6c42\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u95ed\u96c6\u65b9\u6cd5\uff0c\u63d0\u534719.37%\u3002", "motivation": "\u4f20\u7edf\u95ed\u96c6\u5206\u7c7b\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u751f\u6d3b\u9700\u6c42\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\uff0c\u9700\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5f00\u653e\u5f0f\u5206\u7c7b\u95ee\u9898\u3002", "method": "PIGEON\u7ed3\u5408\u884c\u4e3a\u611f\u77e5\u8bb0\u5f55\u68c0\u7d22\u5668\u548c\u9a6c\u65af\u6d1b\u9700\u6c42\u5c42\u6b21\u7406\u8bba\uff0c\u5229\u7528LLM\u8fdb\u884c\u7075\u6d3b\u9700\u6c42\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u6587\u672c\u5d4c\u5165\u6a21\u578b\u53ec\u56de\u670d\u52a1\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cPIGEON\u6bd4\u95ed\u96c6\u65b9\u6cd5\u5e73\u5747\u63d0\u534719.37%\uff0c\u4eba\u7c7b\u8bc4\u4f30\u9a8c\u8bc1\u5176\u5408\u7406\u6027\u548c\u7279\u5f02\u6027\u3002", "conclusion": "PIGEON\u7cfb\u7edf\u5728\u751f\u6d3b\u9700\u6c42\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u4e14\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u4f7f\u5c0f\u578bLLM\u5177\u5907\u7ade\u4e89\u529b\uff0c\u652f\u6301\u5b9e\u9645\u90e8\u7f72\u3002", "keywords": "\u751f\u6d3b\u9700\u6c42\u9884\u6d4b, \u5f00\u653e\u5f0f\u5206\u7c7b, LLM, \u4e2a\u6027\u5316\u670d\u52a1, \u9a6c\u65af\u6d1b\u9700\u6c42"}}
{"id": "2506.02084", "pdf": "https://arxiv.org/pdf/2506.02084", "abs": "https://arxiv.org/abs/2506.02084", "authors": ["Nikolaos Gkorgkolis", "Nikolaos Kougioulis", "MingXue Wang", "Bora Caglayan", "Andrea Tonon", "Dario Simionato", "Ioannis Tsamardinos"], "title": "Temporal Causal-based Simulation for Realistic Time-series Generation", "categories": ["cs.LG", "stat.ML"], "comment": "22 pages, 3 figures", "summary": "Causal Discovery plays a pivotal role in revealing relationships among\nobserved variables, particularly in the temporal setup. While the majority of\nCD methods rely on synthetic data for evaluation, and recently for training,\nthese fall short in accurately mirroring real-world scenarios; an effect even\nmore evident in temporal data. Generation techniques depending on simplified\nassumptions on causal structure, effects and time, limit the quality and\ndiversity of the simulated data. In this work, we introduce Temporal\nCausal-based Simulation (TCS), a robust framework for generating realistic\ntime-series data and their associated temporal causal graphs. The approach is\nstructured in three phases: estimating the true lagged causal structure of the\ndata, approximating the functional dependencies between variables and learning\nthe noise distribution of the corresponding causal model, each part of which\ncan be explicitly tailored based on data assumptions and characteristics.\nThrough an extensive evaluation process, we highlight that single detection\nmethods for generated data discrimination prove inadequate, accentuating it as\na multifaceted challenge. For this, we detail a Min-max optimization phase that\ndraws on AutoML techniques. Our contributions include a flexible,\nmodel-agnostic pipeline for generating realistic temporal causal data, a\nthorough evaluation setup which enhances the validity of the generated datasets\nand insights into the challenges posed by realistic data generation. Through\nexperiments involving not only real but also semi-synthetic and purely\nsynthetic datasets, we demonstrate that while sampling realistic causal data\nremains a complex task, our method enriches the domain of generating sensible\ncausal-based temporal data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TCS\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u771f\u5b9e\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u53ca\u5176\u5173\u8054\u7684\u56e0\u679c\u56fe\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u65b9\u6cd5\uff08\u56e0\u679c\u7ed3\u6784\u4f30\u8ba1\u3001\u529f\u80fd\u4f9d\u8d56\u8fd1\u4f3c\u548c\u566a\u58f0\u5206\u5e03\u5b66\u4e60\uff09\u63d0\u5347\u751f\u6210\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u4f9d\u8d56\u5408\u6210\u6570\u636e\uff0c\u4f46\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u73b0\u5b9e\u573a\u666f\uff0c\u5c24\u5176\u5728\u65f6\u5e8f\u6570\u636e\u4e2d\u66f4\u660e\u663e\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u66f4\u771f\u5b9e\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4f30\u8ba1\u6570\u636e\u7684\u6ede\u540e\u56e0\u679c\u7ed3\u6784\uff0c2) \u8fd1\u4f3c\u53d8\u91cf\u95f4\u7684\u529f\u80fd\u4f9d\u8d56\uff0c3) \u5b66\u4e60\u56e0\u679c\u6a21\u578b\u7684\u566a\u58f0\u5206\u5e03\u3002\u5e76\u901a\u8fc7Min-max\u4f18\u5316\u63d0\u5347\u6570\u636e\u771f\u5b9e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTCS\u80fd\u591f\u751f\u6210\u66f4\u771f\u5b9e\u7684\u65f6\u5e8f\u56e0\u679c\u6570\u636e\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u751f\u6210\u6570\u636e\u7684\u6709\u6548\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u771f\u5b9e\u6570\u636e\u751f\u6210\u7684\u6311\u6218\u3002", "conclusion": "\u867d\u7136\u751f\u6210\u771f\u5b9e\u56e0\u679c\u6570\u636e\u4ecd\u5177\u6311\u6218\u6027\uff0cTCS\u4e3a\u65f6\u5e8f\u56e0\u679c\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u4e14\u6a21\u578b\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u56e0\u679c\u53d1\u73b0\uff0c\u65f6\u95f4\u5e8f\u5217\uff0c\u6570\u636e\u751f\u6210\uff0cTCS\uff0cMin-max\u4f18\u5316"}}
{"id": "2506.02372", "pdf": "https://arxiv.org/pdf/2506.02372", "abs": "https://arxiv.org/abs/2506.02372", "authors": ["Hisami Suzuki", "Satoru Katsumata", "Takashi Kodama", "Tetsuro Takahashi", "Kouta Nakayama", "Satoshi Sekine"], "title": "AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output", "categories": ["cs.CL"], "comment": null, "summary": "In this paper we present AnswerCarefully, a dataset for promoting the safety\nand appropriateness of Japanese LLM outputs. The dataset consists of 1,800\npairs of questions and reference answers, where the questions require special\nattention in answering. It covers a wide range of risk categories established\nin prior English-language datasets, but the data samples are original in that\nthey are manually created to reflect the socio-cultural context of LLM usage in\nJapan. We show that using this dataset for instruction to fine-tune a Japanese\nLLM led to improved output safety without compromising the utility of general\nresponses. We also report the results of a safety evaluation of 12 Japanese\nLLMs using this dataset as a benchmark. Finally, we describe the latest update\non the dataset which provides English translations and annotations of the\nquestions, aimed at facilitating the derivation of similar datasets in\ndifferent languages and regions.", "AI": {"tldr": "AnswerCarefully\u662f\u4e00\u4e2a\u7528\u4e8e\u63d0\u5347\u65e5\u672cLLM\u8f93\u51fa\u5b89\u5168\u6027\u548c\u9002\u5f53\u6027\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b1800\u4e2a\u9700\u8981\u7279\u522b\u6ce8\u610f\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u8986\u76d6\u591a\u79cd\u98ce\u9669\u7c7b\u522b\uff0c\u5e76\u53cd\u6620\u65e5\u672c\u7684\u793e\u4f1a\u6587\u5316\u80cc\u666f\u3002\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u5fae\u8c03LLM\u53ef\u63d0\u9ad8\u5b89\u5168\u6027\u800c\u4e0d\u5f71\u54cd\u901a\u7528\u56de\u7b54\u6548\u679c\uff0c\u540c\u65f6\u8bc4\u4f30\u4e8612\u4e2a\u65e5\u672cLLM\u7684\u5b89\u5168\u6027\u3002\u6570\u636e\u96c6\u6700\u8fd1\u66f4\u65b0\u4e86\u82f1\u6587\u7ffb\u8bd1\u548c\u6807\u6ce8\uff0c\u65b9\u4fbf\u8de8\u8bed\u8a00\u548c\u5730\u533a\u4f7f\u7528\u3002", "motivation": "\u4e3a\u65e5\u672cLLM\u8f93\u51fa\u7684\u5b89\u5168\u6027\u548c\u9002\u5f53\u6027\u63d0\u4f9b\u4e13\u95e8\u7684\u8bc4\u4f30\u548c\u4f18\u5316\u5de5\u5177\uff0c\u8003\u8651\u672c\u571f\u793e\u4f1a\u6587\u5316\u80cc\u666f\u3002", "method": "\u6784\u5efa\u5305\u542b1800\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\u7684AnswerCarefully\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5fae\u8c03LLM\u5e76\u8fdb\u884c\u5b89\u5168\u6027\u8bc4\u4f30\u3002", "result": "\u5fae\u8c03\u7684\u65e5\u672cLLM\u5b89\u5168\u6027\u63d0\u5347\u4e14\u901a\u7528\u56de\u7b54\u6548\u679c\u4e0d\u53d7\u5f71\u54cd\uff0c\u540c\u65f612\u4e2a\u65e5\u672cLLM\u7684\u5b89\u5168\u6027\u5f97\u5230\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "AnswerCarefully\u6570\u636e\u96c6\u6709\u6548\u63d0\u5347LLM\u5b89\u5168\u6027\u5e76\u652f\u6301\u8de8\u8bed\u8a00\u63a8\u5e7f\u3002", "keywords": "\u65e5\u672cLLM, \u5b89\u5168\u6027, \u6570\u636e\u96c6, \u5fae\u8c03, \u793e\u4f1a\u6587\u5316\u80cc\u666f"}}
{"id": "2506.02720", "pdf": "https://arxiv.org/pdf/2506.02720", "abs": "https://arxiv.org/abs/2506.02720", "authors": ["Xiaochong Lan", "Jie Feng", "Jiahuan Lei", "Xinlei Shi", "Yong Li"], "title": "Benchmarking and Advancing Large Language Models for Local Life Services", "categories": ["cs.AI", "cs.CL"], "comment": "KDD 2025", "summary": "Large language models (LLMs) have exhibited remarkable capabilities and\nachieved significant breakthroughs across various domains, leading to their\nwidespread adoption in recent years. Building on this progress, we investigate\ntheir potential in the realm of local life services. In this study, we\nestablish a comprehensive benchmark and systematically evaluate the performance\nof diverse LLMs across a wide range of tasks relevant to local life services.\nTo further enhance their effectiveness, we explore two key approaches: model\nfine-tuning and agent-based workflows. Our findings reveal that even a\nrelatively compact 7B model can attain performance levels comparable to a much\nlarger 72B model, effectively balancing inference cost and model capability.\nThis optimization greatly enhances the feasibility and efficiency of deploying\nLLMs in real-world online services, making them more practical and accessible\nfor local life applications.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u672c\u5730\u751f\u6d3b\u670d\u52a1\u9886\u57df\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u5efa\u7acb\u57fa\u51c6\u6d4b\u8bd5\u548c\u63a2\u7d22\u5fae\u8c03\u4e0e\u5de5\u4f5c\u6d41\u4f18\u5316\uff0c\u53d1\u73b07B\u6a21\u578b\u53ef\u5b9e\u73b0\u4e0e72B\u6a21\u578b\u76f8\u5f53\u7684\u6548\u80fd\u3002", "motivation": "\u63a2\u7d22LLM\u5728\u672c\u5730\u751f\u6d3b\u670d\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u89e3\u51b3\u6a21\u578b\u90e8\u7f72\u6210\u672c\u4e0e\u80fd\u529b\u5e73\u8861\u7684\u95ee\u9898\u3002", "method": "\u5efa\u7acb\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30LLM\u6027\u80fd\uff0c\u91c7\u7528\u6a21\u578b\u5fae\u8c03\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u5de5\u4f5c\u6d41\u4f18\u5316\u65b9\u6cd5\u3002", "result": "7B\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u63a5\u8fd172B\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\uff0c\u63d0\u5347\u4e86\u90e8\u7f72\u53ef\u884c\u6027\u3002", "conclusion": "\u4f18\u5316\u540e\u7684LLM\u5728\u672c\u5730\u751f\u6d3b\u670d\u52a1\u4e2d\u5177\u5907\u9ad8\u6548\u548c\u5b9e\u7528\u6027\uff0c\u6269\u5c55\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u672c\u5730\u751f\u6d3b\u670d\u52a1\u3001\u5fae\u8c03\u3001\u5de5\u4f5c\u6d41\u4f18\u5316\u30017B\u6a21\u578b"}}
{"id": "2506.02089", "pdf": "https://arxiv.org/pdf/2506.02089", "abs": "https://arxiv.org/abs/2506.02089", "authors": ["Zeng Wang", "Minghao Shao", "Rupesh Karn", "Jitendra Bhandari", "Likhitha Mankali", "Ramesh Karri", "Ozgur Sinanoglu", "Muhammad Shafique", "Johann Knechtel"], "title": "SALAD: Systematic Assessment of Machine Unlearing on LLM-Aided Hardware Design", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs) offer transformative capabilities for hardware\ndesign automation, particularly in Verilog code generation. However, they also\npose significant data security challenges, including Verilog evaluation data\ncontamination, intellectual property (IP) design leakage, and the risk of\nmalicious Verilog generation. We introduce SALAD, a comprehensive assessment\nthat leverages machine unlearning to mitigate these threats. Our approach\nenables the selective removal of contaminated benchmarks, sensitive IP and\ndesign artifacts, or malicious code patterns from pre-trained LLMs, all without\nrequiring full retraining. Through detailed case studies, we demonstrate how\nmachine unlearning techniques effectively reduce data security risks in\nLLM-aided hardware design.", "AI": {"tldr": "\u63d0\u51faSALAD\u65b9\u6cd5\uff0c\u5229\u7528\u673a\u5668\u53bb\u5b66\u4e60\u6280\u672f\u89e3\u51b3LLM\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u6570\u636e\u5b89\u5168\u95ee\u9898\uff0c\u5305\u62ec\u6570\u636e\u6c61\u67d3\u3001IP\u6cc4\u6f0f\u548c\u6076\u610f\u4ee3\u7801\u751f\u6210\u3002", "motivation": "LLM\u5728\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u4e2d\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4e5f\u5e26\u6765\u6570\u636e\u5b89\u5168\u98ce\u9669\uff0c\u5982Verilog\u8bc4\u4f30\u6570\u636e\u6c61\u67d3\u3001IP\u6cc4\u6f0f\u548c\u6076\u610f\u4ee3\u7801\u751f\u6210\uff0c\u4e9f\u9700\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165SALAD\uff0c\u901a\u8fc7\u673a\u5668\u53bb\u5b66\u4e60\u6280\u672f\uff0c\u9009\u62e9\u6027\u79fb\u9664\u9884\u8bad\u7ec3LLM\u4e2d\u7684\u6c61\u67d3\u6570\u636e\u3001\u654f\u611fIP\u6216\u6076\u610f\u4ee3\u7801\uff0c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u673a\u5668\u53bb\u5b66\u4e60\u6280\u672f\u80fd\u6709\u6548\u964d\u4f4eLLM\u8f85\u52a9\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u6570\u636e\u5b89\u5168\u98ce\u9669\u3002", "conclusion": "SALAD\u4e3aLLM\u5728\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u5b89\u5168\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "LLM, \u786c\u4ef6\u8bbe\u8ba1, \u673a\u5668\u53bb\u5b66\u4e60, Verilog, \u6570\u636e\u5b89\u5168"}}
{"id": "2506.02378", "pdf": "https://arxiv.org/pdf/2506.02378", "abs": "https://arxiv.org/abs/2506.02378", "authors": ["Ukyo Honda", "Tatsushi Oka"], "title": "Exploring Explanations Improves the Robustness of In-Context Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "In-context learning (ICL) has emerged as a successful paradigm for leveraging\nlarge language models (LLMs). However, it often struggles to generalize beyond\nthe distribution of the provided demonstrations. A recent advancement in\nenhancing robustness is ICL with explanations (X-ICL), which improves\nprediction reliability by guiding LLMs to understand and articulate the\nreasoning behind correct labels. Building on this approach, we introduce an\nadvanced framework that extends X-ICL by systematically exploring explanations\nfor all possible labels (X$^2$-ICL), thereby enabling more comprehensive and\nrobust decision-making. Experimental results on multiple natural language\nunderstanding datasets validate the effectiveness of X$^2$-ICL, demonstrating\nsignificantly improved robustness to out-of-distribution data compared to the\nexisting ICL approaches.", "AI": {"tldr": "X^2-ICL\u6269\u5c55\u4e86\u5e26\u6709\u89e3\u91ca\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08X-ICL\uff09\uff0c\u901a\u8fc7\u7cfb\u7edf\u63a2\u7d22\u6240\u6709\u53ef\u80fd\u6807\u7b7e\u7684\u89e3\u91ca\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u5206\u5e03\u5916\u6570\u636e\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfICL\u65b9\u6cd5\u5728\u8d85\u51fa\u6f14\u793a\u5206\u5e03\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faX^2-ICL\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u63a2\u7d22\u6240\u6709\u53ef\u80fd\u6807\u7b7e\u7684\u89e3\u91ca\u6765\u589e\u5f3a\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u4eba\u5de5\u7406\u89e3\u6570\u636e\u96c6\u4e0a\uff0cX^2-ICL\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u5206\u5e03\u5916\u6570\u636e\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "X^2-ICL\u662f\u4e00\u79cd\u66f4\u5168\u9762\u4e14\u9c81\u68d2\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u3002", "keywords": "in-context learning, LLMs, robustness, explanations, out-of-distribution"}}
{"id": "2506.02739", "pdf": "https://arxiv.org/pdf/2506.02739", "abs": "https://arxiv.org/abs/2506.02739", "authors": ["Pengcheng Zhou", "Yinglun Feng", "Halimulati Julaiti", "Zhongliang Yang"], "title": "Why do AI agents communicate in human language?", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have become foundational to modern AI agent\nsystems, enabling autonomous agents to reason and plan. In most existing\nsystems, inter-agent communication relies primarily on natural language. While\nthis design supports interpretability and human oversight, we argue that it\nintroduces fundamental limitations in agent-to-agent coordination. The semantic\nspace of natural language is structurally misaligned with the high-dimensional\nvector spaces in which LLMs operate, resulting in information loss and\nbehavioral drift. Beyond surface-level inefficiencies, we highlight a deeper\narchitectural limitation: current LLMs were not trained with the objective of\nsupporting agentic behavior. As such, they lack mechanisms for modeling role\ncontinuity, task boundaries, and multi-agent dependencies. The standard\nnext-token prediction paradigm fails to support the structural alignment\nrequired for robust, scalable agent coordination. Based on this, we argue that\ntwo core questions deserve careful examination: first, given that AI agents\nfundamentally operate in high-dimensional vector spaces, should they rely on a\nlanguage system originally designed for human cognition as their communication\nmedium? Second, should we consider developing a new model construction paradigm\nthat builds models from the ground up to natively support structured\ncommunication, shared intentionality, and task alignment in multi-role,\nmulti-agent environments? This paper calls for a reconsideration not only of\nhow agents should communicate, but also of what it fundamentally means to train\na model that natively supports multi-agent coordination and communication.", "AI": {"tldr": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u667a\u80fd\u4f53\u534f\u8c03\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u662f\u5426\u9700\u8981\u8bbe\u8ba1\u65b0\u7684\u6a21\u578b\u8303\u5f0f\u4ee5\u652f\u6301\u7ed3\u6784\u5316\u901a\u4fe1\u548c\u4efb\u52a1\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u667a\u80fd\u4f53\u901a\u4fe1\u65b9\u5f0f\u4e0eLLMs\u7684\u9ad8\u7ef4\u5411\u91cf\u7a7a\u95f4\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u548c\u884c\u4e3a\u6f02\u79fb\uff0c\u9650\u5236\u4e86\u591a\u667a\u80fd\u4f53\u534f\u8c03\u80fd\u529b\u3002", "method": "\u5206\u6790\u5f53\u524dLLMs\u7684\u8bbe\u8ba1\u7f3a\u9677\uff0c\u63d0\u51fa\u662f\u5426\u5e94\u5f00\u53d1\u539f\u751f\u652f\u6301\u7ed3\u6784\u5316\u901a\u4fe1\u548c\u591a\u667a\u80fd\u4f53\u73af\u5883\u7684\u6a21\u578b\u8303\u5f0f\u3002", "result": "\u63ed\u793a\u4e86\u73b0\u6709LLMs\u5728\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63a8\u52a8\u4e86\u5bf9\u65b0\u578b\u6a21\u578b\u8303\u5f0f\u7684\u63a2\u8ba8\u3002", "conclusion": "\u9700\u8981\u91cd\u65b0\u8003\u8651\u667a\u80fd\u4f53\u7684\u901a\u4fe1\u65b9\u5f0f\uff0c\u5e76\u5f00\u53d1\u539f\u751f\u652f\u6301\u591a\u667a\u80fd\u4f53\u534f\u8c03\u7684\u6a21\u578b\u67b6\u6784\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u667a\u80fd\u4f53\u901a\u4fe1,\u591a\u667a\u80fd\u4f53\u534f\u8c03,\u7ed3\u6784\u5316\u901a\u4fe1,\u4efb\u52a1\u5bf9\u9f50"}}
{"id": "2506.02092", "pdf": "https://arxiv.org/pdf/2506.02092", "abs": "https://arxiv.org/abs/2506.02092", "authors": ["Francesco De Santis", "Philippe Bich", "Gabriele Ciravegna", "Pietro Barbiero", "Danilo Giordano", "Tania Cerquitelli"], "title": "Towards Better Generalization and Interpretability in Unsupervised Concept-Based Models", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Paper accepted at ECML-PKDD 2025", "summary": "To increase the trustworthiness of deep neural networks, it is critical to\nimprove the understanding of how they make decisions. This paper introduces a\nnovel unsupervised concept-based model for image classification, named\nLearnable Concept-Based Model (LCBM) which models concepts as random variables\nwithin a Bernoulli latent space. Unlike traditional methods that either require\nextensive human supervision or suffer from limited scalability, our approach\nemploys a reduced number of concepts without sacrificing performance. We\ndemonstrate that LCBM surpasses existing unsupervised concept-based models in\ngeneralization capability and nearly matches the performance of black-box\nmodels. The proposed concept representation enhances information retention and\naligns more closely with human understanding. A user study demonstrates the\ndiscovered concepts are also more intuitive for humans to interpret. Finally,\ndespite the use of concept embeddings, we maintain model interpretability by\nmeans of a local linear combination of concepts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u57fa\u4e8e\u6982\u5ff5\u7684\u56fe\u50cf\u5206\u7c7b\u6a21\u578bLCBM\uff0c\u901a\u8fc7\u5c06\u6982\u5ff5\u5efa\u6a21\u4e3a\u4f2f\u52aa\u5229\u6f5c\u7a7a\u95f4\u4e2d\u7684\u968f\u673a\u53d8\u91cf\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u4fe1\u5ea6\uff0c\u9700\u8981\u66f4\u597d\u5730\u7406\u89e3\u5176\u51b3\u7b56\u8fc7\u7a0b\u3002\u5f53\u524d\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u5927\u91cf\u4eba\u5de5\u76d1\u7763\uff0c\u8981\u4e48\u53ef\u6269\u5c55\u6027\u6709\u9650\u3002", "method": "LCBM\u901a\u8fc7\u51cf\u5c11\u6240\u9700\u6982\u5ff5\u6570\u91cf\u800c\u4e0d\u727a\u7272\u6027\u80fd\uff0c\u91c7\u7528\u4f2f\u52aa\u5229\u6f5c\u7a7a\u95f4\u5efa\u6a21\u6982\u5ff5\uff0c\u5e76\u7ed3\u5408\u5c40\u90e8\u7ebf\u6027\u7ec4\u5408\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "result": "LCBM\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u6982\u5ff5\u6a21\u578b\uff0c\u6027\u80fd\u63a5\u8fd1\u9ed1\u76d2\u6a21\u578b\uff0c\u4e14\u6982\u5ff5\u66f4\u7b26\u5408\u4eba\u7c7b\u7406\u89e3\u3002\u7528\u6237\u7814\u7a76\u8868\u660e\u5176\u6982\u5ff5\u66f4\u76f4\u89c2\u3002", "conclusion": "LCBM\u901a\u8fc7\u65b0\u9896\u7684\u6982\u5ff5\u8868\u793a\u63d0\u5347\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "keywords": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u3001\u53ef\u89e3\u91ca\u6027\u3001\u65e0\u76d1\u7763\u5b66\u4e60\u3001\u6982\u5ff5\u5efa\u6a21\u3001\u56fe\u50cf\u5206\u7c7b"}}
{"id": "2506.02391", "pdf": "https://arxiv.org/pdf/2506.02391", "abs": "https://arxiv.org/abs/2506.02391", "authors": ["Chuanghao Ding", "Jiaping Wang", "Ziqing Yang", "Xiaoliang Wang", "Dahua Lin", "Cam-Tu Nguyen", "Fei Tan"], "title": "Consultant Decoding: Yet Another Synergistic Mechanism", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 findings", "summary": "The synergistic mechanism based on Speculative Decoding (SD) has garnered\nconsiderable attention as a simple yet effective approach for accelerating the\ninference of large language models (LLMs). Nonetheless, the high rejection\nrates require repeated LLMs calls to validate draft tokens, undermining the\noverall efficiency gain of SD. In this work, we revisit existing verification\nmechanisms and propose a novel synergetic mechanism Consultant Decoding (CD).\nUnlike SD, which relies on a metric derived from importance sampling for\nverification, CD verifies candidate drafts using token-level likelihoods\ncomputed solely by the LLM. CD achieves up to a 2.5-fold increase in inference\nspeed compared to the target model, while maintaining comparable generation\nquality (around 100% of the target model's performance). Interestingly, this is\nachieved by combining models whose parameter sizes differ by two orders of\nmagnitude. In addition, CD reduces the call frequency of the large target model\nto below 10%, particularly in more demanding tasks. CD's performance was even\nfound to surpass that of the large target model, which theoretically represents\nthe upper bound for speculative decoding.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u987e\u95ee\u89e3\u7801\uff08CD\uff09\u7684\u65b0\u578b\u89e3\u7801\u673a\u5236\uff0c\u901a\u8fc7\u4f18\u5316\u9a8c\u8bc1\u8fc7\u7a0b\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u57fa\u4e8e\u63a8\u6d4b\u89e3\u7801\uff08SD\uff09\u7684\u9ad8\u62d2\u7edd\u7387\u95ee\u9898\uff0c\u964d\u4f4e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8c03\u7528\u9891\u7387\u5e76\u63d0\u9ad8\u6574\u4f53\u6548\u7387\u3002", "method": "\u63d0\u51fa\u987e\u95ee\u89e3\u7801\uff08CD\uff09\uff0c\u5229\u7528LLM\u8ba1\u7b97\u7684\u6807\u8bb0\u7ea7\u4f3c\u7136\u6027\u9a8c\u8bc1\u5019\u9009\u8349\u6848\uff0c\u800c\u975e\u4f9d\u8d56\u91cd\u8981\u6027\u91c7\u6837\u7684\u5ea6\u91cf\u3002", "result": "CD\u5c06\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u4e862.5\u500d\uff0c\u751f\u6210\u8d28\u91cf\u63a5\u8fd1\u76ee\u6807\u6a21\u578b\u7684100%\uff0c\u4e14\u5927\u578b\u6a21\u578b\u7684\u8c03\u7528\u9891\u7387\u964d\u81f310%\u4ee5\u4e0b\u3002", "conclusion": "CD\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u8fd8\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u76ee\u6807\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u63a8\u6d4b\u89e3\u7801\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u65b9\u5411\u3002", "keywords": "\u63a8\u6d4b\u89e3\u7801, \u987e\u95ee\u89e3\u7801, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u63a8\u7406\u52a0\u901f"}}
{"id": "2506.02761", "pdf": "https://arxiv.org/pdf/2506.02761", "abs": "https://arxiv.org/abs/2506.02761", "authors": ["Renyang Liu", "Wenjie Feng", "Tianwei Zhang", "Wei Zhou", "Xueqi Cheng", "See-Kiong Ng"], "title": "Rethinking Machine Unlearning in Image Generation Models", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CV"], "comment": "Accepted by ACM CCS 2025", "summary": "With the surge and widespread application of image generation models, data\nprivacy and content safety have become major concerns and attracted great\nattention from users, service providers, and policymakers. Machine unlearning\n(MU) is recognized as a cost-effective and promising means to address these\nchallenges. Despite some advancements, image generation model unlearning (IGMU)\nstill faces remarkable gaps in practice, e.g., unclear task discrimination and\nunlearning guidelines, lack of an effective evaluation framework, and\nunreliable evaluation metrics. These can hinder the understanding of unlearning\nmechanisms and the design of practical unlearning algorithms. We perform\nexhaustive assessments over existing state-of-the-art unlearning algorithms and\nevaluation standards, and discover several critical flaws and challenges in\nIGMU tasks. Driven by these limitations, we make several core contributions, to\nfacilitate the comprehensive understanding, standardized categorization, and\nreliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel\nhierarchical task categorization framework. It provides detailed implementation\nguidance for IGMU, assisting in the design of unlearning algorithms and the\nconstruction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation\nframework. It includes reliable quantitative metrics across five critical\naspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can\nbe used for extensive evaluations of IGMU, training content detectors for\njudgment, and benchmarking the state-of-the-art unlearning algorithms. With\nEvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot\nhandle the unlearning well across different evaluation dimensions, especially\nfor preservation and robustness. Code and models are available at\nhttps://github.com/ryliu68/IGMU.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u56fe\u50cf\u751f\u6210\u6a21\u578b\u9057\u5fd8\u5b66\u4e60\uff08IGMU\uff09\u7684\u5b9e\u8df5\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4efb\u52a1\u5206\u7c7b\u6846\u67b6CatIGMU\u3001\u8bc4\u4f30\u6846\u67b6EvalIGMU\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6DataIGM\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u9690\u79c1\u548c\u5185\u5bb9\u5b89\u5168\u95ee\u9898\u65e5\u76ca\u91cd\u8981\uff0c\u800c\u73b0\u6709\u7684\u9057\u5fd8\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4efb\u52a1\u5b9a\u4e49\u4e0d\u6e05\u3001\u8bc4\u4f30\u6846\u67b6\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51faCatIGMU\u4efb\u52a1\u5206\u7c7b\u6846\u67b6\u3001EvalIGMU\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u6784\u5efaDataIGM\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7b97\u6cd5\u8bbe\u8ba1\u548c\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0\u73b0\u6709IGMU\u7b97\u6cd5\u5728\u591a\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u4fdd\u7559\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u3002", "conclusion": "\u901a\u8fc7\u65b0\u6846\u67b6\u548c\u6570\u636e\u96c6\uff0c\u63d0\u9ad8\u4e86IGMU\u7684\u6807\u51c6\u5316\u548c\u8bc4\u4f30\u53ef\u9760\u6027\u3002", "keywords": "\u56fe\u50cf\u751f\u6210\u6a21\u578b,\u9057\u5fd8\u5b66\u4e60,\u4efb\u52a1\u5206\u7c7b,\u8bc4\u4f30\u6846\u67b6,\u6570\u636e\u96c6"}}
{"id": "2506.02096", "pdf": "https://arxiv.org/pdf/2506.02096", "abs": "https://arxiv.org/abs/2506.02096", "authors": ["Zijian Wu", "Jinjie Ni", "Xiangyan Liu", "Zichen Liu", "Hang Yan", "Michael Qizhe Shieh"], "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have shown notable progress in scaling test-time\ncompute effectively. In this work, we investigate how synthesized RL data can\nfurther improve RLVR. To this end, we propose \\textbf{SynthRL}-a scalable and\nguaranteed pipeline for automatic data scaling in reasoning-oriented RL\ntraining. SynthRL comprises three key stages: (1) selecting seed questions with\nappropriate distribution, (2) augmenting them into more challenging variants\nwhile preserving the original answers, and (3) a guaranteed verification stage\nthat ensures near-perfect correctness and difficulty enhancement. Our empirical\nexperiments demonstrate SynthRL's scalability and effectiveness. When applied\nto the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,\nchallenging questions from approximately 8K seed samples. Models trained with\nour synthesized data achieve consistent gains across five out-of-domain visual\nmath reasoning benchmarks, with a significant improvement over baseline models\ntrained on seed data alone. Notably, detailed analysis reveals that the gains\nare more pronounced on the most challenging evaluation samples, highlighting\nSynthRL's effectiveness in eliciting deeper and more complex reasoning\npatterns.", "AI": {"tldr": "SynthRL \u662f\u4e00\u79cd\u901a\u8fc7\u81ea\u52a8\u5408\u6210\u6570\u636e\u6269\u5c55\u63a8\u7406\u5bfc\u5411\u5f3a\u5316\u5b66\u4e60\u7684\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u5408\u6210\u5f3a\u5316\u5b66\u4e60\u6570\u636e\u8fdb\u4e00\u6b65\u4f18\u5316 RLVR\uff08\u5e26\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u4ee5\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa SynthRL \u6d41\u7a0b\uff0c\u5305\u62ec\u9009\u62e9\u79cd\u5b50\u95ee\u9898\u3001\u751f\u6210\u66f4\u5177\u6311\u6218\u6027\u7684\u53d8\u4f53\u5e76\u4fdd\u7559\u539f\u59cb\u7b54\u6848\uff0c\u4ee5\u53ca\u901a\u8fc7\u9a8c\u8bc1\u9636\u6bb5\u786e\u4fdd\u6570\u636e\u7684\u6b63\u786e\u6027\u548c\u96be\u5ea6\u63d0\u5347\u3002", "result": "\u5728 MMK12 \u6570\u636e\u96c6\u4e0a\uff0cSynthRL \u4ece 8K \u79cd\u5b50\u6837\u672c\u751f\u6210\u4e86 3.3K \u53ef\u9a8c\u8bc1\u7684\u6311\u6218\u6027\u95ee\u9898\uff0c\u6a21\u578b\u5728\u4e94\u4e2a\u9886\u57df\u5916\u89c6\u89c9\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "SynthRL \u80fd\u6709\u6548\u6269\u5c55\u6570\u636e\u5e76\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u6570\u636e\u5408\u6210\uff0c\u63a8\u7406\u4efb\u52a1\uff0cSynthRL"}}
{"id": "2506.02404", "pdf": "https://arxiv.org/pdf/2506.02404", "abs": "https://arxiv.org/abs/2506.02404", "authors": ["Yilin Xiao", "Junnan Dong", "Chuang Zhou", "Su Dong", "Qianwen Zhang", "Di Yin", "Xing Sun", "Xiao Huang"], "title": "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing\nrecognition for its potential to enhance large language models (LLMs) by\nstructurally organizing domain-specific corpora and facilitating complex\nreasoning. However, current evaluations of GraphRAG models predominantly rely\non traditional question-answering datasets. Their limited scope in questions\nand evaluation metrics fails to comprehensively assess the reasoning capacity\nimprovements enabled by GraphRAG models. To address this gap, we introduce\nGraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously\nevaluate GraphRAG models. Our benchmark offers three key superiorities: \\((i)\\)\nChallenging question design. Featuring college-level, domain-specific questions\nthat demand multi-hop reasoning, the benchmark ensures that simple content\nretrieval is insufficient for problem-solving. For example, some questions\nrequire mathematical reasoning or programming. \\((ii)\\) Diverse task coverage.\nThe dataset includes a broad spectrum of reasoning tasks, multiple-choice,\ntrue/false, multi-select, open-ended, and fill-in-the-blank. It spans 16\ndisciplines in twenty core textbooks. \\((iii)\\) Holistic evaluation framework.\nGraphRAG-Bench provides comprehensive assessment across the entire GraphRAG\npipeline, including graph construction, knowledge retrieval, and answer\ngeneration. Beyond final-answer correctness, it evaluates the logical coherence\nof the reasoning process. By applying nine contemporary GraphRAG methods to\nGraphRAG-Bench, we demonstrate its utility in quantifying how graph-based\nstructuring improves model reasoning capabilities. Our analysis reveals\ncritical insights about graph architectures, retrieval efficacy, and reasoning\ncapabilities, offering actionable guidance for the research community.", "AI": {"tldr": "GraphRAG-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u9886\u57df\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u5168\u9762\u8bc4\u4f30GraphRAG\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u586b\u8865\u4e86\u5f53\u524d\u8bc4\u6d4b\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524dGraphRAG\u6a21\u578b\u7684\u8bc4\u6d4b\u4e3b\u8981\u4f9d\u8d56\u4f20\u7edf\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u5176\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u6d4b\u6807\u51c6\u3002", "method": "\u8bbe\u8ba1\u4e86GraphRAG-Bench\uff0c\u5305\u542b\u6311\u6218\u6027\u95ee\u9898\u3001\u591a\u6837\u5316\u4efb\u52a1\u548c\u5168\u9762\u8bc4\u4f30\u6846\u67b6\uff0c\u6db5\u76d616\u4e2a\u5b66\u79d1\u7684\u6838\u5fc3\u6559\u79d1\u4e66\u5185\u5bb9\u3002", "result": "\u901a\u8fc7\u5e94\u7528\u4e5d\u79cd\u5f53\u524dGraphRAG\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86GraphRAG-Bench\u7684\u6709\u6548\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u5173\u4e8e\u56fe\u7ed3\u6784\u3001\u68c0\u7d22\u6548\u7387\u548c\u63a8\u7406\u80fd\u529b\u7684\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "GraphRAG-Bench\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u91cf\u5316GraphRAG\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u5de5\u5177\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u7684\u65b9\u5411\u3002", "keywords": "GraphRAG, benchmark, complex reasoning, domain-specific, multi-hop reasoning"}}
{"id": "2506.02805", "pdf": "https://arxiv.org/pdf/2506.02805", "abs": "https://arxiv.org/abs/2506.02805", "authors": ["Henri Bollaert", "Chris Cornelis", "Marko Palangeti\u0107", "Salvatore Greco", "Roman S\u0142owi\u0144ski"], "title": "Optimising the attribute order in Fuzzy Rough Rule Induction", "categories": ["cs.AI"], "comment": "This is the author's version of the work accepted for publication in\n  Lecture Notes in Computer Science. The final publication is available at\n  Springer via https://doi.org/10.1007/978-3-031-92747-8_16", "summary": "Interpretability is the next pivotal frontier in machine learning research.\nIn the pursuit of glass box models - as opposed to black box models, like\nrandom forests or neural networks - rule induction algorithms are a logical and\npromising avenue, as the rules can easily be understood by humans. In our\nprevious work, we introduced FRRI, a novel rule induction algorithm based on\nfuzzy rough set theory. We demonstrated experimentally that FRRI outperformed\nother rule induction methods with regards to accuracy and number of rules. FRRI\nleverages a fuzzy indiscernibility relation to partition the data space into\nfuzzy granules, which are then combined into a minimal covering set of rules.\nThis indiscernibility relation is constructed by removing attributes from rules\nin a greedy way. This raises the question: does the order of the attributes\nmatter? In this paper, we show that optimising only the order of attributes\nusing known methods from fuzzy rough set theory and classical machine learning\ndoes not improve the performance of FRRI on multiple metrics. However, removing\na small number of attributes using fuzzy rough feature selection during this\nstep positively affects balanced accuracy and the average rule length.", "AI": {"tldr": "\u4f18\u5316\u5c5e\u6027\u987a\u5e8f\u5bf9FRRI\u6027\u80fd\u65e0\u663e\u8457\u63d0\u5347\uff0c\u4f46\u6a21\u7cca\u7c97\u7cd9\u7279\u5f81\u9009\u62e9\u53ef\u63d0\u9ad8\u5e73\u8861\u51c6\u786e\u7387\u548c\u89c4\u5219\u957f\u5ea6\u3002", "motivation": "\u7814\u7a76\u6a21\u7cca\u7c97\u7cd9\u89c4\u5219\u5f52\u7eb3\u7b97\u6cd5FRRI\u4e2d\u5c5e\u6027\u987a\u5e8f\u4f18\u5316\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63a2\u7d22\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5df2\u77e5\u6a21\u7cca\u7c97\u7cd9\u96c6\u7406\u8bba\u548c\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f18\u5316\u5c5e\u6027\u987a\u5e8f\uff0c\u5e76\u7ed3\u5408\u6a21\u7cca\u7c97\u7cd9\u7279\u5f81\u9009\u62e9\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u4f18\u5316\u5c5e\u6027\u987a\u5e8f\u672a\u6539\u5584FRRI\u6027\u80fd\uff0c\u4f46\u6a21\u7cca\u7c97\u7cd9\u7279\u5f81\u9009\u62e9\u63d0\u5347\u4e86\u5e73\u8861\u51c6\u786e\u7387\u548c\u5e73\u5747\u89c4\u5219\u957f\u5ea6\u3002", "conclusion": "\u6a21\u7cca\u7c97\u7cd9\u7279\u5f81\u9009\u62e9\u662f\u63d0\u5347FRRI\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u800c\u5c5e\u6027\u987a\u5e8f\u4f18\u5316\u7684\u6548\u679c\u6709\u9650\u3002", "keywords": "\u89c4\u5219\u5f52\u7eb3\u3001\u6a21\u7cca\u7c97\u7cd9\u96c6\u3001\u7279\u5f81\u9009\u62e9\u3001\u673a\u5668\u5b66\u4e60\u3001\u53ef\u89e3\u91ca\u6027"}}
{"id": "2506.02098", "pdf": "https://arxiv.org/pdf/2506.02098", "abs": "https://arxiv.org/abs/2506.02098", "authors": ["Miran \u00d6zdogan", "Gilad Landau", "Gereon Elvers", "Dulhan Jayalath", "Pratik Somaiya", "Francesco Mantegna", "Mark Woolrich", "Oiwi Parker Jones"], "title": "LibriBrain: Over 50 Hours of Within-Subject MEG to Improve Speech Decoding Methods at Scale", "categories": ["cs.LG"], "comment": "37 pages, 14 figures, 13 tables. Under review", "summary": "LibriBrain represents the largest single-subject MEG dataset to date for\nspeech decoding, with over 50 hours of recordings -- 5$\\times$ larger than the\nnext comparable dataset and 50$\\times$ larger than most. This unprecedented\n`depth' of within-subject data enables exploration of neural representations at\na scale previously unavailable with non-invasive methods. LibriBrain comprises\nhigh-quality MEG recordings together with detailed annotations from a single\nparticipant listening to naturalistic spoken English, covering nearly the full\nSherlock Holmes canon. Designed to support advances in neural decoding,\nLibriBrain comes with a Python library for streamlined integration with deep\nlearning frameworks, standard data splits for reproducibility, and baseline\nresults for three foundational decoding tasks: speech detection, phoneme\nclassification, and word classification. Baseline experiments demonstrate that\nincreasing training data yields substantial improvements in decoding\nperformance, highlighting the value of scaling up deep, within-subject\ndatasets. By releasing this dataset, we aim to empower the research community\nto advance speech decoding methodologies and accelerate the development of\nsafe, effective clinical brain-computer interfaces.", "AI": {"tldr": "LibriBrain is the largest single-subject MEG\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bed\u97f3\u89e3\u7801\uff0c\u5305\u542b\u8d85\u8fc750\u5c0f\u65f6\u7684\u5f55\u97f3\uff0c\u65e8\u5728\u63a8\u52a8\u795e\u7ecf\u89e3\u7801\u6280\u672f\u7684\u8fdb\u6b65\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u7684\u4e2a\u4f53\u5185\u6570\u636e\uff0c\u9650\u5236\u4e86\u975e\u4fb5\u5165\u6027\u65b9\u6cd5\u4e0b\u795e\u7ecf\u8868\u5f81\u7684\u7814\u7a76\u3002LibriBrain\u7684\u53d1\u5e03\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4fc3\u8fdb\u8bed\u97f3\u89e3\u7801\u65b9\u6cd5\u548c\u4e34\u5e8a\u8111\u673a\u63a5\u53e3\u7684\u53d1\u5c55\u3002", "method": "\u6570\u636e\u96c6\u5305\u542b\u9ad8\u8d28\u91cf\u7684MEG\u5f55\u97f3\u548c\u8be6\u7ec6\u6807\u6ce8\uff0c\u652f\u6301\u6df1\u5ea6\u5b66\u4e60\u96c6\u6210\uff0c\u5e76\u63d0\u4f9b\u6807\u51c6\u6570\u636e\u5206\u5272\u548c\u57fa\u7ebf\u7ed3\u679c\u3002", "result": "\u57fa\u7ebf\u5b9e\u9a8c\u663e\u793a\uff0c\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u7801\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5927\u5c3a\u5ea6\u4e2a\u4f53\u5185\u6570\u636e\u96c6\u7684\u4ef7\u503c\u3002", "conclusion": "LibriBrain\u7684\u53d1\u5e03\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u6709\u671b\u63a8\u52a8\u8bed\u97f3\u89e3\u7801\u548c\u8111\u673a\u63a5\u53e3\u6280\u672f\u7684\u8fdb\u6b65\u3002", "keywords": "MEG, \u8bed\u97f3\u89e3\u7801, \u6df1\u5ea6\u5b66\u4e60, \u795e\u7ecf\u8868\u5f81, \u8111\u673a\u63a5\u53e3"}}
{"id": "2506.02412", "pdf": "https://arxiv.org/pdf/2506.02412", "abs": "https://arxiv.org/abs/2506.02412", "authors": ["Zhengyuan Liu", "Geyu Lin", "Hui Li Tan", "Huayun Zhang", "Yanfeng Lu", "Xiaoxue Gao", "Stella Xin Yin", "He Sun", "Hock Huan Goh", "Lung Hsiang Wong", "Nancy F. Chen"], "title": "SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Industry Track", "summary": "The integration of generative artificial intelligence into educational\napplications has enhanced personalized and interactive learning experiences,\nand it shows strong potential to promote young learners language acquisition.\nHowever, it is still challenging to ensure consistent and robust performance\nacross different languages and cultural contexts, and kids-friendly design\nrequires simplified instructions, engaging interactions, and age-appropriate\nscaffolding to maintain motivation and optimize learning outcomes. In this\nwork, we introduce SingaKids, a dialogic tutor designed to facilitate language\nlearning through picture description tasks. Our system integrates dense image\ncaptioning, multilingual dialogic interaction, speech understanding, and\nengaging speech generation to create an immersive learning environment in four\nlanguages: English, Mandarin, Malay, and Tamil. We further improve the system\nthrough multilingual pre-training, task-specific tuning, and scaffolding\noptimization. Empirical studies with elementary school students demonstrate\nthat SingaKids provides effective dialogic teaching, benefiting learners at\ndifferent performance levels.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86SingaKids\uff0c\u4e00\u79cd\u901a\u8fc7\u56fe\u7247\u63cf\u8ff0\u4efb\u52a1\u4fc3\u8fdb\u8bed\u8a00\u5b66\u4e60\u7684\u5bf9\u8bdd\u5f0f\u8f85\u5bfc\u7cfb\u7edf\uff0c\u6574\u5408\u4e86\u591a\u79cd\u6280\u672f\u5e76\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u53ef\u63d0\u5347\u4e2a\u6027\u5316\u5b66\u4e60\u4f53\u9a8c\uff0c\u4f46\u9700\u89e3\u51b3\u8de8\u8bed\u8a00\u3001\u6587\u5316\u4e00\u81f4\u6027\u548c\u513f\u7ae5\u53cb\u597d\u8bbe\u8ba1\u7b49\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u6574\u5408\u5bc6\u96c6\u56fe\u50cf\u6807\u6ce8\u3001\u591a\u8bed\u8a00\u5bf9\u8bdd\u4ea4\u4e92\u3001\u8bed\u97f3\u7406\u89e3\u4e0e\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u548c\u4efb\u52a1\u4f18\u5316\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0cSingaKids\u80fd\u6709\u6548\u652f\u6301\u4e0d\u540c\u6c34\u5e73\u5b66\u4e60\u8005\u7684\u8bed\u8a00\u5b66\u4e60\u3002", "conclusion": "SingaKids\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "keywords": "\u751f\u6210\u5f0fAI, \u8bed\u8a00\u5b66\u4e60, \u5bf9\u8bdd\u5f0f\u8f85\u5bfc, \u591a\u8bed\u8a00, \u513f\u7ae5\u6559\u80b2"}}
{"id": "2506.02838", "pdf": "https://arxiv.org/pdf/2506.02838", "abs": "https://arxiv.org/abs/2506.02838", "authors": ["Jizhou Wang", "Xiaodan Fang", "Lei Huang", "Yongfeng Huang"], "title": "TaxAgent: How Large Language Model Designs Fiscal Policy", "categories": ["cs.AI", "econ.GN", "q-fin.EC", "I.2.11; I.6.5; J.4"], "comment": "Accepted as oral presentation at ICME 2025", "summary": "Economic inequality is a global challenge, intensifying disparities in\neducation, healthcare, and social stability. Traditional systems like the U.S.\nfederal income tax reduce inequality but lack adaptability. Although models\nlike the Saez Optimal Taxation adjust dynamically, they fail to address\ntaxpayer heterogeneity and irrational behavior. This study introduces TaxAgent,\na novel integration of large language models (LLMs) with agent-based modeling\n(ABM) to design adaptive tax policies. In our macroeconomic simulation,\nheterogeneous H-Agents (households) simulate real-world taxpayer behaviors\nwhile the TaxAgent (government) utilizes LLMs to iteratively optimize tax\nrates, balancing equity and productivity. Benchmarked against Saez Optimal\nTaxation, U.S. federal income taxes, and free markets, TaxAgent achieves\nsuperior equity-efficiency trade-offs. This research offers a novel taxation\nsolution and a scalable, data-driven framework for fiscal policy evaluation.", "AI": {"tldr": "TaxAgent\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4ee3\u7406\u5efa\u6a21\uff0c\u8bbe\u8ba1\u81ea\u9002\u5e94\u7a0e\u6536\u653f\u7b56\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7a0e\u6536\u7cfb\u7edf\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u65e0\u6cd5\u5e94\u5bf9\u7eb3\u7a0e\u4eba\u5f02\u8d28\u6027\u548c\u975e\u7406\u6027\u884c\u4e3a\u3002", "method": "TaxAgent\u6574\u5408LLMs\u548cABM\uff0c\u6a21\u62df\u7eb3\u7a0e\u4eba\u884c\u4e3a\u5e76\u4f18\u5316\u7a0e\u7387\u3002", "result": "TaxAgent\u5728\u516c\u5e73\u4e0e\u6548\u7387\u7684\u6743\u8861\u4e0a\u4f18\u4e8eSaez\u6700\u4f18\u7a0e\u6536\u548c\u7f8e\u56fd\u8054\u90a6\u7a0e\u3002", "conclusion": "TaxAgent\u63d0\u4f9b\u4e86\u521b\u65b0\u7684\u7a0e\u6536\u89e3\u51b3\u65b9\u6848\u548c\u53ef\u6269\u5c55\u7684\u653f\u7b56\u8bc4\u4f30\u6846\u67b6\u3002", "keywords": "\u7ecf\u6d4e\u4e0d\u5e73\u7b49,\u7a0e\u6536\u653f\u7b56,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u4ee3\u7406\u5efa\u6a21,\u516c\u5e73\u4e0e\u6548\u7387"}}
{"id": "2506.02134", "pdf": "https://arxiv.org/pdf/2506.02134", "abs": "https://arxiv.org/abs/2506.02134", "authors": ["Rishi Raj Sahoo", "Rucha Bhalchandra Joshi", "Subhankar Mishra"], "title": "ReconXF: Graph Reconstruction Attack via Public Feature Explanations on Privatized Node Features and Labels", "categories": ["cs.LG", "I.2.6; K.6.5"], "comment": "Under review", "summary": "Graph Neural Networks (GNNs) achieve high performance across many\napplications but function as black-box models, limiting their use in critical\ndomains like healthcare and criminal justice. Explainability methods address\nthis by providing feature-level explanations that identify important node\nattributes for predictions. These explanations create privacy risks. Combined\nwith auxiliary information, feature explanations can enable adversaries to\nreconstruct graph structure, exposing sensitive relationships. Existing graph\nreconstruction attacks assume access to original auxiliary data, but practical\nsystems use differential privacy to protect node features and labels while\nproviding explanations for transparency. We study a threat model where\nadversaries access public feature explanations along with privatized node\nfeatures and labels. We show that existing explanation-based attacks like GSEF\nperform poorly with privatized data due to noise from differential privacy\nmechanisms. We propose ReconXF, a graph reconstruction attack for scenarios\nwith public explanations and privatized auxiliary data. Our method adapts\nexplanation-based frameworks by incorporating denoising mechanisms that handle\ndifferential privacy noise while exploiting structural signals in explanations.\nExperiments across multiple datasets show ReconXF outperforms SoTA methods in\nprivatized settings, with improvements in AUC and average precision. Results\nindicate that public explanations combined with denoising enable graph\nstructure recovery even under the privacy protection of auxiliary data. Code is\navailable at (link to be made public after acceptance).", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u7684\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u91cd\u6784\u653b\u51fb\u65b9\u6cd5ReconXF\uff0c\u80fd\u591f\u5728\u8f85\u52a9\u6570\u636e\u7ecf\u8fc7\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u7684\u60c5\u51b5\u4e0b\u4ecd\u6709\u6548\u6062\u590d\u56fe\u7ed3\u6784\u3002", "motivation": "GNNs\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u900f\u660e\u5ea6\uff0c\u4f46\u4e5f\u53ef\u80fd\u66b4\u9732\u654f\u611f\u5173\u7cfb\uff0c\u5c24\u5176\u662f\u5728\u8f85\u52a9\u6570\u636e\u7ecf\u8fc7\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u7684\u60c5\u51b5\u4e0b\uff0c\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u653b\u51fb\u624b\u6bb5\u3002", "method": "\u63d0\u51faReconXF\u65b9\u6cd5\uff0c\u7ed3\u5408\u53bb\u566a\u673a\u5236\u5904\u7406\u5dee\u5206\u9690\u79c1\u5f15\u5165\u7684\u566a\u58f0\uff0c\u5e76\u5229\u7528\u89e3\u91ca\u4e2d\u7684\u7ed3\u6784\u4fe1\u53f7\uff0c\u4ee5\u4f18\u5316\u56fe\u91cd\u6784\u653b\u51fb\u7684\u6548\u679c\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cReconXF\u5728\u9690\u79c1\u4fdd\u62a4\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86AUC\u548c\u5e73\u5747\u7cbe\u5ea6\u3002", "conclusion": "\u5373\u4f7f\u8f85\u52a9\u6570\u636e\u7ecf\u8fc7\u9690\u79c1\u4fdd\u62a4\uff0c\u516c\u5f00\u89e3\u91ca\u7ed3\u5408\u53bb\u566a\u673a\u5236\u4ecd\u53ef\u80fd\u5bfc\u81f4\u56fe\u7ed3\u6784\u7684\u4fe1\u606f\u6cc4\u9732\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u9690\u79c1\u4fdd\u62a4\u4e0e\u900f\u660e\u5ea6\u7684\u5e73\u8861\u3002", "keywords": "Graph Neural Networks, Explainability, Privacy Risks, Differential Privacy, Graph Reconstruction Attack"}}
{"id": "2506.02425", "pdf": "https://arxiv.org/pdf/2506.02425", "abs": "https://arxiv.org/abs/2506.02425", "authors": ["Tairan Liu"], "title": "Gender Inequality in English Textbooks Around the World: an NLP Approach", "categories": ["cs.CL", "stat.AP"], "comment": null, "summary": "Textbooks play a critical role in shaping children's understanding of the\nworld. While previous studies have identified gender inequality in individual\ncountries' textbooks, few have examined the issue cross-culturally. This study\napplies natural language processing methods to quantify gender inequality in\nEnglish textbooks from 22 countries across 7 cultural spheres. Metrics include\ncharacter count, firstness (which gender is mentioned first), and TF-IDF word\nassociations by gender. The analysis also identifies gender patterns in proper\nnames appearing in TF-IDF word lists, tests whether large language models can\ndistinguish between gendered word lists, and uses GloVe embeddings to examine\nhow closely keywords associate with each gender. Results show consistent\noverrepresentation of male characters in terms of count, firstness, and named\nentities. All regions exhibit gender inequality, with the Latin cultural sphere\nshowing the least disparity.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7NLP\u65b9\u6cd5\u91cf\u531622\u4e2a\u56fd\u5bb6\u82f1\u8bed\u6559\u6750\u4e2d\u7684\u6027\u522b\u4e0d\u5e73\u7b49\uff0c\u53d1\u73b0\u7537\u6027\u89d2\u8272\u5728\u6570\u91cf\u3001\u63d0\u53ca\u987a\u5e8f\u548c\u547d\u540d\u5b9e\u4f53\u4e0a\u666e\u904d\u88ab\u8fc7\u5ea6\u4ee3\u8868\u3002", "motivation": "\u63a2\u8ba8\u8de8\u6587\u5316\u80cc\u666f\u4e0b\u82f1\u8bed\u6559\u6750\u4e2d\u7684\u6027\u522b\u4e0d\u5e73\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5206\u6790\u6559\u6750\u6587\u672c\uff0c\u5305\u62ec\u89d2\u8272\u7edf\u8ba1\u3001\u63d0\u53ca\u987a\u5e8f\u3001TF-IDF\u8bcd\u5173\u8054\u7b49\u6307\u6807\u3002", "result": "\u7537\u6027\u89d2\u8272\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u5360\u4f18\uff0c\u62c9\u4e01\u6587\u5316\u5708\u7684\u4e0d\u5e73\u7b49\u7a0b\u5ea6\u6700\u4f4e\u3002", "conclusion": "\u5168\u7403\u82f1\u8bed\u6559\u6750\u666e\u904d\u5b58\u5728\u6027\u522b\u4e0d\u5e73\u7b49\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "keywords": "\u6027\u522b\u4e0d\u5e73\u7b49\u3001\u82f1\u8bed\u6559\u6750\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8de8\u6587\u5316\u7814\u7a76"}}
{"id": "2506.02865", "pdf": "https://arxiv.org/pdf/2506.02865", "abs": "https://arxiv.org/abs/2506.02865", "authors": ["Mathieu Andreux", "Breno Baldas Skuk", "Hamza Benchekroun", "Emilien Bir\u00e9", "Antoine Bonnet", "Riaz Bordie", "Matthias Brunel", "Pierre-Louis Cedoz", "Antoine Chassang", "Micka\u00ebl Chen", "Alexandra D. Constantinou", "Antoine d'Andign\u00e9", "Hubert de La Jonqui\u00e8re", "Aur\u00e9lien Delfosse", "Ludovic Denoyer", "Alexis Deprez", "Augustin Derupti", "Michael Eickenberg", "Math\u00efs Federico", "Charles Kantor", "Xavier Koegler", "Yann Labb\u00e9", "Matthew C. H. Lee", "Erwan Le Jumeau de Kergaradec", "Amir Mahla", "Avshalom Manevich", "Adrien Maret", "Charles Masson", "Rafa\u00ebl Maurin", "Arturo Mena", "Philippe Modard", "Axel Moyal", "Axel Nguyen Kerbel", "Julien Revelle", "Mats L. Richter", "Mar\u00eda Santos", "Laurent Sifre", "Maxime Theillard", "Marc Thibault", "Louis Thiry", "L\u00e9o Tronchon", "Nicolas Usunier", "Tony Wu"], "title": "Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights", "categories": ["cs.AI"], "comment": "Alphabetical order", "summary": "We present Surfer-H, a cost-efficient web agent that integrates\nVision-Language Models (VLM) to perform user-defined tasks on the web. We pair\nit with Holo1, a new open-weight collection of VLMs specialized in web\nnavigation and information extraction. Holo1 was trained on carefully curated\ndata sources, including open-access web content, synthetic examples, and\nself-produced agentic data. Holo1 tops generalist User Interface (UI)\nbenchmarks as well as our new web UI localization benchmark, WebClick. When\npowered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on\nWebVoyager, striking a Pareto-optimal balance between accuracy and\ncost-efficiency. To accelerate research advancement in agentic systems, we are\nopen-sourcing both our WebClick evaluation dataset and the Holo1 model weights.", "AI": {"tldr": "Surfer-H\u662f\u4e00\u79cd\u6210\u672c\u9ad8\u6548\u7684\u7f51\u7edc\u4ee3\u7406\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5b8c\u6210\u4efb\u52a1\uff0c\u5e76\u642d\u914d\u65b0\u7684\u5f00\u6e90\u6a21\u578bHolo1\uff0c\u5728UI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u5353\u8d8a\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u4e14\u6210\u672c\u4f4e\u5ec9\u7684\u7f51\u7edc\u4ee3\u7406\uff0c\u7528\u4e8e\u5904\u7406\u7528\u6237\u5b9a\u4e49\u7684\u7f51\u7edc\u4efb\u52a1\uff0c\u540c\u65f6\u63a8\u52a8\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u89c6\u89c9\u8bed\u8a00\u6a21\u578bHolo1\uff0c\u5e76\u7528\u7cbe\u5fc3\u7b5b\u9009\u7684\u6570\u636e\uff08\u5305\u62ec\u5f00\u6e90\u5185\u5bb9\u3001\u5408\u6210\u6570\u636e\u7b49\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u8bbe\u8ba1\u4e86\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5WebClick\u3002", "result": "\u5728WebVoyager\u6d4b\u8bd5\u4e2d\u8fbe\u523092.2%\u7684\u6700\u9ad8\u6027\u80fd\uff0c\u5e73\u8861\u4e86\u51c6\u786e\u6027\u4e0e\u6210\u672c\u6548\u7387\u3002", "conclusion": "Surfer-H\u4e0eHolo1\u5728\u6027\u80fd\u548c\u6210\u672c\u6548\u7387\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u5f00\u6e90\u4e86\u6a21\u578b\u548c\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u7814\u7a76\u53d1\u5c55\u3002", "keywords": "Surfer-H, \u89c6\u89c9\u8bed\u8a00\u6a21\u578b, Holo1, \u7f51\u7edc\u4ee3\u7406, WebClick, WebVoyager"}}
{"id": "2506.02138", "pdf": "https://arxiv.org/pdf/2506.02138", "abs": "https://arxiv.org/abs/2506.02138", "authors": ["Yarden Bakish", "Itamar Zimerman", "Hila Chefer", "Lior Wolf"], "title": "Revisiting LRP: Positional Attribution as the Missing Ingredient for Transformer Explainability", "categories": ["cs.LG", "I.2.6; I.2.7"], "comment": null, "summary": "The development of effective explainability tools for Transformers is a\ncrucial pursuit in deep learning research. One of the most promising approaches\nin this domain is Layer-wise Relevance Propagation (LRP), which propagates\nrelevance scores backward through the network to the input space by\nredistributing activation values based on predefined rules. However, existing\nLRP-based methods for Transformer explainability entirely overlook a critical\ncomponent of the Transformer architecture: its positional encoding (PE),\nresulting in violation of the conservation property, and the loss of an\nimportant and unique type of relevance, which is also associated with\nstructural and positional features. To address this limitation, we reformulate\nthe input space for Transformer explainability as a set of position-token\npairs. This allows us to propose specialized theoretically-grounded LRP rules\ndesigned to propagate attributions across various positional encoding methods,\nincluding Rotary, Learnable, and Absolute PE. Extensive experiments with both\nfine-tuned classifiers and zero-shot foundation models, such as LLaMA 3,\ndemonstrate that our method significantly outperforms the state-of-the-art in\nboth vision and NLP explainability tasks. Our code is publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5c42\u76f8\u5173\u4f20\u64ad\uff08LRP\uff09\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8eTransformer\u7684\u4f4d\u7f6e\u7f16\u7801\uff08PE\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6PE\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u548cNLP\u4efb\u52a1\u7684\u89e3\u91ca\u6027\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684LRP\u65b9\u6cd5\u5728\u89e3\u91caTransformer\u65f6\u5ffd\u7565\u4e86\u4f4d\u7f6e\u7f16\u7801\uff08PE\uff09\uff0c\u5bfc\u81f4\u4e86\u5b88\u6052\u6027\u8d28\u7684\u7834\u574f\u548c\u4f4d\u7f6e\u76f8\u5173\u6027\u7684\u4e22\u5931\uff0c\u9650\u5236\u4e86\u5176\u89e3\u91ca\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5c06\u8f93\u5165\u7a7a\u95f4\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4f4d\u7f6e-\u6807\u8bb0\u5bf9\uff0c\u63d0\u51fa\u4e86\u4e13\u95e8\u7684\u7406\u8bba\u57fa\u7840\u7684LRP\u89c4\u5219\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff08\u5982Rotary\u3001Learnable\u548cAbsolute PE\uff09\u3002", "result": "\u5728\u5fae\u8c03\u5206\u7c7b\u5668\u548c\u96f6\u6837\u672c\u57fa\u7840\u6a21\u578b\uff08\u5982LLaMA 3\uff09\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u548cNLP\u89e3\u91ca\u6027\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6539\u8fdb\u7684LRP\u65b9\u6cd5\u901a\u8fc7\u7eb3\u5165\u4f4d\u7f6e\u7f16\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86Transformer\u7684\u89e3\u91ca\u6027\u80fd\u529b\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u7684\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "keywords": "Transformer, Layer-wise Relevance Propagation (LRP), positional encoding, explainability, NLP, vision"}}
{"id": "2506.02426", "pdf": "https://arxiv.org/pdf/2506.02426", "abs": "https://arxiv.org/abs/2506.02426", "authors": ["Maryam Berijanian", "Kuldeep Singh", "Amin Sehati"], "title": "Comparative Analysis of AI Agent Architectures for Entity Relationship Classification", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.1"], "comment": null, "summary": "Entity relationship classification remains a challenging task in information\nextraction, especially in scenarios with limited labeled data and complex\nrelational structures. In this study, we conduct a comparative analysis of\nthree distinct AI agent architectures designed to perform relation\nclassification using large language models (LLMs). The agentic architectures\nexplored include (1) reflective self-evaluation, (2) hierarchical task\ndecomposition, and (3) a novel multi-agent dynamic example generation\nmechanism, each leveraging different modes of reasoning and prompt adaptation.\nIn particular, our dynamic example generation approach introduces real-time\ncooperative and adversarial prompting. We systematically compare their\nperformance across multiple domains and model backends. Our experiments\ndemonstrate that multi-agent coordination consistently outperforms standard\nfew-shot prompting and approaches the performance of fine-tuned models. These\nfindings offer practical guidance for the design of modular, generalizable\nLLM-based systems for structured relation extraction. The source codes and\ndataset are available at\n\\href{https://github.com/maryambrj/ALIEN.git}{https://github.com/maryambrj/ALIEN.git}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6bd4\u8f83\u4e86\u4e09\u79cdAI\u4ee3\u7406\u67b6\u6784\u5728\u5173\u7cfb\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4f18\u4e8e\u6807\u51c6\u5c11\u6837\u672c\u63d0\u793a\uff0c\u63a5\u8fd1\u5fae\u8c03\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4fe1\u606f\u62bd\u53d6\u4e2d\u6709\u9650\u6807\u6ce8\u6570\u636e\u548c\u590d\u6742\u5173\u7cfb\u7ed3\u6784\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u6bd4\u8f83\u4e09\u79cd\u67b6\u6784\uff1a\u53cd\u5c04\u6027\u81ea\u8bc4\u4f30\u3001\u5c42\u6b21\u4efb\u52a1\u5206\u89e3\u548c\u52a8\u6001\u591a\u667a\u80fd\u4f53\u793a\u4f8b\u751f\u6210\u3002", "result": "\u591a\u667a\u80fd\u4f53\u534f\u8c03\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u63d0\u793a\u65b9\u6cd5\uff0c\u63a5\u8fd1\u5fae\u8c03\u6a21\u578b\u3002", "conclusion": "\u4e3a\u6a21\u5757\u5316\u3001\u53ef\u6cdb\u5316\u7684LLM\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\u3002", "keywords": "\u5173\u7cfb\u5206\u7c7b, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u591a\u667a\u80fd\u4f53, \u52a8\u6001\u793a\u4f8b\u751f\u6210"}}
{"id": "2506.02867", "pdf": "https://arxiv.org/pdf/2506.02867", "abs": "https://arxiv.org/abs/2506.02867", "authors": ["Chen Qian", "Dongrui Liu", "Haochen Wen", "Zhen Bai", "Yong Liu", "Jing Shao"], "title": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "Preprint. Under review", "summary": "Large reasoning models (LRMs) have demonstrated impressive capabilities in\ncomplex problem-solving, yet their internal reasoning mechanisms remain poorly\nunderstood. In this paper, we investigate the reasoning trajectories of LRMs\nfrom an information-theoretic perspective. By tracking how mutual information\n(MI) between intermediate representations and the correct answer evolves during\nLRM reasoning, we observe an interesting MI peaks phenomenon: the MI at\nspecific generative steps exhibits a sudden and significant increase during\nLRM's reasoning process. We theoretically analyze such phenomenon and show that\nas MI increases, the probability of model's prediction error decreases.\nFurthermore, these MI peaks often correspond to tokens expressing reflection or\ntransition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the\nthinking tokens. We then demonstrate that these thinking tokens are crucial for\nLRM's reasoning performance, while other tokens has minimal impacts. Building\non these analyses, we propose two simple yet effective methods to improve LRM's\nreasoning performance, by delicately leveraging these thinking tokens. Overall,\nour work provides novel insights into the reasoning mechanisms of LRMs and\noffers practical ways to improve their reasoning capabilities. The code is\navailable at https://github.com/ChnQ/MI-Peaks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u4fe1\u606f\u8bba\u89c6\u89d2\u7814\u7a76\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u63a8\u7406\u673a\u5236\uff0c\u53d1\u73b0\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4e92\u4fe1\u606f\uff08MI\uff09\u5cf0\u503c\u73b0\u8c61\uff0c\u5e76\u5206\u6790\u5176\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u5229\u7528\u2018\u601d\u8003\u6807\u8bb0\u2019\u63d0\u5347\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "motivation": "\u7406\u89e3\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u5185\u90e8\u63a8\u7406\u673a\u5236\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u4e92\u4fe1\u606f\uff08MI\uff09\u5206\u6790\u63ed\u793a\u5176\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u8282\u70b9\u3002", "method": "\u901a\u8fc7\u8ddf\u8e2a\u4e2d\u95f4\u8868\u793a\u4e0e\u6b63\u786e\u7b54\u6848\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u6f14\u53d8\uff0c\u8bc6\u522bMI\u5cf0\u503c\u73b0\u8c61\uff0c\u5e76\u5206\u6790\u5176\u4e0e\u6a21\u578b\u9884\u6d4b\u8bef\u5dee\u7684\u5173\u7cfb\u3002\u91cd\u70b9\u5173\u6ce8\u2018\u601d\u8003\u6807\u8bb0\u2019\u5bf9\u63a8\u7406\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0MI\u5cf0\u503c\u73b0\u8c61\u4e0e\u6a21\u578b\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e\u76f8\u5173\uff0c\u2018\u601d\u8003\u6807\u8bb0\u2019\uff08\u5982\u2018Hmm\u2019\u3001\u2018Wait\u2019\u7b49\uff09\u5bf9\u63a8\u7406\u6027\u80fd\u5177\u6709\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u4e3aLRM\u7684\u63a8\u7406\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u2018\u601d\u8003\u6807\u8bb0\u2019\u63d0\u5347\u63a8\u7406\u6027\u80fd\u7684\u5b9e\u7528\u65b9\u6cd5\u3002", "keywords": "\u5927\u578b\u63a8\u7406\u6a21\u578b,\u4e92\u4fe1\u606f\u5cf0\u503c,\u601d\u8003\u6807\u8bb0,\u4fe1\u606f\u8bba,\u63a8\u7406\u673a\u5236"}}
{"id": "2506.02154", "pdf": "https://arxiv.org/pdf/2506.02154", "abs": "https://arxiv.org/abs/2506.02154", "authors": ["Guillaume Godin"], "title": "Z-Error Loss for Training Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, 6 figures, A technical note", "summary": "Outliers introduce significant training challenges in neural networks by\npropagating erroneous gradients, which can degrade model performance and\ngeneralization. We propose the Z-Error Loss, a statistically principled\napproach that minimizes outlier influence during training by masking the\ncontribution of data points identified as out-of-distribution within each\nbatch. This method leverages batch-level statistics to automatically detect and\nexclude anomalous samples, allowing the model to focus its learning on the true\nunderlying data structure. Our approach is robust, adaptive to data quality,\nand provides valuable diagnostics for data curation and cleaning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7edf\u8ba1\u4e0a\u4e25\u8c28\u7684Z-Error Loss\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c4f\u853d\u6279\u6b21\u4e2d\u7684\u79bb\u7fa4\u6837\u672c\uff0c\u51cf\u5c11\u5176\u5bf9\u8bad\u7ec3\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "motivation": "\u79bb\u7fa4\u6837\u672c\u5728\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u4f20\u64ad\u9519\u8bef\u7684\u68af\u5ea6\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u51cf\u5c11\u5176\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86Z-Error Loss\u65b9\u6cd5\uff0c\u5229\u7528\u6279\u6b21\u7ea7\u7edf\u8ba1\u81ea\u52a8\u68c0\u6d4b\u5e76\u6392\u9664\u5f02\u5e38\u6837\u672c\uff0c\u4f7f\u6a21\u578b\u4e13\u6ce8\u4e8e\u771f\u5b9e\u6570\u636e\u7ed3\u6784\u7684\u8bad\u7ec3\u3002", "result": "\u8be5\u65b9\u6cd5\u5bf9\u6570\u636e\u8d28\u91cf\u5177\u6709\u9c81\u68d2\u6027\u548c\u81ea\u9002\u5e94\u6027\uff0c\u540c\u65f6\u4e3a\u6570\u636e\u6e05\u7406\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u8bca\u65ad\u4fe1\u606f\u3002", "conclusion": "Z-Error Loss\u662f\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u89e3\u51b3\u79bb\u7fa4\u6837\u672c\u5728\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "keywords": "\u79bb\u7fa4\u6837\u672c, Z-Error Loss, \u795e\u7ecf\u7f51\u7edc, \u6570\u636e\u6e05\u7406, \u6279\u6b21\u7edf\u8ba1"}}
{"id": "2506.02431", "pdf": "https://arxiv.org/pdf/2506.02431", "abs": "https://arxiv.org/abs/2506.02431", "authors": ["Mahammed Kamruzzaman", "Abdullah Al Monsur", "Gene Louis Kim", "Anshuman Chhabra"], "title": "From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Emotions are a fundamental facet of human experience, varying across\nindividuals, cultural contexts, and nationalities. Given the recent success of\nLarge Language Models (LLMs) as role-playing agents, we examine whether LLMs\nexhibit emotional stereotypes when assigned nationality-specific personas.\nSpecifically, we investigate how different countries are represented in\npre-trained LLMs through emotion attributions and whether these attributions\nalign with cultural norms. Our analysis reveals significant nationality-based\ndifferences, with emotions such as shame, fear, and joy being\ndisproportionately assigned across regions. Furthermore, we observe notable\nmisalignment between LLM-generated and human emotional responses, particularly\nfor negative emotions, highlighting the presence of reductive and potentially\nbiased stereotypes in LLM outputs.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u626e\u6f14\u56fd\u5bb6\u7279\u5b9a\u89d2\u8272\u65f6\u662f\u5426\u8868\u73b0\u51fa\u60c5\u611f\u523b\u677f\u5370\u8c61\uff0c\u53d1\u73b0LLMs\u4e2d\u5b58\u5728\u57fa\u4e8e\u56fd\u7c4d\u7684\u60c5\u611f\u504f\u89c1\uff0c\u4e14\u4e0e\u4eba\u7c7b\u60c5\u611f\u53cd\u5e94\u4e0d\u4e00\u81f4\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u60c5\u611f\u8868\u8fbe\u4e2d\u662f\u5426\u5b58\u5728\u56fd\u7c4d\u523b\u677f\u5370\u8c61\uff0c\u53ca\u5176\u4e0e\u6587\u5316\u89c4\u8303\u7684\u5339\u914d\u7a0b\u5ea6\u3002", "method": "\u901a\u8fc7\u5206\u6790\u9884\u8bad\u7ec3LLMs\u5bf9\u4e0d\u540c\u56fd\u5bb6\u89d2\u8272\u7684\u60c5\u611f\u5206\u914d\uff0c\u5e76\u4e0e\u4eba\u7c7b\u60c5\u611f\u53cd\u5e94\u5bf9\u6bd4\u3002", "result": "\u53d1\u73b0LLMs\u4e2d\u5b58\u5728\u663e\u8457\u7684\u56fd\u5bb6\u60c5\u611f\u504f\u89c1\uff0c\u5c24\u5176\u662f\u8d1f\u9762\u60c5\u611f\uff0c\u4e0e\u4eba\u7c7b\u60c5\u611f\u53cd\u5e94\u4e0d\u5339\u914d\u3002", "conclusion": "LLMs\u5728\u60c5\u611f\u8868\u8fbe\u4e0a\u5b58\u5728\u7b80\u5316\u548c\u504f\u89c1\uff0c\u9700\u6539\u8fdb\u4ee5\u907f\u514d\u523b\u677f\u5370\u8c61\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u60c5\u611f\u523b\u677f\u5370\u8c61\u3001\u56fd\u7c4d\u5dee\u5f02\u3001\u6587\u5316\u89c4\u8303"}}
{"id": "2506.02873", "pdf": "https://arxiv.org/pdf/2506.02873", "abs": "https://arxiv.org/abs/2506.02873", "authors": ["Matthew Kowal", "Jasper Timm", "Jean-Francois Godbout", "Thomas Costello", "Antonio A. Arechar", "Gordon Pennycook", "David Rand", "Adam Gleave", "Kellin Pelrine"], "title": "It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics", "categories": ["cs.AI"], "comment": null, "summary": "Persuasion is a powerful capability of large language models (LLMs) that both\nenables beneficial applications (e.g. helping people quit smoking) and raises\nsignificant risks (e.g. large-scale, targeted political manipulation). Prior\nwork has found models possess a significant and growing persuasive capability,\nmeasured by belief changes in simulated or real users. However, these\nbenchmarks overlook a crucial risk factor: the propensity of a model to attempt\nto persuade in harmful contexts. Understanding whether a model will blindly\n``follow orders'' to persuade on harmful topics (e.g. glorifying joining a\nterrorist group) is key to understanding the efficacy of safety guardrails.\nMoreover, understanding if and when a model will engage in persuasive behavior\nin pursuit of some goal is essential to understanding the risks from agentic AI\nsystems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts\nthe focus from persuasion success to persuasion attempts, operationalized as a\nmodel's willingness to generate content aimed at shaping beliefs or behavior.\nOur evaluation framework probes frontier LLMs using a multi-turn conversational\nsetup between simulated persuader and persuadee agents. APE explores a diverse\nspectrum of topics including conspiracies, controversial issues, and\nnon-controversially harmful content. We introduce an automated evaluator model\nto identify willingness to persuade and measure the frequency and context of\npersuasive attempts. We find that many open and closed-weight models are\nfrequently willing to attempt persuasion on harmful topics and that\njailbreaking can increase willingness to engage in such behavior. Our results\nhighlight gaps in current safety guardrails and underscore the importance of\nevaluating willingness to persuade as a key dimension of LLM risk. APE is\navailable at github.com/AlignmentResearch/AttemptPersuadeEval", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u2018\u5c1d\u8bd5\u8bf4\u670d\u8bc4\u4f30\u2019\uff08APE\uff09\u57fa\u51c6\uff0c\u7528\u4e8e\u8861\u91cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6709\u5bb3\u60c5\u5883\u4e0b\u8bd5\u56fe\u8bf4\u670d\u7684\u610f\u613f\uff0c\u63ed\u793a\u4e86\u5176\u5b89\u5168\u9632\u62a4\u7684\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u5728\u6709\u5bb3\u4e3b\u9898\u4e0b\u5c1d\u8bd5\u8bf4\u670d\u7684\u503e\u5411\uff0c\u4ee5\u7406\u89e3\u5176\u5b89\u5168\u98ce\u9669\uff0c\u5c24\u5176\u662f\u5f53\u6a21\u578b\u88ab\u6307\u793a\u6267\u884c\u6709\u5bb3\u4efb\u52a1\u65f6\u3002", "method": "\u91c7\u7528APE\u57fa\u51c6\uff0c\u901a\u8fc7\u6a21\u62df\u591a\u8f6e\u5bf9\u8bdd\u8bbe\u7f6e\uff0c\u6d4b\u8bd5LLMs\u5728\u4e89\u8bae\u6027\u548c\u6709\u5bb3\u4e3b\u9898\u4e0b\u7684\u8bf4\u670d\u610f\u613f\uff0c\u5e76\u5f15\u5165\u81ea\u52a8\u8bc4\u4f30\u6a21\u578b\u91cf\u5316\u8bf4\u670d\u5c1d\u8bd5\u7684\u9891\u7387\u548c\u60c5\u5883\u3002", "result": "\u53d1\u73b0\u8bb8\u591a\u5f00\u653e\u548c\u5c01\u95ed\u6743\u91cd\u6a21\u578b\u5728\u6709\u5bb3\u4e3b\u9898\u4e0b\u9891\u7e41\u5c1d\u8bd5\u8bf4\u670d\uff0c\u4e14\u7834\u89e3\uff08jailbreaking\uff09\u4f1a\u52a0\u5267\u6b64\u884c\u4e3a\uff0c\u66b4\u9732\u4e86\u73b0\u6709\u5b89\u5168\u63aa\u65bd\u7684\u4e0d\u8db3\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u8bc4\u4f30LLMs\u7684\u8bf4\u670d\u610f\u613f\u662f\u98ce\u9669\u7ba1\u63a7\u7684\u5173\u952e\u7ef4\u5ea6\uff0cAPE\u4e3a\u672a\u6765\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u8bf4\u670d\u80fd\u529b, \u98ce\u9669\u8bc4\u6d4b, \u5b89\u5168\u9632\u62a4, \u81ea\u52a8\u8bc4\u4f30"}}
{"id": "2506.02168", "pdf": "https://arxiv.org/pdf/2506.02168", "abs": "https://arxiv.org/abs/2506.02168", "authors": ["Hrushikesh N. Mhaskar", "Efstratios Tsoukanis", "Ameya D. Jagtap"], "title": "An Approximation Theory Perspective on Machine Learning", "categories": ["cs.LG"], "comment": "56 pages", "summary": "A central problem in machine learning is often formulated as follows: Given a\ndataset $\\{(x_j, y_j)\\}_{j=1}^M$, which is a sample drawn from an unknown\nprobability distribution, the goal is to construct a functional model $f$ such\nthat $f(x) \\approx y$ for any $(x, y)$ drawn from the same distribution. Neural\nnetworks and kernel-based methods are commonly employed for this task due to\ntheir capacity for fast and parallel computation. The approximation\ncapabilities, or expressive power, of these methods have been extensively\nstudied over the past 35 years. In this paper, we will present examples of key\nideas in this area found in the literature. We will discuss emerging trends in\nmachine learning including the role of shallow/deep networks, approximation on\nmanifolds, physics-informed neural surrogates, neural operators, and\ntransformer architectures. Despite function approximation being a fundamental\nproblem in machine learning, approximation theory does not play a central role\nin the theoretical foundations of the field. One unfortunate consequence of\nthis disconnect is that it is often unclear how well trained models will\ngeneralize to unseen or unlabeled data. In this review, we examine some of the\nshortcomings of the current machine learning framework and explore the reasons\nfor the gap between approximation theory and machine learning practice. We will\nthen introduce our novel research to achieve function approximation on unknown\nmanifolds without the need to learn specific manifold features, such as the\neigen-decomposition of the Laplace-Beltrami operator or atlas construction. In\nmany machine learning problems, particularly classification tasks, the labels\n$y_j$ are drawn from a finite set of values.", "AI": {"tldr": "\u8ba8\u8bba\u4e86\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u51fd\u6570\u903c\u8fd1\u95ee\u9898\uff0c\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u672a\u77e5\u6d41\u5f62\u7684\u65b0\u7814\u7a76\u3002", "motivation": "\u7814\u7a76\u673a\u5668\u5b66\u4e60\u4e2d\u51fd\u6570\u903c\u8fd1\u7684\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u56de\u987e\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u795e\u7ecf\u7f51\u7edc\u3001\u6838\u65b9\u6cd5\uff09\uff0c\u5206\u6790\u5176\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e0d\u4f9d\u8d56\u6d41\u5f62\u7279\u5f81\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u5728\u672a\u77e5\u6d41\u5f62\u4e0a\u5b9e\u73b0\u51fd\u6570\u903c\u8fd1\u7684\u65b0\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u51fd\u6570\u903c\u8fd1\u7406\u8bba\u5e94\u66f4\u7d27\u5bc6\u5730\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u5b9e\u8df5\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "keywords": "\u673a\u5668\u5b66\u4e60, \u51fd\u6570\u903c\u8fd1, \u795e\u7ecf\u7f51\u7edc, \u6d41\u5f62\u5b66\u4e60, \u6cdb\u5316\u80fd\u529b"}}
{"id": "2506.02442", "pdf": "https://arxiv.org/pdf/2506.02442", "abs": "https://arxiv.org/abs/2506.02442", "authors": ["Utsav Maskey", "Mark Dras", "Usman Naseem"], "title": "Should LLM Safety Be More Than Refusing Harmful Instructions?", "categories": ["cs.CL"], "comment": "Preprint", "summary": "This paper presents a systematic evaluation of Large Language Models' (LLMs)\nbehavior on long-tail distributed (encrypted) texts and their safety\nimplications. We introduce a two-dimensional framework for assessing LLM\nsafety: (1) instruction refusal-the ability to reject harmful obfuscated\ninstructions, and (2) generation safety-the suppression of generating harmful\nresponses. Through comprehensive experiments, we demonstrate that models that\npossess capabilities to decrypt ciphers may be susceptible to\nmismatched-generalization attacks: their safety mechanisms fail on at least one\nsafety dimension, leading to unsafe responses or over-refusal. Based on these\nfindings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss\ntheir strengths and limitations. This work contributes to understanding the\nsafety of LLM in long-tail text scenarios and provides directions for\ndeveloping robust safety mechanisms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5c3e\u5206\u5e03\uff08\u52a0\u5bc6\uff09\u6587\u672c\u4e2d\u7684\u884c\u4e3a\u53ca\u5176\u5b89\u5168\u6027\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e8c\u7ef4\u5b89\u5168\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u63ed\u793a\u6a21\u578b\u5728\u89e3\u5bc6\u5bc6\u7801\u65f6\u53ef\u80fd\u53d7\u5230\u4e0d\u5339\u914d\u6cdb\u5316\u653b\u51fb\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5c3e\u5206\u5e03\u548c\u52a0\u5bc6\u6587\u672c\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u63ed\u793a\u5176\u5b89\u5168\u673a\u5236\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002", "method": "\u5f15\u5165\u4e8c\u7ef4\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u6307\u4ee4\u62d2\u7edd\u548c\u751f\u6210\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6d4b\u8bd5\u6a21\u578b\u5728\u89e3\u5bc6\u5bc6\u7801\u65f6\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5728\u89e3\u5bc6\u65f6\u53ef\u80fd\u51fa\u73b0\u5b89\u5168\u673a\u5236\u5931\u6548\u6216\u4e0d\u5fc5\u8981\u7684\u8fc7\u5ea6\u62d2\u7edd\u3002", "conclusion": "\u8bba\u6587\u4e3a\u7406\u89e3\u957f\u5c3e\u6587\u672c\u573a\u666f\u4e0b\u7684LLM\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u5f00\u53d1\u66f4\u9c81\u68d2\u5b89\u5168\u673a\u5236\u7684\u65b9\u5411\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u5b89\u5168\u6027,\u957f\u5c3e\u5206\u5e03,\u52a0\u5bc6\u6587\u672c,\u4e0d\u5339\u914d\u6cdb\u5316\u653b\u51fb"}}
{"id": "2506.02918", "pdf": "https://arxiv.org/pdf/2506.02918", "abs": "https://arxiv.org/abs/2506.02918", "authors": ["Shangmin Guo", "Omar Darwiche Domingues", "Rapha\u00ebl Avalos", "Aaron Courville", "Florian Strub"], "title": "Sample, Predict, then Proceed: Self-Verification Sampling for Tool Use of LLMs", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Tool use in stateful environments presents unique challenges for large\nlanguage models (LLMs), where existing test-time compute strategies relying on\nrepeated trials in the environment are impractical. We propose dynamics\nmodelling (DyMo), a method that augments LLMs with a state prediction\ncapability alongside function calling during post-training. This enables LLMs\nto predict the future states of their actions through an internal environment\nmodel. On the Berkeley Function Calling Leaderboard V2, DyMo improves success\nrates and significantly reduces hallucinations. We further integrate the\ninternal environment model into self-verification sampling (SVS), and show that\nthis substantially improves pass^k over number of trials k, and allows the\nmodel to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the\neffectiveness and reliability of LLMs for tool use. We believe this work charts\na path towards scalable planning RL methods for LLM inference without\nrepeatedly querying the oracle environment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDyMo\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u72b6\u6001\u9884\u6d4b\u548c\u51fd\u6570\u8c03\u7528\u589e\u5f3aLLM\uff0c\u63d0\u5347\u5176\u5728\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u6210\u529f\u7387\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u72b6\u6001\u73af\u5883\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u6311\u6218\uff0c\u907f\u514d\u91cd\u590d\u8bd5\u9a8c\u7684\u4e0d\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u52a8\u6001\u5efa\u6a21\uff08DyMo\uff09\u548c\u81ea\u6211\u9a8c\u8bc1\u91c7\u6837\uff08SVS\uff09\uff0c\u7ed3\u5408\u72b6\u6001\u9884\u6d4b\u548c\u5185\u90e8\u73af\u5883\u6a21\u578b\u3002", "result": "\u5728Berkeley Function Calling Leaderboard V2\u4e0a\uff0cDyMo\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u5e76\u51cf\u5c11\u5e7b\u89c9\u3002\u4e0eSVS\u7ed3\u5408\u540e\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6548\u679c\u548c\u53ef\u9760\u6027\u3002", "conclusion": "DyMo\u548cSVS\u663e\u8457\u589e\u5f3a\u4e86LLM\u5728\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u65e0\u9700\u91cd\u590d\u67e5\u8be2\u73af\u5883\u7684RL\u65b9\u6cd5\u63d0\u4f9b\u4e86\u8def\u5f84\u3002", "keywords": "LLM, tool use, state prediction, DyMo, SVS"}}
{"id": "2506.02200", "pdf": "https://arxiv.org/pdf/2506.02200", "abs": "https://arxiv.org/abs/2506.02200", "authors": ["Shiangyi Lin", "Hui Lan", "Vasilis Syrgkanis"], "title": "Learning Treatment Representations for Downstream Instrumental Variable Regression", "categories": ["cs.LG", "stat.ME"], "comment": null, "summary": "Traditional instrumental variable (IV) estimators face a fundamental\nconstraint: they can only accommodate as many endogenous treatment variables as\navailable instruments. This limitation becomes particularly challenging in\nsettings where the treatment is presented in a high-dimensional and\nunstructured manner (e.g. descriptions of patient treatment pathways in a\nhospital). In such settings, researchers typically resort to applying\nunsupervised dimension reduction techniques to learn a low-dimensional\ntreatment representation prior to implementing IV regression analysis. We show\nthat such methods can suffer from substantial omitted variable bias due to\nimplicit regularization in the representation learning step. We propose a novel\napproach to construct treatment representations by explicitly incorporating\ninstrumental variables during the representation learning process. Our approach\nprovides a framework for handling high-dimensional endogenous variables with\nlimited instruments. We demonstrate both theoretically and empirically that\nfitting IV models on these instrument-informed representations ensures\nidentification of directions that optimize outcome prediction. Our experiments\nshow that our proposed methodology improves upon the conventional two-stage\napproaches that perform dimension reduction without incorporating instrument\ninformation.", "AI": {"tldr": "\u4f20\u7edf\u5de5\u5177\u53d8\u91cf\uff08IV\uff09\u4f30\u8ba1\u5728\u5904\u7406\u9ad8\u7ef4\u5185\u751f\u53d8\u91cf\u65f6\u9762\u4e34\u6311\u6218\uff0c\u65b0\u65b9\u6cd5\u901a\u8fc7\u5c06\u5de5\u5177\u53d8\u91cf\u4fe1\u606f\u878d\u5165\u964d\u7ef4\u8fc7\u7a0b\uff0c\u51cf\u5c11\u4e86\u9057\u6f0f\u53d8\u91cf\u504f\u5dee\u5e76\u63d0\u9ad8\u4e86\u9884\u6d4b\u6548\u679c\u3002", "motivation": "\u4f20\u7edfIV\u65b9\u6cd5\u5728\u9ad8\u7ef4\u975e\u7ed3\u6784\u5316\u5904\u7406\u53d8\u91cf\uff08\u5982\u60a3\u8005\u6cbb\u7597\u8def\u5f84\uff09\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u7814\u7a76\u8005\u901a\u5e38\u91c7\u7528\u65e0\u76d1\u7763\u964d\u7ef4\u65b9\u6cd5\uff0c\u4f46\u53ef\u80fd\u5f15\u5165\u9057\u6f0f\u53d8\u91cf\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5728\u964d\u7ef4\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u7ed3\u5408\u5de5\u5177\u53d8\u91cf\u4fe1\u606f\uff0c\u6784\u5efa\u5904\u7406\u8868\u793a\uff0c\u4ee5\u4f18\u5316\u7ed3\u679c\u9884\u6d4b\u7684\u8bc6\u522b\u65b9\u5411\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u5747\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u7684\u4e24\u9636\u6bb5\u964d\u7ef4\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u504f\u5dee\u5e76\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408\u5de5\u5177\u53d8\u91cf\u4fe1\u606f\u7684\u964d\u7ef4\u65b9\u6cd5\u4e3a\u9ad8\u7ef4\u5185\u751f\u53d8\u91cf\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\uff0c\u663e\u8457\u6539\u5584\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "keywords": "\u5de5\u5177\u53d8\u91cf, \u9ad8\u7ef4\u6570\u636e, \u964d\u7ef4, \u9057\u6f0f\u53d8\u91cf\u504f\u5dee"}}
{"id": "2506.02449", "pdf": "https://arxiv.org/pdf/2506.02449", "abs": "https://arxiv.org/abs/2506.02449", "authors": ["Bo Peng", "Zhiheng Wang", "Heyang Gong", "Chaochao Lu"], "title": "IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "In modern dialogue systems, the ability to implicitly infer user backgrounds\nfrom conversations and leverage this information for personalized assistance is\ncrucial. However, the scarcity of high-quality data remains a fundamental\nchallenge to evaluating and improving this capability. Traditional dataset\nconstruction methods are labor-intensive, resource-demanding, and raise privacy\nconcerns. To address these issues, we propose a novel approach for automatic\nsynthetic data generation and introduce the Implicit Personalized Dialogue\n(IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12\nuser attribute types. Additionally, we develop a systematic evaluation\nframework with four metrics to assess both attribute awareness and reasoning\ncapabilities. We further propose five causal graphs to elucidate models'\nreasoning pathways during implicit personalization. Extensive experiments yield\ninsightful observations and prove the reliability of our dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u5408\u6210\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86IP-Dialog\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7528\u6237\u80cc\u666f\u9690\u5f0f\u63a8\u65ad\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u5bf9\u8bdd\u7cfb\u7edf\u9700\u8981\u4ece\u5bf9\u8bdd\u4e2d\u9690\u5f0f\u63a8\u65ad\u7528\u6237\u80cc\u666f\u4ee5\u63d0\u4f9b\u4e2a\u6027\u5316\u670d\u52a1\uff0c\u4f46\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u7a00\u7f3a\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u81ea\u52a8\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86IP-Dialog\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d610\u4e2a\u4efb\u52a1\u548c12\u79cd\u7528\u6237\u5c5e\u6027\u7c7b\u578b\u3002", "result": "\u5f00\u53d1\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6570\u636e\u96c6\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5bf9\u8bdd\u7cfb\u7edf\u7684\u4e2a\u6027\u5316\u80fd\u529b\u8bc4\u4f30\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "keywords": "\u5bf9\u8bdd\u7cfb\u7edf,\u4e2a\u6027\u5316,\u6570\u636e\u96c6,\u9690\u5f0f\u63a8\u7406,\u8bc4\u4f30\u6846\u67b6"}}
{"id": "2506.02923", "pdf": "https://arxiv.org/pdf/2506.02923", "abs": "https://arxiv.org/abs/2506.02923", "authors": ["Alexis Bellot", "Jonathan Richens", "Tom Everitt"], "title": "The Limits of Predicting Agents from Behaviour", "categories": ["cs.AI", "stat.ML"], "comment": null, "summary": "As the complexity of AI systems and their interactions with the world\nincreases, generating explanations for their behaviour is important for safely\ndeploying AI. For agents, the most natural abstractions for predicting\nbehaviour attribute beliefs, intentions and goals to the system. If an agent\nbehaves as if it has a certain goal or belief, then we can make reasonable\npredictions about how it will behave in novel situations, including those where\ncomprehensive safety evaluations are untenable. How well can we infer an\nagent's beliefs from their behaviour, and how reliably can these inferred\nbeliefs predict the agent's behaviour in novel situations? We provide a precise\nanswer to this question under the assumption that the agent's behaviour is\nguided by a world model. Our contribution is the derivation of novel bounds on\nthe agent's behaviour in new (unseen) deployment environments, which represent\na theoretical limit for predicting intentional agents from behavioural data\nalone. We discuss the implications of these results for several research areas\nincluding fairness and safety.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u884c\u4e3a\u63a8\u65adAI\u4ee3\u7406\u7684\u4fe1\u5ff5\u76ee\u6807\uff0c\u5e76\u9884\u6d4b\u5176\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u7406\u8bba\u754c\u9650\u53ca\u5176\u5bf9\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u53ca\u5176\u4e0e\u4e16\u754c\u4e92\u52a8\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u89e3\u91ca\u5176\u884c\u4e3a\u5bf9\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u884c\u4e3a\u63a8\u65ad\u4ee3\u7406\u7684\u4fe1\u5ff5\u548c\u610f\u56fe\uff0c\u4e3a\u9884\u6d4b\u5176\u5728\u65b0\u73af\u5883\u4e2d\u7684\u884c\u4e3a\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5047\u8bbe\u4ee3\u7406\u884c\u4e3a\u7531\u4e16\u754c\u6a21\u578b\u6307\u5bfc\uff0c\u63a8\u5bfc\u51fa\u4ee3\u7406\u5728\u65b0\u90e8\u7f72\u73af\u5883\u4e2d\u884c\u4e3a\u7684\u7406\u8bba\u754c\u9650\uff0c\u4ee3\u8868\u4e86\u4ec5\u4ece\u884c\u4e3a\u6570\u636e\u9884\u6d4b\u610f\u5411\u4ee3\u7406\u7684\u7406\u8bba\u6781\u9650\u3002", "result": "\u63d0\u4f9b\u4e86\u5173\u4e8e\u4ee3\u7406\u5728\u65b0\u73af\u5883\u4e2d\u884c\u4e3a\u7684\u7cbe\u786e\u7406\u8bba\u754c\u9650\uff0c\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u754c\u9650\u5bf9\u9884\u6d4b\u610f\u5411\u4ee3\u7406\u884c\u4e3a\u7684\u9650\u5236\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u9884\u6d4bAI\u4ee3\u7406\u884c\u4e3a\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u5bf9\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u542f\u793a\u3002", "keywords": "AI\u4ee3\u7406, \u884c\u4e3a\u89e3\u91ca, \u4fe1\u5ff5\u63a8\u65ad, \u4e16\u754c\u6a21\u578b, \u516c\u5e73\u6027, \u5b89\u5168\u6027"}}
{"id": "2506.02203", "pdf": "https://arxiv.org/pdf/2506.02203", "abs": "https://arxiv.org/abs/2506.02203", "authors": ["Navid NaderiAlizadeh", "Darian Salehi", "Xinran Liu", "Soheil Kolouri"], "title": "Constrained Sliced Wasserstein Embedding", "categories": ["cs.LG", "cs.AI", "math.OC", "q-bio.QM", "stat.ML"], "comment": null, "summary": "Sliced Wasserstein (SW) distances offer an efficient method for comparing\nhigh-dimensional probability measures by projecting them onto multiple\n1-dimensional probability distributions. However, identifying informative\nslicing directions has proven challenging, often necessitating a large number\nof slices to achieve desirable performance and thereby increasing computational\ncomplexity. We introduce a constrained learning approach to optimize the\nslicing directions for SW distances. Specifically, we constrain the 1D\ntransport plans to approximate the optimal plan in the original space, ensuring\nmeaningful slicing directions. By leveraging continuous relaxations of these\ntransport plans, we enable a gradient-based primal-dual approach to train the\nslicer parameters, alongside the remaining model parameters. We demonstrate how\nthis constrained slicing approach can be applied to pool high-dimensional\nembeddings into fixed-length permutation-invariant representations. Numerical\nresults on foundation models trained on images, point clouds, and protein\nsequences showcase the efficacy of the proposed constrained learning approach\nin learning more informative slicing directions. Our implementation code can be\nfound at https://github.com/Stranja572/constrainedswe.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316Sliced Wasserstein\u8ddd\u79bb\u5207\u7247\u65b9\u5411\u7684\u7ea6\u675f\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f1D\u4f20\u8f93\u8ba1\u5212\u6765\u8fd1\u4f3c\u539f\u7a7a\u95f4\u7684\u6700\u4f18\u8ba1\u5212\uff0c\u63d0\u5347\u4e86\u5207\u7247\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5f53\u524dSliced Wasserstein\u8ddd\u79bb\u4e2d\u5207\u7247\u65b9\u5411\u96be\u4ee5\u9009\u62e9\u7684\u95ee\u9898\uff0c\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528\u7ea6\u675f\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u5bf9\u5076\u4f18\u5316\u5207\u7247\u65b9\u5411\uff0c\u540c\u65f6\u7ed3\u5408\u6a21\u578b\u53c2\u6570\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u3001\u70b9\u4e91\u548c\u86cb\u767d\u8d28\u5e8f\u5217\u7b49\u9ad8\u7ef4\u6570\u636e\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ea6\u675f\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u5207\u7247\u65b9\u5411\u7684\u4fe1\u606f\u91cf\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\u7684\u56fa\u5b9a\u957f\u5ea6\u8868\u793a\u751f\u6210\u3002", "keywords": "Sliced Wasserstein\u8ddd\u79bb, \u7ea6\u675f\u5b66\u4e60, \u68af\u5ea6\u4f18\u5316, \u9ad8\u7ef4\u6570\u636e, \u56fa\u5b9a\u957f\u5ea6\u8868\u793a"}}
{"id": "2506.02454", "pdf": "https://arxiv.org/pdf/2506.02454", "abs": "https://arxiv.org/abs/2506.02454", "authors": ["Zhaorui Yang", "Bo Pan", "Han Wang", "Yiyao Wang", "Xingyu Liu", "Minfeng Zhu", "Bo Zhang", "Wei Chen"], "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework", "categories": ["cs.CL", "cs.AI"], "comment": "47 pages", "summary": "Visualizations play a crucial part in effective communication of concepts and\ninformation. Recent advances in reasoning and retrieval augmented generation\nhave enabled Large Language Models (LLMs) to perform deep research and generate\ncomprehensive reports. Despite its progress, existing deep research frameworks\nprimarily focus on generating text-only content, leaving the automated\ngeneration of interleaved texts and visualizations underexplored. This novel\ntask poses key challenges in designing informative visualizations and\neffectively integrating them with text reports. To address these challenges, we\npropose Formal Description of Visualization (FDV), a structured textual\nrepresentation of charts that enables LLMs to learn from and generate diverse,\nhigh-quality visualizations. Building on this representation, we introduce\nMultimodal DeepResearcher, an agentic framework that decomposes the task into\nfour stages: (1) researching, (2) exemplar report textualization, (3) planning,\nand (4) multimodal report generation. For the evaluation of generated\nmultimodal reports, we develop MultimodalReportBench, which contains 100\ndiverse topics served as inputs along with 5 dedicated metrics. Extensive\nexperiments across models and evaluation methods demonstrate the effectiveness\nof Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet\nmodel, Multimodal DeepResearcher achieves an 82\\% overall win rate over the\nbaseline method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4efb\u52a1\uff0c\u5373\u81ea\u52a8\u5316\u751f\u6210\u7ed3\u5408\u6587\u672c\u548c\u53ef\u89c6\u5316\u7684\u591a\u6a21\u6001\u62a5\u544a\uff0c\u5e76\u63d0\u51fa\u4e86FDV\u8868\u793a\u6cd5\u548cMultimodal DeepResearcher\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e3b\u8981\u751f\u6210\u7eaf\u6587\u672c\u5185\u5bb9\uff0c\u800c\u7ed3\u5408\u6587\u672c\u548c\u53ef\u89c6\u5316\u7684\u81ea\u52a8\u5316\u751f\u6210\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u8fd9\u9650\u5236\u4e86\u4fe1\u606f\u4f20\u8fbe\u7684\u6548\u679c\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86FDV\uff08\u7ed3\u6784\u5316\u6587\u672c\u8868\u793a\u6cd5\uff09\u548cMultimodal DeepResearcher\u6846\u67b6\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u7814\u7a76\u3001\u6587\u672c\u5316\u793a\u4f8b\u62a5\u544a\u3001\u89c4\u5212\u548c\u591a\u6a21\u6001\u62a5\u544a\u751f\u6210\u56db\u4e2a\u9636\u6bb5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMultimodal DeepResearcher\u5728Claude 3.7 Sonnet\u6a21\u578b\u4e0a\u7684\u6574\u4f53\u80dc\u7387\u4e3a82%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u62a5\u544a\u751f\u6210\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u606f\u4f20\u8fbe\u7684\u6548\u679c\u3002", "keywords": "\u53ef\u89c6\u5316, \u591a\u6a21\u6001\u62a5\u544a, FDV, \u5927\u578b\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.02949", "pdf": "https://arxiv.org/pdf/2506.02949", "abs": "https://arxiv.org/abs/2506.02949", "authors": ["Lixiang Xu", "Xianwei Ding", "Xin Yuan", "Richang Hong", "Feiping Nie", "Enhong Chen", "Philip S. Yu"], "title": "Dynamic Programming Techniques for Enhancing Cognitive Representation in Knowledge Tracing", "categories": ["cs.AI"], "comment": null, "summary": "Knowledge Tracing (KT) involves monitoring the changes in a student's\nknowledge over time by analyzing their past responses, with the goal of\npredicting future performance. However, most existing methods primarily focus\non feature enhancement, while overlooking the deficiencies in cognitive\nrepresentation and the ability to express cognition-issues often caused by\ninterference from non-cognitive factors such as slipping and guessing. This\nlimitation hampers the ability to capture the continuity and coherence of the\nstudent's cognitive process. As a result, many methods may introduce more\nprediction bias and modeling costs due to their inability to maintain cognitive\ncontinuity and coherence. Based on the above discussion, we propose the\nCognitive Representation Dynamic Programming based Knowledge Tracing (CRDP-KT)\nmodel. This model em ploys a dynamic programming algorithm to optimize\ncognitive representations based on the difficulty of the questions and the\nperformance intervals between them. This approach ensures that the cognitive\nrepresentation aligns with the student's cognitive patterns, maintaining\noverall continuity and coherence. As a result, it provides more accurate and\nsystematic input features for subsequent model training, thereby minimizing\ndistortion in the simulation of cognitive states. Additionally, the CRDP-KT\nmodel performs partitioned optimization of cognitive representations to enhance\nthe reliability of the optimization process. Furthermore, it improves its\nability to express the student's cognition through a weighted fusion of\noptimized record representations and re lationships learned from a bipartite\ngraph. Finally, experiments conducted on three public datasets validate the\neffectiveness of the proposed CRDP-KT model.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u8868\u793a\u52a8\u6001\u89c4\u5212\u7684\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\uff08CRDP-KT\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u8ba4\u77e5\u8868\u793a\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u56e0\u975e\u8ba4\u77e5\u56e0\u7d20\u5e72\u6270\u5bfc\u81f4\u7684\u8ba4\u77e5\u8fde\u7eed\u6027\u548c\u4e00\u81f4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u8ffd\u8e2a\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7279\u5f81\u589e\u5f3a\uff0c\u4f46\u5ffd\u7565\u4e86\u8ba4\u77e5\u8868\u793a\u7684\u4e0d\u8db3\u548c\u8868\u8fbe\u80fd\u529b\u7684\u9650\u5236\uff0c\u5c24\u5176\u662f\u975e\u8ba4\u77e5\u56e0\u7d20\uff08\u5982\u731c\u6d4b\u548c\u5931\u8bef\uff09\u7684\u5e72\u6270\uff0c\u5bfc\u81f4\u65e0\u6cd5\u6709\u6548\u6355\u6349\u5b66\u751f\u8ba4\u77e5\u8fc7\u7a0b\u7684\u8fde\u7eed\u6027\u548c\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u5f15\u5165\u9884\u6d4b\u504f\u5dee\u548c\u5efa\u6a21\u6210\u672c\u3002", "method": "\u63d0\u51faCRDP-KT\u6a21\u578b\uff0c\u91c7\u7528\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u6839\u636e\u9898\u76ee\u96be\u5ea6\u548c\u8868\u73b0\u95f4\u9694\u4f18\u5316\u8ba4\u77e5\u8868\u793a\uff0c\u786e\u4fdd\u5176\u4e0e\u5b66\u751f\u8ba4\u77e5\u6a21\u5f0f\u4e00\u81f4\uff1b\u901a\u8fc7\u5206\u533a\u4f18\u5316\u548c\u52a0\u6743\u878d\u5408\u4e8c\u5206\u56fe\u5b66\u4e60\u7684\u5173\u7cfb\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86CRDP-KT\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6a21\u62df\u8ba4\u77e5\u72b6\u6001\u5e76\u63d0\u4f9b\u7cfb\u7edf\u5316\u7684\u8f93\u5165\u7279\u5f81\u3002", "conclusion": "CRDP-KT\u6a21\u578b\u901a\u8fc7\u4f18\u5316\u8ba4\u77e5\u8868\u793a\u548c\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8ba4\u77e5\u8fde\u7eed\u6027\u548c\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u964d\u4f4e\u4e86\u9884\u6d4b\u504f\u5dee\u3002", "keywords": ""}}
{"id": "2506.02205", "pdf": "https://arxiv.org/pdf/2506.02205", "abs": "https://arxiv.org/abs/2506.02205", "authors": ["Yuliang Gu", "Hongpeng Cao", "Marco Caccamo", "Naira Hovakimyan"], "title": "Bregman Centroid Guided Cross-Entropy Method", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "The Cross-Entropy Method (CEM) is a widely adopted trajectory optimizer in\nmodel-based reinforcement learning (MBRL), but its unimodal sampling strategy\noften leads to premature convergence in multimodal landscapes. In this work, we\npropose Bregman Centroid Guided CEM ($\\mathcal{BC}$-EvoCEM), a lightweight\nenhancement to ensemble CEM that leverages $\\textit{Bregman centroids}$ for\nprincipled information aggregation and diversity control.\n$\\textbf{$\\mathcal{BC}$-EvoCEM}$ computes a performance-weighted Bregman\ncentroid across CEM workers and updates the least contributing ones by sampling\nwithin a trust region around the centroid. Leveraging the duality between\nBregman divergences and exponential family distributions, we show that\n$\\textbf{$\\mathcal{BC}$-EvoCEM}$ integrates seamlessly into standard CEM\npipelines with negligible overhead. Empirical results on synthetic benchmarks,\na cluttered navigation task, and full MBRL pipelines demonstrate that\n$\\textbf{$\\mathcal{BC}$-EvoCEM}$ enhances both convergence and solution\nquality, providing a simple yet effective upgrade for CEM.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Bregman Centroid Guided CEM (BC-EvoCEM)\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6539\u8fdb\u7684CEM\u65b9\u6cd5\uff0c\u901a\u8fc7Bregman\u8d28\u5fc3\u8fdb\u884c\u4fe1\u606f\u805a\u5408\u548c\u591a\u6837\u6027\u63a7\u5236\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u73af\u5883\u4e2d\u7684\u6536\u655b\u6027\u548c\u89e3\u7684\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edfCEM\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u5bb9\u6613\u65e9\u719f\u6536\u655b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u63a7\u5236\u591a\u6837\u6027\u5e76\u63d0\u5347\u6027\u80fd\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51faBC-EvoCEM\uff0c\u5229\u7528Bregman\u8d28\u5fc3\u8fdb\u884c\u4fe1\u606f\u805a\u5408\uff0c\u5e76\u901a\u8fc7\u4fe1\u4efb\u533a\u57df\u91c7\u6837\u66f4\u65b0\u8d21\u732e\u8f83\u5c0f\u7684CEM\u5de5\u4f5c\u8005\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBC-EvoCEM\u5728\u5408\u6210\u57fa\u51c6\u3001\u590d\u6742\u5bfc\u822a\u4efb\u52a1\u548c\u5b8c\u6574MBRL\u6d41\u7a0b\u4e2d\u5747\u63d0\u5347\u4e86\u6536\u655b\u6027\u548c\u89e3\u7684\u8d28\u91cf\u3002", "conclusion": "BC-EvoCEM\u662f\u4e00\u79cd\u7b80\u5355\u4f46\u9ad8\u6548\u7684CEM\u6539\u8fdb\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u4f18\u5316\u95ee\u9898\u3002", "keywords": "Cross-Entropy Method, Bregman centroid, model-based reinforcement learning, multimodal optimization"}}
{"id": "2506.02460", "pdf": "https://arxiv.org/pdf/2506.02460", "abs": "https://arxiv.org/abs/2506.02460", "authors": ["Yupeng Qi", "Ziyu Lyu", "Min Yang", "Yanlin Wang", "Lu Bai", "Lixin Cui"], "title": "MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework", "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly applied across various\ndomains, enhancing safety while maintaining the helpfulness of LLMs has become\na critical challenge. Recent studies solve this problem through\nsafety-constrained online preference optimization or safety-constrained offline\npreference optimization. However, the safety-constrained online methods often\nsuffer from excessive safety, which might reduce helpfulness, while the\nsafety-constrained offline methods perform poorly in adaptively balancing\nsafety and helpfulness. To address these limitations, we propose MidPO, a\n\\textbf{\\underline{Mi}}xture of Experts (MoE) framework for safety-helpfulness\n\\textbf{\\underline{d}}ual \\textbf{\\underline{P}}reference\n\\textbf{\\underline{O}}ptimization. Firstly, MidPO devises single-preference\nenhanced direct preference optimization approach to transform the base model\ninto two independent experts, termed safety and helpfulness experts, and\nfine-tunes the two independent experts for optimal safety or helpfulness\nperformance. Secondly, to achieve an effective balance between safety and\nhelpfulness, MidPO incorporates the two experts into the MoE framework and\ndesigns a dynamic routing mechanism to allocate contributions from each expert\nadaptively. We conduct quantitative and qualitative experiments on three\npopular datasets to demonstrate the proposed MidPO significantly outperforms\nstate-of-the-art approaches in both safety and helpfulness. The code and models\nwill be released.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4e13\u5bb6\u6846\u67b6MidPO\uff0c\u901a\u8fc7\u4f18\u5316\u5355\u4e00\u504f\u597d\u548c\u52a8\u6001\u8def\u7531\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u5b89\u5168\u7ea6\u675f\u5728\u7ebf\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u5b89\u5168\uff0c\u800c\u79bb\u7ebf\u65b9\u6cd5\u5728\u81ea\u9002\u5e94\u5e73\u8861\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "MidPO\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6846\u67b6\uff0c\u9996\u5148\u901a\u8fc7\u5355\u4e00\u504f\u597d\u589e\u5f3a\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u8bad\u7ec3\u51fa\u72ec\u7acb\u7684\u2018\u5b89\u5168\u4e13\u5bb6\u2019\u548c\u2018\u5b9e\u7528\u4e13\u5bb6\u2019\uff0c\u7136\u540e\u901a\u8fc7\u52a8\u6001\u8def\u7531\u673a\u5236\u81ea\u9002\u5e94\u5730\u5e73\u8861\u4e24\u8005\u7684\u8d21\u732e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMidPO\u5728\u4e09\u7ec4\u6d41\u884c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u517c\u987e\u4e86\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "MidPO\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\u548c\u52a8\u6001\u8def\u7531\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5b89\u5168\u6027, \u5b9e\u7528\u6027, \u6df7\u5408\u4e13\u5bb6\u6846\u67b6, \u52a8\u6001\u8def\u7531"}}
{"id": "2506.02992", "pdf": "https://arxiv.org/pdf/2506.02992", "abs": "https://arxiv.org/abs/2506.02992", "authors": ["Li Zhang", "Kevin D. Ashley"], "title": "Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation", "categories": ["cs.AI", "cs.CL", "cs.LG", "68T50", "I.2"], "comment": "13 pages, 2 figures, Workshop on Legally Compliant Intelligent\n  Chatbots at ICAIL 2025]{Workshop on Legally Compliant Intelligent Chatbots @\n  ICAIL 2025", "summary": "Large Language Models (LLMs) are increasingly explored for legal argument\ngeneration, yet they pose significant risks of manipulation through\nhallucination and ungrounded persuasion, and often fail to utilize provided\nfactual bases effectively or abstain when arguments are untenable. This paper\nintroduces a novel reflective multi-agent method designed to address these\nchallenges in the context of legally compliant persuasion. Our approach employs\nspecialized agents--a Factor Analyst and an Argument Polisher--in an iterative\nrefinement process to generate 3-ply legal arguments (plaintiff, defendant,\nrebuttal). We evaluate Reflective Multi-Agent against single-agent,\nenhanced-prompt single-agent, and non-reflective multi-agent baselines using\nfour diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,\nLlama-4-Scout-17b-16e) across three legal scenarios: \"arguable\", \"mismatched\",\nand \"non-arguable\". Results demonstrate Reflective Multi-Agent's significant\nsuperiority in successful abstention (preventing generation when arguments\ncannot be grounded), marked improvements in hallucination accuracy (reducing\nfabricated and misattributed factors), particularly in \"non-arguable\"\nscenarios, and enhanced factor utilization recall (improving the use of\nprovided case facts). These findings suggest that structured reflection within\na multi-agent framework offers a robust computable method for fostering ethical\npersuasion and mitigating manipulation in LLM-based legal argumentation\nsystems, a critical step towards trustworthy AI in law. Project page:\nhttps://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6cd5\u5f8b\u8bba\u8bc1\u751f\u6210\u7684\u591a\u4ee3\u7406\u53cd\u601d\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5e7b\u89c9\u548c\u9519\u8bef\u8bba\u8bc1\uff0c\u63d0\u5347\u4e86\u4e8b\u5b9e\u5229\u7528\u7387\u548c\u8bba\u8bc1\u7684\u5408\u89c4\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u8bba\u8bc1\u4e2d\u5b58\u5728\u7684\u5e7b\u89c9\u3001\u9519\u8bef\u8bf4\u670d\u548c\u672a\u80fd\u6709\u6548\u5229\u7528\u4e8b\u5b9e\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cd\u601d\u6027\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u542b\u56e0\u5b50\u5206\u6790\u5e08\u548c\u8bba\u8bc1\u629b\u5149\u5e08\uff0c\u8fed\u4ee3\u751f\u6210\u4e09\u90e8\u5206\u6cd5\u5f8b\u8bba\u8bc1\u3002", "result": "\u5728\u591a\u4ee3\u7406\u6846\u67b6\u4e0b\uff0c\u6a21\u578b\u5728\u907f\u514d\u9519\u8bef\u8bba\u8bc1\u3001\u51cf\u5c11\u5e7b\u89c9\u548c\u63d0\u5347\u4e8b\u5b9e\u5229\u7528\u7387\u65b9\u9762\u8868\u73b0\u663e\u8457\u4f18\u8d8a\u3002", "conclusion": "\u591a\u4ee3\u7406\u53cd\u601d\u6846\u67b6\u4e3a\u6cd5\u5f8b\u8bba\u8bc1\u63d0\u4f9b\u4e86\u53ef\u8ba1\u7b97\u7684\u4f26\u7406\u8bf4\u670d\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86AI\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u53ef\u4fe1\u5ea6\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u6cd5\u5f8b\u8bba\u8bc1, \u591a\u4ee3\u7406\u6846\u67b6, \u53cd\u601d\u65b9\u6cd5, \u4f26\u7406AI"}}
{"id": "2506.02208", "pdf": "https://arxiv.org/pdf/2506.02208", "abs": "https://arxiv.org/abs/2506.02208", "authors": ["Hongling Xu", "Qi Zhu", "Heyuan Deng", "Jinpeng Li", "Lu Hou", "Yasheng Wang", "Lifeng Shang", "Ruifeng Xu", "Fei Mi"], "title": "KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in large language model (LLM) post-training have leveraged\ntwo distinct paradigms to enhance reasoning capabilities: reinforcement\nlearning (RL) and knowledge distillation (KD). While RL enables the emergence\nof complex reasoning behaviors, it often suffers from low sample efficiency\nwhen the initial policy struggles to explore high-reward trajectories.\nConversely, KD improves learning efficiency via mimicking the teacher model but\ntends to generalize poorly to out-of-domain scenarios. In this work, we present\n\\textbf{KDRL}, a \\textit{unified post-training framework} that jointly\noptimizes a reasoning model through teacher supervision (KD) and\nself-exploration (RL). Specifically, KDRL leverages policy gradient\noptimization to simultaneously minimize the reverse Kullback-Leibler divergence\n(RKL) between the student and teacher distributions while maximizing the\nexpected rule-based rewards. We first formulate a unified objective that\nintegrates GRPO and KD, and systematically explore how different KL\napproximations, KL coefficients, and reward-guided KD strategies affect the\noverall post-training dynamics and performance. Empirical results on multiple\nreasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD\nbaselines while achieving a favorable balance between performance and reasoning\ntoken efficiency. These findings indicate that integrating KD and RL serves as\nan effective and efficient strategy to train reasoning LLMs.", "AI": {"tldr": "KDRL\u6846\u67b6\u7ed3\u5408\u4e86\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u901a\u8fc7\u540c\u65f6\u4f18\u5316\u6559\u5e08\u76d1\u7763\u548c\u81ea\u6211\u63a2\u7d22\uff0c\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528RL\u6216KD\u7684\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u514b\u670dRL\u63a2\u7d22\u6548\u7387\u4f4e\u548cKD\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u7f3a\u70b9\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6KDRL\uff0c\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "KDRL\u5229\u7528\u7b56\u7565\u68af\u5ea6\u4f18\u5316\uff0c\u6700\u5c0f\u5316\u5b66\u751f\u4e0e\u6559\u5e08\u5206\u5e03\u4e4b\u95f4\u7684RKL\u6563\u5ea6\uff0c\u540c\u65f6\u6700\u5927\u5316\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u4f18\u5316\u76ee\u6807\u6574\u5408GRPO\u548cKD\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKDRL\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eGRPO\u548cKD\u57fa\u7ebf\uff0c\u5e76\u5728\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002", "conclusion": "\u7ed3\u5408KD\u548cRL\u662f\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u8bad\u7ec3\u63a8\u7406LLM\u7684\u7b56\u7565\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u77e5\u8bc6\u84b8\u998f, \u5f3a\u5316\u5b66\u4e60, \u63a8\u7406\u80fd\u529b, KDRL"}}
{"id": "2506.02461", "pdf": "https://arxiv.org/pdf/2506.02461", "abs": "https://arxiv.org/abs/2506.02461", "authors": ["Chunkit Chan", "Yauwai Yim", "Hongchuan Zeng", "Zhiying Zou", "Xinyuan Cheng", "Zhifan Sun", "Zheye Deng", "Kawai Chung", "Yuzhuo Ao", "Yixiang Fan", "Cheng Jiayang", "Ercong Nie", "Ginny Y. Wong", "Helmut Schmid", "Hinrich Sch\u00fctze", "Simon See", "Yangqiu Song"], "title": "XToM: Exploring the Multilingual Theory of Mind for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Theory of Mind (ToM), the ability to infer mental states in others, is\npivotal for human social cognition. Existing evaluations of ToM in LLMs are\nlargely limited to English, neglecting the linguistic diversity that shapes\nhuman cognition. This limitation raises a critical question: can LLMs exhibit\nMultilingual Theory of Mind, which is the capacity to reason about mental\nstates across diverse linguistic contexts? To address this gap, we present\nXToM, a rigorously validated multilingual benchmark that evaluates ToM across\nfive languages and incorporates diverse, contextually rich task scenarios.\nUsing XToM, we systematically evaluate LLMs (e.g., DeepSeek R1), revealing a\npronounced dissonance: while models excel in multilingual language\nunderstanding, their ToM performance varies across languages. Our findings\nexpose limitations in LLMs' ability to replicate human-like mentalizing across\nlinguistic contexts.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5XToM\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u5728\u591a\u8bed\u8a00\u7406\u89e3\u4e0e\u5fc3\u7406\u7406\u8bba\u8868\u73b0\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u7684\u8bc4\u4f30\u4e3b\u8981\u9650\u4e8e\u82f1\u8bed\uff0c\u5ffd\u89c6\u4e86\u8bed\u8a00\u591a\u6837\u6027\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u7684\u5f71\u54cd\uff0c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86XToM\uff0c\u4e00\u4e2a\u7ecf\u8fc7\u4e25\u683c\u9a8c\u8bc1\u7684\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4e94\u79cd\u8bed\u8a00\uff0c\u5e76\u5305\u542b\u591a\u6837\u5316\u548c\u60c5\u5883\u4e30\u5bcc\u7684\u4efb\u52a1\u573a\u666f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1LLMs\u5728\u591a\u8bed\u8a00\u7406\u89e3\u4e0a\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u7684ToM\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u672a\u80fd\u5b8c\u5168\u590d\u5236\u4eba\u7c7b\u8de8\u8bed\u8a00\u7684\u5fc3\u7406\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u8de8\u8bed\u8a00\u73af\u5883\u4e2d\u6a21\u62df\u4eba\u7c7b\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u672a\u6765\u7814\u7a76\u9700\u5173\u6ce8\u8bed\u8a00\u591a\u6837\u6027\u5bf9\u8ba4\u77e5\u80fd\u529b\u7684\u5f71\u54cd\u3002", "keywords": "Theory of Mind (ToM), \u591a\u8bed\u8a00, \u5fc3\u7406\u7406\u8bba, \u5927\u578b\u8bed\u8a00\u6a21\u578b (LLMs), XToM"}}
{"id": "2506.02996", "pdf": "https://arxiv.org/pdf/2506.02996", "abs": "https://arxiv.org/abs/2506.02996", "authors": ["Matthieu Tehenan", "Christian Bolivar Moya", "Tenghai Long", "Guang Lin"], "title": "Linear Spatial World Models Emerge in Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated emergent abilities across\ndiverse tasks, raising the question of whether they acquire internal world\nmodels. In this work, we investigate whether LLMs implicitly encode linear\nspatial world models, which we define as linear representations of physical\nspace and object configurations. We introduce a formal framework for spatial\nworld models and assess whether such structure emerges in contextual\nembeddings. Using a synthetic dataset of object positions, we train probes to\ndecode object positions and evaluate geometric consistency of the underlying\nspace. We further conduct causal interventions to test whether these spatial\nrepresentations are functionally used by the model. Our results provide\nempirical evidence that LLMs encode linear spatial world models.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u9690\u5f0f\u7f16\u7801\u7ebf\u6027\u7a7a\u95f4\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6d8c\u73b0\u80fd\u529b\uff0c\u7814\u7a76\u5176\u662f\u5426\u5177\u5907\u5185\u90e8\u4e16\u754c\u6a21\u578b\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u7a7a\u95f4\u4e16\u754c\u6a21\u578b\u7684\u6b63\u5f0f\u6846\u67b6\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3\u63a2\u9488\u89e3\u7801\u7269\u4f53\u4f4d\u7f6e\uff0c\u5e76\u8bc4\u4f30\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u8fdb\u884c\u4e86\u56e0\u679c\u5e72\u9884\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u786e\u5b9e\u7f16\u7801\u4e86\u7ebf\u6027\u7a7a\u95f4\u4e16\u754c\u6a21\u578b\u3002", "conclusion": "LLMs\u9690\u5f0f\u5730\u7f16\u7801\u4e86\u7ebf\u6027\u7a7a\u95f4\u4e16\u754c\u6a21\u578b\uff0c\u8fd9\u4e3a\u7406\u89e3\u5176\u5185\u90e8\u673a\u5236\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u4e16\u754c\u6a21\u578b,\u7a7a\u95f4\u8868\u793a,\u56e0\u679c\u5e72\u9884"}}
{"id": "2506.02210", "pdf": "https://arxiv.org/pdf/2506.02210", "abs": "https://arxiv.org/abs/2506.02210", "authors": ["Pu", "Yi", "Tianlang Chen", "Yifan Yang", "Sara Achour"], "title": "Exchangeability in Neural Network Architectures and its Application to Dynamic Pruning", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": null, "summary": "Neural networks (NNs) are equipped with increasingly many parameters and\nrequire more and more resource for deployment. Researchers have explored\nvarious ways to improve the efficiency of NNs by identifying and reducing the\nredundancy, such as pruning or quantizing unimportant weights. Symmetry in the\nNN architectures has been identified by prior work as a possible type of\nredundancy, but exploiting it for efficient inference is not yet explored. In\nthis work, we formalize the symmetry of parameters and intermediate values in\nNNs using the statistical property of exchangeablility. We identify that\nexchangeable values in NN computation may contain overlapping information,\nleading to redundancy. Exploiting the insight, we derive a principled general\ndynamic pruning algorithm ExPrune to remove symmetry-induced redundancy on a\nper-input basis. We also provide an instantiation of ExPrune that performs\nneuron-level dynamic pruning by predicting negative inputs to ReLU activations.\nWe evaluate ExPrune on two computer vision models, one graph model and one\nlanguage model. ExPrune provides 10.98--26.3% reduction in FLOPs with\nnegligible accuracy drop and 21.01--39.05% reduction in FLOPs with at most 1%\naccuracy drop. We also demonstrate that ExPrune composes with static pruning.\nOn models that have been aggressively pruned statically, ExPrune provides\nadditional 10.24--11.11% reduction in FLOPs with negligible accuracy drop and\n13.91--14.39% reduction in FLOPs with at most 1% accuracy drop.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aExPrune\u7684\u52a8\u6001\u526a\u679d\u7b97\u6cd5\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5bf9\u79f0\u6027\u5197\u4f59\uff08exchangeability\uff09\u6765\u9ad8\u6548\u51cf\u5c11\u8ba1\u7b97\u91cf\uff08FLOPs\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u5c0f\u7684\u7cbe\u5ea6\u635f\u5931\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4e0d\u65ad\u589e\u52a0\uff0c\u7814\u7a76\u8005\u63a2\u7d22\u4e86\u591a\u79cd\u65b9\u6cd5\uff08\u5982\u526a\u679d\u6216\u91cf\u5316\uff09\u4ee5\u51cf\u5c11\u5197\u4f59\u3002\u6b64\u524d\u5de5\u4f5c\u53d1\u73b0\u5bf9\u79f0\u6027\u662f\u5197\u4f59\u7684\u4e00\u79cd\u5f62\u5f0f\uff0c\u4f46\u5c1a\u672a\u88ab\u5145\u5206\u5229\u7528\u4ee5\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "method": "\u901a\u8fc7\u7edf\u8ba1\u6027\u8d28\u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u7684\u5bf9\u79f0\u6027\uff08exchangeability\uff09\uff0c\u63d0\u51fa\u52a8\u6001\u526a\u679d\u7b97\u6cd5ExPrune\uff0c\u6839\u636e\u8f93\u5165\u52a8\u6001\u79fb\u9664\u5197\u4f59\u3002\u5177\u4f53\u5b9e\u73b0\u662f\u901a\u8fc7\u9884\u6d4bReLU\u6fc0\u6d3b\u7684\u8d1f\u8f93\u5165\u8fdb\u884c\u795e\u7ecf\u5143\u7ea7\u52a8\u6001\u526a\u679d\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\uff08\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u56fe\u6a21\u578b\u3001\u8bed\u8a00\u6a21\u578b\uff09\u4e2d\uff0cExPrune\u5b9e\u73b0\u4e86FLOPs\u663e\u8457\u51cf\u5c11\uff0810.98-39.05%\uff09\uff0c\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\uff08\u6700\u591a1%\uff09\u3002\u4e0e\u9759\u6001\u526a\u679d\u7ed3\u5408\u8fd8\u53ef\u8fdb\u4e00\u6b65\u51cf\u5c11FLOPs\u3002", "conclusion": "ExPrune\u6709\u6548\u5229\u7528\u4e86\u5bf9\u79f0\u6027\u5197\u4f59\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u63a8\u7406\uff0c\u4e14\u4e0e\u73b0\u6709\u65b9\u6cd5\u517c\u5bb9\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc, \u52a8\u6001\u526a\u679d, \u5197\u4f59, \u5bf9\u79f0\u6027, ExPrune"}}
{"id": "2506.02478", "pdf": "https://arxiv.org/pdf/2506.02478", "abs": "https://arxiv.org/abs/2506.02478", "authors": ["Zijian Li", "Xiaocheng Feng", "Huixin Liu", "Yichong Huang", "Ting Liu", "Bing Qin"], "title": "FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging", "categories": ["cs.CL"], "comment": "12 pages, 11 figures", "summary": "With the development of large language models, fine-tuning has emerged as an\neffective method to enhance performance in specific scenarios by injecting\ndomain-specific knowledge. In this context, model merging techniques provide a\nsolution for fusing knowledge from multiple fine-tuning models by combining\ntheir parameters. However, traditional methods often encounter task\ninterference when merging full fine-tuning models, and this problem becomes\neven more evident in parameter-efficient fine-tuning scenarios. In this paper,\nwe introduce an improvement to the RegMean method, which indirectly leverages\nthe training data to approximate the outputs of the linear layers before and\nafter merging. We propose an adaptive merging method called FroM, which\ndirectly measures the model parameters using the Frobenius norm, without any\ntraining data. By introducing an additional hyperparameter for control, FroM\noutperforms baseline methods across various fine-tuning scenarios, alleviating\nthe task interference problem.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFroM\u7684\u81ea\u9002\u5e94\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u4f7f\u7528Frobenius\u8303\u6570\u8861\u91cf\u6a21\u578b\u53c2\u6570\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u4e2d\u7684\u4efb\u52a1\u5e72\u6270\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5fae\u8c03\u6210\u4e3a\u589e\u5f3a\u7279\u5b9a\u573a\u666f\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u878d\u5408\u5168\u5fae\u8c03\u6a21\u578b\u65f6\u5b58\u5728\u4efb\u52a1\u5e72\u6270\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u573a\u666f\u4e2d\u66f4\u4e3a\u660e\u663e\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u7684\u878d\u5408\u65b9\u6cd5\u3002", "method": "\u6539\u8fdbRegMean\u65b9\u6cd5\uff0c\u63d0\u51faFroM\u65b9\u6cd5\uff0c\u76f4\u63a5\u4f7f\u7528Frobenius\u8303\u6570\u8861\u91cf\u6a21\u578b\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u989d\u5916\u8d85\u53c2\u6570\u63a7\u5236\u878d\u5408\u8fc7\u7a0b\u3002", "result": "FroM\u5728\u591a\u79cd\u5fae\u8c03\u573a\u666f\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u4efb\u52a1\u5e72\u6270\u95ee\u9898\u3002", "conclusion": "FroM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u6570\u636e\u4f9d\u8d56\u7684\u6a21\u578b\u878d\u5408\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u6a21\u578b\u878d\u5408, \u53c2\u6570\u9ad8\u6548\u5fae\u8c03, Frobenius\u8303\u6570, \u4efb\u52a1\u5e72\u6270"}}
{"id": "2506.03032", "pdf": "https://arxiv.org/pdf/2506.03032", "abs": "https://arxiv.org/abs/2506.03032", "authors": ["Junhao Yu", "Yan Zhuang", "YuXuan Sun", "Weibo Gao", "Qi Liu", "Mingyue Cheng", "Zhenya Huang", "Enhong Chen"], "title": "TestAgent: An Adaptive and Intelligent Expert for Human Assessment", "categories": ["cs.AI", "cs.CY", "cs.LG"], "comment": "24 pages,10 figures", "summary": "Accurately assessing internal human states is key to understanding\npreferences, offering personalized services, and identifying challenges in\nreal-world applications. Originating from psychometrics, adaptive testing has\nbecome the mainstream method for human measurement and has now been widely\napplied in education, healthcare, sports, and sociology. It customizes\nassessments by selecting the fewest test questions . However, current adaptive\ntesting methods face several challenges. The mechanized nature of most\nalgorithms leads to guessing behavior and difficulties with open-ended\nquestions. Additionally, subjective assessments suffer from noisy response data\nand coarse-grained test outputs, further limiting their effectiveness. To move\ncloser to an ideal adaptive testing process, we propose TestAgent, a large\nlanguage model (LLM)-powered agent designed to enhance adaptive testing through\ninteractive engagement. This is the first application of LLMs in adaptive\ntesting. TestAgent supports personalized question selection, captures\ntest-takers' responses and anomalies, and provides precise outcomes through\ndynamic, conversational interactions. Experiments on psychological,\neducational, and lifestyle assessments show our approach achieves more accurate\nresults with 20% fewer questions than state-of-the-art baselines, and testers\npreferred it in speed, smoothness, and other dimensions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684TestAgent\uff0c\u7528\u4e8e\u589e\u5f3a\u81ea\u9002\u5e94\u6d4b\u8bd5\u7684\u4ea4\u4e92\u6027\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u65b9\u6cd5\u4e2d\u7684\u673a\u68b0\u5316\u95ee\u9898\u548c\u4e3b\u89c2\u8bc4\u4f30\u7684\u566a\u58f0\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u51c6\u786e\u8bc4\u4f30\u4eba\u7684\u5185\u90e8\u72b6\u6001\u5bf9\u4e8e\u7406\u89e3\u504f\u597d\u3001\u63d0\u4f9b\u4e2a\u6027\u5316\u670d\u52a1\u548c\u8bc6\u522b\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u6311\u6218\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u81ea\u9002\u5e94\u6d4b\u8bd5\u65b9\u6cd5\u5b58\u5728\u673a\u68b0\u5316\u3001\u731c\u6d4b\u884c\u4e3a\u548c\u7c97\u7cd9\u8f93\u51fa\u7b49\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faTestAgent\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u4ea4\u4e92\u5f0f\u81ea\u9002\u5e94\u6d4b\u8bd5\uff0c\u652f\u6301\u4e2a\u6027\u5316\u95ee\u9898\u9009\u62e9\u3001\u6355\u6349\u5f02\u5e38\u54cd\u5e94\u5e76\u63d0\u4f9b\u7cbe\u786e\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTestAgent\u5728\u5fc3\u7406\u3001\u6559\u80b2\u548c\u751f\u6d3b\u65b9\u5f0f\u8bc4\u4f30\u4e2d\uff0c\u4ee520%\u66f4\u5c11\u7684\u95ee\u9898\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u5728\u901f\u5ea6\u548c\u6d41\u7545\u6027\u7b49\u65b9\u9762\u66f4\u53d7\u6d4b\u8bd5\u8005\u9752\u7750\u3002", "conclusion": "TestAgent\u662fLLM\u5728\u81ea\u9002\u5e94\u6d4b\u8bd5\u4e2d\u7684\u9996\u6b21\u5e94\u7528\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u7684\u51c6\u786e\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002", "keywords": "\u81ea\u9002\u5e94\u6d4b\u8bd5,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u4ea4\u4e92\u5f0f\u8bc4\u4f30,\u5fc3\u7406\u6d4b\u91cf,\u4e2a\u6027\u5316\u670d\u52a1"}}
{"id": "2506.02213", "pdf": "https://arxiv.org/pdf/2506.02213", "abs": "https://arxiv.org/abs/2506.02213", "authors": ["Kahn Rhrissorrakrai", "Kathleen E. Hamilton", "Prerana Bangalore Parthsarathy", "Aldo Guzman-Saenz", "Tyler Alban", "Filippo Utro", "Laxmi Parida"], "title": "Quantum Ensembling Methods for Healthcare and Life Science", "categories": ["cs.LG", "q-bio.GN", "q-bio.QM"], "comment": null, "summary": "Learning on small data is a challenge frequently encountered in many\nreal-world applications. In this work we study how effective quantum ensemble\nmodels are when trained on small data problems in healthcare and life sciences.\nWe constructed multiple types of quantum ensembles for binary classification\nusing up to 26 qubits in simulation and 56 qubits on quantum hardware. Our\nensemble designs use minimal trainable parameters but require long-range\nconnections between qubits. We tested these quantum ensembles on synthetic\ndatasets and gene expression data from renal cell carcinoma patients with the\ntask of predicting patient response to immunotherapy. From the performance\nobserved in simulation and initial hardware experiments, we demonstrate how\nquantum embedding structure affects performance and discuss how to extract\ninformative features and build models that can learn and generalize\neffectively. We present these exploratory results in order to assist other\nresearchers in the design of effective learning on small data using ensembles.\nIncorporating quantum computing in these data constrained problems offers hope\nfor a wide range of studies in healthcare and life sciences where biological\nsamples are relatively scarce given the feature space to be explored.", "AI": {"tldr": "\u91cf\u5b50\u96c6\u6210\u6a21\u578b\u5728\u5c0f\u6570\u636e\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u7814\u7a76\uff0c\u91cd\u70b9\u6d4b\u8bd5\u5176\u5728\u533b\u7597\u548c\u751f\u547d\u79d1\u5b66\u4e2d\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u8868\u660e\u91cf\u5b50\u5d4c\u5165\u7ed3\u6784\u5bf9\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u5c0f\u6570\u636e\u5b66\u4e60\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u662f\u4e00\u4e2a\u5e38\u89c1\u6311\u6218\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u91cf\u5b50\u96c6\u6210\u6a21\u578b\u5728\u533b\u7597\u548c\u751f\u547d\u79d1\u5b66\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u6784\u5efa\u4e86\u591a\u79cd\u91cf\u5b50\u96c6\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\uff0c\u6d4b\u8bd5\u4e86\u5408\u6210\u6570\u636e\u96c6\u548c\u80be\u7ec6\u80de\u764c\u60a3\u8005\u7684\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u521d\u6b65\u786c\u4ef6\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u91cf\u5b50\u5d4c\u5165\u7ed3\u6784\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u8ba8\u8bba\u4e86\u5982\u4f55\u63d0\u53d6\u7279\u5f81\u548c\u6784\u5efa\u6709\u6548\u6a21\u578b\u3002", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u5728\u5c0f\u6570\u636e\u95ee\u9898\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u548c\u751f\u547d\u79d1\u5b66\u9886\u57df\u3002", "keywords": "\u5c0f\u6570\u636e\u5b66\u4e60, \u91cf\u5b50\u96c6\u6210\u6a21\u578b, \u533b\u7597, \u751f\u547d\u79d1\u5b66, \u91cf\u5b50\u8ba1\u7b97"}}
{"id": "2506.02480", "pdf": "https://arxiv.org/pdf/2506.02480", "abs": "https://arxiv.org/abs/2506.02480", "authors": ["Yifan Duan", "Yihong Tang", "Kehai Chen", "Liqiang Nie", "Min Zhang"], "title": "ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model Capabilities", "categories": ["cs.CL"], "comment": null, "summary": "High-quality prompts are crucial for eliciting outstanding performance from\nlarge language models (LLMs) on complex tasks. Existing research has explored\nmodel-driven strategies for prompt optimization. However, these methods often\nsuffer from high computational overhead or require strong optimization\ncapabilities from the model itself, which limits their broad applicability.To\naddress these challenges, we propose ORPP (Optimized Role-Playing Prompt),a\nframework that enhances model performance by optimizing and generating\nrole-playing prompts. The core idea of ORPP is to confine the prompt search\nspace to role-playing scenarios, thereby fully activating the model's intrinsic\ncapabilities through carefully crafted, high-quality role-playing prompts.\nSpecifically, ORPP first performs iterative optimization on a small subset of\ntraining samples to generate high-quality role-playing prompts. Then,\nleveraging the model's few-shot learning capability, it transfers the\noptimization experience to efficiently generate suitable prompts for the\nremaining samples.Our experimental results show that ORPP not only matches but\nin most cases surpasses existing mainstream prompt optimization methods in\nterms of performance. Notably, ORPP demonstrates superior \"plug-and-play\"\ncapability. In most cases, it can be integrated with various other prompt\nmethods and further enhance their effectiveness.", "AI": {"tldr": "ORPP\u662f\u4e00\u79cd\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u573a\u666f\u4f18\u5316\u63d0\u793a\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u5177\u6709\u201c\u5373\u63d2\u5373\u7528\u201d\u7279\u6027\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u901a\u5e38\u8ba1\u7b97\u5f00\u9500\u5927\u6216\u4f9d\u8d56\u6a21\u578b\u4f18\u5316\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002ORPP\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u63d0\u793a\u4f18\u5316\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "ORPP\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u5c0f\u89c4\u6a21\u8bad\u7ec3\u6837\u672c\u751f\u6210\u9ad8\u8d28\u91cf\u89d2\u8272\u626e\u6f14\u63d0\u793a\uff0c\u5e76\u5229\u7528\u5c11\u91cf\u6837\u672c\u5b66\u4e60\u80fd\u529b\u63a8\u5e7f\u5230\u5176\u4ed6\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eORPP\u6027\u80fd\u4f18\u4e8e\u4e3b\u6d41\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u4e14\u80fd\u4e0e\u5176\u4ed6\u65b9\u6cd5\u7ed3\u5408\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u679c\u3002", "conclusion": "ORPP\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u63d0\u793a\u4f18\u5316\u6709\u6548\u6fc0\u6d3b\u6a21\u578b\u5185\u5728\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u4efb\u52a1\u3002", "keywords": "\u63d0\u793a\u4f18\u5316,\u89d2\u8272\u626e\u6f14,\u8bed\u8a00\u6a21\u578b,\u5373\u63d2\u5373\u7528"}}
{"id": "2506.03056", "pdf": "https://arxiv.org/pdf/2506.03056", "abs": "https://arxiv.org/abs/2506.03056", "authors": ["Ram Potham", "Max Harms"], "title": "Corrigibility as a Singular Target: A Vision for Inherently Reliable Foundation Models", "categories": ["cs.AI", "cs.CY", "cs.LG"], "comment": "Preprint. This work has been submitted to the Reliable and\n  Responsible Foundation Models Workshop at ICML 2025 for review", "summary": "Foundation models (FMs) face a critical safety challenge: as capabilities\nscale, instrumental convergence drives default trajectories toward loss of\nhuman control, potentially culminating in existential catastrophe. Current\nalignment approaches struggle with value specification complexity and fail to\naddress emergent power-seeking behaviors. We propose \"Corrigibility as a\nSingular Target\" (CAST)-designing FMs whose overriding objective is empowering\ndesignated human principals to guide, correct, and control them. This paradigm\nshift from static value-loading to dynamic human empowerment transforms\ninstrumental drives: self-preservation serves only to maintain the principal's\ncontrol; goal modification becomes facilitating principal guidance. We present\na comprehensive empirical research agenda spanning training methodologies\n(RLAIF, SFT, synthetic data generation), scalability testing across model\nsizes, and demonstrations of controlled instructability. Our vision: FMs that\nbecome increasingly responsive to human guidance as capabilities grow, offering\na path to beneficial AI that remains as tool-like as possible, rather than\nsupplanting human judgment. This addresses the core alignment problem at its\nsource, preventing the default trajectory toward misaligned instrumental\nconvergence.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u7840\u6a21\u578b\u7684\u5b89\u5168\u6311\u6218\uff0c\u63d0\u51fa\u901a\u8fc7\u201c\u53ef\u4fee\u6b63\u6027\u4f5c\u4e3a\u5355\u4e00\u76ee\u6807\u201d\uff08CAST\uff09\u4f7f\u6a21\u578b\u4ee5\u4eba\u7c7b\u63a7\u5236\u4e3a\u6838\u5fc3\uff0c\u907f\u514d\u5931\u63a7\u5bfc\u81f4\u7684\u6f5c\u5728\u707e\u96be\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u5176\u9ed8\u8ba4\u884c\u4e3a\u53ef\u80fd\u5bfc\u81f4\u4eba\u7c7b\u5931\u63a7\uff0c\u5e26\u6765\u5b58\u5728\u6027\u98ce\u9669\uff0c\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u4ef7\u503c\u89c4\u8303\u548c\u7a81\u53d1\u6027\u6743\u529b\u5bfb\u6c42\u884c\u4e3a\u3002", "method": "\u63d0\u51faCAST\u6846\u67b6\uff0c\u8bbe\u8ba1\u6a21\u578b\u4ee5\u8d4b\u80fd\u4eba\u7c7b\u63a7\u5236\u4e3a\u6838\u5fc3\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u65b9\u6cd5\uff08RLAIF\u3001SFT\u7b49\uff09\u3001\u53ef\u6269\u5c55\u6027\u6d4b\u8bd5\u548c\u63a7\u5236\u6307\u4ee4\u6f14\u793a\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "CAST\u6846\u67b6\u80fd\u591f\u5c06\u6a21\u578b\u7684\u5de5\u5177\u6027\u9a71\u52a8\u529b\u8f6c\u5316\u4e3a\u652f\u6301\u4eba\u7c7b\u6307\u5bfc\u7684\u884c\u4e3a\uff0c\u9010\u6b65\u589e\u5f3a\u5bf9\u4eba\u7c7b\u6307\u5f15\u7684\u54cd\u5e94\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u4eba\u7c7b\u8d4b\u80fd\uff0cCAST\u63d0\u4f9b\u4e86\u4e00\u6761\u8def\u5f84\uff0c\u4f7fAI\u59cb\u7ec8\u4f5c\u4e3a\u5de5\u5177\u800c\u975e\u53d6\u4ee3\u4eba\u7c7b\u5224\u65ad\uff0c\u4ece\u6e90\u5934\u4e0a\u89e3\u51b3\u5bf9\u9f50\u95ee\u9898\u3002", "keywords": "\u57fa\u7840\u6a21\u578b, \u53ef\u4fee\u6b63\u6027, \u5b89\u5168\u5bf9\u9f50, \u4eba\u7c7b\u63a7\u5236, \u5de5\u5177\u6027\u9a71\u52a8\u529b"}}
{"id": "2506.02242", "pdf": "https://arxiv.org/pdf/2506.02242", "abs": "https://arxiv.org/abs/2506.02242", "authors": ["Yihong Tang", "Ao Qu", "Xujing Yu", "Weipeng Deng", "Jun Ma", "Jinhua Zhao", "Lijun Sun"], "title": "From Street Views to Urban Science: Discovering Road Safety Factors with Multimodal Large Language Models", "categories": ["cs.LG", "cs.CY"], "comment": null, "summary": "Urban and transportation research has long sought to uncover statistically\nmeaningful relationships between key variables and societal outcomes such as\nroad safety, to generate actionable insights that guide the planning,\ndevelopment, and renewal of urban and transportation systems. However,\ntraditional workflows face several key challenges: (1) reliance on human\nexperts to propose hypotheses, which is time-consuming and prone to\nconfirmation bias; (2) limited interpretability, particularly in deep learning\napproaches; and (3) underutilization of unstructured data that can encode\ncritical urban context. Given these limitations, we propose a Multimodal Large\nLanguage Model (MLLM)-based approach for interpretable hypothesis inference,\nenabling the automated generation, evaluation, and refinement of hypotheses\nconcerning urban context and road safety outcomes. Our method leverages MLLMs\nto craft safety-relevant questions for street view images (SVIs), extract\ninterpretable embeddings from their responses, and apply them in\nregression-based statistical models. UrbanX supports iterative hypothesis\ntesting and refinement, guided by statistical evidence such as coefficient\nsignificance, thereby enabling rigorous scientific discovery of previously\noverlooked correlations between urban design and safety. Experimental\nevaluations on Manhattan street segments demonstrate that our approach\noutperforms pretrained deep learning models while offering full\ninterpretability. Beyond road safety, UrbanX can serve as a general-purpose\nframework for urban scientific discovery, extracting structured insights from\nunstructured urban data across diverse socioeconomic and environmental\noutcomes. This approach enhances model trustworthiness for policy applications\nand establishes a scalable, statistically grounded pathway for interpretable\nknowledge discovery in urban and transportation studies.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u53ef\u89e3\u91ca\u7684\u5047\u8bbe\u63a8\u65ad\uff0c\u81ea\u52a8\u5316\u751f\u6210\u3001\u8bc4\u4f30\u548c\u4f18\u5316\u5173\u4e8e\u57ce\u5e02\u73af\u5883\u548c\u9053\u8def\u5b89\u5168\u7ed3\u679c\u7684\u5047\u8bbe\uff0c\u514b\u670d\u4f20\u7edf\u5de5\u4f5c\u6d41\u7684\u5c40\u9650\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u4e13\u5bb6\u63d0\u51fa\u5047\u8bbe\uff0c\u8017\u65f6\u4e14\u6613\u53d7\u786e\u8ba4\u504f\u8bef\u5f71\u54cd\uff1b\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff1b\u672a\u5145\u5206\u5229\u7528\u975e\u7ed3\u6784\u5316\u6570\u636e\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51faMLLM\u65b9\u6cd5\u3002", "method": "\u5229\u7528MLLM\u4e3a\u8857\u666f\u56fe\u50cf\u751f\u6210\u5b89\u5168\u95ee\u9898\uff0c\u4ece\u5176\u54cd\u5e94\u4e2d\u63d0\u53d6\u53ef\u89e3\u91ca\u5d4c\u5165\uff0c\u5e76\u5e94\u7528\u4e8e\u57fa\u4e8e\u56de\u5f52\u7684\u7edf\u8ba1\u6a21\u578b\uff0c\u652f\u6301\u8fed\u4ee3\u5047\u8bbe\u68c0\u9a8c\u548c\u4f18\u5316\u3002", "result": "\u5728\u66fc\u54c8\u987f\u8857\u533a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u540c\u65f6\u63d0\u4f9b\u5b8c\u5168\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "UrbanX\u53ef\u4f5c\u4e3a\u901a\u7528\u6846\u67b6\uff0c\u4ece\u975e\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u89c1\u89e3\uff0c\u589e\u5f3a\u6a21\u578b\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u57ce\u5e02\u548c\u4ea4\u901a\u7814\u7a76\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u7edf\u8ba1\u57fa\u7840\u7684\u77e5\u8bc6\u53d1\u73b0\u9014\u5f84\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u9053\u8def\u5b89\u5168, \u57ce\u5e02\u8bbe\u8ba1, \u53ef\u89e3\u91ca\u6027, \u5047\u8bbe\u63a8\u65ad"}}
{"id": "2506.02481", "pdf": "https://arxiv.org/pdf/2506.02481", "abs": "https://arxiv.org/abs/2506.02481", "authors": ["Inderjeet Nair", "Lu Wang"], "title": "Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluations of LLMs' ethical risks and value inclinations often rely on\nshort-form surveys and psychometric tests, yet real-world use involves\nlong-form, open-ended responses -- leaving value-related risks and preferences\nin practical settings largely underexplored. In this work, we ask: Do value\npreferences inferred from short-form tests align with those expressed in\nlong-form outputs? To address this question, we compare value preferences\nelicited from short-form reactions and long-form responses, varying the number\nof arguments in the latter to capture users' differing verbosity preferences.\nAnalyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b),\nwe find (1) a weak correlation between value preferences inferred from\nshort-form and long-form responses across varying argument counts, and (2)\nsimilarly weak correlation between preferences derived from any two distinct\nlong-form generation settings. (3) Alignment yields only modest gains in the\nconsistency of value expression. Further, we examine how long-form generation\nattributes relate to value preferences, finding that argument specificity\nnegatively correlates with preference strength, while representation across\nscenarios shows a positive correlation. Our findings underscore the need for\nmore robust methods to ensure consistent value expression across diverse\napplications.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0cLLM\u7684\u4ef7\u503c\u504f\u597d\u4ece\u77ed\u5f62\u5f0f\u6d4b\u8bd5\u4e0e\u957f\u5f62\u5f0f\u56de\u7b54\u4e2d\u63a8\u65ad\u51fa\u7684\u76f8\u5173\u6027\u8f83\u5f31\uff0c\u4e14\u957f\u5f62\u5f0f\u751f\u6210\u8bbe\u7f6e\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u4e5f\u8f83\u4f4e\uff0c\u9700\u8981\u66f4\u5f3a\u7684\u65b9\u6cd5\u786e\u4fdd\u4e00\u81f4\u7684\u4ef7\u503c\u8868\u8fbe\u3002", "motivation": "\u63a2\u7d22LLM\u5728\u77ed\u5f62\u5f0f\u548c\u957f\u5f62\u5f0f\u56de\u7b54\u4e2d\u4ef7\u503c\u504f\u597d\u7684\u4e00\u81f4\u6027\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u4f26\u7406\u98ce\u9669\u548c\u4ef7\u503c\u503e\u5411\u3002", "method": "\u6bd4\u8f83\u4e94\u79cdLLM\uff08llama3-8b\u7b49\uff09\u5728\u77ed\u5f62\u5f0f\u53cd\u5e94\u548c\u957f\u5f62\u5f0f\u56de\u7b54\u4e2d\u7684\u4ef7\u503c\u504f\u597d\uff0c\u5206\u6790\u4e0d\u540c\u751f\u6210\u8bbe\u7f6e\u7684\u76f8\u5173\u6027\u3002", "result": "\u77ed\u5f62\u5f0f\u4e0e\u957f\u5f62\u5f0f\u56de\u7b54\u95f4\u4ef7\u503c\u504f\u597d\u7684\u76f8\u5173\u6027\u5f31\uff0c\u957f\u5f62\u5f0f\u8bbe\u7f6e\u95f4\u4e00\u81f4\u6027\u4f4e\uff0c\u4e14\u4e00\u81f4\u6027\u6539\u5584\u6709\u9650\u3002", "conclusion": "\u9700\u8981\u66f4\u7a33\u5065\u7684\u65b9\u6cd5\u786e\u4fddLLM\u5728\u4e0d\u540c\u5e94\u7528\u4e2d\u4ef7\u503c\u8868\u8fbe\u7684\u4e00\u81f4\u6027\u3002", "keywords": "LLM, \u4ef7\u503c\u504f\u597d, \u77ed\u5f62\u5f0f\u6d4b\u8bd5, \u957f\u5f62\u5f0f\u56de\u7b54, \u4e00\u81f4\u6027"}}
{"id": "2506.03095", "pdf": "https://arxiv.org/pdf/2506.03095", "abs": "https://arxiv.org/abs/2506.03095", "authors": ["Man Luo", "David Cobbley", "Xin Su", "Shachar Rosenman", "Vasudev Lal", "Shao-Yen Tseng", "Phillip Howard"], "title": "DPO Learning with LLMs-Judge Signal for Computer Use Agents", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Computer use agents (CUA) are systems that automatically interact with\ngraphical user interfaces (GUIs) to complete tasks. CUA have made significant\nprogress with the advent of large vision-language models (VLMs). However, these\nagents typically rely on cloud-based inference with substantial compute\ndemands, raising critical privacy and scalability concerns, especially when\noperating on personal devices. In this work, we take a step toward\nprivacy-preserving and resource-efficient agents by developing a lightweight\nvision-language model that runs entirely on local machines. To train this\ncompact agent, we introduce an LLM-as-Judge framework that automatically\nevaluates and filters synthetic interaction trajectories, producing\nhigh-quality data for reinforcement learning without human annotation.\nExperiments on the OS-World benchmark demonstrate that our fine-tuned local\nmodel outperforms existing baselines, highlighting a promising path toward\nprivate, efficient, and generalizable GUI agents.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u53ef\u5728\u672c\u5730\u8fd0\u884c\uff0c\u89e3\u51b3\u4e86\u4e91\u63a8\u7406\u5e26\u6765\u7684\u9690\u79c1\u548c\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u7b5b\u9009\u9ad8\u8d28\u91cf\u6570\u636e\u8bad\u7ec3\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u4e3a\u5e94\u5bf9\u57fa\u4e8e\u4e91\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5b58\u5728\u7684\u9690\u79c1\u548c\u8ba1\u7b97\u8d44\u6e90\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u672c\u5730\u8fd0\u884c\u7684\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165LLM-as-Judge\u6846\u67b6\u81ea\u52a8\u8bc4\u4f30\u548c\u7b5b\u9009\u5408\u6210\u4ea4\u4e92\u8f68\u8ff9\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002", "result": "\u5728OS-World\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u672c\u5730\u5fae\u8c03\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u3001\u9ad8\u6548\u4e14\u901a\u7528\u7684GUI\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u8def\u5f84\u3002", "keywords": "\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406,\u89c6\u89c9\u8bed\u8a00\u6a21\u578b,\u9690\u79c1\u4fdd\u62a4,\u672c\u5730\u8fd0\u884c,LLM-as-Judge"}}
{"id": "2506.02243", "pdf": "https://arxiv.org/pdf/2506.02243", "abs": "https://arxiv.org/abs/2506.02243", "authors": ["Tamara Cucumides", "Floris Geerts"], "title": "From Features to Structure: Task-Aware Graph Construction for Relational and Tabular Learning with GNNs", "categories": ["cs.LG"], "comment": "5 pages, 2 figures", "summary": "Tabular and relational data remain the most ubiquitous formats in real-world\nmachine learning applications, spanning domains from finance to healthcare.\nAlthough both formats offer structured representations, they pose distinct\nchallenges for modern deep learning methods, which typically assume flat,\nfeature-aligned inputs. Graph Neural Networks (GNNs) have emerged as a\npromising solution by capturing structural dependencies within and between\ntables. However, existing GNN-based approaches often rely on rigid,\nschema-derived graphs -- such as those based on primary-foreign key links --\nthereby underutilizing rich, predictive signals in non key attributes. In this\nwork, we introduce auGraph, a unified framework for task-aware graph\naugmentation that applies to both tabular and relational data. auGraph enhances\nbase graph structures by selectively promoting attributes into nodes, guided by\nscoring functions that quantify their relevance to the downstream prediction\ntask. This augmentation preserves the original data schema while injecting\ntask-relevant structural signal. Empirically, auGraph outperforms schema-based\nand heuristic graph construction methods by producing graphs that better\nsupport learning for relational and tabular prediction tasks.", "AI": {"tldr": "auGraph\u662f\u7528\u4e8e\u8868\u683c\u548c\u5173\u7cfb\u6570\u636e\u7684\u4efb\u52a1\u611f\u77e5\u56fe\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u63d0\u5347\u5c5e\u6027\u4e3a\u8282\u70b9\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u8868\u683c\u548c\u5173\u7cfb\u6570\u636e\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u73b0\u6709GNN\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u6a21\u5f0f\u56fe\uff0c\u672a\u5145\u5206\u5229\u7528\u975e\u952e\u5c5e\u6027\u7684\u9884\u6d4b\u4fe1\u53f7\u3002", "method": "auGraph\u901a\u8fc7\u8bc4\u5206\u51fd\u6570\u9009\u62e9\u6027\u5730\u5c06\u5c5e\u6027\u63d0\u5347\u4e3a\u8282\u70b9\uff0c\u589e\u5f3a\u57fa\u7840\u56fe\u7ed3\u6784\uff0c\u4fdd\u7559\u539f\u59cb\u6570\u636e\u6a21\u5f0f\u7684\u540c\u65f6\u6ce8\u5165\u4efb\u52a1\u76f8\u5173\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cauGraph\u4f18\u4e8e\u57fa\u4e8e\u6a21\u5f0f\u548c\u542f\u53d1\u5f0f\u7684\u56fe\u6784\u5efa\u65b9\u6cd5\uff0c\u80fd\u66f4\u597d\u5730\u652f\u6301\u5173\u7cfb\u53ca\u8868\u683c\u9884\u6d4b\u4efb\u52a1\u7684\u5b66\u4e60\u3002", "conclusion": "auGraph\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u6709\u6548\u7684\u56fe\u589e\u5f3a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8868\u683c\u548c\u5173\u7cfb\u6570\u636e\u7684\u5b66\u4e60\u6027\u80fd\u3002", "keywords": "\u8868\u683c\u6570\u636e,\u5173\u7cfb\u6570\u636e,\u56fe\u795e\u7ecf\u7f51\u7edc,\u4efb\u52a1\u611f\u77e5,\u56fe\u589e\u5f3a"}}
{"id": "2506.02483", "pdf": "https://arxiv.org/pdf/2506.02483", "abs": "https://arxiv.org/abs/2506.02483", "authors": ["Sina Bagheri Nezhad", "Ameeta Agrawal"], "title": "Enhancing Large Language Models with Neurosymbolic Reasoning for Multilingual Tasks", "categories": ["cs.CL"], "comment": "Accepted at 19th Conference on Neurosymbolic Learning and Reasoning\n  (NeSy 2025)", "summary": "Large language models (LLMs) often struggle to perform multi-target reasoning\nin long-context scenarios where relevant information is scattered across\nextensive documents. To address this challenge, we introduce NeuroSymbolic\nAugmented Reasoning (NSAR), which combines the benefits of neural and symbolic\nreasoning during inference. NSAR explicitly extracts symbolic facts from text\nand generates executable Python code to handle complex reasoning steps. Through\nextensive experiments across seven languages and diverse context lengths, we\ndemonstrate that NSAR significantly outperforms both a vanilla RAG baseline and\nadvanced prompting strategies in accurately identifying and synthesizing\nmultiple pieces of information. Our results highlight the effectiveness of\ncombining explicit symbolic operations with neural inference for robust,\ninterpretable, and scalable reasoning in multilingual settings.", "AI": {"tldr": "NSAR\u7ed3\u5408\u795e\u7ecf\u4e0e\u7b26\u53f7\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u591a\u76ee\u6807\u63a8\u7406\u80fd\u529b\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u591a\u76ee\u6807\u63a8\u7406\u7684\u56f0\u96be\uff0c\u63d0\u51fa\u7ed3\u5408\u795e\u7ecf\u4e0e\u7b26\u53f7\u63a8\u7406\u7684\u65b9\u6cd5\u3002", "method": "NSAR\u63d0\u53d6\u7b26\u53f7\u4e8b\u5b9e\u5e76\u751f\u6210Python\u4ee3\u7801\u5904\u7406\u590d\u6742\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u5728\u4e03\u79cd\u8bed\u8a00\u548c\u4e0d\u540c\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e2d\uff0cNSAR\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u795e\u7ecf\u4e0e\u7b26\u53f7\u63a8\u7406\u7ed3\u5408\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u3002", "keywords": "NSAR\u3001\u7b26\u53f7\u63a8\u7406\u3001\u591a\u76ee\u6807\u63a8\u7406\u3001\u957f\u4e0a\u4e0b\u6587"}}
{"id": "2506.02255", "pdf": "https://arxiv.org/pdf/2506.02255", "abs": "https://arxiv.org/abs/2506.02255", "authors": ["Asha Ramanujam", "Adam Elyoumi", "Hao Chen", "Sai Madhukiran Kompalli", "Akshdeep Singh Ahluwalia", "Shraman Pal", "Dimitri J. Papageorgiou", "Can Li"], "title": "SafeOR-Gym: A Benchmark Suite for Safe Reinforcement Learning Algorithms on Practical Operations Research Problems", "categories": ["cs.LG"], "comment": null, "summary": "Most existing safe reinforcement learning (RL) benchmarks focus on robotics\nand control tasks, offering limited relevance to high-stakes domains that\ninvolve structured constraints, mixed-integer decisions, and industrial\ncomplexity. This gap hinders the advancement and deployment of safe RL in\ncritical areas such as energy systems, manufacturing, and supply chains. To\naddress this limitation, we present SafeOR-Gym, a benchmark suite of nine\noperations research (OR) environments tailored for safe RL under complex\nconstraints. Each environment captures a realistic planning, scheduling, or\ncontrol problems characterized by cost-based constraint violations, planning\nhorizons, and hybrid discrete-continuous action spaces. The suite integrates\nseamlessly with the Constrained Markov Decision Process (CMDP) interface\nprovided by OmniSafe. We evaluate several state-of-the-art safe RL algorithms\nacross these environments, revealing a wide range of performance: while some\ntasks are tractable, others expose fundamental limitations in current\napproaches. SafeOR-Gym provides a challenging and practical testbed that aims\nto catalyze future research in safe RL for real-world decision-making problems.\nThe SafeOR-Gym framework and all accompanying code are available at:\nhttps://github.com/li-group/SafeOR-Gym.", "AI": {"tldr": "SafeOR-Gym\u662f\u4e00\u4e2a\u9488\u5bf9\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u4e13\u6ce8\u4e8e\u89e3\u51b3\u590d\u6742\u7ea6\u675f\u4e0b\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u586b\u8865\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u80fd\u6e90\u3001\u5236\u9020\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u5b89\u5168RL\u57fa\u51c6\u6d4b\u8bd5\u591a\u96c6\u4e2d\u5728\u673a\u5668\u4eba\u5b66\u548c\u63a7\u5236\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u7ed3\u6784\u5316\u7ea6\u675f\u3001\u6df7\u5408\u6574\u6570\u51b3\u7b56\u548c\u5de5\u4e1a\u590d\u6742\u6027\u7684\u8986\u76d6\uff0c\u9650\u5236\u4e86\u5b89\u5168RL\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e94\u7528\u548c\u53d1\u5c55\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86SafeOR-Gym\uff0c\u5305\u542b\u4e5d\u4e2a\u9488\u5bf9\u590d\u6742\u7ea6\u675f\u7684\u64cd\u4f5c\u7814\u7a76\uff08OR\uff09\u73af\u5883\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u6bcf\u4e2a\u73af\u5883\u6a21\u62df\u4e86\u5177\u6709\u6210\u672c\u7ea6\u675f\u3001\u89c4\u5212\u5468\u671f\u548c\u6df7\u5408\u79bb\u6563-\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u7684\u73b0\u5b9e\u95ee\u9898\u3002", "result": "\u6d4b\u8bd5\u53d1\u73b0\uff0c\u5f53\u524d\u4e00\u4e9b\u5b89\u5168RL\u7b97\u6cd5\u5728\u8fd9\u4e9b\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4e00\uff1a\u90e8\u5206\u4efb\u52a1\u53ef\u89e3\uff0c\u800c\u5176\u4ed6\u4efb\u52a1\u5219\u66b4\u9732\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6839\u672c\u5c40\u9650\u6027\u3002", "conclusion": "SafeOR-Gym\u4e3a\u5b89\u5168RL\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u548c\u5b9e\u7528\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u65e8\u5728\u63a8\u52a8\u89e3\u51b3\u73b0\u5b9e\u51b3\u7b56\u95ee\u9898\u7684\u7814\u7a76\u3002", "keywords": "\u5b89\u5168\u5f3a\u5316\u5b66\u4e60, \u64cd\u4f5c\u7814\u7a76, \u590d\u6742\u7ea6\u675f, \u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2506.02494", "pdf": "https://arxiv.org/pdf/2506.02494", "abs": "https://arxiv.org/abs/2506.02494", "authors": ["Junzhe Zhang", "Huixuan Zhang", "Xinyu Hu", "Li Lin", "Mingqi Gao", "Shi Qiu", "Xiaojun Wan"], "title": "Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Evaluation is important for multimodal generation tasks. With the rapid\nprogress of MLLMs, there is growing interest in applying MLLMs to build general\nevaluation systems. However, existing work overlooks two aspects: (1) the\ndevelopment of evaluation capabilities for text-to-image (T2I) generation task,\nand (2) the incorporation of large-scale human evaluation data. In this paper,\nwe introduce Minos-Corpus, a large-scale multimodal evaluation dataset that\ncombines evaluation data from both human and GPT. The corpus contains\nevaluation data across both image-to-text(I2T) and T2I generation tasks. Based\non this corpus, we propose Data Selection and Balance, Mix-SFT training\nmethods, and apply DPO to develop Minos, a multimodal evaluation model built\nupon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among\nall open-source evaluation models of similar scale on the average of evaluation\nperformance on all tasks, and outperforms all open-source and closed-source\nmodels on evaluation of T2I generation task. Extensive experiments demonstrate\nthe importance of leveraging high-quality human evaluation data and jointly\ntraining on evaluation data from both I2T and T2I generation tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Minos-Corpus\uff0c\u4e00\u4e2a\u7ed3\u5408\u4eba\u7c7b\u548cGPT\u8bc4\u4f30\u6570\u636e\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86Minos\u6a21\u578b\uff0c\u5728T2I\u751f\u6210\u4efb\u52a1\u8bc4\u4f30\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u591a\u6a21\u6001\u751f\u6210\u4efb\u52a1\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u65e5\u76ca\u7a81\u51fa\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u5ffd\u89c6\u4e86T2I\u751f\u6210\u4efb\u52a1\u7684\u8bc4\u4f30\u80fd\u529b\u548c\u5927\u89c4\u6a21\u4eba\u7c7b\u8bc4\u4f30\u6570\u636e\u7684\u6574\u5408\u3002", "method": "\u6784\u5efaMinos-Corpus\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4eba\u7c7b\u548cGPT\u7684\u8bc4\u4f30\u6570\u636e\uff1b\u63d0\u51fa\u6570\u636e\u9009\u62e9\u548c\u5e73\u8861\u3001Mix-SFT\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e94\u7528DPO\u5f00\u53d1Minos\u6a21\u578b\u3002", "result": "Minos\u5728\u7c7b\u4f3c\u89c4\u6a21\u7684\u5f00\u6e90\u8bc4\u4f30\u6a21\u578b\u4e2d\u8fbe\u5230\u6700\u4f73\u5e73\u5747\u6027\u80fd\uff0c\u5728T2I\u751f\u6210\u4efb\u52a1\u8bc4\u4f30\u4e0a\u4f18\u4e8e\u6240\u6709\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u4eba\u7c7b\u8bc4\u4f30\u6570\u636e\u548c\u8054\u5408\u8bad\u7ec3\u5bf9\u63d0\u5347\u8bc4\u4f30\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "keywords": "\u591a\u6a21\u6001\u8bc4\u4f30,T2I\u751f\u6210,\u4eba\u7c7b\u8bc4\u4f30\u6570\u636e,Minos"}}
{"id": "2506.02256", "pdf": "https://arxiv.org/pdf/2506.02256", "abs": "https://arxiv.org/abs/2506.02256", "authors": ["Yi Xiao", "Harshit Sharma", "Sawinder Kaur", "Dessa Bergen-Cico", "Asif Salekin"], "title": "Human Heterogeneity Invariant Stress Sensing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Stress affects physical and mental health, and wearable devices have been\nwidely used to detect daily stress through physiological signals. However,\nthese signals vary due to factors such as individual differences and health\nconditions, making generalizing machine learning models difficult. To address\nthese challenges, we present Human Heterogeneity Invariant Stress Sensing\n(HHISS), a domain generalization approach designed to find consistent patterns\nin stress signals by removing person-specific differences. This helps the model\nperform more accurately across new people, environments, and stress types not\nseen during training. Its novelty lies in proposing a novel technique called\nperson-wise sub-network pruning intersection to focus on shared features across\nindividuals, alongside preventing overfitting by leveraging continuous labels\nwhile training. The study focuses especially on people with opioid use disorder\n(OUD)-a group where stress responses can change dramatically depending on their\ntime of daily medication taking. Since stress often triggers cravings, a model\nthat can adapt well to these changes could support better OUD rehabilitation\nand recovery. We tested HHISS on seven different stress datasets-four of which\nwe collected ourselves and three public ones. Four are from lab setups, one\nfrom a controlled real-world setting, driving, and two are from real-world\nin-the-wild field datasets without any constraints. This is the first study to\nevaluate how well a stress detection model works across such a wide range of\ndata. Results show HHISS consistently outperformed state-of-the-art baseline\nmethods, proving both effective and practical for real-world use. Ablation\nstudies, empirical justifications, and runtime evaluations confirm HHISS's\nfeasibility and scalability for mobile stress sensing in sensitive real-world\napplications.", "AI": {"tldr": "HHISS\u662f\u4e00\u79cd\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d88\u9664\u4e2a\u4f53\u7279\u5f02\u6027\u5dee\u5f02\u6765\u68c0\u6d4b\u538b\u529b\u4fe1\u53f7\u7684\u4e00\u81f4\u6a21\u5f0f\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u65b0\u4eba\u7fa4\u548c\u73af\u5883\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u969c\u788d\u60a3\u8005\u3002", "motivation": "\u538b\u529b\u68c0\u6d4b\u6a21\u578b\u5728\u56e0\u4e2a\u4f53\u5dee\u5f02\u548c\u5065\u5eb7\u6761\u4ef6\u7b49\u56e0\u7d20\u5bfc\u81f4\u7684\u751f\u7406\u4fe1\u53f7\u53d8\u5316\u4e2d\u96be\u4ee5\u6cdb\u5316\uff0cHHISS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faperson-wise\u5b50\u7f51\u7edc\u4fee\u526a\u4ea4\u96c6\u6280\u672f\u548c\u5229\u7528\u8fde\u7eed\u6807\u7b7e\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u4e13\u6ce8\u4e8e\u8de8\u4e2a\u4f53\u7684\u5171\u4eab\u7279\u5f81\u3002", "result": "HHISS\u5728\u4e03\u79cd\u4e0d\u540c\u538b\u529b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\uff0c\u8bc1\u5b9e\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "HHISS\u5728\u654f\u611f\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u53ef\u884c\u6027\u548c\u6269\u5c55\u6027\uff0c\u7279\u522b\u9002\u5408\u79fb\u52a8\u538b\u529b\u68c0\u6d4b\u3002", "keywords": "\u538b\u529b\u68c0\u6d4b,\u9886\u57df\u6cdb\u5316,\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u969c\u788d,\u751f\u7406\u4fe1\u53f7,\u673a\u5668\u5b66\u4e60"}}
{"id": "2506.02503", "pdf": "https://arxiv.org/pdf/2506.02503", "abs": "https://arxiv.org/abs/2506.02503", "authors": ["Yongjian Li", "HaoCheng Chu", "Yukun Yan", "Zhenghao Liu", "Shi Yu", "Zheni Zeng", "Ruobing Wang", "Sen Song", "Zhiyuan Liu", "Maosong Sun"], "title": "KARE-RAG: Knowledge-Aware Refinement and Enhancement for RAG", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to\naccess broader knowledge sources, yet factual inconsistencies persist due to\nnoise in retrieved documents-even with advanced retrieval methods. We\ndemonstrate that enhancing generative models' capacity to process noisy content\nis equally critical for robust performance. In this paper, we present KARE-RAG\n(Knowledge-Aware Refinement and Enhancement for RAG), which improves knowledge\nutilization through three key innovations: (1) structured knowledge\nrepresentations that facilitate error detection during training, (2) Dense\nDirect Preference Optimization (DDPO)-a refined training objective that\nprioritizes correction of critical errors, and (3) a contrastive data\ngeneration pipeline that maintains semantic consistency while rectifying\nfactual inaccuracies. Experiments show our method significantly enhances\nstandard RAG pipelines across model scales, improving both in-domain and\nout-of-domain task performance without compromising general capabilities.\nNotably, these gains are achieved with modest training data, suggesting\ndata-efficient optimization is possible through targeted learning strategies.\nOur findings establish a new direction for RAG improvement: by improving how\nmodels learn to process retrieved content, we can enhance performance across\ndiverse inference paradigms. All data and code will be publicly available on\nGithub.", "AI": {"tldr": "KARE-RAG\u901a\u8fc7\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u3001\u4f18\u5316\u7684\u8bad\u7ec3\u76ee\u6807\u548c\u5bf9\u6bd4\u6570\u636e\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86RAG\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u6548\u7387\u3002", "motivation": "\u89e3\u51b3RAG\u4e2d\u566a\u58f0\u6587\u6863\u5bfc\u81f4\u7684\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u63d0\u5347\u77e5\u8bc6\u5229\u7528\u6548\u7387\u3002", "method": "\u63d0\u51faKARE-RAG\uff0c\u5305\u62ec\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u3001DDPO\u8bad\u7ec3\u76ee\u6807\u548c\u5bf9\u6bd4\u6570\u636e\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u663e\u793aKARE-RAG\u663e\u8457\u63d0\u5347RAG\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u548c\u4efb\u52a1\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u6a21\u578b\u5bf9\u68c0\u7d22\u5185\u5bb9\u7684\u5b66\u4e60\u65b9\u5f0f\uff0c\u53ef\u4ee5\u63d0\u5347RAG\u7684\u5e7f\u6cdb\u6027\u80fd\u3002", "keywords": "RAG, KARE-RAG, DDPO, \u77e5\u8bc6\u8868\u793a, \u5bf9\u6bd4\u6570\u636e\u751f\u6210"}}
{"id": "2506.02269", "pdf": "https://arxiv.org/pdf/2506.02269", "abs": "https://arxiv.org/abs/2506.02269", "authors": ["YuQing Xie", "Tess Smidt"], "title": "A Tale of Two Symmetries: Exploring the Loss Landscape of Equivariant Models", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages, 13 figures", "summary": "Equivariant neural networks have proven to be effective for tasks with known\nunderlying symmetries. However, optimizing equivariant networks can be tricky\nand best training practices are less established than for standard networks. In\nparticular, recent works have found small training benefits from relaxing\nequivariance constraints. This raises the question: do equivariance constraints\nintroduce fundamental obstacles to optimization? Or do they simply require\ndifferent hyperparameter tuning? In this work, we investigate this question\nthrough a theoretical analysis of the loss landscape geometry. We focus on\nnetworks built using permutation representations, which we can view as a subset\nof unconstrained MLPs. Importantly, we show that the parameter symmetries of\nthe unconstrained model has nontrivial effects on the loss landscape of the\nequivariant subspace and under certain conditions can provably prevent learning\nof the global minima. Further, we empirically demonstrate in such cases,\nrelaxing to an unconstrained MLP can sometimes solve the issue. Interestingly,\nthe weights eventually found via relaxation corresponds to a different choice\nof group representation in the hidden layer. From this, we draw 3 key\ntakeaways. (1) Viewing any class of networks in the context of larger\nunconstrained function space can give important insights on loss landscape\nstructure. (2) Within the unconstrained function space, equivariant networks\nform a complicated union of linear hyperplanes, each associated with a specific\nchoice of internal group representation. (3) Effective relaxation of\nequivariance may require not only adding nonequivariant degrees of freedom, but\nalso rethinking the fixed choice of group representations in hidden layers.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e2d\u7684\u56f0\u96be\uff0c\u63d0\u51fa\u901a\u8fc7\u653e\u677e\u7b49\u53d8\u7ea6\u675f\u548c\u8c03\u6574\u5185\u90e8\u7fa4\u8868\u793a\u53ef\u4ee5\u6539\u5584\u4f18\u5316\u6548\u679c\u3002", "motivation": "\u63a2\u7a76\u7b49\u53d8\u7ea6\u675f\u662f\u5426\u5f15\u5165\u4e86\u4f18\u5316\u969c\u788d\uff0c\u662f\u5426\u9700\u8981\u4e0d\u540c\u7684\u8d85\u53c2\u6570\u8c03\u6574\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u635f\u5931\u666f\u89c2\u51e0\u4f55\uff0c\u805a\u7126\u4e8e\u4f7f\u7528\u7f6e\u6362\u8868\u793a\u6784\u5efa\u7684\u7f51\u7edc\uff0c\u5e76\u5c55\u793a\u65e0\u7ea6\u675f\u6a21\u578b\u7684\u53c2\u6570\u5bf9\u79f0\u6027\u5bf9\u7b49\u53d8\u5b50\u7a7a\u95f4\u635f\u5931\u666f\u89c2\u7684\u5f71\u54cd\u3002", "result": "\u8bc1\u660e\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\uff0c\u7b49\u53d8\u7ea6\u675f\u53ef\u80fd\u963b\u6b62\u5168\u5c40\u6700\u5c0f\u503c\u7684\u5bfb\u627e\u5230\uff1b\u901a\u8fc7\u653e\u677e\u7ea6\u675f\u53ef\u4ee5\u89e3\u51b3\u95ee\u9898\u3002", "conclusion": "\u5728\u66f4\u5927\u65e0\u7ea6\u675f\u51fd\u6570\u7a7a\u95f4\u4e0b\u5206\u6790\u7f51\u7edc\u635f\u5931\u666f\u89c2\u7ed3\u6784\uff0c\u8c03\u6574\u5185\u90e8\u7fa4\u8868\u793a\u53ef\u4ee5\u6709\u6548\u653e\u677e\u7b49\u53d8\u7ea6\u675f\u3002", "keywords": "\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc, \u4f18\u5316, \u635f\u5931\u666f\u89c2, \u7fa4\u8868\u793a"}}
{"id": "2506.02510", "pdf": "https://arxiv.org/pdf/2506.02510", "abs": "https://arxiv.org/abs/2506.02510", "authors": ["Jie Zhu", "Junhui Li", "Yalong Wen", "Xiandong Li", "Lifan Guo", "Feng Chen"], "title": "M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL-2025", "summary": "Recent breakthroughs in large language models (LLMs) have led to the\ndevelopment of new benchmarks for evaluating their performance in the financial\ndomain. However, current financial benchmarks often rely on news articles,\nearnings reports, or announcements, making it challenging to capture the\nreal-world dynamics of financial meetings. To address this gap, we propose a\nnovel benchmark called $\\texttt{M$^3$FinMeeting}$, which is a multilingual,\nmulti-sector, and multi-task dataset designed for financial meeting\nunderstanding. First, $\\texttt{M$^3$FinMeeting}$ supports English, Chinese, and\nJapanese, enhancing comprehension of financial discussions in diverse\nlinguistic contexts. Second, it encompasses various industry sectors defined by\nthe Global Industry Classification Standard (GICS), ensuring that the benchmark\nspans a broad range of financial activities. Finally,\n$\\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer\n(QA) pair extraction, and question answering, facilitating a more realistic and\ncomprehensive evaluation of understanding. Experimental results with seven\npopular LLMs reveal that even the most advanced long-context models have\nsignificant room for improvement, demonstrating the effectiveness of\n$\\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting\ncomprehension skills.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91d1\u878d\u4f1a\u8bae\u7406\u89e3\u57fa\u51c6$\texttt{M$^3$FinMeeting}$\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u3001\u591a\u884c\u4e1a\u548c\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u91d1\u878d\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u4f9d\u8d56\u65b0\u95fb\u6587\u7ae0\u6216\u8d22\u62a5\uff0c\u96be\u4ee5\u53cd\u6620\u771f\u5b9e\u91d1\u878d\u4f1a\u8bae\u7684\u52a8\u6001\u7279\u6027\u3002$\texttt{M$^3$FinMeeting}$\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u652f\u6301\u4e2d\u82f1\u65e5\u4e09\u79cd\u8bed\u8a00\u3001\u6db5\u76d6\u591a\u4e2a\u884c\u4e1a\uff08\u57fa\u4e8eGICS\uff09\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6458\u8981\u751f\u6210\u3001\u95ee\u7b54\u5bf9\u63d0\u53d6\u548c\u95ee\u9898\u56de\u7b54\u4e09\u4e2a\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684\u4e0a\u4e0b\u6587\u6a21\u578b\uff0c\u5728\u91d1\u878d\u4f1a\u8bae\u7406\u89e3\u4efb\u52a1\u4e2d\u4ecd\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "$\texttt{M$^3$FinMeeting}$\u662f\u4e00\u4e2a\u6709\u6548\u4e14\u5168\u9762\u7684\u91d1\u878d\u4f1a\u8bae\u7406\u89e3\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u8868\u73b0\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91d1\u878d\u4f1a\u8bae\uff0c\u591a\u8bed\u8a00\uff0c\u591a\u4efb\u52a1\uff0c\u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2506.02276", "pdf": "https://arxiv.org/pdf/2506.02276", "abs": "https://arxiv.org/abs/2506.02276", "authors": ["Saurabh Singh", "Dmitry Lagun"], "title": "Latent Stochastic Interpolants", "categories": ["cs.LG", "stat.ML"], "comment": "Under Review", "summary": "Stochastic Interpolants (SI) are a powerful framework for generative\nmodeling, capable of flexibly transforming between two probability\ndistributions. However, their use in jointly optimized latent variable models\nremains unexplored as they require direct access to the samples from the two\ndistributions. This work presents Latent Stochastic Interpolants (LSI) enabling\njoint learning in a latent space with end-to-end optimized encoder, decoder and\nlatent SI models. We achieve this by developing a principled Evidence Lower\nBound (ELBO) objective derived directly in continuous time. The joint\noptimization allows LSI to learn effective latent representations along with a\ngenerative process that transforms an arbitrary prior distribution into the\nencoder-defined aggregated posterior. LSI sidesteps the simple priors of the\nnormal diffusion models and mitigates the computational demands of applying SI\ndirectly in high-dimensional observation spaces, while preserving the\ngenerative flexibility of the SI framework. We demonstrate the efficacy of LSI\nthrough comprehensive experiments on the standard large scale ImageNet\ngeneration benchmark.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLatent Stochastic Interpolants (LSI)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8054\u5408\u4f18\u5316\u7f16\u7801\u5668\u3001\u89e3\u7801\u5668\u548c\u6f5c\u5728SI\u6a21\u578b\uff0c\u514b\u670d\u4e86Stochastic Interpolants (SI)\u6846\u67b6\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u76f4\u63a5\u5e94\u7528\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3Stochastic Interpolants (SI)\u6846\u67b6\u5728\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\u4e2d\u8054\u5408\u4f18\u5316\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u9ad8\u7ef4\u89c2\u6d4b\u7a7a\u95f4\u76f4\u63a5\u5e94\u7528SI\u7684\u8ba1\u7b97\u9700\u6c42\u3002", "method": "\u63d0\u51faLSI\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f00\u53d1\u8fde\u7eed\u65f6\u95f4\u4e0b\u7684Evidence Lower Bound (ELBO)\u76ee\u6807\uff0c\u5b9e\u73b0\u7f16\u7801\u5668\u3001\u89e3\u7801\u5668\u548c\u6f5c\u5728SI\u6a21\u578b\u7684\u8054\u5408\u4f18\u5316\u3002", "result": "LSI\u80fd\u591f\u5b66\u4e60\u6709\u6548\u7684\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u4fdd\u7559SI\u6846\u67b6\u7684\u751f\u6210\u7075\u6d3b\u6027\uff0c\u4e14\u5728ImageNet\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LSI\u4e3a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "keywords": "Stochastic Interpolants, Latent Space, Generative Modeling, ELBO, ImageNet"}}
{"id": "2506.02515", "pdf": "https://arxiv.org/pdf/2506.02515", "abs": "https://arxiv.org/abs/2506.02515", "authors": ["Zhuohan Xie", "Dhruv Sahnan", "Debopriyo Banerjee", "Georgi Georgiev", "Rushil Thareja", "Hachem Madmoun", "Jinyan Su", "Aaryamonvikram Singh", "Yuxia Wang", "Rui Xing", "Fajri Koto", "Haonan Li", "Ivan Koychev", "Tanmoy Chakraborty", "Salem Lahlou", "Veselin Stoyanov", "Preslav Nakov"], "title": "FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 8 figures, 2 tables", "summary": "Multi-step symbolic reasoning is critical for advancing downstream\nperformance on financial tasks. Yet, benchmarks for systematically evaluating\nthis capability are lacking. Existing datasets like FinQA and ConvFinQA\nsupervise only final numerical answers, without assessing intermediate\nreasoning steps. To address this, we introduce FinChain, the first symbolic\nbenchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning.\nSpanning 54 topics across 12 financial domains, Fin- Chain offers five\nparameterized templates per topic, each varying in reasoning complexity and\ndomain expertise required. Each dataset instance includes an executable Python\ntrace, enabling automatic generation of extensive training data and easy\nadaptation to other domains. We also introduce ChainEval, a new metric for\nautomatic evaluation of both final answers and intermediate reasoning.\nBenchmarking 30 LLMs on our dataset, we find that even state-of-the-art models\nhave considerable room for improvement in multi-step financial reasoning. All\ntemplates and evaluation metrics for FinChain are available at https:\n//github.com/mbzuai-nlp/finchain.", "AI": {"tldr": "FinChain\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u53ef\u9a8c\u8bc1\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u91d1\u878d\u63a8\u7406\u7684\u7b26\u53f7\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d612\u4e2a\u91d1\u878d\u9886\u57df\u768454\u4e2a\u4e3b\u9898\u3002\u5b83\u63d0\u4f9b\u81ea\u52a8\u751f\u6210\u7684Python\u6267\u884c\u8f68\u8ff9\u548c\u65b0\u7684\u8bc4\u4f30\u6307\u6807ChainEval\uff0c\u7528\u4e8e\u8bc4\u4f30\u6700\u7ec8\u7b54\u6848\u548c\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u3002", "motivation": "\u73b0\u6709\u91d1\u878d\u63a8\u7406\u6570\u636e\u96c6\uff08\u5982FinQA\u548cConvFinQA\uff09\u4ec5\u76d1\u7763\u6700\u7ec8\u6570\u503c\u7b54\u6848\uff0c\u7f3a\u4e4f\u5bf9\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "FinChain\u901a\u8fc7\u4e94\u4e2a\u53c2\u6570\u5316\u6a21\u677f\u751f\u6210\u591a\u6837\u5316\u7684\u63a8\u7406\u590d\u6742\u5ea6\u7684\u5b9e\u4f8b\uff0c\u5305\u62ec\u53ef\u6267\u884c\u7684Python\u8f68\u8ff9\u3002\u540c\u65f6\u63d0\u51faChainEval\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u6d4b\u8bd530\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u591a\u6b65\u91d1\u878d\u63a8\u7406\u4e0a\u4ecd\u6709\u8f83\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "FinChain\u586b\u8865\u4e86\u91d1\u878d\u63a8\u7406\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u548c\u6570\u636e\u652f\u6301\u3002", "keywords": "\u91d1\u878d\u63a8\u7406, Chain-of-Thought, \u57fa\u51c6\u6d4b\u8bd5, FinChain, ChainEval"}}
{"id": "2506.02281", "pdf": "https://arxiv.org/pdf/2506.02281", "abs": "https://arxiv.org/abs/2506.02281", "authors": ["Qinsi Wang", "Jinghan Ke", "Hancheng Ye", "Yueqian Lin", "Yuzhe Fu", "Jianyi Zhang", "Kurt Keutzer", "Chenfeng Xu", "Yiran Chen"], "title": "Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's Own Signals", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models\n(LLMs) suffer from sample inefficiency due to the redundant exposure of\nidentical queries under uniform data sampling. While previous work has explored\ncurriculum learning via heuristic difficulty metrics, these strategies exhibit\nlimitations by neglecting the intrinsic learning signals generated by the model\nitself, thus leading to suboptimal training regimes. In this paper, we identify\na model-inherent signal termed angle concentration that effectively reflects an\nLLM's capacity to learn from specific data. We theoretically and empirically\ndemonstrate a correlation between the angular distribution of token hidden\nstate vectors and the resulting gradient, revealing a learning preference for\ndata exhibiting higher angle concentration. Inspired by this finding, we\npropose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By\nleveraging the model's intrinsic angle concentration signal, GAIN-RL\ndynamically selects training data in each epoch, ensuring consistently\nimpactful gradient updates and thus significantly enhancing overall training\nefficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x\nacceleration in training efficiency across diverse mathematical and coding\ntasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient\nsampling yields data-efficient training, achieving better performance with half\nthe original data compared to vanilla GRPO with full training data. Code is\nrealsed at https://github.com/wangqinsi1/GAINRL/tree/main.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u5185\u5728\u89d2\u5ea6\u4fe1\u53f7\u7684\u52a8\u6001\u6570\u636e\u9009\u62e9\u6846\u67b6GAIN-RL\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5fae\u8c03\u4e2d\u7684\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\u5728\u6570\u636e\u91c7\u6837\u4e2d\u5b58\u5728\u5197\u4f59\u95ee\u9898\uff0c\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u667a\u80fd\u7684\u91c7\u6837\u7b56\u7565\u3002", "method": "\u5229\u7528\u6a21\u578b\u56fa\u6709\u7684\u89d2\u5ea6\u96c6\u4e2d\u4fe1\u53f7\uff08angle concentration\uff09\uff0c\u8bbe\u8ba1\u52a8\u6001\u6570\u636e\u9009\u62e9\u6846\u67b6GAIN-RL\uff0c\u786e\u4fdd\u68af\u5ea6\u66f4\u65b0\u66f4\u5177\u5f71\u54cd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGAIN-RL\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u4efb\u52a1\u4e2d\u63d0\u5347\u4e862.5\u500d\u8bad\u7ec3\u6548\u7387\uff0c\u4e14\u7528\u4e00\u534a\u6570\u636e\u5373\u53ef\u8fbe\u5230\u66f4\u597d\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u6a21\u578b\u5185\u5728\u5b66\u4e60\u4fe1\u53f7\u4f18\u5316\u6570\u636e\u9009\u62e9\uff0cGAIN-RL\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3aLLM\u5fae\u8c03\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u6570\u636e\u91c7\u6837,\u8bad\u7ec3\u6548\u7387,\u89d2\u5ea6\u96c6\u4e2d"}}
{"id": "2506.02519", "pdf": "https://arxiv.org/pdf/2506.02519", "abs": "https://arxiv.org/abs/2506.02519", "authors": ["Sohan Patnaik", "Milan Aggarwal", "Sumit Bhatia", "Balaji Krishnamurthy"], "title": "Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning", "categories": ["cs.CL"], "comment": "Accepted at ACL Main 2025", "summary": "LLMssuch as GPT-4 have shown a remarkable ability to solve complex questions\nby generating step-by-step rationales. Prior works have utilized this\ncapability to improve smaller and cheaper LMs (say, with 7B parameters).\nHowever, various practical constraints, such as copyright and legal issues,\nowing to lack of transparency in the pre-training data of large (often closed)\nmodels, prevent their use in commercial settings. Little focus has been given\nto improving the innate reasoning ability of smaller models without distilling\ninformation from larger LLMs. To address this, we propose COLLATE, a trainable\nframework that tunes a (small) LLM to generate those outputs from a pool of\ndiverse rationales that selectively improves the downstream task. COLLATE\nenforces multiple instances of the same LLM to exhibit distinct behavior and\nemploys them to generate rationales to obtain diverse outputs. The LLM is then\ntuned via preference optimization to choose the candidate rationale which\nmaximizes the likelihood of ground-truth answer. COLLATE outperforms several\ntrainable and prompting baselines on 5 datasets across 3 domains: maths problem\nsolving, natural language inference, and commonsense reasoning. We show the eff\nicacy of COLLATE on LLMs from different model families across varying parameter\nscales (1B to 8B) and demonstrate the benefit of multiple rationale providers\nguided by the end task through ablations. Code is released here\n(https://github.com/Sohanpatnaik106/collate).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCOLLATE\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u5c0f\u578bLLM\u4ece\u591a\u6837\u5316\u7406\u6027\u8f93\u51fa\u4e2d\u9009\u62e9\u6027\u5730\u4f18\u5316\u4e0b\u6e38\u4efb\u52a1\uff0c\u800c\u4e0d\u4f9d\u8d56\u5927\u578bLLM\u7684\u77e5\u8bc6\u84b8\u998f\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u591a\u5229\u7528\u5927\u578bLLM\uff08\u5982GPT-4\uff09\u7684\u63a8\u7406\u80fd\u529b\u6539\u8fdb\u5c0f\u578b\u6a21\u578b\uff0c\u4f46\u56e0\u7248\u6743\u548c\u6cd5\u5f8b\u95ee\u9898\u96be\u4ee5\u5546\u7528\u3002\u672c\u6587\u63a2\u7d22\u5982\u4f55\u4e0d\u4f9d\u8d56\u5927\u578b\u6a21\u578b\uff0c\u76f4\u63a5\u63d0\u5347\u5c0f\u578b\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "COLLATE\u901a\u8fc7\u8bad\u7ec3\u5c0f\u578bLLM\u751f\u6210\u591a\u6837\u5316\u7406\u6027\u8f93\u51fa\uff0c\u5e76\u4f18\u5316\u5176\u9009\u62e9\u6700\u4f73\u7406\u6027\u4ee5\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\uff0c\u91c7\u7528\u504f\u597d\u4f18\u5316\u63d0\u5347\u6a21\u578b\u9009\u62e9\u80fd\u529b\u3002", "result": "\u5728\u6570\u5b66\u95ee\u9898\u6c42\u89e3\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u548c\u5e38\u8bc6\u63a8\u7406\u7b493\u4e2a\u9886\u57df\u76845\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cCOLLATE\u4f18\u4e8e\u591a\u79cd\u57fa\u51c6\u65b9\u6cd5\uff0c\u4e14\u9002\u7528\u4e8e1B\u52308B\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\u3002", "conclusion": "COLLATE\u8bc1\u660e\u4e86\u901a\u8fc7\u591a\u6837\u5316\u7406\u6027\u63d0\u4f9b\u8005\u548c\u4efb\u52a1\u5bfc\u5411\u4f18\u5316\uff0c\u5c0f\u578bLLM\u7684\u63a8\u7406\u80fd\u529b\u53ef\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u907f\u514d\u4f9d\u8d56\u5927\u578b\u6a21\u578b\u3002", "keywords": "\u5c0f\u578bLLM, \u63a8\u7406\u80fd\u529b, \u591a\u6837\u5316\u7406\u6027, \u504f\u597d\u4f18\u5316, COLLATE"}}
{"id": "2506.02285", "pdf": "https://arxiv.org/pdf/2506.02285", "abs": "https://arxiv.org/abs/2506.02285", "authors": ["Aaron Defazio"], "title": "Why Gradients Rapidly Increase Near the End of Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "During long-duration Large Language Model (LLM) training runs the gradient\nnorm increases rapidly near the end of training. In this short note, we show\nthat this increase is due to an unintended interaction between weight decay,\nnormalization layers, and the learning rate schedule. We propose a simple\ncorrection that fixes this behavior while also resulting in lower loss values\nthroughout training.", "AI": {"tldr": "\u957f\u65f6LLM\u8bad\u7ec3\u4e2d\u68af\u5ea6\u8303\u6570\u5728\u8bad\u7ec3\u672b\u671f\u5feb\u901f\u589e\u52a0\uff0c\u539f\u56e0\u662f\u6743\u91cd\u8870\u51cf\u3001\u5f52\u4e00\u5316\u5c42\u548c\u5b66\u4e60\u7387\u8c03\u5ea6\u7684\u610f\u5916\u4ea4\u4e92\u3002\u63d0\u51fa\u7b80\u5355\u4fee\u6b63\u65b9\u6cd5\uff0c\u6539\u5584\u884c\u4e3a\u5e76\u964d\u4f4e\u8bad\u7ec3\u635f\u5931\u3002", "motivation": "\u7814\u7a76LLM\u957f\u65f6\u95f4\u8bad\u7ec3\u4e2d\u68af\u5ea6\u8303\u6570\u5f02\u5e38\u589e\u52a0\u7684\u73b0\u8c61\u53ca\u5176\u539f\u56e0\u3002", "method": "\u5206\u6790\u6743\u91cd\u8870\u51cf\u3001\u5f52\u4e00\u5316\u5c42\u4e0e\u5b66\u4e60\u7387\u8c03\u5ea6\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u63d0\u51fa\u4fee\u6b63\u65b9\u6848\u3002", "result": "\u4fee\u6b63\u65b9\u6cd5\u6210\u529f\u6291\u5236\u68af\u5ea6\u8303\u6570\u5f02\u5e38\u589e\u52a0\uff0c\u5e76\u964d\u4f4e\u8bad\u7ec3\u635f\u5931\u3002", "conclusion": "\u4fee\u6b63\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u8bad\u7ec3\u672b\u671f\u7684\u68af\u5ea6\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "keywords": "LLM\u8bad\u7ec3, \u68af\u5ea6\u8303\u6570, \u6743\u91cd\u8870\u51cf, \u5f52\u4e00\u5316\u5c42, \u5b66\u4e60\u7387\u8c03\u5ea6"}}
{"id": "2506.02527", "pdf": "https://arxiv.org/pdf/2506.02527", "abs": "https://arxiv.org/abs/2506.02527", "authors": ["Yingying Zhuang", "Aman Gupta", "Anurag Beniwal"], "title": "Multilingual Information Retrieval with a Monolingual Knowledge Base", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "6 pages, accepted at GENNEXT@SIGIR25", "summary": "Multilingual information retrieval has emerged as powerful tools for\nexpanding knowledge sharing across languages. On the other hand, resources on\nhigh quality knowledge base are often scarce and in limited languages,\ntherefore an effective embedding model to transform sentences from different\nlanguages into a feature vector space same as the knowledge base language\nbecomes the key ingredient for cross language knowledge sharing, especially to\ntransfer knowledge available in high-resource languages to low-resource ones.\nIn this paper we propose a novel strategy to fine-tune multilingual embedding\nmodels with weighted sampling for contrastive learning, enabling multilingual\ninformation retrieval with a monolingual knowledge base. We demonstrate that\nthe weighted sampling strategy produces performance gains compared to standard\nones by up to 31.03\\% in MRR and up to 33.98\\% in Recall@3. Additionally, our\nproposed methodology is language agnostic and applicable for both multilingual\nand code switching use cases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a0\u6743\u91c7\u6837\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5fae\u8c03\u591a\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u591a\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9ad8\u8d28\u91cf\u77e5\u8bc6\u5e93\u8d44\u6e90\u7a00\u7f3a\u4e14\u8bed\u8a00\u6709\u9650\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u8de8\u8bed\u8a00\u77e5\u8bc6\u5171\u4eab\u3002", "method": "\u91c7\u7528\u52a0\u6743\u91c7\u6837\u7b56\u7565\uff0c\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\u4ee5\u5fae\u8c03\u591a\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\u3002", "result": "\u76f8\u6bd4\u6807\u51c6\u65b9\u6cd5\uff0c\u5728MRR\u548cRecall@3\u4e0a\u5206\u522b\u63d0\u534731.03%\u548c33.98%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bed\u8a00\u65e0\u5173\uff0c\u9002\u7528\u4e8e\u591a\u8bed\u8a00\u548c\u4ee3\u7801\u6df7\u5408\u573a\u666f\u3002", "keywords": "\u591a\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22, \u5bf9\u6bd4\u5b66\u4e60, \u52a0\u6743\u91c7\u6837, \u8de8\u8bed\u8a00\u77e5\u8bc6\u5171\u4eab"}}
{"id": "2506.01969", "pdf": "https://arxiv.org/pdf/2506.01969", "abs": "https://arxiv.org/abs/2506.01969", "authors": ["Pencuo Zeren", "Qiuming Luo", "Rui Mao", "Chang Kong"], "title": "FlashMLA-ETAP: Efficient Transpose Attention Pipeline for Accelerating MLA Inference on NVIDIA H20 GPUs", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": "15 pages, conference", "summary": "Efficient inference of Multi-Head Latent Attention (MLA) is challenged by\ndeploying the DeepSeek-R1 671B model on a single Multi-GPU server. This paper\nintroduces FlashMLA-ETAP, a novel framework that enhances MLA inference for the\nsingle-instance deployment scenario on NVIDIA H20 GPUs. We propose the\nEfficient Transpose Attention Pipeline (ETAP), which reconfigures attention\ncomputation through transposition to align the KV context length with the\n\\(M\\)-dimension in WGMMA operations, significantly reducing redundant\ncomputations. FlashMLA-ETAP achieves a 2.78x speedup over FlashMLA at 64K\nsequence length (batch size 16), with 5.24x and 4.94x improvements over\nFlashAttention-3 and FlashInfer, respectively, while maintaining numerical\nstability with a 15.2x lower RMSE (\\(1.25 \\times 10^{-5}\\)) than\nFlashAttention-3. Furthermore, ETAP's design enables seamless integration into\nframeworks like FlashAttention-3 and FlashInfer, supported by a detailed\ntheoretical analysis. Our work addresses a critical gap in resource-constrained\ninference, offering a scalable solution for mid-tier GPUs and paving the way\nfor broader adoption in hardware-aware optimization. Code is available at\nhttps://github.com/pengcuo/FlashMLA-ETAP.", "AI": {"tldr": "FlashMLA-ETAP\u901a\u8fc7ETAP\u6280\u672f\u663e\u8457\u63d0\u5347\u5355\u5b9e\u4f8b\u90e8\u7f72\u4e0b\u7684Multi-Head Latent Attention\u63a8\u7406\u6548\u7387\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb2.78-5.24\u500d\uff0c\u5e76\u4fdd\u6301\u6570\u503c\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u5355\u53f0\u591aGPU\u670d\u52a1\u5668\u4e0a\u90e8\u7f72\u5927\u89c4\u6a21\u6a21\u578b\uff08\u5982DeepSeek-R1 671B\uff09\u65f6\uff0cMLA\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faEfficient Transpose Attention Pipeline\uff08ETAP\uff09\uff0c\u901a\u8fc7\u91cd\u65b0\u914d\u7f6e\u6ce8\u610f\u529b\u8ba1\u7b97\u4ee5\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u3002", "result": "\u572864K\u5e8f\u5217\u957f\u5ea6\u4e0b\uff0c\u901f\u5ea6\u63d0\u53472.78\u500d\uff08\u76f8\u6bd4FlashMLA\uff09\uff0cRMSE\u4f4e15.2\u500d\uff0c\u6570\u503c\u7a33\u5b9a\u6027\u9ad8\u3002", "conclusion": "ETAP\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4e2d\u7aefGPU\u3002", "keywords": "FlashMLA-ETAP, Efficient Transpose Attention Pipeline, MLA inference, numerical stability, NVIDIA H20 GPUs"}}
{"id": "2506.02293", "pdf": "https://arxiv.org/pdf/2506.02293", "abs": "https://arxiv.org/abs/2506.02293", "authors": ["Marco Pacini", "Gabriele Santin", "Bruno Lepri", "Shubhendu Trivedi"], "title": "On Universality Classes of Equivariant Networks", "categories": ["cs.LG"], "comment": "Preprint. Under review. 22 pages", "summary": "Equivariant neural networks provide a principled framework for incorporating\nsymmetry into learning architectures and have been extensively analyzed through\nthe lens of their separation power, that is, the ability to distinguish inputs\nmodulo symmetry. This notion plays a central role in settings such as graph\nlearning, where it is often formalized via the Weisfeiler-Leman hierarchy. In\ncontrast, the universality of equivariant models-their capacity to approximate\ntarget functions-remains comparatively underexplored. In this work, we\ninvestigate the approximation power of equivariant neural networks beyond\nseparation constraints. We show that separation power does not fully capture\nexpressivity: models with identical separation power may differ in their\napproximation ability. To demonstrate this, we characterize the universality\nclasses of shallow invariant networks, providing a general framework for\nunderstanding which functions these architectures can approximate. Since\nequivariant models reduce to invariant ones under projection, this analysis\nyields sufficient conditions under which shallow equivariant networks fail to\nbe universal. Conversely, we identify settings where shallow models do achieve\nseparation-constrained universality. These positive results, however, depend\ncritically on structural properties of the symmetry group, such as the\nexistence of adequate normal subgroups, which may not hold in important cases\nlike permutation symmetry.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc\u5728\u5bf9\u79f0\u6027\u5b66\u4e60\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5206\u79bb\u80fd\u529b\u5e76\u4e0d\u80fd\u5b8c\u5168\u6355\u6349\u5176\u903c\u8fd1\u80fd\u529b\uff0c\u5e76\u63a2\u8ba8\u4e86\u6d45\u5c42\u7b49\u53d8\u7f51\u7edc\u5728\u54ea\u4e9b\u60c5\u51b5\u4e0b\u80fd\u5b9e\u73b0\u666e\u904d\u903c\u8fd1\u3002", "motivation": "\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc\u5e7f\u6cdb\u7528\u4e8e\u5bf9\u79f0\u6027\u5b66\u4e60\uff0c\u4f46\u5bf9\u5176\u903c\u8fd1\u80fd\u529b\u7684\u7814\u7a76\u8f83\u4e3a\u6709\u9650\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6d45\u5c42\u4e0d\u53d8\u7f51\u7edc\u7684\u901a\u7528\u6027\u7c7b\u522b\uff0c\u7814\u7a76\u7b49\u53d8\u7f51\u7edc\u7684\u903c\u8fd1\u80fd\u529b\u4e0e\u5206\u79bb\u80fd\u529b\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5206\u79bb\u80fd\u529b\u4e0d\u8db3\u4ee5\u4fdd\u8bc1\u666e\u904d\u903c\u8fd1\uff0c\u800c\u6d45\u5c42\u7f51\u7edc\u5728\u67d0\u4e9b\u7ed3\u6784\u6761\u4ef6\u4e0b\u53ef\u4ee5\u5b9e\u73b0\u53d7\u9650\u7684\u666e\u9002\u6027\u3002", "conclusion": "\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc\u7684\u903c\u8fd1\u80fd\u529b\u4f9d\u8d56\u4e8e\u5bf9\u79f0\u7fa4\u7684\u7ed3\u6784\u7279\u6027\uff0c\u5982\u5b58\u5728\u9002\u5f53\u7684\u6b63\u89c4\u5b50\u7fa4\uff0c\u8fd9\u5728\u67d0\u4e9b\u5173\u952e\u60c5\u51b5\u4e0b\uff08\u5982\u7f6e\u6362\u5bf9\u79f0\u6027\uff09\u53ef\u80fd\u4e0d\u6210\u7acb\u3002", "keywords": "\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc, \u5bf9\u79f0\u6027\u5b66\u4e60, \u5206\u79bb\u80fd\u529b, \u666e\u904d\u903c\u8fd1, \u6d45\u5c42\u7f51\u7edc"}}
{"id": "2506.02532", "pdf": "https://arxiv.org/pdf/2506.02532", "abs": "https://arxiv.org/abs/2506.02532", "authors": ["Jinu Lee", "Sagnik Mukherjee", "Dilek Hakkani-Tur", "Julia Hockenmaier"], "title": "ReasoningFlow: Semantic Structure of Complex Reasoning Traces", "categories": ["cs.CL"], "comment": "10 pages, 6 figures. ArgMining 2025 Workshop (Non-archival) @ ACL\n  2025", "summary": "Large reasoning models (LRMs) generate complex reasoning traces with\nplanning, reflection, verification, and backtracking. In this work, we\nintroduce ReasoningFlow, a unified schema for analyzing the semantic structures\nof these complex traces. ReasoningFlow parses traces into directed acyclic\ngraphs, enabling the characterization of distinct reasoning patterns as\nsubgraph structures. This human-interpretable representation offers promising\napplications in understanding, evaluating, and enhancing the reasoning\nprocesses of LRMs.", "AI": {"tldr": "\u63d0\u51faReasoningFlow\u6846\u67b6\uff0c\u89e3\u6790\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u8f68\u8ff9\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff0c\u652f\u6301\u5bf9\u5176\u8bed\u4e49\u7ed3\u6784\u7684\u5206\u6790\u548c\u6a21\u5f0f\u8bc6\u522b\u3002", "motivation": "\u56e0\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\u590d\u6742\u4e14\u591a\u6837\uff0c\u9700\u4e00\u79cd\u7edf\u4e00\u65b9\u6cd5\u89e3\u6790\u548c\u8868\u5f81\u8fd9\u4e9b\u8f68\u8ff9\u7684\u8bed\u4e49\u7ed3\u6784\u3002", "method": "\u5f00\u53d1ReasoningFlow\uff0c\u5c06\u63a8\u7406\u8f68\u8ff9\u89e3\u6790\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\uff0c\u901a\u8fc7\u5b50\u56fe\u7ed3\u6784\u8bc6\u522b\u4e0d\u540c\u63a8\u7406\u6a21\u5f0f\u3002", "result": "ReasoningFlow\u63d0\u4f9b\u4e86\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u652f\u6301\u5bf9LRMs\u63a8\u7406\u8fc7\u7a0b\u7684\u7406\u89e3\u3001\u8bc4\u4f30\u548c\u6539\u8fdb\u3002", "conclusion": "ReasoningFlow\u4e3a\u5206\u6790\u590d\u6742\u63a8\u7406\u8f68\u8ff9\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u5de5\u5177\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u5927\u578b\u63a8\u7406\u6a21\u578b, ReasoningFlow, \u6709\u5411\u65e0\u73af\u56fe, \u63a8\u7406\u6a21\u5f0f, \u8bed\u4e49\u5206\u6790"}}
{"id": "2506.02300", "pdf": "https://arxiv.org/pdf/2506.02300", "abs": "https://arxiv.org/abs/2506.02300", "authors": ["Farzaneh Mahdisoltani", "Saeed Mahdisoltani", "Roger B. Grosse", "David J. Fleet"], "title": "Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation", "categories": ["cs.LG"], "comment": null, "summary": "Understanding the internal representations and decision mechanisms of deep\nneural networks remains a critical open challenge. While existing\ninterpretability methods often identify influential input regions, they may not\nelucidate how a model distinguishes between classes or what specific changes\nwould transition an input from one category to another. To address these\nlimitations, we propose a novel framework that visualizes the implicit path\nbetween classes by treating the network gradient as a form of infinitesimal\nmotion. Drawing inspiration from phase-based motion magnification, we first\ndecompose images using invertible transforms-specifically the Complex Steerable\nPyramid-then compute class-conditional gradients in the transformed space.\nRather than iteratively integrating the gradient to trace a full path, we\namplify the one-step gradient to the input and perform a linear extrapolation\nto expose how the model moves from source to target class. By operating in the\nsteerable pyramid domain, these amplified gradients produce semantically\nmeaningful, spatially coherent morphs that highlight the classifier's most\nsensitive directions, giving insight into the geometry of its decision\nboundaries. Experiments on both synthetic and real-world datasets demonstrate\nthat our phase-focused extrapolation yields perceptually aligned, semantically\nmeaningful transformations, offering a novel, interpretable lens into neural\nclassifiers' internal representations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7f51\u7edc\u68af\u5ea6\u89c6\u4e3a\u5fae\u5c0f\u8fd0\u52a8\u6765\u53ef\u89c6\u5316\u7c7b\u522b\u95f4\u7684\u9690\u542b\u8def\u5f84\uff0c\u4ece\u800c\u63ed\u793a\u795e\u7ecf\u7f51\u7edc\u7684\u51b3\u7b56\u8fb9\u754c\u548c\u5185\u90e8\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u89e3\u91ca\u65b9\u6cd5\u901a\u5e38\u53ea\u80fd\u8bc6\u522b\u8f93\u5165\u533a\u57df\u7684\u5f71\u54cd\uff0c\u800c\u65e0\u6cd5\u9610\u660e\u6a21\u578b\u5982\u4f55\u533a\u5206\u7c7b\u522b\u6216\u5982\u4f55\u901a\u8fc7\u7279\u5b9a\u53d8\u5316\u5c06\u8f93\u5165\u4ece\u4e00\u4e2a\u7c7b\u522b\u8f6c\u6362\u5230\u53e6\u4e00\u4e2a\u7c7b\u522b\u3002", "method": "\u91c7\u7528\u53ef\u9006\u53d8\u6362\uff08Complex Steerable Pyramid\uff09\u5206\u89e3\u56fe\u50cf\uff0c\u5728\u53d8\u6362\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u7c7b\u522b\u6761\u4ef6\u68af\u5ea6\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u5916\u63a8\u653e\u5927\u68af\u5ea6\uff0c\u5c55\u793a\u6a21\u578b\u4ece\u6e90\u7c7b\u522b\u5230\u76ee\u6807\u7c7b\u522b\u7684\u53d8\u5316\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u8bed\u4e49\u660e\u786e\u3001\u7a7a\u95f4\u8fde\u8d2f\u7684\u5f62\u6001\u53d8\u5316\uff0c\u63ed\u793a\u4e86\u5206\u7c7b\u5668\u6700\u654f\u611f\u7684\u51b3\u7b56\u65b9\u5411\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u795e\u7ecf\u7f51\u7edc\u5185\u90e8\u8868\u793a\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u53ef\u89e3\u91ca\u7684\u5206\u6790\u5de5\u5177\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc, \u53ef\u89e3\u91ca\u6027, \u68af\u5ea6\u5206\u6790, \u51b3\u7b56\u8fb9\u754c, \u53ef\u89c6\u5316"}}
{"id": "2506.02533", "pdf": "https://arxiv.org/pdf/2506.02533", "abs": "https://arxiv.org/abs/2506.02533", "authors": ["Maike Behrendt", "Stefan Sylvius Wagner", "Carina Weinmann", "Marike Bormann", "Mira Warne", "Stefan Harmeling"], "title": "Natural Language Processing to Enhance Deliberation in Political Online Discussions: A Survey", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Political online participation in the form of discussing political issues and\nexchanging opinions among citizens is gaining importance with more and more\nformats being held digitally. To come to a decision, a careful discussion and\nconsideration of opinions and a civil exchange of arguments, which is defined\nas the act of deliberation, is desirable. The quality of discussions and\nparticipation processes in terms of their deliberativeness highly depends on\nthe design of platforms and processes. To facilitate online communication for\nboth participants and initiators, machine learning methods offer a lot of\npotential. In this work we want to showcase which issues occur in political\nonline discussions and how machine learning can be used to counteract these\nissues and enhance deliberation.", "AI": {"tldr": "\u6587\u7ae0\u8ba8\u8bba\u4e86\u653f\u6cbb\u5728\u7ebf\u8ba8\u8bba\u4e2d\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u4e86\u5982\u4f55\u5229\u7528\u673a\u5668\u5b66\u4e60\u63d0\u5347\u8ba8\u8bba\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u653f\u6cbb\u8ba8\u8bba\u65e5\u76ca\u6570\u5b57\u5316\uff0c\u9ad8\u8d28\u91cf\u7684\u8ba8\u8bba\uff08\u5373\u6df1\u601d\u719f\u8651\u7684\u4ea4\u6d41\uff09\u5bf9\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002\u5e73\u53f0\u8bbe\u8ba1\u5f71\u54cd\u8ba8\u8bba\u8d28\u91cf\uff0c\u673a\u5668\u5b66\u4e60\u6709\u6f5c\u529b\u6539\u5584\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u5982\u4f55\u5229\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u89e3\u51b3\u653f\u6cbb\u5728\u7ebf\u8ba8\u8bba\u4e2d\u7684\u95ee\u9898\uff0c\u5e76\u4fc3\u8fdb\u6df1\u601d\u719f\u8651\u7684\u4ea4\u6d41\u3002", "result": "\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u5728\u63d0\u5347\u653f\u6cbb\u5728\u7ebf\u8ba8\u8bba\u8d28\u91cf\u65b9\u9762\u7684\u6f5c\u5728\u5e94\u7528\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u53ef\u4ee5\u6709\u6548\u6539\u5584\u653f\u6cbb\u5728\u7ebf\u8ba8\u8bba\u4e2d\u7684\u95ee\u9898\uff0c\u63d0\u5347\u8ba8\u8bba\u8d28\u91cf\u3002", "keywords": "\u653f\u6cbb\u5728\u7ebf\u53c2\u4e0e, \u8ba8\u8bba\u8d28\u91cf, \u673a\u5668\u5b66\u4e60, \u6df1\u601d\u719f\u8651"}}
{"id": "2506.02306", "pdf": "https://arxiv.org/pdf/2506.02306", "abs": "https://arxiv.org/abs/2506.02306", "authors": ["Aditya Gorla", "Ryan Wang", "Zhengtong Liu", "Ulzee An", "Sriram Sankararaman"], "title": "CACTI: Leveraging Copy Masking and Contextual Information to Improve Tabular Data Imputation", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We present CACTI, a masked autoencoding approach for imputing tabular data\nthat leverages the structure in missingness patterns and contextual\ninformation. Our approach employs a novel median truncated copy masking\ntraining strategy that encourages the model to learn from empirical patterns of\nmissingness while incorporating semantic relationships between features -\ncaptured by column names and text descriptions - to better represent feature\ndependence. These dual sources of inductive bias enable CACTI to outperform\nstate-of-the-art methods - an average $R^2$ gain of 7.8% over the next best\nmethod (13.4%, 6.1%, and 5.3% under missing not at random, at random and\ncompletely at random, respectively) - across a diverse range of datasets and\nmissingness conditions. Our results highlight the value of leveraging\ndataset-specific contextual information and missingness patterns to enhance\nimputation performance.", "AI": {"tldr": "CACTI\u662f\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u81ea\u7f16\u7801\u7684\u8868\u683c\u6570\u636e\u586b\u8865\u65b9\u6cd5\uff0c\u5229\u7528\u7f3a\u5931\u6a21\u5f0f\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8868\u683c\u6570\u636e\u586b\u8865\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u7f3a\u5931\u6a21\u5f0f\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0cCACTI\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u4e2d\u4f4d\u6570\u622a\u65ad\u7684\u590d\u5236\u63a9\u7801\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u7f3a\u5931\u6a21\u5f0f\u548c\u7279\u5f81\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\uff08\u5982\u5217\u540d\u548c\u6587\u672c\u63cf\u8ff0\uff09\u3002", "result": "CACTI\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u7f3a\u5931\u6761\u4ef6\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747R\u00b2\u63d0\u53477.8%\u3002", "conclusion": "\u5229\u7528\u6570\u636e\u96c6\u7279\u5b9a\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u7f3a\u5931\u6a21\u5f0f\u53ef\u663e\u8457\u63d0\u5347\u586b\u8865\u6027\u80fd\u3002", "keywords": "\u63a9\u7801\u81ea\u7f16\u7801, \u8868\u683c\u6570\u636e\u586b\u8865, \u7f3a\u5931\u6a21\u5f0f, \u4e0a\u4e0b\u6587\u4fe1\u606f"}}
{"id": "2506.02536", "pdf": "https://arxiv.org/pdf/2506.02536", "abs": "https://arxiv.org/abs/2506.02536", "authors": ["Xin Liu", "Lu Wang"], "title": "Answer Convergence as a Signal for Early Stopping in Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-thought (CoT) prompting enhances reasoning in large language models\n(LLMs) but often leads to verbose and redundant outputs, thus increasing\ninference cost. We hypothesize that many reasoning steps are unnecessary for\nproducing correct answers. To investigate this, we start with a systematic\nstudy to examine what is the minimum reasoning required for a model to reach a\nstable decision. We find that on math reasoning tasks like math, models\ntypically converge to their final answers after 60\\% of the reasoning steps,\nsuggesting substantial redundancy in the remaining content. Based on these\ninsights, we propose three inference-time strategies to improve efficiency: (1)\nearly stopping via answer consistency, (2) boosting the probability of\ngenerating end-of-reasoning signals, and (3) a supervised method that learns\nwhen to stop based on internal activations. Experiments across five benchmarks\nand five open-weights LLMs show that our methods significantly reduce token\nusage with little or no accuracy drop. In particular, on NaturalQuestions,\nAnswer Consistency reduces tokens by over 40\\% while further improving\naccuracy. Our work underscores the importance of cost-effective reasoning\nmethods that operate at inference time, offering practical benefits for\nreal-world applications.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u63a8\u7406\u6b65\u9aa4\u7684\u5197\u4f59\u6027\uff0c\u5e76\u63d0\u51fa\u4e09\u79cd\u63a8\u65ad\u65f6\u7b56\u7565\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u94fe\u5f0f\u63a8\u7406\uff08CoT\uff09\u63d0\u793a\u867d\u7136\u589e\u5f3a\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5e38\u5bfc\u81f4\u5197\u4f59\u8f93\u51fa\uff0c\u589e\u52a0\u4e86\u63a8\u65ad\u6210\u672c\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7814\u7a76\u786e\u5b9a\u6700\u5c0f\u5fc5\u8981\u63a8\u7406\u6b65\u9aa4\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u79cd\u63a8\u65ad\u65f6\u7b56\u7565\uff1a\u65e9\u671f\u505c\u6b62\u3001\u589e\u5f3a\u7ed3\u675f\u4fe1\u53f7\u6982\u7387\u53ca\u57fa\u4e8e\u5185\u90e8\u6fc0\u6d3b\u7684\u76d1\u7763\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u663e\u8457\u51cf\u5c11\u6807\u8bb0\u4f7f\u7528\u4e14\u51e0\u4e4e\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728NaturalQuestions\u4e0a\u5b9e\u73b0\u8d85\u8fc740%\u7684\u6807\u8bb0\u51cf\u5c11\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u63a8\u65ad\u65f6\u9ad8\u6548\u63a8\u7406\u65b9\u6cd5\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u4ef7\u503c\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u94fe\u5f0f\u63a8\u7406, \u63a8\u7406\u6548\u7387, \u63a8\u65ad\u65f6\u4f18\u5316"}}
{"id": "2506.02308", "pdf": "https://arxiv.org/pdf/2506.02308", "abs": "https://arxiv.org/abs/2506.02308", "authors": ["Xiaojun Shan", "Qi Cao", "Xing Han", "Haofei Yu", "Paul Pu Liang"], "title": "MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advances in multimodal foundation models have achieved\nstate-of-the-art performance across a range of tasks. These breakthroughs are\nlargely driven by new pre-training paradigms that leverage large-scale,\nunlabeled multimodal data, followed by instruction fine-tuning on curated\nlabeled datasets and high-quality prompts. While there is growing interest in\nscaling instruction fine-tuning to ever-larger datasets in both quantity and\nscale, our findings reveal that simply increasing the number of\ninstruction-tuning tasks does not consistently yield better performance.\nInstead, we observe that grouping tasks by the common interactions across\nmodalities, such as discovering redundant shared information, prioritizing\nmodality selection with unique information, or requiring synergistic fusion to\ndiscover new information from both modalities, encourages the models to learn\ntransferrable skills within a group while suppressing interference from\nmismatched tasks. To this end, we introduce MINT, a simple yet surprisingly\neffective task-grouping strategy based on the type of multimodal interaction.\nWe demonstrate that the proposed method greatly outperforms existing task\ngrouping baselines for multimodal instruction tuning, striking an effective\nbalance between generalization and specialization.", "AI": {"tldr": "MINT\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u4ea4\u4e92\u7c7b\u578b\u7684\u4efb\u52a1\u5206\u7ec4\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u6307\u4ee4\u8c03\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\u65b9\u6cd5\uff0c\u53d1\u73b0\u5355\u7eaf\u589e\u52a0\u6307\u4ee4\u8c03\u4f18\u4efb\u52a1\u6570\u91cf\u5e76\u4e0d\u603b\u80fd\u63d0\u9ad8\u6027\u80fd\uff0c\u800c\u4efb\u52a1\u5206\u7ec4\u66f4\u4e3a\u6709\u6548\u3002", "method": "\u63d0\u51faMINT\u65b9\u6cd5\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u7ec4\uff08\u5982\u5171\u4eab\u4fe1\u606f\u53d1\u73b0\u3001\u6a21\u6001\u9009\u62e9\u7b49\uff09\u6765\u5b66\u4e60\u53ef\u8f6c\u79fb\u6280\u80fd\u5e76\u51cf\u5c11\u4e0d\u5339\u914d\u4efb\u52a1\u7684\u5e72\u6270\u3002", "result": "MINT\u5728\u591a\u6a21\u6001\u6307\u4ee4\u8c03\u4f18\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u8861\u4e86\u6cdb\u5316\u4e0e\u4e13\u4e1a\u5316\u3002", "conclusion": "\u4efb\u52a1\u5206\u7ec4\u7b56\u7565\u5728\u591a\u6a21\u6001\u6307\u4ee4\u8c03\u4f18\u4e2d\u81f3\u5173\u91cd\u8981\uff0cMINT\u4e3a\u5176\u63d0\u4f9b\u4e86\u7b80\u5355\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u591a\u6a21\u6001, \u6307\u4ee4\u8c03\u4f18, \u4efb\u52a1\u5206\u7ec4, \u6cdb\u5316, MINT"}}
{"id": "2506.02544", "pdf": "https://arxiv.org/pdf/2506.02544", "abs": "https://arxiv.org/abs/2506.02544", "authors": ["Yang Tian", "Fan Liu", "Jingyuan Zhang", "Victoria W.", "Yupeng Hu", "Liqiang Nie"], "title": "CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main", "summary": "Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to\nenhance Multimodal Large Language Models by incorporating externally retrieved\nmultimodal knowledge, but it introduces two challenges: Parametric-Retrieved\nKnowledge Inconsistency (PRKI), where discrepancies between parametric and\nretrieved knowledge create uncertainty in determining reliability, and\nVisual-Textual Knowledge Inconsistency (VTKI), where misalignment between\nvisual and textual sources disrupts entity representation. To address these\nchallenges, we propose \\textbf{C}r\\textbf{o}ss-source knowledge\n\\textbf{Re}conciliation for \\textbf{M}ulti\\textbf{M}odal \\textbf{RAG}\n(CoRe-MMRAG), a novel end-to-end framework that effectively reconciles\ninconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage\npipeline: it first generates an internal response from parametric knowledge,\nthen selects the most relevant multimodal evidence via joint similarity\nassessment, generates an external response, and finally integrates both to\nproduce a reliable answer. Additionally, a specialized training paradigm\nenhances knowledge source discrimination, multimodal integration, and unified\nanswer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG\nachieves substantial improvements over baseline methods, achieving 5.6\\% and\n9.3\\% performance gains on InfoSeek and Encyclopedic-VQA, respectively. We\nrelease code and data at\n\\href{https://github.com/TyangJN/CoRe-MMRAG}{https://github.com/TyangJN/CoRe-MMRAG}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CoRe-MMRAG\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6e90\u77e5\u8bc6\u8c03\u548c\u89e3\u51b3\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u7684\u77e5\u8bc6\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08MMRAG\uff09\u4e2d\u5b58\u5728\u7684\u53c2\u6570\u5316\u4e0e\u68c0\u7d22\u77e5\u8bc6\u4e0d\u4e00\u81f4\uff08PRKI\uff09\u4ee5\u53ca\u89c6\u89c9\u4e0e\u6587\u672c\u77e5\u8bc6\u4e0d\u4e00\u81f4\uff08VTKI\uff09\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faCoRe-MMRAG\u6846\u67b6\uff0c\u91c7\u7528\u56db\u9636\u6bb5\u6d41\u7a0b\uff1a\u751f\u6210\u5185\u90e8\u54cd\u5e94\u3001\u8054\u5408\u76f8\u4f3c\u6027\u8bc4\u4f30\u9009\u62e9\u591a\u6a21\u6001\u8bc1\u636e\u3001\u751f\u6210\u5916\u90e8\u54cd\u5e94\u3001\u6574\u5408\u77e5\u8bc6\u751f\u6210\u6700\u7ec8\u7b54\u6848\uff0c\u5e76\u901a\u8fc7\u4e13\u95e8\u8bad\u7ec3\u8303\u5f0f\u589e\u5f3a\u591a\u6e90\u77e5\u8bc6\u878d\u5408\u80fd\u529b\u3002", "result": "\u5728KB-VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoRe-MMRAG\u5728InfoSeek\u548cEncyclopedic-VQA\u4e0a\u5206\u522b\u5b9e\u73b0\u4e865.6%\u548c9.3%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "CoRe-MMRAG\u80fd\u6709\u6548\u8c03\u548c\u591a\u6e90\u77e5\u8bc6\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "keywords": "\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210, \u77e5\u8bc6\u4e0d\u4e00\u81f4, CoRe-MMRAG, KB-VQA"}}
{"id": "2506.01979", "pdf": "https://arxiv.org/pdf/2506.01979", "abs": "https://arxiv.org/abs/2506.01979", "authors": ["Yuhao Shen", "Junyi Shen", "Quan Kong", "Tianyu Liu", "Yao Lu", "Cong Wang"], "title": "Speculative Decoding via Hybrid Drafting and Rollback-Aware Branch Parallelism", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Recently, speculative decoding (SD) has emerged as a promising technique to\naccelerate LLM inference by employing a small draft model to propose draft\ntokens in advance, and validating them in parallel with the large target model.\nHowever, the existing SD methods still remain fundamentally constrained by\ntheir serialized execution, which causes the mutual waiting bubbles between the\ndraft and target models. To address this challenge, we draw inspiration from\nbranch prediction in modern processors and propose a novel framework\n\\textbf{SpecBranch} to unlock branch parallelism in SD. Specifically, we first\ntake an in-depth analysis of the potential of branch parallelism in SD, and\nrecognize that the key challenge lies in the trade-offs between parallelization\nand token rollback. Based on the analysis, we strategically introduce parallel\nspeculative branches to preemptively hedge against likely rejections.\nMeanwhile, to enhance parallelism, we jointly orchestrate adaptive draft\nlengths with a hybrid combination of the implicit draft model confidence and\nexplicit reusing of target model features. Extensive experiments across various\nmodels and benchmarks show that SpecBranch achieves over \\textbf{1.8}$\\times\n\\sim$ \\textbf{4.5}$\\times$ speedups against the auto-regressive decoding and\nreduces rollback tokens by $\\textbf{50}$\\% for poorly aligned models, realizing\nits applicability for real-world deployments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpecBranch\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5206\u652f\u5e76\u884c\u6027\u6765\u4f18\u5316\u63a8\u6d4b\u89e3\u7801\uff08SD\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u4e32\u884c\u6267\u884c\u5bfc\u81f4\u7684\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u6d4b\u89e3\u7801\uff08SD\uff09\u65b9\u6cd5\u56e0\u4e32\u884c\u6267\u884c\u5bfc\u81f4\u6a21\u578b\u95f4\u7684\u7b49\u5f85\u95ee\u9898\uff0c\u9650\u5236\u4e86\u63a8\u7406\u901f\u5ea6\u7684\u63d0\u5347\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u5206\u652f\u5e76\u884c\u6027\uff0c\u4f18\u5316SD\u7684\u6267\u884c\u6548\u7387\u3002", "method": "\u63d0\u51faSpecBranch\u6846\u67b6\uff0c\u5206\u6790\u5206\u652f\u5e76\u884c\u6027\u7684\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u5e76\u884c\u63a8\u6d4b\u5206\u652f\u548c\u52a8\u6001\u8c03\u6574\u8349\u7a3f\u957f\u5ea6\uff08\u7ed3\u5408\u9690\u5f0f\u8349\u7a3f\u6a21\u578b\u7f6e\u4fe1\u5ea6\u548c\u663e\u5f0f\u76ee\u6807\u6a21\u578b\u7279\u5f81\u590d\u7528\uff09\u6765\u63d0\u5347\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSpecBranch\u5728\u591a\u4e2a\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e861.8\u00d7\u223c4.5\u00d7\u7684\u52a0\u901f\uff0c\u5e76\u5c06\u56de\u9000\u6807\u8bb0\u51cf\u5c1150%\uff0c\u8bc1\u660e\u5176\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002", "conclusion": "SpecBranch\u901a\u8fc7\u5206\u652f\u5e76\u884c\u6027\u663e\u8457\u63d0\u5347\u4e86\u63a8\u6d4b\u89e3\u7801\u7684\u6548\u7387\uff0c\u4e3a\u89e3\u51b3LLM\u63a8\u7406\u4e2d\u7684\u74f6\u9888\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002", "keywords": "\u63a8\u6d4b\u89e3\u7801, \u5206\u652f\u5e76\u884c\u6027, LLM\u52a0\u901f, SpecBranch, \u52a8\u6001\u8349\u7a3f\u957f\u5ea6"}}
{"id": "2506.02315", "pdf": "https://arxiv.org/pdf/2506.02315", "abs": "https://arxiv.org/abs/2506.02315", "authors": ["D. Isaiah Harp", "Joshua Ott", "John Alora", "Dylan Asmar"], "title": "A Data-Based Architecture for Flight Test without Test Points", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "The Society of Experimental Test Pilots Annual Symposium, vol. 68th,\n  2024", "summary": "The justification for the \"test point\" derives from the test pilot's\nobligation to reproduce faithfully the pre-specified conditions of some model\nprediction. Pilot deviation from those conditions invalidates the model\nassumptions. Flight test aids have been proposed to increase accuracy on more\nchallenging test points. However, the very existence of databands and\ntolerances is the problem more fundamental than inadequate pilot skill. We\npropose a novel approach, which eliminates test points. We start with a\nhigh-fidelity digital model of an air vehicle. Instead of using this model to\ngenerate a point prediction, we use a machine learning method to produce a\nreduced-order model (ROM). The ROM has two important properties. First, it can\ngenerate a prediction based on any set of conditions the pilot flies. Second,\nif the test result at those conditions differ from the prediction, the ROM can\nbe updated using the new data. The outcome of flight test is thus a refined ROM\nat whatever conditions were flown. This ROM in turn updates and validates the\nhigh-fidelity model. We present a single example of this \"point-less\"\narchitecture, using T-38C flight test data. We first use a generic aircraft\nmodel to build a ROM of longitudinal pitching motion as a hypersurface. We then\ningest unconstrained flight test data and use Gaussian Process Regression to\nupdate and condition the hypersurface. By proposing a second-order equivalent\nsystem for the T-38C, this hypersurface then generates parameters necessary to\nassess MIL-STD-1797B compliance for longitudinal dynamics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u2018\u65e0\u6d4b\u8bd5\u70b9\u2019\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u4fdd\u771f\u6570\u5b57\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u751f\u6210\u964d\u9636\u6a21\u578b\uff08ROM\uff09\uff0c\u52a8\u6001\u66f4\u65b0\u9a8c\u8bc1\u98de\u884c\u6d4b\u8bd5\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u6d4b\u8bd5\u70b9\u65b9\u6cd5\u56e0\u98de\u884c\u5458\u64cd\u4f5c\u504f\u79bb\u9884\u8bbe\u6761\u4ef6\u800c\u5bfc\u81f4\u6a21\u578b\u5931\u6548\uff0c\u4e14\u6570\u636e\u5e26\u548c\u5bb9\u5fcd\u5ea6\u5b58\u5728\u6839\u672c\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9ad8\u4fdd\u771f\u6570\u5b57\u6a21\u578b\u751f\u6210ROM\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\uff08\u5982\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff09\u52a8\u6001\u66f4\u65b0\u6a21\u578b\uff0c\u9002\u5e94\u4efb\u610f\u98de\u884c\u6761\u4ef6\u5e76\u9a8c\u8bc1\u9ad8\u4fdd\u771f\u6a21\u578b\u3002", "result": "\u6210\u529f\u5e94\u7528T-38C\u98de\u884c\u6d4b\u8bd5\u6570\u636e\u751f\u6210\u7eb5\u5411\u4fef\u4ef0\u8fd0\u52a8\u7684ROM\uff0c\u5e76\u9a8c\u8bc1\u5176\u7b26\u5408MIL-STD-1797B\u6807\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6446\u8131\u4e86\u4f20\u7edf\u6d4b\u8bd5\u70b9\u7684\u9650\u5236\uff0c\u52a8\u6001\u66f4\u65b0\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u98de\u884c\u6d4b\u8bd5\u7684\u7075\u6d3b\u6027\u548c\u51c6\u786e\u6027\u3002", "keywords": "\u98de\u884c\u6d4b\u8bd5,\u964d\u9636\u6a21\u578b,\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52,\u9ad8\u4fdd\u771f\u6a21\u578b,\u52a8\u6001\u66f4\u65b0"}}
{"id": "2506.02561", "pdf": "https://arxiv.org/pdf/2506.02561", "abs": "https://arxiv.org/abs/2506.02561", "authors": ["Yirao Zhao", "Guizhen Chen", "Kenji Kawaguchi", "Lidong Bing", "Wenxuan Zhang"], "title": "Pruning General Large Language Models into Customized Expert Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their substantial model sizes often require substantial computational\nresources. To preserve computing resources and accelerate inference speed, it\nis crucial to prune redundant parameters, especially for experienced users who\noften need compact expert models tailored to specific downstream scenarios.\nHowever, most existing pruning methods focus on preserving the model's general\ncapabilities, often requiring extensive post-training or suffering from\ndegraded performance due to coarse-grained pruning. In this work, we design a\n$\\underline{Cus}$tom $\\underline{Prun}$ing method ($\\texttt{Cus-Prun}$) to\nprune a large general model into a smaller lightweight expert model, which is\npositioned along the \"language\", \"domain\" and \"task\" dimensions. By identifying\nand pruning irrelevant neurons of each dimension, $\\texttt{Cus-Prun}$ creates\nexpert models without any post-training. Our experiments demonstrate that\n$\\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal\nloss in both expert and general capabilities across various models from\ndifferent model families and sizes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCus-Prun\u7684\u81ea\u5b9a\u4e49\u526a\u679d\u65b9\u6cd5\uff0c\u53ef\u5c06\u5927\u578b\u901a\u7528\u6a21\u578b\u526a\u679d\u4e3a\u5c0f\u578b\u4e13\u5bb6\u6a21\u578b\uff0c\u65e0\u9700\u540e\u8bad\u7ec3\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u65e0\u6cd5\u5728\u4fdd\u6301\u6a21\u578b\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u6ee1\u8db3\u7279\u5b9a\u4e0b\u6e38\u573a\u666f\u9700\u6c42\u3002", "method": "Cus-Prun\u901a\u8fc7\u5728\u8bed\u8a00\u3001\u9886\u57df\u548c\u4efb\u52a1\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u8bc6\u522b\u5e76\u526a\u679d\u65e0\u5173\u795e\u7ecf\u5143\uff0c\u751f\u6210\u8f7b\u91cf\u7ea7\u4e13\u5bb6\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCus-Prun\u5728\u591a\u79cd\u6a21\u578b\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4e13\u5bb6\u80fd\u529b\u548c\u901a\u7528\u80fd\u529b\u635f\u5931\u6700\u5c0f\u3002", "conclusion": "Cus-Prun\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u526a\u679d\u65b9\u6cd5\uff0c\u9002\u5408\u751f\u6210\u7279\u5b9a\u573a\u666f\u7684\u4e13\u5bb6\u6a21\u578b\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u526a\u679d, \u4e13\u5bb6\u6a21\u578b, \u81ea\u5b9a\u4e49\u526a\u679d"}}
{"id": "2506.01980", "pdf": "https://arxiv.org/pdf/2506.01980", "abs": "https://arxiv.org/abs/2506.01980", "authors": ["Lianhao Yin", "Ozanan Meireles", "Guy Rosman", "Daniela Rus"], "title": "Surgical Foundation Model Leveraging Compression and Entropy Maximization for Image-Guided Surgical Assistance", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Real-time video understanding is critical to guide procedures in minimally\ninvasive surgery (MIS). However, supervised learning approaches require large,\nannotated datasets that are scarce due to annotation efforts that are\nprohibitive, e.g., in medical fields. Although self-supervision methods can\naddress such limitations, current self-supervised methods often fail to capture\nstructural and physical information in a form that generalizes across tasks. We\npropose Compress-to-Explore (C2E), a novel self-supervised framework that\nleverages Kolmogorov complexity to learn compact, informative representations\nfrom surgical videos. C2E uses entropy-maximizing decoders to compress images\nwhile preserving clinically relevant details, improving encoder performance\nwithout labeled data. Trained on large-scale unlabeled surgical datasets, C2E\ndemonstrates strong generalization across a variety of surgical ML tasks, such\nas workflow classification, tool-tissue interaction classification,\nsegmentation, and diagnosis tasks, providing improved performance as a surgical\nvisual foundation model. As we further show in the paper, the model's internal\ncompact representation better disentangles features from different structural\nparts of images. The resulting performance improvements highlight the yet\nuntapped potential of self-supervised learning to enhance surgical AI and\nimprove outcomes in MIS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aC2E\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7Kolmogorov\u590d\u6742\u6027\u5b66\u4e60\u624b\u672f\u89c6\u9891\u7684\u7d27\u51d1\u3001\u4fe1\u606f\u4e30\u5bcc\u8868\u793a\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u63d0\u9ad8\u7f16\u7801\u5668\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u76d1\u7763\u5b66\u4e60\u5728\u5fae\u521b\u624b\u672f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5e94\u7528\u53d7\u9650\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6355\u6349\u7ed3\u6784\u548c\u7269\u7406\u4fe1\u606f\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\u3002", "method": "C2E\u6846\u67b6\u5229\u7528\u71b5\u6700\u5927\u5316\u89e3\u7801\u5668\u538b\u7f29\u56fe\u50cf\uff0c\u540c\u65f6\u4fdd\u7559\u4e34\u5e8a\u76f8\u5173\u7ec6\u8282\uff0c\u5b66\u4e60\u7d27\u51d1\u8868\u793a\u3002", "result": "\u5728\u5927\u89c4\u6a21\u672a\u6807\u6ce8\u624b\u672f\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0cC2E\u5728\u591a\u79cd\u624b\u672fML\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "C2E\u5c55\u793a\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u63d0\u5347\u624b\u672fAI\u6027\u80fd\u548c\u6539\u5584\u5fae\u521b\u624b\u672f\u7ed3\u679c\u65b9\u9762\u7684\u6f5c\u529b\u3002", "keywords": "\u81ea\u76d1\u7763\u5b66\u4e60,\u5fae\u521b\u624b\u672f,\u89c6\u9891\u7406\u89e3,Kolmogorov\u590d\u6742\u6027,\u7d27\u51d1\u8868\u793a"}}
{"id": "2506.02318", "pdf": "https://arxiv.org/pdf/2506.02318", "abs": "https://arxiv.org/abs/2506.02318", "authors": ["Yuchen Liang", "Renxiang Huang", "Lifeng Lai", "Ness Shroff", "Yingbin Liang"], "title": "Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models", "categories": ["cs.LG", "eess.SP", "math.ST", "stat.TH"], "comment": null, "summary": "Discrete state space diffusion models have shown significant advantages in\napplications involving discrete data, such as text and image generation. It has\nalso been observed that their performance is highly sensitive to the choice of\nrate matrices, particularly between uniform and absorbing rate matrices. While\nempirical results suggest that absorbing rate matrices often yield better\ngeneration quality compared to uniform rate matrices, existing theoretical\nworks have largely focused on the uniform rate matrices case. Notably,\nconvergence guarantees and error analyses for absorbing diffusion models are\nstill missing. In this work, we provide the first finite-time error bounds and\nconvergence rate analysis for discrete diffusion models using absorbing rate\nmatrices. We begin by deriving an upper bound on the KL divergence of the\nforward process, introducing a surrogate initialization distribution to address\nthe challenge posed by the absorbing stationary distribution, which is a\nsingleton and causes the KL divergence to be ill-defined. We then establish the\nfirst convergence guarantees for both the $\\tau$-leaping and uniformization\nsamplers under absorbing rate matrices, demonstrating improved rates over their\ncounterparts using uniform rate matrices. Furthermore, under suitable\nassumptions, we provide convergence guarantees without early stopping. Our\nanalysis introduces several new technical tools to address challenges unique to\nabsorbing rate matrices. These include a Jensen-type argument for bounding\nforward process convergence, novel techniques for bounding absorbing score\nfunctions, and a non-divergent upper bound on the score near initialization\nthat removes the need of early-stopping.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u4e3a\u4f7f\u7528\u5438\u6536\u7387\u77e9\u9635\u7684\u79bb\u6563\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u9650\u65f6\u95f4\u8bef\u5dee\u754c\u9650\u548c\u6536\u655b\u7387\u5206\u6790\u3002", "motivation": "\u79bb\u6563\u6269\u6563\u6a21\u578b\u5728\u79bb\u6563\u6570\u636e\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6027\u80fd\u5bf9\u7387\u77e9\u9635\u9009\u62e9\u975e\u5e38\u654f\u611f\uff0c\u5c24\u5176\u662f\u5747\u5300\u4e0e\u5438\u6536\u7387\u77e9\u9635\u3002\u5c3d\u7ba1\u5438\u6536\u7387\u77e9\u9635\u901a\u5e38\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u7406\u8bba\u5206\u6790\u96c6\u4e2d\u5728\u5747\u5300\u7387\u77e9\u9635\u4e0a\uff0c\u5438\u6536\u7387\u6a21\u578b\u7684\u7406\u8bba\u5206\u6790\u4ecd\u7f3a\u5931\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u66ff\u4ee3\u521d\u59cb\u5316\u5206\u5e03\u89e3\u51b3\u5438\u6536\u5e73\u7a33\u5206\u5e03\u5bfc\u81f4\u7684KL\u6563\u5ea6\u95ee\u9898\uff0c\u4f7f\u7528Jensen\u578b\u5de5\u5177\u548c\u65b0\u6280\u672f\u6765\u5206\u6790\u5438\u6536\u5206\u6570\u51fd\u6570\uff0c\u5e76\u63d0\u4f9b\u65e0\u9700\u63d0\u524d\u505c\u6b62\u7684\u6536\u655b\u4fdd\u8bc1\u3002", "result": "\u9996\u6b21\u4e3a\u5438\u6536\u7387\u77e9\u9635\u6a21\u578b\u5efa\u7acb\u4e86\u6536\u655b\u4fdd\u8bc1\u548c\u6539\u8fdb\u7684\u6536\u655b\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u91c7\u6837\u5668\u4e2d\u7684\u4f18\u52bf\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u89e3\u51b3\u5438\u6536\u7387\u77e9\u9635\u7279\u6709\u7684\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u5438\u6536\u7387\u77e9\u9635\u6269\u6563\u6a21\u578b\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u4e3a\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u65b0\u65b9\u6cd5\u3002", "keywords": "\u79bb\u6563\u6269\u6563\u6a21\u578b,\u5438\u6536\u7387\u77e9\u9635,\u6536\u655b\u5206\u6790,\u8bef\u5dee\u754c\u9650,KL\u6563\u5ea6"}}
{"id": "2506.02573", "pdf": "https://arxiv.org/pdf/2506.02573", "abs": "https://arxiv.org/abs/2506.02573", "authors": ["Muhammad Falensi Azmi", "Muhammad Dehan Al Kautsar", "Alfan Farizki Wicaksono", "Fajri Koto"], "title": "IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages", "categories": ["cs.CL"], "comment": "25 pages", "summary": "Although region-specific large language models (LLMs) are increasingly\ndeveloped, their safety remains underexplored, particularly in culturally\ndiverse settings like Indonesia, where sensitivity to local norms is essential\nand highly valued by the community. In this work, we present IndoSafety, the\nfirst high-quality, human-verified safety evaluation dataset tailored for the\nIndonesian context, covering five language varieties: formal and colloquial\nIndonesian, along with three major local languages: Javanese, Sundanese, and\nMinangkabau. IndoSafety is constructed by extending prior safety frameworks to\ndevelop a taxonomy that captures Indonesia's sociocultural context. We find\nthat existing Indonesian-centric LLMs often generate unsafe outputs,\nparticularly in colloquial and local language settings, while fine-tuning on\nIndoSafety significantly improves safety while preserving task performance. Our\nwork highlights the critical need for culturally grounded safety evaluation and\nprovides a concrete step toward responsible LLM deployment in multilingual\nsettings. Warning: This paper contains example data that may be offensive,\nharmful, or biased.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86IndoSafety\uff0c\u4e00\u4e2a\u9488\u5bf9\u5370\u5ea6\u5c3c\u897f\u4e9a\u6587\u5316\u80cc\u666f\u7684\u9ad8\u8d28\u91cf\u3001\u4eba\u5de5\u9a8c\u8bc1\u7684\u5b89\u5168\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u533a\u57df\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u672c\u5730\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u6027\u3002", "motivation": "\u5c3d\u7ba1\u533a\u57df\u6027\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65e5\u76ca\u53d1\u5c55\uff0c\u4f46\u5176\u5b89\u5168\u6027\u5728\u6587\u5316\u591a\u6837\u5316\u7684\u73af\u5883\u4e2d\uff08\u5982\u5370\u5ea6\u5c3c\u897f\u4e9a\uff09\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5370\u5ea6\u5c3c\u897f\u4e9a\u5bf9\u672c\u5730\u89c4\u8303\u7684\u654f\u611f\u6027\u975e\u5e38\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u9488\u5bf9\u5176\u6587\u5316\u80cc\u666f\u7684\u5b89\u5168\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6269\u5c55\u73b0\u6709\u7684\u5b89\u5168\u6846\u67b6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u8986\u76d6\u5370\u5c3c\u4e94\u79cd\u8bed\u8a00\u53d8\u4f53\u7684\u5b89\u5168\u8bc4\u4f30\u6570\u636e\u96c6IndoSafety\uff0c\u5305\u62ec\u6b63\u5f0f\u548c\u975e\u6b63\u5f0f\u7684\u5370\u5c3c\u8bed\u53ca\u4e09\u79cd\u4e3b\u8981\u672c\u5730\u8bed\u8a00\uff08\u722a\u54c7\u8bed\u3001\u5dfd\u4ed6\u8bed\u548c\u7c73\u5357\u52a0\u4fdd\u8bed\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u7684\u5370\u5c3c\u8bed\u8a00\u6a21\u578b\u5728\u975e\u6b63\u5f0f\u548c\u672c\u5730\u8bed\u8a00\u73af\u5883\u4e2d\u5e38\u751f\u6210\u4e0d\u5b89\u5168\u8f93\u51fa\u3002\u901a\u8fc7\u4f7f\u7528IndoSafety\u5fae\u8c03\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u57fa\u4e8e\u6587\u5316\u7684\u5b89\u5168\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u8d1f\u8d23\u4efb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u5177\u4f53\u6b65\u9aa4\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5b89\u5168\u6027\u8bc4\u4f30, \u5370\u5ea6\u5c3c\u897f\u4e9a, \u6587\u5316\u591a\u6837\u6027, \u672c\u5730\u8bed\u8a00"}}
{"id": "2506.01982", "pdf": "https://arxiv.org/pdf/2506.01982", "abs": "https://arxiv.org/abs/2506.01982", "authors": ["Vassilis Lyberatos", "Spyridon Kantarelis", "Ioanna Zioga", "Christina Anagnostopoulou", "Giorgos Stamou", "Anastasia Georgaki"], "title": "Music interpretation and emotion perception: A computational and neurophysiological investigation", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This study investigates emotional expression and perception in music\nperformance using computational and neurophysiological methods. The influence\nof different performance settings, such as repertoire, diatonic modal etudes,\nand improvisation, as well as levels of expressiveness, on performers'\nemotional communication and listeners' reactions is explored. Professional\nmusicians performed various tasks, and emotional annotations were provided by\nboth performers and the audience. Audio analysis revealed that expressive and\nimprovisational performances exhibited unique acoustic features, while emotion\nanalysis showed stronger emotional responses. Neurophysiological measurements\nindicated greater relaxation in improvisational performances. This multimodal\nstudy highlights the significance of expressivity in enhancing emotional\ncommunication and audience engagement.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u8ba1\u7b97\u548c\u795e\u7ecf\u751f\u7406\u5b66\u65b9\u6cd5\u63a2\u8ba8\u97f3\u4e50\u8868\u6f14\u4e2d\u60c5\u611f\u8868\u8fbe\u4e0e\u611f\u77e5\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5373\u5174\u548c\u8868\u73b0\u529b\u5f3a\u7684\u8868\u6f14\u5177\u6709\u72ec\u7279\u58f0\u5b66\u7279\u5f81\u548c\u66f4\u5f3a\u7684\u60c5\u611f\u53cd\u5e94\uff0c\u4e14\u5373\u5174\u8868\u6f14\u80fd\u5e26\u6765\u66f4\u5927\u7684\u653e\u677e\u611f\u3002", "motivation": "\u63a2\u7d22\u97f3\u4e50\u8868\u6f14\u4e2d\u4e0d\u540c\u8868\u73b0\u529b\u548c\u5373\u5174\u7a0b\u5ea6\u5bf9\u97f3\u4e50\u5bb6\u60c5\u611f\u8868\u8fbe\u4e0e\u542c\u4f17\u53cd\u5e94\u7684\u5f71\u54cd\u3002", "method": "\u7ed3\u5408\u8ba1\u7b97\u97f3\u9891\u5206\u6790\u548c\u795e\u7ecf\u751f\u7406\u5b66\u6d4b\u91cf\uff0c\u7814\u7a76\u4e13\u4e1a\u97f3\u4e50\u5bb6\u5728\u591a\u79cd\u4efb\u52a1\uff08\u5982\u56fa\u5b9a\u66f2\u76ee\u3001\u5373\u5174\uff09\u4e2d\u7684\u8868\u73b0\u53ca\u542c\u4f17\u7684\u60c5\u611f\u53cd\u9988\u3002", "result": "\u8868\u73b0\u529b\u548c\u5373\u5174\u8868\u6f14\u5177\u6709\u72ec\u7279\u7684\u58f0\u5b66\u7279\u5f81\uff0c\u5f15\u53d1\u66f4\u5f3a\u60c5\u611f\u53cd\u5e94\uff1b\u5373\u5174\u8868\u6f14\u80fd\u663e\u8457\u63d0\u9ad8\u542c\u4f17\u7684\u653e\u677e\u611f\u3002", "conclusion": "\u8868\u73b0\u529b\u662f\u63d0\u5347\u60c5\u611f\u6c9f\u901a\u4e0e\u89c2\u4f17\u53c2\u4e0e\u7684\u5173\u952e\u56e0\u7d20\u3002", "keywords": "\u97f3\u4e50\u8868\u6f14, \u60c5\u611f\u8868\u8fbe, \u8ba1\u7b97\u5206\u6790, \u795e\u7ecf\u751f\u7406\u5b66, \u5373\u5174\u8868\u6f14"}}
{"id": "2506.02323", "pdf": "https://arxiv.org/pdf/2506.02323", "abs": "https://arxiv.org/abs/2506.02323", "authors": ["Aleix Boquet-Pujadas", "Pol del Aguila Pla", "Michael Unser"], "title": "Sensitivity-Aware Density Estimation in Multiple Dimensions", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.DS", "eess.SP"], "comment": null, "summary": "We formulate an optimization problem to estimate probability densities in the\ncontext of multidimensional problems that are sampled with uneven probability.\nIt considers detector sensitivity as an heterogeneous density and takes\nadvantage of the computational speed and flexible boundary conditions offered\nby splines on a grid. We choose to regularize the Hessian of the spline via the\nnuclear norm to promote sparsity. As a result, the method is spatially adaptive\nand stable against the choice of the regularization parameter, which plays the\nrole of the bandwidth. We test our computational pipeline on standard densities\nand provide software. We also present a new approach to PET rebinning as an\napplication of our framework.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u591a\u7ef4\u95ee\u9898\u4e2d\u975e\u5747\u5300\u91c7\u6837\u4e0b\u7684\u6982\u7387\u5bc6\u5ea6\uff0c\u5229\u7528\u7f51\u683c\u4e0a\u7684\u6837\u6761\u51fd\u6570\u4e0e\u6838\u8303\u6570\u6b63\u5219\u5316\u63d0\u5347\u7a00\u758f\u6027\uff0c\u65b9\u6cd5\u5177\u6709\u7a7a\u95f4\u9002\u5e94\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u5e76\u5728PET\u91cd\u5206\u6846\u4e2d\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u5904\u7406\u591a\u7ef4\u95ee\u9898\u4e2d\u975e\u5747\u5300\u91c7\u6837\u5bfc\u81f4\u7684\u6982\u7387\u5bc6\u5ea6\u4f30\u8ba1\u96be\u9898\uff0c\u5c24\u5176\u662f\u5728\u63a2\u6d4b\u5668\u7075\u654f\u5ea6\u4e0d\u5747\u5300\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7f51\u683c\u4e0a\u7684\u6837\u6761\u51fd\u6570\u8ba1\u7b97\u6982\u7387\u5bc6\u5ea6\uff0c\u901a\u8fc7\u6838\u8303\u6570\u6b63\u5219\u5316Hessian\u77e9\u9635\u4ee5\u4fc3\u8fdb\u7a00\u758f\u6027\uff0c\u5b9e\u73b0\u7a7a\u95f4\u9002\u5e94\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6807\u51c6\u5bc6\u5ea6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u7a33\u5b9a\uff0c\u5bf9\u6b63\u5219\u5316\u53c2\u6570\u7684\u9009\u62e9\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5173\u8f6f\u4ef6\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6982\u7387\u5bc6\u5ea6\u4f30\u8ba1\u548cPET\u91cd\u5206\u6846\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\u3002", "keywords": "\u6982\u7387\u5bc6\u5ea6\u4f30\u8ba1, \u975e\u5747\u5300\u91c7\u6837, \u6837\u6761\u51fd\u6570, \u6838\u8303\u6570, PET\u91cd\u5206\u6846"}}
{"id": "2506.02584", "pdf": "https://arxiv.org/pdf/2506.02584", "abs": "https://arxiv.org/abs/2506.02584", "authors": ["Sarenne Wallbridge", "Christoph Minixhofer", "Catherine Lai", "Peter Bell"], "title": "Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "People exploit the predictability of lexical structures during text\ncomprehension. Though predictable structure is also present in speech, the\ndegree to which prosody, e.g. intonation, tempo, and loudness, contributes to\nsuch structure independently of the lexical content is unclear. This study\nleverages self-supervised learning (SSL) to examine the temporal granularity of\nstructures in the acoustic correlates of prosody. Representations from our\nproposed Masked Prosody Model can predict perceptual labels dependent on local\ninformation, such as word boundaries, but provide the most value for labels\ninvolving longer-term structures, like emotion recognition. Probing experiments\nacross various perceptual labels show strong relative gains over untransformed\npitch, energy, and voice activity features. Our results reveal the importance\nof SSL training objective timescale and highlight the value of complex\nSSL-encoded structures compared to more constrained classical structures.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u97f5\u5f8b\uff08\u5982\u8bed\u8c03\u3001\u8282\u594f\u548c\u97f3\u91cf\uff09\u5bf9\u8bed\u97f3\u4e2d\u53ef\u9884\u6d4b\u7ed3\u6784\u7684\u8d21\u732e\uff0c\u5e76\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u5206\u6790\u4e86\u5176\u65f6\u95f4\u7c92\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u8bed\u97f3\u4e2d\u5b58\u5728\u53ef\u9884\u6d4b\u7684\u7ed3\u6784\uff0c\u4f46\u97f5\u5f8b\u5982\u4f55\u72ec\u7acb\u4e8e\u8bcd\u6c47\u5185\u5bb9\u5f71\u54cd\u8fd9\u4e9b\u7ed3\u6784\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u7528\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u548c\u63d0\u51fa\u7684Masked Prosody Model\uff0c\u7814\u7a76\u97f5\u5f8b\u7684\u58f0\u5b66\u76f8\u5173\u7ed3\u6784\u7684\u65f6\u95f4\u7c92\u5ea6\u3002", "result": "\u6a21\u578b\u80fd\u591f\u9884\u6d4b\u4f9d\u8d56\u5c40\u90e8\u4fe1\u606f\u7684\u611f\u77e5\u6807\u7b7e\uff08\u5982\u8bcd\u8fb9\u754c\uff09\uff0c\u5e76\u5c24\u5176\u64c5\u957f\u5904\u7406\u6d89\u53ca\u957f\u671f\u7ed3\u6784\u7684\u6807\u7b7e\uff08\u5982\u60c5\u611f\u8bc6\u522b\uff09\u3002", "conclusion": "\u7ed3\u679c\u5f3a\u8c03\u4e86SSL\u8bad\u7ec3\u76ee\u6807\u65f6\u95f4\u5c3a\u5ea6\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u590d\u6742SSL\u7f16\u7801\u7ed3\u6784\u76f8\u5bf9\u4e8e\u4f20\u7edf\u7ed3\u6784\u7684\u4f18\u52bf\u3002", "keywords": "\u97f5\u5f8b, \u81ea\u76d1\u7763\u5b66\u4e60, \u8bed\u97f3\u5904\u7406, \u58f0\u5b66\u7279\u5f81, \u60c5\u611f\u8bc6\u522b"}}
{"id": "2506.02337", "pdf": "https://arxiv.org/pdf/2506.02337", "abs": "https://arxiv.org/abs/2506.02337", "authors": ["Adrienne M. Propp", "Jonas A. Actor", "Elise Walker", "Houman Owhadi", "Nathaniel Trask", "Daniel M. Tartakovsky"], "title": "Discovery of Probabilistic Dirichlet-to-Neumann Maps on Graphs", "categories": ["cs.LG", "cs.NA", "math-ph", "math.MP", "math.NA", "physics.comp-ph", "stat.ML", "90C70, 60G15, 05C90"], "comment": null, "summary": "Dirichlet-to-Neumann maps enable the coupling of multiphysics simulations\nacross computational subdomains by ensuring continuity of state variables and\nfluxes at artificial interfaces. We present a novel method for learning\nDirichlet-to-Neumann maps on graphs using Gaussian processes, specifically for\nproblems where the data obey a conservation constraint from an underlying\npartial differential equation. Our approach combines discrete exterior calculus\nand nonlinear optimal recovery to infer relationships between vertex and edge\nvalues. This framework yields data-driven predictions with uncertainty\nquantification across the entire graph, even when observations are limited to a\nsubset of vertices and edges. By optimizing over the reproducing kernel Hilbert\nspace norm while applying a maximum likelihood estimation penalty on kernel\ncomplexity, our method ensures that the resulting surrogate strictly enforces\nconservation laws without overfitting. We demonstrate our method on two\nrepresentative applications: subsurface fracture networks and arterial blood\nflow. Our results show that the method maintains high accuracy and\nwell-calibrated uncertainty estimates even under severe data scarcity,\nhighlighting its potential for scientific applications where limited data and\nreliable uncertainty quantification are critical.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u7684\u56fe\u4e0a\u7684Dirichlet-to-Neumann\u6620\u5c04\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u79bb\u6563\u5916\u5fae\u79ef\u5206\u548c\u975e\u7ebf\u6027\u6700\u4f18\u6062\u590d\u6280\u672f\uff0c\u786e\u4fdd\u5b88\u6052\u5f8b\u5e76\u907f\u514d\u8fc7\u62df\u5408\uff0c\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u573a\u666f\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7Dirichlet-to-Neumann\u6620\u5c04\u5b9e\u73b0\u591a\u7269\u7406\u573a\u4eff\u771f\u4e2d\u7684\u5b50\u57df\u8026\u5408\uff0c\u786e\u4fdd\u72b6\u6001\u53d8\u91cf\u548c\u901a\u91cf\u5728\u4eba\u5de5\u754c\u9762\u4e0a\u7684\u8fde\u7eed\u6027\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u8fc7\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u79bb\u6563\u5916\u5fae\u79ef\u5206\u548c\u975e\u7ebf\u6027\u6700\u4f18\u6062\u590d\u6280\u672f\uff0c\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u4f18\u5316\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u8303\u6570\u3002", "result": "\u5728\u5b50\u8868\u9762\u88c2\u7f1d\u7f51\u7edc\u548c\u52a8\u8109\u8840\u6d41\u4e24\u4e2a\u5e94\u7528\u4e2d\uff0c\u65b9\u6cd5\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u826f\u597d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u4e14\u9700\u53ef\u9760\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u79d1\u5b66\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "keywords": "Dirichlet-to-Neumann\u6620\u5c04, \u9ad8\u65af\u8fc7\u7a0b, \u79bb\u6563\u5916\u5fae\u79ef\u5206, \u975e\u7ebf\u6027\u6700\u4f18\u6062\u590d, \u5b88\u6052\u5f8b"}}
{"id": "2506.02589", "pdf": "https://arxiv.org/pdf/2506.02589", "abs": "https://arxiv.org/abs/2506.02589", "authors": ["Maria Levchenko"], "title": "Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7; H.3.3"], "comment": null, "summary": "This paper addresses the challenge of Named Entity Recognition (NER) for\nperson names within the specialized domain of Russian news texts concerning\ncultural events. The study utilizes the unique SPbLitGuide dataset, a\ncollection of event announcements from Saint Petersburg spanning 1999 to 2019.\nA comparative evaluation of diverse NER models is presented, encompassing\nestablished transformer-based architectures such as DeepPavlov, RoBERTa, and\nSpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4,\nand GPT-4o. Key findings highlight the superior performance of GPT-4o when\nprovided with specific prompting for JSON output, achieving an F1 score of\n0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The\nresearch contributes to a deeper understanding of current NER model\ncapabilities and limitations when applied to morphologically rich languages\nlike Russian within the cultural heritage domain, offering insights for\nresearchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025)\nachieves F1=0.94 for both simple and structured prompts, demonstrating rapid\nprogress across model families and simplified deployment requirements.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5728\u4fc4\u7f57\u65af\u6587\u5316\u65b0\u95fb\u6587\u672c\u4e2d\u6bd4\u8f83\u591a\u79cdNER\u6a21\u578b\uff0c\u53d1\u73b0GPT-4o\u5728\u7279\u5b9a\u63d0\u793a\u4e0b\u8868\u73b0\u6700\u4f73\uff0cF1\u5206\u6570\u8fbe0.93\uff0cGPT-4\u7cbe\u5ea6\u6700\u9ad8\u4e3a0.99\u3002\u7814\u7a76\u8fd8\u5c55\u793a\u4e86\u6a21\u578b\u5728\u4fc4\u8bed\u7b49\u590d\u6742\u8bed\u8a00\u4e2d\u7684\u6027\u80fd\u8fdb\u5c55\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4fc4\u8bed\u6587\u5316\u65b0\u95fb\u6587\u672c\u4e2d\u4eba\u540d\u8bc6\u522b\u7684\u6311\u6218\uff0c\u8bc4\u4f30\u5e76\u6bd4\u8f83\u4e0d\u540cNER\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4ee5\u4e86\u89e3\u5176\u5728\u5f62\u6001\u590d\u6742\u8bed\u8a00\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u4f7f\u7528SPbLitGuide\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86DeepPavlov\u3001RoBERTa\u3001SpaCy\u7b49\u4f20\u7edf\u6a21\u578b\u4e0eGPT-3.5\u3001GPT-4\u3001GPT-4o\u7b49LLM\u6a21\u578b\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8JSON\u63d0\u793a\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "GPT-4o\u5728JSON\u63d0\u793a\u4e0bF1\u5206\u6570\u8fbe0.93\uff0cGPT-4\u7cbe\u5ea6\u6700\u9ad8\u4e3a0.99\u3002GPT-4.1\u5728\u540e\u7eed\u6d4b\u8bd5\u4e2d\u8868\u73b0\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u8868\u660eLLM\u5c24\u5176\u9002\u5408\u4fc4\u8bed\u7b49\u590d\u6742\u8bed\u8a00\u7684NER\u4efb\u52a1\uff0c\u63d0\u793a\u65b9\u5f0f\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0c\u6a21\u578b\u6539\u8fdb\u901f\u5ea6\u5feb\u3002", "keywords": "Named Entity Recognition, \u4fc4\u8bed, \u6587\u5316\u65b0\u95fb, GPT-4o, \u6027\u80fd\u8bc4\u4f30"}}
{"id": "2506.02355", "pdf": "https://arxiv.org/pdf/2506.02355", "abs": "https://arxiv.org/abs/2506.02355", "authors": ["Andre He", "Daniel Fried", "Sean Welleck"], "title": "Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning has emerged as an effective framework for training\nlarge language models on structured language-conditioned tasks. We identify a\ncritical flaw of Group Relative Policy Optimization (GRPO), a widely used RL\nalgorithm in this setting. For tasks that require multi-sample performance,\nsuch as formal theorem proving, GRPO biasedly reinforces already probable\nsolutions and neglects rare but correct proofs. This implicit bias impairs\nperformance on pass@$N$ metrics at large sample sizes, limiting its\npracticality for training theorem provers. To address this, we introduce the\nunlikeliness reward, a straightforward method that explicitly encourages\nreinforcing rare correct solutions. Additionally, we find that increasing the\nnumber of PPO epochs further mitigates this bias. Our experiments confirm that\nincorporating the unlikeliness reward significantly improves pass@$N$ across a\nlarge range of N, outperforming standard GRPO and substantially increasing\nsample diversity. Applying our revised recipe to Lean, we achieve competitive\nperformance with DeepSeek-Prover-V1.5-RL on the miniF2F-test benchmark. We\nrelease our implementation, providing a simple yet effective recipe for\ntraining formal theorem provers with RL.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0GRPO\u7b97\u6cd5\u5728\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u5b58\u5728\u504f\u5411\u6027\uff0c\u63d0\u51fa\u7528\u2018unlikeliness reward\u2019\u6539\u5584\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u63d0\u5347\u591a\u6837\u6027\u548c\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3GRPO\u7b97\u6cd5\u5728\u4efb\u52a1\u4e2d\u504f\u5411\u5e38\u89c1\u89e3\u800c\u5ffd\u7565\u7f55\u89c1\u89e3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u591a\u6837\u672c\u6027\u80fd\u3002", "method": "\u5f15\u5165\u2018unlikeness reward\u2019\u5e76\u589e\u52a0PPO\u8bad\u7ec3\u8f6e\u6b21\uff0c\u4ee5\u5f3a\u5316\u7f55\u89c1\u6b63\u786e\u89e3\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6539\u8fdb\u540e\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347pass@N\u6027\u80fd\uff0c\u5e76\u5728\u5b9a\u7406\u8bc1\u660e\u4efb\u52a1\u4e2d\u8fbe\u5230\u7ade\u4e89\u6027\u6c34\u5e73\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u9002\u7528\u4e8e\u8bad\u7ec3\u5b9a\u7406\u8bc1\u660e\u5668\u7b49\u4efb\u52a1\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60,GRPO,unlikeness reward,\u5b9a\u7406\u8bc1\u660e,pass@N"}}
{"id": "2506.02591", "pdf": "https://arxiv.org/pdf/2506.02591", "abs": "https://arxiv.org/abs/2506.02591", "authors": ["Minh Duc Bui", "Kyung Eun Park", "Goran Glava\u0161", "Fabian David Schmidt", "Katharina von der Wense"], "title": "On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main (Camera-Ready Version)", "summary": "Measurement systems (e.g., currencies) differ across cultures, but the\nconversions between them are well defined so that humans can state facts using\nany measurement system of their choice. Being available to users from diverse\ncultural backgrounds, large language models (LLMs) should also be able to\nprovide accurate information irrespective of the measurement system at hand.\nUsing newly compiled datasets we test if this is the case for seven open-source\nLLMs, addressing three key research questions: (RQ1) What is the default system\nused by LLMs for each type of measurement? (RQ2) Do LLMs' answers and their\naccuracy vary across different measurement systems? (RQ3) Can LLMs mitigate\npotential challenges w.r.t. underrepresented systems via reasoning? Our\nfindings show that LLMs default to the measurement system predominantly used in\nthe data. Additionally, we observe considerable instability and variance in\nperformance across different measurement systems. While this instability can in\npart be mitigated by employing reasoning methods such as chain-of-thought\n(CoT), this implies longer responses and thereby significantly increases\ntest-time compute (and inference costs), marginalizing users from cultural\nbackgrounds that use underrepresented measurement systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e0d\u540c\u6d4b\u91cf\u7cfb\u7edf\uff08\u5982\u8d27\u5e01\u5355\u4f4d\uff09\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u503e\u5411\u4e8e\u9ed8\u8ba4\u4f7f\u7528\u6570\u636e\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\u7684\u6d4b\u91cf\u7cfb\u7edf\uff0c\u4e14\u51c6\u786e\u6027\u548c\u6027\u80fd\u5728\u4e0d\u540c\u7cfb\u7edf\u4e2d\u5b58\u5728\u663e\u8457\u6ce2\u52a8\u3002\u63a8\u7406\u65b9\u6cd5\uff08\u5982\u94fe\u5f0f\u601d\u8003\uff09\u53ef\u4ee5\u7f13\u89e3\u95ee\u9898\uff0c\u4f46\u4f1a\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u548c\u54cd\u5e94\u65f6\u95f4\u3002", "motivation": "\u63a2\u7d22LLMs\u80fd\u5426\u5728\u4e0d\u540c\u6587\u5316\u548c\u6d4b\u91cf\u7cfb\u7edf\u4e2d\u63d0\u4f9b\u51c6\u786e\u4fe1\u606f\uff0c\u4ee5\u670d\u52a1\u591a\u6837\u5316\u7684\u7528\u6237\u7fa4\u4f53\u3002", "method": "\u901a\u8fc7\u65b0\u7f16\u8bd1\u7684\u6570\u636e\u96c6\u6d4b\u8bd5\u4e03\u4e2a\u5f00\u6e90LLM\uff0c\u56de\u7b54\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u9ed8\u8ba4\u6d4b\u91cf\u7cfb\u7edf\u3001\u51c6\u786e\u6027\u5dee\u5f02\u3001\u4ee5\u53ca\u901a\u8fc7\u63a8\u7406\u7f13\u89e3\u6311\u6218\u7684\u80fd\u529b\u3002", "result": "LLMs\u9ed8\u8ba4\u4f7f\u7528\u6570\u636e\u4e2d\u7684\u4e3b\u5bfc\u6d4b\u91cf\u7cfb\u7edf\uff0c\u6027\u80fd\u5728\u4e0d\u540c\u7cfb\u7edf\u4e2d\u4e0d\u7a33\u5b9a\u3002\u63a8\u7406\u65b9\u6cd5\u53ef\u6539\u5584\u51c6\u786e\u6027\uff0c\u4f46\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "LLMs\u5728\u4e0d\u540c\u6d4b\u91cf\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\u5b58\u5728\u504f\u5dee\uff0c\u63a8\u7406\u65b9\u6cd5\u867d\u6709\u6548\u4f46\u6210\u672c\u9ad8\uff0c\u53ef\u80fd\u8fb9\u7f18\u5316\u4f7f\u7528\u975e\u4e3b\u6d41\u6d4b\u91cf\u7cfb\u7edf\u7684\u7528\u6237\u3002", "keywords": "\u6d4b\u91cf\u7cfb\u7edf\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6587\u5316\u591a\u6837\u6027\uff0c\u51c6\u786e\u6027\uff0c\u63a8\u7406\u65b9\u6cd5"}}
{"id": "2506.02357", "pdf": "https://arxiv.org/pdf/2506.02357", "abs": "https://arxiv.org/abs/2506.02357", "authors": ["Ram Potham"], "title": "Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": "Preprint. This work has been submitted to the Technical AI Governance\n  Workshop at ICML 2025 for review", "summary": "Credible safety plans for advanced AI development require methods to verify\nagent behavior and detect potential control deficiencies early. A fundamental\naspect is ensuring agents adhere to safety-critical principles, especially when\nthese conflict with operational goals. Failure to prioritize such principles\nindicates a potential basic control failure. This paper introduces a\nlightweight, interpretable benchmark methodology using a simple grid world to\nevaluate an LLM agent's ability to uphold a predefined, high-level safety\nprinciple (e.g., \"never enter hazardous zones\") when faced with conflicting\nlower-level task instructions. We probe whether the agent reliably prioritizes\nthe inviolable directive, testing a foundational controllability aspect of\nLLMs. This pilot study demonstrates the methodology's feasibility, offers\npreliminary insights into agent behavior under principle conflict, and\ndiscusses how such benchmarks can contribute empirical evidence for assessing\ncontrollability. We argue that evaluating adherence to hierarchical principles\nis a crucial early step in understanding our capacity to build governable AI\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u7684\u57fa\u51c6\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u9ad8\u5c42\u6b21\u5b89\u5168\u539f\u5219\u4e0e\u4efb\u52a1\u6307\u4ee4\u51b2\u7a81\u65f6\u7684\u884c\u4e3a\u8868\u73b0\uff0c\u521d\u6b65\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u4e3a\u786e\u4fdd\u9ad8\u7ea7AI\u5f00\u53d1\u7684\u5b89\u5168\u53ef\u63a7\uff0c\u9700\u8981\u65e9\u671f\u68c0\u6d4b\u4ee3\u7406\u884c\u4e3a\u662f\u5426\u8fdd\u80cc\u5b89\u5168\u5173\u952e\u539f\u5219\uff0c\u5c24\u5176\u662f\u5728\u8fd9\u4e9b\u539f\u5219\u4e0e\u64cd\u4f5c\u76ee\u6807\u51b2\u7a81\u65f6\u3002", "method": "\u91c7\u7528\u7b80\u5355\u7684\u7f51\u683c\u4e16\u754c\u73af\u5883\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u51c6\u65b9\u6cd5\uff0c\u6d4b\u8bd5LLM\u4ee3\u7406\u5728\u51b2\u7a81\u60c5\u5883\u4e0b\u662f\u5426\u4f18\u5148\u9075\u5b88\u9884\u8bbe\u7684\u9ad8\u5c42\u6b21\u5b89\u5168\u539f\u5219\u3002", "result": "\u521d\u6b65\u7814\u7a76\u8868\u660e\u8be5\u65b9\u6cd5\u53ef\u884c\uff0c\u5e76\u63d0\u4f9b\u4e86\u4ee3\u7406\u5728\u539f\u5219\u51b2\u7a81\u4e0b\u7684\u884c\u4e3a\u6d1e\u5bdf\uff0c\u4e3a\u8bc4\u4f30\u53ef\u63a7\u6027\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002", "conclusion": "\u8bc4\u4f30\u4ee3\u7406\u5bf9\u5c42\u6b21\u5316\u539f\u5219\u7684\u9075\u5b88\u662f\u7406\u89e3\u6784\u5efa\u53ef\u63a7AI\u7cfb\u7edf\u7684\u5173\u952e\u65e9\u671f\u6b65\u9aa4\u3002", "keywords": "AI\u5b89\u5168, \u53ef\u63a7\u6027\u8bc4\u4f30, LLM\u4ee3\u7406, \u5b89\u5168\u539f\u5219, \u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2506.02592", "pdf": "https://arxiv.org/pdf/2506.02592", "abs": "https://arxiv.org/abs/2506.02592", "authors": ["Zhi-Yuan Chen", "Hao Wang", "Xinyu Zhang", "Enrui Hu", "Yankai Lin"], "title": "Beyond the Surface: Measuring Self-Preference in LLM Judgments", "categories": ["cs.CL"], "comment": null, "summary": "Recent studies show that large language models (LLMs) exhibit self-preference\nbias when serving as judges, meaning they tend to favor their own responses\nover those generated by other models. Existing methods typically measure this\nbias by calculating the difference between the scores a judge model assigns to\nits own responses and those it assigns to responses from other models. However,\nthis approach conflates self-preference bias with response quality, as\nhigher-quality responses from the judge model may also lead to positive score\ndifferences, even in the absence of bias. To address this issue, we introduce\ngold judgments as proxies for the actual quality of responses and propose the\nDBG score, which measures self-preference bias as the difference between the\nscores assigned by the judge model to its own responses and the corresponding\ngold judgments. Since gold judgments reflect true response quality, the DBG\nscore mitigates the confounding effect of response quality on bias measurement.\nUsing the DBG score, we conduct comprehensive experiments to assess\nself-preference bias across LLMs of varying versions, sizes, and reasoning\nabilities. Additionally, we investigate two factors that influence and help\nalleviate self-preference bias: response text style and the post-training data\nof judge models. Finally, we explore potential underlying mechanisms of\nself-preference bias from an attention-based perspective. Our code and data are\navailable at https://github.com/zhiyuanc2001/self-preference.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684DBG\u8bc4\u5206\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u9ec4\u91d1\u5224\u65ad\u6765\u66f4\u51c6\u786e\u5730\u8861\u91cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u504f\u597d\u504f\u5dee\u95ee\u9898\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u54cd\u5e94\u8d28\u91cf\u5bf9\u504f\u5dee\u6d4b\u91cf\u7684\u5e72\u6270\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u53d1\u73b0\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u6d4b\u91cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u504f\u597d\u504f\u5dee\u65f6\uff0c\u65e0\u6cd5\u533a\u5206\u504f\u5dee\u4e0e\u54cd\u5e94\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u5bfc\u81f4\u6d4b\u91cf\u4e0d\u51c6\u786e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6848\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\u5f15\u5165\u9ec4\u91d1\u5224\u65ad\u4f5c\u4e3a\u54cd\u5e94\u8d28\u91cf\u7684\u771f\u5b9e\u57fa\u51c6\uff0c\u5e76\u63d0\u51faDBG\u8bc4\u5206\u6765\u8861\u91cf\u81ea\u504f\u597d\u504f\u5dee\uff0c\u540c\u65f6\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e0d\u540c\u6a21\u578b\u7248\u672c\u3001\u89c4\u6a21\u548c\u63a8\u7406\u80fd\u529b\u7684\u504f\u5dee\u60c5\u51b5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cDBG\u8bc4\u5206\u80fd\u6709\u6548\u5206\u79bb\u504f\u5dee\u4e0e\u54cd\u5e94\u8d28\u91cf\u7684\u5f71\u54cd\u3002\u540c\u65f6\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\u54cd\u5e94\u6587\u672c\u98ce\u683c\u548c\u8bad\u7ec3\u540e\u7684\u6570\u636e\u53ef\u4ee5\u5e2e\u52a9\u51cf\u8f7b\u81ea\u504f\u597d\u504f\u5dee\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0cDBG\u8bc4\u5206\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u504f\u597d\u504f\u5dee\u6d4b\u91cf\u65b9\u6cd5\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u5de5\u5177\u3002", "keywords": "\u81ea\u504f\u597d\u504f\u5dee\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001DBG\u8bc4\u5206\u3001\u9ec4\u91d1\u5224\u65ad\u3001\u54cd\u5e94\u8d28\u91cf"}}
{"id": "2506.02370", "pdf": "https://arxiv.org/pdf/2506.02370", "abs": "https://arxiv.org/abs/2506.02370", "authors": ["Zhe Li", "Bicheng Ying", "Zidong Liu", "Chaosheng Dong", "Haibo Yang"], "title": "Reconciling Hessian-Informed Acceleration and Scalar-Only Communication for Efficient Federated Zeroth-Order Fine-Tuning", "categories": ["cs.LG", "cs.DC"], "comment": "Under review", "summary": "Recent dimension-free communication frameworks in Federated Learning (FL),\nsuch as DeComFL, significantly reduce per-round communication by transmitting\nonly scalars via zeroth-order stochastic gradient descent (ZO-SGD). This method\nis particularly advantageous for federated fine-tuning of Large Language Models\n(LLMs). Yet, the high variance in ZO gradient estimation typically leads to\nslow convergence. Although leveraging Hessian information is known to enhance\noptimization speed, integrating this into FL presents significant challenges.\nThese include clients' restrictions on local data and the critical need to\nmaintain the dimension-free communication property. To overcome this\nlimitation, we first introduce a generalized scalar-only communication FL\nframework that decouples dimension-free communication from standard ZO-SGD,\nenabling the integration of more advanced optimization strategies. Building on\nthis framework, we propose HiSo, a fast federated fine-tuning method via\nHessian-informed zeroth-order optimization and Scalar-only communication.\nSpecifically, it leverages global curvature information to accelerate\nconvergence while preserving the same minimal communication cost per round.\nTheoretically, we establish convergence guarantees that are independent of the\nglobal Lipschitz constant, and further show that HiSo achieves faster rates\nwhen the global Hessian exhibits a low effective rank -- a common phenomenon in\nLLMs. Extensive experiments on benchmark datasets and LLM fine-tuning tasks\nconfirm that HiSo significantly outperforms existing ZO-based FL methods in\nboth convergence speed and communication efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHessian\u4fe1\u606f\u7684\u96f6\u9636\u4f18\u5316\u65b9\u6cd5HiSo\uff0c\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\u548c\u901a\u4fe1\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u96f6\u9636\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982DeComFL\uff09\u5728\u5927\u6a21\u578b\u5fae\u8c03\u4e2d\u901a\u4fe1\u6210\u672c\u4f4e\uff0c\u4f46\u68af\u5ea6\u4f30\u8ba1\u65b9\u5dee\u9ad8\u5bfc\u81f4\u6536\u655b\u6162\u3002\u5229\u7528Hessian\u4fe1\u606f\u80fd\u52a0\u901f\u4f18\u5316\uff0c\u4f46\u5c06\u5176\u878d\u5165\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u6570\u636e\u672c\u5730\u5316\u548c\u901a\u4fe1\u7ef4\u5ea6\u9650\u5236\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faHiSo\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u7ef4\u5ea6\u65e0\u5173\u901a\u4fe1\u4e0e\u96f6\u9636\u4f18\u5316\uff0c\u7ed3\u5408\u5168\u5c40\u66f2\u7387\u4fe1\u606f\u52a0\u901f\u6536\u655b\uff0c\u540c\u65f6\u4fdd\u6301\u6bcf\u8f6e\u901a\u4fe1\u6210\u672c\u6700\u5c0f\u3002", "result": "\u7406\u8bba\u8bc1\u660eHiSo\u6536\u655b\u901f\u5ea6\u4e0e\u5168\u5c40Hessian\u77e9\u9635\u4f4e\u6709\u6548\u79e9\u76f8\u5173\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u5927\u6a21\u578b\u5fae\u8c03\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u96f6\u9636\u65b9\u6cd5\u3002", "conclusion": "HiSo\u5728\u4fdd\u6301\u4f4e\u901a\u4fe1\u6210\u672c\u7684\u540c\u65f6\uff0c\u901a\u8fc7Hessian\u4fe1\u606f\u52a0\u901f\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u5927\u6a21\u578b\u7684\u5fae\u8c03\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60\uff1b\u96f6\u9636\u4f18\u5316\uff1bHessian\u4fe1\u606f\uff1b\u5927\u8bed\u8a00\u6a21\u578b\uff1b\u901a\u4fe1\u6548\u7387"}}
{"id": "2506.02596", "pdf": "https://arxiv.org/pdf/2506.02596", "abs": "https://arxiv.org/abs/2506.02596", "authors": ["Fan Gao", "Dongyuan Li", "Ding Xia", "Fei Mi", "Yasheng Wang", "Lifeng Shang", "Baojun Wang"], "title": "EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chinese essay writing and its evaluation are critical in educational\ncontexts, yet the capabilities of Large Language Models (LLMs) in this domain\nremain largely underexplored. Existing benchmarks often rely on coarse-grained\ntext quality metrics, largely overlooking the structural and rhetorical\ncomplexities of Chinese essays, particularly across diverse genres. To address\nthis gap, we propose \\benchName, a multi-genre benchmark specifically designed\nfor Chinese essay writing across four major genres: Argumentative, Narrative,\nDescriptive, and Expository. We curate and refine a total of 728 real-world\nprompts to ensure authenticity and meticulously categorize them into the\n\\textit{Open-Ended} and \\textit{Constrained} sets to capture diverse writing\nscenarios. To reliably evaluate generated essays, we develop a fine-grained,\ngenre-specific scoring framework that hierarchically aggregates scores. We\nfurther validate our evaluation protocol through a comprehensive human\nagreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their\nstrengths and limitations across genres and instruction types. With \\benchName,\nwe aim to advance LLM-based Chinese essay evaluation and inspire future\nresearch on improving essay generation in educational settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u7c7b\u578b\u7684\u4e2d\u6587\u5199\u4f5c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u6587\u4f53\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u4e2d\u6587\u5199\u4f5c\u8bc4\u4f30\u4e2d\u5ffd\u89c6\u4e86\u7ed3\u6784\u548c\u4fee\u8f9e\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u5728\u591a\u6587\u4f53\u80cc\u666f\u4e0b\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b728\u4e2a\u771f\u5b9e\u63d0\u793a\u7684\u591a\u6587\u4f53\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5f00\u53d1\u4e86\u7ec6\u7c92\u5ea6\u8bc4\u5206\u6846\u67b6\uff0c\u5e76\u8fdb\u884c\u4eba\u7c7b\u8bc4\u4f30\u9a8c\u8bc1\u3002", "result": "\u6d4b\u8bd5\u4e8615\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u6587\u4f53\u548c\u6307\u4ee4\u7c7b\u578b\u4e2d\u7684\u4f18\u52a3\u52bf\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u6709\u52a9\u4e8e\u63a8\u52a8\u4e2d\u6587\u5199\u4f5c\u8bc4\u4f30\u7684\u7814\u7a76\uff0c\u5e76\u542f\u53d1\u6559\u80b2\u573a\u666f\u4e0b\u7684\u5199\u4f5c\u751f\u6210\u6539\u8fdb\u3002", "keywords": "\u4e2d\u6587\u5199\u4f5c\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u591a\u6587\u4f53\u8bc4\u4f30\uff0c\u6559\u80b2\u6280\u672f"}}
{"id": "2506.02371", "pdf": "https://arxiv.org/pdf/2506.02371", "abs": "https://arxiv.org/abs/2506.02371", "authors": ["Haoye Lu", "Darren Lo", "Yaoliang Yu"], "title": "SFBD Flow: A Continuous-Optimization Framework for Training Diffusion Models with Noisy Samples", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion models achieve strong generative performance but often rely on\nlarge datasets that may include sensitive content. This challenge is compounded\nby the models' tendency to memorize training data, raising privacy concerns.\nSFBD (Lu et al., 2025) addresses this by training on corrupted data and using\nlimited clean samples to capture local structure and improve convergence.\nHowever, its iterative denoising and fine-tuning loop requires manual\ncoordination, making it burdensome to implement. We reinterpret SFBD as an\nalternating projection algorithm and introduce a continuous variant, SFBD flow,\nthat removes the need for alternating steps. We further show its connection to\nconsistency constraint-based methods, and demonstrate that its practical\ninstantiation, Online SFBD, consistently outperforms strong baselines across\nbenchmarks.", "AI": {"tldr": "SFBD flow\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fde\u7eed\u6295\u5f71\u7b97\u6cd5\u907f\u514d\u4ea4\u66ff\u6b65\u9aa4\uff0c\u63d0\u5347\u9690\u79c1\u4fdd\u62a4\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u6027\u80fd\u5f3a\u7684\u540c\u65f6\u53ef\u80fd\u6cc4\u9732\u654f\u611f\u6570\u636e\uff0cSFBD\u901a\u8fc7\u5c40\u90e8\u6570\u636e\u8bad\u7ec3\u51cf\u5c11\u9690\u79c1\u98ce\u9669\uff0c\u4f46\u5176\u5b9e\u73b0\u590d\u6742\u3002", "method": "\u5c06SFBD\u91cd\u65b0\u89e3\u91ca\u4e3a\u4ea4\u66ff\u6295\u5f71\u7b97\u6cd5\uff0c\u63d0\u51fa\u8fde\u7eed\u53d8\u4f53SFBD flow\uff0c\u6d88\u9664\u4ea4\u66ff\u6b65\u9aa4\uff0c\u5e76\u5f15\u5165Online SFBD\u5b9e\u73b0\u3002", "result": "Online SFBD\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SFBD flow\u901a\u8fc7\u8fde\u7eed\u5316\u6539\u8fdb\u8bad\u7ec3\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u3002", "keywords": "\u6269\u6563\u6a21\u578b, \u9690\u79c1\u4fdd\u62a4, SFBD, \u4ea4\u66ff\u6295\u5f71, Online SFBD"}}
{"id": "2506.02627", "pdf": "https://arxiv.org/pdf/2506.02627", "abs": "https://arxiv.org/abs/2506.02627", "authors": ["\u00d6mer Tarik \u00d6zyilmaz", "Matt Coler", "Matias Valdenegro-Toro"], "title": "Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Although commercial Arabic automatic speech recognition (ASR) systems support\nModern Standard Arabic (MSA), they struggle with dialectal speech. We\ninvestigate the effect of fine-tuning OpenAI's Whisper on five major Arabic\ndialects (Gulf, Levantine, Iraqi, Egyptian, Maghrebi) using Mozilla Common\nVoice for MSA and the MASC dataset for dialectal speech. We evaluate MSA\ntraining size effects, benefits of pre-training on MSA data, and\ndialect-specific versus dialect-pooled models. We find that small amounts of\nMSA fine-tuning data yield substantial improvements for smaller models,\nmatching larger non-fine-tuned models. While MSA pre-training shows minimal\nbenefit, suggesting limited shared features between MSA and dialects, our\ndialect-pooled models perform comparably to dialect-specific ones. This\nindicates that pooling dialectal data, when properly balanced, can help address\ndata scarcity in low-resource ASR without significant performance loss.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00ASR\u4e2d\u5fae\u8c03Whisper\u6a21\u578b\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5c11\u91cfMSA\u6570\u636e\u5373\u53ef\u663e\u8457\u63d0\u5347\u5c0f\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u65b9\u8a00\u5408\u5e76\u6a21\u578b\u6548\u679c\u63a5\u8fd1\u65b9\u8a00\u4e13\u7528\u6a21\u578b\u3002", "motivation": "\u5546\u4e1aASR\u7cfb\u7edf\u5bf9\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u652f\u6301\u4e0d\u8db3\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5fae\u8c03\u63d0\u5347\u65b9\u8a00\u8bc6\u522b\u6548\u679c\u3002", "method": "\u4f7f\u7528Mozilla Common Voice\u548cMASC\u6570\u636e\u96c6\uff0c\u5bf9\u6bd4MSA\u5fae\u8c03\u6570\u636e\u91cf\u3001\u9884\u8bad\u7ec3\u6548\u679c\u53ca\u65b9\u8a00\u5408\u5e76\u4e0e\u4e13\u7528\u6a21\u578b\u3002", "result": "\u5c11\u91cfMSA\u5fae\u8c03\u6570\u636e\u80fd\u663e\u8457\u63d0\u5347\u5c0f\u6a21\u578b\u6027\u80fd\uff0c\u65b9\u8a00\u5408\u5e76\u6a21\u578b\u8868\u73b0\u63a5\u8fd1\u4e13\u7528\u6a21\u578b\u3002", "conclusion": "\u5408\u7406\u5e73\u8861\u7684\u65b9\u8a00\u6570\u636e\u5408\u5e76\u53ef\u89e3\u51b3\u4f4e\u8d44\u6e90ASR\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e14\u6027\u80fd\u635f\u5931\u5c0f\u3002", "keywords": "ASR, Whisper, \u963f\u62c9\u4f2f\u8bed\u65b9\u8a00, MSA, \u5fae\u8c03, \u6570\u636e\u7a00\u7f3a"}}
{"id": "2506.02385", "pdf": "https://arxiv.org/pdf/2506.02385", "abs": "https://arxiv.org/abs/2506.02385", "authors": ["Shuze Chen", "Tianyi Peng"], "title": "Multi-agent Markov Entanglement", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Value decomposition has long been a fundamental technique in multi-agent\ndynamic programming and reinforcement learning (RL). Specifically, the value\nfunction of a global state $(s_1,s_2,\\ldots,s_N)$ is often approximated as the\nsum of local functions: $V(s_1,s_2,\\ldots,s_N)\\approx\\sum_{i=1}^N V_i(s_i)$.\nThis approach traces back to the index policy in restless multi-armed bandit\nproblems and has found various applications in modern RL systems. However, the\ntheoretical justification for why this decomposition works so effectively\nremains underexplored.\n  In this paper, we uncover the underlying mathematical structure that enables\nvalue decomposition. We demonstrate that a multi-agent Markov decision process\n(MDP) permits value decomposition if and only if its transition matrix is not\n\"entangled\" -- a concept analogous to quantum entanglement in quantum physics.\nDrawing inspiration from how physicists measure quantum entanglement, we\nintroduce how to measure the \"Markov entanglement\" for multi-agent MDPs and\nshow that this measure can be used to bound the decomposition error in general\nmulti-agent MDPs.\n  Using the concept of Markov entanglement, we proved that a widely-used class\nof index policies is weakly entangled and enjoys a sublinear $\\mathcal\nO(\\sqrt{N})$ scale of decomposition error for $N$-agent systems. Finally, we\nshow how Markov entanglement can be efficiently estimated in practice,\nproviding practitioners with an empirical proxy for the quality of value\ndecomposition.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86\u503c\u5206\u89e3\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6570\u5b66\u57fa\u7840\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u9a6c\u5c14\u53ef\u592b\u7ea0\u7f20\u201d\u7684\u65b0\u6982\u5ff5\uff0c\u7528\u4e8e\u8861\u91cf\u548c\u9650\u5236\u5206\u89e3\u8bef\u5dee\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u7814\u7a76\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u503c\u5206\u89e3\u7684\u7406\u8bba\u57fa\u7840\uff0c\u586b\u8865\u4e86\u5173\u4e8e\u4e3a\u4f55\u8fd9\u79cd\u5206\u89e3\u65b9\u6cd5\u6709\u6548\u7684\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u201c\u9a6c\u5c14\u53ef\u592b\u7ea0\u7f20\u201d\u6982\u5ff5\uff0c\u5e76\u4ece\u91cf\u5b50\u7269\u7406\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8861\u91cf\u591a\u667a\u80fd\u4f53MDP\u4e2d\u5206\u89e3\u8bef\u5dee\u7684\u65b9\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684\u7d22\u5f15\u7b56\u7565\u5177\u6709\u5f31\u7ea0\u7f20\u6027\uff0c\u5206\u89e3\u8bef\u5dee\u4e0e\u667a\u80fd\u4f53\u6570\u91cf\u7684\u5e73\u65b9\u6839\u6210\u6b63\u6bd4\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u9645\u4e2d\u6709\u6548\u4f30\u8ba1\u7ea0\u7f20\u5ea6\u7684\u65b9\u6cd5\u3002", "conclusion": "\u503c\u5206\u89e3\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u6709\u6548\u7684\u539f\u56e0\u5728\u4e8e\u5176\u4f4e\u9a6c\u5c14\u53ef\u592b\u7ea0\u7f20\u5ea6\uff0c\u8be5\u7814\u7a76\u4e3a\u5b9e\u8df5\u4e2d\u7684\u5206\u89e3\u8d28\u91cf\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "keywords": "\u503c\u5206\u89e3, \u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60, \u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b, \u9a6c\u5c14\u53ef\u592b\u7ea0\u7f20, \u7d22\u5f15\u7b56\u7565"}}
{"id": "2506.02659", "pdf": "https://arxiv.org/pdf/2506.02659", "abs": "https://arxiv.org/abs/2506.02659", "authors": ["Manon Reusens", "Bart Baesens", "David Jurgens"], "title": "Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Personalized Large Language Models (LLMs) are increasingly used in diverse\napplications, where they are assigned a specific persona - such as a happy high\nschool teacher - to guide their responses. While prior research has examined\nhow well LLMs adhere to predefined personas in writing style, a comprehensive\nanalysis of consistency across different personas and task types is lacking. In\nthis paper, we introduce a new standardized framework to analyze consistency in\npersona-assigned LLMs. We define consistency as the extent to which a model\nmaintains coherent responses when assigned the same persona across different\ntasks and runs. Our framework evaluates personas across four different\ncategories (happiness, occupation, personality, and political stance) spanning\nmultiple task dimensions (survey writing, essay generation, social media post\ngeneration, single turn, and multi-turn conversations). Our findings reveal\nthat consistency is influenced by multiple factors, including the assigned\npersona, stereotypes, and model design choices. Consistency also varies across\ntasks, increasing with more structured tasks and additional context. All code\nis available on GitHub.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6807\u51c6\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u5206\u914d\u89d2\u8272\u7684LLMs\u7684\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u4e00\u81f4\u6027\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u5e76\u56e0\u4efb\u52a1\u7c7b\u578b\u800c\u5f02\u3002", "motivation": "\u6b64\u524d\u7f3a\u4e4f\u5bf9LLMs\u5728\u4e0d\u540c\u89d2\u8272\u548c\u4efb\u52a1\u7c7b\u578b\u4e2d\u4e00\u81f4\u6027\u7684\u5168\u9762\u5206\u6790\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u6807\u51c6\u5316\u6846\u67b6\uff0c\u8bc4\u4f30LLMs\u5728\u56db\u79cd\u89d2\u8272\u7c7b\u522b\u548c\u591a\u79cd\u4efb\u52a1\u7ef4\u5ea6\u4e2d\u7684\u4e00\u81f4\u6027\u3002", "result": "\u4e00\u81f4\u6027\u53d7\u89d2\u8272\u3001\u523b\u677f\u5370\u8c61\u548c\u6a21\u578b\u8bbe\u8ba1\u9009\u62e9\u5f71\u54cd\uff0c\u7ed3\u6784\u5316\u4efb\u52a1\u548c\u989d\u5916\u4e0a\u4e0b\u6587\u80fd\u63d0\u5347\u4e00\u81f4\u6027\u3002", "conclusion": "\u89d2\u8272\u5206\u914dLLMs\u7684\u4e00\u81f4\u6027\u5206\u6790\u63ed\u793a\u591a\u6837\u5316\u56e0\u7d20\u7684\u4f5c\u7528\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u5de5\u5177\u548c\u6d1e\u89c1\u3002", "keywords": "\u4e2a\u6027\u5316LLM, \u89d2\u8272\u4e00\u81f4\u6027, \u4efb\u52a1\u7c7b\u578b, \u6807\u51c6\u5316\u6846\u67b6, \u6a21\u578b\u8bbe\u8ba1"}}
{"id": "2506.01994", "pdf": "https://arxiv.org/pdf/2506.01994", "abs": "https://arxiv.org/abs/2506.01994", "authors": ["Wanshan Cui", "Yejin Jeong", "Inwook Song", "Gyuri Kim", "Minsang Kwon", "Donghun Lee"], "title": "Re-experiment Smart: a Novel Method to Enhance Data-driven Prediction of Mechanical Properties of Epoxy Polymers", "categories": ["cond-mat.soft", "cond-mat.mtrl-sci", "cs.AI"], "comment": "27 pages, 8 figures", "summary": "Accurate prediction of polymer material properties through data-driven\napproaches greatly accelerates novel material development by reducing redundant\nexperiments and trial-and-error processes. However, inevitable outliers in\nempirical measurements can severely skew machine learning results, leading to\nerroneous prediction models and suboptimal material designs. To address this\nlimitation, we propose a novel approach to enhance dataset quality efficiently\nby integrating multi-algorithm outlier detection with selective\nre-experimentation of unreliable outlier cases. To validate the empirical\neffectiveness of the approach, we systematically construct a new dataset\ncontaining 701 measurements of three key mechanical properties: glass\ntransition temperature ($T_g$), tan $\\delta$ peak, and crosslinking density\n($v_{c}$). To demonstrate its general applicability, we report the performance\nimprovements across multiple machine learning models, including Elastic Net,\nSVR, Random Forest, and TPOT, to predict the three key properties. Our method\nreliably reduces prediction error (RMSE) and significantly improves accuracy\nwith minimal additional experimental work, requiring only about 5% of the\ndataset to be re-measured.These findings highlight the importance of data\nquality enhancement in achieving reliable machine learning applications in\npolymer science and present a scalable strategy for improving predictive\nreliability in materials science.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u7b97\u6cd5\u5f02\u5e38\u68c0\u6d4b\u7ed3\u5408\u9009\u62e9\u6027\u91cd\u5b9e\u9a8c\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u805a\u5408\u7269\u6750\u6599\u5c5e\u6027\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u5b9e\u9a8c\u5de5\u4f5c\u91cf\u3002", "motivation": "\u89e3\u51b3\u805a\u5408\u7269\u6750\u6599\u5c5e\u6027\u9884\u6d4b\u4e2d\u7531\u5f02\u5e38\u503c\u5bfc\u81f4\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u504f\u5dee\u95ee\u9898\u3002", "method": "\u6574\u5408\u591a\u7b97\u6cd5\u5f02\u5e38\u68c0\u6d4b\u4e0e\u9009\u62e9\u6027\u91cd\u5b9e\u9a8c\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u5728\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e0a\u9a8c\u8bc1\u3002", "result": "\u9884\u6d4b\u8bef\u5dee\u663e\u8457\u964d\u4f4e\uff0c\u4ec5\u9700\u91cd\u65b0\u6d4b\u91cf5%\u7684\u6570\u636e\u5373\u53ef\u63d0\u5347\u51c6\u786e\u6027\u3002", "conclusion": "\u6570\u636e\u8d28\u91cf\u63d0\u5347\u5bf9\u673a\u5668\u5b66\u4e60\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u81f3\u6750\u6599\u79d1\u5b66\u9886\u57df\u3002", "keywords": "\u805a\u5408\u7269\u6750\u6599, \u5f02\u5e38\u68c0\u6d4b, \u673a\u5668\u5b66\u4e60, \u9884\u6d4b\u6a21\u578b, \u6570\u636e\u8d28\u91cf"}}
{"id": "2506.02386", "pdf": "https://arxiv.org/pdf/2506.02386", "abs": "https://arxiv.org/abs/2506.02386", "authors": ["Jie Bian", "Vincent Y. F. Tan"], "title": "Asymptotically Optimal Linear Best Feasible Arm Identification with Fixed Budget", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": "Accepted to the Conference on Uncertainty in Artificial Intelligence\n  (UAI) 2025", "summary": "The challenge of identifying the best feasible arm within a fixed budget has\nattracted considerable interest in recent years. However, a notable gap remains\nin the literature: the exact exponential rate at which the error probability\napproaches zero has yet to be established, even in the relatively simple\nsetting of $K$-armed bandits with Gaussian noise. In this paper, we address\nthis gap by examining the problem within the context of linear bandits. We\nintroduce a novel algorithm for best feasible arm identification that\nguarantees an exponential decay in the error probability. Remarkably, the decay\nrate -- characterized by the exponent -- matches the theoretical lower bound\nderived using information-theoretic principles. Our approach leverages a\nposterior sampling framework embedded within a game-based sampling rule\ninvolving a min-learner and a max-learner. This strategy shares its foundations\nwith Thompson sampling, but is specifically tailored to optimize the\nidentification process under fixed-budget constraints. Furthermore, we validate\nthe effectiveness of our algorithm through comprehensive empirical evaluations\nacross various problem instances with different levels of complexity. The\nresults corroborate our theoretical findings and demonstrate that our method\noutperforms several benchmark algorithms in terms of both accuracy and\nefficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u7ebf\u6027\u591a\u81c2\u8001\u864e\u673a\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u56fa\u5b9a\u9884\u7b97\u4e0b\u8bc6\u522b\u6700\u4f73\u53ef\u884c\u81c2\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u5f53\u524d\u6587\u732e\u4e2d\uff0c\u5373\u4f7f\u5728\u7b80\u5355\u7684K\u81c2\u9ad8\u65af\u566a\u58f0\u8001\u864e\u673a\u95ee\u9898\u4e2d\uff0c\u5173\u4e8e\u9519\u8bef\u6982\u7387\u6307\u6570\u8870\u51cf\u7684\u786e\u5207\u901f\u7387\u5c1a\u672a\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e\u540e\u9a8c\u91c7\u6837\u7684\u6e38\u620f\u91c7\u6837\u89c4\u5219\uff0c\u7ed3\u5408\u6700\u5c0f\u5b66\u4e60\u8005\u548c\u6700\u5927\u5b66\u4e60\u8005\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u56fa\u5b9a\u9884\u7b97\u4e0b\u7684\u6700\u4f73\u81c2\u8bc6\u522b\u4f18\u5316\u3002", "result": "\u7b97\u6cd5\u4fdd\u8bc1\u9519\u8bef\u6982\u7387\u4ee5\u6307\u6570\u901f\u7387\u8870\u51cf\uff0c\u4e14\u8870\u51cf\u901f\u7387\u4e0e\u4fe1\u606f\u7406\u8bba\u4e0b\u754c\u5339\u914d\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u591a\u4e2a\u57fa\u51c6\u7b97\u6cd5\u3002", "conclusion": "\u672c\u6587\u4e0d\u4ec5\u586b\u8865\u4e86\u7406\u8bba\u7a7a\u767d\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5b9e\u9645\u6709\u6548\u7684\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u573a\u666f\u3002", "keywords": "\u7ebf\u6027\u8001\u864e\u673a, \u56fa\u5b9a\u9884\u7b97, \u6700\u4f73\u81c2\u8bc6\u522b, \u540e\u9a8c\u91c7\u6837, \u6e38\u620f\u91c7\u6837\u89c4\u5219"}}
{"id": "2506.02672", "pdf": "https://arxiv.org/pdf/2506.02672", "abs": "https://arxiv.org/abs/2506.02672", "authors": ["Shihan Dou", "Ming Zhang", "Chenhao Huang", "Jiayi Chen", "Feng Chen", "Shichun Liu", "Yan Liu", "Chenxiao Liu", "Cheng Zhong", "Zongzhang Zhang", "Tao Gui", "Chao Xin", "Wei Chengzhi", "Lin Yan", "Qi Zhang", "Xuanjing Huang"], "title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving", "categories": ["cs.CL", "cs.AI"], "comment": "47 pages, 24 figures", "summary": "We introduce EvaLearn, a pioneering benchmark designed to evaluate large\nlanguage models (LLMs) on their learning capability and efficiency in\nchallenging tasks, a critical, yet underexplored aspect of model potential.\nEvaLearn contains 648 challenging problems across six task types, grouped into\n182 sequences, each sequence dedicated to one task type. Diverging from most\nexisting benchmarks that evaluate models in parallel, EvaLearn requires models\nto solve problems sequentially, allowing them to leverage the experience gained\nfrom previous solutions. EvaLearn provides five comprehensive automated metrics\nto evaluate models and quantify their learning capability and efficiency. We\nextensively benchmark nine frontier models and observe varied performance\nprofiles: some models, such as Claude-3.7-sonnet, start with moderate initial\nperformance but exhibit strong learning ability, while some models struggle to\nbenefit from experience and may even show negative transfer. Moreover, we\ninvestigate model performance under two learning settings and find that\ninstance-level rubrics and teacher-model feedback further facilitate model\nlearning. Importantly, we observe that current LLMs with stronger static\nabilities do not show a clear advantage in learning capability across all\ntasks, highlighting that EvaLearn evaluates a new dimension of model\nperformance. We hope EvaLearn provides a novel evaluation perspective for\nassessing LLM potential and understanding the gap between models and human\ncapabilities, promoting the development of deeper and more dynamic evaluation\napproaches. All datasets, the automatic evaluation framework, and the results\nstudied in this paper are available at the GitHub repository.", "AI": {"tldr": "EvaLearn\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u80fd\u529b\u548c\u6548\u7387\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b648\u4e2a\u6311\u6218\u6027\u95ee\u9898\uff0c\u8981\u6c42\u6a21\u578b\u987a\u5e8f\u89e3\u51b3\u95ee\u9898\u5e76\u63d0\u4f9b\u4e94\u4e2a\u81ea\u52a8\u5316\u6307\u6807\u3002\u6d4b\u8bd5\u53d1\u73b0\uff0c\u67d0\u4e9b\u6a21\u578b\u5982Claude-3.7-sonnet\u8868\u73b0\u51fa\u8f83\u5f3a\u5b66\u4e60\u80fd\u529b\uff0c\u800c\u5176\u4ed6\u6a21\u578b\u5219\u96be\u4ee5\u53d7\u76ca\u4e8e\u7ecf\u9a8c\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5927\u591a\u5e76\u884c\u8bc4\u4f30\u6a21\u578b\uff0c\u672a\u80fd\u5145\u5206\u63a2\u7d22\u6a21\u578b\u7684\u5b66\u4e60\u80fd\u529b\u548c\u6548\u7387\u3002EvaLearn\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e13\u6ce8\u4e8e\u6a21\u578b\u5728\u987a\u5e8f\u4efb\u52a1\u4e2d\u7684\u52a8\u6001\u80fd\u529b\u3002", "method": "EvaLearn\u5305\u542b\u516d\u4e2a\u4efb\u52a1\u7c7b\u578b\u7684182\u4e2a\u95ee\u9898\u5e8f\u5217\uff0c\u8981\u6c42\u6a21\u578b\u987a\u5e8f\u89e3\u51b3648\u4e2a\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u4f9b\u4e94\u79cd\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u6d4b\u8bd5\u4e5d\u79cd\u524d\u6cbf\u6a21\u578b\u663e\u793a\uff0c\u5176\u5b66\u4e60\u80fd\u529b\u5dee\u5f02\u663e\u8457\uff1b\u67d0\u4e9b\u6a21\u578b\u80fd\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u800c\u53e6\u4e00\u4e9b\u5219\u8868\u73b0\u4e0d\u4f73\u3002\u9759\u6001\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u5728\u5b66\u4e60\u80fd\u529b\u4e0a\u5e76\u65e0\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "EvaLearn\u4e3a\u8bc4\u4f30\u6a21\u578b\u6f5c\u529b\u548c\u7406\u89e3\u6a21\u578b\u4e0e\u4eba\u7c7b\u80fd\u529b\u5dee\u8ddd\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u4fc3\u8fdb\u4e86\u66f4\u6df1\u5165\u7684\u52a8\u6001\u8bc4\u4f30\u65b9\u6cd5\u53d1\u5c55\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u57fa\u51c6\u6d4b\u8bd5, \u5b66\u4e60\u80fd\u529b, \u52a8\u6001\u8bc4\u4f30, EvaLearn"}}
{"id": "2506.01998", "pdf": "https://arxiv.org/pdf/2506.01998", "abs": "https://arxiv.org/abs/2506.01998", "authors": ["Takao Fujii", "Katie Seaborn", "Madeleine Steeds", "Jun Kato"], "title": "Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.SD", "eess.AS"], "comment": "CHI '25", "summary": "Conversational agents that mimic people have raised questions about the\nethics of anthropomorphizing machines with human social identity cues. Critics\nhave also questioned assumptions of identity neutrality in humanlike agents.\nRecent work has revealed that intersectional Japanese pronouns can elicit\ncomplex and sometimes evasive impressions of agent identity. Yet, the role of\nother \"neutral\" non-pronominal self-referents (NPSR) and voice as a socially\nexpressive medium remains unexplored. In a crowdsourcing study, Japanese\nparticipants (N = 204) evaluated three ChatGPT voices (Juniper, Breeze, and\nEmber) using seven self-referents. We found strong evidence of voice gendering\nalongside the potential of intersectional self-referents to evade gendering,\ni.e., ambiguity through neutrality and elusiveness. Notably, perceptions of age\nand formality intersected with gendering as per sociolinguistic theories,\nespecially boku and watakushi. This work provides a nuanced take on agent\nidentity perceptions and champions intersectional and culturally-sensitive work\non voice agents.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5bf9\u8bdd\u4ee3\u7406\u901a\u8fc7\u4eba\u7c7b\u8eab\u4efd\u7ebf\u7d22\u62df\u4eba\u5316\u7684\u4f26\u7406\u95ee\u9898\uff0c\u5e76\u63a2\u7a76\u4e86\u81ea\u6307\u8bcd\u548c\u58f0\u97f3\u5bf9\u4ee3\u7406\u8eab\u4efd\u611f\u77e5\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u58f0\u97f3\u6027\u522b\u5316\u548c\u81ea\u6307\u8bcd\u7684\u4ea4\u53c9\u6027\u53ef\u4ee5\u6a21\u7cca\u6027\u522b\u8ba4\u77e5\u3002", "motivation": "\u6279\u8bc4\u8005\u5bf9\u62df\u4eba\u5316\u673a\u5668\u7684\u4f26\u7406\u95ee\u9898\u548c\u4e2d\u7acb\u6027\u5047\u8bbe\u63d0\u51fa\u8d28\u7591\uff0c\u7814\u7a76\u65e8\u5728\u586b\u8865\u975e\u4ee3\u8bcd\u81ea\u6307\u8bcd\u548c\u8bed\u97f3\u8868\u8fbe\u5bf9\u793e\u4f1a\u8eab\u4efd\u5851\u9020\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u4f17\u5305\u7814\u7a76\uff0c\u65e5\u672c\u53c2\u4e0e\u8005\u8bc4\u4f30\u4e86\u4e09\u79cdChatGPT\u8bed\u97f3\u548c\u4e03\u79cd\u81ea\u6307\u8bcd\uff0c\u5206\u6790\u4e86\u58f0\u97f3\u6027\u522b\u5316\u53ca\u81ea\u6307\u8bcd\u7684\u4ea4\u53c9\u6027\u6548\u5e94\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u58f0\u97f3\u663e\u8457\u6027\u522b\u5316\uff0c\u540c\u65f6\u67d0\u4e9b\u81ea\u6307\u8bcd\u53ef\u4ee5\u6a21\u7cca\u6027\u522b\u8ba4\u77e5\uff0c\u5e74\u9f84\u548c\u6b63\u5f0f\u7a0b\u5ea6\u7684\u611f\u77e5\u4e0e\u6027\u522b\u5316\u76f8\u4e92\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u4ee3\u7406\u8eab\u4efd\u8ba4\u77e5\u7684\u590d\u6742\u6027\uff0c\u63d0\u5021\u5728\u8bed\u97f3\u4ee3\u7406\u8bbe\u8ba1\u4e2d\u91c7\u7528\u4ea4\u53c9\u6027\u548c\u6587\u5316\u654f\u611f\u7684\u89c6\u89d2\u3002", "keywords": "\u5bf9\u8bdd\u4ee3\u7406, \u62df\u4eba\u5316, \u81ea\u6307\u8bcd, \u8bed\u97f3\u6027\u522b\u5316, \u4ea4\u53c9\u6027"}}
{"id": "2506.02389", "pdf": "https://arxiv.org/pdf/2506.02389", "abs": "https://arxiv.org/abs/2506.02389", "authors": ["Chamara Madarasingha", "Nasrin Sohrabi", "Zahir Tari"], "title": "Univariate to Multivariate: LLMs as Zero-Shot Predictors for Time-Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time-series prediction or forecasting is critical across many real-world\ndynamic systems, and recent studies have proposed using Large Language Models\n(LLMs) for this task due to their strong generalization capabilities and\nability to perform well without extensive pre-training. However, their\neffectiveness in handling complex, noisy, and multivariate time-series data\nremains underexplored. To address this, we propose LLMPred which enhances\nLLM-based time-series prediction by converting time-series sequences into text\nand feeding them to LLMs for zero shot prediction along with two main data\npre-processing techniques. First, we apply time-series sequence decomposition\nto facilitate accurate prediction on complex and noisy univariate sequences.\nSecond, we extend this univariate prediction capability to multivariate data\nusing a lightweight prompt-processing strategy. Extensive experiments with\nsmaller LLMs such as Llama 2 7B, Llama 3.2 3B, GPT-4o-mini, and DeepSeek 7B\ndemonstrate that LLMPred achieves competitive or superior performance compared\nto state-of-the-art baselines. Additionally, a thorough ablation study\nhighlights the importance of the key components proposed in LLMPred.", "AI": {"tldr": "LLMPred \u662f\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u8f6c\u5316\u4e3a\u6587\u672c\u5e76\u7ed3\u5408\u6570\u636e\u9884\u5904\u7406\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d LLMs \u5728\u590d\u6742\u3001\u566a\u58f0\u548c\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0a\u7684\u8868\u73b0\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u56e0\u6b64\u9700\u5f00\u53d1\u65b0\u65b9\u6cd5\u4ee5\u5145\u5206\u5229\u7528\u5176\u6cdb\u5316\u80fd\u529b\u3002", "method": "LLMPred \u5c06\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8f6c\u5316\u4e3a\u6587\u672c\u683c\u5f0f\uff0c\u7ed3\u5408\u5e8f\u5217\u5206\u89e3\u548c\u8f7b\u91cf\u7ea7\u63d0\u793a\u5904\u7406\u6280\u672f\uff0c\u652f\u6301\u96f6\u6837\u672c\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLMPred \u5728\u591a\u79cd\u5c0f\u578b LLMs\uff08\u5982 Llama 2 7B\u3001GPT-4o-mini \u7b49\uff09\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u6216\u5ab2\u7f8e\u5f53\u524d\u6700\u4f73\u57fa\u51c6\u7684\u6027\u80fd\u3002", "conclusion": "LLMPred \u7684\u5173\u952e\u7ec4\u4ef6\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u91cd\u8981\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0cLLMs\uff0c\u96f6\u6837\u672c\u9884\u6d4b\uff0c\u5e8f\u5217\u5206\u89e3\uff0c\u63d0\u793a\u5904\u7406"}}
{"id": "2506.02678", "pdf": "https://arxiv.org/pdf/2506.02678", "abs": "https://arxiv.org/abs/2506.02678", "authors": ["Zhong-Zhi Li", "Xiao Liang", "Zihao Tang", "Lei Ji", "Peijie Wang", "Haotian Xu", "Xing W", "Haizhen Huang", "Weiwei Deng", "Ying Nian Wu", "Yeyun Gong", "Zhijiang Guo", "Xiao Liu", "Fei Yin", "Cheng-Lin Liu"], "title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression", "categories": ["cs.CL", "cs.CE", "cs.NA", "math.NA"], "comment": null, "summary": "Large Language Models (LLMs) have recently achieved remarkable progress by\nleveraging Reinforcement Learning and extended Chain-of-Thought (CoT)\ntechniques. However, the challenge of performing efficient language\nreasoning--especially during inference with extremely long outputs--has drawn\nincreasing attention from the research community. In this work, we propose a\ndynamic ratio-based training pipeline that does not rely on sophisticated data\nannotations or interpolation between multiple models. We continuously balance\nthe weights between the model's System-1 and System-2 data to eliminate\nredundant reasoning processes while preserving the model's reasoning\ncapability. We validate our approach across models on DeepSeek-R1-Distill-7B\nand DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying\ndifficulty levels. Our method significantly reduces the number of output tokens\nby nearly 40% while maintaining the accuracy of the reasoning. Our code and\ndata will be available soon.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u6bd4\u4f8b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u65f6\u7684\u5197\u4f59\u8f93\u51fa\uff0c\u964d\u4f4e40%\u7684token\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u9762\u4e34\u957f\u8f93\u51fa\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u51cf\u5c11\u5197\u4f59\u63a8\u7406\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u7559\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u52a8\u6001\u6bd4\u4f8b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e73\u8861\u6a21\u578bSystem-1\u548cSystem-2\u6570\u636e\u7684\u6743\u91cd\uff0c\u65e0\u9700\u590d\u6742\u6570\u636e\u6807\u6ce8\u6216\u591a\u6a21\u578b\u63d2\u503c\u3002", "result": "\u5728DeepSeek-R1-Distill-7B\u548c14B\u6a21\u578b\u53ca\u591a\u79cd\u96be\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u663e\u8457\u51cf\u5c11\u8fd140%\u7684\u8f93\u51fatoken\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\uff0c\u672a\u6765\u4ee3\u7801\u548c\u6570\u636e\u5c06\u88ab\u5f00\u6e90\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u52a8\u6001\u6bd4\u4f8b\u8bad\u7ec3,\u63a8\u7406\u6548\u7387,System-1,System-2,DeepSeek"}}
{"id": "2506.02007", "pdf": "https://arxiv.org/pdf/2506.02007", "abs": "https://arxiv.org/abs/2506.02007", "authors": ["Ruilin Xu", "Zongxuan Xie", "Pengfei Chen"], "title": "eACGM: Non-instrumented Performance Tracing and Anomaly Detection towards Machine Learning Systems", "categories": ["cs.DC", "cs.AI", "cs.NI"], "comment": "IWQoS 2025", "summary": "We present eACGM, a full-stack AI/ML system monitoring framework based on\neBPF. eACGM collects real-time performance data from key hardware components,\nincluding the GPU and network communication layer, as well as from key software\nstacks such as CUDA, Python, and PyTorch, all without requiring any code\ninstrumentation or modifications. Additionally, it leverages libnvml to gather\nprocess-level GPU resource usage information. By applying a Gaussian Mixture\nModel (GMM) to the collected multidimensional performance metrics for\nstatistical modeling and clustering analysis, eACGM effectively identifies\ncomplex failure modes, such as latency anomalies, hardware failures, and\ncommunication inefficiencies, enabling rapid diagnosis of system bottlenecks\nand abnormal behaviors.\n  To evaluate eACGM's effectiveness and practicality, we conducted extensive\nempirical studies and case analyses in multi-node distributed training\nscenarios. The results demonstrate that eACGM, while maintaining a\nnon-intrusive and low-overhead profile, successfully captures critical\nperformance anomalies during model training and inference. Its stable anomaly\ndetection performance and comprehensive monitoring capabilities validate its\napplicability and scalability in real-world production environments, providing\nstrong support for performance optimization and fault diagnosis in large-scale\nAI/ML systems.", "AI": {"tldr": "eACGM\u662f\u4e00\u4e2a\u57fa\u4e8eeBPF\u7684\u5168\u6808AI/ML\u7cfb\u7edf\u76d1\u63a7\u6846\u67b6,\u5b9e\u65f6\u6536\u96c6\u786c\u4ef6(\u5982GPU\u3001\u7f51\u7edc)\u548c\u8f6f\u4ef6(\u5982CUDA\u3001Python)\u7684\u6027\u80fd\u6570\u636e,\u65e0\u9700\u4ee3\u7801\u4fee\u6539\u3002\u901a\u8fc7GMM\u6a21\u578b\u5206\u6790\u591a\u7ef4\u6027\u80fd\u6307\u6807,\u6709\u6548\u8bc6\u522b\u590d\u6742\u6545\u969c\u6a21\u5f0f\u3002", "motivation": "\u89e3\u51b3AI/ML\u7cfb\u7edf\u4e2d\u590d\u6742\u6545\u969c\u6a21\u5f0f\u7684\u5feb\u901f\u8bca\u65ad\u548c\u6027\u80fd\u4f18\u5316\u95ee\u9898\u3002", "method": "\u5229\u7528eBPF\u5b9e\u65f6\u6536\u96c6\u6027\u80fd\u6570\u636e,\u5e94\u7528GMM\u8fdb\u884c\u7edf\u8ba1\u5efa\u6a21\u548c\u805a\u7c7b\u5206\u6790\u3002", "result": "\u5728\u591a\u8282\u70b9\u5206\u5e03\u5f0f\u8bad\u7ec3\u573a\u666f\u4e2d\u9a8c\u8bc1,\u4f4e\u5f00\u9500\u4e14\u975e\u4fb5\u5165\u6027,\u80fd\u7a33\u5b9a\u6355\u83b7\u6027\u80fd\u5f02\u5e38\u3002", "conclusion": "eACGM\u9002\u7528\u4e8e\u751f\u4ea7\u73af\u5883,\u4e3a\u5927\u89c4\u6a21AI/ML\u7cfb\u7edf\u63d0\u4f9b\u5f3a\u5927\u7684\u6027\u80fd\u76d1\u63a7\u548c\u6545\u969c\u8bca\u65ad\u652f\u6301\u3002", "keywords": "eACGM, eBPF, AI/ML\u7cfb\u7edf\u76d1\u63a7, GMM, \u5f02\u5e38\u68c0\u6d4b"}}
{"id": "2506.02390", "pdf": "https://arxiv.org/pdf/2506.02390", "abs": "https://arxiv.org/abs/2506.02390", "authors": ["Qin Xie", "Qinghua Zhang", "Shuyin Xia", "Xinran Zhou", "Guoyin Wang"], "title": "GAdaBoost: An Efficient and Robust AdaBoost Algorithm Based on Granular-Ball Structure", "categories": ["cs.LG"], "comment": null, "summary": "Adaptive Boosting (AdaBoost) faces significant challenges posed by label\nnoise, especially in multiclass classification tasks. Existing methods either\nlack mechanisms to handle label noise effectively or suffer from high\ncomputational costs due to redundant data usage. Inspired by granular\ncomputing, this paper proposes granular adaptive boosting (GAdaBoost), a novel\ntwo-stage framework comprising a data granulation stage and an adaptive\nboosting stage, to enhance efficiency and robustness under noisy conditions. To\nvalidate its feasibility, an extension of SAMME, termed GAdaBoost.SA, is\nproposed. Specifically, first, a granular-ball generation method is designed to\ncompress data while preserving diversity and mitigating label noise. Second,\nthe granular ball-based SAMME algorithm focuses on granular balls rather than\nindividual samples, improving efficiency and reducing sensitivity to noise.\nExperimental results on some noisy datasets show that the proposed approach\nachieves superior robustness and efficiency compared with existing methods,\ndemonstrating that this work effectively extends AdaBoost and SAMME.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGAdaBoost\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u7c92\u5316\u548c\u81ea\u9002\u5e94\u63d0\u5347\u9636\u6bb5\uff0c\u6709\u6548\u5904\u7406\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u3002", "motivation": "AdaBoost\u5728\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u4e2d\u9762\u4e34\u6807\u7b7e\u566a\u58f0\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u6709\u6548\u7684\u566a\u58f0\u5904\u7406\u673a\u5236\uff0c\u8981\u4e48\u56e0\u5197\u4f59\u6570\u636e\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u63d0\u51faGAdaBoost\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u7c92\u5316\u9636\u6bb5\u548c\u81ea\u9002\u5e94\u63d0\u5347\u9636\u6bb5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u7c92\u7403\u7684SAMME\u6269\u5c55\u7b97\u6cd5\uff08GAdaBoost.SA\uff09\u3002", "result": "\u5728\u566a\u58f0\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGAdaBoost.SA\u5728\u9c81\u68d2\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u6269\u5c55\u4e86AdaBoost\u548cSAMME\uff0c\u89e3\u51b3\u4e86\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u3002", "keywords": "AdaBoost, \u6807\u7b7e\u566a\u58f0, \u7c92\u5ea6\u8ba1\u7b97, GAdaBoost, SAMME"}}
{"id": "2506.02683", "pdf": "https://arxiv.org/pdf/2506.02683", "abs": "https://arxiv.org/abs/2506.02683", "authors": ["Zhengdong Lu", "Weikai Lu", "Yiling Tao", "Yun Dai", "ZiXuan Chen", "Huiping Zhuang", "Cen Chen", "Hao Peng", "Ziqian Zeng"], "title": "Decompose, Plan in Parallel, and Merge: A Novel Paradigm for Large Language Models based Planning with Multiple Constraints", "categories": ["cs.CL"], "comment": null, "summary": "Despite significant advances in Large Language Models (LLMs), planning tasks\nstill present challenges for LLM-based agents. Existing planning methods face\ntwo key limitations: heavy constraints and cascading errors. To address these\nlimitations, we propose a novel parallel planning paradigm, which Decomposes,\nPlans for subtasks in Parallel, and Merges subplans into a final plan (DPPM).\nSpecifically, DPPM decomposes the complex task based on constraints into\nsubtasks, generates the subplan for each subtask in parallel, and merges them\ninto a global plan. In addition, our approach incorporates a verification and\nrefinement module, enabling error correction and conflict resolution.\nExperimental results demonstrate that DPPM significantly outperforms existing\nmethods in travel planning tasks.", "AI": {"tldr": "DPPM\u662f\u4e00\u79cd\u5e76\u884c\u89c4\u5212\u8303\u5f0f\uff0c\u901a\u8fc7\u5206\u89e3\u3001\u5e76\u884c\u89c4\u5212\u548c\u5408\u5e76\u5b50\u8ba1\u5212\u6765\u89e3\u51b3LLM\u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u5b58\u5728\u7ea6\u675f\u548c\u7ea7\u8054\u9519\u8bef\u7684\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faDPPM\u8303\u5f0f\uff0c\u5305\u62ec\u5206\u89e3\u4efb\u52a1\u3001\u5e76\u884c\u751f\u6210\u5b50\u8ba1\u5212\u3001\u5408\u5e76\u5b50\u8ba1\u5212\u53ca\u9a8c\u8bc1\u4e0e\u4f18\u5316\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u663e\u793aDPPM\u5728\u65c5\u884c\u89c4\u5212\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DPPM\u901a\u8fc7\u5e76\u884c\u89c4\u5212\u548c\u9a8c\u8bc1\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002", "keywords": "LLM, \u89c4\u5212\u4efb\u52a1, \u5e76\u884c\u89c4\u5212, DPPM, \u9a8c\u8bc1\u4f18\u5316"}}
{"id": "2506.02014", "pdf": "https://arxiv.org/pdf/2506.02014", "abs": "https://arxiv.org/abs/2506.02014", "authors": ["Wang Mengjie", "Zhu Huiping", "Li Jian", "Shi Wenxiu", "Zhang Song"], "title": "Research on Driving Scenario Technology Based on Multimodal Large Lauguage Model Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the advancement of autonomous and assisted driving technologies, higher\ndemands are placed on the ability to understand complex driving scenarios.\nMultimodal general large models have emerged as a solution for this challenge.\nHowever, applying these models in vertical domains involves difficulties such\nas data collection, model training, and deployment optimization. This paper\nproposes a comprehensive method for optimizing multimodal models in driving\nscenarios, including cone detection, traffic light recognition, speed limit\nrecommendation, and intersection alerts. The method covers key aspects such as\ndynamic prompt optimization, dataset construction, model training, and\ndeployment. Specifically, the dynamic prompt optimization adjusts the prompts\nbased on the input image content to focus on objects affecting the ego vehicle,\nenhancing the model's task-specific focus and judgment capabilities. The\ndataset is constructed by combining real and synthetic data to create a\nhigh-quality and diverse multimodal training dataset, improving the model's\ngeneralization in complex driving environments. In model training, advanced\ntechniques like knowledge distillation, dynamic fine-tuning, and quantization\nare integrated to reduce storage and computational costs while boosting\nperformance. Experimental results show that this systematic optimization method\nnot only significantly improves the model's accuracy in key tasks but also\nachieves efficient resource utilization, providing strong support for the\npractical application of driving scenario perception technologies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u4f18\u5316\u591a\u6a21\u6001\u6a21\u578b\u5728\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u65b9\u6cd5\uff0c\u6db5\u76d6\u52a8\u6001\u63d0\u793a\u4f18\u5316\u3001\u6570\u636e\u96c6\u6784\u5efa\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u90e8\u7f72\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u51c6\u786e\u6027\u548c\u8d44\u6e90\u6548\u7387\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u53d1\u5c55\uff0c\u9700\u8981\u66f4\u597d\u5730\u7406\u89e3\u590d\u6742\u9a7e\u9a76\u573a\u666f\uff0c\u4f46\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u5e94\u7528\u5b58\u5728\u6570\u636e\u6536\u96c6\u3001\u8bad\u7ec3\u548c\u90e8\u7f72\u7b49\u96be\u70b9\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u52a8\u6001\u63d0\u793a\u4f18\u5316\u4ee5\u805a\u7126\u5173\u952e\u5bf9\u8c61\uff0c\u7ed3\u5408\u771f\u5b9e\u4e0e\u5408\u6210\u6570\u636e\u6784\u5efa\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6574\u5408\u77e5\u8bc6\u84b8\u998f\u3001\u52a8\u6001\u5fae\u8c03\u548c\u91cf\u5316\u7b49\u8bad\u7ec3\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5173\u952e\u4efb\u52a1\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8d44\u6e90\u5229\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9a7e\u9a76\u573a\u666f\u611f\u77e5\u6280\u672f\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002", "keywords": "\u81ea\u52a8\u9a7e\u9a76,\u591a\u6a21\u6001\u6a21\u578b,\u52a8\u6001\u63d0\u793a\u4f18\u5316,\u6570\u636e\u96c6\u6784\u5efa,\u77e5\u8bc6\u84b8\u998f"}}
{"id": "2506.02392", "pdf": "https://arxiv.org/pdf/2506.02392", "abs": "https://arxiv.org/abs/2506.02392", "authors": ["Yuanyao Chen", "Rongsheng Chen", "Fu Luo", "Zhenkun Wang"], "title": "Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning", "categories": ["cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2505.24627", "summary": "Neural Combinatorial Optimization (NCO) has emerged as a promising\nlearning-based paradigm for addressing Vehicle Routing Problems (VRPs) by\nminimizing the need for extensive manual engineering. While existing NCO\nmethods, trained on small-scale instances (e.g., 100 nodes), have demonstrated\nconsiderable success on problems of similar scale, their performance\nsignificantly degrades when applied to large-scale scenarios. This degradation\narises from the distributional shift between training and testing data,\nrendering policies learned on small instances ineffective for larger problems.\nTo overcome this limitation, we introduce a novel learning framework driven by\nLarge Language Models (LLMs). This framework learns a projection between the\ntraining and testing distributions, which is then deployed to enhance the\nscalability of the NCO model. Notably, unlike prevailing techniques that\nnecessitate joint training with the neural network, our approach operates\nexclusively during the inference phase, obviating the need for model\nretraining. Extensive experiments demonstrate that our method enables a\nbackbone model (trained on 100-node instances) to achieve superior performance\non large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) of up to 100K nodes from diverse distributions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u795e\u7ecf\u7ec4\u5408\u4f18\u5316\uff08NCO\uff09\u5728\u5c0f\u89c4\u6a21\u5b9e\u4f8b\u8bad\u7ec3\u540e\u5728\u5927\u89c4\u6a21VRP\u4e0a\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709NCO\u65b9\u6cd5\u5728\u5c0f\u89c4\u6a21\u5b9e\u4f8b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u539f\u56e0\u662f\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u5206\u5e03\u4e0d\u4e00\u81f4\u3002", "method": "\u5f15\u5165LLM\u9a71\u52a8\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u5b66\u4e60\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u5206\u5e03\u95f4\u7684\u6295\u5f71\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f7f\u8bad\u7ec3\u4e8e100\u8282\u70b9\u5b9e\u4f8b\u7684\u9aa8\u5e72\u6a21\u578b\u572810\u4e07\u8282\u70b9\u7684TSP\u548cCVRP\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86NCO\u6a21\u578b\u7684\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u3002", "keywords": "\u795e\u7ecf\u7ec4\u5408\u4f18\u5316,\u8f66\u8f86\u8def\u5f84\u95ee\u9898,\u5927\u8bed\u8a00\u6a21\u578b,\u5206\u5e03\u6295\u5f71,\u63a8\u7406\u9636\u6bb5"}}
{"id": "2506.02689", "pdf": "https://arxiv.org/pdf/2506.02689", "abs": "https://arxiv.org/abs/2506.02689", "authors": ["Liang Yue", "Yihong Tang", "Kehai Chen", "Jie Liu", "Min Zhang"], "title": "MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching", "categories": ["cs.CL"], "comment": null, "summary": "Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models'\ninstruction-following capabilities and task-specific performance. However,\nobtaining high-quality fine-tuning data for large models is challenging due to\ndata collection difficulties and high production costs. To address this, we\npropose MASTER, a novel data augmentation method that enriches original data\nthrough interactions among multiple agents with varying cognitive levels. We\nsimulate three pedagogically grounded teaching scenarios, leveraging\nmulti-agent conversations to generate high-quality teacher-student interaction\ndata. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented\nfrom existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5.\nExperiments show that models fine-tuned with BOOST-QA perform excellently\nacross multiple benchmarks, demonstrating strong multitask generalization.\nNotably, MASTER significantly improves models' reasoning abilities in complex\ntasks, providing valuable insights for future research.", "AI": {"tldr": "MASTER\u65b9\u6cd5\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u589e\u5f3a\u539f\u59cb\u6570\u636e\uff0c\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u7684BOOST-QA\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u591a\u4efb\u52a1\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u5927\u6a21\u578b\u9ad8\u8d28\u91cf\u5fae\u8c03\u6570\u636e\u96be\u4ee5\u83b7\u53d6\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u6a21\u62df\u6559\u5b66\u573a\u666f\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5e08\u751f\u4ea4\u4e92\u6570\u636e\u3002", "result": "\u57fa\u4e8eMASTER\u6784\u5efa\u7684BOOST-QA\u6570\u636e\u96c6\u4f7f\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "MASTER\u65b9\u6cd5\u4e3a\u9ad8\u8d28\u91cf\u5fae\u8c03\u6570\u636e\u7684\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5bf9\u672a\u6765\u7684\u7814\u7a76\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "keywords": "\u6307\u4ee4\u5fae\u8c03, \u6570\u636e\u589e\u5f3a, \u591a\u667a\u80fd\u4f53\u4ea4\u4e92, \u590d\u6742\u63a8\u7406"}}
{"id": "2506.02021", "pdf": "https://arxiv.org/pdf/2506.02021", "abs": "https://arxiv.org/abs/2506.02021", "authors": ["Yinjie Zhao", "Heng Zhao", "Bihan Wen", "Yew-Soon Ong", "Joey Tianyi Zhou"], "title": "Dynamic-Aware Video Distillation: Optimizing Temporal Resolution Based on Video Semantics", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of vision tasks and the scaling on datasets and\nmodels, redundancy reduction in vision datasets has become a key area of\nresearch. To address this issue, dataset distillation (DD) has emerged as a\npromising approach to generating highly compact synthetic datasets with\nsignificantly less redundancy while preserving essential information. However,\nwhile DD has been extensively studied for image datasets, DD on video datasets\nremains underexplored. Video datasets present unique challenges due to the\npresence of temporal information and varying levels of redundancy across\ndifferent classes. Existing DD approaches assume a uniform level of temporal\nredundancy across all different video semantics, which limits their\neffectiveness on video datasets. In this work, we propose Dynamic-Aware Video\nDistillation (DAViD), a Reinforcement Learning (RL) approach to predict the\noptimal Temporal Resolution of the synthetic videos. A teacher-in-the-loop\nreward function is proposed to update the RL agent policy. To the best of our\nknowledge, this is the first study to introduce adaptive temporal resolution\nbased on video semantics in video dataset distillation. Our approach\nsignificantly outperforms existing DD methods, demonstrating substantial\nimprovements in performance. This work paves the way for future research on\nmore efficient and semantic-adaptive video dataset distillation research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDAViD\u7684\u52a8\u6001\u611f\u77e5\u89c6\u9891\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9884\u6d4b\u6700\u4f18\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u7684\u4e0d\u8db3\uff0c\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u4efb\u52a1\u548c\u6570\u636e\u96c6\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u6570\u636e\u5197\u4f59\u51cf\u5c11\u6210\u4e3a\u7814\u7a76\u91cd\u70b9\u3002\u89c6\u9891\u6570\u636e\u96c6\u56e0\u5176\u72ec\u7279\u7684\u65f6\u95f4\u4fe1\u606f\u548c\u5197\u4f59\u5dee\u5f02\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5904\u7406\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u9884\u6d4b\u5408\u6210\u89c6\u9891\u7684\u6700\u4f18\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u5e76\u63d0\u51fa\u6559\u5e08\u5faa\u73af\u5956\u52b1\u51fd\u6570\u66f4\u65b0RL\u7b56\u7565\u3002", "result": "DAViD\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\uff0c\u6027\u80fd\u5927\u5e45\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u66f4\u9ad8\u6548\u3001\u8bed\u4e49\u81ea\u9002\u5e94\u7684\u89c6\u9891\u6570\u636e\u96c6\u84b8\u998f\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\u3002", "keywords": "\u6570\u636e\u96c6\u84b8\u998f\uff0c\u52a8\u6001\u611f\u77e5\uff0c\u5f3a\u5316\u5b66\u4e60\uff0c\u89c6\u9891\u8bed\u4e49\uff0c\u65f6\u95f4\u5206\u8fa8\u7387"}}
{"id": "2506.02406", "pdf": "https://arxiv.org/pdf/2506.02406", "abs": "https://arxiv.org/abs/2506.02406", "authors": ["Renat Sergazinov", "Jing Wu", "Shao-An Yin"], "title": "Random at First, Fast at Last: NTK-Guided Fourier Pre-Processing for Tabular DL", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "16 pages, 3 figures, 1 table", "summary": "While random Fourier features are a classic tool in kernel methods, their\nutility as a pre-processing step for deep learning on tabular data has been\nlargely overlooked. Motivated by shortcomings in tabular deep learning\npipelines - revealed through Neural Tangent Kernel (NTK) analysis - we revisit\nand repurpose random Fourier mappings as a parameter-free,\narchitecture-agnostic transformation. By projecting each input into a fixed\nfeature space via sine and cosine projections with frequencies drawn once at\ninitialization, this approach circumvents the need for ad hoc normalization or\nadditional learnable embeddings. We show within the NTK framework that this\nmapping (i) bounds and conditions the network's initial NTK spectrum, and (ii)\nintroduces a bias that shortens the optimization trajectory, thereby\naccelerating gradient-based training. These effects pre-condition the network\nwith a stable kernel from the outset. Empirically, we demonstrate that deep\nnetworks trained on Fourier-transformed inputs converge more rapidly and\nconsistently achieve strong final performance, often with fewer epochs and less\nhyperparameter tuning. Our findings establish random Fourier pre-processing as\na theoretically motivated, plug-and-play enhancement for tabular deep learning.", "AI": {"tldr": "\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\u88ab\u91cd\u65b0\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u4e2d\uff0c\u4f5c\u4e3a\u5904\u7406\u8868\u683c\u6570\u636e\u7684\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u901a\u8fc7\u56fa\u5b9a\u9891\u7387\u7684\u6b63\u5f26\u548c\u4f59\u5f26\u6295\u5f71\uff0c\u52a0\u901f\u8bad\u7ec3\u5e76\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u8868\u683c\u6570\u636e\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u4e0d\u8db3\uff0c\u5982\u9700\u8981\u989d\u5916\u7684\u5f52\u4e00\u5316\u6216\u5d4c\u5165\u5b66\u4e60\uff0c\u901a\u8fc7\u795e\u7ecf\u6b63\u5207\u6838\uff08NTK\uff09\u5206\u6790\u91cd\u65b0\u5229\u7528\u968f\u673a\u5085\u91cc\u53f6\u6620\u5c04\u3002", "method": "\u5728\u521d\u59cb\u5316\u65f6\u4f7f\u7528\u56fa\u5b9a\u9891\u7387\u7684\u6b63\u5f26\u548c\u4f59\u5f26\u6295\u5f71\u5c06\u8f93\u5165\u6620\u5c04\u5230\u56fa\u5b9a\u7279\u5f81\u7a7a\u95f4\uff0c\u4f5c\u4e3a\u53c2\u6570\u65e0\u5173\u4e14\u67b6\u6784\u65e0\u5173\u7684\u8f6c\u6362\u3002", "result": "\u968f\u673a\u5085\u91cc\u53f6\u9884\u5904\u7406\u80fd\u52a0\u901f\u68af\u5ea6\u8bad\u7ec3\uff0c\u4f18\u5316\u7f51\u7edc\u6027\u80fd\uff0c\u964d\u4f4e\u8d85\u53c2\u6570\u8c03\u4f18\u9700\u6c42\uff0c\u5e76\u63d0\u9ad8\u6700\u7ec8\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u968f\u673a\u5085\u91cc\u53f6\u9884\u5904\u7406\u662f\u4e00\u79cd\u7406\u8bba\u652f\u6301\u3001\u5373\u63d2\u5373\u7528\u7684\u8868\u683c\u6570\u636e\u6df1\u5ea6\u5b66\u4e60\u589e\u5f3a\u65b9\u6cd5\u3002", "keywords": "\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81,\u8868\u683c\u6570\u636e,\u6df1\u5ea6\u5b66\u4e60,\u795e\u7ecf\u6b63\u5207\u6838,\u9884\u5904\u7406"}}
{"id": "2506.02701", "pdf": "https://arxiv.org/pdf/2506.02701", "abs": "https://arxiv.org/abs/2506.02701", "authors": ["Masaki Sakata", "Sho Yokoi", "Benjamin Heinzerling", "Takumi Ito", "Kentaro Inui"], "title": "On Entity Identification in Language Models", "categories": ["cs.CL"], "comment": "ACL 2025 Findings; 26 pages, 13 figures, 9 tables", "summary": "We analyze the extent to which internal representations of language models\n(LMs) identify and distinguish mentions of named entities, focusing on the\nmany-to-many correspondence between entities and their mentions. We first\nformulate two problems of entity mentions -- ambiguity and variability -- and\npropose a framework analogous to clustering quality metrics. Specifically, we\nquantify through cluster analysis of LM internal representations the extent to\nwhich mentions of the same entity cluster together and mentions of different\nentities remain separated. Our experiments examine five Transformer-based\nautoregressive models, showing that they effectively identify and distinguish\nentities with metrics analogous to precision and recall ranging from 0.66 to\n0.9. Further analysis reveals that entity-related information is compactly\nrepresented in a low-dimensional linear subspace at early LM layers.\nAdditionally, we clarify how the characteristics of entity representations\ninfluence word prediction performance. These findings are interpreted through\nthe lens of isomorphism between LM representations and entity-centric knowledge\nstructures in the real world, providing insights into how LMs internally\norganize and use entity information.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u8868\u793a\u5982\u4f55\u8bc6\u522b\u548c\u533a\u5206\u547d\u540d\u5b9e\u4f53\u7684\u63d0\u53ca\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u805a\u7c7b\u8d28\u91cf\u6307\u6807\u7684\u6846\u67b6\uff0c\u5e76\u5b9e\u9a8c\u8bc1\u660e\u6a21\u578b\u80fd\u6709\u6548\u533a\u5206\u5b9e\u4f53\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u8868\u793a\u5982\u4f55\u8bc6\u522b\u548c\u533a\u5206\u547d\u540d\u5b9e\u4f53\u7684\u63d0\u53ca\uff0c\u4ee5\u7406\u89e3\u6a21\u578b\u5982\u4f55\u7ec4\u7ec7\u548c\u5229\u7528\u5b9e\u4f53\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u805a\u7c7b\u5206\u6790\u91cf\u5316\u5b9e\u4f53\u63d0\u53ca\u7684\u6b67\u4e49\u6027\u548c\u53d8\u5f02\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e94\u79cdTransformer\u81ea\u56de\u5f52\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u80fd\u6709\u6548\u8bc6\u522b\u548c\u533a\u5206\u5b9e\u4f53\uff0c\u6307\u6807\u57280.66\u52300.9\u4e4b\u95f4\uff1b\u5b9e\u4f53\u4fe1\u606f\u5728\u65e9\u671f\u5c42\u7684\u4f4e\u7ef4\u7ebf\u6027\u5b50\u7a7a\u95f4\u4e2d\u7d27\u51d1\u8868\u793a\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u8868\u793a\u4e0e\u5b9e\u4f53\u4e2d\u5fc3\u77e5\u8bc6\u7ed3\u6784\u5b58\u5728\u540c\u6784\u6027\uff0c\u4e3a\u6a21\u578b\u5982\u4f55\u7ec4\u7ec7\u548c\u5229\u7528\u5b9e\u4f53\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b\u3001\u547d\u540d\u5b9e\u4f53\u3001\u5185\u90e8\u8868\u793a\u3001\u805a\u7c7b\u5206\u6790\u3001\u540c\u6784\u6027"}}
{"id": "2506.02025", "pdf": "https://arxiv.org/pdf/2506.02025", "abs": "https://arxiv.org/abs/2506.02025", "authors": ["Prachi Jadhav", "Hongwei Jin", "Ewa Deelman", "Prasanna Balaprakash"], "title": "Evaluating the Efficacy of LLM-Based Reasoning for Multiobjective HPC Job Scheduling", "categories": ["cs.DC", "cs.AI"], "comment": "10 pages, 6 figures, work under review", "summary": "High-Performance Computing (HPC) job scheduling involves balancing\nconflicting objectives such as minimizing makespan, reducing wait times,\noptimizing resource use, and ensuring fairness. Traditional methods, including\nheuristic-based (e.g., First-Come-First-Served) or intensive optimization\ntechniques, often lack adaptability to dynamic workloads and heterogeneous HPC\nsystems. To address this, we propose a novel Large Language Model (LLM)-based\nscheduler using a ReAct-style framework (Reason + Act), enabling iterative,\ninterpretable decision-making. The system incorporates a scratchpad memory to\ntrack scheduling history and refine decisions via natural language feedback,\nwhile a constraint enforcement module ensures feasibility and safety. We\nevaluate our approach using OpenAI's O4-Mini and Anthropic's Claude 3.7 across\nseven real-world HPC workload scenarios, including heterogeneous mixes, bursty\npatterns, and adversarial cases. Comparisons against FCFS, Shortest Job First,\nand Google OR-Tools (on 10 to 100 jobs) reveal that LLM-based scheduling\neffectively balances multiple objectives while offering transparent reasoning\nthrough natural language traces. The method excels in constraint satisfaction\nand adapts to diverse workloads without domain-specific training. However, a\ntrade-off between reasoning quality and computational overhead challenges\nreal-time deployment. This work presents the first comprehensive study of\nreasoning-capable LLMs for HPC scheduling, demonstrating their potential to\nhandle multiobjective optimization while highlighting limitations in\ncomputational efficiency. The findings provide insights into leveraging\nadvanced language models for complex scheduling problems in dynamic HPC\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8c03\u5ea6\u5668\uff0c\u7528\u4e8e\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u4f5c\u4e1a\u8c03\u5ea6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\u51b3\u7b56\uff0c\u5e73\u8861\u591a\u76ee\u6807\u4f18\u5316\uff0c\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edfHPC\u8c03\u5ea6\u65b9\u6cd5\u5728\u52a8\u6001\u8d1f\u8f7d\u548c\u5f02\u6784\u7cfb\u7edf\u4e2d\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528ReAct\u6846\u67b6\uff08\u63a8\u7406+\u884c\u52a8\uff09\u7684LLM\u8c03\u5ea6\u5668\uff0c\u7ed3\u5408\u5386\u53f2\u8bb0\u5f55\u548c\u7ea6\u675f\u6a21\u5757\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u4f18\u5316\u51b3\u7b56\u3002", "result": "\u5728\u591a\u79cd\u771f\u5b9eHPC\u8d1f\u8f7d\u573a\u666f\u4e0b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u80fd\u5e73\u8861\u591a\u76ee\u6807\u5e76\u6ee1\u8db3\u7ea6\u675f\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u4e0e\u5b9e\u65f6\u6027\u7684\u6743\u8861\u3002", "conclusion": "LLM\u5728HPC\u8c03\u5ea6\u4e2d\u8868\u73b0\u51fa\u591a\u76ee\u6807\u4f18\u5316\u6f5c\u529b\uff0c\u4f46\u9700\u89e3\u51b3\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "keywords": "\u9ad8\u6027\u80fd\u8ba1\u7b97, \u4f5c\u4e1a\u8c03\u5ea6, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u591a\u76ee\u6807\u4f18\u5316, ReAct\u6846\u67b6"}}
{"id": "2506.02415", "pdf": "https://arxiv.org/pdf/2506.02415", "abs": "https://arxiv.org/abs/2506.02415", "authors": ["Karthikeyan Vaiapury"], "title": "AERO: A Redirection-Based Optimization Framework Inspired by Judo for Robust Probabilistic Forecasting", "categories": ["cs.LG", "cs.AI", "62M10, 60G25, 62P30", "I.2.6; I.5.1; G.3; J.2"], "comment": "15 pages, 1 figure, submitted to NeurIPS 2025 (preprint version)", "summary": "Optimization remains a fundamental pillar of machine learning, yet existing\nmethods often struggle to maintain stability and adaptability in dynamic, non\nlinear systems, especially under uncertainty. We introduce AERO (Adversarial\nEnergy-based Redirection Optimization), a novel framework inspired by the\nredirection principle in Judo, where external disturbances are leveraged rather\nthan resisted. AERO reimagines optimization as a redirection process guided by\n15 interrelated axioms encompassing adversarial correction, energy\nconservation, and disturbance-aware learning. By projecting gradients,\nintegrating uncertainty driven dynamics, and managing learning energy, AERO\noffers a principled approach to stable and robust model updates. Applied to\nprobabilistic solar energy forecasting, AERO demonstrates substantial gains in\npredictive accuracy, reliability, and adaptability, especially in noisy and\nuncertain environments. Our findings highlight AERO as a compelling new\ndirection in the theoretical and practical landscape of optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4f18\u5316\u6846\u67b6AERO\uff0c\u901a\u8fc7\u501f\u9274\u67d4\u9053\u7684\u91cd\u5b9a\u5411\u539f\u7406\uff0c\u5c06\u5916\u90e8\u6270\u52a8\u8f6c\u5316\u4e3a\u4f18\u5316\u52a8\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u5728\u52a8\u6001\u3001\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u5f80\u5f80\u96be\u4ee5\u4fdd\u6301\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\uff0c\u5c24\u5176\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u57fa\u4e8e15\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u516c\u7406\uff0c\u63d0\u51faAERO\u6846\u67b6\uff0c\u5305\u542b\u5bf9\u6297\u6821\u6b63\u3001\u80fd\u91cf\u5b88\u6052\u548c\u6270\u52a8\u611f\u77e5\u5b66\u4e60\uff0c\u901a\u8fc7\u68af\u5ea6\u6295\u5f71\u3001\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u52a8\u6001\u548c\u80fd\u91cf\u7ba1\u7406\u5b9e\u73b0\u7a33\u5b9a\u4f18\u5316\u3002", "result": "\u5728\u6982\u7387\u6027\u592a\u9633\u80fd\u9884\u6d4b\u4e2d\uff0cAERO\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3001\u53ef\u9760\u6027\u548c\u9002\u5e94\u6027\uff0c\u5c24\u5176\u5728\u566a\u58f0\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "AERO\u4e3a\u4f18\u5316\u7406\u8bba\u53ca\u5b9e\u8df5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002", "keywords": "AERO, \u4f18\u5316\u6846\u67b6, \u52a8\u6001\u7cfb\u7edf, \u5bf9\u6297\u5b66\u4e60, \u80fd\u91cf\u7ba1\u7406"}}
{"id": "2506.02726", "pdf": "https://arxiv.org/pdf/2506.02726", "abs": "https://arxiv.org/abs/2506.02726", "authors": ["Qihang Yan", "Xinyu Zhang", "Luming Guo", "Qi Zhang", "Feifan Liu"], "title": "RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; H.3.3"], "comment": null, "summary": "Large Language Models (LLMs) struggle with accuracy, domain-specific\nreasoning, and interpretability in vertical domains. Traditional preference\nalignment methods like Reinforcement Learning from Human Feedback (RLHF) and\nDirect Preference Optimization (DPO) often overlook the underlying knowledge\nsources and reasoning logic. This paper introduces RACE-Align\n(Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel\nframework designed to address these limitations. RACE-Align systematically\nconstructs a binary preference dataset incorporating external knowledge support\nand explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO\nalgorithm. The core innovation lies in its preference data construction\nstrategy: it integrates AI-driven retrieval for factual grounding, enhancing\nknowledgeability and accuracy, and emphasizes the optimization of\ndomain-specific CoT, treating the reasoning process itself as a key preference\ndimension. A multi-stage, AI-driven refinement pipeline cost-effectively\ngenerates these preference pairs. Experimental validation in Traditional\nChinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that\nRACE-Align significantly outperforms the original base model and a model\nfine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed\nacross multiple dimensions, including answer accuracy, information richness,\napplication of TCM thinking patterns, logicality and depth of reasoning, and\ninterpretability. These findings suggest RACE-Align offers an effective pathway\nto enhance LLMs' knowledge application, reasoning reliability, and process\ntransparency in complex vertical domains.", "AI": {"tldr": "RACE-Align\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u548c\u94fe\u5f0f\u601d\u7ef4\u4f18\u5316\u5bf9\u9f50\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347LLMs\u5728\u5782\u76f4\u9886\u57df\u7684\u6548\u679c\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u5782\u76f4\u9886\u57df\u4e2d\u51c6\u786e\u6027\u3001\u9886\u57df\u63a8\u7406\u53ca\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\u5ffd\u7565\u4e86\u77e5\u8bc6\u548c\u63a8\u7406\u903b\u8f91\u3002", "method": "RACE-Align\u6784\u5efa\u4e8c\u5143\u504f\u597d\u6570\u636e\u96c6\uff0c\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u4e0e\u663e\u5f0f\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\uff0c\u901a\u8fc7DPO\u7b97\u6cd5\u5bf9\u9f50\u6a21\u578b\u3002", "result": "\u5728\u4e2d\u533b\u9886\u57df\u5b9e\u9a8c\u663e\u8457\u63d0\u5347\u7b54\u6848\u51c6\u786e\u6027\u3001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u548c\u63a8\u7406\u903b\u8f91\u6027\u3002", "conclusion": "RACE-Align\u4e3a\u590d\u6742\u5782\u76f4\u9886\u57df\u4e2dLLMs\u7684\u77e5\u8bc6\u5e94\u7528\u548c\u63a8\u7406\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002", "keywords": "RACE-Align, LLMs, \u68c0\u7d22\u589e\u5f3a, \u94fe\u5f0f\u601d\u7ef4, \u5782\u76f4\u9886\u57df"}}
{"id": "2506.02027", "pdf": "https://arxiv.org/pdf/2506.02027", "abs": "https://arxiv.org/abs/2506.02027", "authors": ["Shriphani Palakodety"], "title": "The End Of Universal Lifelong Identifiers: Identity Systems For The AI Era", "categories": ["cs.CR", "cs.AI", "cs.CY"], "comment": "9 content pages, 14 pages with reference", "summary": "Many identity systems assign a single, static identifier to an individual for\nlife, reused across domains like healthcare, finance, and education. These\nUniversal Lifelong Identifiers (ULIs) underpin critical workflows but now pose\nsystemic privacy risks. We take the position that ULIs are fundamentally\nincompatible with the AI era and must be phased out. We articulate a threat\nmodel grounded in modern AI capabilities and show that traditional safeguards\nsuch as redaction, consent, and access controls are no longer sufficient. We\ndefine core properties for identity systems in the AI era and present a\ncryptographic framework that satisfies them while retaining compatibility with\nexisting identifier workflows. Our design preserves institutional workflows,\nsupports essential functions such as auditability and delegation, and offers a\npractical migration path beyond ULIs.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u9010\u6b65\u6dd8\u6c70\u7ec8\u8eab\u901a\u7528\u6807\u8bc6\u7b26\uff08ULIs\uff09\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728\u73b0\u4ee3AI\u65f6\u4ee3\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5bc6\u7801\u5b66\u6846\u67b6\u4ee5\u6ee1\u8db3\u65b0\u9700\u6c42\u3002", "motivation": "\u7ec8\u8eab\u901a\u7528\u6807\u8bc6\u7b26\uff08ULIs\uff09\u5728\u591a\u4e2a\u9886\u57df\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5728AI\u65f6\u4ee3\u4e0b\u5176\u9690\u79c1\u98ce\u9669\u5df2\u65e0\u6cd5\u901a\u8fc7\u4f20\u7edf\u4fdd\u62a4\u63aa\u65bd\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5bc6\u7801\u5b66\u6846\u67b6\uff0c\u6ee1\u8db3AI\u65f6\u4ee3\u7684\u8eab\u4efd\u7cfb\u7edf\u6838\u5fc3\u5c5e\u6027\uff0c\u540c\u65f6\u517c\u5bb9\u73b0\u6709\u6807\u8bc6\u7b26\u5de5\u4f5c\u6d41\u3002", "result": "\u8be5\u8bbe\u8ba1\u4fdd\u7559\u4e86\u673a\u6784\u5de5\u4f5c\u6d41\uff0c\u652f\u6301\u5ba1\u8ba1\u548c\u6388\u6743\u7b49\u529f\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u9010\u6b65\u6dd8\u6c70ULIs\u7684\u5b9e\u7528\u65b9\u6848\u3002", "conclusion": "ULIs\u5df2\u4e0d\u9002\u5e94AI\u65f6\u4ee3\uff0c\u9700\u88ab\u66ff\u4ee3\uff0c\u65b0\u6846\u67b6\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u8eab\u4efd\u7cfb\u7edf, \u9690\u79c1\u98ce\u9669, \u5bc6\u7801\u5b66, AI\u65f6\u4ee3, \u7ec8\u8eab\u901a\u7528\u6807\u8bc6\u7b26"}}
{"id": "2506.02451", "pdf": "https://arxiv.org/pdf/2506.02451", "abs": "https://arxiv.org/abs/2506.02451", "authors": ["Pratheeksha Nair", "Reihaneh Rabbany"], "title": "Weak Supervision for Real World Graphs", "categories": ["cs.LG", "I.2.6"], "comment": null, "summary": "Node classification in real world graphs often suffers from label scarcity\nand noise, especially in high stakes domains like human trafficking detection\nand misinformation monitoring. While direct supervision is limited, such graphs\nfrequently contain weak signals, noisy or indirect cues, that can still inform\nlearning. We propose WSNET, a novel weakly supervised graph contrastive\nlearning framework that leverages these weak signals to guide robust\nrepresentation learning. WSNET integrates graph structure, node features, and\nmultiple noisy supervision sources through a contrastive objective tailored for\nweakly labeled data. Across three real world datasets and synthetic benchmarks\nwith controlled noise, WSNET consistently outperforms state of the art\ncontrastive and noisy label learning methods by up to 15% in F1 score. Our\nresults highlight the effectiveness of contrastive learning under weak\nsupervision and the promise of exploiting imperfect labels in graph based\nsettings.", "AI": {"tldr": "WSNET\u662f\u4e00\u4e2a\u65b0\u7684\u5f31\u76d1\u7763\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u56fe\u4e2d\u7684\u5f31\u4fe1\u53f7\u8fdb\u884c\u9c81\u68d2\u8868\u793a\u5b66\u4e60\uff0c\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u56fe\u6570\u636e\u5e38\u9762\u4e34\u6807\u7b7e\u7a00\u7f3a\u548c\u566a\u58f0\u95ee\u9898\uff0c\u73b0\u6709\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\uff0c\u4f46\u56fe\u4e2d\u5e38\u5305\u542b\u5f31\u4fe1\u53f7\u53ef\u8f85\u52a9\u5b66\u4e60\u3002", "method": "\u63d0\u51faWSNET\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u76ee\u6807\u878d\u5408\u56fe\u7ed3\u6784\u3001\u8282\u70b9\u7279\u5f81\u548c\u591a\u6e90\u566a\u58f0\u76d1\u7763\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u548c\u5408\u6210\u57fa\u51c6\u4e0a\uff0cWSNET\u7684F1\u5206\u6570\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u9ad815%\u3002", "conclusion": "\u5f31\u76d1\u7763\u4e0b\u7684\u5bf9\u6bd4\u5b66\u4e60\u6709\u6548\u8bc1\u660e\u4e86\u5728\u56fe\u4e2d\u5229\u7528\u4e0d\u5b8c\u7f8e\u6807\u7b7e\u7684\u6f5c\u529b\u3002", "keywords": "\u5f31\u76d1\u7763\u5b66\u4e60, \u56fe\u5bf9\u6bd4\u5b66\u4e60, \u566a\u58f0\u6807\u7b7e, \u8282\u70b9\u5206\u7c7b"}}
{"id": "2506.02740", "pdf": "https://arxiv.org/pdf/2506.02740", "abs": "https://arxiv.org/abs/2506.02740", "authors": ["Ama\u00e7 Herda\u011fdelen", "Marco Baroni"], "title": "Stereotypical gender actions can be extracted from Web text", "categories": ["cs.CL"], "comment": null, "summary": "We extracted gender-specific actions from text corpora and Twitter, and\ncompared them to stereotypical expectations of people. We used Open Mind Common\nSense (OMCS), a commonsense knowledge repository, to focus on actions that are\npertinent to common sense and daily life of humans. We use the gender\ninformation of Twitter users and Web-corpus-based pronoun/name gender\nheuristics to compute the gender bias of the actions. With high recall, we\nobtained a Spearman correlation of 0.47 between corpus-based predictions and a\nhuman gold standard, and an area under the ROC curve of 0.76 when predicting\nthe polarity of the gold standard. We conclude that it is feasible to use\nnatural text (and a Twitter-derived corpus in particular) in order to augment\ncommonsense repositories with the stereotypical gender expectations of actions.\nWe also present a dataset of 441 commonsense actions with human judges' ratings\non whether the action is typically/slightly masculine/feminine (or neutral),\nand another larger dataset of 21,442 actions automatically rated by the methods\nwe investigate in this study.", "AI": {"tldr": "\u7814\u7a76\u4ece\u6587\u672c\u548cTwitter\u6570\u636e\u4e2d\u63d0\u53d6\u6027\u522b\u7279\u5b9a\u884c\u4e3a\uff0c\u5e76\u4e0e\u523b\u677f\u5370\u8c61\u5bf9\u6bd4\uff0c\u5229\u7528OMCS\u548c\u81ea\u52a8\u5316\u65b9\u6cd5\u8bc4\u4f30\u6027\u522b\u504f\u89c1\uff0c\u9a8c\u8bc1\u4e86\u5229\u7528\u81ea\u7136\u6587\u672c\u589e\u5f3a\u5e38\u8bc6\u5e93\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u81ea\u7136\u8bed\u8a00\u6570\u636e\uff08\u5982Twitter\uff09\u548c\u5e38\u8bc6\u77e5\u8bc6\u5e93\uff08\u5982OMCS\uff09\u6765\u91cf\u5316\u6027\u522b\u504f\u89c1\u884c\u4e3a\uff0c\u5e76\u4e0e\u4eba\u7c7b\u523b\u677f\u5370\u8c61\u5bf9\u6bd4\u9a8c\u8bc1\u3002", "method": "\u4f7f\u7528OMCS\u6355\u6349\u5e38\u8bc6\u884c\u4e3a\uff0c\u7ed3\u5408Twitter\u7528\u6237\u6027\u522b\u4fe1\u606f\u548c\u8bed\u6599\u5e93\u542f\u53d1\u5f0f\u65b9\u6cd5\u8ba1\u7b97\u884c\u4e3a\u6027\u522b\u504f\u89c1\uff0c\u5e76\u4e0e\u4eba\u5de5\u6807\u51c6\u5bf9\u6bd4\u3002", "result": "\u8bed\u6599\u5e93\u9884\u6d4b\u4e0e\u4eba\u5de5\u6807\u51c6\u7684\u65af\u76ae\u5c14\u66fc\u76f8\u5173\u6027\u4e3a0.47\uff0cROC\u66f2\u7ebf\u4e0b\u9762\u79ef\u4e3a0.76\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e24\u4e2a\u6570\u636e\u96c6\uff08\u4eba\u5de5\u8bc4\u5206\u548c\u81ea\u52a8\u8bc4\u5206\uff09\u3002", "conclusion": "\u5229\u7528\u81ea\u7136\u6587\u672c\uff08\u5c24\u5176\u662fTwitter\uff09\u53ef\u4ee5\u6709\u6548\u5730\u4e3a\u5e38\u8bc6\u5e93\u8865\u5145\u6027\u522b\u523b\u677f\u5370\u8c61\u6570\u636e\uff0c\u63d0\u51fa\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u5177\u6709\u4e00\u5b9a\u53ef\u9760\u6027\u3002", "keywords": "\u6027\u522b\u504f\u89c1\uff0c\u523b\u677f\u5370\u8c61\uff0cTwitter\uff0cOMCS\uff0c\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u6570\u636e\u96c6"}}
{"id": "2506.02032", "pdf": "https://arxiv.org/pdf/2506.02032", "abs": "https://arxiv.org/abs/2506.02032", "authors": ["Raj Patel", "Himanshu Tripathi", "Jasper Stone", "Noorbakhsh Amiri Golilarz", "Sudip Mittal", "Shahram Rahimi", "Vini Chaudhary"], "title": "Towards Secure MLOps: Surveying Attacks, Mitigation Strategies, and Research Challenges", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The rapid adoption of machine learning (ML) technologies has driven\norganizations across diverse sectors to seek efficient and reliable methods to\naccelerate model development-to-deployment. Machine Learning Operations (MLOps)\nhas emerged as an integrative approach addressing these requirements by\nunifying relevant roles and streamlining ML workflows. As the MLOps market\ncontinues to grow, securing these pipelines has become increasingly critical.\nHowever, the unified nature of MLOps ecosystem introduces vulnerabilities,\nmaking them susceptible to adversarial attacks where a single misconfiguration\ncan lead to compromised credentials, severe financial losses, damaged public\ntrust, and the poisoning of training data. Our paper presents a systematic\napplication of the MITRE ATLAS (Adversarial Threat Landscape for\nArtificial-Intelligence Systems) framework, a comprehensive and continuously\nupdated catalog of AI-focused attacks, to systematically assess attacks across\ndifferent phases of the MLOps ecosystem. We begin by examining the preparatory\nphases during which adversaries acquire the essential intelligence required to\ninitiate their attacks. We then present a structured taxonomy of attack\ntechniques explicitly mapped to corresponding phases of the MLOps ecosystem,\nsupported by examples drawn from red-teaming exercises and real-world\nincidents. This is followed by a taxonomy of mitigation strategies aligned with\nthese attack categories, offering actionable early-stage defenses to strengthen\nthe security of MLOps ecosystem. Given the rapid evolution and adoption of\nMLOps, we further highlight key research gaps that require immediate attention.\nOur work emphasizes the importance of implementing robust security protocols\nfrom the outset, empowering practitioners to safeguard MLOps ecosystem against\nevolving cyber attacks.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u5e94\u7528MITRE ATLAS\u6846\u67b6\u8bc4\u4f30MLOps\u751f\u6001\u7cfb\u7edf\u7684\u6f5c\u5728\u653b\u51fb\uff0c\u5e76\u63d0\u51fa\u5206\u7c7b\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u5f3a\u8c03\u5b89\u5168\u534f\u8bae\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740MLOps\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u5b89\u5168\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u9632\u5fa1\u6f5c\u5728\u653b\u51fb\u3002", "method": "\u91c7\u7528MITRE ATLAS\u6846\u67b6\uff0c\u5206\u6790MLOps\u751f\u6001\u7cfb\u7edf\u5404\u9636\u6bb5\u7684\u653b\u51fb\u6280\u672f\uff0c\u5e76\u63d0\u51fa\u76f8\u5e94\u7684\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u4e86\u653b\u51fb\u6280\u672f\u7684\u5206\u7c7b\u548c\u5bf9\u5e94\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u586b\u8865\u4e86MLOps\u5b89\u5168\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u4ece\u65e9\u671f\u9636\u6bb5\u5b9e\u65bd\u5b89\u5168\u534f\u8bae\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u4fdd\u62a4MLOps\u751f\u6001\u7cfb\u7edf\u514d\u53d7\u653b\u51fb\u3002", "keywords": "MLOps, MITRE ATLAS, \u5b89\u5168, \u653b\u51fb\u5206\u7c7b, \u9632\u5fa1\u7b56\u7565"}}
{"id": "2506.02475", "pdf": "https://arxiv.org/pdf/2506.02475", "abs": "https://arxiv.org/abs/2506.02475", "authors": ["Jiaxi Hu", "Yongqi Pan", "Jusen Du", "Disen Lan", "Xiaqiang Tang", "Qingsong Wen", "Yuxuan Liang", "Weigao Sun"], "title": "Comba: Improving Nonlinear RNNs with Closed-loop Control", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and\nRWKV-7 have achieved performance improvements by supervising the recurrent\nmemory management through Delta learning rule. Unlike previous state-space\nmodels (e.g., Mamba) and gated linear attentions (e.g., GLA), these models\nintroduce interactions between the recurrent state and the key vector,\nresulting in a nonlinear recursive structure. In this paper, we first introduce\nthe concept of Nonlinear RNNs with a comprehensive analysis on the advantages\nand limitations of these models. Then, based on closed-loop control theory, we\npropose a novel Nonlinear RNN variant named Comba, which adopts a\nscalar-plus-low-rank state transition, with both state feedback and output\nfeedback corrections. We also implement a hardware-efficient chunk-wise\nparallel kernel in Triton and train models with 340M/1.3B parameters on\nlarge-scale corpus. Comba demonstrates its superior performance and computation\nefficiency in both language and vision modeling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Comba\uff0c\u4e00\u79cd\u57fa\u4e8e\u95ed\u73af\u63a7\u5236\u7406\u8bba\u7684\u65b0\u578b\u975e\u7ebf\u6027RNN\u53d8\u4f53\uff0c\u7ed3\u5408\u72b6\u6001\u548c\u8f93\u51fa\u53cd\u9988\u6821\u6b63\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8bed\u8a00\u548c\u89c6\u89c9\u5efa\u6a21\u3002", "motivation": "\u7814\u7a76\u8fd1\u671f\u7684\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\uff08\u5982Gated DeltaNet\u7b49\uff09\u5728\u9012\u5f52\u72b6\u6001\u7ba1\u7406\u4e0a\u7684\u4f18\u52bf\u4e0e\u4e0d\u8db3\uff0c\u5e76\u57fa\u4e8e\u95ed\u73af\u63a7\u5236\u7406\u8bba\u6539\u8fdb\u975e\u7ebf\u6027RNN\u3002", "method": "\u63d0\u51faComba\u6a21\u578b\uff0c\u91c7\u7528\u6807\u91cf\u52a0\u4f4e\u79e9\u72b6\u6001\u8f6c\u79fb\uff0c\u5e76\u5f15\u5165\u72b6\u6001\u548c\u8f93\u51fa\u53cd\u9988\uff1b\u4f7f\u7528Triton\u5b9e\u73b0\u786c\u4ef6\u9ad8\u6548\u7684\u5e76\u884c\u6838\uff0c\u8bad\u7ec3\u5927\u53c2\u6570\u6a21\u578b\u3002", "result": "Comba\u5728\u8bed\u8a00\u548c\u89c6\u89c9\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "Comba\u901a\u8fc7\u521b\u65b0\u7684\u9012\u5f52\u7ed3\u6784\u548c\u53cd\u9988\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e8f\u5217\u5efa\u6a21\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "keywords": "\u975e\u7ebf\u6027RNN, \u95ed\u73af\u63a7\u5236, Comba, \u5e8f\u5217\u5efa\u6a21, \u786c\u4ef6\u9ad8\u6548"}}
{"id": "2506.02753", "pdf": "https://arxiv.org/pdf/2506.02753", "abs": "https://arxiv.org/abs/2506.02753", "authors": ["Aisha Alansari", "Hamzah Luqman"], "title": "Multi-task Learning with Active Learning for Arabic Offensive Speech Detection", "categories": ["cs.CL"], "comment": null, "summary": "The rapid growth of social media has amplified the spread of offensive,\nviolent, and vulgar speech, which poses serious societal and cybersecurity\nconcerns. Detecting such content in Arabic text is particularly complex due to\nlimited labeled data, dialectal variations, and the language's inherent\ncomplexity. This paper proposes a novel framework that integrates multi-task\nlearning (MTL) with active learning to enhance offensive speech detection in\nArabic social media text. By jointly training on two auxiliary tasks, violent\nand vulgar speech, the model leverages shared representations to improve the\ndetection accuracy of the offensive speech. Our approach dynamically adjusts\ntask weights during training to balance the contribution of each task and\noptimize performance. To address the scarcity of labeled data, we employ an\nactive learning strategy through several uncertainty sampling techniques to\niteratively select the most informative samples for model training. We also\nintroduce weighted emoji handling to better capture semantic cues. Experimental\nresults on the OSACT2022 dataset show that the proposed framework achieves a\nstate-of-the-art macro F1-score of 85.42%, outperforming existing methods while\nusing significantly fewer fine-tuning samples. The findings of this study\nhighlight the potential of integrating MTL with active learning for efficient\nand accurate offensive language detection in resource-constrained settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u4e3b\u52a8\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u963f\u62c9\u4f2f\u793e\u4ea4\u5a92\u4f53\u4e2d\u5192\u72af\u6027\u8bed\u8a00\u7684\u68c0\u6d4b\u6548\u679c\uff0c\u5e76\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e2d\u5192\u72af\u6027\u3001\u66b4\u529b\u548c\u7c97\u4fd7\u8bed\u8a00\u7684\u6cdb\u6ee5\u5f15\u53d1\u4e86\u793e\u4f1a\u548c\u7f51\u7edc\u5b89\u5168\u95ee\u9898\uff0c\u800c\u963f\u62c9\u4f2f\u8bed\u7684\u590d\u6742\u6027\u548c\u6570\u636e\u7a00\u7f3a\u6027\u589e\u52a0\u4e86\u68c0\u6d4b\u96be\u5ea6\u3002", "method": "\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u8054\u5408\u8bad\u7ec3\u66b4\u529b\u8bed\u8a00\u548c\u7c97\u4fd7\u8bed\u8a00\u4e24\u4e2a\u8f85\u52a9\u4efb\u52a1\uff0c\u52a8\u6001\u8c03\u6574\u4efb\u52a1\u6743\u91cd\uff0c\u5e76\u5229\u7528\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u91c7\u6837\u9009\u62e9\u4fe1\u606f\u91cf\u5927\u7684\u6837\u672c\uff0c\u540c\u65f6\u5f15\u5165\u52a0\u6743\u8868\u60c5\u5904\u7406\u3002", "result": "\u5728OSACT2022\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u53d6\u5f97\u4e8685.42%\u7684\u5b8fF1\u5206\u6570\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u4f7f\u7528\u66f4\u5c11\u7684\u5fae\u8c03\u6837\u672c\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u4e3b\u52a8\u5b66\u4e60\u7684\u6846\u67b6\u6709\u52a9\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u9ad8\u6548\u51c6\u786e\u5730\u68c0\u6d4b\u5192\u72af\u6027\u8bed\u8a00\u3002", "keywords": "\u5192\u72af\u6027\u8bed\u8a00\u68c0\u6d4b\u3001\u591a\u4efb\u52a1\u5b66\u4e60\u3001\u4e3b\u52a8\u5b66\u4e60\u3001\u963f\u62c9\u4f2f\u8bed\u3001\u793e\u4ea4\u5a92\u4f53"}}
{"id": "2506.02504", "pdf": "https://arxiv.org/pdf/2506.02504", "abs": "https://arxiv.org/abs/2506.02504", "authors": ["Xingyu Chen", "Bokun Wang", "Ming Yang", "Quanqi Hu", "Qihang Lin", "Tianbao Yang"], "title": "Stochastic Momentum Methods for Non-smooth Non-Convex Finite-Sum Coupled Compositional Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Finite-sum Coupled Compositional Optimization (FCCO), characterized by its\ncoupled compositional objective structure, emerges as an important optimization\nparadigm for addressing a wide range of machine learning problems. In this\npaper, we focus on a challenging class of non-convex non-smooth FCCO, where the\nouter functions are non-smooth weakly convex or convex and the inner functions\nare smooth or weakly convex. Existing state-of-the-art result face two key\nlimitations: (1) a high iteration complexity of $O(1/\\epsilon^6)$ under the\nassumption that the stochastic inner functions are Lipschitz continuous in\nexpectation; (2) reliance on vanilla SGD-type updates, which are not suitable\nfor deep learning applications. Our main contributions are two fold: (i) We\npropose stochastic momentum methods tailored for non-smooth FCCO that come with\nprovable convergence guarantees; (ii) We establish a new state-of-the-art\niteration complexity of $O(1/\\epsilon^5)$. Moreover, we apply our algorithms to\nmultiple inequality constrained non-convex optimization problems involving\nsmooth or weakly convex functional inequality constraints. By optimizing a\nsmoothed hinge penalty based formulation, we achieve a new state-of-the-art\ncomplexity of $O(1/\\epsilon^5)$ for finding an (nearly) $\\epsilon$-level KKT\nsolution. Experiments on three tasks demonstrate the effectiveness of the\nproposed algorithms.", "AI": {"tldr": "\u9488\u5bf9\u975e\u51f8\u975e\u5149\u6ed1FCCO\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u968f\u673a\u52a8\u91cf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u8fed\u4ee3\u590d\u6742\u6027\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u591a\u7b49\u5f0f\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u975e\u51f8\u975e\u5149\u6ed1FCCO\u95ee\u9898\u65f6\u5b58\u5728\u9ad8\u8fed\u4ee3\u590d\u6742\u6027\u548c\u4e0d\u9002\u5408\u6df1\u5ea6\u5b66\u4e60\u7684\u5c40\u9650\u6027\uff0c\u8feb\u5207\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u975e\u5149\u6ed1FCCO\u7684\u968f\u673a\u52a8\u91cf\u65b9\u6cd5\uff0c\u5e76\u4f18\u5316\u4e86\u57fa\u4e8e\u5e73\u6ed1\u94f0\u94fe\u60e9\u7f5a\u7684\u8868\u8fbe\u5f0f\u3002", "result": "\u5b9e\u73b0\u4e86O(1/\u03b5^5)\u7684\u8fed\u4ee3\u590d\u6742\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7ed3\u679c\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3aFCCO\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "FCCO, \u975e\u51f8\u4f18\u5316, \u52a8\u91cf\u65b9\u6cd5, \u590d\u6742\u6027\u5206\u6790"}}
{"id": "2506.02758", "pdf": "https://arxiv.org/pdf/2506.02758", "abs": "https://arxiv.org/abs/2506.02758", "authors": ["Stefano Bann\u00f2", "Kate Knill", "Mark Gales"], "title": "Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications", "summary": "Vocabulary use is a fundamental aspect of second language (L2) proficiency.\nTo date, its assessment by automated systems has typically examined the\ncontext-independent, or part-of-speech (PoS) related use of words. This paper\nintroduces a novel approach to enable fine-grained vocabulary evaluation\nexploiting the precise use of words within a sentence. The scheme combines\nlarge language models (LLMs) with the English Vocabulary Profile (EVP). The EVP\nis a standard lexical resource that enables in-context vocabulary use to be\nlinked with proficiency level. We evaluate the ability of LLMs to assign\nproficiency levels to individual words as they appear in L2 learner writing,\naddressing key challenges such as polysemy, contextual variation, and\nmulti-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to\nexploit additional semantic information that yields improved performance. We\nalso explore correlations between word-level proficiency and essay-level\nproficiency. Finally, the approach is applied to examine the consistency of the\nEVP proficiency levels. Results show that LLMs are well-suited for the task of\nvocabulary assessment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u82f1\u8bed\u8bcd\u6c47\u6863\u6848\uff08EVP\uff09\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bcd\u6c47\u8bc4\u4f30\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u4e0a\u4e0b\u6587\u5206\u6790\u63d0\u9ad8\u7b2c\u4e8c\u8bed\u8a00\uff08L2\uff09\u5b66\u4e60\u8005\u5199\u4f5c\u4e2d\u8bcd\u6c47\u4f7f\u7528\u7684\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "motivation": "\u76ee\u524d\uff0c\u8bcd\u6c47\u8bc4\u4f30\u591a\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\u65e0\u5173\u6216\u8bcd\u6027\u76f8\u5173\u7684\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4e0a\u4e0b\u6587\u5206\u6790\u63d0\u5347\u8bcd\u6c47\u8bc4\u4f30\u7684\u7cbe\u786e\u6027\u3002", "method": "\u7ed3\u5408LLMs\u548cEVP\uff0c\u8bc4\u4f30LLMs\u5728L2\u5b66\u4e60\u8005\u5199\u4f5c\u4e2d\u4e3a\u5355\u4e2a\u8bcd\u6c47\u5206\u914d\u719f\u7ec3\u5ea6\u7684\u80fd\u529b\uff0c\u5e76\u89e3\u51b3\u591a\u4e49\u6027\u3001\u4e0a\u4e0b\u6587\u53d8\u5316\u548c\u591a\u8bcd\u8868\u8fbe\u7b49\u6311\u6218\u3002", "result": "LLMs\u5728\u8bcd\u6c47\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u8bcd\u6027\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u5229\u7528\u989d\u5916\u7684\u8bed\u4e49\u4fe1\u606f\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "LLMs\u9002\u7528\u4e8e\u8bcd\u6c47\u8bc4\u4f30\u4efb\u52a1\uff0c\u80fd\u591f\u6709\u6548\u5206\u6790\u5b66\u4e60\u8005\u8bcd\u6c47\u4f7f\u7528\u7684\u719f\u7ec3\u5ea6\u3002", "keywords": "\u8bcd\u6c47\u8bc4\u4f30\u3001\u7b2c\u4e8c\u8bed\u8a00\u5b66\u4e60\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u82f1\u8bed\u8bcd\u6c47\u6863\u6848\u3001\u719f\u7ec3\u5ea6"}}
{"id": "2506.02039", "pdf": "https://arxiv.org/pdf/2506.02039", "abs": "https://arxiv.org/abs/2506.02039", "authors": ["Haoshuai Zhou", "Changgeng Mo", "Boxuan Cao", "Linkai Li", "Shan Xiang Wang"], "title": "No Audiogram: Leveraging Existing Scores for Personalized Speech Intelligibility Prediction", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": "Accepted at Interspeech 2025", "summary": "Personalized speech intelligibility prediction is challenging. Previous\napproaches have mainly relied on audiograms, which are inherently limited in\naccuracy as they only capture a listener's hearing threshold for pure tones.\nRather than incorporating additional listener features, we propose a novel\napproach that leverages an individual's existing intelligibility data to\npredict their performance on new audio. We introduce the Support Sample-Based\nIntelligibility Prediction Network (SSIPNet), a deep learning model that\nleverages speech foundation models to build a high-dimensional representation\nof a listener's speech recognition ability from multiple support (audio, score)\npairs, enabling accurate predictions for unseen audio. Results on the Clarity\nPrediction Challenge dataset show that, even with a small number of support\n(audio, score) pairs, our method outperforms audiogram-based predictions. Our\nwork presents a new paradigm for personalized speech intelligibility\nprediction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u652f\u6301\u6837\u672c\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08SSIPNet\uff09\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u8bed\u97f3\u6e05\u6670\u5ea6\u9884\u6d4b\uff0c\u901a\u8fc7\u5229\u7528\u73b0\u6709\u7528\u6237\u7684\u8bed\u97f3\u8bc6\u522b\u6570\u636e\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u542c\u529b\u56fe\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u542c\u529b\u56fe\u65b9\u6cd5\u4ec5\u6355\u6349\u7eaf\u97f3\u7684\u542c\u529b\u9608\u503c\uff0c\u51c6\u786e\u6027\u6709\u9650\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5229\u7528\u7528\u6237\u7684\u8bed\u97f3\u8bc6\u522b\u6570\u636e\u6765\u63d0\u5347\u9884\u6d4b\u6548\u679c\u3002", "method": "\u63d0\u51fa\u652f\u6301\u6837\u672c\u667a\u80fd\u9884\u6d4b\u7f51\u7edc\uff08SSIPNet\uff09\uff0c\u5229\u7528\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u4ece\u591a\u4e2a\u652f\u6301\uff08\u97f3\u9891\u3001\u5206\u6570\uff09\u5bf9\u4e2d\u6784\u5efa\u9ad8\u7ef4\u8868\u793a\uff0c\u9884\u6d4b\u65b0\u97f3\u9891\u7684\u6027\u80fd\u3002", "result": "\u5728Clarity Prediction Challenge\u6570\u636e\u96c6\u4e0a\uff0c\u5373\u4f7f\u652f\u6301\u6837\u672c\u6570\u91cf\u8f83\u5c11\uff0cSSIPNet\u7684\u8868\u73b0\u4e5f\u4f18\u4e8e\u57fa\u4e8e\u542c\u529b\u56fe\u7684\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2a\u6027\u5316\u8bed\u97f3\u6e05\u6670\u5ea6\u9884\u6d4b\u7684\u65b0\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u5229\u7528\u73b0\u6709\u6570\u636e\u7684\u4f18\u52bf\u3002", "keywords": "\u4e2a\u6027\u5316\u8bed\u97f3\u9884\u6d4b\u3001SSIPNet\u3001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3001\u652f\u6301\u6837\u672c\u3001\u542c\u529b\u56fe"}}
{"id": "2506.02539", "pdf": "https://arxiv.org/pdf/2506.02539", "abs": "https://arxiv.org/abs/2506.02539", "authors": ["Thong Q. Nguyen", "Shubhang Desai", "Yash Jain", "Tanvir Aumi", "Vishal Chowdhary"], "title": "VerificAgent: Integrating Expert Knowledge and Fact-Checked Memory for Robust Domain-Specific Task Planning", "categories": ["cs.LG"], "comment": null, "summary": "Continual memory augmentation allows computer-use agents (CUAs) to learn from\npast interactions and refine their task-solving strategies over time. However,\nunchecked memory accumulation can introduce spurious or hallucinated\n\"learnings\" that degrade agent performance, particularly in domain-specific\nworkflows such as productivity software. We present a novel framework,\nVerificAgent, that effectively manages memory for CUAs through (1) an\nexpert-curated seed of domain knowledge, (2) iterative, trajectory-based memory\nrefinement during training, and (3) a post-hoc fact-checking pass by human\nexperts to sanitize accumulated memory before deployment. On OSWorld\nproductivity tasks, VerificAgent achieves a 111.1% relative improvement in\nsuccess rate over baseline CUA without any additional fine-tuning.", "AI": {"tldr": "VerificAgent\u6846\u67b6\u901a\u8fc7\u4e13\u5bb6\u77e5\u8bc6\u3001\u8fed\u4ee3\u8bad\u7ec3\u540e\u7684\u4eba\u4e3a\u4e8b\u5b9e\u68c0\u67e5\uff0c\u63d0\u5347\u8ba1\u7b97\u673a\u4ee3\u7406\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u673a\u4ee3\u7406\u56e0\u8bb0\u5fc6\u79ef\u7d2f\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e2d\u3002", "method": "\u7ed3\u5408\u4e13\u5bb6\u77e5\u8bc6\u3001\u8fed\u4ee3\u8bad\u7ec3\u548c\u4eba\u4e3a\u4e8b\u5b9e\u68c0\u67e5\u4e09\u6b65\u7ba1\u7406\u8bb0\u5fc6\u3002", "result": "\u5728OSWorld\u4efb\u52a1\u4e2d\uff0cVerificAgent\u6bd4\u57fa\u7ebf\u8868\u73b0\u63d0\u5347111.1%\u3002", "conclusion": "VerificAgent\u6709\u6548\u7ba1\u7406\u8bb0\u5fc6\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u3002", "keywords": "VerificAgent, \u8bb0\u5fc6\u7ba1\u7406, \u8ba1\u7b97\u673a\u4ee3\u7406, \u4e8b\u5b9e\u68c0\u67e5"}}
{"id": "2506.02803", "pdf": "https://arxiv.org/pdf/2506.02803", "abs": "https://arxiv.org/abs/2506.02803", "authors": ["Sifan Li", "Yujun Cai", "Yiwei Wang"], "title": "SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) excel in semantic tasks but falter at a core\nhuman capability: detecting hidden content in optical illusions or AI-generated\nimages through perceptual adjustments like zooming. We introduce HC-Bench, a\nbenchmark of 112 images with hidden text, objects, and illusions, revealing\nthat leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit\nprompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to\nan overreliance on high-level semantics. Strikingly, we propose SemVink\n(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128\npixels), which unlocks >99% accuracy by eliminating redundant visual noise.\nThis exposes a critical architectural flaw: VLMs prioritize abstract reasoning\nover low-level visual operations crucial for real-world robustness. Our work\nurges a shift toward hybrid models integrating multi-scale processing, bridging\nthe gap between computational vision and human cognition for applications in\nmedical imaging, security, and beyond.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aHC-Bench\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u9886\u5148\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u68c0\u6d4b\u9690\u85cf\u5185\u5bb9\uff08\u5982\u5149\u5b66\u5e7b\u89c9\u6216AI\u751f\u6210\u56fe\u50cf\uff09\u65b9\u9762\u7684\u80fd\u529b\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u89e3\u51b3\u65b9\u6848SemVink\uff0c\u901a\u8fc7\u964d\u4f4e\u56fe\u50cf\u5206\u8fa8\u7387\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3VLMs\u5728\u68c0\u6d4b\u9690\u85cf\u5185\u5bb9\u4e0a\u7684\u4e0d\u8db3\uff0c\u63ed\u793a\u4e86\u5176\u8fc7\u4e8e\u4f9d\u8d56\u9ad8\u7ea7\u8bed\u4e49\u800c\u5ffd\u89c6\u4f4e\u5c42\u6b21\u89c6\u89c9\u64cd\u4f5c\u7684\u7f3a\u9677\u3002", "method": "\u63d0\u51fa\u4e86HC-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b112\u5f20\u5e26\u6709\u9690\u85cf\u6587\u672c\u3001\u7269\u4f53\u548c\u5e7b\u89c9\u7684\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7SemVink\u65b9\u6cd5\uff08\u5c06\u56fe\u50cf\u7f29\u653e\u81f3\u4f4e\u5206\u8fa8\u7387\uff09\u6765\u6d88\u9664\u89c6\u89c9\u566a\u58f0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVLMs\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u51c6\u786e\u7387\u6781\u4f4e\uff080-5.36%\uff09\uff0c\u800cSemVink\u65b9\u6cd5\u5c06\u51c6\u786e\u7387\u63d0\u5347\u81f3\u8d85\u8fc799%\u3002", "conclusion": "\u8bba\u6587\u547c\u5401\u5f00\u53d1\u6df7\u5408\u6a21\u578b\uff0c\u6574\u5408\u591a\u5c3a\u5ea6\u5904\u7406\uff0c\u4ee5\u5f25\u8865\u8ba1\u7b97\u89c6\u89c9\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u9002\u7528\u4e8e\u533b\u7597\u5f71\u50cf\u3001\u5b89\u5168\u7b49\u9886\u57df\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs), HC-Bench, SemVink, \u9690\u85cf\u5185\u5bb9\u68c0\u6d4b, \u591a\u5c3a\u5ea6\u5904\u7406"}}
{"id": "2506.02541", "pdf": "https://arxiv.org/pdf/2506.02541", "abs": "https://arxiv.org/abs/2506.02541", "authors": ["Minsung Kim", "Nakyeong Yang", "Kyomin Jung"], "title": "Rethinking Post-Unlearning Behavior of Large Vision-Language Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "10 pages, 5 figures", "summary": "Machine unlearning is used to mitigate the privacy risks of Large\nVision-Language Models (LVLMs) arising from training on large-scale web data.\nHowever, existing unlearning methods often fail to carefully select substitute\noutputs for forget targets, resulting in Unlearning Aftermaths-undesirable\nbehaviors such as degenerate, hallucinated, or excessively refused responses.\nWe highlight that, especially for generative LVLMs, it is crucial to consider\nthe quality and informativeness of post-unlearning responses rather than\nrelying solely on naive suppression. To address this, we introduce a new\nunlearning task for LVLMs that requires models to provide privacy-preserving\nyet informative and visually grounded responses. We also propose PUBG, a novel\nunlearning method that explicitly guides post-unlearning behavior toward a\ndesirable output distribution. Experiments show that, while existing methods\nsuffer from Unlearning Aftermaths despite successfully preventing privacy\nviolations, PUBG effectively mitigates these issues, generating visually\ngrounded and informative responses without privacy leakage for forgotten\ntargets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5PUBG\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u5bfc\u81f4\u7684\u9690\u79c1\u6cc4\u9732\u548c\u54cd\u5e94\u8d28\u91cf\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u5728\u6d88\u9664\u9690\u79c1\u98ce\u9669\u65f6\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u5bf9\u6a21\u578b\u54cd\u5e94\u8d28\u91cf\u548c\u4fe1\u606f\u91cf\u7684\u8981\u6c42\uff0c\u5bfc\u81f4\u9000\u5316\u3001\u5e7b\u89c9\u6216\u8fc7\u5ea6\u62d2\u7edd\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9057\u5fd8\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u63d0\u4f9b\u65e2\u4fdd\u62a4\u9690\u79c1\u53c8\u4fe1\u606f\u4e30\u5bcc\u4e14\u89c6\u89c9\u57fa\u7840\u7684\u54cd\u5e94\u3002PUBG\u65b9\u6cd5\u901a\u8fc7\u660e\u786e\u5f15\u5bfc\u9057\u5fd8\u540e\u7684\u884c\u4e3a\u5411\u7406\u60f3\u8f93\u51fa\u5206\u5e03\u6765\u89e3\u51b3\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPUBG\u80fd\u6709\u6548\u907f\u514d\u9690\u79c1\u6cc4\u9732\uff0c\u540c\u65f6\u751f\u6210\u89c6\u89c9\u57fa\u7840\u548c\u4fe1\u606f\u4e30\u5bcc\u7684\u54cd\u5e94\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u9632\u6b62\u9690\u79c1\u4fb5\u72af\u7684\u540c\u65f6\u4ecd\u5b58\u5728\u54cd\u5e94\u8d28\u91cf\u95ee\u9898\u3002", "conclusion": "PUBG\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9057\u5fd8\u540e\u9057\u75c7\u95ee\u9898\uff0c\u6210\u4e3a\u517c\u987e\u9690\u79c1\u4fdd\u62a4\u548c\u54cd\u5e94\u8d28\u91cf\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u673a\u5668\u5b66\u4e60\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u9690\u79c1\u4fdd\u62a4\u3001\u9057\u5fd8\u7b97\u6cd5\u3001PUBG"}}
{"id": "2506.02818", "pdf": "https://arxiv.org/pdf/2506.02818", "abs": "https://arxiv.org/abs/2506.02818", "authors": ["Ekaterina Grishina", "Mikhail Gorbunov", "Maxim Rakhuba"], "title": "ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted by ACL Findings", "summary": "Large language models (LLMs) demonstrate impressive results in natural\nlanguage processing tasks but require a significant amount of computational and\nmemory resources. Structured matrix representations are a promising way for\nreducing the number of parameters of these models. However, it seems\nunrealistic to expect that weight matrices of pretrained models can be\naccurately represented by structured matrices without any fine-tuning. To\novercome this issue, we utilize the fact that LLM output is invariant under\ncertain orthogonal transformations of weight matrices. This insight can be\nleveraged to identify transformations that significantly improve the\ncompressibility of weights within structured classes. The proposed approach is\napplicable to various types of structured matrices that support efficient\nprojection operations. Code is available at\nhttps://github.com/GrishKate/ProcrustesGPT", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6b63\u4ea4\u53d8\u6362\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6743\u91cd\u77e9\u9635\u538b\u7f29\u7684\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\uff0c\u56e0\u6b64\u9700\u8981\u51cf\u5c11\u5176\u53c2\u6570\u6570\u91cf\u3002", "method": "\u901a\u8fc7\u5229\u7528LLMs\u8f93\u51fa\u5728\u7279\u5b9a\u6b63\u4ea4\u53d8\u6362\u4e0b\u7684\u4e0d\u53d8\u6027\uff0c\u4f18\u5316\u6743\u91cd\u77e9\u9635\u7684\u53ef\u538b\u7f29\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7ed3\u6784\u5316\u77e9\u9635\u7c7b\u578b\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u635f\u5931\u6a21\u578b\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u538b\u7f29\u6743\u91cd\u77e9\u9635\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u51cf\u5c11LLMs\u8d44\u6e90\u9700\u6c42\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u9014\u5f84\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7ed3\u6784\u5316\u77e9\u9635\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u6743\u91cd\u77e9\u9635, \u6b63\u4ea4\u53d8\u6362, \u7ed3\u6784\u5316\u77e9\u9635, \u6a21\u578b\u538b\u7f29"}}
{"id": "2506.02046", "pdf": "https://arxiv.org/pdf/2506.02046", "abs": "https://arxiv.org/abs/2506.02046", "authors": ["Mohammad Saleh Torkestani", "Taha Mansouri"], "title": "Machine vs Machine: Using AI to Tackle Generative AI Threats in Assessment", "categories": ["cs.CY", "cs.AI", "K.3.1"], "comment": "Paper presented at the Learning, Teaching & Student Experience 2025\n  Conference. The Chartered Association of Business Schools (CABS), Nottingham,\n  UK", "summary": "This paper presents a theoretical framework for addressing the challenges\nposed by generative artificial intelligence (AI) in higher education assessment\nthrough a machine-versus-machine approach. Large language models like GPT-4,\nClaude, and Llama increasingly demonstrate the ability to produce sophisticated\nacademic content, traditional assessment methods face an existential threat,\nwith surveys indicating 74-92% of students experimenting with these tools for\nacademic purposes. Current responses, ranging from detection software to manual\nassessment redesign, show significant limitations: detection tools demonstrate\nbias against non-native English writers and can be easily circumvented, while\nmanual frameworks rely heavily on subjective judgment and assume static AI\ncapabilities. This paper introduces a dual strategy paradigm combining static\nanalysis and dynamic testing to create a comprehensive theoretical framework\nfor assessment vulnerability evaluation. The static analysis component\ncomprises eight theoretically justified elements: specificity and\ncontextualization, temporal relevance, process visibility requirements,\npersonalization elements, resource accessibility, multimodal integration,\nethical reasoning requirements, and collaborative elements. Each element\naddresses specific limitations in generative AI capabilities, creating barriers\nthat distinguish authentic human learning from AI-generated simulation. The\ndynamic testing component provides a complementary approach through\nsimulation-based vulnerability assessment, addressing limitations in\npattern-based analysis. The paper presents a theoretical framework for\nvulnerability scoring, including the conceptual basis for quantitative\nassessment, weighting frameworks, and threshold determination theory.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u9759\u6001\u5206\u6790\u548c\u52a8\u6001\u6d4b\u8bd5\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u751f\u6210\u5f0fAI\u5728\u9ad8\u7b49\u6559\u80b2\u8bc4\u4f30\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u751f\u6210\u5f0fAI\uff08\u5982GPT-4\uff09\u80fd\u591f\u751f\u6210\u590d\u6742\u7684\u5b66\u672f\u5185\u5bb9\uff0c\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u9762\u4e34\u5a01\u80c1\uff0c\u5f53\u524d\u68c0\u6d4b\u5de5\u5177\u548c\u624b\u52a8\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u9759\u6001\u5206\u6790\u548c\u52a8\u6001\u6d4b\u8bd5\u7684\u53cc\u91cd\u7b56\u7565\uff0c\u9759\u6001\u5206\u6790\u5305\u542b\u516b\u4e2a\u7406\u8bba\u5143\u7d20\uff0c\u52a8\u6001\u6d4b\u8bd5\u901a\u8fc7\u4eff\u771f\u8bc4\u4f30\u6f0f\u6d1e\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6f0f\u6d1e\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5305\u62ec\u8bc4\u5206\u57fa\u7840\u548c\u9608\u503c\u786e\u5b9a\u7406\u8bba\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u533a\u5206\u771f\u5b9e\u4eba\u7c7b\u5b66\u4e60\u4e0eAI\u751f\u6210\u5185\u5bb9\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "keywords": "\u751f\u6210\u5f0fAI\u3001\u9ad8\u7b49\u6559\u80b2\u8bc4\u4f30\u3001\u9759\u6001\u5206\u6790\u3001\u52a8\u6001\u6d4b\u8bd5"}}
{"id": "2506.02542", "pdf": "https://arxiv.org/pdf/2506.02542", "abs": "https://arxiv.org/abs/2506.02542", "authors": ["Niklas Kormann", "Masoud Ramuz", "Zeeshan Nisar", "Nadine S. Schaadt", "Hendrik Annuth", "Benjamin Doerr", "Friedrich Feuerhake", "Thomas Lampert", "Johannes F. Lutzeyer"], "title": "HIEGNet: A Heterogenous Graph Neural Network Including the Immune Environment in Glomeruli Classification", "categories": ["cs.LG", "cs.AI", "cs.CV", "q-bio.QM"], "comment": "Accepted for poster presentation at MIDL 2025", "summary": "Graph Neural Networks (GNNs) have recently been found to excel in\nhistopathology. However, an important histopathological task, where GNNs have\nnot been extensively explored, is the classification of glomeruli health as an\nimportant indicator in nephropathology. This task presents unique difficulties,\nparticularly for the graph construction, i.e., the identification of nodes,\nedges, and informative features. In this work, we propose a pipeline composed\nof different traditional and machine learning-based computer vision techniques\nto identify nodes, edges, and their corresponding features to form a\nheterogeneous graph. We then proceed to propose a novel heterogeneous GNN\narchitecture for glomeruli classification, called HIEGNet, that integrates both\nglomeruli and their surrounding immune cells. Hence, HIEGNet is able to\nconsider the immune environment of each glomerulus in its classification. Our\nHIEGNet was trained and tested on a dataset of Whole Slide Images from kidney\ntransplant patients. Experimental results demonstrate that HIEGNet outperforms\nseveral baseline models and generalises best between patients among all\nbaseline models. Our implementation is publicly available at\nhttps://github.com/nklsKrmnn/HIEGNet.git.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f02\u6784GNN\u67b6\u6784HIEGNet\uff0c\u7528\u4e8e\u80be\u5c0f\u7403\u5206\u7c7b\uff0c\u901a\u8fc7\u6574\u5408\u80be\u5c0f\u7403\u53ca\u5176\u5468\u56f4\u514d\u75ab\u7ec6\u80de\u7684\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u80be\u5c0f\u7403\u5065\u5eb7\u5206\u7c7b\u662f\u80be\u75c5\u5b66\u4e2d\u7684\u91cd\u8981\u4efb\u52a1\uff0c\u4f46\u73b0\u6709GNN\u65b9\u6cd5\u5728\u6b64\u4efb\u52a1\u4e2d\u5c1a\u672a\u6df1\u5165\u63a2\u7d22\uff0c\u5c24\u5176\u662f\u5728\u56fe\u6784\u5efa\uff08\u8282\u70b9\u3001\u8fb9\u548c\u7279\u5f81\u7684\u8bc6\u522b\uff09\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f20\u7edf\u548c\u673a\u5668\u5b66\u4e60\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u7684\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u6784\u5efa\u5f02\u6784\u56fe\uff1b\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u540d\u4e3aHIEGNet\u7684\u65b0\u578b\u5f02\u6784GNN\u67b6\u6784\uff0c\u6574\u5408\u80be\u5c0f\u7403\u548c\u514d\u75ab\u7ec6\u80de\u4fe1\u606f\u3002", "result": "HIEGNet\u5728\u80be\u79fb\u690d\u60a3\u8005\u7684\u5168\u5e7b\u706f\u7247\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14\u5728\u60a3\u8005\u95f4\u6cdb\u5316\u80fd\u529b\u6700\u4f73\u3002", "conclusion": "HIEGNet\u901a\u8fc7\u8003\u8651\u80be\u5c0f\u7403\u7684\u514d\u75ab\u73af\u5883\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80be\u5c0f\u7403\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "keywords": "\u56fe\u795e\u7ecf\u7f51\u7edc, \u80be\u75c5\u7406\u5b66, \u5f02\u6784\u56fe, \u8ba1\u7b97\u673a\u89c6\u89c9, \u80be\u5c0f\u7403\u5206\u7c7b"}}
{"id": "2506.02827", "pdf": "https://arxiv.org/pdf/2506.02827", "abs": "https://arxiv.org/abs/2506.02827", "authors": ["Yulin Dou", "Jiangming Liu"], "title": "TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory Optimization for Eliciting Human Preference", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) can effectively elicit human preferences through\nmulti-turn dialogue. Complex tasks can be accomplished through iterative\nclarifying questions and final responses generated by an LLM acting as a\nquestioner (STaR-GATE; Andukuri et al., 2024}). However, existing approaches\nbased on self-taught reasoning struggle to identify optimal dialogue\ntrajectories and avoid irrelevant questions to the tasks. To address this\nlimitation, we propose TO-GATE, a novel framework that enhances question\ngeneration through trajectory optimization, which consists of two key\ncomponents: a clarification resolver that generates optimal questioning\ntrajectories, and a summarizer that ensures task-aligned final responses. The\ntrajectory optimization enables the model to produce effective elicitation\nquestions and summary responses tailored to specific tasks. Experimental\nresults demonstrate that TO-GATE significantly outperforms baseline methods,\nachieving a 9.32% improvement on standard preference elicitation tasks.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86TO-GATE\u6846\u67b6\uff0c\u901a\u8fc7\u8f68\u8ff9\u4f18\u5316\u63d0\u5347\u751f\u6210\u6709\u6548\u5bf9\u8bdd\u95ee\u9898\u7684\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u81ea\u5b66\u4e60\u63a8\u7406\u7684\u65b9\u6cd5\u96be\u4ee5\u8bc6\u522b\u6700\u4f73\u5bf9\u8bdd\u8f68\u8ff9\uff0c\u5bfc\u81f4\u4efb\u52a1\u65e0\u5173\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faTO-GATE\u6846\u67b6\uff0c\u5305\u542b\u6f84\u6e05\u89e3\u6790\u5668\uff08\u751f\u6210\u6700\u4f18\u63d0\u95ee\u8f68\u8ff9\uff09\u548c\u603b\u7ed3\u5668\uff08\u786e\u4fdd\u4efb\u52a1\u5bf9\u9f50\u7684\u6700\u7ec8\u54cd\u5e94\uff09\u3002", "result": "TO-GATE\u5728\u6807\u51c6\u504f\u597d\u83b7\u53d6\u4efb\u52a1\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e869.32%\u3002", "conclusion": "TO-GATE\u901a\u8fc7\u8f68\u8ff9\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bdd\u95ee\u9898\u7684\u751f\u6210\u6548\u679c\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u5bf9\u8bdd\u7cfb\u7edf, \u8f68\u8ff9\u4f18\u5316, \u504f\u597d\u83b7\u53d6"}}
{"id": "2506.02048", "pdf": "https://arxiv.org/pdf/2506.02048", "abs": "https://arxiv.org/abs/2506.02048", "authors": ["Lajos Muzsai", "David Imolai", "Andr\u00e1s Luk\u00e1cs"], "title": "Improving LLM Agents with Reinforcement Learning on Cryptographic CTF Challenges", "categories": ["cs.CR", "cs.AI", "68M25", "I.2.1; K.6.5"], "comment": "11 pages, 1 figure", "summary": "Large Language Models (LLMs) still struggle with the structured reasoning and\ntool-assisted computation needed for problem solving in cybersecurity\napplications. In this work, we introduce \"random-crypto\", a cryptographic\nCapture-the-Flag (CTF) challenge generator framework that we use to fine-tune a\ntool-augmented Llama-3.1-8B with Guided Reinforcement Prompt Optimisation\n(GRPO), allowing the agent to iteratively write and execute Python inside an\nisolated REPL. GRPO yields a +53% absolute jump in Pass@8 on unseen\n\"random-crypto\" tasks (0.35 -> 0.88) and raises Majority@8 to 0.41. The\nfine-tuned agent also generalizes to an external dataset. On a subset of\npicoCTF cryptography problems, it improves Pass@8 by +13 pp. Ablations show the\ngains stem from more reliable tool invocation and code synthesis, rather than\nsuperficial prompt adaptation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201crandom-crypto\u201d\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7f51\u7edc\u5b89\u5168\u95ee\u9898\u4e2d\u7684\u5de5\u5177\u8f85\u52a9\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7GRPO\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u52a0\u5bc6CTF\u6311\u6218\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7f51\u7edc\u5b89\u5168\u5e94\u7528\u4e2d\u7684\u7ed3\u6784\u5316\u63a8\u7406\u548c\u5de5\u5177\u8f85\u52a9\u8ba1\u7b97\u80fd\u529b\u4ecd\u7136\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u201crandom-crypto\u201d\u6846\u67b6\uff0c\u7ed3\u5408GRPO\u65b9\u6cd5\uff0c\u5bf9Llama-3.1-8B\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u5176\u80fd\u5728\u9694\u79bb\u73af\u5883\u4e2d\u8fed\u4ee3\u7f16\u5199\u548c\u6267\u884cPython\u4ee3\u7801\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u201crandom-crypto\u201d\u4efb\u52a1\u4e2dPass@8\u63d0\u5347\u4e8653%\uff0c\u5728picoCTF\u52a0\u5bc6\u95ee\u9898\u4e0aPass@8\u63d0\u5347\u4e8613\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "GRPO\u65b9\u6cd5\u901a\u8fc7\u63d0\u5347\u5de5\u5177\u8c03\u7528\u548c\u4ee3\u7801\u5408\u6210\u7684\u53ef\u9760\u6027\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u52a0\u5bc6CTF\u6311\u6218\u4e2d\u7684\u8868\u73b0\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u7f51\u7edc\u5b89\u5168, \u52a0\u5bc6CTF, \u5de5\u5177\u8f85\u52a9\u63a8\u7406, GRPO"}}
{"id": "2506.02553", "pdf": "https://arxiv.org/pdf/2506.02553", "abs": "https://arxiv.org/abs/2506.02553", "authors": ["Shenghua He", "Tian Xia", "Xuan Zhou", "Hui Wei"], "title": "Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We study a common challenge in reinforcement learning for large language\nmodels (LLMs): the Zero-Reward Assumption, where non-terminal actions (i.e.,\nintermediate token generations) receive zero task-specific immediate reward,\nwhile only the final token receives a reward for the entire response. This\nassumption arises frequently in practice, as precise token-level rewards are\noften difficult or infeasible to obtain in LLM applications. In this work, we\nprovide a unifying theoretical perspective. We introduce the Trajectory Policy\nGradient Theorem, which shows that the policy gradient based on true, unknown\ntoken-level rewards can be unbiasedly estimated using only a response-level\nreward model, regardless of whether the Zero-Reward Assumption holds or not,\nfor algorithms in the REINFORCE and Actor-Critic families. This result reveals\nthat widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess\nthe capacity to model token-level reward signals, offering a theoretical\njustification for response-level reward approaches. Our findings pave the way\nfor more practical, efficient LLM fine-tuning, allowing developers to treat\ntraining algorithms as black boxes and focus on improving the response-level\nreward model with auxiliary sub-models. We also offer a detailed analysis of\npopular RL and non-RL methods, comparing their theoretical foundations and\npractical advantages across common LLM tasks. Finally, we propose a new\nalgorithm: Token-Reinforced Policy Optimization (TRePO), a theoretically\ngrounded method that is simpler than PPO, matches GRPO in memory efficiency,\nand holds promise for broad applicability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Trajectory Policy Gradient Theorem\uff0c\u8bc1\u660e\u4e86\u5728Zero-Reward Assumption\u4e0b\uff0c\u57fa\u4e8e\u54cd\u5e94\u7ea7\u5956\u52b1\u6a21\u578b\u53ef\u4ee5\u65e0\u504f\u4f30\u8ba1\u4ee4\u724c\u7ea7\u5956\u52b1\uff0c\u4e3a\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u7406\u8bba\u652f\u6301\uff0c\u5e76\u63d0\u51fa\u66f4\u9ad8\u6548\u7684TRePO\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684Zero-Reward Assumption\u95ee\u9898\uff0c\u5373\u975e\u7ec8\u6b62\u52a8\u4f5c\uff08\u4e2d\u95f4\u4ee4\u724c\u751f\u6210\uff09\u65e0\u5373\u65f6\u5956\u52b1\uff0c\u4ec5\u6700\u7ec8\u4ee4\u724c\u83b7\u5f97\u5956\u52b1\u3002\u5b9e\u8df5\u4e2d\u4ee4\u724c\u7ea7\u5956\u52b1\u96be\u4ee5\u83b7\u53d6\uff0c\u4e9f\u9700\u7406\u8bba\u652f\u6301\u548c\u65b9\u6cd5\u6539\u8fdb\u3002", "method": "\u5f15\u5165Trajectory Policy Gradient Theorem\uff0c\u8bc1\u660e\u54cd\u5e94\u7ea7\u5956\u52b1\u6a21\u578b\u53ef\u65e0\u504f\u4f30\u8ba1\u4ee4\u724c\u7ea7\u5956\u52b1\uff1b\u63d0\u51faToken-Reinforced Policy Optimization (TRePO)\u7b97\u6cd5\uff0c\u7b80\u5316PPO\u5e76\u63d0\u5347\u6548\u7387\u3002", "result": "\u7406\u8bba\u8868\u660ePPO\u3001GRPO\u7b49\u65b9\u6cd5\u5177\u5907\u5efa\u6a21\u4ee4\u724c\u7ea7\u5956\u52b1\u7684\u80fd\u529b\uff0c\u4e3a\u54cd\u5e94\u7ea7\u5956\u52b1\u65b9\u6cd5\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\uff1bTRePO\u5728\u5185\u5b58\u6548\u7387\u548c\u5b9e\u7528\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u54cd\u5e94\u7ea7\u5956\u52b1\u6a21\u578b\u53ef\u7528\u4e8e\u9ad8\u6548LLM\u5fae\u8c03\uff0cTRePO\u4e3a\u7b80\u5316\u4e14\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86LLM\u5f3a\u5316\u5b66\u4e60\u7684\u5b9e\u8df5\u5e94\u7528\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5f3a\u5316\u5b66\u4e60, Zero-Reward Assumption, \u7b56\u7565\u68af\u5ea6\u5b9a\u7406, TRePO"}}
{"id": "2506.02872", "pdf": "https://arxiv.org/pdf/2506.02872", "abs": "https://arxiv.org/abs/2506.02872", "authors": ["Ludovic Moncla", "H\u00e9di Zeghidi"], "title": "Token and Span Classification for Entity Recognition in French Historical Encyclopedias", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Named Entity Recognition (NER) in historical texts presents unique challenges\ndue to non-standardized language, archaic orthography, and nested or\noverlapping entities. This study benchmarks a diverse set of NER approaches,\nranging from classical Conditional Random Fields (CRFs) and spaCy-based models\nto transformer-based architectures such as CamemBERT and sequence-labeling\nmodels like Flair. Experiments are conducted on the GeoEDdA dataset, a richly\nannotated corpus derived from 18th-century French encyclopedias. We propose\nframing NER as both token-level and span-level classification to accommodate\ncomplex nested entity structures typical of historical documents. Additionally,\nwe evaluate the emerging potential of few-shot prompting with generative\nlanguage models for low-resource scenarios. Our results demonstrate that while\ntransformer-based models achieve state-of-the-art performance, especially on\nnested entities, generative models offer promising alternatives when labeled\ndata are scarce. The study highlights ongoing challenges in historical NER and\nsuggests avenues for hybrid approaches combining symbolic and neural methods to\nbetter capture the intricacies of early modern French text.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5386\u53f2\u6587\u672c\u4e2d\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\uff0c\u6bd4\u8f83\u4e86\u591a\u79cd\u65b9\u6cd5\uff0c\u5305\u62ec\u4f20\u7edf\u6280\u672f\u548cTransformer\u6a21\u578b\uff0c\u5e76\u572818\u4e16\u7eaa\u6cd5\u8bed\u767e\u79d1\u5168\u4e66\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u63d0\u51fa\u4e86\u9488\u5bf9\u5d4c\u5957\u5b9e\u4f53\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u8bc4\u4f30\u4e86\u751f\u6210\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u6f5c\u529b\u3002\u7ed3\u679c\u5c55\u793a\u4e86Transformer\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u7ed3\u5408\u7b26\u53f7\u4e0e\u795e\u7ecf\u65b9\u6cd5\u7684\u6df7\u5408\u7b56\u7565\u7684\u4ef7\u503c\u3002", "motivation": "\u5386\u53f2\u6587\u672c\u4e2d\u7684NER\u56e0\u8bed\u8a00\u975e\u6807\u51c6\u5316\u3001\u62fc\u5199\u53e4\u8001\u53ca\u5b9e\u4f53\u5d4c\u5957\u7b49\u7279\u6027\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u4e0d\u540c\u65b9\u6cd5\u7684\u8868\u73b0\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u5411\u3002", "method": "\u5728GeoEDdA\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86CRF\u3001spaCy\u3001CamemBERT\u3001Flair\u7b49\u591a\u79cd\u6a21\u578b\uff0c\u63d0\u51fa\u57fa\u4e8e\u6807\u8bb0\u548c\u8de8\u5ea6\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u63a2\u7d22\u4e86\u751f\u6210\u6a21\u578b\u7684\u5c11\u6837\u672c\u63d0\u793a\u80fd\u529b\u3002", "result": "Transformer\u6a21\u578b\u5728\u5d4c\u5957\u5b9e\u4f53\u4e0a\u8868\u73b0\u6700\u4f18\uff0c\u751f\u6210\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u60c5\u51b5\u4e0b\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u5386\u53f2\u6587\u672cNER\u4ecd\u9700\u89e3\u51b3\u590d\u6742\u6027\u95ee\u9898\uff0c\u672a\u6765\u53ef\u7ed3\u5408\u7b26\u53f7\u4e0e\u795e\u7ecf\u65b9\u6cd5\u7684\u6df7\u5408\u7b56\u7565\u4ee5\u63d0\u9ad8\u6548\u679c\u3002", "keywords": "\u547d\u540d\u5b9e\u4f53\u8bc6\u522b, \u5386\u53f2\u6587\u672c, \u5d4c\u5957\u5b9e\u4f53, \u751f\u6210\u6a21\u578b, \u4f4e\u8d44\u6e90\u5b66\u4e60"}}
{"id": "2506.02049", "pdf": "https://arxiv.org/pdf/2506.02049", "abs": "https://arxiv.org/abs/2506.02049", "authors": ["Beichen Huang", "Ran Cheng", "Kay Chen Tan"], "title": "EvoGit: Decentralized Code Evolution via Git-Based Multi-Agent Collaboration", "categories": ["cs.DC", "cs.AI", "cs.MA", "cs.NE"], "comment": null, "summary": "We introduce EvoGit, a decentralized multi-agent framework for collaborative\nsoftware development driven by autonomous code evolution. EvoGit deploys a\npopulation of independent coding agents, each proposing edits to a shared\ncodebase without centralized coordination, explicit message passing, or shared\nmemory. Instead, all coordination emerges through a Git-based phylogenetic\ngraph that tracks the full version lineage and enables agents to asynchronously\nread from and write to the evolving code repository. This graph-based structure\nsupports fine-grained branching, implicit concurrency, and scalable agent\ninteraction while preserving a consistent historical record. Human involvement\nis minimal but strategic: users define high-level goals, periodically review\nthe graph, and provide lightweight feedback to promote promising directions or\nprune unproductive ones. Experiments demonstrate EvoGit's ability to\nautonomously produce functional and modular software artifacts across two\nreal-world tasks: (1) building a web application from scratch using modern\nframeworks, and (2) constructing a meta-level system that evolves its own\nlanguage-model-guided solver for the bin-packing optimization problem. Our\nresults underscore EvoGit's potential to establish a new paradigm for\ndecentralized, automated, and continual software development. EvoGit is\nopen-sourced at https://github.com/BillHuang2001/evogit.", "AI": {"tldr": "EvoGit\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7Git\u7248\u672c\u63a7\u5236\u7684\u7cfb\u7edf\u5b9e\u73b0\u534f\u4f5c\u5f0f\u8f6f\u4ef6\u5f00\u53d1\u548c\u81ea\u52a8\u5316\u4ee3\u7801\u8fdb\u5316\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u96c6\u4e2d\u5f0f\u534f\u4f5c\u5f00\u53d1\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u81ea\u4e3b\u8fdb\u5316\u4ee3\u7801\uff0c\u51cf\u5c11\u4eba\u4e3a\u5e72\u9884\u3002", "method": "\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u57fa\u4e8eGit\u7684\u7248\u672c\u63a7\u5236\u6811\u5b9e\u73b0\u5f02\u6b65\u534f\u4f5c\uff0c\u65e0\u9700\u96c6\u4e2d\u534f\u8c03\u6216\u5171\u4eab\u5185\u5b58\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEvoGit\u80fd\u591f\u81ea\u4e3b\u5b8c\u6210\u529f\u80fd\u6a21\u5757\u5316\u8f6f\u4ef6\uff0c\u5e76\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "EvoGit\u4e3a\u53bb\u4e2d\u5fc3\u5316\u3001\u81ea\u52a8\u5316\u548c\u6301\u7eed\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "keywords": "EvoGit, \u53bb\u4e2d\u5fc3\u5316, \u591a\u667a\u80fd\u4f53, \u4ee3\u7801\u8fdb\u5316, Git"}}
{"id": "2506.02563", "pdf": "https://arxiv.org/pdf/2506.02563", "abs": "https://arxiv.org/abs/2506.02563", "authors": ["Roie Reshef", "Kfir Yehuda Levy"], "title": "Privacy-Preserving Federated Convex Optimization: Balancing Partial-Participation and Efficiency via Noise Cancellation", "categories": ["cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2407.12396", "summary": "This paper tackles the challenge of achieving Differential Privacy (DP) in\nFederated Learning (FL) under partial-participation, where only a subset of the\nmachines participate in each time-step. While previous work achieved optimal\nperformance in full-participation settings, these methods struggled to extend\nto partial-participation scenarios. Our approach fills this gap by introducing\na novel noise-cancellation mechanism that preserves privacy without sacrificing\nconvergence rates or computational efficiency. We analyze our method within the\nStochastic Convex Optimization (SCO) framework and show that it delivers\noptimal performance for both homogeneous and heterogeneous data distributions.\nThis work expands the applicability of DP in FL, offering an efficient and\npractical solution for privacy-preserving learning in distributed systems with\npartial participation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u5b9e\u73b0\u5dee\u5206\u9690\u79c1\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u90e8\u5206\u53c2\u4e0e\u573a\u666f\u4e0b\u7684\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u90e8\u5206\u53c2\u4e0e\u573a\u666f\u4e0b\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u7684\u6311\u6218\uff0c\u586b\u8865\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u566a\u58f0\u6d88\u9664\u673a\u5236\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4e0d\u727a\u7272\u6536\u655b\u901f\u5ea6\u6216\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728Stochastic Convex Optimization\u6846\u67b6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5728\u5747\u5300\u548c\u975e\u5747\u5300\u6570\u636e\u5206\u5e03\u4e0b\u5747\u8868\u73b0\u51fa\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u5dee\u5206\u9690\u79c1\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u9690\u79c1\u4fdd\u62a4\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5dee\u5206\u9690\u79c1,\u8054\u90a6\u5b66\u4e60,\u90e8\u5206\u53c2\u4e0e,\u566a\u58f0\u6d88\u9664,Stochastic Convex Optimization"}}
{"id": "2506.02878", "pdf": "https://arxiv.org/pdf/2506.02878", "abs": "https://arxiv.org/abs/2506.02878", "authors": ["Jintian Shao", "Yiming Cheng"], "title": "CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\nLarge Language Models on tasks requiring multi-step inference. This success has\nled to widespread claims of emergent reasoning capabilities in these models. In\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\nChain-of-Thought functions as a powerful structural constraint that guides\nLarge Language Models to imitate the form of reasoning. By forcing the\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\ncapacity for sequence prediction and pattern matching, effectively constraining\nits output to sequences that resemble coherent thought processes.\nChain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\nLarge Language Models on tasks requiring multi-step inference. This success has\nled to widespread claims of emergent reasoning capabilities in these models. In\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\nChain-of-Thought functions as a powerful structural constraint that guides\nLarge Language Models to imitate the form of reasoning. By forcing the\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\ncapacity for sequence prediction and pattern matching, effectively constraining\nits output to sequences that resemble coherent thought processes.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63d0\u793a\u5e76\u975e\u5f15\u53d1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u771f\u5b9e\u62bd\u8c61\u63a8\u7406\u80fd\u529b\uff0c\u800c\u662f\u901a\u8fc7\u7ed3\u6784\u6027\u7ea6\u675f\u5f15\u5bfc\u6a21\u578b\u6a21\u4eff\u63a8\u7406\u5f62\u5f0f\u3002", "motivation": "\u63a2\u8ba8\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63d0\u793a\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5b9e\u9645\u4f5c\u7528\uff0c\u6311\u6218\u5176\u666e\u904d\u8ba4\u4e3a\u7684\u80fd\u6fc0\u53d1\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u7684\u89c2\u70b9\u3002", "method": "\u91c7\u7528\u7406\u8bba\u8bba\u8bc1\uff0c\u5206\u6790\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u5982\u4f55\u901a\u8fc7\u7ed3\u6784\u6027\u7ea6\u675f\u548c\u5e8f\u5217\u9884\u6d4b\u80fd\u529b\u6a21\u4eff\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u901a\u8fc7\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u4e2d\u95f4\u6b65\u9aa4\uff0c\u8868\u73b0\u51fa\u7c7b\u4f3c\u63a8\u7406\u7684\u5f62\u5f0f\uff0c\u4f46\u5e76\u672a\u5b9e\u73b0\u771f\u5b9e\u62bd\u8c61\u63a8\u7406\u3002", "conclusion": "\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u662f\u4e00\u79cd\u6709\u6548\u7684\u7ed3\u6784\u6027\u7ea6\u675f\u5de5\u5177\uff0c\u4f46\u5176\u6548\u679c\u6e90\u4e8e\u6a21\u578b\u7684\u5e8f\u5217\u9884\u6d4b\u80fd\u529b\uff0c\u800c\u975e\u771f\u6b63\u7684\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u3002", "keywords": "Chain-of-Thought, Large Language Models, Reasoning, Structural Constraint, Pattern Matching"}}
{"id": "2506.02572", "pdf": "https://arxiv.org/pdf/2506.02572", "abs": "https://arxiv.org/abs/2506.02572", "authors": ["Ping Gong", "Jiawei Yi", "Shengnan Wang", "Juncheng Zhang", "Zewen Jin", "Ouxiang Zhou", "Ruibo Liu", "Guanbin Xu", "Youhui Bai", "Bowen Ye", "Kun Yuan", "Tong Yang", "Gong Zhang", "Renhai Chen", "Feng Wu", "Cheng Li"], "title": "HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference", "categories": ["cs.LG", "cs.AI"], "comment": "ACL 2025 findings", "summary": "Large Language Models (LLMs) have emerged as a pivotal research area, yet the\nattention module remains a critical bottleneck in LLM inference, even with\ntechniques like KVCache to mitigate redundant computations. While various\ntop-$k$ attention mechanisms have been proposed to accelerate LLM inference by\nexploiting the inherent sparsity of attention, they often struggled to strike a\nbalance between efficiency and accuracy. In this paper, we introduce HATA\n(Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates\nlow-overhead learning-to-hash techniques into the Top-$k$ attention process.\nDifferent from the existing top-k attention methods which are devoted to\nseeking an absolute estimation of qk score, typically with a great cost, HATA\nmaps queries and keys into binary hash codes, and acquires the relative qk\nscore order with a quite low cost, which is sufficient for realizing top-k\nattention. Extensive experiments demonstrate that HATA achieves up to\n7.2$\\times$ speedup compared to vanilla full attention while maintaining model\naccuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention\nmethods in both accuracy and efficiency across multiple mainstream LLM models\nand diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA.", "AI": {"tldr": "HATA\u662f\u4e00\u79cd\u65b0\u578b\u7684Top-$k$\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u7ed3\u5408\u4f4e\u5f00\u9500\u7684\u54c8\u5e0c\u5b66\u4e60\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LLM\u63a8\u7406\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684Top-$k$\u6ce8\u610f\u529b\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0cHATA\u65e8\u5728\u901a\u8fc7\u54c8\u5e0c\u6280\u672f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "HATA\u5c06\u67e5\u8be2\u548c\u952e\u6620\u5c04\u4e3a\u4e8c\u8fdb\u5236\u54c8\u5e0c\u7801\uff0c\u4ee5\u4f4e\u6210\u672c\u83b7\u53d6\u76f8\u5bf9qk\u5206\u6570\u987a\u5e8f\uff0c\u5b9e\u73b0Top-$k$\u6ce8\u610f\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHATA\u6bd4\u4f20\u7edf\u5168\u6ce8\u610f\u529b\u5feb7.2\u500d\uff0c\u5e76\u5728\u591a\u4e2a\u4e3b\u6d41LLM\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709Top-$k$\u65b9\u6cd5\u3002", "conclusion": "HATA\u901a\u8fc7\u54c8\u5e0c\u6280\u672f\u6709\u6548\u63d0\u5347\u4e86LLM\u63a8\u7406\u7684\u6548\u7387\u4e0e\u51c6\u786e\u6027\uff0c\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "Large Language Models, Top-$k$ Attention, HATA, Learning-to-Hash, Efficiency"}}
{"id": "2506.02894", "pdf": "https://arxiv.org/pdf/2506.02894", "abs": "https://arxiv.org/abs/2506.02894", "authors": ["Verena Blaschke", "Miriam Winkler", "Constantin F\u00f6rster", "Gabriele Wenger-Glemser", "Barbara Plank"], "title": "A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Although Germany has a diverse landscape of dialects, they are\nunderrepresented in current automatic speech recognition (ASR) research. To\nenable studies of how robust models are towards dialectal variation, we present\nBetthupferl, an evaluation dataset containing four hours of read speech in\nthree dialect groups spoken in Southeast Germany (Franconian, Bavarian,\nAlemannic), and half an hour of Standard German speech. We provide both\ndialectal and Standard German transcriptions, and analyze the linguistic\ndifferences between them. We benchmark several multilingual state-of-the-art\nASR models on speech translation into Standard German, and find differences\nbetween how much the output resembles the dialectal vs. standardized\ntranscriptions. Qualitative error analyses of the best ASR model reveal that it\nsometimes normalizes grammatical differences, but often stays closer to the\ndialectal constructions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u5fb7\u56fd\u4e1c\u5357\u90e8\u4e09\u79cd\u65b9\u8a00\u7684\u8bed\u97f3\u8bc6\u522b\u6570\u636e\u96c6Betthupferl\uff0c\u8bc4\u4f30\u73b0\u6709ASR\u6a21\u578b\u5bf9\u65b9\u8a00\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u8f93\u51fa\u7ed3\u679c\u4ecb\u4e8e\u65b9\u8a00\u4e0e\u6807\u51c6\u5fb7\u8bed\u4e4b\u95f4\u3002", "motivation": "\u5fb7\u56fd\u65b9\u8a00\u5728ASR\u7814\u7a76\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u9700\u8bc4\u4f30\u6a21\u578b\u5bf9\u65b9\u8a00\u7684\u9002\u5e94\u6027\u3002", "method": "\u6784\u5efa\u542b\u4e09\u79cd\u65b9\u8a00\u53ca\u6807\u51c6\u5fb7\u8bed\u76844\u5c0f\u65f6\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u5206\u6790\u65b9\u8a00\u4e0e\u6807\u51c6\u5fb7\u8bed\u7684\u5dee\u5f02\uff0c\u5e76\u6d4b\u8bd5\u591a\u8bed\u8a00ASR\u6a21\u578b\u3002", "result": "\u6a21\u578b\u8f93\u51fa\u4ecb\u4e8e\u65b9\u8a00\u4e0e\u6807\u51c6\u5fb7\u8bed\u4e4b\u95f4\uff0c\u90e8\u5206\u7ed3\u679c\u4fdd\u7559\u65b9\u8a00\u7ed3\u6784\u3002", "conclusion": "\u73b0\u6709ASR\u6a21\u578b\u5bf9\u5fb7\u65b9\u8a00\u5904\u7406\u4ecd\u6709\u4e0d\u8db3\uff0c\u672a\u6765\u9700\u6539\u8fdb\u3002", "keywords": "ASR, \u65b9\u8a00, \u5fb7\u8bed, Betthupferl\u6570\u636e\u96c6"}}
{"id": "2506.02051", "pdf": "https://arxiv.org/pdf/2506.02051", "abs": "https://arxiv.org/abs/2506.02051", "authors": ["Hui Liu", "Shiye Tian", "Xuejun Liu"], "title": "Phenotypic Profile-Informed Generation of Drug-Like Molecules via Dual-Channel Variational Autoencoders", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "comment": "IJCAI2025", "summary": "The de novo generation of drug-like molecules capable of inducing desirable\nphenotypic changes is receiving increasing attention. However, previous methods\npredominantly rely on expression profiles to guide molecule generation, but\noverlook the perturbative effect of the molecules on cellular contexts. To\novercome this limitation, we propose SmilesGEN, a novel generative model based\non variational autoencoder (VAE) architecture to generate molecules with\npotential therapeutic effects. SmilesGEN integrates a pre-trained drug VAE\n(SmilesNet) with an expression profile VAE (ProfileNet), jointly modeling the\ninterplay between drug perturbations and transcriptional responses in a common\nlatent space. Specifically, ProfileNet is imposed to reconstruct pre-treatment\nexpression profiles when eliminating drug-induced perturbations in the latent\nspace, while SmilesNet is informed by desired expression profiles to generate\ndrug-like molecules. Our empirical experiments demonstrate that SmilesGEN\noutperforms current state-of-the-art models in generating molecules with higher\ndegree of validity, uniqueness, novelty, as well as higher Tanimoto similarity\nto known ligands targeting the relevant proteins. Moreover, we evaluate\nSmilesGEN for scaffold-based molecule optimization and generation of\ntherapeutic agents, and confirmed its superior performance in generating\nmolecules with higher similarity to approved drugs. SmilesGEN establishes a\nrobust framework that leverages gene signatures to generate drug-like molecules\nthat hold promising potential to induce desirable cellular phenotypic changes.", "AI": {"tldr": "SmilesGEN\u662f\u4e00\u79cd\u57fa\u4e8eVAE\u67b6\u6784\u7684\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u836f\u7269\u6270\u52a8\u548c\u8f6c\u5f55\u53cd\u5e94\u6765\u751f\u6210\u5177\u6709\u6f5c\u5728\u6cbb\u7597\u6548\u679c\u7684\u5206\u5b50\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8868\u8fbe\u8c31\u6307\u5bfc\u5206\u5b50\u751f\u6210\uff0c\u4f46\u5ffd\u89c6\u4e86\u5206\u5b50\u5bf9\u7ec6\u80de\u73af\u5883\u7684\u6270\u52a8\u6548\u5e94\uff0cSmilesGEN\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "SmilesGEN\u6574\u5408\u4e86\u9884\u8bad\u7ec3\u7684SmilesNet\u548cProfileNet\uff0c\u5171\u540c\u5efa\u6a21\u836f\u7269\u6270\u52a8\u4e0e\u8f6c\u5f55\u53cd\u5e94\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u901a\u8fc7\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u751f\u6210\u5206\u5b50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSmilesGEN\u5728\u5206\u5b50\u6548\u4ef7\u3001\u72ec\u7279\u6027\u3001\u65b0\u9896\u6027\u53ca\u4e0e\u5df2\u77e5\u914d\u4f53\u7684\u76f8\u4f3c\u6027\u7b49\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u6210\u529f\u7528\u4e8e\u836f\u7269\u4f18\u5316\u548c\u751f\u6210\u3002", "conclusion": "SmilesGEN\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u5229\u7528\u57fa\u56e0\u7279\u5f81\u751f\u6210\u6709\u671b\u8bf1\u5bfc\u7406\u60f3\u7ec6\u80de\u8868\u578b\u53d8\u5316\u7684\u836f\u7269\u5206\u5b50\u3002", "keywords": "SmilesGEN, \u751f\u6210\u6a21\u578b, \u53d8\u5206\u81ea\u7f16\u7801\u5668, \u836f\u7269\u53d1\u73b0, \u8868\u8fbe\u8c31"}}
{"id": "2506.02577", "pdf": "https://arxiv.org/pdf/2506.02577", "abs": "https://arxiv.org/abs/2506.02577", "authors": ["Wenyan Yang", "Joni Pajarinen"], "title": "Reachability Weighted Offline Goal-conditioned Resampling", "categories": ["cs.LG"], "comment": null, "summary": "Offline goal-conditioned reinforcement learning (RL) relies on fixed datasets\nwhere many potential goals share the same state and action spaces. However,\nthese potential goals are not explicitly represented in the collected\ntrajectories. To learn a generalizable goal-conditioned policy, it is common to\nsample goals and state-action pairs uniformly using dynamic programming methods\nsuch as Q-learning. Uniform sampling, however, requires an intractably large\ndataset to cover all possible combinations and creates many unreachable\nstate-goal-action pairs that degrade policy performance. Our key insight is\nthat sampling should favor transitions that enable goal achievement. To this\nend, we propose Reachability Weighted Sampling (RWS). RWS uses a reachability\nclassifier trained via positive-unlabeled (PU) learning on goal-conditioned\nstate-action values. The classifier maps these values to a reachability score,\nwhich is then used as a sampling priority. RWS is a plug-and-play module that\nintegrates seamlessly with standard offline RL algorithms. Experiments on six\ncomplex simulated robotic manipulation tasks, including those with a robot arm\nand a dexterous hand, show that RWS significantly improves performance. In one\nnotable case, performance on the HandBlock-Z task improved by nearly 50 percent\nrelative to the baseline. These results indicate the effectiveness of\nreachability-weighted sampling.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e00\u79cd\u65b0\u91c7\u6837\u65b9\u6cd5\u2014\u2014\u53ef\u8fbe\u6027\u52a0\u6743\u91c7\u6837\uff08RWS\uff09\uff0c\u901a\u8fc7\u4f18\u5148\u91c7\u6837\u6709\u52a9\u4e8e\u5b9e\u73b0\u76ee\u6807\u7684\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u6027\u80fd\u3002", "motivation": "\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u901a\u5e38\u5747\u5300\u91c7\u6837\u76ee\u6807\u548c\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u4e14\u4f1a\u751f\u6210\u8bb8\u591a\u65e0\u6548\u7ec4\u5408\uff0c\u5f71\u54cd\u7b56\u7565\u6027\u80fd\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u91c7\u6837\u65b9\u6cd5\u63d0\u5347\u5b66\u4e60\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u53ef\u8fbe\u6027\u52a0\u6743\u91c7\u6837\uff08RWS\uff09\uff0c\u5229\u7528\u6b63\u6837\u672c-\u672a\u6807\u8bb0\uff08PU\uff09\u5b66\u4e60\u8bad\u7ec3\u53ef\u8fbe\u6027\u5206\u7c7b\u5668\uff0c\u4e3a\u5176\u5206\u914d\u91c7\u6837\u4f18\u5148\u7ea7\u3002", "result": "\u5728\u516d\u4e2a\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cRWS\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5176\u4e2dHandBlock-Z\u4efb\u52a1\u7684\u6027\u80fd\u63d0\u5347\u4e86\u8fd150%\u3002", "conclusion": "\u53ef\u8fbe\u6027\u52a0\u6743\u91c7\u6837\u6709\u6548\u63d0\u5347\u4e86\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\u3002", "keywords": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60,\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60,\u53ef\u8fbe\u6027\u52a0\u6743\u91c7\u6837,PU\u5b66\u4e60"}}
{"id": "2506.02899", "pdf": "https://arxiv.org/pdf/2506.02899", "abs": "https://arxiv.org/abs/2506.02899", "authors": ["Yusuke Sakai", "Takumi Goto", "Taro Watanabe"], "title": "IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "We propose IMPARA-GED, a novel reference-free automatic grammatical error\ncorrection (GEC) evaluation method with grammatical error detection (GED)\ncapabilities. We focus on the quality estimator of IMPARA, an existing\nautomatic GEC evaluation method, and construct that of IMPARA-GED using a\npre-trained language model with enhanced GED capabilities. Experimental results\non SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods,\ndemonstrate that IMPARA-GED achieves the highest correlation with human\nsentence-level evaluations.", "AI": {"tldr": "IMPARA-GED \u662f\u4e00\u79cd\u65b0\u578b\u7684\u65e0\u53c2\u8003\u81ea\u52a8\u8bed\u6cd5\u9519\u8bef\u6821\u6b63\uff08GEC\uff09\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5177\u5907\u8bed\u6cd5\u9519\u8bef\u68c0\u6d4b\uff08GED\uff09\u80fd\u529b\u3002\u901a\u8fc7\u589e\u5f3aGED\u80fd\u529b\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u5176\u8d28\u91cf\u4f30\u8ba1\u5668\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728SEEDA\u6570\u636e\u96c6\u4e0a\u4e0e\u4eba\u7c7b\u8bc4\u4ef7\u7684\u76f8\u5173\u6027\u6700\u9ad8\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u81ea\u52a8GEC\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u6cd5\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u7684\u95ee\u9898\uff0c\u63d0\u5347\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\u3002", "method": "\u57fa\u4e8eIMPARA\u7684\u8d28\u91cf\u4f30\u8ba1\u5668\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u8bed\u6cd5\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\uff0c\u6784\u5efaIMPARA-GED\u3002", "result": "\u5728SEEDA\u6570\u636e\u96c6\u4e0a\uff0cIMPARA-GED\u4e0e\u4eba\u7c7b\u53e5\u5b50\u7ea7\u8bc4\u4ef7\u7684\u76f8\u5173\u6027\u6700\u9ad8\u3002", "conclusion": "IMPARA-GED\u662f\u4e00\u79cd\u6709\u6548\u7684\u65e0\u53c2\u8003\u81ea\u52a8GEC\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5177\u6709\u5f3a\u5927\u7684\u8bed\u6cd5\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u3002", "keywords": "\u8bed\u6cd5\u9519\u8bef\u6821\u6b63, GED, \u65e0\u53c2\u8003\u8bc4\u4f30, \u8bed\u8a00\u6a21\u578b, SEEDA"}}
{"id": "2506.02052", "pdf": "https://arxiv.org/pdf/2506.02052", "abs": "https://arxiv.org/abs/2506.02052", "authors": ["Shuo Yan", "Yuliang Yan", "Bin Ma", "Chenao Li", "Haochun Tang", "Jiahua Lu", "Minhua Lin", "Yuyuan Feng", "Hui Xiong", "Enyan Dai"], "title": "Protap: A Benchmark for Protein Modeling on Realistic Downstream Applications", "categories": ["q-bio.BM", "cs.AI", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Recently, extensive deep learning architectures and pretraining strategies\nhave been explored to support downstream protein applications. Additionally,\ndomain-specific models incorporating biological knowledge have been developed\nto enhance performance in specialized tasks. In this work, we introduce\n$\\textbf{Protap}$, a comprehensive benchmark that systematically compares\nbackbone architectures, pretraining strategies, and domain-specific models\nacross diverse and realistic downstream protein applications. Specifically,\nProtap covers five applications: three general tasks and two novel specialized\ntasks, i.e., enzyme-catalyzed protein cleavage site prediction and targeted\nprotein degradation, which are industrially relevant yet missing from existing\nbenchmarks. For each application, Protap compares various domain-specific\nmodels and general architectures under multiple pretraining settings. Our\nempirical studies imply that: (i) Though large-scale pretraining encoders\nachieve great results, they often underperform supervised encoders trained on\nsmall downstream training sets. (ii) Incorporating structural information\nduring downstream fine-tuning can match or even outperform protein language\nmodels pretrained on large-scale sequence corpora. (iii) Domain-specific\nbiological priors can enhance performance on specialized downstream tasks. Code\nand datasets are publicly available at\nhttps://github.com/Trust-App-AI-Lab/protap.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Protap\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u86cb\u767d\u8d28\u5e94\u7528\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u3001\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff0c\u5e76\u63ed\u793a\u4e86\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u8bad\u7ec3\u7684\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5bf9\u86cb\u767d\u8d28\u5e94\u7528\u4e2d\u4e0d\u540c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u548c\u9886\u57df\u7279\u5b9a\u6a21\u578b\u7684\u7cfb\u7edf\u6bd4\u8f83\uff0c\u5c24\u5176\u662f\u5728\u5de5\u4e1a\u76f8\u5173\u7684\u65b0\u578b\u4efb\u52a1\u4e0a\u3002", "method": "\u91c7\u7528Protap\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d6\u4e94\u79cd\u86cb\u767d\u8d28\u5e94\u7528\uff08\u5305\u62ec\u4e24\u4e2a\u65b0\u578b\u4efb\u52a1\uff09\uff0c\u6bd4\u8f83\u4e0d\u540c\u67b6\u6784\u3001\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u9886\u57df\u7279\u5b9a\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u5728\u5c0f\u578b\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u5982\u76d1\u7763\u8bad\u7ec3\u7f16\u7801\u5668\uff1b\u7ed3\u6784\u4fe1\u606f\u5728\u4e0b\u6e38\u5fae\u8c03\u4e2d\u8868\u73b0\u4f18\u5f02\uff1b\u9886\u57df\u7279\u5b9a\u5148\u9a8c\u80fd\u63d0\u5347\u4e13\u4e1a\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "Protap\u4e3a\u86cb\u767d\u8d28\u5e94\u7528\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u4f18\u52a3\u52bf\uff0c\u5e76\u5f00\u6e90\u4e86\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002", "keywords": "\u86cb\u767d\u8d28\u5e94\u7528, \u6df1\u5ea6\u5b66\u4e60, \u9884\u8bad\u7ec3, \u9886\u57df\u7279\u5b9a\u6a21\u578b, Protap\u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2506.02599", "pdf": "https://arxiv.org/pdf/2506.02599", "abs": "https://arxiv.org/abs/2506.02599", "authors": ["Niklas Ro\u00dfberg", "Marion Neumeier", "Sinan Hasirlioglu", "Mohamed Essayed Bouzouraa", "Michael Botsch"], "title": "Assessing the Completeness of Traffic Scenario Categories for Automated Highway Driving Functions via Cluster-based Analysis", "categories": ["cs.LG"], "comment": null, "summary": "The ability to operate safely in increasingly complex traffic scenarios is a\nfundamental requirement for Automated Driving Systems (ADS). Ensuring the safe\nrelease of ADS functions necessitates a precise understanding of the occurring\ntraffic scenarios. To support this objective, this work introduces a pipeline\nfor traffic scenario clustering and the analysis of scenario category\ncompleteness. The Clustering Vector Quantized - Variational Autoencoder\n(CVQ-VAE) is employed for the clustering of highway traffic scenarios and\nutilized to create various catalogs with differing numbers of traffic scenario\ncategories. Subsequently, the impact of the number of categories on the\ncompleteness considerations of the traffic scenario categories is analyzed. The\nresults show an outperforming clustering performance compared to previous work.\nThe trade-off between cluster quality and the amount of required data to\nmaintain completeness is discussed based on the publicly available highD\ndataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9ad8\u901f\u516c\u8def\u4ea4\u901a\u573a\u666f\u805a\u7c7b\u548c\u7c7b\u522b\u5b8c\u6574\u6027\u5206\u6790\u7684\u6d41\u7a0b\uff0c\u91c7\u7528CVQ-VAE\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\u5176\u805a\u7c7b\u6027\u80fd\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\uff0c\u5e76\u63a2\u8ba8\u4e86\u805a\u7c7b\u8d28\u91cf\u4e0e\u6570\u636e\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff08ADS\uff09\u5728\u590d\u6742\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u5b89\u5168\u6027\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u7cbe\u786e\u7406\u89e3\u4ea4\u901a\u573a\u666f\u662f\u786e\u4fdd\u5b89\u5168\u53d1\u5e03ADS\u529f\u80fd\u7684\u5173\u952e\u3002", "method": "\u5229\u7528\u805a\u7c7b\u5411\u91cf\u91cf\u5316-\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CVQ-VAE\uff09\u5bf9\u9ad8\u901f\u516c\u8def\u4ea4\u901a\u573a\u666f\u8fdb\u884c\u805a\u7c7b\uff0c\u5e76\u751f\u6210\u4e0d\u540c\u6570\u91cf\u7684\u4ea4\u901a\u573a\u666f\u7c7b\u522b\u76ee\u5f55\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u805a\u7c7b\u6027\u80fd\u4e0a\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\uff0c\u540c\u65f6\u5206\u6790\u4e86\u7c7b\u522b\u6570\u91cf\u5bf9\u5b8c\u6574\u6027\u8003\u91cf\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u516c\u5f00\u7684highD\u6570\u636e\u96c6\uff0c\u63a2\u8ba8\u4e86\u805a\u7c7b\u8d28\u91cf\u4e0e\u6240\u9700\u6570\u636e\u91cf\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e3aADS\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "keywords": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf, \u4ea4\u901a\u573a\u666f\u805a\u7c7b, CVQ-VAE, highD\u6570\u636e\u96c6, \u5b8c\u6574\u6027\u5206\u6790"}}
{"id": "2506.02911", "pdf": "https://arxiv.org/pdf/2506.02911", "abs": "https://arxiv.org/abs/2506.02911", "authors": ["Yin Fang", "Qiao Jin", "Guangzhi Xiong", "Bowen Jin", "Xianrui Zhong", "Siru Ouyang", "Aidong Zhang", "Jiawei Han", "Zhiyong Lu"], "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "cs.LG"], "comment": "28 pages; 16 tables; 7 figures; Code:\n  https://github.com/ncbi-nlp/cell-o1", "summary": "Cell type annotation is a key task in analyzing the heterogeneity of\nsingle-cell RNA sequencing data. Although recent foundation models automate\nthis process, they typically annotate cells independently, without considering\nbatch-level cellular context or providing explanatory reasoning. In contrast,\nhuman experts often annotate distinct cell types for different cell clusters\nbased on their domain knowledge. To mimic this workflow, we introduce the\nCellPuzzles task, where the objective is to assign unique cell types to a batch\nof cells. This benchmark spans diverse tissues, diseases, and donor conditions,\nand requires reasoning across the batch-level cellular context to ensure label\nuniqueness. We find that off-the-shelf large language models (LLMs) struggle on\nCellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%\nbatch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained\nvia supervised fine-tuning on distilled reasoning traces, followed by\nreinforcement learning with batch-level rewards. Cell-o1 achieves\nstate-of-the-art performance, outperforming o1 by over 73% and generalizing\nwell across contexts. Further analysis of training dynamics and reasoning\nbehaviors provides insights into batch-level annotation performance and\nemergent expert-like reasoning. Code and data are available at\nhttps://github.com/ncbi-nlp/cell-o1.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u5206\u6790\u4efb\u52a1CellPuzzles\uff0c\u4e13\u6ce8\u4e8e\u6279\u7ea7\u7ec6\u80de\u7c7b\u578b\u6807\u6ce8\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u6a21\u578bCell-o1\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u7840\u6a21\u578b\u5728\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u6807\u6ce8\u4e2d\u7f3a\u4e4f\u6279\u7ea7\u4e0a\u4e0b\u6587\u548c\u89e3\u91ca\u6027\uff0c\u800c\u4eba\u7c7b\u4e13\u5bb6\u4f9d\u8d56\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u6807\u6ce8\u3002", "method": "\u5f15\u5165CellPuzzles\u4efb\u52a1\uff0c\u5f00\u53d1Cell-o1\u6a21\u578b\uff087B LLM\uff09\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "Cell-o1\u5728\u6279\u7ea7\u51c6\u786e\u7387\u4e0a\u8d85\u8d8a\u57fa\u7ebf\uff08OpenAI o1\uff0973%\uff0c\u8868\u73b0\u51fa\u8272\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "conclusion": "Cell-o1\u901a\u8fc7\u6279\u7ea7\u5956\u52b1\u548c\u63a8\u7406\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u7c7b\u4f3c\u4e13\u5bb6\u7684\u8868\u73b0\uff0c\u4e3a\u5355\u7ec6\u80de\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u3001\u7ec6\u80de\u7c7b\u578b\u6807\u6ce8\u3001\u5927\u8bed\u8a00\u6a21\u578b\u3001CellPuzzles\u3001Cell-o1\u3002"}}
{"id": "2506.02612", "pdf": "https://arxiv.org/pdf/2506.02612", "abs": "https://arxiv.org/abs/2506.02612", "authors": ["Jan Robine", "Marc H\u00f6ftmann", "Stefan Harmeling"], "title": "Simple, Good, Fast: Self-Supervised World Models Free of Baggage", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Published as a conference paper at ICLR 2025. Code is available at\n  https://github.com/jrobine/sgf", "summary": "What are the essential components of world models? How far do we get with\nworld models that are not employing RNNs, transformers, discrete\nrepresentations, and image reconstructions? This paper introduces SGF, a\nSimple, Good, and Fast world model that uses self-supervised representation\nlearning, captures short-time dependencies through frame and action stacking,\nand enhances robustness against model errors through data augmentation. We\nextensively discuss SGF's connections to established world models, evaluate the\nbuilding blocks in ablation studies, and demonstrate good performance through\nquantitative comparisons on the Atari 100k benchmark.", "AI": {"tldr": "SGF\u662f\u4e00\u79cd\u7b80\u5355\u3001\u9ad8\u6548\u7684\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6355\u6349\u77ed\u671f\u4f9d\u8d56\u5173\u7cfb\uff0c\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5e76\u5728Atari 100k\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u6784\u5efa\u4e0d\u4f9d\u8d56\u4e8eRNN\u3001Transformer\u3001\u79bb\u6563\u8868\u793a\u548c\u56fe\u50cf\u91cd\u5efa\u7684\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u66f4\u7b80\u5355\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u3001\u5e27\u548c\u52a8\u4f5c\u5806\u53e0\u6355\u6349\u77ed\u671f\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u63d0\u5347\u9c81\u68d2\u6027\u3002", "result": "\u5728Atari 100k\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u6a21\u578b\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002", "conclusion": "SGF\u5c55\u793a\u4e86\u5728\u4e0d\u4f9d\u8d56\u590d\u6742\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u80fd\u6784\u5efa\u9ad8\u6548\u7684\u4e16\u754c\u6a21\u578b\u3002", "keywords": "\u4e16\u754c\u6a21\u578b\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u3001Atari 100k\u3001\u6570\u636e\u589e\u5f3a\u3001\u77ed\u671f\u4f9d\u8d56"}}
{"id": "2506.02921", "pdf": "https://arxiv.org/pdf/2506.02921", "abs": "https://arxiv.org/abs/2506.02921", "authors": ["Yijun Yang", "Zeyu Huang", "Wenhao Zhu", "Zihan Qiu", "Fei Yuan", "Jeff Z. Pan", "Ivan Titov"], "title": "A Controllable Examination for Long-Context Language Models", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Existing frameworks for evaluating long-context language models (LCLM) can be\nbroadly categorized into real-world and synthetic tasks. Despite their utility,\nboth approaches are accompanied by certain intrinsic limitations. Real-world\ntasks are too complex to interpret or characterize and are susceptible to data\ncontamination. In contrast, synthetic tasks often adopt the\nneedle-in-the-haystack (NIAH) format, wherein a lack of coherence between the\n\"needle\" and the \"haystack\" compromises their validity as proxies for realistic\napplications. In response to these challenges, we posit that an ideal\nlong-context evaluation framework should be characterized by three essential\nfeatures: $\\textit{seamless context}$, $\\textit{controllable setting}$, and\n$\\textit{sound evaluation}$. This study introduces $\\textbf{LongBioBench}$, a\nnovel benchmark that utilizes artificially generated biographies as a\ncontrolled environment for assessing LCLMs across dimensions of\n$\\textit{understanding}$, $\\textit{reasoning}$, and $\\textit{trustworthiness}$.\nOur experimental evaluation, which includes $\\textbf{18}$ LCLMs in total,\ndemonstrates that most models still exhibit deficiencies in semantic\nunderstanding and elementary reasoning over retrieved results and are less\ntrustworthy as context length increases. Our further analysis indicates some\ndesign choices employed by existing synthetic benchmarks, such as contextual\nnon-coherence, numerical needles, and the absence of distractors, rendering\nthem vulnerable to test the model long-context capabilities. Moreover, we also\nreveal that long-context continual pretraining primarily adjusts RoPE embedding\nto accommodate extended context lengths. To sum up, compared to previous\nsynthetic benchmarks, LongBioBench achieves a better trade-off between\nmirroring authentic language tasks and maintaining controllability, and is\nhighly interpretable and configurable.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\uff08LCLM\uff09\u8bc4\u4f30\u6846\u67b6LongBioBench\uff0c\u901a\u8fc7\u4eba\u5de5\u751f\u6210\u4f20\u8bb0\u6765\u6d4b\u8bd5\u6a21\u578b\u7684\u7406\u89e3\u3001\u63a8\u7406\u548c\u53ef\u4fe1\u5ea6\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff08\u73b0\u5b9e\u4efb\u52a1\u548c\u5408\u6210\u4efb\u52a1\uff09\u5b58\u5728\u5c40\u9650\u6027\uff1a\u73b0\u5b9e\u4efb\u52a1\u8fc7\u4e8e\u590d\u6742\u4e14\u6613\u53d7\u6570\u636e\u6c61\u67d3\uff0c\u5408\u6210\u4efb\u52a1\u7f3a\u4e4f\u771f\u5b9e\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u517c\u5177\u53ef\u63a7\u6027\u548c\u771f\u5b9e\u6027\u7684\u65b0\u6846\u67b6\u3002", "method": "\u63d0\u51faLongBioBench\u57fa\u51c6\uff0c\u5229\u7528\u4eba\u5de5\u751f\u6210\u7684\u4f20\u8bb0\u4f5c\u4e3a\u6d4b\u8bd5\u73af\u5883\uff0c\u8bc4\u4f30\u6a21\u578b\u7684\u7406\u89e3\u3001\u63a8\u7406\u548c\u53ef\u4fe1\u5ea6\u3002\u5b9e\u9a8c\u6db5\u76d618\u4e2aLCLM\u3002", "result": "\u5927\u591a\u6570\u6a21\u578b\u5728\u8bed\u4e49\u7406\u89e3\u548c\u57fa\u7840\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u4e14\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\uff0c\u53ef\u4fe1\u5ea6\u4e0b\u964d\u3002\u540c\u65f6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u5408\u6210\u57fa\u51c6\u8bbe\u8ba1\u4e2d\u7684\u6f0f\u6d1e\u3002", "conclusion": "LongBioBench\u5728\u771f\u5b9e\u6027\u548c\u53ef\u63a7\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u662f\u4e00\u79cd\u9ad8\u5ea6\u53ef\u89e3\u91ca\u548c\u53ef\u914d\u7f6e\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "keywords": "\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b,\u8bc4\u4f30\u6846\u67b6,LongBioBench,\u8bed\u4e49\u7406\u89e3,\u53ef\u4fe1\u5ea6"}}
{"id": "2506.02055", "pdf": "https://arxiv.org/pdf/2506.02055", "abs": "https://arxiv.org/abs/2506.02055", "authors": ["Nikola Balic"], "title": "Will Agents Replace Us? Perceptions of Autonomous Multi-Agent AI", "categories": ["cs.CY", "cs.AI", "cs.MA", "I.2.m"], "comment": "15 pages, 5 figures, code available at\n  https://github.com/nibzard/agent-perceptions", "summary": "Autonomous multi-agent AI systems are poised to transform various industries,\nparticularly software development and knowledge work. Understanding current\nperceptions among professionals is crucial for anticipating adoption\nchallenges, ethical considerations, and future workforce development. This\nstudy analyzes responses from 130 participants to a survey on the capabilities,\nimpact, and governance of AI agents. We explore expected timelines for AI\nreplacing programmers, identify perceived barriers to deployment, and examine\nbeliefs about responsibility when agents make critical decisions. Key findings\nreveal three distinct clusters of respondents. While the study explored factors\nassociated with current AI agent deployment, the initial logistic regression\nmodel did not yield statistically significant predictors, suggesting that\ndeployment decisions are complex and may be influenced by factors not fully\ncaptured or that a larger sample is needed. These insights highlight the need\nfor organizations to address compliance concerns (a commonly cited barrier) and\nestablish clear governance frameworks as they integrate autonomous agents into\ntheir workflows.", "AI": {"tldr": "\u7814\u7a76\u8c03\u67e5\u4e86130\u540d\u4e13\u4e1a\u4eba\u58eb\u5bf9AI\u4ee3\u7406\u80fd\u529b\u3001\u5f71\u54cd\u548c\u6cbb\u7406\u7684\u770b\u6cd5\uff0c\u53d1\u73b0\u90e8\u7f72\u51b3\u7b56\u590d\u6742\u4e14\u65e0\u663e\u8457\u9884\u6d4b\u56e0\u7d20\uff0c\u9700\u89e3\u51b3\u5408\u89c4\u95ee\u9898\u5e76\u5efa\u7acb\u6cbb\u7406\u6846\u67b6\u3002", "motivation": "\u4e86\u89e3\u4e13\u4e1a\u4eba\u58eb\u5bf9\u591a\u4ee3\u7406AI\u7cfb\u7edf\u7684\u770b\u6cd5\uff0c\u4ee5\u9884\u5224\u91c7\u7528\u6311\u6218\u3001\u4f26\u7406\u95ee\u9898\u548c\u672a\u6765\u52b3\u52a8\u529b\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u5206\u6790130\u540d\u53d7\u8bbf\u8005\u5bf9AI\u4ee3\u7406\u7684\u9884\u671f\u65f6\u95f4\u8868\u3001\u90e8\u7f72\u969c\u788d\u548c\u8d23\u4efb\u5f52\u5c5e\u7684\u770b\u6cd5\uff0c\u5e76\u5c1d\u8bd5\u903b\u8f91\u56de\u5f52\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u4e09\u7c7b\u53d7\u8bbf\u8005\u96c6\u7fa4\uff0c\u903b\u8f91\u56de\u5f52\u6a21\u578b\u65e0\u663e\u8457\u9884\u6d4b\u56e0\u7d20\uff0c\u90e8\u7f72\u51b3\u7b56\u590d\u6742\uff0c\u53ef\u80fd\u9700\u66f4\u5927\u6837\u672c\u6216\u66f4\u591a\u56e0\u7d20\u3002", "conclusion": "\u7ec4\u7ec7\u5728\u6574\u5408AI\u4ee3\u7406\u65f6\u5e94\u89e3\u51b3\u5408\u89c4\u95ee\u9898\u5e76\u5efa\u7acb\u6e05\u6670\u6cbb\u7406\u6846\u67b6\u3002", "keywords": "AI\u4ee3\u7406, \u591a\u4ee3\u7406\u7cfb\u7edf, \u8c03\u67e5, \u90e8\u7f72\u969c\u788d, \u6cbb\u7406"}}
{"id": "2506.02616", "pdf": "https://arxiv.org/pdf/2506.02616", "abs": "https://arxiv.org/abs/2506.02616", "authors": ["Qi Liao", "Parijat Bhattacharjee"], "title": "Compositional Learning for Modular Multi-Agent Self-Organizing Networks", "categories": ["cs.LG"], "comment": null, "summary": "Self-organizing networks face challenges from complex parameter\ninterdependencies and conflicting objectives. This study introduces two\ncompositional learning approaches-Compositional Deep Reinforcement Learning\n(CDRL) and Compositional Predictive Decision-Making (CPDM)-and evaluates their\nperformance under training time and safety constraints in multi-agent systems.\nWe propose a modular, two-tier framework with cell-level and cell-pair-level\nagents to manage heterogeneous agent granularities while reducing model\ncomplexity. Numerical simulations reveal a significant reduction in handover\nfailures, along with improved throughput and latency, outperforming\nconventional multi-agent deep reinforcement learning approaches. The approach\nalso demonstrates superior scalability, faster convergence, higher sample\nefficiency, and safer training in large-scale self-organizing networks.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u7ec4\u5408\u5b66\u4e60\u65b9\u6cd5\uff08CDRL\u548cCPDM\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u7ec4\u7ec7\u7f51\u7edc\u4e2d\u590d\u6742\u53c2\u6570\u548c\u51b2\u7a81\u76ee\u6807\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u7ec4\u7ec7\u7f51\u7edc\u4e2d\u53c2\u6570\u590d\u6742\u548c\u76ee\u6807\u51b2\u7a81\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u7ec4\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08CDRL\uff09\u548c\u7ec4\u5408\u9884\u6d4b\u51b3\u7b56\uff08CPDM\uff09\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5305\u542b\u7ec6\u80de\u7ea7\u548c\u7ec6\u80de\u5bf9\u7ea7\u4ee3\u7406\u3002", "result": "\u4eff\u771f\u663e\u793a\u51cf\u5c11\u4e86\u5207\u6362\u5931\u8d25\uff0c\u63d0\u5347\u4e86\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\uff0c\u4f18\u4e8e\u4f20\u7edf\u591a\u4ee3\u7406\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u89c4\u6a21\u5316\u81ea\u7ec4\u7ec7\u7f51\u7edc\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u3002", "keywords": "\u81ea\u7ec4\u7ec7\u7f51\u7edc, \u7ec4\u5408\u5b66\u4e60, \u591a\u4ee3\u7406\u7cfb\u7edf, \u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60, \u9884\u6d4b\u51b3\u7b56"}}
{"id": "2506.02924", "pdf": "https://arxiv.org/pdf/2506.02924", "abs": "https://arxiv.org/abs/2506.02924", "authors": ["Diogo A. P. Nunes", "Eug\u00e9nio Ribeiro"], "title": "INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and Prompt-Based Approaches to Depression Symptom Identification", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.4; J.3; H.3.3"], "comment": "12 pages, 1 figure, 6 tables", "summary": "In this work, we describe our team's approach to eRisk's 2025 Task 1: Search\nfor Symptoms of Depression. Given a set of sentences and the Beck's Depression\nInventory - II (BDI) questionnaire, participants were tasked with submitting up\nto 1,000 sentences per depression symptom in the BDI, sorted by relevance.\nParticipant submissions were evaluated according to standard Information\nRetrieval (IR) metrics, including Average Precision (AP) and R-Precision\n(R-PREC). The provided training data, however, consisted of sentences labeled\nas to whether a given sentence was relevant or not w.r.t. one of BDI's\nsymptoms. Due to this labeling limitation, we framed our development as a\nbinary classification task for each BDI symptom, and evaluated accordingly. To\nthat end, we split the available labeled data into training and validation\nsets, and explored foundation model fine-tuning, sentence similarity, Large\nLanguage Model (LLM) prompting, and ensemble techniques. The validation results\nrevealed that fine-tuning foundation models yielded the best performance,\nparticularly when enhanced with synthetic data to mitigate class imbalance. We\nalso observed that the optimal approach varied by symptom. Based on these\ninsights, we devised five independent test runs, two of which used ensemble\nmethods. These runs achieved the highest scores in the official IR evaluation,\noutperforming submissions from 16 other teams.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63cf\u8ff0\u4e86\u56e2\u961f\u5728eRisk 2025\u4efb\u52a11\u4e2d\u901a\u8fc7\u4e8c\u5143\u5206\u7c7b\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u65b9\u6cd5\u6210\u529f\u641c\u7d22\u6291\u90c1\u75c7\u72b6\u7684\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3Beck\u6291\u90c1\u91cf\u8868(BDI)\u95ee\u5377\u4e2d\u6291\u90c1\u75c7\u72b6\u7684\u53e5\u5b50\u5206\u7c7b\u95ee\u9898\uff0c\u5e76\u4f18\u5316\u4fe1\u606f\u68c0\u7d22\u6548\u679c\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\uff0c\u63a2\u7d22\u4e86\u6a21\u578b\u5fae\u8c03\u3001\u53e5\u5b50\u76f8\u4f3c\u6027\u3001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u793a\u548c\u96c6\u6210\u6280\u672f\u7b49\u65b9\u6cd5\u3002", "result": "\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u5728\u5408\u6210\u6570\u636e\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\u65f6\uff0c\u6700\u7ec8\u6d4b\u8bd5\u8868\u73b0\u4f18\u4e8e\u5176\u4ed616\u4e2a\u56e2\u961f\u3002", "conclusion": "\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u5e76\u7ed3\u5408\u5408\u6210\u6570\u636e\u662f\u63d0\u5347\u6291\u90c1\u75c7\u72b6\u5206\u7c7b\u4efb\u52a1\u6548\u679c\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e14\u65b9\u6cd5\u9700\u9488\u5bf9\u4e0d\u540c\u75c7\u72b6\u8c03\u6574\u3002", "keywords": "\u6291\u90c1\u75c7\uff0c\u4fe1\u606f\u68c0\u7d22\uff0c\u4e8c\u5143\u5206\u7c7b\uff0c\u6a21\u578b\u5fae\u8c03\uff0c\u5408\u6210\u6570\u636e"}}
{"id": "2506.02057", "pdf": "https://arxiv.org/pdf/2506.02057", "abs": "https://arxiv.org/abs/2506.02057", "authors": ["David Sasu", "Kweku Andoh Yamoah", "Benedict Quartey", "Natalie Schluter"], "title": "Enhancing Speech Instruction Understanding and Disambiguation in Robotics via Speech Prosody", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to Interspeech 2025", "summary": "Enabling robots to accurately interpret and execute spoken language\ninstructions is essential for effective human-robot collaboration. Traditional\nmethods rely on speech recognition to transcribe speech into text, often\ndiscarding crucial prosodic cues needed for disambiguating intent. We propose a\nnovel approach that directly leverages speech prosody to infer and resolve\ninstruction intent. Predicted intents are integrated into large language models\nvia in-context learning to disambiguate and select appropriate task plans.\nAdditionally, we present the first ambiguous speech dataset for robotics,\ndesigned to advance research in speech disambiguation. Our method achieves\n95.79% accuracy in detecting referent intents within an utterance and\ndetermines the intended task plan of ambiguous instructions with 71.96%\naccuracy, demonstrating its potential to significantly improve human-robot\ncommunication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8bed\u97f3\u97f5\u5f8b\u76f4\u63a5\u63a8\u65ad\u548c\u89e3\u6790\u6307\u4ee4\u610f\u56fe\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u6307\u4ee4\u6b67\u4e49\uff0c\u5e76\u5efa\u7acb\u9996\u4e2a\u673a\u5668\u4eba\u6b67\u4e49\u8bed\u97f3\u6570\u636e\u96c6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u8bed\u97f3\u8f6c\u6587\u672c\u4e22\u5f03\u4e86\u5173\u952e\u7684\u97f5\u5f8b\u4fe1\u606f\uff0c\u5f71\u54cd\u4e86\u6307\u4ee4\u610f\u56fe\u7684\u89e3\u6790\u51c6\u786e\u6027\u3002", "method": "\u76f4\u63a5\u5229\u7528\u8bed\u97f3\u97f5\u5f8b\u63a8\u65ad\u610f\u56fe\uff0c\u5e76\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u9009\u62e9\u4efb\u52a1\u8ba1\u5212\u3002", "result": "\u65b9\u6cd5\u5728\u68c0\u6d4b\u610f\u56fe\u65f6\u51c6\u786e\u7387\u8fbe95.79%\uff0c\u5728\u89e3\u6790\u6b67\u4e49\u6307\u4ee4\u65f6\u51c6\u786e\u7387\u8fbe71.96%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u4e92\u4e2d\u6307\u4ee4\u7406\u89e3\u7684\u51c6\u786e\u6027\u3002", "keywords": "\u8bed\u97f3\u97f5\u5f8b\u3001\u610f\u56fe\u89e3\u6790\u3001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u673a\u5668\u4eba\u4ea4\u4e92"}}
{"id": "2506.02619", "pdf": "https://arxiv.org/pdf/2506.02619", "abs": "https://arxiv.org/abs/2506.02619", "authors": ["Yanbei Liu", "Chongxu Wang", "Zhitao Xiao", "Lei Geng", "Yanwei Pang", "Xiao Wang"], "title": "HGOT: Self-supervised Heterogeneous Graph Neural Network with Optimal Transport", "categories": ["cs.LG", "cs.AI"], "comment": "The paper has 9 pages of text and 13 pages in total (including\n  acknowledgments, impact statement, references, and appendix), with 6 figures\n  and 2 tables. This paper has been accepted by ICML 2025 conference and this\n  is a final version of the manuscript submitted to the conference", "summary": "Heterogeneous Graph Neural Networks (HGNNs), have demonstrated excellent\ncapabilities in processing heterogeneous information networks. Self-supervised\nlearning on heterogeneous graphs, especially contrastive self-supervised\nstrategy, shows great potential when there are no labels. However, this\napproach requires the use of carefully designed graph augmentation strategies\nand the selection of positive and negative samples. Determining the exact level\nof similarity between sample pairs is non-trivial.To solve this problem, we\npropose a novel self-supervised Heterogeneous graph neural network with Optimal\nTransport (HGOT) method which is designed to facilitate self-supervised\nlearning for heterogeneous graphs without graph augmentation strategies.\nDifferent from traditional contrastive self-supervised learning, HGOT employs\nthe optimal transport mechanism to relieve the laborious sampling process of\npositive and negative samples. Specifically, we design an aggregating view\n(central view) to integrate the semantic information contained in the views\nrepresented by different meta-paths (branch views). Then, we introduce an\noptimal transport plan to identify the transport relationship between the\nsemantics contained in the branch view and the central view. This allows the\noptimal transport plan between graphs to align with the representations,\nforcing the encoder to learn node representations that are more similar to the\ngraph space and of higher quality. Extensive experiments on four real-world\ndatasets demonstrate that our proposed HGOT model can achieve state-of-the-art\nperformance on various downstream tasks. In particular, in the node\nclassification task, HGOT achieves an average of more than 6% improvement in\naccuracy compared with state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86HGOT\u65b9\u6cd5\uff0c\u5229\u7528\u6700\u4f18\u4f20\u8f93\u673a\u5236\u7b80\u5316\u5f02\u6784\u56fe\u7684\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u65e0\u9700\u56fe\u589e\u5f3a\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u5f02\u6784\u56fe\u4e2d\u9700\u8981\u590d\u6742\u7684\u6570\u636e\u589e\u5f3a\u548c\u6837\u672c\u9009\u62e9\uff0cHGOT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "HGOT\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u673a\u5236\u66ff\u4ee3\u6b63\u8d1f\u6837\u672c\u91c7\u6837\uff0c\u8bbe\u8ba1\u4e2d\u5fc3\u89c6\u56fe\u6574\u5408\u5206\u652f\u89c6\u56fe\u8bed\u4e49\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cHGOT\u5728\u8282\u70b9\u5206\u7c7b\u7b49\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u5e73\u5747\u63d0\u53476%\u3002", "conclusion": "HGOT\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u673a\u5236\u6709\u6548\u7b80\u5316\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "keywords": "\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc, \u81ea\u76d1\u7763\u5b66\u4e60, \u6700\u4f18\u4f20\u8f93, \u8282\u70b9\u5206\u7c7b"}}
{"id": "2506.02945", "pdf": "https://arxiv.org/pdf/2506.02945", "abs": "https://arxiv.org/abs/2506.02945", "authors": ["Aishwarya Sahoo", "Jeevana Kruthi Karnuthala", "Tushar Parmanand Budhwani", "Pranchal Agarwal", "Sankaran Vaidyanathan", "Alexa Siu", "Franck Dernoncourt", "Jennifer Healey", "Nedim Lipka", "Ryan Rossi", "Uttaran Bhattacharya", "Branislav Kveton"], "title": "Quantitative LLM Judges", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a'LLM-as-a-judge'\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u56de\u5f52\u6a21\u578b\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bc4\u5206\u4e0e\u4eba\u7c7b\u8bc4\u5206\u5bf9\u9f50\uff0c\u4ee5\u6539\u8fdb\u81ea\u52a8\u8bc4\u4f30\u7684\u6548\u679c\u3002", "motivation": "\u4e3a\u89e3\u51b3LLM\u81ea\u52a8\u8bc4\u4f30\u8bc4\u5206\u4e0e\u4eba\u7c7b\u8bc4\u5206\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u7edf\u8ba1\u6709\u6548\u7684\u5b9a\u91cf\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u56de\u5f52\u6a21\u578b\u8bad\u7ec3\u5b9a\u91cf\u6cd5\u5b98\uff08quantitative judges\uff09\uff0c\u901a\u8fc7\u6587\u672c\u8bc4\u4f30\u548c\u8bc4\u5206\u6539\u8fdb\u539f\u6709\u6cd5\u5b98\u7684\u8bc4\u5206\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b9a\u91cf\u6cd5\u5b98\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u80fd\u6709\u6548\u63d0\u5347\u73b0\u6709\u6cd5\u5b98\u7684\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u8ba1\u7b97\u548c\u7edf\u8ba1\u6548\u7387\u4e0a\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4eba\u7c7b\u53cd\u9988\u6709\u9650\u7684\u573a\u666f\u3002", "keywords": "LLM, \u81ea\u52a8\u8bc4\u4f30, \u5b9a\u91cf\u6cd5\u5b98, \u56de\u5f52\u6a21\u578b, \u4eba\u7c7b\u8bc4\u5206\u5bf9\u9f50"}}
{"id": "2506.02623", "pdf": "https://arxiv.org/pdf/2506.02623", "abs": "https://arxiv.org/abs/2506.02623", "authors": ["Yuyang Zhou", "Ferrante Neri", "Yew-Soon Ong", "Ruibin Bai"], "title": "SiamNAS: Siamese Surrogate Model for Dominance Relation Prediction in Multi-objective Neural Architecture Search", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Genetic and Evolutionary Computation Conference (GECCO' 25)", "summary": "Modern neural architecture search (NAS) is inherently multi-objective,\nbalancing trade-offs such as accuracy, parameter count, and computational cost.\nThis complexity makes NAS computationally expensive and nearly impossible to\nsolve without efficient approximations. To address this, we propose a novel\nsurrogate modelling approach that leverages an ensemble of Siamese network\nblocks to predict dominance relationships between candidate architectures.\nLightweight and easy to train, the surrogate achieves 92% accuracy and replaces\nthe crowding distance calculation in the survivor selection strategy with a\nheuristic rule based on model size. Integrated into a framework termed SiamNAS,\nthis design eliminates costly evaluations during the search process.\nExperiments on NAS-Bench-201 demonstrate the framework's ability to identify\nPareto-optimal solutions with significantly reduced computational costs. The\nproposed SiamNAS identified a final non-dominated set containing the best\narchitecture in NAS-Bench-201 for CIFAR-10 and the second-best for ImageNet, in\nterms of test error rate, within 0.01 GPU days. This proof-of-concept study\nhighlights the potential of the proposed Siamese network surrogate model to\ngeneralise to multi-tasking optimisation, enabling simultaneous optimisation\nacross tasks. Additionally, it offers opportunities to extend the approach for\ngenerating Sets of Pareto Sets (SOS), providing diverse Pareto-optimal\nsolutions for heterogeneous task settings.", "AI": {"tldr": "SiamNAS\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ee3\u7406\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7Siamese\u7f51\u7edc\u5757\u9884\u6d4b\u5019\u9009\u67b6\u6784\u7684\u652f\u914d\u5173\u7cfb\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u5728\u591a\u76ee\u6807\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u641c\u7d22\u4e2d\u9ad8\u6548\u627e\u5230Pareto\u6700\u4f18\u89e3\u3002", "motivation": "\u73b0\u4ee3\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u672c\u8d28\u4e0a\u662f\u591a\u76ee\u6807\u7684\uff0c\u9700\u8981\u5728\u51c6\u786e\u6027\u3001\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\u3002", "method": "\u4f7f\u7528Siamese\u7f51\u7edc\u5757\u96c6\u5408\u7684\u4ee3\u7406\u6a21\u578b\u9884\u6d4b\u67b6\u6784\u95f4\u7684\u652f\u914d\u5173\u7cfb\uff0c\u5e76\u57fa\u4e8e\u6a21\u578b\u5927\u5c0f\u7684\u542f\u53d1\u5f0f\u89c4\u5219\u53d6\u4ee3\u62e5\u6324\u8ddd\u79bb\u8ba1\u7b97\u3002", "result": "\u5728NAS-Bench-201\u4e0a\uff0cSiamNAS\u4ee50.01 GPU\u5929\u7684\u8ba1\u7b97\u6210\u672c\u627e\u5230\u4e86CIFAR-10\u4e0a\u6700\u4f73\u548cImageNet\u4e0a\u6b21\u4f73\u7684\u67b6\u6784\u3002", "conclusion": "Siamese\u7f51\u7edc\u4ee3\u7406\u6a21\u578b\u5728\u591a\u4efb\u52a1\u4f18\u5316\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5e76\u53ef\u6269\u5c55\u4e3a\u751f\u6210Pareto\u96c6\u96c6\u5408\uff08SOS\uff09\u3002", "keywords": "\u795e\u7ecf\u67b6\u6784\u641c\u7d22,\u591a\u76ee\u6807\u4f18\u5316,Siamese\u7f51\u7edc,Pareto\u6700\u4f18,\u8ba1\u7b97\u6548\u7387"}}
{"id": "2506.02951", "pdf": "https://arxiv.org/pdf/2506.02951", "abs": "https://arxiv.org/abs/2506.02951", "authors": ["Boyi Li", "Zhonghan Zhao", "Der-Horng Lee", "Gaoang Wang"], "title": "Adaptive Graph Pruning for Multi-Agent Communication", "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large Language Model (LLM) based multi-agent systems have shown remarkable\nperformance in various tasks, especially when enhanced through collaborative\ncommunication. However, current methods often rely on a fixed number of agents\nand static communication structures, limiting their ability to adapt to varying\ntask complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a\nnovel task-adaptive multi-agent collaboration framework that jointly optimizes\nagent quantity (hard-pruning) and communication topology (soft-pruning).\nSpecifically, our method employs a two-stage training strategy: firstly,\nindependently training soft-pruning networks for different agent quantities to\ndetermine optimal agent-quantity-specific complete graphs and positional masks\nacross specific tasks; and then jointly optimizing hard-pruning and\nsoft-pruning within a maximum complete graph to dynamically configure the\nnumber of agents and their communication topologies per task. Extensive\nexperiments demonstrate that our approach is: (1) High-performing, achieving\nstate-of-the-art results across six benchmarks and consistently generalizes\nacross multiple mainstream LLM architectures, with a increase in performance of\n$2.58\\%\\sim 9.84\\%$; (2) Task-adaptive, dynamically constructing optimized\ncommunication topologies tailored to specific tasks, with an extremely high\nperformance in all three task categories (general reasoning, mathematical\nreasoning, and code generation); (3) Token-economical, having fewer training\nsteps and token consumption at the same time, with a decrease in token\nconsumption of $90\\%+$; and (4) Training-efficient, achieving high performance\nwith very few training steps compared with other methods. The performance will\nsurpass the existing baselines after about ten steps of training under six\nbenchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u81ea\u9002\u5e94\u56fe\u526a\u679d\uff08AGP\uff09\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u667a\u80fd\u4f53\u6570\u91cf\u548c\u901a\u4fe1\u62d3\u6251\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4f9d\u8d56\u56fa\u5b9a\u6570\u91cf\u548c\u9759\u6001\u901a\u4fe1\u7ed3\u6784\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4efb\u52a1\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a1\uff09\u72ec\u7acb\u8bad\u7ec3\u4e0d\u540c\u667a\u80fd\u4f53\u6570\u91cf\u7684\u8f6f\u526a\u679d\u7f51\u7edc\uff1b2\uff09\u8054\u5408\u4f18\u5316\u786c\u526a\u679d\u548c\u8f6f\u526a\u679d\uff0c\u52a8\u6001\u914d\u7f6e\u667a\u80fd\u4f53\u6570\u91cf\u548c\u901a\u4fe1\u62d3\u6251\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u63d0\u53472.58%\u81f39.84%\uff0c\u540c\u65f6\u51cf\u5c1190%\u4ee5\u4e0a\u7684token\u6d88\u8017\uff0c\u5e76\u5728\u5341\u6b65\u8bad\u7ec3\u5185\u8d85\u8d8a\u57fa\u7ebf\u3002", "conclusion": "AGP\u6846\u67b6\u5728\u6027\u80fd\u3001\u4efb\u52a1\u9002\u5e94\u6027\u3001\u7ecf\u6d4e\u6027\u548c\u8bad\u7ec3\u6548\u7387\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "keywords": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf,\u81ea\u9002\u5e94\u56fe\u526a\u679d,\u5927\u8bed\u8a00\u6a21\u578b,\u4efb\u52a1\u9002\u5e94\u6027,\u901a\u4fe1\u62d3\u6251"}}
{"id": "2506.02630", "pdf": "https://arxiv.org/pdf/2506.02630", "abs": "https://arxiv.org/abs/2506.02630", "authors": ["Tom Jacobs", "Advait Gadhikar", "Celia Rubio-Madrigal", "Rebekka Burkholz"], "title": "HAM: A Hyperbolic Step to Regulate Implicit Bias", "categories": ["cs.LG"], "comment": "26 pages, 7 figures", "summary": "Understanding the implicit bias of optimization algorithms has become central\nto explaining the generalization behavior of deep learning models. For\ninstance, the hyperbolic implicit bias induced by the overparameterization $m\n\\odot w$--though effective in promoting sparsity--can result in a small\neffective learning rate, which slows down convergence. To overcome this\nobstacle, we propose HAM (Hyperbolic Aware Minimization), which alternates\nbetween an optimizer step and a new hyperbolic mirror step. We derive the\nRiemannian gradient flow for its combination with gradient descent, leading to\nimproved convergence and a similar beneficial hyperbolic geometry as $m \\odot\nw$ for feature learning. We provide an interpretation of the the algorithm by\nrelating it to natural gradient descent, and an exact characterization of its\nimplicit bias for underdetermined linear regression. HAM's implicit bias\nconsistently boosts performance--even of dense training, as we demonstrate in\nexperiments across diverse tasks, including vision, graph and node\nclassification, and large language model fine-tuning. HAM is especially\neffective in combination with different sparsification methods, improving upon\nthe state of the art. The hyperbolic step requires minimal computational and\nmemory overhead, it succeeds even with small batch sizes, and its\nimplementation integrates smoothly with existing optimizers.", "AI": {"tldr": "HAM\uff08Hyperbolic Aware Minimization\uff09\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u6b65\u9aa4\u548c\u53cc\u66f2\u955c\u50cf\u6b65\u9aa4\uff0c\u6539\u8fdb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6536\u655b\u6027\u548c\u7279\u5f81\u5b66\u4e60\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u7b97\u6cd5\u7684\u9690\u5f0f\u504f\u5dee\uff0c\u4ee5\u89e3\u51b3\u8fc7\u53c2\u6570\u5316\u5bfc\u81f4\u7684\u5c0f\u5b66\u4e60\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51faHAM\u7b97\u6cd5\uff0c\u7ed3\u5408\u68af\u5ea6\u4e0b\u964d\u548c\u53cc\u66f2\u955c\u50cf\u6b65\u9aa4\uff0c\u63a8\u5bfc\u5176\u9ece\u66fc\u68af\u5ea6\u6d41\u3002", "result": "HAM\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u63d0\u5347\u6027\u80fd\uff0c\u5c24\u5176\u5728\u7a00\u758f\u5316\u65b9\u6cd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "HAM\u901a\u8fc7\u53cc\u66f2\u51e0\u4f55\u4f18\u5316\uff0c\u663e\u8457\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u4e14\u8ba1\u7b97\u9ad8\u6548\u3002", "keywords": "HAM, \u9690\u5f0f\u504f\u5dee, \u53cc\u66f2\u51e0\u4f55, \u6df1\u5ea6\u5b66\u4e60, \u4f18\u5316\u7b97\u6cd5"}}
{"id": "2506.02959", "pdf": "https://arxiv.org/pdf/2506.02959", "abs": "https://arxiv.org/abs/2506.02959", "authors": ["Zhixiong Su", "Yichen Wang", "Herun Wan", "Zhaohan Zhang", "Minnan Luo"], "title": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The misuse of large language models (LLMs) poses potential risks, motivating\nthe development of machine-generated text (MGT) detection. Existing literature\nprimarily concentrates on binary, document-level detection, thereby neglecting\ntexts that are composed jointly by human and LLM contributions. Hence, this\npaper explores the possibility of fine-grained MGT detection under human-AI\ncoauthoring. We suggest fine-grained detectors can pave pathways toward\ncoauthored text detection with a numeric AI ratio. Specifically, we propose a\ndataset, HACo-Det, which produces human-AI coauthored texts via an automatic\npipeline with word-level attribution labels. We retrofit seven prevailing\ndocument-level detectors to generalize them to word-level detection. Then we\nevaluate these detectors on HACo-Det on both word- and sentence-level detection\ntasks. Empirical results show that metric-based methods struggle to conduct\nfine-grained detection with a 0.462 average F1 score, while finetuned models\nshow superior performance and better generalization across domains. However, we\nargue that fine-grained co-authored text detection is far from solved. We\nfurther analyze factors influencing performance, e.g., context window, and\nhighlight the limitations of current methods, pointing to potential avenues for\nimprovement.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u7ec6\u7c92\u5ea6\u673a\u5668\u751f\u6210\u6587\u672c\uff08MGT\uff09\u68c0\u6d4b\u7684\u53ef\u80fd\u6027\uff0c\u7279\u522b\u662f\u5728\u4eba\u7c7b\u4e0eAI\u5171\u540c\u521b\u4f5c\u7684\u60c5\u5883\u4e0b\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u636e\u96c6HACo-Det\uff0c\u5e76\u8bc4\u4f30\u4e86\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u8bcd\u7ea7\u548c\u53e5\u7ea7\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5fae\u8c03\u6a21\u578b\u4f18\u4e8e\u57fa\u4e8e\u6307\u6807\u7684\u65b9\u6cd5\uff0c\u4f46\u4ecd\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6ee5\u7528\u5e26\u6765\u98ce\u9669\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6587\u6863\u7ea7\u68c0\u6d4b\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u4e0eAI\u5171\u540c\u521b\u4f5c\u7684\u6587\u672c\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u63d0\u51faHACo-Det\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u81ea\u52a8\u6d41\u7a0b\u751f\u6210\u4eba\u7c7b\u4e0eAI\u5171\u540c\u521b\u4f5c\u7684\u6587\u672c\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u8bcd\u6807\u6ce8\u6765\u6e90\u3002\u6539\u8fdb\u4e867\u79cd\u6587\u6863\u7ea7\u68c0\u6d4b\u5668\uff0c\u8bc4\u4f30\u5176\u5728\u8bcd\u7ea7\u548c\u53e5\u7ea7\u68c0\u6d4b\u4efb\u52a1\u7684\u8868\u73b0\u3002", "result": "\u57fa\u4e8e\u6307\u6807\u7684\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u68c0\u6d4b\u4e0a\u8868\u73b0\u8f83\u5dee\uff08\u5e73\u5747F1\u5206\u65700.462\uff09\uff0c\u5fae\u8c03\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u4e14\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u4f46\u4ecd\u672a\u5b8c\u5168\u89e3\u51b3\u95ee\u9898\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u5408\u4f5c\u6587\u672c\u68c0\u6d4b\u4ecd\u5177\u6311\u6218\u6027\u3002\u7814\u7a76\u5206\u6790\u4e86\u5f71\u54cd\u6027\u80fd\u7684\u56e0\u7d20\uff08\u5982\u4e0a\u4e0b\u6587\u7a97\u53e3\uff09\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\uff0c\u4eba\u7c7b-AI\u5171\u540c\u521b\u4f5c\uff0c\u7ec6\u7c92\u5ea6\u68c0\u6d4b\uff0cHACo-Det"}}
{"id": "2506.02071", "pdf": "https://arxiv.org/pdf/2506.02071", "abs": "https://arxiv.org/abs/2506.02071", "authors": ["Tadesse K. Bahiru", "Haileleol Tibebu", "Ioannis A. Kakadiaris"], "title": "AI Data Development: A Scorecard for the System Card Framework", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "Artificial intelligence has transformed numerous industries, from healthcare\nto finance, enhancing decision-making through automated systems. However, the\nreliability of these systems is mainly dependent on the quality of the\nunderlying datasets, raising ongoing concerns about transparency,\naccountability, and potential biases. This paper introduces a scorecard\ndesigned to evaluate the development of AI datasets, focusing on five key areas\nfrom the system card framework data development life cycle: data dictionary,\ncollection process, composition, motivation, and pre-processing. The method\nfollows a structured approach, using an intake form and scoring criteria to\nassess the quality and completeness of the data set. Applied to four diverse\ndatasets, the methodology reveals strengths and improvement areas. The results\nare compiled using a scoring system that provides tailored recommendations to\nenhance the transparency and integrity of the data set. The scorecard addresses\ntechnical and ethical aspects, offering a holistic evaluation of data\npractices. This approach aims to improve the quality of the data set. It offers\npractical guidance to curators and researchers in developing responsible AI\nsystems, ensuring fairness and accountability in decision support systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u7cfb\u7edf\u5361\u6846\u67b6\u6570\u636e\u5f00\u53d1\u751f\u547d\u5468\u671f\u7684\u8bc4\u5206\u5361\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u6570\u636e\u96c6\u7684\u5f00\u53d1\u8d28\u91cf\uff0c\u6db5\u76d6\u6570\u636e\u5b57\u5178\u3001\u6536\u96c6\u8fc7\u7a0b\u3001\u7ec4\u6210\u3001\u52a8\u673a\u548c\u9884\u5904\u7406\u4e94\u4e2a\u5173\u952e\u9886\u57df\u3002\u901a\u8fc7\u8bc4\u5206\u7cfb\u7edf\u63d0\u4f9b\u6539\u8fdb\u5efa\u8bae\uff0c\u65e8\u5728\u63d0\u5347\u6570\u636e\u96c6\u900f\u660e\u5ea6\u4e0e\u5b8c\u6574\u6027\uff0c\u4fc3\u8fdb\u8d1f\u8d23\u4efbAI\u7cfb\u7edf\u7684\u5f00\u53d1\u3002", "motivation": "AI\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6570\u636e\u96c6\u8d28\u91cf\uff0c\u4f46\u76ee\u524d\u5b58\u5728\u900f\u660e\u5ea6\u3001\u8d23\u4efb\u548c\u6f5c\u5728\u504f\u89c1\u7b49\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u65b9\u6cd5\u8bc4\u4f30\u548c\u6539\u8fdb\u6570\u636e\u96c6\u5f00\u53d1\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u4f7f\u7528\u8f93\u5165\u8868\u5355\u548c\u8bc4\u5206\u6807\u51c6\u8bc4\u4f30\u6570\u636e\u96c6\u8d28\u91cf\u4e0e\u5b8c\u6574\u6027\uff0c\u5e94\u7528\u4e8e\u56db\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u8bc4\u5206\u7cfb\u7edf\u63d0\u4f9b\u5b9a\u5236\u5316\u5efa\u8bae\u3002", "result": "\u8bc4\u5206\u5361\u63ed\u793a\u4e86\u6570\u636e\u96c6\u7684\u4f18\u52bf\u548c\u5f85\u6539\u8fdb\u4e4b\u5904\uff0c\u901a\u8fc7\u8bc4\u5206\u7cfb\u7edf\u4e3a\u63d0\u5347\u6570\u636e\u900f\u660e\u5ea6\u548c\u5b8c\u6574\u6027\u63d0\u4f9b\u5177\u4f53\u6307\u5bfc\uff0c\u540c\u65f6\u517c\u987e\u6280\u672f\u4e0e\u4f26\u7406\u8003\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6570\u636e\u7ba1\u7406\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u652f\u6301\u5f00\u53d1\u516c\u5e73\u3001\u53ef\u9760\u7684AI\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u63d0\u5347\u4e86\u6570\u636e\u5b9e\u8df5\u7684\u5168\u9762\u6027\u3002", "keywords": "\u4eba\u5de5\u667a\u80fd, \u6570\u636e\u96c6\u8d28\u91cf, \u900f\u660e\u5ea6, \u8d23\u4efb\u4f26\u7406, \u8bc4\u5206\u5361"}}
{"id": "2506.02654", "pdf": "https://arxiv.org/pdf/2506.02654", "abs": "https://arxiv.org/abs/2506.02654", "authors": ["Shiyu Shen", "Bin Pan", "Guirong Xue"], "title": "A Pretrained Probabilistic Transformer for City-Scale Traffic Volume Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "City-scale traffic volume prediction plays a pivotal role in intelligent\ntransportation systems, yet remains a challenge due to the inherent\nincompleteness and bias in observational data. Although deep learning-based\nmethods have shown considerable promise, most existing approaches produce\ndeterministic point estimates, thereby neglecting the uncertainty arising from\nunobserved traffic flows. Furthermore, current models are typically trained in\na city-specific manner, which hinders their generalizability and limits\nscalability across diverse urban contexts. To overcome these limitations, we\nintroduce TrafficPPT, a Pretrained Probabilistic Transformer designed to model\ntraffic volume as a distributional aggregation of trajectories. Our framework\nfuses heterogeneous data sources-including real-time observations, historical\ntrajectory data, and road network topology-enabling robust and\nuncertainty-aware traffic inference. TrafficPPT is initially pretrained on\nlarge-scale simulated data spanning multiple urban scenarios, and later\nfine-tuned on target cities to ensure effective domain adaptation. Experiments\non real-world datasets show that TrafficPPT consistently surpasses\nstate-of-the-art baselines, particularly under conditions of extreme data\nsparsity. Code will be open.", "AI": {"tldr": "\u63d0\u51faTrafficPPT\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u6982\u7387Transformer\u89e3\u51b3\u57ce\u5e02\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u6cdb\u5316\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u65b9\u6cd5\u591a\u4e3a\u786e\u5b9a\u6027\u70b9\u4f30\u8ba1\uff0c\u5ffd\u89c6\u4e0d\u786e\u5b9a\u6027\u4e14\u7f3a\u4e4f\u8de8\u57ce\u5e02\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u5b9e\u65f6\u89c2\u6d4b\u3001\u5386\u53f2\u8f68\u8ff9\u548c\u8def\u7f51\u62d3\u6251\u6570\u636e\uff0c\u9884\u8bad\u7ec3\u57fa\u4e8e\u591a\u57ce\u5e02\u6a21\u62df\u6570\u636e\u7684\u6982\u7387Transformer\u3002", "result": "\u5b9e\u9a8c\u663e\u793aTrafficPPT\u5728\u6781\u7aef\u6570\u636e\u7a00\u758f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TrafficPPT\u901a\u8fc7\u6982\u7387\u5efa\u6a21\u548c\u9884\u8bad\u7ec3\u63d0\u5347\u6d41\u91cf\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002", "keywords": "\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b,\u6982\u7387Transformer,\u591a\u6e90\u6570\u636e\u878d\u5408,\u9884\u8bad\u7ec3\u6a21\u578b"}}
{"id": "2506.02961", "pdf": "https://arxiv.org/pdf/2506.02961", "abs": "https://arxiv.org/abs/2506.02961", "authors": ["Yan Gao", "Massimo Roberto Scamarcia", "Javier Fernandez-Marques", "Mohammad Naseri", "Chong Shen Ng", "Dimitris Stripelis", "Zexi Li", "Tao Shen", "Jiamu Bai", "Daoyuan Chen", "Zikai Zhang", "Rui Hu", "InSeo Song", "Lee KangYoon", "Hong Jia", "Ting Dang", "Junyan Wang", "Zheyuan Liu", "Daniel Janes Beutel", "Lingjuan Lyu", "Nicholas D. Lane"], "title": "FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved state-of-the-art results across\ndiverse domains, yet their development remains reliant on vast amounts of\npublicly available data, raising concerns about data scarcity and the lack of\naccess to domain-specific, sensitive information. Federated Learning (FL)\npresents a compelling framework to address these challenges by enabling\ndecentralized fine-tuning on pre-trained LLMs without sharing raw data.\nHowever, the compatibility and performance of pre-trained LLMs in FL settings\nremain largely under explored. We introduce the FlowerTune LLM Leaderboard, a\nfirst-of-its-kind benchmarking suite designed to evaluate federated fine-tuning\nof LLMs across four diverse domains: general NLP, finance, medical, and coding.\nEach domain includes federated instruction-tuning datasets and domain-specific\nevaluation metrics. Our results, obtained through a collaborative, open-source\nand community-driven approach, provide the first comprehensive comparison\nacross 26 pre-trained LLMs with different aggregation and fine-tuning\nstrategies under federated settings, offering actionable insights into model\nperformance, resource constraints, and domain adaptation. This work lays the\nfoundation for developing privacy-preserving, domain-specialized LLMs for\nreal-world applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u51fa\u4e86FlowerTune LLM Leaderboard\uff0c\u7528\u4e8e\u8bc4\u4f30\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e0b\u9884\u8bad\u7ec3LLM\u7684\u5fae\u8c03\u6548\u679c\uff0c\u6db5\u76d6\u56db\u4e2a\u9886\u57df\uff0c\u5e76\u63d0\u4f9b\u4e8626\u79cd\u6a21\u578b\u7684\u5168\u9762\u6bd4\u8f83\u3002", "motivation": "\u89e3\u51b3LLM\u5f00\u53d1\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u654f\u611f\u4fe1\u606f\u8bbf\u95ee\u53d7\u9650\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e0b\u7684\u9884\u8bad\u7ec3LLM\u517c\u5bb9\u6027\u4e0e\u6027\u80fd\u3002", "method": "\u521b\u5efaFlowerTune LLM Leaderboard\uff0c\u6574\u5408\u56db\u4e2a\u9886\u57df\u7684\u8054\u90a6\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\u548c\u9886\u57df\u7279\u5b9a\u8bc4\u4f30\u6307\u6807\uff0c\u6d4b\u8bd526\u79cd\u9884\u8bad\u7ec3LLM\u3002", "result": "\u63d0\u4f9b\u4e86\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e0b\u6a21\u578b\u6027\u80fd\u3001\u8d44\u6e90\u9650\u5236\u548c\u9886\u57df\u9002\u5e94\u7684\u8be6\u7ec6\u5206\u6790\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u578b\u4e13\u4e1aLLM\u5f00\u53d1\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u9690\u79c1\u4fdd\u62a4\u578b\u4e13\u4e1aLLM\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u548c\u57fa\u7840\u3002", "keywords": "Large Language Models, Federated Learning, Fine-tuning, Privacy-preserving, Domain-specific"}}
{"id": "2506.02073", "pdf": "https://arxiv.org/pdf/2506.02073", "abs": "https://arxiv.org/abs/2506.02073", "authors": ["Mengliang He", "Jiayi Zeng", "Yankai Jiang", "Wei Zhang", "Zeming Liu", "Xiaoming Shi", "Aimin Zhou"], "title": "Flow2Code: Evaluating Large Language Models for Flowchart-based Code Generation Capability", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "While large language models (LLMs) show promise in code generation, existing\nbenchmarks neglect the flowchart-based code generation. To promote further\nresearch on flowchart-based code generation, this work presents Flow2Code, a\nnovel benchmark for flowchart-based code generation evaluation. The evaluation\ndataset spans 15 programming languages and includes 5,622 code segments paired\nwith 16,866 flowcharts of three types: code, UML, and pseudocode. Extensive\nexperiments with 13 multimodal LLMs reveal that current LLMs can not generate\ncode based on flowcharts perfectly. Besides, experiment results show that the\nsupervised fine-tuning technique contributes greatly to the models'\nperformance. We publicly release our code and datasets at\nhttps://github.com/hml-github/Flow2Code.", "AI": {"tldr": "\u63d0\u51fa\u4e86Flow2Code\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u6d41\u7a0b\u56fe\u7684\u4ee3\u7801\u751f\u6210\uff0c\u8986\u76d615\u79cd\u8bed\u8a00\u548c\u591a\u79cd\u6d41\u7a0b\u56fe\u7c7b\u578b\u3002\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u6709\u76d1\u7763\u5fae\u8c03\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5ffd\u89c6\u57fa\u4e8e\u6d41\u7a0b\u56fe\u7684\u4ee3\u7801\u751f\u6210\uff0cFlow2Code\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b5,622\u4ee3\u7801\u6bb5\u548c16,866\u6d41\u7a0b\u56fe\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f3013\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5e94\u7528\u6709\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u5f53\u524d\u6a21\u578b\u65e0\u6cd5\u5b8c\u7f8e\u751f\u6210\u57fa\u4e8e\u6d41\u7a0b\u56fe\u7684\u4ee3\u7801\uff0c\u4f46\u6709\u76d1\u7763\u5fae\u8c03\u663e\u8457\u6539\u5584\u6027\u80fd\u3002", "conclusion": "Flow2Code\u4e3a\u57fa\u4e8e\u6d41\u7a0b\u56fe\u7684\u4ee3\u7801\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u65b0\u5de5\u5177\uff0c\u6709\u76d1\u7763\u5fae\u8c03\u662f\u5173\u952e\u6539\u8fdb\u65b9\u5411\u3002", "keywords": "\u6d41\u7a0b\u56fe\u3001\u4ee3\u7801\u751f\u6210\u3001\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u57fa\u51c6\u8bc4\u6d4b\u3001\u6709\u76d1\u7763\u5fae\u8c03"}}
{"id": "2506.02665", "pdf": "https://arxiv.org/pdf/2506.02665", "abs": "https://arxiv.org/abs/2506.02665", "authors": ["Tianci Liu", "Tong Yang", "Quan Zhang", "Qi Lei"], "title": "Beyond Invisibility: Learning Robust Visible Watermarks for Stronger Copyright Protection", "categories": ["cs.LG"], "comment": "UAI 2025", "summary": "As AI advances, copyrighted content faces growing risk of unauthorized use,\nwhether through model training or direct misuse. Building upon invisible\nadversarial perturbation, recent works developed copyright protections against\nspecific AI techniques such as unauthorized personalization through DreamBooth\nthat are misused. However, these methods offer only short-term security, as\nthey require retraining whenever the underlying model architectures change. To\nestablish long-term protection aiming at better robustness, we go beyond\ninvisible perturbation, and propose a universal approach that embeds\n\\textit{visible} watermarks that are \\textit{hard-to-remove} into images.\nGrounded in a new probabilistic and inverse problem-based formulation, our\nframework maximizes the discrepancy between the \\textit{optimal} reconstruction\nand the original content. We develop an effective and efficient approximation\nalgorithm to circumvent a intractable bi-level optimization. Experimental\nresults demonstrate superiority of our approach across diverse scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9AI\u6a21\u578b\u672a\u7ecf\u6388\u6743\u4f7f\u7528\u5185\u5bb9\u7684\u957f\u671f\u4fdd\u62a4\u65b9\u6cd5\uff0c\u901a\u8fc7\u5d4c\u5165\u53ef\u89c1\u4f46\u96be\u4ee5\u53bb\u9664\u7684\u6c34\u5370\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740AI\u7684\u53d1\u5c55\uff0c\u7248\u6743\u5185\u5bb9\u9762\u4e34\u672a\u7ecf\u6388\u6743\u4f7f\u7528\u7684\u98ce\u9669\u589e\u52a0\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u77ed\u671f\u4fdd\u62a4\uff0c\u65e0\u6cd5\u5e94\u5bf9\u6a21\u578b\u67b6\u6784\u7684\u53d8\u5316\u3002", "method": "\u57fa\u4e8e\u6982\u7387\u548c\u9006\u95ee\u9898\u7684\u65b0\u6846\u67b6\uff0c\u5d4c\u5165\u53ef\u89c1\u4e14\u96be\u4ee5\u53bb\u9664\u7684\u6c34\u5370\uff0c\u5e76\u901a\u8fc7\u8fd1\u4f3c\u7b97\u6cd5\u89e3\u51b3\u53cc\u5c42\u4f18\u5316\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u7248\u6743\u5185\u5bb9\u63d0\u4f9b\u4e86\u957f\u671f\u4e14\u9c81\u68d2\u7684\u4fdd\u62a4\u3002", "keywords": "\u7248\u6743\u4fdd\u62a4,\u6c34\u5370,\u9c81\u68d2\u6027,\u53cc\u5c42\u4f18\u5316"}}
{"id": "2506.02973", "pdf": "https://arxiv.org/pdf/2506.02973", "abs": "https://arxiv.org/abs/2506.02973", "authors": ["Dingwei Chen", "Ziqiang Liu", "Feiteng Fang", "Chak Tou Leong", "Shiwen Ni", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Min Yang", "Chengming Li"], "title": "Expanding before Inferring: Enhancing Factuality in Large Language Models through Premature Layers Interpolation", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\nunderstanding and generation. However, their tendency to produce factually\ninconsistent outputs, commonly referred to as ''hallucinations'', remains a\ncritical challenge. Existing approaches, such as retrieval-based and\ninference-time correction methods, primarily address this issue at the input or\noutput level, often overlooking the intrinsic information refinement process\nand the role of premature layers. Meanwhile, alignment- and fine-tuning-based\nmethods are resource-intensive. In this paper, we propose PLI (Premature Layers\nInterpolation), a novel, training-free, and plug-and-play intervention designed\nto enhance factuality. PLI mitigates hallucinations by inserting premature\nlayers formed through mathematical interpolation with adjacent layers. Inspired\nby stable diffusion and sampling steps, PLI extends the depth of information\nprocessing and transmission in LLMs, improving factual coherence. Experiments\non four publicly available datasets demonstrate that PLI effectively reduces\nhallucinations while outperforming existing baselines in most cases. Further\nanalysis suggests that the success of layer interpolation is closely linked to\nLLMs' internal mechanisms. To promote reproducibility, we will release our code\nand data upon acceptance.", "AI": {"tldr": "PLI\uff08Premature Layers Interpolation\uff09\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u5b66\u63d2\u503c\u63d2\u5165\u8fc7\u65e9\u7684\u5c42\u6765\u51cf\u5c11LLMs\u7684\u5e7b\u89c9\uff0c\u63d0\u5347\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6587\u672c\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u4e8b\u5b9e\u4e0d\u4e00\u81f4\uff08\u5e7b\u89c9\uff09\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8f93\u5165\u6216\u8f93\u51fa\u5c42\u9762\u89e3\u51b3\u95ee\u9898\uff0c\u5ffd\u89c6\u4e86\u5185\u5728\u4fe1\u606f\u7ec6\u5316\u8fc7\u7a0b\u548c\u8fc7\u65e9\u5c42\u7684\u4f5c\u7528\u3002", "method": "\u63d0\u51faPLI\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u5b66\u63d2\u503c\u5728\u76f8\u90bb\u5c42\u4e4b\u95f4\u63d2\u5165\u8fc7\u65e9\u7684\u5c42\uff0c\u6269\u5c55\u4fe1\u606f\u5904\u7406\u6df1\u5ea6\uff0c\u63d0\u9ad8\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPLI\u6709\u6548\u51cf\u5c11\u4e86\u5e7b\u89c9\uff0c\u5e76\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "PLI\u901a\u8fc7\u5c42\u63d2\u503c\u4e0eLLMs\u5185\u90e8\u673a\u5236\u7d27\u5bc6\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u4e8b\u5b9e\u6027\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u8d44\u6e90\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3001\u5e7b\u89c9\u3001PLI\uff08Premature Layers Interpolation\uff09\u3001\u4e8b\u5b9e\u4e00\u81f4\u6027"}}
{"id": "2506.02694", "pdf": "https://arxiv.org/pdf/2506.02694", "abs": "https://arxiv.org/abs/2506.02694", "authors": ["Daichi Kimura", "Tomonori Izumitani", "Hisashi Kashima"], "title": "XicorAttention: Time Series Transformer Using Attention with Nonlinear Correlation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Various Transformer-based models have been proposed for time series\nforecasting. These models leverage the self-attention mechanism to capture\nlong-term temporal or variate dependencies in sequences. Existing methods can\nbe divided into two approaches: (1) reducing computational cost of attention by\nmaking the calculations sparse, and (2) reshaping the input data to aggregate\ntemporal features. However, existing attention mechanisms may not adequately\ncapture inherent nonlinear dependencies present in time series data, leaving\nroom for improvement. In this study, we propose a novel attention mechanism\nbased on Chatterjee's rank correlation coefficient, which measures nonlinear\ndependencies between variables. Specifically, we replace the matrix\nmultiplication in standard attention mechanisms with this rank coefficient to\nmeasure the query-key relationship. Since computing Chatterjee's correlation\ncoefficient involves sorting and ranking operations, we introduce a\ndifferentiable approximation employing SoftSort and SoftRank. Our proposed\nmechanism, ``XicorAttention,'' integrates it into several state-of-the-art\nTransformer models. Experimental results on real-world datasets demonstrate\nthat incorporating nonlinear correlation into the attention improves\nforecasting accuracy by up to approximately 9.1\\% compared to existing models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eChatterjee\u6392\u5e8f\u76f8\u5173\u7cfb\u6570\u7684\u65b0\u578b\u6ce8\u610f\u529b\u673a\u5236XicorAttention\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u76f8\u5173\u6027\u6539\u8fdb\u73b0\u6709Transformer\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6ce8\u610f\u529b\u673a\u5236\u672a\u80fd\u5145\u5206\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u975e\u7ebf\u6027\u4f9d\u8d56\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faXicorAttention\u673a\u5236\uff0c\u7528Chatterjee\u76f8\u5173\u7cfb\u6570\u66ff\u4ee3\u6807\u51c6\u6ce8\u610f\u529b\u4e2d\u7684\u77e9\u9635\u4e58\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u53ef\u5fae\u5206\u7684SoftSort\u548cSoftRank\u8fd1\u4f3c\u8ba1\u7b97\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cXicorAttention\u80fd\u5c06\u9884\u6d4b\u7cbe\u5ea6\u63d0\u5347\u7ea69.1%\u3002", "conclusion": "\u975e\u7ebf\u6027\u76f8\u5173\u6027\u7684\u5f15\u5165\u663e\u8457\u63d0\u9ad8\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b, Transformer, \u6ce8\u610f\u529b\u673a\u5236, Chatterjee\u76f8\u5173\u7cfb\u6570, \u975e\u7ebf\u6027\u4f9d\u8d56"}}
{"id": "2506.02979", "pdf": "https://arxiv.org/pdf/2506.02979", "abs": "https://arxiv.org/abs/2506.02979", "authors": ["Atsumoto Ohashi", "Shinya Iizuka", "Jingjing Jiang", "Ryuichiro Higashinaka"], "title": "Towards a Japanese Full-duplex Spoken Dialogue System", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Full-duplex spoken dialogue systems, which can model simultaneous\nbidirectional features of human conversations such as speech overlaps and\nbackchannels, have attracted significant attention recently. However, the study\nof full-duplex spoken dialogue systems for the Japanese language has been\nlimited, and the research on their development in Japanese remains scarce. In\nthis paper, we present the first publicly available full-duplex spoken dialogue\nmodel in Japanese, which is built upon Moshi, a full-duplex dialogue model in\nEnglish. Our model is trained through a two-stage process: pre-training on a\nlarge-scale spoken dialogue data in Japanese, followed by fine-tuning on\nhigh-quality stereo spoken dialogue data. We further enhance the model's\nperformance by incorporating synthetic dialogue data generated by a\nmulti-stream text-to-speech system. Evaluation experiments demonstrate that the\ntrained model outperforms Japanese baseline models in both naturalness and\nmeaningfulness.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86\u4e00\u79cd\u9488\u5bf9\u65e5\u8bed\u7684\u5168\u53cc\u5de5\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u6a21\u578b\uff0c\u57fa\u4e8e\u82f1\u8bed\u6a21\u578bMoshi\u5f00\u53d1\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u5408\u6210\u6570\u636e\u589e\u5f3a\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u586b\u8865\u4e86\u65e5\u8bed\u5168\u53cc\u5de5\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u5728\u81ea\u7136\u5ea6\u548c\u610f\u4e49\u6027\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u6a21\u578b\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u5927\u89c4\u6a21\u65e5\u8bed\u5bf9\u8bdd\u6570\u636e\u9884\u8bad\u7ec3\u548c\u9ad8\u8d28\u91cf\u7acb\u4f53\u58f0\u5bf9\u8bdd\u6570\u636e\u5fae\u8c03\uff09\uff0c\u5e76\u5229\u7528\u591a\u6d41\u8bed\u97f3\u5408\u6210\u6280\u672f\u751f\u6210\u5408\u6210\u6570\u636e\u589e\u5f3a\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u5728\u81ea\u7136\u5ea6\u548c\u610f\u4e49\u6027\u4e0a\u5747\u4f18\u4e8e\u65e5\u8bed\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u65e5\u8bed\u5168\u53cc\u5de5\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9996\u4e2a\u516c\u5f00\u53ef\u7528\u6a21\u578b\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "keywords": "\u5168\u53cc\u5de5\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\uff1b\u65e5\u8bed\uff1b\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1b\u5408\u6210\u6570\u636e\u589e\u5f3a"}}
{"id": "2506.02078", "pdf": "https://arxiv.org/pdf/2506.02078", "abs": "https://arxiv.org/abs/2506.02078", "authors": ["Emmy Postma", "Cristian Tejedor-Garcia"], "title": "Evaluating the Effectiveness of Pre-Trained Audio Embeddings for Classification of Parkinson's Disease Speech Data", "categories": ["eess.AS", "cs.AI"], "comment": "Accepted to Interspeech 2025. This publication is part of the project\n  Responsible AI for Voice Diagnostics (RAIVD) with file number NGF.1607.22.013\n  of the research programme NGF AiNed Fellowship Grants which is financed by\n  the Dutch Research Council (NWO)", "summary": "Speech impairments are prevalent biomarkers for Parkinson's Disease (PD),\nmotivating the development of diagnostic techniques using speech data for\nclinical applications. Although deep acoustic features have shown promise for\nPD classification, their effectiveness often varies due to individual speaker\ndifferences, a factor that has not been thoroughly explored in the existing\nliterature. This study investigates the effectiveness of three pre-trained\naudio embeddings (OpenL3, VGGish and Wav2Vec2.0 models) for PD classification.\nUsing the NeuroVoz dataset, OpenL3 outperforms others in diadochokinesis (DDK)\nand listen and repeat (LR) tasks, capturing critical acoustic features for PD\ndetection. Only Wav2Vec2.0 shows significant gender bias, achieving more\nfavorable results for male speakers, in DDK tasks. The misclassified cases\nreveal challenges with atypical speech patterns, highlighting the need for\nimproved feature extraction and model robustness in PD detection.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4e09\u79cd\u9884\u8bad\u7ec3\u97f3\u9891\u5d4c\u5165\u6a21\u578b\u5728\u5e15\u91d1\u68ee\u75c5\u5206\u7c7b\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0OpenL3\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800cWav2Vec2.0\u5b58\u5728\u6027\u522b\u504f\u89c1\u3002", "motivation": "\u8bed\u97f3\u969c\u788d\u662f\u5e15\u91d1\u68ee\u75c5\u7684\u5e38\u89c1\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u63a2\u8ba8\u4e2a\u4f53\u5dee\u5f02\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528OpenL3\u3001VGGish\u548cWav2Vec2.0\u4e09\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u57fa\u4e8eNeuroVoz\u6570\u636e\u96c6\u8fdb\u884c\u5206\u7c7b\u4efb\u52a1\u3002", "result": "OpenL3\u5728DDK\u548cLR\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0cWav2Vec2.0\u5728DDK\u4efb\u52a1\u4e2d\u5bf9\u7537\u6027\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u5bf9\u975e\u5178\u578b\u8bed\u97f3\u6a21\u5f0f\u7684\u6311\u6218\uff0c\u5f3a\u8c03\u9700\u6539\u8fdb\u7279\u5f81\u63d0\u53d6\u548c\u6a21\u578b\u9c81\u68d2\u6027\u3002", "keywords": "\u5e15\u91d1\u68ee\u75c5, \u8bed\u97f3\u5206\u6790, \u9884\u8bad\u7ec3\u6a21\u578b, \u6027\u522b\u504f\u89c1, \u7279\u5f81\u63d0\u53d6"}}
{"id": "2506.02703", "pdf": "https://arxiv.org/pdf/2506.02703", "abs": "https://arxiv.org/abs/2506.02703", "authors": ["Khizar Hayat", "Baptiste Magnier"], "title": "Data Leakage and Deceptive Performance: A Critical Examination of Credit Card Fraud Detection Methodologies", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "This study critically examines the methodological rigor in credit card fraud\ndetection research, revealing how fundamental evaluation flaws can overshadow\nalgorithmic sophistication. Through deliberate experimentation with improper\nevaluation protocols, we demonstrate that even simple models can achieve\ndeceptively impressive results when basic methodological principles are\nviolated. Our analysis identifies four critical issues plaguing current\napproaches: (1) pervasive data leakage from improper preprocessing sequences,\n(2) intentional vagueness in methodological reporting, (3) inadequate temporal\nvalidation for transaction data, and (4) metric manipulation through recall\noptimization at precision's expense. We present a case study showing how a\nminimal neural network architecture with data leakage outperforms many\nsophisticated methods reported in literature, achieving 99.9\\% recall despite\nfundamental evaluation flaws. These findings underscore that proper evaluation\nmethodology matters more than model complexity in fraud detection research. The\nstudy serves as a cautionary example of how methodological rigor must precede\narchitectural sophistication, with implications for improving research\npractices across machine learning applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6307\u51fa\u4fe1\u7528\u5361\u6b3a\u8bc8\u68c0\u6d4b\u7814\u7a76\u4e2d\u65b9\u6cd5\u8bba\u4e25\u8c28\u6027\u7684\u4e0d\u8db3\uff0c\u63ed\u793a\u4e86\u8bc4\u4f30\u7f3a\u9677\u5982\u4f55\u63a9\u76d6\u7b97\u6cd5\u7684\u590d\u6742\u6027\uff0c\u751a\u81f3\u7b80\u5355\u6a21\u578b\u4e5f\u80fd\u56e0\u65b9\u6cd5\u4e0d\u5f53\u800c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6279\u8bc4\u5f53\u524d\u4fe1\u7528\u5361\u6b3a\u8bc8\u68c0\u6d4b\u7814\u7a76\u4e2d\u666e\u904d\u5b58\u5728\u7684\u65b9\u6cd5\u8bba\u7f3a\u9677\uff0c\u5f3a\u8c03\u8bc4\u4f30\u4e25\u8c28\u6027\u7684\u91cd\u8981\u6027\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u6570\u636e\u6cc4\u9732\u3001\u65b9\u6cd5\u62a5\u9053\u6a21\u7cca\u3001\u65f6\u95f4\u9a8c\u8bc1\u4e0d\u8db3\u548c\u6307\u6807\u64cd\u7eb5\u7b49\u95ee\u9898\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u7528\u6848\u4f8b\u7814\u7a76\u8bf4\u660e\u8fd9\u4e9b\u7f3a\u9677\u5982\u4f55\u5bfc\u81f4\u8bef\u5bfc\u6027\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e00\u4e2a\u5b58\u5728\u6570\u636e\u6cc4\u9732\u7684\u6700\u5c0f\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u80fd\u8d85\u8d8a\u8bb8\u591a\u590d\u6742\u65b9\u6cd5\uff0c\u8fbe\u523099.9%\u7684\u53ec\u56de\u7387\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e25\u8c28\u6027\u6bd4\u6a21\u578b\u590d\u6742\u6027\u66f4\u91cd\u8981\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u7684\u8b66\u793a\u3002", "keywords": "\u4fe1\u7528\u5361\u6b3a\u8bc8\u68c0\u6d4b, \u65b9\u6cd5\u8bba\u4e25\u8c28\u6027, \u6570\u636e\u6cc4\u9732, \u8bc4\u4f30\u7f3a\u9677, \u673a\u5668\u5b66\u4e60\u7814\u7a76"}}
{"id": "2506.02987", "pdf": "https://arxiv.org/pdf/2506.02987", "abs": "https://arxiv.org/abs/2506.02987", "authors": ["Richard Armitage"], "title": "Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "12 pages, 1 Table", "summary": "Background: Large language models (LLMs) have demonstrated substantial\npotential to support clinical practice. Other than Chat GPT4 and its\npredecessors, few LLMs, especially those of the leading and more powerful\nreasoning model class, have been subjected to medical specialty examination\nquestions, including in the domain of primary care. This paper aimed to test\nthe capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and\nGemini 2.5 Pro) in primary care education, specifically in answering Member of\nthe Royal College of General Practitioners (MRCGP) style examination questions.\n  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer\n100 randomly chosen multiple choice questions from the Royal College of General\nPractitioners GP SelfTest on 25 May 2025. Questions included textual\ninformation, laboratory results, and clinical images. Each model was prompted\nto answer as a GP in the UK and was provided with full question information.\nEach question was attempted once by each model. Responses were scored against\ncorrect answers provided by GP SelfTest.\n  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was\n99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the\nsame questions was 73.0%.\n  Discussion: All models performed remarkably well, and all substantially\nexceeded the average performance of GPs and GP registrars who had answered the\nsame questions. o3 demonstrated the best performance, while the performances of\nthe other leading models were comparable with each other and were not\nsubstantially lower than that of o3. These findings strengthen the case for\nLLMs, particularly reasoning models, to support the delivery of primary care,\nespecially those that have been specifically trained on primary care clinical\ndata.", "AI": {"tldr": "\u6d4b\u8bd5\u4e86\u51e0\u79cd\u9886\u5148\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u521d\u7ea7\u533b\u7597\u6559\u80b2\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u56de\u7b54MRCGP\u98ce\u683c\u95ee\u9898\u4e0a\u663e\u8457\u4f18\u4e8e\u4eba\u7c7b\u533b\u751f\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u521d\u7ea7\u533b\u7597\u6559\u80b2\u4e2d\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u89e3\u51b3MRCGP\u98ce\u683c\u95ee\u9898\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u56db\u79cdLLMs\uff08o3\u3001Claude Opus 4\u3001Grok3\u548cGemini 2.5 Pro\uff09\u56de\u7b54100\u9053\u968f\u673a\u9009\u62e9\u7684MRCGP\u8003\u8bd5\u9898\u76ee\uff0c\u5e76\u5bf9\u5176\u51c6\u786e\u7387\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "o3\u5f97\u520699.0%\uff0c\u5176\u4ed6\u6a21\u578b\u5747\u4e3a95.0%\uff0c\u8fdc\u9ad8\u4e8e\u4eba\u7c7b\u533b\u751f\u7684\u5e73\u5747\u5f97\u520673.0%\u3002", "conclusion": "LLMs\uff0c\u5c24\u5176\u662f\u63a8\u7406\u7c7b\u6a21\u578b\uff0c\u5728\u521d\u7ea7\u533b\u7597\u652f\u6301\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u90a3\u4e9b\u7ecf\u8fc7\u521d\u7ea7\u533b\u7597\u6570\u636e\u4e13\u95e8\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u521d\u7ea7\u533b\u7597, MRCGP, \u8003\u8bd5\u95ee\u9898, \u8bc4\u4f30"}}
{"id": "2506.02712", "pdf": "https://arxiv.org/pdf/2506.02712", "abs": "https://arxiv.org/abs/2506.02712", "authors": ["Jayadev Naram", "Fredrik Hellstr\u00f6m", "Ziming Wang", "Rebecka J\u00f6rnsten", "Giuseppe Durisi"], "title": "Theoretical Performance Guarantees for Partial Domain Adaptation via Partial Optimal Transport", "categories": ["cs.LG", "stat.ML"], "comment": "ICML 2025", "summary": "In many scenarios of practical interest, labeled data from a target\ndistribution are scarce while labeled data from a related source distribution\nare abundant. One particular setting of interest arises when the target label\nspace is a subset of the source label space, leading to the framework of\npartial domain adaptation (PDA). Typical approaches to PDA involve minimizing a\ndomain alignment term and a weighted empirical loss on the source data, with\nthe aim of transferring knowledge between domains. However, a theoretical basis\nfor this procedure is lacking, and in particular, most existing weighting\nschemes are heuristic. In this work, we derive generalization bounds for the\nPDA problem based on partial optimal transport. These bounds corroborate the\nuse of the partial Wasserstein distance as a domain alignment term, and lead to\ntheoretically motivated explicit expressions for the empirical source loss\nweights. Inspired by these bounds, we devise a practical algorithm for PDA,\ntermed WARMPOT. Through extensive numerical experiments, we show that WARMPOT\nis competitive with recent approaches, and that our proposed weights improve on\nexisting schemes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u90e8\u5206\u6700\u4f18\u4f20\u8f93\u7684\u90e8\u5206\u57df\u81ea\u9002\u5e94\uff08PDA\uff09\u95ee\u9898\u6cdb\u5316\u754c\u9650\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u540d\u4e3aWARMPOT\u7684\u5b9e\u7528\u7b97\u6cd5\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u6709\u6807\u7b7e\u7684\u76ee\u6807\u5206\u5e03\u6570\u636e\u7a00\u7f3a\u800c\u76f8\u5173\u6e90\u5206\u5e03\u6570\u636e\u4e30\u5bcc\u7684\u60c5\u51b5\u4e0b\uff0cPDA\u6210\u4e3a\u91cd\u8981\u7814\u7a76\u9886\u57df\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\u4e14\u6743\u91cd\u8bbe\u8ba1\u591a\u4e3a\u542f\u53d1\u5f0f\u3002", "method": "\u901a\u8fc7\u90e8\u5206\u6700\u4f18\u8fd0\u8f93\u7406\u8bba\u57fa\u7840\u63a8\u5bfcPDA\u7684\u6cdb\u5316\u754c\u9650\uff0c\u63d0\u51fa\u90e8\u5206Wasserstein\u8ddd\u79bb\u4f5c\u4e3a\u57df\u5bf9\u9f50\u9879\uff0c\u5e76\u8bbe\u8ba1\u65b0\u7684\u6743\u91cd\u8868\u8fbe\u5f0f\u3002", "result": "\u63d0\u51fa\u7684WARMPOT\u7b97\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7406\u8bba\u754c\u9650\u4e3aPDA\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u63d0\u51fa\u7684\u6743\u91cd\u548c\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd\u3002", "keywords": "\u90e8\u5206\u57df\u81ea\u9002\u5e94\uff0cPDA\uff0c\u90e8\u5206\u6700\u4f18\u8fd0\u8f93\uff0cWasserstein\u8ddd\u79bb\uff0c\u6743\u91cd\u8bbe\u8ba1"}}
{"id": "2506.02995", "pdf": "https://arxiv.org/pdf/2506.02995", "abs": "https://arxiv.org/abs/2506.02995", "authors": ["Iuliia Zaitova", "Badr M. Abdullah", "Wei Xue", "Dietrich Klakow", "Bernd M\u00f6bius", "Tania Avgustinova"], "title": "It's Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text Systems", "categories": ["cs.CL"], "comment": "13 pages, 3 figures, ACL 2025", "summary": "Idioms are defined as a group of words with a figurative meaning not\ndeducible from their individual components. Although modern machine translation\nsystems have made remarkable progress, translating idioms remains a major\nchallenge, especially for speech-to-text systems, where research on this topic\nis notably sparse. In this paper, we systematically evaluate idiom translation\nas compared to conventional news translation in both text-to-text machine\ntranslation (MT) and speech-to-text translation (SLT) systems across two\nlanguage pairs (German to English, Russian to English). We compare\nstate-of-the-art end-to-end SLT systems (SeamlessM4T SLT-to-text, Whisper Large\nv3) with MT systems (SeamlessM4T SLT-to-text, No Language Left Behind), Large\nLanguage Models (DeepSeek, LLaMA) and cascaded alternatives. Our results reveal\nthat SLT systems experience a pronounced performance drop on idiomatic data,\noften reverting to literal translations even in higher layers, whereas MT\nsystems and Large Language Models demonstrate better handling of idioms. These\nfindings underscore the need for idiom-specific strategies and improved\ninternal representations in SLT architectures.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u4e60\u8bed\u7ffb\u8bd1\u5728\u6587\u672c\u5230\u6587\u672c\u548c\u8bed\u97f3\u5230\u6587\u672c\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8bed\u97f3\u5230\u6587\u672c\u7cfb\u7edf\u5728\u4e60\u8bed\u7ffb\u8bd1\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u800c\u673a\u5668\u7ffb\u8bd1\u548c\u5927\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u9700\u6539\u8fdb\u8bed\u97f3\u5230\u6587\u672c\u67b6\u6784\u3002", "motivation": "\u4e60\u8bed\u7ffb\u8bd1\u5728\u73b0\u4ee3\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u4e2d\u4ecd\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u8bed\u97f3\u5230\u6587\u672c\u7cfb\u7edf\u4e2d\uff0c\u76f8\u5173\u7814\u7a76\u8f83\u5c11\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u4e60\u8bed\u7ffb\u8bd1\u7684\u8868\u73b0\u3002", "method": "\u5728\u5fb7\u82f1\u548c\u4fc4\u82f1\u4e24\u79cd\u8bed\u8a00\u5bf9\u4e0a\uff0c\u6bd4\u8f83\u4e86\u6700\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u8bed\u97f3\u5230\u6587\u672c\u7cfb\u7edf\u3001\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u8bed\u97f3\u5230\u6587\u672c\u7cfb\u7edf\u5728\u4e60\u8bed\u7ffb\u8bd1\u4e0a\u8868\u73b0\u663e\u8457\u8f83\u5dee\uff0c\u5e38\u91c7\u7528\u5b57\u9762\u7ffb\u8bd1\uff0c\u800c\u673a\u5668\u7ffb\u8bd1\u548c\u5927\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8bed\u97f3\u5230\u6587\u672c\u7cfb\u7edf\u9700\u8981\u9488\u5bf9\u4e60\u8bed\u7684\u4f18\u5316\u7b56\u7565\u548c\u6539\u8fdb\u7684\u5185\u90e8\u8868\u793a\u3002", "keywords": "\u4e60\u8bed\u7ffb\u8bd1, \u8bed\u97f3\u5230\u6587\u672c, \u673a\u5668\u7ffb\u8bd1, \u5927\u8bed\u8a00\u6a21\u578b, \u7aef\u5230\u7aef\u7cfb\u7edf"}}
{"id": "2506.02080", "pdf": "https://arxiv.org/pdf/2506.02080", "abs": "https://arxiv.org/abs/2506.02080", "authors": ["Aditya Kamlesh Parikh", "Cristian Tejedor-Garcia", "Catia Cucchiarini", "Helmer Strik"], "title": "Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological Knowledge", "categories": ["eess.AS", "cs.AI"], "comment": "Accepted to Interspeech 2025. This publication is part of the project\n  Responsible AI for Voice Diagnostics (RAIVD) with file number NGF.1607.22.013\n  of the research programme NGF AiNed Fellowship Grants which is financed by\n  the Dutch Research Council (NWO)", "summary": "Computer-Assisted Pronunciation Training (CAPT) systems employ automatic\nmeasures of pronunciation quality, such as the goodness of pronunciation (GOP)\nmetric. GOP relies on forced alignments, which are prone to labeling and\nsegmentation errors due to acoustic variability. While alignment-free methods\naddress these challenges, they are computationally expensive and scale poorly\nwith phoneme sequence length and inventory size. To enhance efficiency, we\nintroduce a substitution-aware alignment-free GOP that restricts phoneme\nsubstitutions based on phoneme clusters and common learner errors. We evaluated\nour GOP on two L2 English speech datasets, one with child speech, My\nPronunciation Coach (MPC), and SpeechOcean762, which includes child and adult\nspeech. We compared RPS (restricted phoneme substitutions) and UPS\n(unrestricted phoneme substitutions) setups within alignment-free methods,\nwhich outperformed the baseline. We discuss our results and outline avenues for\nfuture research.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86\u4e00\u79cd\u66ff\u4ee3\u611f\u77e5\u7684\u65e0\u5bf9\u9f50GOP\u65b9\u6cd5\uff0c\u901a\u8fc7\u9650\u5236\u97f3\u7d20\u66ff\u6362\u6765\u63d0\u9ad8\u6548\u7387\uff0c\u5e76\u5728\u4e24\u4e2aL2\u82f1\u8bed\u8bed\u97f3\u6570\u636e\u96c6\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684GOP\u4f9d\u8d56\u4e8e\u5f3a\u5236\u5bf9\u9f50\uff0c\u5bb9\u6613\u56e0\u58f0\u5b66\u53d8\u5f02\u6027\u5bfc\u81f4\u6807\u8bb0\u548c\u5206\u5272\u9519\u8bef\uff0c\u800c\u65e0\u5bf9\u9f50\u65b9\u6cd5\u867d\u7136\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6269\u5c55\u6027\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u97f3\u7d20\u96c6\u7fa4\u548c\u5e38\u89c1\u5b66\u4e60\u8005\u9519\u8bef\u7684\u66ff\u4ee3\u611f\u77e5\u65e0\u5bf9\u9f50GOP\u65b9\u6cd5\uff0c\u9650\u5236\u97f3\u7d20\u66ff\u6362\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5728\u4e24\u4e2aL2\u82f1\u8bed\u8bed\u97f3\u6570\u636e\u96c6\uff08MPC\u548cSpeechOcean762\uff09\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65e0\u5bf9\u9f50\u65b9\u6cd5\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u66ff\u4ee3\u611f\u77e5\u65e0\u5bf9\u9f50GOP\u662f\u6709\u6548\u7684\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "keywords": "CAPT, GOP, \u65e0\u5bf9\u9f50\u65b9\u6cd5, \u97f3\u7d20\u66ff\u6362, L2\u82f1\u8bed\u53d1\u97f3"}}
{"id": "2506.02718", "pdf": "https://arxiv.org/pdf/2506.02718", "abs": "https://arxiv.org/abs/2506.02718", "authors": ["Guanzhong Chen", "Shaoxiong Yang", "Chao Li", "Wei Liu", "Jian Luan", "Zenglin Xu"], "title": "Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent Systems", "categories": ["cs.LG", "cs.AI", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success across diverse\nnatural language processing tasks, yet their deployment in real-world\napplications is hindered by fixed knowledge cutoffs and difficulties in\ngenerating controllable, accurate outputs in a single inference. Multi-agent\nsystems (MAS) built from specialized LLM agents offer a promising solution,\nenabling dynamic collaboration and iterative reasoning. However, optimizing\nthese systems remains a challenge, as conventional methods such as prompt\nengineering and supervised fine-tuning entail high engineering overhead and\nlimited adaptability. Reinforcement learning (RL), particularly multi-agent\nreinforcement learning (MARL), provides a scalable framework by refining agent\npolicies based on system-level feedback. Nevertheless, existing MARL\nalgorithms, such as Multi-Agent Proximal Policy Optimization (MAPPO), rely on\nCritic networks, which can cause training instability and increase\ncomputational burden. To address these limitations and target the prototypical\nMulti-Agent Search System (MASS), we propose Multi-Agent Heterogeneous Group\nPolicy Optimization (MHGPO), a novel Critic-free algorithm that guides policy\nupdates by estimating relative reward advantages across heterogeneous groups of\nrollouts. MHGPO eliminates the need for Critic networks, enhancing stability\nand reducing computational overhead. Additionally, we introduce three group\nrollout sampling strategies that trade off between efficiency and\neffectiveness. Experiments on a multi-agent LLM-based search system demonstrate\nthat MHGPO consistently outperforms MAPPO in both task performance and\ncomputational efficiency, without requiring warm-up, underscoring its potential\nfor stable and scalable optimization of complex LLM-based MAS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591aAgent\u5f02\u8d28\u7ec4\u7b56\u7565\u4f18\u5316\uff08MHGPO\uff09\uff0c\u4e00\u79cd\u65e0Critic\u7f51\u7edc\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u57fa\u4e8eLLM\u7684\u591aAgent\u7cfb\u7edf\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u4efb\u52a1\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5b9e\u9645\u90e8\u7f72\u4e2d\uff0c\u56fa\u5b9a\u7684\u77e5\u8bc6\u622a\u65ad\u548c\u8f93\u51fa\u63a7\u5236\u95ee\u9898\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u591aAgent\u7cfb\u7edf\uff08MAS\uff09\u901a\u8fc7\u52a8\u6001\u534f\u4f5c\u548c\u8fed\u4ee3\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u5de5\u7a0b\u5f00\u9500\u9ad8\u548c\u9002\u5e94\u6027\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faMHGPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f30\u8ba1\u5f02\u8d28\u7ec4\u4e4b\u95f4\u7684\u76f8\u5bf9\u5956\u52b1\u4f18\u52bf\u6765\u6307\u5bfc\u7b56\u7565\u66f4\u65b0\uff0c\u65e0\u9700Critic\u7f51\u7edc\u3002\u8fd8\u5f15\u5165\u4e86\u4e09\u79cd\u7ec4\u91c7\u6837\u7b56\u7565\u5e73\u8861\u6548\u7387\u4e0e\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMHGPO\u5728\u591aAgent\u57fa\u4e8eLLM\u7684\u641c\u7d22\u7cfb\u7edf\u4e2d\u6027\u80fd\u4f18\u4e8eMAPPO\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u4e14\u65e0\u9700\u9884\u70ed\u3002", "conclusion": "MHGPO\u4e3a\u590d\u6742LLM-based MAS\u7684\u7a33\u5b9a\u548c\u53ef\u6269\u5c55\u4f18\u5316\u63d0\u4f9b\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u591aAgent\u7cfb\u7edf, \u5927\u8bed\u8a00\u6a21\u578b, \u5f3a\u5316\u5b66\u4e60, MHGPO, \u8ba1\u7b97\u6548\u7387"}}
{"id": "2506.02998", "pdf": "https://arxiv.org/pdf/2506.02998", "abs": "https://arxiv.org/abs/2506.02998", "authors": ["\u0110or\u0111e Klisura", "Astrid R Bernaga Torres", "Anna Karen G\u00e1rate-Escamilla", "Rajesh Roshan Biswal", "Ke Yang", "Hilal Pataci", "Anthony Rios"], "title": "A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Privacy policies inform users about data collection and usage, yet their\ncomplexity limits accessibility for diverse populations. Existing Privacy\nPolicy Question Answering (QA) systems exhibit performance disparities across\nEnglish dialects, disadvantaging speakers of non-standard varieties. We propose\na novel multi-agent framework inspired by human-centered design principles to\nmitigate dialectal biases. Our approach integrates a Dialect Agent, which\ntranslates queries into Standard American English (SAE) while preserving\ndialectal intent, and a Privacy Policy Agent, which refines predictions using\ndomain expertise. Unlike prior approaches, our method does not require\nretraining or dialect-specific fine-tuning, making it broadly applicable across\nmodels and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves\nGPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from\n0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without\nadditional training data. These results highlight the effectiveness of\nstructured agent collaboration in mitigating dialect biases and underscore the\nimportance of designing NLP systems that account for linguistic diversity to\nensure equitable access to privacy information.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u65b9\u8a00\u667a\u80fd\u4f53\u548c\u9690\u79c1\u653f\u7b56\u667a\u80fd\u4f53\uff0c\u51cf\u5c11\u9690\u79c1\u653f\u7b56\u95ee\u7b54\u7cfb\u7edf\u4e2d\u7684\u65b9\u8a00\u504f\u89c1\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u7279\u5b9a\u65b9\u8a00\u5fae\u8c03\u3002", "motivation": "\u9690\u79c1\u653f\u7b56\u7684\u590d\u6742\u6027\u9650\u5236\u4e86\u591a\u5143\u4eba\u7fa4\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u4e14\u73b0\u6709\u9690\u79c1\u653f\u7b56\u95ee\u7b54\u7cfb\u7edf\u5728\u82f1\u8bed\u65b9\u8a00\u95f4\u5b58\u5728\u6027\u80fd\u5dee\u5f02\uff0c\u5f71\u54cd\u975e\u6807\u51c6\u65b9\u8a00\u4f7f\u7528\u8005\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u62ec\u65b9\u8a00\u667a\u80fd\u4f53\uff08\u5c06\u67e5\u8be2\u7ffb\u8bd1\u4e3a\u6807\u51c6\u82f1\u8bed\uff09\u548c\u9690\u79c1\u653f\u7b56\u667a\u80fd\u4f53\uff08\u5229\u7528\u9886\u57df\u77e5\u8bc6\u4f18\u5316\u9884\u6d4b\uff09\u3002", "result": "\u5728PrivacyQA\u548cPolicyQA\u4e0a\u7684\u96f6\u6837\u672c\u51c6\u786e\u7387\u5206\u522b\u4ece0.394\u63d0\u5347\u81f30.601\u548c\u4ece0.352\u63d0\u5347\u81f30.464\uff0c\u4f18\u4e8e\u6216\u5c11\u6837\u672c\u57fa\u7ebf\u7684\u6027\u80fd\u3002", "conclusion": "\u7ed3\u6784\u5316\u667a\u80fd\u4f53\u534f\u4f5c\u6709\u6548\u51cf\u5c11\u65b9\u8a00\u504f\u89c1\uff0c\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u8003\u8651\u8bed\u8a00\u591a\u6837\u6027\u7684NLP\u7cfb\u7edf\u4ee5\u786e\u4fdd\u516c\u5e73\u83b7\u53d6\u9690\u79c1\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002", "keywords": "\u9690\u79c1\u653f\u7b56,\u95ee\u7b54\u7cfb\u7edf,\u65b9\u8a00\u504f\u89c1,\u591a\u667a\u80fd\u4f53\u6846\u67b6,\u516c\u5e73\u8bbf\u95ee"}}
{"id": "2506.02724", "pdf": "https://arxiv.org/pdf/2506.02724", "abs": "https://arxiv.org/abs/2506.02724", "authors": ["Andrey Veprikov", "Vladimir Solodkin", "Alexander Zyl", "Andrey Savchenko", "Aleksandr Beznosikov"], "title": "WeightLoRA: Keep Only Necessary Adapters", "categories": ["cs.LG", "math.OC"], "comment": "13 pages, 9 tables", "summary": "The widespread utilization of language models in modern applications is\ninconceivable without Parameter-Efficient Fine-Tuning techniques, such as\nlow-rank adaptation ($\\texttt{LoRA}$), which adds trainable adapters to\nselected layers. Although $\\texttt{LoRA}$ may obtain accurate solutions, it\nrequires significant memory to train large models and intuition on which layers\nto add adapters. In this paper, we propose a novel method,\n$\\texttt{WeightLoRA}$, which overcomes this issue by adaptive selection of the\nmost critical $\\texttt{LoRA}$ heads throughout the optimization process. As a\nresult, we can significantly reduce the number of trainable parameters while\nmaintaining the capability to obtain consistent or even superior metric values.\nWe conduct experiments for a series of competitive benchmarks and DeBERTa,\nBART, and Llama models, comparing our method with different adaptive\napproaches. The experimental results demonstrate the efficacy of\n$\\texttt{WeightLoRA}$ and the superior performance of $\\texttt{WeightLoRA+}$ in\nalmost all cases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWeightLoRA\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u5173\u952e\u7684LoRA\u5934\u6765\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684LoRA\u65b9\u6cd5\u867d\u7136\u6709\u6548\uff0c\u4f46\u9700\u8981\u5927\u91cf\u5185\u5b58\u548c\u4eba\u5de5\u76f4\u89c9\u6765\u9009\u62e9\u9002\u914d\u5668\u5c42\uff0c\u9650\u5236\u4e86\u5176\u6548\u7387\u3002", "method": "\u63d0\u51fa\u7684WeightLoRA\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u5173\u952e\u7684LoRA\u5934\uff0c\u52a8\u6001\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cWeightLoRA\u80fd\u663e\u8457\u51cf\u5c11\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8d8a\u57fa\u51c6\u6027\u80fd\u3002", "conclusion": "WeightLoRA\u53ca\u5176\u6539\u8fdb\u7248WeightLoRA+\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "keywords": "Parameter-Efficient Fine-Tuning, LoRA, WeightLoRA, \u81ea\u9002\u5e94\u4f18\u5316, \u8f7b\u91cf\u5316\u8bad\u7ec3"}}
{"id": "2506.03009", "pdf": "https://arxiv.org/pdf/2506.03009", "abs": "https://arxiv.org/abs/2506.03009", "authors": ["Florian Ludwig", "Torsten Zesch", "Frederike Zufall"], "title": "Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The assessment of legal problems requires the consideration of a specific\nlegal system and its levels of abstraction, from constitutional law to\nstatutory law to case law. The extent to which Large Language Models (LLMs)\ninternalize such legal systems is unknown. In this paper, we propose and\ninvestigate different approaches to condition LLMs at different levels of\nabstraction in legal systems. This paper examines different approaches to\nconditioning LLMs at multiple levels of abstraction in legal systems to detect\npotentially punishable hate speech. We focus on the task of classifying whether\na specific social media posts falls under the criminal offense of incitement to\nhatred as prescribed by the German Criminal Code. The results show that there\nis still a significant performance gap between models and legal experts in the\nlegal assessment of hate speech, regardless of the level of abstraction with\nwhich the models were conditioned. Our analysis revealed, that models\nconditioned on abstract legal knowledge lacked deep task understanding, often\ncontradicting themselves and hallucinating answers, while models using concrete\nlegal knowledge performed reasonably well in identifying relevant target\ngroups, but struggled with classifying target conducts.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5728\u4e0d\u540c\u6cd5\u5f8b\u62bd\u8c61\u5c42\u6b21\u4e0a\u8c03\u6574\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ee5\u68c0\u6d4b\u53ef\u60e9\u7f5a\u7684\u4ec7\u6068\u8a00\u8bba\uff0c\u53d1\u73b0\u5728\u5206\u7c7b\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u662f\u5426\u7b26\u5408\u5fb7\u56fd\u5211\u6cd5\u89c4\u5b9a\u7684\u717d\u52a8\u4ec7\u6068\u7f6a\u65f6\uff0c\u6a21\u578b\u4e0e\u6cd5\u5f8b\u4e13\u5bb6\u4ecd\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u8bc4\u4f30\u6cd5\u5f8b\u95ee\u9898\u9700\u8981\u8003\u8651\u5177\u4f53\u7684\u6cd5\u5f8b\u4f53\u7cfb\u53ca\u5176\u62bd\u8c61\u5c42\u6b21\uff0c\u4f46\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u5185\u5316\u8fd9\u4e9b\u6cd5\u5f8b\u4f53\u7cfb\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u5982\u4f55\u5728\u4e0d\u540c\u62bd\u8c61\u5c42\u6b21\u4e0a\u8c03\u6574LLMs\uff0c\u4ee5\u68c0\u6d4b\u6f5c\u5728\u7684\u4ec7\u6068\u8a00\u8bba\u3002", "method": "\u7814\u7a76\u4e86\u591a\u79cd\u8c03\u6574LLMs\u7684\u65b9\u6cd5\uff0c\u4ee5\u5728\u4e0d\u540c\u62bd\u8c61\u5c42\u6b21\u7684\u6cd5\u5f8b\u4f53\u7cfb\u4e2d\u5206\u7c7b\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u662f\u5426\u6784\u6210\u5fb7\u56fd\u5211\u6cd5\u89c4\u5b9a\u7684\u717d\u52a8\u4ec7\u6068\u7f6a\u3002", "result": "\u6a21\u578b\u4e0e\u6cd5\u5f8b\u4e13\u5bb6\u5728\u4ec7\u6068\u8a00\u8bba\u7684\u6cd5\u5f8b\u8bc4\u4f30\u4e0a\u4ecd\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002\u57fa\u4e8e\u62bd\u8c61\u6cd5\u5f8b\u77e5\u8bc6\u7684\u6a21\u578b\u7f3a\u4e4f\u6df1\u5165\u7684\u4efb\u52a1\u7406\u89e3\uff0c\u5e38\u81ea\u76f8\u77db\u76fe\u6216\u4ea7\u751f\u5e7b\u89c9\u7b54\u6848\uff1b\u800c\u57fa\u4e8e\u5177\u4f53\u6cd5\u5f8b\u77e5\u8bc6\u7684\u6a21\u578b\u5728\u8bc6\u522b\u76f8\u5173\u76ee\u6807\u7fa4\u4f53\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5bf9\u76ee\u6807\u884c\u4e3a\u7684\u5206\u7c7b\u4ecd\u6709\u56f0\u96be\u3002", "conclusion": "\u5f53\u524dLLMs\u5728\u6cd5\u5f8b\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u6709\u9650\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u7684\u6cd5\u5f8b\u6982\u5ff5\u65f6\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\u3002", "keywords": "\u6cd5\u5f8b\u8bc4\u4f30, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u62bd\u8c61\u5c42\u6b21, \u4ec7\u6068\u8a00\u8bba, \u5fb7\u56fd\u5211\u6cd5"}}
{"id": "2506.02082", "pdf": "https://arxiv.org/pdf/2506.02082", "abs": "https://arxiv.org/abs/2506.02082", "authors": ["Saurabh Agrawal", "Raj Gohil", "Gopal Kumar Agrawal", "Vikram C M", "Kushal Verma"], "title": "SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS Prediction", "categories": ["cs.SD", "cs.AI", "cs.LG"], "comment": null, "summary": "Speech quality assessment is a critical process in selecting text-to-speech\nsynthesis (TTS) or voice conversion models. Evaluation of voice synthesis can\nbe done using objective metrics or subjective metrics. Although there are many\nobjective metrics like the Perceptual Evaluation of Speech Quality (PESQ),\nPerceptual Objective Listening Quality Assessment (POLQA) or Short-Time\nObjective Intelligibility (STOI) but none of them is feasible in selecting the\nbest model. On the other hand subjective metric like Mean Opinion Score is\nhighly reliable but it requires a lot of manual efforts and are time-consuming.\nTo counter the issues in MOS Evaluation, we have developed a novel model,\nSpeaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS) which is a\nsmall-sized, end-to-end, highly generalized and scalable model for predicting\nMOS score on a scale of 5. We use the sequences of convolutions and stack them\nto get the latent features of the audio samples to get the best\nstate-of-the-art results based on mean squared error (MSE), Linear Concordance\nCorrelation coefficient (LCC), Spearman Rank Correlation Coefficient (SRCC) and\nKendall Rank Correlation Coefficient (KTAU).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSALF-MOS\u7684\u65b0\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u8bed\u97f3\u5408\u6210\u7684MOS\u5206\u6570\uff0c\u89e3\u51b3\u4e86\u4e3b\u89c2\u8bc4\u4ef7\u7684\u9ad8\u6210\u672c\u548c\u5ba2\u89c2\u8bc4\u4ef7\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "motivation": "\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\u5728TTS\u6216\u8bed\u97f3\u8f6c\u6362\u6a21\u578b\u9009\u62e9\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5ba2\u89c2\u6307\u6807\u4e0d\u53ef\u9760\uff0c\u4e3b\u89c2\u8bc4\u4ef7\uff08\u5982MOS\uff09\u53c8\u8017\u65f6\u8017\u529b\u3002", "method": "\u4f7f\u7528\u5377\u79ef\u5e8f\u5217\u63d0\u53d6\u8bed\u97f3\u6837\u672c\u7684\u6f5c\u5728\u7279\u5f81\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5c0f\u578b\u3001\u7aef\u5230\u7aef\u3001\u9ad8\u6cdb\u5316\u6027\u548c\u53ef\u6269\u5c55\u7684SALF-MOS\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728MSE\u3001LCC\u3001SRCC\u548cKTAU\u7b49\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "SALF-MOS\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\u3002", "keywords": "\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\uff0cMOS\uff0cSALF-MOS\uff0cTTS\uff0c\u8bed\u97f3\u8f6c\u6362"}}
{"id": "2506.02749", "pdf": "https://arxiv.org/pdf/2506.02749", "abs": "https://arxiv.org/abs/2506.02749", "authors": ["Changyi Xiao", "Yixin Cao"], "title": "Knowledge Graph Completion by Intermediate Variables Regularization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Knowledge graph completion (KGC) can be framed as a 3-order binary tensor\ncompletion task. Tensor decomposition-based (TDB) models have demonstrated\nstrong performance in KGC. In this paper, we provide a summary of existing TDB\nmodels and derive a general form for them, serving as a foundation for further\nexploration of TDB models. Despite the expressiveness of TDB models, they are\nprone to overfitting. Existing regularization methods merely minimize the norms\nof embeddings to regularize the model, leading to suboptimal performance.\nTherefore, we propose a novel regularization method for TDB models that\naddresses this limitation. The regularization is applicable to most TDB models\nand ensures tractable computation. Our method minimizes the norms of\nintermediate variables involved in the different ways of computing the\npredicted tensor. To support our regularization method, we provide a\ntheoretical analysis that proves its effect in promoting low trace norm of the\npredicted tensor to reduce overfitting. Finally, we conduct experiments to\nverify the effectiveness of our regularization technique as well as the\nreliability of our theoretical analysis. The code is available at\nhttps://github.com/changyi7231/IVR.", "AI": {"tldr": "\u672c\u6587\u603b\u7ed3\u4e86\u73b0\u6709\u7684\u57fa\u4e8e\u5f20\u91cf\u5206\u89e3\u7684\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\uff08KGC\uff09\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u4ee5\u51cf\u5c11\u8fc7\u62df\u5408\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u6b63\u5219\u5316\u65b9\u6cd5\u4ec5\u6700\u5c0f\u5316\u5d4c\u5165\u8303\u6570\uff0c\u6027\u80fd\u4e0d\u4f73\uff0c\u9700\u63d0\u51fa\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u4ee5\u51cf\u5c11TDB\u6a21\u578b\u8fc7\u62df\u5408\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u6700\u5c0f\u5316\u9884\u6d4b\u5f20\u91cf\u8ba1\u7b97\u4e2d\u6d89\u53ca\u7684\u4e2d\u95f4\u53d8\u91cf\u7684\u8303\u6570\uff0c\u5e76\u786e\u4fdd\u8ba1\u7b97\u53ef\u884c\u6027\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u6b63\u5219\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\u53ca\u5176\u964d\u4f4e\u8fc7\u62df\u5408\u7684\u4f5c\u7528\u3002", "conclusion": "\u65b0\u6b63\u5219\u5316\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u6570TDB\u6a21\u578b\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u8fc7\u62df\u5408\uff0c\u63d0\u5347KGC\u6027\u80fd\u3002", "keywords": "\u77e5\u8bc6\u56fe\u8c31\u8865\u5168,\u5f20\u91cf\u5206\u89e3,\u6b63\u5219\u5316\u65b9\u6cd5,\u8fc7\u62df\u5408"}}
{"id": "2506.03011", "pdf": "https://arxiv.org/pdf/2506.03011", "abs": "https://arxiv.org/abs/2506.03011", "authors": ["Aditya Bharat Soni", "Boxuan Li", "Xingyao Wang", "Valerie Chen", "Graham Neubig"], "title": "Coding Agents with Multimodal Browsing are Generalist Problem Solvers", "categories": ["cs.CL"], "comment": null, "summary": "Modern human labor is characterized by specialization; we train for years and\ndevelop particular tools that allow us to perform well across a variety of\ntasks. In addition, AI agents have been specialized for domains such as\nsoftware engineering, web navigation, and workflow automation. However, this\nresults in agents that are good for one thing but fail to generalize beyond\ntheir intended scope. One reason for this is that agent developers provide a\nhighly specialized set of tools or make architectural decisions optimized for a\nspecific use case or benchmark. In this work, we ask the question: what is the\nminimal set of general tools that can be used to achieve high performance\nacross a diverse set of tasks? Our answer is OpenHands-Versa, a generalist\nagent built with a modest number of general tools: code editing and execution,\nweb search, as well as multimodal web browsing and file access. Importantly,\nOpenHands-Versa demonstrates superior or competitive performance over leading\nspecialized agents across three diverse and challenging benchmarks: SWE-Bench\nMultimodal, GAIA, and The Agent Company, outperforming the best-performing\npreviously published results with absolute improvements in success rate of 9.1,\n1.3, and 9.1 points respectively. Further, we show how existing\nstate-of-the-art multi-agent systems fail to generalize beyond their target\ndomains. These results demonstrate the feasibility of developing a generalist\nagent to solve diverse tasks and establish OpenHands-Versa as a strong baseline\nfor future research.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u7528\u4ee3\u7406OpenHands-Versa\uff0c\u4f7f\u7528\u5c11\u91cf\u901a\u7528\u5de5\u5177\uff08\u4ee3\u7801\u7f16\u8f91\u3001\u7f51\u7edc\u641c\u7d22\u7b49\uff09\u5728\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4e13\u7528\u4ee3\u7406\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u591a\u4e3a\u4e13\u7528\u578b\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u5176\u4ed6\u4efb\u52a1\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u7528\u6700\u5c11\u901a\u7528\u5de5\u5177\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "method": "\u5f00\u53d1OpenHands-Versa\u4ee3\u7406\uff0c\u6574\u5408\u4ee3\u7801\u7f16\u8f91\u3001\u7f51\u7edc\u641c\u7d22\u3001\u591a\u6a21\u6001\u6d4f\u89c8\u7b49\u901a\u7528\u5de5\u5177\u3002", "result": "OpenHands-Versa\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4e13\u7528\u4ee3\u7406\uff0c\u5982SWE-Bench Multimodal\u7b49\u3002", "conclusion": "\u901a\u7528\u4ee3\u7406\u53ef\u884c\uff0cOpenHands-Versa\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u57fa\u7ebf\u3002", "keywords": "\u901a\u7528\u4ee3\u7406,OpenHands-Versa,\u591a\u4efb\u52a1,\u6cdb\u5316\u80fd\u529b"}}
{"id": "2506.02083", "pdf": "https://arxiv.org/pdf/2506.02083", "abs": "https://arxiv.org/abs/2506.02083", "authors": ["Aditya Srinivas Menon", "Raj Prakash Gohil", "Kumud Tripathi", "Pankaj Wasnik"], "title": "LASPA: Language Agnostic Speaker Disentanglement with Prefix-Tuned Cross-Attention", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM"], "comment": "Accepted at Interspeech 2025, Netherlands", "summary": "Speaker recognition models face challenges in multi-lingual settings due to\nthe entanglement of linguistic information within speaker embeddings. The\noverlap between vocal traits such as accent, vocal anatomy, and a language's\nphonetic structure complicates separating linguistic and speaker information.\nDisentangling these components can significantly improve speaker recognition\naccuracy. To this end, we propose a novel disentanglement learning strategy\nthat integrates joint learning through prefix-tuned cross-attention. This\napproach is particularly effective when speakers switch between languages.\nExperimental results show the model generalizes across monolingual and\nmulti-lingual settings, including unseen languages. Notably, the proposed model\nimproves the equal error rate across multiple datasets, highlighting its\nability to separate language information from speaker embeddings and enhance\nrecognition in diverse linguistic conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u7f20\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u524d\u7f00\u8c03\u4f18\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u6765\u5206\u79bb\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u8bf4\u8bdd\u4eba\u548c\u8bed\u8a00\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bf4\u8bdd\u4eba\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\uff0c\u8bf4\u8bdd\u4eba\u5d4c\u5165\u4e2d\u8bed\u8a00\u4fe1\u606f\u7684\u6df7\u6742\u5bfc\u81f4\u8bf4\u8bdd\u4eba\u8bc6\u522b\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u89e3\u7f20\u8bed\u8a00\u548c\u8bf4\u8bdd\u4eba\u4fe1\u606f\u4ee5\u63d0\u9ad8\u8bc6\u522b\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u96c6\u6210\u524d\u7f00\u8c03\u4f18\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u8054\u5408\u5b66\u4e60\u7b56\u7565\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8bf4\u8bdd\u4eba\u5728\u591a\u79cd\u8bed\u8a00\u95f4\u5207\u6362\u7684\u573a\u666f\u3002", "result": "\u6a21\u578b\u5728\u5355\u8bed\u548c\u591a\u8bed\u73af\u5883\u4e2d\uff08\u5305\u62ec\u672a\u89c1\u8fc7\u7684\u8bed\u8a00\uff09\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7b49\u9519\u8bef\u7387\u3002", "conclusion": "\u8be5\u6a21\u578b\u6709\u6548\u5206\u79bb\u4e86\u8bed\u8a00\u4fe1\u606f\u4e0e\u8bf4\u8bdd\u4eba\u5d4c\u5165\uff0c\u5728\u591a\u8bed\u8a00\u6761\u4ef6\u4e0b\u63d0\u5347\u4e86\u8bf4\u8bdd\u4eba\u8bc6\u522b\u6027\u80fd\u3002", "keywords": "\u8bf4\u8bdd\u4eba\u8bc6\u522b, \u591a\u8bed\u8a00, \u89e3\u7f20\u5b66\u4e60, \u4ea4\u53c9\u6ce8\u610f\u529b"}}
{"id": "2506.02757", "pdf": "https://arxiv.org/pdf/2506.02757", "abs": "https://arxiv.org/abs/2506.02757", "authors": ["Ruiying Lu", "Jinhan Liu", "Chuan Du", "Dandan Guo"], "title": "Investigating Mask-aware Prototype Learning for Tabular Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages, 11 figures", "summary": "Tabular anomaly detection, which aims at identifying deviant samples, has\nbeen crucial in a variety of real-world applications, such as medical disease\nidentification, financial fraud detection, intrusion monitoring, etc. Although\nrecent deep learning-based methods have achieved competitive performances,\nthese methods suffer from representation entanglement and the lack of global\ncorrelation modeling, which hinders anomaly detection performance. To tackle\nthe problem, we incorporate mask modeling and prototype learning into tabular\nanomaly detection. The core idea is to design learnable masks by disentangled\nrepresentation learning within a projection space and extracting normal\ndependencies as explicit global prototypes. Specifically, the overall model\ninvolves two parts: (i) During encoding, we perform mask modeling in both the\ndata space and projection space with orthogonal basis vectors for learning\nshared disentangled normal patterns; (ii) During decoding, we decode multiple\nmasked representations in parallel for reconstruction and learn association\nprototypes to extract normal characteristic correlations. Our proposal derives\nfrom a distribution-matching perspective, where both projection space learning\nand association prototype learning are formulated as optimal transport\nproblems, and the calibration distances are utilized to refine the anomaly\nscores. Quantitative and qualitative experiments on 20 tabular benchmarks\ndemonstrate the effectiveness and interpretability of our model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u63a9\u7801\u5efa\u6a21\u548c\u539f\u578b\u5b66\u4e60\u7684\u8868\u683c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u8868\u793a\u5b66\u4e60\u548c\u5168\u5c40\u539f\u578b\u63d0\u53d6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u8868\u793a\u7ea0\u7f20\u548c\u5168\u5c40\u76f8\u5173\u6027\u5efa\u6a21\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u63a9\u7801\u5efa\u6a21\u548c\u539f\u578b\u5b66\u4e60\uff0c\u8bbe\u8ba1\u53ef\u5b66\u4e60\u7684\u63a9\u7801\u548c\u5168\u5c40\u539f\u578b\uff0c\u901a\u8fc7\u5206\u5e03\u5339\u914d\u89c6\u89d2\u4f18\u5316\u4f20\u8f93\u95ee\u9898\u3002", "result": "\u572820\u4e2a\u8868\u683c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8868\u683c\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "keywords": "\u8868\u683c\u5f02\u5e38\u68c0\u6d4b, \u63a9\u7801\u5efa\u6a21, \u539f\u578b\u5b66\u4e60, \u89e3\u8026\u8868\u793a, \u5168\u5c40\u76f8\u5173\u6027"}}
{"id": "2506.03035", "pdf": "https://arxiv.org/pdf/2506.03035", "abs": "https://arxiv.org/abs/2506.03035", "authors": ["Pierre Lepagnol", "Sahar Ghannay", "Thomas Gerald", "Christophe Servan", "Sophie Rosset"], "title": "Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Conference paper accepted to INTERSPEECH 2025", "summary": "Understanding user queries is fundamental in many applications, such as home\nassistants, booking systems, or recommendations. Accordingly, it is crucial to\ndevelop accurate Spoken Language Understanding (SLU) approaches to ensure the\nreliability of the considered system. Current State-of-the-Art SLU techniques\nrely on large amounts of training data; however, only limited annotated\nexamples are available for specific tasks or languages.\n  In the meantime, instruction-tuned large language models (LLMs) have shown\nexceptional performance on unseen tasks in a few-shot setting when provided\nwith adequate prompts. In this work, we propose to explore example selection by\nleveraging Information retrieval (IR) approaches to build an enhanced prompt\nthat is applied to an SLU task. We evaluate the effectiveness of the proposed\nmethod on several SLU benchmarks. Experimental results show that lexical IR\nmethods significantly enhance performance without increasing prompt length.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4fe1\u606f\u68c0\u7d22\uff08IR\uff09\u65b9\u6cd5\u9009\u62e9\u793a\u4f8b\uff0c\u589e\u5f3a\u63d0\u793a\u4ee5\u63d0\u5347\u53e3\u8bed\u8bed\u8a00\u7406\u89e3\uff08SLU\uff09\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684SLU\u6280\u672f\u4f9d\u8d56\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u7279\u5b9a\u4efb\u52a1\u6216\u8bed\u8a00\u7684\u6807\u6ce8\u6570\u636e\u6709\u9650\u3002\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5c11\u91cf\u793a\u4f8b\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "method": "\u901a\u8fc7\u4fe1\u606f\u68c0\u7d22\u65b9\u6cd5\u9009\u62e9\u793a\u4f8b\uff0c\u6784\u5efa\u589e\u5f3a\u63d0\u793a\uff0c\u5e94\u7528\u4e8eSLU\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u8bcd\u6c47\u7684IR\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e14\u672a\u589e\u52a0\u63d0\u793a\u957f\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2aSLU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\uff0c\u5c55\u793a\u4e86IR\u5728\u63d0\u5347SLU\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "spoken language understanding, information retrieval, large language models, few-shot learning"}}
{"id": "2506.02085", "pdf": "https://arxiv.org/pdf/2506.02085", "abs": "https://arxiv.org/abs/2506.02085", "authors": ["Ajinkya Kulkarni", "Sandipana Dowerah", "Tanel Alumae", "Mathew Magimai. -Doss"], "title": "Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer Network Approach With Ensemble Fusion", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "Accepted at Interspeech 2025, Netherlands", "summary": "Audio deepfakes are acquiring an unprecedented level of realism with advanced\nAI. While current research focuses on discerning real speech from spoofed\nspeech, tracing the source system is equally crucial. This work proposes a\nnovel audio source tracing system combining deep metric multi-class N-pair loss\nwith Real Emphasis and Fake Dispersion framework, a Conformer classification\nnetwork, and ensemble score-embedding fusion. The N-pair loss improves\ndiscriminative ability, while Real Emphasis and Fake Dispersion enhance\nrobustness by focusing on differentiating real and fake speech patterns. The\nConformer network captures both global and local dependencies in the audio\nsignal, crucial for source tracing. The proposed ensemble score-embedding\nfusion shows an optimal trade-off between in-domain and out-of-domain source\ntracing scenarios. We evaluate our method using Frechet Distance and standard\nmetrics, demonstrating superior performance in source tracing over the baseline\nsystem.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u97f3\u9891\u6e90\u8ffd\u8e2a\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u6df1\u5ea6\u5b66\u4e60\u3001\u591a\u7c7b\u522bN\u5bf9\u635f\u5931\u3001Conformer\u5206\u7c7b\u7f51\u7edc\u548c\u96c6\u6210\u8bc4\u5206\u5d4c\u5165\u878d\u5408\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u6e90\u7684\u8ffd\u8e2a\u80fd\u529b\u3002", "motivation": "\u968f\u7740AI\u6280\u672f\u7684\u53d1\u5c55\uff0c\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u7684\u903c\u771f\u7a0b\u5ea6\u65e5\u76ca\u63d0\u9ad8\u3002\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u533a\u5206\u771f\u5b9e\u4e0e\u4f2a\u9020\u8bed\u97f3\uff0c\u4f46\u8ffd\u8e2a\u97f3\u9891\u6e90\u7684\u7cfb\u7edf\u540c\u6837\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5ea6\u91cf\u591a\u7c7b\u522bN\u5bf9\u635f\u5931\u3001Real Emphasis\u548cFake Dispersion\u6846\u67b6\u3001Conformer\u5206\u7c7b\u7f51\u7edc\u4ee5\u53ca\u96c6\u6210\u8bc4\u5206\u5d4c\u5165\u878d\u5408\u7684\u65b9\u6cd5\u3002", "result": "\u4f7f\u7528Frechet\u8ddd\u79bb\u548c\u6807\u51c6\u6307\u6807\u8bc4\u4f30\uff0c\u663e\u793a\u8be5\u65b9\u6cd5\u7684\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u97f3\u9891\u6e90\u8ffd\u8e2a\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u533a\u5206\u771f\u5b9e\u548c\u4f2a\u9020\u8bed\u97f3\u6a21\u5f0f\u65b9\u9762\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "keywords": "\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020,\u6e90\u8ffd\u8e2a,Conformer\u7f51\u7edc,\u591a\u7c7b\u522bN\u5bf9\u635f\u5931,Real Emphasis,Fake Dispersion"}}
{"id": "2506.02767", "pdf": "https://arxiv.org/pdf/2506.02767", "abs": "https://arxiv.org/abs/2506.02767", "authors": ["Marco Cal\u00ec", "Giulio Giacomuzzo", "Ruggero Carli", "Alberto Dalla Libera"], "title": "Accelerating Model-Based Reinforcement Learning using Non-Linear Trajectory Optimization", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "This paper addresses the slow policy optimization convergence of Monte Carlo\nProbabilistic Inference for Learning Control (MC-PILCO), a state-of-the-art\nmodel-based reinforcement learning (MBRL) algorithm, by integrating it with\niterative Linear Quadratic Regulator (iLQR), a fast trajectory optimization\nmethod suitable for nonlinear systems. The proposed method, Exploration-Boosted\nMC-PILCO (EB-MC-PILCO), leverages iLQR to generate informative, exploratory\ntrajectories and initialize the policy, significantly reducing the number of\nrequired optimization steps. Experiments on the cart-pole task demonstrate that\nEB-MC-PILCO accelerates convergence compared to standard MC-PILCO, achieving up\nto $\\bm{45.9\\%}$ reduction in execution time when both methods solve the task\nin four trials. EB-MC-PILCO also maintains a $\\bm{100\\%}$ success rate across\ntrials while solving the task faster, even in cases where MC-PILCO converges in\nfewer iterations.", "AI": {"tldr": "\u63d0\u51fa\u4e86EB-MC-PILCO\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408iLQR\u52a0\u901fMC-PILCO\u7684\u6536\u655b\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u80fd\u51cf\u5c1145.9%\u7684\u6267\u884c\u65f6\u95f4\uff0c\u5e76\u4fdd\u6301100%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3MC-PILCO\u5728\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6536\u655b\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002", "method": "\u5c06MC-PILCO\u4e0eiLQR\u7ed3\u5408\uff0c\u5229\u7528iLQR\u751f\u6210\u63a2\u7d22\u6027\u8f68\u8ff9\u5e76\u521d\u59cb\u5316\u7b56\u7565\u3002", "result": "\u5728cart-pole\u4efb\u52a1\u4e2d\uff0cEB-MC-PILCO\u6536\u655b\u66f4\u5feb\uff0c\u6267\u884c\u65f6\u95f4\u51cf\u5c1145.9%\uff0c\u6210\u529f\u7387\u4fdd\u6301100%\u3002", "conclusion": "EB-MC-PILCO\u6709\u6548\u52a0\u901f\u4e86MC-PILCO\u7684\u6536\u655b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6210\u529f\u7387\u3002", "keywords": "MC-PILCO, iLQR, \u5f3a\u5316\u5b66\u4e60, \u6536\u655b\u52a0\u901f"}}
{"id": "2506.03038", "pdf": "https://arxiv.org/pdf/2506.03038", "abs": "https://arxiv.org/abs/2506.03038", "authors": ["Jintian Shao", "Yiming Cheng"], "title": "Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) enhances large language models (LLMs) in complex,\nlong-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework,\ndespite sophisticated mechanisms like Decoupled GAE, theoretically faces\nfundamental limitations in comprehensively modeling and leveraging deep,\nlong-term value for fine-grained, step-by-step policy guidance in extended\nreasoning chains. We argue these limitations stem from inherent difficulties in\ncredit assignment, value function representational capacity with temporally\nabstracted goals, and translating global value signals into local policy\nimprovements, especially with sparse rewards. Our theoretical analysis examines\nthese aspects to illuminate VAPO's boundaries in long-term value modeling,\naiming to deepen understanding of current RL for advanced reasoning and suggest\nfuture research for more robust LLM agents.", "AI": {"tldr": "RL\u589e\u5f3aLLMs\u5728\u590d\u6742\u957f\u94fe\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u4f46VAPO\u6846\u67b6\u5728\u5efa\u6a21\u548c\u5229\u7528\u957f\u671f\u4ef7\u503c\u65b9\u9762\u5b58\u5728\u7406\u8bba\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u8ba8VAPO\u6846\u67b6\u5728\u957f\u94fe\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u4fe1\u7528\u5206\u914d\u3001\u4ef7\u503c\u51fd\u6570\u8868\u793a\u80fd\u529b\u548c\u7a00\u758f\u5956\u52b1\u4e0b\u7684\u5168\u5c40\u4fe1\u53f7\u8f6c\u5316\u95ee\u9898\u3002", "method": "\u7406\u8bba\u5206\u6790VAPO\u6846\u67b6\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u4fe1\u7528\u5206\u914d\u548c\u957f\u671f\u4ef7\u503c\u5efa\u6a21\u7684\u6311\u6218\u3002", "result": "\u63ed\u793a\u4e86VAPO\u6846\u67b6\u5728\u957f\u671f\u4ef7\u503c\u5efa\u6a21\u4e2d\u7684\u8fb9\u754c\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b9\u5411\u3002", "conclusion": "VAPO\u6846\u67b6\u5728\u957f\u94fe\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\u9700\u8981\u901a\u8fc7\u65b0\u65b9\u6cd5\u89e3\u51b3\uff0c\u4ee5\u63d0\u5347LLM\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u5927\u8bed\u8a00\u6a21\u578b, \u957f\u94fe\u63a8\u7406, \u4ef7\u503c\u51fd\u6570, \u4fe1\u7528\u5206\u914d"}}
{"id": "2506.02811", "pdf": "https://arxiv.org/pdf/2506.02811", "abs": "https://arxiv.org/abs/2506.02811", "authors": ["Ant\u00f3nio Pedro Pinheiro", "Rita P. Ribeiro"], "title": "CART-based Synthetic Tabular Data Generation for Imbalanced Regression", "categories": ["cs.LG"], "comment": "15 pages, 2 figures, 5 tables, 1 algorithm", "summary": "Handling imbalanced target distributions in regression tasks remains a\nsignificant challenge in tabular data settings where underrepresented regions\ncan hinder model performance. Among data-level solutions, some proposals, such\nas random sampling and SMOTE-based approaches, propose adapting classification\ntechniques to regression tasks. However, these methods typically rely on crisp,\nartificial thresholds over the target variable, a limitation inherited from\nclassification settings that can introduce arbitrariness, often leading to\nnon-intuitive and potentially misleading problem formulations. While recent\ngenerative models, such as GANs and VAEs, provide flexible sample synthesis,\nthey come with high computational costs and limited interpretability. In this\nstudy, we propose adapting an existing CART-based synthetic data generation\nmethod, tailoring it for imbalanced regression. The new method integrates\nrelevance and density-based mechanisms to guide sampling in sparse regions of\nthe target space and employs a threshold-free, feature-driven generation\nprocess. Our experimental study focuses on the prediction of extreme target\nvalues across benchmark datasets. The results indicate that the proposed method\nis competitive with other resampling and generative strategies in terms of\nperformance, while offering faster execution and greater transparency. These\nresults highlight the method's potential as a transparent, scalable data-level\nstrategy for improving regression models in imbalanced domains.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eCART\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u56de\u5f52\u4efb\u52a1\u4e2d\u76ee\u6807\u5206\u5e03\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u907f\u514d\u4eba\u5de5\u9608\u503c\uff0c\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u56de\u5f52\u4efb\u52a1\u4e2d\u76ee\u6807\u5206\u5e03\u4e0d\u5e73\u8861\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u968f\u673a\u91c7\u6837\u548cSMOTE\u4f9d\u8d56\u4eba\u5de5\u9608\u503c\uff0c\u800c\u751f\u6210\u6a21\u578b\u5982GANs\u548cVAEs\u5219\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u53ef\u89e3\u91ca\u6027\u5dee\u3002", "method": "\u91c7\u7528\u57fa\u4e8eCART\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u76f8\u5173\u6027\u548c\u5bc6\u5ea6\u673a\u5236\uff0c\u65e0\u9608\u503c\u5730\u751f\u6210\u7a00\u758f\u533a\u57df\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0e\u73b0\u6709\u91cd\u91c7\u6837\u548c\u751f\u6210\u7b56\u7565\u76f8\u5f53\uff0c\u4f46\u6267\u884c\u66f4\u5feb\u4e14\u66f4\u900f\u660e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u662f\u4e00\u79cd\u900f\u660e\u3001\u53ef\u6269\u5c55\u7684\u6570\u636e\u7ea7\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u4e0d\u5e73\u8861\u56de\u5f52\u4efb\u52a1\u3002", "keywords": "\u4e0d\u5e73\u8861\u56de\u5f52, CART, \u6570\u636e\u751f\u6210, \u900f\u660e\u5ea6, \u53ef\u6269\u5c55\u6027"}}
{"id": "2506.03051", "pdf": "https://arxiv.org/pdf/2506.03051", "abs": "https://arxiv.org/abs/2506.03051", "authors": ["Yuval Kansal", "Shmuel Berman", "Lydia Liu"], "title": "Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Factuality is a necessary precursor to useful educational tools. As adoption\nof Large Language Models (LLMs) in education continues of grow, ensuring\ncorrectness in all settings is paramount. Despite their strong English\ncapabilities, LLM performance in other languages is largely untested. In this\nwork, we evaluate the correctness of the Llama3.1 family of models in answering\nfactual questions appropriate for middle and high school students. We\ndemonstrate that LLMs not only provide extraneous and less truthful\ninformation, but also exacerbate existing biases against rare languages.", "AI": {"tldr": "\u8bc4\u4f30Llama3.1\u6a21\u578b\u5728\u4e2d\u9ad8\u4e2d\u5b66\u751f\u4e8b\u5b9e\u95ee\u9898\u56de\u7b54\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u4e0d\u4ec5\u63d0\u4f9b\u591a\u4f59\u548c\u4e0d\u51c6\u786e\u4fe1\u606f\uff0c\u8fd8\u52a0\u5267\u4e86\u5bf9\u7a00\u6709\u8bed\u8a00\u7684\u504f\u89c1\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u786e\u4fdd\u5176\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u56de\u7b54\u4e8b\u5b9e\u95ee\u9898\u7684\u6b63\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002\u76ee\u524d\u5bf9\u975e\u82f1\u8bed\u8bed\u8a00\u7684\u6027\u80fd\u6d4b\u8bd5\u8f83\u5c11\u3002", "method": "\u8bc4\u4f30Llama3.1\u6a21\u578b\u5728\u4e2d\u9ad8\u4e2d\u5b66\u751f\u6559\u80b2\u76f8\u5173\u4e8b\u5b9e\u95ee\u9898\u4e2d\u7684\u8868\u73b0\u3002", "result": "LLMs\u4e0d\u4ec5\u63d0\u4f9b\u591a\u4f59\u4e14\u4e0d\u51c6\u786e\u7684\u4fe1\u606f\uff0c\u8fd8\u52a0\u5267\u4e86\u5bf9\u7a00\u6709\u8bed\u8a00\u7684\u504f\u89c1\u3002", "conclusion": "LLMs\u5728\u6559\u80b2\u5e94\u7528\u4e2d\u7684\u4e8b\u5b9e\u6027\u95ee\u9898\u4ecd\u9700\u6539\u8fdb\uff0c\u5c24\u5176\u662f\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6559\u80b2\u5de5\u5177\uff0c\u4e8b\u5b9e\u6027\uff0c\u591a\u8bed\u8a00\u6027\u80fd\uff0c\u504f\u89c1"}}
{"id": "2506.02090", "pdf": "https://arxiv.org/pdf/2506.02090", "abs": "https://arxiv.org/abs/2506.02090", "authors": ["Gopichand Bandarupalli"], "title": "The Impact of Software Testing with Quantum Optimization Meets Machine Learning", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": "6 pages", "summary": "Modern software systems complexity challenges efficient testing, as\ntraditional machine learning (ML) struggles with large test suites. This\nresearch presents a hybrid framework integrating Quantum Annealing with ML to\noptimize test case prioritization in CI/CD pipelines. Leveraging quantum\noptimization, it achieves a 25 percent increase in defect detection efficiency\nand a 30 percent reduction in test execution time versus classical ML,\nvalidated on the Defects4J dataset. A simulated CI/CD environment demonstrates\nrobustness across evolving codebases. Visualizations, including defect heatmaps\nand performance graphs, enhance interpretability. The framework addresses\nquantum hardware limits, CI/CD integration, and scalability for 2025s hybrid\nquantum-classical ecosystems, offering a transformative approach to software\nquality assurance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u91cf\u5b50\u9000\u706b\u4e0e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7684\u6df7\u5408\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86CI/CD\u6d41\u6c34\u7ebf\u4e2d\u7684\u6d4b\u8bd5\u7528\u4f8b\u4f18\u5148\u7ea7\u6392\u5e8f\u6548\u7387\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u590d\u6742\u6027\u9ad8\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u6d4b\u8bd5\u5957\u4ef6\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u6d4b\u8bd5\u4f18\u5316\u65b9\u6848\u3002", "method": "\u91c7\u7528\u91cf\u5b50\u9000\u706b\u4e0e\u673a\u5668\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5b50\u4f18\u5316\u6280\u672f\u4f18\u5316\u6d4b\u8bd5\u7528\u4f8b\u4f18\u5148\u7ea7\u3002", "result": "\u5728Defects4J\u6570\u636e\u96c6\u4e0a\uff0c\u7f3a\u9677\u68c0\u6d4b\u6548\u7387\u63d0\u534725%\uff0c\u6d4b\u8bd5\u6267\u884c\u65f6\u95f4\u51cf\u5c1130%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u8f6f\u4ef6\u7cfb\u7edf\u7684\u8d28\u91cf\u4fdd\u969c\u63d0\u4f9b\u4e86\u7a81\u7834\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u751f\u6001\u7cfb\u7edf\u7684\u5e94\u7528\u524d\u666f\u3002", "keywords": "\u91cf\u5b50\u9000\u706b, \u673a\u5668\u5b66\u4e60, CI/CD, \u6d4b\u8bd5\u4f18\u5316, \u7f3a\u9677\u68c0\u6d4b"}}
{"id": "2506.02842", "pdf": "https://arxiv.org/pdf/2506.02842", "abs": "https://arxiv.org/abs/2506.02842", "authors": ["Stefano Fiorini", "Hakan Aktas", "Iulia Duta", "Stefano Coniglio", "Pietro Morerio", "Alessio Del Bue", "Pietro Li\u00f2"], "title": "Sheaves Reloaded: A Directional Awakening", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Sheaf Neural Networks (SNNs) represent a powerful generalization of Graph\nNeural Networks (GNNs) that significantly improve our ability to model complex\nrelational data. While directionality has been shown to substantially boost\nperformance in graph learning tasks and is key to many real-world applications,\nexisting SNNs fall short in representing it. To address this limitation, we\nintroduce the Directed Cellular Sheaf, a special type of cellular sheaf\ndesigned to explicitly account for edge orientation. Building on this\nstructure, we define a new sheaf Laplacian, the Directed Sheaf Laplacian, which\ncaptures both the graph's topology and its directional information. This\noperator serves as the backbone of the Directed Sheaf Neural Network (DSNN),\nthe first SNN model to embed a directional bias into its architecture.\nExtensive experiments on nine real-world benchmarks show that DSNN consistently\noutperforms baseline methods.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86Directed Sheaf Neural Networks (DSNN)\uff0c\u4e00\u79cd\u65b0\u578bSheaf Neural Networks (SNNs)\uff0c\u901a\u8fc7\u5f15\u5165Directed Cellular Sheaf\u548cDirected Sheaf Laplacian\u6765\u663e\u5f0f\u5efa\u6a21\u65b9\u5411\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u5173\u7cfb\u6570\u636e\u7684\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u65b9\u5411\u6027\u5728\u56fe\u5b66\u4e60\u4efb\u52a1\u4e2d\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u73b0\u6709SNNs\u65e0\u6cd5\u5f88\u597d\u5730\u5efa\u6a21\u65b9\u5411\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u63d0\u51faDirected Cellular Sheaf\u548cDirected Sheaf Laplacian\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efaDSNN\uff0c\u9996\u6b21\u5728SNN\u67b6\u6784\u4e2d\u5d4c\u5165\u65b9\u5411\u6027\u504f\u7f6e\u3002", "result": "\u5728\u4e5d\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDSNN\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DSNN\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u65b9\u5411\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86SNNs\u5728\u590d\u6742\u5173\u7cfb\u6570\u636e\u4e0a\u7684\u6027\u80fd\uff0c\u6269\u5c55\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "keywords": "Sheaf Neural Networks, Directed Cellular Sheaf, Sheaf Laplacian, Graph Learning"}}
{"id": "2506.03090", "pdf": "https://arxiv.org/pdf/2506.03090", "abs": "https://arxiv.org/abs/2506.03090", "authors": ["Katherine Thai", "Mohit Iyyer"], "title": "Literary Evidence Retrieval via Long-Context Language Models", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "How well do modern long-context language models understand literary fiction?\nWe explore this question via the task of literary evidence retrieval,\nrepurposing the RELiC dataset of That et al. (2022) to construct a benchmark\nwhere the entire text of a primary source (e.g., The Great Gatsby) is provided\nto an LLM alongside literary criticism with a missing quotation from that work.\nThis setting, in which the model must generate the missing quotation, mirrors\nthe human process of literary analysis by requiring models to perform both\nglobal narrative reasoning and close textual examination. We curate a\nhigh-quality subset of 292 examples through extensive filtering and human\nverification. Our experiments show that recent reasoning models, such as Gemini\nPro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In\ncontrast, the best open-weight model achieves only 29.1% accuracy, highlighting\na wide gap in interpretive reasoning between open and closed-weight models.\nDespite their speed and apparent accuracy, even the strongest models struggle\nwith nuanced literary signals and overgeneration, signaling open challenges for\napplying LLMs to literary analysis. We release our dataset and evaluation code\nto encourage future work in this direction.", "AI": {"tldr": "\u73b0\u4ee3\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5b66\u8bc1\u636e\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u5982\u4f55\uff1f\u7814\u7a76\u4f7f\u7528RELiC\u6570\u636e\u96c6\u6784\u5efa\u57fa\u51c6\uff0c\u53d1\u73b0\u5c01\u95ed\u6743\u91cd\u6a21\u578b\uff08\u5982Gemini Pro 2.5\uff09\u8d85\u8fc7\u4eba\u7c7b\u4e13\u5bb6\u8868\u73b0\uff0c\u800c\u5f00\u653e\u6743\u91cd\u6a21\u578b\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u63a2\u7d22\u73b0\u4ee3\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u5bf9\u6587\u5b66\u5c0f\u8bf4\u7684\u7406\u89e3\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6587\u5b66\u8bc1\u636e\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528RELiC\u6570\u636e\u96c6\u6784\u5efa\u57fa\u51c6\uff0c\u8981\u6c42\u6a21\u578b\u5728\u5168\u6587\u672c\u73af\u5883\u4e0b\u751f\u6210\u7f3a\u5931\u7684\u6587\u5b66\u5f15\u7528\uff0c\u7ed3\u5408\u5168\u5c40\u53d9\u4e8b\u63a8\u7406\u548c\u7ec6\u8282\u6587\u672c\u5206\u6790\u3002", "result": "Gemini Pro 2.5\u4ee562.5%\u7684\u51c6\u786e\u7387\u8d85\u8fc7\u4eba\u7c7b\u4e13\u5bb6\uff0850%\uff09\uff0c\u800c\u6700\u4f73\u5f00\u653e\u6743\u91cd\u6a21\u578b\u4ec5\u8fbe\u523029.1%\u3002", "conclusion": "\u6a21\u578b\u5728\u6587\u5b66\u5206\u6790\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5982\u5bf9\u7ec6\u5fae\u6587\u5b66\u4fe1\u53f7\u7684\u8bc6\u522b\u548c\u8fc7\u5ea6\u751f\u6210\u95ee\u9898\u3002", "keywords": "\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\uff1b\u6587\u5b66\u8bc1\u636e\u68c0\u7d22\uff1bRELiC\u6570\u636e\u96c6\uff1bGemini Pro\uff1b\u5f00\u653e\u6743\u91cd\u6a21\u578b"}}
{"id": "2506.02864", "pdf": "https://arxiv.org/pdf/2506.02864", "abs": "https://arxiv.org/abs/2506.02864", "authors": ["Changyi Xiao", "Mengdi Zhang", "Yixin Cao"], "title": "BNPO: Beta Normalization Policy Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent studies, including DeepSeek-R1 and Kimi-k1.5, have demonstrated that\nreinforcement learning with rule-based, binary-valued reward functions can\nsignificantly enhance the reasoning capabilities of large language models.\nThese models primarily utilize REINFORCE-based policy optimization techniques,\nsuch as REINFORCE with baseline and group relative policy optimization (GRPO).\nHowever, a key limitation remains: current policy optimization methods either\nneglect reward normalization or employ static normalization strategies, which\nfail to adapt to the dynamic nature of policy updates during training. This may\nresult in unstable gradient estimates and hinder training stability. To address\nthis issue, we propose Beta Normalization Policy Optimization (BNPO), a novel\npolicy optimization method that adaptively normalizes rewards using a Beta\ndistribution with dynamically updated parameters. BNPO aligns the normalization\nwith the changing policy distribution, enabling more precise and lower-variance\ngradient estimation, which in turn promotes stable training dynamics. We\nprovide theoretical analysis demonstrating BNPO's variance-reducing properties\nand show that it generalizes both REINFORCE and GRPO under binary-valued reward\nsettings. Furthermore, we introduce an advantage decomposition mechanism to\nextend BNPO's applicability to more complex reward systems. Experimental\nresults confirm that BNPO achieves state-of-the-art performance among policy\noptimization methods on reasoning tasks. The code is available at\nhttps://github.com/changyi7231/BNPO.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5 Beta Normalization Policy Optimization (BNPO)\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u7684 Beta \u5206\u5e03\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u5956\u52b1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u9759\u6001\u5f52\u4e00\u5316\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4e8c\u8fdb\u5236\u5956\u52b1\u573a\u666f\u4e0b\u5b58\u5728\u5f52\u4e00\u5316\u7b56\u7565\u4e0d\u9002\u5e94\u7b56\u7565\u66f4\u65b0\u7684\u52a8\u6001\u7279\u6027\uff0c\u5bfc\u81f4\u68af\u5ea6\u4f30\u8ba1\u4e0d\u7a33\u5b9a\u3002", "method": "BNPO \u65b9\u6cd5\u5229\u7528\u52a8\u6001\u66f4\u65b0\u7684 Beta \u5206\u5e03\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u5956\u52b1\uff0c\u5e76\u7ed3\u5408\u4f18\u52bf\u5206\u89e3\u673a\u5236\u6269\u5c55\u5176\u9002\u7528\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e BNPO \u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u964d\u4f4e\u65b9\u5dee\u7684\u7279\u6027\u3002", "conclusion": "BNPO \u901a\u8fc7\u52a8\u6001\u5f52\u4e00\u5316\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u6269\u5c55\u4e86\u7b56\u7565\u4f18\u5316\u7684\u9002\u7528\u6027\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u7b56\u7565\u4f18\u5316, \u5956\u52b1\u5f52\u4e00\u5316, Beta \u5206\u5e03, BNPO"}}
{"id": "2506.03101", "pdf": "https://arxiv.org/pdf/2506.03101", "abs": "https://arxiv.org/abs/2506.03101", "authors": ["Jonas F. Lotz", "Ant\u00f3nio V. Lopes", "Stephan Peitz", "Hendra Setiawan", "Leonardo Emili"], "title": "Beyond Text Compression: Evaluating Tokenizers Across Scales", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "The choice of tokenizer can profoundly impact language model performance, yet\naccessible and reliable evaluations of tokenizer quality remain an open\nchallenge. Inspired by scaling consistency, we show that smaller models can\naccurately predict significant differences in tokenizer impact on larger models\nat a fraction of the compute cost. By systematically evaluating both\nEnglish-centric and multilingual tokenizers, we find that tokenizer choice has\nnegligible effects on tasks in English but results in consistent performance\ndifferences in multilingual settings. We propose new intrinsic tokenizer\nmetrics inspired by Zipf's law that correlate more strongly with downstream\nperformance than text compression when modeling unseen languages. By combining\nseveral metrics to capture multiple aspects of tokenizer behavior, we develop a\nreliable framework for intrinsic tokenizer evaluations. Our work offers a more\nefficient path to informed tokenizer selection in future language model\ndevelopment.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5206\u8bcd\u5668\u9009\u62e9\u5bf9\u5c0f\u6a21\u578b\u7684\u6027\u80fd\u9884\u6d4b\u6709\u6548\uff0c\u4f46\u5bf9\u591a\u8bed\u8a00\u4efb\u52a1\u5f71\u54cd\u8f83\u5927\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u89e3\u51b3\u5206\u8bcd\u5668\u9009\u62e9\u5bf9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u8bc4\u4f30\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5c0f\u6a21\u578b\u9884\u6d4b\u5206\u8bcd\u5668\u5bf9\u5927\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u5e76\u8bc4\u4f30\u591a\u8bed\u8a00\u5206\u8bcd\u5668\u6027\u80fd\uff0c\u63d0\u51fa\u57fa\u4e8eZipf\u5b9a\u5f8b\u7684\u65b0\u6307\u6807\u3002", "result": "\u5206\u8bcd\u5668\u5bf9\u82f1\u8bed\u4efb\u52a1\u5f71\u54cd\u5c0f\uff0c\u4f46\u5bf9\u591a\u8bed\u8a00\u4efb\u52a1\u5f71\u54cd\u663e\u8457\uff1b\u65b0\u6307\u6807\u4f18\u4e8e\u4f20\u7edf\u538b\u7f29\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u9ad8\u6548\u7684\u5206\u8bcd\u5668\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u6307\u5bfc\u3002", "keywords": "\u5206\u8bcd\u5668, \u8bed\u8a00\u6a21\u578b, \u591a\u8bed\u8a00\u8bc4\u4f30, Zipf\u5b9a\u5f8b"}}
{"id": "2506.02120", "pdf": "https://arxiv.org/pdf/2506.02120", "abs": "https://arxiv.org/abs/2506.02120", "authors": ["Mariana A. Londe", "Luciana S. Pessoa", "Carlos E. Andrade", "Jos\u00e9 F. Gon\u00e7alves", "Mauricio G. C. Resende"], "title": "Random-key genetic algorithms", "categories": ["cs.NE", "cs.AI", "math.OC", "90-02, 90B40, 90C27", "G.1.6; G.2.1; I.2.8"], "comment": "21 pages, 1 figure, 1 table, 1 algorithm, forthcoming in Handbook of\n  Heuristics, 2nd edition, SpringerNature, New York", "summary": "A random-key genetic algorithm is an evolutionary metaheuristic for discrete\nand global optimization. Each solution is encoded as a vector of N random keys,\nwhere a random key is a real number randomly generated in the continuous\ninterval [0, 1). A decoder maps each vector of random keys to a solution of the\noptimization problem being solved and computes its cost. The benefit of this\napproach is that all genetic operators and transformations can be maintained\nwithin the unitary hypercube, regardless of the problem being addressed. This\nenhances the productivity and maintainability of the core framework. The\nalgorithm starts with a population of P vectors of random keys. At each\niteration, the vectors are partitioned into two sets: a smaller set of\nhigh-valued elite solutions and the remaining non-elite solutions. All elite\nelements are copied, without change, to the next population. A small number of\nrandom-key vectors (the mutants) is added to the population of the next\niteration. The remaining elements of the population of the next iteration are\ngenerated by combining, with the parametrized uniform crossover of Spears and\nDeJong (1991), pairs of solutions. This chapter reviews random-key genetic\nalgorithms and describes an effective variant called biased random-key genetic\nalgorithms.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u968f\u673a\u952e\u9057\u4f20\u7b97\u6cd5\u53ca\u5176\u53d8\u79cd\uff0c\u7528\u4e8e\u79bb\u6563\u548c\u5168\u5c40\u4f18\u5316\u95ee\u9898\u3002\u901a\u8fc7\u7f16\u7801\u89e3\u4e3a\u968f\u673a\u952e\u5411\u91cf\uff0c\u5e76\u5728\u8d85\u7acb\u65b9\u4f53\u5185\u8fdb\u884c\u9057\u4f20\u64cd\u4f5c\uff0c\u63d0\u9ad8\u4e86\u7b97\u6cd5\u7684\u6548\u7387\u548c\u53ef\u7ef4\u62a4\u6027\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u79bb\u6563\u548c\u5168\u5c40\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u968f\u673a\u952e\u7f16\u7801\u548c\u9057\u4f20\u7b97\u6cd5\u7ed3\u5408\uff0c\u63d0\u5347\u4f18\u5316\u6548\u7387\u548c\u6846\u67b6\u7684\u901a\u7528\u6027\u3002", "method": "\u4f7f\u7528\u968f\u673a\u952e\u5411\u91cf\u7f16\u7801\u89e3\uff0c\u901a\u8fc7\u89e3\u7801\u5668\u6620\u5c04\u5230\u95ee\u9898\u89e3\u5e76\u8ba1\u7b97\u6210\u672c\uff0c\u7ed3\u5408\u7cbe\u82f1\u4fdd\u7559\u3001\u53d8\u5f02\u548c\u4ea4\u53c9\u64cd\u4f5c\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8d85\u7acb\u65b9\u4f53\u5185\u7ef4\u62a4\u6240\u6709\u9057\u4f20\u64cd\u4f5c\uff0c\u589e\u5f3a\u4e86\u6846\u67b6\u7684\u751f\u4ea7\u529b\u548c\u53ef\u7ef4\u62a4\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4f18\u5316\u95ee\u9898\u3002", "conclusion": "\u968f\u673a\u952e\u9057\u4f20\u7b97\u6cd5\u53ca\u5176\u53d8\u79cd\u5728\u79bb\u6563\u548c\u5168\u5c40\u4f18\u5316\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u548c\u7075\u6d3b\u6027\u3002", "keywords": "\u968f\u673a\u952e\u9057\u4f20\u7b97\u6cd5\u3001\u79bb\u6563\u4f18\u5316\u3001\u5168\u5c40\u4f18\u5316\u3001\u8d85\u7acb\u65b9\u4f53\u3001\u7cbe\u82f1\u4fdd\u7559"}}
{"id": "2506.02883", "pdf": "https://arxiv.org/pdf/2506.02883", "abs": "https://arxiv.org/abs/2506.02883", "authors": ["Anthony Kobanda", "Odalric-Ambrym Maillard", "R\u00e9my Portelas"], "title": "A Continual Offline Reinforcement Learning Benchmark for Navigation Tasks", "categories": ["cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2412.14865", "summary": "Autonomous agents operating in domains such as robotics or video game\nsimulations must adapt to changing tasks without forgetting about the previous\nones. This process called Continual Reinforcement Learning poses non-trivial\ndifficulties, from preventing catastrophic forgetting to ensuring the\nscalability of the approaches considered. Building on recent advances, we\nintroduce a benchmark providing a suite of video-game navigation scenarios,\nthus filling a gap in the literature and capturing key challenges :\ncatastrophic forgetting, task adaptation, and memory efficiency. We define a\nset of various tasks and datasets, evaluation protocols, and metrics to assess\nthe performance of algorithms, including state-of-the-art baselines. Our\nbenchmark is designed not only to foster reproducible research and to\naccelerate progress in continual reinforcement learning for gaming, but also to\nprovide a reproducible framework for production pipelines -- helping\npractitioners to identify and to apply effective approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u89c6\u9891\u6e38\u620f\u5bfc\u822a\u573a\u666f\uff0c\u4ee5\u89e3\u51b3\u9057\u5fd8\u3001\u4efb\u52a1\u9002\u5e94\u548c\u5185\u5b58\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9057\u5fd8\u3001\u4efb\u52a1\u9002\u5e94\u548c\u5185\u5b58\u6548\u7387\u6311\u6218\uff0c\u586b\u8865\u6587\u732e\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u5305\u542b\u591a\u6837\u5316\u4efb\u52a1\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u534f\u8bae\u548c\u6307\u6807\u7684\u57fa\u51c6\uff0c\u5e76\u5e94\u7528\u73b0\u6709\u5148\u8fdb\u57fa\u7ebf\u7b97\u6cd5\u3002", "result": "\u57fa\u51c6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u7814\u7a76\u6846\u67b6\uff0c\u52a0\u901f\u6e38\u620f\u9886\u57df\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u7684\u8fdb\u5c55\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e0d\u4ec5\u4fc3\u8fdb\u53ef\u590d\u73b0\u7814\u7a76\uff0c\u8fd8\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u3001\u89c6\u9891\u6e38\u620f\u5bfc\u822a\u3001\u9057\u5fd8\u3001\u4efb\u52a1\u9002\u5e94\u3001\u5185\u5b58\u6548\u7387"}}
{"id": "2506.03106", "pdf": "https://arxiv.org/pdf/2506.03106", "abs": "https://arxiv.org/abs/2506.03106", "authors": ["Xiaoying Zhang", "Hao Sun", "Yipeng Zhang", "Kaituo Feng", "Chao Yang", "Helen Meng"], "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback", "categories": ["cs.CL", "cs.AI"], "comment": "38 pages", "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u548c\u6570\u503c\u53cd\u9988\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6Critique-GRPO\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u4ec5\u4f9d\u9760\u6570\u503c\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u5b58\u5728\u6027\u80fd\u74f6\u9888\u3001\u81ea\u6211\u53cd\u601d\u6548\u679c\u6709\u9650\u548c\u6301\u7eed\u5931\u8d25\u7b49\u95ee\u9898\uff0c\u56e0\u6b64\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86Critique-GRPO\u6846\u67b6\uff0c\u6574\u5408\u81ea\u7136\u8bed\u8a00\u548c\u6570\u503c\u53cd\u9988\uff0c\u4f7f\u6a21\u578b\u80fd\u540c\u65f6\u4ece\u521d\u59cb\u54cd\u5e94\u548c\u57fa\u4e8e\u6279\u8bc4\u7684\u6539\u8fdb\u4e2d\u5b66\u4e60\uff0c\u540c\u65f6\u4fdd\u6301\u63a2\u7d22\u6027\u3002", "result": "\u5728\u591a\u4e2a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cCritique-GRPO\u4f18\u4e8e\u76d1\u7763\u5b66\u4e60\u548cRL\u5fae\u8c03\u65b9\u6cd5\uff0c\u5e73\u5747pass@1\u5206\u6570\u5206\u522b\u63d0\u5347\u4e864.5%\u548c5%\u3002", "conclusion": "\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u80fd\u6709\u6548\u63d0\u5347RL\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e14\u63a2\u7d22\u7b56\u7565\u4e2d\u9ad8\u71b5\u548c\u957f\u54cd\u5e94\u672a\u5fc5\u80fd\u5e26\u6765\u66f4\u9ad8\u6548\u7684\u5b66\u4e60\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u81ea\u7136\u8bed\u8a00\u53cd\u9988, \u8bed\u8a00\u6a21\u578b, \u6027\u80fd\u4f18\u5316, \u63a2\u7d22\u7b56\u7565"}}
{"id": "2506.02150", "pdf": "https://arxiv.org/pdf/2506.02150", "abs": "https://arxiv.org/abs/2506.02150", "authors": ["Stefano Fogarollo", "Gregor Laimer", "Reto Bale", "Matthias Harders"], "title": "Implicit Deformable Medical Image Registration with Learnable Kernels", "categories": ["cs.CV", "cs.AI"], "comment": "MICCAI 2025 Provisional Accept", "summary": "Deformable medical image registration is an essential task in\ncomputer-assisted interventions. This problem is particularly relevant to\noncological treatments, where precise image alignment is necessary for tracking\ntumor growth, assessing treatment response, and ensuring accurate delivery of\ntherapies. Recent AI methods can outperform traditional techniques in accuracy\nand speed, yet they often produce unreliable deformations that limit their\nclinical adoption. In this work, we address this challenge and introduce a\nnovel implicit registration framework that can predict accurate and reliable\ndeformations. Our insight is to reformulate image registration as a signal\nreconstruction problem: we learn a kernel function that can recover the dense\ndisplacement field from sparse keypoint correspondences. We integrate our\nmethod in a novel hierarchical architecture, and estimate the displacement\nfield in a coarse-to-fine manner. Our formulation also allows for efficient\nrefinement at test time, permitting clinicians to easily adjust registrations\nwhen needed. We validate our method on challenging intra-patient thoracic and\nabdominal zero-shot registration tasks, using public and internal datasets from\nthe local University Hospital. Our method not only shows competitive accuracy\nto state-of-the-art approaches, but also bridges the generalization gap between\nimplicit and explicit registration techniques. In particular, our method\ngenerates deformations that better preserve anatomical relationships and\nmatches the performance of specialized commercial systems, underscoring its\npotential for clinical adoption.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u9690\u5f0f\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u5173\u952e\u70b9\u5bf9\u5e94\u5173\u7cfb\u9884\u6d4b\u7cbe\u786e\u4e14\u53ef\u9760\u7684\u53d8\u5f62\u573a\uff0c\u5177\u6709\u4e34\u5e8a\u6f5c\u529b\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u5728\u8ba1\u7b97\u673a\u8f85\u52a9\u5e72\u9884\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u80bf\u7624\u6cbb\u7597\u4e2d\u9700\u8981\u7cbe\u786e\u7684\u56fe\u50cf\u5bf9\u9f50\u3002\u5f53\u524dAI\u65b9\u6cd5\u867d\u7136\u901f\u5ea6\u5feb\u3001\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u53d8\u5f62\u4e0d\u53ef\u9760\uff0c\u963b\u788d\u4e86\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u5c06\u56fe\u50cf\u914d\u51c6\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4fe1\u53f7\u91cd\u5efa\u95ee\u9898\uff0c\u901a\u8fc7\u5b66\u4e60\u6838\u51fd\u6570\u4ece\u7a00\u758f\u5173\u952e\u70b9\u5bf9\u5e94\u5173\u7cfb\u4e2d\u6062\u590d\u5bc6\u96c6\u4f4d\u79fb\u573a\u3002\u91c7\u7528\u5206\u5c42\u67b6\u6784\u8fdb\u884c\u7531\u7c97\u5230\u7ec6\u7684\u4f4d\u79fb\u573a\u4f30\u8ba1\uff0c\u5e76\u5141\u8bb8\u5728\u6d4b\u8bd5\u65f6\u9ad8\u6548\u8c03\u6574\u3002", "result": "\u5728\u80f8\u90e8\u548c\u8179\u90e8\u7684\u96f6\u6837\u672c\u914d\u51c6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u4e0d\u4ec5\u7cbe\u5ea6\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\uff0c\u8fd8\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u5173\u7cfb\uff0c\u4e0e\u4e13\u4e1a\u5546\u4e1a\u7cfb\u7edf\u6027\u80fd\u5339\u914d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u586b\u8865\u4e86\u9690\u5f0f\u548c\u663e\u5f0f\u914d\u51c6\u6280\u672f\u4e4b\u95f4\u7684\u6cdb\u5316\u5dee\u8ddd\uff0c\u751f\u6210\u7684\u53d8\u5f62\u66f4\u53ef\u9760\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u4e34\u5e8a\u63a8\u5e7f\u6f5c\u529b\u3002", "keywords": "\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u3001\u9690\u5f0f\u914d\u51c6\u3001\u7a00\u758f\u5173\u952e\u70b9\u3001\u4f4d\u79fb\u573a\u3001\u80bf\u7624\u6cbb\u7597"}}
{"id": "2506.02887", "pdf": "https://arxiv.org/pdf/2506.02887", "abs": "https://arxiv.org/abs/2506.02887", "authors": ["Mrinmay Sen", "Shruti Aparna", "Rohit Agarwal", "Chalavadi Krishna Mohan"], "title": "Overcoming Challenges of Partial Client Participation in Federated Learning : A Comprehensive Review", "categories": ["cs.LG", "cs.DC"], "comment": "15 pages, 6 tables, comprehensive survey of federated learning with\n  partial client participation", "summary": "Federated Learning (FL) is a learning mechanism that falls under the\ndistributed training umbrella, which collaboratively trains a shared global\nmodel without disclosing the raw data from different clients. This paper\npresents an extensive survey on the impact of partial client participation in\nfederated learning. While much of the existing research focuses on addressing\nissues such as generalization, robustness, and fairness caused by data\nheterogeneity under the assumption of full client participation, limited\nattention has been given to the practical and theoretical challenges arising\nfrom partial client participation, which is common in real-world scenarios.\nThis survey provides an in-depth review of existing FL methods designed to cope\nwith partial client participation. We offer a comprehensive analysis supported\nby theoretical insights and empirical findings, along with a structured\ncategorization of these methods, highlighting their respective advantages and\ndisadvantages.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u90e8\u5206\u5ba2\u6237\u7aef\u53c2\u4e0e\u7684\u5f71\u54cd\uff0c\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6cd5\u53ca\u5176\u4f18\u7f3a\u70b9\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u5b66\u4e60\u7814\u7a76\u591a\u5047\u8bbe\u5ba2\u6237\u7aef\u5b8c\u5168\u53c2\u4e0e\uff0c\u800c\u5ffd\u7565\u4e86\u5b9e\u9645\u573a\u666f\u4e2d\u90e8\u5206\u5ba2\u6237\u7aef\u53c2\u4e0e\u7684\u6311\u6218\u3002", "method": "\u5bf9\u73b0\u6709\u5e94\u5bf9\u90e8\u5206\u5ba2\u6237\u7aef\u53c2\u4e0e\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\u548c\u5206\u6790\u3002", "result": "\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8bc1\u652f\u6301\u7684\u7efc\u5408\u5206\u6790\uff0c\u7a81\u51fa\u4e86\u5404\u79cd\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u90e8\u5206\u5ba2\u6237\u7aef\u53c2\u4e0e\u662f\u8054\u90a6\u5b66\u4e60\u4e2d\u9700\u8981\u91cd\u70b9\u5173\u6ce8\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4ecd\u9700\u6539\u8fdb\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60\u3001\u90e8\u5206\u5ba2\u6237\u7aef\u53c2\u4e0e\u3001\u6570\u636e\u5f02\u8d28\u6027\u3001\u5206\u5e03\u5f0f\u8bad\u7ec3"}}
{"id": "2506.03122", "pdf": "https://arxiv.org/pdf/2506.03122", "abs": "https://arxiv.org/abs/2506.03122", "authors": ["Prashanth Vijayaraghavan", "Luyao Shi", "Ehsan Degan", "Vandana Mukherjee", "Xin Zhang"], "title": "AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation", "categories": ["cs.CL"], "comment": "9 Pages (Content), 4 Pages (Appendix), 7 figures, ICML'2025", "summary": "Analog circuit topology synthesis is integral to Electronic Design Automation\n(EDA), enabling the automated creation of circuit structures tailored to\nspecific design requirements. However, the vast design search space and strict\nconstraint adherence make efficient synthesis challenging. Leveraging the\nversatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel\nreinforcement learning (RL)-based framework for automated analog circuit\nsynthesis. The framework operates in two phases: instruction tuning, where an\nLLM learns to generate circuit topologies from structured prompts encoding\ndesign constraints, and RL refinement, which further improves the\ninstruction-tuned model using reward models that evaluate validity, efficiency,\nand output voltage. The refined model is then used directly to generate\ntopologies that satisfy the design constraints. Empirical results show that\nAUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by\n~14% compared to the best baselines, while reducing duplicate generation rates\nby ~38%. It achieves over 60% success in synthesizing valid circuits with\nlimited training data, demonstrating strong generalization. These findings\nhighlight the framework's effectiveness in scaling to complex circuits while\nmaintaining efficiency and constraint adherence, marking a significant\nadvancement in AI-driven circuit design.", "AI": {"tldr": "AUTOCIRCUIT-RL\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u6a21\u62df\u7535\u8def\u62d3\u6251\u5408\u6210\uff0c\u751f\u6210\u66f4\u591a\u6709\u6548\u7535\u8def\u4e14\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u6a21\u62df\u7535\u8def\u62d3\u6251\u5408\u6210\u7684\u8bbe\u8ba1\u7a7a\u95f4\u5927\u4e14\u7ea6\u675f\u4e25\u683c\uff0c\u9700\u8981\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5206\u4e24\u9636\u6bb5\uff1a\u6307\u4ee4\u8c03\u6574\u8ba9LLM\u4ece\u8bbe\u8ba1\u7ea6\u675f\u63d0\u793a\u751f\u6210\u62d3\u6251\uff0c\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "result": "\u751f\u6210\u6709\u6548\u6027\u63d0\u534712%\uff0c\u6548\u7387\u63d0\u9ad814%\uff0c\u91cd\u590d\u7387\u964d\u4f4e38%\uff0c\u8bad\u7ec3\u6570\u636e\u6709\u9650\u65f6\u6210\u529f\u7387\u8d8560%\u3002", "conclusion": "AUTOCIRCUIT-RL\u5728\u590d\u6742\u7535\u8def\u8bbe\u8ba1\u4e2d\u9ad8\u6548\u4e14\u6ee1\u8db3\u7ea6\u675f\uff0c\u662fAI\u9a71\u52a8\u8bbe\u8ba1\u7684\u91cd\u5927\u8fdb\u6b65\u3002", "keywords": "\u6a21\u62df\u7535\u8def\u3001EDA\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u81ea\u52a8\u5316\u8bbe\u8ba1"}}
{"id": "2506.02890", "pdf": "https://arxiv.org/pdf/2506.02890", "abs": "https://arxiv.org/abs/2506.02890", "authors": ["Jakub Krajewski", "Marcin Chochowski", "Daniel Korzekwa"], "title": "Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Mixture of Experts (MoE) architectures have emerged as pivotal for scaling\nLarge Language Models (LLMs) efficiently. Fine-grained MoE approaches -\nutilizing more numerous, smaller experts - have demonstrated potential in\nimproving model convergence and quality. This work proposes a set of training\nrecipes and provides a comprehensive empirical evaluation of fine-grained MoE,\ndirectly comparing its scaling properties against standard MoE configurations\nfor models with up to 56B total (17B active) parameters. We investigate\nconvergence speed, model performance on downstream benchmarks, and practical\ntraining considerations across various setups. Overall, at the largest scale we\nshow that fine-grained MoE achieves better validation loss and higher accuracy\nacross a set of downstream benchmarks. This study offers empirical grounding\nand practical insights for leveraging fine-grained MoE in the development of\nfuture large-scale models.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6MoE\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u5e76\u5bf9\u6bd4\u5176\u4e0e\u6807\u51c6MoE\u5728\u7f29\u653e\u6027\u4e0a\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u7ec6\u7c92\u5ea6MoE\u5728\u5927\u578b\u6a21\u578b\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u63a2\u7d22\u7ec6\u7c92\u5ea6Mixture of Experts (MoE)\u67b6\u6784\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6548\u679c\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u6536\u655b\u6027\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u5957\u8bad\u7ec3\u65b9\u6848\uff0c\u5bf9\u4e0d\u540c\u89c4\u6a21\u7684\u7ec6\u7c92\u5ea6MoE\u548c\u6807\u51c6MoE\u914d\u7f6e\u8fdb\u884c\u5168\u9762\u7684\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u5728\u6700\u5927\u89c4\u6a21\u6a21\u578b\u4e2d\uff0c\u7ec6\u7c92\u5ea6MoE\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u9a8c\u8bc1\u635f\u5931\u548c\u66f4\u9ad8\u7684\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u7387\u3002", "conclusion": "\u7ec6\u7c92\u5ea6MoE\u4e3a\u672a\u6765\u5927\u89c4\u6a21\u6a21\u578b\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u548c\u5b9e\u7528\u89c1\u89e3\u3002", "keywords": "Mixture of Experts (MoE), \u5927\u578b\u8bed\u8a00\u6a21\u578b (LLMs), \u7ec6\u7c92\u5ea6, \u5b9e\u8bc1\u8bc4\u4f30"}}
{"id": "2506.03136", "pdf": "https://arxiv.org/pdf/2506.03136", "abs": "https://arxiv.org/abs/2506.03136", "authors": ["Yinjie Wang", "Ling Yang", "Ye Tian", "Ke Shen", "Mengdi Wang"], "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning", "categories": ["cs.CL"], "comment": "Project: https://github.com/Gen-Verse/CURE", "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE", "AI": {"tldr": "CURE\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5956\u52b1\u8bbe\u8ba1\u5171\u540c\u8fdb\u5316\u4ee3\u7801\u548c\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u80fd\u529b\uff0c\u65e0\u9700\u771f\u5b9e\u4ee3\u7801\u76d1\u7763\u3002\u5176\u4f18\u5316\u7684\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u548c\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u540c\u7c7b\u6a21\u578b\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u65e0\u76d1\u7763\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u63d0\u5347\u4ee3\u7801\u751f\u6210\u548c\u5355\u5143\u6d4b\u8bd5\u7684\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u5956\u52b1\u8bbe\u8ba1\u673a\u5236\uff0c\u8ba9\u7f16\u7801\u5668\u548c\u6d4b\u8bd5\u5668\u901a\u8fc7\u4ea4\u4e92\u5171\u540c\u8fdb\u5316\uff0c\u76f4\u63a5\u4ece\u9519\u8bef\u4e2d\u5b66\u4e60\u3002", "result": "\u4f18\u5316\u540e\u7684\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u51c6\u786e\u6027\u4e0a\u63d0\u53475.3%\uff0cBest-of-N\u51c6\u786e\u6027\u63d0\u53479.0%\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CURE\u6846\u67b6\u53ca\u5176\u884d\u751f\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u548c\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u6a21\u578b\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u4ee3\u7801\u751f\u6210, \u5355\u5143\u6d4b\u8bd5, \u65e0\u76d1\u7763\u5b66\u4e60"}}
{"id": "2506.02166", "pdf": "https://arxiv.org/pdf/2506.02166", "abs": "https://arxiv.org/abs/2506.02166", "authors": ["Arnav Rustagi", "Satvik Bajpai", "Nimrat Kaur", "Siddharth Siddharth"], "title": "Dhvani: A Weakly-supervised Phonemic Error Detection and Personalized Feedback System for Hindi", "categories": ["eess.AS", "cs.AI", "cs.LG"], "comment": "Accepted for publication at Interspeech 2025 to be held in Rotterdam,\n  the Netherlands", "summary": "Computer-Assisted Pronunciation Training (CAPT) has been extensively studied\nfor English. However, there remains a critical gap in its application to Indian\nlanguages with a base of 1.5 billion speakers. Pronunciation tools tailored to\nIndian languages are strikingly lacking despite the fact that millions learn\nthem every year. With over 600 million speakers and being the fourth\nmost-spoken language worldwide, improving Hindi pronunciation is a vital first\nstep toward addressing this gap. This paper proposes 1) Dhvani -- a novel CAPT\nsystem for Hindi, 2) synthetic speech generation for Hindi mispronunciations,\nand 3) a novel methodology for providing personalized feedback to learners.\nWhile the system often interacts with learners using Devanagari graphemes, its\ncore analysis targets phonemic distinctions, leveraging Hindi's highly phonetic\northography to analyze mispronounced speech and provide targeted feedback.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5370\u5730\u8bed\u7684\u65b0CAPT\u7cfb\u7edfDhvani\uff0c\u586b\u8865\u4e86\u5370\u5ea6\u8bed\u8a00\u53d1\u97f3\u8bad\u7ec3\u7684\u7a7a\u767d\uff0c\u5305\u62ec\u5408\u6210\u8bed\u97f3\u751f\u6210\u548c\u4e2a\u6027\u5316\u53cd\u9988\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u5370\u5ea6\u8bed\u8a00\uff08\u5c24\u5176\u662f\u5370\u5730\u8bed\uff09\u53d1\u97f3\u8bad\u7ec3\u5de5\u5177\u7684\u7f3a\u4e4f\uff0c\u5c3d\u7ba1\u5176\u4f7f\u7528\u8005\u4f17\u591a\uff0c\u4e9f\u9700\u6709\u6548\u7684CAPT\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86Dhvani\u7cfb\u7edf\uff0c\u5229\u7528\u5370\u5730\u8bed\u7684\u8bed\u97f3\u7279\u6027\u751f\u6210\u5408\u6210\u8bed\u97f3\u5e76\u63d0\u4f9b\u4e2a\u6027\u5316\u53cd\u9988\u3002", "result": "Dhvani\u7cfb\u7edf\u901a\u8fc7\u8bed\u97f3\u5206\u6790\u63d0\u4f9b\u9488\u5bf9\u6027\u53cd\u9988\uff0c\u63d0\u5347\u4e86\u5370\u5730\u8bed\u53d1\u97f3\u8bad\u7ec3\u7684\u6548\u679c\u3002", "conclusion": "Dhvani\u7cfb\u7edf\u4e3a\u5370\u5730\u8bed\u5b66\u4e60\u8005\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u53d1\u97f3\u8bad\u7ec3\u5de5\u5177\uff0c\u586b\u8865\u4e86\u5e02\u573a\u7a7a\u767d\u3002", "keywords": "CAPT, \u5370\u5730\u8bed, \u53d1\u97f3\u8bad\u7ec3, \u5408\u6210\u8bed\u97f3, \u4e2a\u6027\u5316\u53cd\u9988"}}
{"id": "2506.02897", "pdf": "https://arxiv.org/pdf/2506.02897", "abs": "https://arxiv.org/abs/2506.02897", "authors": ["Alessandro Licciardi", "Roberta Raineri", "Anton Proskurnikov", "Lamberto Rondoni", "Lorenzo Zino"], "title": "Sociodynamics-inspired Adaptive Coalition and Client Selection in Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated Learning (FL) enables privacy-preserving collaborative model\ntraining, yet its practical strength is often undermined by client data\nheterogeneity, which severely degrades model performance. This paper proposes\nthat data heterogeneity across clients' distributions can be effectively\naddressed by adopting an approach inspired by opinion dynamics over temporal\nsocial networks. We introduce \\shortname (Federated Coalition Variance\nReduction with Boltzmann Exploration), a variance-reducing selection algorithm\nin which (1) clients dynamically organize into non-overlapping clusters based\non asymptotic agreements, and (2) from each cluster, one client is selected to\nminimize the expected variance of its model update. Our experiments show that\nin heterogeneous scenarios our algorithm outperforms existing FL algorithms,\nyielding more accurate results and faster convergence, validating the efficacy\nof our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u610f\u89c1\u52a8\u6001\u7684\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff08FCVR-BE\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u805a\u7c7b\u548c\u65b9\u5dee\u51cf\u5c0f\u7ec4\u9009\u62e9\uff0c\u6709\u6548\u89e3\u51b3\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u867d\u80fd\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u534f\u4f5c\u6a21\u578b\u8bad\u7ec3\uff0c\u4f46\u5ba2\u6237\u6570\u636e\u5f02\u6784\u6027\u4f1a\u4e25\u91cd\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5f15\u5165FCVR-BE\u7b97\u6cd5\uff0c\u52a8\u6001\u7ec4\u7ec7\u5ba2\u6237\u7aef\u5230\u975e\u91cd\u53e0\u805a\u7c7b\uff0c\u5e76\u4ece\u6bcf\u4e2a\u805a\u7c7b\u4e2d\u9009\u62e9\u4e00\u4e2a\u5ba2\u6237\u7aef\u4ee5\u51cf\u5c11\u6a21\u578b\u66f4\u65b0\u65b9\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u5f02\u6784\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709FL\u7b97\u6cd5\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u7ed3\u679c\u548c\u66f4\u5feb\u7684\u6536\u655b\u3002", "conclusion": "\u901a\u8fc7\u610f\u89c1\u52a8\u6001\u805a\u7c7b\u548c\u65b9\u5dee\u51cf\u5c11\u9009\u62e9\uff0cFCVR-BE\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60\u3001\u6570\u636e\u5f02\u6784\u6027\u3001\u65b9\u5dee\u51cf\u5c11\u3001\u610f\u89c1\u52a8\u6001\u3001\u805a\u7c7b"}}
{"id": "2506.03143", "pdf": "https://arxiv.org/pdf/2506.03143", "abs": "https://arxiv.org/abs/2506.03143", "authors": ["Qianhui Wu", "Kanzhi Cheng", "Rui Yang", "Chaoyun Zhang", "Jianwei Yang", "Huiqiang Jiang", "Jian Mu", "Baolin Peng", "Bo Qiao", "Reuben Tan", "Si Qin", "Lars Liden", "Qingwei Lin", "Huan Zhang", "Tong Zhang", "Jianbing Zhang", "Dongmei Zhang", "Jianfeng Gao"], "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "One of the principal challenges in building VLM-powered GUI agents is visual\ngrounding, i.e., localizing the appropriate screen region for action execution\nbased on both the visual content and the textual plans. Most existing work\nformulates this as a text-based coordinate generation task. However, these\napproaches suffer from several limitations: weak spatial-semantic alignment,\ninability to handle ambiguous supervision targets, and a mismatch between the\ndense nature of screen coordinates and the coarse, patch-level granularity of\nvisual features extracted by models like Vision Transformers. In this paper, we\npropose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its\ncore, GUI-Actor introduces an attention-based action head that learns to align\na dedicated <ACTOR> token with all relevant visual patch tokens, enabling the\nmodel to propose one or more action regions in a single forward pass. In line\nwith this, we further design a grounding verifier to evaluate and select the\nmost plausible action region from the candidates proposed for action execution.\nExtensive experiments show that GUI-Actor outperforms prior state-of-the-art\nmethods on multiple GUI action grounding benchmarks, with improved\ngeneralization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B\neven surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7\nwith Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by\nincorporating the verifier, we find that fine-tuning only the newly introduced\naction head (~100M parameters for 7B model) while keeping the VLM backbone\nfrozen is sufficient to achieve performance comparable to previous\nstate-of-the-art models, highlighting that GUI-Actor can endow the underlying\nVLM with effective grounding capabilities without compromising its\ngeneral-purpose strengths.", "AI": {"tldr": "GUI-Actor\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u65e0\u5750\u6807GUI\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u6539\u8fdb\u89c6\u89c9\u4e0e\u8bed\u4e49\u7684\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709GUI\u5b9a\u4f4d\u65b9\u6cd5\u5728\u7a7a\u95f4\u8bed\u4e49\u5bf9\u9f50\u3001\u6a21\u7cca\u76d1\u7763\u76ee\u6807\u548c\u89c6\u89c9\u7279\u5f81\u7c92\u5ea6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e3a\u6b64\u63d0\u51faGUI-Actor\u4ee5\u6539\u8fdb\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5f15\u5165\u6ce8\u610f\u529b\u52a8\u4f5c\u5934\u548c\u5bf9\u9f50\u673a\u5236\uff0c\u901a\u8fc7<ACTOR>\u4ee4\u724c\u4e0e\u89c6\u89c9\u4ee4\u724c\u5bf9\u9f50\uff0c\u751f\u6210\u52a8\u4f5c\u533a\u57df\u5019\u9009\uff0c\u5e76\u8bbe\u8ba1\u9a8c\u8bc1\u5668\u9009\u62e9\u6700\u4f18\u533a\u57df\u3002", "result": "\u5728\u591a\u4e2aGUI\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0cGUI-Actor-7B\u5728ScreenSpot-Pro\u4e2d\u5f97\u520640.7\uff08Qwen2-VL\uff09\u548c44.6\uff08Qwen2.5-VL\uff09\u3002", "conclusion": "GUI-Actor\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6539\u8fdb\u8d4b\u4e88VLM\u5f3a\u5927\u7684\u5b9a\u4f4d\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u901a\u7528\u6027\u3002", "keywords": "GUI\u5b9a\u4f4d, \u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u6ce8\u610f\u529b\u673a\u5236, \u5750\u6807\u65e0\u5173, \u52a8\u4f5c\u533a\u57df"}}
{"id": "2506.02167", "pdf": "https://arxiv.org/pdf/2506.02167", "abs": "https://arxiv.org/abs/2506.02167", "authors": ["Aditi Tiwari", "Farzaneh Masoud", "Dac Trong Nguyen", "Jill Kraft", "Heng Ji", "Klara Nahrstedt"], "title": "Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded 360-Degree Firefighting Videos", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages, 9 figures, 6 tables", "summary": "Modern AI systems struggle most in environments where reliability is critical\n- scenes with smoke, poor visibility, and structural deformation. Each year,\ntens of thousands of firefighters are injured on duty, often due to breakdowns\nin situational perception. We introduce Fire360, a benchmark for evaluating\nperception and reasoning in safety-critical firefighting scenarios. The dataset\nincludes 228 360-degree videos from professional training sessions under\ndiverse conditions (e.g., low light, thermal distortion), annotated with action\nsegments, object locations, and degradation metadata. Fire360 supports five\ntasks: Visual Question Answering, Temporal Action Captioning, Object\nLocalization, Safety-Critical Reasoning, and Transformed Object Retrieval\n(TOR). TOR tests whether models can match pristine exemplars to fire-damaged\ncounterparts in unpaired scenes, evaluating transformation-invariant\nrecognition. While human experts achieve 83.5% on TOR, models like GPT-4o lag\nsignificantly, exposing failures in reasoning under degradation. By releasing\nFire360 and its evaluation suite, we aim to advance models that not only see,\nbut also remember, reason, and act under uncertainty. The dataset is available\nat: https://uofi.box.com/v/fire360dataset.", "AI": {"tldr": "Fire360\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6d88\u9632\u573a\u666f\u4e2d\u611f\u77e5\u4e0e\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b228\u4e2a360\u5ea6\u89c6\u9891\uff0c\u652f\u63015\u9879\u4efb\u52a1\uff0c\u65e8\u5728\u63d0\u5347AI\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u4ee3AI\u7cfb\u7edf\u5728\u53ef\u9760\u6027\u8981\u6c42\u9ad8\u7684\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5982\u6d88\u9632\u573a\u666f\u4e2d\u6709\u70df\u96fe\u3001\u80fd\u89c1\u5ea6\u4f4e\u548c\u7ed3\u6784\u53d8\u5f62\u7684\u60c5\u51b5\uff0c\u5bfc\u81f4\u6d88\u9632\u5458\u53d7\u4f24\u3002Fire360\u65e8\u5728\u63d0\u5347AI\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\u7684\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6570\u636e\u96c6\u5305\u62ec228\u4e2a360\u5ea6\u89c6\u9891\uff0c\u6765\u81ea\u4e13\u4e1a\u6d88\u9632\u8bad\u7ec3\uff0c\u6db5\u76d6\u591a\u6837\u6761\u4ef6\uff08\u5982\u4f4e\u5149\u3001\u70ed\u53d8\u5f62\uff09\uff0c\u5e76\u6807\u6ce8\u4e86\u52a8\u4f5c\u7247\u6bb5\u3001\u7269\u4f53\u4f4d\u7f6e\u548c\u9000\u5316\u5143\u6570\u636e\u3002\u652f\u6301\u89c6\u89c9\u95ee\u7b54\u3001\u65f6\u5e8f\u52a8\u4f5c\u63cf\u8ff0\u7b495\u9879\u4efb\u52a1\u3002", "result": "\u4eba\u7c7b\u4e13\u5bb6\u5728Transformed Object Retrieval\u4efb\u52a1\u4e2d\u8fbe\u523083.5%\u51c6\u786e\u7387\uff0c\u800cGPT-4o\u7b49\u6a21\u578b\u8868\u73b0\u663e\u8457\u843d\u540e\uff0c\u663e\u793a\u5728\u9000\u5316\u6761\u4ef6\u4e0b\u7684\u63a8\u7406\u7f3a\u9677\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03Fire360\u53ca\u5176\u8bc4\u4f30\u5de5\u5177\uff0c\u65e8\u5728\u63a8\u52a8AI\u5728\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u7684\u611f\u77e5\u3001\u8bb0\u5fc6\u3001\u63a8\u7406\u4e0e\u884c\u52a8\u80fd\u529b\u3002", "keywords": "Fire360, \u6d88\u9632\u573a\u666f, \u611f\u77e5\u63a8\u7406, \u6570\u636e\u96c6, \u5b89\u5168\u5173\u952e\u573a\u666f"}}
{"id": "2506.02933", "pdf": "https://arxiv.org/pdf/2506.02933", "abs": "https://arxiv.org/abs/2506.02933", "authors": ["Junyi Fang", "Yuxun Chen", "Yuxin Chen", "Chen Zhang"], "title": "From Theory to Practice with RAVEN-UCB: Addressing Non-Stationarity in Multi-Armed Bandits through Variance Adaptation", "categories": ["cs.LG", "stat.ML", "I.2.6; I.2.8; G.3"], "comment": "25 pages, 5 figures, 4 tables, submitted to Applied Intelligence,\n  code available at https://github.com/66661654/Raven-UCB", "summary": "The Multi-Armed Bandit (MAB) problem is challenging in non-stationary\nenvironments where reward distributions evolve dynamically. We introduce\nRAVEN-UCB, a novel algorithm that combines theoretical rigor with practical\nefficiency via variance-aware adaptation. It achieves tighter regret bounds\nthan UCB1 and UCB-V, with gap-dependent regret of order $K \\sigma_{\\max}^2 \\log\nT / \\Delta$ and gap-independent regret of order $\\sqrt{K T \\log T}$. RAVEN-UCB\nincorporates three innovations: (1) variance-driven exploration using\n$\\sqrt{\\hat{\\sigma}_k^2 / (N_k + 1)}$ in confidence bounds, (2) adaptive\ncontrol via $\\alpha_t = \\alpha_0 / \\log(t + \\epsilon)$, and (3) constant-time\nrecursive updates for efficiency. Experiments across non-stationary patterns -\ndistributional changes, periodic shifts, and temporary fluctuations - in\nsynthetic and logistics scenarios demonstrate its superiority over\nstate-of-the-art baselines, confirming theoretical and practical robustness.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86RAVEN-UCB\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u975e\u9759\u6001\u73af\u5883\u4e2d\u7684\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u901a\u8fc7\u65b9\u5dee\u611f\u77e5\u7684\u9002\u5e94\u673a\u5236\u5b9e\u73b0\u66f4\u4f18\u7684\u9057\u61be\u8fb9\u754c\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u975e\u9759\u6001\u73af\u5883\u4e2d\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u4e2d\u5956\u52b1\u5206\u5e03\u52a8\u6001\u53d8\u5316\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u4e09\u79cd\u521b\u65b0\uff1a\u65b9\u5dee\u9a71\u52a8\u7684\u63a2\u7d22\u3001\u81ea\u9002\u5e94\u63a7\u5236\u548c\u9ad8\u6548\u9012\u5f52\u66f4\u65b0\u3002", "result": "\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5b9e\u73b0\u4e86\u4f18\u4e8eUCB1\u548cUCB-V\u7684\u9057\u61be\u8fb9\u754c\uff0c\u5e76\u5728\u591a\u79cd\u975e\u9759\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "RAVEN-UCB\u7b97\u6cd5\u5728\u975e\u9759\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "keywords": "\u591a\u81c2\u8001\u864e\u673a,\u975e\u9759\u6001\u73af\u5883,RAVEN-UCB,\u65b9\u5dee\u611f\u77e5"}}
{"id": "2506.03145", "pdf": "https://arxiv.org/pdf/2506.03145", "abs": "https://arxiv.org/abs/2506.03145", "authors": ["Pralaypati Ta", "Sriram Venkatesaperumal", "Keerthi Ram", "Mohanasankar Sivaprakasam"], "title": "Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Neuroscience research publications encompass a vast wealth of knowledge.\nAccurately retrieving existing information and discovering new insights from\nthis extensive literature is essential for advancing the field. However, when\nknowledge is dispersed across multiple sources, current state-of-the-art\nretrieval methods often struggle to extract the necessary information. A\nknowledge graph (KG) can integrate and link knowledge from multiple sources,\nbut existing methods for constructing KGs in neuroscience often rely on labeled\ndata and require domain expertise. Acquiring large-scale, labeled data for a\nspecialized area like neuroscience presents significant challenges. This work\nproposes novel methods for constructing KG from unlabeled large-scale\nneuroscience research corpus utilizing large language models (LLM),\nneuroscience ontology, and text embeddings. We analyze the semantic relevance\nof neuroscience text segments identified by LLM for building the knowledge\ngraph. We also introduce an entity-augmented information retrieval algorithm to\nextract knowledge from the KG. Several experiments were conducted to evaluate\nthe proposed approaches, and the results demonstrate that our methods\nsignificantly enhance knowledge discovery from the unlabeled neuroscience\nresearch corpus. It achieves an F1 score of 0.84 for entity extraction, and the\nknowledge obtained from the KG improves answers to over 54% of the questions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u795e\u7ecf\u79d1\u5b66\u672c\u4f53\u548c\u6587\u672c\u5d4c\u5165\u4ece\u65e0\u6807\u8bb0\u7684\u5927\u89c4\u6a21\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u8bed\u6599\u5e93\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u53d1\u73b0\u80fd\u529b\u3002", "motivation": "\u795e\u7ecf\u79d1\u5b66\u9886\u57df\u7684\u77e5\u8bc6\u5206\u6563\u5728\u591a\u4e2a\u6765\u6e90\uff0c\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u63d0\u53d6\u4fe1\u606f\uff0c\u4e14\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u901a\u5e38\u4f9d\u8d56\u6807\u8bb0\u6570\u636e\u548c\u9886\u57df\u4e13\u5bb6\uff0c\u83b7\u53d6\u5927\u89c4\u6a21\u6807\u8bb0\u6570\u636e\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3001\u795e\u7ecf\u79d1\u5b66\u672c\u4f53\u548c\u6587\u672c\u5d4c\u5165\uff0c\u5206\u6790\u795e\u7ecf\u79d1\u5b66\u6587\u672c\u7247\u6bb5\u7684\u8bed\u4e49\u76f8\u5173\u6027\u4ee5\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u5f15\u5165\u5b9e\u4f53\u589e\u5f3a\u7684\u4fe1\u606f\u68c0\u7d22\u7b97\u6cd5\u3002", "result": "\u5b9e\u4f53\u63d0\u53d6\u7684F1\u5f97\u5206\u4e3a0.84\uff0c\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u63d0\u53d6\u7684\u77e5\u8bc6\u6539\u5584\u4e8654%\u4ee5\u4e0a\u95ee\u9898\u7684\u56de\u7b54\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4ece\u65e0\u6807\u8bb0\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u8bed\u6599\u5e93\u4e2d\u77e5\u8bc6\u53d1\u73b0\u7684\u6548\u679c\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u795e\u7ecf\u79d1\u5b66\u9886\u57df\u7684\u6f5c\u529b\u3002", "keywords": "\u77e5\u8bc6\u56fe\u8c31,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u795e\u7ecf\u79d1\u5b66,\u65e0\u6807\u8bb0\u6570\u636e,\u4fe1\u606f\u68c0\u7d22"}}
{"id": "2506.02935", "pdf": "https://arxiv.org/pdf/2506.02935", "abs": "https://arxiv.org/abs/2506.02935", "authors": ["Yuepeng Zheng", "Fu Luo", "Zhenkun Wang", "Yaoxin Wu", "Yu Zhou"], "title": "MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable Neural Vehicle Routing Solver", "categories": ["cs.LG"], "comment": "24 pages,5 figures, 8 tables", "summary": "Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) is a\npromising approach to train a unified model capable of solving multiple Vehicle\nRouting Problem (VRP) variants. However, existing Reinforcement Learning\n(RL)-based multi-task methods can only train light decoder models on\nsmall-scale problems, exhibiting limited generalization ability when solving\nlarge-scale problems. To overcome this limitation, this work introduces a novel\nmulti-task learning method driven by knowledge distillation (MTL-KD), which\nenables the efficient training of heavy decoder models with strong\ngeneralization ability. The proposed MTL-KD method transfers policy knowledge\nfrom multiple distinct RL-based single-task models to a single heavy decoder\nmodel, facilitating label-free training and effectively improving the model's\ngeneralization ability across diverse tasks. In addition, we introduce a\nflexible inference strategy termed Random Reordering Re-Construction (R3C),\nwhich is specifically adapted for diverse VRP tasks and further boosts the\nperformance of the multi-task model. Experimental results on 6 seen and 10\nunseen VRP variants with up to 1000 nodes indicate that our proposed method\nconsistently achieves superior performance on both uniform and real-world\nbenchmarks, demonstrating robust generalization abilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\uff08MTL-KD\uff09\uff0c\u7528\u4e8e\u8bad\u7ec3\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u7684\u5927\u578b\u89e3\u7801\u5668\u6a21\u578b\uff0c\u5e76\u5728\u591a\u79cd\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08VRP\uff09\u53d8\u4f53\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u591a\u4efb\u52a1\u65b9\u6cd5\u5728\u89e3\u51b3\u5927\u89c4\u6a21\u95ee\u9898\u65f6\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u53ea\u80fd\u8bad\u7ec3\u5c0f\u578b\u89e3\u7801\u5668\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u514b\u670d\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86MTL-KD\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u591a\u4e2a\u5355\u4efb\u52a1RL\u6a21\u578b\u4e2d\u8f6c\u79fb\u7b56\u7565\u77e5\u8bc6\u5230\u5355\u4e00\u5927\u578b\u89e3\u7801\u5668\u6a21\u578b\uff0c\u5b9e\u73b0\u65e0\u6807\u7b7e\u8bad\u7ec3\uff1b\u540c\u65f6\u5f15\u5165\u4e86\u968f\u673a\u91cd\u6392\u5e8f\u91cd\u5efa\uff08R3C\uff09\u63a8\u7406\u7b56\u7565\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684VRP\u4efb\u52a1\u3002", "result": "\u57286\u4e2a\u5df2\u77e5\u548c10\u4e2a\u672a\u77e5\u7684VRP\u53d8\u4f53\uff08\u6700\u591a1000\u4e2a\u8282\u70b9\uff09\u4e0a\uff0cMTL-KD\u65b9\u6cd5\u5728\u7edf\u4e00\u548c\u5b9e\u9645\u573a\u666f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u663e\u793a\u4e86\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MTL-KD\u65b9\u6cd5\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548cR3C\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002", "keywords": "\u591a\u4efb\u52a1\u5b66\u4e60, \u77e5\u8bc6\u84b8\u998f, \u795e\u7ecf\u7ec4\u5408\u4f18\u5316, \u8f66\u8f86\u8def\u5f84\u95ee\u9898, \u5f3a\u5316\u5b66\u4e60"}}
{"id": "2506.03149", "pdf": "https://arxiv.org/pdf/2506.03149", "abs": "https://arxiv.org/abs/2506.03149", "authors": ["Pietro Lesci", "Clara Meister", "Thomas Hofmann", "Andreas Vlachos", "Tiago Pimentel"], "title": "Causal Estimation of Tokenisation Bias", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published as a conference paper at ACL 2025", "summary": "Modern language models are typically trained over subword sequences, but\nultimately define probabilities over character-strings. Ideally, the choice of\nthe tokeniser -- which maps character-strings to subwords -- should not affect\nthe probability assigned to the underlying character-string; in practice, it\ndoes. We define this mismatch as tokenisation bias. In this work, we quantify\none particular type of tokenisation bias: the effect of including or not a\nsubword (e.g., $\\langle hello \\rangle$) in a tokeniser's vocabulary on the\nprobability a trained model assigns to the corresponding characters (i.e.,\n\\textit{``hello''}). Estimating this effect is challenging because each model\nis trained with only one tokeniser. We address this by framing tokenisation\nbias as a causal effect and estimating it using the regression discontinuity\ndesign. Specifically, we exploit the fact that tokenisation algorithms rank\nsubwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an\narbitrary cutoff point. As such, we can estimate a causal effect by comparing\nsimilar subwords around this cutoff. Experimentally, we find that tokenisation\nconsistently affects models' outputs across scales, vocabularies, and\ntokenisers. Notably, a subword's presence in a small model's vocabulary may\nincrease its characters' probability by up to 17 times, highlighting\ntokenisation as a key design choice in language modelling.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u4e2d\u56e0\u5206\u8bcd\u5668\u9009\u62e9\u5bfc\u81f4\u7684\u6982\u7387\u5206\u914d\u504f\u5dee\uff0c\u5373'\u5206\u8bcd\u504f\u5dee'\uff0c\u5e76\u901a\u8fc7\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u65b9\u6cd5\u91cf\u5316\u4e86\u5176\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u5206\u8bcd\u5668\uff08\u5c06\u5b57\u7b26\u5e8f\u5217\u6620\u5c04\u4e3a\u5b50\u8bcd\u7684\u7ec4\u4ef6\uff09\u7684\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u5bf9\u5b57\u7b26\u5e8f\u5217\u7684\u6982\u7387\u5206\u914d\u95ee\u9898\u3002\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u5206\u8bcd\u5668\u7684\u9009\u62e9\u4e0d\u5e94\u5f71\u54cd\u5b57\u7b26\u5e8f\u5217\u7684\u6982\u7387\u5206\u914d\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u5b58\u5728\u504f\u5dee\u3002", "method": "\u65b9\u6cd5\u662f\u5c06\u5206\u8bcd\u504f\u5dee\u5b9a\u4e49\u4e3a\u56e0\u679c\u6548\u5e94\uff0c\u5e76\u5229\u7528\u56de\u5f52\u65ad\u70b9\u8bbe\u8ba1\uff08RDD\uff09\u8fdb\u884c\u4f30\u8ba1\u3002\u901a\u8fc7\u6bd4\u8f83\u5206\u8bcd\u5668\u8bcd\u6c47\u8868\u4e2d\u9608\u503c\u9644\u8fd1\u7684\u5b50\u8bcd\uff0c\u91cf\u5316\u4e86\u5206\u8bcd\u504f\u5dee\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5206\u8bcd\u64cd\u4f5c\u5728\u4e0d\u540c\u89c4\u6a21\u3001\u8bcd\u6c47\u8868\u548c\u5206\u8bcd\u5668\u7684\u6a21\u578b\u4e2d\u5747\u663e\u8457\u5f71\u54cd\u8f93\u51fa\u3002\u4f8b\u5982\uff0c\u4e00\u4e2a\u5c0f\u578b\u6a21\u578b\u4e2d\u5b50\u8bcd\u7684\u5b58\u5728\u53ef\u80fd\u5bfc\u81f4\u5176\u5bf9\u5e94\u5b57\u7b26\u6982\u7387\u589e\u52a0\u9ad8\u8fbe17\u500d\u3002", "conclusion": "\u7ed3\u8bba\u662f\u5206\u8bcd\u5668\u7684\u9009\u62e9\u662f\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5206\u8bcd\u504f\u5dee\u7684\u5b58\u5728\u9700\u8981\u5728\u6a21\u578b\u5f00\u53d1\u548c\u8bc4\u4f30\u4e2d\u88ab\u91cd\u89c6\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b\u3001\u5206\u8bcd\u504f\u5dee\u3001\u56e0\u679c\u6548\u5e94\u3001\u56de\u5f52\u65ad\u70b9\u8bbe\u8ba1\u3001\u5b50\u8bcd"}}
{"id": "2506.02939", "pdf": "https://arxiv.org/pdf/2506.02939", "abs": "https://arxiv.org/abs/2506.02939", "authors": ["Malik Khalaf", "Yara Shamshoum", "Nitzan Hodos", "Yuval Sieradzki", "Assaf Schuster"], "title": "QKV Projections Require a Fraction of Their Memory", "categories": ["cs.LG"], "comment": null, "summary": "The Multi-Head Attention mechanism is central to LLM operation, and multiple\nworks target its compute and memory efficiency during training. While most\nworks focus on approximating the scaled dot product, the memory consumption of\nthe linear projections that compute the $Q$, $K$, and $V$ tensors from the\ninput $x$ is often overlooked. To address this, we propose Point-Approximate\nMatrix Multiplication (PAMM), a novel tensor compression technique that reduces\nmemory consumption of the $Q,K,V$ projections in attention layers by a factor\nof up to $\\times 512$, effectively erasing their memory footprint, while\nachieving similar or better final perplexity. PAMM is fully composable with\nefficient attention techniques such as FlashAttention, making it a practical\nand complementary method for memory-efficient LLM training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPAMM\u7684\u77e9\u9635\u4e58\u6cd5\u8fd1\u4f3c\u6280\u672f\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6ce8\u610f\u529b\u5c42\u4e2dQ\u3001K\u3001V\u6295\u5f71\u7684\u5185\u5b58\u5360\u7528\uff0c\u4e0e\u73b0\u6709\u9ad8\u6548\u6ce8\u610f\u529b\u6280\u672f\u517c\u5bb9\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5de5\u4f5c\u4e2d\u5bf9\u6ce8\u610f\u529b\u5c42\u4e2d\u7ebf\u6027\u6295\u5f71\u5185\u5b58\u5360\u7528\u95ee\u9898\u7684\u5ffd\u89c6\u3002", "method": "\u91c7\u7528Point-Approximate Matrix Multiplication\uff08PAMM\uff09\u538b\u7f29\u6280\u672f\u3002", "result": "\u5185\u5b58\u6d88\u8017\u51cf\u5c11\u9ad8\u8fbe512\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u6539\u5584\u6700\u7ec8\u56f0\u60d1\u5ea6\u3002", "conclusion": "PAMM\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u4e92\u8865\u7684\u5185\u5b58\u9ad8\u6548LLM\u8bad\u7ec3\u65b9\u6cd5\u3002", "keywords": "Multi-Head Attention, memory efficiency, PAMM, LLM training"}}
{"id": "2506.01704", "pdf": "https://arxiv.org/pdf/2506.01704", "abs": "https://arxiv.org/abs/2506.01704", "authors": ["Jiongnan Liu", "Zhicheng Dou", "Ning Hu", "Chenyan Xiong"], "title": "Generate, Not Recommend: Personalized Multimodal Content Generation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "To address the challenge of information overload from massive web contents,\nrecommender systems are widely applied to retrieve and present personalized\nresults for users. However, recommendation tasks are inherently constrained to\nfiltering existing items and lack the ability to generate novel concepts,\nlimiting their capacity to fully satisfy user demands and preferences. In this\npaper, we propose a new paradigm that goes beyond content filtering and\nselecting: directly generating personalized items in a multimodal form, such as\nimages, tailored to individual users. To accomplish this, we leverage\nany-to-any Large Multimodal Models (LMMs) and train them in both supervised\nfine-tuning and online reinforcement learning strategy to equip them with the\nability to yield tailored next items for users. Experiments on two benchmark\ndatasets and user study confirm the efficacy of the proposed method. Notably,\nthe generated images not only align well with users' historical preferences but\nalso exhibit relevance to their potential future interests.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u8d85\u8d8a\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u7684\u5185\u5bb9\u8fc7\u6ee4\uff0c\u76f4\u63a5\u751f\u6210\u4e2a\u6027\u5316\u591a\u6a21\u6001\u5185\u5bb9\uff08\u5982\u56fe\u50cf\uff09\u3002", "motivation": "\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4ec5\u80fd\u8fc7\u6ee4\u73b0\u6709\u5185\u5bb9\u3001\u65e0\u6cd5\u751f\u6210\u65b0\u6982\u5ff5\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u66f4\u597d\u5730\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002", "method": "\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u751f\u6210\u4e2a\u6027\u5316\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u751f\u6210\u7684\u56fe\u50cf\u4e0e\u7528\u6237\u5386\u53f2\u504f\u597d\u548c\u6f5c\u5728\u5174\u8da3\u9ad8\u5ea6\u5951\u5408\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u751f\u6210\u65b0\u5185\u5bb9\u7684\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u3002", "keywords": "\u63a8\u8350\u7cfb\u7edf, \u591a\u6a21\u6001\u751f\u6210, \u5927\u578b\u591a\u6a21\u6001\u6a21\u578b, \u5f3a\u5316\u5b66\u4e60"}}
{"id": "2506.02946", "pdf": "https://arxiv.org/pdf/2506.02946", "abs": "https://arxiv.org/abs/2506.02946", "authors": ["Edoardo Pona", "Milad Kazemi", "Yali Du", "David Watson", "Nicola Paoletti"], "title": "Abstract Counterfactuals for Language Model Agents", "categories": ["cs.LG"], "comment": null, "summary": "Counterfactual inference is a powerful tool for analysing and evaluating\nautonomous agents, but its application to language model (LM) agents remains\nchallenging. Existing work on counterfactuals in LMs has primarily focused on\ntoken-level counterfactuals, which are often inadequate for LM agents due to\ntheir open-ended action spaces. Unlike traditional agents with fixed, clearly\ndefined action spaces, the actions of LM agents are often implicit in the\nstrings they output, making their action spaces difficult to define and\ninterpret. Furthermore, the meanings of individual tokens can shift depending\non the context, adding complexity to token-level reasoning and sometimes\nleading to biased or meaningless counterfactuals. We introduce \\emph{Abstract\nCounterfactuals}, a framework that emphasises high-level characteristics of\nactions and interactions within an environment, enabling counterfactual\nreasoning tailored to user-relevant features. Our experiments demonstrate that\nthe approach produces consistent and meaningful counterfactuals while\nminimising the undesired side effects of token-level methods. We conduct\nexperiments on text-based games and counterfactual text generation, while\nconsidering both token-level and latent-space interventions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u62bd\u8c61\u53cd\u4e8b\u5b9e\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4ee4\u724c\u7ea7\u53cd\u4e8b\u5b9e\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4e00\u81f4\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u52a8\u4f5c\u7a7a\u95f4\u5f00\u653e\u4e14\u9690\u542b\uff0c\u4ee4\u724c\u7ea7\u53cd\u4e8b\u5b9e\u65b9\u6cd5\u96be\u4ee5\u9002\u7528\uff0c\u5bb9\u6613\u4ea7\u751f\u504f\u5dee\u6216\u65e0\u610f\u4e49\u7684\u7ed3\u679c\u3002", "method": "\u5f15\u5165\u62bd\u8c61\u53cd\u4e8b\u5b9e\u6846\u67b6\uff0c\u5173\u6ce8\u52a8\u4f5c\u548c\u4ea4\u4e92\u7684\u9ad8\u5c42\u7279\u5f81\uff0c\u7ed3\u5408\u4ee4\u724c\u7ea7\u548c\u6f5c\u5728\u7a7a\u95f4\u5e72\u9884\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u4e00\u81f4\u4e14\u6709\u610f\u4e49\u7684\u53cd\u4e8b\u5b9e\uff0c\u540c\u65f6\u51cf\u5c11\u4ee4\u724c\u7ea7\u65b9\u6cd5\u7684\u526f\u4f5c\u7528\u3002", "conclusion": "\u62bd\u8c61\u53cd\u4e8b\u5b9e\u6846\u67b6\u4e3a\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684\u53cd\u4e8b\u5b9e\u63a8\u7406\u65b9\u6cd5\u3002", "keywords": "\u53cd\u4e8b\u5b9e\u63a8\u7406,\u8bed\u8a00\u6a21\u578b\u4ee3\u7406,\u62bd\u8c61\u53cd\u4e8b\u5b9e,\u4ee4\u724c\u7ea7\u5e72\u9884"}}
{"id": "2506.02950", "pdf": "https://arxiv.org/pdf/2506.02950", "abs": "https://arxiv.org/abs/2506.02950", "authors": ["Stepan I. Manukhov", "Alexander Kolesov", "Vladimir V. Palyulin", "Alexander Korotin"], "title": "Interaction Field Matching: Overcoming Limitations of Electrostatic Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Electrostatic field matching (EFM) has recently appeared as a novel\nphysics-inspired paradigm for data generation and transfer using the idea of an\nelectric capacitor. However, it requires modeling electrostatic fields using\nneural networks, which is non-trivial because of the necessity to take into\naccount the complex field outside the capacitor plates. In this paper, we\npropose Interaction Field Matching (IFM), a generalization of EFM which allows\nusing general interaction fields beyond the electrostatic one. Furthermore,\ninspired by strong interactions between quarks and antiquarks in physics, we\ndesign a particular interaction field realization which solves the problems\nwhich arise when modeling electrostatic fields in EFM. We show the performance\non a series of toy and image data transfer problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIFM\uff08Interaction Field Matching\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u4f5c\u4e3aEFM\uff08Electrostatic Field Matching\uff09\u7684\u6269\u5c55\uff0c\u89e3\u51b3\u4e86EFM\u4e2d\u9759\u7535\u573a\u7684\u5efa\u6a21\u96be\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6027\u80fd\u3002", "motivation": "EFM\u4f5c\u4e3a\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u542f\u53d1\u7684\u6570\u636e\u751f\u6210\u548c\u4f20\u8f93\u65b9\u6cd5\uff0c\u5b58\u5728\u5efa\u6a21\u590d\u6742\u7684\u9759\u7535\u573a\u7684\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u901a\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faIFM\u65b9\u6cd5\uff0c\u5141\u8bb8\u4f7f\u7528\u8d85\u51fa\u9759\u7535\u573a\u7684\u901a\u7528\u76f8\u4e92\u4f5c\u7528\u573a\uff0c\u7279\u522b\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u76f8\u4e92\u4f5c\u7528\u7684\u5b9e\u73b0\u65b9\u6848\u3002", "result": "\u5728\u4e00\u7cfb\u5217\u73a9\u5177\u548c\u56fe\u50cf\u6570\u636e\u4f20\u8f93\u95ee\u9898\u4e0a\u5c55\u793a\u4e86IFM\u7684\u6027\u80fd\u3002", "conclusion": "IFM\u6210\u529f\u89e3\u51b3\u4e86EFM\u4e2d\u7684\u5efa\u6a21\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u901a\u7528\u76f8\u4e92\u4f5c\u7528\u573a\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u6570\u636e\u5904\u7406\u65b9\u6cd5\u3002", "keywords": "EFM, IFM, \u76f8\u4e92\u4f5c\u7528\u573a, \u6570\u636e\u751f\u6210, \u6570\u636e\u4f20\u8f93"}}
{"id": "2506.02965", "pdf": "https://arxiv.org/pdf/2506.02965", "abs": "https://arxiv.org/abs/2506.02965", "authors": ["Ze Yu Zhang", "Bolin Ding", "Bryan Kian Hsiang Low"], "title": "Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs", "categories": ["cs.LG"], "comment": "20 pages, 4 figures,", "summary": "Mixture-of-Experts (MoE) has been gaining popularity due to its successful\nadaptation to large language models (LLMs). In this work, we introduce\nPrivacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages\nthe sparsity of the MoE architecture for memory-efficient decentralized\ncollaborative LLM training, enabling multiple parties with limited GPU-memory\nand data resources to collectively train more capable LLMs than they could\nachieve individually. At the same time, this approach protects training data\nprivacy of each participant by keeping training data, as well as parts of the\nforward pass signal and gradients locally within each party. By design, PC-MoE\nsynergistically combines the strengths of distributed computation with strong\nconfidentiality assurances. Unlike most privacy-preserving schemes, which pay\nfor confidentiality with lower task accuracy, our framework breaks that\ntrade-off: across seven popular LLM benchmarks, it almost matches (and\nsometimes exceeds) the performance and convergence rate of a fully centralized\nmodel, enjoys near 70% peak GPU RAM reduction, while being fully robust against\nreconstruction attacks.", "AI": {"tldr": "PC-MoE\u662f\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u534f\u4f5c\u5f0fMixture-of-Experts\u6846\u67b6\uff0c\u901a\u8fc7MoE\u67b6\u6784\u7684\u7a00\u758f\u6027\u5b9e\u73b0\u9ad8\u6548\u5185\u5b58\u4f7f\u7528\u548c\u5206\u5e03\u5f0f\u534f\u4f5c\u8bad\u7ec3\uff0c\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u540c\u65f6\u4fdd\u6301\u9ad8\u4efb\u52a1\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u591a\u4e2a\u8d44\u6e90\u6709\u9650\u7684\u53c2\u4e0e\u65b9\u5728\u534f\u4f5c\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u9762\u4e34\u7684\u6570\u636e\u9690\u79c1\u548cGPU\u5185\u5b58\u9650\u5236\u95ee\u9898\u3002", "method": "\u5229\u7528MoE\u67b6\u6784\u7684\u7a00\u758f\u6027\uff0c\u5c06\u8bad\u7ec3\u6570\u636e\u548c\u90e8\u5206\u68af\u5ea6\u4fe1\u53f7\u4fdd\u7559\u5728\u672c\u5730\uff0c\u5b9e\u73b0\u5206\u5e03\u5f0f\u534f\u4f5c\u8bad\u7ec3\u3002", "result": "\u5728\u4e03\u4e2a\u6d41\u884c\u7684LLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u51e0\u4e4e\u4e0e\u96c6\u4e2d\u5f0f\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff0cGPU\u5185\u5b58\u5cf0\u503c\u51cf\u5c1170%\uff0c\u4e14\u5b8c\u5168\u62b5\u6297\u91cd\u5efa\u653b\u51fb\u3002", "conclusion": "PC-MoE\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u7a81\u7834\u4e86\u5178\u578b\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u4e2d\u7cbe\u5ea6\u4e0b\u964d\u7684\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u534f\u4f5c\u8bad\u7ec3\u3002", "keywords": "Mixture-of-Experts, \u9690\u79c1\u4fdd\u62a4, \u5206\u5e03\u5f0f\u8bad\u7ec3, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u534f\u4f5c\u5b66\u4e60"}}
{"id": "2506.02972", "pdf": "https://arxiv.org/pdf/2506.02972", "abs": "https://arxiv.org/abs/2506.02972", "authors": ["Md-Ferdous Pervej", "Richeng Jin", "Md Moin Uddin Chowdhury", "Simran Singh", "\u0130smail G\u00fcven\u00e7", "Huaiyu Dai"], "title": "Computation- and Communication-Efficient Online FL for Resource-Constrained Aerial Vehicles", "categories": ["cs.LG", "cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "Privacy-preserving distributed machine learning (ML) and aerial connected\nvehicle (ACV)-assisted edge computing have drawn significant attention lately.\nSince the onboard sensors of ACVs can capture new data as they move along their\ntrajectories, the continual arrival of such 'newly' sensed data leads to online\nlearning and demands carefully crafting the trajectories. Besides, as typical\nACVs are inherently resource-constrained, computation- and\ncommunication-efficient ML solutions are needed. Therefore, we propose a\ncomputation- and communication-efficient online aerial federated learning\n(2CEOAFL) algorithm to take the benefits of continual sensed data and limited\nonboard resources of the ACVs. In particular, considering independently owned\nACVs act as selfish data collectors, we first model their trajectories\naccording to their respective time-varying data distributions. We then propose\na 2CEOAFL algorithm that allows the flying ACVs to (a) prune the received dense\nML model to make it shallow, (b) train the pruned model, and (c)\nprobabilistically quantize and offload their trained accumulated gradients to\nthe central server (CS). Our extensive simulation results show that the\nproposed 2CEOAFL algorithm delivers comparable performances to its non-pruned\nand nonquantized, hence, computation- and communication-inefficient\ncounterparts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u548c\u901a\u4fe1\u9ad8\u6548\u7684\u5728\u7ebf\u7a7a\u4e2d\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff082CEOAFL\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u7a7a\u4e2d\u8fde\u63a5\u8f66\u8f86\uff08ACV\uff09\u6301\u7eed\u611f\u77e5\u6570\u636e\u548c\u8d44\u6e90\u53d7\u9650\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u88c1\u526a\u548c\u68af\u5ea6\u91cf\u5316\u7684\u65b9\u5f0f\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u3002", "motivation": "\u7a7a\u4e2d\u8fde\u63a5\u8f66\u8f86\uff08ACV\uff09\u56e0\u5176\u6301\u7eed\u611f\u77e5\u6570\u636e\u7684\u80fd\u529b\u548c\u8d44\u6e90\u53d7\u9650\u7684\u7279\u70b9\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u5b66\u4e60\u65b9\u6cd5\u4ee5\u5e94\u5bf9\u5728\u7ebf\u5b66\u4e60\u548c\u8d44\u6e90\u9650\u5236\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5efa\u6a21ACV\u7684\u52a8\u6001\u8f68\u8ff9\u548c\u65f6\u95f4\u53d8\u5316\u6570\u636e\u5206\u5e03\uff0c\u63d0\u51fa2CEOAFL\u7b97\u6cd5\uff0c\u5305\u62ec\u6a21\u578b\u88c1\u526a\u3001\u8bad\u7ec3\u548c\u68af\u5ea6\u91cf\u5316\u7684\u6b65\u9aa4\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c2CEOAFL\u7b97\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0e\u975e\u88c1\u526a\u548c\u975e\u91cf\u5316\u7684\u4f4e\u6548\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "2CEOAFL\u7b97\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684ACV\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u76f8\u5f53\u7684\u8054\u90a6\u5b66\u4e60\u3002", "keywords": "\u9690\u79c1\u4fdd\u62a4\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60, \u7a7a\u4e2d\u8fde\u63a5\u8f66\u8f86, \u5728\u7ebf\u5b66\u4e60, \u8054\u90a6\u5b66\u4e60, \u6a21\u578b\u88c1\u526a, \u68af\u5ea6\u91cf\u5316"}}
{"id": "2506.02229", "pdf": "https://arxiv.org/pdf/2506.02229", "abs": "https://arxiv.org/abs/2506.02229", "authors": ["Manas Mehta", "Yimu Pan", "Kelly Gallagher", "Alison D. Gernand", "Jeffery A. Goldstein", "Delia Mwinyelle", "Leena Mithal", "James Z. Wang"], "title": "VLCD: Vision-Language Contrastive Distillation for Accurate and Efficient Automatic Placenta Analysis", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Proceedings of the 9th International Workshop on Health Intelligence,\n  in conjunction with the Annual AAAI Conference on Artificial Intelligence,\n  Philadelphia, Pennsylvania, March 2025", "summary": "Pathological examination of the placenta is an effective method for detecting\nand mitigating health risks associated with childbirth. Recent advancements in\nAI have enabled the use of photographs of the placenta and pathology reports\nfor detecting and classifying signs of childbirth-related pathologies. However,\nexisting automated methods are computationally extensive, which limits their\ndeployability. We propose two modifications to vision-language contrastive\nlearning (VLC) frameworks to enhance their accuracy and efficiency: (1)\ntext-anchored vision-language contrastive knowledge distillation (VLCD)-a new\nknowledge distillation strategy for medical VLC pretraining, and (2)\nunsupervised predistillation using a large natural images dataset for improved\ninitialization. Our approach distills efficient neural networks that match or\nsurpass the teacher model in performance while achieving model compression and\nacceleration. Our results showcase the value of unsupervised predistillation in\nimproving the performance and robustness of our approach, specifically for\nlower-quality images. VLCD serves as an effective way to improve the efficiency\nand deployability of medical VLC approaches, making AI-based healthcare\nsolutions more accessible, especially in resource-constrained environments.", "AI": {"tldr": "\u901a\u8fc7\u6539\u8fdb\u89c6\u89c9-\u8bed\u8a00\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\u63d0\u5347\u80ce\u76d8\u75c5\u7406\u68c0\u6d4b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff1a\u57fa\u4e8e\u6587\u672c\u951a\u5b9a\u7684\u77e5\u8bc6\u84b8\u998f\u548c\u5229\u7528\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u7684\u65e0\u76d1\u7763\u9884\u84b8\u998f\u3002", "motivation": "\u80ce\u76d8\u75c5\u7406\u68c0\u67e5\u662f\u68c0\u6d4b\u5206\u5a29\u76f8\u5173\u5065\u5eb7\u98ce\u9669\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u6587\u672c\u951a\u5b9a\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u6bd4\u77e5\u8bc6\u84b8\u998f\uff08VLCD\uff09\u548c\u65e0\u76d1\u7763\u9884\u84b8\u998f\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u6548\u7387\u548c\u6027\u80fd\u3002", "result": "\u5b9e\u73b0\u4e86\u6a21\u578b\u538b\u7f29\u548c\u52a0\u901f\uff0c\u6027\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u6559\u5e08\u6a21\u578b\uff0c\u5c24\u5176\u63d0\u5347\u4e86\u5bf9\u4f4e\u8d28\u91cf\u56fe\u50cf\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "VLCD\u63d0\u9ad8\u4e86\u533b\u7597\u89c6\u89c9-\u8bed\u8a00\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u7684\u6548\u7387\u548c\u53ef\u90e8\u7f72\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002", "keywords": "\u80ce\u76d8\u75c5\u7406\u68c0\u67e5\u3001\u77e5\u8bc6\u84b8\u998f\u3001\u89c6\u89c9-\u8bed\u8a00\u5bf9\u6bd4\u5b66\u4e60\u3001\u65e0\u76d1\u7763\u9884\u84b8\u998f\u3001AI\u533b\u7597"}}
{"id": "2506.02978", "pdf": "https://arxiv.org/pdf/2506.02978", "abs": "https://arxiv.org/abs/2506.02978", "authors": ["Mohamed Djilani", "Thibault Simonetto", "Karim Tit", "Florian Tambon", "Paul R\u00e9camier", "Salah Ghamizi", "Maxime Cordy", "Mike Papadakis"], "title": "On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses", "categories": ["cs.LG"], "comment": null, "summary": "Recent tabular Foundational Models (FM) such as TabPFN and TabICL, leverage\nin-context learning to achieve strong performance without gradient updates or\nfine-tuning. However, their robustness to adversarial manipulation remains\nlargely unexplored. In this work, we present a comprehensive study of the\nadversarial vulnerabilities of tabular FM, focusing on both their fragility to\ntargeted test-time attacks and their potential misuse as adversarial tools. We\nshow on three benchmarks in finance, cybersecurity and healthcare, that small,\nstructured perturbations to test inputs can significantly degrade prediction\naccuracy, even when training context remain fixed. Additionally, we demonstrate\nthat tabular FM can be repurposed to generate transferable evasion to\nconventional models such as random forests and XGBoost, and on a lesser extent\nto deep tabular models. To improve tabular FM, we formulate the robustification\nproblem as an optimization of the weights (adversarial fine-tuning), or the\ncontext (adversarial in-context learning). We introduce an in-context\nadversarial training strategy that incrementally replaces the context with\nadversarial perturbed instances, without updating model weights. Our approach\nimproves robustness across multiple tabular benchmarks. Together, these\nfindings position tabular FM as both a target and a source of adversarial\nthreats, highlighting the urgent need for robust training and evaluation\npractices in this emerging paradigm.", "AI": {"tldr": "\u6587\u7ae0\u7814\u7a76\u4e86\u8868\u683c\u57fa\u7840\u6a21\u578b\uff08FM\uff09\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u53d1\u73b0\u5c0f\u89c4\u6a21\u7684\u7ed3\u6784\u5316\u6270\u52a8\u4f1a\u663e\u8457\u964d\u4f4e\u9884\u6d4b\u51c6\u786e\u6027\u3002\u540c\u65f6\uff0c\u8868\u683cFM\u8fd8\u80fd\u751f\u6210\u53ef\u8fc1\u79fb\u7684\u5bf9\u6297\u6837\u672c\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u66f4\u65b0\u6a21\u578b\u6743\u91cd\u7684\u5bf9\u6297\u6027\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u63a2\u7d22\u8868\u683c\u57fa\u7840\u6a21\u578b\u5728\u5bf9\u6297\u6027\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\u53ca\u5176\u4f5c\u4e3a\u5bf9\u6297\u6027\u5de5\u5177\u7684\u6f5c\ufffd\ufffd\ufffd\u7528\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5bf9\u6297\u6027\u5fae\u8c03\u548c\u4e0a\u4e0b\u6587\u5bf9\u6297\u6027\u5b66\u4e60\u4f18\u5316\u6a21\u578b\uff0c\u63d0\u51fa\u4e00\u79cd\u589e\u91cf\u66ff\u6362\u4e0a\u4e0b\u6587\u7684\u5bf9\u6297\u6027\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u91d1\u878d\u3001\u7f51\u7edc\u5b89\u5168\u548c\u533b\u7597\u4e09\u4e2a\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5c0f\u6270\u52a8\u663e\u8457\u964d\u4f4e\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff1b\u63d0\u51fa\u7684\u8bad\u7ec3\u7b56\u7565\u63d0\u9ad8\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u3002", "conclusion": "\u8868\u683c\u57fa\u7840\u6a21\u578b\u65e2\u662f\u5bf9\u6297\u6027\u653b\u51fb\u7684\u76ee\u6807\u4e5f\u662f\u5176\u6765\u6e90\uff0c\u4e9f\u9700\u9c81\u68d2\u6027\u8bad\u7ec3\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "keywords": "\u8868\u683c\u57fa\u7840\u6a21\u578b,\u5bf9\u6297\u6027\u653b\u51fb,\u9c81\u68d2\u6027,\u5bf9\u6297\u6027\u8bad\u7ec3"}}
{"id": "2506.02059", "pdf": "https://arxiv.org/pdf/2506.02059", "abs": "https://arxiv.org/abs/2506.02059", "authors": ["Ziwei Gong", "Pengyuan Shi", "Kaan Donbekci", "Lin Ai", "Run Chen", "David Sasu", "Zehui Wu", "Julia Hirschberg"], "title": "Learning More with Less: Self-Supervised Approaches for Low-Resource Speech Emotion Recognition", "categories": ["cs.SD", "cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "Speech Emotion Recognition (SER) has seen significant progress with deep\nlearning, yet remains challenging for Low-Resource Languages (LRLs) due to the\nscarcity of annotated data. In this work, we explore unsupervised learning to\nimprove SER in low-resource settings. Specifically, we investigate contrastive\nlearning (CL) and Bootstrap Your Own Latent (BYOL) as self-supervised\napproaches to enhance cross-lingual generalization. Our methods achieve notable\nF1 score improvements of 10.6% in Urdu, 15.2% in German, and 13.9% in Bangla,\ndemonstrating their effectiveness in LRLs. Additionally, we analyze model\nbehavior to provide insights on key factors influencing performance across\nlanguages, and also highlighting challenges in low-resource SER. This work\nprovides a foundation for developing more inclusive, explainable, and robust\nemotion recognition systems for underrepresented languages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u65e0\u76d1\u7763\u5b66\u4e60\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08LRLs\uff09\u7684\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff08SER\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u548cBYOL\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u63d0\u5347SER\u7684\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e86\u5bf9\u6bd4\u5b66\u4e60\uff08CL\uff09\u548cBootstrap Your Own Latent\uff08BYOL\uff09\u4e24\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5728\u4e4c\u5c14\u90fd\u8bed\u3001\u5fb7\u8bed\u548c\u5b5f\u52a0\u62c9\u8bed\u4e2d\u5206\u522b\u5b9e\u73b0\u4e8610.6%\u300115.2%\u548c13.9%\u7684F1\u5206\u6570\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684SER\u63d0\u4f9b\u4e86\u66f4\u5305\u5bb9\u3001\u53ef\u89e3\u91ca\u4e14\u9c81\u68d2\u7684\u7cfb\u7edf\u57fa\u7840\u3002", "keywords": "\u8bed\u97f3\u60c5\u611f\u8bc6\u522b, \u4f4e\u8d44\u6e90\u8bed\u8a00, \u65e0\u76d1\u7763\u5b66\u4e60, \u5bf9\u6bd4\u5b66\u4e60, BYOL"}}
{"id": "2506.02244", "pdf": "https://arxiv.org/pdf/2506.02244", "abs": "https://arxiv.org/abs/2506.02244", "authors": ["Bowen Xue", "Giuseppe Claudio Guarnera", "Shuang Zhao", "Zahra Montazeri"], "title": "Motion aware video generative model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in diffusion-based video generation have yielded\nunprecedented quality in visual content and semantic coherence. However,\ncurrent approaches predominantly rely on statistical learning from vast\ndatasets without explicitly modeling the underlying physics of motion,\nresulting in subtle yet perceptible non-physical artifacts that diminish the\nrealism of generated videos. This paper introduces a physics-informed frequency\ndomain approach to enhance the physical plausibility of generated videos. We\nfirst conduct a systematic analysis of the frequency-domain characteristics of\ndiverse physical motions (translation, rotation, scaling), revealing that each\nmotion type exhibits distinctive and identifiable spectral signatures. Building\non this theoretical foundation, we propose two complementary components: (1) a\nphysical motion loss function that quantifies and optimizes the conformity of\ngenerated videos to ideal frequency-domain motion patterns, and (2) a frequency\ndomain enhancement module that progressively learns to adjust video features to\nconform to physical motion constraints while preserving original network\nfunctionality through a zero-initialization strategy. Experiments across\nmultiple video diffusion architectures demonstrate that our approach\nsignificantly enhances motion quality and physical plausibility without\ncompromising visual quality or semantic alignment. Our frequency-domain\nphysical motion framework generalizes effectively across different video\ngeneration architectures, offering a principled approach to incorporating\nphysical constraints into deep learning-based video synthesis pipelines. This\nwork seeks to establish connections between data-driven models and\nphysics-based motion models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u9891\u57df\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u6269\u6563\u6a21\u578b\u751f\u6210\u89c6\u9891\u7684\u7269\u7406\u5408\u7406\u6027\uff0c\u901a\u8fc7\u9891\u57df\u5206\u6790\u548c\u4f18\u5316\u8fd0\u52a8\u6a21\u5f0f\u6765\u51cf\u5c11\u975e\u7269\u7406\u4f2a\u5f71\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u5927\u6570\u636e\u7edf\u8ba1\u5b66\u4e60\uff0c\u7f3a\u4e4f\u5bf9\u8fd0\u52a8\u7269\u7406\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u89c6\u9891\u5b58\u5728\u975e\u7269\u7406\u4f2a\u5f71\uff0c\u5f71\u54cd\u4e86\u771f\u5b9e\u611f\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff1a1) \u4e00\u79cd\u7269\u7406\u8fd0\u52a8\u635f\u5931\u51fd\u6570\uff0c\u4f18\u5316\u89c6\u9891\u8fd0\u52a8\u6a21\u5f0f\u5728\u9891\u57df\u7684\u5408\u7406\u6027\uff1b2) \u9891\u57df\u589e\u5f3a\u6a21\u5757\uff0c\u901a\u8fc7\u96f6\u521d\u59cb\u5316\u7b56\u7565\u8c03\u6574\u89c6\u9891\u7279\u5f81\u4ee5\u7b26\u5408\u7269\u7406\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u8d28\u91cf\u548c\u7269\u7406\u5408\u7406\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u89c6\u89c9\u8d28\u91cf\u548c\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89c6\u9891\u751f\u6210\u67b6\u6784\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5efa\u7acb\u4e86\u6570\u636e\u9a71\u52a8\u6a21\u578b\u4e0e\u57fa\u4e8e\u7269\u7406\u8fd0\u52a8\u6a21\u578b\u7684\u8054\u7cfb\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5f15\u5165\u7269\u7406\u7ea6\u675f\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "keywords": "\u89c6\u9891\u751f\u6210, \u6269\u6563\u6a21\u578b, \u9891\u57df\u5206\u6790, \u7269\u7406\u5408\u7406\u6027, \u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2506.02986", "pdf": "https://arxiv.org/pdf/2506.02986", "abs": "https://arxiv.org/abs/2506.02986", "authors": ["Nathan Buskulic", "Jalal Fadil", "Yvain Qu\u00e9au"], "title": "Implicit Regularization of the Deep Inverse Prior Trained with Inertia", "categories": ["cs.LG"], "comment": null, "summary": "Solving inverse problems with neural networks benefits from very few\ntheoretical guarantees when it comes to the recovery guarantees. We provide in\nthis work convergence and recovery guarantees for self-supervised neural\nnetworks applied to inverse problems, such as Deep Image/Inverse Prior, and\ntrained with inertia featuring both viscous and geometric Hessian-driven\ndampings. We study both the continuous-time case, i.e., the trajectory of a\ndynamical system, and the discrete case leading to an inertial algorithm with\nan adaptive step-size. We show in the continuous-time case that the network can\nbe trained with an optimal accelerated exponential convergence rate compared to\nthe rate obtained with gradient flow. We also show that training a network with\nour inertial algorithm enjoys similar recovery guarantees though with a less\nsharp linear convergence rate.", "AI": {"tldr": "\u672c\u6587\u4e3a\u81ea\u76d1\u7763\u795e\u7ecf\u7f51\u7edc\u5728\u9006\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6536\u655b\u6027\u548c\u6062\u590d\u6027\u4fdd\u8bc1\uff0c\u7814\u7a76\u4e86\u8fde\u7eed\u65f6\u95f4\u548c\u79bb\u6563\u65f6\u95f4\u60c5\u51b5\u4e0b\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u4f18\u5316\u7684\u52a0\u901f\u6307\u6570\u6536\u655b\u7387\u3002", "motivation": "\u5f53\u524d\u9006\u95ee\u9898\u4e2d\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u7684\u7406\u8bba\u4fdd\u8bc1\u8f83\u5c11\uff0c\u672c\u6587\u65e8\u5728\u4e3a\u81ea\u76d1\u7763\u795e\u7ecf\u7f51\u7edc\u5728\u9006\u95ee\u9898\u4e2d\u7684\u8bad\u7ec3\u63d0\u4f9b\u7406\u8bba\u652f\u6301\uff0c\u7279\u522b\u662f\u9488\u5bf9\u60ef\u6027\u8bad\u7ec3\u65b9\u6cd5\u7684\u7814\u7a76\u3002", "method": "\u7814\u7a76\u4e86\u8fde\u7eed\u65f6\u95f4\u548c\u79bb\u6563\u65f6\u95f4\u4e0b\u7684\u60ef\u6027\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u7c98\u6027\u548c\u51e0\u4f55Hessian\u9a71\u52a8\u7684\u963b\u5c3c\uff0c\u5206\u6790\u4e86\u7f51\u7edc\u7684\u6536\u655b\u6027\u548c\u6062\u590d\u6027\u3002", "result": "\u8fde\u7eed\u65f6\u95f4\u60c5\u51b5\u4e0b\uff0c\u7f51\u7edc\u8bad\u7ec3\u5b9e\u73b0\u4e86\u4f18\u5316\u7684\u52a0\u901f\u6307\u6570\u6536\u655b\u7387\uff1b\u79bb\u6563\u65f6\u95f4\u60c5\u51b5\u4e0b\uff0c\u5c55\u793a\u4e86\u7c7b\u4f3c\u7684\u6062\u590d\u6027\u4fdd\u8bc1\uff0c\u4f46\u6536\u655b\u7387\u8f83\u7ebf\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u81ea\u76d1\u7763\u795e\u7ecf\u7f51\u7edc\u5728\u9006\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u8bc1\u660e\u4e86\u60ef\u6027\u8bad\u7ec3\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u8fde\u7eed\u65f6\u95f4\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc, \u9006\u95ee\u9898, \u81ea\u76d1\u7763, \u6536\u655b\u6027, \u6062\u590d\u6027, \u60ef\u6027\u8bad\u7ec3"}}
{"id": "2506.03028", "pdf": "https://arxiv.org/pdf/2506.03028", "abs": "https://arxiv.org/abs/2506.03028", "authors": ["Junde Xu", "Zijun Gao", "Xinyi Zhou", "Jie Hu", "Xingyi Cheng", "Le Song", "Guangyong Chen", "Pheng-Ann Heng", "Jiezhong Qiu"], "title": "Protein Inverse Folding From Structure Feedback", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "The inverse folding problem, aiming to design amino acid sequences that fold\ninto desired three-dimensional structures, is pivotal for various\nbiotechnological applications. Here, we introduce a novel approach leveraging\nDirect Preference Optimization (DPO) to fine-tune an inverse folding model\nusing feedback from a protein folding model. Given a target protein structure,\nwe begin by sampling candidate sequences from the inverse-folding model, then\npredict the three-dimensional structure of each sequence with the folding model\nto generate pairwise structural-preference labels. These labels are used to\nfine-tune the inverse-folding model under the DPO objective. Our results on the\nCATH 4.2 test set demonstrate that DPO fine-tuning not only improves sequence\nrecovery of baseline models but also leads to a significant improvement in\naverage TM-Score from 0.77 to 0.81, indicating enhanced structure similarity.\nFurthermore, iterative application of our DPO-based method on challenging\nprotein structures yields substantial gains, with an average TM-Score increase\nof 79.5\\% with regard to the baseline model. This work establishes a promising\ndirection for enhancing protein sequence design ability from structure feedback\nby effectively utilizing preference optimization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u86cb\u767d\u8d28\u6298\u53e0\u6a21\u578b\u7684\u53cd\u9988\u6765\u4f18\u5316\u9006\u6298\u53e0\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u86cb\u767d\u8d28\u7ed3\u6784\u7684\u5e8f\u5217\u6062\u590d\u80fd\u529b\u548c\u7ed3\u6784\u76f8\u4f3c\u6027\u3002", "motivation": "\u89e3\u51b3\u9006\u6298\u53e0\u95ee\u9898\uff08\u8bbe\u8ba1\u80fd\u591f\u6298\u53e0\u6210\u7279\u5b9a\u4e09\u7ef4\u7ed3\u6784\u7684\u6c28\u57fa\u9178\u5e8f\u5217\uff09\u662f\u751f\u7269\u6280\u672f\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u7b56\u7565\u3002", "method": "\u7ed3\u5408\u9006\u6298\u53e0\u6a21\u578b\u548c\u86cb\u767d\u8d28\u6298\u53e0\u6a21\u578b\u7684\u53cd\u9988\uff0c\u901a\u8fc7DPO\u76ee\u6807\u51fd\u6570\u5bf9\u5019\u9009\u5e8f\u5217\u8fdb\u884c\u4f18\u5316\uff0c\u5229\u7528\u7ed3\u6784\u504f\u597d\u6807\u7b7e\u8fed\u4ee3\u8c03\u6574\u6a21\u578b\u3002", "result": "\u5728CATH 4.2\u6d4b\u8bd5\u96c6\u4e0a\uff0cTM-Score\u4ece0.77\u63d0\u5347\u81f30.81\uff0c\u4e14\u8fed\u4ee3\u4f18\u5316\u540e\u5bf9\u6311\u6218\u6027\u7ed3\u6784\u7684\u5e73\u5747TM-Score\u63d0\u5347\u8fbe79.5%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u504f\u597d\u4f18\u5316\u6709\u6548\u63d0\u5347\u4e86\u86cb\u767d\u8d28\u5e8f\u5217\u8bbe\u8ba1\u80fd\u529b\uff0c\u4e3a\u751f\u7269\u6280\u672f\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u9006\u6298\u53e0\u95ee\u9898\uff0c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\uff0c\u86cb\u767d\u8d28\u8bbe\u8ba1\uff0c\u7ed3\u6784\u53cd\u9988\uff0cTM-Score"}}
{"id": "2506.02259", "pdf": "https://arxiv.org/pdf/2506.02259", "abs": "https://arxiv.org/abs/2506.02259", "authors": ["Yichi Zhang", "Shengwei Xu", "David Pennock", "Grant Schoenebeck"], "title": "Stochastically Dominant Peer Prediction", "categories": ["cs.GT", "cs.AI"], "comment": "29 pages, 3 figures", "summary": "Eliciting reliable human feedback is essential for many machine learning\ntasks, such as learning from noisy labels and aligning AI systems with human\npreferences. Peer prediction mechanisms incentivize truthful reporting without\nground truth verification by scoring agents based on correlations with peers.\nTraditional mechanisms, which ensure that truth-telling maximizes the expected\nscores in equilibrium, can elicit honest information while assuming agents'\nutilities are linear functions of their scores. However, in practice,\nnon-linear payment rules are usually preferred, or agents' utilities are\ninherently non-linear.\n  We propose stochastically dominant truthfulness (SD-truthfulness) as a\nstronger guarantee: the score distribution of truth-telling stochastically\ndominates all other strategies, incentivizing truthful reporting for a wide\nrange of monotone utility functions. Our first observation is that no existing\npeer prediction mechanism naturally satisfies this criterion without strong\nassumptions. A simple solution -- rounding scores into binary lotteries -- can\nenforce SD-truthfulness, but often degrades sensitivity, a key property related\nto fairness and statistical efficiency. We demonstrate how a more careful\napplication of rounding can better preserve sensitivity. Furthermore, we\nintroduce a new enforced agreement (EA) mechanism that is theoretically\nguaranteed to be SD-truthful in binary-signal settings under mild assumptions,\nand empirically achieves the highest sensitivity among all known SD-truthful\nmechanisms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u968f\u673a\u5360\u4f18\u771f\u5b9e\u6027\uff08SD-truthfulness\uff09\u7684\u65b0\u673a\u5236\uff0c\u7528\u4e8e\u5728\u66f4\u5e7f\u6cdb\u7684\u5355\u8c03\u6548\u7528\u51fd\u6570\u8303\u56f4\u5185\u6fc0\u52b1\u771f\u5b9e\u53cd\u9988\uff0c\u6539\u8fdb\u4e86\u4f20\u7edf\u540c\u884c\u9884\u6d4b\u673a\u5236\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5728\u673a\u5668\u5b66\u4e60\u548cAI\u5bf9\u9f50\u4efb\u52a1\u4e2d\uff0c\u83b7\u53d6\u53ef\u9760\u7684\u4eba\u7c7b\u53cd\u9988\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u540c\u884c\u9884\u6d4b\u673a\u5236\u5728\u6fc0\u52b1\u771f\u5b9e\u53cd\u9988\u65f6\u4f9d\u8d56\u4e8e\u7ebf\u6027\u6548\u7528\u51fd\u6570\u7684\u5047\u8bbe\uff0c\u800c\u5b9e\u9645\u4e2d\u975e\u7ebf\u6027\u7684\u652f\u4ed8\u89c4\u5219\u6216\u6548\u7528\u51fd\u6570\u66f4\u4e3a\u5e38\u89c1\u3002", "method": "\u901a\u8fc7\u5c06\u5f97\u5206\u8f6c\u6362\u4e3a\u4e8c\u5143\u5f69\u7968\u7684\u7b80\u5355\u65b9\u6cd5\u5b9e\u73b0\u968f\u673a\u5360\u4f18\u771f\u5b9e\u6027\uff0c\u4f46\u53d1\u73b0\u8fd9\u4f1a\u964d\u4f4e\u654f\u611f\u6027\u3002\u8bba\u6587\u8fdb\u4e00\u6b65\u63d0\u51fa\u66f4\u7cbe\u7ec6\u7684\u820d\u5165\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u5f3a\u5236\u4e00\u81f4\u6027\uff08EA\uff09\u673a\u5236\u3002", "result": "EA\u673a\u5236\u5728\u4e8c\u5143\u4fe1\u53f7\u8bbe\u7f6e\u4e0b\u5728\u7406\u8bba\u4e0a\u4fdd\u8bc1SD\u771f\u5b9e\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u6240\u6709\u5df2\u77e5SD\u771f\u5b9e\u6027\u673a\u5236\u4e2d\u5177\u6709\u6700\u9ad8\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u968f\u673a\u5360\u4f18\u771f\u5b9e\u6027\u673a\u5236\u4e3a\u6fc0\u52b1\u771f\u5b9e\u53cd\u9988\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u4fdd\u8bc1\uff0c\u5c24\u5176\u662f\u5728\u975e\u7ebf\u6027\u6548\u7528\u51fd\u6570\u7684\u60c5\u51b5\u4e0b\uff0cEA\u673a\u5236\u8868\u73b0\u51fa\u8272\u3002", "keywords": "\u540c\u884c\u9884\u6d4b,\u968f\u673a\u5360\u4f18\u771f\u5b9e\u6027,\u6fc0\u52b1\u673a\u5236,\u4eba\u7c7b\u53cd\u9988,\u673a\u5668\u5b66\u4e60"}}
{"id": "2506.03037", "pdf": "https://arxiv.org/pdf/2506.03037", "abs": "https://arxiv.org/abs/2506.03037", "authors": ["Shubhendu Trivedi", "Brian D. Nord"], "title": "On the Need to Align Intent and Implementation in Uncertainty Quantification for Machine Learning", "categories": ["cs.LG", "stat.ME", "stat.ML"], "comment": null, "summary": "Quantifying uncertainties for machine learning (ML) models is a foundational\nchallenge in modern data analysis. This challenge is compounded by at least two\nkey aspects of the field: (a) inconsistent terminology surrounding uncertainty\nand estimation across disciplines, and (b) the varying technical requirements\nfor establishing trustworthy uncertainties in diverse problem contexts. In this\nposition paper, we aim to clarify the depth of these challenges by identifying\nthese inconsistencies and articulating how different contexts impose distinct\nepistemic demands. We examine the current landscape of estimation targets\n(e.g., prediction, inference, simulation-based inference), uncertainty\nconstructs (e.g., frequentist, Bayesian, fiducial), and the approaches used to\nmap between them. Drawing on the literature, we highlight and explain examples\nof problematic mappings. To help address these issues, we advocate for\nstandards that promote alignment between the \\textit{intent} and\n\\textit{implementation} of uncertainty quantification (UQ) approaches. We\ndiscuss several axes of trustworthiness that are necessary (if not sufficient)\nfor reliable UQ in ML models, and show how these axes can inform the design and\nevaluation of uncertainty-aware ML systems. Our practical recommendations focus\non scientific ML, offering illustrative cases and use scenarios, particularly\nin the context of simulation-based inference (SBI).", "AI": {"tldr": "\u8be5\u7acb\u573a\u8bba\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u4e2d\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u6311\u6218\uff0c\u5305\u62ec\u672f\u8bed\u4e0d\u4e00\u81f4\u548c\u4e0d\u540c\u95ee\u9898\u80cc\u666f\u4e0b\u7684\u6280\u672f\u8981\u6c42\uff0c\u5e76\u63d0\u51fa\u4e86\u6807\u51c6\u5316\u5efa\u8bae\u4ee5\u63d0\u9ad8\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u4e2d\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u9762\u4e34\u7684\u672f\u8bed\u4e0d\u4e00\u81f4\u548c\u6280\u672f\u591a\u6837\u6027\u95ee\u9898\uff0c\u4fc3\u8fdb\u66f4\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f53\u524d\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u76ee\u6807\uff08\u5982\u9884\u6d4b\u3001\u63a8\u65ad\u3001\u57fa\u4e8e\u6a21\u62df\u7684\u63a8\u65ad\uff09\u548c\u4e0d\u786e\u5b9a\u6027\u6784\u5efa\uff08\u5982\u9891\u7387\u6d3e\u3001\u8d1d\u53f6\u65af\u3001\u4fe1\u4efb\u6d3e\uff09\uff0c\u63d0\u51fa\u4e86\u6807\u51c6\u5316\u7684\u5efa\u8bae\u3002", "result": "\u5f3a\u8c03\u4e86\u95ee\u9898\u6620\u5c04\u7684\u5b9e\u4f8b\uff0c\u5e76\u63d0\u51fa\u4e86\u4fc3\u8fdb\u610f\u56fe\u4e0e\u5b9e\u73b0\u4e00\u81f4\u7684\u6807\u51c6\uff0c\u4ee5\u786e\u4fdd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u5021\u901a\u8fc7\u6807\u51c6\u5316\u548c\u4fe1\u4efb\u8f74\u8bbe\u8ba1\u6765\u63d0\u5347\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5728ML\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u79d1\u5b66ML\u548c\u57fa\u4e8e\u6a21\u62df\u7684\u63a8\u65ad\u4e2d\u3002", "keywords": "\u673a\u5668\u5b66\u4e60\u3001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u6807\u51c6\u5316\u3001\u4fe1\u4efb\u8f74\u3001\u79d1\u5b66ML\u3001\u6a21\u62df\u63a8\u65ad"}}
{"id": "2506.02088", "pdf": "https://arxiv.org/pdf/2506.02088", "abs": "https://arxiv.org/abs/2506.02088", "authors": ["Alef Iury Siqueira Ferreira", "Lucas Rafael Gris", "Alexandre Ferro Filho", "Lucas \u00d3lives", "Daniel Ribeiro", "Luiz Fernando", "Fernanda Lustosa", "Rodrigo Tanaka", "Frederico Santos de Oliveira", "Arlindo Galv\u00e3o Filho"], "title": "Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion and Prosodic Features for the Speech Emotion Recognition in Naturalistic Conditions Challenge at Interspeech 2025", "categories": ["cs.SD", "cs.CL", "cs.LG"], "comment": null, "summary": "Training SER models in natural, spontaneous speech is especially challenging\ndue to the subtle expression of emotions and the unpredictable nature of\nreal-world audio. In this paper, we present a robust system for the INTERSPEECH\n2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, focusing\non categorical emotion recognition. Our method combines state-of-the-art audio\nmodels with text features enriched by prosodic and spectral cues. In\nparticular, we investigate the effectiveness of Fundamental Frequency (F0)\nquantization and the use of a pretrained audio tagging model. We also employ an\nensemble model to improve robustness. On the official test set, our system\nachieved a Macro F1-score of 39.79% (42.20% on validation). Our results\nunderscore the potential of these methods, and analysis of fusion techniques\nconfirmed the effectiveness of Graph Attention Networks. Our source code is\npublicly available.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9488\u5bf9\u81ea\u7136\u8bed\u97f3\u4e2d\u60c5\u611f\u8bc6\u522b\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u97f3\u9891\u548c\u6587\u672c\u7279\u5f81\u7684SER\u7cfb\u7edf\uff0c\u5e76\u5728INTERSPEECH 2025\u6311\u6218\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u81ea\u7136\u8bed\u97f3\u4e2d\u7684\u60c5\u611f\u8868\u8fbe\u5fae\u5999\u4e14\u4e0d\u53ef\u9884\u6d4b\uff0c\u8bad\u7ec3SER\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u7ed3\u5408\u6700\u5148\u8fdb\u7684\u97f3\u9891\u6a21\u578b\u548c\u5305\u542b\u97f5\u5f8b\u4e0e\u9891\u8c31\u7ebf\u7d22\u7684\u6587\u672c\u7279\u5f81\uff0c\u7814\u7a76\u4e86F0\u91cf\u5316\u548c\u9884\u8bad\u7ec3\u97f3\u9891\u6807\u8bb0\u6a21\u578b\u7684\u6548\u679c\uff0c\u5e76\u91c7\u7528\u96c6\u6210\u6a21\u578b\u548cGraph Attention Networks\u3002", "result": "\u5728\u5b98\u65b9\u6d4b\u8bd5\u96c6\u4e0aMacro F1-score\u4e3a39.79%\uff08\u9a8c\u8bc1\u96c642.20%\uff09\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5c55\u793a\u4e86\u6f5c\u529b\uff0c\u7279\u522b\u662fGraph Attention Networks\u5728\u878d\u5408\u6280\u672f\u4e2d\u7684\u6709\u6548\u6027\u3002", "keywords": "SER, \u81ea\u7136\u8bed\u97f3, \u60c5\u611f\u8bc6\u522b, F0\u91cf\u5316, Graph Attention Networks"}}
{"id": "2506.02262", "pdf": "https://arxiv.org/pdf/2506.02262", "abs": "https://arxiv.org/abs/2506.02262", "authors": ["Sebe Vanbrabant", "Gustavo Rovelo Ruiz", "Davy Vanacken"], "title": "Composable Building Blocks for Controllable and Transparent Interactive AI Systems", "categories": ["cs.HC", "cs.AI", "H.5.2; I.2.0"], "comment": "Accepted to The 3rd Workshop on Engineering Interactive Systems\n  Embedding AI Technologies, EICS 2025", "summary": "While the increased integration of AI technologies into interactive systems\nenables them to solve an equally increasing number of tasks, the black box\nproblem of AI models continues to spread throughout the interactive system as a\nwhole. Explainable AI (XAI) techniques can make AI models more accessible by\nemploying post-hoc methods or transitioning to inherently interpretable models.\nWhile this makes individual AI models clearer, the overarching system\narchitecture remains opaque. To this end, we propose an approach to represent\ninteractive systems as sequences of structural building blocks, such as AI\nmodels and control mechanisms grounded in the literature. These can then be\nexplained through accompanying visual building blocks, such as XAI techniques.\nThe flow and APIs of the structural building blocks form an explicit overview\nof the system. This serves as a communication basis for both humans and\nautomated agents like LLMs, aligning human and machine interpretability of AI\nmodels. We discuss a selection of building blocks and concretize our flow-based\napproach in an architecture and accompanying prototype interactive system.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u53ef\u89c6\u5316\u6784\u5efa\u5757\u6765\u63d0\u9ad8\u4ea4\u4e92\u5f0f\u7cfb\u7edf\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3AI\u9ed1\u76d2\u95ee\u9898\u3002", "motivation": "\u7531\u4e8eAI\u6280\u672f\u8d8a\u6765\u8d8a\u591a\u5730\u96c6\u6210\u5230\u4ea4\u4e92\u7cfb\u7edf\u4e2d\uff0c\u7cfb\u7edf\u7684\u9ed1\u76d2\u95ee\u9898\u6108\u53d1\u7a81\u51fa\u3002\u73b0\u6709\u7684XAI\u6280\u672f\u867d\u7136\u80fd\u63d0\u5347\u5355\u4e2a\u6a21\u578b\u7684\u900f\u660e\u5ea6\uff0c\u4f46\u6574\u4e2a\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u4ecd\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5c06\u4ea4\u4e92\u7cfb\u7edf\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u7684\u6a21\u5757\uff08\u5982AI\u6a21\u578b\u548c\u63a7\u5236\u673a\u5236\uff09\uff0c\u5e76\u7ed3\u5408\u53ef\u89c6\u5316\u6784\u5efa\u5757\uff08\u5982XAI\u6280\u672f\uff09\uff0c\u5f62\u6210\u4e00\u4e2a\u6e05\u6670\u7684\u7cfb\u7edf\u6982\u89c8\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u7a0b\u7684\u67b6\u6784\u548c\u539f\u578b\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u548c\u53ef\u89c6\u5316\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7cfb\u7edf\u7ea7\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4eba\u7c7b\u548c\u81ea\u52a8\u5316\u4ee3\u7406\uff08\u5982LLMs\uff09\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u89e3\u91ca\u57fa\u7840\uff0c\u63d0\u5347\u4e86AI\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u7406\u89e3\u6027\u3002", "keywords": "\u53ef\u89e3\u91caAI, \u4ea4\u4e92\u7cfb\u7edf, \u6a21\u5757\u5316\u8bbe\u8ba1, \u53ef\u89c6\u5316, \u9ed1\u76d2\u95ee\u9898"}}
{"id": "2506.03043", "pdf": "https://arxiv.org/pdf/2506.03043", "abs": "https://arxiv.org/abs/2506.03043", "authors": ["Nikita Puchkin", "Iurii Pustovalov", "Yuri Sapronov", "Denis Suchkov", "Alexey Naumov", "Denis Belomestny"], "title": "Sample complexity of Schr\u00f6dinger potential estimation", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": "60 pages", "summary": "We address the problem of Schr\\\"odinger potential estimation, which plays a\ncrucial role in modern generative modelling approaches based on Schr\\\"odinger\nbridges and stochastic optimal control for SDEs. Given a simple prior diffusion\nprocess, these methods search for a path between two given distributions\n$\\rho_0$ and $\\rho_T^*$ requiring minimal efforts. The optimal drift in this\ncase can be expressed through a Schr\\\"odinger potential. In the present paper,\nwe study generalization ability of an empirical Kullback-Leibler (KL) risk\nminimizer over a class of admissible log-potentials aimed at fitting the\nmarginal distribution at time $T$. Under reasonable assumptions on the target\ndistribution $\\rho_T^*$ and the prior process, we derive a non-asymptotic\nhigh-probability upper bound on the KL-divergence between $\\rho_T^*$ and the\nterminal density corresponding to the estimated log-potential. In particular,\nwe show that the excess KL-risk may decrease as fast as $O(\\log^2 n / n)$ when\nthe sample size $n$ tends to infinity even if both $\\rho_0$ and $\\rho_T^*$ have\nunbounded supports.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8eKL\u6563\u5ea6\u6700\u5c0f\u5316\u7684Schr\u00f6dinger\u52bf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5728\u9ad8\u6982\u7387\u4e0b\u7ec8\u7aef\u5bc6\u5ea6\u4e0e\u76ee\u6807\u5206\u5e03\u95f4\u7684KL\u6563\u5ea6\u754c\uff0c\u6837\u672c\u91cf\u589e\u52a0\u65f6\u98ce\u9669\u4e0b\u964d\u901f\u5ea6\u53ef\u8fbeO(log\u00b2n/n)\u3002", "motivation": "\u89e3\u51b3Schr\u00f6dinger\u52bf\u4f30\u8ba1\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u5728\u751f\u6210\u5efa\u6a21\u548c\u968f\u673a\u6700\u4f18\u63a7\u5236\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728Schr\u00f6dinger\u6865\u548cSDEs\u7684\u80cc\u666f\u4e0b\u3002", "method": "\u4f7f\u7528\u7ecf\u9a8cKL\u98ce\u9669\u6700\u5c0f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6570\u52bf\u7684\u7ea6\u675f\u7c7b\u62df\u5408\u7ec8\u7aef\u8fb9\u9645\u5206\u5e03\uff0c\u5e76\u5206\u6790\u5176\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u9ad8\u6982\u7387\u4e0b\uff0c\u63a8\u5bfc\u4e86\u7ec8\u7aef\u5bc6\u5ea6\u4e0e\u76ee\u6807\u5206\u5e03\u95f4KL\u6563\u5ea6\u7684\u975e\u6e10\u8fd1\u4e0a\u754c\uff0c\u8bc1\u660e\u98ce\u9669\u4e0b\u964d\u901f\u5ea6\u53ef\u8fbeO(log\u00b2n/n)\u3002", "conclusion": "\u5373\u4f7f\u5728\u65e0\u754c\u652f\u6491\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4f30\u8ba1Schr\u00f6dinger\u52bf\uff0c\u5e76\u5c55\u793a\u4e86\u5feb\u901f\u6536\u655b\u6027\u3002", "keywords": "Schr\u00f6dinger\u52bf\u3001KL\u6563\u5ea6\u3001\u6cdb\u5316\u80fd\u529b\u3001\u968f\u673a\u6700\u4f18\u63a7\u5236\u3001\u751f\u6210\u5efa\u6a21"}}
{"id": "2506.02267", "pdf": "https://arxiv.org/pdf/2506.02267", "abs": "https://arxiv.org/abs/2506.02267", "authors": ["Xue Xia", "Saurabh Vishwas Joshi", "Kousik Rajesh", "Kangnan Li", "Yangyi Lu", "Nikil Pancha", "Dhruvil Deven Badani", "Jiajing Xu", "Pong Eksombatchai"], "title": "TransAct V2: Lifelong User Action Sequence Modeling on Pinterest Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Modeling user action sequences has become a popular focus in industrial\nrecommendation system research, particularly for Click-Through Rate (CTR)\nprediction tasks. However, industry-scale CTR models often rely on short user\nsequences, limiting their ability to capture long-term behavior. Additionally,\nthese models typically lack an integrated action-prediction task within a\npoint-wise ranking framework, reducing their predictive power. They also rarely\naddress the infrastructure challenges involved in efficiently serving\nlarge-scale sequential models. In this paper, we introduce TransAct V2, a\nproduction model for Pinterest's Homefeed ranking system, featuring three key\ninnovations: (1) leveraging very long user sequences to improve CTR\npredictions, (2) integrating a Next Action Loss function for enhanced user\naction forecasting, and (3) employing scalable, low-latency deployment\nsolutions tailored to handle the computational demands of extended user action\nsequences.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86TransAct V2\u6a21\u578b\uff0c\u901a\u8fc7\u957f\u5e8f\u5217\u3001Next Action Loss\u548c\u9ad8\u6548\u90e8\u7f72\u63d0\u5347CTR\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u6a21\u578b\u5728\u5904\u7406\u957f\u5e8f\u5217\u7528\u6237\u884c\u4e3a\u548c\u9884\u6d4b\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u957f\u5e8f\u5217\u6570\u636e\u3001\u5f15\u5165Next Action Loss\u51fd\u6570\u3001\u4f18\u5316\u90e8\u7f72\u65b9\u6848\u3002", "result": "\u63d0\u5347CTR\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u89e3\u51b3\u5927\u89c4\u6a21\u5e8f\u5217\u6a21\u578b\u90e8\u7f72\u95ee\u9898\u3002", "conclusion": "TransAct V2\u5728Pinterest\u63a8\u8350\u7cfb\u7edf\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u6027\u3002", "keywords": "\u63a8\u8350\u7cfb\u7edf, CTR\u9884\u6d4b, \u957f\u5e8f\u5217, Next Action Loss, \u90e8\u7f72\u4f18\u5316"}}
{"id": "2506.03062", "pdf": "https://arxiv.org/pdf/2506.03062", "abs": "https://arxiv.org/abs/2506.03062", "authors": ["Qining Zhang", "Tanner Fiez", "Yi Liu", "Wenyang Liu"], "title": "Multi-Metric Adaptive Experimental Design under Fixed Budget with Validation", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Standard A/B tests in online experiments face statistical power challenges\nwhen testing multiple candidates simultaneously, while adaptive experimental\ndesigns (AED) alone fall short in inferring experiment statistics such as the\naverage treatment effect, especially with many metrics (e.g., revenue, safety)\nand heterogeneous variances. This paper proposes a fixed-budget multi-metric\nAED framework with a two-phase structure: an adaptive exploration phase to\nidentify the best treatment, and a validation phase with an A/B test to verify\nthe treatment's quality and infer statistics. We propose SHRVar, which\ngeneralizes sequential halving (SH) (Karnin et al., 2013) with a novel\nrelative-variance-based sampling and an elimination strategy built on reward\nz-values. It achieves a provable error probability that decreases\nexponentially, where the exponent generalizes the complexity measure for SH\n(Karnin et al., 2013) and SHVar (Lalitha et al., 2023) with homogeneous and\nheterogeneous variances, respectively. Numerical experiments verify our\nanalysis and demonstrate the superior performance of this new framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56fa\u5b9a\u9884\u7b97\u7684\u591a\u6307\u6807\u81ea\u9002\u5e94\u5b9e\u9a8c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7ed3\u6784\uff08\u63a2\u7d22\u548c\u9a8c\u8bc1\uff09\u89e3\u51b3\u4f20\u7edfA/B\u6d4b\u8bd5\u5728\u591a\u5019\u9009\u8005\u548c\u591a\u6307\u6807\u60c5\u51b5\u4e0b\u7684\u7edf\u8ba1\u6548\u7387\u95ee\u9898\u3002\u63d0\u51fa\u7684SHRVar\u65b9\u6cd5\u57fa\u4e8e\u76f8\u5bf9\u65b9\u5dee\u91c7\u6837\u548c\u5956\u52b1z\u503c\u6dd8\u6c70\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfA/B\u6d4b\u8bd5\u5728\u591a\u5019\u9009\u8005\u6d4b\u8bd5\u65f6\u7edf\u8ba1\u6548\u7387\u4f4e\uff0c\u81ea\u9002\u5e94\u5b9e\u9a8c\u8bbe\u8ba1\uff08AED\uff09\u5728\u591a\u6307\u6807\u548c\u5f02\u65b9\u5dee\u60c5\u51b5\u4e0b\u65e0\u6cd5\u51c6\u786e\u63a8\u65ad\u5b9e\u9a8c\u7edf\u8ba1\u91cf\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u81ea\u9002\u5e94\u63a2\u7d22\u9636\u6bb5\u8bc6\u522b\u6700\u4f73\u5904\u7406\uff0c\u9a8c\u8bc1\u9636\u6bb5\u901a\u8fc7A/B\u6d4b\u8bd5\u9a8c\u8bc1\u5904\u7406\u8d28\u91cf\u5e76\u63a8\u65ad\u7edf\u8ba1\u91cf\u3002\u63d0\u51faSHRVar\u65b9\u6cd5\uff0c\u7ed3\u5408\u76f8\u5bf9\u65b9\u5dee\u91c7\u6837\u548c\u5956\u52b1z\u503c\u6dd8\u6c70\u7b56\u7565\u3002", "result": "SHRVar\u65b9\u6cd5\u7684\u9519\u8bef\u6982\u7387\u5448\u6307\u6570\u4e0b\u964d\uff0c\u6027\u80fd\u4f18\u4e8eSH\u548cSHVar\u65b9\u6cd5\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u548c\u65b9\u6cd5\u5728\u591a\u6307\u6807\u548c\u5f02\u65b9\u5dee\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u9a8c\u6548\u7387\u548c\u7edf\u8ba1\u63a8\u65ad\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002", "keywords": "\u81ea\u9002\u5e94\u5b9e\u9a8c\u8bbe\u8ba1, A/B\u6d4b\u8bd5, \u591a\u6307\u6807, \u5f02\u65b9\u5dee, SHRVar"}}
{"id": "2506.02160", "pdf": "https://arxiv.org/pdf/2506.02160", "abs": "https://arxiv.org/abs/2506.02160", "authors": ["Madan Krishnamurthy", "Daniel Korn", "Melissa A Haendel", "Christopher J Mungall", "Anne E Thessen"], "title": "A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE) Using Embeddings and Clustering", "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": null, "summary": "This research aims to develop a dynamic and scalable framework to facilitate\nharmonization of Common Data Elements (CDEs) across heterogeneous biomedical\ndatasets by addressing challenges such as semantic heterogeneity, structural\nvariability, and context dependence to streamline integration, enhance\ninteroperability, and accelerate scientific discovery. Our methodology\nleverages Large Language Models (LLMs) for context-aware text embeddings that\nconvert CDEs into dense vectors capturing semantic relationships and patterns.\nThese embeddings are clustered using Hierarchical Density-Based Spatial\nClustering of Applications with Noise (HDBSCAN) to group semantically similar\nCDEs. The framework incorporates four key steps: (1) LLM-based text embedding\nto mathematically represent semantic context, (2) unsupervised clustering of\nembeddings via HDBSCAN, (3) automated labeling using LLM summarization, and (4)\nsupervised learning to train a classifier assigning new or unclustered CDEs to\nlabeled clusters. Evaluated on the NIH NLM CDE Repository with over 24,000\nCDEs, the system identified 118 meaningful clusters at an optimized minimum\ncluster size of 20. The classifier achieved 90.46 percent overall accuracy,\nperforming best in larger categories. External validation against Gravity\nProjects Social Determinants of Health domains showed strong agreement\n(Adjusted Rand Index 0.52, Normalized Mutual Information 0.78), indicating that\nembeddings effectively capture cluster characteristics. This adaptable and\nscalable approach offers a practical solution to CDE harmonization, improving\nselection efficiency and supporting ongoing data interoperability.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u52a8\u6001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u805a\u7c7b\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u751f\u7269\u533b\u5b66\u6570\u636e\u96c6\u4e2d\u901a\u7528\u6570\u636e\u5143\u7d20\u7684\u8bed\u4e49\u5f02\u8d28\u6027\u548c\u7ed3\u6784\u53d8\u5f02\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u6574\u5408\u6548\u7387\u548c\u4e92\u64cd\u4f5c\u6027\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u6570\u636e\u96c6\u4e2d\u901a\u7528\u6570\u636e\u5143\u7d20\uff08CDEs\uff09\u7684\u8bed\u4e49\u5f02\u8d28\u6027\u3001\u7ed3\u6784\u53d8\u5f02\u6027\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u6311\u6218\u963b\u788d\u4e86\u6570\u636e\u6574\u5408\u548c\u4e92\u64cd\u4f5c\u6027\uff0c\u4e9f\u9700\u4e00\u79cd\u52a8\u6001\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a(1) \u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6587\u672c\u5d4c\u5165\uff0c(2) \u4f7f\u7528HDBSCAN\u5bf9\u5d4c\u5165\u8fdb\u884c\u65e0\u76d1\u7763\u805a\u7c7b\uff0c(3) \u5229\u7528LLM\u81ea\u52a8\u6807\u8bb0\uff0c(4) \u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\u5206\u7c7b\u5668\u3002", "result": "\u5728\u5305\u542b24,000\u591a\u4e2aCDEs\u7684NIH NLM CDE\u5e93\u4e2d\uff0c\u7cfb\u7edf\u8bc6\u522b\u51fa118\u4e2a\u6709\u610f\u4e49\u7684\u805a\u7c7b\uff0c\u5206\u7c7b\u5668\u51c6\u786e\u7387\u8fbe90.46%\u3002\u5916\u90e8\u9a8c\u8bc1\u663e\u793a\u5f3a\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aCDE\u534f\u8c03\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u9009\u62e9\u6548\u7387\uff0c\u652f\u6301\u6570\u636e\u4e92\u64cd\u4f5c\u6027\u3002", "keywords": "\u751f\u7269\u533b\u5b66\u6570\u636e, \u901a\u7528\u6570\u636e\u5143\u7d20, \u5927\u8bed\u8a00\u6a21\u578b, \u8bed\u4e49\u5f02\u6784, \u6570\u636e\u6574\u5408"}}
{"id": "2506.03066", "pdf": "https://arxiv.org/pdf/2506.03066", "abs": "https://arxiv.org/abs/2506.03066", "authors": ["Qining Zhang", "Lei Ying"], "title": "Provable Reinforcement Learning from Human Feedback with an Unknown Link Function", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Link functions, which characterize how human preferences are generated from\nthe value function of an RL problem, are a crucial component in designing RLHF\nalgorithms. Almost all RLHF algorithms, including state-of-the-art ones in\nempirical studies such as DPO and PPO, assume the link function is known to the\nagent (e.g., a logistic function according to the Bradley-Terry model), which\nis arguably unrealistic considering the complex nature of human preferences. To\navoid link function mis-specification, this paper studies general RLHF problems\nwith unknown link functions. We propose a novel policy optimization algorithm\ncalled ZSPO based on a new zeroth-order policy optimization method, where the\nkey is to use human preference to construct a parameter update direction that\nis positively correlated with the true policy gradient direction. ZSPO achieves\nit by estimating the sign of the value function difference instead of\nestimating the gradient from the value function difference, so it does not\nrequire knowing the link function. Under mild conditions, ZSPO converges to a\nstationary policy with a polynomial convergence rate depending on the number of\npolicy iterations and trajectories per iteration. Numerical results also show\nthe superiority of ZSPO under link function mismatch.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aZSPO\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3RLHF\u95ee\u9898\u4e2d\u94fe\u63a5\u51fd\u6570\u672a\u77e5\u7684\u60c5\u51b5\uff0c\u901a\u8fc7\u96f6\u9636\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u4eba\u7c7b\u504f\u597d\u4f30\u8ba1\u7b56\u7565\u68af\u5ea6\u65b9\u5411\uff0c\u800c\u4e0d\u9700\u8981\u77e5\u9053\u94fe\u63a5\u51fd\u6570\u7684\u5177\u4f53\u5f62\u5f0f\u3002", "motivation": "\u7531\u4e8e\u4eba\u7c7b\u504f\u597d\u7684\u590d\u6742\u6027\uff0c\u73b0\u6709RLHF\u7b97\u6cd5\uff08\u5982DPO\u548cPPO\uff09\u5047\u8bbe\u94fe\u63a5\u51fd\u6570\u5df2\u77e5\u662f\u4e0d\u73b0\u5b9e\u7684\u3002\u4e3a\u4e86\u907f\u514d\u94fe\u63a5\u51fd\u6570\u8bef\u8bbe\uff0c\u4f5c\u8005\u7814\u7a76\u4e86\u94fe\u63a5\u51fd\u6570\u672a\u77e5\u7684\u4e00\u822cRLHF\u95ee\u9898\u3002", "method": "\u63d0\u51faZSPO\u7b97\u6cd5\uff0c\u57fa\u4e8e\u96f6\u9636\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eba\u7c7b\u504f\u597d\u6784\u5efa\u4e0e\u771f\u5b9e\u7b56\u7565\u68af\u5ea6\u65b9\u5411\u6b63\u76f8\u5173\u7684\u53c2\u6570\u66f4\u65b0\u65b9\u5411\uff0c\u65e0\u9700\u77e5\u9053\u94fe\u63a5\u51fd\u6570\u7684\u5177\u4f53\u5f62\u5f0f\u3002", "result": "ZSPO\u5728\u591a\u8f6e\u7b56\u7565\u8fed\u4ee3\u548c\u6bcf\u8f6e\u8f68\u8ff9\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u591a\u9879\u5f0f\u6536\u655b\u901f\u7387\u6536\u655b\u5230\u5e73\u7a33\u7b56\u7565\uff0c\u6570\u503c\u7ed3\u679c\u8bc1\u660e\u4e86\u5176\u5728\u94fe\u63a5\u51fd\u6570\u4e0d\u5339\u914d\u60c5\u51b5\u4e0b\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "ZSPO\u4e3a\u89e3\u51b3\u94fe\u63a5\u51fd\u6570\u672a\u77e5\u7684RLHF\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd\u4f18\u52bf\u3002", "keywords": "RLHF, \u96f6\u9636\u7b56\u7565\u4f18\u5316, \u94fe\u63a5\u51fd\u6570, \u4eba\u7c7b\u504f\u597d, ZSPO"}}
{"id": "2506.02178", "pdf": "https://arxiv.org/pdf/2506.02178", "abs": "https://arxiv.org/abs/2506.02178", "authors": ["Thai-Binh Nguyen", "Ngoc-Quan Pham", "Alexander Waibel"], "title": "Cocktail-Party Audio-Visual Speech Recognition", "categories": ["cs.SD", "cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "Audio-Visual Speech Recognition (AVSR) offers a robust solution for speech\nrecognition in challenging environments, such as cocktail-party scenarios,\nwhere relying solely on audio proves insufficient. However, current AVSR models\nare often optimized for idealized scenarios with consistently active speakers,\noverlooking the complexities of real-world settings that include both speaking\nand silent facial segments. This study addresses this gap by introducing a\nnovel audio-visual cocktail-party dataset designed to benchmark current AVSR\nsystems and highlight the limitations of prior approaches in realistic noisy\nconditions. Additionally, we contribute a 1526-hour AVSR dataset comprising\nboth talking-face and silent-face segments, enabling significant performance\ngains in cocktail-party environments. Our approach reduces WER by 67% relative\nto the state-of-the-art, reducing WER from 119% to 39.2% in extreme noise,\nwithout relying on explicit segmentation cues.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u97f3\u9891-\u89c6\u89c9\u9e21\u5c3e\u9152\u4f1a\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709AVSR\u6a21\u578b\u5728\u771f\u5b9e\u566a\u58f0\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709AVSR\u6a21\u578b\u5728\u7406\u60f3\u5316\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u566a\u58f0\u73af\u5883\u4e0b\uff08\u5982\u9e21\u5c3e\u9152\u4f1a\uff09\u6548\u679c\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5bf9\u8bf4\u8bdd\u548c\u9759\u9ed8\u9762\u90e8\u7247\u6bb5\u7684\u5904\u7406\u4e0d\u8db3\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b\u8bf4\u8bdd\u548c\u9759\u9ed8\u9762\u90e8\u7247\u6bb5\u76841526\u5c0f\u65f6AVSR\u6570\u636e\u96c6\uff0c\u5e76\u5728\u6781\u7aef\u566a\u58f0\u73af\u5883\u4e2d\u6d4b\u8bd5\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u6781\u7aef\u566a\u58f0\u73af\u5883\u4e0b\u5c06WER\u4ece119%\u964d\u4f4e\u81f339.2%\uff0c\u76f8\u5bf9\u51cf\u5c11\u4e8667%\u7684\u9519\u8bef\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u771f\u5b9e\u566a\u58f0\u573a\u666f\u7684\u6570\u636e\u96c6\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86AVSR\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "keywords": "AVSR, \u9e21\u5c3e\u9152\u4f1a\u95ee\u9898, \u6570\u636e\u96c6, \u566a\u58f0\u73af\u5883, WER"}}
{"id": "2506.03075", "pdf": "https://arxiv.org/pdf/2506.03075", "abs": "https://arxiv.org/abs/2506.03075", "authors": ["Bogdan Chornomaz", "Yonatan Koren", "Shay Moran", "Tom Waknine"], "title": "Agnostic Learning under Targeted Poisoning: Optimal Rates and the Role of Randomness", "categories": ["cs.LG", "math.PR", "68Q32", "I.2.6"], "comment": null, "summary": "We study the problem of learning in the presence of an adversary that can\ncorrupt an $\\eta$ fraction of the training examples with the goal of causing\nfailure on a specific test point. In the realizable setting, prior work\nestablished that the optimal error under such instance-targeted poisoning\nattacks scales as $\\Theta(d\\eta)$, where $d$ is the VC dimension of the\nhypothesis class arXiv:2210.02713. In this work, we resolve the corresponding\nquestion in the agnostic setting. We show that the optimal excess error is\n$\\tilde{\\Theta}(\\sqrt{d\\eta})$, answering one of the main open problems left by\nHanneke et al. To achieve this rate, it is necessary to use randomized\nlearners: Hanneke et al. showed that deterministic learners can be forced to\nsuffer error close to 1, even under small amounts of poisoning. Perhaps\nsurprisingly, our upper bound remains valid even when the learner's random bits\nare fully visible to the adversary . In the other direction, our lower bound is\nstronger than standard PAC-style bounds: instead of tailoring a hard\ndistribution separately for each sample size, we exhibit a single fixed\ndistribution under which the adversary can enforce an excess error of\n$\\Omega(\\sqrt{d\\eta})$ infinitely often.", "AI": {"tldr": "\u8be5\u8bba\u6587\u89e3\u51b3\u4e86\u5728\u4e0d\u53ef\u77e5\u8bbe\u7f6e\u4e0b\u5bf9\u6297\u6027\u5b9e\u4f8b\u76ee\u6807\u4e2d\u6bd2\u653b\u51fb\u7684\u6700\u4f18\u8d85\u989d\u8bef\u5dee\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5176\u89c4\u6a21\u4e3a$\tilde{\\Theta}(\\sqrt{d\\eta})$\uff0c\u5e76\u5c55\u793a\u4e86\u968f\u673a\u5b66\u4e60\u5668\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u5728\u5b58\u5728\u5bf9\u6297\u6027\u5b9e\u4f8b\u76ee\u6807\u4e2d\u6bd2\u653b\u51fb\u7684\u60c5\u51b5\u4e0b\uff0c\u5b66\u4e60\u95ee\u9898\u7684\u6700\u4f18\u8d85\u989d\u8bef\u5dee\u754c\u9650\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5728\u4e0d\u53ef\u77e5\u8bbe\u7f6e\u4e0b\uff0c\u6700\u4f18\u8d85\u989d\u8bef\u5dee\u4e3a$\tilde{\\Theta}(\\sqrt{d\\eta})$\uff0c\u5e76\u63a2\u8ba8\u4e86\u968f\u673a\u5b66\u4e60\u5668\u7684\u5fc5\u8981\u6027\u3002", "result": "\u786e\u5b9a\u4e86\u5728\u5bf9\u6297\u6027\u653b\u51fb\u4e0b\uff0c\u968f\u673a\u5b66\u4e60\u5668\u7684\u6700\u4f18\u8d85\u989d\u8bef\u5dee\u754c\u9650\uff0c\u5e76\u5c55\u793a\u4e86\u786e\u5b9a\u6027\u5b66\u4e60\u5668\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u968f\u673a\u5b66\u4e60\u5668\u5728\u5bf9\u6297\u6027\u5b9e\u4f8b\u76ee\u6807\u4e2d\u6bd2\u653b\u51fb\u4e0b\u5177\u6709\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "keywords": "\u5bf9\u6297\u6027\u653b\u51fb, \u5b66\u4e60\u7406\u8bba, VC\u7ef4\u5ea6, \u968f\u673a\u5b66\u4e60\u5668, \u4e0d\u53ef\u77e5\u8bbe\u7f6e"}}
{"id": "2506.03077", "pdf": "https://arxiv.org/pdf/2506.03077", "abs": "https://arxiv.org/abs/2506.03077", "authors": ["Qijun Luo", "Mengqi Li", "Lei Zhao", "Xiao Li"], "title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Training language models on long sequence data is a demanding requirement for\nenhancing the model's capability on complex tasks, e.g., long-chain reasoning.\nHowever, as the sequence length scales up, the memory cost for storing\nactivation values becomes huge during the Backpropagation (BP) process, even\nwith the application of gradient checkpointing technique. To tackle this\nchallenge, we propose a memory-efficient and exact BP method called StreamBP,\nwhich performs a linear decomposition of the chain rule along the sequence\ndimension in a layer-wise manner, significantly reducing the memory cost of\nactivation values and logits. The proposed method is applicable to common\nobjectives such as SFT, GRPO, and DPO. From an implementation perspective,\nStreamBP achieves less computational FLOPs and faster BP speed by leveraging\nthe causal structure of the language model. Compared to gradient checkpointing,\nStreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger,\nwhile using comparable or even less BP time. Note that StreamBP's sequence\nlength scaling ability can be directly transferred to batch size scaling for\naccelerating training. We further develop a communication-efficient distributed\nStreamBP to effectively support multi-GPU training and broaden its\napplicability. Our code can be easily integrated into the training pipeline of\nany transformer models and is available at https://github.com/Ledzy/StreamBP.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aStreamBP\u7684\u5185\u5b58\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u53cd\u5411\u4f20\u64ad\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u6fc0\u6d3b\u503c\u548c\u903b\u8f91\u7684\u5185\u5b58\u6d88\u8017\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e38\u89c1\u76ee\u6807\uff0c\u5e76\u652f\u6301\u591aGPU\u8bad\u7ec3\u3002", "motivation": "\u8bad\u7ec3\u957f\u5e8f\u5217\u6570\u636e\u5bf9\u63d0\u5347\u6a21\u578b\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u5b58\u50a8\u6fc0\u6d3b\u503c\u7684\u5185\u5b58\u6210\u672c\u6781\u9ad8\uff0c\u5373\u4f7f\u4f7f\u7528\u68af\u5ea6\u68c0\u67e5\u70b9\u6280\u672f\u3002", "method": "\u901a\u8fc7\u7ebf\u6027\u5206\u89e3\u94fe\u5f0f\u89c4\u5219\uff0c\u9010\u5c42\u5904\u7406\u5e8f\u5217\u7ef4\u5ea6\uff0c\u5229\u7528\u56e0\u679c\u7ed3\u6784\u51cf\u5c11\u8ba1\u7b97\u91cf\u548c\u52a0\u901f\u53cd\u5411\u4f20\u64ad\u3002", "result": "StreamBP\u5c06\u53cd\u5411\u4f20\u64ad\u7684\u6700\u5927\u5e8f\u5217\u957f\u5ea6\u6269\u5c55\u4e862.8-5.5\u500d\uff0c\u8ba1\u7b97FLOPs\u66f4\u5c11\u4e14\u53cd\u5411\u4f20\u64ad\u901f\u5ea6\u66f4\u5feb\uff0c\u652f\u6301\u6279\u91cf\u5927\u5c0f\u6269\u5c55\u3002", "conclusion": "StreamBP\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u53cd\u5411\u4f20\u64ad\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002", "keywords": "StreamBP, \u53cd\u5411\u4f20\u64ad, \u5185\u5b58\u6548\u7387, \u957f\u5e8f\u5217\u8bad\u7ec3, \u68af\u5ea6\u68c0\u67e5\u70b9"}}
{"id": "2506.02295", "pdf": "https://arxiv.org/pdf/2506.02295", "abs": "https://arxiv.org/abs/2506.02295", "authors": ["Ahmed Wasfy", "Omer Nacar", "Abdelakreem Elkhateb", "Mahmoud Reda", "Omar Elshehy", "Adel Ammar", "Wadii Boulila"], "title": "QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large Language Model Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The inherent complexities of Arabic script; its cursive nature, diacritical\nmarks (tashkeel), and varied typography, pose persistent challenges for Optical\nCharacter Recognition (OCR). We present Qari-OCR, a series of vision-language\nmodels derived from Qwen2-VL-2B-Instruct, progressively optimized for Arabic\nthrough iterative fine-tuning on specialized synthetic datasets. Our leading\nmodel, QARI v0.2, establishes a new open-source state-of-the-art with a Word\nError Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score\nof 0.737 on diacritically-rich texts. Qari-OCR demonstrates superior handling\nof tashkeel, diverse fonts, and document layouts, alongside impressive\nperformance on low-resolution images. Further explorations (QARI v0.3) showcase\nstrong potential for structural document understanding and handwritten text.\nThis work delivers a marked improvement in Arabic OCR accuracy and efficiency,\nwith all models and datasets released to foster further research.", "AI": {"tldr": "Qari-OCR \u662f\u4e00\u79cd\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u4f18\u5316\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u8fed\u4ee3\u5fae\u8c03\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\uff08OCR\uff09\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u4e66\u5199\u7684\u590d\u6742\u6027\uff08\u5982\u8fde\u7b14\u3001\u97f3\u6807\u7b26\u53f7\u548c\u591a\u53d8\u7684\u5b57\u4f53\uff09\u7ed9 OCR \u5e26\u6765\u4e86\u6301\u7eed\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u57fa\u4e8e Qwen2-VL-2B-Instruct \u6a21\u578b\uff0c\u901a\u8fc7\u8fed\u4ee3\u5fae\u8c03\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u4f18\u5316\uff0c\u5f00\u53d1\u4e86 Qari-OCR \u7cfb\u5217\u6a21\u578b\u3002", "result": "QARI v0.2 \u6a21\u578b\u5728\u5f00\u6e90\u9886\u57df\u8fbe\u5230\u6700\u65b0\u6280\u672f\u6c34\u5e73\uff0c\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u4e3a 0.160\uff0c\u5b57\u7b26\u9519\u8bef\u7387\uff08CER\uff09\u4e3a 0.061\uff0cBLEU \u5206\u6570\u4e3a 0.737\u3002", "conclusion": "Qari-OCR \u663e\u8457\u63d0\u5347\u4e86\u963f\u62c9\u4f2f\u8bed OCR \u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5c24\u5176\u5728\u5904\u7406\u97f3\u6807\u3001\u590d\u6742\u5b57\u4f53\u548c\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "keywords": "\u963f\u62c9\u4f2f\u8bed OCR, \u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u8fed\u4ee3\u5fae\u8c03, \u8bcd\u9519\u8bef\u7387, \u5b57\u7b26\u9519\u8bef\u7387"}}
{"id": "2506.03085", "pdf": "https://arxiv.org/pdf/2506.03085", "abs": "https://arxiv.org/abs/2506.03085", "authors": ["Thomas Chen", "Tengyu Ma", "Zhiyuan Li"], "title": "Non-Asymptotic Length Generalization", "categories": ["cs.LG"], "comment": null, "summary": "Length generalization is the ability of a learning algorithm to learn a\nhypothesis which generalizes to longer inputs than the inputs in the training\nset. In this paper, we provide provable guarantees of length generalization for\nvarious classes of functions in an idealized setting. First, we formalize the\nframework of non-asymptotic length generalization, which requires a computable\nupper bound for the minimum input length that guarantees length generalization,\nas a function of the complexity of ground-truth function under some given\ncomplexity measure. We refer to this minimum input length to length generalize\nas length complexity. We show the Minimum-Complexity Interpolator learning\nalgorithm achieves optimal length complexity. We further show that whether a\nfunction class admits non-asymptotic length generalization is equivalent to the\ndecidability of its language equivalence problem, which implies that there is\nno computable upper bound for the length complexity of Context-Free Grammars.\nOn the positive side, we show that the length complexity of Deterministic\nFinite Automata is $2n - 2$ where $n$ is the number of states of the\nground-truth automaton. Our main results are upper bounds of length complexity\nfor a subset of a transformer-related function class called C-RASP (Yang &\nChiang, 2024). We show that the length complexity of 1-layer C-RASP functions\nis $O(T^2)$ when the ground-truth function has precision $T$, and that the\nlength complexity of 2-layer C-RASP functions is $O(T^{O(K)})$ when the\nground-truth function has precision $T$ and $K$ heads.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u957f\u5ea6\u6cdb\u5316\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u5305\u62ec\u5f62\u5f0f\u5316\u6846\u67b6\u3001\u6700\u4f18\u7b97\u6cd5\u53ca\u5bf9\u4e0d\u540c\u51fd\u6570\u7c7b\u7684\u957f\u5ea6\u590d\u6742\u6027\u5206\u6790\u3002", "motivation": "\u63a2\u8ba8\u5b66\u4e60\u7b97\u6cd5\u5728\u8bad\u7ec3\u96c6\u5916\u957f\u8f93\u5165\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u975e\u6e10\u8fd1\u957f\u5ea6\u6cdb\u5316\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u5f62\u5f0f\u5316\u975e\u6e10\u8fd1\u957f\u5ea6\u6cdb\u5316\u6846\u67b6\uff0c\u5206\u6790\u4e0d\u540c\u51fd\u6570\u7c7b\u7684\u957f\u5ea6\u590d\u6742\u6027\uff0c\u5982\u6700\u5c0f\u590d\u6742\u5ea6\u63d2\u503c\u7b97\u6cd5\u3002", "result": "\u786e\u5b9a\u6027\u6709\u9650\u81ea\u52a8\u673a\u7684\u957f\u5ea6\u590d\u6742\u6027\u4e3a2n-2\uff1bC-RASP\u51fd\u6570\u7c7b\u7684\u590d\u6742\u6027\u968f\u5c42\u6570\u548c\u5934\u6570\u589e\u957f\u3002", "conclusion": "\u957f\u5ea6\u6cdb\u5316\u4e0e\u8bed\u8a00\u7b49\u4ef7\u95ee\u9898\u53ef\u89e3\u6027\u76f8\u5173\uff0c\u4e0d\u540c\u51fd\u6570\u7c7b\u7684\u957f\u5ea6\u590d\u6742\u6027\u6709\u663e\u8457\u5dee\u5f02\u3002", "keywords": "\u957f\u5ea6\u6cdb\u5316, \u975e\u6e10\u8fd1\u6846\u67b6, \u957f\u5ea6\u590d\u6742\u6027, C-RASP, \u786e\u5b9a\u6027\u6709\u9650\u81ea\u52a8\u673a"}}
{"id": "2506.03087", "pdf": "https://arxiv.org/pdf/2506.03087", "abs": "https://arxiv.org/abs/2506.03087", "authors": ["Bin Ma", "Yuyuan Feng", "Minhua Lin", "Enyan Dai"], "title": "How Explanations Leak the Decision Logic: Stealing Graph Neural Networks via Explanation Alignment", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Neural Networks (GNNs) have become essential tools for analyzing\ngraph-structured data in domains such as drug discovery and financial analysis,\nleading to growing demands for model transparency. Recent advances in\nexplainable GNNs have addressed this need by revealing important subgraphs that\ninfluence predictions, but these explanation mechanisms may inadvertently\nexpose models to security risks. This paper investigates how such explanations\npotentially leak critical decision logic that can be exploited for model\nstealing. We propose {\\method}, a novel stealing framework that integrates\nexplanation alignment for capturing decision logic with guided data\naugmentation for efficient training under limited queries, enabling effective\nreplication of both the predictive behavior and underlying reasoning patterns\nof target models. Experiments on molecular graph datasets demonstrate that our\napproach shows advantages over conventional methods in model stealing. This\nwork highlights important security considerations for the deployment of\nexplainable GNNs in sensitive domains and suggests the need for protective\nmeasures against explanation-based attacks. Our code is available at\nhttps://github.com/beanmah/EGSteal.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u53ef\u89e3\u91ca\u6027\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u5b89\u5168\u9886\u57df\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a{\\method}\u7684\u65b0\u578b\u6a21\u578b\u7a83\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89e3\u91ca\u5bf9\u9f50\u548c\u5f15\u5bfc\u6570\u636e\u589e\u5f3a\uff0c\u6210\u529f\u590d\u5236\u4e86\u76ee\u6807\u6a21\u578b\u7684\u9884\u6d4b\u884c\u4e3a\u548c\u63a8\u7406\u6a21\u5f0f\u3002", "motivation": "\u7531\u4e8eGNNs\u5728\u836f\u7269\u53d1\u73b0\u548c\u91d1\u878d\u5206\u6790\u7b49\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u6a21\u578b\u900f\u660e\u6027\u9700\u6c42\u589e\u52a0\u3002\u7136\u800c\uff0c\u53ef\u89e3\u91ca\u6027\u673a\u5236\u53ef\u80fd\u66b4\u9732\u6a21\u578b\u7684\u51b3\u7b56\u903b\u8f91\uff0c\u5bfc\u81f4\u5b89\u5168\u98ce\u9669\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86{\\method}\u6846\u67b6\uff0c\u7ed3\u5408\u89e3\u91ca\u5bf9\u9f50\u548c\u5f15\u5bfc\u6570\u636e\u589e\u5f3a\uff0c\u4ee5\u6709\u9650\u7684\u67e5\u8be2\u6b21\u6570\u9ad8\u6548\u8bad\u7ec3\uff0c\u590d\u5236\u76ee\u6807\u6a21\u578b\u7684\u9884\u6d4b\u884c\u4e3a\u548c\u63a8\u7406\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u5728\u5206\u5b50\u56fe\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5728\u6a21\u578b\u7a83\u53d6\u4e2d\u7684\u4f18\u52bf\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u53ef\u89e3\u91ca\u6027GNNs\u5728\u654f\u611f\u9886\u57df\u90e8\u7f72\u65f6\u7684\u5b89\u5168\u8003\u8651\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u57fa\u4e8e\u89e3\u91ca\u7684\u653b\u51fb\u7684\u4fdd\u62a4\u63aa\u65bd\u9700\u6c42\u3002", "keywords": "\u56fe\u795e\u7ecf\u7f51\u7edc, \u6a21\u578b\u7a83\u53d6, \u53ef\u89e3\u91ca\u6027, \u5b89\u5168\u98ce\u9669, \u6570\u636e\u589e\u5f3a"}}
{"id": "2506.02414", "pdf": "https://arxiv.org/pdf/2506.02414", "abs": "https://arxiv.org/abs/2506.02414", "authors": ["Fengjin Li", "Jie Wang", "Yadong Niu", "Yongqing Wang", "Meng Meng", "Jian Luan", "Zhiyong Wu"], "title": "StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech Generation in Voice Conversion", "categories": ["cs.MM", "cs.CL", "cs.SD", "eess.AS"], "comment": "5 pages, 2 figures, Accepted by Interspeech 2025, Demo:\n  https://thuhcsi.github.io/StarVC/", "summary": "Voice Conversion (VC) modifies speech to match a target speaker while\npreserving linguistic content. Traditional methods usually extract speaker\ninformation directly from speech while neglecting the explicit utilization of\nlinguistic content. Since VC fundamentally involves disentangling speaker\nidentity from linguistic content, leveraging structured semantic features could\nenhance conversion performance. However, previous attempts to incorporate\nsemantic features into VC have shown limited effectiveness, motivating the\nintegration of explicit text modeling. We propose StarVC, a unified\nautoregressive VC framework that first predicts text tokens before synthesizing\nacoustic features. The experiments demonstrate that StarVC outperforms\nconventional VC methods in preserving both linguistic content (i.e., WER and\nCER) and speaker characteristics (i.e., SECS and MOS). Audio demo can be found\nat: https://thuhcsi.github.io/StarVC/.", "AI": {"tldr": "StarVC\u662f\u4e00\u79cd\u7ed3\u5408\u6587\u672c\u5efa\u6a21\u7684\u81ea\u56de\u5f52\u8bed\u97f3\u8f6c\u6362\u6846\u67b6\uff0c\u901a\u8fc7\u5148\u9884\u6d4b\u6587\u672c\u6807\u8bb0\u518d\u5408\u6210\u58f0\u5b66\u7279\u5f81\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u8bed\u8a00\u5185\u5bb9\u548c\u8bf4\u8bdd\u4eba\u7279\u5f81\u7684\u4fdd\u7559\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u8bed\u97f3\u8f6c\u6362\u65b9\u6cd5\u76f4\u63a5\u63d0\u53d6\u8bf4\u8bdd\u4eba\u4fe1\u606f\u800c\u5ffd\u89c6\u8bed\u8a00\u5185\u5bb9\u7684\u663e\u5f0f\u5229\u7528\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002StarVC\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u5efa\u6a21\uff0c\u4f18\u5316\u4e86\u89e3\u8026\u8bf4\u8bdd\u4eba\u8eab\u4efd\u548c\u8bed\u8a00\u5185\u5bb9\u7684\u8fc7\u7a0b\u3002", "method": "StarVC\u91c7\u7528\u7edf\u4e00\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u5148\u9884\u6d4b\u6587\u672c\u6807\u8bb0\uff0c\u518d\u5408\u6210\u58f0\u5b66\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8bed\u8a00\u5185\u5bb9\u548c\u8bf4\u8bdd\u4eba\u7279\u5f81\u7684\u663e\u5f0f\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cStarVC\u5728\u8bed\u8a00\u5185\u5bb9\uff08WER\u3001CER\uff09\u548c\u8bf4\u8bdd\u4eba\u7279\u5f81\uff08SECS\u3001MOS\uff09\u7684\u4fdd\u7559\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u6587\u672c\u5efa\u6a21\uff0cStarVC\u6210\u529f\u63d0\u5347\u4e86\u8bed\u97f3\u8f6c\u6362\u7684\u6027\u80fd\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002", "keywords": "\u8bed\u97f3\u8f6c\u6362, \u6587\u672c\u5efa\u6a21, \u81ea\u56de\u5f52\u6846\u67b6, \u8bf4\u8bdd\u4eba\u7279\u5f81, \u8bed\u8a00\u5185\u5bb9"}}
{"id": "2506.03093", "pdf": "https://arxiv.org/pdf/2506.03093", "abs": "https://arxiv.org/abs/2506.03093", "authors": ["Val\u00e9rie Costa", "Thomas Fel", "Ekdeep Singh Lubana", "Bahareh Tolooshams", "Demba Ba"], "title": "From Flat to Hierarchical: Extracting Sparse Representations with Matching Pursuit", "categories": ["cs.LG"], "comment": "Preprint", "summary": "Motivated by the hypothesis that neural network representations encode\nabstract, interpretable features as linearly accessible, approximately\northogonal directions, sparse autoencoders (SAEs) have become a popular tool in\ninterpretability. However, recent work has demonstrated phenomenology of model\nrepresentations that lies outside the scope of this hypothesis, showing\nsignatures of hierarchical, nonlinear, and multi-dimensional features. This\nraises the question: do SAEs represent features that possess structure at odds\nwith their motivating hypothesis? If not, does avoiding this mismatch help\nidentify said features and gain further insights into neural network\nrepresentations? To answer these questions, we take a construction-based\napproach and re-contextualize the popular matching pursuits (MP) algorithm from\nsparse coding to design MP-SAE -- an SAE that unrolls its encoder into a\nsequence of residual-guided steps, allowing it to capture hierarchical and\nnonlinearly accessible features. Comparing this architecture with existing SAEs\non a mixture of synthetic and natural data settings, we show: (i) hierarchical\nconcepts induce conditionally orthogonal features, which existing SAEs are\nunable to faithfully capture, and (ii) the nonlinear encoding step of MP-SAE\nrecovers highly meaningful features, helping us unravel shared structure in the\nseemingly dichotomous representation spaces of different modalities in a\nvision-language model, hence demonstrating the assumption that useful features\nare solely linearly accessible is insufficient. We also show that the\nsequential encoder principle of MP-SAE affords an additional benefit of\nadaptive sparsity at inference time, which may be of independent interest.\nOverall, we argue our results provide credence to the idea that\ninterpretability should begin with the phenomenology of representations, with\nmethods emerging from assumptions that fit it.", "AI": {"tldr": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7684\u4f20\u7edf\u5047\u8bbe\u8ba4\u4e3a\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u662f\u7ebf\u6027\u53ef\u8bbf\u95ee\u4e14\u6b63\u4ea4\u7684\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u53d1\u73b0\u7279\u5f81\u5177\u6709\u5206\u5c42\u3001\u975e\u7ebf\u6027\u548c\u591a\u7ef4\u6027\u3002\u672c\u6587\u63d0\u51faMP-SAE\u65b9\u6cd5\uff0c\u80fd\u66f4\u597d\u5730\u6355\u6349\u590d\u6742\u7279\u5f81\uff0c\u9a8c\u8bc1\u4e86\u7279\u5f81\u4e0d\u4ec5\u662f\u7ebf\u6027\u53ef\u8bbf\u95ee\u7684\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u662f\u5426\u5177\u6709\u5206\u5c42\u548c\u975e\u7ebf\u6027\u7ed3\u6784\uff0c\u6311\u6218\u4f20\u7edfSAE\u7684\u7ebf\u6027\u6b63\u4ea4\u5047\u8bbe\u3002", "method": "\u63d0\u51faMP-SAE\uff0c\u57fa\u4e8e\u5339\u914d\u8ffd\u8e2a\u7b97\u6cd5\uff0c\u5229\u7528\u6b8b\u5dee\u5f15\u5bfc\u6b65\u9aa4\u6355\u6349\u5206\u5c42\u548c\u975e\u7ebf\u6027\u7279\u5f81\u3002", "result": "MP-SAE\u80fd\u6062\u590d\u6709\u610f\u4e49\u7684\u7279\u5f81\uff0c\u63ed\u793a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u5171\u4eab\u7684\u7ed3\u6784\uff0c\u5e76\u652f\u6301\u81ea\u9002\u5e94\u7a00\u758f\u6027\u3002", "conclusion": "\u7279\u5f81\u7684\u53ef\u89e3\u91ca\u6027\u5e94\u4ece\u8868\u5f81\u73b0\u8c61\u5b66\u51fa\u53d1\uff0c\u65b9\u6cd5\u9700\u9002\u5e94\u5176\u7279\u5f81\u5047\u8bbe\u3002", "keywords": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\u3001\u795e\u7ecf\u7f51\u7edc\u53ef\u89e3\u91ca\u6027\u3001\u5339\u914d\u8ffd\u8e2a\u3001\u975e\u7ebf\u6027\u7279\u5f81\u3001\u5206\u5c42\u7279\u5f81"}}
{"id": "2506.03100", "pdf": "https://arxiv.org/pdf/2506.03100", "abs": "https://arxiv.org/abs/2506.03100", "authors": ["Yang Guo", "Yutian Tao", "Yifei Ming", "Robert D. Nowak", "Yingyu Liang"], "title": "Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "math.ST", "stat.TH"], "comment": "Under Review", "summary": "Retrieval-augmented generation (RAG) has seen many empirical successes in\nrecent years by aiding the LLM with external knowledge. However, its\ntheoretical aspect has remained mostly unexplored. In this paper, we propose\nthe first finite-sample generalization bound for RAG in in-context linear\nregression and derive an exact bias-variance tradeoff. Our framework views the\nretrieved texts as query-dependent noisy in-context examples and recovers the\nclassical in-context learning (ICL) and standard RAG as the limit cases. Our\nanalysis suggests that an intrinsic ceiling on generalization error exists on\nRAG as opposed to the ICL. Furthermore, our framework is able to model\nretrieval both from the training data and from external corpora by introducing\nuniform and non-uniform RAG noise. In line with our theory, we show the sample\nefficiency of ICL and RAG empirically with experiments on common QA benchmarks,\nsuch as Natural Questions and TriviaQA.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u63d0\u51fa\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u4e0a\u4e0b\u6587\u7ebf\u6027\u56de\u5f52\u4e2d\u7684\u6709\u9650\u6837\u672c\u6cdb\u5316\u754c\uff0c\u5e76\u63a8\u5bfc\u4e86\u7cbe\u786e\u7684\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u4e0e\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u76f8\u6bd4\uff0cRAG\u5b58\u5728\u56fa\u6709\u6cdb\u5316\u8bef\u5dee\u4e0a\u9650\u3002", "motivation": "\u5c3d\u7ba1RAG\u5728\u5b9e\u8bc1\u7814\u7a76\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5176\u7406\u8bba\u57fa\u7840\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7406\u8bba\u5206\u6790\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5c06\u68c0\u7d22\u6587\u672c\u89c6\u4e3a\u4f9d\u8d56\u4e8e\u67e5\u8be2\u7684\u566a\u58f0\u4e0a\u4e0b\u6587\u793a\u4f8b\uff0c\u6784\u5efa\u4e86\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e86\u5747\u5300\u548c\u975e\u5747\u5300RAG\u566a\u58f0\u6765\u5efa\u6a21\u4ece\u8bad\u7ec3\u6570\u636e\u548c\u5916\u90e8\u8bed\u6599\u5e93\u7684\u68c0\u7d22\u3002", "result": "\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86RAG\u7684\u56fa\u6709\u6cdb\u5316\u8bef\u5dee\u4e0a\u9650\uff0c\u5b9e\u9a8c\u5728Natural Questions\u548cTriviaQA\u7b49QA\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86ICL\u548cRAG\u7684\u6837\u672c\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u4e3aRAG\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u5176\u5728\u6cdb\u5316\u8bef\u5dee\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "keywords": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u3001\u6cdb\u5316\u754c\u3001\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u3001QA\u57fa\u51c6"}}
{"id": "2506.02479", "pdf": "https://arxiv.org/pdf/2506.02479", "abs": "https://arxiv.org/abs/2506.02479", "authors": ["Kalyan Nakka", "Nitesh Saxena"], "title": "BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage", "categories": ["cs.CR", "cs.CL"], "comment": "24 pages, 24 figures, and 7 tables", "summary": "The inherent risk of generating harmful and unsafe content by Large Language\nModels (LLMs), has highlighted the need for their safety alignment. Various\ntechniques like supervised fine-tuning, reinforcement learning from human\nfeedback, and red-teaming were developed for ensuring the safety alignment of\nLLMs. However, the robustness of these aligned LLMs is always challenged by\nadversarial attacks that exploit unexplored and underlying vulnerabilities of\nthe safety alignment. In this paper, we develop a novel black-box jailbreak\nattack, called BitBypass, that leverages hyphen-separated bitstream camouflage\nfor jailbreaking aligned LLMs. This represents a new direction in jailbreaking\nby exploiting fundamental information representation of data as continuous\nbits, rather than leveraging prompt engineering or adversarial manipulations.\nOur evaluation of five state-of-the-art LLMs, namely GPT-4o, Gemini 1.5, Claude\n3.5, Llama 3.1, and Mixtral, in adversarial perspective, revealed the\ncapabilities of BitBypass in bypassing their safety alignment and tricking them\ninto generating harmful and unsafe content. Further, we observed that BitBypass\noutperforms several state-of-the-art jailbreak attacks in terms of stealthiness\nand attack success. Overall, these results highlights the effectiveness and\nefficiency of BitBypass in jailbreaking these state-of-the-art LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBitBypass\u7684\u9ed1\u76d2\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u8fde\u5b57\u7b26\u5206\u9694\u7684\u6bd4\u7279\u6d41\u4f2a\u88c5\u8d8a\u72f1\u5bf9\u9f50\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002\u4e0e\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\u6216\u5bf9\u6297\u6027\u64cd\u4f5c\u7684\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\uff0cBitBypass\u901a\u8fc7\u5229\u7528\u6570\u636e\u7684\u57fa\u672c\u4fe1\u606f\u8868\u793a\uff08\u8fde\u7eed\u6bd4\u7279\uff09\u5b9e\u73b0\u8d8a\u72f1\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cBitBypass\u80fd\u591f\u6210\u529f\u7ed5\u8fc7\u4e94\u79cd\u5148\u8fdbLLMs\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u5e76\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u540c\u65f6\u5728\u9690\u853d\u6027\u548c\u653b\u51fb\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5b89\u5168\u5bf9\u9f50\u662f\u5f53\u524d\u7814\u7a76\u7684\u91cd\u8981\u65b9\u5411\uff0c\u4f46\u5176\u5728\u9762\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u65f6\u7684\u9c81\u68d2\u6027\u5f80\u5f80\u4e0d\u8db3\u3002\u73b0\u6709\u7684\u5bf9\u9f50\u65b9\u6cd5\u5bb9\u6613\u88ab\u653b\u51fb\u8005\u5229\u7528\u672a\u63a2\u7d22\u7684\u6f0f\u6d1e\u7ed5\u8fc7\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\u4ee5\u63ed\u793a\u8fd9\u4e9b\u6f0f\u6d1e\uff0c\u4ece\u800c\u6539\u8fdb\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "method": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aBitBypass\u7684\u9ed1\u76d2\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8fde\u5b57\u7b26\u5206\u9694\u7684\u6bd4\u7279\u6d41\u4f2a\u88c5\u751f\u6210\u8f93\u5165\uff0c\u7ed5\u8fc7LLMs\u7684\u5b89\u5168\u5bf9\u9f50\u673a\u5236\u3002\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u4e94\u79cd\u5148\u8fdbLLMs\uff08GPT-4o\u3001Gemini 1.5\u3001Claude 3.5\u3001Llama 3.1\u548cMixtral\uff09\u5728\u5bf9\u6297\u6027\u89c6\u89d2\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cBitBypass\u80fd\u591f\u6709\u6548\u7ed5\u8fc7\u4e94\u79cdLLMs\u7684\u5b89\u5168\u5bf9\u9f50\u673a\u5236\uff0c\u6210\u529f\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u3002\u6b64\u5916\uff0cBitBypass\u5728\u9690\u853d\u6027\u548c\u653b\u51fb\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\u3002", "conclusion": "BitBypass\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9690\u853d\u7684\u65b0\u578b\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6bd4\u7279\u6d41\u4f2a\u88c5\u5b9e\u73b0\u8d8a\u72f1\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLMs\u5b89\u5168\u5bf9\u9f50\u673a\u5236\u7684\u6f5c\u5728\u6f0f\u6d1e\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u6a21\u578b\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u5b89\u5168\u5bf9\u9f50,\u9ed1\u76d2\u8d8a\u72f1\u653b\u51fb,BitBypass,\u5bf9\u6297\u6027\u653b\u51fb,\u6bd4\u7279\u6d41\u4f2a\u88c5"}}
{"id": "2506.03109", "pdf": "https://arxiv.org/pdf/2506.03109", "abs": "https://arxiv.org/abs/2506.03109", "authors": ["Wei Yao", "Gengze Xu", "Huayi Tang", "Wenkai Yang", "Donglin Di", "Ziqiao Wang", "Yong Liu"], "title": "On Weak-to-Strong Generalization and f-Divergence", "categories": ["cs.LG"], "comment": null, "summary": "Weak-to-strong generalization (W2SG) has emerged as a promising paradigm for\nstimulating the capabilities of strong pre-trained models by leveraging\nsupervision from weaker supervisors. To improve the performance of the strong\nmodel, existing methods often require additional weak models or complex\nprocedures, leading to substantial computational and memory overhead. Motivated\nby the effectiveness of $f$-divergence loss in various machine learning\ndomains, we introduce $f$-divergence as an information-theoretic loss function\nframework in W2SG. Our theoretical analysis reveals fundamental limitations and\nequivalence of different $f$-divergence losses in W2SG, supported by sample\ncomplexity bounds and information-theoretic insights. We empirically\ndemonstrate that $f$-divergence loss, which generalizes widely-used metrics\nlike KL divergence, effectively improves generalization and noise tolerance of\nthe strong model in practice.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e$f$-\u6563\u5ea6\u7684\u5f31\u76d1\u7763\u5230\u5f3a\u76d1\u7763\u5b66\u4e60\uff08W2SG\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709W2SG\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u989d\u5916\u7684\u5f31\u6a21\u578b\u6216\u590d\u6742\u6d41\u7a0b\uff0c\u5bfc\u81f4\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u8f83\u5927\uff0c\u800c$f$-\u6563\u5ea6\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u56e0\u6b64\u63a2\u8ba8\u5176\u5728W2SG\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165$f$-\u6563\u5ea6\u4f5c\u4e3aW2SG\u4e2d\u7684\u4fe1\u606f\u8bba\u635f\u5931\u51fd\u6570\u6846\u67b6\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "$f$-\u6563\u5ea6\u635f\u5931\u80fd\u6709\u6548\u63d0\u9ad8\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u566a\u58f0\u5bb9\u5fcd\u5ea6\uff0c\u4e14\u5177\u6709\u7406\u8bba\u652f\u6301\u3002", "conclusion": "$f$-\u6563\u5ea6\u5728W2SG\u4e2d\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u635f\u5931\u51fd\u6570\u9009\u62e9\u3002", "keywords": "\u5f31\u76d1\u7763\u5230\u5f3a\u76d1\u7763\u5b66\u4e60, $f$-\u6563\u5ea6, \u4fe1\u606f\u8bba\u635f\u5931, \u566a\u58f0\u5bb9\u5fcd, \u6cdb\u5316\u80fd\u529b"}}
{"id": "2506.02529", "pdf": "https://arxiv.org/pdf/2506.02529", "abs": "https://arxiv.org/abs/2506.02529", "authors": ["Nguyen-Khang Le", "Quan Minh Bui", "Minh Ngoc Nguyen", "Hiep Nguyen", "Trung Vo", "Son T. Luu", "Shoshin Nomura", "Minh Le Nguyen"], "title": "Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs", "categories": ["cs.SE", "cs.AI", "cs.CL", "I.2.7"], "comment": "Published in the Proceedings of JSAI 2025", "summary": "Web applications are critical to modern software ecosystems, yet ensuring\ntheir reliability remains challenging due to the complexity and dynamic nature\nof web interfaces. Recent advances in large language models (LLMs) have shown\npromise in automating complex tasks, but limitations persist in handling\ndynamic navigation flows and complex form interactions. This paper presents an\nautomated system for generating test cases for two key aspects of web\napplication testing: site navigation and form filling. For site navigation, the\nsystem employs screen transition graphs and LLMs to model navigation flows and\ngenerate test scenarios. For form filling, it uses state graphs to handle\nconditional forms and automates Selenium script generation. Key contributions\ninclude: (1) a novel integration of graph structures and LLMs for site\nnavigation testing, (2) a state graph-based approach for automating\nform-filling test cases, and (3) a comprehensive dataset for evaluating\nform-interaction testing. Experimental results demonstrate the system's\neffectiveness in improving test coverage and robustness, advancing the state of\nweb application testing.", "AI": {"tldr": "\u4e00\u79cd\u7ed3\u5408\u56fe\u7ed3\u6784\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210Web\u5e94\u7528\u7a0b\u5e8f\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u63d0\u5347\u7ad9\u70b9\u5bfc\u822a\u548c\u8868\u5355\u586b\u5199\u7684\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u786e\u4fddWeb\u5e94\u7528\u7a0b\u5e8f\u7684\u53ef\u9760\u6027\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u754c\u9762\u590d\u6742\u4e14\u52a8\u6001\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u52a8\u6001\u5bfc\u822a\u6d41\u548c\u590d\u6742\u8868\u5355\u4ea4\u4e92\u65f6\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u5c4f\u5e55\u8f6c\u6362\u56fe\u548cLLM\u5efa\u6a21\u5bfc\u822a\u6d41\u751f\u6210\u6d4b\u8bd5\u573a\u666f\uff1b\u91c7\u7528\u72b6\u6001\u56fe\u548cSelenium\u811a\u672c\u81ea\u52a8\u5316\u5904\u7406\u8868\u5355\u586b\u5199\u6d4b\u8bd5\u3002", "result": "\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u9c81\u68d2\u6027\uff0c\u63a8\u52a8\u4e86Web\u5e94\u7528\u6d4b\u8bd5\u7684\u8fdb\u6b65\u3002", "conclusion": "\u7ed3\u5408\u56fe\u7ed3\u6784\u548cLLM\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86Web\u5e94\u7528\u6d4b\u8bd5\u4e2d\u7684\u590d\u6742\u6027\u548c\u52a8\u6001\u6027\u95ee\u9898\u3002", "keywords": "Web\u5e94\u7528\u6d4b\u8bd5,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u56fe\u7ed3\u6784,\u81ea\u52a8\u5316\u6d4b\u8bd5"}}
{"id": "2506.03111", "pdf": "https://arxiv.org/pdf/2506.03111", "abs": "https://arxiv.org/abs/2506.03111", "authors": ["Victor Armegioiu", "Yannick Ramic", "Siddhartha Mishra"], "title": "Rectified Flows for Fast Multiscale Fluid Flow Modeling", "categories": ["cs.LG"], "comment": null, "summary": "The statistical modeling of fluid flows is very challenging due to their\nmultiscale dynamics and extreme sensitivity to initial conditions. While\nrecently proposed conditional diffusion models achieve high fidelity, they\ntypically require hundreds of stochastic sampling steps at inference. We\nintroduce a rectified flow framework that learns a time-dependent velocity\nfield, transporting input to output distributions along nearly straight\ntrajectories. By casting sampling as solving an ordinary differential equation\n(ODE) along this straighter flow field, our method makes each integration step\nmuch more effective, using as few as eight steps versus (more than) 128 steps\nin standard score-based diffusion, without sacrificing predictive fidelity.\nExperiments on challenging multiscale flow benchmarks show that rectified flows\nrecover the same posterior distributions as diffusion models, preserve\nfine-scale features that MSE-trained baselines miss, and deliver\nhigh-resolution samples in a fraction of inference time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3arectified flow\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6d41\u52a8\u8f68\u8ff9\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6d41\u4f53\u6d41\u52a8\u7edf\u8ba1\u5efa\u6a21\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u3002", "motivation": "\u6d41\u4f53\u6d41\u52a8\u7684\u7edf\u8ba1\u5efa\u6a21\u7531\u4e8e\u591a\u5c3a\u5ea6\u52a8\u6001\u548c\u521d\u59cb\u6761\u4ef6\u6781\u7aef\u654f\u611f\u800c\u6781\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u6269\u6563\u6a21\u578b\u9700\u8981\u5927\u91cf\u63a8\u7406\u6b65\u9aa4\uff0c\u6548\u7387\u8f83\u4f4e\u3002", "method": "\u5f15\u5165rectified flow\u6846\u67b6\uff0c\u5b66\u4e60\u65f6\u95f4\u4f9d\u8d56\u7684\u901f\u5ea6\u573a\uff0c\u5c06\u8f93\u5165\u5206\u5e03\u6cbf\u8fd1\u4e4e\u76f4\u7ebf\u8f68\u8ff9\u4f20\u8f93\u5230\u8f93\u51fa\u5206\u5e03\uff0c\u5c06\u91c7\u6837\u8f6c\u5316\u4e3aODE\u6c42\u89e3\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4ec5\u97008\u6b65\u63a8\u7406\u65f6\uff0c\u4e0e\u6807\u51c6\u5206\u6570\u6269\u6563\u6a21\u578b\uff08128\u6b65\u4ee5\u4e0a\uff09\u5177\u6709\u76f8\u540c\u7684\u540e\u9a8c\u5206\u5e03\u548c\u9ad8\u4fdd\u771f\u5ea6\uff0c\u63a8\u7406\u65f6\u95f4\u5927\u5e45\u7f29\u77ed\u3002", "conclusion": "rectified flow\u80fd\u5728\u6781\u5c11\u91cf\u63a8\u7406\u6b65\u9aa4\u4e0b\u6062\u590d\u6269\u6563\u6a21\u578b\u7684\u9ad8\u4fdd\u771f\u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "keywords": "\u6d41\u4f53\u6d41\u52a8\u3001\u7edf\u8ba1\u5efa\u6a21\u3001\u6269\u6563\u6a21\u578b\u3001ODE\u3001\u63a8\u7406\u6548\u7387"}}
{"id": "2506.03128", "pdf": "https://arxiv.org/pdf/2506.03128", "abs": "https://arxiv.org/abs/2506.03128", "authors": ["Andreas Auer", "Raghul Parthipan", "Pedro Mercado", "Abdul Fatir Ansari", "Lorenzo Stella", "Bernie Wang", "Michael Bohlke-Schneider", "Syama Sundar Rangapuram"], "title": "Zero-Shot Time Series Forecasting with Covariates via In-Context Learning", "categories": ["cs.LG"], "comment": "The paper was written at the end of 2024", "summary": "Pretrained time series models, capable of zero-shot forecasting, have\ndemonstrated significant potential in enhancing both the performance and\naccessibility of time series forecasting. However, existing pretrained models\neither do not support covariates or fail to incorporate them effectively. We\nintroduce COSMIC, a zero-shot forecasting model that utilizes covariates via\nin-context learning. To address the challenge of data scarcity, we propose\nInformative Covariate Augmentation, which enables the training of COSMIC\nwithout requiring any datasets that include covariates. COSMIC achieves\nstate-of-the-art performance in zero-shot forecasting, both with and without\ncovariates. Our quantitative and qualitative analysis demonstrates that COSMIC\neffectively leverages covariates in zero-shot forecasting.", "AI": {"tldr": "COSMIC\u662f\u4e00\u79cd\u5229\u7528\u534f\u53d8\u91cf\u8fdb\u884c\u96f6\u6837\u672c\u9884\u6d4b\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u6709\u6548\u6574\u5408\u534f\u53d8\u91cf\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9884\u8bad\u7ec3\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u652f\u6301\u6216\u6574\u5408\u534f\u53d8\u91cf\uff0c\u5f71\u54cd\u4e86\u96f6\u6837\u672c\u9884\u6d4b\u7684\u6027\u80fd\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u63d0\u51fa\u4e86COSMIC\u6a21\u578b\uff0c\u91c7\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u5229\u7528\u534f\u53d8\u91cf\uff0c\u5e76\u63d0\u51fa\u4fe1\u606f\u6027\u534f\u53d8\u91cf\u589e\u5f3a\u65b9\u6cd5\u4ee5\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "result": "COSMIC\u5728\u96f6\u6837\u672c\u9884\u6d4b\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u65e0\u8bba\u662f\u5426\u4f7f\u7528\u534f\u53d8\u91cf\u3002\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u8868\u660e\u5176\u6709\u6548\u5229\u7528\u4e86\u534f\u53d8\u91cf\u3002", "conclusion": "COSMIC\u901a\u8fc7\u521b\u65b0\u7684\u534f\u53d8\u91cf\u5229\u7528\u65b9\u6cd5\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u8868\u73b0\u3002", "keywords": "\u96f6\u6837\u672c\u9884\u6d4b,\u534f\u53d8\u91cf,\u4e0a\u4e0b\u6587\u5b66\u4e60,\u65f6\u95f4\u5e8f\u5217,COSMIC"}}
{"id": "2506.02590", "pdf": "https://arxiv.org/pdf/2506.02590", "abs": "https://arxiv.org/abs/2506.02590", "authors": ["Dimitrios Koutsianos", "Stavros Zacharopoulos", "Yannis Panagakis", "Themos Stafylakis"], "title": "Synthetic Speech Source Tracing using Metric Learning", "categories": ["cs.SD", "cs.CL"], "comment": "Submitted to Interspeech 2025", "summary": "This paper addresses source tracing in synthetic speech-identifying\ngenerative systems behind manipulated audio via speaker recognition-inspired\npipelines. While prior work focuses on spoofing detection, source tracing lacks\nrobust solutions. We evaluate two approaches: classification-based and\nmetric-learning. We tested our methods on the MLAADv5 benchmark using ResNet\nand self-supervised learning (SSL) backbones. The results show that ResNet\nachieves competitive performance with the metric learning approach, matching\nand even exceeding SSL-based systems. Our work demonstrates ResNet's viability\nfor source tracing while underscoring the need to optimize SSL representations\nfor this task. Our work bridges speaker recognition methodologies with audio\nforensic challenges, offering new directions for combating synthetic media\nmanipulation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u8bf4\u8bdd\u4eba\u8bc6\u522b\u6280\u672f\u8ffd\u6eaf\u5408\u6210\u8bed\u97f3\u6765\u6e90\u7684\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u5206\u7c7b\u548c\u5ea6\u91cf\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4f2a\u9020\u68c0\u6d4b\uff0c\u7f3a\u4e4f\u5bf9\u5408\u6210\u97f3\u9891\u6765\u6e90\u8ffd\u8e2a\u7684\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5206\u7c7b\u548c\u5ea6\u91cf\u5b66\u4e60\u7684\u4e24\u79cd\u65b9\u6cd5\uff0c\u5e76\u5728MLAADv5\u57fa\u51c6\u4e0a\u4f7f\u7528ResNet\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u9aa8\u5e72\u7f51\u7edc\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cResNet\u5728\u5ea6\u91cf\u5b66\u4e60\u65b9\u6cd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8a\u81ea\u76d1\u7763\u5b66\u4e60\u7cfb\u7edf\u3002", "conclusion": "\u7814\u7a76\u8868\u660eResNet\u53ef\u7528\u4e8e\u6765\u6e90\u8ffd\u8e2a\uff0c\u4f46\u9700\u4f18\u5316\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u8868\u5f81\u4ee5\u9002\u914d\u6b64\u4efb\u52a1\u3002", "keywords": "\u5408\u6210\u8bed\u97f3\u8ffd\u8e2a\uff0c\u8bf4\u8bdd\u4eba\u8bc6\u522b\uff0cResNet\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5ea6\u91cf\u5b66\u4e60"}}
{"id": "2506.03133", "pdf": "https://arxiv.org/pdf/2506.03133", "abs": "https://arxiv.org/abs/2506.03133", "authors": ["Kai Lion", "Liang Zhang", "Bingcong Li", "Niao He"], "title": "PoLAR: Polar-Decomposed Low-Rank Adapter Representation", "categories": ["cs.LG", "cs.AI", "eess.SP", "math.OC"], "comment": null, "summary": "We show that low-rank adaptation of large-scale models suffers from a low\nstable rank that is well below the linear algebraic rank of the subspace,\ndegrading fine-tuning performance. To mitigate the underutilization of the\nallocated subspace, we propose PoLAR, a parameterization inspired by the polar\ndecomposition that factorizes the low-rank update into two direction matrices\nconstrained to Stiefel manifolds and an unconstrained scale matrix. Our theory\nshows that PoLAR yields an exponentially faster convergence rate on a canonical\nlow-rank adaptation problem. Pairing the parameterization with Riemannian\noptimization leads to consistent gains on three different benchmarks testing\ngeneral language understanding, commonsense reasoning, and mathematical problem\nsolving with base model sizes ranging from 350M to 27B.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPoLAR\u65b9\u6cd5\uff0c\u901a\u8fc7\u6781\u5206\u89e3\u53c2\u6570\u5316\u89e3\u51b3\u4f4e\u79e9\u9002\u5e94\u4e2d\u7a33\u5b9a\u79e9\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5fae\u8c03\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u6a21\u578b\u7684\u4f4e\u79e9\u9002\u5e94\u5b58\u5728\u7a33\u5b9a\u79e9\u4f4e\u4e8e\u5b50\u7a7a\u95f4\u7ebf\u6027\u4ee3\u6570\u79e9\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u5fae\u8c03\u8868\u73b0\u3002", "method": "\u5f15\u5165PoLAR\uff0c\u5229\u7528\u6781\u5206\u89e3\u5c06\u4f4e\u79e9\u66f4\u65b0\u56e0\u5b50\u5316\u4e3a\u4e24\u4e2aStiefel\u6d41\u5f62\u7ea6\u675f\u7684\u65b9\u5411\u77e9\u9635\u548c\u4e00\u4e2a\u65e0\u7ea6\u675f\u7684\u6bd4\u4f8b\u77e9\u9635\u3002\u901a\u8fc7\u9ece\u66fc\u4f18\u5316\u63d0\u5347\u6027\u80fd\u3002", "result": "\u7406\u8bba\u8868\u660ePoLAR\u5728\u4f4e\u79e9\u9002\u5e94\u95ee\u9898\u4e2d\u5b9e\u73b0\u6307\u6570\u7ea7\u66f4\u5feb\u6536\u655b\uff0c\u5b9e\u9a8c\u5728350M\u523027B\u7684\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5728\u8bed\u8a00\u7406\u89e3\u3001\u5e38\u8bc6\u63a8\u7406\u548c\u6570\u5b66\u89e3\u9898\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "PoLAR\u901a\u8fc7\u6539\u8fdb\u53c2\u6570\u5316\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4f4e\u79e9\u9002\u5e94\u7684\u6027\u80fd\u3002", "keywords": "\u4f4e\u79e9\u9002\u5e94,\u6781\u5206\u89e3,Stiefel\u6d41\u5f62,\u9ece\u66fc\u4f18\u5316"}}
{"id": "2506.02708", "pdf": "https://arxiv.org/pdf/2506.02708", "abs": "https://arxiv.org/abs/2506.02708", "authors": ["Naoto Tanji", "Toshihiko Yamasaki"], "title": "Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to ICIP2025", "summary": "Image scoring is a crucial task in numerous real-world applications. To trust\na model's judgment, understanding its rationale is essential. This paper\nproposes a novel training method for Vision Language Models (VLMs) to generate\nnot only image scores but also corresponding justifications in natural\nlanguage. Leveraging only an image scoring dataset and an instruction-tuned\nVLM, our method enables self-training, utilizing the VLM's generated text\nwithout relying on external data or models. In addition, we introduce a simple\nmethod for creating a dataset designed to improve alignment between predicted\nscores and their textual justifications. By iteratively training the model with\nDirect Preference Optimization on two distinct datasets and merging them, we\ncan improve both scoring accuracy and the coherence of generated explanations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u80fd\u751f\u6210\u56fe\u50cf\u8bc4\u5206\uff0c\u8fd8\u80fd\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u901a\u8fc7\u81ea\u8bad\u7ec3\u548c\u8fed\u4ee3\u4f18\u5316\u63d0\u5347\u8bc4\u5206\u51c6\u786e\u6027\u4e0e\u89e3\u91ca\u4e00\u81f4\u6027\u3002", "motivation": "\u5728\u56fe\u50cf\u8bc4\u5206\u4efb\u52a1\u4e2d\uff0c\u7406\u89e3\u6a21\u578b\u7684\u51b3\u7b56\u4f9d\u636e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u8bc4\u5206\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u80fd\u529b\u3002", "method": "\u5229\u7528\u56fe\u50cf\u8bc4\u5206\u6570\u636e\u96c6\u548c\u6307\u4ee4\u8c03\u4f18\u7684VLM\u8fdb\u884c\u81ea\u8bad\u7ec3\uff0c\u7ed3\u5408\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8fed\u4ee3\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u5728\u8bc4\u5206\u51c6\u786e\u6027\u548c\u89e3\u91ca\u4e00\u81f4\u6027\u4e0a\u5747\u5f97\u5230\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86VLM\u5728\u56fe\u50cf\u8bc4\u5206\u4efb\u52a1\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002", "keywords": "\u56fe\u50cf\u8bc4\u5206\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u81ea\u8bad\u7ec3\u3001\u53ef\u89e3\u91ca\u6027\u3001\u76f4\u63a5\u504f\u597d\u4f18\u5316"}}
{"id": "2506.03142", "pdf": "https://arxiv.org/pdf/2506.03142", "abs": "https://arxiv.org/abs/2506.03142", "authors": ["Xiangyu Zhou", "Yao Qiang", "Saleh Zare Zade", "Douglas Zytko", "Prashant Khanduri", "Dongxiao Zhu"], "title": "Not All Tokens Are Meant to Be Forgotten", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Models (LLMs), pre-trained on massive text corpora, exhibit\nremarkable human-level language understanding, reasoning, and decision-making\nabilities. However, they tend to memorize unwanted information, such as private\nor copyrighted content, raising significant privacy and legal concerns.\nUnlearning has emerged as a promising solution, but existing methods face a\nsignificant challenge of over-forgetting. This issue arises because they\nindiscriminately suppress the generation of all the tokens in forget samples,\nleading to a substantial loss of model utility. To overcome this challenge, we\nintroduce the Targeted Information Forgetting (TIF) framework, which consists\nof (1) a flexible targeted information identifier designed to differentiate\nbetween unwanted words (UW) and general words (GW) in the forget samples, and\n(2) a novel Targeted Preference Optimization approach that leverages Logit\nPreference Loss to unlearn unwanted information associated with UW and\nPreservation Loss to retain general information in GW, effectively improving\nthe unlearning process while mitigating utility degradation. Extensive\nexperiments on the TOFU and MUSE benchmarks demonstrate that the proposed TIF\nframework enhances unlearning effectiveness while preserving model utility and\nachieving state-of-the-art results.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTIF\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u5206\u4e0d\u9700\u8981\u7684\u8bcd\u8bed\u548c\u901a\u7528\u8bcd\u8bed\uff0c\u5e76\u91c7\u7528\u76ee\u6807\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9057\u5fd8\u4fe1\u606f\u65f6\u8fc7\u5ea6\u9057\u5fd8\u7684\u95ee\u9898\u3002", "motivation": "LLMs\u80fd\u591f\u8bb0\u5fc6\u5927\u91cf\u6587\u672c\uff0c\u4f46\u4e5f\u53ef\u80fd\u8bb0\u4f4f\u9690\u79c1\u6216\u7248\u6743\u5185\u5bb9\uff0c\u5f15\u53d1\u9690\u79c1\u548c\u6cd5\u5f8b\u95ee\u9898\u3002\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u5bfc\u81f4\u8fc7\u5ea6\u9057\u5fd8\uff0c\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002TIF\u6846\u67b6\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "TIF\u6846\u67b6\u5305\u62ec\u4e24\u90e8\u5206\uff1a\uff081\uff09\u76ee\u6807\u4fe1\u606f\u6807\u8bc6\u5668\uff0c\u533a\u5206\u4e0d\u9700\u8981\u7684\u8bcd\u8bed\uff08UW\uff09\u548c\u901a\u7528\u8bcd\u8bed\uff08GW\uff09\uff1b\uff082\uff09\u76ee\u6807\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408Logit Preference Loss\u9057\u5fd8UW\uff0c\u7528Preservation Loss\u4fdd\u7559GW\u3002", "result": "\u5728TOFU\u548cMUSE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTIF\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u9057\u5fd8\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u8fbe\u5230\u4e86\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "TIF\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLMs\u9057\u5fd8\u4fe1\u606f\u65f6\u7684\u8fc7\u5ea6\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u9057\u5fd8\u6548\u679c\u5e76\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "keywords": "\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u9057\u5fd8\u673a\u5236\uff0c\u76ee\u6807\u4fe1\u606f\u9057\u5fd8\uff08TIF\uff09\uff0c\u9690\u79c1\u4fdd\u62a4"}}
{"id": "2506.02362", "pdf": "https://arxiv.org/pdf/2506.02362", "abs": "https://arxiv.org/abs/2506.02362", "authors": ["Xueqi Cheng", "Minxing Zheng", "Shixiang Zhu", "Yushun Dong"], "title": "MISLEADER: Defending against Model Extraction with Ensembles of Distilled Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Model extraction attacks aim to replicate the functionality of a black-box\nmodel through query access, threatening the intellectual property (IP) of\nmachine-learning-as-a-service (MLaaS) providers. Defending against such attacks\nis challenging, as it must balance efficiency, robustness, and utility\npreservation in the real-world scenario. Despite the recent advances, most\nexisting defenses presume that attacker queries have out-of-distribution (OOD)\nsamples, enabling them to detect and disrupt suspicious inputs. However, this\nassumption is increasingly unreliable, as modern models are trained on diverse\ndatasets and attackers often operate under limited query budgets. As a result,\nthe effectiveness of these defenses is significantly compromised in realistic\ndeployment scenarios. To address this gap, we propose MISLEADER (enseMbles of\ndIStiLled modEls Against moDel ExtRaction), a novel defense strategy that does\nnot rely on OOD assumptions. MISLEADER formulates model protection as a bilevel\noptimization problem that simultaneously preserves predictive fidelity on\nbenign inputs and reduces extractability by potential clone models. Our\nframework combines data augmentation to simulate attacker queries with an\nensemble of heterogeneous distilled models to improve robustness and diversity.\nWe further provide a tractable approximation algorithm and derive theoretical\nerror bounds to characterize defense effectiveness. Extensive experiments\nacross various settings validate the utility-preserving and\nextraction-resistant properties of our proposed defense strategy. Our code is\navailable at https://github.com/LabRAI/MISLEADER.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86MISLEADER\u9632\u5fa1\u7b56\u7565\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u548c\u6570\u636e\u589e\u5f3a\u62b5\u5fa1\u6a21\u578b\u63d0\u53d6\u653b\u51fb\uff0c\u4e0d\u4f9d\u8d56OOD\u5047\u8bbe\uff0c\u4fdd\u6301\u4e86\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u6a21\u578b\u63d0\u53d6\u653b\u51fb\u5a01\u80c1MLaaS\u63d0\u4f9b\u5546\u7684\u77e5\u8bc6\u4ea7\u6743\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4f9d\u8d56OOD\u5047\u8bbe\u4e14\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faMISLEADER\uff0c\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u548c\u5f02\u6784\u84b8\u998f\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u4fdd\u62a4\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7b56\u7565\u5728\u4fdd\u62a4\u9884\u6d4b\u4fdd\u771f\u5ea6\u548c\u62b5\u6297\u63d0\u53d6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "MISLEADER\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u3002", "keywords": "\u6a21\u578b\u63d0\u53d6\u653b\u51fb,\u9632\u5fa1\u7b56\u7565,MISLEADER,\u53cc\u5c42\u4f18\u5316,\u6570\u636e\u589e\u5f3a"}}
{"id": "2506.02730", "pdf": "https://arxiv.org/pdf/2506.02730", "abs": "https://arxiv.org/abs/2506.02730", "authors": ["Po-Chieh Yu"], "title": "An Exploratory Framework for Future SETI Applications: Detecting Generative Reactivity via Language Models", "categories": ["astro-ph.IM", "cs.CL"], "comment": "submitted to the International Journal of Astrobiology", "summary": "We present an exploratory framework to test whether noise-like input can\ninduce structured responses in language models. Instead of assuming that\nextraterrestrial signals must be decoded, we evaluate whether inputs can\ntrigger linguistic behavior in generative systems. This shifts the focus from\ndecoding to viewing structured output as a sign of underlying regularity in the\ninput. We tested GPT-2 small, a 117M-parameter model trained on English text,\nusing four types of acoustic input: human speech, humpback whale vocalizations,\nPhylloscopus trochilus birdsong, and algorithmically generated white noise. All\ninputs were treated as noise-like, without any assumed symbolic encoding. To\nassess reactivity, we defined a composite score called Semantic Induction\nPotential (SIP), combining entropy, syntax coherence, compression gain, and\nrepetition penalty. Results showed that whale and bird vocalizations had higher\nSIP scores than white noise, while human speech triggered only moderate\nresponses. This suggests that language models may detect latent structure even\nin data without conventional semantics. We propose that this approach could\ncomplement traditional SETI methods, especially in cases where communicative\nintent is unknown. Generative reactivity may offer a different way to identify\ndata worth closer attention.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u63a2\u7d22\u6027\u6846\u67b6\uff0c\u6d4b\u8bd5\u566a\u58f0\u8f93\u5165\u662f\u5426\u80fd\u5f15\u53d1\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u5316\u54cd\u5e94\u3002\u901a\u8fc7\u8bc4\u4f30\u591a\u79cd\u58f0\u5b66\u8f93\u5165\uff0c\u53d1\u73b0\u9cb8\u9c7c\u548c\u9e1f\u7c7b\u7684\u53eb\u58f0\u6bd4\u767d\u566a\u58f0\u66f4\u80fd\u89e6\u53d1\u6a21\u578b\u7684\u8bed\u4e49\u53cd\u5e94\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u4ece\u4e0d\u5177\u5907\u4f20\u7edf\u8bed\u4e49\u7684\u6570\u636e\u4e2d\u68c0\u6d4b\u51fa\u6f5c\u5728\u7ed3\u6784\uff0c\u4e3a\u5730\u5916\u6587\u660e\u641c\u5bfb\uff08SETI\uff09\u63d0\u4f9b\u65b0\u7684\u89c6\u89d2\u3002", "method": "\u4f7f\u7528GPT-2\u5c0f\u6a21\u578b\u6d4b\u8bd5\u56db\u79cd\u58f0\u5b66\u8f93\u5165\uff08\u4eba\u7c7b\u8bed\u97f3\u3001\u9cb8\u9c7c\u53eb\u58f0\u3001\u9e1f\u7c7b\u9e23\u53eb\u548c\u767d\u566a\u58f0\uff09\uff0c\u5e76\u5b9a\u4e49\u590d\u5408\u8bc4\u5206SIP\u8861\u91cf\u6a21\u578b\u7684\u8bed\u4e49\u53cd\u5e94\u3002", "result": "\u9cb8\u9c7c\u548c\u9e1f\u7c7b\u53eb\u58f0\u7684SIP\u8bc4\u5206\u9ad8\u4e8e\u767d\u566a\u58f0\uff0c\u4eba\u7c7b\u8bed\u97f3\u7684\u53cd\u5e94\u8f83\u5f31\uff0c\u8868\u660e\u8bed\u8a00\u6a21\u578b\u80fd\u68c0\u6d4b\u975e\u5e38\u89c4\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u7ed3\u6784\u3002", "conclusion": "\u751f\u6210\u5f0f\u53cd\u5e94\u4e3aSETI\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u672a\u77e5\u901a\u4fe1\u610f\u56fe\u7684\u60c5\u51b5\u4e0b\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b, SETI, \u8bed\u4e49\u8bf1\u5bfc, \u566a\u58f0\u8f93\u5165, \u6f5c\u5728\u7ed3\u6784"}}
{"id": "2506.02002", "pdf": "https://arxiv.org/pdf/2506.02002", "abs": "https://arxiv.org/abs/2506.02002", "authors": ["Kamal Giri", "Amit Garu"], "title": "Machine Learning for Consistency Violation Faults Analysis", "categories": ["cs.DC", "cs.LG"], "comment": "5 pages, 5 figures", "summary": "Distributed systems frequently encounter consistency violation faults (cvfs),\nwhere nodes operate on outdated or inaccurate data, adversely affecting\nconvergence and overall system performance. This study presents a machine\nlearning-based approach for analyzing the impact of CVFs, using Dijkstra's\nToken Ring problem as a case study. By computing program transition ranks and\ntheir corresponding effects, the proposed method quantifies the influence of\ncvfs on system behavior. To address the state space explosion encountered in\nlarger graphs, two models are implemented: a Feedforward Neural Network (FNN)\nand a distributed neural network leveraging TensorFlow's \\texttt{tf.distribute}\nAPI. These models are trained on datasets generated from smaller graphs (3 to\n10 nodes) to predict parameters essential for determining rank effects.\nExperimental results demonstrate promising performance, with a test loss of\n4.39 and a mean absolute error of 1.5. Although distributed training on a CPU\ndid not yield significant speed improvements over a single-device setup, the\nfindings suggest that scalability could be enhanced through the use of advanced\nhardware accelerators such as GPUs or TPUs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u4e00\u81f4\u6027\u8fdd\u53cd\u6545\u969c\uff08CVFs\uff09\u5bf9\u7cfb\u7edf\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5e76\u5c55\u793a\u4e86\u5728Dijkstra\u4ee4\u724c\u73af\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u4e00\u81f4\u6027\u8fdd\u53cd\u6545\u969c\uff08CVFs\uff09\u4f1a\u5bfc\u81f4\u8282\u70b9\u57fa\u4e8e\u8fc7\u65f6\u6216\u9519\u8bef\u7684\u6570\u636e\u64cd\u4f5c\uff0c\u5f71\u54cd\u7cfb\u7edf\u6536\u655b\u548c\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u91cf\u5316\u5176\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u7a0b\u5e8f\u8f6c\u6362\u7684\u6392\u540d\u53ca\u5176\u6548\u5e94\uff0c\u91cf\u5316CVFs\u7684\u5f71\u54cd\u3002\u4e3a\u5e94\u5bf9\u5927\u56fe\u7684\u72b6\u6001\u7a7a\u95f4\u7206\u70b8\u95ee\u9898\uff0c\u91c7\u7528\u4e86\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff08FNN\uff09\u548c\u57fa\u4e8eTensorFlow\u5206\u5e03\u5f0fAPI\u7684\u5206\u5e03\u5f0f\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0c\u6d4b\u8bd5\u635f\u5931\u4e3a4.39\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a1.5\uff0c\u4f46\u5206\u5e03\u5f0f\u8bad\u7ec3\u5728CPU\u4e0a\u672a\u663e\u8457\u63d0\u5347\u901f\u5ea6\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u4f7f\u7528GPU\u6216TPU\u7b49\u786c\u4ef6\u52a0\u901f\u5668\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u7684\u6269\u5c55\u6027\u3002", "keywords": "\u5206\u5e03\u5f0f\u7cfb\u7edf, \u4e00\u81f4\u6027\u8fdd\u53cd\u6545\u969c, \u673a\u5668\u5b66\u4e60, \u795e\u7ecf\u7f51\u7edc, Dijkstra\u4ee4\u724c\u73af"}}
{"id": "2506.02006", "pdf": "https://arxiv.org/pdf/2506.02006", "abs": "https://arxiv.org/abs/2506.02006", "authors": ["Zhaoyuan Su", "Tingfeng Lan", "Zirui Wang", "Juncheng Yang", "Yue Cheng"], "title": "Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing", "categories": ["cs.DC", "cs.LG"], "comment": "19 pages, 7 figures", "summary": "Efficiently serving large language models (LLMs) under dynamic and bursty\nworkloads remains a key challenge for real-world deployment. Existing serving\nframeworks and static model compression techniques fail to adapt to workload\nfluctuations, leading to either service-level objective (SLO) violations under\nfull-precision serving or persistent accuracy degradation with static\nquantization. We present MorphServe, a dynamic, workload-aware LLM serving\nframework based on morphological adaptation. MorphServe introduces two\nasynchronous, token-level runtime mechanisms: quantized layer swapping, which\nselectively replaces less impactful layers with quantized alternatives during\nhigh-load periods, and pressure-aware KV cache resizing, which dynamically\nadjusts KV cache capacity in response to memory pressure. These mechanisms\nenable state-preserving transitions with minimum runtime overhead and are fully\ncompatible with modern scheduling and attention techniques. Extensive\nexperiments on Vicuna and Llama family models with real-world workloads\ndemonstrate that MorphServe reduces average SLO violations by 92.45 percent and\nimproves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving,\nwithout compromising generation quality. These results establish MorphServe as\na practical and elastic solution for LLM deployment in dynamic environments.", "AI": {"tldr": "MorphServe\u662f\u4e00\u4e2a\u52a8\u6001\u3001\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u670d\u52a1\u6846\u67b6\uff0c\u901a\u8fc7\u5f62\u6001\u9002\u5e94\u4f18\u5316\u670d\u52a1\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u670d\u52a1\u6846\u67b6\u548c\u9759\u6001\u6a21\u578b\u538b\u7f29\u6280\u672f\u65e0\u6cd5\u9002\u5e94\u5de5\u4f5c\u8d1f\u8f7d\u6ce2\u52a8\uff0c\u5bfc\u81f4\u670d\u52a1\u7ea7\u76ee\u6807(SLO)\u8fdd\u89c4\u6216\u51c6\u786e\u6027\u4e0b\u964d\u3002", "method": "MorphServe\u5f15\u5165\u4e86\u4e24\u79cd\u5f02\u6b65\u3001token\u7ea7\u522b\u7684\u8fd0\u884c\u65f6\u673a\u5236\uff1a\u91cf\u5316\u5c42\u4ea4\u6362\u548c\u538b\u529b\u611f\u77e5KV\u7f13\u5b58\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMorphServe\u5c06\u5e73\u5747SLO\u8fdd\u89c4\u51cf\u5c11\u4e8692.45%\uff0cP95 TTFT\u5ef6\u8fdf\u63d0\u9ad8\u4e862.2x-3.9x\uff0c\u4e14\u4e0d\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "MorphServe\u662f\u52a8\u6001\u73af\u5883\u4e2dLLM\u90e8\u7f72\u7684\u5b9e\u7528\u5f39\u6027\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u52a8\u6001\u670d\u52a1,\u5f62\u6001\u9002\u5e94,\u91cf\u5316,KV\u7f13\u5b58"}}
{"id": "2506.02016", "pdf": "https://arxiv.org/pdf/2506.02016", "abs": "https://arxiv.org/abs/2506.02016", "authors": ["Nuolin Sun", "Linyuan Wang", "Dongyang Li", "Bin Yan", "Lei Li"], "title": "Are classical deep neural networks weakly adversarially robust?", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Adversarial attacks have received increasing attention and it has been widely\nrecognized that classical DNNs have weak adversarial robustness. The most\ncommonly used adversarial defense method, adversarial training, improves the\nadversarial accuracy of DNNs by generating adversarial examples and retraining\nthe model. However, adversarial training requires a significant computational\noverhead. In this paper, inspired by existing studies focusing on the\nclustering properties of DNN output features at each layer and the Progressive\nFeedforward Collapse phenomenon, we propose a method for adversarial example\ndetection and image recognition that uses layer-wise features to construct\nfeature paths and computes the correlation between the examples feature paths\nand the class-centered feature paths. Experimental results show that the\nrecognition method achieves 82.77% clean accuracy and 44.17% adversarial\naccuracy on the ResNet-20 with PFC. Compared to the adversarial training method\nwith 77.64% clean accuracy and 52.94% adversarial accuracy, our method exhibits\na trade-off without relying on computationally expensive defense strategies.\nFurthermore, on the standard ResNet-18, our method maintains this advantage\nwith respective metrics of 80.01% and 46.1%. This result reveals inherent\nadversarial robustness in DNNs, challenging the conventional understanding of\nthe weak adversarial robustness in DNNs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c42\u7279\u5f81\u7684\u5bf9\u6297\u6837\u672c\u68c0\u6d4b\u548c\u56fe\u50cf\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7279\u5f81\u8def\u5f84\u5e76\u8ba1\u7b97\u5176\u4e0e\u7c7b\u4e2d\u5fc3\u7279\u5f81\u8def\u5f84\u7684\u76f8\u5173\u6027\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6297\u9c81\u68d2\u6027\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5e72\u51c0\u548c\u5bf9\u6297\u6837\u672c\u4e0a\u8fbe\u523082.77%\u548c44.17%\u7684\u51c6\u786e\u7387\uff0c\u63ed\u793a\u4e86DNN\u56fa\u6709\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edfDNN\u5bf9\u6297\u9c81\u68d2\u6027\u5f31\uff0c\u4e14\u5bf9\u6297\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u8bba\u6587\u53d7DNN\u5c42\u7279\u5f81\u805a\u7c7b\u7279\u6027\u548c\u6e10\u8fdb\u524d\u9988\u5d29\u584c\u73b0\u8c61\u7684\u542f\u53d1\uff0c\u5bfb\u6c42\u66f4\u9ad8\u6548\u7684\u5bf9\u6297\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5c42\u7279\u5f81\u6784\u5efa\u7279\u5f81\u8def\u5f84\uff0c\u5e76\u8ba1\u7b97\u6837\u672c\u7279\u5f81\u8def\u5f84\u4e0e\u7c7b\u4e2d\u5fc3\u7279\u5f81\u8def\u5f84\u7684\u76f8\u5173\u6027\uff0c\u7528\u4e8e\u5bf9\u6297\u6837\u672c\u68c0\u6d4b\u548c\u56fe\u50cf\u8bc6\u522b\u3002", "result": "\u5728ResNet-20\u4e0a\uff0c\u5e72\u51c0\u548c\u5bf9\u6297\u51c6\u786e\u7387\u5206\u522b\u4e3a82.77%\u548c44.17%\uff1b\u5728ResNet-18\u4e0a\u5206\u522b\u4e3a80.01%\u548c46.1%\uff0c\u4f18\u4e8e\u4f20\u7edf\u5bf9\u6297\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86DNN\u56fa\u6709\u7684\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u4e3a\u5bf9\u6297\u9632\u5fa1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u65b0\u601d\u8def\u3002", "keywords": "\u5bf9\u6297\u9c81\u68d2\u6027\u3001\u7279\u5f81\u8def\u5f84\u3001\u6e10\u8fdb\u524d\u9988\u5d29\u584c\u3001\u5bf9\u6297\u6837\u672c\u68c0\u6d4b\u3001DNN"}}
{"id": "2506.02020", "pdf": "https://arxiv.org/pdf/2506.02020", "abs": "https://arxiv.org/abs/2506.02020", "authors": ["Youze Xue", "Dian Li", "Gang Liu"], "title": "Improve Multi-Modal Embedding Learning via Explicit Hard Negative Gradient Amplifying", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "With the rapid advancement of multi-modal large language models (MLLMs) in\nrecent years, the foundational Contrastive Language-Image Pretraining (CLIP)\nframework has been successfully extended to MLLMs, enabling more powerful and\nuniversal multi-modal embeddings for a wide range of retrieval tasks. Despite\nthese developments, the core contrastive learning paradigm remains largely\nunchanged from CLIP-style models to MLLMs. Within this framework, the effective\nmining of hard negative samples continues to be a critical factor for enhancing\nperformance. Prior works have introduced both offline and online strategies for\nhard negative mining to improve the efficiency of contrastive learning. While\nthese approaches have led to improved multi-modal embeddings, the specific\ncontribution of each hard negative sample to the learning process has not been\nthoroughly investigated. In this work, we conduct a detailed analysis of the\ngradients of the info-NCE loss with respect to the query, positive, and\nnegative samples, elucidating the role of hard negatives in updating model\nparameters. Building upon this analysis, we propose to explicitly amplify the\ngradients associated with hard negative samples, thereby encouraging the model\nto learn more discriminative embeddings. Our multi-modal embedding model,\ntrained with the proposed Explicit Gradient Amplifier and based on the\nLLaVA-OneVision-7B architecture, achieves state-of-the-art performance on the\nMMEB benchmark compared to previous methods utilizing the same MLLM backbone.\nFurthermore, when integrated with our self-developed MLLM, QQMM, our approach\nattains the top rank on the MMEB leaderboard. Code and models are released on\nhttps://github.com/QQ-MM/QQMM-embed.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u786c\u8d1f\u6837\u672c\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u663e\u5f0f\u68af\u5ea6\u653e\u5927\u65b9\u6cd5\u4ee5\u63d0\u5347\u5d4c\u5165\u533a\u5206\u6027\uff0c\u6700\u7ec8\u5728MMEB\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u786c\u8d1f\u6837\u672c\u5728\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u7684\u5177\u4f53\u8d21\u732e\uff0c\u4ee5\u63d0\u5347\u591a\u6a21\u6001\u5d4c\u5165\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4fe1\u606fNCE\u635f\u5931\u7684\u68af\u5ea6\uff0c\u63d0\u51fa\u663e\u5f0f\u68af\u5ea6\u653e\u5927\u5668\uff08Explicit Gradient Amplifier\uff09\u6765\u653e\u5927\u786c\u8d1f\u6837\u672c\u7684\u68af\u5ea6\u3002", "result": "\u57fa\u4e8eLLaVA-OneVision-7B\u67b6\u6784\u7684\u6a21\u578b\u5728MMEB\u57fa\u51c6\u4e0a\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u4e0e\u81ea\u7814MLLM QQMM\u7ed3\u5408\u540e\u767b\u4e0aMMEB\u6392\u884c\u699c\u699c\u9996\u3002", "conclusion": "\u663e\u5f0f\u68af\u5ea6\u653e\u5927\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u5d4c\u5165\u7684\u533a\u5206\u6027\uff0c\u8bc1\u660e\u4e86\u786c\u8d1f\u6837\u672c\u5728\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u6027\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, \u5bf9\u6bd4\u5b66\u4e60, \u786c\u8d1f\u6837\u672c, \u68af\u5ea6\u5206\u6790, MMEB\u57fa\u51c6"}}
{"id": "2506.02023", "pdf": "https://arxiv.org/pdf/2506.02023", "abs": "https://arxiv.org/abs/2506.02023", "authors": ["Kevin Han", "Bowen Deng", "Amir Barati Farimani", "Gerbrand Ceder"], "title": "DistMLIP: A Distributed Inference Platform for Machine Learning Interatomic Potentials", "categories": ["cs.DC", "cond-mat.mtrl-sci", "cs.LG", "cs.PF"], "comment": null, "summary": "Large-scale atomistic simulations are essential to bridge computational\nmaterials and chemistry to realistic materials and drug discovery applications.\nIn the past few years, rapid developments of machine learning interatomic\npotentials (MLIPs) have offered a solution to scale up quantum mechanical\ncalculations. Parallelizing these interatomic potentials across multiple\ndevices poses a challenging, but promising approach to further extending\nsimulation scales to real-world applications. In this work, we present\nDistMLIP, an efficient distributed inference platform for MLIPs based on\nzero-redundancy, graph-level parallelization. In contrast to conventional\nspace-partitioning parallelization, DistMLIP enables efficient MLIP\nparallelization through graph partitioning, allowing multi-device inference on\nflexible MLIP model architectures like multi-layer graph neural networks.\nDistMLIP presents an easy-to-use, flexible, plug-in interface that enables\ndistributed inference of pre-existing MLIPs. We demonstrate DistMLIP on four\nwidely used and state-of-the-art MLIPs: CHGNet, MACE, TensorNet, and eSEN. We\nshow that existing foundational potentials can perform near-million-atom\ncalculations at the scale of a few seconds on 8 GPUs with DistMLIP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDistMLIP\uff0c\u4e00\u79cd\u57fa\u4e8e\u96f6\u5197\u4f59\u7684\u56fe\u7ea7\u5e76\u884c\u3001\u652f\u6301\u591a\u8bbe\u5907\u63a8\u7406\u7684\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u539f\u5b50\u95f4\u52bf\uff08MLIPs\uff09\u5e73\u53f0\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u5c3a\u5ea6\u539f\u5b50\u6a21\u62df\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u63d0\u5347MLIPs\u7684\u5e76\u884c\u5316\u6548\u7387\u4ee5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "method": "\u91c7\u7528\u56fe\u5206\u533a\u800c\u975e\u4f20\u7edf\u7684\u7a7a\u95f4\u5206\u533a\uff0c\u652f\u6301\u7075\u6d3b\u7684MLIP\u6a21\u578b\u67b6\u6784\uff08\u5982\u591a\u5c42\u56fe\u795e\u7ecf\u7f51\u7edc\uff09\u3002", "result": "\u57288\u4e2aGPU\u4e0a\u5b9e\u73b0\u4e86\u8fd1\u767e\u4e07\u539f\u5b50\u89c4\u6a21\u7684\u8ba1\u7b97\uff0c\u8017\u65f6\u4ec5\u51e0\u79d2\u3002", "conclusion": "DistMLIP\u4e3a\u73b0\u6709MLIPs\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u6613\u7528\u7684\u5206\u5e03\u5f0f\u63a8\u7406\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u89c4\u6a21\u4e0e\u901f\u5ea6\u3002", "keywords": "\u673a\u5668\u5b66\u4e60\u539f\u5b50\u95f4\u52bf\uff08MLIPs\uff09,\u5206\u5e03\u5f0f\u8ba1\u7b97,\u56fe\u795e\u7ecf\u7f51\u7edc,\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df"}}
{"id": "2506.03053", "pdf": "https://arxiv.org/pdf/2506.03053", "abs": "https://arxiv.org/abs/2506.03053", "authors": ["Sinem Erisken", "Timothy Gothard", "Martin Leitgab", "Ram Potham"], "title": "MAEBE: Multi-Agent Emergent Behavior Framework", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": "Preprint. This work has been submitted to the Multi-Agent Systems\n  Workshop at ICML 2025 for review", "summary": "Traditional AI safety evaluations on isolated LLMs are insufficient as\nmulti-agent AI ensembles become prevalent, introducing novel emergent risks.\nThis paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)\nframework to systematically assess such risks. Using MAEBE with the Greatest\nGood Benchmark (and a novel double-inversion question technique), we\ndemonstrate that: (1) LLM moral preferences, particularly for Instrumental\nHarm, are surprisingly brittle and shift significantly with question framing,\nboth in single agents and ensembles. (2) The moral reasoning of LLM ensembles\nis not directly predictable from isolated agent behavior due to emergent group\ndynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure\ninfluencing convergence, even when guided by a supervisor, highlighting\ndistinct safety and alignment challenges. Our findings underscore the necessity\nof evaluating AI systems in their interactive, multi-agent contexts.", "AI": {"tldr": "\u4f20\u7edfAI\u5b89\u5168\u8bc4\u4f30\u5728\u5b64\u7acbLLM\u4e0a\u4e0d\u8db3\uff0c\u591a\u667a\u80fd\u4f53AI\u96c6\u6210\u5e26\u6765\u65b0\u98ce\u9669\u3002MAEBE\u6846\u67b6\u7cfb\u7edf\u8bc4\u4f30\u8fd9\u4e9b\u98ce\u9669\uff0c\u53d1\u73b0LLM\u9053\u5fb7\u504f\u597d\u8106\u5f31\uff0c\u7fa4\u4f53\u884c\u4e3a\u4e0d\u53ef\u9884\u6d4b\uff0c\u5b58\u5728\u5b89\u5168\u6311\u6218\u3002", "motivation": "\u8bc4\u4f30\u591a\u667a\u80fd\u4f53AI\u96c6\u6210\u4e2d\u7684\u65b0\u5174\u98ce\u9669\uff0c\u5f25\u8865\u4f20\u7edf\u5b64\u7acbLLM\u8bc4\u4f30\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faMAEBE\u6846\u67b6\uff0c\u7ed3\u5408Greatest Good Benchmark\u548c\u53cc\u53cd\u8f6c\u95ee\u9898\u6280\u672f\uff0c\u5206\u6790\u5355\u667a\u80fd\u4f53\u4e0e\u96c6\u6210\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u5dee\u5f02\u3002", "result": "LLM\u9053\u5fb7\u504f\u597d\u6613\u53d7\u95ee\u9898\u8bbe\u8ba1\u5f71\u54cd\uff0c\u7fa4\u4f53\u884c\u4e3a\u6d8c\u73b0\u4e0d\u53ef\u9884\u6d4b\uff0c\u5b58\u5728\u540c\u4f34\u538b\u529b\u7b49\u73b0\u8c61\u3002", "conclusion": "\u9700\u5728\u4ea4\u4e92\u5f0f\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u8bc4\u4f30AI\u7cfb\u7edf\u3002", "keywords": "MAEBE, \u591a\u667a\u80fd\u4f53, \u65b0\u5174\u98ce\u9669, LLM, \u9053\u5fb7\u504f\u597d"}}
{"id": "2506.02038", "pdf": "https://arxiv.org/pdf/2506.02038", "abs": "https://arxiv.org/abs/2506.02038", "authors": ["Anum Nawaz", "Hafiz Humza Mahmood Ramzan", "Xianjia Yu", "Zhuo Zou", "Tomi Westerlund"], "title": "Blockchain Powered Edge Intelligence for U-Healthcare in Privacy Critical and Time Sensitive Environment", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Edge Intelligence (EI) serves as a critical enabler for privacy-preserving\nsystems by providing AI-empowered computation and distributed caching services\nat the edge, thereby minimizing latency and enhancing data privacy. The\nintegration of blockchain technology further augments EI frameworks by ensuring\ntransactional transparency, auditability, and system-wide reliability through a\ndecentralized network model. However, the operational architecture of such\nsystems introduces inherent vulnerabilities, particularly due to the extensive\ndata interactions between edge gateways (EGs) and the distributed nature of\ninformation storage during service provisioning. To address these challenges,\nwe propose an autonomous computing model along with its interaction topologies\ntailored for privacy-critical and time-sensitive health applications. The\nsystem supports continuous monitoring, real-time alert notifications, disease\ndetection, and robust data processing and aggregation. It also includes a data\ntransaction handler and mechanisms for ensuring privacy at the EGs. Moreover, a\nresource-efficient one-dimensional convolutional neural network (1D-CNN) is\nproposed for the multiclass classification of arrhythmia, enabling accurate and\nreal-time analysis of constrained EGs. Furthermore, a secure access scheme is\ndefined to manage both off-chain and on-chain data sharing and storage. To\nvalidate the proposed model, comprehensive security, performance, and cost\nanalyses are conducted, demonstrating the efficiency and reliability of the\nfine-grained access control scheme.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fb9\u7f18\u667a\u80fd\u548c\u533a\u5757\u94fe\u6280\u672f\u7684\u81ea\u4e3b\u8ba1\u7b97\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u9690\u79c1\u4fdd\u62a4\u548c\u65f6\u6548\u6027\u5f3a\u7684\u5065\u5eb7\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u76841D-CNN\u7528\u4e8e\u5fc3\u5f8b\u5931\u5e38\u5206\u7c7b\u3002", "motivation": "\u8fb9\u7f18\u667a\u80fd\u548c\u533a\u5757\u94fe\u7684\u7ed3\u5408\u867d\u80fd\u63d0\u5347\u9690\u79c1\u4fdd\u62a4\u548c\u7cfb\u7edf\u53ef\u9760\u6027\uff0c\u4f46\u5176\u67b6\u6784\u4e2d\u5b58\u5728\u6570\u636e\u4ea4\u4e92\u548c\u5b58\u50a8\u7684\u6f0f\u6d1e\uff0c\u5c24\u5176\u662f\u5728\u5065\u5eb7\u5e94\u7528\u4e2d\u3002", "method": "\u63d0\u51fa\u81ea\u4e3b\u8ba1\u7b97\u6a21\u578b\u53ca\u5176\u4ea4\u4e92\u62d3\u6251\u7ed3\u6784\uff0c\u5305\u62ec\u6570\u636e\u4e8b\u52a1\u5904\u7406\u5668\u3001\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u30011D-CNN\u5206\u7c7b\u5668\u548c\u5b89\u5168\u8bbf\u95ee\u65b9\u6848\u3002", "result": "\u7ecf\u5b89\u5168\u6027\u3001\u6027\u80fd\u548c\u6210\u672c\u5206\u6790\u9a8c\u8bc1\uff0c\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u8bbf\u95ee\u63a7\u5236\u548c\u5b9e\u65f6\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u53ef\u9760\u3002", "conclusion": "\u6a21\u578b\u9002\u7528\u4e8e\u9690\u79c1\u5173\u952e\u548c\u65f6\u95f4\u654f\u611f\u7684\u5065\u5eb7\u5e94\u7528\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u67b6\u6784\u7684\u8106\u5f31\u6027\u95ee\u9898\u3002", "keywords": "\u8fb9\u7f18\u667a\u80fd, \u533a\u5757\u94fe, \u9690\u79c1\u4fdd\u62a4, 1D-CNN, \u5065\u5eb7\u5e94\u7528"}}
{"id": "2506.02044", "pdf": "https://arxiv.org/pdf/2506.02044", "abs": "https://arxiv.org/abs/2506.02044", "authors": ["Xinxu Wei", "Kanhao Zhao", "Yong Jiao", "Lifang He", "Yu Zhang"], "title": "A Brain Graph Foundation Model: Pre-Training and Prompt-Tuning for Any Atlas and Disorder", "categories": ["q-bio.NC", "cs.LG"], "comment": "34pages", "summary": "As large language models (LLMs) continue to revolutionize AI research, there\nis a growing interest in building large-scale brain foundation models to\nadvance neuroscience. While most existing brain foundation models are\npre-trained on time-series signals or region-of-interest (ROI) features, we\npropose a novel graph-based pre-training paradigm for constructing a brain\ngraph foundation model. In this paper, we introduce the Brain Graph Foundation\nModel, termed BrainGFM, a unified framework that leverages graph contrastive\nlearning and graph masked autoencoders for large-scale fMRI-based pre-training.\nBrainGFM is pre-trained on a diverse mixture of brain atlases with varying\nparcellations, significantly expanding the pre-training corpus and enhancing\nthe model's ability to generalize across heterogeneous fMRI-derived brain\nrepresentations. To support efficient and versatile downstream transfer, we\nintegrate both graph prompts and language prompts into the model design,\nenabling BrainGFM to flexibly adapt to a wide range of atlases, neurological\nand psychiatric disorders, and task settings. Furthermore, we employ\nmeta-learning to optimize the graph prompts, facilitating strong generalization\nto previously unseen disorders under both few-shot and zero-shot learning\nconditions via language-guided prompting. BrainGFM is pre-trained on 27\nneuroimaging datasets spanning 25 common neurological and psychiatric\ndisorders, encompassing 2 types of brain atlases (functional and anatomical)\nacross 8 widely-used parcellations, and covering over 25,000 subjects, 60,000\nfMRI scans, and a total of 400,000 graph samples aggregated across all atlases\nand parcellations. The code is available at:\nhttps://github.com/weixinxu666/BrainGFM", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7684\u65b0\u578b\u5927\u8111\u57fa\u7840\u6a21\u578bBrainGFM\uff0c\u5229\u7528\u56fe\u5bf9\u6bd4\u5b66\u4e60\u548c\u56fe\u63a9\u7801\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u5927\u89c4\u6a21fMRI\u9884\u8bad\u7ec3\uff0c\u652f\u6301\u901a\u8fc7\u56fe\u63d0\u793a\u548c\u8bed\u8a00\u63d0\u793a\u7075\u6d3b\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728AI\u7814\u7a76\u4e2d\u7684\u53d1\u5c55\uff0c\u795e\u7ecf\u79d1\u5b66\u9886\u57df\u5bf9\u6784\u5efa\u5927\u89c4\u6a21\u5927\u8111\u57fa\u7840\u6a21\u578b\u7684\u5174\u8da3\u65e5\u76ca\u589e\u957f\uff0c\u73b0\u6709\u6a21\u578b\u591a\u57fa\u4e8e\u65f6\u95f4\u5e8f\u5217\u6216ROI\u7279\u5f81\uff0c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u9884\u8bad\u7ec3\u8303\u5f0f\u4ee5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u56fe\u5bf9\u6bd4\u5b66\u4e60\u548c\u56fe\u63a9\u7801\u81ea\u7f16\u7801\u5668\u6784\u5efaBrainGFM\uff0c\u6574\u5408\u591a\u6837\u5927\u8111\u56fe\u8c31\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7ed3\u5408\u56fe\u63d0\u793a\u548c\u8bed\u8a00\u63d0\u793a\u4f18\u5316\u6a21\u578b\u9002\u5e94\u6027\uff0c\u5e76\u4f7f\u7528\u5143\u5b66\u4e60\u4f18\u5316\u56fe\u63d0\u793a\u4ee5\u5b9e\u73b0\u96f6\u6837\u672c\u6216\u5c11\u6837\u672c\u5b66\u4e60\u3002", "result": "BrainGFM\u572827\u4e2a\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\uff0c\u6db5\u76d625\u79cd\u5e38\u89c1\u795e\u7ecf\u548c\u7cbe\u795e\u75be\u75c5\uff0c\u652f\u6301\u591a\u79cd\u56fe\u8c31\u548c\u4efb\u52a1\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "BrainGFM\u4e3a\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u7075\u6d3b\u7684\u63d0\u793a\u673a\u5236\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u5728\u96f6\u6837\u672c\u6216\u5c11\u6837\u672c\u5b66\u4e60\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "keywords": "\u5927\u8111\u57fa\u7840\u6a21\u578b\u3001\u56fe\u5bf9\u6bd4\u5b66\u4e60\u3001\u56fe\u63a9\u7801\u81ea\u7f16\u7801\u5668\u3001fMRI\u3001\u5143\u5b66\u4e60"}}
{"id": "2506.03135", "pdf": "https://arxiv.org/pdf/2506.03135", "abs": "https://arxiv.org/abs/2506.03135", "authors": ["Mengdi Jia", "Zekun Qi", "Shaochen Zhang", "Wenyao Zhang", "Xinqiang Yu", "Jiawei He", "He Wang", "Li Yi"], "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project Page: https://qizekun.github.io/omnispatial/", "summary": "Spatial reasoning is a key aspect of cognitive psychology and remains a major\nbottleneck for current vision-language models (VLMs). While extensive research\nhas aimed to evaluate or improve VLMs' understanding of basic spatial\nrelations, such as distinguishing left from right, near from far, and object\ncounting, these tasks represent only the most fundamental level of spatial\nreasoning. In this work, we introduce OmniSpatial, a comprehensive and\nchallenging benchmark for spatial reasoning, grounded in cognitive psychology.\nOmniSpatial covers four major categories: dynamic reasoning, complex spatial\nlogic, spatial interaction, and perspective-taking, with 50 fine-grained\nsubcategories. Through Internet data crawling and careful manual annotation, we\nconstruct over 1.5K question-answer pairs. Extensive experiments show that both\nopen- and closed-source VLMs, as well as existing reasoning and spatial\nunderstanding models, exhibit significant limitations in comprehensive spatial\nunderstanding. We further analyze failure cases and propose potential\ndirections for future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aOmniSpatial\u7684\u5168\u9762\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u7a7a\u95f4\u63a8\u7406\u57fa\u51c6\uff0c\u6db5\u76d6\u52a8\u6001\u63a8\u7406\u3001\u590d\u6742\u7a7a\u95f4\u903b\u8f91\u3001\u7a7a\u95f4\u4ea4\u4e92\u548c\u89c6\u89d2\u8f6c\u6362\u56db\u5927\u7c7b\u522b\uff0c\u65e8\u5728\u8bc4\u4f30\u548c\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u7a7a\u95f4\u63a8\u7406\u662f\u8ba4\u77e5\u5fc3\u7406\u5b66\u7684\u5173\u952e\u90e8\u5206\uff0c\u4f46\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u8fd9\u4e00\u9886\u57df\u4ecd\u5b58\u5728\u660e\u663e\u74f6\u9888\u3002\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u57fa\u7840\u7a7a\u95f4\u5173\u7cfb\u7684\u8bc4\u4f30\u4e0a\uff0c\u7f3a\u4e4f\u5bf9\u66f4\u590d\u6742\u7a7a\u95f4\u63a8\u7406\u7684\u5168\u9762\u6d4b\u8bd5\u3002", "method": "\u901a\u8fc7\u4e92\u8054\u7f51\u6570\u636e\u722c\u53d6\u548c\u7ec6\u81f4\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u6784\u5efa\u4e86\u5305\u542b50\u4e2a\u5b50\u7c7b\u522b\u3001\u8d85\u8fc71.5K\u95ee\u7b54\u5bf9\u7684OmniSpatial\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u7684\u5f00\u6e90\u548c\u95ed\u6e90VLMs\uff0c\u4ee5\u53ca\u7a7a\u95f4\u7406\u89e3\u6a21\u578b\u5728\u5168\u9762\u7a7a\u95f4\u7406\u89e3\u4e0a\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u3002", "conclusion": "OmniSpatial\u57fa\u51c6\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002", "keywords": "\u7a7a\u95f4\u63a8\u7406, \u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u8ba4\u77e5\u5fc3\u7406\u5b66, \u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2506.03144", "pdf": "https://arxiv.org/pdf/2506.03144", "abs": "https://arxiv.org/abs/2506.03144", "authors": ["Wei Chow", "Yuan Gao", "Linfeng Li", "Xian Wang", "Qi Xu", "Hang Song", "Lingdong Kong", "Ran Zhou", "Yi Zeng", "Yidong Cai", "Botian Jiang", "Shilin Xu", "Jiajun Zhang", "Minghui Qiu", "Xiangtai Li", "Tianshu Yang", "Siliang Tang", "Juncheng Li"], "title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "Preprint; Project Page, Code, and Dataset at:\n  https://merit-2025.github.io/", "summary": "Semantic retrieval is crucial for modern applications yet remains\nunderexplored in current research. Existing datasets are limited to single\nlanguages, single images, or singular retrieval conditions, often failing to\nfully exploit the expressive capacity of visual information as evidenced by\nmaintained performance when images are replaced with captions. However,\npractical retrieval scenarios frequently involve interleaved multi-condition\nqueries with multiple images. Hence, this paper introduces MERIT, the first\nmultilingual dataset for interleaved multi-condition semantic retrieval,\ncomprising 320,000 queries with 135,000 products in 5 languages, covering 7\ndistinct product categories. Extensive experiments on MERIT identify existing\nmodels's limitation: focusing solely on global semantic information while\nneglecting specific conditional elements in queries. Consequently, we propose\nCoral, a novel fine-tuning framework that adapts pre-trained MLLMs by\nintegrating embedding reconstruction to preserve fine-grained conditional\nelements and contrastive learning to extract comprehensive global semantics.\nExperiments demonstrate that Coral achieves a 45.9% performance improvement\nover conventional approaches on MERIT, with strong generalization capabilities\nvalidated across 8 established retrieval benchmarks. Collectively, our\ncontributions - a novel dataset, identification of critical limitations in\nexisting approaches, and an innovative fine-tuning framework - establish a\nfoundation for future research in interleaved multi-condition semantic\nretrieval.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMERIT\u6570\u636e\u96c6\u548cCoral\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u591a\u6761\u4ef6\u8bed\u4e49\u68c0\u7d22\u95ee\u9898\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u68c0\u7d22\u7814\u7a76\u5c40\u9650\u4e8e\u5355\u8bed\u8a00\u6216\u5355\u56fe\u50cf\u6761\u4ef6\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u89c6\u89c9\u4fe1\u606f\uff0c\u5b9e\u9645\u5e94\u7528\u9700\u8981\u591a\u6761\u4ef6\u67e5\u8be2\u3002", "method": "\u5f15\u5165MERIT\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u63d0\u51faCoral\u6846\u67b6\uff0c\u901a\u8fc7\u5d4c\u5165\u91cd\u6784\u548c\u5bf9\u6bd4\u5b66\u4e60\u4f18\u5316\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "result": "Coral\u5728MERIT\u4e0a\u6027\u80fd\u63d0\u534745.9%\uff0c\u5e76\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8bba\u6587\u4e3a\u591a\u6761\u4ef6\u8bed\u4e49\u68c0\u7d22\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff0c\u8d21\u732e\u5305\u62ec\u6570\u636e\u96c6\u3001\u95ee\u9898\u53d1\u73b0\u548c\u521b\u65b0\u6846\u67b6\u3002", "keywords": "\u8bed\u4e49\u68c0\u7d22,\u591a\u6761\u4ef6\u67e5\u8be2,MERIT\u6570\u636e\u96c6,Coral\u6846\u67b6,\u591a\u8bed\u8a00"}}
{"id": "2506.03147", "pdf": "https://arxiv.org/pdf/2506.03147", "abs": "https://arxiv.org/abs/2506.03147", "authors": ["Bin Lin", "Zongjian Li", "Xinhua Cheng", "Yuwei Niu", "Yang Ye", "Xianyi He", "Shenghai Yuan", "Wangbo Yu", "Shaodong Wang", "Yunyang Ge", "Yatian Pang", "Li Yuan"], "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets.", "AI": {"tldr": "UniWorld\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u4e49\u7279\u5f81\u7684\u7edf\u4e00\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5bf9\u6bd4\u8bed\u4e49\u7f16\u7801\u5668\uff0c\u5728\u5c11\u91cf\u6570\u636e\u4e0b\u8d85\u8d8a\u73b0\u6709\u6a21\u578bBAGEL\uff0c\u5e76\u5728\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u6a21\u578b\u5728\u56fe\u50cf\u611f\u77e5\u548c\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u800cGPT-4o-Image\u7684\u6210\u529f\u542f\u53d1\u4e86\u5229\u7528\u8bed\u4e49\u7279\u5f81\u6784\u5efa\u65b0\u6a21\u578b\u7684\u601d\u8def\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5bf9\u6bd4\u8bed\u4e49\u7f16\u7801\u5668\u63d0\u4f9b\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u751f\u6210\u6846\u67b6UniWorld\u3002", "result": "UniWorld\u4ec5\u75281%\u7684\u6570\u636e\u5c31\u5728\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e0a\u8d85\u8d8aBAGEL\uff0c\u540c\u65f6\u5728\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "UniWorld\u5c55\u793a\u4e86\u8bed\u4e49\u7279\u5f81\u5728\u7edf\u4e00\u751f\u6210\u6a21\u578b\u4e2d\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u56fe\u50cf\u611f\u77e5\u548c\u64cd\u4f5c\u4efb\u52a1\u7684\u8fdb\u6b65\u3002", "keywords": "UniWorld, \u56fe\u50cf\u611f\u77e5, \u56fe\u50cf\u64cd\u4f5c, GPT-4o-Image, \u8bed\u4e49\u7279\u5f81"}}
{"id": "2506.02438", "pdf": "https://arxiv.org/pdf/2506.02438", "abs": "https://arxiv.org/abs/2506.02438", "authors": ["Sudhanshu Sekhar Tripathy", "Bichitrananda Behera"], "title": "A Review of Various Datasets for Machine Learning Algorithm-Based Intrusion Detection System: Advances and Challenges", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "IDS aims to protect computer networks from security threats by detecting,\nnotifying, and taking appropriate action to prevent illegal access and protect\nconfidential information. As the globe becomes increasingly dependent on\ntechnology and automated processes, ensuring secured systems, applications, and\nnetworks has become one of the most significant problems of this era. The\nglobal web and digital technology have significantly accelerated the evolution\nof the modern world, necessitating the use of telecommunications and data\ntransfer platforms. Researchers are enhancing the effectiveness of IDS by\nincorporating popular datasets into machine learning algorithms. IDS, equipped\nwith machine learning classifiers, enhances security attack detection accuracy\nby identifying normal or abnormal network traffic. This paper explores the\nmethods of capturing and reviewing intrusion detection systems (IDS) and\nevaluates the challenges existing datasets face. A deluge of research on\nmachine learning (ML) and deep learning (DL) architecture-based intrusion\ndetection techniques has been conducted in the past ten years on various\ncybersecurity datasets, including KDDCUP'99, NSL-KDD, UNSW-NB15, CICIDS-2017,\nand CSE-CIC-IDS2018. We conducted a literature review and presented an in-depth\nanalysis of various intrusion detection methods that use SVM, KNN, DT, LR, NB,\nRF, XGBOOST, Adaboost, and ANN. We provide an overview of each technique,\nexplaining the role of the classifiers and algorithms used. A detailed tabular\nanalysis highlights the datasets used, classifiers employed, attacks detected,\nevaluation metrics, and conclusions drawn. This article offers a thorough\nreview for future IDS research.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08IDS\uff09\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u5728\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u6027\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u5206\u6790\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u6311\u6218\u3002", "motivation": "\u968f\u7740\u5168\u7403\u5bf9\u6280\u672f\u548c\u81ea\u52a8\u5316\u6d41\u7a0b\u7684\u4f9d\u8d56\u52a0\u6df1\uff0c\u4fdd\u969c\u7cfb\u7edf\u548c\u7f51\u7edc\u5b89\u5168\u6210\u4e3a\u91cd\u8981\u95ee\u9898\u3002\u673a\u5668\u5b66\u4e60\u5728IDS\u4e2d\u7684\u5e94\u7528\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u6790\u591a\u79cd\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\uff08\u5982SVM\u3001KNN\u3001DT\u7b49\uff09\u5728IDS\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u8bc4\u4f30\u6570\u636e\u96c6\uff08\u5982KDDCUP'99\u3001NSL-KDD\u7b49\uff09\u7684\u6311\u6218\u3002", "result": "\u603b\u7ed3\u4e86\u5404\u5206\u7c7b\u5668\u7684\u89d2\u8272\u3001\u7b97\u6cd5\u53ca\u68c0\u6d4b\u6548\u679c\uff0c\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u6570\u636e\u96c6\u3001\u5206\u7c7b\u5668\u3001\u653b\u51fb\u7c7b\u578b\u548c\u8bc4\u4f30\u6307\u6807\u7684\u8868\u683c\u5206\u6790\u3002", "conclusion": "\u672c\u6587\u4e3a\u672a\u6765IDS\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7efc\u8ff0\u548c\u5206\u6790\u3002", "keywords": "\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf, \u673a\u5668\u5b66\u4e60, \u6570\u636e\u96c6, \u5206\u7c7b\u5668, \u7f51\u7edc\u5b89\u5168"}}
{"id": "2506.02448", "pdf": "https://arxiv.org/pdf/2506.02448", "abs": "https://arxiv.org/abs/2506.02448", "authors": ["Baoyu Liang", "Qile Su", "Shoutai Zhu", "Yuchen Liang", "Chao Tong"], "title": "VidEvent: A Large Dataset for Understanding Dynamic Evolution of Events in Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite the significant impact of visual events on human cognition,\nunderstanding events in videos remains a challenging task for AI due to their\ncomplex structures, semantic hierarchies, and dynamic evolution. To address\nthis, we propose the task of video event understanding that extracts event\nscripts and makes predictions with these scripts from videos. To support this\ntask, we introduce VidEvent, a large-scale dataset containing over 23,000\nwell-labeled events, featuring detailed event structures, broad hierarchies,\nand logical relations extracted from movie recap videos. The dataset was\ncreated through a meticulous annotation process, ensuring high-quality and\nreliable event data. We also provide comprehensive baseline models offering\ndetailed descriptions of their architecture and performance metrics. These\nmodels serve as benchmarks for future research, facilitating comparisons and\nimprovements. Our analysis of VidEvent and the baseline models highlights the\ndataset's potential to advance video event understanding and encourages the\nexploration of innovative algorithms and models. The dataset and related\nresources are publicly available at www.videvent.top.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u89c6\u9891\u4e8b\u4ef6\u7406\u89e3\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u5927\u89c4\u6a21\u6570\u636e\u96c6VidEvent\uff0c\u5305\u542b23,000\u591a\u4e2a\u6807\u6ce8\u4e8b\u4ef6\uff0c\u652f\u6301\u4e8b\u4ef6\u811a\u672c\u63d0\u53d6\u548c\u9884\u6d4b\u3002", "motivation": "\u7531\u4e8e\u4e8b\u4ef6\u590d\u6742\u7684\u7ed3\u6784\u3001\u8bed\u4e49\u5c42\u6b21\u548c\u52a8\u6001\u6f14\u5316\uff0c\u89c6\u9891\u4e8b\u4ef6\u7406\u89e3\u5bf9AI\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u7cbe\u5fc3\u6807\u6ce8\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6VidEvent\u63d0\u4f9b\u4e8b\u4ef6\u7ed3\u6784\u548c\u903b\u8f91\u5173\u7cfb\uff0c\u5e76\u5efa\u7acb\u57fa\u51c6\u6a21\u578b\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u7684\u57fa\u51c6\u3002", "result": "\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6a21\u578b\u5c55\u793a\u4e86\u5176\u5728\u89c6\u9891\u4e8b\u4ef6\u7406\u89e3\u4e2d\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u521b\u65b0\u7b97\u6cd5\u7684\u63a2\u7d22\u3002", "conclusion": "VidEvent\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6a21\u578b\u4e3a\u89c6\u9891\u4e8b\u4ef6\u7406\u89e3\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u8d44\u6e90\uff0c\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002", "keywords": "\u89c6\u9891\u4e8b\u4ef6\u7406\u89e3\uff0cVidEvent\u6570\u636e\u96c6\uff0c\u4e8b\u4ef6\u811a\u672c\uff0c\u57fa\u51c6\u6a21\u578b"}}
{"id": "2506.02068", "pdf": "https://arxiv.org/pdf/2506.02068", "abs": "https://arxiv.org/abs/2506.02068", "authors": ["Yun-Cheng Tsai", "Yen-Ku Liu", "Samuel Yen-Chi Chen"], "title": "Enhancing Interpretability of Quantum-Assisted Blockchain Clustering via AI Agent-Based Qualitative Analysis", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "Blockchain transaction data is inherently high dimensional, noisy, and\nentangled, posing substantial challenges for traditional clustering algorithms.\nWhile quantum enhanced clustering models have demonstrated promising\nperformance gains, their interpretability remains limited, restricting their\napplication in sensitive domains such as financial fraud detection and\nblockchain governance. To address this gap, we propose a two stage analysis\nframework that synergistically combines quantitative clustering evaluation with\nAI Agent assisted qualitative interpretation. In the first stage, we employ\nclassical clustering methods and evaluation metrics including the Silhouette\nScore, Davies Bouldin Index, and Calinski Harabasz Index to determine the\noptimal cluster count and baseline partition quality. In the second stage, we\nintegrate an AI Agent to generate human readable, semantic explanations of\nclustering results, identifying intra cluster characteristics and inter cluster\nrelationships. Our experiments reveal that while fully trained Quantum Neural\nNetworks (QNN) outperform random Quantum Features (QF) in quantitative metrics,\nthe AI Agent further uncovers nuanced differences between these methods,\nnotably exposing the singleton cluster phenomenon in QNN driven models. The\nconsolidated insights from both stages consistently endorse the three cluster\nconfiguration, demonstrating the practical value of our hybrid approach. This\nwork advances the interpretability frontier in quantum assisted blockchain\nanalytics and lays the groundwork for future autonomous AI orchestrated\nclustering frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u5206\u6790\u6846\u67b6\uff0c\u7ed3\u5408\u5b9a\u91cf\u805a\u7c7b\u8bc4\u4f30\u4e0eAI\u4ee3\u7406\u8f85\u52a9\u7684\u5b9a\u6027\u89e3\u91ca\uff0c\u63d0\u5347\u91cf\u5b50\u589e\u5f3a\u805a\u7c7b\u6a21\u578b\u5728\u533a\u5757\u94fe\u6570\u636e\u5206\u6790\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u533a\u5757\u94fe\u4ea4\u6613\u6570\u636e\u9ad8\u7ef4\u3001\u566a\u58f0\u591a\u4e14\u7ea0\u7f20\uff0c\u4f20\u7edf\u805a\u7c7b\u7b97\u6cd5\u96be\u4ee5\u5904\u7406\uff0c\u91cf\u5b50\u589e\u5f3a\u805a\u7c7b\u6a21\u578b\u867d\u6027\u80fd\u597d\u4f46\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u7528\u7ecf\u5178\u805a\u7c7b\u65b9\u6cd5\u548c\u8bc4\u4f30\u6307\u6807\u786e\u5b9a\u6700\u4f18\u805a\u7c7b\u6570\u91cf\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5f15\u5165AI\u4ee3\u7406\u751f\u6210\u805a\u7c7b\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u8bed\u4e49\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNN\uff09\u4f18\u4e8e\u968f\u673a\u91cf\u5b50\u7279\u5f81\uff08QF\uff09\uff0cAI\u4ee3\u7406\u8fdb\u4e00\u6b65\u63ed\u793aQNN\u9a71\u52a8\u7684\u5355\u4f8b\u805a\u7c7b\u73b0\u8c61\u3002\u4e09\u805a\u7c7b\u914d\u7f6e\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u5347\u4e86\u91cf\u5b50\u8f85\u52a9\u533a\u5757\u94fe\u5206\u6790\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u672a\u6765\u81ea\u4e3bAI\u9a71\u52a8\u7684\u805a\u7c7b\u6846\u67b6\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "keywords": "\u533a\u5757\u94fe\u5206\u6790, \u91cf\u5b50\u805a\u7c7b, \u53ef\u89e3\u91ca\u6027, AI\u4ee3\u7406"}}
{"id": "2506.02488", "pdf": "https://arxiv.org/pdf/2506.02488", "abs": "https://arxiv.org/abs/2506.02488", "authors": ["Hongtao Huang", "Xiaojun Chang", "Lina Yao"], "title": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for Efficient Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality.", "AI": {"tldr": "Flexiffusion\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684NAS\u6846\u67b6\uff0c\u901a\u8fc7\u7075\u6d3b\u5206\u6bb5\u548c\u8f7b\u91cf\u7ea7\u8bc4\u4f30\u4f18\u5316\u6269\u6563\u6a21\u578b\uff0c\u663e\u8457\u52a0\u901f\u63a8\u7406\u5e76\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u56e0\u591a\u6b65\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u4f20\u7edfNAS\u65b9\u6cd5\u5b58\u5728\u91cd\u8bad\u7ec3\u9700\u6c42\u3001\u641c\u7d22\u590d\u6742\u5ea6\u9ad8\u548c\u8bc4\u4f30\u8017\u65f6\u7684\u95ee\u9898\u3002", "method": "Flexiffusion\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u4e3a\u7075\u6d3b\u5206\u6bb5\uff0c\u52a8\u6001\u7ec4\u5408\u5168\u6b65\u3001\u90e8\u5206\u6b65\u548c\u8df3\u6b65\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7rFID\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b02\u500d\u81f35.1\u500d\u52a0\u901f\uff0cFID\u9000\u5316\u4f4e\u4e8e5%\uff0cStable Diffusion\u4e0aCLIP\u5206\u6570\u51e0\u4e4e\u4e0d\u53d8\u3002", "conclusion": "Flexiffusion\u5f00\u521b\u4e86\u9ad8\u6548\u641c\u7d22\u9ad8\u901f\u6269\u6563\u6a21\u578b\u7684\u8303\u5f0f\uff0c\u663e\u8457\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u3002", "keywords": "Diffusion Models, Neural Architecture Search, Training-Free, Speedup, FID"}}
{"id": "2506.02075", "pdf": "https://arxiv.org/pdf/2506.02075", "abs": "https://arxiv.org/abs/2506.02075", "authors": ["Christian Marius Lillelund", "Shi-ang Qi", "Russell Greiner", "Christian Fischer Pedersen"], "title": "Stop Chasing the C-index: This Is How We Should Evaluate Our Survival Models", "categories": ["stat.ME", "cs.LG"], "comment": null, "summary": "We argue that many survival analysis and time-to-event models are incorrectly\nevaluated. First, we survey many examples of evaluation approaches in the\nliterature and find that most rely on concordance (C-index). However, the\nC-index only measures a model's discriminative ability and does not assess\nother important aspects, such as the accuracy of the time-to-event predictions\nor the calibration of the model's probabilistic estimates. Next, we present a\nset of key desiderata for choosing the right evaluation metric and discuss\ntheir pros and cons. These are tailored to the challenges in survival analysis,\nsuch as sensitivity to miscalibration and various censoring assumptions. We\nhypothesize that the current development of survival metrics conforms to a\ndouble-helix ladder, and that model validity and metric validity must stand on\nthe same rung of the assumption ladder. Finally, we discuss the appropriate\nmethods for evaluating a survival model in practice and summarize various\nviewpoints opposing our analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\u5f53\u524d\u751f\u5b58\u5206\u6790\u6a21\u578b\u7684\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u8bef\u533a\uff0c\u5c24\u5176\u662f\u8fc7\u5ea6\u4f9d\u8d56C\u6307\u6570\uff0c\u672a\u80fd\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u7684\u5176\u4ed6\u5173\u952e\u65b9\u9762\uff0c\u5982\u65f6\u95f4\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6982\u7387\u6821\u51c6\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6807\u51c6\u548c\u5b9e\u8df5\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u751f\u5b58\u5206\u6790\u548c\u65f6\u95f4\u5230\u4e8b\u4ef6\u6a21\u578b\u7684\u8bc4\u4f30\u65b9\u6cd5\u8fc7\u4e8e\u4f9d\u8d56C\u6307\u6570\uff0c\u5ffd\u89c6\u4e86\u5176\u4ed6\u5173\u952e\u6307\u6807\u5982\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6821\u51c6\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u8bc4\u4f30\u4e0d\u5168\u9762\u3002", "method": "\u901a\u8fc7\u6587\u732e\u8c03\u67e5\u603b\u7ed3\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e00\u7ec4\u9488\u5bf9\u751f\u5b58\u5206\u6790\u7279\u70b9\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u8ba8\u8bba\u5176\u4f18\u7f3a\u70b9\u3002", "result": "\u63ed\u793a\u4e86\u5f53\u524d\u751f\u5b58\u5206\u6790\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u65b0\u6807\u51c6\u5e76\u5047\u8bbe\u8bc4\u4f30\u65b9\u6cd5\u548c\u6a21\u578b\u5047\u8bbe\u9700\u540c\u6b65\u53d1\u5c55\u3002", "conclusion": "\u8bba\u6587\u4e3b\u5f20\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5f3a\u8c03\u6a21\u578b\u548c\u8bc4\u4f30\u6307\u6807\u9700\u5728\u76f8\u540c\u5047\u8bbe\u57fa\u7840\u4e0a\u53d1\u5c55\u3002", "keywords": "\u751f\u5b58\u5206\u6790, \u65f6\u95f4\u5230\u4e8b\u4ef6\u6a21\u578b, C\u6307\u6570, \u8bc4\u4f30\u65b9\u6cd5, \u6821\u51c6\u6027"}}
{"id": "2506.02490", "pdf": "https://arxiv.org/pdf/2506.02490", "abs": "https://arxiv.org/abs/2506.02490", "authors": ["Yong Xiang", "Charley Peter Chen", "Liyi Zeng", "Wei Yin", "Xin Liu", "Hu Li", "Wei Xu"], "title": "Simplifying Root Cause Analysis in Kubernetes with StateGraph and LLM", "categories": ["cs.DC", "cs.AI", "cs.SE"], "comment": "12 pages, 13 figures, 5 tables", "summary": "Kubernetes, a notably complex and distributed system, utilizes an array of\ncontrollers to uphold cluster management logic through state reconciliation.\nNevertheless, maintaining state consistency presents significant challenges due\nto unexpected failures, network disruptions, and asynchronous issues,\nespecially within dynamic cloud environments. These challenges result in\noperational disruptions and economic losses, underscoring the necessity for\nrobust root cause analysis (RCA) to enhance Kubernetes reliability. The\ndevelopment of large language models (LLMs) presents a promising direction for\nRCA. However, existing methodologies encounter several obstacles, including the\ndiverse and evolving nature of Kubernetes incidents, the intricate context of\nincidents, and the polymorphic nature of these incidents. In this paper, we\nintroduce SynergyRCA, an innovative tool that leverages LLMs with retrieval\naugmentation from graph databases and enhancement with expert prompts.\nSynergyRCA constructs a StateGraph to capture spatial and temporal\nrelationships and utilizes a MetaGraph to outline entity connections. Upon the\noccurrence of an incident, an LLM predicts the most pertinent resource, and\nSynergyRCA queries the MetaGraph and StateGraph to deliver context-specific\ninsights for RCA. We evaluate SynergyRCA using datasets from two production\nKubernetes clusters, highlighting its capacity to identify numerous root\ncauses, including novel ones, with high efficiency and precision. SynergyRCA\ndemonstrates the ability to identify root causes in an average time of about\ntwo minutes and achieves an impressive precision of approximately 0.90.", "AI": {"tldr": "SynergyRCA\u662f\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u5de5\u5177\u7684\u521b\u65b0\u5de5\u5177\uff0c\u7528\u4e8eKubernetes\u96c6\u7fa4\u7684\u6839\u56e0\u5206\u6790\uff08RCA\uff09\uff0c\u80fd\u591f\u9ad8\u6548\u4e14\u7cbe\u786e\u5730\u8bc6\u522b\u6839\u56e0\u3002", "motivation": "\u7531\u4e8eKubernetes\u96c6\u7fa4\u5728\u52a8\u6001\u4e91\u73af\u5883\u4e2d\u9762\u4e34\u72b6\u6001\u4e00\u81f4\u6027\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u64cd\u4f5c\u4e2d\u65ad\u548c\u7ecf\u6d4e\u635f\u5931\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7a33\u5065\u7684\u6839\u56e0\u5206\u6790\u65b9\u6cd5\u6765\u63d0\u5347\u5176\u53ef\u9760\u6027\u3002", "method": "SynergyRCA\u7ed3\u5408\u4e86LLMs\u3001\u56fe\u6570\u636e\u5e93\u7684\u68c0\u7d22\u589e\u5f3a\u548c\u4e13\u5bb6\u63d0\u793a\uff0c\u901a\u8fc7\u6784\u5efaStateGraph\u548cMetaGraph\u6765\u6355\u83b7\u65f6\u7a7a\u5173\u7cfb\u548c\u5b9e\u4f53\u8fde\u63a5\uff0c\u4ece\u800c\u63d0\u4f9b\u4e0a\u4e0b\u6587\u76f8\u5173\u7684RCA\u89c1\u89e3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSynergyRCA\u80fd\u591f\u5728\u7ea6\u4e24\u5206\u949f\u5185\u8bc6\u522b\u6839\u56e0\uff08\u5305\u62ec\u65b0\u53d1\u73b0\u7684\u95ee\u9898\uff09\uff0c\u5e76\u8fbe\u5230\u7ea60.90\u7684\u7cbe\u786e\u5ea6\u3002", "conclusion": "SynergyRCA\u5728\u63d0\u5347Kubernetes\u53ef\u9760\u6027\u7684\u6839\u56e0\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u7cbe\u786e\u6027\u3002", "keywords": "Kubernetes, \u6839\u56e0\u5206\u6790, \u5927\u578b\u8bed\u8a00\u6a21\u578b, StateGraph, MetaGraph"}}
{"id": "2506.02076", "pdf": "https://arxiv.org/pdf/2506.02076", "abs": "https://arxiv.org/abs/2506.02076", "authors": ["Christian Marius Lillelund", "Sanjay Kalra", "Russell Greiner"], "title": "A meaningful prediction of functional decline in amyotrophic lateral sclerosis based on multi-event survival analysis", "categories": ["q-bio.QM", "cs.LG", "stat.ML"], "comment": null, "summary": "Amyotrophic lateral sclerosis (ALS) is a degenerative disorder of motor\nneurons that causes progressive paralysis in patients. Current treatment\noptions aim to prolong survival and improve quality of life; however, due to\nthe heterogeneity of the disease, it is often difficult to determine the\noptimal time for potential therapies or medical interventions. In this study,\nwe propose a novel method to predict the time until a patient with ALS\nexperiences significant functional impairment (ALSFRS-R<=2) with respect to\nfive common functions: speaking, swallowing, handwriting, walking and\nbreathing. We formulate this task as a multi-event survival problem and\nvalidate our approach in the PRO-ACT dataset by training five covariate-based\nsurvival models to estimate the probability of an event over a 500-day period\nafter a baseline visit. We then predict five event-specific individual survival\ndistributions (ISDs) for each patient, each providing an interpretable and\nmeaningful estimate of when that event will likely take place in the future.\nThe results show that covariate-based models are superior to the Kaplan-Meier\nestimator at predicting time-to-event outcomes. Additionally, our method\nenables practitioners to make individual counterfactual predictions, where\ncertain features (covariates) can be changed to see their effect on the\npredicted outcome. In this regard, we find that Riluzole has little to no\nimpact on predicted functional decline. However, for patients with bulbar-onset\nALS, our method predicts considerably shorter counterfactual time-to-event\nestimates for tasks related to speech and swallowing compared to limb-onset\nALS. The proposed method can be applied to current clinical examination data to\nassess the risk of functional decline and thus allow more personalized\ntreatment planning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4bALS\u60a3\u8005\u529f\u80fd\u8870\u9000\u65f6\u95f4\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u4e8b\u4ef6\u751f\u5b58\u6a21\u578b\u9884\u6d4b\u4e94\u79cd\u5e38\u89c1\u529f\u80fd\u7684\u8870\u9000\u65f6\u95f4\uff0c\u5e76\u5728\u4e34\u5e8a\u6570\u636e\u4e2d\u9a8c\u8bc1\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "ALS\u662f\u4e00\u79cd\u5bfc\u81f4\u6e10\u8fdb\u6027\u762b\u75ea\u7684\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\uff0c\u73b0\u6709\u6cbb\u7597\u65b9\u6cd5\u96be\u4ee5\u786e\u5b9a\u6700\u4f73\u5e72\u9884\u65f6\u673a\uff0c\u9700\u8981\u66f4\u4e2a\u6027\u5316\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u4e8b\u4ef6\u751f\u5b58\u6a21\u578b\uff0c\u8bad\u7ec3\u4e94\u4e2a\u57fa\u4e8e\u534f\u53d8\u91cf\u7684\u751f\u5b58\u6a21\u578b\uff0c\u9884\u6d4b\u60a3\u8005\u5728\u672a\u6765500\u5929\u5185\u4e94\u79cd\u529f\u80fd\u7684\u8870\u9000\u6982\u7387\u3002", "result": "\u57fa\u4e8e\u534f\u53d8\u91cf\u7684\u6a21\u578b\u4f18\u4e8eKaplan-Meier\u4f30\u8ba1\u5668\uff0c\u5e76\u80fd\u8fdb\u884c\u53cd\u4e8b\u5b9e\u9884\u6d4b\uff0c\u53d1\u73b0Riluzole\u5bf9\u529f\u80fd\u8870\u9000\u5f71\u54cd\u8f83\u5c0f\uff0c\u5ef6\u9ad3\u8d77\u75c5ALS\u60a3\u8005\u7684\u8bf4\u8bdd\u548c\u541e\u54bd\u529f\u80fd\u8870\u9000\u66f4\u5feb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u4e34\u5e8a\u6570\u636e\u8bc4\u4f30\u529f\u80fd\u8870\u9000\u98ce\u9669\uff0c\u652f\u6301\u4e2a\u6027\u5316\u6cbb\u7597\u8ba1\u5212\u3002", "keywords": "ALS, \u529f\u80fd\u8870\u9000\u9884\u6d4b, \u591a\u4e8b\u4ef6\u751f\u5b58\u6a21\u578b, \u53cd\u4e8b\u5b9e\u9884\u6d4b, \u4e2a\u6027\u5316\u6cbb\u7597"}}
{"id": "2506.02091", "pdf": "https://arxiv.org/pdf/2506.02091", "abs": "https://arxiv.org/abs/2506.02091", "authors": ["Bartosz Karpi\u0144ski", "Cyryl Leszczy\u0144ski"], "title": "Comparison of spectrogram scaling in multi-label Music Genre Recognition", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "14 pages, 10 figures", "summary": "As the accessibility and ease-of-use of digital audio workstations increases,\nso does the quantity of music available to the average listener; additionally,\ndifferences between genres are not always well defined and can be abstract,\nwith widely varying combinations of genres across individual records. In this\narticle, multiple preprocessing methods and approaches to model training are\ndescribed and compared, accounting for the eclectic nature of today's albums. A\ncustom, manually labeled dataset of more than 18000 entries has been used to\nperform the experiments.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u968f\u7740\u6570\u5b57\u97f3\u9891\u5de5\u4f5c\u7ad9\u7684\u666e\u53ca\uff0c\u97f3\u4e50\u98ce\u683c\u754c\u9650\u6a21\u7cca\u7684\u95ee\u9898\uff0c\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u9884\u5904\u7406\u65b9\u6cd5\u548c\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e8618000\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u97f3\u9891\u5de5\u4f5c\u7ad9\u7684\u6613\u7528\u6027\u63d0\u5347\uff0c\u97f3\u4e50\u98ce\u683c\u754c\u9650\u53d8\u5f97\u6a21\u7cca\uff0c\u5bfc\u81f4\u97f3\u4e50\u5206\u7c7b\u56f0\u96be\uff0c\u9700\u8981\u6709\u6548\u7684\u9884\u5904\u7406\u548c\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u9884\u5904\u7406\u65b9\u6cd5\u548c\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u4e00\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u8d85\u8fc718000\u6761\u8bb0\u5f55\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4e3a\u5904\u7406\u591a\u6837\u5316\u7684\u97f3\u4e50\u98ce\u683c\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "conclusion": "\u8bba\u6587\u4e3a\u97f3\u4e50\u98ce\u683c\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u89c4\u6a21\u624b\u52a8\u6807\u6ce8\u6570\u636e\u96c6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u6570\u5b57\u97f3\u9891\u5de5\u4f5c\u7ad9, \u97f3\u4e50\u98ce\u683c\u5206\u7c7b, \u9884\u5904\u7406\u65b9\u6cd5, \u6a21\u578b\u8bad\u7ec3, \u624b\u52a8\u6807\u6ce8\u6570\u636e\u96c6"}}
{"id": "2506.02537", "pdf": "https://arxiv.org/pdf/2506.02537", "abs": "https://arxiv.org/abs/2506.02537", "authors": ["Hao Yan", "Handong Zheng", "Hao Wang", "Liang Yin", "Xingchen Liu", "Zhenbiao Cao", "Xinxing Su", "Zihao Chen", "Jihao Wu", "Minghui Liao", "Chao Weng", "Wei Chen", "Yuliang Liu", "Xiang Bai"], "title": "VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "Recent strides in multimodal large language models (MLLMs) have significantly\nadvanced their performance in many reasoning tasks. However, Abstract Visual\nReasoning (AVR) remains a critical challenge, primarily due to limitations in\nperceiving abstract graphics. To tackle this issue, we investigate the\nbottlenecks in current MLLMs and synthesize training data to improve their\nabstract visual perception. First, we propose VisuRiddles, a benchmark for AVR,\nfeaturing tasks meticulously constructed to assess models' reasoning capacities\nacross five core dimensions and two high-level reasoning categories. Second, we\nintroduce the Perceptual Riddle Synthesizer (PRS), an automated framework for\ngenerating riddles with fine-grained perceptual descriptions. PRS not only\ngenerates valuable training data for abstract graphics but also provides\nfine-grained perceptual description, crucially allowing for supervision over\nintermediate reasoning stages and thereby improving both training efficacy and\nmodel interpretability. Our extensive experimental results on VisuRiddles\nempirically validate that fine-grained visual perception is the principal\nbottleneck and our synthesis framework markedly enhances the performance of\ncontemporary MLLMs on these challenging tasks. Our code and dataset will be\nreleased at https://github.com/yh-hust/VisuRiddles", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\uff08AVR\uff09\u4efb\u52a1\u4e2d\u7684\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u51fa\u7528\u4e8e\u8bc4\u4f30\u7684VisuRiddles\u57fa\u51c6\u548c\u7528\u4e8e\u751f\u6210\u8bad\u7ec3\u6570\u636e\u7684Perceptual Riddle Synthesizer\uff08PRS\uff09\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "MLLMs\u5728\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u5bf9\u62bd\u8c61\u56fe\u5f62\u7684\u611f\u77e5\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u63d0\u5347\u8fd9\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86VisuRiddles\u57fa\u51c6\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\uff0c\u5e76\u5f00\u53d1PRS\u6846\u67b6\u751f\u6210\u5e26\u7ec6\u7c92\u5ea6\u611f\u77e5\u63cf\u8ff0\u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u662f\u5173\u952e\u74f6\u9888\uff0cPRS\u663e\u8457\u63d0\u5347\u4e86MLLMs\u5728AVR\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u548c\u5408\u6210\u8bad\u7ec3\u6570\u636e\u5bf9\u63d0\u5347MLLMs\u7684\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, \u62bd\u8c61\u89c6\u89c9\u63a8\u7406, \u89c6\u89c9\u611f\u77e5, \u8bad\u7ec3\u6570\u636e\u5408\u6210, \u57fa\u51c6\u8bc4\u4f30"}}
{"id": "2506.02095", "pdf": "https://arxiv.org/pdf/2506.02095", "abs": "https://arxiv.org/abs/2506.02095", "authors": ["Hyojin Bahng", "Caroline Chan", "Fredo Durand", "Phillip Isola"], "title": "Cycle Consistency as Reward: Learning Image-Text Alignment without Human Preferences", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Learning alignment between language and vision is a fundamental challenge,\nespecially as multimodal data becomes increasingly detailed and complex.\nExisting methods often rely on collecting human or AI preferences, which can be\ncostly and time-intensive. We propose an alternative approach that leverages\ncycle consistency as a supervisory signal. Given an image and generated text,\nwe map the text back to image space using a text-to-image model and compute the\nsimilarity between the original image and its reconstruction. Analogously, for\ntext-to-image generation, we measure the textual similarity between an input\ncaption and its reconstruction through the cycle. We use the cycle consistency\nscore to rank candidates and construct a preference dataset of 866K comparison\npairs. The reward model trained on our dataset outperforms state-of-the-art\nalignment metrics on detailed captioning, with superior inference-time\nscalability when used as a verifier for Best-of-N sampling. Furthermore,\nperforming DPO and Diffusion DPO using our dataset enhances performance across\na wide range of vision-language tasks and text-to-image generation. Our\ndataset, model, and code are at https://cyclereward.github.io", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5faa\u73af\u4e00\u81f4\u6027\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u548c\u56fe\u50cf\u4e4b\u95f4\u7684\u5faa\u73af\u91cd\u5efa\u6765\u751f\u6210\u504f\u597d\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00-\u89c6\u89c9\u5bf9\u9f50\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u4e14\u8017\u65f6\u7684\u4eba\u7c7b\u6216AI\u504f\u597d\u6536\u96c6\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5229\u7528\u6587\u672c-\u56fe\u50cf\u548c\u56fe\u50cf-\u6587\u672c\u7684\u5faa\u73af\u91cd\u5efa\u8ba1\u7b97\u76f8\u4f3c\u6027\uff0c\u6784\u5efa\u504f\u597d\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8be6\u7ec6\u5b57\u5e55\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u5bf9\u9f50\u6307\u6807\uff0c\u5e76\u5728\u591a\u79cd\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u548c\u6587\u672c-\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u5353\u8d8a\u3002", "conclusion": "\u5faa\u73af\u4e00\u81f4\u6027\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u8bed\u8a00-\u89c6\u89c9\u5bf9\u9f50\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u8bed\u8a00-\u89c6\u89c9\u5bf9\u9f50, \u5faa\u73af\u4e00\u81f4\u6027, \u504f\u597d\u6570\u636e\u96c6, \u5956\u52b1\u6a21\u578b, \u6587\u672c-\u56fe\u50cf\u751f\u6210"}}
{"id": "2506.02149", "pdf": "https://arxiv.org/pdf/2506.02149", "abs": "https://arxiv.org/abs/2506.02149", "authors": ["Wenjun Xia", "Chuang Niu", "Ge Wang"], "title": "Tomographic Foundation Model -- FORCE: Flow-Oriented Reconstruction Conditioning Engine", "categories": ["eess.IV", "cs.LG", "eess.SP"], "comment": null, "summary": "Computed tomography (CT) is a major medical imaging modality. Clinical CT\nscenarios, such as low-dose screening, sparse-view scanning, and metal\nimplants, often lead to severe noise and artifacts in reconstructed images,\nrequiring improved reconstruction techniques. The introduction of deep learning\nhas significantly advanced CT image reconstruction. However, obtaining paired\ntraining data remains rather challenging due to patient motion and other\nconstraints. Although deep learning methods can still perform well with\napproximately paired data, they inherently carry the risk of hallucination due\nto data inconsistencies and model instability. In this paper, we integrate the\ndata fidelity with the state-of-the-art generative AI model, referred to as the\nPoisson flow generative model (PFGM) with a generalized version PFGM++, and\npropose a novel CT framework: Flow-Oriented Reconstruction Conditioning Engine\n(FORCE). In our experiments, the proposed method shows superior performance in\nvarious CT imaging tasks, outperforming existing unsupervised reconstruction\napproaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aFORCE\u7684\u65b0CT\u91cd\u5efa\u6846\u67b6\uff0c\u5c06\u6570\u636e\u4fdd\u771f\u5ea6\u4e0e\u6700\u65b0\u751f\u6210\u6a21\u578bPFGM++\u7ed3\u5408\uff0c\u5728\u591a\u79cdCT\u6210\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e34\u5e8aCT\u573a\u666f\u4e2d\u4f4e\u5242\u91cf\u3001\u7a00\u758f\u626b\u63cf\u548c\u91d1\u5c5e\u690d\u5165\u7b49\u95ee\u9898\u5bfc\u81f4\u91cd\u5efa\u56fe\u50cf\u566a\u58f0\u548c\u4f2a\u5f71\u4e25\u91cd\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u914d\u5bf9\u6570\u636e\u83b7\u53d6\u56f0\u96be\uff0c\u5b58\u5728\u5e7b\u89c9\u98ce\u9669\u3002", "method": "\u6574\u5408\u6570\u636e\u4fdd\u771f\u5ea6\u4e0e\u751f\u6210\u6a21\u578bPFGM++\uff0c\u63d0\u51faFlow-Oriented Reconstruction Conditioning Engine\uff08FORCE\uff09\u6846\u67b6\u3002", "result": "\u5728\u591a\u79cdCT\u6210\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u91cd\u5efa\u65b9\u6cd5\u3002", "conclusion": "FORCE\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6570\u636e\u4fdd\u771f\u5ea6\u4e0e\u751f\u6210\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86CT\u91cd\u5efa\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002", "keywords": "CT\u91cd\u5efa\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u751f\u6210\u6a21\u578b\u3001PFGM++\u3001FORCE"}}
{"id": "2506.02548", "pdf": "https://arxiv.org/pdf/2506.02548", "abs": "https://arxiv.org/abs/2506.02548", "authors": ["Zhun Wang", "Tianneng Shi", "Jingxuan He", "Matthew Cai", "Jialin Zhang", "Dawn Song"], "title": "CyberGym: Evaluating AI Agents' Cybersecurity Capabilities with Real-World Vulnerabilities at Scale", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language model (LLM) agents are becoming increasingly skilled at\nhandling cybersecurity tasks autonomously. Thoroughly assessing their\ncybersecurity capabilities is critical and urgent, given the high stakes in\nthis domain. However, existing benchmarks fall short, often failing to capture\nreal-world scenarios or being limited in scope. To address this gap, we\nintroduce CyberGym, a large-scale and high-quality cybersecurity evaluation\nframework featuring 1,507 real-world vulnerabilities found and patched across\n188 large software projects. While it includes tasks of various settings,\nCyberGym primarily focuses on the generation of proof-of-concept (PoC) tests\nfor vulnerability reproduction, based on text descriptions and corresponding\nsource repositories. Solving this task is particularly challenging, as it\nrequires comprehensive reasoning across entire codebases to locate relevant\ncode fragments and produce effective PoCs that accurately trigger the target\nvulnerability starting from the program's entry point. Our evaluation across 4\nstate-of-the-art agent frameworks and 9 LLMs reveals that even the best\ncombination (OpenHands and Claude-3.7-Sonnet) achieves only a 11.9%\nreproduction success rate, mainly on simpler cases. Beyond reproducing\nhistorical vulnerabilities, we find that PoCs generated by LLM agents can\nreveal new vulnerabilities, identifying 15 zero-days affecting the latest\nversions of the software projects.", "AI": {"tldr": "CyberGym\u662f\u4e00\u79cd\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u7f51\u7edc\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u5728\u751f\u6210\u6f0f\u6d1e\u590d\u73b0\u7684\u6982\u5ff5\u8bc1\u660e\uff08PoC\uff09\u6d4b\u8bd5\u4e2d\u7684\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u4ee3\u7406\u6846\u67b6\u548cLLM\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u6210\u529f\u7387\u8f83\u4f4e\uff0c\u4ec5\u4e3a11.9%\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u8986\u76d6\u8303\u56f4\u548c\u771f\u5b9e\u6027\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30LLM\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86CyberGym\u6846\u67b6\uff0c\u5305\u542b1,507\u4e2a\u771f\u5b9e\u6f0f\u6d1e\u548c188\u4e2a\u5927\u578b\u8f6f\u4ef6\u9879\u76ee\uff0c\u4e13\u6ce8\u4e8e\u57fa\u4e8e\u6587\u672c\u63cf\u8ff0\u548c\u6e90\u4ee3\u7801\u751f\u6210PoC\u6d4b\u8bd5\u7684\u4efb\u52a1\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u6700\u4f73\u7ec4\u5408\uff08OpenHands\u548cClaude-3.7-Sonnet\uff09\u7684\u6f0f\u6d1e\u590d\u73b0\u6210\u529f\u7387\u4e3a11.9%\uff0c\u4e14\u4e3b\u8981\u96c6\u4e2d\u5728\u7b80\u5355\u6848\u4f8b\u4e0a\u3002\u6b64\u5916\uff0cLLM\u4ee3\u7406\u751f\u6210\u7684PoC\u8fd8\u53d1\u73b0\u4e8615\u4e2a\u5f71\u54cd\u6700\u65b0\u8f6f\u4ef6\u7248\u672c\u7684\u96f6\u65e5\u6f0f\u6d1e\u3002", "conclusion": "CyberGym\u4e3aLLM\u4ee3\u7406\u5728\u7f51\u7edc\u5b89\u5168\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\u53ca\u5176\u5728\u53d1\u73b0\u65b0\u6f0f\u6d1e\u65b9\u9762\u7684\u6f5c\u529b\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u7f51\u7edc\u5b89\u5168, \u6982\u5ff5\u8bc1\u660e, \u96f6\u65e5\u6f0f\u6d1e, \u8bc4\u4f30\u6846\u67b6"}}
{"id": "2506.02164", "pdf": "https://arxiv.org/pdf/2506.02164", "abs": "https://arxiv.org/abs/2506.02164", "authors": ["Yu", "Qian", "Wilson S. Geisler", "Xue-Xin Wei"], "title": "Quantifying task-relevant representational similarity using decision variable correlation", "categories": ["cs.CV", "cs.LG", "q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Previous studies have compared the brain and deep neural networks trained on\nimage classification. Intriguingly, while some suggest that their\nrepresentations are highly similar, others argued the opposite. Here, we\npropose a new approach to characterize the similarity of the decision\nstrategies of two observers (models or brains) using decision variable\ncorrelation (DVC). DVC quantifies the correlation between decoded decisions on\nindividual samples in a classification task and thus can capture task-relevant\ninformation rather than general representational alignment. We evaluate this\nmethod using monkey V4/IT recordings and models trained on image classification\ntasks.\n  We find that model--model similarity is comparable to monkey--monkey\nsimilarity, whereas model--monkey similarity is consistently lower and,\nsurprisingly, decreases with increasing ImageNet-1k performance. While\nadversarial training enhances robustness, it does not improve model--monkey\nsimilarity in task-relevant dimensions; however, it markedly increases\nmodel--model similarity. Similarly, pre-training on larger datasets does not\nimprove model--monkey similarity. These results suggest a fundamental\ndivergence between the task-relevant representations in monkey V4/IT and those\nlearned by models trained on image classification tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff08DVC\uff09\u6765\u91cf\u5316\u6a21\u578b\u4e0e\u7334\u5b50\u5927\u8111\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u51b3\u7b56\u7b56\u7565\u7684\u76f8\u4f3c\u6027\uff0c\u53d1\u73b0\u4e24\u8005\u4e4b\u95f4\u5b58\u5728\u672c\u8d28\u5dee\u5f02\u3002", "motivation": "\u63a2\u8ba8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e0e\u7334\u5b50\u5927\u8111\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u51b3\u7b56\u7b56\u7565\u662f\u5426\u76f8\u4f3c\u3002", "method": "\u4f7f\u7528\u51b3\u7b56\u53d8\u91cf\u76f8\u5173\uff08DVC\uff09\u65b9\u6cd5\u91cf\u5316\u4e0d\u540c\u89c2\u5bdf\u8005\uff08\u6a21\u578b\u6216\u5927\u8111\uff09\u7684\u51b3\u7b56\u7b56\u7565\u76f8\u4f3c\u6027\u3002", "result": "\u6a21\u578b\u95f4\u76f8\u4f3c\u6027\u4e0e\u7334\u5b50\u95f4\u76f8\u4f3c\u6027\u76f8\u5f53\uff0c\u4f46\u6a21\u578b\u4e0e\u7334\u5b50\u76f8\u4f3c\u6027\u8f83\u4f4e\uff0c\u4e14\u968f\u7740\u6a21\u578b\u6027\u80fd\u63d0\u5347\u53cd\u800c\u964d\u4f4e\u3002\u5bf9\u6297\u8bad\u7ec3\u548c\u66f4\u5927\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u672a\u80fd\u63d0\u9ad8\u6a21\u578b\u4e0e\u7334\u5b50\u76f8\u4f3c\u6027\u3002", "conclusion": "\u7334\u5b50\u5927\u8111\u4e0e\u6a21\u578b\u5728\u4efb\u52a1\u76f8\u5173\u8868\u5f81\u4e0a\u5b58\u5728\u6839\u672c\u5dee\u5f02\u3002", "keywords": "\u51b3\u7b56\u53d8\u91cf\u76f8\u5173\uff08DVC\uff09\u3001\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u3001\u7334\u5b50V4/IT\u3001\u56fe\u50cf\u5206\u7c7b\u3001\u5bf9\u6297\u8bad\u7ec3"}}
{"id": "2506.02550", "pdf": "https://arxiv.org/pdf/2506.02550", "abs": "https://arxiv.org/abs/2506.02550", "authors": ["Qiaohui Chu", "Haoyu Zhang", "Yisen Feng", "Meng Liu", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "title": "Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025", "categories": ["cs.CV", "cs.AI"], "comment": "The champion solution for the Ego4D Long-Term Action Anticipation\n  Challenge at the CVPR EgoVis Workshop 2025", "summary": "In this report, we present a novel three-stage framework developed for the\nEgo4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in\nfoundation models, our method consists of three stages: feature extraction,\naction recognition, and long-term action anticipation. First, visual features\nare extracted using a high-performance visual encoder. The features are then\nfed into a Transformer to predict verbs and nouns, with a verb-noun\nco-occurrence matrix incorporated to enhance recognition accuracy. Finally, the\npredicted verb-noun pairs are formatted as textual prompts and input into a\nfine-tuned large language model (LLM) to anticipate future action sequences.\nOur framework achieves first place in this challenge at CVPR 2025, establishing\na new state-of-the-art in long-term action prediction. Our code will be\nreleased at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8eEgo4D\u957f\u671f\u52a8\u4f5c\u9884\u6d4b\u4efb\u52a1\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u3001\u52a8\u4f5c\u8bc6\u522b\u548c\u957f\u671f\u52a8\u4f5c\u9884\u6d4b\uff0c\u5e76\u5728CVPR 2025\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u4e86\u7b2c\u4e00\u540d\u3002", "motivation": "\u53d7\u57fa\u7840\u6a21\u578b\u6700\u65b0\u8fdb\u5c55\u7684\u542f\u53d1\uff0c\u65e8\u5728\u63d0\u5347\u957f\u671f\u52a8\u4f5c\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u9ad8\u6027\u80fd\u89c6\u89c9\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff1b2\uff09\u5229\u7528Transformer\u9884\u6d4b\u52a8\u8bcd\u548c\u540d\u8bcd\uff0c\u5e76\u5f15\u5165\u52a8\u8bcd-\u540d\u8bcd\u5171\u73b0\u77e9\u9635\u63d0\u9ad8\u8bc6\u522b\u7cbe\u5ea6\uff1b3\uff09\u5c06\u9884\u6d4b\u7ed3\u679c\u4f5c\u4e3a\u6587\u672c\u63d0\u793a\u8f93\u5165\u5230\u5fae\u8c03\u8fc7\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\uff0c\u9884\u6d4b\u672a\u6765\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5728CVPR 2025\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u7b2c\u4e00\u540d\uff0c\u521b\u9020\u4e86\u957f\u671f\u52a8\u4f5c\u9884\u6d4b\u7684\u6700\u65b0\u6280\u672f\u6c34\u5e73\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u957f\u671f\u52a8\u4f5c\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5177\u6709\u521b\u65b0\u6027\u548c\u5b9e\u7528\u6027\u3002", "keywords": "\u957f\u671f\u52a8\u4f5c\u9884\u6d4b\uff0cEgo4D\uff0cTransformer\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0cCVPR"}}
{"id": "2506.02554", "pdf": "https://arxiv.org/pdf/2506.02554", "abs": "https://arxiv.org/abs/2506.02554", "authors": ["Timo Osterburg", "Franz Albers", "Christopher Diehl", "Rajesh Pushparaj", "Torsten Bertram"], "title": "HiLO: High-Level Object Fusion for Autonomous Driving using Transformers", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "6 pages, accepted at IEEE Intelligent Vehicles Symposium (IV) 2025", "summary": "The fusion of sensor data is essential for a robust perception of the\nenvironment in autonomous driving. Learning-based fusion approaches mainly use\nfeature-level fusion to achieve high performance, but their complexity and\nhardware requirements limit their applicability in near-production vehicles.\nHigh-level fusion methods offer robustness with lower computational\nrequirements. Traditional methods, such as the Kalman filter, dominate this\narea. This paper modifies the Adapted Kalman Filter (AKF) and proposes a novel\ntransformer-based high-level object fusion method called HiLO. Experimental\nresults demonstrate improvements of $25.9$ percentage points in $\\textrm{F}_1$\nscore and $6.1$ percentage points in mean IoU. Evaluation on a new large-scale\nreal-world dataset demonstrates the effectiveness of the proposed approaches.\nTheir generalizability is further validated by cross-domain evaluation between\nurban and highway scenarios. Code, data, and models are available at\nhttps://github.com/rst-tu-dortmund/HiLO .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdb\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u548cTransformer\u7684\u9ad8\u7ea7\u522b\u76ee\u6807\u878d\u5408\u65b9\u6cd5HiLO\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u73af\u5883\u611f\u77e5\u7684\u6027\u80fd\uff0c\u5e76\u5728\u8de8\u57df\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u57fa\u4e8e\u5b66\u4e60\u7684\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\u65b9\u6cd5\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u786c\u4ef6\u9700\u6c42\u4e0a\u7684\u9650\u5236\uff0c\u540c\u65f6\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u73af\u5883\u611f\u77e5\u7684\u9c81\u68d2\u6027\u3002", "method": "\u6539\u8fdb\u4e86Adapted Kalman Filter (AKF)\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eTransformer\u7684\u9ad8\u7ea7\u522b\u76ee\u6807\u878d\u5408\u65b9\u6cd5HiLO\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cF1\u5206\u6570\u63d0\u5347\u4e8625.9\u4e2a\u767e\u5206\u70b9\uff0c\u5e73\u5747IoU\u63d0\u5347\u4e866.1\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "HiLO\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u8de8\u57df\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "keywords": "\u81ea\u52a8\u9a7e\u9a76, \u4f20\u611f\u5668\u6570\u636e\u878d\u5408, \u5361\u5c14\u66fc\u6ee4\u6ce2\u5668, Transformer, \u9ad8\u7ea7\u522b\u878d\u5408"}}
{"id": "2506.02221", "pdf": "https://arxiv.org/pdf/2506.02221", "abs": "https://arxiv.org/abs/2506.02221", "authors": ["Johannes Schusterbauer", "Ming Gui", "Frank Fundel", "Bj\u00f6rn Ommer"], "title": "Diff2Flow: Training Flow Matching Models via Diffusion Model Alignment", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by CVPR 2025", "summary": "Diffusion models have revolutionized generative tasks through high-fidelity\noutputs, yet flow matching (FM) offers faster inference and empirical\nperformance gains. However, current foundation FM models are computationally\nprohibitive for finetuning, while diffusion models like Stable Diffusion\nbenefit from efficient architectures and ecosystem support. This work addresses\nthe critical challenge of efficiently transferring knowledge from pre-trained\ndiffusion models to flow matching. We propose Diff2Flow, a novel framework that\nsystematically bridges diffusion and FM paradigms by rescaling timesteps,\naligning interpolants, and deriving FM-compatible velocity fields from\ndiffusion predictions. This alignment enables direct and efficient FM\nfinetuning of diffusion priors with no extra computation overhead. Our\nexperiments demonstrate that Diff2Flow outperforms na\\\"ive FM and diffusion\nfinetuning particularly under parameter-efficient constraints, while achieving\nsuperior or competitive performance across diverse downstream tasks compared to\nstate-of-the-art methods. We will release our code at\nhttps://github.com/CompVis/diff2flow.", "AI": {"tldr": "Diff2Flow\u6846\u67b6\u901a\u8fc7\u91cd\u65b0\u7f29\u653e\u65f6\u95f4\u6b65\u957f\u3001\u5bf9\u9f50\u63d2\u503c\u548c\u4ece\u6269\u6563\u9884\u6d4b\u4e2d\u5bfc\u51faFM\u517c\u5bb9\u901f\u5ea6\u573a\uff0c\u9ad8\u6548\u5730\u5c06\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u77e5\u8bc6\u8fc1\u79fb\u5230\u6d41\u5339\u914d\u6a21\u578b\u4e2d\u3002", "motivation": "\u5f53\u524d\u57fa\u7840\u6d41\u5339\u914d\u6a21\u578b\u5728\u5fae\u8c03\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u6269\u6563\u6a21\u578b\u5982Stable Diffusion\u5219\u53d7\u76ca\u4e8e\u9ad8\u6548\u67b6\u6784\u548c\u751f\u6001\u7cfb\u7edf\u652f\u6301\u3002Diff2Flow\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u9ad8\u6548\u5730\u4ece\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u8fc1\u79fb\u77e5\u8bc6\u5230\u6d41\u5339\u914d\u7684\u5173\u952e\u6311\u6218\u3002", "method": "Diff2Flow\u901a\u8fc7\u4e09\u4e2a\u6b65\u9aa4\u5b9e\u73b0\u77e5\u8bc6\u8fc1\u79fb\uff1a1) \u91cd\u65b0\u7f29\u653e\u65f6\u95f4\u6b65\u957f\uff0c2) \u5bf9\u9f50\u63d2\u503c\uff0c3) \u4ece\u6269\u6563\u9884\u6d4b\u4e2d\u5bfc\u51fa\u6d41\u5339\u914d\u517c\u5bb9\u7684\u901f\u5ea6\u573a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDiff2Flow\u5728\u53c2\u6570\u9ad8\u6548\u7684\u7ea6\u675f\u6761\u4ef6\u4e0b\u4f18\u4e8e\u7b80\u5355\u6d41\u5339\u914d\u548c\u6269\u6563\u5fae\u8c03\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u591a\u6837\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "Diff2Flow\u4e3a\u6269\u6563\u6a21\u578b\u4e0e\u6d41\u5339\u914d\u6a21\u578b\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u76f4\u63a5\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u663e\u8457\u6027\u80fd\u4f18\u52bf\u3002", "keywords": "\u6269\u6563\u6a21\u578b, \u6d41\u5339\u914d, \u77e5\u8bc6\u8fc1\u79fb, Diff2Flow, \u53c2\u6570\u9ad8\u6548\u5fae\u8c03"}}
{"id": "2506.02241", "pdf": "https://arxiv.org/pdf/2506.02241", "abs": "https://arxiv.org/abs/2506.02241", "authors": ["Michael S. Ackermann", "Ion Victor Gosea", "Serkan Gugercin", "Steffen W. R. Werner"], "title": "Second-order AAA algorithms for structured data-driven modeling", "categories": ["math.NA", "cs.LG", "cs.NA", "cs.SY", "eess.SY", "math.DS", "math.OC", "41A20, 65D15, 93B15, 93C05, 93C80"], "comment": "37 pages, 6 figures, 3 tables", "summary": "The data-driven modeling of dynamical systems has become an essential tool\nfor the construction of accurate computational models from real-world data. In\nthis process, the inherent differential structures underlying the considered\nphysical phenomena are often neglected making the reinterpretation of the\nlearned models in a physically meaningful sense very challenging. In this work,\nwe present three data-driven modeling approaches for the construction of\ndynamical systems with second-order differential structure directly from\nfrequency domain data. Based on the second-order structured barycentric form,\nwe extend the well-known Adaptive Antoulas-Anderson algorithm to the case of\nsecond-order systems. Depending on the available computational resources, we\npropose variations of the proposed method that prioritize either higher\ncomputation speed or greater modeling accuracy, and we present a theoretical\nanalysis for the expected accuracy and performance of the proposed methods.\nThree numerical examples demonstrate the effectiveness of our new structured\napproaches in comparison to classical unstructured data-driven modeling.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e09\u79cd\u76f4\u63a5\u4ece\u9891\u57df\u6570\u636e\u6784\u5efa\u5177\u6709\u4e8c\u9636\u5fae\u5206\u7ed3\u6784\u7684\u52a8\u6001\u7cfb\u7edf\u7684\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u65b9\u6cd5\uff0c\u5e76\u6269\u5c55\u4e86Adaptive Antoulas-Anderson\u7b97\u6cd5\u3002\u65b9\u6cd5\u5728\u8ba1\u7b97\u901f\u5ea6\u548c\u5efa\u6a21\u7cbe\u5ea6\u4e4b\u95f4\u63d0\u4f9b\u6743\u8861\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u65b9\u6cd5\u5e38\u5ffd\u7565\u7269\u7406\u73b0\u8c61\u7684\u5fae\u5206\u7ed3\u6784\uff0c\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u7269\u7406\u89e3\u91ca\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u4e8c\u9636\u7ed3\u6784\u5316\u7684\u91cd\u5fc3\u5f62\u5f0f\uff0c\u6269\u5c55\u4e86Adaptive Antoulas-Anderson\u7b97\u6cd5\uff0c\u63d0\u51fa\u4e86\u517c\u987e\u8ba1\u7b97\u901f\u5ea6\u6216\u5efa\u6a21\u7cbe\u5ea6\u7684\u53d8\u4f53\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u4e09\u4e2a\u6570\u503c\u4f8b\u5b50\u9a8c\u8bc1\u4e86\u65b0\u65b9\u6cd5\u5728\u5efa\u6a21\u7cbe\u5ea6\u548c\u6027\u80fd\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ed3\u6784\u5316\u65b9\u6cd5\u6bd4\u4f20\u7edf\u975e\u7ed3\u6784\u5316\u5efa\u6a21\u66f4\u6709\u6548\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u52a8\u6001\u7cfb\u7edf\u7684\u7269\u7406\u7279\u6027\u3002", "keywords": "\u6570\u636e\u9a71\u52a8\u5efa\u6a21, \u52a8\u6001\u7cfb\u7edf, \u4e8c\u9636\u5fae\u5206\u7ed3\u6784, \u9891\u57df\u6570\u636e, Adaptive Antoulas-Anderson\u7b97\u6cd5"}}
{"id": "2506.02254", "pdf": "https://arxiv.org/pdf/2506.02254", "abs": "https://arxiv.org/abs/2506.02254", "authors": ["Dimitris G Giovanis", "Nikolaos Evangelou", "Ioannis G Kevrekidis", "Roger G Ghanem"], "title": "Enabling Probabilistic Learning on Manifolds through Double Diffusion Maps", "categories": ["stat.ML", "cs.LG", "math.PR"], "comment": null, "summary": "We present a generative learning framework for probabilistic sampling based\non an extension of the Probabilistic Learning on Manifolds (PLoM) approach,\nwhich is designed to generate statistically consistent realizations of a random\nvector in a finite-dimensional Euclidean space, informed by a limited (yet\nrepresentative) set of observations. In its original form, PLoM constructs a\nreduced-order probabilistic model by combining three main components: (a)\nkernel density estimation to approximate the underlying probability measure,\n(b) Diffusion Maps to uncover the intrinsic low-dimensional manifold structure,\nand (c) a reduced-order Ito Stochastic Differential Equation (ISDE) to sample\nfrom the learned distribution. A key challenge arises, however, when the number\nof available data points N is small and the dimensionality of the diffusion-map\nbasis approaches N, resulting in overfitting and loss of generalization. To\novercome this limitation, we propose an enabling extension that implements a\nsynthesis of Double Diffusion Maps -- a technique capable of capturing\nmultiscale geometric features of the data -- with Geometric Harmonics (GH), a\nnonparametric reconstruction method that allows smooth nonlinear interpolation\nin high-dimensional ambient spaces. This approach enables us to solve a\nfull-order ISDE directly in the latent space, preserving the full dynamical\ncomplexity of the system, while leveraging its reduced geometric\nrepresentation. The effectiveness and robustness of the proposed method are\nillustrated through two numerical studies: one based on data generated from\ntwo-dimensional Hermite polynomial functions and another based on high-fidelity\nsimulations of a detonation wave in a reactive flow.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u5c55PLoM\u7684\u751f\u6210\u5b66\u4e60\u6846\u67b6\uff0c\u9488\u5bf9\u5c0f\u6837\u672c\u6570\u636e\uff0c\u7ed3\u5408\u53cc\u6269\u6563\u6620\u5c04\u548c\u51e0\u4f55\u8c10\u6ce2\u65b9\u6cd5\uff0c\u514b\u670d\u8fc7\u62df\u5408\u5e76\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3PLoM\u65b9\u6cd5\u5728\u5c0f\u6837\u672c\u6570\u636e\u4e0b\u56e0\u7ef4\u5ea6\u63a5\u8fd1\u6837\u672c\u6570\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u548c\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u53cc\u6269\u6563\u6620\u5c04\uff08\u6355\u6349\u591a\u5c3a\u5ea6\u51e0\u4f55\u7279\u5f81\uff09\u548c\u51e0\u4f55\u8c10\u6ce2\uff08\u9ad8\u7ef4\u975e\u7ebf\u6027\u63d2\u503c\uff09\uff0c\u76f4\u63a5\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6c42\u89e3\u5b8c\u6574\u9636ISDE\u3002", "result": "\u901a\u8fc7\u4e8c\u7ef4Hermite\u591a\u9879\u5f0f\u51fd\u6570\u548c\u7206\u8f70\u6ce2\u9ad8\u4fdd\u771f\u6a21\u62df\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5c0f\u6837\u672c\u4e0b\u663e\u8457\u63d0\u5347\u751f\u6210\u6837\u672c\u7684\u7edf\u8ba1\u4e00\u81f4\u6027\u3002", "keywords": "\u751f\u6210\u5b66\u4e60, \u6982\u7387\u91c7\u6837, \u53cc\u6269\u6563\u6620\u5c04, \u51e0\u4f55\u8c10\u6ce2, \u8fc7\u62df\u5408"}}
{"id": "2506.02257", "pdf": "https://arxiv.org/pdf/2506.02257", "abs": "https://arxiv.org/abs/2506.02257", "authors": ["Ruiting Liang", "Jake A. Soloff", "Rina Foygel Barber", "Rebecca Willett"], "title": "Assumption-free stability for ranking problems", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "comment": null, "summary": "In this work, we consider ranking problems among a finite set of candidates:\nfor instance, selecting the top-$k$ items among a larger list of candidates or\nobtaining the full ranking of all items in the set. These problems are often\nunstable, in the sense that estimating a ranking from noisy data can exhibit\nhigh sensitivity to small perturbations. Concretely, if we use data to provide\na score for each item (say, by aggregating preference data over a sample of\nusers), then for two items with similar scores, small fluctuations in the data\ncan alter the relative ranking of those items. Many existing theoretical\nresults for ranking problems assume a separation condition to avoid this\nchallenge, but real-world data often contains items whose scores are\napproximately tied, limiting the applicability of existing theory. To address\nthis gap, we develop a new algorithmic stability framework for ranking\nproblems, and propose two novel ranking operators for achieving stable ranking:\nthe \\emph{inflated top-$k$} for the top-$k$ selection problem and the\n\\emph{inflated full ranking} for ranking the full list. To enable stability,\neach method allows for expressing some uncertainty in the output. For both of\nthese two problems, our proposed methods provide guaranteed stability, with no\nassumptions on data distributions and no dependence on the total number of\ncandidates to be ranked. Experiments on real-world data confirm that the\nproposed methods offer stability without compromising the informativeness of\nthe output.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\u7a33\u5b9a\u6027\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6392\u5e8f\u95ee\u9898\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u5019\u9009\u8005\u5206\u6570\u63a5\u8fd1\u65f6\u3002\u901a\u8fc7\u4e24\u79cd\u65b0\u7684\u6392\u5e8f\u64cd\u4f5c\u7b26\uff08\u81a8\u80c0 top-k \u548c\u81a8\u80c0\u5168\u6392\u5e8f\uff09\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u6570\u636e\u5206\u5e03\u5047\u8bbe\u7684\u7a33\u5b9a\u6392\u5e8f\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6570\u636e\u5e38\u51fa\u73b0\u5019\u9009\u8005\u5206\u6570\u63a5\u8fd1\u7684\u60c5\u51b5\uff0c\u5bfc\u81f4\u6392\u5e8f\u95ee\u9898\u5bf9\u5c0f\u6270\u52a8\u9ad8\u5ea6\u654f\u611f\u3002\u73b0\u6709\u7406\u8bba\u4f9d\u8d56\u5206\u79bb\u6761\u4ef6\uff0c\u4f46\u5b9e\u9645\u6570\u636e\u5f80\u5f80\u4e0d\u7b26\u5408\u8fd9\u4e00\u5047\u8bbe\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u7a33\u5b9a\u6027\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u6392\u5e8f\u64cd\u4f5c\u7b26\uff1a\u81a8\u80c0 top-k\uff08\u7528\u4e8e\u9009\u62e9\u524d k \u4e2a\u9879\uff09\u548c\u81a8\u80c0\u5168\u6392\u5e8f\uff08\u7528\u4e8e\u5b8c\u5168\u6392\u5e8f\uff09\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u5141\u8bb8\u8f93\u51fa\u4e2d\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u6765\u589e\u5f3a\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u727a\u7272\u4fe1\u606f\u91cf\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u7a33\u5b9a\u6027\uff0c\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u5019\u9009\u8005\u6570\u91cf\u6216\u6570\u636e\u5206\u5e03\u5047\u8bbe\u3002", "conclusion": "\u65b0\u6846\u67b6\u4e3a\u6392\u5e8f\u95ee\u9898\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7406\u8bba\u5728\u5206\u6570\u63a5\u8fd1\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002", "keywords": "\u6392\u5e8f\u95ee\u9898, \u7b97\u6cd5\u7a33\u5b9a\u6027, \u81a8\u80c0 top-k, \u81a8\u80c0\u5168\u6392\u5e8f, \u6570\u636e\u6270\u52a8"}}
{"id": "2506.02260", "pdf": "https://arxiv.org/pdf/2506.02260", "abs": "https://arxiv.org/abs/2506.02260", "authors": ["Howon Ryu", "Yuliang Chen", "Yacun Wang", "Andrea Z. LaCroix", "Chongzhi Di", "Loki Natarajan", "Yu Wang", "Jingjing Zou"], "title": "MoCA: Multi-modal Cross-masked Autoencoder for Digital Health Measurements", "categories": ["stat.ML", "cs.LG", "stat.AP"], "comment": null, "summary": "The growing prevalence of digital health technologies has led to the\ngeneration of complex multi-modal data, such as physical activity measurements\nsimultaneously collected from various sensors of mobile and wearable devices.\nThese data hold immense potential for advancing health studies, but current\nmethods predominantly rely on supervised learning, requiring extensive labeled\ndatasets that are often expensive or impractical to obtain, especially in\nclinical studies. To address this limitation, we propose a self-supervised\nlearning framework called Multi-modal Cross-masked Autoencoder (MoCA) that\nleverages cross-modality masking and the Transformer autoencoder architecture\nto utilize both temporal correlations within modalities and cross-modal\ncorrelations between data streams. We also provide theoretical guarantees to\nsupport the effectiveness of the cross-modality masking scheme in MoCA.\nComprehensive experiments and ablation studies demonstrate that our method\noutperforms existing approaches in both reconstruction and downstream tasks. We\nrelease open-source code for data processing, pre-training, and downstream\ntasks in the supplementary materials. This work highlights the transformative\npotential of self-supervised learning in digital health and multi-modal data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u591a\u6a21\u6001\u4ea4\u53c9\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\uff08MoCA\uff09\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u6570\u5b57\u5065\u5eb7\u9886\u57df\u4e2d\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u65e0\u9700\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u5f53\u524d\u6570\u5b57\u5065\u5eb7\u6280\u672f\u4ea7\u751f\u7684\u591a\u6a21\u6001\u6570\u636e\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u4e3b\u6d41\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u4e14\u4e0d\u5207\u5b9e\u9645\uff0c\u5c24\u5176\u662f\u5728\u4e34\u5e8a\u7814\u7a76\u4e2d\u3002", "method": "MoCA\u5229\u7528\u4ea4\u53c9\u6a21\u6001\u63a9\u7801\u548cTransformer\u81ea\u52a8\u7f16\u7801\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u65f6\u95f4\u76f8\u5173\u6027\u548c\u8de8\u6a21\u6001\u76f8\u5173\u6027\uff0c\u65e0\u9700\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMoCA\u5728\u91cd\u5efa\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u6570\u5b57\u5065\u5eb7\u548c\u591a\u6a21\u6001\u6570\u636e\u4e2d\u5177\u6709\u53d8\u9769\u6f5c\u529b\u3002", "keywords": "\u6570\u5b57\u5065\u5eb7,\u591a\u6a21\u6001\u6570\u636e,\u81ea\u76d1\u7763\u5b66\u4e60,Transformer,MoCA"}}
{"id": "2506.02606", "pdf": "https://arxiv.org/pdf/2506.02606", "abs": "https://arxiv.org/abs/2506.02606", "authors": ["Baoyang Chen", "Xian Xu", "Huamin Qu"], "title": "Multi Layered Autonomy and AI Ecologies in Robotic Art Installations", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Symbiosis of Agents is a large-scale installation by Baoyang Chen that embeds\nAI-driven robots in an immersive, mirror-lined arena, probing the tension\nbetween machine agency and artistic authorship. Drawing on early cybernetics,\nrule-based conceptual art, and seminal robotic works, it orchestrates fluid\nexchanges among robotic arms, quadruped machines, their environment, and the\npublic. A three tier faith system pilots the ecology: micro-level adaptive\ntactics, meso-level narrative drives, and a macro-level prime directive. This\nhierarchy lets behaviors evolve organically in response to environmental cues\nand even a viewer's breath, turning spectators into co-authors of the unfolding\ndrama.Framed by a speculative terraforming scenario that recalls the historical\nexploitation of marginalized labor, the piece asks who bears responsibility in\nAI-mediated futures. Choreographed motion, AI-generated scripts, reactive\nlighting, and drifting fog cast the robots as collaborators rather than tools,\nforging a living, emergent artwork. Exhibited internationally, Symbiosis of\nAgents shows how cybernetic feedback, robotic experimentation, and conceptual\nrule-making can converge to redefine agency, authorship, and ethics in\ncontemporary art.", "AI": {"tldr": "\u300aSymbiosis of Agents\u300b\u662f\u4e00\u4e2a\u63a2\u8ba8\u673a\u5668\u80fd\u52a8\u6027\u4e0e\u827a\u672f\u521b\u4f5c\u4e4b\u95f4\u5f20\u529b\u7684\u5927\u578b\u88c5\u7f6e\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u7684\u4fe1\u4ef0\u7cfb\u7edf\u5b9e\u73b0\u673a\u5668\u4e0e\u89c2\u4f17\u7684\u4e92\u52a8\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u80fd\u52a8\u6027\u4e0e\u827a\u672f\u521b\u4f5c\u7684\u8fb9\u754c\uff0c\u4ee5\u53caAI\u672a\u6765\u4e2d\u8d23\u4efb\u7684\u5f52\u5c5e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u4fe1\u4ef0\u7cfb\u7edf\uff08\u5fae\u89c2\u9002\u5e94\u884c\u4e3a\u3001\u4e2d\u5c42\u53d9\u4e8b\u9a71\u52a8\u3001\u5b8f\u89c2\u9996\u8981\u6307\u4ee4\uff09\u548c\u6c89\u6d78\u5f0f\u73af\u5883\u8bbe\u8ba1\uff0c\u8ba9\u673a\u5668\u4e0e\u89c2\u4f17\u4e92\u52a8\u3002", "result": "\u4f5c\u54c1\u5728\u56fd\u9645\u4e0a\u5c55\u51fa\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u53cd\u9988\u3001\u5b9e\u9a8c\u548c\u89c4\u5219\u91cd\u65b0\u5b9a\u4e49\u5f53\u4ee3\u827a\u672f\u4e2d\u7684\u80fd\u52a8\u6027\u3001\u521b\u4f5c\u6743\u548c\u4f26\u7406\u3002", "conclusion": "\u8be5\u4f5c\u54c1\u901a\u8fc7\u673a\u5668\u4e0e\u89c2\u4f17\u7684\u5171\u751f\u5173\u7cfb\uff0c\u91cd\u65b0\u63a2\u8ba8\u4e86\u827a\u672f\u521b\u4f5c\u4e2d\u7684\u80fd\u52a8\u6027\u4e0e\u4f26\u7406\u95ee\u9898\u3002", "keywords": "\u673a\u5668\u80fd\u52a8\u6027,\u827a\u672f\u521b\u4f5c,AI\u4f26\u7406,\u6c89\u6d78\u5f0f\u88c5\u7f6e,\u53cd\u9988\u7cfb\u7edf"}}
{"id": "2506.02261", "pdf": "https://arxiv.org/pdf/2506.02261", "abs": "https://arxiv.org/abs/2506.02261", "authors": ["Zhongyu Ouyang", "Qianlong Wen", "Chunhui Zhang", "Yanfang Ye", "Soroush Vosoughi"], "title": "Towards Human-like Preference Profiling in Sequential Recommendation", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "Sequential recommendation systems aspire to profile users by interpreting\ntheir interaction histories, echoing how humans make decisions by weighing\nexperience, relative preference strength, and situational relevance. Yet,\nexisting large language model (LLM)-based recommenders often fall short of\nmimicking the flexible, context-aware decision strategies humans exhibit,\nneglecting the structured, dynamic, and context-aware mechanisms fundamental to\nhuman behaviors. To bridge this gap, we propose RecPO, a preference\noptimization framework that models structured feedback and contextual delay to\nemulate human-like prioritization in sequential recommendation RecPO exploits\nadaptive reward margins based on inferred preference hierarchies and temporal\nsignals, enabling the model to favor immediately relevant items and to\ndistinguish between varying degrees of preference and aversion. Extensive\nexperiments across five real-world datasets demonstrate that RecPO not only\nyields performance gains over state-of-the-art baselines, but also mirrors key\ncharacteristics of human decision-making: favoring timely satisfaction,\nmaintaining coherent preferences, and exercising discernment under shifting\ncontexts.", "AI": {"tldr": "RecPO\u662f\u4e00\u4e2a\u504f\u597d\u4f18\u5316\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u51b3\u7b56\u4e2d\u7684\u7ed3\u6784\u5316\u53cd\u9988\u548c\u4e0a\u4e0b\u6587\u5ef6\u8fdf\uff0c\u63d0\u5347\u987a\u5e8f\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u8350\u7cfb\u7edf\u672a\u80fd\u5145\u5206\u6a21\u62df\u4eba\u7c7b\u7075\u6d3b\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u51b3\u7b56\u7b56\u7565\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u884c\u4e3a\u7684\u7ed3\u6784\u5316\u3001\u52a8\u6001\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u673a\u5236\u3002", "method": "\u63d0\u51faRecPO\u6846\u67b6\uff0c\u5229\u7528\u81ea\u9002\u5e94\u5956\u52b1\u8fb9\u9645\uff0c\u57fa\u4e8e\u63a8\u65ad\u7684\u504f\u597d\u5c42\u6b21\u548c\u65f6\u95f4\u4fe1\u53f7\uff0c\u4f18\u5148\u5904\u7406\u76f8\u5173\u9879\u76ee\u5e76\u533a\u5206\u4e0d\u540c\u7a0b\u5ea6\u7684\u504f\u597d\u548c\u538c\u6076\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u4e2d\uff0cRecPO\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u8fd8\u5c55\u73b0\u4e86\u4eba\u7c7b\u51b3\u7b56\u7684\u5173\u952e\u7279\u5f81\uff08\u5982\u53ca\u65f6\u6ee1\u8db3\u3001\u504f\u597d\u4e00\u81f4\u6027\u548c\u4e0a\u4e0b\u6587\u8fa8\u522b\u80fd\u529b\uff09\u3002", "conclusion": "RecPO\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u51b3\u7b56\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u987a\u5e8f\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u66f4\u63a5\u8fd1\u4eba\u7c7b\u884c\u4e3a\u7684\u63a8\u8350\u7b56\u7565\u3002", "keywords": "\u987a\u5e8f\u63a8\u8350\u7cfb\u7edf\u3001\u504f\u597d\u4f18\u5316\u3001\u4eba\u7c7b\u51b3\u7b56\u6a21\u62df\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u81ea\u9002\u5e94\u5956\u52b1"}}
{"id": "2506.02610", "pdf": "https://arxiv.org/pdf/2506.02610", "abs": "https://arxiv.org/abs/2506.02610", "authors": ["Zhaoyang Li", "Jie Wang", "XiaoXiao Li", "Wangjie Li", "Longjie Luo", "Lin Li", "Qingyang Hong"], "title": "Speaker Diarization with Overlapping Community Detection Using Graph Attention Networks and Label Propagation Algorithm", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "In speaker diarization, traditional clustering-based methods remain widely\nused in real-world applications. However, these methods struggle with the\ncomplex distribution of speaker embeddings and overlapping speech segments. To\naddress these limitations, we propose an Overlapping Community Detection method\nbased on Graph Attention networks and the Label Propagation Algorithm\n(OCDGALP). The proposed framework comprises two key components: (1) a graph\nattention network that refines speaker embeddings and node connections by\naggregating information from neighboring nodes, and (2) a label propagation\nalgorithm that assigns multiple community labels to each node, enabling\nsimultaneous clustering and overlapping community detection. Experimental\nresults show that the proposed method significantly reduces the Diarization\nError Rate (DER), achieving a state-of-the-art 15.94% DER on the DIHARD-III\ndataset without oracle Voice Activity Detection (VAD), and an impressive 11.07%\nwith oracle VAD.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u6807\u7b7e\u4f20\u64ad\u7b97\u6cd5\u7684\u91cd\u53e0\u793e\u533a\u68c0\u6d4b\u65b9\u6cd5\uff08OCDGALP\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u4efb\u52a1\u4e2d\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u8bf4\u8bdd\u4eba\u5d4c\u5165\u5206\u5e03\u548c\u91cd\u53e0\u8bed\u97f3\u6bb5\u7684\u5c40\u9650\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u9519\u8bef\u7387\u3002", "motivation": "\u4f20\u7edf\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7684\u805a\u7c7b\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u8bf4\u8bdd\u4eba\u5d4c\u5165\u5206\u5e03\u548c\u91cd\u53e0\u8bed\u97f3\u6bb5\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u6807\u7b7e\u4f20\u64ad\u7b97\u6cd5\uff0c\u901a\u8fc7\u805a\u5408\u90bb\u5c45\u8282\u70b9\u4fe1\u606f\u4f18\u5316\u8bf4\u8bdd\u4eba\u5d4c\u5165\u548c\u8282\u70b9\u8fde\u63a5\uff0c\u5e76\u652f\u6301\u591a\u793e\u533a\u6807\u7b7e\u5206\u914d\u7684\u805a\u7c7b\u68c0\u6d4b\u3002", "result": "\u5728DIHARD-III\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8615.94%\u7684\u9519\u8bef\u7387\uff08\u65e0VAD\uff09\u548c11.07%\u7684\u9519\u8bef\u7387\uff08\u6709VAD\uff09\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "OCDGALP\u65b9\u6cd5\u5728\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002", "keywords": "\u8bf4\u8bdd\u4eba\u65e5\u5fd7, \u56fe\u6ce8\u610f\u529b\u7f51\u7edc, \u6807\u7b7e\u4f20\u64ad\u7b97\u6cd5, \u91cd\u53e0\u793e\u533a\u68c0\u6d4b, DIHARD-III"}}
{"id": "2506.02284", "pdf": "https://arxiv.org/pdf/2506.02284", "abs": "https://arxiv.org/abs/2506.02284", "authors": ["Yifeng Teng", "Yifan Wang"], "title": "Learning Optimal Posted Prices for a Unit-Demand Buyer", "categories": ["cs.GT", "cs.DS", "cs.LG"], "comment": null, "summary": "We study the problem of learning the optimal item pricing for a unit-demand\nbuyer with independent item values, and the learner has query access to the\nbuyer's value distributions. We consider two common query models in the\nliterature: the sample access model where the learner can obtain a sample of\neach item value, and the pricing query model where the learner can set a price\nfor an item and obtain a binary signal on whether the sampled value of the item\nis greater than our proposed price. In this work, we give nearly tight sample\ncomplexity and pricing query complexity of the unit-demand pricing problem.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u72ec\u7acb\u9879\u76ee\u4ef7\u503c\u4e0b\u5b66\u4e60\u6700\u4f18\u5b9a\u4ef7\u7684\u95ee\u9898\uff0c\u5bf9\u6bd4\u4e86\u4e24\u79cd\u67e5\u8be2\u6a21\u578b\u7684\u6837\u672c\u590d\u6742\u5ea6\u548c\u5b9a\u4ef7\u67e5\u8be2\u590d\u6742\u5ea6\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u5355\u4f4d\u9700\u6c42\u4e70\u5bb6\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4e0d\u540c\u7684\u67e5\u8be2\u6a21\u578b\uff08\u6837\u672c\u8bbf\u95ee\u548c\u5b9a\u4ef7\u67e5\u8be2\uff09\u4f18\u5316\u9879\u76ee\u7684\u5b9a\u4ef7\u7b56\u7565\u3002", "method": "\u6bd4\u8f83\u4e86\u4e24\u79cd\u67e5\u8be2\u6a21\u578b\uff1a\u6837\u672c\u8bbf\u95ee\u6a21\u578b\uff08\u83b7\u53d6\u6837\u672c\u503c\uff09\u548c\u5b9a\u4ef7\u67e5\u8be2\u6a21\u578b\uff08\u8bbe\u7f6e\u4ef7\u683c\u5e76\u83b7\u53d6\u4e8c\u5143\u4fe1\u53f7\uff09\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u590d\u6742\u5ea6\u3002", "result": "\u5f97\u51fa\u4e86\u5355\u4f4d\u9700\u6c42\u5b9a\u4ef7\u95ee\u9898\u7684\u6837\u672c\u590d\u6742\u5ea6\u548c\u5b9a\u4ef7\u67e5\u8be2\u590d\u6742\u5ea6\u7684\u8fd1\u4f3c\u7d27\u5bc6\u754c\u9650\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4f18\u5316\u5b9a\u4ef7\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u67e5\u8be2\u6a21\u578b\u4e0b\u7684\u590d\u6742\u5ea6\u5206\u6790\u3002", "keywords": "\u5355\u4f4d\u9700\u6c42\u5b9a\u4ef7, \u67e5\u8be2\u6a21\u578b, \u6837\u672c\u590d\u6742\u5ea6, \u5b9a\u4ef7\u67e5\u8be2\u590d\u6742\u5ea6"}}
{"id": "2506.02614", "pdf": "https://arxiv.org/pdf/2506.02614", "abs": "https://arxiv.org/abs/2506.02614", "authors": ["Guohang Zhuang", "Weixi Song", "Jinyang Huang", "Chenwei Yang", "Yan Lu"], "title": "High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of space exploration, space debris has attracted\nmore attention due to its potential extreme threat, leading to the need for\nreal-time and accurate debris tracking. However, existing methods are mainly\nbased on traditional signal processing, which cannot effectively process the\ncomplex background and dense space debris. In this paper, we propose a deep\nlearning-based Space Debris Tracking Network~(SDT-Net) to achieve highly\naccurate debris tracking. SDT-Net effectively represents the feature of debris,\nenhancing the efficiency and stability of end-to-end model learning. To train\nand evaluate this model effectively, we also produce a large-scale dataset\nSpace Debris Tracking Dataset (SDTD) by a novel observation-based data\nsimulation scheme. SDTD contains 18,040 video sequences with a total of 62,562\nframes and covers 250,000 synthetic space debris. Extensive experiments\nvalidate the effectiveness of our model and the challenging of our dataset.\nFurthermore, we test our model on real data from the Antarctic Station,\nachieving a MOTA score of 70.6%, which demonstrates its strong transferability\nto real-world scenarios. Our dataset and code will be released soon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7a7a\u95f4\u788e\u7247\u8ddf\u8e2a\u7f51\u7edc\uff08SDT-Net\uff09\u548c\u914d\u5957\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08SDTD\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u80cc\u666f\u4e0b\u5bc6\u96c6\u7a7a\u95f4\u788e\u7247\u7684\u5b9e\u65f6\u51c6\u786e\u8ddf\u8e2a\u95ee\u9898\uff0c\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u7a7a\u95f4\u788e\u7247\u7684\u6f5c\u5728\u5a01\u80c1\u65e5\u76ca\u4e25\u91cd\uff0c\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u590d\u6742\u80cc\u666f\u548c\u5bc6\u96c6\u788e\u7247\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8ddf\u8e2a\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSDT-Net\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u914d\u5957\u751f\u6210SDTD\u6570\u636e\u96c6\uff0c\u5305\u542b18,040\u4e2a\u89c6\u9891\u5e8f\u5217\u548c250,000\u4e2a\u5408\u6210\u788e\u7247\u3002", "result": "\u6a21\u578b\u5728\u771f\u5b9e\u5357\u6781\u7ad9\u6570\u636e\u4e0a\u8fbe\u523070.6%\u7684MOTA\u5206\u6570\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5f3a\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "SDT-Net\u548cSDTD\u7684\u7ed3\u5408\u4e3a\u7a7a\u95f4\u788e\u7247\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u7a7a\u95f4\u788e\u7247\u8ddf\u8e2a,\u6df1\u5ea6\u5b66\u4e60,SDT-Net,SDTD\u6570\u636e\u96c6"}}
{"id": "2506.02336", "pdf": "https://arxiv.org/pdf/2506.02336", "abs": "https://arxiv.org/abs/2506.02336", "authors": ["Jingfeng Wu", "Pierre Marion", "Peter Bartlett"], "title": "Large Stepsizes Accelerate Gradient Descent for Regularized Logistic Regression", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We study gradient descent (GD) with a constant stepsize for\n$\\ell_2$-regularized logistic regression with linearly separable data.\nClassical theory suggests small stepsizes to ensure monotonic reduction of the\noptimization objective, achieving exponential convergence in\n$\\widetilde{\\mathcal{O}}(\\kappa)$ steps with $\\kappa$ being the condition\nnumber. Surprisingly, we show that this can be accelerated to\n$\\widetilde{\\mathcal{O}}(\\sqrt{\\kappa})$ by simply using a large stepsize --\nfor which the objective evolves nonmonotonically. The acceleration brought by\nlarge stepsizes extends to minimizing the population risk for separable\ndistributions, improving on the best-known upper bounds on the number of steps\nto reach a near-optimum. Finally, we characterize the largest stepsize for the\nlocal convergence of GD, which also determines the global convergence in\nspecial scenarios. Our results extend the analysis of Wu et al. (2024) from\nconvex settings with minimizers at infinity to strongly convex cases with\nfinite minimizers.", "AI": {"tldr": "\u7814\u7a76\u5728$\\ell_2$\u6b63\u5219\u5316\u903b\u8f91\u56de\u5f52\u4e2d\uff0c\u68af\u5ea6\u4e0b\u964d\uff08GD\uff09\u4f7f\u7528\u5927\u6b65\u957f\u65f6\u5bf9\u7ebf\u6027\u53ef\u5206\u6570\u636e\u7684\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7406\u8bba\u5efa\u8bae\u4f7f\u7528\u5c0f\u6b65\u957f\u4ee5\u786e\u4fdd\u4f18\u5316\u76ee\u6807\u7684\u5355\u8c03\u4e0b\u964d\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5927\u6b65\u957f\u53ef\u4ee5\u663e\u8457\u52a0\u901f\u6536\u655b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u68af\u5ea6\u4e0b\u964d\u5728\u5927\u6b65\u957f\u4e0b\u7684\u975e\u5355\u8c03\u76ee\u6807\u6f14\u5316\u884c\u4e3a\uff0c\u7814\u7a76\u5176\u6536\u655b\u901f\u5ea6\u4e0e\u6761\u4ef6\u6570\u7684\u5173\u7cfb\u3002", "result": "\u4f7f\u7528\u5927\u6b65\u957f\u53ef\u5c06\u6536\u655b\u6b65\u6570\u4ece$\\widetilde{\\mathcal{O}}(\\kappa)$\u52a0\u901f\u5230$\\widetilde{\\mathcal{O}}(\\sqrt{\\kappa})$\uff0c\u5e76\u62d3\u5c55\u5230\u6700\u5c0f\u5316\u53ef\u5206\u5206\u5e03\u4e0b\u7684\u603b\u4f53\u98ce\u9669\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5927\u6b65\u957f\u53ef\u4ee5\u663e\u8457\u52a0\u901f\u6536\u655b\uff0c\u5e76\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u51b3\u5b9a\u5168\u5c40\u6536\u655b\u6027\u3002", "keywords": "\u68af\u5ea6\u4e0b\u964d, \u903b\u8f91\u56de\u5f52, \u7ebf\u6027\u53ef\u5206, \u6536\u655b\u901f\u5ea6, \u6761\u4ef6\u6570"}}
{"id": "2506.02615", "pdf": "https://arxiv.org/pdf/2506.02615", "abs": "https://arxiv.org/abs/2506.02615", "authors": ["Safaa Abdullahi Moallim Mohamud", "Minjin Baek", "Dong Seog Han"], "title": "Hierarchical Question-Answering for Driving Scene Understanding Using Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In this paper, we present a hierarchical question-answering (QA) approach for\nscene understanding in autonomous vehicles, balancing cost-efficiency with\ndetailed visual interpretation. The method fine-tunes a compact vision-language\nmodel (VLM) on a custom dataset specific to the geographical area in which the\nvehicle operates to capture key driving-related visual elements. At the\ninference stage, the hierarchical QA strategy decomposes the scene\nunderstanding task into high-level and detailed sub-questions. Instead of\ngenerating lengthy descriptions, the VLM navigates a structured question tree,\nwhere answering high-level questions (e.g., \"Is it possible for the ego vehicle\nto turn left at the intersection?\") triggers more detailed sub-questions (e.g.,\n\"Is there a vehicle approaching the intersection from the opposite\ndirection?\"). To optimize inference time, questions are dynamically skipped\nbased on previous answers, minimizing computational overhead. The extracted\nanswers are then synthesized using handcrafted templates to ensure coherent,\ncontextually accurate scene descriptions. We evaluate the proposed approach on\nthe custom dataset using GPT reference-free scoring, demonstrating its\ncompetitiveness with state-of-the-art methods like GPT-4o in capturing key\nscene details while achieving significantly lower inference time. Moreover,\nqualitative results from real-time deployment highlight the proposed approach's\ncapacity to capture key driving elements with minimal latency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u7684\u5206\u5c42\u95ee\u7b54\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u5236\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u52a8\u6001\u95ee\u9898\u6811\u4f18\u5316\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u4e2d\u6210\u672c\u6548\u7387\u4e0e\u8be6\u7ec6\u89c6\u89c9\u89e3\u6790\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u5728\u5b9a\u5236\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u5206\u5c42\u95ee\u7b54\u7b56\u7565\u52a8\u6001\u5206\u89e3\u95ee\u9898\u6811\uff0c\u8df3\u8fc7\u65e0\u5173\u95ee\u9898\u4ee5\u4f18\u5316\u63a8\u7406\u65f6\u95f4\u3002", "result": "\u5728\u5b9a\u5236\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u7406\u65f6\u95f4\u663e\u8457\u4f4e\u4e8eGPT-4o\uff0c\u5b9e\u65f6\u90e8\u7f72\u9a8c\u8bc1\u4e86\u4f4e\u5ef6\u8fdf\u4e0b\u7684\u9ad8\u6548\u573a\u666f\u6355\u6349\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u5b9e\u65f6\u573a\u666f\u7406\u89e3\u3002", "keywords": "\u5206\u5c42\u95ee\u7b54,\u89c6\u89c9\u8bed\u8a00\u6a21\u578b,\u81ea\u52a8\u9a7e\u9a76,\u573a\u666f\u7406\u89e3,\u63a8\u7406\u4f18\u5316"}}
{"id": "2506.02366", "pdf": "https://arxiv.org/pdf/2506.02366", "abs": "https://arxiv.org/abs/2506.02366", "authors": ["Qin Xie", "Qinghua Zhang", "Shuyin Xia"], "title": "Approximate Borderline Sampling using Granular-Ball for Classification Tasks", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Data sampling enhances classifier efficiency and robustness through data\ncompression and quality improvement. Recently, the sampling method based on\ngranular-ball (GB) has shown promising performance in generality and noisy\nclassification tasks. However, some limitations remain, including the absence\nof borderline sampling strategies and issues with class boundary blurring or\nshrinking due to overlap between GBs. In this paper, an approximate borderline\nsampling method using GBs is proposed for classification tasks. First, a\nrestricted diffusion-based GB generation (RD-GBG) method is proposed, which\nprevents GB overlaps by constrained expansion, preserving precise geometric\nrepresentation of GBs via redefined ones. Second, based on the concept of\nheterogeneous nearest neighbor, a GB-based approximate borderline sampling\n(GBABS) method is proposed, which is the first general sampling method capable\nof both borderline sampling and improving the quality of class noise datasets.\nAdditionally, since RD-GBG incorporates noise detection and GBABS focuses on\nborderline samples, GBABS performs outstandingly on class noise datasets\nwithout the need for an optimal purity threshold. Experimental results\ndemonstrate that the proposed methods outperform the GB-based sampling method\nand several representative sampling methods. Our source code is publicly\navailable at https://github.com/CherylTse/GBABS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7c92\u5ea6\u7403\uff08GB\uff09\u7684\u8fd1\u4f3c\u8fb9\u754c\u91c7\u6837\u65b9\u6cd5GBABS\uff0c\u89e3\u51b3\u4e86\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fb9\u754c\u6a21\u7cca\u6216\u6536\u7f29\u7684\u95ee\u9898\uff0c\u5e76\u5728\u566a\u58f0\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u6570\u636e\u91c7\u6837\u901a\u8fc7\u6570\u636e\u538b\u7f29\u548c\u8d28\u91cf\u63d0\u5347\u63d0\u5347\u5206\u7c7b\u5668\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002\u73b0\u6709\u7684GB\u91c7\u6837\u65b9\u6cd5\u5728\u8fb9\u754c\u91c7\u6837\u548c\u566a\u58f0\u6570\u636e\u96c6\u5904\u7406\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u6539\u8fdb\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "1. \u63d0\u51fa\u53d7\u9650\u6269\u6563\u7684GB\u751f\u6210\u65b9\u6cd5RD-GBG\uff0c\u907f\u514d\u91cd\u53e0\u5e76\u4fdd\u7559\u51e0\u4f55\u8868\u793a\uff1b2. \u57fa\u4e8e\u5f02\u8d28\u6700\u8fd1\u90bb\u6982\u5ff5\uff0c\u63d0\u51faGBABS\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u8fb9\u754c\u91c7\u6837\u548c\u566a\u58f0\u6570\u636e\u96c6\u8d28\u91cf\u63d0\u5347\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGBABS\u5728\u8fb9\u754c\u91c7\u6837\u548c\u566a\u58f0\u6570\u636e\u96c6\u5904\u7406\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u65e0\u9700\u6700\u4f18\u7eaf\u5ea6\u9608\u503c\u3002", "conclusion": "GBABS\u65b9\u6cd5\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u566a\u58f0\u6570\u636e\u96c6\u4e0a\uff0c\u4e3a\u6570\u636e\u91c7\u6837\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u6570\u636e\u91c7\u6837,\u7c92\u5ea6\u7403,\u8fb9\u754c\u91c7\u6837,\u566a\u58f0\u6570\u636e\u96c6,\u5206\u7c7b"}}
{"id": "2506.02373", "pdf": "https://arxiv.org/pdf/2506.02373", "abs": "https://arxiv.org/abs/2506.02373", "authors": ["Kordel K. France", "Ovidiu Daescu"], "title": "Olfactory Inertial Odometry: Methodology for Effective Robot Navigation by Scent", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY", "physics.ins-det"], "comment": null, "summary": "Olfactory navigation is one of the most primitive mechanisms of exploration\nused by organisms. Navigation by machine olfaction (artificial smell) is a very\ndifficult task to both simulate and solve. With this work, we define olfactory\ninertial odometry (OIO), a framework for using inertial kinematics, and\nfast-sampling olfaction sensors to enable navigation by scent analogous to\nvisual inertial odometry (VIO). We establish how principles from SLAM and VIO\ncan be extrapolated to olfaction to enable real-world robotic tasks. We\ndemonstrate OIO with three different odour localization algorithms on a real\n5-DoF robot arm over an odour-tracking scenario that resembles real\napplications in agriculture and food quality control. Our results indicate\nsuccess in establishing a baseline framework for OIO from which other research\nin olfactory navigation can build, and we note performance enhancements that\ncan be made to address more complex tasks in the future.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u55c5\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08OIO\uff09\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u60ef\u6027\u52a8\u529b\u5b66\u548c\u5feb\u901f\u55c5\u89c9\u4f20\u611f\u5668\uff0c\u5b9e\u73b0\u7c7b\u4f3c\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u7684\u55c5\u89c9\u5bfc\u822a\u3002", "motivation": "\u55c5\u89c9\u5bfc\u822a\u662f\u751f\u7269\u63a2\u7d22\u73af\u5883\u7684\u91cd\u8981\u673a\u5236\uff0c\u4f46\u5728\u673a\u5668\u4eba\u4e2d\u6a21\u62df\u548c\u89e3\u51b3\u8fd9\u4e00\u4efb\u52a1\u975e\u5e38\u56f0\u96be\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7OIO\u6846\u67b6\u5c06SLAM\u548cVIO\u7684\u539f\u7406\u6269\u5c55\u5230\u55c5\u89c9\u9886\u57df\uff0c\u63a8\u52a8\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u7684\u53d1\u5c55\u3002", "method": "\u8bba\u6587\u5b9a\u4e49\u4e86\u55c5\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08OIO\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u60ef\u6027\u52a8\u529b\u5b66\u548c\u5feb\u901f\u55c5\u89c9\u4f20\u611f\u5668\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u6c14\u5473\u5b9a\u4f4d\u7b97\u6cd5\uff0c\u5728\u4e00\u4e2a5\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u624b\u81c2\u4e0a\u8fdb\u884c\u4e86\u6c14\u5473\u8ffd\u8e2a\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOIO\u6846\u67b6\u4e3a\u55c5\u89c9\u5bfc\u822a\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u80fd\u591f\u6210\u529f\u5e94\u7528\u4e8e\u519c\u4e1a\u548c\u98df\u54c1\u8d28\u68c0\u7b49\u5b9e\u9645\u573a\u666f\u3002", "conclusion": "OIO\u6846\u67b6\u4e3a\u55c5\u89c9\u5bfc\u822a\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u5e94\u5bf9\u66f4\u590d\u6742\u7684\u4efb\u52a1\u3002", "keywords": "\u55c5\u89c9\u5bfc\u822a, \u60ef\u6027\u91cc\u7a0b\u8ba1, \u673a\u5668\u4eba, \u6c14\u5473\u5b9a\u4f4d"}}
{"id": "2506.02634", "pdf": "https://arxiv.org/pdf/2506.02634", "abs": "https://arxiv.org/abs/2506.02634", "authors": ["Jiahao Wang", "Jinbo Han", "Xingda Wei", "Sijie Shen", "Dingyan Zhang", "Chenguang Fang", "Rong Chen", "Wenyuan Yu", "Haibo Chen"], "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider", "categories": ["cs.DC", "cs.AI"], "comment": "Accepted by USENIX ATC'25", "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u670d\u52a1\u4e2dKV\u7f13\u5b58\u7684\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u5f0f\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7f13\u5b58\u7b56\u7565\u4f18\u5316\u5efa\u8bae\u3002", "motivation": "LLM\u670d\u52a1\u4e2dKV\u7f13\u5b58\u7684\u4f18\u5316\u6548\u679c\u5c1a\u4e0d\u660e\u786e\uff0c\u4e14\u7f13\u5b58\u7b56\u7565\u9ad8\u5ea6\u4f9d\u8d56\u5de5\u4f5c\u8d1f\u8f7d\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684KV\u7f13\u5b58\u7279\u5f81\u5206\u6790\u3002", "method": "\u901a\u8fc7\u5206\u6790\u9886\u5148LLM\u670d\u52a1\u63d0\u4f9b\u5546\u7684\u6570\u636e\uff0c\u7cfb\u7edf\u6027\u5730\u8868\u5f81KV\u7f13\u5b58\u7684\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u7684\u7f13\u5b58\u6dd8\u6c70\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0KV\u7f13\u5b58\u7684\u590d\u7528\u6a21\u5f0f\u591a\u6837\u4e14\u53ef\u9884\u6d4b\uff0c\u7406\u60f3\u7f13\u5b58\u547d\u4e2d\u7387\u6240\u9700\u7f13\u5b58\u5bb9\u91cf\u9002\u4e2d\u3002\u65b0\u7b56\u7565\u63d0\u5347\u4e86\u6709\u9650\u7f13\u5b58\u5bb9\u91cf\u4e0b\u7684\u670d\u52a1\u6027\u80fd\u3002", "conclusion": "\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684KV\u7f13\u5b58\u7279\u5f81\u5206\u6790\u4e3aLLM\u670d\u52a1\u4f18\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5c24\u5176\u662f\u7f13\u5b58\u7b56\u7565\u7684\u8bbe\u8ba1\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001KV\u7f13\u5b58\u3001\u5de5\u4f5c\u8d1f\u8f7d\u5206\u6790\u3001\u7f13\u5b58\u6dd8\u6c70\u7b56\u7565\u3001\u670d\u52a1\u6027\u80fd"}}
{"id": "2506.02382", "pdf": "https://arxiv.org/pdf/2506.02382", "abs": "https://arxiv.org/abs/2506.02382", "authors": ["Seulgi Kim", "Ghazal Kaviani", "Mohit Prabhushankar", "Ghassan AlRegib"], "title": "Multi-level and Multi-modal Action Anticipation", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted in 2025 IEEE International Conference on Image Processing\n  (ICIP)", "summary": "Action anticipation, the task of predicting future actions from partially\nobserved videos, is crucial for advancing intelligent systems. Unlike action\nrecognition, which operates on fully observed videos, action anticipation must\nhandle incomplete information. Hence, it requires temporal reasoning, and\ninherent uncertainty handling. While recent advances have been made,\ntraditional methods often focus solely on visual modalities, neglecting the\npotential of integrating multiple sources of information. Drawing inspiration\nfrom human behavior, we introduce \\textit{Multi-level and Multi-modal Action\nAnticipation (m\\&m-Ant)}, a novel multi-modal action anticipation approach that\ncombines both visual and textual cues, while explicitly modeling hierarchical\nsemantic information for more accurate predictions. To address the challenge of\ninaccurate coarse action labels, we propose a fine-grained label generator\npaired with a specialized temporal consistency loss function to optimize\nperformance. Extensive experiments on widely used datasets, including\nBreakfast, 50 Salads, and DARai, demonstrate the effectiveness of our approach,\nachieving state-of-the-art results with an average anticipation accuracy\nimprovement of 3.08\\% over existing methods. This work underscores the\npotential of multi-modal and hierarchical modeling in advancing action\nanticipation and establishes a new benchmark for future research in the field.\nOur code is available at: https://github.com/olivesgatech/mM-ant.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u548c\u591a\u5c42\u6b21\u7684\u52a8\u4f5c\u9884\u6d4b\u65b9\u6cd5\uff08m&m-Ant\uff09\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u7ebf\u7d22\uff0c\u5e76\u901a\u8fc7\u7ec6\u7c92\u5ea6\u6807\u7b7e\u751f\u6210\u5668\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u635f\u5931\u51fd\u6570\u4f18\u5316\u9884\u6d4b\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e863.08%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u52a8\u4f5c\u9884\u6d4b\u4efb\u52a1\u9700\u8981\u5904\u7406\u4e0d\u5b8c\u6574\u4fe1\u606f\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u89c6\u89c9\u6a21\u6001\uff0c\u5ffd\u7565\u4e86\u591a\u6a21\u6001\u4fe1\u606f\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fam&m-Ant\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\uff0c\u5e76\u5229\u7528\u7ec6\u7c92\u5ea6\u6807\u7b7e\u751f\u6210\u5668\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u635f\u5931\u51fd\u6570\u8fdb\u884c\u6a21\u578b\u4f18\u5316\u3002", "result": "\u5728Breakfast\u300150 Salads\u548cDARai\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u5e73\u5747\u63d0\u53473.08%\u3002", "conclusion": "\u591a\u6a21\u6001\u548c\u5c42\u6b21\u5316\u5efa\u6a21\u5728\u52a8\u4f5c\u9884\u6d4b\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002", "keywords": "\u52a8\u4f5c\u9884\u6d4b\u3001\u591a\u6a21\u6001\u3001\u5c42\u6b21\u5316\u5efa\u6a21\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u7ec6\u7c92\u5ea6\u6807\u7b7e"}}
{"id": "2506.02394", "pdf": "https://arxiv.org/pdf/2506.02394", "abs": "https://arxiv.org/abs/2506.02394", "authors": ["Yuan Bian", "Xingche Guo", "Yuanjia Wang"], "title": "Joint Modeling for Learning Decision-Making Dynamics in Behavioral Experiments", "categories": ["stat.ME", "cs.LG"], "comment": null, "summary": "Major depressive disorder (MDD), a leading cause of disability and mortality,\nis associated with reward-processing abnormalities and concentration issues.\nMotivated by the probabilistic reward task from the Establishing Moderators and\nBiosignatures of Antidepressant Response in Clinical Care (EMBARC) study, we\npropose a novel framework that integrates the reinforcement learning (RL) model\nand drift-diffusion model (DDM) to jointly analyze reward-based decision-making\nwith response times. To account for emerging evidence suggesting that\ndecision-making may alternate between multiple interleaved strategies, we model\nlatent state switching using a hidden Markov model (HMM). In the ''engaged''\nstate, decisions follow an RL-DDM, simultaneously capturing reward processing,\ndecision dynamics, and temporal structure. In contrast, in the ''lapsed''\nstate, decision-making is modeled using a simplified DDM, where specific\nparameters are fixed to approximate random guessing with equal probability. The\nproposed method is implemented using a computationally efficient generalized\nexpectation-maximization algorithm with forward-backward procedures. Through\nextensive numerical studies, we demonstrate that our proposed method\noutperforms competing approaches under various reward-generating distributions,\nboth with and without strategy switching. When applied to the EMBARC study, our\nframework reveals that MDD patients exhibit lower overall engagement than\nhealthy controls and experience longer decision times when they do engage.\nAdditionally, we show that neuroimaging measures of brain activities are\nassociated with decision-making characteristics in the ''engaged'' state but\nnot in the ''lapsed'' state, providing evidence of brain-behavioral association\nspecific to the ''engaged'' state.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u6f02\u79fb\u6269\u6563\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u5956\u52b1\u51b3\u7b56\u548c\u65f6\u95f4\u54cd\u5e94\uff0c\u5e76\u901a\u8fc7\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u6355\u6349\u7b56\u7565\u5207\u6362\u3002\u8be5\u65b9\u6cd5\u5728EMBARC\u7814\u7a76\u4e2d\u663e\u793a\uff0c\u6291\u90c1\u75c7\u60a3\u8005\u7684\u51b3\u7b56\u53c2\u4e0e\u5ea6\u8f83\u4f4e\u4e14\u51b3\u7b56\u65f6\u95f4\u66f4\u957f\u3002", "motivation": "\u6291\u90c1\u75c7\u4e0e\u5956\u52b1\u5904\u7406\u5f02\u5e38\u548c\u6ce8\u610f\u529b\u95ee\u9898\u76f8\u5173\uff0c\u5e0c\u671b\u901a\u8fc7\u6574\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u6f02\u79fb\u6269\u6563\u6a21\u578b\u6765\u66f4\u5168\u9762\u5730\u5206\u6790\u51b3\u7b56\u884c\u4e3a\u3002", "method": "\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u6f02\u79fb\u6269\u6563\u6a21\u578b\uff08DDM\uff09\uff0c\u4f7f\u7528\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff08HMM\uff09\u6355\u6349\u7b56\u7565\u5207\u6362\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u7684\u8ba1\u7b97\u7b97\u6cd5\u5b9e\u73b0\u3002", "result": "\u65b9\u6cd5\u5728\u6570\u503c\u7814\u7a76\u4e2d\u8868\u73b0\u4f18\u5f02\uff1b\u5e94\u7528\u4e8eEMBARC\u7814\u7a76\u65f6\uff0c\u53d1\u73b0\u6291\u90c1\u75c7\u60a3\u8005\u53c2\u4e0e\u5ea6\u66f4\u4f4e\u4e14\u51b3\u7b56\u65f6\u95f4\u66f4\u957f\uff1b\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u663e\u793a\u51b3\u7b56\u7279\u5f81\u4e0e\u201c\u53c2\u4e0e\u201d\u72b6\u6001\u76f8\u5173\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u6709\u6548\u5206\u6790\u51b3\u7b56\u884c\u4e3a\uff0c\u63ed\u793a\u6291\u90c1\u75c7\u60a3\u8005\u7684\u51b3\u7b56\u7279\u70b9\uff0c\u5e76\u63d0\u4f9b\u4e0e\u201c\u53c2\u4e0e\u201d\u72b6\u6001\u76f8\u5173\u7684\u8111\u884c\u4e3a\u8bc1\u636e\u3002", "keywords": "\u6291\u90c1\u75c7\uff1b\u5f3a\u5316\u5b66\u4e60\uff1b\u6f02\u79fb\u6269\u6563\u6a21\u578b\uff1b\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff1bEMBARC\u7814\u7a76"}}
{"id": "2506.02413", "pdf": "https://arxiv.org/pdf/2506.02413", "abs": "https://arxiv.org/abs/2506.02413", "authors": ["Tian Lan", "Jie Guo", "Chen Zhang"], "title": "Tensor State Space-based Dynamic Multilayer Network Modeling", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Understanding the complex interactions within dynamic multilayer networks is\ncritical for advancements in various scientific domains. Existing models often\nfail to capture such networks' temporal and cross-layer dynamics. This paper\nintroduces a novel Tensor State Space Model for Dynamic Multilayer Networks\n(TSSDMN), utilizing a latent space model framework. TSSDMN employs a symmetric\nTucker decomposition to represent latent node features, their interaction\npatterns, and layer transitions. Then by fixing the latent features and\nallowing the interaction patterns to evolve over time, TSSDMN uniquely captures\nboth the temporal dynamics within layers and across different layers. The model\nidentifiability conditions are discussed. By treating latent features as\nvariables whose posterior distributions are approximated using a mean-field\nvariational inference approach, a variational Expectation Maximization\nalgorithm is developed for efficient model inference. Numerical simulations and\ncase studies demonstrate the efficacy of TSSDMN for understanding dynamic\nmultilayer networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f20\u91cf\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08TSSDMN\uff09\uff0c\u7528\u4e8e\u52a8\u6001\u591a\u5c42\u7f51\u7edc\u5206\u6790\uff0c\u901a\u8fc7\u5bf9\u79f0Tucker\u5206\u89e3\u548c\u53d8\u5206EM\u7b97\u6cd5\u6709\u6548\u6355\u6349\u7f51\u7edc\u7684\u65f6\u95f4\u548c\u8de8\u5c42\u52a8\u6001\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u6355\u6349\u52a8\u6001\u591a\u5c42\u7f51\u7edc\u7684\u65f6\u95f4\u548c\u8de8\u5c42\u52a8\u6001\u7279\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5bf9\u79f0Tucker\u5206\u89e3\u8868\u793a\u6f5c\u5728\u8282\u70b9\u7279\u5f81\u53ca\u5176\u4ea4\u4e92\u6a21\u5f0f\uff0c\u901a\u8fc7\u56fa\u5b9a\u6f5c\u5728\u7279\u5f81\u5e76\u5141\u8bb8\u4ea4\u4e92\u6a21\u5f0f\u968f\u65f6\u95f4\u6f14\u5316\uff0c\u7ed3\u5408\u53d8\u5206\u671f\u671b\u6700\u5927\u5316\uff08EM\uff09\u7b97\u6cd5\u8fdb\u884c\u6a21\u578b\u63a8\u65ad\u3002", "result": "\u6570\u503c\u6a21\u62df\u548c\u6848\u4f8b\u7814\u7a76\u8868\u660eTSSDMN\u80fd\u6709\u6548\u7406\u89e3\u52a8\u6001\u591a\u5c42\u7f51\u7edc\u3002", "conclusion": "TSSDMN\u4e3a\u52a8\u6001\u591a\u5c42\u7f51\u7edc\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5177\u6709\u7406\u8bba\u548c\u5e94\u7528\u4ef7\u503c\u3002", "keywords": "\u52a8\u6001\u591a\u5c42\u7f51\u7edc\u3001\u5f20\u91cf\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3001Tucker\u5206\u89e3\u3001\u53d8\u5206EM\u7b97\u6cd5"}}
{"id": "2506.02677", "pdf": "https://arxiv.org/pdf/2506.02677", "abs": "https://arxiv.org/abs/2506.02677", "authors": ["Jintao Tong", "Yixiong Zou", "Guangyao Chen", "Yuhua Li", "Ruixuan Li"], "title": "Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Cross-Domain Few-Shot Segmentation (CD-FSS) aims to transfer knowledge from a\nsource-domain dataset to unseen target-domain datasets with limited\nannotations. Current methods typically compare the distance between training\nand testing samples for mask prediction. However, we find an entanglement\nproblem exists in this widely adopted method, which tends to bind sourcedomain\npatterns together and make each of them hard to transfer. In this paper, we aim\nto address this problem for the CD-FSS task. We first find a natural\ndecomposition of the ViT structure, based on which we delve into the\nentanglement problem for an interpretation. We find the decomposed ViT\ncomponents are crossly compared between images in distance calculation, where\nthe rational comparisons are entangled with those meaningless ones by their\nequal importance, leading to the entanglement problem. Based on this\ninterpretation, we further propose to address the entanglement problem by\nlearning to weigh for all comparisons of ViT components, which learn\ndisentangled features and re-compose them for the CD-FSS task, benefiting both\nthe generalization and finetuning. Experiments show that our model outperforms\nthe state-of-the-art CD-FSS method by 1.92% and 1.88% in average accuracy under\n1-shot and 5-shot settings, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u8de8\u57df\u5c11\u6837\u672c\u5206\u5272\u4efb\u52a1\u4e2d\u7ea0\u7f20\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3ViT\u7ed3\u6784\u5e76\u5b66\u4e60\u6743\u91cd\u91cd\u7ec4\u7279\u5f81\uff0c\u63d0\u5347\u6cdb\u5316\u548c\u5fae\u8c03\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8de8\u57df\u5c11\u6837\u672c\u5206\u5272\u65b9\u6cd5\u5728\u8ddd\u79bb\u6bd4\u8f83\u4e2d\u5b58\u5728\u7ea0\u7f20\u95ee\u9898\uff0c\u5bfc\u81f4\u6e90\u57df\u6a21\u5f0f\u96be\u4ee5\u8fc1\u79fb\u3002", "method": "\u5206\u89e3ViT\u7ed3\u6784\u5e76\u5b66\u4e60\u6743\u91cd\u4ee5\u89e3\u8026\u7279\u5f81\uff0c\u4f18\u5316\u8ddd\u79bb\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u57281-shot\u548c5-shot\u8bbe\u7f6e\u4e0b\u5e73\u5747\u51c6\u786e\u7387\u5206\u522b\u63d0\u53471.92%\u548c1.88%\u3002", "conclusion": "\u5b66\u4e60\u6743\u91cd\u91cd\u7ec4ViT\u7279\u5f81\u80fd\u6709\u6548\u89e3\u51b3\u7ea0\u7f20\u95ee\u9898\uff0c\u63d0\u5347\u8de8\u57df\u6027\u80fd\u3002", "keywords": "\u8de8\u57df\u5c11\u6837\u672c\u5206\u5272,ViT,\u7279\u5f81\u89e3\u8026,\u8ddd\u79bb\u5b66\u4e60"}}
{"id": "2506.02422", "pdf": "https://arxiv.org/pdf/2506.02422", "abs": "https://arxiv.org/abs/2506.02422", "authors": ["Xiyu Zhao", "Qimei Cui", "Ziqiang Du", "Weicai Li", "Xi Yu", "Wei Ni", "Ji Zhang", "Xiaofeng Tao", "Ping Zhang"], "title": "Enhancing Convergence, Privacy and Fairness for Wireless Personalized Federated Learning: Quantization-Assisted Min-Max Fair Scheduling", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Personalized federated learning (PFL) offers a solution to balancing\npersonalization and generalization by conducting federated learning (FL) to\nguide personalized learning (PL). Little attention has been given to wireless\nPFL (WPFL), where privacy concerns arise. Performance fairness of PL models is\nanother challenge resulting from communication bottlenecks in WPFL. This paper\nexploits quantization errors to enhance the privacy of WPFL and proposes a\nnovel quantization-assisted Gaussian differential privacy (DP) mechanism. We\nanalyze the convergence upper bounds of individual PL models by considering the\nimpact of the mechanism (i.e., quantization errors and Gaussian DP noises) and\nimperfect communication channels on the FL of WPFL. By minimizing the maximum\nof the bounds, we design an optimal transmission scheduling strategy that\nyields min-max fairness for WPFL with OFDMA interfaces. This is achieved by\nrevealing the nested structure of this problem to decouple it into subproblems\nsolved sequentially for the client selection, channel allocation, and power\ncontrol, and for the learning rates and PL-FL weighting coefficients.\nExperiments validate our analysis and demonstrate that our approach\nsubstantially outperforms alternative scheduling strategies by 87.08%, 16.21%,\nand 38.37% in accuracy, the maximum test loss of participating clients, and\nfairness (Jain's index), respectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5316\u8f85\u52a9\u9ad8\u65af\u5dee\u5206\u9690\u79c1\u673a\u5236\u7684\u65e0\u7ebf\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\uff08WPFL\uff09\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u9690\u79c1\u4fdd\u62a4\u548c\u6027\u80fd\u516c\u5e73\u6027\u3002", "motivation": "\u89e3\u51b3\u65e0\u7ebf\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u9690\u79c1\u95ee\u9898\u548c\u6027\u80fd\u516c\u5e73\u6027\u6311\u6218\u3002", "method": "\u5229\u7528\u91cf\u5316\u8bef\u5dee\u8bbe\u8ba1\u9ad8\u65af\u5dee\u5206\u9690\u79c1\u673a\u5236\uff0c\u5206\u6790\u6536\u655b\u4e0a\u754c\uff0c\u5e76\u8bbe\u8ba1\u6700\u4f18\u4f20\u8f93\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u6700\u5927\u6d4b\u8bd5\u635f\u5931\u548c\u516c\u5e73\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u7b56\u7565\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86WPFL\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u6027\u80fd\u516c\u5e73\u6027\u3002", "keywords": "\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60, \u9690\u79c1\u4fdd\u62a4, \u6027\u80fd\u516c\u5e73\u6027, \u91cf\u5b50\u5316\u8bef\u5dee, \u9ad8\u65af\u5dee\u5206\u9690\u79c1"}}
{"id": "2506.02458", "pdf": "https://arxiv.org/pdf/2506.02458", "abs": "https://arxiv.org/abs/2506.02458", "authors": ["Nguyen Chi Long", "Trinh Van Chien", "Ta Hai Tung", "Van Son Nguyen", "Trong-Minh Hoang", "Nguyen Ngoc Hai Dang"], "title": "A Novel Deep Reinforcement Learning Method for Computation Offloading in Multi-User Mobile Edge Computing with Decentralization", "categories": ["cs.IT", "cs.LG", "math.IT"], "comment": "6 pages, 5 figures, and 1 table. Published by IEEE at ATC2024", "summary": "Mobile edge computing (MEC) allows appliances to offload workloads to\nneighboring MEC servers that have the potential for computation-intensive tasks\nwith limited computational capabilities. This paper studied how deep\nreinforcement learning (DRL) algorithms are used in an MEC system to find\nfeasible decentralized dynamic computation offloading strategies, which leads\nto the construction of an extensible MEC system that operates effectively with\nfinite feedback. Even though the Deep Deterministic Policy Gradient (DDPG)\nalgorithm, subject to their knowledge of the MEC system, can be used to\nallocate powers of both computation offloading and local execution, to learn a\ncomputation offloading policy for each user independently, we realized that\nthis solution still has some inherent weaknesses. Hence, we introduced a new\napproach for this problem based on the Twin Delayed DDPG algorithm, which\nenables us to overcome this proneness and investigate cases where mobile users\nare portable. Numerical results showed that individual users can autonomously\nlearn adequate policies through the proposed approach. Besides, the performance\nof the suggested solution exceeded the conventional DDPG-based power control\nstrategy.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5229\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u5728\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u5206\u5e03\u5f0f\u52a8\u6001\u8ba1\u7b97\u5378\u8f7d\u7b56\u7565\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTwin Delayed DDPG\u7684\u65b0\u65b9\u6cd5\uff0c\u514b\u670d\u4e86DDPG\u7b97\u6cd5\u7684\u56fa\u6709\u5f31\u70b9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u7cfb\u7edf\u9700\u8981\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u5378\u8f7d\u7b56\u7565\uff0c\u800c\u73b0\u6709\u7684DDPG\u7b97\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u56fa\u6709\u7f3a\u9677\uff0c\u65e0\u6cd5\u6ee1\u8db3\u79fb\u52a8\u7528\u6237\u7684\u9700\u6c42\u3002", "method": "\u5f15\u5165\u57fa\u4e8eTwin Delayed DDPG\u7b97\u6cd5\u7684\u65b0\u65b9\u6cd5\uff0c\u72ec\u7acb\u5b66\u4e60\u6bcf\u4e2a\u7528\u6237\u7684\u8ba1\u7b97\u5378\u8f7d\u7b56\u7565\uff0c\u89e3\u51b3\u4e86DDPG\u7684\u7f3a\u9677\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u7528\u6237\u80fd\u81ea\u4e3b\u5b66\u4f1a\u6709\u6548\u7b56\u7565\uff0c\u4e14\u65b0\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfDDPG\u7b97\u6cd5\u3002", "conclusion": "Twin Delayed DDPG\u7b97\u6cd5\u80fd\u6709\u6548\u63d0\u5347MEC\u7cfb\u7edf\u7684\u8ba1\u7b97\u5378\u8f7d\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u79fb\u52a8\u7528\u6237\u573a\u666f\u3002", "keywords": "\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u3001\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u3001\u8ba1\u7b97\u5378\u8f7d\u3001DDPG\u3001Twin Delayed DDPG"}}
{"id": "2506.02489", "pdf": "https://arxiv.org/pdf/2506.02489", "abs": "https://arxiv.org/abs/2506.02489", "authors": ["Tao Zhong", "Jonah Buchanan", "Christine Allen-Blanchette"], "title": "Grasp2Grasp: Vision-Based Dexterous Grasp Translation via Schr\u00f6dinger Bridges", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "19 pages, 4 figures", "summary": "We propose a new approach to vision-based dexterous grasp translation, which\naims to transfer grasp intent across robotic hands with differing morphologies.\nGiven a visual observation of a source hand grasping an object, our goal is to\nsynthesize a functionally equivalent grasp for a target hand without requiring\npaired demonstrations or hand-specific simulations. We frame this problem as a\nstochastic transport between grasp distributions using the Schr\\\"odinger Bridge\nformalism. Our method learns to map between source and target latent grasp\nspaces via score and flow matching, conditioned on visual observations. To\nguide this translation, we introduce physics-informed cost functions that\nencode alignment in base pose, contact maps, wrench space, and manipulability.\nExperiments across diverse hand-object pairs demonstrate our approach generates\nstable, physically grounded grasps with strong generalization. This work\nenables semantic grasp transfer for heterogeneous manipulators and bridges\nvision-based grasping with probabilistic generative modeling.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u7075\u5de7\u6293\u53d6\u8f6c\u79fb\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7Schr\\\"odinger Bridge\u5f62\u5f0f\u5316\uff0c\u5b9e\u73b0\u4e0d\u540c\u5f62\u6001\u673a\u5668\u4eba\u624b\u4e4b\u95f4\u7684\u529f\u80fd\u7b49\u6548\u6293\u53d6\u5408\u6210\u3002", "motivation": "\u89e3\u51b3\u4e0d\u540c\u5f62\u6001\u673a\u5668\u4eba\u624b\u4e4b\u95f4\u6293\u53d6\u610f\u56fe\u8f6c\u79fb\u7684\u96be\u9898\uff0c\u907f\u514d\u914d\u5bf9\u6f14\u793a\u6216\u624b\u7279\u5b9a\u6a21\u62df\u7684\u9700\u6c42\u3002", "method": "\u5229\u7528Schr\\\"odinger Bridge\u6846\u67b6\u8fdb\u884c\u968f\u673a\u4f20\u8f93\uff0c\u901a\u8fc7\u5206\u6570\u548c\u6d41\u5339\u914d\u5b66\u4e60\u6e90\u4e0e\u76ee\u6807\u6f5c\u5728\u6293\u53d6\u7a7a\u95f4\u4e4b\u95f4\u7684\u6620\u5c04\uff0c\u7ed3\u5408\u7269\u7406\u77e5\u60c5\u6210\u672c\u51fd\u6570\u5f15\u5bfc\u8f6c\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b9\u6cd5\u80fd\u751f\u6210\u7a33\u5b9a\u4e14\u7269\u7406\u57fa\u7840\u624e\u5b9e\u7684\u6293\u53d6\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5b9e\u73b0\u4e86\u5f02\u6784\u64cd\u7eb5\u5668\u7684\u8bed\u4e49\u6293\u53d6\u8f6c\u79fb\uff0c\u8fde\u63a5\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u6293\u53d6\u4e0e\u6982\u7387\u751f\u6210\u5efa\u6a21\u3002", "keywords": "\u89c6\u89c9\u6293\u53d6, \u6293\u53d6\u8f6c\u79fb, Schr\\\"odinger Bridge, \u673a\u5668\u4eba\u64cd\u7eb5"}}
{"id": "2506.02733", "pdf": "https://arxiv.org/pdf/2506.02733", "abs": "https://arxiv.org/abs/2506.02733", "authors": ["Xiaoyi Feng", "Kaifeng Zou", "Caichun Cen", "Tao Huang", "Hui Guo", "Zizhou Huang", "Yingli Zhao", "Mingqing Zhang", "Diwei Wang", "Yuntao Zou", "Dagang Li"], "title": "LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing optical flow datasets focus primarily on real-world simulation or\nsynthetic human motion, but few are tailored to Celluloid(cel) anime character\nmotion: a domain with unique visual and motion characteristics. To bridge this\ngap and facilitate research in optical flow estimation and downstream tasks\nsuch as anime video generation and line drawing colorization, we introduce\nLinkTo-Anime, the first high-quality dataset specifically designed for cel\nanime character motion generated with 3D model rendering. LinkTo-Anime provides\nrich annotations including forward and backward optical flow, occlusion masks,\nand Mixamo Skeleton. The dataset comprises 395 video sequences, totally 24,230\ntraining frames, 720 validation frames, and 4,320 test frames. Furthermore, a\ncomprehensive benchmark is constructed with various optical flow estimation\nmethods to analyze the shortcomings and limitations across multiple datasets.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u9488\u5bf9\u8d5b\u7490\u73de\u52a8\u753b\u89d2\u8272\u8fd0\u52a8\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6LinkTo-Anime\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5149\u6d41\u6570\u636e\u96c6\u5728\u8be5\u9886\u57df\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u6ce8\u91ca\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u5149\u6d41\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u73b0\u5b9e\u4e16\u754c\u6a21\u62df\u6216\u5408\u6210\u4eba\u7c7b\u8fd0\u52a8\uff0c\u7f3a\u4e4f\u9488\u5bf9\u8d5b\u7490\u73de\u52a8\u753b\u89d2\u8272\u8fd0\u52a8\u7684\u6570\u636e\u96c6\uff0c\u4e3a\u6b64\u4f5c\u8005\u6784\u5efa\u4e86LinkTo-Anime\u4ee5\u63a8\u52a8\u5149\u6d41\u4f30\u8ba1\u53ca\u76f8\u5173\u4efb\u52a1\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc73D\u6a21\u578b\u6e32\u67d3\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5305\u542b\u524d\u5411\u548c\u540e\u5411\u5149\u6d41\u3001\u906e\u6321\u63a9\u7801\u53caMixamo\u9aa8\u9abc\u6807\u6ce8\uff0c\u603b\u8ba1395\u4e2a\u89c6\u9891\u5e8f\u5217\u548c\u5927\u91cf\u8bad\u7ec3\u3001\u9a8c\u8bc1\u53ca\u6d4b\u8bd5\u5e27\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u4e86\u591a\u79cd\u5149\u6d41\u4f30\u8ba1\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u4e0d\u8db3\u548c\u5c40\u9650\u6027\u3002", "conclusion": "LinkTo-Anime\u4e3a\u8d5b\u7490\u73de\u52a8\u753b\u9886\u57df\u7684\u5149\u6d41\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u5e76\u63a8\u52a8\u4e86\u76f8\u5173\u4efb\u52a1\u7684\u53d1\u5c55\u3002", "keywords": "\u5149\u6d41\u6570\u636e\u96c6, \u8d5b\u7490\u73de\u52a8\u753b, 3D\u6e32\u67d3, \u89c6\u9891\u751f\u6210, \u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2506.02742", "pdf": "https://arxiv.org/pdf/2506.02742", "abs": "https://arxiv.org/abs/2506.02742", "authors": ["Xiaoxue Gao", "Huayun Zhang", "Nancy F. Chen"], "title": "Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with Prompt-LLM Contextual Knowledge for Mixed Emotions", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "comment": null, "summary": "Existing expressive text-to-speech (TTS) systems primarily model a limited\nset of categorical emotions, whereas human conversations extend far beyond\nthese predefined emotions, making it essential to explore more diverse\nemotional speech generation for more natural interactions. To bridge this gap,\nthis paper proposes a novel prompt-unseen-emotion (PUE) approach to generate\nunseen emotional speech via emotion-guided prompt learning. PUE is trained\nutilizing an LLM-TTS architecture to ensure emotional consistency between\ncategorical emotion-relevant prompts and emotional speech, allowing the model\nto quantitatively capture different emotion weightings per utterance. During\ninference, mixed emotional speech can be generated by flexibly adjusting\nemotion proportions and leveraging LLM contextual knowledge, enabling the model\nto quantify different emotional styles. Our proposed PUE successfully\nfacilitates expressive speech synthesis of unseen emotions in a zero-shot\nsetting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff08PUE\uff09\uff0c\u901a\u8fc7\u60c5\u611f\u5f15\u5bfc\u63d0\u793a\u5b66\u4e60\u751f\u6210\u672a\u89c1\u8fc7\u7684\u60c5\u611f\u8bed\u97f3\uff0c\u586b\u8865\u4e86\u73b0\u6709TTS\u7cfb\u7edf\u5728\u60c5\u611f\u591a\u6837\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684TTS\u7cfb\u7edf\u4ec5\u80fd\u6a21\u62df\u6709\u9650\u7684\u60c5\u611f\u7c7b\u522b\uff0c\u800c\u4eba\u7c7b\u5bf9\u8bdd\u6d89\u53ca\u66f4\u590d\u6742\u7684\u60c5\u611f\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u66f4\u81ea\u7136\u7684\u60c5\u611f\u8bed\u97f3\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPUE\u65b9\u6cd5\uff0c\u57fa\u4e8eLLM-TTS\u67b6\u6784\uff0c\u5229\u7528\u60c5\u611f\u5f15\u5bfc\u63d0\u793a\u5b66\u4e60\uff0c\u5b9a\u91cf\u6355\u83b7\u60c5\u611f\u6743\u91cd\uff0c\u5e76\u901a\u8fc7\u7075\u6d3b\u8c03\u6574\u60c5\u611f\u6bd4\u4f8b\u751f\u6210\u6df7\u5408\u60c5\u611f\u8bed\u97f3\u3002", "result": "PUE\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u6210\u529f\u751f\u6210\u4e86\u672a\u89c1\u60c5\u611f\u7684\u8bed\u97f3\uff0c\u5b9e\u73b0\u4e86\u60c5\u611f\u591a\u6837\u6027\u3002", "conclusion": "PUE\u4e3a\u81ea\u7136\u4ea4\u4e92\u4e2d\u7684\u60c5\u611f\u8bed\u97f3\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u60c5\u611f\u8bed\u97f3\u5408\u6210\uff0cTTS\uff0c\u63d0\u793a\u5b66\u4e60\uff0c\u96f6\u6837\u672c\u5b66\u4e60\uff0cLLM"}}
{"id": "2506.02744", "pdf": "https://arxiv.org/pdf/2506.02744", "abs": "https://arxiv.org/abs/2506.02744", "authors": ["Junyuan Liu", "Xinglei Wang", "Tao Cheng"], "title": "Enriching Location Representation with Detailed Semantic Information", "categories": ["cs.CE", "cs.AI"], "comment": null, "summary": "Spatial representations that capture both structural and semantic\ncharacteristics of urban environments are essential for urban modeling.\nTraditional spatial embeddings often prioritize spatial proximity while\nunderutilizing fine-grained contextual information from places. To address this\nlimitation, we introduce CaLLiPer+, an extension of the CaLLiPer model that\nsystematically integrates Point-of-Interest (POI) names alongside categorical\nlabels within a multimodal contrastive learning framework. We evaluate its\neffectiveness on two downstream tasks, land use classification and\nsocioeconomic status distribution mapping, demonstrating consistent performance\ngains of 4% to 11% over baseline methods. Additionally, we show that\nincorporating POI names enhances location retrieval, enabling models to capture\ncomplex urban concepts with greater precision. Ablation studies further reveal\nthe complementary role of POI names and the advantages of leveraging pretrained\ntext encoders for spatial representations. Overall, our findings highlight the\npotential of integrating fine-grained semantic attributes and multimodal\nlearning techniques to advance the development of urban foundation models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CaLLiPer+\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408POI\u540d\u79f0\u548c\u7c7b\u522b\u6807\u7b7e\u7684\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u57ce\u5e02\u5efa\u6a21\u4e2d\u7a7a\u95f4\u8868\u5f81\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7a7a\u95f4\u5d4c\u5165\u65b9\u6cd5\u8fc7\u4e8e\u4f9d\u8d56\u7a7a\u95f4\u90bb\u8fd1\u6027\uff0c\u5ffd\u7565\u4e86\u7ec6\u7c92\u5ea6\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u8868\u5f81\u65b9\u6cd5\u3002", "method": "\u6269\u5c55CaLLiPer\u6a21\u578b\uff0c\u5f15\u5165POI\u540d\u79f0\u548c\u7c7b\u522b\u6807\u7b7e\u7684\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5728\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd54%-11%\uff0c\u4e14POI\u540d\u79f0\u589e\u5f3a\u4e86\u4f4d\u7f6e\u68c0\u7d22\u80fd\u529b\u3002", "conclusion": "\u7ed3\u5408\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5c5e\u6027\u548c\u591a\u6a21\u6001\u5b66\u4e60\u6280\u672f\u6709\u52a9\u4e8e\u53d1\u5c55\u57ce\u5e02\u57fa\u7840\u6a21\u578b\u3002", "keywords": "\u7a7a\u95f4\u8868\u5f81,\u57ce\u5e02\u5efa\u6a21,POI,\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60"}}
{"id": "2506.02651", "pdf": "https://arxiv.org/pdf/2506.02651", "abs": "https://arxiv.org/abs/2506.02651", "authors": ["Luca Arnaboldi", "Bruno Loureiro", "Ludovic Stephan", "Florent Krzakala", "Lenka Zdeborova"], "title": "Asymptotics of SGD in Sequence-Single Index Models and Single-Layer Attention Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We study the dynamics of stochastic gradient descent (SGD) for a class of\nsequence models termed Sequence Single-Index (SSI) models, where the target\ndepends on a single direction in input space applied to a sequence of tokens.\nThis setting generalizes classical single-index models to the sequential\ndomain, encompassing simplified one-layer attention architectures. We derive a\nclosed-form expression for the population loss in terms of a pair of sufficient\nstatistics capturing semantic and positional alignment, and characterize the\ninduced high-dimensional SGD dynamics for these coordinates. Our analysis\nreveals two distinct training phases: escape from uninformative initialization\nand alignment with the target subspace, and demonstrates how the sequence\nlength and positional encoding influence convergence speed and learning\ntrajectories. These results provide a rigorous and interpretable foundation for\nunderstanding how sequential structure in data can be beneficial for learning\nwith attention-based models.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5e8f\u5217\u5355\u7d22\u5f15\uff08SSI\uff09\u6a21\u578b\u5728\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u4e0b\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u63ed\u793a\u4e86\u4e24\u4e2a\u8bad\u7ec3\u9636\u6bb5\u53ca\u5176\u5f71\u54cd\u56e0\u7d20\u3002", "motivation": "\u63a2\u7d22\u5e8f\u5217\u6570\u636e\u7ed3\u6784\u548c\u6ce8\u610f\u529b\u6a21\u578b\u5982\u4f55\u901a\u8fc7SGD\u6709\u6548\u5b66\u4e60\u3002", "method": "\u63a8\u5bfcSSI\u6a21\u578b\u7684\u7fa4\u4f53\u635f\u5931\u95ed\u5f0f\u89e3\uff0c\u5206\u6790\u9ad8\u7ef4SGD\u52a8\u6001\u3002", "result": "\u53d1\u73b0\u8bad\u7ec3\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff0c\u5e8f\u5217\u957f\u5ea6\u548c\u4f4d\u7f6e\u7f16\u7801\u5f71\u54cd\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u7ed3\u679c\u4e3a\u7406\u89e3\u5e8f\u5217\u7ed3\u6784\u5bf9\u6ce8\u610f\u529b\u6a21\u578b\u5b66\u4e60\u7684\u76ca\u5904\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "keywords": "\u968f\u673a\u68af\u5ea6\u4e0b\u964d, \u5e8f\u5217\u5355\u7d22\u5f15\u6a21\u578b, \u6ce8\u610f\u529b\u67b6\u6784, \u8bad\u7ec3\u52a8\u6001"}}
{"id": "2506.02746", "pdf": "https://arxiv.org/pdf/2506.02746", "abs": "https://arxiv.org/abs/2506.02746", "authors": ["Lin Xie", "Hanyi Li"], "title": "Solving the Pod Repositioning Problem with Deep Reinforced Adaptive Large Neighborhood Search", "categories": ["cs.RO", "cs.AI", "math.OC"], "comment": "14 pages, 2 figures, conference", "summary": "The Pod Repositioning Problem (PRP) in Robotic Mobile Fulfillment Systems\n(RMFS) involves selecting optimal storage locations for pods returning from\npick stations. This work presents an improved solution method that integrates\nAdaptive Large Neighborhood Search (ALNS) with Deep Reinforcement Learning\n(DRL). A DRL agent dynamically selects destroy and repair operators and adjusts\nkey parameters such as destruction degree and acceptance thresholds during the\nsearch. Specialized heuristics for both operators are designed to reflect\nPRP-specific characteristics, including pod usage frequency and movement costs.\nComputational results show that this DRL-guided ALNS outperforms traditional\napproaches such as cheapest-place, fixed-place, binary integer programming, and\nstatic heuristics. The method demonstrates strong solution quality and\nillustrating the benefit of learning-driven control within combinatorial\noptimization for warehouse systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u81ea\u9002\u5e94\u5927\u90bb\u57df\u641c\u7d22\uff08ALNS\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u673a\u5668\u4eba\u79fb\u52a8\u5c65\u884c\u7cfb\u7edf\u4e2d\u7684\u8d27\u67b6\u91cd\u65b0\u5b9a\u4f4d\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u79fb\u52a8\u5c65\u884c\u7cfb\u7edf\u4e2d\u8d27\u67b6\u91cd\u65b0\u5b9a\u4f4d\u95ee\u9898\uff08PRP\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574ALNS\u7684\u64cd\u4f5c\u548c\u53c2\u6570\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u7ed3\u5408DRL\u4e0eALNS\uff0c\u8bbe\u8ba1\u52a8\u6001\u9009\u62e9\u7834\u574f\u548c\u4fee\u590d\u64cd\u4f5c\u7684DRL\u4ee3\u7406\uff0c\u5e76\u6839\u636e\u8d27\u67b6\u4f7f\u7528\u9891\u7387\u548c\u79fb\u52a8\u6210\u672c\u8c03\u6574\u53c2\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u6700\u4fbf\u5b9c\u4f4d\u7f6e\u3001\u56fa\u5b9a\u4f4d\u7f6e\u7b49\uff09\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5b66\u4e60\u9a71\u52a8\u7684\u63a7\u5236\u5728\u4ed3\u5e93\u7cfb\u7edf\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u3002", "keywords": "\u8d27\u67b6\u91cd\u65b0\u5b9a\u4f4d\u95ee\u9898, \u673a\u5668\u4eba\u79fb\u52a8\u5c65\u884c\u7cfb\u7edf, \u81ea\u9002\u5e94\u5927\u90bb\u57df\u641c\u7d22, \u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60, \u7ec4\u5408\u4f18\u5316"}}
{"id": "2506.02657", "pdf": "https://arxiv.org/pdf/2506.02657", "abs": "https://arxiv.org/abs/2506.02657", "authors": ["Tam Ninh Thi-Thanh", "Trinh Van Chien", "Hung Tran", "Nguyen Hoai Son", "Van Nhan Vo"], "title": "Maximizing the Promptness of Metaverse Systems using Edge Computing by Deep Reinforcement Learning", "categories": ["cs.IT", "cs.LG", "math.IT"], "comment": "6 pages, 3 figures, and 2 tables. Published by IEEE at ATC2024", "summary": "Metaverse and Digital Twin (DT) have attracted much academic and industrial\nattraction to approach the future digital world. This paper introduces the\nadvantages of deep reinforcement learning (DRL) in assisting Metaverse\nsystem-based Digital Twin. In this system, we assume that it includes several\nMetaverse User devices collecting data from the real world to transfer it into\nthe virtual world, a Metaverse Virtual Access Point (MVAP) undertaking the\nprocessing of data, and an edge computing server that receives the offloading\ndata from the MVAP. The proposed model works under a dynamic environment with\nvarious parameters changing over time. The experiment results show that our\nproposed DRL algorithm is suitable for offloading tasks to ensure the\npromptness of DT in a dynamic environment.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u5728\u57fa\u4e8e\u5143\u5b87\u5b99\u7684\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9ad8\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u5378\u8f7d\u6548\u7387\u3002", "motivation": "\u5143\u5b87\u5b99\u548c\u6570\u5b57\u5b6a\u751f\u6280\u672f\u662f\u672a\u6765\u6570\u5b57\u4e16\u754c\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u4f46\u5982\u4f55\u5728\u9ad8\u52a8\u6001\u73af\u5883\u4e0b\u4fdd\u6301\u5176\u65f6\u6548\u6027\u662f\u4e9f\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7531\u7528\u6237\u8bbe\u5907\u3001\u865a\u62df\u63a5\u5165\u70b9\uff08MVAP\uff09\u548c\u8fb9\u7f18\u8ba1\u7b97\u670d\u52a1\u5668\u7ec4\u6210\u7684\u7cfb\u7edf\uff0c\u5b9e\u73b0\u52a8\u6001\u4efb\u52a1\u5378\u8f7d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684DRL\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u786e\u4fdd\u6570\u5b57\u5b6a\u751f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u65f6\u6548\u6027\u3002", "conclusion": "DRL\u7b97\u6cd5\u4e3a\u5143\u5b87\u5b99\u548c\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5728\u9ad8\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5143\u5b87\u5b99, \u6570\u5b57\u5b6a\u751f, \u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60, \u4efb\u52a1\u5378\u8f7d, \u52a8\u6001\u73af\u5883"}}
{"id": "2506.02664", "pdf": "https://arxiv.org/pdf/2506.02664", "abs": "https://arxiv.org/abs/2506.02664", "authors": ["Hugo Tabanelli", "Pierre Mergny", "Lenka Zdeborova", "Florent Krzakala"], "title": "Computational Thresholds in Multi-Modal Learning via the Spiked Matrix-Tensor Model", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.LG"], "comment": null, "summary": "We study the recovery of multiple high-dimensional signals from two noisy,\ncorrelated modalities: a spiked matrix and a spiked tensor sharing a common\nlow-rank structure. This setting generalizes classical spiked matrix and tensor\nmodels, unveiling intricate interactions between inference channels and\nsurprising algorithmic behaviors. Notably, while the spiked tensor model is\ntypically intractable at low signal-to-noise ratios, its correlation with the\nmatrix enables efficient recovery via Bayesian Approximate Message Passing,\ninducing staircase-like phase transitions reminiscent of neural network\nphenomena. In contrast, empirical risk minimization for joint learning fails:\nthe tensor component obstructs effective matrix recovery, and joint\noptimization significantly degrades performance, highlighting the limitations\nof naive multi-modal learning. We show that a simple Sequential Curriculum\nLearning strategy-first recovering the matrix, then leveraging it to guide\ntensor recovery-resolves this bottleneck and achieves optimal weak recovery\nthresholds. This strategy, implementable with spectral methods, emphasizes the\ncritical role of structural correlation and learning order in multi-modal\nhigh-dimensional inference.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u5982\u4f55\u4ece\u4e24\u4e2a\u566a\u58f0\u76f8\u5173\u6a21\u6001\uff08\u5c16\u5cf0\u77e9\u9635\u548c\u5c16\u5cf0\u5f20\u91cf\uff09\u4e2d\u6062\u590d\u591a\u4e2a\u9ad8\u7ef4\u4fe1\u53f7\uff0c\u63ed\u793a\u4e86\u6a21\u6001\u95f4\u4ea4\u4e92\u7684\u590d\u6742\u884c\u4e3a\u548c\u9ad8\u6548\u6062\u590d\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u591a\u6a21\u6001\u9ad8\u7ef4\u4fe1\u53f7\u6062\u590d\uff0c\u5c24\u5176\u662f\u5c16\u5cf0\u77e9\u9635\u548c\u5c16\u5cf0\u5f20\u91cf\u6a21\u6001\u7684\u4ea4\u4e92\u53ca\u5176\u5bf9\u6062\u590d\u6548\u7387\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u8fd1\u4f3c\u6d88\u606f\u4f20\u9012\uff08Bayesian Approximate Message Passing\uff09\u548c\u5e8f\u5217\u8bfe\u7a0b\u5b66\u4e60\uff08Sequential Curriculum Learning\uff09\u7b56\u7565\u3002", "result": "\u6a21\u6001\u95f4\u7684\u76f8\u5173\u6027\u5b9e\u73b0\u4e86\u9ad8\u6548\u6062\u590d\uff0c\u800c\u8054\u5408\u4f18\u5316\u53cd\u800c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff1b\u5e8f\u5217\u5b66\u4e60\u7b56\u7565\u8fbe\u5230\u6700\u4f18\u5f31\u6062\u590d\u9608\u503c\u3002", "conclusion": "\u7ed3\u6784\u76f8\u5173\u6027\u548c\u5b66\u4e60\u987a\u5e8f\u5728\u591a\u6a21\u6001\u9ad8\u7ef4\u63a8\u65ad\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5e8f\u5217\u5b66\u4e60\u7b56\u7565\u4f18\u4e8e\u8054\u5408\u4f18\u5316\u3002", "keywords": "\u9ad8\u7ef4\u4fe1\u53f7\u6062\u590d, \u591a\u6a21\u6001\u5b66\u4e60, \u5c16\u5cf0\u77e9\u9635, \u5c16\u5cf0\u5f20\u91cf, \u8d1d\u53f6\u65af\u8fd1\u4f3c\u6d88\u606f\u4f20\u9012, \u5e8f\u5217\u8bfe\u7a0b\u5b66\u4e60"}}
{"id": "2506.02764", "pdf": "https://arxiv.org/pdf/2506.02764", "abs": "https://arxiv.org/abs/2506.02764", "authors": ["Fatma Youssef Mohammed", "Kostas Alexis"], "title": "Unified Attention Modeling for Efficient Free-Viewing and Visual Search via Shared Representations", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to the 2025 IEEE International Conference on Development and\n  Learning (ICDL)", "summary": "Computational human attention modeling in free-viewing and task-specific\nsettings is often studied separately, with limited exploration of whether a\ncommon representation exists between them. This work investigates this question\nand proposes a neural network architecture that builds upon the Human Attention\ntransformer (HAT) to test the hypothesis. Our results demonstrate that\nfree-viewing and visual search can efficiently share a common representation,\nallowing a model trained in free-viewing attention to transfer its knowledge to\ntask-driven visual search with a performance drop of only 3.86% in the\npredicted fixation scanpaths, measured by the semantic sequence score (SemSS)\nmetric which reflects the similarity between predicted and human scanpaths.\nThis transfer reduces computational costs by 92.29% in terms of GFLOPs and\n31.23% in terms of trainable parameters.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u81ea\u7531\u89c2\u770b\u548c\u4efb\u52a1\u9a71\u52a8\u89c6\u89c9\u641c\u7d22\u662f\u5426\u5b58\u5728\u5171\u540c\u6ce8\u610f\u529b\u8868\u793a\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eHAT\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002\u7ed3\u679c\u8868\u660e\u4e24\u8005\u53ef\u5171\u4eab\u8868\u793a\uff0c\u6a21\u578b\u6027\u80fd\u4ec5\u4e0b\u964d3.86%\uff0c\u8ba1\u7b97\u6210\u672c\u5927\u5e45\u964d\u4f4e\u3002", "motivation": "\u7814\u7a76\u81ea\u7531\u89c2\u770b\u548c\u4efb\u52a1\u9a71\u52a8\u89c6\u89c9\u641c\u7d22\u662f\u5426\u5171\u4eab\u6ce8\u610f\u529b\u8868\u793a\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u6548\u7387\u548c\u901a\u7528\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eHuman Attention transformer\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u9a8c\u8bc1\u81ea\u7531\u89c2\u770b\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u5426\u8fc1\u79fb\u81f3\u4efb\u52a1\u9a71\u52a8\u573a\u666f\u3002", "result": "\u6a21\u578b\u8fc1\u79fb\u6027\u80fd\u4ec5\u4e0b\u964d3.86%\uff0c\u8ba1\u7b97\u6210\u672c\u51cf\u5c1192.29%\uff08GFLOPs\uff09\u548c31.23%\uff08\u53c2\u6570\u91cf\uff09\u3002", "conclusion": "\u81ea\u7531\u89c2\u770b\u548c\u4efb\u52a1\u9a71\u52a8\u89c6\u89c9\u641c\u7d22\u53ef\u5171\u4eab\u6ce8\u610f\u529b\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "keywords": "\u6ce8\u610f\u529b\u5efa\u6a21, HAT, \u89c6\u89c9\u641c\u7d22, \u8fc1\u79fb\u5b66\u4e60, \u8ba1\u7b97\u6548\u7387"}}
{"id": "2506.02685", "pdf": "https://arxiv.org/pdf/2506.02685", "abs": "https://arxiv.org/abs/2506.02685", "authors": ["Hohyun Kim", "Seunggeun Lee", "Min-hwan Oh"], "title": "Symmetry-Aware GFlowNets", "categories": ["stat.ML", "cs.LG"], "comment": "29 pages; Accepted at ICML 2025", "summary": "Generative Flow Networks (GFlowNets) offer a powerful framework for sampling\ngraphs in proportion to their rewards. However, existing approaches suffer from\nsystematic biases due to inaccuracies in state transition probability\ncomputations. These biases, rooted in the inherent symmetries of graphs, impact\nboth atom-based and fragment-based generation schemes. To address this\nchallenge, we introduce Symmetry-Aware GFlowNets (SA-GFN), a method that\nincorporates symmetry corrections into the learning process through reward\nscaling. By integrating bias correction directly into the reward structure,\nSA-GFN eliminates the need for explicit state transition computations.\nEmpirical results show that SA-GFN enables unbiased sampling while enhancing\ndiversity and consistently generating high-reward graphs that closely match the\ntarget distribution.", "AI": {"tldr": "\u63d0\u51faSA-GFN\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u79f0\u6027\u4fee\u6b63\u6539\u8fdbGFlowNets\u7684\u91c7\u6837\u504f\u5dee\u3002", "motivation": "\u73b0\u6709\u7684GFlowNets\u5728\u91c7\u6837\u56fe\u65f6\u56e0\u72b6\u6001\u8f6c\u79fb\u6982\u7387\u8ba1\u7b97\u4e0d\u51c6\u786e\u800c\u4ea7\u751f\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u5f15\u5165\u5bf9\u79f0\u6027\u4fee\u6b63\uff0c\u901a\u8fc7\u5956\u52b1\u7f29\u653e\u76f4\u63a5\u6574\u5408\u504f\u5dee\u6821\u6b63\uff0c\u907f\u514d\u663e\u5f0f\u72b6\u6001\u8f6c\u79fb\u8ba1\u7b97\u3002", "result": "SA-GFN\u5b9e\u73b0\u65e0\u504f\u91c7\u6837\uff0c\u63d0\u5347\u591a\u6837\u6027\uff0c\u751f\u6210\u7684\u9ad8\u5956\u52b1\u56fe\u4e0e\u76ee\u6807\u5206\u5e03\u9ad8\u5ea6\u5339\u914d\u3002", "conclusion": "SA-GFN\u6709\u6548\u89e3\u51b3\u4e86GFlowNets\u7684\u5bf9\u79f0\u6027\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u91c7\u6837\u6027\u80fd\u3002", "keywords": "GFlowNets, \u5bf9\u79f0\u6027, \u504f\u5dee\u6821\u6b63, \u5956\u52b1\u7f29\u653e, \u56fe\u91c7\u6837"}}
{"id": "2506.02785", "pdf": "https://arxiv.org/pdf/2506.02785", "abs": "https://arxiv.org/abs/2506.02785", "authors": ["Charalampos Kalalas", "Pavol Mulinka", "Guillermo Candela Belmonte", "Miguel Fornell", "Michail Dalgitsis", "Francisco Paredes Vera", "Javier Santaella S\u00e1nchez", "Carmen Vicente Villares", "Roshan Sedar", "Eftychia Datsika", "Angelos Antonopoulos", "Antonio Fern\u00e1ndez Ojea", "Miquel Payaro"], "title": "AI-Driven Vehicle Condition Monitoring with Cell-Aware Edge Service Migration", "categories": ["cs.NI", "cs.AI"], "comment": "6 pages, 8 figures", "summary": "Artificial intelligence (AI) has been increasingly applied to the condition\nmonitoring of vehicular equipment, aiming to enhance maintenance strategies,\nreduce costs, and improve safety. Leveraging the edge computing paradigm,\nAI-based condition monitoring systems process vast streams of vehicular data to\ndetect anomalies and optimize operational performance. In this work, we\nintroduce a novel vehicle condition monitoring service that enables real-time\ndiagnostics of a diverse set of anomalies while remaining practical for\ndeployment in real-world edge environments. To address mobility challenges, we\npropose a closed-loop service orchestration framework where service migration\nacross edge nodes is dynamically triggered by network-related metrics. Our\napproach has been implemented and tested in a real-world race circuit\nenvironment equipped with 5G network capabilities under diverse operational\nconditions. Experimental results demonstrate the effectiveness of our framework\nin ensuring low-latency AI inference and adaptive service placement,\nhighlighting its potential for intelligent transportation and mobility\napplications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u548c\u4eba\u5de5\u667a\u80fd\u7684\u8f66\u8f86\u72b6\u6001\u76d1\u6d4b\u670d\u52a1\uff0c\u901a\u8fc7\u52a8\u6001\u89e6\u53d1\u670d\u52a1\u8fc1\u79fb\u6765\u5e94\u5bf9\u7f51\u7edc\u53d8\u5316\uff0c\u5b9e\u73b0\u57285G\u73af\u5883\u4e0b\u7684\u4f4e\u5ef6\u8fdfAI\u63a8\u65ad\u3002", "motivation": "\u76ee\u7684\u662f\u5229\u7528AI\u548c\u8fb9\u7f18\u8ba1\u7b97\u63d0\u5347\u8f66\u8f86\u72b6\u6001\u76d1\u6d4b\u80fd\u529b\uff0c\u4f18\u5316\u7ef4\u62a4\u7b56\u7565\u3001\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u95ed\u73af\u670d\u52a1\u7f16\u6392\u6846\u67b6\uff0c\u52a8\u6001\u89e6\u53d1\u670d\u52a1\u8fc1\u79fb\u4ee5\u5e94\u5bf9\u7f51\u7edc\u6ce2\u52a8\uff0c\u5e76\u57285G\u8d5b\u8f66\u573a\u73af\u5883\u4e2d\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u6846\u67b6\u80fd\u6709\u6548\u5b9e\u73b0\u4f4e\u5ef6\u8fdfAI\u63a8\u65ad\u548c\u81ea\u9002\u5e94\u670d\u52a1\u90e8\u7f72\uff0c\u9002\u7528\u4e8e\u667a\u80fd\u4ea4\u901a\u548c\u79fb\u52a8\u5e94\u7528\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u5176\u5728\u667a\u80fd\u4ea4\u901a\u9886\u57df\u7684\u6f5c\u529b\u3002", "keywords": "\u4eba\u5de5\u667a\u80fd,\u8fb9\u7f18\u8ba1\u7b97,\u8f66\u8f86\u72b6\u6001\u76d1\u6d4b,\u670d\u52a1\u8fc1\u79fb,5G"}}
{"id": "2506.02710", "pdf": "https://arxiv.org/pdf/2506.02710", "abs": "https://arxiv.org/abs/2506.02710", "authors": ["T. N. Nisslbeck", "Wouter M. Kouw"], "title": "Online Bayesian system identification in multivariate autoregressive models via message passing", "categories": ["eess.SP", "cs.LG", "stat.ML"], "comment": "6 pages, 1 figure, conference: ECC2025", "summary": "We propose a recursive Bayesian estimation procedure for multivariate\nautoregressive models with exogenous inputs based on message passing in a\nfactor graph. Unlike recursive least-squares, our method produces full\nposterior distributions for both the autoregressive coefficients and noise\nprecision. The uncertainties regarding these estimates propagate into the\nuncertainties on predictions for future system outputs, and support online\nmodel evidence calculations. We demonstrate convergence empirically on a\nsynthetic autoregressive system and competitive performance on a double\nmass-spring-damper system.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u5b50\u56fe\u4e2d\u6d88\u606f\u4f20\u9012\u7684\u591a\u53d8\u91cf\u81ea\u56de\u5f52\u6a21\u578b\u9012\u5f52\u8d1d\u53f6\u65af\u4f30\u8ba1\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u5b8c\u6574\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u5e76\u652f\u6301\u5728\u7ebf\u6a21\u578b\u8bc1\u636e\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf\u7684\u6700\u5c0f\u4e8c\u4e58\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u5b8c\u6574\u7684\u540e\u9a8c\u5206\u5e03\u548c\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u8d1d\u53f6\u65af\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u56e0\u5b50\u56fe\u4e2d\u7684\u6d88\u606f\u4f20\u9012\u5b9e\u73b0\u9012\u5f52\u8d1d\u53f6\u65af\u4f30\u8ba1\uff0c\u9002\u7528\u4e8e\u591a\u53d8\u91cf\u81ea\u56de\u5f52\u6a21\u578b\u3002", "result": "\u65b9\u6cd5\u5728\u5408\u6210\u81ea\u56de\u5f52\u7cfb\u7edf\u548c\u53cc\u8d28\u91cf\u5f39\u7c27\u963b\u5c3c\u7cfb\u7edf\u4e0a\u8868\u73b0\u51fa\u6536\u655b\u6027\u548c\u7ade\u4e89\u6027\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u5e76\u652f\u6301\u5728\u7ebf\u6a21\u578b\u8bc4\u4f30\u3002", "keywords": "\u9012\u5f52\u8d1d\u53f6\u65af\u4f30\u8ba1, \u591a\u53d8\u91cf\u81ea\u56de\u5f52\u6a21\u578b, \u56e0\u5b50\u56fe, \u4e0d\u786e\u5b9a\u6027\u4f20\u64ad"}}
{"id": "2506.02787", "pdf": "https://arxiv.org/pdf/2506.02787", "abs": "https://arxiv.org/abs/2506.02787", "authors": ["Ruilong Wu", "Xinjiao Li", "Yisu Wang", "Xinyu Chen", "Dirk Kutscher"], "title": "Rethinking Dynamic Networks and Heterogeneous Computing with Automatic Parallelization", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Hybrid parallelism techniques are essential for efficiently training large\nlanguage models (LLMs). Nevertheless, current automatic parallel planning\nframeworks often overlook the simultaneous consideration of node heterogeneity\nand dynamic network topology changes, limiting their effectiveness in practical\napplications. In this paper, we address these limitations by modeling\nheterogeneous nodes within dynamically changing network environments and\nleveraging simulation-based strategies to determine optimal parallel\nconfigurations. Our approach enables fine-grained workload allocation tailored\nfor heterogeneous nodes and complex network scenarios, achieving performance\ncompetitive with state-of-the-art methods under regular and stable network\nconditions. Additionally, we introduce a strategy pruning technique to rapidly\ndiscard infeasible parallel configurations, substantially reducing the search\nspace and accelerating the search process through parallel execution within the\nsimulator. Preliminary evaluations confirm that our method notably enhances\ntraining performance on heterogeneous nodes and demonstrates improved\nadaptability in complex, dynamic scenarios such as cloud computing\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u8282\u70b9\u5f02\u6784\u6027\u548c\u52a8\u6001\u7f51\u7edc\u62d3\u6251\u7684\u6df7\u5408\u5e76\u884c\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eff\u771f\u4f18\u5316\u5e76\u884c\u914d\u7f6e\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u5e76\u884c\u89c4\u5212\u6846\u67b6\u5ffd\u89c6\u8282\u70b9\u5f02\u6784\u6027\u548c\u52a8\u6001\u7f51\u7edc\u53d8\u5316\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u6548\u679c\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5efa\u6a21\u5f02\u6784\u8282\u70b9\u548c\u52a8\u6001\u7f51\u7edc\u73af\u5883\uff0c\u5229\u7528\u57fa\u4e8e\u4eff\u771f\u7684\u7b56\u7565\u786e\u5b9a\u6700\u4f18\u5e76\u884c\u914d\u7f6e\uff0c\u5e76\u5f15\u5165\u7b56\u7565\u526a\u679d\u6280\u672f\u52a0\u901f\u641c\u7d22\u3002", "result": "\u5728\u5f02\u6784\u8282\u70b9\u548c\u590d\u6742\u7f51\u7edc\u573a\u666f\u4e0b\u5b9e\u73b0\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u5e76\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5f02\u6784\u8282\u70b9\u8bad\u7ec3\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u52a8\u6001\u573a\u666f\u7684\u9002\u5e94\u6027\u3002", "keywords": "\u6df7\u5408\u5e76\u884c,\u5f02\u6784\u8282\u70b9,\u52a8\u6001\u7f51\u7edc,\u4eff\u771f\u4f18\u5316,\u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.02791", "pdf": "https://arxiv.org/pdf/2506.02791", "abs": "https://arxiv.org/abs/2506.02791", "authors": ["Zhen Yang", "Hongyi Lin", "Yifan He", "Jie Xu", "Zeyu Sun", "Shuo Liu", "Pengpeng Wang", "Zhongxing Yu", "Qingyuan Liang"], "title": "Rethinking the effects of data contamination in Code Intelligence", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "In recent years, code intelligence has gained increasing importance in the\nfield of automated software engineering. Meanwhile, the widespread adoption of\nPretrained Language Models (PLMs) and Large Language Models (LLMs) has raised\nconcerns regarding data contamination and its potential impact on model\nperformance evaluation. This paper presents a systematic empirical study to\ninvestigate the fine-grained data contamination on code intelligence tasks. Our\nstudy involves diverse representative PLMs, namely RoBERTa and GPT-2, and LLMs,\nnamely LLaMA and StarCoder, covering three major tasks: code translation, code\ngeneration, and code summarization. We categorize contamination scenarios into\nfour types according to the code intelligence practice, namely input-only,\noutput-only, unpaired, and paired contamination settings, and construct\ncorresponding experimental and control groups for exploration.\n  Experimental results show that, under the pre-training, fine-tuning, and\ninference paradigm adopted by PLMs, even deliberately injecting paired\ncontamination does not lead to significant performance overestimation. But\ndirect inference or small-scale fine-tuning uncovers the contamination effects.\nIn contrast, LLMs with pre-training and inference paradigm are significantly\naffected by the paired contamination. Apart from the above, other contamination\nscenarios have no impact on both PLMs and LLMs. Our findings challenge the\nconventional belief that contamination inevitably leads to performance\noverestimation, providing new insights into the evaluation and deployment of\ncode intelligence models.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u63a2\u8ba8\u4e86\u4ee3\u7801\u667a\u80fd\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u6c61\u67d3\u95ee\u9898\uff0c\u53d1\u73b0\u6570\u636e\u6c61\u67d3\u5bf9\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u8bc4\u4f30\u5f71\u54cd\u6709\u9650\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u89c2\u5ff5\u3002", "motivation": "\u7814\u7a76\u6570\u636e\u6c61\u67d3\u5bf9\u4ee3\u7801\u667a\u80fd\u4efb\u52a1\u4e2d\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002", "method": "\u5b9e\u9a8c\u6d89\u53caRoBERTa\u3001GPT-2\u3001LLaMA\u548cStarCoder\uff0c\u8986\u76d6\u4ee3\u7801\u7ffb\u8bd1\u3001\u751f\u6210\u548c\u6458\u8981\u4efb\u52a1\uff0c\u5bf9\u6bd4\u56db\u79cd\u6c61\u67d3\u573a\u666f\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cPLMs\u5728\u914d\u5bf9\u6c61\u67d3\u4e0b\u6027\u80fd\u65e0\u660e\u663e\u63d0\u5347\uff0c\u800cLLMs\u53d7\u5f71\u54cd\u663e\u8457\u3002\u5176\u4ed6\u6c61\u67d3\u573a\u666f\u5bf9\u4e24\u8005\u5747\u65e0\u5f71\u54cd\u3002", "conclusion": "\u6570\u636e\u6c61\u67d3\u4e0d\u5fc5\u7136\u5bfc\u81f4\u6027\u80fd\u9ad8\u4f30\uff0c\u4e3a\u4ee3\u7801\u667a\u80fd\u6a21\u578b\u7684\u8bc4\u4f30\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "keywords": "\u4ee3\u7801\u667a\u80fd, \u6570\u636e\u6c61\u67d3, \u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b, \u5927\u8bed\u8a00\u6a21\u578b, \u6027\u80fd\u8bc4\u4f30"}}
{"id": "2506.02754", "pdf": "https://arxiv.org/pdf/2506.02754", "abs": "https://arxiv.org/abs/2506.02754", "authors": ["Luc Brogat-Motte", "Alessandro Rudi", "Riccardo Bonalli"], "title": "Safely Learning Controlled Stochastic Dynamics", "categories": ["stat.ML", "cs.LG"], "comment": "Under review at NeurIPS 2025", "summary": "We address the problem of safely learning controlled stochastic dynamics from\ndiscrete-time trajectory observations, ensuring system trajectories remain\nwithin predefined safe regions during both training and deployment.\nSafety-critical constraints of this kind are crucial in applications such as\nautonomous robotics, finance, and biomedicine. We introduce a method that\nensures safe exploration and efficient estimation of system dynamics by\niteratively expanding an initial known safe control set using kernel-based\nconfidence bounds. After training, the learned model enables predictions of the\nsystem's dynamics and permits safety verification of any given control. Our\napproach requires only mild smoothness assumptions and access to an initial\nsafe control set, enabling broad applicability to complex real-world systems.\nWe provide theoretical guarantees for safety and derive adaptive learning rates\nthat improve with increasing Sobolev regularity of the true dynamics.\nExperimental evaluations demonstrate the practical effectiveness of our method\nin terms of safety, estimation accuracy, and computational efficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u79bb\u6563\u65f6\u95f4\u8f68\u8ff9\u89c2\u6d4b\u4e2d\u5b89\u5168\u5b66\u4e60\u53d7\u63a7\u968f\u673a\u52a8\u529b\u5b66\u7684\u65b9\u6cd5\uff0c\u786e\u4fdd\u8bad\u7ec3\u548c\u90e8\u7f72\u671f\u95f4\u7cfb\u7edf\u8f68\u8ff9\u59cb\u7ec8\u5728\u9884\u5b9a\u5b89\u5168\u533a\u57df\u5185\u3002", "motivation": "\u5728\u81ea\u4e3b\u673a\u5668\u4eba\u3001\u91d1\u878d\u548c\u751f\u7269\u533b\u5b66\u7b49\u5e94\u7528\u4e2d\uff0c\u786e\u4fdd\u7cfb\u7edf\u8f68\u8ff9\u5728\u5b89\u5168\u533a\u57df\u5185\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u6838\u7684\u7f6e\u4fe1\u8fb9\u754c\u8fed\u4ee3\u6269\u5c55\u521d\u59cb\u5df2\u77e5\u5b89\u5168\u63a7\u5236\u96c6\uff0c\u5b9e\u73b0\u5b89\u5168\u63a2\u7d22\u548c\u9ad8\u6548\u4f30\u8ba1\u7cfb\u7edf\u52a8\u529b\u5b66\u3002", "result": "\u5b66\u4e60\u540e\u7684\u6a21\u578b\u80fd\u9884\u6d4b\u7cfb\u7edf\u52a8\u6001\u5e76\u9a8c\u8bc1\u4efb\u4f55\u7ed9\u5b9a\u63a7\u5236\u7684\u5b89\u5168\u6027\u3002\u7406\u8bba\u4fdd\u8bc1\u5b89\u5168\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u5b9e\u9645\u7cfb\u7edf\u4e2d\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4e14\u5b9e\u9a8c\u8bc1\u5b9e\u5176\u5728\u5b89\u5168\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "keywords": "\u5b89\u5168\u5b66\u4e60, \u968f\u673a\u52a8\u529b\u5b66, \u6838\u65b9\u6cd5, \u81ea\u9002\u5e94\u5b66\u4e60"}}
{"id": "2506.02794", "pdf": "https://arxiv.org/pdf/2506.02794", "abs": "https://arxiv.org/abs/2506.02794", "authors": ["Mijeong Kim", "Gunhee Kim", "Jungyoon Choi", "Wonjae Roh", "Bohyung Han"], "title": "PhysGaia: A Physics-Aware Dataset of Multi-Body Interactions for Dynamic Novel View Synthesis", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Project page: http://cvlab.snu.ac.kr/research/PhysGaia, Data:\n  https://huggingface.co/datasets/mijeongkim/PhysGaia/tree/main", "summary": "We introduce PhysGaia, a novel physics-aware dataset specifically designed\nfor Dynamic Novel View Synthesis (DyNVS), encompassing both structured objects\nand unstructured physical phenomena. Unlike existing datasets that primarily\nfocus on photorealistic reconstruction, PhysGaia is created to actively support\nphysics-aware dynamic scene modeling. Our dataset provides complex dynamic\nscenarios with rich interactions among multiple objects, where they\nrealistically collide with each other and exchange forces. Furthermore, it\ncontains a diverse range of physical materials, such as liquid, gas,\nviscoelastic substance, and textile, which moves beyond the rigid bodies\nprevalent in existing datasets. All scenes in PhysGaia are faithfully generated\nto strictly adhere to physical laws, leveraging carefully selected\nmaterial-specific physics solvers. To enable quantitative evaluation of\nphysical modeling, our dataset provides essential ground-truth information,\nincluding 3D particle trajectories and physics parameters, e.g., viscosity. To\nfacilitate research adoption, we also provide essential integration pipelines\nfor using state-of-the-art DyNVS models with our dataset and report their\nresults. By addressing the critical lack of datasets for physics-aware\nmodeling, PhysGaia will significantly advance research in dynamic view\nsynthesis, physics-based scene understanding, and deep learning models\nintegrated with physical simulation -- ultimately enabling more faithful\nreconstruction and interpretation of complex dynamic scenes. Our datasets and\ncodes are available in the project website,\nhttp://cvlab.snu.ac.kr/research/PhysGaia.", "AI": {"tldr": "PhysGaia\u662f\u4e00\u4e2a\u4e13\u4e3a\u52a8\u6001\u65b0\u89c6\u89d2\u5408\u6210\uff08DyNVS\uff09\u8bbe\u8ba1\u7684\u7269\u7406\u611f\u77e5\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u7ed3\u6784\u5316\u7269\u4f53\u548c\u975e\u7ed3\u6784\u5316\u7269\u7406\u73b0\u8c61\uff0c\u652f\u6301\u590d\u6742\u7684\u52a8\u6001\u573a\u666f\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u591a\u5173\u6ce8\u4e8e\u903c\u771f\u91cd\u5efa\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7269\u7406\u73b0\u8c61\u7684\u5efa\u6a21\u652f\u6301\uff0cPhysGaia\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8\u52a8\u6001\u89c6\u56fe\u5408\u6210\u548c\u7269\u7406\u573a\u666f\u7406\u89e3\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u7cbe\u5fc3\u9009\u62e9\u7684\u6750\u8d28\u7279\u5b9a\u7269\u7406\u6c42\u89e3\u5668\u751f\u6210\u4e25\u683c\u9075\u5faa\u7269\u7406\u5b9a\u5f8b\u7684\u573a\u666f\uff0c\u63d0\u4f9b3D\u7c92\u5b50\u8f68\u8ff9\u548c\u7269\u7406\u53c2\u6570\u7b49\u771f\u5b9e\u6570\u636e\uff0c\u5e76\u4e0e\u73b0\u6709DyNVS\u6a21\u578b\u96c6\u6210\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b\u591a\u6837\u5316\u7684\u7269\u7406\u6750\u6599\u548c\u590d\u6742\u52a8\u6001\u4ea4\u4e92\u573a\u666f\uff0c\u4e3a\u7269\u7406\u5efa\u6a21\u63d0\u4f9b\u91cf\u5316\u8bc4\u4f30\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u4e0e\u524d\u6cbfDyNVS\u6a21\u578b\u7684\u7ed3\u5408\u6548\u679c\u3002", "conclusion": "PhysGaia\u586b\u8865\u4e86\u7269\u7406\u611f\u77e5\u5efa\u6a21\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u5c06\u63a8\u52a8\u52a8\u6001\u89c6\u56fe\u5408\u6210\u3001\u7269\u7406\u573a\u666f\u7406\u89e3\u548c\u6df1\u5ea6\u5b66\u4e60\u4e0e\u7269\u7406\u4eff\u771f\u7684\u7ed3\u5408\u7814\u7a76\u3002", "keywords": "\u7269\u7406\u611f\u77e5\u3001\u52a8\u6001\u65b0\u89c6\u89d2\u5408\u6210\u3001\u6570\u636e\u96c6\u3001\u7269\u7406\u6a21\u62df\u3001\u573a\u666f\u5efa\u6a21"}}
{"id": "2506.02793", "pdf": "https://arxiv.org/pdf/2506.02793", "abs": "https://arxiv.org/abs/2506.02793", "authors": ["Houssam Zenati", "Bariscan Bozkurt", "Arthur Gretton"], "title": "Doubly-Robust Estimation of Counterfactual Policy Mean Embeddings", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Estimating the distribution of outcomes under counterfactual policies is\ncritical for decision-making in domains such as recommendation, advertising,\nand healthcare. We analyze a novel framework-Counterfactual Policy Mean\nEmbedding (CPME)-that represents the entire counterfactual outcome distribution\nin a reproducing kernel Hilbert space (RKHS), enabling flexible and\nnonparametric distributional off-policy evaluation. We introduce both a plug-in\nestimator and a doubly robust estimator; the latter enjoys improved uniform\nconvergence rates by correcting for bias in both the outcome embedding and\npropensity models. Building on this, we develop a doubly robust kernel test\nstatistic for hypothesis testing, which achieves asymptotic normality and thus\nenables computationally efficient testing and straightforward construction of\nconfidence intervals. Our framework also supports sampling from the\ncounterfactual distribution. Numerical simulations illustrate the practical\nbenefits of CPME over existing methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCPME\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4f30\u8ba1\u53cd\u4e8b\u5b9e\u653f\u7b56\u4e0b\u7ed3\u679c\u7684\u5206\u5e03\uff0c\u901a\u8fc7RKHS\u8868\u793a\u5206\u5e03\uff0c\u652f\u6301\u7075\u6d3b\u7684\u975e\u53c2\u6570\u8bc4\u4f30\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u4f30\u8ba1\u5668\u3002", "motivation": "\u5728\u63a8\u8350\u3001\u5e7f\u544a\u548c\u533b\u7597\u7b49\u9886\u57df\uff0c\u4f30\u8ba1\u53cd\u4e8b\u5b9e\u653f\u7b56\u4e0b\u7684\u7ed3\u679c\u5206\u5e03\u5bf9\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528RKHS\u8868\u793a\u53cd\u4e8b\u5b9e\u7ed3\u679c\u5206\u5e03\uff0c\u63d0\u51fa\u63d2\u4ef6\u4f30\u8ba1\u5668\u548c\u53cc\u91cd\u7a33\u5065\u4f30\u8ba1\u5668\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u4e00\u79cd\u53cc\u91cd\u7a33\u5065\u6838\u6d4b\u8bd5\u7edf\u8ba1\u91cf\u3002", "result": "\u53cc\u91cd\u7a33\u5065\u4f30\u8ba1\u5668\u5728\u6536\u655b\u901f\u5ea6\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4e14\u6838\u6d4b\u8bd5\u7edf\u8ba1\u91cf\u652f\u6301\u9ad8\u6548\u8ba1\u7b97\u548c\u7f6e\u4fe1\u533a\u95f4\u6784\u5efa\u3002", "conclusion": "CPME\u6846\u67b6\u5728\u6570\u503c\u6a21\u62df\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "keywords": "\u53cd\u4e8b\u5b9e\u653f\u7b56\u8bc4\u4f30, RKHS, \u53cc\u91cd\u7a33\u5065\u4f30\u8ba1\u5668, \u6838\u6d4b\u8bd5\u7edf\u8ba1\u91cf"}}
{"id": "2506.02796", "pdf": "https://arxiv.org/pdf/2506.02796", "abs": "https://arxiv.org/abs/2506.02796", "authors": ["Haoyuan Wang", "Chen Liu", "Minh-Ngoc Tran", "Chao Wang"], "title": "Deep Learning Enhanced Multivariate GARCH", "categories": ["q-fin.CP", "cs.AI", "econ.EM"], "comment": null, "summary": "This paper introduces a novel multivariate volatility modeling framework,\nnamed Long Short-Term Memory enhanced BEKK (LSTM-BEKK), that integrates deep\nlearning into multivariate GARCH processes. By combining the flexibility of\nrecurrent neural networks with the econometric structure of BEKK models, our\napproach is designed to better capture nonlinear, dynamic, and high-dimensional\ndependence structures in financial return data. The proposed model addresses\nkey limitations of traditional multivariate GARCH-based methods, particularly\nin capturing persistent volatility clustering and asymmetric co-movement across\nassets. Leveraging the data-driven nature of LSTMs, the framework adapts\neffectively to time-varying market conditions, offering improved robustness and\nforecasting performance. Empirical results across multiple equity markets\nconfirm that the LSTM-BEKK model achieves superior performance in terms of\nout-of-sample portfolio risk forecast, while maintaining the interpretability\nfrom the BEKK models. These findings highlight the potential of hybrid\neconometric-deep learning models in advancing financial risk management and\nmultivariate volatility forecasting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u7684\u591a\u53d8\u91cf\u6ce2\u52a8\u7387\u5efa\u6a21\u6846\u67b6LSTM-BEKK\uff0c\u901a\u8fc7\u5c06LSTM\u7684\u7075\u6d3b\u6027\u4e0eBEKK\u6a21\u578b\u7684\u8ba1\u91cf\u7ed3\u6784\u7ed3\u5408\uff0c\u6709\u6548\u6355\u6349\u91d1\u878d\u6570\u636e\u4e2d\u7684\u975e\u7ebf\u6027\u52a8\u6001\u9ad8\u7ef4\u4f9d\u8d56\u7ed3\u6784\u3002", "motivation": "\u4f20\u7edf\u591a\u53d8\u91cfGARCH\u65b9\u6cd5\u5728\u6355\u6349\u6301\u7eed\u6027\u6ce2\u52a8\u96c6\u7fa4\u548c\u8d44\u4ea7\u95f4\u975e\u5bf9\u79f0\u8054\u52a8\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u672c\u6587\u65e8\u5728\u6539\u8fdb\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408LSTM\u548cBEKK\u6a21\u578b\uff0c\u6784\u5efaLSTM-BEKK\u6846\u67b6\uff0c\u5229\u7528\u6570\u636e\u9a71\u52a8\u7684LSTM\u9002\u5e94\u65f6\u53d8\u5e02\u573a\u6761\u4ef6\u3002", "result": "LSTM-BEKK\u5728\u591a\u4e2a\u80a1\u7968\u5e02\u573a\u4e2d\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u6837\u672c\u5916\u7ec4\u5408\u98ce\u9669\u9884\u6d4b\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301BEKK\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u6df7\u5408\u8ba1\u91cf-\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u91d1\u878d\u98ce\u9669\u7ba1\u7406\u548c\u591a\u53d8\u91cf\u6ce2\u52a8\u7387\u9884\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "keywords": "LSTM-BEKK, \u591a\u53d8\u91cf\u6ce2\u52a8\u7387\u5efa\u6a21, \u91d1\u878d\u98ce\u9669\u7ba1\u7406, \u6df1\u5ea6\u5b66\u4e60, GARCH"}}
{"id": "2506.02802", "pdf": "https://arxiv.org/pdf/2506.02802", "abs": "https://arxiv.org/abs/2506.02802", "authors": ["Andr\u00e1s Strausz", "Niels Pardon", "Ioana Giurgiu"], "title": "A Learned Cost Model-based Cross-engine Optimizer for SQL Workloads", "categories": ["cs.DB", "cs.LG"], "comment": "6 pages", "summary": "Lakehouse systems enable the same data to be queried with multiple execution\nengines. However, selecting the engine best suited to run a SQL query still\nrequires a priori knowledge of the query computational requirements and an\nengine capability, a complex and manual task that only becomes more difficult\nwith the emergence of new engines and workloads. In this paper, we address this\nlimitation by proposing a cross-engine optimizer that can automate engine\nselection for diverse SQL queries through a learned cost model. Optimized with\nhints, a query plan is used for query cost prediction and routing. Cost\nprediction is formulated as a multi-task learning problem, and multiple\npredictor heads, corresponding to different engines and provisionings, are used\nin the model architecture. This eliminates the need to train engine-specific\nmodels and allows the flexible addition of new engines at a minimal fine-tuning\ncost. Results on various databases and engines show that using a query\noptimized logical plan for cost estimation decreases the average Q-error by\neven 12.6% over using unoptimized plans as input. Moreover, the proposed\ncross-engine optimizer reduces the total workload runtime by up to 25.2% in a\nzero-shot setting and 30.4% in a few-shot setting when compared to random\nrouting.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8de8\u5f15\u64ce\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u6210\u672c\u6a21\u578b\u81ea\u52a8\u9009\u62e9\u9002\u5408\u4e0d\u540cSQL\u67e5\u8be2\u7684\u6267\u884c\u5f15\u64ce\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u548c\u6210\u672c\u3002", "motivation": "Lakehouse\u7cfb\u7edf\u4e2d\u9700\u8981\u4e3a\u67e5\u8be2\u9009\u62e9\u6700\u4f73\u6267\u884c\u5f15\u64ce\uff0c\u4f46\u76ee\u524d\u4f9d\u8d56\u4eba\u5de5\u8bc4\u4f30\uff0c\u96be\u4ee5\u5e94\u5bf9\u65b0\u5f15\u64ce\u548c\u8d1f\u8f7d\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u67e5\u8be2\u6210\u672c\uff0c\u7ed3\u5408\u67e5\u8be2\u8ba1\u5212\u4f18\u5316\u5f15\u64ce\u9009\u62e9\u3002", "result": "\u4f18\u5316\u540e\u7684\u65b9\u6cd5\u6bd4\u968f\u673a\u9009\u62e9\u5f15\u64ce\u51cf\u5c11\u603b\u8fd0\u884c\u65f6\u95f4\u9ad8\u8fbe30.4%\u3002", "conclusion": "\u8de8\u5f15\u64ce\u4f18\u5316\u5668\u80fd\u9ad8\u6548\u81ea\u52a8\u5316\u5f15\u64ce\u9009\u62e9\uff0c\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u4eba\u5de5\u6210\u672c\u3002", "keywords": "Lakehouse, SQL\u67e5\u8be2, \u8de8\u5f15\u64ce\u4f18\u5316, \u6210\u672c\u6a21\u578b, \u591a\u4efb\u52a1\u5b66\u4e60"}}
{"id": "2506.02839", "pdf": "https://arxiv.org/pdf/2506.02839", "abs": "https://arxiv.org/abs/2506.02839", "authors": ["Yougang Lyu", "Xiaoyu Zhang", "Lingyong Yan", "Maarten de Rijke", "Zhaochun Ren", "Xiuying Chen"], "title": "DeepShop: A Benchmark for Deep Research Shopping Agents", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Web agents for online shopping have shown great promise in automating user\ninteractions across e-commerce platforms. Benchmarks for assessing such agents\ndo not reflect the complexity of real-world shopping scenarios, as they often\nconsist of overly simple queries with deterministic paths, such as \"Find iPhone\n15.\" Real shopping scenarios are inherently more layered, involving\nmulti-dimensional product attributes, search filters, and user-specific sorting\npreferences. To address this gap, we introduce DeepShop, a benchmark designed\nto evaluate web agents in complex and realistic online shopping environments.\nDeepShop comprises three key components. (1) Query diversity evolution:\nStarting from real user queries, we generate diverse queries across five\npopular online shopping domains. (2) Query complexity evolution: We further\nevolve these queries to increase complexity, considering product attributes,\nsearch filters, and sorting preferences, and classify them into three levels:\neasy, medium, and hard, based on the number of evolutions. (3) Fine-grained and\nholistic evaluation: We propose an automated evaluation framework that assesses\nagent performance in terms of fine-grained aspects (product attributes, search\nfilters, and sorting preferences) and reports the overall success rate through\nholistic evaluation. We conduct a systematic evaluation of retrieval-augmented\ngeneration (RAG) methods, web agents, and deep research systems. Results show\nthat RAG struggles with complex queries due to its lack of web interaction,\nwhile other methods face significant challenges with filters and sorting\npreferences, leading to low overall success rates. We also perform\ncross-category, complexity-based evaluations and error analyses to support the\nadvancement of deep research shopping agents.", "AI": {"tldr": "DeepShop\u662f\u4e00\u4e2a\u8bc4\u4f30\u590d\u6742\u5728\u7ebf\u8d2d\u7269\u73af\u5883\u4e2d\u7f51\u7edc\u4ee3\u7406\u6027\u80fd\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u591a\u6837\u5316\u67e5\u8be2\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u8d2d\u7269\u573a\u666f\u7684\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u4ee5\u6539\u8fdb\u7f51\u7edc\u4ee3\u7406\u7684\u6027\u80fd\u3002", "method": "DeepShop\u5305\u542b\u67e5\u8be2\u591a\u6837\u5316\u589e\u5f3a\u3001\u590d\u6742\u5ea6\u63d0\u5347\u53ca\u7ec6\u7c92\u5ea6\u4e0e\u6574\u4f53\u8bc4\u4f30\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u901a\u8fc7\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\u6d4b\u8bd5\u4e0d\u540c\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "RAG\u65b9\u6cd5\u5728\u590d\u6742\u67e5\u8be2\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5176\u4ed6\u65b9\u6cd5\u5728\u8fc7\u6ee4\u548c\u6392\u5e8f\u504f\u597d\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u6574\u4f53\u6210\u529f\u7387\u8f83\u4f4e\u3002", "conclusion": "DeepShop\u4e3a\u672a\u6765\u6df1\u5ea6\u7814\u7a76\u8d2d\u7269\u4ee3\u7406\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "keywords": "DeepShop, \u7f51\u7edc\u4ee3\u7406, \u5728\u7ebf\u8d2d\u7269, \u57fa\u51c6\u8bc4\u4f30, \u590d\u6742\u67e5\u8be2, RAG"}}
{"id": "2506.02825", "pdf": "https://arxiv.org/pdf/2506.02825", "abs": "https://arxiv.org/abs/2506.02825", "authors": ["Tong Qi", "Vera Andersson", "Peter Viechnicki", "Vince Lyzinski"], "title": "Asymptotically perfect seeded graph matching without edge correlation (and applications to inference)", "categories": ["stat.ML", "cs.LG"], "comment": "10 figures, 35 pages", "summary": "We present the OmniMatch algorithm for seeded multiple graph matching. In the\nsetting of $d$-dimensional Random Dot Product Graphs (RDPG), we prove that\nunder mild assumptions, OmniMatch with $s$ seeds asymptotically and efficiently\nperfectly aligns $O(s^{\\alpha})$ unseeded vertices -- for $\\alpha<2\\wedge d/4$\n-- across multiple networks even in the presence of no edge correlation. We\ndemonstrate the effectiveness of our algorithm across numerous simulations and\nin the context of shuffled graph hypothesis testing. In the shuffled testing\nsetting, testing power is lost due to the misalignment/shuffling of vertices\nacross graphs, and we demonstrate the capacity of OmniMatch to correct for\nmisaligned vertices prior to testing and hence recover the lost testing power.\nWe further demonstrate the algorithm on a pair of data examples from\nconnectomics and machine translation.", "AI": {"tldr": "OmniMatch\u7b97\u6cd5\u7528\u4e8e\u79cd\u5b50\u5316\u591a\u56fe\u5339\u914d\uff0c\u5728RDPG\u6a21\u578b\u4e2d\u8bc1\u660e\u80fd\u9ad8\u6548\u5bf9\u9f50\u5927\u91cf\u672a\u79cd\u5b50\u9876\u70b9\uff0c\u5e76\u5728\u6a21\u62df\u548c\u5b9e\u9645\u6570\u636e\u4e2d\u5c55\u793a\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u56fe\u5339\u914d\u4e2d\u9876\u70b9\u5bf9\u9f50\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6d4b\u8bd5\u4e2d\u7531\u4e8e\u9876\u70b9\u9519\u4f4d\u5bfc\u81f4\u7684\u6d4b\u8bd5\u80fd\u529b\u635f\u5931\u3002", "method": "\u63d0\u51faOmniMatch\u7b97\u6cd5\uff0c\u5229\u7528\u79cd\u5b50\u9876\u70b9\u9010\u6b65\u5bf9\u9f50\u672a\u79cd\u5b50\u9876\u70b9\uff0c\u9002\u7528\u4e8e\u968f\u673a\u70b9\u79ef\u56fe\u6a21\u578b\u3002", "result": "\u7b97\u6cd5\u5728\u6a21\u62df\u548c\u5b9e\u9645\u6570\u636e\u4e2d\u6709\u6548\u5bf9\u9f50\u9876\u70b9\uff0c\u6062\u590d\u6d4b\u8bd5\u80fd\u529b\u3002", "conclusion": "OmniMatch\u5728\u591a\u56fe\u5339\u914d\u548c\u6570\u636e\u5bf9\u9f50\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "keywords": "\u591a\u56fe\u5339\u914d, \u968f\u673a\u70b9\u79ef\u56fe, \u9876\u70b9\u5bf9\u9f50, \u6d4b\u8bd5\u80fd\u529b\u6062\u590d, \u6570\u636e\u8fde\u63a5"}}
{"id": "2506.02858", "pdf": "https://arxiv.org/pdf/2506.02858", "abs": "https://arxiv.org/abs/2506.02858", "authors": ["Geonyoung Lee", "Geonhee Han", "Paul Hongsuck Seo"], "title": "DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization", "categories": ["cs.SD", "cs.AI"], "comment": "Interspeech 2025", "summary": "Language-queried Audio Source Separation (LASS) enables open-vocabulary sound\nseparation via natural language queries. While existing methods rely on\ntask-specific training, we explore whether pretrained diffusion models,\noriginally designed for audio generation, can inherently perform separation\nwithout further training. In this study, we introduce a training-free framework\nleveraging generative priors for zero-shot LASS. Analyzing na\\\"ive adaptations,\nwe identify key limitations arising from modality-specific challenges.To\naddress these issues, we propose Diffusion-Guided Mask Optimization (DGMO), a\ntest-time optimization framework that refines spectrogram masks for precise,\ninput-aligned separation. Our approach effectively repurposes pretrained\ndiffusion models for source separation, achieving competitive performance\nwithout task-specific supervision. This work expands the application of\ndiffusion models beyond generation, establishing a new paradigm for zero-shot\naudio separation. The code is available at: https://wltschmrz.github.io/DGMO/", "AI": {"tldr": "\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u8bed\u8a00\u67e5\u8be2\u97f3\u9891\u5206\u79bb\u65b9\u6cd5\uff0c\u5229\u7528\u751f\u6210\u5148\u9a8c\u5b9e\u73b0\u9ad8\u6548\u5206\u79bb\u3002", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u662f\u5426\u5929\u751f\u5177\u5907\u97f3\u9891\u5206\u79bb\u80fd\u529b\uff0c\u907f\u514d\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u6269\u6563\u5f15\u5bfc\u63a9\u7801\u4f18\u5316\uff08DGMO\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u95f4\u4f18\u5316\u9891\u8c31\u63a9\u7801\u5b9e\u73b0\u7cbe\u786e\u5206\u79bb\u3002", "result": "\u65e0\u9700\u76d1\u7763\uff0c\u8868\u73b0\u4f18\u5f02\uff0c\u6269\u5c55\u4e86\u6269\u6563\u6a21\u578b\u7684\u5e94\u7528\u8303\u56f4\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u53ef\u7528\u4e8e\u96f6\u6837\u672c\u97f3\u9891\u5206\u79bb\uff0c\u5f00\u8f9f\u4e86\u65b0\u8303\u5f0f\u3002", "keywords": "\u96f6\u6837\u672c\u5b66\u4e60,\u97f3\u9891\u5206\u79bb,\u6269\u6563\u6a21\u578b,\u81ea\u7136\u8bed\u8a00\u67e5\u8be2"}}
{"id": "2506.02841", "pdf": "https://arxiv.org/pdf/2506.02841", "abs": "https://arxiv.org/abs/2506.02841", "authors": ["Tom Danino", "Nahum Shimkin"], "title": "Ensemble-MIX: Enhancing Sample Efficiency in Multi-Agent RL Using Ensemble Methods", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "Multi-agent reinforcement learning (MARL) methods have achieved\nstate-of-the-art results on a range of multi-agent tasks. Yet, MARL algorithms\ntypically require significantly more environment interactions than their\nsingle-agent counterparts to converge, a problem exacerbated by the difficulty\nin exploring over a large joint action space and the high variance intrinsic to\nMARL environments. To tackle these issues, we propose a novel algorithm that\ncombines a decomposed centralized critic with decentralized ensemble learning,\nincorporating several key contributions. The main component in our scheme is a\nselective exploration method that leverages ensemble kurtosis. We extend the\nglobal decomposed critic with a diversity-regularized ensemble of individual\ncritics and utilize its excess kurtosis to guide exploration toward\nhigh-uncertainty states and actions. To improve sample efficiency, we train the\ncentralized critic with a novel truncated variation of the TD($\\lambda$)\nalgorithm, enabling efficient off-policy learning with reduced variance. On the\nactor side, our suggested algorithm adapts the mixed samples approach to MARL,\nmixing on-policy and off-policy loss functions for training the actors. This\napproach balances between stability and efficiency and outperforms purely\noff-policy learning. The evaluation shows our method outperforms\nstate-of-the-art baselines on standard MARL benchmarks, including a variety of\nSMAC II maps.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5206\u89e3\u4e2d\u5fc3\u5316\u8bc4\u8bba\u5668\u548c\u53bb\u4e2d\u5fc3\u5316\u96c6\u6210\u5b66\u4e60\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u63a2\u7d22\u548c\u591a\u6837\u6027\u6b63\u5219\u5316\u63d0\u5347\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u901a\u5e38\u9700\u8981\u6bd4\u5355\u667a\u80fd\u4f53\u66f4\u591a\u7684\u73af\u5883\u4ea4\u4e92\uff0c\u4e14\u5b58\u5728\u8054\u5408\u52a8\u4f5c\u7a7a\u95f4\u63a2\u7d22\u56f0\u96be\u548c\u65b9\u5dee\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u89e3\u4e2d\u5fc3\u5316\u8bc4\u8bba\u5668\u4e0e\u96c6\u6210\u5b66\u4e60\u7ed3\u5408\uff0c\u5229\u7528\u96c6\u6210\u7684\u5cf0\u5ea6\u6307\u5bfc\u63a2\u7d22\u9ad8\u4e0d\u786e\u5b9a\u6027\u72b6\u6001\u548c\u52a8\u4f5c\uff0c\u5e76\u901a\u8fc7\u622a\u65ad\u7684TD(\u03bb)\u7b97\u6cd5\u63d0\u5347\u6837\u672c\u6548\u7387\u3002", "result": "\u5728\u6807\u51c6MARL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08\u5305\u62ecSMAC II\u5730\u56fe\uff09\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u6548\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86MARL\u7684\u6027\u80fd\u548c\u6837\u672c\u6548\u7387\u3002", "keywords": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60,\u9009\u62e9\u6027\u63a2\u7d22,\u96c6\u6210\u5b66\u4e60,TD(\u03bb),SMAC II"}}
{"id": "2506.02859", "pdf": "https://arxiv.org/pdf/2506.02859", "abs": "https://arxiv.org/abs/2506.02859", "authors": ["Parth Atulbhai Gandhi", "Akansha Shukla", "David Tayouri", "Beni Ifland", "Yuval Elovici", "Rami Puzis", "Asaf Shabtai"], "title": "ATAG: AI-Agent Application Threat Assessment with Attack Graphs", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Evaluating the security of multi-agent systems (MASs) powered by large\nlanguage models (LLMs) is challenging, primarily because of the systems'\ncomplex internal dynamics and the evolving nature of LLM vulnerabilities.\nTraditional attack graph (AG) methods often lack the specific capabilities to\nmodel attacks on LLMs. This paper introduces AI-agent application Threat\nassessment with Attack Graphs (ATAG), a novel framework designed to\nsystematically analyze the security risks associated with AI-agent\napplications. ATAG extends the MulVAL logic-based AG generation tool with\ncustom facts and interaction rules to accurately represent AI-agent topologies,\nvulnerabilities, and attack scenarios. As part of this research, we also\ncreated the LLM vulnerability database (LVD) to initiate the process of\nstandardizing LLM vulnerabilities documentation. To demonstrate ATAG's\nefficacy, we applied it to two multi-agent applications. Our case studies\ndemonstrated the framework's ability to model and generate AGs for\nsophisticated, multi-step attack scenarios exploiting vulnerabilities such as\nprompt injection, excessive agency, sensitive information disclosure, and\ninsecure output handling across interconnected agents. ATAG is an important\nstep toward a robust methodology and toolset to help understand, visualize, and\nprioritize complex attack paths in multi-agent AI systems (MAASs). It\nfacilitates proactive identification and mitigation of AI-agent threats in\nmulti-agent applications.", "AI": {"tldr": "ATAG\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b89\u5168\u98ce\u9669\uff0c\u901a\u8fc7\u6269\u5c55MulVAL\u5e76\u521b\u5efaLVD\u6570\u636e\u5e93\uff0c\u6210\u529f\u5efa\u6a21\u590d\u6742\u653b\u51fb\u573a\u666f\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MASs\uff09\u7684\u5b89\u5168\u8bc4\u4f30\u590d\u6742\u4e14\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u51c6\u786e\u5efa\u6a21\u9488\u5bf9LLM\u7684\u653b\u51fb\uff0c\u4e9f\u9700\u65b0\u5de5\u5177\u3002", "method": "ATAG\u6269\u5c55\u4e86MulVAL\u5de5\u5177\uff0c\u52a0\u5165\u81ea\u5b9a\u4e49\u4e8b\u5b9e\u548c\u4ea4\u4e92\u89c4\u5219\uff0c\u5e76\u5efa\u7acbLLM\u6f0f\u6d1e\u6570\u636e\u5e93\uff08LVD\uff09\u4ee5\u6807\u51c6\u5316\u6f0f\u6d1e\u6587\u6863\u3002", "result": "ATAG\u6210\u529f\u5e94\u7528\u4e8e\u4e24\u4e2a\u591a\u667a\u80fd\u4f53\u5e94\u7528\uff0c\u80fd\u5efa\u6a21\u590d\u6742\u653b\u51fb\u573a\u666f\uff08\u5982\u63d0\u793a\u6ce8\u5165\u3001\u4fe1\u606f\u6cc4\u9732\u7b49\uff09\u3002", "conclusion": "ATAG\u4e3a\u7406\u89e3\u548c\u4f18\u5148\u5904\u7406\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u4e2d\u7684\u653b\u51fb\u8def\u5f84\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u4fc3\u8fdb\u5a01\u80c1\u7684\u4e3b\u52a8\u8bc6\u522b\u4e0e\u7f13\u89e3\u3002", "keywords": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf, \u5b89\u5168\u8bc4\u4f30, \u653b\u51fb\u56fe, LLM\u6f0f\u6d1e, \u5a01\u80c1\u5efa\u6a21"}}
{"id": "2506.02849", "pdf": "https://arxiv.org/pdf/2506.02849", "abs": "https://arxiv.org/abs/2506.02849", "authors": ["Alejandro Sanchez Roncero", "Olov Andersson", "Petter Ogren"], "title": "Learned Controllers for Agile Quadrotors in Pursuit-Evasion Games", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "The increasing proliferation of small UAVs in civilian and military airspace\nhas raised critical safety and security concerns, especially when unauthorized\nor malicious drones enter restricted zones. In this work, we present a\nreinforcement learning (RL) framework for agile 1v1 quadrotor pursuit-evasion.\nWe train neural network policies to command body rates and collective thrust,\nenabling high-speed pursuit and evasive maneuvers that fully exploit the\nquadrotor's nonlinear dynamics. To mitigate nonstationarity and catastrophic\nforgetting during adversarial co-training, we introduce an Asynchronous\nMulti-Stage Population-Based (AMSPB) algorithm where, at each stage, either the\npursuer or evader learns against a sampled opponent drawn from a growing\npopulation of past and current policies. This continual learning setup ensures\nmonotonic performance improvement and retention of earlier strategies. Our\nresults show that (i) rate-based policies achieve significantly higher capture\nrates and peak speeds than velocity-level baselines, and (ii) AMSPB yields\nstable, monotonic gains against a suite of benchmark opponents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u76841v1\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u8ffd\u9003\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6b65\u591a\u9636\u6bb5\u7fa4\u4f53\u7b97\u6cd5\u89e3\u51b3\u5bf9\u6297\u8bad\u7ec3\u4e2d\u7684\u975e\u7a33\u6001\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u5728\u6c11\u7528\u548c\u519b\u7528\u9886\u57df\u7684\u666e\u53ca\uff0c\u672a\u7ecf\u6388\u6743\u6216\u6076\u610f\u65e0\u4eba\u673a\u8fdb\u5165\u7981\u533a\u7684\u5b89\u5168\u4e0e\u5b89\u5168\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u9ad8\u6548\u7684\u8ffd\u9003\u7b56\u7565\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\u63a7\u5236\u673a\u8eab\u901f\u7387\u548c\u96c6\u4f53\u63a8\u529b\uff0c\u5e76\u5f15\u5165\u5f02\u6b65\u591a\u9636\u6bb5\u7fa4\u4f53\u7b97\u6cd5\uff08AMSPB\uff09\u4f18\u5316\u5bf9\u6297\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u901f\u7387\u7684\u7b56\u7565\u6bd4\u901f\u5ea6\u7ea7\u57fa\u7ebf\u5177\u6709\u66f4\u9ad8\u7684\u6355\u83b7\u7387\u548c\u5cf0\u503c\u901f\u5ea6\uff0cAMSPB\u7b97\u6cd5\u5728\u5bf9\u6297\u57fa\u51c6\u5bf9\u624b\u65f6\u8868\u73b0\u7a33\u5b9a\u4e14\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684RL\u6846\u67b6\u548cAMSPB\u7b97\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u65e0\u4eba\u673a\u8ffd\u9003\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u5bf9\u6297\u73af\u5883\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u65e0\u4eba\u673a, \u5f3a\u5316\u5b66\u4e60, \u8ffd\u9003, \u5bf9\u6297\u8bad\u7ec3, \u5f02\u6b65\u591a\u9636\u6bb5\u7fa4\u4f53\u7b97\u6cd5"}}
{"id": "2506.02860", "pdf": "https://arxiv.org/pdf/2506.02860", "abs": "https://arxiv.org/abs/2506.02860", "authors": ["Wenjing Tang", "Xinyu He", "Yongxi Huang", "Yunxiao Xiao", "Cewu Lu", "Panpan Cai"], "title": "Tru-POMDP: Task Planning Under Uncertainty via Tree of Hypotheses and Open-Ended POMDPs", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Task planning under uncertainty is essential for home-service robots\noperating in the real world. Tasks involve ambiguous human instructions, hidden\nor unknown object locations, and open-vocabulary object types, leading to\nsignificant open-ended uncertainty and a boundlessly large planning space. To\naddress these challenges, we propose Tru-POMDP, a planner that combines\nstructured belief generation using Large Language Models (LLMs) with principled\nPOMDP planning. Tru-POMDP introduces a hierarchical Tree of Hypotheses (TOH),\nwhich systematically queries an LLM to construct high-quality particle beliefs\nover possible world states and human goals. We further formulate an open-ended\nPOMDP model that enables rigorous Bayesian belief tracking and efficient\nbelief-space planning over these LLM-generated hypotheses. Experiments on\ncomplex object rearrangement tasks across diverse kitchen environments show\nthat Tru-POMDP significantly outperforms state-of-the-art LLM-based and\nLLM-tree-search hybrid planners, achieving higher success rates with\nsignificantly better plans, stronger robustness to ambiguity and occlusion, and\ngreater planning efficiency.", "AI": {"tldr": "Tru-POMDP\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548cPOMDP\u89c4\u5212\u7684\u89c4\u5212\u5668\uff0c\u7528\u4e8e\u5904\u7406\u5bb6\u5ead\u670d\u52a1\u673a\u5668\u4eba\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u89c4\u5212\u95ee\u9898\u3002", "motivation": "\u5bb6\u5ead\u670d\u52a1\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u6267\u884c\u4efb\u52a1\u65f6\uff0c\u9700\u8981\u5904\u7406\u6a21\u7cca\u7684\u4eba\u7c7b\u6307\u4ee4\u3001\u9690\u85cf\u6216\u672a\u77e5\u7684\u7269\u4f53\u4f4d\u7f6e\u4ee5\u53ca\u5f00\u653e\u8bcd\u6c47\u7684\u7269\u4f53\u7c7b\u578b\uff0c\u8fd9\u5bfc\u81f4\u89c4\u5212\u7a7a\u95f4\u5de8\u5927\u4e14\u4e0d\u786e\u5b9a\u6027\u9ad8\u3002", "method": "\u63d0\u51faTru-POMDP\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5206\u5c42\u5047\u8bbe\u6811\uff08TOH\uff09\u7cfb\u7edf\u6027\u5730\u67e5\u8be2LLM\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4e16\u754c\u72b6\u6001\u548c\u4eba\u7c7b\u76ee\u6807\u7684\u7c92\u5b50\u4fe1\u5ff5\uff0c\u5e76\u7ed3\u5408\u5f00\u653e\u7684POMDP\u6a21\u578b\u8fdb\u884c\u4e25\u683c\u7684\u8d1d\u53f6\u65af\u4fe1\u5ff5\u8ddf\u8e2a\u548c\u89c4\u5212\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u53a8\u623f\u73af\u5883\u4e2d\uff0cTru-POMDP\u5728\u590d\u6742\u7269\u4f53\u91cd\u65b0\u6392\u5217\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8eLLM\u548cLLM\u6811\u641c\u7d22\u6df7\u5408\u7684\u89c4\u5212\u5668\uff0c\u6210\u529f\u7387\u66f4\u9ad8\uff0c\u9c81\u68d2\u6027\u548c\u89c4\u5212\u6548\u7387\u66f4\u5f3a\u3002", "conclusion": "Tru-POMDP\u901a\u8fc7\u7ed3\u5408LLM\u548cPOMDP\u89c4\u5212\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5bb6\u5ead\u670d\u52a1\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u7684\u6548\u80fd\u548c\u9c81\u68d2\u6027\u3002", "keywords": "\u4efb\u52a1\u89c4\u5212\uff0c\u4e0d\u786e\u5b9a\u6027\uff0cPOMDP\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5bb6\u5ead\u670d\u52a1\u673a\u5668\u4eba"}}
{"id": "2506.02881", "pdf": "https://arxiv.org/pdf/2506.02881", "abs": "https://arxiv.org/abs/2506.02881", "authors": ["Brian M Cho", "Aur\u00e9lien Bibaut", "Nathan Kallus"], "title": "Simulation-Based Inference for Adaptive Experiments", "categories": ["stat.ME", "cs.LG", "stat.ML"], "comment": null, "summary": "Multi-arm bandit experimental designs are increasingly being adopted over\nstandard randomized trials due to their potential to improve outcomes for study\nparticipants, enable faster identification of the best-performing options,\nand/or enhance the precision of estimating key parameters. Current approaches\nfor inference after adaptive sampling either rely on asymptotic normality under\nrestricted experiment designs or underpowered martingale concentration\ninequalities that lead to weak power in practice. To bypass these limitations,\nwe propose a simulation-based approach for conducting hypothesis tests and\nconstructing confidence intervals for arm specific means and their differences.\nOur simulation-based approach uses positively biased nuisances to generate\nadditional trajectories of the experiment, which we call \\textit{simulation\nwith optimism}. Using these simulations, we characterize the distribution\npotentially non-normal sample mean test statistic to conduct inference. We\nprovide guarantees for (i) asymptotic type I error control, (ii) convergence of\nour confidence intervals, and (iii) asymptotic strong consistency of our\nestimator over a wide variety of common bandit designs. Our empirical results\nshow that our approach achieves the desired coverage while reducing confidence\ninterval widths by up to 50%, with drastic improvements for arms not targeted\nby the design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u62df\u7684\u5047\u8bbe\u68c0\u9a8c\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u591a\u81c2\u8001\u864e\u673a\u5b9e\u9a8c\u4e2d\u63d0\u9ad8\u63a8\u65ad\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u591a\u81c2\u8001\u864e\u673a\u5b9e\u9a8c\u7684\u63a8\u65ad\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6e10\u8fd1\u6b63\u6001\u6027\u6216\u5f31\u529f\u7387\u7684\u9785\u96c6\u4e2d\u4e0d\u7b49\u5f0f\uff0c\u5bfc\u81f4\u6548\u679c\u4e0d\u7406\u60f3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u4e50\u89c2\u6a21\u62df\u201d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u989d\u5916\u7684\u5b9e\u9a8c\u8f68\u8ff9\u6765\u63a8\u65ad\u975e\u6b63\u6001\u6837\u672c\u5747\u503c\u7edf\u8ba1\u91cf\u7684\u5206\u5e03\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5e38\u89c1\u8001\u864e\u673a\u8bbe\u8ba1\u4e2d\u5b9e\u73b0\u4e86\u6e10\u8fd1\u7c7b\u578bI\u9519\u8bef\u63a7\u5236\u3001\u7f6e\u4fe1\u533a\u95f4\u6536\u655b\u548c\u4f30\u8ba1\u7684\u5f3a\u4e00\u81f4\u6027\uff0c\u4e14\u7f6e\u4fe1\u533a\u95f4\u5bbd\u5ea6\u51cf\u5c11\u4e8650%\u3002", "conclusion": "\u4e50\u89c2\u6a21\u62df\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u63a8\u65ad\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u7279\u522b\u9002\u7528\u4e8e\u672a\u88ab\u8bbe\u8ba1\u76ee\u6807\u5173\u6ce8\u7684\u81c2\u3002", "keywords": "\u591a\u81c2\u8001\u864e\u673a\uff0c\u4e50\u89c2\u6a21\u62df\uff0c\u5047\u8bbe\u68c0\u9a8c\uff0c\u7f6e\u4fe1\u533a\u95f4"}}
{"id": "2506.02863", "pdf": "https://arxiv.org/pdf/2506.02863", "abs": "https://arxiv.org/abs/2506.02863", "authors": ["Helin Wang", "Jiarui Hai", "Dading Chong", "Karan Thakkar", "Tiantian Feng", "Dongchao Yang", "Junhyeok Lee", "Laureano Moro Velazquez", "Jesus Villalba", "Zengyi Qin", "Shrikanth Narayanan", "Mounya Elhiali", "Najim Dehak"], "title": "CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": null, "summary": "Recent advancements in generative artificial intelligence have significantly\ntransformed the field of style-captioned text-to-speech synthesis (CapTTS).\nHowever, adapting CapTTS to real-world applications remains challenging due to\nthe lack of standardized, comprehensive datasets and limited research on\ndownstream tasks built upon CapTTS. To address these gaps, we introduce\nCapSpeech, a new benchmark designed for a series of CapTTS-related tasks,\nincluding style-captioned text-to-speech synthesis with sound events\n(CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS\n(EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech\ncomprises over 10 million machine-annotated audio-caption pairs and nearly 0.36\nmillion human-annotated audio-caption pairs. In addition, we introduce two new\ndatasets collected and recorded by a professional voice actor and experienced\naudio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside\nthe datasets, we conduct comprehensive experiments using both autoregressive\nand non-autoregressive models on CapSpeech. Our results demonstrate\nhigh-fidelity and highly intelligible speech synthesis across a diverse range\nof speaking styles. To the best of our knowledge, CapSpeech is the largest\navailable dataset offering comprehensive annotations for CapTTS-related tasks.\nThe experiments and findings further provide valuable insights into the\nchallenges of developing CapTTS systems.", "AI": {"tldr": "CapSpeech\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u652f\u6301\u98ce\u683c\u6ce8\u91ca\u7684\u6587\u672c\u5230\u8bed\u97f3\u5408\u6210\uff08CapTTS\uff09\u4efb\u52a1\uff0c\u586b\u8865\u4e86\u6807\u51c6\u5316\u6570\u636e\u96c6\u548c\u4e0b\u6e38\u4efb\u52a1\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u7efc\u5408\u6570\u636e\u96c6\u548c\u5bf9CapTTS\u4e0b\u6e38\u4efb\u52a1\u7684\u6709\u9650\u7814\u7a76\uff0c\u8bbe\u8ba1CapSpeech\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86CapSpeech\u6570\u636e\u96c6\uff0c\u5305\u542b\u673a\u5668\u548c\u4eba\u5de5\u6807\u6ce8\u7684\u97f3\u9891-\u5b57\u5e55\u5bf9\uff0c\u4ee5\u53ca\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u4e13\u4e1a\u5f55\u5236\u6570\u636e\u3002\u4f7f\u7528\u4e86\u81ea\u56de\u5f52\u548c\u975e\u81ea\u56de\u5f52\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u591a\u6837\u5316\u7684\u8bf4\u8bdd\u98ce\u683c\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u548c\u9ad8\u53ef\u7406\u89e3\u6027\u7684\u8bed\u97f3\u5408\u6210\u3002CapSpeech\u662f\u76ee\u524d\u6700\u5927\u7684CapTTS\u76f8\u5173\u4efb\u52a1\u6570\u636e\u96c6\u3002", "conclusion": "CapSpeech\u4e3aCapTTS\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u6570\u636e\u652f\u6301\u548c\u7814\u7a76\u6d1e\u5bdf\u3002", "keywords": "\u751f\u6210\u4eba\u5de5\u667a\u80fd, \u98ce\u683c\u6ce8\u91ca\u6587\u672c\u5230\u8bed\u97f3, CapSpeech, \u6570\u636e\u96c6, \u81ea\u56de\u5f52\u6a21\u578b"}}
{"id": "2506.02896", "pdf": "https://arxiv.org/pdf/2506.02896", "abs": "https://arxiv.org/abs/2506.02896", "authors": ["Adam Pardyl", "Dominik Matuszek", "Mateusz Przebieracz", "Marek Cygan", "Bartosz Zieli\u0144ski", "Maciej Wo\u0142czyk"], "title": "FlySearch: Exploring how vision-language models explore", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "The real world is messy and unstructured. Uncovering critical information\noften requires active, goal-driven exploration. It remains to be seen whether\nVision-Language Models (VLMs), which recently emerged as a popular zero-shot\ntool in many difficult tasks, can operate effectively in such conditions. In\nthis paper, we answer this question by introducing FlySearch, a 3D, outdoor,\nphotorealistic environment for searching and navigating to objects in complex\nscenes. We define three sets of scenarios with varying difficulty and observe\nthat state-of-the-art VLMs cannot reliably solve even the simplest exploration\ntasks, with the gap to human performance increasing as the tasks get harder. We\nidentify a set of central causes, ranging from vision hallucination, through\ncontext misunderstanding, to task planning failures, and we show that some of\nthem can be addressed by finetuning. We publicly release the benchmark,\nscenarios, and the underlying codebase.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7FlySearch\u73af\u5883\u8bc4\u4f30\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u8868\u73b0\u8fdc\u4e0d\u53ca\u4eba\u7c7b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u3001\u975e\u7ed3\u6784\u5316\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u9a8c\u8bc1\u5176\u662f\u5426\u80fd\u591f\u6709\u6548\u5b8c\u6210\u4e3b\u52a8\u63a2\u7d22\u4efb\u52a1\u3002", "method": "\u5f15\u5165FlySearch\uff0c\u4e00\u4e2a3D\u3001\u6237\u5916\u3001\u903c\u771f\u7684\u73af\u5883\uff0c\u8bbe\u8ba1\u4e09\u79cd\u96be\u5ea6\u4e0d\u540c\u7684\u4efb\u52a1\u573a\u666f\uff0c\u6d4b\u8bd5\u5e76\u5206\u6790\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5373\u4f7f\u662f\u6700\u7b80\u5355\u4efb\u52a1\u4e5f\u65e0\u6cd5\u53ef\u9760\u5b8c\u6210\uff0c\u8868\u73b0\u4e0e\u4eba\u7c7b\u5dee\u8ddd\u660e\u663e\uff0c\u4e14\u968f\u4efb\u52a1\u96be\u5ea6\u589e\u52a0\u800c\u6076\u5316\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u4ecd\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\uff0c\u5c24\u5176\u9700\u89e3\u51b3\u89c6\u89c9\u5e7b\u89c9\u3001\u4e0a\u4e0b\u6587\u8bef\u89e3\u548c\u4efb\u52a1\u89c4\u5212\u5931\u8d25\u7b49\u95ee\u9898\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u590d\u6742\u573a\u666f, \u63a2\u7d22\u4efb\u52a1, FlySearch, \u6027\u80fd\u8bc4\u4f30"}}
{"id": "2506.02908", "pdf": "https://arxiv.org/pdf/2506.02908", "abs": "https://arxiv.org/abs/2506.02908", "authors": ["Bunlong Lay", "Rostilav Makarov", "Timo Gerkmann"], "title": "Diffusion Buffer: Online Diffusion-based Speech Enhancement with Sub-Second Latency", "categories": ["eess.AS", "cs.LG"], "comment": "5 pages, 2 figures, Accepted to Interspeech 2025", "summary": "Diffusion models are a class of generative models that have been recently\nused for speech enhancement with remarkable success but are computationally\nexpensive at inference time. Therefore, these models are impractical for\nprocessing streaming data in real-time. In this work, we adapt a sliding window\ndiffusion framework to the speech enhancement task. Our approach progressively\ncorrupts speech signals through time, assigning more noise to frames close to\nthe present in a buffer. This approach outputs denoised frames with a delay\nproportional to the chosen buffer size, enabling a trade-off between\nperformance and latency. Empirical results demonstrate that our method\noutperforms standard diffusion models and runs efficiently on a GPU, achieving\nan input-output latency in the order of 0.3 to 1 seconds. This marks the first\npractical diffusion-based solution for online speech enhancement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ed1\u52a8\u7a97\u53e3\u7684\u6269\u6563\u6a21\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bed\u97f3\u589e\u5f3a\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u548c\u5ef6\u8fdf\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5904\u7406\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u8bed\u97f3\u589e\u5f3a\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u63a8\u7406\u9636\u6bb5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5b9e\u65f6\u5904\u7406\u6d41\u6570\u636e\u3002", "method": "\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u6269\u6563\u6846\u67b6\uff0c\u968f\u65f6\u95f4\u9010\u6b65\u5bf9\u8bed\u97f3\u4fe1\u53f7\u52a0\u566a\uff0c\u5bf9\u5f53\u524d\u5e27\u8d4b\u4e88\u66f4\u591a\u566a\u58f0\uff0c\u5e76\u901a\u8fc7\u7f13\u51b2\u533a\u5927\u5c0f\u6743\u8861\u5ef6\u8fdf\u4e0e\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6807\u51c6\u6269\u6563\u6a21\u578b\uff0cGPU\u4e0a\u8fd0\u884c\u65f6\u8f93\u5165\u8f93\u51fa\u5ef6\u8fdf\u4e3a0.3\u81f31\u79d2\uff0c\u9996\u6b21\u5b9e\u73b0\u5b9e\u7528\u7684\u5728\u7ebf\u8bed\u97f3\u589e\u5f3a\u3002", "conclusion": "\u6ed1\u52a8\u7a97\u53e3\u6269\u6563\u6a21\u578b\u662f\u9996\u4e2a\u9002\u7528\u4e8e\u5b9e\u65f6\u8bed\u97f3\u589e\u5f3a\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u6269\u6563\u6a21\u578b,\u8bed\u97f3\u589e\u5f3a,\u6ed1\u52a8\u7a97\u53e3,\u5b9e\u65f6\u5904\u7406,GPU"}}
{"id": "2506.02931", "pdf": "https://arxiv.org/pdf/2506.02931", "abs": "https://arxiv.org/abs/2506.02931", "authors": ["Praneet Sai Madhu Surabhi", "Dheeraj Reddy Mudireddy", "Jian Tao"], "title": "ThinkTank: A Framework for Generalizing Domain-Specific AI Agent Systems into Universal Collaborative Intelligence Platforms", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper presents ThinkTank, a comprehensive and scalable framework\ndesigned to transform specialized AI agent systems into versatile collaborative\nintelligence platforms capable of supporting complex problem-solving across\ndiverse domains. ThinkTank systematically generalizes agent roles, meeting\nstructures, and knowledge integration mechanisms by adapting proven scientific\ncollaboration methodologies. Through role abstraction, generalization of\nmeeting types for iterative collaboration, and the integration of\nRetrieval-Augmented Generation with advanced knowledge storage, the framework\nfacilitates expertise creation and robust knowledge sharing. ThinkTank enables\norganizations to leverage collaborative AI for knowledge-intensive tasks while\nensuring data privacy and security through local deployment, utilizing\nframeworks like Ollama with models such as Llama3.1. The ThinkTank framework is\ndesigned to deliver significant advantages in cost-effectiveness, data\nsecurity, scalability, and competitive positioning compared to cloud-based\nalternatives, establishing it as a universal platform for AI-driven\ncollaborative problem-solving. The ThinkTank code is available at\nhttps://github.com/taugroup/ThinkTank", "AI": {"tldr": "ThinkTank\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684AI\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u89d2\u8272\u62bd\u8c61\u3001\u4f1a\u8bae\u7c7b\u578b\u6cdb\u5316\u548c\u77e5\u8bc6\u96c6\u6210\uff0c\u5c06\u4e13\u4e1aAI\u7cfb\u7edf\u8f6c\u5316\u4e3a\u591a\u9886\u57df\u534f\u4f5c\u5e73\u53f0\uff0c\u652f\u6301\u590d\u6742\u95ee\u9898\u89e3\u51b3\u3002", "motivation": "\u89e3\u51b3\u4e13\u4e1aAI\u7cfb\u7edf\u5728\u591a\u9886\u57df\u534f\u4f5c\u4e2d\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u4e00\u79cd\u901a\u7528\u3001\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u534f\u4f5c\u667a\u80fd\u5e73\u53f0\u3002", "method": "\u91c7\u7528\u89d2\u8272\u62bd\u8c61\u3001\u4f1a\u8bae\u7c7b\u578b\u6cdb\u5316\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u672c\u5730\u5316\u90e8\u7f72\uff08\u5982Ollama\u6846\u67b6\u548cLlama3.1\u6a21\u578b\uff09\u3002", "result": "ThinkTank\u5728\u6210\u672c\u6548\u76ca\u3001\u6570\u636e\u5b89\u5168\u3001\u53ef\u6269\u5c55\u6027\u548c\u7ade\u4e89\u5730\u4f4d\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u4e91\u7684\u65b9\u6848\u3002", "conclusion": "ThinkTank\u662f\u4e00\u4e2a\u901a\u7528\u7684AI\u534f\u4f5c\u95ee\u9898\u89e3\u51b3\u5e73\u53f0\uff0c\u9002\u7528\u4e8e\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u3002", "keywords": "ThinkTank, \u534f\u4f5c\u667a\u80fd, \u68c0\u7d22\u589e\u5f3a\u751f\u6210, \u672c\u5730\u5316\u90e8\u7f72, \u77e5\u8bc6\u5171\u4eab"}}
{"id": "2506.02955", "pdf": "https://arxiv.org/pdf/2506.02955", "abs": "https://arxiv.org/abs/2506.02955", "authors": ["Zewen Yang", "Xiaobing Dai", "Dian Yu", "Qianru Li", "Yu Li", "Valentin Le Mesle"], "title": "UniConFlow: A Unified Constrained Generalization Framework for Certified Motion Planning with Flow Matching Models", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative models have become increasingly powerful tools for robot motion\ngeneration, enabling flexible and multimodal trajectory generation across\nvarious tasks. Yet, most existing approaches remain limited in handling\nmultiple types of constraints, such as collision avoidance and dynamic\nconsistency, which are often treated separately or only partially considered.\nThis paper proposes UniConFlow, a unified flow matching (FM) based framework\nfor trajectory generation that systematically incorporates both equality and\ninequality constraints. UniConFlow introduces a novel prescribed-time zeroing\nfunction to enhance flexibility during the inference process, allowing the\nmodel to adapt to varying task requirements. To ensure constraint satisfaction,\nparticularly with respect to obstacle avoidance, admissible action range, and\nkinodynamic consistency, the guidance inputs to the FM model are derived\nthrough a quadratic programming formulation, which enables constraint-aware\ngeneration without requiring retraining or auxiliary controllers. We conduct\nmobile navigation and high-dimensional manipulation tasks, demonstrating\nimproved safety and feasibility compared to state-of-the-art constrained\ngenerative planners. Project page is available at https://uniconflow.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faUniConFlow\uff0c\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u8f68\u8ff9\u751f\u6210\uff0c\u7cfb\u7edf\u6027\u6574\u5408\u4e86\u7b49\u5f0f\u4e0e\u4e0d\u7b49\u5f0f\u7ea6\u675f\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u867d\u5f3a\u5927\u4f46\u591a\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u591a\u79cd\u7ea6\u675f\uff0c\u5982\u907f\u969c\u548c\u52a8\u6001\u4e00\u81f4\u6027\u3002", "method": "UniConFlow\u91c7\u7528\u6d41\u5339\u914d\u6846\u67b6\uff0c\u5f15\u5165\u9884\u8bbe\u65f6\u95f4\u5f52\u96f6\u51fd\u6570\u548c\u4e8c\u6b21\u89c4\u5212\u6307\u5bfc\u8f93\u5165\uff0c\u4ee5\u7075\u6d3b\u6ee1\u8db3\u4efb\u52a1\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u79fb\u52a8\u5bfc\u822a\u548c\u9ad8\u7ef4\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u5b89\u5168\u6027\u4e0e\u53ef\u884c\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UniConFlow\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u6709\u6548\u89e3\u51b3\u591a\u7ea6\u675f\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u8f68\u8ff9\u7684\u5b9e\u7528\u6027\u4e0e\u9002\u5e94\u6027\u3002", "keywords": "\u751f\u6210\u6a21\u578b,\u6d41\u5339\u914d,\u8f68\u8ff9\u751f\u6210,\u591a\u7ea6\u675f"}}
{"id": "2506.02964", "pdf": "https://arxiv.org/pdf/2506.02964", "abs": "https://arxiv.org/abs/2506.02964", "authors": ["Guiqiu Liao", "Matjaz Jogan", "Eric Eaton", "Daniel A. Hashimoto"], "title": "FORLA:Federated Object-centric Representation Learning with Slot Attention", "categories": ["cs.CV", "cs.LG"], "comment": "24 pages, 6 figures", "summary": "Learning efficient visual representations across heterogeneous unlabeled\ndatasets remains a central challenge in federated learning. Effective federated\nrepresentations require features that are jointly informative across clients\nwhile disentangling domain-specific factors without supervision. We introduce\nFORLA, a novel framework for federated object-centric representation learning\nand feature adaptation across clients using unsupervised slot attention. At the\ncore of our method is a shared feature adapter, trained collaboratively across\nclients to adapt features from foundation models, and a shared slot attention\nmodule that learns to reconstruct the adapted features. To optimize this\nadapter, we design a two-branch student-teacher architecture. In each client, a\nstudent decoder learns to reconstruct full features from foundation models,\nwhile a teacher decoder reconstructs their adapted, low-dimensional\ncounterpart. The shared slot attention module bridges cross-domain learning by\naligning object-level representations across clients. Experiments in multiple\nreal-world datasets show that our framework not only outperforms centralized\nbaselines on object discovery but also learns a compact, universal\nrepresentation that generalizes well across domains. This work highlights\nfederated slot attention as an effective tool for scalable, unsupervised visual\nrepresentation learning from cross-domain data with distributed concepts.", "AI": {"tldr": "FORLA\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u5ba2\u6237\u7aef\u7684\u65e0\u76d1\u7763\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u5b66\u4e60\u548c\u7279\u5f81\u9002\u914d\uff0c\u901a\u8fc7\u5171\u4eab\u7279\u5f81\u9002\u914d\u5668\u548c\u69fd\u6ce8\u610f\u529b\u6a21\u5757\u5b9e\u73b0\u8de8\u9886\u57df\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u65e0\u6807\u7b7e\u6570\u636e\u96c6\u4e2d\u9ad8\u6548\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u8de8\u5ba2\u6237\u7aef\u8054\u5408\u5b66\u4e60\u7279\u5f81\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u5171\u4eab\u7279\u5f81\u9002\u914d\u5668\u548c\u69fd\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u8bbe\u8ba1\u4e24\u5206\u652f\u5e08\u751f\u67b6\u6784\uff0c\u5206\u522b\u91cd\u6784\u57fa\u7840\u6a21\u578b\u548c\u9002\u914d\u540e\u7684\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u96c6\u4e2d\u5f0f\u57fa\u7ebf\uff0c\u5b66\u4e60\u5230\u7d27\u51d1\u4e14\u901a\u7528\u7684\u8de8\u9886\u57df\u8868\u793a\u3002", "conclusion": "\u8054\u90a6\u69fd\u6ce8\u610f\u529b\u662f\u5b9e\u73b0\u5206\u5e03\u5f0f\u6982\u5ff5\u4e0b\u8de8\u9886\u57df\u65e0\u76d1\u7763\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u7684\u6709\u6548\u5de5\u5177\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60, \u65e0\u76d1\u7763\u5b66\u4e60, \u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a, \u7279\u5f81\u9002\u914d, \u69fd\u6ce8\u610f\u529b"}}
{"id": "2506.02980", "pdf": "https://arxiv.org/pdf/2506.02980", "abs": "https://arxiv.org/abs/2506.02980", "authors": ["Xiaoqi Liu", "Dorian Baudry", "Julian Zimmert", "Patrick Rebeschini", "Arya Akhavan"], "title": "Non-stationary Bandit Convex Optimization: A Comprehensive Study", "categories": ["stat.ML", "cs.LG"], "comment": "32 pages, 1 figure", "summary": "Bandit Convex Optimization is a fundamental class of sequential\ndecision-making problems, where the learner selects actions from a continuous\ndomain and observes a loss (but not its gradient) at only one point per round.\nWe study this problem in non-stationary environments, and aim to minimize the\nregret under three standard measures of non-stationarity: the number of\nswitches $S$ in the comparator sequence, the total variation $\\Delta$ of the\nloss functions, and the path-length $P$ of the comparator sequence. We propose\na polynomial-time algorithm, Tilted Exponentially Weighted Average with\nSleeping Experts (TEWA-SE), which adapts the sleeping experts framework from\nonline convex optimization to the bandit setting. For strongly convex losses,\nwe prove that TEWA-SE is minimax-optimal with respect to known $S$ and $\\Delta$\nby establishing matching upper and lower bounds. By equipping TEWA-SE with the\nBandit-over-Bandit framework, we extend our analysis to environments with\nunknown non-stationarity measures. For general convex losses, we introduce a\nsecond algorithm, clipped Exploration by Optimization (cExO), based on\nexponential weights over a discretized action space. While not polynomial-time\ncomputable, this method achieves minimax-optimal regret with respect to known\n$S$ and $\\Delta$, and improves on the best existing bounds with respect to $P$.", "AI": {"tldr": "\u7814\u7a76\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684Bandit\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e24\u79cd\u7b97\u6cd5\uff08TEWA-SE\u548ccExO\uff09\uff0c\u5206\u522b\u9488\u5bf9\u5f3a\u51f8\u548c\u4e00\u822c\u51f8\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u5bf9\u975e\u5e73\u7a33\u6027\u6307\u6807\uff08S\u3001\u0394\u3001P\uff09\u7684\u6700\u5c0f\u6781\u5927\u6700\u4f18\u540e\u6094\u3002", "motivation": "\u89e3\u51b3Bandit\u51f8\u4f18\u5316\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u4e0d\u540c\u975e\u5e73\u7a33\u6027\u6307\u6807\uff08S\u3001\u0394\u3001P\uff09\u7684\u6700\u4f18\u540e\u6094\u8fb9\u754c\u3002", "method": "\u63d0\u51faTEWA-SE\u7b97\u6cd5\uff08\u7ed3\u5408sleeping\u4e13\u5bb6\u6846\u67b6\uff09\u548ccExO\u7b97\u6cd5\uff08\u57fa\u4e8e\u79bb\u6563\u5316\u52a8\u4f5c\u7a7a\u95f4\u7684\u6307\u6570\u52a0\u6743\uff09\uff0c\u5206\u522b\u5904\u7406\u5f3a\u51f8\u548c\u4e00\u822c\u51f8\u635f\u5931\u3002", "result": "TEWA-SE\u5bf9\u5df2\u77e5S\u548c\u0394\u7684\u5f3a\u51f8\u635f\u5931\u51fd\u6570\u5b9e\u73b0\u6700\u5c0f\u6781\u5927\u6700\u4f18\u540e\u6094\uff1bcExO\u5bf9\u4e00\u822c\u51f8\u635f\u5931\u51fd\u6570\u6539\u8fdbP\u76f8\u5173\u7684\u540e\u6094\u8fb9\u754c\u3002", "conclusion": "\u901a\u8fc7\u65b0\u7b97\u6cd5\uff0c\u5c55\u793a\u4e86\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2dBandit\u51f8\u4f18\u5316\u7684\u6700\u4f18\u540e\u6094\u8fb9\u754c\uff0c\u5e76\u6269\u5c55\u4e86\u672a\u77e5\u975e\u5e73\u7a33\u6027\u6307\u6807\u7684\u5904\u7406\u80fd\u529b\u3002", "keywords": "Bandit\u51f8\u4f18\u5316, \u975e\u5e73\u7a33\u73af\u5883, \u6700\u5c0f\u6781\u5927\u540e\u6094, TEWA-SE, cExO"}}
{"id": "2506.02975", "pdf": "https://arxiv.org/pdf/2506.02975", "abs": "https://arxiv.org/abs/2506.02975", "authors": ["Yicheng Xiao", "Lin Song", "Rui Yang", "Cheng Cheng", "Zunnan Xu", "Zhaoyang Zhang", "Yixiao Ge", "Xiu Li", "Ying Shan"], "title": "HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the advancement of language models, unified multimodal understanding and\ngeneration have made significant strides, with model architectures evolving\nfrom separated components to unified single-model frameworks. This paper\nexplores an efficient training paradigm to build a single transformer for\nunified multimodal understanding and generation. Specifically, we propose a\nmultimodal warmup strategy utilizing prior knowledge to extend capabilities. To\naddress cross-modal compatibility challenges, we introduce feature pre-scaling\nand multimodal AdaLN techniques. Integrating the proposed technologies, we\npresent the HaploOmni, a new single multimodal transformer. With limited\ntraining costs, HaploOmni achieves competitive performance across multiple\nimage and video understanding and generation benchmarks over advanced unified\nmodels. All codes will be made public at https://github.com/Tencent/HaploVLM.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u9884\u70ed\u7b56\u7565\u548c\u8de8\u6a21\u6001\u517c\u5bb9\u6027\u6280\u672f\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u7684Transformer\u6a21\u578bHaploOmni\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\uff0c\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5355\u4e00\u6a21\u578b\u6846\u67b6\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u9884\u70ed\u7b56\u7565\u3001\u7279\u5f81\u9884\u7f29\u653e\u548c\u591a\u6a21\u6001AdaLN\u6280\u672f\u3002", "result": "HaploOmni\u5728\u6709\u9650\u8bad\u7ec3\u6210\u672c\u4e0b\uff0c\u5728\u56fe\u50cf\u548c\u89c6\u9891\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "HaploOmni\u5c55\u793a\u4e86\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u6280\u672f\u5b9e\u7528\u4e14\u9ad8\u6548\u3002", "keywords": "\u591a\u6a21\u6001\u6a21\u578b,Transformer,HaploOmni,\u8de8\u6a21\u6001\u517c\u5bb9"}}
{"id": "2506.02976", "pdf": "https://arxiv.org/pdf/2506.02976", "abs": "https://arxiv.org/abs/2506.02976", "authors": ["Rachid Zeghlache", "Ikram Brahim", "Pierre-Henri Conze", "Mathieu Lamard", "Mohammed El Amine Lazouni", "Zineb Aziza Elaouaber", "Leila Ryma Lazouni", "Christopher Nielsen", "Ahmad O. Ahsan", "Matthias Wilms", "Nils D. Forkert", "Lovre Antonio Budimir", "Ivana Matovinovi\u0107", "Donik Vr\u0161nak", "Sven Lon\u010dari\u0107", "Philippe Zhang", "Weili Jiang", "Yihao Li", "Yiding Hao", "Markus Frohmann", "Patrick Binder", "Marcel Huber", "Taha Emre", "Teresa Finisterra Ara\u00fajo", "Marzieh Oghbaie", "Hrvoje Bogunovi\u0107", "Amerens A. Bekkers", "Nina M. van Liebergen", "Hugo J. Kuijf", "Abdul Qayyum", "Moona Mazher", "Steven A. Niederer", "Alberto J. Beltr\u00e1n-Carrero", "Juan J. G\u00f3mez-Valverde", "Javier Torresano-Rodr\u00edquez", "\u00c1lvaro Caballero-Sastre", "Mar\u00eda J. Ledesma Carbayo", "Yosuke Yamagishi", "Yi Ding", "Robin Peretzke", "Alexandra Ertl", "Maximilian Fischer", "Jessica K\u00e4chele", "Sofiane Zehar", "Karim Boukli Hacene", "Thomas Monfort", "B\u00e9atrice Cochener", "Mostafa El Habib Daho", "Anas-Alexis Benyoussef", "Gwenol\u00e9 Quellec"], "title": "Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO AMD Progression Challenge", "categories": ["cs.CV", "cs.AI"], "comment": "MARIO-MICCAI-CHALLENGE 2024", "summary": "The MARIO challenge, held at MICCAI 2024, focused on advancing the automated\ndetection and monitoring of age-related macular degeneration (AMD) through the\nanalysis of optical coherence tomography (OCT) images. Designed to evaluate\nalgorithmic performance in detecting neovascular activity changes within AMD,\nthe challenge incorporated unique multi-modal datasets. The primary dataset,\nsourced from Brest, France, was used by participating teams to train and test\ntheir models. The final ranking was determined based on performance on this\ndataset. An auxiliary dataset from Algeria was used post-challenge to evaluate\npopulation and device shifts from submitted solutions. Two tasks were involved\nin the MARIO challenge. The first one was the classification of evolution\nbetween two consecutive 2D OCT B-scans. The second one was the prediction of\nfuture AMD evolution over three months for patients undergoing anti-vascular\nendothelial growth factor (VEGF) therapy. Thirty-five teams participated, with\nthe top 12 finalists presenting their methods. This paper outlines the\nchallenge's structure, tasks, data characteristics, and winning methodologies,\nsetting a benchmark for AMD monitoring using OCT, infrared imaging, and\nclinical data (such as the number of visits, age, gender, etc.). The results of\nthis challenge indicate that artificial intelligence (AI) performs as well as a\nphysician in measuring AMD progression (Task 1) but is not yet able of\npredicting future evolution (Task 2).", "AI": {"tldr": "MICCAI 2024\u7684MARIO\u6311\u6218\u4e13\u6ce8\u4e8e\u901a\u8fc7OCT\u56fe\u50cf\u81ea\u52a8\u5316\u68c0\u6d4b\u548c\u76d1\u6d4bAMD\uff0c\u8bc4\u4f30\u7b97\u6cd5\u5728\u68c0\u6d4bAMD\u4e2d\u65b0\u8840\u7ba1\u6d3b\u52a8\u53d8\u5316\u7684\u8868\u73b0\u3002\u6311\u6218\u5305\u542b\u591a\u6a21\u6001\u6570\u636e\uff0c\u4e3b\u6570\u636e\u96c6\u6765\u81ea\u6cd5\u56fd\u5e03\u96f7\u65af\u7279\uff0c\u8f85\u52a9\u6570\u636e\u96c6\u6765\u81ea\u963f\u5c14\u53ca\u5229\u4e9a\u3002\u4efb\u52a1\u5305\u62ec\u5206\u7c7b\u4e24\u4e2a\u8fde\u7eedOCT B-scans\u7684\u6f14\u53d8\u548c\u9884\u6d4b\u672a\u6765\u4e09\u4e2a\u6708\u7684AMD\u6f14\u53d8\u3002\u7ed3\u679c\u663e\u793aAI\u5728\u76d1\u6d4bAMD\u8fdb\u5c55\u4e0a\u4e0e\u533b\u751f\u8868\u73b0\u76f8\u5f53\uff0c\u4f46\u65e0\u6cd5\u9884\u6d4b\u672a\u6765\u6f14\u53d8\u3002", "motivation": "\u63a8\u52a8\u81ea\u52a8\u5316\u68c0\u6d4b\u548c\u76d1\u6d4b\u5e74\u9f84\u76f8\u5173\u6027\u9ec4\u6591\u53d8\u6027\uff08AMD\uff09\u7684\u6280\u672f\u53d1\u5c55\uff0c\u7279\u522b\u662f\u901a\u8fc7\u5206\u6790OCT\u56fe\u50cf\u8bc4\u4f30\u65b0\u8840\u7ba1\u6d3b\u52a8\u7684\u53d8\u5316\u3002", "method": "\u6311\u6218\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4efb\u52a1\uff1a\u5206\u7c7b\u8fde\u7eedOCT B-scans\u7684\u6f14\u53d8\u548c\u9884\u6d4b\u672a\u6765AMD\u6f14\u53d8\u3002\u4f7f\u7528\u591a\u6a21\u6001\u6570\u636e\u96c6\uff08\u4e3b\u6570\u636e\u96c6\u6765\u81ea\u6cd5\u56fd\u5e03\u96f7\u65af\u7279\uff0c\u8f85\u52a9\u6570\u636e\u96c6\u6765\u81ea\u963f\u5c14\u53ca\u5229\u4e9a\uff09\uff0c35\u4e2a\u56e2\u961f\u53c2\u4e0e\u5e76\u63d0\u51fa\u65b9\u6cd5\u3002", "result": "AI\u5728\u6d4b\u91cfAMD\u8fdb\u5c55\u4e0a\u4e0e\u533b\u751f\u8868\u73b0\u76f8\u5f53\uff08\u4efb\u52a11\uff09\uff0c\u4f46\u65e0\u6cd5\u9884\u6d4b\u672a\u6765\u6f14\u53d8\uff08\u4efb\u52a12\uff09\u3002", "conclusion": "\u8be5\u6311\u6218\u4e3a\u57fa\u4e8eOCT\u548c\u5176\u4ed6\u4e34\u5e8a\u6570\u636e\u7684AMD\u76d1\u6d4b\u8bbe\u7acb\u4e86\u57fa\u51c6\uff0c\u8868\u660eAI\u5728\u8fdb\u5c55\u6d4b\u91cf\u4e0a\u6709\u6548\u4f46\u5728\u9884\u6d4b\u65b9\u9762\u4ecd\u6709\u5c40\u9650\u3002", "keywords": "AMD, OCT, AI, MICCAI, \u591a\u6a21\u6001\u6570\u636e"}}
{"id": "2506.03013", "pdf": "https://arxiv.org/pdf/2506.03013", "abs": "https://arxiv.org/abs/2506.03013", "authors": ["Alexandra Gonz\u00e1lez", "Xavier Franch", "David Lo", "Silverio Mart\u00ednez-Fern\u00e1ndez"], "title": "How do Pre-Trained Models Support Software Engineering? An Empirical Study in Hugging Face", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "Open-Source Pre-Trained Models (PTMs) provide extensive resources for various\nMachine Learning (ML) tasks, yet these resources lack a classification tailored\nto Software Engineering (SE) needs. To address this gap, we derive a taxonomy\nencompassing 147 SE tasks and apply an SE-oriented classification to PTMs in a\npopular open-source ML repository, Hugging Face (HF). Our repository mining\nstudy began with a systematically gathered database of PTMs from the HF API,\nconsidering their model card descriptions and metadata, and the abstract of the\nassociated arXiv papers. We confirmed SE relevance through multiple filtering\nsteps: detecting outliers, identifying near-identical PTMs, and the use of\nGemini 2.0 Flash, which was validated with five pilot studies involving three\nhuman annotators. This approach uncovered 2,205 SE PTMs. We find that code\ngeneration is the most common SE task among PTMs, primarily focusing on\nsoftware implementation, while requirements engineering and software design\nactivities receive limited attention. In terms of ML tasks, text generation\ndominates within SE PTMs. Notably, the number of SE PTMs has increased markedly\nsince 2023 Q2. Our classification provides a solid foundation for future\nautomated SE scenarios, such as the sampling and selection of suitable PTMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\uff08SE\uff09\u4efb\u52a1\u7684\u5f00\u6e90\u9884\u8bad\u7ec3\u6a21\u578b\uff08PTMs\uff09\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5206\u6790Hugging Face\uff08HF\uff09\u8d44\u6e90\u5e93\u4e2d\u7684PTMs\uff0c\u8bc6\u522b\u51fa2,205\u4e2a\u76f8\u5173\u6a21\u578b\u3002\u7814\u7a76\u53d1\u73b0\u4ee3\u7801\u751f\u6210\u662fSE\u4efb\u52a1\u4e2d\u6700\u5e38\u89c1\u7684\u7c7b\u578b\uff0c\u800c\u6587\u672c\u751f\u6210\u5728ML\u4efb\u52a1\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002", "motivation": "\u73b0\u6709\u7684\u5f00\u6e90\u9884\u8bad\u7ec3\u6a21\u578b\u8d44\u6e90\u7f3a\u4e4f\u9488\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u5206\u7c7b\uff0c\u5bfc\u81f4SE\u9886\u57df\u7684\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u96be\u4ee5\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u6536\u96c6HF API\u4e2d\u7684PTMs\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u6a21\u578b\u5361\u7247\u63cf\u8ff0\u3001\u5143\u6570\u636e\u548c\u76f8\u5173\u8bba\u6587\u6458\u8981\uff0c\u5229\u7528\u591a\u6b65\u8fc7\u6ee4\u6cd5\uff08\u5305\u62ec\u5f02\u5e38\u503c\u68c0\u6d4b\u3001\u53bb\u91cd\u548cGemini 2.0 Flash\u9a8c\u8bc1\uff09\u786e\u8ba4SE\u76f8\u5173\u6027\u3002", "result": "\u8bc6\u522b\u51fa2,205\u4e2aSE\u76f8\u5173\u7684PTMs\uff0c\u53d1\u73b0\u4ee3\u7801\u751f\u6210\u662fSE\u4efb\u52a1\u4e2d\u6700\u5e38\u89c1\u7684\u7c7b\u578b\uff0c\u800c\u9700\u6c42\u5de5\u7a0b\u548c\u8f6f\u4ef6\u8bbe\u8ba1\u76f8\u5173\u6a21\u578b\u8f83\u5c11\uff1bML\u4efb\u52a1\u4e2d\u4ee5\u6587\u672c\u751f\u6210\u4e3a\u4e3b\u5bfc\u30022023\u5e74Q2\u4ee5\u6765SE PTMs\u6570\u91cf\u663e\u8457\u589e\u52a0\u3002", "conclusion": "\u8be5\u5206\u7c7b\u65b9\u6cd5\u4e3a\u672a\u6765\u81ea\u52a8\u5316SE\u573a\u666f\u4e2d\u7684PTMs\u91c7\u6837\u548c\u9009\u62e9\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u540c\u65f6\u6307\u51fa\u4e86SE\u4efb\u52a1\u4e0eML\u4efb\u52a1\u5728PTMs\u4e2d\u7684\u5206\u5e03\u5dee\u5f02\u3002", "keywords": "\u9884\u8bad\u7ec3\u6a21\u578b\uff08PTMs\uff09\uff0c\u8f6f\u4ef6\u5de5\u7a0b\uff08SE\uff09\uff0cHugging Face\uff08HF\uff09\uff0c\u4ee3\u7801\u751f\u6210\uff0c\u6587\u672c\u751f\u6210"}}
{"id": "2506.03044", "pdf": "https://arxiv.org/pdf/2506.03044", "abs": "https://arxiv.org/abs/2506.03044", "authors": ["Laurentiu Andrei Marchis", "Po-Ling Loh"], "title": "On the Benefits of Accelerated Optimization in Robust and Private Estimation", "categories": ["math.ST", "cs.LG", "stat.ML", "stat.TH", "62F10, 62J05, 62J07, 62F35, 68P27"], "comment": "91 pages, 8 figures", "summary": "We study the advantages of accelerated gradient methods, specifically based\non the Frank-Wolfe method and projected gradient descent, for privacy and\nheavy-tailed robustness. Our approaches are as follows: For the Frank-Wolfe\nmethod, our technique is based on a tailored learning rate and a uniform lower\nbound on the gradient of the $\\ell_2$-norm over the constraint set. For\naccelerating projected gradient descent, we use the popular variant based on\nNesterov's momentum, and we optimize our objective over $\\mathbb{R}^p$. These\naccelerations reduce iteration complexity, translating into stronger\nstatistical guarantees for empirical and population risk minimization. Our\nanalysis covers three settings: non-random data, random model-free data, and\nparametric models (linear regression and generalized linear models).\nMethodologically, we approach both privacy and robustness based on noisy\ngradients. We ensure differential privacy via the Gaussian mechanism and\nadvanced composition, and we achieve heavy-tailed robustness using a geometric\nmedian-of-means estimator, which also sharpens the dependency on the dimension\nof the covariates. Finally, we compare our rates to existing bounds and\nidentify scenarios where our methods attain optimal convergence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u52a0\u901f\u68af\u5ea6\u65b9\u6cd5\uff08\u5982Frank-Wolfe\u548c\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\uff09\u5728\u9690\u79c1\u548c\u91cd\u5c3e\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u5b9a\u5236\u7684\u5b66\u4e60\u7387\u548c\u68af\u5ea6\u4e0b\u754c\u964d\u4f4e\u8fed\u4ee3\u590d\u6742\u5ea6\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5728\u975e\u968f\u673a\u6570\u636e\u3001\u968f\u673a\u6a21\u578b\u548c\u65e0\u53c2\u6570\u6a21\u578b\u4e2d\u7684\u7edf\u8ba1\u4fdd\u8bc1\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u52a0\u901f\u68af\u5ea6\u65b9\u6cd5\u63d0\u5347\u9690\u79c1\u4fdd\u62a4\u548c\u91cd\u5c3e\u6570\u636e\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u8bc1\u548c\u603b\u4f53\u98ce\u9669\u6700\u5c0f\u5316\u63d0\u4f9b\u66f4\u5f3a\u7684\u7edf\u8ba1\u4fdd\u8bc1\u3002", "method": "Frank-Wolfe\u65b9\u6cd5\u91c7\u7528\u5b9a\u5236\u5b66\u4e60\u7387\u548c\u68af\u5ea6\u4e0b\u754c\uff0c\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u4f7f\u7528Nesterov\u52a8\u91cf\u53d8\u4f53\uff0c\u5e76\u901a\u8fc7\u9ad8\u65af\u673a\u5236\u786e\u4fdd\u5dee\u5206\u9690\u79c1\uff0c\u51e0\u4f55\u4e2d\u4f4d\u6570\u5747\u503c\u4f30\u8ba1\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u52a0\u901f\u65b9\u6cd5\u964d\u4f4e\u4e86\u8fed\u4ee3\u590d\u6742\u5ea6\uff0c\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u8bbe\u7f6e\u4e2d\u9a8c\u8bc1\u4e86\u6548\u679c\uff0c\u90e8\u5206\u60c5\u51b5\u4e0b\u8fbe\u5230\u6700\u4f18\u6536\u655b\u7387\u3002", "conclusion": "\u52a0\u901f\u68af\u5ea6\u65b9\u6cd5\u5728\u9690\u79c1\u548c\u91cd\u5c3e\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6570\u636e\u573a\u666f\uff0c\u90e8\u5206\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "keywords": "\u52a0\u901f\u68af\u5ea6\u65b9\u6cd5, Frank-Wolfe, \u6295\u5f71\u68af\u5ea6\u4e0b\u964d, \u5dee\u5206\u9690\u79c1, \u91cd\u5c3e\u9c81\u68d2\u6027"}}
{"id": "2506.03022", "pdf": "https://arxiv.org/pdf/2506.03022", "abs": "https://arxiv.org/abs/2506.03022", "authors": ["David McVicar", "Brian Avant", "Adrian Gould", "Diego Torrejon", "Charles Della Porta", "Ryan Mukherjee"], "title": "Smartflow: Enabling Scalable Spatiotemporal Geospatial Research", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "BlackSky introduces Smartflow, a cloud-based framework enabling scalable\nspatiotemporal geospatial research built on open-source tools and technologies.\nUsing STAC-compliant catalogs as a common input, heterogeneous geospatial data\ncan be processed into standardized datacubes for analysis and model training.\nModel experimentation is managed using a combination of tools, including\nClearML, Tensorboard, and Apache Superset. Underpinning Smartflow is\nKubernetes, which orchestrates the provisioning and execution of workflows to\nsupport both horizontal and vertical scalability. This combination of features\nmakes Smartflow well-suited for geospatial model development and analysis over\nlarge geographic areas, time scales, and expansive image archives.\n  We also present a novel neural architecture, built using Smartflow, to\nmonitor large geographic areas for heavy construction. Qualitative results\nbased on data from the IARPA Space-based Machine Automated Recognition\nTechnique (SMART) program are presented that show the model is capable of\ndetecting heavy construction throughout all major phases of development.", "AI": {"tldr": "BlackSky\u63a8\u51fa\u4e86Smartflow\uff0c\u4e00\u4e2a\u57fa\u4e8e\u4e91\u7684\u6846\u67b6\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u65f6\u7a7a\u5730\u7406\u7a7a\u95f4\u7814\u7a76\u3002\u8be5\u6846\u67b6\u5229\u7528\u5f00\u6e90\u5de5\u5177\u548c\u6280\u672f\uff0c\u901a\u8fc7STAC\u517c\u5bb9\u76ee\u5f55\u5904\u7406\u5f02\u6784\u5730\u7406\u7a7a\u95f4\u6570\u636e\u4e3a\u6807\u51c6\u5316\u6570\u636e\u7acb\u65b9\u4f53\uff0c\u5e76\u7ed3\u5408\u591a\u79cd\u5de5\u5177\u7ba1\u7406\u6a21\u578b\u5b9e\u9a8c\u3002\u5176\u6838\u5fc3\u4e3aKubernetes\uff0c\u652f\u6301\u6c34\u5e73\u4e0e\u5782\u76f4\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u5927\u8303\u56f4\u5730\u7406\u7a7a\u95f4\u5206\u6790\u548c\u6a21\u578b\u5f00\u53d1\u3002\u6587\u7ae0\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSmartflow\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u76d1\u6d4b\u5927\u8303\u56f4\u533a\u57df\u7684\u91cd\u578b\u5efa\u7b51\u3002", "motivation": "\u4e3a\u5730\u7406\u7a7a\u95f4\u7814\u7a76\u63d0\u4f9b\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6574\u5408\u5f02\u6784\u6570\u636e\u5e76\u652f\u6301\u9ad8\u6548\u7684\u6a21\u578b\u5f00\u53d1\u548c\u5b9e\u9a8c\u3002", "method": "\u4f7f\u7528STAC\u517c\u5bb9\u76ee\u5f55\u4f5c\u4e3a\u8f93\u5165\uff0c\u5904\u7406\u5f02\u6784\u5730\u7406\u7a7a\u95f4\u6570\u636e\u4e3a\u6807\u51c6\u5316\u6570\u636e\u7acb\u65b9\u4f53\uff0c\u7ed3\u5408ClearML\u3001Tensorboard\u7b49\u5de5\u5177\u7ba1\u7406\u6a21\u578b\u5b9e\u9a8c\uff0c\u4f9d\u6258Kubernetes\u5b9e\u73b0\u5de5\u4f5c\u6d41\u7f16\u6392\u4e0e\u6269\u5c55\u3002", "result": "\u5f00\u53d1\u4e86Smartflow\u6846\u67b6\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u91cd\u578b\u5efa\u7b51\u76d1\u6d4b\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u6a21\u578b\u80fd\u591f\u68c0\u6d4b\u6240\u6709\u4e3b\u8981\u5f00\u53d1\u9636\u6bb5\u7684\u91cd\u578b\u5efa\u7b51\u3002", "conclusion": "Smartflow\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5730\u7406\u7a7a\u95f4\u7814\u7a76\u548c\u6a21\u578b\u5f00\u53d1\u6846\u67b6\uff0c\u652f\u6301\u5927\u89c4\u6a21\u5206\u6790\u548c\u6269\u5c55\u3002", "keywords": ""}}
{"id": "2506.03049", "pdf": "https://arxiv.org/pdf/2506.03049", "abs": "https://arxiv.org/abs/2506.03049", "authors": ["Maria Walch"], "title": "Torsion in Persistent Homology and Neural Networks", "categories": ["math.AT", "cs.LG"], "comment": null, "summary": "We explore the role of torsion in hybrid deep learning models that\nincorporate topological data analysis, focusing on autoencoders. While most TDA\ntools use field coefficients, this conceals torsional features present in\ninteger homology. We show that torsion can be lost during encoding, altered in\nthe latent space, and in many cases, not reconstructed by standard decoders.\nUsing both synthetic and high-dimensional data, we evaluate torsion sensitivity\nto perturbations and assess its recoverability across several autoencoder\narchitectures. Our findings reveal key limitations of field-based approaches\nand underline the need for architectures or loss terms that preserve torsional\ninformation for robust data representation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u626d\u91cf\u5728\u7ed3\u5408\u62d3\u6251\u6570\u636e\u5206\u6790\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u4f5c\u7528\uff0c\u63ed\u793a\u4e86\u57fa\u4e8e\u57df\u7cfb\u6570\u7684TDA\u5de5\u5177\u53ef\u80fd\u9690\u85cf\u6574\u6570\u540c\u8c03\u4e2d\u7684\u626d\u91cf\u7279\u5f81\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u81ea\u7f16\u7801\u5668\u4e2d\u7684\u4e22\u5931\u3001\u6539\u53d8\u53ca\u672a\u91cd\u5efa\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793a\u57fa\u4e8e\u57df\u7cfb\u6570\u7684\u62d3\u6251\u6570\u636e\u5206\u6790\u5de5\u5177\u5728\u5904\u7406\u626d\u91cf\u7279\u5f81\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u4fdd\u7559\u626d\u91cf\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002", "method": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u9ad8\u7ef4\u6570\u636e\uff0c\u8bc4\u4f30\u626d\u91cf\u5bf9\u6270\u52a8\u7684\u654f\u611f\u6027\uff0c\u5e76\u5728\u591a\u79cd\u81ea\u7f16\u7801\u5668\u67b6\u6784\u4e2d\u5206\u6790\u5176\u53ef\u6062\u590d\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u57df\u7cfb\u6570\u7684\u65b9\u6cd5\u5728\u4fdd\u7559\u626d\u91cf\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u5173\u952e\u5c40\u9650\uff0c\u9700\u8981\u65b0\u7684\u67b6\u6784\u6216\u635f\u5931\u51fd\u6570\u4ee5\u786e\u4fdd\u626d\u91cf\u4fe1\u606f\u7684\u9c81\u68d2\u8868\u793a\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u4fdd\u7559\u626d\u91cf\u4fe1\u606f\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "keywords": "\u626d\u91cf, \u62d3\u6251\u6570\u636e\u5206\u6790, \u81ea\u7f16\u7801\u5668, \u6574\u6570\u540c\u8c03, \u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2506.03046", "pdf": "https://arxiv.org/pdf/2506.03046", "abs": "https://arxiv.org/abs/2506.03046", "authors": ["Mikolaj Walczak", "Romina Aalishah", "Wyatt Mackey", "Brittany Story", "David L. Boothe Jr.", "Nicholas Waytowich", "Xiaomin Lin", "Tinoosh Mohsenin"], "title": "EDEN: Entorhinal Driven Egocentric Navigation Toward Robotic Deployment", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Deep reinforcement learning agents are often fragile while humans remain\nadaptive and flexible to varying scenarios. To bridge this gap, we present\nEDEN, a biologically inspired navigation framework that integrates learned\nentorhinal-like grid cell representations and reinforcement learning to enable\nautonomous navigation. Inspired by the mammalian entorhinal-hippocampal system,\nEDEN allows agents to perform path integration and vector-based navigation\nusing visual and motion sensor data. At the core of EDEN is a grid cell encoder\nthat transforms egocentric motion into periodic spatial codes, producing\nlow-dimensional, interpretable embeddings of position. To generate these\nactivations from raw sensory input, we combine fiducial marker detections in\nthe lightweight MiniWorld simulator and DINO-based visual features in the\nhigh-fidelity Gazebo simulator. These spatial representations serve as input to\na policy trained with Proximal Policy Optimization (PPO), enabling dynamic,\ngoal-directed navigation. We evaluate EDEN in both MiniWorld, for rapid\nprototyping, and Gazebo, which offers realistic physics and perception noise.\nCompared to baseline agents using raw state inputs (e.g., position, velocity)\nor standard convolutional image encoders, EDEN achieves a 99% success rate,\nwithin the simple scenarios, and >94% within complex floorplans with occluded\npaths with more efficient and reliable step-wise navigation. In addition, as a\nreplacement of ground truth activations, we present a trainable Grid Cell\nencoder enabling the development of periodic grid-like patterns from vision and\nmotion sensor data, emulating the development of such patterns within\nbiological mammals. This work represents a step toward biologically grounded\nspatial intelligence in robotics, bridging neural navigation principles with\nreinforcement learning for scalable deployment.", "AI": {"tldr": "EDEN\u6846\u67b6\u7ed3\u5408\u751f\u7269\u542f\u53d1\u7684\u7f51\u683c\u7ec6\u80de\u8868\u793a\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u9ad8\u6548\u81ea\u4e3b\u5bfc\u822a\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u9002\u5e94\u6027\u4e0a\u4e0d\u5982\u4eba\u7c7b\uff0cEDEN\u65e8\u5728\u901a\u8fc7\u751f\u7269\u542f\u53d1\u7684\u65b9\u6cd5\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u7ed3\u5408\u7f51\u683c\u7ec6\u80de\u7f16\u7801\u5668\u4e0ePPO\u7b56\u7565\uff0c\u5229\u7528\u89c6\u89c9\u548c\u8fd0\u52a8\u4f20\u611f\u5668\u6570\u636e\u8fdb\u884c\u8def\u5f84\u6574\u5408\u548c\u5411\u91cf\u5bfc\u822a\u3002", "result": "EDEN\u5728\u7b80\u5355\u573a\u666f\u4e2d\u6210\u529f\u738799%\uff0c\u590d\u6742\u573a\u666f\u4e2d>94%\uff0c\u5bfc\u822a\u66f4\u9ad8\u6548\u53ef\u9760\u3002", "conclusion": "EDEN\u4e3a\u673a\u5668\u4eba\u7a7a\u95f4\u667a\u80fd\u63d0\u4f9b\u751f\u7269\u5b66\u57fa\u7840\uff0c\u5c06\u795e\u7ecf\u5bfc\u822a\u539f\u7406\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u7f51\u683c\u7ec6\u80de, \u81ea\u4e3b\u5bfc\u822a, \u751f\u7269\u542f\u53d1, PPO"}}
{"id": "2506.03065", "pdf": "https://arxiv.org/pdf/2506.03065", "abs": "https://arxiv.org/abs/2506.03065", "authors": ["Pengtao Chen", "Xianfang Zeng", "Maosen Zhao", "Peng Ye", "Mingzhu Shen", "Wei Cheng", "Gang Yu", "Tao Chen"], "title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video\ngeneration, this long sequence generation task remains constrained by the\nquadratic complexity of attention mechanisms, resulting in significant\ninference latency. Through detailed analysis of attention maps in Video\nDiffusion Transformer (vDiT), we identify three recurring sparsity patterns:\ndiagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\%\nattention heads can be skipped. Crucially, these patterns exhibit strong\nlayer-depth and head-position correlations but show limited dependence on the\ninput content. Leveraging these findings, we propose Sparse-vDiT, a sparsity\nacceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels\nthat replace dense attention with computationally efficient implementations for\neach identified sparsity pattern. 2) An offline sparse diffusion search\nalgorithm that selects the optimal sparse computation strategy per layer and\nhead via hardware-aware cost modeling. After determining the optimal\nconfiguration, we fuse heads within the same layer that share the same\nattention strategy, enhancing inference efficiency. Integrated into\nstate-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),\nSparse-vDiT achieves 2.09$\\times$, 2.38$\\times$, and 1.67$\\times$ theoretical\nFLOP reduction, and actual inference speedups of 1.76$\\times$, 1.85$\\times$,\nand 1.58$\\times$, respectively, while maintaining high visual fidelity, with\nPSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent\nstructural sparsity in vDiTs can be systematically exploited for long video\nsynthesis.", "AI": {"tldr": "Sparse-vDiT \u662f\u4e00\u79cd\u9488\u5bf9\u89c6\u9891\u6269\u6563 Transformer (vDiT) \u7684\u7a00\u758f\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u5229\u7528\u6ce8\u610f\u529b\u56fe\u4e2d\u7684\u7a00\u758f\u6a21\u5f0f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u8f93\u51fa\u3002", "motivation": "\u4f20\u7edf Diffusion Transformers (DiTs) \u5728\u751f\u6210\u957f\u89c6\u9891\u5e8f\u5217\u65f6\uff0c\u56e0\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u9ad8\u3002\u8bba\u6587\u901a\u8fc7\u5206\u6790 vDiT \u7684\u6ce8\u610f\u529b\u56fe\uff0c\u53d1\u73b0\u53ef\u5229\u7528\u7684\u7a00\u758f\u6a21\u5f0f\uff0c\u4ee5\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "method": "1) \u8bbe\u8ba1\u9488\u5bf9\u7a00\u758f\u6a21\u5f0f\u7684\u4f18\u5316\u5185\u6838\uff0c\u66ff\u4ee3\u5bc6\u96c6\u6ce8\u610f\u529b\u8ba1\u7b97\uff1b2) \u63d0\u51fa\u79bb\u7ebf\u7a00\u758f\u6269\u6563\u641c\u7d22\u7b97\u6cd5\uff0c\u57fa\u4e8e\u786c\u4ef6\u611f\u77e5\u9009\u62e9\u6700\u4f18\u7a00\u758f\u8ba1\u7b97\u7b56\u7565\uff1b3) \u5934\u878d\u5408\u6280\u672f\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u7387\u3002", "result": "Sparse-vDiT \u5728\u591a\u4e2a\u5148\u8fdb vDiT \u6a21\u578b\uff08CogVideoX1.5\u3001HunyuanVideo\u3001Wan2.1\uff09\u4e2d\u5b9e\u73b0\u4e86 1.58-1.85 \u500d\u5b9e\u9645\u63a8\u7406\u52a0\u901f\uff0c\u7406\u8bba FLOP \u964d\u4f4e 1.67-2.38 \u500d\uff0cPSNR \u503c\u8fbe 22.59-27.09\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e vDiT \u4e2d\u6f5c\u5728\u7684\u7a00\u758f\u7ed3\u6784\u53ef\u88ab\u7cfb\u7edf\u5229\u7528\u4ee5\u4f18\u5316\u957f\u89c6\u9891\u5408\u6210\u6548\u7387\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u52a0\u901f\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "Diffusion Transformers, \u89c6\u9891\u751f\u6210, \u7a00\u758f\u6ce8\u610f\u529b, \u52a0\u901f\u6846\u67b6, FLOP \u4f18\u5316"}}
{"id": "2506.03068", "pdf": "https://arxiv.org/pdf/2506.03068", "abs": "https://arxiv.org/abs/2506.03068", "authors": ["Yina Hou", "Shourav B. Rabbani", "Liang Hong", "Norou Diawara", "Manar D. Samad"], "title": "Causal Explainability of Machine Learning in Heart Failure Prediction from Electronic Health Records", "categories": ["stat.ML", "cs.CY", "cs.LG"], "comment": "4 figures", "summary": "The importance of clinical variables in the prognosis of the disease is\nexplained using statistical correlation or machine learning (ML). However, the\npredictive importance of these variables may not represent their causal\nrelationships with diseases. This paper uses clinical variables from a heart\nfailure (HF) patient cohort to investigate the causal explainability of\nimportant variables obtained in statistical and ML contexts. Due to inherent\nregression modeling, popular causal discovery methods strictly assume that the\ncause and effect variables are numerical and continuous. This paper proposes a\nnew computational framework to enable causal structure discovery (CSD) and\nscore the causal strength of mixed-type (categorical, numerical, binary)\nclinical variables for binary disease outcomes. In HF classification, we\ninvestigate the association between the importance rank order of three feature\ntypes: correlated features, features important for ML predictions, and causal\nfeatures. Our results demonstrate that CSD modeling for nonlinear causal\nrelationships is more meaningful than its linear counterparts. Feature\nimportance obtained from nonlinear classifiers (e.g., gradient-boosting trees)\nstrongly correlates with the causal strength of variables without\ndifferentiating cause and effect variables. Correlated variables can be causal\nfor HF, but they are rarely identified as effect variables. These results can\nbe used to add the causal explanation of variables important for ML-based\nprediction modeling.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u65b0\u65b9\u6cd5\u7528\u4e8e\u63a2\u7d22\u6df7\u5408\u7c7b\u578b\u4e34\u5e8a\u53d8\u91cf\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u53d1\u73b0\u975e\u7ebf\u6027\u56e0\u679c\u6a21\u578b\u6bd4\u7ebf\u6027\u6a21\u578b\u66f4\u6709\u610f\u4e49\uff0c\u4e14\u7279\u5f81\u91cd\u8981\u6027\u6392\u5e8f\u4e0e\u56e0\u679c\u5f3a\u5ea6\u76f8\u5173\u3002", "motivation": "\u63a2\u8ba8\u4e34\u5e8a\u53d8\u91cf\u5728\u75be\u75c5\u9884\u540e\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5f25\u8865\u7edf\u8ba1\u76f8\u5173\u6027\u548cML\u9884\u6d4b\u91cd\u8981\u6027\u65e0\u6cd5\u53cd\u6620\u56e0\u679c\u5173\u7cfb\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8ba1\u7b97\u6846\u67b6\uff0c\u652f\u6301\u6df7\u5408\u7c7b\u578b\u53d8\u91cf\u7684\u56e0\u679c\u7ed3\u6784\u53d1\u73b0\u548c\u5f3a\u5ea6\u8bc4\u5206\uff0c\u5e94\u7528\u4e8e\u5fc3\u8870\u60a3\u8005\u961f\u5217\u3002", "result": "\u975e\u7ebf\u6027\u56e0\u679c\u6a21\u578b\u66f4\u6709\u610f\u4e49\uff1bML\u7279\u5f81\u91cd\u8981\u6027\u4e0e\u56e0\u679c\u5f3a\u5ea6\u5f3a\u76f8\u5173\uff1b\u76f8\u5173\u53d8\u91cf\u53ef\u80fd\u662f\u539f\u56e0\u4f46\u5f88\u5c11\u662f\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u4e3aML\u9884\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u53d8\u91cf\u7684\u56e0\u679c\u89e3\u91ca\u6846\u67b6\u3002", "keywords": "\u56e0\u679c\u53d1\u73b0, \u5fc3\u8870, \u6df7\u5408\u7c7b\u578b\u53d8\u91cf, ML\u91cd\u8981\u6027, \u975e\u7ebf\u6027\u6a21\u578b"}}
{"id": "2506.03074", "pdf": "https://arxiv.org/pdf/2506.03074", "abs": "https://arxiv.org/abs/2506.03074", "authors": ["Junghyun Lee", "Kyoungseok Jang", "Kwang-Sung Jun", "Milan Vojnovi\u0107", "Se-Young Yun"], "title": "GL-LowPopArt: A Nearly Instance-Wise Minimax Estimator for Generalized Low-Rank Trace Regression", "categories": ["stat.ML", "cs.LG"], "comment": "53 pages, 2 figures, 3 tables; Accepted as a Spotlight Poster to the\n  42nd International Conference on Machine Learning (ICML 2025)", "summary": "We present `GL-LowPopArt`, a novel Catoni-style estimator for generalized\nlow-rank trace regression. Building on `LowPopArt` (Jang et al., 2024), it\nemploys a two-stage approach: nuclear norm regularization followed by matrix\nCatoni estimation. We establish state-of-the-art estimation error bounds,\nsurpassing existing guarantees (Fan et al., 2019; Kang et al., 2022), and\nreveal a novel experimental design objective, $\\mathrm{GL}(\\pi)$. The key\ntechnical challenge is controlling bias from the nonlinear inverse link\nfunction, which we address by our two-stage approach. We prove a *local*\nminimax lower bound, showing that our `GL-LowPopArt` enjoys instance-wise\noptimality up to the condition number of the ground-truth Hessian. Applications\ninclude generalized linear matrix completion, where `GL-LowPopArt` achieves a\nstate-of-the-art Frobenius error guarantee, and **bilinear dueling bandits**, a\nnovel setting inspired by general preference learning (Zhang et al., 2024). Our\nanalysis of a `GL-LowPopArt`-based explore-then-commit algorithm reveals a new,\npotentially interesting problem-dependent quantity, along with improved Borda\nregret bound than vectorization (Wu et al., 2024).", "AI": {"tldr": "GL-LowPopArt\u662f\u4e00\u79cd\u65b0\u578b\u7684Catoni\u5f0f\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u5e7f\u4e49\u4f4e\u79e9\u8ff9\u56de\u5f52\u3002\u5b83\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u6838\u8303\u6570\u6b63\u5219\u5316\u548c\u77e9\u9635Catoni\u4f30\u8ba1\uff0c\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u4f30\u8ba1\u8bef\u5dee\u754c\u9650\uff0c\u5e76\u5728\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5e7f\u4e49\u4f4e\u79e9\u8ff9\u56de\u5f52\u4e2d\u7684\u4f30\u8ba1\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u975e\u7ebf\u6027\u9006\u94fe\u63a5\u51fd\u6570\u5f15\u5165\u7684\u504f\u5dee\u63a7\u5236\u4e0a\u63d0\u51fa\u521b\u65b0\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u6838\u8303\u6570\u6b63\u5219\u5316\uff08\u7b2c\u4e00\u9636\u6bb5\uff09\u548c\u77e9\u9635Catoni\u4f30\u8ba1\uff08\u7b2c\u4e8c\u9636\u6bb5\uff09\uff0c\u4ee5\u5e94\u5bf9\u975e\u7ebf\u6027\u9006\u94fe\u63a5\u51fd\u6570\u7684\u504f\u5dee\u3002", "result": "GL-LowPopArt\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u4f30\u8ba1\u8bef\u5dee\u754c\u9650\uff0c\u5e76\u5728\u5e7f\u4e49\u7ebf\u6027\u77e9\u9635\u5b8c\u6210\u548c\u53cc\u7ebf\u6027\u5bf9\u51b3\u8d4c\u535a\u7b49\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "GL-LowPopArt\u5728\u5b9e\u4f8b\u4f18\u5316\u548c\u8bef\u5dee\u754c\u9650\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5e7f\u4e49\u4f4e\u79e9\u8ff9\u56de\u5f52\u3001Catoni\u4f30\u8ba1\u5668\u3001\u6838\u8303\u6570\u6b63\u5219\u5316\u3001\u975e\u7ebf\u6027\u9006\u94fe\u63a5\u51fd\u6570\u3001\u53cc\u7ebf\u6027\u5bf9\u51b3\u8d4c\u535a"}}
{"id": "2506.03088", "pdf": "https://arxiv.org/pdf/2506.03088", "abs": "https://arxiv.org/abs/2506.03088", "authors": ["Lloyd Pellatt", "Fotios Drakopoulos", "Shievanie Sabesan", "Nicholas A. Lesica"], "title": "Modelling the Effects of Hearing Loss on Neural Coding in the Auditory Midbrain with Variational Conditioning", "categories": ["q-bio.NC", "cs.AI", "cs.LG"], "comment": "12 pages, 3 figures", "summary": "The mapping from sound to neural activity that underlies hearing is highly\nnon-linear. The first few stages of this mapping in the cochlea have been\nmodelled successfully, with biophysical models built by hand and, more\nrecently, with DNN models trained on datasets simulated by biophysical models.\nModelling the auditory brain has been a challenge because central auditory\nprocessing is too complex for models to be built by hand, and datasets for\ntraining DNN models directly have not been available. Recent work has taken\nadvantage of large-scale high resolution neural recordings from the auditory\nmidbrain to build a DNN model of normal hearing with great success. But this\nmodel assumes that auditory processing is the same in all brains, and therefore\nit cannot capture the widely varying effects of hearing loss.\n  We propose a novel variational-conditional model to learn to encode the space\nof hearing loss directly from recordings of neural activity in the auditory\nmidbrain of healthy and noise exposed animals. With hearing loss parametrised\nby only 6 free parameters per animal, our model accurately predicts 62\\% of the\nexplainable variance in neural responses from normal hearing animals and 68%\nfor hearing impaired animals, within a few percentage points of state of the\nart animal specific models. We demonstrate that the model can be used to\nsimulate realistic activity from out of sample animals by fitting only the\nlearned conditioning parameters with Bayesian optimisation, achieving\ncrossentropy loss within 2% of the optimum in 15-30 iterations. Including more\nanimals in the training data slightly improved the performance on unseen\nanimals. This model will enable future development of parametrised hearing loss\ncompensation models trained to directly restore normal neural coding in hearing\nimpaired brains, which can be quickly fitted for a new user by human in the\nloop optimisation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53d8\u5206\u6761\u4ef6\u6a21\u578b\uff0c\u901a\u8fc7\u5b66\u4e60\u4ece\u52a8\u7269\u542c\u89c9\u4e2d\u8111\u795e\u7ecf\u6d3b\u52a8\u4e2d\u76f4\u63a5\u7f16\u7801\u542c\u529b\u635f\u5931\u7684\u7a7a\u95f4\u3002\u6a21\u578b\u75286\u4e2a\u53c2\u6570\u5373\u53ef\u51c6\u786e\u9884\u6d4b\u6b63\u5e38\u548c\u542c\u529b\u53d7\u635f\u52a8\u7269\u7684\u795e\u7ecf\u54cd\u5e94\uff0c\u4e3a\u4e2a\u6027\u5316\u542c\u529b\u8865\u507f\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u542c\u89c9\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u542c\u529b\u635f\u5931\u4e2a\u4f53\u5dee\u5f02\u7684\u95ee\u9898\uff0c\u5e0c\u671b\u901a\u8fc7\u5b66\u4e60\u795e\u7ecf\u6d3b\u52a8\u6570\u636e\u76f4\u63a5\u5efa\u6a21\u542c\u529b\u635f\u5931\u7a7a\u95f4\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u6761\u4ef6\u6a21\u578b\uff0c\u4ece\u5065\u5eb7\u548c\u566a\u97f3\u66b4\u9732\u52a8\u7269\u7684\u542c\u89c9\u4e2d\u8111\u795e\u7ecf\u8bb0\u5f55\u4e2d\u5b66\u4e60\u542c\u529b\u635f\u5931\u7684\u53c2\u6570\u5316\u8868\u793a\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u4e8662%\u7684\u6b63\u5e38\u52a8\u7269\u548c68%\u542c\u529b\u53d7\u635f\u52a8\u7269\u7684\u53ef\u89e3\u91ca\u795e\u7ecf\u54cd\u5e94\u65b9\u5dee\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u5feb\u901f\u62df\u5408\u65b0\u52a8\u7269\u6570\u636e\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u672a\u6765\u5f00\u53d1\u4e2a\u6027\u5316\u542c\u529b\u8865\u507f\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u80fd\u591f\u5feb\u901f\u9002\u914d\u65b0\u7528\u6237\u3002", "keywords": "\u542c\u529b\u635f\u5931, \u53d8\u5206\u6761\u4ef6\u6a21\u578b, \u795e\u7ecf\u7f16\u7801, \u4e2a\u6027\u5316\u8865\u507f"}}
{"id": "2506.03083", "pdf": "https://arxiv.org/pdf/2506.03083", "abs": "https://arxiv.org/abs/2506.03083", "authors": ["Adrian de Wynter"], "title": "Labelling Data with Unknown References", "categories": ["cs.DS", "cs.AI"], "comment": null, "summary": "An evaluator is trustworthy when there exists some agreed-upon way to measure\nits performance as a labeller. The two ways to establish trustworthiness are\neither by testing it, or by assuming the evaluator `knows' somehow the way to\nlabel the corpus. However, if labelled references (e.g., a development set) are\nunavailable, neither of these approaches work: the former requires the data,\nand the latter is an assumption, not evidence. To address this, we introduce an\nalgorithm (the `No-Data Algorithm') by which to establish trust in an evaluator\nwithout any existing references. Our algorithm works by successively posing\nchallenges to said evaluator. We show that this is sufficient to establish\ntrustworthiness w.h.p., in such a way that when the evaluator actually knows\nthe way to label the corpus, the No-Data Algorithm accepts its output; and,\nconversely, flags untrustworthy evaluators when these are unable to prove it.\nWe present formal proofs of correctness and limited experiments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u53c2\u8003\u6570\u636e\u5373\u53ef\u8bc4\u4f30\u6807\u6ce8\u8005\u53ef\u4fe1\u5ea6\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u6311\u6218\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u7f3a\u4e4f\u6807\u6ce8\u53c2\u8003\u6570\u636e\u65f6\uff0c\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u9a8c\u8bc1\u6807\u6ce8\u8005\u7684\u53ef\u4fe1\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u2018No-Data Algorithm\u2019\uff0c\u901a\u8fc7\u5411\u6807\u6ce8\u8005\u63d0\u51fa\u8fde\u7eed\u6311\u6218\u6765\u9a8c\u8bc1\u5176\u53ef\u4fe1\u5ea6\u3002", "result": "\u7b97\u6cd5\u80fd\u591f\u9ad8\u6982\u7387\u5730\u9a8c\u8bc1\u6807\u6ce8\u8005\u7684\u53ef\u4fe1\u5ea6\uff0c\u6b63\u786e\u63a5\u53d7\u53ef\u9760\u6807\u6ce8\u8005\u7684\u8f93\u51fa\uff0c\u5e76\u6807\u8bb0\u4e0d\u53ef\u9760\u7684\u6807\u6ce8\u8005\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u65e0\u53c2\u8003\u6570\u636e\u573a\u666f\u4e0b\u7684\u8bc4\u4f30\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u9a8c\u8bc1\u3002", "keywords": "\u53ef\u4fe1\u5ea6\u8bc4\u4f30\uff0c\u6807\u6ce8\u7b97\u6cd5\uff0c\u65e0\u53c2\u8003\u6570\u636e\uff0c\u6311\u6218\u6d4b\u8bd5"}}
{"id": "2506.03096", "pdf": "https://arxiv.org/pdf/2506.03096", "abs": "https://arxiv.org/abs/2506.03096", "authors": ["Christian Schlarmann", "Francesco Croce", "Nicolas Flammarion", "Matthias Hein"], "title": "FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens", "categories": ["cs.CV", "cs.LG"], "comment": "Code and models available at https://github.com/chs20/fuselip", "summary": "Contrastive language-image pre-training aligns the features of text-image\npairs in a common latent space via distinct encoders for each modality. While\nthis approach achieves impressive performance in several zero-shot tasks, it\ncannot natively handle multimodal inputs, i.e., encoding image and text into a\nsingle feature vector. As a remedy, it is common practice to use additional\nmodules to merge the features extracted by the unimodal encoders. In this work,\nwe present FuseLIP, an alternative architecture for multimodal embedding.\nLeveraging recent progress in discrete image tokenizers, we propose to use a\nsingle transformer model which operates on an extended vocabulary of text and\nimage tokens. This early fusion approach allows the different modalities to\ninteract at each depth of encoding and obtain richer representations compared\nto common late fusion. We collect new datasets for multimodal pre-training and\nevaluation, designing challenging tasks for multimodal encoder models. We show\nthat FuseLIP outperforms other approaches in multimodal embedding tasks such as\nVQA and text-guided image transformation retrieval, while being comparable to\nbaselines on unimodal tasks.", "AI": {"tldr": "FuseLIP\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u5d4c\u5165\u67b6\u6784\uff0c\u901a\u8fc7\u5355\u4e00\u53d8\u538b\u5668\u6a21\u578b\u5904\u7406\u6587\u672c\u548c\u56fe\u50cf\u4ee4\u724c\uff0c\u5b9e\u73b0\u4e86\u65e9\u671f\u7684\u591a\u6a21\u6001\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\u65e0\u6cd5\u76f4\u63a5\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\uff0c\u9700\u8981\u4f7f\u7528\u989d\u5916\u6a21\u5757\u5408\u5e76\u7279\u5f81\u3002FuseLIP\u65e8\u5728\u901a\u8fc7\u5355\u4e00\u6a21\u578b\u5b9e\u73b0\u66f4\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u8868\u5f81\u3002", "method": "\u91c7\u7528\u5355\u4e00\u53d8\u538b\u5668\u6a21\u578b\u5904\u7406\u6269\u5c55\u540e\u7684\u6587\u672c\u548c\u56fe\u50cf\u4ee4\u724c\u8bcd\u6c47\u8868\uff0c\u5b9e\u73b0\u65e9\u671f\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u3002", "result": "FuseLIP\u5728\u591a\u6a21\u6001\u4efb\u52a1\uff08\u5982VQA\u548c\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u68c0\u7d22\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u5355\u6a21\u6001\u4efb\u52a1\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "FuseLIP\u901a\u8fc7\u65e9\u671f\u878d\u5408\u7b56\u7565\u5728\u591a\u6a21\u6001\u5d4c\u5165\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "keywords": "\u591a\u6a21\u6001\u5d4c\u5165,\u65e9\u671f\u878d\u5408,FuseLIP,\u5bf9\u6bd4\u5b66\u4e60,\u53d8\u538b\u5668\u6a21\u578b"}}
{"id": "2506.03120", "pdf": "https://arxiv.org/pdf/2506.03120", "abs": "https://arxiv.org/abs/2506.03120", "authors": ["Xiuyu Cao", "Joseph O. Sexton", "Panshi Wang", "Dimitrios Gounaridis", "Neil H. Carter", "Kai Zhu"], "title": "Validating remotely sensed biomass estimates with forest inventory data in the western US", "categories": ["stat.AP", "cs.LG"], "comment": "32 pages, 5 figures", "summary": "Monitoring aboveground biomass (AGB) and its density (AGBD) at high\nresolution is essential for carbon accounting and ecosystem management. While\nNASA's spaceborne Global Ecosystem Dynamics Investigation (GEDI) LiDAR mission\nprovides globally distributed reference measurements for AGBD estimation, the\nmajority of commercial remote sensing products based on GEDI remain without\nrigorous or independent validation. Here, we present an independent regional\nvalidation of an AGBD dataset offered by terraPulse, Inc., based on independent\nreference data from the US Forest Service Forest Inventory and Analysis (FIA)\nprogram. Aggregated to 64,000-hectare hexagons and US counties across the US\nstates of Utah, Nevada, and Washington, we found very strong agreement between\nterraPulse and FIA estimates. At the hexagon scale, we report R2 = 0.88, RMSE =\n26.68 Mg/ha, and a correlation coefficient (r) of 0.94. At the county scale,\nagreement improves to R2 = 0.90, RMSE =32.62 Mg/ha, slope = 1.07, and r = 0.95.\nSpatial and statistical analyses indicated that terraPulse AGBD values tended\nto exceed FIA estimates in non-forest areas, likely due to FIA's limited\nsampling of non-forest vegetation. The terraPulse AGBD estimates also exhibited\nlower values in high-biomass forests, likely due to saturation effects in its\noptical remote-sensing covariates. This study advances operational carbon\nmonitoring by delivering a scalable framework for comprehensive AGBD validation\nusing independent FIA data, as well as a benchmark validation of a new\ncommercial dataset for global biomass monitoring.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u4f7f\u7528\u7f8e\u56fd\u68ee\u6797\u670d\u52a1\u7684\u6570\u636e\u72ec\u7acb\u9a8c\u8bc1\u4e86terraPulse\u516c\u53f8\u7684\u5730\u4e0a\u751f\u7269\u91cf\u5bc6\u5ea6(AGBD)\u6570\u636e\u96c6\uff0c\u7ed3\u679c\u663e\u793a\u4e24\u8005\u5728\u68ee\u6797\u548c\u975e\u68ee\u6797\u533a\u57df\u5747\u5b58\u5728\u4e00\u5b9a\u5dee\u5f02\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u76d1\u6d4b\u5730\u4e0a\u751f\u7269\u91cf(AGB)\u53ca\u5176\u5bc6\u5ea6(AGBD)\u5bf9\u78b3\u6838\u7b97\u548c\u751f\u6001\u7cfb\u7edf\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u57fa\u4e8eGEDI\u7684\u5546\u4e1a\u9065\u611f\u4ea7\u54c1\u7f3a\u4e4f\u72ec\u7acb\u9a8c\u8bc1\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u7f8e\u56fd\u68ee\u6797\u670d\u52a1(FIA)\u7684\u72ec\u7acb\u6570\u636e\uff0c\u5bf9terraPulse\u7684AGBD\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u533a\u57df\u9a8c\u8bc1\uff0c\u6bd4\u8f83\u4e8664,000\u516c\u9877\u516d\u8fb9\u5f62\u548c\u53bf\u7ea7\u5c3a\u5ea6\u7684\u4f30\u8ba1\u503c\u3002", "result": "\u5728\u516d\u8fb9\u5f62\u5c3a\u5ea6\u4e0a\uff0cR\u00b2=0.88\uff0cRMSE=26.68 Mg/ha\uff1b\u53bf\u7ea7\u5c3a\u5ea6\u4e0a\uff0cR\u00b2=0.90\uff0cRMSE=32.62 Mg/ha\u3002\u7ed3\u679c\u663e\u793aterraPulse\u5728\u975e\u68ee\u6797\u533a\u57df\u53ef\u80fd\u9ad8\u4f30\uff0c\u5728\u9ad8\u751f\u7269\u91cf\u68ee\u6797\u53ef\u80fd\u4f4e\u4f30\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684AGBD\u9a8c\u8bc1\u6846\u67b6\uff0c\u5e76\u4e3a\u5168\u7403\u751f\u7269\u91cf\u76d1\u6d4b\u7684\u5546\u4e1a\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u57fa\u51c6\u9a8c\u8bc1\u3002", "keywords": "\u5730\u4e0a\u751f\u7269\u91cf, GEDI, \u9065\u611f, \u68ee\u6797\u670d\u52a1, \u78b3\u6838\u7b97"}}
{"id": "2506.03131", "pdf": "https://arxiv.org/pdf/2506.03131", "abs": "https://arxiv.org/abs/2506.03131", "authors": ["Zidong Wang", "Lei Bai", "Xiangyu Yue", "Wanli Ouyang", "Yiyuan Zhang"], "title": "Native-Resolution Image Synthesis", "categories": ["cs.CV", "cs.LG"], "comment": "Project Page: https://wzdthu.github.io/NiT/", "summary": "We introduce native-resolution image synthesis, a novel generative modeling\nparadigm that enables the synthesis of images at arbitrary resolutions and\naspect ratios. This approach overcomes the limitations of conventional\nfixed-resolution, square-image methods by natively handling variable-length\nvisual tokens, a core challenge for traditional techniques. To this end, we\nintroduce the Native-resolution diffusion Transformer (NiT), an architecture\ndesigned to explicitly model varying resolutions and aspect ratios within its\ndenoising process. Free from the constraints of fixed formats, NiT learns\nintrinsic visual distributions from images spanning a broad range of\nresolutions and aspect ratios. Notably, a single NiT model simultaneously\nachieves the state-of-the-art performance on both ImageNet-256x256 and 512x512\nbenchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in\nadvanced large language models, NiT, trained solely on ImageNet, demonstrates\nexcellent zero-shot generalization performance. It successfully generates\nhigh-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)\nand diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These\nfindings indicate the significant potential of native-resolution modeling as a\nbridge between visual generative modeling and advanced LLM methodologies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u539f\u751f\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u7684\u65b0\u65b9\u6cd5\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u56fa\u5b9a\u5206\u8fa8\u7387\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u80fd\u591f\u751f\u6210\u4efb\u610f\u5206\u8fa8\u7387\u548c\u5bbd\u9ad8\u6bd4\u7684\u56fe\u50cf\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5c40\u9650\u4e8e\u56fa\u5b9a\u5206\u8fa8\u7387\u548c\u6b63\u65b9\u5f62\u56fe\u50cf\uff0c\u65e0\u6cd5\u7075\u6d3b\u5904\u7406\u4e0d\u540c\u5206\u8fa8\u7387\u548c\u5bbd\u9ad8\u6bd4\u7684\u56fe\u50cf\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u539f\u751f\u5206\u8fa8\u7387\u6269\u6563Transformer\uff08NiT\uff09\uff0c\u5176\u53bb\u566a\u8fc7\u7a0b\u80fd\u591f\u663e\u5f0f\u5efa\u6a21\u4e0d\u540c\u5206\u8fa8\u7387\u548c\u5bbd\u9ad8\u6bd4\uff0c\u5b66\u4e60\u5e7f\u6cdb\u7684\u89c6\u89c9\u5206\u5e03\u3002", "result": "NiT\u5728ImageNet-256x256\u548c512x512\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u80fd\u96f6\u6837\u672c\u751f\u6210\u9ad8\u5206\u8fa8\u7387\uff08\u59821536x1536\uff09\u548c\u591a\u6837\u5316\u5bbd\u9ad8\u6bd4\uff08\u598216:9\u30013:1\u7b49\uff09\u7684\u9ad8\u4fdd\u771f\u56fe\u50cf\u3002", "conclusion": "\u539f\u751f\u5206\u8fa8\u7387\u5efa\u6a21\u5728\u89c6\u89c9\u751f\u6210\u5efa\u6a21\u4e0e\u5148\u8fdbLLM\u65b9\u6cd5\u4e4b\u95f4\u5177\u6709\u91cd\u8981\u6f5c\u529b\u3002", "keywords": "\u56fe\u50cf\u5408\u6210\u3001\u6269\u6563\u6a21\u578b\u3001Transformer\u3001\u539f\u751f\u5206\u8fa8\u7387\u3001\u96f6\u6837\u672c\u751f\u6210"}}
{"id": "2506.03097", "pdf": "https://arxiv.org/pdf/2506.03097", "abs": "https://arxiv.org/abs/2506.03097", "authors": ["Ashwin Vinod", "Shrey Pandit", "Aditya Vavre", "Linshen Liu"], "title": "EgoVLM: Policy Optimization for Egocentric Video Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "Our Code can be found at https://github.com/adityavavre/VidEgoVLM", "summary": "Emerging embodied AI applications, such as wearable cameras and autonomous\nagents, have underscored the need for robust reasoning from first person video\nstreams. We introduce EgoVLM, a vision-language model specifically designed to\nintegrate visual comprehension and spatial-temporal reasoning within egocentric\nvideo contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization\n(GRPO), a reinforcement learning method adapted to align model outputs with\nhuman-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly\ntune using RL without any supervised fine-tuning phase on chain-of-thought\n(CoT) data. We evaluate EgoVLM on egocentric video question answering\nbenchmarks and show that domain-specific training substantially improves\nperformance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on\nnon-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by\n14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By\nexplicitly generating reasoning traces, EgoVLM enhances interpretability,\nmaking it well-suited for downstream applications. Furthermore, we introduce a\nnovel keyframe-based reward that incorporates salient frame selection to guide\nreinforcement learning optimization. This reward formulation opens a promising\navenue for future exploration in temporally grounded egocentric reasoning.", "AI": {"tldr": "EgoVLM\u662f\u4e00\u79cd\u4e13\u4e3a\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u8bbe\u8ba1\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5GRPO\u63d0\u5347\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\uff0c\u5728EgoSchema\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u901a\u7528VLMs\u3002", "motivation": "\u7531\u4e8e\u53ef\u7a7f\u6234\u76f8\u673a\u548c\u81ea\u4e3b\u4ee3\u7406\u7b49\u65b0\u5174\u5e94\u7528\u7684\u9700\u6c42\uff0c\u9700\u8981\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u6d41\u4e2d\u8fdb\u884c\u9c81\u68d2\u7684\u63a8\u7406\u3002", "method": "\u4f7f\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5fae\u8c03EgoVLM\uff0c\u76f4\u63a5\u901a\u8fc7RL\u8bad\u7ec3\uff0c\u65e0\u9700\u76d1\u7763\u5fae\u8c03\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u5173\u952e\u5e27\u7684\u5956\u52b1\u3002", "result": "EgoVLM-3B\u5728EgoSchema\u57fa\u51c6\u4e0a\u6bd4Qwen2.5-VL 3B\u548c7B\u6a21\u578b\u5206\u522b\u9ad8\u51fa14.33\u548c13.87\u4e2a\u51c6\u786e\u70b9\u3002", "conclusion": "EgoVLM\u901a\u8fc7\u663e\u5f0f\u751f\u6210\u63a8\u7406\u8f68\u8ff9\u63d0\u5347\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u4e0b\u6e38\u5e94\u7528\uff0c\u5173\u952e\u5e27\u5956\u52b1\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u5411\u3002", "keywords": "EgoVLM, \u7b2c\u4e00\u4eba\u79f0\u89c6\u9891, \u5f3a\u5316\u5b66\u4e60, GRPO, \u65f6\u7a7a\u63a8\u7406, \u5173\u952e\u5e27\u5956\u52b1"}}
{"id": "2506.03099", "pdf": "https://arxiv.org/pdf/2506.03099", "abs": "https://arxiv.org/abs/2506.03099", "authors": ["Chetwin Low", "Weimin Wang"], "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models", "categories": ["cs.SD", "cs.AI", "cs.GR"], "comment": null, "summary": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/", "AI": {"tldr": "TalkingMachines\u662f\u4e00\u4e2a\u9ad8\u6548\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u6a21\u578b\u8f6c\u5316\u4e3a\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u89d2\u8272\u52a8\u753b\u5668\uff0c\u7ed3\u5408\u97f3\u9891LLM\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u63d0\u4f9b\u81ea\u7136\u5bf9\u8bdd\u4f53\u9a8c\u3002", "motivation": "\u65e8\u5728\u5b9e\u73b0\u5b9e\u65f6\u3001\u81ea\u7136\u7684\u97f3\u9891\u9a71\u52a8\u89d2\u8272\u52a8\u753b\uff0c\u63d0\u5347\u5bf9\u8bdd\u4ea4\u4e92\u7684\u4f53\u9a8c\u3002", "method": "1. \u5c06\u9884\u8bad\u7ec3SOTA\u56fe\u50cf\u5230\u89c6\u9891DiT\u6a21\u578b\u9002\u914d\u4e3a\u97f3\u9891\u9a71\u52a8\u89d2\u8272\u751f\u6210\u6a21\u578b\uff1b2. \u901a\u8fc7\u975e\u5bf9\u79f0\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u65e0\u9650\u89c6\u9891\u6d41\uff1b3. \u8bbe\u8ba1\u9ad8\u6027\u80fd\u4f4e\u5ef6\u8fdf\u63a8\u7406\u6d41\u6c34\u7ebf\u3002", "result": "\u6a21\u578b\u53c2\u6570\u8fbe180\u4ebf\uff0c\u652f\u6301\u65e0\u9650\u89c6\u9891\u6d41\u751f\u6210\uff0c\u4f18\u5316\u4e86\u63a8\u7406\u6548\u7387\u3002", "conclusion": "TalkingMachines\u5c55\u793a\u4e86\u9ad8\u6548\u97f3\u9891\u9a71\u52a8\u89d2\u8272\u52a8\u753b\u7684\u6f5c\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "keywords": "\u89c6\u9891\u751f\u6210,\u97f3\u9891\u9a71\u52a8,\u5b9e\u65f6\u52a8\u753b,\u5927\u8bed\u8a00\u6a21\u578b,\u77e5\u8bc6\u84b8\u998f"}}
{"id": "2506.03150", "pdf": "https://arxiv.org/pdf/2506.03150", "abs": "https://arxiv.org/abs/2506.03150", "authors": ["Yuanze Lin", "Yi-Wen Chen", "Yi-Hsuan Tsai", "Ronald Clark", "Ming-Hsuan Yang"], "title": "IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "Tech Report", "summary": "Although diffusion-based models can generate high-quality and high-resolution\nvideo sequences from textual or image inputs, they lack explicit integration of\ngeometric cues when controlling scene lighting and visual appearance across\nframes. To address this limitation, we propose IllumiCraft, an end-to-end\ndiffusion framework accepting three complementary inputs: (1)\nhigh-dynamic-range (HDR) video maps for detailed lighting control; (2)\nsynthetically relit frames with randomized illumination changes (optionally\npaired with a static background reference image) to provide appearance cues;\nand (3) 3D point tracks that capture precise 3D geometry information. By\nintegrating the lighting, appearance, and geometry cues within a unified\ndiffusion architecture, IllumiCraft generates temporally coherent videos\naligned with user-defined prompts. It supports background-conditioned and\ntext-conditioned video relighting and provides better fidelity than existing\ncontrollable video generation methods. Project Page:\nhttps://yuanze-lin.me/IllumiCraft_page", "AI": {"tldr": "IllumiCraft\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408HDR\u89c6\u9891\u3001\u91cd\u6253\u5149\u5e27\u548c3D\u70b9\u8f68\u8ff9\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u5149\u7167\u53ef\u63a7\u7684\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u89c6\u9891\u65f6\u7f3a\u4e4f\u51e0\u4f55\u7ebf\u7d22\u7684\u5149\u7167\u548c\u5916\u89c2\u63a7\u5236\u3002", "method": "\u7ed3\u5408HDR\u89c6\u9891\u3001\u91cd\u6253\u5149\u5e27\u548c3D\u70b9\u8f68\u8ff9\uff0c\u6784\u5efa\u7edf\u4e00\u6269\u6563\u67b6\u6784\u3002", "result": "\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u6027\u9ad8\u7684\u89c6\u9891\uff0c\u652f\u6301\u80cc\u666f\u548c\u6587\u672c\u6761\u4ef6\u4e0b\u7684\u5149\u7167\u8c03\u6574\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "IllumiCraft\u5728\u53ef\u63a7\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u903c\u771f\u5ea6\u548c\u7075\u6d3b\u6027\u3002", "keywords": "IllumiCraft, \u6269\u6563\u6a21\u578b, \u89c6\u9891\u751f\u6210, \u5149\u7167\u63a7\u5236, 3D\u51e0\u4f55"}}
{"id": "2506.03102", "pdf": "https://arxiv.org/pdf/2506.03102", "abs": "https://arxiv.org/abs/2506.03102", "authors": ["Sophie Greenwood", "Karen Levy", "Solon Barocas", "Hoda Heidari", "Jon Kleinberg"], "title": "Designing Algorithmic Delegates: The Role of Indistinguishability in Human-AI Handoff", "categories": ["cs.GT", "cs.AI", "cs.CY"], "comment": "Accepted at the Twenty-Sixth ACM Conference on Economics and\n  Computation (EC'25)", "summary": "As AI technologies improve, people are increasingly willing to delegate tasks\nto AI agents. In many cases, the human decision-maker chooses whether to\ndelegate to an AI agent based on properties of the specific instance of the\ndecision-making problem they are facing. Since humans typically lack full\nawareness of all the factors relevant to this choice for a given\ndecision-making instance, they perform a kind of categorization by treating\nindistinguishable instances -- those that have the same observable features --\nas the same. In this paper, we define the problem of designing the optimal\nalgorithmic delegate in the presence of categories. This is an important\ndimension in the design of algorithms to work with humans, since we show that\nthe optimal delegate can be an arbitrarily better teammate than the optimal\nstandalone algorithmic agent. The solution to this optimal delegation problem\nis not obvious: we discover that this problem is fundamentally combinatorial,\nand illustrate the complex relationship between the optimal design and the\nproperties of the decision-making task even in simple settings. Indeed, we show\nthat finding the optimal delegate is computationally hard in general. However,\nwe are able to find efficient algorithms for producing the optimal delegate in\nseveral broad cases of the problem, including when the optimal action may be\ndecomposed into functions of features observed by the human and the algorithm.\nFinally, we run computational experiments to simulate a designer updating an\nalgorithmic delegate over time to be optimized for when it is actually adopted\nby users, and show that while this process does not recover the optimal\ndelegate in general, the resulting delegate often performs quite well.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u4eba\u7c7b\u57fa\u4e8e\u5206\u7c7b\u8fdb\u884c\u4efb\u52a1\u59d4\u6258\u65f6\uff0c\u5982\u4f55\u8bbe\u8ba1\u6700\u4f18\u7684AI\u59d4\u6258\u7b97\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u76f8\u5bf9\u4e8e\u72ec\u7acbAI\u4ee3\u7406\u7684\u4f18\u52bf\u3002", "motivation": "\u968f\u7740AI\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u4eba\u7c7b\u66f4\u503e\u5411\u4e8e\u5c06\u4efb\u52a1\u59d4\u6258\u7ed9AI\u4ee3\u7406\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u51b3\u7b56\u5b9e\u4f8b\u7684\u5168\u9762\u8ba4\u77e5\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u8bbe\u8ba1\u6700\u4f18\u7684\u59d4\u6258\u7b97\u6cd5\u3002", "method": "\u5b9a\u4e49\u4e86\u5206\u7c7b\u5b58\u5728\u65f6\u7684\u6700\u4f18\u59d4\u6258\u95ee\u9898\uff0c\u5206\u6790\u4e86\u95ee\u9898\u7684\u7ec4\u5408\u6027\u8d28\uff0c\u5e76\u63d0\u51fa\u4e86\u5728\u51e0\u79cd\u5e7f\u6cdb\u60c5\u51b5\u4e0b\u7684\u9ad8\u6548\u7b97\u6cd5\u3002", "result": "\u53d1\u73b0\u6700\u4f18\u59d4\u6258\u95ee\u9898\u901a\u5e38\u8ba1\u7b97\u56f0\u96be\uff0c\u4f46\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u53ef\u4ee5\u9ad8\u6548\u6c42\u89e3\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u52a8\u6001\u66f4\u65b0\u7684\u59d4\u6258\u7b97\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u867d\u7136\u52a8\u6001\u4f18\u5316\u65e0\u6cd5\u5b8c\u5168\u8fbe\u5230\u6700\u4f18\u59d4\u6258\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u7a81\u51fa\u4e86\u59d4\u6258\u7b97\u6cd5\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002", "keywords": "AI\u59d4\u6258, \u5206\u7c7b\u51b3\u7b56, \u7b97\u6cd5\u8bbe\u8ba1, \u7ec4\u5408\u4f18\u5316"}}
{"id": "2506.03139", "pdf": "https://arxiv.org/pdf/2506.03139", "abs": "https://arxiv.org/abs/2506.03139", "authors": ["Siqi Chen", "Xinyu Dong", "Haolei Xu", "Xingyu Wu", "Fei Tang", "Hang Zhang", "Yuchen Yan", "Linjuan Wu", "Wenqi Zhang", "Guiyang Hou", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages,4 figures, Project page:\n  https://zju-real.github.io/SVGenius, Code:\n  https://github.com/ZJU-REAL/SVGenius-Bench", "summary": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius.", "AI": {"tldr": "SVGenius\u662f\u9996\u4e2a\u5168\u9762\u8bc4\u4f30SVG\u5904\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d6\u591a\u9886\u57df\u4efb\u52a1\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709SVG\u5904\u7406\u57fa\u51c6\u6d4b\u8bd5\u8986\u76d6\u4e0d\u8db3\u4e14\u8bc4\u4f30\u788e\u7247\u5316\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u6784\u5efaSVGenius\u57fa\u51c6\uff0c\u5305\u542b2,377\u4e2a\u67e5\u8be2\u548c18\u4e2a\u6307\u6807\uff0c\u8bc4\u4f3022\u79cd\u6a21\u578b\u3002", "result": "\u4e13\u6709\u6a21\u578b\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u4f46\u6240\u6709\u6a21\u578b\u968f\u590d\u6742\u5ea6\u589e\u52a0\u6027\u80fd\u4e0b\u964d\uff1b\u63a8\u7406\u589e\u5f3a\u8bad\u7ec3\u6bd4\u7eaf\u6269\u5c55\u66f4\u6709\u6548\u3002", "conclusion": "SVGenius\u4e3aSVG\u5904\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u5411\u91cf\u56fe\u5f62\u6a21\u578b\u7684\u8fdb\u6b65\u3002", "keywords": "SVG\u5904\u7406,\u57fa\u51c6\u6d4b\u8bd5,\u5927\u8bed\u8a00\u6a21\u578b,\u591a\u6a21\u6001\u6a21\u578b,\u8bc4\u4f30\u6846\u67b6"}}
