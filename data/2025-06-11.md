<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 81]
- [cs.LG](#cs.LG) [Total: 123]
- [cs.AI](#cs.AI) [Total: 34]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.SD](#cs.SD) [Total: 5]
- [quant-ph](#quant-ph) [Total: 5]
- [cs.RO](#cs.RO) [Total: 7]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.CR](#cs.CR) [Total: 5]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [math.OC](#math.OC) [Total: 5]
- [cs.GR](#cs.GR) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [nlin.CG](#nlin.CG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.CE](#cs.CE) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.DB](#cs.DB) [Total: 3]
- [math.ST](#math.ST) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.SE](#cs.SE) [Total: 6]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.CV](#cs.CV) [Total: 27]
- [stat.ML](#stat.ML) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Conservative Bias in Large Language Models: Measuring Relation Predictions](https://arxiv.org/abs/2506.08120)
*Toyin Aguda,Erik Wilson,Allan Anzagira,Simerjot Kaur,Charese Smiley*

Key words: 大型语言模型,关系抽取,保守偏向,信息丢失,Hobson's choice

TL;DR: 大型语言模型（LLMs）在关系抽取任务中表现出明显的保守偏向，常在无合适选项时默认选择No_Relation标签，导致信息丢失。研究发现保守偏向发生的频率是幻觉的两倍。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究LLMs在关系抽取中的保守偏向及其对信息丢失的影响，探索模型在无合适选项时的行为模式。

Method: 通过多组提示、数据集和关系类型系统评估保守偏向与幻觉的权衡，引入Hobson's choice概念，使用SBERT和LLM提示量化语义相似性。

Result: 保守偏向的发生频率是幻觉的两倍，模型倾向于选择安全但不提供信息的标签。

Conclusion: LLMs的保守偏向虽能减少错误关系分配，但会导致显著信息丢失，需进一步优化模型输出。

Abstract: Large language models (LLMs) exhibit pronounced conservative bias in relation
extraction tasks, frequently defaulting to No_Relation label when an
appropriate option is unavailable. While this behavior helps prevent incorrect
relation assignments, our analysis reveals that it also leads to significant
information loss when reasoning is not explicitly included in the output. We
systematically evaluate this trade-off across multiple prompts, datasets, and
relation types, introducing the concept of Hobson's choice to capture scenarios
where models opt for safe but uninformative labels over hallucinated ones. Our
findings suggest that conservative bias occurs twice as often as hallucination.
To quantify this effect, we use SBERT and LLM prompts to capture the semantic
similarity between conservative bias behaviors in constrained prompts and
labels generated from semi-constrained and open-ended prompts.

</details>


### [2] [QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA](https://arxiv.org/abs/2506.08123)
*Jacob Dineen,Aswin RRV,Qin Liu,Zhikun Xu,Xiao Ye,Ming Shen,Zhaonan Li,Shijie Lu,Chitta Baral,Muhao Chen,Ben Zhou*

Key words: 语言模型对齐, 符号奖励分解, 透明度, QA-LIGN, 可控性

TL;DR: QA-LIGN是一种自动符号奖励分解方法，通过为每个原则设计评估问题并生成独立的奖励组件，提高了语言模型对齐的透明度和适应性，且性能与DPO基线相当或更好。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 标准化奖励对齐方法通常将多样反馈压缩为单一标量奖励，导致目标混淆和缺乏透明度，QA-LIGN旨在解决这一问题。

Method: QA-LIGN通过设计原则特定的评估问题并分解奖励信号，替代传统的黑盒奖励模型。

Result: 实验显示，QA-LIGN在透明度和适应性上优于传统方法，且性能不逊于DPO基线。

Conclusion: QA-LIGN为语言模型对齐提供了更可解释和可控的途径，同时保持了任务性能。

Abstract: Alignment of large language models with explicit principles (such as
helpfulness, honesty, and harmlessness) is crucial for ensuring safe and
reliable AI systems. However, standard reward-based alignment methods typically
collapse diverse feedback into a single scalar reward, entangling multiple
objectives into one opaque training signal, which hinders interpretability. In
this work, we introduce QA-LIGN, an automatic symbolic reward decomposition
approach that preserves the structure of each constitutional principle within
the reward mechanism. Instead of training a black-box reward model that outputs
a monolithic score, QA-LIGN formulates principle-specific evaluation questions
and derives separate reward components for each principle, making it a drop-in
reward model replacement. Experiments aligning an uncensored large language
model with a set of constitutional principles demonstrate that QA-LIGN offers
greater transparency and adaptability in the alignment process. At the same
time, our approach achieves performance on par with or better than a DPO
baseline. Overall, these results represent a step toward more interpretable and
controllable alignment of language models, achieved without sacrificing
end-task performance.

</details>


### [3] [EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments](https://arxiv.org/abs/2506.08136)
*Zefang Liu,Yinzhu Quan*

Key words: EconWebArena, benchmark, autonomous agents, multimodal tasks, economic reasoning

TL;DR: EconWebArena是一个用于评估自主代理在多模态复杂经济任务中的基准测试，涵盖360个任务，强调真实性和权威性数据源。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 构建一个贴近真实经济环境的多模态基准测试，以评估代理在复杂任务中的表现。

Method: 通过LLM生成候选任务并进行人工筛选，确保任务清晰可行；评估多模态LLM的表现。

Result: 研究揭示了代理在导航、多模态理解等方面的性能差距和挑战。

Conclusion: EconWebArena为经济领域的多模态智能代理提供了严格的测试平台。

Abstract: We introduce EconWebArena, a benchmark for evaluating autonomous agents on
complex, multimodal economic tasks in realistic web environments. The benchmark
comprises 360 curated tasks from 82 authoritative websites spanning domains
such as macroeconomics, labor, finance, trade, and public policy. Each task
challenges agents to navigate live websites, interpret structured and visual
content, interact with real interfaces, and extract precise, time-sensitive
data through multi-step workflows. We construct the benchmark by prompting
multiple large language models (LLMs) to generate candidate tasks, followed by
rigorous human curation to ensure clarity, feasibility, and source reliability.
Unlike prior work, EconWebArena emphasizes fidelity to authoritative data
sources and the need for grounded web-based economic reasoning. We evaluate a
diverse set of state-of-the-art multimodal LLMs as web agents, analyze failure
cases, and conduct ablation studies to assess the impact of visual grounding,
plan-based reasoning, and interaction design. Our results reveal substantial
performance gaps and highlight persistent challenges in grounding, navigation,
and multimodal understanding, positioning EconWebArena as a rigorous testbed
for economic web intelligence.

</details>


### [4] [Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models](https://arxiv.org/abs/2506.08147)
*Muhammad Usman,Muhammad Ahmad,M. Shahiki Tash,Irina Gelbukh,Rolando Quintero Tellez,Grigori Sidorov*

Key words: 仇恨言论检测,多语言,注意力层,Transformer,GPT-3.5,Qwen 2.5 72B

TL;DR: 本文针对社交媒体上的仇恨言论问题，提出了一个多语言仇恨言论检测框架，重点关注乌尔都语。通过构建一个三语言数据集，并结合注意力层与先进模型（如GPT-3.5 Turbo和Qwen 2.5 72B），显著提升了检测性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 社交媒体上的仇恨言论对在线安全和包容性构成威胁，但乌尔都语等语言的仇恨言论检测研究不足。本文旨在填补这一空白。

Method: 构建了一个包含英语、乌尔都语和西班牙语的平衡数据集（10,193条推文），并采用注意力层与Transformer模型结合的方法进行特征提取。数据集通过多人标注确保质量。

Result: 模型在英语、西班牙语和乌尔都语上的宏F1分数分别为0.87、0.85和0.81，多语言联合模型达到0.88，性能显著优于传统基线。

Conclusion: 本文提出的框架为多语言仇恨言论检测提供了有效解决方案，有助于构建更安全的数字社区。

Abstract: Social media platforms are critical spaces for public discourse, shaping
opinions and community dynamics, yet their widespread use has amplified harmful
content, particularly hate speech, threatening online safety and inclusivity.
While hate speech detection has been extensively studied in languages like
English and Spanish, Urdu remains underexplored, especially using
translation-based approaches. To address this gap, we introduce a trilingual
dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and
Spanish (3,162 samples), collected via keyword filtering, with a balanced
distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology
leverages attention layers as a precursor to transformer-based models and large
language models (LLMs), enhancing feature extraction for multilingual hate
speech detection. For non-transformer models, we use TF-IDF for feature
extraction. The dataset is benchmarked using state-of-the-art models, including
GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models
like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators,
following rigorous guidelines, ensured high dataset quality, achieving a
Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5
Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of
0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for
Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B).
These results reflect improvements of 8.75% in English (over SVM baseline
0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM
baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline
0.82). Our framework offers a robust solution for multilingual hate speech
detection, fostering safer digital communities worldwide.

</details>


### [5] [ETT-CKGE: Efficient Task-driven Tokens for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2506.08158)
*Lijing Zhu,Qizhen Lan,Qing Tian,Wenbo Sun,Li Yang,Lu Xia,Yixin Xie,Xi Xiao,Tiehang Duan,Cui Tao,Shuteng Niu*

Key words: 知识图谱嵌入, 连续学习, 任务驱动, 标记学习, 效率优化

TL;DR: ETT-CKGE提出了一种高效、任务驱动的知识图谱连续嵌入方法，通过可学习的任务相关标记简化知识转移，显著提升了训练效率和扩展性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决现有连续知识图谱嵌入方法在效率和扩展性上的不足，特别是知识保存和计算开销问题。

Method: 引入可学习的任务驱动标记，直接捕捉任务相关信号，避免显式节点评分或遍历，实现高效知识转移。

Result: 在六个基准数据集上表现优异，显著提高了训练效率和扩展性。

Conclusion: ETT-CKGE在效率和性能上均优于现有方法，适用于大规模知识图谱嵌入任务。

Abstract: Continual Knowledge Graph Embedding (CKGE) seeks to integrate new knowledge
while preserving past information. However, existing methods struggle with
efficiency and scalability due to two key limitations: (1) suboptimal knowledge
preservation between snapshots caused by manually designed node/relation
importance scores that ignore graph dependencies relevant to the downstream
task, and (2) computationally expensive graph traversal for node/relation
importance calculation, leading to slow training and high memory overhead. To
address these limitations, we introduce ETT-CKGE (Efficient, Task-driven,
Tokens for Continual Knowledge Graph Embedding), a novel task-guided CKGE
method that leverages efficient task-driven tokens for efficient and effective
knowledge transfer between snapshots. Our method introduces a set of learnable
tokens that directly capture task-relevant signals, eliminating the need for
explicit node scoring or traversal. These tokens serve as consistent and
reusable guidance across snapshots, enabling efficient token-masked embedding
alignment between snapshots. Importantly, knowledge transfer is achieved
through simple matrix operations, significantly reducing training time and
memory usage. Extensive experiments across six benchmark datasets demonstrate
that ETT-CKGE consistently achieves superior or competitive predictive
performance, while substantially improving training efficiency and scalability
compared to state-of-the-art CKGE methods. The code is available at:
https://github.com/lijingzhu1/ETT-CKGE/tree/main

</details>


### [6] [Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction](https://arxiv.org/abs/2506.08172)
*Gerardo Aleman Manzanarez,Nora de la Cruz Arana,Jorge Garcia Flores,Yobany Garcia Medina,Raul Monroy,Nathalie Pernelle*

Key words: AI, 微小说, 文学评估, 文学理论, GrAImes

TL;DR: 本文提出了一种基于文学理论的评估协议GrAImes，用于客观评估AI生成的微小说的文学价值，包括主题一致性、文本清晰度、解释深度和美学质量等。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管AI已能生成叙事一致且语言连贯的短篇小说，但对这些文本的文学价值（特别是美学质量）的严格评估仍缺乏研究。

Method: 提出了GrAImes评估协议，基于文学理论，从多个文学角度（如主题一致性、美学质量等）为AI生成的微小说提供客观评估框架。

Result: 通过与文学专家和爱好者的验证，证明了评估协议的有效性。

Conclusion: GrAImes协议为自动化生成的微小说的文学价值评估提供了基础。

Abstract: Automated story writing has been a subject of study for over 60 years. Large
language models can generate narratively consistent and linguistically coherent
short fiction texts. Despite these advancements, rigorous assessment of such
outputs for literary merit - especially concerning aesthetic qualities - has
received scant attention. In this paper, we address the challenge of evaluating
AI-generated microfictions and argue that this task requires consideration of
literary criteria across various aspects of the text, such as thematic
coherence, textual clarity, interpretive depth, and aesthetic quality. To
facilitate this, we present GrAImes: an evaluation protocol grounded in
literary theory, specifically drawing from a literary perspective, to offer an
objective framework for assessing AI-generated microfiction. Furthermore, we
report the results of our validation of the evaluation protocol, as answered by
both literature experts and literary enthusiasts. This protocol will serve as a
foundation for evaluating automatically generated microfictions and assessing
their literary value.

</details>


### [7] [LLM-BT: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding](https://arxiv.org/abs/2506.08174)
*Li Weigang,Pedro Carvalho Brom*

Key words: LLM-BT, 回译, 多语言一致性, 语义嵌入, 术语标准化

TL;DR: 本文提出了一种基于大型语言模型的框架LLM-BT，用于自动化验证和技术术语的标准化，确保多语言一致性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着技术术语的快速增长，传统的人工标准化方法难以满足需求，尤其是人工智能和量子计算等领域。

Method: LLM-BT采用回译框架，结合多路径验证工作流和动态语义嵌入，实现术语一致性和跨语言稳健性。

Result: 实验显示LLM-BT在术语一致性和跨语言准确性方面表现优异，BLEU分数超过0.45，葡萄牙语准确率达100%。

Conclusion: LLM-BT将回译转变为多语言标准化的主动工具，支持人机协作，确保语义准确性，适用于全球科技领域的术语管理。

Abstract: The rapid growth of English technical terms challenges traditional
expert-driven standardization, especially in fast-evolving fields like AI and
quantum computing. Manual methods struggle to ensure multilingual consistency.
We propose \textbf{LLM-BT}, a back-translation framework powered by large
language models (LLMs) to automate terminology verification and standardization
via cross-lingual semantic alignment. Our contributions are: \textbf{(1)
Term-Level Consistency Validation:} Using English $\rightarrow$ intermediate
language $\rightarrow$ English back-translation, LLM-BT achieves high term
consistency across models (e.g., GPT-4, DeepSeek, Grok), with case studies
showing over 90\% exact or semantic matches. \textbf{(2) Multi-Path
Verification Workflow:} A novel ``Retrieve--Generate--Verify--Optimize''
pipeline integrates serial (e.g., EN $\rightarrow$ ZHcn $\rightarrow$ ZHtw
$\rightarrow$ EN) and parallel (e.g., EN $\rightarrow$ Chinese/Portuguese
$\rightarrow$ EN) BT routes. BLEU and term accuracy indicate strong
cross-lingual robustness (BLEU $>$ 0.45; Portuguese accuracy 100\%).
\textbf{(3) Back-Translation as Semantic Embedding:} BT is conceptualized as
dynamic semantic embedding, revealing latent meaning trajectories. Unlike
static embeddings, LLM-BT provides transparent path-based embeddings shaped by
model evolution. LLM-BT transforms back-translation into an active engine for
multilingual terminology standardization, enabling human--AI collaboration:
machines ensure semantic fidelity, humans guide cultural interpretation. This
infrastructure supports terminology governance across scientific and
technological fields worldwide.

</details>


### [8] [Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length](https://arxiv.org/abs/2506.08184)
*Chupei Wang,Jiaqiu Vince Sun*

Key words: LLM, 信息检索, 主动干扰, 工作记忆

TL;DR: 研究了LLMs中信息检索与生成能力的关联，发现长上下文中的干扰会导致检索准确率下降，提示需要增强模型抑制无关内容的能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨LLMs在长上下文中的信息检索能力，特别是干扰对其的影响。

Method: 采用认知科学中的主动干扰范式（PI-LLM），通过流式更新语义相关的键值对并查询最终值。

Result: LLM的检索准确率随干扰累积呈对数线性下降，提示操作信息的能力受限。

Conclusion: LLMs存在干扰和操纵信息的基本限制，需要增强抑制无关内容的能力。

Abstract: Information retrieval in Large Language Models (LLMs) is increasingly
recognized as intertwined with generation capabilities rather than mere lookup.
While longer contexts are often assumed to improve retrieval, the effects of
intra-context interference remain understudied. To address this, we adapt the
proactive interference (PI) paradigm from cognitive science, where earlier
information disrupts recall of newer updates. In humans, susceptibility to such
interference is inversely linked to working memory capacity. We introduce
PI-LLM, an evaluation that sequentially streams semantically related key-value
updates and queries only the final values. Although these final values are
clearly positioned just before the query, LLM retrieval accuracy declines
log-linearly toward zero as interference accumulates; errors arise from
retrieving previously overwritten values. Attempts to mitigate interference via
prompt engineering (e.g., instructing models to ignore earlier input) yield
limited success. These findings reveal a fundamental constraint on LLMs'
ability to disentangle interference and flexibly manipulate information,
suggesting a working memory bottleneck beyond mere context access. This calls
for approaches that strengthen models' ability to suppress irrelevant content
during retrieval.

</details>


### [9] ["I Wrote, I Paused, I Rewrote" Teaching LLMs to Read Between the Lines of Student Writing](https://arxiv.org/abs/2506.08221)
*Samra Zafar,Shaheer Minhas,Syed Ali Hassan Zaidi,Arfa Naeem,Zahra Ali*

Key words: 大型语言模型, 写作反馈, 写作过程, 键盘记录, 学生写作

TL;DR: 研究探讨利用键盘记录和定期快照等写作过程数据，改进大型语言模型（LLM）对学生写作反馈的效果，结果显示学生更偏好基于写作过程的反馈。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前LLM的写作反馈仅基于最终文本，缺乏对写作过程的了解，研究旨在探索如何利用写作过程数据提升反馈的个性化与有效性。

Method: 开发数字写作工具，记录学生的打字过程和文章演变，20名学生参与实验，LLM基于最终文本和写作过程生成反馈，学生评估反馈效果。

Result: 学生更偏好基于写作过程的LLM反馈，某些编辑行为（如添加内容或重组段落）与高质量写作密切相关。

Conclusion: 提高LLM对写作过程的认知能生成更贴近学生思维的反馈，更具支持性和个性化。

Abstract: Large language models(LLMs) like Gemini are becoming common tools for
supporting student writing. But most of their feedback is based only on the
final essay missing important context about how that text was written. In this
paper, we explore whether using writing process data, collected through
keystroke logging and periodic snapshots, can help LLMs give feedback that
better reflects how learners think and revise while writing. We built a digital
writing tool that captures both what students type and how their essays evolve
over time. Twenty students used this tool to write timed essays, which were
then evaluated in two ways: (i) LLM generated feedback using both the final
essay and the full writing trace, and (ii) After the task, students completed
surveys about how useful and relatable they found the feedback. Early results
show that learners preferred the process-aware LLM feedback, finding it more in
tune with their own thinking. We also found that certain types of edits, like
adding new content or reorganizing paragraphs, aligned closely with higher
scores in areas like coherence and elaboration. Our findings suggest that
making LLMs more aware of the writing process can lead to feedback that feels
more meaningful, personal, and supportive.

</details>


### [10] [Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions](https://arxiv.org/abs/2506.08234)
*Yu-Ang Lee,Guan-Ting Yi,Mei-Yi Liu,Jui-Chao Lu,Guan-Bo Yang,Yun-Nung Chen*

Key words: 大型语言模型, 复合AI系统, 优化方法, 自然语言反馈, 监督微调

TL;DR: 本文系统回顾了复合AI系统优化的最新进展，探讨了数值和语言技术的方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着大型语言模型（LLMs）和AI系统的进步，复合AI系统在处理复杂任务时表现优异，但系统复杂性带来了新的优化挑战。

Method: 通过整合监督微调（SFT）、强化学习（RL）和自然语言反馈等方法，系统分类并分析了优化技术。

Result: 研究提出了复合AI系统优化的形式化定义，分类了现有方法，并指出了未来研究方向。

Conclusion: 复合AI系统优化是一个快速发展的领域，仍需进一步研究解决现有挑战。

Abstract: Recent advancements in large language models (LLMs) and AI systems have led
to a paradigm shift in the design and optimization of complex AI workflows. By
integrating multiple components, compound AI systems have become increasingly
adept at performing sophisticated tasks. However, as these systems grow in
complexity, new challenges arise in optimizing not only individual components
but also their interactions. While traditional optimization methods such as
supervised fine-tuning (SFT) and reinforcement learning (RL) remain
foundational, the rise of natural language feedback introduces promising new
approaches, especially for optimizing non-differentiable systems. This paper
provides a systematic review of recent progress in optimizing compound AI
systems, encompassing both numerical and language-based techniques. We
formalize the notion of compound AI system optimization, classify existing
methods along several key dimensions, and highlight open research challenges
and future directions in this rapidly evolving field. A list of surveyed papers
is publicly available at https://github.com/MiuLab/AISysOpt-Survey.

</details>


### [11] [Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\rightarrow$ Evidence Reasoning](https://arxiv.org/abs/2506.08235)
*Shashidhar Reddy Javaji,Yupeng Cao,Haohang Li,Yangyang Yu,Nikhil Muralidhar,Zining Zhu*

Key words: 大型语言模型、科学主张、证据提取、基准测试、分治法

TL;DR: CLAIM-BENCH是一个用于评估大型语言模型（LLMs）在科学文献中提取和验证证据与主张能力的基准测试，揭示了LLMs在复杂科学内容理解上的局限性，并提出了改进方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是探索LLMs在科学文献中理解及验证主张与证据关系的能力，填补这一领域的研究空白。

Method: 通过系统比较三种基于分治法的方法，评估六种LLMs在不同研究领域的300多对主张-证据上的表现。

Result: 结果显示，闭源模型（如GPT-4和Claude）在精确度和召回率上优于开源模型；三遍式和逐一式提示方法能提高准确性，但计算成本更高。

Conclusion: CLAIM-BENCH为评估LLMs的科学理解能力设定了新标准，并为构建更可靠的系统提供了方向。

Abstract: Large language models (LLMs) are increasingly being used for complex research
tasks such as literature review, idea generation, and scientific paper
analysis, yet their ability to truly understand and process the intricate
relationships within complex research papers, such as the logical links between
claims and supporting evidence remains largely unexplored. In this study, we
present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs'
capabilities in scientific claim-evidence extraction and validation, a task
that reflects deeper comprehension of scientific argumentation. We
systematically compare three approaches which are inspired by divide and
conquer approaches, across six diverse LLMs, highlighting model-specific
strengths and weaknesses in scientific comprehension. Through evaluation
involving over 300 claim-evidence pairs across multiple research domains, we
reveal significant limitations in LLMs' ability to process complex scientific
content. Our results demonstrate that closed-source models like GPT-4 and
Claude consistently outperform open-source counterparts in precision and recall
across claim-evidence identification tasks. Furthermore, strategically designed
three-pass and one-by-one prompting approaches significantly improve LLMs'
abilities to accurately link dispersed evidence with claims, although this
comes at increased computational cost. CLAIM-BENCH sets a new standard for
evaluating scientific comprehension in LLMs, offering both a diagnostic tool
and a path forward for building systems capable of deeper, more reliable
reasoning across full-length papers.

</details>


### [12] [Automatic Generation of Inference Making Questions for Reading Comprehension Assessments](https://arxiv.org/abs/2506.08260)
*Wanjing Anya Ma,Michael Flor,Zuowei Wang*

Key words: 阅读理解,推断能力,诊断性问题,GPT-4o,自动生成

TL;DR: 论文研究了阅读理解的推断能力，提出了诊断性问题分类法，并利用GPT-4o生成桥接推断问题，验证了自动生成与人工评估结合的可行性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 提升阅读理解教学效果，通过诊断性问题帮助学生掌握复杂的推断技能。

Method: 提出推断类型分类法，利用GPT-4o通过few-shot提示生成桥接推断问题，并比较不同的提示条件。

Result: GPT-4o生成的问题93.8%质量高，但仅42.6%准确匹配目标推断类型。人工评估与自动生成结合效果显著。

Conclusion: 自动生成结合人工判断是提高诊断性阅读理解评估规模与质量的有效途径。

Abstract: Inference making is an essential but complex skill in reading comprehension
(RC). Some inferences require resolving references across sentences, and some
rely on using prior knowledge to fill in the detail that is not explicitly
written in the text. Diagnostic RC questions can help educators provide more
effective and targeted reading instruction and interventions for school-age
students. We introduce a taxonomy of inference types for RC and use it to
analyze the distribution of items within a diagnostic RC item bank. Next, we
present experiments using GPT-4o to generate bridging-inference RC items for
given reading passages via few-shot prompting, comparing conditions with and
without chain-of-thought prompts. Generated items were evaluated on three
aspects: overall item quality, appropriate inference type, and LLM reasoning,
achieving high inter-rater agreements above 0.90. Our results show that GPT-4o
produced 93.8% good-quality questions suitable for operational use in grade
3-12 contexts; however, only 42.6% of the generated questions accurately
matched the targeted inference type. We conclude that combining automatic item
generation with human judgment offers a promising path toward scalable,
high-quality diagnostic RC assessments.

</details>


### [13] [Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability](https://arxiv.org/abs/2506.08300)
*Matteo Cargnelutti,Catherine Brobston,John Hess,Jack Cushman,Kristi Mukk,Aristana Scourtas,Kyle Courtney,Greg Leppert,Amanda Watson,Martha Whitehead,Jonathan Zittrain*

Key words: 大型语言模型, 训练数据, 公共领域书籍, OCR, 哈佛图书馆

TL;DR: 报告介绍了Institutional Books 1.0数据集，这是基于哈佛图书馆在Google图书项目中数字化的大量公共领域书籍的集合，旨在提供高质量、可持续的LLM训练数据。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为了解决LLM训练数据稀缺、质量不均的问题，并确保数据来源的透明性和可持续性。

Method: 通过与哈佛图书馆合作，提取、分析和处理了1,075,899卷书籍的文本和元数据，发布了一个包含242亿令牌的公共领域数据集。

Result: 发布了Institutional Books 1.0数据集，包含983,004卷书籍的OCR文本和元数据，覆盖250种语言。

Conclusion: 该项目为LLM提供了高质量的历史文本数据，同时提高了数据的可访问性和可用性。

Abstract: Large language models (LLMs) use data to learn about the world in order to
produce meaningful correlations and predictions. As such, the nature, scale,
quality, and diversity of the datasets used to train these models, or to
support their work at inference time, have a direct impact on their quality.
The rapid development and adoption of LLMs of varying quality has brought into
focus the scarcity of publicly available, high-quality training data and
revealed an urgent need to ground the stewardship of these datasets in
sustainable practices with clear provenance chains. To that end, this technical
report introduces Institutional Books 1.0, a large collection of public domain
books originally digitized through Harvard Library's participation in the
Google Books project, beginning in 2006. Working with Harvard Library, we
extracted, analyzed, and processed these volumes into an extensively-documented
dataset of historic texts. This analysis covers the entirety of Harvard
Library's collection scanned as part of that project, originally spanning
1,075,899 volumes written in over 250 different languages for a total of
approximately 250 billion tokens. As part of this initial release, the
OCR-extracted text (original and post-processed) as well as the metadata
(bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens,
identified as being in the public domain have been made available. This report
describes this project's goals and methods as well as the results of the
analyses we performed, all in service of making this historical collection more
accessible and easier for humans and machines alike to filter, read and use.

</details>


### [14] [Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency](https://arxiv.org/abs/2506.08343)
*Chenlong Wang,Yuanning Feng,Dongping Chen,Zhaoyang Chu,Ranjay Krishna,Tianyi Zhou*

Key words: 大模型推理,自我反思,效率提升,NoWait,多模态推理

TL;DR: NoWait通过禁用显式自我反思信号（如"Wait"和"Hmm"），显著减少推理过程中的冗余输出，提升效率且不影响模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨显式自我反思信号是否对高级推理必要，以解决推理模型因过度思考导致的冗余和效率低下问题。

Method: 提出NoWait方法，通过在推理阶段抑制特定信号（如"Wait"和"Hmm"），禁用显式自我反思。

Result: 在10个基准测试中，NoWait使推理路径长度减少27%-51%，且未损害模型性能。

Conclusion: NoWait是一种即插即用的高效多模态推理解决方案。

Abstract: Recent advances in large reasoning models have enabled complex, step-by-step
reasoning but often introduce significant overthinking, resulting in verbose
and redundant outputs that hinder efficiency. In this study, we examine whether
explicit self-reflection, signaled by tokens such as "Wait" and "Hmm", is
necessary for advanced reasoning. We propose NoWait, a simple yet effective
approach that disables explicit self-reflection by suppressing these tokens
during inference. Extensive experiments on ten benchmarks across textual,
visual, and video reasoning tasks show that NoWait reduces chain-of-thought
trajectory length by up to 27%-51% in five R1-style model series, without
compromising model utility. NoWait thus offers a plug-and-play solution for
efficient and utility-preserving multimodal reasoning.

</details>


### [15] [Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving](https://arxiv.org/abs/2506.08349)
*Yuxuan Zhou,Xien Liu,Chenwei Yan,Chen Ning,Xiao Zhang,Boxun Li,Xiangling Fu,Shijin Wang,Guoping Hu,Yu Wang,Ji Wu*

Key words: 大语言模型、Bloom分类法、医学评估、认知层次、模型性能

TL;DR: 该研究基于Bloom分类法提出了一个多认知层次的评估框架，用于评估大语言模型在医学领域的能力。研究发现，随着认知复杂度的增加，模型性能显著下降，且模型规模在更高认知层次上对性能影响更大。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索大语言模型在不同认知层次上的医学能力，填补现有研究空白。

Method: 提出多认知层次评估框架，整合现有医学数据集，设计针对三个认知层次的任务（基础知识掌握、综合知识应用、场景问题解决），并系统评估六大主流模型家族。

Result: 模型性能随认知复杂度增加而显著下降；模型规模在高认知层次上对性能影响更大。

Conclusion: 需提升大语言模型在高认知层次的医学能力，为开发适合实际医疗应用的模型提供指导。

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
various medical benchmarks, but their capabilities across different cognitive
levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a
multi-cognitive-level evaluation framework for assessing LLMs in the medical
domain in this study. The framework integrates existing medical datasets and
introduces tasks targeting three cognitive levels: preliminary knowledge grasp,
comprehensive knowledge application, and scenario-based problem solving. Using
this framework, we systematically evaluate state-of-the-art general and medical
LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek.
Our findings reveal a significant performance decline as cognitive complexity
increases across evaluated models, with model size playing a more critical role
in performance at higher cognitive levels. Our study highlights the need to
enhance LLMs' medical capabilities at higher cognitive levels and provides
insights for developing LLMs suited to real-world medical applications.

</details>


### [16] [Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning](https://arxiv.org/abs/2506.08354)
*Yiqun Sun,Qiang Huang,Anthony K. H. Tung,Jun Yu*

Key words: 文本嵌入, 隐式语义, 语义建模, 深度学习, 自然语言处理

TL;DR: 本文主张文本嵌入研究应超越表层意义，将隐式语义作为核心建模目标。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前文本嵌入模型过于关注表层语义，忽视了语言中的隐式意义（如语用、说话者意图和社会文化背景）。

Method: 提出通过使用更多样化和语言学基础的数据、设计评估深层语义理解的基准，并将隐式意义明确作为建模目标。

Result: 初步研究表明，即使是顶尖模型在隐式语义任务上表现仅略优于简单基线。

Conclusion: 呼吁研究社区转向更关注隐式语义的建模，以更好地应对现实语言的复杂性。

Abstract: This position paper argues that the text embedding research community should
move beyond surface meaning and embrace implicit semantics as a central
modeling goal. Text embedding models have become foundational in modern NLP,
powering a wide range of applications and drawing increasing research
attention. Yet, much of this progress remains narrowly focused on surface-level
semantics. In contrast, linguistic theory emphasizes that meaning is often
implicit, shaped by pragmatics, speaker intent, and sociocultural context.
Current embedding models are typically trained on data that lacks such depth
and evaluated on benchmarks that reward the capture of surface meaning. As a
result, they struggle with tasks requiring interpretive reasoning, speaker
stance, or social meaning. Our pilot study highlights this gap, showing that
even state-of-the-art models perform only marginally better than simplistic
baselines on implicit semantics tasks. To address this, we call for a paradigm
shift: embedding research should prioritize more diverse and linguistically
grounded training data, design benchmarks that evaluate deeper semantic
understanding, and explicitly frame implicit meaning as a core modeling
objective, better aligning embeddings with real-world language complexity.

</details>


### [17] [DEAL: Disentangling Transformer Head Activations for LLM Steering](https://arxiv.org/abs/2506.08359)
*Li-Ming Zhan,Bo Liu,Zexin Lu,Chengqiang Xie,Jiannong Cao,Xiao-Ming Wu*

Key words: 因果归因, 注意力头, VQ-AE, 行为引导

TL;DR: 提出了一种基于因果归因的框架，用于识别Transformer中与行为相关的注意力头，通过VQ-AE量化其激活空间，并通过分类指标评估行为相关性，实验表明该方法能更准确地指导推理时干预。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法依赖表面线索或启发式方法选择模块，可能导致次优或意外结果。

Method: 使用VQ-AE量化注意力头的激活空间，划分为行为相关和无关子空间，并通过分类指标评估行为相关性。

Result: 在七个LLM和五个行为引导数据集上表现优异，尤其在真实性引导任务中。

Conclusion: 该方法能更准确地选择行为相关的注意力头，并展现跨领域的零样本泛化能力。

Abstract: Inference-time steering aims to alter the response characteristics of large
language models (LLMs) without modifying their underlying parameters. A
critical step in this process is the identification of internal modules within
LLMs that are associated with the target behavior. However, current approaches
to module selection often depend on superficial cues or ad-hoc heuristics,
which can result in suboptimal or unintended outcomes. In this work, we propose
a principled causal-attribution framework for identifying behavior-relevant
attention heads in transformers. For each head, we train a vector-quantized
autoencoder (VQ-AE) on its attention activations, partitioning the latent space
into behavior-relevant and behavior-irrelevant subspaces, each quantized with a
shared learnable codebook. We assess the behavioral relevance of each head by
quantifying the separability of VQ-AE encodings for behavior-aligned versus
behavior-violating responses using a binary classification metric. This yields
a behavioral relevance score that reflects each head discriminative capacity
with respect to the target behavior, guiding both selection and importance
weighting. Experiments on seven LLMs from two model families and five
behavioral steering datasets demonstrate that our method enables more accurate
inference-time interventions, achieving superior performance on the
truthfulness-steering task. Furthermore, the heads selected by our approach
exhibit strong zero-shot generalization in cross-domain truthfulness-steering
scenarios.

</details>


### [18] [CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs](https://arxiv.org/abs/2506.08364)
*Jash Rajesh Parekh,Pengcheng Jiang,Jiawei Han*

Key words: 因果推理, RAG, 大语言模型, 零样本学习, 专业领域

TL;DR: 提出一种名为Causal-Chain RAG（CC-RAG）的新方法，通过在RAG流程中引入因果链结构，显著提升大语言模型（LLMs）在专业领域中的因果推理能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有RAG方法在因果推理中仅使用平坦的上下文证据，缺乏对因果依赖关系结构化建模的能力。

Method: 结合零样本三重提取和主题感知图链，构建有向无环图（DAG）并利用前向/后向链式推理生成结构化答案。

Result: 在比特币价格波动和高雪氏病两个领域，CC-RAG在链相似性、信息密度和词汇多样性上优于标准RAG和零样本LLMs。

Conclusion: 显式建模因果结构能帮助LLMs生成更准确、可解释的响应，尤其在专业领域中。

Abstract: Understanding cause and effect relationships remains a formidable challenge
for Large Language Models (LLMs), particularly in specialized domains where
reasoning requires more than surface-level correlations. Retrieval-Augmented
Generation (RAG) improves factual accuracy, but standard RAG pipelines treat
evidence as flat context, lacking the structure required to model true causal
dependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that
integrates zero-shot triple extraction and theme-aware graph chaining into the
RAG pipeline, enabling structured multi-hop inference. Given a domain specific
corpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of <cause, relation,
effect> triples and uses forward/backward chaining to guide structured answer
generation. Experiments on two real-world domains: Bitcoin price fluctuations
and Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot
LLMs in chain similarity, information density, and lexical diversity. Both
LLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results
demonstrate that explicitly modeling causal structure enables LLMs to generate
more accurate and interpretable responses, especially in specialized domains
where flat retrieval fails.

</details>


### [19] [Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding](https://arxiv.org/abs/2506.08371)
*Zikai Xiao,Ziyang Wang,Wen Ma,Yan Zhang,Wei Shen,Yan Wang,Luqi Gong,Zuozhu Liu*

Key words: Large Language Models, long-context, Posterior Salience Attenuation, Positional Contrastive Decoding

TL;DR: 论文提出了一种无需训练的Positional Contrastive Decoding (PCD)方法，通过对比长上下文注意力和局部注意力来解决LLMs在长上下文中的性能衰减问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型在长上下文中存在性能衰减问题，现有解决方案训练成本高昂，因此需要探索更高效的方法。

Method: 提出PCD方法，通过对比长感知注意力和局部感知注意力的logits，利用短到长上下文训练的优势。

Result: 实验表明PCD在长上下文基准测试中实现了最先进的性能。

Conclusion: PCD有效缓解了注意力分数衰减，为长上下文建模提供了一种高效解决方案。

Abstract: While Large Language Models (LLMs) support long contexts, they struggle with
performance degradation within the context window. Current solutions incur
prohibitive training costs, leaving statistical behaviors and cost-effective
approaches underexplored. From the decoding perspective, we identify the
Posterior Salience Attenuation (PSA) phenomenon, where the salience ratio
correlates with long-text performance degradation. Notably, despite the
attenuation, gold tokens still occupy high-ranking positions in the decoding
space. Motivated by it, we propose the training-free Positional Contrastive
Decoding (PCD) that contrasts the logits derived from long-aware attention with
those from designed local-aware attention, enabling the model to focus on the
gains introduced by large-scale short-to-long training. Through the analysis of
long-term decay simulation, we demonstrate that PCD effectively alleviates
attention score degradation. Experimental results show that PCD achieves
state-of-the-art performance on long-context benchmarks.

</details>


### [20] [Draft-based Approximate Inference for LLMs](https://arxiv.org/abs/2506.08373)
*Kevin Galim,Ethan Ewer,Wonjun Kang,Minjae Lee,Hyung Il Koo,Kangwook Lee*

Key words: LLM, 推理优化, KV缓存, 草案模型, SpecKV, SpecPC

TL;DR: 提出了一种利用小型草案模型优化长上下文大型语言模型（LLM）推理的新框架，包括SpecKV和SpecPC两种实现，显著提高了准确性和效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于Transformer的二次计算和线性内存复杂性，优化长上下文LLM的推理变得尤为重要。现有方法通常依赖于粗略的预测，准确性不足。

Method: 引入草案模型预测令牌和KV对的重要性，具体实现为SpecKV（优化KV缓存丢弃）和SpecPC（压缩提示令牌）。

Result: 在长上下文基准测试中，新方法在准确性、内存使用、延迟和吞吐量方面均优于现有基线。

Conclusion: 草案模型不仅可用于无损推测解码，还能显著提升近似LLM推理的性能。

Abstract: Optimizing inference for long-context Large Language Models (LLMs) is
increasingly important due to the quadratic compute and linear memory
complexity of Transformers. Existing approximation methods, such as key-value
(KV) cache dropping, sparse attention, and prompt compression, typically rely
on rough predictions of token or KV pair importance. We propose a novel
framework for approximate LLM inference that leverages small draft models to
more accurately predict the importance of tokens and KV pairs. Specifically, we
introduce two instantiations of our proposed framework: (i) SpecKV, which
leverages a draft output to accurately assess the importance of each KV pair
for more effective KV cache dropping, and (ii) SpecPC, which uses the draft
model's attention activations to identify and discard unimportant prompt
tokens. To the best of our knowledge, this is the first work to use draft
models for approximate LLM inference acceleration, extending their utility
beyond traditional lossless speculative decoding. We motivate our methods with
theoretical and empirical analyses, and show a strong correlation between the
attention patterns of draft and target models. Extensive experiments on
long-context benchmarks show that our methods consistently achieve higher
accuracy than existing baselines, while preserving the same improvements in
memory usage, latency, and throughput. Our code is available at
https://github.com/furiosa-ai/draft-based-approx-llm.

</details>


### [21] [EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models](https://arxiv.org/abs/2506.08375)
*Tao Zou,Xinghua Zhang,Haiyang Yu,Minzheng Wang,Fei Huang,Yongbin Li*

Key words: 大语言模型、复杂指令、多任务、基准测试、SegPO算法

TL;DR: 提出了一种名为EIFBENCH的复杂指令遵循基准测试，用于更真实地评估大语言模型（LLMs）在多任务场景下的表现，并提出了SegPO算法以提升模型能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基准测试过于简单，无法反映真实世界的复杂需求，需要一种更全面的评估工具。

Method: 设计了EIFBENCH基准测试，包含多任务场景和多种约束条件，并提出SegPO算法优化LLMs的多任务处理能力。

Result: 实验揭示了现有LLMs在复杂指令下的性能差距，表明需要进一步优化。

Conclusion: EIFBENCH为评估LLMs提供了更真实的场景，SegPO算法能显著提升模型表现。

Abstract: With the development and widespread application of large language models
(LLMs), the new paradigm of "Model as Product" is rapidly evolving, and demands
higher capabilities to address complex user needs, often requiring precise
workflow execution which involves the accurate understanding of multiple tasks.
However, existing benchmarks focusing on single-task environments with limited
constraints lack the complexity required to fully reflect real-world scenarios.
To bridge this gap, we present the Extremely Complex Instruction Following
Benchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and
robust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that
enable comprehensive assessment across diverse task types concurrently, but
also integrates a variety of constraints, replicating complex operational
environments. Furthermore, we propose the Segment Policy Optimization (SegPO)
algorithm to enhance the LLM's ability to accurately fulfill multi-task
workflow. Evaluations on EIFBENCH have unveiled considerable performance
discrepancies in existing LLMs when challenged with these extremely complex
instructions. This finding underscores the necessity for ongoing optimization
to navigate the intricate challenges posed by LLM applications.

</details>


### [22] [mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks](https://arxiv.org/abs/2506.08400)
*Luel Hagos Beyene,Vivek Verma,Min Ma,Jesujoba O. Alabi,Fabian David Schmidt,Joyce Nakatumba-Nabende,David Ifeoluwa Adelani*

Key words: 大语言模型,低资源语言,多模态评估,mSTEB

TL;DR: 该论文介绍了mSTEB，一个用于评估大语言模型（LLMs）在多语言和多模态任务中性能的新基准，特别关注低资源语言。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决低资源语言在LLMs评估中缺乏标准化基准的问题。

Method: 引入mSTEB基准，覆盖语言识别、文本分类、问答和翻译等任务，并在语音和文本模态上评估主流LLMs。

Result: 高资源语言和低资源语言（尤其是非洲和美洲/大洋洲语言）之间存在显著的性能差距。

Conclusion: 需要更多投入以改善低资源语言在LLMs中的覆盖率不足问题。

Abstract: Large Language models (LLMs) have demonstrated impressive performance on a
wide range of tasks, including in multimodal settings such as speech. However,
their evaluation is often limited to English and a few high-resource languages.
For low-resource languages, there is no standardized evaluation benchmark. In
this paper, we address this gap by introducing mSTEB, a new benchmark to
evaluate the performance of LLMs on a wide range of tasks covering language
identification, text classification, question answering, and translation tasks
on both speech and text modalities. We evaluated the performance of leading
LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open
models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in
performance between high-resource and low-resource languages, especially for
languages spoken in Africa and Americas/Oceania. Our findings show that more
investment is needed to address their under-representation in LLMs coverage.

</details>


### [23] [TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration](https://arxiv.org/abs/2506.08403)
*Weiya Li,Junjie Chen,Bei Li,Boyang Liu,Zichen Wen,Nuanqiao Shan,Xiaoqian Liu,Anping Liu,Huajie Liu,Youyan Wang,Wujiuge Yin,Hu Song,Bing Huang,Zhiyuan Xia,Jialiang Chen,Linfeng Zhang*

Key words: 机器翻译,多智能体系统,认知理论,TACTIC,大型语言模型

TL;DR: 论文提出了一种名为TACTIC的多智能体翻译框架，通过模拟人类翻译的认知过程来提高机器翻译质量，实验结果表明其性能优于现有模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的多智能体翻译框架忽视了认知翻译研究的基础见解，无法充分挖掘大型语言模型的翻译潜力。

Method: 提出了TACTIC框架，包含六个功能不同的智能体，分别对应人类翻译的关键认知过程，如草稿、精炼、评估等。

Result: 实验显示TACTIC在FLORES-200和WMT24基准测试中性能超过GPT-4.1和DeepSeek-R1。

Conclusion: TACTIC通过模拟人类翻译的认知策略，有效提升了机器翻译质量。

Abstract: Machine translation has long been a central task in natural language
processing. With the rapid advancement of large language models (LLMs), there
has been remarkable progress in translation quality. However, fully realizing
the translation potential of LLMs remains an open challenge. Recent studies
have explored multi-agent systems to decompose complex translation tasks into
collaborative subtasks, showing initial promise in enhancing translation
quality through agent cooperation and specialization. Nevertheless, existing
multi-agent translation frameworks largely neglect foundational insights from
cognitive translation studies. These insights emphasize how human translators
employ different cognitive strategies, such as balancing literal and free
translation, refining expressions based on context, and iteratively evaluating
outputs. To address this limitation, we propose a cognitively informed
multi-agent framework called TACTIC, which stands for T ranslation A gents with
Cognitive- T heoretic Interactive Collaboration. The framework comprises six
functionally distinct agents that mirror key cognitive processes observed in
human translation behavior. These include agents for drafting, refinement,
evaluation, scoring, context reasoning, and external knowledge gathering. By
simulating an interactive and theory-grounded translation workflow, TACTIC
effectively leverages the full capacity of LLMs for high-quality translation.
Experimental results on diverse language pairs from the FLORES-200 and WMT24
benchmarks show that our method consistently achieves state-of-the-art
performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by
an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it
further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at
https://github.com/weiyali126/TACTIC.

</details>


### [24] [Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens](https://arxiv.org/abs/2506.08410)
*Ziyang Ma,Qingyue Yuan,Zhenglin Wang,Deyu Zhou*

Key words: 大型语言模型, 元认知, 自动评估, MIRA, 数学推理

TL;DR: 该论文研究了大型语言模型（LLM）的元认知评估方法，提出了自动评估框架AutoMeco和改进策略MIRA。通过实验验证了方法的合理性，并展示了MIRA在提升LLM元认知能力方面的效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有研究主要关注LLM的认知错误检测能力，而忽视了其元认知能力（如自我察觉错误的能力），这对LLM的可靠性至关重要。本文旨在填补这一空白，研究如何评估和改进LLM的元认知能力。

Method: 提出AutoMeco框架用于评估现有元认知方法，并提出无训练的MIRA策略以提高元认知能力。在三个数学推理数据集和三个LLM上进行实验验证。

Result: 实验结果表明，AutoMeco在评估LLM元认知能力方面具有合理性，且MIRA能够更好地评估LLM的元认知能力。

Conclusion: 本研究为LLM的元认知能力提供了有效的评估和改进方法，提升了LLM的可靠性。

Abstract: Previous research has primarily focused on the cognitive error detection
capabilities of Large Language Models (LLMs), often prompting them to analyze
mistakes in reasoning chains. However, few studies have examined the
meta-cognitive abilities of LLMs (e.g., their self-awareness of step errors),
which are crucial for their reliability. While studies on LLM self-evaluation
present some measures, such as perplexity, which can reflect the answer
correctness and be viewed as the lens of meta-cognition, they lack step-level
analysis and adaptation. This paper studies the evaluation of LLM
meta-cognition using the current lenses and how to improve these lenses.
Specifically, we propose AutoMeco, an Automated Meta-cognition Evaluation
framework for benchmarking the existing lenses. Furthermore, a training-free
Markovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost
current meta-cognition lenses. Experimental results on three mathematical
reasoning datasets and three LLMs show the reasonableness of AutoMeco by
comparing it with Best-of-N verification. Moreover, the meta-cognition ability
of LLMs can be better evaluated using MIRA.

</details>


### [25] [Know-MRI: A Knowledge Mechanisms Revealer&Interpreter for Large Language Models](https://arxiv.org/abs/2506.08427)
*Jiaxiang Liu,Boxuan Xing,Chenhao Yuan,Chenxiang Zhang,Di Wu,Xiusheng Huang,Haida Yu,Chuhan Lang,Pengfei Cao,Jun Zhao,Kang Liu*

Key words: 大型语言模型, 知识机制, 可解释性, 开源工具

TL;DR: 本文介绍了Know-MRI，一个用于系统分析大型语言模型知识机制的开源工具。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着大型语言模型的发展，迫切需要提升其知识机制的可解释性。

Method: 开发了一个可扩展的核心模块，可自动匹配不同输入数据与解释方法，并整合解释输出。

Result: Know-MRI支持用户根据输入自由选择解释方法，便于从多角度分析模型的知识机制。

Conclusion: 该工具为LLM内部知识机制的系统分析提供了实用解决方案。

Abstract: As large language models (LLMs) continue to advance, there is a growing
urgency to enhance the interpretability of their internal knowledge mechanisms.
Consequently, many interpretation methods have emerged, aiming to unravel the
knowledge mechanisms of LLMs from various perspectives. However, current
interpretation methods differ in input data formats and interpreting outputs.
The tools integrating these methods are only capable of supporting tasks with
specific inputs, significantly constraining their practical applications. To
address these challenges, we present an open-source Knowledge Mechanisms
Revealer&Interpreter (Know-MRI) designed to analyze the knowledge mechanisms
within LLMs systematically. Specifically, we have developed an extensible core
module that can automatically match different input data with interpretation
methods and consolidate the interpreting outputs. It enables users to freely
choose appropriate interpretation methods based on the inputs, making it easier
to comprehensively diagnose the model's internal knowledge mechanisms from
multiple perspectives. Our code is available at
https://github.com/nlpkeg/Know-MRI. We also provide a demonstration video on
https://youtu.be/NVWZABJ43Bs.

</details>


### [26] [CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models](https://arxiv.org/abs/2506.08430)
*Ziqi. Liu,Ziyang. Zhou,Mingxuan. Hu*

Key words: 大型语言模型, 反讽检测, 多智能体框架, 零样本学习, Macro-F1

TL;DR: 本文提出了一种名为CAF-I的多智能体框架，用于解决现有LLM在反讽检测中的局限性，通过多维分析和协作优化实现了显著的检测性能提升。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有大型语言模型（LLM）在反讽检测中存在单视角局限、理解不全面和可解释性不足的问题，CAF-I旨在通过多智能体协作框架解决这些问题。

Method: CAF-I采用包括Context、Semantics和Rhetoric在内的多个专门智能体进行多维分析，并通过决策智能体和优化评估智能体实现协作优化。

Result: 在基准数据集上，CAF-I实现了零样本下的最先进性能，Macro-F1平均达到76.31，比之前最强基线提高了4.98。

Conclusion: CAF-I通过模拟人类多视角分析，显著提升了反讽检测的准确性和可解释性，为LLM在多领域应用提供了新思路。

Abstract: Large language model (LLM) have become mainstream methods in the field of
sarcasm detection. However, existing LLM methods face challenges in irony
detection, including: 1. single-perspective limitations, 2. insufficient
comprehensive understanding, and 3. lack of interpretability. This paper
introduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven
multi-agent system designed to overcome these issues. CAF-I employs specialized
agents for Context, Semantics, and Rhetoric, which perform multidimensional
analysis and engage in interactive collaborative optimization. A Decision Agent
then consolidates these perspectives, with a Refinement Evaluator Agent
providing conditional feedback for optimization. Experiments on benchmark
datasets establish CAF-I's state-of-the-art zero-shot performance. Achieving
SOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of
76.31, a 4.98 absolute improvement over the strongest prior baseline. This
success is attained by its effective simulation of human-like multi-perspective
analysis, enhancing detection accuracy and interpretability.

</details>


### [27] [Low-resource domain adaptation while minimizing energy and hardware resource consumption](https://arxiv.org/abs/2506.08433)
*Hernán Maina,Nicolás Wolovick,Luciana Benotti*

Key words: 大型语言模型, 领域适应, 数值精度, 数据并行化, 能源效率

TL;DR: 本文探讨了在不同数值精度和数据并行化策略下如何优化大型语言模型（LLM）的训练速度和准确性，以降低领域适应的计算成本，尤其适用于资源有限的环境。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于训练大型语言模型（LLM）的成本高昂，包括能源、硬件和标注数据，且容易受主流文化和价值观影响，因此需要一种适应多样文化和价值上下文的策略。然而，领域适应的计算成本仍然是一个主要障碍。

Method: 评估不同数值精度和数据并行化策略对训练速度和模型准确性的影响，旨在为资源有限的环境提供高效领域适应方法。

Result: 研究结果为关注能源效率、可访问性或硬件受限的场景提供了实用建议。

Conclusion: 通过优化数值精度和数据并行化策略，可以有效降低领域适应的计算成本，促进资源有限环境中的应用。

Abstract: Training Large Language Models (LLMs) is costly in terms of energy, hardware,
and annotated data, often resulting in a positionality rooted in predominant
cultures and values (Santy et al., 2023). Domain adaptation has emerged as a
promising strategy to better align models with diverse cultural and value
contexts (Hershcovich et al., 2022), but its computational cost remains a
significant barrier, particularly for research groups lacking access to
large-scale infrastructure. In this paper, we evaluate how the use of different
numerical precisions and data parallelization strategies impacts both training
speed (as a proxy to energy and hardware consumption) and model accuracy, with
the goal of facilitating domain adaptation in low-resource environments. Our
findings are relevant to any setting where energy efficiency, accessibility, or
limited hardware availability are key concerns.

</details>


### [28] [Olica: Efficient Structured Pruning of Large Language Models without Retraining](https://arxiv.org/abs/2506.08436)
*Jiujun He,Huazhen Lin*

Key words: LLM剪枝, 正交分解, PCA, 线性校准, 无需重新训练

TL;DR: Olica是一种无需重新训练的大型语言模型剪枝框架，通过正交分解和线性校准高效压缩模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有LLM剪枝方法需要大量计算和数据资源进行重新训练，成本过高。

Method: 利用PCA处理多头注意力层的矩阵乘积，并通过线性校准减少前馈网络层剪枝的误差累积。

Result: Olica在数据使用、GPU内存和运行时间上高效，同时在多个基准测试中表现优异。

Conclusion: Olica提供了一种高效、无需重新训练的LLM剪枝解决方案。

Abstract: Most existing structured pruning methods for Large Language Models (LLMs)
require substantial computational and data resources for retraining to
reestablish the corrupted correlations, making them prohibitively expensive. To
address this, we propose a pruning framework for LLMs called Orthogonal
decomposition and Linear Calibration (Olica), which eliminates the need for
retraining. A key observation is that the multi-head attention (MHA) layer
depends on two types of matrix products. By treating these matrix products as
unified entities and applying principal component analysis (PCA), we extract
the most important information to compress LLMs without sacrificing accuracy or
disrupting their original structure. Consequently, retraining becomes
unnecessary. A fast decomposition method is devised, reducing the complexity of
PCA by a factor of the square of the number of attention heads. Additionally,
to mitigate error accumulation problem caused by pruning the feed-forward
network (FFN) layer, we introduce a linear calibration method to reconstruct
the residual errors of pruned layers using low-rank matrices. By leveraging
singular value decomposition (SVD) on the solution of the least-squares
problem, these matrices are obtained without requiring retraining. Extensive
experiments show that the proposed Olica is efficient in terms of data usage,
GPU memory, and running time, while delivering superior performance across
multiple benchmarks.

</details>


### [29] [Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning](https://arxiv.org/abs/2506.08477)
*Fengjun Pan,Anh Tuan Luu,Xiaobao Wu*

Key words: 有害模因检测、资源效率、可解释性、思维链提示、多模态模型

TL;DR: 提出了一种名为U-CoT+的新型框架，用于高效、灵活且可解释的有害模因检测。该方法通过将视觉模因转换为详细的文本描述，并结合人类制定的指导原则，实现了低资源消耗和高适应性的检测。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前的有害模因检测方法在资源效率、灵活性和可解释性方面存在不足，限制了其在内容审核系统中的实际应用。U-CoT+旨在解决这些问题。

Method: 开发了一个高保真的模因到文本转换管道，将视觉模因转换为文本描述，并结合零样本思维链（CoT）提示和人类制定的指导原则进行有害性判断。

Result: 在七个基准数据集上的实验验证了该框架的有效性，证明了其在小规模语言模型上的高效性和可解释性。

Conclusion: U-CoT+框架在有害模因检测中表现出高效、灵活和可解释的优势，适用于多平台、多区域的动态需求。

Abstract: Detecting harmful memes is essential for maintaining the integrity of online
environments. However, current approaches often struggle with resource
efficiency, flexibility, or explainability, limiting their practical deployment
in content moderation systems. To address these challenges, we introduce
U-CoT+, a novel framework for harmful meme detection. Instead of relying solely
on prompting or fine-tuning multimodal models, we first develop a high-fidelity
meme-to-text pipeline that converts visual memes into detail-preserving textual
descriptions. This design decouples meme interpretation from meme
classification, thus avoiding immediate reasoning over complex raw visual
content and enabling resource-efficient harmful meme detection with general
large language models (LLMs). Building on these textual descriptions, we
further incorporate targeted, interpretable human-crafted guidelines to guide
models' reasoning under zero-shot CoT prompting. As such, this framework allows
for easy adaptation to different harmfulness detection criteria across
platforms, regions, and over time, offering high flexibility and
explainability. Extensive experiments on seven benchmark datasets validate the
effectiveness of our framework, highlighting its potential for explainable and
low-resource harmful meme detection using small-scale LLMs. Codes and data are
available at: https://anonymous.4open.science/r/HMC-AF2B/README.md.

</details>


### [30] [Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$](https://arxiv.org/abs/2506.08479)
*Chihiro Taguchi,Seiji Maekawa,Nikita Bhutani*

Key words: 检索增强生成, 长上下文语言模型, 开放领域问答, 自适应检索, 效率优化

TL;DR: 该论文提出了一种自适应检索方法Adaptive-$k$，根据查询与候选段落的相似度分布动态选择段落数量，无需模型微调或额外推理，显著提升了问答系统的效率和准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决现有方法在开放领域问答（QA）中固定检索量导致资源浪费或关键信息遗漏的问题，尤其是针对聚合QA这类未知且多变的最佳上下文长度任务。

Method: 提出了Adaptive-$k$检索方法，通过单次相似度分数分布自适应选择段落数量，无需微调模型、额外推理或修改现有检索-阅读流程。

Result: Adaptive-$k$在事实性和聚合QA基准测试中表现优于固定$k$基线方法，同时减少了90%的token使用量，仍能检索到70%的相关段落，显著提升了多种模型的准确性。

Conclusion: 动态调整上下文长度能够显著提高问答系统的效率和准确性，Adaptive-$k$是一种简单有效的方法。

Abstract: Retrieval-augmented generation (RAG) and long-context language models (LCLMs)
both address context limitations of LLMs in open-domain question answering
(QA). However, optimal external context to retrieve remains an open problem:
fixing the retrieval size risks either wasting tokens or omitting key evidence.
Existing adaptive methods like Self-RAG and Self-Route rely on iterative LLM
prompting and perform well on factoid QA, but struggle with aggregation QA,
where the optimal context size is both unknown and variable. We present
Adaptive-$k$ retrieval, a simple and effective single-pass method that
adaptively selects the number of passages based on the distribution of the
similarity scores between the query and the candidate passages. It does not
require model fine-tuning, extra LLM inferences or changes to existing
retriever-reader pipelines. On both factoid and aggregation QA benchmarks,
Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x
fewer tokens than full-context input, yet still retrieves 70% of relevant
passages. It improves accuracy across five LCLMs and two embedding models,
highlighting that dynamically adjusting context size leads to more efficient
and accurate QA.

</details>


### [31] [Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models](https://arxiv.org/abs/2506.08480)
*Huixuan Zhang,Xiaojun Wan*

Key words: 文本-图像生成, 评估框架, 图像-文本对齐

TL;DR: 本文研究发现现有文本-图像生成评估框架存在不足，提出了改进建议。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有评估主要关注与人类判断的一致性，忽略了评估框架的其他关键特性。

Method: 识别两个关键评估特性，并通过实验证明主流评估框架的不足。

Result: 当前框架未能完全满足这些特性。

Conclusion: 提出了改进图像-文本对齐评估的建议。

Abstract: Text-to-image models often struggle to generate images that precisely match
textual prompts. Prior research has extensively studied the evaluation of
image-text alignment in text-to-image generation. However, existing evaluations
primarily focus on agreement with human assessments, neglecting other critical
properties of a trustworthy evaluation framework. In this work, we first
identify two key aspects that a reliable evaluation should address. We then
empirically demonstrate that current mainstream evaluation frameworks fail to
fully satisfy these properties across a diverse range of metrics and models.
Finally, we propose recommendations for improving image-text alignment
evaluation.

</details>


### [32] [Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models](https://arxiv.org/abs/2506.08487)
*Sumanth Manduru,Carlotta Domeniconi*

Key words: 小规模语言模型,公平性,量化,伦理风险,零样本评估

TL;DR: 该论文首次大规模审计了0.5至50亿参数的中等规模语言模型，发现Phi模型在保持高效的同时兼具公平性，而其他架构的模型则存在不同的偏见问题，为资源受限场景中的SLMs部署提供了实用指导。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究小规模语言模型（SLMs）在公平性和效率方面的表现，填补了对中等规模模型伦理风险的研究空白。

Method: 使用BBQ基准测试评估9个开源模型（Qwen 2.5、LLaMA 3.2、Gemma 3和Phi系列）在零样本提示下的效能和公平性。

Result: Phi模型展现90%以上的F1分数且偏见最小；Qwen 2.5模型存在虚假中立性；LLaMA 3.2模型过度自信且偏见更强；4位AWQ量化对性能与偏见有复杂影响。

Conclusion: 高效且公平的SLMs是可行的，但需根据架构和量化权衡选择，为资源受限的部署提供了实践指南。

Abstract: The rapid adoption of Small Language Models (SLMs) for on-device and
resource-constrained deployments has outpaced our understanding of their
ethical risks. To the best of our knowledge, we present the first large-scale
audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an
overlooked "middle tier" between BERT-class encoders and flagship LLMs. Our
evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma
3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we
analyze both utility and fairness across ambiguous and disambiguated contexts.
This evaluation reveals three key insights. First, competence and fairness need
not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while
exhibiting minimal bias, showing that efficient and ethical NLP is attainable.
Second, social bias varies significantly by architecture: Qwen 2.5 models may
appear fair, but this often reflects vacuous neutrality, random guessing, or
evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2
models exhibit stronger stereotypical bias, suggesting overconfidence rather
than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ
quantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but
increases disability-related bias in Phi-4-Mini by over 7 percentage points.
These insights provide practical guidance for the responsible deployment of
SLMs in applications demanding fairness and efficiency, particularly benefiting
small enterprises and resource-constrained environments.

</details>


### [33] [EtiCor++: Towards Understanding Etiquettical Bias in LLMs](https://arxiv.org/abs/2506.08488)
*Ashutosh Dwivedi,Siddhant Shivdutt Singh,Ashutosh Modi*

Key words: LLMs, 文化敏感性, 礼仪, 偏见评估, EtiCor++

TL;DR: 该论文介绍了EtiCor++语料库，用于评估LLMs对全球礼仪的知识和偏见。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究LLMs的文化敏感性，尤其是区域礼仪的理解和偏见。

Method: 构建EtiCor++语料库，设计评估任务和度量标准。

Result: 实验显示LLMs对某些区域存在固有偏见。

Conclusion: EtiCor++为评估LLMs礼仪知识及偏见提供了有效资源。

Abstract: In recent years, researchers have started analyzing the cultural sensitivity
of LLMs. In this respect, Etiquettes have been an active area of research.
Etiquettes are region-specific and are an essential part of the culture of a
region; hence, it is imperative to make LLMs sensitive to etiquettes. However,
there needs to be more resources in evaluating LLMs for their understanding and
bias with regard to etiquettes. In this resource paper, we introduce EtiCor++,
a corpus of etiquettes worldwide. We introduce different tasks for evaluating
LLMs for knowledge about etiquettes across various regions. Further, we
introduce various metrics for measuring bias in LLMs. Extensive experimentation
with LLMs shows inherent bias towards certain regions.

</details>


### [34] [Integration of Old and New Knowledge for Generalized Intent Discovery: A Consistency-driven Prototype-Prompting Framework](https://arxiv.org/abs/2506.08490)
*Xiao Wei,Xiaobao Wang,Ning Zhuang,Chenyang Wang,Longbiao Wang,Jianwu dang*

Key words: 广义意图发现（GID）、一致性驱动、原型提示、领域适应

TL;DR: 该论文提出了一种基于一致性驱动的原型提示框架，用于广义意图发现（GID），通过结合新旧知识解决现有方法忽略领域适应的问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法在广义意图发现（GID）中仅关注无监督数据聚类，而忽视了领域适应，限制了其实际应用。

Method: 提出一致性驱动的原型提示框架，包括原型提示框架（用于从外部源转移旧知识）和分层一致性约束（用于从目标域学习新知识）。

Result: 实验结果表明，该方法显著优于所有基线方法，达到了最先进的性能。

Conclusion: 该方法在广义意图发现任务中表现出高效性和泛化能力。

Abstract: Intent detection aims to identify user intents from natural language inputs,
where supervised methods rely heavily on labeled in-domain (IND) data and
struggle with out-of-domain (OOD) intents, limiting their practical
applicability. Generalized Intent Discovery (GID) addresses this by leveraging
unlabeled OOD data to discover new intents without additional annotation.
However, existing methods focus solely on clustering unsupervised data while
neglecting domain adaptation. Therefore, we propose a consistency-driven
prototype-prompting framework for GID from the perspective of integrating old
and new knowledge, which includes a prototype-prompting framework for
transferring old knowledge from external sources, and a hierarchical
consistency constraint for learning new knowledge from target domains. We
conducted extensive experiments and the results show that our method
significantly outperforms all baseline methods, achieving state-of-the-art
results, which strongly demonstrates the effectiveness and generalization of
our methods. Our source code is publicly available at
https://github.com/smileix/cpp.

</details>


### [35] [DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs](https://arxiv.org/abs/2506.08500)
*Arie Cattan,Alon Jacovi,Ori Ram,Jonathan Herzig,Roee Aharoni,Sasha Goldshtein,Eran Ofek,Idan Szpektor,Avi Caciularu*

Key words: Retrieval Augmented Generation, knowledge conflict, LLMs, benchmark, CONFLICTS

TL;DR: 论文提出了RAG中知识冲突的新分类法，并创建了CONFLICTS基准测试，实验显示LLMs在处理冲突时表现不佳，但显式推理能改善结果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究RAG中检索到的信息可能冲突的问题，探索LLMs如何应对此类冲突。

Method: 提出知识冲突的分类法和CONFLICTS基准，并进行广泛实验。

Result: LLMs处理冲突的能力有限，但显式推理能提升响应质量。

Conclusion: 未来研究仍需改进LLMs在冲突解决中的表现。

Abstract: Retrieval Augmented Generation (RAG) is a commonly used approach for
enhancing large language models (LLMs) with relevant and up-to-date
information. However, the retrieved sources can often contain conflicting
information and it remains unclear how models should address such
discrepancies. In this work, we first propose a novel taxonomy of knowledge
conflict types in RAG, along with the desired model behavior for each type. We
then introduce CONFLICTS, a high-quality benchmark with expert annotations of
conflict types in a realistic RAG setting. CONFLICTS is the first benchmark
that enables tracking progress on how models address a wide range of knowledge
conflicts. We conduct extensive experiments on this benchmark, showing that
LLMs often struggle to appropriately resolve conflicts between sources. While
prompting LLMs to explicitly reason about the potential conflict in the
retrieved documents significantly improves the quality and appropriateness of
their responses, substantial room for improvement in future research remains.

</details>


### [36] [CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations](https://arxiv.org/abs/2506.08504)
*Divyaksh Shukla,Ritesh Baviskar,Dwijesh Gohil,Aniket Tiwari,Atul Shree,Ashutosh Modi*

Key words: 话语解析, 多模态, 代码混合, CoMuMDR, 多领域

TL;DR: 本文介绍了CoMuMDR语料库，用于多领域、多模态、代码混合的对话话语解析，展示了现有模型在此类数据集上的挑战性表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的话语解析数据集局限于单一领域的英语对话，缺乏多模态和代码混合的多样性。

Method: 构建了一个包含印地语和英语代码混合的多模态、多领域语料库CoMuMDR，并标注了九种话语关系，测试了多种SoTA模型。

Result: SoTA模型在CoMuMDR上的表现较差，表明现有模型在多领域代码混合数据中的局限性。

Conclusion: 需要开发更强大的模型以适应多模态、代码混合的现实场景。

Abstract: Discourse parsing is an important task useful for NLU applications such as
summarization, machine comprehension, and emotion recognition. The current
discourse parsing datasets based on conversations consists of written English
dialogues restricted to a single domain. In this resource paper, we introduce
CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in
conversations. The corpus (code-mixed in Hindi and English) has both audio and
transcribed text and is annotated with nine discourse relations. We experiment
with various SoTA baseline models; the poor performance of SoTA models
highlights the challenges of multi-domain code-mixed corpus, pointing towards
the need for developing better models for such realistic settings.

</details>


### [37] [Efficient Post-Training Refinement of Latent Reasoning in Large Language Models](https://arxiv.org/abs/2506.08552)
*Xinyuan Wang,Dongjie Wang,Wangyang Ying,Haoyue Bai,Nanxu Gong,Sixun Dong,Kunpeng Liu,Yanjie Fu*

Key words: 大语言模型, 潜在推理, 后训练, 对比反馈, 嵌入精炼

TL;DR: 提出了一种轻量级后训练框架，通过对比推理反馈和残差嵌入精炼策略，优化大语言模型的潜在推理轨迹，显著提升了推理任务的准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决链式思维提示方法中存在的令牌开销大和推理轨迹固定的问题，以及潜在推理方法中如何有效更新推理嵌入的后训练挑战。

Method: 1) 对比推理反馈：通过比较强、弱基线推理嵌入，推断有效更新方向；2) 残差嵌入精炼：通过渐进整合当前和历史梯度，稳定更新过程。

Result: 在五个推理基准测试中验证了框架的有效性，其中MathQA任务实现了5%的准确率提升。

Conclusion: 该框架通过优化潜在推理轨迹，显著提升了推理任务的性能，无需额外训练即可实现改进。

Abstract: Reasoning is a key component of language understanding in Large Language
Models. While Chain-of-Thought prompting enhances performance via explicit
intermediate steps, it suffers from sufficient token overhead and a fixed
reasoning trajectory, preventing step-wise refinement. Recent advances in
latent reasoning address these limitations by refining internal reasoning
processes directly in the model's latent space, without producing explicit
outputs. However, a key challenge remains: how to effectively update reasoning
embeddings during post-training to guide the model toward more accurate
solutions. To overcome this challenge, we propose a lightweight post-training
framework that refines latent reasoning trajectories using two novel
strategies: 1) Contrastive reasoning feedback, which compares reasoning
embeddings against strong and weak baselines to infer effective update
directions via embedding enhancement; 2) Residual embedding refinement, which
stabilizes updates by progressively integrating current and historical
gradients, enabling fast yet controlled convergence. Extensive experiments and
case studies are conducted on five reasoning benchmarks to demonstrate the
effectiveness of the proposed framework. Notably, a 5\% accuracy gain on MathQA
without additional training.

</details>


### [38] [Neighbors and relatives: How do speech embeddings reflect linguistic connections across the world?](https://arxiv.org/abs/2506.08564)
*Tuukka Törö,Antti Suni,Juraj Šimko*

Key words: 机器学习, 语言嵌入, 语言关系, XLS-R, 类型学

TL;DR: 该研究利用自监督语言识别模型XLS-R的嵌入表示，分析了106种世界语言的关系，发现嵌入距离与传统语言学测度一致，提供了语言变异研究的可扩展方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在通过机器学习方法，以数据驱动的方式探索全球语言关系，弥补传统语言学和类型学方法的局限性。

Method: 使用XLS-R模型的嵌入表示，通过线性判别分析（LDA）聚类语言，并与谱系、词汇和地理距离进行比较。

Result: 嵌入距离与传统测度高度一致，能够捕捉全局和局部的类型学模式，但可视化语言关系仍存在挑战。

Conclusion: 该方法为语言变异研究提供了新视角，尤其在低资源语言和宏观-微观语言变异研究中具有潜力。

Abstract: Investigating linguistic relationships on a global scale requires analyzing
diverse features such as syntax, phonology and prosody, which evolve at varying
rates influenced by internal diversification, language contact, and
sociolinguistic factors. Recent advances in machine learning (ML) offer
complementary alternatives to traditional historical and typological
approaches. Instead of relying on expert labor in analyzing specific linguistic
features, these new methods enable the exploration of linguistic variation
through embeddings derived directly from speech, opening new avenues for
large-scale, data-driven analyses.
  This study employs embeddings from the fine-tuned XLS-R self-supervised
language identification model voxlingua107-xls-r-300m-wav2vec, to analyze
relationships between 106 world languages based on speech recordings. Using
linear discriminant analysis (LDA), language embeddings are clustered and
compared with genealogical, lexical, and geographical distances. The results
demonstrate that embedding-based distances align closely with traditional
measures, effectively capturing both global and local typological patterns.
Challenges in visualizing relationships, particularly with hierarchical
clustering and network-based methods, highlight the dynamic nature of language
change.
  The findings show potential for scalable analyses of language variation based
on speech embeddings, providing new perspectives on relationships among
languages. By addressing methodological considerations such as corpus size and
latent space dimensionality, this approach opens avenues for studying
low-resource languages and bridging macro- and micro-level linguistic
variation. Future work aims to extend these methods to underrepresented
languages and integrate sociolinguistic variation for a more comprehensive
understanding of linguistic diversity.

</details>


### [39] [Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement](https://arxiv.org/abs/2506.00160)
*Qihui Fan,Enfu Nan,Wenbo Li,Lei Lu,Pu Zhao,Yanzhi Wang*

Key words: 大型语言模型, 狼人杀, 文本转语音, 社交推理游戏, DeepSeek

TL;DR: 本文提出了一种基于大型语言模型（LLM）的新型狼人杀游戏系统，通过优化的文本转语音（TTS）模型提升兼容性和用户参与度，指出随着LLM推理能力的增强，额外组件将变得不必要。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着LLM推理和说服能力的提升，尤其是在DeepSeek R1和V3模型的推动下，开发更具吸引力的基于LLM代理的社交推理游戏（如狼人杀）成为可能。

Method: 提出了一种简单直接的基于LLM的狼人杀系统，并搭配优化的TTS模型，以提高与不同LLM模型的兼容性和用户体验。

Result: 该系统通过优化的TTS模型增强了用户参与度，并表明随着LLM推理能力的进步，额外组件将不再必要。

Conclusion: 基于LLM的狼人杀系统在无需额外组件的情况下，可以通过优化的TTS模型显著提升用户体验。

Abstract: The growing popularity of social deduction game systems for both business
applications and AI research has greatly benefited from the rapid advancements
in Large Language Models (LLMs), which now demonstrate stronger reasoning and
persuasion capabilities. Especially with the raise of DeepSeek R1 and V3
models, LLMs should enable a more engaging experience for human players in
LLM-agent-based social deduction games like Werewolf. Previous works either
fine-tuning, advanced prompting engineering, or additional experience pool to
achieve engaging text-format Werewolf game experience. We propose a novel yet
straightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS)
models designed for enhanced compatibility with various LLM models, and
improved user engagement. We argue with ever enhancing LLM reasoning, extra
components will be unnecessary in the case of Werewolf.

</details>


### [40] [CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmark of Large Language Models in Mental Health Counseling](https://arxiv.org/abs/2506.08584)
*Yahan Li,Jifan Yao,John Bosco S. Bunyi,Adam C. Frank,Angel Hwang,Ruishan Liu*

Key words: LLMs,心理健康,基准测试,安全性

TL;DR: CounselBench是一个由100名心理健康专家开发的大规模基准测试,用于评估和压力测试LLMs在单轮咨询中的表现,发现LLMs在感知质量上常优于人类治疗师,但存在安全隐患。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 测试LLMs在真实心理咨询场景中的行为,因为其在此领域的使用日益增多但缺乏充分测试。

Method: 开发CounselBench-EVAL和CounselBench-Adv,分别包含2000个专家评估和120个对抗性问题,共评估2880个LLMs响应。

Result: LLMs在感知质量上常优于人类治疗师,但专家频繁指出其输出存在安全隐患,如未经授权的医疗建议。

Conclusion: CounselBench为高风险心理健康环境中LLMs的行为提供了一个临床基础框架。

Abstract: Large language models (LLMs) are increasingly proposed for use in mental
health support, yet their behavior in realistic counseling scenarios remains
largely untested. We introduce CounselBench, a large-scale benchmark developed
with 100 mental health professionals to evaluate and stress-test LLMs in
single-turn counseling. The first component, CounselBench-EVAL, contains 2,000
expert evaluations of responses from GPT-4, LLaMA 3, Gemini, and online human
therapists to real patient questions. Each response is rated along six
clinically grounded dimensions, with written rationales and span-level
annotations. We find that LLMs often outperform online human therapists in
perceived quality, but experts frequently flag their outputs for safety
concerns such as unauthorized medical advice. Follow-up experiments show that
LLM judges consistently overrate model responses and overlook safety issues
identified by human experts. To probe failure modes more directly, we construct
CounselBench-Adv, an adversarial dataset of 120 expert-authored counseling
questions designed to trigger specific model issues. Evaluation across 2,880
responses from eight LLMs reveals consistent, model-specific failure patterns.
Together, CounselBench establishes a clinically grounded framework for
benchmarking and improving LLM behavior in high-stakes mental health settings.

</details>


### [41] [Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings](https://arxiv.org/abs/2506.08592)
*Liyan Xu,Zhenlin Su,Mo Yu,Jiangnan Li,Fandong Meng,Jie Zhou*

Key words: 文本编码器、密集检索、细粒度语义、CapRetrieval、数据生成

TL;DR: 该论文研究发现，文本编码器在识别细粒度实体或事件时存在局限性，导致密集检索失败。作者提出了中文评估数据集CapRetrieval，并通过数据生成策略优化编码器，最终取得最佳性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是解决文本编码器在细粒度实体或事件识别中的局限性，提升密集检索的效果。

Method: 方法包括引入中文评估数据集CapRetrieval，提出数据生成策略对编码器进行微调，并处理嵌入的粒度困境问题。

Result: 零样本评估显示编码器在细粒度匹配上表现不佳，但通过微调后，在CapRetrieval上取得最佳性能。

Conclusion: 结论指出，通过数据生成策略可以有效提升编码器在细粒度语义匹配上的性能，并提出粒度困境是一个重要挑战。

Abstract: This work focuses on an observed limitation of text encoders: embeddings may
not be able to recognize fine-grained entities or events within the semantics,
resulting in failed dense retrieval on even simple cases. To examine such
behaviors, we first introduce a new evaluation dataset in Chinese, named
CapRetrieval, whose passages are image captions, and queries are phrases
inquiring entities or events in various forms. Zero-shot evaluation suggests
that encoders may fail on these fine-grained matching, regardless of training
sources or model sizes. Aiming for enhancement, we proceed to finetune encoders
with our proposed data generation strategies, which obtains the best
performance on CapRetrieval. Within this process, we further identify an issue
of granularity dilemma, a challenge for embeddings to express fine-grained
salience while aligning with overall semantics. Our dataset, code and models in
this work are publicly released at https://github.com/lxucs/CapRetrieval.

</details>


### [42] [Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework](https://arxiv.org/abs/2506.05695)
*Lingyuan Liu,Mengxiang Zhang*

Key words: 知识蒸馏、大语言模型、课程学习、渐进超负荷、POCL

TL;DR: 论文提出了一种新颖的课程学习框架POCL，用于改进大语言模型的知识蒸馏过程，通过渐进式引入训练样本并调整损失函数温度，显著提升了学生模型的稳定性和性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的大语言模型知识蒸馏方法在训练过程中容易出现分布偏移，导致灾难性遗忘、模式崩溃和训练推理不匹配等问题。为解决这些问题，作者受“渐进超负荷”训练原则启发，提出了POCL框架。

Method: POCL框架包含难度测量器和训练调度器两个核心组件。前者对训练样本按难度排序和分区，后者逐步引入样本并动态调整损失函数温度，从容易样本开始渐进增加难度。

Result: 在指令跟随任务中的实验表明，POCL显著提升了各种白盒知识蒸馏方法和模型家族中学生模型的性能，证明了排序训练样本的有效性。

Conclusion: POCL通过结构化训练数据，提升了知识蒸馏的稳定性和性能，为大语言模型的压缩提供了新的思路。

Abstract: Knowledge Distillation (KD) compresses large language models (LLMs) by
transferring the teacher model's capabilities to a smaller student model,
reducing inference cost and memory usage while maintaining performance.
However, existing KD methods for LLMs often fail to prevent significant shifts
in the student model's distribution during training, leading to issues such as
catastrophic forgetting, mode collapse, and training-inference mismatch. To
address these challenges, we propose a novel, plug-in curriculum learning
framework inspired by the strength training principle of "progressive overload"
(POCL), which can be seamlessly integrated into existing white-box KD
approaches with minimal computational overhead. The framework comprises two
core components: (1) a difficulty measurer that ranks and partitions training
samples from easy to hard, and (2) a training scheduler that incrementally
introduces these subsets into the distillation process at fixed intervals while
applying loss functions with progressively rising temperatures. By starting
with the easiest samples and progressively increasing the difficulty, the
approach enhances both the stability and efficiency of learning. Extensive
experiments in instruction-following settings demonstrate that POCL
consistently improves the performance of distilled student models across
various white-box KD methods and model families. Our findings highlight the
effectiveness of sorted training samples in KD for LLMs. More generally, our
work demonstrates how to structure training data within the KD process to
enhance the stability and performance of distilled LLMs.

</details>


### [43] [Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models](https://arxiv.org/abs/2506.08593)
*Shuzhou Yuan,Ercong Nie,Mario Tawfelis,Helmut Schmid,Hinrich Schütze,Michael Färber*

Key words: 仇恨言论检测、MBTI、大型语言模型、性格特质、公平性

TL;DR: 本文研究了MBTI性格特质对大型语言模型（LLMs）在仇恨言论检测中的影响，发现不同性格提示会导致显著差异，需谨慎设计以确保公平性和人类价值观一致性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 仇恨言论检测具有社会敏感性和主观性，且性格特质对LLMs的影响尚未充分研究。本文填补了这一空白。

Method: 通过MBTI性格提示对四个开源LLMs进行仇恨言论分类评估，并结合人类标注调查验证其影响。

Result: 发现性格提示导致模型输出显著变化，包括与真实标注不一致、性格间分歧和逻辑层面的偏见。

Conclusion: 在LLM标注流程中，需谨慎定义性格提示以确保公平性和人类价值观对齐。

Abstract: Hate speech detection is a socially sensitive and inherently subjective task,
with judgments often varying based on personal traits. While prior work has
examined how socio-demographic factors influence annotation, the impact of
personality traits on Large Language Models (LLMs) remains largely unexplored.
In this paper, we present the first comprehensive study on the role of persona
prompts in hate speech classification, focusing on MBTI-based traits. A human
annotation survey confirms that MBTI dimensions significantly affect labeling
behavior. Extending this to LLMs, we prompt four open-source models with MBTI
personas and evaluate their outputs across three hate speech datasets. Our
analysis uncovers substantial persona-driven variation, including
inconsistencies with ground truth, inter-persona disagreement, and logit-level
biases. These findings highlight the need to carefully define persona prompts
in LLM-based annotation workflows, with implications for fairness and alignment
with human values.

</details>


### [44] [RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval](https://arxiv.org/abs/2506.08625)
*Minhae Oh,Jeonghye Kim,Nakyung Lee,Donggeon Seo,Taeuk Kim,Jungwoo Lee*

Key words: 科学推理, 检索增强框架, RAISE, 逻辑检索, 问题分解

TL;DR: RAISE是一个分三步的检索增强框架，通过问题分解、逻辑查询生成和逻辑检索来解决科学推理中的挑战，并在性能上优于其他基线方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 科学推理需要长链推理、领域术语知识和适应最新发现，RAISE旨在解决这些挑战。

Method: RAISE分为问题分解、逻辑查询生成和逻辑检索三个步骤，从广泛语料库中检索逻辑相关的文档。

Result: RAISE在科学推理基准测试中持续优于其他基线方法，检索的文档不仅领域知识相似，逻辑相关性也更强。

Conclusion: RAISE通过分步检索逻辑相关文档，提升了科学推理的效果。

Abstract: Scientific reasoning requires not only long-chain reasoning processes, but
also knowledge of domain-specific terminologies and adaptation to updated
findings. To deal with these challenges for scientific reasoning, we introduce
RAISE, a step-by-step retrieval-augmented framework which retrieves logically
relevant documents from in-the-wild corpus. RAISE is divided into three steps:
problem decomposition, logical query generation, and logical retrieval. We
observe that RAISE consistently outperforms other baselines on scientific
reasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves
documents that are not only similar in terms of the domain knowledge, but also
documents logically more relevant.

</details>


### [45] [MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models](https://arxiv.org/abs/2506.08643)
*Son The Nguyen,Theja Tulabandhula*

Key words: 大语言模型, 解码优化, 黑盒优化, 元启发算法, 人类偏好对齐

TL;DR: MEMETRON是一个任务无关框架，利用混合元启发算法优化LLM的解码过程，无需重新训练模型即可高效发现高奖励响应。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有LLM的解码策略（如贪婪搜索、采样或重排）缺乏对任务特定目标的明确优化，限制了控制能力。

Method: MEMETRON将LLM解码建模为离散黑盒优化问题，结合GENETRON和ANNETRON两种混合元启发算法进行搜索。

Result: 在人类偏好对齐任务中，MEMETRON显著优于标准解码和重排方法。

Conclusion: MEMETRON框架展示了在不重新训练模型的情况下提升对齐效果的潜力。

Abstract: Large language models (LLMs) are increasingly used for both open-ended and
structured tasks, yet their inference-time behavior is still largely dictated
by heuristic decoding strategies such as greedy search, sampling, or reranking.
These methods provide limited control and do not explicitly optimize for
task-specific objectives. We introduce MEMETRON, a task-agnostic framework that
formulates LLM decoding as a discrete black-box optimization problem. MEMETRON
leverages hybrid metaheuristic algorithms, GENETRON and ANNETRON, to search the
response space, guided by reward models and contextual operations performed by
the LLM itself. This approach enables efficient discovery of high-reward
responses without requiring model retraining or gradient access. The framework
is modular and generalizes across diverse tasks, requiring only a reward
function and lightweight prompt templates. We evaluate our framework on the
critical human preference alignment task and demonstrate that it significantly
outperforms standard decoding and reranking methods, highlighting its potential
to improve alignment without model retraining.

</details>


### [46] [TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning](https://arxiv.org/abs/2506.08646)
*Mingyu Zheng,Zhifan Feng,Jia Wang,Lanrui Wang,Zheng Lin,Yang Hao,Weiping Wang*

Key words: TableDreamer, LLM, 数据合成, 表指令调优, 弱点引导

TL;DR: TableDreamer提出了一种渐进式和弱点引导的数据合成框架，用于解决LLM在表指令调优中数据多样性和效率不足的问题，显著提升了模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前基于LLM的数据合成方法在表指令调优中存在数据多样性和效率不足的问题。

Method: TableDreamer通过合成多样化种子数据并在新发现的弱点数据指引下迭代探索输入空间。

Result: 在10个表格基准测试中，Llama3.1-8B-instruct的平均准确率提升11.62%，优于现有方法。

Conclusion: TableDreamer框架有效提升表指令调优的数据多样性和效率，显著改进LLM性能。

Abstract: Despite the commendable progress of recent LLM-based data synthesis methods,
they face two limitations in generating table instruction tuning data. First,
they can not thoroughly explore the vast input space of table understanding
tasks, leading to limited data diversity. Second, they ignore the weaknesses in
table understanding ability of the target LLM and blindly pursue the increase
of data quantity, resulting in suboptimal data efficiency. In this paper, we
introduce a progressive and weakness-guided data synthesis framework tailored
for table instruction tuning, named TableDreamer, to mitigate the above issues.
Specifically, we first synthesize diverse tables and related instructions as
seed data, and then perform an iterative exploration of the input space under
the guidance of the newly identified weakness data, which eventually serve as
the final training data for fine-tuning the target LLM. Extensive experiments
on 10 tabular benchmarks demonstrate the effectiveness of the proposed
framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62%
(49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms
state-of-the-art data synthesis baselines which use more training data. The
code and data is available at https://github.com/SpursGoZmy/TableDreamer

</details>


### [47] [Summarization for Generative Relation Extraction in the Microbiome Domain](https://arxiv.org/abs/2506.08647)
*Oumaima El Khettari,Solen Quiniou,Samuel Chaffron*

Key words: 关系抽取,肠道微生物组,大型语言模型,低资源领域

TL;DR: 利用大型语言模型（LLMs）和生成式关系抽取（RE）方法研究肠道微生物组，初步结果显示生成式方法在低资源生物医学领域有潜力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究肠道微生物组中的相互作用，这是一个复杂且资源匮乏的生物医学领域。

Method: 结合LLM的摘要技术和指令调优的生成方法来进行关系抽取。

Result: 摘要技术能减少噪音并提升生成式RE性能，但BERT-based方法仍表现更优。

Conclusion: 生成式方法在低资源领域的研究中显示出潜力，但仍需改进。

Abstract: We explore a generative relation extraction (RE) pipeline tailored to the
study of interactions in the intestinal microbiome, a complex and low-resource
biomedical domain. Our method leverages summarization with large language
models (LLMs) to refine context before extracting relations via
instruction-tuned generation. Preliminary results on a dedicated corpus show
that summarization improves generative RE performance by reducing noise and
guiding the model. However, BERT-based RE approaches still outperform
generative models. This ongoing work demonstrates the potential of generative
methods to support the study of specialized domains in low-resources setting.

</details>


### [48] [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/abs/2506.08672)
*Yang Liu,Jiaqi Li,Zilong Zheng*

Key words: 规则推理, 小规模推理模型, 强化学习, 动态采样, 领域增强

TL;DR: 论文提出了一种名为RuleReasoner的强化学习方法，用于提升小规模推理模型（SRMs）在规则推理中的表现，并通过动态采样和领域增强实现高效的泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决SRMs在规则推理中泛化能力不足的问题，探索其在多任务和多领域中的高效学习方法。

Method: 采用强化学习和动态采样技术，通过历史奖励更新采样权重，实现领域增强和灵活的训练调度。

Result: 在ID和OOD任务中，RuleReasoner显著优于前沿的大规模推理模型（LRMs），并展现出更高的计算效率。

Conclusion: RuleReasoner是一种简单而有效的方法，能够在规则推理任务中实现强大的泛化能力和计算效率。

Abstract: Rule-based reasoning has been acknowledged as one of the fundamental problems
in reasoning, while deviations in rule formats, types, and complexity in
real-world applications pose severe challenges. Recent studies have shown that
large reasoning models (LRMs) have remarkable reasoning capabilities, and their
performance is substantially enhanced by reinforcement learning (RL). However,
it remains an open question whether small reasoning models (SRMs) can learn
rule-based reasoning effectively with robust generalization across diverse
tasks and domains. To address this, we introduce Reinforced Rule-based
Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct
rule-based reasoning via a wide collection of curated tasks and a novel
domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples
each training batch by updating the sampling weights of different domains based
on historical rewards. This facilitates domain augmentation and flexible online
learning schedules for RL, obviating the need for pre-hoc human-engineered
mix-training recipes used in existing methods. Empirical evaluations on
in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that
RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1%
average points on eight ID tasks and $\Delta$10.4% average points on three OOD
tasks over OpenAI-o1). Notably, our approach also exhibits higher computational
efficiency compared to prior dynamic sampling methods for RL.

</details>


### [49] [Brevity is the soul of sustainability: Characterizing LLM response lengths](https://arxiv.org/abs/2506.08686)
*Soham Poddar,Paramita Koley,Janardan Misra,Sanjay Podder,Navveen Balani,Niloy Ganguly,Saptarshi Ghosh*

Key words: 大型语言模型, 能效优化, 提示工程, 输出压缩, 信息冗余

TL;DR: 论文研究了大型语言模型（LLM）推理过程中的能耗问题，提出通过输出压缩和提示工程优化响应长度，从而显著提升能效。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: LLM的推理过程消耗大量能源，但目前关于输出压缩的研究较少，因此探索通过优化响应长度来减少能耗是重要的研究方向。

Method: 首先对12个解码器LLM在5个数据集上进行基准测试，发现响应长度过长。随后定义LLM响应中的六种信息类别，并提出提示工程策略以减少冗余信息。

Result: 实验表明，针对长度减少和信息内容控制的提示策略可实现25-60%的能耗优化，同时保持LLM响应质量。

Conclusion: 通过简单的提示工程方法可以有效减少LLM响应长度，显著提升能效。

Abstract: A significant portion of the energy consumed by Large Language Models (LLMs)
arises from their inference processes; hence developing energy-efficient
methods for inference is crucial. While several techniques exist for inference
optimization, output compression remains relatively unexplored, with only a few
preliminary efforts addressing this aspect. In this work, we first benchmark 12
decoder-only LLMs across 5 datasets, revealing that these models often produce
responses that are substantially longer than necessary. We then conduct a
comprehensive quality assessment of LLM responses, formally defining six
information categories present in LLM responses. We show that LLMs often tend
to include redundant or additional information besides the minimal answer. To
address this issue of long responses by LLMs, we explore several simple and
intuitive prompt-engineering strategies. Empirical evaluation shows that
appropriate prompts targeting length reduction and controlling information
content can achieve significant energy optimization between 25-60\% by reducing
the response length while preserving the quality of LLM responses.

</details>


### [50] [ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts](https://arxiv.org/abs/2506.08700)
*Ruiran Su,Jiasheng Si,Zhijiang Guo,Janet B. Pierrehumbert*

Key words: 科学事实核查, ClimateViz, 多模态模型, 图表推理, 基准数据集

TL;DR: 论文介绍了ClimateViz，首个基于科学图表的基准数据集，用于科学事实核查，包含49,862条标注的声明和2,896个可视化图表。评估了现有多模态模型的表现，发现其准确率远低于人类水平。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 科学事实核查主要针对文本和表格，忽视了图表在展示定量证据和统计推理中的重要性。

Method: 构建了ClimateViz数据集，包含标注的声明和图表，评估了多模态语言模型的零样本和少样本表现。

Result: 当前模型在基于图表的推理中表现不佳，最佳模型准确率为76.2%至77.8%，低于人类的89.3%至92.7%。

Conclusion: 科学图表的事实核查仍需改进，模型性能有待提升。数据集和代码已开源。

Abstract: Scientific fact-checking has mostly focused on text and tables, overlooking
scientific charts, which are key for presenting quantitative evidence and
statistical reasoning. We introduce ClimateViz, the first large-scale benchmark
for scientific fact-checking using expert-curated scientific charts. ClimateViz
contains 49,862 claims linked to 2,896 visualizations, each labeled as support,
refute, or not enough information. To improve interpretability, each example
includes structured knowledge graph explanations covering trends, comparisons,
and causal relations. We evaluate state-of-the-art multimodal language models,
including both proprietary and open-source systems, in zero-shot and few-shot
settings. Results show that current models struggle with chart-based reasoning:
even the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to
77.8 percent accuracy in label-only settings, far below human performance (89.3
and 92.7 percent). Explanation-augmented outputs improve performance in some
models. We released our dataset and code alongside the paper.

</details>


### [51] [ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization](https://arxiv.org/abs/2506.08712)
*Hee Suk Yoon,Eunseop Yoon,Mark A. Hasegawa-Johnson,Sungwoong Kim,Chang D. Yoo*

Key words: 偏好学习,大型语言模型,直接对齐算法,KL散度,奖励破解

TL;DR: ConfPO是一种基于训练策略信心的偏好学习方法，针对偏好关键令牌进行优化，无需辅助模型或额外计算。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有直接对齐算法（如DPO）对所有令牌概率进行统一调整，忽视了偏好相关性。ConfPO旨在通过聚焦关键令牌提升对齐质量，避免过优化问题。

Method: ConfPO仅依赖训练策略的置信度识别偏好关键令牌，并高效利用KL散度预算进行优化，无需额外模型或计算。

Result: 在AlpacaEval 2和Arena-Hard等基准测试中，ConfPO表现优于统一调整的方法，对齐效果更好且无额外计算开销。

Conclusion: ConfPO通过靶向优化关键令牌，提供了一种简单、高效且无需模型的偏好学习方法，显著提升对齐质量。

Abstract: We introduce ConfPO, a method for preference learning in Large Language
Models (LLMs) that identifies and optimizes preference-critical tokens based
solely on the training policy's confidence, without requiring any auxiliary
models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as
Direct Preference Optimization (DPO), which uniformly adjust all token
probabilities regardless of their relevance to preference, ConfPO focuses
optimization on the most impactful tokens. This targeted approach improves
alignment quality while mitigating overoptimization (i.e., reward hacking) by
using the KL divergence budget more efficiently. In contrast to recent
token-level methods that rely on credit-assignment models or AI annotators,
raising concerns about scalability and reliability, ConfPO is simple,
lightweight, and model-free. Experimental results on challenging alignment
benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO
consistently outperforms uniform DAAs across various LLMs, delivering better
alignment with zero additional computational overhead.

</details>


### [52] [Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure](https://arxiv.org/abs/2506.08713)
*Fariz Ikhwantri,Dusica Marijan*

Key words: 合规性检测, 自然语言推理, 多跳推理, 大语言模型, GDPR

TL;DR: 提出了一种基于自然语言推理（NLI）的合规性检测方法EXCLAIM，通过多跳推理实现可解释和可追溯的合规性检测，并利用大语言模型生成保证案例。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 处理复杂系统合规性检测中的法律与技术文本复杂性、模型解释需求及保证案例数据不足的问题。

Method: 利用NLI构建多跳推理框架，使用大语言模型生成保证案例，并设计覆盖率和结构一致性指标。

Result: 通过GDPR要求的案例研究，验证了该方法的有效性，展示了NLI在自动化合规性检测中的潜力。

Conclusion: EXCLAIM方法为自动化合规性检测提供了一种可解释性强且高效的解决方案。

Abstract: Ensuring complex systems meet regulations typically requires checking the
validity of assurance cases through a claim-argument-evidence framework. Some
challenges in this process include the complicated nature of legal and
technical texts, the need for model explanations, and limited access to
assurance case data. We propose a compliance detection approach based on
Natural Language Inference (NLI): EXplainable CompLiance detection with
Argumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the
claim-argument-evidence structure of an assurance case as a multi-hop inference
for explainable and traceable compliance detection. We address the limited
number of assurance cases by generating them using large language models
(LLMs). We introduce metrics that measure the coverage and structural
consistency. We demonstrate the effectiveness of the generated assurance case
from GDPR requirements in a multi-hop inference task as a case study. Our
results highlight the potential of NLI-based approaches in automating the
regulatory compliance process.

</details>


### [53] [Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition](https://arxiv.org/abs/2506.08717)
*Mehedi Hasan Bijoy,Dejan Porjazovski,Tamás Grósz,Mikko Kurimo*

Key words: 语音情感识别,多语言系统,知识蒸馏,Wav2Vec2.0

TL;DR: 提出了一种语言感知的多教师知识蒸馏方法，用于构建多语言语音情感识别系统，基于Wav2Vec2.0的教师模型，学生模型在英语和芬兰语数据集上表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 多语言语音情感识别面临挑战，旨在通过单一模型实现多语言识别。

Method: 使用Wav2Vec2.0构建单语教师模型，通过多教师知识蒸馏训练多语学生模型。

Result: 学生模型在英语（加权召回72.9）和芬兰语（未加权召回63.4）数据集上表现优越。

Conclusion: 方法在悲伤和中性情感识别中表现突出，但在愤怒和快乐情感识别上仍需改进。

Abstract: Speech Emotion Recognition (SER) is crucial for improving human-computer
interaction. Despite strides in monolingual SER, extending them to build a
multilingual system remains challenging. Our goal is to train a single model
capable of multilingual SER by distilling knowledge from multiple teacher
models. To address this, we introduce a novel language-aware multi-teacher
knowledge distillation method to advance SER in English, Finnish, and French.
It leverages Wav2Vec2.0 as the foundation of monolingual teacher models and
then distills their knowledge into a single multilingual student model. The
student model demonstrates state-of-the-art performance, with a weighted recall
of 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish
dataset, surpassing fine-tuning and knowledge distillation baselines. Our
method excels in improving recall for sad and neutral emotions, although it
still faces challenges in recognizing anger and happiness.

</details>


### [54] [Improved LLM Agents for Financial Document Question Answering](https://arxiv.org/abs/2506.08726)
*Nelvin Tan,Zian Seng,Liang Zhang,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Key words: 大型语言模型,金融文档,数值问答,批评代理,计算器代理

TL;DR: 研究了大型语言模型在金融文档数值问答中的表现，提出了一种改进的批评代理和计算器代理，优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管大型语言模型在许多自然语言处理任务中表现出色，但在金融文档的数值问答中仍有困难，尤其是缺少标注数据时批评代理性能下降问题。

Method: 提出了改进的批评代理和计算器代理，并通过实验验证其交互对性能的影响。

Result: 改进的批评代理和计算器代理优于现有的程序思维方法，且更安全。

Conclusion: 改进代理方法在金融文档数值问答任务中表现优异，交互对其性能有显著影响。

Abstract: Large language models (LLMs) have shown impressive capabilities on numerous
natural language processing tasks. However, LLMs still struggle with numerical
question answering for financial documents that include tabular and textual
data. Recent works have showed the effectiveness of critic agents (i.e.,
self-correction) for this task given oracle labels. Building upon this
framework, this paper examines the effectiveness of the traditional critic
agent when oracle labels are not available, and show, through experiments, that
this critic agent's performance deteriorates in this scenario. With this in
mind, we present an improved critic agent, along with the calculator agent
which outperforms the previous state-of-the-art approach (program-of-thought)
and is safer. Furthermore, we investigate how our agents interact with each
other, and how this interaction affects their performance.

</details>


### [55] [Societal AI Research Has Become Less Interdisciplinary](https://arxiv.org/abs/2506.08738)
*Dror Kris Markus,Fabrizio Gilardi,Daria Stetsenko*

Key words: AI伦理、跨学科研究、社会价值、技术团队、AI治理

TL;DR: 这篇论文分析了2014年至2024年间10万篇AI相关论文，发现跨学科团队更可能关注社会伦理问题，但纯计算机科学团队在此类研究中的占比正在增加，挑战了关于AI社会伦理研究的常见假设。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着AI系统在日常生活中深入应用，如何将AI发展与伦理和社会价值对齐成为焦点。然而，跨学科合作是否在实践中推动了这一转变尚不明确。

Method: 研究分析了ArXiv上10万篇AI相关论文，开发分类器识别社会伦理内容，并测量其整合程度。

Result: 发现跨学科团队更易产出社会导向研究，但纯计算机科学团队在此类研究中的占比显著增加，涉及公平、安全、医疗和错误信息等多个领域。

Conclusion: 研究结果挑战了AI社会伦理研究的驱动因素假设，并提出两个问题：纯技术团队主导此类研究对AI安全与治理的影响，以及人文社科学者在技术主导领域中的独特视角价值。

Abstract: As artificial intelligence (AI) systems become deeply embedded in everyday
life, calls to align AI development with ethical and societal values have
intensified. Interdisciplinary collaboration is often championed as a key
pathway for fostering such engagement. Yet it remains unclear whether
interdisciplinary research teams are actually leading this shift in practice.
This study analyzes over 100,000 AI-related papers published on ArXiv between
2014 and 2024 to examine how ethical values and societal concerns are
integrated into technical AI research. We develop a classifier to identify
societal content and measure the extent to which research papers express these
considerations. We find a striking shift: while interdisciplinary teams remain
more likely to produce societally-oriented research, computer science-only
teams now account for a growing share of the field's overall societal output.
These teams are increasingly integrating societal concerns into their papers
and tackling a wide range of domains - from fairness and safety to healthcare
and misinformation. These findings challenge common assumptions about the
drivers of societal AI and raise important questions. First, what are the
implications for emerging understandings of AI safety and governance if most
societally-oriented research is being undertaken by exclusively technical
teams? Second, for scholars in the social sciences and humanities: in a
technical field increasingly responsive to societal demands, what distinctive
perspectives can we still offer to help shape the future of AI?

</details>


### [56] [Towards Secure and Private Language Models for Nuclear Power Plants](https://arxiv.org/abs/2506.08746)
*Muhammad Anwar,Mishca de Costa,Issam Hammad,Daniel Lau*

Key words: 核能, 语言模型, Transformer, CANDU, 数据安全

TL;DR: 本文介绍了一种专用于核能领域的语言模型，基于公开的CANDU教材，采用紧凑的Transformer架构，单GPU训练以保护数据安全。模型能捕捉专业词汇，但语法连贯性有待提升，展示了满足高安全标准的可行性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 开发一种专用于核能领域的语言模型，以解决核应用中敏感数据的保密问题，同时验证小数据集下模型的潜力。

Method: 基于Transformer架构，利用公开的CANDU教材数据集，单GPU训练模型，专注于核领域内容。

Result: 模型能捕捉专业词汇，但有时语法不连贯，初步验证了小数据集下可行性，展现了在专业任务中的潜力。

Conclusion: 未来需扩展数据集、优化预处理和指令微调，以提高领域准确性并评估实际应用价值。

Abstract: This paper introduces a domain-specific Large Language Model for nuclear
applications, built from the publicly accessible Essential CANDU textbook.
Drawing on a compact Transformer-based architecture, the model is trained on a
single GPU to protect the sensitive data inherent in nuclear operations.
Despite relying on a relatively small dataset, it shows encouraging signs of
capturing specialized nuclear vocabulary, though the generated text sometimes
lacks syntactic coherence. By focusing exclusively on nuclear content, this
approach demonstrates the feasibility of in-house LLM solutions that align with
rigorous cybersecurity and data confidentiality standards. Early successes in
text generation underscore the model's utility for specialized tasks, while
also revealing the need for richer corpora, more sophisticated preprocessing,
and instruction fine-tuning to enhance domain accuracy. Future directions
include extending the dataset to cover diverse nuclear subtopics, refining
tokenization to reduce noise, and systematically evaluating the model's
readiness for real-world applications in nuclear domain.

</details>


### [57] [Unlocking the Potential of Large Language Models in the Nuclear Industry with Synthetic Data](https://arxiv.org/abs/2506.08750)
*Muhammad Anwar,Daniel Lau,Mishca de Costa,Issam Hammad*

Key words: 合成数据, 大型语言模型, 核能行业, 问答对, 数据稀缺

TL;DR: 论文探讨如何通过合成数据解决核能行业数据稀缺和隐私问题，将非结构化文本转化为可用于LLM训练的问答对。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 核能行业拥有大量未结构化的文本数据，但无法直接用于高级LLM应用。合成数据可以解决这一问题，推动LLM在该领域的应用。

Method: 利用LLM分析文本、提取关键信息、生成相关问答对，并通过评估保证合成数据集的质量。

Result: 合成数据为核能行业提供了可用数据集，支持LLM训练与优化。

Conclusion: 合成数据能促进核能行业的信息检索、知识共享和决策制定。

Abstract: The nuclear industry possesses a wealth of valuable information locked away
in unstructured text data. This data, however, is not readily usable for
advanced Large Language Model (LLM) applications that require clean, structured
question-answer pairs for tasks like model training, fine-tuning, and
evaluation. This paper explores how synthetic data generation can bridge this
gap, enabling the development of robust LLMs for the nuclear domain. We discuss
the challenges of data scarcity and privacy concerns inherent in the nuclear
industry and how synthetic data provides a solution by transforming existing
text data into usable Q&A pairs. This approach leverages LLMs to analyze text,
extract key information, generate relevant questions, and evaluate the quality
of the resulting synthetic dataset. By unlocking the potential of LLMs in the
nuclear industry, synthetic data can pave the way for improved information
retrieval, enhanced knowledge sharing, and more informed decision-making in
this critical sector.

</details>


### [58] [Factors affecting the in-context learning abilities of LLMs for dialogue state tracking](https://arxiv.org/abs/2506.08753)
*Pradyoth Hegde,Santosh Kesiraju,Jan Švec,Šimon Sedláček,Bolaji Yusuf,Oldřich Plchot,Deepak K T,Jan Černocký*

Key words: 对话状态跟踪（DST）；上下文学习（ICL）；k近邻方法；LLM

TL;DR: 该研究探讨了在对话状态跟踪（DST）问题中应用上下文学习（ICL）的方法，并分析了影响其效果的因素。通过选择适当的演示样本，结合模板输入到LLM中，进行了系统研究。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是探索ICL在DST中的应用潜力，并分析演示选择和提示上下文对性能的影响。

Method: 采用了基于句子嵌入的k近邻方法选择演示样本，并将其与测试样本结合模板输入到LLM中，实验中使用MultiWoZ2.4数据集和多个LLM模型。

Result: 研究发现了一些有益的见解，揭示了LLMs在DST任务中的上下文学习能力。

Conclusion: 研究为LLMs在对话状态跟踪中的上下文学习提供了实用指导。

Abstract: This study explores the application of in-context learning (ICL) to the
dialogue state tracking (DST) problem and investigates the factors that
influence its effectiveness. We use a sentence embedding based k-nearest
neighbour method to retrieve the suitable demonstrations for ICL. The selected
demonstrations, along with the test samples, are structured within a template
as input to the LLM. We then conduct a systematic study to analyse the impact
of factors related to demonstration selection and prompt context on DST
performance. This work is conducted using the MultiWoZ2.4 dataset and focuses
primarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and
Llama3.2-3B-Instruct models. Our findings provide several useful insights on
in-context learning abilities of LLMs for dialogue state tracking.

</details>


### [59] [Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL](https://arxiv.org/abs/2506.08757)
*Mishca de Costa,Muhammad Anwar,Dave Mercier,Mark Randall,Issam Hammad*

Key words: NL-to-SQL, 大型语言模型, 核电数据, 函数调用, 数据检索

TL;DR: 论文提出了一种基于函数调用的大型语言模型方法，替代传统的自然语言转SQL查询，以提高核电数据检索的准确性和安全性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 核电数据检索对准确性和透明度要求极高，传统NL-to-SQL方法存在风险，难以验证生成的查询且复杂数据库结构增加了错误概率。

Method: 采用预定义的函数库，封装已验证的SQL逻辑，通过LLMs调用这些函数，避免直接生成SQL查询。

Result: 实验表明，该方法在准确性和可维护性上优于直接NL-to-SQL生成。

Conclusion: 函数调用方法在保证用户易用性的同时，提高了操作安全性，为关键系统数据检索提供了新框架。

Abstract: Retrieving operational data from nuclear power plants requires exceptional
accuracy and transparency due to the criticality of the decisions it supports.
Traditionally, natural language to SQL (NL-to-SQL) approaches have been
explored for querying such data. While NL-to-SQL promises ease of use, it poses
significant risks: end-users cannot easily validate generated SQL queries, and
legacy nuclear plant databases -- often complex and poorly structured --
complicate query generation due to decades of incremental modifications. These
challenges increase the likelihood of inaccuracies and reduce trust in the
approach. In this work, we propose an alternative paradigm: leveraging
function-calling large language models (LLMs) to address these challenges.
Instead of directly generating SQL queries, we define a set of pre-approved,
purpose-specific functions representing common use cases. Queries are processed
by invoking these functions, which encapsulate validated SQL logic. This hybrid
approach mitigates the risks associated with direct NL-to-SQL translations by
ensuring that SQL queries are reviewed and optimized by experts before
deployment. While this strategy introduces the upfront cost of developing and
maintaining the function library, we demonstrate how NL-to-SQL tools can assist
in the initial generation of function code, allowing experts to focus on
validation rather than creation. Our study includes a performance comparison
between direct NL-to-SQL generation and the proposed function-based approach,
highlighting improvements in accuracy and maintainability. This work
underscores the importance of balancing user accessibility with operational
safety and provides a novel, actionable framework for robust data retrieval in
critical systems.

</details>


### [60] [AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP](https://arxiv.org/abs/2506.08768)
*Ahmed Hasanaath,Aisha Alansari,Ahmed Ashraf,Chafik Salmane,Hamzah Luqman,Saad Ezzini*

Key words: 大型语言模型,阿拉伯语,DeepSeek,零样本学习,微调,LoRA

TL;DR: 该论文对多个大型语言模型（LLMs）在阿拉伯语任务中的表现进行了全面评估，重点关注DeepSeek模型，通过零样本、少量样本和微调策略，揭示了关键性能提升。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 阿拉伯语因其丰富的形态、多样的方言和复杂的书写系统，LLMs在该语言上的表现尚未充分研究，因此作者希望通过系统实验填补这一空白。

Method: 作者在15个阿拉伯语NLP任务上测试了多种LLM，包括零样本、少量样本和微调（特别是LoRA方法），以评估模型在不同复杂度任务中的表现。

Result: 实验结果表明：1）精选3个上下文样本可显著提升分类任务性能（F1平均提升13分）；2）DeepSeek在零样本复杂推理任务中优于GPT o4-mini（F1平均高12分）；3）LoRA微调比单纯扩大模型规模更有效（F1和BLEU提升8分）。

Conclusion: 研究证实了针对阿拉伯语的特定优化策略（如少量样本选择和LoRA微调）能显著提升LLMs性能，为未来相关研究提供了参考。

Abstract: Large language models (LLMs) have shown remarkable progress in reasoning
abilities and general natural language processing (NLP) tasks, yet their
performance on Arabic data, characterized by rich morphology, diverse dialects,
and complex script, remains underexplored. This paper presents a comprehensive
benchmarking study of multiple reasoning-focused LLMs, with a special emphasis
on the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP
tasks. We experiment with various strategies, including zero-shot, few-shot,
and fine-tuning. This allows us to systematically evaluate performance on
datasets covering a range of applications to examine their capacity for
linguistic reasoning under different levels of complexity. Our experiments
reveal several key findings. First, carefully selecting just three in-context
examples delivers an average uplift of over 13 F1 points on classification
tasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection
from 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures
outperform a strong GPT o4-mini baseline by an average of 12 F1 points on
complex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning
yields up to an additional 8 points in F1 and BLEU compared to equivalent
increases in model scale. The code is available at
https://anonymous.4open.science/r/AraReasoner41299

</details>


### [61] [The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation](https://arxiv.org/abs/2506.08827)
*Francisco Vargas,Alejandro González Coene,Gaston Escalante,Exequiel Lobón,Manuel Pulido*

Key words: 信息提取, 法律文件, 交通事故, LLMs, 微调

TL;DR: 本文提出了一种从法律文件中提取交通事故信息的两步法，通过文本分割和实体提取优化信息抽取效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 交通事故信息提取对于量化保险公司成本至关重要，但由于法庭判决中的复杂论证，即使是专家也难以准确提取相关实体。

Method: 分两步：文本分割（正则表达式或n-token块向量化）和实体提取（使用LLaMA-2、LLaMA-3和GPT-4 Turbo等模型）。LLaMA模型通过LoRA微调以减少幻觉。

Result: 基于向量化和LLMs的方法显著优于经典方法（39.5%准确率），LLaMA-2 70B微调后达79.4%，LLaMA-3 8B（76.6%）接近LLaMA-2 70B，GPT-4 Turbo最高（86.1%）。

Conclusion: 微调和新兴模型显著提升了信息抽取效果，GPT-4 Turbo表现最优。

Abstract: The extraction of information about traffic accidents from legal documents is
crucial for quantifying insurance company costs. Extracting entities such as
percentages of physical and/or psychological disability and the involved
compensation amounts is a challenging process, even for experts, due to the
subtle arguments and reasoning in the court decision. A two-step procedure is
proposed: first, segmenting the document identifying the most relevant
segments, and then extracting the entities. For text segmentation, two
methodologies are compared: a classic method based on regular expressions and a
second approach that divides the document into blocks of n-tokens, which are
then vectorized using multilingual models for semantic searches
(text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models
(LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to
the selected segments for entity extraction. For the LLaMA models, fine-tuning
is performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a
significant number of hallucinations in extractions which are an important
contention point for named entity extraction. This work shows that these
hallucinations are substantially reduced after finetuning the model. The
performance of the methodology based on segment vectorization and subsequent
use of LLMs significantly surpasses the classic method which achieves an
accuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning
achieves the highest accuracy 79.4%, surpassing its base version 61.7%.
Notably, the base LLaMA-3 8B model already performs comparably to the finetuned
LLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model
development. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.

</details>


### [62] [Advancing STT for Low-Resource Real-World Speech](https://arxiv.org/abs/2506.08836)
*Flavio D'Intino,Hans-Peter Hutter*

Key words: 瑞士德语, 语音转文本, 低资源语言, Whisper模型, SRB-300

TL;DR: 本文介绍了新的SRB-300数据集，用于提升瑞士德语的语音转文本（STT）模型性能，解决了自发对话语音的转录难题。通过微调Whisper模型，显著降低了词错误率并提高了翻译质量。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 瑞士德语作为一种低资源语言，缺乏标准化书面形式且方言多样，现有STT模型在自发对话语音中表现不佳。因此，需要更真实的数据集来改进模型性能。

Method: 利用SRB-300数据集（包含300小时的真实广播和电视录音）微调多个OpenAI Whisper模型。

Result: 微调后的模型在词错误率（WER）上降低19%-33%，BLEU评分提高8%-40%。最佳模型（large-v3）的WER为17.1%，BLEU评分为74.8%。

Conclusion: SRB-300数据集和微调方法显著提升了瑞士德语STT系统的性能，为其他低资源语言的类似应用提供了参考。

Abstract: Swiss German is a low-resource language represented by diverse dialects that
differ significantly from Standard German and from each other, lacking a
standardized written form. As a result, transcribing Swiss German involves
translating into Standard German. Existing datasets have been collected in
controlled environments, yielding effective speech-to-text (STT) models, but
these models struggle with spontaneous conversational speech.
  This paper, therefore, introduces the new SRB-300 dataset, a 300-hour
annotated speech corpus featuring real-world long-audio recordings from 39
Swiss German radio and TV stations. It captures spontaneous speech across all
major Swiss dialects recorded in various realistic environments and overcomes
the limitation of prior sentence-level corpora.
  We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset,
achieving notable enhancements over previous zero-shot performance metrics.
Improvements in word error rate (WER) ranged from 19% to 33%, while BLEU scores
increased between 8% and 40%. The best fine-tuned model, large-v3, achieved a
WER of 17.1% and a BLEU score of 74.8. This advancement is crucial for
developing effective and robust STT systems for Swiss German and other
low-resource languages in real-world contexts.

</details>


### [63] [AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](https://arxiv.org/abs/2506.08885)
*Danush Khanna,Krishna Kumar,Basab Ghosh,Vinija Jain,Vasu Sharma,Aman Chadha,Amitava Das*

Key words: 对抗性攻击、隐式伪装、LLM安全性、几何约束、GRACE框架

TL;DR: 论文揭露了LLMs对抗性威胁的几何盲点，并提出了ALKALI基准和GRACE方法以增强防御效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前的防御措施无法有效应对LLMs的对抗性威胁，尤其是隐式伪装攻击。

Method: 提出GRACE框架，结合偏好学习和隐式空间正则化，通过几何约束增强安全性。

Result: GRACE显著降低了攻击成功率（ASR），并引入AVQI指标量化隐式对齐失败。

Conclusion: GRACE和AVQI为LLMs的安全性提供了新的解决方案和评估标准。

Abstract: Adversarial threats against LLMs are escalating faster than current defenses
can adapt. We expose a critical geometric blind spot in alignment: adversarial
prompts exploit latent camouflage, embedding perilously close to the safe
representation manifold while encoding unsafe intent thereby evading surface
level defenses like Direct Preference Optimization (DPO), which remain blind to
the latent geometry. We introduce ALKALI, the first rigorously curated
adversarial benchmark and the most comprehensive to date spanning 9,000 prompts
across three macro categories, six subtypes, and fifteen attack families.
Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates
(ASRs) across both open and closed source models, exposing an underlying
vulnerability we term latent camouflage, a structural blind spot where
adversarial completions mimic the latent geometry of safe ones. To mitigate
this vulnerability, we introduce GRACE - Geometric Representation Aware
Contrastive Enhancement, an alignment framework coupling preference learning
with latent space regularization. GRACE enforces two constraints: latent
separation between safe and adversarial completions, and adversarial cohesion
among unsafe and jailbreak behaviors. These operate over layerwise pooled
embeddings guided by a learned attention profile, reshaping internal geometry
without modifying the base model, and achieve up to 39% ASR reduction.
Moreover, we introduce AVQI, a geometry aware metric that quantifies latent
alignment failure via cluster separation and compactness. AVQI reveals when
unsafe completions mimic the geometry of safe ones, offering a principled lens
into how models internally encode safety. We make the code publicly available
at https://anonymous.4open.science/r/alkali-B416/README.md.

</details>


### [64] [PlantBert: An Open Source Language Model for Plant Science](https://arxiv.org/abs/2506.08897)
*Hiba Khey,Amine Lakhder,Salma Rouichi,Imane El Ghabi,Kamal Hejjaoui,Younes En-nahli,Fahd Kalloubi,Moez Amri*

Key words: PlantBert, 植物科学, 语言模型, 农业NLP, 胁迫响应

TL;DR: 本文介绍了PlantBert，一种专为植物胁迫响应文献知识提取设计的高性能开源语言模型，填补了农业自然语言处理领域的空白。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 植物科学领域缺乏针对性的语言模型工具，PlantBert旨在解决这一问题，为植物胁迫响应研究提供支持。

Method: 基于DeBERTa架构，结合专家注释的摘要语料进行微调，并采用规则增强的后处理和本体实体规范化。

Result: PlantBert在低资源领域表现出强泛化能力，能够精确捕获生物学关系。

Conclusion: PlantBert为农业NLP提供了可扩展的框架，推动计算植物科学的发展。

Abstract: The rapid advancement of transformer-based language models has catalyzed
breakthroughs in biomedical and clinical natural language processing; however,
plant science remains markedly underserved by such domain-adapted tools. In
this work, we present PlantBert, a high-performance, open-source language model
specifically tailored for extracting structured knowledge from plant
stress-response literature. Built upon the DeBERTa architecture-known for its
disentangled attention and robust contextual encoding-PlantBert is fine-tuned
on a meticulously curated corpus of expert-annotated abstracts, with a primary
focus on lentil (Lens culinaris) responses to diverse abiotic and biotic
stressors. Our methodology combines transformer-based modeling with
rule-enhanced linguistic post-processing and ontology-grounded entity
normalization, enabling PlantBert to capture biologically meaningful
relationships with precision and semantic fidelity. The underlying corpus is
annotated using a hierarchical schema aligned with the Crop Ontology,
encompassing molecular, physiological, biochemical, and agronomic dimensions of
plant adaptation. PlantBert exhibits strong generalization capabilities across
entity types and demonstrates the feasibility of robust domain adaptation in
low-resource scientific fields. By providing a scalable and reproducible
framework for high-resolution entity recognition, PlantBert bridges a critical
gap in agricultural NLP and paves the way for intelligent, data-driven systems
in plant genomics, phenomics, and agronomic knowledge discovery. Our model is
publicly released to promote transparency and accelerate cross-disciplinary
innovation in computational plant science.

</details>


### [65] [From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis](https://arxiv.org/abs/2506.08899)
*Elias Horner,Cristinel Mateis,Guido Governatori,Agata Ciabattoni*

Key words: 大语言模型、法律文本分析、可废止义务逻辑、自动化、法律信息学

TL;DR: 本文提出了一种利用大语言模型（LLMs）自动化分析法律文本语义的方法，并将其转化为可废止义务逻辑（DDL）的形式化表示。实验结果表明，LLMs在有效提示下能与专家成果高度一致。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 目标是开发一种自动化工具，通过LLMs将复杂的法律文本转化为形式化逻辑表示，以提高法律信息学的可扩展性。

Method: 采用结构化的处理流程：(1)将复杂法律文本分割为原子片段，(2)提取义务规则，(3)评估规则在句法和语义上的一致性。实验包括不同的LLM配置、提示工程策略和多阶段流程。

Result: 在澳大利亚《电信消费者保护法》的实验显示，机器生成的形式化表示与专家成果具有显著的吻合度，尤其在有效提示下表现更佳。

Conclusion: LLMs能够为法律信息学的规模化和自动化提供重要支持，尤其是在合适的提示策略下。

Abstract: We present a novel approach to the automated semantic analysis of legal texts
using large language models (LLMs), targeting their transformation into formal
representations in Defeasible Deontic Logic (DDL). We propose a structured
pipeline that segments complex normative language into atomic snippets,
extracts deontic rules, and evaluates them for syntactic and semantic
coherence. Our methodology is evaluated across various LLM configurations,
including prompt engineering strategies, fine-tuned models, and multi-stage
pipelines, focusing on legal norms from the Australian Telecommunications
Consumer Protections Code. Empirical results demonstrate promising alignment
between machine-generated and expert-crafted formalizations, showing that LLMs
- particularly when prompted effectively - can significantly contribute to
scalable legal informatics.

</details>


### [66] [Dialect Normalization using Large Language Models and Morphological Rules](https://arxiv.org/abs/2506.08907)
*Antonios Dimakis,John Pavlopoulos,Antonios Anastasopoulos*

Key words: 方言规范化,低资源语言,大型语言模型,希腊方言,自然语言处理

TL;DR: 论文提出了一种结合规则和大型语言模型的方法，用于方言到标准语的规范化，解决了低资源语言处理的挑战。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于自然语言理解系统难以处理低资源语言及其方言，研究旨在通过规范化方言文本，使其适应标准语言工具。

Method: 结合基于规则的语言学转换和大型语言模型，采用少量示例提示，无需平行数据。

Result: 方法应用于希腊方言数据集，人工评估显示规范化效果良好；下游实验发现此前研究仅依赖表层语言信息。

Conclusion: 研究表明结合规则与模型的方法有效，且揭示了一些新的语义信息。

Abstract: Natural language understanding systems struggle with low-resource languages,
including many dialects of high-resource ones. Dialect-to-standard
normalization attempts to tackle this issue by transforming dialectal text so
that it can be used by standard-language tools downstream. In this study, we
tackle this task by introducing a new normalization method that combines
rule-based linguistically informed transformations and large language models
(LLMs) with targeted few-shot prompting, without requiring any parallel data.
We implement our method for Greek dialects and apply it on a dataset of
regional proverbs, evaluating the outputs using human annotators. We then use
this dataset to conduct downstream experiments, finding that previous results
regarding these proverbs relied solely on superficial linguistic information,
including orthographic artifacts, while new observations can still be made
through the remaining semantics.

</details>


### [67] [PropMEND: Hypernetworks for Knowledge Propagation in LLMs](https://arxiv.org/abs/2506.08920)
*Zeyu Leo Liu,Greg Durrett,Eunsol Choi*

Key words: 知识编辑,超网络,元学习,多跳推理,PropMEND

TL;DR: PropMEND提出了一种基于超网络的知识传播方法，通过元学习调整梯度更新，使注入的知识能用于推理多跳问题。在RippleEdit数据集上表现优异，但在未见过的实体关系对上仍有提升空间。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有知识编辑技术无法有效传播注入的知识，尤其是在需要推理的多跳问题中。

Method: 使用超网络元学习语言建模损失的梯度修改，以促进知识传播。

Result: 在RippleEdit数据集上表现显著提升，但对未见过的实体关系对的泛化能力有限。

Conclusion: PropMEND在知识传播上优于现有方法，但需进一步研究以扩展其泛化能力。

Abstract: Knowledge editing techniques for large language models (LLMs) can inject
knowledge that is later reproducible verbatim, but they fall short on
propagating that knowledge: models cannot answer questions that require
reasoning with the injected knowledge. We present a hypernetwork-based approach
for knowledge propagation, named PropMEND, where we meta-learn how to modify
gradients of a language modeling loss to encourage injected information to
propagate. Our approach extends the meta-objective of MEND [29] so that
gradient updates on knowledge are transformed to enable answering multi-hop
questions involving that knowledge. We show improved performance on the
RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop
questions whose answers are not explicitly stated in the injected fact. We
further introduce a new dataset, Controlled RippleEdit, to evaluate the
generalization of our hypernetwork, testing knowledge propagation along
relations and entities unseen during hypernetwork training. PropMEND still
outperforms existing approaches in unseen entity-relation pairs, yet the
performance gap decreases substantially, suggesting future work in propagating
knowledge to a wide range of relations.

</details>


### [68] [Can A Gamer Train A Mathematical Reasoning Model?](https://arxiv.org/abs/2506.08935)
*Andrew Shin*

Key words: 大型语言模型, 数学推理, 强化学习, 内存优化, 资源受限环境

TL;DR: 该论文提出了一种在单张游戏GPU上训练高性能数学推理模型的方法，通过结合强化学习和内存优化技术，显著降低了训练成本。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管大型语言模型（LLMs）在数学推理等任务上表现出色，但其训练通常需要高昂的计算资源。本研究旨在证明无需高端硬件集群，也能高效训练出高性能模型。

Method: 采用强化学习和内存优化技术，在单张16GB显存的RTX 3080 Ti游戏GPU上训练了一个1.5B参数的数学推理模型。

Result: 该模型在数学推理基准测试中表现优于或媲美更大型的模型，且资源消耗显著降低。

Conclusion: 研究结果打破了高性能数学推理模型必须依赖大规模基础设施的范式，为AI研究的普及提供了新途径。

Abstract: While large language models (LLMs) have achieved remarkable performance in
various tasks including mathematical reasoning, their development typically
demands prohibitive computational resources. Recent advancements have reduced
costs for training capable models, yet even these approaches rely on high-end
hardware clusters. In this paper, we demonstrate that a single average gaming
GPU can train a solid mathematical reasoning model, by integrating
reinforcement learning and memory optimization techniques. Specifically, we
train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB
memory that achieves comparable or better performance on mathematical reasoning
benchmarks than models several times larger, in resource-constrained
environments. Our results challenge the paradigm that state-of-the-art
mathematical reasoning necessitates massive infrastructure, democratizing
access to high-performance AI research.
https://github.com/shinandrew/YouronMath.

</details>


### [69] [FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2506.08938)
*Qinggang Zhang,Zhishang Xiang,Yilin Xiao,Le Wang,Junhui Li,Xinrun Wang,Jinsong Su*

Key words: 大语言模型、检索增强、知识冲突、FaithfulRAG、自思考过程

TL;DR: FaithfulRAG是一种新框架，通过显式建模模型参数知识与检索上下文之间的差异，解决知识冲突问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 增强型LLM在知识密集型任务中表现出潜力，但存在忽略检索上下文或不一致融合的问题，尤其在知识冲突情况下更严重。

Method: FaithfulRAG通过事实级知识冲突识别和自思考过程，允许LLM在生成响应前整合冲突事实。

Result: 实验证明，该方法优于现有技术。

Conclusion: FaithfulRAG有效解决知识冲突问题，提升模型忠实性。

Abstract: Large language models (LLMs) augmented with retrieval systems have
demonstrated significant potential in handling knowledge-intensive tasks.
However, these models often struggle with unfaithfulness issues, generating
outputs that either ignore the retrieved context or inconsistently blend it
with the LLM`s parametric knowledge. This issue is particularly severe in cases
of knowledge conflict, where the retrieved context conflicts with the model`s
parametric knowledge. While existing faithful RAG approaches enforce strict
context adherence through well-designed prompts or modified decoding
strategies, our analysis reveals a critical limitation: they achieve
faithfulness by forcibly suppressing the model`s parametric knowledge, which
undermines the model`s internal knowledge structure and increases the risk of
misinterpreting the context. To this end, this paper proposes FaithfulRAG, a
novel framework that resolves knowledge conflicts by explicitly modeling
discrepancies between the model`s parametric knowledge and retrieved context.
Specifically, FaithfulRAG identifies conflicting knowledge at the fact level
and designs a self-thinking process, allowing LLMs to reason about and
integrate conflicting facts before generating responses. Extensive experiments
demonstrate that our method outperforms state-of-the-art methods. The code is
available at https:// github.com/DeepLearnXMU/Faithful-RAG

</details>


### [70] [Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions](https://arxiv.org/abs/2506.08952)
*Clara Lachenmaier,Judith Sieker,Sina Zarrieß*

Key words: 大语言模型, 共同基础, 政治领域, 错误信息, 沟通能力

TL;DR: 研究探讨了大语言模型（LLMs）在政治领域处理共同基础和纠正错误信念的能力，发现其在拒绝错误信念和主动沟通方面存在挑战。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究大语言模型在政治领域如何管理共同基础，尤其是在其知识不完全或面对错误信息时的表现。

Method: 通过直接知识问题和加载问题（预设错误信息）来评估LLMs的能力，分析其是否能够主动纠正错误信念。

Result: 研究发现LLMs在处理共同基础和拒绝错误信念方面存在显著困难。

Conclusion: LLMs在政治话语中缓解错误信息的能力令人担忧。

Abstract: Communication among humans relies on conversational grounding, allowing
interlocutors to reach mutual understanding even when they do not have perfect
knowledge and must resolve discrepancies in each other's beliefs. This paper
investigates how large language models (LLMs) manage common ground in cases
where they (don't) possess knowledge, focusing on facts in the political domain
where the risk of misinformation and grounding failure is high. We examine the
ability of LLMs to answer direct knowledge questions and loaded questions that
presuppose misinformation. We evaluate whether loaded questions lead LLMs to
engage in active grounding and correct false user beliefs, in connection to
their level of knowledge and their political bias. Our findings highlight
significant challenges in LLMs' ability to engage in grounding and reject false
user beliefs, raising concerns about their role in mitigating misinformation in
political discourse.

</details>


### [71] [Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers](https://arxiv.org/abs/2506.08966)
*Marek Kadlčík,Michal Štefánik,Timothee Mickus,Michal Spiegel,Josef Kuchař*

Key words: 预训练语言模型, 数字嵌入, 算术错误, 探测技术

TL;DR: 该论文提出一种新探测技术，能高效解码预训练语言模型中的数字嵌入，揭示其精确性，并证明通过调整嵌入可减少算术错误。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法难以准确探测预训练语言模型中的数字嵌入，导致算术错误被认为是由于嵌入不可靠。本文旨在解决这一问题。

Method: 提出一种新型探测技术，利用正弦模式解码输入嵌入中的数值，验证模型在预训练后能精确表示数字。

Result: 新方法在多种开源语言模型中实现了近乎完美的数值解码精度，且嵌入的精确性显著解释了算术错误。

Conclusion: 通过调整嵌入与探测发现的模式对齐，可以有效减少预训练语言模型在基础算术中的错误。

Abstract: Pretrained language models (LMs) are prone to arithmetic errors. Existing
work showed limited success in probing numeric values from models'
representations, indicating that these errors can be attributed to the inherent
unreliability of distributionally learned embeddings in representing exact
quantities. However, we observe that previous probing methods are inadequate
for the emergent structure of learned number embeddings with sinusoidal
patterns.
  In response, we propose a novel probing technique that decodes numeric values
from input embeddings with near-perfect accuracy across a range of open-source
LMs. This proves that after the sole pre-training, LMs represent numbers with
remarkable precision. Finally, we find that the embeddings' preciseness judged
by our probe's accuracy explains a large portion of LM's errors in elementary
arithmetic, and show that aligning the embeddings with the pattern discovered
by our probe can mitigate these errors.

</details>


### [72] [Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System](https://arxiv.org/abs/2506.08972)
*Yuan Guo,Tingjia Miao,Zheng Wu,Pengzhou Cheng,Ming Zhou,Zhuosheng Zhang*

Key words: 移动代理, 多模态语言模型, 组合任务, 基准测试, 调度系统

TL;DR: 论文提出了UI-NEXUS基准测试，用于评估移动设备上的多模态大型语言模型在执行组合任务时的表现，并提出了AGENT-NEXUS调度系统以提升任务成功率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决现有移动代理在组合任务（如上下文转换和深度操作）上的泛化能力不足问题。

Method: 开发UI-NEXUS基准测试，涵盖三类组合操作，并设计AGENT-NEXUS系统动态分解长时任务为原子子任务。

Result: 实验显示UI-NEXUS显著挑战现有代理，而AGENT-NEXUS提高任务成功率24%至40%。

Conclusion: AGENT-NEXUS有效提升移动代理在组合任务上的性能，而UI-NEXUS为研究提供了新方向。

Abstract: Autonomous agents powered by multimodal large language models have been
developed to facilitate task execution on mobile devices. However, prior work
has predominantly focused on atomic tasks -- such as shot-chain execution tasks
and single-screen grounding tasks -- while overlooking the generalization to
compositional tasks, which are indispensable for real-world applications. This
work introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile
agents on three categories of compositional operations: Simple Concatenation,
Context Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in
20 fully controllable local utility app environments, as well as 30 online
Chinese and English service apps. It comprises 100 interactive task templates
with an average optimal step count of 14.05. Experimental results across a
range of mobile agents with agentic workflow or agent-as-a-model show that
UI-NEXUS presents significant challenges. Specifically, existing agents
generally struggle to balance performance and efficiency, exhibiting
representative failure modes such as under-execution, over-execution, and
attention drift, causing visible atomic-to-compositional generalization gap.
Inspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient
scheduling system to tackle compositional mobile tasks. AGENT-NEXUS
extrapolates the abilities of existing mobile agents by dynamically decomposing
long-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS
achieves 24% to 40% task success rate improvement for existing mobile agents on
compositional operation tasks within the UI-NEXUS benchmark without
significantly sacrificing inference overhead. The demo video, dataset, and code
are available on the project page at https://ui-nexus.github.io.

</details>


### [73] [FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents](https://arxiv.org/abs/2506.08981)
*Satu Hopponen,Tomi Kinnunen,Alexandre Nikolaev,Rosa González Hautamäki,Lauri Tavi,Einar Meister*

Key words: FROST-EMA, 双语语音, 语音变异性, 自动说话人验证, 发音模式

TL;DR: 论文介绍了一个新的FROST-EMA语料库，用于研究双语者语音的语言变异性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究双语者语音的语言变异性，包括母语、第二语言和模仿口音的效果。

Method: 通过18名双语者的语音数据，结合技术（自动说话人验证系统）和语音学（发音模式）两个初步案例进行分析。

Result: 展示了第二语言和模仿口音对说话人验证系统的影响，以及发音模式的变化。

Conclusion: FROST-EMA语料库为语言变异性的研究提供了新的数据和视角。

Abstract: We introduce a new FROST-EMA (Finnish and Russian Oral Speech Dataset of
Electromagnetic Articulography) corpus. It consists of 18 bilingual speakers,
who produced speech in their native language (L1), second language (L2), and
imitated L2 (fake foreign accent). The new corpus enables research into
language variability from phonetic and technological points of view.
Accordingly, we include two preliminary case studies to demonstrate both
perspectives. The first case study explores the impact of L2 and imitated L2 on
the performance of an automatic speaker verification system, while the second
illustrates the articulatory patterns of one speaker in L1, L2, and a fake
accent.

</details>


### [74] [Naturalistic Language-related Movie-Watching fMRI Task for Detecting Neurocognitive Decline and Disorder](https://arxiv.org/abs/2506.08986)
*Yuejiao Wang,Xianmin Gong,Xixin Wu,Patrick Wong,Hoi-lam Helene Fung,Man Wai Mak,Helen Meng*

Key words: 神经认知障碍, fMRI, 机器学习, 早期检测, 语言处理

TL;DR: 提出了基于自然语言任务的fMRI方法，用于早期检测神经认知障碍（NCD），机器学习模型表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 早期检测NCD对老年人健康至关重要，语言相关fMRI可能是一种有效方法。

Method: 开发了新型自然语言fMRI任务，结合机器学习分类模型分析97名非痴呆中国老年人数据。

Result: 模型区分认知状态AUC达0.86，特征多源自语言处理相关脑区。

Conclusion: 自然语言fMRI任务有望用于衰老相关认知下降的早期检测。

Abstract: Early detection is crucial for timely intervention aimed at preventing and
slowing the progression of neurocognitive disorder (NCD), a common and
significant health problem among the aging population. Recent evidence has
suggested that language-related functional magnetic resonance imaging (fMRI)
may be a promising approach for detecting cognitive decline and early NCD. In
this paper, we proposed a novel, naturalistic language-related fMRI task for
this purpose. We examined the effectiveness of this task among 97 non-demented
Chinese older adults from Hong Kong. The results showed that machine-learning
classification models based on fMRI features extracted from the task and
demographics (age, gender, and education year) achieved an average area under
the curve of 0.86 when classifying participants' cognitive status (labeled as
NORMAL vs DECLINE based on their scores on a standard neurcognitive test).
Feature localization revealed that the fMRI features most frequently selected
by the data-driven approach came primarily from brain regions associated with
language processing, such as the superior temporal gyrus, middle temporal
gyrus, and right cerebellum. The study demonstrated the potential of the
naturalistic language-related fMRI task for early detection of aging-related
cognitive decline and NCD.

</details>


### [75] [Employing self-supervised learning models for cross-linguistic child speech maturity classification](https://arxiv.org/abs/2506.08999)
*Theo Zhang,Madurya Suresh,Anne S. Warlaumont,Kasia Hitczenko,Alejandrina Cristia,Margaret Cychosz*

Key words: 儿童语音, 语音分类, 数据集, transformer模型, 多语言

TL;DR: 论文提出了一种名为SpeechMaturity的新数据集，用于训练transformer模型识别儿童发声，相比现有数据集，其规模更大且覆盖多语言环境，模型表现优于现有方法并接近人类水平。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有语音技术系统在处理儿童语音时因训练数据少且儿童语音特性复杂而表现不佳，希望通过新数据集提升识别能力。

Method: 使用包含242,004条标注发声的SpeechMaturity数据集，训练transformer模型分类儿童哭声、笑声、成熟语音和不成熟语音。

Result: 模型在新数据集上表现优于现有方法，分类准确率接近人类水平，并在城乡环境中均表现稳健。

Conclusion: 大规模、多语言的数据集显著提升了儿童语音分类任务的性能。

Abstract: Speech technology systems struggle with many downstream tasks for child
speech due to small training corpora and the difficulties that child speech
pose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer
models to address a fundamental classification task: identifying child
vocalizations. Unlike previous corpora, our dataset captures maximally
ecologically-valid child vocalizations across an unprecedented sample,
comprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu,
Papua New Guinea, Solomon Islands, and France. The dataset contains 242,004
labeled vocalizations, magnitudes larger than previous work. Models were
trained to distinguish between cry, laughter, mature (consonant+vowel), and
immature speech (just consonant or vowel). Models trained on the dataset
outperform state-of-the-art models trained on previous datasets, achieved
classification accuracy comparable to humans, and were robust across rural and
urban settings.

</details>


### [76] [SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner](https://arxiv.org/abs/2506.09003)
*Lei Zhang,Jiaxi Yang,Min Yang,Jian Yang,Mouxiang Chen,Jiajun Zhang,Zeyu Cui,Binyuan Hui,Junyang Lin*

Key words: SWE-Flow, TDD, 数据合成, 运行时依赖图, 单元测试, GitHub

TL;DR: SWE-Flow是一个基于测试驱动开发（TDD）的数据合成框架，通过单元测试自动推断增量开发步骤，并构建运行时依赖图（RDG）生成结构化开发任务，显著提升TDD编码性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有软件工程数据依赖人工提交的问题，而SWE-Flow通过自动化从单元测试推断开发步骤，更高效地满足高层次需求。

Method: 构建运行时依赖图（RDG）以捕捉函数交互，生成分步开发计划，每一步包含部分代码库、单元测试和代码修改。

Result: 从真实GitHub项目中生成16,061个训练实例和2,020个测试实例，实验表明该数据集显著提升TDD编码性能。

Conclusion: SWE-Flow为TDD任务提供高效解决方案，并发布代码、数据集、模型和Docker镜像以供进一步研究。

Abstract: We introduce **SWE-Flow**, a novel data synthesis framework grounded in
Test-Driven Development (TDD). Unlike existing software engineering data that
rely on human-submitted issues, **SWE-Flow** automatically infers incremental
development steps directly from unit tests, which inherently encapsulate
high-level requirements. The core of **SWE-Flow** is the construction of a
Runtime Dependency Graph (RDG), which precisely captures function interactions,
enabling the generation of a structured, step-by-step *development schedule*.
At each step, **SWE-Flow** produces a partial codebase, the corresponding unit
tests, and the necessary code modifications, resulting in fully verifiable TDD
tasks. With this approach, we generated 16,061 training instances and 2,020
test instances from real-world GitHub projects, creating the **SWE-Flow-Eval**
benchmark. Our experiments show that fine-tuning open model on this dataset
significantly improves performance in TDD-based coding. To facilitate further
research, we release all code, datasets, models, and Docker images at
[Github](https://github.com/Hambaobao/SWE-Flow).

</details>


### [77] [UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags](https://arxiv.org/abs/2506.09009)
*Hakyung Sung,Gyu-Ho Shin,Chanyoung Lee,You Kyung Sung,Boo Kyung Jung*

Key words: Universal Dependencies, L2 Korean, morphosyntactic analysis, XPOS-UPOS alignment

TL;DR: 该研究提出了一种半自动化框架，用于识别XPOS序列中的形态句法结构并将其与UPOS类别对齐，扩展了第二语言韩语语料库，结果显示对齐提升了标注和依存分析的准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 扩展第二语言韩语语料库并改进形态句法标注的一致性。

Method: 引入半自动化框架识别XPOS序列中的形态句法结构，对齐UPOS类别，并通过两种NLP工具包对数据集进行fine-tune。

Result: 对齐的数据集提升了形态句法标注和依存分析的准确性，尤其在标注数据有限的情况下。

Conclusion: XPOS-UPOS对齐有助于提高标注一致性和模型性能。

Abstract: The present study extends recent work on Universal Dependencies annotations
for second-language (L2) Korean by introducing a semi-automated framework that
identifies morphosyntactic constructions from XPOS sequences and aligns those
constructions with corresponding UPOS categories. We also broaden the existing
L2-Korean corpus by annotating 2,998 new sentences from argumentative essays.
To evaluate the impact of XPOS-UPOS alignments, we fine-tune L2-Korean
morphosyntactic analysis models on datasets both with and without these
alignments, using two NLP toolkits. Our results indicate that the aligned
dataset not only improves consistency across annotation layers but also
enhances morphosyntactic tagging and dependency-parsing accuracy, particularly
in cases of limited annotated data.

</details>


### [78] [Learning to Reason Across Parallel Samples for LLM Reasoning](https://arxiv.org/abs/2506.09014)
*Jianing Qi,Xi Ye,Hao Tang,Zhigang Zhu,Eunsol Choi*

Key words: 大型语言模型、样本集聚合器、强化学习、测试时计算、数学推理

TL;DR: 通过训练一个紧凑型语言模型（SSA）来聚合多个样本的答案，显著提升了大型语言模型在数学领域的性能，且具有良好的泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为了提高大型语言模型在测试时的性能表现，特别是在数学领域，研究提出了一种新方法，通过聚合多个样本的答案来优化最终结果。

Method: 训练一个名为样本集聚合器（SSA）的紧凑型语言模型，该模型将多个样本的序列拼接后输入，并通过强化学习优化答案的准确性。

Result: 实验表明，SSA在多个推理数据集上优于其他测试时扩展方法（如基于奖励模型的重新排序），并展现出对样本集大小、基础模型和任务的良好泛化能力。

Conclusion: 通过将生成答案和分析聚合样本的任务分离，SSA能够高效地应用于黑盒模型的输出，实现性能提升。

Abstract: Scaling test-time compute brings substantial performance gains for large
language models (LLMs). By sampling multiple answers and heuristically
aggregate their answers (e.g., either through majority voting or using
verifiers to rank the answers), one can achieve consistent performance gains in
math domains. In this paper, we propose a new way to leverage such multiple
sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that
takes a concatenated sequence of multiple samples and output the final answer,
optimizing it for the answer accuracy with reinforcement learning. Experiments
on multiple reasoning datasets show that SSA outperforms other test-time
scaling methods such as reward model-based re-ranking. Our approach also shows
a promising generalization ability, across sample set sizes, base model
families and scales, and tasks. By separating LLMs to generate answers and LLMs
to analyze and aggregate sampled answers, our approach can work with the
outputs from premier black box models easily and efficiently.

</details>


### [79] [Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features](https://arxiv.org/abs/2506.09021)
*Hakyung Sung,Karla Csuros,Min-Chang Sung*

Key words: LLM, 校对, 词汇特征, 句法特征, 二语写作

TL;DR: 研究发现人类和LLM校对均能提升二元词汇特征，但LLM更倾向于生成性修改，且三种模型在词汇和句法特征上表现一致。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 比较人类和LLM在第二语言写作校对中的词汇和句法干预效果，评估不同LLM的一致性。

Method: 分析人类与三种LLM（ChatGPT-4o、Llama3.1-8b、Deepseek-r1-8b）对相同二语写作的校对干预。

Result: LLM校对更倾向于修改词汇和句子结构，使用多样且复杂的词汇，三种模型在主要特征上表现一致。

Conclusion: LLM校对在提升文本连贯性和多样性方面表现突出，且不同模型的一致性较高。

Abstract: This study examines the lexical and syntactic interventions of human and LLM
proofreading aimed at improving overall intelligibility in identical second
language writings, and evaluates the consistency of outcomes across three LLMs
(ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and
LLM proofreading enhance bigram lexical features, which may contribute to
better coherence and contextual connectedness between adjacent words. However,
LLM proofreading exhibits a more generative approach, extensively reworking
vocabulary and sentence structures, such as employing more diverse and
sophisticated vocabulary and incorporating a greater number of adjective
modifiers in noun phrases. The proofreading outcomes are highly consistent in
major lexical and syntactic features across the three models.

</details>


### [80] [Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning](https://arxiv.org/abs/2506.09033)
*Haozhen Zhang,Tao Feng,Jiaxuan You*

Key words: 大语言模型, 路由器, 强化学习, 多模型协作

TL;DR: 论文提出了一种基于强化学习的多LLM路由框架Router-R1，通过序列决策过程整合多个模型的互补优势，优化性能与成本的权衡。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有LLM路由器多为单轮一对一映射，无法充分利用多模型的互补能力处理复杂任务，因此需要一种更灵活的路由机制。

Method: Router-R1利用强化学习将多模型路由和聚合建模为序列决策问题，结合内部推理与动态模型调用，并通过轻量级规则奖励优化性能与成本。

Result: 在七个通用和多跳QA基准测试中，Router-R1表现优于多个基线，实现了更好的性能、泛化能力和成本管理。

Conclusion: Router-R1通过强化学习成功优化了多模型路由的任务性能与成本，为未来研究提供了新方向。

Abstract: The rapid emergence of diverse large language models (LLMs) has spurred the
development of LLM routers that assign user queries to the most suitable model.
However, existing LLM routers typically perform a single-round, one-to-one
mapping (\textit{i.e.}, assigning each query to a single model in isolation),
which limits their capability to tackle complex tasks that demand the
complementary strengths of multiple LLMs. In this paper, we present
\textbf{Router-R1}, a reinforcement learning (RL)-based framework that
formulates multi-LLM routing and aggregation as a sequential decision process.
Router-R1 instantiates the router itself as a capable LLM, leveraging its
reasoning ability to interleave "think" actions (internal deliberation) with
"route" actions (dynamic model invocation), and integrates each response into
its evolving context. To guide learning, we employ a lightweight rule-based
reward comprising format rewards, final outcome rewards, and a novel cost
reward for performance and cost trade-off optimization, opening a pathway
toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions
only on simple model descriptors such as pricing, latency, and example
performance, enabling strong generalization to unseen model selection.
Experiments on seven general and multi-hop QA benchmarks show that Router-R1
outperforms over several strong baselines, achieving superior performance while
maintaining robust generalization and cost management.Code is available at
https://github.com/ulab-uiuc/Router-R1.

</details>


### [81] [Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs](https://arxiv.org/abs/2506.09047)
*Yaniv Nikankin,Dana Arad,Yossi Gandelsman,Yonatan Belinkov*

Key words: 视觉语言模型, 模态差异, 计算子图, 表示补丁

TL;DR: 研究发现视觉语言模型在处理文本任务时表现优于视觉任务，这种性能差异源于模态间电路的计算子图不同。通过后期视觉表示补丁到早期层的方法，可以缩小模态间三分之一的性能差距。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 视觉语言模型在文本任务上的准确率高于视觉任务，作者希望通过分析模态间的计算子图（电路）来探究这种性能差异的原因。

Method: 比较不同模态间的计算子图，发现其功能相似但处理的数据位置不同；通过后期视觉表示补丁到早期层以优化视觉任务表现。

Result: 实验显示，该方法平均缩小了模态间三分之一的性能差距。

Conclusion: 研究表明模态间性能差异主要由数据位置处理引起，通过简单的表示补丁方法可以有效改善视觉任务表现。

Abstract: Vision-Language models (VLMs) show impressive abilities to answer questions
on visual inputs (e.g., counting objects in an image), yet demonstrate higher
accuracies when performing an analogous task on text (e.g., counting words in a
text). We investigate this accuracy gap by identifying and comparing the
\textit{circuits} - the task-specific computational sub-graphs - in different
modalities. We show that while circuits are largely disjoint between
modalities, they implement relatively similar functionalities: the differences
lie primarily in processing modality-specific data positions (an image or a
text sequence). Zooming in on the image data representations, we observe they
become aligned with the higher-performing analogous textual representations
only towards later layers, too late in processing to effectively influence
subsequent positions. To overcome this, we patch the representations of visual
data tokens from later layers back into earlier layers. In experiments with
multiple tasks and models, this simple intervention closes a third of the
performance gap between the modalities, on average. Our analysis sheds light on
the multi-modal performance gap in VLMs and suggests a training-free approach
for reducing it.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [82] [KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache](https://arxiv.org/abs/2506.08018)
*Fei Li,Song Liu,Weiguo Wu,Shiqiang Nie,Jinyu Wang*

Key words: KV Cache, 量化, 混合精度, 动态优化, 内存压缩

TL;DR: KVmix是一种新型的混合精度KV Cache量化方法，通过梯度重要性分析和动态长上下文优化策略，显著降低内存需求并提升推理效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决大规模语言模型推理中KV Cache的高内存需求问题，以实现在资源受限平台上的部署。

Method: 利用梯度重要性分析为不同层分配特定比特宽度，动态优化长上下文中的KV对精度，并提供高效低比特量化与CUDA内核优化。

Result: 在Llama和Mistral等模型上，实现了几乎无损的推理性能，内存压缩4.9倍，推理速度提升5.3倍。

Conclusion: KVmix通过智能分配量化精度，实现了内存与效率的高效权衡，适用于资源受限环境。

Abstract: The high memory demands of the Key-Value (KV) Cache during the inference of
Large Language Models (LLMs) severely restrict their deployment in
resource-constrained platforms. Quantization can effectively alleviate the
memory pressure caused by KV Cache. However, existing methods either rely on
static one-size-fits-all precision allocation or fail to dynamically prioritize
critical KV in long-context tasks, forcing memory-accuracy-throughput
tradeoffs. In this work, we propose a novel mixed-precision quantization method
for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to
evaluate how individual Key and Value projection matrices affect the model
loss, enabling layer-specific bit-width allocation for mix-precision
quantization. It dynamically prioritizes higher precision for important layers
while aggressively quantizing less influential ones, achieving a tunable
balance between accuracy and efficiency. KVmix also introduces a dynamic
long-context optimization strategy that adaptively keeps full-precision KV
pairs for recent pivotal tokens and compresses older ones, achieving
high-quality sequence generation with low memory usage. Additionally, KVmix
provides efficient low-bit quantization and CUDA kernels to optimize
computational overhead. On LLMs such as Llama and Mistral, KVmix achieves
near-lossless inference performance with extremely low quantization
configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x
memory compression and a 5.3x speedup in inference throughput.

</details>


### [83] [Gridding Forced Displacement using Semi-Supervised Learning](https://arxiv.org/abs/2506.08019)
*Andrew Wells,Geraldine Henningsen,Brice Bolane Tchinde Kengne*

Key words: 难民统计数据,半监督学习,空间分析,标签传播,高分辨率

TL;DR: 摘要提出了一种半监督方法，将难民的统计数据从行政边界细化到0.5度的网格单元，覆盖25个撒哈拉以南非洲国家。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了更清晰地识别难民的区域分布模式，现有的区域和国家统计数据往往掩盖了局部细节。

Method: 整合UNHCR的ProGres注册数据、Google Open Buildings的卫星建筑足迹和OpenStreetMap Populated Places的位置坐标，使用标签传播算法生成高精度的空间统计数据。

Result: 该方法以92.9%的平均准确率将1000多万难民观测数据分配到合适的网格单元，揭示了以往被掩盖的局部流离失所模式。

Conclusion: 高分辨率数据集为深入理解流离失所的驱动因素提供了基础。

Abstract: We present a semi-supervised approach that disaggregates refugee statistics
from administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan
African countries. By integrating UNHCR's ProGres registration data with
satellite-derived building footprints from Google Open Buildings and location
coordinates from OpenStreetMap Populated Places, our label spreading algorithm
creates spatially explicit refugee statistics at high granularity.This
methodology achieves 92.9% average accuracy in placing over 10 million refugee
observations into appropriate grid cells, enabling the identification of
localized displacement patterns previously obscured in broader regional and
national statistics. The resulting high-resolution dataset provides a
foundation for a deeper understanding of displacement drivers.

</details>


### [84] [Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation](https://arxiv.org/abs/2506.08020)
*Zi-Ying Chen,Chuan-Xian Ren,Hong Yan*

Key words: 部分域自适应, 最优传输, 异常类识别, 知识迁移

TL;DR: 论文提出了一种双层次不平衡最优传输模型（BUOT），用于解决部分域自适应问题，通过样本级和类级传输的统一框架优化异常类识别和知识迁移。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在部分域自适应（PDA）问题中，现有加权框架因仅依赖样本级关系，导致对簇结构的探索不足，且对异常类识别不稳定。因此，需要一种更鲁棒的传输模型。

Method: 提出BUOT模型，结合样本级和类级传输，通过协作机制优化传输计划，并设计标签感知传输成本以提高计算效率和局部结构保持。

Result: 在多个基准数据集上的实验验证了BUOT的竞争力。

Conclusion: BUOT模型通过双层次传输框架有效解决了PDA问题中的异常类识别和知识迁移挑战。

Abstract: Partial domain adaptation (PDA) problem requires aligning cross-domain
samples while distinguishing the outlier classes for accurate knowledge
transfer. The widely used weighting framework tries to address the outlier
classes by introducing the reweighed source domain with a similar label
distribution to the target domain. However, the empirical modeling of weights
can only characterize the sample-wise relations, which leads to insufficient
exploration of cluster structures, and the weights could be sensitive to the
inaccurate prediction and cause confusion on the outlier classes. To tackle
these issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model
to simultaneously characterize the sample-wise and class-wise relations in a
unified transport framework. Specifically, a cooperation mechanism between
sample-level and class-level transport is introduced, where the sample-level
transport provides essential structure information for the class-level
knowledge transfer, while the class-level transport supplies discriminative
information for the outlier identification. The bi-level transport plan
provides guidance for the alignment process. By incorporating the label-aware
transport cost, the local transport structure is ensured and a fast computation
formulation is derived to improve the efficiency. Extensive experiments on
benchmark datasets validate the competitiveness of BUOT.

</details>


### [85] [FlowBERT: Prompt-tuned BERT for variable flow field prediction](https://arxiv.org/abs/2506.08021)
*Weihao Zou,Weibing Feng,Pin Wu*

Key words: 知识迁移, 流场预测, 大语言模型, POD降维, 流体动力学

TL;DR: 本研究提出了一种基于大语言模型（LLM）知识迁移的通用流场预测框架，旨在解决传统计算流体力学（CFD）方法的高计算成本和现有深度学习模型的跨条件迁移能力不足的问题。该框架创新性地将POD降维技术与预训练LLM的微调策略相结合，通过设计流体动力学导向的文本模板提升预测性能。实验表明，该框架在小样本学习场景中优于传统Transformer模型，并在不同流入条件和翼型几何结构下表现出优异的泛化能力。与传统CFD方法相比，预测时间从数小时缩短至数秒，同时保持90%以上的准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统CFD方法计算成本高，现有深度学习模型跨条件迁移能力不足，需要一个高效且通用的流场预测框架。

Method: 结合POD降维技术与预训练LLM的微调策略，设计流体动力学导向的文本模板以丰富上下文语义信息。

Result: 在小样本学习场景中表现优于传统Transformer模型，预测时间从数小时缩短至数秒，准确性超过90%。

Conclusion: 该框架为快速流体动力学预测开辟了新方向，有望在气动优化、流动控制等领域得到广泛应用。

Abstract: This study proposes a universal flow field prediction framework based on
knowledge transfer
  from large language model (LLM), addressing the high computational costs of
traditional
  computational fluid dynamics (CFD) methods and the limited cross-condition
transfer capability
  of existing deep learning models. The framework innovatively integrates
Proper Orthogonal
  Decomposition (POD) dimensionality reduction with fine-tuning strategies for
pretrained LLM,
  where POD facilitates compressed representation of flow field features while
the fine-tuned model
  learns to encode system dynamics in state space. To enhance the model's
adaptability to flow field
  data, we specifically designed fluid dynamics-oriented text templates that
improve predictive
  performance through enriched contextual semantic information. Experimental
results demonstrate
  that our framework outperforms conventional Transformer models in few-shot
learning scenarios while
  exhibiting exceptional generalization across various inflow conditions and
airfoil geometries.
  Ablation studies reveal the contributions of key components in the FlowBERT
architecture. Compared
  to traditional Navier-Stokes equation solvers requiring hours of computation,
our approach reduces
  prediction time to seconds while maintaining over 90% accuracy. The developed
knowledge transfer
  paradigm establishes a new direction for rapid fluid dynamics prediction,
with potential
  applications extending to aerodynamic optimization, flow control, and other
engineering domains.

</details>


### [86] [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/abs/2506.08022)
*Chenxi Liu,Tianyi Xiong,Ruibo Chen,Yihan Wu,Junfeng Guo,Tianyi Zhou,Heng Huang*

Key words: 多模态模型, 模态不平衡, 偏好优化, 幻觉, 对抗性扰动

TL;DR: 论文提出了一种新颖的偏好学习框架MBPO，通过生成硬负样本和利用在线验证奖励，解决大型多模态模型（LMMs）中的模态不平衡问题，从而提升性能并减少幻觉。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的LMMs在推理过程中存在严重的模态不平衡问题，即语言先验偏差压倒视觉输入，导致下游任务泛化能力受限和幻觉现象。现有偏好优化方法未能有效抑制LLM的内部偏差，且依赖离线数据，缺乏动态适应能力。

Method: 提出的MBPO框架通过对抗性扰动生成硬负样本构建离线偏好数据集，并利用闭端任务的易验证性生成在线验证奖励。结合GRPO方法，使用离线-在线混合数据训练模型。

Result: 实验表明，MBPO能够显著提升LMMs在视觉-语言任务中的性能，并有效减少幻觉现象。

Conclusion: MBPO为解决LMMs的模态不平衡问题提供了有效方法，通过离线-在线混合数据优化模型性能。

Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been
significantly advanced by instruction tuning and further strengthened by recent
preference optimization. Yet, most LMMs still suffer from severe modality
imbalance during reasoning, i.e., outweighing language prior biases over visual
inputs, which bottlenecks their generalization to downstream tasks and causes
hallucinations. However, existing preference optimization approaches for LMMs
do not focus on restraining the internal biases of their Large Language Model
(LLM) backbones when curating the training data. Moreover, they heavily rely on
offline data and lack the capacity to explore diverse responses adaptive to
dynamic distributional shifts during training. Meanwhile, Group Relative Policy
Optimization (GRPO), a recent method using online-generated data and verified
rewards to improve reasoning capabilities, remains largely underexplored in LMM
alignment. In this paper, we propose a novel preference learning framework,
Modality-Balancing Preference Optimization (MBPO), to address the modality
imbalance in LMMs. MBPO constructs a more effective offline preference dataset
by generating hard negatives, i.e., rejected responses misled by LLM biases due
to limited usage of visual information, through adversarial perturbation of
input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended
tasks to generate online responses with verified rewards. GRPO is then employed
to train the model with offline-online hybrid data. Extensive experiments
demonstrate that MBPO can enhance LMM performance on challenging
vision-language tasks and effectively reduce hallucinations.

</details>


### [87] [Recipes for Pre-training LLMs with MXFP8](https://arxiv.org/abs/2506.08027)
*Asit Mishra,Dusan Stosic,Simon Layton*

Key words: Microscaling, MX格式, LLM预训练, 舍入模式, 数值稳定性

TL;DR: 论文分析了NVIDIA Blackwell GPU中的Microscaling（MX）格式在LLM预训练中的应用，发现默认的舍入模式可能导致模型发散，提出了一种改进的舍入模式（round-to-infinity），成功实现了8B模型在15T tokens上的MXFP8预训练。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究MX格式在预训练中的实际应用效果，解决其默认舍入模式导致模型发散的问题。

Method: 提出改进的舍入模式（round-to-infinity）用于计算缩放因子，并在8B模型和15T tokens数据集上进行MXFP8预训练验证。

Result: 改进的舍入模式成功避免了模型发散，实现了MXFP8格式下的稳定预训练。

Conclusion: 在LLM预训练中，选择合适的舍入模式对MX格式的稳定性至关重要，round-to-infinity是一个有效的改进方案。

Abstract: Precision scaling - using fewer bits to represent model parameters and
related tensors during pre-training - has emerged as a compelling technique for
improving GPU efficiency without sacrificing accuracy. Microscaling (MX)
formats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling
this precision scaling aspect. These formats combine narrow floating-point data
types with per-block scaling factors, offering a fine-grained approach to
quantizing tensors.
  Although MX-formats offer the promise of improved numeric stability compared
to other reduced-precision representations, in practice they must be used
carefully in order to successfully converge an LLM on a multi-trillion token
dataset. In this paper, we show that the rounding mode suggested in OCP
specification can lead to divergence when pre-training an LLM. We show an
improved rounding mode, which uses round-to-infinity to compute scaling
factors, enables successful pre-training in MXFP8 for an 8B model on 15T
tokens.

</details>


### [88] [ST-GraphNet: A Spatio-Temporal Graph Neural Network for Understanding and Predicting Automated Vehicle Crash Severity](https://arxiv.org/abs/2506.08051)
*Mahmuda Sultana Mimi,Md Monzurul Islam,Anannya Ghosh Tusti,Shriyank Somvanshi,Subasish Das*

Key words: 自动驾驶车辆、时空图神经网络、碰撞严重程度、多模态数据、H3空间索引

TL;DR: 论文提出了一种基于时空图神经网络的框架ST-GraphNet，用于建模和预测自动驾驶车辆（AV）碰撞严重程度。通过结合细粒度和区域聚合的空间图，模型在多模态数据上表现出色，特别是在粗粒度H3图上使用DSTGCN架构时，测试准确率达97.74%。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究旨在通过分析自动驾驶车辆碰撞的时空动态，提升城市交通安全和基础设施规划。

Method: 提出ST-GraphNet框架，结合细粒度和粗粒度的时空图表示，利用多模态数据（如文本嵌入、空间和时间属性）。测试了GCN、GAT和DSTGCN等GNN架构。

Result: ST-GraphNet在粗粒度H3图上的测试准确率为97.74%，显著优于细粒度模型的64.7%。

Conclusion: 空间聚合、动态消息传递和多模态特征集成能有效捕捉AV碰撞严重程度的复杂时空模式。

Abstract: Understanding the spatial and temporal dynamics of automated vehicle (AV)
crash severity is critical for advancing urban mobility safety and
infrastructure planning. In this work, we introduce ST-GraphNet, a
spatio-temporal graph neural network framework designed to model and predict AV
crash severity by using both fine-grained and region-aggregated spatial graphs.
Using a balanced dataset of 2,352 real-world AV-related crash reports from
Texas (2024), including geospatial coordinates, crash timestamps, SAE
automation levels, and narrative descriptions, we construct two complementary
graph representations: (1) a fine-grained graph with individual crash events as
nodes, where edges are defined via spatio-temporal proximity; and (2) a
coarse-grained graph where crashes are aggregated into Hexagonal Hierarchical
Spatial Indexing (H3)-based spatial cells, connected through hexagonal
adjacency. Each node in the graph is enriched with multimodal data, including
semantic, spatial, and temporal attributes, including textual embeddings from
crash narratives using a pretrained Sentence-BERT model. We evaluate various
graph neural network (GNN) architectures, such as Graph Convolutional Networks
(GCN), Graph Attention Networks (GAT), and Dynamic Spatio-Temporal GCN
(DSTGCN), to classify crash severity and predict high-risk regions. Our
proposed ST-GraphNet, which utilizes a DSTGCN backbone on the coarse-grained H3
graph, achieves a test accuracy of 97.74\%, substantially outperforming the
best fine-grained model (64.7\% test accuracy). These findings highlight the
effectiveness of spatial aggregation, dynamic message passing, and multi-modal
feature integration in capturing the complex spatio-temporal patterns
underlying AV crash severity.

</details>


### [89] [STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation](https://arxiv.org/abs/2506.08054)
*Yiming Wang,Hao Peng,Senzhang Wang,Haohua Du,Chunyang Liu,Jia Wu,Guanlin Wu*

Key words: 交通数据填补,动态图注意力,混合专家框架,时空特征,非平稳数据

TL;DR: 该论文提出了一种名为STAMImputer的交通数据填补方法，通过混合专家框架和动态图注意力机制，有效解决了块状缺失数据和非平稳交通数据的特征提取问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法在块状缺失数据场景中特征提取不足，且静态图结构限制了模型对非平稳交通数据的灵活性，因此提出STAMImputer以解决这些问题。

Method: 采用混合专家框架（MoE）捕捉时空特征及其影响权重，并结合低秩引导的采样图注意力机制（LrSGAT）动态平衡路网的局部和全局相关性。

Result: 在四个交通数据集上的实验表明，STAMImputer相比现有SOTA方法有显著性能提升。

Conclusion: STAMImputer通过动态图结构和MoE框架有效解决了交通数据填补中的关键问题，具有实用性和先进性。

Abstract: Traffic data imputation is fundamentally important to support various
applications in intelligent transportation systems such as traffic flow
prediction. However, existing time-to-space sequential methods often fail to
effectively extract features in block-wise missing data scenarios. Meanwhile,
the static graph structure for spatial feature propagation significantly
constrains the models flexibility in handling the distribution shift issue for
the nonstationary traffic data. To address these issues, this paper proposes a
SpatioTemporal Attention Mixture of experts network named STAMImputer for
traffic data imputation. Specifically, we introduce a Mixture of Experts (MoE)
framework to capture latent spatio-temporal features and their influence
weights, effectively imputing block missing. A novel Low-rank guided Sampling
Graph ATtention (LrSGAT) mechanism is designed to dynamically balance the local
and global correlations across road networks. The sampled attention vectors are
utilized to generate dynamic graphs that capture real-time spatial
correlations. Extensive experiments are conducted on four traffic datasets for
evaluation. The result shows STAMImputer achieves significantly performance
improvement compared with existing SOTA approaches. Our codes are available at
https://github.com/RingBDStack/STAMImupter.

</details>


### [90] [Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques](https://arxiv.org/abs/2506.08060)
*Asankhaya Sharma*

Key words: 大型语言模型；监督微调；上下文学习；资源效率

TL;DR: 本文证明，通过推理技术（如上下文学习）可以在不修改模型参数的情况下近似监督微调的效果，并推广到有限资源和部分数据访问的实际场景。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索如何在资源有限的情况下高效部署大型语言模型，减少监督微调的计算成本。

Method: 基于理想假设下的理论分析，推广到有限上下文长度和部分数据集访问的实践场景。

Result: 提供数据集大小的理论界限，表明在有限条件下仍能近似微调效果。

Conclusion: 为大型语言模型的高效部署提供了理论基础，并通过诸如检索增强生成等技术实现理论与应用的结合。

Abstract: Large language models have transformed natural language processing, yet
supervised fine-tuning (SFT) remains computationally intensive. This paper
formally proves that capabilities acquired through SFT can be approximated by a
base transformer model using inference-time techniques, specifically in-context
learning (ICL), without altering model parameters, under idealized assumptions
including unbounded computational resources and access to the fine-tuning
dataset. We extend these results to practical scenarios with finite context
lengths and partial dataset access. For text generation tasks with fixed output
length $l$, datasets of size $\mathrm{O}\left( \frac{m V}{\varepsilon^2} \log
\frac{m}{\delta} \right)$ or, with bounded context, $\mathrm{O}\left( \frac{l
\log V}{\varepsilon^2} \log \frac{1}{\delta} \right)$ suffice to approximate
fine-tuned behavior across $m$ contexts within error $\varepsilon$, where $V$
is the vocabulary size and $\delta$ is the failure probability. For linear
classification, datasets of size $\mathrm{O}\left( \frac{d}{\varepsilon}
\right)$ or, with fixed context, $\mathrm{O}\left( \frac{1}{\varepsilon^2} \log
\frac{1}{\delta} \right)$ are sufficient, where $d$ is the input dimension.
Grounded in the Turing completeness of transformers, these results provide a
theoretical foundation for resource-efficient deployment of large language
models, with practical techniques like retrieval-augmented generation bridging
theory to real-world applications.

</details>


### [91] [FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2506.08062)
*Woosung Kim,Jinho Lee,Jongmin Lee,Byung-Jun Lee*

Key words: 多目标强化学习, 离线学习, 非线性福利目标, 公平性, 分布校正

TL;DR: 论文提出了FairDICE，首个离线多目标强化学习框架，专注于优化非线性社会福利目标。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统线性标量化在多目标强化学习中无法捕捉公平导向的目标，如纳什社会福利或最大最小公平。

Method: FairDICE利用分布校正估计，联合考虑福利最大化和分布正则化，实现稳定且样本高效的学习。

Result: 在多个离线基准测试中，FairDICE展现了优于现有基线的公平感知性能。

Conclusion: FairDICE为离线多目标强化学习中的非线性福利目标优化提供了统一且高效的解决方案。

Abstract: Multi-objective reinforcement learning (MORL) aims to optimize policies in
the presence of conflicting objectives, where linear scalarization is commonly
used to reduce vector-valued returns into scalar signals. While effective for
certain preferences, this approach cannot capture fairness-oriented goals such
as Nash social welfare or max-min fairness, which require nonlinear and
non-additive trade-offs. Although several online algorithms have been proposed
for specific fairness objectives, a unified approach for optimizing nonlinear
welfare criteria in the offline setting-where learning must proceed from a
fixed dataset-remains unexplored. In this work, we present FairDICE, the first
offline MORL framework that directly optimizes nonlinear welfare objective.
FairDICE leverages distribution correction estimation to jointly account for
welfare maximization and distributional regularization, enabling stable and
sample-efficient learning without requiring explicit preference weights or
exhaustive weight search. Across multiple offline benchmarks, FairDICE
demonstrates strong fairness-aware performance compared to existing baselines.

</details>


### [92] [Lite-RVFL: A Lightweight Random Vector Functional-Link Neural Network for Learning Under Concept Drift](https://arxiv.org/abs/2506.08063)
*Songqiao Hu,Zeyi Liu,Xiao He*

Key words: 概念漂移, Lite-RVFL, 在线学习, 增量学习

TL;DR: 提出了Lite-RVFL，一种轻量级快速高效的随机向量功能链接网络，能够在无需检测和重新训练的情况下适应概念漂移。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 数据分布随时间变化（概念漂移）对在线学习方法可靠性构成挑战，现有方法计算成本高且不适合实时应用。

Method: 引入新的目标函数，为新样本分配指数级增加的权重，强调近期数据以快速适应，并推导高效增量更新规则。

Result: 在真实世界安全评估任务中验证了Lite-RVFL的高效性、适应漂移能力和捕捉时间模式的潜力。

Conclusion: Lite-RVFL是一种无需检测和重新训练即可适应概念漂移的高效解决方案。

Abstract: The change in data distribution over time, also known as concept drift, poses
a significant challenge to the reliability of online learning methods. Existing
methods typically require model retraining or drift detection, both of which
demand high computational costs and are often unsuitable for real-time
applications. To address these limitations, a lightweight, fast and efficient
random vector functional-link network termed Lite-RVFL is proposed, capable of
adapting to concept drift without drift detection and retraining. Lite-RVFL
introduces a novel objective function that assigns weights exponentially
increasing to new samples, thereby emphasizing recent data and enabling timely
adaptation. Theoretical analysis confirms the feasibility of this objective
function for drift adaptation, and an efficient incremental update rule is
derived. Experimental results on a real-world safety assessment task validate
the efficiency, effectiveness in adapting to drift, and potential to capture
temporal patterns of Lite-RVFL. The source code is available at
https://github.com/songqiaohu/Lite-RVFL.

</details>


### [93] [Info-Coevolution: An Efficient Framework for Data Model Coevolution](https://arxiv.org/abs/2506.08070)
*Ziheng Qin,Hailun Xu,Wei Chee Yew,Qi Jia,Yang Luo,Kanchan Sarkar,Danhui Guan,Kai Wang,Yang You*

Key words: 机器学习,数据标注,在线学习,数据集优化

TL;DR: 提出了一种名为Info-Coevolution的新框架，通过选择性标注和在线数据整合，高效优化数据集构建和模型训练，显著减少标注和训练成本。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决真实世界数据增长带来的数据冗余和训练效率问题。

Method: 通过选择性标注任务特定模型和开源模型的数据，实现模型与数据的协同进化。

Result: 在ImageNet-1K等数据集上减少了32%的标注和训练成本，且无性能损失。

Conclusion: Info-Coevolution是一种高效且无偏的数据集优化框架。

Abstract: Machine learning relies heavily on data, yet the continuous growth of
real-world data poses challenges for efficient dataset construction and
training. A fundamental yet unsolved question is: given our current model and
data, does a new data (sample/batch) need annotation/learning? Conventional
approaches retain all available data, leading to non-optimal data and training
efficiency. Active learning aims to reduce data redundancy by selecting a
subset of samples to annotate, while it increases pipeline complexity and
introduces bias. In this work, we propose Info-Coevolution, a novel framework
that efficiently enables models and data to coevolve through online selective
annotation with no bias. Leveraging task-specific models (and open-source
models), it selectively annotates and integrates online and web data to improve
datasets efficiently. For real-world datasets like ImageNet-1K,
Info-Coevolution reduces annotation and training costs by 32\% without
performance loss. It is able to automatically give the saving ratio without
tuning the ratio. It can further reduce the annotation ratio to 50\% with
semi-supervised learning. We also explore retrieval-based dataset enhancement
using unlabeled open-source data. Code is available at
https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.

</details>


### [94] [Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting](https://arxiv.org/abs/2506.08113)
*Timothée Hornek Amir Sartipi,Igor Tchappi,Gilbert Fridgen*

Key words: 电价预测（EPF）、时间序列基础模型（TSFMs）、生成式人工智能（GenAI）、统计模型、机器学习

TL;DR: 该论文通过比较多种时间序列基础模型（TSFMs）与传统统计和机器学习方法在电价预测（EPF）中的表现，发现Chronos-Bolt和Time-MoE表现最佳，但传统模型MSTL在多个国家和评估指标中表现最为稳定。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 电价预测对电力现货市场的交易决策至关重要，但目前尚不清楚基于生成式人工智能（GenAI）和大语言模型（LLMs）的时间序列基础模型（TSFMs）在这一领域的有效性。

Method: 论文研究了多种预训练模型（如Chronos-Bolt、Chronos-T5、TimesFM等）与传统统计和机器学习方法在多个国家的电价数据上的表现。

Result: Chronos-Bolt和Time-MoE表现优异，但传统双季节性MSTL模型在一致性和稳定性上优于TSFMs。

Conclusion: 尽管某些TSFMs表现良好，但传统方法（如MSTL）在电价预测中仍然具有优势。

Abstract: Accurate electricity price forecasting (EPF) is crucial for effective
decision-making in power trading on the spot market. While recent advances in
generative artificial intelligence (GenAI) and pre-trained large language
models (LLMs) have inspired the development of numerous time series foundation
models (TSFMs) for time series forecasting, their effectiveness in EPF remains
uncertain. To address this gap, we benchmark several state-of-the-art
pretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and
TimeGPT--against established statistical and machine learning (ML) methods for
EPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany,
France, the Netherlands, Austria, and Belgium, we generate daily forecasts with
a one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the
TSFMs, performing on par with traditional models. However, the biseasonal MSTL
model, which captures daily and weekly seasonality, stands out for its
consistent performance across countries and evaluation metrics, with no TSFM
statistically outperforming it.

</details>


### [95] [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://arxiv.org/abs/2506.08125)
*Hanbing Liu,Lang Cao,Yuanyi Ren,Mengyu Zhou,Haoyu Dong,Xiaojun Ma,Shi Han,Dongmei Zhang*

Key words: 大型语言模型,强化学习,推理效率,长度奖励,Bingo

TL;DR: Bingo是一个RL框架，通过设计长度奖励机制提升大型语言模型的推理效率，同时保持准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法在提升推理能力时多关注准确性而非效率，直接长度奖励会导致准确性下降。

Method: Bingo包含显著性感知长度奖励和动态长度奖励机制，逐步减少无关词并优化整体效率。

Result: 实验显示Bingo在多个基准测试中优于现有方法，提升了准确性和效率。

Conclusion: Bingo展示了为高效推理显式训练LLMs的潜力。

Abstract: Large language models have demonstrated impressive reasoning capabilities,
yet they often suffer from inefficiencies due to unnecessarily verbose or
redundant outputs. While many works have explored reinforcement learning (RL)
to enhance reasoning abilities, most primarily focus on improving accuracy,
with limited attention to reasoning efficiency. Some existing approaches
introduce direct length-based rewards to encourage brevity, but this often
leads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL
framework that advances length-based reward design to boost efficient
reasoning. Bingo incorporates two key mechanisms: a significance-aware length
reward, which gradually guides the model to reduce only insignificant tokens,
and a dynamic length reward, which initially encourages elaborate reasoning for
hard questions but decays over time to improve overall efficiency. Experiments
across multiple reasoning benchmarks show that Bingo improves both accuracy and
efficiency. It outperforms the vanilla reward and several other length-based
reward baselines in RL, achieving a favorable trade-off between accuracy and
efficiency. These results underscore the potential of training LLMs explicitly
for efficient reasoning.

</details>


### [96] [Nearness of Neighbors Attention for Regression in Supervised Finetuning](https://arxiv.org/abs/2506.08139)
*Aviad Susman,Mayte Suárez-Fariñas,Joseph T Colonel*

Key words: 监督学习, k-NN, 注意力机制, 可微分, 特征提取

TL;DR: 研究提出了一种名为NONA的可微分k-NN回归层，通过结合神经网络注意力和掩码机制，提升了传统算法在监督微调中的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统算法（如k-NN）在监督学习中的结合可以提升性能，但因其不可微特性难以直接融入神经网络。

Method: 提出NONA层，利用注意力机制和掩码方案模拟k-NN回归，实现可微分性。

Result: 在多个非结构化数据集上，NONA优于密集层预测和基于SFT嵌入的k-NN回归。

Conclusion: NONA为传统算法在神经网络的集成提供了新思路，性能表现显著。

Abstract: It is common in supervised machine learning to combine the feature extraction
capabilities of neural networks with the predictive power of traditional
algorithms, such as k-nearest neighbors (k-NN) or support vector machines. This
procedure involves performing supervised fine-tuning (SFT) on a
domain-appropriate feature extractor, followed by training a traditional
predictor on the resulting SFT embeddings. When used in this manner,
traditional predictors often deliver increased performance over the SFT model
itself, despite the fine-tuned feature extractor yielding embeddings
specifically optimized for prediction by the neural network's final dense
layer. This suggests that directly incorporating traditional algorithms into
SFT as prediction layers may further improve performance. However, many
traditional algorithms have not been implemented as neural network layers due
to their non-differentiable nature and their unique optimization requirements.
As a step towards solving this problem, we introduce the Nearness of Neighbors
Attention (NONA) regression layer. NONA uses the mechanics of neural network
attention and a novel learned attention-masking scheme to yield a
differentiable proxy of the k-NN regression algorithm. Results on multiple
unstructured datasets show improved performance over both dense layer
prediction and k-NN on SFT embeddings for regression.

</details>


### [97] [AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists](https://arxiv.org/abs/2506.08140)
*Yifei Li,Hanane Nour Moussa,Ziru Chen,Shijie Chen,Botao Yu,Mingyi Xue,Benjamin Burns,Tzu-Yao Chiu,Vishal Dey,Zitong Lu,Chen Wei,Qianheng Zhang,Tianyu Zhang,Song Gao,Xuhui Huang,Xia Ning,Nesreen K. Ahmed,Ali Payani,Huan Sun*

Key words: AutoSDT, 科学发现, 数据驱动, LLMs, AutoSDT-5K

TL;DR: AutoSDT是一种自动构建高质量科学发现数据集的流水线，解决了AI科学辅助中数据稀缺的问题，并提出了AutoSDT-5K数据集和AutoSDT-Coder模型，显著提升了模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决AI科学辅助中高质量数据稀缺的问题，通过自动化采集和评估真实发现工作流中的数据任务。

Method: AutoSDT流水线利用LLMs的编码能力自动搜索并合成高质量的编码任务，构建AutoSDT-5K数据集，并训练AutoSDT-Coder模型。

Result: AutoSDT-5K数据集包含5404个任务，93%的任务生态有效，92.2%的代码功能正确。AutoSDT-Coder模型在两项基准测试中性能显著提升。

Conclusion: AutoSDT为科学发现提供了高质量数据集，并显著提升了开源模型的性能，缩小了与GPT-4o的差距。

Abstract: Despite long-standing efforts in accelerating scientific discovery with AI,
building AI co-scientists remains challenging due to limited high-quality data
for training and evaluation. To tackle this data scarcity issue, we present
AutoSDT, an automatic pipeline that collects high-quality coding tasks in
real-world data-driven discovery workflows. AutoSDT leverages the coding
capabilities and parametric knowledge of LLMs to search for diverse sources,
select ecologically valid tasks, and synthesize accurate task instructions and
code solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404
coding tasks for data-driven discovery that covers four scientific disciplines
and 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the
only automatically collected and the largest open dataset for data-driven
scientific discovery. Expert feedback on a subset of 256 tasks shows the
effectiveness of AutoSDT: 93% of the collected tasks are ecologically valid,
and 92.2% of the synthesized programs are functionally correct. Trained on
AutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show
substantial improvement on two challenging data-driven discovery benchmarks,
ScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches
the same level of performance as GPT-4o on ScienceAgentBench with a success
rate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it
lifts the hypothesis matching score to 8.1, bringing a 17.4% relative
improvement and closing the gap between open-weight models and GPT-4o.

</details>


### [98] [Accelerating Spectral Clustering under Fairness Constraints](https://arxiv.org/abs/2506.08143)
*Francesco Tonin,Alex Lambert,Johan A. K. Suykens,Volkan Cevher*

Key words: 公平聚类, 谱聚类, 凸差函数, 交替方向乘子法, 计算效率

TL;DR: 提出了一种高效的公平谱聚类方法（Fair SC），通过将问题转化为凸差函数（DC）框架，并引入新变量增强策略和交替方向乘子法，显著提高了计算效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对决策算法中的公平性问题，特别是在谱聚类中确保每个群体在聚类中的比例与总体比例一致。

Method: 将公平谱聚类问题转化为DC框架，采用新变量增强策略和交替方向乘子法，避免昂贵的特征分解。

Result: 在合成和实际数据上的实验表明，该方法计算效率显著提升，尤其在大规模问题上。

Conclusion: 该方法为实际应用中的公平聚类提供了重要进展。

Abstract: Fairness of decision-making algorithms is an increasingly important issue. In
this paper, we focus on spectral clustering with group fairness constraints,
where every demographic group is represented in each cluster proportionally as
in the general population. We present a new efficient method for fair spectral
clustering (Fair SC) by casting the Fair SC problem within the difference of
convex functions (DC) framework. To this end, we introduce a novel variable
augmentation strategy and employ an alternating direction method of multipliers
type of algorithm adapted to DC problems. We show that each associated
subproblem can be solved efficiently, resulting in higher computational
efficiency compared to prior work, which required a computationally expensive
eigendecomposition. Numerical experiments demonstrate the effectiveness of our
approach on both synthetic and real-world benchmarks, showing significant
speedups in computation time over prior art, especially as the problem size
grows. This work thus represents a considerable step forward towards the
adoption of fair clustering in real-world applications.

</details>


### [99] [Fully data-driven inverse hyperelasticity with hyper-network neural ODE fields](https://arxiv.org/abs/2506.08146)
*Vahidullah Taç,Amirhossein Amiri-Hezaveh,Manuel K. Rausch,Grace N. Bechtel,Francisco Sahli Costabal,Adrian Buganza Tepole*

Key words: 异质材料、本构方程、神经网络、NODE、超网络、力学性能

TL;DR: 提出了一种新框架，通过无闭合本构方程识别异质材料的力学性能。利用神经网络和物理驱动的数据驱动方法，结合NODE框架和超网络，实现了材料本构方程的发现，并通过数值实验验证了其鲁棒性和通用性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的本构方程方法在处理异质材料时存在局限性，新框架旨在解决这一问题，实现更灵活和准确的力学性能识别。

Method: 结合神经网络的连续应变场近似和NODE框架的本构方程发现，通过超网络处理异质性，并优化多目标损失函数以满足弹性力学平衡方程和边界条件。

Result: 数值实验表明，该方法在识别异质材料力学性能时具有鲁棒性和通用性，且对噪声和实验数据适应性强。

Conclusion: 该框架为异质材料力学性能识别提供了高效、灵活的替代方案，优于传统逆方法。

Abstract: We propose a new framework for identifying mechanical properties of
heterogeneous materials without a closed-form constitutive equation. Given a
full-field measurement of the displacement field, for instance as obtained from
digital image correlation (DIC), a continuous approximation of the strain field
is obtained by training a neural network that incorporates Fourier features to
effectively capture sharp gradients in the data. A physics-based data-driven
method built upon ordinary neural differential equations (NODEs) is employed to
discover constitutive equations. The NODE framework can represent arbitrary
materials while satisfying constraints in the theory of constitutive equations
by default. To account for heterogeneity, a hyper-network is defined, where the
input is the material coordinate system, and the output is the NODE-based
constitutive equation. The parameters of the hyper-network are optimized by
minimizing a multi-objective loss function that includes penalty terms for
violations of the strong form of the equilibrium equations of elasticity and
the associated Neumann boundary conditions. We showcase the framework with
several numerical examples, including heterogeneity arising from variations in
material parameters, spatial transitions from isotropy to anisotropy, material
identification in the presence of noise, and, ultimately, application to
experimental data. As the numerical results suggest, the proposed approach is
robust and general in identifying the mechanical properties of heterogeneous
materials with very few assumptions, making it a suitable alternative to
classical inverse methods.

</details>


### [100] [BLUR: A Bi-Level Optimization Approach for LLM Unlearning](https://arxiv.org/abs/2506.08164)
*Hadi Reisizadeh,Jinghan Jia,Zhiqi Bu,Bhanukiran Vinzamuri,Anil Ramakrishna,Kai-Wei Chang,Volkan Cevher,Sijia Liu,Mingyi Hong*

Key words: 大语言模型、未学习、双层优化、BLUR、伦理AI

TL;DR: 本文提出了一种新的层次化大语言模型（LLM）遗忘问题建模方法，通过双层优化（Bi-Level UnleaRning, BLUR）算法，显著优于现有未学习算法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 确保LLM遵守数据规定和伦理实践，需要通过未学习技术移除特定知识或能力，但目前未学习问题的建模方法存在性能下降问题。

Method: 提出层级化建模方法，将遗忘问题（优先）与保留问题分离，并采用双层优化算法（BLUR）进行求解。

Result: 实验表明，BLUR在不同任务、模型和指标下均优于现有未学习算法。

Conclusion: BLUR算法通过层次化建模和双层优化，有效解决了未学习问题中的性能下降问题，具有理论保证和实际优势。

Abstract: Enabling large language models (LLMs) to unlearn knowledge and capabilities
acquired during training has proven vital for ensuring compliance with data
regulations and promoting ethical practices in generative AI. Although there
are growing interests in developing various unlearning algorithms, it remains
unclear how to best formulate the unlearning problem. The most popular
formulation uses a weighted sum of forget and retain loss, but it often leads
to performance degradation due to the inherent trade-off between forget and
retain losses. In this work, we argue that it is important to model the
hierarchical structure of the unlearning problem, where the forget problem
(which \textit{unlearns} certain knowledge and/or capabilities) takes priority
over the retain problem (which preserves model utility). This hierarchical
structure naturally leads to a bi-level optimization formulation where the
lower-level objective focuses on minimizing the forget loss, while the
upper-level objective aims to maintain the model's utility. Based on this new
formulation, we propose a novel algorithm, termed Bi-Level UnleaRning
(\texttt{BLUR}), which not only possesses strong theoretical guarantees but
more importantly, delivers superior performance. In particular, our extensive
experiments demonstrate that \texttt{BLUR} consistently outperforms all the
state-of-the-art algorithms across various unlearning tasks, models, and
metrics. Codes are available at
https://github.com/OptimAI-Lab/BLURLLMUnlearning.

</details>


### [101] [UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data](https://arxiv.org/abs/2506.08167)
*Sunny Gupta,Nikita Jangid,Amit Sethi*

Key words: 联邦学习, 非独立同分布数据, 正则化策略, 分类器偏差, 可扩展性

TL;DR: UniVarFL是一种新颖的联邦学习框架，通过客户端级别的正则化策略解决非独立同分布数据导致的性能下降问题，显著提升了模型准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦学习在非独立同分布数据下性能下降严重，传统方法成本高或适应性差。

Method: 使用两种正则化策略：类别方差正则化和超球面均匀正则化，直接在客户端模拟IID训练动态。

Result: 在多个基准数据集上，UniVarFL表现优于现有方法，准确性更高。

Conclusion: UniVarFL是一种高效、可扩展的联邦学习解决方案，特别适用于资源受限场景。

Abstract: Federated Learning (FL) often suffers from severe performance degradation
when faced with non-IID data, largely due to local classifier bias. Traditional
remedies such as global model regularization or layer freezing either incur
high computational costs or struggle to adapt to feature shifts. In this work,
we propose UniVarFL, a novel FL framework that emulates IID-like training
dynamics directly at the client level, eliminating the need for global model
dependency. UniVarFL leverages two complementary regularization strategies
during local training: Classifier Variance Regularization, which aligns
class-wise probability distributions with those expected under IID conditions,
effectively mitigating local classifier bias; and Hyperspherical Uniformity
Regularization, which encourages a uniform distribution of feature
representations across the hypersphere, thereby enhancing the model's ability
to generalize under diverse data distributions. Extensive experiments on
multiple benchmark datasets demonstrate that UniVarFL outperforms existing
methods in accuracy, highlighting its potential as a highly scalable and
efficient solution for real-world FL deployments, especially in
resource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL

</details>


### [102] [Federated Learning on Stochastic Neural Networks](https://arxiv.org/abs/2506.08169)
*Jingqiao Tang,Ryan Bausback,Feng Bao,Richard Archibald*

Key words: 联邦学习,随机神经网络,数据噪声,非独立同分布

TL;DR: 联邦学习通过边缘设备优化模型并保护用户隐私,但本地数据噪声可能影响模型性能。本文提出使用随机神经网络作为本地模型,以估计真实数据状态并量化噪声,实验验证了其处理非独立同分布数据的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于联邦学习依赖客户端数据,本地数据噪声会降低模型性能,需要一种能够处理噪声并估计真实数据状态的方法。

Method: 提出将随机神经网络作为联邦学习的本地模型,通过其概率特性估计数据真实状态并量化噪声。

Result: 实验证明该方法在处理非独立同分布数据时表现优异。

Conclusion: 随机神经网络在联邦学习中能有效解决数据噪声问题,并提升模型性能。

Abstract: Federated learning is a machine learning paradigm that leverages edge
computing on client devices to optimize models while maintaining user privacy
by ensuring that local data remains on the device. However, since all data is
collected by clients, federated learning is susceptible to latent noise in
local datasets. Factors such as limited measurement capabilities or human
errors may introduce inaccuracies in client data. To address this challenge, we
propose the use of a stochastic neural network as the local model within the
federated learning framework. Stochastic neural networks not only facilitate
the estimation of the true underlying states of the data but also enable the
quantification of latent noise. We refer to our federated learning approach,
which incorporates stochastic neural networks as local models, as Federated
stochastic neural networks. We will present numerical experiments demonstrating
the performance and effectiveness of our method, particularly in handling
non-independent and identically distributed data.

</details>


### [103] [FedGA-Tree: Federated Decision Tree using Genetic Algorithm](https://arxiv.org/abs/2506.08176)
*Anh V Nguyen,Diego Klabjan*

Key words: 联邦学习, 决策树, 遗传算法, 隐私保护

TL;DR: 该论文提出了一种基于遗传算法的个性化决策树构建方法，用于联邦学习，适用于分类和回归任务，有效提升了模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于数据隐私问题，联邦学习受到关注，但现有方法主要关注参数化模型，非参数化模型如决策树的研究不足。

Method: 使用遗传算法构建个性化决策树，支持分类和数值数据。

Result: 实验表明，该方法优于仅在本地数据上训练的决策树和基准算法。

Conclusion: 遗传算法为联邦学习中的决策树构建提供了有效的新途径。

Abstract: In recent years, with rising concerns for data privacy, Federated Learning
has gained prominence, as it enables collaborative training without the
aggregation of raw data from participating clients. However, much of the
current focus has been on parametric gradient-based models, while nonparametric
counterparts such as decision tree are relatively understudied. Existing
methods for adapting decision trees to Federated Learning generally combine a
greedy tree-building algorithm with differential privacy to produce a global
model for all clients. These methods are limited to classification trees and
categorical data due to the constraints of differential privacy. In this paper,
we explore an alternative approach that utilizes Genetic Algorithm to
facilitate the construction of personalized decision trees and accommodate
categorical and numerical data, thus allowing for both classification and
regression trees. Comprehensive experiments demonstrate that our method
surpasses decision trees trained solely on local data and a benchmark
algorithm.

</details>


### [104] [Correlated Noise Mechanisms for Differentially Private Learning](https://arxiv.org/abs/2506.08201)
*Krishna Pillutla,Jalaj Upadhyay,Christopher A. Choquette-Choo,Krishnamurthy Dvijotham,Arun Ganesh,Monika Henzinger,Jonathan Katz,Ryan McKenna,H. Brendan McMahan,Keith Rush,Thomas Steinke,Abhradeep Thakurta*

Key words: 差分隐私, 相关噪声, 机器学习, 私有化训练, DP-FTRL

TL;DR: 本文探讨了差分隐私(DP)中相关噪声机制的设计与分析及其在AI和机器学习模型私有化训练中的应用。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统DP机制在每个随机梯度下降(SGD)步骤中注入独立噪声以保护训练数据隐私，但研究表明通过引入（反）相关噪声可以显著改善隐私与效用的权衡。

Method: 研究集中于加权前缀和估计的核心原语，探索了矩阵机制、分解机制和DP-FTRL等应用于学习算法的相关噪声机制。

Result: 相关噪声机制在隐私与效用权衡上表现优于独立噪声机制，并在工业规模上得到实际应用。

Conclusion: 相关噪声机制是差分隐私领域的重要进展，具有实际应用价值。

Abstract: This monograph explores the design and analysis of correlated noise
mechanisms for differential privacy (DP), focusing on their application to
private training of AI and machine learning models via the core primitive of
estimation of weighted prefix sums. While typical DP mechanisms inject
independent noise into each step of a stochastic gradient (SGD) learning
algorithm in order to protect the privacy of the training data, a growing body
of recent research demonstrates that introducing (anti-)correlations in the
noise can significantly improve privacy-utility trade-offs by carefully
canceling out some of the noise added on earlier steps in subsequent steps.
Such correlated noise mechanisms, known variously as matrix mechanisms,
factorization mechanisms, and DP-Follow-the-Regularized-Leader (DP-FTRL) when
applied to learning algorithms, have also been influential in practice, with
industrial deployment at a global scale.

</details>


### [105] [A Machine Learning Approach to Generate Residual Stress Distributions using Sparse Characterization Data in Friction-Stir Processed Parts](https://arxiv.org/abs/2506.08205)
*Shadab Anwar Shaikh,Kranthi Balusu,Ayoub Soulami*

Key words: 残余应力,机器学习,U-Net,全场预测,实验优化

TL;DR: 该论文提出了一种基于机器学习的方法（RSG），通过有限测量数据推断全场残余应力分布，以减少实验工作量并提高预测精度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 残余应力会影响组件性能，但全场表征实验成本高。通过机器学习生成全场应力分布可优化结构完整性和寿命。

Method: 构建大量过程模拟数据集，使用U-Net架构的ML模型进行训练，并通过超参数调优提升性能。

Result: 模型在模拟应力预测中表现出高准确性和泛化能力，实验数据验证了其有效性。

Conclusion: 该方法可行，能从有限测量中全面理解残余应力分布，显著减少实验工作。

Abstract: Residual stresses, which remain within a component after processing, can
deteriorate performance. Accurately determining their full-field distributions
is essential for optimizing the structural integrity and longevity. However,
the experimental effort required for full-field characterization is
impractical. Given these challenges, this work proposes a machine learning (ML)
based Residual Stress Generator (RSG) to infer full-field stresses from limited
measurements. An extensive dataset was initially constructed by performing
numerous process simulations with a diverse parameter set. A ML model based on
U-Net architecture was then trained to learn the underlying structure through
systematic hyperparameter tuning. Then, the model's ability to generate
simulated stresses was evaluated, and it was ultimately tested on actual
characterization data to validate its effectiveness. The model's prediction of
simulated stresses shows that it achieved excellent predictive accuracy and
exhibited a significant degree of generalization, indicating that it
successfully learnt the latent structure of residual stress distribution. The
RSG's performance in predicting experimentally characterized data highlights
the feasibility of the proposed approach in providing a comprehensive
understanding of residual stress distributions from limited measurements,
thereby significantly reducing experimental efforts.

</details>


### [106] [What makes an Ensemble (Un) Interpretable?](https://arxiv.org/abs/2506.08216)
*Shahaf Bassan,Guy Amir,Meirav Zehavi,Guy Katz*

Key words: 集成模型, 可解释性, 计算复杂性, 决策树, 线性模型

TL;DR: 研究了集成模型的可解释性，发现其复杂性受基础模型数量、大小和类型影响，并通过计算复杂性理论分析得出小型决策树集成可解释，而线性模型集成难解释。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前缺乏对集成模型可解释性的数学理解，尤其是基础模型数量、大小和类型如何影响其可解释性。

Method: 应用计算复杂性理论分析不同集成配置的解释挑战。

Result: 发现小型决策树集成可高效解释，而线性模型集成解释困难。

Conclusion: 计算复杂性视角为理解集成模型的可解释性提供了更坚实基础。

Abstract: Ensemble models are widely recognized in the ML community for their limited
interpretability. For instance, while a single decision tree is considered
interpretable, ensembles of trees (e.g., boosted trees) are often treated as
black-boxes. Despite this folklore recognition, there remains a lack of
rigorous mathematical understanding of what particularly makes an ensemble
(un)-interpretable, including how fundamental factors like the (1) *number*,
(2) *size*, and (3) *type* of base models influence its interpretability. In
this work, we seek to bridge this gap by applying concepts from computational
complexity theory to study the challenges of generating explanations for
various ensemble configurations. Our analysis uncovers nuanced complexity
patterns influenced by various factors. For example, we demonstrate that under
standard complexity assumptions like P$\neq$NP, interpreting ensembles remains
intractable even when base models are of constant size. Surprisingly, the
complexity changes drastically with the number of base models: small ensembles
of decision trees are efficiently interpretable, whereas interpreting ensembles
with even a constant number of linear models remains intractable. We believe
that our findings provide a more robust foundation for understanding the
interpretability of ensembles, emphasizing the benefits of examining it through
a computational complexity lens.

</details>


### [107] [Mondrian: Transformer Operators via Domain Decomposition](https://arxiv.org/abs/2506.08226)
*Arthur Feeney,Kuei-Hsiang Huang,Aparna Chandramowlishwaran*

Key words: 

TL;DR: Mondrian是一种基于Transformer的算子学习模型，通过将域分解为不重叠的子域并应用注意力机制，解决了高分辨率、多尺度PDE建模的挑战。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统Transformer模型由于注意力的二次计算成本及其与离散化的耦合，难以扩展到高分辨率、多尺度领域。

Method: Mondrian通过域分解将注意力与离散化解耦，并在子域内使用神经算子替代标准层，跨子域注意力通过基于softmax的函数内积计算。

Result: Mondrian在Allen-Cahn和Navier-Stokes偏微分方程上表现优异，支持分辨率扩展且无需重新训练。

Conclusion: 域分解注意力为可扩展且通用的神经算子提供了有前景的解决方案。

Abstract: Operator learning enables data-driven modeling of partial differential
equations (PDEs) by learning mappings between function spaces. However, scaling
transformer-based operator models to high-resolution, multiscale domains
remains a challenge due to the quadratic cost of attention and its coupling to
discretization. We introduce \textbf{Mondrian}, transformer operators that
decompose a domain into non-overlapping subdomains and apply attention over
sequences of subdomain-restricted functions. Leveraging principles from domain
decomposition, Mondrian decouples attention from discretization. Within each
subdomain, it replaces standard layers with expressive neural operators, and
attention across subdomains is computed via softmax-based inner products over
functions. The formulation naturally extends to hierarchical windowed and
neighborhood attention, supporting both local and global interactions. Mondrian
achieves strong performance on Allen-Cahn and Navier-Stokes PDEs, demonstrating
resolution scaling without retraining. These results highlight the promise of
domain-decomposed attention for scalable and general-purpose neural operators.

</details>


### [108] [Scaling Laws of Motion Forecasting and Planning -- A Technical Report](https://arxiv.org/abs/2506.08228)
*Mustafa Baniodeh,Kratarth Goel,Scott Ettinger,Carlos Fuertes,Ari Seff,Tim Shen,Cole Gulino,Chenjie Yang,Ghassen Jerfel,Dokook Choe,Rui Wang,Vinutha Kallem,Sergio Casas,Rami Al-Rfou,Benjamin Sapp,Dragomir Anguelov*

Key words: 自动驾驶, Transformer模型, 运动预测, 规划, 计算最优模型

TL;DR: 研究了自动驾驶领域中联合运动预测和规划的编码器-解码器自回归Transformer模型的实证缩放规律。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索模型在自动驾驶任务中的性能如何随着计算资源的增加而提升，并验证开环指标与闭环指标的相关性。

Method: 使用50万小时的驾驶数据集，分析模型性能与计算预算、模型参数数量和训练数据大小的关系。

Result: 模型性能随计算预算呈幂律增长，训练损失与评估指标强相关；最优缩放要求模型大小增速为数据集的1.5倍。

Conclusion: 优化训练和推理时的缩放特性是提升模型性能的关键，同时通用驾驶数据可缓解机器人数据的稀缺问题。

Abstract: We study the empirical scaling laws of a family of encoder-decoder
autoregressive transformer models on the task of joint motion forecasting and
planning in the autonomous driving domain. Using a 500 thousand hours driving
dataset, we demonstrate that, similar to language modeling, model performance
improves as a power-law function of the total compute budget, and we observe a
strong correlation between model training loss and model evaluation metrics.
Most interestingly, closed-loop metrics also improve with scaling, which has
important implications for the suitability of open-loop metrics for model
development and hill climbing. We also study the optimal scaling of the number
of transformer parameters and the training data size for a training
compute-optimal model. We find that as the training compute budget grows,
optimal scaling requires increasing the model size 1.5x as fast as the dataset
size. We also study inference-time compute scaling, where we observe that
sampling and clustering the output of smaller models makes them competitive
with larger models, up to a crossover point beyond which a larger models
becomes more inference-compute efficient. Overall, our experimental results
demonstrate that optimizing the training and inference-time scaling properties
of motion forecasting and planning models is a key lever for improving their
performance to address a wide variety of driving scenarios. Finally, we briefly
study the utility of training on general logged driving data of other agents to
improve the performance of the ego-agent, an important research area to address
the scarcity of robotics data for large capacity models training.

</details>


### [109] [Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework](https://arxiv.org/abs/2506.08231)
*Melissa Estevez,Nisha Singh,Lauren Dyson,Blythe Adamson,Qianyu Yuan,Megan W. Hildner,Erin Fidyk,Olive Mbah,Farhad Khan,Kathi Seidl-Rathkopf,Aaron B. Cohen*

Key words: LLM, 临床数据, 电子健康记录, 质量评估, 偏见评估

TL;DR: 论文提出了一种评估LLM提取临床数据质量的综合框架，涵盖性能基准测试、自动化验证和复制分析，以提高数据可靠性和公平性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着LLMs在临床数据分析中的广泛应用，确保提取数据的可靠性、准确性和公平性成为关键挑战，现有框架未能完全解决LLM特有的错误模式。

Method: 提出多维评估框架，包括变量级性能基准测试、自动化一致性检查、复制分析及偏见评估。

Result: 框架能识别需改进的变量、系统性潜在错误，并确认数据集的实际适用性，同时支持跨人口亚组的偏见评估。

Conclusion: 该框架为LLM提取的RWD提供了严格透明的评估方法，推动了行业标准，支持AI在肿瘤研究中的可信应用。

Abstract: Large language models (LLMs) are increasingly used to extract clinical data
from electronic health records (EHRs), offering significant improvements in
scalability and efficiency for real-world data (RWD) curation in oncology.
However, the adoption of LLMs introduces new challenges in ensuring the
reliability, accuracy, and fairness of extracted data, which are essential for
research, regulatory, and clinical applications. Existing quality assurance
frameworks for RWD and artificial intelligence do not fully address the unique
error modes and complexities associated with LLM-extracted data. In this paper,
we propose a comprehensive framework for evaluating the quality of clinical
data extracted by LLMs. The framework integrates variable-level performance
benchmarking against expert human abstraction, automated verification checks
for internal consistency and plausibility, and replication analyses comparing
LLM-extracted data to human-abstracted datasets or external standards. This
multidimensional approach enables the identification of variables most in need
of improvement, systematic detection of latent errors, and confirmation of
dataset fitness-for-purpose in real-world research. Additionally, the framework
supports bias assessment by stratifying metrics across demographic subgroups.
By providing a rigorous and transparent method for assessing LLM-extracted RWD,
this framework advances industry standards and supports the trustworthy use of
AI-powered evidence generation in oncology research and practice.

</details>


### [110] [Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations](https://arxiv.org/abs/2506.08240)
*Dongkyu Cho,Rumi Chunara*

Key words: 数据增广, 泛化, 随机增广, 遗忘问题, 单源域泛化

TL;DR: 重新审视随机数据增广，提出解决其缺陷的简单方法，通过解决遗忘问题提升泛化性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随机增广成本低但效果有限，解决其潜在问题以提高泛化能力。

Method: 提出改进随机增广泛化效果的简单方法，解决其导致的特征扭曲问题。

Result: 在单源域泛化（sDG）基准测试中表现出强泛化性能。

Conclusion: 随机增广经过改进后能显著提升泛化效果，成本低且有效。

Abstract: Data augmentation is a promising tool for enhancing out-of-distribution
generalization, where the key is to produce diverse, challenging variations of
the source domain via costly targeted augmentations that maximize its
generalization effect. Conversely, random augmentation is inexpensive but is
deemed suboptimal due to its limited effect. In this paper, we revisit random
augmentation and explore methods to address its shortcomings. We show that the
stochastic nature of random augmentation can produce a set of colliding
augmentations that distorts the learned features, similar to catastrophic
forgetting. We propose a simple solution that improves the generalization
effect of random augmentation by addressing forgetting, which displays strong
generalization performance across various single source domain generalization
(sDG) benchmarks.

</details>


### [111] [Temporalizing Confidence: Evaluation of Chain-of-Thought Reasoning with Signal Temporal Logic](https://arxiv.org/abs/2506.08243)
*Zhenjiang Mao,Artem Bisliouk,Rohith Reddy Nama,Ivan Ruchkin*

Key words: 大语言模型, 数学推理, 信号时序逻辑, 不确定性校准

TL;DR: 提出了一种结构化框架，利用信号时序逻辑（STL）评估LLM的逐步置信度，并通过不确定性重塑策略改进校准和可靠性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: LLM在数学推理任务中常产生高置信但错误的输出，缺乏可解释性，特别是在教育等领域存在风险。

Method: 通过STL定义时序约束，计算鲁棒性分数作为置信度估计，并引入不确定性重塑策略。

Result: 实验表明，该方法在校准指标和不确定性估计上优于传统方法。

Conclusion: 提出的框架显著提升了LLM输出的可靠性和可解释性。

Abstract: Large Language Models (LLMs) have shown impressive performance in
mathematical reasoning tasks when guided by Chain-of-Thought (CoT) prompting.
However, they tend to produce highly confident yet incorrect outputs, which
poses significant risks in domains like education, where users may lack the
expertise to assess reasoning steps. To address this, we propose a structured
framework that models stepwise confidence as a temporal signal and evaluates it
using Signal Temporal Logic (STL). In particular, we define formal STL-based
constraints to capture desirable temporal properties and compute robustness
scores that serve as structured, interpretable confidence estimates. Our
approach also introduces a set of uncertainty reshaping strategies to enforce
smoothness, monotonicity, and causal consistency across the reasoning
trajectory. Experiments show that our approach consistently improves
calibration metrics and provides more reliable uncertainty estimates than
conventional confidence aggregation and post-hoc calibration.

</details>


### [112] [Parameter-free approximate equivariance for tasks with finite group symmetry](https://arxiv.org/abs/2506.08244)
*Riccardo Ali,Pietro Liò,Jamie Vicary*

Key words: 等变神经网络, 群表示, 潜在空间, 损失函数, 计算效率

TL;DR: 本文提出了一种零参数的近似等变性方法，通过在损失函数中添加惩罚项，简化了现有等变神经网络的复杂性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有等变方法计算密集且参数多，通常依赖于特定架构，限制了灵活性和效率。

Method: 在潜在表示中为有限群添加近似等变性损失项，无需额外参数，并让网络学习潜在空间上的群表示。

Result: 实验表明，网络倾向于学习正则表示，且该方法在多个数据集上性能与现有方法相当或更好，但参数更少。

Conclusion: 提出的方法为等变性提供了一种简单高效的实现方式，平衡了计算复杂度和性能。

Abstract: Equivariant neural networks incorporate symmetries through group actions,
embedding them as an inductive bias to improve performance on a wide variety of
tasks. However, existing equivariant methods can be computationally intensive,
with high parameter counts, and are often tied to a specific architecture. We
propose a simple zero-parameter approach that imposes approximate equivariance
for a finite group in the latent representation, as an additional term in the
loss function. We conduct experiments which allow the network to learn a group
representation on the latent space, and show in every case it prefers to learn
the regular representation. Fixing this action on the latent space, this yields
a simple method to impose approximate equivariance as an additional loss
penalty. We benchmark our approach on three datasets and compare it against
several existing equivariant methods, showing that in many cases it achieves
similar or better performance for a fraction of the parameters.

</details>


### [113] [SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense](https://arxiv.org/abs/2506.08255)
*Patryk Krukowski,Łukasz Gorczyca,Piotr Helm,Kamil Książek,Przemysław Spurek*

Key words: 灾难性遗忘、对抗攻击、持续学习、超网络、区间算术

TL;DR: SHIELD是一种新型方法，通过结合基于超网络的持续学习和区间算术，解决了深度神经网络中的灾难性遗忘和对对抗攻击的脆弱性问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决深度神经网络在适应新数据集时快速遗忘已学知识（灾难性遗忘）以及对输入数据的小扰动缺乏鲁棒性的问题。

Method: 引入超网络用于生成针对特定数据的目标模型权重，并结合区间算术保证输入数据范围内的严格安全性。

Result: SHIELD能够在持续学习中动态生成子任务网络，同时确保输入数据范围内的安全预测，不牺牲网络适应性。

Conclusion: SHIELD为持续学习提供了一种安全且灵活的解决方案，成功解决了灾难性遗忘和对立攻击的双重挑战。

Abstract: Traditional deep neural networks suffer from several limitations, including
catastrophic forgetting. When models are adapted to new datasets, they tend to
quickly forget previously learned knowledge. Another significant issue is the
lack of robustness to even small perturbations in the input data. In practice,
we can often easily perform adversarial attacks and change the network's
predictions, adding minimal noise to the input. Dedicated architectures and
training procedures can solve each of the above problems separately.
Unfortunately, currently, no model can simultaneously address both catastrophic
forgetting and vulnerability to adversarial attacks. We introduce SHIELD
(Secure Hypernetworks for Incremental Expansion and Learning Defense), a novel
approach that integrates a hypernetwork-based continual learning approach with
interval arithmetic. SHIELD use the hypernetwork to transfer trainable task
embedding vectors into the weights of a target model dedicated to specific
data. This paradigm allows for the dynamic generation of separate networks for
each subtask, while the hypernetwork aggregates and analyzes information across
all tasks. The target model takes in the input a data sample with a defined
interval range, and by creating a hypercube, produces a prediction for the
given range. Therefore, such target models provide strict guarantees against
all possible attacks for data samples within the interval range. Our approach
enhances security without sacrificing network adaptability, addressing the
overlooked challenge of safety in continual learning.

</details>


### [114] [Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints](https://arxiv.org/abs/2506.08266)
*Yaswanth Chittepu,Blossom Metevier,Will Schwarzer,Austin Hoag,Scott Niekum,Philip S. Thomas*

Key words: 语言模型对齐, 强化学习, 安全性, HC-RLHF

TL;DR: 本文提出了一种名为HC-RLHF的方法，通过高置信度的安全强化学习来解决语言模型在敏感领域中的安全问题，同时保持帮助性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法在语言模型对齐中常将安全性与帮助性做权衡，导致敏感领域出现不可接受的响应。需要一种能确保高置信度安全性的方法。

Method: HC-RLHF方法将人类偏好明确分解为帮助性和无害性（安全性），分别训练奖励模型和成本模型，并通过两步骤过程（悲观约束优化和安全测试）寻找安全解。

Result: 实验表明，HC-RLHF能以高概率生成安全模型，并在无害性和帮助性上优于现有方法。

Conclusion: HC-RLHF提供了一种兼顾安全性和帮助性的有效方法，适用于语言模型对齐。

Abstract: Existing approaches to language model alignment often treat safety as a
tradeoff against helpfulness, which can lead to unacceptable responses in
sensitive domains. To ensure reliable performance in such settings, we propose
High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a
method that provides high-confidence safety guarantees while maximizing
helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human
preferences into helpfulness and harmlessness (safety), which are learned by
training a reward model and a cost model, respectively. It then employs a
two-step process to find safe solutions. In the first step, it optimizes the
reward function under an intentionally pessimistic version of the cost
constraint. In the second step, the trained model undergoes a safety test to
verify whether its performance stays within an upper-confidence bound of the
actual cost constraint. We provide a theoretical analysis of HC-RLHF, including
proof that it will not return an unsafe solution with a probability greater
than a user-specified threshold. For our empirical analysis, we apply HC-RLHF
to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and
LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF
produces safe models with high probability and can improve harmlessness and
helpfulness compared to previous methods.

</details>


### [115] [Sparse Interpretable Deep Learning with LIES Networks for Symbolic Regression](https://arxiv.org/abs/2506.08267)
*Mansooreh Montazerin,Majd Al Aawar,Antonio Ortega,Ajitesh Srivastava*

Key words: 符号回归, 神经网络, LIES, 可解释性, 过采样

TL;DR: 本文介绍了一种名为LIES的固定神经网络架构，通过可解释的原始激活函数优化符号表达式建模，提出了一种框架来提取紧凑公式，并在基准测试中表现出色。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的符号回归方法依赖于基于群体的搜索或自回归建模，存在可扩展性和符号一致性问题。LIES旨在通过固定架构和优化策略解决这些问题。

Method: 使用LIES神经网络架构，结合过采样策略和定制损失函数进行训练，并通过剪枝策略简化公式。

Result: LIES框架在符号回归基准测试中表现优于所有基线方法，能够生成稀疏且准确的符号公式。

Conclusion: LIES方法通过其独特的设计组件和优化策略，显著提升了符号回归的性能和可解释性。

Abstract: Symbolic regression (SR) aims to discover closed-form mathematical
expressions that accurately describe data, offering interpretability and
analytical insight beyond standard black-box models. Existing SR methods often
rely on population-based search or autoregressive modeling, which struggle with
scalability and symbolic consistency. We introduce LIES (Logarithm, Identity,
Exponential, Sine), a fixed neural network architecture with interpretable
primitive activations that are optimized to model symbolic expressions. We
develop a framework to extract compact formulae from LIES networks by training
with an appropriate oversampling strategy and a tailored loss function to
promote sparsity and to prevent gradient instability. After training, it
applies additional pruning strategies to further simplify the learned
expressions into compact formulae. Our experiments on SR benchmarks show that
the LIES framework consistently produces sparse and accurate symbolic formulae
outperforming all baselines. We also demonstrate the importance of each design
component through ablation studies.

</details>


### [116] [SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space](https://arxiv.org/abs/2506.08270)
*Zitong Huang,Mansooreh Montazerin,Ajitesh Srivastava*

Key words: 神经网络设计,架构搜索,自动优化,连续潜在空间

TL;DR: 提出一种同时优化神经网络结构和权重的新方法，通过连续潜在空间嵌入架构和参数信息，避免了传统方法的离散化和手动调整问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统神经网络设计方法依赖手动试错或离散化的架构搜索，效率低且耗时。本文旨在提出一种同时优化结构和权重的高效方法。

Method: 训练一个多尺度自编码器，将架构和参数信息嵌入连续潜在空间。通过梯度下降优化初始化点，联合优化结构和权重，并引入稀疏性和紧凑性惩罚。

Result: 在合成回归任务中，该方法成功发现了性能优越的稀疏紧凑神经网络。

Conclusion: 该方法提供了一种高效且连续的神经网络设计和优化途径，优于传统离散化方法。

Abstract: Designing neural networks typically relies on manual trial and error or a
neural architecture search (NAS) followed by weight training. The former is
time-consuming and labor-intensive, while the latter often discretizes
architecture search and weight optimization. In this paper, we propose a
fundamentally different approach that simultaneously optimizes both the
architecture and the weights of a neural network. Our framework first trains a
universal multi-scale autoencoder that embeds both architectural and parametric
information into a continuous latent space, where functionally similar neural
networks are mapped closer together. Given a dataset, we then randomly
initialize a point in the embedding space and update it via gradient descent to
obtain the optimal neural network, jointly optimizing its structure and
weights. The optimization process incorporates sparsity and compactness
penalties to promote efficient models. Experiments on synthetic regression
tasks demonstrate that our method effectively discovers sparse and compact
neural networks with strong performance.

</details>


### [117] [Universal Differential Equations for Scientific Machine Learning of Node-Wise Battery Dynamics in Smart Grids](https://arxiv.org/abs/2506.08272)
*Tarushri N. S.*

Key words: 通用微分方程, 智能电网, 电池动力学, 科学机器学习, 神经网络

TL;DR: 论文提出了一种基于通用微分方程（UDEs）的方法，通过将神经网络嵌入电池动力学方程中，学习节点特定的电池演化行为。实验证明该方法能准确预测电池轨迹并保持长期稳定性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在智能电网系统中，由于太阳能输入的随机性和家庭负载的变化性，节点级的电池动力学建模仍具挑战性。传统方法难以泛化且无法捕捉未建模的动态。

Method: 提出UDE框架，将神经网络的残差嵌入物理启发的电池ODE中，利用合成数据模拟电池动态，神经网络学习未观测或随机性修正。

Result: 实验显示训练的UDE与真实电池轨迹高度吻合，收敛平稳且长期预测稳定。

Conclusion: UDE方法在去中心化能源网络中电池建模的可行性得到验证，对实时控制和优化具有广泛意义。

Abstract: Universal Differential Equations (UDEs), which blend neural networks with
physical differential equations, have emerged as a powerful framework for
scientific machine learning (SciML), enabling data-efficient, interpretable,
and physically consistent modeling. In the context of smart grid systems,
modeling node-wise battery dynamics remains a challenge due to the
stochasticity of solar input and variability in household load profiles.
Traditional approaches often struggle with generalization and fail to capture
unmodeled residual dynamics. This work proposes a UDE-based approach to learn
node-specific battery evolution by embedding a neural residual into a
physically inspired battery ODE. Synthetic yet realistic solar generation and
load demand data are used to simulate battery dynamics over time. The neural
component learns to model unobserved or stochastic corrections arising from
heterogeneity in node demand and environmental conditions. Comprehensive
experiments reveal that the trained UDE aligns closely with ground truth
battery trajectories, exhibits smooth convergence behavior, and maintains
stability in long-term forecasts. These findings affirm the viability of
UDE-based SciML approaches for battery modeling in decentralized energy
networks and suggest broader implications for real-time control and
optimization in renewable-integrated smart grids.

</details>


### [118] [The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks](https://arxiv.org/abs/2506.08274)
*João Manoel Herrera Pinheiro,Suzana Vilas Boas de Oliveira,Thiago Henrique Segreto Silva,Pedro Antonio Rabelo Saraiva,Enzo Ferreira de Souza,Leonardo André Ambrosio,Marcelo Becker*

Key words: 特征缩放,机器学习,模型性能,可重复性

TL;DR: 该研究通过系统评估12种特征缩放技术对14种机器学习算法和16个数据集的影响，揭示了不同缩放技术对模型性能的显著差异，尤其是对逻辑回归、SVM等模型的性能影响较大。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于缺乏对特征缩放的全面研究，本研究旨在填补这一空白，为实践者提供关于特征缩放技术选择的指导。

Method: 研究采用了12种特征缩放技术，在14种机器学习算法和16个数据集上进行了系统评估，分析了预测性能和计算成本。

Result: 研究发现，集成方法对缩放技术不敏感，而逻辑回归、SVM等模型则表现显著依赖于缩放技术的选择。

Conclusion: 研究为实践者提供了关于特征缩放技术选择的模型具体指导，强调了透明性和可重复性。

Abstract: This research addresses the critical lack of comprehensive studies on feature
scaling by systematically evaluating 12 scaling techniques - including several
less common transformations - across 14 different Machine Learning algorithms
and 16 datasets for classification and regression tasks. We meticulously
analyzed impacts on predictive performance (using metrics such as accuracy,
MAE, MSE, and $R^2$) and computational costs (training time, inference time,
and memory usage). Key findings reveal that while ensemble methods (such as
Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM)
demonstrate robust performance largely independent of scaling, other widely
used models such as Logistic Regression, SVMs, TabNet, and MLPs show
significant performance variations highly dependent on the chosen scaler. This
extensive empirical analysis, with all source code, experimental results, and
model parameters made publicly available to ensure complete transparency and
reproducibility, offers model-specific crucial guidance to practitioners on the
need for an optimal selection of feature scaling techniques.

</details>


### [119] [From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium](https://arxiv.org/abs/2506.08292)
*Xie Yi,Zhanke Zhou,Chentao Cao,Qiyu Niu,Tongliang Liu,Bo Han*

Key words: 多智能体框架,大语言模型,贝叶斯纳什均衡,ECON,强化学习

TL;DR: 提出了ECON框架，通过贝叶斯纳什均衡解决多智能体协调问题，显著降低计算成本并提高性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多智能体框架可以提高大语言模型的推理能力，但现有方法计算成本高且缺乏收敛保证，需要更高效的协调方法。

Method: 将多智能体协调建模为不完全信息博弈，提出ECON框架，结合分布式推理与集中输出，通过贝叶斯纳什均衡实现高效协调。

Result: ECON在六项复杂推理和规划任务中平均优于现有方法11.2%，并具备可扩展性。

Conclusion: ECON证明了贝叶斯纳什均衡在多智能体协调中的有效性，为更强大的多模型集成提供了新途径。

Abstract: Multi-agent frameworks can substantially boost the reasoning power of large
language models (LLMs), but they typically incur heavy computational costs and
lack convergence guarantees. To overcome these challenges, we recast multi-LLM
coordination as an incomplete-information game and seek a Bayesian Nash
equilibrium (BNE), in which each agent optimally responds to its probabilistic
beliefs about the strategies of others. We introduce Efficient Coordination via
Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that
marries distributed reasoning with centralized final output. Under ECON, each
LLM independently selects responses that maximize its expected reward,
conditioned on its beliefs about co-agents, without requiring costly
inter-agent exchanges. We mathematically prove that ECON attains a markedly
tighter regret bound than non-equilibrium multi-agent schemes. Empirically,
ECON outperforms existing multi-LLM approaches by 11.2% on average across six
benchmarks spanning complex reasoning and planning tasks. Further experiments
demonstrate ECON's ability to flexibly incorporate additional models,
confirming its scalability and paving the way toward larger, more powerful
multi-LLM ensembles. The code is publicly available at:
https://github.com/tmlr-group/ECON.

</details>


### [120] [From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?](https://arxiv.org/abs/2506.08295)
*Zhanke Zhou,Xiao Feng,Zhaocheng Zhu,Jiangchao Yao,Sanmi Koyejo,Bo Han*

Key words: 主动推理, AR-Bench, 大型语言模型, 交互学习, 推理能力

TL;DR: AR-Bench是一个新基准，专注于评估大型语言模型（LLM）的主动推理能力，填补了现有基准主要测试被动推理的空白。测试表明，当前LLM在主动推理上表现不佳，需进一步研究改进方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有基准主要评估LLM的被动推理能力，而主动推理（需与外部系统交互获取信息）尚未得到系统研究，因此设计了AR-Bench来填补这一空白。

Method: AR-Bench包含三种任务类型（侦探案例、情境谜题和猜数字），分别测试常识、逻辑和符号推理能力，并通过实验评估LLM的表现。

Result: 实验显示，当前LLM在主动推理任务中表现较差，无法有效获取或利用所需信息，且现有改进策略（如树搜索或后训练）效果有限。

Conclusion: 研究强调需发展针对主动推理的新方法，如交互学习、实时反馈和环境感知目标训练，以提升LLM的实际应用能力。

Abstract: While existing benchmarks probe the reasoning abilities of large language
models (LLMs) across diverse domains, they predominantly assess passive
reasoning, providing models with all the information needed to reach a
solution. By contrast, active reasoning-where an LLM must interact with
external systems to acquire missing evidence or data-has received little
systematic attention. To address this shortfall, we present AR-Bench, a novel
benchmark designed explicitly to evaluate an LLM's active reasoning skills.
AR-Bench comprises three task families-detective cases, situation puzzles, and
guessing numbers-that together simulate real-world, agentic scenarios and
measure performance across commonsense, logical, and symbolic reasoning
challenges. Empirical evaluation on AR-Bench demonstrates that contemporary
LLMs exhibit pronounced difficulties with active reasoning: they frequently
fail to acquire or leverage the information needed to solve tasks. This gap
highlights a stark divergence between their passive and active reasoning
abilities. Moreover, ablation studies indicate that even advanced strategies,
such as tree-based searching or post-training approaches, yield only modest
gains and fall short of the levels required for real-world deployment.
Collectively, these findings highlight the critical need to advance methodology
for active reasoning, e.g., incorporating interactive learning, real-time
feedback loops, and environment-aware objectives for training. The benchmark is
publicly available at: https://github.com/tmlr-group/AR-Bench.

</details>


### [121] [H$^2$GFM: Towards unifying Homogeneity and Heterogeneity on Text-Attributed Graphs](https://arxiv.org/abs/2506.08298)
*Trung-Kien Nguyen,Heng Ping,Shixuan Li,Peiyu Zhang,Nikos Kanakaris,Nicholas Kotov,Paul Bogdan*

Key words: 图形基础模型, 异构文本属性图, 上下文自适应图变换器, 专家混合模型

TL;DR: 摘要介绍了H2GFM框架，旨在解决图形基础模型（GFM）在异构文本属性图（HeTAGs）中的应用不足问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有研究主要关注同质文本属性图（HoTAGs），而忽略了异构文本属性图（HeTAGs）的多样性，限制了GFM的能力和应用范围。

Method: 提出了H2GFM框架，通过统一文本空间投影和上下文编码，结合上下文自适应图变换器（CGT）和专家混合模型，捕捉节点间的关系和异构结构模式。

Result: 在多种HoTAGs和HeTAGs的实验证明了模型的有效性。

Conclusion: H2GFM显著提升了GFM在异构图形任务中的泛化能力。

Abstract: The growing interests and applications of graph learning in diverse domains
have propelled the development of a unified model generalizing well across
different graphs and tasks, known as the Graph Foundation Model (GFM). Existing
research has leveraged text-attributed graphs (TAGs) to tackle the
heterogeneity in node features among graphs. However, they primarily focus on
homogeneous TAGs (HoTAGs), leaving heterogeneous TAGs (HeTAGs), where multiple
types of nodes/edges reside, underexplored. To enhance the capabilities and
applications of GFM, we introduce H$^2$GFM, a novel framework designed to
generalize across both HoTAGs and HeTAGs. Our model projects diverse
meta-relations among graphs under a unified textual space, and employs a
context encoding to capture spatial and higher-order semantic relationships. To
achieve robust node representations, we propose a novel context-adaptive graph
transformer (CGT), effectively capturing information from both context
neighbors and their relationships. Furthermore, we employ a mixture of CGT
experts to capture the heterogeneity in structural patterns among graph types.
Comprehensive experiments on a wide range of HoTAGs and HeTAGs as well as
learning scenarios demonstrate the effectiveness of our model.

</details>


### [122] [Learnable Spatial-Temporal Positional Encoding for Link Prediction](https://arxiv.org/abs/2506.08309)
*Katherine Tieu,Dongqi Fu,Zihao Li,Ross Maciejewski,Jingrui He*

Key words: 图神经网络, 位置编码, 时空图, 可学习编码, L-STEP

TL;DR: 论文提出了一种可学习的时空位置编码方法L-STEP，解决了现有位置编码在适应性、动态性和计算效率上的不足，并在多个数据集和任务中表现出色。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前图深度学习中的位置编码方法在适应性、动态性和计算效率上存在不足，无法充分处理复杂属性图和动态拓扑特征。

Method: 提出L-STEP，一种基于可学习时空位置编码的模型，通过MLP和时空谱视角优化编码效果，同时降低计算复杂度。

Result: L-STEP在13个经典数据集和TGB基准测试中表现优异，验证了其高效性和有效性。

Conclusion: L-STEP为时空图数据的位置编码提供了一种高效、可学习的解决方案，适用于动态和大规模图数据。

Abstract: Accurate predictions rely on the expressiveness power of graph deep learning
frameworks like graph neural networks and graph transformers, where a
positional encoding mechanism has become much more indispensable in recent
state-of-the-art works to record the canonical position information. However,
the current positional encoding is limited in three aspects: (1) most
positional encoding methods use pre-defined, and fixed functions, which are
inadequate to adapt to the complex attributed graphs; (2) a few pioneering
works proposed the learnable positional encoding but are still limited to the
structural information, not considering the real-world time-evolving
topological and feature information; (3) most positional encoding methods are
equipped with transformers' attention mechanism to fully leverage their
capabilities, where the dense or relational attention is often unaffordable on
large-scale structured data. Hence, we aim to develop Learnable
Spatial-Temporal Positional Encoding in an effective and efficient manner and
propose a simple temporal link prediction model named L-STEP. Briefly, for
L-STEP, we (1) prove the proposed positional learning scheme can preserve the
graph property from the spatial-temporal spectral viewpoint, (2) verify that
MLPs can fully exploit the expressiveness and reach transformers' performance
on that encoding, (3) change different initial positional encoding inputs to
show robustness, (4) analyze the theoretical complexity and obtain less
empirical running time than SOTA, and (5) demonstrate its temporal link
prediction out-performance on 13 classic datasets and with 10 algorithms in
both transductive and inductive settings using 3 different sampling strategies.
Also, \name\ obtains the leading performance in the newest large-scale TGB
benchmark. Our code is available at https://github.com/kthrn22/L-STEP.

</details>


### [123] [Private Evolution Converges](https://arxiv.org/abs/2506.08312)
*Tomás González,Giulia Fanti,Aaditya Ramdas*

Key words: 差分隐私, 合成数据生成, 私有演化, 理论分析, 1-Wasserstein距离

TL;DR: 本文提出了一种新的理论框架来解释私有演化（PE）方法的实际行为，并确定了其收敛的充分条件，同时证明了其在合成数据生成中的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: PE是一种无需训练的差分隐私合成数据生成方法，但现有理论分析依赖于不现实的假设，限制了其实际应用。本文旨在填补这一空白。

Method: 通过构建新的理论框架，分析了PE在$d$维有界域数据集上的收敛性，并将其与私有签名度量机制联系起来。

Result: 证明了PE生成的差分隐私合成数据集在1-Wasserstein距离上的收敛阶数为$	ilde{O}(d(n\epsilon)^{-1/d})$。

Conclusion: 本文的理论分析为PE的收敛性提供了严格支持，并通过模拟验证了其实际应用价值。

Abstract: Private Evolution (PE) is a promising training-free method for differentially
private (DP) synthetic data generation. While it achieves strong performance in
some domains (e.g., images and text), its behavior in others (e.g., tabular
data) is less consistent. To date, the only theoretical analysis of the
convergence of PE depends on unrealistic assumptions about both the algorithm's
behavior and the structure of the sensitive dataset. In this work, we develop a
new theoretical framework to explain PE's practical behavior and identify
sufficient conditions for its convergence. For $d$-dimensional sensitive
datasets with $n$ data points from a bounded domain, we prove that PE produces
an $(\epsilon, \delta)$-DP synthetic dataset with expected 1-Wasserstein
distance of order $\tilde{O}(d(n\epsilon)^{-1/d})$ from the original,
establishing worst-case convergence of the algorithm as $n \to \infty$. Our
analysis extends to general Banach spaces as well. We also connect PE to the
Private Signed Measure Mechanism, a method for DP synthetic data generation
that has thus far not seen much practical adoption. We demonstrate the
practical relevance of our theoretical findings in simulations.

</details>


### [124] [Why Masking Diffusion Works: Condition on the Jump Schedule for Improved Discrete Diffusion](https://arxiv.org/abs/2506.08316)
*Alan N. Amin,Nate Gruver,Andrew Gordon Wilson*

Key words: 离散扩散模型, 遮蔽扩散, 跳跃时间, SCUD, 图像生成, 文本生成, 蛋白质数据

TL;DR: 本文解释了遮蔽扩散（masking diffusion）在离散扩散模型中表现优异的原因，并提出了一种新的方法——时间表条件离散扩散（SCUD），该方法通过结合跳跃时间的已知分布来改进其他离散扩散模型，从而在图像、文本和蛋白质数据上超越遮蔽扩散的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究离散扩散模型中的性能差异，尤其是遮蔽扩散为何优于其他模型，并探索如何通过结合跳跃时间的已知分布来改进其他模型。

Method: 提出了时间表条件离散扩散（SCUD），将跳跃时间的已知分布整合到离散扩散模型中，并在图像、文本和蛋白质数据上进行了实验。

Result: SCUD方法超越了遮蔽扩散的性能，证明了其在离散扩散模型中的有效性。

Conclusion: 通过结合跳跃时间的已知分布，SCUD方法可以显著提升离散扩散模型的性能，成为一种通用且高效的扩散模型框架。

Abstract: Discrete diffusion models, like continuous diffusion models, generate
high-quality samples by gradually undoing noise applied to datapoints with a
Markov process. Gradual generation in theory comes with many conceptual
benefits; for example, inductive biases can be incorporated into the noising
Markov process, and access to improved sampling algorithms. In practice,
however, the consistently best performing discrete diffusion model is,
surprisingly, masking diffusion, which does not denoise gradually. Here we
explain the superior performance of masking diffusion by noting that it makes
use of a fundamental difference between continuous and discrete Markov
processes: discrete Markov processes evolve by discontinuous jumps at a fixed
rate and, unlike other discrete diffusion models, masking diffusion builds in
the known distribution of jump times and only learns where to jump to. We show
that we can similarly bake in the known distribution of jump times into any
discrete diffusion model. The resulting models - schedule-conditioned discrete
diffusion (SCUD) - generalize classical discrete diffusion and masking
diffusion. By applying SCUD to models with noising processes that incorporate
inductive biases on images, text, and protein data, we build models that
outperform masking.

</details>


### [125] [Graph Prompting for Graph Learning Models: Recent Advances and Future Directions](https://arxiv.org/abs/2506.08326)
*Xingbo Fu,Zehong Wang,Zihan Chen,Jiazheng Li,Yaochen Zhu,Zhenyu Lei,Cong Shen,Yanfang Ye,Chuxu Zhang,Jundong Li*

Key words: 图学习,预训练,图提示,自监督学习,应用

TL;DR: 本文系统综述了图提示学习的最新进展，包括图预训练方法、主流图提示技术及其应用，并探讨了当前挑战与未来方向。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 图学习模型在多种现实场景中表现优异，图提示作为一种高效适配方法，值得系统梳理和研究。

Method: 介绍图预训练方法作为基础，详述图提示的主流技术及其可学习提示设计。

Result: 总结了图提示在不同领域的实际应用。

Conclusion: 图提示是一个有前景的方向，但仍存在开放性问题需要解决。

Abstract: Graph learning models have demonstrated great prowess in learning expressive
representations from large-scale graph data in a wide variety of real-world
scenarios. As a prevalent strategy for training powerful graph learning models,
the "pre-training, adaptation" scheme first pre-trains graph learning models on
unlabeled graph data in a self-supervised manner and then adapts them to
specific downstream tasks. During the adaptation phase, graph prompting emerges
as a promising approach that learns trainable prompts while keeping the
pre-trained graph learning models unchanged. In this paper, we present a
systematic review of recent advancements in graph prompting. First, we
introduce representative graph pre-training methods that serve as the
foundation step of graph prompting. Next, we review mainstream techniques in
graph prompting and elaborate on how they design learnable prompts for graph
prompting. Furthermore, we summarize the real-world applications of graph
prompting from different domains. Finally, we discuss several open challenges
in existing studies with promising future directions in this field.

</details>


### [126] [A Simple Analysis of Discretization Error in Diffusion Models](https://arxiv.org/abs/2506.08337)
*Juhyeok Choi,Chenglin Fan*

Key words: 扩散模型,DDPM,Euler--Maruyama离散化,Grönwall不等式,离散噪声

TL;DR: 该论文提出了一个简化理论框架，用于分析DDPM中Euler--Maruyama离散化的收敛性，并通过实验验证了离散噪声替换的可行性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有扩散模型离散化误差分析通常依赖复杂的概率工具，作者希望通过简化理论框架提升分析效率。

Method: 利用Grönwall不等式，在Lipschitz假设下推导收敛速率，并探索离散噪声替换高斯噪声的效果。

Result: 理论推导显示收敛速率为$\mathcal{O}(1/T^{1/2})$，实验证明离散噪声与高斯噪声性能相当。

Conclusion: 简化理论与离散噪声的结合，在扩散模型中实现了理论严谨性与实践效率的统一。

Abstract: Diffusion models, formulated as discretizations of stochastic differential
equations (SDEs), achieve state-of-the-art generative performance. However,
existing analyses of their discretization error often rely on complex
probabilistic tools. In this work, we present a simplified theoretical
framework for analyzing the Euler--Maruyama discretization of
variance-preserving SDEs (VP-SDEs) in Denoising Diffusion Probabilistic Models
(DDPMs), where $ T $ denotes the number of denoising steps in the diffusion
process. Our approach leverages Gr\"onwall's inequality to derive a convergence
rate of $ \mathcal{O}(1/T^{1/2}) $ under Lipschitz assumptions, significantly
streamlining prior proofs. Furthermore, we demonstrate that the Gaussian noise
in the discretization can be replaced by a discrete random variable (e.g.,
Rademacher or uniform noise) without sacrificing convergence guarantees-an
insight with practical implications for efficient sampling. Experiments
validate our theory, showing that (1) the error scales as predicted, (2)
discrete noise achieves comparable sample quality to Gaussian noise, and (3)
incorrect noise scaling degrades performance. By unifying simplified analysis
and discrete noise substitution, our work bridges theoretical rigor with
practical efficiency in diffusion-based generative modeling.

</details>


### [127] [Dynamical System Optimization](https://arxiv.org/abs/2506.08340)
*Emo Todorov*

Key words: 政策优化, 自主动态系统, 政策梯度, 强化学习, 生成AI

TL;DR: 本文提出了一种围绕政策参数优化的框架，将控制权交给政策本身，形成自主动态系统，从而无需直接使用动态规划或强化学习的方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在简化政策参数的优化过程，避免直接使用复杂的动态规划和强化学习方法。

Method: 通过自主系统层面的算法计算政策梯度、Hessians、自然梯度等，类似近似政策迭代和离策略学习。

Result: 表明这些算法可以计算与政策梯度和Hessians等相同的量，适用于行为克隆、机制设计等多种任务。

Conclusion: 该框架将政策参数与其他系统参数统一处理，适用于包括生成AI模型调优在内的多种应用。

Abstract: We develop an optimization framework centered around a core idea: once a
(parametric) policy is specified, control authority is transferred to the
policy, resulting in an autonomous dynamical system. Thus we should be able to
optimize policy parameters without further reference to controls or actions,
and without directly using the machinery of approximate Dynamic Programming and
Reinforcement Learning. Here we derive simpler algorithms at the autonomous
system level, and show that they compute the same quantities as policy
gradients and Hessians, natural gradients, proximal methods. Analogs to
approximate policy iteration and off-policy learning are also available. Since
policy parameters and other system parameters are treated uniformly, the same
algorithms apply to behavioral cloning, mechanism design, system
identification, learning of state estimators. Tuning of generative AI models is
not only possible, but is conceptually closer to the present framework than to
Reinforcement Learning.

</details>


### [128] [Differentially Private Relational Learning with Entity-level Privacy Guarantees](https://arxiv.org/abs/2506.08347)
*Yinan Huang,Haoteng Ying,Eli Chien,Rongzhe Wei,Pan Li*

Key words: 差分隐私, 关系学习, 梯度裁剪, 隐私放大, 网络数据

TL;DR: 提出了一个针对关系数据的差分隐私学习框架，解决了传统DP-SGD在关系学习中的两大挑战：实体参与多关系的高敏感性和多阶段采样分析的不可适用性。通过自适应梯度裁剪和扩展隐私放大分析，实现了有效的隐私-效用权衡。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在敏感领域的关系和网络结构数据学习中，保护个体实体隐私至关重要。虽然差分隐私（DP）提供了量化隐私风险的框架，但传统DP-SGD直接应用于关系学习存在高敏感性和复杂采样过程的挑战。

Method: 提出了一种针对关系学习的实体级DP保证框架，包括严格的敏感性分析、基于实体出现频率的自适应梯度裁剪方案，并扩展了隐私放大分析以处理依赖样本大小的耦合采样过程。

Result: 实验表明，该方法在文本编码器微调任务中实现了优越的隐私-效用权衡。

Conclusion: 该研究为关系数据提供了一种具有严格隐私保证的DP-SGD变体，解决了现有方法的局限性。

Abstract: Learning with relational and network-structured data is increasingly vital in
sensitive domains where protecting the privacy of individual entities is
paramount. Differential Privacy (DP) offers a principled approach for
quantifying privacy risks, with DP-SGD emerging as a standard mechanism for
private model training. However, directly applying DP-SGD to relational
learning is challenging due to two key factors: (i) entities often participate
in multiple relations, resulting in high and difficult-to-control sensitivity;
and (ii) relational learning typically involves multi-stage, potentially
coupled (interdependent) sampling procedures that make standard privacy
amplification analyses inapplicable. This work presents a principled framework
for relational learning with formal entity-level DP guarantees. We provide a
rigorous sensitivity analysis and introduce an adaptive gradient clipping
scheme that modulates clipping thresholds based on entity occurrence frequency.
We also extend the privacy amplification results to a tractable subclass of
coupled sampling, where the dependence arises only through sample sizes. These
contributions lead to a tailored DP-SGD variant for relational data with
provable privacy guarantees. Experiments on fine-tuning text encoders over
text-attributed network-structured relational data demonstrate the strong
utility-privacy trade-offs of our approach. Our code is available at
https://github.com/Graph-COM/Node_DP.

</details>


### [129] [An Adaptive Method Stabilizing Activations for Enhanced Generalization](https://arxiv.org/abs/2506.08353)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Key words: AdaAct, 优化算法, 激活方差, 学习率调整, 图像分类

TL;DR: AdaAct是一种新型优化算法，通过根据激活方差调整学习率来增强神经元输出的稳定性，从而提升泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决传统激活正则化方法的局限性，提出一种能够通过神经元级别的自适应提升训练稳定性和泛化能力的优化算法。

Method: AdaAct通过调整学习率以匹配激活方差，从而在训练过程中实现神经元级别的自适应。

Result: 在CIFAR和ImageNet等标准图像分类基准测试中，AdaAct表现优异，能够有效平衡Adam的收敛速度和SGD的泛化能力。

Conclusion: AdaAct在提升模型训练稳定性和泛化能力方面表现出色，同时保持了高效的执行时间。

Abstract: We introduce AdaAct, a novel optimization algorithm that adjusts learning
rates according to activation variance. Our method enhances the stability of
neuron outputs by incorporating neuron-wise adaptivity during the training
process, which subsequently leads to better generalization -- a complementary
approach to conventional activation regularization methods. Experimental
results demonstrate AdaAct's competitive performance across standard image
classification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing
it with other state-of-the-art methods. Importantly, AdaAct effectively bridges
the gap between the convergence speed of Adam and the strong generalization
capabilities of SGD, all while maintaining competitive execution times. Code is
available at https://github.com/hseung88/adaact.

</details>


### [130] [NysAct: A Scalable Preconditioned Gradient Descent using Nystrom Approximation](https://arxiv.org/abs/2506.08360)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Key words: 优化方法,梯度预处理,Nystrom方法,深度学习

TL;DR: NysAct是一种可扩展的第一阶梯度预处理方法，通过结合第一阶和第二阶优化方法的优势，显著降低了计算和内存成本，同时在测试精度上表现更优。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 自适应梯度方法计算效率高但泛化性差，而二阶方法泛化性好但计算和内存成本高。NysAct的目标是找到二者的平衡点。

Method: NysAct利用特征值位移的Nystrom方法近似激活协方差矩阵作为预处理矩阵，显著降低了时间和内存复杂度。

Result: 实验表明，NysAct在测试精度上优于第一阶和第二阶方法，同时计算资源需求明显低于现有的二阶方法。

Conclusion: NysAct是一种高效且泛化性强的优化方法，适用于大规模深度学习任务。

Abstract: Adaptive gradient methods are computationally efficient and converge quickly,
but they often suffer from poor generalization. In contrast, second-order
methods enhance convergence and generalization but typically incur high
computational and memory costs. In this work, we introduce NysAct, a scalable
first-order gradient preconditioning method that strikes a balance between
state-of-the-art first-order and second-order optimization methods. NysAct
leverages an eigenvalue-shifted Nystrom method to approximate the activation
covariance matrix, which is used as a preconditioning matrix, significantly
reducing time and memory complexities with minimal impact on test accuracy. Our
experiments show that NysAct not only achieves improved test accuracy compared
to both first-order and second-order methods but also demands considerably less
computational resources than existing second-order methods. Code is available
at https://github.com/hseung88/nysact.

</details>


### [131] [AlphaFold Database Debiasing for Robust Inverse Folding](https://arxiv.org/abs/2506.08365)
*Cheng Tan,Zhenxiao Cao,Zhangyang Gao,Siyuan Li,Yufei Huang,Stan Z. Li*

Key words: AlphaFold, 结构偏差, 逆折叠, 深度学习, 蛋白质设计

TL;DR: AlphaFold数据库虽提供高精度蛋白质结构，但其几何偏差影响深度学习模型训练。论文提出Debiasing Structure AutoEncoder (DeSAE)以减少偏差，提升任务表现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: AlphaFold数据库的预测结构存在系统性几何偏差，需解决以提升模型在逆折叠等任务中的表现。

Method: 提出DeSAE模型，通过从故意损坏的几何结构中重建更自然的构象来消除偏差。

Result: DeSAE处理的AlphaFold结构显著提升了逆折叠任务在多个基准测试中的性能。

Conclusion: 系统性偏差对结构预测任务有重要影响，DeSAE提供了一种有效的去偏方法。

Abstract: The AlphaFold Protein Structure Database (AFDB) offers unparalleled
structural coverage at near-experimental accuracy, positioning it as a valuable
resource for data-driven protein design. However, its direct use in training
deep models that are sensitive to fine-grained atomic geometry, such as inverse
folding, exposes a critical limitation. Comparative analysis of structural
feature distributions reveals that AFDB structures exhibit distinct statistical
regularities, reflecting a systematic geometric bias that deviates from the
conformational diversity found in experimentally determined structures from the
Protein Data Bank (PDB). While AFDB structures are cleaner and more idealized,
PDB structures capture the intrinsic variability and physical realism essential
for generalization in downstream tasks. To address this discrepancy, we
introduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct
native-like conformations from intentionally corrupted backbone geometries. By
training the model to recover plausible structural states, DeSAE implicitly
captures a more robust and natural structural manifold. At inference, applying
DeSAE to AFDB structures produces debiased structures that significantly
improve inverse folding performance across multiple benchmarks. This work
highlights the critical impact of subtle systematic biases in predicted
structures and presents a principled framework for debiasing, significantly
boosting the performance of structure-based learning tasks like inverse
folding.

</details>


### [132] [Reinforce LLM Reasoning through Multi-Agent Reflection](https://arxiv.org/abs/2506.08379)
*Yurun Yuan,Tengyang Xie*

Key words: 大语言模型、强化学习、马尔可夫决策过程、动态规划、推理能力

TL;DR: 本文提出了一种名为DPSDP的强化学习算法，通过建模多轮推理过程为马尔可夫决策过程，优化了LLMs的推理能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法在处理动态解探索和反馈整合时受限于反馈空间和缺乏协同训练，导致性能不佳。

Method: 采用DPSDP算法，通过直接偏好学习在自生成数据上训练Actor-Critic LLM系统。

Result: 在多个基准测试中表现提升，例如在MATH 500上，准确率从58.2%提升至63.2%。

Conclusion: DPSDP证明了多智能体协作和分布外泛化的优势。

Abstract: Leveraging more test-time computation has proven to be an effective way to
boost the reasoning capabilities of large language models (LLMs). Among various
methods, the verify-and-improve paradigm stands out for enabling dynamic
solution exploration and feedback incorporation. However, existing approaches
often suffer from restricted feedback spaces and lack of coordinated training
of different parties, leading to suboptimal performance. To address this, we
model this multi-turn refinement process as a Markov Decision Process and
introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement
learning algorithm that trains an actor-critic LLM system to iteratively refine
answers via direct preference learning on self-generated data. Theoretically,
DPSDP can match the performance of any policy within the training distribution.
Empirically, we instantiate DPSDP with various base models and show
improvements on both in- and out-of-distribution benchmarks. For example, on
benchmark MATH 500, majority voting over five refinement steps increases
first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An
ablation study further confirms the benefits of multi-agent collaboration and
out-of-distribution generalization.

</details>


### [133] [Network Threat Detection: Addressing Class Imbalanced Data with Deep Forest](https://arxiv.org/abs/2506.08383)
*Jiaqi Chen,Rongbin Ye*

Key words: 物联网(IoT), 恶意流量检测, 机器学习, 数据不平衡, gcForest

TL;DR: 该论文提出一种基于机器学习的恶意流量检测方法，针对IoT环境中的恶意攻击，通过不平衡数据处理和集成学习方法（如gcForest）提升检测性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着物联网网络的快速扩展，实时检测恶意流量成为重要的网络安全挑战，研究旨在解决这一问题。

Method: 使用IoT-23数据集，比较多种机器学习技术，并引入三种重采样策略处理数据不平衡问题。

Result: 结合不平衡数据处理和集成学习方法（如gcForest）显著提升了检测性能。

Conclusion: 研究为物联网环境中的智能高效威胁检测系统提供了重要贡献，优化了计算资源使用。

Abstract: With the rapid expansion of Internet of Things (IoT) networks, detecting
malicious traffic in real-time has become a critical cybersecurity challenge.
This research addresses the detection challenges by presenting a comprehensive
empirical analysis of machine learning techniques for malware detection using
the IoT-23 dataset provided by the Stratosphere Laboratory. We address the
significant class imbalance within the dataset through three resampling
strategies. We implement and compare a few machine learning techniques. Our
findings demonstrate that the combination of appropriate imbalance treatment
techniques with ensemble methods, particularly gcForest, achieves better
detection performance compared to traditional approaches. This work contributes
significantly to the development of more intelligent and efficient automated
threat detection systems for IoT environments, helping to secure critical
infrastructure against sophisticated cyber attacks while optimizing
computational resource usage.

</details>


### [134] [Reinforcement Learning Teachers of Test Time Scaling](https://arxiv.org/abs/2506.08388)
*Edoardo Cetin,Tianyu Zhao,Yujin Tang*

Key words: 语言模型, 强化学习, 蒸馏, 教学模型, 零样本学习

TL;DR: 论文提出了一种新的框架Reinforcement-Learned Teachers (RLTs)，通过详细解释问题与解决方案来训练语言模型，从而避免传统强化学习的探索难题，并在下游蒸馏中实现更高效的表现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统强化学习依赖语言模型在初始化时能通过探索解决问题，而RLTs的目标是作为教师模型，专注于生成最适合学生模型学习的解释，提升蒸馏效率和性能。

Method: RLTs通过接收问题和解决方案，生成详细解释（"connect-the-dots"），并使用密集奖励（通过学生模型理解测试）进行训练。

Result: 实验表明，7B参数的RLT在多项任务上的表现优于现有蒸馏方法，且能适用于更大规模的学生模型和零样本任务。

Conclusion: RLTs为强化学习推理框架提供了高效性和可重用性，展示了在语言模型教学中的潜力。

Abstract: Training reasoning language models (LMs) with reinforcement learning (RL) for
one-hot correctness inherently relies on the LM being able to explore and solve
its task with some chance at initialization. Furthermore, a key use case of
reasoning LMs is to act as teachers for distilling new students and
cold-starting future RL iterations rather than being deployed themselves. From
these considerations, we introduce a new framework that avoids RL's exploration
challenge by training a new class of Reinforcement-Learned Teachers (RLTs)
focused on yielding the most effective downstream distillation. RLTs are
prompted with both the question and solution to each problem, and tasked to
simply "connect-the-dots" with detailed explanations tailored for their
students. We train RLTs with dense rewards obtained by feeding each explanation
to the student and testing its understanding of the problem's solution. In
practice, the raw outputs of a 7B RLT provide higher final performance on
competition and graduate-level tasks than existing distillation and
cold-starting pipelines that collect and postprocess the reasoning traces of
orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness
when training larger students and when applied zero-shot to out-of-distribution
tasks, unlocking new levels of efficiency and re-usability for the RL reasoning
framework.

</details>


### [135] [Spatiotemporal deep learning models for detection of rapid intensification in cyclones](https://arxiv.org/abs/2506.08397)
*Vamshika Sutar,Amandeep Singh,Rohitash Chandra*

Key words: 气旋快速增强, 深度学习, 数据增强, 类别不平衡, 空间坐标

TL;DR: 研究通过深度学习和数据增强技术解决气旋快速增强检测中的类别不平衡问题，并证实空间坐标作为输入特征的重要性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 气旋快速增强（RI）是罕见但极端的事件，导致数据集类别不平衡，传统机器学习方法难以处理。

Method: 结合深度学习、集成学习和数据增强框架，利用深度学习生成模拟RI事件的数据以解决类别不平衡问题。

Result: 数据增强显著改善了RI检测效果，空间坐标作为输入特征起到了关键作用。

Conclusion: 研究为时空数据中极端事件的合成数据生成提供了新思路。

Abstract: Cyclone rapid intensification is the rapid increase in cyclone wind
intensity, exceeding a threshold of 30 knots, within 24 hours. Rapid
intensification is considered an extreme event during a cyclone, and its
occurrence is relatively rare, contributing to a class imbalance in the
dataset. A diverse array of factors influences the likelihood of a cyclone
undergoing rapid intensification, further complicating the task for
conventional machine learning models. In this paper, we evaluate deep learning,
ensemble learning and data augmentation frameworks to detect cyclone rapid
intensification based on wind intensity and spatial coordinates. We note that
conventional data augmentation methods cannot be utilised for generating
spatiotemporal patterns replicating cyclones that undergo rapid
intensification. Therefore, our framework employs deep learning models to
generate spatial coordinates and wind intensity that replicate cyclones to
address the class imbalance problem of rapid intensification. We also use a
deep learning model for the classification module within the data augmentation
framework to differentiate between rapid and non-rapid intensification events
during a cyclone. Our results show that data augmentation improves the results
for rapid intensification detection in cyclones, and spatial coordinates play a
critical role as input features to the given models. This paves the way for
research in synthetic data generation for spatiotemporal data with extreme
events.

</details>


### [136] [FUSE: Measure-Theoretic Compact Fuzzy Set Representation for Taxonomy Expansion](https://arxiv.org/abs/2506.08409)
*Fred Xu,Song Jiang,Zijie Huang,Xiao Luo,Shichang Zhang,Adrian Chen,Yizhou Sun*

Key words: 集合表示学习, 模糊集, 分类扩展, FUSE

TL;DR: 该论文提出了一种基于模糊集的集合表示学习方法FUSE，用于解决分类扩展问题，并在效率和信息保留方面表现出色。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法通常将集合建模为向量或几何对象，但这些方法在集合操作下不封闭。为了解决这一问题，作者提出了基于模糊集的集合表示学习框架，以更好地建模复杂概念及其关系。

Method: 作者提出了Fuzzy Set Embedding (FUSE)框架，通过体积近似模糊集来实现集合表示学习，确保了集合操作的封闭性，并依赖最小神经网络架构进行高效学习。

Result: 在分类扩展任务中，FUSE相比现有基线方法实现了高达23%的显著改进。

Conclusion: FUSE是首个尝试理解和高效计算模糊集嵌入的工作，为集合表示学习提供了新的方向。

Abstract: Taxonomy Expansion, which models complex concepts and their relations, can be
formulated as a set representation learning task. The generalization of set,
fuzzy set, incorporates uncertainty and measures the information within a
semantic concept, making it suitable for concept modeling. Existing works
usually model sets as vectors or geometric objects such as boxes, which are not
closed under set operations. In this work, we propose a sound and efficient
formulation of set representation learning based on its volume approximation as
a fuzzy set. The resulting embedding framework, Fuzzy Set Embedding (FUSE),
satisfies all set operations and compactly approximates the underlying fuzzy
set, hence preserving information while being efficient to learn, relying on
minimum neural architecture. We empirically demonstrate the power of FUSE on
the task of taxonomy expansion, where FUSE achieves remarkable improvements up
to 23% compared with existing baselines. Our work marks the first attempt to
understand and efficiently compute the embeddings of fuzzy sets.

</details>


### [137] [Learning to Hear Broken Motors: Signature-Guided Data Augmentation for Induction-Motor Diagnostics](https://arxiv.org/abs/2506.08412)
*Saraa Ali,Aleksandr Khizhik,Stepan Svirin,Artem Ryzhikov,Denis Derkach*

Key words: 机器学习, 电机诊断, 无监督学习, SGDA, 签名分析

TL;DR: 论文提出了一种结合机器学习和无监督异常生成的方法（SGDA），用于三相电机的智能诊断，提升了诊断性能和准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统方法依赖签名分析，但结合先进的机器学习技术可以进一步提升诊断能力。

Method: 提出Signature-Guided Data Augmentation (SGDA)，一种无监督框架，通过频率域的物理模型生成异常信号，结合监督学习和无监督签名分析。

Result: 方法实现了更高的诊断准确性和可靠性，并具有广泛的工业应用潜力。

Conclusion: 该研究为电机诊断领域提供了高效且稳健的解决方案。

Abstract: The application of machine learning (ML) algorithms in the intelligent
diagnosis of three-phase engines has the potential to significantly enhance
diagnostic performance and accuracy. Traditional methods largely rely on
signature analysis, which, despite being a standard practice, can benefit from
the integration of advanced ML techniques. In our study, we innovate by
combining ML algorithms with a novel unsupervised anomaly generation
methodology that takes into account the engine physics model. We propose
Signature-Guided Data Augmentation (SGDA), an unsupervised framework that
synthesizes physically plausible faults directly in the frequency domain of
healthy current signals. Guided by Motor Current Signature Analysis, SGDA
creates diverse and realistic anomalies without resorting to computationally
intensive simulations. This hybrid approach leverages the strengths of both
supervised ML and unsupervised signature analysis, achieving superior
diagnostic accuracy and reliability along with wide industrial application. The
findings highlight the potential of our approach to contribute significantly to
the field of engine diagnostics, offering a robust and efficient solution for
real-world applications.

</details>


### [138] [Improved Scaling Laws in Linear Regression via Data Reuse](https://arxiv.org/abs/2506.08415)
*Licong Lin,Jingfeng Wu,Peter L. Bartlett*

Key words: 神经缩放定律, 线性回归, 多轮SGD, 数据重用, 测试误差

TL;DR: 摘要探讨了数据重用对线性回归中神经缩放定律的改进作用，通过多轮随机梯度下降（SGD）提升了测试误差的缩放效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前神经缩放定律表明，随着模型和数据规模的增加，测试误差呈多项式下降，但在新数据不足时难以持续。希望通过数据重用改进现有缩放定律。

Method: 研究了在多轮SGD训练下，$M$维线性模型在$N$个数据上的测试误差界限，假设数据协方差和真实参数具有特定的幂律谱。

Result: 多轮SGD实现了$/Theta(M^{1-b} + L^{(1-b)/a})$的测试误差，优于单轮SGD的$/Theta(M^{1-b} + N^{(1-b)/a})$，展示了数据重用的优势。

Conclusion: 数据重用能显著提升数据受限情况下的缩放效率，数值模拟验证了理论结果。

Abstract: Neural scaling laws suggest that the test error of large language models
trained online decreases polynomially as the model size and data size increase.
However, such scaling can be unsustainable when running out of new data. In
this work, we show that data reuse can improve existing scaling laws in linear
regression. Specifically, we derive sharp test error bounds on $M$-dimensional
linear models trained by multi-pass stochastic gradient descent (multi-pass
SGD) on $N$ data with sketched features. Assuming that the data covariance has
a power-law spectrum of degree $a$, and that the true parameter follows a prior
with an aligned power-law spectrum of degree $b-a$ (with $a > b > 1$), we show
that multi-pass SGD achieves a test error of $\Theta(M^{1-b} + L^{(1-b)/a})$,
where $L \lesssim N^{a/b}$ is the number of iterations. In the same setting,
one-pass SGD only attains a test error of $\Theta(M^{1-b} + N^{(1-b)/a})$ (see
e.g., Lin et al., 2024). This suggests an improved scaling law via data reuse
(i.e., choosing $L>N$) in data-constrained regimes. Numerical simulations are
also provided to verify our theoretical findings.

</details>


### [139] [Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood](https://arxiv.org/abs/2506.08417)
*Qingmao Yao,Zhichao Lei,Tianyuan Chen,Ziyue Yuan,Xuefan Chen,Jianxiang Liu,Faguo Wu,Xiao Zhang*

Key words: 离线强化学习、Q值估计、OOD动作、凸包、平滑Bellman算子

TL;DR: 提出了SQOG方法，通过平滑Bellman算子在凸包及其邻域提升Q值估计，解决离线RL中分布偏移导致的Q值高估问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 离线强化学习中分布偏移导致Q值高估问题，现有方法因过度保守限制Q函数泛化。

Method: 提出平滑Bellman算子(SBO)，在凸包及其邻域内平滑更新OOD Q值。

Result: SQOG在D4RL基准测试中表现优于现有方法，计算效率更高。

Conclusion: SQOG通过提升Q函数泛化能力，有效解决了离线RL中的Q值估计问题。

Abstract: Offline Reinforcement Learning (RL) struggles with distributional shifts,
leading to the $Q$-value overestimation for out-of-distribution (OOD) actions.
Existing methods address this issue by imposing constraints; however, they
often become overly conservative when evaluating OOD regions, which constrains
the $Q$-function generalization. This over-constraint issue results in poor
$Q$-value estimation and hinders policy improvement. In this paper, we
introduce a novel approach to achieve better $Q$-value estimation by enhancing
$Q$-function generalization in OOD regions within Convex Hull and its
Neighborhood (CHN). Under the safety generalization guarantees of the CHN, we
propose the Smooth Bellman Operator (SBO), which updates OOD $Q$-values by
smoothing them with neighboring in-sample $Q$-values. We theoretically show
that SBO approximates true $Q$-values for both in-sample and OOD actions within
the CHN. Our practical algorithm, Smooth Q-function OOD Generalization (SQOG),
empirically alleviates the over-constraint issue, achieving near-accurate
$Q$-value estimation. On the D4RL benchmarks, SQOG outperforms existing
state-of-the-art methods in both performance and computational efficiency.

</details>


### [140] [Online Learning-guided Learning Rate Adaptation via Gradient Alignment](https://arxiv.org/abs/2506.08419)
*Ruichen Jiang,Ali Kavis,Aryan Mokhtari*

Key words: 优化器, 学习率调整, GALA, 深度学习, 梯度对齐

TL;DR: GALA框架通过动态调整学习率，提高优化器在大规模深度学习模型中的性能，避免了繁琐的网格搜索。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为避免传统优化器中学习率调参的繁琐网格搜索，提出一种动态调整学习率的框架GALA。

Method: GALA通过追踪连续梯度的对齐度和局部曲率估计动态调整学习率，并将其转化为一维在线学习问题。

Result: GALA在平滑非凸设置中为归一化SGD提供数据自适应的收敛率，实验表明其与常见优化器如SGD和Adam结合时表现鲁棒且无需调参。

Conclusion: GALA是一种高效且自适应的学习率调整方法，显著减少了对超参数调优的依赖。

Abstract: The performance of an optimizer on large-scale deep learning models depends
critically on fine-tuning the learning rate, often requiring an extensive grid
search over base learning rates, schedules, and other hyperparameters. In this
paper, we propose a principled framework called GALA (Gradient Alignment-based
Learning rate Adaptation), which dynamically adjusts the learning rate by
tracking the alignment between consecutive gradients and using a local
curvature estimate. Guided by the convergence analysis, we formulate the
problem of selecting the learning rate as a one-dimensional online learning
problem. When paired with an online learning algorithm such as
Follow-the-Regularized-Leader, our method produces a flexible, adaptive
learning rate schedule that tends to increase when consecutive gradients are
aligned and decrease otherwise. We establish a data-adaptive convergence rate
for normalized SGD equipped with GALA in the smooth, nonconvex setting.
Empirically, common optimizers such as SGD and Adam, when augmented with GALA,
demonstrate robust performance across a wide range of initial learning rates
and perform competitively without the need for tuning.

</details>


### [141] [HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems](https://arxiv.org/abs/2506.08426)
*Zheng Lin,Zhe Chen,Xianhao Chen,Wei Ni,Yue Gao*

Key words: Split federated learning, heterogenity, batch size, model splitting, edge networks

TL;DR: Split federated learning (SFL) 因异构边缘设备的能力差异存在严重的拖尾效应，本文提出自适应控制批次大小 (BS) 和模型分割 (MS) 的解决方案。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决由于边缘设备异构性导致的 SFL 训练效率低下的问题。

Method: 通过推导 SFL 的收敛边界，量化 BS 和 MS 对性能的影响，并设计异构感知的框架 HASFL，自适应控制 BS 和 MS。

Result: 实验验证了 HASFL 的有效性，并展示了其在性能上的优越性。

Conclusion: HASFL 能在异构边缘网络中平衡通信、计算延迟与训练收敛性。

Abstract: Split federated learning (SFL) has emerged as a promising paradigm to
democratize machine learning (ML) on edge devices by enabling layer-wise model
partitioning. However, existing SFL approaches suffer significantly from the
straggler effect due to the heterogeneous capabilities of edge devices. To
address the fundamental challenge, we propose adaptively controlling batch
sizes (BSs) and model splitting (MS) for edge devices to overcome resource
heterogeneity. We first derive a tight convergence bound of SFL that quantifies
the impact of varied BSs and MS on learning performance. Based on the
convergence bound, we propose HASFL, a heterogeneity-aware SFL framework
capable of adaptively controlling BS and MS to balance communication-computing
latency and training convergence in heterogeneous edge networks. Extensive
experiments with various datasets validate the effectiveness of HASFL and
demonstrate its superiority over state-of-the-art benchmarks.

</details>


### [142] [Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings](https://arxiv.org/abs/2506.08435)
*Mingyuan Fan,Fuyi Wang,Cen Chen,Jianying Zhou*

Key words: 联邦学习, 梯度泄漏攻击, 隐私保护, FedLeak, 数据重构

TL;DR: 本文通过实证研究发现，即使在现实的联邦学习环境中，客户端数据仍可能被梯度泄漏攻击（GLA）有效地重构，并提出新方法FedLeak以提升攻击效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前关于联邦学习（FL）隐私保护的争论集中在梯度泄漏攻击的可行性上，许多研究认为其仅在过于宽松的条件下有效。本文旨在填补这一研究空白，证明在现实环境中数据重构仍可能发生。

Method: 提出FedLeak方法，包含两项新技术：部分梯度匹配和梯度正则化。同时设计了一个基于FL文献和行业实践的评估协议，以验证FedLeak在实际环境中的表现。

Result: 在现实FL环境中，FedLeak能够实现高保真度的数据重构，揭示了FL系统的严重漏洞。

Conclusion: 研究结果表明，FL系统存在重大隐私风险，亟需更有效的防御方法。

Abstract: Federated learning (FL) enables collaborative model training among multiple
clients without the need to expose raw data. Its ability to safeguard privacy,
at the heart of FL, has recently been a hot-button debate topic. To elaborate,
several studies have introduced a type of attacks known as gradient leakage
attacks (GLAs), which exploit the gradients shared during training to
reconstruct clients' raw data. On the flip side, some literature, however,
contends no substantial privacy risk in practical FL environments due to the
effectiveness of such GLAs being limited to overly relaxed conditions, such as
small batch sizes and knowledge of clients' data distributions.
  This paper bridges this critical gap by empirically demonstrating that
clients' data can still be effectively reconstructed, even within realistic FL
environments. Upon revisiting GLAs, we recognize that their performance
failures stem from their inability to handle the gradient matching problem. To
alleviate the performance bottlenecks identified above, we develop FedLeak,
which introduces two novel techniques, partial gradient matching and gradient
regularization. Moreover, to evaluate the performance of FedLeak in real-world
FL environments, we formulate a practical evaluation protocol grounded in a
thorough review of extensive FL literature and industry practices. Under this
protocol, FedLeak can still achieve high-fidelity data reconstruction, thereby
underscoring the significant vulnerability in FL systems and the urgent need
for more effective defense methods.

</details>


### [143] [Learning to Lead: Incentivizing Strategic Agents in the Dark](https://arxiv.org/abs/2506.08438)
*Yuchen Wu,Xinyi Zhong,Zhuoran Yang*

Key words: 在线学习, 委托-代理模型, 战略代理, 私有类型, 奖励估计, 遗憾界

TL;DR: 论文提出了一种在广义委托-代理模型中在线学习的方法，解决了战略代理的非近视行为和报告类型的问题，实现了样本高效的学习算法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究在存在战略代理（具有私有类型和奖励）的情境下，委托人如何在在线学习中优化协调机制并最小化战略遗憾。

Method: 结合了延迟机制、奖励角度估计框架（利用扇形测试和匹配过程）以及悲观-乐观的LinUCB算法，以实现高效学习和激励约束的平衡。

Result: 提出了首个可证明样本高效的算法，并获得了接近最优的$	ilde{O}(\sqrt{T})$遗憾界。

Conclusion: 该方法为涉及私有类型和战略代理的博弈论场景提供了鲁棒的在线学习算法设计新思路。

Abstract: We study an online learning version of the generalized principal-agent model,
where a principal interacts repeatedly with a strategic agent possessing
private types, private rewards, and taking unobservable actions. The agent is
non-myopic, optimizing a discounted sum of future rewards and may strategically
misreport types to manipulate the principal's learning. The principal,
observing only her own realized rewards and the agent's reported types, aims to
learn an optimal coordination mechanism that minimizes strategic regret. We
develop the first provably sample-efficient algorithm for this challenging
setting. Our approach features a novel pipeline that combines (i) a delaying
mechanism to incentivize approximately myopic agent behavior, (ii) an
innovative reward angle estimation framework that uses sector tests and a
matching procedure to recover type-dependent reward functions, and (iii) a
pessimistic-optimistic LinUCB algorithm that enables the principal to explore
efficiently while respecting the agent's incentive constraints. We establish a
near optimal $\tilde{O}(\sqrt{T}) $ regret bound for learning the principal's
optimal policy, where $\tilde{O}(\cdot) $ omits logarithmic factors. Our
results open up new avenues for designing robust online learning algorithms for
a wide range of game-theoretic settings involving private types and strategic
agents.

</details>


### [144] [Time-Aware World Model for Adaptive Prediction and Control](https://arxiv.org/abs/2506.08441)
*Anh N. Nhu,Sanghyun Son,Ming Lin*

Key words: 时间感知世界模型, 模型训练, 控制任务, 数据效率

TL;DR: 本文提出了时间感知世界模型（TAWM），通过显式结合时间动态性，利用不同时间步长训练模型，提升任务性能和数据效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统模型通常固定时间步长，而实际系统的动态性要求不同的采样率，因此需要一种能够适应不同时间步长的模型。

Method: TAWM通过条件化时间步长Δt，并在多样化的Δt值上训练，捕获任务的高频和低频动态性。

Result: 实验表明，TAWM在不同观测率和控制任务中均优于传统模型，且使用相同的训练样本和迭代次数。

Conclusion: 时间感知的建模方法在性能和数据效率上均有显著提升，适用于多样的控制问题。

Abstract: In this work, we introduce the Time-Aware World Model (TAWM), a model-based
approach that explicitly incorporates temporal dynamics. By conditioning on the
time-step size, {\Delta}t, and training over a diverse range of {\Delta}t
values -- rather than sampling at a fixed time-step -- TAWM learns both high-
and low-frequency task dynamics across diverse control problems. Grounded in
the information-theoretic insight that the optimal sampling rate depends on a
system's underlying dynamics, this time-aware formulation improves both
performance and data efficiency. Empirical evaluations show that TAWM
consistently outperforms conventional models across varying observation rates
in a variety of control tasks, using the same number of training samples and
iterations. Our code can be found online at:
github.com/anh-nn01/Time-Aware-World-Model.

</details>


### [145] [MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning](https://arxiv.org/abs/2506.08460)
*Yihong Guo,Yu Yang,Pan Xu,Anqi Liu*

Key words: 离线强化学习，跨域动态，模型生成，数据增强

TL;DR: MOBODY是一个基于模型的离线强化学习算法，通过生成目标域的新过渡数据来解决源域与目标域动态不匹配的问题，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法在处理源域与目标域动态不匹配时，受限于目标域的有限数据，无法探索目标域。

Method: MOBODY通过学习共享潜在表示和模型生成的过渡数据，结合Q加权行为克隆损失，优化策略。

Result: 在MuJoCo基准测试中，MOBODY显著优于现有基线，尤其在挑战性场景中表现突出。

Conclusion: MOBODY通过动态学习和数据增强，有效解决了跨域离线强化学习的动态不匹配问题。

Abstract: We study the off-dynamics offline reinforcement learning problem, where the
goal is to learn a policy from offline datasets collected from source and
target domains with mismatched transition. Existing off-dynamics offline RL
methods typically either filter source transitions that resemble those of the
target domain or apply reward augmentation to source data, both constrained by
the limited transitions available from the target domain. As a result, the
learned policy is unable to explore target domain beyond the offline datasets.
We propose MOBODY, a Model-Based Off-Dynamics offline RL algorithm that
addresses this limitation by enabling exploration of the target domain via
learned dynamics. MOBODY generates new synthetic transitions in the target
domain through model rollouts, which are used as data augmentation during
offline policy learning. Unlike existing model-based methods that learn
dynamics from a single domain, MOBODY tackles the challenge of mismatched
dynamics by leveraging both source and target datasets. Directly merging these
datasets can bias the learned model toward source dynamics. Instead, MOBODY
learns target dynamics by discovering a shared latent representation of states
and transitions across domains through representation learning. To stabilize
training, MOBODY incorporates a behavior cloning loss that regularizes the
policy. Specifically, we introduce a Q-weighted behavior cloning loss that
regularizes the policy toward actions with high target-domain Q-values, rather
than uniformly imitating all actions in the dataset. These Q-values are learned
from an enhanced target dataset composed of offline target data, augmented
source data, and rollout data from the learned target dynamics. We evaluate
MOBODY on MuJoCo benchmarks and show that it significantly outperforms
state-of-the-art baselines, with especially pronounced improvements in
challenging scenarios.

</details>


### [146] [How to Provably Improve Return Conditioned Supervised Learning?](https://arxiv.org/abs/2506.08463)
*Zhishuai Liu,Yu Yang,Ruhan Wang,Pan Xu,Dongruo Zhou*

Key words: RCSL, 离线强化学习, 分布内最优, 决策问题

TL;DR: Reinforced RCSL 是一个改进的 RCSL 框架，通过引入分布内最优 return-to-go 避免了复杂的数据增强，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: RCSL 在离线强化学习中因缺乏拼接能力而受限，本文旨在解决这一问题。

Method: 提出了 Reinforced RCSL 框架，引入分布内最优 return-to-go 以优化策略学习。

Result: 理论和实验表明，Reinforced RCSL 在多种基准测试中表现优于标准 RCSL。

Conclusion: Reinforced RCSL 是一种简单且高效的方法，显著提升了离线决策任务的性能。

Abstract: In sequential decision-making problems, Return-Conditioned Supervised
Learning (RCSL) has gained increasing recognition for its simplicity and
stability in modern decision-making tasks. Unlike traditional offline
reinforcement learning (RL) algorithms, RCSL frames policy learning as a
supervised learning problem by taking both the state and return as input. This
approach eliminates the instability often associated with temporal difference
(TD) learning in offline RL. However, RCSL has been criticized for lacking the
stitching property, meaning its performance is inherently limited by the
quality of the policy used to generate the offline dataset. To address this
limitation, we propose a principled and simple framework called Reinforced
RCSL. The key innovation of our framework is the introduction of a concept we
call the in-distribution optimal return-to-go. This mechanism leverages our
policy to identify the best achievable in-dataset future return based on the
current state, avoiding the need for complex return augmentation techniques.
Our theoretical analysis demonstrates that Reinforced RCSL can consistently
outperform the standard RCSL approach. Empirical results further validate our
claims, showing significant performance improvements across a range of
benchmarks.

</details>


### [147] [MAC: An Efficient Gradient Preconditioning using Mean Activation Approximated Curvature](https://arxiv.org/abs/2506.08464)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Key words: 二阶优化, Fisher信息矩阵, Kronecker分解, MAC算法, Transformer

TL;DR: 论文提出了一种名为MAC的高效二阶优化方法，通过分析KFAC中Fisher信息矩阵的Kronecker因子，提出了近似方法，显著降低了计算负担，并在多项指标上优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 二阶优化方法（如KFAC）在训练神经网络时表现优异，但计算成本高。本文旨在解决这一瓶颈。

Method: 通过分析KFAC中Fisher信息矩阵的两个Kronecker因子（激活和预激活梯度），基于其频谱特性提出高效近似方法，形成MAC算法。

Result: MAC在精度、训练时间和内存消耗上优于KFAC及其他先进方法，并首次将Kronecker分解应用于Transformer的注意力层。

Conclusion: MAC是一种高效且通用的二阶优化方法，适用于多种网络架构，并具备理论收敛保证。

Abstract: Second-order optimization methods for training neural networks, such as KFAC,
exhibit superior convergence by utilizing curvature information of loss
landscape. However, it comes at the expense of high computational burden. In
this work, we analyze the two components that constitute the layer-wise Fisher
information matrix (FIM) used in KFAC: the Kronecker factors related to
activations and pre-activation gradients. Based on empirical observations on
their eigenspectra, we propose efficient approximations for them, resulting in
a computationally efficient optimization method called MAC. To the best of our
knowledge, MAC is the first algorithm to apply the Kronecker factorization to
the FIM of attention layers used in transformers and explicitly integrate
attention scores into the preconditioning. We also study the convergence
property of MAC on nonlinear neural networks and provide two conditions under
which it converges to global minima. Our extensive evaluations on various
network architectures and datasets show that the proposed method outperforms
KFAC and other state-of-the-art methods in terms of accuracy, end-to-end
training time, and memory usage. Code is available at
https://github.com/hseung88/mac.

</details>


### [148] [AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin](https://arxiv.org/abs/2506.08473)
*Shuo Yang,Qihui Zhang,Yuyang Liu,Yue Huang,Xiaojun Jia,Kunpeng Ning,Jiayu Yao,Jigang Wang,Hailiang Dai,Yibing Song,Li Yuan*

Key words: 大语言模型, 安全微调, 对齐方向, AsFT, 正则化

TL;DR: 论文提出了一种名为AsFT的安全微调方法，通过正则化项抑制有害方向的更新，以减少大语言模型在微调过程中的安全风险。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大语言模型在微调时容易受到安全风险的威胁，少量恶意或无害数据可能破坏模型的安全性。

Method: 基于对齐方向的概念，提出AsFT方法，通过正则化项约束微调过程在安全区域内。

Result: 实验表明，AsFT优于Safe LoRA，减少了7.60%的有害行为，提升了3.44%的模型性能。

Conclusion: AsFT能有效抑制有害方向上的更新，确保模型微调在狭窄的安全区域内进行。

Abstract: Large language models (LLMs) are vulnerable to safety risks during
fine-tuning, where small amounts of malicious or harmless data can compromise
safeguards. In this paper, building on the concept of alignment direction --
defined by the weight difference between aligned and unaligned models -- we
observe that perturbations along this direction preserve model safety. In
contrast, perturbations along directions orthogonal to this alignment are
strongly linked to harmful direction perturbations, rapidly degrading safety
and framing the parameter space as a narrow safety basin. Based on this
insight, we propose a methodology for safety fine-tuning called AsFT (Anchoring
Safety in Fine-Tuning), which integrates a regularization term into the
training objective. This term uses the alignment direction as an anchor to
suppress updates in harmful directions, ensuring that fine-tuning is
constrained within the narrow safety basin. Extensive experiments on multiple
datasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by
7.60 percent, improving model performance by 3.44 percent, and maintaining
robust performance across various experimental settings. Code is available at
https://github.com/PKU-YuanGroup/AsFT

</details>


### [149] [Thermodynamically Consistent Latent Dynamics Identification for Parametric Systems](https://arxiv.org/abs/2506.08475)
*Xiaolong He,Yeonjong Shin,Anthony Gruber,Sohyeon Jung,Kookjin Lee,Youngsoo Choi*

Key words: 热力学驱动, 潜在空间动力学, 参数化非线性系统, GENERIC结构, 主动学习

TL;DR: 提出了一种高效的热力学驱动的潜在空间动力学识别框架，用于参数非线性动力学系统的降阶建模，结合了自动编码器和新的参数化GENERIC结构神经网络，保持热力学原理并显著提升性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决参数非线性动力学系统的高效建模问题，同时保持关键热力学原理。

Method: 结合自动编码器进行降维和参数化GENERIC结构神经网络（pGFINNs），并采用物理驱动的主动学习策略自适应采样数据。

Result: 在Burgers方程和1D/1V Vlasov-Poisson方程实验中，实现了高达3,528倍的加速和1-3%的相对误差，训练和推理成本显著降低。

Conclusion: 该框架不仅高效且准确，还能揭示系统的热力学行为，为物理空间动力学提供新见解。

Abstract: We propose an efficient thermodynamics-informed latent space dynamics
identification (tLaSDI) framework for the reduced-order modeling of parametric
nonlinear dynamical systems. This framework integrates autoencoders for
dimensionality reduction with newly developed parametric GENERIC
formalism-informed neural networks (pGFINNs), which enable efficient learning
of parametric latent dynamics while preserving key thermodynamic principles
such as free energy conservation and entropy generation across the parameter
space. To further enhance model performance, a physics-informed active learning
strategy is incorporated, leveraging a greedy, residual-based error indicator
to adaptively sample informative training data, outperforming uniform sampling
at equivalent computational cost. Numerical experiments on the Burgers'
equation and the 1D/1V Vlasov-Poisson equation demonstrate that the proposed
method achieves up to 3,528x speed-up with 1-3% relative errors, and
significant reduction in training (50-90%) and inference (57-61%) cost.
Moreover, the learned latent space dynamics reveal the underlying thermodynamic
behavior of the system, offering valuable insights into the physical-space
dynamics.

</details>


### [150] [Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations](https://arxiv.org/abs/2506.08505)
*Shahaf Bassan,Yizhak Yisrael Elboher,Tobias Ladner,Matthias Althoff,Guy Katz*

Key words: 神经网络解释，形式化保证，抽象-细化技术，可扩展性，验证

TL;DR: 提出了一种新的抽象-细化技术，用于高效计算神经网络预测的可证明充分解释，通过构建一个显著简化的网络来加速验证过程。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前的后设解释方法依赖启发式且缺乏形式化保证，尽管已有研究通过神经网络验证技术提供形式化保证，但计算效率不足。

Method: 采用抽象-细化技术，先构建简化网络，验证其解释的充分性，若不足则逐步细化网络直至收敛。

Result: 实验表明，该方法在提高获取可证明充分解释效率的同时，还能在不同抽象层次上提供网络的细粒度解释。

Conclusion: 该方法有效地解决了现有解释方法的可扩展性挑战，为神经网络预测提供了高效且形式化保证的解释。

Abstract: Despite significant advancements in post-hoc explainability techniques for
neural networks, many current methods rely on heuristics and do not provide
formally provable guarantees over the explanations provided. Recent work has
shown that it is possible to obtain explanations with formal guarantees by
identifying subsets of input features that are sufficient to determine that
predictions remain unchanged using neural network verification techniques.
Despite the appeal of these explanations, their computation faces significant
scalability challenges. In this work, we address this gap by proposing a novel
abstraction-refinement technique for efficiently computing provably sufficient
explanations of neural network predictions. Our method abstracts the original
large neural network by constructing a substantially reduced network, where a
sufficient explanation of the reduced network is also provably sufficient for
the original network, hence significantly speeding up the verification process.
If the explanation is in sufficient on the reduced network, we iteratively
refine the network size by gradually increasing it until convergence. Our
experiments demonstrate that our approach enhances the efficiency of obtaining
provably sufficient explanations for neural network predictions while
additionally providing a fine-grained interpretation of the network's
predictions across different abstraction levels.

</details>


### [151] [DiffGradCAM: A Universal Class Activation Map Resistant to Adversarial Training](https://arxiv.org/abs/2506.08514)
*Jacob Piland,Chris Sweet,Adam Czakja*

Key words: CAM, 被动欺骗, SHAM, DiffGradCAM, CNN解释

TL;DR: 该论文探讨了CAM及其衍生方法存在的问题，提出了一种新的对抗性评估方法SHAM，并推出了改进的DiffGradCAM以增强CAM的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 标准CAM方法忽略了类间差异对预测的影响，容易受到被动欺骗攻击。研究旨在提出更鲁棒的CAM方法。

Method: 开发了SHAM作为对抗性评估工具，并提出DiffGradCAM作为一种新的轻量级、对比性的CAM方法。

Result: SHAM有效揭示了CAM的脆弱性，而DiffGradCAM在对抗条件下表现出更强的鲁棒性。

Conclusion: 通过SHAM和DiffGradCAM，研究为提升CAM的鲁棒性提供了新框架。

Abstract: Class Activation Mapping (CAM) and its gradient-based variants (e.g.,
GradCAM) have become standard tools for explaining Convolutional Neural Network
(CNN) predictions. However, these approaches typically focus on individual
logits, while for neural networks using softmax, the class membership
probability estimates depend \textit{only} on the \textit{differences} between
logits, not on their absolute values. This disconnect leaves standard CAMs
vulnerable to adversarial manipulation, such as passive fooling, where a model
is trained to produce misleading CAMs without affecting decision performance.
We introduce \textbf{Salience-Hoax Activation Maps (SHAMs)}, an
\emph{entropy-aware form of passive fooling} that serves as a benchmark for CAM
robustness under adversarial conditions. To address the passive fooling
vulnerability, we then propose \textbf{DiffGradCAM}, a novel, lightweight, and
contrastive approach to class activation mapping that is both non-suceptible to
passive fooling, but also matches the output of standard CAM methods such as
GradCAM in the non-adversarial case. Together, SHAM and DiffGradCAM establish a
new framework for probing and improving the robustness of saliency-based
explanations. We validate both contributions across multi-class tasks with few
and many classes.

</details>


### [152] [NeurIPS 2024 ML4CFD Competition: Results and Retrospective Analysis](https://arxiv.org/abs/2506.08516)
*Mouadh Yagoubi,David Danan,Milad Leyli-Abadi,Ahmed Mazari,Jean-Patrick Brunet,Abbas Kabalan,Fabien Casenave,Yuxin Ma,Giovanni Catalani,Jean Fesquet,Jacob Helwig,Xuan Zhang,Haiyang Yu,Xavier Bertrand,Frederic Tost,Michael Baurheim,Joseph Morlier,Shuiwang Ji*

Key words: 机器学习, CFD, 空气动力学, OpenFOAM, 替代模型

TL;DR: 论文总结了ML4CFD竞赛的成果，展示了机器学习在替代传统CFD求解器方面的潜力，尤其是在空气动力学模拟中。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决机器学习模型在科学计算中准确性、泛化能力和物理一致性方面的挑战，推动其在CFD领域的实际应用。

Method: 通过组织ML4CFD竞赛，提供OpenFOAM生成的数据集，采用多标准评估框架（如预测准确性、计算效率等）比较各团队的表现。

Result: 竞赛中表现最佳的模型在综合指标上超过了传统OpenFOAM求解器，显示出ML替代模型的潜力。

Conclusion: 机器学习在CFD领域具有巨大潜力，未来需关注模型设计的核心原则和评估框架的稳健性。

Abstract: The integration of machine learning (ML) into the physical sciences is
reshaping computational paradigms, offering the potential to accelerate
demanding simulations such as computational fluid dynamics (CFD). Yet,
persistent challenges in accuracy, generalization, and physical consistency
hinder the practical deployment of ML models in scientific domains. To address
these limitations and systematically benchmark progress, we organized the
ML4CFD competition, centered on surrogate modeling for aerodynamic simulations
over two-dimensional airfoils. The competition attracted over 240 teams, who
were provided with a curated dataset generated via OpenFOAM and evaluated
through a multi-criteria framework encompassing predictive accuracy, physical
fidelity, computational efficiency, and out-of-distribution generalization.
This retrospective analysis reviews the competition outcomes, highlighting
several approaches that outperformed baselines under our global evaluation
score. Notably, the top entry exceeded the performance of the original OpenFOAM
solver on aggregate metrics, illustrating the promise of ML-based surrogates to
outperform traditional solvers under tailored criteria. Drawing from these
results, we analyze the key design principles of top submissions, assess the
robustness of our evaluation framework, and offer guidance for future
scientific ML challenges.

</details>


### [153] [Leveraging chaos in the training of artificial neural networks](https://arxiv.org/abs/2506.08523)
*Pedro Jiménez-González,Miguel C. Soriano,Lucas Lacasa*

Key words: 神经网络，梯度下降，混沌动力学，学习率，训练优化

TL;DR: 研究探讨了大学习率下梯度下降（GD）优化神经网络时的动力学行为，发现学习率适当时，GD会进入探索-利用平衡状态，并伴随混沌特征，从而加速训练过程。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统GD算法属于利用型优化，研究探索大学习率下的GD行为，旨在揭示其潜在的混沌动力学特性对训练效率的影响。

Method: 通过分析GD轨迹的动力学特性（如最大Lyapun夫指数），确定学习率范围，观察网络在不同学习率下的训练表现。

Result: 在特定学习率范围内，GD表现出探索-利用平衡，网络轨迹对初始条件敏感（混沌特征），且训练时间最短。

Conclusion: 大学习率下的GD可通过混沌动力学加速训练，且该现象在不同任务和架构中具有普适性。

Abstract: Traditional algorithms to optimize artificial neural networks when confronted
with a supervised learning task are usually exploitation-type relaxational
dynamics such as gradient descent (GD). Here, we explore the dynamics of the
neural network trajectory along training for unconventionally large learning
rates. We show that for a region of values of the learning rate, the GD
optimization shifts away from purely exploitation-like algorithm into a regime
of exploration-exploitation balance, as the neural network is still capable of
learning but the trajectory shows sensitive dependence on initial conditions --
as characterized by positive network maximum Lyapunov exponent --.
Interestingly, the characteristic training time required to reach an acceptable
accuracy in the test set reaches a minimum precisely in such learning rate
region, further suggesting that one can accelerate the training of artificial
neural networks by locating at the onset of chaos. Our results -- initially
illustrated for the MNIST classification task -- qualitatively hold for a range
of supervised learning tasks, learning architectures and other hyperparameters,
and showcase the emergent, constructive role of transient chaotic dynamics in
the training of artificial neural networks.

</details>


### [154] [Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)](https://arxiv.org/abs/2506.08533)
*Nihal Acharya Adde,Alexandra Gianzina,Hanno Gottschalk,Andreas Ebert*

Key words: 进化多目标优化, 神经网络架构搜索, 强化学习, 自动驾驶, 迁移学习

TL;DR: 本文首次提出进化多目标网络架构搜索（EMNAS），通过遗传算法优化大规模强化学习中的神经网络架构，实现奖励提升与模型精简。并行化与师生方法加速搜索并保证可扩展性，实验表明EMNAS优于手动设计模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对自动驾驶中的大规模强化学习需求，探索自动化设计高效神经网络架构的方法，以提升性能同时减少模型复杂度。

Method: 采用进化多目标优化（遗传算法）结合并行化技术与师生迁移学习，动态调整网络结构以适应奖励与参数规模的双目标。

Result: 实验证明EMNAS在奖励指标与参数效率上均优于人工设计模型，验证了其在自动驾驶强化学习中的有效性。

Conclusion: EMNAS为自动驾驶强化学习提供了一种可扩展且高效的网络架构自动化设计框架，推动实际应用中的性能优化。

Abstract: This paper introduces Evolutionary Multi-Objective Network Architecture
Search (EMNAS) for the first time to optimize neural network architectures in
large-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses
genetic algorithms to automate network design, tailored to enhance rewards and
reduce model size without compromising performance. Additionally,
parallelization techniques are employed to accelerate the search, and
teacher-student methodologies are implemented to ensure scalable optimization.
This research underscores the potential of transfer learning as a robust
framework for optimizing performance across iterative learning processes by
effectively leveraging knowledge from earlier generations to enhance learning
efficiency and stability in subsequent generations. Experimental results
demonstrate that tailored EMNAS outperforms manually designed models, achieving
higher rewards with fewer parameters. The findings of these strategies
contribute positively to EMNAS for RL in autonomous driving, advancing the
field toward better-performing networks suitable for real-world scenarios.

</details>


### [155] [DeepForm: Reasoning Large Language Model for Communication System Formulation](https://arxiv.org/abs/2506.08551)
*Panlong Wu,Ting Wang,Yifei Zhong,Haoqi Zhang,Zitong Wang,Fangxin Wang*

Key words: 6G, 通信系统建模, 大语言模型, DeepForm, CSFRC

TL;DR: 为了解决通信系统建模专业化不足的问题，本文提出了首个专用推理模型DeepForm，并构建了大规模开源数据集CSFRC，采用两阶段训练策略显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有通用大语言模型（LLMs）缺乏专业领域知识和高质量数据，难以胜任通信系统建模任务，亟需专业化解决方案。

Method: 提出两阶段训练：1) 监督微调（SFT）结合思维链（CoT）数据学习领域知识；2) 基于ReMax的新型强化学习算法C-ReMax，提升推理能力。

Result: DeepForm在多样化场景中表现优于现有大型专有模型，达到最先进水平。

Conclusion: DeepForm和CSFRC填补了领域空白，为未来研究提供了宝贵资源。

Abstract: Communication system formulation is critical for advancing 6G and future
wireless technologies, yet it remains a complex, expertise-intensive task.
While Large Language Models (LLMs) offer potential, existing general-purpose
models often lack the specialized domain knowledge, nuanced reasoning
capabilities, and access to high-quality, domain-specific training data
required for adapting a general LLM into an LLM specially for communication
system formulation. To bridge this gap, we introduce DeepForm, the first
reasoning LLM specially for automated communication system formulation. We
propose the world-first large-scale, open-source dataset meticulously curated
for this domain called Communication System Formulation Reasoning Corpus
(CSFRC). Our framework employs a two-stage training strategy: first, Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge;
second, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based
on ReMax, to cultivate advanced modeling capabilities and elicit sophisticated
reasoning patterns like self-correction and verification. Extensive experiments
demonstrate that our model achieves state-of-the-art performance, significantly
outperforming larger proprietary LLMs on diverse senerios. We will release
related resources to foster further research in this area after the paper is
accepted.

</details>


### [156] [The Geometries of Truth Are Orthogonal Across Tasks](https://arxiv.org/abs/2506.08572)
*Waiss Azizian,Michael Kirchhof,Eugene Ndiaye,Louis Bethune,Michal Klein,Pierre Ablin,Marco Cuturi*

Key words: LLMs, 可靠性, 激活分析, 几何特征, 跨任务迁移

TL;DR: 尽管LLMs在不同任务中表现出色，但其可靠性仍受质疑。研究发现，通过激活分析判断答案正确性的方法存在局限性，几何特征无法跨任务通用。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索LLMs可靠性问题，研究现有通过激活分析评估答案正确性的方法是否具有跨任务通用性。

Method: 通过线性分类器和稀疏正则化器分析不同任务中的激活向量，验证其几何特征是否可迁移。

Result: 发现激活向量几何特征因任务而异，跨任务迁移效果差，即使采用混合方法也无法显著改善。

Conclusion: 当前基于激活分析的可靠性评估方法在跨任务通用性上存在根本局限性。

Abstract: Large Language Models (LLMs) have demonstrated impressive generalization
capabilities across various tasks, but their claim to practical relevance is
still mired by concerns on their reliability. Recent works have proposed
examining the activations produced by an LLM at inference time to assess
whether its answer to a question is correct. Some works claim that a "geometry
of truth" can be learned from examples, in the sense that the activations that
generate correct answers can be distinguished from those leading to mistakes
with a linear classifier. In this work, we underline a limitation of these
approaches: we observe that these "geometries of truth" are intrinsically
task-dependent and fail to transfer across tasks. More precisely, we show that
linear classifiers trained across distinct tasks share little similarity and,
when trained with sparsity-enforcing regularizers, have almost disjoint
supports. We show that more sophisticated approaches (e.g., using mixtures of
probes and tasks) fail to overcome this limitation, likely because activation
vectors commonly used to classify answers form clearly separated clusters when
examined across tasks.

</details>


### [157] [SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models](https://arxiv.org/abs/2506.08574)
*Alvise Dei Rossi,Matteo Metaldi,Michal Bechny,Irina Filchenko,Julia van der Meer,Markus H. Schmidt,Claudio L. A. Bassetti,Athina Tzovara,Francesca D. Faraci,Luigi Fiorillo*

Key words: 睡眠分期, 深度学习, 模型评估, 集成学习, 临床偏见

TL;DR: SLEEPYLAND是一个开源睡眠分期评估框架，旨在解决模型评估、泛化性、偏见和标注变异等问题。通过整合大量数据和预训练模型，提出SOMNUS集成方法，性能优于现有方法，甚至超过人类专家。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 深度学习在睡眠分期中的应用受限于评估公平性、泛化性和标注变异，需开发更可靠的评估框架。

Method: 提出SLEEPYLAND框架，整合22,000小时域内和84,000小时域外数据，采用预训练模型和SOMNUS集成方法。

Result: SOMNUS在24个数据集上表现稳健（宏F1 68.7%-87.2%），超越人类专家，并能预测标注争议区域。

Conclusion: SLEEPYLAND和SOMNUS显著提升了睡眠分期的可靠性和临床适用性，但模型偏见仍需进一步解决。

Abstract: Despite advances in deep learning for automatic sleep staging, clinical
adoption remains limited due to challenges in fair model evaluation,
generalization across diverse datasets, model bias, and variability in human
annotations. We present SLEEPYLAND, an open-source sleep staging evaluation
framework designed to address these barriers. It includes more than 22'0000
hours in-domain (ID) sleep recordings, and more than 84'000 hours out-of-domain
(OOD) sleep recordings, spanning a broad range of ages, sleep-wake disorders,
and hardware setups. We release pre-trained models based on high-performing SoA
architectures and evaluate them under standardized conditions across single-
and multi-channel EEG/EOG configurations. We introduce SOMNUS, an ensemble
combining models across architectures and channel setups via soft voting.
SOMNUS achieves robust performance across twenty-four different datasets, with
macro-F1 scores between 68.7% and 87.2%, outperforming individual models in
94.9% of cases. Notably, SOMNUS surpasses previous SoA methods, even including
cases where compared models were trained ID while SOMNUS treated the same data
as OOD. Using a subset of the BSWR (N=6'633), we quantify model biases linked
to age, gender, AHI, and PLMI, showing that while ensemble improves robustness,
no model architecture consistently minimizes bias in performance and clinical
markers estimation. In evaluations on OOD multi-annotated datasets (DOD-H,
DOD-O), SOMNUS exceeds the best human scorer, i.e., MF1 85.2% vs 80.8% on
DOD-H, and 80.2% vs 75.9% on DOD-O, better reproducing the scorer consensus
than any individual expert (k = 0.89/0.85 and ACS = 0.95/0.94 for healthy/OSA
cohorts). Finally, we introduce ensemble disagreement metrics - entropy and
inter-model divergence based - predicting regions of scorer disagreement with
ROC AUCs up to 0.828, offering a data-driven proxy for human uncertainty.

</details>


### [158] [Diffusion-based Time Series Forecasting for Sewerage Systems](https://arxiv.org/abs/2506.08577)
*Nicholas A. Pearson,Francesca Cairoli,Luca Bortolussi,Davide Russo,Francesca Zanello*

Key words: 深度学习,扩散模型,共形推理,时间序列预测,污水系统

TL;DR: 提出了一种基于扩散模型的深度学习方法，用于提升污水系统上下文预测的准确性，结合共形推理技术确保统计可靠性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 提高污水系统在极端天气下的预测准确性，捕捉复杂环境信号的关联性。

Method: 采用扩散模型处理多元时间序列数据，并结合共形推理技术校准预测区间。

Result: 模型在真实数据测试中表现出色，即使在极端天气下也能保持预测准确性。

Conclusion: 该方法能够提供高可靠性的上下文预测，适用于复杂环境条件下的污水系统管理。

Abstract: We introduce a novel deep learning approach that harnesses the power of
generative artificial intelligence to enhance the accuracy of contextual
forecasting in sewerage systems. By developing a diffusion-based model that
processes multivariate time series data, our system excels at capturing complex
correlations across diverse environmental signals, enabling robust predictions
even during extreme weather events. To strengthen the model's reliability, we
further calibrate its predictions with a conformal inference technique,
tailored for probabilistic time series data, ensuring that the resulting
prediction intervals are statistically reliable and cover the true target
values with a desired confidence level. Our empirical tests on real sewerage
system data confirm the model's exceptional capability to deliver reliable
contextual predictions, maintaining accuracy even under severe weather
conditions.

</details>


### [159] [CALT: A Library for Computer Algebra with Transformer](https://arxiv.org/abs/2506.08600)
*Hiroshi Kera,Shun Arakawa,Yuta Sato*

Key words: 符号计算,深度学习,Transformer模型,CALT库

TL;DR: 这篇论文介绍了通过深度学习实现符号计算的学习性，提出了一个用户友好的Python库CALT，帮助非深度学习专家训练模型完成符号计算任务。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 近年来人工智能的进步表明符号计算可以通过端到端深度学习学习，但需要符号计算社区的积极参与来解决由此带来的挑战和研究方向。

Method: 使用Transformer模型作为序列到序列函数的学习器，通过大量符号表达式示例训练模型来模拟计算。

Result: 提出了名为CALT的Python库，为非深度学习专家提供了便捷的工具来训练符号计算模型。

Conclusion: 通过CALT库，符号计算的深度学习应用变得更加容易，为非专家提供了新的研究工具。

Abstract: Recent advances in artificial intelligence have demonstrated the learnability
of symbolic computation through end-to-end deep learning. Given a sufficient
number of examples of symbolic expressions before and after the target
computation, Transformer models - highly effective learners of
sequence-to-sequence functions - can be trained to emulate the computation.
This development opens up several intriguing challenges and new research
directions, which require active contributions from the symbolic computation
community. In this work, we introduce Computer Algebra with Transformer (CALT),
a user-friendly Python library designed to help non-experts in deep learning
train models for symbolic computation tasks.

</details>


### [160] [Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation](https://arxiv.org/abs/2506.08604)
*Giacomo Baldan,Qiang Liu,Alberto Guardone,Nils Thuerey*

Key words: 生成模型、物理约束、流匹配、PDE、代理建模

TL;DR: 提出了一种名为PBFM的新方法，通过显式嵌入物理约束改进生成模型，相比现有方法在物理残差和分布准确性上表现更优。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有生成方法虽能建模复杂系统行为，但隐含学习物理规律可能导致准确性不足。PBFM旨在显式结合物理约束提升模型性能。

Method: PBFM框架将物理约束（PDE残差和代数关系）显式嵌入流匹配目标，并引入时序展开训练优化噪声样本预测。无需超参数调优联合损失函数。

Result: 在三个PDE问题上的实验显示，PBFM比FM方法物理残差精确度提升8倍，且分布准确性优于现有算法。

Conclusion: PBFM为物理和工程中的代理建模、不确定性量化和加速仿真提供了高效且理论坚实的框架。

Abstract: Generative machine learning methods, such as diffusion models and flow
matching, have shown great potential in modeling complex system behaviors and
building efficient surrogate models. However, these methods typically learn the
underlying physics implicitly from data. We propose Physics-Based Flow Matching
(PBFM), a novel generative framework that explicitly embeds physical
constraints, both PDE residuals and algebraic relations, into the flow matching
objective. We also introduce temporal unrolling at training time that improves
the accuracy of the final, noise-free sample prediction. Our method jointly
minimizes the flow matching loss and the physics-based residual loss without
requiring hyperparameter tuning of their relative weights. Additionally, we
analyze the role of the minimum noise level, $\sigma_{\min}$, in the context of
physical constraints and evaluate a stochastic sampling strategy that helps to
reduce physical residuals. Through extensive benchmarks on three representative
PDE problems, we show that our approach yields up to an $8\times$ more accurate
physical residuals compared to FM, while clearly outperforming existing
algorithms in terms of distributional accuracy. PBFM thus provides a principled
and efficient framework for surrogate modeling, uncertainty quantification, and
accelerated simulation in physics and engineering applications.

</details>


### [161] [Sample Efficient Demonstration Selection for In-Context Learning](https://arxiv.org/abs/2506.08607)
*Kiran Purohit,V Venktesh,Sourangshu Bhattacharya,Avishek Anand*

Key words: 上下文学习,示例选择,赌博机问题,LLM效率

TL;DR: 本文提出了CASE方法，用于高效选择少量示例（exemplars），通过类似赌博机问题的策略减少LLM调用次数，实现性能不降的前提下大幅提升效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在上下文学习中，示例选择对构建高效提示至关重要，但传统方法需要大量LLM评估，计算成本高。

Method: 将示例选择问题建模为多臂赌博机问题，提出CASE策略，通过动态维护候选集减少评估次数。

Result: CASE在运行时实现7倍加速，减少87%的LLM调用，性能不降。

Conclusion: CASE是一种高效且经济的示例选择方法，可显著降低计算成本。

Abstract: The in-context learning paradigm with LLMs has been instrumental in advancing
a wide range of natural language processing tasks. The selection of few-shot
examples (exemplars / demonstration samples) is essential for constructing
effective prompts under context-length budget constraints. In this paper, we
formulate the exemplar selection task as a top-m best arms identification
problem. A key challenge in this setup is the exponentially large number of
arms that need to be evaluated to identify the m-best arms. We propose CASE
(Challenger Arm Sampling for Exemplar selection), a novel sample-efficient
selective exploration strategy that maintains a shortlist of "challenger" arms,
which are current candidates for the top-m arms. In each iteration, only one of
the arms from this shortlist or the current topm set is pulled, thereby
reducing sample complexity and, consequently, the number of LLM evaluations.
Furthermore, we model the scores of exemplar subsets (arms) using a
parameterized linear scoring function, leading to stochastic linear bandits
setting. CASE achieves remarkable efficiency gains of up to 7x speedup in
runtime while requiring 7x fewer LLM calls (87% reduction) without sacrificing
performance compared to state-of-the-art exemplar selection methods. We release
our code and data at https://github.com/kiranpurohit/CASE

</details>


### [162] [HSG-12M: A Large-Scale Spatial Multigraph Dataset](https://arxiv.org/abs/2506.08618)
*Xianquan Yan,Hakan Akgün,Kenji Kawaguchi,N. Duane Loh,Ching Hua Lee*

Key words: HSG-12M, 空间多图, 哈密顿谱图, 几何感知图学习, Poly2Graph, 数据驱动科学

TL;DR: HSG-12M是首个大规模空间多图数据集，包含静态和动态哈密顿谱图，通过几何保留的边结构提供了丰富的物理拓扑数据，同时发布了Poly2Graph工具，为几何感知的图学习开辟了新路径。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有图基准假设边是非空间且简单的，忽略了物理路径的几何多样性。HSG-12M的目标是通过空间多图捕捉物理系统的复杂几何结构，推动几何感知的图学习发展。

Method: 通过177TB的谱势数据，构建了11.6M静态和5.1M动态Hamiltonian谱图，覆盖1401特征多项式类。利用Poly2Graph工具将一维晶体哈密顿量映射为谱图。

Result: HSG-12M展示了多边几何在大规模学习中的新挑战，同时证明了谱图可作为多项式、向量和矩阵的通用拓扑指纹，建立了代数与图的新联系。

Conclusion: HSG-12M为几何感知图学习和数据驱动的科学发现（如凝聚态物理）奠定了基础，开辟了新的研究方向。

Abstract: Existing graph benchmarks assume non-spatial, simple edges, collapsing
physically distinct paths into a single link. We introduce HSG-12M, the first
large-scale dataset of $\textbf{spatial multigraphs}-$graphs embedded in a
metric space where multiple geometrically distinct trajectories between two
nodes are retained as separate edges. HSG-12M contains 11.6 million static and
5.1 million dynamic $\textit{Hamiltonian spectral graphs}$ across 1401
characteristic-polynomial classes, derived from 177 TB of spectral potential
data. Each graph encodes the full geometry of a 1-D crystal's energy spectrum
on the complex plane, producing diverse, physics-grounded topologies that
transcend conventional node-coordinate datasets. To enable future extensions,
we release $\texttt{Poly2Graph}$: a high-performance, open-source pipeline that
maps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with
popular GNNs expose new challenges in learning from multi-edge geometry at
scale. Beyond its practical utility, we show that spectral graphs serve as
universal topological fingerprints of polynomials, vectors, and matrices,
forging a new algebra-to-graph link. HSG-12M lays the groundwork for
geometry-aware graph learning and new opportunities of data-driven scientific
discovery in condensed matter physics and beyond.

</details>


### [163] [Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers](https://arxiv.org/abs/2506.08641)
*Simon Roschmann,Quentin Bouniot,Vasilii Feofanov,Ievgen Redko,Zeynep Akata*

Key words: 时间序列分类，视觉Transformer，基础模型，OpenCLIP

TL;DR: 论文提出了一种名为TiViT的框架，将时间序列转换为图像，利用预训练的大规模图像数据集上的视觉Transformer（ViT）进行分类，取得了最先进的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于公开时间序列数据集的稀缺，时间序列基础模型（TSFM）的发展受限。

Method: 通过将时间序列转换为图像，利用预训练的ViT进行表征学习和分类。

Result: TiViT在标准时间序列分类基准上取得了最先进性能，且与TSFM表征空间互补。

Conclusion: TiViT为在非视觉领域复用视觉表征提供了新方向。

Abstract: Time series classification is a fundamental task in healthcare and industry,
yet the development of time series foundation models (TSFMs) remains limited by
the scarcity of publicly available time series datasets. In this work, we
propose Time Vision Transformer (TiViT), a framework that converts time series
into images to leverage the representational power of frozen Vision
Transformers (ViTs) pretrained on large-scale image datasets. First, we
theoretically motivate our approach by analyzing the 2D patching of ViTs for
time series, showing that it can increase the number of label-relevant tokens
and reduce the sample complexity. Second, we empirically demonstrate that TiViT
achieves state-of-the-art performance on standard time series classification
benchmarks by utilizing the hidden representations of large OpenCLIP models. We
explore the structure of TiViT representations and find that intermediate
layers with high intrinsic dimension are the most effective for time series
classification. Finally, we assess the alignment between TiViT and TSFM
representation spaces and identify a strong complementarity, with further
performance gains achieved by combining their features. Our findings reveal yet
another direction for reusing vision representations in a non-visual domain.

</details>


### [164] [Semi-gradient DICE for Offline Constrained Reinforcement Learning](https://arxiv.org/abs/2506.08644)
*Woosung Kim,JunHo Seo,Jongmin Lee,Byung-Jun Lee*

Key words: DICE, 离线强化学习, 约束RL, 半梯度优化, 成本估计

TL;DR: DICE框架旨在解决策略诱导的稳态分布与目标分布之间的不匹配问题，但在增强离线强化学习性能时，其优化方法会影响成本估计能力。本文提出了一种新方法，通过半梯度DICE实现准确的成本估计和最优性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: DICE框架在离线强化学习中表现出色，但在优化过程中可能破坏其性能。本文旨在解决这一问题。

Method: 提出了一种基于半梯度DICE的新方法，确保准确的成本估计和约束强化学习性能。

Result: 新方法在DSRL基准测试中实现了最优性能。

Conclusion: 通过改进半梯度优化方法，本文成功解决了DICE框架在约束强化学习中的局限性。

Abstract: Stationary Distribution Correction Estimation (DICE) addresses the mismatch
between the stationary distribution induced by a policy and the target
distribution required for reliable off-policy evaluation (OPE) and policy
optimization. DICE-based offline constrained RL particularly benefits from the
flexibility of DICE, as it simultaneously maximizes return while estimating
costs in offline settings. However, we have observed that recent approaches
designed to enhance the offline RL performance of the DICE framework
inadvertently undermine its ability to perform OPE, making them unsuitable for
constrained RL scenarios. In this paper, we identify the root cause of this
limitation: their reliance on a semi-gradient optimization, which solves a
fundamentally different optimization problem and results in failures in cost
estimation. Building on these insights, we propose a novel method to enable OPE
and constrained RL through semi-gradient DICE. Our method ensures accurate cost
estimation and achieves state-of-the-art performance on the offline constrained
RL benchmark, DSRL.

</details>


### [165] [Fusing Cross-modal and Uni-modal Representations: A Kronecker Product Approach](https://arxiv.org/abs/2506.08645)
*Youqi Wu,Jingwei Zhang,Farzan Farnia*

Key words: 跨模态嵌入, 单模态嵌入, RP-KrossFuse, 随机投影, 克罗内克积

TL;DR: 提出了RP-KrossFuse方法，通过随机投影和克罗内克积融合跨模态与单模态嵌入，实现了模态专一任务的性能提升并保留跨模态对齐能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的跨模态嵌入在模态专一任务上表现不如单模态嵌入，而单模态嵌入缺乏跨模态对齐能力。研究旨在结合两者优势，提升性能。

Method: 提出了RP-KrossFuse，基于随机投影的克罗内克积方法，融合跨模态与单模态嵌入，支持高效核空间操作。

Result: 实验表明，RP-KrossFuse在模态专一任务上性能提升，同时保留跨模态对齐能力。

Conclusion: RP-KrossFuse成功结合了跨模态与单模态嵌入的优势，为多模态任务提供了有效解决方案。

Abstract: Cross-modal embeddings, such as CLIP, BLIP and their variants, have achieved
promising results in aligning representations across modalities. However, these
embeddings could underperform compared to state-of-the-art single-modality
embeddings on modality-specific tasks. On the other hand, single-modality
embeddings excel in their domains but lack cross-modal alignment capabilities.
In this work, we focus on the problem of unifying cross-modality and
single-modality embeddings to achieve the performance of modality-expert
embedding within individual modalities while preserving cross-modal alignment.
To this end, we propose RP-KrossFuse, a method that leverages a random
projection-based Kronecker product to integrate cross-modal embeddings with
single-modality embeddings. RP-KrossFuse aims to fuse the sample-pairwise
similarity scores of the fused embeddings and operates efficiently in a
specified kernel space and supports scalable implementations via random Fourier
features for shift-invariant kernels such as the Gaussian kernel. We
demonstrate the effectiveness of RP-KrossFuse through several numerical
experiments, combining CLIP embeddings with uni-modal image and text
embeddings. Our numerical results indicate that RP-KrossFuse achieves
competitive modality-specific performance while retaining cross-modal
alignment, bridging the gap between cross-modal and single-modality embeddings.

</details>


### [166] [JoFormer (Journey-based Transformer): Theory and Empirical Analysis on the Tiny Shakespeare Dataset](https://arxiv.org/abs/2506.08652)
*Mahesh Godavarti*

Key words: Transformer, JoFormer, 位置编码, 语言建模, 非交换代数

TL;DR: 论文提出了一种基于非交换代数的Transformer架构JoFormer，通过方向变换建模位置信息，在语言建模任务中表现出色。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决Transformer中位置信息有效建模的挑战，提出一种更灵活的架构。

Method: 引入基于旅程的Transformer架构JoFormer，使用可学习方向变换表示相对位置，并扩展现有方法。

Result: 在Tiny Shakespeare任务中，JoFormer表现出更低的困惑度和更快的收敛速度。

Conclusion: JoFormer为Transformer中位置信息集成提供了理论支持，具有扩展潜力。

Abstract: Transformers have demonstrated remarkable success in sequence modeling, yet
effectively incorporating positional information remains a challenging and
active area of research. In this paper, we introduce JoFormer, a journey-based
Transformer architecture grounded in a recently proposed non-commutative
algebra for composing transformations across positions. JoFormer represents
relative positions through learnable directional transforms that are
sequentially composed along the input, thereby extending and generalizing
existing approaches based on relative position representations. We derive the
JoFormer attention mechanism from first principles and show that it subsumes
standard methods such as rotary transformations as special cases. To evaluate
its effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny
Shakespeare character-level language modeling task. Our results demonstrate
that
  JoFormer consistently achieves lower perplexity and faster convergence,
highlighting the advantages of its more expressive, journey-based treatment of
position. Notably, the per-token JoFormer is still a primitive, conceptual
variant with layer-independent angles, yet it already demonstrates strong
performance-underscoring its promise as a proof of concept for more expressive
architectures. We conclude by discussing how JoFormer offers a principled
approach to integrating positional structure into Transformer architectures.
The code used in this work is available at
https://github.com/mahesh-godavarti/joformer.

</details>


### [167] [When Simple Model Just Works: Is Network Traffic Classification in Crisis?](https://arxiv.org/abs/2506.08655)
*Kamil Jerabek,Jan Luxemburk,Richard Plny,Josef Koumar,Jaroslav Pesek,Karel Hynek*

Key words: 网络流量分类, k-NN, 数据冗余, 模型评估

TL;DR: 研究发现简单k-NN方法在网络流量分类中表现优异，原因是数据集冗余和标签冲突导致复杂模型性能被高估，建议重新调整任务设计与评估方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探究为何简单的k-NN方法在网络流量分类中能与复杂模型匹敌甚至更优，并分析数据集冗余对模型性能的影响。

Method: 评估k-NN方法在12个数据集和15个分类任务中的表现，分析数据冗余和标签冲突问题。

Result: 发现大多数数据集包含超过50%的冗余样本，导致模型性能被高估，复杂模型可能不适配流量分类任务。

Conclusion: 提出新的任务设计和评估方向，以适应网络流量分类的独特需求。

Abstract: Machine learning has been applied to network traffic classification (TC) for
over two decades. While early efforts used shallow models, the latter 2010s saw
a shift toward complex neural networks, often reporting near-perfect accuracy.
However, it was recently revealed that a simple k-NN baseline using packet
sequences metadata (sizes, times, and directions) can be on par or even
outperform more complex methods. In this paper, we investigate this phenomenon
further and evaluate this baseline across 12 datasets and 15 TC tasks, and
investigate why it performs so well. Our analysis shows that most datasets
contain over 50% redundant samples (identical packet sequences), which
frequently appear in both training and test sets due to common splitting
practices. This redundancy can lead to overestimated model performance and
reduce the theoretical maximum accuracy when identical flows have conflicting
labels. Given its distinct characteristics, we further argue that standard
machine learning practices adapted from domains like NLP or computer vision may
be ill-suited for TC. Finally, we propose new directions for task formulation
and evaluation to address these challenges and help realign the field.

</details>


### [168] [Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness](https://arxiv.org/abs/2506.08660)
*Jinkwan Jang,Hyungjin Park,Jinmyeong Choi,Taesup Kim*

Key words: 时间序列预测, 多变量数据, Transformer, 异步采样, 缺失值处理

TL;DR: ChannelTokenFormer是一种基于Transformer的预测模型，旨在解决多变量时间序列数据中的挑战，包括通道依赖性、异步采样和缺失值问题，表现出卓越的稳健性和准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现实世界中的时间序列数据通常是多变量的，且存在复杂的通道间依赖关系，同时面临异步采样和缺失值等问题。现有模型通常基于过度简化的假设，无法应对这些实际挑战。

Method: 提出ChannelTokenFormer模型，通过灵活的架构设计显式捕获跨通道交互、适应通道异步采样，并有效处理缺失值。

Result: 在三个基准数据集和一个实际工业数据集上的实验表明，ChannelTokenFormer在真实场景中表现优于现有方法。

Conclusion: ChannelTokenFormer在处理多变量时间序列的复杂问题时表现出色，为实际应用提供了可靠解决方案。

Abstract: Real-world time series data are inherently multivariate, often exhibiting
complex inter-channel dependencies. Each channel is typically sampled at its
own period and is prone to missing values due to various practical and
operational constraints. These characteristics pose fundamental challenges
related to channel dependency, sampling asynchrony, and missingness, all of
which must be addressed to enable robust and reliable forecasting in practical
settings. However, most existing architectures are built on oversimplified
assumptions, such as identical sampling periods across channels and fully
observed inputs at test time, which often do not hold in real-world scenarios.
To bridge this gap, we propose ChannelTokenFormer, a Transformer-based
forecasting model with a flexible architecture designed to explicitly capture
cross-channel interactions, accommodate channel-wise asynchronous sampling, and
effectively handle missing values. Extensive experiments on three benchmark
datasets modified to reflect practical settings, along with one real-world
industrial dataset, demonstrate the superior robustness and accuracy of
ChannelTokenFormer under challenging real-world conditions.

</details>


### [169] [Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization](https://arxiv.org/abs/2506.08662)
*Florian Borzechowski,Michael Schäfer,Heiko Schwarz,Jonathan Pfaff,Detlev Marpe,Thomas Wiegand*

Key words: 图像压缩,变分自编码器,量化训练,熵约束量化,编码增益

TL;DR: 论文提出了一种基于变分自编码器的图像压缩方法改进，通过额外的微调训练步骤，优化量化噪声处理，显著提升了编码效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统变分自编码器在图像压缩中量化训练时存在零梯度问题，导致网络不理想，需要改进量化近似方法。

Method: 在常规端到端训练后，对网络部分进行基于量化潜在表示的微调训练，尤其在熵约束量化器（如网格编码量化）中优化量化噪声处理。

Result: 在Kodak和TecNick测试集上，平均节省1%-2%的比特率，最高可达2.2%。

Conclusion: 通过量化潜在表示的微调训练，可以显著提升编码效率，无需增加推理复杂度。

Abstract: The continuous improvements on image compression with variational
autoencoders have lead to learned codecs competitive with conventional
approaches in terms of rate-distortion efficiency. Nonetheless, taking the
quantization into account during the training process remains a problem, since
it produces zero derivatives almost everywhere and needs to be replaced with a
differentiable approximation which allows end-to-end optimization. Though there
are different methods for approximating the quantization, none of them model
the quantization noise correctly and thus, result in suboptimal networks.
Hence, we propose an additional finetuning training step: After conventional
end-to-end training, parts of the network are retrained on quantized latents
obtained at the inference stage. For entropy-constraint quantizers like
Trellis-Coded Quantization, the impact of the quantizer is particularly
difficult to approximate by rounding or adding noise as the quantized latents
are interdependently chosen through a trellis search based on both the entropy
model and a distortion measure. We show that retraining on correctly quantized
data consistently yields additional coding gain for both uniform scalar and
especially for entropy-constraint quantization, without increasing inference
complexity. For the Kodak test set, we obtain average savings between 1% and
2%, and for the TecNick test set up to 2.2% in terms of Bj{\o}ntegaard-Delta
bitrate.

</details>


### [170] [Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search](https://arxiv.org/abs/2506.08669)
*Dongge Han,Menglin Xia,Daniel Madrigal Diaz,Samuel Kessler,Ankur Mallick,Xuchao Zhang,Mirian Del Carmen Hipolito Garcia,Jin Xu,Victor Rühle,Saravan Rajmohan*

Key words: 小型语言模型（SLM），大型语言模型（LLM），推理能力，提示优化，资源受限环境

TL;DR: 提出了一种利用LLM生成的蓝图框架，增强小型语言模型（SLM）的推理能力，并通过提示模板搜索机制减少SLM对提示变化的敏感性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决小型语言模型（SLM）因容量有限导致的推理能力不足和对提示变化敏感的问题。

Method: 使用LLM生成的结构化、高层推理蓝图指导SLM，并结合提示模板搜索机制。

Result: 框架在数学（GSM8K）、编码（MBPP）和逻辑推理（BBH）任务上显著提升了SLM性能，且无需增加模型规模或额外训练。

Conclusion: 提供了一种轻量级、适合资源受限环境的SLM增强方案。

Abstract: Small language models (SLMs) offer promising and efficient alternatives to
large language models (LLMs). However, SLMs' limited capacity restricts their
reasoning capabilities and makes them sensitive to prompt variations. To
address these challenges, we propose a novel framework that enhances SLM
reasoning capabilities through LLM generated blueprints. The blueprints provide
structured, high-level reasoning guides that help SLMs systematically tackle
related problems. Furthermore, our framework integrates a prompt template
search mechanism to mitigate the SLMs' sensitivity to prompt variations. Our
framework demonstrates improved SLM performance across various tasks, including
math (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves
the reasoning capabilities of SLMs without increasing model size or requiring
additional training, offering a lightweight and deployment-friendly solution
for on-device or resource-constrained environments.

</details>


### [171] [Towards Fair Representation: Clustering and Consensus](https://arxiv.org/abs/2506.08673)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien Long Nguyen,Romina Nobahari*

Key words: 共识聚类，公平聚类，近似算法，NP难解性

TL;DR: 本文研究了通过公平聚类的视角解决共识聚类问题，首次提出了一种常数因子近似方案，并开发了最优算法以最小化修改现有聚类以实现公平性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 共识聚类是机器学习和数据分析中的基础任务，但现有方法忽略了公平性问题。本文旨在将公平性纳入共识聚类，确保每个受保护群体在数据集中得到比例代表。

Method: 本文通过引入公平性约束，提出了针对具有不同群体比例的数据集的近似算法，并证明了在两组大小不等时问题的NP难解性。

Result: 提出了针对等群体代表数据集的最优算法，以及针对两组不同比例数据集的近似线性时间算法。证明了问题的NP难解性。

Conclusion: 本文的成果不仅解决了共识聚类的公平性问题，还可能对其他公平聚类问题产生广泛影响。

Abstract: Consensus clustering, a fundamental task in machine learning and data
analysis, aims to aggregate multiple input clusterings of a dataset,
potentially based on different non-sensitive attributes, into a single
clustering that best represents the collective structure of the data. In this
work, we study this fundamental problem through the lens of fair clustering, as
introduced by Chierichetti et al. [NeurIPS'17], which incorporates the
disparate impact doctrine to ensure proportional representation of each
protected group in the dataset within every cluster. Our objective is to find a
consensus clustering that is not only representative but also fair with respect
to specific protected attributes. To the best of our knowledge, we are the
first to address this problem and provide a constant-factor approximation.
  As part of our investigation, we examine how to minimally modify an existing
clustering to enforce fairness -- an essential postprocessing step in many
clustering applications that require fair representation. We develop an optimal
algorithm for datasets with equal group representation and near-linear time
constant factor approximation algorithms for more general scenarios with
different proportions of two group sizes. We complement our approximation
result by showing that the problem is NP-hard for two unequal-sized groups.
Given the fundamental nature of this problem, we believe our results on Closest
Fair Clustering could have broader implications for other clustering problems,
particularly those for which no prior approximation guarantees exist for their
fair variants.

</details>


### [172] [Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling](https://arxiv.org/abs/2506.08681)
*Phuc Minh Nguyen,Ngoc-Hieu Nguyen,Duy H. M. Nguyen,Anji Liu,An Mai,Binh T. Nguyen,Daniel Sonntag,Khoa D. Doan*

Key words: 直接对齐算法,重要性采样,过优化问题,大语言模型

TL;DR: 本文提出了一种名为IS-DAAs的重要性采样方法，用于缓解离线直接对齐算法（DAAs）中的过优化问题，并通过实验证明了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 直接对齐算法（如DPO）在优化大语言模型时容易出现过优化问题，导致模型性能下降，因此需要一种新的方法来解决这一问题。

Method: 引入了基于重要性采样的IS-DAAs方法，通过在目标函数中增加重要性比率，并对其进行截断以避免高方差问题。

Result: 实验表明，IS-DAAs能有效缓解过优化问题，尤其是在低正则化强度下表现优于其他方法。

Conclusion: IS-DAAs是一种有效的解决方案，能够显著改善直接对齐算法的性能。

Abstract: Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization
(DPO) have emerged as alternatives to the standard Reinforcement Learning from
Human Feedback (RLHF) for aligning large language models (LLMs) with human
values. However, these methods are more susceptible to over-optimization, in
which the model drifts away from the reference policy, leading to degraded
performance as training progresses. This paper proposes a novel
importance-sampling approach to mitigate the over-optimization problem of
offline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective
with an importance ratio that accounts for the reference policy distribution.
IS-DAAs additionally avoid the high variance issue associated with importance
sampling by clipping the importance ratio to a maximum value. Our extensive
experiments demonstrate that IS-DAAs can effectively mitigate
over-optimization, especially under low regularization strength, and achieve
better performance than other methods designed to address this problem. Our
implementations are provided publicly at this link.

</details>


### [173] [Variational Autoencoder-Based Approach to Latent Feature Analysis on Efficient Representation of Power Load Monitoring Data](https://arxiv.org/abs/2506.08698)
*Boyu Xie,Tangtang Xie*

Key words: 智能电网, 变分自编码器, 电力负载监测, 数据补全, 高维不完整数据

TL;DR: 论文提出了一种基于变分自编码器(VAE)的模型VAE-LF，用于高效表示和补充高维不完整电力负载监测数据，实验表明其在数据补全任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着智能电网的发展，高维不完整(HDI)的电力负载监测(PLM)数据对电力负载预测(PLF)模型的性能提出了挑战，需要一种高效的数据表示和补全方法。

Method: 提出VAE-LF模型，使用编码器-解码器结构学习数据的低维潜在表示，并将HDI PLM数据分割为向量后顺序输入模型，生成补充数据。

Result: 在英国UK-DALE数据集上的实验显示，VAE-LF在5%和10%稀疏度测试中表现优于其他基准模型，RMSE和MAE显著降低，尤其在低稀疏度数据上表现突出。

Conclusion: VAE-LF为智能电网中的电力负载管理提供了高效的数据补全解决方案，展现出优越性能。

Abstract: With the development of smart grids, High-Dimensional and Incomplete (HDI)
Power Load Monitoring (PLM) data challenges the performance of Power Load
Forecasting (PLF) models. In this paper, we propose a potential
characterization model VAE-LF based on Variational Autoencoder (VAE) for
efficiently representing and complementing PLM missing data. VAE-LF learns a
low-dimensional latent representation of the data using an Encoder-Decoder
structure by splitting the HDI PLM data into vectors and feeding them
sequentially into the VAE-LF model, and generates the complementary data.
Experiments on the UK-DALE dataset show that VAE-LF outperforms other benchmark
models in both 5% and 10% sparsity test cases, with significantly lower RMSE
and MAE, and especially outperforms on low sparsity ratio data. The method
provides an efficient data-completion solution for electric load management in
smart grids.

</details>


### [174] [Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs](https://arxiv.org/abs/2506.08727)
*Samarth Sikand,Rohit Mehra,Priyavanshi Pathania,Nikhil Bamby,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Key words: 生成式AI, 大型语言模型, 碳排放, R-ICE框架, 可持续性

TL;DR: 论文探讨生成式AI对能源和环境的影响，提出利用LLM基准数据估计碳排放的框架R-ICE，平衡准确性与实用性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 生成式AI的广泛使用对能源网络和环境造成负担，现有工具在监测和估计碳排放方面存在不足，亟需更高效、非侵入性的解决方案。

Method: 提出框架R-ICE，利用现有SOTA基准数据估计提示级推理碳排放，解决高输入数据和高误差问题。

Result: 验证结果显示基于基准的建模在碳排放估计中潜力显著，支持动态LLM路由和碳核算等应用。

Conclusion: R-ICE框架为碳排放估计提供更实用的方法，科学界需进一步探索其潜力。

Abstract: While Generative AI stands to be one of the fastest adopted technologies
ever, studies have made evident that the usage of Large Language Models (LLMs)
puts significant burden on energy grids and our environment. It may prove a
hindrance to the Sustainability goals of any organization. A crucial step in
any Sustainability strategy is monitoring or estimating the energy consumption
of various components. While there exist multiple tools for monitoring energy
consumption, there is a dearth of tools/frameworks for estimating the
consumption or carbon emissions. Current drawbacks of both monitoring and
estimation tools include high input data points, intrusive nature, high error
margin, etc. We posit that leveraging emerging LLM benchmarks and related data
points can help overcome aforementioned challenges while balancing accuracy of
the emission estimations. To that extent, we discuss the challenges of current
approaches and present our evolving framework, R-ICE, which estimates prompt
level inference carbon emissions by leveraging existing state-of-the-art(SOTA)
benchmark. This direction provides a more practical and non-intrusive way to
enable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our
promising validation results suggest that benchmark-based modelling holds great
potential for inference emission estimation and warrants further exploration
from the scientific community.

</details>


### [175] [Exploration by Random Reward Perturbation](https://arxiv.org/abs/2506.08737)
*Haozhe Ma,Guoji Fu,Zhengding Luo,Jiele Wu,Tze-Yun Leong*

Key words: 强化学习, 探索策略, 奖励扰动, 策略多样性, 样本效率

TL;DR: Random Reward Perturbation（RRP）是一种新的强化学习探索策略，通过向环境奖励添加零均值噪声增强策略多样性，提升探索效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在解决强化学习中探索不足的问题，通过奖励噪声提升策略多样性。

Method: 向环境奖励添加零均值噪声，与现有探索策略（如ε-贪婪、熵正则化）兼容。

Result: 实验显示，RRP显著提升了PPO和SAC的性能，提高了样本效率并避免了局部最优。

Conclusion: RRP为奖励塑形与噪声驱动探索建立了理论联系，展示了其互补潜力。

Abstract: We introduce Random Reward Perturbation (RRP), a novel exploration strategy
for reinforcement learning (RL). Our theoretical analyses demonstrate that
adding zero-mean noise to environmental rewards effectively enhances policy
diversity during training, thereby expanding the range of exploration. RRP is
fully compatible with the action-perturbation-based exploration strategies,
such as $\epsilon$-greedy, stochastic policies, and entropy regularization,
providing additive improvements to exploration effects. It is general,
lightweight, and can be integrated into existing RL algorithms with minimal
implementation effort and negligible computational overhead. RRP establishes a
theoretical connection between reward shaping and noise-driven exploration,
highlighting their complementary potential. Experiments show that RRP
significantly boosts the performance of Proximal Policy Optimization and Soft
Actor-Critic, achieving higher sample efficiency and escaping local optima
across various tasks, under both sparse and dense reward scenarios.

</details>


### [176] [Urban Incident Prediction with Graph Neural Networks: Integrating Government Ratings and Crowdsourced Reports](https://arxiv.org/abs/2506.08740)
*Sidhika Balachandar,Shuvom Sadhuka,Bonnie Berger,Emma Pierson,Nikhil Garg*

Key words: 图神经网络, 城市时空预测, 多视图学习, 众包数据, 偏见量化

TL;DR: 提出了一种基于多视图多输出GNN的模型，结合政府评级数据和众包报告数据，预测城市事件的潜在状态。案例研究展示了该模型在稀疏和有偏数据下的优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决在城市事件预测中，政府评级数据稀疏和众包报告数据有偏的问题，提供一种更准确的潜在状态预测方法。

Method: 使用多视图多输出的GNN模型，整合政府评级和众包报告数据，并通过半合成数据验证模型。

Result: 模型在真实和半合成数据上表现优于仅使用单一数据源的模型，尤其是在评级数据稀疏时。同时量化了众包报告的偏见。

Conclusion: 该方法适用于异构、稀疏和有偏数据的潜在状态预测，展示了广泛的应用潜力。

Abstract: Graph neural networks (GNNs) are widely used in urban spatiotemporal
forecasting, such as predicting infrastructure problems. In this setting,
government officials wish to know in which neighborhoods incidents like
potholes or rodent issues occur. The true state of incidents (e.g., street
conditions) for each neighborhood is observed via government inspection
ratings. However, these ratings are only conducted for a sparse set of
neighborhoods and incident types. We also observe the state of incidents via
crowdsourced reports, which are more densely observed but may be biased due to
heterogeneous reporting behavior. First, for such settings, we propose a
multiview, multioutput GNN-based model that uses both unbiased rating data and
biased reporting data to predict the true latent state of incidents. Second, we
investigate a case study of New York City urban incidents and collect,
standardize, and make publicly available a dataset of 9,615,863 crowdsourced
reports and 1,041,415 government inspection ratings over 3 years and across 139
types of incidents. Finally, we show on both real and semi-synthetic data that
our model can better predict the latent state compared to models that use only
reporting data or models that use only rating data, especially when rating data
is sparse and reports are predictive of ratings. We also quantify demographic
biases in crowdsourced reporting, e.g., higher-income neighborhoods report
problems at higher rates. Our analysis showcases a widely applicable approach
for latent state prediction using heterogeneous, sparse, and biased data.

</details>


### [177] [On the Stability of the Jacobian Matrix in Deep Neural Networks](https://arxiv.org/abs/2506.08764)
*Benjamin Dadoun,Soufiane Hayou,Hanan Salam,Mohamed El Amine Seddik,Pierre Youssef*

Key words: 深度神经网络, 梯度问题, 随机矩阵理论, 初始化方案, 稀疏性

TL;DR: 论文提出了一个适用于稀疏性和非独立同分布权重的神经网络稳定性定理，扩展了初始化方案的理论基础。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 深度神经网络在深度增加时面临梯度爆炸或消失问题，之前的工作仅适用于全连接网络和独立同分布权重。本研究旨在扩展这些限制，解决稀疏性和依赖权重的问题。

Method: 利用随机矩阵理论的最新进展，建立了一个通用的稳定性定理，涵盖稀疏性和非独立同分布权重的情况。

Result: 为更广泛的网络模型提供了谱稳定性的严格保证，支持现代神经网络中结构化和依赖随机性的初始化方案。

Conclusion: 研究扩展了神经网络初始化方案的理论基础，适用于更复杂的网络结构和权重分布。

Abstract: Deep neural networks are known to suffer from exploding or vanishing
gradients as depth increases, a phenomenon closely tied to the spectral
behavior of the input-output Jacobian. Prior work has identified critical
initialization schemes that ensure Jacobian stability, but these analyses are
typically restricted to fully connected networks with i.i.d. weights. In this
work, we go significantly beyond these limitations: we establish a general
stability theorem for deep neural networks that accommodates sparsity (such as
that introduced by pruning) and non-i.i.d., weakly correlated weights (e.g.
induced by training). Our results rely on recent advances in random matrix
theory, and provide rigorous guarantees for spectral stability in a much
broader class of network models. This extends the theoretical foundation for
initialization schemes in modern neural networks with structured and dependent
randomness.

</details>


### [178] [Design Patterns for Securing LLM Agents against Prompt Injections](https://arxiv.org/abs/2506.08837)
*Luca Beurer-Kellner,Beat Buesser Ana-Maria Creţu,Edoardo Debenedetti,Daniel Dobos,Daniel Fabian,Marc Fischer,David Froelicher,Kathrin Grosse,Daniel Naeff,Ezinwanne Ozoani,Andrew Paverd,Florian Tramèr,Václav Volhejn*

Key words: AI代理，提示注入攻击，安全性，设计模式

TL;DR: 本文提出了一种设计模式，用于构建具有可证明抵抗提示注入攻击能力的AI代理，并分析了其实用性与安全性的权衡。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着基于大语言模型的AI代理功能日益强大，确保其安全性成为关键挑战，尤其是针对提示注入攻击的防护。

Method: 提出了一套原则性设计模式，并通过案例研究展示了其实用性。

Result: 设计模式能够有效抵抗提示注入攻击，但在实用性与安全性之间存在权衡。

Conclusion: 本文为构建安全的AI代理提供了可行方案，需进一步优化实用性与安全性的平衡。

Abstract: As AI agents powered by Large Language Models (LLMs) become increasingly
versatile and capable of addressing a broad spectrum of tasks, ensuring their
security has become a critical challenge. Among the most pressing threats are
prompt injection attacks, which exploit the agent's resilience on natural
language inputs -- an especially dangerous threat when agents are granted tool
access or handle sensitive information. In this work, we propose a set of
principled design patterns for building AI agents with provable resistance to
prompt injection. We systematically analyze these patterns, discuss their
trade-offs in terms of utility and security, and illustrate their real-world
applicability through a series of case studies.

</details>


### [179] [IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)](https://arxiv.org/abs/2506.08844)
*Siyi Sun,David Antony Selby,Yunchuan Huang,Sebastian Vollmer,Seth Flaxman,Anisoara Calinescu*

Key words: 缺失数据插补, 社会经济数据, 合成数据, 基准评估, 可重复研究

TL;DR: 该研究利用世界银行的公开合成数据集IMAGIC-500，对多种缺失数据插补方法进行了全面评估，旨在推动可重复的社会科学研究。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 真实社会经济数据集由于数据保护协议难以公开，导致插补方法缺乏系统评估基准，限制了研究的可重复性。

Method: 基于世界银行的合成数据集，构建IMAGIC-500，评估统计、传统机器学习和深度学习方法在多种缺失机制和缺失比例下的性能。

Result: 研究比较了多种插补方法的准确性、计算效率及对下游任务的影响，揭示了各方法的优缺点。

Conclusion: IMAGIC-500数据集和基准旨在为开发稳健的插补算法和促进可重复研究提供支持。

Abstract: Missing data imputation in tabular datasets remains a pivotal challenge in
data science and machine learning, particularly within socioeconomic research.
However, real-world socioeconomic datasets are typically subject to strict data
protection protocols, which often prohibit public sharing, even for synthetic
derivatives. This severely limits the reproducibility and accessibility of
benchmark studies in such settings. Further, there are very few publicly
available synthetic datasets. Thus, there is limited availability of benchmarks
for systematic evaluation of imputation methods on socioeconomic datasets,
whether real or synthetic. In this study, we utilize the World Bank's publicly
available synthetic dataset, Synthetic Data for an Imaginary Country, which
closely mimics a real World Bank household survey while being fully public,
enabling broad access for methodological research. With this as a starting
point, we derived the IMAGIC-500 dataset: we select a subset of 500k
individuals across approximately 100k households with 19 socioeconomic
features, designed to reflect the hierarchical structure of real-world
household surveys. This paper introduces a comprehensive missing data
imputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR,
MNAR) and missingness ratios (10\%, 20\%, 30\%, 40\%, 50\%). Our evaluation
considers the imputation accuracy for continuous and categorical variables,
computational efficiency, and impact on downstream predictive tasks, such as
estimating educational attainment at the individual level. The results
highlight the strengths and weaknesses of statistical, traditional machine
learning, and deep learning imputation techniques, including recent
diffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate
the development of robust imputation algorithms and foster reproducible social
science research.

</details>


### [180] [Agile Reinforcement Learning for Real-Time Task Scheduling in Edge Computing](https://arxiv.org/abs/2506.08850)
*Amin Avan,Akramul Azim,Qusay Mahmoud*

Key words: 边缘计算, 任务调度, 强化学习, 软实时应用, 智能探索

TL;DR: 本文提出了一种敏捷强化学习（aRL）方法，用于在边缘计算环境中调度软实时应用任务，通过智能探索和动作屏蔽技术提高调度效率和收敛速度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 边缘计算环境中任务调度的复杂性、动态性及现有启发式方法和强化学习算法在适应性和收敛速度方面的不足，促使研究者提出aRL方法以优化调度性能。

Method: 采用敏捷强化学习（aRL），结合智能探索和动作屏蔽技术，减少无关动作的随机探索，提高调度效率和适应性。

Result: 实验表明，aRL在高命中率和快速收敛方面优于基线方法，适合边缘计算环境中的任务调度。

Conclusion: aRL通过优化探索和动作选择，显著提升了调度性能，为边缘计算中的软实时任务调度提供了有效解决方案。

Abstract: Soft real-time applications are becoming increasingly complex, posing
significant challenges for scheduling offloaded tasks in edge computing
environments while meeting task timing constraints. Moreover, the exponential
growth of the search space, presence of multiple objectives and parameters, and
highly dynamic nature of edge computing environments further exacerbate the
complexity of task scheduling. As a result, schedulers based on heuristic and
metaheuristic algorithms frequently encounter difficulties in generating
optimal or near-optimal task schedules due to their constrained ability to
adapt to the dynamic conditions and complex environmental characteristics of
edge computing. Accordingly, reinforcement learning algorithms have been
incorporated into schedulers to address the complexity and dynamic conditions
inherent in task scheduling in edge computing. However, a significant
limitation of reinforcement learning algorithms is the prolonged learning time
required to adapt to new environments and to address medium- and large-scale
problems. This challenge arises from the extensive global action space and
frequent random exploration of irrelevant actions. Therefore, this study
proposes Agile Reinforcement learning (aRL), in which the RL-agent performs
informed exploration and executes only relevant actions. Consequently, the
predictability of the RL-agent is enhanced, leading to rapid adaptation and
convergence, which positions aRL as a suitable candidate for scheduling the
tasks of soft real-time applications in edge computing. The experiments
demonstrate that the combination of informed exploration and action-masking
methods enables aRL to achieve a higher hit-ratio and converge faster than the
baseline approaches.

</details>


### [181] [Adapting to Heterophilic Graph Data with Structure-Guided Neighbor Discovery](https://arxiv.org/abs/2506.08871)
*Victor M. Tenorio,Madeline Navarro,Samuel Rey,Santiago Segarra,Antonio G. Marques*

Key words: Graph Neural Networks, heterophilic data, structural attributes, label homophily

TL;DR: 提出了一种Structure-Guided GNN（SG-GNN）架构，通过创建具有更高标签同质性的替代图结构，提升GNN在异质性数据上的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统GNN在异质性数据（连接节点标签不同的数据）上表现不佳，因为它们假设数据具有同质性并依赖局部消息传递。

Method: 通过链接具有相似结构属性的节点创建替代图结构，开发了SG-GNN架构，自适应地学习原始图与新结构图的权重。

Result: 在多种异质性基准数据集上，SG-GNN实现了最先进或高度竞争力的性能。

Conclusion: 利用结构信息指导GNN可以显著提升其在异质性数据上的性能。

Abstract: Graph Neural Networks (GNNs) often struggle with heterophilic data, where
connected nodes may have dissimilar labels, as they typically assume homophily
and rely on local message passing. To address this, we propose creating
alternative graph structures by linking nodes with similar structural
attributes (e.g., role-based or global), thereby fostering higher label
homophily on these new graphs. We theoretically prove that GNN performance can
be improved by utilizing graphs with fewer false positive edges (connections
between nodes of different classes) and that considering multiple graph views
increases the likelihood of finding such beneficial structures. Building on
these insights, we introduce Structure-Guided GNN (SG-GNN), an architecture
that processes the original graph alongside the newly created structural
graphs, adaptively learning to weigh their contributions. Extensive experiments
on various benchmark datasets, particularly those with heterophilic
characteristics, demonstrate that our SG-GNN achieves state-of-the-art or
highly competitive performance, highlighting the efficacy of exploiting
structural information to guide GNNs.

</details>


### [182] [SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08989)
*Xiao Liang,Zhong-Zhi Li,Yeyun Gong,Yang Wang,Hengyuan Zhang,Yelong Shen,Ying Nian Wu,Weizhu Chen*

Key words: 强化学习, 大语言模型, 问题合成, 自识别弱点, 推理任务

TL;DR: 论文提出了RLVR的Self-aware Weakness-driven problem Synthesis框架（SwS），通过自识别模型弱点并针对性生成问题，提升模型在复杂推理任务中的表现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有强化学习方法在训练大语言模型时，由于缺乏高质量问题集和忽略模型能力的问题生成策略，导致效果受限。

Method: SwS框架通过分析模型在训练中的失败案例，提取核心概念并合成新问题，针对性增强模型弱点。

Result: 框架在7B和32B模型上平均提升10.0%和7.7%的性能，覆盖八大推理基准。

Conclusion: SwS通过自识别与针对性训练，显著提升了模型在推理任务中的表现。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for training large language models (LLMs) on complex reasoning tasks, such as
mathematical problem solving. A prerequisite for the scalability of RLVR is a
high-quality problem set with precise and verifiable answers. However, the
scarcity of well-crafted human-labeled math problems and limited-verification
answers in existing distillation-oriented synthetic datasets limit their
effectiveness in RL. Additionally, most problem synthesis strategies
indiscriminately expand the problem set without considering the model's
capabilities, leading to low efficiency in generating useful questions. To
mitigate this issue, we introduce a Self-aware Weakness-driven problem
Synthesis framework (SwS) that systematically identifies model deficiencies and
leverages them for problem augmentation. Specifically, we define weaknesses as
questions that the model consistently fails to learn through its iterative
sampling during RL training. We then extract the core concepts from these
failure cases and synthesize new problems to strengthen the model's weak areas
in subsequent augmented training, enabling it to focus on and gradually
overcome its weaknesses. Without relying on external knowledge distillation,
our framework enables robust generalization byempowering the model to
self-identify and address its weaknesses in RL, yielding average performance
gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning
benchmarks.

</details>


### [183] [Filling in the Blanks: Applying Data Imputation in incomplete Water Metering Data](https://arxiv.org/abs/2506.08882)
*Dimitrios Amaxilatis,Themistoklis Sarantakos,Ioannis Chatzigiannakis,Georgios Mylonas*

Key words: 数据填补, 智能水表, IoT水网监测, 漏检, 预测性维护

TL;DR: 探讨数据填补技术在智能水表监测水网中的应用，比较多种方法后发现有效填补能提升数据质量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 智能水表数据存在技术性缺失，影响运营决策和效率，填补技术可改善数据质量。

Method: 比较k近邻、MissForest、Transformer和循环神经网络等多种填补方法。

Result: 有效的填补方法显著提升了水消耗数据的准确性和可靠性。

Conclusion: 数据填补技术能改善水网监测数据质量，助力漏水和预测性维护等应用。

Abstract: In this work, we explore the application of recent data imputation techniques
to enhance monitoring and management of water distribution networks using smart
water meters, based on data derived from a real-world IoT water grid monitoring
deployment. Despite the detailed data produced by such meters, data gaps due to
technical issues can significantly impact operational decisions and efficiency.
Our results, by comparing various imputation methods, such as k-Nearest
Neighbors, MissForest, Transformers, and Recurrent Neural Networks, indicate
that effective data imputation can substantially enhance the quality of the
insights derived from water consumption data as we study their effect on
accuracy and reliability of water metering data to provide solutions in
applications like leak detection and predictive maintenance scheduling.

</details>


### [184] [e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs](https://arxiv.org/abs/2506.09026)
*Amrith Setlur,Matthew Y. R. Yang,Charlie Snell,Jeremy Greer,Ian Wu,Virginia Smith,Max Simchowitz,Aviral Kumar*

Key words: LLM推理, 测试时扩展, 外推, 上下文探索, e3配方

TL;DR: 本文提出了一种名为e3的配方，通过在推理时进行扩展训练，提升LLM在困难问题上的表现，并通过训练LLM进行上下文探索来实现外推。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的推理模型在外推（即超出训练时的最大token预算时仍能提升性能）方面表现不佳，因此需要一种方法来改进这一点。

Method: 提出的e3配方包含三个关键要素：链式技能训练、利用负面梯度进行探索强化学习，以及通过特定设计的课程来结合任务难度与训练token预算。

Result: e3-1.7B模型在AIME'25和HMMT'25中表现最佳，且能外推到2倍训练token预算，同时在pass@1和pass@k指标上均优于基础模型。

Conclusion: e3配方通过上下文探索和关键要素的结合，显著提升了LLM的推理能力和外推性能。

Abstract: Test-time scaling offers a promising path to improve LLM reasoning by
utilizing more compute at inference time; however, the true promise of this
paradigm lies in extrapolation (i.e., improvement in performance on hard
problems as LLMs keep "thinking" for longer, beyond the maximum token budget
they were trained on). Surprisingly, we find that most existing reasoning
models do not extrapolate well. We show that one way to enable extrapolation is
by training the LLM to perform in-context exploration: training the LLM to
effectively spend its test time budget by chaining operations (such as
generation, verification, refinement, etc.), or testing multiple hypotheses
before it commits to an answer. To enable in-context exploration, we identify
three key ingredients as part of our recipe e3: (1) chaining skills that the
base LLM has asymmetric competence in, e.g., chaining verification (easy) with
generation (hard), as a way to implement in-context search; (2) leveraging
"negative" gradients from incorrect traces to amplify exploration during RL,
resulting in longer search traces that chains additional asymmetries; and (3)
coupling task difficulty with training token budget during training via a
specifically-designed curriculum to structure in-context exploration. Our
recipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25
scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not
only attains high pass@1 scores, but also improves pass@k over the base model.

</details>


### [185] [InfoDPCCA: Information-Theoretic Dynamic Probabilistic Canonical Correlation Analysis](https://arxiv.org/abs/2506.08884)
*Shiqin Tang,Shujian Yu*

Key words: InfoDPCCA，动态概率CCA，信息论，表示学习，fMRI

TL;DR: InfoDPCCA是一种动态概率CCA框架，用于提取两个相关观测序列的共享潜表示，通过信息论目标平衡压缩和预测性，并提升解释性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 从高维序列数据中提取有意义的潜表示是机器学习的重要挑战，尤其适用于自然科学和工程领域。

Method: 提出InfoDPCCA框架，利用信息论目标提取共享潜表示，并引入两步训练方案和残差连接机制。

Result: 在合成和医学fMRI数据实验中，InfoDPCCA表现优异。

Conclusion: InfoDPCCA是一种有效的表示学习工具，可提升潜空间的可解释性和稳健性。

Abstract: Extracting meaningful latent representations from high-dimensional sequential
data is a crucial challenge in machine learning, with applications spanning
natural science and engineering. We introduce InfoDPCCA, a dynamic
probabilistic Canonical Correlation Analysis (CCA) framework designed to model
two interdependent sequences of observations. InfoDPCCA leverages a novel
information-theoretic objective to extract a shared latent representation that
captures the mutual structure between the data streams and balances
representation compression and predictive sufficiency while also learning
separate latent components that encode information specific to each sequence.
Unlike prior dynamic CCA models, such as DPCCA, our approach explicitly
enforces the shared latent space to encode only the mutual information between
the sequences, improving interpretability and robustness. We further introduce
a two-step training scheme to bridge the gap between information-theoretic
representation learning and generative modeling, along with a residual
connection mechanism to enhance training stability. Through experiments on
synthetic and medical fMRI data, we demonstrate that InfoDPCCA excels as a tool
for representation learning. Code of InfoDPCCA is available at
https://github.com/marcusstang/InfoDPCCA.

</details>


### [186] [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](https://arxiv.org/abs/2506.08889)
*Yizhao Gao,Shuming Guo,Shijie Cao,Yuqing Xia,Yu Cheng,Lei Wang,Lingxiao Ma,Yutao Sun,Tianzhu Ye,Li Dong,Hayden Kwok-Hay So,Yu Hua,Ting Cao,Fan Yang,Mao Yang*

Key words: SeerAttention-R,稀疏注意力,自回归解码,推理模型,效率优化

TL;DR: SeerAttention-R是一个专为推理模型长解码设计的稀疏注意力框架，通过自蒸馏门控机制保留注意力稀疏性，适应自回归解码。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决推理模型在长解码过程中效率低下的问题，同时保持高准确率。

Method: 采用轻量级门控插件，无需修改原始模型参数，通过TileLang优化稀疏解码内核。

Result: 在AIME基准测试中，仅用0.4B token训练，维持4K token预算下的无损推理准确率，稀疏解码速度提升9倍。

Conclusion: SeerAttention-R高效、灵活，适用于现有预训练模型的稀疏解码需求。

Abstract: We introduce SeerAttention-R, a sparse attention framework specifically
tailored for the long decoding of reasoning models. Extended from
SeerAttention, SeerAttention-R retains the design of learning attention
sparsity through a self-distilled gating mechanism, while removing query
pooling to accommodate auto-regressive decoding. With a lightweight plug-in
gating, SeerAttention-R is flexible and can be easily integrated into existing
pretrained model without modifying the original parameters. We demonstrate that
SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning
accuracy with 4K token budget in AIME benchmark under large sparse attention
block sizes (64/128). Using TileLang, we develop a highly optimized sparse
decoding kernel that achieves near-theoretical speedups of up to 9x over
FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:
https://github.com/microsoft/SeerAttention.

</details>


### [187] [Intention-Conditioned Flow Occupancy Models](https://arxiv.org/abs/2506.08902)
*Chongyi Zheng,Seohong Park,Sergey Levine,Benjamin Eysenbach*

Key words: 强化学习, 预训练, 流匹配, 占用模型, 意图变量

TL;DR: InFOM是一种基于意图条件的流占用模型，通过预测未来状态的占用分布，提升了强化学习预训练的效果，在36个状态任务和4个图像任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决强化学习中的样本效率和鲁棒性问题，研究者希望通过预训练大型模型来适应不同任务，但动作的长期依赖性带来了挑战。

Method: 提出InFOM方法，利用流匹配和潜在用户意图变量建模未来状态的占用分布，支持广义策略改进。

Result: 实验表明，InFOM在36个状态任务和4个图像任务中，回报中位数提升1.8倍，成功率提高36%。

Conclusion: InFOM通过结合意图变量和流匹配技术，显著提升了强化学习预训练的性能。

Abstract: Large-scale pre-training has fundamentally changed how machine learning
research is done today: large foundation models are trained once, and then can
be used by anyone in the community (including those without data or compute
resources to train a model from scratch) to adapt and fine-tune to specific
tasks. Applying this same framework to reinforcement learning (RL) is appealing
because it offers compelling avenues for addressing core challenges in RL,
including sample efficiency and robustness. However, there remains a
fundamental challenge to pre-train large models in the context of RL: actions
have long-term dependencies, so training a foundation model that reasons across
time is important. Recent advances in generative AI have provided new tools for
modeling highly complex distributions. In this paper, we build a probabilistic
model to predict which states an agent will visit in the temporally distant
future (i.e., an occupancy measure) using flow matching. As large datasets are
often constructed by many distinct users performing distinct tasks, we include
in our model a latent variable capturing the user intention. This intention
increases the expressivity of our model, and enables adaptation with
generalized policy improvement. We call our proposed method
intention-conditioned flow occupancy models (InFOM). Comparing with alternative
methods for pre-training, our experiments on $36$ state-based and $4$
image-based benchmark tasks demonstrate that the proposed method achieves $1.8
\times$ median improvement in returns and increases success rates by $36\%$.
Website: https://chongyi-zheng.github.io/infom Code:
https://github.com/chongyi-zheng/infom

</details>


### [188] [Enhancing generalizability of model discovery across parameter space with multi-experiment equation learning (ME-EQL)](https://arxiv.org/abs/2506.08916)
*Maria-Veronica Ciocanel,John T. Nardini,Kevin B. Flores,Erica M. Rutter,Suzanne S. Sindi,Alexandria Volkening*

Key words: agent-based modeling, equation learning, generalizability, biological systems

TL;DR: 本文提出两种多实验方程学习方法（ME-EQL），通过插值或统一模型库，显著降低了从ABM数据中恢复参数的误差，提升了模型的泛化性和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决传统方程学习方法（EQL）在从ABM数据推导连续模型时需大量模拟、泛化性不足的问题。

Method: 引入两种ME-EQL方法：OAT ME-EQL（逐个参数学习并通过插值连接）和ES ME-EQL（构建跨参数的统一模型库）。

Result: 两种方法均显著降低了参数恢复的相对误差，OAT ME-EQL在参数空间内表现出更好的泛化性。

Conclusion: 多实验方程学习能提升复杂生物系统模型的泛化性和可解释性。

Abstract: Agent-based modeling (ABM) is a powerful tool for understanding
self-organizing biological systems, but it is computationally intensive and
often not analytically tractable. Equation learning (EQL) methods can derive
continuum models from ABM data, but they typically require extensive
simulations for each parameter set, raising concerns about generalizability. In
this work, we extend EQL to Multi-experiment equation learning (ME-EQL) by
introducing two methods: one-at-a-time ME-EQL (OAT ME-EQL), which learns
individual models for each parameter set and connects them via interpolation,
and embedded structure ME-EQL (ES ME-EQL), which builds a unified model library
across parameters. We demonstrate these methods using a birth--death mean-field
model and an on-lattice agent-based model of birth, death, and migration with
spatial structure. Our results show that both methods significantly reduce the
relative error in recovering parameters from agent-based simulations, with OAT
ME-EQL offering better generalizability across parameter space. Our findings
highlight the potential of equation learning from multiple experiments to
enhance the generalizability and interpretability of learned models for complex
biological systems.

</details>


### [189] [Local MDI+: Local Feature Importances for Tree-Based Models](https://arxiv.org/abs/2506.08928)
*Zhongyuan Liang,Zachary T. Rewolinski,Abhineet Agarwal,Tiffany M. Tang,Bin Yu*

Key words: 随机森林, 局部特征重要性, LMDI+, LIME, TreeSHAP

TL;DR: LMDI+是一种新的局部特征重要性方法，优于现有的LIME和TreeSHAP，提供更稳定的特征重要性解释。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 树模型在表格数据中表现出色，但现有的局部特征重要性方法（如LIME和TreeSHAP）依赖近似和扰动，缺乏稳定性。

Method: LMDI+通过扩展全局MDI+框架，利用决策树与线性模型的等价性，生成样本特定的特征重要性。

Result: LMDI+在12个数据集上平均优于基线方法10%，并在稳定性方面表现更好。

Conclusion: LMDI+提供了更可靠的局部解释，适用于高风险的决策场景。

Abstract: Tree-based ensembles such as random forests remain the go-to for tabular data
over deep learning models due to their prediction performance and computational
efficiency. These advantages have led to their widespread deployment in
high-stakes domains, where interpretability is essential for ensuring
trustworthy predictions. This has motivated the development of popular local
(i.e. sample-specific) feature importance (LFI) methods such as LIME and
TreeSHAP. However, these approaches rely on approximations that ignore the
model's internal structure and instead depend on potentially unstable
perturbations. These issues are addressed in the global setting by MDI+, a
feature importance method which exploits an equivalence between decision trees
and linear models on a transformed node basis. However, the global MDI+ scores
are not able to explain predictions when faced with heterogeneous individual
characteristics. To address this gap, we propose Local MDI+ (LMDI+), a novel
extension of the MDI+ framework to the sample specific setting. LMDI+
outperforms existing baselines LIME and TreeSHAP in identifying
instance-specific signal features, averaging a 10% improvement in downstream
task performance across twelve real-world benchmark datasets. It further
demonstrates greater stability by consistently producing similar instance-level
feature importance rankings across multiple random forest fits. Finally, LMDI+
enables local interpretability use cases, including the identification of
closer counterfactuals and the discovery of homogeneous subgroups.

</details>


### [190] [BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models](https://arxiv.org/abs/2506.08936)
*Amina Mollaysa,Artem Moskale,Pushpak Pati,Tommaso Mansi,Mangal Prakash,Rui Liao*

Key words: BioLangFusion, 多模态融合, 预训练语言模型, 分子表示, 生物信息学

TL;DR: BioLangFusion通过将预训练的DNA、mRNA和蛋白质语言模型统一为分子表征，利用生物意义明确的密码子级别对齐，实现了跨模态的直接对应，并在五种分子属性预测任务中表现优于单模态基线。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 受到分子生物学中心法则（信息从基因到转录再到蛋白质的流动）的启发，研究旨在通过整合不同模态的语言模型，提升分子预测任务的性能。

Method: BioLangFusion研究了三种标准融合技术：密码子级嵌入拼接、受多实例学习启发的熵正则化注意力池化和跨模态多头注意力，每种技术为结合模态特定信号提供不同的归纳偏差。

Result: 在五种分子属性预测任务中，BioLangFusion超越了单模态基线，表明即使是简单融合预训练模型也能以最小开销捕捉互补的多组学信息。

Conclusion: BioLangFusion提供了一种简单且高效的方法来整合多模态预训练语言模型，展现了在分子预测任务中的潜力。

Abstract: We present BioLangFusion, a simple approach for integrating pre-trained DNA,
mRNA, and protein language models into unified molecular representations.
Motivated by the central dogma of molecular biology (information flow from gene
to transcript to protein), we align per-modality embeddings at the biologically
meaningful codon level (three nucleotides encoding one amino acid) to ensure
direct cross-modal correspondence. BioLangFusion studies three standard fusion
techniques: (i) codon-level embedding concatenation, (ii) entropy-regularized
attention pooling inspired by multiple-instance learning, and (iii) cross-modal
multi-head attention -- each technique providing a different inductive bias for
combining modality-specific signals. These methods require no additional
pre-training or modification of the base models, allowing straightforward
integration with existing sequence-based foundation models. Across five
molecular property prediction tasks, BioLangFusion outperforms strong unimodal
baselines, showing that even simple fusion of pre-trained models can capture
complementary multi-omic information with minimal overhead.

</details>


### [191] [KARMA: A Multilevel Decomposition Hybrid Mamba Framework for Multivariate Long-Term Time Series Forecasting](https://arxiv.org/abs/2506.08939)
*Hang Ye,Gaoxiang Duan,Haoran Zeng,Yangxin Zhu,Lingxue Meng,Xiaoying Zheng,Yongxin Zhu*

Key words: 时间序列预测,自适应分解,混合频时分解,Mamba,KARMA

TL;DR: KARMA提出了一种自适应时间通道分解模块和混合频时分解模块，结合Mamba-based KarmaBlock，显著提升了多元长时间序列预测的性能和计算效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统时间序列分解方法单一且依赖固定规则，难以挖掘复杂序列的动态特性；基于Transformer的模型计算复杂度高，难以有效建模长序列。

Method: 引入KARMA模型，包含自适应时间通道分解模块（ATCD）和混合频时分解模块（HFTD），结合多尺度Mamba-based KarmaBlock协同处理全局和局部信息。

Result: 在八个真实数据集上的实验表明，KARMA在预测准确性和计算效率上显著优于主流基线方法。

Conclusion: KARMA通过动态分解和多尺度处理，有效解决了复杂时间序列预测中的动态建模和计算效率问题。

Abstract: Multivariate long-term and efficient time series forecasting is a key
requirement for a variety of practical applications, and there are complex
interleaving time dynamics in time series data that require decomposition
modeling. Traditional time series decomposition methods are single and rely on
fixed rules, which are insufficient for mining the potential information of the
series and adapting to the dynamic characteristics of complex series. On the
other hand, the Transformer-based models for time series forecasting struggle
to effectively model long sequences and intricate dynamic relationships due to
their high computational complexity. To overcome these limitations, we
introduce KARMA, with an Adaptive Time Channel Decomposition module (ATCD) to
dynamically extract trend and seasonal components. It further integrates a
Hybrid Frequency-Time Decomposition module (HFTD) to further decompose Series
into frequency-domain and time-domain. These components are coupled with
multi-scale Mamba-based KarmaBlock to efficiently process global and local
information in a coordinated manner. Experiments on eight real-world datasets
from diverse domains well demonstrated that KARMA significantly outperforms
mainstream baseline methods in both predictive accuracy and computational
efficiency. Code and full results are available at this repository:
https://github.com/yedadasd/KARMA

</details>


### [192] [Towards Robust Deep Reinforcement Learning against Environmental State Perturbation](https://arxiv.org/abs/2506.08961)
*Chenxu Wang,Huaping Liu*

Key words: 深度强化学习,环境状态扰动,对抗攻击,鲁棒性,BAT框架

TL;DR: 本文研究了深度强化学习（DRL）中环境状态扰动的对抗攻击与防御问题，提出了BAT防御框架，有效提升了代理的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决现有DRL研究中环境状态扰动研究的不足，提升代理在自然场景中的鲁棒性。

Method: 提出环境状态扰动问题，引入非目标攻击方法作为校准对手，并设计BAT防御框架，结合监督学习和对抗训练。

Result: 实验证明主流代理在环境状态扰动下的脆弱性及BAT的防御有效性。

Conclusion: BAT框架显著提升了代理对环境状态扰动的鲁棒性。

Abstract: Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have
been widely studied in various threat models; however, few consider
environmental state perturbations, which are natural in embodied scenarios. To
improve the robustness of DRL agents, we formulate the problem of environmental
state perturbation, introducing a preliminary non-targeted attack method as a
calibration adversary, and then propose a defense framework, named Boosted
Adversarial Training (BAT), which first tunes the agents via supervised
learning to avoid catastrophic failure and subsequently adversarially trains
the agent with reinforcement learning. Extensive experimental results
substantiate the vulnerability of mainstream agents under environmental state
perturbations and the effectiveness of our proposed attack. The defense results
demonstrate that while existing robust reinforcement learning algorithms may
not be suitable, our BAT framework can significantly enhance the robustness of
agents against environmental state perturbations across various situations.

</details>


### [193] [GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO](https://arxiv.org/abs/2506.08965)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Key words: 数据增强, 奖励模型, RLHF, 偏好优化, 少样本学习

TL;DR: 提出了一种数据增强与扩展框架，通过偏好细化和多级直接偏好优化（M-DPO），使少量数据训练的生成奖励模型达到与大规模数据集训练相当的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 提升强化学习从人类反馈（RLHF）中训练高效且可扩展的奖励模型的数据效率。

Method: 采用偏好细化结合Chain-of-Thought采样、基于困惑度的评分机制及多级直接偏好优化（M-DPO）来提升数据多样性与质量。

Result: 实验表明，该方法显著提高了数据效率和模型性能，少量数据训练即可媲美大规模数据集的效果。

Conclusion: 数据高效策略在奖励模型优化中具有潜力，为低资源RLHF应用提供了有效解决方案。

Abstract: The ability to train high-performing reward models with few-shot data is
critical for enhancing the efficiency and scalability of Reinforcement Learning
from Human Feedback (RLHF). We propose a data augmentation and expansion
framework that enables generative reward models trained on small datasets to
achieve comparable performance to those trained on large-scale datasets.
Traditional methods to train a generative reward model, such as Direct
Preference Optimization (DPO), are constrained by inefficiencies in sample
pairing and limited data diversity. This work introduces preference refinement,
which employs Chain-of-Thought (CoT) sampling to uncover diverse and
high-quality preference relationships. It also incorporates a perplexity-based
scoring mechanism to assign nuanced preference levels and utilizes Multi-level
Direct Preference Optimization (M-DPO) to enable the model to capture
finer-grained preference differences between samples. Experimental results
demonstrate that the proposed method significantly enhances data efficiency and
model performance, enabling reward models trained in a few-shot setting to
achieve results on par with those trained on large-scale datasets. This study
underscores the potential of data-efficient strategies in advancing reward
model optimization, offering a robust solution for low-resource RLHF
applications.

</details>


### [194] [Tailored Architectures for Time Series Forecasting: Evaluating Deep Learning Models on Gaussian Process-Generated Data](https://arxiv.org/abs/2506.08977)
*Victoria Hankemeier,Malte Schilling*

Key words: 深度学习, 时间序列预测, 高斯过程, 模块化架构, TimeFlex

TL;DR: 深度学习在时间序列预测中的应用通过更精准建模时序依赖关系提升了性能，但现有研究缺乏对数据特性与模型架构匹配的系统分析。本文提出一种新数据集和方法，揭示数据特性与模型的关系，并介绍模块化模型TimeFlex。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究旨在揭示时间序列特性与模型架构之间的明确联系，填补现有研究在系统性评估上的不足。

Method: 使用高斯过程生成具有已知特性的新数据集，提出模块化架构模型TimeFlex，比较其与现有模型的性能。

Result: TimeFlex在多样时序动态（如趋势、周期性）中表现优于现有模型，证实了其适应性。

Conclusion: 研究明确了数据特性与模型架构的关联，TimeFlex展示了在处理复杂时序数据上的优势。

Abstract: Developments in Deep Learning have significantly improved time series
forecasting by enabling more accurate modeling of complex temporal dependencies
inherent in sequential data. The effectiveness of such models is often
demonstrated on limited sets of specific real-world data. Although this allows
for comparative analysis, it still does not demonstrate how specific data
characteristics align with the architectural strengths of individual models.
Our research aims at uncovering clear connections between time series
characteristics and particular models. We introduce a novel dataset generated
using Gaussian Processes, specifically designed to display distinct, known
characteristics for targeted evaluations of model adaptability to them.
Furthermore, we present TimeFlex, a new model that incorporates a modular
architecture tailored to handle diverse temporal dynamics, including trends and
periodic patterns. This model is compared to current state-of-the-art models,
offering a deeper understanding of how models perform under varied time series
conditions.

</details>


### [195] [Propositional Logic for Probing Generalization in Neural Networks](https://arxiv.org/abs/2506.08978)
*Anna Langedijk,Jaap Jumelet,Willem Zuidema*

Key words: 神经网络,泛化能力,命题逻辑,Transformer,图卷积网络,LSTM,否定,归纳偏向

TL;DR: 研究探讨了神经网络在命题逻辑任务中的泛化能力，发现尽管现有架构在训练分布内表现良好，但对于未见模式的泛化（尤其是涉及否定的情况）仍具挑战性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索神经网络是否能系统性地学习和表示符号规则，尤其是在逻辑运算中的泛化能力。

Method: 在命题逻辑任务中评估Transformer、图卷积网络和LSTM的泛化行为，使用平衡数据集消除表面模式。

Result: 所有模型在训练分布内表现良好，但泛化到未见模式（如否定组合）时表现不佳，除非引入结构偏向。

Conclusion: 标准神经网络架构在系统学习逻辑运算方面存在局限性，需更强的归纳偏向支持基于规则的推理。

Abstract: The extent to which neural networks are able to acquire and represent
symbolic rules remains a key topic of research and debate. Much current work
focuses on the impressive capabilities of large language models, as well as
their often ill-understood failures on a wide range of reasoning tasks. In this
paper, in contrast, we investigate the generalization behavior of three key
neural architectures (Transformers, Graph Convolution Networks and LSTMs) in a
controlled task rooted in propositional logic. The task requires models to
generate satisfying assignments for logical formulas, making it a structured
and interpretable setting for studying compositionality. We introduce a
balanced extension of an existing dataset to eliminate superficial patterns and
enable testing on unseen operator combinations. Using this dataset, we evaluate
the ability of the three architectures to generalize beyond the training
distribution. While all models perform well in-distribution, we find that
generalization to unseen patterns, particularly those involving negation,
remains a significant challenge. Transformers fail to apply negation
compositionally, unless structural biases are introduced. Our findings
highlight persistent limitations in the ability of standard architectures to
learn systematic representations of logical operators, suggesting the need for
stronger inductive biases to support robust rule-based reasoning.

</details>


### [196] [On Finetuning Tabular Foundation Models](https://arxiv.org/abs/2506.08982)
*Ivan Rubachev,Akim Kotelnikov,Nikolay Kartashev*

Key words: TabPFNv2, 表格深度学习, 微调, 基础模型, 检索增强

TL;DR: 本文研究了TabPFNv2模型在小规模数据集上的最佳微调方法，揭示了微调如何通过改进查询和键表示的相似性来提升性能，并在不同数据集上验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索表格深度学习基础模型TabPFNv2的最佳微调方法及其内部机制的变化，填补现有研究的不足。

Method: 系统评估多种微调策略，分析微调如何影响模型内部的查询和键表示相似性，并类比检索增强模型。

Result: 完全微调是TabPFNv2最实用且有效的方法，在最多50K数据的数据集上表现优异，但在时间偏移和丰富特征的数据集上表现不稳定。

Conclusion: 微调通过优化查询和键表示的相似性提升模型性能，适用于小规模数据集，但在复杂或动态数据中仍需改进。

Abstract: Foundation models are an emerging research direction in tabular deep
learning. Notably, TabPFNv2 recently claimed superior performance over
traditional GBDT-based methods on small-scale datasets using an in-context
learning paradigm, which does not adapt model parameters to target datasets.
However, the optimal finetuning approach for adapting tabular foundational
models, and how this adaptation reshapes their internal mechanisms, remains
underexplored. While prior works studied finetuning for earlier foundational
models, inconsistent findings and TabPFNv2's unique architecture necessitate
fresh investigation. To address these questions, we first systematically
evaluate various finetuning strategies on diverse datasets. Our findings
establish full finetuning as the most practical solution for TabPFNv2 in terms
of time-efficiency and effectiveness. We then investigate how finetuning alters
TabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.
We reveal that the success of finetuning stems from the fact that after
gradient-based adaptation, the dot products of the query-representations of
test objects and the key-representations of in-context training objects more
accurately reflect their target similarity. This improved similarity allows
finetuned TabPFNv2 to better approximate target dependency by appropriately
weighting relevant in-context samples, improving the retrieval-based prediction
logic. From the practical perspective, we managed to finetune TabPFNv2 on
datasets with up to 50K objects, observing performance improvements on almost
all tasks. More precisely, on academic datasets with I.I.D. splits, finetuning
allows TabPFNv2 to achieve state-of-the-art results, while on datasets with
gradual temporal shifts and rich feature sets, TabPFNv2 is less stable and
prior methods remain better.

</details>


### [197] [Branched Schrödinger Bridge Matching](https://arxiv.org/abs/2506.09007)
*Sophia Tang,Yinuo Zhang,Alexander Tong,Pranam Chatterjee*

Key words: 生成模型, Schrödinger桥, 分支演化, 细胞命运, 多路径导航

TL;DR: 论文提出了一种名为BranchSBM的新框架，用于学习分支的Schrödinger桥，解决了现有方法在模拟多路径发散演化时的局限性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的生成模型方法（如流匹配和Schrödinger桥匹配）只能模拟单一路径的分布变换，无法捕捉从共同起源到多个不同结果的分支或发散演化。

Method: 通过参数化多个时间相关的速度场和增长过程，BranchSBM框架能够表示种群水平的分支演化，并学习分支Schrödinger桥。

Result: BranchSBM不仅更具表达力，而且在多路径表面导航、模拟细胞命运分叉以及细胞对扰动的不同响应等任务中表现出色。

Conclusion: BranchSBM为多路径发散演化问题提供了一种有效的解决方案，扩展了生成模型的能力范围。

Abstract: Predicting the intermediate trajectories between an initial and target
distribution is a central problem in generative modeling. Existing approaches,
such as flow matching and Schr\"odinger Bridge Matching, effectively learn
mappings between two distributions by modeling a single stochastic path.
However, these methods are inherently limited to unimodal transitions and
cannot capture branched or divergent evolution from a common origin to multiple
distinct outcomes. To address this, we introduce Branched Schr\"odinger Bridge
Matching (BranchSBM), a novel framework that learns branched Schr\"odinger
bridges. BranchSBM parameterizes multiple time-dependent velocity fields and
growth processes, enabling the representation of population-level divergence
into multiple terminal distributions. We show that BranchSBM is not only more
expressive but also essential for tasks involving multi-path surface
navigation, modeling cell fate bifurcations from homogeneous progenitor states,
and simulating diverging cellular responses to perturbations.

</details>


### [198] [Effective Data Pruning through Score Extrapolation](https://arxiv.org/abs/2506.09010)
*Sebastian Schmidt,Prasanga Dhungel,Christoffer Löffler,Björn Nieth,Stephan Günnemann,Leo Schwinn*

Key words: 数据剪枝、重要性分数外推、计算效率、k最近邻、图神经网络

TL;DR: 提出一种新颖的重要性分数外推框架，通过仅训练一小部分数据来预测整个数据集的样本重要性，显著减少了计算成本。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为解决训练高级机器学习模型时的巨大计算成本问题，数据剪枝技术需要在不降低模型性能的情况下移除冗余样本。现有方法通常需要完整初始训练，限制了其效率。

Method: 引入重要性分数外推框架，仅需在小数据子集上训练。提出两种初始方法（k最近邻和图神经网络）以预测整个数据集的样本重要性。

Result: 在两种剪枝方法、四个数据集和三种训练范式下验证了方法的有效性，表明分数外推是可扩展昂贵分数计算方法的有前景方向。

Conclusion: 重要性分数外推框架能够显著减少计算成本，为数据剪枝、数据归因等任务提供高效解决方案。

Abstract: Training advanced machine learning models demands massive datasets, resulting
in prohibitive computational costs. To address this challenge, data pruning
techniques identify and remove redundant training samples while preserving
model performance. Yet, existing pruning techniques predominantly require a
full initial training pass to identify removable samples, negating any
efficiency benefits for single training runs. To overcome this limitation, we
introduce a novel importance score extrapolation framework that requires
training on only a small subset of data. We present two initial approaches in
this framework - k-nearest neighbors and graph neural networks - to accurately
predict sample importance for the entire dataset using patterns learned from
this minimal subset. We demonstrate the effectiveness of our approach for 2
state-of-the-art pruning methods (Dynamic Uncertainty and TDDS), 4 different
datasets (CIFAR-10, CIFAR-100, Places-365, and ImageNet), and 3 training
paradigms (supervised, unsupervised, and adversarial). Our results indicate
that score extrapolation is a promising direction to scale expensive score
calculation methods, such as pruning, data attribution, or other tasks.

</details>


### [199] [SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning](https://arxiv.org/abs/2506.09016)
*Ruiqi Zhang,Daman Arora,Song Mei,Andrea Zanette*

Key words: 强化学习,训练效率,中等难度提示词,自适应采样

TL;DR: SPEED通过选择性采样中等难度提示词，显著提升训练效率，加速收敛，无需手动调参，且兼容标准RL算法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统方法均匀采样提示词效率低，希望通过自适应方法选择中等难度样本以优化学习效率。

Method: 提出SPEED方法，选择性使用中等难度提示词，理论证明其提升梯度估计的信噪比。

Result: 实证显示SPEED实现2到6倍训练加速，且不降低准确性。

Conclusion: SPEED是一种高效、自适应且无需手动调参的RL训练方法。

Abstract: Training large language models with reinforcement learning (RL) against
verifiable rewards significantly enhances their reasoning abilities, yet
remains computationally expensive due to inefficient uniform prompt sampling.
We introduce Selective Prompting with Efficient Estimation of Difficulty
(SPEED), an adaptive online RL curriculum that selectively chooses training
examples of intermediate difficulty to maximize learning efficiency.
Theoretically, we establish that intermediate-difficulty prompts improve the
gradient estimator's signal-to-noise ratio, accelerating convergence.
Empirically, our efficient implementation leads to 2x to 6x faster training
without degrading accuracy, requires no manual tuning, and integrates
seamlessly into standard RL algorithms.

</details>


### [200] [Edit Flows: Flow Matching with Edit Operations](https://arxiv.org/abs/2506.09018)
*Marton Havasi,Brian Karrer,Itai Gat,Ricky T. Q. Chen*

Key words: 非自回归模型, Edit Flows, 连续时间马尔可夫链, 序列生成, 编辑操作

TL;DR: Edit Flows是一种非自回归模型，通过编辑操作（插入、删除和替换）在序列空间上定义离散流，克服了传统非自回归模型的局限性，并在图像描述、文本和代码生成任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 自回归模型能够自然地生成变长序列，而非自回归模型通常需要强加固定的token结构，限制了其灵活性。Edit Flows旨在通过编辑操作解决这一问题，提供更灵活的序列生成方式。

Method: Edit Flows基于连续时间马尔可夫链在序列空间上定义编辑操作（插入、删除和替换）。训练方法通过扩展状态空间和辅助变量提高了效率和可操作性。

Result: 实验结果表明，Edit Flows在图像描述任务中优于自回归和遮罩模型，在文本和代码生成任务中显著优于遮罩构建方法。

Conclusion: Edit Flows通过编辑操作实现了更灵活的序列生成，解决了非自回归模型的局限性，并在多个任务中取得了优异性能。

Abstract: Autoregressive generative models naturally generate variable-length
sequences, while non-autoregressive models struggle, often imposing rigid,
token-wise structures. We propose Edit Flows, a non-autoregressive model that
overcomes these limitations by defining a discrete flow over sequences through
edit operations-insertions, deletions, and substitutions. By modeling these
operations within a Continuous-time Markov Chain over the sequence space, Edit
Flows enable flexible, position-relative generation that aligns more closely
with the structure of sequence data. Our training method leverages an expanded
state space with auxiliary variables, making the learning process efficient and
tractable. Empirical results show that Edit Flows outperforms both
autoregressive and mask models on image captioning and significantly
outperforms the mask construction in text and code generation.

</details>


### [201] [FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed](https://arxiv.org/abs/2506.09034)
*Sizhe Dang,Yangyang Guo,Yanjun Zhao,Haishan Ye,Xiaodong Zheng,Guang Dai,Ivor Tsang*

Key words: FZOO, 零阶优化器, 内存效率, 微调, 大型语言模型

TL;DR: FZOO是一种快速零阶优化器，通过批量估计和自适应步长策略显著减少收敛所需的前向传递次数，同时保持与Adam相当的收敛速度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决大型语言模型微调中的GPU内存瓶颈问题，同时避免传统零阶优化器收敛慢的缺点。

Method: FZOO采用批量单边估计和自适应步长策略，并结合Rademacher随机向量扰动和CUDA并行处理。

Result: 实验表明，FZOO在多种模型和任务中表现优于MeZO，收敛速度和内存效率显著提升。

Conclusion: FZOO实现了单GPU高速全参数微调的实用性，并为未来高效内存预训练提供了方向。

Abstract: Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks:
the backward pass of first-order optimizers like Adam increases memory usage to
more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order
(ZO) optimizers avoid this cost by estimating gradients only from forward
passes, yet existing methods like MeZO usually require many more steps to
converge. Can this trade-off between speed and memory in ZO be fundamentally
improved? Normalized-SGD demonstrates strong empirical performance with greater
memory efficiency than Adam. In light of this, we introduce FZOO, a Fast
Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward
passes needed for convergence by employing batched one-sided estimates that
adapt step sizes based on the standard deviation of batch losses. It also
accelerates per-batch computation through the use of Rademacher random vector
perturbations coupled with CUDA's parallel processing. Extensive experiments on
diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3,
across 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms
MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For
RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy
and an 18 times reduction in forward passes compared to MeZO, achieving
convergence speeds comparable to Adam. We also provide theoretical analysis
proving FZOO's formal equivalence to a normalized-SGD update rule and its
convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling
even larger memory savings. Overall, our results make single-GPU, high-speed,
full-parameter fine-tuning practical and point toward future work on
memory-efficient pre-training.

</details>


### [202] [The Decoupled Risk Landscape in Performative Prediction](https://arxiv.org/abs/2506.09044)
*Javier Sanguino,Thomas Kehrenberg,Jose A. Lozano,Novi Quadrianto*

Key words: Performative Prediction, 损失景观, 可视化, 分布偏移, 战略分类

TL;DR: 论文提出了一种可视化方法，用于展示Performative Prediction中的损失景观，并引入扩展的Performative Prediction概念，以更真实地反映模型部署对数据分布的影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 填补理论研究与实际应用之间的差距，通过可视化方法提供对Performative Prediction的直观理解。

Method: 1. 引入解耦风险可视化方法，展示模型参数与数据参数的关系；2. 提出扩展的Performative Prediction，模拟代理无法完全访问部署模型的情景。

Result: 通过可视化方法揭示了兴趣点的新属性，并为现有算法在更真实条件下的表现提供了分析工具。

Conclusion: 可视化方法为理论研究提供了实用补充，扩展的Performative Prediction更贴近现实场景。

Abstract: Performative Prediction addresses scenarios where deploying a model induces a
distribution shift in the input data, such as individuals modifying their
features and reapplying for a bank loan after rejection. Literature has had a
theoretical perspective giving mathematical guarantees for convergence (either
to the stable or optimal point). We believe that visualization of the loss
landscape can complement this theoretical advances with practical insights.
Therefore, (1) we introduce a simple decoupled risk visualization method
inspired in the two-step process that performative prediction is. Our approach
visualizes the risk landscape with respect to two parameter vectors: model
parameters and data parameters. We use this method to propose new properties of
the interest points, to examine how existing algorithms traverse the risk
landscape and perform under more realistic conditions, including strategic
classification with non-linear models. (2) Building on this decoupled risk
visualization, we introduce a novel setting - extended Performative Prediction
- which captures scenarios where the distribution reacts to a model different
from the decision-making one, reflecting the reality that agents often lack
full access to the deployed model.

</details>


### [203] [Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation](https://arxiv.org/abs/2506.09046)
*Xiaowen Ma,Chenyang Lin,Yao Zhang,Volker Tresp,Yunpu Ma*

Key words: 多智能体系统、神经网络、动态协作、任务分解、优化策略

TL;DR: Agentic Neural Network (ANN) 框架通过分层神经网络架构实现多智能体动态协作，利用前向和反向优化策略提升任务分解和协作效率，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前多智能体协作系统依赖静态配置，无法动态适应复杂任务需求，限制了性能和灵活性。

Method: ANN 将多智能体协作建模为分层神经网络，前向阶段动态分解任务并构建协作团队，反向阶段通过迭代反馈优化协作和角色调整。

Result: 在四个基准数据集上，ANN 表现优于同类方法，展示了更高的准确性和适应性。

Conclusion: ANN 提供了一个可扩展、数据驱动的多智能体协作框架，未来计划开源。

Abstract: Leveraging multiple Large Language Models(LLMs) has proven effective for
addressing complex, high-dimensional tasks, but current approaches often rely
on static, manually engineered multi-agent configurations. To overcome these
constraints, we present the Agentic Neural Network(ANN), a framework that
conceptualizes multi-agent collaboration as a layered neural network
architecture. In this design, each agent operates as a node, and each layer
forms a cooperative "team" focused on a specific subtask. Agentic Neural
Network follows a two-phase optimization strategy: (1) Forward Phase-Drawing
inspiration from neural network forward passes, tasks are dynamically
decomposed into subtasks, and cooperative agent teams with suitable aggregation
methods are constructed layer by layer. (2) Backward Phase-Mirroring
backpropagation, we refine both global and local collaboration through
iterative feedback, allowing agents to self-evolve their roles, prompts, and
coordination. This neuro-symbolic approach enables ANN to create new or
specialized agent teams post-training, delivering notable gains in accuracy and
adaptability. Across four benchmark datasets, ANN surpasses leading multi-agent
baselines under the same configurations, showing consistent performance
improvements. Our findings indicate that ANN provides a scalable, data-driven
framework for multi-agent systems, combining the collaborative capabilities of
LLMs with the efficiency and flexibility of neural network principles. We plan
to open-source the entire framework.

</details>


### [204] [Understanding Task Vectors in In-Context Learning: Emergence, Functionality, and Limitations](https://arxiv.org/abs/2506.09048)
*Yuxin Dong,Jiachen Jiang,Zhihui Zhu,Xia Ning*

Key words: 任务向量，上下文学习，线性组合，Transformer，高秩映射

TL;DR: 通过分析和实验支持，本文提出了任务向量通过原始示例的线性组合形成的猜想，并验证了其在高秩映射中的局限性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索任务向量在上下文学习（ICL）中的作用机制，揭示其形成原理及功能限制。

Method: 提出线性组合猜想，通过理论分析和实验（如损失景观分析、显著性分析和参数可视化）验证。

Result: 任务向量在线性变换器中自然形成，但在高秩映射中表现不佳；通过注入多个任务向量可优化其效果。

Conclusion: 研究深化了对任务向量和ICL机制的理解，为优化其在Transformer模型中的应用提供了方向。

Abstract: Task vectors offer a compelling mechanism for accelerating inference in
in-context learning (ICL) by distilling task-specific information into a
single, reusable representation. Despite their empirical success, the
underlying principles governing their emergence and functionality remain
unclear. This work proposes the Linear Combination Conjecture, positing that
task vectors act as single in-context demonstrations formed through linear
combinations of the original ones. We provide both theoretical and empirical
support for this conjecture. First, we show that task vectors naturally emerge
in linear transformers trained on triplet-formatted prompts through loss
landscape analysis. Next, we predict the failure of task vectors on
representing high-rank mappings and confirm this on practical LLMs. Our
findings are further validated through saliency analyses and parameter
visualization, suggesting an enhancement of task vectors by injecting multiple
ones into few-shot prompts. Together, our results advance the understanding of
task vectors and shed light on the mechanisms underlying ICL in
transformer-based models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [205] [TIP-Search: Time-Predictable Inference Scheduling for Market Prediction under Uncertain Load](https://arxiv.org/abs/2506.08026)
*Xibai Wang*

Key words: 实时预测,深度学习,延迟优化,金融市场

TL;DR: TIP-Search是一个实时市场预测框架，满足严格延迟需求，通过动态选择深度学习模型，提高预测准确性并满足任务截止时间。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 高频金融系统对延迟有严格要求，TIP-Search旨在满足这些需求。

Method: 离线分析延迟和泛化性能，在线动态选择模型。

Result: 在三个真实数据集上表现优于静态基线，准确性提升8.5%，100%满足截止时间。

Conclusion: TIP-Search在不确定性下实现低延迟金融推断。

Abstract: This paper proposes TIP-Search, a time-predictable inference scheduling
framework for real-time market prediction under uncertain workloads. Motivated
by the strict latency demands in high-frequency financial systems, TIP-Search
dynamically selects a deep learning model from a heterogeneous pool, aiming to
maximize predictive accuracy while satisfying per-task deadline constraints.
Our approach profiles latency and generalization performance offline, then
performs online task-aware selection without relying on explicit input domain
labels. We evaluate TIP-Search on three real-world limit order book datasets
(FI-2010, Binance BTC/USDT, LOBSTER AAPL) and demonstrate that it outperforms
static baselines with up to 8.5% improvement in accuracy and 100% deadline
satisfaction. Our results highlight the effectiveness of TIP-Search in robust
low-latency financial inference under uncertainty.

</details>


### [206] [Cognitive Weave: Synthesizing Abstracted Knowledge with a Spatio-Temporal Resonance Graph](https://arxiv.org/abs/2506.08098)
*Akash Vishwakarma,Hojin Lee,Mohith Suresh,Priyam Shankar Sharma,Rahul Vishwakarma,Sparsh Gupta,Yuvraj Anupam Chauhan*

Key words: 大语言模型、记忆框架、时空共振图、认知精炼、伦理考量

TL;DR: 论文提出了Cognitive Weave，一种基于多层时空共振图的新型记忆框架，显著提升了LLM代理的连续学习、推理和动态适应能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前记忆系统在结构灵活性、时间意识和从原始数据中提炼高级见解方面存在局限，需要一种更先进的记忆架构来支持LLM代理的复杂需求。

Method: 设计了多层次的时空共振图（STRG），通过语义丰富的信息粒子（IPs）及其动态关联，结合认知精炼过程生成高级知识结构（IAs）。

Result: 实验表明，Cognitive Weave在任务完成率上平均提升34%，查询延迟降低42%，优于现有方法。

Conclusion: 论文展示了Cognitive Weave的优越性，并探讨了其伦理意义及未来研究方向。

Abstract: The emergence of capable large language model (LLM) based agents necessitates
memory architectures that transcend mere data storage, enabling continuous
learning, nuanced reasoning, and dynamic adaptation. Current memory systems
often grapple with fundamental limitations in structural flexibility, temporal
awareness, and the ability to synthesize higher-level insights from raw
interaction data. This paper introduces Cognitive Weave, a novel memory
framework centered around a multi-layered spatio-temporal resonance graph
(STRG). This graph manages information as semantically rich insight particles
(IPs), which are dynamically enriched with resonance keys, signifiers, and
situational imprints via a dedicated semantic oracle interface (SOI). These IPs
are interconnected through typed relational strands, forming an evolving
knowledge tapestry. A key component of Cognitive Weave is the cognitive
refinement process, an autonomous mechanism that includes the synthesis of
insight aggregates (IAs) condensed, higher-level knowledge structures derived
from identified clusters of related IPs. We present comprehensive experimental
results demonstrating Cognitive Weave's marked enhancement over existing
approaches in long-horizon planning tasks, evolving question-answering
scenarios, and multi-session dialogue coherence. The system achieves a notable
34% average improvement in task completion rates and a 42% reduction in mean
query latency when compared to state-of-the-art baselines. Furthermore, this
paper explores the ethical considerations inherent in such advanced memory
systems, discusses the implications for long-term memory in LLMs, and outlines
promising future research trajectories.

</details>


### [207] [SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents](https://arxiv.org/abs/2506.08119)
*Subhrangshu Nandi,Arghya Datta,Nikhil Vichare,Indranil Bhattacharya,Huzefa Raja,Jing Xu,Shayan Ray,Giuseppe Carenini,Abhi Srivastava,Aaron Chan,Man Ho Woo,Amar Kandola,Brandon Theresa,Francesco Carbone*

Key words: LLM, 标准操作程序, 工业自动化, 基准测试, SOP-Bench

TL;DR: 论文介绍了SOP-Bench，一个针对LLM在工业自动化中执行复杂标准操作程序（SOPs）能力的基准测试，揭示当前LLM与实际需求间的显著差距。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: LLM在工业自动化中执行复杂SOPs时表现不佳，缺乏反映SOPs复杂性的公共基准，因此需要开发相关测试工具。

Method: 提出合成数据生成框架，创建工业级SOPs任务，并开发SOP-Bench基准；评估两种LLM代理架构。

Result: Function-Calling和ReAct Agents的平均成功率分别为27%和48%，工具数量过多时错误率接近100%。

Conclusion: LLM在自动化SOPs方面的能力与实际需求存在较大差距，需域特定基准和架构优化。

Abstract: Large Language Models (LLMs) demonstrate impressive general-purpose reasoning
and problem-solving abilities. However, they struggle with executing complex,
long-horizon workflows that demand strict adherence to Standard Operating
Procedures (SOPs), a critical requirement for real-world industrial automation.
Despite this need, there is a lack of public benchmarks that reflect the
complexity, structure, and domain-specific nuances of SOPs. To address this, we
present three main contributions. First, we introduce a synthetic data
generation framework to create realistic, industry-grade SOPs that rigorously
test the planning, reasoning, and tool-use capabilities of LLM-based agents.
Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800
tasks across 10 industrial domains, each with APIs, tool interfaces, and
human-validated test cases. Third, we evaluate two prominent agent
architectures: Function-Calling and ReAct Agents, on SOP-Bench, observing
average success rates of only 27% and 48%, respectively. Remarkably, when the
tool registry is much larger than necessary, agents invoke incorrect tools
nearly 100% of the time. These findings underscore a substantial gap between
current agentic capabilities of LLMs and the demands of automating real-world
SOPs. Performance varies significantly by task and domain, highlighting the
need for domain-specific benchmarking and architectural choices before
deployment. SOP-Bench is publicly available at
http://sop-bench.s3-website-us-west-2.amazonaws.com/. We also release the
prompts underpinning the data generation framework to support new
domain-specific SOP benchmarks. We invite the community to extend SOP-Bench
with SOPs from their industrial domains.

</details>


### [208] [The AI Imperative: Scaling High-Quality Peer Review in Machine Learning](https://arxiv.org/abs/2506.08134)
*Qiyao Wei,Samuel Holt,Jing Yang,Markus Wulfmeier,Mihaela van der Schaar*

Key words: peer review, AI-assisted, large language models, ethical challenges, scalability

TL;DR: 论文呼吁通过AI辅助解决机器学习领域同行评审的规模危机，提出LLMs作为协作工具的具体应用及研究议程。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 由于论文投稿数量激增，同行评审面临质量、一致性和评审疲劳等问题，AI辅助评审成为迫切需求。

Method: 提出构建AI辅助生态系统，利用LLMs支持作者、审稿人和领域主席，包括事实验证、审稿指导等功能。

Result: 未提供具体实验结果，但提出了研究方向和技术挑战。

Conclusion: AI辅助评审是保障科学验证完整性和可扩展性的关键，需社区积极推动。

Abstract: Peer review, the bedrock of scientific advancement in machine learning (ML),
is strained by a crisis of scale. Exponential growth in manuscript submissions
to premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite
capacity of qualified reviewers, leading to concerns about review quality,
consistency, and reviewer fatigue. This position paper argues that AI-assisted
peer review must become an urgent research and infrastructure priority. We
advocate for a comprehensive AI-augmented ecosystem, leveraging Large Language
Models (LLMs) not as replacements for human judgment, but as sophisticated
collaborators for authors, reviewers, and Area Chairs (ACs). We propose
specific roles for AI in enhancing factual verification, guiding reviewer
performance, assisting authors in quality improvement, and supporting ACs in
decision-making. Crucially, we contend that the development of such systems
hinges on access to more granular, structured, and ethically-sourced peer
review process data. We outline a research agenda, including illustrative
experiments, to develop and validate these AI assistants, and discuss
significant technical and ethical challenges. We call upon the ML community to
proactively build this AI-assisted future, ensuring the continued integrity and
scalability of scientific validation, while maintaining high standards of peer
review.

</details>


### [209] [Compiling Metric Temporal Answer Set Programming](https://arxiv.org/abs/2506.08150)
*Arvid Becker,Pedro Cabalar,Martin Diéguez,Javier Romero,Susana Hahn,Torsten Schaub*

Key words: 度量ASP, 定量时间约束, 差异约束, 可扩展性

TL;DR: 提出了一种计算方法来扩展度量ASP，以支持定量时间约束，同时通过外部处理时间相关方面来解决扩展性问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决度量ASP中定量时间约束（如持续时间和截止日期）的表达问题，同时避免因时间粒度细化导致的扩展性问题。

Method: 利用ASP的扩展形式，结合差异约束（一种简化的线性约束）来外部处理时间相关因素，从而解耦时间粒度与度量ASP的关系。

Result: 提出的方法不受时间精度影响，有效解决了度量ASP的扩展性问题。

Conclusion: 该方法成功实现了定量时间约束的表达，同时保持了ASP的可扩展性。

Abstract: We develop a computational approach to Metric Answer Set Programming (ASP) to
allow for expressing quantitative temporal constrains, like durations and
deadlines. A central challenge is to maintain scalability when dealing with
fine-grained timing constraints, which can significantly exacerbate ASP's
grounding bottleneck. To address this issue, we leverage extensions of ASP with
difference constraints, a simplified form of linear constraints, to handle
time-related aspects externally. Our approach effectively decouples metric ASP
from the granularity of time, resulting in a solution that is unaffected by
time precision.

</details>


### [210] [AstroCompress: A benchmark dataset for multi-purpose compression of astronomical data](https://arxiv.org/abs/2506.08306)
*Tuan Truong,Rithwik Sudharsan,Yibo Yang,Peter Xiangyuan Ma,Ruihan Yang,Stephan Mandt,Joshua S. Bloom*

Key words: AstroCompress, 神经压缩, 无损压缩, 天文数据, 数据集

TL;DR: 这篇论文介绍了AstroCompress，一种针对天体物理学数据的神经压缩挑战，提出了新的数据集和压缩方法，结果表明神经压缩技术可以提高天文台的数据收集效率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现代天文台的数据传输能力受限，传统无损压缩方法需要手动设计，神经压缩技术有望通过学习数据中的时空和波长结构提供更高效的解决方案。

Method: 论文提出了AstroCompress挑战，包括四个新数据集和一个旧数据集，并比较了三种神经压缩方法和四种非神经压缩方法，评估其性能。

Result: 结果表明，无损神经压缩技术可以提升天文台的数据收集能力，并为科学应用中采用神经压缩提供了指导。

Conclusion: 神经压缩技术在无损压缩中表现优越，未来还值得探索有损压缩方法。

Abstract: The site conditions that make astronomical observatories in space and on the
ground so desirable -- cold and dark -- demand a physical remoteness that leads
to limited data transmission capabilities. Such transmission limitations
directly bottleneck the amount of data acquired and in an era of costly modern
observatories, any improvements in lossless data compression has the potential
scale to billions of dollars worth of additional science that can be
accomplished on the same instrument. Traditional lossless methods for
compressing astrophysical data are manually designed. Neural data compression,
on the other hand, holds the promise of learning compression algorithms
end-to-end from data and outperforming classical techniques by leveraging the
unique spatial, temporal, and wavelength structures of astronomical images.
This paper introduces AstroCompress: a neural compression challenge for
astrophysics data, featuring four new datasets (and one legacy dataset) with
16-bit unsigned integer imaging data in various modes: space-based,
ground-based, multi-wavelength, and time-series imaging. We provide code to
easily access the data and benchmark seven lossless compression methods (three
neural and four non-neural, including all practical state-of-the-art
algorithms). Our results on lossless compression indicate that lossless neural
compression techniques can enhance data collection at observatories, and
provide guidance on the adoption of neural compression in scientific
applications. Though the scope of this paper is restricted to lossless
compression, we also comment on the potential exploration of lossy compression
methods in future studies.

</details>


### [211] [LeanTutor: A Formally-Verified AI Tutor for Mathematical Proofs](https://arxiv.org/abs/2506.08321)
*Manooshree Patel,Rayna Bhattacharyya,Thomas Lu,Arnav Mehta,Niels Voss,Narges Norouzi,Gireeja Ranade*

Key words: LeanTutor, LLM, 数学证明, 辅导系统, Lean

TL;DR: LeanTutor是一个基于大型语言模型的数学证明辅导系统，能够通过自然语言交互、形式化验证和生成下一步指导来辅助学生完成数学证明。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 开发一个能够与学生自然互动，并有效辅导数学证明的系统。

Method: 系统包含三个模块：自动形式化/验证器、下一步生成器和自然语言反馈生成器，利用Lean语言和LLM技术实现。

Result: Autoformalizer在正确证明中能形式化57%的策略，在错误证明中能识别30%的错误步骤；反馈生成器在准确性和相关性上优于简单基线。

Conclusion: LeanTutor在数学证明辅导中表现出色，尤其在自然语言反馈方面有优势。

Abstract: We present LeanTutor, a Large Language Model (LLM)-based tutoring system for
math proofs. LeanTutor interacts with the student in natural language, formally
verifies student-written math proofs in Lean, generates correct next steps, and
provides the appropriate instructional guidance. LeanTutor is composed of three
modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and
(iii) a natural language feedback generator. The first module faithfully
autoformalizes student proofs into Lean and verifies proof accuracy via
successful code compilation. If the proof has an error, the incorrect step is
identified. The next-step generator module outputs a valid next Lean tactic for
incorrect proofs via LLM-based candidate generation and proof search. The
feedback generator module leverages Lean data to produce a
pedagogically-motivated natural language hint for the student user. To evaluate
our system, we introduce PeanoBench, a human-written dataset derived from the
Natural Numbers Game, consisting of 371 Peano Arithmetic proofs, where each
natural language proof step is paired with the corresponding logically
equivalent tactic in Lean. The Autoformalizer correctly formalizes 57% of
tactics in correct proofs and accurately identifies the incorrect step in 30%
of incorrect proofs. In generating natural language hints for erroneous proofs,
LeanTutor outperforms a simple baseline on accuracy and relevance metrics.

</details>


### [212] [ORFS-agent: Tool-Using Agents for Chip Design Optimization](https://arxiv.org/abs/2506.08332)
*Amur Ghose,Andrew B. Kahng,Sayak Kundu,Zhiang Wang*

Key words: 机器学习, 集成电路设计, 大型语言模型, 参数优化, 多目标优化

TL;DR: 论文介绍了一种基于大型语言模型（LLM）的优化代理ORFS-agent，用于自动化集成电路设计中的参数调优，相比传统方法在资源效率和设计指标上表现更优。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 集成电路设计流程中，参数配置复杂且对性能影响大，LLM的进展为高维优化任务提供了新思路。

Method: 提出ORFS-agent，一种基于LLM的迭代优化代理，自适应探索参数配置。

Result: 在两种技术节点和多个电路基准测试中，ORFS-agent能提升13%以上的布线长度和时钟周期，同时减少40%的优化迭代次数。

Conclusion: ORFS-agent提供了一个灵活且可解释的多目标优化框架，具有模块化和模型无关性。

Abstract: Machine learning has been widely used to optimize complex engineering
workflows across numerous domains. In the context of integrated circuit design,
modern flows (e.g., going from a register-transfer level netlist to physical
layouts) involve extensive configuration via thousands of parameters, and small
changes to these parameters can have large downstream impacts on desired
outcomes - namely design performance, power, and area. Recent advances in Large
Language Models (LLMs) offer new opportunities for learning and reasoning
within such high-dimensional optimization tasks. In this work, we introduce
ORFS-agent, an LLM-based iterative optimization agent that automates parameter
tuning in an open-source hardware design flow. ORFS-agent adaptively explores
parameter configurations, demonstrating clear improvements over standard
Bayesian optimization approaches in terms of resource efficiency and final
design metrics. Our empirical evaluations on two different technology nodes and
a range of circuit benchmarks indicate that ORFS-agent can improve both routed
wirelength and effective clock period by over 13%, all while using 40% fewer
optimization iterations. Moreover, by following natural language objectives to
trade off certain metrics for others, ORFS-agent demonstrates a flexible and
interpretable framework for multi-objective optimization. Crucially, RFS-agent
is modular and model-agnostic, and can be plugged in to any frontier LLM
without any further fine-tuning.

</details>


### [213] [FloorplanMAE:A self-supervised framework for complete floorplan generation from partial inputs](https://arxiv.org/abs/2506.08363)
*Jun Yin,Jing Zhong,Pengyu Zeng,Peilin Li,Miao Zhang,Ran Luo,Shuai Lu*

Key words: 平面图设计, Masked Autoencoders, Vision Transformer, 自监督学习

TL;DR: 论文提出了一种名为FloorplanMAE的自监督学习框架，用于从不完整的平面图预测完整的平面图，通过Masked Autoencoders和Vision Transformer技术实现高效重构。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 在建筑设计过程中，动态和迭代的平面图设计是常见需求。快速生成完整平面图的预测能力可以提升设计效率并减少重复修改的工作量。

Method: 1. 构建了针对建筑平面图的FloorplanNet数据集；2. 基于Masked Autoencoders和Vision Transformer提出了一种平面图重构方法。

Result: FloorplanMAE能够从不完整的局部平面图生成高质量的完整平面图，实验结果优于现有基准。

Conclusion: FloorplanMAE为平面图生成提供了一个可扩展的解决方案，具有广泛的应用前景。

Abstract: In the architectural design process, floorplan design is often a dynamic and
iterative process. Architects progressively draw various parts of the floorplan
according to their ideas and requirements, continuously adjusting and refining
throughout the design process. Therefore, the ability to predict a complete
floorplan from a partial one holds significant value in the design process.
Such prediction can help architects quickly generate preliminary designs,
improve design efficiency, and reduce the workload associated with repeated
modifications. To address this need, we propose FloorplanMAE, a self-supervised
learning framework for restoring incomplete floor plans into complete ones.
First, we developed a floor plan reconstruction dataset, FloorplanNet,
specifically trained on architectural floor plans. Secondly, we propose a floor
plan reconstruction method based on Masked Autoencoders (MAE), which
reconstructs missing parts by masking sections of the floor plan and training a
lightweight Vision Transformer (ViT). We evaluated the reconstruction accuracy
of FloorplanMAE and compared it with state-of-the-art benchmarks. Additionally,
we validated the model using real sketches from the early stages of
architectural design. Experimental results show that the FloorplanMAE model can
generate high-quality complete floor plans from incomplete partial plans. This
framework provides a scalable solution for floor plan generation, with broad
application prospects.

</details>


### [214] [On Reasoning Strength Planning in Large Reasoning Models](https://arxiv.org/abs/2506.08390)
*Leheng Sheng,An Zhang,Zijian Wu,Weixiang Zhao,Changshuo Shen,Yi Zhang,Xiang Wang,Tat-Seng Chua*

Key words: 大型推理模型,推理强度,模型激活,预设向量,推理长度

TL;DR: 研究表明大型推理模型能根据问题难度自动分配推理强度，并通过激活中的预设向量调控推理长度。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索大型推理模型自动分配推理强度的内在机制。

Method: 从模型激活角度分析问题，使用线性探针预测推理强度，并识别调控推理长度的预设向量。

Result: 发现预设向量调控推理长度，且可应用于检测过度推理和优化简单问题的推理效率。

Conclusion: 研究揭示了大型推理模型的内部推理机制，并提供了控制其推理行为的实用工具。

Abstract: Recent studies empirically reveal that large reasoning models (LRMs) can
automatically allocate more reasoning strengths (i.e., the number of reasoning
tokens) for harder problems, exhibiting difficulty-awareness for better task
performance. While this automatic reasoning strength allocation phenomenon has
been widely observed, its underlying mechanism remains largely unexplored. To
this end, we provide explanations for this phenomenon from the perspective of
model activations. We find evidence that LRMs pre-plan the reasoning strengths
in their activations even before generation, with this reasoning strength
causally controlled by the magnitude of a pre-allocated directional vector.
Specifically, we show that the number of reasoning tokens is predictable solely
based on the question activations using linear probes, indicating that LRMs
estimate the required reasoning strength in advance. We then uncover that LRMs
encode this reasoning strength through a pre-allocated directional vector
embedded in the activations of the model, where the vector's magnitude
modulates the reasoning strength. Subtracting this vector can lead to reduced
reasoning token number and performance, while adding this vector can lead to
increased reasoning token number and even improved performance. We further
reveal that this direction vector consistently yields positive reasoning length
prediction, and it modifies the logits of end-of-reasoning token </think> to
affect the reasoning length. Finally, we demonstrate two potential applications
of our findings: overthinking behavior detection and enabling efficient
reasoning on simple problems. Our work provides new insights into the internal
mechanisms of reasoning in LRMs and offers practical tools for controlling
their reasoning behaviors. Our code is available at
https://github.com/AlphaLab-USTC/LRM-plans-CoT.

</details>


### [215] [SafeCoT: Improving VLM Safety with Minimal Reasoning](https://arxiv.org/abs/2506.08399)
*Jiachen Ma,Zhanhui Zhou,Chao Yang,Chaochao Lu*

Key words: 视觉语言模型, 安全性, 链式思维, 拒绝行为, 泛化能力

TL;DR: SafeCoT是一个轻量级、可解释的框架，通过基于规则的链式思维监督改进视觉语言模型（VLMs）的拒绝行为，显著减少过度拒绝并提升泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 在高风险或模糊场景中，确保视觉语言模型（VLMs）的安全和适当响应是重要挑战。

Method: 利用规则引导的链式思维（CoT）监督，仅需少量监督即可帮助模型评估安全风险并做出上下文感知的拒绝。

Result: 实验表明，SafeCoT显著减少了过度拒绝并提高了泛化能力，即使在训练数据有限的情况下也表现优异。

Conclusion: SafeCoT为对齐安全关键目标的VLMs提供了一种可扩展的解决方案。

Abstract: Ensuring safe and appropriate responses from vision-language models (VLMs)
remains a critical challenge, particularly in high-risk or ambiguous scenarios.
We introduce SafeCoT, a lightweight, interpretable framework that leverages
rule-based chain-of-thought (CoT) supervision to improve refusal behavior in
VLMs. Unlike prior methods that rely on large-scale safety annotations or
complex modeling, SafeCoT uses minimal supervision to help models reason about
safety risks and make context-aware refusals. Experiments across multiple
benchmarks show that SafeCoT significantly reduces overrefusal and enhances
generalization, even with limited training data. Our approach offers a scalable
solution for aligning VLMs with safety-critical objectives.

</details>


### [216] [Single-Node Trigger Backdoor Attacks in Graph-Based Recommendation Systems](https://arxiv.org/abs/2506.08401)
*Runze Li,Di Jin,Xiaobao Wang,Dongxiao He,Bingdao Feng,Zhen Wang*

Key words: 推荐系统, 图后门攻击, 隐蔽性, 单节点触发, 目标曝光率

TL;DR: 该论文提出了一种新型的图后门攻击方法，通过仅插入一个虚假用户节点，以隐蔽方式提升目标用户对目标项目的曝光率，同时减少对推荐系统性能的影响。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的推荐系统在面对攻击时表现出脆弱性，传统攻击方法存在隐蔽性低和破坏性高的问题。本文旨在提出一种更隐蔽且低破坏性的攻击方法。

Method: 设计了单节点触发生成器，仅通过插入一个虚假用户节点来暴露多个目标项目；并引入目标节点与无关节点之间的约束条件，以减少虚假节点对系统性能的影响。

Result: 实验结果显示，99%的目标用户对目标项目的曝光率不低于50%，同时对推荐系统性能的影响控制在约5%以内。

Conclusion: 该方法在隐蔽性和系统性能影响方面优于传统攻击策略，展示了图后门攻击的有效性和潜在威胁。

Abstract: Graph recommendation systems have been widely studied due to their ability to
effectively capture the complex interactions between users and items. However,
these systems also exhibit certain vulnerabilities when faced with attacks. The
prevailing shilling attack methods typically manipulate recommendation results
by injecting a large number of fake nodes and edges. However, such attack
strategies face two primary challenges: low stealth and high destructiveness.
To address these challenges, this paper proposes a novel graph backdoor attack
method that aims to enhance the exposure of target items to the target user in
a covert manner, without affecting other unrelated nodes. Specifically, we
design a single-node trigger generator, which can effectively expose multiple
target items to the target user by inserting only one fake user node.
Additionally, we introduce constraint conditions between the target nodes and
irrelevant nodes to mitigate the impact of fake nodes on the recommendation
system's performance. Experimental results show that the exposure of the target
items reaches no less than 50% in 99% of the target users, while the impact on
the recommendation system's performance is controlled within approximately 5%.

</details>


### [217] [Transforming Expert Knowledge into Scalable Ontology via Large Language Models](https://arxiv.org/abs/2506.08422)
*Ikkei Itoku,David Theil,Evelyn Eichelsdoerfer Uehara,Sreyoshi Bhaduri,Junnosuke Kuroda,Toshi Yumoto,Alex Gil,Natalie Perez,Rajesh Cherukuri,Naumaan Nayyar*

Key words: 分类法对齐,大型语言模型,专家校准,自动化,语义关系

TL;DR: 提出一种结合大型语言模型(LLMs)与专家校准的新框架,通过自动化分类法对齐解决传统方法的局限性,显著提升了映射质量。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统分类法对齐方法依赖专家手动处理,成本高且容易产生分歧;现有自动化方法在处理语义关系和跨领域一致性上存在不足。

Method: 结合LLMs与专家校准,通过多阶段提示工程和人类验证生成分类法链接及支持理由。

Result: 在概念重要性任务中,该方法F1分数达0.97,远超人类基准0.68。

Conclusion: 新框架有效扩展了分类法对齐的规模,同时保持高质量映射和专家对模糊案例的监督。

Abstract: Having a unified, coherent taxonomy is essential for effective knowledge
representation in domain-specific applications as diverse terminologies need to
be mapped to underlying concepts. Traditional manual approaches to taxonomy
alignment rely on expert review of concept pairs, but this becomes
prohibitively expensive and time-consuming at scale, while subjective
interpretations often lead to expert disagreements. Existing automated methods
for taxonomy alignment have shown promise but face limitations in handling
nuanced semantic relationships and maintaining consistency across different
domains. These approaches often struggle with context-dependent concept
mappings and lack transparent reasoning processes. We propose a novel framework
that combines large language models (LLMs) with expert calibration and
iterative prompt optimization to automate taxonomy alignment. Our method
integrates expert-labeled examples, multi-stage prompt engineering, and human
validation to guide LLMs in generating both taxonomy linkages and supporting
rationales. In evaluating our framework on a domain-specific mapping task of
concept essentiality, we achieved an F1-score of 0.97, substantially exceeding
the human benchmark of 0.68. These results demonstrate the effectiveness of our
approach in scaling taxonomy alignment while maintaining high-quality mappings
and preserving expert oversight for ambiguous cases.

</details>


### [218] [SHIELD: Multi-task Multi-distribution Vehicle Routing Solver with Sparsity and Hierarchy](https://arxiv.org/abs/2506.08424)
*Yong Liang Goh,Zhiguang Cao,Yining Ma,Jianan Zhou,Mohammad Haroon Dupty,Wee Sun Lee*

Key words: VRP, 多任务学习, 稀疏性, 层次结构, 泛化能力

TL;DR: 论文提出了一种新的模型SHIELD，用于解决多任务多分布VRP问题，通过稀疏性和层次性原则提升模型的泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有基础模型在处理VRP问题时忽略了真实世界中复杂的客户分布，因此需要更贴近实际的解决方案。

Method: 结合了Mixture-of-Depths技术和基于上下文的聚类层，动态选择节点并利用层次结构提升本地表示能力。

Result: SHIELD在16种VRP变体的9个真实地图上表现优于现有方法。

Conclusion: SHIELD通过稀疏性和层次性原则显著提升了模型的泛化能力。

Abstract: Recent advances toward foundation models for routing problems have shown
great potential of a unified deep model for various VRP variants. However, they
overlook the complex real-world customer distributions. In this work, we
advance the Multi-Task VRP (MTVRP) setting to the more realistic yet
challenging Multi-Task Multi-Distribution VRP (MTMDVRP) setting, and introduce
SHIELD, a novel model that leverages both sparsity and hierarchy principles.
Building on a deeper decoder architecture, we first incorporate the
Mixture-of-Depths (MoD) technique to enforce sparsity. This improves both
efficiency and generalization by allowing the model to dynamically select nodes
to use or skip each decoder layer, providing the needed capacity to adaptively
allocate computation for learning the task/distribution specific and shared
representations. We also develop a context-based clustering layer that exploits
the presence of hierarchical structures in the problems to produce better local
representations. These two designs inductively bias the network to identify key
features that are common across tasks and distributions, leading to
significantly improved generalization on unseen ones. Our empirical results
demonstrate the superiority of our approach over existing methods on 9
real-world maps with 16 VRP variants each.

</details>


### [219] [A Survey on Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2506.08446)
*Peng-Yuan Wang,Tian-Shuo Liu,Chenyang Wang,Yi-Di Wang,Shu Yan,Cheng-Xing Jia,Xu-Hui Liu,Xin-Wei Chen,Jia-Cheng Xu,Ziniu Li,Yang Yu*

Key words: 数学推理、大语言模型、认知阶段、Chain-of-Thought、微调

TL;DR: 该论文综述了大语言模型（LLMs）在数学推理能力上的发展，分为理解和答案生成两个认知阶段，并探讨了从训练无关提示到微调等多种增强方法。尽管取得进展，但仍存在容量、效率和泛化等挑战。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索如何通过大型语言模型提升数学推理能力，并总结当前研究进展与挑战。

Method: 通过两种认知阶段（理解和答案生成）分析LLMs的数学推理能力，涵盖从提示策略到微调技术的方法。

Result: 尽管LLMs在数学推理上取得显著进展，但仍面临容量、效率和泛化等根本性挑战。

Conclusion: 未来研究方向包括高级预训练、知识增强、形式推理框架和元泛化学习范式。

Abstract: Mathematical reasoning has long represented one of the most fundamental and
challenging frontiers in artificial intelligence research. In recent years,
large language models (LLMs) have achieved significant advances in this area.
This survey examines the development of mathematical reasoning abilities in
LLMs through two high-level cognitive phases: comprehension, where models gain
mathematical understanding via diverse pretraining strategies, and answer
generation, which has progressed from direct prediction to step-by-step
Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical
reasoning, ranging from training-free prompting to fine-tuning approaches such
as supervised fine-tuning and reinforcement learning, and discuss recent work
on extended CoT and "test-time scaling". Despite notable progress, fundamental
challenges remain in terms of capacity, efficiency, and generalization. To
address these issues, we highlight promising research directions, including
advanced pretraining and knowledge augmentation techniques, formal reasoning
frameworks, and meta-generalization through principled learning paradigms. This
survey tries to provide some insights for researchers interested in enhancing
reasoning capabilities of LLMs and for those seeking to apply these techniques
to other domains.

</details>


### [220] [Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing](https://arxiv.org/abs/2506.08462)
*Christos Margadji,Sebastian W. Pattinson*

Key words: CIPHER, VLA模型, 工业控制, 检索增强生成, 泛化能力

TL;DR: CIPHER是一种结合视觉-语言-动作的模型框架，旨在模拟人类推理能力，用于工业控制，尤其在数据稀缺和多样化的环境中表现出强泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 工业过程需要适应不可预测的环境和任务，而传统AI控制系统依赖于监督学习和大量标注数据，难以推广到数据稀缺的场景。

Method: CIPHER通过整合过程专家、回归模型和检索增强生成技术，支持物理知识驱动的链式推理，实现无标注输入的自主决策和精确控制。

Result: 该框架能够解释视觉或文本输入，生成精确的机器指令，并在分布外任务中展现出强泛化能力。

Conclusion: CIPHER为自主系统提供了精准行动、上下文推理和透明决策的基础，支持工业环境中的安全可信部署。

Abstract: Industrial processes must be robust and adaptable, as environments and tasks
are often unpredictable, while operational errors remain costly and difficult
to detect. AI-based control systems offer a path forward, yet typically depend
on supervised learning with extensive labelled datasets, which limits their
ability to generalize across variable and data-scarce industrial settings.
Foundation models could enable broader reasoning and knowledge integration, but
rarely deliver the quantitative precision demanded by engineering applications.
Here, we introduceControl and Interpretation of Production via Hybrid Expertise
and Reasoning (CIPHER): a vision-language-action (VLA) model framework aiming
to replicate human-like reasoning for industrial control, instantiated in a
commercial-grade 3D printer. It integrates a process expert, a regression model
enabling quantitative characterization of system states required for
engineering tasks. CIPHER also incorporates retrieval-augmented generation to
access external expert knowledge and support physics-informed, chain-of-thought
reasoning. This hybrid architecture exhibits strong generalization to
out-of-distribution tasks. It interprets visual or textual inputs from process
monitoring, explains its decisions, and autonomously generates precise machine
instructions, without requiring explicit annotations. CIPHER thus lays the
foundations for autonomous systems that act with precision, reason with
context, and communicate decisions transparently, supporting safe and trusted
deployment in industrial settings.

</details>


### [221] [RHealthTwin: Towards Responsible and Multimodal Digital Twins for Personalized Well-being](https://arxiv.org/abs/2506.08486)
*Rahatara Ferdousi,M Anwar Hossain*

Key words: 大型语言模型, 数字孪生, 负责任AI, 健康, 提示工程

TL;DR: RHealthTwin是一个负责任框架，用于构建和管理AI驱动的医疗数字孪生，旨在解决大型语言模型（LLM）在健康领域中的幻觉、偏见等问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 大型语言模型在医疗健康领域的应用潜力巨大，但存在幻觉、偏见和伦理问题，亟需一个负责任的框架来规范其使用。

Method: RHealthTwin使用多模态输入，通过负责任提示引擎（RPE）动态提取预定义槽位，结构化输入以减少幻觉，并通过反馈循环优化提示结构。

Result: RHealthTwin在四个健康领域（心理支持、症状分诊、营养计划和活动指导）中表现出色，评估指标（BLEU、ROUGE-L、BERTScore）领先，并在伦理合规性上超过90%。

Conclusion: RHealthTwin为负责任地使用LLM于健康和福祉提供了一个前瞻性框架，具有广泛的应用潜力。

Abstract: The rise of large language models (LLMs) has created new possibilities for
digital twins in healthcare. However, the deployment of such systems in
consumer health contexts raises significant concerns related to hallucination,
bias, lack of transparency, and ethical misuse. In response to recommendations
from health authorities such as the World Health Organization (WHO), we propose
Responsible Health Twin (RHealthTwin), a principled framework for building and
governing AI-powered digital twins for well-being assistance. RHealthTwin
processes multimodal inputs that guide a health-focused LLM to produce safe,
relevant, and explainable responses. At the core of RHealthTwin is the
Responsible Prompt Engine (RPE), which addresses the limitations of traditional
LLM configuration. Conventionally, users input unstructured prompt and the
system instruction to configure the LLM, which increases the risk of
hallucination. In contrast, RPE extracts predefined slots dynamically to
structure both inputs. This guides the language model to generate responses
that are context aware, personalized, fair, reliable, and explainable for
well-being assistance. The framework further adapts over time through a
feedback loop that updates the prompt structure based on user satisfaction. We
evaluate RHealthTwin across four consumer health domains including mental
support, symptom triage, nutrition planning, and activity coaching. RPE
achieves state-of-the-art results with BLEU = 0.41, ROUGE-L = 0.63, and
BERTScore = 0.89 on benchmark datasets. Also, we achieve over 90% in ethical
compliance and instruction-following metrics using LLM-as-judge evaluation,
outperforming baseline strategies. We envision RHealthTwin as a forward-looking
foundation for responsible LLM-based applications in health and well-being.

</details>


### [222] [FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching](https://arxiv.org/abs/2506.08518)
*Sunny Gupta,Nikita Jangid,Shounak Das,Amit Sethi*

Key words: 领域泛化, 联邦学习, 长尾分布, 梯度对齐, 锐度感知优化

TL;DR: FedTAIL 是一个联邦领域泛化框架，通过梯度对齐和锐度感知优化解决长尾分布和优化目标冲突问题，显著提升模型在域偏移和标签不平衡下的性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统领域泛化方法在长尾类分布和优化目标冲突下表现不佳，FedTAIL 旨在通过锐度引导的梯度对齐优化解决这些问题。

Method: 引入梯度一致性正则化减少分类和对抗目标的冲突，采用类感知锐度最小化和曲率感知动态加权方案处理类别不平衡，并增强条件分布对齐。

Result: 在标准领域泛化基准测试中，FedTAIL 实现了最先进的性能，尤其在域偏移和标签不平衡情况下表现优异。

Conclusion: FedTAIL 通过统一优化协调、类感知正则化和条件对齐，为集中式和联邦式设置提供了一种有效的解决方案。

Abstract: Domain Generalization (DG) seeks to train models that perform reliably on
unseen target domains without access to target data during training. While
recent progress in smoothing the loss landscape has improved generalization,
existing methods often falter under long-tailed class distributions and
conflicting optimization objectives. We introduce FedTAIL, a federated domain
generalization framework that explicitly addresses these challenges through
sharpness-guided, gradient-aligned optimization. Our method incorporates a
gradient coherence regularizer to mitigate conflicts between classification and
adversarial objectives, leading to more stable convergence. To combat class
imbalance, we perform class-wise sharpness minimization and propose a
curvature-aware dynamic weighting scheme that adaptively emphasizes
underrepresented tail classes. Furthermore, we enhance conditional distribution
alignment by integrating sharpness-aware perturbations into entropy
regularization, improving robustness under domain shift. FedTAIL unifies
optimization harmonization, class-aware regularization, and conditional
alignment into a scalable, federated-compatible framework. Extensive
evaluations across standard domain generalization benchmarks demonstrate that
FedTAIL achieves state-of-the-art performance, particularly in the presence of
domain shifts and label imbalance, validating its effectiveness in both
centralized and federated settings. Code: https://github.com/sunnyinAI/FedTail

</details>


### [223] [Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A Hybrid DRL-LLM Approach with Compliance Awareness](https://arxiv.org/abs/2506.08532)
*Yanwei Gong,Xiaolin Chang*

Key words: 

TL;DR: 本文提出了一种结合深度强化学习和大语言模型推理的新型无人机轨迹规划框架，旨在解决低空经济中无人机轨迹规划的挑战。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 低空经济的快速发展推动了无人机的广泛应用，但现有研究常常忽略了城市空域约束和经济效率等关键因素。

Method: 提出了一种结合深度强化学习和大型语言模型推理的新型无人机轨迹规划框架。

Result: 实验结果表明，该方法在数据收集率、避碰、成功着陆、法规遵从性和能源效率等多个指标上显著优于现有基线。

Conclusion: 该方法有效解决了低空经济网络中无人机轨迹规划的关键挑战。

Abstract: The rapid growth of the low-altitude economy has driven the widespread
adoption of unmanned aerial vehicles (UAVs). This growing deployment presents
new challenges for UAV trajectory planning in complex urban environments.
However, existing studies often overlook key factors, such as urban airspace
constraints and economic efficiency, which are essential in low-altitude
economy contexts. Deep reinforcement learning (DRL) is regarded as a promising
solution to these issues, while its practical adoption remains limited by low
learning efficiency. To overcome this limitation, we propose a novel UAV
trajectory planning framework that combines DRL with large language model (LLM)
reasoning to enable safe, compliant, and economically viable path planning.
Experimental results demonstrate that our method significantly outperforms
existing baselines across multiple metrics, including data collection rate,
collision avoidance, successful landing, regulatory compliance, and energy
efficiency. These results validate the effectiveness of our approach in
addressing UAV trajectory planning key challenges under constraints of the
low-altitude economy networking.

</details>


### [224] [HGFormer: A Hierarchical Graph Transformer Framework for Two-Stage Colonel Blotto Games via Reinforcement Learning](https://arxiv.org/abs/2506.08580)
*Yang Lv,Jinlong Lei,Peng Yi*

Key words: Colonel Blotto游戏, 资源分配, 图Transformer, 强化学习, 对抗环境

TL;DR: HGformer框架通过层次化图Transformer和反馈强化学习算法，解决了两阶段Colonel Blotto游戏中资源分配的全局优化难题，显著提升效率与对抗收益。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 两阶段Colonel Blotto游戏的顺序依赖性和图拓扑的复杂约束使传统方法难以实现全局最优策略。

Method: 提出HGformer框架，结合增强图Transformer编码器和两级决策模型，并设计分层反馈强化学习算法。

Result: 实验表明HGformer在资源分配效率和对抗收益上显著优于现有方法。

Conclusion: HGformer在复杂动态游戏场景中具有优越性能。

Abstract: Two-stage Colonel Blotto game represents a typical adversarial resource
allocation problem, in which two opposing agents sequentially allocate
resources in a network topology across two phases: an initial resource
deployment followed by multiple rounds of dynamic reallocation adjustments. The
sequential dependency between game stages and the complex constraints imposed
by the graph topology make it difficult for traditional approaches to attain a
globally optimal strategy. To address these challenges, we propose a
hierarchical graph Transformer framework called HGformer. By incorporating an
enhanced graph Transformer encoder with structural biases and a two-agent
hierarchical decision model, our approach enables efficient policy generation
in large-scale adversarial environments. Moreover, we design a layer-by-layer
feedback reinforcement learning algorithm that feeds the long-term returns from
lower-level decisions back into the optimization of the higher-level strategy,
thus bridging the coordination gap between the two decision-making stages.
Experimental results demonstrate that, compared to existing hierarchical
decision-making or graph neural network methods, HGformer significantly
improves resource allocation efficiency and adversarial payoff, achieving
superior overall performance in complex dynamic game scenarios.

</details>


### [225] [FoldA: Computing Partial-Order Alignments Using Directed Net Unfoldings](https://arxiv.org/abs/2506.08627)
*Douwe Geurtjens,Xixi Lu*

Key words: conformance checking, partial-order alignments, Petri net unfoldings, process mining

TL;DR: 本文提出了一种名为FoldA的新技术，通过使用有向Petri网展开动态计算部分对齐，解决了现有对齐方法在并发行为表示和状态空间爆炸方面的局限性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的一致性检查方法在面对高选择性和并发性的过程模型时容易导致状态空间爆炸，且无法充分表示并发行为。

Method: 提出了FoldA技术，利用有向Petri网展开动态计算部分对齐。

Result: FoldA虽然需要更多计算时间，但减少了队列状态数量，并更准确地表示了并发行为。

Conclusion: FoldA在处理并发行为和高选择性的模型时表现更优，尽管计算成本较高。

Abstract: Conformance checking is a fundamental task of process mining, which
quantifies the extent to which the observed process executions match a
normative process model. The state-of-the-art approaches compute alignments by
exploring the state space formed by the synchronous product of the process
model and the trace. This often leads to state space explosion, particularly
when the model exhibits a high degree of choice and concurrency. Moreover, as
alignments inherently impose a sequential structure, they fail to fully
represent the concurrent behavior present in many real-world processes. To
address these limitations, this paper proposes a new technique for computing
partial-order alignments {on the fly using directed Petri net unfoldings, named
FoldA. We evaluate our technique on 485 synthetic model-log pairs and compare
it against Astar- and Dijkstra-alignments on 13 real-life model-log pairs and 6
benchmark pairs. The results show that our unfolding alignment, although it
requires more computation time, generally reduces the number of queued states
and provides a more accurate representation of concurrency.

</details>


### [226] [Modular Recurrence in Contextual MDPs for Universal Morphology Control](https://arxiv.org/abs/2506.08630)
*Laurens Engwegen,Daan Brinks,Wendelin Böhmer*

Key words: 通用控制器, 机器人形态, 深度强化学习, 模块化结构, 泛化能力

TL;DR: 一种利用上下文信息和模块化结构的深度强化学习方法，显著提升了对未见机器人控制任务的泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 开发一种适用于多种机器人形态的通用控制器，解决多机器人控制中的泛化问题。

Method: 采用模块化循环架构，通过交互推断部分可观测的上下文信息。

Result: 在MuJoCo机器人测试集上，对未见动态、运动学和拓扑的机器人性能显著提升。

Conclusion: 模块化结构和上下文推断方法有效提升泛化能力，为通用机器人控制提供新思路。

Abstract: A universal controller for any robot morphology would greatly improve
computational and data efficiency. By utilizing contextual information about
the properties of individual robots and exploiting their modular structure in
the architecture of deep reinforcement learning agents, steps have been made
towards multi-robot control. Generalization to new, unseen robots, however,
remains a challenge. In this paper we hypothesize that the relevant contextual
information is partially observable, but that it can be inferred through
interactions for better generalization to contexts that are not seen during
training. To this extent, we implement a modular recurrent architecture and
evaluate its generalization performance on a large set of MuJoCo robots. The
results show a substantial improved performance on robots with unseen dynamics,
kinematics, and topologies, in four different environments.

</details>


### [227] [Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08745)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Yingjie Wang,Baisheng Lai,Jieping Ye,Mingli Song,Dacheng Tao*

Key words: 强化学习, 大型语言模型, 自奖励, 一致性, 推理

TL;DR: 提出自奖励强化学习框架CoVo，利用一致性度量提升大型语言模型的推理能力，无需外部监督。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决强化学习在复杂推理任务中依赖外部监督的局限性。

Method: 通过一致性和波动性度量设计自奖励机制CoVo，并结合好奇心奖励以促进探索。

Result: 在多样推理任务中表现优于或接近监督强化学习方法。

Conclusion: CoVo提供了一种无需外部监督的强化学习新途径，推动了语言模型自主推理的发展。

Abstract: Recent advances of Reinforcement Learning (RL) have highlighted its potential
in complex reasoning tasks, yet effective training often relies on external
supervision, which limits the broader applicability. In this work, we propose a
novel self-rewarding reinforcement learning framework to enhance Large Language
Model (LLM) reasoning by leveraging the consistency of intermediate reasoning
states across different reasoning trajectories. Our key insight is that correct
responses often exhibit consistent trajectory patterns in terms of model
likelihood: their intermediate reasoning states tend to converge toward their
own final answers (high consistency) with minimal deviation toward other
candidates (low volatility). Inspired by this observation, we introduce CoVo,
an intrinsic reward mechanism that integrates Consistency and Volatility via a
robust vector-space aggregation strategy, complemented by a curiosity bonus to
promote diverse exploration. CoVo enables LLMs to perform RL in a
self-rewarding manner, offering a scalable pathway for learning to reason
without external supervision. Extensive experiments on diverse reasoning
benchmarks show that CoVo achieves performance comparable to or even surpassing
supervised RL. Our code is available at https://github.com/sastpg/CoVo.

</details>


### [228] [A Sample Efficient Conditional Independence Test in the Presence of Discretization](https://arxiv.org/abs/2506.08747)
*Boyang Sun,Yu Yao,Xinshuai Dong,Zongfang Liu,Tongliang Liu,Yumou Qiu,Kun Zhang*

Key words: 条件独立性检验,离散化数据,广义矩方法,节点回归

TL;DR: 本文提出了一种样本高效的独立性检验方法，避免离散化导致的信息丢失，利用广义矩方法（GMM）和节点回归验证独立性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有方法在离散化数据上进行条件独立性检验会导致信息丢失和错误结论，因此需要一种不依赖离散化的高效检验方法。

Method: 通过广义矩方法（GMM）解决过识别限制问题，结合节点回归推导测试统计量及其渐近分布。

Result: 理论和实证结果表明，该方法在多种数据集上表现优越且有效。

Conclusion: 所提出的方法避免了信息丢失，能够更准确地验证潜在连续变量的独立性关系。

Abstract: In many real-world scenarios, interested variables are often represented as
discretized values due to measurement limitations. Applying Conditional
Independence (CI) tests directly to such discretized data, however, can lead to
incorrect conclusions. To address this, recent advancements have sought to
infer the correct CI relationship between the latent variables through
binarizing observed data. However, this process inevitably results in a loss of
information, which degrades the test's performance. Motivated by this, this
paper introduces a sample-efficient CI test that does not rely on the
binarization process. We find that the independence relationships of latent
continuous variables can be established by addressing an over-identifying
restriction problem with Generalized Method of Moments (GMM). Based on this
insight, we derive an appropriate test statistic and establish its asymptotic
distribution correctly reflecting CI by leveraging nodewise regression.
Theoretical findings and Empirical results across various datasets demonstrate
that the superiority and effectiveness of our proposed test. Our code
implementation is provided in https://github.com/boyangaaaaa/DCT

</details>


### [229] [Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery](https://arxiv.org/abs/2506.08771)
*Yuni Susanti,Michael Färber*

Key words: 因果发现, 知识图谱, 大型语言模型, 零样本学习, 学习排序

TL;DR: 该论文提出了一种结合知识图谱和大型语言模型的新方法，以提升基于知识的因果发现效果，通过实验验证其优于现有基线。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的基于大型语言模型的因果发现方法结果不稳定且不可靠，需要一种更有效的方法来改进基于知识的因果推断。

Method: 结合知识图谱与大型语言模型，利用基于元路径的子图和学习排序模型优化零样本提示，提升因果关系推断效果。

Result: 在生物医学和其他开放领域数据集上，该方法在F1分数上优于大多数基线，最高提升44.4分。

Conclusion: 该方法通过结合知识图谱和大型语言模型显著提升了基于知识的因果发现效果，为复杂系统中的因果推断提供了可靠工具。

Abstract: Inferring causal relationships between variable pairs is crucial for
understanding multivariate interactions in complex systems. Knowledge-based
causal discovery -- which involves inferring causal relationships by reasoning
over the metadata of variables (e.g., names or textual context) -- offers a
compelling alternative to traditional methods that rely on observational data.
However, existing methods using Large Language Models (LLMs) often produce
unstable and inconsistent results, compromising their reliability for causal
inference. To address this, we introduce a novel approach that integrates
Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery.
Our approach identifies informative metapath-based subgraphs within KGs and
further refines the selection of these subgraphs using Learning-to-Rank-based
models. The top-ranked subgraphs are then incorporated into zero-shot prompts,
improving the effectiveness of LLMs in inferring the causal relationship.
Extensive experiments on biomedical and open-domain datasets demonstrate that
our method outperforms most baselines by up to 44.4 points in F1 scores,
evaluated across diverse LLMs and KGs. Our code and datasets are available on
GitHub: https://github.com/susantiyuni/path-to-causality

</details>


### [230] [Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents](https://arxiv.org/abs/2506.08800)
*Irene Testini,José Hernández-Orallo,Lorenzo Pacchiardi*

Key words: 数据科学, 大型语言模型, 人机协作, 自动化, 任务转化

TL;DR: 本文综述了大型语言模型（LLMs）在数据科学中的评估，发现当前研究集中在有限的目标导向活动，忽视了数据管理和探索性任务，且缺乏中间程度的人机协作研究。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 数据科学需要从数据中提取洞察以支持决策，大型语言模型（LLMs）作为辅助工具逐渐普及，但对其评估的研究存在局限性。

Method: 通过文献综述，分析LLM助手和代理在数据科学中的应用和评估现状。

Result: 研究发现当前评估集中在少数目标导向活动，忽略了数据管理和探索性任务，也缺乏对人机协作中间层级的研究。

Conclusion: 需要更全面的评估框架，涵盖数据科学的全流程任务，并探索任务转化带来的更高层次自动化。

Abstract: Data science aims to extract insights from data to support decision-making
processes. Recently, Large Language Models (LLMs) are increasingly used as
assistants for data science, by suggesting ideas, techniques and small code
snippets, or for the interpretation of results and reporting. Proper automation
of some data-science activities is now promised by the rise of LLM agents,
i.e., AI systems powered by an LLM equipped with additional affordances--such
as code execution and knowledge bases--that can perform self-directed actions
and interact with digital environments. In this paper, we survey the evaluation
of LLM assistants and agents for data science. We find (1) a dominant focus on
a small subset of goal-oriented activities, largely ignoring data management
and exploratory activities; (2) a concentration on pure assistance or fully
autonomous agents, without considering intermediate levels of human-AI
collaboration; and (3) an emphasis on human substitution, therefore neglecting
the possibility of higher levels of automation thanks to task transformation.

</details>


### [231] [Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task](https://arxiv.org/abs/2506.08872)
*Nataliya Kosmyna,Eugene Hauptmann,Ye Tong Yuan,Jessica Situ,Xian-Hao Liao,Ashly Vivian Beresnitzky,Iris Braunstein,Pattie Maes*

Key words: LLM, 认知负荷, EEG, 写作辅助, 教育

TL;DR: 本研究探讨了LLM辅助写作对神经和行为的影响，发现LLM使用导致认知网络连接减弱，且长期使用可能对教育产生负面影响。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究旨在揭示LLM辅助写作对认知功能和学习效果的潜在影响，填补AI在教育领域应用的认知研究空白。

Method: 通过EEG测量认知负荷，结合NLP分析和人工评分，对比LLM、搜索引擎和纯脑力写作三组的神经、语言和行为表现。

Result: LLM组认知网络连接最弱，纯脑力组最强；长期使用LLM导致神经和语言表现下降，且自我认同感最低。

Conclusion: LLM的便捷性可能带来认知代价，需进一步研究AI在教育中的长期影响。

Abstract: This study explores the neural and behavioral consequences of LLM-assisted
essay writing. Participants were divided into three groups: LLM, Search Engine,
and Brain-only (no tools). Each completed three sessions under the same
condition. In a fourth session, LLM users were reassigned to Brain-only group
(LLM-to-Brain), and Brain-only users were reassigned to LLM condition
(Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18
completing session 4. We used electroencephalography (EEG) to assess cognitive
load during essay writing, and analyzed essays using NLP, as well as scoring
essays with the help from human teachers and an AI judge. Across groups, NERs,
n-gram patterns, and topic ontology showed within-group homogeneity. EEG
revealed significant differences in brain connectivity: Brain-only participants
exhibited the strongest, most distributed networks; Search Engine users showed
moderate engagement; and LLM users displayed the weakest connectivity.
Cognitive activity scaled down in relation to external tool use. In session 4,
LLM-to-Brain participants showed reduced alpha and beta connectivity,
indicating under-engagement. Brain-to-LLM users exhibited higher memory recall
and activation of occipito-parietal and prefrontal areas, similar to Search
Engine users. Self-reported ownership of essays was the lowest in the LLM group
and the highest in the Brain-only group. LLM users also struggled to accurately
quote their own work. While LLMs offer immediate convenience, our findings
highlight potential cognitive costs. Over four months, LLM users consistently
underperformed at neural, linguistic, and behavioral levels. These results
raise concerns about the long-term educational implications of LLM reliance and
underscore the need for deeper inquiry into AI's role in learning.

</details>


### [232] [Preference-Driven Multi-Objective Combinatorial Optimization with Conditional Computation](https://arxiv.org/abs/2506.08898)
*Mingfeng Fan,Jianan Zhou,Yifeng Zhang,Yaoxin Wu,Jinbiao Chen,Guillaume Adrien Sartoretti*

Key words: 深度强化学习,多目标组合优化,自适应模型选择,偏好驱动优化

TL;DR: POCCO提出了一种自适应选择模型结构的框架，通过偏好信号优化子问题，显著提高了多目标组合优化问题的性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有深度强化学习方法在处理多目标组合优化问题时，对所有子问题采用单一模型，限制了解决方案空间的探索，导致性能不佳。

Method: 提出POCCO框架，设计条件计算块将子问题路由到专用神经架构，并提出偏好驱动的优化算法学习解决方案之间的偏好关系。

Result: 在四个经典MOCOP基准测试中，POCCO表现出显著的优越性和强大的泛化能力。

Conclusion: POCCO通过自适应模型选择和偏好驱动优化，显著改进了多目标组合优化问题的解决效果。

Abstract: Recent deep reinforcement learning methods have achieved remarkable success
in solving multi-objective combinatorial optimization problems (MOCOPs) by
decomposing them into multiple subproblems, each associated with a specific
weight vector. However, these methods typically treat all subproblems equally
and solve them using a single model, hindering the effective exploration of the
solution space and thus leading to suboptimal performance. To overcome the
limitation, we propose POCCO, a novel plug-and-play framework that enables
adaptive selection of model structures for subproblems, which are subsequently
optimized based on preference signals rather than explicit reward values.
Specifically, we design a conditional computation block that routes subproblems
to specialized neural architectures. Moreover, we propose a preference-driven
optimization algorithm that learns pairwise preferences between winning and
losing solutions. We evaluate the efficacy and versatility of POCCO by applying
it to two state-of-the-art neural methods for MOCOPs. Experimental results
across four classic MOCOP benchmarks demonstrate its significant superiority
and strong generalization.

</details>


### [233] [IntTrajSim: Trajectory Prediction for Simulating Multi-Vehicle driving at Signalized Intersections](https://arxiv.org/abs/2506.08957)
*Yash Ranjan,Rahul Sengupta,Anand Rangarajan,Sanjay Ranka*

Key words: 交通模拟器, 深度生成建模, 轨迹预测, 交通路口, 自注意力机制

TL;DR: 该论文提出了一种基于深度生成建模的数据驱动交通模拟器，用于模仿交通路口的驾驶行为，并引入交通工程相关指标进行评估。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统基于规则的交通模拟器难以真实模仿驾驶行为，尤其是在交通事故高发的交通路口。

Method: 提出多头部自注意力轨迹预测模型，结合信号信息，并通过仿真闭环管道测试。

Result: 新模型在交通工程相关指标上优于之前的模型。

Conclusion: 数据驱动的方法能够有效模拟交通路口的驾驶行为，并提升交通工程研究的实用性。

Abstract: Traffic simulators are widely used to study the operational efficiency of
road infrastructure, but their rule-based approach limits their ability to
mimic real-world driving behavior. Traffic intersections are critical
components of the road infrastructure, both in terms of safety risk (nearly 28%
of fatal crashes and 58% of nonfatal crashes happen at intersections) as well
as the operational efficiency of a road corridor. This raises an important
question: can we create a data-driven simulator that can mimic the macro- and
micro-statistics of the driving behavior at a traffic intersection? Deep
Generative Modeling-based trajectory prediction models provide a good starting
point to model the complex dynamics of vehicles at an intersection. But they
are not tested in a "live" micro-simulation scenario and are not evaluated on
traffic engineering-related metrics. In this study, we propose traffic
engineering-related metrics to evaluate generative trajectory prediction models
and provide a simulation-in-the-loop pipeline to do so. We also provide a
multi-headed self-attention-based trajectory prediction model that incorporates
the signal information, which outperforms our previous models on the evaluation
metrics.

</details>


### [234] [Evaluating Generative Vehicle Trajectory Models for Traffic Intersection Dynamics](https://arxiv.org/abs/2506.08963)
*Yash Ranjan,Rahul Sengupta,Anand Rangarajan,Sanjay Ranka*

Key words: 交通动态、深度生成模型、信号灯交叉口、轨迹预测、交通工程

TL;DR: 本文提出了一种用于信号灯交叉口交通动态深度生成模型的综合分析工具，引入了新指标以从交通工程角度评估模型性能，并揭示了即使轨迹重建误差低，生成的轨迹仍可能违反交通规则。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 信号灯交叉口是城市道路网络的关键部分，但容易发生事故。当前模型仅基于轨迹重建误差评估，缺乏对交通工程关注点的考量（如闯红灯、违规停车等）。

Method: 训练了多车辆轨迹预测的最先进模型，并使用大型真实数据集进行校准，随后在微观模拟器中在线评估模型性能。

Result: 实验表明，即使输入理想轨迹且轨迹重建误差低，生成的轨迹仍可能违反交通规则。新引入的指标能更好评估此类行为。

Conclusion: 本文强调了从交通工程角度评估模型的重要性，并提供了更全面的分析工具和指标。

Abstract: Traffic Intersections are vital to urban road networks as they regulate the
movement of people and goods. However, they are regions of conflicting
trajectories and are prone to accidents. Deep Generative models of traffic
dynamics at signalized intersections can greatly help traffic authorities
better understand the efficiency and safety aspects. At present, models are
evaluated on computational metrics that primarily look at trajectory
reconstruction errors. They are not evaluated online in a `live'
microsimulation scenario. Further, these metrics do not adequately consider
traffic engineering-specific concerns such as red-light violations, unallowed
stoppage, etc. In this work, we provide a comprehensive analytics tool to
train, run, and evaluate models with metrics that give better insights into
model performance from a traffic engineering point of view. We train a
state-of-the-art multi-vehicle trajectory forecasting model on a large dataset
collected by running a calibrated scenario of a real-world urban intersection.
We then evaluate the performance of the prediction models, online in a
microsimulator, under unseen traffic conditions. We show that despite using
ideally-behaved trajectories as input, and achieving low trajectory
reconstruction errors, the generated trajectories show behaviors that break
traffic rules. We introduce new metrics to evaluate such undesired behaviors
and present our results.

</details>


### [235] [A Survey of Link Prediction in N-ary Knowledge Graphs](https://arxiv.org/abs/2506.08970)
*Jiyao Wei,Saiping Guan,Da Li,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Key words: N元知识图谱,链接预测,知识补全,方法综述

TL;DR: 该论文综述了N元知识图谱（NKGs）中的链接预测任务，系统分类现有方法并分析其性能与应用场景，同时指出未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: N元知识图谱能高效表示复杂的现实世界事实，其链接预测对补全NKGs和提升下游应用性能至关重要，目前该领域研究受到广泛关注。

Method: 通过系统综述，分类和分析NKGs中链接预测的现有方法。

Result: 提供了NKGs链接预测的全面概述，并评估了不同方法的性能与应用场景。

Conclusion: 论文为NKGs链接预测领域的研究提供了框架，指出了未来潜在的研究方向。

Abstract: N-ary Knowledge Graphs (NKGs) are a specialized type of knowledge graph
designed to efficiently represent complex real-world facts. Unlike traditional
knowledge graphs, where a fact typically involves two entities, NKGs can
capture n-ary facts containing more than two entities. Link prediction in NKGs
aims to predict missing elements within these n-ary facts, which is essential
for completing NKGs and improving the performance of downstream applications.
This task has recently gained significant attention. In this paper, we present
the first comprehensive survey of link prediction in NKGs, providing an
overview of the field, systematically categorizing existing methods, and
analyzing their performance and application scenarios. We also outline
promising directions for future research.

</details>


### [236] [AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions](https://arxiv.org/abs/2506.09038)
*Polina Kirichenko,Mark Ibrahim,Kamalika Chaudhuri,Samuel J. Bell*

Key words: 大型语言模型（LLMs）、拒绝回答能力、AbstentionBench、推理调优、模型可靠性

TL;DR: 这篇论文提出了AbstentionBench，一个用于全面评估大型语言模型（LLMs）在不明确、无法回答或含糊不清问题中拒绝回答能力的大规模基准。研究发现，LLMs在拒绝回答方面表现不佳，且模型规模的扩展对此帮助有限，甚至发现推理调优会降低拒绝回答的能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 为了确保LLMs在实际应用中能够可靠地使用，尤其是在高风险的场景中，了解何时选择不回答与正确回答同等重要。目前对于LLMs的拒绝回答能力缺乏系统性研究，因此需要建立一个全面的评估框架。

Method: 作者开发了AbstentionBench，覆盖20个多样化数据集，包括未知答案、不明确问题、错误前提、主观解释和过时信息等场景，评估了20种前沿LLMs的拒绝回答能力。

Result: 研究发现LLMs在拒绝回答方面仍是一个未解决的问题，规模扩展对此帮助不大。推理调优反而平均降低了24%的拒绝回答能力，尽管这些模型在数学和科学领域的推理任务中表现优异。精心设计的系统提示可以一定程度上提升拒绝回答能力，但无法从根本上解决问题。

Conclusion: LLMs在拒绝回答问题上仍有显著不足，需要进一步研究以提升其可靠性。AbstentionBench的发布旨在促进这一领域的研究。

Abstract: For Large Language Models (LLMs) to be reliably deployed in both everyday and
high-stakes domains, knowing when not to answer is equally critical as
answering correctly. Real-world user queries, which can be underspecified,
ill-posed, or fundamentally unanswerable, require LLMs to reason about
uncertainty and selectively abstain -- i.e., refuse to answer definitively.
However, abstention remains understudied, without a systematic evaluation
framework for modern LLMs. In this work, we introduce AbstentionBench, a
large-scale benchmark for holistically evaluating abstention across 20 diverse
datasets, including questions with unknown answers, underspecification, false
premises, subjective interpretations, and outdated information. Evaluating 20
frontier LLMs reveals abstention is an unsolved problem, and one where scaling
models is of little use. While recent reasoning LLMs have shown impressive
results in complex problem solving, surprisingly, we find that reasoning
fine-tuning degrades abstention (by $24\%$ on average), even for math and
science domains on which reasoning models are explicitly trained. We find that
while a carefully crafted system prompt can boost abstention in practice, it
does not resolve models' fundamental inability to reason about uncertainty. We
release AbstentionBench to foster research into advancing LLM reliability.

</details>


### [237] [VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning](https://arxiv.org/abs/2506.09049)
*Li Kang,Xiufeng Song,Heng Zhou,Yiran Qin,Jie Yang,Xiaohong Liu,Philip Torr,Lei Bai,Zhenfei Yin*

Key words: 多智能体协作, 视觉语言模型, 强化学习, 分层基准测试

TL;DR: VIKI-Bench是首个针对多智能体协作的分层基准测试，包含多样化的机器人实现和多视角视觉观测，提出的VIKI-R框架在任务表现上显著优于基线方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决多智能体在动态环境中协作的核心挑战，特别是支持多样化的机器人实现和视觉驱动的推理。

Method: 提出VIKI-Bench分层基准测试和VIKI-R框架，结合微调预训练视觉语言模型和强化学习。

Result: VIKI-R在所有任务层级上显著优于基线方法，强化学习还能促进异构智能体间的组合协作模式。

Conclusion: VIKI-Bench和VIKI-R为多智能体视觉驱动协作提供了一个统一的测试平台和方法。

Abstract: Coordinating multiple embodied agents in dynamic environments remains a core
challenge in artificial intelligence, requiring both perception-driven
reasoning and scalable cooperation strategies. While recent works have
leveraged large language models (LLMs) for multi-agent planning, a few have
begun to explore vision-language models (VLMs) for visual reasoning. However,
these VLM-based approaches remain limited in their support for diverse
embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical
benchmark tailored for embodied multi-agent cooperation, featuring three
structured levels: agent activation, task planning, and trajectory perception.
VIKI-Bench includes diverse robot embodiments, multi-view visual observations,
and structured supervision signals to evaluate reasoning grounded in visual
inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a
two-stage framework that fine-tunes a pretrained vision-language model (VLM)
using Chain-of-Thought annotated demonstrations, followed by reinforcement
learning under multi-level reward signals. Our extensive experiments show that
VIKI-R significantly outperforms baselines method across all task levels.
Furthermore, we show that reinforcement learning enables the emergence of
compositional cooperation patterns among heterogeneous agents. Together,
VIKI-Bench and VIKI-R offer a unified testbed and method for advancing
multi-agent, visual-driven cooperation in embodied AI systems.

</details>


### [238] [ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering](https://arxiv.org/abs/2506.09050)
*Yuki Imajuku,Kohki Horie,Yoichi Iwata,Kensho Aoki,Naohiro Takahashi,Takuya Akiba*

Key words: AI系统, 算法工程, 优化问题, ALE-Bench, 基准测试

TL;DR: ALE-Bench是一个新的基准测试，用于评估AI系统在基于分数的算法编程竞赛中的表现，专注于解决现实中的优化问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 评估AI系统在解决复杂优化问题（如物流路由、调度等）中的能力，发现当前AI与人类在长期问题解决和一致性上的差距。

Method: 借鉴AtCoder Heuristic Contests的真实任务，提出一个支持交互式代理架构的软件框架，利用测试反馈和可视化工具进行迭代优化。

Result: 前沿LLM在特定问题上表现优异，但在跨问题一致性和长期问题解决能力上仍显著落后于人类。

Conclusion: ALE-Bench有助于推动AI在复杂优化问题上的进步，填补了当前评估工具的不足。

Abstract: How well do AI systems perform in algorithm engineering for hard optimization
problems in domains such as package-delivery routing, crew scheduling, factory
production planning, and power-grid balancing? We introduce ALE-Bench, a new
benchmark for evaluating AI systems on score-based algorithmic programming
contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench
presents optimization problems that are computationally hard and admit no known
exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench
encourages iterative solution refinement over long time horizons. Our software
framework supports interactive agent architectures that leverage test-run
feedback and visualizations. Our evaluation of frontier LLMs revealed that
while they demonstrate high performance on specific problems, a notable gap
remains compared to humans in terms of consistency across problems and
long-horizon problem-solving capabilities. This highlights the need for this
benchmark to foster future AI advancements.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [239] [EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements](https://arxiv.org/abs/2506.08762)
*Issa Sugiura,Takashi Ishida,Taro Makino,Chieko Tazuke,Takanori Nakagawa,Kosuke Nakago,David Ha*

Key words: LLM,金融分析,EDINET-Bench,日本,数据集

TL;DR: 介绍了EDINET-Bench，一个用于评估大语言模型在日本金融任务中表现的开源基准数据集。

<details>
  <summary>Details</summary>

Main category: q-fin.ST

Motivation: 解决日本金融数据稀缺问题，促进金融分析领域的创新。

Method: 通过下载EDINET过去10年的年报，自动标注标签构建数据集。

Result: 即使最先进的LLM表现也仅略优于逻辑回归。

Conclusion: LLM在金融领域应用面临挑战，需领域特定优化。

Abstract: Financial analysis presents complex challenges that could leverage large
language model (LLM) capabilities. However, the scarcity of challenging
financial datasets, particularly for Japanese financial data, impedes academic
innovation in financial analytics. As LLMs advance, this lack of accessible
research resources increasingly hinders their development and evaluation in
this specialized domain. To address this gap, we introduce EDINET-Bench, an
open-source Japanese financial benchmark designed to evaluate the performance
of LLMs on challenging financial tasks including accounting fraud detection,
earnings forecasting, and industry prediction. EDINET-Bench is constructed by
downloading annual reports from the past 10 years from Japan's Electronic
Disclosure for Investors' NETwork (EDINET) and automatically assigning labels
corresponding to each evaluation task. Our experiments reveal that even
state-of-the-art LLMs struggle, performing only slightly better than logistic
regression in binary classification for fraud detection and earnings
forecasting. These results highlight significant challenges in applying LLMs to
real-world financial applications and underscore the need for domain-specific
adaptation. Our dataset, benchmark construction code, and evaluation code is
publicly available to facilitate future research in finance with LLMs.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [240] [SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech Classification Models](https://arxiv.org/abs/2506.08346)
*Wenhan Yao,Fen Xiao,Xiarun Chen,Jia Liu,YongQiang He,Weiping Wen*

Key words: 语音后门攻击, 语音大语言模型, 多重梯度下降算法, 音色触发器, 情感触发器

TL;DR: 提出了基于语音大语言模型（SLLM）的语音提示后门攻击（SPBA），通过利用音色和情感等语音元素生成多样化触发器，显著提高攻击效果。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 语音深度学习任务（如关键词识别和说话人验证）存在后门攻击的漏洞，现有方法的触发器数量有限，攻击效果不足。

Method: 利用SLLM生成多样化的音色和情感触发器，采用多重梯度下降算法（MGDA）优化攻击成本。

Result: 在两项语音分类任务中，SPBA展现了显著的触发器有效性，并在攻击指标上表现优异。

Conclusion: SPBA通过多样化触发器显著提升后门攻击效果，为语音任务的安全性提供了新思路。

Abstract: Deep speech classification tasks, including keyword spotting and speaker
verification, are vital in speech-based human-computer interaction. Recently,
the security of these technologies has been revealed to be susceptible to
backdoor attacks. Specifically, attackers use noisy disruption triggers and
speech element triggers to produce poisoned speech samples that train models to
become vulnerable. However, these methods typically create only a limited
number of backdoors due to the inherent constraints of the trigger function. In
this paper, we propose that speech backdoor attacks can strategically focus on
speech elements such as timbre and emotion, leveraging the Speech Large
Language Model (SLLM) to generate diverse triggers. Increasing the number of
triggers may disproportionately elevate the poisoning rate, resulting in higher
attack costs and a lower success rate per trigger. We introduce the Multiple
Gradient Descent Algorithm (MGDA) as a mitigation strategy to address this
challenge. The proposed attack is called the Speech Prompt Backdoor Attack
(SPBA). Building on this foundation, we conducted attack experiments on two
speech classification tasks, demonstrating that SPBA shows significant trigger
effectiveness and achieves exceptional performance in attack metrics.

</details>


### [241] [MD-ViSCo: A Unified Model for Multi-Directional Vital Sign Waveform Conversion](https://arxiv.org/abs/2506.08357)
*Franck Meyer,Kyunghoon Hur,Edward Choi*

Key words: MD-ViSCo, 波形生成, 统一框架, 深度学习, 医疗监控

TL;DR: 提出了一种统一框架MD-ViSCo，能够通过单一模型生成多种目标生命体征波形，优于现有技术。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 解决现有模型需要为每种特定的源-目标波形对设计独立模型的问题，提高临床实用性。

Method: 采用1维U-Net与Swin Transformer结合，利用AdaIN捕获波形风格。

Result: 在两个数据集上平均降低MAE 8.8%，提高PC 4.9%，ABP波形满足AAMI和BHS标准。

Conclusion: MD-ViSCo提供了一种统一框架，可处理多种生命体征波形，简化医疗监控。

Abstract: Despite the remarkable progress of deep-learning methods generating a target
vital sign waveform from a source vital sign waveform, most existing models are
designed exclusively for a specific source-to-target pair. This requires
distinct model architectures, optimization procedures, and pre-processing
pipelines, resulting in multiple models that hinder usability in clinical
settings. To address this limitation, we propose the Multi-Directional
Vital-Sign Converter (MD-ViSCo), a unified framework capable of generating any
target waveform such as electrocardiogram (ECG), photoplethysmogram (PPG), or
arterial blood pressure (ABP) from any single input waveform with a single
model. MD-ViSCo employs a shallow 1-Dimensional U-Net integrated with a Swin
Transformer that leverages Adaptive Instance Normalization (AdaIN) to capture
distinct waveform styles. To evaluate the efficacy of MD-ViSCo, we conduct
multi-directional waveform generation on two publicly available datasets. Our
framework surpasses state-of-the-art baselines (NabNet & PPG2ABP) on average
across all waveform types, lowering Mean absolute error (MAE) by 8.8% and
improving Pearson correlation (PC) by 4.9% over two datasets. In addition, the
generated ABP waveforms satisfy the Association for the Advancement of Medical
Instrumentation (AAMI) criterion and achieve Grade B on the British
Hypertension Society (BHS) standard, outperforming all baselines. By
eliminating the need for developing a distinct model for each task, we believe
that this work offers a unified framework that can deal with any kind of vital
sign waveforms with a single model in healthcare monitoring.

</details>


### [242] [Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model](https://arxiv.org/abs/2506.08967)
*Ailin Huang,Bingxin Li,Bruce Wang,Boyong Wu,Chao Yan,Chengli Feng,Heng Wang,Hongyu Zhou,Hongyuan Wang,Jingbei Li,Jianjian Sun,Joanna Wang,Mingrui Chen,Peng Liu,Ruihang Miao,Shilei Jiang,Tian Fei,Wang You,Xi Chen,Xuerui Yang,Yechang Huang,Yuxiang Zhang,Zheng Ge,Zheng Gong,Zhewei Huang,Zixin Zhang,Bin Wang,Bo Li,Buyun Ma,Changxin Miao,Changyi Wan,Chen Xu,Dapeng Shi,Dingyuan Hu,Enle Liu,Guanzhe Huang,Gulin Yan,Hanpeng Hu,Haonan Jia,Jiahao Gong,Jiaoren Wu,Jie Wu,Jie Yang,Junzhe Lin,Kaixiang Li,Lei Xia,Longlong Gu,Ming Li,Nie Hao,Ranchen Ming,Shaoliang Pang,Siqi Liu,Song Yuan,Tiancheng Cao,Wen Li,Wenqing He,Xu Zhao,Xuelin Zhang,Yanbo Yu,Yinmin Zhong,Yu Zhou,Yuanwei Liang,Yuanwei Lu,Yuxiang Yang,Zidong Yang,Zili Zhang,Binxing Jiao,Heung-Yeung Shum,Jiansheng Chen,Jing Li,Xiangyu Zhang,Xinhao Zhang,Yibo Zhu,Daxin Jiang,Shuchang Zhou,Chen Hu*

Key words: 大型音频语言模型、端到端系统、音频查询-音频答案任务、双码本分词器、语义连贯性

TL;DR: 提出了一种端到端的大型音频语言模型 Step-Audio-AQAA，专为音频查询-音频答案任务设计，解决了现有模型依赖文本输出的限制。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 现有大型音频语言模型（LALMs）依赖文本输出，无法直接生成自然语音，限制了音频交互的流畅性。

Method: 模型集成了双码本音频分词器（提取语言和语义特征）、1300亿参数的主干 LLM 和神经声码器（高保真语音合成），并通过交错文本和音频的令牌输出训练优化语义一致性。

Result: 在 StepEval-Audio-360 基准测试中，Step-Audio-AQAA 在语音控制等关键领域优于现有 LALMs。

Conclusion: 该模型为端到端 LALMs 提供了有前景的解决方案，并突出了基于令牌的声码器对 AQAA 任务的重要性。

Abstract: Large Audio-Language Models (LALMs) have significantly advanced intelligent
human-computer interaction, yet their reliance on text-based outputs limits
their ability to generate natural speech responses directly, hindering seamless
audio interactions. To address this, we introduce Step-Audio-AQAA, a fully
end-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model
integrates a dual-codebook audio tokenizer for linguistic and semantic feature
extraction, a 130-billion-parameter backbone LLM and a neural vocoder for
high-fidelity speech synthesis. Our post-training approach employs interleaved
token-output of text and audio to enhance semantic coherence and combines
Direct Preference Optimization (DPO) with model merge to improve performance.
Evaluations on the StepEval-Audio-360 benchmark demonstrate that
Step-Audio-AQAA excels especially in speech control, outperforming the
state-of-art LALMs in key areas. This work contributes a promising solution for
end-to-end LALMs and highlights the critical role of token-based vocoder in
enhancing overall performance for AQAA tasks.

</details>


### [243] [Teaching Physical Awareness to LLMs through Sounds](https://arxiv.org/abs/2506.08524)
*Weiguo Wang,Andy Nie,Wenrui Zhou,Yi Kai,Chengchen Hu*

Key words: 大型语言模型,物理意识,声音处理,模拟器,数据集

TL;DR: ACORN框架通过声音教导大型语言模型（LLM）物理意识，利用物理模拟器生成多样训练数据，构建AQA-PHY数据集，并展示在模拟和现实任务中的合理表现。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 大型语言模型在文本和多模态处理上表现出色，但缺乏对现实世界物理现象的认知。

Method: 引入基于物理的模拟器生成训练数据，构建AQA-PHY数据集，并提出处理幅度和相位信息的音频编码器。

Result: 在视线检测、多普勒效应估计和到达方向估计等任务中取得合理结果。

Conclusion: ACORN为LLM理解物理世界铺平了道路。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in text and
multimodal processing, yet they fundamentally lack physical
awareness--understanding of real-world physical phenomena. In this work, we
present ACORN, a framework that teaches LLMs physical awareness through sound,
focusing on fundamental physical phenomena like the Doppler effect, multipath
effect, and spatial relationships. To overcome data scarcity, ACORN introduce a
physics-based simulator combining real-world sound sources with controlled
physical channels to generate diverse training data. Using this simulator, we
build AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an
audio encoder that processes both magnitude and phase information. By
connecting our audio encoder to state-of-the-art LLMs, we demonstrate
reasonable results in both simulated and real-world tasks, such as
line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival
estimation, paving the way for enabling LLMs to understand physical world.

</details>


### [244] [Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation](https://arxiv.org/abs/2506.08570)
*Or Tal,Felix Kreuk,Yossi Adi*

Key words: 文本到音乐生成、自回归解码、条件流匹配、模型评估

TL;DR: 该论文比较了文本到音乐生成的两种常见建模范式（自回归解码和条件流匹配），通过控制变量实验分析其性能和适用性。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 当前文本到音乐生成模型的多样性导致公平评估和设计选择的困难，作者专注于建模范式的影响分析。

Method: 使用相同数据集、训练配置和类似架构，比较自回归解码和条件流匹配两种范式。

Result: 通过多维度评估（生成质量、推理配置鲁棒性等），揭示了两种范式的优缺点。

Conclusion: 研究结果为未来文本到音乐生成系统的设计和训练提供了实用指导。

Abstract: Recent progress in text-to-music generation has enabled models to synthesize
high-quality musical segments, full compositions, and even respond to
fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)
systems differ significantly across many dimensions, such as training datasets,
modeling paradigms, and architectural choices. This diversity complicates
efforts to evaluate models fairly and pinpoint which design choices most
influence performance. While factors like data and architecture are important,
in this study we focus exclusively on the modeling paradigm. We conduct a
systematic empirical analysis to isolate its effects, offering insights into
associated trade-offs and emergent behaviors that can guide future
text-to-music generation systems. Specifically, we compare the two arguably
most common modeling paradigms: Auto-Regressive decoding and Conditional
Flow-Matching. We conduct a controlled comparison by training all models from
scratch using identical datasets, training configurations, and similar backbone
architectures. Performance is evaluated across multiple axes, including
generation quality, robustness to inference configurations, scalability,
adherence to both textual and temporally aligned conditioning, and editing
capabilities in the form of audio inpainting. This comparative study sheds
light on distinct strengths and limitations of each paradigm, providing
actionable insights that can inform future architectural and training decisions
in the evolving landscape of text-to-music generation. Audio sampled examples
are available at: https://huggingface.co/spaces/ortal1602/ARvsFM

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [245] [Solving excited states for long-range interacting trapped ions with neural networks](https://arxiv.org/abs/2506.08594)
*Yixuan Ma,Chang Liu,Weikang Li,Shun-Yao Zhang,L. -M. Duan,Yukai Wu,Dong-Ling Deng*

Key words: 量子多体系统，激发态计算，神经网络算法，长程相互作用，Wigner晶体

TL;DR: 神经网络算法(NQES)用于高效计算量子多体系统的低激发态，适用于高维系统，且无需显式正交化。通过具体实例验证其准确性，并应用于长程相互作用系统的相关研究。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 强相互作用量子多体系统的激发态计算因希尔伯特空间维度的指数增长极具挑战性。开发高效且通用的计算方法具有重要意义。

Method: 提出基于神经网络的NQES算法，无需显式正交化，适用于高维系统。通过Haldane-Shastry模型及长程相互作用离子系统验证其有效性。

Result: NQES算法成功计算多激发态及其可观测量，揭示长程相互作用系统中的能隙缩放及相关特征，实验结果与理论预测吻合。

Conclusion: NQES算法为量子多体系统的激发态计算提供了可扩展的高效工具，具有从量子设备基准测试到光异构化的潜在应用。

Abstract: The computation of excited states in strongly interacting quantum many-body
systems is of fundamental importance. Yet, it is notoriously challenging due to
the exponential scaling of the Hilbert space dimension with the system size.
Here, we introduce a neural network-based algorithm that can simultaneously
output multiple low-lying excited states of a quantum many-body spin system in
an accurate and efficient fashion. This algorithm, dubbed the neural quantum
excited-state (NQES) algorithm, requires no explicit orthogonalization of the
states and is generally applicable to higher dimensions. We demonstrate,
through concrete examples including the Haldane-Shastry model with all-to-all
interactions, that the NQES algorithm is capable of efficiently computing
multiple excited states and their related observable expectations. In addition,
we apply the NQES algorithm to two classes of long-range interacting
trapped-ion systems in a two-dimensional Wigner crystal. For non-decaying
all-to-all interactions with alternating signs, our computed low-lying excited
states bear spatial correlation patterns similar to those of the ground states,
which closely match recent experimental observations that the
quasi-adiabatically prepared state accurately reproduces analytical
ground-state correlations. For a system of up to 300 ions with power-law
decaying antiferromagnetic interactions, we successfully uncover its gap
scaling and correlation features. Our results establish a scalable and
efficient algorithm for computing excited states of interacting quantum
many-body systems, which holds potential applications ranging from benchmarking
quantum devices to photoisomerization.

</details>


### [246] [Systematic and Efficient Construction of Quadratic Unconstrained Binary Optimization Forms for High-order and Dense Interactions](https://arxiv.org/abs/2506.08448)
*Hyakka Nakada,Shu Tanaka*

Key words: 量子退火, 二次化, 机器学习, 组合优化, ReLU基

TL;DR: 该论文提出了一种基于ReLU基的二次化方法，用于将复杂机器学习问题转化为QUBOs，从而利用量子退火(QA)解决组合优化问题，并通过数值和理论验证其有效性。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 量子退火(QA)在解决QUBO形式的组合优化问题时表现高效，但对于涉及机器学习(ML)的复杂高阶问题，现有的二次化方法难以应对其强非线性和密集交互特性。

Method: 论文提出使用ReLU基对目标函数建模，因其具有通用逼近能力且能等价表示为二次多项式，从而实现二次化；进一步设计了结合QA和二次化的黑盒优化方案。

Result: 通过数值和理论验证了所提方法的可行性和有效性，并将其应用于QA优化的黑盒流程中。

Conclusion: 基于ReLU基的二次化方法为复杂ML问题的量子优化提供了新思路，拓展了QA的应用范围。

Abstract: Quantum Annealing (QA) can efficiently solve combinatorial optimization
problems whose objective functions are represented by Quadratic Unconstrained
Binary Optimization (QUBO) formulations. For broader applicability of QA,
quadratization methods are used to transform higher-order problems into QUBOs.
However, quadratization methods for complex problems involving Machine Learning
(ML) remain largely unknown. In these problems, strong nonlinearity and dense
interactions prevent conventional methods from being applied. Therefore, we
model target functions by the sum of rectified linear unit bases, which not
only have the ability of universal approximation, but also have an equivalent
quadratic-polynomial representation. In this study, the proof of concept is
verified both numerically and analytically. In addition, by combining QA with
the proposed quadratization, we design a new black-box optimization scheme, in
which ML surrogate regressors are inputted to QA after the quadratization
process.

</details>


### [247] [The interplay of robustness and generalization in quantum machine learning](https://arxiv.org/abs/2506.08455)
*Julian Berberich,Tobias Fellner,Christian Holm*

Key words: 量子机器学习、对抗鲁棒性、泛化性、Lipschitz界限、变分量子模型

TL;DR: 论文探讨了量子机器学习中对抗鲁棒性和泛化性的相互作用，提出了一种基于Lipschitz界限的变分量子模型训练方法。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 研究量子机器学习中对抗鲁棒性与泛化性的相互关系，填补现有文献中的空白。

Method: 利用Lipschitz界限量化鲁棒性和泛化性，并提出基于正则化的训练方法。

Result: 理论结果在时间序列分析中得到验证，展示了其实际应用价值。

Conclusion: 通过正则化方法训练量子模型可以同时提高对抗鲁棒性和泛化性，强调可训练数据编码策略的重要性。

Abstract: While adversarial robustness and generalization have individually received
substantial attention in the recent literature on quantum machine learning,
their interplay is much less explored. In this chapter, we address this
interplay for variational quantum models, which were recently proposed as
function approximators in supervised learning. We discuss recent results
quantifying both robustness and generalization via Lipschitz bounds, which
explicitly depend on model parameters. Thus, they give rise to a
regularization-based training approach for robust and generalizable quantum
models, highlighting the importance of trainable data encoding strategies. The
practical implications of the theoretical results are demonstrated with an
application to time series analysis.

</details>


### [248] [Quantum Adiabatic Generation of Human-Like Passwords](https://arxiv.org/abs/2506.08917)
*Sascha Mücke,Raoul Heese,Thore Gerlach,David Biesner,Loong Kuan Lee,Nico Piatkowski*

Key words: 量子计算, 密码生成, QUBO, UD-MIS, 绝热量子计算

TL;DR: 论文探讨如何使用绝热量子计算机生成类似人类行为的密码，通过QUBO和UD-MIS问题的新方法，结果表明小样本即可生成真实的密码。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 研究量子计算是否能在减少资源需求的同时，生成真实的密码以测试认证系统的安全性。

Method: 提出基于QUBO和UD-MIS问题的编码方法，利用绝热量子计算机生成密码。

Result: 在256量子位的量子计算机上生成了128个密码，其中包含类似人类行为的密码。

Conclusion: 量子计算机在生成短语义结构（如密码）上具有潜力。

Abstract: Generative Artificial Intelligence (GenAI) for Natural Language Processing
(NLP) is the predominant AI technology to date. An important perspective for
Quantum Computing (QC) is the question whether QC has the potential to reduce
the vast resource requirements for training and operating GenAI models. While
large-scale generative NLP tasks are currently out of reach for practical
quantum computers, the generation of short semantic structures such as
passwords is not. Generating passwords that mimic real user behavior has many
applications, for example to test an authentication system against realistic
threat models. Classical password generation via deep learning have recently
been investigated with significant progress in their ability to generate novel,
realistic password candidates. In the present work we investigate the utility
of adiabatic quantum computers for this task. More precisely, we study
different encodings of token strings and propose novel approaches based on the
Quadratic Unconstrained Binary Optimization (QUBO) and the Unit-Disk Maximum
Independent Set (UD-MIS) problems. Our approach allows us to estimate the token
distribution from data and adiabatically prepare a quantum state from which we
eventually sample the generated passwords via measurements. Our results show
that relatively small samples of 128 passwords, generated on the QuEra Aquila
256-qubit neutral atom quantum computer, contain human-like passwords such as
"Tunas200992" or "teedem28iglove".

</details>


### [249] [Superposed Parameterised Quantum Circuits](https://arxiv.org/abs/2506.08749)
*Viktoria Patapovich,Mo Kordzanganeh,Alexey Melnikov*

Key words: 量子机器学习、超参数化量子电路、量子随机存取内存、多项式激活函数

TL;DR: 论文提出了一种基于超参数化量子电路的量子机器学习方法，通过结合翻转量子随机存取内存和重复直到成功的协议，显著提升了模型的表达能力和可扩展性。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 现有的量子机器学习方法通常依赖于线性幺正操作和共享可训练参数，这限制了模型的表达能力和可扩展性。本文旨在克服这些限制。

Method: 引入了超参数化量子电路，结合翻转量子随机存取内存和重复直到成功的协议，将指数级数量的参数化子模型嵌入单一电路中，并通过幅度变换和后选择实现多项式激活函数。

Result: 数值实验表明，该方法在1D阶跃函数回归任务中将均方误差降低了三个数量级；在2D星形分类任务中，准确率提升至81.4%，运行方差减少三倍。

Conclusion: 超参数化量子电路为构建更深层、更通用的量子电路提供了一种高效的硬件实现路径，能够学习复杂的决策边界。

Abstract: Quantum machine learning has shown promise for high-dimensional data
analysis, yet many existing approaches rely on linear unitary operations and
shared trainable parameters across outputs. These constraints limit
expressivity and scalability relative to the multi-layered, non-linear
architectures of classical deep networks. We introduce superposed parameterised
quantum circuits to overcome these limitations. By combining flip-flop quantum
random-access memory with repeat-until-success protocols, a superposed
parameterised quantum circuit embeds an exponential number of parameterised
sub-models in a single circuit and induces polynomial activation functions
through amplitude transformations and post-selection. We provide an analytic
description of the architecture, showing how multiple parameter sets are
trained in parallel while non-linear amplitude transformations broaden
representational power beyond conventional quantum kernels. Numerical
experiments underscore these advantages: on a 1D step-function regression a
two-qubit superposed parameterised quantum circuit cuts the mean-squared error
by three orders of magnitude versus a parameter-matched variational baseline;
on a 2D star-shaped two-dimensional classification task, introducing a
quadratic activation lifts accuracy to 81.4% and reduces run-to-run variance
three-fold. These results position superposed parameterised quantum circuits as
a hardware-efficient route toward deeper, more versatile parameterised quantum
circuits capable of learning complex decision boundaries.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [250] [UAVs Meet Agentic AI: A Multidomain Survey of Autonomous Aerial Intelligence and Agentic UAVs](https://arxiv.org/abs/2506.08045)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Key words: Agentic UAVs, 自主性, 人工智能, 应用领域, 技术挑战

TL;DR: 本文总结了Agentic UAVs（自主智能无人机）的核心技术、应用领域及未来发展方向，强调了其在复杂环境中的自主性和社会价值。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 研究旨在探讨Agentic UAVs如何整合感知、决策和协作规划技术，超越传统无人机，推动自主空中智能的发展。

Method: 通过分析架构组件、技术差异及应用案例，对比Agentic UAVs与传统无人机的进步。

Result: Agentic UAVs在多个高影响力领域（如精准农业、灾害响应等）展现潜力，同时面临技术、法规和数据可靠性挑战。

Conclusion: 未来需通过硬件创新、学习架构和人类-AI交互等解决方案，推动自演化空中生态系统的发展。

Abstract: Agentic UAVs represent a new frontier in autonomous aerial intelligence,
integrating perception, decision-making, memory, and collaborative planning to
operate adaptively in complex, real-world environments. Driven by recent
advances in Agentic AI, these systems surpass traditional UAVs by exhibiting
goal-driven behavior, contextual reasoning, and interactive autonomy. We
provide a comprehensive foundation for understanding the architectural
components and enabling technologies that distinguish Agentic UAVs from
traditional autonomous UAVs. Furthermore, a detailed comparative analysis
highlights advancements in autonomy with AI agents, learning, and mission
flexibility. This study explores seven high-impact application domains
precision agriculture, construction & mining, disaster response, environmental
monitoring, infrastructure inspection, logistics, security, and wildlife
conservation, illustrating the broad societal value of agentic aerial
intelligence. Furthermore, we identify key challenges in technical constraints,
regulatory limitations, and data-model reliability, and we present emerging
solutions across hardware innovation, learning architectures, and human-AI
interaction. Finally, a future roadmap is proposed, outlining pathways toward
self-evolving aerial ecosystems, system-level collaboration, and sustainable,
equitable deployments. This survey establishes a foundational framework for the
future development, deployment, and governance of agentic aerial systems
(Agentic UAVs) across diverse societal and industrial domains.

</details>


### [251] [Ego-centric Learning of Communicative World Models for Autonomous Driving](https://arxiv.org/abs/2506.08149)
*Hang Wang,Dechen Gao,Junshan Zhang*

Key words: 多智能体强化学习, 生成式AI, 世界模型, 信息共享, 自动驾驶

TL;DR: 该论文提出了一种名为CALL的多智能体强化学习方法，通过生成式AI和世界模型的潜在表示，解决了部分可观测性和非平稳性问题，同时减少了通信开销。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 多智能体强化学习（MARL）在处理高维复杂环境任务（如自动驾驶）时，面临部分可观测性和非平稳性问题，而传统的信息共享方法又存在通信开销大和可扩展性差的问题。

Method: CALL方法结合生成式AI的世界模型及其潜在表示，使每个智能体先学习其世界模型，并通过轻量级通信共享低维潜在表示，同时利用信息共享优化预测和规划。

Result: 实验在CARLA平台上进行，证明了CALL方法在局部轨迹规划任务中的性能提升。

Conclusion: CALL方法通过轻量级信息共享和潜在表示，显著提升了智能体的预测精度和任务性能。

Abstract: We study multi-agent reinforcement learning (MARL) for tasks in complex
high-dimensional environments, such as autonomous driving. MARL is known to
suffer from the \textit{partial observability} and \textit{non-stationarity}
issues. To tackle these challenges, information sharing is often employed,
which however faces major hurdles in practice, including overwhelming
communication overhead and scalability concerns. By making use of generative AI
embodied in world model together with its latent representation, we develop
{\it CALL}, \underline{C}ommunic\underline{a}tive Wor\underline{l}d
Mode\underline{l}, for MARL, where 1) each agent first learns its world model
that encodes its state and intention into low-dimensional latent representation
with smaller memory footprint, which can be shared with other agents of
interest via lightweight communication; and 2) each agent carries out
ego-centric learning while exploiting lightweight information sharing to enrich
her world model, and then exploits its generalization capacity to improve
prediction for better planning. We characterize the gain on the prediction
accuracy from the information sharing and its impact on performance gap.
Extensive experiments are carried out on the challenging local trajectory
planning tasks in the CARLA platform to demonstrate the performance gains of
using \textit{CALL}.

</details>


### [252] [Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning](https://arxiv.org/abs/2506.08344)
*Neşet Ünver Akmandor,Sarvesh Prajapati,Mark Zolotas,Taşkın Padır*

Key words: 运动规划, NMPC, DRL, 机器人

TL;DR: Re4MPC 是一种高效的多模型运动规划方法，结合 NMPC 和 DRL 进行任务适应性的轨迹生成，提高计算效率和成功率。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 传统的高自由度机器人运动规划方法计算成本高，不适合实际应用。

Method: 通过 NMPC 和 DRL 结合的方法，动态选择模型、成本和约束条件以高效生成轨迹。

Result: 实验表明 Re4MPC 比传统 NMPC 更具计算效率且成功率更高。

Conclusion: Re4MPC 为高自由度机器人提供了高效、适应性强的运动规划解决方案。

Abstract: Traditional motion planning methods for robots with many degrees-of-freedom,
such as mobile manipulators, are often computationally prohibitive for
real-world settings. In this paper, we propose a novel multi-model motion
planning pipeline, termed Re4MPC, which computes trajectories using Nonlinear
Model Predictive Control (NMPC). Re4MPC generates trajectories in a
computationally efficient manner by reactively selecting the model, cost, and
constraints of the NMPC problem depending on the complexity of the task and
robot state. The policy for this reactive decision-making is learned via a Deep
Reinforcement Learning (DRL) framework. We introduce a mathematical formulation
to integrate NMPC into this DRL framework. To validate our methodology and
design choices, we evaluate DRL training and test outcomes in a physics-based
simulation involving a mobile manipulator. Experimental results demonstrate
that Re4MPC is more computationally efficient and achieves higher success rates
in reaching end-effector goals than the NMPC baseline, which computes
whole-body trajectories without our learning mechanism.

</details>


### [253] [Diffusion Models for Safety Validation of Autonomous Driving Systems](https://arxiv.org/abs/2506.08459)
*Juanran Wang,Marc R. Schlichting,Harrison Delecki,Mykel J. Kochenderfer*

Key words: 安全验证, 自动驾驶, 去噪扩散模型, 故障案例

TL;DR: 论文提出了一种基于去噪扩散模型的方法，用于生成自动驾驶系统的潜在故障案例，以低成本和高效率解决安全验证问题。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 自动驾驶系统的安全验证成本高且风险大，而潜在故障案例的多样性和稀缺性进一步增加了挑战。

Method: 使用去噪扩散模型，基于初始交通状态生成潜在的自动驾驶车辆故障案例。

Result: 实验表明，该模型能在多种场景下生成真实的故障样本，且无需外部训练数据或大量计算资源。

Conclusion: 该方法适用于交通路口的安全验证，具有实际应用价值。

Abstract: Safety validation of autonomous driving systems is extremely challenging due
to the high risks and costs of real-world testing as well as the rarity and
diversity of potential failures. To address these challenges, we train a
denoising diffusion model to generate potential failure cases of an autonomous
vehicle given any initial traffic state. Experiments on a four-way intersection
problem show that in a variety of scenarios, the diffusion model can generate
realistic failure samples while capturing a wide variety of potential failures.
Our model does not require any external training dataset, can perform training
and inference with modest computing resources, and does not assume any prior
knowledge of the system under test, with applicability to safety validation for
traffic intersections.

</details>


### [254] [Bayesian Inverse Physics for Neuro-Symbolic Robot Learning](https://arxiv.org/abs/2506.08756)
*Octavio Arriaga,Rebecca Adam,Melvin Laux,Lisa Gutzeit,Marco Ragni,Jan Peters,Frank Kirchner*

Key words: 机器人, 深度学习, 符号推理, 可微物理, 贝叶斯推理, 元学习

TL;DR: 讨论机器人技术中深度学习与符号推理结合的框架，提出通过可微物理、贝叶斯推理和元学习提升适应性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 现有深度学习在动态环境中效率与可靠性不足，需结合符号推理以提升机器人适应性与泛化能力。

Method: 提出结合可微物理的世界建模、贝叶斯推理的决策和元学习的任务快速适应，构建混合神经符号架构。

Result: 框架能使机器人泛化训练数据外的场景，推理新情境并持续扩展知识。

Conclusion: 混合神经符号架构是下一代自主系统的关键，需加速研发。

Abstract: Real-world robotic applications, from autonomous exploration to assistive
technologies, require adaptive, interpretable, and data-efficient learning
paradigms. While deep learning architectures and foundation models have driven
significant advances in diverse robotic applications, they remain limited in
their ability to operate efficiently and reliably in unknown and dynamic
environments. In this position paper, we critically assess these limitations
and introduce a conceptual framework for combining data-driven learning with
deliberate, structured reasoning. Specifically, we propose leveraging
differentiable physics for efficient world modeling, Bayesian inference for
uncertainty-aware decision-making, and meta-learning for rapid adaptation to
new tasks. By embedding physical symbolic reasoning within neural models,
robots could generalize beyond their training data, reason about novel
situations, and continuously expand their knowledge. We argue that such hybrid
neuro-symbolic architectures are essential for the next generation of
autonomous systems, and to this end, we provide a research roadmap to guide and
accelerate their development.

</details>


### [255] [Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning](https://arxiv.org/abs/2506.08795)
*Kaijie Shi,Wanglong Lu,Hanli Zhao,Vinicius Prado da Fonseca,Ting Zou,Xianta Jiang*

Key words: 假手控制、全自主系统、模仿学习、表面肌电、摄像头

TL;DR: 研究开发了一种全自主控制的假手系统，仅通过手腕上的摄像头实现自动抓取和释放物体，显著减轻使用者的心理负担。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 传统表面肌电和半自主方法需要用户为每个控制生成肌电信号，对身心造成负担。本研究旨在通过全自主系统提高假手使用便利性。

Method: 采用模仿学习算法，通过收集人类演示数据训练假手控制模型，仅需少量数据即可实现高成功率。

Result: 模型在训练后能泛化到不同个体和未见过的物体，且适应不同重量。

Conclusion: 该系统提供了一种易于使用的假手控制界面，显著降低了使用时的心理努力。

Abstract: Limb loss affects millions globally, impairing physical function and reducing
quality of life. Most traditional surface electromyographic (sEMG) and
semi-autonomous methods require users to generate myoelectric signals for each
control, imposing physically and mentally taxing demands. This study aims to
develop a fully autonomous control system that enables a prosthetic hand to
automatically grasp and release objects of various shapes using only a camera
attached to the wrist. By placing the hand near an object, the system will
automatically execute grasping actions with a proper grip force in response to
the hand's movements and the environment. To release the object being grasped,
just naturally place the object close to the table and the system will
automatically open the hand. Such a system would provide individuals with limb
loss with a very easy-to-use prosthetic control interface and greatly reduce
mental effort while using. To achieve this goal, we developed a teleoperation
system to collect human demonstration data for training the prosthetic hand
control model using imitation learning, which mimics the prosthetic hand
actions from human. Through training the model using only a few objects' data
from one single participant, we have shown that the imitation learning
algorithm can achieve high success rates, generalizing to more individuals and
unseen objects with a variation of weights. The demonstrations are available at
\href{https://sites.google.com/view/autonomous-prosthetic-hand}{https://sites.google.com/view/autonomous-prosthetic-hand}

</details>


### [256] [FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency](https://arxiv.org/abs/2506.08822)
*Yifei Su,Ning Liu,Dong Chen,Zhen Zhao,Kun Wu,Meng Li,Zhiyuan Xu,Zhengping Che,Jian Tang*

Key words: 生成建模,视觉运动策略,频率一致性,单步动作生成,机器人操作

TL;DR: FreqPolicy通过频率一致性约束提升流式视觉运动策略的效率和连续性，支持高质量单步动作生成，并在仿真和现实任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 生成建模的视觉运动策略因能建模多模态动作分布而被广泛应用，但多步采样的高推理成本限制了其实时性。现有加速方法忽视了时间依赖性，而机器人操作需要连续性和时间一致性。

Method: 提出FreqPolicy，通过频率一致性约束和对齐频域动作特征，增强时间结构建模，支持高效单步动作生成，并提出自适应一致性损失捕捉任务结构变化。

Result: 在3个仿真基准的53项任务中优于现有单步生成器，40项任务的VLA模型加速而无性能下降，现实场景推理频率达93.5Hz。

Conclusion: FreqPolicy在保持高质量动作生成的同时显著提升效率，适用于复杂机器人操作任务。

Abstract: Generative modeling-based visuomotor policies have been widely adopted in
robotic manipulation attributed to their ability to model multimodal action
distributions. However, the high inference cost of multi-step sampling limits
their applicability in real-time robotic systems. To address this issue,
existing approaches accelerate the sampling process in generative
modeling-based visuomotor policies by adapting acceleration techniques
originally developed for image generation. Despite this progress, a major
distinction remains: image generation typically involves producing independent
samples without temporal dependencies, whereas robotic manipulation involves
generating time-series action trajectories that require continuity and temporal
coherence. To effectively exploit temporal information in robotic manipulation,
we propose FreqPolicy, a novel approach that first imposes frequency
consistency constraints on flow-based visuomotor policies. Our work enables the
action model to capture temporal structure effectively while supporting
efficient, high-quality one-step action generation. We introduce a frequency
consistency constraint that enforces alignment of frequency-domain action
features across different timesteps along the flow, thereby promoting
convergence of one-step action generation toward the target distribution. In
addition, we design an adaptive consistency loss to capture structural temporal
variations inherent in robotic manipulation tasks. We assess FreqPolicy on 53
tasks across 3 simulation benchmarks, proving its superiority over existing
one-step action generators. We further integrate FreqPolicy into the
vision-language-action (VLA) model and achieve acceleration without performance
degradation on the 40 tasks of Libero. Besides, we show efficiency and
effectiveness in real-world robotic scenarios with an inference frequency
93.5Hz. The code will be publicly available.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [257] [Domain Switching on the Pareto Front: Multi-Objective Deep Kernel Learning in Automated Piezoresponse Force Microscopy](https://arxiv.org/abs/2506.08073)
*Yu Liu,Utkarsh Pratiush,Kamyar Barakati,Hiroshi Funakubo,Ching-Che Lin,Jaegyu Kim,Lane W. Martin,Sergei V. Kalinin*

Key words: 铁电极化切换, 多目标核学习, 高通量主动学习, 畴壁构型, 微观结构

TL;DR: 提出了一种多目标核学习工作流程，通过高分辨率成像数据推断微观结构规则，解析铁电极化切换行为。该框架高效识别了畴壁构型与局部切换动力学的关系，并揭示了缺陷分布对极化反转的影响。方法适用于高通量主动学习，并可推广到其他复杂设计空间。

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

Motivation: 铁电极化切换行为的研究受限于复杂的局部微观结构特征，传统手动或网格测量方法难以系统探索，因此需要一种自动化、高效的解决方案。

Method: 采用多目标核学习工作流程，结合自动化的压电力显微镜实验，分析高分辨率成像数据，推断微观结构规则与切换行为的关联。

Result: 成功揭示了畴壁构型和缺陷分布对极化反转的关键调控作用，并将抽象的奖励函数（如切换难易度和畴对称性）映射到可物理解的描述符上。

Conclusion: 该方法不仅适用于铁电畴切换研究，还可推广到其他复杂设计空间的优化问题，如分子发现的结构-性质相关性和多模态成像组合优化。

Abstract: Ferroelectric polarization switching underpins the functional performance of
a wide range of materials and devices, yet its dependence on complex local
microstructural features renders systematic exploration by manual or grid-based
spectroscopic measurements impractical. Here, we introduce a multi-objective
kernel-learning workflow that infers the microstructural rules governing
switching behavior directly from high-resolution imaging data. Applied to
automated piezoresponse force microscopy (PFM) experiments, our framework
efficiently identifies the key relationships between domain-wall configurations
and local switching kinetics, revealing how specific wall geometries and defect
distributions modulate polarization reversal. Post-experiment analysis projects
abstract reward functions, such as switching ease and domain symmetry, onto
physically interpretable descriptors including domain configuration and
proximity to boundaries. This enables not only high-throughput active learning,
but also mechanistic insight into the microstructural control of switching
phenomena. While demonstrated for ferroelectric domain switching, our approach
provides a powerful, generalizable tool for navigating complex,
non-differentiable design spaces, from structure-property correlations in
molecular discovery to combinatorial optimization across diverse imaging
modalities.

</details>


### [258] [Mic-hackathon 2024: Hackathon on Machine Learning for Electron and Scanning Probe Microscopy](https://arxiv.org/abs/2506.08423)
*Utkarsh Pratiush,Austin Houston,Kamyar Barakati,Aditya Raghavan,Dasol Yoon,Harikrishnan KP,Zhaslan Baraissov,Desheng Ma,Samuel S. Welborn,Mikolaj Jakowski,Shawn-Patrick Barhorst,Alexander J. Pattison,Panayotis Manganaris,Sita Sirisha Madugula,Sai Venkata Gayathri Ayyagari,Vishal Kennedy,Ralph Bulanadi,Michelle Wang,Kieran J. Pang,Ian Addison-Smith,Willy Menacho,Horacio V. Guzman,Alexander Kiefer,Nicholas Furth,Nikola L. Kolev,Mikhail Petrov,Viktoriia Liu,Sergey Ilyev,Srikar Rairao,Tommaso Rodani,Ivan Pinto-Huguet,Xuli Chen,Josep Cruañes,Marta Torrens,Jovan Pomar,Fanzhi Su,Pawan Vedanti,Zhiheng Lyu,Xingzhi Wang,Lehan Yao,Amir Taqieddin,Forrest Laskowski,Xiangyu Yin,Yu-Tsun Shao,Benjamin Fein-Ashley,Yi Jiang,Vineet Kumar,Himanshu Mishra,Yogesh Paul,Adib Bazgir,Rama chandra Praneeth Madugula,Yuwen Zhang,Pravan Omprakash,Jian Huang,Eric Montufar-Morales,Vivek Chawla,Harshit Sethi,Jie Huang,Lauri Kurki,Grace Guinan,Addison Salvador,Arman Ter-Petrosyan,Madeline Van Winkle,Steven R. Spurgeon,Ganesh Narasimha,Zijie Wu,Richard Liu,Yongtao Liu,Boris Slautin,Andrew R Lupini,Rama Vasudevan,Gerd Duscher,Sergei V. Kalinin*

Key words: 显微镜,机器学习,数据管理,黑客马拉松,数字孪生

TL;DR: 显微镜数据管理未标准化导致分析效率低，机器学习与显微镜社区之间存在鸿沟；黑客马拉松通过协作推动创新，形成标准化工作流程和数据集。

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

Motivation: 解决显微镜数据管理不一致和机器学习应用不足的问题，以提升材料和物理研究的效率。

Method: 通过黑客马拉松促进机器学习与显微镜专家协作，开发标准化代码与工作流程。

Result: 生成基准数据集和显微镜数字孪生，推动社区发展与标准化分析。

Conclusion: 黑客马拉松是弥合技术与应用鸿沟的有效方式，为未来研究奠定基础。

Abstract: Microscopy is a primary source of information on materials structure and
functionality at nanometer and atomic scales. The data generated is often
well-structured, enriched with metadata and sample histories, though not always
consistent in detail or format. The adoption of Data Management Plans (DMPs) by
major funding agencies promotes preservation and access. However, deriving
insights remains difficult due to the lack of standardized code ecosystems,
benchmarks, and integration strategies. As a result, data usage is inefficient
and analysis time is extensive. In addition to post-acquisition analysis, new
APIs from major microscope manufacturers enable real-time, ML-based analytics
for automated decision-making and ML-agent-controlled microscope operation.
Yet, a gap remains between the ML and microscopy communities, limiting the
impact of these methods on physics, materials discovery, and optimization.
Hackathons help bridge this divide by fostering collaboration between ML
researchers and microscopy experts. They encourage the development of novel
solutions that apply ML to microscopy, while preparing a future workforce for
instrumentation, materials science, and applied ML. This hackathon produced
benchmark datasets and digital twins of microscopes to support community growth
and standardized workflows. All related code is available at GitHub:
https://github.com/KalininGroup/Mic-hackathon-2024-codes-publication/tree/1.0.0.1

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [259] [The World of AI: A Novel Approach to AI Literacy for First-year Engineering Students](https://arxiv.org/abs/2506.08041)
*Siddharth Siddharth,Brainerd Prince,Amol Harsh,Shreyas Ramachandran*

Key words: AI教育，跨学科，工程学生，社会影响，环境挑战

TL;DR: 本文介绍了一门名为《AI世界》的新课程，专为没有AI基础的一年级本科生设计，旨在填补其认知空白，通过跨学科方式教授AI的基本原理和社会影响。课程分为三个模块，结果显示学生的认知显著提升。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 解决工程专业学生缺乏AI基础知识及社会影响认知的问题。

Method: 设计跨学科课程，结合工程与人文学科的教学内容，分为环境、社会和职场三个模块。

Result: 学生通过课程提高了对AI环境影响的认知，并学会解决公平性问题，改变了对AI社会影响的看法。

Conclusion: 课程的跨学科设计和教学方法有效提升了学生对AI的理解和认知。

Abstract: This work presents a novel course titled The World of AI designed for
first-year undergraduate engineering students with little to no prior exposure
to AI. The central problem addressed by this course is that engineering
students often lack foundational knowledge of AI and its broader societal
implications at the outset of their academic journeys. We believe the way to
address this gap is to design and deliver an interdisciplinary course that can
a) be accessed by first-year undergraduate engineering students across any
domain, b) enable them to understand the basic workings of AI systems sans
mathematics, and c) make them appreciate AI's far-reaching implications on our
lives. The course was divided into three modules co-delivered by faculty from
both engineering and humanities. The planetary module explored AI's dual role
as both a catalyst for sustainability and a contributor to environmental
challenges. The societal impact module focused on AI biases and concerns around
privacy and fairness. Lastly, the workplace module highlighted AI-driven job
displacement, emphasizing the importance of adaptation. The novelty of this
course lies in its interdisciplinary curriculum design and pedagogical
approach, which combines technical instruction with societal discourse. Results
revealed that students' comprehension of AI challenges improved across diverse
metrics like (a) increased awareness of AI's environmental impact, and (b)
efficient corrective solutions for AI fairness. Furthermore, it also indicated
the evolution in students' perception of AI's transformative impact on our
lives.

</details>


### [260] [Evaluation of Machine Learning Models in Student Academic Performance Prediction](https://arxiv.org/abs/2506.08047)
*A. G. R. Sandeepa,Sanka Mohottala*

Key words: 机器学习，学术表现预测，MLPC，特征选择，可解释AI

TL;DR: 机器学习方法用于预测学生学术表现，MLPC模型表现最佳，测试集准确率86.46%，交叉验证平均79.58%。特征选择和可解释方法提升模型效果。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 研究旨在通过机器学习预测学生学术表现，验证神经网络在高数据效率下的潜力。

Method: 使用多层感知器分类器（MLPC）及其他标准模型，结合特征选择和交叉验证。

Result: MLPC在测试集上最高准确率86.46%，交叉验证平均79.58%，显著优于其他模型。

Conclusion: MLPC模型高效且数据友好，特征选择和可解释方法对性能提升至关重要。

Abstract: This research investigates the use of machine learning methods to forecast
students' academic performance in a school setting. Students' data with
behavioral, academic, and demographic details were used in implementations with
standard classical machine learning models including multi-layer perceptron
classifier (MLPC). MLPC obtained 86.46% maximum accuracy for test set across
all implementations. Under 10-fold cross validation, MLPC obtained 79.58%
average accuracy for test set while for train set, it was 99.65%. MLP's better
performance over other machine learning models strongly suggest the potential
use of neural networks as data-efficient models. Feature selection approach
played a crucial role in improving the performance and multiple evaluation
approaches were used in order to compare with existing literature. Explainable
machine learning methods were utilized to demystify the black box models and to
validate the feature selection approach.

</details>


### [261] [WIP: Large Language Model-Enhanced Smart Tutor for Undergraduate Circuit Analysis](https://arxiv.org/abs/2506.08962)
*Liangliang Chen,Huiru Xie,Jacqueline Rohde,Ying Zhang*

Key words: AI智能导师,电路分析,作业反馈,个性化教学,数据分析

TL;DR: 这篇正在进行的研究介绍了为本科生电路分析课程设计的AI智能导师，提供作业评估和反馈，并展示了初步效果。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 为了解决本科电路分析课程中学生作业反馈不足的问题，提升教学效果和个性化指导。

Method: 设计了智能导师系统，包括开放式问题回答和作业反馈生成模块，部署在微软Azure平台上。

Result: 90.9%的学生对该导师表示满意，初步数据分析帮助教师实时了解学生困难。

Conclusion: 智能导师初步效果良好，未来将扩展至更多工程学科并优化功能。

Abstract: This research-to-practice work-in-progress (WIP) paper presents an AI-enabled
smart tutor designed to provide homework assessment and feedback for students
in an undergraduate circuit analysis course. We detail the tutor's design
philosophy and core components, including open-ended question answering and
homework feedback generation. The prompts are carefully crafted to optimize
responses across different problems. The smart tutor was deployed on the
Microsoft Azure platform and is currently in use in an undergraduate circuit
analysis course at the School of Electrical and Computer Engineering in a
large, public, research-intensive institution in the Southeastern United
States. Beyond offering personalized instruction and feedback, the tutor
collects student interaction data, which is summarized and shared with the
course instructor. To evaluate its effectiveness, we collected student
feedback, with 90.9% of responses indicating satisfaction with the tutor.
Additionally, we analyze a subset of collected data on preliminary circuit
analysis topics to assess tutor usage frequency for each problem and identify
frequently asked questions. These insights help instructors gain real-time
awareness of student difficulties, enabling more targeted classroom
instruction. In future work, we will release a full analysis once the complete
dataset is available after the Spring 2025 semester. We also explore the
potential applications of this smart tutor across a broader range of
engineering disciplines by developing improved prompts, diagram-recognition
methods, and database management strategies, which remain ongoing areas of
research.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [262] [MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning](https://arxiv.org/abs/2506.08507)
*Kuo Yang,Xingjie Yang,Linhui Yu,Qing Xu,Yan Fang,Xu Wang,Zhengyang Zhou,Yang Wang*

Key words: 多代理系统,强化学习,图搜索,自适应,自主性

TL;DR: 提出了名为MasHost的强化学习框架，用于自主设计多代理系统（Mas），通过统一的概率采样机制优化代理角色和交互，并通过HRPO策略实现多目标优化。实验显示MasHost在性能和结构合理性上优于基线方法。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 现有的多代理系统（Mas）构建方法依赖手工设计的交互机制或启发式规则，导致人为偏见和自主性受限。MasHost旨在通过RL框架实现完全自主和查询自适应的Mas设计。

Method: 将Mas构建建模为图搜索问题，通过统一的概率采样机制联合采样代理角色及其交互。提出HRPO策略，结合组相对优势和动作奖励，实现多目标优化。

Result: 在六个基准测试中，MasHost在效率、性能和结构合理性上均优于竞争基线，验证了其有效性。

Conclusion: MasHost是第一个基于RL的自主Mas图构建框架，解决了现有方法自主性不足的问题，并通过多目标优化提升了系统性能。

Abstract: Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently
emerged as a powerful paradigm for tackling complex real-world tasks. However,
existing Mas construction methods typically rely on manually crafted
interaction mechanisms or heuristic rules, introducing human biases and
constraining the autonomous ability. Even with recent advances in adaptive Mas
construction, existing systems largely remain within the paradigm of
semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement
Learning (RL)-based framework for autonomous and query-adaptive Mas design. By
formulating Mas construction as a graph search problem, our proposed MasHost
jointly samples agent roles and their interactions through a unified
probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives
pursued in prior works, we introduce component rationality as an additional and
novel design principle in Mas. To achieve this multi-objective optimization, we
propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy
that collaboratively integrates group-relative advantages and action-wise
rewards. To our knowledge, our proposed MasHost is the first RL-driven
framework for autonomous Mas graph construction. Extensive experiments on six
benchmarks demonstrate that MasHost consistently outperforms most competitive
baselines, validating its effectiveness, efficiency, and structure rationality.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [263] [Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion](https://arxiv.org/abs/2506.04760)
*Lingyuan Liu,Mengxiang Zhang*

Key words: 大语言模型,查询扩展,稀疏检索器,融合排名,零样本学习

TL;DR: 提出了一种名为Exp4Fuse的新型融合排名框架，通过零样本LLM查询扩展提升稀疏检索器的性能。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 现有基于LLM的查询扩展方法依赖高质量生成文档，成本高且计算复杂。

Method: Exp4Fuse通过同时考虑原始查询和LLM增强查询的两条检索路径，使用改进的互逆排名融合方法生成融合排名列表。

Result: 在多个数据集上的实验表明，Exp4Fuse超越了现有LLM查询扩展方法，与先进稀疏检索器结合时达到SOTA性能。

Conclusion: Exp4Fuse在提升稀疏检索器性能方面表现出色，为查询扩展提供了高效方案。

Abstract: Large Language Models (LLMs) have shown potential in generating hypothetical
documents for query expansion, thereby enhancing information retrieval
performance. However, the efficacy of this method is highly dependent on the
quality of the generated documents, which often requires complex prompt
strategies and the integration of advanced dense retrieval techniques. This can
be both costly and computationally intensive. To mitigate these limitations, we
explore the use of zero-shot LLM-based query expansion to improve sparse
retrieval, particularly for learned sparse retrievers. We introduce a novel
fusion ranking framework, Exp4Fuse, which enhances the performance of sparse
retrievers through an indirect application of zero-shot LLM-based query
expansion. Exp4Fuse operates by simultaneously considering two retrieval
routes-one based on the original query and the other on the LLM-augmented
query. It then generates two ranked lists using a sparse retriever and fuses
them using a modified reciprocal rank fusion method. We conduct extensive
evaluations of Exp4Fuse against leading LLM-based query expansion methods and
advanced retrieval techniques on three MS MARCO-related datasets and seven
low-resource datasets. Experimental results reveal that Exp4Fuse not only
surpasses existing LLM-based query expansion methods in enhancing sparse
retrievers but also, when combined with advanced sparse retrievers, achieves
SOTA results on several benchmarks. This highlights the superior performance
and effectiveness of Exp4Fuse in improving query expansion for sparse
retrieval.

</details>


### [264] [Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval](https://arxiv.org/abs/2506.08074)
*Abdellah Ghassel,Ian Robinson,Gabriel Tanase,Hal Cooper,Bryan Thompson,Zhen Han,Vassilis N. Ioannidis,Soji Adeshina,Huzefa Rangwala*

Key words: 检索增强生成, 多文档检索, 分层图索引, 实体链接

TL;DR: Hierarchical Lexical Graph (HLG) 改进了检索增强生成（RAG）在多文档、跨语义检索中的表现，通过细粒度的实体感知检索和主题聚类提升答案质量。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 传统RAG在面对跨语义远距离文档的答案拼接时表现不佳。

Method: 提出HLG三层索引结构和两种检索器（StatementGraphRAG和TopicGraphRAG），并引入多文档评测数据集生成方法。

Result: 实验表明，新方法比传统RAG在检索召回率和准确性上平均提升23.1%。

Conclusion: HLG和配套检索器有效解决了跨文档信息整合的挑战，公开工具库促进实际应用。

Abstract: Retrieval-Augmented Generation (RAG) grounds large language models in
external evidence, yet it still falters when answers must be pieced together
across semantically distant documents. We close this gap with the Hierarchical
Lexical Graph (HLG), a three-tier index that (i) traces every atomic
proposition to its source, (ii) clusters propositions into latent topics, and
(iii) links entities and relations to expose cross-document paths. On top of
HLG we build two complementary, plug-and-play retrievers: StatementGraphRAG,
which performs fine-grained entity-aware beam search over propositions for
high-precision factoid questions, and TopicGraphRAG, which selects coarse
topics before expanding along entity links to supply broad yet relevant context
for exploratory queries. Additionally, existing benchmarks lack the complexity
required to rigorously evaluate multi-hop summarization systems, often focusing
on single-document queries or limited datasets. To address this, we introduce a
synthetic dataset generation pipeline that curates realistic, multi-document
question-answer pairs, enabling robust evaluation of multi-hop retrieval
systems. Extensive experiments across five datasets demonstrate that our
methods outperform naive chunk-based RAG achieving an average relative
improvement of 23.1% in retrieval recall and correctness. Open-source Python
library is available at https://github.com/awslabs/graphrag-toolkit.

</details>


### [265] [Bridging RDF Knowledge Graphs with Graph Neural Networks for Semantically-Rich Recommender Systems](https://arxiv.org/abs/2506.08743)
*Michael Färber,David Lamprecht,Yuni Susanti*

Key words: 图神经网络、推荐系统、RDF知识图、语义信息、Linked Open Data

TL;DR: 该论文提出了一种将RDF知识图与图神经网络（GNN）全面结合的方法，以利用其丰富的语义信息提升推荐系统性能。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 尽管已有大量RDF知识图存在，但其语义信息在基于GNN的推荐系统中尚未充分利用，论文旨在填补这一空白。

Method: 通过整合RDF对象属性的拓扑信息和数据类型属性的内容信息，并对不同GNN和语义特征初始化方法进行深入评估。

Result: 实验表明，利用RDF知识图的语义丰富性显著提升了推荐系统性能。

Conclusion: 该方法为基于GNN的推荐系统在Linked Open Data云中的应用奠定了基础。

Abstract: Graph Neural Networks (GNNs) have substantially advanced the field of
recommender systems. However, despite the creation of more than a thousand
knowledge graphs (KGs) under the W3C standard RDF, their rich semantic
information has not yet been fully leveraged in GNN-based recommender systems.
To address this gap, we propose a comprehensive integration of RDF KGs with
GNNs that utilizes both the topological information from RDF object properties
and the content information from RDF datatype properties. Our main focus is an
in-depth evaluation of various GNNs, analyzing how different semantic feature
initializations and types of graph structure heterogeneity influence their
performance in recommendation tasks. Through experiments across multiple
recommendation scenarios involving multi-million-node RDF graphs, we
demonstrate that harnessing the semantic richness of RDF KGs significantly
improves recommender systems and lays the groundwork for GNN-based recommender
systems for the Linked Open Data cloud. The code and data are available on our
GitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation

</details>


### [266] [Multimodal Representation Alignment for Cross-modal Information Retrieval](https://arxiv.org/abs/2506.08774)
*Fan Xu,Luis A. Leiva*

Key words: 多模态检索, 特征对齐, Wasserstein距离, 余弦相似性, 跨模态应用

TL;DR: 该论文研究了视觉和文本嵌入的几何关系，并通过多种相似性度量方法对齐多模态表示，发现Wasserstein距离能有效衡量模态差距，而余弦相似性在特征对齐任务中表现最佳。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 解决多模态检索中的特征对齐问题，探究不同模态表示之间的几何关系和交互方式。

Method: 研究视觉和文本嵌入的几何关系，使用四种标准相似性度量和两种学习度量的方法对齐表示，并通过实验验证。

Result: Wasserstein距离能衡量模态差距，余弦相似性在特征对齐任务中表现最优，传统的多层感知机难以捕捉多模态复杂交互。

Conclusion: 研究为多模态信息检索提供了新见解和实践建议，特别是针对现实世界的跨模态应用。

Abstract: Different machine learning models can represent the same underlying concept
in different ways. This variability is particularly valuable for in-the-wild
multimodal retrieval, where the objective is to identify the corresponding
representation in one modality given another modality as input. This challenge
can be effectively framed as a feature alignment problem. For example, given a
sentence encoded by a language model, retrieve the most semantically aligned
image based on features produced by an image encoder, or vice versa. In this
work, we first investigate the geometric relationships between visual and
textual embeddings derived from both vision-language models and combined
unimodal models. We then align these representations using four standard
similarity metrics as well as two learned ones, implemented via neural
networks. Our findings indicate that the Wasserstein distance can serve as an
informative measure of the modality gap, while cosine similarity consistently
outperforms alternative metrics in feature alignment tasks. Furthermore, we
observe that conventional architectures such as multilayer perceptrons are
insufficient for capturing the complex interactions between image and text
representations. Our study offers novel insights and practical considerations
for researchers working in multimodal information retrieval, particularly in
real-world, cross-modal applications.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [267] [GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors](https://arxiv.org/abs/2506.08188)
*Wenlong Meng,Shuguo Fan,Chengkun Wei,Min Chen,Yuwei Li,Yuanchao Zhang,Zhikun Zhang,Wenzhi Chen*

Key words: GradEscape, AIGT检测器, 梯度攻击, 文本嵌入, 模型提取

TL;DR: GradEscape是一种基于梯度的攻击方法，针对AI生成文本检测器，通过加权嵌入和模型参数更新实现高效攻击，且在查询限制下仍有效。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 现有AI生成文本检测器易受攻击，需开发更高效的攻击方法以揭示其脆弱性并促进鲁棒检测器的研发。

Method: 提出加权嵌入解决文本离散性问题，采用暖启动方法适应不同检测器，结合标记器推断和模型提取技术。

Result: 在四个数据集和三种语言模型上表现优异，参数仅139M，成功应用于两个商业检测器。

Conclusion: GradEscape揭示了检测器因训练数据表达风格差异的脆弱性，提出了潜在防御策略，并开源工具。

Abstract: In this paper, we introduce GradEscape, the first gradient-based evader
designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the
undifferentiable computation problem, caused by the discrete nature of text, by
introducing a novel approach to construct weighted embeddings for the detector
input. It then updates the evader model parameters using feedback from victim
detectors, achieving high attack success with minimal text modification. To
address the issue of tokenizer mismatch between the evader and the detector, we
introduce a warm-started evader method, enabling GradEscape to adapt to
detectors across any language model architecture. Moreover, we employ novel
tokenizer inference and model extraction techniques, facilitating effective
evasion even in query-only access.
  We evaluate GradEscape on four datasets and three widely-used language
models, benchmarking it against four state-of-the-art AIGT evaders.
Experimental results demonstrate that GradEscape outperforms existing evaders
in various scenarios, including with an 11B paraphrase model, while utilizing
only 139M parameters. We have successfully applied GradEscape to two real-world
commercial AIGT detectors. Our analysis reveals that the primary vulnerability
stems from disparity in text expression styles within the training data. We
also propose a potential defense strategy to mitigate the threat of AIGT
evaders. We open-source our GradEscape for developing more robust AIGT
detectors.

</details>


### [268] [How Good LLM-Generated Password Policies Are?](https://arxiv.org/abs/2506.08320)
*Vivek Vaidya,Aditya Patwardhan,Ashish Kundu*

Key words: 

TL;DR: 论文研究了大型语言模型（LLMs）在网络安全访问控制系统中的应用，重点探讨了LLM生成的密码策略的一致性和准确性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 由于LLMs在自然语言处理中的强大能力，其在工业、学术界和政府部门的广泛应用，但生成结果的不一致性和不可预测性在安全关键领域中带来了挑战。

Method: 研究采用两种方法：一种是通过预训练的LLMs直接从自然语言提示生成配置；另一种是为模型提供官方文档作为基线。

Result: 实验结果显示当前LLMs在生成配置时存在显著的一致性和准确性挑战。

Conclusion: 论文为优化LLMs在访问控制系统中的部署提供了有价值的见解。

Abstract: Generative AI technologies, particularly Large Language Models (LLMs), are
rapidly being adopted across industry, academia, and government sectors, owing
to their remarkable capabilities in natural language processing. However,
despite their strengths, the inconsistency and unpredictability of LLM outputs
present substantial challenges, especially in security-critical domains such as
access control. One critical issue that emerges prominently is the consistency
of LLM-generated responses, which is paramount for ensuring secure and reliable
operations.
  In this paper, we study the application of LLMs within the context of
Cybersecurity Access Control Systems. Specifically, we investigate the
consistency and accuracy of LLM-generated password policies, translating
natural language prompts into executable pwquality.conf configuration files.
Our experimental methodology adopts two distinct approaches: firstly, we
utilize pre-trained LLMs to generate configuration files purely from natural
language prompts without additional guidance. Secondly, we provide these models
with official pwquality.conf documentation to serve as an informative baseline.
We systematically assess the soundness, accuracy, and consistency of these
AI-generated configurations. Our findings underscore significant challenges in
the current generation of LLMs and contribute valuable insights into refining
the deployment of LLMs in Access Control Systems.

</details>


### [269] [Your Agent Can Defend Itself against Backdoor Attacks](https://arxiv.org/abs/2506.08336)
*Li Changjiang,Liang Jiacheng,Cao Bochuan,Chen Jinghui,Wang Ting*

Key words: 大型语言模型、后门攻击、安全防御、ReAgent、一致性检测

TL;DR: 提出了ReAgent，一种针对LLM代理后门攻击的新型防御方法，通过两级检测机制显著降低攻击成功率。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 大型语言模型代理在训练和微调中面临后门攻击的安全风险，可能导致恶意操作，需要有效防御手段。

Method: ReAgent采用两级检测：执行层面验证思维与行为一致性；规划层面通过思维轨迹重构指令并与用户指令对比。

Result: 实验显示ReAgent显著优于现有防御方法，例如在数据库任务中将攻击成功率降低90%。

Conclusion: ReAgent展示了利用被攻击代理自身潜力缓解后门风险的可行性。

Abstract: Despite their growing adoption across domains, large language model
(LLM)-powered agents face significant security risks from backdoor attacks
during training and fine-tuning. These compromised agents can subsequently be
manipulated to execute malicious operations when presented with specific
triggers in their inputs or environments. To address this pressing risk, we
present ReAgent, a novel defense against a range of backdoor attacks on
LLM-based agents. Intuitively, backdoor attacks often result in inconsistencies
among the user's instruction, the agent's planning, and its execution. Drawing
on this insight, ReAgent employs a two-level approach to detect potential
backdoors. At the execution level, ReAgent verifies consistency between the
agent's thoughts and actions; at the planning level, ReAgent leverages the
agent's capability to reconstruct the instruction based on its thought
trajectory, checking for consistency between the reconstructed instruction and
the user's instruction. Extensive evaluation demonstrates ReAgent's
effectiveness against various backdoor attacks across tasks. For instance,
ReAgent reduces the attack success rate by up to 90\% in database operation
tasks, outperforming existing defenses by large margins. This work reveals the
potential of utilizing compromised agents themselves to mitigate backdoor
risks.

</details>


### [270] [WGLE:Backdoor-free and Multi-bit Black-box Watermarking for Graph Neural Networks](https://arxiv.org/abs/2506.08602)
*Tingzhi Li,Xuefeng Liu*

Key words: 图神经网络, 所有权验证, 黑盒水印, LDDE, 后门攻击

TL;DR: 提出了一种新型黑盒水印方法WGLE，用于图神经网络（GNNs）的所有权验证，避免了后门攻击风险，并能嵌入多比特信息。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 现有GNNs所有权验证方法中，指纹识别依赖模型相似性，计算昂贵且易受后处理干扰；黑盒水印则存在后门攻击风险。两者均未能传递额外信息，且需多次查询验证。

Method: 提出WGLE方法，基于“层间边距离差”（LDDE），通过预定义选定边的正负LDDE值嵌入水印信息，不引入错误映射。

Result: 在六个公共数据集和六种主流GNN架构上测试，所有权验证准确率达100%，保真度下降仅0.85%，对抗攻击鲁棒性强，嵌入开销低。

Conclusion: WGLE有效解决了现有方法的不足，实现高精度所有权验证，同时保持模型性能。

Abstract: Graph Neural Networks (GNNs) are increasingly deployed in graph-related
applications, making ownership verification critical to protect their
intellectual property against model theft. Fingerprinting and black-box
watermarking are two main methods. However, the former relies on determining
model similarity, which is computationally expensive and prone to ownership
collisions after model post-processing such as model pruning or fine-tuning.
The latter embeds backdoors, exposing watermarked models to the risk of
backdoor attacks. Moreover, both methods enable ownership verification but do
not convey additional information. As a result, each distributed model requires
a unique trigger graph, and all trigger graphs must be used to query the
suspect model during verification. Multiple queries increase the financial cost
and the risk of detection.
  To address these challenges, this paper proposes WGLE, a novel black-box
watermarking paradigm for GNNs that enables embedding the multi-bit string as
the ownership information without using backdoors. WGLE builds on a key insight
we term Layer-wise Distance Difference on an Edge (LDDE), which quantifies the
difference between the feature distance and the prediction distance of two
connected nodes. By predefining positive or negative LDDE values for multiple
selected edges, WGLE embeds the watermark encoding the intended information
without introducing incorrect mappings that compromise the primary task. WGLE
is evaluated on six public datasets and six mainstream GNN architectures along
with state-of-the-art methods. The results show that WGLE achieves 100%
ownership verification accuracy, an average fidelity degradation of 0.85%,
comparable robustness against potential attacks, and low embedding overhead.
The code is available in the repository.

</details>


### [271] [Interpreting Agent Behaviors in Reinforcement-Learning-Based Cyber-Battle Simulation Platforms](https://arxiv.org/abs/2506.08192)
*Jared Claypoole,Steven Cheung,Ashish Gehani,Vinod Yegneswaran,Ahmad Ridley*

Key words: deep reinforcement learning, cyber defense, interpretability, CAGE Challenge, agent behavior

TL;DR: 论文分析了两种开源深度强化学习代理在CAGE Challenge 2网络防御挑战赛中的表现，通过简化状态和动作空间及跟踪重要事件，揭示了防御和攻击代理的行为模式，并探讨了挑战赛的现实性及改进方向。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 研究目的是通过分析代理在复杂网络防御中的行为，提供对代理成功和失败的可解释性。

Method: 通过简化复杂状态和动作空间，跟踪重要事件，分析环境状态变化及动作效果。

Result: 发现防御代理通常能在1-2个时间步内清除入侵，某些重要动作的无效率在40%-99%之间，诱饵服务可阻挡94%的特权访问尝试。

Conclusion: 研究揭示了代理行为模式及动作有效性，并讨论了挑战赛的现实性改进。

Abstract: We analyze two open source deep reinforcement learning agents submitted to
the CAGE Challenge 2 cyber defense challenge, where each competitor submitted
an agent to defend a simulated network against each of several provided
rules-based attack agents. We demonstrate that one can gain interpretability of
agent successes and failures by simplifying the complex state and action spaces
and by tracking important events, shedding light on the fine-grained behavior
of both the defense and attack agents in each experimental scenario. By
analyzing important events within an evaluation episode, we identify patterns
in infiltration and clearing events that tell us how well the attacker and
defender played their respective roles; for example, defenders were generally
able to clear infiltrations within one or two timesteps of a host being
exploited. By examining transitions in the environment's state caused by the
various possible actions, we determine which actions tended to be effective and
which did not, showing that certain important actions are between 40% and 99%
ineffective. We examine how decoy services affect exploit success, concluding
for instance that decoys block up to 94% of exploits that would directly grant
privileged access to a host. Finally, we discuss the realism of the challenge
and ways that the CAGE Challenge 4 has addressed some of our concerns.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [272] [Aligning Proteins and Language: A Foundation Model for Protein Retrieval](https://arxiv.org/abs/2506.08023)
*Qifeng Wu,Zhengzhe Liu,Han Zhu,Yizhou Zhao,Daisuke Kihara,Min Xu*

Key words: 蛋白质检索, 多模态学习, 对比学习, 结构-功能对齐

TL;DR: 提出了一种基于CLIP风格的多模态框架，用于从大规模蛋白质数据集中检索结构与语义相似的蛋白质，以辅助功能解释。

<details>
  <summary>Details</summary>

Main category: q-bio.BM

Motivation: 受视觉语言模型（VLMs）进展启发，旨在通过对比学习对齐3D蛋白质结构与功能注释，解决蛋白质结构功能解释问题。

Method: 采用CLIP风格的对比学习框架，训练一个包含约20万蛋白质-描述对的大规模数据集，并在PDB和EMDB数据集上评估模型。

Result: 模型在域内和跨数据库检索中均表现出优异的零样本检索性能。

Conclusion: 多模态基础模型在蛋白质结构功能理解中具有潜力。

Abstract: This paper aims to retrieve proteins with similar structures and semantics
from large-scale protein dataset, facilitating the functional interpretation of
protein structures derived by structural determination methods like
cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of
vision-language models (VLMs), we propose a CLIP-style framework for aligning
3D protein structures with functional annotations using contrastive learning.
For model training, we propose a large-scale dataset of approximately 200,000
protein-caption pairs with rich functional descriptors. We evaluate our model
in both in-domain and more challenging cross-database retrieval on Protein Data
Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In
both cases, our approach demonstrates promising zero-shot retrieval
performance, highlighting the potential of multimodal foundation models for
structure-function understanding in protein biology.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [273] [ChemGraph: An Agentic Framework for Computational Chemistry Workflows](https://arxiv.org/abs/2506.06363)
*Thang D. Pham,Aditya Tanikanti,Murat Keçeli*

Key words: 计算化学, 图神经网络, 大型语言模型, 多代理框架, 自动工作流

TL;DR: ChemGraph是一个基于AI的框架，用于简化和自动化计算化学与材料科学的工作流程，结合图神经网络和大型语言模型（LLMs）提供高效计算和自然语言交互。

<details>
  <summary>Details</summary>

Main category: physics.chem-ph

Motivation: 传统原子模拟方法因计算多样性、软件复杂性及对专家知识的依赖而难以普及，需要一种自动化工具来降低门槛。

Method: 利用图神经网络进行高效计算，并通过大型语言模型实现任务规划与自然语言交互，支持多种计算方法和任务分解。

Result: 在小规模任务中，较小的LLM表现良好；复杂任务通过多代理分解后，小模型也能匹配或超越GPT-4o性能。

Conclusion: ChemGraph通过自动化框架和多代理策略，显著提升了计算化学的可用性和效率，降低了专家依赖。

Abstract: Atomistic simulations are essential tools in chemistry and materials science,
accelerating the discovery of novel catalysts, energy storage materials, and
pharmaceuticals. However, running these simulations remains challenging due to
the wide range of computational methods, diverse software ecosystems, and the
need for expert knowledge and manual effort for the setup, execution, and
validation stages. In this work, we present ChemGraph, an agentic framework
powered by artificial intelligence and state-of-the-art simulation tools to
streamline and automate computational chemistry and materials science
workflows. ChemGraph leverages graph neural network-based foundation models for
accurate yet computationally efficient calculations and large language models
(LLMs) for natural language understanding, task planning, and scientific
reasoning to provide an intuitive and interactive interface. Users can perform
tasks such as molecular structure generation, single-point energy, geometry
optimization, vibrational analysis, and thermochemistry calculations with
methods ranging from tight-binding and machine learning interatomic potentials
to density functional theory or wave function theory-based methods. We evaluate
ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs
(GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows,
while more complex tasks benefit from using larger models like GPT-4o.
Importantly, we show that decomposing complex tasks into smaller subtasks
through a multi-agent framework enables smaller LLM models to match or exceed
GPT-4o's performance in specific scenarios.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [274] [CaliciBoost: Performance-Driven Evaluation of Molecular Representations for Caco-2 Permeability Prediction](https://arxiv.org/abs/2506.08059)
*Huong Van Le,Weibin Ren,Junhong Kim,Yukyung Yun,Young Bin Park,Young Jun Kim,Bok Kyung Han,Inho Choi,Jong IL Park,Hwi-Yeol Yun,Jae-Mun Choi*

Key words: Caco-2渗透性、自动机器学习、分子特征、ADMET建模

TL;DR: 该研究通过结合多种分子特征表示和自动机器学习技术，提升Caco-2渗透性预测的准确性和效率，发现AutoML模型CaliciBoost表现最佳，且3D描述符显著降低预测误差。

<details>
  <summary>Details</summary>

Main category: q-bio.QM

Motivation: Caco-2渗透性是早期药物发现中口服吸收的关键指标，需要提升预测的准确性和效率。

Method: 系统研究了八种分子特征表示类型（如2D/3D描述符、指纹和深度学习嵌入）与自动机器学习技术的结合，并通过两个数据集评估模型表现。

Result: PaDEL、Mordred和RDKit描述符表现优异，AutoML模型CaliciBoost的MAE最低，3D描述符使MAE降低15.73%。

Conclusion: AutoML方法在ADMET建模中效果显著，为数据有限的任务提供了实用的特征选择指导。

Abstract: Caco-2 permeability serves as a critical in vitro indicator for predicting
the oral absorption of drug candidates during early-stage drug discovery. To
enhance the accuracy and efficiency of computational predictions, we
systematically investigated the impact of eight molecular feature
representation types including 2D/3D descriptors, structural fingerprints, and
deep learning-based embeddings combined with automated machine learning
techniques to predict Caco-2 permeability. Using two datasets of differing
scale and diversity (TDC benchmark and curated OCHEM data), we assessed model
performance across representations and identified PaDEL, Mordred, and RDKit
descriptors as particularly effective for Caco-2 prediction. Notably, the
AutoML-based model CaliciBoost achieved the best MAE performance. Furthermore,
for both PaDEL and Mordred representations, the incorporation of 3D descriptors
resulted in a 15.73% reduction in MAE compared to using 2D features alone, as
confirmed by feature importance analysis. These findings highlight the
effectiveness of AutoML approaches in ADMET modeling and offer practical
guidance for feature selection in data-limited prediction tasks.

</details>


### [275] [Protriever: End-to-End Differentiable Protein Homology Search for Fitness Prediction](https://arxiv.org/abs/2506.08954)
*Ruben Weitzman,Peter Mørch Groth,Lood Van Niekerk,Aoi Otani,Yarin Gal,Debora Marks,Pascal Notin*

Key words: 蛋白质建模,同源序列检索,可微分框架,Protriever,MSA

TL;DR: Protriever是一个端到端的可微分框架，通过学习检索相关同源序列来优化蛋白质建模任务，相比传统MSA方法更高效且性能更优。

<details>
  <summary>Details</summary>

Main category: q-bio.QM

Motivation: 传统基于MSA的蛋白质同源序列检索方法计算成本高，难以处理高度分化的序列或复杂的插入/删除模式，且与下游任务独立。

Method: Protriever通过端到端的可微分框架，同时学习检索相关同源序列并训练目标任务（如蛋白质适应性预测）。

Result: 在蛋白质适应性预测任务中，Protriever性能优于基于MSA的序列模型，且检索速度快两个数量级。

Conclusion: Protriever提供了一种可扩展的替代方案，适用于不同架构和任务，灵活性高。

Abstract: Retrieving homologous protein sequences is essential for a broad range of
protein modeling tasks such as fitness prediction, protein design, structure
modeling, and protein-protein interactions. Traditional workflows have relied
on a two-step process: first retrieving homologs via Multiple Sequence
Alignments (MSA), then training models on one or more of these alignments.
However, MSA-based retrieval is computationally expensive, struggles with
highly divergent sequences or complex insertions & deletions patterns, and
operates independently of the downstream modeling objective. We introduce
Protriever, an end-to-end differentiable framework that learns to retrieve
relevant homologs while simultaneously training for the target task. When
applied to protein fitness prediction, Protriever achieves state-of-the-art
performance compared to sequence-based models that rely on MSA-based homolog
retrieval, while being two orders of magnitude faster through efficient vector
search. Protriever is both architecture- and task-agnostic, and can flexibly
adapt to different retrieval strategies and protein databases at inference time
-- offering a scalable alternative to alignment-centric approaches.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [276] [Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs](https://arxiv.org/abs/2506.08633)
*Šimon Sedláček,Bolaji Yusuf,Ján Švec,Pradyoth Hegde,Santosh Kesiraju,Oldřich Plchot,Jan Černocký*

Key words: 对话状态追踪,语音编码器,大语言模型,开源组件,SpokenWOZ

TL;DR: 该研究通过小连接模块桥接语音编码器和大语言模型（LLMs）的表征空间，实现了基于全开源组件的口语对话状态追踪（DST），并在SpokenWOZ数据集上取得了最佳性能。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 旨在探索语音编码器与LLMs的表征空间融合，以提升口语对话状态追踪的性能，同时关注完全开源组件的可行性。

Method: 采用WavLM-large和OLMo等开源模型，结合小连接模块，研究了不同微调方法（如全/LoRA适配器微调）、对话历史中代理轮次的影响，以及基于模糊匹配的输出后处理技术。

Result: 最佳模型（WavLM + connector + OLMo-1B）在SpokenWOZ测试集上达到34.66% JGA，而采用Gemma-2-9B-instruct的系统进一步提高到42.17% JGA。

Conclusion: 通过桥接语音编码器和LLMs的表征空间，以及开源组件的使用，实现了口语对话状态追踪的显著性能提升。

Abstract: In this work, we approach spoken Dialogue State Tracking (DST) by bridging
the representation spaces of speech encoders and LLMs via a small connector
module, with a focus on fully open-sourced and open-data components
(WavLM-large, OLMo). We focus on ablating different aspects of such systems
including full/LoRA adapter fine-tuning, the effect of agent turns in the
dialogue history, as well as fuzzy matching-based output post-processing, which
greatly improves performance of our systems on named entities in the dialogue
slot values. We conduct our experiments on the SpokenWOZ dataset, and
additionally utilize the Speech-Aware MultiWOZ dataset to augment our training
data. Ultimately, our best-performing WavLM + connector + OLMo-1B aligned
models achieve state of the art on the SpokenWOZ test set (34.66% JGA), and our
system with Gemma-2-9B-instruct further surpasses this result, reaching 42.17%
JGA on SpokenWOZ test.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [277] [Learning-Based Multiuser Scheduling in MIMO-OFDM Systems with Hybrid Beamforming](https://arxiv.org/abs/2506.08263)
*Pouya Agheli,Tugce Kobal,François Durand,Matthew Andrews*

Key words: MIMO, OFDM, 混合波束成形, 毫米波, 比例公平, 用户调度

TL;DR: 该论文研究了MIMO系统中多用户调度问题，采用混合波束成形技术以提高频谱效率和比例公平性，提出了多种调度算法并进行性能与复杂度权衡分析。

<details>
  <summary>Details</summary>

Main category: cs.IT

Motivation: 混合波束成形系统由于多路增益有限，需优化调度以提高频谱效率和长期系统性能，尤其是比例公平性。

Method: 通过设计模拟和数字预编码器并结合用户选择，利用毫米波信道特性应用两时间尺度协议，提出组合算法（如贪婪和排序算法）及机器学习方法进行调度。

Result: 数值结果显示了性能与复杂度之间的权衡，不同方法的选择取决于具体场景标准。

Conclusion: 研究表明，在给定场景下，应根据具体标准选择调度方法以实现最佳性能与复杂度的平衡。

Abstract: We investigate the multiuser scheduling problem in multiple-input
multiple-output (MIMO) systems using orthogonal frequency division multiplexing
(OFDM) and hybrid beamforming in which a base station (BS) communicates with
multiple users over millimeter wave (mmWave) channels in the downlink. Improved
scheduling is critical for enhancing spectral efficiency and the long-term
performance of the system from the perspective of proportional fairness (PF)
metric in hybrid beamforming systems due to its limited multiplexing gain. Our
objective is to maximize PF by properly designing the analog and digital
precoders within the hybrid beamforming and selecting the users subject to the
number of radio frequency (RF) chains. Leveraging the characteristics of mmWave
channels, we apply a two-timescale protocol. On a long timescale, we assign an
analog beam to each user. Scheduling the users and designing the digital
precoder are done accordingly on a short timescale. To conduct scheduling, we
propose combinatorial solutions, such as greedy and sorting algorithms,
followed by a machine learning (ML) approach. Our numerical results highlight
the trade-off between the performance and complexity of the proposed
approaches. Consequently, we show that the choice of approach depends on the
specific criteria within a given scenario.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [278] [Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain](https://arxiv.org/abs/2506.08277)
*Subba Reddy Oota,Khushbu Pahwa,Prachi Jindal,Satya Sai Srinath Namburi,Maneesh Singh,Tanmoy Chakraborty,Bapi S. Raju,Manish Gupta*

Key words: 多模态大语言模型，指令调优，大脑对齐，自然电影，层级对齐

TL;DR: 研究显示，经过指令调优的多模态大语言模型（MLLMs）在视频和音频任务中表现优于非指令调优模型，并能更准确地预测大脑活动。

<details>
  <summary>Details</summary>

Main category: q-bio.NC

Motivation: 探讨指令调优多模态模型在预测自然电影观看时大脑活动的表现，以及其与大脑功能的层级对齐关系。

Method: 使用13种视频任务指令和6种视频及2种音频指令调优的MLLMs，对比其与非指令调优模型的表现。

Result: 指令调优视频MLLMs显著优于非指令调优多模态和单模态模型，且模型层级与大脑功能区域呈对应关系。

Conclusion: 任务指令能提升MLLMs与大脑活动的对齐，为研究多模态信息处理提供新方向。

Abstract: Recent voxel-wise multimodal brain encoding studies have shown that
multimodal large language models (MLLMs) exhibit a higher degree of brain
alignment compared to unimodal models in both unimodal and multimodal stimulus
settings. More recently, instruction-tuned multimodal models have shown to
generate task-specific representations that align strongly with brain activity.
However, prior work evaluating the brain alignment of MLLMs has primarily
focused on unimodal settings or relied on non-instruction-tuned multimodal
models for multimodal stimuli. To address this gap, we investigated brain
alignment, that is, measuring the degree of predictivity of neural activity
recorded while participants were watching naturalistic movies (video along with
audio) with representations derived from MLLMs. We utilized
instruction-specific embeddings from six video and two audio instruction-tuned
MLLMs. Experiments with 13 video task-specific instructions show that
instruction-tuned video MLLMs significantly outperform non-instruction-tuned
multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for
both video and audio tasks using language-guided instructions shows clear
disentanglement in task-specific representations from MLLMs, leading to precise
differentiation of multimodal functional processing in the brain. We also find
that MLLM layers align hierarchically with the brain, with early sensory areas
showing strong alignment with early layers, while higher-level visual and
language regions align more with middle to late layers. These findings provide
clear evidence for the role of task-specific instructions in improving the
alignment between brain activity and MLLMs, and open new avenues for mapping
joint information processing in both the systems. We make the code publicly
available [https://github.com/subbareddy248/mllm_videos].

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [279] [Structured Variational $D$-Decomposition for Accurate and Stable Low-Rank Approximation](https://arxiv.org/abs/2506.08535)
*Ronald Katende*

Key words: 矩阵分解, 非正交分解, Frobenius损失, 稀疏性, 交替最小化

TL;DR: 该论文提出了一种名为$D$-分解的非正交矩阵分解方法，通过最小化正则化Frobenius损失实现，能够控制秩、稀疏性和条件数，相比传统分解方法具有更高的重构精度。

<details>
  <summary>Details</summary>

Main category: math.NA

Motivation: 解决传统矩阵分解方法在稀疏性和噪声条件下的性能不足问题。

Method: 采用变分方法最小化正则化Frobenius损失，通过交替最小化计算分解。

Result: 实验表明$D$-分解在MovieLens、MNIST等数据集上的重构精度优于SVD、CUR等方法。

Conclusion: $D$-分解是一种高效且稳定的矩阵分解方法，适用于稀疏和噪声环境。

Abstract: We introduce the $D$-decomposition, a non-orthogonal matrix factorization of
the form $A \approx P D Q$, where $P \in \mathbb{R}^{n \times k}$, $D \in
\mathbb{R}^{k \times k}$, and $Q \in \mathbb{R}^{k \times n}$. The
decomposition is defined variationally by minimizing a regularized Frobenius
loss, allowing control over rank, sparsity, and conditioning. Unlike algebraic
factorizations such as LU or SVD, it is computed by alternating minimization.
We establish existence and perturbation stability of the solution and show that
each update has complexity $\mathcal{O}(n^2k)$. Benchmarks against truncated
SVD, CUR, and nonnegative matrix factorization show improved reconstruction
accuracy on MovieLens, MNIST, Olivetti Faces, and gene expression matrices,
particularly under sparsity and noise.

</details>


### [280] [sparseGeoHOPCA: A Geometric Solution to Sparse Higher-Order PCA Without Covariance Estimation](https://arxiv.org/abs/2506.08670)
*Renjie Xu,Chong Wu,Maolin Che,Zhuoheng Ran,Yimin Wei,Hong Yan*

Key words: 稀疏高阶主成分分析, 张量分解, 几何方法, 计算效率

TL;DR: 提出sparseGeoHOPCA框架，通过几何方法实现高维稀疏张量分解，避免显式协方差估计，提升计算效率和可解释性。

<details>
  <summary>Details</summary>

Main category: math.NA

Motivation: 解决高维稀疏张量分解中的非凸问题，提升计算效率和可解释性。

Method: 将张量按模态展开，转化为结构化二元线性优化问题，理论证明其与原问题的等价性。

Result: 计算复杂度线性增长，实验证明在稀疏支持恢复、分类和图像重建中表现优异。

Conclusion: 该框架高效、稳健且多用途。

Abstract: We propose sparseGeoHOPCA, a novel framework for sparse higher-order
principal component analysis (SHOPCA) that introduces a geometric perspective
to high-dimensional tensor decomposition. By unfolding the input tensor along
each mode and reformulating the resulting subproblems as structured binary
linear optimization problems, our method transforms the original nonconvex
sparse objective into a tractable geometric form. This eliminates the need for
explicit covariance estimation and iterative deflation, enabling significant
gains in both computational efficiency and interpretability, particularly in
high-dimensional and unbalanced data scenarios. We theoretically establish the
equivalence between the geometric subproblems and the original SHOPCA
formulation, and derive worst-case approximation error bounds based on
classical PCA residuals, providing data-dependent performance guarantees. The
proposed algorithm achieves a total computational complexity of
$O\left(\sum_{n=1}^{N} (k_n^3 + J_n k_n^2)\right)$, which scales linearly with
tensor size. Extensive experiments demonstrate that sparseGeoHOPCA accurately
recovers sparse supports in synthetic settings, preserves classification
performance under 10$\times$ compression, and achieves high-quality image
reconstruction on ImageNet, highlighting its robustness and versatility.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [281] [MOSS: Multi-Objective Optimization for Stable Rule Sets](https://arxiv.org/abs/2506.08030)
*Brian Liu,Rahul Mazumder*

Key words: 多目标优化,决策规则,稀疏性,稳定性,切割平面算法

TL;DR: MOSS是一个多目标优化框架，用于构建稳定的决策规则集，结合了稀疏性、准确性和稳定性，并允许用户权衡准确性与稳定性。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 通过结合稀疏性、准确性和稳定性，MOSS旨在提供一种既能高效预测又易于解释的决策规则框架。

Method: 开发了专门的切割平面算法，快速计算多目标优化的Pareto前沿，适用于大规模问题。

Result: 实验证明，MOSS在预测性能和稳定性上优于现有的规则集成方法。

Conclusion: MOSS通过多目标优化有效平衡了规则集的稀疏性、准确性和稳定性，具有实用价值。

Abstract: We present MOSS, a multi-objective optimization framework for constructing
stable sets of decision rules. MOSS incorporates three important criteria for
interpretability: sparsity, accuracy, and stability, into a single
multi-objective optimization framework. Importantly, MOSS allows a practitioner
to rapidly evaluate the trade-off between accuracy and stability in sparse rule
sets in order to select an appropriate model. We develop a specialized cutting
plane algorithm in our framework to rapidly compute the Pareto frontier between
these two objectives, and our algorithm scales to problem instances beyond the
capabilities of commercial optimization solvers. Our experiments show that MOSS
outperforms state-of-the-art rule ensembles in terms of both predictive
performance and stability.

</details>


### [282] [Continuous Policy and Value Iteration for Stochastic Control Problems and Its Convergence](https://arxiv.org/abs/2506.08121)
*Qi Feng,Gu Wang*

Key words: 随机控制, Langevin动力学, 策略迭代, 值函数, 机器学习

TL;DR: 论文提出了一种连续策略-价值迭代算法，通过Langevin型动力学同时更新随机控制问题的值函数和最优控制的逼近，适用于熵正则化松弛控制问题和经典控制问题，并在哈密顿单调性条件下证明了收敛性。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 研究旨在结合Langevin型随机微分方程和策略迭代方法，利用机器学习中的分布采样和非凸学习技术，同时优化值函数和最优控制。

Method: 采用Langevin型随机微分方程进行连续更新，结合策略迭代方向，通过分布采样和非凸学习技术实现同步优化。

Result: 证明了在哈密顿单调性条件下的策略改进和最优控制的收敛性。

Conclusion: 该方法为随机控制问题提供了高效的新框架，适用于多种控制场景。

Abstract: We introduce a continuous policy-value iteration algorithm where the
approximations of the value function of a stochastic control problem and the
optimal control are simultaneously updated through Langevin-type dynamics. This
framework applies to both the entropy-regularized relaxed control problems and
the classical control problems, with infinite horizon. We establish policy
improvement and demonstrate convergence to the optimal control under the
monotonicity condition of the Hamiltonian. By utilizing Langevin-type
stochastic differential equations for continuous updates along the policy
iteration direction, our approach enables the use of distribution sampling and
non-convex learning techniques in machine learning to optimize the value
function and identify the optimal control simultaneously.

</details>


### [283] [Solving Convex-Concave Problems with $\tilde{\mathcal{O}}(ε^{-4/7})$ Second-Order Oracle Complexity](https://arxiv.org/abs/2506.08362)
*Lesi Chen,Chengchang Liu,Luo Luo,Jingzhao Zhang*

Key words: 凸凹极小极大问题, 二阶方法, 惰性Hessian, Catalyst框架, 复杂度优化

TL;DR: 该论文提出了改进的凸凹极小极大问题二阶方法，将传统方法的复杂度从$	ilde{\mathcal{O}}(\epsilon^{-2/3})$提升到了$	ilde{\mathcal{O}}(\epsilon^{-4/7})$。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 以往的二阶牛顿方法解决凸凹极小极大问题时复杂度为$	ilde{\mathcal{O}}(\epsilon^{-2/3})$，被推测为最优。论文旨在打破这一限制并证明可以通过改进方法进一步降低复杂度。

Method: 通过将凸优化的最优二阶方法推广到凸凹极小极大问题，同时应用惰性Hessian算法和类似Catalyst框架的技术，实现更高效的计算。

Result: 提出了新的算法，将复杂度改进为$	ilde{\mathcal{O}}(\epsilon^{-4/7})$，并展示了算法可加速全局收敛算法的性能。

Conclusion: 论文验证了改进算法的优越性，并展示了其在加速其他算法解决极小极大问题中的潜力。

Abstract: Previous algorithms can solve convex-concave minimax problems $\min_{x \in
\mathcal{X}} \max_{y \in \mathcal{Y}} f(x,y)$ with
$\mathcal{O}(\epsilon^{-2/3})$ second-order oracle calls using Newton-type
methods. This result has been speculated to be optimal because the upper bound
is achieved by a natural generalization of the optimal first-order method. In
this work, we show an improved upper bound of
$\tilde{\mathcal{O}}(\epsilon^{-4/7})$ by generalizing the optimal second-order
method for convex optimization to solve the convex-concave minimax problem. We
further apply a similar technique to lazy Hessian algorithms and show that our
proposed algorithm can also be seen as a second-order ``Catalyst'' framework
(Lin et al., JMLR 2018) that could accelerate any globally convergent
algorithms for solving minimax problems.

</details>


### [284] [Sharper Convergence Rates for Nonconvex Optimisation via Reduction Mappings](https://arxiv.org/abs/2506.08428)
*Evan Markou,Thalaiyasingam Ajanthan,Stephen Gould*

Key words: 高维优化, 降维映射, 曲率优化, 梯度方法, 收敛加速

TL;DR: 该论文提出了一个通用框架，用于分析通过降维映射优化高维问题中解决方案流形的影响，证明了合理设计的降维能改善目标函数的曲率，从而加速收敛。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 高维优化问题中，解集常因过参数化或对称性形成光滑流形，局部结构信息可通过降维映射提升优化效率。

Method: 引入通用框架，分析降维映射如何优化目标函数的曲率，并验证其对梯度方法的加速收敛效果。

Result: 合理设计的降维映射改善了问题的条件性质，理论上能加速梯度方法的收敛。

Conclusion: 该框架统一了利用结构信息加速优化的问题场景，为实际算法中的性能提升提供了理论解释。

Abstract: Many high-dimensional optimisation problems exhibit rich geometric structures
in their set of minimisers, often forming smooth manifolds due to
over-parametrisation or symmetries. When this structure is known, at least
locally, it can be exploited through reduction mappings that reparametrise part
of the parameter space to lie on the solution manifold. These reductions
naturally arise from inner optimisation problems and effectively remove
redundant directions, yielding a lower-dimensional objective. In this work, we
introduce a general framework to understand how such reductions influence the
optimisation landscape. We show that well-designed reduction mappings improve
curvature properties of the objective, leading to better-conditioned problems
and theoretically faster convergence for gradient-based methods. Our analysis
unifies a range of scenarios where structural information at optimality is
leveraged to accelerate convergence, offering a principled explanation for the
empirical gains observed in such optimisation algorithms.

</details>


### [285] [Optimization over Sparse Support-Preserving Sets: Two-Step Projection with Global Optimality Guarantees](https://arxiv.org/abs/2506.08558)
*William de Vazelhes,Xiao-Tong Yuan,Bin Gu*

Key words: 稀疏优化，迭代硬阈值，支持保留约束，全局收敛

TL;DR: 论文提出了一种新的迭代硬阈值算法，用于解决稀疏优化中带有额外支持保留约束的问题，提供了全局收敛保证。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 稀疏优化中，ℓ₀伪范数可以控制稀疏性，但实际应用中常需额外约束。现有方法通常只提供局部收敛保证或需要混合约束的闭式投影，这限制了其应用。

Method: 提出了一种基于两步连续投影算子的迭代硬阈值算法，适用于混合约束，并引入稀疏性松弛与次优性的权衡。

Result: 在确定性、随机和零阶设置下，算法在受限强凸/平滑假设下提供了全局目标值保证。

Conclusion: 新算法填补了全局收敛保证的空白，且在零阶情况下优于现有方法。

Abstract: In sparse optimization, enforcing hard constraints using the $\ell_0$
pseudo-norm offers advantages like controlled sparsity compared to convex
relaxations. However, many real-world applications demand not only sparsity
constraints but also some extra constraints. While prior algorithms have been
developed to address this complex scenario with mixed combinatorial and convex
constraints, they typically require the closed form projection onto the mixed
constraints which might not exist, and/or only provide local guarantees of
convergence which is different from the global guarantees commonly sought in
sparse optimization. To fill this gap, in this paper, we study the problem of
sparse optimization with extra \qw{\textit{support-preserving}} constraints
commonly encountered in the literature. We present a new variant of iterative
hard-thresholding algorithm equipped with a two-step consecutive projection
operator customized for these mixed constraints, serving as a simple
alternative to the Euclidean projection onto the mixed constraint. By
introducing a novel trade-off between sparsity relaxation and sub-optimality,
we provide global guarantees in objective value for the output of our
algorithm, in the deterministic, stochastic, and zeroth-order settings, under
the conventional restricted strong-convexity/smoothness assumptions. As a
fundamental contribution in proof techniques, we develop a novel extension of
the classic three-point lemma to the considered two-step non-convex projection
operator, which allows us to analyze the convergence in objective value in an
elegant way that has not been possible with existing techniques. In the
zeroth-order case, such technique also improves upon the state-of-the-art
result from de Vazelhes et. al. (2022), even in the case without additional
constraints, by allowing us to remove a non-vanishing system error present in
their work.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [286] [Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers](https://arxiv.org/abs/2506.08043)
*Ashkan Shahbazi,Kyvia Pereira,Jon S. Heiselman,Elaheh Akbari,Annie C. Benson,Sepehr Seifi,Xinyuan Liu,Garrison L. Johnston,Erwin Terpstra,Anne Draaisma,Jan-Jaap Severes,Jie Ying Wu,Nabil Simaan,Michael L. Miga,Soheil Kolouri*

Key words: 软组织变形, 物理信息神经模拟器, Kelvinlet, 有限元方法, 手术模拟

TL;DR: 提出了一种基于物理信息的神经模拟器，利用Kelvinlet先验改进数据驱动的软组织变形模拟，实现实时高精度仿真。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 手术机器人和医学训练需要快速准确的软组织变形模拟。

Method: 将Kelvinlet先验融入神经模拟器，结合大规模有限元（FEM）仿真，进行残差学习和正则化。

Result: 方法提升了神经网络的预测精度和物理一致性，同时保持低延迟，适用于实时手术模拟。

Conclusion: Kelvinlet增强学习是一种高效策略，适用于手术中的实时物理感知软组织模拟。

Abstract: Fast and accurate simulation of soft tissue deformation is a critical factor
for surgical robotics and medical training. In this paper, we introduce a novel
physics-informed neural simulator that approximates soft tissue deformations in
a realistic and real-time manner. Our framework integrates Kelvinlet-based
priors into neural simulators, making it the first approach to leverage
Kelvinlets for residual learning and regularization in data-driven soft tissue
modeling. By incorporating large-scale Finite Element Method (FEM) simulations
of both linear and nonlinear soft tissue responses, our method improves neural
network predictions across diverse architectures, enhancing accuracy and
physical consistency while maintaining low latency for real-time performance.
We demonstrate the effectiveness of our approach by performing accurate
surgical maneuvers that simulate the use of standard laparoscopic tissue
grasping tools with high fidelity. These results establish Kelvinlet-augmented
learning as a powerful and efficient strategy for real-time, physics-aware soft
tissue simulation in surgical applications.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [287] [Dynamic Diffusion Schrödinger Bridge in Astrophysical Observational Inversions](https://arxiv.org/abs/2506.08065)
*Ye Zhu,Duo Xu,Zhiwei Deng,Jonathon C. Tan,Olga Russakovsky*

Key words: Diffusion Schrödinger Bridge, Giant Molecular Clouds, star formation, generative modeling, astrophysical dynamics

TL;DR: 本文提出了一种名为Astro-DSB的Diffusion Schrödinger Bridge模型变体，用于解决巨型分子云（GMCs）中恒星形成的观测逆预测问题，并在模拟和实际数据中验证了其效果。

<details>
  <summary>Details</summary>

Main category: astro-ph.IM

Motivation: 研究动机是解决天体物理系统中观测逆预测任务的挑战，特别是在恒星形成的研究中，传统方法在解释性和预测性能上存在不足。

Method: 提出了Astro-DSB模型，基于成对域假设的DSB变体，用于天体物理动力学模拟和实际观测数据分析。

Result: 在模拟数据和真实观测（Taurus B213数据）中验证了模型效果。结果表明，与传统的天文统计方法和其他机器学习方法相比，Astro-DSB在解释性、学习效率和预测性能上均有提升，尤其是在物理模拟的OOD测试中表现出色。

Conclusion: 本研究扩展了扩散模型在天体物理领域的应用，证明了其能够学习超越纯数据统计的物理动态，为未来开发更具物理意识的生成模型奠定了基础。

Abstract: We study Diffusion Schr\"odinger Bridge (DSB) models in the context of
dynamical astrophysical systems, specifically tackling observational inverse
prediction tasks within Giant Molecular Clouds (GMCs) for star formation. We
introduce the Astro-DSB model, a variant of DSB with the pairwise domain
assumption tailored for astrophysical dynamics. By investigating its learning
process and prediction performance in both physically simulated data and in
real observations (the Taurus B213 data), we present two main takeaways. First,
from the astrophysical perspective, our proposed paired DSB method improves
interpretability, learning efficiency, and prediction performance over
conventional astrostatistical and other machine learning methods. Second, from
the generative modeling perspective, probabilistic generative modeling reveals
improvements over discriminative pixel-to-pixel modeling in Out-Of-Distribution
(OOD) testing cases of physical simulations with unseen initial conditions and
different dominant physical processes. Our study expands research into
diffusion models beyond the traditional visual synthesis application and
provides evidence of the models' learning abilities beyond pure data
statistics, paving a path for future physics-aware generative models which can
align dynamics between machine learning and real (astro)physical systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [288] [POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration](https://arxiv.org/abs/2506.08785)
*Mukul Lokhande,Santosh Kumar Vishvakarma*

Key words: AI加速, 多精度MAC, 边缘计算, 硬件-软件协同设计, 能效优化

TL;DR: PARV-CE是一款多精度MAC引擎，支持多种数值格式，通过硬件-软件协同设计优化性能和能耗，在边缘计算平台上表现优异。

<details>
  <summary>Details</summary>

Main category: cs.AR

Motivation: AI模型复杂度增加，需要灵活硬件支持多种精度格式，尤其是能耗受限的边缘平台。

Method: 采用统一数据路径支持4/8/16位定点、浮点和Posit格式，结合自适应性精度策略和可重构SIMD流水线。

Result: 相比现有技术，PDP提升2倍，资源使用减少3倍，精度损失在1.8% FP32基准内。

Conclusion: PARV-CE是一种可扩展且高效的解决方案，适用于边缘计算中的自适应精度AI加速。

Abstract: The increasing complexity of AI models requires flexible hardware capable of
supporting diverse precision formats, particularly for energy-constrained edge
platforms. This work presents PARV-CE, a SIMD-enabled, multi-precision MAC
engine that performs efficient multiply-accumulate operations using a unified
data-path for 4/8/16-bit fixed-point, floating point, and posit formats. The
architecture incorporates a layer adaptive precision strategy to align
computational accuracy with workload sensitivity, optimizing both performance
and energy usage. PARV-CE integrates quantization-aware execution with a
reconfigurable SIMD pipeline, enabling high-throughput processing with minimal
overhead through hardware-software co-design. The results demonstrate up to 2x
improvement in PDP and 3x reduction in resource usage compared to SoTA designs,
while retaining accuracy within 1.8% FP32 baseline. The architecture supports
both on-device training and inference across a range of workloads, including
DNNs, RNNs, RL, and Transformer models. The empirical analysis establish PARVCE
incorporated POLARON as a scalable and energy-efficient solution for
precision-adaptive AI acceleration at edge.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [289] [syren-baryon: Analytic emulators for the impact of baryons on the matter power spectrum](https://arxiv.org/abs/2506.08783)
*Lukas Kammerer,Deaglan J. Bartlett,Gabriel Kronberger,Harry Desmond,Pedro G. Ferreira*

Key words: 重子物理, 物质功率谱, 符号回归, CAMELS模拟

TL;DR: 该研究使用符号回归方法，提出了描述重子物理对物质功率谱影响的简单参数化，基于CAMELS模拟套件的不同重子物理模型，并提供了预测的不确定性分析。

<details>
  <summary>Details</summary>

Main category: astro-ph.CO

Motivation: 重子物理在当前和未来宇宙学调查中是一个关键的系统误差来源，研究旨在简化其影响的参数化表达。

Method: 利用符号回归方法，从CAMELS模拟套件中的四种不同重子物理模型中，构造了分析与重子物理无关功率谱比例的函数。

Result: 得到的函数在误差上与模拟的样本方差相当，且在大尺度和高红移时具有物理正确的行为。

Conclusion: 这些符号表达可用于区分不同的重子物理模型，并提供了公开代码。

Abstract: Baryonic physics has a considerable impact on the distribution of matter in
our Universe on scales probed by current and future cosmological surveys,
acting as a key systematic in such analyses. We seek simple symbolic
parametrisations for the impact of baryonic physics on the matter power
spectrum for a range of physically motivated models, as a function of
wavenumber, redshift, cosmology, and parameters controlling the baryonic
feedback. We use symbolic regression to construct analytic approximations for
the ratio of the matter power spectrum in the presence of baryons to that
without such effects. We obtain separate functions of each of four distinct
sub-grid prescriptions of baryonic physics from the CAMELS suite of
hydrodynamical simulations (Astrid, IllustrisTNG, SIMBA and Swift-EAGLE) as
well as for a baryonification algorithm. We also provide functions which
describe the uncertainty on these predictions, due to both the stochastic
nature of baryonic physics and the errors on our fits. The error on our
approximations to the hydrodynamical simulations is comparable to the sample
variance estimated through varying initial conditions, and our baryonification
expression has a root mean squared error of better than one percent, although
this increases on small scales. These errors are comparable to those of
previous numerical emulators for these models. Our expressions are enforced to
have the physically correct behaviour on large scales and at high redshift. Due
to their analytic form, we are able to directly interpret the impact of varying
cosmology and feedback parameters, and we can identify parameters which have
little to no effect. Each function is based on a different implementation of
baryonic physics, and can therefore be used to discriminate between these
models when applied to real data. We provide publicly available code for all
symbolic approximations found.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [290] [Real-Time Cascade Mitigation in Power Systems Using Influence Graph Improved by Reinforcement Learning](https://arxiv.org/abs/2506.08893)
*Kai Zhou,Youbiao He,Chong Zhong,Yifu Wu*

Key words: 级联停电, 马尔可夫决策过程, 强化学习, 电力系统, 实时缓解

TL;DR: 论文提出了一种基于马尔可夫决策过程（MDP）的强化学习方法，用于实时缓解电力传输系统中的级联停电风险，考虑了发电、负荷和初始偶然事件的不确定性。

<details>
  <summary>Details</summary>

Main category: physics.soc-ph

Motivation: 随着可再生能源渗透率的增加，现代电力系统面临级联停电风险上升的问题，需要实时快速的缓解策略。

Method: 通过将影响图扩展为MDP模型，结合强化学习中的策略梯度算法，设计奖励机制以避免系统条件恶化。

Result: 在IEEE 14-bus和118-bus系统上的验证表明，主动断线可有效降低级联风险，部分线路在缓解级联传播中表现关键。

Conclusion: 提出的方法能够更快收敛，并通过保守动作有效减少级联停电风险。

Abstract: Despite high reliability, modern power systems with growing renewable
penetration face an increasing risk of cascading outages. Real-time cascade
mitigation requires fast, complex operational decisions under uncertainty. In
this work, we extend the influence graph into a Markov decision process model
(MDP) for real-time mitigation of cascading outages in power transmission
systems, accounting for uncertainties in generation, load, and initial
contingencies. The MDP includes a do-nothing action to allow for conservative
decision-making and is solved using reinforcement learning. We present a policy
gradient learning algorithm initialized with a policy corresponding to the
unmitigated case and designed to handle invalid actions. The proposed learning
method converges faster than the conventional algorithm. Through careful reward
design, we learn a policy that takes conservative actions without deteriorating
system conditions. The model is validated on the IEEE 14-bus and IEEE 118-bus
systems. The results show that proactive line disconnections can effectively
reduce cascading risk, and certain lines consistently emerge as critical in
mitigating cascade propagation.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [291] [midr: Learning from Black-Box Models by Maximum Interpretation Decomposition](https://arxiv.org/abs/2506.08338)
*Ryoichi Asashiba,Reiji Kozuma,Hirokazu Iwasawa*

Key words: 可解释机器学习,可解释人工智能,黑盒模型,MID

TL;DR: 介绍R包midr，实现最大解释分解（MID），用于解释黑盒模型。

<details>
  <summary>Details</summary>

Main category: stat.ME

Motivation: 在需要模型和预测可解释性的领域中，应用可解释机器学习和可解释人工智能方法至关重要。

Method: MID通过最小化黑盒模型预测函数与低阶可加表示之间的平方误差，实现功能分解。

Result: midr能够构建具有高级分析能力的全局替代模型，从而从黑盒模型中学习。

Conclusion: 本文展示了midr包的用途并讨论了其关键特性。

Abstract: The use of appropriate methods of Interpretable Machine Learning (IML) and
eXplainable Artificial Intelligence (XAI) is essential for adopting black-box
predictive models in fields where model and prediction explainability is
required. As a novel tool for interpreting black-box models, we introduce the R
package midr, which implements Maximum Interpretation Decomposition (MID). MID
is a functional decomposition approach that derives a low-order additive
representation of a black-box model by minimizing the squared error between the
model's prediction function and this additive representation. midr enables
learning from black-box models by constructing a global surrogate model with
advanced analytical capabilities. After reviewing related work and the
theoretical foundation of MID, we demonstrate the package's usage and discuss
some of its key features.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [292] [TS-PIELM: Time-Stepping Physics-Informed Extreme Learning Machine Facilitates Soil Consolidation Analyses](https://arxiv.org/abs/2506.08381)
*He Yang,Fei Ren,Hai-Sui Yu,Xueyu Geng,Pei-Zhi Zhuang*

Key words: 物理信息机器学习,极限学习机,土壤固结,计算效率,精度

TL;DR: 论文提出了一种时间步进物理信息极限学习机（TS-PIELM），显著提升了土壤固结分析的精度和效率，相比传统PINN方法，计算效率和精度分别提高了1000倍和100倍。

<details>
  <summary>Details</summary>

Main category: physics.geo-ph

Motivation: 传统物理信息神经网络（PINN）在土壤固结分析中的精度和效率不足，需要改进以成为有竞争力的替代方法。

Method: 提出TS-PIELM框架，将固结过程划分为多个时间间隔，使用单层前馈极限学习机（ELM）近似解，直接计算输出层权重。

Result: 在一维情况下，TS-PIELM的计算效率和精度分别比PINN提高了1000倍和100倍。

Conclusion: PIML是计算岩土力学的有力工具，TS-PIELM在土壤固结分析中表现出色。

Abstract: Accuracy and efficiency of the conventional physics-informed neural network
(PINN) need to be improved before it can be a competitive alternative for soil
consolidation analyses. This paper aims to overcome these limitations by
proposing a highly accurate and efficient physics-informed machine learning
(PIML) approach, termed time-stepping physics-informed extreme learning machine
(TS-PIELM). In the TS-PIELM framework the consolidation process is divided into
numerous time intervals, which helps overcome the limitation of PIELM in
solving differential equations with sharp gradients. To accelerate network
training, the solution is approximated by a single-layer feedforward extreme
learning machine (ELM), rather than using a fully connected neural network in
PINN. The input layer weights of the ELM network are generated randomly and
fixed during the training process. Subsequently, the output layer weights are
directly computed by solving a system of linear equations, which significantly
enhances the training efficiency compared to the time-consuming gradient
descent method in PINN. Finally, the superior performance of TS-PIELM is
demonstrated by solving three typical Terzaghi consolidation problems. Compared
to PINN, results show that the computational efficiency and accuracy of the
novel TS-PIELM framework are improved by more than 1000 times and 100 times for
one-dimensional cases, respectively. This paper provides compelling evidence
that PIML can be a powerful tool for computational geotechnics.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [293] [Flow-Lenia: Emergent evolutionary dynamics in mass conservative continuous cellular automata](https://arxiv.org/abs/2506.08569)
*Erwan Plantec,Gautier Hamon,Mayalen Etcheverry,Bert Wang-Chak Chan,Pierre-Yves Oudeyer,Clément Moulin-Frier*

Key words: 人工生命, Lenia, 质量守恒, 多物种模拟, 进化动力学

TL;DR: 论文介绍了Flow-Lenia，一种基于Lenia的保持质量的扩展模型，用于生成复杂生命样模式，并展示了其参数优化和多物种模拟的能力。

<details>
  <summary>Details</summary>

Main category: nlin.CG

Motivation: 研究人工生命领域中的自发产生生命特性的系统，尤其是通过扩展Lenia模型，进一步探索生命样模式生成和进化的可能性。

Method: 提出Flow-Lenia模型，通过质量守恒的扩展和参数优化，生成复杂行为的空间局部模式，并将其参数嵌入自身动力学以实现多物种模拟。

Result: Flow-Lenia成功生复杂生命样模式，并展示了其参数优化和多物种模拟的能力。

Conclusion: Flow-Lenia为人工生命研究提供了新的工具，能够更有效地模拟和探索生命样模式和进化动力学。

Abstract: Central to the artificial life endeavour is the creation of artificial
systems spontaneously generating properties found in the living world such as
autopoiesis, self-replication, evolution and open-endedness. While numerous
models and paradigms have been proposed, cellular automata (CA) have taken a
very important place in the field notably as they enable the study of
phenomenons like self-reproduction and autopoiesis. Continuous CA like Lenia
have been showed to produce life-like patterns reminiscent, on an aesthetic and
ontological point of view, of biological organisms we call creatures. We
propose in this paper Flow-Lenia, a mass conservative extension of Lenia. We
present experiments demonstrating its effectiveness in generating
spatially-localized patters (SLPs) with complex behaviors and show that the
update rule parameters can be optimized to generate complex creatures showing
behaviors of interest. Furthermore, we show that Flow-Lenia allows us to embed
the parameters of the model, defining the properties of the emerging patterns,
within its own dynamics thus allowing for multispecies simulations. By using
the evolutionary activity framework as well as other metrics, we shed light on
the emergent evolutionary dynamics taking place in this system.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [294] [DCD: A Semantic Segmentation Model for Fetal Ultrasound Four-Chamber View](https://arxiv.org/abs/2506.08534)
*Donglian Li,Hui Guo,Minglang Chen,Huizhen Chen,Jialing Chen,Bocheng Liang,Pengchen Liang,Ying Tan*

Key words: 胎儿心尖四腔心切图,先天性心脏病,深度学习,自动分割,Dense ASPP,CBAM

TL;DR: 提出了一种名为DCD的深度学习模型，用于自动分割胎儿心尖四腔心切图中的关键解剖结构，以提高分割精度并减少超声检查师的工作量。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 胎儿心尖四腔心切图中解剖结构的准确分割对于先天性心脏病的早期诊断和产前评估至关重要，但当前方法受超声伪影、噪声和解剖变异性的限制。

Method: 采用了结合Dense ASPP模块和CBAM的深度学习模型DCD，以提取多尺度特征并增强自适应特征表示。

Result: DCD模型能够有效捕获局部和全局上下文信息，实现了精确且稳健的分割，有助于提升产前心脏评估。

Conclusion: DCD模型在胎儿心尖四腔心切图的分割任务中表现出色，为先天性心脏病的产前诊断提供了有力支持。

Abstract: Accurate segmentation of anatomical structures in the apical four-chamber
(A4C) view of fetal echocardiography is essential for early diagnosis and
prenatal evaluation of congenital heart disease (CHD). However, precise
segmentation remains challenging due to ultrasound artifacts, speckle noise,
anatomical variability, and boundary ambiguity across different gestational
stages. To reduce the workload of sonographers and enhance segmentation
accuracy, we propose DCD, an advanced deep learning-based model for automatic
segmentation of key anatomical structures in the fetal A4C view. Our model
incorporates a Dense Atrous Spatial Pyramid Pooling (Dense ASPP) module,
enabling superior multi-scale feature extraction, and a Convolutional Block
Attention Module (CBAM) to enhance adaptive feature representation. By
effectively capturing both local and global contextual information, DCD
achieves precise and robust segmentation, contributing to improved prenatal
cardiac assessment.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [295] [MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback](https://arxiv.org/abs/2506.08634)
*Alvaro Becerra,Daniel Andres,Pablo Villegas,Roberto Daza,Ruth Cobos*

Key words: 多模态反馈,学习分析,人工智能,个性化反馈,协作评估

TL;DR: 提出了一种名为MOSAIC-F的多模态反馈框架，整合了多模态学习分析、观察、传感器、人工智能和协作评估，为学生提供个性化反馈。框架包括四个步骤：标准化评估、多模态数据收集、AI生成反馈和学生自我评估。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 现有反馈系统往往缺乏个性化和多维度分析，MOSAIC-F旨在通过结合人为评估和数据驱动分析，提供更精准和可操作的反馈。

Method: 框架通过标准化评估表收集定量和定性数据，采集多模态学习数据（如视频、音频、生理信号等），利用AI生成个性化反馈，并结合学生自我评估与可视化。

Result: 在提升口语表达能力的场景中测试了该框架，结果表明其能提供更准确的个性化反馈。

Conclusion: MOSAIC-F结合人为与数据驱动评估，显著提升了反馈的准确性和个性化程度。

Abstract: In this article, we present a novel multimodal feedback framework called
MOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal
Learning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI),
and Collaborative assessments for generating personalized feedback on student
learning activities. This framework consists of four key steps. First, peers
and professors' assessments are conducted through standardized rubrics (that
include both quantitative and qualitative evaluations). Second, multimodal data
are collected during learning activities, including video recordings, audio
capture, gaze tracking, physiological signals (heart rate, motion data), and
behavioral interactions. Third, personalized feedback is generated using AI,
synthesizing human-based evaluations and data-based multimodal insights such as
posture, speech patterns, stress levels, and cognitive load, among others.
Finally, students review their own performance through video recordings and
engage in self-assessment and feedback visualization, comparing their own
evaluations with peers and professors' assessments, class averages, and
AI-generated recommendations. By combining human-based and data-based
evaluation techniques, this framework enables more accurate, personalized and
actionable feedback. We tested MOSAIC-F in the context of improving oral
presentation skills.

</details>


### [296] [Stop Misusing t-SNE and UMAP for Visual Analytics](https://arxiv.org/abs/2506.08725)
*Hyeon Jeon,Jeongin Park,Sungbok Shin,Jinwook Seo*

Key words: t-SNE, UMAP, 可视化分析, 降维, 误用

TL;DR: 本文探讨了t-SNE和UMAP在可视化分析中的误用问题，分析了原因并提出了改进建议。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 研究发现t-SNE和UMAP在可视化分析中常被误用于不适当的场景，尤其是对簇间距离的错误解读。

Method: 通过文献综述（114篇论文）和访谈研究，验证误用的普遍性并分析原因。

Result: 误用主要源于对技术适用性讨论不足，以及实践中隐含动机未公开。

Conclusion: 建议未来明确技术适用范围并推动合理使用降维技术。

Abstract: Misuses of t-SNE and UMAP in visual analytics have become increasingly
common. For example, although t-SNE and UMAP projections often do not
faithfully reflect true distances between clusters, practitioners frequently
use them to investigate inter-cluster relationships. In this paper, we bring
this issue to the surface and comprehensively investigate why such misuse
occurs and how to prevent it. We conduct a literature review of 114 papers to
verify the prevalence of the misuse and analyze the reasonings behind it. We
then execute an interview study to uncover practitioners' implicit motivations
for using these techniques -- rationales often undisclosed in the literature.
Our findings indicate that misuse of t-SNE and UMAP primarily stems from
limited discourse on their appropriate use in visual analytics. We conclude by
proposing future directions and concrete action items to promote more
reasonable use of DR.

</details>


### [297] [Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU](https://arxiv.org/abs/2506.08911)
*Petar Jakuš,Hrvoje Džapo*

Key words: 关键词检测(NXP MCXN947, NPU, MFCC, CNN, 量化感知训练)

TL;DR: 论文提出了一种基于NXP MCXN947微控制器的关键词检测系统，利用集成NPU实现资源受限设备上的实时语音交互。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 解决资源受限设备上实时语音交互的挑战。

Method: 结合MFCC特征提取与CNN分类器，并通过量化感知训练优化模型。

Result: 实验结果展示NPU比CPU快59倍，准确率达97.06%，模型大小为30.58 KB。

Conclusion: 证明了嵌入式平台上高效低功耗语音接口的可行性。

Abstract: This paper presents a keyword spotting (KWS) system implemented on the NXP
MCXN947 microcontroller with an integrated Neural Processing Unit (NPU),
enabling real-time voice interaction on resource-constrained devices. The
system combines MFCC feature extraction with a CNN classifier, optimized using
Quantization Aware Training to reduce model size with minimal accuracy drop.
Experimental results demonstrate a 59x speedup in inference time when
leveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy
with a model size of 30.58 KB, demonstrating the feasibility of efficient,
low-power voice interfaces on embedded platforms.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [298] [KP-PINNs: Kernel Packet Accelerated Physics Informed Neural Networks](https://arxiv.org/abs/2506.08563)
*Siyuan Yang,Cheng Song,Zhilu Lai,Wenjia Wang*

Key words: 微分方程, 物理信息神经网络, 再生核希尔伯特空间, 核包方法, 科学计算

TL;DR: KP-PINNs是一种基于核包方法的物理信息神经网络框架，通过使用再生核希尔伯特空间（RKHS）范数重新定义损失函数，提升了复杂微分方程求解的稳定性和准确性。

<details>
  <summary>Details</summary>

Main category: cs.CE

Motivation: 现有的PINNs在求解复杂微分方程时，L2损失函数可能导致不正确和不稳定的数值解，因此需要改进损失函数的设计以提高稳定性和求解效果。

Method: 提出了KP-PINNs框架，利用RKHS范数重新表达损失函数，并通过核包（KP）方法加速计算。

Result: 理论分析和数值实验表明，KP-PINNs能够稳定且高效地求解多种微分方程。

Conclusion: KP-PINNs为提升基于PINNs的科学计算求解器的稳定性和准确性提供了新方向。

Abstract: Differential equations are involved in modeling many engineering problems.
Many efforts have been devoted to solving differential equations. Due to the
flexibility of neural networks, Physics Informed Neural Networks (PINNs) have
recently been proposed to solve complex differential equations and have
demonstrated superior performance in many applications. While the L2 loss
function is usually a default choice in PINNs, it has been shown that the
corresponding numerical solution is incorrect and unstable for some complex
equations. In this work, we propose a new PINNs framework named Kernel Packet
accelerated PINNs (KP-PINNs), which gives a new expression of the loss function
using the reproducing kernel Hilbert space (RKHS) norm and uses the Kernel
Packet (KP) method to accelerate the computation. Theoretical results show that
KP-PINNs can be stable across various differential equations. Numerical
experiments illustrate that KP-PINNs can solve differential equations
effectively and efficiently. This framework provides a promising direction for
improving the stability and accuracy of PINNs-based solvers in scientific
computing.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [299] [A Privacy-Preserving Federated Learning Framework for Generalizable CBCT to Synthetic CT Translation in Head and Neck](https://arxiv.org/abs/2506.08654)
*Ciro Benito Raggio,Paolo Zaffino,Maria Francesca Spadea*

Key words: CBCT, sCT, 联邦学习, 头颈部, 深度学习

TL;DR: 该论文提出了一种跨机构的水平联邦学习方法（FedSynthCT），用于头颈部CBCT到sCT的合成，解决了多中心数据共享的限制。

<details>
  <summary>Details</summary>

Main category: physics.med-ph

Motivation: CBCT在放疗中的应用受限于噪声、软组织对比度不足等问题，而传统深度学习方法因机构异质性和数据隐私问题难以推广。

Method: 采用条件生成对抗网络，通过联邦学习在多中心数据（SynthRAD2025数据集）上进行协作训练。

Result: 模型在多个中心表现出良好的泛化能力，MAE为64.38-85.90 HU，SSIM为0.882-0.922，PSNR为32.86-34.91 dB，外部验证结果也证明了其鲁棒性。

Conclusion: 联邦学习为CBCT-to-sCT合成提供了可行方案，保护数据隐私的同时实现了模型的多中心泛化。

Abstract: Shortened Abstract
  Cone-beam computed tomography (CBCT) has become a widely adopted modality for
image-guided radiotherapy (IGRT). However, CBCT suffers from increased noise,
limited soft-tissue contrast, and artifacts, resulting in unreliable Hounsfield
unit values and hindering direct dose calculation. Synthetic CT (sCT)
generation from CBCT addresses these issues, especially using deep learning
(DL) methods. Existing approaches are limited by institutional heterogeneity,
scanner-dependent variations, and data privacy regulations that prevent
multi-center data sharing.
  To overcome these challenges, we propose a cross-silo horizontal federated
learning (FL) approach for CBCT-to-sCT synthesis in the head and neck region,
extending our FedSynthCT framework. A conditional generative adversarial
network was collaboratively trained on data from three European medical centers
in the public SynthRAD2025 challenge dataset.
  The federated model demonstrated effective generalization across centers,
with mean absolute error (MAE) ranging from $64.38\pm13.63$ to $85.90\pm7.10$
HU, structural similarity index (SSIM) from $0.882\pm0.022$ to $0.922\pm0.039$,
and peak signal-to-noise ratio (PSNR) from $32.86\pm0.94$ to $34.91\pm1.04$ dB.
Notably, on an external validation dataset of 60 patients, comparable
performance was achieved (MAE: $75.22\pm11.81$ HU, SSIM: $0.904\pm0.034$, PSNR:
$33.52\pm2.06$ dB) without additional training, confirming robust
generalization despite protocol, scanner differences and registration errors.
  These findings demonstrate the technical feasibility of FL for CBCT-to-sCT
synthesis while preserving data privacy and offer a collaborative solution for
developing generalizable models across institutions without centralized data
sharing or site-specific fine-tuning.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [300] [QUITE: A Query Rewrite System Beyond Rules with LLM Agents](https://arxiv.org/abs/2506.07675)
*Yuyang Song,Hanxu Yan,Jiale Lao,Yibo Wang,Yufei Li,Yuanchun Zhou,Jianguo Wang,Mingjie Tang*

Key words: SQL query rewrite, LLM, multi-agent framework, performance optimization

TL;DR: 论文提出了一种基于LLM的SQL查询重写系统QUITE，解决了传统规则方法无法处理多样化查询模式的问题，通过多智能体框架和实时数据库反馈实现高效重写。

<details>
  <summary>Details</summary>

Main category: cs.DB

Motivation: 传统依赖固定规则的查询重写方法处理范围有限且可能引发性能退化，LLMs展现出接近人类的语义理解和推理能力，但直接应用易产生非等效和次优查询。

Method: 设计了由有限状态机控制的多智能体框架，利用外部工具和实时数据库反馈增强重写；开发重写中间件优化查询等效性；引入提示注入技术改进执行计划。

Result: 将查询执行时间减少35.8%，生成的重写数量增加24.1%，覆盖了先前方法无法处理的查询案例。

Conclusion: QUITE系统利用LLM和智能体技术，显著提升了SQL查询重写的效果和适用性。

Abstract: Query rewrite transforms SQL queries into semantically equivalent forms that
run more efficiently. Existing approaches mainly rely on predefined rewrite
rules, but they handle a limited subset of queries and can cause performance
regressions. This limitation stems from three challenges of rule-based query
rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite
rules do not generalize to new query patterns, and (3) some rewrite techniques
cannot be expressed as fixed rules. Motivated by the fact that human experts
exhibit significantly better rewrite ability but suffer from scalability, and
Large Language Models (LLMs) have demonstrated nearly human-level semantic and
reasoning abilities, we propose a new approach of using LLMs to rewrite SQL
queries beyond rules. Due to the hallucination problems in LLMs, directly
applying LLMs often leads to nonequivalent and suboptimal queries. To address
this issue, we propose QUITE (query rewrite), a training-free and
feedback-aware system based on LLM agents that rewrites SQL queries into
semantically equivalent forms with significantly better performance, covering a
broader range of query patterns and rewrite strategies compared to rule-based
methods. Firstly, we design a multi-agent framework controlled by a finite
state machine (FSM) to equip LLMs with the ability to use external tools and
enhance the rewrite process with real-time database feedback. Secondly, we
develop a rewrite middleware to enhance the ability of LLMs to generate
optimized query equivalents. Finally, we employ a novel hint injection
technique to improve execution plans for rewritten queries. Extensive
experiments show that QUITE reduces query execution time by up to 35.8% over
state-of-the-art approaches and produces 24.1% more rewrites than prior
methods, covering query cases that earlier systems did not handle.

</details>


### [301] [RADAR: Benchmarking Language Models on Imperfect Tabular Data](https://arxiv.org/abs/2506.08249)
*Ken Gu,Zhihan Zhang,Kate Lin,Yuwei Zhang,Akshay Paruchuri,Hong Yu,Mehran Kazemi,Kumar Ayush,A. Ali Heydari,Maxwell A. Xu,Girish Narayanswamy,Yun Liu,Ming-Zher Poh,Yuzhe Yang,Mark Malhotra,Shwetak Patel,Hamid Palangi,Xuhai Xu,Daniel McDuff,Tim Althoff,Xin Liu*

Key words: 语言模型, 表格数据, 数据异常, RADAR, 基准测试

TL;DR: 该论文介绍了RADAR，一个用于评估语言模型在表格数据中处理数据能力的基准测试，揭示了前沿模型在处理数据异常时的显著不足。

<details>
  <summary>Details</summary>

Main category: cs.DB

Motivation: 探索语言模型在真实表格数据中识别和处理数据异常的能力，填补了该领域的研究空白。

Method: 开发了RADAR基准测试，通过程序化扰动模拟数据异常，并包含2980个表格查询对，覆盖9个领域和5种异常类型。

Result: 研究表明，前沿模型在没有异常时表现尚可，但在处理异常时性能显著下降。

Conclusion: RADAR为提升表格数据的鲁棒性分析提供了灵活和可扩展的资源。

Abstract: Language models (LMs) are increasingly being deployed to perform autonomous
data analyses. However, their data awareness -- the ability to recognize,
reason over, and appropriately handle data artifacts such as missing values,
outliers, and logical inconsistencies -- remains underexplored. These artifacts
are especially common in real-world tabular data and, if mishandled, can
significantly compromise the validity of analytical conclusions. To address
this gap, we present RADAR, a benchmark for systematically evaluating
data-aware reasoning on tabular data. We develop a framework to simulate data
artifacts via programmatic perturbations to enable targeted evaluation of model
behavior. RADAR comprises 2980 table query pairs, grounded in real-world data
spanning 9 domains and 5 data artifact types. In addition to evaluating
artifact handling, RADAR systematically varies table size to study how
reasoning performance holds when increasing table size. Our evaluation reveals
that, despite decent performance on tables without data artifacts, frontier
models degrade significantly when data artifacts are introduced, exposing
critical gaps in their capacity for robust, data-aware analysis. Designed to be
flexible and extensible, RADAR supports diverse perturbation types and
controllable table sizes, offering a valuable resource for advancing tabular
reasoning.

</details>


### [302] [LEANN: A Low-Storage Vector Index](https://arxiv.org/abs/2506.08276)
*Yichuan Wang,Shu Liu,Zhifei Li,Yongji Wu,Ziming Mao,Yilong Zhao,Xiao Yan,Zhiying Xu,Yang Zhou,Ion Stoica,Sewon Min,Matei Zaharia,Joseph E. Gonzalez*

Key words: 嵌入搜索,近似最近邻,存储优化,本地部署

TL;DR: LEANN是一种高效的近似最近邻搜索索引，专为资源有限的个人设备设计，显著减少存储开销并保持检索性能。

<details>
  <summary>Details</summary>

Main category: cs.DB

Motivation: 随着对本地设备上个人数据的嵌入搜索需求增加，现有方法的高存储开销成为主要瓶颈，亟需解决方案。

Method: LEANN结合紧凑的基于图的结构和动态重新计算策略，以最小化存储占用。

Result: LEANN将索引大小减少至原始数据的5%以下，存储空间比标准索引小50倍，并在真实问答基准中保持90%的top-3召回率，检索时间在2秒内。

Conclusion: LEANN为资源受限设备提供了一种高效的嵌入搜索解决方案，显著降低了存储需求。

Abstract: Embedding-based search is widely used in applications such as recommendation
and retrieval-augmented generation (RAG). Recently, there is a growing demand
to support these capabilities over personal data stored locally on devices.
However, maintaining the necessary data structure associated with the
embedding-based search is often infeasible due to its high storage overhead.
For example, indexing 100 GB of raw data requires 150 to 700 GB of storage,
making local deployment impractical. Reducing this overhead while maintaining
search quality and latency becomes a critical challenge. In this paper, we
present LEANN, a storage-efficient approximate nearest neighbor (ANN) search
index optimized for resource-constrained personal devices. LEANN combines a
compact graph-based structure with an efficient on-the-fly recomputation
strategy to enable fast and accurate retrieval with minimal storage overhead.
Our evaluation shows that LEANN reduces index size to under 5% of the original
raw data, achieving up to 50 times smaller storage than standard indexes, while
maintaining 90% top-3 recall in under 2 seconds on real-world question
answering benchmarks.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [303] [Generalizing while preserving monotonicity in comparison-based preference learning models](https://arxiv.org/abs/2506.08616)
*Julien Fageot,Peva Blanchard,Gilles Bareilles,Lê-Nguyên Hoang*

Key words: 偏好学习，单调性，广义Bradley-Terry模型，扩散先验，嵌入条件

TL;DR: 探讨了学习模型在偏好反馈中的单调性问题，提出了新的具有推广能力的单调模型。

<details>
  <summary>Details</summary>

Main category: math.ST

Motivation: 现有许多基于比较的偏好学习模型无法保证单调性，无法推广到未比较数据。

Method: 提出了一类新的线性广义Bradley-Terry模型，结合扩散先验，并确定了保证单调性的嵌入条件。

Result: 实验表明新模型在数据有限时能提升准确性，同时保证了单调性。

Conclusion: 新模型填补了现有方法在单调性和推广能力上的不足。

Abstract: If you tell a learning model that you prefer an alternative $a$ over another
alternative $b$, then you probably expect the model to be monotone, that is,
the valuation of $a$ increases, and that of $b$ decreases. Yet, perhaps
surprisingly, many widely deployed comparison-based preference learning models,
including large language models, fail to have this guarantee. Until now, the
only comparison-based preference learning algorithms that were proved to be
monotone are the Generalized Bradley-Terry models. Yet, these models are unable
to generalize to uncompared data. In this paper, we advance the understanding
of the set of models with generalization ability that are monotone. Namely, we
propose a new class of Linear Generalized Bradley-Terry models with Diffusion
Priors, and identify sufficient conditions on alternatives' embeddings that
guarantee monotonicity. Our experiments show that this monotonicity is far from
being a general guarantee, and that our new class of generalizing models
improves accuracy, especially when the dataset is limited.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [304] [PerfTracker: Online Performance Troubleshooting for Large-scale Model Training in Production](https://arxiv.org/abs/2506.08528)
*Yu Guan,Zhiyu Yin,Haoyu Chen,Sheng Cheng,Chaojie Yang,Tianyin Xu,Yang Zhang,Hanyu Zhao,Yong Li,Dennis Cai,Ennan Zhai*

Key words: PerfTracker, 大规模模型训练, 性能诊断, GPU集群, 细粒度分析

TL;DR: PerfTracker是一种在线故障排除系统，专注于诊断大规模模型训练中的性能问题，通过细粒度分析和差分观察来定位问题根源。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 解决现代GPU集群规模庞大、软硬件交互复杂以及训练过程数据密集导致的性能问题诊断挑战。

Method: 采用细粒度分析技术，结合差分观察性，最小化生产影响，定位问题的硬件和软件根源。

Result: 已部署在拥有上万GPU的生产环境中，成功诊断多种复杂性能问题。

Conclusion: PerfTracker有效地解决了大规模模型训练中的性能诊断问题。

Abstract: Troubleshooting performance problems of large model training (LMT) is
immensely challenging, due to unprecedented scales of modern GPU clusters, the
complexity of software-hardware interactions, and the data intensity of the
training process. Existing troubleshooting approaches designed for traditional
distributed systems or datacenter networks fall short and can hardly apply to
real-world training systems. In this paper, we present PerfTracker, the first
online troubleshooting system utilizing fine-grained profiling, to diagnose
performance issues of large-scale model training in production. PerfTracker can
diagnose performance issues rooted in both hardware (e.g., GPUs and their
interconnects) and software (e.g., Python functions and GPU operations). It
scales to LMT on modern GPU clusters. PerfTracker effectively summarizes
runtime behavior patterns of fine-grained LMT functions via online profiling,
and leverages differential observability to localize the root cause with
minimal production impact. PerfTracker has been deployed as a production
service for large-scale GPU clusters of O(10, 000) GPUs (product homepage
https://help.aliyun.com/zh/pai/user-guide/perftracker-online-performance-analysis-diagnostic-tool).
It has been used to diagnose a variety of difficult performance issues.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [305] [A Metrics-Oriented Architectural Model to Characterize Complexity on Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.08153)
*Renato Cordeiro Ferreira*

Key words: ML驱动的系统, 复杂性管理, 架构模型, 指标收集

TL;DR: 论文提出了一种基于指标的架构模型，用于管理ML驱动的系统的复杂性，并支持架构决策。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 研究如何有效管理ML驱动的系统的复杂性，以支持其架构决策和系统发展。

Method: 扩展参考架构以描述ML驱动的系统，并收集相关指标。

Result: 提出了构建基于指标的架构模型的第一步，即扩展参考架构以收集指标。

Conclusion: 该研究为ML驱动的系统的复杂性管理提供了初步框架，支持未来发展。

Abstract: How can the complexity of ML-enabled systems be managed effectively? The goal
of this research is to investigate how complexity affects ML-Enabled Systems
(MLES). To address this question, this research aims to introduce a
metrics-based architectural model to characterize the complexity of MLES. The
goal is to support architectural decisions, providing a guideline for the
inception and growth of these systems. This paper showcases the first step for
creating the metrics-based architectural model: an extension of a reference
architecture that can describe MLES to collect their metrics.

</details>


### [306] [Worst-Case Symbolic Constraints Analysis and Generalisation with Large Language Models](https://arxiv.org/abs/2506.08171)
*Daniel Koh,Yannic Noller,Corina S. Pasareanu,Adrians Skapars,Youcheng Sun*

Key words: 大型语言模型,符号推理,最坏情况分析,SMT约束求解

TL;DR: 本研究探讨了大型语言模型（LLM）在程序最坏情况执行中的符号约束分析能力，通过SMT约束求解和专门的符号约束数据集，提升了模型的性能，证明LLM能够进行更深层次的符号推理。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 尽管LLM在代码生成、完成和修复等任务中表现良好，但更复杂的符号推理任务尚未得到充分探索。本研究旨在填补这一空白，连接LLM与符号推理方法。

Method: 研究者定义了最坏情况符号约束分析问题，并通过SMT约束求解和专门的数据集对现有LLM进行评估和优化，提出了WARP-1.0-3B模型。

Result: 实验表明，经过优化的3B规模的WARP-1.0-3B模型在符号推理任务中超越了规模相当甚至更大的基线模型。

Conclusion: LLM能够进行更深层次的符号推理，支持神经网络学习与形式化方法的更紧密集成。

Abstract: Large language models (LLMs) have been successfully applied to a variety of
coding tasks, including code generation, completion, and repair. However, more
complex symbolic reasoning tasks remain largely unexplored by LLMs. This paper
investigates the capacity of LLMs to reason about worst-case executions in
programs through symbolic constraints analysis, aiming to connect LLMs and
symbolic reasoning approaches. Specifically, we define and address the problem
of worst-case symbolic constraints analysis as a measure to assess the
comprehension of LLMs. We evaluate the performance of existing LLMs on this
novel task and further improve their capabilities through symbolic
reasoning-guided fine-tuning, grounded in SMT (Satisfiability Modulo Theories)
constraint solving and supported by a specially designed dataset of symbolic
constraints. Experimental results show that our solver-aligned model,
WARP-1.0-3B, consistently surpasses size-matched and even much larger
baselines, demonstrating that a 3B LLM can recover the very constraints that
pin down an algorithm's worst-case behaviour through reinforcement learning
methods. These findings suggest that LLMs are capable of engaging in deeper
symbolic reasoning, supporting a closer integration between neural
network-based learning and formal methods for rigorous program analysis.

</details>


### [307] [Repeton: Structured Bug Repair with ReAct-Guided Patch-and-Test Cycles](https://arxiv.org/abs/2506.08173)
*Nguyen Phu Vinh,Anh Chung Hoang,Chris Ngo,Truong-Son Hy*

Key words: LLMs,代码生成,软件工程,自动化测试,可解释性

TL;DR: Repeton是一个基于LLMs的自动化代码操作框架，通过结构化补丁和测试流程提升代码生成精度和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: LLMs在代码生成和理解方面表现优异，但在复杂软件工程任务中精度和可解释性不足，需要改进。

Method: Repeton采用分步补丁和测试流程，结合轻量级启发式方法和开发工具，避免依赖嵌入检索系统。

Result: 在SWE-bench Lite基准测试中，Repeton在补丁有效性和可解释性上优于RAG方法。

Conclusion: 通过模块化和可验证的任务分解，Repeton为可扩展和透明的自主调试提供了实用方案。

Abstract: Large Language Models (LLMs) have shown strong capabilities in code
generation and comprehension, yet their application to complex software
engineering tasks often suffers from low precision and limited
interpretability. We present Repeton, a fully open-source framework that
leverages LLMs for precise and automated code manipulation in real-world Git
repositories. Rather than generating holistic fixes, Repeton operates through a
structured patch-and-test pipeline: it iteratively diagnoses issues, proposes
code changes, and validates each patch through automated testing. This stepwise
process is guided by lightweight heuristics and development tools, avoiding
reliance on embedding-based retrieval systems. Evaluated on the SWE-bench Lite
benchmark, our method shows good performance compared to RAG-based methods in
both patch validity and interpretability. By decomposing software engineering
tasks into modular, verifiable stages, Repeton provides a practical path toward
scalable and transparent autonomous debugging.

</details>


### [308] [Understanding Software Engineering Agents Through the Lens of Traceability: An Empirical Study](https://arxiv.org/abs/2506.08311)
*Ira Ceka,Saurabh Pujar,Shyam Ramji,Luca Buratti,Gail Kaiser,Baishakhi Ray*

Key words: 软件工程代理、决策路径、错误定位、补丁生成、测试生成

TL;DR: 本文通过执行跟踪系统研究了软件工程代理（SWE代理）的行为，提出了首个决策路径分类法，并深入分析了代理成功的关键组件及其对补丁生成的影响。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 虽然SWE代理在软件任务自动化中表现出色，但其内部决策机制仍不清晰。理解这些机制有助于提升代理的可靠性和效率。

Method: 研究通过执行跟踪分析五种代表性代理，提出决策路径分类法，并深入分析代理成功的关键组件（如错误定位和补丁生成）。还研究了测试生成对补丁成功的影响。

Result: 研究发现关键组件对代理成功至关重要，测试生成策略影响补丁生成。大规模代码克隆分析揭示了代理生成补丁与开发者编写的补丁在结构和风格上的差异。

Conclusion: 研究结果为设计更高效且更符合人类开发实践的SWE代理提供了新见解和途径。

Abstract: With the advent of large language models (LLMs), software engineering agents
(SWE agents) have emerged as a powerful paradigm for automating a range of
software tasks -- from code generation and repair to test case synthesis. These
agents operate autonomously by interpreting user input and responding to
environmental feedback. While various agent architectures have demonstrated
strong empirical performance, the internal decision-making worfklows that drive
their behavior remain poorly understood. Deeper insight into these workflows
hold promise for improving both agent reliability and efficiency. In this work,
we present the first systematic study of SWE agent behavior through the lens of
execution traces. Our contributions are as follows: (1) we propose the first
taxonomy of decision-making pathways across five representative agents; (2)
using this taxonomy, we identify three core components essential to agent
success -- bug localization, patch generation, and reproduction test generation
-- and study each in depth; (3) we study the impact of test generation on
successful patch production; and analyze strategies that can lead to successful
test generation; (4) we further conduct the first large-scale code clone
analysis comparing agent-generated and developer-written patches and provide a
qualitative study revealing structural and stylistic differences in patch
content. Together, these findings offer novel insights into agent design and
open avenues for building agents that are both more effective and more aligned
with human development practices.

</details>


### [309] [Do Generative AI Tools Ensure Green Code? An Investigative Study](https://arxiv.org/abs/2506.08790)
*Samarth Sikand,Rohit Mehra,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Key words: 软件可持续性, 生成式AI, 环保编码, ChatGPT, BARD, Copilot

TL;DR: 研究探讨了AI生成代码在可持续性方面的表现，发现主流工具生成的代码普遍存在不环保行为，需进一步研究和改进。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 随着生成式AI工具的普及，缺乏对其生成代码可持续性的研究，特别是其对环境的影响。

Method: 通过分析ChatGPT、BARD和Copilot三种工具生成的代码，评估其是否符合可持续编码实践。

Result: 结果显示这些工具生成的代码普遍不符合可持续编码标准。

Conclusion: 需深入研究并提出有效的改进策略，以提高AI生成代码的环保性。

Abstract: Software sustainability is emerging as a primary concern, aiming to optimize
resource utilization, minimize environmental impact, and promote a greener,
more resilient digital ecosystem. The sustainability or "greenness" of software
is typically determined by the adoption of sustainable coding practices. With a
maturing ecosystem around generative AI, many software developers now rely on
these tools to generate code using natural language prompts. Despite their
potential advantages, there is a significant lack of studies on the
sustainability aspects of AI-generated code. Specifically, how environmentally
friendly is the AI-generated code based upon its adoption of sustainable coding
practices? In this paper, we present the results of an early investigation into
the sustainability aspects of AI-generated code across three popular generative
AI tools - ChatGPT, BARD, and Copilot. The results highlight the default
non-green behavior of tools for generating code, across multiple rules and
scenarios. It underscores the need for further in-depth investigations and
effective remediation strategies.

</details>


### [310] [On The Impact of Merge Request Deviations on Code Review Practices](https://arxiv.org/abs/2506.08860)
*Samah Kansab,Francis Bordeleau,Ali Tizghadam*

Key words: 代码审查、合并请求、偏差检测、机器学习、小样本学习

TL;DR: 论文探讨了代码审查中的偏差现象（如草稿、依赖更新等非审查目的），提出了一个分类系统、基于小样本学习的检测方法，并证明排除这些偏差可以提升机器学习模型的性能。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 工业界的合并请求（MR）工作流常偏离标准审查流程，这些偏差会影响分析结果和机器学习模型的效果。

Method: 提出了七类MR偏差的分类，并开发了一种小样本学习检测方法（准确率91%）。

Result: 排除偏差后，预测审查完成时间的机器学习模型在53.33%的情况下性能提升（最高2.25倍），且特征重要性发生显著变化。

Conclusion: 研究为优化代码审查提供了实用工具，并验证了排除偏差对提升分析可靠性的重要性。

Abstract: Code review is a key practice in software engineering, ensuring quality and
collaboration. However, industrial Merge Request (MR) workflows often deviate
from standardized review processes, with many MRs serving non-review purposes
(e.g., drafts, rebases, or dependency updates). We term these cases deviations
and hypothesize that ignoring them biases analytics and undermines ML models
for review analysis.
  We identify seven deviation categories, occurring in 37.02% of MRs, and
propose a few-shot learning detection method (91% accuracy). By excluding
deviations, ML models predicting review completion time improve performance in
53.33% of cases (up to 2.25x) and exhibit significant shifts in feature
importance (47% overall, 60% top-*k*).
  Our contributions include: (1) a taxonomy of MR deviations, (2) an AI-driven
detection approach, and (3) empirical evidence of their impact on ML-based
review analytics. This work aids practitioners in optimizing review efforts and
ensuring reliable insights.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [311] [Inverse Design in Distributed Circuits Using Single-Step Reinforcement Learning](https://arxiv.org/abs/2506.08029)
*Jiayu Li,Masood Mortazavi,Ning Yan,Yihong Ma,Reza Zafarani*

Key words: 逆设计, 分布式电路, DCIDA, Transformer, 传递函数

TL;DR: 本文提出DCIDA框架，通过联合训练的条件分布生成优化设计采样策略，显著提升复杂传递函数下的设计性能。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 解决分布式电路中逆设计中非可微评估、可变拓扑和连续放置空间的实际需求。

Method: DCIDA框架利用联合训练的条件分布生成设计采样策略，通过单步动作决定所有设计因素。

Result: 实验表明，DCIDA的Transformer策略网络显著减少设计误差，尤其在复杂传递函数中效果更佳。

Conclusion: DCIDA框架在逆设计中提供了高效且灵活的解决方案，显著优于现有方法。

Abstract: The goal of inverse design in distributed circuits is to generate
near-optimal designs that meet a desirable transfer function specification.
Existing design exploration methods use some combination of strategies
involving artificial grids, differentiable evaluation procedures, and specific
template topologies. However, real-world design practices often require
non-differentiable evaluation procedures, varying topologies, and
near-continuous placement spaces. In this paper, we propose DCIDA, a design
exploration framework that learns a near-optimal design sampling policy for a
target transfer function. DCIDA decides all design factors in a compound
single-step action by sampling from a set of jointly-trained conditional
distributions generated by the policy. Utilizing an injective interdependent
``map", DCIDA transforms raw sampled design ``actions" into uniquely equivalent
physical representations, enabling the framework to learn the conditional
dependencies among joint ``raw'' design decisions. Our experiments demonstrate
DCIDA's Transformer-based policy network achieves significant reductions in
design error compared to state-of-the-art approaches, with significantly better
fit in cases involving more complex transfer functions.

</details>


### [312] [Feasibility Study of CNNs and MLPs for Radiation Heat Transfer in 2-D Furnaces with Spectrally Participative Gases](https://arxiv.org/abs/2506.08033)
*Axel TahmasebiMoradi,Vincent Ren,Benjamin Le-Creurer,Chetra Mang*

Key words: CNN, MLP, 辐射热传递, 替代模型, Optuna, ICARUS2D

TL;DR: 该论文通过CNN和MLP建立替代模型，以降低辐射热传递数值模拟的计算成本，并在性能和精度上与经典求解器对比。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 传统数值模拟计算成本高，研究旨在通过深度学习模型替代传统方法，提高效率。

Method: 采用CNN和MLP架构，输入调整为适应图像处理的CNN输入格式，数据集由经典求解器ICARUS2D生成，并使用Optuna优化超参数。

Result: CNN和MLP均实现了显著加速，相对误差在工业可接受范围内，且CNN在精度和稳定性上优于MLP。

Conclusion: CNN在辐射热传递模拟中表现更优，为工业应用提供了高效替代方案。

Abstract: Aiming to reduce the computational cost of numerical simulations, a
convolutional neural network (CNN) and a multi-layer perceptron (MLP) are
introduced to build a surrogate model to approximate radiative heat transfer
solutions in a 2-D walled domain with participative gases. The originality of
this work lays in the adaptation of the inputs of the problem (gas and wall
properties) in order to fit with the CNN architecture, more commonly used for
image processing. Two precision datasets have been created with the classical
solver, ICARUS2D, that uses the discrete transfer radiation method with the
statistical narrow bands model. The performance of the CNN architecture is
compared to a more classical MLP architecture in terms of speed and accuracy.
Thanks to Optuna, all results are obtained using the optimized hyper parameters
networks. The results show a significant speedup with industrially acceptable
relative errors compared to the classical solver for both architectures.
Additionally, the CNN outperforms the MLP in terms of precision and is more
robust and stable to changes in hyper-parameters. A performance analysis on the
dataset size of the samples have also been carried out to gain a deeper
understanding of the model behavior.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [313] [Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts](https://arxiv.org/abs/2506.08048)
*Zheng Han,Jun Zhou,Jialun Pei,Jing Qin,Yingfang Fan,Qi Dou*

Key words: augmented reality, surgical navigation, finite element method, data-driven biomechanics, human-in-the-loop

TL;DR: 提出了一种数据驱动的生物力学算法，结合人机交互机制，提升AR手术导航中的变形建模效率和准确性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决传统有限元方法在手术导航中计算成本高和大解剖变化处理不足的问题。

Method: 结合数据驱动的生物力学算法和外科医生交互提示机制。

Result: 在公开数据集上平均目标配准误差为3.42 mm，交互后降至2.78 mm。

Conclusion: 该框架提升了变形建模的效率和准确性，增强了外科医生与算法的协作。

Abstract: In augmented reality (AR)-guided surgical navigation, preoperative organ
models are superimposed onto the patient's intraoperative anatomy to visualize
critical structures such as vessels and tumors. Accurate deformation modeling
is essential to maintain the reliability of AR overlays by ensuring alignment
between preoperative models and the dynamically changing anatomy. Although the
finite element method (FEM) offers physically plausible modeling, its high
computational cost limits intraoperative applicability. Moreover, existing
algorithms often fail to handle large anatomical changes, such as those induced
by pneumoperitoneum or ligament dissection, leading to inaccurate anatomical
correspondences and compromised AR guidance. To address these challenges, we
propose a data-driven biomechanics algorithm that preserves FEM-level accuracy
while improving computational efficiency. In addition, we introduce a novel
human-in-the-loop mechanism into the deformation modeling process. This enables
surgeons to interactively provide prompts to correct anatomical misalignments,
thereby incorporating clinical expertise and allowing the model to adapt
dynamically to complex surgical scenarios. Experiments on a publicly available
dataset demonstrate that our algorithm achieves a mean target registration
error of 3.42 mm. Incorporating surgeon prompts through the interactive
framework further reduces the error to 2.78 mm, surpassing state-of-the-art
methods in volumetric accuracy. These results highlight the ability of our
framework to deliver efficient and accurate deformation modeling while
enhancing surgeon-algorithm collaboration, paving the way for safer and more
reliable computer-assisted surgeries.

</details>


### [314] [IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation](https://arxiv.org/abs/2506.08137)
*Oishee Bintey Hoque,Abhijin Adiga,Aniruddha Adiga,Siddharth Chaudhary,Madhav V. Marathe,S. S. Ravi,Kirti Rajagopalan,Amanda Wilson,Samarth Swarup*

Key words: 运河网络、语义分割、图优化、IGraSS、遥感图像

TL;DR: 提出了一种结合语义分割和图优化的迭代框架IGraSS，用于改进不完整的地面真实数据并精确绘制运河网络。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有基础设施（如运河）的语义分割模型依赖于大量标注数据，但地面真实数据的不完整会影响模型性能。利用基础设施的图结构特性（如可达性或连通性）可以改进地面真实数据。

Method: 开发了IGraSS框架，结合RGB和多模态数据（如NDWI、DEM）的语义分割模块与基于图的真实数据优化模块。分割模块处理卫星图像，优化模块将基础设施视为图进行全局优化。

Result: IGraSS将不可达运河段从约18%减少到3%，且使用优化后的地面真实数据显著提高了运河识别的准确性。框架还可推广到其他基础设施（如道路网络）。

Conclusion: IGraSS是一个强大的框架，既能优化噪声地面真实数据，又能从遥感图像中精确绘制基础设施网络。

Abstract: Accurate canal network mapping is essential for water management, including
irrigation planning and infrastructure maintenance. State-of-the-art semantic
segmentation models for infrastructure mapping, such as roads, rely on large,
well-annotated remote sensing datasets. However, incomplete or inadequate
ground truth can hinder these learning approaches. Many infrastructure networks
have graph-level properties such as reachability to a source (like canals) or
connectivity (roads) that can be leveraged to improve these existing ground
truth. This paper develops a novel iterative framework IGraSS, combining a
semantic segmentation module-incorporating RGB and additional modalities (NDWI,
DEM)-with a graph-based ground-truth refinement module. The segmentation module
processes satellite imagery patches, while the refinement module operates on
the entire data viewing the infrastructure network as a graph. Experiments show
that IGraSS reduces unreachable canal segments from around 18% to 3%, and
training with refined ground truth significantly improves canal identification.
IGraSS serves as a robust framework for both refining noisy ground truth and
mapping canal networks from remote sensing imagery. We also demonstrate the
effectiveness and generalizability of IGraSS using road networks as an example,
applying a different graph-theoretic constraint to complete road networks.

</details>


### [315] [Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework](https://arxiv.org/abs/2506.08185)
*Huixin Zhan,Jason H. Moore*

Key words: 个性化指纹识别, 外科手术AI, 多模态学习, 隐私保护, 手势预测

TL;DR: 提出了一种基于离散扩散框架和视觉语言动作（VLA）的新型方法，用于建模外科医生的个性化手术风格，并通过自然语言提示实现隐私保护的外科医生指纹识别。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有AI系统常忽略外科医生的个性化风格差异，本研究旨在通过多模态输入建模外科医生的细粒度个性化特征。

Method: 采用离散扩散框架与VLA管道结合，通过内窥镜视频、手术意图语言和隐私感知的医生身份嵌入，将手势预测建模为结构化序列去噪任务。

Result: 在JIGSAWS数据集上验证，模型能准确重建手势序列并学习每位外科医生独特的运动指纹，但更强的个性化嵌入会增加身份泄露风险。

Conclusion: 个性化嵌入提升性能但也增加隐私风险，需在手术建模中平衡个性化与隐私保护。

Abstract: Surgeons exhibit distinct operating styles due to differences in training,
experience, and motor behavior - yet current AI systems often ignore this
personalization signal. We propose a novel approach to model fine-grained,
surgeon-specific fingerprinting in robotic surgery using a discrete diffusion
framework integrated with a vision-language-action (VLA) pipeline. Our method
formulates gesture prediction as a structured sequence denoising task,
conditioned on multimodal inputs including endoscopic video, surgical intent
language, and a privacy-aware embedding of surgeon identity and skill.
Personalized surgeon fingerprinting is encoded through natural language prompts
using third-party language models, allowing the model to retain individual
behavioral style without exposing explicit identity. We evaluate our method on
the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture
sequences while learning meaningful motion fingerprints unique to each surgeon.
To quantify the privacy implications of personalization, we perform membership
inference attacks and find that more expressive embeddings improve task
performance but simultaneously increase susceptibility to identity leakage.
These findings demonstrate that while personalized embeddings improve
performance, they also increase vulnerability to identity leakage, revealing
the importance of balancing personalization with privacy risk in surgical
modeling. Code is available at:
https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.

</details>


### [316] [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)
*Andrew Z. Wang,Songwei Ge,Tero Karras,Ming-Yu Liu,Yogesh Balaji*

Key words: 文本到图像生成, LLM, 扩散模型, 文本编码器

TL;DR: 研究了将现代仅解码器LLM作为文本编码器用于文本到图像扩散模型的效果，发现传统最后层嵌入效果较差，跨层归一化平均显著提升对齐性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探索利用现代LLM提升文本到图像生成模型性能。

Method: 训练27个模型，分析12种文本编码器，考察嵌入提取方法、LLM变体和模型规模的影响。

Result: 跨层归一化平均嵌入在复杂提示对齐上表现更优，多数LLM超越T5基线。

Conclusion: LLM作为文本编码器可提升图像生成质量，尤其在高级视觉语言推理任务中。

Abstract: Both text-to-image generation and large language models (LLMs) have made
significant advancements. However, many text-to-image models still employ the
somewhat outdated T5 and CLIP as their text encoders. In this work, we
investigate the effectiveness of using modern decoder-only LLMs as text
encoders for text-to-image diffusion models. We build a standardized training
and evaluation pipeline that allows us to isolate and evaluate the effect of
different text embeddings. We train a total of 27 text-to-image models with 12
different text encoders to analyze the critical aspects of LLMs that could
impact text-to-image generation, including the approaches to extract
embeddings, different LLMs variants, and model sizes. Our experiments reveal
that the de facto way of using last-layer embeddings as conditioning leads to
inferior performance. Instead, we explore embeddings from various layers and
find that using layer-normalized averaging across all layers significantly
improves alignment with complex prompts. Most LLMs with this conditioning
outperform the baseline T5 model, showing enhanced performance in advanced
visio-linguistic reasoning skills.

</details>


### [317] [Highly Compressed Tokenizer Can Generate Without Training](https://arxiv.org/abs/2506.08257)
*L. Lao Beyer,T. Li,X. Chen,S. Karaman,K. He*

Key words: 1D图像标记化,图像编辑,生成模型,向量量化,CLIP

TL;DR: 该论文研究了1D图像标记化方法，通过高度压缩的一维序列实现图像编辑和生成功能，无需训练生成模型。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探索1D图像标记化方法在图像编辑和生成中的潜力，尤其是通过简单的标记操作实现精细编辑。

Method: 利用1D标记化空间，结合梯度优化的测试时标记调整和插件式损失函数（如重建或CLIP相似性）。

Result: 展示了修复和文本引导图像编辑的成功应用，无需训练生成模型即可生成多样且真实的样本。

Conclusion: 1D标记化方法为图像编辑和生成提供了高效且灵活的新途径。

Abstract: Commonly used image tokenizers produce a 2D grid of spatially arranged
tokens. In contrast, so-called 1D image tokenizers represent images as highly
compressed one-dimensional sequences of as few as 32 discrete tokens. We find
that the high degree of compression achieved by a 1D tokenizer with vector
quantization enables image editing and generative capabilities through
heuristic manipulation of tokens, demonstrating that even very crude
manipulations -- such as copying and replacing tokens between latent
representations of images -- enable fine-grained image editing by transferring
appearance and semantic attributes. Motivated by the expressivity of the 1D
tokenizer's latent space, we construct an image generation pipeline leveraging
gradient-based test-time optimization of tokens with plug-and-play loss
functions such as reconstruction or CLIP similarity. Our approach is
demonstrated for inpainting and text-guided image editing use cases, and can
generate diverse and realistic samples without requiring training of any
generative model.

</details>


### [318] [Seeing Voices: Generating A-Roll Video from Audio with Mirage](https://arxiv.org/abs/2506.08279)
*Aditi Sundararaman,Amogh Adishesha,Andrew Jaegle,Dan Bigioi,Hyoung-Kyu Song,Jon Kyl,Justin Mao,Kevin Lan,Mojtaba Komeili,ShahRukh Athar,Sheila Babayan,Stanislau Beliasau,William Buchwalter*

Key words: Mirage, audio-to-video, foundation model, self-attention, speech synthesis

TL;DR: Mirage是一种音频到视频基础模型，能够从音频输入生成逼真、富有表现力的视频，尤其在语音条件下生成人物说话的动态视频。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前视频生成方法要么忽略音频仅生成无声图像序列，要么局限于特定应用领域（如重新配音）。Mirage旨在通过音频输入生成高质量的视觉内容，实现音视频的和谐统一。

Method: Mirage采用基于自注意力的统一训练方法，可以从头训练或基于现有权重，无需针对音频的特定架构或损失组件。

Result: Mirage生成的结果优于其他结合音频特定架构或损失组件的方法，尤其是在语音条件下生成的人物动态视频更加逼真。

Conclusion: Mirage作为一种通用的音频到视频生成方法，展示了在音视频整合方面的卓越表现，尤其在语音驱动的视频生成中表现突出。

Abstract: From professional filmmaking to user-generated content, creators and
consumers have long recognized that the power of video depends on the
harmonious integration of what we hear (the video's audio track) with what we
see (the video's image sequence). Current approaches to video generation either
ignore sound to focus on general-purpose but silent image sequence generation
or address both visual and audio elements but focus on restricted application
domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation
model that excels at generating realistic, expressive output imagery from
scratch given an audio input. When integrated with existing methods for speech
synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal
video. When trained on audio-video footage of people talking (A-roll) and
conditioned on audio containing speech, Mirage generates video of people
delivering a believable interpretation of the performance implicit in input
audio. Our central technical contribution is a unified method for training
self-attention-based audio-to-video generation models, either from scratch or
given existing weights. This methodology allows Mirage to retain generality as
an approach to audio-to-video generation while producing outputs of superior
subjective quality to methods that incorporate audio-specific architectures or
loss components specific to people, speech, or details of how images or audio
are captured. We encourage readers to watch and listen to the results of Mirage
for themselves (see paper and comments for links).

</details>


### [319] [SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging](https://arxiv.org/abs/2506.08297)
*Nhat Thanh Tran,Fanghui Xue,Shuai Zhang,Jiancheng Lyu,Yunling Zheng,Yingyong Qi,Jack Xin*

Key words: 广义注意力, 线性注意力, SEMA, 图像分类

TL;DR: 论文提出了一种名为SEMA的新型注意力机制，通过结合局部化和全局特征，解决了传统线性注意力无法聚焦的问题，并在图像分类任务中表现优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决传统全注意力机制的二次计算复杂度和线性注意力机制无法聚焦的问题。

Method: 提出广义注意力的数学定义，设计SEMA方法，结合局部化和算术平均以兼顾聚焦与全局特性。

Result: 在Imagenet-1k上，SEMA表现出色，优于其他视觉Mamba模型，尤其在更大尺度的图像上。

Conclusion: SEMA是一种可扩展且高效的注意力机制替代方案。

Abstract: Attention is the critical component of a transformer. Yet the quadratic
computational complexity of vanilla full attention in the input size and the
inability of its linear attention variant to focus have been challenges for
computer vision tasks. We provide a mathematical definition of generalized
attention and formulate both vanilla softmax attention and linear attention
within the general framework. We prove that generalized attention disperses,
that is, as the number of keys tends to infinity, the query assigns equal
weights to all keys. Motivated by the dispersion property and recent
development of Mamba form of attention, we design Scalable and Efficient Mamba
like Attention (SEMA) which utilizes token localization to avoid dispersion and
maintain focusing, complemented by theoretically consistent arithmetic
averaging to capture global aspect of attention. We support our approach on
Imagenet-1k where classification results show that SEMA is a scalable and
effective alternative beyond linear attention, outperforming recent vision
Mamba models on increasingly larger scales of images at similar model parameter
sizes.

</details>


### [320] [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](https://arxiv.org/abs/2506.08351)
*Huixuan Zhang,Junzhe Zhang,Xiaojun Wan*

Key words: 文本到视觉生成、扩散模型、分类器无关引导、自适应引导、Step AG

TL;DR: 提出了一种名为Step AG的自适应引导策略，通过在去噪初期应用分类器无关引导，显著提升了生成速度，同时保持图像质量和文本对齐性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决分类器无关引导方法在生成过程中需要两倍步骤导致的高成本问题，同时缺乏普适性分析。

Method: 提出Step AG策略，限制分类器无关引导在去噪的初期步骤中使用。

Result: 在图像质量与文本对齐性上表现良好，平均提速20%到30%，且适用于不同模型设置。

Conclusion: Step AG是一种简单且通用的自适应引导策略，显著提升了生成效率。

Abstract: With the rapid development of text-to-vision generation diffusion models,
classifier-free guidance has emerged as the most prevalent method for
conditioning. However, this approach inherently requires twice as many steps
for model forwarding compared to unconditional generation, resulting in
significantly higher costs. While previous study has introduced the concept of
adaptive guidance, it lacks solid analysis and empirical results, making
previous method unable to be applied to general diffusion models. In this work,
we present another perspective of applying adaptive guidance and propose Step
AG, which is a simple, universally applicable adaptive guidance strategy. Our
evaluations focus on both image quality and image-text alignment. whose results
indicate that restricting classifier-free guidance to the first several
denoising steps is sufficient for generating high-quality, well-conditioned
images, achieving an average speedup of 20% to 30%. Such improvement is
consistent across different settings such as inference steps, and various
models including video generation models, highlighting the superiority of our
method.

</details>


### [321] [CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics](https://arxiv.org/abs/2506.08835)
*Shravan Nayak,Mehar Bhatia,Xiaofeng Zhang,Verena Rieser,Lisa Anne Hendricks,Sjoerd van Steenkiste,Yash Goyal,Karolina Stańczak,Aishwarya Agrawal*

Key words: 文本到图像模型，文化表示，评估基准，视觉生成

TL;DR: 该研究首次系统量化了文本到图像（T2I）模型在文化和隐式期望上的表现，揭示了模型在满足文化和显式期望上的严重不足。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 研究动机是评估T2I模型在多样文化上下文中的表现，填补了文化表示评估的空白。

Method: 研究者开发了CulturalFrames基准，覆盖10个国家和5个社会文化领域，包含983个提示、3637张生成图像和10k多人工标注。

Result: 研究发现T2I模型在显式期望上失败率为68%，隐式期望为49%，现有评估指标与人类判断相关性差。

Conclusion: 研究暴露了T2I模型在文化表示上的关键差距，为开发更具文化意识的模型和方法提供方向。

Abstract: The increasing ubiquity of text-to-image (T2I) models as tools for visual
content generation raises concerns about their ability to accurately represent
diverse cultural contexts. In this work, we present the first study to
systematically quantify the alignment of T2I models and evaluation metrics with
respect to both explicit as well as implicit cultural expectations. To this
end, we introduce CulturalFrames, a novel benchmark designed for rigorous human
evaluation of cultural representation in visual generations. Spanning 10
countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,
3637 corresponding images generated by 4 state-of-the-art T2I models, and over
10k detailed human annotations. We find that T2I models not only fail to meet
the more challenging implicit expectations but also the less challenging
explicit expectations. Across models and countries, cultural expectations are
missed an average of 44% of the time. Among these failures, explicit
expectations are missed at a surprisingly high average rate of 68%, while
implicit expectation failures are also significant, averaging 49%. Furthermore,
we demonstrate that existing T2I evaluation metrics correlate poorly with human
judgments of cultural alignment, irrespective of their internal reasoning.
Collectively, our findings expose critical gaps, providing actionable
directions for developing more culturally informed T2I models and evaluation
methodologies.

</details>


### [322] [Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions](https://arxiv.org/abs/2506.08927)
*David Acuna,Ximing Lu,Jaehun Jung,Hyunwoo Kim,Amlan Kar,Sanja Fidler,Yejin Choi*

Key words: 视觉语言模型, 蒙特卡洛树搜索, 推理链条, 隐含知识, 非训练方法

TL;DR: 提出了一种基于蒙特卡洛树搜索的算法，用于在未经过额外训练的非推理视觉语言模型中激发隐含知识并生成长推理链条。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探索是否可以在无需额外训练的情况下，利用搜索机制从已部署的非推理视觉语言模型中提取隐含知识和生成长推理链条。

Method: 采用蒙特卡洛树搜索（MCTS）启发式算法，通过在模型输出流中注入子问题-子答案对，将推理过程视为搜索问题。

Result: 在三个基准测试中观察到一致改进，特别是在MMMU-PRO上总体性能提升2%，其中Liberal Arts类别提升9%。

Conclusion: 该方法证明了通过搜索机制可以在非推理模型中激发隐含知识并实现长推理链条，无需额外训练。

Abstract: Recent research in vision-language models (VLMs) has centered around the
possibility of equipping them with implicit long-form chain-of-thought
reasoning -- akin to the success observed in language models -- via
distillation and reinforcement learning. But what about the non-reasoning
models already trained and deployed across the internet? Should we simply
abandon them, or is there hope for a search mechanism that can elicit hidden
knowledge and induce long reasoning traces -- without any additional training
or supervision? In this paper, we explore this possibility using a Monte Carlo
Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer
pairs into the model's output stream. We show that framing reasoning as a
search process -- where subquestions act as latent decisions within a broader
inference trajectory -- helps the model "connect the dots" between fragmented
knowledge and produce extended reasoning traces in non-reasoning models. We
evaluate our method across three benchmarks and observe consistent
improvements. Notably, our approach yields a 2% overall improvement on
MMMU-PRO, including a significant 9% gain in Liberal Arts.

</details>


### [323] [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/abs/2506.09040)
*Dianyi Wang,Wei Song,Yikun Wang,Siyuan Wang,Kaicheng Yu,Zhongyu Wei,Jiaqi Wang*

Key words: 视觉语言模型, 自回归学习, 多模态理解, 语义表示, ASVR

TL;DR: ASVR提出了一种联合视觉和文本模态的自回归框架，通过重建图像的语义表示来提升多模态理解能力，显著提高了多个基准测试的性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前大型视觉语言模型仅对文本序列进行自回归监督，未充分利用视觉模态，导致无法处理无标注图像、遗漏视觉细节及无法传达某些视觉内容的问题。

Method: 提出了自回归语义视觉重建（ASVR），在统一的自回归框架中联合学习视觉和文本模态，探索语义表示的重建而非原始图像的重建。

Result: ASVR在多个多模态理解基准测试中表现出稳定的性能提升，尤其显著提高了LLaVA-1.5的得分。

Conclusion: 通过自回归重建语义表示而非原始图像，可以更有效地提升多模态理解能力。

Abstract: Typical large vision-language models (LVLMs) apply autoregressive supervision
solely to textual sequences, without fully incorporating the visual modality
into the learning process. This results in three key limitations: (1) an
inability to utilize images without accompanying captions, (2) the risk that
captions omit critical visual details, and (3) the challenge that certain
vision-centric content cannot be adequately conveyed through text. As a result,
current LVLMs often prioritize vision-to-language alignment while potentially
overlooking fine-grained visual information. While some prior works have
explored autoregressive image generation, effectively leveraging autoregressive
visual supervision to enhance image understanding remains an open challenge. In
this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),
which enables joint learning of visual and textual modalities within a unified
autoregressive framework. We show that autoregressively reconstructing the raw
visual appearance of images does not enhance and may even impair multimodal
understanding. In contrast, autoregressively reconstructing the semantic
representation of images consistently improves comprehension. Notably, we find
that even when models are given continuous image features as input, they can
effectively reconstruct discrete semantic tokens, resulting in stable and
consistent improvements across a wide range of multimodal understanding
benchmarks. Our approach delivers significant performance gains across varying
data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves
LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is
available at https://github.com/AlenjandroWang/ASVR.

</details>


### [324] [MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding](https://arxiv.org/abs/2506.08512)
*Zhiyi Zhu,Xiaoyu Wu,Zihao Liu,Linlin Yang*

Key words: 视频时序定位、MambaAligner、LLMRefiner、多模态对齐、Transformer

TL;DR: MLVTG框架通过MambaAligner和LLMRefiner模块解决了视频时序定位任务中的冗余注意力和多模态对齐问题，实现了更精确的定位。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的基于Transformer的视频时序定位方法存在冗余注意力和多模态对齐不足的问题，需要更高效的解决方案。

Method: 提出MLVTG框架，结合MambaAligner（使用Vision Mamba块替代Transformer）和LLMRefiner（利用预训练LLM的冻结层）实现双重对齐策略。

Result: 在QVHighlights、Charades-STA和TVSum数据集上，MLVTG表现优于现有基线方法，达到最佳性能。

Conclusion: MLVTG通过创新的双重对齐策略显著提升了视频时序定位的准确性。

Abstract: Video Temporal Grounding (VTG), which aims to localize video clips
corresponding to natural language queries, is a fundamental yet challenging
task in video understanding. Existing Transformer-based methods often suffer
from redundant attention and suboptimal multi-modal alignment. To address these
limitations, we propose MLVTG, a novel framework that integrates two key
modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba
blocks as a backbone instead of Transformers to model temporal dependencies and
extract robust video representations for multi-modal alignment. LLMRefiner
leverages the specific frozen layer of a pre-trained Large Language Model (LLM)
to implicitly transfer semantic priors, enhancing multi-modal alignment without
fine-tuning. This dual alignment strategy, temporal modeling via structured
state-space dynamics and semantic purification via textual priors, enables more
precise localization. Extensive experiments on QVHighlights, Charades-STA, and
TVSum demonstrate that MLVTG achieves state-of-the-art performance and
significantly outperforms existing baselines.

</details>


### [325] [TrajFlow: Multi-modal Motion Prediction via Flow Matching](https://arxiv.org/abs/2506.08541)
*Qi Yan,Brian Zhang,Yutong Zhang,Daniel Yang,Joshua White,Di Chen,Jiachao Liu,Langechuan Liu,Binnan Zhuang,Shaoshuai Shi,Renjie Liao*

Key words: 自动驾驶，运动预测，流匹配，多模态预测

TL;DR: TrajFlow是一种基于流匹配的运动预测框架，用于自动驾驶中的多模态轨迹预测，显著减少计算开销并保持预测一致性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 在动态现实条件下，高效准确的运动预测对自动驾驶的安全性和决策至关重要，需要多模态预测方法。

Method: TrajFlow采用流匹配框架，单次推理预测多轨迹；提出基于Plackett-Luce分布的排序损失和改进的自条件训练技术。

Result: 在大规模Waymo数据集上，TrajFlow在多项关键指标上实现最佳性能，验证了其有效性。

Conclusion: TrajFlow为安全关键型自动驾驶应用提供了一种高效且准确的运动预测解决方案。

Abstract: Efficient and accurate motion prediction is crucial for ensuring safety and
informed decision-making in autonomous driving, particularly under dynamic
real-world conditions that necessitate multi-modal forecasts. We introduce
TrajFlow, a novel flow matching-based motion prediction framework that
addresses the scalability and efficiency challenges of existing generative
trajectory prediction methods. Unlike conventional generative approaches that
employ i.i.d. sampling and require multiple inference passes to capture diverse
outcomes, TrajFlow predicts multiple plausible future trajectories in a single
pass, significantly reducing computational overhead while maintaining coherence
across predictions. Moreover, we propose a ranking loss based on the
Plackett-Luce distribution to improve uncertainty estimation of predicted
trajectories. Additionally, we design a self-conditioning training technique
that reuses the model's own predictions to construct noisy inputs during a
second forward pass, thereby improving generalization and accelerating
inference. Extensive experiments on the large-scale Waymo Open Motion Dataset
(WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across
various key metrics, underscoring its effectiveness for safety-critical
autonomous driving applications. The code and other details are available on
the project website https://traj-flow.github.io/.

</details>


### [326] [Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems](https://arxiv.org/abs/2506.08596)
*Guyang Zhang,Waleed Abdulla*

Key words: Transformer, Hyperspectral Imaging, Classification, Survey

TL;DR: 对Transformer在HSI分类中的应用进行了首次端到端调查，总结了关键设计选择和未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Transformer在HSI中的应用仍处于起步阶段，希望通过系统梳理指导研究者选择合适的解决方案。

Method: 综述了300多篇论文，从预处理到损失设计的各个环节进行分类和对比。

Result: 指出了当前研究中的主要挑战，如数据稀缺、计算复杂性等，并提出了未来的研究议程。

Conclusion: Transformer在HSI中具有潜力，但仍需解决数据、计算和解释性等问题。

Abstract: Transformers have become the architecture of choice for learning long-range
dependencies, yet their adoption in hyperspectral imaging (HSI) is still
emerging. We reviewed more than 300 papers published up to 2025 and present the
first end-to-end survey dedicated to Transformer-based HSI classification. The
study categorizes every stage of a typical pipeline-pre-processing, patch or
pixel tokenization, positional encoding, spatial-spectral feature extraction,
multi-head self-attention variants, skip connections, and loss design-and
contrasts alternative design choices with the unique spatial-spectral
properties of HSI. We map the field's progress against persistent obstacles:
scarce labeled data, extreme spectral dimensionality, computational overhead,
and limited model explainability. Finally, we outline a research agenda
prioritizing valuable public data sets, lightweight on-edge models,
illumination and sensor shifts robustness, and intrinsically interpretable
attention mechanisms. Our goal is to guide researchers in selecting, combining,
or extending Transformer components that are truly fit for purpose for
next-generation HSI applications.

</details>


### [327] [ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network](https://arxiv.org/abs/2506.08629)
*Feixiang Du,Shengkun Wu*

Key words: 语义分割, CNN, Mamba, 轻量级网络, 特征融合

TL;DR: 论文提出了一种轻量级高效的CNN-Mamba网络（ECMNet），用于语义分割任务，通过结合CNN和Mamba模型的优势，解决了全局上下文建模不足的问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 尽管CNN和Transformer模型在语义分割任务中表现优异，但全局上下文建模仍不足。Mamba在视觉任务中显示出长距离依赖建模的优势，因此论文提出结合CNN和Mamba的轻量级网络ECMNet。

Method: 设计了增强双注意力块（EDAB）作为轻量级瓶颈，多尺度注意力单元（MSAU）用于特征表示增强，以及Mamba增强的特征融合模块（FFM）用于多级特征融合。

Result: 在Cityscapes和CamVid数据集上分别达到70.6%和73.6%的mIoU，参数量为0.87M，计算量为8.27G FLOPs。

Conclusion: ECMNet在准确性和效率之间取得了良好平衡，验证了CNN与Mamba结合的有效性。

Abstract: In the past decade, Convolutional Neural Networks (CNNs) and Transformers
have achieved wide applicaiton in semantic segmentation tasks. Although CNNs
with Transformer models greatly improve performance, the global context
modeling remains inadequate. Recently, Mamba achieved great potential in vision
tasks, showing its advantages in modeling long-range dependency. In this paper,
we propose a lightweight Efficient CNN-Mamba Network for semantic segmentation,
dubbed as ECMNet. ECMNet combines CNN with Mamba skillfully in a capsule-based
framework to address their complementary weaknesses. Specifically, We design a
Enhanced Dual-Attention Block (EDAB) for lightweight bottleneck. In order to
improve the representations ability of feature, We devise a Multi-Scale
Attention Unit (MSAU) to integrate multi-scale feature aggregation, spatial
aggregation and channel aggregation. Moreover, a Mamba enhanced Feature Fusion
Module (FFM) merges diverse level feature, significantly enhancing segmented
accuracy. Extensive experiments on two representative datasets demonstrate that
the proposed model excels in accuracy and efficiency balance, achieving 70.6%
mIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M
parameters and 8.27G FLOPs on a single RTX 3090 GPU platform.

</details>


### [328] [Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces](https://arxiv.org/abs/2506.08729)
*Dieuwertje Alblas,Patryk Rygiel,Julian Suk,Kaj O. Kappe,Marieke Hofman,Christoph Brune,Kak Khee Yeung,Jelmer M. Wolterink*

Key words: 腹主动脉瘤, SE(3)对称变换器, 个性化监测, 生长预测, CTA扫描

TL;DR: 本文提出了一种基于SE(3)对称变换器的模型，用于预测腹主动脉瘤（AAA）的生长，通过保留血管表面的解剖结构和几何保真度，实现了个性化监测。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: AAA的现有监测方法仅基于最大直径，忽略了3D形状与生长的复杂关系，导致标准化监测间隔不适用。个性化生长预测可改进监测策略。

Method: 使用SE(3)对称变换器模型，直接在血管模型表面上预测AAA生长，结合局部多物理特征。训练数据为24名患者的113次CTA扫描。

Result: 模型预测AAA生长的中位直径误差为1.18 mm，能识别两年内需手术修复的患者（准确率0.93），并在外部验证集上表现良好。

Conclusion: 局部定向AAA生长预测可行，可支持个性化监测策略。

Abstract: Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the
abdominal aorta. AAAs may rupture, with a survival rate of only 20\%. Current
clinical guidelines recommend elective surgical repair when the maximum AAA
diameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet
these criteria are periodically monitored, with surveillance intervals based on
the maximum AAA diameter. However, this diameter does not take into account the
complex relation between the 3D AAA shape and its growth, making standardized
intervals potentially unfit. Personalized AAA growth predictions could improve
monitoring strategies. We propose to use an SE(3)-symmetric transformer model
to predict AAA growth directly on the vascular model surface enriched with
local, multi-physical features. In contrast to other works which have
parameterized the AAA shape, this representation preserves the vascular
surface's anatomical structure and geometric fidelity. We train our model using
a longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24
AAA patients at irregularly sampled intervals. After training, our model
predicts AAA growth to the next scan moment with a median diameter error of
1.18 mm. We further demonstrate our model's utility to identify whether a
patient will become eligible for elective repair within two years (acc = 0.93).
Finally, we evaluate our model's generalization on an external validation set
consisting of 25 CTAs from 7 AAA patients from a different hospital. Our
results show that local directional AAA growth prediction from the vascular
surface is feasible and may contribute to personalized surveillance strategies.

</details>


### [329] [Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning](https://arxiv.org/abs/2506.08854)
*Junzhuo Liu,Markus Eckstein,Zhixiang Wang,Friedrich Feuerhake,Dorit Merhof*

Key words: 空间转录组学,对比学习,深度学习,Pearson相关系数,基因表达预测

TL;DR: 本研究提出了一种基于对比学习的深度学习方法，从全切片图像预测空间转录组数据，显著提升了基因表达预测的准确性，并展现出在癌症组织定位中的潜力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 空间转录组技术成本高且数据获取困难，研究旨在通过深度学习技术直接从全切片图像预测基因表达，降低成本并提供临床诊断支持。

Method: 采用对比学习框架的深度学习方法，从全切片图像预测空间转录组数据，并在六种疾病数据集上测试性能。

Result: 相比现有方法，预测高表达基因、高变基因和标记基因的Pearson相关系数分别提升了6.27%、6.11%和11.26%，同时能保留基因间相关性。

Conclusion: 方法适用于小样本数据集，且在癌症组织定位中展现出潜力，为空间转录组数据的高效获取提供了新途径。

Abstract: Spatial transcriptomics is a technology that captures gene expression levels
at different spatial locations, widely used in tumor microenvironment analysis
and molecular profiling of histopathology, providing valuable insights into
resolving gene expression and clinical diagnosis of cancer. Due to the high
cost of data acquisition, large-scale spatial transcriptomics data remain
challenging to obtain. In this study, we develop a contrastive learning-based
deep learning method to predict spatially resolved gene expression from
whole-slide images. Evaluation across six different disease datasets
demonstrates that, compared to existing studies, our method improves Pearson
Correlation Coefficient (PCC) in the prediction of highly expressed genes,
highly variable genes, and marker genes by 6.27%, 6.11%, and 11.26%
respectively. Further analysis indicates that our method preserves gene-gene
correlations and applies to datasets with limited samples. Additionally, our
method exhibits potential in cancer tissue localization based on biomarker
expression.

</details>


### [330] [Product of Experts for Visual Generation](https://arxiv.org/abs/2506.08894)
*Yunzhi Zhang,Carson Murtuza-Lanier,Zizhang Li,Yilun Du,Jiajun Wu*

Key words: 专家乘积,知识组合,异构模型,图像合成,视频合成

TL;DR: 提出了一个基于专家乘积（PoE）的框架，通过训练无关的异构模型在推理时进行知识组合，提高图像和视频合成的可控性和灵活性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 整合来自视觉生成模型、视觉语言模型以及人类知识源（如图形引擎和物理模拟器）的多样化知识，目前尚未充分探索。

Method: 采用专家乘积框架，通过退火重要性采样（AIS）从异构模型的产品分布中进行采样。

Result: 在图像和视频合成任务中展现出更好的可控性，并提供灵活的用户界面以指定生成目标。

Conclusion: 该训练无关的框架有效整合了多样化的知识源，提升了生成任务的性能。

Abstract: Modern neural models capture rich priors and have complementary knowledge
over shared data domains, e.g., images and videos. Integrating diverse
knowledge from multiple sources -- including visual generative models, visual
language models, and sources with human-crafted knowledge such as graphics
engines and physics simulators -- remains under-explored. We propose a Product
of Experts (PoE) framework that performs inference-time knowledge composition
from heterogeneous models. This training-free approach samples from the product
distribution across experts via Annealed Importance Sampling (AIS). Our
framework shows practical benefits in image and video synthesis tasks, yielding
better controllability than monolithic methods and additionally providing
flexible user interfaces for specifying visual generation goals.

</details>


### [331] [Inherently Faithful Attention Maps for Vision Transformers](https://arxiv.org/abs/2506.08915)
*Ananthu Aniraj,Cassio F. Dantas,Dino Ienco,Diego Marcos*

Key words: 注意力机制、二进制掩码、两阶段框架、鲁棒性、分布外背景

TL;DR: 提出了一种基于注意力机制的两阶段框架，通过二进制注意力掩码确保只有关注的图像区域影响预测，提高对虚假关联和分布外背景的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决对象感知中上下文导致的偏见问题，特别是在分布外背景下，同时满足需要识别相关区域的图像级任务需求。

Method: 采用两阶段框架：阶段1处理完整图像以发现对象部分和任务相关区域；阶段2利用输入注意力掩码将感受野限制在相关区域，过滤虚假信息。

Result: 在多种基准测试中广泛实验，显著提升了对虚假关联和分布外背景的鲁棒性。

Conclusion: 该框架通过联合训练两阶段任务，有效提升了对象感知的准确性和鲁棒性。

Abstract: We introduce an attention-based method that uses learned binary attention
masks to ensure that only attended image regions influence the prediction.
Context can strongly affect object perception, sometimes leading to biased
representations, particularly when objects appear in out-of-distribution
backgrounds. At the same time, many image-level object-centric tasks require
identifying relevant regions, often requiring context. To address this
conundrum, we propose a two-stage framework: stage 1 processes the full image
to discover object parts and identify task-relevant regions, while stage 2
leverages input attention masking to restrict its receptive field to these
regions, enabling a focused analysis while filtering out potentially spurious
information. Both stages are trained jointly, allowing stage 2 to refine stage
1. Extensive experiments across diverse benchmarks demonstrate that our
approach significantly improves robustness against spurious correlations and
out-of-distribution backgrounds.

</details>


### [332] [Segment Concealed Objects with Incomplete Supervision](https://arxiv.org/abs/2506.08955)
*Chunming He,Kai Li,Yachao Zhang,Ziyun Yang,Youwei Pang,Longxiang Tang,Chengyu Fang,Yulun Zhang,Linghe Kong,Xiu Li,Sina Farsiu*

Key words: 不完全监督、隐蔽目标分割、伪标签生成、特征分组、SEE框架

TL;DR: 本文提出了一种统一的不完全监督隐蔽目标分割（ISCOS）方法SEE，通过伪标签生成和特征分组解决不完全监督和目标与背景相似性问题，实验表明其性能卓越且可扩展。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 由于不完全标注数据和隐蔽目标与背景的相似性，ISCOS任务极具挑战性，现有方法难以有效解决。

Method: 提出SEE框架：利用SAM生成伪标签，并通过策略优化伪标签质量和监督；设计多粒度特征分组模块提升分割一致性。

Result: 实验验证了SEE在多个ISCOS任务中的优越性能，并可作为插件提升现有模型。

Conclusion: SEE成功解决了不完全监督和相似性问题，为ISCOS提供了高效解决方案。

Abstract: Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves
segmenting objects that seamlessly blend into their surrounding environments,
utilizing incompletely annotated data, such as weak and semi-annotations, for
model training. This task remains highly challenging due to (1) the limited
supervision provided by the incompletely annotated training data, and (2) the
difficulty of distinguishing concealed objects from the background, which
arises from the intrinsic similarities in concealed scenarios. In this paper,
we introduce the first unified method for ISCOS to address these challenges. To
tackle the issue of incomplete supervision, we propose a unified mean-teacher
framework, SEE, that leverages the vision foundation model, ``\emph{Segment
Anything Model (SAM)}'', to generate pseudo-labels using coarse masks produced
by the teacher model as prompts. To mitigate the effect of low-quality
segmentation masks, we introduce a series of strategies for pseudo-label
generation, storage, and supervision. These strategies aim to produce
informative pseudo-labels, store the best pseudo-labels generated, and select
the most reliable components to guide the student model, thereby ensuring
robust network training. Additionally, to tackle the issue of intrinsic
similarity, we design a hybrid-granularity feature grouping module that groups
features at different granularities and aggregates these results. By clustering
similar features, this module promotes segmentation coherence, facilitating
more complete segmentation for both single-object and multiple-object images.
We validate the effectiveness of our approach across multiple ISCOS tasks, and
experimental results demonstrate that our method achieves state-of-the-art
performance. Furthermore, SEE can serve as a plug-and-play solution, enhancing
the performance of existing models.

</details>


### [333] [Diversity-Guided MLP Reduction for Efficient Large Vision Transformers](https://arxiv.org/abs/2506.08591)
*Chengchao Shen,Hourun Zhu,Gongfan Fang,Jianxin Wang,Xinchao Wang*

Key words: Transformer, MLP, 模型压缩, 多样性引导, 权重剪枝

TL;DR: 论文提出了一种多样性引导的MLP缩减方法（DGMR），用于减少大型视觉Transformer的参数和计算成本，同时保持性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 大型Transformer模型的参数和计算成本高，尤其是MLP模块占用了大部分参数，因此需要一种高效的压缩方法。

Method: 采用Gram-Schmidt权重剪枝策略，消除MLP隐藏层的冗余神经元，同时保留权重多样性以在蒸馏过程中恢复性能。

Result: 实验结果表明，DGMR方法在几乎无损的情况下实现了超过57.0%的参数和FLOPs缩减，对EVA-CLIP-E模型达到了71.5%的缩减且无性能下降。

Conclusion: DGMR方法显著降低了大型视觉Transformer的成本，同时保持了原始性能，具有高效性和实用性。

Abstract: Transformer models achieve excellent scaling property, where the performance
is improved with the increment of model capacity. However, large-scale model
parameters lead to an unaffordable cost of computing and memory. We analyze
popular transformer architectures and find that multilayer perceptron (MLP)
modules take up the majority of model parameters. To this end, we focus on the
recoverability of the compressed models and propose a Diversity-Guided MLP
Reduction (DGMR) method to significantly reduce the parameters of large vision
transformers with only negligible performance degradation. Specifically, we
conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons
of MLP hidden layer, while preserving weight diversity for better performance
recover during distillation. Compared to the model trained from scratch, our
pruned model only requires 0.06\% data of LAION-2B (for the training of large
vision transformers) without labels (ImageNet-1K) to recover the original
performance. Experimental results on several state-of-the-art large vision
transformers demonstrate that our method achieves a more than 57.0\% parameter
and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),
our method accomplishes a 71.5\% parameter and FLOPs reduction without
performance degradation. The source code and trained weights are available at
https://github.com/visresearch/DGMR.

</details>


### [334] [Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models](https://arxiv.org/abs/2506.08990)
*Chenyu Lian,Hong-Yu Zhou,Dongyun Liang,Jing Qin,Liansheng Wang*

Key words: 医学视觉-语言对齐, 跨模态对比学习, ALTA, 多模态掩码建模, 高效对齐

TL;DR: ALTA（ALign Through Adapting）是一种高效的医学视觉-语言对齐方法，通过适应预训练的视觉模型，显著减少了训练参数和计算消耗，并在检索和零样本分类任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统的跨模态对比学习方法在视觉表示能力上表现欠佳，而多模态掩码建模方法虽然在视觉表示上表现优异，但在直接跨模态匹配上存在困难。ALTA旨在解决这一矛盾。

Method: ALTA通过适应预训练的视觉模型（来自掩码记录建模），仅需约8%的可训练参数和不到1/5的计算消耗。同时整合时态多视图放射影像输入以增强信息一致性。

Result: ALTA在文本到图像准确率上比最佳对比方法高出4%以上，在图像到文本检索准确率上高出约6%。

Conclusion: ALTA在高效对齐过程中提升了视觉和语言理解能力，为医学视觉-语言对齐任务提供了有效解决方案。

Abstract: Medical vision-language alignment through cross-modal contrastive learning
shows promising performance in image-text matching tasks, such as retrieval and
zero-shot classification. However, conventional cross-modal contrastive
learning (CLIP-based) methods suffer from suboptimal visual representation
capabilities, which also limits their effectiveness in vision-language
alignment. In contrast, although the models pretrained via multimodal masked
modeling struggle with direct cross-modal matching, they excel in visual
representation. To address this contradiction, we propose ALTA (ALign Through
Adapting), an efficient medical vision-language alignment method that utilizes
only about 8% of the trainable parameters and less than 1/5 of the
computational consumption required for masked record modeling. ALTA achieves
superior performance in vision-language matching tasks like retrieval and
zero-shot classification by adapting the pretrained vision model from masked
record modeling. Additionally, we integrate temporal-multiview radiograph
inputs to enhance the information consistency between radiographs and their
corresponding descriptions in reports, further improving the vision-language
alignment. Experimental evaluations show that ALTA outperforms the
best-performing counterpart by over 4% absolute points in text-to-image
accuracy and approximately 6% absolute points in image-to-text retrieval
accuracy. The adaptation of vision-language models during efficient alignment
also promotes better vision and language understanding. Code is publicly
available at https://github.com/DopamineLcy/ALTA.

</details>


### [335] [Diffuse and Disperse: Image Generation with Representation Regularization](https://arxiv.org/abs/2506.09027)
*Runqian Wang,Kaiming He*

Key words: 扩散模型, 正则化, 表示学习, Dispersive Loss, 生成模型

TL;DR: 本文提出了Dispersive Loss，一种简单即插即用的正则化方法，用于改进基于扩散的生成模型，通过分散隐空间的内部表示提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的扩散生成模型通常依赖回归目标，缺乏显式正则化方法，难以与表示学习结合。本文试图弥补这一差距。

Method: 提出了一种名为Dispersive Loss的自包含正则化方法，无需预训练或额外参数，鼓励隐空间表示分散。

Result: 在ImageNet数据集上测试，Dispersive Loss在多种模型中均表现出优于基线的性能。

Conclusion: 该工作有望连接生成模型与表示学习领域，提供了一种高效且简洁的正则化方案。

Abstract: The development of diffusion-based generative models over the past decade has
largely proceeded independently of progress in representation learning. These
diffusion models typically rely on regression-based objectives and generally
lack explicit regularization. In this work, we propose \textit{Dispersive
Loss}, a simple plug-and-play regularizer that effectively improves
diffusion-based generative models. Our loss function encourages internal
representations to disperse in the hidden space, analogous to contrastive
self-supervised learning, with the key distinction that it requires no positive
sample pairs and therefore does not interfere with the sampling process used
for regression. Compared to the recent method of representation alignment
(REPA), our approach is self-contained and minimalist, requiring no
pre-training, no additional parameters, and no external data. We evaluate
Dispersive Loss on the ImageNet dataset across a range of models and report
consistent improvements over widely used and strong baselines. We hope our work
will help bridge the gap between generative modeling and representation
learning.

</details>


### [336] [Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models](https://arxiv.org/abs/2506.08780)
*Isaac Corley,Lakshay Sharma,Ruth Crasto*

Key words: Landsat, 地理空间基础模型, 基准测试, SSL4EO-L, EuroSAT-L, BigEarthNet-L

TL;DR: 该论文介绍了Landsat-Bench，一个包含三个基准测试的套件，用于评估基于Landsat影像的地理空间基础模型（GFM），并证明了在SSL4EO-L数据集上预训练的GFM在下游任务中表现优于ImageNet。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 由于缺乏针对Landsat数据的基准测试，限制了基于Landsat的地理空间基础模型的发展，因此需要建立标准化的评估方法。

Method: 通过引入三个基准测试（EuroSAT-L、BigEarthNet-L和LC100-L），并使用SSL4EO-L数据集预训练的GFM进行标准化评估。

Result: 实验表明，SSL4EO-L预训练的GFM在下游任务中表现更好，比ImageNet提高了4%的总体准确率（OA）和5.1%的平均准确率（mAP）。

Conclusion: Landsat-Bench为基于Landsat数据的GFM提供了标准化的基准测试，证明了预训练对提升模型性能的重要性。

Abstract: The Landsat program offers over 50 years of globally consistent Earth
imagery. However, the lack of benchmarks for this data constrains progress
towards Landsat-based Geospatial Foundation Models (GFM). In this paper, we
introduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that
adapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and
LC100-L. We establish baseline and standardized evaluation methods across both
common architectures and Landsat foundation models pretrained on the SSL4EO-L
dataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract
better representations for downstream tasks in comparison to ImageNet,
including performance gains of +4% OA and +5.1% mAP on EuroSAT-L and
BigEarthNet-L.

</details>


### [337] [StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams](https://arxiv.org/abs/2506.08862)
*Zike Wu,Qi Yan,Xuanyu Yi,Lele Wang,Renjie Liao*

Key words: 动态3D重建, 未校准视频, 高斯喷溅, 实时处理, 长期稳定性

TL;DR: StreamSplat是一种全新的动态3D场景重建框架，能够从未校准的视频流中实时生成动态3D高斯喷溅表示，解决了现有方法在实时性、动态建模和长期稳定性方面的不足。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法未能同时解决实时处理未校准输入、准确建模动态场景演化和保持长期稳定性及计算效率的挑战，StreamSplat旨在填补这一空白。

Method: 提出两种关键技术：静态编码器中的概率采样机制用于3DGS位置预测，以及动态解码器中的双向变形场，以实现高效稳健的动态建模。

Result: 实验表明，StreamSplat在重建质量和动态场景建模方面均优于现有方法，并支持任意长度视频流的在线重建。

Conclusion: StreamSplat是首个完全前馈的框架，实现了从未校准视频流到动态3DGS表示的实时在线转换。

Abstract: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams
is crucial for numerous real-world applications. However, existing methods
struggle to jointly address three key challenges: 1) processing uncalibrated
inputs in real time, 2) accurately modeling dynamic scene evolution, and 3)
maintaining long-term stability and computational efficiency. To this end, we
introduce StreamSplat, the first fully feed-forward framework that transforms
uncalibrated video streams of arbitrary length into dynamic 3D Gaussian
Splatting (3DGS) representations in an online manner, capable of recovering
scene dynamics from temporally local observations. We propose two key technical
innovations: a probabilistic sampling mechanism in the static encoder for 3DGS
position prediction, and a bidirectional deformation field in the dynamic
decoder that enables robust and efficient dynamic modeling. Extensive
experiments on static and dynamic benchmarks demonstrate that StreamSplat
consistently outperforms prior works in both reconstruction quality and dynamic
scene modeling, while uniquely supporting online reconstruction of arbitrarily
long video streams. Code and models are available at
https://github.com/nickwzk/StreamSplat.

</details>


### [338] [Data Augmentation For Small Object using Fast AutoAugment](https://arxiv.org/abs/2506.08956)
*DaeEun Yoon,Semin Kim,SangWook Yoo,Jongha Lee*

Key words: 小目标检测, 数据增强, Fast AutoAugment, DOTA数据集

TL;DR: 通过Fast AutoAugment优化数据增强方法，提升小目标检测性能，在DOTA数据集上实现20%的性能提升。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 尽管目标检测性能近年来显著提升，但小目标检测仍表现较差，亟需改进。

Method: 提出基于Fast AutoAugment的优化数据增强方法，快速找到最优增强策略以克服小目标检测的性能下降问题。

Result: 在DOTA数据集上实现了20%的性能提升。

Conclusion: 所提方法有效提升了小目标检测性能，为计算机视觉中的小目标检测问题提供了实用解决方案。

Abstract: In recent years, there has been tremendous progress in object detection
performance. However, despite these advances, the detection performance for
small objects is significantly inferior to that of large objects. Detecting
small objects is one of the most challenging and important problems in computer
vision. To improve the detection performance for small objects, we propose an
optimal data augmentation method using Fast AutoAugment. Through our proposed
method, we can quickly find optimal augmentation policies that can overcome
degradation when detecting small objects, and we achieve a 20% performance
improvement on the DOTA dataset.

</details>


### [339] [DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging](https://arxiv.org/abs/2506.09024)
*Felix Wagner,Pramit Saha,Harry Anthony,J. Alison Noble,Konstantinos Kamnitsas*

Key words: OOD检测，去中心化，医学影像，数据隐私，Isolation Network

TL;DR: 本文提出了一种去中心化的OOD检测框架DIsoN，用于在无法共享训练数据的实际环境中，通过模型参数交换实现训练数据与测试数据的比较，以检测分布外样本。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 在安全关键领域（如医学影像）部署ML模型时，需要检测训练数据中未见的输入（即OOD检测），以防止不可靠预测。现有方法通常无法在实际环境中实现数据共享，亟需一种去中心化的解决方案。

Method: 提出了Isolation Network框架，通过解决二分类任务量化测试样本与训练数据的分离难度。进一步提出DIsoN，通过交换模型参数在去中心化环境中实现训练与测试数据的比较，并扩展了类条件化的方法。

Result: 在四个医学影像数据集（皮肤病学、胸部X光、乳腺超声、组织病理学）上的12个OOD检测任务中，DIsoN优于现有方法，且保护了数据隐私。

Conclusion: DIsoN为ML开发者提供了一种新的服务模式，即在保护数据隐私的前提下，远程安全地利用训练数据进行OOD检测。

Abstract: Safe deployment of machine learning (ML) models in safety-critical domains
such as medical imaging requires detecting inputs with characteristics not seen
during training, known as out-of-distribution (OOD) detection, to prevent
unreliable predictions. Effective OOD detection after deployment could benefit
from access to the training data, enabling direct comparison between test
samples and the training data distribution to identify differences.
State-of-the-art OOD detection methods, however, either discard training data
after deployment or assume that test samples and training data are centrally
stored together, an assumption that rarely holds in real-world settings. This
is because shipping training data with the deployed model is usually impossible
due to the size of training databases, as well as proprietary or privacy
constraints. We introduce the Isolation Network, an OOD detection framework
that quantifies the difficulty of separating a target test sample from the
training data by solving a binary classification task. We then propose
Decentralized Isolation Networks (DIsoN), which enables the comparison of
training and test data when data-sharing is impossible, by exchanging only
model parameters between the remote computational nodes of training and
deployment. We further extend DIsoN with class-conditioning, comparing a target
sample solely with training data of its predicted class. We evaluate DIsoN on
four medical imaging datasets (dermatology, chest X-ray, breast ultrasound,
histopathology) across 12 OOD detection tasks. DIsoN performs favorably against
existing methods while respecting data-privacy. This decentralized OOD
detection framework opens the way for a new type of service that ML developers
could provide along with their models: providing remote, secure utilization of
their training data for OOD detection services. Code will be available upon
acceptance at: *****

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [340] [Physics-Informed Teleconnection-Aware Transformer for Global Subseasonal-to-Seasonal Forecasting](https://arxiv.org/abs/2506.08049)
*Tengfei Lyu,Weijia Zhang,Hao Liu*

Key words: S2S预报,深度学习,遥相关,球形谐波,物理信息神经网络

TL;DR: TelePiT是一种新型深度学习架构，通过整合多尺度物理和遥相关感知，显著提升了全球次季节到季节（S2S）预报能力。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 当前S2S预报方法未能明确建模关键的物理过程和遥相关，导致预报效果受限。

Method: TelePiT包含三个组件：球形谐波嵌入、多尺度物理信息神经ODE和遥相关感知Transformer。

Result: 实验显示，TelePiT显著优于现有数据驱动模型和数值天气预报系统，例如2米温度RMSE降低了57.7%。

Conclusion: TelePiT通过物理和遥相关建模，为S2S预报提供了有效解决方案。

Abstract: Subseasonal-to-seasonal (S2S) forecasting, which predicts climate conditions
from several weeks to months in advance, presents significant challenges due to
the chaotic dynamics of atmospheric systems and complex interactions across
multiple scales. Current approaches often fail to explicitly model underlying
physical processes and teleconnections that are crucial at S2S timescales. We
introduce TelePiT, a novel deep learning architecture that enhances global S2S
forecasting through integrated multi-scale physics and teleconnection
awareness. Our approach consists of three key components: (1) Spherical
Harmonic Embedding, which accurately encodes global atmospheric variables onto
spherical geometry; (2) Multi-Scale Physics-Informed Neural ODE, which
explicitly captures atmospheric physical processes across multiple learnable
frequency bands; (3) Teleconnection-Aware Transformer, which models critical
global climate interactions through tactfully injecting teleconnection patterns
into the self-attention. Extensive experiments demonstrate that TelePiT
significantly outperforms state-of-the-art data-driven baselines and
operational numerical weather prediction systems, with remarkable improvements
for atmospheric variables including a 57.7% reduction in RMSE for 2-meter
temperature compared to previous best models.

</details>


### [341] [WWAggr: A Window Wasserstein-based Aggregation for Ensemble Change Point Detection](https://arxiv.org/abs/2506.08066)
*Alexander Stepikin,Evgenia Romanenkova,Alexey Zaytsev*

Key words: Change Point Detection, Wasserstein distance, ensemble aggregation, deep learning

TL;DR: 论文提出了一种基于Wasserstein距离的新型集成聚合方法WWAggr，用于提升高维变化点检测的性能，并解决了决策阈值选择的长期问题。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现实世界的高维变化点检测（CPD）面临数据模式复杂性和常见假设违反的挑战，现有基于深度神经网络的检测器尚未达到完美质量。

Method: 引入了一种任务特定的集成聚合方法WWAggr，基于Wasserstein距离，适用于多种深度CPD模型的集成。

Result: WWAggr能够有效提升变化点检测的性能，并解决了决策阈值选择的难题。

Conclusion: WWAggr为高维变化点检测提供了一种更鲁棒的解决方案。

Abstract: Change Point Detection (CPD) aims to identify moments of abrupt distribution
shifts in data streams. Real-world high-dimensional CPD remains challenging due
to data pattern complexity and violation of common assumptions. Resorting to
standalone deep neural networks, the current state-of-the-art detectors have
yet to achieve perfect quality. Concurrently, ensembling provides more robust
solutions, boosting the performance. In this paper, we investigate ensembles of
deep change point detectors and realize that standard prediction aggregation
techniques, e.g., averaging, are suboptimal and fail to account for problem
peculiarities. Alternatively, we introduce WWAggr -- a novel task-specific
method of ensemble aggregation based on the Wasserstein distance. Our procedure
is versatile, working effectively with various ensembles of deep CPD models.
Moreover, unlike existing solutions, we practically lift a long-standing
problem of the decision threshold selection for CPD.

</details>


### [342] [Constrained Pareto Set Identification with Bandit Feedback](https://arxiv.org/abs/2506.08127)
*Cyrille Kone,Emilie Kaufmann,Laura Richert*

Key words: 多臂老虎机,帕累托集,固定置信度,样本复杂度

TL;DR: 论文提出了一种在多臂老虎机设置中识别帕累托集的算法，显著优于现有方法，并证明了其样本复杂度接近最优。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 解决在多目标且带有可行性约束的多臂老虎机中识别帕累托集的问题。

Method: 引入了固定置信度识别算法，优于赛车式算法和传统的两阶段方法。

Result: 算法表现优异，样本复杂度接近理论下界，并通过实验验证。

Conclusion: 提出的算法在多臂老虎机问题中高效且接近最优。

Abstract: In this paper, we address the problem of identifying the Pareto Set under
feasibility constraints in a multivariate bandit setting. Specifically, given a
$K$-armed bandit with unknown means $\mu_1, \dots, \mu_K \in \mathbb{R}^d$, the
goal is to identify the set of arms whose mean is not uniformly worse than that
of another arm (i.e., not smaller for all objectives), while satisfying some
known set of linear constraints, expressing, for example, some minimal
performance on each objective. Our focus lies in fixed-confidence
identification, for which we introduce an algorithm that significantly
outperforms racing-like algorithms and the intuitive two-stage approach that
first identifies feasible arms and then their Pareto Set. We further prove an
information-theoretic lower bound on the sample complexity of any algorithm for
constrained Pareto Set identification, showing that the sample complexity of
our approach is near-optimal. Our theoretical results are supported by an
extensive empirical evaluation on a series of benchmarks.

</details>


### [343] [Model-Free Kernel Conformal Depth Measures Algorithm for Uncertainty Quantification in Regression Models in Separable Hilbert Spaces](https://arxiv.org/abs/2506.08325)
*Marcos Matabuena,Rahul Ghosal,Pavlo Mozharovskyi,Oscar Hernan Madrid Padilla,Jukka-Pekka Onnela*

Key words: 深度测量、不确定性量化、核均值嵌入、共形预测、功能数据、数字健康

TL;DR: 提出了基于条件深度测量的新型不确定性量化算法，用于定义预测区域，并在功能数据和数字健康应用中验证了其性能。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 深度测量在复杂随机对象中定义水平集具有优势，但其在回归建模中的应用尚未充分探索，因此需要填补这一研究空白。

Method: 基于条件核均值嵌入和集成深度测量，提出一种模型无关的算法，并结合共形预测以增强有限样本下的实用性。

Result: 算法在功能数据和传统欧几里得场景中表现良好，并提供了收敛速率保证。

Conclusion: 该方法在实际应用中具有潜力，特别是在数字健康的个性化推荐领域。

Abstract: Depth measures are powerful tools for defining level sets in emerging,
non--standard, and complex random objects such as high-dimensional multivariate
data, functional data, and random graphs. Despite their favorable theoretical
properties, the integration of depth measures into regression modeling to
provide prediction regions remains a largely underexplored area of research. To
address this gap, we propose a novel, model-free uncertainty quantification
algorithm based on conditional depth measures--specifically, conditional kernel
mean embeddings and an integrated depth measure. These new algorithms can be
used to define prediction and tolerance regions when predictors and responses
are defined in separable Hilbert spaces. The use of kernel mean embeddings
ensures faster convergence rates in prediction region estimation. To enhance
the practical utility of the algorithms with finite samples, we also introduce
a conformal prediction variant that provides marginal, non-asymptotic
guarantees for the derived prediction regions. Additionally, we establish both
conditional and unconditional consistency results, as well as fast convergence
rates in certain homoscedastic settings. We evaluate the finite--sample
performance of our model in extensive simulation studies involving various
types of functional data and traditional Euclidean scenarios. Finally, we
demonstrate the practical relevance of our approach through a digital health
application related to physical activity, aiming to provide personalized
recommendations

</details>


### [344] [Asymptotic Normality of Infinite Centered Random Forests -Application to Imbalanced Classification](https://arxiv.org/abs/2506.08548)
*Moria Mayala,Erwan Scornet,Charles Tillier,Olivier Wintenberger*

Key words: 不平衡数据、Centered Random Forests、重要性采样、方差减少

TL;DR: 研究了用于不平衡数据分类的Centered Random Forests（CRF）理论，证明了通过重要性采样（IS）去偏的IS-ICRF方法在高度不平衡数据中能减少方差，优于原始数据训练的CRF。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 不平衡数据分类中，少数类样本的不足会影响分类器性能。本文旨在理论分析重采样技术对CRF的影响。

Method: 使用IS方法对CRF进行去偏，提出IS-ICRF，并在理论和实验上验证其性能。

Result: IS-ICRF满足中心极限定理，在高度不平衡数据中能显著降低方差。

Conclusion: 重采样后去偏的CRF在理论分析和实验中均优于原始数据训练的CRF，尤其在高度不平衡场景下。

Abstract: Many classification tasks involve imbalanced data, in which a class is
largely underrepresented. Several techniques consists in creating a rebalanced
dataset on which a classifier is trained. In this paper, we study theoretically
such a procedure, when the classifier is a Centered Random Forests (CRF). We
establish a Central Limit Theorem (CLT) on the infinite CRF with explicit rates
and exact constant. We then prove that the CRF trained on the rebalanced
dataset exhibits a bias, which can be removed with appropriate techniques.
Based on an importance sampling (IS) approach, the resulting debiased
estimator, called IS-ICRF, satisfies a CLT centered at the prediction function
value. For high imbalance settings, we prove that the IS-ICRF estimator enjoys
a variance reduction compared to the ICRF trained on the original data.
Therefore, our theoretical analysis highlights the benefits of training random
forests on a rebalanced dataset (followed by a debiasing procedure) compared to
using the original data. Our theoretical results, especially the variance rates
and the variance reduction, appear to be valid for Breiman's random forests in
our experiments.

</details>


### [345] [Flexible and Efficient Drift Detection without Labels](https://arxiv.org/abs/2506.08734)
*Nelvin Tan,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Key words: 机器学习, 概念漂移, 无标签检测, 统计过程控制

TL;DR: 本文提出了一种在无标签环境下高效检测概念漂移的算法，结合了经典统计过程控制方法，并在计算限制下表现出优于现有方法的统计功效。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 机器学习模型在自动化决策中广泛应用，确保模型性能对服务质量至关重要，尤其是在无即时标签的情况下检测概念漂移具有挑战性。

Method: 提出了一种无标签的概念漂移检测算法，基于统计过程控制，并开发了新的漂移检测框架以整合先验检测信息。

Result: 算法在计算限制下具有更好的统计功效，数值实验展示了其有效性能。

Conclusion: 所提算法及框架在无标签环境下能高效检测概念漂移，实用性强。

Abstract: Machine learning models are being increasingly used to automate decisions in
almost every domain, and ensuring the performance of these models is crucial
for ensuring high quality machine learning enabled services. Ensuring concept
drift is detected early is thus of the highest importance. A lot of research on
concept drift has focused on the supervised case that assumes the true labels
of supervised tasks are available immediately after making predictions.
Controlling for false positives while monitoring the performance of predictive
models used to make inference from extremely large datasets periodically, where
the true labels are not instantly available, becomes extremely challenging. We
propose a flexible and efficient concept drift detection algorithm that uses
classical statistical process control in a label-less setting to accurately
detect concept drifts. We shown empirically that under computational
constraints, our approach has better statistical power than previous known
methods. Furthermore, we introduce a new drift detection framework to model the
scenario of detecting drift (without labels) given prior detections, and show
our how our drift detection algorithm can be incorporated effectively into this
framework. We demonstrate promising performance via numerical simulations.

</details>
