<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 26]
- [cs.LG](#cs.LG) [Total: 58]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.NE](#cs.NE) [Total: 6]
- [cs.SI](#cs.SI) [Total: 3]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 2]
- [eess.SY](#eess.SY) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.RO](#cs.RO) [Total: 14]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [hep-th](#hep-th) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [eess.SP](#eess.SP) [Total: 4]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.CV](#cs.CV) [Total: 23]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification](https://arxiv.org/abs/2505.05583)
*Qianbo Zang,Christophe Zgrzendek,Igor Tchappi,Afshin Khadangi,Johannes Sedlmeir*

Main category: cs.CL

TL;DR: 该论文提出了KG-HTC方法，结合知识图谱与大型语言模型（LLMs）解决层次化文本分类（HTC）中的数据不足、大标签空间和长尾分布问题。通过RAG方法检索相关子图，增强LLMs对标签语义的理解，实验显示在零样本场景下显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 由于现实场景中标注数据稀缺且HTC面临大标签空间和长尾分布问题，作者提出利用知识图谱为LLMs提供结构化语义上下文，以改进分类效果。

Method: 采用检索增强生成（RAG）方法从知识图谱中检索与输入文本相关的子图，结合LLMs进行层次化分类。

Result: 在WoS、DBpedia和Amazon三个数据集上，KG-HTC在零样本设置下显著优于基线方法，尤其在深层层次上表现更优。

Conclusion: 通过知识图谱增强LLMs能有效解决HTC中的大标签空间和长尾分布问题，展现了结构化知识在分类任务中的重要性。

Abstract: Hierarchical Text Classification (HTC) involves assigning documents to labels
organized within a taxonomy. Most previous research on HTC has focused on
supervised methods. However, in real-world scenarios, employing supervised HTC
can be challenging due to a lack of annotated data. Moreover, HTC often faces
issues with large label spaces and long-tail distributions. In this work, we
present Knowledge Graphs for zero-shot Hierarchical Text Classification
(KG-HTC), which aims to address these challenges of HTC in applications by
integrating knowledge graphs with Large Language Models (LLMs) to provide
structured semantic context during classification. Our method retrieves
relevant subgraphs from knowledge graphs related to the input text using a
Retrieval-Augmented Generation (RAG) approach. Our KG-HTC can enhance LLMs to
understand label semantics at various hierarchy levels. We evaluate KG-HTC on
three open-source HTC datasets: WoS, DBpedia, and Amazon. Our experimental
results show that KG-HTC significantly outperforms three baselines in the
strict zero-shot setting, particularly achieving substantial improvements at
deeper levels of the hierarchy. This evaluation demonstrates the effectiveness
of incorporating structured knowledge into LLMs to address HTC's challenges in
large label spaces and long-tailed label distributions. Our code is available
at: https://github.com/QianboZang/KG-HTC.

</details>


### [2] [Privacy-Preserving Transformers: SwiftKey's Differential Privacy Implementation](https://arxiv.org/abs/2505.05648)
*Abdelrahman Abouelenin,Mohamed Abdelrehim,Raffy Fahim,Amr Hendy,Mohamed Afify*

Main category: cs.CL

TL;DR: 论文通过在SwiftKey中训练一个基于差分隐私（DP）的transformer语言模型，研究了模型大小、运行速度和准确性之间的权衡，并展示了在内存和速度方面相比生产级GRU的改进。


<details>
  <summary>Details</summary>
Motivation: 动机在于开发一个更高效、准确且隐私保护的手机输入法语言模型，以平衡模型性能与资源消耗。

Method: 通过缩小GPT2架构适应手机输入法大小，并结合两阶段训练：通用数据预训练和差分隐私微调，最后使用ONNX集成模型。

Result: 实验结果显示，在下一词预测和准确性上取得小幅但一致的提升，同时内存和速度消耗增长较为平缓。

Conclusion: 证明了缩小版transformer结合差分隐私训练在手机输入法中的可行性，优于现有GRU模型。

Abstract: In this paper we train a transformer using differential privacy (DP) for
language modeling in SwiftKey. We run multiple experiments to balance the
trade-off between the model size, run-time speed and accuracy. We show that we
get small and consistent gains in the next-word-prediction and accuracy with
graceful increase in memory and speed compared to the production GRU. This is
obtained by scaling down a GPT2 architecture to fit the required size and a two
stage training process that builds a seed model on general data and DP
finetunes it on typing data. The transformer is integrated using ONNX offering
both flexibility and efficiency.

</details>


### [3] [Exploration of COVID-19 Discourse on Twitter: American Politician Edition](https://arxiv.org/abs/2505.05687)
*Cindy Kim,Daniela Puchall,Jiangyi Liang,Jiwon Kim*

Main category: cs.CL

TL;DR: 论文探讨了COVID-19大流行期间美国两大政党（共和党与民主党）在推特上的言论差异，通过文本分析技术揭示了他们在态度和应对方式上的不同，并提出了通过语言模型预测政治立场的方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解COVID-19大流行如何加剧了美国两党的政治极化，尤其是通过分析他们在社交媒体上的言论差异。

Method: 使用bag-of-words、bigram和TF-IDF模型，分析了两党政客的推文关键词、主题和情感，进而构建分类算法预测推文的政治立场。

Result: 民主党更关注疫情伤亡和医疗建议，而共和党更注重政治责任如通过媒体更新信息和监控病毒进展。分类算法能有效区分推文的政治立场。

Conclusion: 研究表明疫情加剧了两党的言论差异，提出的分类方法有助于理解社交媒体上的政治极化现象。

Abstract: The advent of the COVID-19 pandemic has undoubtedly affected the political
scene worldwide and the introduction of new terminology and public opinions
regarding the virus has further polarized partisan stances. Using a collection
of tweets gathered from leading American political figures online (Republican
and Democratic), we explored the partisan differences in approach, response,
and attitude towards handling the international crisis. Implementation of the
bag-of-words, bigram, and TF-IDF models was used to identify and analyze
keywords, topics, and overall sentiments from each party. Results suggest that
Democrats are more concerned with the casualties of the pandemic, and give more
medical precautions and recommendations to the public whereas Republicans are
more invested in political responsibilities such as keeping the public updated
through media and carefully watching the progress of the virus. We propose a
systematic approach to predict and distinguish a tweet's political stance (left
or right leaning) based on its COVID-19 related terms using different
classification algorithms on different language models.

</details>


### [4] [Assessing Robustness to Spurious Correlations in Post-Training Language Models](https://arxiv.org/abs/2505.05704)
*Julia Shuieh,Prasann Singhal,Apaar Shanker,John Heyer,George Pu,Samuel Denton*

Main category: cs.CL

TL;DR: 文章比较了监督微调（SFT）、直接偏好优化（DPO）和卡尼曼-特沃斯基优化（KTO）在虚假相关性条件下的表现，发现不同方法在不同任务中各有优劣。


<details>
  <summary>Details</summary>
Motivation: 由于实际训练数据中存在虚假相关性，可能导致模型性能或泛化能力下降，本文旨在系统评估不同微调算法在虚假相关性条件下的表现。

Method: 在数学推理、指令遵循和文档问答等任务中，比较了SFT、DPO和KTO算法，并设置了不同虚假相关性程度（10% vs. 90%）和两种虚假特征类型（“特征模糊性”和“分布狭窄性”）。

Result: 偏好方法（DPO/KTO）在数学推理任务中表现相对稳健，而SFT在复杂上下文任务中性能更强。

Conclusion: 没有一种后训练策略在所有场景下都最优，最佳选择取决于目标任务类型和虚假相关性的性质。

Abstract: Supervised and preference-based fine-tuning techniques have become popular
for aligning large language models (LLMs) with user intent and correctness
criteria. However, real-world training data often exhibits spurious
correlations -- arising from biases, dataset artifacts, or other "shortcut"
features -- that can compromise a model's performance or generalization. In
this paper, we systematically evaluate three post-training algorithms --
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO
(Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and
spuriousness conditions. Our tasks span mathematical reasoning, constrained
instruction-following, and document-grounded question answering. We vary the
degree of spurious correlation (10% vs. 90%) and investigate two forms of
artifacts: "Feature Ambiguity" and "Distributional Narrowness." Our results
show that the models often but not always degrade under higher spuriousness.
The preference-based methods (DPO/KTO) can demonstrate relative robustness in
mathematical reasoning tasks. By contrast, SFT maintains stronger performance
in complex, context-intensive tasks. These findings highlight that no single
post-training strategy universally outperforms in all scenarios; the best
choice depends on the type of target task and the nature of spurious
correlations.

</details>


### [5] [TopicVD: A Topic-Based Dataset of Video-Guided Multimodal Machine Translation for Documentaries](https://arxiv.org/abs/2505.05714)
*Jinze Lv,Jian Chen,Zi Long,Xianghua Fu,Yin Chen*

Main category: cs.CL

TL;DR: 该研究提出了TopicVD数据集，支持基于主题的视频多模态机器翻译（MMT），并在文档翻译中验证了视觉信息和全局上下文对翻译性能的提升作用。


<details>
  <summary>Details</summary>
Motivation: 现有MMT数据集多为静态图像或短视频片段，难以满足实际任务（如纪录片翻译）的需求。研究希望通过TopicVD数据集推动视频引导的MMT研究。

Method: 收集纪录片视频-字幕对并按主题分类，提出基于跨模态双向注意力模块的MMT模型。

Result: 视觉信息显著提升翻译性能，但模型在领域外场景表现下降，全局上下文则有效改善翻译效果。

Conclusion: TopicVD为视频MMT研究提供新基准，同时揭示了领域适应和全局上下文利用的重要性。

Abstract: Most existing multimodal machine translation (MMT) datasets are predominantly
composed of static images or short video clips, lacking extensive video data
across diverse domains and topics. As a result, they fail to meet the demands
of real-world MMT tasks, such as documentary translation. In this study, we
developed TopicVD, a topic-based dataset for video-supported multimodal machine
translation of documentaries, aiming to advance research in this field. We
collected video-subtitle pairs from documentaries and categorized them into
eight topics, such as economy and nature, to facilitate research on domain
adaptation in video-guided MMT. Additionally, we preserved their contextual
information to support research on leveraging the global context of
documentaries in video-guided MMT. To better capture the shared semantics
between text and video, we propose an MMT model based on a cross-modal
bidirectional attention module. Extensive experiments on the TopicVD dataset
demonstrate that visual information consistently improves the performance of
the NMT model in documentary translation. However, the MMT model's performance
significantly declines in out-of-domain scenarios, highlighting the need for
effective domain adaptation methods. Additionally, experiments demonstrate that
global context can effectively improve translation performance. % Dataset and
our implementations are available at https://github.com/JinzeLv/TopicVD

</details>


### [6] [Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions](https://arxiv.org/abs/2505.05755)
*Dhruvesh Patel,Aishwarya Sahoo,Avinash Amballa,Tahira Naseem,Tim G. J. Rudner,Andrew McCallum*

Main category: cs.CL

TL;DR: 插入语言模型（ILMs）通过在任意位置插入令牌解决了自回归模型和掩码扩散模型的局限性，表现出更强的灵活性和性能。


<details>
  <summary>Details</summary>
Motivation: 自回归模型（ARMs）和掩码扩散模型（MDMs）在处理复杂约束或非顺序依赖的序列时存在不足，ILMs旨在解决这些限制。

Method: 提出插入语言模型（ILMs），通过联合选择插入位置和词汇元素，并采用定制的网络参数化和去噪目标进行训练。

Result: ILMs在规划任务上优于ARMs和MDMs，在无条件文本生成任务上与ARMs相当，同时在任意长度文本填充中比MDMs更灵活。

Conclusion: ILMs提供了一种灵活且高效的方法来解决序列生成中的顺序和约束问题，扩展了现有模型的能力。

Abstract: Autoregressive models (ARMs), which predict subsequent tokens one-by-one
``from left to right,'' have achieved significant success across a wide range
of sequence generation tasks. However, they struggle to accurately represent
sequences that require satisfying sophisticated constraints or whose sequential
dependencies are better addressed by out-of-order generation. Masked Diffusion
Models (MDMs) address some of these limitations, but the process of unmasking
multiple tokens simultaneously in MDMs can introduce incoherences, and MDMs
cannot handle arbitrary infilling constraints when the number of tokens to be
filled in is not known in advance. In this work, we introduce Insertion
Language Models (ILMs), which learn to insert tokens at arbitrary positions in
a sequence -- that is, they select jointly both the position and the vocabulary
element to be inserted. By inserting tokens one at a time, ILMs can represent
strong dependencies between tokens, and their ability to generate sequences in
arbitrary order allows them to accurately model sequences where token
dependencies do not follow a left-to-right sequential structure. To train ILMs,
we propose a tailored network parameterization and use a simple denoising
objective. Our empirical evaluation demonstrates that ILMs outperform both ARMs
and MDMs on common planning tasks. Furthermore, we show that ILMs outperform
MDMs and perform on par with ARMs in an unconditional text generation task
while offering greater flexibility than MDMs in arbitrary-length text
infilling.

</details>


### [7] [Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM](https://arxiv.org/abs/2505.05772)
*Zehao Fan,Garrett Gagnon,Zhenyu Liu,Liu Liu*

Main category: cs.CL

TL;DR: STARC是一种新颖的稀疏优化数据映射方案，专为PIM架构上的高效LLM解码设计，通过语义相似性聚类KV对，减少内存访问开销，显著降低了延迟和能耗，同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在自回归解码时因频繁内存访问和KV缓存增长导致内存带宽瓶颈，当前PIM架构对稀疏访问模式处理效率低，需针对性优化。

Method: 提出STARC方案，通过语义相似性聚类KV对并映射到连续内存区域，利用预计算中心实现选择性注意力，减少数据移动和并行处理开销。

Result: 在HBM-PIM系统上，STARC比常见稀疏方法降低注意力层延迟19%-31%，能耗19%-27%；在KV缓存预算1024下，延迟和能耗分别减少54%-74%和45%-67%。

Conclusion: STARC在PIM架构上实现了高效、硬件友好的长上下文LLM推理，为稀疏注意力优化提供了可行方案。

Abstract: Transformer-based models are the foundation of modern machine learning, but
their execution, particularly during autoregressive decoding in large language
models (LLMs), places significant pressure on memory systems due to frequent
memory accesses and growing key-value (KV) caches. This creates a bottleneck in
memory bandwidth, especially as context lengths increase. Processing-in-memory
(PIM) architectures are a promising solution, offering high internal bandwidth
and compute parallelism near memory. However, current PIM designs are primarily
optimized for dense attention and struggle with the dynamic, irregular access
patterns introduced by modern KV cache sparsity techniques. Consequently, they
suffer from workload imbalance, reducing throughput and resource utilization.
In this work, we propose STARC, a novel sparsity-optimized data mapping scheme
tailored specifically for efficient LLM decoding on PIM architectures. STARC
clusters KV pairs by semantic similarity and maps them to contiguous memory
regions aligned with PIM bank structures. During decoding, queries retrieve
relevant tokens at cluster granularity by matching against precomputed
centroids, enabling selective attention and parallel processing without
frequent reclustering or data movement overhead. Experiments on the HBM-PIM
system show that, compared to common token-wise sparsity methods, STARC reduces
attention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a
KV cache budget of 1024, it achieves up to 54%--74% latency reduction and
45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC
maintains model accuracy comparable to state-of-the-art sparse attention
methods, demonstrating its effectiveness in enabling efficient and
hardware-friendly long-context LLM inference on PIM architectures.

</details>


### [8] [Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice Questions When Students' (Mis)Understanding Is Hinted](https://arxiv.org/abs/2505.05815)
*Machi Shimmei,Masaki Uto,Yuichiroh Matsubayashi,Kentaro Inui,Aditi Mallavarapu,Noboru Matsuda*

Main category: cs.CL

TL;DR: 本研究开发了一种创新的提示技术AnaQuest，用于使用预训练大型语言模型生成选择题（MCQ）。AnaQuest通过分析学生回答生成正确和错误选项，结合形成性和总结性评估。实验表明，AnaQuest生成的题目在难度和区分度上更接近人类编写的题目。


<details>
  <summary>Details</summary>
Motivation: 解决使用AI生成高质量选择题的挑战，尤其是生成具有教育意义的干扰选项（错误选项），以提升评估效果。

Method: 结合形成性和总结性评估，学生先回答开放问题，AnaQuest分析其回答生成MCQ选项。使用IRT比较AnaQuest、ChatGPT和人类生成题目的特性。

Result: 专家评价显示，AI生成的题目与人类编写的题目有效性相当；IRT分析表明，AnaQuest生成的题目（尤其是干扰项）在难度和区分度上更接近人类题目。

Conclusion: AnaQuest在生成教育有效选择题（尤其是干扰项）方面表现更优，接近人类水平，为AI在教育评估中的应用提供了新思路。

Abstract: The primary goal of this study is to develop and evaluate an innovative
prompting technique, AnaQuest, for generating multiple-choice questions (MCQs)
using a pre-trained large language model. In AnaQuest, the choice items are
sentence-level assertions about complex concepts. The technique integrates
formative and summative assessments. In the formative phase, students answer
open-ended questions for target concepts in free text. For summative
assessment, AnaQuest analyzes these responses to generate both correct and
incorrect assertions. To evaluate the validity of the generated MCQs, Item
Response Theory (IRT) was applied to compare item characteristics between MCQs
generated by AnaQuest, a baseline ChatGPT prompt, and human-crafted items. An
empirical study found that expert instructors rated MCQs generated by both AI
models to be as valid as those created by human instructors. However, IRT-based
analysis revealed that AnaQuest-generated questions - particularly those with
incorrect assertions (foils) - more closely resembled human-crafted items in
terms of difficulty and discrimination than those produced by ChatGPT.

</details>


### [9] [Symbol-based entity marker highlighting for enhanced text mining in materials science with generative AI](https://arxiv.org/abs/2505.05864)
*Junhyeong Lee,Jong Min Yuk,Chan-Woo Lee*

Main category: cs.CL

TL;DR: 提出了一种混合文本挖掘框架，结合了多步和直接方法的优势，通过实体标记技术提升了实体识别性能，显著改善了最终结构化数据的质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法（多步和直接方法）在独立应用时存在局限性，因此需要一种结合两者优势的新方法来更有效地从无结构科学文本中提取结构化数据。

Method: 提出了一种混合框架，首先将原始文本转换为实体识别文本，再进一步结构化；并引入实体标记技术以提升实体识别性能。

Result: 在三个基准数据集（MatScholar、SOFC 和 SOFC slot NER）上，该方法超越了之前的实体识别方法，实体级 F1 分数提升了 58%，关系级 F1 分数提升了 83%。

Conclusion: 混合框架通过结合多步和直接方法的优势，并利用实体标记技术，显著提升了从科学文本中提取结构化数据的性能。

Abstract: The construction of experimental datasets is essential for expanding the
scope of data-driven scientific discovery. Recent advances in natural language
processing (NLP) have facilitated automatic extraction of structured data from
unstructured scientific literature. While existing approaches-multi-step and
direct methods-offer valuable capabilities, they also come with limitations
when applied independently. Here, we propose a novel hybrid text-mining
framework that integrates the advantages of both methods to convert
unstructured scientific text into structured data. Our approach first
transforms raw text into entity-recognized text, and subsequently into
structured form. Furthermore, beyond the overall data structuring framework, we
also enhance entity recognition performance by introducing an entity marker-a
simple yet effective technique that uses symbolic annotations to highlight
target entities. Specifically, our entity marker-based hybrid approach not only
consistently outperforms previous entity recognition approaches across three
benchmark datasets (MatScholar, SOFC, and SOFC slot NER) but also improve the
quality of final structured data-yielding up to a 58% improvement in
entity-level F1 score and up to 83% improvement in relation-level F1 score
compared to direct approach.

</details>


### [10] [Elastic Weight Consolidation for Full-Parameter Continual Pre-Training of Gemma2](https://arxiv.org/abs/2505.05946)
*Vytenis Šliogeris,Povilas Daniušis,Artūras Nakvosas*

Main category: cs.CL

TL;DR: 实验研究了在Gemma2模型上应用弹性权重巩固（EWC）来缓解灾难性遗忘，并发现EWC对新任务学习也有潜在益处。


<details>
  <summary>Details</summary>
Motivation: 探索在大型语言模型（LLM）预训练中，如何通过EWC减少灾难性遗忘并提升对新任务的学习效果。

Method: 在Gemma2模型上应用EWC，测试其在多种语言理解基准和困惑度基准上的表现，包括英语和立陶宛语版本。

Result: EWC不仅有效缓解了灾难性遗忘，还对新任务学习有潜在益处。

Conclusion: EWC是一种有效的持续学习方法，适用于LLM的预训练和新任务学习。

Abstract: This technical report describes an experiment on autoregressive pre-training
of Gemma2 2 billion parameter large language model (LLM) with 10\% on the
Lithuanian language component of CulturaX from the point of view of continual
learning. We apply elastic weight consolidation (EWC) to the full set of the
model's parameters and investigate language understanding benchmarks,
consisting of Arc, Belebele, Gsm8K, Hellaswag, MMLU, TruthfulQA, and Winogrande
sets (both in English and Lithuanian versions), and perplexity benchmarks. We
empirically demonstrate that EWC regularisation allows us not only to mitigate
catastrophic forgetting effects but also that it is potentially beneficial for
learning of the new task with LLMs.

</details>


### [11] [Summarisation of German Judgments in conjunction with a Class-based Evaluation](https://arxiv.org/abs/2505.05947)
*Bianca Steffes,Nils Torben Wiedemann,Alexander Gratz,Pamela Hochreither,Jana Elina Meyer,Katharina Luise Schilke*

Main category: cs.CL

TL;DR: 该研究通过微调基于解码器的大语言模型，自动生成了德国判决书的摘要（指导原则）。研究在训练前为判决书添加了法律实体信息，并通过定义评估标准来衡量摘要的语言、相关性、完整性和正确性。结果显示，使用法律实体有助于模型找到相关内容，但生成的摘要质量尚不足以实际应用。


<details>
  <summary>Details</summary>
Motivation: 法律专家在日常工作中需要处理大量冗长的法律文件，自动摘要技术可以显著提升他们的工作效率。本研究旨在通过自动化方法生成德国判决书的摘要，帮助法律专家快速获取关键信息。

Method: 研究采用基于解码器的大语言模型，并在训练前对德国判决书进行法律实体信息的增强。模型通过微调生成摘要，并通过定义的评估类别（语言、相关性、完整性和正确性）进行评价。

Result: 实验结果表明，加入法律实体信息确实有助于模型识别相关内容，但生成的摘要在质量上仍无法满足实际应用的需求。

Conclusion: 尽管法律实体信息的加入提升了模型的性能，但生成的摘要质量仍需进一步提高才能用于实践。未来的研究可以探索更多优化方法或结合其他技术以提升摘要的实用性。

Abstract: The automated summarisation of long legal documents can be a great aid for
legal experts in their daily work. We automatically create summaries (guiding
principles) of German judgments by fine-tuning a decoder-based large language
model. We enrich the judgments with information about legal entities before the
training. For the evaluation of the created summaries, we define a set of
evaluation classes which allows us to measure their language, pertinence,
completeness and correctness. Our results show that employing legal entities
helps the generative model to find the relevant content, but the quality of the
created summaries is not yet sufficient for a use in practice.

</details>


### [12] [NeoQA: Evidence-based Question Answering with Generated News Events](https://arxiv.org/abs/2505.05949)
*Max Glockner,Xiang Jiang,Leonardo F. R. Ribeiro,Iryna Gurevych,Markus Dreyer*

Main category: cs.CL

TL;DR: 该论文提出了NeoQA基准，用于评估增强检索生成（RAG）在大型语言模型（LLMs）中的表现，通过虚构新闻事件和实体构建数据集，以避免模型依赖预训练知识。


<details>
  <summary>Details</summary>
Motivation: 由于现有基准易过时且难以区分基于证据的推理与记忆，需要一种新方法来准确评估LLMs的检索与推理能力。

Method: 构建虚构新闻事件的时间线和知识库，生成问答对，确保模型只能依靠检索到的证据回答问题。

Result: LLMs在处理问题与证据间的细微不匹配或信息缺失时表现不佳，显示出基于证据推理的关键局限性。

Conclusion: NeoQA为评估基于证据的问题回答提供了新平台，揭示了LLMs在证据推理中的不足。

Abstract: Evaluating Retrieval-Augmented Generation (RAG) in large language models
(LLMs) is challenging because benchmarks can quickly become stale. Questions
initially requiring retrieval may become answerable from pretraining knowledge
as newer models incorporate more recent information during pretraining, making
it difficult to distinguish evidence-based reasoning from recall. We introduce
NeoQA (News Events for Out-of-training Question Answering), a benchmark
designed to address this issue. To construct NeoQA, we generated timelines and
knowledge bases of fictional news events and entities along with news articles
and Q\&A pairs to prevent LLMs from leveraging pretraining knowledge, ensuring
that no prior evidence exists in their training data. We propose our dataset as
a new platform for evaluating evidence-based question answering, as it requires
LLMs to generate responses exclusively from retrieved evidence and only when
sufficient evidence is available. NeoQA enables controlled evaluation across
various evidence scenarios, including cases with missing or misleading details.
Our findings indicate that LLMs struggle to distinguish subtle mismatches
between questions and evidence, and suffer from short-cut reasoning when key
information required to answer a question is missing from the evidence,
underscoring key limitations in evidence-based reasoning.

</details>


### [13] [Towards Developmentally Plausible Rewards: Communicative Success as a Learning Signal for Interactive Language Models](https://arxiv.org/abs/2505.05970)
*Lennart Stöpler,Rufat Asadli,Mitja Nikolaus,Ryan Cotterell,Alex Warstadt*

Main category: cs.CL

TL;DR: 本文提出了一种受儿童语言学习启发的交互式语言模型训练方法。通过单轮对话中的奖励机制，研究发现虽然通信限制能改变说话者行为，但尚未提升语言评估表现，未来需改进任务设计和训练配置。


<details>
  <summary>Details</summary>
Motivation: 受儿童语言学习启发，研究如何在交互环境中训练语言模型，尤其是如何通过单轮对话中的奖励机制实现通信成功。

Method: 使用强化学习微调语言模型，通过抽象问答情境实现通信成功的信号传递，并测试其对语法性的间接影响。

Result: 研究发现通信限制能改变说话者行为，但未显著提升语言评估表现。

Conclusion: 虽然交互训练方法可行，但需进一步优化任务设计和训练配置以提升语言学习效果。

Abstract: We propose a method for training language models in an interactive setting
inspired by child language acquisition. In our setting, a speaker attempts to
communicate some information to a listener in a single-turn dialogue and
receives a reward if communicative success is achieved. Unlike earlier related
work using image--caption data for interactive reference games, we
operationalize communicative success in a more abstract language-only
question--answering setting. First, we present a feasibility study
demonstrating that our reward provides an indirect signal about grammaticality.
Second, we conduct experiments using reinforcement learning to fine-tune
language models. We observe that cognitively plausible constraints on the
communication channel lead to interpretable changes in speaker behavior.
However, we do not yet see improvements on linguistic evaluations from our
training regime. We outline potential modifications to the task design and
training configuration that could better position future work to use our
methodology to observe the benefits of interaction on language learning in
computational cognitive models.

</details>


### [14] [An Exploratory Analysis on the Explanatory Potential of Embedding-Based Measures of Semantic Transparency for Malay Word Recognition](https://arxiv.org/abs/2505.05973)
*M. Maziyah Mohamed,R. H. Baayen*

Main category: cs.CL

TL;DR: 该论文研究了语义透明度在词汇识别中的作用，提出并测试了五种基于词嵌入的语义透明度测量方法，并验证了这些方法对阅读反应时间的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨语义透明度的计算操作化及其对词汇识别的影响，特别是在马来语前缀词中的应用。

Method: 通过t-SNE聚类分析、线性判别分析和广义加性混合模型，基于词嵌入和位移向量测量语义透明度，并分析其对词汇决策反应时间的影响。

Result: 五种测量方法均能显著预测词汇决策反应时间，其中与词簇质心的相关性测量效果最佳。

Conclusion: 基于词嵌入的语义透明度测量方法有效，且质心相关性是最具预测力的指标，为形态加工研究提供了新工具。

Abstract: Studies of morphological processing have shown that semantic transparency is
crucial for word recognition. Its computational operationalization is still
under discussion. Our primary objectives are to explore embedding-based
measures of semantic transparency, and assess their impact on reading. First,
we explored the geometry of complex words in semantic space. To do so, we
conducted a t-distributed Stochastic Neighbor Embedding clustering analysis on
4,226 Malay prefixed words. Several clusters were observed for complex words
varied by their prefix class. Then, we derived five simple measures, and
investigated whether they were significant predictors of lexical decision
latencies. Two sets of Linear Discriminant Analyses were run in which the
prefix of a word is predicted from either word embeddings or shift vectors
(i.e., a vector subtraction of the base word from the derived word). The
accuracy with which the model predicts the prefix of a word indicates the
degree of transparency of the prefix. Three further measures were obtained by
comparing embeddings between each word and all other words containing the same
prefix (i.e., centroid), between each word and the shift from their base word,
and between each word and the predicted word of the Functional Representations
of Affixes in Compositional Semantic Space model. In a series of Generalized
Additive Mixed Models, all measures predicted decision latencies after
accounting for word frequency, word length, and morphological family size. The
model that included the correlation between each word and their centroid as a
predictor provided the best fit to the data.

</details>


### [15] [Exploring the Feasibility of Multilingual Grammatical Error Correction with a Single LLM up to 9B parameters: A Comparative Study of 17 Models](https://arxiv.org/abs/2505.06004)
*Dawid Wisniewski,Antoni Solarski,Artur Nowakowski*

Main category: cs.CL

TL;DR: 总结17种流行模型在英文、德文、意大利文和瑞典文的语法纠正任务中的表现，推荐6种在多语言任务中表现优秀的模型，并指出Gemma 9B是目前最佳选择。


<details>
  <summary>Details</summary>
Motivation: 探索单模型处理多语言语法纠正任务的性能，以解决多语言环境下的语法错误问题。

Method: 分析17种模型在四种语言中的输出，重点关注减少语法错误的同时最小化改动。

Result: 列出6种在所有四种语言中提升语法正确性的模型，Gemma 9B表现最佳。

Conclusion: 提供了多语言语法纠正任务的模型推荐，并指出当前最佳选择为Gemma 9B。

Abstract: Recent language models can successfully solve various language-related tasks,
and many understand inputs stated in different languages. In this paper, we
explore the performance of 17 popular models used to correct grammatical issues
in texts stated in English, German, Italian, and Swedish when using a single
model to correct texts in all those languages. We analyze the outputs generated
by these models, focusing on decreasing the number of grammatical errors while
keeping the changes small. The conclusions drawn help us understand what
problems occur among those models and which models can be recommended for
multilingual grammatical error correction tasks. We list six models that
improve grammatical correctness in all four languages and show that Gemma 9B is
currently the best performing one for the languages considered.

</details>


### [16] [Do Not Change Me: On Transferring Entities Without Modification in Neural Machine Translation -- a Multilingual Perspective](https://arxiv.org/abs/2505.06010)
*Dawid Wisniewski,Mikolaj Pokrywka,Zofia Rostek*

Main category: cs.CL

TL;DR: 论文研究了当前流行NMT模型在翻译时保留实体（如URL、IBAN号码等）的能力，分析了其错误原因，并提出一个新多语言合成数据集以评估实体传输质量。


<details>
  <summary>Details</summary>
Motivation: 尽管当前机器翻译模型在多数场景下表现优秀，但在处理特定实体（如URL、电子邮件等）时仍存在不足，因此需要评估和改进。

Method: 通过测试OPUS、Google Translate、MADLAD和EuroLLM等模型在英、德、波、乌四种语言间的实体保留能力，分析错误类型和原因，并提出新的合成数据集。

Result: 研究揭示了某些实体类别（如表情符号）对模型的挑战性，并展示了新数据集在评估实体传输质量上的潜力。

Conclusion: 论文强调了实体保留在机器翻译中的重要性，新数据集为未来研究提供了工具，并指出需要进一步改进模型在此方面的表现。

Abstract: Current machine translation models provide us with high-quality outputs in
most scenarios. However, they still face some specific problems, such as
detecting which entities should not be changed during translation. In this
paper, we explore the abilities of popular NMT models, including models from
the OPUS project, Google Translate, MADLAD, and EuroLLM, to preserve entities
such as URL addresses, IBAN numbers, or emails when producing translations
between four languages: English, German, Polish, and Ukrainian. We investigate
the quality of popular NMT models in terms of accuracy, discuss errors made by
the models, and examine the reasons for errors. Our analysis highlights
specific categories, such as emojis, that pose significant challenges for many
models considered. In addition to the analysis, we propose a new multilingual
synthetic dataset of 36,000 sentences that can help assess the quality of
entity transfer across nine categories and four aforementioned languages.

</details>


### [17] [Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation](https://arxiv.org/abs/2505.06027)
*Stefan Vasilev,Christian Herold,Baohao Liao,Seyyed Hadi Hashemi,Shahram Khadivi,Christof Monz*

Main category: cs.CL

TL;DR: Unilogit是一种新型自蒸馏方法，用于大型语言模型的机器遗忘。它通过动态调整目标logits来选择性遗忘特定信息，同时保持模型整体性能，优于现有方法如NPO和UnDIAL。


<details>
  <summary>Details</summary>
Motivation: 解决数据隐私法规（如GDPR）要求的选择性遗忘问题，同时不损害模型整体性能。

Method: 动态调整目标logits，利用当前模型输出生成更准确的自蒸馏目标，无需额外超参数。

Result: 在公共基准和内部电商数据集上表现优异，平衡遗忘与保留目标，优于现有方法。

Conclusion: Unilogit在多种场景下表现稳健，证明了其在机器遗忘任务中的实用性和有效性。

Abstract: This paper introduces Unilogit, a novel self-distillation method for machine
unlearning in Large Language Models. Unilogit addresses the challenge of
selectively forgetting specific information while maintaining overall model
utility, a critical task in compliance with data privacy regulations like GDPR.
Unlike prior methods that rely on static hyperparameters or starting model
outputs, Unilogit dynamically adjusts target logits to achieve a uniform
probability for the target token, leveraging the current model's outputs for
more accurate self-distillation targets. This approach not only eliminates the
need for additional hyperparameters but also enhances the model's ability to
approximate the golden targets. Extensive experiments on public benchmarks and
an in-house e-commerce dataset demonstrate Unilogit's superior performance in
balancing forget and retain objectives, outperforming state-of-the-art methods
such as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness
across various scenarios, highlighting its practical applicability and
effectiveness in achieving efficacious machine unlearning.

</details>


### [18] [Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health Information](https://arxiv.org/abs/2505.06046)
*Joshua Harris,Fan Grayson,Felix Feldman,Timothy Laurence,Toby Nonnenmacher,Oliver Higgins,Leo Loman,Selina Patel,Thomas Finnie,Samuel Collins,Michael Borowitz*

Main category: cs.CL

TL;DR: 论文提出新基准PubHealthBench，评估大型语言模型（LLMs）在公共健康领域的知识表现，发现顶尖模型在选择题回答中表现优异（>90%），但自由回答得分较低（<75%）。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，了解其在特定领域（如公共健康）的知识准确性至关重要，尤其是这些模型的错误可能对公众健康产生重大影响。

Method: 通过自动流程创建包含8000多个问题的PubHealthBench基准，评估24个LLMs在选择题和自由回答中的表现，并发布相关数据集。

Result: 顶尖LLMs（如GPT-4.5）在选择题中得分>90%，超越人类简单搜索引擎使用；但自由回答得分均低于75%。

Conclusion: 尽管SOTA LLMs在公共健康信息方面表现出潜力，但在自由回答场景仍需额外保障措施或工具以确保准确性。

Abstract: As Large Language Models (LLMs) become widely accessible, a detailed
understanding of their knowledge within specific domains becomes necessary for
successful real world use. This is particularly critical in public health,
where failure to retrieve relevant, accurate, and current information could
significantly impact UK residents. However, currently little is known about LLM
knowledge of UK Government public health information. To address this issue,
this paper introduces a new benchmark, PubHealthBench, with over 8000 questions
for evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form
responses to public health queries, created via an automated pipeline. We also
release a new dataset of the extracted UK Government public health guidance
documents used as source text for PubHealthBench. Assessing 24 LLMs on
PubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a
high degree of knowledge, achieving >90% in the MCQA setup, and outperform
humans with cursory search engine use. However, in the free form setup we see
lower performance with no model scoring >75%. Therefore, whilst there are
promising signs that state of the art (SOTA) LLMs are an increasingly accurate
source of public health information, additional safeguards or tools may still
be needed when providing free form responses on public health topics.

</details>


### [19] [Attention on Multiword Expressions: A Multilingual Study of BERT-based Models with Regard to Idiomaticity and Microsyntax](https://arxiv.org/abs/2505.06062)
*Iuliia Zaitova,Vitalii Hirak,Badr M. Abdullah,Dietrich Klakow,Bernd Möbius,Tania Avgustinova*

Main category: cs.CL

TL;DR: 分析显示，在特定任务上微调的BERT模型对多词表达（MWEs）的注意力分配受到任务类型（语义或句法）的显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨微调是否影响BERT模型对多词表达（MWEs）的注意力，以及这种注意力在语义和句法任务中的差异。

Method: 使用六种印欧语言的单语模型和数据集，比较预训练和微调后的BERT模型在MWEs上的注意力分数。

Result: 微调显著影响模型对MWEs的注意力分配，语义任务使模型对惯用语的注意力在各层更均匀，句法任务则增强低层对微句法单元的关注。

Conclusion: 微调方向（语义或句法）直接影响BERT模型对多词表达的关注模式，验证了任务类型对模型注意力机制的关键作用。

Abstract: This study analyzes the attention patterns of fine-tuned encoder-only models
based on the BERT architecture (BERT-based models) towards two distinct types
of Multiword Expressions (MWEs): idioms and microsyntactic units (MSUs). Idioms
present challenges in semantic non-compositionality, whereas MSUs demonstrate
unconventional syntactic behavior that does not conform to standard grammatical
categorizations. We aim to understand whether fine-tuning BERT-based models on
specific tasks influences their attention to MWEs, and how this attention
differs between semantic and syntactic tasks. We examine attention scores to
MWEs in both pre-trained and fine-tuned BERT-based models. We utilize
monolingual models and datasets in six Indo-European languages - English,
German, Dutch, Polish, Russian, and Ukrainian. Our results show that
fine-tuning significantly influences how models allocate attention to MWEs.
Specifically, models fine-tuned on semantic tasks tend to distribute attention
to idiomatic expressions more evenly across layers. Models fine-tuned on
syntactic tasks show an increase in attention to MSUs in the lower layers,
corresponding with syntactic processing requirements.

</details>


### [20] [Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models](https://arxiv.org/abs/2505.06110)
*Jugal Gajjar,Kaustik Ranaware*

Main category: cs.CL

TL;DR: 论文提出了一种基于Transformer的早期融合多模态情感分析方法，使用CMU-MOSEI数据集，整合文本、音频和视觉模态，取得了高准确率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 旨在探索多模态数据（文本、音频、视觉）在情感分析中的联合建模能力，验证早期融合策略及Transformer架构的有效性。

Method: 采用BERT编码器分别处理各模态数据，通过早期融合（拼接嵌入）整合特征，使用Adam优化器（lr=1e-4）、dropout（0.3）和早停法训练模型。

Result: 模型在测试集上达到7类分类准确率97.87%和F1分数0.9682，MAE为0.1060，表明对情感强度的预测精确。

Conclusion: Transformer架构结合早期融合能有效捕捉跨模态交互，未来可比较不同融合策略或提升模型可解释性。

Abstract: This project performs multimodal sentiment analysis using the CMU-MOSEI
dataset, using transformer-based models with early fusion to integrate text,
audio, and visual modalities. We employ BERT-based encoders for each modality,
extracting embeddings that are concatenated before classification. The model
achieves strong performance, with 97.87\% 7-class accuracy and a 0.9682
F1-score on the test set, demonstrating the effectiveness of early fusion in
capturing cross-modal interactions. The training utilized Adam optimization
(lr=1e-4), dropout (0.3), and early stopping to ensure generalization and
robustness. Results highlight the superiority of transformer architectures in
modeling multimodal sentiment, with a low MAE (0.1060) indicating precise
sentiment intensity prediction. Future work may compare fusion strategies or
enhance interpretability. This approach utilizes multimodal learning by
effectively combining linguistic, acoustic, and visual cues for sentiment
analysis.

</details>


### [21] [LLMs Get Lost In Multi-Turn Conversation](https://arxiv.org/abs/2505.06120)
*Philippe Laban,Hiroaki Hayashi,Yingbo Zhou,Jennifer Neville*

Main category: cs.CL

TL;DR: 这篇论文通过大规模实验比较了大型语言模型（LLM）在单轮和多轮对话中的表现，发现所有测试的LLM在多轮对话中性能平均下降39%，主要原因是早期假设错误导致无法恢复。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLM在多轮对话中的表现，因为现有评估多集中在单轮、明确指令场景，而用户在实际使用中常出现指令不完整或需要多次交互的情况。

Method: 方法包括大规模模拟实验，对比LLM在单轮和多轮对话中的性能，并分析了20万+模拟对话，将性能下降分解为能力轻微下降和不可靠性显著增加两部分。

Result: 结果显示，所有测试的LLM在多轮对话中性能平均下降39%，主要是由于模型在早期错误假设并过度依赖这些假设，导致后续对话无法纠正。

Conclusion: 结论指出，LLM在多轮对话中的性能下降显著，尤其是在早期错误决策后难以恢复，这为未来优化LLM的对话能力提供了方向。

Abstract: Large Language Models (LLMs) are conversational interfaces. As such, LLMs
have the potential to assist their users not only when they can fully specify
the task at hand, but also to help them define, explore, and refine what they
need through multi-turn conversational exchange. Although analysis of LLM
conversation logs has confirmed that underspecification occurs frequently in
user instructions, LLM evaluation has predominantly focused on the single-turn,
fully-specified instruction setting. In this work, we perform large-scale
simulation experiments to compare LLM performance in single- and multi-turn
settings. Our experiments confirm that all the top open- and closed-weight LLMs
we test exhibit significantly lower performance in multi-turn conversations
than single-turn, with an average drop of 39% across six generation tasks.
Analysis of 200,000+ simulated conversations decomposes the performance
degradation into two components: a minor loss in aptitude and a significant
increase in unreliability. We find that LLMs often make assumptions in early
turns and prematurely attempt to generate final solutions, on which they overly
rely. In simpler terms, we discover that *when LLMs take a wrong turn in a
conversation, they get lost and do not recover*.

</details>


### [22] [Towards Robust Few-Shot Text Classification Using Transformer Architectures and Dual Loss Strategies](https://arxiv.org/abs/2505.06145)
*Xu Han,Yumeng Sun,Weiqiang Huang,Hongye Zheng,Junliang Du*

Main category: cs.CL

TL;DR: 本文提出了一种结合自适应微调、对比学习和正则化优化的策略，以提升基于Transformer的模型在小样本文本分类任务中的表现。实验表明，该方法能有效提升分类准确性并缓解过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 在小样本环境中，传统方法难以捕捉文本特征并区分语义模糊的类别，因此需要结合多种技术提升模型性能。

Method: 采用自适应微调、对比学习和正则化优化策略，结合不同Transformer模型（如T5-small、DeBERTa-v3等）进行实验。

Result: 在FewRel 2.0数据集上，尤其在5-shot设置中，模型能更有效地学习文本特征并提高分类准确性，同时增强了泛化能力。

Conclusion: 通过结合对比损失和正则化损失，模型在小样本任务中表现更稳定且准确，验证了Transformer和生成架构的有效性。

Abstract: Few-shot text classification has important application value in low-resource
environments. This paper proposes a strategy that combines adaptive
fine-tuning, contrastive learning, and regularization optimization to improve
the classification performance of Transformer-based models. Experiments on the
FewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base perform
well in few-shot tasks, especially in the 5-shot setting, which can more
effectively capture text features and improve classification accuracy. The
experiment also found that there are significant differences in the
classification difficulty of different relationship categories. Some categories
have fuzzy semantic boundaries or complex feature distributions, making it
difficult for the standard cross entropy loss to learn the discriminative
information required to distinguish categories. By introducing contrastive loss
and regularization loss, the generalization ability of the model is enhanced,
effectively alleviating the overfitting problem in few-shot environments. In
addition, the research results show that the use of Transformer models or
generative architectures with stronger self-attention mechanisms can help
improve the stability and accuracy of few-shot classification.

</details>


### [23] [Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study](https://arxiv.org/abs/2505.06149)
*Faeze Ghorbanpour,Daryna Dementieva,Alexander Fraser*

Main category: cs.CL

TL;DR: 该研究评估了基于LLM提示的多语言仇恨言论检测方法，发现零样本和少样本提示在真实数据集上表现不如微调模型，但在泛化测试中表现更好，且提示设计对性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论检测方法多忽视语言多样性，而多语言大模型（如LLaMA、Aya等）在此领域的零样本和少样本性能尚未充分探索。

Method: 研究通过多种提示技术在八种非英语语言中评估LLM的零样本和少样本检测能力，并与微调编码器模型对比。

Result: 零样本和少样本提示在多数真实数据集中表现不及微调模型，但在泛化测试中更优，且提示设计需针对不同语言定制。

Conclusion: 提示设计对LLM在多语言仇恨言论检测中的性能至关重要，未来需进一步优化提示技术。

Abstract: Despite growing interest in automated hate speech detection, most existing
approaches overlook the linguistic diversity of online content. Multilingual
instruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ
offer promising capabilities across languages, but their effectiveness in
identifying hate speech through zero-shot and few-shot prompting remains
underexplored. This work evaluates LLM prompting-based detection across eight
non-English languages, utilizing several prompting techniques and comparing
them to fine-tuned encoder models. We show that while zero-shot and few-shot
prompting lag behind fine-tuned encoder models on most of the real-world
evaluation sets, they achieve better generalization on functional tests for
hate speech detection. Our study also reveals that prompt design plays a
critical role, with each language often requiring customized prompting
techniques to maximize performance.

</details>


### [24] [A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets](https://arxiv.org/abs/2505.06150)
*Ryan Lagasse,Aidan Kiernans,Avijit Ghosh,Shiri Dori-Hacohen*

Main category: cs.CL

TL;DR: 该论文提出了一个考虑数据组成的扩展定律，用于在固定计算预算下微调大语言模型（LLMs），发现数据量对模型性能有决定性影响。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅通过总标记数衡量训练数据，而论文表明数据量和平均标记长度（数据集体积）对模型性能至关重要，从而需要更精确的扩展定律。

Method: 通过实验在BRICC数据集和MMLU数据集的子集上验证，采用多种子采样策略评估数据组成对标记效率的影响。

Result: 实验结果表明，数据组成显著影响标记效率，验证了扩展定律的有效性。

Conclusion: 研究为资源受限环境下的LLM微调提供了更精确的扩展定律，强调了数据集体积的重要性。

Abstract: We introduce a scaling law for fine-tuning large language models (LLMs) under
fixed compute budgets that explicitly accounts for data composition.
Conventional approaches measure training data solely by total tokens, yet the
number of examples and their average token length -- what we term \emph{dataset
volume} -- play a decisive role in model performance. Our formulation is tuned
following established procedures. Experiments on the BRICC dataset
\cite{salavati2024reducing} and subsets of the MMLU dataset
\cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple
subsampling strategies, reveal that data composition significantly affects
token efficiency. These results motivate refined scaling laws for practical LLM
fine-tuning in resource-constrained settings.

</details>


### [25] [Estimating Quality in Therapeutic Conversations: A Multi-Dimensional Natural Language Processing Framework](https://arxiv.org/abs/2505.06151)
*Alice Rueda,Argyrios Perivolaris,Niloy Roy,Dylan Weston,Sarmed Shaya,Zachary Cote,Martin Ivanov,Bazen G. Teferra,Yuqi Wu,Sirisha Rambhatla,Divya Sharma,Andrew Greenshaw,Rakesh Jetly,Yanbo Zhang,Bo Cao,Reza Samavi,Sridhar Krishnan,Venkat Bhat*

Main category: cs.CL

TL;DR: 论文提出了一种基于自然语言处理的多维框架，用于客观分类心理治疗对话中的参与质量，并在增强数据上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解和改善心理治疗中客户与治疗师的互动质量，这对治疗效果至关重要。

Method: 方法包括从253个治疗对话文本中提取42个特征，使用Random Forest、Cat-Boost和SVM等分类器进行训练和评估，并通过数据增强提升性能。

Result: 结果显示，Random Forest在增强数据上的准确率达88.9%，AUC达94.6%，会话动态和语义相似性是关键特征。

Conclusion: 结论是该框架可扩展且数据驱动，能实时反馈治疗质量，未来可扩展至多模态评估。

Abstract: Engagement between client and therapist is a critical determinant of
therapeutic success. We propose a multi-dimensional natural language processing
(NLP) framework that objectively classifies engagement quality in counseling
sessions based on textual transcripts. Using 253 motivational interviewing
transcripts (150 high-quality, 103 low-quality), we extracted 42 features
across four domains: conversational dynamics, semantic similarity as topic
alignment, sentiment classification, and question detection. Classifiers,
including Random Forest (RF), Cat-Boost, and Support Vector Machines (SVM),
were hyperparameter tuned and trained using a stratified 5-fold
cross-validation and evaluated on a holdout test set. On balanced
(non-augmented) data, RF achieved the highest classification accuracy (76.7%),
and SVM achieved the highest AUC (85.4%). After SMOTE-Tomek augmentation,
performance improved significantly: RF achieved up to 88.9% accuracy, 90.0%
F1-score, and 94.6% AUC, while SVM reached 81.1% accuracy, 83.1% F1-score, and
93.6% AUC. The augmented data results reflect the potential of the framework in
future larger-scale applications. Feature contribution revealed conversational
dynamics and semantic similarity between clients and therapists were among the
top contributors, led by words uttered by the client (mean and standard
deviation). The framework was robust across the original and augmented datasets
and demonstrated consistent improvements in F1 scores and recall. While
currently text-based, the framework supports future multimodal extensions
(e.g., vocal tone, facial affect) for more holistic assessments. This work
introduces a scalable, data-driven method for evaluating engagement quality of
the therapy session, offering clinicians real-time feedback to enhance the
quality of both virtual and in-person therapeutic interactions.

</details>


### [26] [Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies](https://arxiv.org/abs/2505.06186)
*Massimiliano Pronesti,Joao Bettencourt-Silva,Paul Flanagan,Alessandra Pascale,Oisin Redmond,Anya Belz,Yufang Hou*

Main category: cs.CL

TL;DR: 该论文提出了一个名为URCA的检索增强生成框架，用于从生物医学研究中提取科学证据，并创建了一个名为CochraneForest的数据集来支持这一任务。实验结果表明URCA在F1得分上比现有方法高出10.3%，但也凸显了该任务的复杂性。


<details>
  <summary>Details</summary>
Motivation: 解决从生物医学研究中提取科学证据的任务，尤其是在证据冲突的临床问题中，这对综合生物医学证据至关重要。

Method: 创建了CochraneForest数据集，并提出了URCA框架，这是一个检索增强的生成方法，专门针对证据提取任务的独特挑战。

Result: URCA在实验中表现优于现有方法，F1得分提高了10.3%，但任务复杂度较高，表明仍需进一步研究。

Conclusion: URCA为自动化证据合成系统提供了有前景的解决方案，但CochraneForest数据集的复杂性为未来研究设立了高标准的测试平台。

Abstract: Extracting scientific evidence from biomedical studies for clinical research
questions (e.g., Does stem cell transplantation improve quality of life in
patients with medically refractory Crohn's disease compared to placebo?) is a
crucial step in synthesising biomedical evidence. In this paper, we focus on
the task of document-level scientific evidence extraction for clinical
questions with conflicting evidence. To support this task, we create a dataset
called CochraneForest, leveraging forest plots from Cochrane systematic
reviews. It comprises 202 annotated forest plots, associated clinical research
questions, full texts of studies, and study-specific conclusions. Building on
CochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a
retrieval-augmented generation framework designed to tackle the unique
challenges of evidence extraction. Our experiments show that URCA outperforms
the best existing methods by up to 10.3% in F1 score on this task. However, the
results also underscore the complexity of CochraneForest, establishing it as a
challenging testbed for advancing automated evidence synthesis systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [27] [Continuous Thought Machines](https://arxiv.org/abs/2505.05522)
*Luke Darlow,Ciaran Regan,Sebastian Risi,Jeffrey Seely,Llion Jones*

Main category: cs.LG

TL;DR: 论文提出一种名为CTM的模型，通过引入神经元级时间处理和神经同步来模拟生物大脑的时序动态，平衡计算效率与生物真实性，在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 挑战深度学习忽略神经时序动态的范式，通过模拟生物神经活动提升模型的生物真实性和表现力。

Method: 提出CTM模型，核心创新为神经元级时间处理和神经同步作为潜在表征。

Result: CTM在ImageNet分类、迷宫求解、问答等任务中表现优异，支持自适应计算。

Conclusion: CTM是向生物合理且强大AI系统迈进的重要一步，但目标是分享创新而非追求新SOTA。

Abstract: Biological brains demonstrate complex neural activity, where the timing and
interplay between neurons is critical to how brains process information. Most
deep learning architectures simplify neural activity by abstracting away
temporal dynamics. In this paper we challenge that paradigm. By incorporating
neuron-level processing and synchronization, we can effectively reintroduce
neural timing as a foundational element. We present the Continuous Thought
Machine (CTM), a model designed to leverage neural dynamics as its core
representation. The CTM has two core innovations: (1) neuron-level temporal
processing, where each neuron uses unique weight parameters to process a
history of incoming signals; and (2) neural synchronization employed as a
latent representation. The CTM aims to strike a balance between oversimplified
neuron abstractions that improve computational efficiency, and biological
realism. It operates at a level of abstraction that effectively captures
essential temporal dynamics while remaining computationally tractable for deep
learning. We demonstrate the CTM's strong performance and versatility across a
range of challenging tasks, including ImageNet-1K classification, solving 2D
mazes, sorting, parity computation, question-answering, and RL tasks. Beyond
displaying rich internal representations and offering a natural avenue for
interpretation owing to its internal process, the CTM is able to perform tasks
that require complex sequential reasoning. The CTM can also leverage adaptive
compute, where it can stop earlier for simpler tasks, or keep computing when
faced with more challenging instances. The goal of this work is to share the
CTM and its associated innovations, rather than pushing for new
state-of-the-art results. To that end, we believe the CTM represents a
significant step toward developing more biologically plausible and powerful
artificial intelligence systems.

</details>


### [28] [A critical assessment of reinforcement learning methods for microswimmer navigation in complex flows](https://arxiv.org/abs/2505.05525)
*Selim Mecanna,Aurore Loisy,Christophe Eloy*

Main category: cs.LG

TL;DR: 该论文评估了强化学习方法在部分可观测流中的导航性能，发现常用算法（如Q-Learning和A2C）表现不佳，而PPO算法则表现出色，接近理论最优性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在定量评估强化学习方法在流体流动导航中的表现，以解决小型浮游生物或自主机器人在海洋中的导航问题，并验证算法选择与优化的关键性。

Method: 通过引入一个方向导航的明确问题，比较Q-Learning、A2C和PPO在多种流动（如Taylor-Green涡流和二维湍流）中的性能，并采用向量化环境和广义优势估计等技术优化PPO。

Result: PPO在复杂流中表现出接近理论最优的导航性能，显著优于Q-Learning和A2C，且具有更好的鲁棒性。

Conclusion: 算法选择、实现细节和超参数优化对于复杂流中的智能导航策略至关重要，PPO是当前最优解决方案。

Abstract: Navigating in a fluid flow while being carried by it, using only information
accessible from on-board sensors, is a problem commonly faced by small
planktonic organisms. It is also directly relevant to autonomous robots
deployed in the oceans. In the last ten years, the fluid mechanics community
has widely adopted reinforcement learning, often in the form of its simplest
implementations, to address this challenge. But it is unclear how good are the
strategies learned by these algorithms. In this paper, we perform a
quantitative assessment of reinforcement learning methods applied to navigation
in partially observable flows. We first introduce a well-posed problem of
directional navigation for which a quasi-optimal policy is known analytically.
We then report on the poor performance and robustness of commonly used
algorithms (Q-Learning, Advantage Actor Critic) in flows regularly encountered
in the literature: Taylor-Green vortices, Arnold-Beltrami-Childress flow, and
two-dimensional turbulence. We show that they are vastly surpassed by PPO
(Proximal Policy Optimization), a more advanced algorithm that has established
dominance across a wide range of benchmarks in the reinforcement learning
community. In particular, our custom implementation of PPO matches the
theoretical quasi-optimal performance in turbulent flow and does so in a robust
manner. Reaching this result required the use of several additional techniques,
such as vectorized environments and generalized advantage estimation, as well
as hyperparameter optimization. This study demonstrates the importance of
algorithm selection, implementation details, and fine-tuning for discovering
truly smart autonomous navigation strategies in complex flows.

</details>


### [29] [ADMM-Based Training for Spiking Neural Networks](https://arxiv.org/abs/2505.05527)
*Giovanni Perin,Cesare Bidini,Riccardo Mazzieri,Michele Rossi*

Main category: cs.LG

TL;DR: 提出了一种基于ADMM的新型SNN训练方法，解决传统反向传播方法在SNN训练中的局限性，包括可扩展性差和数值不精确的问题。


<details>
  <summary>Details</summary>
Motivation: 传统SNN训练方法（如带替代梯度的反向传播）存在可扩展性低和数值不精确的问题，因此需要一种更高效的训练算法。

Method: 采用交替方向乘子法（ADMM）训练SNN，通过问题形式化和闭式更新解决SNN阶跃函数的不可微性问题。

Result: 实验验证了ADMM优化器的收敛性和潜力，并展示了改进该方法的新研究方向。

Conclusion: ADMM为SNN训练提供了有效方案，未来研究可进一步优化其性能。

Abstract: In recent years, spiking neural networks (SNNs) have gained momentum due to
their high potential in time-series processing combined with minimal energy
consumption. However, they still lack a dedicated and efficient training
algorithm. The popular backpropagation with surrogate gradients, adapted from
stochastic gradient descent (SGD)-derived algorithms, has several drawbacks
when used as an optimizer for SNNs. Specifically, it suffers from low
scalability and numerical imprecision. In this paper, we propose a novel SNN
training method based on the alternating direction method of multipliers
(ADMM). Our ADMM-based training aims to solve the problem of the SNN step
function's non-differentiability. We formulate the problem, derive closed-form
updates, and empirically show the optimizer's convergence properties, great
potential, and possible new research directions to improve the method in a
simulated proof-of-concept.

</details>


### [30] [Low-bit Model Quantization for Deep Neural Networks: A Survey](https://arxiv.org/abs/2505.05530)
*Kai Liu,Qian Zheng,Kaiwen Tao,Zhiteng Li,Haotong Qin,Wenbo Li,Yong Guo,Xianglong Liu,Linghe Kong,Guihai Chen,Yulun Zhang,Xiaokang Yang*

Main category: cs.LG

TL;DR: 该论文综述了过去五年深度神经网络（DNN）低比特量化领域的研究进展，对现有量化方法进行了分类和比较，并探讨了潜在研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管DNN在许多领域取得了显著成功，但其高计算成本和模型大小限制了实际部署。量化技术能有效减少计算开销和存储需求，但会导致精度损失。因此，研究如何高效量化和弥补精度损失变得至关重要。

Method: 论文对现有量化方法进行了系统分类，分为8个主要类别和24个子类别，并提供了详细的比较分析。

Result: 通过对量化方法的分类和比较，论文总结了当前研究的核心技术和趋势，并提供了一个开源项目列表供参考。

Conclusion: 量化技术是DNN部署的关键环节，未来研究应关注如何进一步优化量化效率和精度平衡，同时探索新的研究方向。

Abstract: With unprecedented rapid development, deep neural networks (DNNs) have deeply
influenced almost all fields. However, their heavy computation costs and model
sizes are usually unacceptable in real-world deployment. Model quantization, an
effective weight-lighting technique, has become an indispensable procedure in
the whole deployment pipeline. The essence of quantization acceleration is the
conversion from continuous floating-point numbers to discrete integer ones,
which significantly speeds up the memory I/O and calculation, i.e., addition
and multiplication. However, performance degradation also comes with the
conversion because of the loss of precision. Therefore, it has become
increasingly popular and critical to investigate how to perform the conversion
and how to compensate for the information loss. This article surveys the recent
five-year progress towards low-bit quantization on DNNs. We discuss and compare
the state-of-the-art quantization methods and classify them into 8 main
categories and 24 sub-categories according to their core techniques.
Furthermore, we shed light on the potential research opportunities in the field
of model quantization. A curated list of model quantization is provided at
https://github.com/Kai-Liu001/Awesome-Model-Quantization.

</details>


### [31] [Rethinking Graph Contrastive Learning through Relative Similarity Preservation](https://arxiv.org/abs/2505.05533)
*Zhiyuan Ning,Pengfei Wang,Ziyue Qiao,Pengyang Wang,Yuanchun Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种新的图对比学习框架RELGCL，利用图中结构距离与标签一致性的自然关系，通过相对相似性目标提升性能，超越了传统的绝对相似性方法。


<details>
  <summary>Details</summary>
Motivation: 传统的图对比学习沿用计算机视觉中的绝对相似性方法，但忽视了图的离散、非欧几里得特性，导致视图生成破坏语义有效性且相似性验证不可靠。

Method: 通过分析11个真实图数据，发现标签一致性随结构距离增加的衰减模式，提出RELGCL框架，包含成对和列表两种实现方式，利用集体相似性目标保留图的自然相对相似性。

Result: 实验表明，RELGCL在20种现有方法中表现最优，无论是同质性还是异质性图均验证了其有效性。

Conclusion: 相比人工定义的绝对相似性，利用图中自然的相对相似性模式能更有效地提升图对比学习性能，推动了该领域的发展。

Abstract: Graph contrastive learning (GCL) has achieved remarkable success by following
the computer vision paradigm of preserving absolute similarity between
augmented views. However, this approach faces fundamental challenges in graphs
due to their discrete, non-Euclidean nature -- view generation often breaks
semantic validity and similarity verification becomes unreliable. Through
analyzing 11 real-world graphs, we discover a universal pattern transcending
the homophily-heterophily dichotomy: label consistency systematically
diminishes as structural distance increases, manifesting as smooth decay in
homophily graphs and oscillatory decay in heterophily graphs. We establish
theoretical guarantees for this pattern through random walk theory, proving
label distribution convergence and characterizing the mechanisms behind
different decay behaviors. This discovery reveals that graphs naturally encode
relative similarity patterns, where structurally closer nodes exhibit
collectively stronger semantic relationships. Leveraging this insight, we
propose RELGCL, a novel GCL framework with complementary pairwise and listwise
implementations that preserve these inherent patterns through collective
similarity objectives. Extensive experiments demonstrate that our method
consistently outperforms 20 existing approaches across both homophily and
heterophily graphs, validating the effectiveness of leveraging natural relative
similarity over artificial absolute similarity.

</details>


### [32] [Cardioformer: Advancing AI in ECG Analysis with Multi-Granularity Patching and ResNet](https://arxiv.org/abs/2505.05538)
*Md Kamrujjaman Mobin,Md Saiful Islam,Sadik Al Barid,Md Masum*

Main category: cs.LG

TL;DR: Cardioformer 是一个创新的多粒度混合模型，通过跨通道分块、分层残差学习和双阶段自注意力机制，解决了心电图分类中局部形态细节和长程时间依赖性的挑战。该模型在三个基准数据集上表现优异，泛化能力突出。


<details>
  <summary>Details</summary>
Motivation: 现有的心电图分类方法难以同时捕捉局部形态细节和长程时间依赖性，为此开发了Cardioformer来填补这一技术空白。

Method: 采用跨通道分块、分层残差学习和双阶段自注意力机制，结合多尺度标记嵌入来捕获细粒度局部特征和全局上下文信息。

Result: 在MIMIC-IV、PTB-XL和PTB数据集上，Cardioformer的AUROC分别达到96.34±0.11、89.99±0.12和95.59±1.66，优于PatchTST、Reformer、Transformer和Medformer。同时还展示了强大的跨数据集泛化能力。

Conclusion: Cardioformer显著提升了心电图自动分析的性能，为心血管疾病的准确和稳健诊断铺平了道路。源代码已公开。

Abstract: Electrocardiogram (ECG) classification is crucial for automated cardiac
disease diagnosis, yet existing methods often struggle to capture local
morphological details and long-range temporal dependencies simultaneously. To
address these challenges, we propose Cardioformer, a novel multi-granularity
hybrid model that integrates cross-channel patching, hierarchical residual
learning, and a two-stage self-attention mechanism. Cardioformer first encodes
multi-scale token embeddings to capture fine-grained local features and global
contextual information and then selectively fuses these representations through
intra- and inter-granularity self-attention. Extensive evaluations on three
benchmark ECG datasets under subject-independent settings demonstrate that
model consistently outperforms four state-of-the-art baselines. Our
Cardioformer model achieves the AUROC of 96.34$\pm$0.11, 89.99$\pm$0.12, and
95.59$\pm$1.66 in MIMIC-IV, PTB-XL and PTB dataset respectively outperforming
PatchTST, Reformer, Transformer, and Medformer models. It also demonstrates
strong cross-dataset generalization, achieving 49.18% AUROC on PTB and 68.41%
on PTB-XL when trained on MIMIC-IV. These findings underscore the potential of
Cardioformer to advance automated ECG analysis, paving the way for more
accurate and robust cardiovascular disease diagnosis. We release the source
code at https://github.com/KMobin555/Cardioformer.

</details>


### [33] [Griffin: Towards a Graph-Centric Relational Database Foundation Model](https://arxiv.org/abs/2505.05568)
*Yanbo Wang,Xiyuan Wang,Quan Gan,Minjie Wang,Qibin Yang,David Wipf,Muhan Zhang*

Main category: cs.LG

TL;DR: Griffin是一个专为关系数据库设计的首个基础模型，通过统一数据编码器和任务解码器处理多样化任务，并在架构中加入交叉注意力模块和新型聚合器。预训练采用单表和多表数据集，利用高级编码器和创新组件捕捉关系数据的复杂性。在大规模异构时态图上的评估显示，Griffin性能优于或接近独立训练的模型，在低数据场景下表现优异，并展现出强迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型多为针对单一关系数据库任务的小模型，缺乏统一处理多样化任务的能力。Griffin旨在填补这一空白，提供一种通用的基础模型，以高效处理关系数据库中的复杂任务。

Method: Griffin采用统一的数据编码器和任务解码器架构，结合交叉注意力模块和新型聚合器。预训练阶段使用单表和多表数据集，利用高级编码器处理分类、数值和元数据特征，同时引入创新的交叉注意力模块和改进的消息传递神经网络（MPNNs）以捕捉关系数据的复杂性。

Result: 在大规模异构时态图上的实验表明，Griffin在性能上优于或与独立训练的模型相当，尤其在低数据场景下表现突出。此外，模型显示出强大的迁移能力，能够在新数据集和任务上快速适应。

Conclusion: Griffin作为关系数据库的首个基础模型，展现出通用性和高效性，为解决多样化任务提供了新思路。其创新架构和预训练策略为未来研究奠定了基础。

Abstract: We introduce Griffin, the first foundation model attemptation designed
specifically for Relational Databases (RDBs). Unlike previous smaller models
focused on single RDB tasks, Griffin unifies the data encoder and task decoder
to handle diverse tasks. Additionally, we enhance the architecture by
incorporating a cross-attention module and a novel aggregator. Griffin utilizes
pretraining on both single-table and RDB datasets, employing advanced encoders
for categorical, numerical, and metadata features, along with innovative
components such as cross-attention modules and enhanced message-passing neural
networks (MPNNs) to capture the complexities of relational data. Evaluated on
large-scale, heterogeneous, and temporal graphs extracted from RDBs across
various domains (spanning over 150 million nodes), Griffin demonstrates
superior or comparable performance to individually trained models, excels in
low-data scenarios, and shows strong transferability with similarity and
diversity in pretraining across new datasets and tasks, highlighting its
potential as a universally applicable foundation model for RDBs. Code available
at https://github.com/yanxwb/Griffin.

</details>


### [34] [PyTDC: A multimodal machine learning training, evaluation, and inference platform for biomedical foundation models](https://arxiv.org/abs/2505.05577)
*Alejandro Velez-Arce,Marinka Zitnik*

Main category: cs.LG

TL;DR: PyTDC 是一个开源的机器学习平台，旨在整合多模态生物数据并提供训练、评估和推理的端到端基础设施。它首次引入了单细胞药物靶点提名任务，并展示了现有方法在该任务上的局限性，为生物医学 AI 的未来研究提供了方向。


<details>
  <summary>Details</summary>
Motivation: 现有的生物医学基准测试缺乏整合多模态生物数据和多样化机器学习任务的端到端基础设施，PyTDC 旨在填补这一空白，推动生物医学 AI 的发展。

Method: 设计了 PyTDC 平台，整合分布式、异构和持续更新的数据源和模型权重，并标准化了基准测试和推理端点。通过一个首创的单细胞药物靶点提名任务进行案例研究。

Result: 现有图表示学习和领域特定方法在该任务上表现不佳，但一种上下文感知的几何深度学习方法表现优于基线方法。然而，该方法无法泛化到未知细胞类型或整合额外模态。

Conclusion: PyTDC 能够促进多模态、上下文感知的基础模型的发展，为解决生物医学 AI 中的开放问题提供了新的研究方向。

Abstract: Existing biomedical benchmarks do not provide end-to-end infrastructure for
training, evaluation, and inference of models that integrate multimodal
biological data and a broad range of machine learning tasks in therapeutics. We
present PyTDC, an open-source machine-learning platform providing streamlined
training, evaluation, and inference software for multimodal biological AI
models. PyTDC unifies distributed, heterogeneous, continuously updated data
sources and model weights and standardizes benchmarking and inference
endpoints. This paper discusses the components of PyTDC's architecture and, to
our knowledge, the first-of-its-kind case study on the introduced single-cell
drug-target nomination ML task. We find state-of-the-art methods in graph
representation learning and domain-specific methods from graph theory perform
poorly on this task. Though we find a context-aware geometric deep learning
method that outperforms the evaluated SoTA and domain-specific baseline
methods, the model is unable to generalize to unseen cell types or incorporate
additional modalities, highlighting PyTDC's capacity to facilitate an exciting
avenue of research developing multimodal, context-aware, foundation models for
open problems in biomedical AI.

</details>


### [35] [Anticipating Gaming to Incentivize Improvement: Guiding Agents in (Fair) Strategic Classification](https://arxiv.org/abs/2505.05594)
*Sura Alhanouti,Parinaz Naghizadeh*

Main category: cs.LG

TL;DR: 该论文研究了人类在面对机器学习算法决策时的策略行为，包括提升自身能力（改进）或操纵特征（欺骗）的选择，并通过博弈论模型分析了算法设计者如何引导这些行为以促进公平。


<details>
  <summary>Details</summary>
Motivation: 动机在于理解人类如何应对日益影响关键决策的机器学习算法，并探讨算法设计者如何通过策略性设计激励改进而非操纵行为，从而促进系统公平性。

Method: 方法是通过Stackelberg博弈模型，将公司部署（公平）分类器与个体策略性反应相结合，并考虑了操纵和改进的不同成本及随机效果。

Result: 结果表明，代理人的反应可分为不同类别，并据此优化分类器设计；战略性策略不仅能防止操纵，还能激励代理人选择改进。

Conclusion: 结论强调了公司预见策略行为的重要性，指出合理的战略政策可以促进公平并激励积极行为。

Abstract: As machine learning algorithms increasingly influence critical decision
making in different application areas, understanding human strategic behavior
in response to these systems becomes vital. We explore individuals' choice
between genuinely improving their qualifications (``improvement'') vs.
attempting to deceive the algorithm by manipulating their features
(``manipulation'') in response to an algorithmic decision system. We further
investigate an algorithm designer's ability to shape these strategic responses,
and its fairness implications. Specifically, we formulate these interactions as
a Stackelberg game, where a firm deploys a (fair) classifier, and individuals
strategically respond. Our model incorporates both different costs and
stochastic efficacy for manipulation and improvement. The analysis reveals
different potential classes of agent responses, and characterizes optimal
classifiers accordingly. Based on these, we highlight the impact of the firm's
anticipation of strategic behavior, identifying when and why a (fair) strategic
policy can not only prevent manipulation, but also incentivize agents to opt
for improvement.

</details>


### [36] [This part looks alike this: identifying important parts of explained instances and prototypes](https://arxiv.org/abs/2505.05597)
*Jacek Karolczak,Jerzy Stefanowski*

Main category: cs.LG

TL;DR: 提出新方法识别原型中最相关特征，提高用户理解且保持预测准确性


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的解释方法未能有效突出关键特征，需改进以增强用户理解

Method: 结合特征重要性分数强调实例与原型重叠特征，并优化原型选择算法以提升多样性

Result: 在六个基准数据集上验证，方法提升了用户理解且未降低预测准确率

Conclusion: 通过突出相关特征和原型多样性，新方法在解释性与模型性能间取得平衡

Abstract: Although prototype-based explanations provide a human-understandable way of
representing model predictions they often fail to direct user attention to the
most relevant features. We propose a novel approach to identify the most
informative features within prototypes, termed alike parts. Using feature
importance scores derived from an agnostic explanation method, it emphasizes
the most relevant overlapping features between an instance and its nearest
prototype. Furthermore, the feature importance score is incorporated into the
objective function of the prototype selection algorithms to promote global
prototypes diversity. Through experiments on six benchmark datasets, we
demonstrate that the proposed approach improves user comprehension while
maintaining or even increasing predictive accuracy.

</details>


### [37] [The Evolution of Embedding Table Optimization and Multi-Epoch Training in Pinterest Ads Conversion](https://arxiv.org/abs/2505.05605)
*Andrew Qiu,Shubham Barhate,Hin Wai Lui,Runze Su,Rafael Rios Müller,Kungang Li,Ling Leng,Han Sun,Shayan Ehsani,Zhifang Liu*

Main category: cs.LG

TL;DR: 本文总结了Pinterest广告转化模型中嵌入表优化和多轮训练的实践经验，提出了频率自适应学习率方法来解决多轮过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在在线广告转化预测中广泛应用，但高基数分类特征的嵌入表训练存在梯度稀疏、标签稀疏等问题，导致收敛慢和过拟合。

Method: 研究了嵌入表优化和多轮训练技术，提出频率自适应学习率方法，并与嵌入重新初始化方法进行比较。

Result: 实验表明，提出的Sparse Optimizer加速了收敛，频率自适应学习率能有效缓解多轮过拟合，尤其是在标签稀疏的任务中。

Conclusion: 频率自适应学习率是一种有效解决嵌入表多轮过拟合的方法，适用于工业级大规模数据集。

Abstract: Deep learning for conversion prediction has found widespread applications in
online advertising. These models have become more complex as they are trained
to jointly predict multiple objectives such as click, add-to-cart, checkout and
other conversion types. Additionally, the capacity and performance of these
models can often be increased with the use of embedding tables that encode high
cardinality categorical features such as advertiser, user, campaign, and
product identifiers (IDs). These embedding tables can be pre-trained, but also
learned end-to-end jointly with the model to directly optimize the model
objectives. Training these large tables is challenging due to: gradient
sparsity, the high cardinality of the categorical features, the non-uniform
distribution of IDs and the very high label sparsity. These issues make
training prone to both slow convergence and overfitting after the first epoch.
Previous works addressed the multi-epoch overfitting issue by using: stronger
feature hashing to reduce cardinality, filtering of low frequency IDs,
regularization of the embedding tables, re-initialization of the embedding
tables after each epoch, etc. Some of these techniques reduce overfitting at
the expense of reduced model performance if used too aggressively. In this
paper, we share key learnings from the development of embedding table
optimization and multi-epoch training in Pinterest Ads Conversion models. We
showcase how our Sparse Optimizer speeds up convergence, and how multi-epoch
overfitting varies in severity between different objectives in a multi-task
model depending on label sparsity. We propose a new approach to deal with
multi-epoch overfitting: the use of a frequency-adaptive learning rate on the
embedding tables and compare it to embedding re-initialization. We evaluate
both methods offline using an industrial large-scale production dataset.

</details>


### [38] [On Corruption-Robustness in Performative Reinforcement Learning](https://arxiv.org/abs/2505.05609)
*Vasilis Pollatos,Debmalya Mandal,Goran Radanovic*

Main category: cs.LG

TL;DR: 该论文提出了一种在数据被污染的情况下进行强化学习的方法，通过凸凹优化和鲁棒梯度估计，实现了近似稳定策略的收敛。


<details>
  <summary>Details</summary>
Motivation: 研究在数据被污染的情况下，如何扩展重复重训练方法以保持强化学习的稳定性。

Method: 采用凸凹优化处理被污染的梯度，并设计了一种问题特定的鲁棒均值估计器。

Result: 理论证明方法能收敛到近似稳定策略，误差与污染程度的平方根成正比。

Conclusion: 实验证实了考虑数据污染对强化学习的重要性，提出的方法有效。

Abstract: In performative Reinforcement Learning (RL), an agent faces a
policy-dependent environment: the reward and transition functions depend on the
agent's policy. Prior work on performative RL has studied the convergence of
repeated retraining approaches to a performatively stable policy. In the finite
sample regime, these approaches repeatedly solve for a saddle point of a
convex-concave objective, which estimates the Lagrangian of a regularized
version of the reinforcement learning problem. In this paper, we aim to extend
such repeated retraining approaches, enabling them to operate under corrupted
data. More specifically, we consider Huber's $\epsilon$-contamination model,
where an $\epsilon$ fraction of data points is corrupted by arbitrary
adversarial noise. We propose a repeated retraining approach based on
convex-concave optimization under corrupted gradients and a novel
problem-specific robust mean estimator for the gradients. We prove that our
approach exhibits last-iterate convergence to an approximately stable policy,
with the approximation error linear in $\sqrt{\epsilon}$. We experimentally
demonstrate the importance of accounting for corruption in performative RL.

</details>


### [39] [SPIN-ODE: Stiff Physics-Informed Neural ODE for Chemical Reaction Rate Estimation](https://arxiv.org/abs/2505.05625)
*Wenqing Peng,Zhi-Song Liu,Michael Boy*

Main category: cs.LG

TL;DR: 为了解决复杂化学反应中速率常数估计的难题，作者提出了SPIN-ODE框架，通过三阶段优化过程学习化学反应的连续轨迹，提取速率系数，并进一步优化估计结果。


<details>
  <summary>Details</summary>
Motivation: 真实大气化学反应中的刚性导致基于学习的方法在速率常数估计中面临训练不稳定和收敛困难的问题，亟需新的解决方案。

Method: 1. 使用潜在神经ODE学习化学浓度及其时间导数的连续轨迹；2. 通过化学反应神经网络（CRNN）提取速率系数；3. 通过神经ODE求解器微调CRNN以提升估计精度。

Result: 在合成和真实数据集上的实验验证了SPIN-ODE的有效性和鲁棒性。

Conclusion: 作为首个针对刚性神经ODE的化学速率系数发现研究，该工作为神经网络与详细化学的整合开辟了有前景的方向。

Abstract: Estimating rate constants from complex chemical reactions is essential for
advancing detailed chemistry. However, the stiffness inherent in real-world
atmospheric chemistry systems poses severe challenges, leading to training
instability and poor convergence that hinder effective rate constant estimation
using learning-based approaches. To address this, we propose a Stiff
Physics-Informed Neural ODE framework (SPIN-ODE) for chemical reaction
modelling. Our method introduces a three-stage optimisation process: first, a
latent neural ODE learns the continuous and differentiable trajectory between
chemical concentrations and their time derivatives; second, an explicit
Chemical Reaction Neural Network (CRNN) extracts the underlying rate
coefficients based on the learned dynamics; and third, fine-tune CRNN using a
neural ODE solver to further improve rate coefficient estimation. Extensive
experiments on both synthetic and newly proposed real-world datasets validate
the effectiveness and robustness of our approach. As the first work on stiff
Neural ODEs for chemical rate coefficient discovery, our study opens promising
directions for integrating neural networks with detailed chemistry.

</details>


### [40] [EquiHGNN: Scalable Rotationally Equivariant Hypergraph Neural Networks](https://arxiv.org/abs/2505.05650)
*Tien Dang,Truong-Son Hy*

Main category: cs.LG

TL;DR: 该论文提出了EquiHGNN，一种基于超图的多分子相互作用建模框架，利用对称性感知表示提升分子建模效果，特别在大分子上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统基于图的模型只能处理二元关系，无法充分捕捉分子间复杂的高阶相互作用。超图通过多边连接扩展了图的表示能力，更适合建模复杂分子系统

Method: 提出了EquiHGNN框架，引入等变超图神经网络，通过强制相关变换群下的等变性来保持几何和拓扑特性，构建更具鲁棒性的物理意义表示

Result: 实验表明对称性约束带来了显著性能提升。高阶相互作用对小分子帮助有限，但能持续超越二维图模型在大分子上的表现。加入几何特征进一步提升了性能

Conclusion: 空间信息在分子学习中具有重要价值，结合高阶结构和几何特征的等变超图网络能更有效地建模复杂分子系统，特别是大分子

Abstract: Molecular interactions often involve high-order relationships that cannot be
fully captured by traditional graph-based models limited to pairwise
connections. Hypergraphs naturally extend graphs by enabling multi-way
interactions, making them well-suited for modeling complex molecular systems.
In this work, we introduce EquiHGNN, an Equivariant HyperGraph Neural Network
framework that integrates symmetry-aware representations to improve molecular
modeling. By enforcing the equivariance under relevant transformation groups,
our approach preserves geometric and topological properties, leading to more
robust and physically meaningful representations. We examine a range of
equivariant architectures and demonstrate that integrating symmetry constraints
leads to notable performance gains on large-scale molecular datasets.
Experiments on both small and large molecules show that high-order interactions
offer limited benefits for small molecules but consistently outperform 2D
graphs on larger ones. Adding geometric features to these high-order structures
further improves the performance, emphasizing the value of spatial information
in molecular learning. Our source code is available at
https://github.com/HySonLab/EquiHGNN/

</details>


### [41] [Conditional Front-door Adjustment for Heterogeneous Treatment Assignment Effect Estimation Under Non-adherence](https://arxiv.org/abs/2505.05677)
*Winston Chen,Trenton Chang,Jenna Wiens*

Main category: cs.LG

TL;DR: 本文研究了在非依从性存在下，CFD方法比SBD方法能提供更低方差的治疗效果估计，尤其是在真实治疗效果较小时。通过引入LobsterNet模型，可以进一步减小估计误差。


<details>
  <summary>Details</summary>
Motivation: 在非依从性情况下，现有方法（如SBD和CFD）虽能无偏估计治疗效果，但其估计方差的影响尚未充分探讨。本文旨在比较CFD与SBD的方差性能，并改进CFD的估计效果。

Method: 作者理论分析和实验验证了CFD在真实效果较小时的方差优势，并提出了LobsterNet这一多任务神经网络，通过联合建模多个干扰参数来实现CFD估计。

Result: 实证研究发现，CFD在真实效果较小时方差更低，且LobsterNet在半合成及真实数据集上均显著降低了估计误差。

Conclusion: 研究证实，CFD结合共享干扰参数建模（如LobsterNet）能在非依从性情境下提升治疗效果估计的准确性。

Abstract: Estimates of heterogeneous treatment assignment effects can inform treatment
decisions. Under the presence of non-adherence (e.g., patients do not adhere to
their assigned treatment), both the standard backdoor adjustment (SBD) and the
conditional front-door adjustment (CFD) can recover unbiased estimates of the
treatment assignment effects. However, the estimation variance of these
approaches may vary widely across settings, which remains underexplored in the
literature. In this work, we demonstrate theoretically and empirically that CFD
yields lower-variance estimates than SBD when the true effect of treatment
assignment is small (i.e., assigning an intervention leads to small changes in
patients' future outcome). Additionally, since CFD requires estimating multiple
nuisance parameters, we introduce LobsterNet, a multi-task neural network that
implements CFD with joint modeling of the nuisance parameters. Empirically,
LobsterNet reduces estimation error across several semi-synthetic and
real-world datasets compared to baselines. Our findings suggest CFD with shared
nuisance parameter modeling can improve treatment assignment effect estimation
under non-adherence.

</details>


### [42] [Interactive Diabetes Risk Prediction Using Explainable Machine Learning: A Dash-Based Approach with SHAP, LIME, and Comorbidity Insights](https://arxiv.org/abs/2505.05683)
*Udaya Allani*

Main category: cs.LG

TL;DR: 本研究开发了一个基于网络的交互式健康风险预测工具，用于评估糖尿病风险，采用多种机器学习模型和采样策略，其中LightGBM结合欠采样表现最佳。工具还整合了SHAP和LIME解释预测，并通过Dash UI提供用户友好交互。


<details>
  <summary>Details</summary>
Motivation: 旨在通过机器学习和可解释性工具提高糖尿病风险的预测准确性和用户理解，促进数据驱动的健康意识。

Method: 基于2015年CDC BRFSS数据集，评估了Logistic回归、随机森林、XGBoost、LightGBM、KNN和神经网络等多种模型，并结合原始数据、SMOTE和欠采样策略。

Result: LightGBM结合欠采样策略实现了最佳召回率，适合风险检测。工具通过SHAP和LIME提供预测解释，并利用Pearson分析突出 comorbidities 相关性。

Conclusion: 研究表明，该工具在糖尿病风险预测中表现优异，同时通过交互式界面和解释性分析提升了用户体验和健康意识。

Abstract: This study presents a web-based interactive health risk prediction tool
designed to assess diabetes risk using machine learning models. Built on the
2015 CDC BRFSS dataset, the study evaluates models including Logistic
Regression, Random Forest, XGBoost, LightGBM, KNN, and Neural Networks under
original, SMOTE, and undersampling strategies. LightGBM with undersampling
achieved the best recall, making it ideal for risk detection. The tool
integrates SHAP and LIME to explain predictions and highlights comorbidity
correlations using Pearson analysis. A Dash-based UI enables user-friendly
interaction with model predictions, personalized suggestions, and feature
insights, supporting data-driven health awareness.

</details>


### [43] [Hypergraph Neural Sheaf Diffusion: A Symmetric Simplicial Set Framework for Higher-Order Learning](https://arxiv.org/abs/2505.05702)
*Seongjin Choi,Gahee Kim,Yong-Geun Oh*

Main category: cs.LG

TL;DR: 本文解决了超图中缺乏固有邻接关系和方向系统的问题，通过从超图直接导出的对称单纯集来定义高阶邻接关系，并构建了Hypergraph Neural Sheaf Diffusion (HNSD)方法来进行超图学习。


<details>
  <summary>Details</summary>
Motivation: 超图缺乏固有邻接关系和方向系统，导致构造任意度数层拉普拉斯算子存在挑战。研究旨在通过对称单纯集解决这些问题并扩展神经层扩散方法到超图。

Method: 提出对称单纯集从超图中编码所有可能的定向子关系，并通过该方法构建层拉普拉斯算子。进一步提出HNSD方法，利用标准化零度层拉普拉斯算子在对称单纯集上进行超图学习。

Result: 标准化零度层拉普拉斯算子在对称单纯集上与传统图标准化层拉普拉斯算子在图上一致。HNSD在多个基准测试中表现优异。

Conclusion: 对称单纯集成功解决了超图的方向和邻接问题，HNSD为超图学习提供了理论基础和有效方法。

Abstract: The absence of intrinsic adjacency relations and orientation systems in
hypergraphs creates fundamental challenges for constructing sheaf Laplacians of
arbitrary degrees. We resolve these limitations through symmetric simplicial
sets derived directly from hypergraphs, which encode all possible oriented
subrelations within each hyperedge as ordered tuples. This construction
canonically defines adjacency via facet maps while inherently preserving
hyperedge provenance. We establish that the normalized degree zero sheaf
Laplacian on our induced symmetric simplicial set reduces exactly to the
traditional graph normalized sheaf Laplacian when restricted to graphs,
validating its mathematical consistency with prior graph-based sheaf theory.
Furthermore, the induced structure preserves all structural information from
the original hypergraph, ensuring that every multi-way relational detail is
faithfully retained. Leveraging this framework, we introduce Hypergraph Neural
Sheaf Diffusion (HNSD), the first principled extension of Neural Sheaf
Diffusion (NSD) to hypergraphs. HNSD operates via normalized degree zero sheaf
Laplacians over symmetric simplicial sets, resolving orientation ambiguity and
adjacency sparsity inherent to hypergraph learning. Experimental evaluations
demonstrate HNSD's competitive performance across established benchmarks.

</details>


### [44] [Crowding Out The Noise: Algorithmic Collective Action Under Differential Privacy](https://arxiv.org/abs/2505.05707)
*Rushabh Solanki,Meghana Bhange,Ulrich Aïvodji,Elliot Creager*

Main category: cs.LG

TL;DR: 论文探讨了AI信任部署中，集体行为如何与隐私保护技术（如差分隐私）相互作用，发现差分隐私虽保护数据但也限制了集体行为的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索AI信任部署中，由普通用户通过数据共享引导AI行为的集体行动与隐私保护技术的冲突。

Method: 通过理论分析和实验模拟，研究了差分隐私随机梯度下降（DPSGD）对集体行动效果的影响。

Result: 差分隐私保护了数据，但同时提高了集体行动的难度，其成功与集体规模及隐私参数密切相关。

Conclusion: 隐私保护与集体行动之间存在权衡，需在提升AI可信度的同时找到平衡点。

Abstract: The integration of AI into daily life has generated considerable attention
and excitement, while also raising concerns about automating algorithmic harms
and re-entrenching existing social inequities. While the responsible deployment
of trustworthy AI systems is a worthy goal, there are many possible ways to
realize it, from policy and regulation to improved algorithm design and
evaluation. In fact, since AI trains on social data, there is even a
possibility for everyday users, citizens, or workers to directly steer its
behavior through Algorithmic Collective Action, by deliberately modifying the
data they share with a platform to drive its learning process in their favor.
This paper considers how these grassroots efforts to influence AI interact with
methods already used by AI firms and governments to improve model
trustworthiness. In particular, we focus on the setting where the AI firm
deploys a differentially private model, motivated by the growing regulatory
focus on privacy and data protection. We investigate how the use of
Differentially Private Stochastic Gradient Descent (DPSGD) affects the
collective's ability to influence the learning process. Our findings show that
while differential privacy contributes to the protection of individual data, it
introduces challenges for effective algorithmic collective action. We
characterize lower bounds on the success of algorithmic collective action under
differential privacy as a function of the collective's size and the firm's
privacy parameters, and verify these trends experimentally by simulating
collective action during the training of deep neural network classifiers across
several datasets.

</details>


### [45] [Automated Learning of Semantic Embedding Representations for Diffusion Models](https://arxiv.org/abs/2505.05732)
*Limai Jiang,Yunpeng Cai*

Main category: cs.LG

TL;DR: 本文提出了一种通过多级去噪自编码器框架扩展去噪扩散模型（DDMs）表征能力的方法，引入时序一致的扩散变换器和额外的时间步编码器，验证了DDMs在表征学习中的优越性。


<details>
  <summary>Details</summary>
Motivation: 尽管去噪扩散模型（DDMs）在生成任务中表现优异，但其在表征学习方面的潜力尚未充分挖掘。本文旨在扩展DDMs的表征能力，使其不仅适用于生成任务，还能用于通用深度学习应用。

Method: 使用多级去噪自编码器框架，引入时序一致的扩散变换器和时间步编码器，通过自条件扩散学习在去噪马尔可夫链上获取嵌入表征。

Result: 在多个数据集上的实验表明，通过该方法学习的最优嵌入表征优于现有的自监督表征学习方法，展示了卓越的判别性语义表征质量。

Conclusion: 本文证明了DDMs不仅适合生成任务，还可能为通用深度学习应用带来优势，扩展了其在表征学习领域的潜力。

Abstract: Generative models capture the true distribution of data, yielding
semantically rich representations. Denoising diffusion models (DDMs) exhibit
superior generative capabilities, though efficient representation learning for
them are lacking. In this work, we employ a multi-level denoising autoencoder
framework to expand the representation capacity of DDMs, which introduces
sequentially consistent Diffusion Transformers and an additional
timestep-dependent encoder to acquire embedding representations on the
denoising Markov chain through self-conditional diffusion learning.
Intuitively, the encoder, conditioned on the entire diffusion process,
compresses high-dimensional data into directional vectors in latent under
different noise levels, facilitating the learning of image embeddings across
all timesteps. To verify the semantic adequacy of embeddings generated through
this approach, extensive experiments are conducted on various datasets,
demonstrating that optimally learned embeddings by DDMs surpass
state-of-the-art self-supervised representation learning methods in most cases,
achieving remarkable discriminative semantic representation quality. Our work
justifies that DDMs are not only suitable for generative tasks, but also
potentially advantageous for general-purpose deep learning applications.

</details>


### [46] [Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering](https://arxiv.org/abs/2505.05738)
*Yiming Niu,Jinliang Deng,Lulu Zhang,Zimu Zhou,Yongxin Tong*

Main category: cs.LG

TL;DR: FOCUS是一种新型多变量时间序列预测方法，通过离线聚类提取原型来简化长距离依赖建模，降低计算复杂度，同时保持高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的方法计算复杂度高，需解决多变量时间序列预测中长距离依赖和实体间交互的问题。

Method: 引入FOCUS方法，利用离线聚类获取原型，在线阶段动态适配输入片段与高级事件间的依赖关系。

Result: 实验表明FOCUS在准确性上达到最优，同时显著降低计算成本。

Conclusion: FOCUS通过原型聚类有效简化长距离依赖建模，兼顾高效与准确。

Abstract: Accurate and efficient multivariate time series (MTS) forecasting is
essential for applications such as traffic management and weather prediction,
which depend on capturing long-range temporal dependencies and interactions
between entities. Existing methods, particularly those based on Transformer
architectures, compute pairwise dependencies across all time steps, leading to
a computational complexity that scales quadratically with the length of the
input. To overcome these challenges, we introduce the Forecaster with Offline
Clustering Using Segments (FOCUS), a novel approach to MTS forecasting that
simplifies long-range dependency modeling through the use of prototypes
extracted via offline clustering. These prototypes encapsulate high-level
events in the real-world system underlying the data, summarizing the key
characteristics of similar time segments. In the online phase, FOCUS
dynamically adapts these patterns to the current input and captures
dependencies between the input segment and high-level events, enabling both
accurate and efficient forecasting. By identifying prototypes during the
offline clustering phase, FOCUS reduces the computational complexity of
modeling long-range dependencies in the online phase to linear scaling.
Extensive experiments across diverse benchmarks demonstrate that FOCUS achieves
state-of-the-art accuracy while significantly reducing computational costs.

</details>


### [47] [Deep-ICE: The first globally optimal algorithm for empirical risk minimization of two-layer maxout and ReLU networks](https://arxiv.org/abs/2505.05740)
*Xi He,Yi Miao,Max A. Little*

Main category: cs.LG

TL;DR: 该论文提出了一种针对两层maxout和ReLU网络的全局最优算法，旨在最小化误分类数，并在小数据集上验证其精确解，同时通过核心集选择扩展至大规模数据集，性能提升20-30%。


<details>
  <summary>Details</summary>
Motivation: 现有的梯度下降和支持向量机方法在两层maxout和ReLU网络的训练中存在局限性，尤其无法保证全局最优解。本文首次提出全局最优算法，填补这一空白。

Method: 算法基于全局最优的ERM框架设计，时间复杂度为$O(N^{DK+1})$，并通过新型核心集选择方法扩展到大规模数据集。

Result: 实验验证了算法在小数据集上的精确性，核心集扩展后显著提升性能（误分类减少20-30%），超越梯度下降和SVM方法。

Conclusion: 该算法是首个针对两层maxout/ReLU网络的全局最优解方案，核心集方法有效平衡了计算复杂度与性能，为大规模应用提供了新思路。

Abstract: This paper introduces the first globally optimal algorithm for the empirical
risk minimization problem of two-layer maxout and ReLU networks, i.e.,
minimizing the number of misclassifications. The algorithm has a worst-case
time complexity of $O\left(N^{DK+1}\right)$, where $K$ denotes the number of
hidden neurons and $D$ represents the number of features. It can be can be
generalized to accommodate arbitrary computable loss functions without
affecting its computational complexity. Our experiments demonstrate that the
proposed algorithm provides provably exact solutions for small-scale datasets.
To handle larger datasets, we introduce a novel coreset selection method that
reduces the data size to a manageable scale, making it feasible for our
algorithm. This extension enables efficient processing of large-scale datasets
and achieves significantly improved performance, with a 20-30\% reduction in
misclassifications for both training and prediction, compared to
state-of-the-art approaches (neural networks trained using gradient descent and
support vector machines), when applied to the same models (two-layer networks
with fixed hidden nodes and linear models).

</details>


### [48] [Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification](https://arxiv.org/abs/2505.05744)
*Ruxue Shi,Hengrui Gu,Xu Shen,Xin Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的上下文学习框架，旨在解决LLMs在表格预测中资源需求高、演示选择不足及可解释性差的问题，通过结合LLMs的解释指导更小的SLM模型，提升了预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLMs的方法在表格学习中存在资源消耗大、演示选择不佳及可解释性差的问题，限制了其实际应用。因此，作者希望通过LLMs生成解释来指导更小的SLM模型，从而提升性能并增强可解释性。

Method: 框架分为三步：(i) LLMs生成候选演示的解释；(ii) 利用解释指导演示选择；(iii) 结合解释和演示指导SLM进行可解释预测。

Result: 实验表明该方法在多种表格数据集的平均准确率提升了5.31%。

Conclusion: 该框架通过LLMs的解释指导SLM，显著提升了表格预测的性能和可解释性，为实际应用提供了有效解决方案。

Abstract: Large Language Models (LLMs) have shown remarkable ability in solving complex
tasks, making them a promising tool for enhancing tabular learning. However,
existing LLM-based methods suffer from high resource requirements, suboptimal
demonstration selection, and limited interpretability, which largely hinder
their prediction performance and application in the real world. To overcome
these problems, we propose a novel in-context learning framework for tabular
prediction. The core idea is to leverage the explanations generated by LLMs to
guide a smaller, locally deployable Surrogate Language Model (SLM) to make
interpretable tabular predictions. Specifically, our framework mainly involves
three stages: (i) Post Hoc Explanation Generation, where LLMs are utilized to
generate explanations for question-answer pairs in candidate demonstrations,
providing insights into the reasoning behind the answer. (ii) Post Hoc
Explanation-Guided Demonstrations Selection, which utilizes explanations
generated by LLMs to guide the process of demonstration selection from
candidate demonstrations. (iii) Post Hoc Explanation-Guided Interpretable SLM
Prediction, which utilizes the demonstrations obtained in step (ii) as
in-context and merges corresponding explanations as rationales to improve the
performance of SLM and guide the model to generate interpretable outputs.
Experimental results highlight the framework's effectiveness, with an average
accuracy improvement of 5.31% across various tabular datasets in diverse
domains.

</details>


### [49] [BMMDetect: A Multimodal Deep Learning Framework for Comprehensive Biomedical Misconduct Detection](https://arxiv.org/abs/2505.05763)
*Yize Zhou,Jie Zhang,Meijie Wang,Lun Yu*

Main category: cs.LG

TL;DR: BMMDetect是一个多模态深度学习框架，整合期刊元数据、语义嵌入和GPT-4o挖掘的文本属性，用于全面评估生物医学研究中的学术不端行为，性能优于单模态基线8.6%。


<details>
  <summary>Details</summary>
Motivation: 现有方法在学术不端检测中存在算法局限性和分析流程碎片化的问题，需要更全面的解决方案。

Method: 结合期刊元数据（SJR、机构数据）、语义嵌入（PubMedBERT）和GPT-4o挖掘的文本属性（方法统计、数据异常），提出多模态融合框架BMMDetect。

Result: AUC达到74.33%，优于单模态基线8.6%，并展示了跨生物医学子领域的可迁移性。

Conclusion: BMMDetect推动了可扩展、可解释的研究诚信保护工具的发展。

Abstract: Academic misconduct detection in biomedical research remains challenging due
to algorithmic narrowness in existing methods and fragmented analytical
pipelines. We present BMMDetect, a multimodal deep learning framework that
integrates journal metadata (SJR, institutional data), semantic embeddings
(PubMedBERT), and GPT-4o-mined textual attributes (methodological statistics,
data anomalies) for holistic manuscript evaluation. Key innovations include:
(1) multimodal fusion of domain-specific features to reduce detection bias; (2)
quantitative evaluation of feature importance, identifying journal authority
metrics (e.g., SJR-index) and textual anomalies (e.g., statistical outliers) as
dominant predictors; and (3) the BioMCD dataset, a large-scale benchmark with
13,160 retracted articles and 53,411 controls. BMMDetect achieves 74.33% AUC,
outperforming single-modality baselines by 8.6%, and demonstrates
transferability across biomedical subfields. This work advances scalable,
interpretable tools for safeguarding research integrity.

</details>


### [50] [Rethinking Graph Out-Of-Distribution Generalization: A Learnable Random Walk Perspective](https://arxiv.org/abs/2505.05785)
*Henan Sun,Xunkai Li,Lei Zhu,Junyi Han,Guang Zeng,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 论文提出了一种基于可学习随机游走（LRW）视角的方法LRW-OOD，用于提升图神经网络在分布外（OOD）泛化中的性能。通过参数化转移矩阵和使用KDE-based MI损失函数，该方法显著提升了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的图OOD方法通常依赖于固定拓扑或图谱作为不变量，但这些假设可能与真实场景不符。因此，作者提出LRW作为不变量知识的实现方式，以更灵活地适应分布变化。

Method: 方法包括参数化转移矩阵的LRW采样器和路径编码器，并提出基于KDE的互信息损失函数，以生成符合OOD原则的随机游走序列。

Result: 实验表明，LRW-OOD在多种分布偏移下有效提升了泛化性能，准确率比现有最优方法提高了3.87%。

Conclusion: LRW视角为图OOD泛化提供了新思路，LRW-OOD通过灵活建模转移矩阵和优化损失函数，显著提升了模型在分布外场景下的表现。

Abstract: Out-Of-Distribution (OOD) generalization has gained increasing attentions for
machine learning on graphs, as graph neural networks (GNNs) often exhibit
performance degradation under distribution shifts. Existing graph OOD methods
tend to follow the basic ideas of invariant risk minimization and structural
causal models, interpreting the invariant knowledge across datasets under
various distribution shifts as graph topology or graph spectrum. However, these
interpretations may be inconsistent with real-world scenarios, as neither
invariant topology nor spectrum is assured. In this paper, we advocate the
learnable random walk (LRW) perspective as the instantiation of invariant
knowledge, and propose LRW-OOD to realize graph OOD generalization learning.
Instead of employing fixed probability transition matrix (i.e.,
degree-normalized adjacency matrix), we parameterize the transition matrix with
an LRW-sampler and a path encoder. Furthermore, we propose the kernel density
estimation (KDE)-based mutual information (MI) loss to generate random walk
sequences that adhere to OOD principles. Extensive experiment demonstrates that
our model can effectively enhance graph OOD generalization under various types
of distribution shifts and yield a significant accuracy improvement of 3.87%
over state-of-the-art graph OOD generalization baselines.

</details>


### [51] [Improving Generalizability of Kolmogorov-Arnold Networks via Error-Correcting Output Codes](https://arxiv.org/abs/2505.05798)
*Youngjoon Lee,Jinu Gong,Joonhyuk Kang*

Main category: cs.LG

TL;DR: KAN结合ECOC提升多类医学图像分类性能


<details>
  <summary>Details</summary>
Motivation: 提高KAN在多类分类任务中的鲁棒性和泛化能力，特别是在医疗AI应用中

Method: 将ECOC集成到KAN框架中，将多类分类转化为多个二元任务，并通过汉明距离解码提升鲁棒性

Result: 在血细胞分类数据集中表现优于原始KAN，且在FastKAN和FasterKAN变体中ECOC均能持续提升性能

Conclusion: ECOC显著增强了KAN在关键医疗AI应用中的泛化能力，是首次将ECOC与KAN结合用于多类医学图像分类

Abstract: Kolmogorov-Arnold Networks (KAN) offer universal function approximation using
univariate spline compositions without nonlinear activations. In this work, we
integrate Error-Correcting Output Codes (ECOC) into the KAN framework to
transform multi-class classification into multiple binary tasks, improving
robustness via Hamming-distance decoding. Our proposed KAN with ECOC method
outperforms vanilla KAN on a challenging blood cell classification dataset,
achieving higher accuracy under diverse hyperparameter settings. Ablation
studies further confirm that ECOC consistently enhances performance across
FastKAN and FasterKAN variants. These results demonstrate that ECOC integration
significantly boosts KAN generalizability in critical healthcare AI
applications. To the best of our knowledge, this is the first integration of
ECOC with KAN for enhancing multi-class medical image classification
performance.

</details>


### [52] [MxMoE: Mixed-precision Quantization for MoE with Accuracy and Performance Co-Design](https://arxiv.org/abs/2505.05799)
*Haojie Duanmu,Xiuhong Li,Zhihang Yuan,Size Zheng,Jiangfei Duan,Xingcheng Zhang,Dahua Lin*

Main category: cs.LG

TL;DR: MoE模型的量化部署存在挑战，MxMoE框架通过混合精度优化解决参数敏感性和专家激活频率差异问题，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: MoE模型因参数多和计算需求大而难以部署，需探索量化方法以提升效率。

Method: 提出MxMoE框架，结合参数敏感性和专家激活频率，设计混合精度配置并自动生成优化的GroupGEMM内核。

Result: MxMoE在2.25-bit下比GPTQ降低2.4倍困惑度，速度比全精度快3.4倍，比均匀量化快29.4%。

Conclusion: MxMoE通过混合精度优化有效提升了MoE模型的部署效率和性能。

Abstract: Mixture-of-Experts (MoE) models face deployment challenges due to their large
parameter counts and computational demands. We explore quantization for MoE
models and highlight two key insights: 1) linear blocks exhibit varying
quantization sensitivity, and 2) divergent expert activation frequencies create
heterogeneous computational characteristics. Based on these observations, we
introduce MxMoE, a mixed-precision optimization framework for MoE models that
considers both algorithmic and system perspectives. MxMoE navigates the design
space defined by parameter sensitivity, expert activation dynamics, and
hardware resources to derive efficient mixed-precision configurations.
Additionally, MxMoE automatically generates optimized mixed-precision GroupGEMM
kernels, enabling parallel execution of GEMMs with different precisions.
Evaluations show that MxMoE outperforms existing methods, achieving 2.4 lower
Wikitext-2 perplexity than GPTQ at 2.25-bit and delivering up to 3.4x speedup
over full precision, as well as up to 29.4% speedup over uniform quantization
at equivalent accuracy with 5-bit weight-activation quantization. Our code is
available at https://github.com/cat538/MxMoE.

</details>


### [53] [A novel Neural-ODE model for the state of health estimation of lithium-ion battery using charging curve](https://arxiv.org/abs/2505.05803)
*Yiming Li,Man He,Jiapeng Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为ACLA的混合模型，用于提升锂离子电池健康状态（SOH）估计的泛化能力。该模型结合了注意力机制、CNN和LSTM，基于ANODE框架，通过充电阶段的特定电压时间数据预测SOH和剩余寿命。实验结果表明，ACLA在测试数据集上的RMSE低至1.01%和2.24%，表现优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 锂离子电池的SOH对电动车安全运行至关重要，但现有方法的泛化能力有限。本文旨在提出一种更通用的数据驱动方法来解决这一问题。

Method: 结合注意力机制、CNN和LSTM的混合模型ACLA，基于ANODE框架，使用充电阶段的特定电压时间数据作为输入，输出SOH和剩余寿命。模型在NASA和Oxford数据集上训练，在TJU和HUST数据集上验证。

Result: ACLA在TJU和HUST数据集上的SOH估计RMSE分别为1.01%和2.24%，表现优于NODE和ANODE基准模型。

Conclusion: ACLA通过多模态结合显著提升了SOH估计的泛化性和准确性，为实际应用提供了可靠工具。

Abstract: The state of health (SOH) of lithium-ion batteries (LIBs) is crucial for
ensuring the safe and reliable operation of electric vehicles. Nevertheless,
the prevailing SOH estimation methods often have limited generalizability. This
paper introduces a data-driven approach for estimating the SOH of LIBs, which
is designed to improve generalization. We construct a hybrid model named ACLA,
which integrates the attention mechanism, convolutional neural network (CNN),
and long short-term memory network (LSTM) into the augmented neural ordinary
differential equation (ANODE) framework. This model employs normalized charging
time corresponding to specific voltages in the constant current charging phase
as input and outputs the SOH as well as remaining useful of life. The model is
trained on NASA and Oxford datasets and validated on the TJU and HUST datasets.
Compared to the benchmark models NODE and ANODE, ACLA exhibits higher accuracy
with root mean square errors (RMSE) for SOH estimation as low as 1.01% and
2.24% on the TJU and HUST datasets, respectively.

</details>


### [54] [BCE vs. CE in Deep Feature Learning](https://arxiv.org/abs/2505.05813)
*Qiufu Li,Huibin Xiao,Linlin Shen*

Main category: cs.LG

TL;DR: 本文比较了二元交叉熵（BCE）和多类交叉熵（CE）在深度特征学习中的表现，首次证明BCE也能在达到最小值时最大化类内紧凑性和类间差异性，即导致神经崩塌（NC）。研究发现BCE通过调整所有样本的决策分数来增强特征属性，实验结果显示BCE能提升分类性能并带来更好的特征紧凑性和差异性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解二元交叉熵（BCE）在多类任务中的表现，并验证其在深度特征学习中对类内紧凑性和类间差异性的影响。

Method: 通过理论分析和实验验证，比较BCE和CE在深度特征学习中的表现，证明BCE也能导致神经崩塌（NC），并探讨其与CE在训练机制上的不同。

Result: 实验结果显示BCE能提升分类性能，并带来更好的类内紧凑性和类间差异性。

Conclusion: 结论表明BCE在多类任务中表现良好，并能有效增强特征的紧凑性和差异性。

Abstract: When training classification models, it expects that the learned features are
compact within classes, and can well separate different classes. As the
dominant loss function for training classification models, minimizing
cross-entropy (CE) loss maximizes the compactness and distinctiveness, i.e.,
reaching neural collapse (NC). The recent works show that binary CE (BCE)
performs also well in multi-class tasks. In this paper, we compare BCE and CE
in deep feature learning. For the first time, we prove that BCE can also
maximize the intra-class compactness and inter-class distinctiveness when
reaching its minimum, i.e., leading to NC. We point out that CE measures the
relative values of decision scores in the model training, implicitly enhancing
the feature properties by classifying samples one-by-one. In contrast, BCE
measures the absolute values of decision scores and adjust the
positive/negative decision scores across all samples to uniformly high/low
levels. Meanwhile, the classifier biases in BCE present a substantial
constraint on the decision scores to explicitly enhance the feature properties
in the training. The experimental results are aligned with above analysis, and
show that BCE could improve the classification and leads to better compactness
and distinctiveness among sample features. The codes will be released.

</details>


### [55] [New Statistical and Computational Results for Learning Junta Distributions](https://arxiv.org/abs/2505.05819)
*Lorenzo Beretta*

Main category: cs.LG

TL;DR: 论文研究了在$"${0, 1}^n$上学习k-junta分布的问题，表明学习它与学习带噪声的k-奇偶函数（LPN）计算等价，并提出了一种统计复杂度接近最优的算法。


<details>
  <summary>Details</summary>
Motivation: 研究k-junta分布的学习问题，旨在揭示其与LPN问题的计算等价性，并探索最优学习方法。

Method: 提出了一种学习junta分布的算法，统计复杂度接近最优（多对数因子内），且计算复杂度与之前非最优算法相当。

Result: 证明了算法在统计和计算上的最优性，除非LPN问题有突破。

Conclusion: 该算法在统计和计算上均难以显著改进，为k-junta分布学习提供了理论保障。

Abstract: We study the problem of learning junta distributions on $\{0, 1\}^n$, where a
distribution is a $k$-junta if its probability mass function depends on a
subset of at most $k$ variables. We make two main contributions:
  - We show that learning $k$-junta distributions is \emph{computationally}
equivalent to learning $k$-parity functions with noise (LPN), a landmark
problem in computational learning theory.
  - We design an algorithm for learning junta distributions whose statistical
complexity is optimal, up to polylogarithmic factors. Computationally, our
algorithm matches the complexity of previous (non-sample-optimal) algorithms.
  Combined, our two contributions imply that our algorithm cannot be
significantly improved, statistically or computationally, barring a
breakthrough for LPN.

</details>


### [56] [Mixed-Integer Optimization for Responsible Machine Learning](https://arxiv.org/abs/2505.05857)
*Nathan Justin,Qingshi Sun,Andrés Gómez,Phebe Vayanos*

Main category: cs.LG

TL;DR: 该教程论文介绍了如何使用混合整数优化（MIO）框架在机器学习中嵌入公平性、透明性等责任性考量，并保持性能。讨论了理论和实践，提供了解决MIO问题的策略和工具。


<details>
  <summary>Details</summary>
Motivation: 机器学习在关键敏感领域的广泛应用引发了对公平性、透明性等问题的关注，需要一种能够保证性能的同时嵌入责任性考量的方法。

Method: 采用混合整数优化（MIO）框架，直接在学习过程中纳入责任性约束（如公平性、透明性），并通过数学公式和示例展示解决MIO问题的策略和工具。

Result: MIO为责任性机器学习提供了一种高效且可保证性能的解决方案，能够学习透明模型并满足领域特定约束。

Conclusion: 论文总结了MIO在责任性机器学习中的实用性，但也指出了当前局限性和未来研究方向。

Abstract: In the last few decades, Machine Learning (ML) has achieved significant
success across domains ranging from healthcare, sustainability, and the social
sciences, to criminal justice and finance. But its deployment in increasingly
sophisticated, critical, and sensitive areas affecting individuals, the groups
they belong to, and society as a whole raises critical concerns around
fairness, transparency, robustness, and privacy, among others. As the
complexity and scale of ML systems and of the settings in which they are
deployed grow, so does the need for responsible ML methods that address these
challenges while providing guaranteed performance in deployment.
  Mixed-integer optimization (MIO) offers a powerful framework for embedding
responsible ML considerations directly into the learning process while
maintaining performance. For example, it enables learning of inherently
transparent models that can conveniently incorporate fairness or other domain
specific constraints. This tutorial paper provides an accessible and
comprehensive introduction to this topic discussing both theoretical and
practical aspects. It outlines some of the core principles of responsible ML,
their importance in applications, and the practical utility of MIO for building
ML models that align with these principles. Through examples and mathematical
formulations, it illustrates practical strategies and available tools for
efficiently solving MIO problems for responsible ML. It concludes with a
discussion on current limitations and open research questions, providing
suggestions for future work.

</details>


### [57] [Open Set Label Shift with Test Time Out-of-Distribution Reference](https://arxiv.org/abs/2505.05868)
*Changkun Ye,Russell Tsuchida,Lars Petersson,Nick Barnes*

Main category: cs.LG

TL;DR: 该论文提出了一种用于估计源和目标开放集标签分布的三个阶段方法，使用ID/OOD分类器，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决开放集标签偏移（OSLS）问题，即目标分布中存在源分布未见的类别时标签分布的变化问题。

Method: 构建三个阶段：1) 估计源标签分布中的OOD类别；2) 使用EM算法对目标标签分布进行最大似然估计；3) 在放宽OOD分类器假设下估计目标OOD类别。

Result: 实验表明该方法在多种开放集标签偏移设置下有效，无需重新训练即可调整源分布分类器至目标分布。

Conclusion: 该方法通过三阶段估计有效解决OSLS问题，为标签分布偏移提供了一种实用的解决方案。

Abstract: Open set label shift (OSLS) occurs when label distributions change from a
source to a target distribution, and the target distribution has an additional
out-of-distribution (OOD) class. In this work, we build estimators for both
source and target open set label distributions using a source domain
in-distribution (ID) classifier and an ID/OOD classifier. With reasonable
assumptions on the ID/OOD classifier, the estimators are assembled into a
sequence of three stages: 1) an estimate of the source label distribution of
the OOD class, 2) an EM algorithm for Maximum Likelihood estimates (MLE) of the
target label distribution, and 3) an estimate of the target label distribution
of OOD class under relaxed assumptions on the OOD classifier. The sampling
errors of estimates in 1) and 3) are quantified with a concentration
inequality. The estimation result allows us to correct the ID classifier
trained on the source distribution to the target distribution without
retraining. Experiments on a variety of open set label shift settings
demonstrate the effectiveness of our model. Our code is available at
https://github.com/ChangkunYe/OpenSetLabelShift.

</details>


### [58] [Short-circuiting Shortcuts: Mechanistic Investigation of Shortcuts in Text Classification](https://arxiv.org/abs/2505.06032)
*Leon Eshuijs,Shihan Wang,Antske Fokkens*

Main category: cs.LG

TL;DR: 论文研究语言模型如何依赖虚假相关性（捷径）进行预测，重点关注模型决策机制中捷径的处理方式。通过电影评论中演员名字作为可控捷径，利用机理可解释性方法识别关注捷径的注意力头，发现这些注意力头会导致模型过早做出决策。作者提出基于注意力头的标记归因方法（HTA），有效检测并针对性缓解捷径问题。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型在决策机制中如何处理依赖虚假相关性的问题，以改进模型的可解释性和鲁棒性。

Method: 使用电影评论中的演员名字作为可控捷径，应用机理可解释性方法识别相关注意力头，提出HTA方法追踪中间决策到输入标记。

Result: 发现特定注意力头会促使模型过早依赖捷径做出决策，HTA方法能有效检测并缓解此类问题。

Conclusion: 通过HTA方法可以增强模型的可解释性，并有针对性地减少对虚假相关性的依赖。

Abstract: Reliance on spurious correlations (shortcuts) has been shown to underlie many
of the successes of language models. Previous work focused on identifying the
input elements that impact prediction. We investigate how shortcuts are
actually processed within the model's decision-making mechanism. We use actor
names in movie reviews as controllable shortcuts with known impact on the
outcome. We use mechanistic interpretability methods and identify specific
attention heads that focus on shortcuts. These heads gear the model towards a
label before processing the complete input, effectively making premature
decisions that bypass contextual analysis. Based on these findings, we
introduce Head-based Token Attribution (HTA), which traces intermediate
decisions back to input tokens. We show that HTA is effective in detecting
shortcuts in LLMs and enables targeted mitigation by selectively deactivating
shortcut-related attention heads.

</details>


### [59] [Generative Discovery of Partial Differential Equations by Learning from Math Handbooks](https://arxiv.org/abs/2505.05869)
*Hao Xu,Yuntian Chen,Rui Cao,Tianning Tang,Mengge Du,Jian Li,Adrian H. Callaghan,Dongxiao Zhang*

Main category: cs.LG

TL;DR: 该研究提出了一种知识引导的方法，结合数学手册中的现有偏微分方程（PDEs），训练生成模型EqGPT来自动发现PDEs。该方法在复杂时空项和高维场景中表现优异，并成功发现了一个新型PDE。


<details>
  <summary>Details</summary>
Motivation: 纯粹的数据驱动方法在PDE发现中存在搜索空间与优化效率的矛盾，而结合已有知识可以提高发现效率并扩展应用范围。

Method: 将现有PDE编码为句法结构，训练生成模型EqGPT，并构建生成-评估-优化的循环框架来自主识别最优PDE。

Result: 实验显示，该方法能高精度、高效地恢复多种PDE形式，并推广到不规则空间域和高维场景，还成功发现了一种新型PDE。

Conclusion: 该框架在复杂系统和实际场景中具有显著潜力，支持科学发现。

Abstract: Data driven discovery of partial differential equations (PDEs) is a promising
approach for uncovering the underlying laws governing complex systems. However,
purely data driven techniques face the dilemma of balancing search space with
optimization efficiency. This study introduces a knowledge guided approach that
incorporates existing PDEs documented in a mathematical handbook to facilitate
the discovery process. These PDEs are encoded as sentence like structures
composed of operators and basic terms, and used to train a generative model,
called EqGPT, which enables the generation of free form PDEs. A loop of
generation evaluation optimization is constructed to autonomously identify the
most suitable PDE. Experimental results demonstrate that this framework can
recover a variety of PDE forms with high accuracy and computational efficiency,
particularly in cases involving complex temporal derivatives or intricate
spatial terms, which are often beyond the reach of conventional methods. The
approach also exhibits generalizability to irregular spatial domains and higher
dimensional settings. Notably, it succeeds in discovering a previously
unreported PDE governing strongly nonlinear surface gravity waves propagating
toward breaking, based on real world experimental data, highlighting its
applicability to practical scenarios and its potential to support scientific
discovery.

</details>


### [60] [A 3D pocket-aware and evolutionary conserved interaction guided diffusion model for molecular optimization](https://arxiv.org/abs/2505.05874)
*Anjie Qiao,Hao Zhang,Qianmu Yuan,Qirui Deng,Jingtian Su,Weifeng Huang,Huihao Zhou,Guo-Bo Li,Zhen Wang,Jinping Lei*

Main category: cs.LG

TL;DR: DiffDecip是一种新型3D目标感知扩散模型，通过结合蛋白质-配体结合相互作用和蛋白质残基的进化保守信息，优化分子生成，特别是提高与高度保守残基的非共价相互作用。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成与特定蛋白质结合的分子时，可能忽略与高度保守残基的相互作用，而这些残基对蛋白质功能和配体生物活性至关重要。

Method: 开发了DiffDecip模型，将蛋白质-配体结合相互作用和残基保守信息整合到扩散和采样过程中，用于支架修饰的分子优化。

Result: DiffDecip在分子优化方面优于基线模型DiffDec，能生成与蛋白质口袋中高度保守残基形成更多非共价相互作用的高亲和力分子。

Conclusion: DiffDecip通过结合保守信息显著提升了分子生成的亲和力和功能性，为基于结构的药物设计提供了新工具。

Abstract: Generating molecules that bind to specific protein targets via diffusion
models has shown good promise for structure-based drug design and molecule
optimization. Especially, the diffusion models with binding interaction
guidance enables molecule generation with high affinity through forming
favorable interaction within protein pocket. However, the generated molecules
may not form interactions with the highly conserved residues, which are
important for protein functions and bioactivities of the ligands. Herein, we
developed a new 3D target-aware diffusion model DiffDecip, which explicitly
incorporates the protein-ligand binding interactions and evolutionary
conservation information of protein residues into both diffusion and sampling
process, for molecule optimization through scaffold decoration. The model
performance revealed that DiffDecip outperforms baseline model DiffDec on
molecule optimization towards higher affinity through forming more non-covalent
interactions with highly conserved residues in the protein pocket.

</details>


### [61] [Multi-Modal Molecular Representation Learning via Structure Awareness](https://arxiv.org/abs/2505.05877)
*Rong Yin,Ruyue Liu,Xiaoshuai Hao,Xingrui Zhou,Yong Liu,Can Ma,Weiping Wang*

Main category: cs.LG

TL;DR: MMSA框架通过多模态自监督预训练和结构感知模块，提升了分子表示学习，在MoleculeNet基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法直接融合模态信息，忽略了模态间交互和高阶关系及不变特征的潜力。

Method: 提出MMSA框架，包含多模态分子表示学习模块和结构感知模块，利用超图结构建模分子间高阶关系，并引入记忆机制存储不变知识。

Result: 在MoleculeNet基准上，MMSA平均ROC-AUC提升1.8%至9.6%。

Conclusion: MMSA通过模态协同和结构感知，显著提升了分子表示质量，为药物发现提供了更有效的工具。

Abstract: Accurate extraction of molecular representations is a critical step in the
drug discovery process. In recent years, significant progress has been made in
molecular representation learning methods, among which multi-modal molecular
representation methods based on images, and 2D/3D topologies have become
increasingly mainstream. However, existing these multi-modal approaches often
directly fuse information from different modalities, overlooking the potential
of intermodal interactions and failing to adequately capture the complex
higher-order relationships and invariant features between molecules. To
overcome these challenges, we propose a structure-awareness-based multi-modal
self-supervised molecular representation pre-training framework (MMSA) designed
to enhance molecular graph representations by leveraging invariant knowledge
between molecules. The framework consists of two main modules: the multi-modal
molecular representation learning module and the structure-awareness module.
The multi-modal molecular representation learning module collaboratively
processes information from different modalities of the same molecule to
overcome intermodal differences and generate a unified molecular embedding.
Subsequently, the structure-awareness module enhances the molecular
representation by constructing a hypergraph structure to model higher-order
correlations between molecules. This module also introduces a memory mechanism
for storing typical molecular representations, aligning them with memory
anchors in the memory bank to integrate invariant knowledge, thereby improving
the model generalization ability. Extensive experiments have demonstrated the
effectiveness of MMSA, which achieves state-of-the-art performance on the
MoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to
9.6% over baseline methods.

</details>


### [62] [IRNN: Innovation-driven Recurrent Neural Network for Time-Series Data Modeling and Prediction](https://arxiv.org/abs/2505.05916)
*Yifan Zhou,Yibo Wang,Chao Shang*

Main category: cs.LG

TL;DR: 该论文提出了一种称为创新驱动RNN（IRNN）的新架构，通过引入卡尔曼滤波中的“创新”概念，将预测误差作为附加输入信号来提升RNN的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的时间序列数据富含时序信息，但现有RNN在捕捉动态和预测未来演变方面仍有改进空间。论文受RNN与线性状态空间模型（如卡尔曼滤波）相似性的启发，探索利用“创新”概念优化RNN。

Method: 提出IRNN架构，将历史预测误差作为额外输入更新隐藏状态；设计专门训练算法IU-BPTT，交替更新“创新”数据和网络参数梯度下降。

Result: 在多个真实数据集上，IRNN显著提升了预测精度，且未大幅增加训练成本。

Conclusion: IRNN通过整合“创新”机制有效提升了RNN的时序预测性能，为时间序列建模提供了新思路。

Abstract: Many real-world datasets are time series that are sequentially collected and
contain rich temporal information. Thus, a common interest in practice is to
capture dynamics of time series and predict their future evolutions. To this
end, the recurrent neural network (RNN) has been a prevalent and effective
machine learning option, which admits a nonlinear state-space model
representation. Motivated by the resemblance between RNN and Kalman filter (KF)
for linear state-space models, we propose in this paper Innovation-driven RNN
(IRNN), a novel RNN architecture tailored to time-series data modeling and
prediction tasks. By adapting the concept of "innovation" from KF to RNN, past
prediction errors are adopted as additional input signals to update hidden
states of RNN and boost prediction performance. Since innovation data depend on
network parameters, existing training algorithms for RNN do not apply to IRNN
straightforwardly. Thus, a tailored training algorithm dubbed input
updating-based back-propagation through time (IU-BPTT) is further proposed,
which alternates between updating innovations and optimizing network parameters
via gradient descent. Experiments on real-world benchmark datasets show that
the integration of innovations into various forms of RNN leads to remarkably
improved prediction accuracy of IRNN without increasing the training cost
substantially.

</details>


### [63] [Autoencoder-Based Hybrid Replay for Class-Incremental Learning](https://arxiv.org/abs/2505.05926)
*Milad Khademi Nori,Il-Min Kim,Guanghui Wang*

Main category: cs.LG

TL;DR: 论文提出了一种基于自动编码器的混合回放策略（AHR），通过新型混合自动编码器（HAE）作为压缩器，显著降低内存需求，并保持高性能表现。


<details>
  <summary>Details</summary>
Motivation: 为了在类别增量学习（CIL）中减少任务混淆和灾难性遗忘，尤其是在任务数量增加时，需要有效策略降低内存和计算复杂度。

Method: 引入HAE作为压缩器，结合粒子的能量最小化方程和排斥力算法，实现增量嵌入和新类中心的分布，同时用于分类和回放生成。

Result: AHR在相同内存/计算预算下，表现优于现有基准，并实现了对内存需求的显著优化（最坏情况下降至$\mathcal{O}(0.1 t)$）。

Conclusion: AHR通过结合HAE的能量最小化机制，实现了高性能和低内存需求的平衡，为CIL提供了一种高效解决方案。

Abstract: In class-incremental learning (CIL), effective incremental learning
strategies are essential to mitigate task confusion and catastrophic
forgetting, especially as the number of tasks $t$ increases. Current exemplar
replay strategies impose $\mathcal{O}(t)$ memory/compute complexities. We
propose an autoencoder-based hybrid replay (AHR) strategy that leverages our
new hybrid autoencoder (HAE) to function as a compressor to alleviate the
requirement for large memory, achieving $\mathcal{O}(0.1 t)$ at the worst case
with the computing complexity of $\mathcal{O}(t)$ while accomplishing
state-of-the-art performance. The decoder later recovers the exemplar data
stored in the latent space, rather than in raw format. Additionally, HAE is
designed for both discriminative and generative modeling, enabling
classification and replay capabilities, respectively. HAE adopts the charged
particle system energy minimization equations and repulsive force algorithm for
the incremental embedding and distribution of new class centroids in its latent
space. Our results demonstrate that AHR consistently outperforms recent
baselines across multiple benchmarks while operating with the same
memory/compute budgets. The source code is included in the supplementary
material and will be open-sourced upon publication.

</details>


### [64] [FloE: On-the-Fly MoE Inference](https://arxiv.org/abs/2505.05950)
*Yuxin Zhou,Zheng Li,Jun Zhang,Jue Wang,Yiping Wang,Zhongle Xie,Ke Chen,Lidan Shou*

Main category: cs.LG

TL;DR: FloE是一个针对内存受限GPU的MoE推理系统，通过压缩激活专家参数减少数据传输负担，显著提升推理速度并降低内存占用。


<details>
  <summary>Details</summary>
Motivation: 随着MoE模型的普及，内存受限设备上的高效推理需求日益增长，现有方法因PCIe带宽限制无法满足低延迟场景。

Method: 利用稀疏激活专家内部的冗余性，采用参数矩阵压缩技术和低成本稀疏预测，减少数据传输负载。

Result: 在Mixtral-8x7B上实现每专家参数9.3倍压缩，内存占用降低8.5倍（11GB VRAM），单卡推理速度提升48.7倍。

Conclusion: FloE有效解决了MoE模型在资源受限设备上的部署难题，为低延迟推理提供了可行方案。

Abstract: With the widespread adoption of Mixture-of-Experts (MoE) models, there is a
growing demand for efficient inference on memory-constrained devices. While
offloading expert parameters to CPU memory and loading activated experts on
demand has emerged as a potential solution, the large size of activated experts
overburdens the limited PCIe bandwidth, hindering the effectiveness in
latency-sensitive scenarios. To mitigate this, we propose FloE, an on-the-fly
MoE inference system on memory-constrained GPUs. FloE is built on the insight
that there exists substantial untapped redundancy within sparsely activated
experts. It employs various compression techniques on the expert's internal
parameter matrices to reduce the data movement load, combined with low-cost
sparse prediction, achieving perceptible inference acceleration in wall-clock
time on resource-constrained devices. Empirically, FloE achieves a 9.3x
compression of parameters per expert in Mixtral-8x7B; enables deployment on a
GPU with only 11GB VRAM, reducing the memory footprint by up to 8.5x; and
delivers a 48.7x inference speedup compared to DeepSpeed-MII on a single
GeForce RTX 3090.

</details>


### [65] [Learning Power Control Protocol for In-Factory 6G Subnetworks](https://arxiv.org/abs/2505.05967)
*Uyoata E. Uyoata,Gilberto Berardinelli,Ramoni Adeogun*

Main category: cs.LG

TL;DR: 该论文提出了一种多智能体强化学习框架，用于在6G工厂子网络中自主优化信号和功率控制，减少信号开销并保持接近理想性能。


<details>
  <summary>Details</summary>
Motivation: 现有的功率控制方法主要关注数据平面，忽视了信号开销的影响，且依赖网络中心的假设（如完整且最新的信道状态信息）。本研究旨在解决这些不足。

Method: 采用多智能体强化学习（MARL）框架，将问题建模为部分可观察马尔可夫决策过程（POMDP），并使用多智能体近端策略优化（MAPPO）算法。

Result: 仿真结果表明，该方法将信号开销降低了8倍，同时缓冲刷新率仅比理想性能低5%。

Conclusion: 学习型方法在工厂子网络中能有效平衡信号开销与性能，为6G短距离通信提供了实用解决方案。

Abstract: In-X Subnetworks are envisioned to meet the stringent demands of short-range
communication in diverse 6G use cases. In the context of In-Factory scenarios,
effective power control is critical to mitigating the impact of interference
resulting from potentially high subnetwork density. Existing approaches to
power control in this domain have predominantly emphasized the data plane,
often overlooking the impact of signaling overhead. Furthermore, prior work has
typically adopted a network-centric perspective, relying on the assumption of
complete and up-to-date channel state information (CSI) being readily available
at the central controller. This paper introduces a novel multi-agent
reinforcement learning (MARL) framework designed to enable access points to
autonomously learn both signaling and power control protocols in an In-Factory
Subnetwork environment. By formulating the problem as a partially observable
Markov decision process (POMDP) and leveraging multi-agent proximal policy
optimization (MAPPO), the proposed approach achieves significant advantages.
The simulation results demonstrate that the learning-based method reduces
signaling overhead by a factor of 8 while maintaining a buffer flush rate that
lags the ideal "Genie" approach by only 5%.

</details>


### [66] [Offline Multi-agent Reinforcement Learning via Score Decomposition](https://arxiv.org/abs/2505.05968)
*Dan Qiao,Wenhao Li,Shanchao Yang,Hongyuan Zha,Baoxiang Wang*

Main category: cs.LG

TL;DR: 论文提出了一种基于扩散生成模型的两阶段框架，解决了离线多智能体强化学习中的分布偏移问题，并通过实验验证了其优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 离线多智能体强化学习因联合动作空间的高维性和协调策略的多样性面临分布偏移的挑战，现有方法难以应对OOD动作且性能不佳。

Method: 采用扩散生成模型显式建模复杂行为策略，并通过顺序评分函数分解机制规范化个体策略，实现分散执行。

Result: 在连续控制任务中表现最佳，归一化回报比现有方法高26.3%。

Conclusion: 该框架为离线协调和合作多智能体系统中的均衡选择提供了新思路。

Abstract: Offline multi-agent reinforcement learning (MARL) faces critical challenges
due to distributional shifts, further exacerbated by the high dimensionality of
joint action spaces and the diversity in coordination strategies and quality
among agents. Conventional approaches, including independent learning
frameworks and value decomposition methods based on pessimistic principles,
remain susceptible to out-of-distribution (OOD) joint actions and often yield
suboptimal performance. Through systematic analysis of prevalent offline MARL
benchmarks, we identify that this limitation primarily stems from the
inherently multimodal nature of joint collaborative policies induced by offline
data collection. To address these challenges, we propose a novel two-stage
framework: First, we employ a diffusion-based generative model to explicitly
capture the complex behavior policy, enabling accurate modeling of diverse
multi-agent coordination patterns. Second, we introduce a sequential score
function decomposition mechanism to regularize individual policies and enable
decentralized execution. Extensive experiments on continuous control tasks
demonstrate state-of-the-art performance across multiple standard offline MARL
benchmarks, outperforming existing methods by 26.3\% in normalized returns. Our
approach provides new insights into offline coordination and equilibrium
selection in cooperative multi-agent systems.

</details>


### [67] [Architectural Exploration of Hybrid Neural Decoders for Neuromorphic Implantable BMI](https://arxiv.org/abs/2505.05983)
*Vivek Mohan,Biyan Zhou,Zhou Wang,Anil Bharath,Emmanuel Drakakis,Arindam Basu*

Main category: cs.LG

TL;DR: 这篇论文介绍了一种高效的神经拟态植入式脑机接口解码流程，利用事件基神经传感方案的稀疏数据，通过可调事件过滤器（EvFilter）大幅减少处理事件数量，实现了高解码性能，同时显著降低了计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 传统脑机接口系统通常需要进行信号恢复、尖峰检测或排序，计算和内存需求高。本文旨在开发一种高效的解码流程，降低资源消耗，适合低功耗植入或可穿戴设备。

Method: 提出了一种可调事件过滤器（EvFilter），同时作为尖峰检测器（EvFilter-SPD），大幅减少处理事件数量；采用ANN和SNN解码器，避免传统信号处理步骤。

Result: EvFilter将处理事件数量减少192倍至554倍；SNN解码器比NN和LSTM解码器减少5-23倍计算和内存需求，ST-NN解码器性能接近LSTM但资源需求减少2.5倍。解码性能高达R^2=0.73。

Conclusion: 该方法显著降低了资源消耗，适用于低功耗植入或可穿戴脑机接口，无需传统信号处理步骤，同时保持高解码性能。

Abstract: This work presents an efficient decoding pipeline for neuromorphic
implantable brain-machine interfaces (Neu-iBMI), leveraging sparse neural event
data from an event-based neural sensing scheme. We introduce a tunable event
filter (EvFilter), which also functions as a spike detector (EvFilter-SPD),
significantly reducing the number of events processed for decoding by 192X and
554X, respectively. The proposed pipeline achieves high decoding performance,
up to R^2=0.73, with ANN- and SNN-based decoders, eliminating the need for
signal recovery, spike detection, or sorting, commonly performed in
conventional iBMI systems. The SNN-Decoder reduces computations and memory
required by 5-23X compared to NN-, and LSTM-Decoders, while the ST-NN-Decoder
delivers similar performance to an LSTM-Decoder requiring 2.5X fewer resources.
This streamlined approach significantly reduces computational and memory
demands, making it ideal for low-power, on-implant, or wearable iBMIs.

</details>


### [68] [Differentiable Fuzzy Neural Networks for Recommender Systems](https://arxiv.org/abs/2505.06000)
*Stephan Bartl,Kevin Innerebner,Elisabeth Lex*

Main category: cs.LG

TL;DR: 该论文探讨了如何通过神经符号方法（模糊神经网络）提高推荐系统的透明度和用户信任度，同时保持性能竞争力。


<details>
  <summary>Details</summary>
Motivation: 随着推荐系统日益复杂，透明性对增强用户信任、问责和法规遵从至关重要。神经符号方法结合了符号推理和子符号学习，为实现透明且以用户为中心的系统提供了可能。

Method: 采用模糊神经网络（FNNs）作为神经符号方法，学习基于逻辑的规则，每条规则对应一个模糊逻辑表达式，从而实现透明的推荐决策过程。

Result: 在合成数据集和MovieLens 1M数据集上的实验表明，该方法不仅能准确捕捉用户行为，还提供了透明的决策过程，且性能与现有推荐算法相当。

Conclusion: 这种可微分的神经符号方法不仅提升了推荐系统的透明度，还能与其他神经模型集成，为开发混合型透明推荐系统奠定了基础。

Abstract: As recommender systems become increasingly complex, transparency is essential
to increase user trust, accountability, and regulatory compliance.
Neuro-symbolic approaches that integrate symbolic reasoning with sub-symbolic
learning offer a promising approach toward transparent and user-centric
systems. In this work-in-progress, we investigate using fuzzy neural networks
(FNNs) as a neuro-symbolic approach for recommendations that learn logic-based
rules over predefined, human-readable atoms. Each rule corresponds to a fuzzy
logic expression, making the recommender's decision process inherently
transparent. In contrast to black-box machine learning methods, our approach
reveals the reasoning behind a recommendation while maintaining competitive
performance. We evaluate our method on a synthetic and MovieLens 1M datasets
and compare it to state-of-the-art recommendation algorithms. Our results
demonstrate that our approach accurately captures user behavior while providing
a transparent decision-making process. Finally, the differentiable nature of
this approach facilitates an integration with other neural models, enabling the
development of hybrid, transparent recommender systems.

</details>


### [69] [Fuzzy-UCS Revisited: Self-Adaptation of Rule Representations in Michigan-Style Learning Fuzzy-Classifier Systems](https://arxiv.org/abs/2505.06017)
*Hiroki Shiraishi,Yohei Hayamizu,Tomonori Hashiyama*

Main category: cs.LG

TL;DR: 本文提出了一种名为Adaptive-UCS的自监督学习模糊分类系统，通过自适应规则表示机制优化分类性能，实验证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统规则表示在未知数据特性时表现不佳，因此需要一种自适应机制来提升分类系统的性能。

Method: 提出Adaptive-UCS，引入模糊指示器作为新规则参数，通过进化算子优化规则表示，动态选择矩形或三角形隶属函数。

Result: 实验表明Adaptive-UCS在分类准确率上优于传统方法，且在噪声输入和现实问题中表现稳健。

Conclusion: Adaptive-UCS通过自适应规则表示显著提升了分类性能，适用于复杂且不确定的数据环境。

Abstract: This paper focuses on the impact of rule representation in Michigan-style
Learning Fuzzy-Classifier Systems (LFCSs) on its classification performance. A
well-representation of the rules in an LFCS is crucial for improving its
performance. However, conventional rule representations frequently need help
addressing problems with unknown data characteristics. To address this issue,
this paper proposes a supervised LFCS (i.e., Fuzzy-UCS) with a self-adaptive
rule representation mechanism, entitled Adaptive-UCS. Adaptive-UCS incorporates
a fuzzy indicator as a new rule parameter that sets the membership function of
a rule as either rectangular (i.e., crisp) or triangular (i.e., fuzzy) shapes.
The fuzzy indicator is optimized with evolutionary operators, allowing the
system to search for an optimal rule representation. Results from extensive
experiments conducted on continuous space problems demonstrate that
Adaptive-UCS outperforms other UCSs with conventional crisp-hyperrectangular
and fuzzy-hypertrapezoidal rule representations in classification accuracy.
Additionally, Adaptive-UCS exhibits robustness in the case of noisy inputs and
real-world problems with inherent uncertainty, such as missing values, leading
to stable classification performance.

</details>


### [70] [Universal Approximation Theorem for Deep Q-Learning via FBSDE System](https://arxiv.org/abs/2505.06023)
*Qian Qi*

Main category: cs.LG

TL;DR: 该论文为特定设计的深度Q网络（DQN）架构建立了通用逼近定理（UAT），该架构旨在模拟贝尔曼更新中的迭代优化过程，表明网络深度与值函数迭代直接相关。


<details>
  <summary>Details</summary>
Motivation: 现有通用逼近定理未能利用最优Q函数的结构特性，该研究旨在填补这一空白，设计一种与强化学习问题结构更紧密相关的逼近理论。

Method: 研究者利用了向后随机微分方程（BSDEs）理论工具，并通过深度残差网络架构来逼近贝尔曼算子的作用。

Result: 证明了在标准利普希茨假设下，网络层可以有效地模拟贝尔曼算子，且误差传播可控。

Conclusion: 通过动态系统视角分析网络操作，表明了网络深度与值函数迭代的关联，为强化学习问题提供了更具结构性的逼近理论支持。

Abstract: The approximation capabilities of Deep Q-Networks (DQNs) are commonly
justified by general Universal Approximation Theorems (UATs) that do not
leverage the intrinsic structural properties of the optimal Q-function, the
solution to a Bellman equation. This paper establishes a UAT for a class of
DQNs whose architecture is designed to emulate the iterative refinement process
inherent in Bellman updates. A central element of our analysis is the
propagation of regularity: while the transformation induced by a single Bellman
operator application exhibits regularity, for which Backward Stochastic
Differential Equations (BSDEs) theory provides analytical tools, the uniform
regularity of the entire sequence of value iteration iterates--specifically,
their uniform Lipschitz continuity on compact domains under standard Lipschitz
assumptions on the problem data--is derived from finite-horizon dynamic
programming principles. We demonstrate that layers of a deep residual network,
conceived as neural operators acting on function spaces, can approximate the
action of the Bellman operator. The resulting approximation theorem is thus
intrinsically linked to the control problem's structure, offering a proof
technique wherein network depth directly corresponds to iterations of value
function refinement, accompanied by controlled error propagation. This
perspective reveals a dynamic systems view of the network's operation on a
space of value functions.

</details>


### [71] [PYRREGULAR: A Unified Framework for Irregular Time Series, with Classification Benchmarks](https://arxiv.org/abs/2505.06047)
*Francesco Spinnato,Cristiano Landi*

Main category: cs.LG

TL;DR: 本文提出了一个统一的框架和首个标准化数据集库，用于不规则时间序列分类，旨在集中研究努力并提升方法评估的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决不规则时间序列数据因记录频率、观测时长和缺失值等差异带来的挑战，同时避免现有研究中工具和方法的碎片化。

Method: 基于通用数组格式构建标准化数据集库，并在34个数据集上对12种分类器模型进行基准测试。

Result: 建立了一个统一的框架和数据集库，支持跨领域方法评估。

Conclusion: 该工作为不规则时间序列分类研究提供了集中化的资源和更可靠的评估基础。

Abstract: Irregular temporal data, characterized by varying recording frequencies,
differing observation durations, and missing values, presents significant
challenges across fields like mobility, healthcare, and environmental science.
Existing research communities often overlook or address these challenges in
isolation, leading to fragmented tools and methods. To bridge this gap, we
introduce a unified framework, and the first standardized dataset repository
for irregular time series classification, built on a common array format to
enhance interoperability. This repository comprises 34 datasets on which we
benchmark 12 classifier models from diverse domains and communities. This work
aims to centralize research efforts and enable a more robust evaluation of
irregular temporal data analysis methods.

</details>


### [72] [Safe-EF: Error Feedback for Nonsmooth Constrained Optimization](https://arxiv.org/abs/2505.06053)
*Rustem Islamov,Yarden As,Ilyas Fatkhullin*

Main category: cs.LG

TL;DR: 论文针对联邦学习中的通信瓶颈问题，提出了一种新算法Safe-EF，通过压缩技术和误差反馈优化通信效率，同时确保安全约束，并在强化学习实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中模型更新维度高导致通信瓶颈，现有压缩技术可能降低性能，且误差反馈方法多限于平滑无约束问题，而实际应用常需处理非平滑目标和安全约束。

Method: 论文提出Safe-EF算法，结合压缩技术和误差反馈，支持非平滑凸问题中的安全约束，并在随机环境下扩展，匹配理论下界。

Result: 实验在分布式人形机器人训练的强化学习场景中验证了Safe-EF的有效性，确保了安全性并显著降低通信复杂度。

Conclusion: 该研究填补了误差反馈在非平滑带约束问题中的理论空白，提出的Safe-EF算法具有实际应用价值，特别是在需要安全约束的场景。

Abstract: Federated learning faces severe communication bottlenecks due to the high
dimensionality of model updates. Communication compression with contractive
compressors (e.g., Top-K) is often preferable in practice but can degrade
performance without proper handling. Error feedback (EF) mitigates such issues
but has been largely restricted for smooth, unconstrained problems, limiting
its real-world applicability where non-smooth objectives and safety constraints
are critical. We advance our understanding of EF in the canonical non-smooth
convex setting by establishing new lower complexity bounds for first-order
algorithms with contractive compression. Next, we propose Safe-EF, a novel
algorithm that matches our lower bound (up to a constant) while enforcing
safety constraints essential for practical applications. Extending our approach
to the stochastic setting, we bridge the gap between theory and practical
implementation. Extensive experiments in a reinforcement learning setup,
simulating distributed humanoid robot training, validate the effectiveness of
Safe-EF in ensuring safety and reducing communication complexity.

</details>


### [73] [Fault Diagnosis of 3D-Printed Scaled Wind Turbine Blades](https://arxiv.org/abs/2505.06080)
*Luis Miguel Esquivel-Sancho,Maryam Ghandchi Tehrani,Mauricio Muñoz-Arias,Mahmoud Askari*

Main category: cs.LG

TL;DR: 这项研究提出了一种结合3D打印、有限元模拟、实验模态分析和机器学习的风力涡轮机叶片故障检测方法，验证了振动模式对结构异常的敏感性，分类准确率超过94%。


<details>
  <summary>Details</summary>
Motivation: 风力涡轮机叶片的故障检测对保障风能设备的安全和效率至关重要，研究旨在开发一种集成方法以高效识别结构异常。

Method: 使用3D打印的缩比模型，引入裂纹损伤，通过有限元分析和实验模态分析验证，并用机器学习分类器处理振动数据。

Result: 机器学习分类器准确率超过94%，振动模式3、4、6对结构异常敏感。

Conclusion: 该方法证明了数值模拟与实验验证结合的可行性，为风能结构健康监测系统提供了新途径。

Abstract: This study presents an integrated methodology for fault detection in wind
turbine blades using 3D-printed scaled models, finite element simulations,
experimental modal analysis, and machine learning techniques. A scaled model of
the NREL 5MW blade was fabricated using 3D printing, and crack-type damages
were introduced at critical locations. Finite Element Analysis was employed to
predict the impact of these damages on the natural frequencies, with the
results validated through controlled hammer impact tests. Vibration data was
processed to extract both time-domain and frequency-domain features, and key
discriminative variables were identified using statistical analyses (ANOVA).
Machine learning classifiers, including Support Vector Machine and K-Nearest
Neighbors, achieved classification accuracies exceeding 94%. The results
revealed that vibration modes 3, 4, and 6 are particularly sensitive to
structural anomalies for this blade. This integrated approach confirms the
feasibility of combining numerical simulations with experimental validations
and paves the way for structural health monitoring systems in wind energy
applications.

</details>


### [74] [Deep Diffusion Maps](https://arxiv.org/abs/2505.06087)
*Sergio García-Heredia,Ángela Fernández,Carlos M. Alaíz*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度学习的新方法来改进扩散映射（Diffusion Maps），解决了传统方法的计算复杂性和无法应用于新数据的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散映射等非线性降维方法虽然有效，但存在计算复杂度高、内存消耗大且无法直接应用于新数据的问题，因此需要一种更高效、灵活的方法。

Method: 通过将扩散映射重构为无约束最小化问题，并设计神经网络训练目标函数，使其无需谱分解即可计算嵌入结果，且适用于训练集内外数据。

Result: 该方法在真实和合成数据集上的表现与传统扩散映射和Nystrom方法进行了对比，验证了其有效性和优越性。

Conclusion: 提出的深度学习框架显著提升了扩散映射的计算效率和适用性，为非线性降维提供了新的解决方案。

Abstract: One of the fundamental problems within the field of machine learning is
dimensionality reduction. Dimensionality reduction methods make it possible to
combat the so-called curse of dimensionality, visualize high-dimensional data
and, in general, improve the efficiency of storing and processing large data
sets. One of the best-known nonlinear dimensionality reduction methods is
Diffusion Maps. However, despite their virtues, both Diffusion Maps and many
other manifold learning methods based on the spectral decomposition of kernel
matrices have drawbacks such as the inability to apply them to data outside the
initial set, their computational complexity, and high memory costs for large
data sets. In this work, we propose to alleviate these problems by resorting to
deep learning. Specifically, a new formulation of Diffusion Maps embedding is
offered as a solution to a certain unconstrained minimization problem and,
based on it, a cost function to train a neural network which computes Diffusion
Maps embedding -- both inside and outside the training sample -- without the
need to perform any spectral decomposition. The capabilities of this approach
are compared on different data sets, both real and synthetic, with those of
Diffusion Maps and the Nystrom method.

</details>


### [75] [UniSymNet: A Unified Symbolic Network Guided by Transformer](https://arxiv.org/abs/2505.06091)
*Xinxin Li,Juan Zhang,Da Li,Xingyu Liu,Jin Xu,Junping Yin*

Main category: cs.LG

TL;DR: 基于神经网络的符号网络在符号回归中存在二元非线性算子难以扩展和固定架构导致过拟合的问题。本文提出统一的符号网络(UniSymNet),将二元算子统一为嵌套一元算子,并预训练Transformer模型指导结构选择,优化参数学习。实验表明UniSymNet在高维和低维基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有符号网络中二元非线性算子难以自然扩展到多元算子,以及固定架构训练易导致高复杂度和过拟合的问题。

Method: 提出Unified Symbolic Network(UniSymNet),统一二元非线性算子为嵌套一元算子;预训练Transformer模型,采用新的标签编码方法指导结构选择;目标特定的优化策略学习网络参数。

Result: UniSymNet在拟合精度、符号解率和表达式复杂度方面表现优异,在低维Standard Benchmarks和高维SRBench上均有竞争力。

Conclusion: UniSymNet通过统一算子和优化学习策略,显著提升了符号网络的性能,为符号回归提供了新的解决方案。

Abstract: Symbolic Regression (SR) is a powerful technique for automatically
discovering mathematical expressions from input data. Mainstream SR algorithms
search for the optimal symbolic tree in a vast function space, but the
increasing complexity of the tree structure limits their performance. Inspired
by neural networks, symbolic networks have emerged as a promising new paradigm.
However, most existing symbolic networks still face certain challenges: binary
nonlinear operators $\{\times, \div\}$ cannot be naturally extended to
multivariate operators, and training with fixed architecture often leads to
higher complexity and overfitting. In this work, we propose a Unified Symbolic
Network that unifies nonlinear binary operators into nested unary operators and
define the conditions under which UniSymNet can reduce complexity. Moreover, we
pre-train a Transformer model with a novel label encoding method to guide
structural selection, and adopt objective-specific optimization strategies to
learn the parameters of the symbolic network. UniSymNet shows high fitting
accuracy, excellent symbolic solution rate, and relatively low expression
complexity, achieving competitive performance on low-dimensional Standard
Benchmarks and high-dimensional SRBench.

</details>


### [76] [LLMs Outperform Experts on Challenging Biology Benchmarks](https://arxiv.org/abs/2505.06108)
*Lennart Justen*

Main category: cs.LG

TL;DR: 研究评估了27种大型语言模型在8个生物领域基准测试中的表现，发现模型性能显著提升，部分模型甚至超越专家水平。推理扩展功能如Claude 3.7 Sonnet提升了性能，但某些基准测试出现饱和现象。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在生物领域的实际能力，以了解其进步和局限性。

Method: 在8个生物领域的基准测试中，对27种模型进行10次独立运行评估。

Result: 模型性能显著提升，部分超越专家水平；推理扩展功能有效，但某些基准测试出现饱和。

Conclusion: 需开发更先进的评估方法以应对AI技术的快速发展。

Abstract: This study systematically evaluates 27 frontier Large Language Models on
eight diverse biology benchmarks spanning molecular biology, genetics, cloning,
virology, and biosecurity. Models from major AI developers released between
November 2022 and April 2025 were assessed through ten independent runs per
benchmark. The findings reveal dramatic improvements in biological
capabilities. Top model performance increased more than 4-fold on the
challenging text-only subset of the Virology Capabilities Test over the study
period, with the top model now performing twice as well as expert virologists.
Several models now match or exceed expert-level performance on other
challenging benchmarks, including LAB-Bench CloningScenarios and the biology
subsets of GPQA and WMDP. Contrary to expectations, chain-of-thought did not
substantially improve performance over zero-shot evaluation, while extended
reasoning features in o3-mini and Claude 3.7 Sonnet typically improved
performance as predicted by inference scaling. Benchmarks such as PubMedQA and
the MMLU and WMDP biology subsets exhibited performance plateaus well below
100%, suggesting benchmark saturation and errors in the underlying benchmark
data. The analysis highlights the need for more sophisticated evaluation
methodologies as AI systems continue to advance.

</details>


### [77] [FIC-TSC: Learning Time Series Classification with Fisher Information Constraint](https://arxiv.org/abs/2505.06114)
*Xiwen Chen,Wenhui Zhu,Peijie Qiu,Hao Wang,Huayu Li,Zihan Li,Yalin Wang,Aristeidis Sotiras,Abolfazl Razi*

Main category: cs.LG

TL;DR: 论文提出了一种基于Fisher信息的训练框架FIC-TSC，用于提升时间序列分类任务的泛化性，解决了域偏移问题，并在多个数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理时间序列分类任务中的域偏移问题时表现不佳，尤其是实例归一化方法在分类任务中效果有限。因此，需要一种新的训练框架来提升模型的泛化能力。

Method: 提出FIC-TSC框架，利用Fisher信息作为约束条件，理论分析和实验验证其能够引导模型收敛到更平坦的极小值，从而增强对分布偏移的适应性。

Result: 在30个UEA多变量和85个UCR单变量数据集上的实验表明，FIC-TSC优于14种最新方法。

Conclusion: FIC-TSC通过Fisher信息约束有效地提升了时间序列分类任务的泛化性能，为解决域偏移问题提供了新思路。

Abstract: Analyzing time series data is crucial to a wide spectrum of applications,
including economics, online marketplaces, and human healthcare. In particular,
time series classification plays an indispensable role in segmenting different
phases in stock markets, predicting customer behavior, and classifying worker
actions and engagement levels. These aspects contribute significantly to the
advancement of automated decision-making and system optimization in real-world
applications. However, there is a large consensus that time series data often
suffers from domain shifts between training and test sets, which dramatically
degrades the classification performance. Despite the success of (reversible)
instance normalization in handling the domain shifts for time series regression
tasks, its performance in classification is unsatisfactory. In this paper, we
propose \textit{FIC-TSC}, a training framework for time series classification
that leverages Fisher information as the constraint. We theoretically and
empirically show this is an efficient and effective solution to guide the model
converge toward flatter minima, which enhances its generalizability to
distribution shifts. We rigorously evaluate our method on 30 UEA multivariate
and 85 UCR univariate datasets. Our empirical results demonstrate the
superiority of the proposed method over 14 recent state-of-the-art methods.

</details>


### [78] [Wasserstein Distances Made Explainable: Insights into Dataset Shifts and Transport Phenomena](https://arxiv.org/abs/2505.06123)
*Philip Naumann,Jacob Kauffmann,Grégoire Montavon*

Main category: cs.LG

TL;DR: 本文提出了一种基于可解释AI的新方法，用于高效准确地归因Wasserstein距离到不同数据组件，如子群、特征或可解释子空间。


<details>
  <summary>Details</summary>
Motivation: 现有的Wasserstein距离计算或传输映射分析不足以理解其高低的具体原因，需要一种更深入的归因方法。

Method: 基于可解释AI的框架，通过分析数据子群、输入特征或可解释子空间来归因Wasserstein距离。

Result: 方法在多种数据集和Wasserstein距离规格下实现高准确性，并通过两个实际用例验证其实用性。

Conclusion: 该方法为Wasserstein距离提供了更深入的理解工具，增强了其实用性和可解释性。

Abstract: Wasserstein distances provide a powerful framework for comparing data
distributions. They can be used to analyze processes over time or to detect
inhomogeneities within data. However, simply calculating the Wasserstein
distance or analyzing the corresponding transport map (or coupling) may not be
sufficient for understanding what factors contribute to a high or low
Wasserstein distance. In this work, we propose a novel solution based on
Explainable AI that allows us to efficiently and accurately attribute
Wasserstein distances to various data components, including data subgroups,
input features, or interpretable subspaces. Our method achieves high accuracy
across diverse datasets and Wasserstein distance specifications, and its
practical utility is demonstrated in two use cases.

</details>


### [79] [Realistic Adversarial Attacks for Robustness Evaluation of Trajectory Prediction Models via Future State Perturbation](https://arxiv.org/abs/2505.06134)
*Julian F. Schumann,Jeroen Hagenus,Frederik Baymler Mathiesen,Arkady Zgonnikov*

Main category: cs.LG

TL;DR: 该论文提出了一种通过扰动对抗智能体的过去和未来状态来更全面评估轨迹预测模型鲁棒性的方法，揭示了现有模型在真实场景中可能存在的关键弱点。


<details>
  <summary>Details</summary>
Motivation: 当前轨迹预测模型的对抗攻击方法仅关注扰动过去位置，可能导致评估过于乐观，忽视实际交通中的关键漏洞。

Method: 提出了一种新颖的对抗攻击方法，通过扰动对抗智能体的过去和未来状态，并结合动态约束和战术行为保留，生成更现实的对抗轨迹。

Result: 在先进预测模型上测试表明，该方法显著增加了预测错误率和碰撞率，并暴露了模型无法检测潜在碰撞等关键弱点。

Conclusion: 需要更全面的对抗测试来更好地评估和提高自动驾驶轨迹预测模型的可靠性。

Abstract: Trajectory prediction is a key element of autonomous vehicle systems,
enabling them to anticipate and react to the movements of other road users.
Evaluating the robustness of prediction models against adversarial attacks is
essential to ensure their reliability in real-world traffic. However, current
approaches tend to focus on perturbing the past positions of surrounding
agents, which can generate unrealistic scenarios and overlook critical
vulnerabilities. This limitation may result in overly optimistic assessments of
model performance in real-world conditions.
  In this work, we demonstrate that perturbing not just past but also future
states of adversarial agents can uncover previously undetected weaknesses and
thereby provide a more rigorous evaluation of model robustness. Our novel
approach incorporates dynamic constraints and preserves tactical behaviors,
enabling more effective and realistic adversarial attacks. We introduce new
performance measures to assess the realism and impact of these adversarial
trajectories. Testing our method on a state-of-the-art prediction model
revealed significant increases in prediction errors and collision rates under
adversarial conditions. Qualitative analysis further showed that our attacks
can expose critical weaknesses, such as the inability of the model to detect
potential collisions in what appear to be safe predictions. These results
underscore the need for more comprehensive adversarial testing to better
evaluate and improve the reliability of trajectory prediction models for
autonomous vehicles.

</details>


### [80] [On the Depth of Monotone ReLU Neural Networks and ICNNs](https://arxiv.org/abs/2505.06169)
*Egor Bakaev,Florestan Brunck,Christoph Hertrich,Daniel Reichman,Amir Yehudayoff*

Main category: cs.LG

TL;DR: 该论文研究了两种ReLU神经网络模型（单调网络和输入凸神经网络）的表达能力，重点探讨了深度对计算能力的影响，并证明了相关下界。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解ReLU神经网络模型（特别是单调网络和输入凸神经网络）在计算最大函数时的表达能力限制，以及它们之间的深度分离。

Method: 论文通过深度与多面体几何的深层联系，以及三角剖分的等周性质，证明了相关下界和分离性。

Result: 证明了单调网络无法计算或逼近最大函数，输入凸神经网络对最大函数的深度复杂度有一个尖锐的n下界；同时还证明了ReLU网络与输入凸神经网络之间的深度分离。

Conclusion: 研究揭示了ReLU神经网络模型在不同计算任务中的表达能力限制，为理解网络深度与计算能力的关系提供了理论依据。

Abstract: We study two models of ReLU neural networks: monotone networks (ReLU$^+$) and
input convex neural networks (ICNN). Our focus is on expressivity, mostly in
terms of depth, and we prove the following lower bounds. For the maximum
function MAX$_n$ computing the maximum of $n$ real numbers, we show that
ReLU$^+$ networks cannot compute MAX$_n$, or even approximate it. We prove a
sharp $n$ lower bound on the ICNN depth complexity of MAX$_n$. We also prove
depth separations between ReLU networks and ICNNs; for every $k$, there is a
depth-2 ReLU network of size $O(k^2)$ that cannot be simulated by a depth-$k$
ICNN. The proofs are based on deep connections between neural networks and
polyhedral geometry, and also use isoperimetric properties of triangulations.

</details>


### [81] [A Large Language Model-Enhanced Q-learning for Capacitated Vehicle Routing Problem with Time Windows](https://arxiv.org/abs/2505.06178)
*Linjiang Cao,Maonan Wang,Xi Xiong*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的基于大语言模型（LLM）增强的Q学习框架，用于解决带有实时紧急约束的CVRPTW问题。通过自适应两阶段训练机制和三层自校正机制，实验结果表明该框架相较于传统Q学习成本降低7.3%，且收敛所需的训练步骤更少。


<details>
  <summary>Details</summary>
Motivation: CVRPTW作为一种典型的NP难组合优化问题，其复杂性受到车辆容量和时间窗口约束的影响，传统方法面临巨大挑战。大语言模型的发展为解决该问题提供了新思路。

Method: 论文提出了一种LLM增强的Q学习框架，包含自适应两阶段训练（从LLM引导的探索阶段过渡到自主优化的Q网络阶段）和三层自校正机制（语法验证、语义验证和物理约束执行）。

Result: 实验结果显示，该框架相比传统Q学习平均降低成本7.3%，并且收敛所需的训练步骤更少。

Conclusion: 该研究表明，LLM增强的Q学习框架在解决CVRPTW问题时具有显著优势，能够有效降低成本并提高效率。

Abstract: The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a
classic NP-hard combinatorial optimization problem widely applied in logistics
distribution and transportation management. Its complexity stems from the
constraints of vehicle capacity and time windows, which pose significant
challenges to traditional approaches. Advances in Large Language Models (LLMs)
provide new possibilities for finding approximate solutions to CVRPTW. This
paper proposes a novel LLM-enhanced Q-learning framework to address the CVRPTW
with real-time emergency constraints. Our solution introduces an adaptive
two-phase training mechanism that transitions from the LLM-guided exploration
phase to the autonomous optimization phase of Q-network. To ensure reliability,
we design a three-tier self-correction mechanism based on the Chain-of-Thought
(CoT) for LLMs: syntactic validation, semantic verification, and physical
constraint enforcement. In addition, we also prioritized replay of the
experience generated by LLMs to amplify the regulatory role of LLMs in the
architecture. Experimental results demonstrate that our framework achieves a
7.3\% average reduction in cost compared to traditional Q-learning, with fewer
training steps required for convergence.

</details>


### [82] [Brain Hematoma Marker Recognition Using Multitask Learning: SwinTransformer and Swin-Unet](https://arxiv.org/abs/2505.06185)
*Kodai Hirata,Tsuyoshi Okita*

Main category: cs.LG

TL;DR: 提出了基于多任务学习的MTL-Swin-Unet方法，利用Transformer同时进行分类与语义分割，通过结合语义分割与图像重构的表示提升图像表征能力，实验表明在有无协变量偏移的场景下均优于其他分类器。


<details>
  <summary>Details</summary>
Motivation: 解决由虚假关联（spurious-correlation）导致的图像表征不足问题，通过结合多任务学习提升模型鲁棒性。

Method: 采用基于Transformer的Swin-Unet架构，同时执行图像分类与语义分割任务，并引入语义分割和图像重构的表示以增强图像表征。

Result: 在无协变量偏移的测试数据（同患者切片）中F值表现最优；在有协变量偏移的测试数据（不同患者切片）中AUC指标领先。

Conclusion: MTL-Swin-Unet通过多任务学习有效提升模型性能，尤其在应对数据分布变化时表现稳定。

Abstract: This paper proposes a method MTL-Swin-Unet which is multi-task learning using
transformers for classification and semantic segmentation. For
spurious-correlation problems, this method allows us to enhance the image
representation with two other image representations: representation obtained by
semantic segmentation and representation obtained by image reconstruction. In
our experiments, the proposed method outperformed in F-value measure than other
classifiers when the test data included slices from the same patient (no
covariate shift). Similarly, when the test data did not include slices from the
same patient (covariate shift setting), the proposed method outperformed in AUC
measure.

</details>


### [83] [Auto Tensor Singular Value Thresholding: A Non-Iterative and Rank-Free Framework for Tensor Denoising](https://arxiv.org/abs/2505.06203)
*Hiroki Hasegawa,Yukihiko Okada*

Main category: cs.LG

TL;DR: 本文提出了一种新的低秩张量近似方法，通过统计基础的奇异值阈值处理自动提取重要成分，无需预定义秩或迭代优化，在噪声高维数据中表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 随着数据维度的增加，传统矩阵降噪方法无法保持数据结构和准确性，而经典张量分解方法需要预定义秩且计算成本高，因此需要一种更高效实用的方法。

Method: 采用统计基础的奇异值阈值处理对张量模态矩阵化，自动提取重要成分，避免预定义秩和迭代优化。

Result: 在合成和真实张量数据上的实验表明，该方法在估计精度和计算效率上均优于现有技术，尤其在高维噪声环境中表现突出。

Conclusion: 该方法提供了一种高效、自动化的张量降噪解决方案，显著提升了高维噪声数据下的预测准确性。

Abstract: In modern data-driven tasks such as classification, optimization, and
forecasting, mitigating the effects of intrinsic noise is crucial for improving
predictive accuracy. While numerous denoising techniques have been developed,
the rising dimensionality of real-world datasets limits conventional
matrix-based methods in preserving data structure and accuracy. This challenge
has led to increasing interest in tensor-based approaches, which naturally
capture multi-way data relationships. However, classical tensor decomposition
methods (e.g., HOSVD, HOOI) typically require pre-specified ranks and iterative
optimization, making them computationally expensive and less practical. In this
work, we propose a novel low-rank approximation method for tensor data that
avoids these limitations. Our approach applies statistically grounded singular
value thresholding to mode-wise matricizations, enabling automatic extraction
of significant components without requiring prior rank specification or
iterative refinement. Experiments on synthetic and real-world tensors show that
our method consistently outperforms existing techniques in terms of estimation
accuracy and computational efficiency, especially in noisy high-dimensional
settings.

</details>


### [84] [Towards a Unified Representation Evaluation Framework Beyond Downstream Tasks](https://arxiv.org/abs/2505.06224)
*Christos Plachouras,Julien Guinot,George Fazekas,Elio Quinton,Emmanouil Benetos,Johan Pauwels*

Main category: cs.LG

TL;DR: 该论文指出下游探测在评估模型表示时的局限性，并提出一个标准化协议来量化信息性、等变性、不变性和解耦性，发现了不同模型在表示属性上的显著差异。


<details>
  <summary>Details</summary>
Motivation: 当前下游探测方法主要评估任务相关信息，但忽略了表示的其他重要属性（如等变性、不变性和解耦性），缺乏统一的评估框架。本文旨在填补这一空白。

Method: 引入一个标准化协议，量化表示的信息性、等变性、不变性和解耦性，并在图像和语音领域的多种模型上进行评估。

Result: 发现下游性能相似的模型在这些属性上表现显著不同，提示其性能机制在功能上存在差异。

Conclusion: 表示评估需超越下游探测，新的研究方向应关注表示属性的理解与改进。

Abstract: Downstream probing has been the dominant method for evaluating model
representations, an important process given the increasing prominence of
self-supervised learning and foundation models. However, downstream probing
primarily assesses the availability of task-relevant information in the model's
latent space, overlooking attributes such as equivariance, invariance, and
disentanglement, which contribute to the interpretability, adaptability, and
utility of representations in real-world applications. While some attempts have
been made to measure these qualities in representations, no unified evaluation
framework with modular, generalizable, and interpretable metrics exists.
  In this paper, we argue for the importance of representation evaluation
beyond downstream probing. We introduce a standardized protocol to quantify
informativeness, equivariance, invariance, and disentanglement of factors of
variation in model representations. We use it to evaluate representations from
a variety of models in the image and speech domains using different
architectures and pretraining approaches on identified controllable factors of
variation. We find that representations from models with similar downstream
performance can behave substantially differently with regard to these
attributes. This hints that the respective mechanisms underlying their
downstream performance are functionally different, prompting new research
directions to understand and improve representations.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [85] [Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods](https://arxiv.org/abs/2505.05541)
*Markov Grey,Charbel-Raphaël Segerie*

Main category: cs.AI

TL;DR: 本文提议AI安全评估领域的系统分类，强调超越基准测试，关注能力上限、行为倾向和控制有效性，并提出评估方法及挑战与研究机遇。


<details>
  <summary>Details</summary>
Motivation: 随着AI前沿系统能力提升，需要更有效的评估方法确保安全并指导治理，弥补传统基准测试的不足。

Method: 通过行为技术（如scaffolding、红队测试）和内部技术（如表征分析、机制可解释性）评估能力、倾向和控制。

Result: 提出三维度分类框架，识别关键安全能力（如网络攻击、欺骗）和行为倾向（如权力寻求），并整合到治理框架中。

Conclusion: 为AI安全评估提供系统参考，强调需解决能力证明缺失、模型“懈怠”等挑战，并指出未来研究方向。

Abstract: As frontier AI systems advance toward transformative capabilities, we need a
parallel transformation in how we measure and evaluate these systems to ensure
safety and inform governance. While benchmarks have been the primary method for
estimating model capabilities, they often fail to establish true upper bounds
or predict deployment behavior. This literature review consolidates the rapidly
evolving field of AI safety evaluations, proposing a systematic taxonomy around
three dimensions: what properties we measure, how we measure them, and how
these measurements integrate into frameworks. We show how evaluations go beyond
benchmarks by measuring what models can do when pushed to the limit
(capabilities), the behavioral tendencies exhibited by default (propensities),
and whether our safety measures remain effective even when faced with
subversive adversarial AI (control). These properties are measured through
behavioral techniques like scaffolding, red teaming and supervised fine-tuning,
alongside internal techniques such as representation analysis and mechanistic
interpretability. We provide deeper explanations of some safety-critical
capabilities like cybersecurity exploitation, deception, autonomous
replication, and situational awareness, alongside concerning propensities like
power-seeking and scheming. The review explores how these evaluation methods
integrate into governance frameworks to translate results into concrete
development decisions. We also highlight challenges to safety evaluations -
proving absence of capabilities, potential model sandbagging, and incentives
for "safetywashing" - while identifying promising research directions. By
synthesizing scattered resources, this literature review aims to provide a
central reference point for understanding AI safety evaluations.

</details>


### [86] [HiBayES: A Hierarchical Bayesian Modeling Framework for AI Evaluation Statistics](https://arxiv.org/abs/2505.05602)
*Lennart Luettgau,Harry Coppock,Magda Dubois,Christopher Summerfield,Cozmin Ududec*

Main category: cs.AI

TL;DR: 介绍了HiBayES，一个基于层次贝叶斯模型的框架，用于解决AI评估中的不确定性和复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统发展，评估其能力时的不确定性和复杂性成为挑战，需要一个能系统量化不确定性的方法。

Method: 采用层次贝叶斯模型和广义线性模型(GLM)，结合贝叶斯数据分析，实现稳健的推断和参数估计。

Result: HiBayES在低数据场景下表现良好，提供了不确定性的量化方法，并发布了Beta版软件包。

Conclusion: HiBayES为AI评估提供了新的统计框架，尤其是在复杂和低数据场景下表现出色。

Abstract: As Large Language Models (LLMs) and other AI systems evolve, robustly
estimating their capabilities from inherently stochastic outputs while
systematically quantifying uncertainty in these estimates becomes increasingly
important. Further, advanced AI evaluations often have a nested hierarchical
structure, exhibit high levels of complexity, and come with high costs in
testing the most advanced AI systems. To address these challenges, we introduce
HiBayES, a generalizable Hierarchical Bayesian modeling framework for AI
Evaluation Statistics. HiBayES supports robust inferences in classical
question-answer benchmarks and advanced agentic evaluations, particularly in
low-data scenarios (e.g., < 20 data points per evaluation). Built on
Generalized Linear Models (GLMs), Bayesian data analysis, and formal model
comparison, HiBayES provides principled uncertainty quantification and robust
parameter estimation. This paper offers a comprehensive introduction to
HiBayES, including illustrative examples, comparisons to conventional
statistical methods, and practical guidance for implementing multilevel
Bayesian GLMs. Additionally, we provide a HiBayES software package [4] (Beta
version) for out-of-the-box implementation.

</details>


### [87] [scDrugMap: Benchmarking Large Foundation Models for Drug Response Prediction](https://arxiv.org/abs/2505.05612)
*Qing Wang,Yining Pan,Minghao Zhou,Zijia Tang,Yanfei Wang,Guangyu Wang,Qianqian Song*

Main category: cs.AI

TL;DR: scDrugMap是一個整合框架，用於單細胞數據中的藥物反應預測，評估了多種基礎模型並提供了友好的用戶界面。


<details>
  <summary>Details</summary>
Motivation: 藥物抗性是癌症治療的主要挑戰，單細胞分析有助於理解細胞異質性，但大規模基礎模型在單細胞數據中的藥物反應預測應用尚未充分探索。

Method: 開發了scDrugMap框架，評估了8種單細胞模型和2種大型語言模型，使用32.6萬個細胞的主要數據集和1.88萬個細胞的驗證集，並採用層凍結和LoRA微調策略進行模型性能比較。

Result: 在pooled-data情境下，scFoundation表現最佳（F1分數0.971）；在cross-data情境下，UCE微調後表現最好（F1分數0.774），而scGPT在零樣本學習中領先（F1分數0.858）。

Conclusion: scDrugMap首次大規模比較了基礎模型在單細胞藥物反應預測中的性能，並提供了一個易於使用的平台，推動藥物研發和轉化研究。

Abstract: Drug resistance presents a major challenge in cancer therapy. Single cell
profiling offers insights into cellular heterogeneity, yet the application of
large-scale foundation models for predicting drug response in single cell data
remains underexplored. To address this, we developed scDrugMap, an integrated
framework featuring both a Python command-line interface and a web server for
drug response prediction. scDrugMap evaluates a wide range of foundation
models, including eight single-cell models and two large language models, using
a curated dataset of over 326,000 cells in the primary collection and 18,800
cells in the validation set, spanning 36 datasets and diverse tissue and cancer
types. We benchmarked model performance under pooled-data and cross-data
evaluation settings, employing both layer freezing and Low-Rank Adaptation
(LoRA) fine-tuning strategies. In the pooled-data scenario, scFoundation
achieved the best performance, with mean F1 scores of 0.971 (layer freezing)
and 0.947 (fine-tuning), outperforming the lowest-performing model by over 50%.
In the cross-data setting, UCE excelled post fine-tuning (mean F1: 0.774),
while scGPT led in zero-shot learning (mean F1: 0.858). Overall, scDrugMap
provides the first large-scale benchmark of foundation models for drug response
prediction in single-cell data and serves as a user-friendly, flexible platform
for advancing drug discovery and translational research.

</details>


### [88] [Leveraging Large Language Models for enzymatic reaction prediction and characterization](https://arxiv.org/abs/2505.05616)
*Lorenzo Di Fruscia,Jana Marie Weber*

Main category: cs.AI

TL;DR: 这篇论文评估了Llama-3.1家族LLMs（8B和70B）在酶反应预测中的表现，包括单任务和多任务学习策略，发现多任务学习利用共享信息提升了预测效果，但也指出了EC分类等局限性。


<details>
  <summary>Details</summary>
Motivation: 酶反应预测在生物催化、代谢工程和药物发现中至关重要，但现有方法复杂且资源密集。LLMs在科学领域的成功应用为这一任务提供了新思路。

Method: 使用Llama-3.1模型（8B和70B），通过LoRA适配器进行参数高效微调，评估其在酶委员会编号预测、正向合成和逆向合成任务中的表现，并比较单任务与多任务学习策略。

Result: 微调后的LLMs能够捕获生物化学知识，多任务学习通过共享酶信息提升了正逆向合成的预测能力，但EC分类等任务仍存在挑战。

Conclusion: LLMs在酶反应预测中展现出潜力，多任务学习有效，但需进一步改进以应对复杂分类问题。

Abstract: Predicting enzymatic reactions is crucial for applications in biocatalysis,
metabolic engineering, and drug discovery, yet it remains a complex and
resource-intensive task. Large Language Models (LLMs) have recently
demonstrated remarkable success in various scientific domains, e.g., through
their ability to generalize knowledge, reason over complex structures, and
leverage in-context learning strategies. In this study, we systematically
evaluate the capability of LLMs, particularly the Llama-3.1 family (8B and
70B), across three core biochemical tasks: Enzyme Commission number prediction,
forward synthesis, and retrosynthesis. We compare single-task and multitask
learning strategies, employing parameter-efficient fine-tuning via LoRA
adapters. Additionally, we assess performance across different data regimes to
explore their adaptability in low-data settings. Our results demonstrate that
fine-tuned LLMs capture biochemical knowledge, with multitask learning
enhancing forward- and retrosynthesis predictions by leveraging shared
enzymatic information. We also identify key limitations, for example challenges
in hierarchical EC classification schemes, highlighting areas for further
improvement in LLM-driven biochemical modeling.

</details>


### [89] [Prompted Meta-Learning for Few-shot Knowledge Graph Completion](https://arxiv.org/abs/2505.05684)
*Han Wu,Jie Yin*

Main category: cs.AI

TL;DR: 论文提出了一种名为PromptMeta的新框架，通过结合元语义和关系信息来解决少样本知识图谱补全问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有少样本知识图谱补全方法忽视知识图谱丰富语义的问题。

Method: 提出了PromptMeta框架，包含元语义提示池和可学习的融合提示，动态结合元语义与任务特定关系信息。

Result: 在两个基准数据集上的实验证明了该方法的有效性。

Conclusion: PromptMeta框架在少样本知识图谱补全任务中表现优异，能够有效利用元语义信息。

Abstract: Few-shot knowledge graph completion (KGC) has obtained significant attention
due to its practical applications in real-world scenarios, where new knowledge
often emerges with limited available data. While most existing methods for
few-shot KGC have predominantly focused on leveraging relational information,
rich semantics inherent in KGs have been largely overlooked. To address this
gap, we propose a novel prompted meta-learning (PromptMeta) framework that
seamlessly integrates meta-semantics with relational information for few-shot
KGC. PrompMeta has two key innovations: (1) a meta-semantic prompt pool that
captures and consolidates high-level meta-semantics, enabling effective
knowledge transfer and adaptation to rare and newly emerging relations. (2) a
learnable fusion prompt that dynamically combines meta-semantic information
with task-specific relational information tailored to different few-shot tasks.
Both components are optimized together with model parameters within a
meta-learning framework. Extensive experiments on two benchmark datasets
demonstrate the effectiveness of our approach.

</details>


### [90] [Pretraining a Shared Q-Network for Data-Efficient Offline Reinforcement Learning](https://arxiv.org/abs/2505.05701)
*Jongchan Park,Mingyu Park,Donghwan Lee*

Main category: cs.AI

TL;DR: 提出一种简单有效的预训练方法，通过共享Q网络结构提升离线强化学习的数据效率，实验证明其在多种数据集和基准上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习需要从静态数据集中学习策略，但数据收集成本高且受限，如何用最小数据集学习最优策略是关键问题。

Method: 采用共享Q网络结构，通过监督回归任务预训练网络以预测下一状态和Q值，并与现有离线强化学习方法结合。

Result: 在D4RL、Robomimic和V-D4RL等基准上提升了性能，仅用10%数据集即可超越标准算法的全数据集表现。

Conclusion: 该方法显著提升了数据效率，适用于不同数据质量和分布，为离线强化学习提供了一种高效解决方案。

Abstract: Offline reinforcement learning (RL) aims to learn a policy from a static
dataset without further interactions with the environment. Collecting
sufficiently large datasets for offline RL is exhausting since this data
collection requires colossus interactions with environments and becomes tricky
when the interaction with the environment is restricted. Hence, how an agent
learns the best policy with a minimal static dataset is a crucial issue in
offline RL, similar to the sample efficiency problem in online RL. In this
paper, we propose a simple yet effective plug-and-play pretraining method to
initialize a feature of a $Q$-network to enhance data efficiency in offline RL.
Specifically, we introduce a shared $Q$-network structure that outputs
predictions of the next state and $Q$-value. We pretrain the shared $Q$-network
through a supervised regression task that predicts a next state and trains the
shared $Q$-network using diverse offline RL methods. Through extensive
experiments, we empirically demonstrate that our method enhances the
performance of existing popular offline RL methods on the D4RL, Robomimic and
V-D4RL benchmarks. Furthermore, we show that our method significantly boosts
data-efficient offline RL across various data qualities and data distributions
trough D4RL and ExoRL benchmarks. Notably, our method adapted with only 10% of
the dataset outperforms standard algorithms even with full datasets.

</details>


### [91] [APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning](https://arxiv.org/abs/2505.05758)
*Azim Ospanov,Roozbeh Yousefzadeh*

Main category: cs.AI

TL;DR: APOLLO是一个结合Lean编译器和大语言模型（LLM）的模块化流程，通过自动化修复证明语法错误、隔离失败子引理、调用LLM进行目标修复等方式，显著提升了定理证明的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在生成完全正确的形式化证明时的困难，通过结合Lean编译器的验证能力和LLM的推理能力，减少采样预算并提高准确性。

Method: APOLLO流程包括LLM生成证明、代理分析并修复语法错误、利用Lean识别错误、隔离子引理、调用自动求解器，并以低采样预算迭代修复。

Result: 在miniF2F基准测试中，APOLLO将7B参数模型的准确率提升至75.0%，并将Goedel-Prover-SFT的准确率提高至65.6%，同时显著降低样本复杂度。

Conclusion: 定向编译器引导的LLM输出修复在效率和正确性上带来显著提升，为可扩展的自动化定理证明提供了一种通用范式。

Abstract: Formal reasoning and automated theorem proving constitute a challenging
subfield of machine learning, in which machines are tasked with proving
mathematical theorems using formal languages like Lean. A formal verification
system can check whether a formal proof is correct or not almost
instantaneously, but generating a completely correct formal proof with large
language models (LLMs) remains a formidable task. The usual approach in the
literature is to prompt the LLM many times (up to several thousands) until one
of the generated proofs passes the verification system. In this work, we
present APOLLO (Automated PrOof repair via LLM and Lean cOllaboration), a
modular, model-agnostic pipeline that combines the strengths of the Lean
compiler with an LLM's reasoning abilities to achieve better proof-generation
results at a low sampling budget. Apollo directs a fully automated process in
which the LLM generates proofs for theorems, a set of agents analyze the
proofs, fix the syntax errors, identify the mistakes in the proofs using Lean,
isolate failing sub-lemmas, utilize automated solvers, and invoke an LLM on
each remaining goal with a low top-K budget. The repaired sub-proofs are
recombined and reverified, iterating up to a user-controlled maximum number of
attempts. On the miniF2F benchmark, we establish a new state-of-the-art
accuracy of 75.0% among 7B-parameter models while keeping the sampling budget
below one thousand. Moreover, Apollo raises the state-of-the-art accuracy for
Goedel-Prover-SFT to 65.6% while cutting sample complexity from 25,600 to a few
hundred. General-purpose models (o3-mini, o4-mini) jump from 3-7% to over 40%
accuracy. Our results demonstrate that targeted, compiler-guided repair of LLM
outputs yields dramatic gains in both efficiency and correctness, suggesting a
general paradigm for scalable automated theorem proving.

</details>


### [92] [Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing Low-Level Process Event Streams](https://arxiv.org/abs/2505.05880)
*Bettina Fazzinga,Sergio Flesca,Filippo Furfaro,Luigi Pontieri,Francesco Scala*

Main category: cs.AI

TL;DR: 论文提出了一种结合序列标注模型与抽象论证框架（AAF）的神经符号方法，用于高效解决事件到活动映射的不确定性问题，同时减少数据标注和计算成本。


<details>
  <summary>Details</summary>
Motivation: 现代企业和组织需要监控和分析过程轨迹，但在事件与参考业务活动之间存在映射不确定性时，传统方法可能导致低信息量结果和高计算成本。需要一种绿色AI解决方案，减少环境和计算负担。

Method: 采用神经符号方法，先通过序列标注模型生成候选事件解释，再利用AAF框架进行推理优化，结合先验知识弥补数据稀缺。

Result: 实验证实该方法在数据稀缺和严格约束条件下有效，能够平衡计算效率和解释能力。

Conclusion: 提出的方法在减少标注和计算成本的同时，通过结合数据驱动和符号推理，提升了解释的可靠性和实用性。

Abstract: Monitoring and analyzing process traces is a critical task for modern
companies and organizations. In scenarios where there is a gap between trace
events and reference business activities, this entails an interpretation
problem, amounting to translating each event of any ongoing trace into the
corresponding step of the activity instance. Building on a recent approach that
frames the interpretation problem as an acceptance problem within an Abstract
Argumentation Framework (AAF), one can elegantly analyze plausible event
interpretations (possibly in an aggregated form), as well as offer explanations
for those that conflict with prior process knowledge. Since, in settings where
event-to-activity mapping is highly uncertain (or simply under-specified) this
reasoning-based approach may yield lowly-informative results and heavy
computation, one can think of discovering a sequencetagging model, trained to
suggest highly-probable candidate event interpretations in a context-aware way.
However, training such a model optimally may require using a large amount of
manually-annotated example traces. Considering the urgent need of developing
Green AI solutions enabling environmental and societal sustainability (with
reduced labor/computational costs and carbon footprint), we propose a
data/computation-efficient neuro-symbolic approach to the problem, where the
candidate interpretations returned by the example-driven sequence tagger is
refined by the AAF-based reasoner. This allows us to also leverage prior
knowledge to compensate for the scarcity of example data, as confirmed by
experimental results; clearly, this property is particularly useful in settings
where data annotation and model optimization costs are subject to stringent
constraints.

</details>


### [93] [Pseudo-Boolean d-DNNF Compilation for Expressive Feature Modeling Constructs](https://arxiv.org/abs/2505.05976)
*Chico Sundermann,Stefan Vill,Elias Kuiter,Sebastian Krieter,Thomas Thüm,Matthias Tichy*

Main category: cs.AI

TL;DR: 本文提出了一种伪布尔编码方法，用于表达复杂的特征模型依赖关系，并通过编译为布尔d-DNNF加速自动推理，显著优于现有CNF方法。


<details>
  <summary>Details</summary>
Motivation: 现有特征模型中的高级构造（如基数约束）难以有效转换为CNF，限制了自动推理的适用性。本文旨在缩短表达能力强的高级构造与高效自动推理之间的差距。

Method: 提出伪布尔编码方法，将特征模型转换为伪布尔公式，并通过编译为布尔d-DNNF格式进行高效分析。

Result: 实验表明，伪布尔编码在转换时间和推理效率上均显著优于传统CNF方法，尤其适用于高级构造。

Conclusion: 伪布尔编码结合d-DNNF编译不仅提升了复杂特征模型的推理效率，也在基础构造上表现出竞争力。

Abstract: Configurable systems typically consist of reusable assets that have
dependencies between each other. To specify such dependencies, feature models
are commonly used. As feature models in practice are often complex, automated
reasoning is typically employed to analyze the dependencies. Here, the de facto
standard is translating the feature model to conjunctive normal form (CNF) to
enable employing off-the-shelf tools, such as SAT or #SAT solvers. However,
modern feature-modeling dialects often contain constructs, such as cardinality
constraints, that are ill-suited for conversion to CNF. This mismatch between
the input of reasoning engines and the available feature-modeling dialects
limits the applicability of the more expressive constructs. In this work, we
shorten this gap between expressive constructs and scalable automated
reasoning. Our contribution is twofold: First, we provide a pseudo-Boolean
encoding for feature models, which facilitates smaller representations of
commonly employed constructs compared to Boolean encoding. Second, we propose a
novel method to compile pseudo-Boolean formulas to Boolean d-DNNF. With the
compiled d-DNNFs, we can resort to a plethora of efficient analyses already
used in feature modeling. Our empirical evaluation shows that our proposal
substantially outperforms the state-of-the-art based on CNF inputs for
expressive constructs. For every considered dataset representing different
feature models and feature-modeling constructs, the feature models can be
significantly faster translated to pseudo-Boolean than to CNF. Overall,
deriving d-DNNFs from a feature model with the targeted expressive constraints
can be substantially accelerated using our pseudo-Boolean approach.
Furthermore, our approach is competitive on feature models with only basic
constructs.

</details>


### [94] [ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding](https://arxiv.org/abs/2505.06020)
*Shuai Wang,Ivona Najdenkoska,Hongyi Zhu,Stevan Rudinac,Monika Kackovic,Nachoem Wijnberg,Marcel Worring*

Main category: cs.AI

TL;DR: ArtRAG是一种无需训练的新框架，通过结合结构化知识和检索增强生成（RAG），为多视角艺术品解释提供支持。它从领域文本构建艺术上下文知识图（ACKG），并在推理时检索相关子图以指导生成，显著提升艺术描述的准确性和文化深度。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）在艺术品解释中难以捕捉细微的文化、历史和风格内涵，需要一种能融合多视角知识的方法。

Method: ArtRAG通过构建ACKG（涵盖艺术家、流派、主题等实体），并设计多粒度检索器选择相关子图，增强MLLMs的生成能力。

Result: 在SemArt和Artpedia数据集上，ArtRAG优于多个基线模型，人类评估也证实其生成内容更具文化洞察力和连贯性。

Conclusion: ArtRAG证明了结构化知识与检索增强生成的结合能有效提升艺术解释质量，为跨模态艺术理解提供了新方向。

Abstract: Understanding visual art requires reasoning across multiple perspectives --
cultural, historical, and stylistic -- beyond mere object recognition. While
recent multimodal large language models (MLLMs) perform well on general image
captioning, they often fail to capture the nuanced interpretations that fine
art demands. We propose ArtRAG, a novel, training-free framework that combines
structured knowledge with retrieval-augmented generation (RAG) for
multi-perspective artwork explanation. ArtRAG automatically constructs an Art
Context Knowledge Graph (ACKG) from domain-specific textual sources, organizing
entities such as artists, movements, themes, and historical events into a rich,
interpretable graph. At inference time, a multi-granular structured retriever
selects semantically and topologically relevant subgraphs to guide generation.
This enables MLLMs to produce contextually grounded, culturally informed art
descriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG
outperforms several heavily trained baselines. Human evaluations further
confirm that ArtRAG generates coherent, insightful, and culturally enriched
interpretations.

</details>


### [95] [Why Are You Wrong? Counterfactual Explanations for Language Grounding with 3D Objects](https://arxiv.org/abs/2505.06030)
*Tobias Preintner,Weixuan Yuan,Qi Huang,Adrian König,Thomas Bäck,Elena Raponi,Niki van Stein*

Main category: cs.AI

TL;DR: 本文提出了一种通过生成反事实样本来解释模型错误预测原因的方法，帮助理解模型在3D对象指代任务中的行为。


<details>
  <summary>Details</summary>
Motivation: 解决3D对象指代任务中模型错误预测的困惑，增强对模型行为的理解。

Method: 生成反事实样本，修正描述以使其被模型正确预测，保持原始描述结构和语义相似性。

Result: 在ShapeTalk数据集上评估，反事实样本能揭示描述弱点、模型偏见，提升模型理解。

Conclusion: 该方法为实践者和工程师提供了改进交互与模型的工具，增强了模型的可解释性。

Abstract: Combining natural language and geometric shapes is an emerging research area
with multiple applications in robotics and language-assisted design. A crucial
task in this domain is object referent identification, which involves selecting
a 3D object given a textual description of the target. Variability in language
descriptions and spatial relationships of 3D objects makes this a complex task,
increasing the need to better understand the behavior of neural network models
in this domain. However, limited research has been conducted in this area.
Specifically, when a model makes an incorrect prediction despite being provided
with a seemingly correct object description, practitioners are left wondering:
"Why is the model wrong?". In this work, we present a method answering this
question by generating counterfactual examples. Our method takes a
misclassified sample, which includes two objects and a text description, and
generates an alternative yet similar formulation that would have resulted in a
correct prediction by the model. We have evaluated our approach with data from
the ShapeTalk dataset along with three distinct models. Our counterfactual
examples maintain the structure of the original description, are semantically
similar and meaningful. They reveal weaknesses in the description, model bias
and enhance the understanding of the models behavior. Theses insights help
practitioners to better interact with systems as well as engineers to improve
models.

</details>


### [96] [Seqret: Mining Rule Sets from Event Sequences](https://arxiv.org/abs/2505.06049)
*Aleena Siji,Joscha Cüppers,Osman Ali Mian,Jilles Vreeken*

Main category: cs.AI

TL;DR: 提出了一种名为Seqret的方法，用于从事件序列数据中发现条件和无条件的依赖关系，通过形式化为最小描述长度原则来生成简洁且非冗余的规则集，并在实验验证中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多忽略条件依赖，仅关注顺序模式的发现，本文旨在发现事件序列数据中的条件和无条件依赖关系。

Method: 提出Seqret方法，形式化问题为最小描述长度原则，搜索并生成简洁且非冗余的规则集。

Result: 在合成数据集上能准确还原真实规则，在真实数据集中也能发现有用规则，优于现有技术。

Conclusion: Seqret方法在发现条件和无条件依赖关系方面表现出色，为事件序列数据挖掘提供了有效工具。

Abstract: Summarizing event sequences is a key aspect of data mining. Most existing
methods neglect conditional dependencies and focus on discovering sequential
patterns only. In this paper, we study the problem of discovering both
conditional and unconditional dependencies from event sequence data. We do so
by discovering rules of the form $X \rightarrow Y$ where $X$ and $Y$ are
sequential patterns. Rules like these are simple to understand and provide a
clear description of the relation between the antecedent and the consequent. To
discover succinct and non-redundant sets of rules we formalize the problem in
terms of the Minimum Description Length principle. As the search space is
enormous and does not exhibit helpful structure, we propose the Seqret method
to discover high-quality rule sets in practice. Through extensive empirical
evaluation we show that unlike the state of the art, Seqret ably recovers the
ground truth on synthetic datasets and finds useful rules from real datasets.

</details>


### [97] [Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog Generation using LLMs](https://arxiv.org/abs/2505.06096)
*Sam Bush,Matthew DeLorenzo,Phat Tieu,Jeyavijayan Rajendran*

Main category: cs.AI

TL;DR: 论文提出了一种评估Verilog训练的LLMs生成受版权保护代码风险的基准，并发布了一个包含22万文件的开源Verilog数据集FreeSet。通过自动化的数据集整理框架和持续的预训练，微调的FreeV模型在版权侵犯风险（3%）和功能生成（VerilogEval pass@10提升10%）方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在硬件设计任务（如生成功能Verilog代码）上的能力有限，且现有数据集规模小、版权检查不足，可能导致微调后的LLM侵犯版权。为此，研究旨在评估这一风险并提供解决方案。

Method: 研究提出了一个评估基准以量化风险，并开发了开源数据集FreeSet（22万文件）及其自动化整理框架。通过持续预训练方法微调Llama模型，得到Verilog专用模型FreeV。

Result: FreeV的版权侵犯风险最低（仅3%），且在功能生成上优于基线模型（VerilogEval pass@10提升10%）。

Conclusion: 开源数据集和自动化框架显著降低了LLM的版权风险，同时提升了模型在Verilog生成任务上的性能，为硬件设计领域的AI应用提供了更安全的解决方案。

Abstract: Limitations in Large Language Model (LLM) capabilities for hardware design
tasks, such as generating functional Verilog codes, have motivated various
fine-tuning optimizations utilizing curated hardware datasets from open-source
repositories. However, these datasets remain limited in size and contain
minimal checks on licensing for reuse, resulting in potential copyright
violations by fine-tuned LLMs. Therefore, we propose an evaluation benchmark to
estimate the risk of Verilog-trained LLMs to generate copyright-protected
codes. To minimize this risk, we present an open-source Verilog dataset,
FreeSet, containing over 220k files, along with the automated dataset curation
framework utilized to provide additional guarantees of fair-use Verilog data.
We then execute an LLM fine-tuning framework consisting of continual
pre-training, resulting in a fine-tuned Llama model for Verilog, FreeV. Our
results indicate that FreeV demonstrates the smallest risk of
copyright-infringement among prior works, with only a 3% violation rate.
Furthermore, experimental results demonstrate improvements in Verilog
generation functionality over its baseline model, improving VerilogEval pass@10
rates by over 10%.

</details>


### [98] [Neuro-Symbolic Concepts](https://arxiv.org/abs/2505.06191)
*Jiayuan Mao,Joshua B. Tenenbaum,Jiajun Wu*

Main category: cs.AI

TL;DR: 本文提出了一种基于概念中心的框架，通过神经符号概念构建持续学习和灵活推理的智能体，具备高效数据利用、组合泛化等优势。


<details>
  <summary>Details</summary>
Motivation: 为了解决智能体在不同领域任务中持续学习和灵活推理的挑战，本文提出了一种结合神经和符号表示的概念中心范式。

Method: 采用神经符号概念（如对象、关系和动作概念），通过符号程序与神经网络的混合表示，支持概念的组合与重用。

Result: 该框架在2D图像、视频、3D场景和机器人操作任务中展现了高效学习、组合泛化和零样本迁移能力。

Conclusion: 概念中心框架为多领域任务提供了高效、可扩展的解决方案，尤其在数据利用和泛化能力方面表现突出。

Abstract: This article presents a concept-centric paradigm for building agents that can
learn continually and reason flexibly. The concept-centric agent utilizes a
vocabulary of neuro-symbolic concepts. These concepts, such as object,
relation, and action concepts, are grounded on sensory inputs and actuation
outputs. They are also compositional, allowing for the creation of novel
concepts through their structural combination. To facilitate learning and
reasoning, the concepts are typed and represented using a combination of
symbolic programs and neural network representations. Leveraging such
neuro-symbolic concepts, the agent can efficiently learn and recombine them to
solve various tasks across different domains, ranging from 2D images, videos,
3D scenes, and robotic manipulation tasks. This concept-centric framework
offers several advantages, including data efficiency, compositional
generalization, continual learning, and zero-shot transfer.

</details>


### [99] [L2R: Learning to Reduce Search Space for Generalizable Neural Routing Solver](https://arxiv.org/abs/2503.03137)
*Changliang Zhou,Xi Lin,Zhenkun Wang,Qingfu Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种基于学习的搜索空间缩减方法，用于提高神经组合优化（NCO）在大规模问题中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有NCO方法在大规模问题上计算复杂度高且难以高效捕捉结构模式，因此需改进其性能。

Method: 通过动态选择候选节点的学习模型，缩减搜索空间，替代传统固定启发式方法。

Result: 实验表明，该方法仅基于100节点训练，即可泛化至百万级TSP和CVRP问题，且保持解的质量。

Conclusion: 该方法显著提升了NCO在大规模问题上的表现，展现了出色的泛化能力。

Abstract: Constructive neural combinatorial optimization (NCO) has attracted growing
research attention due to its ability to solve complex routing problems without
relying on handcrafted rules. However, existing NCO methods face significant
challenges in generalizing to large-scale problems due to high computational
complexity and inefficient capture of structural patterns. To address this
issue, we propose a novel learning-based search space reduction method that
adaptively selects a small set of promising candidate nodes at each step of the
constructive NCO process. Unlike traditional methods that rely on fixed
heuristics, our selection model dynamically prioritizes nodes based on learned
patterns, significantly reducing the search space while maintaining solution
quality. Experimental results demonstrate that our method, trained solely on
100-node instances from uniform distribution, generalizes remarkably well to
large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) instances with up to 1 million nodes from the uniform
distribution and over 80K nodes from other distributions.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [100] [FedAvgen: Metadata for Model Aggregation In Communication Systems](https://arxiv.org/abs/2505.05486)
*Anthony Kiggundu,Dennis Krummacker,Hans D. Schotten*

Main category: cs.NE

TL;DR: 该论文提出了一种基于元启发式算法（FedAvgen）的联邦学习方法，通过与模型权重空间关联的元数据（表型与基因型）优化全局模型的聚合效果，并对比了FedAvg和FedSGD两种基线方法。


<details>
  <summary>Details</summary>
Motivation: 为提升业务效率并降低成本，联邦学习中需解决设备多样性导致的模型聚合挑战。研究者提出通过元启发式算法（FedAvgen）将预训练模型的权重空间视为基因型，以优化全局模型聚合。

Method: 采用FedAvgen算法，将预训练模型的权重空间作为元数据（基因型），与模型表现（表型）关联，模拟遗传进化过程进行全局模型聚合。

Result: 与FedAvg和FedSGD相比，FedAvgen在全局模型聚合中展现出更优的泛化能力。

Conclusion: FedAvgen通过引入元启发式算法有效解决了设备多样性问题，为联邦学习的全局模型聚合提供了新思路。

Abstract: To improve business efficiency and minimize costs, Artificial Intelligence
(AI) practitioners have adopted a shift from formulating models from scratch
towards sharing pretrained models. The pretrained models are then aggregated
into a global model with higher generalization capabilities, which is
afterwards distributed to the client devices. This approach is known as
federated learning and inherently utilizes different techniques to select the
candidate client models averaged to obtain the global model. This approach, in
the case of communication systems, faces challenges arising from the
existential diversity in device profiles. The multiplicity in profiles
motivates our conceptual assessment of a metaheuristic algorithm (FedAvgen),
which relates each pretrained model with its weight space as metadata, to a
phenotype and genotype, respectively. This parent-child genetic evolution
characterizes the global averaging step in federated learning. We then compare
the results of our approach to two widely adopted baseline federated learning
algorithms like Federated Averaging (FedAvg) and Federated Stochastic Gradient
Descent (FedSGD).

</details>


### [101] [Evolutionary thoughts: integration of large language models and evolutionary algorithms](https://arxiv.org/abs/2505.05756)
*Antonio Jimeno Yepes,Pieter Barnard*

Main category: cs.NE

TL;DR: 论文提出了一种结合大语言模型（LLMs）和进化算法（EAs）的高效评估框架，以解决LLMs在复杂场景中的推理局限性和EAs搜索空间过大的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在复杂或新颖场景中容易产生幻觉或陷入局部解，而进化算法（EAs）虽擅长探索复杂搜索空间，但仍面临计算瓶颈。为了结合两者的优势并克服各自的局限性，研究提出了新的解决方案。

Method: 引入了一个高效的评估框架，兼容现有原始定义，并利用LLMs优化了进化搜索策略，以更聚焦的方式探索解决方案空间。

Result: 实证结果表明，LLMs能够生成更优质的候选解，提升了进化算法的表现。

Conclusion: 结合LLMs和EAs的策略有效提升了复杂搜索问题的解决能力，为LLMs和进化计算的融合提供了新的思路。

Abstract: Large Language Models (LLMs) have unveiled remarkable capabilities in
understanding and generating both natural language and code, but LLM reasoning
is prone to hallucination and struggle with complex, novel scenarios, often
getting stuck on partial or incorrect solutions. However, the inherent ability
of Evolutionary Algorithms (EAs) to explore extensive and complex search spaces
makes them particularly effective in scenarios where traditional optimization
methodologies may falter. However, EAs explore a vast search space when applied
to complex problems.
  To address the computational bottleneck of evaluating large populations,
particularly crucial for complex evolutionary tasks, we introduce a highly
efficient evaluation framework. This implementation maintains compatibility
with existing primitive definitions, ensuring the generation of valid
individuals.
  Using LLMs, we propose an enhanced evolutionary search strategy that enables
a more focused exploration of expansive solution spaces. LLMs facilitate the
generation of superior candidate solutions, as evidenced by empirical results
demonstrating their efficacy in producing improved outcomes.

</details>


### [102] [Evolutionary Optimization for the Classification of Small Molecules Regulating the Circadian Rhythm Period: A Reliable Assessment](https://arxiv.org/abs/2505.05485)
*Antonio Arauzo-Azofra,Jose Molina-Baena,Maria Luque-Rodriguez*

Main category: cs.NE

TL;DR: 该研究利用进化优化技术提升小分子分类性能，改进生物钟调节分子的识别方法，并验证了其在提高分类准确性和减少过拟合方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 识别影响生物钟周期的小分子对靶向治疗至关重要，传统分类方法可能存在不足，因此探索进化优化技术以提升分类性能。

Method: 采用进化算法优化特征选择和分类性能，应用多种机器学习分类器，并通过准确率和泛化能力评估效果。

Result: 进化优化方法显著提升了分类准确性并减少过拟合，方差惩罚机制增强了模型的实用可靠性。

Conclusion: 进化优化是分类生物钟调节小分子的有效策略，兼具预测性能提升和模型稳健性。

Abstract: The circadian rhythm plays a crucial role in regulating biological processes,
and its disruption is linked to various health issues. Identifying small
molecules that influence the circadian period is essential for developing
targeted therapies. This study explores the use of evolutionary optimization
techniques to enhance the classification of these molecules. We applied an
evolutionary algorithm to optimize feature selection and classification
performance. Several machine learning classifiers were employed, and
performance was evaluated using accuracy and generalization ability. The
findings demonstrate that the proposed evolutionary optimization method
improves classification accuracy and reduces overfitting compared to baseline
models. Additionally, the use of variance in accuracy as a penalty factor may
enhance the model's reliability for real-world applications. Our study confirms
that evolutionary optimization is an effective strategy for classifying small
molecules regulating the circadian rhythm. The proposed approach not only
improves predictive performance but also ensures a more robust model.

</details>


### [103] [Akkumula: Evidence accumulation driver models with Spiking Neural Networks](https://arxiv.org/abs/2505.05489)
*Alberto Morando*

Main category: cs.NE

TL;DR: 本文介绍了Akkumula框架，该框架利用深度学习技术模拟驾驶员感知信息累积过程，以改进车辆控制的生态有效性。


<details>
  <summary>Details</summary>
Motivation: 由于现有驾驶员模型缺乏标准构建方法，导致适应性不足且效率低下，本文旨在通过深度学习技术解决这些问题。

Method: 基于Spiking Neural Networks构建Akkumula框架，结合梯度优化和大批量训练，并通过神经网络提取感知输入。

Result: 模型在测试场实验中表现良好，能准确拟合车辆控制（刹车、加速、转向）的时间过程。

Conclusion: Akkumula不仅兼容现有机器学习架构，还能高效处理大数据集并适应多样化驾驶场景，同时保持核心机制的透明度。

Abstract: Processes of evidence accumulation for motor control contribute to the
ecological validity of driver models. According to established theories of
cognition, drivers make control adjustments when a process of accumulation of
perceptual inputs reaches a decision boundary. Unfortunately, there is not a
standard way for building such models, limiting their use. Current
implementations are hand-crafted, lack adaptability, and rely on inefficient
optimization techniques that do not scale well with large datasets. This paper
introduces Akkumula, an evidence accumulation modelling framework built using
deep learning techniques to leverage established coding libraries, gradient
optimization, and large batch training. The core of the library is based on
Spiking Neural Networks, whose operation mimic the evidence accumulation
process in the biological brain. The model was tested on data collected during
a test-track experiment. Results are promising. The model fits well the time
course of vehicle control (brake, accelerate, steering) based on vehicle sensor
data. The perceptual inputs are extracted by a dedicated neural network,
increasing the context-awareness of the model in dynamic scenarios. Akkumula
integrates with existing machine learning architectures, benefits from
continuous advancements in deep learning, efficiently processes large datasets,
adapts to diverse driving scenarios, and maintains a degree of transparency in
its core mechanisms.

</details>


### [104] [How to Train Your Metamorphic Deep Neural Network](https://arxiv.org/abs/2505.05510)
*Thomas Sommariva,Simone Calderara,Angelo Porrello*

Main category: cs.NE

TL;DR: 该论文提出了一种改进的Neural Metamorphosis (NeuMeta)训练算法，通过分块增量训练、INR初始化和批量归一化替换策略，实现了全网络变形，减少了精度损失。


<details>
  <summary>Details</summary>
Motivation: 原始的NeuMeta方法仅在模型的最后一层有效，限制了其广泛应用。因此，作者提出了一种新方法，旨在扩展NeuMeta的能力，使其适用于整个网络，同时保持较高的精度。

Method: 方法包括分块增量训练、INR初始化和批量归一化替换策略，这些步骤共同实现了全网络变形。

Result: 改进后的NeuMeta方法在广泛的压缩比范围内保持了具有竞争力的精度，为深度学习模型的灵活高效部署提供了可扩展的解决方案。

Conclusion: 通过提出的训练算法，NeuMeta的能力得到了显著提升，能够实现全网络变形，且在精度上仅有最小程度的下降，为深度学习模型的适应性部署提供了新途径。

Abstract: Neural Metamorphosis (NeuMeta) is a recent paradigm for generating neural
networks of varying width and depth. Based on Implicit Neural Representation
(INR), NeuMeta learns a continuous weight manifold, enabling the direct
generation of compressed models, including those with configurations not seen
during training. While promising, the original formulation of NeuMeta proves
effective only for the final layers of the undelying model, limiting its
broader applicability. In this work, we propose a training algorithm that
extends the capabilities of NeuMeta to enable full-network metamorphosis with
minimal accuracy degradation. Our approach follows a structured recipe
comprising block-wise incremental training, INR initialization, and strategies
for replacing batch normalization. The resulting metamorphic networks maintain
competitive accuracy across a wide range of compression ratios, offering a
scalable solution for adaptable and efficient deployment of deep models. The
code is available at: https://github.com/TSommariva/HTTY_NeuMeta.

</details>


### [105] [Economic Analysis and Optimization of Energy Storage Configuration for Park Power Systems Based on Random Forest and Genetic Algorithm](https://arxiv.org/abs/2505.05511)
*Yanghui Song,Aoqi Li,Lilei Huo*

Main category: cs.NE

TL;DR: 该研究分析了不同条件下公园的经济表现，重点关注能源存储系统部署前后的运营成本和电力负载平衡，发现优化储能配置可降低成本并提升经济效益。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索能源存储系统在公园经济表现中的作用，特别是光伏和风力输出对成本的影响，以实现电力系统的可持续发展。

Method: 首先使用随机森林模型分析未配置储能的园区经济表现，随后模拟部署50kW/100kWh储能系统后的运营，最后利用遗传算法优化储能配置。

Result: 部署储能后，各园区的风光弃电量减少，运营成本降低；遗传算法优化后，A、B、C园区的经济指标均得到改善。

Conclusion: 优化储能配置可帮助园区降低成本、提升经济效益，并促进电力系统的可持续发展。

Abstract: This study aims to analyze the economic performance of various parks under
different conditions, particularly focusing on the operational costs and power
load balancing before and after the deployment of energy storage systems.
Firstly, the economic performance of the parks without energy storage was
analyzed using a random forest model. Taking Park A as an example, it was found
that the cost had the greatest correlation with electricity purchase, followed
by photovoltaic output, indicating that solar and wind power output are key
factors affecting economic performance. Subsequently, the operation of the
parks after the configuration of a 50kW/100kWh energy storage system was
simulated, and the total cost and operation strategy of the energy storage
system were calculated. The results showed that after the deployment of energy
storage, the amount of wind and solar power curtailment in each park decreased,
and the operational costs were reduced. Finally, a genetic algorithm was used
to optimize the energy storage configuration of each park. The energy storage
operation strategy was optimized through fitness functions, crossover
operations, and mutation operations. After optimization, the economic
indicators of Parks A, B, and C all improved. The research results indicate
that by optimizing energy storage configuration, each park can reduce costs,
enhance economic benefits, and achieve sustainable development of the power
system.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [106] [From Millions of Tweets to Actionable Insights: Leveraging LLMs for User Profiling](https://arxiv.org/abs/2505.06184)
*Vahid Rahimzadeh,Ali Hamzehpour,Azadeh Shakery,Masoud Asadpour*

Main category: cs.SI

TL;DR: 该论文提出了一种基于大语言模型（LLM）的用户画像方法，通过领域定义语句生成可解释的自然语言用户画像，显著优于现有方法9.8%。


<details>
  <summary>Details</summary>
Motivation: 解决现有用户画像技术缺乏可迁移性、不可解释、依赖大量标注数据或预定义分类的问题。

Method: 两阶段方法：1）半监督过滤与领域知识库结合；2）生成抽象（综合描述）和抽取（代表性推文）用户画像。

Result: 在波斯政治Twitter数据集上，该方法比最先进的LLM和传统方法高出9.8%。

Conclusion: 该方法能生成灵活、适应性强且可解释的用户画像，减少对标注数据的依赖。

Abstract: Social media user profiling through content analysis is crucial for tasks
like misinformation detection, engagement prediction, hate speech monitoring,
and user behavior modeling. However, existing profiling techniques, including
tweet summarization, attribute-based profiling, and latent representation
learning, face significant limitations: they often lack transferability,
produce non-interpretable features, require large labeled datasets, or rely on
rigid predefined categories that limit adaptability. We introduce a novel large
language model (LLM)-based approach that leverages domain-defining statements,
which serve as key characteristics outlining the important pillars of a domain
as foundations for profiling. Our two-stage method first employs
semi-supervised filtering with a domain-specific knowledge base, then generates
both abstractive (synthesized descriptions) and extractive (representative
tweet selections) user profiles. By harnessing LLMs' inherent knowledge with
minimal human validation, our approach is adaptable across domains while
reducing the need for large labeled datasets. Our method generates
interpretable natural language user profiles, condensing extensive user data
into a scale that unlocks LLMs' reasoning and knowledge capabilities for
downstream social network tasks. We contribute a Persian political Twitter (X)
dataset and an LLM-based evaluation framework with human validation.
Experimental results show our method significantly outperforms state-of-the-art
LLM-based and traditional methods by 9.8%, demonstrating its effectiveness in
creating flexible, adaptable, and interpretable user profiles.

</details>


### [107] [A Noise-Resilient Semi-Supervised Graph Autoencoder for Overlapping Semantic Community Detection](https://arxiv.org/abs/2505.05965)
*Abdelfateh Bekkair,Slimane Bellaouar,Slimane Oulad-Naoui*

Main category: cs.SI

TL;DR: 提出了一种半监督图自编码器，结合多头注意力与模块化最大化，用于检测重叠社区，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现实网络中重叠社区检测的挑战，需整合拓扑、节点属性和先验信息，并处理噪声问题。

Method: 采用噪声抵抗架构和语义半监督设计，通过模块化约束优化社区质量，融合结构、属性和先验知识。

Result: 在NMI和F1分数上优于现有方法，即使在60%特征噪声下仍保持稳定性能。

Conclusion: 整合属性语义和结构模式对复杂网络中社区发现至关重要。

Abstract: Community detection in networks with overlapping structures remains a
significant challenge, particularly in noisy real-world environments where
integrating topology, node attributes, and prior information is critical. To
address this, we propose a semi-supervised graph autoencoder that combines
graph multi-head attention and modularity maximization to robustly detect
overlapping communities. The model learns semantic representations by fusing
structural, attribute, and prior knowledge while explicitly addressing noise in
node features. Key innovations include a noise-resistant architecture and a
semantic semi-supervised design optimized for community quality through
modularity constraints. Experiments demonstrate superior performance the model
outperforms state-of-the-art methods in overlapping community detection
(improvements in NMI and F1-score) and exhibits exceptional robustness to
attribute noise, maintaining stable performance under 60\% feature corruption.
These results highlight the importance of integrating attribute semantics and
structural patterns for accurate community discovery in complex networks.

</details>


### [108] [On the Price of Differential Privacy for Spectral Clustering over Stochastic Block Models](https://arxiv.org/abs/2505.05816)
*Antti Koskela,Mohamed Seif,Andrea J. Goldsmith*

Main category: cs.SI

TL;DR: 该论文研究了在随机块模型（SBMs）中实现隐私保护的谱聚类方法，提出了基于边缘差分隐私（DP）的算法，并探讨了隐私预算与社区标签准确恢复之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于如何在保证数据隐私（通过边缘差分隐私）的同时，有效地进行社区检测，解决隐私与准确性之间的权衡问题。

Method: 采用谱聚类方法，结合边缘差分隐私技术，设计了隐私保护算法，用于社区标签的恢复。

Result: 研究建立了信息论条件，确保了在边缘差分隐私下社区标签的准确恢复，并提供了理论保证。

Conclusion: 该论文为在隐私保护条件下进行社区检测提供了有效的方法和理论支持，平衡了隐私与准确性之间的需求。

Abstract: We investigate privacy-preserving spectral clustering for community detection
within stochastic block models (SBMs). Specifically, we focus on edge
differential privacy (DP) and propose private algorithms for community
recovery. Our work explores the fundamental trade-offs between the privacy
budget and the accurate recovery of community labels. Furthermore, we establish
information-theoretic conditions that guarantee the accuracy of our methods,
providing theoretical assurances for successful community recovery under edge
DP.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [109] [AgentXploit: End-to-End Redteaming of Black-Box AI Agents](https://arxiv.org/abs/2505.05849)
*Zhun Wang,Vincent Siu,Zhe Ye,Tianneng Shi,Yuzhou Nie,Xuandong Zhao,Chenguang Wang,Wenbo Guo,Dawn Song*

Main category: cs.CR

TL;DR: AgentXploit是一个黑盒模糊测试框架，用于自动发现并利用LLM代理中的间接提示注入漏洞，性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）代理的强大功能带来了间接提示注入的安全风险，需要一种自动化的方法来发现和利用这些漏洞。

Method: 通过构建高质量初始种子库，并使用基于蒙特卡洛树搜索（MCTS）的种子选择算法迭代优化输入，以最大化发现漏洞的概率。

Result: 在AgentDojo和VWA-adv基准测试中，成功率分别达到71%和70%，且具有强跨任务和跨模型的迁移能力。

Conclusion: AgentXploit在现实环境中也表现出色，能够有效操控代理行为，突显了其安全威胁和实用价值。

Abstract: The strong planning and reasoning capabilities of Large Language Models
(LLMs) have fostered the development of agent-based systems capable of
leveraging external tools and interacting with increasingly complex
environments. However, these powerful features also introduce a critical
security risk: indirect prompt injection, a sophisticated attack vector that
compromises the core of these agents, the LLM, by manipulating contextual
information rather than direct user prompts. In this work, we propose a generic
black-box fuzzing framework, AgentXploit, designed to automatically discover
and exploit indirect prompt injection vulnerabilities across diverse LLM
agents. Our approach starts by constructing a high-quality initial seed corpus,
then employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS)
to iteratively refine inputs, thereby maximizing the likelihood of uncovering
agent weaknesses. We evaluate AgentXploit on two public benchmarks, AgentDojo
and VWA-adv, where it achieves 71% and 70% success rates against agents based
on o3-mini and GPT-4o, respectively, nearly doubling the performance of
baseline attacks. Moreover, AgentXploit exhibits strong transferability across
unseen tasks and internal LLMs, as well as promising results against defenses.
Beyond benchmark evaluations, we apply our attacks in real-world environments,
successfully misleading agents to navigate to arbitrary URLs, including
malicious sites.

</details>


### [110] [LiteLMGuard: Seamless and Lightweight On-Device Prompt Filtering for Safeguarding Small Language Models against Quantization-induced Risks and Vulnerabilities](https://arxiv.org/abs/2505.05619)
*Kalyan Nakka,Jimmy Dani,Ausmit Mondal,Nitesh Saxena*

Main category: cs.CR

TL;DR: 论文提出了LiteLMGuard (LLMG)，一种针对量化小型语言模型（SLMs）的实时提示防护机制，旨在解决因量化导致的公平性、伦理和隐私风险。LLMG通过深度学习分类任务过滤有害提示，分类准确率达97.75%，能防御87%以上的有害提示，且延迟低至135毫秒。


<details>
  <summary>Details</summary>
Motivation: 由于量化SLM在资源受限的设备上部署时可能引发公平性、伦理和隐私问题，甚至无需对抗性操作即可响应有害查询，因此需要一种实时防护机制以提升安全性和信任度。

Method: 提出LLMG，将其设计为模型无关的提示防护工具，基于深度学习（ELECTRA模型）的提示可答性分类任务，利用语义理解过滤不应由SLM回答的查询。使用自建数据集Answerable-or-Not进行训练与评估。

Result: LLMG在安全有效性测试中防御了87%以上的有害提示（包括直接指令和越狱攻击），并在开放知识攻击场景中表现优异。提示过滤准确率达94%，平均延迟135毫秒，用户开销可忽略。

Conclusion: LLMG为量化SLM提供了一种高效、低延迟的实时防护方案，显著提升了设备端语言模型的安全性和可靠性，且具备模型无关的通用性。

Abstract: The growing adoption of Large Language Models (LLMs) has influenced the
development of their lighter counterparts-Small Language Models (SLMs)-to
enable on-device deployment across smartphones and edge devices. These SLMs
offer enhanced privacy, reduced latency, server-free functionality, and
improved user experience. However, due to resource constraints of on-device
environment, SLMs undergo size optimization through compression techniques like
quantization, which can inadvertently introduce fairness, ethical and privacy
risks. Critically, quantized SLMs may respond to harmful queries directly,
without requiring adversarial manipulation, raising significant safety and
trust concerns.
  To address this, we propose LiteLMGuard (LLMG), an on-device prompt guard
that provides real-time, prompt-level defense for quantized SLMs. Additionally,
our prompt guard is designed to be model-agnostic such that it can be
seamlessly integrated with any SLM, operating independently of underlying
architectures. Our LLMG formalizes prompt filtering as a deep learning
(DL)-based prompt answerability classification task, leveraging semantic
understanding to determine whether a query should be answered by any SLM. Using
our curated dataset, Answerable-or-Not, we trained and fine-tuned several DL
models and selected ELECTRA as the candidate, with 97.75% answerability
classification accuracy.
  Our safety effectiveness evaluations demonstrate that LLMG defends against
over 87% of harmful prompts, including both direct instruction and jailbreak
attack strategies. We further showcase its ability to mitigate the Open
Knowledge Attacks, where compromised SLMs provide unsafe responses without
adversarial prompting. In terms of prompt filtering effectiveness, LLMG
achieves near state-of-the-art filtering accuracy of 94%, with an average
latency of 135 ms, incurring negligible overhead for users.

</details>


### [111] [A Taxonomy of Attacks and Defenses in Split Learning](https://arxiv.org/abs/2505.05872)
*Aqsa Shabbir,Halil İbrahim Kanpak,Alptekin Küpçü,Sinem Sav*

Main category: cs.CR

TL;DR: Split Learning (SL) facilitates distributed deep learning but faces privacy and security risks like information leakage and adversarial attacks. This study categorizes attacks and defenses in SL, identifying gaps and future directions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic understanding of privacy and security threats in Split Learning (SL) and propose a structured approach to categorize attacks and defenses.

Method: A comprehensive taxonomy is developed, classifying attacks and defenses in SL based on strategies, constraints, and effectiveness.

Result: The study identifies key vulnerabilities in SL and organizes existing defense mechanisms, while highlighting unresolved challenges.

Conclusion: The research underscores the need for further investigation into SL's security and privacy issues, offering a foundation for future work in this evolving field.

Abstract: Split Learning (SL) has emerged as a promising paradigm for distributed deep
learning, allowing resource-constrained clients to offload portions of their
model computation to servers while maintaining collaborative learning. However,
recent research has demonstrated that SL remains vulnerable to a range of
privacy and security threats, including information leakage, model inversion,
and adversarial attacks. While various defense mechanisms have been proposed, a
systematic understanding of the attack landscape and corresponding
countermeasures is still lacking. In this study, we present a comprehensive
taxonomy of attacks and defenses in SL, categorizing them along three key
dimensions: employed strategies, constraints, and effectiveness. Furthermore,
we identify key open challenges and research gaps in SL based on our
systematization, highlighting potential future directions.

</details>


### [112] [CAPE: Context-Aware Prompt Perturbation Mechanism with Differential Privacy](https://arxiv.org/abs/2505.05922)
*Haoqi Wu,Wei Dai,Li Wang,Qiang Yan*

Main category: cs.CR

TL;DR: 论文提出Cape，一种基于差分隐私的上下文感知提示扰动机制，旨在提升LLMs推理效率及隐私-效用平衡。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs在文本理解和生成上的广泛应用，用户敏感数据泄露风险引起关注，现有方案在效率、隐私和效用间难以平衡。

Method: 提出Cape：1. 混合效用函数更好捕获 token 相似性；2. 桶化采样机制处理大采样空间的长尾现象。

Result: 多数据集实验和消融研究表明，Cape在隐私-效用平衡上优于现有最优方法。

Conclusion: Cape通过差分隐私和优化采样机制，显著提升了LLMs的隐私保护能力，同时保持了高效推理。

Abstract: Large Language Models (LLMs) have gained significant popularity due to their
remarkable capabilities in text understanding and generation. However, despite
their widespread deployment in inference services such as ChatGPT, concerns
about the potential leakage of sensitive user data have arisen. Existing
solutions primarily rely on privacy-enhancing technologies to mitigate such
risks, facing the trade-off among efficiency, privacy, and utility. To narrow
this gap, we propose Cape, a context-aware prompt perturbation mechanism based
on differential privacy, to enable efficient inference with an improved
privacy-utility trade-off. Concretely, we introduce a hybrid utility function
that better captures the token similarity. Additionally, we propose a
bucketized sampling mechanism to handle large sampling space, which might lead
to long-tail phenomenons. Extensive experiments across multiple datasets, along
with ablation studies, demonstrate that Cape achieves a better privacy-utility
trade-off compared to prior state-of-the-art works.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [113] [Understanding Stragglers in Large Model Training Using What-if Analysis](https://arxiv.org/abs/2505.05713)
*Jinkun Lin,Ziheng Jiang,Zuquan Song,Sida Zhao,Menghan Yu,Zhanghan Wang,Chenyuan Wang,Zuocheng Shi,Xiang Shi,Wei Jia,Zherui Liu,Shuguang Wang,Haibin Lin,Xiu Liu,Aurojit Panda,Jinyang Li*

Main category: cs.DC

TL;DR: 这篇论文研究了LLM训练中的拖后腿现象（stragglers），通过五个月的跟踪数据，使用假设分析方法探讨了其发生频率、影响、时空模式及根本原因。


<details>
  <summary>Details</summary>
Motivation: LLM训练对分布式计算要求极高，常因拖后腿问题（少数慢速工作节点导致整体停滞）而受限。ByteDance发现拖后腿问题不仅源于硬件故障，还涉及复杂因素，因此需系统性研究。

Method: 基于ByteDance LLM训练集群的五个月跟踪数据，采用假设分析方法，对比实际拖后腿场景与无拖后腿的理想场景。

Result: 分析揭示了拖后腿问题的发生频率、对训练性能的影响、时空模式及潜在根本原因，为优化LLM训练提供了依据。

Conclusion: 拖后腿问题是LLM训练的重要瓶颈，需针对性优化。假设分析方法是识别和解决此类问题的有效工具。

Abstract: Large language model (LLM) training is one of the most demanding distributed
computations today, often requiring thousands of GPUs with frequent
synchronization across machines. Such a workload pattern makes it susceptible
to stragglers, where the training can be stalled by few slow workers. At
ByteDance we find stragglers are not trivially always caused by hardware
failures, but can arise from multiple complex factors. This work aims to
present a comprehensive study on the straggler issues in LLM training, using a
five-month trace collected from our ByteDance LLM training cluster. The core
methodology is what-if analysis that simulates the scenario without any
stragglers and contrasts with the actual case. We use this method to study the
following questions: (1) how often do stragglers affect training jobs, and what
effect do they have on job performance; (2) do stragglers exhibit temporal or
spatial patterns; and (3) what are the potential root causes for stragglers?

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [114] [Trading Under Uncertainty: A Distribution-Based Strategy for Futures Markets Using FutureQuant Transformer](https://arxiv.org/abs/2505.05595)
*Wenhao Guo,Yuda Wang,Zeqiao Huang,Changjiang Zhang,Shumin ma*

Main category: q-fin.TR

TL;DR: FutureQuant Transformer模型利用注意力机制预测期货价格范围和波动性，优于传统点预测模型，平均每30分钟交易收益0.1193%。


<details>
  <summary>Details</summary>
Motivation: 传统期货交易中复杂的实时数据和变量（如限价订单簿）使价格预测困难，需更高效的模型提供丰富交易策略。

Method: 使用注意力机制的FutureQuant Transformer模型，结合RSI、ATR和布林带等因子，预测价格范围及波动性。

Result: 模型显著提升风险管理能力，平均每30分钟交易收益0.1193%，优于现有最佳模型。

Conclusion: FutureQuant Transformer为期货交易的预测分析带来重要突破，尤其在波动性高的市场中表现卓越。

Abstract: In the complex landscape of traditional futures trading, where vast data and
variables like real-time Limit Order Books (LOB) complicate price predictions,
we introduce the FutureQuant Transformer model, leveraging attention mechanisms
to navigate these challenges. Unlike conventional models focused on point
predictions, the FutureQuant model excels in forecasting the range and
volatility of future prices, thus offering richer insights for trading
strategies. Its ability to parse and learn from intricate market patterns
allows for enhanced decision-making, significantly improving risk management
and achieving a notable average gain of 0.1193% per 30-minute trade over
state-of-the-art models with a simple algorithm using factors such as RSI, ATR,
and Bollinger Bands. This innovation marks a substantial leap forward in
predictive analytics within the volatile domain of futures trading.

</details>


### [115] [FlowHFT: Flow Policy Induced Optimal High-Frequency Trading under Diverse Market Conditions](https://arxiv.org/abs/2505.05784)
*Yang Li,Zhi Chen,Steve Yang*

Main category: q-fin.TR

TL;DR: FlowHFT是一种基于流匹配策略的模仿学习框架，能从多个专家模型中学习交易策略，并根据市场状态自适应调整投资决策。


<details>
  <summary>Details</summary>
Motivation: 传统高频交易模型依赖历史数据，假设未来市场状态类似，限制了模型的适应性和有效性。现实市场是动态多样的，需要更灵活的解决方案。

Method: FlowHFT通过流匹配策略从多专家模型学习，并结合网格搜索微调机制，提升了复杂市场环境下的性能。

Result: 测试表明，FlowHFT在不同市场环境下表现均优于单一专家模型，展现了其适应性和性能优势。

Conclusion: FlowHFT通过模仿学习和自适应策略，为高频交易提供了更灵活、高效的解决方案。

Abstract: High-frequency trading (HFT) is an investing strategy that continuously
monitors market states and places bid and ask orders at millisecond speeds.
Traditional HFT approaches fit models with historical data and assume that
future market states follow similar patterns. This limits the effectiveness of
any single model to the specific conditions it was trained for. Additionally,
these models achieve optimal solutions only under specific market conditions,
such as assumptions about stock price's stochastic process, stable order flow,
and the absence of sudden volatility. Real-world markets, however, are dynamic,
diverse, and frequently volatile. To address these challenges, we propose the
FlowHFT, a novel imitation learning framework based on flow matching policy.
FlowHFT simultaneously learns strategies from numerous expert models, each
proficient in particular market scenarios. As a result, our framework can
adaptively adjust investment decisions according to the prevailing market
state. Furthermore, FlowHFT incorporates a grid-search fine-tuning mechanism.
This allows it to refine strategies and achieve superior performance even in
complex or extreme market scenarios where expert strategies may be suboptimal.
We test FlowHFT in multiple market environments. We first show that flow
matching policy is applicable in stochastic market environments, thus enabling
FlowHFT to learn trading strategies under different market conditions. Notably,
our single framework consistently achieves performance superior to the best
expert for each market condition.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [116] [An Overview of the Prospects and Challenges of Using Artificial Intelligence for Energy Management Systems in Microgrids](https://arxiv.org/abs/2505.05498)
*Noor ul Misbah Khanum,Hayssam Dahrouj,Ramesh C. Bansal,Hissam Mouayad Tawfik*

Main category: eess.SY

TL;DR: 该论文探讨了AI在微电网能量管理系统中的潜力，分析了AI在解决可再生能源预测、网络安全、成本控制等挑战时的效率，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 微电网在可持续发展中至关重要，但其面临多种能量管理挑战（如预测可靠性、网络安全等），AI被视为解决这些问题的有效工具。

Method: 通过研究AI在微电网能量管理系统中的适用性和效率，分析AI如何实现技术与经济目标。

Result: AI展示了优化微电网能量管理的巨大潜力，特别是在提升效率和可靠性方面。

Conclusion: 未来研究方向包括自愈微电网、区块链技术集成、IoT应用等，需解决可解释性、数据隐私和生成式AI等问题。

Abstract: Microgrids have emerged as a pivotal solution in the quest for a sustainable
and energy-efficient future. While microgrids offer numerous advantages, they
are also prone to issues related to reliably forecasting renewable energy
demand and production, protecting against cyberattacks, controlling operational
costs, optimizing power flow, and regulating the performance of energy
management systems (EMS). Tackling these energy management challenges is
essential to facilitate microgrid applications and seamlessly incorporate
renewable energy resources. Artificial intelligence (AI) has recently
demonstrated immense potential for optimizing energy management in microgrids,
providing efficient and reliable solutions. This paper highlights the combined
benefits of enabling AI-based methodologies in the energy management systems of
microgrids by examining the applicability and efficiency of AI-based EMS in
achieving specific technical and economic objectives. The paper also points out
several future research directions that promise to spearhead AI-driven EMS,
namely the development of self-healing microgrids, integration with blockchain
technology, use of Internet of things (IoT), and addressing interpretability,
data privacy, scalability, and the prospects to generative AI in the context of
future AI-based EMS.

</details>


### [117] [Human-in-the-Loop AI for HVAC Management Enhancing Comfort and Energy Efficiency](https://arxiv.org/abs/2505.05796)
*Xinyu Liang,Frits de Nijs,Buser Say,Hao Wang*

Main category: eess.SY

TL;DR: 这篇论文提出了一个基于人机交互（HITL）的人工智能框架，用于优化HVAC系统的性能，通过结合实时用户反馈和电价波动，实现节能与舒适度的动态平衡。


<details>
  <summary>Details</summary>
Motivation: HVAC系统占全球建筑能耗的38%，传统系统无法动态响应电价变化或个人舒适偏好，导致能耗增加和舒适度下降。因此，需要一种能自适应学习和调整的解决方案。

Method: 提出了一种结合实时用户反馈和强化学习的框架，通过预测模型优化HVAC运行，无需预定义信息即可动态调整。

Result: 模拟结果显示，该方法在降低能耗成本的同时保持或提升了舒适度，优于传统方法。

Conclusion: 这种反馈驱动的框架为HVAC系统提供了一种可扩展的解决方案，平衡了个人偏好与经济环保目标。

Abstract: Heating, Ventilation, and Air Conditioning (HVAC) systems account for
approximately 38% of building energy consumption globally, making them one of
the most energy-intensive services. The increasing emphasis on energy
efficiency and sustainability, combined with the need for enhanced occupant
comfort, presents a significant challenge for traditional HVAC systems. These
systems often fail to dynamically adjust to real-time changes in electricity
market rates or individual comfort preferences, leading to increased energy
costs and reduced comfort. In response, we propose a Human-in-the-Loop (HITL)
Artificial Intelligence framework that optimizes HVAC performance by
incorporating real-time user feedback and responding to fluctuating electricity
prices. Unlike conventional systems that require predefined information about
occupancy or comfort levels, our approach learns and adapts based on ongoing
user input. By integrating the occupancy prediction model with reinforcement
learning, the system improves operational efficiency and reduces energy costs
in line with electricity market dynamics, thereby contributing to demand
response initiatives. Through simulations, we demonstrate that our method
achieves significant cost reductions compared to baseline approaches while
maintaining or enhancing occupant comfort. This feedback-driven approach
ensures personalized comfort control without the need for predefined settings,
offering a scalable solution that balances individual preferences with economic
and environmental goals.

</details>


### [118] [Leveraging Multi-Task Learning for Multi-Label Power System Security Assessment](https://arxiv.org/abs/2505.06207)
*Muhy Eddin Za'ter,Amir Sajad,Bri-Mathias Hodge*

Main category: eess.SY

TL;DR: 本文提出了一种新的电力系统安全评估方法，利用多任务学习（MTL）将问题重新定义为多标签分类任务，通过共享编码器和多个解码器提升评估精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 电力系统安全评估通常需要独立分析多个稳定性问题（如静态、电压、暂态和小信号稳定性），现有方法在准确性和效率上存在不足，因此引入多任务学习框架来同时优化这些问题。

Method: 采用多任务学习框架，设计共享编码器和多个解码器，分别对应不同稳定性任务，实现知识共享和高效评估。并在IEEE 68总线系统上进行实验。

Result: 实验表明，该方法在电力系统安全评估的多项稳定性指标上优于现有最先进的机器学习方法，展现出更高的准确性和可解释性。

Conclusion: 该多任务学习框架为电力系统安全评估提供了一种高效且准确的解决方案，通过知识共享和多标签分类显著提升了性能。

Abstract: This paper introduces a novel approach to the power system security
assessment using Multi-Task Learning (MTL), and reformulating the problem as a
multi-label classification task. The proposed MTL framework simultaneously
assesses static, voltage, transient, and small-signal stability, improving both
accuracy and interpretability with respect to the most state of the art machine
learning methods. It consists of a shared encoder and multiple decoders,
enabling knowledge transfer between stability tasks. Experiments on the IEEE
68-bus system demonstrate a measurable superior performance of the proposed
method compared to the extant state-of-the-art approaches.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [119] [Unsupervised Blind Speech Separation with a Diffusion Prior](https://arxiv.org/abs/2505.05657)
*Zhongweiyang Xu,Xulin Fan,Zhong-Qiu Wang,Xilin Jiang,Romit Roy Choudhury*

Main category: eess.AS

TL;DR: 论文提出了ArrayDPS方法，以无监督、阵列无关的方式解决盲语音分离问题，通过扩散后验采样和优化近似实现，性能优于无监督基线方法。


<details>
  <summary>Details</summary>
Motivation: 盲语音分离是一个具有挑战性的逆问题，涉及未知的麦克风阵列几何形状、房间脉冲响应和语音源。传统方法通常依赖监督学习或已知阵列信息，而本研究旨在开发一种无监督且不依赖于阵列信息的方法。

Method: ArrayDPS基于扩散后验采样（DPS），通过单独优化问题近似房间声学和麦克风间相对传递函数，结合扩散先验，迭代实现语音分离。仅需单说话人语音扩散模型作为先验，无需阵列信息。

Result: ArrayDPS在信号失真比（SDR）上优于所有无监督基线方法，并与监督方法性能相当。

Conclusion: ArrayDPS提供了一种高效的无监督盲语音分离解决方案，无需阵列信息，具有实际应用潜力。

Abstract: Blind Speech Separation (BSS) aims to separate multiple speech sources from
audio mixtures recorded by a microphone array. The problem is challenging
because it is a blind inverse problem, i.e., the microphone array geometry, the
room impulse response (RIR), and the speech sources, are all unknown. We
propose ArrayDPS to solve the BSS problem in an unsupervised, array-agnostic,
and generative manner. The core idea builds on diffusion posterior sampling
(DPS), but unlike DPS where the likelihood is tractable, ArrayDPS must
approximate the likelihood by formulating a separate optimization problem. The
solution to the optimization approximates room acoustics and the relative
transfer functions between microphones. These approximations, along with the
diffusion priors, iterate through the ArrayDPS sampling process and ultimately
yield separated voice sources. We only need a simple single-speaker speech
diffusion model as a prior along with the mixtures recorded at the microphones;
no microphone array information is necessary. Evaluation results show that
ArrayDPS outperforms all baseline unsupervised methods while being comparable
to supervised methods in terms of SDR. Audio demos are provided at:
https://arraydps.github.io/ArrayDPSDemo/.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [120] [Minimal Sequent Calculus for Teaching First-Order Logic: Lessons Learned](https://arxiv.org/abs/2505.05988)
*Jørgen Villadsen*

Main category: cs.LO

TL;DR: MiniCalc是一个用于教授一阶逻辑的网页应用，基于最小序贯演算，并可选在Isabelle证明助手中验证证明。


<details>
  <summary>Details</summary>
Motivation: 旨在通过MiniCalc工具教授一阶逻辑，并分享近年来在大学使用该工具的经验。

Method: 基于最小序贯演算开发网页应用，支持Isabelle验证。

Result: 提供了使用MiniCalc工具的教学经验和反馈。

Conclusion: MiniCalc作为一种教学工具，有效辅助了一阶逻辑的教学，并结合Isabelle增强了验证能力。

Abstract: MiniCalc is a web app for teaching first-order logic based on a minimal
sequent calculus. As an option the proofs can be verified in the Isabelle proof
assistant. We present the lessons learned using the tool in recent years at our
university.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [121] [CLAM: Continuous Latent Action Models for Robot Learning from Unlabeled Demonstrations](https://arxiv.org/abs/2505.04999)
*Anthony Liang,Pavel Czempin,Matthew Hong,Yutai Zhou,Erdem Biyik,Stephen Tu*

Main category: cs.RO

TL;DR: 摘要主要介绍了CLAM（连续潜在动作模型），该模型通过无监督学习从无标记的观测数据中学习潜在动作标签，解决了模仿学习中需要大量标记专家示范的问题。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习依赖大量标记专家示范，成本高且难以扩展。本文旨在利用丰富的无标记观测数据（如视频示范），通过无监督学习解决这一瓶颈，尤其是在需要精细动作的复杂机器人任务中。

Method: 提出CLAM模型，结合两个关键设计：(a) 使用连续潜在动作标签而非离散表示，(b) 联合训练动作解码器，确保潜在动作空间能以少量标记示例快速映射到真实动作。标记数据可来自非最优示范，无需专家数据。

Result: 在DMControl（运动）和MetaWorld（操作）的连续控制任务以及真实WidowX机械臂上，CLAM显著优于现有方法，任务成功率提升2-3倍。

Conclusion: CLAM通过无监督学习从无标记数据中学习连续潜在动作，大幅减少对标记专家数据的依赖，且在复杂任务中表现优越。

Abstract: Learning robot policies using imitation learning requires collecting large
amounts of costly action-labeled expert demonstrations, which fundamentally
limits the scale of training data. A promising approach to address this
bottleneck is to harness the abundance of unlabeled observations-e.g., from
video demonstrations-to learn latent action labels in an unsupervised way.
However, we find that existing methods struggle when applied to complex robot
tasks requiring fine-grained motions. We design continuous latent action models
(CLAM) which incorporate two key ingredients we find necessary for learning to
solve complex continuous control tasks from unlabeled observation data: (a)
using continuous latent action labels instead of discrete representations, and
(b) jointly training an action decoder to ensure that the latent action space
can be easily grounded to real actions with relatively few labeled examples.
Importantly, the labeled examples can be collected from non-optimal play data,
enabling CLAM to learn performant policies without access to any action-labeled
expert data. We demonstrate on continuous control benchmarks in DMControl
(locomotion) and MetaWorld (manipulation), as well as on a real WidowX robot
arm that CLAM significantly outperforms prior state-of-the-art methods,
remarkably with a 2-3x improvement in task success rate compared to the best
baseline. Videos and code can be found at clamrobot.github.io.

</details>


### [122] [Adaptive Stress Testing Black-Box LLM Planners](https://arxiv.org/abs/2505.05665)
*Neeloy Chakraborty,John Pohovey,Melkior Ornik,Katherine Driggs-Campbell*

Main category: cs.RO

TL;DR: 该论文提出了一种使用自适应压力测试（AST）和蒙特卡洛树搜索（MCTS）的新方法，通过生成多样化的提示扰动来检测大语言模型（LLMs）的不安全或幻觉输出，从而在安全关键场景中提高模型的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在决策任务中表现出色，但其可能产生不安全或幻觉输出的风险需要被检测，尤其是在安全关键场景中。

Method: 论文提出了一种结合AST和MCTS的方法，通过高效搜索提示扰动空间来发现导致模型高不确定性的场景和提示。

Result: 通过生成多样化的MCTS提示扰动树，该方法能够在运行时自动生成影响模型不确定性的提示，并为实时信任评估提供支持。

Conclusion: 该方法有效地提升了在安全关键场景中对LLM输出的可靠性评估能力。

Abstract: Large language models (LLMs) have recently demonstrated success in
generalizing across decision-making tasks including planning, control and
prediction, but their tendency to hallucinate unsafe and undesired outputs
poses risks. We argue that detecting such failures is necessary, especially in
safety-critical scenarios. Existing black-box methods often detect
hallucinations by identifying inconsistencies across multiple samples. Many of
these approaches typically introduce prompt perturbations like randomizing
detail order or generating adversarial inputs, with the intuition that a
confident model should produce stable outputs. We first perform a manual case
study showing that other forms of perturbations (e.g., adding noise, removing
sensor details) cause LLMs to hallucinate in a driving environment. We then
propose a novel method for efficiently searching the space of prompt
perturbations using Adaptive Stress Testing (AST) with Monte-Carlo Tree Search
(MCTS). Our AST formulation enables discovery of scenarios and prompts that
cause language models to act with high uncertainty. By generating MCTS prompt
perturbation trees across diverse scenarios, we show that offline analyses can
be used at runtime to automatically generate prompts that influence model
uncertainty, and to inform real-time trust assessments of an LLM.

</details>


### [123] [Flight Validation of Learning-Based Trajectory Optimization for the Astrobee Free-Flyer](https://arxiv.org/abs/2505.05588)
*Somrita Banerjee,Abhishek Cauligi,Marco Pavone*

Main category: cs.RO

TL;DR: 机器学习加速太空机器人轨迹优化，首次在国际空间站验证学习型控制。


<details>
  <summary>Details</summary>
Motivation: 解决太空应用中轨迹优化计算量大的问题，提升资源受限平台上的实时优化效率。

Method: 结合GuSTO序列凸编程框架和离线训练的神经网络，生成初始轨迹以加速优化。

Result: 成功在国际空间站使用Astrobee飞行机器人验证了方法的有效性。

Conclusion: 该方法为资源有限的太空平台提供了更快的实时轨迹优化解决方案。

Abstract: Although widely used in commercial and industrial robotics, trajectory
optimization has seen limited use in space applications due to its high
computational demands. In this work, we present flight results from experiments
with the Astrobee free-flying robot on board the International Space Station
(ISS), that demonstrate how machine learning can accelerate on-board trajectory
optimization while preserving theoretical solver guarantees. To the best of the
authors' knowledge, this is the first-ever demonstration of learning-based
control on the ISS. Our approach leverages the GuSTO sequential convex
programming framework and uses a neural network, trained offline, to map
problem parameters to effective initial ``warm-start'' trajectories, paving the
way for faster real-time optimization on resource-constrained space platforms.

</details>


### [124] [CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory](https://arxiv.org/abs/2505.05622)
*Weichen Zhang,Chen Gao,Shiquan Yu,Ruiying Peng,Baining Zhao,Qian Zhang,Jinqiang Cui,Xinlei Chen,Yong Li*

Main category: cs.RO

TL;DR: 本文提出CityNavAgent，一种基于大语言模型的智能体，通过分层语义规划模块和全局记忆模块显著降低城市空中视觉-语言导航的复杂性，实现最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有地面视觉-语言导航智能体在空中导航中因缺乏预定义导航图和动作空间扩展而表现不佳的问题。

Method: 设计分层语义规划模块（HSPM）将长程任务分解为子目标，并利用全局记忆模块存储历史轨迹以简化导航。

Result: 实验表明该方法在连续城市环境中实现最佳性能，各模块均有效。

Conclusion: CityNavAgent通过分层规划和全局记忆成功解决了空中视觉-语言导航的挑战，展现了实际部署潜力。

Abstract: Aerial vision-and-language navigation (VLN), requiring drones to interpret
natural language instructions and navigate complex urban environments, emerges
as a critical embodied AI challenge that bridges human-robot interaction, 3D
spatial reasoning, and real-world deployment. Although existing ground VLN
agents achieved notable results in indoor and outdoor settings, they struggle
in aerial VLN due to the absence of predefined navigation graphs and the
exponentially expanding action space in long-horizon exploration. In this work,
we propose \textbf{CityNavAgent}, a large language model (LLM)-empowered agent
that significantly reduces the navigation complexity for urban aerial VLN.
Specifically, we design a hierarchical semantic planning module (HSPM) that
decomposes the long-horizon task into sub-goals with different semantic levels.
The agent reaches the target progressively by achieving sub-goals with
different capacities of the LLM. Additionally, a global memory module storing
historical trajectories into a topological graph is developed to simplify
navigation for visited targets. Extensive benchmark experiments show that our
method achieves state-of-the-art performance with significant improvement.
Further experiments demonstrate the effectiveness of different modules of
CityNavAgent for aerial VLN in continuous city environments. The code is
available at \href{https://github.com/VinceOuti/CityNavAgent}{link}.

</details>


### [125] [Closing the Loop: Motion Prediction Models beyond Open-Loop Benchmarks](https://arxiv.org/abs/2505.05638)
*Mohamed-Khalil Bouzidi,Christian Schlauch,Nicole Scheuerer,Yue Yao,Nadja Klein,Daniel Göhring,Jörg Reichardt*

Main category: cs.RO

TL;DR: 论文发现，运动预测模型的开放式预测精度提升并不总是转化为闭环驾驶行为的改进，且小型化模型有时表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有运动预测竞赛和基准过于关注开放式预测的微小精度提升，而忽视了其对实际自动驾驶系统闭环性能的影响。

Method: 系统评估了最先进的运动预测模型与运动规划器之间的交互，并测试了参数减少的模型变体。

Result: 结果表明，开放式预测精度与闭环驾驶行为无直接关联，且部分小型化模型（参数减少86%）表现相当或更好。

Conclusion: 设计预测模型时需考虑时间一致性和规划器兼容性，而非仅追求开放式精度；小型化模型可能更具实用性。

Abstract: Fueled by motion prediction competitions and benchmarks, recent years have
seen the emergence of increasingly large learning based prediction models, many
with millions of parameters, focused on improving open-loop prediction accuracy
by mere centimeters. However, these benchmarks fail to assess whether such
improvements translate to better performance when integrated into an autonomous
driving stack. In this work, we systematically evaluate the interplay between
state-of-the-art motion predictors and motion planners. Our results show that
higher open-loop accuracy does not always correlate with better closed-loop
driving behavior and that other factors, such as temporal consistency of
predictions and planner compatibility, also play a critical role. Furthermore,
we investigate downsized variants of these models, and, surprisingly, find that
in some cases models with up to 86% fewer parameters yield comparable or even
superior closed-loop driving performance. Our code is available at
https://github.com/continental/pred2plan.

</details>


### [126] [Towards Embodiment Scaling Laws in Robot Locomotion](https://arxiv.org/abs/2505.05753)
*Bo Ai,Liu Dai,Nico Bohlinger,Dichen Li,Tongzhou Mu,Zhanxin Wu,K. Fay,Henrik I. Christensen,Jan Peters,Hao Su*

Main category: cs.RO

TL;DR: 论文研究了通过增加训练中的机器人形态数量，可以提升未见形态的泛化能力，并在真实世界中展示了零次迁移的能力。


<details>
  <summary>Details</summary>
Motivation: 开发能适应多种任务、环境和形态的通用智能体是机器人与人工智能的重要挑战。本文聚焦形态多样性，验证形态数量对泛化的影响。

Method: 通过过程化生成约1000种形态的数据集，训练通用策略处理多样的观测与动作空间，并测试形态数量对泛化的作用。

Result: 增加训练形态能显著提升未见形态的泛化能力，且效果优于固定小规模形态的数据增强。最佳策略能零次迁移至真实机器人（如Unitree Go2和H1）。

Conclusion: 形态扩展是实现通用具身智能的有效路径，对可配置机器人的自适应控制、形态与控制的协同设计等具有重要意义。

Abstract: Developing generalist agents that can operate across diverse tasks,
environments, and physical embodiments is a grand challenge in robotics and
artificial intelligence. In this work, we focus on the axis of embodiment and
investigate embodiment scaling laws$\unicode{x2013}$the hypothesis that
increasing the number of training embodiments improves generalization to unseen
ones. Using robot locomotion as a test bed, we procedurally generate a dataset
of $\sim$1,000 varied embodiments, spanning humanoids, quadrupeds, and
hexapods, and train generalist policies capable of handling diverse observation
and action spaces on random subsets. We find that increasing the number of
training embodiments improves generalization to unseen ones, and scaling
embodiments is more effective in enabling embodiment-level generalization than
scaling data on small, fixed sets of embodiments. Notably, our best policy,
trained on the full dataset, zero-shot transfers to novel embodiments in the
real world, such as Unitree Go2 and H1. These results represent a step toward
general embodied intelligence, with potential relevance to adaptive control for
configurable robots, co-design of morphology and control, and beyond.

</details>


### [127] [Multi-Agent Systems for Robotic Autonomy with LLMs](https://arxiv.org/abs/2505.05762)
*Junhong Chen,Ziqi Yang,Haoyuan G Xu,Dandan Zhang,George Mylonas*

Main category: cs.RO

TL;DR: 本文提出了基于大型语言模型（LLMs）的多智能体框架，用于机器人任务分析、机械设计和路径生成，实验表明该系统能有效设计可行机器人及其控制策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在AI和机器人领域备受关注，研究旨在通过多智能体框架提升机器人系统开发的效率和可访问性。

Method: 框架包含三个核心智能体：任务分析员、机器人设计员和强化学习设计员，输出为多模态结果（如代码文件或技术报告）。

Result: 实验表明，系统能为给定任务设计可行的机器人及控制策略，展现出在研究和工业应用中的潜力。

Conclusion: 该系统通过LLMs和多智能体协作显著提升了机器人开发的效率和实用性，适用于广泛场景。

Abstract: Since the advent of Large Language Models (LLMs), various research based on
such models have maintained significant academic attention and impact,
especially in AI and robotics. In this paper, we propose a multi-agent
framework with LLMs to construct an integrated system for robotic task
analysis, mechanical design, and path generation. The framework includes three
core agents: Task Analyst, Robot Designer, and Reinforcement Learning Designer.
Outputs are formatted as multimodal results, such as code files or technical
reports, for stronger understandability and usability. To evaluate
generalizability comparatively, we conducted experiments with models from both
GPT and DeepSeek. Results demonstrate that the proposed system can design
feasible robots with control strategies when appropriate task inputs are
provided, exhibiting substantial potential for enhancing the efficiency and
accessibility of robotic system development in research and industrial
applications.

</details>


### [128] [UniVLA: Learning to Act Anywhere with Task-centric Latent Actions](https://arxiv.org/abs/2505.06111)
*Qingwen Bu,Yanting Yang,Jisong Cai,Shenyuan Gao,Guanghui Ren,Maoqing Yao,Ping Luo,Hongyang Li*

Main category: cs.RO

TL;DR: UniVLA is a new framework for learning cross-embodiment vision-language-action policies, using latent action models to transfer knowledge across diverse robots and environments, achieving superior efficiency and performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing robot learning approaches rely heavily on action-annotated data, limiting their adaptability to different embodiments and environments. UniVLA aims to overcome this by leveraging cross-embodiment learning from diverse videos.

Method: UniVLA derives task-centric action representations from videos using a latent action model in the DINO feature space, integrating language instructions to filter irrelevant dynamics.

Result: UniVLA achieves state-of-the-art performance in manipulation and navigation benchmarks, with significant efficiency gains (1/20 pretraining compute and 1/10 downstream data vs. OpenVLA). Performance improves further with heterogeneous data.

Conclusion: UniVLA demonstrates scalable and efficient robot policy learning, showcasing potential for generalist robot applications through cross-embodiment knowledge transfer.

Abstract: A generalist robot should perform effectively across various environments.
However, most existing approaches heavily rely on scaling action-annotated data
to enhance their capabilities. Consequently, they are often limited to single
physical specification and struggle to learn transferable knowledge across
different embodiments and environments. To confront these limitations, we
propose UniVLA, a new framework for learning cross-embodiment
vision-language-action (VLA) policies. Our key innovation is to derive
task-centric action representations from videos with a latent action model.
This enables us to exploit extensive data across a wide spectrum of embodiments
and perspectives. To mitigate the effect of task-irrelevant dynamics, we
incorporate language instructions and establish a latent action model within
the DINO feature space. Learned from internet-scale videos, the generalist
policy can be deployed to various robots through efficient latent action
decoding. We obtain state-of-the-art results across multiple manipulation and
navigation benchmarks, as well as real-robot deployments. UniVLA achieves
superior performance over OpenVLA with less than 1/20 of pretraining compute
and 1/10 of downstream data. Continuous performance improvements are observed
as heterogeneous data, even including human videos, are incorporated into the
training pipeline. The results underscore UniVLA's potential to facilitate
scalable and efficient robot policy learning.

</details>


### [129] [Learning to Drive Anywhere with Model-Based Reannotation11](https://arxiv.org/abs/2505.05592)
*Noriaki Hirose,Lydia Ignatova,Kyle Stachowicz,Catherine Glossop,Sergey Levine,Dhruv Shah*

Main category: cs.RO

TL;DR: 论文提出了一种名为MBRA的框架，利用模型重新标注被动收集的数据，以训练通用的视觉导航策略LogoNav。该方法在大规模数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉导航策略受限于数据集规模小、多样性不足，难以实现广泛的泛化能力。作者希望通过利用被动收集的众包数据和YouTube视频，尽管其质量较低或缺少动作标签，来克服这一限制。

Method: 作者提出了MBRA框架，利用学习到的短时模型专家模型为被动数据集重新标注或生成高质量的动作标签。随后，将这些数据提炼为一个长时导航策略LogoNav，该策略基于视觉目标或GPS航点。

Result: LogoNav在未经见的环境（室内外）中实现了超过300米的稳健导航，并在全球六个城市的真实环境中（包括人群密集场景）验证了其泛化能力。该方法达到了当前最优性能。

Conclusion: 研究表明，通过MBRA处理被动数据可以显著提升导航策略的泛化能力，为机器人视觉导航提供了新的数据利用思路。

Abstract: Developing broadly generalizable visual navigation policies for robots is a
significant challenge, primarily constrained by the availability of
large-scale, diverse training data. While curated datasets collected by
researchers offer high quality, their limited size restricts policy
generalization. To overcome this, we explore leveraging abundant, passively
collected data sources, including large volumes of crowd-sourced teleoperation
data and unlabeled YouTube videos, despite their potential for lower quality or
missing action labels. We propose Model-Based ReAnnotation (MBRA), a framework
that utilizes a learned short-horizon, model-based expert model to relabel or
generate high-quality actions for these passive datasets. This relabeled data
is then distilled into LogoNav, a long-horizon navigation policy conditioned on
visual goals or GPS waypoints. We demonstrate that LogoNav, trained using
MBRA-processed data, achieves state-of-the-art performance, enabling robust
navigation over distances exceeding 300 meters in previously unseen indoor and
outdoor environments. Our extensive real-world evaluations, conducted across a
fleet of robots (including quadrupeds) in six cities on three continents,
validate the policy's ability to generalize and navigate effectively even
amidst pedestrians in crowded settings.

</details>


### [130] [Efficient Sensorimotor Learning for Open-world Robot Manipulation](https://arxiv.org/abs/2505.06136)
*Yifeng Zhu*

Main category: cs.RO

TL;DR: 论文提出了开环世界机器人操控的新方法，重点是利用有限的演示数据中的规律进行高效学习，从而实现对新任务和对象的快速适应。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器人未预编程或预训练的任务、对象或场景快速适应问题，研究提出利用数据中的规律来实现高效学习和泛化。

Method: 通过对象中心先验、空间理解能力和技能复用三种方法，实现小样本学习和视频观察模仿。

Result: 该方法使机器人能够从少量数据中学习通用操控策略，并能持续模仿多个任务。

Conclusion: 研究成果为构建低成本数据适应、易与人交互的通用个人机器人奠定了基础。

Abstract: This dissertation considers Open-world Robot Manipulation, a manipulation
problem where a robot must generalize or quickly adapt to new objects, scenes,
or tasks for which it has not been pre-programmed or pre-trained. This
dissertation tackles the problem using a methodology of efficient sensorimotor
learning. The key to enabling efficient sensorimotor learning lies in
leveraging regular patterns that exist in limited amounts of demonstration
data. These patterns, referred to as ``regularity,'' enable the data-efficient
learning of generalizable manipulation skills. This dissertation offers a new
perspective on formulating manipulation problems through the lens of
regularity. Building upon this notion, we introduce three major contributions.
First, we introduce methods that endow robots with object-centric priors,
allowing them to learn generalizable, closed-loop sensorimotor policies from a
small number of teleoperation demonstrations. Second, we introduce methods that
constitute robots' spatial understanding, unlocking their ability to imitate
manipulation skills from in-the-wild video observations. Last but not least, we
introduce methods that enable robots to identify reusable skills from their
past experiences, resulting in systems that can continually imitate multiple
tasks in a sequential manner. Altogether, the contributions of this
dissertation help lay the groundwork for building general-purpose personal
robots that can quickly adapt to new situations or tasks with low-cost data
collection and interact easily with humans. By enabling robots to learn and
generalize from limited data, this dissertation takes a step toward realizing
the vision of intelligent robotic assistants that can be seamlessly integrated
into everyday scenarios.

</details>


### [131] [Let Humanoids Hike! Integrative Skill Development on Complex Trails](https://arxiv.org/abs/2505.06218)
*Kwan-Yee Lin,Stella X. Yu*

Main category: cs.RO

TL;DR: 该论文提出了一种名为LEGO-H的学习框架，旨在训练人形机器人自主徒步复杂小径，整合视觉感知、决策制定和运动执行能力。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人研究存在碎片化问题，无法应对徒步复杂地形的挑战，缺乏长期目标和情境感知。LEGO-H旨在填补这一空白。

Method: 通过两种技术创新实现：一是基于时间视觉变换器的分层强化学习框架，预测未来目标指导运动；二是潜在关节运动模式表示结合分层度量学习，实现策略从训练到实际执行的平滑转移。

Result: 实验表明，LEGO-H能应对多样的物理和环境挑战，无需依赖预定义运动模式。

Conclusion: LEGO-H展示了徒步作为机器人自主性测试平台的潜力，并为未来人形机器人发展提供了基准。

Abstract: Hiking on complex trails demands balance, agility, and adaptive
decision-making over unpredictable terrain. Current humanoid research remains
fragmented and inadequate for hiking: locomotion focuses on motor skills
without long-term goals or situational awareness, while semantic navigation
overlooks real-world embodiment and local terrain variability. We propose
training humanoids to hike on complex trails, driving integrative skill
development across visual perception, decision making, and motor execution. We
develop a learning framework, LEGO-H, that enables a vision-equipped humanoid
robot to hike complex trails autonomously. We introduce two technical
innovations: 1) A temporal vision transformer variant - tailored into
Hierarchical Reinforcement Learning framework - anticipates future local goals
to guide movement, seamlessly integrating locomotion with goal-directed
navigation. 2) Latent representations of joint movement patterns, combined with
hierarchical metric learning - enhance Privileged Learning scheme - enable
smooth policy transfer from privileged training to onboard execution. These
components allow LEGO-H to handle diverse physical and environmental challenges
without relying on predefined motion patterns. Experiments across varied
simulated trails and robot morphologies highlight LEGO-H's versatility and
robustness, positioning hiking as a compelling testbed for embodied autonomy
and LEGO-H as a baseline for future humanoid development.

</details>


### [132] [Physics-informed Temporal Difference Metric Learning for Robot Motion Planning](https://arxiv.org/abs/2505.05691)
*Ruiqi Ni,Zherong Pan,Ahmed H Qureshi*

Main category: cs.RO

TL;DR: 本文提出了一种新型自监督学习方法来更准确地解决运动规划中的Eikonal方程，增强了在复杂和未知环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法在复杂环境中难以维持Eikonal方程的关键特性（如最优值函数和测地距离），限制了其性能。

Method: 采用基于时间差分度量的自监督学习方法，强制满足贝尔曼最优性原则，避免局部极小值，同时通过度量学习保留测地特性。

Result: 在2至12自由度机器人配置的复杂和未知环境中，该方法显著优于现有自监督方法。

Conclusion: 所提方法通过更精确地解决Eikonal方程，有效提升了复杂环境中的运动规划性能，并具备良好的泛化能力。

Abstract: The motion planning problem involves finding a collision-free path from a
robot's starting to its target configuration. Recently, self-supervised
learning methods have emerged to tackle motion planning problems without
requiring expensive expert demonstrations. They solve the Eikonal equation for
training neural networks and lead to efficient solutions. However, these
methods struggle in complex environments because they fail to maintain key
properties of the Eikonal equation, such as optimal value functions and
geodesic distances. To overcome these limitations, we propose a novel
self-supervised temporal difference metric learning approach that solves the
Eikonal equation more accurately and enhances performance in solving complex
and unseen planning tasks. Our method enforces Bellman's principle of
optimality over finite regions, using temporal difference learning to avoid
spurious local minima while incorporating metric learning to preserve the
Eikonal equation's essential geodesic properties. We demonstrate that our
approach significantly outperforms existing self-supervised learning methods in
handling complex environments and generalizing to unseen environments, with
robot configurations ranging from 2 to 12 degrees of freedom (DOF).

</details>


### [133] [Collecting Human Motion Data in Large and Occlusion-Prone Environments using Ultra-Wideband Localization](https://arxiv.org/abs/2505.05851)
*Janik Kaden,Maximilian Hilger,Tim Schreiter,Marius Schaab,Thomas Graichen,Andrey Rudenko,Ulrich Heinkel,Achim J. Lilienthal*

Main category: cs.RO

TL;DR: 研究探讨了使用超宽带（UWB）定位技术作为可扩展替代方案，用于在拥挤和易遮挡环境中捕捉人体运动，并结合多种传感器模态进行验证。


<details>
  <summary>Details</summary>
Motivation: 随着机器人越来越多地融入人类环境，理解和预测人体运动对于安全高效的互动至关重要。传统运动捕捉系统硬件设置复杂、成本高且难以部署在新环境中，因此需要一种更可扩展的解决方案。

Method: 采用UWB定位技术，并结合眼动追踪、机器人LiDAR和雷达传感器等多种感知模态，同时在模拟博物馆环境中记录运动捕捉数据作为地面真值进行对比评估。

Result: 研究提供了超过130分钟的多模态数据，展示了UWB技术在复杂环境中的潜力，为在更大规模场景（如仓库、机场等）中评估此类技术奠定了基础。

Conclusion: UWB技术可作为视觉系统之外的另一种选择，为复杂环境中的人体运动数据采集提供了一种可扩展且准确的解决方案。

Abstract: With robots increasingly integrating into human environments, understanding
and predicting human motion is essential for safe and efficient interactions.
Modern human motion and activity prediction approaches require high quality and
quantity of data for training and evaluation, usually collected from motion
capture systems, onboard or stationary sensors. Setting up these systems is
challenging due to the intricate setup of hardware components, extensive
calibration procedures, occlusions, and substantial costs. These constraints
make deploying such systems in new and large environments difficult and limit
their usability for in-the-wild measurements. In this paper we investigate the
possibility to apply the novel Ultra-Wideband (UWB) localization technology as
a scalable alternative for human motion capture in crowded and occlusion-prone
environments. We include additional sensing modalities such as eye-tracking,
onboard robot LiDAR and radar sensors, and record motion capture data as ground
truth for evaluation and comparison. The environment imitates a museum setup,
with up to four active participants navigating toward random goals in a natural
way, and offers more than 130 minutes of multi-modal data. Our investigation
provides a step toward scalable and accurate motion data collection beyond
vision-based systems, laying a foundation for evaluating sensing modalities
like UWB in larger and complex environments like warehouses, airports, or
convention centers.

</details>


### [134] [Active Perception for Tactile Sensing: A Task-Agnostic Attention-Based Approach](https://arxiv.org/abs/2505.06182)
*Tim Schneider,Cristiana de Farias,Roberto Calandra,Liming Chen,Jan Peters*

Main category: cs.RO

TL;DR: 提出了一种名为TAP的任务无关主动感知框架，结合强化学习和Transformer架构，用于解决部分可观测环境中的挑战，并在触觉数字识别和姿态估计任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 人类通过触觉探索来识别物体属性，而机器人需要类似的多模态感知能力。现有方法通常针对特定任务设计，缺乏通用性。

Method: TAP框架整合了Soft Actor-Critic (SAC)和CrossQ算法，通过统一优化目标联合训练感知模块和决策策略，且设计为任务无关。

Result: 在Tactile MNIST触觉数字识别和触觉姿态估计任务中取得高准确率，验证了框架的通用性和有效性。

Conclusion: TAP作为一种通用框架，有潜力推动机器人主动触觉感知的发展。

Abstract: Humans make extensive use of haptic exploration to map and identify the
properties of the objects that we touch. In robotics, active tactile perception
has emerged as an important research domain that complements vision for tasks
such as object classification, shape reconstruction, and manipulation. This
work introduces TAP (Task-agnostic Active Perception) -- a novel framework that
leverages reinforcement learning (RL) and transformer-based architectures to
address the challenges posed by partially observable environments. TAP
integrates Soft Actor-Critic (SAC) and CrossQ algorithms within a unified
optimization objective, jointly training a perception module and
decision-making policy. By design, TAP is completely task-agnostic and can, in
principle, generalize to any active perception problem. We evaluate TAP across
diverse tasks, including toy examples and realistic applications involving
haptic exploration of 3D models from the Tactile MNIST benchmark. Experiments
demonstrate the efficacy of TAP, achieving high accuracies on the Tactile MNIST
haptic digit recognition task and a tactile pose estimation task. These
findings underscore the potential of TAP as a versatile and generalizable
framework for advancing active tactile perception in robotics.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [135] [Structure & Quality: Conceptual and Formal Foundations for the Mind-Body Problem](https://arxiv.org/abs/2505.05481)
*Ryan Williams*

Main category: q-bio.NC

TL;DR: 这篇论文从结构（structure）与质（quality）的关系入手，探索了意识难题，提出了一种新的分析框架（Q-S空间），并基于此分类了五种可能的关系，同时探讨了各类的哲学意义。


<details>
  <summary>Details</summary>
Motivation: 传统意识研究常区分物理与心智，本文则试图从更基础的结构与质的关系切入，为意识问题提供新的分析视角。

Method: 采用信息论方法，开发了量化结构-质互定性的指标，并构建了Q-S空间模型，通过形式化与概念模型分析其关系。

Result: 提出了五种结构-质关系的分类，并探讨了其在功能主义、涌现论等哲学流派中的意义。同时，框架为定性系统的演化提供了理论约束。

Conclusion: 通过结构与质的关系研究，为意识难题提供了新的分析工具和分类体系，并拓展了其在演化理论中的应用潜力。

Abstract: This paper explores the hard problem of consciousness from a different
perspective. Instead of drawing distinctions between the physical and the
mental, an exploration of a more foundational relationship is examined: the
relationship between structure and quality.
  Information-theoretic measures are developed to quantify the mutual
determinability between structure and quality, including a novel Q-S space for
analyzing fidelity between the two domains. This novel space naturally points
toward a five-fold categorization of possible relationships between structural
and qualitative properties, illustrating each through conceptual and formal
models.
  The ontological implications of each category are examined, shedding light on
debates around functionalism, emergentism, idealism, panpsychism, and neutral
monism.
  This new line of inquiry has established a framework for deriving theoretical
constraints on qualitative systems undergoing evolution that is explored in my
companion paper, Qualia & Natural Selection.

</details>


### [136] [Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience](https://arxiv.org/abs/2505.05515)
*Zinan Liu,Haoran Li,Jingyi Lu,Gaoyuan Ma,Xu Hong,Giovanni Iacca,Arvind Kumar,Shaojun Tang,Lin Wang*

Main category: q-bio.NC

TL;DR: 本文提出了一种基于神经科学的新框架，用于解释和增强AI的自主推理能力，通过模拟人脑的感知、维度、逻辑和交互推理类型，为智能系统的开发提供了理论和实践指导。


<details>
  <summary>Details</summary>
Motivation: 现有自主AI的推理机制仍不明确，尤其是与生物神经机制的对比。本文旨在从神经科学角度出发，提出一种新的推理框架，以填补这一理论空白并推动更通用的智能体开发。

Method: 通过三个神经科学定义和数学、生物学基础，构建了一个统一的推理框架，涵盖感知、维度、逻辑和交互四种核心推理类型，并分析了现有AI方法的理论、计算设计和局限性。

Result: 提出的框架不仅分类和评估了现有AI推理方法，还为构建更具通用性和认知对齐的智能体提供了新方向，并提出了类似思维链提示的神经启发方法。

Conclusion: 该研究通过结合认知神经科学与AI，为智能系统的自主推理能力提供了理论基础和实践路径，并指明了未来研究方向。

Abstract: Autonomous AI is no longer a hard-to-reach concept, it enables the agents to
move beyond executing tasks to independently addressing complex problems,
adapting to change while handling the uncertainty of the environment. However,
what makes the agents truly autonomous? It is agentic reasoning, that is
crucial for foundation models to develop symbolic logic, statistical
correlations, or large-scale pattern recognition to process information, draw
inferences, and make decisions. However, it remains unclear why and how
existing agentic reasoning approaches work, in comparison to biological
reasoning, which instead is deeply rooted in neural mechanisms involving
hierarchical cognition, multimodal integration, and dynamic interactions. In
this work, we propose a novel neuroscience-inspired framework for agentic
reasoning. Grounded in three neuroscience-based definitions and supported by
mathematical and biological foundations, we propose a unified framework
modeling reasoning from perception to action, encompassing four core types,
perceptual, dimensional, logical, and interactive, inspired by distinct
functional roles observed in the human brain. We apply this framework to
systematically classify and analyze existing AI reasoning methods, evaluating
their theoretical foundations, computational designs, and practical
limitations. We also explore its implications for building more generalizable,
cognitively aligned agents in physical and virtual environments. Finally,
building on our framework, we outline future directions and propose new
neural-inspired reasoning methods, analogous to chain-of-thought prompting. By
bridging cognitive neuroscience and AI, this work offers a theoretical
foundation and practical roadmap for advancing agentic reasoning in intelligent
systems. The associated project can be found at:
https://github.com/BioRAILab/Awesome-Neuroscience-Agent-Reasoning .

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [137] [MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills](https://arxiv.org/abs/2505.06176)
*Niladri Shekhar Dutt,Duygu Ceylan,Niloy J. Mitra*

Main category: cs.GR

TL;DR: 该论文提出了一种利用多模态大语言模型（MLLM）进行照片修复的方法，通过训练模型理解图像处理操作，并基于专家编辑的照片生成推理数据集，最终实现可解释且能保留图像细节的修复建议。


<details>
  <summary>Details</summary>
Motivation: 传统生成式编辑（如基于文本或笔触的编辑）可能不可预测地改变原始对象身份，而专业摄影师更倾向于保守的传统程序化编辑。但由于程序化编辑操作复杂，对新手来说难以规划。因此，作者希望探索MLLM能否通过学习和规划这些操作，提供专业质量的修复建议。

Method: 首先训练MLLM通过解决视觉谜题来理解图像处理操作。然后，利用专家编辑的照片合成推理数据集，通过程序化操纵专家编辑并基于视觉调整预训练LLM，生成微调所需的推理数据。最终，模型可以规划并提出编辑序列。

Result: 实验表明，该方法在多种测试案例中表现优于现有的生成式和程序化替代方案，尤其在解释性和身份保留方面有显著优势。

Conclusion: 通过MLLM学习和规划程序化编辑操作，可以实现可解释、保留细节的高质量照片修复，为新手提供接近专业的编辑工具。

Abstract: Retouching is an essential task in post-manipulation of raw photographs.
Generative editing, guided by text or strokes, provides a new tool accessible
to users but can easily change the identity of the original objects in
unacceptable and unpredictable ways. In contrast, although traditional
procedural edits, as commonly supported by photoediting tools (e.g., Gimp,
Lightroom), are conservative, they are still preferred by professionals.
Unfortunately, professional quality retouching involves many individual
procedural editing operations that is challenging to plan for most novices. In
this paper, we ask if a multimodal large language model (MLLM) can be taught to
critique raw photographs, suggest suitable remedies, and finally realize them
with a given set of pre-authored procedural image operations. We demonstrate
that MLLMs can be first made aware of the underlying image processing
operations, by training them to solve specially designed visual puzzles.
Subsequently, such an operation-aware MLLM can both plan and propose edit
sequences. To facilitate training, given a set of expert-edited photos, we
synthesize a reasoning dataset by procedurally manipulating the expert edits
and then grounding a pretrained LLM on the visual adjustments, to synthesize
reasoning for finetuning. The proposed retouching operations are, by
construction, understandable by the users, preserve object details and
resolution, and can be optionally overridden. We evaluate our setup on a
variety of test examples and show advantages, in terms of explainability and
identity preservation, over existing generative and other procedural
alternatives. Code, data, models, and supplementary results can be found via
our project website at https://monetgpt.github.io.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [138] [A Machine-Learning Compositional Study of Exoplanetary Material Accreted Onto Five Helium-Atmosphere White Dwarfs with $\texttt{cecilia}$](https://arxiv.org/abs/2505.06228)
*Mariona Badenas-Agusti,Siyi Xu,Andrew Vanderburg,Kishalay De,Patrick Dufour,Laura K. Rogers,Susana Hoyos,Simon Blouin,Javier Viaña,Amy Bonsor,Ben Zuckerman*

Main category: astro-ph.EP

TL;DR: 利用机器学习工具cecilia分析了五个金属污染的白矮星大气参数和化学成分，与传统方法精度相当，发现其污染物成分与原始CI球粒陨石相似，并发现某些恒星可能存在富氧物质的吸积。


<details>
  <summary>Details</summary>
Motivation: 传统方法对白矮星大气成分分析耗时且依赖人工，机器学习工具cecilia有望高效处理大量光谱数据，推动统计研究。

Method: 结合SDSS和Keck/ESI光谱数据，通过贝叶斯联合拟合迭代分析波长范围3800-9000Å的光谱，测量多种元素的丰度。

Result: 测得2-6种元素丰度（精度约0.20 dex），其中SDSS J0859+5732和SDSS J2311-0041检测到O、Mg、Si、Ca和Fe，且污染物成分与CI球粒陨石一致（1-2σ）。部分白矮星存在显著富氧迹象（>2σ）。

Conclusion: cecilia可高效分析污染白矮星，未来大规模光谱数据将助力统计研究外太阳系物质组成。

Abstract: We present the first application of the Machine Learning (ML) pipeline
$\texttt{cecilia}$ to determine the physical parameters and photospheric
composition of five metal-polluted He-atmosphere white dwarfs without
well-characterised elemental abundances. To achieve this, we perform a joint
and iterative Bayesian fit to their $\textit{SDSS}$ (R=2,000) and
$\textit{Keck/ESI}$ (R=4,500) optical spectra, covering the wavelength range
from about 3,800\r{A} to 9,000\r{A}. Our analysis measures the abundances of at
least two $-$and up to six$-$ chemical elements in their atmospheres with a
predictive accuracy similar to that of conventional WD analysis techniques
($\approx$0.20 dex). The white dwarfs with the largest number of detected heavy
elements are SDSS J0859$+$5732 and SDSS J2311$-$0041, which simultaneously
exhibit O, Mg, Si, Ca, and Fe in their $\textit{Keck/ESI}$ spectra. For all
systems, we find that the bulk composition of their pollutants is largely
consistent with those of primitive CI chondrites to within 1-2$\sigma$. We also
find evidence of statistically significant ($>2\sigma$) oxygen excesses for
SDSS J0859$+$5732 and SDSS J2311$-$0041, which could point to the accretion of
oxygen-rich exoplanetary material. In the future, as wide-field astronomical
surveys deliver millions of public WD spectra to the scientific community,
$\texttt{cecilia}$ aspires to unlock population-wide studies of polluted WDs,
therefore helping to improve our statistical knowledge of extrasolar
compositions.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [139] [Modeling Multi-Hop Semantic Paths for Recommendation in Heterogeneous Information Networks](https://arxiv.org/abs/2505.05989)
*Hongye Zheng,Yue Xing,Lipeng Zhu,Xu Han,Junliang Du,Wanyu Cui*

Main category: cs.IR

TL;DR: 该论文提出了一种多跳路径感知推荐框架，通过路径选择、语义表示和注意力融合三阶段建模用户偏好，实验证明其在异构信息网络中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决异构信息网络中的路径建模问题，通过多跳路径捕捉高阶交互语义，提升推荐系统的效果。

Method: 方法包括路径选择（过滤冗余信息）、语义表示（序列建模编码实体和关系）和注意力融合（加权生成用户兴趣表示）。

Result: 在Amazon-Book等真实数据集上的实验显示，该方法在HR@10、Recall@10和Precision@10等指标上显著优于现有模型。

Conclusion: 结论表明该方法通过结合异构网络结构信息和推荐算法设计，为复杂数据环境中的用户偏好建模提供了更灵活有效的范式。

Abstract: This study focuses on the problem of path modeling in heterogeneous
information networks and proposes a multi-hop path-aware recommendation
framework. The method centers on multi-hop paths composed of various types of
entities and relations. It models user preferences through three stages: path
selection, semantic representation, and attention-based fusion. In the path
selection stage, a path filtering mechanism is introduced to remove redundant
and noisy information. In the representation learning stage, a sequential
modeling structure is used to jointly encode entities and relations, preserving
the semantic dependencies within paths. In the fusion stage, an attention
mechanism assigns different weights to each path to generate a global user
interest representation. Experiments conducted on real-world datasets such as
Amazon-Book show that the proposed method significantly outperforms existing
recommendation models across multiple evaluation metrics, including HR@10,
Recall@10, and Precision@10. The results confirm the effectiveness of multi-hop
paths in capturing high-order interaction semantics and demonstrate the
expressive modeling capabilities of the framework in heterogeneous
recommendation scenarios. This method provides both theoretical and practical
value by integrating structural information modeling in heterogeneous networks
with recommendation algorithm design. It offers a more expressive and flexible
paradigm for learning user preferences in complex data environments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [140] [An Automated LLM-based Pipeline for Asset-Level Database Creation to Assess Deforestation Impact](https://arxiv.org/abs/2505.05494)
*Avanija Menon,Ovidiu Serban*

Main category: cs.DB

TL;DR: 本研究提出了一种自动化端到端数据提取流程，利用LLMs技术创建、清洁和验证结构化数据库，针对高毁林风险行业，显著提升数据提取准确性。


<details>
  <summary>Details</summary>
Motivation: 因欧盟《反毁林条例》（EUDR）要求企业证明其产品未导致毁林，而现有数据库缺乏细节，依赖广泛财务指标和手动收集，限制了合规性和环境建模准确性。

Method: 采用IRZ-CoT提示提升提取精度，结合RAV流程实时检索网络数据以提高可靠性，应用于SEC EDGAR文件。

Result: 相比传统零样本提示方法，该流程在提取精度和验证覆盖上表现显著提升。

Conclusion: 该研究推动了NLP驱动的合规、CSR及ESG自动化，具有广泛行业适用性。

Abstract: The European Union Deforestation Regulation (EUDR) requires companies to
prove their products do not contribute to deforestation, creating a critical
demand for precise, asset-level environmental impact data. Current databases
lack the necessary detail, relying heavily on broad financial metrics and
manual data collection, which limits regulatory compliance and accurate
environmental modeling. This study presents an automated, end-to-end data
extraction pipeline that uses LLMs to create, clean, and validate structured
databases, specifically targeting sectors with a high risk of deforestation.
The pipeline introduces Instructional, Role-Based, Zero-Shot Chain-of-Thought
(IRZ-CoT) prompting to enhance data extraction accuracy and a
Retrieval-Augmented Validation (RAV) process that integrates real-time web
searches for improved data reliability. Applied to SEC EDGAR filings in the
Mining, Oil & Gas, and Utilities sectors, the pipeline demonstrates significant
improvements over traditional zero-shot prompting approaches, particularly in
extraction accuracy and validation coverage. This work advances NLP-driven
automation for regulatory compliance, CSR (Corporate Social Responsibility),
and ESG, with broad sectoral applicability.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [141] [A Common Interface for Automatic Differentiation](https://arxiv.org/abs/2505.05542)
*Guillaume Dalle,Adrian Hill*

Main category: cs.MS

TL;DR: DifferentiationInterface.jl是一个Julia包，提供统一的前端接口，简化多种自动微分后端的比较和模块化开发。


<details>
  <summary>Details</summary>
Motivation: 针对科学机器学习任务中自定义代码较多的情况，选择适合的自动微分系统至关重要。

Method: 通过内置准备机制，利用各后端的优势并分摊一次性计算开销，支持高级功能（如稀疏处理）而无需用户额外负担。

Result: 实现了多种自动微分后端的无缝集成与高效利用。

Conclusion: DifferentiationInterface.jl为科学机器学习提供了灵活的自动微分解决方案，简化了开发流程。

Abstract: For scientific machine learning tasks with a lot of custom code, picking the
right Automatic Differentiation (AD) system matters. Our Julia package
DifferentiationInterface.jl provides a common frontend to a dozen AD backends,
unlocking easy comparison and modular development. In particular, its built-in
preparation mechanism leverages the strengths of each backend by amortizing
one-time computations. This is key to enabling sophisticated features like
sparsity handling without putting additional burdens on the user.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [142] [What Is Next for LLMs? Next-Generation AI Computing Hardware Using Photonic Chips](https://arxiv.org/abs/2505.05794)
*Renjie Li,Wenjie Wei,Qi Xin,Xiaoli Liu,Sixuan Mao,Erik Ma,Zijian Chen,Malu Zhang,Haizhou Li,Zhaoyu Zhang*

Main category: cs.AR

TL;DR: 本文综述了面向下一代生成式AI计算的光子硬件，探讨了超快矩阵运算的光子神经网络架构和新型神经形态器件，分析了光子计算在大型语言模型（LLM）中的潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）的计算需求激增，传统冯·诺依曼架构面临能效瓶颈，亟需探索光子计算等新兴硬件以提升能效和吞吐量。

Method: 综述了集成光子神经网络架构（如马赫-曾德尔干涉仪网格、激光器、波长复用微环谐振器）和神经形态器件（如脉冲神经网络电路、混合自旋光子突触），结合二维材料（如石墨烯）以实现可调谐调制器。分析了Transformer架构在光子硬件上的映射策略。

Result: 光子计算系统在吞吐量和能效上可能远超电子处理器，但需解决长上下文窗口、长令牌序列的内存问题及超大数据集的存储技术瓶颈。

Conclusion: 光子硬件为LLM的高效能计算提供了新路径，但其实际应用依赖内存和存储技术的突破。

Abstract: Large language models (LLMs) are rapidly pushing the limits of contemporary
computing hardware. For example, training GPT-3 has been estimated to consume
around 1300 MWh of electricity, and projections suggest future models may
require city-scale (gigawatt) power budgets. These demands motivate exploration
of computing paradigms beyond conventional von Neumann architectures. This
review surveys emerging photonic hardware optimized for next-generation
generative AI computing. We discuss integrated photonic neural network
architectures (e.g., Mach-Zehnder interferometer meshes, lasers,
wavelength-multiplexed microring resonators) that perform ultrafast matrix
operations. We also examine promising alternative neuromorphic devices,
including spiking neural network circuits and hybrid spintronic-photonic
synapses, which combine memory and processing. The integration of
two-dimensional materials (graphene, TMDCs) into silicon photonic platforms is
reviewed for tunable modulators and on-chip synaptic elements.
Transformer-based LLM architectures (self-attention and feed-forward layers)
are analyzed in this context, identifying strategies and challenges for mapping
dynamic matrix multiplications onto these novel hardware substrates. We then
dissect the mechanisms of mainstream LLMs, such as ChatGPT, DeepSeek, and
LLaMA, highlighting their architectural similarities and differences. We
synthesize state-of-the-art components, algorithms, and integration methods,
highlighting key advances and open issues in scaling such systems to mega-sized
LLM models. We find that photonic computing systems could potentially surpass
electronic processors by orders of magnitude in throughput and energy
efficiency, but require breakthroughs in memory, especially for long-context
windows and long token sequences, and in storage of ultra-large datasets.

</details>


### [143] [LightNobel: Improving Sequence Length Limitation in Protein Structure Prediction Model via Adaptive Activation Quantization](https://arxiv.org/abs/2505.05893)
*Seunghee Han,Soongyu Choi,Joo-Young Kim*

Main category: cs.AR

TL;DR: 提出了LightNobel，一种软硬件协同设计的加速器，用于解决蛋白质结构预测模型（PPM）在处理长序列时的可扩展性问题，通过量化技术和硬件优化显著提高了速度和能效。


<details>
  <summary>Details</summary>
Motivation: 目前蛋白质结构预测模型（如AlphaFold2和ESMFold）在处理长氨基酸序列时面临内存和计算资源的巨大需求，限制了其实际应用。

Method: 提出了Token-wise Adaptive Activation Quantization (AAQ)软件方法，并结合了多精度可重构矩阵处理单元（RMPU）和多功能向量处理单元（VVPU）的硬件设计。

Result: LightNobel相比NVIDIA A100和H100 GPU，实现了最高8.44倍和8.41倍的速度提升，以及37.29倍和43.35倍的能效提升，同时将峰值内存需求降低了120.05倍。

Conclusion: LightNobel通过软硬件协同设计有效解决了PPM的可扩展性问题，为长序列蛋白质的分析提供了高效解决方案。

Abstract: Recent advances in Protein Structure Prediction Models (PPMs), such as
AlphaFold2 and ESMFold, have revolutionized computational biology by achieving
unprecedented accuracy in predicting three-dimensional protein folding
structures. However, these models face significant scalability challenges,
particularly when processing proteins with long amino acid sequences (e.g.,
sequence length > 1,000). The primary bottleneck that arises from the
exponential growth in activation sizes is driven by the unique data structure
in PPM, which introduces an additional dimension that leads to substantial
memory and computational demands. These limitations have hindered the effective
scaling of PPM for real-world applications, such as analyzing large proteins or
complex multimers with critical biological and pharmaceutical relevance.
  In this paper, we present LightNobel, the first hardware-software co-designed
accelerator developed to overcome scalability limitations on the sequence
length in PPM. At the software level, we propose Token-wise Adaptive Activation
Quantization (AAQ), which leverages unique token-wise characteristics, such as
distogram patterns in PPM activations, to enable fine-grained quantization
techniques without compromising accuracy. At the hardware level, LightNobel
integrates the multi-precision reconfigurable matrix processing unit (RMPU) and
versatile vector processing unit (VVPU) to enable the efficient execution of
AAQ. Through these innovations, LightNobel achieves up to 8.44x, 8.41x speedup
and 37.29x, 43.35x higher power efficiency over the latest NVIDIA A100 and H100
GPUs, respectively, while maintaining negligible accuracy loss. It also reduces
the peak memory requirement up to 120.05x in PPM, enabling scalable processing
for proteins with long sequences.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [144] [Evolutionary ecology of words](https://arxiv.org/abs/2505.05863)
*Reiji Suzuki,Takaya Arita*

Main category: q-bio.PE

TL;DR: 该论文提出了一种基于大型语言模型（LLM）的进化生态学模型，通过模拟代理之间的交互和词语突变为词汇进化提供了新视角。初步实验显示种群多样性的独特演化和适应极端环境的物种优势。


<details>
  <summary>Details</summary>
Motivation: 研究旨在扩展进化博弈理论和基于代理的模型，利用大型语言模型（LLM）的丰富表达能力，探索词汇的进化生态学。

Method: 构建了一个空间环境模型，代理通过LLM生成的短词或短语交互，胜者替代败者词汇，并可能发生基于LLM输出的词汇突变。

Result: 初步实验表明种群从已知物种出发，以渐进和间断平衡方式演化出多样物种，最终特定类型（如陆地动物、海洋生物）占据主导。长期实验中观察到多样物种的共存。

Conclusion: 模型展示了词汇进化的多样性和适应性，为进化生态学和语言模型结合提供了新思路。

Abstract: We propose a model for the evolutionary ecology of words as one attempt to
extend evolutionary game theory and agent-based models by utilizing the rich
linguistic expressions of Large Language Models (LLMs). Our model enables the
emergence and evolution of diverse and infinite options for interactions among
agents. Within the population, each agent possesses a short word (or phrase)
generated by an LLM and moves within a spatial environment. When agents become
adjacent, the outcome of their interaction is determined by the LLM based on
the relationship between their words, with the loser's word being replaced by
the winner's. Word mutations, also based on LLM outputs, may occur. We
conducted preliminary experiments assuming that ``strong animal species" would
survive. The results showed that from an initial population consisting of
well-known species, many species emerged both gradually and in a punctuated
equilibrium manner. Each trial demonstrated the unique evolution of diverse
populations, with one type of large species becoming dominant, such as
terrestrial animals, marine life, or extinct species, which were ecologically
specialized and adapted ones across diverse extreme habitats. We also conducted
a long-term experiment with a large population, demonstrating the emergence and
coexistence of diverse species.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [145] [AI-powered virtual eye: perspective, challenges and opportunities](https://arxiv.org/abs/2505.05516)
*Yue Wu,Yibo Guo,Yulong Yan,Jiancheng Yang,Xin Zhou,Ching-Yu Cheng,Danli Shi,Mingguang He*

Main category: q-bio.TO

TL;DR: 该论文提出了一种名为‘虚拟眼’的下一代AI驱动平台，旨在通过互联的基础模型模拟眼睛的复杂结构和生物功能，利用多组学和大规模数据集推动个性化眼科护理和研究。


<details>
  <summary>Details</summary>
Motivation: 当前的AI、成像和多组学技术进步为构建高保真的人类眼睛数字复制品提供了可能，目标是革命化个性化眼科护理并加速眼部健康与疾病研究。

Method: 提出了一种整合多模态、多尺度、动态预测能力和反馈机制的AI驱动方法，依赖大规模多模态数据集、生成式AI、基础模型和基于代理的架构。

Result: 虚拟眼平台虽面临解释性、伦理、数据处理和评估等挑战，但有望成为眼科研究和个性化护理的革命性工具。

Conclusion: 虚拟眼展现了通过AI技术模拟和优化眼科护理的巨大潜力和前景，尽管仍有一些技术和伦理挑战需克服。

Abstract: We envision the "virtual eye" as a next-generation, AI-powered platform that
uses interconnected foundation models to simulate the eye's intricate structure
and biological function across all scales. Advances in AI, imaging, and
multiomics provide a fertile ground for constructing a universal, high-fidelity
digital replica of the human eye. This perspective traces the evolution from
early mechanistic and rule-based models to contemporary AI-driven approaches,
integrating in a unified model with multimodal, multiscale, dynamic predictive
capabilities and embedded feedback mechanisms. We propose a development roadmap
emphasizing the roles of large-scale multimodal datasets, generative AI,
foundation models, agent-based architectures, and interactive interfaces.
Despite challenges in interpretability, ethics, data processing and evaluation,
the virtual eye holds the potential to revolutionize personalized ophthalmic
care and accelerate research into ocular health and disease.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [146] [Machine learning automorphic forms for black holes](https://arxiv.org/abs/2505.05549)
*Vishnu Jejjala,Suresh Nampuri,Dumisani Nxumalo,Pratik Roy,Abinash Swain*

Main category: hep-th

TL;DR: 训练神经网络预测模形式和雅可比形式的模重量，结果表明在负权形式中表现良好，验证了机器学���在识别引力系统中模对称性的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用机器学习技术识别和预测模形式、雅可比形式及模拟模形式在BPS黑洞简并中的对称性组织方式，为量子引力中的对称性自动检测提供新途径。

Method: 使用前馈神经网络训练自Dedekind eta函数、Eisenstein级数和雅可比theta函数导出的自守形式的傅里叶系数，预测模重量。

Result: 在负权模形式和拟模形式中表现优异，尤其在精确黑洞计数公式中；但在正权形式和复杂雅可比theta函数组合中准确率较低。

Conclusion: 研究验证了机器学习在识别引力系统模对称性中的概念可行性，为量子引力中的对称性自动检测与验证提供了潜在路径。

Abstract: Modular, Jacobi, and mock-modular forms serve as generating functions for BPS
black hole degeneracies. By training feed-forward neural networks on Fourier
coefficients of automorphic forms derived from the Dedekind eta function,
Eisenstein series, and Jacobi theta functions, we demonstrate that machine
learning techniques can accurately predict modular weights from truncated
expansions. Our results reveal strong performance for negative weight modular
and quasi-modular forms, particularly those arising in exact black hole
counting formulae, with lower accuracy for positive weights and more
complicated combinations of Jacobi theta functions. This study establishes a
proof of concept for using machine learning to identify how data is organized
in terms of modular symmetries in gravitational systems and suggests a pathway
toward automated detection and verification of symmetries in quantum gravity.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [147] [Learning-Augmented Algorithms for Boolean Satisfiability](https://arxiv.org/abs/2505.06146)
*Idan Attias,Xing Gao,Lev Reyzin*

Main category: cs.DS

TL;DR: 该论文研究了在机器学习的预测信息辅助下，如何改进布尔可满足性问题（SAT）的决策和优化算法，通过两种类型的“建议”（子集建议和标签建议）显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 利用机器学习提供的信息（“建议”）来改进经典问题的算法表现，特别是在布尔可满足性问题中，通过加速决策过程或提高近似比来实现性能提升。

Method: 采用了两种形式的建议：子集建议（随机提供最优解的部分变量）和标签建议（提供所有变量的噪声预测），分别用于加速PPSZ算法的决策过程和改进优化问题的近似比。

Result: 在决策问题中，子集建议将PPSZ算法的运行时间指数级加速；在优化问题中，子集建议将近似比改进为$\alpha + (1 - \alpha)\epsilon$，标签建议则在特定条件下实现了接近最优的近似。

Conclusion: 机器学习建议可以有效提升SAT问题的算法性能，尤其是在决策速度和优化质量方面展现了显著改进。

Abstract: Learning-augmented algorithms are a prominent recent development in beyond
worst-case analysis. In this framework, a problem instance is provided with a
prediction (``advice'') from a machine-learning oracle, which provides partial
information about an optimal solution, and the goal is to design algorithms
that leverage this advice to improve worst-case performance. We study the
classic Boolean satisfiability (SAT) decision and optimization problems within
this framework using two forms of advice. ``Subset advice" provides a random
$\epsilon$ fraction of the variables from an optimal assignment, whereas
``label advice" provides noisy predictions for all variables in an optimal
assignment.
  For the decision problem $k$-SAT, by using the subset advice we accelerate
the exponential running time of the PPSZ family of algorithms due to Paturi,
Pudlak, Saks and Zane, which currently represent the state of the art in the
worst case. We accelerate the running time by a multiplicative factor of
$2^{-c}$ in the base of the exponent, where $c$ is a function of $\epsilon$ and
$k$. For the optimization problem, we show how to incorporate subset advice in
a black-box fashion with any $\alpha$-approximation algorithm, improving the
approximation ratio to $\alpha + (1 - \alpha)\epsilon$. Specifically, we
achieve approximations of $0.94 + \Omega(\epsilon)$ for MAX-$2$-SAT, $7/8 +
\Omega(\epsilon)$ for MAX-$3$-SAT, and $0.79 + \Omega(\epsilon)$ for MAX-SAT.
Moreover, for label advice, we obtain near-optimal approximation for instances
with large average degree, thereby generalizing recent results on MAX-CUT and
MAX-$2$-LIN.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [148] [GenAI in Entrepreneurship: a systematic review of generative artificial intelligence in entrepreneurship research: current issues and future directions](https://arxiv.org/abs/2505.05523)
*Anna Kusetogullari,Huseyin Kusetogullari,Martin Andersson,Tony Gorschek*

Main category: econ.GN

TL;DR: 该论文通过系统性文献综述，分析了生成式人工智能（GenAI）对创业研究的影响，识别了五个主要主题集群，并讨论了未来研究方向、文献缺口及伦理问题。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏关于GenAI在创业研究中作用的知识，论文旨在填补这一空白。

Method: 采用自然语言处理和无监督机器学习技术（如TF-IDF向量化、PCA和层次聚类），分析了83篇来自Web of Science和Scopus的同行评审文章。

Result: 识别出五个主要主题集群：数字转型与行为模型、GenAI增强的教育与学习系统、可持续创新与战略AI影响、商业模式与市场趋势、以及数据驱动的创业技术趋势。

Conclusion: 强调了未来需要对GenAI作为创业外部推动者进行宏观研究，并探讨有效的监管框架以促进商业实验和创新。

Abstract: Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)
are recognized to have significant effects on industry and business dynamics,
not least because of their impact on the preconditions for entrepreneurship.
There is still a lack of knowledge of GenAI as a theme in entrepreneurship
research. This paper presents a systematic literature review aimed at
identifying and analyzing the evolving landscape of research on the effects of
GenAI on entrepreneurship. We analyze 83 peer-reviewed articles obtained from
leading academic databases: Web of Science and Scopus. Using natural language
processing and unsupervised machine learning techniques with TF-IDF
vectorization, Principal Component Analysis (PCA), and hierarchical clustering,
five major thematic clusters are identified: (1) Digital Transformation and
Behavioral Models, (2) GenAI-Enhanced Education and Learning Systems, (3)
Sustainable Innovation and Strategic AI Impact, (4) Business Models and Market
Trends, and (5) Data-Driven Technological Trends in Entrepreneurship. Based on
the review, we discuss future research directions, gaps in the current
literature, as well as ethical concerns raised in the literature. We highlight
the need for more macro-level research on GenAI and LLMs as external enablers
for entrepreneurship and for research on effective regulatory frameworks that
facilitate business experimentation, innovation, and further technology
development.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [149] [Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications](https://arxiv.org/abs/2505.05736)
*Da Wu,Zhanliang Wang,Quan Nguyen,Zhuoran Xu,Kai Wang*

Main category: q-bio.QM

TL;DR: MINT框架通过偏好优化将单模态大模型与多模态生物医学数据的领域特定决策模式对齐，显著提升了文本或图像单独输入下的预测性能。


<details>
  <summary>Details</summary>
Motivation: 由于高质量多模态生物医学数据稀缺，限制了大型语言模型（LLMs）在生物医学任务中的微调效果，MINT旨在解决这一挑战。

Method: MINT采用Odds Ratio Preference Optimization (ORPO)框架，利用上游多模态机器学习模型生成偏好数据集，将领域知识传递至下游单模态模型。

Result: 在罕见遗传疾病预测和组织类型分类中，MINT对齐的模型性能超越SFT、RAG、DPO等方法，甚至优于更大规模的模型。

Conclusion: MINT通过偏好优化实现了单模态LLMs与高质量多模态专业知识的对齐，为生物医学任务提供了高效解决方案。

Abstract: The scarcity of high-quality multimodal biomedical data limits the ability to
effectively fine-tune pretrained Large Language Models (LLMs) for specialized
biomedical tasks. To address this challenge, we introduce MINT (Multimodal
Integrated kNowledge Transfer), a framework that aligns unimodal large decoder
models with domain-specific decision patterns from multimodal biomedical data
through preference optimization. While MINT supports different optimization
techniques, we primarily implement it with the Odds Ratio Preference
Optimization (ORPO) framework as its backbone. This strategy enables the
aligned LLMs to perform predictive tasks using text-only or image-only inputs
while retaining knowledge learnt from multimodal data. MINT leverages an
upstream multimodal machine learning (MML) model trained on high-quality
multimodal data to transfer domain-specific insights to downstream text-only or
image-only LLMs. We demonstrate its effectiveness through two key applications:
(1) Rare genetic disease prediction from texts, where MINT uses a multimodal
encoder model, trained on facial photos and clinical notes, to generate a
preference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite
relying on text input only, the MINT-derived model outperforms models trained
with SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue
type classification using cell nucleus images, where MINT uses a
vision-language foundation model as the preference generator, containing
knowledge learnt from both text and histopathological images to align
downstream image-only models. The resulting MINT-derived model significantly
improves the performance of Llama 3.2-Vision-11B-Instruct on tissue type
classification. In summary, MINT provides an effective strategy to align
unimodal LLMs with high-quality multimodal expertise through preference
optimization.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [150] [Predicting Diabetic Macular Edema Treatment Responses Using OCT: Dataset and Methods of APTOS Competition](https://arxiv.org/abs/2505.05768)
*Weiyi Zhang,Peranut Chotcomwongse,Yinwen Li,Pusheng Xu,Ruijie Yao,Lianhao Zhou,Yuxuan Zhou,Hui Feng,Qiping Zhou,Xinyue Wang,Shoujin Huang,Zihao Jin,Florence H. T. Chung,Shujun Wang,Yalin Zheng,Mingguang He,Danli Shi,Paisan Ruamviboonsuk*

Main category: eess.IV

TL;DR: 该研究通过亚太远程眼科学会大数据竞赛，利用OCT图像预测糖尿病黄斑水肿患者的抗VEGF治疗反应，展示了AI在个性化治疗中的潜力。


<details>
  <summary>Details</summary>
Motivation: 糖尿病黄斑水肿（DME）患者对抗VEGF治疗的反应差异显著，需要分层预测以优化个性化治疗策略。目前缺乏相关研究，因此通过竞赛探索预治疗分层方法。

Method: 组织亚太远程眼科学会大数据竞赛，提供2,000名患者的OCT图像数据集，分为四项子任务，吸引170支团队参与并提交预测模型。

Result: 最佳团队的AUC达到80.06%，表明AI可为临床决策提供有效支持。

Conclusion: 竞赛证明了AI在DME个性化治疗预测中的价值，为未来临床应用奠定了基础。

Abstract: Diabetic macular edema (DME) significantly contributes to visual impairment
in diabetic patients. Treatment responses to intravitreal therapies vary,
highlighting the need for patient stratification to predict therapeutic
benefits and enable personalized strategies. To our knowledge, this study is
the first to explore pre-treatment stratification for predicting DME treatment
responses. To advance this research, we organized the 2nd Asia-Pacific
Tele-Ophthalmology Society (APTOS) Big Data Competition in 2021. The
competition focused on improving predictive accuracy for anti-VEGF therapy
responses using ophthalmic OCT images. We provided a dataset containing tens of
thousands of OCT images from 2,000 patients with labels across four sub-tasks.
This paper details the competition's structure, dataset, leading methods, and
evaluation metrics. The competition attracted strong scientific community
participation, with 170 teams initially registering and 41 reaching the final
round. The top-performing team achieved an AUC of 80.06%, highlighting the
potential of AI in personalized DME treatment and clinical decision-making.

</details>


### [151] [The Application of Deep Learning for Lymph Node Segmentation: A Systematic Review](https://arxiv.org/abs/2505.06118)
*Jingguo Qu,Xinyang Han,Man-Lik Chui,Yao Pu,Simon Takadiyi Gunda,Ziman Chen,Jing Qin,Ann Dorothy King,Winnie Chiu-Wing Chu,Jing Cai,Michael Tin-Cheung Ying*

Main category: eess.IV

TL;DR: 该论文探讨了深度学习技术在淋巴结自动分割中的应用，分析了不同架构的优缺点，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统淋巴结分割方法受限于手动绘制和操作者水平的差异，深度学习技术为提高准确性提供了新可能。

Method: 研究评估了卷积神经网络、编码器-解码器网络和Transformer等深度学习架构在多模态医学影像分析中的应用。

Result: 尽管取得进展，但仍面临淋巴结形状多样、标记数据稀缺及跨模态方法不足等挑战。

Conclusion: 首次全面综述深度学习在淋巴结分割中的应用，并建议未来研究关注多模态融合、迁移学习和大规模预训练模型。

Abstract: Automatic lymph node segmentation is the cornerstone for advances in computer
vision tasks for early detection and staging of cancer. Traditional
segmentation methods are constrained by manual delineation and variability in
operator proficiency, limiting their ability to achieve high accuracy. The
introduction of deep learning technologies offers new possibilities for
improving the accuracy of lymph node image analysis. This study evaluates the
application of deep learning in lymph node segmentation and discusses the
methodologies of various deep learning architectures such as convolutional
neural networks, encoder-decoder networks, and transformers in analyzing
medical imaging data across different modalities. Despite the advancements, it
still confronts challenges like the shape diversity of lymph nodes, the
scarcity of accurately labeled datasets, and the inadequate development of
methods that are robust and generalizable across different imaging modalities.
To the best of our knowledge, this is the first study that provides a
comprehensive overview of the application of deep learning techniques in lymph
node segmentation task. Furthermore, this study also explores potential future
research directions, including multimodal fusion techniques, transfer learning,
and the use of large-scale pre-trained models to overcome current limitations
while enhancing cancer diagnosis and treatment planning strategies.

</details>


### [152] [V-EfficientNets: Vector-Valued Efficiently Scaled Convolutional Neural Network Models](https://arxiv.org/abs/2505.05659)
*Guilherme Vieira Neto,Marcos Eduardo Valle*

Main category: eess.IV

TL;DR: 论文提出了V-EfficientNets，它是一种针对向量值数据优化的EfficientNet扩展模型，在医学图像分类任务中表现出色，取得了99.46%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在训练时学习特征通道之间的相关性，而向量值神经网络直接将多维数据视为连贯实体。为了更高效地处理向量值数据，作者提出了V-EfficientNets。

Method: 通过扩展EfficientNet，设计了V-EfficientNets，专门用于处理任意向量值数据。模型优化了参数分配，平衡了网络的宽度、深度和分辨率。

Result: 在ALL-IDB2数据集上的急性淋巴细胞白血病检测任务中，V-EfficientNets达到了99.46%的平均准确率，显著减少了参数数量且优于现有最先进模型。

Conclusion: V-EfficientNets在处理向量值数据时表现出高效性和卓越性能，为医学图像分类等任务提供了新的解决方案。

Abstract: EfficientNet models are convolutional neural networks optimized for parameter
allocation by jointly balancing network width, depth, and resolution. Renowned
for their exceptional accuracy, these models have become a standard for image
classification tasks across diverse computer vision benchmarks. While
traditional neural networks learn correlations between feature channels during
training, vector-valued neural networks inherently treat multidimensional data
as coherent entities, taking for granted the inter-channel relationships. This
paper introduces vector-valued EfficientNets (V-EfficientNets), a novel
extension of EfficientNet designed to process arbitrary vector-valued data. The
proposed models are evaluated on a medical image classification task, achieving
an average accuracy of 99.46% on the ALL-IDB2 dataset for detecting acute
lymphoblastic leukemia. V-EfficientNets demonstrate remarkable efficiency,
significantly reducing parameters while outperforming state-of-the-art models,
including the original EfficientNet. The source code is available at
https://github.com/mevalle/v-nets.

</details>


### [153] [Equivariant Imaging Biomarkers for Robust Unsupervised Segmentation of Histopathology](https://arxiv.org/abs/2505.05689)
*Fuyao Chen,Yuexi Du,Tal Zeevi,Nicha C. Dvornek,John A. Onofrey*

Main category: eess.IV

TL;DR: 研究提出一种基于对称卷积核的等变病理生物标记方法，以提高病理图像分析的鲁棒性和泛化能力，尤其针对旋转不变性问题，在50例前列腺癌数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统病理图像分析依赖人工，效率低且一致性差；现有机器学习模型缺乏旋转不变性，无法适应病理图像的无取向特性，限制了泛化能力。研究者希望通过改进模型增强病理分析的鲁棒性和准确性。

Method: 提出一种基于对称卷积核的无监督分割方法，生成等变的病理生物标记，确保模型对旋转和反射具有不变性。方法在前列腺组织微阵列图像（Gleason 2019数据集）上验证。

Result: 相比标准卷积核模型，新方法提取的生物标记在旋转鲁棒性上表现更优，提升了病理分析的准确性和泛化能力。

Conclusion: 通过等变成像技术，该方法有望超越前列腺癌领域，提升病理诊断和预后的整体能力。

Abstract: Histopathology evaluation of tissue specimens through microscopic examination
is essential for accurate disease diagnosis and prognosis. However, traditional
manual analysis by specially trained pathologists is time-consuming,
labor-intensive, cost-inefficient, and prone to inter-rater variability,
potentially affecting diagnostic consistency and accuracy. As digital pathology
images continue to proliferate, there is a pressing need for automated analysis
to address these challenges. Recent advancements in artificial
intelligence-based tools such as machine learning (ML) models, have
significantly enhanced the precision and efficiency of analyzing
histopathological slides. However, despite their impressive performance, ML
models are invariant only to translation, lacking invariance to rotation and
reflection. This limitation restricts their ability to generalize effectively,
particularly in histopathology, where images intrinsically lack meaningful
orientation. In this study, we develop robust, equivariant histopathological
biomarkers through a novel symmetric convolutional kernel via unsupervised
segmentation. The approach is validated using prostate tissue micro-array (TMA)
images from 50 patients in the Gleason 2019 Challenge public dataset. The
biomarkers extracted through this approach demonstrate enhanced robustness and
generalizability against rotation compared to models using standard convolution
kernels, holding promise for enhancing the accuracy, consistency, and
robustness of ML models in digital pathology. Ultimately, this work aims to
improve diagnostic and prognostic capabilities of histopathology beyond
prostate cancer through equivariant imaging.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [154] [OccuEMBED: Occupancy Extraction Merged with Building Energy Disaggregation for Occupant-Responsive Operation at Scale](https://arxiv.org/abs/2505.05478)
*Yufei Zhang,Andrew Sonta*

Main category: eess.SP

TL;DR: 论文提出了OccuEMBED框架，通过智能电表数据推断建筑占用情况和系统操作，以提升能效和居住者舒适度。结合概率占用生成器和KAN支持的负载分解器，在合成和真实数据集中表现优异。


<details>
  <summary>Details</summary>
Motivation: 建筑占全球能耗和排放的很大比例，需优化运营以支持电网稳定。现有自动化系统缺乏对居住者信息的整合，难以平衡能效与舒适度。

Method: 提出OccuEMBED框架，包含概率占用生成器和基于KAN的负载分解器，通过深度学习嵌入占用模式和负载-占用-天气关系。

Result: 在离散占用推断中平均F1分数>0.8，连续占用比RMSE为0.1-0.2。可集成至负载监控平台，支持居住者响应策略。

Conclusion: OccuEMBED为规模化居住者-centric建筑管理系统奠定了基础，适应能源系统变革挑战。

Abstract: Buildings account for a significant share of global energy consumption and
emissions, making it critical to operate them efficiently. As electricity grids
become more volatile with renewable penetration, buildings must provide
flexibility to support grid stability. Building automation plays a key role in
enhancing efficiency and flexibility via centralized operations, but it must
prioritize occupant-centric strategies to balance energy and comfort targets.
However, incorporating occupant information into large-scale, centralized
building operations remains challenging due to data limitations. We investigate
the potential of using whole-building smart meter data to infer both occupancy
and system operations. Integrating these insights into data-driven building
energy analysis allows more occupant-centric energy-saving and flexibility at
scale. Specifically, we propose OccuEMBED, a unified framework for occupancy
inference and system-level load analysis. It combines two key components: a
probabilistic occupancy profile generator, and a controllable and interpretable
load disaggregator supported by Kolmogorov-Arnold Networks (KAN). This design
embeds knowledge of occupancy patterns and load-occupancy-weather relationships
into deep learning models. We conducted comprehensive evaluations to
demonstrate its effectiveness across synthetic and real-world datasets compared
to various occupancy inference baselines. OccuEMBED always achieved average F1
scores above 0.8 in discrete occupancy inference and RMSE within 0.1-0.2 for
continuous occupancy ratios. We further demonstrate how OccuEMBED integrates
with building load monitoring platforms to display occupancy profiles, analyze
system-level operations, and inform occupant-responsive strategies. Our model
lays a robust foundation in scaling occupant-centric building management
systems to meet the challenges of an evolving energy system.

</details>


### [155] [Improving Local Air Quality Predictions Using Transfer Learning on Satellite Data and Graph Neural Networks](https://arxiv.org/abs/2505.05479)
*Finn Gueterbock,Raul Santos-Rodriguez,Jeffrey N. Clark*

Main category: eess.SP

TL;DR: 摘要提出了一种利用卫星和气象数据，结合迁移学习和GraphSAGE框架的新方法，用于在监测稀疏的城区预测NO2浓度。模型在伦敦数据预训练后，在布里斯托尔实现了比基线模型更低的误差。


<details>
  <summary>Details</summary>
Motivation: 空气污染是全球健康的重要威胁，尤其NO2在监测稀疏的城区影响显著。现有监测网络难以覆盖所有区域，亟需低成本高精度的预测方法。

Method: 结合迁移学习和GraphSAGE框架，利用卫星和气象数据，通过自回归和迁移学习提升预测精度，尤其在数据稀缺地区如布里斯托尔。

Result: 模型在伦敦数据预训练后，相比基线模型，NRMSE降低8.6%，梯度RMSE降低32.6%。

Conclusion: 该方法展示了虚拟传感器在低成本空气质量监测中的潜力，为气候和健康干预提供了可行方案。

Abstract: Air pollution is a significant global health risk, contributing to millions
of premature deaths annually. Nitrogen dioxide (NO2), a harmful pollutant,
disproportionately affects urban areas where monitoring networks are often
sparse. We propose a novel method for predicting NO2 concentrations at
unmonitored locations using transfer learning with satellite and meteorological
data. Leveraging the GraphSAGE framework, our approach integrates
autoregression and transfer learning to enhance predictive accuracy in
data-scarce regions like Bristol. Pre-trained on data from London, UK, our
model achieves a 8.6% reduction in Normalised Root Mean Squared Error (NRMSE)
and a 32.6% reduction in Gradient RMSE compared to a baseline model. This work
demonstrates the potential of virtual sensors for cost-effective air quality
monitoring, contributing to actionable insights for climate and health
interventions.

</details>


### [156] [Turbo-ICL: In-Context Learning-Based Turbo Equalization](https://arxiv.org/abs/2505.06175)
*Zihang Song,Matteo Zecchin,Bipin Rajendran,Osvaldo Simeone*

Main category: eess.SP

TL;DR: 论文提出了一种新颖的上下文学习（ICL）框架，用于编码多输入多输出（MIMO）系统中的软输入软输出信道均衡。通过结合解码器反馈和提示增强技术，ICL模型能够迭代优化符号估计，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决传统线性假设在低分辨率量化等复杂场景下失效的问题，通过利用大语言模型（LLMs）的灵感，开发一种能够直接学习符号后验分布的ICL框架。

Method: 提出了一种基于Transformer和状态空间架构的ICL模型，利用提示增强技术将解码器的外部信息作为额外上下文，实现迭代优化。

Result: 仿真结果表明，ICL均衡器在传统方法失效的场景下表现优异，尤其是在训练数据有限或资源受限时，Transformer和状态空间模型分别显示出不同的优势。

Conclusion: ICL框架为解决复杂信道条件下的均衡问题提供了有效方案，展示了其在非线性和资源受限场景下的潜力。

Abstract: This paper introduces a novel in-context learning (ICL) framework, inspired
by large language models (LLMs), for soft-input soft-output channel
equalization in coded multiple-input multiple-output (MIMO) systems. The
proposed approach learns to infer posterior symbol distributions directly from
a prompt of pilot signals and decoder feedback. A key innovation is the use of
prompt augmentation to incorporate extrinsic information from the decoder
output as additional context, enabling the ICL model to refine its symbol
estimates iteratively across turbo decoding iterations. Two model variants,
based on Transformer and state-space architectures, are developed and
evaluated. Extensive simulations demonstrate that, when traditional linear
assumptions break down, e.g., in the presence of low-resolution quantization,
ICL equalizers consistently outperform conventional model-based baselines, even
when the latter are provided with perfect channel state information. Results
also highlight the advantage of Transformer-based models under limited training
diversity, as well as the efficiency of state-space models in
resource-constrained scenarios.

</details>


### [157] [Multi-User Beamforming with Deep Reinforcement Learning in Sensing-Aided Communication](https://arxiv.org/abs/2505.05956)
*Xiyu Wang,Gilberto Berardinelli,Hei Victor Cheng,Petar Popovski,Ramoni Adeogun*

Main category: eess.SP

TL;DR: 这篇论文研究了利用传感辅助通信优化毫米波通信中的波束管理问题，提出了多波束方案和深度强化学习方法，显著提高了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 毫米波通信中，移动用户容易因波束漂移而失败。传感技术可以无需用户反馈且低开销地及时调整波束，从而解决这一问题。

Method: 提出了多波束方案，根据用户需求动态分配波束，并开发了基于深度强化学习的波束分配优化方法，仅依赖传感回波。同时，还提出了一种基于AoD和CRLB的启发式方法作为对比。

Result: 深度强化学习方法在吞吐量上显著优于传统波束扫描和AoD方法，且对不同用户速度具有鲁棒性。

Conclusion: 研究表明，深度强化学习方法能有效优化波束管理，提升毫米波通信性能，且无需用户反馈或先验状态信息。

Abstract: Mobile users are prone to experience beam failure due to beam drifting in
millimeter wave (mmWave) communications. Sensing can help alleviate beam
drifting with timely beam changes and low overhead since it does not need user
feedback. This work studies the problem of optimizing sensing-aided
communication by dynamically managing beams allocated to mobile users. A
multi-beam scheme is introduced, which allocates multiple beams to the users
that need an update on the angle of departure (AoD) estimates and a single beam
to the users that have satisfied AoD estimation precision. A deep reinforcement
learning (DRL) assisted method is developed to optimize the beam allocation
policy, relying only upon the sensing echoes. For comparison, a heuristic
AoD-based method using approximated Cram\'er-Rao lower bound (CRLB) for
allocation is also presented. Both methods require neither user feedback nor
prior state evolution information. Results show that the DRL-assisted method
achieves a considerable gain in throughput than the conventional beam sweeping
method and the AoD-based method, and it is robust to different user speeds.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [158] [Optimal Regret of Bernoulli Bandits under Global Differential Privacy](https://arxiv.org/abs/2505.05613)
*Achraf Azize,Yulian Wu,Junya Honda,Francesco Orabona,Shinji Ito,Debabrota Basu*

Main category: stat.ML

TL;DR: 论文研究了在ϵ-全局差分隐私（DP）约束下的随机赌博机问题中的遗憾最小化问题，提出了更紧的遗憾下界和上界，并设计了两类渐近最优的DP算法（DP-KLUCB和DP-IMED），其遗憾匹配新的下界。


<details>
  <summary>Details</summary>
Motivation: 在序列学习算法广泛应用于实际时，如何在保证数据隐私的同时维持其效用成为关键问题。现有研究在ϵ-全局DP下的遗憾界限存在较大差距，因此需要重新审视并改进。

Method: 1. 提出一种新的信息论量刻画ϵ-全局DP在随机赌博机中的难度，并证明更紧的遗憾下界；2. 设计两类DP算法（DP-KLUCB和DP-IMED），通过臂依赖阶段和拉普拉斯噪声实现隐私保护。

Result: 新算法（DP-KLUCB和DP-IMED）的遗憾渐近匹配新下界，且无需遗忘历史奖励即可实现最优，同时提出了一种新的拉普拉斯机制下伯努利变量和的集中不等式。

Conclusion: 论文推翻了‘遗忘历史奖励是设计全局DP下最优赌博机算法的必要条件’的猜想，为DP下的序列学习提供了更高效的理论和算法支持。

Abstract: As sequential learning algorithms are increasingly applied to real life,
ensuring data privacy while maintaining their utilities emerges as a timely
question. In this context, regret minimisation in stochastic bandits under
$\epsilon$-global Differential Privacy (DP) has been widely studied. Unlike
bandits without DP, there is a significant gap between the best-known regret
lower and upper bound in this setting, though they "match" in order. Thus, we
revisit the regret lower and upper bounds of $\epsilon$-global DP algorithms
for Bernoulli bandits and improve both. First, we prove a tighter regret lower
bound involving a novel information-theoretic quantity characterising the
hardness of $\epsilon$-global DP in stochastic bandits. Our lower bound
strictly improves on the existing ones across all $\epsilon$ values. Then, we
choose two asymptotically optimal bandit algorithms, i.e. DP-KLUCB and DP-IMED,
and propose their DP versions using a unified blueprint, i.e., (a) running in
arm-dependent phases, and (b) adding Laplace noise to achieve privacy. For
Bernoulli bandits, we analyse the regrets of these algorithms and show that
their regrets asymptotically match our lower bound up to a constant arbitrary
close to 1. This refutes the conjecture that forgetting past rewards is
necessary to design optimal bandit algorithms under global DP. At the core of
our algorithms lies a new concentration inequality for sums of Bernoulli
variables under Laplace mechanism, which is a new DP version of the Chernoff
bound. This result is universally useful as the DP literature commonly treats
the concentrations of Laplace noise and random variables separately, while we
couple them to yield a tighter bound.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [159] [MDDFNet: Mamba-based Dynamic Dual Fusion Network for Traffic Sign Detection](https://arxiv.org/abs/2505.05491)
*TianYi Yu*

Main category: cs.CV

TL;DR: 论文提出了一种新的目标检测网络MDDFNet，用于交通标志检测，解决了特征提取单一和尺寸变化处理能力不足的问题，实验证明其在TT100K数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有交通标志检测方法存在特征提取单一和难以处理多尺度目标的问题，这些问题在通用目标检测任务中也普遍存在。

Method: 提出MDDFNet网络，结合动态双融合模块和Mamba骨干网络，动态双融合模块整合多样空间和语义信息，Mamba骨干实现全局和局部特征自适应融合。

Result: 在TT100K数据集上，MDDFNet性能优于其他先进检测器，同时保持实时处理能力。

Conclusion: MDDFNet有效解决了小目标交通标志检测的挑战，兼具高效性和性能优势。

Abstract: The Detection of small objects, especially traffic signs, is a critical
sub-task in object detection and autonomous driving. Despite signficant
progress in previous research, two main challenges remain. First, the issue of
feature extraction being too singular. Second, the detection process struggles
to efectively handle objects of varying sizes or scales. These problems are
also prevalent in general object detection tasks. To address these challenges,
we propose a novel object detection network, Mamba-based Dynamic Dual Fusion
Network (MDDFNet), for traffic sign detection. The network integrates a dynamic
dual fusion module and a Mamba-based backbone to simultaneously tackle the
aforementioned issues. Specifically, the dynamic dual fusion module utilizes
multiple branches to consolidate various spatial and semantic information, thus
enhancing feature diversity. The Mamba-based backbone leverages global feature
fusion and local feature interaction, combining features in an adaptive manner
to generate unique classification characteristics. Extensive experiments
conducted on the TT100K (Tsinghua-Tencent 100K) datasets demonstrate that
MDDFNet outperforms other state-of-the-art detectors, maintaining real-time
processing capabilities of single-stage models while achieving superior
performance. This confirms the efectiveness of MDDFNet in detecting small
traffic signs.

</details>


### [160] [DetoxAI: a Python Toolkit for Debiasing Deep Learning Models in Computer Vision](https://arxiv.org/abs/2505.05492)
*Ignacy Stępka,Lukasz Sztukiewicz,Michał Wiliński,Jerzy Stefanowski*

Main category: cs.CV

TL;DR: 本文介绍了DetoxAI，一个用于改善深度学习视觉分类器公平性的开源Python库。


<details>
  <summary>Details</summary>
Motivation: 大多数现有的机器学习公平性解决方案针对表格数据，不适用于依赖深度学习的视觉分类任务。

Method: DetoxAI实现了最先进的去偏算法、公平性度量和可视化工具，支持通过内部表征干预去偏。

Result: 该库提供了归因可视化工具和定量公平性度量，展示了如何缓解偏见。

Conclusion: 本文展示了DetoxAI的动机、设计和使用案例，为工程师和研究人员提供了实际价值。

Abstract: While machine learning fairness has made significant progress in recent
years, most existing solutions focus on tabular data and are poorly suited for
vision-based classification tasks, which rely heavily on deep learning. To
bridge this gap, we introduce DetoxAI, an open-source Python library for
improving fairness in deep learning vision classifiers through post-hoc
debiasing. DetoxAI implements state-of-the-art debiasing algorithms, fairness
metrics, and visualization tools. It supports debiasing via interventions in
internal representations and includes attribution-based visualization tools and
quantitative algorithmic fairness metrics to show how bias is mitigated. This
paper presents the motivation, design, and use cases of DetoxAI, demonstrating
its tangible value to engineers and researchers.

</details>


### [161] [Preliminary Explorations with GPT-4o(mni) Native Image Generation](https://arxiv.org/abs/2505.05501)
*Pu Cao,Feng Zhou,Junyi Ji,Qingye Kong,Zhixiang Lv,Mingjian Zhang,Xuekun Zhao,Siqi Wu,Yinghui Lin,Qing Song,Lu Yang*

Main category: cs.CV

TL;DR: 本文评估了GPT-4o在多种视觉生成任务中的能力，发现其在通用合成任务中表现优异，但在空间推理、知识密集型任务等方面仍有局限。


<details>
  <summary>Details</summary>
Motivation: 探索GPT-4o在视觉生成任务中的能力，尤其是在多模态条件下的表现。

Method: 构建任务分类法并设计测试样本，对GPT-4o在六类任务（传统图像生成、判别任务、知识/常识生成、空间/时间感知生成）中进行了全面评估。

Result: GPT-4o在文本到图像生成、视觉风格化等通用任务中表现优异，但在空间推理、知识密集型任务（如科学插图）中存在幻觉或错误。

Conclusion: GPT-4o在多模态生成领域有显著进步，但尚未达到专业或安全关键领域的可靠应用水平。

Abstract: Recently, the visual generation ability by GPT-4o(mni) has been unlocked by
OpenAI. It demonstrates a very remarkable generation capability with excellent
multimodal condition understanding and varied task instructions. In this paper,
we aim to explore the capabilities of GPT-4o across various tasks. Inspired by
previous study, we constructed a task taxonomy along with a carefully curated
set of test samples to conduct a comprehensive qualitative test. Benefiting
from GPT-4o's powerful multimodal comprehension, its image-generation process
demonstrates abilities surpassing those of traditional image-generation tasks.
Thus, regarding the dimensions of model capabilities, we evaluate its
performance across six task categories: traditional image generation tasks,
discriminative tasks, knowledge-based generation, commonsense-based generation,
spatially-aware image generation, and temporally-aware image generation. These
tasks not only assess the quality and conditional alignment of the model's
outputs but also probe deeper into GPT-4o's understanding of real-world
concepts. Our results reveal that GPT-4o performs impressively well in
general-purpose synthesis tasks, showing strong capabilities in text-to-image
generation, visual stylization, and low-level image processing. However,
significant limitations remain in its ability to perform precise spatial
reasoning, instruction-grounded generation, and consistent temporal prediction.
Furthermore, when faced with knowledge-intensive or domain-specific scenarios,
such as scientific illustrations or mathematical plots, the model often
exhibits hallucinations, factual errors, or structural inconsistencies. These
findings suggest that while GPT-4o marks a substantial advancement in unified
multimodal generation, there is still a long way to go before it can be
reliably applied to professional or safety-critical domains.

</details>


### [162] [X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP](https://arxiv.org/abs/2505.05528)
*Hanxun Huang,Sarah Erfani,Yige Li,Xingjun Ma,James Bailey*

Main category: cs.CV

TL;DR: X-Transfer是一种新型攻击方法，通过生成通用对抗扰动（UAP），揭示了CLIP模型在不同样本、任务和领域的通用对抗脆弱性，显著优于现有UAP方法。


<details>
  <summary>Details</summary>
Motivation: 针对CLIP模型及其下游任务中对抗扰动的敏感性日益突出，研究其广泛适用性的对抗脆弱性以提升模型安全。

Method: 采用动态选择合适代理模型的代理缩放策略（surrogate scaling），生成具有超级可迁移性的UAP，实现跨数据、领域、模型和任务的攻击。

Result: 实验表明，X-Transfer显著优于现有UAP方法，确立了CLIP模型对抗可迁移性的新基准。

Conclusion: X-Transfer展示了CLIP模型在对抗攻击中的通用脆弱性，为未来模型安全性研究提供了重要参考。

Abstract: As Contrastive Language-Image Pre-training (CLIP) models are increasingly
adopted for diverse downstream tasks and integrated into large vision-language
models (VLMs), their susceptibility to adversarial perturbations has emerged as
a critical concern. In this work, we introduce \textbf{X-Transfer}, a novel
attack method that exposes a universal adversarial vulnerability in CLIP.
X-Transfer generates a Universal Adversarial Perturbation (UAP) capable of
deceiving various CLIP encoders and downstream VLMs across different samples,
tasks, and domains. We refer to this property as \textbf{super
transferability}--a single perturbation achieving cross-data, cross-domain,
cross-model, and cross-task adversarial transferability simultaneously. This is
achieved through \textbf{surrogate scaling}, a key innovation of our approach.
Unlike existing methods that rely on fixed surrogate models, which are
computationally intensive to scale, X-Transfer employs an efficient surrogate
scaling strategy that dynamically selects a small subset of suitable surrogates
from a large search space. Extensive evaluations demonstrate that X-Transfer
significantly outperforms previous state-of-the-art UAP methods, establishing a
new benchmark for adversarial transferability across CLIP models. The code is
publicly available in our
\href{https://github.com/HanxunH/XTransferBench}{GitHub repository}.

</details>


### [163] [GaMNet: A Hybrid Network with Gabor Fusion and NMamba for Efficient 3D Glioma Segmentation](https://arxiv.org/abs/2505.05520)
*Chengwei Ye,Huanzhen Zhang,Yufei Lin,Kangsheng Wang,Linuo Xu,Shuyan Liu*

Main category: cs.CV

TL;DR: GaMNet提出了一种结合NMamba模块和多尺度CNN的方法，用于高效分割脑胶质瘤，同时提升模型的可解释性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于CNN和Transformer的模型在脑瘤分割中常因全局上下文建模不足或计算复杂度高，难以在移动医疗设备上实时应用。

Method: 集成了NMamba模块进行全局建模，多尺度CNN用于局部特征提取，并使用多尺度Gabor滤波器提升模型可解释性。

Result: 实验表明，GaMNet在分割精度上优于现有方法，显著减少假阳性和假阴性，提升临床诊断可靠性，同时参数量更少、计算更快。

Conclusion: GaMNet为脑胶质瘤分割提供了一种高效、轻量且可解释的解决方案，适用于实时医疗应用。

Abstract: Gliomas are aggressive brain tumors that pose serious health risks. Deep
learning aids in lesion segmentation, but CNN and Transformer-based models
often lack context modeling or demand heavy computation, limiting real-time use
on mobile medical devices. We propose GaMNet, integrating the NMamba module for
global modeling and a multi-scale CNN for efficient local feature extraction.
To improve interpretability and mimic the human visual system, we apply Gabor
filters at multiple scales. Our method achieves high segmentation accuracy with
fewer parameters and faster computation. Extensive experiments show GaMNet
outperforms existing methods, notably reducing false positives and negatives,
which enhances the reliability of clinical diagnosis.

</details>


### [164] [Prompt to Polyp: Clinically-Aware Medical Image Synthesis with Diffusion Models](https://arxiv.org/abs/2505.05573)
*Mikhail Chaichuk,Sushant Gautam,Steven Hicks,Elena Tutubalina*

Main category: cs.CV

TL;DR: 该研究比较了两种生成医学图像的文本到图像合成方法：微调大型预训练扩散模型和小型领域特定模型训练。提出了一种优化的MSDM模型，结果表明大型模型保真度高，但MSDM模型在降低计算成本的同时保持可比质量。


<details>
  <summary>Details</summary>
Motivation: 解决医疗AI中的数据稀缺问题，同时保护患者隐私，通过文本描述生成逼真的医学图像。

Method: 1. 微调大型预训练的潜在扩散模型（如FLUX、Kandinsky）。2. 训练小型领域特定模型（MSDM），整合临床文本编码器、变分自编码器和交叉注意力机制。在结肠镜（MedVQA-GI）和放射学（ROCOv2）数据集上评估。

Result: 大型模型生成图像保真度更高，但MSDM在降低计算成本的同时实现了可比的质量，专家评估揭示了两种方法的优缺点。

Conclusion: MSDM是一种计算高效的替代方案，适用于医学图像生成任务，平衡了质量与资源消耗。

Abstract: The generation of realistic medical images from text descriptions has
significant potential to address data scarcity challenges in healthcare AI
while preserving patient privacy. This paper presents a comprehensive study of
text-to-image synthesis in the medical domain, comparing two distinct
approaches: (1) fine-tuning large pre-trained latent diffusion models and (2)
training small, domain-specific models. We introduce a novel model named MSDM,
an optimized architecture based on Stable Diffusion that integrates a clinical
text encoder, variational autoencoder, and cross-attention mechanisms to better
align medical text prompts with generated images. Our study compares two
approaches: fine-tuning large pre-trained models (FLUX, Kandinsky) versus
training compact domain-specific models (MSDM). Evaluation across colonoscopy
(MedVQA-GI) and radiology (ROCOv2) datasets reveals that while large models
achieve higher fidelity, our optimized MSDM delivers comparable quality with
lower computational costs. Quantitative metrics and qualitative evaluations by
medical experts reveal strengths and limitations of each approach.

</details>


### [165] [ReactDance: Progressive-Granular Representation for Long-Term Coherent Reactive Dance Generation](https://arxiv.org/abs/2505.05589)
*Jingzhong Lin,Yuanyuan Qi,Xinru Li,Wenxuan Huang,Xiangfeng Xu,Bangyan Li,Xuejiao Wang,Gaoqi He*

Main category: cs.CV

TL;DR: ReactDance是一种基于扩散模型的新框架，用于高保真的反应式舞蹈生成，通过多尺度解耦运动表示和局部块上下文的采样策略，解决了现有方法在交互保真度、同步性和时间一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于强调全局约束和优化，忽略了局部信息（如细粒度空间交互和局部时间上下文），导致交互保真度、同步性和时间一致性不足。

Method: 提出了Group Residual Finite Scalar Quantization（GRFSQ）多尺度解耦运动表示和Blockwise Local Context（BLC）采样策略，结合Layer-Decoupled Classifier-free Guidance（LDCFG）实现多尺度控制。

Result: 在标准基准测试中，ReactDance超越了现有方法，达到了最先进的性能。

Conclusion: ReactDance通过多尺度表示和局部上下文采样，显著提升了反应式舞蹈生成的保真度和一致性，为舞蹈合成领域提供了新的解决方案。

Abstract: Reactive dance generation (RDG) produces follower movements conditioned on
guiding dancer and music while ensuring spatial coordination and temporal
coherence. However, existing methods overemphasize global constraints and
optimization, overlooking local information, such as fine-grained spatial
interactions and localized temporal context. Therefore, we present ReactDance,
a novel diffusion-based framework for high-fidelity RDG with long-term
coherence and multi-scale controllability. Unlike existing methods that
struggle with interaction fidelity, synchronization, and temporal consistency
in duet synthesis, our approach introduces two key innovations: 1)Group
Residual Finite Scalar Quantization (GRFSQ), a multi-scale disentangled motion
representation that captures interaction semantics from coarse body rhythms to
fine-grained joint dynamics, and 2)Blockwise Local Context (BLC), a sampling
strategy eliminating error accumulation in long sequence generation via local
block causal masking and periodic positional encoding. Built on the decoupled
multi-scale GRFSQ representation, we implement a diffusion model
withLayer-Decoupled Classifier-free Guidance (LDCFG), allowing granular control
over motion semantics across scales. Extensive experiments on standard
benchmarks demonstrate that ReactDance surpasses existing methods, achieving
state-of-the-art performance.

</details>


### [166] [Enhancing Satellite Object Localization with Dilated Convolutions and Attention-aided Spatial Pooling](https://arxiv.org/abs/2505.05599)
*Seraj Al Mahmud Mostafa,Chenxi Wang,Jia Yue,Yuta Hozumi,Jianwu Wang*

Main category: cs.CV

TL;DR: 论文提出YOLO-DCAP模型，针对卫星图像中物体定位的高变异性、低分辨率和噪声干扰等问题，通过多尺度扩张残差卷积和注意力辅助空间池化模块改进YOLOv5，显著提升定位精度。


<details>
  <summary>Details</summary>
Motivation: 卫星图像中物体定位因高变异性、低分辨率和噪声干扰（如云层、城市灯光）而极具挑战性，尤其针对三个特定数据集（重力波、中层顶槽和海洋涡旋）。

Method: YOLO-DCAP结合多尺度扩张残差卷积（MDRC）捕捉多尺度特征，并引入注意力辅助空间池化（AaSP）模块聚焦全局相关区域，提升特征选择能力。

Result: 实验显示，YOLO-DCAP在mAP50和IoU上分别平均比基础模型提升20.95%和32.23%，优于现有最佳方法7.35%和9.84%，且在三个数据集中表现一致。

Conclusion: YOLO-DCAP通过结构改进显著提升了卫星图像中物体定位的鲁棒性和泛化能力，代码已开源。

Abstract: Object localization in satellite imagery is particularly challenging due to
the high variability of objects, low spatial resolution, and interference from
noise and dominant features such as clouds and city lights. In this research,
we focus on three satellite datasets: upper atmospheric Gravity Waves (GW),
mesospheric Bores (Bore), and Ocean Eddies (OE), each presenting its own unique
challenges. These challenges include the variability in the scale and
appearance of the main object patterns, where the size, shape, and feature
extent of objects of interest can differ significantly. To address these
challenges, we introduce YOLO-DCAP, a novel enhanced version of YOLOv5 designed
to improve object localization in these complex scenarios. YOLO-DCAP
incorporates a Multi-scale Dilated Residual Convolution (MDRC) block to capture
multi-scale features at scale with varying dilation rates, and an
Attention-aided Spatial Pooling (AaSP) module to focus on the global relevant
spatial regions, enhancing feature selection. These structural improvements
help to better localize objects in satellite imagery. Experimental results
demonstrate that YOLO-DCAP significantly outperforms both the YOLO base model
and state-of-the-art approaches, achieving an average improvement of 20.95% in
mAP50 and 32.23% in IoU over the base model, and 7.35% and 9.84% respectively
over state-of-the-art alternatives, consistently across all three satellite
datasets. These consistent gains across all three satellite datasets highlight
the robustness and generalizability of the proposed approach. Our code is open
sourced at
https://github.com/AI-4-atmosphere-remote-sensing/satellite-object-localization.

</details>


### [167] [Looking Beyond Language Priors: Enhancing Visual Comprehension and Attention in Multimodal Models](https://arxiv.org/abs/2505.05626)
*Aarti Ghatkesar,Uddeshya Upadhyay,Ganesh Venkatesh*

Main category: cs.CV

TL;DR: 本文提出了一种提升多模态大语言模型（MLLMs）视觉与语言深度融合的方法，通过增强其视觉理解能力并确保视觉信息主导语言生成，最终在视觉依赖任务上实现了10分的提升。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）存在视觉输入利用不足的问题，往往过度依赖语言先验，作者希望通过增强模型的视觉理解能力并确保视觉信息主导语言生成来解决这一问题。

Method: 作者首先分析了MLLMs内部如何构建对图像区域的视觉理解，并引入技术手段来增强这种能力。这些技术旨在加深模型对视觉内容的理解，并确保视觉信息能有效指导语言生成。通过上游分析量化模型预测视觉依赖标记的能力。

Result: 实验结果显示，所提出的方法在视觉依赖任务上实现了10分的提升，证明了其在多模态理解上的优越性。

Conclusion: 本文提出的方法有效增强了MLLMs对视觉信息的理解和利用能力，显著提升了模型在视觉相关任务上的表现。

Abstract: Achieving deep alignment between vision and language remains a central
challenge for Multimodal Large Language Models (MLLMs). These models often fail
to fully leverage visual input, defaulting to strong language priors. Our
approach first provides insights into how MLLMs internally build visual
understanding of image regions and then introduces techniques to amplify this
capability. Specifically, we explore techniques designed both to deepen the
model's understanding of visual content and to ensure that these visual
insights actively guide language generation. We demonstrate the superior
multimodal understanding of our resultant model through a detailed upstream
analysis quantifying its ability to predict visually-dependent tokens as well
as 10 pt boost on visually challenging tasks.

</details>


### [168] [Lost in OCR Translation? Vision-Based Approaches to Robust Document Retrieval](https://arxiv.org/abs/2505.05666)
*Alexander Most,Joseph Winjum,Ayan Biswas,Shawn Jones,Nishath Rajiv Ranasinghe,Dan O'Malley,Manish Bhattarai*

Main category: cs.CV

TL;DR: 研究了基于视觉的RAG系统（ColPali）与传统OCR-based RAG系统的对比，发现视觉方法在已微调文档上表现良好，而OCR方法在未见过的多样质量文档上泛化能力更强，并讨论了计算效率与语义准确性的权衡。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG系统依赖OCR处理扫描文档，但OCR在复杂或质量差文档中容易出错。研究比较视觉嵌入（跳过OCR）与传统OCR-based方法的效果差异。

Method: 使用Llama 3.2 (90B)和Nougat OCR，对比视觉RAG（ColPali）和OCR-based RAG，引入语义答案评估基准衡量端到端问答性能。

Result: 视觉RAG在微调文档上表现优，但OCR-based RAG对多样质量未见过文档泛化能力更强。

Conclusion: OCR-based方法在泛化性上更优，视觉RAG在特定场景高效，实际应用中需权衡计算效率与语义准确性。

Abstract: Retrieval-Augmented Generation (RAG) has become a popular technique for
enhancing the reliability and utility of Large Language Models (LLMs) by
grounding responses in external documents. Traditional RAG systems rely on
Optical Character Recognition (OCR) to first process scanned documents into
text. However, even state-of-the-art OCRs can introduce errors, especially in
degraded or complex documents. Recent vision-language approaches, such as
ColPali, propose direct visual embedding of documents, eliminating the need for
OCR. This study presents a systematic comparison between a vision-based RAG
system (ColPali) and more traditional OCR-based pipelines utilizing Llama 3.2
(90B) and Nougat OCR across varying document qualities. Beyond conventional
retrieval accuracy metrics, we introduce a semantic answer evaluation benchmark
to assess end-to-end question-answering performance. Our findings indicate that
while vision-based RAG performs well on documents it has been fine-tuned on,
OCR-based RAG is better able to generalize to unseen documents of varying
quality. We highlight the key trade-offs between computational efficiency and
semantic accuracy, offering practical guidance for RAG practitioners in
selecting between OCR-dependent and vision-based document retrieval systems in
production environments.

</details>


### [169] [HyperspectralMAE: The Hyperspectral Imagery Classification Model using Fourier-Encoded Dual-Branch Masked Autoencoder](https://arxiv.org/abs/2505.05710)
*Wooyoung Jeong,Hyun Jae Park,Seonghun Jeong,Jong Wook Jang,Tae Hoon Lim,Dae Seoung Kim*

Main category: cs.CV

TL;DR: HyperspectralMAE是一种基于Transformer的双重掩码预训练模型，通过同时掩码空间和光谱维度，结合谐波傅里叶位置编码，实现高维光谱图像的高效表示和下游任务迁移学习。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像的高维特性在空间和光谱域均带来挑战，传统方法难以有效提取特征，因此需要设计一种能同时学习空间和光谱信息的模型。

Method: 采用双重掩码策略（50%空间块和50%光谱带），结合谐波傅里叶位置编码和MSE+SAM重建目标，预训练大规模高光谱数据后微调下游任务。

Result: 在Indian Pines土地覆盖分类任务上达到SOTA，验证了双重掩码预训练和波长感知嵌入的有效性。

Conclusion: 双重掩码和谐波编码显著提升了高光谱图像的重建及下游任务表现，验证了模型设计的合理性。

Abstract: Hyperspectral imagery provides rich spectral detail but poses unique
challenges because of its high dimensionality in both spatial and spectral
domains. We propose \textit{HyperspectralMAE}, a Transformer-based foundation
model for hyperspectral data that employs a \textit{dual masking} strategy:
during pre-training we randomly occlude 50\% of spatial patches and 50\% of
spectral bands. This forces the model to learn representations capable of
reconstructing missing information across both dimensions. To encode spectral
order, we introduce learnable harmonic Fourier positional embeddings based on
wavelength. The reconstruction objective combines mean-squared error (MSE) with
the spectral angle mapper (SAM) to balance pixel-level accuracy and
spectral-shape fidelity.
  The resulting model contains about $1.8\times10^{8}$ parameters and produces
768-dimensional embeddings, giving it sufficient capacity for transfer
learning. We pre-trained HyperspectralMAE on two large hyperspectral corpora --
NASA EO-1 Hyperion ($\sim$1\,600 scenes, $\sim$$3\times10^{11}$ pixel spectra)
and DLR EnMAP Level-0 ($\sim$1\,300 scenes, $\sim$$3\times10^{11}$ pixel
spectra) -- and fine-tuned it for land-cover classification on the Indian Pines
benchmark. HyperspectralMAE achieves state-of-the-art transfer-learning
accuracy on Indian Pines, confirming that masked dual-dimensional pre-training
yields robust spectral-spatial representations. These results demonstrate that
dual masking and wavelength-aware embeddings advance hyperspectral image
reconstruction and downstream analysis.

</details>


### [170] [Towards Facial Image Compression with Consistency Preserving Diffusion Prior](https://arxiv.org/abs/2505.05870)
*Yimin Zhou,Yichong Xia,Bin Chen,Baoyi An,Haoqian Wang,Zhi Wang,Yaowei Wang,Zikun Zhou*

Main category: cs.CV

TL;DR: 提出FaSDiff方法，通过频率增强稳定扩散先验，在低比特率下提升人脸图像压缩质量和下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸图像压缩方法在低比特率下重建质量不佳，且直接应用扩散方法会因高频信息丢失影响下游任务。

Method: FaSDiff结合高频敏感压缩器和混合低频增强模块，通过端到端框架捕获细节并稳定调制扩散先验。

Result: FaSDiff在视觉质量和机器视觉准确性上均优于现有方法。

Conclusion: FaSDiff成功平衡了人类视觉感知与机器视觉需求，代码将在论文接受后公开。

Abstract: With the widespread application of facial image data across various domains,
the efficient storage and transmission of facial images has garnered
significant attention. However, the existing learned face image compression
methods often produce unsatisfactory reconstructed image quality at low bit
rates. Simply adapting diffusion-based compression methods to facial
compression tasks results in reconstructed images that perform poorly in
downstream applications due to insufficient preservation of high-frequency
information. To further explore the diffusion prior in facial image
compression, we propose Facial Image Compression with a Stable Diffusion Prior
(FaSDiff), a method that preserves consistency through frequency enhancement.
FaSDiff employs a high-frequency-sensitive compressor in an end-to-end
framework to capture fine image details and produce robust visual prompts.
Additionally, we introduce a hybrid low-frequency enhancement module that
disentangles low-frequency facial semantics and stably modulates the diffusion
prior alongside visual prompts. The proposed modules allow FaSDiff to leverage
diffusion priors for superior human visual perception while minimizing
performance loss in machine vision due to semantic inconsistency. Extensive
experiments show that FaSDiff outperforms state-of-the-art methods in balancing
human visual quality and machine vision accuracy. The code will be released
after the paper is accepted.

</details>


### [171] [Leveraging Vision-Language Models for Visual Grounding and Analysis of Automotive UI](https://arxiv.org/abs/2505.05895)
*Benjamin Raphael Ernhofer,Daniil Prokhorov,Jannica Langner,Dominik Bollmann*

Main category: cs.CV

TL;DR: 该论文提出了一种基于视觉-语言框架的汽车信息娱乐系统理解与交互方法，发布了开源数据集AutomotiveUI-Bench-4K，并展示了通过微调模型ELAM在跨领域泛化上的显著性能提升，同时验证了低成本数据收集和微调方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现代汽车信息娱乐系统频繁的用户界面更新和多样化设计需求，推动AI在该领域的理解和交互能力。

Method: 采用视觉-语言框架，结合开源数据集和合成数据管道，利用LoRa微调Molmo-7B模型，并引入视觉定位和评估能力，最终得到ELAM模型。

Result: ELAM在AutomotiveUI-Bench-4K上表现优异，ScreenSpot任务平均准确率达80.4%，跨领域泛化能力提升5.2%。

Conclusion: 通过数据收集和模型微调，实现了汽车UI理解与交互的显著进步，且方法成本低廉，适用于消费级GPU部署。

Abstract: Modern automotive infotainment systems require intelligent and adaptive
solutions to handle frequent User Interface (UI) updates and diverse design
variations. We introduce a vision-language framework for understanding and
interacting with automotive infotainment systems, enabling seamless adaptation
across different UI designs. To further support research in this field, we
release AutomotiveUI-Bench-4K, an open-source dataset of 998 images with 4,208
annotations. Additionally, we present a synthetic data pipeline to generate
training data. We fine-tune a Molmo-7B-based model using Low-Rank Adaptation
(LoRa) and incorporating reasoning generated by our pipeline, along with visual
grounding and evaluation capabilities. The fine-tuned Evaluative Large Action
Model (ELAM) achieves strong performance on AutomotiveUI-Bench-4K (model and
dataset are available on Hugging Face) and demonstrating strong cross-domain
generalization, including a +5.2% improvement on ScreenSpot over the baseline
model. Notably, our approach achieves 80.4% average accuracy on ScreenSpot,
closely matching or even surpassing specialized models for desktop, mobile, and
web, such as ShowUI, despite being trained for the infotainment domain. This
research investigates how data collection and subsequent fine-tuning can lead
to AI-driven progress within automotive UI understanding and interaction. The
applied method is cost-efficient and fine-tuned models can be deployed on
consumer-grade GPUs.

</details>


### [172] [Examining the Source of Defects from a Mechanical Perspective for 3D Anomaly Detection](https://arxiv.org/abs/2505.05901)
*Hanzhe Liang,Aoran Wang,Jie Zhou,Xin Jin,Can Gao,Jinbao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于异常原因的3D异常检测方法（MC4AD），通过力学互补框架生成内外部纠正力，结合多样化异常生成模块和纠正力预测网络，实现了高效且轻量化的异常检测。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测多关注结构性异常，而本文从异常成因出发，通过模拟内外部纠正力提升检测效果。

Method: 提出了MC4AD框架，包括DA-Gen模块生成多样化异常，CFP-Net预测纠正力，并设计了对称损失和整体损失的组合损失函数。

Result: 在自建和现有五个数据集上实现了九项最先进性能，模型参数最少且推理速度最快。

Conclusion: MC4AD不仅高效轻量化，还通过分层质量控制策略和三支决策拓展了工业应用场景。

Abstract: In this paper, we go beyond identifying anomalies only in structural terms
and think about better anomaly detection motivated by anomaly causes. Most
anomalies are regarded as the result of unpredictable defective forces from
internal and external sources, and their opposite forces are sought to correct
the anomalies. We introduced a Mechanics Complementary framework for 3D anomaly
detection (MC4AD) to generate internal and external Corrective forces for each
point. A Diverse Anomaly-Generation (DA-Gen) module is first proposed to
simulate various anomalies. Then, we present a Corrective Force Prediction
Network (CFP-Net) with complementary representations for point-level
representation to simulate the different contributions of internal and external
corrective forces. A combined loss was proposed, including a new symmetric loss
and an overall loss, to constrain the corrective forces properly. As a
highlight, we consider 3D anomaly detection in industry more comprehensively,
creating a hierarchical quality control strategy based on a three-way decision
and contributing a dataset named Anomaly-IntraVariance with intraclass variance
to evaluate the model. On the proposed and existing five datasets, we obtained
nine state-of-the-art performers with the minimum parameters and the fastest
inference speed. The source is available at
https://github.com/hzzzzzhappy/MC4AD

</details>


### [173] [Achieving 3D Attention via Triplet Squeeze and Excitation Block](https://arxiv.org/abs/2505.05943)
*Maan Alhazmi,Abdulrahman Altahhan*

Main category: cs.CV

TL;DR: 该论文提出了一种结合Triplet注意力和Squeeze-and-Excitation（TripSE）的新注意力机制，并在ResNet18、DenseNet和ConvNeXt架构中验证其有效性，尤其在ConvNeXt上表现突出，在FER2013数据集上达到了78.27%的准确率。


<details>
  <summary>Details</summary>
Motivation: ConvNeXt及其变体的出现证明了CNN在视觉任务中的潜力，尤其是面部表情识别（FER）。为了进一步提升性能，作者提出了一种新的注意力机制TripSE，以增强CNN模型的表达能力。

Method: 提出了四种结合Triplet注意力和Squeeze-and-Excitation（TripSE）的变体，并将其应用于ResNet18、DenseNet和ConvNeXt架构中，验证其普适性和效果。

Result: TripSE显著提升了CNN模型的性能，尤其在ConvNeXt架构上表现最佳。在四个数据集（CIFAR100、ImageNet、FER2013和AffectNet）上的实验结果显示，ConvNeXt+TripSE在FER2013数据集上达到了78.27%的准确率，刷新了该数据集的记录。

Conclusion: TripSE机制的引入有效提升了CNN模型的性能，尤其是在面部表情识别任务中，ConvNeXt+TripSE展现了目前最先进的表现。这一方法为CNN模型的设计提供了新的方向。

Abstract: The emergence of ConvNeXt and its variants has reaffirmed the conceptual and
structural suitability of CNN-based models for vision tasks, re-establishing
them as key players in image classification in general, and in facial
expression recognition (FER) in particular. In this paper, we propose a new set
of models that build on these advancements by incorporating a new set of
attention mechanisms that combines Triplet attention with
Squeeze-and-Excitation (TripSE) in four different variants. We demonstrate the
effectiveness of these variants by applying them to the ResNet18, DenseNet and
ConvNext architectures to validate their versatility and impact. Our study
shows that incorporating a TripSE block in these CNN models boosts their
performances, particularly for the ConvNeXt architecture, indicating its
utility. We evaluate the proposed mechanisms and associated models across four
datasets, namely CIFAR100, ImageNet, FER2013 and AffectNet datasets, where
ConvNext with TripSE achieves state-of-the-art results with an accuracy of
\textbf{78.27\%} on the popular FER2013 dataset, a new feat for this dataset.

</details>


### [174] [Web2Grasp: Learning Functional Grasps from Web Images of Hand-Object Interactions](https://arxiv.org/abs/2505.05517)
*Hongyi Chen,Yunchao Yao,Yufei Ye,Zhixuan Xu,Homanga Bharadhwaj,Jiashun Wang,Shubham Tulsiani,Zackory Erickson,Jeffrey Ichnowski*

Main category: cs.CV

TL;DR: 论文提出了一种利用网络图像提取人类抓取信息的方法，通过重建人机交互3D网格并训练抓取模型，有效提升了机器人手的灵活抓取能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多侧重于简单的力量抓取或依赖昂贵的遥控演示，而网络图像能提供自然的交互信息以降低成本并提升功能性。

Method: 从RGB图像重建人机交互3D网格，重新映射到机器人手，并与精确3D模型对齐；利用模拟器扩展抓取数据。

Result: 模型在仿真中达到75.8%（已知物体）和61.8%（全部物体）成功率，模拟增强后提升至83.4%；实物测试成功率为85%。

Conclusion: 网络数据驱动的抓取模型高效且成本低，模拟增强显著提升性能，验证了方法的实际可行性。

Abstract: Functional grasp is essential for enabling dexterous multi-finger robot hands
to manipulate objects effectively. However, most prior work either focuses on
power grasping, which simply involves holding an object still, or relies on
costly teleoperated robot demonstrations to teach robots how to grasp each
object functionally. Instead, we propose extracting human grasp information
from web images since they depict natural and functional object interactions,
thereby bypassing the need for curated demonstrations. We reconstruct human
hand-object interaction (HOI) 3D meshes from RGB images, retarget the human
hand to multi-finger robot hands, and align the noisy object mesh with its
accurate 3D shape. We show that these relatively low-quality HOI data from
inexpensive web sources can effectively train a functional grasping model. To
further expand the grasp dataset for seen and unseen objects, we use the
initially-trained grasping policy with web data in the IsaacGym simulator to
generate physically feasible grasps while preserving functionality. We train
the grasping model on 10 object categories and evaluate it on 9 unseen objects,
including challenging items such as syringes, pens, spray bottles, and tongs,
which are underrepresented in existing datasets. The model trained on the web
HOI dataset, achieving a 75.8% success rate on seen objects and 61.8% across
all objects in simulation, with a 6.7% improvement in success rate and a 1.8x
increase in functionality ratings over baselines. Simulator-augmented data
further boosts performance from 61.8% to 83.4%. The sim-to-real transfer to the
LEAP Hand achieves a 85% success rate. Project website is at:
https://webgrasp.github.io/.

</details>


### [175] [Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments](https://arxiv.org/abs/2505.05540)
*Pranav Guruprasad,Yangyue Wang,Sudipta Chowdhury,Harshvardhan Sikka*

Main category: cs.CV

TL;DR: 论文介绍了一个名为MultiNet v0.2的基准测试，用于评估视觉-语言-动作（VLA）模型在零样本泛化能力上的表现，发现现有模型在分布外（OOD）任务中的泛化能力有限，但VLA模型表现较好，且模型性能受提示工程影响显著。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在通用机器人系统中的重要性日益凸显，但对其零样本泛化能力的系统评估仍不足。论文旨在填补这一空白，通过设计测试基准分析模型在复杂环境中的表现。

Method: 提出MultiNet v0.2基准，评估了包括GPT-4o、OpenVLA等多种VLM和VLA模型在Procgen基准中的多样性任务上的表现，重点关注零样本泛化能力。

Result: 研究发现：(1) 所有模型在OOD任务中泛化能力有限；(2) VLA模型表现优于其他模型；(3) VLM变种在提示工程约束下表现显著提升。

Conclusion: VLA模型在零样本泛化上具有优势，但仍有改进空间，提示工程对模型性能影响显著。

Abstract: Vision-language-action (VLA) models represent an important step toward
general-purpose robotic systems by integrating visual perception, language
understanding, and action execution. However, systematic evaluation of these
models, particularly their zero-shot generalization capabilities in
out-of-distribution (OOD) environments, remains limited. In this paper, we
introduce MultiNet v0.2, a comprehensive benchmark designed to evaluate and
analyze the generalization performance of state-of-the-art VLM and VLA
models-including GPT-4o, GPT-4.1, OpenVLA,Pi0 Base, and Pi0 FAST-on diverse
procedural tasks from the Procgen benchmark. Our analysis reveals several
critical insights: (1) all evaluated models exhibit significant limitations in
zero-shot generalization to OOD tasks, with performance heavily influenced by
factors such as action representation and task complexit; (2) VLAs generally
outperform other models due to their robust architectural design; and (3) VLM
variants demonstrate substantial improvements when constrained appropriately,
highlighting the sensitivity of model performance to precise prompt
engineering.

</details>


### [176] [MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks](https://arxiv.org/abs/2505.06152)
*Wenqi Zeng,Yuqi Sun,Chenxi Ma,Weimin Tan,Bo Yan*

Main category: cs.CV

TL;DR: 论文提出首个大规模皮肤病多模态数据集MM-Skin，包含近10k高质量图文对，并基于此开发了皮肤病专用VLM模型SkinVL，在多种任务中表现优于通用及医疗VLM模型。


<details>
  <summary>Details</summary>
Motivation: 现有皮肤病多模态数据集缺乏专业性文本描述，限制了皮肤病视觉-语言模型（VLM）的发展，需要构建更专业的数据集和模型以提升诊断能力。

Method: 构建MM-Skin数据集（含3种影像模态和近10k图文对），并生成27k多样化VQA样本；基于此开发皮肤病专用VLM模型SkinVL。

Result: SkinVL在VQA、监督微调和零样本分类任务中，8个数据集上均优于通用及医疗VLM模型。

Conclusion: MM-Skin和SkinVL为皮肤病临床VLM助手的发展提供了重要支持，数据集已开源。

Abstract: Medical vision-language models (VLMs) have shown promise as clinical
assistants across various medical fields. However, specialized dermatology VLM
capable of delivering professional and detailed diagnostic analysis remains
underdeveloped, primarily due to less specialized text descriptions in current
dermatology multimodal datasets. To address this issue, we propose MM-Skin, the
first large-scale multimodal dermatology dataset that encompasses 3 imaging
modalities, including clinical, dermoscopic, and pathological and nearly 10k
high-quality image-text pairs collected from professional textbooks. In
addition, we generate over 27k diverse, instruction-following vision question
answering (VQA) samples (9 times the size of current largest dermatology VQA
dataset). Leveraging public datasets and MM-Skin, we developed SkinVL, a
dermatology-specific VLM designed for precise and nuanced skin disease
interpretation. Comprehensive benchmark evaluations of SkinVL on VQA,
supervised fine-tuning (SFT) and zero-shot classification tasks across 8
datasets, reveal its exceptional performance for skin diseases in comparison to
both general and medical VLM models. The introduction of MM-Skin and SkinVL
offers a meaningful contribution to advancing the development of clinical
dermatology VLM assistants. MM-Skin is available at
https://github.com/ZwQ803/MM-Skin

</details>


### [177] [Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data](https://arxiv.org/abs/2505.05752)
*Amin Ghafourian,Andrew Lee,Dechen Gao,Tyler Beer,Kin Yen,Iman Soltani*

Main category: cs.CV

TL;DR: 本文提出了一种利用点云数据自动化几何测量和合规评估的框架，结合深度学习与几何/信号处理技术，以提升基础设施检查的效率和准确性。通过应用于ADA标准的人行道坡道合规评估验证，展示了该方法的潜力和可靠性。


<details>
  <summary>Details</summary>
Motivation: 自动化基础设施检查能提高效率、准确性和可扩展性，而点云数据在该领域的应用尚有探索空间。

Method: 整合深度学习检测与分割技术，结合几何和信号处理方法，构建自动化测量与评估框架。

Result: 实验表明，该方法在评估ADA合规性时准确可靠，与人工测量结果相符，能显著减少人力成本并提高一致性。

Conclusion: 该框架不仅适用于ADA合规评估，还为更广泛的基础设施检查和自动化施工评估奠定了基础，推动了点云数据在这些领域的应用。

Abstract: Automation can play a prominent role in improving efficiency, accuracy, and
scalability in infrastructure surveying and assessing construction and
compliance standards. This paper presents a framework for automation of
geometric measurements and compliance assessment using point cloud data. The
proposed approach integrates deep learning-based detection and segmentation, in
conjunction with geometric and signal processing techniques, to automate
surveying tasks. As a proof of concept, we apply this framework to
automatically evaluate the compliance of curb ramps with the Americans with
Disabilities Act (ADA), demonstrating the utility of point cloud data in survey
automation. The method leverages a newly collected, large annotated dataset of
curb ramps, made publicly available as part of this work, to facilitate robust
model training and evaluation. Experimental results, including comparison with
manual field measurements of several ramps, validate the accuracy and
reliability of the proposed method, highlighting its potential to significantly
reduce manual effort and improve consistency in infrastructure assessment.
Beyond ADA compliance, the proposed framework lays the groundwork for broader
applications in infrastructure surveying and automated construction evaluation,
promoting wider adoption of point cloud data in these domains. The annotated
database, manual ramp survey data, and developed algorithms are publicly
available on the project's GitHub page:
https://github.com/Soltanilara/SurveyAutomation.

</details>


### [178] [Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition](https://arxiv.org/abs/2505.05829)
*Zhiyuan Chen,Keyi Li,Yifan Jia,Le Ye,Yufei Ma*

Main category: cs.CV

TL;DR: 该论文提出了一种增量校准缓存方法，用于加速扩散变压器（DiT）模型，减少计算复杂度，同时保持生成质量。通过低秩近似和通道感知SVD技术，有效校准缓存并处理异常激活，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散变压器（DiT）模型虽然生成能力强且可扩展，但其迭代性质导致高计算复杂度，限制了实际部署。现有缓存加速方法因缺乏校准可能导致质量下降，因此需要一种无需训练的优化方案。

Method: 提出增量校准缓存技术，利用预训练模型生成校准参数，通过低秩近似优化。针对异常激活引入通道感知SVD，进一步强化校准效果。

Result: 实验表明，该方法在相同计算资源下性能优于朴素缓存方法，相比35步DDIM减少45%以上计算，IS提升12，FID仅增加不到0.06。

Conclusion: 增量校准缓存是一种高效的DiT加速方法，显著降低计算负担且几乎不影响生成质量，适用于实际部署。

Abstract: Diffusion transformer (DiT) models have achieved remarkable success in image
generation, thanks for their exceptional generative capabilities and
scalability. Nonetheless, the iterative nature of diffusion models (DMs)
results in high computation complexity, posing challenges for deployment.
Although existing cache-based acceleration methods try to utilize the inherent
temporal similarity to skip redundant computations of DiT, the lack of
correction may induce potential quality degradation. In this paper, we propose
increment-calibrated caching, a training-free method for DiT acceleration,
where the calibration parameters are generated from the pre-trained model
itself with low-rank approximation. To deal with the possible correction
failure arising from outlier activations, we introduce channel-aware Singular
Value Decomposition (SVD), which further strengthens the calibration effect.
Experimental results show that our method always achieve better performance
than existing naive caching methods with a similar computation resource budget.
When compared with 35-step DDIM, our method eliminates more than 45%
computation and improves IS by 12 at the cost of less than 0.06 FID increase.
Code is available at https://github.com/ccccczzy/icc.

</details>


### [179] [Automated Knot Detection and Pairing for Wood Analysis in the Timber Industry](https://arxiv.org/abs/2505.05845)
*Guohao Lin,Shidong Pan,Rasul Khanbayov,Changxi Yang,Ani Khaloian-Sarnaghi,Andriy Kovryga*

Main category: cs.CV

TL;DR: 论文提出了一个基于机器学习的轻量级全自动管道，用于木材中结的检测和配对，提高了效率并验证了AI在木材科学中的潜力。


<details>
  <summary>Details</summary>
Motivation: 木材中的结对其美观和结构完整性至关重要，而传统的手动标注效率低下，因此需要自动化解决方案。

Method: 论文分两个阶段：检测阶段使用YOLOv8l进行高精度结检测，配对阶段通过多维特征提取和三元组神经网络实现结的配对。

Result: 检测阶段的mAP@0.5达到0.887，配对阶段准确率为0.85，验证了方法的有效性。

Conclusion: 该方案展示了AI在木材科学和工业中的潜力，显著提高了结检测和配对的效率。

Abstract: Knots in wood are critical to both aesthetics and structural integrity,
making their detection and pairing essential in timber processing. However,
traditional manual annotation was labor-intensive and inefficient,
necessitating automation. This paper proposes a lightweight and fully automated
pipeline for knot detection and pairing based on machine learning techniques.
In the detection stage, high-resolution surface images of wooden boards were
collected using industrial-grade cameras, and a large-scale dataset was
manually annotated and preprocessed. After the transfer learning, the YOLOv8l
achieves an mAP@0.5 of 0.887. In the pairing stage, detected knots were
analyzed and paired based on multidimensional feature extraction. A triplet
neural network was used to map the features into a latent space, enabling
clustering algorithms to identify and pair corresponding knots. The triplet
network with learnable weights achieved a pairing accuracy of 0.85. Further
analysis revealed that he distances from the knot's start and end points to the
bottom of the wooden board, and the longitudinal coordinates play crucial roles
in achieving high pairing accuracy. Our experiments validate the effectiveness
of the proposed solution, demonstrating the potential of AI in advancing wood
science and industry.

</details>


### [180] [Register and CLS tokens yield a decoupling of local and global features in large ViTs](https://arxiv.org/abs/2505.05892)
*Alexander Lappe,Martin A. Giese*

Main category: cs.CV

TL;DR: DINOv2模型的注意力图存在瑕疵，影响性能和可解释性。通过引入register token改善了注意力图质量，但全局与局部特征的整合仍不准确。CLS token会引发类似问题，研究指出了提升视觉模型可解释性的方向。


<details>
  <summary>Details</summary>
Motivation: DINOv2模型的注意力图因存储全局信息的冗余局部patch token而出现瑕疵，影响了模型的可解释性和密集图像任务的性能。研究旨在解决这一问题。

Method: 引入额外的register token存储全局信息，分析其对全局与局部特征关系的影响，并比较CLS token的类似行为。

Result: register token改善了注意力图的清晰度，但未能准确反映局部信息的整合；CLS token在无register token的模型中表现类似。

Conclusion: 大型ViT注意力图的解释需谨慎；通过明确register和CLS token的负面影响，为提升模型可解释性提供了路径。

Abstract: Recent work has shown that the attention maps of the widely popular DINOv2
model exhibit artifacts, which hurt both model interpretability and performance
on dense image tasks. These artifacts emerge due to the model repurposing patch
tokens with redundant local information for the storage of global image
information. To address this problem, additional register tokens have been
incorporated in which the model can store such information instead. We
carefully examine the influence of these register tokens on the relationship
between global and local image features, showing that while register tokens
yield cleaner attention maps, these maps do not accurately reflect the
integration of local image information in large models. Instead, global
information is dominated by information extracted from register tokens, leading
to a disconnect between local and global features. Inspired by these findings,
we show that the CLS token itself, which can be interpreted as a register,
leads to a very similar phenomenon in models without explicit register tokens.
Our work shows that care must be taken when interpreting attention maps of
large ViTs. Further, by clearly attributing the faulty behaviour to register
and CLS tokens, we show a path towards more interpretable vision models.

</details>


### [181] [From Pixels to Perception: Interpretable Predictions via Instance-wise Grouped Feature Selection](https://arxiv.org/abs/2505.06003)
*Moritz Vandenhirtz,Julia E. Vogt*

Main category: cs.CV

TL;DR: 提出了一种通过实例级别稀疏化实现可解释性预测的方法，学习语义区域的掩码并动态确定稀疏度，实验证明其比现有方法更易理解。


<details>
  <summary>Details</summary>
Motivation: 理解机器学习模型的决策过程对任务、数据及模型失败原因的分析至关重要，旨在提升预测的可解释性。

Method: 采用实例级别的输入图像稀疏化方法，在语义像素区域学习掩码，并动态确定每个实例所需的稀疏度。

Result: 在半合成和自然图像数据集上，该方法生成的预测比现有基准更易于人类理解。

Conclusion: 提出的方法能生成更具语义意义的预测，提升了模型的可解释性。

Abstract: Understanding the decision-making process of machine learning models provides
valuable insights into the task, the data, and the reasons behind a model's
failures. In this work, we propose a method that performs inherently
interpretable predictions through the instance-wise sparsification of input
images. To align the sparsification with human perception, we learn the masking
in the space of semantically meaningful pixel regions rather than on
pixel-level. Additionally, we introduce an explicit way to dynamically
determine the required level of sparsity for each instance. We show empirically
on semi-synthetic and natural image datasets that our inherently interpretable
classifier produces more meaningful, human-understandable predictions than
state-of-the-art benchmarks.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [182] [Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities](https://arxiv.org/abs/2505.06085)
*Hiari Pizzini Cavagna,Daniele Cesarini,Andrea Bartolini*

Main category: cs.PF

TL;DR: 论文评估了Tenstorrent Grayskull e75 RISC-V加速器在低精度线性代数运算中的性能，并与英特尔Sapphire Rapids及NVIDIA GPU对比，显示其在功耗与计算吞吐量上的竞争优势。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和大语言模型（LLMs）服务的需求增长，需要优化计算效率和能耗的专用硬件架构，Grayskull的潜力需要验证。

Method: 通过分析Grayskull的执行模型、网格大小、矩阵维度、数据格式及数值精度对效率的影响，并与主流硬件（如英特尔Sapphire Rapids和NVIDIA V100/A100）进行性能对比。

Result: NVIDIA GPU在原始性能上占优，但Grayskull在功耗与计算吞吐量（1.55 TFLOPs/Watt @ BF16）上表现出竞争力。

Conclusion: Grayskull在能效比方面具有实际应用价值，尤其在注重能耗的场景中可替代传统GPU。

Abstract: The increasing demand for generative AI as Large Language Models (LLMs)
services has driven the need for specialized hardware architectures that
optimize computational efficiency and energy consumption. This paper evaluates
the performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic
linear algebra kernels at reduced numerical precision, a fundamental operation
in LLM computations. We present a detailed characterization of Grayskull's
execution model, gridsize, matrix dimensions, data formats, and numerical
precision impact computational efficiency. Furthermore, we compare Grayskull's
performance against state-of-the-art architectures with tensor acceleration,
including Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).
Whilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a
competitive trade-off between power consumption and computational throughput,
reaching a peak of 1.55 TFLOPs/Watt with BF16.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [183] [PyResBugs: A Dataset of Residual Python Bugs for Natural Language-Driven Fault Injection](https://arxiv.org/abs/2505.05777)
*Domenico Cotroneo,Giuseppe De Rosa,Pietro Liguori*

Main category: cs.SE

TL;DR: PyResBugs是一个精心整理的Python框架中的遗留缺陷数据集，包含缺陷及其修复版本，并标注多级自然语言描述，支持自然语言驱动的故障注入。


<details>
  <summary>Details</summary>
Motivation: 传统测试难以检测到的缺陷在后期生产中暴露，PyResBugs旨在填补软件故障注入技术与真实世界代表性之间的鸿沟。

Method: 收集Python框架中的遗留缺陷，配对缺陷与修复版本，标注多级自然语言描述，支持自然语言驱动的故障注入。

Result: PyResBugs为Python系统的AI驱动自动化测试研究提供了高质量资源。

Conclusion: PyResBugs通过自然语言驱动的故障注入，提升了软件系统的真实世界故障模拟能力。

Abstract: This paper presents PyResBugs, a curated dataset of residual bugs, i.e.,
defects that persist undetected during traditional testing but later surface in
production, collected from major Python frameworks. Each bug in the dataset is
paired with its corresponding fault-free (fixed) version and annotated with
multi-level natural language (NL) descriptions. These NL descriptions enable
natural language-driven fault injection, offering a novel approach to
simulating real-world faults in software systems. By bridging the gap between
software fault injection techniques and real-world representativeness,
PyResBugs provides researchers with a high-quality resource for advancing
AI-driven automated testing in Python systems.

</details>


### [184] [PRIMG : Efficient LLM-driven Test Generation Using Mutant Prioritization](https://arxiv.org/abs/2505.05584)
*Mohamed Salah Bouafif,Mohammad Hamdaqa,Edward Zulkoski*

Main category: cs.SE

TL;DR: PRIMG框架结合突变优先级和测试生成模块，显著减少了测试套件规模，同时保持高突变覆盖率，并通过LLMs生成高质量测试用例。


<details>
  <summary>Details</summary>
Motivation: 突变测试常导致测试套件过大，增加计算开销。PRIMG旨在通过智能化的突变优先级和测试生成来优化这一问题。

Method: PRIMG包含突变优先级模块（基于子图机器学习的突变预测）和测试生成模块（利用LLMs生成并迭代优化测试用例）。

Result: 在Solidity项目中验证时，PRIMG显著降低测试套件规模且保持高突变覆盖率，优先级模块表现优于随机选择。

Conclusion: PRIMG有效提升突变分数和测试用例质量，解决了LLMs在边缘案例和复杂逻辑中的局限性。

Abstract: Mutation testing is a widely recognized technique for assessing and enhancing
the effectiveness of software test suites by introducing deliberate code
mutations. However, its application often results in overly large test suites,
as developers generate numerous tests to kill specific mutants, increasing
computational overhead. This paper introduces PRIMG (Prioritization and
Refinement Integrated Mutation-driven Generation), a novel framework for
incremental and adaptive test case generation for Solidity smart contracts.
PRIMG integrates two core components: a mutation prioritization module, which
employs a machine learning model trained on mutant subsumption graphs to
predict the usefulness of surviving mutants, and a test case generation module,
which utilizes Large Language Models (LLMs) to generate and iteratively refine
test cases to achieve syntactic and behavioral correctness.
  We evaluated PRIMG on real-world Solidity projects from Code4Arena to assess
its effectiveness in improving mutation scores and generating high-quality test
cases. The experimental results demonstrate that PRIMG significantly reduces
test suite size while maintaining high mutation coverage. The prioritization
module consistently outperformed random mutant selection, enabling the
generation of high-impact tests with reduced computational effort. Furthermore,
the refining process enhanced the correctness and utility of LLM-generated
tests, addressing their inherent limitations in handling edge cases and complex
program logic.

</details>


### [185] [Enhancing Large Language Models with Faster Code Preprocessing for Vulnerability Detection](https://arxiv.org/abs/2505.05600)
*José Gonçalves,Miguel Silva,Eva Maia,Isabel Praça*

Main category: cs.SE

TL;DR: SCoPE2是一种改进的预处理工具，显著减少处理时间并提升LLM在漏洞检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高漏洞检测的准确性，需要一种能标准化代码表示且高效的预处理工具。

Method: 基于SCoPE框架，开发了改进版本SCoPE2，并比较了两者在处理时间和内存使用上的性能。

Result: SCoPE2将处理时间减少了97.3%，并提升了LLM的F1分数。

Conclusion: SCoPE2通过改进预处理方法，显著提升了漏洞检测的效率和效果。

Abstract: The application of Artificial Intelligence has become a powerful approach to
detecting software vulnerabilities. However, effective vulnerability detection
relies on accurately capturing the semantic structure of code and its
contextual relationships. Given that the same functionality can be implemented
in various forms, a preprocessing tool that standardizes code representation is
important. This tool must be efficient, adaptable across programming languages,
and capable of supporting new transformations. To address this challenge, we
build on the existing SCoPE framework and introduce SCoPE2, an enhanced version
with improved performance. We compare both versions in terms of processing time
and memory usage and evaluate their impact on a Large Language Model (LLM) for
vulnerability detection. Our results show a 97.3\% reduction in processing time
with SCoPE2, along with an improved F1-score for the LLM, solely due to the
refined preprocessing approach.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [186] [Fast Differentiable Modal Simulation of Non-linear Strings, Membranes, and Plates](https://arxiv.org/abs/2505.05940)
*Rodrigo Diaz,Mark Sandler*

Main category: cs.SD

TL;DR: 该论文提出了一种基于JAX库的快速、可微分、GPU加速的模态框架，显著提升了非线性振动模型（如von Kármán板）的计算效率和可逆建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统非线性振动模型（如von Kármán板）的计算效率低且缺乏可微分性，限制了逆建模和实时应用。

Method: 利用JAX库构建了GPU加速的模态框架，支持高效模拟和基于梯度的逆建模。

Result: 实验表明，该框架在计算性能上显著优于CPU和GPU传统实现，并成功从合成和实验数据中恢复物理参数（如张力、刚度和几何形状）。

Conclusion: 该方法虽然对参数初始化更敏感，但提供了更高的可解释性和更紧凑的参数化，代码已开源以支持未来研究。

Abstract: Modal methods for simulating vibrations of strings, membranes, and plates are
widely used in acoustics and physically informed audio synthesis. However,
traditional implementations, particularly for non-linear models like the von
K\'arm\'an plate, are computationally demanding and lack differentiability,
limiting inverse modelling and real-time applications. We introduce a fast,
differentiable, GPU-accelerated modal framework built with the JAX library,
providing efficient simulations and enabling gradient-based inverse modelling.
Benchmarks show that our approach significantly outperforms CPU and GPU-based
implementations, particularly for simulations with many modes. Inverse
modelling experiments demonstrate that our approach can recover physical
parameters, including tension, stiffness, and geometry, from both synthetic and
experimental data. Although fitting physical parameters is more sensitive to
initialisation compared to other methods, it provides greater interpretability
and more compact parameterisation. The code is released as open source to
support future research and applications in differentiable physical modelling
and sound synthesis.

</details>


### [187] [Learning Music Audio Representations With Limited Data](https://arxiv.org/abs/2505.06042)
*Christos Plachouras,Emmanouil Benetos,Johan Pauwels*

Main category: cs.SD

TL;DR: 研究探讨了在有限数据条件下，不同音乐音频表示模型的性能表现，发现某些情况下小数据集模型甚至随机模型的表现可与大数据集模型媲美，但手工特征在部分任务中仍优于学习到的表示。


<details>
  <summary>Details</summary>
Motivation: 解决在音频数据或标注稀缺的情境下（如小众音乐传统、非流行流派或个性化音乐创作与聆听），大型深度学习模型性能受限的问题，探索有限数据条件下模型的行为。

Method: 通过训练多种架构、训练范式及输入时长的音乐音频表示模型（数据量从5到8,000分钟不等），在多种音乐信息检索任务中评估其表现，并分析其对噪声的鲁棒性。

Result: 在特定条件下，有限数据或随机模型的表示性能与大数据集模型相当，但手工特征在部分任务中表现更优。

Conclusion: 有限数据条件下的模型表现可能被低估，未来研究可探索如何进一步提升小数据集的模型性能，同时手工特征在某些场景中仍具竞争力。

Abstract: Large deep-learning models for music, including those focused on learning
general-purpose music audio representations, are often assumed to require
substantial training data to achieve high performance. If true, this would pose
challenges in scenarios where audio data or annotations are scarce, such as for
underrepresented music traditions, non-popular genres, and personalized music
creation and listening. Understanding how these models behave in limited-data
scenarios could be crucial for developing techniques to tackle them.
  In this work, we investigate the behavior of several music audio
representation models under limited-data learning regimes. We consider music
models with various architectures, training paradigms, and input durations, and
train them on data collections ranging from 5 to 8,000 minutes long. We
evaluate the learned representations on various music information retrieval
tasks and analyze their robustness to noise. We show that, under certain
conditions, representations from limited-data and even random models perform
comparably to ones from large-dataset models, though handcrafted features
outperform all learned representations in some tasks.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [188] [An empathic GPT-based chatbot to talk about mental disorders with Spanish teenagers](https://arxiv.org/abs/2505.05828)
*Alba María Mármol-Romero,Manuel García-Vega,Miguel Ángel García-Cumbreras,Arturo Montejo-Ráez*

Main category: cs.HC

TL;DR: 论文摘要：介绍了一种基于聊天机器人的系统，通过自我披露技术提高西班牙年轻人对特定心理障碍的认知。系统结合封闭和开放对话，根据用户敏感度调整话题。结果显示，此类系统对青少年具有吸引力并能提升心理障碍意识。


<details>
  <summary>Details</summary>
Motivation: 旨在通过技术手段提升青少年对心理障碍的认知，采用聊天机器人形式增强互动性与同理心。

Method: 研究采用混合封闭与开放对话的聊天机器人系统，结合GPT-3模型进行开放对话，针对12-18岁西班牙青少年群体进行测试。

Result: 系统受到青少年欢迎，并有效提高了他们对特定心理障碍的认知。

Conclusion: 基于聊天机器人的自我披露技术是提升青少年心理障碍意识的有效工具。

Abstract: This paper presents a chatbot-based system to engage young Spanish people in
the awareness of certain mental disorders through a self-disclosure technique.
The study was carried out in a population of teenagers aged between 12 and 18
years. The dialogue engine mixes closed and open conversations, so certain
controlled messages are sent to focus the chat on a specific disorder, which
will change over time. Once a set of trial questions is answered, the system
can initiate the conversation on the disorder under the focus according to the
user's sensibility to that disorder, in an attempt to establish a more
empathetic communication. Then, an open conversation based on the GPT-3
language model is initiated, allowing the user to express themselves with more
freedom. The results show that these systems are of interest to young people
and could help them become aware of certain mental disorders.

</details>


### [189] [Would You Rely on an Eerie Agent? A Systematic Review of the Impact of the Uncanny Valley Effect on Trust in Human-Agent Interaction](https://arxiv.org/abs/2505.05543)
*Ahdiyeh Alipour,Tilo Hartmann,Maryam Alimardani*

Main category: cs.HC

TL;DR: 论文综述了‘恐怖谷效应’（UVE）对人类信任人工代理的影响，分析了现有研究的方法和局限性，并提出了一个分类信任测量方法的新框架。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能代理在日常生活中的普及，理解人类如何感知和信任这些代理变得至关重要。研究探讨了UVE与信任之间的关系，填补了现有文献中缺乏系统性理解的空白。

Method: 遵循PRISMA指南，系统性搜索并分析了53项同时研究UVE和信任的实证研究，内容涵盖代理类型、交互方式、方法和测量工具。

Result: 发现多数研究依赖静态图像或假设场景，缺乏实时交互，且主要使用主观信任测量方法。提出了一个基于最佳实践的分类框架。

Conclusion: 这项综述为理解UVE和信任的相互关系提供了基础，为未来研究指明了方向。

Abstract: Trust is a fundamental component of human-agent interaction. With the
increasing presence of artificial agents in daily life, it is essential to
understand how people perceive and trust these agents. One of the key
challenges affecting this perception is the Uncanny Valley Effect (UVE), where
increasingly human-like artificial beings can be perceived as eerie or
repelling. Despite growing interest in trust and the UVE, existing research
varies widely in terms of how these concepts are defined and operationalized.
This inconsistency raises important questions about how and under what
conditions the UVE influences trust in agents. A systematic understanding of
their relationship is currently lacking. This review aims to examine the impact
of the UVE on human trust in agents and to identify methodological patterns,
limitations, and gaps in the existing empirical literature. Following PRISMA
guidelines, a systematic search identified 53 empirical studies that
investigated both UVE-related constructs and trust or trust-related outcomes.
Studies were analyzed based on a structured set of categories, including types
of agents and interactions, methodological and measurement approaches, and key
findings. The results of our systematic review reveal that most studies rely on
static images or hypothetical scenarios with limited real-time interaction, and
the majority use subjective trust measures. This review offers a novel
framework for classifying trust measurement approaches with regard to the
best-practice criteria for empirically investigating the UVE. As the first
systematic attempt to map the intersection of UVE and trust, this review
contributes to a deeper understanding of their interplay and offers a
foundation for future research. Keywords: the uncanny valley effect, trust,
human-likeness, affinity response, human-agent interaction

</details>


### [190] [Extending Stress Detection Reproducibility to Consumer Wearable Sensors](https://arxiv.org/abs/2505.05694)
*Ohida Binte Amin,Varun Mishra,Tinashe M. Tapera,Robert Volpe,Aarti Sathyanarayana*

Main category: cs.HC

TL;DR: 本文研究了可穿戴传感器在压力检测模型中的可重复性，比较了研究级设备与消费级设备的性能。Biopac MP160表现最佳，而Garmin Forerunner 55s在实际应用中显示出潜力。


<details>
  <summary>Details</summary>
Motivation: 目前大多数研究集中在单一数据集上，缺乏对不同设备、人群或研究条件下模型可重复性的评估。本文旨在填补这一空白。

Method: 通过测试多种设备（Biopac MP160, Polar H10, Empatica E4, Garmin Forerunner 55s），在35名本科生中进行标准化压力诱导任务，评估设备特异性压力检测性能。

Result: Biopac MP160表现最稳定，而Garmin Forerunner 55s在实时压力监测中表现出色，尤其在心算任务中AUROC高达0.961。

Conclusion: 研究揭示了硬件-模型兼容性对压力检测通用性的挑战，但消费级设备如Garmin Forerunner 55s在现实场景中具有潜力。

Abstract: Wearable sensors are widely used to collect physiological data and develop
stress detection models. However, most studies focus on a single dataset,
rarely evaluating model reproducibility across devices, populations, or study
conditions. We previously assessed the reproducibility of stress detection
models across multiple studies, testing models trained on one dataset against
others using heart rate (with R-R interval) and electrodermal activity (EDA).
In this study, we extended our stress detection reproducibility to consumer
wearable sensors. We compared validated research-grade devices, to consumer
wearables - Biopac MP160, Polar H10, Empatica E4, to the Garmin Forerunner 55s,
assessing device-specific stress detection performance by conducting a new
stress study on undergraduate students. Thirty-five students completed three
standardized stress-induction tasks in a lab setting. Biopac MP160 performed
the best, being consistent with our expectations of it as the gold standard,
though performance varied across devices and models. Combining heart rate
variability (HRV) and EDA enhanced stress prediction across most scenarios.
However, Empatica E4 showed variability; while HRV and EDA improved stress
detection in leave-one-subject-out (LOSO) evaluations (AUROC up to 0.953),
device-specific limitations led to underperformance when tested with our
pre-trained stress detection tool (AUROC 0.723), highlighting generalizability
challenges related to hardware-model compatibility. Garmin Forerunner 55s
demonstrated strong potential for real-world stress monitoring, achieving the
best mental arithmetic stress detection performance in LOSO (AUROC up to 0.961)
comparable to research-grade devices like Polar H10 (AUROC 0.954), and Empatica
E4 (AUROC 0.905 with HRV-only model and AUROC 0.953 with HRV+EDA model), with
the added advantage of consumer-friendly wearability for free-living contexts.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [191] [DaringFed: A Dynamic Bayesian Persuasion Pricing for Online Federated Learning under Two-sided Incomplete Information](https://arxiv.org/abs/2505.05842)
*Yun Xin,Jianfeng Lu,Shuqin Cao,Gang Li,Haozhao Wang,Guanghui Wen*

Main category: cs.GT

TL;DR: 论文提出了一种名为DaringFed的动态贝叶斯说服定价机制，用于在线联邦学习（OFL）中以激励客户端参与。该机制在双向不完全信息（TII）条件下动态分配奖励，优化了模型准确性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 在线联邦学习（OFL）需要客户端动态参与，但传统的激励机制无法应对双向不完全信息（TII）条件下的动态资源变化。因此，设计一种有效的动态激励机制成为关键。

Method: 论文将服务器与客户端的交互建模为一个动态信号与定价分配的贝叶斯说服博弈问题，证明了其存在唯一的纳什均衡，并在一侧不完全信息条件下推导出最优设计，进一步分析了TII条件下的近似最优设计。

Result: 实验结果表明，DaringFed在真实数据集上优化了16.99%的准确性和收敛速度，在合成数据集上验证了其能有效提升服务器效用达12.6%。

Conclusion: DaringFed在双向不完全信息条件下通过动态激励机制显著提升了在线联邦学习的参与度和性能，为实际应用提供了可行的解决方案。

Abstract: Online Federated Learning (OFL) is a real-time learning paradigm that
sequentially executes parameter aggregation immediately for each random
arriving client. To motivate clients to participate in OFL, it is crucial to
offer appropriate incentives to offset the training resource consumption.
However, the design of incentive mechanisms in OFL is constrained by the
dynamic variability of Two-sided Incomplete Information (TII) concerning
resources, where the server is unaware of the clients' dynamically changing
computational resources, while clients lack knowledge of the real-time
communication resources allocated by the server. To incentivize clients to
participate in training by offering dynamic rewards to each arriving client, we
design a novel Dynamic Bayesian persuasion pricing for online Federated
learning (DaringFed) under TII. Specifically, we begin by formulating the
interaction between the server and clients as a dynamic signaling and pricing
allocation problem within a Bayesian persuasion game, and then demonstrate the
existence of a unique Bayesian persuasion Nash equilibrium. By deriving the
optimal design of DaringFed under one-sided incomplete information, we further
analyze the approximate optimal design of DaringFed with a specific bound under
TII. Finally, extensive evaluation conducted on real datasets demonstrate that
DaringFed optimizes accuracy and converges speed by 16.99%, while experiments
with synthetic datasets validate the convergence of estimate unknown values and
the effectiveness of DaringFed in improving the server's utility by up to
12.6%.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [192] [Efficient Quantum Convolutional Neural Networks for Image Classification: Overcoming Hardware Constraints](https://arxiv.org/abs/2505.05957)
*Peter Röseler,Oliver Schaudt,Helmut Berg,Christian Bauckhage,Matthias Koch*

Main category: quant-ph

TL;DR: 论文提出了一种量子卷积神经网络（QCNN）的编码方案，降低输入维度，并在49量子位的架构上直接处理MNIST图像，无需传统降维预处理。实验在IBM量子处理器上实现96.08%的分类准确率，超过传统方法的71.74%。


<details>
  <summary>Details</summary>
Motivation: 量子计算为神经网络架构提供了新的可能性，但现有NISQ设备的硬件限制使得量子CNN的实现具有挑战性。研究旨在解决这一问题。

Method: 提出了一种降低输入维度的编码方案，并设计了一个基于表达性、纠缠和复杂性特征的自动化框架来选择QCNN的基本组件PQCs。

Result: 在IBM Heron r2量子处理器上，实现了96.08%的分类准确率，显著高于传统方法的71.74%，且收敛速度更快。

Conclusion: 该研究验证了量子计算在图像分类中的潜力，是首批在真实量子硬件上实现的高性能图像分类方案之一。

Abstract: While classical convolutional neural networks (CNNs) have revolutionized
image classification, the emergence of quantum computing presents new
opportunities for enhancing neural network architectures. Quantum CNNs (QCNNs)
leverage quantum mechanical properties and hold potential to outperform
classical approaches. However, their implementation on current noisy
intermediate-scale quantum (NISQ) devices remains challenging due to hardware
limitations. In our research, we address this challenge by introducing an
encoding scheme that significantly reduces the input dimensionality. We
demonstrate that a primitive QCNN architecture with 49 qubits is sufficient to
directly process $28\times 28$ pixel MNIST images, eliminating the need for
classical dimensionality reduction pre-processing. Additionally, we propose an
automated framework based on expressibility, entanglement, and complexity
characteristics to identify the building blocks of QCNNs, parameterized quantum
circuits (PQCs). Our approach demonstrates advantages in accuracy and
convergence speed with a similar parameter count compared to both hybrid QCNNs
and classical CNNs. We validated our experiments on IBM's Heron r2 quantum
processor, achieving $96.08\%$ classification accuracy, surpassing the
$71.74\%$ benchmark of traditional approaches under identical training
conditions. These results represent one of the first implementations of image
classifications on real quantum hardware and validate the potential of quantum
computing in this area.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [193] [Differentiating Emigration from Return Migration of Scholars Using Name-Based Nationality Detection Models](https://arxiv.org/abs/2505.06107)
*Faeze Ghorbanpour,Thiago Zordan Malaguth,Aliakbar Akbaritabar*

Main category: cs.DL

TL;DR: 论文提出了一种通过全名检测国籍的方法，解决了移民研究中因隐私导致国籍数据缺失的问题，并验证了该方法在研究学者迁移中的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决因隐私问题导致的国籍数据缺失，尤其是在移民研究中区分移民和回归移民的挑战。

Method: 使用从Wikipedia收集的260万姓名-国籍对作为训练数据，通过基于字符的机器学习模型分类国籍，并在Scopus数据中应用。

Result: 模型在不同粒度分类中表现良好（加权F1最高84%），发现学术起源低估了回归移民规模，尤其在多样性国家如美国、澳大利亚和加拿大。

Conclusion: 该方法有效解决了移民研究中的左截尾问题，为使用数字痕迹数据研究迁移提供了新思路。

Abstract: Most web and digital trace data do not include information about an
individual's nationality due to privacy concerns. The lack of data on
nationality can create challenges for migration research. It can lead to a
left-censoring issue since we are uncertain about the migrant's country of
origin. Once we observe an emigration event, if we know the nationality, we can
differentiate it from return migration. We propose methods to detect the
nationality with the least available data, i.e., full names. We use the
detected nationality in comparison with the country of academic origin, which
is a common approach in studying the migration of researchers. We gathered 2.6
million unique name-nationality pairs from Wikipedia and categorized them into
families of nationalities with three granularity levels to use as our training
data. Using a character-based machine learning model, we achieved a weighted F1
score of 84% for the broadest and 67% for the most granular, country-level
categorization. In our empirical study, we used the trained and tested model to
assign nationality to 8+ million scholars' full names in Scopus data. Our
results show that using the country of first publication as a proxy for
nationality underestimates the size of return flows, especially for countries
with a more diverse academic workforce, such as the USA, Australia, and Canada.
We found that around 48% of emigration from the USA was return migration once
we used the country of name origin, in contrast to 33% based on academic
origin. In the most recent period, 79% of scholars whose affiliation has
consistently changed from the USA to China, and are considered emigrants, have
Chinese names in contrast to 41% with a Chinese academic origin. Our proposed
methods for addressing left-censoring issues are beneficial for other research
that uses digital trace data to study migration.

</details>
